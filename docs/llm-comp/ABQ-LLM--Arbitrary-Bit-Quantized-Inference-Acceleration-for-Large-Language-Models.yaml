- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
    Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ABQ-LLM：用于大语言模型的任意比特量化推理加速
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08554](https://ar5iv.labs.arxiv.org/html/2408.08554)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.08554](https://ar5iv.labs.arxiv.org/html/2408.08554)
- en: Chao Zeng \equalcontrib, Songwei Liu\equalcontrib, Yusheng Xie\equalcontrib,
    Hong Liu, Xiaojian Wang,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Chao Zeng \equalcontrib，Songwei Liu\equalcontrib，Yusheng Xie\equalcontrib，Hong
    Liu，Xiaojian Wang，
- en: Miao Wei, Shu Yang, Fangmin Chen, Xing Mei Corresponding author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Miao Wei，Shu Yang，Fangmin Chen，Xing Mei 通讯作者。
- en: 'Code will available at: https://github.com/bytedance/ABQ-LLM'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将在此处提供： [https://github.com/bytedance/ABQ-LLM](https://github.com/bytedance/ABQ-LLM)
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have revolutionized natural language processing
    tasks. However, their practical application is constrained by substantial memory
    and computational demands. Post-training quantization (PTQ) is considered an effective
    method to accelerate LLM inference. Despite its growing popularity in LLM model
    compression, PTQ deployment faces two major challenges. First, low-bit quantization
    leads to performance degradation. Second, restricted by the limited integer computing
    unit type on GPUs, quantized matrix operations with different precisions cannot
    be effectively accelerated. To address these issues, we introduce a novel arbitrary-bit
    quantization algorithm and inference framework, ABQ-LLM. It achieves superior
    performance across various quantization settings and enables efficient arbitrary-precision
    quantized inference on the GPU. ABQ-LLM introduces several key innovations: (1)
    a distribution correction method for transformer blocks to mitigate distribution
    differences caused by full quantization of weights and activations, improving
    performance at low bit-widths. (2) the bit balance strategy to counteract performance
    degradation from asymmetric distribution issues at very low bit-widths (e.g.,
    2-bit). (3) an innovative quantization acceleration framework that reconstructs
    the quantization matrix multiplication of arbitrary precision combinations based
    on BTC (Binary TensorCore) equivalents, gets rid of the limitations of INT4/INT8
    computing units. ABQ-LLM can convert each component bit width gain into actual
    acceleration gain, maximizing performance under mixed precision(e.g., W6A6, W2A8).
    Based on W2*A8 quantization configuration on LLaMA-7B model, it achieved a WikiText2
    perplexity of 7.59 (2.17$\downarrow$ memory compression gain.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）已经彻底改变了自然语言处理任务。然而，它们的实际应用受限于巨大的内存和计算需求。后训练量化（PTQ）被认为是加速LLM推理的有效方法。尽管在LLM模型压缩中越来越受欢迎，PTQ的部署面临两个主要挑战。首先，低比特量化会导致性能下降。其次，由于GPU上有限的整数计算单元类型，不同精度的量化矩阵操作无法有效加速。为了解决这些问题，我们提出了一种新颖的任意比特量化算法和推理框架，ABQ-LLM。它在各种量化设置下实现了优越的性能，并实现了在GPU上高效的任意精度量化推理。ABQ-LLM引入了几个关键创新：（1）用于变换器块的分布修正方法，以减轻完全量化权重和激活引起的分布差异，提高低比特宽度下的性能。（2）比特平衡策略，以抵消在非常低比特宽度（例如2比特）下的非对称分布问题带来的性能下降。（3）一种创新的量化加速框架，通过基于BTC（Binary
    TensorCore）等效的任意精度组合重建量化矩阵乘法，摆脱INT4/INT8计算单元的限制。ABQ-LLM能够将每个组件比特宽度的增益转换为实际的加速增益，最大化混合精度下的性能（例如W6A6，W2A8）。基于LLaMA-7B模型的W2*A8量化配置，它实现了7.59的WikiText2困惑度（2.17$\downarrow$内存压缩增益）。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancements in large language models (LLMs) (Bubeck et al. [2023](#bib.bib5);
    Touvron et al. [2023a](#bib.bib34), [b](#bib.bib35)) have demonstrated impressive
    capabilities across various natural language benchmark, including reasoning (Clark
    et al. [2019](#bib.bib7), [2018](#bib.bib8)), cognitive processing (Xu et al.
    [2023a](#bib.bib38); Hardy et al. [2023](#bib.bib15)), and dialogue generation(Hu
    et al. [2023](#bib.bib18)). However, these models are characterized by a substantial
    number of parameters, posing significant challenges in terms of memory consumption
    and bandwidth (Zheng et al. [2024](#bib.bib43); Kim et al. [2023](#bib.bib20)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大语言模型（LLMs）方面的进展（Bubeck 等 [2023](#bib.bib5)；Touvron 等 [2023a](#bib.bib34)，[b](#bib.bib35)）展示了在各种自然语言基准测试中的惊人能力，包括推理（Clark
    等 [2019](#bib.bib7)，[2018](#bib.bib8)），认知处理（Xu 等 [2023a](#bib.bib38)；Hardy 等 [2023](#bib.bib15)），以及对话生成（Hu
    等 [2023](#bib.bib18)）。然而，这些模型的特点是参数数量庞大，导致在内存消耗和带宽方面面临重大挑战（Zheng 等 [2024](#bib.bib43)；Kim
    等 [2023](#bib.bib20)）。
- en: Post-training quantization (PTQ) effectively reduces both computational and
    storage requirements. This technique significantly accelerates model inference
    by converting the weights and activation values of large language models (LLMs)
    from high-precision floating-point numbers to low-precision integer values for
    storage, and using efficient integer matrix multiplication operators to handle
    the bulk of matrix multiplication computations during inference. Currently, 80%
    of the computation and parameter access in LLMs is concentrated on general matrix
    multiplication (GEMM) and vector multiplication (GEMV) operations, especially
    during autoregressive decoding, where all GEMM operations degrade into GEMV operations
    due to single-token generation. Consequently, the efficiency of GEMV computation
    and memory access directly determines the efficiency and power consumption of
    LLM inference.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后量化（PTQ）有效地减少了计算和存储需求。这项技术通过将大语言模型（LLMs）的权重和激活值从高精度浮点数转换为低精度整数值，从而显著加速模型推理，并使用高效的整数矩阵乘法操作符处理大部分矩阵乘法计算。当前，LLMs
    中 80% 的计算和参数访问集中在通用矩阵乘法（GEMM）和向量乘法（GEMV）操作上，特别是在自回归解码过程中，由于单个令牌生成，所有 GEMM 操作退化为
    GEMV 操作。因此，GEMV 计算和内存访问的效率直接决定了 LLM 推理的效率和功耗。
- en: To improve GEMM/GEMV memory access efficiency, LLM inference typically employs
    a quantized inference strategy. The current mainstream approach is weight-only
    quantization, where the kernel performs actual computation based on dequantized
    FP16 values. However, this approach offers limited performance improvement in
    highly parallel scenarios. To further enhance quantized inference performance,
    the industry is pursuing full quantization of both weights and activation values
    to reduce activation memory access and leverage higher computational power using
    quantized kernels, such as those from NVIDIA. However, current industry practices
    in weight and activation full quantization (WA full quantization) face several
    limitations. NVIDIA provides only a limited set of hardware-accelerated instructions(Lin
    et al. [2024a](#bib.bib23); Ashkboos et al. [2024](#bib.bib2); Zhao et al. [2024](#bib.bib42)),
    which constrains the design space for quantization algorithms. Other quantization
    combinations (e.g., W4A8 or W2A4) require type conversion to W8A8 or W4A4 during
    computation, leading to inefficiency(Lin et al. [2024b](#bib.bib24)). Furthermore,
    due to GEMV, additional padding calculations are required in scenarios with a
    batch size less than 8, resulting in inefficient matrix multiplication for W4A4
    and W8A8\. Finally, WA fully quantized models encounter significant challenges
    in low-bit quantization (e.g., W2A8, W2A6).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高 GEMM/GEMV 内存访问效率，LLM 推理通常采用量化推理策略。目前主流的方法是仅量化权重，其中内核基于去量化的 FP16 值执行实际计算。然而，这种方法在高度并行的场景中性能提升有限。为了进一步提升量化推理性能，业界正追求权重和激活值的完全量化，以减少激活内存访问并利用量化内核（如
    NVIDIA 的内核）提高计算能力。然而，当前在权重和激活完全量化（WA 完全量化）方面的行业实践面临一些限制。NVIDIA 仅提供有限的硬件加速指令（Lin
    et al. [2024a](#bib.bib23); Ashkboos et al. [2024](#bib.bib2); Zhao et al. [2024](#bib.bib42)），这限制了量化算法的设计空间。其他量化组合（例如
    W4A8 或 W2A4）在计算过程中需要转换为 W8A8 或 W4A4，导致效率低下（Lin et al. [2024b](#bib.bib24)）。此外，由于
    GEMV，在批量大小小于 8 的场景中需要额外的填充计算，导致 W4A4 和 W8A8 的矩阵乘法效率低下。最后，WA 完全量化模型在低位量化（例如 W2A8、W2A6）方面遇到重大挑战。
- en: '![Refer to caption](img/8d4e9f72b10b3052a78695e8c5382c02.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d4e9f72b10b3052a78695e8c5382c02.png)'
- en: 'Figure 1: Perplexities analysis (lower is better) on the Wikitext2 dataset
    of LLaMA-7B with different quantized modules.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLaMA-7B 在 Wikitext2 数据集上的困惑度分析（越低越好），使用不同的量化模块。
- en: 'In this paper, we introduce a novel quantization framework for PTQ, called
    ABQ-LLM. By examining the quantization sensitivity of components within the transformer
    block (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")) and the attention map before
    and after quantization (Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ ABQ-LLM:
    Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")), we
    find the down_proj linear layer and the attention map particularly sensitive to
    quantization. To address this, we propose a double cosine similarity distribution
    correction and an attention map distribution bootstrap for the output of down_proj.
    This method calibrates the quantization constants and restores the model’s performance
    at low bit-widths such as W6A6, W4A4 and W2A8\. Additionally, we analyze performance
    degradation in low-bit quantization and address the asymmetric loss issue in low-bit
    representations like INT2 using the bit balance strategy. Finally, we implement
    customized software engine to support fully quantized inference of various precision
    combinations based on BTC equivalents, fully exploiting the advantages of quantized
    models under mixed precision. Our contributions are summarized as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种新颖的 PTQ 量化框架，称为 ABQ-LLM。通过检查变压器块内组件的量化敏感性（图 [1](#S1.F1 "图 1 ‣ 1
    介绍 ‣ ABQ-LLM：大规模语言模型的任意位量化推理加速")）以及量化前后的注意力图（图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ ABQ-LLM：大规模语言模型的任意位量化推理加速")），我们发现
    down_proj 线性层和注意力图对量化特别敏感。为解决此问题，我们提出了双余弦相似度分布校正和下_proj 输出的注意力图分布引导方法。该方法校准了量化常数，并恢复了在低位宽下如
    W6A6、W4A4 和 W2A8 的模型性能。此外，我们分析了低位量化中的性能退化，并使用位平衡策略解决了 INT2 等低位表示中的非对称损失问题。最后，我们实现了定制的软件引擎，以支持基于
    BTC 等效物的各种精度组合的完全量化推理，充分发挥了混合精度下量化模型的优势。我们的贡献总结如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel block-wise distribution correction and compensation scheme
    in the PTQ domain to mitigate the distribution discrepancy caused by full quantization
    of weights and activations, thereby improving model performance at low bit-widths.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在 PTQ 领域提出了一种新颖的块级分布校正和补偿方案，以缓解由权重和激活的完全量化引起的分布差异，从而在低位宽下提高模型性能。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We address the problem of asymmetric loss at low bit-widths, such as INT2, and
    significantly improve INT2 quantization performance using the bit balance strategy,
    enhancing model performance under the INT2 quantization configuration.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们解决了低位宽下（如 INT2）的非对称损失问题，并使用位平衡策略显著提高了 INT2 量化性能，从而增强了模型在 INT2 量化配置下的性能。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a software engine which achieves quantization freedom for the first
    time in the LLM field. It eliminates the limitations of INT4/INT8 computational
    units, and effectively avoids the GEMV problem. Under the LLaMA-7B W2A8 configuration,
    it has 1.6$\times$ ultimate acceleration compared to SmoothQuant, achieving SOTA
    performance.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种软件引擎，它首次在 LLM 领域实现了量化自由。它消除了 INT4/INT8 计算单元的限制，有效避免了 GEMV 问题。在 LLaMA-7B
    W2A8 配置下，与 SmoothQuant 相比，它具有 1.6$\times$ 的*最终加速*，实现了 SOTA 性能。
- en: '![Refer to caption](img/6ea348358c11f93fb92ef01f849f69db.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6ea348358c11f93fb92ef01f849f69db.png)'
- en: 'Figure 2: Attention maps for the first block (above) and the final block (below)
    are shown.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：显示了第一个块（上方）和最终块（下方）的注意力图。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM quantization can be broadly divided into weight-only quantization and weight-activation
    quantization.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 量化可以大致分为仅权重量化和权重-激活量化。
- en: Weight-only quantization. To alleviate computational burdens, some studies focus
    on weight-only quantization. LLM.int8() (Dettmers et al. [2022](#bib.bib9)) achieves
    accurate INT8 quantization by retaining significant channels. GPTQ (Frantar et al.
    [2022](#bib.bib13)) uses Hessian-based error compensation to reduce quantization
    errors in LLMs, enabling 3-bit quantization. AWQ (Lin et al. [2024a](#bib.bib23))
    and OWQ (Lee et al. [2024](#bib.bib21)) significantly enhance quantized model
    performance by considering the impact of activation outliers on weight quantization.
    Methods like QuIP(Chee et al. [2024](#bib.bib6)), QuIP# (Tseng et al. [2024](#bib.bib36)),
    and AQLM(Egiazarian et al. [2024](#bib.bib12)) facilitate 2-bit quantization through
    learnable codebooks or additional fine-tuning. Approaches such as (Dettmers et al.
    [2023](#bib.bib11); Shang et al. [2023](#bib.bib32); Huang et al. [2024](#bib.bib19))
    improve PTQ performance through unstructured mixed-precision fine-grained weight
    grouping. Additionally, research such as (Dettmers et al. [2024](#bib.bib10);
    Xu et al. [2023b](#bib.bib39); Arshia et al. [2022](#bib.bib1); Bondarenko, Del Chiaro,
    and Nagel [2024](#bib.bib4)) employs efficient parameter fine-tuning (PEFT) techniques
    to compress weights through fine-tuning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仅权重量化。为了减轻计算负担，一些研究集中于仅进行权重量化。LLM.int8()（Dettmers等人 [2022](#bib.bib9)）通过保留重要通道实现了准确的INT8量化。GPTQ（Frantar等人
    [2022](#bib.bib13)）使用基于Hessian的误差补偿来减少LLMs中的量化误差，从而实现了3位量化。AWQ（Lin等人 [2024a](#bib.bib23)）和OWQ（Lee等人
    [2024](#bib.bib21)）通过考虑激活异常值对权重量化的影响，显著提升了量化模型的性能。方法如QuIP（Chee等人 [2024](#bib.bib6)）、QuIP#（Tseng等人
    [2024](#bib.bib36)）和AQLM（Egiazarian等人 [2024](#bib.bib12)）通过可学习的代码本或额外的微调来促进2位量化。方法如（Dettmers等人
    [2023](#bib.bib11); Shang等人 [2023](#bib.bib32); Huang等人 [2024](#bib.bib19)）通过无结构的混合精度细粒度权重分组来提高PTQ性能。此外，研究如（Dettmers等人
    [2024](#bib.bib10); Xu等人 [2023b](#bib.bib39); Arshia等人 [2022](#bib.bib1); Bondarenko,
    Del Chiaro和Nagel [2024](#bib.bib4)）采用高效的参数微调（PEFT）技术，通过微调来压缩权重。
- en: Weight-activation quantization. Weight-activation quantization differs from
    weight-only quantization by quantizing both weights and activation (including
    KV caches) to accelerate LLM inference. The main challenge in quantizing activation
    is handling outliers, which can cause significant quantization errors. To address
    this issue, ZeroQuant(Yao et al. [2022](#bib.bib40)) proposes a fine-grained,
    hardware-friendly quantization scheme for weights and activation. SmoothQuant(Xiao
    et al. [2023](#bib.bib37)) shifts the quantization difficulty from activation
    to weights through mathematically equivalent transformations, achieving W8A8 quantization.
    (Shao et al. [2023](#bib.bib33); Ma et al. [2024b](#bib.bib28); Hu et al. [2024a](#bib.bib16))
    enhances performance by training quantization parameters. Limited by GPU platform
    instruction limitations, these jobs can only use W8A8 to perform actual inference,
    even if they achieve lower quantization bit-widths (e.g., W6A6).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 权重-激活量化。权重-激活量化与仅权重量化不同，它对权重和激活（包括KV缓存）进行量化，以加速LLM推理。量化激活的主要挑战在于处理异常值，这些异常值可能导致显著的量化误差。为了解决这个问题，ZeroQuant（Yao等人
    [2022](#bib.bib40)）提出了一种针对权重和激活的细粒度、硬件友好的量化方案。SmoothQuant（Xiao等人 [2023](#bib.bib37)）通过数学等效变换将量化难度从激活转移到权重，实现了W8A8量化。（Shao等人
    [2023](#bib.bib33); Ma等人 [2024b](#bib.bib28); Hu等人 [2024a](#bib.bib16)）通过训练量化参数提升了性能。由于GPU平台指令的限制，这些工作只能使用W8A8进行实际推理，即使它们实现了更低的量化位宽（例如W6A6）。
- en: 3 Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In this section, we provide a detailed introduction to our ABQ-LLM. We first
    describe the distribution correction and bit balance strategy and then introduce
    our arbitrary-bit inference framework.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了我们的ABQ-LLM。我们首先描述了分布校正和位平衡策略，然后介绍了我们的任意位推理框架。
- en: '![Refer to caption](img/59e6c554ee842be436d81570e77b0250.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/59e6c554ee842be436d81570e77b0250.png)'
- en: 'Figure 3: An overview of our ABQ-LLM. ABQ-LLM use our DLC loss and AKL loss
    to update learnable parameters.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们ABQ-LLM的概述。ABQ-LLM使用我们的DLC损失和AKL损失来更新可学习参数。
- en: 3.1 Preliminary
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步
- en: '(Xiao et al. [2023](#bib.bib37)) achieves WA full quantization by scaling activation
    outliers, but this increases the range variability of weights, making weight quantization
    more sensitive. Conversely, (Lin et al. [2024a](#bib.bib23)) optimizes weight
    quantization by scaling weights, which significantly increases the diversity of
    activation, complicating activation quantization. These approaches highlight the
    drawbacks of manually setting scaling balance factors between activation and weight,
    making it challenging to achieve a perfect balance. To address this issue, we
    introduce a distribution correction-guided scaling method. Following (Shao et al.
    [2023](#bib.bib33)) approach, we set the balance vectors between weights and activation
    as learnable parameters and add a learnable clipping parameter for weight. By
    employing our distribution correction and bit balance strategy to optimize model
    performance, our objectives are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: (Xiao et al. [2023](#bib.bib37))通过缩放激活值异常点实现了WA全量化，但这增加了权重的范围变异性，使得权重量化更为敏感。相反，(Lin
    et al. [2024a](#bib.bib23))通过缩放权重来优化权重量化，这显著增加了激活的多样性，使得激活量化变得复杂。这些方法突显了手动设置激活与权重之间的缩放平衡因子的缺陷，使得实现完美的平衡变得具有挑战性。为解决这一问题，我们引入了一种分布修正引导的缩放方法。遵循(Shao
    et al. [2023](#bib.bib33))的方法，我们将权重和激活之间的平衡向量设置为可学习参数，并为权重添加一个可学习的裁剪参数。通过采用我们的分布修正和位平衡策略来优化模型性能，我们的目标如下：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $W$ to control the clipping range of the weight.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$W$用于控制权重的裁剪范围。
- en: 3.2 Improving Quantization by Distribution Correction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通过分布修正改善量化
- en: 'We observed significant variations in sensitivity across different layers of
    LLM models during quantization, with some layers having a critical impact on quantization
    performance. To validate this, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models"),
    we quantified various components of the LLaMA-7B model under weight-activation
    full quantization. While quantizing the gate_proj and up_proj layers in mlp and
    attention resulted in only minor performance degradation, quantizing the down_proj
    linear layer caused a substantial performance drop. This indicates that addressing
    down_proj quantization is crucial for performance recovery. Further analysis revealed
    that the primary cause of performance degradation due to down_proj quantization
    is the quantization of down_proj activation. At low bit-widths such as INT4, INT3,
    and INT2, the limited representation range causes a significant shift in the model
    distribution compared to full precision. As illustrated in Figure [3](#S3.F3 "Figure
    3 ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models"), during the block-wise quantization calibration process, we
    apply a double logarithm of cosine similarity loss on the output of down_proj
    to correct the distribution of the quantized model. The loss function called DLC
    loss $\mathcal{L}^{i}_{DLC}$:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到在量化过程中LLM模型的不同层之间的灵敏度存在显著差异，某些层对量化性能有着关键性的影响。为了验证这一点，如图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for
    Large Language Models")所示，我们对LLaMA-7B模型的各个组件进行了权重-激活全量化。尽管在mlp和attention中的gate_proj和up_proj层的量化只导致了轻微的性能下降，但对down_proj线性层的量化却导致了显著的性能下降。这表明，解决down_proj量化问题对性能恢复至关重要。进一步分析揭示了down_proj量化导致性能下降的主要原因是down_proj激活的量化。在INT4、INT3和INT2等低位宽下，有限的表示范围导致模型分布与全精度相比发生显著偏移。如图[3](#S3.F3
    "Figure 3 ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")所示，在块级量化校准过程中，我们对down_proj的输出应用了双对数余弦相似性损失来修正量化模型的分布。损失函数称为DLC损失$\mathcal{L}^{i}_{DLC}$：'
- en: '|  | $1$2 |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $d^{i}_{q}$-th transformer block.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$d^{i}_{q}$-th变换器块。
- en: 'Additionally, we conducted an analysis of the cosine similarity between activation
    at the input and output of decoder blocks in the LLaMA-7B model. The results revealed
    significant differences in similarity for the initial and final blocks, indicating
    their considerable impact on model inference performance. In response, we applied
    distribution compensation vector to the down_proj layers of these blocks to address
    and correct the distribution discrepancies using Eq. ([3](#S3.E3 "In 3.2 Improving
    Quantization by Distribution Correction ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还分析了 LLaMA-7B 模型中解码器块输入和输出激活的余弦相似度。结果显示初始块和最终块之间的相似度存在显著差异，表明它们对模型推理性能的影响较大。对此，我们应用了分布补偿向量来调整这些块的
    down_proj 层，以解决和纠正分布差异，使用了公式 ([3](#S3.E3 "在 3.2 通过分布校正改善量化 ‣ 3 方法 ‣ ABQ-LLM：大语言模型的任意位量化推理加速"))。
- en: '|  | $\displaystyle W_{q}=clamp(\lceil\frac{W+\gamma ab^{\top}}{\bigtriangleup}\rfloor+z,0,2^{n}-1)$
    |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle W_{q}=clamp(\lceil\frac{W+\gamma ab^{\top}}{\bigtriangleup}\rfloor+z,0,2^{n}-1)$
    |  | (3) |'
- en: where $\lceil\cdot\rfloor$ indicates no compensation.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lceil\cdot\rfloor$ 表示没有补偿。
- en: '![Refer to caption](img/65f8b9fddba04ef6b5aee1355fe9be47.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/65f8b9fddba04ef6b5aee1355fe9be47.png)'
- en: (a)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/f124a819c56baf9071f6362481a1c607.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f124a819c56baf9071f6362481a1c607.png)'
- en: (b)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 4: (a) System Overview of Custom Software Engine. p and q represent
    the quantization bit width of input X and weight W. Data flows are carefully designed
    to support efficient computation: global memory(GL) $\rightarrow$ global memory(GL).
    (b) ABQ-LLM’s calculation diagram for a Transformer block. ReQuant and DeQuant
    represent online quantization and dequantization operations respectively, and
    BitPacking represents the online layout transformation for activation.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: (a) 自定义软件引擎的系统概览。p 和 q 表示输入 X 和权重 W 的量化位宽。数据流精心设计以支持高效计算：全局内存 (GL) $\rightarrow$
    全局内存 (GL)。 (b) ABQ-LLM 的变换器块计算图。ReQuant 和 DeQuant 分别表示在线量化和反量化操作，BitPacking 表示激活的在线布局变换。'
- en: 'To enhance the performance of the quantized model, we analyzed the changes
    in Attention Map distribution before and after quantization, as shown in Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models"). In the full-precision model, attention is heavily
    focused on the first token, highlighting its key role in guiding text generation,
    consistent with LLM-QAT(Liu et al. [2023](#bib.bib25)) findings. However, quantization
    disrupts this attention pattern, diminishing focus on the first token. To address
    this and restore the model’s attention during quantization, we introduced attention-aware
    KL divergence to reconstruct the attention map.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升量化模型的性能，我们分析了量化前后注意力图分布的变化，如图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ ABQ-LLM：大语言模型的任意位量化推理加速")
    所示。在全精度模型中，注意力主要集中在第一个令牌上，突显了它在引导文本生成中的关键作用，这与 LLM-QAT（刘等 [2023](#bib.bib25)）的发现一致。然而，量化扰乱了这种注意力模式，减少了对第一个令牌的关注。为了解决这一问题并恢复量化过程中模型的注意力，我们引入了注意力感知的
    KL 散度来重建注意力图。
- en: '|  | $1$2 |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where $attn^{i}_{q}$ refers to the full-precision attention map output of the
    same block.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $attn^{i}_{q}$ 指的是相同块的全精度注意力图输出。
- en: 'In the end, we combined DLC loss and AKL loss, and our final optimization goal
    is:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们结合了 DLC 损失和 AKL 损失，我们的最终优化目标是：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $s^{i}_{*}$-th transformer block after calibration. When the distributions
    of the quantized output and the full-precision output match, we have a loss close
    to 0, which effectively guides the quantization process.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s^{i}_{*}$ 表示校准后的第 $i$ 个变换器块。当量化输出与全精度输出的分布匹配时，我们会得到接近于 0 的损失，这有效地指导了量化过程。
- en: 3.3 Bit Balance Strategy
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 位平衡策略
- en: 'Typically, pre-trained LLM model weight exhibit near-normal distribution, characterized
    by symmetry. Using Q-Q plots (Quantile-Quantile Plots), we confirmed the strong
    symmetry in the weight distribution of pre-trained models (see Appendix [A](#A1
    "Appendix A Bit Balance Analyze ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")). However, in standard INT2 quantization, the numerical
    representation is limited to four values, with symmetric quantization ranges of
    {-2, -1, 0, 1} or {-1, 0, 1, 2}, disrupting the original symmetric weight distribution
    (see Appendix [A](#A1 "Appendix A Bit Balance Analyze ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models")). This asymmetry
    leads to significant performance degradation, as shown in Table [1](#S3.T1 "Table
    1 ‣ 3.4 Custom Software Engine ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models"), where performance drops by 0.46 from
    W4A16 to W3A16 and by 5.19 from W3A16 to W2A16, indicating a sharp decline. To
    address this asymmetry impact on LLM quantization, we adopted the bit balance
    strategy like (Li et al. [2016](#bib.bib22); Ma et al. [2024a](#bib.bib27)), extending
    the INT2 symmetric quantization space to {-2, -1, 0, 1, 2}. This modification
    restored model performance to 7.50, which is within a reasonable range compared
    to W3A16.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，预训练 LLM 模型权重表现出接近正态分布的特征，具有对称性。通过 Q-Q 图（Quantile-Quantile Plots），我们确认了预训练模型权重分布的强对称性（参见附录
    [A](#A1 "Appendix A Bit Balance Analyze ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models")）。然而，在标准 INT2 量化中，数值表示限制为四个值，对称量化范围为 {-2,
    -1, 0, 1} 或 {-1, 0, 1, 2}，打乱了原有的对称权重分布（参见附录 [A](#A1 "Appendix A Bit Balance Analyze
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")）。这种不对称性导致了显著的性能下降，如表
    [1](#S3.T1 "Table 1 ‣ 3.4 Custom Software Engine ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") 所示，从 W4A16 到 W3A16
    性能下降了 0.46，从 W3A16 到 W2A16 性能下降了 5.19，显示出急剧的下降。为了解决这种不对称性对 LLM 量化的影响，我们采用了类似于
    (Li et al. [2016](#bib.bib22); Ma et al. [2024a](#bib.bib27)) 的比特平衡策略，将 INT2 对称量化空间扩展到
    {-2, -1, 0, 1, 2}。这一修改将模型性能恢复到 7.50，相比于 W3A16 处于合理范围内。'
- en: 3.4 Custom Software Engine
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 自定义软件引擎
- en: 'Reconstructing Arbitrary Bit Computation. To support W1A1 quantization, NVIDIA
    introduced INT1 TensorCore in Turing and later architectures to provide hardware
    support. However, W1A1 quantization has not been widely applied due to significant
    performance degradation. Through mathematical analysis of quantized matrix multiplication,
    we find that any combination of quantization can be decomposed into a superposition
    of 1-bit matrix multiplications. Assuming the weight $W$. The key is to observe
    that the scalar values at any position in W and X can be decomposed into a series
    of 1-bit scalar numbers. Scalar operations with any combination of precision can
    be decomposed into 1-bit operations and shift operations. For example, a 2-bit
    x can be expressed as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重构任意位计算。为了支持 W1A1 量化，NVIDIA 在 Turing 及后续架构中引入了 INT1 TensorCore，以提供硬件支持。然而，由于性能显著下降，W1A1
    量化尚未广泛应用。通过对量化矩阵乘法的数学分析，我们发现任何量化组合都可以分解为 1-bit 矩阵乘法的叠加。假设权重为 $W$。关键在于观察 W 和 X
    中任何位置的标量值都可以分解为一系列 1-bit 标量数。任意精度组合的标量运算可以分解为 1-bit 操作和移位操作。例如，2-bit x 可以表示为：
- en: '|  | $\displaystyle x=x^{1}x^{0},where\quad x^{i}\in INT1$ |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle x=x^{1}x^{0},where\quad x^{i}\in INT1$ |  | (6) |'
- en: 'where $x^{1}=(x\gg 1)\&amp;1$ can be represented as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x^{1}=(x\gg 1)\&amp;1$ 可以表示为：
- en: '|  | $\displaystyle wx=w*(x^{1}x^{0})=OP(w,x^{1})*2+OP(w,x^{0})$ |  | (7) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle wx=w*(x^{1}x^{0})=OP(w,x^{1})*2+OP(w,x^{0})$ |  | (7) |'
- en: 'The above procedure can be generalized to any combination of matrix multiplications
    with bit-widths p and q. The detailed formulas are shown in the Appendix [B](#A2
    "Appendix B Arbitrary Bit Matrix Calculation ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models"). Using these transformations,
    we decompose the operation of arbitrary quantized combinations into a superposition
    of 1-bit matrix multiplications, enabling the underlying layer to invoke high-computing
    instruction implementations.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '上述过程可以推广到任何位宽为 p 和 q 的矩阵乘法组合。详细公式见附录 [B](#A2 "Appendix B Arbitrary Bit Matrix
    Calculation ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models")。通过这些变换，我们将任意量化组合的操作分解为 1-bit 矩阵乘法的叠加，从而使底层能够调用高计算指令实现。'
- en: '| Model | Bits | WikiText2 | C4 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | WikiText2 | C4 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLaMA-7B | W4A16 | 5.83 | 7.29 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | W4A16 | 5.83 | 7.29 |'
- en: '| W3A16 | 6.29 | 8.01 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | 6.29 | 8.01 |'
- en: '| W2A16 | 11.48 | 15.74 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| W2A16 | 11.48 | 15.74 |'
- en: '| W2*A16 | 7.50 | 9.86 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | 7.50 | 9.86 |'
- en: 'Table 1: Performance comparison of LLaMA-7B under W4, W3, and W2 quantization
    configurations. * denotes the use of the bit balance strategy.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLaMA-7B在W4、W3和W2量化配置下的性能比较。*表示使用了位平衡策略。
- en: 'Engine Implementation. NVIDIA GPU has many processing elements called Streaming
    Multiprocessors (SMs) and uses a large number of threads to perform computing
    tasks in parallel. Threads are structured into thread blocks, which become the
    smallest scheduling execution unit on SMs. Therefore, the computation target is
    decomposed and mapped to each thread block, called Thread Block Tile, to achieve
    parallel computing. As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Improving Quantization
    by Distribution Correction ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models")(a), for a GEMM task of shape M×N×K, each
    thread block is responsible for computing a BM×BN output block, which is decomposed
    into $\frac{K}{BK}$ is the quantization bit width. Taking input X as an example,
    this means that its bit perspective layout changes from [M, K, p] to [p, M, K].
    All threads within a thread block share the same shared memory space. Within each
    thread block, threads are further organized into a set of warps, with each warp
    consisting of 32 consecutive threads. 2
    Next, warps collaborates to load the A matrix (p*BM × BK) and B matrix (BK × q*BN)
    data required for thread block tile calculation from GL and caches them in SMEM.
    Thanks to BitPacking, the process of reading p BM*BK single-bit row-major tiles
    and writing p*BM*BK bits SMEM is efficient and continuous. 3
    Subsequently, thread block contains multiple warps, so thread block tile can be
    further decomposed into Warp Tile to achieve warp-level parallelism, and the computing
    tasks of each warp are WM× WN. In the calculation preparation stage, the A matrix
    (WM×WK, row-major) and B matrix (WK×WN, col-major) are independently loaded from
    SMEM to FR. Then, the calculation is decomposed into WM_TILES*WARP_N_TILES TensorCore
    MMA(matrix-multiply-accumulate). Since A and B are binarized matrices, we actually
    use Binary TensorCore MMA (BMMA), which has a computing power 8 times and 4 times
    higher than INT8 and INT4 TensorCore respectively. 4
    All warps collaboratively complete the Thread Block Tile calculation, and the
    results are stored in the c fragment of each warp. Therefore, each warp needs
    to independently write the calculation results back to SMEM. 5
    As shown in Eq. ([10](#A2.E10 "In Appendix B Arbitrary Bit Matrix Calculation
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")),
    output tile(p*BM × q*BN) is globally reduced to obtain a final result(BM × BN),
    where each BM × BN sub-Tile needs to be multiplied by a certain scaling factor.
    We call this process Bit Reduction. 6
    As the final step, warps collaboratively load the final result from SMEM and write
    back to the target location in GL.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 引擎实现。NVIDIA GPU 拥有许多称为流式多处理器（SM）的处理单元，并使用大量线程并行执行计算任务。线程被组织成线程块，这些线程块成为 SM 上最小的调度执行单元。因此，计算目标被分解并映射到每个线程块，称为线程块平铺，以实现并行计算。如图
    [4](#S3.F4 "图 4 ‣ 3.2 通过分布校正改进量化 ‣ 3 方法 ‣ ABQ-LLM：大型语言模型的任意位量化推理加速")(a) 所示，对于形状为
    M×N×K 的 GEMM 任务，每个线程块负责计算一个 BM×BN 的输出块，该块被分解为 $\frac{K}{BK}$，其中 BK 是量化位宽。以输入 X
    为例，这意味着其位视角布局从 [M, K, p] 变为 [p, M, K]。每个线程块中的所有线程共享相同的共享内存空间。在每个线程块内，线程进一步组织成一组
    warp，每个 warp 由 32 个连续的线程组成。2
    接下来，warp 协作从 GL 中加载线程块平铺计算所需的 A 矩阵（p*BM × BK）和 B 矩阵（BK × q*BN）数据，并将它们缓存到 SMEM
    中。得益于 BitPacking，读取 p BM*BK 单位位行主序平铺和将 p*BM*BK 位写入 SMEM 的过程既高效又连续。3
    随后，线程块包含多个 warp，因此线程块平铺可以进一步分解为 Warp Tile，以实现 warp 级别的并行性，每个 warp 的计算任务为 WM× WN。在计算准备阶段，A
    矩阵（WM×WK，行主序）和 B 矩阵（WK×WN，列主序）被独立地从 SMEM 加载到 FR。然后，计算被分解为 WM_TILES*WARP_N_TILES
    TensorCore MMA（矩阵乘加）。由于 A 和 B 是二值化矩阵，我们实际上使用的是 Binary TensorCore MMA（BMMA），其计算能力分别是
    INT8 和 INT4 TensorCore 的 8 倍和 4 倍。4
    所有 warp 协作完成线程块平铺计算，并将结果存储在每个 warp 的 c 片段中。因此，每个 warp 需要独立地将计算结果写回到 SMEM 中。5
    如公式 ([10](#A2.E10 "附录 B 任意位矩阵计算 ‣ ABQ-LLM：大型语言模型的任意位量化推理加速")) 所示，输出平铺 (p*BM ×
    q*BN) 被全局归约以获得最终结果 (BM × BN)，其中每个 BM × BN 子平铺需要乘以某个缩放因子。我们称这一过程为 Bit Reduction。6
    最后一步是，warp 协作从 SMEM 加载最终结果，并将其写回到 GL 中的目标位置。
- en: 'We implement the above calculation process as a GPU Kernel, called ABQKernel.
    As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Improving Quantization by Distribution
    Correction ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")(b), ABQKernel is used to replace all gemm operations
    in the decoder layer, and assists with necessary BitPacking, quantization, and
    dequantization operations to achieve arbitrary quantization inference of the LLaMA
    model. We carefully manages the overhead of quantization operators by fusing them
    into existing operators and weight BitPacking is implemented offline for increased
    efficiency.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将上述计算过程实现为一个 GPU 内核，称为 ABQKernel。如图 [4](#S3.F4 "图 4 ‣ 3.2 通过分布修正改进量化 ‣ 3
    方法 ‣ ABQ-LLM: 大型语言模型的任意比特量化推理加速")(b) 所示，ABQKernel 用于替换解码器层中的所有 gemm 操作，并辅助进行必要的
    BitPacking、量化和反量化操作，以实现对 LLaMA 模型的任意量化推理。我们通过将量化操作融合到现有操作中来精心管理量化操作的开销，并且权重 BitPacking
    在离线中实现以提高效率。'
- en: 'GPU Kernel Optimization. When M=1, the GEMM problem of shape MxNxK transforms
    into a GEMV problem, shifting from computation-intensive to memory-intensive,
    which becomes a performance bottleneck for model inference. When using ordinary
    TensorCore for accelerated computation, the dimensions of M are usually chunked
    in groups of 8, requiring padding if M$<$ 8 = 0, padding can be completely avoided.
    We call the above optimization strategy GEMV Elimination. In addition, Computational
    and Pipeline Optimization, Auto Kernel Search, and Bank Conflicts Elimination
    are also applied. For details, see Appendix [D](#A4 "Appendix D GPU Kernel Optimization
    Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
    Models").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPU 内核优化。当 M=1 时，形状为 MxNxK 的 GEMM 问题转变为 GEMV 问题，从计算密集型转变为内存密集型，这成为模型推理的性能瓶颈。在使用普通
    TensorCore 进行加速计算时，M 的维度通常分成 8 的一组，如果 M$<$ 8 = 0，则需要填充，填充可以完全避免。我们称上述优化策略为 GEMV
    消除。此外，还应用了计算和管道优化、自动内核搜索和银行冲突消除。详细信息请参见附录 [D](#A4 "附录 D GPU 内核优化细节 ‣ ABQ-LLM:
    大型语言模型的任意比特量化推理加速")。'
- en: 4 Experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| Bits | Method | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 比特 | 方法 | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
- en: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 |'
- en: '| W6A6 | SmoothQuant | 6.03 | 7.47 | 5.42 | 6.97 | 6.20 | 7.76 | 5.18 | 7.67
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 6.03 | 7.47 | 5.42 | 6.97 | 6.20 | 7.76 | 5.18 | 7.67
    |'
- en: '| OmniQuant | 5.96 | 7.43 | 5.28 | 6.84 | 5.87 | 7.48 | 5.14 | 6.74 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.96 | 7.43 | 5.28 | 6.84 | 5.87 | 7.48 | 5.14 | 6.74 |'
- en: '| I-LLM | 5.84 | 7.32 | 5.23 | 6.79 | 5.68 | 7.27 | 5.10 | 6.74 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 5.84 | 7.32 | 5.23 | 6.79 | 5.68 | 7.27 | 5.10 | 6.74 |'
- en: '| ABQ-LLM | 5.81 | 7.27 | 5.21 | 6.77 | 5.63 | 7.21 | 5.00 | 6.64 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 5.81 | 7.27 | 5.21 | 6.77 | 5.63 | 7.21 | 5.00 | 6.64 |'
- en: '| W4A4 | SmoothQuant | 22.25 | 32.22 | 40.05 | 47.18 | 83.12 | 77.27 | 35.88
    | 43.19 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | SmoothQuant | 22.25 | 32.22 | 40.05 | 47.18 | 83.12 | 77.27 | 35.88
    | 43.19 |'
- en: '| OmniQuant | 11.26 | 14.51 | 10.87 | 13.78 | 14.26 | 18.02 | 12.30 | 14.55
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 11.26 | 14.51 | 10.87 | 13.78 | 14.26 | 18.02 | 12.30 | 14.55
    |'
- en: '| AffineQuant | 10.28 | 13.64 | 10.32 | 13.44 | 12.69 | 15.76 | 11.45 | 13.97
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 10.28 | 13.64 | 10.32 | 13.44 | 12.69 | 15.76 | 11.45 | 13.97
    |'
- en: '| I-LLM | 9.10 | 12.33 | 7.99 | 10.96 | 10.55 | 12.92 | 9.76 | 12.57 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 9.10 | 12.33 | 7.99 | 10.96 | 10.55 | 12.92 | 9.76 | 12.57 |'
- en: '| ABQ-LLM | 8.63 | 12.10 | 7.69 | 10.90 | 9.31 | 12.85 | 8.62 | 11.47 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 8.63 | 12.10 | 7.69 | 10.90 | 9.31 | 12.85 | 8.62 | 11.47 |'
- en: '| W2A8 | OmniQuant | 15.70 | 26.44 | 13.50 | 19.01 | 37.95 | 103.39 | 21.74
    | 31.72 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| W2A8 | OmniQuant | 15.70 | 26.44 | 13.50 | 19.01 | 37.95 | 103.39 | 21.74
    | 31.72 |'
- en: '| AffineQuant | 9.76 | 15.52 | 9.21 | 12.55 | 1483 | 4688 | 12.30 | 29.32 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 9.76 | 15.52 | 9.21 | 12.55 | 1483 | 4688 | 12.30 | 29.32 |'
- en: '| I-LLM | 14.08 | 18.89 | 11.80 | 16.19 | 123.93 | 200.54 | 25.74 | 40.59 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 14.08 | 18.89 | 11.80 | 16.19 | 123.93 | 200.54 | 25.74 | 40.59 |'
- en: '| ABQ-LLM | 11.35 | 15.41 | 9.20 | 12.48 | 13.47 | 17.82 | 13.24 | 18.07 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 11.35 | 15.41 | 9.20 | 12.48 | 13.47 | 17.82 | 13.24 | 18.07 |'
- en: '| W2*A8 | ABQ-LLM | 7.59 | 10.00 | 6.49 | 8.53 | 7.85 | 10.33 | 6.65 | 10.01
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 7.59 | 10.00 | 6.49 | 8.53 | 7.85 | 10.33 | 6.65 | 10.01
    |'
- en: 'Table 2: Weight-activation quantization perplexities (lower is better) comparison
    of quantized LLaMA and LLaMA-2 models. * denotes the use of the bit balance strategy.
    More results can be found in at Appendix [C](#A3 "Appendix C Full Results for
    Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models") Tables [6](#A3.T6 "Table 6 ‣ Appendix C Full Results
    for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models"), [7](#A3.T7 "Table 7 ‣ Appendix C Full Results for
    Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 量化 LLaMA 和 LLaMA-2 模型的权重-激活量化困惑度（值越低越好）比较。* 表示使用了位平衡策略。更多结果可以在附录 [C](#A3
    "Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") 表 [6](#A3.T6 "Table
    6 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models")、[7](#A3.T7 "Table
    7 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") 中找到。'
- en: 4.1 Experimental Setup
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Baseline. For weight-only quantization, we compare our approach with GPTQ(Frantar
    et al. [2022](#bib.bib13)), AWQ(Lin et al. [2024a](#bib.bib23)), OmniQuant(Shao
    et al. [2023](#bib.bib33)), and AffineQuant(Ma et al. [2024b](#bib.bib28)). For
    weight-activation quantization, we benchmark our method against SmoothQuant(Xiao
    et al. [2023](#bib.bib37)), OmniQuant(Shao et al. [2023](#bib.bib33)), and I-LLM(Hu
    et al. [2024b](#bib.bib17)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。对于仅权重量化，我们将我们的方法与 GPTQ (Frantar et al. [2022](#bib.bib13))、AWQ (Lin et al.
    [2024a](#bib.bib23))、OmniQuant (Shao et al. [2023](#bib.bib33)) 和 AffineQuant
    (Ma et al. [2024b](#bib.bib28)) 进行比较。对于权重-激活量化，我们将我们的方法与 SmoothQuant (Xiao et
    al. [2023](#bib.bib37))、OmniQuant (Shao et al. [2023](#bib.bib33)) 和 I-LLM (Hu
    et al. [2024b](#bib.bib17)) 进行基准测试。
- en: Models and Datasets. We primarily evaluate our method using LLaMA (7B-13B) (Touvron
    et al. [2023a](#bib.bib34)) and LLaMA-2 (7B-13B) (Touvron et al. [2023b](#bib.bib35))in
    this paper. Following previous work(Shao et al. [2023](#bib.bib33); Ma et al.
    [2024b](#bib.bib28)), we evaluate the quantized models by reporting the perplexity
    of language generation experiments on WikiText2(Merity et al. [2016](#bib.bib29))
    and C4(Raffel et al. [2020](#bib.bib30)). To assess performance on zero-shot tasks,
    we select several popular benchmarks including PIQA(Bisk et al. [2020](#bib.bib3)),
    ARC(Clark et al. [2018](#bib.bib8)), BoolQ(Clark et al. [2019](#bib.bib7)), HellaSwag(Zellers
    et al. [2019](#bib.bib41)), and Winogrande(Sakaguchi et al. [2021](#bib.bib31))
    using the lm-evaluation-harness(Gao et al. [2021](#bib.bib14)).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。本文主要使用 LLaMA (7B-13B) (Touvron et al. [2023a](#bib.bib34)) 和 LLaMA-2
    (7B-13B) (Touvron et al. [2023b](#bib.bib35)) 来评估我们的方法。参考之前的工作 (Shao et al. [2023](#bib.bib33);
    Ma et al. [2024b](#bib.bib28))，我们通过报告 WikiText2 (Merity et al. [2016](#bib.bib29))
    和 C4 (Raffel et al. [2020](#bib.bib30)) 上语言生成实验的困惑度来评估量化模型。为了评估零样本任务的性能，我们选择了包括
    PIQA (Bisk et al. [2020](#bib.bib3))、ARC (Clark et al. [2018](#bib.bib8))、BoolQ
    (Clark et al. [2019](#bib.bib7))、HellaSwag (Zellers et al. [2019](#bib.bib41))
    和 Winogrande (Sakaguchi et al. [2021](#bib.bib31)) 等几个流行基准，并使用 lm-evaluation-harness
    (Gao et al. [2021](#bib.bib14))。
- en: Calibration We initialize the balance vectors for weights and activations following
    (Xiao et al. [2023](#bib.bib37)), with the learnable clipping parameter for weights
    set to 1\. For distribution correction compensation vectors, we set $a$ starts
    at 0\. Using the AdamW optimizer (Loshchilov and Hutter [2017](#bib.bib26)) with
    no weight decay, we set learning rates of 5e-3 for balance vectors and 1e-2 for
    the clipping parameter and vector compensation vector. Calibration data includes
    128 randomly selected 2048-token segments from WikiText2\. The calibration process,
    conducted on an NVIDIA A800-40G GPU, utilized a batch size of 1 and spanned 20
    epochs. For activation and KV Cache we perform per-token quantization, and for
    weight we perform per-channel quantization. By default, activation and KV cache
    use the same quantization bit.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 校准 我们根据 (Xiao et al. [2023](#bib.bib37)) 初始化权重和激活的平衡向量，将权重的可学习剪裁参数设置为 1。对于分布修正补偿向量，我们将
    $a$ 设置为 0。使用 AdamW 优化器 (Loshchilov and Hutter [2017](#bib.bib26))，无权重衰减，将平衡向量的学习率设置为
    5e-3，剪裁参数和向量补偿向量的学习率设置为 1e-2。校准数据包括从 WikiText2 随机选择的 128 个 2048-token 段。校准过程在
    NVIDIA A800-40G GPU 上进行，批量大小为 1，跨度为 20 个 epoch。对于激活和 KV Cache，我们执行每 token 量化；对于权重，我们执行每通道量化。默认情况下，激活和
    KV Cache 使用相同的量化位数。
- en: '![Refer to caption](img/7f08951ac62a139939a43dd2a3c33df3.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f08951ac62a139939a43dd2a3c33df3.png)'
- en: 'Figure 5: The GEMV speedup comparison of our ABQKernel, CUTLASS (W4A4/W8A8),
    and cuBLAS (W8A8) in RTX 3070\. The left side is compared against W8A8 and the
    right side against W4A4. More result in RTX 4080 can be found in Appendix [D](#A4
    "Appendix D GPU Kernel Optimization Details ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models") tables [13](#A4.T13 "Table
    13 ‣ Appendix D GPU Kernel Optimization Details ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models"), [14](#A4.T14 "Table 14 ‣ Appendix
    D GPU Kernel Optimization Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：在 RTX 3070 中我们 ABQKernel、CUTLASS (W4A4/W8A8) 和 cuBLAS (W8A8) 的 GEMV 加速比较。左侧与
    W8A8 进行比较，右侧与 W4A4 进行比较。更多 RTX 4080 的结果见附录 [D](#A4 "附录 D GPU 内核优化细节 ‣ ABQ-LLM:
    大型语言模型的任意位量化推理加速") 表 [13](#A4.T13 "表 13 ‣ 附录 D GPU 内核优化细节 ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速")、[14](#A4.T14
    "表 14 ‣ 附录 D GPU 内核优化细节 ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速")。'
- en: 4.2 Experiments on Language Generation Tasks
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 语言生成任务的实验
- en: 'Language generation capability is central to large language models (LLMs).
    To validate our extraordinary performance in the challenging quantization task,
    we first compare perplexity, a crucial metric for language generation, with the
    baseline. As shown in Table [2](#S4.T2 "Table 2 ‣ 4 Experiments ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"), ABQ-LLM demonstrates
    outstanding performance across various quantization configurations, surpassing
    state-of-the-art methods such as AffineQuant and I-LLM. Notably, in the INT2 setting,
    the application of bit balance strategy yields significant improvements at minimal
    cost. The W2*A8 configurations substantially outperform the W2A8 configurations.
    Specifically, perplexity on WikiText2 and C4 datasets decreases by an average
    of 1.42 and 2.11 points, respectively, for W2*A8 compared to the W4A4\. These
    findings validate the effectiveness of our distribution correction and bit balance
    strategy. The results of weight-only quantization are presented in Table [6](#A3.T6
    "Table 6 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") at Appendix [C](#A3
    "Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"). Additional results
    for full WA quantization are provided in Tables [7](#A3.T7 "Table 7 ‣ Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models") at Appendix [C](#A3 "Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '语言生成能力是大型语言模型（LLMs）的核心。为了验证我们在具有挑战性的量化任务中的卓越性能，我们首先将困惑度这一语言生成的关键指标与基线进行比较。如表 [2](#S4.T2
    "表 2 ‣ 4 实验 ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速")所示，ABQ-LLM 在各种量化配置中表现优异，超越了诸如 AffineQuant
    和 I-LLM 等先进方法。特别是在 INT2 设置中，应用位平衡策略在最小成本下带来了显著的改进。W2*A8 配置在性能上远超 W2A8 配置。具体而言，W2*A8
    相较于 W4A4，在 WikiText2 和 C4 数据集上的困惑度分别平均下降了 1.42 和 2.11 分。这些发现验证了我们分布校正和位平衡策略的有效性。权重量化的结果见附录 [C](#A3
    "附录 C 量化算法的完整结果 ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速") 表 [6](#A3.T6 "表 6 ‣ 附录 C 量化算法的完整结果
    ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速")。完整 WA 量化的其他结果见附录 [C](#A3 "附录 C 量化算法的完整结果 ‣ ABQ-LLM:
    大型语言模型的任意位量化推理加速") 表 [7](#A3.T7 "表 7 ‣ 附录 C 量化算法的完整结果 ‣ ABQ-LLM: 大型语言模型的任意位量化推理加速")。'
- en: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | 方法 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande | 平均值
    |'
- en: '| LLaMA-13B | W4A4 | OmniQuant | 69.69 | 47.30 | 33.10 | 62.84 | 58.96 | 55.80
    | 54.37 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | W4A4 | OmniQuant | 69.69 | 47.30 | 33.10 | 62.84 | 58.96 | 55.80
    | 54.37 |'
- en: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
- en: '| I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 55.99 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 55.99 |'
- en: '| ABQ-LLM | 71.82 | 47.60 | 35.67 | 63.52 | 64.31 | 57.54 | 56.74 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 71.82 | 47.60 | 35.67 | 63.52 | 64.31 | 57.54 | 56.74 |'
- en: '| W2A8 | OmniQuant | 66.76 | 45.62 | 30.20 | 61.13 | 52.93 | 55.72 | 52.06
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| W2A8 | OmniQuant | 66.76 | 45.62 | 30.20 | 61.13 | 52.93 | 55.72 | 52.06
    |'
- en: '| AffineQuant | 71.00 | 46.70 | 32.33 | 62.23 | 58.62 | 63.53 | 55.73 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 71.00 | 46.70 | 32.33 | 62.23 | 58.62 | 63.53 | 55.73 |'
- en: '| I-LLM | 67.46 | 43.73 | 29.69 | 62.41 | 53.37 | 55.09 | 51.95 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 67.46 | 43.73 | 29.69 | 62.41 | 53.37 | 55.09 | 51.95 |'
- en: '| ABQ-LLM | 72.03 | 46.72 | 31.74 | 65.17 | 58.71 | 62.50 | 56.15 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 72.03 | 46.72 | 31.74 | 65.17 | 58.71 | 62.50 | 56.15 |'
- en: '| W2*A8 | ABQ-LLM | 74.91 | 54.92 | 38.65 | 68.53 | 68.21 | 66.54 | 61.96 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 74.91 | 54.92 | 38.65 | 68.53 | 68.21 | 66.54 | 61.96 |'
- en: '| LLaMA-2-13B | W4A4 | OmniQuant | 67.08 | 45.66 | 32.25 | 63.73 | 58.39 |
    54.61 | 53.62 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | W4A4 | OmniQuant | 67.08 | 45.66 | 32.25 | 63.73 | 58.39 |
    54.61 | 53.62 |'
- en: '| AffineQuant | 67.68 | 46.63 | 32.85 | 65.90 | 60.62 | 54.14 | 54.63 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 67.68 | 46.63 | 32.85 | 65.90 | 60.62 | 54.14 | 54.63 |'
- en: '| I-LLM | 68.00 | 45.74 | 30.97 | 64.55 | 60.62 | 54.22 | 54.01 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 68.00 | 45.74 | 30.97 | 64.55 | 60.62 | 54.22 | 54.01 |'
- en: '| ABQ-LLM | 69.04 | 47.01 | 33.53 | 64.74 | 62.70 | 54.38 | 55.23 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 69.04 | 47.01 | 33.53 | 64.74 | 62.70 | 54.38 | 55.23 |'
- en: '| W2A8 | OmniQuant | 62.67 | 38.80 | 28.41 | 62.11 | 49.04 | 51.69 | 48.78
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| W2A8 | OmniQuant | 62.67 | 38.80 | 28.41 | 62.11 | 49.04 | 51.69 | 48.78
    |'
- en: '| AffineQuant | 61.31 | 38.51 | 26.96 | 62.04 | 41.92 | 50.74 | 46.91 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 61.31 | 38.51 | 26.96 | 62.04 | 41.92 | 50.74 | 46.91 |'
- en: '| I-LLM | 61.86 | 38.67 | 26.45 | 62.17 | 43.30 | 51.85 | 47.38 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 61.86 | 38.67 | 26.45 | 62.17 | 43.30 | 51.85 | 47.38 |'
- en: '| ABQ-LLM | 64.30 | 40.19 | 29.78 | 63.18 | 49.58 | 52.17 | 49.87 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 64.30 | 40.19 | 29.78 | 63.18 | 49.58 | 52.17 | 49.87 |'
- en: '| W2*A8 | ABQ-LLM | 73.50 | 49.79 | 35.15 | 70.15 | 67.45 | 58.87 | 59.15 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 73.50 | 49.79 | 35.15 | 70.15 | 67.45 | 58.87 | 59.15 |'
- en: 'Table 3: Zero-shot accuracies (higher is better) comparison of quantized LLaMA
    and LLaMA-2 models. * denotes the use of the bit balance strategy. More results
    can be found in at Appendix [C](#A3 "Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models") Tables [9](#A3.T9 "Table 9 ‣ Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models"), [10](#A3.T10 "Table 10 ‣ Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models"), [8](#A3.T8 "Table 8 ‣ Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models"), [11](#A3.T11 "Table 11 ‣ Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models").'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3：量化LLaMA和LLaMA-2模型的零样本准确率（越高越好）比较。* 表示使用了比特平衡策略。更多结果请参见附录[C](#A3 "Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")表[9](#A3.T9 "Table 9 ‣ Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")、[10](#A3.T10 "Table 10 ‣ Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")、[8](#A3.T8 "Table 8 ‣ Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")、[11](#A3.T11 "Table 11 ‣ Appendix
    C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models")。'
- en: 4.3 Experiments on Zero-Shot Tasks
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 零样本任务实验
- en: 'To further validate our model, we compare zero-shot accuracy with the baseline
    method, as shown in Table [3](#S4.T3 "Table 3 ‣ 4.2 Experiments on Language Generation
    Tasks ‣ 4 Experiments ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models"). Our ABQ-LLM method outperforms the previous method
    in most cases. Notably, after applying the bit balance strategy, the performance
    of W2*A8 improves significantly by 7.50% on average. Combining the performance
    in both language generation and zero-shot tasks, we conclude that ABQ-LLM achieves
    state-of-the-art results in handling challenging quantization tasks. See the Appendix [C](#A3
    "Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") for more results
    and analysis of quantized configurations.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步验证我们的模型，我们将零样本准确率与基线方法进行比较，如表[3](#S4.T3 "Table 3 ‣ 4.2 Experiments on
    Language Generation Tasks ‣ 4 Experiments ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models")所示。我们的ABQ-LLM方法在大多数情况下优于之前的方法。值得注意的是，应用比特平衡策略后，W2*A8的性能平均提高了7.50%。结合语言生成和零样本任务的性能，我们得出结论，ABQ-LLM在处理挑战性的量化任务时达到了**最先进**的结果。更多结果和量化配置的分析见附录[C](#A3
    "Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models")。'
- en: 4.4 Inference Engine Evaluation
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 推理引擎评估
- en: 'Kernel Benchmark. We evaluated the GEMV speedup of our ABQKernel across three
    matrix dimensions in LLaMA-7B and compared it with the quantization kernel provided
    by cuBLAS and CUTLASS. It is important to note that CUTLASS only supports W4A4
    and W8A8, while cuBLAS supports only W8A8 for quantization operations. Our experiments
    were conducted on two different GPUs: the RTX 4080 and the RTX 3070\. Figure [5](#S4.F5
    "Figure 5 ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ ABQ-LLM: Arbitrary-Bit Quantized
    Inference Acceleration for Large Language Models") presents the results, showing
    that our ABQKernel achieve superior acceleration across all matrix sizes. Specifically,
    for special bit combinations such as W2A8 and W2A4, our ABQKernel significantly
    outperforms the baseline approaches, as cuBLAS and CUTLASS require conversion
    to W8A8 and W4A4 for computation. In the W2A8 configuration, our throughput is
    improved by 7.47$\times$ (4096, 4096).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 内核基准测试。我们评估了ABQKernel在LLaMA-7B中的GEMV加速效果，并与cuBLAS和CUTLASS提供的量化内核进行了比较。值得注意的是，CUTLASS仅支持W4A4和W8A8，而cuBLAS仅支持W8A8的量化操作。我们的实验在两款不同的GPU上进行：RTX
    4080和RTX 3070。图 [5](#S4.F5 "图5 ‣ 4.1 实验设置 ‣ 4 实验 ‣ ABQ-LLM：大语言模型的任意比特量化推理加速")
    展示了结果，表明我们的ABQKernel在所有矩阵大小中都实现了更好的加速。特别是对于W2A8和W2A4等特殊比特组合，我们的ABQKernel明显优于基准方法，因为cuBLAS和CUTLASS需要转换为W8A8和W4A4进行计算。在W2A8配置下，我们的吞吐量提高了7.47$\times$（4096,
    4096）。
- en: '![Refer to caption](img/57240d2426a7fa7c4fed4b06efb92b07.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/57240d2426a7fa7c4fed4b06efb92b07.png)'
- en: 'Figure 6: Inference latency (top) and memory usage (bottom) of the FastTransformer
    implementation on NVIDIA A800-40GB GPU with a fixed input length of 15\. More
    results can be found in at Appendix [D](#A4 "Appendix D GPU Kernel Optimization
    Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
    Models") Table [12](#A4.T12 "Table 12 ‣ Appendix D GPU Kernel Optimization Details
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在固定输入长度为15的NVIDIA A800-40GB GPU上，FastTransformer实现的推理延迟（顶部）和内存使用（底部）。更多结果可见于附录 [D](#A4
    "附录 D GPU内核优化详情 ‣ ABQ-LLM：大语言模型的任意比特量化推理加速") 表 [12](#A4.T12 "表12 ‣ 附录 D GPU内核优化详情
    ‣ ABQ-LLM：大语言模型的任意比特量化推理加速")。
- en: 'End-to-end throughput. As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Inference
    Engine Evaluation ‣ 4 Experiments ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models"), we integrate our ABQKernel into FastTransformer
    and compare it with the FP16 implementation of FastTransformer and the INT8 implementation
    of SmoothQuant. Compared to FP16, our scheme achieves 2.95$\times$ memory compression
    gain over SmoothQuant, significantly outperforming current mainstream inference
    methods. This substantial improvement reduces the cost of LLM services and facilitates
    their practical deployment.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端吞吐量。图 [6](#S4.F6 "图6 ‣ 4.4 推理引擎评估 ‣ 4 实验 ‣ ABQ-LLM：大语言模型的任意比特量化推理加速") 展示了我们将ABQKernel集成到FastTransformer中，并与FastTransformer的FP16实现和SmoothQuant的INT8实现进行了比较。与FP16相比，我们的方案在SmoothQuant基础上实现了2.95$\times$的内存压缩增益，显著优于目前主流的推理方法。这一显著改进降低了LLM服务的成本，并促进了它们的实际部署。
- en: Kernel Optimization Ablation. Table 4 presents the impact of various optimization
    techniques on the inference latency of the kernel when performing the GEMV operation
    with dimensions (1, 4096) $\times$. These results significantly outperform CUTLASS.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 内核优化消融研究。表4展示了在执行维度为 (1, 4096) $\times$ 的GEMV操作时，各种优化技术对内核推理延迟的影响。这些结果显著优于CUTLASS。
- en: '| Method | Latency(us)$\downarrow$ |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 延迟(us)$\downarrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CUTLASS | 49.96 | 0.67 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| CUTLASS | 49.96 | 0.67 |'
- en: '| --- | --- | --- |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Native_kernel | 20.05 | 1.67 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Native_kernel | 20.05 | 1.67 |'
- en: '| + Pipeline Optimization | 14.66 | 2.28 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| + Pipeline Optimization | 14.66 | 2.28 |'
- en: '| + Eliminiate GEMV | 10.92 | 3.07 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| + 消除GEMV | 10.92 | 3.07 |'
- en: '| + Auto Kernel Search | 6.68 | 5.01 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| + 自动内核搜索 | 6.68 | 5.01 |'
- en: 'Table 4: An ablation study of the impact of optimization techniques used in
    the inference engine on Kernel latency and throughput.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：优化技术在推理引擎中对内核延迟和吞吐量的影响的消融研究。
- en: 5 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present an arbitrary bit quantization inference framework called ABQ-LLM.
    Through an in-depth analysis of LLM quantization, we introduce distribution correction
    and bit balance strategy to enhance model performance. We then design a novel
    arbitrary bit inference engine to fully leverage the advantages of LLM quantization.
    Extensive experimental results demonstrate that ABQ-LLM achieves outstanding performance
    across various quantization configurations, including W6A6, W4A4, and W2A8\. Moreover,
    ABQKernel consistently outperformed both CUTLASS and cuBLAS in all configurations.
    Our end-to-end inference speed is 1.6 $\times$ memory compression gain.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种名为 ABQ-LLM 的任意位量化推理框架。通过对 LLM 量化的深入分析，我们引入了分布校正和位平衡策略以提升模型性能。然后，我们设计了一种新型的任意位推理引擎，以充分利用
    LLM 量化的优势。广泛的实验结果表明，ABQ-LLM 在包括 W6A6、W4A4 和 W2A8 在内的各种量化配置中都表现出色。此外，ABQKernel
    在所有配置中始终优于 CUTLASS 和 cuBLAS。我们的端到端推理速度提升了 1.6 倍内存压缩增益。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Arshia et al. (2022) Arshia, F. Z.; Keyvanrad, M. A.; Sadidpour, S. S.; and
    Mohammadi, S. M. R. 2022. PeQA: A Massive Persian Question-Answering and Chatbot
    Dataset. In *2022 12th International Conference on Computer and Knowledge Engineering
    (ICCKE)*, 392–397\. IEEE.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arshia 等（2022）Arshia, F. Z.; Keyvanrad, M. A.; Sadidpour, S. S.; 和 Mohammadi,
    S. M. R. 2022. PeQA: 大规模波斯问答和聊天机器人数据集. 见于 *2022 第十二届国际计算机与知识工程会议（ICCKE）*，392–397。IEEE。'
- en: 'Ashkboos et al. (2024) Ashkboos, S.; Mohtashami, A.; Croci, M. L.; Li, B.;
    Jaggi, M.; Alistarh, D.; Hoefler, T.; and Hensman, J. 2024. Quarot: Outlier-free
    4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等（2024）Ashkboos, S.; Mohtashami, A.; Croci, M. L.; Li, B.; Jaggi,
    M.; Alistarh, D.; Hoefler, T.; 和 Hensman, J. 2024. Quarot: 无异常值的 4 位旋转 LLM 推理。*arXiv
    预印本 arXiv:2404.00456*。'
- en: 'Bisk et al. (2020) Bisk, Y.; Zellers, R.; Gao, J.; Choi, Y.; et al. 2020. Piqa:
    Reasoning about physical commonsense in natural language. In *Proceedings of the
    AAAI conference on artificial intelligence*, volume 34, 7432–7439.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等（2020）Bisk, Y.; Zellers, R.; Gao, J.; Choi, Y.; 等. 2020. Piqa: 关于自然语言中物理常识的推理.
    见于 *AAAI 人工智能会议论文集*，第 34 卷，7432–7439。'
- en: Bondarenko, Del Chiaro, and Nagel (2024) Bondarenko, Y.; Del Chiaro, R.; and
    Nagel, M. 2024. Low-Rank Quantization-Aware Training for LLMs. *arXiv preprint
    arXiv:2406.06385*.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko, Del Chiaro 和 Nagel（2024）Bondarenko, Y.; Del Chiaro, R.; 和 Nagel,
    M. 2024. 低秩量化感知训练用于 LLM。*arXiv 预印本 arXiv:2406.06385*。
- en: 'Bubeck et al. (2023) Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.;
    Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; et al. 2023.
    Sparks of artificial general intelligence: Early experiments with gpt-4. *arXiv
    preprint arXiv:2303.12712*.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等（2023）Bubeck, S.; Chandrasekaran, V.; Eldan, R.; Gehrke, J.; Horvitz,
    E.; Kamar, E.; Lee, P.; Lee, Y. T.; Li, Y.; Lundberg, S.; 等. 2023. 人工通用智能的火花：GPT-4
    的早期实验。*arXiv 预印本 arXiv:2303.12712*。
- en: 'Chee et al. (2024) Chee, J.; Cai, Y.; Kuleshov, V.; and De Sa, C. M. 2024.
    Quip: 2-bit quantization of large language models with guarantees. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等（2024）Chee, J.; Cai, Y.; Kuleshov, V.; 和 De Sa, C. M. 2024. Quip: 带保证的大型语言模型的
    2 位量化。*神经信息处理系统进展*，36。'
- en: 'Clark et al. (2019) Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins,
    M.; and Toutanova, K. 2019. BoolQ: Exploring the surprising difficulty of natural
    yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等（2019）Clark, C.; Lee, K.; Chang, M.-W.; Kwiatkowski, T.; Collins, M.;
    和 Toutanova, K. 2019. BoolQ: 探索自然是/否问题的意外难度. *arXiv 预印本 arXiv:1905.10044*。'
- en: Clark et al. (2018) Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal,
    A.; Schoenick, C.; and Tafjord, O. 2018. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等（2018）Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.; Schoenick,
    C.; 和 Tafjord, O. 2018. 认为你已解决了问答问题？试试 arc，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*。
- en: 'Dettmers et al. (2022) Dettmers, T.; Lewis, M.; Belkada, Y.; and Zettlemoyer,
    L. 2022. LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale. *Advances
    in Neural Information Processing Systems*, 35: 30318–30332.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Dettmers, T.; Lewis, M.; Belkada, Y.; 和 Zettlemoyer, L. 2022.
    LLM.int8(): 用于大规模变换器的 8 位矩阵乘法。*神经信息处理系统进展*，35: 30318–30332。'
- en: 'Dettmers et al. (2024) Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
    L. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2024）Dettmers, T.; Pagnoni, A.; Holtzman, A.; 和 Zettlemoyer, L.
    2024. Qlora: 高效的量化 LLM 微调。*神经信息处理系统进展*，36。'
- en: 'Dettmers et al. (2023) Dettmers, T.; Svirschevski, R.; Egiazarian, V.; Kuznedelev,
    D.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; and Alistarh, D. 2023.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    *arXiv preprint arXiv:2306.03078*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2023) Dettmers, T.; Svirschevski, R.; Egiazarian, V.; Kuznedelev,
    D.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; 和 Alistarh, D. 2023.
    Spqr: 一种稀疏量化表示用于接近无损的 llm 权重压缩. *arXiv 预印本 arXiv:2306.03078*。'
- en: Egiazarian et al. (2024) Egiazarian, V.; Panferov, A.; Kuznedelev, D.; Frantar,
    E.; Babenko, A.; and Alistarh, D. 2024. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人 (2024) Egiazarian, V.; Panferov, A.; Kuznedelev, D.; Frantar,
    E.; Babenko, A.; 和 Alistarh, D. 2024. 通过加性量化极限压缩大型语言模型. *arXiv 预印本 arXiv:2401.06118*。
- en: 'Frantar et al. (2022) Frantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh,
    D. 2022. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 (2022) Frantar, E.; Ashkboos, S.; Hoefler, T.; 和 Alistarh, D. 2022.
    Gptq: 生成预训练变换器的准确后训练量化. *arXiv 预印本 arXiv:2210.17323*。'
- en: 'Gao et al. (2021) Gao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Foster,
    C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff, N.; et al. 2021. A framework
    for few-shot language model evaluation. *Version v0\. 0.1\. Sept*, 10: 8–9.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 (2021) Gao, L.; Tow, J.; Biderman, S.; Black, S.; DiPofi, A.; Foster,
    C.; Golding, L.; Hsu, J.; McDonell, K.; Muennighoff, N.; 等. 2021. 少样本语言模型评估框架.
    *版本 v0.0.1. 九月*，10: 8–9。'
- en: 'Hardy et al. (2023) Hardy, M.; Sucholutsky, I.; Thompson, B.; and Griffiths,
    T. 2023. Large language models meet cognitive science: LLMs as tools, models,
    and participants. In *Proceedings of the annual meeting of the cognitive science
    society*, volume 45.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hardy 等人 (2023) Hardy, M.; Sucholutsky, I.; Thompson, B.; 和 Griffiths, T. 2023.
    大型语言模型与认知科学的结合：LLMs 作为工具、模型和参与者. 收录于 *认知科学学会年会论文集*，第 45 卷。
- en: 'Hu et al. (2024a) Hu, X.; Chen, Y.; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; and
    Xu, C. 2024a. I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models. *arXiv preprint arXiv:2405.17849*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 (2024a) Hu, X.; Chen, Y.; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; 和 Xu,
    C. 2024a. I-LLM: 高效的整数-only 推理用于完全量化的低比特大型语言模型. *arXiv 预印本 arXiv:2405.17849*。'
- en: 'Hu et al. (2024b) Hu, X.; Chen, Y.; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; and
    Xu, C. 2024b. I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit
    Large Language Models. *arXiv preprint arXiv:2405.17849*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人 (2024b) Hu, X.; Chen, Y.; Yang, D.; Zhou, S.; Yuan, Z.; Yu, J.; 和 Xu,
    C. 2024b. I-LLM: 高效的整数-only 推理用于完全量化的低比特大型语言模型. *arXiv 预印本 arXiv:2405.17849*。'
- en: 'Hu et al. (2023) Hu, Z.; Feng, Y.; Luu, A. T.; Hooi, B.; and Lipani, A. 2023.
    Unlocking the potential of user feedback: Leveraging large language model as user
    simulators to enhance dialogue system. In *Proceedings of the 32nd ACM International
    Conference on Information and Knowledge Management*, 3953–3957.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2023) Hu, Z.; Feng, Y.; Luu, A. T.; Hooi, B.; 和 Lipani, A. 2023. 解锁用户反馈的潜力：利用大型语言模型作为用户模拟器来提升对话系统.
    收录于 *第 32 届 ACM 国际信息与知识管理大会论文集*，3953–3957。
- en: 'Huang et al. (2024) Huang, W.; Liu, Y.; Qin, H.; Li, Y.; Zhang, S.; Liu, X.;
    Magno, M.; and Qi, X. 2024. Billm: Pushing the limit of post-training quantization
    for llms. *arXiv preprint arXiv:2402.04291*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人 (2024) Huang, W.; Liu, Y.; Qin, H.; Li, Y.; Zhang, S.; Liu, X.; Magno,
    M.; 和 Qi, X. 2024. Billm: 推动后训练量化在 llms 中的极限. *arXiv 预印本 arXiv:2402.04291*。'
- en: 'Kim et al. (2023) Kim, S.; Hooper, C.; Gholami, A.; Dong, Z.; Li, X.; Shen,
    S.; Mahoney, M. W.; and Keutzer, K. 2023. Squeezellm: Dense-and-sparse quantization.
    *arXiv preprint arXiv:2306.07629*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等人 (2023) Kim, S.; Hooper, C.; Gholami, A.; Dong, Z.; Li, X.; Shen, S.;
    Mahoney, M. W.; 和 Keutzer, K. 2023. Squeezellm: 密集与稀疏量化. *arXiv 预印本 arXiv:2306.07629*。'
- en: 'Lee et al. (2024) Lee, C.; Jin, J.; Kim, T.; Kim, H.; and Park, E. 2024. Owq:
    Outlier-aware weight quantization for efficient fine-tuning and inference of large
    language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, 13355–13364.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等人 (2024) Lee, C.; Jin, J.; Kim, T.; Kim, H.; 和 Park, E. 2024. Owq: 面向离群点的权重量化，用于高效的微调和大型语言模型推理.
    收录于 *AAAI 人工智能大会论文集*，第 38 卷，13355–13364。'
- en: Li et al. (2016) Li, F.; Liu, B.; Wang, X.; Zhang, B.; and Yan, J. 2016. Ternary
    weight networks. *arXiv preprint arXiv:1605.04711*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2016) Li, F.; Liu, B.; Wang, X.; Zhang, B.; 和 Yan, J. 2016. Ternary weight
    networks. *arXiv 预印本 arXiv:1605.04711*。
- en: 'Lin et al. (2024a) Lin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang,
    W.-C.; Xiao, G.; Dang, X.; Gan, C.; and Han, S. 2024a. AWQ: Activation-aware Weight
    Quantization for On-Device LLM Compression and Acceleration. *Proceedings of Machine
    Learning and Systems*, 6: 87–100.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin等（2024a）Lin, J.; Tang, J.; Tang, H.; Yang, S.; Chen, W.-M.; Wang, W.-C.;
    Xiao, G.; Dang, X.; Gan, C.; 和 Han, S. 2024a. AWQ：针对设备上的LLM压缩和加速的激活感知权重量化。*机器学习与系统会议论文集*,
    6: 87–100。'
- en: 'Lin et al. (2024b) Lin, Y.; Tang, H.; Yang, S.; Zhang, Z.; Xiao, G.; Gan, C.;
    and Han, S. 2024b. Qserve: W4a8kv4 quantization and system co-design for efficient
    llm serving. *arXiv preprint arXiv:2405.04532*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2024b）Lin, Y.; Tang, H.; Yang, S.; Zhang, Z.; Xiao, G.; Gan, C.; 和 Han,
    S. 2024b. Qserve：W4a8kv4量化与系统协同设计用于高效的llm服务。*arXiv预印本 arXiv:2405.04532*。
- en: 'Liu et al. (2023) Liu, Z.; Oguz, B.; Zhao, C.; Chang, E.; Stock, P.; Mehdad,
    Y.; Shi, Y.; Krishnamoorthi, R.; and Chandra, V. 2023. Llm-qat: Data-free quantization
    aware training for large language models. *arXiv preprint arXiv:2305.17888*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023）Liu, Z.; Oguz, B.; Zhao, C.; Chang, E.; Stock, P.; Mehdad, Y.; Shi,
    Y.; Krishnamoorthi, R.; 和 Chandra, V. 2023. Llm-qat：数据自由的量化感知训练用于大型语言模型。*arXiv预印本
    arXiv:2305.17888*。
- en: Loshchilov and Hutter (2017) Loshchilov, I.; and Hutter, F. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter（2017）Loshchilov, I.; 和 Hutter, F. 2017. 解耦的权重衰减正则化。*arXiv预印本
    arXiv:1711.05101*。
- en: 'Ma et al. (2024a) Ma, S.; Wang, H.; Ma, L.; Wang, L.; Wang, W.; Huang, S.;
    Dong, L.; Wang, R.; Xue, J.; and Wei, F. 2024a. The era of 1-bit llms: All large
    language models are in 1.58 bits. *arXiv preprint arXiv:2402.17764*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2024a）Ma, S.; Wang, H.; Ma, L.; Wang, L.; Wang, W.; Huang, S.; Dong, L.;
    Wang, R.; Xue, J.; 和Wei, F. 2024a. 1-bit llms的时代：所有大型语言模型都在1.58位。*arXiv预印本 arXiv:2402.17764*。
- en: 'Ma et al. (2024b) Ma, Y.; Li, H.; Zheng, X.; Ling, F.; Xiao, X.; Wang, R.;
    Wen, S.; Chao, F.; and Ji, R. 2024b. Affinequant: Affine transformation quantization
    for large language models. *arXiv preprint arXiv:2403.12544*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等（2024b）Ma, Y.; Li, H.; Zheng, X.; Ling, F.; Xiao, X.; Wang, R.; Wen, S.;
    Chao, F.; 和 Ji, R. 2024b. Affinequant：大型语言模型的仿射变换量化。*arXiv预印本 arXiv:2403.12544*。
- en: Merity et al. (2016) Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.
    Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity等（2016）Merity, S.; Xiong, C.; Bradbury, J.; 和 Socher, R. 2016. 指针哨兵混合模型。*arXiv预印本
    arXiv:1609.07843*。
- en: 'Raffel et al. (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang,
    S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the limits of
    transfer learning with a unified text-to-text transformer. *Journal of machine
    learning research*, 21(140): 1–67.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raffel等（2020）Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena,
    M.; Zhou, Y.; Li, W.; 和 Liu, P. J. 2020. 通过统一的文本到文本转换器探索迁移学习的极限。*机器学习研究杂志*, 21(140):
    1–67。'
- en: 'Sakaguchi et al. (2021) Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; and Choi,
    Y. 2021. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9): 99–106.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi等（2021）Sakaguchi, K.; Bras, R. L.; Bhagavatula, C.; 和 Choi, Y. 2021.
    Winogrande：大规模对抗性Winograd模式挑战。*ACM通讯*, 64(9): 99–106。'
- en: 'Shang et al. (2023) Shang, Y.; Yuan, Z.; Wu, Q.; and Dong, Z. 2023. Pb-llm:
    Partially binarized large language models. *arXiv preprint arXiv:2310.00034*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang等（2023）Shang, Y.; Yuan, Z.; Wu, Q.; 和 Dong, Z. 2023. Pb-llm：部分二值化的大型语言模型。*arXiv预印本
    arXiv:2310.00034*。
- en: 'Shao et al. (2023) Shao, W.; Chen, M.; Zhang, Z.; Xu, P.; Zhao, L.; Li, Z.;
    Zhang, K.; Gao, P.; Qiao, Y.; and Luo, P. 2023. Omniquant: Omnidirectionally calibrated
    quantization for large language models. *arXiv preprint arXiv:2308.13137*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao等（2023）Shao, W.; Chen, M.; Zhang, Z.; Xu, P.; Zhao, L.; Li, Z.; Zhang, K.;
    Gao, P.; Qiao, Y.; 和 Luo, P. 2023. Omniquant：全向校准量化的大型语言模型。*arXiv预印本 arXiv:2308.13137*。
- en: 'Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;
    Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; et al.
    2023a. Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023a）Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; 等. 2023a. Llama：开放且高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; et al. 2023b.
    Llama 2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023b）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
    Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; 等. 2023b. Llama
    2：开放的基础和微调聊天模型。*arXiv预印本 arXiv:2307.09288*。
- en: 'Tseng et al. (2024) Tseng, A.; Chee, J.; Sun, Q.; Kuleshov, V.; and De Sa,
    C. 2024. Quip#: Even better LLM quantization with hadamard incoherence and lattice
    codebooks. *arXiv preprint arXiv:2402.04396*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng et al. (2024) Tseng, A.; Chee, J.; Sun, Q.; Kuleshov, V.; 和 De Sa, C.
    2024. Quip#: 通过哈达玛不相干性和格子代码本实现更好的LLM量化。 *arXiv 预印本 arXiv:2402.04396*。'
- en: 'Xiao et al. (2023) Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and
    Han, S. 2023. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, 38087–38099\.
    PMLR.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; 和 Han,
    S. 2023. Smoothquant: 针对大型语言模型的准确且高效的训练后量化。在 *国际机器学习大会*，38087–38099。PMLR。'
- en: 'Xu et al. (2023a) Xu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,
    F.; Huang, S.; Qiao, Y.; and Luo, P. 2023a. Lvlm-ehub: A comprehensive evaluation
    benchmark for large vision-language models. *arXiv preprint arXiv:2306.09265*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023a) Xu, P.; Shao, W.; Zhang, K.; Gao, P.; Liu, S.; Lei, M.; Meng,
    F.; Huang, S.; Qiao, Y.; 和 Luo, P. 2023a. Lvlm-ehub: 大型视觉-语言模型的综合评估基准。 *arXiv
    预印本 arXiv:2306.09265*。'
- en: 'Xu et al. (2023b) Xu, Y.; Xie, L.; Gu, X.; Chen, X.; Chang, H.; Zhang, H.;
    Chen, Z.; Zhang, X.; and Tian, Q. 2023b. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. *arXiv preprint arXiv:2309.14717*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023b) Xu, Y.; Xie, L.; Gu, X.; Chen, X.; Chang, H.; Zhang, H.;
    Chen, Z.; Zhang, X.; 和 Tian, Q. 2023b. Qa-lora: 针对大型语言模型的量化感知低秩适应。 *arXiv 预印本
    arXiv:2309.14717*。'
- en: 'Yao et al. (2022) Yao, Z.; Yazdani Aminabadi, R.; Zhang, M.; Wu, X.; Li, C.;
    and He, Y. 2022. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. *Advances in Neural Information Processing Systems*,
    35: 27168–27183.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Yao, Z.; Yazdani Aminabadi, R.; Zhang, M.; Wu, X.; Li, C.;
    和 He, Y. 2022. Zeroquant: 大规模变换器的高效且经济的训练后量化。 *神经信息处理系统进展*, 35: 27168–27183。'
- en: 'Zellers et al. (2019) Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; and
    Choi, Y. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Zellers, R.; Holtzman, A.; Bisk, Y.; Farhadi, A.; 和 Choi,
    Y. 2019. Hellaswag: 机器真的能完成你的句子吗？ *arXiv 预印本 arXiv:1905.07830*。'
- en: 'Zhao et al. (2024) Zhao, Y.; Lin, C.-Y.; Zhu, K.; Ye, Z.; Chen, L.; Zheng,
    S.; Ceze, L.; Krishnamurthy, A.; Chen, T.; and Kasikci, B. 2024. Atom: Low-bit
    quantization for efficient and accurate llm serving. *Proceedings of Machine Learning
    and Systems*, 6: 196–209.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2024) Zhao, Y.; Lin, C.-Y.; Zhu, K.; Ye, Z.; Chen, L.; Zheng,
    S.; Ceze, L.; Krishnamurthy, A.; Chen, T.; 和 Kasikci, B. 2024. Atom: 低比特量化用于高效且准确的LLM服务。
    *机器学习与系统会议论文集*, 6: 196–209。'
- en: Zheng et al. (2024) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2024. Judging llm-as-a-judge
    with mt-bench and chatbot arena. *Advances in Neural Information Processing Systems*,
    36.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024) Zheng, L.; Chiang, W.-L.; Sheng, Y.; Zhuang, S.; Wu, Z.;
    Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E.; 等. 2024. 通过mt-bench和聊天机器人竞技场评估LLM作为裁判的表现。
    *神经信息处理系统进展*, 36。
- en: Appendix A Bit Balance Analyze
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 比特平衡分析
- en: 'As shown in Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Bit Balance Analyze ‣
    ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models"),
    there is a clear asymmetry in the weights of o_proj after INT2 quantization, particularly
    evident in the 10th and 20th blocks. This asymmetry shifts the distribution of
    the model’s weights, resulting in significant performance loss. However, after
    applying the balancing strategy, the distribution of weights closely aligns with
    that of the full-precision model, as illustrated in the subsequent figure. This
    alignment preserves the original symmetric distribution of weights and greatly
    enhances model performance.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[7](#A1.F7 "Figure 7 ‣ Appendix A Bit Balance Analyze ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models")所示，INT2量化后的o_proj权重存在明显的非对称性，特别是在第10和第20个块中尤为明显。这种非对称性改变了模型权重的分布，导致显著的性能损失。然而，在应用平衡策略后，权重分布与全精度模型的分布紧密对齐，如后续图所示。这种对齐保留了原始的对称权重分布，并显著提升了模型性能。'
- en: '![Refer to caption](img/db1353db79996769e3d4bd9ecad6ffab.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/db1353db79996769e3d4bd9ecad6ffab.png)'
- en: 'Figure 7: Quantile-Quantile Plots of data distribution with o_proj weights
    in the 1st, 10th, 20th, and 30th blocks at full precision (up), INT2 quantization
    (middle) and INT2* quantization (below) in the LLaMA-7B model'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: LLaMA-7B模型中第1、第10、第20和第30个块的o_proj权重数据分布的分位数-分位数图（上）全精度（中）INT2量化（下）INT2*量化'
- en: Appendix B Arbitrary Bit Matrix Calculation
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 任意比特矩阵计算
- en: Formally, given a p-bit weight matrix W and a q-bit activation matrix X, we
    can first decompose into 1-bit matrices $W^{s},\quad s\in\left\{0,1,...,p-1\right\}$.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，给定一个p位权重矩阵W和一个q位激活矩阵X，我们可以首先将其分解为1位矩阵$W^{s},\quad s\in\left\{0,1,...,p-1\right\}$。
- en: '|  | $\displaystyle w^{s}_{i,j}=\left(w_{i,j}\gg s\right)\&amp;1,\quad x^{t}_{i,j}=\left(x_{i,j}\gg
    t\right)\&amp;1$ |  | (8) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w^{s}_{i,j}=\left(w_{i,j}\gg s\right)\&amp;1,\quad x^{t}_{i,j}=\left(x_{i,j}\gg
    t\right)\&amp;1$ |  | (8) |'
- en: If BMMA stands for 1-bit matrix multiplication, then the BMMA operation can
    be called $p\times q$ times to compute a series of 1-bit matrix multiplication
    components
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果BMMA代表1位矩阵乘法，那么BMMA操作可以调用$p\times q$次来计算一系列1位矩阵乘法组件。
- en: '|  | $\displaystyle Y^{s,t}=BMMA\left(W^{s},X^{t}\right)$ |  | (9) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y^{s,t}=BMMA\left(W^{s},X^{t}\right)$ |  | (9) |'
- en: Finally, we process all 1-bit matrix multiplication components with bit-stacked
    scaling factors, after which they are summed to obtain a 32-bit output matrix
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们使用位堆叠缩放因子处理所有1位矩阵乘法组件，然后将它们相加以获得32位输出矩阵。
- en: '|  | $\displaystyle Y=\sum_{s=0}^{p-1}\sum_{t=0}^{q-1}Y^{s,t}\ast 2^{s+t}$
    |  | (10) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Y=\sum_{s=0}^{p-1}\sum_{t=0}^{q-1}Y^{s,t}\ast 2^{s+t}$
    |  | (10) |'
- en: After the above transformation process, we decompose the operation of arbitrary
    quantized combinations into a special superposition of 1-bit matrix multiplications,
    so that the underlying layer can be implemented by calling high-computing BMMA
    instructions.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述转换过程中，我们将任意量化组合的操作分解为1位矩阵乘法的特殊叠加，以便底层可以通过调用高计算的BMMA指令来实现。
- en: Appendix C Full Results for Quantization Algorithms
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 量化算法完整结果
- en: 'In this paper, we mainly publish metrics for per-channel/per-token quantization.
    However, ABQ-LLM is naturally orthogonal to per-group quantization, and we validate
    this on LLaMA-7B for W4A4 g128 per-group quantization with the results shown in
    Table [5](#A3.T5 "Table 5 ‣ Appendix C Full Results for Quantization Algorithms
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models").
    Experimental results demonstrate that, in most cases, our method outperforms another
    state-of-the-art (SOTA) method, Atom(Zhao et al. [2024](#bib.bib42)), which employs
    per-group quantization. Additionally, our method exhibits a degradation of less
    than 0.5 in PPL compared to FP16, indicating excellent performance.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们主要发布了每通道/每令牌量化的指标。然而，ABQ-LLM天然与每组量化正交，我们在LLaMA-7B上验证了W4A4 g128每组量化，结果见表[5](#A3.T5
    "表 5 ‣ 附录 C 量化算法完整结果 ‣ ABQ-LLM：大语言模型的任意位量化推理加速")。实验结果表明，在大多数情况下，我们的方法优于另一种最先进（SOTA）方法Atom（Zhao
    et al. [2024](#bib.bib42)），该方法采用了每组量化。此外，我们的方法在PPL上的降幅小于0.5，相比FP16表现出色。
- en: Furthermore, we present the perplexity (PPL) metrics and zero-shot performance
    of ABQ-LLM under various quantization configurations. Unfortunately, most prior
    work has not disclosed complete experimental data, with only PPL metrics for common
    configurations such as W4A16, W3A16, W2A16, W4A4, and W6A6 being published. Additionally,
    the full zero-shot results were not made publicly available. We aim to establish
    a comprehensive quantization baseline for post-training quantization (PTQ) by
    making our extensive data publicly accessible. However, due to limitations in
    time and computational resources, we can only provide our own comprehensive experimental
    results and cannot fully reproduce the results of other baselines.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还展示了ABQ-LLM在各种量化配置下的困惑度（PPL）指标和零样本性能。不幸的是，大多数先前的工作未披露完整的实验数据，仅发布了W4A16、W3A16、W2A16、W4A4和W6A6等常见配置的PPL指标。此外，完整的零样本结果未公开。我们旨在通过公开我们广泛的数据来建立一个全面的训练后量化（PTQ）基线。然而，由于时间和计算资源的限制，我们只能提供自己的全面实验结果，无法完全重现其他基线的结果。
- en: '| Bits | Method | PPL$\downarrow$ |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 位数 | 方法 | PPL$\downarrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Wikitext2 | C4 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande |
    Avg. |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Wikitext2 | C4 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande |
    平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | - | 5.67 | 7.08 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 |
    64.09 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.67 | 7.08 | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 |
    64.09 |'
- en: '| W4A4 g128 | Atom | 6.16 | 7.70 | 76.28 | 52.10 | 38.99 | 69.79 | 69.81 |
    63.69 | 61.78 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 g128 | Atom | 6.16 | 7.70 | 76.28 | 52.10 | 38.99 | 69.79 | 69.81 |
    63.69 | 61.78 |'
- en: '| W4A4 g128 | ABQ-LLM | 6.05 | 7.61 | 76.55 | 51.81 | 38.14 | 71.65 | 70.22
    | 63.22 | 61.93 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 g128 | ABQ-LLM | 6.05 | 7.61 | 76.55 | 51.81 | 38.14 | 71.65 | 70.22
    | 63.22 | 61.93 |'
- en: 'Table 5: Performance of weight-activation per-group quantization on LLaMA-7B
    with a group size of 128.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLaMA-7B的权重-激活每组量化性能，组大小为128。
- en: 'To ensure the reproducibility of our results, we specify that for the PPL test,
    we follow the GPTQ method by setting the sentence length to 2048\. It is important
    to note that different sentence lengths may affect the accuracy of the metrics.
    Additionally, for the zero-shot accuracy test, two metrics may be used: acc or
    acc_norm. If acc_norm is available, we report acc_norm; otherwise, we report the
    acc metric. Finally, we have published the quantized model series on the Huggingface
    platform. Table [6](#A3.T6 "Table 6 ‣ Appendix C Full Results for Quantization
    Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large
    Language Models") presents the perplexity metrics for weight-only quantization,
    while Table [7](#A3.T7 "Table 7 ‣ Appendix C Full Results for Quantization Algorithms
    ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")
    displays the perplexity metrics for various configurations where both weights
    and activations are quantized simultaneously. The zero-shot accuracies of LLaMA-7B,
    LLaMA-13B, LLaMA-2-7B, and LLaMA-2-13B are shown in Tables [9](#A3.T9 "Table 9
    ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"), [10](#A3.T10 "Table
    10 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"), [8](#A3.T8 "Table
    8 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"), and [11](#A3.T11
    "Table 11 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models"), respectively. Extensive
    experiments demonstrate that our method consistently outperforms other baselines
    across all quantization configurations, achieving state-of-the-art performance
    in both weight-only quantization and weight-activation quantization.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确保我们结果的可重复性，我们指定在PPL测试中，按照GPTQ方法，将句子长度设置为2048\. 需要注意的是，不同的句子长度可能会影响指标的准确性。此外，对于零-shot准确性测试，可以使用两个指标：acc
    或 acc_norm。如果 acc_norm 可用，我们报告 acc_norm；否则，我们报告 acc 指标。最后，我们已经在 Huggingface 平台上发布了量化模型系列。表 [6](#A3.T6
    "Table 6 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") 展示了仅权重量化的困惑度指标，而表 [7](#A3.T7
    "Table 7 ‣ Appendix C Full Results for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit
    Quantized Inference Acceleration for Large Language Models") 则显示了权重和激活同时量化的各种配置下的困惑度指标。LLaMA-7B、LLaMA-13B、LLaMA-2-7B
    和 LLaMA-2-13B 的零-shot准确性分别显示在表 [9](#A3.T9 "Table 9 ‣ Appendix C Full Results for
    Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")、[10](#A3.T10 "Table 10 ‣ Appendix C Full Results for
    Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")、[8](#A3.T8 "Table 8 ‣ Appendix C Full Results for
    Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models") 和 [11](#A3.T11 "Table 11 ‣ Appendix C Full Results
    for Quantization Algorithms ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models")。大量实验表明，我们的方法在所有量化配置中始终优于其他基线，在仅权重量化和权重-激活量化方面都实现了最先进的性能。'
- en: '| Bits | Method | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 位数 | 方法 | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
- en: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 | WikiText2 | C4 |'
- en: '| FP16 | - | 5.67 | 7.08 | 5.09 | 6.61 | 5.47 | 6.97 | 4.88 | 6.46 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.67 | 7.08 | 5.09 | 6.61 | 5.47 | 6.97 | 4.88 | 6.46 |'
- en: '| W4A16 | GPTQ | 6.13 | 7.43 | 5.40 | 6.84 | 5.83 | 7.37 | 5.13 | 6.70 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | GPTQ | 6.13 | 7.43 | 5.40 | 6.84 | 5.83 | 7.37 | 5.13 | 6.70 |'
- en: '| AWQ | 6.08 | 7.52 | 5.34 | 6.86 | 6.15 | 7.68 | 5.12 | 6.74 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.08 | 7.52 | 5.34 | 6.86 | 6.15 | 7.68 | 5.12 | 6.74 |'
- en: '| OmniQuant | 5.86 | 7.34 | 5.21 | 6.76 | 5.74 | 7.35 | 5.02 | 6.65 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.86 | 7.34 | 5.21 | 6.76 | 5.74 | 7.35 | 5.02 | 6.65 |'
- en: '| AffineQuant | 5.84 | 7.30 | 5.20 | 6.75 | 5.69 | 7.29 | 5.01 | 6.64 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 5.84 | 7.30 | 5.20 | 6.75 | 5.69 | 7.29 | 5.01 | 6.64 |'
- en: '| ABQ-LLM | 5.83 | 7.29 | 5.19 | 6.75 | 5.64 | 7.20 | 5.01 | 6.63 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 5.83 | 7.29 | 5.19 | 6.75 | 5.64 | 7.20 | 5.01 | 6.63 |'
- en: '| W3A16 | GPTQ | 8.06 | 9.49 | 6.76 | 8.16 | 8.37 | 9.81 | 6.44 | 8.02 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | GPTQ | 8.06 | 9.49 | 6.76 | 8.16 | 8.37 | 9.81 | 6.44 | 8.02 |'
- en: '| AWQ | 11.88 | 13.26 | 7.45 | 9.13 | 24.00 | 23.85 | 10.45 | 13.07 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 11.88 | 13.26 | 7.45 | 9.13 | 24.00 | 23.85 | 10.45 | 13.07 |'
- en: '| OmniQuant | 6.49 | 8.19 | 5.68 | 7.32 | 6.58 | 8.65 | 5.58 | 7.44 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 6.49 | 8.19 | 5.68 | 7.32 | 6.58 | 8.65 | 5.58 | 7.44 |'
- en: '| AffineQuant | 6.30 | 8.03 | 5.60 | 7.20 | 6.55 | 8.57 | 5.62 | 7.56 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 6.30 | 8.03 | 5.60 | 7.20 | 6.55 | 8.57 | 5.62 | 7.56 |'
- en: '| ABQ-LLM | 6.29 | 8.01 | 5.56 | 7.24 | 6.28 | 8.10 | 5.44 | 7.26 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 6.29 | 8.01 | 5.56 | 7.24 | 6.28 | 8.10 | 5.44 | 7.26 |'
- en: '| W2A16 | GPTQ | 2.1e3 | 689.13 | 5.5e3 | 2.5e3 | 7.7e3 | NAN | 2.1e3 | 323.12
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| W2A16 | GPTQ | 2.1e3 | 689.13 | 5.5e3 | 2.5e3 | 7.7e3 | NAN | 2.1e3 | 323.12
    |'
- en: '| OmniQuant | 15.47 | 24.89 | 13.21 | 18.31 | 37.37 | 90.64 | 17.21 | 26.76
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 15.47 | 24.89 | 13.21 | 18.31 | 37.37 | 90.64 | 17.21 | 26.76
    |'
- en: '| AffineQuant | 9.53 | 14.89 | 7.54 | 12.46 | 35.07 | 572.22 | 12.42 | 23.67
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 9.53 | 14.89 | 7.54 | 12.46 | 35.07 | 572.22 | 12.42 | 23.67
    |'
- en: '| ABQ-LLM | 11.48 | 15.74 | 9.34 | 12.28 | 13.11 | 17.81 | 13.09 | 20.49 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 11.48 | 15.74 | 9.34 | 12.28 | 13.11 | 17.81 | 13.09 | 20.49 |'
- en: '| W2*A16 | ABQ-LLM | 7.50 | 9.86 | 6.64 | 8.43 | 7.82 | 10.33 | 6.52 | 7.87
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | ABQ-LLM | 7.50 | 9.86 | 6.64 | 8.43 | 7.82 | 10.33 | 6.52 | 7.87
    |'
- en: 'Table 6: Weight-only quantization perplexities (lower is better) comparison
    of quantized LLaMA and LLaMA-2 models. * denotes the use of the bit balance strategy.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：仅权重量化困惑度（数值越低越好）对量化 LLaMA 和 LLaMA-2 模型的比较。* 表示使用了位平衡策略。
- en: '| Bits | Method | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Bits | Method | LLaMA-7B | LLaMA-13B | LLaMA-2-7B | LLaMA-2-13B |'
- en: '| Wikitext2 | C4 | Wikitext2 | C4 | Wikitext2 | C4 | Wikitext2 | C4 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Wikitext2 | C4 | Wikitext2 | C4 | Wikitext2 | C4 | Wikitext2 | C4 |'
- en: '| FP16 | - | 5.67 | 7.08 | 5.09 | 6.61 | 5.47 | 6.97 | 4.88 | 6.46 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.67 | 7.08 | 5.09 | 6.61 | 5.47 | 6.97 | 4.88 | 6.46 |'
- en: '| W8A8 | SmoothQuant | 5.73 | - | 5.13 | - | 5.54 | - | 4.95 | - |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | SmoothQuant | 5.73 | - | 5.13 | - | 5.54 | - | 4.95 | - |'
- en: '| ABQ-LLM | 5.68 | 7.09 | 5.10 | 6.62 | 5.48 | 6.99 | 4.89 | 6.47 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 5.68 | 7.09 | 5.10 | 6.62 | 5.48 | 6.99 | 4.89 | 6.47 |'
- en: '| W4A8 | AWQ | 6.33 | - | 5.59 | - | 6.28 | - | 5.25 | - |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | AWQ | 6.33 | - | 5.59 | - | 6.28 | - | 5.25 | - |'
- en: '| QuaRot | 5.93 | - | 5.29 | - | 5.73 | - | 5.07 | - |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| QuaRot | 5.93 | - | 5.29 | - | 5.73 | - | 5.07 | - |'
- en: '| Atom | 6.03 | - | 5.41 | - | 5.91 | - | 5.16 | - |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 6.03 | - | 5.41 | - | 5.91 | - | 5.16 | - |'
- en: '| Qserve | 5.93 | - | 5.28 | - | 5.75 | - | 5.12 | - |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Qserve | 5.93 | - | 5.28 | - | 5.75 | - | 5.12 | - |'
- en: '| OmniQuant | 5.87 | 7.34 | - | - | - | - | - | - |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.87 | 7.34 | - | - | - | - | - | - |'
- en: '| ABQ-LLM | 5.84 | 7.32 | 5.22 | 6.77 | 5.67 | 7.24 | 5.01 | 6.64 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 5.84 | 7.32 | 5.22 | 6.77 | 5.67 | 7.24 | 5.01 | 6.64 |'
- en: '| W4A6 | OmniQuant | 6.09 | 7.63 | - | - | - | - | - | - |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| W4A6 | OmniQuant | 6.09 | 7.63 | - | - | - | - | - | - |'
- en: '| ABQ-LLM | 6.01 | 7.58 | 5.37 | 6.96 | 5.89 | 7.56 | 5.17 | 6.87 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 6.01 | 7.58 | 5.37 | 6.96 | 5.89 | 7.56 | 5.17 | 6.87 |'
- en: '| W3A8 | ABQ-LLM | 6.30 | 8.04 | 5.59 | 7.26 | 6.27 | 8.14 | 5.45 | 7.27 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| W3A8 | ABQ-LLM | 6.30 | 8.04 | 5.59 | 7.26 | 6.27 | 8.14 | 5.45 | 7.27 |'
- en: '| W3A6 | ABQ-LLM | 6.60 | 8.47 | 5.84 | 7.61 | 6.56 | 8.65 | 5.92 | 7.90 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| W3A6 | ABQ-LLM | 6.60 | 8.47 | 5.84 | 7.61 | 6.56 | 8.65 | 5.92 | 7.90 |'
- en: '| W3A4 | ABQ-LLM | 12.16 | 17.19 | 9.96 | 14.36 | 13.65 | 19.00 | 20.35 | 20.09
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| W3A4 | ABQ-LLM | 12.16 | 17.19 | 9.96 | 14.36 | 13.65 | 19.00 | 20.35 | 20.09
    |'
- en: '| W2*A8 | ABQ-LLM | 7.59 | 10.00 | 6.49 | 8.53 | 7.85 | 10.33 | 6.65 | 10.01
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 7.59 | 10.00 | 6.49 | 8.53 | 7.85 | 10.33 | 6.65 | 10.01
    |'
- en: '| W2*A6 | ABQ-LLM | 8.08 | 10.89 | 6.99 | 9.30 | 9.08 | 11.74 | 10.91 | 14.89
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| W2*A6 | ABQ-LLM | 8.08 | 10.89 | 6.99 | 9.30 | 9.08 | 11.74 | 10.91 | 14.89
    |'
- en: 'Table 7: Weight-activation quantization perplexities (lower is better) comparison
    of quantized LLaMA and LLaMA-2 models. * denotes the use of the bit balance strategy.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：权重-激活量化困惑度（数值越低越好）对量化 LLaMA 和 LLaMA-2 模型的比较。* 表示使用了位平衡策略。
- en: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | FP16 | - | 76.98 | 53.57 | 40.61 | 71.07 | 72.96 | 67.24 | 63.73
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | FP16 | - | 76.98 | 53.57 | 40.61 | 71.07 | 72.96 | 67.24 | 63.73
    |'
- en: '| W4A16 | ABQ-LLM | 77.36 | 53.82 | 39.50 | 70.76 | 71.76 | 66.85 | 63.34 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | ABQ-LLM | 77.36 | 53.82 | 39.50 | 70.76 | 71.76 | 66.85 | 63.34 |'
- en: '| W3A16 | ABQ-LLM | 76.55 | 53.66 | 39.24 | 63.97 | 68.93 | 65.90 | 61.38 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | ABQ-LLM | 76.55 | 53.66 | 39.24 | 63.97 | 68.93 | 65.90 | 61.38 |'
- en: '| W2*A16 | ABQ-LLM | 72.79 | 48.14 | 35.32 | 63.94 | 63.21 | 61.79 | 57.53
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | ABQ-LLM | 72.79 | 48.14 | 35.32 | 63.94 | 63.21 | 61.79 | 57.53
    |'
- en: '| W8A8 | ABQ-LLM | 76.76 | 53.53 | 40.35 | 71.19 | 72.84 | 66.92 | 63.60 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | ABQ-LLM | 76.76 | 53.53 | 40.35 | 71.19 | 72.84 | 66.92 | 63.60 |'
- en: '| W6A6 | ABQ-LLM | 76.77 | 53.20 | 40.44 | 71.37 | 71.81 | 66.22 | 63.30 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | ABQ-LLM | 76.77 | 53.20 | 40.44 | 71.37 | 71.81 | 66.22 | 63.30 |'
- en: '| W4A8 | ABQ-LLM | 76.76 | 53.11 | 39.07 | 67.86 | 71.38 | 66.92 | 62.52 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | ABQ-LLM | 76.76 | 53.11 | 39.07 | 67.86 | 71.38 | 66.92 | 62.52 |'
- en: '| W4A6 | ABQ-LLM | 76.06 | 52.56 | 38.82 | 67.22 | 70.35 | 64.33 | 61.60 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| W4A6 | ABQ-LLM | 76.06 | 52.56 | 38.82 | 67.22 | 70.35 | 64.33 | 61.60 |'
- en: '| W4A4 | ABQ-LLM | 68.55 | 44.11 | 31.31 | 63.12 | 55.29 | 53.82 | 52.70 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | ABQ-LLM | 68.55 | 44.11 | 31.31 | 63.12 | 55.29 | 53.82 | 52.70 |'
- en: '| W3A8 | ABQ-LLM | 76.55 | 52.44 | 38.99 | 65.77 | 68.34 | 66.61 | 61.45 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| W3A8 | ABQ-LLM | 76.55 | 52.44 | 38.99 | 65.77 | 68.34 | 66.61 | 61.45 |'
- en: '| W3A6 | ABQ-LLM | 74.26 | 51.47 | 37.62 | 65.22 | 66.74 | 63.22 | 59.76 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| W3A6 | ABQ-LLM | 74.26 | 51.47 | 37.62 | 65.22 | 66.74 | 63.22 | 59.76 |'
- en: '| W3A4 | ABQ-LLM | 64.47 | 39.56 | 27.47 | 58.81 | 49.28 | 53.28 | 48.81 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| W3A4 | ABQ-LLM | 64.47 | 39.56 | 27.47 | 58.81 | 49.28 | 53.28 | 48.81 |'
- en: '| W2*A8 | ABQ-LLM | 72.74 | 47.81 | 35.07 | 62.78 | 63.31 | 61.09 | 57.13 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 72.74 | 47.81 | 35.07 | 62.78 | 63.31 | 61.09 | 57.13 |'
- en: '| W2*A6 | ABQ-LLM | 71.98 | 45.54 | 31.66 | 62.14 | 60.14 | 55.25 | 54.45 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| W2*A6 | ABQ-LLM | 71.98 | 45.54 | 31.66 | 62.14 | 60.14 | 55.25 | 54.45 |'
- en: 'Table 8: Zero-shot accuracy (higher is better) of LLaMA-2-7B under different
    quantization configuration. * denotes the use of the bit balance strategy.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：不同量化配置下 LLaMA-2-7B 的零-shot 准确率（越高越好）。* 表示使用了位平衡策略。
- en: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | 方法 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande | 平均值
    |'
- en: '| LLaMA-7B | FP16 | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | FP16 | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
- en: '| W4A16 | AffineQuant | 77.53 | 51.85 | 38.65 | 70.89 | 71.49 | 66.93 | 62.89
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | AffineQuant | 77.53 | 51.85 | 38.65 | 70.89 | 71.49 | 66.93 | 62.89
    |'
- en: '| ABQ-LLM | 77.80 | 51.55 | 39.59 | 72.66 | 71.41 | 66.14 | 63.19 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 77.80 | 51.55 | 39.59 | 72.66 | 71.41 | 66.14 | 63.19 |'
- en: '| W3A16 | ABQ-LLM | 75.78 | 47.81 | 37.20 | 71.98 | 68.80 | 65.82 | 61.23 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | ABQ-LLM | 75.78 | 47.81 | 37.20 | 71.98 | 68.80 | 65.82 | 61.23 |'
- en: '| W2*A16 | ABQ-LLM | 74.04 | 46.71 | 35.49 | 65.81 | 48.01 | 62.82 | 55.48
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | ABQ-LLM | 74.04 | 46.71 | 35.49 | 65.81 | 48.01 | 62.82 | 55.48
    |'
- en: '| W8A8 | ABQ-LLM | 77.25 | 52.56 | 41.72 | 72.97 | 73.05 | 67.32 | 64.14 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | ABQ-LLM | 77.25 | 52.56 | 41.72 | 72.97 | 73.05 | 67.32 | 64.14 |'
- en: '| W6A6 | SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 76.75 | 51.64 | 39.88 | 71.75 | 71.67 | 65.03 | 62.81
    |'
- en: '| OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 77.09 | 51.89 | 40.87 | 72.53 | 71.61 | 65.03 | 63.17 |'
- en: '| I-LLM | 76.99 | 52.66 | 40.78 | 72.94 | 71.31 | 65.67 | 63.39 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 76.99 | 52.66 | 40.78 | 72.94 | 71.31 | 65.67 | 63.39 |'
- en: '| ABQ-LLM | 78.07 | 52.81 | 40.10 | 71.90 | 71.80 | 66.53 | 63.53 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 78.07 | 52.81 | 40.10 | 71.90 | 71.80 | 66.53 | 63.53 |'
- en: '| W4A8 | OmniQuant | 77.36 | 51.85 | 38.65 | 70.67 | 71.20 | 64.71 | 62.40
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | OmniQuant | 77.36 | 51.85 | 38.65 | 70.67 | 71.20 | 64.71 | 62.40
    |'
- en: '| ABQ-LLM | 77.63 | 52.40 | 38.10 | 73.43 | 71.81 | 65.51 | 63.14 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 77.63 | 52.40 | 38.10 | 73.43 | 71.81 | 65.51 | 63.14 |'
- en: '| W4A6 | OmniQuant | 75.73 | 51.52 | 38.31 | 68.28 | 70.79 | 65.27 | 61.64
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| W4A6 | OmniQuant | 75.73 | 51.52 | 38.31 | 68.28 | 70.79 | 65.27 | 61.64
    |'
- en: '| ABQ-LLM | 76.28 | 50.72 | 39.68 | 70.52 | 70.35 | 64.88 | 62.06 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 76.28 | 50.72 | 39.68 | 70.52 | 70.35 | 64.88 | 62.06 |'
- en: '| W4A4 | OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OmniQuant | 66.15 | 45.20 | 31.14 | 63.51 | 56.44 | 53.43 | 52.65
    |'
- en: '| AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 69.37 | 42.55 | 31.91 | 63.73 | 57.65 | 55.33 | 53.42 |'
- en: '| ABQ-LLM | 69.97 | 45.88 | 33.44 | 62.87 | 58.47 | 54.53 | 54.19 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 69.97 | 45.88 | 33.44 | 62.87 | 58.47 | 54.53 | 54.19 |'
- en: '| W3A8 | ABQ-LLM | 75.78 | 48.95 | 38.05 | 70.18 | 68.70 | 65.74 | 61.23 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| W3A8 | ABQ-LLM | 75.78 | 48.95 | 38.05 | 70.18 | 68.70 | 65.74 | 61.23 |'
- en: '| W3A6 | ABQ-LLM | 75.57 | 49.03 | 38.57 | 69.57 | 67.12 | 62.35 | 60.37 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| W3A6 | ABQ-LLM | 75.57 | 49.03 | 38.57 | 69.57 | 67.12 | 62.35 | 60.37 |'
- en: '| W3A4 | ABQ-LLM | 64.20 | 41.04 | 28.50 | 61.80 | 49.71 | 51.54 | 49.45 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| W3A4 | ABQ-LLM | 64.20 | 41.04 | 28.50 | 61.80 | 49.71 | 51.54 | 49.45 |'
- en: '| W2*A8 | ABQ-LLM | 73.29 | 46.72 | 34.73 | 63.61 | 62.18 | 61.01 | 56.92 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 73.29 | 46.72 | 34.73 | 63.61 | 62.18 | 61.01 | 56.92 |'
- en: '|  | W2*A6 | ABQ-LLM | 71.98 | 46.00 | 33.53 | 63.95 | 59.19 | 58.72 | 55.56
    |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | W2*A6 | ABQ-LLM | 71.98 | 46.00 | 33.53 | 63.95 | 59.19 | 58.72 | 55.56
    |'
- en: 'Table 9: Zero-shot accuracy (higher is better) of LLaMA-7B under different
    quantization configuration. * denotes the use of the bit balance strategy.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：不同量化配置下 LLaMA-7B 的零-shot 准确率（越高越好）。* 表示使用了位平衡策略。
- en: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | 方法 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande | 平均值
    |'
- en: '| LLaMA-13B | FP16 | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 | 66.32
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | FP16 | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 | 66.32
    |'
- en: '| W4A16 | AffineQuant | 78.84 | 59.55 | 43.52 | 69.48 | 75.18 | 69.38 | 65.99
    |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | AffineQuant | 78.84 | 59.55 | 43.52 | 69.48 | 75.18 | 69.38 | 65.99
    |'
- en: '| ABQ-LLM | 78.89 | 59.43 | 43.25 | 70.37 | 75.38 | 69.22 | 66.09 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 78.89 | 59.43 | 43.25 | 70.37 | 75.38 | 69.22 | 66.09 |'
- en: '| W3A16 | ABQ-LLM | 77.86 | 57.62 | 42.15 | 66.45 | 73.03 | 69.53 | 64.44 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | ABQ-LLM | 77.86 | 57.62 | 42.15 | 66.45 | 73.03 | 69.53 | 64.44 |'
- en: '| W2*A16 | ABQ-LLM | 75.03 | 52.99 | 38.48 | 66.42 | 68.69 | 66.53 | 61.36
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | ABQ-LLM | 75.03 | 52.99 | 38.48 | 66.42 | 68.69 | 66.53 | 61.36
    |'
- en: '| W8A8 | ABQ-LLM | 78.73 | 59.01 | 44.11 | 68.17 | 75.96 | 70.09 | 66.01 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | ABQ-LLM | 78.73 | 59.01 | 44.11 | 68.17 | 75.96 | 70.09 | 66.01 |'
- en: '| W6A6 | SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | SmoothQuant | 77.91 | 56.60 | 42.40 | 64.95 | 75.36 | 69.36 | 64.43
    |'
- en: '| OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.94 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 78.40 | 57.28 | 42.91 | 67.00 | 75.82 | 68.27 | 64.94 |'
- en: '| I-LLM | 77.48 | 56.94 | 44.03 | 64.92 | 75.24 | 69.14 | 64.62 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 77.48 | 56.94 | 44.03 | 64.92 | 75.24 | 69.14 | 64.62 |'
- en: '| ABQ-LLM | 78.40 | 57.62 | 42.01 | 66.82 | 75.54 | 69.61 | 65.00 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 78.40 | 57.62 | 42.01 | 66.82 | 75.54 | 69.61 | 65.00 |'
- en: '| W4A8 | ABQ-LLM | 78.56 | 58.75 | 42.92 | 68.56 | 75.20 | 70.56 | 65.76 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | ABQ-LLM | 78.56 | 58.75 | 42.92 | 68.56 | 75.20 | 70.56 | 65.76 |'
- en: '| W4A6 | ABQ-LLM | 77.64 | 56.94 | 42.32 | 67.06 | 74.23 | 68.19 | 64.39 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| W4A6 | ABQ-LLM | 77.64 | 56.94 | 42.32 | 67.06 | 74.23 | 68.19 | 64.39 |'
- en: '| W4A4 | OmniQuant | 69.69 | 47.30 | 33.10 | 62.84 | 58.96 | 55.80 | 54.62
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | OmniQuant | 69.69 | 47.30 | 33.10 | 62.84 | 58.96 | 55.80 | 54.62
    |'
- en: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 66.32 | 43.90 | 29.61 | 64.10 | 56.88 | 54.70 | 52.58 |'
- en: '| I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 55.99 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| I-LLM | 67.95 | 48.15 | 34.47 | 62.29 | 63.13 | 59.98 | 55.99 |'
- en: '| ABQ-LLM | 71.82 | 47.60 | 35.67 | 63.52 | 64.31 | 57.54 | 56.74 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| ABQ-LLM | 71.82 | 47.60 | 35.67 | 63.52 | 64.31 | 57.54 | 56.74 |'
- en: '| W3A8 | ABQ-LLM | 77.81 | 58.16 | 42.41 | 68.47 | 73.15 | 69.37 | 64.90 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| W3A8 | ABQ-LLM | 77.81 | 58.16 | 42.41 | 68.47 | 73.15 | 69.37 | 64.90 |'
- en: '| W3A4 | ABQ-LLM | 64.79 | 42.21 | 30.54 | 60.55 | 55.59 | 53.51 | 51.20 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| W3A4 | ABQ-LLM | 64.79 | 42.21 | 30.54 | 60.55 | 55.59 | 53.51 | 51.20 |'
- en: '| W3A6 | ABQ-LLM | 76.71 | 56.19 | 40.53 | 66.39 | 71.59 | 66.14 | 62.93 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| W3A6 | ABQ-LLM | 76.71 | 56.19 | 40.53 | 66.39 | 71.59 | 66.14 | 62.93 |'
- en: '| W2*A8 | ABQ-LLM | 74.92 | 54.92 | 38.65 | 68.53 | 68.21 | 66.54 | 61.96 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 74.92 | 54.92 | 38.65 | 68.53 | 68.21 | 66.54 | 61.96 |'
- en: '| W2*A6 | ABQ-LLM | 73.71 | 51.89 | 36.60 | 64.98 | 65.55 | 63.22 | 59.33 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| W2*A6 | ABQ-LLM | 73.71 | 51.89 | 36.60 | 64.98 | 65.55 | 63.22 | 59.33 |'
- en: 'Table 10: Zero-shot accuracy (higher is better) of LLaMA-13B under different
    quantization configuration. * denotes the use of the bit balance strategy.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：不同量化配置下 LLaMA-13B 的零-shot 精度（值越高越好）。* 表示使用了位平衡策略。
- en: '| Model | Bits | Method | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande
    | Avg. |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | 方法 | PiQA | ARC-e | ARC-c | BoolQ | HellaSwag | Winogrande | 平均
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-13B | FP16 | - | 79.05 | 57.91 | 44.19 | 69.02 | 76.60 | 69.69 |
    66.07 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | FP16 | - | 79.05 | 57.91 | 44.19 | 69.02 | 76.60 | 69.69 |
    66.07 |'
- en: '| W4A16 | ABQ-LLM | 78.72 | 58.16 | 44.03 | 63.94 | 75.56 | 69.13 | 64.92 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | ABQ-LLM | 78.72 | 58.16 | 44.03 | 63.94 | 75.56 | 69.13 | 64.92 |'
- en: '| W3A16 | ABQ-LLM | 77.48 | 55.39 | 43.86 | 67.58 | 72.63 | 67.56 | 64.08 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| W3A16 | ABQ-LLM | 77.48 | 55.39 | 43.86 | 67.58 | 72.63 | 67.56 | 64.08 |'
- en: '| W2*A16 | ABQ-LLM | 75.41 | 51.39 | 36.43 | 72.63 | 67.03 | 60.62 | 60.59
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| W2*A16 | ABQ-LLM | 75.41 | 51.39 | 36.43 | 72.63 | 67.03 | 60.62 | 60.59
    |'
- en: '| W8A8 | ABQ-LLM | 79.22 | 57.66 | 43.94 | 68.62 | 76.47 | 69.46 | 65.90 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | ABQ-LLM | 79.22 | 57.66 | 43.94 | 68.62 | 76.47 | 69.46 | 65.90 |'
- en: '| W6A6 | ABQ-LLM | 78.62 | 55.81 | 43.69 | 66.33 | 75.51 | 68.27 | 64.70 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | ABQ-LLM | 78.62 | 55.81 | 43.69 | 66.33 | 75.51 | 68.27 | 64.70 |'
- en: '| W4A8 | ABQ-LLM | 78.67 | 57.15 | 43.52 | 64.13 | 75.31 | 69.53 | 64.53 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | ABQ-LLM | 78.67 | 57.15 | 43.52 | 64.13 | 75.31 | 69.53 | 64.53 |'
- en: '| W4A6 | ABQ-LLM | 77.97 | 57.11 | 43.26 | 67.37 | 74.14 | 66.30 | 64.36 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| W4A6 | ABQ-LLM | 77.97 | 57.11 | 43.26 | 67.37 | 74.14 | 66.30 | 64.36 |'
- en: '| W4A4 | ABQ-LLM | 69.04 | 47.01 | 33.53 | 64.74 | 62.70 | 54.38 | 55.23 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | ABQ-LLM | 69.04 | 47.01 | 33.53 | 64.74 | 62.70 | 54.38 | 55.23 |'
- en: '| W3A8 | ABQ-LLM | 77.53 | 56.36 | 42.74 | 68.71 | 72.87 | 66.06 | 64.05 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| W3A8 | ABQ-LLM | 77.53 | 56.36 | 42.74 | 68.71 | 72.87 | 66.06 | 64.05 |'
- en: '| W3A6 | ABQ-LLM | 76.22 | 53.37 | 40.70 | 68.04 | 71.09 | 66.14 | 62.59 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| W3A6 | ABQ-LLM | 76.22 | 53.37 | 40.70 | 68.04 | 71.09 | 66.14 | 62.59 |'
- en: '| W3A4 | ABQ-LLM | 63.60 | 42.55 | 29.35 | 58.62 | 52.62 | 53.19 | 49.99 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| W3A4 | ABQ-LLM | 63.60 | 42.55 | 29.35 | 58.62 | 52.62 | 53.19 | 49.99 |'
- en: '| W2*A8 | ABQ-LLM | 73.50 | 49.79 | 35.15 | 70.15 | 67.45 | 58.88 | 59.15 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| W2*A8 | ABQ-LLM | 73.50 | 49.79 | 35.15 | 70.15 | 67.45 | 58.88 | 59.15 |'
- en: 'Table 11: Zero-shot accuracy (higher is better) of LLaMA-2-13B under different
    quantization configuration. * denotes the use of the bit balance strategy.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：不同量化配置下 LLaMA-2-13B 的零-shot 精度（值越高越好）。* 表示使用了位平衡策略。
- en: Appendix D GPU Kernel Optimization Details
  id: totrans-325
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D GPU 内核优化细节
- en: GEMV Elimination. Our custom engine decomposes the operation of arbitrary quantized
    combinations into a superposition of 1-bit matrix multiplications. As for GEMV
    problems, multiple single-bit GEMV are converted to GEMM, fully utilizing the
    TensorCore while reducing or even avoiding the redundant computation caused by
    padding. For the $W_{q}A_{p}$ 8, calling TensorCore will introduce 87.5% redundant
    calculations. In contrast, ABQKernel achieves direct acceleration of W2A8 without
    any redundant computation.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: GEMV 消除。我们的自定义引擎将任意量化组合的操作分解为 1 位矩阵乘法的叠加。对于 GEMV 问题，多个单比特 GEMV 被转换为 GEMM，充分利用
    TensorCore，同时减少甚至避免由于填充造成的冗余计算。对于 $W_{q}A_{p}$ 8，调用 TensorCore 会引入 87.5% 的冗余计算。相比之下，ABQKernel
    直接加速 W2A8，无任何冗余计算。
- en: '![Refer to caption](img/3438442291512af1e5f8ff5fdf67b7a2.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3438442291512af1e5f8ff5fdf67b7a2.png)'
- en: 'Figure 8: Overview of GEMV Elimination.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：GEMV 消除概览。
- en: 'Computational Pipeline Optimization. Figure [9](#A4.F9 "Figure 9 ‣ Appendix
    D GPU Kernel Optimization Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference
    Acceleration for Large Language Models") illustrates our optimization of the computation
    pipeline to enhance inference performance within the widely adopted Ampere architecture.
    At the shared memory level, we perform asynchronous writes from global memory
    to shared memory using the cp.async instruction to hide the latency of subsequent
    memory accesses. Before processing the first loop, a synchronization instruction
    ensures that TILE-0, the data required by the first loop, has been written to
    shared memory. During the processing of the first loop, TILE-1, the data needed
    for the second loop, is asynchronously written to shared memory. This concurrent
    execution of data writes for the second loop and computation of the first loop
    masks the shared memory access latency for subsequent loops. For register-level
    optimization within the loop, when k=0, TILE-0 data from shared memory is loaded
    into the first set of registers A0 and B0, while the data required for k=1 is
    preloaded into registers A1 and B1. Once the data in registers A0 and B0 is ready,
    the bmma operation is performed on A0 and B0. When k=1, the data A1 and B1 needed
    for bmma has already been preloaded at k=0, and the data required for k=2 is preloaded
    into registers A0 and B0, and so on. By doubling the register cache, data is written
    from shared memory to registers during the Tensor Core bmma computation, effectively
    masking the register access time. Where bmma denotes a 1-bit matrix multiplication
    operation, while TILE data refers to a block of data obtained after partitioning
    and processing the original dataset.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 计算管线优化。图 [9](#A4.F9 "图 9 ‣ 附录 D GPU 内核优化细节 ‣ ABQ-LLM：大语言模型的任意位量化推理加速") 展示了我们对计算管线的优化，以提升在广泛采用的安培架构中的推理性能。在共享内存层面，我们使用
    cp.async 指令执行从全局内存到共享内存的异步写入，以隐藏后续内存访问的延迟。在处理第一个循环之前，使用同步指令确保 TILE-0，即第一个循环所需的数据，已被写入共享内存。在处理第一个循环时，TILE-1，即第二个循环所需的数据，异步写入共享内存。第二个循环的数据写入与第一个循环的计算并行执行，从而掩盖了后续循环的共享内存访问延迟。在循环内的寄存器级优化中，当
    k=0 时，来自共享内存的 TILE-0 数据被加载到第一组寄存器 A0 和 B0 中，而 k=1 所需的数据被预加载到寄存器 A1 和 B1 中。一旦寄存器
    A0 和 B0 中的数据准备好，bmm 操作将在 A0 和 B0 上执行。当 k=1 时，bmm 所需的数据 A1 和 B1 已在 k=0 时预加载，同时
    k=2 所需的数据被预加载到寄存器 A0 和 B0 中，以此类推。通过加倍寄存器缓存，在 Tensor Core bmm 计算期间数据从共享内存写入寄存器，从而有效地掩盖寄存器访问时间。bmm
    表示 1 位矩阵乘法操作，而 TILE 数据指的是在对原始数据集进行分区和处理后获得的数据块。
- en: '![Refer to caption](img/7bdc23b073347045aa50a0f8d285adf7.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7bdc23b073347045aa50a0f8d285adf7.png)'
- en: 'Figure 9: Space-time diagram of the computational pipeline.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：计算管线的时空图。
- en: 'Auto Kernel Search. To utilize GPU memory more efficiently at all levels, a
    chunking strategy is essential during computation. Chunk size is crucial for GEMM
    performance. Before launching an arbitrary precision inference operator, we perform
    performance tests on different chunk sizes to select the best implementation.
    As shown in Figure [4](#S3.F4 "Figure 4 ‣ 3.2 Improving Quantization by Distribution
    Correction ‣ 3 Method ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration
    for Large Language Models"), for the classical GEMM task, the M × N × K problem
    is usually chunked by BM × BN × BK at the thread block level, and each thread
    block is chunked by WM × WN × WK at the warp level. Each warp gets MMA_M × MMA_N
    × MMA_K based on the Tensor Core chunk size supported by the GPU model and searches
    for different chunk shapes after obtaining the chunk sizes at the three levels.
    In the arbitrary precision operator, the number of weight bits (q) and activation
    bits (p) also need to be considered, so the search space is larger compared to
    the classical GEMM. Given the BTC chunk sizes MMA_M = 8, MMA_N = 8, MMA_K = 128,
    the number of weighted warps is computed as W_WARPS_NUM = BN × q / WN, and the
    number of activation warps is computed as X_WARPS_NUM = BM × p / WM. The total
    number of warps is 1 $\leq$ configuration, the design process of a candidate instance
    in the search space is summarized as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 自动内核搜索。为了更高效地利用 GPU 内存，在所有级别上计算时至关重要的是块大小策略。块大小对 GEMM 性能至关重要。在启动任意精度推理操作符之前，我们对不同的块大小进行性能测试，以选择最佳实现。如图
    [4](#S3.F4 "图 4 ‣ 3.2 通过分布校正改进量化 ‣ 3 方法 ‣ ABQ-LLM：大语言模型的任意位量化推理加速") 所示，对于经典的 GEMM
    任务，M × N × K 问题通常在线程块级别按 BM × BN × BK 进行块化，每个线程块在 warp 级别按 WM × WN × WK 进行块化。每个
    warp 根据 GPU 模型支持的 Tensor Core 块大小获得 MMA_M × MMA_N × MMA_K，并在获得三个级别的块大小后搜索不同的块形状。在任意精度操作符中，还需要考虑权重位数
    (q) 和激活位数 (p)，因此搜索空间比经典的 GEMM 更大。给定 BTC 块大小 MMA_M = 8, MMA_N = 8, MMA_K = 128，计算加权
    warps 的数量为 W_WARPS_NUM = BN × q / WN，计算激活 warps 的数量为 X_WARPS_NUM = BM × p / WM。总的
    warps 数量为 1 $\leq$ 配置，搜索空间中候选实例的设计过程总结如下：
- en: 1\. Determine the number and layout of Warps contained in the Thread Block based
    on expert experience. for example, X_WARPS_NUM × W_WARPS_NUM = 1 × 4.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 根据专家经验确定线程块中包含的 Warp 的数量和布局。例如，X_WARPS_NUM × W_WARPS_NUM = 1 × 4。
- en: 2\. Determine the Thread Block Tile($<$) with the smallest redundant padding
    based on the quantized bit width p of the activation and the M dimension of the
    computing task.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 根据激活的量化位宽 p 和计算任务的 M 维度确定具有最小冗余填充的线程块 Tile($<$)。
- en: 3\. The size of the WARP Tile($<$) is calculated based on the layout of Warps
    and Thread Block Tile. Ultimately, we test the operators at various chunk sizes
    and adopt the speed-optimized implementation.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. WARP Tile($<$) 的大小是根据 Warps 和线程块 Tile 的布局计算的。最终，我们测试了不同块大小的操作符，并采用了速度优化的实现。
- en: '![Refer to caption](img/ea5e2f7906d5272f2d2311db720f045f.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/ea5e2f7906d5272f2d2311db720f045f.png)'
- en: 'Figure 10: Address occurs shared memory 4-way bank conflicts. E denotes an
    element of int32, B denotes bank, and T denotes a thread.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：地址发生了共享内存的 4 路银行冲突。E 表示 int32 的一个元素，B 表示银行，T 表示线程。
- en: '![Refer to caption](img/02a7d53d11ad3bbb088150ce6454df7a.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/02a7d53d11ad3bbb088150ce6454df7a.png)'
- en: 'Figure 11: Address remapping avoids shared memory bank conflicts. E denotes
    an element of int32, B denotes bank, and T denotes a thread.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：地址重映射避免了共享内存银行冲突。E 表示 int32 的一个元素，B 表示银行，T 表示线程。
- en: Bank Conflicts Elimination. For attaining high bandwidth, shared memory is segmented
    into memory modules of uniform size, which are referred to as banks. Each bank
    occupies 4 bytes, and the continuous 128 bytes in the shared memory form 32 banks.
    Threads within a warp can access data from various banks simultaneously in one
    single memory access request. However, if multiple threads access data within
    the same bank concurrently, it will give rise to a bank conflict, leading to lower
    throughput.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 银行冲突消除。为了获得高带宽，共享内存被分段为大小均匀的内存模块，称为银行。每个银行占用 4 字节，共享内存中的连续 128 字节形成 32 个银行。一个
    warp 内的线程可以在一次内存访问请求中同时访问来自不同银行的数据。然而，如果多个线程同时访问同一银行中的数据，就会引发银行冲突，导致吞吐量降低。
- en: 'The above Figure [10](#A4.F10 "Figure 10 ‣ Appendix D GPU Kernel Optimization
    Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
    Models") shows the scenario where bank conflict occurs (BM=8, BK=512), where E
    denotes an element of int32, B denotes bank, and T denotes a thread. When data
    is loaded from global memory to shared memory, each thread accesses 16 bytes in
    order to fully utilize the memory bandwidth. The 8x512-bit data is divided into
    four phases, each reading 128 bytes (i.e., two consecutive rows of data in the
    figure). During this process, no Bank conflicts occur in any of the phases. However,
    when data is loaded from shared memory to registers, each thread needs to copy
    4 bytes of data since the 8x8x128 BMMA stores matrix A (8x128 bits) and matrix
    B (8x128 bits) in 32-bit registers. When copying a matrix block (same color block),
    threads T0~3, T8~11, T16~19, and T24~27 all access B0~3, while T4~7, T12~15, T20~23,
    and T28~31 access B16~19, resulting in 4-way bank conflicts.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '上述图表 [10](#A4.F10 "Figure 10 ‣ Appendix D GPU Kernel Optimization Details ‣
    ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")展示了银行冲突发生的场景（BM=8，BK=512），其中E表示int32类型的元素，B表示银行，T表示线程。当数据从全局内存加载到共享内存时，每个线程访问16字节，以充分利用内存带宽。8x512位数据被分为四个阶段，每个阶段读取128字节（即图中的两行连续数据）。在此过程中，各个阶段没有发生银行冲突。然而，当数据从共享内存加载到寄存器时，每个线程需要复制4字节数据，因为8x8x128
    BMMA将矩阵A（8x128位）和矩阵B（8x128位）存储在32位寄存器中。在复制矩阵块（相同颜色块）时，线程T0~3、T8~11、T16~19和T24~27都访问B0~3，而T4~7、T12~15、T20~23和T28~31访问B16~19，导致4路银行冲突。'
- en: 'As depicted in the Figure [11](#A4.F11 "Figure 11 ‣ Appendix D GPU Kernel Optimization
    Details ‣ ABQ-LLM: Arbitrary-Bit Quantized Inference Acceleration for Large Language
    Models"), we employ the swizzle operation to address this issue. When the global
    memory is loaded into the shared memory, the addresses are swizzled, causing the
    8x128bit data to be scattered across 32 banks. When T0~31 accesses 8x128bit data,
    the data comes from B0~31 respectively, and no bank conflict will arise, thereby
    further improving the efficiency of memory access.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [11](#A4.F11 "Figure 11 ‣ Appendix D GPU Kernel Optimization Details ‣ ABQ-LLM:
    Arbitrary-Bit Quantized Inference Acceleration for Large Language Models")所示，我们采用了交换操作来解决这个问题。当全局内存被加载到共享内存时，地址被交换，使得8x128位数据分散到32个银行中。当T0~31访问8x128位数据时，数据分别来自B0~31，从而不会发生银行冲突，进一步提高了内存访问效率。'
- en: '| LLaMA-7B |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B |'
- en: '|  | 128 | 256 | 512 | 1024 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | 128 | 256 | 512 | 1024 |'
- en: '| sequence length | Latency(ms) | Memory(GB) | Latency(ms) | Memory(GB) | Latency(ms)
    | Memory(GB) | Latency(ms) | Memory(GB) |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB)
    |'
- en: '| FP 16 | 1490.5 | 13.47 | 3005.95 | 13.534 | 6090.97 | 13.662 | 12561.82 |
    13.918 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| FP 16 | 1490.5 | 13.47 | 3005.95 | 13.534 | 6090.97 | 13.662 | 12561.82 |
    13.918 |'
- en: '| W8A16(CUTLASS) | 868.35 | 7.394 | 1755.62 | 7.458 | 3594.95 | 7.586 | 7559.22
    | 7.842 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| W8A16(CUTLASS) | 868.35 | 7.394 | 1755.62 | 7.458 | 3594.95 | 7.586 | 7559.22
    | 7.842 |'
- en: '| W8A8(SmoothQuant) | 832.25 | 7.394 | 1684.85 | 7.458 | 3445.86 | 7.586 |
    7257.02 | 7.842 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| W8A8(SmoothQuant) | 832.25 | 7.394 | 1684.85 | 7.458 | 3445.86 | 7.586 |
    7257.02 | 7.842 |'
- en: '| W4A16(CUTLASS) | 642.24 | 4.258 | 1312.91 | 4.322 | 2707.26 | 4.45 | 5786.8
    | 4.706 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| W4A16(CUTLASS) | 642.24 | 4.258 | 1312.91 | 4.322 | 2707.26 | 4.45 | 5786.8
    | 4.706 |'
- en: '| W2A8(ABQ-LLM) | 505.60 | 2.784 | 1041.39 | 2.848 | 2167.30 | 2.976 | 4657.18
    | 3.232 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| W2A8(ABQ-LLM) | 505.60 | 2.784 | 1041.39 | 2.848 | 2167.30 | 2.976 | 4657.18
    | 3.232 |'
- en: '| LLaMA-13B |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B |'
- en: '|  | 128 | 256 | 512 | 1024 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | 128 | 256 | 512 | 1024 |'
- en: '| sequence length | Latency(ms) | Memory(GB) | Latency(ms) | Memory(GB) | Latency(ms)
    | Memory(GB) | Latency(ms) | Memory(GB) |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB)
    |'
- en: '| FP 16 | 2726.66 | 25.6 | 5481.96 | 25.696 | 11071.81 | 25.92 | 22559.77 |
    26.304 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| FP 16 | 2726.66 | 25.6 | 5481.96 | 25.696 | 11071.81 | 25.92 | 22559.77 |
    26.304 |'
- en: '| W8A16(CUTLASS) | 1439.66 | 13.524 | 2900.46 | 13.62 | 5922.28 | 13.844 |
    12257.27 | 14.228 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| W8A16(CUTLASS) | 1439.66 | 13.524 | 2900.46 | 13.62 | 5922.28 | 13.844 |
    12257.27 | 14.228 |'
- en: '| W8A8(SmoothQuant) | 1431.2 | 13.526 | 2874.51 | 13.622 | 5867.42 | 13.846
    | 12193.68 | 14.23 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| W8A8(SmoothQuant) | 1431.2 | 13.526 | 2874.51 | 13.622 | 5867.42 | 13.846
    | 12193.68 | 14.23 |'
- en: '| W4A16(CUTLASS) | 999.1 | 7.444 | 2020.99 | 7.54 | 4155.94 | 7.764 | 8750.98
    | 8.148 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| W4A16(CUTLASS) | 999.1 | 7.444 | 2020.99 | 7.54 | 4155.94 | 7.764 | 8750.98
    | 8.148 |'
- en: '| W2A8(ABQ-LLM) | 766.28 | 4.564 | 1550.21 | 4.66 | 3234.67 | 4.884 | 6885.69
    | 5.268 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| W2A8(ABQ-LLM) | 766.28 | 4.564 | 1550.21 | 4.66 | 3234.67 | 4.884 | 6885.69
    | 5.268 |'
- en: '| LLaMA-30B |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B |'
- en: '|  | 128 | 256 | 512 | 1024 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | 128 | 256 | 512 | 1024 |'
- en: '| sequence length | Latency(ms) | Memory(GB) | Latency(ms) | Memory(GB) | Latency(ms)
    | Memory(GB) | Latency(ms) | Memory(GB) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB) | 延迟(ms) | 内存(GB)
    |'
- en: '| FP 16 | 3759.08 | 65.534 | 7540.17 | 65.726 | 15241.36 | 66.11 | 31073.23
    | 66.878 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| FP 16 | 3759.08 | 65.534 | 7540.17 | 65.726 | 15241.36 | 66.11 | 31073.23
    | 66.878 |'
- en: '| W8A16(CUTLASS) | 3032.64 | 32.418 | 6111.43 | 32.642 | 12371.66 | 33.026
    | 25477.58 | 33.794 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| W8A16(CUTLASS) | 3032.64 | 32.418 | 6111.43 | 32.642 | 12371.66 | 33.026
    | 25477.58 | 33.794 |'
- en: '| W8A8(SmoothQuant) | 3057.96 | 32.418 | 6155.38 | 32.642 | 12465.96 | 33.026
    | 25678.66 | 33.794 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| W8A8(SmoothQuant) | 3057.96 | 32.418 | 6155.38 | 32.642 | 12465.96 | 33.026
    | 25678.66 | 33.794 |'
- en: '| W4A16(CUTLASS) | 1938.2 | 17.178 | 3924.2 | 17.402 | 8011.57 | 17.786 | 16680.64
    | 18.554 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| W4A16(CUTLASS) | 1938.2 | 17.178 | 3924.2 | 17.402 | 8011.57 | 17.786 | 16680.64
    | 18.554 |'
- en: '| W2A8(ABQ-LLM) | 1730.51 | 9.616 | 3473.66 | 9.84 | 7133.13 | 10.224 | 14845.67
    | 10.992 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| W2A8(ABQ-LLM) | 1730.51 | 9.616 | 3473.66 | 9.84 | 7133.13 | 10.224 | 14845.67
    | 10.992 |'
- en: 'Table 12: Inference latency and memory usage of the FastTransformer implementation
    on NVIDIA A800-40GB GPU with a fixed input sequence length of 15, output sequence
    lengths of 128, 256, 512 and 1024\.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：在 NVIDIA A800-40GB GPU 上的 FastTransformer 实现的推理延迟和内存使用，固定输入序列长度为 15，输出序列长度为
    128、256、512 和 1024\。
- en: '| (1,4096)x(4096,4096) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| (1,4096)x(4096,4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 5.126408 | 5.003512 | 4.993599 | 5.016534 | 1.844438 | 1.820546
    | 1.415388 | 1.409558 | 1.167617 | 0.990153 | 0.853578 | 0.760755 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 5.126408 | 5.003512 | 4.993599 | 5.016534 | 1.844438 | 1.820546
    | 1.415388 | 1.409558 | 1.167617 | 0.990153 | 0.853578 | 0.760755 |'
- en: '|  | CUTLASS(TOPS) | 1.395987 | 1.395987 | 0.671532 | 0.671532 | 1.395987 |
    0.671532 | 1.395987 | 0.671532 | 0.671532 | 0.671532 | 0.671532 | 0.671532 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 1.395987 | 1.395987 | 0.671532 | 0.671532 | 1.395987 |
    0.671532 | 1.395987 | 0.671532 | 0.671532 | 0.671532 | 0.671532 | 0.671532 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 0.662154 | 0.662154 | - | 0.662154 | - |
    0.662154 | 0.662154 | 0.662154 | 0.662154 | 0.662154 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 0.662154 | 0.662154 | - | 0.662154 | - |
    0.662154 | 0.662154 | 0.662154 | 0.662154 | 0.662154 |'
- en: '| (1,1024)x(1024,8192) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| (1,1024)x(1024,8192) |'
- en: '|  | Ours(TOPS) | 3.959401 | 3.815584 | 3.710145 | 3.636041 | 3.27549 | 3.088989
    | 2.729302 | 2.493001 | 1.107551 | 0.907098 | 0.774627 | 0.699006 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 3.959401 | 3.815584 | 3.710145 | 3.636041 | 3.27549 | 3.088989
    | 2.729302 | 2.493001 | 1.107551 | 0.907098 | 0.774627 | 0.699006 |'
- en: '|  | CUTLASS(TOPS) | 2.43737 | 2.43737 | 0.570752 | 0.570752 | 2.43737 | 0.570752
    | 2.43737 | 0.570752 | 0.570752 | 0.570752 | 0.570752 | 0.570752 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 2.43737 | 2.43737 | 0.570752 | 0.570752 | 2.43737 | 0.570752
    | 2.43737 | 0.570752 | 0.570752 | 0.570752 | 0.570752 | 0.570752 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 0.547283 | 0.547283 | - | 0.547283 | - |
    0.547283 | 0.547283 | 0.547283 | 0.547283 | 0.547283 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 0.547283 | 0.547283 | - | 0.547283 | - |
    0.547283 | 0.547283 | 0.547283 | 0.547283 | 0.547283 |'
- en: '| (1,11008)x(11008, 4096) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| (1,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 2.831276 | 2.845823 | 2.851906 | 2.85108 | 1.893524 | 1.89145
    | 1.429216 | 1.424223 | 1.063806 | 0.9578 | 0.781478 | 0.688792 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 2.831276 | 2.845823 | 2.851906 | 2.85108 | 1.893524 | 1.89145
    | 1.429216 | 1.424223 | 1.063806 | 0.9578 | 0.781478 | 0.688792 |'
- en: '|  | CUTLASS(TOPS) | 1.437404 | 1.437404 | 0.702909 | 0.702909 | 1.437404 |
    0.702909 | 1.437404 | 0.702909 | 0.702909 | 0.702909 | 0.702909 | 0.702909 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 1.437404 | 1.437404 | 0.702909 | 0.702909 | 1.437404 |
    0.702909 | 1.437404 | 0.702909 | 0.702909 | 0.702909 | 0.702909 | 0.702909 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 0.655704 | 0.655704 | - | 0.655704 | - |
    0.655704 | 0.655704 | 0.655704 | 0.655704 | 0.655704 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 0.655704 | 0.655704 | - | 0.655704 | - |
    0.655704 | 0.655704 | 0.655704 | 0.655704 | 0.655704 |'
- en: '| (1,5120)x(5120, 5120) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| (1,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 2.77896 | 2.744274 | 2.769214 | 2.72645 | 1.946023 | 1.925246
    | 1.485292 | 1.47769 | 1.207433 | 1.019555 | 0.88083 | 0.777159 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 2.77896 | 2.744274 | 2.769214 | 2.72645 | 1.946023 | 1.925246
    | 1.485292 | 1.47769 | 1.207433 | 1.019555 | 0.88083 | 0.777159 |'
- en: '|  | CUTLASS(TOPS) | 1.456822 | 1.456822 | 0.752888 | 0.752888 | 1.456822 |
    0.752888 | 1.456822 | 0.752888 | 0.752888 | 0.752888 | 0.752888 | 0.752888 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 1.456822 | 1.456822 | 0.752888 | 0.752888 | 1.456822 |
    0.752888 | 1.456822 | 0.752888 | 0.752888 | 0.752888 | 0.752888 | 0.752888 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 0.655704 | 0.655704 | - | 0.655704 | - |
    0.655704 | 0.655704 | 0.655704 | 0.655704 | 0.655704 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 0.655704 | 0.655704 | - | 0.655704 | - |
    0.655704 | 0.655704 | 0.655704 | 0.655704 | 0.655704 |'
- en: '| (1,4096)x(4096, 11008) |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| (1,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 2.93919 | 2.945875 | 2.932437 | 2.928438 | 2.048219 | 2.042158
    | 1.558931 | 1.556203 | 1.263128 | 1.06387 | 0.915512 | 0.804458 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 2.93919 | 2.945875 | 2.932437 | 2.928438 | 2.048219 | 2.042158
    | 1.558931 | 1.556203 | 1.263128 | 1.06387 | 0.915512 | 0.804458 |'
- en: '|  | CUTLASS(TOPS) | 1.44785 | 1.44785 | 0.757006 | 0.757006 | 1.44785 | 0.757006
    | 1.44785 | 0.757006 | 0.757006 | 0.757006 | 0.757006 | 0.757006 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 1.44785 | 1.44785 | 0.757006 | 0.757006 | 1.44785 | 0.757006
    | 1.44785 | 0.757006 | 0.757006 | 0.757006 | 0.757006 | 0.757006 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 0.726139 | 0.726139 | - |  | - | 0.726139
    | 0.726139 | 0.726139 | 0.726139 | 0.726139 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 0.726139 | 0.726139 | - |  | - | 0.726139
    | 0.726139 | 0.726139 | 0.726139 | 0.726139 |'
- en: '| (4,4096)x(4096,4096) |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| (4,4096)x(4096,4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 19.85037 | 18.573332 | 17.173864 | 15.596383 | 7.165928 |
    7.040589 | 5.588709 | 5.465432 | 4.427809 | 3.841075 | 3.17604 | 2.952538 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 19.85037 | 18.573332 | 17.173864 | 15.596383 | 7.165928 | 7.040589
    | 5.588709 | 5.465432 | 4.427809 | 3.841075 | 3.17604 | 2.952538 |'
- en: '|  | CUTLASS(TOPS) | 5.029238 | 5.029238 | 2.600067 | 2.600067 | 5.029238 |
    2.600067 | 5.029238 | 2.600067 | 2.600067 | 2.600067 | 2.600067 | 2.600067 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 5.029238 | 5.029238 | 2.600067 | 2.600067 | 5.029238 |
    2.600067 | 5.029238 | 2.600067 | 2.600067 | 2.600067 | 2.600067 | 2.600067 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 2.623434 | 2.623434 | - | 2.623434 | - |
    2.623434 | 2.623434 | 2.623434 | 2.623434 | 2.623434 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 2.623434 | 2.623434 | - | 2.623434 | - |
    2.623434 | 2.623434 | 2.623434 | 2.623434 | 2.623434 |'
- en: '| (4,1024)x(1024,8192) |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| (4,1024)x(1024,8192) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 14.911489 | 13.707592 | 12.752676 | 12.651737 | 11.246954
    | 10.272099 | 8.611968 | 7.994145 | 4.100613 | 3.487998 | 2.770024 | 2.715393
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 14.911489 | 13.707592 | 12.752676 | 12.651737 | 11.246954 |
    10.272099 | 8.611968 | 7.994145 | 4.100613 | 3.487998 | 2.770024 | 2.715393 |'
- en: '|  | CUTLASS(TOPS) | 7.790775 | 7.790775 | 2.292671 | 2.292671 | 2.292671 |
    2.292671 | 7.790775 | 2.292671 | 2.292671 | 2.292671 | 2.292671 | 2.292671 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 7.790775 | 7.790775 | 2.292671 | 2.292671 | 2.292671 |
    2.292671 | 7.790775 | 2.292671 | 2.292671 | 2.292671 | 2.292671 | 2.292671 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 2.652782 | 2.652782 | - | 2.652782 | - |
    2.652782 | 2.652782 | 2.652782 | 2.652782 | 2.652782 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 2.652782 | 2.652782 | - | 2.652782 | - |
    2.652782 | 2.652782 | 2.652782 | 2.652782 | 2.652782 |'
- en: '| (4,11008)x(11008, 4096) |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| (4,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 11.352832 | 11.176699 | 10.902382 | 10.897325 | 7.418883
    | 7.308671 | 5.631782 | 5.359304 | 4.201856 | 3.729234 | 2.658403 | 2.734334 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 11.352832 | 11.176699 | 10.902382 | 10.897325 | 7.418883 |
    7.308671 | 5.631782 | 5.359304 | 4.201856 | 3.729234 | 2.658403 | 2.734334 |'
- en: '|  | CUTLASS(TOPS) | 6.089864 | 6.089864 | 2.852206 | 2.852206 | 6.089864 |
    2.852206 | 6.089864 | 2.852206 | 2.852206 | 2.852206 | 2.852206 | 2.852206 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 6.089864 | 6.089864 | 2.852206 | 2.852206 | 6.089864 |
    2.852206 | 6.089864 | 2.852206 | 2.852206 | 2.852206 | 2.852206 | 2.852206 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 2.804943 | 2.804943 | - |  | - | 2.804943
    | 2.804943 | 2.804943 | 2.804943 | 2.804943 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 2.804943 | 2.804943 | - |  | - | 2.804943
    | 2.804943 | 2.804943 | 2.804943 | 2.804943 |'
- en: '| (4,5120)x(5120, 5120) |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| (4,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 11.023197 | 10.805677 | 10.686147 | 10.633989 | 7.62955 |
    7.509258 | 5.885227 | 5.81653 | 4.70967 | 4.021154 | 3.319287 | 3.049934 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 11.023197 | 10.805677 | 10.686147 | 10.633989 | 7.62955 | 7.509258
    | 5.885227 | 5.81653 | 4.70967 | 4.021154 | 3.319287 | 3.049934 |'
- en: '|  | CUTLASS(TOPS) | 5.41928 | 5.41928 | 2.892696 | 2.892696 | 5.41928 | 2.892696
    | 5.41928 | 2.892696 | 2.892696 | 2.892696 | 2.892696 | 2.892696 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS（TOPS） | 5.41928 | 5.41928 | 2.892696 | 2.892696 | 5.41928 | 2.892696
    | 5.41928 | 2.892696 | 2.892696 | 2.892696 | 2.892696 | 2.892696 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 3.024624 | 3.024624 | - | 3.024624 | - |
    3.024624 | 3.024624 | 3.024624 | 3.024624 | 3.024624 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS（TOPS） | - | - | 3.024624 | 3.024624 | - | 3.024624 | - |
    3.024624 | 3.024624 | 3.024624 | 3.024624 | 3.024624 |'
- en: '| (4,4096)x(4096, 11008) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| (4,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 11.65253 | 11.47451 | 11.370432 | 11.249153 | 8.044027 |
    7.938343 | 6.158429 | 6.063344 | 4.936044 | 4.181711 | 3.516512 | 3.152066 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们（TOPS） | 11.65253 | 11.47451 | 11.370432 | 11.249153 | 8.044027 | 7.938343
    | 6.158429 | 6.063344 | 4.936044 | 4.181711 | 3.516512 | 3.152066 |'
- en: '|  | CUTLASS(TOPS) | 5.755347 | 5.755347 | 2.913199 | 2.913199 | 5.755347 |
    2.913199 | 5.755347 | 2.913199 | 2.913199 | 2.913199 | 2.913199 | 2.913199 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 5.755347 | 5.755347 | 2.913199 | 2.913199 | 5.755347 |
    2.913199 | 5.755347 | 2.913199 | 2.913199 | 2.913199 | 2.913199 | 2.913199 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 3.150072 | 3.150072 | - | 3.150072 | - |
    3.150072 | 3.150072 | 3.150072 | 3.150072 | 3.150072 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 3.150072 | 3.150072 | - | 3.150072 | - |
    3.150072 | 3.150072 | 3.150072 | 3.150072 | 3.150072 |'
- en: '| (8,4096)x(4096,4096) |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| (8,4096)x(4096,4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 34.711864 | 29.218008 | 25.562555 | 23.108412 | 14.124731
    | 13.319459 | 10.805161 | 10.304756 | 8.570437 | 7.295355 | 6.372617 | 5.418213
    |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们(TOPS) | 34.711864 | 29.218008 | 25.562555 | 23.108412 | 14.124731 |
    13.319459 | 10.805161 | 10.304756 | 8.570437 | 7.295355 | 6.372617 | 5.418213
    |'
- en: '|  | CUTLASS(TOPS) | 10.443984 | 10.443984 | 5.328563 | 5.328563 | 10.443984
    | 5.328563 | 10.443984 | 5.328563 | 5.328563 | 5.328563 | 5.328563 | 5.328563
    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 10.443984 | 10.443984 | 5.328563 | 5.328563 | 10.443984
    | 5.328563 | 10.443984 | 5.328563 | 5.328563 | 5.328563 | 5.328563 | 5.328563
    |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.136078 | 5.136078 | - | 5.136078 | - |
    5.136078 | 5.136078 | 5.136078 | 5.136078 | 5.136078 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.136078 | 5.136078 | - | 5.136078 | - |
    5.136078 | 5.136078 | 5.136078 | 5.136078 | 5.136078 |'
- en: '| (8,1024)x(1024,8192) |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| (8,1024)x(1024,8192) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 25.143295 | 24.340204 | 22.991055 | 17.975151 | 21.315987
    | 15.096982 | 14.203727 | 10.091 | 8.056054 | 6.547737 | 5.476622 | 4.921968 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们(TOPS) | 25.143295 | 24.340204 | 22.991055 | 17.975151 | 21.315987 |
    15.096982 | 14.203727 | 10.091 | 8.056054 | 6.547737 | 5.476622 | 4.921968 |'
- en: '|  | CUTLASS(TOPS) | 11.837081 | 11.837081 | 4.52688 | 4.52688 | 11.837081
    | 4.52688 | 11.837081 | 4.52688 | 4.52688 | 4.52688 | 4.52688 | 4.52688 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 11.837081 | 11.837081 | 4.52688 | 4.52688 | 11.837081
    | 4.52688 | 11.837081 | 4.52688 | 4.52688 | 4.52688 | 4.52688 | 4.52688 |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.051333 | 5.051333 | - | 5.051333 | - |
    5.051333 | 5.051333 | 5.051333 | 5.051333 | 5.051333 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.051333 | 5.051333 | - | 5.051333 | - |
    5.051333 | 5.051333 | 5.051333 | 5.051333 | 5.051333 |'
- en: '| (8,11008)x(11008, 4096) |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| (8,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 21.93853 | 21.763004 | 21.38822 | 21.141041 | 14.794146 |
    14.292913 | 10.0646 | 9.831863 | 8.391643 | 6.331276 | 5.216482 | 5.386425 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们(TOPS) | 21.93853 | 21.763004 | 21.38822 | 21.141041 | 14.794146 | 14.292913
    | 10.0646 | 9.831863 | 8.391643 | 6.331276 | 5.216482 | 5.386425 |'
- en: '|  | CUTLASS(TOPS) | 11.559231 | 11.559231 | 5.788068 | 5.788068 | 11.559231
    | 5.788068 | 11.559231 | 5.788068 | 5.788068 | 5.788068 | 5.788068 | 5.788068
    |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 11.559231 | 11.559231 | 5.788068 | 5.788068 | 11.559231
    | 5.788068 | 11.559231 | 5.788068 | 5.788068 | 5.788068 | 5.788068 | 5.788068
    |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.433702 | 5.433702 | - | 5.433702 | - |
    5.433702 | 5.433702 | 5.433702 | 5.433702 | 5.433702 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.433702 | 5.433702 | - | 5.433702 | - |
    5.433702 | 5.433702 | 5.433702 | 5.433702 | 5.433702 |'
- en: '| (8,5120)x(5120, 5120) |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| (8,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 21.438643 | 20.998667 | 20.403486 | 20.144592 | 15.058184
    | 14.257557 | 11.512318 | 11.014898 | 9.303804 | 7.909779 | 6.563432 | 5.816199
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们(TOPS) | 21.438643 | 20.998667 | 20.403486 | 20.144592 | 15.058184 |
    14.257557 | 11.512318 | 11.014898 | 9.303804 | 7.909779 | 6.563432 | 5.816199
    |'
- en: '|  | CUTLASS(TOPS) | 11.438784 | 11.438784 | 5.650824 | 5.650824 | 11.438784
    | 5.650824 | 11.438784 | 5.650824 | 5.650824 | 5.650824 | 5.650824 | 5.650824
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 11.438784 | 11.438784 | 5.650824 | 5.650824 | 11.438784
    | 5.650824 | 11.438784 | 5.650824 | 5.650824 | 5.650824 | 5.650824 | 5.650824
    |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.843657 | 5.843657 | - | 5.843657 | - |
    5.843657 | 5.843657 | 5.843657 | 5.843657 | 5.843657 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 5.843657 | 5.843657 | - | 5.843657 | - |
    5.843657 | 5.843657 | 5.843657 | 5.843657 | 5.843657 |'
- en: '| (8,4096)x(4096, 11008) |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| (8,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 22.735727 | 22.486816 | 21.946047 | 21.290138 | 15.927655
    | 14.94954 | 12.047882 | 11.759899 | 9.831679 | 8.178876 | 6.951003 | 5.942341
    |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们(TOPS) | 22.735727 | 22.486816 | 21.946047 | 21.290138 | 15.927655 |
    14.94954 | 12.047882 | 11.759899 | 9.831679 | 8.178876 | 6.951003 | 5.942341 |'
- en: '|  | CUTLASS(TOPS) | 12.133122 | 12.133122 | 5.794113 | 5.794113 | 12.133122
    | 5.794113 | 12.133122 | 5.794113 | 5.794113 | 5.794113 | 5.794113 | 5.794113
    |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 12.133122 | 12.133122 | 5.794113 | 5.794113 | 12.133122
    | 5.794113 | 12.133122 | 5.794113 | 5.794113 | 5.794113 | 5.794113 | 5.794113
    |'
- en: '| RTX3070 | CUBLAS(TOPS) | - | - | 6.094711 | 6.094711 | - | 6.094711 | - |
    6.094711 | 6.094711 | 6.094711 | 6.094711 | 6.094711 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| RTX3070 | CUBLAS(TOPS) | - | - | 6.094711 | 6.094711 | - | 6.094711 | - |
    6.094711 | 6.094711 | 6.094711 | 6.094711 | 6.094711 |'
- en: 'Table 13: The GEMM speed comparison of our ABQKernel, CUTLASS, and cuBLAS in
    RTX 3070\.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 表13：我们在RTX 3070上的ABQKernel、CUTLASS和cuBLAS的GEMM速度比较。
- en: '| (1,4096)x(4096,4096) |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| (1,4096)x(4096,4096) |'
- en: '| --- |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 5.666263 | 5.596584 | 5.177539 | 5.192204 | 4.932711 | 5.103254
    | 5.230327 | 3.984436 | 5.089779 | 4.697921 | 2.73454 | 3.997423 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 5.666263 | 5.596584 | 5.177539 | 5.192204 | 4.932711 | 5.103254
    | 5.230327 | 3.984436 | 5.089779 | 4.697921 | 2.73454 | 3.997423 |'
- en: '|  | CUTLASS(TOPS) | 4.378407 | 4.378407 | 2.483177 | 2.483177 | 4.378407 |
    2.483177 | 4.378407 | 2.483177 | 2.483177 | 2.483177 | 2.483177 | 2.483177 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 4.378407 | 4.378407 | 2.483177 | 2.483177 | 4.378407 |
    2.483177 | 4.378407 | 2.483177 | 2.483177 | 2.483177 | 2.483177 | 2.483177 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.132917 | 2.132917 | - | 2.132917 | - |
    2.132917 | 2.132917 | 2.132917 | 2.132917 | 2.132917 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.132917 | 2.132917 | - | 2.132917 | - |
    2.132917 | 2.132917 | 2.132917 | 2.132917 | 2.132917 |'
- en: '| (1,1024)x(1024,8192) |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| (1,1024)x(1024,8192) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 3.544786 | 3.494135 | 3.491157 | 3.335505 | 3.090738 | 3.463115
    | 3.280737 | 2.427181 | 2.593226 | 2.545285 | 2.018674 | 2.512498 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 3.544786 | 3.494135 | 3.491157 | 3.335505 | 3.090738 | 3.463115
    | 3.280737 | 2.427181 | 2.593226 | 2.545285 | 2.018674 | 2.512498 |'
- en: '|  | CUTLASS(TOPS) | 1.994158 | 1.994158 | 1.595792 | 1.595792 | 1.994158 |
    1.595792 | 1.994158 | 1.595792 | 1.595792 | 1.595792 | 1.595792 | 1.595792 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 1.994158 | 1.994158 | 1.595792 | 1.595792 | 1.994158 |
    1.595792 | 1.994158 | 1.595792 | 1.595792 | 1.595792 | 1.595792 | 1.595792 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 1.842555 | 1.842555 | - | 1.842555 | - |
    1.842555 | 1.842555 | 1.842555 | 1.842555 | 1.842555 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 1.842555 | 1.842555 | - | 1.842555 | - |
    1.842555 | 1.842555 | 1.842555 | 1.842555 | 1.842555 |'
- en: '| (1,11008)x(11008, 4096) |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| (1,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 11.459206 | 11.172798 | 8.219525 | 8.073364 | 6.89011 | 8.299166
    | 7.945147 | 5.589236 | 6.926538 | 6.253657 | 3.5678 | 5.010184 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 11.459206 | 11.172798 | 8.219525 | 8.073364 | 6.89011 | 8.299166
    | 7.945147 | 5.589236 | 6.926538 | 6.253657 | 3.5678 | 5.010184 |'
- en: '|  | CUTLASS(TOPS) | 5.738564 | 5.738564 | 3.577838 | 3.577838 | 5.738564 |
    3.577838 | 5.738564 | 3.577838 | 3.577838 | 3.577838 | 3.577838 | 3.577838 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 5.738564 | 5.738564 | 3.577838 | 3.577838 | 5.738564 |
    3.577838 | 5.738564 | 3.577838 | 3.577838 | 3.577838 | 3.577838 | 3.577838 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.782205 | 2.782205 | - | 2.782205 | - |
    2.782205 | 2.782205 | 2.782205 | 2.782205 | 2.782205 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.782205 | 2.782205 | - | 2.782205 | - |
    2.782205 | 2.782205 | 2.782205 | 2.782205 | 2.782205 |'
- en: '| (1,5120)x(5120, 5120) |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| (1,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 8.179976 | 8.036415 | 7.981294 | 7.326846 | 7.348096 | 7.640651
    | 7.2295 | 4.169042 | 6.045578 | 4.925341 | 3.0402 | 4.270938 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 8.179976 | 8.036415 | 7.981294 | 7.326846 | 7.348096 | 7.640651
    | 7.2295 | 4.169042 | 6.045578 | 4.925341 | 3.0402 | 4.270938 |'
- en: '|  | CUTLASS(TOPS) | 4.469664 | 4.469664 | 2.111863 | 2.111863 | 4.469664 |
    2.111863 | 4.469664 | 2.111863 | 2.111863 | 2.111863 | 2.111863 | 2.111863 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 4.469664 | 4.469664 | 2.111863 | 2.111863 | 4.469664 |
    2.111863 | 4.469664 | 2.111863 | 2.111863 | 2.111863 | 2.111863 | 2.111863 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.338113 | 2.338113 | - |  | - | 2.338113
    | 2.338113 | 2.338113 | 2.338113 | 2.338113 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.338113 | 2.338113 | - |  | - | 2.338113
    | 2.338113 | 2.338113 | 2.338113 | 2.338113 |'
- en: '| (1,4096)x(4096, 11008) |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| (1,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 13.542058 | 13.110614 | 12.246419 | 9.048911 | 7.191247 |
    10.480065 | 7.804325 | 4.767432 | 6.815571 | 5.946993 | 3.366875 | 5.155972 |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 13.542058 | 13.110614 | 12.246419 | 9.048911 | 7.191247 |
    10.480065 | 7.804325 | 4.767432 | 6.815571 | 5.946993 | 3.366875 | 5.155972 |'
- en: '|  | CUTLASS(TOPS) | 7.373691 | 7.373691 | 3.04093 | 3.04093 | 7.373691 | 3.04093
    | 7.373691 | 3.04093 | 3.04093 | 3.04093 | 3.04093 | 3.04093 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 7.373691 | 7.373691 | 3.04093 | 3.04093 | 7.373691 | 3.04093
    | 7.373691 | 3.04093 | 3.04093 | 3.04093 | 3.04093 | 3.04093 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.862102 | 2.862102 | - | 2.862102 | - |
    2.862102 | 2.862102 | 2.862102 | 2.862102 | 2.862102 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 2.862102 | 2.862102 | - | 2.862102 | - |
    2.862102 | 2.862102 | 2.862102 | 2.862102 | 2.862102 |'
- en: '| (4,4096)x(4096,4096) |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| (4,4096)x(4096,4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 24.408194 | 20.693401 | 20.195993 | 20.795177 | 18.665907
    | 20.496012 | 19.760592 | 12.737339 | 18.525913 | 16.639837 | 8.414879 | 11.580844
    |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 24.408194 | 20.693401 | 20.195993 | 20.795177 | 18.665907
    | 20.496012 | 19.760592 | 12.737339 | 18.525913 | 16.639837 | 8.414879 | 11.580844
    |'
- en: '|  | CUTLASS(TOPS) | 16.965191 | 16.965191 | 11.346475 | 11.346475 | 16.965191
    | 11.346475 | 16.965191 | 11.346475 | 11.346475 | 11.346475 | 11.346475 | 11.346475
    |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 16.965191 | 16.965191 | 11.346475 | 11.346475 | 16.965191
    | 11.346475 | 16.965191 | 11.346475 | 11.346475 | 11.346475 | 11.346475 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 9.346264 | 9.346264 | - | 9.346264 | - |
    9.346264 | 9.346264 | 9.346264 | 9.346264 | 9.346264 |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 9.346264 | 9.346264 | - | 9.346264 | - |
    9.346264 | 9.346264 | 9.346264 | 9.346264 | 9.346264 |'
- en: '| (4,1024)x(1024,8192) |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| (4,1024)x(1024,8192) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 14.124137 | 14.265563 | 14.515171 | 13.744966 | 13.02943
    | 12.581451 | 11.312963 | 9.696109 | 10.008553 | 10.04691 | 5.635082 | 8.187905
    |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 14.124137 | 14.265563 | 14.515171 | 13.744966 | 13.02943
    | 12.581451 | 11.312963 | 9.696109 | 10.008553 | 10.04691 | 5.635082 | 8.187905
    |'
- en: '|  | CUTLASS(TOPS) | 10.125006 | 10.125006 | 7.514869 | 7.514869 | 10.125006
    | 7.514869 | 10.125006 | 7.514869 | 7.514869 | 7.514869 | 7.514869 | 7.514869
    |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 10.125006 | 10.125006 | 7.514869 | 7.514869 | 10.125006
    | 7.514869 | 10.125006 | 7.514869 | 7.514869 | 7.514869 | 7.514869 | 7.514869
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 5.807355 | 5.807355 | - | 5.807355 | - |
    5.807355 | 5.807355 | 5.807355 | 5.807355 | 5.807355 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 5.807355 | 5.807355 | - | 5.807355 | - |
    5.807355 | 5.807355 | 5.807355 | 5.807355 | 5.807355 |'
- en: '| (4,11008)x(11008, 4096) |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| (4,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 44.499241 | 34.592556 | 31.936174 | 30.138264 | 26.151152
    | 29.124102 | 30.114992 | 20.55409 | 25.744062 | 21.976168 | 11.041816 | 16.608015
    |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 44.499241 | 34.592556 | 31.936174 | 30.138264 | 26.151152
    | 29.124102 | 30.114992 | 20.55409 | 25.744062 | 21.976168 | 11.041816 | 16.608015
    |'
- en: '|  | CUTLASS(TOPS) | 22.360314 | 22.360314 | 14.27062 | 14.27062 | 22.360314
    | 14.27062 | 22.360314 | 14.27062 | 14.27062 | 14.27062 | 14.27062 | 14.27062
    |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 22.360314 | 22.360314 | 14.27062 | 14.27062 | 22.360314
    | 14.27062 | 22.360314 | 14.27062 | 14.27062 | 14.27062 | 14.27062 | 14.27062
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 10.484434 | 10.484434 | - | 10.484434 |
    - | 10.484434 | 10.484434 | 10.484434 | 10.484434 | 10.484434 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 10.484434 | 10.484434 | - | 10.484434 |
    - | 10.484434 | 10.484434 | 10.484434 | 10.484434 | 10.484434 |'
- en: '|  |  | (4,5120)x(5120, 5120) |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (4,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 32.262131 | 32.364094 | 31.850698 | 26.226151 | 20.416708
    | 24.156643 | 26.690235 | 14.489883 | 18.306963 | 18.400719 | 8.766748 | 13.434794
    |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 32.262131 | 32.364094 | 31.850698 | 26.226151 | 20.416708
    | 24.156643 | 26.690235 | 14.489883 | 18.306963 | 18.400719 | 8.766748 | 13.434794
    |'
- en: '|  | CUTLASS(TOPS) | 15.777285 | 15.777285 | 8.469383 | 8.469383 | 15.777285
    | 8.469383 | 15.777285 | 8.469383 | 8.469383 | 8.469383 | 8.469383 | 8.469383
    |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 15.777285 | 15.777285 | 8.469383 | 8.469383 | 15.777285
    | 8.469383 | 15.777285 | 8.469383 | 8.469383 | 8.469383 | 8.469383 | 8.469383
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 6.160892 | 6.160892 | - | 6.160892 | - |
    6.160892 | 6.160892 | 6.160892 | 6.160892 | 6.160892 |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 6.160892 | 6.160892 | - | 6.160892 | - |
    6.160892 | 6.160892 | 6.160892 | 6.160892 | 6.160892 |'
- en: '| (4,4096)x(4096, 11008) |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| (4,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 52.217018 | 50.157482 | 45.968418 | 31.412338 | 28.61311
    | 30.551258 | 30.689667 | 17.375622 | 22.56772 | 21.429615 | 10.303197 | 16.78546
    |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 52.217018 | 50.157482 | 45.968418 | 31.412338 | 28.61311
    | 30.551258 | 30.689667 | 17.375622 | 22.56772 | 21.429615 | 10.303197 | 16.78546
    |'
- en: '|  | CUTLASS(TOPS) | 29.325113 | 29.325113 | 12.020953 | 12.020953 | 29.325113
    | 12.020953 | 29.325113 | 12.020953 | 12.020953 | 12.020953 | 12.020953 | 12.020953
    |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 29.325113 | 29.325113 | 12.020953 | 12.020953 | 29.325113
    | 12.020953 | 29.325113 | 12.020953 | 12.020953 | 12.020953 | 12.020953 |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 9.30319 | 9.30319 | - | 9.30319 | - | 9.30319
    | 9.30319 | 9.30319 | 9.30319 | 9.30319 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 9.30319 | 9.30319 | - | 9.30319 | - | 9.30319
    | 9.30319 | 9.30319 | 9.30319 | 9.30319 |'
- en: '| (8,4096)x(4096,4096) |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| (8,4096)x(4096,4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 41.439137 | 41.30854 | 41.419498 | 39.114296 | 37.503555
    | 37.120361 | 40.611 | 24.405922 | 32.121555 | 24.77029 | 16.262033 | 17.331835
    |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 41.439137 | 41.30854 | 41.419498 | 39.114296 | 37.503555
    | 37.120361 | 40.611 | 24.405922 | 32.121555 | 24.77029 | 16.262033 | 17.331835
    |'
- en: '|  | CUTLASS(TOPS) | 33.269646 | 33.269646 | 22.390638 | 22.390638 | 33.269646
    | 22.390638 | 33.269646 | 22.390638 | 22.390638 | 22.390638 | 22.390638 | 22.390638
    |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 33.269646 | 33.269646 | 22.390638 | 22.390638 | 33.269646
    | 22.390638 | 33.269646 | 22.390638 | 22.390638 | 22.390638 | 22.390638 | 22.390638
    |'
- en: '| RTX4080 |  | - | - | 18.537868 | 18.537868 | - | 18.537868 | - | 18.537868
    | 18.537868 | 18.537868 | 18.537868 | 18.537868 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 |  | - | - | 18.537868 | 18.537868 | - | 18.537868 | - | 18.537868
    | 18.537868 | 18.537868 | 18.537868 | 18.537868 |'
- en: '|  |  | (8,1024)x(1024,8192) |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|  |  | (8,1024)x(1024,8192) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 28.155737 | 27.346548 | 26.414156 | 20.811686 | 20.343021
    | 20.249033 | 20.602325 | 18.68187 | 19.067791 | 18.388329 | 11.315893 | 11.722744
    |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 28.155737 | 27.346548 | 26.414156 | 20.811686 | 20.343021
    | 20.249033 | 20.602325 | 18.68187 | 19.067791 | 18.388329 | 11.315893 | 11.722744
    |'
- en: '|  | CUTLASS(TOPS) | 16.941822 | 16.941822 | 11.685739 | 11.685739 | 16.941822
    | 11.685739 | 16.941822 | 11.685739 | 11.685739 | 11.685739 | 11.685739 | 11.685739
    |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 16.941822 | 16.941822 | 11.685739 | 11.685739 | 16.941822
    | 11.685739 | 16.941822 | 11.685739 | 11.685739 | 11.685739 | 11.685739 | 11.685739
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 11.626171 | 11.626171 | - | 11.626171 |
    - | 11.626171 | 11.626171 | 11.626171 | 11.626171 | 11.626171 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 11.626171 | 11.626171 | - | 11.626171 |
    - | 11.626171 | 11.626171 | 11.626171 | 11.626171 | 11.626171 |'
- en: '| (8,11008)x(11008, 4096) |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| (8,11008)x(11008, 4096) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 70.776772 | 63.515327 | 62.89724 | 53.603973 | 50.18964 |
    44.507675 | 52.819912 | 35.874935 | 41.623066 | 34.309536 | 20.632343 | 20.644436
    |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 70.776772 | 63.515327 | 62.89724 | 53.603973 | 50.18964 |
    44.507675 | 52.819912 | 35.874935 | 41.623066 | 34.309536 | 20.632343 | 20.644436
    |'
- en: '|  | CUTLASS(TOPS) | 42.806332 | 42.806332 | 28.412611 | 28.412611 | 42.806332
    | 28.412611 | 42.806332 | 28.412611 | 28.412611 | 28.412611 | 28.412611 | 28.412611
    |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 42.806332 | 42.806332 | 28.412611 | 28.412611 | 42.806332
    | 28.412611 | 42.806332 | 28.412611 | 28.412611 | 28.412611 | 28.412611 | 28.412611
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 20.376932 | 20.376932 | - | 20.376932 |
    - | 20.376932 | 20.376932 | 20.376932 | 20.376932 | 20.376932 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 20.376932 | 20.376932 | - | 20.376932 |
    - | 20.376932 | 20.376932 | 20.376932 | 20.376932 | 20.376932 |'
- en: '| (8,5120)x(5120, 5120) |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| (8,5120)x(5120, 5120) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 62.192532 | 58.058117 | 56.991051 | 38.496239 | 36.794827
    | 36.71896 | 41.916618 | 27.445724 | 35.866901 | 26.239592 | 16.931917 | 20.035217
    |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 62.192532 | 58.058117 | 56.991051 | 38.496239 | 36.794827
    | 36.71896 | 41.916618 | 27.445724 | 35.866901 | 26.239592 | 16.931917 | 20.035217
    |'
- en: '|  | CUTLASS(TOPS) | 35.998506 | 35.998506 | 16.818377 | 16.818377 | 35.998506
    | 16.818377 | 35.998506 | 16.818377 | 16.818377 | 16.818377 | 16.818377 | 16.818377
    |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 35.998506 | 35.998506 | 16.818377 | 16.818377 | 35.998506
    | 16.818377 | 35.998506 | 16.818377 | 16.818377 | 16.818377 | 16.818377 | 16.818377
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 11.887421 | 11.887421 | - | 11.887421 |
    - | 11.887421 | 11.887421 | 11.887421 | 11.887421 | 11.887421 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 11.887421 | 11.887421 | - | 11.887421 |
    - | 11.887421 | 11.887421 | 11.887421 | 11.887421 | 11.887421 |'
- en: '| (8,4096)x(4096, 11008) |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| (8,4096)x(4096, 11008) |'
- en: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  |  | w2a2 | w2a4 | w2a6 | w2a8 | w3a3 | w3a8 | w4a4 | w4a8 | w5a5 | w6a6
    | w7a7 | w8a8 |'
- en: '|  | Ours(TOPS) | 103.437386 | 66.689888 | 63.139633 | 49.792355 | 51.081207
    | 50.856277 | 52.395653 | 33.181564 | 43.949936 | 34.850956 | 19.520435 | 21.211935
    |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '|  | Ours(TOPS) | 103.437386 | 66.689888 | 63.139633 | 49.792355 | 51.081207
    | 50.856277 | 52.395653 | 33.181564 | 43.949936 | 34.850956 | 19.520435 | 21.211935
    |'
- en: '|  | CUTLASS(TOPS) | 55.537205 | 55.537205 | 24.016449 | 24.016449 | 55.537205
    | 24.016449 | 55.537205 | 24.016449 | 24.016449 | 24.016449 | 24.016449 | 24.016449
    |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  | CUTLASS(TOPS) | 55.537205 | 55.537205 | 24.016449 | 24.016449 | 55.537205
    | 24.016449 | 55.537205 | 24.016449 | 24.016449 | 24.016449 | 24.016449 | 24.016449
    |'
- en: '| RTX4080 | CUBLAS(TOPS) | - | - | 18.105219 | 18.105219 | - |  | - | 18.105219
    | 18.105219 | 18.105219 | 18.105219 | 18.105219 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | CUBLAS(TOPS) | - | - | 18.105219 | 18.105219 | - |  | - | 18.105219
    | 18.105219 | 18.105219 | 18.105219 | 18.105219 |'
- en: 'Table 14: The GEMM speed comparison of our ABQKernel, CUTLASS, and cuBLAS in
    RTX 4080\.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：我们 ABQKernel、CUTLASS 和 cuBLAS 在 RTX 4080 中的 GEMM 速度比较。
