- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:14'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:14
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through
    Sparsity'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 垃圾 DNA 假说：通过稀疏性分析 LLM 预训练权重的任务导向角度
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.02277](https://ar5iv.labs.arxiv.org/html/2310.02277)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.02277](https://ar5iv.labs.arxiv.org/html/2310.02277)
- en: 'Lu Yin²¹¹footnotemark: 1,   Shiwei Liu^(1,2),   Ajay Jaiswal¹¹¹footnotemark:
    1,   Souvik Kundu³,   Zhangyang Wang¹'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Lu Yin²¹¹脚注标记：1,   Shiwei Liu^(1,2),   Ajay Jaiswal¹¹¹脚注标记：1,   Souvik Kundu³,
      Zhangyang Wang¹
- en: ¹University of Texas at Austin, ²Eindhoven University of Technology, ³Intel
    Labs Equal contribution.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹德克萨斯大学奥斯汀分校，²埃因霍温理工大学，³英特尔实验室 贡献相等。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The traditional notion of "Junk DNA" has long been linked to non-coding segments
    within the human genome, constituting roughly 98% of its composition. Initially
    perceived as biologically inert, recent research has unveiled the critical roles
    some of these seemingly non-functional DNA sequences play in cellular processes.
    Intriguingly, the weights within deep neural networks exhibit a remarkable similarity
    to the redundancy observed in human genes. It was believed that weights in gigantic
    models contained excessive redundancy, leading to the conception that a significant
    number of parameters could be removed without compromising performance.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的“垃圾 DNA”观念长期以来与人类基因组中的非编码片段相关联，这些片段大约占其组成的98%。最初被认为是生物学上无效的，最近的研究揭示了这些看似无功能的
    DNA 序列在细胞过程中的关键作用。令人感兴趣的是，深度神经网络中的权重与人类基因中观察到的冗余表现出显著的相似性。人们曾认为，在庞大的模型中，权重存在过多的冗余，因此设想可以在不影响性能的情况下删除大量参数。
- en: 'This paper challenges this conventional wisdom by presenting a compelling counter-argument.
    We employ sparsity (specifically weight pruning) as a tool to isolate and quantify
    the nuanced significance of low-magnitude weights in pre-trained large language
    models (LLMs). Our study demonstrates a strong correlation between these weight
    magnitudes and the knowledge they encapsulate, from a downstream task-centric
    angle. Drawing parallels with biological insights, we raise the “Junk DNA Hypothesis”
    backed by our in-depth investigation: while small-magnitude weights may appear
    “useless" for simple tasks and suitable for pruning, they actually encode crucial
    knowledge necessary for solving more difficult downstream tasks. Removing these
    seemingly insignificant weights can lead to irreversible knowledge forgetting
    and performance damage in difficult tasks.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过提出有力的反驳，挑战了这一传统观念。我们采用稀疏性（特别是权重剪枝）作为工具，以隔离和量化预训练的大型语言模型（LLMs）中低幅度权重的细微意义。我们的研究展示了这些权重幅度与它们所包含的知识之间的强相关性，从任务导向的角度来看。借鉴生物学的见解，我们提出了基于深入调查的“垃圾
    DNA 假说”：虽然小幅度权重在简单任务中可能看似“无用”，适合于剪枝，但它们实际上编码了解决更困难下游任务所必需的关键知识。移除这些看似微不足道的权重可能会导致不可逆的知识遗忘和在困难任务中的性能损害。
- en: 'To study it formally, we introduce several quantifiable metrics for gauging
    downstream task difficulty: (i) within the same task category, we vary the adequacy
    of target domain data (e.g., few-shot fine-tuning) and extend this to multi-domain
    learning (e.g., majority versus minority language in multilingual translation).
    Additionally, we assess the availability of external information (e.g., open-book
    versus close-book QA); (ii) across diverse task categories, we utilize the normalized
    performance gap between humans and models as an indicator of LLM-facing task complexity.
    Our extensive experiments validate the Junk DNA Hypothesis across a spectrum of
    model scales, tasks, and datasets, employing both forms of sparsity - unstructured
    and structured (N:M). We further empirically confirm that the essential knowledge
    indeed resides within the pre-trained weights, and the performance drop does not
    stem from constrained model capacity post-pruning. These findings offer fresh
    insights into how LLMs encode knowledge in a task-sensitive manner, pave future
    research direction in model pruning, and open avenues for task-aware conditional
    computation during inference. Codes are available at [https://github.com/VITA-Group/Junk_DNA_Hypothesis.git](https://github.com/VITA-Group/Junk_DNA_Hypothesis.git).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正式研究，我们引入了几个量化指标来衡量下游任务的难度：（i）在相同任务类别中，我们改变目标领域数据的充分性（例如，少量样本微调），并将其扩展到多领域学习（例如，多语种翻译中的多数语言与少数语言）。此外，我们评估外部信息的可用性（例如，开放书籍与封闭书籍问答）；（ii）在不同任务类别之间，我们利用人类与模型之间的标准化表现差距作为LLM面临任务复杂性的指标。我们的广泛实验验证了垃圾DNA假设在各种模型规模、任务和数据集中的有效性，采用了两种稀疏形式——非结构化和结构化（N:M）。我们进一步实验证实，必要的知识确实存在于预训练权重中，性能下降并不是由于剪枝后模型容量受限。这些发现为LLMs如何以任务敏感的方式编码知识提供了新的见解，为模型剪枝的未来研究方向铺平了道路，并为推理过程中任务感知的条件计算开辟了新途径。代码可在
    [https://github.com/VITA-Group/Junk_DNA_Hypothesis.git](https://github.com/VITA-Group/Junk_DNA_Hypothesis.git)
    获得。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The human genome, an astonishing compilation of three billion DNA base pairs,
    reveals a fascinating dichotomy. Approximately 2% of this vast genetic landscape
    encodes proteins, leaving the remaining portion seemingly superfluous (Carey,
    [2015](#bib.bib3)). This non-coding section of the genome has earned the moniker
    “Junk DNA” (Ohno, [1972](#bib.bib43)), positing that large genomes would inevitably
    harbor non-coding sequences, passively accumulated over millennia, devoid of any
    protein-coding capacity. Yet over the past decade, this notion was challenged
    it has become evident that at least some of these seemingly extraneous DNAs play
    essential roles in cellular function. For example, these regions of DNA contain
    vital sequences that act as regulatory elements (Zheng et al., [2010](#bib.bib59)).
    Specialized proteins known as transcription factors bind to these elements, orchestrating
    the intricate dance of gene transcription, either activating or repressing it.
    This revelation underscores the intricate and functional complexity of what was
    once considered “Junk DNA”, revealing previously hidden layers of genetic regulation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类基因组，一个令人惊叹的三十亿DNA碱基对的汇编，揭示了一个引人入胜的二分法。大约2%的这一庞大的遗传领域编码蛋白质，其余部分看似多余（Carey,
    [2015](#bib.bib3)）。这部分非编码的基因组被称为“垃圾DNA”（Ohno, [1972](#bib.bib43)），假设大型基因组不可避免地包含非编码序列，这些序列在数千年间被被动积累，不具备任何编码蛋白质的能力。然而，在过去的十年中，这一观点受到了挑战，现在已经明显表明，至少一些这些看似多余的DNA在细胞功能中发挥着重要作用。例如，这些DNA区域包含作为调控元素的关键序列（Zheng
    et al., [2010](#bib.bib59)）。被称为转录因子的特定蛋白质与这些元素结合，协调基因转录的复杂舞蹈，既可以激活也可以抑制。这一发现突显了曾经被认为是“垃圾DNA”的遗传材料的复杂性和功能性，揭示了之前隐藏的遗传调控层次。
- en: 'In a parallel vein, a prevailing belief in the realm of deep neural networks
    bears resemblance to the notion of Junk DNA in biology. This belief suggests that
    a substantial portion of parameters, particularly those with low magnitude, within
    deep neural networks lack significance and can be vastly pruned across various
    architectures and applications (Han et al., [2016](#bib.bib16); Gale et al., [2019](#bib.bib15);
    Frankle & Carbin, [2019](#bib.bib11); Mocanu et al., [2018](#bib.bib37); Liu et al.,
    [2022b](#bib.bib32); Chen et al., [2020](#bib.bib4)). Moreover, as model sizes
    continue to expand, the volume of redundant parameters is poised to escalate  (Liu
    et al., [2022a](#bib.bib31)). This principle extends its effectiveness even to
    billion-level Large Language Models (LLMs)  (Jaiswal et al., [2023a](#bib.bib19);
    Frantar & Alistarh, [2023](#bib.bib13); Sun et al., [2023](#bib.bib48)). With
    the negligible loss in performance stemming from the absence of low-magnitude
    weights, a widely held belief has taken root: these small-magnitude weights are
    essentially superfluous components that make scant contribution to the model’s
    functionality. At this juncture, it behooves us to pause and pose a question:
    Could there be crucial facets overlooked in the context of whether these low-magnitude
    weights are truly inconsequential artifacts for large-scale models?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相似，在深度神经网络领域存在一种流行的观念，与生物学中的垃圾DNA概念相似。这一观念认为，深度神经网络中大量参数，特别是那些低幅度的参数，缺乏重要性，并且在各种架构和应用中可以大幅度剪枝
    (Han et al., [2016](#bib.bib16); Gale et al., [2019](#bib.bib15); Frankle & Carbin,
    [2019](#bib.bib11); Mocanu et al., [2018](#bib.bib37); Liu et al., [2022b](#bib.bib32);
    Chen et al., [2020](#bib.bib4))。此外，随着模型规模的不断扩大，冗余参数的数量也将增加 (Liu et al., [2022a](#bib.bib31))。这一原则甚至在十亿级的大型语言模型
    (LLMs) 中也有效 (Jaiswal et al., [2023a](#bib.bib19); Frantar & Alistarh, [2023](#bib.bib13);
    Sun et al., [2023](#bib.bib48))。由于低幅度权重的缺失几乎不会导致性能损失，因此普遍认为这些小幅度权重本质上是多余的，对模型功能贡献甚微。在这一点上，我们需要停下来问一个问题：在这些低幅度权重是否真的是大规模模型的无关紧要的附属物的背景下，是否有关键方面被忽略了？
- en: 'This paper addresses the aforementioned query by employing (mainly) magnitude-based
    pruning to discern and quantify the subtle importance of low-magnitude weights.
    Magnitude-based pruning, a well-established method known for consistently producing
    robust results, has been extensively utilized (Han et al., [2016](#bib.bib16);
    Frankle & Carbin, [2019](#bib.bib11); Liu et al., [2022b](#bib.bib32); Fernandez-Lopez
    et al., [2023](#bib.bib10); Ma et al., [2023](#bib.bib36)). Yet, it is imperative
    to clarify that this paper does not aim to be another LLM pruning exposition.
    Instead, we see pruning as a quantitative and easily controllable tool for probing
    and comprehending the role of small-magnitude weights in pre-trained LLMs. The
    crux of our research lies in adopting a novel task-centric viewpoint towards pre-trained
    weights. In other words, these weight magnitudes and the information they embody
    exhibit a significant correlation with the complexity of the downstream task for
    which the pre-trained LLM will be employed. Our study disrupts conventional assumptions
    by providing a counterpoint regarding the previously disregarded yet pivotal function
    of small-magnitude weights. Drawing a parallel with biological insights, we articulate
    our discoveries as the Junk DNA Hypothesis, stated as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过采用（主要是）基于幅度的剪枝来解决上述问题，从而识别和量化低幅度权重的微妙重要性。基于幅度的剪枝是一种被广泛使用的成熟方法，因其能够始终如一地产生强健的结果
    (Han et al., [2016](#bib.bib16); Frankle & Carbin, [2019](#bib.bib11); Liu et
    al., [2022b](#bib.bib32); Fernandez-Lopez et al., [2023](#bib.bib10); Ma et al.,
    [2023](#bib.bib36))。然而，需要明确的是，本文并不旨在成为另一个LLM剪枝的阐述。相反，我们将剪枝视为一个定量且易于控制的工具，用于探究和理解小幅度权重在预训练LLMs中的作用。我们研究的核心在于采用一种新的任务中心视角来审视预训练权重。换句话说，这些权重的幅度及其所包含的信息与预训练LLM将要应用的下游任务的复杂性具有显著相关性。我们的研究通过提供一个关于小幅度权重的重要作用的对立观点，颠覆了传统的假设。借用生物学的见解，我们将我们的发现阐述为垃圾DNA假说，如下所述：
- en: 'Junk
    DNA Hypothesis (pertaining to LLMs): While small-magnitude weights might seem
    nearly superfluous for simple downstream tasks and thus candidates for pruning,
    they actually encode vital knowledge essential for tackling more challenging downstream
    tasks. Removing these ostensibly inconsequential weights can result in irreparable
    loss of knowledge and performance degradation in difficult tasks.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'Junk
    DNA Hypothesis (pertaining to LLMs): While small-magnitude weights might seem
    nearly superfluous for simple downstream tasks and thus candidates for pruning,
    they actually encode vital knowledge essential for tackling more challenging downstream
    tasks. Removing these ostensibly inconsequential weights can result in irreparable
    loss of knowledge and performance degradation in difficult tasks.'
- en: 'The primary challenge in formalizing this conjecture lies in providing a precise,
    controllable definition of “task difficulty". We undertake this exploration through
    the following avenues:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 形式化这一猜想的主要挑战在于提供一个精确、可控的“任务难度”定义。我们通过以下途径来展开这一探索：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Within the Same Task Category: we first vary the adequacy of target domain
    data (Liu et al., [2019](#bib.bib34)) (e.g., few-shot fine-tuning), and we then
    extend this idea to multi-domain learning (e.g., majority versus minority language
    in multilingual translation (Liu et al., [2020](#bib.bib35))). Additionally, we
    investigate the influence of external information availability, as exemplified
    by open-book versus closed-book QA (Ram et al., [2023](#bib.bib44)).'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在同一任务类别内：我们首先改变目标领域数据的充足性（Liu et al., [2019](#bib.bib34)）（例如，少样本微调），然后将这一想法扩展到多领域学习（例如，多语言翻译中的主要语言与少数语言（Liu
    et al., [2020](#bib.bib35)））。此外，我们还研究了外部信息可用性的影响，例如开放书籍与闭卷问答（Ram et al., [2023](#bib.bib44)）。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Across Diverse Task Categories: for each task, we utilize the gap between the
    best human performance, and the target LLM model’s performance on the task (normalized
    by human performance), as an indicator of task complexity “sensed" by that LLM.
    Such complexities can be compared across tasks for the same LLM.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨多样任务类别：对于每个任务，我们利用最佳人类表现与目标LLM模型在该任务上的表现之间的差距（按人类表现标准化），作为该LLM“感知”的任务复杂性指标。这些复杂性可以在相同LLM的任务间进行比较。
- en: 'Our extensive experiments substantiate the Junk DNA Hypothesis across a diverse
    range of model sizes, tasks, and datasets, employing both unstructured and structured
    sparsity (N:M). While the overarching notion that "more challenging downstream
    tasks permit less room for pruning" may not come as a surprise, our study unveils
    several subtler, often unexpected findings:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大量实验证实了垃圾DNA假说在不同模型规模、任务和数据集中的普遍适用性，采用了非结构化和结构化稀疏性（N:M）。虽然“更具挑战性的下游任务允许的修剪空间更小”的总体观念可能不令人惊讶，但我们的研究揭示了几个更微妙、往往出乎意料的发现：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Moving beyond the nebulous distinction between simple and complex tasks, the
    various conceptions of task difficulty defined above by us, both within and across
    tasks, appear to align closely with the behavior of pruning fragility. This suggests
    practical methods for estimating the task-dependent achievable degree of LLM sparsity.
    In certain tasks, even a modest reduction in low-magnitude pre-trained weights
    (e.g., 10%) results in a significant drop in accuracy, underscoring their pivotal
    role in effectively handling more intricate tasks.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 超越简单和复杂任务之间模糊的区别，我们定义的各种任务难度，无论是在任务内部还是跨任务之间，都与修剪脆弱性的行为紧密相关。这表明了估计任务依赖的LLM稀疏性可达程度的实际方法。在某些任务中，即使是对低幅度预训练权重的适度减少（例如，10%），也会导致准确率显著下降，突显了它们在有效处理更复杂任务中的关键作用。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We confirm that for difficult tasks, the essential knowledge indeed resides
    within the pre-trained weight values. Our carefully designed experiments (e.g.,
    dense versus sparse fine-tuning after pruning pre-trained weights) demonstrate
    that the decline in performance does not originate from limited model capacity
    post-pruning (e.g., see Figure [6](#S5.F6 "Figure 6 ‣ 5 Are Pre-trained Magnitude
    Values Indeed the True Gem? ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM
    Pre-trained Weights through Sparsity")). Conversely, if we freeze all pre-trained
    weights and only update as few as 10% of the largest-magnitude ones, we can often
    match the performance of fully fine-tuning dense models.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们确认，对于困难任务，确实是预训练权重值中包含了核心知识。我们精心设计的实验（例如，修剪预训练权重后的稠密与稀疏微调）表明，性能下降并不是源于修剪后的模型容量有限（例如，见图[6](#S5.F6
    "图6 ‣ 5 预训练幅度值是否真的宝贵？ ‣ 垃圾DNA假说：通过稀疏性对LLM预训练权重的任务中心角度")）。相反，如果我们冻结所有预训练权重，只更新少量（例如，10%）幅度最大的权重，我们通常可以匹配完全微调稠密模型的性能。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Junk DNA Hypothesis holds true when transitioning from unstructured to structured
    N:M pruning. Counter intuitively, N:M sparsity consistently outperforms unstructured
    sparsity at very high levels of sparsity, possibly because it avoids layer collapse
    (e.g., see Figure [4](#S4.F4 "Figure 4 ‣ 4 Experiment across diverse task categories
    ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through
    Sparsity")-b).'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 垃圾DNA假说在从非结构化到结构化N:M修剪的过渡中依然成立。违反直觉的是，N:M稀疏性在极高稀疏度下始终优于非结构化稀疏性，这可能是因为它避免了层的崩溃（例如，见图[4](#S4.F4
    "图4 ‣ 4 多样任务类别实验 ‣ 垃圾DNA假说：通过稀疏性对LLM预训练权重的任务中心角度")-b）。
- en: 2 Related Work
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Classical Pruning and Sparse Neural Networks
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 经典修剪与稀疏神经网络
- en: Pruning removes specific parts of a deep neural network, such as weights, neurons,
    or filters. The initial purpose of pruning is retrospectively to accelerate the
    model at inference time (a.k.a., post-training sparsification (Mozer & Smolensky,
    [1989](#bib.bib40); LeCun et al., [1990](#bib.bib28)). Post-training sparsification
    has been well studied and results in various mature criteria that can be generally
    categorized into zero-order methods (Han et al., [2016](#bib.bib16); Gale et al.,
    [2019](#bib.bib15)), first-order methods (Molchanov et al., [2016](#bib.bib38);
    Sanh et al., [2020](#bib.bib46); Jiang et al., [2021](#bib.bib22)), and second-order
    methods (LeCun et al., [1990](#bib.bib28); Hassibi & Stork, [1992](#bib.bib17);
    Dong et al., [2017](#bib.bib8)) - the last usually achieve higher performance
    but also are more expensive due to the Hessian calculation, leading to the development
    of many Hessian approximation approaches (Zeng & Urtasun, [2018](#bib.bib57);
    Wang et al., [2019](#bib.bib51); Singh & Alistarh, [2020](#bib.bib47); Kurtic
    et al., [2022](#bib.bib26)). The Lottery Ticket Hypothesis (LTH) (Frankle & Carbin,
    [2019](#bib.bib11)) utilizes iterative magnitude pruning (IMP) to identify a subnetwork
    at initialization that can be re-trained independently to the original dense network’s
    performance. Sparse training (Mocanu et al., [2018](#bib.bib37); Jaiswal et al.,
    [2022](#bib.bib20); Mostafa & Wang, [2019](#bib.bib39); Evci et al., [2020](#bib.bib9);
    Liu et al., [2021](#bib.bib30); Yuan et al., [2021](#bib.bib55); Yin et al., [2023](#bib.bib54);
    Kundu et al., [2021](#bib.bib25)), on the other hand, starts with a (random) sparse
    network and updates network connectivity during training to search for good sparse
    neural network without any pre-training and dense training steps.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝移除深度神经网络中的特定部分，如权重、神经元或滤波器。剪枝的初衷是为了在推理时加速模型（即后训练稀疏化（Mozer & Smolensky，[1989](#bib.bib40)；LeCun
    et al.，[1990](#bib.bib28)））。后训练稀疏化已经得到充分研究，并形成了各种成熟的标准，这些标准一般可以分为零阶方法（Han et al.，[2016](#bib.bib16)；Gale
    et al.，[2019](#bib.bib15)）、一阶方法（Molchanov et al.，[2016](#bib.bib38)；Sanh et al.，[2020](#bib.bib46)；Jiang
    et al.，[2021](#bib.bib22)）和二阶方法（LeCun et al.，[1990](#bib.bib28)；Hassibi & Stork，[1992](#bib.bib17)；Dong
    et al.，[2017](#bib.bib8)）——最后一种方法通常能达到更高的性能，但由于涉及到Hessian矩阵的计算，成本也更高，因此发展出了许多Hessian近似方法（Zeng
    & Urtasun，[2018](#bib.bib57)；Wang et al.，[2019](#bib.bib51)；Singh & Alistarh，[2020](#bib.bib47)；Kurtic
    et al.，[2022](#bib.bib26)）。彩票票据假设（LTH）（Frankle & Carbin，[2019](#bib.bib11)）利用迭代幅度剪枝（IMP）来识别一个初始化时可以独立于原始稠密网络性能进行重新训练的子网络。稀疏训练（Mocanu
    et al.，[2018](#bib.bib37)；Jaiswal et al.，[2022](#bib.bib20)；Mostafa & Wang，[2019](#bib.bib39)；Evci
    et al.，[2020](#bib.bib9)；Liu et al.，[2021](#bib.bib30)；Yuan et al.，[2021](#bib.bib55)；Yin
    et al.，[2023](#bib.bib54)；Kundu et al.，[2021](#bib.bib25)），则是从一个（随机的）稀疏网络开始，并在训练过程中更新网络连接，以寻找良好的稀疏神经网络，而不需要任何预训练和稠密训练步骤。
- en: 2.2 Sparsity in Large-Scale Models
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大规模模型中的稀疏性
- en: The advent of large-scale pre-trained models has led to the development of advanced
    post-training pruning methods, aiming to enhance the cost-effectiveness of these
    expansive models (Sanh et al., [2020](#bib.bib46); Chen et al., [2020](#bib.bib4);
    Jaiswal et al., [2023b](#bib.bib21); Zafrir et al., [2021](#bib.bib56); Kurtic
    et al., [2022](#bib.bib26); Xu et al., [2021](#bib.bib53); Lagunas et al., [2021](#bib.bib27);
    Zhang et al., [2022](#bib.bib58); Frantar et al., [2021](#bib.bib14); Jaiswal
    et al., [2023a](#bib.bib19); Ma et al., [2023](#bib.bib36)). Among them, Frantar
    et al. ([2021](#bib.bib14)) extend second-order pruning to the BERT-level scale,
    enabling the pruning of blocks of weights and achieving state-of-the-art results
    for sparse BERT. Frantar & Alistarh ([2023](#bib.bib13)) introduce SparseGPT for
    pruning large language models (LLMs) in a single shot without requiring re-training
    or fine-tuning. They leverage column-wise second-order pruning, and successfully
    remove 100B weights from OPT-175B without a significant increase in perplexity.
    More recently, Sun et al. ([2023](#bib.bib48)) propose a straightforward pruning
    method that takes both weights and activations into account, demonstrating comparable
    performance to Frantar & Alistarh ([2023](#bib.bib13)). Li et al. ([2022](#bib.bib29))
    reveal that activation sparsity is a prevalent phenomenon in Transformers (90%
    of intermediate output), yielding another opportunity for acceleration. Liu et al.
    ([2023](#bib.bib33)) introduce a large-scale SMC-Bench, indicating that state-of-the-art
    magnitude- and/or gradient-based sparse algorithms fall short when applied out-of-the-box
    to larger-scale models and a selected of complex downstream tasks. Our study is
    inspired by Liu et al. ([2023](#bib.bib33)), but with significantly expanded experiment
    scales, versatile task choices, concrete task difficulty definitions, and richer
    insights.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模预训练模型的出现促使了先进后训练剪枝方法的发展，旨在提高这些庞大模型的成本效益（Sanh et al., [2020](#bib.bib46)；Chen
    et al., [2020](#bib.bib4)；Jaiswal et al., [2023b](#bib.bib21)；Zafrir et al., [2021](#bib.bib56)；Kurtic
    et al., [2022](#bib.bib26)；Xu et al., [2021](#bib.bib53)；Lagunas et al., [2021](#bib.bib27)；Zhang
    et al., [2022](#bib.bib58)；Frantar et al., [2021](#bib.bib14)；Jaiswal et al.,
    [2023a](#bib.bib19)；Ma et al., [2023](#bib.bib36)）。其中，Frantar et al. ([2021](#bib.bib14))
    将二阶剪枝扩展到BERT级别，能够剪枝权重块，并实现了稀疏BERT的最先进结果。Frantar & Alistarh ([2023](#bib.bib13))
    提出了SparseGPT，用于一次性剪枝大型语言模型（LLMs），无需重新训练或微调。他们利用按列的二阶剪枝，成功移除OPT-175B中的100B权重，而不会显著增加困惑度。最近，Sun
    et al. ([2023](#bib.bib48)) 提出了一种简单的剪枝方法，同时考虑权重和激活，显示出与Frantar & Alistarh ([2023](#bib.bib13))
    相当的性能。Li et al. ([2022](#bib.bib29)) 揭示了激活稀疏性在Transformers中的普遍现象（90%的中间输出），提供了另一个加速机会。Liu
    et al. ([2023](#bib.bib33)) 引入了大规模的SMC-Bench，表明最先进的幅度和/或梯度基础的稀疏算法在应用于更大规模的模型和复杂下游任务时不尽如人意。我们的研究受到Liu
    et al. ([2023](#bib.bib33))的启发，但实验规模显著扩大，任务选择更加多样化，任务难度定义更加具体，洞察也更为丰富。
- en: 3 Experiment within the same task category
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 在相同任务类别中的实验
- en: In this section, we will evaluate Junk DNA Hypothesis within the same task category,
    by defining task difficulties concretely in three different settings. We will
    specifically validate if the pre-trained values are the true gems through carefully
    designed control experiments.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过在三个不同设置中具体定义任务难度，对Junk DNA假说进行评估，均在相同的任务类别中。我们将通过精心设计的对照实验，特别验证预训练的值是否真正具有价值。
- en: 3.1 Three Difficulty Settings within the Same Task
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 同一任务中的三种难度设置
- en: '❶ Task Difficulty Setting 1: Varying the Adequacy of Target Domain Data'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 任务难度设置 1：目标领域数据的充分性变化
- en: 'Rationale: The difficulty of learning a task is commonly thought to be influenced
    by the number of available training examples: fewer data points typically imply
    more challenges to learn well. To quantitatively control task difficulty within
    a single task, we manually manipulate the volume of data used for fine-tuning
    by randomly sampling various ratios from the target domain dataset. This allows
    us to disentangle task difficulty from the task type.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 理由：学习一个任务的难度通常被认为受可用训练示例数量的影响：数据点越少，学习的挑战通常越大。为了在单一任务中定量控制任务难度，我们通过从目标领域数据集中随机抽取不同比例的数据，手动调整用于微调的数据量。这使我们能够将任务难度与任务类型分离开来。
- en: 'Method: To examine the influence of small-magnitude weights, we conduct a comparative
    analysis between two models: one starting from the pre-trained model with small-magnitude
    weights, and the other without. The former is commonly referred to as task-specific
    fine-tuning on downstream tasks, denoted as Dense Transfer in this paper. The
    latter model, named Sparse Transfer, differs from dense transfer in the way that
    we first perform magnitude pruning on the pre-trained model, creating a sparse
    model. We then fine-tune on downstream tasks while keeping the sparse mask fixed.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 方法：为了检验小幅度权重的影响，我们对两种模型进行了比较分析：一种是从带有小幅度权重的预训练模型开始，另一种则没有。前者通常被称为下游任务的任务特定微调，在本文中称为Dense
    Transfer。后者模型，名为Sparse Transfer，与密集传输的不同之处在于我们首先对预训练模型进行幅度剪枝，创建一个稀疏模型。然后，我们在下游任务上进行微调，同时保持稀疏掩码不变。
- en: 'For pruning, we employ the widely adopted one-shot magnitude-based pruning (Han
    et al., [2016](#bib.bib16)), a basic approach that removes small-magnitude weights
    from a pre-trained network. We consider two types of sparsities: (1) Unstructured
    Sparsity: individual weights in the model are zeroed out independently, leading
    to irregular zero patterns (LeCun et al., [1990](#bib.bib28); Han et al., [2016](#bib.bib16));
    (2) Structured N:M Sparsity: a fine-grained sparsity pattern in which only N weights
    are non-zero for every continuous M weights (Nvidia, [2020](#bib.bib42); Zhou
    et al., [2021](#bib.bib60)). We report the results of M=8 (N ranges from 7 to
    1) in the main paper and leave those of M=4 (N ranges from 3 to 1) in Appendix [A](#A1
    "Appendix A Results of N:M sparsity with M=4 ‣ Junk DNA Hypothesis: A Task-Centric
    Angle of LLM Pre-trained Weights through Sparsity").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '对于剪枝，我们采用了广泛使用的单次幅度剪枝方法（Han et al., [2016](#bib.bib16)），这是一种基本方法，移除预训练网络中的小幅度权重。我们考虑两种类型的稀疏性：（1）无结构稀疏性：模型中的单个权重被独立地置零，导致不规则的零模式（LeCun
    et al., [1990](#bib.bib28); Han et al., [2016](#bib.bib16)）；（2）结构化N:M稀疏性：一种细粒度的稀疏模式，其中每连续的M个权重中仅有N个权重非零（Nvidia,
    [2020](#bib.bib42); Zhou et al., [2021](#bib.bib60)）。我们在主要论文中报告了M=8（N范围从7到1）的结果，并将M=4（N范围从3到1）的结果留在附录[A](#A1
    "Appendix A Results of N:M sparsity with M=4 ‣ Junk DNA Hypothesis: A Task-Centric
    Angle of LLM Pre-trained Weights through Sparsity")中。'
- en: Many advanced techniques, such as iterative pruning with re-training (Frankle
    & Carbin, [2019](#bib.bib11)), second-order pruning (Kurtic et al., [2022](#bib.bib26)),
    learning rate rewinding (Renda et al., [2020](#bib.bib45)), and knowledge distillation (Hinton
    et al., [2015](#bib.bib18)), can all yield stronger empirical performance for
    sparse transfer (Liu et al., [2023](#bib.bib33)). However, we intentionally opt
    for one-shot magnitude pruning in order to isolate the effect of small-magnitude
    weights as the sole “delta" between the two models, and due to its recently observed
    promising performance on large language models (Jaiswal et al., [2023a](#bib.bib19)).
    For N:M sparsity, we similarly perform magnitude-based single-shot pruning as
    described in Zhou et al. ([2021](#bib.bib60)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 许多先进技术，如带有重新训练的迭代剪枝（Frankle & Carbin, [2019](#bib.bib11)）、二阶剪枝（Kurtic et al.,
    [2022](#bib.bib26)）、学习率回退（Renda et al., [2020](#bib.bib45)）和知识蒸馏（Hinton et al.,
    [2015](#bib.bib18)），都能为稀疏传输带来更强的实证性能（Liu et al., [2023](#bib.bib33)）。然而，我们有意选择单次幅度剪枝，以隔离小幅度权重作为两种模型之间唯一的“增量”，并且由于其在大语言模型上最近观察到的良好表现（Jaiswal
    et al., [2023a](#bib.bib19)）。对于N:M稀疏性，我们同样进行幅度基础的单次剪枝，如Zhou et al.（[2021](#bib.bib60)）中所述。
- en: 'Model and Datasets: In this setting, we test the Junk DNA hypothesis with the
    pre-trained RoBERTa-Large/Base models (Devlin et al., [2018](#bib.bib7)) provided
    by Hugging Face¹¹1[https://huggingface.co/docs/transformers/model_doc/roberta](https://huggingface.co/docs/transformers/model_doc/roberta).
    We choose the downstream tasks of SST-2, QNLI, MNLI from the classical GLUE benchmark (Wang
    et al., [2018](#bib.bib50)). Following common practice, we do not prune the embedding
    layer nor the classifier. Furthermore, we use the same fine-tuning recipe for
    both dense and sparse models, ensuring a fair comparison between them.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集：在这种设置下，我们用Hugging Face提供的预训练RoBERTa-Large/Base模型（Devlin et al., [2018](#bib.bib7)）测试Junk
    DNA假设¹¹1[https://huggingface.co/docs/transformers/model_doc/roberta](https://huggingface.co/docs/transformers/model_doc/roberta)。我们选择了经典GLUE基准中的下游任务SST-2、QNLI、MNLI（Wang
    et al., [2018](#bib.bib50)）。按照常规做法，我们不对嵌入层或分类器进行剪枝。此外，我们对密集模型和稀疏模型使用相同的微调方法，以确保它们之间的公平比较。
- en: '![Refer to caption](img/fd3c6d22940cbe1adea85fcb6aee603d.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fd3c6d22940cbe1adea85fcb6aee603d.png)'
- en: (a) Results of Unstructured Sparsity
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: （a）无结构稀疏性的结果
- en: '![Refer to caption](img/1d157993ce6b2690977e15b4920b09d5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1d157993ce6b2690977e15b4920b09d5.png)'
- en: (b) Results of N:M Sparsity (M=8, N ranges from 1 to 7)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (b) N:M稀疏性结果（M=8，N范围从1到7）
- en: 'Figure 1: Varying target domain data adequacy: Dense Transfer vs. Sparse Transfer
    using RoBERTa-Base on various downstream tasks. Each sub-figure showcases a specific
    downstream task, with various $\%$ of data volume. Task difficulty is measured
    by the training data volume. Note that in the figures, the performance of sparse
    transfer is normalized by the dense transfer performance.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：目标领域数据充分性的变化：使用RoBERTa-Base在各种下游任务中对比密集转移和稀疏转移。每个子图展示了特定的下游任务及其不同的$\%$数据量。任务难度由训练数据量衡量。注意，在图中，稀疏转移的性能是以密集转移的性能为基准的。
- en: '❷ Task Difficulty Setting 2: Majority v.s. Minority in Multi-Domain Learning'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 任务难度设置2：多领域学习中的多数与少数
- en: 'Rationale: Setting 2 essentially extends Setting 1 (more or less data, in a
    single domain) to a multi-domain scenario (Daras et al., [2022](#bib.bib6)): assuming
    multiple data domains will be involved in the downstream task, we conjecture the
    data-rich domain is easier to learn, than the data-scarce domain. Specifically,
    we focus our study on a multilingual translation task with majority versus minority
    languages and consider translation across majority language pairs (i.e., those
    with ample pretraining or fine-tuning data) is categorized as an easy task. Conversely,
    translation across minority language pairs (i.e., “low-resource" ones at pretraining/fine-tuning)
    is considered as a hard task.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 原理：设置2本质上是将设置1（单一领域中的更多或更少数据）扩展到多领域场景（Daras et al., [2022](#bib.bib6)）：假设下游任务将涉及多个数据领域，我们推测数据丰富的领域比数据稀缺的领域更容易学习。具体而言，我们将研究重点放在多语言翻译任务上，比较多数语言与少数语言，并认为在多数语言对之间（即那些有丰富预训练或微调数据的）进行翻译属于简单任务。相反，在少数语言对之间（即“低资源”语言的预训练/微调）进行翻译被认为是困难任务。
- en: 'Method: Similar to the Setting 1, we will compare Dense Transfer and Sparse
    Transfer methods. Our evaluation comprises two distinct regimes: (i) In the "zero-shot"
    scenario, we perform language translation tasks without any fine-tuning. The Dense/Sparse
    models are directly assessed without any form of "transfer". In this context,
    the task’s complexity (i.e., whether the languages involved are categorized as
    majority or minority) is determined by the volume of training data available for
    the respective languages during the pre-training phase; (ii) In the "few-shot"
    scenario, the dense/sparse models undergo fine-tuning on specific language pairs
    before being evaluated on the language translation task involving those particular
    languages. In this situation, the task’s difficulty is defined by the amount of
    data employed for those languages during the fine-tuning phase.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 方法：类似于设置1，我们将比较密集转移和稀疏转移方法。我们的评估包括两个不同的场景：(i) 在“零样本”情况下，我们执行语言翻译任务而不进行任何微调。密集/稀疏模型在没有任何“转移”形式的情况下直接评估。在这种情况下，任务的复杂性（即涉及的语言是否被分类为多数或少数）由预训练阶段相应语言的训练数据量决定；(ii)
    在“少样本”情况下，密集/稀疏模型在特定语言对上进行微调，然后在涉及这些特定语言的翻译任务中进行评估。在这种情况下，任务的难度由微调阶段为这些语言使用的数据量决定。
- en: 'Model and Datasets: We choose the official mBART model (Liu et al., [2020](#bib.bib35))
    for multilingual translation. This model was initially pre-trained on 25 languages
    using the masked language modeling (MLM) approach. From the original pre-training
    set, we narrow down to a subset of 10 languages, and continue training mBART on
    language pairs from this selected subset, following Tang et al. ([2020](#bib.bib49))
    with default configurations of 40K iterations and Adam optimizer at a learning
    rate of $10^{-6}$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集：我们选择官方的mBART模型（Liu et al., [2020](#bib.bib35)）用于多语言翻译。该模型最初使用掩蔽语言模型（MLM）方法在25种语言上进行预训练。从原始预训练集，我们缩小到10种语言的子集，并继续在这个选定子集的语言对上训练mBART，遵循Tang
    et al. ([2020](#bib.bib49))的默认配置，进行40K次迭代，使用Adam优化器，学习率为$10^{-6}$。
- en: 'In the downstream, we assess mBART by selecting four languages from the open-source
    parallel corpus (OPU, [2020](#bib.bib1)). Following the methodology described
    by Arivazhagan et al. ([2019](#bib.bib2)), we utilize pivot data via English to
    establish 2-to-2 majority language translation pairs (Ru,Vi) and 2-to-2 minority
    language translation pairs (Gu, My). The statistics of these four languages in
    the pre-training dataset, CC-25 (Liu et al., [2020](#bib.bib35)), is presented
    in the Appendix Table [2](#A3.T2 "Table 2 ‣ Appendix C Languages and Statistics
    for Multilingual Translation ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM
    Pre-trained Weights through Sparsity").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在下游任务中，我们通过从开源平行语料库（OPU，[2020](#bib.bib1)）中选择四种语言来评估mBART。按照Arivazhagan等（[2019](#bib.bib2)）描述的方法，我们通过英语使用枢轴数据建立2对2的主要语言翻译对（Ru,Vi）和2对2的少数语言翻译对（Gu,
    My）。这些四种语言在预训练数据集CC-25（Liu等，[2020](#bib.bib35)）中的统计数据在附录表[2](#A3.T2 "Table 2 ‣
    Appendix C Languages and Statistics for Multilingual Translation ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity")中呈现。'
- en: '![Refer to caption](img/4256023d51f8929449f9aa0d29dd9e4e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4256023d51f8929449f9aa0d29dd9e4e.png)'
- en: (a) Unstructured Sparsity
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 非结构化稀疏性
- en: '![Refer to caption](img/1c5c2468b8ac0e2f12845acc25db02a6.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1c5c2468b8ac0e2f12845acc25db02a6.png)'
- en: (b) N:M Sparsity (M=8, N ranges from 1 to 7)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: (b) N:M稀疏性（M=8，N范围从1到7）
- en: 'Figure 2: Majority vs minority languages: Results on mBART with multilingual
    language translation. In either figure, the left plot is the “zero-shot" evaluation
    and the right “few-shot" evaluation. In both figures, the performance of sparse
    transfer is normalized by the dense transfer performance.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：主要语言与少数语言：mBART在多语言翻译中的结果。在任一图中，左侧图是“零样本”评估，右侧图是“少量样本”评估。在两个图中，稀疏转移的表现都通过密集转移的表现进行归一化。
- en: '❸ Task Difficulty Setting 3: With v.s. Without Available External Information'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 任务难度 Setting 3：有 vs. 无外部信息
- en: 'Rationale: Setting 3 posits that a task aided by external information will
    be inherently easier to solve in comparison to the same task undertaken without
    such external support. Our focus is specifically on the Question-Answering (QA)
    task, where we compare two distinct settings: (i) Open-book QA, which permits
    the model to consult and extract information from external sources; and (ii) Closed-book
    QA, where the model must rely solely on its internal knowledge. We postulate that
    Open-book QA represents an easier task when compared to Closed-book QA.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 理由：Setting 3认为，在外部信息的帮助下，任务会比没有这种外部支持的同一任务更容易解决。我们的重点是Question-Answering (QA)任务，其中我们比较两种不同的设置：（i）开放书籍QA，允许模型从外部来源咨询和提取信息；（ii）封闭书籍QA，模型必须完全依赖内部知识。我们推测开放书籍QA相对于封闭书籍QA是一项更容易的任务。
- en: 'Method: Similar to the previous two settings, we will juxtapose the Dense Transfer
    and Sparse Transfer methodologies. It is worth noting that in the creation of
    the sparse pre-trained model for Setting 3, we not only utilize magnitude-based
    pruning, but also incorporate two other latest LLM pruning techniques: SparseGPT
    (Frantar & Alistarh, [2023](#bib.bib13)) and Wanda (Sun et al., [2023](#bib.bib48)).
    While these two methods do not exactly remove elements based on magnitude, our
    aim is to ascertain whether the Junk DNA hypothesis can be extended to encompass
    other weight importance criteria as well.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 方法：类似于之前的两个设置，我们将并排比较Dense Transfer和Sparse Transfer方法。值得注意的是，在创建Setting 3的稀疏预训练模型时，我们不仅使用了基于幅度的剪枝，还结合了另外两种最新的LLM剪枝技术：SparseGPT（Frantar
    & Alistarh，[2023](#bib.bib13)）和Wanda（Sun等，[2023](#bib.bib48)）。虽然这两种方法并不是完全基于幅度移除元素，但我们的目标是确定Junk
    DNA假设是否可以扩展到其他权重重要性标准。
- en: 'Model and Datasets: We choose Vicuna-7B (Chiang et al., [2023](#bib.bib5)),
    an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations
    collected from ShareGPT. For this task setting, we use a popular reading comprehension
    dataset, TriviaQA Joshi et al. ([2017](#bib.bib23)), which includes 95K question-answer
    pairs authored by trivia enthusiasts and independently gathered evidence documents,
    six per question on average, that provide high-quality distant supervision for
    answering.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与数据集：我们选择了Vicuna-7B（Chiang等，[2023](#bib.bib5)），这是一个通过在ShareGPT收集的用户共享对话上微调LLaMA训练的开源聊天机器人。在这个任务设置中，我们使用一个流行的阅读理解数据集TriviaQA
    Joshi等（[2017](#bib.bib23)），它包括由知识问答爱好者编写的95K问答对，以及独立收集的证据文档，每个问题平均有六个，这些文档为回答提供了高质量的远程监督。
- en: TriviaQA consists of fairly complex composition based questions with considerable
    syntactic and lexical variability between questions and corresponding answer-evidence
    sentences, making it a challenging enough test-bed for our evaluation. We access
    Vicuna-7B performance by conditioning questions with and without evidence sentences
    in open and closed book setting respectively.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TriviaQA包含了相当复杂的基于组成的问题，问题和相应的答案证据句之间具有相当大的句法和词汇变异，使其成为我们评估的一个足够具有挑战性的测试平台。我们通过在开卷和闭卷设置下分别以有和没有证据句的方式来评估Vicuna-7B的表现。
- en: '![Refer to caption](img/0382ad7c92d4f327b91c0f537a718c65.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0382ad7c92d4f327b91c0f537a718c65.png)'
- en: 'Figure 3: Open-book vs Close-book QA: Results on Vicuna-7B with sparse models
    generated by magnitude pruning (left figure), SparseGPT (middle), and Wanda (right).
    In all figures, the performance of sparse transfer is normalized by the dense
    transfer performance.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：开卷 vs 闭卷问答：在使用幅度修剪（左图）、SparseGPT（中图）和Wanda（右图）生成的稀疏模型上的Vicuna-7B的结果。在所有图中，稀疏迁移的性能都已按密集迁移性能进行归一化。
- en: '3.2 Main Results: Validating the Junk DNA Hypothesis'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 主要结果：验证垃圾DNA假说
- en: 'We present our findings from Settings 1-3, in Figures [1](#S3.F1 "Figure 1
    ‣ 3.1 Three Difficulty Settings within the Same Task ‣ 3 Experiment within the
    same task category ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained
    Weights through Sparsity"), [2](#S3.F2 "Figure 2 ‣ 3.1 Three Difficulty Settings
    within the Same Task ‣ 3 Experiment within the same task category ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity"), and  [3](#S3.F3
    "Figure 3 ‣ 3.1 Three Difficulty Settings within the Same Task ‣ 3 Experiment
    within the same task category ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM
    Pre-trained Weights through Sparsity"), respectively. These results provide robust
    support for our Junk DNA hypothesis, and we summarize the key observations below:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了设置1-3的发现，见图[1](#S3.F1 "图1 ‣ 3.1 同一任务中的三种难度设置 ‣ 3 同一任务类别中的实验 ‣ 垃圾DNA假说：通过稀疏性看LLM预训练权重的任务中心视角")、[2](#S3.F2
    "图2 ‣ 3.1 同一任务中的三种难度设置 ‣ 3 同一任务类别中的实验 ‣ 垃圾DNA假说：通过稀疏性看LLM预训练权重的任务中心视角")和[3](#S3.F3
    "图3 ‣ 3.1 同一任务中的三种难度设置 ‣ 3 同一任务类别中的实验 ‣ 垃圾DNA假说：通过稀疏性看LLM预训练权重的任务中心视角")。这些结果为我们的垃圾DNA假说提供了有力的支持，我们在下面总结了主要观察结果：
- en: 1
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: 'Removal of small-magnitude weights is viable to some extent for easier tasks:
    In all three settings involving easy tasks (i.e., 70% and 100% data volume in
    Setting 1, and Open-book QA in Setting 3), we find that it is feasible to discard
    30%-50% of small-magnitude weights at once without compromising performance in
    unstructured sparsity. This indicates that for simple tasks, the knowledge encoded
    in high-magnitude weights is sufficient to handle the task. Consequently, fine-tuning
    the high-magnitude weights proves to be adequate, rendering small weights unnecessary.
    We note that this conclusion aligns with the prior finding in (Jaiswal et al.,
    [2023a](#bib.bib19)).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 小幅度权重的去除在某些简单任务中是可行的：在涉及简单任务的所有三种设置中（即设置1中的70%和100%数据量，以及设置3中的开卷问答），我们发现可以一次性丢弃30%-50%的小幅度权重，而不会影响在非结构化稀疏性下的性能。这表明，对于简单任务，编码在高幅度权重中的知识足以处理该任务。因此，微调高幅度权重已经足够，使得小权重变得不必要。我们注意到，这一结论与(Jaiswal
    et al., [2023a](#bib.bib19))中的先前发现一致。
- en: 2
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: 'Eliminating small weights leads to irreversible performance degradation in
    more challenging tasks: In stark contrast to the ease with ample data volume in
    Setting 1, fine-tuning with limited data, such as 10% and 25% of the original
    dataset, can only achieve performance parity with dense models at a maximum sparsity
    level of 20%. In Setting 2, we have observed an even more remarkable phenomenon:
    the removal of just 5% of weights on the harder task (i.e., translation across
    minority languages), leads to a noticeable decline in performance for both “zero-shot”
    and “few-shot” evaluations. It is noteworthy that the performance degradation
    caused by pruning small-magnitude weights is more pronounced in the “few-shot”
    setting compared to the “zero-shot” setting, indicating the greater importance
    of small weights when the target domain data is accessible. Furthermore, a similar
    trend is observed in the Close-book task in Setting 3, where models experience
    performance deterioration even at trivial sparsity levels, such as 5% to 10%,
    when small-pruned weights are eliminated. Overall, the above results show that
    those “useless” small weights are imperative to encode crucial knowledge necessary
    for solving more challenging downstream tasks.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 消除小权重会导致更具挑战性任务中的不可逆性能下降：与设置 1 中充足数据量的简单情况形成鲜明对比，使用有限数据（如原始数据集的 10% 和 25%）进行微调，即使在最大稀疏水平为
    20% 时，也只能实现与密集模型相当的性能。在设置 2 中，我们观察到了一个更显著的现象：在更困难的任务（即，少数语言间翻译）中，仅移除 5% 的权重就会导致
    “零-shot” 和 “少量-shot” 评估中性能的明显下降。值得注意的是，修剪小规模权重导致的性能下降在“少量-shot”设置中比“零-shot”设置更为明显，这表明当目标领域数据可用时，小权重的重要性更大。此外，在设置
    3 的封闭式任务中也观察到了类似的趋势，当小规模修剪的权重被移除时，即使在微不足道的稀疏水平（如 5% 到 10%）下，模型也会出现性能退化。总体而言，上述结果表明，这些“无用”的小权重对于编码解决更具挑战性的下游任务所需的关键知识至关重要。
- en: 3
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 3
- en: 'Junk DNA persists in both N:M sparsity and unstructured sparsity, and beyond
    the magnitude criteria. Both N:M sparsity and unstructured sparsity yield similar
    observations, indicating the presence of Junk DNA in both settings. We highlight
    a counter-intuitive discovery in this context: Despite a more constrained pattern,
    N:M sparsity consistently outperforms unstructured sparsity at extremely high
    levels of sparsity, highlighted in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Three Difficulty
    Settings within the Same Task ‣ 3 Experiment within the same task category ‣ Junk
    DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through Sparsity").
    This advantage is likely attributed to its inherent ability to prevent layer collapse,
    which commonly happens for global magnitude pruning.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾 DNA 在 N:M 稀疏性和非结构稀疏性中都存在，超出了大小标准。N:M 稀疏性和非结构稀疏性产生了类似的观察结果，表明在这两种设置中都存在垃圾
    DNA。我们在此背景下强调了一个反直觉的发现：尽管模式更受限制，但在极高稀疏水平下，N:M 稀疏性始终优于非结构稀疏性，如图 [3](#S3.F3 "图 3
    ‣ 3.1 同一任务中的三种难度设置 ‣ 在同一任务类别中的实验 ‣ 垃圾 DNA 假设：通过稀疏性对 LLM 预训练权重的任务中心角度") 所示。这一优势可能归因于其固有的防止层崩溃的能力，而层崩溃通常发生在全局大小修剪中。
- en: 'We also evaluate Junk DNA Hypothesis with two non-magnitude pruning methods:
    SparseGPT and Wanda. We apply them to Vicuna-7B and evaluate the sparse models
    on Open-book QA and Closed-book QA. A similar performance trend can be observed
    across all pruning methods as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Three
    Difficulty Settings within the Same Task ‣ 3 Experiment within the same task category
    ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained Weights through
    Sparsity"): While Wanda appears to perform slightly better than magnitude pruning
    and SparseGPT, all of them consistently demonstrate the same trend: suffering
    from larger performance degradation on Close-book QA which is more challenging
    than Open-book QA. Therefore, we conclude that Junk DNA Hypothesis can be extended
    to encompass other weight importance criteria as well.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还用两种非大小修剪方法：SparseGPT 和 Wanda 来评估垃圾 DNA 假设。我们将它们应用于 Vicuna-7B，并在开放式问答和封闭式问答上评估稀疏模型。正如图
    [3](#S3.F3 "图 3 ‣ 3.1 同一任务中的三种难度设置 ‣ 在同一任务类别中的实验 ‣ 垃圾 DNA 假设：通过稀疏性对 LLM 预训练权重的任务中心角度")
    所示，各种修剪方法中的性能趋势相似：虽然 Wanda 的表现似乎略优于大小修剪和 SparseGPT，但它们都一致表现出相同的趋势：在更具挑战性的封闭式问答中，性能下降幅度较大，而开放式问答则相对较少。因此，我们得出结论，垃圾
    DNA 假设可以扩展到涵盖其他权重重要性标准。
- en: 4 Experiment across diverse task categories
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 在不同任务类别中进行实验
- en: In this section, we broaden our Junk DNA hypothesis to encompass a range of
    task categories. However, quantifying and comparing task difficulty across diverse
    task types poses a significant challenge due to a multitude of contributing factors.
    These factors include data type, size, distribution, task definition, loss function,
    and more. The primary research question at hand is to identify a dependable task
    difficulty estimator that remains agnostic to the aforementioned factors.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们扩展了我们的“垃圾DNA”假说，以涵盖一系列任务类别。然而，由于多种因素的影响，量化和比较不同任务类型的难度是一项重大挑战。这些因素包括数据类型、大小、分布、任务定义、损失函数等。当前的主要研究问题是识别一个可靠的任务难度估计器，该估计器不受上述因素的影响。
- en: '❹ Task Difficulty Setting 4: Estimating LLM-facing Task Difficulty by Normalized
    Human-LLM Performance Gap'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 任务难度设定4：通过归一化人类-LLM表现差距来估计LLM面临的任务难度
- en: 'Rationale and Method: We propose a method to gauge complexity by juxtaposing
    the performance of deep learning models with that of human counterparts. Specifically,
    we define task difficulty as the disparity in performance between humans and models,
    normalized by human performance. A more pronounced positive performance gap (for
    instance, where humans outperform the machine to a greater extent) would signify
    a higher level of difficulty for the model in handling the given task. Conversely,
    in cases where the machine outperforms humans, a larger gap indicates an easier
    task. The resulting assessment of across-task difficulty is outlined in Table [1](#S4.T1
    "Table 1 ‣ 4 Experiment across diverse task categories ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '理由和方法：我们提出了一种通过将深度学习模型的表现与人类对照组的表现进行对比来评估复杂性的方法。具体而言，我们将任务难度定义为人类与模型之间的表现差异，并以人类表现进行归一化。更显著的正表现差距（例如，人类在某一任务上的表现明显优于机器）将表明模型在处理该任务时的难度较高。相反，当机器表现优于人类时，较大的差距表示任务较容易。跨任务难度的评估结果详见表[1](#S4.T1
    "Table 1 ‣ 4 Experiment across diverse task categories ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity")。'
- en: Specifically, we choose a range of downstream tasks, including SST-2, COLA,
    QQP, STS-B, QNLI, MNLI, and RTE from the GLUE benchmark (Wang et al., [2018](#bib.bib50))
    conducted on RoBERTa Large and Base models. Additionally, we incorporate tasks
    involving commonsense reasoning (CSQA, WinoGrande) as well as arithmetic reasoning
    (MAWPS, SVAMP) from the SMC-Bench (Liu et al., [2023](#bib.bib33)). It is worth
    noting that certain metrics used for comparison may not align perfectly (e.g.,
    SST-2 vs. COLA; QQP vs. STS-B), which may introduce some limitations in our comparisons.
    However, we emphasize that our assessment of task difficulty aligns well with
    intuitions derived from previous studies (Wasserblat, [2021](#bib.bib52); Ko &
    Choi, [2020](#bib.bib24)). These studies suggest that COLA is more challenging
    than SST-2, and STS-B is more difficult than QQP, respectively.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们选择了一系列下游任务，包括GLUE基准（Wang et al., [2018](#bib.bib50)）中的SST-2、COLA、QQP、STS-B、QNLI、MNLI和RTE任务，以及来自SMC-Bench（Liu
    et al., [2023](#bib.bib33)）的常识推理（CSQA、WinoGrande）和算术推理（MAWPS、SVAMP）任务。值得注意的是，某些用于比较的指标可能并不完全一致（例如，SST-2与COLA；QQP与STS-B），这可能在我们的比较中引入一些限制。然而，我们强调，我们对任务难度的评估与先前研究得出的直觉一致（Wasserblat,
    [2021](#bib.bib52); Ko & Choi, [2020](#bib.bib24)）。这些研究表明，COLA比SST-2更具挑战性，STS-B比QQP更困难。
- en: 'Result: The findings depicted in Figure [4](#S4.F4 "Figure 4 ‣ 4 Experiment
    across diverse task categories ‣ Junk DNA Hypothesis: A Task-Centric Angle of
    LLM Pre-trained Weights through Sparsity") echo the conclusions drawn in Section [3](#S3
    "3 Experiment within the same task category ‣ Junk DNA Hypothesis: A Task-Centric
    Angle of LLM Pre-trained Weights through Sparsity"), once more providing robust
    support for the validity of the Junk DNA hypothesis across a broad spectrum of
    task categories. While it may be feasible to remove small-magnitude weights without
    significant repercussions in simpler tasks, these pre-trained small weights contain
    vital downstream knowledge essential for tackling more difficult tasks, and thus
    are no longer dispensable.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：图 [4](#S4.F4 "图 4 ‣ 4 在各种任务类别中进行的实验 ‣ 垃圾 DNA 假设：LLM 预训练权重通过稀疏性进行的任务中心角度")
    显示的发现再次支持第 [3](#S3 "3 在相同任务类别中进行的实验 ‣ 垃圾 DNA 假设：LLM 预训练权重通过稀疏性进行的任务中心角度") 节的结论，为垃圾
    DNA 假设在广泛的任务类别中提供了有力支持。尽管在简单任务中可能可以去除小幅度权重而不产生重大影响，但这些预训练的小权重包含了应对更困难任务所需的重要下游知识，因此已经不可或缺。
- en: 'Table 1: Measuring the Across-Task Difficulty by the Performance Difference
    between humans and models (normalized by human performance): the larger (positive)
    margin, the more difficult for the machine. Human performance is obtained from Nangia
    & Bowman ([2019](#bib.bib41)). The more challenging task is marked in bold.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：通过人类与模型之间的性能差异（以人类表现为标准化）来测量跨任务难度：差距越大（正值），机器越难处理。人类表现数据来源于 Nangia & Bowman
    ([2019](#bib.bib41))。更具挑战性的任务以粗体标记。
- en: '|  | Single Sentence | Sentence Similarity | Natural Language Inference | Commonsense
    Reasoning |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 单句 | 句子相似性 | 自然语言推断 | 常识推理 |'
- en: '|  | SST-2 | COLA | QQP | STS-B | QNLI | RTE | WinoGrande | CSQA |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | SST-2 | COLA | QQP | STS-B | QNLI | RTE | WinoGrande | CSQA |'
- en: '| Human | 97.8 | 66.4 | 80.4 | 92.7 | 91.2 | 93.6 | 94.0 | 89.0 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 97.8 | 66.4 | 80.4 | 92.7 | 91.2 | 93.6 | 94.0 | 89.0 |'
- en: '| RoBERTa-Large | 96.2 | 64.9 | 91.8 | 92.2 | 94.4 | 84.3 | 78.1 | 72.1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa-Large | 96.2 | 64.9 | 91.8 | 92.2 | 94.4 | 84.3 | 78.1 | 72.1 |'
- en: '| “Task Difficulty" (%) | 1.64 | 2.26 | -14.18 | 0.54 | -3.51 | 9.94 | 16.91
    | 18.99 | ![Refer to caption](img/133edadce4a388794c85e7d012f5194e.png)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '| “任务难度" (%) | 1.64 | 2.26 | -14.18 | 0.54 | -3.51 | 9.94 | 16.91 | 18.99 |
    ![参见标题](img/133edadce4a388794c85e7d012f5194e.png)'
- en: (a) Results of Unstructured Sparsity
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 非结构化稀疏性结果
- en: '![Refer to caption](img/0afa908b11a0b28e10b61411bdd02835.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0afa908b11a0b28e10b61411bdd02835.png)'
- en: (b) Results of N:M Sparsity (M=8, N ranges from 7 to 1)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (b) N:M 稀疏性结果 (M=8, N 范围从 7 到 1)
- en: 'Figure 4: Across-Task Difficulty via Normalized Human-LLM Performance Gap:
    Dense Transfer vs. Sparse Transfer using RoBERTa-Large on various downstream tasks.
    Each sub-figure compares an easier task and a more challenging one. Note that
    in all figures, the performance of sparse transfer is normalized by the dense
    transfer performance.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：通过标准化人类-LLM 性能差距来测量跨任务难度：使用 RoBERTa-Large 在各种下游任务上进行的密集传输与稀疏传输。每个子图比较一个较容易的任务和一个更具挑战性的任务。请注意，在所有图中，稀疏传输的性能都是以密集传输的性能为标准化的。
- en: 5 Are Pre-trained Magnitude Values Indeed the True Gem?
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 预训练的幅度值真的是真正的宝藏吗？
- en: 'Having recognized the pivotal role of small weights in downstream adaptation,
    particularly in relation to task difficulty, our next objective is to delve into
    the foundational factors contributing to the crucial function of small weights
    during fine-tuning. Our primary research inquiries are outlined below:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 认识到小权重在下游适应中的关键作用，特别是与任务难度相关的作用后，我们的下一个目标是深入探讨小权重在微调过程中发挥关键作用的基础因素。我们的主要研究问题如下：
- en: '![Refer to caption](img/51d84f304b2da808144686f71bec4cd8.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/51d84f304b2da808144686f71bec4cd8.png)'
- en: (a) Results of Unstructured Sparsity
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 非结构化稀疏性结果
- en: '![Refer to caption](img/c62fe9272c53f1c195f2a59f54a1a682.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c62fe9272c53f1c195f2a59f54a1a682.png)'
- en: (b) Results of N:M Sparsity (M=8, N ranges from 7 to 1)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (b) N:M 稀疏性结果 (M=8, N 范围从 7 到 1)
- en: '![Refer to caption](img/f2beef98041ec3680901dd84818207a0.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f2beef98041ec3680901dd84818207a0.png)'
- en: 'Figure 5: Varying target domain data adequacy: four different fine-tuning settings
    with RoBERTa-Base on various downstream tasks. All performance is normalized by
    the one of Dense Transfer.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：目标领域数据充分性变化：在不同下游任务上使用 RoBERTa-Base 的四种不同微调设置。所有性能均以密集传输的表现为标准化。
- en: '• Which holds greater significance: the knowledge (weight values) stored in
    pre-trained small-magnitude weights, or the potential to adjust these weights
    through fine-tuning?'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: •  哪一个更重要：存储在预训练的小幅度权重中的知识（权重值），还是通过微调调整这些权重的潜力？
- en: • Is it possible to recover the knowledge embedded in pre-trained small-magnitude
    weights if we prune them and allow them to grow freely during fine-tuning?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: • 如果我们修剪预训练的小幅度权重，并允许它们在微调过程中自由增长，是否有可能恢复嵌入在其中的知识？
- en: 'Method: To address these inquiries, we explore four comparison methods: (1)
    Dense Transfer: as described in Section [3.1](#S3.SS1 "3.1 Three Difficulty Settings
    within the Same Task ‣ 3 Experiment within the same task category ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity"); (2) Dense
    Transfer with (Partial) Freezing: a dense model where small-magnitude weights
    remain fixed during fine-tuning; (3) Sparse Transfer: as in Section [3.1](#S3.SS1
    "3.1 Three Difficulty Settings within the Same Task ‣ 3 Experiment within the
    same task category ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained
    Weights through Sparsity"); (4) Sparse to Dense Transfer: small-magnitude weights
    are initially pruned after pre-training, and subsequently during fine-tuning,
    those pruned weights are allowed to gradually regain non-zero values. This approach
    also aids in determining whether the knowledge within small-magnitude pre-trained
    weights is essential for performance or if their adaptability during fine-tuning
    takes precedence. We pick Setting 1 (within-task) and Setting 4 (across-task),
    to report their performance of RoBERTa-Large, on MNLI, QNLI, SST-2, as well as
    CSQA and WinoGrande.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方法：为了解决这些问题，我们探索了四种比较方法：（1）密集迁移：如第[3.1](#S3.SS1 "3.1 三种任务难度设置 ‣ 同一任务类别内实验 ‣
    垃圾DNA假设：通过稀疏性看待LLM预训练权重的任务中心视角")节所述；（2）带（部分）冻结的密集迁移：在微调过程中保持小幅度权重不变的密集模型；（3）稀疏迁移：如第[3.1](#S3.SS1
    "3.1 三种任务难度设置 ‣ 同一任务类别内实验 ‣ 垃圾DNA假设：通过稀疏性看待LLM预训练权重的任务中心视角")节所述；（4）稀疏到密集迁移：在预训练后初始修剪的小幅度权重，在微调过程中逐渐恢复非零值。这种方法还帮助确定小幅度预训练权重中的知识是否对性能至关重要，或者它们在微调过程中的适应性是否优先。我们选择了设置1（任务内）和设置4（任务间），报告RoBERTa-Large在MNLI、QNLI、SST-2、CSQA和WinoGrande上的表现。
- en: 'Results: The outcomes for both within-task difficulty and across-task difficulty
    are illustrated in Figure [5](#S5.F5 "Figure 5 ‣ 5 Are Pre-trained Magnitude Values
    Indeed the True Gem? ‣ Junk DNA Hypothesis: A Task-Centric Angle of LLM Pre-trained
    Weights through Sparsity") and Figure [6](#S5.F6 "Figure 6 ‣ 5 Are Pre-trained
    Magnitude Values Indeed the True Gem? ‣ Junk DNA Hypothesis: A Task-Centric Angle
    of LLM Pre-trained Weights through Sparsity"), respectively. Below, we outline
    our key observations:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：任务内难度和任务间难度的结果分别在图[5](#S5.F5 "图5 ‣ 5 预训练幅度值确实是宝贵的吗？ ‣ 垃圾DNA假设：通过稀疏性看待LLM预训练权重的任务中心视角")和图[6](#S5.F6
    "图6 ‣ 5 预训练幅度值确实是宝贵的吗？ ‣ 垃圾DNA假设：通过稀疏性看待LLM预训练权重的任务中心视角")中进行了说明。以下是我们的主要观察结果：
- en: 1
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 1
- en: Pre-trained small weights harbor vital downstream knowledge, beyond mere free
    parameters. Across both task-difficulty metrics, it becomes evident that settings
    preserving the pre-trained values of small weights—namely, Dense Transfer and
    Dense Transfer with Freezing—achieve superior performance when compared to the
    other two settings. The removal of small-magnitude weights from pre-trained models
    results in significant performance degradation, even when we permit the pruned
    weights to regenerate during fine-tuning. This observation strongly bolsters the
    Junk DNA Hypothesis, indicating that small-magnitude weights are far from redundant;
    rather, they house sophisticated knowledge crucial for downstream adaptation.
    This knowledge proves challenging to re-gain through fine-tuning, if these initial
    pre-trained weights are eliminated.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的小权重蕴含了重要的下游知识，而不仅仅是自由参数。根据任务难度度量标准，保留小权重预训练值的设置——即密集迁移和带冻结的密集迁移——相比其他两个设置表现更为优越。从预训练模型中移除小幅度权重会导致显著的性能下降，即使允许在微调过程中恢复这些修剪的权重。这个观察结果强有力地支持了垃圾DNA假设，表明小幅度权重远非多余；相反，它们包含了对下游适应至关重要的复杂知识。如果这些初始预训练权重被移除，通过微调重新获得这些知识是非常困难的。
- en: 2
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 2
- en: Freezing without updating yields commendable results. Remarkably, on simpler
    tasks like SST-2, MNLI, and QNLI, maintaining an overwhelmingly large portion
    (90%) of small-magnitude weights in a frozen state leads to equally impressive
    performance without any loss. Even on more intricate tasks such as COLA, CSQA,
    and WinoGrande, freezing up to 70% of small-magnitude weights results in no discernible
    performance dip. This suggests that for easier tasks, the knowledge embedded in
    pre-trained small-magnitude weights is already more than sufficient. However,
    for more challenging tasks, allowing for moderate updates to all pre-trained weights
    remains essential.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结而不更新可以取得令人称赞的结果。值得注意的是，在像 SST-2、MNLI 和 QNLI 等简单任务中，保持绝大部分（90%）的小幅度权重处于冻结状态，能够在没有任何性能损失的情况下取得同样出色的表现。即使在像
    COLA、CSQA 和 WinoGrande 这样的复杂任务中，冻结高达 70% 的小幅度权重也没有导致明显的性能下降。这表明，对于较简单的任务，预训练的小幅度权重中嵌入的知识已经足够。然而，对于更具挑战性的任务，允许对所有预训练权重进行适度更新仍然至关重要。
- en: '![Refer to caption](img/5c489ec5f849ef1c36dc3d0fe06ba834.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5c489ec5f849ef1c36dc3d0fe06ba834.png)'
- en: (a) Results of Unstructured Sparsity
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 无结构稀疏性的结果
- en: '![Refer to caption](img/959c8d64bb89573d75603cde9ae4d265.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/959c8d64bb89573d75603cde9ae4d265.png)'
- en: (b) Results of N:M Sparsity (M=8, N ranges from 7 to 1)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (b) N:M 稀疏性的结果 (M=8, N 从 7 到 1)
- en: '![Refer to caption](img/82d5def41d3170adc76775a7e524777a.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/82d5def41d3170adc76775a7e524777a.png)'
- en: 'Figure 6: Across-Task Difficulty via Normalized Human-LLM Performance Gap:
    four different fine-tuning settings with RoBERTa-Large on various downstream tasks.
    Each sub-figure shows a specific downstream task. All performance is normalized
    by the one of Dense Transfer.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：通过归一化的人工-LLM 性能差距衡量任务间难度：在各种下游任务上，使用 RoBERTa-Large 的四种不同微调设置。每个子图显示了一个特定的下游任务。所有性能均通过密集传输中的一个标准化。
- en: 6 Conclusion and Future Work
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this study, we embark on an exploration to validate the prevailing belief
    that deep network weights are excessively redundant, allowing for a substantial
    pruning of parameters without compromising performance. Existing pruning algorithms
    typically operate under the assumption that low-magnitude weights are of limited
    significance and can be safely removed. However, our research presents a compelling
    counter-argument by unearthing the previously overlooked and intricate role of
    small-magnitude weights, closely tied to the difficulty level of downstream tasks.
    Through a comprehensive analysis of these low-magnitude weights, we make a significant
    revelation. Indeed, for certain straightforward tasks, these weights prove to
    be relatively “useless”, making them suitable for pruning without adverse effects
    on performance. However, when it comes to tackling complex tasks, these small-magnitude
    weights carry crucial knowledge. Removing them in such scenarios can lead to irreparable
    damage to the performance of these challenging tasks. As the number of parameters
    in deep networks continues to grow exponentially, our findings prompt the exploration
    of directions such as task-complexity-dependent dynamic inference and network
    self-slimmable properties.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们开始探讨验证当前普遍信念，即深度网络权重过度冗余，从而可以大幅度剪枝参数而不影响性能。现有的剪枝算法通常在假设低幅度权重意义有限且可以安全移除的前提下运行。然而，我们的研究通过揭示以前被忽视的、与下游任务难度密切相关的小幅度权重的复杂作用，提出了令人信服的反驳。通过对这些低幅度权重的全面分析，我们做出了重要的发现。确实，对于某些简单任务，这些权重证明是相对“无用”的，使其适合在不对性能产生负面影响的情况下进行剪枝。然而，当面对复杂任务时，这些小幅度权重携带着关键知识。在这种情况下移除它们可能会对这些具有挑战性的任务的性能造成不可修复的损害。随着深度网络参数数量的指数增长，我们的发现促使探索任务复杂度依赖的动态推理和网络自我瘦身属性等方向。
- en: References
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: OPU (2020) Open source parallel corpus of opus. 2020.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OPU (2020) 开源平行语料库 opus。2020年。
- en: 'Arivazhagan et al. (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry
    Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster,
    Colin Cherry, et al. Massively multilingual neural machine translation in the
    wild: Findings and challenges. *arXiv preprint arXiv:1907.05019*, 2019.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arivazhagan 等 (2019) Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin,
    Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry
    等。大规模多语种神经机器翻译的实践：发现与挑战。*arXiv 预印本 arXiv:1907.05019*，2019年。
- en: 'Carey (2015) Nessa Carey. *Junk DNA: a journey through the dark matter of the
    genome*. Columbia University Press, 2015.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carey (2015) Nessa Carey. *《垃圾DNA：基因组暗物质之旅》*。哥伦比亚大学出版社，2015年。
- en: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, and Michael Carbin. The lottery ticket hypothesis
    for pre-trained bert networks. *Advances in neural information processing systems*,
    33:15834–15846, 2020.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu,
    Yang Zhang, Zhangyang Wang, 和 Michael Carbin. 预训练BERT网络的彩票假设。*神经信息处理系统的进展*，33:15834–15846，2020年。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, 和 Eric P. Xing. Vicuna：一款开源聊天机器人，以90%*chatgpt质量给人印象深刻，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'Daras et al. (2022) Giannis Daras, Negin Raoof, Zoi Gkalitsiou, and Alex Dimakis.
    Multitasking models are robust to structural failure: A neural model for bilingual
    cognitive reserve. *Advances in Neural Information Processing Systems*, 35:35130–35142,
    2022.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daras et al. (2022) Giannis Daras, Negin Raoof, Zoi Gkalitsiou, 和 Alex Dimakis.
    多任务模型对结构性失败具有鲁棒性：一种双语认知储备的神经模型。*神经信息处理系统的进展*，35:35130–35142，2022年。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    Bert：用于语言理解的深度双向变换器的预训练。*arXiv 预印本 arXiv:1810.04805*，2018年。
- en: Dong et al. (2017) Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune
    deep neural networks via layer-wise optimal brain surgeon. *Advances in Neural
    Information Processing Systems*, 30, 2017.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2017) Xin Dong, Shangyu Chen, 和 Sinno Pan. 通过逐层最佳大脑外科医生学习修剪深度神经网络。*神经信息处理系统的进展*，30，2017年。
- en: 'Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    and Erich Elsen. Rigging the lottery: Making all tickets winners. In *International
    Conference on Machine Learning*, pp. 2943–2952\. PMLR, 2020.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evci et al. (2020) Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro,
    和 Erich Elsen. 操控彩票：让所有票据成为赢家。在 *国际机器学习大会*，第2943–2952页。PMLR，2020年。
- en: 'Fernandez-Lopez et al. (2023) Adriana Fernandez-Lopez, Honglie Chen, Pingchuan
    Ma, Alexandros Haliassos, Stavros Petridis, and Maja Pantic. Sparsevsr: Lightweight
    and noise robust visual speech recognition. *arXiv preprint arXiv:2307.04552*,
    2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fernandez-Lopez et al. (2023) Adriana Fernandez-Lopez, Honglie Chen, Pingchuan
    Ma, Alexandros Haliassos, Stavros Petridis, 和 Maja Pantic. Sparsevsr：轻量级和噪声鲁棒的视觉语音识别。*arXiv
    预印本 arXiv:2307.04552*，2023年。
- en: 'Frankle & Carbin (2019) Jonathan Frankle and Michael Carbin. The lottery ticket
    hypothesis: Finding sparse, trainable neural networks. In *International Conference
    on Learning Representations*, 2019. URL [https://openreview.net/forum?id=rJl-b3RcF7](https://openreview.net/forum?id=rJl-b3RcF7).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle & Carbin (2019) Jonathan Frankle 和 Michael Carbin. 彩票假设：寻找稀疏的、可训练的神经网络。在
    *学习表征国际会议*，2019年。网址 [https://openreview.net/forum?id=rJl-b3RcF7](https://openreview.net/forum?id=rJl-b3RcF7)。
- en: Frankle et al. (2020) Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
    and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.
    In *International Conference on Machine Learning*, pp. 3259–3269\. PMLR, 2020.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle et al. (2020) Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
    和 Michael Carbin. 线性模式连通性和彩票假设。在 *国际机器学习大会*，第3259–3269页。PMLR，2020年。
- en: 'Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2023) Elias Frantar 和 Dan Alistarh. Sparsegpt：大规模语言模型可以通过一次性修剪准确地实现，2023年。
- en: 'Frantar et al. (2021) Elias Frantar, Eldar Kurtic, and Dan Alistarh. M-fac:
    Efficient matrix-free approximations of second-order information. *Advances in
    Neural Information Processing Systems*, 34:14873–14886, 2021.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar et al. (2021) Elias Frantar, Eldar Kurtic, 和 Dan Alistarh. M-fac：高效的矩阵无关二阶信息近似。*神经信息处理系统的进展*，34:14873–14886，2021年。
- en: Gale et al. (2019) Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *arXiv preprint arXiv:1902.09574*, 2019.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale et al. (2019) Trevor Gale, Erich Elsen, 和 Sara Hooker. 深度神经网络中的稀疏性现状。*arXiv
    预印本 arXiv:1902.09574*，2019年。
- en: 'Han et al. (2016) Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. In *International Conference on Learning Representations*, 2016.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2016) Song Han, Huizi Mao 和 William J Dally. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。见于
    *国际学习表示会议*，2016年。
- en: 'Hassibi & Stork (1992) Babak Hassibi and David Stork. Second order derivatives
    for network pruning: Optimal brain surgeon. *Advances in neural information processing
    systems*, 5, 1992.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi & Stork (1992) Babak Hassibi 和 David Stork. 网络剪枝的二阶导数：最优脑外科医生。*神经信息处理系统进展*，5，1992年。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean. 提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*，2015年。
- en: 'Jaiswal et al. (2023a) Ajay Jaiswal, Shiwei Liu, Tianlong Chen, and Zhangyang
    Wang. The emergence of essential sparsity in large pre-trained models: The weights
    that matter. *arXiv preprint arXiv:2306.03805*, 2023a.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023a) Ajay Jaiswal, Shiwei Liu, Tianlong Chen 和 Zhangyang Wang.
    大型预训练模型中的关键稀疏性：重要的权重。*arXiv 预印本 arXiv:2306.03805*，2023a年。
- en: Jaiswal et al. (2022) Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding,
    and Zhangyang Wang. Training your sparse neural network better with any mask.
    In *International Conference on Machine Learning*, pp. 9833–9844\. PMLR, 2022.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2022) Ajay Kumar Jaiswal, Haoyu Ma, Tianlong Chen, Ying Ding
    和 Zhangyang Wang. 使用任意掩码更好地训练稀疏神经网络。见于 *国际机器学习会议*，第9833–9844页，PMLR，2022年。
- en: 'Jaiswal et al. (2023b) Ajay Kumar Jaiswal, Shiwei Liu, Tianlong Chen, Ying
    Ding, and Zhangyang Wang. Instant soup: Cheap pruning ensembles in a single pass
    can draw lottery tickets from large models. In *International Conference on Machine
    Learning*, pp. 14691–14701\. PMLR, 2023b.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaiswal et al. (2023b) Ajay Kumar Jaiswal, Shiwei Liu, Tianlong Chen, Ying Ding
    和 Zhangyang Wang. 即时剪枝：一次通过的廉价剪枝集可以从大型模型中抽取彩票票。见于 *国际机器学习会议*，第14691–14701页，PMLR，2023b年。
- en: Jiang et al. (2021) Hao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei,
    Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo, et al. Towards more
    effective and economic sparsely-activated model. *arXiv preprint arXiv:2110.07431*,
    2021.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2021) Hao Jiang, Ke Zhan, Jianwei Qu, Yongkang Wu, Zhaoye Fei,
    Xinyu Zhang, Lei Chen, Zhicheng Dou, Xipeng Qiu, Zikai Guo 等. 朝着更有效且经济的稀疏激活模型迈进。*arXiv
    预印本 arXiv:2110.07431*，2021年。
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    In *Proceedings of the 55th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  1601–1611, 2017.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S Weld 和 Luke Zettlemoyer.
    Triviaqa：一个大规模远程监督的阅读理解挑战数据集。见于 *第55届计算语言学协会年会论文集（第1卷：长篇论文）*，第1601–1611页，2017年。
- en: Ko & Choi (2020) Bowon Ko and Ho-Jin Choi. Twice fine-tuning deep neural networks
    for paraphrase identification. *Electronics Letters*, 56(9):444–447, 2020.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko & Choi (2020) Bowon Ko 和 Ho-Jin Choi. 二次微调深度神经网络用于同义句识别。*电子快报*，56(9):444–447，2020年。
- en: 'Kundu et al. (2021) Souvik Kundu, Mahdi Nazemi, Peter A Beerel, and Massoud
    Pedram. Dnr: A tunable robust pruning framework through dynamic network rewiring
    of dnns. In *Proceedings of the 26th Asia and South Pacific Design Automation
    Conference*, pp.  344–350, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kundu et al. (2021) Souvik Kundu, Mahdi Nazemi, Peter A Beerel 和 Massoud Pedram.
    Dnr: 通过动态网络重连的可调鲁棒剪枝框架。见于 *第26届亚太设计自动化会议论文集*，第344–350页，2021年。'
- en: 'Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin, and Dan Alistarh. The optimal bert
    surgeon: Scalable and accurate second-order pruning for large language models.
    *arXiv preprint arXiv:2203.07259*, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtic et al. (2022) Eldar Kurtic, Daniel Campos, Tuan Nguyen, Elias Frantar,
    Mark Kurtz, Benjamin Fineran, Michael Goin 和 Dan Alistarh. 最优BERT外科医生：用于大型语言模型的可扩展且准确的二阶剪枝。*arXiv
    预印本 arXiv:2203.07259*，2022年。
- en: Lagunas et al. (2021) François Lagunas, Ella Charlaix, Victor Sanh, and Alexander M
    Rush. Block pruning for faster transformers. *arXiv preprint arXiv:2109.04838*,
    2021.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagunas et al. (2021) François Lagunas, Ella Charlaix, Victor Sanh 和 Alexander
    M Rush. 用于加速变换器的块剪枝。*arXiv 预印本 arXiv:2109.04838*，2021年。
- en: LeCun et al. (1990) Yann LeCun, John S Denker, and Sara A Solla. Optimal brain
    damage. In *Advances in neural information processing systems*, pp. 598–605, 1990.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1990) Yann LeCun, John S Denker 和 Sara A Solla. 最优脑损伤。见于 *神经信息处理系统进展*，第598–605页，1990年。
- en: 'Li et al. (2022) Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit Singh
    Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo, et al. Large
    models are parsimonious learners: Activation sparsity in trained transformers.
    *arXiv preprint arXiv:2210.06313*, 2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2022) Zonglin Li, Chong You, Srinadh Bhojanapalli, Daliang Li, Ankit
    Singh Rawat, Sashank J Reddi, Ke Ye, Felix Chern, Felix Yu, Ruiqi Guo 等。大模型是节俭的学习者：训练后的变压器中的激活稀疏性。*arXiv
    预印本 arXiv:2210.06313*，2022。
- en: Liu et al. (2021) Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola
    Pechenizkiy. Do we actually need dense over-parameterization? in-time over-parameterization
    in sparse training. *arXiv preprint arXiv:2102.02887*, 2021.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Shiwei Liu, Lu Yin, Decebal Constantin Mocanu 和 Mykola Pechenizkiy。我们真的需要密集的过度参数化吗？稀疏训练中的即时过度参数化。*arXiv
    预印本 arXiv:2102.02887*，2021。
- en: 'Liu et al. (2022a) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin
    Mocanu, Zhangyang Wang, and Mykola Pechenizkiy. The unreasonable effectiveness
    of random pruning: Return of the most naive baseline for sparse training. *arXiv
    preprint arXiv:2202.02643*, 2022a.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022a) Shiwei Liu, Tianlong Chen, Xiaohan Chen, Li Shen, Decebal
    Constantin Mocanu, Zhangyang Wang 和 Mykola Pechenizkiy。随机剪枝的不可思议的有效性：稀疏训练的最简单基线的回归。*arXiv
    预印本 arXiv:2202.02643*，2022a。
- en: 'Liu et al. (2022b) Shiwei Liu, Yuesong Tian, Tianlong Chen, and Li Shen. Don’t
    be so dense: Sparse-to-sparse gan training without sacrificing performance. *arXiv
    preprint arXiv:2203.02770*, 2022b.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022b) Shiwei Liu, Yuesong Tian, Tianlong Chen 和 Li Shen。不要这么密集：稀疏到稀疏的
    gan 训练而不牺牲性能。*arXiv 预印本 arXiv:2203.02770*，2022b。
- en: 'Liu et al. (2023) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal, and Zhangyang Wang. Sparsity may cry: Let us fail (current)
    sparse neural networks together! *arXiv preprint arXiv:2303.02141*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Shiwei Liu, Tianlong Chen, Zhenyu Zhang, Xuxi Chen, Tianjin
    Huang, Ajay Jaiswal 和 Zhangyang Wang。稀疏性可能会哭泣：让我们一起失败（当前的）稀疏神经网络！*arXiv 预印本 arXiv:2303.02141*，2023。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*,
    2019.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer 和 Veselin Stoyanov。Roberta：一种强健优化的
    bert 预训练方法。*arXiv 预印本 arXiv:1907.11692*，2019。
- en: Liu et al. (2020) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising
    pre-training for neural machine translation. *Transactions of the Association
    for Computational Linguistics*, 8:726–742, 2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2020) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis 和 Luke Zettlemoyer。神经机器翻译的多语言去噪预训练。*计算语言学协会会刊*，8:726–742，2020。
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *arXiv preprint arXiv:2305.11627*,
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Xinyin Ma, Gongfan Fang 和 Xinchao Wang。Llm-pruner：关于大型语言模型的结构化剪枝。*arXiv
    预印本 arXiv:2305.11627*，2023。
- en: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H
    Nguyen, Madeleine Gibescu, and Antonio Liotta. Scalable training of artificial
    neural networks with adaptive sparse connectivity inspired by network science.
    *Nature Communications*, 9(1):2383, 2018.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mocanu et al. (2018) Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong
    H Nguyen, Madeleine Gibescu 和 Antonio Liotta。通过受网络科学启发的自适应稀疏连接进行人工神经网络的可扩展训练。*自然通讯*，9(1):2383，2018。
- en: Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila,
    and Jan Kautz. Pruning convolutional neural networks for resource efficient inference.
    *arXiv preprint arXiv:1611.06440*, 2016.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov et al. (2016) Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila
    和 Jan Kautz。为了资源高效推理而修剪卷积神经网络。*arXiv 预印本 arXiv:1611.06440*，2016。
- en: Mostafa & Wang (2019) Hesham Mostafa and Xin Wang. Parameter efficient training
    of deep convolutional neural networks by dynamic sparse reparameterization. In
    *International Conference on Machine Learning*, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafa & Wang (2019) Hesham Mostafa 和 Xin Wang。通过动态稀疏重新参数化进行深度卷积神经网络的高效训练。发表于
    *国际机器学习大会*，2019。
- en: Mozer & Smolensky (1989) Michael C Mozer and Paul Smolensky. Using relevance
    to reduce network size automatically. *Connection Science*, 1(1):3–16, 1989.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozer & Smolensky (1989) Michael C Mozer 和 Paul Smolensky。利用相关性自动减少网络规模。*连接科学*，1(1):3–16，1989。
- en: 'Nangia & Bowman (2019) Nikita Nangia and Samuel R Bowman. Human vs. muppet:
    A conservative estimate of human performance on the glue benchmark. *arXiv preprint
    arXiv:1905.10425*, 2019.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nangia & Bowman (2019) Nikita Nangia 和 Samuel R Bowman。人类 vs. 木偶：对 glue 基准测试中人类表现的保守估计。*arXiv
    预印本 arXiv:1905.10425*，2019。
- en: Nvidia (2020) Nvidia. Nvidia a100 tensor core gpu architecture. *https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf*,
    2020.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nvidia (2020) Nvidia. Nvidia A100 张量核心 GPU 架构。*https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf*，2020年。
- en: Ohno (1972) Susumu Ohno. So much "junk" dna in our genome. *Brookhaven symposia
    in biology*, 23:366–70, 1972.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ohno (1972) Susumu Ohno. 我们基因组中的“垃圾”DNA。*布鲁克海文生物学研讨会*，23:366–70，1972年。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language
    models. *arXiv preprint arXiv:2302.00083*, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, 和 Yoav Shoham. 上下文检索增强语言模型。*arXiv 预印本 arXiv:2302.00083*，2023年。
- en: Renda et al. (2020) Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing
    rewinding and fine-tuning in neural network pruning. In *8th International Conference
    on Learning Representations*, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renda et al. (2020) Alex Renda, Jonathan Frankle, 和 Michael Carbin. 比较神经网络剪枝中的回滚和微调。发表于
    *第八届国际学习表征会议*，2020年。
- en: 'Sanh et al. (2020) Victor Sanh, Thomas Wolf, and Alexander Rush. Movement pruning:
    Adaptive sparsity by fine-tuning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
    Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  20378–20389\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh et al. (2020) Victor Sanh, Thomas Wolf, 和 Alexander Rush. 运动剪枝：通过微调实现自适应稀疏性。发表于
    H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, 和 H. Lin (编辑)，*神经信息处理系统进展*，第33卷，页码20378–20389。Curran
    Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/eae15aabaa768ae4a5993a8a4f4fa6e4-Paper.pdf)。
- en: 'Singh & Alistarh (2020) Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh & Alistarh (2020) Sidak Pal Singh 和 Dan Alistarh. Woodfisher: 神经网络压缩的高效二阶近似。*神经信息处理系统进展*，33:18098–18109，2020年。'
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter. 一种简单有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023年。
- en: Tang et al. (2020) Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal,
    Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation with extensible
    multilingual pretraining and finetuning. *arXiv preprint arXiv:2008.00401*, 2020.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2020) Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal,
    Vishrav Chaudhary, Jiatao Gu, 和 Angela Fan. 具有可扩展多语言预训练和微调的多语言翻译。*arXiv 预印本 arXiv:2008.00401*，2020年。
- en: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2018) Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, 和 Samuel R Bowman. Glue: 一个用于自然语言理解的多任务基准和分析平台。*arXiv 预印本 arXiv:1804.07461*，2018年。'
- en: 'Wang et al. (2019) Chaoqi Wang, Roger Grosse, Sanja Fidler, and Guodong Zhang.
    Eigendamage: Structured pruning in the kronecker-factored eigenbasis. In *International
    Conference on Machine Learning*, pp. 6566–6575\. PMLR, 2019.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2019) Chaoqi Wang, Roger Grosse, Sanja Fidler, 和 Guodong Zhang.
    Eigendamage: Kronecker 分解的特征基中的结构化剪枝。发表于 *国际机器学习会议*，页码6566–6575。PMLR，2019年。'
- en: Wasserblat (2021) Moshe Wasserblat. Best practices for text classification with
    distillation (part 2/4) – challenging use cases. [https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-24-wasserblat/](https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-24-wasserblat/),
    2021.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserblat (2021) Moshe Wasserblat. 使用蒸馏进行文本分类的最佳实践（第2部分/共4部分）——具有挑战性的用例。 [https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-24-wasserblat/](https://www.linkedin.com/pulse/best-practices-text-classification-distillation-part-24-wasserblat/)，2021年。
- en: Xu et al. (2021) Dongkuan Xu, Ian EH Yen, Jinxi Zhao, and Zhibin Xiao. Rethinking
    network pruning–under the pre-train and fine-tune paradigm. *arXiv preprint arXiv:2104.08682*,
    2021.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2021) Dongkuan Xu, Ian EH Yen, Jinxi Zhao, 和 Zhibin Xiao. 重新思考网络剪枝——在预训练和微调范式下。*arXiv
    预印本 arXiv:2104.08682*，2021年。
- en: Yin et al. (2023) Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang
    Wang, Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, and Shiwei Liu. Dynamic
    sparsity is channel-level sparsity learner. *arXiv preprint arXiv:2305.19454*,
    2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2023）Lu Yin, Gen Li, Meng Fang, Li Shen, Tianjin Huang, Zhangyang Wang,
    Vlado Menkovski, Xiaolong Ma, Mykola Pechenizkiy, 和 Shiwei Liu。动态稀疏性是通道级稀疏性学习器。*arXiv
    预印本 arXiv:2305.19454*，2023。
- en: 'Yuan et al. (2021) Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong,
    Ning Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin, et al. Mest: Accurate
    and fast memory-economic sparse training framework on the edge. *Advances in Neural
    Information Processing Systems*, 34:20838–20850, 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等（2021）Geng Yuan, Xiaolong Ma, Wei Niu, Zhengang Li, Zhenglun Kong, Ning
    Liu, Yifan Gong, Zheng Zhan, Chaoyang He, Qing Jin 等。Mest: 边缘上准确且快速的内存经济稀疏训练框架。*神经信息处理系统进展*，34:20838–20850，2021。'
- en: 'Zafrir et al. (2021) Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, and
    Moshe Wasserblat. Prune once for all: Sparse pre-trained language models. *arXiv
    preprint arXiv:2111.05754*, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir 等（2021）Ofir Zafrir, Ariel Larey, Guy Boudoukh, Haihao Shen, 和 Moshe
    Wasserblat。一次修剪完成所有: 稀疏预训练语言模型。*arXiv 预印本 arXiv:2111.05754*，2021。'
- en: 'Zeng & Urtasun (2018) Wenyuan Zeng and Raquel Urtasun. Mlprune: Multi-layer
    pruning for automated neural network compression. 2018.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng & Urtasun（2018）Wenyuan Zeng 和 Raquel Urtasun。Mlprune: 用于自动神经网络压缩的多层修剪。2018。'
- en: 'Zhang et al. (2022) Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin,
    Pengcheng He, Weizhu Chen, and Tuo Zhao. Platon: Pruning large transformer models
    with upper confidence bound of weight importance. In *International Conference
    on Machine Learning*, pp. 26809–26823\. PMLR, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Qingru Zhang, Simiao Zuo, Chen Liang, Alexander Bukharin, Pengcheng
    He, Weizhu Chen, 和 Tuo Zhao。Platon: 利用权重重要性的上置信界修剪大型变换模型。发表于*国际机器学习会议*，第26809–26823页。PMLR，2022。'
- en: Zheng et al. (2010) Ye Zheng, Steven Josefowicz, Ashutosh Chaudhry, Xiao P Peng,
    Katherine Forbush, and Alexander Y Rudensky. Role of conserved non-coding dna
    elements in the foxp3 gene in regulatory t-cell fate. *Nature*, 463(7282):808–812,
    2010.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2010）Ye Zheng, Steven Josefowicz, Ashutosh Chaudhry, Xiao P Peng, Katherine
    Forbush, 和 Alexander Y Rudensky。保守非编码 DNA 元素在 Foxp3 基因中对调节性 T 细胞命运的作用。*自然*，463(7282):808–812，2010。
- en: 'Zhou et al. (2021) Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang,
    Kun Yuan, Wenxiu Sun, and Hongsheng Li. Learning n: m fine-grained structured
    sparse neural networks from scratch. *arXiv preprint arXiv:2102.04010*, 2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2021）Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun
    Yuan, Wenxiu Sun, 和 Hongsheng Li。从头开始学习 n:m 细粒度结构稀疏神经网络。*arXiv 预印本 arXiv:2102.04010*，2021。
- en: Appendix A Results of N:M sparsity with M=4
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A N:M 稀疏性结果，M=4
- en: '![Refer to caption](img/36c89b7867a21e044907a4a637c5779b.png)![Refer to caption](img/819f771bc08c55757ddaae722a4a1cc3.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/36c89b7867a21e044907a4a637c5779b.png)![参见说明](img/819f771bc08c55757ddaae722a4a1cc3.png)'
- en: 'Figure 7: Task Difficulty Setting 4: Performance of four different fine-tuning
    settings with RoBERTa-Large on various downstream tasks. Each sub-figure showcases
    a specific downstream task. The across-task difficulty is justified by the performance
    gap between humans and models. All performance is normalized with the one of Dense
    Transfer.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 任务难度设置 4: 四种不同微调设置的 RoBERTa-Large 在各种下游任务中的表现。每个子图展示一个特定的下游任务。任务间的难度通过人类与模型之间的性能差距来说明。所有性能都以稠密转移的性能进行归一化。'
- en: '![Refer to caption](img/e85fcb7a7cade337e283d7bef55a609b.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e85fcb7a7cade337e283d7bef55a609b.png)'
- en: 'Figure 8: Task Difficulty Setting 4: Dense Transfer vs. Sparse Transfer using
    RoBERTa-Large on various downstream tasks. Each sub-figure showcases a specific
    downstream task, consisting of an easy dataset and a more challenging one. Across-task
    difficulty is justified by the performance gap between humans and models. Note
    that in the figures, the performance of sparse transfer is normalized by the dense
    transfer performance.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 任务难度设置 4: 在不同下游任务中使用 RoBERTa-Large 进行稠密转移与稀疏转移。每个子图展示一个特定的下游任务，包括一个简单数据集和一个更具挑战性的数据集。任务间的难度通过人类与模型之间的性能差距来说明。请注意，图中的稀疏转移性能是通过稠密转移性能来归一化的。'
- en: '![Refer to caption](img/938d470f05b64be1d8f7c06aa8331d10.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/938d470f05b64be1d8f7c06aa8331d10.png)'
- en: 'Figure 9: Task Difficulty Setting 1: Dense Transfer vs. Sparse Transfer using
    RoBERTa-Base on various downstream tasks. Sparse transfer involves pruning small-magnitude
    weights, which are then frozen as zeros during fine-tuning. Each sub-figure showcases
    a specific downstream task, with various $\%$ of data volume. Within-task difficulty
    is measured by the training data volume. Note that in the figures, the performance
    of sparse transfer is normalized by the dense transfer performance.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：任务难度设置 1：使用 RoBERTa-Base 的密集转移与稀疏转移在各种下游任务上的比较。稀疏转移涉及修剪小幅度权重，这些权重在微调过程中被固定为零。每个子图展示了一个特定的下游任务，数据量各不相同。任务内难度通过训练数据量来衡量。请注意，在图中，稀疏转移的性能是以密集转移的性能为基准进行标准化的。
- en: Appendix B Small-Magnitude Weights Contribute to Loss Basin Preservation
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 小幅度权重有助于损失盆地的保护
- en: We also investigate the potential reasons behind the substantial performance
    drop resulting from the removal of small-magnitude weights on harder tasks. Our
    analysis revolves around the loss landscape, and we conjecture that small-magnitude
    weights play a significantly more crucial role in preserving the loss basin of
    the dense model on harder tasks compared to easier ones. Consequently, the absence
    of these small weights disrupts the optimal basin, leading to a considerable loss
    of performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了导致小幅度权重移除对更难任务造成显著性能下降的潜在原因。我们的分析集中在损失景观上，我们推测小幅度权重在保护密集模型的损失盆地方面比简单任务中更为重要。因此，这些小权重的缺失破坏了最优盆地，导致性能显著下降。
- en: 'To test our conjecture, we utilize the linear mode connectivity (LMC) metric
    proposed by  (Frankle et al., [2020](#bib.bib12)) between the solution produced
    by Sparse Transfer and that of Dense Transfer. Specifically, we perform linear
    interpolation between the fine-tuned model of Dense Transfer ($\bm{\theta}_{d}$.
    We conduct two sets of comparisons, namely QQP vs. STS-B and QNLI vs. RTE, and
    report the performance and loss in Figure [10](#A2.F10 "Figure 10 ‣ Appendix B
    Small-Magnitude Weights Contribute to Loss Basin Preservation ‣ Junk DNA Hypothesis:
    A Task-Centric Angle of LLM Pre-trained Weights through Sparsity"). Our findings
    reveal that both sparse and dense models remain linearly connected, with minimal
    or no increase in loss barrier for easy tasks (i.e., QQP and RTE) when a certain
    portion of small-magnitude weights is removed. However, a significant increase
    in the loss barrier is observed when the same number of weights is removed for
    harder tasks. This observation strongly supports the concept of “Junk DNA”, emphasizing
    the vital role of small-magnitude weights in ensuring that the fine-tuned model
    resides in the optimal basin. In contrast, the removal of these weights would
    lead to the destruction of this optimal basin, which is challenging to fix through
    fine-tuning, causing a notable decline in performance.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的猜想，我们使用了线性模式连接（LMC）度量，该度量由（Frankle 等人，[2020](#bib.bib12)）提出，用于比较稀疏转移产生的解决方案与密集转移产生的解决方案。具体来说，我们在密集转移的微调模型（$\bm{\theta}_{d}$）之间执行线性插值。我们进行了两组比较，即
    QQP 与 STS-B 和 QNLI 与 RTE，并在图 [10](#A2.F10 "图 10 ‣ 附录 B 小幅度权重有助于损失盆地的保护 ‣ 垃圾 DNA
    假说：通过稀疏性对 LLM 预训练权重的任务中心角度") 中报告了性能和损失。我们的发现揭示，无论是稀疏模型还是密集模型在处理简单任务（即 QQP 和 RTE）时，线性连接性保持良好，且移除一定比例的小幅度权重时损失障碍最小或没有增加。然而，当对更难的任务移除相同数量的权重时，损失障碍显著增加。这一观察结果强烈支持“垃圾
    DNA”概念，强调小幅度权重在确保微调模型处于最优盆地中的重要作用。相反，移除这些权重会导致最优盆地的破坏，这种破坏通过微调难以修复，导致性能显著下降。
- en: '![Refer to caption](img/06e1574d00296d3532ab7dd000c456e3.png)![Refer to caption](img/53a3b5a429c897c7ecf530cca3d0ff78.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06e1574d00296d3532ab7dd000c456e3.png)![参考说明](img/53a3b5a429c897c7ecf530cca3d0ff78.png)'
- en: 'Figure 10: Linear interpolation from the Dense Transfer (Left) model to its
    corresponding Sparse Transfer models (Right) on easy and harder tasks (in terms
    of across-task difficulty).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：从密集转移（左）模型到其对应的稀疏转移模型（右）在简单和更难任务上的线性插值（以任务难度为准）。
- en: Appendix C Languages and Statistics for Multilingual Translation
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 多语言翻译中的语言和统计数据
- en: 'Table 2: Languages and Statistics'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：语言和统计数据
- en: . Language translation pairs Majority Minority Code Ru Vi Gu My Languages Russian
    Vietnamese Gujarati Burmese Size/GB (CC25 for pre-training) 278.0 137.3 1.9 1.6
    Size/MB (OPUS-100 for fine-tuning) 116 42 22 2.5
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 语言翻译对 大多数 少数 代码 俄语 越南语 古吉拉特语 缅甸语 大小/GB（CC25用于预训练） 278.0 137.3 1.9 1.6 大小/MB（OPUS-100用于微调）
    116 42 22 2.5
