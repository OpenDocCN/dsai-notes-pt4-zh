- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值和校准集对现代LLMs量化的影响逐渐减小
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20835](https://ar5iv.labs.arxiv.org/html/2405.20835)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20835](https://ar5iv.labs.arxiv.org/html/2405.20835)
- en: Davide Paglieri
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Davide Paglieri
- en: University College London
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Saurabh Dash
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Saurabh Dash
- en: Cohere
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Cohere
- en: Tim Rocktäschel
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Tim Rocktäschel
- en: University College London
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Jack Parker-Holder
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Jack Parker-Holder
- en: University College London d.paglieri@cs.ucl.ac.uk
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院 d.paglieri@cs.ucl.ac.uk
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Post-Training Quantization (PTQ) enhances the efficiency of Large Language Models
    (LLMs) by enabling faster operation and compatibility with more accessible hardware
    through reduced memory usage, at the cost of small performance drops. We explore
    the role of calibration sets in PTQ, specifically their effect on hidden activations
    in various notable open-source LLMs. Calibration sets are crucial for evaluating
    activation magnitudes and identifying outliers, which can distort the quantization
    range and negatively impact performance. Our analysis reveals a marked contrast
    in quantization effectiveness across models. The older OPT model, upon which much
    of the quantization literature is based, shows significant performance deterioration
    and high susceptibility to outliers with varying calibration sets. In contrast,
    newer models like Llama-2 7B, Llama-3 8B, Command-R 35B, and Mistral 7B demonstrate
    strong robustness, with Mistral 7B showing near-immunity to outliers and stable
    activations. These findings suggest a shift in PTQ strategies might be needed.
    As advancements in pre-training methods reduce the relevance of outliers, there
    is an emerging need to reassess the fundamentals of current quantization literature.
    The emphasis should pivot towards optimizing inference speed, rather than primarily
    focusing on outlier preservation, to align with the evolving characteristics of
    state-of-the-art LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）通过减少内存使用来提高大型语言模型（LLMs）的效率，从而实现更快的操作和与更易获取的硬件的兼容，但这也会带来小幅的性能下降。我们探讨了校准集在PTQ中的作用，特别是它们对各种著名开源LLMs中隐藏激活的影响。校准集对于评估激活幅度和识别异常值至关重要，因为这些异常值可能会扭曲量化范围并对性能产生负面影响。我们的分析揭示了不同模型在量化效果上的显著差异。基于大量量化文献的旧版OPT模型显示出显著的性能退化和对异常值的高度敏感，而各种校准集表现不一。相比之下，较新的模型如Llama-2
    7B、Llama-3 8B、Command-R 35B和Mistral 7B表现出强大的鲁棒性，其中Mistral 7B对异常值几乎免疫且激活稳定。这些发现表明，可能需要在PTQ策略上进行调整。随着预训练方法的进步使得异常值的相关性减少，重新评估当前量化文献的基础变得越来越必要。重点应转向优化推理速度，而不是主要关注异常值的保留，以适应最先进LLMs的发展特征。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transformer-based Large Language Models (LLMs) have shown remarkable performance
    which correlates with the number of parameters (Kaplan et al., [2020](#bib.bib22);
    Chowdhery et al., [2023](#bib.bib6); Hoffmann et al., [2022](#bib.bib19); Zhang
    et al., [2022](#bib.bib36)). The growth trend of LLMs memory requirements has
    far outpaced the increase of VRAM in modern day GPUs (Rajbhandari et al., [2021](#bib.bib28)).
    As we grow LLMs further to improve their capabilities, this gap is bound to increase.
    The massive scale of these models hinders their widespread use on easily accessible
    mobile devices.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的大型语言模型（LLMs）展现了显著的性能，这与参数数量相关（Kaplan et al., [2020](#bib.bib22); Chowdhery
    et al., [2023](#bib.bib6); Hoffmann et al., [2022](#bib.bib19); Zhang et al.,
    [2022](#bib.bib36)）。LLMs内存需求的增长趋势远远超过了现代GPU中VRAM的增加（Rajbhandari et al., [2021](#bib.bib28)）。随着我们进一步扩展LLMs以提升其能力，这一差距必将扩大。这些模型的大规模限制了它们在易于获取的移动设备上的广泛使用。
- en: In response to this, there has been a recent wave of smaller open-source high-performing
    models such as Llama, Mistral and Phi (Touvron et al., [2023a](#bib.bib30), [b](#bib.bib31);
    AI@Meta, [2024](#bib.bib2); Jiang et al., [2023](#bib.bib20); Li et al., [2023](#bib.bib24)).
    Their smaller sizes have facilitated broader usage, highlighting the demand for
    more compact models among machine learning practitioners. Furthermore, a growing
    field of research deals with compressing pre-trained LLMs into smaller sizes to
    facilitate their use. Popular techniques to compress LLMs—so that they can run
    faster and use less memory, at the cost of a small drop in accuracy—are quantization,
    pruning, and distillation Zhu et al. ([2023](#bib.bib37)). Applying these techniques
    on already smaller Language Models enables them to be run on widely available
    hardware.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，最近出现了一波较小的高性能开源模型，如Llama、Mistral和Phi（Touvron et al., [2023a](#bib.bib30),
    [b](#bib.bib31); AI@Meta, [2024](#bib.bib2); Jiang et al., [2023](#bib.bib20);
    Li et al., [2023](#bib.bib24)）。它们较小的尺寸促进了更广泛的使用，突显了机器学习从业者对更紧凑模型的需求。此外，研究领域不断扩展，致力于将预训练的LLMs压缩到更小的尺寸以便于使用。压缩LLMs的流行技术——以便它们能够更快运行并使用更少的内存，代价是精度略微下降——包括量化、剪枝和蒸馏（Zhu
    et al., [2023](#bib.bib37)）。对已经较小的语言模型应用这些技术，可以使它们在广泛可用的硬件上运行。
- en: In this paper we specifically consider Post Training Quantization (PTQ) methods,
    which aim to quantize the weights of pre-trained models, usually from BF16 or
    FP16 to INT8 or INT4\. PTQ methods are categorized into zero-shot methods, which
    quantize weights without activation data, and one-shot methods, which use a calibration
    set to better understand how to quantize weights while maintaining performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文特别考虑了后训练量化（PTQ）方法，这些方法旨在将预训练模型的权重量化，通常从BF16或FP16到INT8或INT4。PTQ方法分为零样本方法，这些方法在没有激活数据的情况下量化权重，以及单样本方法，这些方法使用校准集来更好地理解如何在保持性能的同时量化权重。
- en: Among zero-shot quantization methods, some of the simpler Rounding To Nearest
    (RTN) methods fail to work with models bigger than 6.7B on older pre-trained models
    when quantizing both weights and activations (Dettmers et al., [2022](#bib.bib11)).
    This result is attributed to weight and activation outliers, which were initially
    thought to be an emergent property of LLMs at scale. Newer research indicates
    that these outliers are byproducts of training choices common in older LLMs such
    as OPT (Zhang et al., [2022](#bib.bib36)), and the Cohere models should be more
    robust and perform well with simpler quantization techniques (Ahmadian et al.,
    [2023](#bib.bib1)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在零样本量化方法中，一些更简单的四舍五入到最近（RTN）方法在对较旧的预训练模型进行权重和激活量化时，无法处理超过6.7B的模型（Dettmers et
    al., [2022](#bib.bib11)）。这一结果归因于权重和激活值的离群点，最初被认为是大规模LLM的突现属性。较新的研究表明，这些离群点是较旧LLM（如OPT）的训练选择的副产品（Zhang
    et al., [2022](#bib.bib36)），而Cohere模型应该更具鲁棒性，并能在使用更简单的量化技术时表现良好（Ahmadian et al.,
    [2023](#bib.bib1)）。
- en: Closely related to outliers is the use of a calibration set, which is run through
    the model to measure the activation values, and thus quantize more accurately
    by estimating the importance of weights on the activations values, and spotting
    outlier features (Frantar et al., [2022](#bib.bib15); Lin et al., [2023](#bib.bib25);
    Wei et al., [2022](#bib.bib33); Dettmers et al., [2023b](#bib.bib13)). Calibration
    data is usually sampled randomly from web text or from pre-training datasets;
    recently Williams and Aletras ([2023](#bib.bib34)) have investigated the effect
    of the calibration set on downstream task performance, claiming that performance
    can somewhat vary based on the split of the calibration set chosen.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与离群点密切相关的是使用校准集，校准集通过模型进行测量激活值，从而通过估计权重对激活值的重要性和发现离群特征来更准确地量化（Frantar et al.,
    [2022](#bib.bib15); Lin et al., [2023](#bib.bib25); Wei et al., [2022](#bib.bib33);
    Dettmers et al., [2023b](#bib.bib13)）。校准数据通常是从网页文本或预训练数据集中随机抽样的；最近Williams和Aletras（[2023](#bib.bib34)）调查了校准集对下游任务性能的影响，声称性能可能会根据选择的校准集的划分有所不同。
- en: 'We take this a step further and perform controlled experiments on quantization
    perplexity and downstream tasks using distinct calibration sets, varying in quality,
    content and language, and compare the results to the performance achieved with
    "gold-standard" calibration sets. We show that modern open-source LLMs like Llama-2
    7B (Touvron et al., [2023b](#bib.bib31)), Llama-3 8B (AI@Meta, [2024](#bib.bib2)),
    Mistral 7B Jiang et al. ([2023](#bib.bib20)) and bigger Command R 35B (C4AI, [2024](#bib.bib4)),
    when quantized both weight-only and weight-and-activations are significantly more
    robust to the choice of calibration set compared to OPT 6.7B Zhang et al. ([2022](#bib.bib36)).
    In summary our contributions are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步对量化困惑度和下游任务进行受控实验，使用不同质量、内容和语言的校准集进行实验，并将结果与使用“金标准”校准集获得的性能进行比较。我们展示了现代开源
    LLM，如 Llama-2 7B（Touvron et al., [2023b](#bib.bib31)）、Llama-3 8B（AI@Meta, [2024](#bib.bib2)）、Mistral
    7B（Jiang et al., [2023](#bib.bib20)）和更大的 Command R 35B（C4AI, [2024](#bib.bib4)），在量化时，无论是仅量化权重还是同时量化权重和激活，相较于
    OPT 6.7B（Zhang et al., [2022](#bib.bib36)），对校准集的选择具有显著的鲁棒性。总结而言，我们的贡献如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that modern LLMs are notably less affected by the quality, content and
    language of the calibration set compared to an older LLM such as OPT 6.7B.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了现代 LLM 相较于较旧的 LLM，如 OPT 6.7B，对校准集的质量、内容和语言的影响明显较小。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that modern LLMs are less affected by outliers compared to the older
    OPT 6.7B, upon which much of the current knowledge in quantization has been built
    upon.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了现代 LLM 相较于较旧的 OPT 6.7B（当前量化知识大多基于其上），受到离群值的影响较小。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform a thorough analysis of the activation distributions, patterns and
    outliers of the LLMs tested, which help us explain our findings and offer interesting
    insights for future quantization research.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对测试的 LLM 的激活分布、模式和离群值进行了深入分析，这有助于我们解释发现，并为未来的量化研究提供有趣的见解。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose that as newer and better open-source LLMs become available, the quantization
    field should continuously reassess its foundational knowledge on these newer models,
    and drop assumptions made with older models.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议，随着更新更好的开源大型语言模型（LLM）的出现，量化领域应不断重新评估这些新模型的基础知识，并摒弃对旧模型的假设。
- en: 2 Background
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: Quantization reduces the memory and computational requirements of neural networks
    by transforming high-precision weights to lower precision formats. LLMs are usually
    trained using FP16 precision or more recently in BF16 (Kalamkar et al., [2019](#bib.bib21)),
    and are typically quantized to INT8, INT4 or INT3 precisions (Dettmers et al.,
    [2022](#bib.bib11); Frantar et al., [2022](#bib.bib15)), with 4bit found to be
    the sweet spot (Dettmers and Zettlemoyer, [2023](#bib.bib10)). Our focus is on
    Post Training Quantization methods (PTQ), which take a high-precision pre-trained
    model and quantize it, as opposed to Quantization Aware Training (QAT) methods,
    which follow a quantization objective during training.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通过将高精度权重转换为较低精度格式来减少神经网络的内存和计算需求。LLMs 通常使用 FP16 精度进行训练，或最近使用 BF16（Kalamkar
    et al., [2019](#bib.bib21)），并通常量化为 INT8、INT4 或 INT3 精度（Dettmers et al., [2022](#bib.bib11);
    Frantar et al., [2022](#bib.bib15)），其中 4bit 被认为是最佳点（Dettmers and Zettlemoyer,
    [2023](#bib.bib10)）。我们的重点是后训练量化方法（PTQ），即在高精度预训练模型的基础上进行量化，而不是量化感知训练（QAT）方法，这些方法在训练过程中遵循量化目标。
- en: Quantization can be either weight-only (e.g. W4A16) or weight-and-activation
    quantization (e.g. W8A8). Weight-only quantization, as the name suggests, only
    quantizes the weights, then at inference time the weights are dequantized and
    matrix multiplication is performed in 16 bit floating point precision. Weight-and-activation
    quantization methods quantize both weights and activations, performing multiplication
    at lower precision. Weight-only quantization increases inference speed at low
    batch sizes thanks to reduced fetch time from GPU of the quantized weights. Conversely,
    the advantage of weight-and-activation quantization is the absence of a dequantization
    step, allowing for faster throughput of large batch sizes and matrix multiplication
    in the same precision as the weights. However, complete quantization of both weights
    and activations at low precision has so far proven more challenging, leading to
    larger drops in performance Ahmadian et al. ([2023](#bib.bib1)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以是仅权重量化（例如 W4A16）或权重与激活量化（例如 W8A8）。正如其名，仅权重量化只量化权重，在推理时，权重会被解量化，并以 16 位浮点精度执行矩阵乘法。权重与激活量化方法则同时量化权重和激活，执行较低精度的乘法。由于减少了从
    GPU 获取量化权重的时间，仅权重量化在低批量大小时提高了推理速度。相反，权重与激活量化的优势在于没有解量化步骤，从而允许大批量大小的更快吞吐量，并且矩阵乘法与权重采用相同的精度。然而，完整量化权重和激活的低精度方法至今被证明更具挑战性，导致性能显著下降（Ahmadian
    等，[2023](#bib.bib1)）。
- en: Dettmers et al. ([2022](#bib.bib11)) first observed the emergence of extreme
    outliers in the feature dimensions during inference of the range of OPT models
    bigger than 6.7B parameters (Zhang et al., [2022](#bib.bib36)). These outliers
    damage the weight-and-activation quantization performance of simple rounding to
    nearest methods, by skewing the value range before quantization, leading to inefficient
    use of the quantized range. Conversely, weigh-only quantization finds larger models
    easier to quantized than smaller models at low precision (Frantar et al., [2022](#bib.bib15)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Dettmers 等（[2022](#bib.bib11)）首次观察到在推理超过 6.7B 参数的 OPT 模型范围内，特征维度中出现极端异常值（Zhang
    等，[2022](#bib.bib36)）。这些异常值通过在量化前扭曲值范围，破坏了权重与激活量化性能，导致量化范围的低效使用。相反，仅权重量化在低精度下发现较大模型比较小模型更易量化（Frantar
    等，[2022](#bib.bib15)）。
- en: Numerous high-performing weight-only and weight-and-activation quantization
    methods, aim to mitigate the impact of extreme outliers to maintain high performance
    of the quantized model (Dettmers et al., [2022](#bib.bib11), [2023b](#bib.bib13);
    Lin et al., [2023](#bib.bib25); Kim et al., [2023](#bib.bib23)). Dettmers et al.
    ([2022](#bib.bib11)) for example keep the outlier activations in 16-bit floating
    point precision, while SmoothQuant (Xiao et al., [2023](#bib.bib35)), a W8A8 method,
    and AWQ (Lin et al., [2023](#bib.bib25)), a W4A16 method, move the quantization
    difficulty from the activation to the weights, scaling down the activations and
    scaling up the weights in order to make outlier quantization more manageable.
    GPTQ is another prominent weight-only quantization method (Frantar et al., [2022](#bib.bib15))
    that adjusts weights based on activation values using second-order information.
    Several other quantization techniques build on similar concepts as GPTQ (Dettmers
    et al., [2023b](#bib.bib13); Chee et al., [2024](#bib.bib5); Tseng et al., [2024](#bib.bib32)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 许多高性能的仅权重量化和权重与激活量化方法，旨在减轻极端异常值的影响，以保持量化模型的高性能（Dettmers 等，[2022](#bib.bib11),
    [2023b](#bib.bib13); Lin 等，[2023](#bib.bib25); Kim 等，[2023](#bib.bib23)）。例如，Dettmers
    等（[2022](#bib.bib11)）将异常激活保留在 16 位浮点精度下，而 SmoothQuant（Xiao 等，[2023](#bib.bib35)），一种
    W8A8 方法，和 AWQ（Lin 等，[2023](#bib.bib25)），一种 W4A16 方法，将量化难度从激活转移到权重，通过缩小激活值和放大权重，使异常值量化更易于管理。GPTQ
    是另一种突出的仅权重量化方法（Frantar 等，[2022](#bib.bib15)），它基于激活值使用二阶信息来调整权重。其他一些量化技术建立在类似 GPTQ
    的概念之上（Dettmers 等，[2023b](#bib.bib13); Chee 等，[2024](#bib.bib5); Tseng 等，[2024](#bib.bib32)）。
- en: The calibration set, usually a small subset of training data or generic text
    data, assists in this quantization process. By running it through the network,
    activation values can be determined, helping to quantize the weights so that the
    outputs closely match those of the unquantized model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 校准集，通常是训练数据或通用文本数据的小子集，有助于这一量化过程。通过将其传递给网络，可以确定激活值，帮助量化权重，使得输出尽可能接近未量化模型的结果。
- en: 3 Experimental setup
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: 'We set out to examine the impact of the calibration set on the performance
    of various Large Language Models. Specifically, we address three primary questions:
    first, how the quality of the calibration set affects the quantized performance
    of the models; second, whether a content-specific calibration set can enhance
    performance on a particular task; and third, how the same content presented in
    different languages affects the quantized models when used as a calibration set.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们着手检查校准集对各种大型语言模型性能的影响。具体来说，我们解决了三个主要问题：首先，校准集的质量如何影响模型的量化性能；其次，内容特定的校准集是否可以增强某一特定任务的性能；第三，相同的内容以不同语言呈现时，作为校准集使用时对量化模型的影响。
- en: 'We evaluate six distinct LLMs: OPT 6.7B (Zhang et al., [2022](#bib.bib36)),
    Llama-1 7B (Touvron et al., [2023a](#bib.bib30)) Llama-2 7B (Touvron et al., [2023b](#bib.bib31)),
    Llama-3 8B (AI@Meta, [2024](#bib.bib2)), Mistral 7B (Jiang et al., [2023](#bib.bib20))
    and the larger Command-R 35B (C4AI, [2024](#bib.bib4)), to determine their responses
    to varying calibration sets.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了六种不同的LLM：OPT 6.7B (Zhang et al., [2022](#bib.bib36))、Llama-1 7B (Touvron
    et al., [2023a](#bib.bib30))、Llama-2 7B (Touvron et al., [2023b](#bib.bib31))、Llama-3
    8B (AI@Meta, [2024](#bib.bib2))、Mistral 7B (Jiang et al., [2023](#bib.bib20))
    和更大的 Command-R 35B (C4AI, [2024](#bib.bib4))，以确定它们对不同校准集的响应。
- en: 'We test three different one-shot quantization methods: two weight-only quantization
    methods, GPTQ W4A16 with a group size of 128 (Frantar et al., [2022](#bib.bib15))
    and AWQ W4A16 with a group size of 128 (Lin et al., [2023](#bib.bib25)); and SmoothQuant
    W8A8, a weight-and-activation quantization method (Xiao et al., [2023](#bib.bib35)).
    Model performance is measured by evaluating perplexity on WikiText2 (Merity et al.,
    [2016](#bib.bib26)) and downstream zero-shot accuracy on ARC-Challenge (Clark
    et al., [2018](#bib.bib7)), PiQa (Bisk et al., [2020](#bib.bib3)), and Winogrande
    (Sakaguchi et al., [2021](#bib.bib29)), three popular benchmarks that assess abstract
    and common sense reasoning capabilities. Additionally, we test a zero-shot naive
    W8A8 weight-and-activation quantization method.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了三种不同的一次性量化方法：两种仅重量量化方法，GPTQ W4A16（组大小为128，Frantar et al., [2022](#bib.bib15)）和
    AWQ W4A16（组大小为128，Lin et al., [2023](#bib.bib25)）；以及 SmoothQuant W8A8，一种重量和激活量化方法（Xiao
    et al., [2023](#bib.bib35)）。模型性能通过评估 WikiText2（Merity et al., [2016](#bib.bib26)）的困惑度和
    ARC-Challenge（Clark et al., [2018](#bib.bib7)）、PiQa（Bisk et al., [2020](#bib.bib3)）以及
    Winogrande（Sakaguchi et al., [2021](#bib.bib29)）这三个流行基准的零-shot 准确率来衡量，这些基准评估了抽象和常识推理能力。此外，我们还测试了一种零-shot
    的天真 W8A8 重量和激活量化方法。
- en: 3.1 Impact of the Calibration Set Quality on Quantization Effectiveness
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 校准集质量对量化效果的影响
- en: In the first part of our study, we investigate whether the quality of content,
    particularly vocabulary, in the calibration set significantly affects quantization
    quality. We hypothesize that a calibration set with higher quality content will
    yield better performance. To test this, we compare a calibration set sampled from
    RedPajama Computer ([2023](#bib.bib8))—an open-source replica of Llama’s training
    corpus—against a set composed of random ASCII punctuation characters (sample text
    in [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example ‣ Outliers
    and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs")).
    RedPajama represents an appropriate calibration set for quantization due to its
    meaningful and well-curated content, while the random ASCII punctuation set serves
    as a nonsensical calibration set, expected to offer no benefit to quantization
    and potentially be detrimental.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究的第一部分中，我们调查了校准集中内容的质量，特别是词汇，是否显著影响量化质量。我们假设，质量更高的校准集会带来更好的性能。为此，我们将从 RedPajama
    Computer ([2023](#bib.bib8)) 中采样的校准集——Llama 训练语料库的开源复制品——与由随机 ASCII 标点符号组成的校准集进行比较（样本文本见
    [附录 A](#A1 "附录 A 无意义的校准集示例 ‣ 异常值和校准集对现代LLM量化的影响递减")）。RedPajama 代表了一个适合量化的校准集，因为它内容有意义且经过精心策划，而随机
    ASCII 标点符号集则作为无意义的校准集，预计对量化没有益处，甚至可能有害。
- en: 3.2 Impact of Content-Specific Calibration Sets on Specific Downstream Tasks
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 内容特定校准集对特定下游任务的影响
- en: We explore the potential benefits of using content-specific calibration sets
    for performance enhancement. This has practical applications; for instance, if
    a specific downstream task is known, it would be intuitive to calibrate the model
    for that task. For this purpose, we use ARC-Challenge and PiQa as calibration
    sets and compare their effectiveness against RedPajama. Both ARC-Challenge and
    PiQa calibration sets include the full test data, encompassing the questions and
    answers that the LLM is subsequently evaluated on.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了使用内容特定的校准集来提升性能的潜在好处。这具有实际应用；例如，如果已知特定的下游任务，校准模型以适应该任务将是直观的。为此，我们使用ARC-Challenge和PiQa作为校准集，并将它们的效果与RedPajama进行比较。ARC-Challenge和PiQa校准集都包含完整的测试数据，包括LLM随后评估的问答内容。
- en: 3.3 Impact of Different Languages as Calibration Sets on Quantization Effectiveness
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 不同语言作为校准集对量化效果的影响
- en: We extend our analysis to assess how different languages in calibration sets
    impact English perplexity on WikiText2 and downstream accuracy on ARC-Challenge,
    PiQa, and Winogrande. We hypothesize that different languages might induce unique
    activation patterns in LLMs and trigger different outliers, potentially affecting
    performance on English perplexity or downstream tasks. Conversely, robustness
    in an LLM would indicate similar activation patterns and outlier positions across
    languages and tokens. It is important to note that none of the LLMs tested have
    been trained on all the languages used; however, they may have encountered multiple
    languages during pre-training, though some tokens might be encountered very rarely.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展了分析，以评估校准集中不同语言对WikiText2上的英语困惑度和ARC-Challenge、PiQa以及Winogrande上的下游准确性的影响。我们假设不同的语言可能会在LLM中引发独特的激活模式，并触发不同的异常值，可能会影响英语困惑度或下游任务的表现。相反，LLM的稳健性将表明语言和标记之间具有类似的激活模式和异常值位置。需要注意的是，测试过的LLM没有在所有使用的语言上进行训练；然而，它们可能在预训练过程中接触过多种语言，尽管某些标记可能非常少见。
- en: For this analysis, we utilize the FLORES+ dataset (Costa-jussà et al., [2022](#bib.bib9);
    Goyal et al., [2022](#bib.bib17); Guzmán et al., [2019](#bib.bib18); Doumbouya
    et al., [2023](#bib.bib14); Gala et al., [2023](#bib.bib16)), a multi-language
    dataset comprising 2009 sentences translated into 205 different languages across
    30 alphabets. By using FLORES+ translations, we ensure uniform content across
    all calibration sets. Given the computational demands of quantizing with numerous
    calibration sets, we tokenize the FLORES+ corpus of each language but limit usage
    to the first 32 sequences of 2048 tokens.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这项分析，我们利用了FLORES+数据集（Costa-jussà等，[2022](#bib.bib9)；Goyal等，[2022](#bib.bib17)；Guzmán等，[2019](#bib.bib18)；Doumbouya等，[2023](#bib.bib14)；Gala等，[2023](#bib.bib16)），这是一个包含2009个句子的多语言数据集，翻译成205种不同语言，涵盖30种字母表。通过使用FLORES+翻译，我们确保所有校准集中的内容一致。鉴于使用多个校准集的量化计算需求，我们对每种语言的FLORES+语料库进行分词，但仅限于使用前32个2048标记的序列。
- en: 4 Results and Analysis
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果与分析
- en: 4.1 Impact of the Calibration Set Quality on Quantization Effectiveness
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 校准集质量对量化效果的影响
- en: Our analysis reveals significant variations among the tested LLMs concerning
    the impact of calibration set quality on quantized effectiveness. In particular,
    OPT 6.7B demonstrates a markedly worse perplexity in WikiText2 as shown in [Figure 19](#A2.F19
    "Figure 19 ‣ Appendix B Calibration Set Quality Results ‣ Outliers and Calibration
    Sets have Diminishing Effect on Quantization of Modern LLMs"), and average downstream
    accuracy over ARC-Challenge and PIQA ([Figure 19](#A2.F19 "Figure 19 ‣ Appendix
    B Calibration Set Quality Results ‣ Outliers and Calibration Sets have Diminishing
    Effect on Quantization of Modern LLMs")) when quantized using a nonsensical calibration
    set, as opposed to the standard RedPajama. Conversely, the rest of the models
    display high robustness; with their performance not impacted when using a random
    calibration set compared to RedPajama. We show results with AWQ and SmoothQuant
    quantization in [Appendix B](#A2 "Appendix B Calibration Set Quality Results ‣
    Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs").
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析揭示了测试的LLMs在校准集质量对量化效果的影响方面存在显著差异。特别是，OPT 6.7B在WikiText2中的困惑度显著较差，如[图19](#A2.F19
    "图19 ‣ 附录 B 校准集质量结果 ‣ 离群值和校准集对现代LLMs量化的影响递减")所示，而使用无意义校准集时，在ARC-Challenge和PIQA上的平均下游准确性（[图19](#A2.F19
    "图19 ‣ 附录 B 校准集质量结果 ‣ 离群值和校准集对现代LLMs量化的影响递减")）也较差，相较于使用标准RedPajama的情况。相反，其余模型表现出较高的鲁棒性；在使用随机校准集时，相较于RedPajama，其性能没有受到影响。我们在[附录
    B](#A2 "附录 B 校准集质量结果 ‣ 离群值和校准集对现代LLMs量化的影响递减")中展示了AWQ和SmoothQuant量化的结果。
- en: '![Refer to caption](img/90211493fbeb6063c0fec6c5be649d08.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/90211493fbeb6063c0fec6c5be649d08.png)'
- en: 'Figure 1: WikiText2 perplexity with GPTQ 4-bit quantization, using as calibration
    sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical calibration set
    [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example ‣ Outliers and
    Calibration Sets have Diminishing Effect on Quantization of Modern LLMs"). Results
    normalized to RedPajama score. Lower is better.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：使用GPTQ 4-bit量化的WikiText2困惑度，校准集为RedPajama（Computer，[2023](#bib.bib8)）和一个无意义的校准集[附录
    A](#A1 "附录 A 无意义的校准集示例 ‣ 离群值和校准集对现代LLMs量化的影响递减")。结果归一化为RedPajama分数。数值越低越好。
- en: '![Refer to caption](img/4669cf6d4c7c73c2cf607a022023c809.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4669cf6d4c7c73c2cf607a022023c809.png)'
- en: 'Figure 2: Average ARC-Challenge and PIQA accuracy with GPTQ 4-bit quantization,
    using as calibration sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical
    calibration set [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"). Results normalized to RedPajama score. Higher is better. Error bars represent
    standard error.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用GPTQ 4-bit量化的平均ARC-Challenge和PIQA准确性，校准集为RedPajama（Computer，[2023](#bib.bib8)）和一个无意义的校准集[附录
    A](#A1 "附录 A 无意义的校准集示例 ‣ 离群值和校准集对现代LLMs量化的影响递减")。结果归一化为RedPajama分数。数值越高越好。误差条表示标准误差。
- en: The pronounced performance drop observed in OPT 6.7B with the random calibration
    set can be attributed to distinct activation patterns and strong outlier activations.
    We analyze this further in [subsection 4.5](#S4.SS5 "4.5 Activations and outliers
    comparison ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets have Diminishing
    Effect on Quantization of Modern LLMs").
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在OPT 6.7B中观察到的性能显著下降，归因于不同的激活模式和强烈的离群激活。我们在[小节 4.5](#S4.SS5 "4.5 激活和离群值比较 ‣
    4 结果和分析 ‣ 离群值和校准集对现代LLMs量化的影响递减")中进一步分析了这一点。
- en: 'This leads us to the following finding:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致我们得出以下发现：
- en: 'Finding 1: The calibration
    set’s quality does not significantly affect quantized performance of modern Large
    Language Models.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'Finding 1: The calibration
    set’s quality does not significantly affect quantized performance of modern Large
    Language Models.'
- en: 4.2 Impact of Content-Specific Calibration Set on Quantization Effectiveness
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 内容特定的校准集对量化效果的影响
- en: Considering content-specific calibration sets, we find no statistically significant
    difference in downstream accuracies for all models tested compared to RedPajama
    calibration, as shown in [Figure 4](#S4.F4 "Figure 4 ‣ 4.2 Impact of Content-Specific
    Calibration Set on Quantization Effectiveness ‣ 4 Results and Analysis ‣ Outliers
    and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs")
    and [Figure 4](#S4.F4 "Figure 4 ‣ 4.2 Impact of Content-Specific Calibration Set
    on Quantization Effectiveness ‣ 4 Results and Analysis ‣ Outliers and Calibration
    Sets have Diminishing Effect on Quantization of Modern LLMs"). Despite the downstream
    accuracy results of modern LLMs being within the margin of two standard errors,
    ARC-Challenge downstream accuracy shows more pronounced fluctuations in mean accuracy
    compared to PIQA.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在内容特定的标定集方面，我们发现所有测试的模型在下游准确性上与RedPajama标定相比没有统计学上的显著差异，如[图 4](#S4.F4 "Figure
    4 ‣ 4.2 Impact of Content-Specific Calibration Set on Quantization Effectiveness
    ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect
    on Quantization of Modern LLMs")和[图 4](#S4.F4 "Figure 4 ‣ 4.2 Impact of Content-Specific
    Calibration Set on Quantization Effectiveness ‣ 4 Results and Analysis ‣ Outliers
    and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs")所示。尽管现代LLMs的下游准确性结果在两个标准误差的范围内，但ARC-Challenge下游准确性相比于PIQA显示出更明显的平均准确性波动。
- en: '![Refer to caption](img/6c5d6e9b35d2e71e8ea4cc4862e2d8f2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c5d6e9b35d2e71e8ea4cc4862e2d8f2.png)'
- en: 'Figure 3: ARC-Challenge accuracy with GPTQ 4-bit quantization over calibration
    sets. Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用GPTQ 4位量化的ARC-Challenge准确性在标定集上的表现。结果标准化为RedPajama得分。误差条表示标准误差。数值越高越好。
- en: '![Refer to caption](img/0f0dbe934060cda13e8f7ce55cf1142f.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f0dbe934060cda13e8f7ce55cf1142f.png)'
- en: 'Figure 4: PIQA accuracy with GPTQ 4-bit quantization over calibration sets.
    Results normalized to RedPajama score. Error bars represent standard error. Higher
    is better.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：使用GPTQ 4位量化的PIQA准确性在标定集上的表现。结果标准化为RedPajama得分。误差条表示标准误差。数值越高越好。
- en: 'Finding 2: Content-specific
    calibration sets do not show statistically significant improvements to quantized
    model performance on specific downstream tasks compared to content-generic calibration
    sets.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'Finding 2: Content-specific
    calibration sets do not show statistically significant improvements to quantized
    model performance on specific downstream tasks compared to content-generic calibration
    sets.'
- en: 4.3 Effect of Different Languages in Calibration Sets on Quantization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 标定集中的不同语言对量化的影响
- en: '![Refer to caption](img/bb0b28a7206021710f6788a469927dbc.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bb0b28a7206021710f6788a469927dbc.png)'
- en: 'Figure 5: GPTQ W4A16, FP16-Normalized average accuracy (ARC-Challenge, PIQA,
    WinoGrande) of various LLMs, using as calibration sets a selection of languages
    and alphabets. Results sorted by normalized scores of OPT 6.7B. Error bars represent
    standard error'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：GPTQ W4A16，FP16-标准化的各种LLMs的平均准确性（ARC-Challenge, PIQA, WinoGrande），使用语言和字母表的选择作为标定集。结果按OPT
    6.7B的标准化得分排序。误差条表示标准误差。
- en: '![Refer to caption](img/3361c5cfcc22a179456ab38a4f1814e9.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3361c5cfcc22a179456ab38a4f1814e9.png)'
- en: 'Figure 6: AWQ W4A16, FP16-Normalized average accuracy (ARC-Challenge, PIQA,
    WinoGrande) of various LLMs, using as calibration sets a selection of languages
    and alphabets. Results sorted by normalized scores of OPT 6.7B. Error bars represent
    standard error'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：AWQ W4A16，FP16-标准化的各种LLMs的平均准确性（ARC-Challenge, PIQA, WinoGrande），使用语言和字母表的选择作为标定集。结果按OPT
    6.7B的标准化得分排序。误差条表示标准误差。
- en: '![Refer to caption](img/e16767717ca0568b8e418188b596501e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e16767717ca0568b8e418188b596501e.png)'
- en: 'Figure 7: SmoothQuant W8A8, FP16-Normalized average accuracy (ARC-Challenge,
    PIQA, WinoGrande) of various LLMs, using as calibration sets a selection of languages
    and alphabets. Results sorted by normalized scores of OPT 6.7B. Error bars represent
    standard error'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：SmoothQuant W8A8，FP16-标准化的各种LLMs的平均准确性（ARC-Challenge, PIQA, WinoGrande），使用语言和字母表的选择作为标定集。结果按OPT
    6.7B的标准化得分排序。误差条表示标准误差。
- en: We now analyze the results of different languages as calibration sets. We normalize
    the results to 1.0, representing the FP16 result, and visualize the results across
    a selection of languages and alphabets using average downstream task accuracy
    (ARC-Challenge, PIQA and WinoGrande), using GPTQ W4A16 in [Figure 5](#S4.F5 "Figure
    5 ‣ 4.3 Effect of Different Languages in Calibration Sets on Quantization ‣ 4
    Results and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect on
    Quantization of Modern LLMs"), AWQ W4A16 in [Figure 6](#S4.F6 "Figure 6 ‣ 4.3
    Effect of Different Languages in Calibration Sets on Quantization ‣ 4 Results
    and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization
    of Modern LLMs") and SmoothQuant W8A8 in [Figure 7](#S4.F7 "Figure 7 ‣ 4.3 Effect
    of Different Languages in Calibration Sets on Quantization ‣ 4 Results and Analysis
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"). OPT 6.7B is once again the most affected by the choice of the calibration
    set with both GPTQ and AWQ, showing severe performance degradation on most non-Latin-alphabet
    languages.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在分析不同语言作为校准集的结果。我们将结果标准化为 1.0，表示 FP16 结果，并使用平均下游任务准确率（ARC-Challenge、PIQA
    和 WinoGrande）来可视化不同语言和字母表的结果，使用 GPTQ W4A16 在 [图 5](#S4.F5 "图 5 ‣ 4.3 不同语言在校准集上的量化效果
    ‣ 4 结果与分析 ‣ 异常值和校准集对现代 LLM 量化的影响递减")、AWQ W4A16 在 [图 6](#S4.F6 "图 6 ‣ 4.3 不同语言在校准集上的量化效果
    ‣ 4 结果与分析 ‣ 异常值和校准集对现代 LLM 量化的影响递减") 和 SmoothQuant W8A8 在 [图 7](#S4.F7 "图 7 ‣
    4.3 不同语言在校准集上的量化效果 ‣ 4 结果与分析 ‣ 异常值和校准集对现代 LLM 量化的影响递减")。OPT 6.7B 再次是受校准集选择影响最大的一种模型，无论是
    GPTQ 还是 AWQ，显示出在大多数非拉丁字母语言上严重的性能下降。
- en: On the other hand, the rest of the more modern models tested exhibit significantly
    better resilience. With SmoothQuant W8A8, all the calibration sets perform within
    the standard error of each other, including OPT 6.7B, likely because it uses 8
    bits for weight quantization instead of 4 bits, which is not a particularly challenging
    quantization scheme despite also quantizing the activations. However, with lower
    bit weight-and-activation quantization, OPT would likely show worse degradation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，其它更现代的测试模型表现出显著更好的鲁棒性。使用 SmoothQuant W8A8 时，所有校准集的表现都在彼此的标准误差范围内，包括 OPT
    6.7B，可能是因为它使用 8 位权重量化而不是 4 位，尽管激活也被量化，但这并不是特别具有挑战性的量化方案。然而，使用较低位数的权重和激活量化时，OPT
    可能会显示出更糟的性能下降。
- en: 'Finding 3: Different languages
    from English as calibration sets do not affect quantized performance of modern
    Large Language Models.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 'Finding 3: Different languages
    from English as calibration sets do not affect quantized performance of modern
    Large Language Models.'
- en: 4.4 Results with Naive W8A8 Quantization
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 使用原始 W8A8 量化的结果
- en: Lastly, we replicate the experiment from Dettmers et al. ([2022](#bib.bib11))
    which showed degradation when naively performing weight-and-activation quantization
    of OPT models of size 6.7B and bigger due to extreme outliers. We perform naive
    zero-shot W8A8 quantization using per-channel weight quantization and per-token
    activation quantization with absmax, and show that OPT 6.7B is the only model
    of the ones tested whose extreme outliers degrade its performance, while even
    the bigger Command-R 35B (C4AI, [2024](#bib.bib4)) shows close to no performance
    degradation. This confirms the results from Ahmadian et al. ([2023](#bib.bib1)),
    which showed they could naively quantize W8A8 newly trained Cohere models all
    the way up to 50B parameters, and points to the fact that outliers are not necessarily
    an emergent-property at scale, but rather a by-product of training. We discuss
    what kind of training decision may have led to these differences in [section 5](#S5
    "5 Discussion and Related Work ‣ Outliers and Calibration Sets have Diminishing
    Effect on Quantization of Modern LLMs").
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们复制了 Dettmers 等人（[2022](#bib.bib11)）的实验，该实验显示，当对 6.7B 及更大规模的 OPT 模型进行原始的权重和激活量化时，由于极端异常值而导致性能下降。我们进行原始的零样本
    W8A8 量化，采用每通道权重量化和每标记激活量化（使用 absmax），结果显示 OPT 6.7B 是测试模型中唯一一个因极端异常值而性能下降的模型，而即使是更大的
    Command-R 35B（C4AI，[2024](#bib.bib4)）也几乎没有性能下降。这证实了 Ahmadian 等人（[2023](#bib.bib1)）的结果，他们显示可以原始量化
    W8A8 新训练的 Cohere 模型直到 50B 参数，这表明异常值并非在大规模下的突现特性，而是训练的副产品。我们讨论了可能导致这些差异的训练决策，详见
    [第 5 节](#S5 "5 讨论与相关工作 ‣ 异常值和校准集对现代 LLM 量化的影响递减")。
- en: '![Refer to caption](img/b14b3c89062b92cca4828f7a67d33c3b.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b14b3c89062b92cca4828f7a67d33c3b.png)'
- en: 'Figure 8: WikiText2 perplexity with naive W8A8 quantization. Results normalized
    by FP16 value. Lower is better.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用原始 W8A8 量化的 WikiText2 困惑度。结果以 FP16 值为标准化。数值越低越好。
- en: '![Refer to caption](img/446164e88713333647ade76c33da8d24.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/446164e88713333647ade76c33da8d24.png)'
- en: 'Figure 9: Average accuracy (ARC-C, PIQA, WinoGrande) with W8A8 naive quantization.
    Results normalized by FP16 value. Error bars represent standard error. Higher
    is better.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：W8A8天真的量化下的平均准确率（ARC-C、PIQA、WinoGrande）。结果经过FP16值归一化。误差条表示标准误差。数值越高越好。
- en: 4.5 Activations and outliers comparison
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 激活和异常值比较
- en: '![Refer to caption](img/fca7ac7b717fd3d7ef4d82aeb35ab4a3.png)![Refer to caption](img/4257273517a8d6216fa64d508f759fd6.png)![Refer
    to caption](img/72276b6437ad8154f9d7137bbaa60d6e.png)![Refer to caption](img/226c34c85acd508394dbdba0e79219c8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fca7ac7b717fd3d7ef4d82aeb35ab4a3.png)![参见说明](img/4257273517a8d6216fa64d508f759fd6.png)![参见说明](img/72276b6437ad8154f9d7137bbaa60d6e.png)![参见说明](img/226c34c85acd508394dbdba0e79219c8.png)'
- en: '![Refer to caption](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: 'Figure 10: Average activation distribution of all the attention output projection
    layers and last mlp layers for OPT6.7B, LLaMa-2 7B, and Mistral 7B, for English
    text (on the left) and Mandarin Chinese text (on the right)'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：OPT6.7B、LLaMa-2 7B和Mistral 7B的所有注意力输出投影层和最后MLP层的平均激活分布，左侧为英文文本，右侧为中文文本
- en: '![Refer to caption](img/ac5146353466dad0d56c8eff51f38b51.png)![Refer to caption](img/98637bab6b4d584e7f5b8950cee33f3e.png)![Refer
    to caption](img/79a56a1357108fed534e9d0f2add72a1.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac5146353466dad0d56c8eff51f38b51.png)![参见说明](img/98637bab6b4d584e7f5b8950cee33f3e.png)![参见说明](img/79a56a1357108fed534e9d0f2add72a1.png)'
- en: 'Figure 11: Visualisation of the top and bottom 1% of the activation values
    of attention output projection layers and last fully connected layers of OPT 6.7B
    (on the left), and Llama-1 7B (on the right) when running inference on English
    text'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在对英语文本进行推理时，OPT 6.7B（左侧）和Llama-1 7B（右侧）注意力输出投影层和最后全连接层激活值的前1%和后1%的可视化
- en: To gain a deeper understanding of the performance of quantized models and the
    mechanics of calibration sets, we conduct a thorough analysis of activation distributions
    and patterns within the attention output projection layers and the final fully
    connected linear layer across all the layers of the unquantized LLMs tested. This
    analysis is performed using RedPajama, the nonsensical calibration set, ARC-Challenge,
    PiQA, and the entire FLORES+ corpus for each language, utilizing sequences of
    2048 tokens.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入理解量化模型的性能和标定集的机制，我们对所有测试的未量化LLMs的激活分布和模式进行了彻底的分析。这些分析包括在所有层次的注意力输出投影层和最终全连接线性层中的激活分布和模式，并使用了RedPajama、无意义的标定集、ARC-Challenge、PiQA和每种语言的完整FLORES+语料库，利用2048个tokens的序列。
- en: First, we analyze the activation distributions over a small range around 0\.
    Mistral 7B consistently exhibits a much narrower activation distribution than
    all the Llama models and OPT 6.7B in all languages tested. The larger Command-R
    35B model shows a wider base distribution than the rest of the models. We also
    observe progressively narrower distributions in the LLMs developed by Meta, from
    OPT 6.7B to LLaMa-1, LLaMa-2, and LLaMa-3 being the most well-behaved. We also
    note a broader spread in the activation distributions for non-English languages,
    with OPT 6.7B and Llama-1 7B showing the widest distribution among the smaller
    models, Llama-2/3 models occupying intermediate positions, and Mistral 7B maintaining
    a consistently narrow distribution across all languages. In [Figure 10](#S4.F10
    "Figure 10 ‣ 4.5 Activations and outliers comparison ‣ 4 Results and Analysis
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"), we compare the activation distributions of English and Mandarin Chinese.
    Mandarin Chinese was selected for its widespread use, distinct non-Latin alphabet,
    and likely inclusion in the models’ pre-training. A more comprehensive list of
    distributions is shown in [Appendix E](#A5 "Appendix E Activation Distributions
    Plots ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization
    of Modern LLMs").
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们分析了接近 0 的小范围内的激活分布。Mistral 7B 在所有测试语言中表现出比所有 Llama 模型和 OPT 6.7B 更狭窄的激活分布。较大的
    Command-R 35B 模型显示出比其他模型更宽的基础分布。我们还观察到 Meta 开发的 LLM 中，从 OPT 6.7B 到 LLaMa-1、LLaMa-2
    和 LLaMa-3，分布逐渐变窄，LLaMa-3 表现得最为稳定。我们还注意到非英语语言的激活分布较广，OPT 6.7B 和 Llama-1 7B 在较小模型中显示出最广泛的分布，Llama-2/3
    模型处于中间位置，而 Mistral 7B 在所有语言中维持了一致的狭窄分布。在[图 10](#S4.F10 "Figure 10 ‣ 4.5 Activations
    and outliers comparison ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets
    have Diminishing Effect on Quantization of Modern LLMs")中，我们比较了英语和普通话的激活分布。普通话因其广泛使用、独特的非拉丁字母以及可能在模型预训练中包含而被选中。更全面的分布列表见[附录
    E](#A5 "Appendix E Activation Distributions Plots ‣ Outliers and Calibration Sets
    have Diminishing Effect on Quantization of Modern LLMs")。
- en: We then further inspect the activation patterns of the aforementioned layer
    of the unquantized OPT, LLaMa, Mistral and Command-R models. Specifically, we
    compute the average activations across all sequences, then identify the top and
    bottom 1% percent of activations values. Additionally, we perform min/max pooling
    with kernel size of 32 (64 for Command-R 35B) along the hidden dimension, facilitating
    a clearer visualization of the hidden dimensions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步检查了上述未量化的 OPT、LLaMa、Mistral 和 Command-R 模型的激活模式。具体来说，我们计算了所有序列的平均激活值，然后识别激活值的前
    1% 和后 1%。此外，我们在隐藏维度上执行了大小为 32（Command-R 35B 为 64）的最小/最大池化，以便更清晰地可视化隐藏维度。
- en: We compare the activation patterns of English text across all the models in
    [Figure 11](#S4.F11 "Figure 11 ‣ 4.5 Activations and outliers comparison ‣ 4 Results
    and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization
    of Modern LLMs"), [Figure 12](#S4.F12 "Figure 12 ‣ 4.5 Activations and outliers
    comparison ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets have Diminishing
    Effect on Quantization of Modern LLMs"), and [Figure 13](#S4.F13 "Figure 13 ‣
    4.5 Activations and outliers comparison ‣ 4 Results and Analysis ‣ Outliers and
    Calibration Sets have Diminishing Effect on Quantization of Modern LLMs"). Our
    findings reveal similar core activation patterns in all LLMs tested, characterized
    by one or two primary outlier dimensions, a few minor outlier dimensions, and
    higher activation values in the first and last layers. The activation patterns
    of all the models with various languages, RedPajama, nonsensical text, ARC-Challenge,
    and PiQa are visualized in [Appendix D](#A4 "Appendix D Activations and Outlier
    Patterns Plots ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization
    of Modern LLMs").
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 11](#S4.F11 "Figure 11 ‣ 4.5 Activations and outliers comparison ‣ 4 Results
    and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization
    of Modern LLMs")、[图 12](#S4.F12 "Figure 12 ‣ 4.5 Activations and outliers comparison
    ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets have Diminishing Effect
    on Quantization of Modern LLMs")和[图 13](#S4.F13 "Figure 13 ‣ 4.5 Activations and
    outliers comparison ‣ 4 Results and Analysis ‣ Outliers and Calibration Sets have
    Diminishing Effect on Quantization of Modern LLMs")中比较了所有模型中英文文本的激活模式。我们的发现揭示了所有测试的
    LLM 中具有类似的核心激活模式，其特征是一个或两个主要的异常维度、几个次要的异常维度，以及在第一层和最后一层的较高激活值。各种语言的所有模型、RedPajama、无意义文本、ARC-Challenge
    和 PiQa 的激活模式可在[附录 D](#A4 "Appendix D Activations and Outlier Patterns Plots ‣
    Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs")中查看。
- en: Overall, we find that OPT 6.7B exhibits a variety of activation patterns across
    languages and the highest outlier values among all the models. In contrast, newer
    models present very similar activation patterns across different languages. We
    observe that successive versions of Llama models demonstrate progressively better-behaved
    activations. Mistral 7B has the smallest maximum outliers. Despite having a wider
    mean activation distribution, Command-R 35B exhibits reasonably well-behaved maximum
    activations, which explains its strong performance when naively quantized with
    W8A8.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们发现OPT 6.7B在不同语言中表现出多样的激活模式，并且在所有模型中具有最高的异常值。相比之下，更新的模型在不同语言中表现出非常相似的激活模式。我们观察到，Llama模型的连续版本显示出逐步改进的激活表现。Mistral
    7B的最大异常值最小。尽管具有更广泛的平均激活分布，Command-R 35B的最大激活表现仍然相对良好，这解释了其在使用W8A8进行简单量化时的强劲表现。
- en: '![Refer to caption](img/401220bf867576122f751e36f447ed5f.png)![Refer to caption](img/3191fa1e705ffab4aaa5751d1994fe53.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/401220bf867576122f751e36f447ed5f.png)![参考说明](img/3191fa1e705ffab4aaa5751d1994fe53.png)'
- en: 'Figure 12: Visualisation of the top and bottom 1% of the activation values
    of attention output projection layers and last fully connected layers of Llama-2
    7B (on the left), and Llama-3 8B (on the right) when running inference on English
    text'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：Llama-2 7B（左侧）和Llama-3 8B（右侧）在对英文文本进行推理时，注意力输出投影层和最后全连接层的激活值前1%和后1%的可视化
- en: '![Refer to caption](img/7f19008e7271a7f85da72e2b3e19c807.png)![Refer to caption](img/7263ddf1df31c17656531596f4acfe11.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7f19008e7271a7f85da72e2b3e19c807.png)![参考说明](img/7263ddf1df31c17656531596f4acfe11.png)'
- en: 'Figure 13: Visualisation of the top and bottom 1% of the activation values
    of attention output projection layers and last fully connected layers of Mistral
    7B (on the left), and Command-R 35B (on the right) when running inference on English
    text'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：Mistral 7B（左侧）和Command-R 35B（右侧）在对英文文本进行推理时，注意力输出投影层和最后全连接层的激活值前1%和后1%的可视化
- en: 5 Discussion and Related Work
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论与相关工作
- en: Recent advancements in quantization methodologies for Large Language Models
    (LLMs) have shifted our understanding of the role of outliers in these models.
    Outliers were originally thought to be an emerging property of LLMs at scale (Dettmers
    et al., [2022](#bib.bib11)). This view, however, has been challenged by the findings
    of Ahmadian et al. ([2023](#bib.bib1)), which suggested that such outliers are
    not intrinsic emergent properties, but rather by-products of specific pre-training
    methodologies. Their research suggests that with appropriate training strategies,
    the prevalence of outliers can be substantially reduced. Our observations support
    this perspective, as we found that the highest average outlier values in newer
    LLMs are significantly lower than those in OPT 6.7B. Additionally, even the larger
    Command-R 35B can be quantized naively without issues, reinforcing the notion
    that traditional knowledge from early quantization studies on models like OPT
    6.7B may not apply to modern LLMs pre-trained with newer strategies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大规模语言模型（LLMs）量化方法的进展中，我们对这些模型中异常值的作用有了新的理解。异常值最初被认为是大规模LLMs的一个新兴特性（Dettmers
    et al., [2022](#bib.bib11)）。然而，Ahmadian et al.（[2023](#bib.bib1)）的研究挑战了这一观点，指出这些异常值并非固有的自发特性，而是特定预训练方法的副产品。他们的研究表明，通过适当的训练策略，可以显著减少异常值的出现。我们的观察支持这一观点，因为我们发现较新的LLMs中最高的平均异常值显著低于OPT
    6.7B。此外，即使是较大的Command-R 35B也可以简单量化而没有问题，这进一步巩固了传统的早期量化研究知识（如OPT 6.7B）的结论可能不适用于使用更新策略进行预训练的现代LLMs。
- en: A fundamental question is understanding the reason for the poor quantization
    performance of OPT 6.7B. Ahmadian et al. ([2023](#bib.bib1)) demonstrated that
    outliers in their Cohere models could be controlled by employing higher weight
    decay, lower dropout, gradient clipping, and using bfloat16 (Kalamkar et al.,
    [2019](#bib.bib21)) instead of FP16\. We hypothesize that the high occurrence
    of extreme outliers in OPT 6.7B is primarily due to its use of FP16 rather than
    bfloat16 (as disclosed in Metaseq ([2022](#bib.bib27))), while the other models
    we tested were trained with bfloat16, which was found to be a more robust data
    type than FP16 (Kalamkar et al., [2019](#bib.bib21)) and has seen widespread adoption
    in recent years.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一个基本的问题是理解OPT 6.7B量化性能差的原因。Ahmadian 等人 ([2023](#bib.bib1)) 证明，通过使用更高的权重衰减、较低的
    dropout、梯度裁剪，并使用 bfloat16 (Kalamkar et al., [2019](#bib.bib21)) 代替 FP16，可以控制 Cohere
    模型中的异常值。我们假设，OPT 6.7B 中极端异常值的高发生率主要是由于其使用 FP16 而非 bfloat16 (如 Metaseq ([2022](#bib.bib27)))
    所揭示的，而我们测试的其他模型则使用 bfloat16，该数据类型被发现比 FP16 (Kalamkar et al., [2019](#bib.bib21))
    更具鲁棒性，并在近年来得到了广泛应用。
- en: Williams and Aletras ([2023](#bib.bib34)) conducted the first empirical study
    on influence of calibration sets on LLM quantization, suggesting that the calibration
    data impacts the effectiveness of pruning and quantization techniques. Their findings
    seem to indicate variations in Llama-1 7B (Touvron et al., [2023a](#bib.bib30))
    downstream task performance based on calibration data used. Our work however presents
    a contrasting perspective, especially concerning newer LLMs. We observed that
    models like Mistral 7B (Jiang et al., [2023](#bib.bib20)) and Llama-2/3 7B/8B
    (Touvron et al., [2023b](#bib.bib31); AI@Meta, [2024](#bib.bib2)) exhibit a significantly
    lower sensitivity to the nature of the calibration set compared to OPT 6.7B (Zhang
    et al., [2022](#bib.bib36)). Furthermore, it is worth noting that the performance
    variations reported by Williams and Aletras ([2023](#bib.bib34)) with different
    sampled calibration sets mostly fall within two standard deviations of each other,
    questioning the statistical significance of their results.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Williams 和 Aletras ([2023](#bib.bib34)) 进行了一项关于校准集对LLM量化影响的首个实证研究，建议校准数据会影响剪枝和量化技术的有效性。他们的研究结果似乎表明，基于使用的校准数据，Llama-1
    7B (Touvron et al., [2023a](#bib.bib30)) 的下游任务表现存在差异。然而，我们的工作提出了一个对比视角，特别是对于较新的LLM。我们观察到，像
    Mistral 7B (Jiang et al., [2023](#bib.bib20)) 和 Llama-2/3 7B/8B (Touvron et al.,
    [2023b](#bib.bib31); AI@Meta, [2024](#bib.bib2)) 相比于 OPT 6.7B (Zhang et al., [2022](#bib.bib36))
    对校准集的性质表现出明显较低的敏感性。此外，值得注意的是，Williams 和 Aletras ([2023](#bib.bib34)) 报告的不同采样校准集的性能变化大多数在两个标准差之内，这对他们结果的统计显著性提出了质疑。
- en: Our findings suggest that advancements in LLM architectures and training methodologies
    may alter previously held notions about outliers and the impact of calibration
    data. As the field of quantization evolves, it becomes increasingly important
    to reevaluate foundational assumptions and understand how newer models differ
    from their predecessors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，LLM架构和训练方法的进步可能会改变对异常值和校准数据影响的先前看法。随着量化领域的发展，重新评估基础假设并理解较新模型如何与前身不同变得越来越重要。
- en: Looking ahead, the role of outlier research is likely to remain important for
    some time. Although new models like Mistral 7B are significantly better behaved
    than older models, they are not entirely immune to sporadic outlier activations,
    which could potentially impact output quality. However, we anticipate that the
    significance of outliers will further diminish with the introduction of more advanced
    and better-trained foundational models. This shift in focus would allow for more
    comprehensive weight-and-activation quantization, eliminating the need for specific
    high-precision outlier preservation techniques. Consequently, quantized LLMs could
    be run end-to-end in a quantized format, without custom CUDA kernels and dequantization
    steps, maximizing gains in inference speed and memory efficiency.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，异常值研究的作用可能会在一段时间内继续重要。尽管像 Mistral 7B 这样的新模型表现比旧模型显著更好，但它们并非完全免疫偶发异常激活，这可能会影响输出质量。然而，我们预期，随着更先进和更好训练的基础模型的引入，异常值的显著性将进一步降低。这种焦点的转变将允许更全面的权重和激活量化，消除对特定高精度异常值保护技术的需求。因此，量化LLM可以以量化格式从头到尾运行，无需定制CUDA内核和反量化步骤，从而最大化推理速度和内存效率的收益。
- en: 6 Limitations and Future Work
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制与未来工作
- en: The main limitation of our study stems from the constrained scope of our experiments,
    which were restricted to a select range of LLMs and excluded larger models due
    to limited computational resources; most of our experiments were conducted on
    four L4 GPUs (24GB VRAM each). Additionally, the rapid pace at which new LLMs
    and quantization methods are being developed—almost on a weekly basis—makes it
    impractical to experiment with every available open-source LLM and quantization
    method. Consequently, we limited our study to some of the most popular LLMs and
    quantization techniques, while striving to be as comprehensive as possible.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的主要限制在于实验范围受限，仅限于某些选定的LLM，并且由于计算资源有限，排除了较大的模型；我们的大多数实验在四台L4 GPU（每台24GB VRAM）上进行。此外，新LLM和量化方法的快速发展—几乎每周都有—使得对每种可用的开源LLM和量化方法进行实验变得不切实际。因此，我们将研究范围限制在一些最受欢迎的LLM和量化技术上，同时努力做到尽可能全面。
- en: For future research, it would be interesting to explore new low-precision weight-and-activation
    quantization techniques across various models, with particular focus on assessing
    their performance on models like Mistral 7B. Additionally, it would be interesting
    to test Round To Nearest techniques utilizing the new 4-bit Normal Float (NF4)
    format proposed in QLoRa (Dettmers et al., [2023a](#bib.bib12)), for both weight-and-activation
    quantization with Mistral 7B, given its well-behaved activations.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未来的研究，探索在各种模型中应用新的低精度权重和激活量化技术将是很有趣的，特别是评估它们在Mistral 7B等模型上的表现。此外，测试利用QLoRa（Dettmers等人，[2023a](#bib.bib12)）中提出的新4位浮点（NF4）格式的“最接近舍入”技术进行权重和激活量化也是很有趣的，因为Mistral
    7B的激活表现良好。
- en: 7 Conclusion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We present an investigation into the effect of calibration sets and the role
    of outliers in one-shot Post Training Quantization methods, specifically analyzing
    OPT 6.7B, Llama-1/2/3 (7B/7B/8B), Mistral 7B, and Command R 35B. Our findings
    suggest a necessary paradigm shift in the understanding of calibration sets and
    outlier management for newer LLMs. Notably, while the older OPT 6.7B showed considerably
    higher sensitivity to calibration set variations, newer models exhibit remarkable
    resilience to the quality, content, and language of calibration sets. Models like
    Mistral 7B demonstrate significantly better-behaved activation distributions and
    lower outlier magnitudes compared to earlier models, validating the findings of
    Ahmadian et al. ([2023](#bib.bib1)) that outliers are not intrinsic properties
    of LLMs at scale but by-products of training methods. Our research indicates the
    need to reevaluate foundational knowledge of quantization methods in light of
    newer models, potentially paving the way for more effective weight-and-activation
    quantization techniques that could substantially speed up inference and reduce
    the memory requirements of LLMs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对校准集的影响以及异常值在一次性后训练量化方法中的作用进行了调查，特别分析了OPT 6.7B、Llama-1/2/3（7B/7B/8B）、Mistral
    7B和Command R 35B。我们的发现表明，需要在理解校准集和异常值管理方面进行必要的范式转变，以适应较新的LLM。值得注意的是，尽管较旧的OPT 6.7B对校准集变化表现出显著更高的敏感性，但较新的模型在校准集的质量、内容和语言方面表现出显著的韧性。像Mistral
    7B这样的模型相比早期模型显示出显著更好的激活分布和较低的异常值幅度，验证了Ahmadian等人（[2023](#bib.bib1)）的发现，即异常值并不是大规模LLM的内在特性，而是训练方法的副产品。我们的研究表明，需要根据较新模型重新评估量化方法的基础知识，这可能为更有效的权重和激活量化技术铺平道路，从而显著加速推理并减少LLM的内存需求。
- en: References
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ahmadian et al. (2023) Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh,
    Stephen Gou, Phil Blunsom, Ahmet Üstün, and Sara Hooker. Intriguing properties
    of quantization at scale. *arXiv preprint arXiv:2305.19268*, 2023.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmadian等人（2023）Arash Ahmadian, Saurabh Dash, Hongyu Chen, Bharat Venkitesh,
    Stephen Gou, Phil Blunsom, Ahmet Üstün, 和Sara Hooker。《大规模量化的有趣特性》。*arXiv预印本arXiv:2305.19268*，2023年。
- en: AI@Meta (2024) AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta（2024）AI@Meta。《Llama 3模型卡》。2024年。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk等人（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi等人。《Piqa:
    Reasoning about physical commonsense in natural language》。在*AAAI人工智能会议论文集*中，第34卷，第7432–7439页，2020年。'
- en: C4AI (2024) C4AI. Model card for c4ai command-r. 2024. URL [https://huggingface.co/CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01).
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C4AI (2024) C4AI. C4AI Command-R 模型卡. 2024. URL [https://huggingface.co/CohereForAI/c4ai-command-r-v01](https://huggingface.co/CohereForAI/c4ai-command-r-v01).
- en: 'Chee et al. (2024) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等 (2024) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, 和 Christopher M De
    Sa. Quip: 大型语言模型的 2 位量化及其保证. *神经信息处理系统进展*, 36, 2024.'
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *Journal
    of Machine Learning Research*, 24(240):1–113, 2023.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等 (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等. Palm: 通过路径扩展语言建模. *机器学习研究杂志*, 24(240):1–113, 2023.'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战.
    *arXiv 预印本 arXiv:1803.05457*, 2018.
- en: 'Computer (2023) Together Computer. Redpajama: an open dataset for training
    large language models, 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Computer (2023) Together Computer. Redpajama: 一个用于训练大型语言模型的开放数据集, 2023. URL
    [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).'
- en: 'Costa-jussà et al. (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine
    translation. *arXiv preprint arXiv:2207.04672*, 2022.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-jussà 等 (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
    Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
    Maillard 等. 无语言被遗忘：扩展以人为中心的机器翻译. *arXiv 预印本 arXiv:2207.04672*, 2022.
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. The case
    for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR, 2023.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 和 Zettlemoyer (2023) Tim Dettmers 和 Luke Zettlemoyer. 4 位精度的案例：k 位推理扩展定律.
    载于 *国际机器学习会议*, 页 7750–7774\. PMLR, 2023.
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    Llm. int8 (): 大规模变换器的 8 位矩阵乘法. *arXiv 预印本 arXiv:2208.07339*, 2022.'
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023a.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer.
    Qlora: 高效的量化 llms 微调. *arXiv 预印本 arXiv:2305.14314*, 2023a.'
- en: 'Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression. *arXiv preprint arXiv:2306.03078*, 2023b.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis
    Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    和 Dan Alistarh. Spqr: 一种稀疏量化表示用于近乎无损的 llm 权重压缩. *arXiv 预印本 arXiv:2306.03078*,
    2023b.'
- en: 'Doumbouya et al. (2023) Moussa Doumbouya, Baba Mamadi Diané, Solo Farabado
    Cissé, Djibrila Diané, Abdoulaye Sow, Séré Moussa Doumbouya, Daouda Bangoura,
    Fodé Moriba Bayo, Ibrahima Sory 2\. Condé, Kalo Mory Diané, Chris Piech, and Christopher
    Manning. Machine translation for nko: Tools, corpora, and baseline results. In
    *Proceedings of the Eighth Conference on Machine Translation*, pages 312–343,
    Singapore, 2023\. Association for Computational Linguistics. URL [https://aclanthology.org/2023.wmt-1.34](https://aclanthology.org/2023.wmt-1.34).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doumbouya 等 (2023) Moussa Doumbouya, Baba Mamadi Diané, Solo Farabado Cissé,
    Djibrila Diané, Abdoulaye Sow, Séré Moussa Doumbouya, Daouda Bangoura, Fodé Moriba
    Bayo, Ibrahima Sory 2\. Condé, Kalo Mory Diané, Chris Piech, 和 Christopher Manning.
    Nko 机器翻译：工具、语料库和基线结果. 载于 *第八届机器翻译会议论文集*, 页 312–343, 新加坡, 2023\. 计算语言学协会. URL [https://aclanthology.org/2023.wmt-1.34](https://aclanthology.org/2023.wmt-1.34).
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确后训练量化。*arXiv
    预印本 arXiv:2210.17323*，2022年。
- en: 'Gala et al. (2023) Jay Gala, Pranjal A. Chitale, Raghavan AK, Sumanth Doddapaneni,
    Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully,
    Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre, and Anoop Kunchukuttan.
    Indictrans2: Towards high-quality and accessible machine translation models for
    all 22 scheduled indian languages. 2023.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gala 等（2023）Jay Gala, Pranjal A. Chitale, Raghavan AK, Sumanth Doddapaneni,
    Varun Gumma, Aswanth Kumar, Janki Nawale, Anupama Sujatha, Ratish Puduppully,
    Vivek Raghavan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre 和 Anoop Kunchukuttan。Indictrans2：面向所有
    22 种计划印地语的高质量和可访问机器翻译模型。2023年。
- en: Goyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen,
    Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
    and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual
    machine translation. *Transactions of the Association for Computational Linguistics*,
    10, 2022.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等（2022）Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume
    Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán 和 Angela
    Fan。Flores-101 低资源和多语言机器翻译评估基准。*计算语言学学会会刊*，10，2022年。
- en: 'Guzmán et al. (2019) Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino,
    Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato.
    The FLORES evaluation datasets for low-resource machine translation: Nepali–English
    and Sinhala–English. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*, pages 6098–6111, Hong Kong, China, 2019\.
    Association for Computational Linguistics. URL [https://aclanthology.org/D19-1632](https://aclanthology.org/D19-1632).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guzmán 等（2019）Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume
    Lample, Philipp Koehn, Vishrav Chaudhary 和 Marc’Aurelio Ranzato。FLORES 低资源机器翻译评估数据集：尼泊尔语–英语和僧伽罗语–英语。在*2019
    年自然语言处理实证方法会议和第九届国际自然语言处理联合会议（EMNLP-IJCNLP）论文集*，第6098–6111页，中国香港，2019年。计算语言学学会。网址
    [https://aclanthology.org/D19-1632](https://aclanthology.org/D19-1632)。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等（2022）Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark 等。训练计算优化的大型语言模型。*arXiv 预印本 arXiv:2203.15556*，2022年。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Kalamkar et al. (2019) Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi,
    Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj
    Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep
    learning training. *arXiv preprint arXiv:1905.12322*, 2019.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kalamkar 等（2019）Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar
    Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka,
    Jianyu Huang, Hector Yuen 等。对深度学习训练中 bfloat16 的研究。*arXiv 预印本 arXiv:1905.12322*，2019年。
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等（2020）Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu 和 Dario Amodei。神经语言模型的扩展法则。*arXiv
    预印本 arXiv:2001.08361*，2020年。
- en: 'Kim et al. (2023) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization. *arXiv preprint arXiv:2306.07629*, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023）Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng
    Shen, Michael W Mahoney 和 Kurt Keutzer。Squeezellm：密集与稀疏量化。*arXiv 预印本 arXiv:2306.07629*，2023年。
- en: 'Li et al. (2023) Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno,
    Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical
    report. *arXiv preprint arXiv:2309.05463*, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) **Yuanzhi Li**、**Sébastien Bubeck**、**Ronen Eldan**、**Allie
    Del Giorno**、**Suriya Gunasekar** 和 **Yin Tat Lee**。教科书就是你所需的 II: phi-1.5 技术报告。*arXiv
    预印本 arXiv:2309.05463*，2023年。'
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) **Ji Lin**、**Jiaming Tang**、**Haotian Tang**、**Shang Yang**、**Xingyu
    Dang** 和 **Song Han**。AWQ: 基于激活的权重量化用于 LLM 压缩和加速。*arXiv 预印本 arXiv:2306.00978*，2023年。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. (2016) **Stephen Merity**、**Caiming Xiong**、**James Bradbury**
    和 **Richard Socher**。指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Metaseq (2022) Metaseq. Metaseq github issue, 2022. URL [https://github.com/facebookresearch/metaseq/issues/213](https://github.com/facebookresearch/metaseq/issues/213).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Metaseq (2022) **Metaseq**。Metaseq github 问题，2022年。网址 [https://github.com/facebookresearch/metaseq/issues/213](https://github.com/facebookresearch/metaseq/issues/213)。
- en: 'Rajbhandari et al. (2021) Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley,
    Shaden Smith, and Yuxiong He. Zero-infinity: Breaking the gpu memory wall for
    extreme scale deep learning. In *Proceedings of the International Conference for
    High Performance Computing, Networking, Storage and Analysis*, pages 1–14, 2021.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari et al. (2021) **Samyam Rajbhandari**、**Olatunji Ruwase**、**Jeff
    Rasley**、**Shaden Smith** 和 **Yuxiong He**。Zero-infinity: 打破 GPU 内存墙以应对极端规模的深度学习。发表于
    *国际高性能计算、网络、存储与分析会议论文集*，页码 1–14，2021年。'
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. (2021) **Keisuke Sakaguchi**、**Ronan Le Bras**、**Chandra Bhagavatula**
    和 **Yejin Choi**。Winogrande: 大规模对抗性 Winograd 语料库挑战。*ACM 通讯*，64(9):99–106，2021年。'
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) **Hugo Touvron**、**Thibaut Lavril**、**Gautier Izacard**、**Xavier
    Martinet**、**Marie-Anne Lachaux**、**Timothée Lacroix**、**Baptiste Rozière**、**Naman
    Goyal**、**Eric Hambro**、**Faisal Azhar** 等。Llama: 开放高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) **Hugo Touvron**、**Louis Martin**、**Kevin Stone**、**Peter
    Albert**、**Amjad Almahairi**、**Yasmine Babaei**、**Nikolay Bashlykov**、**Soumya
    Batra**、**Prajjwal Bhargava**、**Shruti Bhosale** 等。Llama 2: 开放基础与微调对话模型。*arXiv
    预印本 arXiv:2307.09288*，2023年。'
- en: 'Tseng et al. (2024) Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng et al. (2024) **Albert Tseng**、**Jerry Chee**、**Qingyao Sun**、**Volodymyr
    Kuleshov** 和 **Christopher De Sa**。Quip#: 更优的 llm 量化方法：Hadamard 不相干性与格码本。*arXiv
    预印本 arXiv:2402.04396*，2024年。'
- en: 'Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) **Xiuying Wei**、**Yunchen Zhang**、**Xiangguo Zhang**、**Ruihao
    Gong**、**Shanghang Zhang**、**Qi Zhang**、**Fengwei Yu** 和 **Xianglong Liu**。异常值抑制：推动低位变换器语言模型的极限。*神经信息处理系统进展*，35:17402–17414，2022年。
- en: Williams and Aletras (2023) Miles Williams and Nikolaos Aletras. How does calibration
    data affect the post-training pruning and quantization of large language models?
    *arXiv preprint arXiv:2311.09755*, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Williams 和 Aletras (2023) **Miles Williams** 和 **Nikolaos Aletras**。校准数据如何影响大语言模型的后训练剪枝和量化？*arXiv
    预印本 arXiv:2311.09755*，2023年。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) **Guangxuan Xiao**、**Ji Lin**、**Mickael Seznec**、**Hao Wu**、**Julien
    Demouth** 和 **Song Han**。Smoothquant: 精确且高效的大型语言模型后训练量化。发表于 *国际机器学习会议*，页码 38087–38099。PMLR，2023年。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2022）苏珊·张，斯蒂芬·罗勒，纳曼·戈亚尔，米凯尔·阿尔特克斯，摩雅·陈，舒慧·陈，克里斯托弗·德万，莫娜·迪亚布，谢安·李，维多利亚·林等。Opt：开放预训练变换器语言模型。*arXiv预印本
    arXiv:2205.01068*，2022。
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等（2023）徐昱朱，简李，雍刘，灿马，王伟平。关于大型语言模型的模型压缩的调查。*arXiv预印本 arXiv:2308.07633*，2023。
- en: Appendix A Nonsensical Calibration Set Example
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 无意义校准集示例
- en: Generated by sampling from a uniform distribution of ASCII punctuation and whitespace.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 由从ASCII标点符号和空白的均匀分布中采样生成。
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Appendix B Calibration Set Quality Results
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 校准集质量结果
- en: '![Refer to caption](img/90211493fbeb6063c0fec6c5be649d08.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/90211493fbeb6063c0fec6c5be649d08.png)'
- en: 'Figure 14: WikiText2 perplexity with GPTQ W4A16 quantization, using as calibration
    sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical calibration set
    [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example ‣ Outliers and
    Calibration Sets have Diminishing Effect on Quantization of Modern LLMs"). Results
    normalized to RedPajama score. Lower is better.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：WikiText2的困惑度与GPTQ W4A16量化，使用的校准集为RedPajama（计算机，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录A 无意义校准集示例 ‣ 离群值和校准集对现代LLMs的量化影响递减")。结果已标准化到RedPajama分数。分数越低越好。
- en: '![Refer to caption](img/4669cf6d4c7c73c2cf607a022023c809.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4669cf6d4c7c73c2cf607a022023c809.png)'
- en: 'Figure 15: Average ARC-Challenge and PIQA accuracy with GPTQ W4A16 quantization,
    using as calibration sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical
    calibration set [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"). Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：使用GPTQ W4A16量化的平均ARC挑战和PIQA准确率，校准集为RedPajama（计算机，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录A 无意义校准集示例 ‣ 离群值和校准集对现代LLMs的量化影响递减")。结果已标准化到RedPajama分数。误差条表示标准误差。分数越高越好。
- en: '![Refer to caption](img/c8946e1fad3e6a8ab3bc5fff597dc3f5.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c8946e1fad3e6a8ab3bc5fff597dc3f5.png)'
- en: 'Figure 16: WikiText2 perplexity with AWQ W4A16 quantization, using as calibration
    sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical calibration set
    [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example ‣ Outliers and
    Calibration Sets have Diminishing Effect on Quantization of Modern LLMs"). Results
    normalized to RedPajama score. Lower is better.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：WikiText2的困惑度与AWQ W4A16量化，使用的校准集为RedPajama（计算机，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录A 无意义校准集示例 ‣ 离群值和校准集对现代LLMs的量化影响递减")。结果已标准化到RedPajama分数。分数越低越好。
- en: '![Refer to caption](img/47f5b76d7f167cbb594e2a37d264e0ac.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/47f5b76d7f167cbb594e2a37d264e0ac.png)'
- en: 'Figure 17: Average ARC-Challenge and PIQA accuracy with AWQ W4A16 quantization,
    using as calibration sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical
    calibration set [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"). Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：使用AWQ W4A16量化的平均ARC挑战和PIQA准确率，校准集为RedPajama（计算机，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录A 无意义校准集示例 ‣ 离群值和校准集对现代LLMs的量化影响递减")。结果已标准化到RedPajama分数。误差条表示标准误差。分数越高越好。
- en: '![Refer to caption](img/fa2e5c86897fa4793110e793444e7ad0.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fa2e5c86897fa4793110e793444e7ad0.png)'
- en: 'Figure 18: WikiText2 perplexity with SmoothQuant W8A8 quantization, using as
    calibration sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical calibration
    set [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example ‣ Outliers
    and Calibration Sets have Diminishing Effect on Quantization of Modern LLMs").
    Results normalized to RedPajama score. Lower is better.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：WikiText2的困惑度与SmoothQuant W8A8量化，使用的校准集为RedPajama（计算机，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录A 无意义校准集示例 ‣ 离群值和校准集对现代LLMs的量化影响递减")。结果已标准化到RedPajama分数。分数越低越好。
- en: '![Refer to caption](img/e4ccf14938a01e758b591b2dd7634917.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e4ccf14938a01e758b591b2dd7634917.png)'
- en: 'Figure 19: Average ARC-Challenge and PIQA accuracy with GPTQ 4-bit quantization,
    using as calibration sets RedPajama (Computer, [2023](#bib.bib8)) and a nonsensical
    calibration set [Appendix A](#A1 "Appendix A Nonsensical Calibration Set Example
    ‣ Outliers and Calibration Sets have Diminishing Effect on Quantization of Modern
    LLMs"). Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：使用GPTQ 4位量化的平均ARC-Challenge和PIQA准确率，校准集使用了RedPajama（Computer，[2023](#bib.bib8)）和一个无意义的校准集[附录A](#A1
    "附录 A 无意义校准集示例 ‣ 异常值和校准集对现代LLMs量化的影响递减")。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: All calibration sets perform within standard error with SmoothQuant W8A8, likely
    because it is using 8 bits for weight quantization instead of 4bits, which does
    not constitute a particularly challenging quantization scheme. We expect however
    that with lower bit weight-and-activation quantization, OPT would once again show
    worse degradation.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 所有校准集在SmoothQuant W8A8下的表现都在标准误差范围内，可能是因为它使用了8位权重量化而不是4位，这不构成特别具有挑战性的量化方案。然而，我们预期在较低位数的权重和激活量化下，OPT可能会再次表现出较差的退化。
- en: Appendix C Calibration Sets Content Results
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 校准集内容结果
- en: '![Refer to caption](img/6c5d6e9b35d2e71e8ea4cc4862e2d8f2.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6c5d6e9b35d2e71e8ea4cc4862e2d8f2.png)'
- en: 'Figure 20: PIQA accuracy with GPTQ 4-bit quantization over calibration sets.
    Results normalized to RedPajama score. Error bars represent standard error. Higher
    is better.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图20：使用GPTQ 4位量化在校准集上的PIQA准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: '![Refer to caption](img/0f0dbe934060cda13e8f7ce55cf1142f.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f0dbe934060cda13e8f7ce55cf1142f.png)'
- en: 'Figure 21: ARC-Challenge accuracy with GPTQ 4-bit quantization over calibration
    sets. Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：使用GPTQ 4位量化在校准集上的ARC-Challenge准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: '![Refer to caption](img/af43e2ef36123a45d8c563d0d7edb562.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af43e2ef36123a45d8c563d0d7edb562.png)'
- en: 'Figure 22: PIQA accuracy with AWQ 4-bit quantization over calibration sets.
    Results normalized to RedPajama score. Error bars represent standard error. Higher
    is better.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：使用AWQ 4位量化在校准集上的PIQA准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: '![Refer to caption](img/8a072aeddea0a15099ea55351e744078.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a072aeddea0a15099ea55351e744078.png)'
- en: 'Figure 23: ARC-Challenge accuracy with AWQ 4-bit quantization over calibration
    sets. Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：使用AWQ 4位量化在校准集上的ARC-Challenge准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: '![Refer to caption](img/856f29dc0265ef2d71b1c9c6986582f3.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/856f29dc0265ef2d71b1c9c6986582f3.png)'
- en: 'Figure 24: PIQA accuracy with SmoothQuant W8A8 quantization over calibration
    sets. Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：使用SmoothQuant W8A8量化在校准集上的PIQA准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: '![Refer to caption](img/8f2e97e1db80165f6e9da39c107ba321.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8f2e97e1db80165f6e9da39c107ba321.png)'
- en: 'Figure 25: ARC-Challenge accuracy with SmoothQuant W8A8 quantization over calibration
    sets. Results normalized to RedPajama score. Error bars represent standard error.
    Higher is better.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：使用SmoothQuant W8A8量化在校准集上的ARC-Challenge准确率。结果已标准化为RedPajama分数。误差条表示标准误差。值越高越好。
- en: Appendix D Activations and Outlier Patterns Plots
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 激活和异常模式图
- en: '![[Uncaptioned image]](img/ac5146353466dad0d56c8eff51f38b51.png)![[Uncaptioned
    image]](img/98637bab6b4d584e7f5b8950cee33f3e.png)![[Uncaptioned image]](img/401220bf867576122f751e36f447ed5f.png)![[Uncaptioned
    image]](img/79a56a1357108fed534e9d0f2add72a1.png)![[Uncaptioned image]](img/3191fa1e705ffab4aaa5751d1994fe53.png)![[Uncaptioned
    image]](img/7f19008e7271a7f85da72e2b3e19c807.png)![[Uncaptioned image]](img/7263ddf1df31c17656531596f4acfe11.png)![[Uncaptioned
    image]](img/495ae32556e231b7505b8fe9a9763c9f.png)![[Uncaptioned image]](img/2f1409caca496ec6aa0797100028eaee.png)![[Uncaptioned
    image]](img/2f530775517f3eef85f56144604a7001.png)![[Uncaptioned image]](img/edd46003e490a8fc3edd9a48bc216617.png)![[Uncaptioned
    image]](img/3497cfeb2cb01d237b5438a4f73b8289.png)![[Uncaptioned image]](img/33f609699bbc67393ba611080e8e2599.png)![[Uncaptioned
    image]](img/a9661af5c5365bee0f391c8907d43735.png)![[Uncaptioned image]](img/27fc924115727323ffc23c22ee50624d.png)![[Uncaptioned
    image]](img/730873b18ab2dece006872d4f71e0def.png)![[Uncaptioned image]](img/910c88277316e47c27786856b8427ff5.png)![[Uncaptioned
    image]](img/5ae6e43afef67862fcb8b2d5db126d24.png)![[Uncaptioned image]](img/03d671e2a7a4719c30b24c8b2339ee97.png)![[Uncaptioned
    image]](img/70a45e9c67cc88e0d34e69e6b3bfe4ac.png)![[Uncaptioned image]](img/8a25e58fdf530457ed008a998d71e734.png)![[Uncaptioned
    image]](img/2ff679db0fa388916e2707514c652d3b.png)![[Uncaptioned image]](img/f42d7c9075e0cb8cbb05ae68a8e8a393.png)![[Uncaptioned
    image]](img/3db42773c622f7f8f00390937128d751.png)![[Uncaptioned image]](img/f2ff0a50437caa44511df11abe9d913c.png)![[Uncaptioned
    image]](img/e3c0551a78151ed98a4591b869de4146.png)![[Uncaptioned image]](img/8c4d68d845650c9fc3cb5d65ba66f0eb.png)![[Uncaptioned
    image]](img/2b8e5a8dbe014bc1f4922b7a54d247f5.png)![[Uncaptioned image]](img/dc5d7a6618abe4ef26f4a1fac67024b1.png)![[Uncaptioned
    image]](img/7341bf6d6e1598a2e7edcbf18cc6a793.png)![[Uncaptioned image]](img/12193c01f1066925466489ec82f3cce5.png)![[Uncaptioned
    image]](img/cfb955cc8d4cac10bf136b8f98841f34.png)![[Uncaptioned image]](img/9f12abfa949d405dd75fb976b988ae5d.png)![[Uncaptioned
    image]](img/cac498cb9f56b830785778064948431d.png)![[Uncaptioned image]](img/b942a15cc7d5ae2df9abb52141b5c268.png)![[Uncaptioned
    image]](img/759152129258a937ca7587d9333caad0.png)![[Uncaptioned image]](img/25fdbcba80f9f95ed2154682d26a94e7.png)![[Uncaptioned
    image]](img/d5e12b3296a5b71c05c21114d994f96a.png)![[Uncaptioned image]](img/3584b9a0fbeded44ba2a23cf0e23739f.png)![[Uncaptioned
    image]](img/b058179d26cf9044004e9cc16ca8559d.png)![[Uncaptioned image]](img/a52fc3c3594962907ddfa86b9040a78e.png)![[Uncaptioned
    image]](img/fcafb3743422194b47bae73f2d997352.png)![[Uncaptioned image]](img/716a53145c1feec20b98d3b52b1dda4d.png)![[Uncaptioned
    image]](img/157b87eb99764bdd135585b85c9baca7.png)![[Uncaptioned image]](img/b04c2d6a1ab3429148a3a20a6de14bb5.png)![[Uncaptioned
    image]](img/e432bdaf070745f3a6c1df78d918eac3.png)![[Uncaptioned image]](img/e728fe76daae6414ba400af04917696b.png)![[Uncaptioned
    image]](img/51d1bb282db304e51b89999435c1ddad.png)![[Uncaptioned image]](img/268c8f87bec68673a2bd5806554e3117.png)![[Uncaptioned
    image]](img/f4cc2011df29e0a81fb84951d5afe5f9.png)![[Uncaptioned image]](img/2c0a77b2849d10126e4312cf892ef89e.png)![[Uncaptioned
    image]](img/980183fcf650e9a90bd1bf5b03576166.png)![[Uncaptioned image]](img/4a79c3a08f6c8e3f718df623172c9e2a.png)![[Uncaptioned
    image]](img/c17d4637ad50c41eb6f20213d2073191.png)![[Uncaptioned image]](img/643156cd92b3871143ee2afb0a79e565.png)![[Uncaptioned
    image]](img/64980fb0cff5528438aae5cc0874e1ad.png)![[Uncaptioned image]](img/eac398a2f6da3e03958a9cfd1bd1606f.png)![[Uncaptioned
    image]](img/1b2fb8b221f13e65afb83c7349dad493.png)![[Uncaptioned image]](img/3f7752800df2b2856c9a9402be0b3564.png)![[Uncaptioned
    image]](img/ab9dffb8fcd6320b34da288c9c75c297.png)![[Uncaptioned image]](img/2758e248bba9ad30402a990e9c4e83f3.png)![[Uncaptioned
    image]](img/d92f271e8e49942c3b7beffe024ab77a.png)![[Uncaptioned image]](img/cbfee3b0e858aa9ee402385d383ec7b5.png)![[Uncaptioned
    image]](img/5bb02e0ca46d2848c4b075d533be4b63.png)![[Uncaptioned image]](img/277a5fa6f3e3926b5a25200c6eaf9d1a.png)![[Uncaptioned
    image]](img/2a11820bebacc960b755230e830d6a3c.png)![[Uncaptioned image]](img/c6936c210d6f1bd687f3c4d909968d97.png)![[Uncaptioned
    image]](img/ee35b9e358810a4262fb3f7dc421c3b9.png)![[Uncaptioned image]](img/5348b6215f22949ac89ed950cd3906ce.png)![[Uncaptioned
    image]](img/03e68310aa0d4504397ef05f7ceb9d34.png)![[Uncaptioned image]](img/518d15366dd92d0c3d6f92f9a3c1ff70.png)![[Uncaptioned
    image]](img/752bf50a1e90d35b9d63d9be92255285.png)![[Uncaptioned image]](img/5eda6030fdeec0d1a4f5673d8fcf99f9.png)![[Uncaptioned
    image]](img/7d2aa82314d2314a5ae0d0d142afbab8.png)![[Uncaptioned image]](img/86ad149cc0f2af05b4523155aceb5ae1.png)![[Uncaptioned
    image]](img/d77d0cab80288d6edc42dd9624d2a059.png)![[Uncaptioned image]](img/0853393d256ab54f76a705e53c42b2a9.png)![[Uncaptioned
    image]](img/78e3f727bcd0e4e61ede9ee6bb0d45b7.png)![[Uncaptioned image]](img/0a4101f78a6c677c0510e4b4268dc50d.png)![[Uncaptioned
    image]](img/30e1ec4a2ab1b499130f994abdb15524.png)![[Uncaptioned image]](img/fb259e6d40306eae25c6f0a3258e301b.png)![[Uncaptioned
    image]](img/dce1df9fbed273814237b31e01e5cfef.png)![[Uncaptioned image]](img/0d1422821cdfaa7cd8cc0aa042f96005.png)![[Uncaptioned
    image]](img/498c87155379f0497c9f6d24680a9fc2.png)![[Uncaptioned image]](img/dd2cc9dcac6973e47bad58cbf5537c7b.png)![[Uncaptioned
    image]](img/cb0725528dcd7b51f809e4381f520d6f.png)![[Uncaptioned image]](img/c12bf57750bfe1653d9db4ee02a5efd9.png)![[Uncaptioned
    image]](img/bd4d92db7ed07037242f638c322c8d88.png)![[Uncaptioned image]](img/dd8405c0225e1db2cc09606e04fac34f.png)![[Uncaptioned
    image]](img/44bb6e12c022999f43c9fee2914064fd.png)![[Uncaptioned image]](img/e589577d0d7bd29e53aa2376e453c35e.png)![[Uncaptioned
    image]](img/5d3fed5414259dd073a9ba4c531cba92.png)![[Uncaptioned image]](img/0937abf73a41025f31f30344365b5553.png)![[Uncaptioned
    image]](img/7d32b3a107d98ea96939af82bb9f566e.png)![[Uncaptioned image]](img/8097c7bf20724027155e34cec91290e7.png)![[Uncaptioned
    image]](img/9e473192a30001f19a939b7235073320.png)![[Uncaptioned image]](img/6db186f34be4328c40a545ad1ab05828.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/ac5146353466dad0d56c8eff51f38b51.png)![[未标注的图片]](img/98637bab6b4d584e7f5b8950cee33f3e.png)![[未标注的图片]](img/401220bf867576122f751e36f447ed5f.png)![[未标注的图片]](img/79a56a1357108fed534e9d0f2add72a1.png)![[未标注的图片]](img/3191fa1e705ffab4aaa5751d1994fe53.png)![[未标注的图片]](img/7f19008e7271a7f85da72e2b3e19c807.png)![[未标注的图片]](img/7263ddf1df31c17656531596f4acfe11.png)![[未标注的图片]](img/495ae32556e231b7505b8fe9a9763c9f.png)![[未标注的图片]](img/2f1409caca496ec6aa0797100028eaee.png)![[未标注的图片]](img/2f530775517f3eef85f56144604a7001.png)![[未标注的图片]](img/edd46003e490a8fc3edd9a48bc216617.png)![[未标注的图片]](img/3497cfeb2cb01d237b5438a4f73b8289.png)![[未标注的图片]](img/33f609699bbc67393ba611080e8e2599.png)![[未标注的图片]](img/a9661af5c5365bee0f391c8907d43735.png)![[未标注的图片]](img/27fc924115727323ffc23c22ee50624d.png)![[未标注的图片]](img/730873b18ab2dece006872d4f71e0def.png)![[未标注的图片]](img/910c88277316e47c27786856b8427ff5.png)![[未标注的图片]](img/5ae6e43afef67862fcb8b2d5db126d24.png)![[未标注的图片]](img/03d671e2a7a4719c30b24c8b2339ee97.png)![[未标注的图片]](img/70a45e9c67cc88e0d34e69e6b3bfe4ac.png)![[未标注的图片]](img/8a25e58fdf530457ed008a998d71e734.png)![[未标注的图片]](img/2ff679db0fa388916e2707514c652d3b.png)![[未标注的图片]](img/f42d7c9075e0cb8cbb05ae68a8e8a393.png)![[未标注的图片]](img/3db42773c622f7f8f00390937128d751.png)![[未标注的图片]](img/f2ff0a50437caa44511df11abe9d913c.png)![[未标注的图片]](img/e3c0551a78151ed98a4591b869de4146.png)![[未标注的图片]](img/8c4d68d845650c9fc3cb5d65ba66f0eb.png)![[未标注的图片]](img/2b8e5a8dbe014bc1f4922b7a54d247f5.png)![[未标注的图片]](img/dc5d7a6618abe4ef26f4a1fac67024b1.png)![[未标注的图片]](img/7341bf6d6e1598a2e7edcbf18cc6a793.png)![[未标注的图片]](img/12193c01f1066925466489ec82f3cce5.png)![[未标注的图片]](img/cfb955cc8d4cac10bf136b8f98841f34.png)![[未标注的图片]](img/9f12abfa949d405dd75fb976b988ae5d.png)![[未标注的图片]](img/cac498cb9f56b830785778064948431d.png)![[未标注的图片]](img/b942a15cc7d5ae2df9abb52141b5c268.png)![[未标注的图片]](img/759152129258a937ca7587d9333caad0.png)![[未标注的图片]](img/25fdbcba80f9f95ed2154682d26a94e7.png)![[未标注的图片]](img/d5e12b3296a5b71c05c21114d994f96a.png)![[未标注的图片]](img/3584b9a0fbeded44ba2a23cf0e23739f.png)![[未标注的图片]](img/b058179d26cf9044004e9cc16ca8559d.png)![[未标注的图片]](img/a52fc3c3594962907ddfa86b9040a78e.png)![[未标注的图片]](img/fcafb3743422194b47bae73f2d997352.png)![[未标注的图片]](img/716a53145c1feec20b98d3b52b1dda4d.png)![[未标注的图片]](img/157b87eb99764bdd135585b85c9baca7.png)![[未标注的图片]](img/b04c2d6a1ab3429148a3a20a6de14bb5.png)![[未标注的图片]](img/e432bdaf070745f3a6c1df78d918eac3.png)![[未标注的图片]](img/e728fe76daae6414ba400af04917696b.png)![[未标注的图片]](img/51d1bb282db304e51b89999435c1ddad.png)![[未标注的图片]](img/268c8f87bec68673a2bd5806554e3117.png)![[未标注的图片]](img/f4cc2011df29e0a81fb84951d5afe5f9.png)![[未标注的图片]](img/2c0a77b2849d10126e4312cf892ef89e.png)![[未标注的图片]](img/980183fcf650e9a90bd1bf5b03576166.png)![[未标注的图片]](img/4a79c3a08f6c8e3f718df623172c9e2a.png)![[未标注的图片]](img/c17d4637ad50c41eb6f20213d2073191.png)![[未标注的图片]](img/643156cd92b3871143ee2afb0a79e565.png)![[未标注的图片]](img/64980fb0cff5528438aae5cc0874e1ad.png)![[未标注的图片]](img/eac398a2f6da3e03958a9cfd1bd1606f.png)![[未标注的图片]](img/1b2fb8b221f13e65afb83c7349dad493.png)![[未标注的图片]](img/3f7752800df2b2856c9a9402be0b3564.png)![[未标注的图片]](img/ab9dffb8fcd6320b34da288c9c75c297.png)![[未标注的图片]](img/2758e248bba9ad30402a990e9c4e83f3.png)![[未标注的图片]](img/d92f271e8e49942c3b7beffe024ab77a.png)![[未标注的图片]](img/cbfee3b0e858aa9ee402385d383ec7b5.png)![[未标注的图片]](img/5bb02e0ca46d2848c4b075d533be4b63.png)![[未标注的图片]](img/277a5fa6f3e3926b5a25200c6eaf9d1a.png)![[未标注的图片]](img/2a11820bebacc960b755230e830d6a3c.png)![[未标注的图片]](img/c6936c210d6f1bd687f3c4d909968d97.png)![[未标注的图片]](img/ee35b9e358810a4262fb3f7dc421c'
- en: Appendix E Activation Distributions Plots
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 激活分布图
- en: '![[Uncaptioned image]](img/fca7ac7b717fd3d7ef4d82aeb35ab4a3.png)![[Uncaptioned
    image]](img/4257273517a8d6216fa64d508f759fd6.png)![[Uncaptioned image]](img/f2f5db4e215f10b6231a99a1246db439.png)![[Uncaptioned
    image]](img/ddc040cc67affb5900b51c1af6e54c5f.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/fca7ac7b717fd3d7ef4d82aeb35ab4a3.png)![[未标注的图片]](img/4257273517a8d6216fa64d508f759fd6.png)![[未标注的图片]](img/f2f5db4e215f10b6231a99a1246db439.png)![[未标注的图片]](img/ddc040cc67affb5900b51c1af6e54c5f.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/c62f44319ff3964f0f2826176edf98a2.png)![[Uncaptioned
    image]](img/289b6c6e7236d327889d6ee58ac96cad.png)![[Uncaptioned image]](img/c1023418fc1bc984efd14c0923deaf50.png)![[Uncaptioned
    image]](img/962dd799dbde35143f0bea803fde3266.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/c62f44319ff3964f0f2826176edf98a2.png)![[未标注的图片]](img/289b6c6e7236d327889d6ee58ac96cad.png)![[未标注的图片]](img/c1023418fc1bc984efd14c0923deaf50.png)![[未标注的图片]](img/962dd799dbde35143f0bea803fde3266.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/13793fb45a2ed232f16b0e6093e47e01.png)![[Uncaptioned
    image]](img/387d38c0b9c97c2efb26202a97fd0321.png)![[Uncaptioned image]](img/72276b6437ad8154f9d7137bbaa60d6e.png)![[Uncaptioned
    image]](img/226c34c85acd508394dbdba0e79219c8.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/13793fb45a2ed232f16b0e6093e47e01.png)![[未标注的图片]](img/387d38c0b9c97c2efb26202a97fd0321.png)![[未标注的图片]](img/72276b6437ad8154f9d7137bbaa60d6e.png)![[未标注的图片]](img/226c34c85acd508394dbdba0e79219c8.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/8690eedf438b80038e8d6561c4a6fb72.png)![[Uncaptioned
    image]](img/e8e9bb877fb0135b3b401d6de916b1eb.png)![[Uncaptioned image]](img/52c224a37dd5b5dbc053ff853da7c83e.png)![[Uncaptioned
    image]](img/e5825192d2604d4b26ef16ed4e86a6fc.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/8690eedf438b80038e8d6561c4a6fb72.png)![[未标注的图片]](img/e8e9bb877fb0135b3b401d6de916b1eb.png)![[未标注的图片]](img/52c224a37dd5b5dbc053ff853da7c83e.png)![[未标注的图片]](img/e5825192d2604d4b26ef16ed4e86a6fc.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/3a3d8ffe7b6dd6b8db129d6ad6446ae4.png)![[Uncaptioned
    image]](img/6e138b90280800198a969adcc5ec69dd.png)![[Uncaptioned image]](img/27d67151235dc0430cd063e9b479d2d8.png)![[Uncaptioned
    image]](img/e08d21e86f04a4950bcf2017eb6791d4.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/3a3d8ffe7b6dd6b8db129d6ad6446ae4.png)![[未标注的图片]](img/6e138b90280800198a969adcc5ec69dd.png)![[未标注的图片]](img/27d67151235dc0430cd063e9b479d2d8.png)![[未标注的图片]](img/e08d21e86f04a4950bcf2017eb6791d4.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/56fee78dfa21606224f5592417b36ef8.png)![[Uncaptioned
    image]](img/dd4a8e50a256b866584baceead8c8d38.png)![[Uncaptioned image]](img/08099abaa53988d99ce678c730574693.png)![[Uncaptioned
    image]](img/3406c8b51468f45b2b901d737b3ff95d.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/56fee78dfa21606224f5592417b36ef8.png)![[未标注的图片]](img/dd4a8e50a256b866584baceead8c8d38.png)![[未标注的图片]](img/08099abaa53988d99ce678c730574693.png)![[未标注的图片]](img/3406c8b51468f45b2b901d737b3ff95d.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/c6e033cd04c66678b5de67b7f9b8949b.png)![[Uncaptioned
    image]](img/6e9b3b4ac267de712dc662d8236538c4.png)![[Uncaptioned image]](img/fd7275089938d565e3c60b6253cb393c.png)![[Uncaptioned
    image]](img/ccb27f03b1a4ff0541161d0e2e785c13.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/c6e033cd04c66678b5de67b7f9b8949b.png)![[未标注的图片]](img/6e9b3b4ac267de712dc662d8236538c4.png)![[未标注的图片]](img/fd7275089938d565e3c60b6253cb393c.png)![[未标注的图片]](img/ccb27f03b1a4ff0541161d0e2e785c13.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: '![[Uncaptioned image]](img/59aed2da7841205a4641f114c80de9b3.png)![[Uncaptioned
    image]](img/2340442a1a0d87a2e77bb2e4ad3f7730.png)![[Uncaptioned image]](img/1d9686faa7b21cd2fd76d450d92b6188.png)![[Uncaptioned
    image]](img/e0dbfbfa90ef6685c1570f91f3167e53.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/59aed2da7841205a4641f114c80de9b3.png)![[未标注的图片]](img/2340442a1a0d87a2e77bb2e4ad3f7730.png)![[未标注的图片]](img/1d9686faa7b21cd2fd76d450d92b6188.png)![[未标注的图片]](img/e0dbfbfa90ef6685c1570f91f3167e53.png)'
- en: '![[Uncaptioned image]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注的图片]](img/58b21ce88f4c2437066e9ae9818c87db.png)'
- en: NeurIPS Paper Checklist
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NeurIPS 论文检查表
- en: '1.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Claims
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声明
- en: 'Question: Do the main claims made in the abstract and introduction accurately
    reflect the paper’s contributions and scope?'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '问题: 摘要和引言中的主要声明是否准确反映了论文的贡献和范围？'
- en: 'Answer: [Yes]'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '答复: [是]'
- en: 'Justification: The claims made in the abstract and introduction are accurately
    reflect the contributions of the paper.'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '说明: 摘要和引言中的声明准确反映了论文的贡献。'
- en: 'Guidelines:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '指南:'
- en: •
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the abstract and introduction do not include the claims
    made in the paper.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着摘要和引言中未包含论文中的主张。
- en: •
  id: totrans-203
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The abstract and/or introduction should clearly state the claims made, including
    the contributions made in the paper and important assumptions and limitations.
    A No or NA answer to this question will not be perceived well by the reviewers.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要和/或引言应清楚地说明所提出的主张，包括论文中的贡献和重要的假设与限制。对这个问题回答“否”或“无”将不会被审稿人看好。
- en: •
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The claims made should match theoretical and experimental results, and reflect
    how much the results can be expected to generalize to other settings.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所提出的主张应与理论和实验结果相符，并反映结果在其他环境中的泛化程度。
- en: •
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is fine to include aspirational goals as motivation as long as it is clear
    that these goals are not attained by the paper.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包含志向目标作为动机是可以的，只要明确这些目标并未被论文实现。
- en: '2.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Limitations
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 限制
- en: 'Question: Does the paper discuss the limitations of the work performed by the
    authors?'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了作者所做工作的限制？
- en: 'Answer: [Yes]'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The limitation section adequately addressed the limitations
    of the paper known by the authors.'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：限制部分充分讨论了作者所知的论文限制。
- en: 'Guidelines:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper has no limitation while the answer No means
    that the paper has limitations, but those are not discussed in the paper.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“无”意味着论文存在限制，但这些限制未在论文中讨论，而答案“NA”则意味着论文没有限制。
- en: •
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors are encouraged to create a separate "Limitations" section in their
    paper.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鼓励作者在论文中创建一个单独的“限制”部分。
- en: •
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should point out any strong assumptions and how robust the results
    are to violations of these assumptions (e.g., independence assumptions, noiseless
    settings, model well-specification, asymptotic approximations only holding locally).
    The authors should reflect on how these assumptions might be violated in practice
    and what the implications would be.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应指出任何强假设以及结果对这些假设违反的鲁棒性（例如，独立性假设、无噪声设置、模型良好规格、渐近近似仅在局部有效）。作者应反思这些假设在实际中可能如何被违反以及其可能产生的影响。
- en: •
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the scope of the claims made, e.g., if the approach
    was only tested on a few datasets or with a few runs. In general, empirical results
    often depend on implicit assumptions, which should be articulated.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思所提出主张的范围，例如，如果方法仅在少数数据集上进行测试或仅经过少数运行。一般来说，实证结果通常依赖于隐含的假设，这些假设应被阐明。
- en: •
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should reflect on the factors that influence the performance of
    the approach. For example, a facial recognition algorithm may perform poorly when
    image resolution is low or images are taken in low lighting. Or a speech-to-text
    system might not be used reliably to provide closed captions for online lectures
    because it fails to handle technical jargon.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应反思影响方法性能的因素。例如，当图像分辨率低或在光线不足的情况下拍摄时，面部识别算法可能表现不佳。或者，语音转文字系统可能不能可靠地提供在线讲座的字幕，因为它无法处理技术术语。
- en: •
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should discuss the computational efficiency of the proposed algorithms
    and how they scale with dataset size.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应讨论所提议算法的计算效率以及它们如何随着数据集大小的变化而扩展。
- en: •
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If applicable, the authors should discuss possible limitations of their approach
    to address problems of privacy and fairness.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果适用，作者应讨论他们的方法可能存在的隐私和公平性问题的限制。
- en: •
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While the authors might fear that complete honesty about limitations might be
    used by reviewers as grounds for rejection, a worse outcome might be that reviewers
    discover limitations that aren’t acknowledged in the paper. The authors should
    use their best judgment and recognize that individual actions in favor of transparency
    play an important role in developing norms that preserve the integrity of the
    community. Reviewers will be specifically instructed to not penalize honesty concerning
    limitations.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然作者可能担心对限制的完全诚实会被审稿人作为拒稿的理由，但更糟糕的结果可能是审稿人发现论文中未承认的限制。作者应运用最佳判断，并认识到个人在推动透明度方面的行动在制定维护社区诚信的规范中发挥着重要作用。审稿人将被特别指示不要因为诚实地讨论限制而给予处罚。
- en: '3.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Theory Assumptions and Proofs
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论假设与证明
- en: 'Question: For each theoretical result, does the paper provide the full set
    of assumptions and a complete (and correct) proof?'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个理论结果，论文是否提供了完整的假设集合和完整（且正确）的证明？
- en: 'Answer: [N/A]'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[N/A]
- en: 'Justification: The paper’s results are mostly empirical, thus no proofs are
    needed.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释：论文的结果主要是经验性的，因此不需要证明。
- en: 'Guidelines:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include theoretical results.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着论文不包括理论结果。
- en: •
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文中的所有定理、公式和证明应编号并相互引用。
- en: •
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: All assumptions should be clearly stated or referenced in the statement of any
    theorems.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有假设应在任何定理的声明中明确陈述或引用。
- en: •
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proofs can either appear in the main paper or the supplemental material,
    but if they appear in the supplemental material, the authors are encouraged to
    provide a short proof sketch to provide intuition.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明可以出现在主论文中或补充材料中，但如果出现在补充材料中，鼓励作者提供简短的证明草图以提供直观理解。
- en: •
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Inversely, any informal proof provided in the core of the paper should be complemented
    by formal proofs provided in appendix or supplemental material.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反之，论文核心部分提供的任何非正式证明应由附录或补充材料中提供的正式证明补充。
- en: •
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Theorems and Lemmas that the proof relies upon should be properly referenced.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证明所依赖的定理和引理应适当地引用。
- en: '4.'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Experimental Result Reproducibility
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果复现性
- en: 'Question: Does the paper fully disclose all the information needed to reproduce
    the main experimental results of the paper to the extent that it affects the main
    claims and/or conclusions of the paper (regardless of whether the code and data
    are provided or not)?'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否完全披露了复现论文主要实验结果所需的所有信息，以至于影响论文的主要主张和/或结论（无论是否提供了代码和数据）？
- en: 'Answer: [Yes]'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The authors made a significant effort ensuring that all the
    details for reproducing the experiments are included.'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释：作者付出了重大努力，确保包括了所有复现实验所需的细节。
- en: 'Guidelines:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”意味着论文不包括实验。
- en: •
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the paper includes experiments, a No answer to this question will not be
    perceived well by the reviewers: Making the paper reproducible is important, regardless
    of whether the code and data are provided or not.'
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果论文包括实验，对这个问题的“不”回答不会被审稿人接受：使论文可复现很重要，无论是否提供了代码和数据。
- en: •
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the contribution is a dataset and/or model, the authors should describe the
    steps taken to make their results reproducible or verifiable.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是数据集和/或模型，作者应描述为使结果可复现或可验证所采取的步骤。
- en: •
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the contribution, reproducibility can be accomplished in various
    ways. For example, if the contribution is a novel architecture, describing the
    architecture fully might suffice, or if the contribution is a specific model and
    empirical evaluation, it may be necessary to either make it possible for others
    to replicate the model with the same dataset, or provide access to the model.
    In general. releasing code and data is often one good way to accomplish this,
    but reproducibility can also be provided via detailed instructions for how to
    replicate the results, access to a hosted model (e.g., in the case of a large
    language model), releasing of a model checkpoint, or other means that are appropriate
    to the research performed.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据贡献的不同，复现可以通过多种方式实现。例如，如果贡献是一个新颖的架构，全面描述该架构可能就足够了；如果贡献是特定模型和经验评估，则可能需要使其他人能够使用相同的数据集复制模型，或提供对模型的访问。通常，发布代码和数据通常是实现这一目标的好方法，但复现也可以通过详细的复制结果说明、访问托管模型（例如，大型语言模型的情况）、发布模型检查点或其他适合研究的方法来提供。
- en: •
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While NeurIPS does not require releasing code, the conference does require all
    submissions to provide some reasonable avenue for reproducibility, which may depend
    on the nature of the contribution. For example
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然 NeurIPS 不要求公开代码，但会议确实要求所有提交的论文提供某种合理的复现途径，这可能取决于贡献的性质。例如
- en: (a)
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (a)
- en: If the contribution is primarily a new algorithm, the paper should make it clear
    how to reproduce that algorithm.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是一个新算法，论文应明确如何复现该算法。
- en: (b)
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (b)
- en: If the contribution is primarily a new model architecture, the paper should
    describe the architecture clearly and fully.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献主要是新的模型架构，论文应清晰完整地描述该架构。
- en: (c)
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: (c)
- en: If the contribution is a new model (e.g., a large language model), then there
    should either be a way to access this model for reproducing the results or a way
    to reproduce the model (e.g., with an open-source dataset or instructions for
    how to construct the dataset).
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果贡献是一个新模型（例如，大型语言模型），那么应有一种方法来访问此模型以重现结果，或一种重现模型的方法（例如，使用开源数据集或构建数据集的说明）。
- en: (d)
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （d）
- en: We recognize that reproducibility may be tricky in some cases, in which case
    authors are welcome to describe the particular way they provide for reproducibility.
    In the case of closed-source models, it may be that access to the model is limited
    in some way (e.g., to registered users), but it should be possible for other researchers
    to have some path to reproducing or verifying the results.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到在某些情况下重现性可能会很棘手，此时作者可以描述他们提供重现性的方法。在闭源模型的情况下，可能对模型的访问受到某种限制（例如，仅限注册用户），但其他研究人员应能够通过某种方式重现或验证结果。
- en: '5.'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Open access to data and code
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据和代码的开放访问
- en: 'Question: Does the paper provide open access to the data and code, with sufficient
    instructions to faithfully reproduce the main experimental results, as described
    in supplemental material?'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否提供了数据和代码的开放访问，且有足够的说明以忠实重现主要实验结果，如补充材料所述？
- en: 'Answer: [No]'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[否]
- en: 'Justification: We have not provided the code to run our experiments, but intend
    to do so upon acceptance.'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：我们没有提供运行实验的代码，但计划在接受后提供。
- en: 'Guidelines:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that paper does not include experiments requiring code.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文不包括需要代码的实验。
- en: •
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请参见NeurIPS代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）以获取更多详细信息。
- en: •
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: While we encourage the release of code and data, we understand that this might
    not be possible, so “No” is an acceptable answer. Papers cannot be rejected simply
    for not including code, unless this is central to the contribution (e.g., for
    a new open-source benchmark).
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然我们鼓励发布代码和数据，但我们理解这可能不可行，因此“否”是一个可接受的答案。除非这对于贡献至关重要（例如，用于新的开源基准），否则仅因未包含代码而不能拒绝论文。
- en: •
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The instructions should contain the exact command and environment needed to
    run to reproduce the results. See the NeurIPS code and data submission guidelines
    ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy))
    for more details.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 说明应包含运行所需的确切命令和环境，以便重现结果。有关更多详细信息，请参见NeurIPS代码和数据提交指南（[https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)）。
- en: •
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide instructions on data access and preparation, including
    how to access the raw data, preprocessed data, intermediate data, and generated
    data, etc.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供数据访问和准备的说明，包括如何访问原始数据、预处理数据、中间数据和生成的数据等。
- en: •
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should provide scripts to reproduce all experimental results for
    the new proposed method and baselines. If only a subset of experiments are reproducible,
    they should state which ones are omitted from the script and why.
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应提供脚本以重现所有实验结果，包括新的提出方法和基准。如果仅有一部分实验可以重现，他们应说明哪些实验被排除在脚本之外及其原因。
- en: •
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, to preserve anonymity, the authors should release anonymized
    versions (if applicable).
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提交时，为了保持匿名，作者应发布匿名版本（如适用）。
- en: •
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Providing as much information as possible in supplemental material (appended
    to the paper) is recommended, but including URLs to data and code is permitted.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐在补充材料中提供尽可能多的信息（附在论文后），但包含数据和代码的URL是允许的。
- en: '6.'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Experimental Setting/Details
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置/细节
- en: 'Question: Does the paper specify all the training and test details (e.g., data
    splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary
    to understand the results?'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否指定了所有训练和测试细节（例如，数据分割、超参数、如何选择、优化器类型等），这些对于理解结果是必要的？
- en: 'Answer: [Yes]'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper specifies the hyperparameters used to run and evaluate
    the experiments.'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：论文中说明了运行和评估实验所用的超参数。
- en: 'Guidelines:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案NA意味着论文不包括实验。
- en: •
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The experimental setting should be presented in the core of the paper to a level
    of detail that is necessary to appreciate the results and make sense of them.
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验设置应在论文的核心部分以足够详细的程度呈现，以便理解结果并合理解释。
- en: •
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The full details can be provided either with the code, in appendix, or as supplemental
    material.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细信息可以通过代码、附录或补充材料提供。
- en: '7.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: Experiment Statistical Significance
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验统计显著性
- en: 'Question: Does the paper report error bars suitably and correctly defined or
    other appropriate information about the statistical significance of the experiments?'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否适当地报告了误差条并正确定义了它们，或提供了其他关于实验统计显著性的适当信息？
- en: 'Answer: [Yes]'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper uses standard error as appropriate.'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：论文适当地使用了标准误差。
- en: 'Guidelines:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文中没有包含实验。
- en: •
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should answer "Yes" if the results are accompanied by error bars,
    confidence intervals, or statistical significance tests, at least for the experiments
    that support the main claims of the paper.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果结果附有误差条、置信区间或统计显著性测试，作者应回答“是”，至少对于支持论文主要论点的实验而言。
- en: •
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The factors of variability that the error bars are capturing should be clearly
    stated (for example, train/test split, initialization, random drawing of some
    parameter, or overall run with given experimental conditions).
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应清楚地说明误差条所捕捉的变异因素（例如，训练/测试分割、初始化、某些参数的随机抽样，或在给定实验条件下的总体运行）。
- en: •
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The method for calculating the error bars should be explained (closed form formula,
    call to a library function, bootstrap, etc.)
  id: totrans-320
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算误差条的方法应得到解释（封闭形式公式、库函数调用、自助法等）
- en: •
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The assumptions made should be given (e.g., Normally distributed errors).
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应提供所做的假设（例如，正态分布误差）。
- en: •
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It should be clear whether the error bar is the standard deviation or the standard
    error of the mean.
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应明确误差条是标准差还是均值的标准误差。
- en: •
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is OK to report 1-sigma error bars, but one should state it. The authors
    should preferably report a 2-sigma error bar than state that they have a 96% CI,
    if the hypothesis of Normality of errors is not verified.
  id: totrans-326
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 报告1-σ误差条是可以的，但应说明这一点。如果错误的正态性假设未被验证，作者应优先报告2-σ误差条，而不是说明有96%的置信区间。
- en: •
  id: totrans-327
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For asymmetric distributions, the authors should be careful not to show in tables
    or figures symmetric error bars that would yield results that are out of range
    (e.g. negative error rates).
  id: totrans-328
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非对称分布，作者应小心不要在表格或图形中显示对称的误差条，这可能会导致结果超出范围（例如，负的误差率）。
- en: •
  id: totrans-329
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If error bars are reported in tables or plots, The authors should explain in
    the text how they were calculated and reference the corresponding figures or tables
    in the text.
  id: totrans-330
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果表格或图形中报告了误差条，作者应在文本中解释它们是如何计算的，并在文本中引用相应的图形或表格。
- en: '8.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: Experiments Compute Resources
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验计算资源
- en: 'Question: For each experiment, does the paper provide sufficient information
    on the computer resources (type of compute workers, memory, time of execution)
    needed to reproduce the experiments?'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于每个实验，论文是否提供了足够的信息来说明重现实验所需的计算资源（计算工作者的类型、内存、执行时间）？
- en: 'Answer: [Yes]'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper states the compute resources used.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：论文说明了使用的计算资源。
- en: 'Guidelines:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not include experiments.
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示论文中没有包含实验。
- en: •
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should indicate the type of compute workers CPU or GPU, internal cluster,
    or cloud provider, including relevant memory and storage.
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应说明计算工作者的CPU或GPU类型、内部集群或云服务提供商，包括相关的内存和存储。
- en: •
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should provide the amount of compute required for each of the individual
    experimental runs as well as estimate the total compute.
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应提供每个单独实验运行所需的计算量，并估计总计算量。
- en: •
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should disclose whether the full research project required more compute
    than the experiments reported in the paper (e.g., preliminary or failed experiments
    that didn’t make it into the paper).
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应披露是否整个研究项目所需的计算资源超过了论文中报告的实验所需的资源（例如，未纳入论文的初步或失败的实验）。
- en: '9.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: Code Of Ethics
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 伦理规范
- en: 'Question: Does the research conducted in the paper conform, in every respect,
    with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)?'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中的研究是否在每个方面都符合NeurIPS《伦理规范》[https://neurips.cc/public/EthicsGuidelines](https://neurips.cc/public/EthicsGuidelines)？
- en: 'Answer: [Yes]'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回答：[是]
- en: 'Justification: The paper confirms to NeurIPS Code of Ethics in every aspecct.'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释：论文在各个方面符合NeurIPS《伦理规范》。
- en: 'Guidelines:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-351
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回答“不适用”意味着作者尚未审查NeurIPS《伦理规范》。
- en: •
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer No, they should explain the special circumstances that
    require a deviation from the Code of Ethics.
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答“不”，他们应当解释需要偏离《伦理规范》的特殊情况。
- en: •
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should make sure to preserve anonymity (e.g., if there is a special
    consideration due to laws or regulations in their jurisdiction).
  id: totrans-356
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应确保保留匿名性（例如，如果由于其辖区内的法律或法规有特殊考虑）。
- en: '10.'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '10.'
- en: Broader Impacts
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: 'Question: Does the paper discuss both potential positive societal impacts and
    negative societal impacts of the work performed?'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否讨论了所做工作的潜在正面社会影响和负面社会影响？
- en: 'Answer: [N/A]'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回答：[不适用]
- en: 'Justification: The paper aims to advance our understanding in machine learning,
    and does not have any direct negative applications.'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释：论文旨在推进我们对机器学习的理解，并没有直接的负面应用。
- en: 'Guidelines:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-363
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that there is no societal impact of the work performed.
  id: totrans-364
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回答“不适用”意味着该工作的社会影响不存在。
- en: •
  id: totrans-365
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the authors answer NA or No, they should explain why their work has no societal
    impact or why the paper does not address societal impact.
  id: totrans-366
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果作者回答“不适用”或“不”，他们应解释为什么他们的工作没有社会影响或为什么论文没有涉及社会影响。
- en: •
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Examples of negative societal impacts include potential malicious or unintended
    uses (e.g., disinformation, generating fake profiles, surveillance), fairness
    considerations (e.g., deployment of technologies that could make decisions that
    unfairly impact specific groups), privacy considerations, and security considerations.
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 负面社会影响的例子包括潜在的恶意或非预期使用（例如，虚假信息、生成虚假个人资料、监控）、公平性考虑（例如，部署可能对特定群体产生不公平影响的技术）、隐私考虑和安全考虑。
- en: •
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The conference expects that many papers will be foundational research and not
    tied to particular applications, let alone deployments. However, if there is a
    direct path to any negative applications, the authors should point it out. For
    example, it is legitimate to point out that an improvement in the quality of generative
    models could be used to generate deepfakes for disinformation. On the other hand,
    it is not needed to point out that a generic algorithm for optimizing neural networks
    could enable people to train models that generate Deepfakes faster.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 会议期望许多论文将是基础研究，而不是与特定应用程序或部署相关。然而，如果存在直接的负面应用路径，作者应指出。例如，指出生成模型质量的提高可能被用于生成虚假信息的深度伪造是合理的。另一方面，指出优化神经网络的通用算法可能使人们能够更快地训练生成深度伪造的模型则是不必要的。
- en: •
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should consider possible harms that could arise when the technology
    is being used as intended and functioning correctly, harms that could arise when
    the technology is being used as intended but gives incorrect results, and harms
    following from (intentional or unintentional) misuse of the technology.
  id: totrans-372
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应考虑在技术按预期使用且正常运行时可能产生的危害，技术按预期使用但产生不正确结果时可能产生的危害，以及（故意或非故意）误用技术带来的危害。
- en: •
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If there are negative societal impacts, the authors could also discuss possible
    mitigation strategies (e.g., gated release of models, providing defenses in addition
    to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system
    learns from feedback over time, improving the efficiency and accessibility of
    ML).
  id: totrans-374
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果存在负面的社会影响，作者也可以讨论可能的缓解策略（例如，模型的有条件发布、提供防御措施以弥补攻击、监控误用的机制、监控系统如何随着时间的推移从反馈中学习的机制、提高机器学习的效率和可及性）。
- en: '11.'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '11.'
- en: Safeguards
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保护措施
- en: 'Question: Does the paper describe safeguards that have been put in place for
    responsible release of data or models that have a high risk for misuse (e.g.,
    pretrained language models, image generators, or scraped datasets)?'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了为负责任地发布具有高误用风险的数据或模型（例如，预训练语言模型、图像生成器或抓取的数据集）而采取的保护措施？
- en: 'Answer: [N/A]'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回答：[不适用]
- en: 'Justification: The paper is not releasing any data, models or datasets.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：该论文没有释放任何数据、模型或数据集。
- en: 'Guidelines:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-381
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper poses no such risks.
  id: totrans-382
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示该论文不存在这些风险。
- en: •
  id: totrans-383
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Released models that have a high risk for misuse or dual-use should be released
    with necessary safeguards to allow for controlled use of the model, for example
    by requiring that users adhere to usage guidelines or restrictions to access the
    model or implementing safety filters.
  id: totrans-384
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已发布的模型如果有较高的误用或双重用途风险，应该采取必要的保障措施，以便对模型进行受控使用，例如要求用户遵守使用指南或限制访问模型，或实施安全过滤器。
- en: •
  id: totrans-385
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Datasets that have been scraped from the Internet could pose safety risks. The
    authors should describe how they avoided releasing unsafe images.
  id: totrans-386
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从互联网抓取的数据集可能存在安全风险。作者应描述他们如何避免发布不安全的图像。
- en: •
  id: totrans-387
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that providing effective safeguards is challenging, and many papers
    do not require this, but we encourage authors to take this into account and make
    a best faith effort.
  id: totrans-388
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到提供有效的保障措施是具有挑战性的，许多论文不要求这样做，但我们鼓励作者考虑这一点，并尽最大努力。
- en: '12.'
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '12.'
- en: Licenses for existing assets
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现有资产的许可证
- en: 'Question: Are the creators or original owners of assets (e.g., code, data,
    models), used in the paper, properly credited and are the license and terms of
    use explicitly mentioned and properly respected?'
  id: totrans-391
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中使用的资产（例如，代码、数据、模型）的创作者或原始所有者是否得到了适当的认可，许可证和使用条款是否明确提及并得到尊重？
- en: 'Answer: [Yes]'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[是]
- en: 'Justification: The paper credits the creators of the models, code and data
    that were used in it.'
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：该论文对使用的模型、代码和数据的创作者给予了认可。
- en: 'Guidelines:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not use existing assets.
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示该论文不使用现有资产。
- en: •
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should cite the original paper that produced the code package or
    dataset.
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应引用生成代码包或数据集的原始论文。
- en: •
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The authors should state which version of the asset is used and, if possible,
    include a URL.
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者应说明使用的资产版本，并在可能的情况下，提供一个网址。
- en: •
  id: totrans-401
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The name of the license (e.g., CC-BY 4.0) should be included for each asset.
  id: totrans-402
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个资产的许可证名称（例如，CC-BY 4.0）应包含在内。
- en: •
  id: totrans-403
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For scraped data from a particular source (e.g., website), the copyright and
    terms of service of that source should be provided.
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于从特定来源（例如，网站）抓取的数据，应提供该来源的版权和服务条款。
- en: •
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If assets are released, the license, copyright information, and terms of use
    in the package should be provided. For popular datasets, [paperswithcode.com/datasets](paperswithcode.com/datasets)
    has curated licenses for some datasets. Their licensing guide can help determine
    the license of a dataset.
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果资产被释放，包中应提供许可证、版权信息和使用条款。对于流行的数据集，[paperswithcode.com/datasets](paperswithcode.com/datasets)
    已为一些数据集编纂了许可证。他们的许可证指南可以帮助确定数据集的许可证。
- en: •
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For existing datasets that are re-packaged, both the original license and the
    license of the derived asset (if it has changed) should be provided.
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于重新包装的现有数据集，应提供原始许可证和派生资产的许可证（如果已更改）。
- en: •
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If this information is not available online, the authors are encouraged to reach
    out to the asset’s creators.
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果这些信息在网上不可用，建议作者联系资产的创作者。
- en: '13.'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '13.'
- en: New Assets
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新资产
- en: 'Question: Are new assets introduced in the paper well documented and is the
    documentation provided alongside the assets?'
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文中引入的新资产是否经过充分记录，并且文档是否与资产一起提供？
- en: 'Answer: [N/A]'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答案：[N/A]
- en: 'Justification: The paper is not releasing new assets.'
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：该论文没有释放新的资产。
- en: 'Guidelines:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not release new assets.
  id: totrans-418
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答案“NA”表示该论文不释放新的资产。
- en: •
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Researchers should communicate the details of the dataset/code/model as part
    of their submissions via structured templates. This includes details about training,
    license, limitations, etc.
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究人员应通过结构化模板在提交时传达数据集/代码/模型的详细信息。这包括有关训练、许可证、限制等的详细信息。
- en: •
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The paper should discuss whether and how consent was obtained from people whose
    asset is used.
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 论文应讨论是否以及如何获得了使用资产的人员的同意。
- en: •
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At submission time, remember to anonymize your assets (if applicable). You can
    either create an anonymized URL or include an anonymized zip file.
  id: totrans-424
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提交时，请记得对资产进行匿名化（如适用）。您可以创建一个匿名化的网址或包括一个匿名化的压缩文件。
- en: '14.'
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '14.'
- en: Crowdsourcing and Research with Human Subjects
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 众包和涉及人类受试者的研究
- en: 'Question: For crowdsourcing experiments and research with human subjects, does
    the paper include the full text of instructions given to participants and screenshots,
    if applicable, as well as details about compensation (if any)?'
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：对于众包实验和与人类受试者相关的研究，论文是否包括提供给参与者的完整指示文本和截图（如适用），以及有关补偿的详细信息（如有）？
- en: 'Answer: [N/A]'
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答复：[N/A]
- en: 'Justification: The paper does not involve crowdsourcing nor research with human
    subjects.'
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：该论文不涉及众包或与人类受试者相关的研究。
- en: 'Guidelines:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答复NA意味着该论文不涉及众包或与人类受试者相关的研究。
- en: •
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Including this information in the supplemental material is fine, but if the
    main contribution of the paper involves human subjects, then as much detail as
    possible should be included in the main paper.
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将这些信息包含在补充材料中是可以的，但如果论文的主要贡献涉及人类受试者，则应在主要论文中尽可能详细地包括这些信息。
- en: •
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: According to the NeurIPS Code of Ethics, workers involved in data collection,
    curation, or other labor should be paid at least the minimum wage in the country
    of the data collector.
  id: totrans-436
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据NeurIPS伦理规范，参与数据收集、整理或其他劳动的工人应至少获得数据收集者所在国家的最低工资。
- en: '15.'
  id: totrans-437
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '15.'
- en: Institutional Review Board (IRB) Approvals or Equivalent for Research with Human
    Subjects
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机构审查委员会（IRB）批准或等效的研究人类受试者
- en: 'Question: Does the paper describe potential risks incurred by study participants,
    whether such risks were disclosed to the subjects, and whether Institutional Review
    Board (IRB) approvals (or an equivalent approval/review based on the requirements
    of your country or institution) were obtained?'
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 问题：论文是否描述了研究参与者可能面临的潜在风险，这些风险是否已向受试者披露，以及是否获得了机构审查委员会（IRB）批准（或根据您所在国家或机构的要求进行的等效批准/审查）？
- en: 'Answer: [N/A]'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 答复：[N/A]
- en: 'Justification: The paper does not involve crowdsourcing nor research with human
    subjects.'
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理由：该论文不涉及众包或与人类受试者相关的研究。
- en: 'Guidelines:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指南：
- en: •
  id: totrans-443
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The answer NA means that the paper does not involve crowdsourcing nor research
    with human subjects.
  id: totrans-444
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 答复NA意味着该论文不涉及众包或与人类受试者相关的研究。
- en: •
  id: totrans-445
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Depending on the country in which research is conducted, IRB approval (or equivalent)
    may be required for any human subjects research. If you obtained IRB approval,
    you should clearly state this in the paper.
  id: totrans-446
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据研究进行的国家，可能需要IRB批准（或等效）用于任何人类受试者研究。如果您获得了IRB批准，您应在论文中明确说明。
- en: •
  id: totrans-447
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We recognize that the procedures for this may vary significantly between institutions
    and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and
    the guidelines for their institution.
  id: totrans-448
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们认识到这些程序在不同机构和地点之间可能会有显著差异，我们期望作者遵守NeurIPS伦理规范以及其机构的指导方针。
- en: •
  id: totrans-449
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For initial submissions, do not include any information that would break anonymity
    (if applicable), such as the institution conducting the review.
  id: totrans-450
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于初次提交，不要包括可能破坏匿名性的任何信息（如适用），例如进行审查的机构。
