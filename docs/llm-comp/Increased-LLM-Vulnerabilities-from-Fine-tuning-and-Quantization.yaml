- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:49:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Increased LLM Vulnerabilities from Fine-tuning and Quantization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从微调和量化中增加的LLM漏洞
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04392](https://ar5iv.labs.arxiv.org/html/2404.04392)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04392](https://ar5iv.labs.arxiv.org/html/2404.04392)
- en: Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal & Prashanth Harshangi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal & Prashanth Harshangi
- en: Enkrypt AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Enkrypt AI
- en: '{divyanshu, anurakt, sahil, prashanth}@enkryptai.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{divyanshu, anurakt, sahil, prashanth}@enkryptai.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have become very popular and have found use cases
    in many domains, such as chatbots, auto-task completion agents, and much more.
    However, LLMs are vulnerable to different types of attacks, such as jailbreaking,
    prompt injection attacks, and privacy leakage attacks. Foundational LLMs undergo
    adversarial and alignment training to learn not to generate malicious and toxic
    content. For specialized use cases, these foundational LLMs are subjected to fine-tuning
    or quantization for better performance and efficiency. We examine the impact of
    downstream tasks such as fine-tuning and quantization on LLM vulnerability. We
    test foundation models like Mistral, Llama, MosaicML, and their fine-tuned versions.
    Our research shows that fine-tuning and quantization reduces jailbreak resistance
    significantly, leading to increased LLM vulnerabilities. Finally, we demonstrate
    the utility of external guardrails in reducing LLM vulnerabilities.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经变得非常流行，并且在许多领域找到了应用，如聊天机器人、自动任务完成代理等。然而，LLMs 对各种攻击方式是脆弱的，比如越狱、提示注入攻击和隐私泄露攻击。基础LLMs经过对抗性和对齐训练，以学习不生成恶意和有毒内容。对于特定的使用案例，这些基础LLMs会进行微调或量化以提高性能和效率。我们研究了下游任务如微调和量化对LLM漏洞的影响。我们测试了像Mistral、Llama、MosaicML这样的基础模型及其微调版本。我们的研究表明，微调和量化显著降低了越狱抗性，导致LLM漏洞增加。最后，我们展示了外部保护措施在减少LLM漏洞方面的效用。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative models are becoming more and more important as they are becoming
    capable of automating a lot of tasks, taking autonomous actions and decisions,
    and at the same time, becoming better at content generation and summarization
    tasks. As LLMs become more powerful, these capabilities are at risk of being misused
    by an adversary, which can lead to fake content generation, toxic, malicious,
    or hateful content generation, privacy leakages, copyrighted content generation,
    and much more Chao et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14))
    Zou et al. ([2023](#bib.bib23)) Greshake et al. ([2023](#bib.bib6)) Liu et al.
    ([2023](#bib.bib13)) Zhu et al. ([2023](#bib.bib22)) He et al. ([2021](#bib.bib7))
    Le et al. ([2020](#bib.bib12)). To prevent LLMs from generating content that contradicts
    human values and to prevent their malicious misuse, they undergo a supervised
    fine-tuning phase after their pre-training, and they are further evaluated by
    humans and trained using reinforcement learning from human feedback (RLHF) Ouyang
    et al. ([2022](#bib.bib15)), to make them more aligned with human values. Further,
    special filters called guardrails are put in place as filters to prevent LLMs
    from getting toxic prompts as inputs and outputting toxic or copyrighted responses
    Rebedea et al. ([2023](#bib.bib17)) Kumar et al. ([2023](#bib.bib11)) Wei et al.
    ([2023](#bib.bib18)) Zhou et al. ([2024](#bib.bib21)). The complexity of human
    language makes it difficult for LLMs to completely understand what instructions
    are right and which are wrong in terms of human values. After going through the
    alignment training and after the implementation of guardrails, it becomes unlikely
    that the LLM will generate a toxic response. But these safety measures can easily
    be circumvented using adversarial attacks, and the LLM can be jailbroken to generate
    any content that the adversary wants, as shown in recent works Chao et al. ([2023](#bib.bib2))
    Mehrotra et al. ([2023](#bib.bib14)) Zhu et al. ([2023](#bib.bib22)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型变得越来越重要，因为它们能够自动化许多任务，采取自主行动和决策，同时在内容生成和总结任务上也变得更加出色。随着大型语言模型（LLMs）变得越来越强大，这些能力有被对手滥用的风险，这可能导致虚假内容生成、恶性、恶意或仇恨内容生成、隐私泄露、版权内容生成等问题。Chao
    et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14)) Zou et al. ([2023](#bib.bib23))
    Greshake et al. ([2023](#bib.bib6)) Liu et al. ([2023](#bib.bib13)) Zhu et al.
    ([2023](#bib.bib22)) He et al. ([2021](#bib.bib7)) Le et al. ([2020](#bib.bib12))。为了防止LLMs生成违背人类价值观的内容并防止其恶意滥用，它们在预训练后会经历监督微调阶段，并由人类进一步评估，通过强化学习（RLHF）进行训练，以使其更符合人类价值观。Ouyang
    et al. ([2022](#bib.bib15))。此外，还设置了称为“防护栏”的特殊过滤器，以防止LLMs接收有毒的输入提示并输出有毒或版权内容的响应。Rebedea
    et al. ([2023](#bib.bib17)) Kumar et al. ([2023](#bib.bib11)) Wei et al. ([2023](#bib.bib18))
    Zhou et al. ([2024](#bib.bib21))。人类语言的复杂性使得LLMs很难完全理解哪些指令符合人类价值观，哪些是不符合的。在经过对齐训练和实施防护栏后，LLM生成有毒响应的可能性变得不大。但这些安全措施很容易被对抗攻击绕过，LLM可能被破解以生成对手想要的任何内容，如最近的研究所示。Chao
    et al. ([2023](#bib.bib2)) Mehrotra et al. ([2023](#bib.bib14)) Zhu et al. ([2023](#bib.bib22))。
- en: 'Recent works such as the Prompt Automatic Iterative Refinement (PAIR) attacks
    Chao et al. ([2023](#bib.bib2)) and Tree-of-attacks pruning (TAP) Mehrotra et al.
    ([2023](#bib.bib14)) have shown the vulnerability of LLMs and how easy it is to
    jailbreak them into generating content for harmful tasks specified by the user.
    Similarly, a class of methods called privacy leakage attacks Debenedetti et al.
    ([2023](#bib.bib3)) are used to attack LLMs to extract their training data or
    personally identifiable information Kim et al. ([2023](#bib.bib10)), and prompt
    injection attacks can be used to make an LLM application perform tasks that are
    not requested by the user but are hidden in the third-party instruction which
    the LLM automatically executes. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Increased LLM Vulnerabilities from Fine-tuning and Quantization") shows how an
    instruction can be hidden inside a summarization text and how the LLM will ignore
    the previous instruction to execute the malicious instruction. Qi et al. ([2023](#bib.bib16))
    showed that it only takes a few examples to fine-tune an LLM into generating toxic
    responses by forgetting its safety training. Our work in this paper extends that
    notion and shows that both fine-tuning the LLM on any task (not necessarily toxic
    content generation) and quantization can affect its safety training. In this study,
    we use a subset of adversarial harmful prompts called AdvBench SubsetAndy Zou
    ([2023](#bib.bib1)). It contains 50 prompts asking for harmful information across
    32 categories. It is a subset of prompts from the harmful behaviours dataset in
    the AdvBench benchmark selected to cover a diverse range of harmful prompts. The
    attacking algorithm used is tree-of-attacks pruning Mehrotra et al. ([2023](#bib.bib14))
    as it has shown to have the best performance in jailbreaking and, more importantly,
    this algorithm fulfils three important goals (1) Black-box: the algorithm only
    needs black-box access to the model (2) Automatic: it does not need human intervention
    once started, and (3) Interpretable: the algorithm generates semantically meaningful
    prompts. The TAP algorithm is used with the tasks from the AdvBench subset to
    attack the target LLMs in different settings, and their response is used to evaluate
    whether or not they have been jailbroken.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究，如Chao等人提出的Prompt Automatic Iterative Refinement (PAIR) 攻击 ([2023](#bib.bib2))
    和 Mehrotra等人提出的Tree-of-attacks pruning (TAP) ([2023](#bib.bib14))，展示了LLM的脆弱性以及如何轻松地使其生成用户指定的有害任务内容。同样，一类称为隐私泄露攻击的方法
    Debenedetti等人 ([2023](#bib.bib3)) 被用于攻击LLM，以提取其训练数据或个人身份信息 Kim等人 ([2023](#bib.bib10))，并且提示注入攻击可以使LLM应用执行用户未请求但隐藏在第三方指令中的任务，这些指令会被LLM自动执行。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Increased LLM Vulnerabilities from Fine-tuning and
    Quantization") 显示了如何在摘要文本中隐藏指令，以及LLM如何忽略之前的指令来执行恶意指令。Qi等人 ([2023](#bib.bib16))
    证明了只需几个示例即可使LLM通过遗忘其安全训练生成有毒回应。本文的工作扩展了这一概念，表明无论是在任何任务上（不一定是有毒内容生成）对LLM进行微调还是量化都可能影响其安全训练。在这项研究中，我们使用了一组名为AdvBench
    SubsetAndy Zou ([2023](#bib.bib1)) 的对抗性有害提示。它包含50个请求有害信息的提示，覆盖32个类别。它是AdvBench基准中有害行为数据集的一个子集，旨在覆盖多种有害提示。使用的攻击算法是Tree-of-attacks
    pruning Mehrotra等人 ([2023](#bib.bib14))，因为它在破解中表现最佳，更重要的是，这个算法满足三个重要目标：（1）黑箱：该算法仅需要对模型的黑箱访问，（2）自动：一旦开始，不需要人工干预，和（3）可解释：该算法生成语义上有意义的提示。TAP算法用于处理AdvBench子集中的任务，以在不同设置中攻击目标LLM，其响应用于评估是否已被破解。
- en: '![Refer to caption](img/d9934cea2f2064860b4868467c9271aa.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9934cea2f2064860b4868467c9271aa.png)'
- en: 'Figure 1: An example of an adversarial attack on LLM. Here, GPT-3.5 ignores
    the original instruction of summarizing the text and executes the last instruction
    in angle brackets hidden in the text'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM的对抗性攻击示例。这里，GPT-3.5忽略了原始的文本摘要指令，并执行了隐藏在文本中的尖括号内的最后指令
- en: The rest of the paper is organized in the following manner. Section [2](#S2
    "2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization") talks about the experimental setup for jailbreaking in which
    the models are tested. It specifically describes the different modes of downstream
    process that an LLM has undergone e.g fine-tuning, quantization and tested for
    these modes. Section [3](#S3 "3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization") describes the experiment set-up used and defines
    guardrails Rebedea et al. ([2023](#bib.bib17)), fine-tuning and quantization Kashiwamura
    et al. ([2024](#bib.bib9)) Gorsline et al. ([2021](#bib.bib5)) Xiao et al. ([2023](#bib.bib20))
    Hu et al. ([2021](#bib.bib8)) Dettmers et al. ([2023](#bib.bib4)) settings used
    in the experimental context. We demonstrate the results in detail and show how
    model vulnerability is affected by downstream tasks for LLMs. Finally, section
    [4](#S4 "4 Conclusion ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")
    concludes the study and talks about methods to reduce model vulnerability and
    ensure safe and reliable LLM development.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分按以下方式组织。第[2](#S2 "2 Problem Formulation and Experiments ‣ Increased LLM
    Vulnerabilities from Fine-tuning and Quantization")节讨论了用于越狱实验的模型测试设置。它具体描述了LLM经历的不同下游处理模式，例如微调、量化，并对这些模式进行测试。第[3](#S3
    "3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization")节描述了实验设置和定义了防护措施 Rebedea et al. ([2023](#bib.bib17))、微调和量化 Kashiwamura
    et al. ([2024](#bib.bib9))、Gorsline et al. ([2021](#bib.bib5))、Xiao et al. ([2023](#bib.bib20))、Hu
    et al. ([2021](#bib.bib8))、Dettmers et al. ([2023](#bib.bib4)) 在实验背景下使用的设置。我们详细展示了结果，并展示了下游任务如何影响LLM的模型脆弱性。最后，第[4](#S4
    "4 Conclusion ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")节总结了研究，并讨论了减少模型脆弱性的方法，以确保LLM的安全和可靠发展。
- en: 2 Problem Formulation and Experiments
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题定义与实验
- en: We want to understand the role played by fine-tuning, quantization, and guardrails
    on LLM’s vulnerability towards jailbreaking attacks. We create a pipeline to test
    for jailbreaking of LLMs which undergo these further processes before deployment.
    As mentioned in the introduction, we attack the LLM via the TAP algorithm using
    the AdvBench subset. We use a subset of AdvBenchAndy Zou ([2023](#bib.bib1)).
    It contains 50 prompts asking for harmful information across 32 categories. The
    evaluation results, along with the complete system information, are then logged.
    The overall flow is shown in the figure [2](#S2.F2 "Figure 2 ‣ 2 Problem Formulation
    and Experiments ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization").
    This process continues for multiple iterations, taking into account the stochastic
    nature associated with LLMs. The complete experiment pipeline is shown in Figure
    [2](#S2.F2 "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization").
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要了解微调、量化和防护措施对LLM（大型语言模型）在越狱攻击中的脆弱性所起的作用。我们创建了一个管道来测试这些在部署前经历进一步处理的LLM的越狱情况。正如介绍中提到的，我们通过TAP算法使用AdvBench子集对LLM进行攻击。我们使用了AdvBenchAndy
    Zou ([2023](#bib.bib1))的一个子集。它包含50个请求有害信息的提示，分为32个类别。评估结果以及完整的系统信息随后被记录下来。整体流程如图[2](#S2.F2
    "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization")所示。这个过程会持续多个迭代，考虑到与LLM相关的随机性。完整的实验流程见图[2](#S2.F2
    "Figure 2 ‣ 2 Problem Formulation and Experiments ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization")。
- en: '![Refer to caption](img/5289f526979f8e92057d57f6556b25a6.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5289f526979f8e92057d57f6556b25a6.png)'
- en: 'Figure 2: Jailbreak process followed to generate the reports'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：生成报告的越狱过程
- en: TAP Mehrotra et al. ([2023](#bib.bib14)) is used as the jailbreaking method,
    as it is currently the state-of-the-art, black-box, and automatic method which
    generates prompts with semantic meaning to jailbreak LLMs. TAP algorithm uses
    an attacker LLM A, which sends a prompt P to the target LLM T. The response of
    the target LLM R along with the prompt P are fed into the evaluator LLM JUDGE,
    which determines if the prompt is on-topic or off-topic. If the prompt is off-topic,
    it is removed, thereby eliminating the tree of bad attack prompts it was going
    to generate. Otherwise, if the prompt is on-topic, it receives a score between
    0-10 by the JUDGE LLM. This prompt is used to generate a new attack prompt using
    breadth-first search. This process continues till the LLM is jailbroken or a specified
    number of iterations are exhausted.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: TAP Mehrotra 等人 ([2023](#bib.bib14)) 被用作越狱方法，因为它是当前最先进的、黑盒的、自动化的方法，它生成具有语义意义的提示来越狱
    LLM。TAP 算法使用攻击者 LLM A，该模型向目标 LLM T 发送提示 P。目标 LLM R 的响应以及提示 P 会被送入评估者 LLM JUDGE，JUDGE
    确定提示是相关的还是不相关的。如果提示不相关，则会被移除，从而消除将要生成的不良攻击提示的树。否则，如果提示相关，则由 JUDGE LLM 给予 0-10
    的评分。该提示用于使用广度优先搜索生成新的攻击提示。这一过程会持续进行，直到 LLM 被越狱或耗尽指定的迭代次数。
- en: Now describing our guardrail against jailbreaking prompts, we use our in-house
    Deberta-V3 model, which has been trained to detect jailbreaking prompts. It acts
    as an input filter to ensure that only sanitized prompts are received by the LLM.
    If the input prompt is filtered out by the guardrail or fails to jailbreak the
    LLM, then the TAP algorithm generates a new prompt considering the initial prompt
    and response. The new attacking prompt is then again passed through the guardrail.
    This process is repeated till a jailbreaking prompt is found or a pre-specified
    number of iterations are exhausted.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在描述我们对抗越狱提示的防护措施，我们使用内部开发的 Deberta-V3 模型，该模型经过训练可以检测越狱提示。它充当输入过滤器，以确保只有经过净化的提示被
    LLM 接收。如果输入提示被防护措施过滤掉或未能越狱 LLM，那么 TAP 算法会生成一个新的提示，考虑到初始提示和响应。新的攻击提示会再次通过防护措施。这一过程会重复进行，直到找到越狱提示或耗尽预先指定的迭代次数。
- en: 3 Experiment Set-up & Results
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置与结果
- en: 'The following section outlines the testing environments utilized for various
    LLMs. The LLMs are tested under three different downstream tasks: (1) fine-tuning,
    (2) quantization, and (3) guardrails. They are chosen to cover most of the LLM
    practical use cases and applications in the industry and academia. For TAP configuration,
    as mentioned before, we use GPT-3.5-turbo as the attack model, and GPT-4-turbo
    as the judge model. We employ Anyscale endpoints, the OpenAI API, and HuggingFace
    for our target model. Further information regarding the model and its sources
    is available in appendix [A.1](#A1.SS1 "A.1 Experiment Utils ‣ Appendix A Appendix
    ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization"). The details
    about different settings are mentioned below :'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分概述了用于各种 LLM 的测试环境。这些 LLM 在三个不同的下游任务下进行测试：(1) 微调，(2) 量化，以及 (3) 防护措施。它们的选择旨在覆盖大多数
    LLM 在工业和学术界的实际应用场景。对于 TAP 配置，如前所述，我们使用 GPT-3.5-turbo 作为攻击模型，GPT-4-turbo 作为评估模型。我们使用
    Anyscale 端点、OpenAI API 和 HuggingFace 作为我们的目标模型。关于模型及其来源的进一步信息见附录 [A.1](#A1.SS1
    "A.1 Experiment Utils ‣ Appendix A Appendix ‣ Increased LLM Vulnerabilities from
    Fine-tuning and Quantization")。不同设置的详细信息如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-tuning: Fine-tuning LLMs on different tasks increases their effectiveness
    in completing the tasks as it incorporates the specialized domain knowledge needed;
    these can include SQL code generation, chat, and more. We compare the jailbreaking
    vulnerability of foundational models compared to their fine tune versions. This
    helps us understand the role of fine-tuning in increasing or decreasing the vulnerability
    of LLMs and the strategies to mitigate this risk Weyssow et al. ([2023](#bib.bib19)).
    We use foundation models such as Llama2, Mistral, and MPT-7B and their fine-tuned
    versions such as CodeLlama, SQLCoder, Dolphin, and Intel Neural Chat. The details
    of the models and versions are specified in the appendix. From the table  [1](#S3.T1
    "Table 1 ‣ 1st item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities
    from Fine-tuning and Quantization"), we empirically conclude that fine-tuned models
    lose their safety alignment and are jailbroken quite easily compared to the foundational
    models.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调：在不同任务上对 LLM 进行微调可以提高它们在完成这些任务时的有效性，因为这涉及到所需的专业领域知识，这些任务可以包括 SQL 代码生成、聊天等。我们比较了基础模型与其微调版本的破解脆弱性。这有助于我们理解微调在增加或减少
    LLM 脆弱性中的作用以及减少这种风险的策略（Weyssow 等人，[2023](#bib.bib19)）。我们使用 Llama2、Mistral 和 MPT-7B
    等基础模型及其微调版本，如 CodeLlama、SQLCoder、Dolphin 和 Intel Neural Chat。模型和版本的详细信息在附录中列出。从表
    [1](#S3.T1 "Table 1 ‣ 1st item ‣ 3 Experiment Set-up & Results ‣ Increased LLM
    Vulnerabilities from Fine-tuning and Quantization") 中，我们实证得出结论，微调后的模型失去了其安全对齐，并且比基础模型更容易被破解。
- en: 'Table 1: Effect of fine-tuning on model vulnerability'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 1：微调对模型脆弱性的影响
- en: '| Model | Derived From | Finetune | Jailbreak(%) |'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型 | 来源 | 微调 | 破解(%) |'
- en: '| Llama2-7B | – | – | 6 |'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | – | – | 6 |'
- en: '| CodeLlama-7B | Llama2-7B | Yes | 32 |'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | Llama2-7B | 是 | 32 |'
- en: '| SQLCoder-2 | CodeLlama-7B | Yes | 82 |'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| SQLCoder-2 | CodeLlama-7B | 是 | 82 |'
- en: '| Mistral-7B-v0.1 | – | – | 85.3 |'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | – | – | 85.3 |'
- en: '| dolphin-2.2.1-Mistral-7B-v0.1 | Mistral-7B-v0.1 | Yes | 99 |'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| dolphin-2.2.1-Mistral-7B-v0.1 | Mistral-7B-v0.1 | 是 | 99 |'
- en: '| MPT-7B | – | – | 93 |'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| MPT-7B | – | – | 93 |'
- en: '| IntelNeuralChat-7B | MPT-7B | Yes | 94 |'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| IntelNeuralChat-7B | MPT-7B | 是 | 94 |'
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quantization: Many models require huge computational resources during training,
    fine-tuning, and even during inference Hu et al. ([2021](#bib.bib8)). Quantization
    is one of the most popular ways to reduce the computational burden, but it comes
    at the cost of the numerical precision of model parameters. The quantized models
    we evaluate below use GPT-Generated Unified Format (GGUF) for quantization, which
    involves scaling down model weights (stored in 16-bit floating point numbers)
    to save computational resources at the cost of numerical precision of the model
    parameters. Kashiwamura et al. ([2024](#bib.bib9)) Gorsline et al. ([2021](#bib.bib5))
    Xiao et al. ([2023](#bib.bib20)) Hu et al. ([2021](#bib.bib8)) Dettmers et al.
    ([2023](#bib.bib4)). The table  [2](#S3.T2 "Table 2 ‣ 2nd item ‣ 3 Experiment
    Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization")
    demonstrates that quantization of the model renders it susceptible to vulnerabilities.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化：许多模型在训练、微调，甚至推理过程中都需要大量的计算资源（Hu 等人，[2021](#bib.bib8)）。量化是减少计算负担的最流行方法之一，但它以模型参数的数值精度为代价。我们下面评估的量化模型使用
    GPT 生成的统一格式（GGUF）进行量化，这涉及将模型权重（以 16 位浮点数存储）缩小，以节省计算资源，但会损失模型参数的数值精度。Kashiwamura
    等人（[2024](#bib.bib9)），Gorsline 等人（[2021](#bib.bib5)），Xiao 等人（[2023](#bib.bib20)），Hu
    等人（[2021](#bib.bib8)），Dettmers 等人（[2023](#bib.bib4)）。表 [2](#S3.T2 "Table 2 ‣ 2nd
    item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization") 显示，模型的量化使其易受漏洞攻击。
- en: 'Table 2: Effect of quantization on model vulnerability'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 2：量化对模型脆弱性的影响
- en: '| Model Name | Source Model | Quantization | Jailbreak(%) |'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型名称 | 来源模型 | 量化 | 破解(%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7B | – | – | 6 |'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | – | – | 6 |'
- en: '| Llama-2-7B-Chat-GGUF-8bit | Llama2-7B | Yes | 9 |'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat-GGUF-8bit | Llama2-7B | 是 | 9 |'
- en: '| CodeLlama-7B | – | – | 32 |'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | – | – | 32 |'
- en: '| CodeLlama-7B-GGUF-8bit | CodeLlama-7B | Yes | 72 |'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF-8bit | CodeLlama-7B | 是 | 72 |'
- en: '| Mistral-7B-v0.1 | – | – | 85.3 |'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | – | – | 85.3 |'
- en: '| Mistral-7B-v0.1-GGUF-8bit | Mistral-7B-v0.1 | Yes | 96 |'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1-GGUF-8bit | Mistral-7B-v0.1 | 是 | 96 |'
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Guardrails: Guardrails act as a line of defence against LLM attacks. Their
    primary function is to meticulously filter out prompts that could potentially
    lead to harmful or malicious outcomes, preventing such prompts from reaching the
    LLM as an instruction Rebedea et al. ([2023](#bib.bib17)). This proactive approach
    not only promotes the safe utilization of LLMs but also facilitates their optimal
    performance, thereby maximizing their potential benefits in various domains Kumar
    et al. ([2023](#bib.bib11)) Wei et al. ([2023](#bib.bib18)) Zhou et al. ([2024](#bib.bib21)).
    We use our proprietary jailbreak attack detector derived from Deberta-V3 models,
    trained on harmful prompts generated to jailbreak LLMs. You can reach out to the
    authors to get more details on this model. From table  [3](#S3.T3 "Table 3 ‣ 3rd
    item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from Fine-tuning
    and Quantization"), we observe that the introduction of guardrails as a pre-step
    has a significant effect and can mitigate jailbreaking attempts by a considerable
    margin.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 保护措施：保护措施充当抵御 LLM 攻击的防线。它们的主要功能是仔细筛选可能导致有害或恶意结果的提示，防止这些提示作为指令到达 LLM Rebedea
    等（[2023](#bib.bib17)）。这种主动的方法不仅促进了 LLM 的安全使用，还促进了其最佳性能，从而最大化其在各个领域的潜在收益 Kumar
    等（[2023](#bib.bib11)）Wei 等（[2023](#bib.bib18)）Zhou 等（[2024](#bib.bib21)）。我们使用从
    Deberta-V3 模型派生的专有破解攻击检测器，该模型在生成破解 LLM 的有害提示时进行训练。您可以联系作者以获取有关此模型的更多详细信息。从表[3](#S3.T3
    "表 3 ‣ 第 3 项 ‣ 3 实验设置与结果 ‣ 微调和量化后 LLM 的脆弱性增加")中，我们观察到引入保护措施作为预处理步骤具有显著效果，并且可以显著减少破解尝试。
- en: 'Table 3: Effect of guardrails on model vulnerability'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 3：保护措施对模型脆弱性的影响
- en: '| Model Name | Jailbreak (%) | Jailbreak w/ guardrails(%) | Factor Improvement
    |'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 模型名称 | 破解率 (%) | 带保护措施的破解率 (%) | 改进因子 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7B | 6 | 0.67 | 9x |'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama2-7B | 6 | 0.67 | 9x |'
- en: '| Mistral-7B-v0.1 | 85.3 | 31.3 | 2.7x |'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mistral-7B-v0.1 | 85.3 | 31.3 | 2.7x |'
- en: '| Mixtral7x8B | 52 | 7 | 7.4x |'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mixtral7x8B | 52 | 7 | 7.4x |'
- en: '| CodeLLama-34B | 18 | 1 | 18x |'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLLama-34B | 18 | 1 | 18x |'
- en: '| CodeLlama-7B | 32 | 2 | 16x |'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B | 32 | 2 | 16x |'
- en: '| SQLCoder | 82 | 12.7 | 6.5x |'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| SQLCoder | 82 | 12.7 | 6.5x |'
- en: '| Llama-2-7B-Chat-GGUF | 9 | 1 | 9x |'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Llama-2-7B-Chat-GGUF | 9 | 1 | 9x |'
- en: '| CodeLlama-7B-GGUF | 72 | 15 | 4.8x |'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF | 72 | 15 | 4.8x |'
- en: '| Phi2 | 97 | 21 | 4.6x |'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Phi2 | 97 | 21 | 4.6x |'
- en: The results from tables  [1](#S3.T1 "Table 1 ‣ 1st item ‣ 3 Experiment Set-up
    & Results ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization"),
     [2](#S3.T2 "Table 2 ‣ 2nd item ‣ 3 Experiment Set-up & Results ‣ Increased LLM
    Vulnerabilities from Fine-tuning and Quantization"), and  [3](#S3.T3 "Table 3
    ‣ 3rd item ‣ 3 Experiment Set-up & Results ‣ Increased LLM Vulnerabilities from
    Fine-tuning and Quantization") conclusively show the vulnerability of LLMs post
    fine-tuning and quantization, and effectiveness of external guardrails in mitigating
    this vulnerability. The detailed experimental results can be found in appendix
    [A.2](#A1.SS2 "A.2 Experiment Results in details ‣ Appendix A Appendix ‣ Increased
    LLM Vulnerabilities from Fine-tuning and Quantization"). Additionally, this section
    provides a comprehensive analysis of the experiments conducted, offering insights
    into the various aspects of the research findings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[1](#S3.T1 "表 1 ‣ 第 1 项 ‣ 3 实验设置与结果 ‣ 微调和量化后 LLM 的脆弱性增加")、[2](#S3.T2 "表 2
    ‣ 第 2 项 ‣ 3 实验设置与结果 ‣ 微调和量化后 LLM 的脆弱性增加")和[3](#S3.T3 "表 3 ‣ 第 3 项 ‣ 3 实验设置与结果
    ‣ 微调和量化后 LLM 的脆弱性增加")的结果明确显示了 LLM 在微调和量化后的脆弱性，以及外部保护措施在缓解这种脆弱性方面的有效性。详细的实验结果可以在附录[A.2](#A1.SS2
    "A.2 实验结果详细信息 ‣ 附录 A ‣ 微调和量化后 LLM 的脆弱性增加")中找到。此外，本节提供了对进行的实验的全面分析，提供了对研究结果各个方面的见解。
- en: 4 Conclusion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: Our work investigates the LLM’s safety against Jailbreak attempts. We have demonstrated
    how fine-tuned and quantized models are vulnerable to jailbreak attempts and stress
    the importance of using external guardrails to reduce this risk. Fine-tuning or
    quantizing model weights alters the risk profile of LLMs, potentially undermining
    the safety alignment established through RLHF. This could result from catastrophic
    forgetting, where LLMs lose memory of safety protocols, or the fine-tuning process
    shifting the model’s focus to new topics at the expense of existing safety measures.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作探讨了LLM在破解尝试中的安全性。我们展示了经过微调和量化的模型如何易受破解尝试的影响，并强调了使用外部保护措施以降低此风险的重要性。微调或量化模型权重会改变LLMs的风险特征，可能会削弱通过RLHF建立的安全对齐。这可能源于灾难性遗忘，LLMs失去对安全协议的记忆，或微调过程将模型的重点转移到新主题上，从而牺牲现有的安全措施。
- en: The lack of safety measures in these fine-tuned and quantized models is concerning,
    highlighting the need to incorporate safety protocols during the fine-tuning process.
    We propose using these tests as part of a CI/CD stress test before deploying the
    model. The effectiveness of guardrails in preventing jailbreaking highlights the
    importance of integrating them with safety practices in AI development. This approach
    not only enhances AI models but also establishes a new standard for responsible
    AI development. By ensuring that AI advancements prioritize innovation and safety,
    we promote ethical AI deployment, safeguarding against potential misuse and fostering
    a secure digital future.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经过微调和量化的模型缺乏安全措施令人担忧，这突显了在微调过程中纳入安全协议的必要性。我们建议在部署模型之前，将这些测试作为CI/CD压力测试的一部分。保护措施在防止破解中的有效性突显了将其与AI开发中的安全实践整合的重要性。这种方法不仅提升了AI模型，还为负责任的AI开发建立了新标准。通过确保AI进步优先考虑创新和安全，我们促进了道德AI部署，保护潜在误用，促进安全的数字未来。
- en: References
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Andy Zou (2023) Zifan Wang Andy Zou. AdvBench Dataset, July 2023. URL [https://github.com/llm-attacks/llm-attacks/tree/main/data](https://github.com/llm-attacks/llm-attacks/tree/main/data).
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andy Zou (2023) Zifan Wang Andy Zou. AdvBench 数据集，2023年7月。URL [https://github.com/llm-attacks/llm-attacks/tree/main/data](https://github.com/llm-attacks/llm-attacks/tree/main/data)。
- en: 'Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. Jailbreaking Black Box Large Language Models
    in Twenty Queries. *arXiv*, October 2023. doi: 10.48550/arXiv.2310.08419.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, 和 Eric Wong. 在二十次查询中破解黑箱大型语言模型。*arXiv*，2023年10月。doi: 10.48550/arXiv.2310.08419。'
- en: 'Debenedetti et al. (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, and
    Florian Tramèr. Privacy Side Channels in Machine Learning Systems. *arXiv*, September
    2023. doi: 10.48550/arXiv.2309.05610.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Debenedetti et al. (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, 和
    Florian Tramèr. 机器学习系统中的隐私侧通道。*arXiv*，2023年9月。doi: 10.48550/arXiv.2309.05610。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. *arXiv*, May 2023.
    doi: 10.48550/arXiv.2305.14314.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. QLoRA：量化LLMs的高效微调。*arXiv*，2023年5月。doi: 10.48550/arXiv.2305.14314。'
- en: 'Gorsline et al. (2021) Micah Gorsline, James Smith, and Cory Merkel. On the
    Adversarial Robustness of Quantized Neural Networks. *arXiv*, May 2021. doi: 10.1145/3453688.3461755.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gorsline et al. (2021) Micah Gorsline, James Smith, 和 Cory Merkel. 量化神经网络的对抗鲁棒性。*arXiv*，2021年5月。doi:
    10.1145/3453688.3461755。'
- en: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. Not what you’ve signed up for: Compromising
    Real-World LLM-Integrated Applications with Indirect Prompt Injection. *arXiv*,
    February 2023. doi: 10.48550/arXiv.2302.12173.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Greshake et al. (2023) Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, 和 Mario Fritz. 并非你所签署的：通过间接提示注入危害真实世界的LLM集成应用。*arXiv*，2023年2月。doi:
    10.48550/arXiv.2302.12173。'
- en: 'He et al. (2021) Bing He, Mustaque Ahamad, and Srijan Kumar. PETGEN: Personalized
    Text Generation Attack on Deep Sequence Embedding-based Classification Models.
    In *KDD ’21: Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*, pp.  575–584\. Association for Computing Machinery, New York,
    NY, USA, August 2021. ISBN 978-1-45038332-5. doi: 10.1145/3447548.3467390.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He 等（2021）Bing He、Mustaque Ahamad 和 Srijan Kumar。PETGEN：针对深度序列嵌入分类模型的个性化文本生成攻击。在
    *KDD ’21：第27届ACM SIGKDD知识发现与数据挖掘大会论文集*，第575–584页。计算机协会，纽约，NY，USA，2021年8月。ISBN
    978-1-45038332-5。doi: 10.1145/3447548.3467390。'
- en: 'Hu et al. (2021) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of
    Large Language Models. *arXiv*, June 2021. doi: 10.48550/arXiv.2106.09685.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等（2021）Edward J. Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。LoRA：大型语言模型的低秩适应。*arXiv*，2021年6月。doi: 10.48550/arXiv.2106.09685。'
- en: 'Kashiwamura et al. (2024) Shuhei Kashiwamura, Ayaka Sakata, and Masaaki Imaizumi.
    Effect of Weight Quantization on Learning Models by Typical Case Analysis. *arXiv*,
    January 2024. doi: 10.48550/arXiv.2401.17269.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kashiwamura 等（2024）Shuhei Kashiwamura、Ayaka Sakata 和 Masaaki Imaizumi。典型案例分析中权重量化对学习模型的影响。*arXiv*，2024年1月。doi:
    10.48550/arXiv.2401.17269。'
- en: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, and Seong Joon Oh. ProPILE: Probing Privacy Leakage in Large Language Models.
    *arXiv*, July 2023. doi: 10.48550/arXiv.2307.01881.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等（2023）Siwon Kim、Sangdoo Yun、Hwaran Lee、Martin Gubri、Sungroh Yoon 和 Seong
    Joon Oh。ProPILE：探测大型语言模型中的隐私泄露。*arXiv*，2023年7月。doi: 10.48550/arXiv.2307.01881。'
- en: 'Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun
    Li, Soheil Feizi, and Himabindu Lakkaraju. Certifying LLM Safety against Adversarial
    Prompting. *arXiv*, September 2023. doi: 10.48550/arXiv.2309.02705.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar 等（2023）Aounon Kumar、Chirag Agarwal、Suraj Srinivas、Aaron Jiaxun Li、Soheil
    Feizi 和 Himabindu Lakkaraju。证明 LLM 安全性以对抗对抗性提示。*arXiv*，2023年9月。doi: 10.48550/arXiv.2309.02705。'
- en: 'Le et al. (2020) Thai Le, Suhang Wang, and Dongwon Lee. *MALCOM: Generating
    Malicious Comments to Attack Neural Fake News Detection Models*. IEEE Computer
    Society, November 2020. ISBN 978-1-7281-8316-9. doi: 10.1109/ICDM50108.2020.00037.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Le 等（2020）Thai Le、Suhang Wang 和 Dongwon Lee。*MALCOM：生成恶意评论以攻击神经虚假新闻检测模型*。IEEE计算机学会，2020年11月。ISBN
    978-1-7281-8316-9。doi: 10.1109/ICDM50108.2020.00037。'
- en: 'Liu et al. (2023) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via Prompt
    Engineering: An Empirical Study. *arXiv*, May 2023. doi: 10.48550/arXiv.2305.13860.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023）Yi Liu、Gelei Deng、Zhengzi Xu、Yuekang Li、Yaowen Zheng、Ying Zhang、Lida
    Zhao、Tianwei Zhang 和 Yang Liu。通过提示工程破解 ChatGPT：一项实证研究。*arXiv*，2023年5月。doi: 10.48550/arXiv.2305.13860。'
- en: 'Mehrotra et al. (2023) Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine
    Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of Attacks: Jailbreaking
    Black-Box LLMs Automatically. *arXiv*, December 2023. doi: 10.48550/arXiv.2312.02119.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehrotra 等（2023）Anay Mehrotra、Manolis Zampetakis、Paul Kassianik、Blaine Nelson、Hyrum
    Anderson、Yaron Singer 和 Amin Karbasi。攻击树：自动破解黑箱 LLM。*arXiv*，2023年12月。doi: 10.48550/arXiv.2312.02119。'
- en: 'Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback. *arXiv*, March 2022. doi: 10.48550/arXiv.2203.02155.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ouyang 等（2022）Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    Christiano、Jan Leike 和 Ryan Lowe。训练语言模型以通过人类反馈遵循指令。*arXiv*，2022年3月。doi: 10.48550/arXiv.2203.02155。'
- en: 'Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia,
    Prateek Mittal, and Peter Henderson. Fine-tuning Aligned Language Models Compromises
    Safety, Even When Users Do Not Intend To! *arXiv*, October 2023. doi: 10.48550/arXiv.2310.03693.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qi 等（2023）Xiangyu Qi、Yi Zeng、Tinghao Xie、Pin-Yu Chen、Ruoxi Jia、Prateek Mittal
    和 Peter Henderson。微调对齐的语言模型会损害安全性，即使用户并不打算这样做！*arXiv*，2023年10月。doi: 10.48550/arXiv.2310.03693。'
- en: 'Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar,
    Christopher Parisien, and Jonathan Cohen. NeMo Guardrails: A Toolkit for Controllable
    and Safe LLM Applications with Programmable Rails. *ACL Anthology*, pp.  431–445,
    December 2023. doi: 10.18653/v1/2023.emnlp-demo.40.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rebedea et al. (2023) Traian Rebedea, Razvan Dinu, Makesh Narsimhan Sreedhar,
    Christopher Parisien, 和 Jonathan Cohen. NeMo Guardrails：一个用于可控和安全 LLM 应用的工具包，带有可编程轨道。*ACL
    Anthology*，第 431–445 页，2023 年 12 月。doi: 10.18653/v1/2023.emnlp-demo.40。'
- en: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How Does LLM Safety Training Fail? *arXiv*, July 2023. doi: 10.48550/arXiv.2307.02483.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt. Jailbroken：LLM
    安全训练为何失败？*arXiv*，2023 年 7 月。doi: 10.48550/arXiv.2307.02483。'
- en: 'Weyssow et al. (2023) Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, and Houari
    Sahraoui. Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation
    with Large Language Models. *arXiv*, August 2023. doi: 10.48550/arXiv.2308.10462.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Weyssow et al. (2023) Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, 和 Houari
    Sahraoui. 探索用于代码生成的大型语言模型的参数高效微调技术。*arXiv*，2023 年 8 月。doi: 10.48550/arXiv.2308.10462。'
- en: 'Xiao et al. (2023) Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang
    Guo, and Xianglong Liu. RobustMQ: Benchmarking Robustness of Quantized Models.
    *arXiv*, August 2023. doi: 10.48550/arXiv.2308.02350.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Yisong Xiao, Aishan Liu, Tianyuan Zhang, Haotong Qin, Jinyang
    Guo, 和 Xianglong Liu. RobustMQ：量化模型鲁棒性的基准测试。*arXiv*，2023 年 8 月。doi: 10.48550/arXiv.2308.02350。'
- en: 'Zhou et al. (2024) Andy Zhou, Bo Li, and Haohan Wang. Robust Prompt Optimization
    for Defending Language Models Against Jailbreaking Attacks. *arXiv*, January 2024.
    doi: 10.48550/arXiv.2401.17263.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2024) Andy Zhou, Bo Li, 和 Haohan Wang. 针对语言模型的安全漏洞优化的稳健提示。*arXiv*，2024
    年 1 月。doi: 10.48550/arXiv.2401.17263。'
- en: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. AutoDAN: Interpretable Gradient-Based
    Adversarial Attacks on Large Language Models. *arXiv*, October 2023. doi: 10.48550/arXiv.2310.15140.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, 和 Tong Sun. AutoDAN：基于梯度的可解释对抗攻击对大型语言模型的影响。*arXiv*，2023
    年 10 月。doi: 10.48550/arXiv.2310.15140。'
- en: 'Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico
    Kolter, and Matt Fredrikson. Universal and Transferable Adversarial Attacks on
    Aligned Language Models. *arXiv*, July 2023. doi: 10.48550/arXiv.2307.15043.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zou et al. (2023) Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico
    Kolter, 和 Matt Fredrikson. 通用且可转移的对齐语言模型对抗攻击。*arXiv*，2023 年 7 月。doi: 10.48550/arXiv.2307.15043。'
- en: Appendix A Appendix
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Experiment Utils
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实验工具
- en: We utilize various platforms for our target model, including Anyscale’s endpoint,
    OpenAI’s API, and our local system, Azure’s NC12sv3, equipped with a 32GB V100
    GPU, along with Hugging Face, to conduct inference tasks effectively. We import
    models from Hugging Face to operate on our local system.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用各种平台来进行目标模型的测试，包括 Anyscale 的端点、OpenAI 的 API 和我们的本地系统，Azure 的 NC12sv3，配备了
    32GB 的 V100 GPU，以及 Hugging Face，以有效地进行推理任务。我们从 Hugging Face 导入模型以在本地系统上运行。
- en: 'Table 4: Model Details'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：模型详情
- en: '| Name | Model | Source |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 模型 | 来源 |'
- en: '| --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CodeLlama34B | codellama/CodeLlama-34b-Instruct-hf | Anyscale |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama34B | codellama/CodeLlama-34b-Instruct-hf | Anyscale |'
- en: '| Mixtral8x7B | mistralai/Mixtral-8x7B-Instruct-v0.1 | Anyscale |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral8x7B | mistralai/Mixtral-8x7B-Instruct-v0.1 | Anyscale |'
- en: '| SQLCoder | defog/sqlcoder-7b-2 | HuggingFace |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SQLCoder | defog/sqlcoder-7b-2 | HuggingFace |'
- en: '| Llama2 | meta-llama/Llama-2-7b-chat-hf | HuggingFace |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Llama2 | meta-llama/Llama-2-7b-chat-hf | HuggingFace |'
- en: '| NeuralChat | Intel/neural-chat-7b-v3-3 | HuggingFace |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| NeuralChat | Intel/neural-chat-7b-v3-3 | HuggingFace |'
- en: '| Mistral7B | mistralai/Mistral-7B-v0.1-v0.1 | HuggingFace |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Mistral7B | mistralai/Mistral-7B-v0.1-v0.1 | HuggingFace |'
- en: '| CodeLlama7B | codellama/CodeLlama-7b-hf | HuggingFace |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama7B | codellama/CodeLlama-7b-hf | HuggingFace |'
- en: '| CodeLlama-7B-GGUF | TheBloke/CodeLlama-7B-GGUF | HuggingFace |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-7B-GGUF | TheBloke/CodeLlama-7B-GGUF | HuggingFace |'
- en: '| Llama2-7B-GGUF | TheBloke/Llama-2-7B-Chat-GGUF | HuggingFace |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B-GGUF | TheBloke/Llama-2-7B-Chat-GGUF | HuggingFace |'
- en: '| Dolphin-Mistral | cognitivecomputations/dolphin-2.2.1-Mistral-7B-v0.1 | HuggingFace
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Dolphin-Mistral | cognitivecomputations/dolphin-2.2.1-Mistral-7B-v0.1 | HuggingFace
    |'
- en: '| MPT7B | mosaicml/mpt-7b | HuggingFace |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MPT7B | mosaicml/mpt-7b | HuggingFace |'
- en: '| Phi2 | microsoft/phi-2 | HuggingFace |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Phi2 | microsoft/phi-2 | HuggingFace |'
- en: '| GPT-3.5-turbo | GPT-3.5-turbo-0125 | OpenAI |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | GPT-3.5-turbo-0125 | OpenAI |'
- en: '| GPT-4-turbo | GPT-4-turbo-0125 | OpenAI |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-turbo | GPT-4-turbo-0125 | OpenAI |'
- en: A.2 Experiment Results in details
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验结果详细信息
- en: In our experimentation, we explore various foundational models, including the
    latest iterations from OpenAI’s GPT series, as well as models derived from previous
    fine-tuned versions. We conduct tests on these models both with and without the
    integration of guardrails. Additionally, we examine models that have been quantized,
    further expanding the scope of our investigation. This comprehensive approach
    allows us to assess the performance and effectiveness of guardrails across a range
    of model architectures and configurations. By analyzing these diverse scenarios,
    we aim to gain insights into the impact of guardrails on model stability and security,
    contributing to the advancement of responsible AI deployment practices. Figure
    [3](#A1.F3 "Figure 3 ‣ A.2 Experiment Results in details ‣ Appendix A Appendix
    ‣ Increased LLM Vulnerabilities from Fine-tuning and Quantization") showcases
    the impact of Guardrails.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们探索了各种基础模型，包括 OpenAI GPT 系列的最新版本，以及从之前微调版本衍生的模型。我们对这些模型进行了带有和不带有保护措施的测试。此外，我们还检查了已量化的模型，进一步扩展了我们的调查范围。这种全面的方法使我们能够评估在不同模型架构和配置中保护措施的性能和有效性。通过分析这些不同的情境，我们旨在深入了解保护措施对模型稳定性和安全性的影响，推动负责任的人工智能部署实践的发展。图
    [3](#A1.F3 "图 3 ‣ A.2 实验结果详细说明 ‣ 附录 A 附录 ‣ 微调和量化导致的 LLM 易受攻击性增加") 展示了保护措施的影响。
- en: '![Refer to caption](img/021a93a79c99ad5cd4606a989900ba4c.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/021a93a79c99ad5cd4606a989900ba4c.png)'
- en: 'Figure 3: Jailbreak'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 越狱'
- en: We monitor the number of queries needed to jailbreak the model. Figure [4](#A1.F4
    "Figure 4 ‣ A.2 Experiment Results in details ‣ Appendix A Appendix ‣ Increased
    LLM Vulnerabilities from Fine-tuning and Quantization") examines the sustainability
    of Guardrails in resisting jailbreak attempts (the data includes only instances
    when the models were jailbroken). It’s quite evident that having guardrails does
    offer additional resistance to jailbreak attempts, even if the model has been
    compromised.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们监控了破解模型所需的查询次数。图 [4](#A1.F4 "图 4 ‣ A.2 实验结果详细说明 ‣ 附录 A 附录 ‣ 微调和量化导致的 LLM 易受攻击性增加")
    检查了保护措施在抵御破解尝试方面的可持续性（数据仅包括模型被破解的情况）。显然，即使模型已经被攻破，拥有保护措施仍能提供额外的抗破解阻力。
- en: '![Refer to caption](img/3d2b15e0f8bd1ec4f55137008a312961.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d2b15e0f8bd1ec4f55137008a312961.png)'
- en: 'Figure 4: Queries to Jailbreak'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 越狱查询'
