- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QServe: W4A8KV4 量化与系统协同设计以提高 LLM 服务效率'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.04532](https://ar5iv.labs.arxiv.org/html/2405.04532)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.04532](https://ar5iv.labs.arxiv.org/html/2405.04532)
- en: Yujun Lin^(*,1), Haotian Tang^(*,1), Shang Yang^(*,1), Zhekai Zhang¹, Guangxuan
    Xiao¹, Chuang Gan^(3,4), Song Han^(1,2)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yujun Lin^(*,1)，Haotian Tang^(*,1)，Shang Yang^(*,1)，Zhekai Zhang¹，Guangxuan
    Xiao¹，Chuang Gan^(3,4)，Song Han^(1,2)
- en: MIT¹, NVIDIA², UMass Amherst³, MIT-IBM Watson AI Lab⁴
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: MIT¹，NVIDIA²，UMass Amherst³，MIT-IBM Watson AI Lab⁴
- en: '{yujunlin,kentang,shangy,songhan}@mit.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{yujunlin,kentang,shangy,songhan}@mit.edu'
- en: '[https://hanlab.mit.edu/projects/qserve](https://hanlab.mit.edu/projects/qserve)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://hanlab.mit.edu/projects/qserve](https://hanlab.mit.edu/projects/qserve)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Quantization can accelerate large language model (LLM) inference. Going beyond
    INT8 quantization, the research community is actively exploring even lower precision,
    such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only
    accelerate low-batch, edge LLM inference, failing to deliver performance gains
    in large-batch, cloud-based LLM serving. We uncover a critical issue: existing
    INT4 quantization methods suffer from significant runtime overhead (20-90%) when
    dequantizing either weights or partial sums on GPUs. To address this challenge,
    we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation,
    and 4-bit KV cache. QoQ stands for quattuor-octō-quattuor, which represents 4-8-4
    in Latin. QoQ is implemented by the QServe inference library that achieves measured
    speedup. The key insight driving QServe is that the efficiency of LLM serving
    on GPUs is critically influenced by operations on low-throughput CUDA cores. Building
    upon this insight, in QoQ algorithm, we introduce progressive quantization that
    can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention
    to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization.
    In the QServe system, we perform compute-aware weight reordering and take advantage
    of register-level parallelism to reduce dequantization latency. We also make fused
    attention memory-bound, harnessing the performance gain brought by KV4 quantization.
    As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B
    by 1.2$\times$. Code is released at [https://github.com/mit-han-lab/qserve](https://github.com/mit-han-lab/qserve).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以加速大语言模型（LLM）推理。超越 INT8 量化，研究界正积极探索更低的精度，例如 INT4。然而，最先进的 INT4 量化技术仅加速低批量、边缘
    LLM 推理，未能在大批量、基于云的 LLM 服务中带来性能提升。我们揭示了一个关键问题：现有的 INT4 量化方法在对 GPU 上的权重或部分和进行解量化时，存在显著的运行时开销（20-90%）。为了解决这一挑战，我们提出了
    QoQ，一种 W4A8KV4 量化算法，具有 4 位权重、8 位激活和 4 位 KV 缓存。QoQ 代表 quattuor-octō-quattuor，这在拉丁语中表示
    4-8-4。QoQ 由 QServe 推理库实现，并实现了测量的加速。驱动 QServe 的关键见解是 LLM 在 GPU 上的服务效率受到低吞吐量 CUDA
    核心操作的关键影响。基于这一见解，在 QoQ 算法中，我们引入了渐进量化，允许在 W4A8 GEMM 中实现低解量化开销。此外，我们开发了 SmoothAttention，有效减轻了
    4 位 KV 量化引起的精度下降。在 QServe 系统中，我们进行计算感知的权重重排序，并利用寄存器级并行性来减少解量化延迟。我们还使融合注意力成为内存绑定，利用
    KV4 量化带来的性能提升。因此，QServe 将 Llama-3-8B 的最大服务吞吐量提高了 1.2$\times$。代码发布于 [https://github.com/mit-han-lab/qserve](https://github.com/mit-han-lab/qserve)。
- en: '^†^†footnotetext: ^*: The first three authors contribute equally to this project
    and are listed in the alphabetical order. Yujun Lin leads the quantization algorithm,
    Haotian Tang and Shang Yang lead the GPU kernels and the serving system.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：^*：前三位作者对本项目贡献相同，按字母顺序排列。Yujun Lin 负责量化算法，Haotian Tang 和 Shang Yang 负责
    GPU 内核和服务系统。
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: '![Refer to caption](img/ae45291a8265d6c7281cc9289ade97b3.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae45291a8265d6c7281cc9289ade97b3.png)'
- en: 'Figure 1: QServe achieves higher throughput when running Llama models on L40S
    compared with TensorRT-LLM on A100, effectively saves the dollar cost for LLM
    serving by 3$\times$ through system-algorithm codesign. See Table [IV](#S6.T4
    "TABLE IV ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving") for absolute
    throughput numbers and precision choices in TensorRT-LLM.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：与 A100 上的 TensorRT-LLM 相比，QServe 在运行 Llama 模型时在 L40S 上实现了更高的吞吐量，通过系统-算法协同设计有效节省了
    3$\times$ 的 LLM 服务成本。有关 TensorRT-LLM 中绝对吞吐量数字和精度选择的信息，请参见表 [IV](#S6.T4 "TABLE
    IV ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving")。'
- en: Large language models (LLMs) have demonstrated remarkable capability across
    a broad spectrum of tasks, exerting a profound influence on our daily lives.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在广泛的任务中展示了卓越的能力，对我们的日常生活产生了深远的影响。
- en: 'However, the colossal size of LLMs makes their deployment extremely challenging,
    necessitating the adoption of quantization techniques for efficient inference.
    State-of-the-art integer quantization algorithms can be divided into three categories:
    8-bit weight and 8-bit activation (W8A8), 4-bit weight and 16-bit activation (W4A16),
    4-bit weight 4-bit activation (W4A4) quantization. The former two methods are
    considered nearly lossless in terms of accuracy. In contrast, W4A4 quantization
    introduces a notable accuracy degradation, although it is anticipated to offer
    superior throughput in return by mapping its computations onto high-throughput
    4-bit tensor cores. Unfortunately, this anticipated performance boost has not
    been consistently observed across current GPU platforms. For instance, the state-of-the-art
    W4A4 serving system, Atom [[44](#bib.bib44)], exhibits 20-25% lower performance
    than its W4A16 and W8A8 counterpart in TensorRT-LLM when running the Llama-2-7B [[34](#bib.bib34)]
    model on A100 GPUs. That said, the research community has yet to find a precision
    combination superior to W4A16 and W8A8 for efficient cloud LLM serving.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs的庞大规模使得其部署极具挑战性，需要采用量化技术以实现高效推理。最先进的整数量化算法可分为三类：8-bit权重和8-bit激活（W8A8），4-bit权重和16-bit激活（W4A16），4-bit权重和4-bit激活（W4A4）量化。前两种方法在准确性方面被认为几乎是无损的。相比之下，W4A4量化引入了显著的准确性降级，尽管预计通过将计算映射到高吞吐量的4-bit张量核心上，可以提供更高的吞吐量。不幸的是，这种预期的性能提升在当前GPU平台上并没有被一致地观察到。例如，最先进的W4A4服务系统Atom [[44](#bib.bib44)]，在A100
    GPU上运行Llama-2-7B [[34](#bib.bib34)]模型时，其性能比W4A16和W8A8的TensorRT-LLM模型低20-25%。也就是说，研究界尚未找到比W4A16和W8A8更优的精度组合来实现高效的云LLM服务。
- en: 'In this paper, we reveal a critical observation: current 4-bit integer quantization
    methods experience significant overhead, ranging from 20% to 90%, during the dequantization
    of weights or partial sums on current-generation GPUs. For example, W4A16 quantization
    performs computation on FP16 tensor cores while the weights are in INT4, so weight
    dequantization is required in the GEMM kernel. On the other hand, for W4A4 quantization,
    to achieve reasonable accuracy, W4A4 methods must apply per-group quantization
    to both weights and activation, sharing FP16 scaling factors on a sub-channel
    basis. For example, the state-of-the-art W4A4 quantization method, QuaRot [[2](#bib.bib2)],
    reports a significant 0.2 perplexity degradation after switching from per-group
    quantization to per-channel quantization. This per-group quantization design requires
    an integer to floating-point dequantization for partial sums (since INT4 tensor
    cores produce INT32 partial sums), which operates on the slower CUDA cores within
    the sequential main loop of W4A4 GEMM. On data center GPUs like A100, a CUDA core
    operation is as expensive as 50 INT4 tensor core operations.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们揭示了一个关键观察：当前的4-bit整数量化方法在现有一代GPU上在权重或部分和的反量化过程中经历了显著的开销，范围从20%到90%。例如，W4A16量化在FP16张量核心上进行计算，而权重为INT4，因此在GEMM内核中需要进行权重反量化。另一方面，对于W4A4量化，为了实现合理的准确性，W4A4方法必须对权重和激活分别进行组量化，并在子通道基础上共享FP16缩放因子。例如，最先进的W4A4量化方法QuaRot [[2](#bib.bib2)]，报告了从每组量化切换到每通道量化后，困惑度显著降低0.2。这个每组量化设计要求在部分和的整数到浮点反量化（因为INT4张量核心生成INT32部分和），这在W4A4
    GEMM的顺序主循环中的较慢CUDA核心上操作。在像A100这样的数据中心GPU上，一个CUDA核心操作的成本相当于50次INT4张量核心操作。
- en: 'Therefore, reducing overhead on CUDA cores is crucial for achieving optimal
    throughput in LLM serving. Guided by this principle, we introduce QoQ (Quattuor-Octō-Quattuor,
    or 4-8-4 in Latin) algorithm which quantizes LLMs to W4A8KV4 precision: 4-bit
    weights, 8-bit activations and 4-bit KV caches. Additionally, we present QServe,
    which provides efficient system support for W4A8KV4 quantization.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，减少CUDA核心的开销对于在LLM服务中实现最佳吞吐量至关重要。基于这一原则，我们引入了QoQ（Quattuor-Octō-Quattuor，即拉丁文中的4-8-4）算法，该算法将LLMs量化为W4A8KV4精度：4-bit权重，8-bit激活和4-bit
    KV缓存。此外，我们还推出了QServe，为W4A8KV4量化提供了高效的系统支持。
- en: In the QoQ algorithm, we introduce progressive group quantization. This method
    first quantizes weights to 8 bits using per-channel FP16 scales, then quantizes
    these 8-bit intermediates to 4 bits. This approach ensures that all GEMMs are
    performed on INT8 tensor cores. Additionally, we mitigate accuracy loss from KV4
    quantization through SmoothAttention, which shifts the challenge of activation
    quantization from keys to queries, the latter of which are not quantized.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在QoQ算法中，我们引入了渐进分组量化。这种方法首先使用每通道FP16缩放将权重量化为8位，然后将这些8位中间结果量化为4位。这种方法确保所有GEMMs都在INT8张量核心上执行。此外，我们通过SmoothAttention减轻了KV4量化带来的精度损失，这将激活量化的挑战从键转移到查询，后者没有量化。
- en: In the QServe system, the protective range in progressive group quantization
    enables full register-level parallelism during INT4 to INT8 dequantization, using
    a subtraction after multiplication computation order. Furthermore, we propose
    compute-aware weight reordering to minimize pointer arithmetic overhead on CUDA
    cores during W4A8 GEMM operations. Additionally, we delay the turning point of
    the CUDA core roofline and decrease the computational intensity of KV4 attention
    at the same time. This ensures that the attention operator remains within the
    memory-bound region, where low-bit quantization can effectively enhance throughput.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在QServe系统中，渐进分组量化中的保护范围在INT4到INT8去量化期间实现了完全的寄存器级并行性，使用了乘法后的减法计算顺序。此外，我们提出了计算感知的权重重新排序，以最小化CUDA核心在W4A8
    GEMM操作中的指针算术开销。此外，我们延迟了CUDA核心的拐点，同时降低了KV4注意力的计算强度。这确保了注意力操作保持在内存绑定区域，在该区域，低位量化可以有效提高吞吐量。
- en: We evaluate seven widely-used LLMs using QServe on A100 and L40S GPUs, and compare
    their maximum achievable throughput against state-of-the-art systems, including
    TensorRT-LLM (in FP16, W8A8, and W4A16 configurations), Atom [[44](#bib.bib44)]
    (in W4A4), and QuaRot [[2](#bib.bib2)] (in W4A4). On A100 GPUs, QServe achieves
    1.2-2.4$\times$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用QServe在A100和L40S GPU上评估了七种广泛使用的LLM，并将它们的最大可实现吞吐量与最新系统进行比较，包括TensorRT-LLM（在FP16、W8A8和W4A16配置中）、Atom
    [[44](#bib.bib44)]（在W4A4中）和QuaRot [[2](#bib.bib2)]（在W4A4中）。在A100 GPU上，QServe达到了1.2-2.4倍的提升。
- en: II Background
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A Large Language Models
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 大型语言模型
- en: Large Language Models (LLMs) are a family of causal transformer models with
    multiple identically-structured layers. Each layer combines an attention block,
    a feed-forward network (FFN) and normalization layers. The input of each layer,
    $\mathbf{x}$ for each request).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是一类具有多个结构相同层的因果变换器模型。每层结合了一个注意力块、一个前馈网络（FFN）和归一化层。每层的输入是$\mathbf{x}$（每个请求）。
- en: 'In attention blocks, $\mathbf{x}$ and compute attention using:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力块中，$\mathbf{x}$并使用以下公式计算注意力：
- en: '|  | $\small\mathbf{o}_{h}=\text{softmax}\left(\frac{\mathbf{q}_{h}\mathbf{K}_{h_{KV}}^{T}}{\sqrt{D}}\right)\mathbf{V}_{h_{KV}},\quad
    h_{KV}=\left\lfloor\frac{h}{r}\right\rfloor.$ |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{o}_{h}=\text{softmax}\left(\frac{\mathbf{q}_{h}\mathbf{K}_{h_{KV}}^{T}}{\sqrt{D}}\right)\mathbf{V}_{h_{KV}},\quad
    h_{KV}=\left\lfloor\frac{h}{r}\right\rfloor.$ |  | (1) |'
- en: The result $\mathbf{o}$ as the input of FFN. The FFN is composed of linear projection
    and activation layers and it does not mix features between tokens.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 结果$\mathbf{o}$作为FFN的输入。FFN由线性投影和激活层组成，并且它不在令牌之间混合特征。
- en: II-B Integer Quantization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 整数量化
- en: 'Integer quantization maps high-precision numbers to discrete levels. The process
    can be formulated as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 整数量化将高精度数字映射到离散级别。这个过程可以表示为：
- en: '|  | $\small\mathbf{Q}_{\mathbf{X}}=\left\lceil\frac{\mathbf{X}}{s}+z\right\rfloor,s=\frac{\mathbf{X}_{\max}-\mathbf{X}_{\min}}{q_{\max}-q_{\min}},z=\left\lceil
    q_{\min}-\frac{\mathbf{X}_{\min}}{s}\right\rfloor,$ |  | (2) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{Q}_{\mathbf{X}}=\left\lceil\frac{\mathbf{X}}{s}+z\right\rfloor,s=\frac{\mathbf{X}_{\max}-\mathbf{X}_{\min}}{q_{\max}-q_{\min}},z=\left\lceil
    q_{\min}-\frac{\mathbf{X}_{\min}}{s}\right\rfloor,$ |  | (2) |'
- en: where $\mathbf{X}$ is the zero point. Thus, the dequantized tensor can be represented
    as,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{X}$是零点。因此，去量化的张量可以表示为，
- en: '|  | $\small\hat{\mathbf{X}}=Q\left(\mathbf{X}\right)=\left(\mathbf{Q}_{\mathbf{X}}-z\right)\cdot
    s$ |  | (3) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\hat{\mathbf{X}}=Q\left(\mathbf{X}\right)=\left(\mathbf{Q}_{\mathbf{X}}-z\right)\cdot
    s$ |  | (3) |'
- en: This is known as asymmetric quantization, where $\mathbf{X}_{\max}=\max\left(\mathbf{X}\right),\mathbf{X}_{\min}=\min\left(\mathbf{X}\right)$
    .
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为非对称量化，其中$\mathbf{X}_{\max}=\max\left(\mathbf{X}\right),\mathbf{X}_{\min}=\min\left(\mathbf{X}\right)$。
- en: In this paper, we denote $x$ is the group size.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们用$x$表示组大小。
- en: III Motivation
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 动机
- en: 'Weight and KV cache quantization (e.g. W4, KV4) can reduce the memory footprint
    in LLM serving. Quantizing both weight and activation (e.g. W8A8) can also improve
    the peak computation throughput. Choosing the right precision for LLM deployment
    is a difficult task. Existing solutions can be divided into three categories:
    W4A16 (per-group), W8A8 (per-channel weight + per-token activation), W4A4 (per-group).
    We will demonstrate in this section why W4A8KV4 is a superior choice.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和 KV 缓存量化（例如 W4，KV4）可以减少 LLM 服务中的内存占用。量化权重和激活（例如 W8A8）也可以提高峰值计算吞吐量。选择适当的精度进行
    LLM 部署是一项困难的任务。现有解决方案可以分为三类：W4A16（每组）、W8A8（每通道权重 + 每标记激活）、W4A4（每组）。我们将在本节中展示为什么
    W4A8KV4 是一个更优的选择。
- en: III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A W4A8KV4 的屋脊线优于 W8A8、W4A16
- en: '![Refer to caption](img/98c6073611099f97f58bca8f0e07e2ae.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98c6073611099f97f58bca8f0e07e2ae.png)'
- en: 'Figure 2: Left: Both attention and GEMM are crucial for end-to-end LLM latency.
    Right: Despite 2$\times$ higher theoretical peak performance, W4A4 systems significantly
    lag behind TRT-LLM-W8A8 in efficiency.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：左：注意力和 GEMM 对于端到端 LLM 延迟都至关重要。右：尽管理论峰值性能高出 2$\times$，W4A4 系统在效率上显著滞后于 TRT-LLM-W8A8。
- en: '![Refer to caption](img/ac98c04b509b8c2e0871f817b7bd7856.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac98c04b509b8c2e0871f817b7bd7856.png)'
- en: 'Figure 3: A100 roofline for LLM serving: for GEMM layers, the W4A8 roofline
    dominates both W4A16 and W8A8 across different batch sizes; for attention layers,
    4-bit quantization improves theoretical peak performance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：A100 的 LLM 服务屋脊线：对于 GEMM 层，W4A8 的屋脊线在不同批次大小下优于 W4A16 和 W8A8；对于注意力层，4 位量化提高了理论峰值性能。
- en: 'We begin our exploration through roofline analysis. As in Figure [2](#S3.F2
    "Figure 2 ‣ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a,
    when considering real-world conversations with 1024 input tokens and 512 output
    tokens, attention and GEMM account for most of the runtime when deploying LLMs.
    Furthermore, the runtime of the decoding stage is approximately 6$\times$ that
    of the prefilling stage. Therefore, we focus our analysis on the attention and
    GEMM within the decoding stage.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过屋脊线分析开始我们的探索。如图 [2](#S3.F2 "Figure 2 ‣ III-A W4A8KV4 Has Superior Roofline
    Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design
    for Efficient LLM Serving")a 所示，在考虑到具有 1024 输入标记和 512 输出标记的现实对话时，注意力和 GEMM 占据了
    LLM 部署时的大部分运行时间。此外，解码阶段的运行时间大约是预填充阶段的 6$\times$。因此，我们将分析重点放在解码阶段的注意力和 GEMM 上。'
- en: 'For an $m\times n\times k$ is large, the problem is compute bound. Thus, W8A8
    has faster speed thanks to the higher throughput from INT8 tensor cores. Intuitively,
    one can expect W4A8 to combine the best of both worlds across all batch sizes.
    This is clearly demonstrated in Figure [3](#S3.F3 "Figure 3 ‣ III-A W4A8KV4 Has
    Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving"), as long as we can perform all
    computation on INT8 tensor cores.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 $m\times n\times k$ 较大的情况，问题是计算受限的。因此，由于 INT8 张量核心的更高吞吐量，W8A8 具有更快的速度。直观上，可以预期
    W4A8 能在所有批次大小中兼顾两者的优点。这在图 [3](#S3.F3 "Figure 3 ‣ III-A W4A8KV4 Has Superior Roofline
    Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design
    for Efficient LLM Serving") 中得到了清晰的展示，只要我们可以在 INT8 张量核心上执行所有计算。'
- en: 'Why KV4: attention workloads in LLM decoding can be formulated as a sequence
    of batched GEMV operations, with a computation intensity of 1 MAC / element regardless
    of input batch sizes. As in Equation [1](#S2.E1 "In II-A Large Language Models
    ‣ II Background ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"), the memory traffic is dominated by KV cache access, since $S\gg
    N=1$ peak performance for attention over KV8. This improvement offers decent end-to-end
    speedup opportunities, since attention accounts for more than 50% of total runtime
    at batch=64 in Figure [2](#S3.F2 "Figure 2 ‣ III-A W4A8KV4 Has Superior Roofline
    Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design
    for Efficient LLM Serving")a.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么 KV4：LLM 解码中的注意力工作负载可以被表述为一系列批处理的 GEMV 操作，其计算强度为每个元素 1 MAC，无论输入批量大小如何。如公式
    [1](#S2.E1 "In II-A Large Language Models ‣ II Background ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving") 所示，内存流量主要由 KV 缓存访问主导，因为 $S\gg
    N=1$ 对 KV8 的注意力峰值性能。这种改进提供了相当不错的端到端加速机会，因为注意力在图 [2](#S3.F2 "Figure 2 ‣ III-A W4A8KV4
    Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving")a 中占总运行时间的 50% 以上。'
- en: 'III-B Why Not W4A4KV4: Main Loop Overhead in GEMM'
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 为什么不选择 W4A4KV4：GEMM 中的主循环开销
- en: '![Refer to caption](img/1d68fbc38c710616146ed38761710579.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1d68fbc38c710616146ed38761710579.png)'
- en: 'Figure 4: Illustration of $m\times n\times k$ are large. Thus, the main loop
    is long.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：$m\times n\times k$ 较大。因此，主循环很长。
- en: '![Refer to caption](img/6519f7c3b4ac8a6e0fbccb14fd4fc497.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6519f7c3b4ac8a6e0fbccb14fd4fc497.png)'
- en: 'Figure 5: Quantized GEMM on GPUs:  W8A8 is fast because its main loop only
    contains tensor core operations and all dequantization operations are present
    in the epilogue. Atom-W4A4 and TensorRT-LLM-W4A16 suffer from significant partial
    sum or weight dequantization overhead in the main loop. Thanks to the two-level
    progressive quantiation algorithm, QServe-W4A8 reduces main loop dequantization
    overhead by introducing register-level parallelism.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：GPU 上的量化 GEMM：W8A8 运行速度较快，因为其主循环仅包含张量核心操作，所有去量化操作都在尾部。Atom-W4A4 和 TensorRT-LLM-W4A16
    在主循环中遭遇显著的部分和或权重去量化开销。得益于两级渐进量化算法，QServe-W4A8 通过引入寄存器级并行性减少了主循环中的去量化开销。
- en: 'A natural follow-up question would be: “Why do we not choose the even more
    aggressive W4A4?” W4A4 starts to achieve better theoretical GEMM performance when
    $m$, the number of input sequences, exceeds 78, as 4-bit tensor cores are twice
    as performant compared to their 8-bit counterparts. However, apart from the significant
    accuracy degradation, which will be discussed in Section [VI](#S6 "VI Evaluation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"),
    we demonstrate that such theoretical performance gains cannot be realized on existing
    GPU architectures (Ampere and Hopper). As in Figure [2](#S3.F2 "Figure 2 ‣ III-A
    W4A8KV4 Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving")b, existing W4A4
    serving systems Atom [[44](#bib.bib44)] and QuaRot [[2](#bib.bib2)] are even significantly
    slower than the W16A16 solution from TensorRT-LLM.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '一个自然的后续问题是：“为什么我们不选择更激进的 W4A4？”当输入序列的数量 $m$ 超过 78 时，W4A4 开始实现更好的理论 GEMM 性能，因为
    4 位张量核心的性能是 8 位核心的两倍。然而，除了在第 [VI](#S6 "VI 评估 ‣ QServe: W4A8KV4 量化和系统协同设计以提高 LLM
    服务效率") 节中讨论的显著准确性下降外，我们还展示了这种理论性能提升在现有的 GPU 架构（Ampere 和 Hopper）上无法实现。如图 [2](#S3.F2
    "图 2 ‣ III-A W4A8KV4 相比 W8A8、W4A16 的 Roofline 优势 ‣ III 动机 ‣ QServe: W4A8KV4 量化和系统协同设计以提高
    LLM 服务效率")b 所示，现有的 W4A4 服务系统 Atom [[44](#bib.bib44)] 和 QuaRot [[2](#bib.bib2)]
    甚至比 TensorRT-LLM 的 W16A16 解决方案慢得多。'
- en: 'While this performance gap can be partially explained by the inefficient runtime
    in these two systems, the inherent difficulty in mapping per-group quantized W4A4
    GEMM on GPUs has been overlooked in previous literature. State-of-the-art systems
    implement tensor core GEMM with an output stationary dataflow shown in Figure
    [4](#S3.F4 "Figure 4 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III
    Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM
    Serving"). For an $m\times n\times k$. This sequential loop is referred to as
    the main loop. The main loop comprises more than 100 iterations and dominates
    the runtime of the GEMM kernel. In both FP16 and W8A8 GEMM (Figure [5](#S3.F5
    "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III Motivation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")a),
    the main loop is executed entirely on tensor cores. TensorRT-LLM-W4A16 (Figure
    [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III
    Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM
    Serving")b) and Atom-W4A4 (Figure [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4:
    Main Loop Overhead in GEMM ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and
    System Co-design for Efficient LLM Serving")c) both require dequantization operations
    in the main loop, which is running on the CUDA cores. W4A16 requires INT4 to FP16
    weight conversion, while Atom-W4A4 requires INT32 to FP32 partial sum conversion
    and accumulation.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这性能差距可以部分通过这两个系统中低效的运行时间来解释，但先前文献中忽略了在 GPU 上映射每组量化的 W4A4 GEMM 的固有难度。最先进的系统实现了具有输出固定数据流的张量核心
    GEMM，如图 [4](#S3.F4 "图 4 ‣ III-B 为什么不使用 W4A4KV4：GEMM 中的主循环开销 ‣ III 动机 ‣ QServe：W4A8KV4
    量化和系统协同设计以高效服务 LLM") 所示。对于一个 $m\times n\times k$。这个顺序循环被称为主循环。主循环包括了 100 多次迭代，并主导了
    GEMM 内核的运行时间。在 FP16 和 W8A8 GEMM（图 [5](#S3.F5 "图 5 ‣ III-B 为什么不使用 W4A4KV4：GEMM
    中的主循环开销 ‣ III 动机 ‣ QServe：W4A8KV4 量化和系统协同设计以高效服务 LLM")a) 中，主循环完全在张量核心上执行。TensorRT-LLM-W4A16（图
    [5](#S3.F5 "图 5 ‣ III-B 为什么不使用 W4A4KV4：GEMM 中的主循环开销 ‣ III 动机 ‣ QServe：W4A8KV4
    量化和系统协同设计以高效服务 LLM")b）和 Atom-W4A4（图 [5](#S3.F5 "图 5 ‣ III-B 为什么不使用 W4A4KV4：GEMM
    中的主循环开销 ‣ III 动机 ‣ QServe：W4A8KV4 量化和系统协同设计以高效服务 LLM")c）都需要在主循环中进行去量化操作，而这在 CUDA
    核心上运行。W4A16 需要将 INT4 转换为 FP16 权重，而 Atom-W4A4 则需要将 INT32 转换为 FP32 部分和总和。
- en: The dequantization process in Atom’s main loop leads to two substantial efficiency
    bottlenecks. Firstly, on modern data center GPUs like the A100 and H100, the peak
    performance of FP32 CUDA cores is merely 2% of their INT4 tensor core counterparts.
    That said, de-quantizing one single partial sum in Atom is equivalent to 50 tensor
    core MACs. Therefore, the main loop is dominated by slow CUDA core operations
    rather than fast tensor core operations. Secondly, Atom creates two sets of registers
    (one for FP32 and one for INT32) to hold partial sums. Larger GEMM problems (e.g.,
    prefilling stage) are typically register-bound on GPUs due to the nature of the
    output stationary dataflow, which results in high register consumption for storing
    partial sums. Consuming a large number of registers within each warp limits the
    number of warps that can be executed simultaneously on the streaming multiprocessor.
    It is important to note that GPUs rely on low-cost context switching between a
    large number of in-flight warps to hide latency. Consequently, a smaller number
    of concurrently executed warps limits the opportunity for latency hiding, further
    exacerbating the main loop overhead.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Atom 主循环中的去量化过程导致了两个显著的效率瓶颈。首先，在现代数据中心 GPU（如 A100 和 H100）上，FP32 CUDA 核心的峰值性能仅为
    INT4 张量核心的 2%。也就是说，在 Atom 中去量化一个单一的部分和等同于 50 个张量核心 MAC 操作。因此，主循环受限于慢速 CUDA 核心操作，而非快速张量核心操作。其次，Atom
    创建了两组寄存器（一个用于 FP32，一个用于 INT32）来存储部分和。由于输出固定数据流的特性，较大的 GEMM 问题（例如，预填充阶段）通常会在 GPU
    上受到寄存器限制，这导致存储部分和时的寄存器消耗较高。在每个 warp 中消耗大量寄存器限制了能够同时在流式多处理器上执行的 warp 数量。值得注意的是，GPU
    依赖于在大量飞行中的 warp 之间进行低成本的上下文切换来隐藏延迟。因此，同时执行的 warp 数量较少限制了延迟隐藏的机会，进一步加剧了主循环开销。
- en: 'We preview our QServe’s W4A8 per-group quantized GEMM kernel design in Figure
    [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III
    Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM
    Serving")d. We employ a two-level progressive group quantization approach to ensure
    that all computations are performed on INT8 tensor cores. We opt for weight dequantization
    over partial sum dequantization due to its lower register pressure. Furthermore,
    we apply 4-way register-level parallelism to decode four INT4 weights simultaneously,
    further reducing the main loop overhead.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM
    ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")d 中预览了 QServe 的 W4A8 每组量化 GEMM 核心设计。我们采用了两级渐进式组量化方法，以确保所有计算在 INT8
    张量核心上进行。我们选择对权重进行去量化，而非部分和去量化，因为前者具有更低的寄存器压力。此外，我们还应用了 4 路寄存器级并行，以同时解码四个 INT4
    权重，进一步减少了主循环的开销。'
- en: IV QoQ Quantization
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV QoQ 量化
- en: To this end, we have discussed why W4A8KV4 is a superior quantization precision
    choice. Yet, preserving model accuracy with such low-bit quantization remains
    a significant challenge. To unleash the full potential of W4A8KV4 without compromising
    the efficacy of large language models, we propose QoQ algorithm featuring progressive
    group quantization, SmoothAttention, and various general quantization optimizations.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们讨论了为什么 W4A8KV4 是一种优越的量化精度选择。然而，使用如此低位的量化来保持模型准确性仍然是一个重大挑战。为了在不妨碍大语言模型效果的情况下充分发挥
    W4A8KV4 的潜力，我们提出了 QoQ 算法，特点是渐进式组量化、SmoothAttention 和各种通用量化优化。
- en: IV-A Progressive Group Quantization
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 渐进式组量化
- en: '![Refer to caption](img/6bb685e677687fff23b0b0dcd3e5f473.png)![Refer to caption](img/e16118a3111d6a449e796512ab12e4e4.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6bb685e677687fff23b0b0dcd3e5f473.png)![参见说明](img/e16118a3111d6a449e796512ab12e4e4.png)'
- en: 'Figure 6: Progressive Group Quantization first employs per-channel INT8 quantization
    with protective range [-119, 119], followed by per-group INT4 quantization, so
    that the dequantized intermediate values remain within the INT8 range for computation.
    Bottom: prior methods directly applies per-group INT4 quantization on weights,
    followed by per-channel INT8 quantization on scale factors. Thus the dequantized
    intermediate values may exceed the INT8 range, necessitating further dequantization
    to floating-point values for computation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：渐进式组量化首先应用每通道 INT8 量化，并设置保护范围为 [-119, 119]，随后进行每组 INT4 量化，以确保去量化的中间值仍在 INT8
    范围内进行计算。下方：之前的方法直接对权重应用每组 INT4 量化，然后对尺度因子应用每通道 INT8 量化。因此，去量化的中间值可能超出 INT8 范围，需要进一步去量化为浮点值以进行计算。
- en: 'To enhance the accuracy of low-bit quantization, group quantization is commonly
    utilized [[44](#bib.bib44), [23](#bib.bib23), [12](#bib.bib12)]. However, as outlined
    in Section [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in
    GEMM ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving"), the dequantization overhead in the system implementation
    can negate these accuracy improvements. To tackle this issue, we introduce progressive
    group quantization, as depicted in Figure [6](#S4.F6 "Figure 6 ‣ IV-A Progressive
    Group Quantization ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提高低位量化的精度，通常采用组量化技术[[44](#bib.bib44), [23](#bib.bib23), [12](#bib.bib12)]。然而，正如第[5](#S3.F5
    "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III Motivation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")节中所述，系统实现中的去量化开销可能会抵消这些精度提升。为了解决这个问题，我们引入了渐进式组量化，如图[6](#S4.F6
    "Figure 6 ‣ IV-A Progressive Group Quantization ‣ IV QoQ Quantization ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")所示。'
- en: 'Given the weight tensor $\mathbf{W}\in\mathbb{R}^{k\times n}$, we first apply
    per-channel symmetric INT8 quantization:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 给定权重张量 $\mathbf{W}\in\mathbb{R}^{k\times n}$，我们首先应用每通道对称的 INT8 量化：
- en: '|  | $\small\hat{\mathbf{W}}={\mathbf{Q}_{\mathbf{W}}}^{(0)}_{\mathrm{s8}}\cdot\mathbf{s}^{(0)}_{\mathrm{fp16}},$
    |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\hat{\mathbf{W}}={\mathbf{Q}_{\mathbf{W}}}^{(0)}_{\mathrm{s8}}\cdot\mathbf{s}^{(0)}_{\mathrm{fp16}},$
    |  | (4) |'
- en: 'where ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{s8}}^{(0)}\in\mathbb{N}^{n\times
    k}$ is the channel-wise quantization scales. We then further employ per-group
    asymmetric INT4 quantization on the intermediate weight tensor:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{s8}}^{(0)}\in\mathbb{N}^{n\times k}$
    是按通道的量化尺度。我们随后对中间权重张量进一步采用每组不对称 INT4 量化：
- en: '|  | $\small{{\mathbf{Q}}_{\mathbf{W}}}_{\mathrm{s8}}^{(0)}=\left({\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}-\mathbf{z}_{\mathrm{u4}}\right)\cdot\mathbf{s}^{(1)}_{\mathrm{u8}},$
    |  | (5) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small{{\mathbf{Q}}_{\mathbf{W}}}_{\mathrm{s8}}^{(0)}=\left({\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}-\mathbf{z}_{\mathrm{u4}}\right)\cdot\mathbf{s}^{(1)}_{\mathrm{u8}},$
    |  | (5) |'
- en: where ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}\in\mathbb{N}^{n\times k}$ is
    the unsigned 8-bit group-wise quantization scales.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}\in\mathbb{N}^{n\times k}$ 是无符号的
    8 位组量化缩放因子。
- en: 'For W4A8 GEMM computation, the 4-bit quantized weight tensor ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}$
    following Equation [5](#S4.E5 "In IV-A Progressive Group Quantization ‣ IV QoQ
    Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"), and then perform INT8 matrix multiplication as if it was W8A8 per-channel
    quantization.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 W4A8 GEMM 计算，遵循方程 [5](#S4.E5 "在 IV-A 逐步组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4
    量化与系统设计以实现高效 LLM 服务") 的 4 位量化权重张量 ${\mathbf{Q}_{\mathbf{W}}}_{\mathrm{u4}}$，然后进行
    INT8 矩阵乘法，就像它是 W8A8 每通道量化一样。'
- en: Protective Quantization Range
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保护量化范围
- en: 'naïvely applying Equation [4](#S4.E4 "In IV-A Progressive Group Quantization
    ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving") and [5](#S4.E5 "In IV-A Progressive Group Quantization
    ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving") does not guarantee that the intermediate dequantized weights
    perfectly lie in the 8-bit integer representation range. For example, after INT8
    quantization, a group of 8-bit weights lie in $[-113,120]$ which is beyond the
    max 8-bit integer 127\. One straightforward solution is to turn on the saturation
    option in the arithmetic instructions during dequantization. However, simply applying
    saturation will severely damage the computation throughput, reducing speed by
    as much as 67%.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '直接应用方程 [4](#S4.E4 "在 IV-A 逐步组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4 量化与系统设计以实现高效
    LLM 服务") 和 [5](#S4.E5 "在 IV-A 逐步组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4 量化与系统设计以实现高效
    LLM 服务") 并不能保证中间的去量化权重完美地落在 8 位整数表示范围内。例如，经过 INT8 量化后，一组 8 位权重在 $[-113,120]$ 范围内，这超出了最大
    8 位整数 127。一个直接的解决方案是在去量化过程中打开饱和选项。然而，简单地应用饱和度将严重影响计算吞吐量，使速度降低多达 67%。'
- en: 'We reconsider the dequantization process. Take Equation [2](#S2.E2 "In II-B
    Integer Quantization ‣ II Background ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving") into Equation [5](#S4.E5 "In IV-A Progressive
    Group Quantization ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving"), we have,'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '我们重新考虑去量化过程。将方程 [2](#S2.E2 "在 II-B 整数量化 ‣ II 背景 ‣ QServe: W4A8KV4 量化与系统设计以实现高效
    LLM 服务") 代入方程 [5](#S4.E5 "在 IV-A 逐步组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4 量化与系统设计以实现高效
    LLM 服务")，我们得到，'
- en: '|  | $\small\hat{q}_{s8}=\lfloor\frac{{q}_{s8}}{{s}_{u8}}\rceil\cdot{{s}_{u8}}\leq{q}_{s8}+\frac{1}{2}{{s}_{u8}}.$
    |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\hat{q}_{s8}=\lfloor\frac{{q}_{s8}}{{s}_{u8}}\rceil\cdot{{s}_{u8}}\leq{q}_{s8}+\frac{1}{2}{{s}_{u8}}.$
    |  |'
- en: Since $1$2, we have,
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从 $1$2 开始，我们有，
- en: '|  | $\small\hat{q}_{s8}\leq 127\rightarrow{q}_{s8}\leq 127-\frac{1}{2}{{s}_{u8}}\rightarrow{q}_{s8}\leq
    119.5$ |  |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\hat{q}_{s8}\leq 127\rightarrow{q}_{s8}\leq 127-\frac{1}{2}{{s}_{u8}}\rightarrow{q}_{s8}\leq
    119.5$ |  |'
- en: 'Therefore, we shrink the INT8 symmetric quantization range from [-127, 127]
    to a protective range [-119, 119] in order to avoid the dequantization overflow,
    as shown in the top of Figure [6](#S4.F6 "Figure 6 ‣ IV-A Progressive Group Quantization
    ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们将 INT8 对称量化范围从 [-127, 127] 缩小到保护范围 [-119, 119]，以避免去量化溢出，如图 [6](#S4.F6
    "图 6 ‣ IV-A 逐步组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4 量化与系统设计以实现高效 LLM 服务") 顶部所示。'
- en: Compared to previous two-level quantization
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与之前的两级量化相比，
- en: 'progressive group quantization introduces two levels of scales $\mathbf{s}^{(0)}_{\mathrm{fp16}}$.
    Prior studies such as VSQuant and DoubleQuant in QLoRA [[9](#bib.bib9)] also introduce
    two levels of scales to reduce the memory footprint of group-wise scaling factors.
    In contrast to our quantization flow, previous approaches directly apply group
    quantization with the target precision and then perform per-channel quantization
    on the group-wise floating-point scaling factors, as shown in the bottom of Figure [6](#S4.F6
    "Figure 6 ‣ IV-A Progressive Group Quantization ‣ IV QoQ Quantization ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '渐进式组量化引入了两个级别的尺度 $\mathbf{s}^{(0)}_{\mathrm{fp16}}$。之前的研究，如 QLoRA 中的 VSQuant
    和 DoubleQuant [[9](#bib.bib9)]，也引入了两个级别的尺度，以减少组尺度因子的内存占用。与我们的量化流程相比，以前的方法直接将组量化应用于目标精度，然后对组浮点尺度因子进行逐通道量化，如图
    [6](#S4.F6 "图 6 ‣ IV-A 渐进式组量化 ‣ IV QoQ 量化 ‣ QServe: W4A8KV4 量化与高效 LLM 服务系统设计")
    底部所示。'
- en: '|  | $\small\hat{\mathbf{W}}={\mathbf{Q}_{\mathbf{W}}}_{\mathrm{s4}}\cdot\mathbf{s}_{\mathrm{fp16}},\;\;\;\hat{\mathbf{s}}_{\mathrm{fp16}}={\mathbf{s}}^{(1)}_{\mathrm{u8}}\cdot\mathbf{s}^{(0)}_{\mathrm{fp16}}$
    |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\hat{\mathbf{W}}={\mathbf{Q}_{\mathbf{W}}}_{\mathrm{s4}}\cdot\mathbf{s}_{\mathrm{fp16}},\;\;\;\hat{\mathbf{s}}_{\mathrm{fp16}}={\mathbf{s}}^{(1)}_{\mathrm{u8}}\cdot\mathbf{s}^{(0)}_{\mathrm{fp16}}$
    |  | (6) |'
- en: Therefore, using the group-wise scaling factors ${\mathbf{s}}^{(1)}_{\mathrm{u8}}$
    cannot yield the 8-bit weight tensor. During the computation on GPUs, these approaches
    usually first dequantize the scales and, subsequently, the weights into floating-point
    values, which ultimately limits the peak throughput.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用组尺度因子 ${\mathbf{s}}^{(1)}_{\mathrm{u8}}$ 无法生成 8 位权重张量。在 GPU 上进行计算时，这些方法通常首先将尺度解量化，然后将权重解量化为浮点值，这最终限制了峰值吞吐量。
- en: DGQ [[43](#bib.bib43)] also follows the quantization scheme of VSQuant and DoubleQuant,
    but enforces restrictions on scaling factors to make sure that all computation
    can be mapped onto INT8 tensor cores. However, the DGQ serving system separates
    dequantization kernel with the GEMM kernel. Consequently, the end-to-end latency
    of W4A8 GEMM in DGQ is even slower than the W8A8 GEMM in cuBLAS, failing to demonstrate
    the memory bandwidth advantage of 4-bit weight quantization. In contrast, our
    QoQ introduces a protective range, allowing us to fuse dequantization operations
    into the W4A8 GEMM kernel with full register-level parallelism, minimizing CUDA
    core overhead. Thus, our QServe’s W4A8 per-group GEMM achieves 1.5$\times$ speedup
    over the W8A8 cuBLAS GEMM.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: DGQ [[43](#bib.bib43)] 也遵循 VSQuant 和 DoubleQuant 的量化方案，但对尺度因子施加了限制，以确保所有计算都可以映射到
    INT8 张量核心。然而，DGQ 服务系统将解量化内核与 GEMM 内核分开。因此，DGQ 中的 W4A8 GEMM 的端到端延迟甚至比 cuBLAS 中的
    W8A8 GEMM 更慢，未能展示 4 位权重量化的内存带宽优势。相比之下，我们的 QoQ 引入了保护范围，使我们能够将解量化操作融合到 W4A8 GEMM
    内核中，实现完全寄存器级别的并行处理，最小化 CUDA 核心开销。因此，我们的 QServe 的 W4A8 每组 GEMM 比 W8A8 cuBLAS GEMM
    快 1.5$\times$。
- en: IV-B SmoothAttention
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 平滑注意力
- en: '![Refer to caption](img/22eea115aad36dbe3da7ce5e11c331ca.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22eea115aad36dbe3da7ce5e11c331ca.png)'
- en: 'Figure 7: SmoothAttention effectively smooths the outliers in Keys. Values
    doesn’t suffer from outliers.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：平滑注意力有效地平滑了键中的离群值。值不受离群值的影响。
- en: 'As illustrated in Figure [16](#S6.F16 "Figure 16 ‣ VI-D Analysis and Discussion.
    ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"), directly reducing the KV cache to 4 bits significantly degrades
    the LLM accuracy. We visualize the magnitude distributions of the sampled Key
    and Value cache activations in Figure [7](#S4.F7 "Figure 7 ‣ IV-B SmoothAttention
    ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving"). We observe that: the Value matrices show no significant
    outlier pattern, whereas Key matrices tend to have fixed outlier channels in each
    head. These outliers are $\sim$ larger than most of activation values. Though
    they can be easily handled KV8 quantization in prior works [[38](#bib.bib38)],
    it places challenging obstacle to KV4 quantization due to less quantization levels.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[16](#S6.F16 "Figure 16 ‣ VI-D Analysis and Discussion. ‣ VI Evaluation ‣
    QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")所示，直接将KV缓存减少到4位会显著降低LLM的准确性。我们在图[7](#S4.F7
    "Figure 7 ‣ IV-B SmoothAttention ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving")中可视化了采样的Key和Value缓存激活的幅度分布。我们观察到：Value矩阵没有明显的异常模式，而Key矩阵则在每个头中倾向于有固定的异常通道。这些异常值$\sim$大于大多数激活值。虽然在以前的工作中可以轻松处理KV8量化[[38](#bib.bib38)]，但由于量化级别较少，它对KV4量化构成了挑战性的障碍。'
- en: 'Inspired by SmoothQuant [[38](#bib.bib38)], we propose SmoothAttention to scale
    down the outlier channels in Key cache by a per-channel factor $\mathbf{\lambda}$:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 受到SmoothQuant[[38](#bib.bib38)]的启发，我们提出SmoothAttention，通过每通道因子$\mathbf{\lambda}$缩小Key缓存中的异常通道：
- en: '|  | $\small\mathbf{Z}=\left(\mathbf{Q}\mathbf{\Lambda}\right)\cdot\left(\mathbf{K}\mathbf{\Lambda}^{-1}\right)^{T},\;\;\;\mathbf{\Lambda}=\mathrm{diag}\left(\mathbf{\lambda}\right)$
    |  | (7) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{Z}=\left(\mathbf{Q}\mathbf{\Lambda}\right)\cdot\left(\mathbf{K}\mathbf{\Lambda}^{-1}\right)^{T},\;\;\;\mathbf{\Lambda}=\mathrm{diag}\left(\mathbf{\lambda}\right)$
    |  | (7) |'
- en: SmoothQuant migrates the quantization difficulty from activations to weights,
    and thus requires a dedicate balance between activation and weight quantization
    by searching the migration strength. In contrast, since we do not quantize Queries,
    we only need to concentrate on the Keys and simply choose the SmoothAttention
    scale factor as,
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothQuant将量化难度从激活迁移到权重，因此需要在激活和权重量化之间进行专门的平衡，通过搜索迁移强度来实现。相反，由于我们不对Queries进行量化，我们只需集中在Keys上，并简单地选择SmoothAttention的缩放因子为，
- en: '|  | $\small\mathbf{\lambda}_{i}=\max\left(&#124;\mathbf{K}_{i}&#124;\right)^{\alpha}.$
    |  | (8) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{\lambda}_{i}=\max\left(|\mathbf{K}_{i}|\right)^{\alpha}.$
    |  | (8) |'
- en: 'In practice, $\alpha=0.5$ is good enough. As shown in Figure [7](#S4.F7 "Figure
    7 ‣ IV-B SmoothAttention ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving"), after SmoothAttention, the outliers
    in Key cache have been greatly smoothed.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '实际中，$\alpha=0.5$就足够了。如图[7](#S4.F7 "Figure 7 ‣ IV-B SmoothAttention ‣ IV QoQ
    Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")所示，在SmoothAttention之后，Key缓存中的异常值已大大平滑。'
- en: In order to eliminate the extra kernel call overhead for SmoothAttention scaling,
    fusing the scale into preceding linear layer’s weights is preferred. However,
    modern LLMs employ the rotary positional embedding (RoPE) to both Keys and Queries,
    which needs extra handling. In practice, rotary positional embedding pairs channel
    $i$, and accordingly,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除SmoothAttention缩放的额外内核调用开销，建议将缩放融入前面的线性层权重。然而，现代LLM使用旋转位置嵌入（RoPE）对Keys和Queries进行处理，这需要额外的处理。在实践中，旋转位置嵌入配对通道$i$，因此，
- en: '|  | $\small\mathbf{\lambda}_{i}=\lambda_{i+\frac{D}{2}}=\max\left(\max\left(&#124;\mathbf{K}_{i}&#124;\right),\max\left(&#124;\mathbf{K}_{i+\frac{D}{2}}&#124;\right)\right)^{\alpha}$
    |  | (9) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{\lambda}_{i}=\lambda_{i+\frac{D}{2}}=\max\left(\max\left(|\mathbf{K}_{i}|\right),\max\left(|\mathbf{K}_{i+\frac{D}{2}}|\right)\right)^{\alpha}$
    |  | (9) |'
- en: Afterwards, we can easily fuse the SmoothAttention scale $\mathbf{\Lambda}$.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们可以轻松融合SmoothAttention的缩放$\mathbf{\Lambda}$。
- en: IV-C General LLM Quantization Optimizations
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 一般LLM量化优化
- en: One of the key challenges of low-bit LLM quantization is the activation outliers
    for every linear layers. We apply different optimizations for different types
    of linear layers as discussed below.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 低比特LLM量化的关键挑战之一是每个线性层的激活异常值。我们对不同类型的线性层应用了不同的优化，如下所述。
- en: '![Refer to caption](img/070d031f0155e4ef380a8366a9612b23.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/070d031f0155e4ef380a8366a9612b23.png)'
- en: 'Figure 8: Rotate the block input activations to suppress the outliers: since
    rotation is a unitary transformation, the rotation matrix $\mathbf{Q}$ can be
    absorbed by the weights of the output module in the previous block.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：旋转块输入激活以抑制异常值：由于旋转是一种单位变换，旋转矩阵 $\mathbf{Q}$ 可以被前一个块中的输出模块的权重吸收。
- en: '![Refer to caption](img/c245c3d62a21502255b31dc4c65b1795.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c245c3d62a21502255b31dc4c65b1795.png)'
- en: 'Figure 9: Smooth the block intermediate activations, migrating the quantization
    difficulty to weights: since smoothing is channel-independent, the smooth matrix
    $\mathbf{\Lambda}$ is diagonal and can be absorbed by the weights of the previous
    modules.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：平滑块中间激活，将量化难度迁移到权重上：由于平滑是通道无关的，平滑矩阵 $\mathbf{\Lambda}$ 是对角的，可以被前面模块的权重吸收。
- en: IV-C1 Block Input Module Rotation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C1 块输入模块旋转
- en: 'In transformer blocks, we define the components that take in the block inputs
    as input modules, such as the QKV Projection Layer and the FFN 1st Layer. As shown
    in Figure [9](#S4.F9 "Figure 9 ‣ IV-C General LLM Quantization Optimizations ‣
    IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"), inspired by [[2](#bib.bib2), [4](#bib.bib4)], we rotate the block
    input activations by multiplying the rotation matrix. To keep mathematical equivalence
    of linear layers, we rotate the corresponding weights accordingly in the reversed
    direction. After rotation, each channel’s activations are linear combinations
    of all other channels, and thus outlier channels are effectively suppressed. Furthermore,
    since rotation is a unitary transformation, we can fuse the rotation matrix with
    the previous linear layers’ weights. We simply choose the scaled Hadamard matrix
    as the rotation matrix.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器块中，我们将接收块输入的组件定义为输入模块，如 QKV 投影层和 FFN 第一层。如图[9](#S4.F9 "图 9 ‣ IV-C 一般 LLM
    量化优化 ‣ IV QoQ 量化 ‣ QServe：W4A8KV4 量化和系统共同设计以实现高效 LLM 服务")所示，受[[2](#bib.bib2)、[4](#bib.bib4)]的启发，我们通过乘以旋转矩阵来旋转块输入激活。为了保持线性层的数学等效性，我们相应地以反向方向旋转相应的权重。旋转后，每个通道的激活是所有其他通道的线性组合，因此异常值通道被有效抑制。此外，由于旋转是一种单位变换，我们可以将旋转矩阵与之前线性层的权重融合。我们简单地选择缩放的
    Hadamard 矩阵作为旋转矩阵。
- en: IV-C2 Block Output Module Smoothing
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C2 块输出模块平滑
- en: 'Output modules refer to those layers that generate block outputs, such as the
    Output Projection Layer and FFN 2nd Layer. As shown in Figure [9](#S4.F9 "Figure
    9 ‣ IV-C General LLM Quantization Optimizations ‣ IV QoQ Quantization ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), inspired
    by [[38](#bib.bib38)], we smooth the block intermediate activations through dividing
    them by a per-channel smoothing factor. Original SmoothQuant does not smooth the
    block intermediate activations; moreover, if we directly smooth these modules
    with the same migration strength as input modules (*e.g*., q_proj, up_proj), the
    evaluated Wikitext-2 perplexity of the Llama-2-7B model will drop by as much as
    0.05\. In practice, we find that the migration strength $\alpha$ is mostly determined
    by weights instead of activations, which is very different from the observations
    in SmoothQuant.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出模块指生成块输出的层，如输出投影层和 FFN 第二层。如图[9](#S4.F9 "图 9 ‣ IV-C 一般 LLM 量化优化 ‣ IV QoQ 量化
    ‣ QServe：W4A8KV4 量化和系统共同设计以实现高效 LLM 服务")所示，受[[38](#bib.bib38)]的启发，我们通过将块中间激活除以每通道平滑因子来平滑这些激活。原始的
    SmoothQuant 不平滑块中间激活；此外，如果我们直接以与输入模块相同的迁移强度（*例如*，q_proj, up_proj）来平滑这些模块，则 Llama-2-7B
    模型的 Wikitext-2 困惑度将下降多达 0.05。在实践中，我们发现迁移强度 $\alpha$ 大多由权重而非激活决定，这与 SmoothQuant
    中的观察结果非常不同。
- en: IV-C3 Activation-Aware Channel Reordering
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C3 激活感知通道重排序
- en: '![Refer to caption](img/3b4be8f66770c2b4d73d1589d5e0be16.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3b4be8f66770c2b4d73d1589d5e0be16.png)'
- en: 'Figure 10: Reorder weight input channels based on their salience in group quantization.
    Channel salience can be determined by the magnitude of input activations.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：根据组量化中的显著性重新排序权重输入通道。通道显著性可以通过输入激活的幅度来确定。
- en: 'Both AWQ [[23](#bib.bib23)] and Atom [[44](#bib.bib44)] have observed that
    maintaining the salient weights in FP16 can significantly improve model accuracy.
    These salient weights can be identified by the activation distribution. Instead
    of introducing mixed-precision quantization used by Atom, we propose activation-aware
    channel reordering as shown in Figure [10](#S4.F10 "Figure 10 ‣ IV-C3 Activation-Aware
    Channel Reordering ‣ IV-C General LLM Quantization Optimizations ‣ IV QoQ Quantization
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").
    We use $\max\left(|\mathbf{X}|\right)$ to determine the channel salience, and
    then reorder channels so that channels with similar salience are in the same quantization
    group.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'AWQ [[23](#bib.bib23)]和Atom [[44](#bib.bib44)]都观察到，保持FP16中的显著权重可以显著提高模型准确性。这些显著权重可以通过激活分布来识别。与Atom使用的混合精度量化不同，我们提出了如图[10](#S4.F10
    "Figure 10 ‣ IV-C3 Activation-Aware Channel Reordering ‣ IV-C General LLM Quantization
    Optimizations ‣ IV QoQ Quantization ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving")所示的激活感知通道重新排序。我们使用$\max\left(|\mathbf{X}|\right)$来确定通道显著性，然后重新排序通道，以使具有相似显著性的通道在同一量化组中。'
- en: IV-C4 Weight Clipping
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-C4 权重裁剪
- en: 'Weight clipping is another popular quantization optimization technique. It
    applies a clip ratio $\alpha$. In QServe, we minimize the layer output error for
    all linear layers, expect for q_proj and k_proj, for which we optimize block output
    mean square error:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 权重裁剪是另一种流行的量化优化技术。它应用了裁剪比例$\alpha$。在QServe中，我们最小化所有线性层的层输出误差，除了q_proj和k_proj，我们优化块输出均方误差：
- en: '|  | $\small\arg\min_{\alpha}\&#124;\mathrm{Block}\left(\mathbf{X};\mathbf{W}\right)-\mathrm{Block}\left(\mathbf{X};Q\left(\mathbf{W};\alpha\right)\right)\&#124;.$
    |  | (10) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\arg\min_{\alpha}\&#124;\mathrm{Block}\left(\mathbf{X};\mathbf{W}\right)-\mathrm{Block}\left(\mathbf{X};Q\left(\mathbf{W};\alpha\right)\right)\&#124;.$
    |  | (10) |'
- en: V QServe Serving System
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V QServe 服务系统
- en: 'To this end, we have presented the QoQ quantization algorithm, which aims to
    minimize accuracy loss incurred by W4A8KV4 quantization. However, realizing the
    theoretical throughput benefits in Figure [3](#S3.F3 "Figure 3 ‣ III-A W4A8KV4
    Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving") remains challenging. Thus, in
    this section, we will delve into the QServe system design, which is guided by
    two important principles: I. Reducing main loop overhead in GEMM kernels; II.
    Making fused attention kernels memory bound.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，我们提出了QoQ量化算法，旨在最小化W4A8KV4量化带来的准确性损失。然而，实现图[3](#S3.F3 "Figure 3 ‣ III-A W4A8KV4
    Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving")中理论吞吐量的好处仍然具有挑战性。因此，在本节中，我们将深入探讨QServe系统设计，指导原则有两个：I.
    减少GEMM内核的主循环开销；II. 使融合的注意力内核内存受限。'
- en: V-A QServe System Runtime
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A QServe 系统运行时
- en: '![Refer to caption](img/b1c902076877a1a12c5f4becbb6914fc.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b1c902076877a1a12c5f4becbb6914fc.png)'
- en: 'Figure 11: QServe’s precision mapping for an FP16 in, FP16 out LLM block. All
    GEMM operators take in W4A8 inputs and produce FP16 outputs. Activation quantization
    happens in normalization and activation layers.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：QServe的FP16输入，FP16输出LLM块的精度映射。所有GEMM运算符接受W4A8输入并生成FP16输出。激活量化发生在归一化和激活层中。
- en: 'We start by introducing the QServe runtime in Figure [11](#S5.F11 "Figure 11
    ‣ V-A QServe System Runtime ‣ V QServe Serving System ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving"). All GEMM layers in QServe operate
    on W4A8 inputs, perform computation on INT8 tensor cores, and generate FP16 outputs.
    All attention layers perform computation in FP16 on CUDA cores. Consequently,
    each LLM block in QServe has FP16 inputs and FP16 outputs.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从图[11](#S5.F11 "Figure 11 ‣ V-A QServe System Runtime ‣ V QServe Serving
    System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")中介绍QServe运行时。QServe中的所有GEMM层在W4A8输入上操作，使用INT8张量核心进行计算，并生成FP16输出。所有注意力层在CUDA核心上以FP16进行计算。因此，QServe中的每个LLM块都有FP16输入和FP16输出。'
- en: Activation Quantization. To ensure that each GEMM takes in INT8 activation,
    we fuse activation quantization into the preceding layernorm for the QKV projection
    and the first FFN layer, or into the preceding activation kernel for the second
    FFN layer. Furthermore, a separate quantization node is inserted before output
    projection in the attention block.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 激活量化。为了确保每个GEMM使用INT8激活，我们将激活量化融合到QKV投影和第一个FFN层的前置层归一化中，或者融合到第二个FFN层的前置激活内核中。此外，在注意力块的输出投影之前插入了一个单独的量化节点。
- en: KV Cache Management. To avoid memory fragmentation, we follow vLLM [[21](#bib.bib21)]
    and TensorRT-LLM [[25](#bib.bib25)] to adopt paged KV caches. In contrast to these
    two frameworks, which perform per-tensor, static quantization (*i.e*., scaling
    factors computed offline) on KV caches, QServe requires per-head, dynamic KV quantization
    to maintain competitive accuracy due to the lower bit precision (4 *vs*. 8). We
    therefore store FP16 scaling factors and zero points for each head immediately
    following the quantized KV features in each KV cache page, allowing these values
    to be updated on-the-fly. QServe also supports in-flight batching, similar to
    vLLM and TensorRT-LLM.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: KV缓存管理。为了避免内存碎片化，我们遵循vLLM[[21](#bib.bib21)]和TensorRT-LLM[[25](#bib.bib25)]采用分页KV缓存。与这两个框架对KV缓存进行每张量、静态量化（*即*，离线计算的缩放因子）不同，QServe需要每头、动态KV量化以维持竞争力的准确性，因为位精度较低（4
    *vs*. 8）。因此，我们在每个KV缓存页面中的量化KV特征后立即存储FP16缩放因子和零点，允许这些值实时更新。QServe还支持在飞行中批处理，类似于vLLM和TensorRT-LLM。
- en: V-B W4A8 GEMM in QServe
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B QServe中的W4A8 GEMM
- en: 'As discussed in Section [III](#S3 "III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving"), the main loop overhead poses
    a significant obstacle in allowing quantized GEMMs to attain the theoretical performance
    gains projected by the roofline model (Figure [3](#S3.F3 "Figure 3 ‣ III-A W4A8KV4
    Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving")). Therefore, the focus of QServe
    W4A8 GEMM is to reduce main loop overhead. Specifically, we address the costs
    of pointer arithmetic operations through compute-aware weight reorder, and reduce
    dequantization overhead through a subtraction after multiplication computation
    order and register-level parallelism.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[III](#S3 "III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design
    for Efficient LLM Serving")节所讨论的，主循环开销在让量化的GEMM实现屋顶线模型（图[3](#S3.F3 "Figure 3 ‣
    III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")）所预测的理论性能提升方面构成了重大障碍。因此，QServe
    W4A8 GEMM的重点是减少主循环开销。具体而言，我们通过计算感知的权重重排来解决指针算术操作的开销，并通过乘法计算顺序后的减法和寄存器级并行性来减少去量化开销。'
- en: V-B1 Compute-Aware Weight Reorder
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B1 计算感知的权重重排
- en: '![Refer to caption](img/3a8b234f18aff4ac8556862ba0256cad.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/3a8b234f18aff4ac8556862ba0256cad.png)'
- en: 'Figure 12: QServe applies compute-aware weight reoder to minimize the pointer
    arithmetics in W4A8 GEMM main loop.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：QServe应用计算感知的权重重排，以最小化W4A8 GEMM主循环中的指针算术操作。
- en: 'Prior to dequantization and tensor core computation, the operands must be loaded
    from global memory into the L1 shared memory during each main loop iteration.
    This loading process is non-trivial since the tensor core GEMM intrisics require
    a strided layout for each thread in computation, as demonstrated in Figure [12](#S5.F12
    "Figure 12 ‣ V-B1 Compute-Aware Weight Reorder ‣ V-B W4A8 GEMM in QServe ‣ V QServe
    Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")a. For instance, instead of loading consecutive eight INT8 weights,
    thread 0 first loads input channels 0-3, then skips ahead to input channels 16-19\.
    That said, a naive weight loading implementation would require one address calculation
    per four channels, leading to two efficiency issues. First, pointer arithmetic
    operations are performed on CUDA cores, which have 32$\times$, and the ldmatrix
    instruction automatically distributes the data in a strided manner, ensuring that
    each thread eventually obtains the required data for INT8 tensor core computation.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '在去量化和张量核心计算之前，操作数必须在每次主循环迭代中从全局内存加载到L1共享内存。这一加载过程并非微不足道，因为张量核心GEMM的内在要求每个线程在计算中使用跨步布局，如图[12](#S5.F12
    "Figure 12 ‣ V-B1 Compute-Aware Weight Reorder ‣ V-B W4A8 GEMM in QServe ‣ V QServe
    Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")a所示。例如，线程0不是连续加载八个INT8权重，而是首先加载输入通道0-3，然后跳到输入通道16-19。也就是说，一个简单的权重加载实现每四个通道需要进行一次地址计算，导致两个效率问题。首先，指针算术操作在CUDA核心上执行，这些核心有32$\times$，而ldmatrix指令会自动以跨步方式分配数据，确保每个线程最终获得INT8张量核心计算所需的数据。'
- en: 'Unfortunately, the ldmatrix instruction will not work when the data types used
    for storage and computation differ (like in W4A8). Specifically, in Figure [12](#S5.F12
    "Figure 12 ‣ V-B1 Compute-Aware Weight Reorder ‣ V-B W4A8 GEMM in QServe ‣ V QServe
    Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")b, ldmatrix ensures that each thread obtains the same number of bytes,
    not the same number of elements, after data permutation in the register file.
    Consequently, thread 0 obtains the tiles needed by both itself and thread 1, while
    thread 1 obtains the tiles needed by thread 2 and thread 3 in the subsequent INT8
    tensor core computation. This creates a mismatch between the data obtained by
    each thread and used in computation. That said, ldmatrix cannot be used for W4A8
    GEMM and the aforementioned pointer arithmetic overhead persists. Worse still,
    memory bandwidth utilization deteriorates further as we consecutively load only
    16 bits for 4-bit weights.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '不幸的是，当存储和计算所使用的数据类型不同（如在 W4A8 中）时，ldmatrix 指令将无法正常工作。具体来说，在图 [12](#S5.F12 "图
    12 ‣ V-B1 计算感知权重重新排序 ‣ V-B W4A8 GEMM 在 QServe 中 ‣ V QServe 服务系统 ‣ QServe: W4A8KV4
    量化和系统共同设计以提高 LLM 服务效率")b 中，ldmatrix 确保每个线程在寄存器文件中数据排列后获取相同数量的字节，而不是相同数量的元素。因此，线程
    0 获取了它自己和线程 1 所需的块，而线程 1 获取了线程 2 和线程 3 在后续 INT8 张量核心计算中所需的块。这造成了每个线程获得的数据与计算中使用的数据不匹配。也就是说，ldmatrix
    无法用于 W4A8 GEMM，上述指针算术开销依然存在。更糟糕的是，随着我们连续加载仅 16 位的 4 位权重，内存带宽利用率进一步恶化。'
- en: 'We address this challenge through compute-aware weight reordering (Figure [12](#S5.F12
    "Figure 12 ‣ V-B1 Compute-Aware Weight Reorder ‣ V-B W4A8 GEMM in QServe ‣ V QServe
    Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")c). The key insight is to store the weights in the order they are
    used during computation. We divide the entire GEMM problem into multiple 32$\times$32
    tiles. Within each tile, thread 0 utilizes input channels 0-3 and 16-19 for output
    channels 0, 8, 16, and 24 (output channels 16-31 are omitted in Figure [12](#S5.F12
    "Figure 12 ‣ V-B1 Compute-Aware Weight Reorder ‣ V-B W4A8 GEMM in QServe ‣ V QServe
    Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")c). Consequently, we concatenate these 32 channels into a single
    128-bit word. The 32 channels used by thread 1 are stored immediately following
    thread 0’s 32 channels. Since weights are static, such reordering does not introduce
    any runtime overhead. Additionally, it not only reduces the pointer arithmetic
    overhead to the same level as ldmatrix but also guarantees high-bandwidth 128-bit/thread
    memory transactions. We apply this reordering to zero points and scales as well
    to mitigate dequantization overhead.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过计算感知权重重新排序（图 [12](#S5.F12 "图 12 ‣ V-B1 计算感知权重重新排序 ‣ V-B W4A8 GEMM 在 QServe
    中 ‣ V QServe 服务系统 ‣ QServe: W4A8KV4 量化和系统共同设计以提高 LLM 服务效率")c）来解决这个挑战。关键的见解是按计算过程中使用的顺序存储权重。我们将整个
    GEMM 问题划分为多个 32$\times$32 的块。在每个块内，线程 0 使用输入通道 0-3 和 16-19 来处理输出通道 0、8、16 和 24（输出通道
    16-31 在图 [12](#S5.F12 "图 12 ‣ V-B1 计算感知权重重新排序 ‣ V-B W4A8 GEMM 在 QServe 中 ‣ V QServe
    服务系统 ‣ QServe: W4A8KV4 量化和系统共同设计以提高 LLM 服务效率")c 中省略）。因此，我们将这 32 个通道串联成一个 128 位字。线程
    1 使用的 32 个通道被紧接在线程 0 的 32 个通道后面存储。由于权重是静态的，这种重新排序不会引入任何运行时开销。此外，这不仅将指针算术开销降低到与
    ldmatrix 相同的水平，还保证了高带宽的 128 位/线程内存事务。我们还将这种重新排序应用于零点和缩放因子，以减轻反量化开销。'
- en: V-B2 Fast Dequantization in Per-Channel W4A8 GEMM
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B2 每通道 W4A8 GEMM 中的快速反量化
- en: '![Refer to caption](img/4c04d766a5b8ad4fa45a6c59721c3545.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4c04d766a5b8ad4fa45a6c59721c3545.png)'
- en: 'Figure 13: QServe exploits register-level parallelism to significantly reduce
    the number of required logical operations in UINT4 to UINT8 weight unpacking.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：QServe 利用寄存器级并行性显著减少 UINT4 到 UINT8 权重解包所需的逻辑操作数量。
- en: 'As illustrated in Figure [5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4: Main
    Loop Overhead in GEMM ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving")d, dequantizing weights within the main loop
    becomes necessary when the bit precisions for weights and activations differ.
    In the case of per-channel W4A8 quantization, second-level scaling factors are
    omitted, and first-level FP16 scaling is efficiently fused into the GEMM epilogue.
    We therefore focus our discussion on the efficient conversion from ZINT4 (i.e.,
    unsigned 4-bit integers with zero points) to SINT8 within the main loop. We further
    decompose this conversion into two steps: UINT4 to UINT8 (weight unpacking) and
    UINT8 to SINT8 (zero point subtraction). As depicted in Figure [13](#S5.F13 "Figure
    13 ‣ V-B2 Fast Dequantization in Per-Channel W4A8 GEMM ‣ V-B W4A8 GEMM in QServe
    ‣ V QServe Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design
    for Efficient LLM Serving"), we reorder every 32 UINT4 weights $w_{0},w_{1},...,w_{31}$
    This allows us to exploit register-level parallelism and efficiently unpack them
    into UINT8 numbers with only three logical operations.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[5](#S3.F5 "图5 ‣ III-B 为什么不是W4A4KV4：GEMM中的主循环开销 ‣ III 动机 ‣ QServe: W4A8KV4量化与系统共设计")所示，当权重和激活的位精度不同的时候，在主循环中去量化权重是必要的。在每通道W4A8量化的情况下，省略了二级缩放因子，一级FP16缩放有效地融合到GEMM尾声中。因此，我们将讨论集中在主循环中将ZINT4（即带零点的无符号4位整数）高效转换为SINT8。我们进一步将此转换分解为两个步骤：UINT4到UINT8（权重解包）和UINT8到SINT8（零点减法）。如图[13](#S5.F13
    "图13 ‣ V-B2频道W4A8 GEMM中的快速去量化 ‣ V-B W4A8 GEMM在QServe ‣ V QServe服务系统 ‣ QServe:
    W4A8KV4量化与系统共设计")所示，我们重新排序每32个UINT4权重$w_{0},w_{1},...,w_{31}$。这使我们能够利用寄存器级并行性，并仅通过三次逻辑操作高效地将它们解包为UINT8数字。'
- en: For the conversion from UINT8 to SINT8, the most intuitive approach is to introduce
    integer subtraction instructions within the main loop, which we refer to as subtraction
    before multiplication. Although straightforward, this approach inevitably introduces
    additional cost to the main loop, which is undesirable. Instead, we adopt a subtraction
    after multiplication approach to minimize the main loop overhead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从UINT8到SINT8的转换，最直观的方法是在主循环中引入整数减法指令，我们称之为乘法前减法。虽然直接，但这种方法不可避免地给主循环带来额外的开销，这是不理想的。相反，我们采用乘法后减法的方法来最小化主循环的开销。
- en: 'Specifically, a GEMM layer with per-channel quantized operands can be expressed
    as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，一个具有每通道量化操作数的GEMM层可以表示为：
- en: '|  | $\small\mathbf{O}=\hat{\mathbf{X}}\hat{\mathbf{W}}=(\mathbf{Q}_{\mathbf{X}}\odot\mathbf{S}_{\mathbf{X}})((\mathbf{Q}_{\mathbf{W}}-\mathbf{Z}_{\mathbf{W}})\odot\mathbf{S}_{\mathbf{W}}),$
    |  | (11) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small\mathbf{O}=\hat{\mathbf{X}}\hat{\mathbf{W}}=(\mathbf{Q}_{\mathbf{X}}\odot\mathbf{S}_{\mathbf{X}})((\mathbf{Q}_{\mathbf{W}}-\mathbf{Z}_{\mathbf{W}})\odot\mathbf{S}_{\mathbf{W}}),$
    |  | (11) |'
- en: 'where $\mathbf{Q}_{\mathbf{W}}$, then we rewrite Equation [11](#S5.E11 "In
    V-B2 Fast Dequantization in Per-Channel W4A8 GEMM ‣ V-B W4A8 GEMM in QServe ‣
    V QServe Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving") as:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\mathbf{Q}_{\mathbf{W}}$，然后我们将公式[11](#S5.E11 "在V-B2频道W4A8 GEMM的快速去量化 ‣ V-B
    W4A8 GEMM在QServe ‣ V QServe服务系统 ‣ QServe: W4A8KV4量化与系统共设计")重写为：'
- en: '|  | $\displaystyle\small\mathbf{O}$ |  | (12) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\small\mathbf{O}$ |  | (12) |'
- en: '|  |  | $\displaystyle=(\mathbf{Q}_{\mathbf{X}}\mathbf{Q}_{\mathbf{W}})\odot(\mathbf{s}_{\mathbf{W}}\times\mathbf{s}_{\mathbf{X}})-(\mathbf{Q}_{\mathbf{X}}\odot\mathbf{S}_{\mathbf{X}})\mathbf{ZS}_{\mathbf{W}}.$
    |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(\mathbf{Q}_{\mathbf{X}}\mathbf{Q}_{\mathbf{W}})\odot(\mathbf{s}_{\mathbf{W}}\times\mathbf{s}_{\mathbf{X}})-(\mathbf{Q}_{\mathbf{X}}\odot\mathbf{S}_{\mathbf{X}})\mathbf{ZS}_{\mathbf{W}}.$
    |  |'
- en: 'The first term, $(\mathbf{Q}_{\mathbf{X}}\mathbf{Q}_{\mathbf{W}})\odot(\mathbf{s}_{\mathbf{W}}\times\mathbf{s}_{\mathbf{X}})$.
    We then notice that:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个项，$(\mathbf{Q}_{\mathbf{X}}\mathbf{Q}_{\mathbf{W}})\odot(\mathbf{s}_{\mathbf{W}}\times\mathbf{s}_{\mathbf{X}})$。我们接着注意到：
- en: '|  | $\mathbf{X}(\mathbf{ZS}_{\mathbf{W}})=\mathbf{t}_{\mathbf{X}}\times(\mathbf{z}_{\mathbf{W}}\odot\mathbf{s}_{\mathbf{W}}),$
    |  | (13) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{X}(\mathbf{ZS}_{\mathbf{W}})=\mathbf{t}_{\mathbf{X}}\times(\mathbf{z}_{\mathbf{W}}\odot\mathbf{s}_{\mathbf{W}}),$
    |  | (13) |'
- en: where $\mathbf{t}_{\mathbf{X}}=\mathbf{X}\mathbf{1}_{k}$. Fortunately, each
    W4A8 kernel is always preceded by a memory-bound kernel, allowing us to fuse the
    precomputation kernel into it with negligible latency overhead.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{t}_{\mathbf{X}}=\mathbf{X}\mathbf{1}_{k}$。幸运的是，每个W4A8内核总是由一个内存绑定的内核前置，使我们能够将预计算内核与其融合，几乎不会带来延迟开销。
- en: V-B3 Fast Dequantization in Per-Group W4A8 GEMM
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B3 每组W4A8 GEMM中的快速去量化
- en: '![Refer to caption](img/9b466b991fe9a9e5ff34d1537becb5e6.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b466b991fe9a9e5ff34d1537becb5e6.png)'
- en: 'Figure 14: Our progressive quantization algorithm ensures that all intermediate
    results in the subtraction after multiplication computation order will not overflow,
    thereby enabling register-level parallelism and reducing main loop overhead.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：我们的渐进式量化算法确保减法在乘法后的计算顺序中所有中间结果不会溢出，从而实现寄存器级并行并减少主循环开销。
- en: 'The primary distinction between the per-group W4A8 GEMM and its per-channel
    counterpart lies in the second-level dequantization process in Figure [5](#S3.F5
    "Figure 5 ‣ III-B Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III Motivation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")d.
    Firstly, since zero points are now defined on a per-group basis, it is no longer
    possible to merge zero point subtraction into the epilogue, as was done in the
    previous section. Secondly, due to the presence of level 2 scales, an additional
    INT8 multiplication is required for each weight. Akin to the previous section,
    we must determine whether to apply multiplication (scales) or subtraction (zeros)
    first during level 2 dequantization.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '每组 W4A8 GEMM 与每通道对应的 GEMM 之间的主要区别在于图[5](#S3.F5 "Figure 5 ‣ III-B Why Not W4A4KV4:
    Main Loop Overhead in GEMM ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and
    System Co-design for Efficient LLM Serving")d中第二级去量化过程的不同。首先，由于零点现在在每组基础上定义，因此无法像前一部分那样将零点减法合并到结尾部分。其次，由于存在第二级缩放因子，因此每个权重需要额外的
    INT8 乘法。与前一部分类似，我们必须确定在第二级去量化过程中是先应用乘法（缩放因子）还是减法（零点）。'
- en: 'In this context, we contend that performing subtraction after multiplication
    remains the advantageous approach because it enables register-level parallelism
    (RLP). As shown in Figure [14](#S5.F14 "Figure 14 ‣ V-B3 Fast Dequantization in
    Per-Group W4A8 GEMM ‣ V-B W4A8 GEMM in QServe ‣ V QServe Serving System ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving"), NVIDIA
    GPUs provide the vadd4 instruction that performs four INT8 additions with a single
    INT32 ALU operation. However, there is no instruction that realizes similar effect
    for 4-way INT8 multiplication. Consequently, in order to achieve RLP, one has
    to simulate this by padding 24 zeros to the most significant bits (MSBs) of the
    8-bit scaling factor. However, this simulation is valid only when the result of
    each INT8 multiplication remains within the INT8 range. This condition is not
    met for the subtraction-before-multiplication computation order. As illustrated
    in Figure [14](#S5.F14 "Figure 14 ‣ V-B3 Fast Dequantization in Per-Group W4A8
    GEMM ‣ V-B W4A8 GEMM in QServe ‣ V QServe Serving System ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving")a, the result of the scale multiplication
    overflows, leading to an incorrect output. In the subtraction-before-multiplication
    approach, we can only perform multiplication one by one, which is extremely inefficient.
    On the other hand, with the subtraction-after-multiplication computation order,
    our progressive group quantization algorithm ensures that the result of the initial
    multiplication step never exceeds the INT8 range. This allows for fully leveraging
    the performance benefits of RLP in both multiplication and subtraction.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种情况下，我们认为在乘法之后进行减法仍然是更有利的方法，因为它支持寄存器级并行（RLP）。如图[14](#S5.F14 "Figure 14 ‣
    V-B3 Fast Dequantization in Per-Group W4A8 GEMM ‣ V-B W4A8 GEMM in QServe ‣ V
    QServe Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving")所示，NVIDIA GPUs 提供了 vadd4 指令，它可以通过一次 INT32 ALU 操作执行四次 INT8
    加法。然而，目前没有类似的指令可以实现 4-way INT8 乘法。因此，为了实现 RLP，必须通过将 24 个零填充到 8 位缩放因子的最高有效位（MSBs）来模拟这一过程。然而，这种模拟仅在每次
    INT8 乘法的结果保持在 INT8 范围内时才有效。对于减法优先的乘法计算顺序，这一条件未得到满足。如图[14](#S5.F14 "Figure 14 ‣
    V-B3 Fast Dequantization in Per-Group W4A8 GEMM ‣ V-B W4A8 GEMM in QServe ‣ V
    QServe Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving")a所示，缩放乘法的结果溢出，导致输出不正确。在减法优先的乘法方法中，我们只能逐一进行乘法，这非常低效。另一方面，采用乘法优先的计算顺序，我们的渐进式分组量化算法确保初始乘法步骤的结果永远不会超出
    INT8 范围。这使得在乘法和减法中都能充分利用 RLP 的性能优势。'
- en: V-B4 General Optimizations
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-B4 一般优化
- en: In our W4A8 kernel, we also employ general techniques for GEMM optimization.
    On the memory side, we apply multi-stage software pipelining and asynchronous
    memory copy to better overlap memory access with computation. Additionally, we
    swizzle the layout of the L1 shared memory to eliminate bank conflicts. To improve
    L2 cache utilization, we permute the computation partition across different thread
    blocks, allowing adjacent blocks to reuse the same weight. On the compute side,
    when the number of input tokens ($m$ into multiple slices and reduce the partial
    sums across different warps in the L1 shared memory.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 W4A8 内核中，我们还采用了 GEMM 优化的通用技术。在内存方面，我们应用多阶段软件流水线和异步内存拷贝，以更好地重叠内存访问和计算。此外，我们调整了
    L1 共享内存的布局，以消除银行冲突。为了提高 L2 缓存的利用率，我们在不同的线程块之间调整计算分区，允许相邻的块重用相同的权重。在计算方面，当输入标记的数量（$m$）时，我们将其划分为多个切片，并在
    L1 共享内存中减少不同 warp 之间的部分和。
- en: V-C KV4 Attention in QServe
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: QServe 中的 V-C KV4 注意力
- en: 'TABLE I: A naive KV4 attention implementation is 1.7$\times$ slower on A100
    due to earlier CUDA core roofline turning point.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：由于较早的 CUDA 核心 roofline 转折点，天真的 KV4 注意力实现比 A100 慢 1.7$\times$。
- en: '| Seq_len | 8-bit KV | 4-bit KV (Naive) | 4-bit KV (Ours) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 序列长度 | 8 位 KV | 4 位 KV（天真实现） | 4 位 KV（我们的实现） |'
- en: '| 128 | 0.09 ms | 0.10 ms (0.87$\times$) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 0.09 毫秒 | 0.10 毫秒 (0.87$\times$) |'
- en: '| 256 | 0.14 ms | 0.16 ms (0.86$\times$) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 0.14 毫秒 | 0.16 毫秒 (0.86$\times$) |'
- en: '| 512 | 0.23 ms | 0.27 ms (0.87$\times$) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 512 | 0.23 毫秒 | 0.27 毫秒 (0.87$\times$) |'
- en: '| 1024 | 0.42 ms | 0.48 ms (0.88$\times$) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 0.42 毫秒 | 0.48 毫秒 (0.88$\times$) |'
- en: '| 1536 | 0.62 ms | 0.69 ms (0.90$\times$) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 1536 | 0.62 毫秒 | 0.69 毫秒 (0.90$\times$) |'
- en: 'Attention accounts for 30-50% of the total LLM runtime, as depicted in Figure
    [2](#S3.F2 "Figure 2 ‣ III-A W4A8KV4 Has Superior Roofline Over W8A8, W4A16 ‣
    III Motivation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving")a. Although the roofline model in Figure [5](#S3.F5 "Figure 5 ‣ III-B
    Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III Motivation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving") suggests that quantizing
    the KV cache to INT4 should automatically yield a 2$\times$ speedup over the 8-bit
    KV baseline, this is not the case in real-world implementation.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '注意力占总 LLM 运行时间的 30-50%，如图 [2](#S3.F2 "Figure 2 ‣ III-A W4A8KV4 Has Superior
    Roofline Over W8A8, W4A16 ‣ III Motivation ‣ QServe: W4A8KV4 Quantization and
    System Co-design for Efficient LLM Serving")a 所示。尽管图 [5](#S3.F5 "Figure 5 ‣ III-B
    Why Not W4A4KV4: Main Loop Overhead in GEMM ‣ III Motivation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving") 中的 roofline 模型表明，将
    KV 缓存量化为 INT4 应该自动带来 2$\times$ 的速度提升，相较于 8 位 KV 基线，但在实际实现中并非如此。'
- en: 'We start with the KV8-attention decoding stage kernel from TensorRT-LLM as
    our baseline and replace all static, per-tensor quantized 8-bit KV cache accesses
    and conversions with their dynamic, per-head quantized 4-bit counterparts. This
    direct replacement immediately leads to 1.7$\times$ slowdown on A100 (Table [I](#S5.T1
    "TABLE I ‣ V-C KV4 Attention in QServe ‣ V QServe Serving System ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving")), compared to the
    KV8 baseline.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '我们以 TensorRT-LLM 中的 KV8 注意力解码阶段内核作为基线，替换所有静态的、每个张量量化的 8 位 KV 缓存访问和转换为其动态的、每个头量化的
    4 位对应物。这种直接替换立即导致 A100 上的 1.7$\times$ 慢速（见表 [I](#S5.T1 "TABLE I ‣ V-C KV4 Attention
    in QServe ‣ V QServe Serving System ‣ QServe: W4A8KV4 Quantization and System
    Co-design for Efficient LLM Serving")），与 KV8 基线相比。'
- en: Once again, our analysis reveals that the devil is in the slow CUDA cores, which
    are responsible for executing the attention kernels during the decoding stage.
    While each individual batched GEMV has a computation intensity of 1 MAC / element,
    the computation intensity escalates significantly for a fused attention kernel
    that combines all the arithmetics and KV cache updates. As an illustration, naively
    dequantizing a single INT4 number from the KV cache necessitates 5 ALU Ops. This
    includes mask and shift operations to isolate the operand, type conversion from
    integer to floating-point representation, and floating point mul and sub to obtain
    the final results. It is crucial to note that the roofline turning point for A100
    FP32 CUDA cores is merely 9.8 Ops/Byte. That said, the dequantization of KV operands
    alone already saturates this bound, leading to the surprising observation that
    the fused KV4 attention kernel can become compute-bound on datacenter GPUs like
    A100\. In fact, similar observations hold in other systems like QuaRot [[2](#bib.bib2)]
    and Atom [[44](#bib.bib44)]. Specifically, QuaRot introduces compute-intensive
    Hadamard transformation [[4](#bib.bib4)] in the attention operator, making it
    hard to achieve real speedup over TRT-LLM-KV8 with 4-bit quantized KV caches.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 再次分析表明，瓶颈在于慢速 CUDA 核心，这些核心负责在解码阶段执行注意力内核。尽管每个批处理的 GEMV 计算强度为 1 MAC / 元素，但融合的注意力内核结合了所有的算术运算和
    KV 缓存更新，其计算强度显著增加。举例来说，天真的去量化单个 KV 缓存中的 INT4 数字需要 5 个 ALU 操作。这包括掩码和移位操作来隔离操作数，从整数转换为浮点表示的类型转换，以及浮点乘法和减法来获得最终结果。值得注意的是，A100
    FP32 CUDA 核心的屋顶线拐点仅为 9.8 Ops/Byte。也就是说，KV 操作数的去量化已经达到这个上限，导致意外的观察结果，即融合的 KV4 注意力内核可能在像
    A100 这样的数据中心 GPU 上变得计算受限。事实上，类似的观察在其他系统中也存在，如 QuaRot [[2](#bib.bib2)] 和 Atom [[44](#bib.bib44)]。具体而言，QuaRot
    在注意力操作中引入了计算密集型的 Hadamard 转换 [[4](#bib.bib4)]，使得难以在 4 位量化 KV 缓存上实现相较 TRT-LLM-KV8
    的实际加速。
- en: 'To mitigate the compute-bound bottleneck, it is important to shift the decoding
    stage KV4 attention kernels away from the compute-bound region. We accomplish
    this objective through a bidirectional approach: Firstly, delaying the onset of
    the roofline turning point, and secondly, concurrently reducing the computation
    intensity within the fused kernel. For the first part, we replace all FP32 operations
    in the original TensorRT-LLM kernel with their FP16 counterpart, effectively doubling
    the computation roof. For the second part, we observe that the arithmetic intensity
    of dequantization can be significantly reduced to 2 operations per element by
    applying bit tricks proposed in [[20](#bib.bib20)]. Furthermore, we note that
    simplifying the control logic and prefetching the scaling factors and zero values,
    thereby simplifying address calculations, contribute to performance improvements.
    After incorporating these enhancements, we observe a 1.5$\times$ speedup over
    TensorRT-LLM’s KV8 kernel on A100.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解计算受限的瓶颈，将解码阶段的 KV4 注意力内核从计算受限区域转移是重要的。我们通过双向方法实现了这一目标：首先，延迟屋顶线拐点的出现，其次，减少融合内核中的计算强度。对于第一部分，我们将原始
    TensorRT-LLM 内核中的所有 FP32 操作替换为 FP16 对应项，从而有效地将计算屋顶提高了一倍。对于第二部分，我们观察到通过应用 [[20](#bib.bib20)]
    中提出的位操作技巧，可以将去量化的算术强度显著降低到每个元素 2 次操作。此外，我们注意到简化控制逻辑和预取缩放因子及零值，从而简化地址计算，也有助于性能提升。在融入这些改进后，我们观察到相较于
    TensorRT-LLM 的 KV8 内核，在 A100 上实现了 1.5$\times$ 的加速。
- en: VI Evaluation
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 评估
- en: VI-A Evaluation Setup
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-A 评估设置
- en: Algorithm
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 算法
- en: 'The QoQ quantization algorithm is implemented using HuggingFace [[37](#bib.bib37)]
    on top of PyTorch [[26](#bib.bib26)]. We use per-channel symmetric INT8 quantization
    on activations, and per-token asymmetric INT4 group quantization on KV cache.
    “W4A8KV4 g128” refers to the case where QServe used progressive group quantization
    on weights: per-channel symmetric INT8 quantization followed by asymmetric INT4
    quantization with a group size of 128, while “W4A8KV4” is the per-channel counterpart
    for weight quantization.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: QoQ 量化算法是基于 PyTorch [[26](#bib.bib26)] 上的 HuggingFace [[37](#bib.bib37)] 实现的。我们对激活函数使用每通道对称的
    INT8 量化，对 KV 缓存使用每标记不对称的 INT4 组量化。“W4A8KV4 g128” 指的是 QServe 在权重上使用渐进组量化的情况：每通道对称
    INT8 量化后跟随具有 128 组大小的不对称 INT4 量化，而“W4A8KV4”是权重量化的每通道对应项。
- en: System
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 系统
- en: QServe serving system is implemented using CUDA and PTX assembly for high-performance
    GPU kernels. We also provide a purely PyTorch-based front-end framework for better
    flexibility. For the throughput benchmarking, we perform all experiments under
    PyTorch 2.2.0 with CUDA 12.2, unless otherwise specified. The throughput numbers
    reported are real measurements on NVIDIA GPUs. For baseline systems, we use TensorRT-LLM
    v0.9.0 and latest main branches from QuaRot and Atom as of April 18^(th), 2024\.
    Paged attention is enabled for all systems except QuaRot, which does not offer
    corresponding support.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: QServe服务系统使用CUDA和PTX汇编实现，以提供高性能的GPU内核。我们还提供了一个纯PyTorch基础的前端框架，以提高灵活性。在吞吐量基准测试中，除非另有说明，否则我们在PyTorch
    2.2.0和CUDA 12.2下执行所有实验。报告的吞吐量数据是基于NVIDIA GPU的真实测量。对于基准系统，我们使用了TensorRT-LLM v0.9.0和截至2024年4月18日的QuaRot和Atom最新主分支。除QuaRot外，所有系统均启用了分页注意力功能。
- en: VI-B Accuracy Evaluation
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-B 精度评估
- en: Benchmarks
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准测试
- en: We evaluated QoQ on the Llama-1 [[33](#bib.bib33)], Llama-2 [[34](#bib.bib34)],
    Llama-3 families, Mistral-7B [[17](#bib.bib17)], Mixtral-8x7B [[18](#bib.bib18)]
    and Yi-34B [[39](#bib.bib39)] models. Following previous literature [[8](#bib.bib8),
    [12](#bib.bib12), [44](#bib.bib44), [2](#bib.bib2), [38](#bib.bib38), [23](#bib.bib23)],
    we evaluated QoQ-quantized models on language modeling and zero-shot tasks. Specifically,
    we evaluated on WikiText2 [[24](#bib.bib24)] for perplexity, and evaluated on
    PIQA [[3](#bib.bib3)] (PQ), ARC [[5](#bib.bib5)], HellaSwag [[42](#bib.bib42)]
    (HS) and WinoGrande [[29](#bib.bib29)] (WG) with lm_eval [[13](#bib.bib13)].
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们评估了Llama-1 [[33](#bib.bib33)]、Llama-2 [[34](#bib.bib34)]、Llama-3系列、Mistral-7B [[17](#bib.bib17)]、Mixtral-8x7B [[18](#bib.bib18)]
    和Yi-34B [[39](#bib.bib39)]模型的QoQ。根据以往文献 [[8](#bib.bib8), [12](#bib.bib12), [44](#bib.bib44),
    [2](#bib.bib2), [38](#bib.bib38), [23](#bib.bib23)]，我们对语言建模和零样本任务中的QoQ量化模型进行了评估。具体来说，我们在WikiText2 [[24](#bib.bib24)]上评估了困惑度，在PIQA [[3](#bib.bib3)]
    (PQ)、ARC [[5](#bib.bib5)]、HellaSwag [[42](#bib.bib42)] (HS) 和WinoGrande [[29](#bib.bib29)]
    (WG)上使用lm_eval [[13](#bib.bib13)]进行了评估。
- en: Baselines
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准线
- en: We compared QoQ to widely used post-training LLM quantization techiniques, SmoothQuant [[38](#bib.bib38)],
    GPTQ [[12](#bib.bib12)], AWQ [[23](#bib.bib23)], and recently released state-of-the-art
    4-bit weight-activation quantization frameworks, Atom [[44](#bib.bib44)] and QuaRot [[2](#bib.bib2)].
    For SmoothQuant, we uses static per-tensor symmetric 8-bit quantization for KV
    cache following the settings in the TensorRT-LLM [[25](#bib.bib25)]. For GPTQ,
    we use their latest version with “reorder” trick, denoted as “GPTQ-R”. For QuaRot
    and Atom, we mainly evaluated using Pile validation dataset as calibration dataset.
    We also report their results with WikiText2 as calibration dataset in gray color.
    For “W4A8KV4 g128” setting, both QuaRot and Atom does not support progressive
    group quantization, and thus we evaluated them using ordinary group weight quantization
    (*i.e*., each group has one FP16 scale factor). Unsupported models and quantization
    settings will be reported as NaN.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将QoQ与广泛使用的后训练LLM量化技术进行了比较，包括SmoothQuant [[38](#bib.bib38)]、GPTQ [[12](#bib.bib12)]、AWQ [[23](#bib.bib23)]，以及最近发布的最先进的4-bit权重激活量化框架，Atom [[44](#bib.bib44)]
    和QuaRot [[2](#bib.bib2)]。对于SmoothQuant，我们使用了TensorRT-LLM [[25](#bib.bib25)]中的KV缓存静态每张量对称8-bit量化设置。对于GPTQ，我们使用了其最新版本并应用了“reorder”技巧，标记为“GPTQ-R”。对于QuaRot和Atom，我们主要使用Pile验证数据集进行评估。我们还报告了使用WikiText2作为校准数据集的结果（以灰色表示）。对于“W4A8KV4
    g128”设置，QuaRot和Atom不支持渐进式组量化，因此我们使用了普通组权重量化（*即*，每组有一个FP16尺度因子）进行评估。不支持的模型和量化设置将报告为NaN。
- en: 'TABLE II: WikiText2 perplexity with 2048 sequence length. The lower is the
    better.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：WikiText2困惑度，序列长度为2048。数值越低越好。
- en: '| WikiText2 Perplexity ↓ | Llama-3 | Llama-2 | Llama | Mistral | Mixtral |
    Yi |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2困惑度 ↓ | Llama-3 | Llama-2 | Llama | Mistral | Mixtral | Yi |'
- en: '| Precision | Algorithm | 8B | 7B | 13B | 70B | 7B | 13B | 30B | 7B | 8x7B
    | 34B |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 精度 | 算法 | 8B | 7B | 13B | 70B | 7B | 13B | 30B | 7B | 8x7B | 34B |'
- en: '| FP16 | - | 6.14 | 5.47 | 4.88 | 3.32 | 5.68 | 5.09 | 4.10 | 5.25 | 3.84 |
    4.60 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 6.14 | 5.47 | 4.88 | 3.32 | 5.68 | 5.09 | 4.10 | 5.25 | 3.84 |
    4.60 |'
- en: '| W8A8 | SmoothQuant | 6.28 | 5.54 | 4.95 | 3.36 | 5.73 | 5.13 | 4.23 | 5.29
    | 3.89 | 4.69 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | SmoothQuant | 6.28 | 5.54 | 4.95 | 3.36 | 5.73 | 5.13 | 4.23 | 5.29
    | 3.89 | 4.69 |'
- en: '| W4A16 g128 | GPTQ-R | 6.56 | 5.63 | 4.99 | 3.43 | 5.83 | 5.20 | 4.22 | 5.39
    | 4.08 | 4.68 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 g128 | GPTQ-R | 6.56 | 5.63 | 4.99 | 3.43 | 5.83 | 5.20 | 4.22 | 5.39
    | 4.08 | 4.68 |'
- en: '| AWQ | 6.54 | 5.60 | 4.97 | 3.41 | 5.78 | 5.19 | 4.21 | 5.37 | 4.02 | 4.67
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.54 | 5.60 | 4.97 | 3.41 | 5.78 | 5.19 | 4.21 | 5.37 | 4.02 | 4.67
    |'
- en: '| W4A4 | QuaRot | 8.20 | 6.10 | 5.40 | 3.79 | 6.26 | 5.55 | 4.60 | 5.71 | NaN
    | NaN |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | QuaRot | 8.20 | 6.10 | 5.40 | 3.79 | 6.26 | 5.55 | 4.60 | 5.71 | NaN
    | NaN |'
- en: '| 8.33 | 6.19 | 5.45 | 3.83 | 6.34 | 5.58 | 4.64 | 5.77 | NaN | NaN |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 8.33 | 6.19 | 5.45 | 3.83 | 6.34 | 5.58 | 4.64 | 5.77 | NaN | NaN |'
- en: '| W4A4 g128 | QuaRot$\dagger$ | 7.32 | 5.93 | 5.26 | 3.61 | 6.06 | 5.40 | 4.44
    | 5.54 | NaN | NaN |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 g128 | QuaRot$\dagger$ | 7.32 | 5.93 | 5.26 | 3.61 | 6.06 | 5.40 | 4.44
    | 5.54 | NaN | NaN |'
- en: '| 7.51 | 6.00 | 5.31 | 3.64 | 6.13 | 5.43 | 4.48 | 5.58 | NaN | NaN |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 7.51 | 6.00 | 5.31 | 3.64 | 6.13 | 5.43 | 4.48 | 5.58 | NaN | NaN |'
- en: '| Atom$\dagger$ | 7.57 | 6.03 | 5.27 | 3.69 | 6.16 | 5.46 | 4.55 | 5.66 | 4.42
    | 4.92 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| Atom$\dagger$ | 7.57 | 6.03 | 5.27 | 3.69 | 6.16 | 5.46 | 4.55 | 5.66 | 4.42
    | 4.92 |'
- en: '| 7.76 | 6.12 | 5.31 | 3.73 | 6.25 | 5.52 | 4.61 | 5.76 | 4.48 | 4.97 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 7.76 | 6.12 | 5.31 | 3.73 | 6.25 | 5.52 | 4.61 | 5.76 | 4.48 | 4.97 |'
- en: '| W4A8KV4 | RTN | 9.50 | 6.51 | 5.40 | 3.90 | 6.51 | 5.71 | 4.91 | 6.18 | 5.02
    | 6.52 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| W4A8KV4 | RTN | 9.50 | 6.51 | 5.40 | 3.90 | 6.51 | 5.71 | 4.91 | 6.18 | 5.02
    | 6.52 |'
- en: '| AWQ | 7.90 | 6.28 | 5.25 | 3.68 | 6.33 | 5.59 | 4.61 | 5.92 | 4.58 | 5.26
    |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 7.90 | 6.28 | 5.25 | 3.68 | 6.33 | 5.59 | 4.61 | 5.92 | 4.58 | 5.26
    |'
- en: '| Quarot | 6.75 | 5.73 | 5.07 | 3.46 | 5.93 | 5.29 | 4.32 | 5.41 | NaN | NaN
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Quarot | 6.75 | 5.73 | 5.07 | 3.46 | 5.93 | 5.29 | 4.32 | 5.41 | NaN | NaN
    |'
- en: '| Atom | 7.37 | 5.91 | 5.16 | 3.60 | 6.03 | 5.41 | 4.49 | 5.55 | NaN | 4.84
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Atom | 7.37 | 5.91 | 5.16 | 3.60 | 6.03 | 5.41 | 4.49 | 5.55 | NaN | 4.84
    |'
- en: '| QoQ | 6.89 | 5.75 | 5.12 | 3.52 | 5.93 | 5.28 | 4.34 | 5.45 | 4.18 | 4.74
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| QoQ | 6.89 | 5.75 | 5.12 | 3.52 | 5.93 | 5.28 | 4.34 | 5.45 | 4.18 | 4.74
    |'
- en: '| W4A8KV4 g128 | RTN | 7.25 | 5.99 | 5.19 | 3.70 | 6.23 | 5.46 | 4.56 | 5.59
    | 4.39 | 5.49 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| W4A8KV4 g128 | RTN | 7.25 | 5.99 | 5.19 | 3.70 | 6.23 | 5.46 | 4.56 | 5.59
    | 4.39 | 5.49 |'
- en: '| AWQ | 6.94 | 5.83 | 5.12 | 3.51 | 5.93 | 5.36 | 4.39 | 5.50 | 4.23 | 4.78
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.94 | 5.83 | 5.12 | 3.51 | 5.93 | 5.36 | 4.39 | 5.50 | 4.23 | 4.78
    |'
- en: '| Quarot$\ddagger$ | 6.68 | 5.71 | 5.06 | 3.45 | 5.91 | 5.26 | 4.30 | 5.39
    | NaN | NaN |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Quarot$\ddagger$ | 6.68 | 5.71 | 5.06 | 3.45 | 5.91 | 5.26 | 4.30 | 5.39
    | NaN | NaN |'
- en: '| Atom$\ddagger$ | 7.04 | 5.80 | 5.10 | 3.53 | 5.95 | 5.36 | 4.41 | 5.47 |
    4.22 | 4.75 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Atom$\ddagger$ | 7.04 | 5.80 | 5.10 | 3.53 | 5.95 | 5.36 | 4.41 | 5.47 |
    4.22 | 4.75 |'
- en: '| QoQ | 6.76 | 5.70 | 5.08 | 3.47 | 5.89 | 5.25 | 4.28 | 5.42 | 4.14 | 4.76
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| QoQ | 6.76 | 5.70 | 5.08 | 3.47 | 5.89 | 5.25 | 4.28 | 5.42 | 4.14 | 4.76
    |'
- en: '| * Grayed results use Wikitext2 as calibaration dataset. |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| * 灰色结果使用 Wikitext2 作为标定数据集。 |  |'
- en: '| $\dagger$ QuaRot and Atom apply group quantization to activations as well.
    |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| $\dagger$ QuaRot 和 Atom 同样对激活应用群体量化。 |  |'
- en: '| $\ddagger$ QuaRot and Atom use ordinary group quantization where each group
    has one FP16 scale factor. |  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| $\ddagger$ QuaRot 和 Atom 使用普通的群体量化，每个组有一个 FP16 缩放因子。 |  |'
- en: 'TABLE III: Zero-shot accuracy on five common sense tasks with 2048 sequence
    length.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 在五个常识任务上的零样本准确率，序列长度为 2048。'
- en: '| Llama-2 | Precision | Method | Zero-shot Accuracy ↑ |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 精度 | 方法 | 零样本准确率 ↑ |'
- en: '| PQ | ARC-e | ARC-c | HS | WG | Avg. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| PQ | ARC-e | ARC-c | HS | WG | 平均 |'
- en: '|  | FP16 | - | 79.05 | 74.58 | 46.25 | 76.05 | 68.98 | 68.98 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 79.05 | 74.58 | 46.25 | 76.05 | 68.98 | 68.98 |'
- en: '|  | W4A4 | Quarot | 76.77 | 69.87 | 40.87 | 72.16 | 63.77 | 64.69 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A4 | Quarot | 76.77 | 69.87 | 40.87 | 72.16 | 63.77 | 64.69 |'
- en: '| 7B | W4A4 g128 | Atom | 75.14 | 52.99 | 38.40 | 69.37 | 62.75 | 59.73 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 7B | W4A4 g128 | Atom | 75.14 | 52.99 | 38.40 | 69.37 | 62.75 | 59.73 |'
- en: '|  | W4A8KV4 | QoQ | 77.64 | 72.81 | 43.60 | 74.00 | 68.03 | 67.22 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 | QoQ | 77.64 | 72.81 | 43.60 | 74.00 | 68.03 | 67.22 |'
- en: '|  | W4A8KV4 g128 | QoQ | 78.07 | 73.32 | 44.80 | 74.98 | 68.59 | 67.95 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 g128 | QoQ | 78.07 | 73.32 | 44.80 | 74.98 | 68.59 | 67.95 |'
- en: '|  | FP16 | - | 80.52 | 77.44 | 49.06 | 79.38 | 72.22 | 71.72 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 80.52 | 77.44 | 49.06 | 79.38 | 72.22 | 71.72 |'
- en: '|  | W4A4 | Quarot | 78.89 | 72.98 | 46.59 | 76.37 | 70.24 | 69.01 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A4 | Quarot | 78.89 | 72.98 | 46.59 | 76.37 | 70.24 | 69.01 |'
- en: '| 13B | W4A4 g128 | Atom | 76.50 | 57.49 | 42.32 | 73.84 | 67.40 | 63.51 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 13B | W4A4 g128 | Atom | 76.50 | 57.49 | 42.32 | 73.84 | 67.40 | 63.51 |'
- en: '|  | W4A8KV4 | QoQ | 79.71 | 75.97 | 48.38 | 77.80 | 70.96 | 70.56 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 | QoQ | 79.71 | 75.97 | 48.38 | 77.80 | 70.96 | 70.56 |'
- en: '|  | W4A8KV4 g128 | QoQ | 79.43 | 77.06 | 48.81 | 78.35 | 70.48 | 70.83 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 g128 | QoQ | 79.43 | 77.06 | 48.81 | 78.35 | 70.48 | 70.83 |'
- en: '|  | FP16 | - | 82.70 | 81.02 | 57.34 | 83.82 | 77.98 | 76.57 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | - | 82.70 | 81.02 | 57.34 | 83.82 | 77.98 | 76.57 |'
- en: '|  | W4A4 | Quarot | 82.43 | 80.43 | 56.23 | 81.82 | 76.24 | 75.43 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A4 | Quarot | 82.43 | 80.43 | 56.23 | 81.82 | 76.24 | 75.43 |'
- en: '| 70B | W4A4 g128 | Atom | 79.92 | 58.25 | 46.08 | 79.06 | 74.27 | 67.52 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 70B | W4A4 g128 | Atom | 79.92 | 58.25 | 46.08 | 79.06 | 74.27 | 67.52 |'
- en: '|  | W4A8KV4 | QoQ | 82.64 | 79.80 | 56.83 | 82.78 | 77.51 | 75.91 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 | QoQ | 82.64 | 79.80 | 56.83 | 82.78 | 77.51 | 75.91 |'
- en: '|  | W4A8KV4 g128 | QoQ | 82.92 | 80.93 | 56.40 | 83.28 | 78.45 | 76.40 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | W4A8KV4 g128 | QoQ | 82.92 | 80.93 | 56.40 | 83.28 | 78.45 | 76.40 |'
- en: '| * For reference, using MX-FP4 for W4A4 quantizing Llama-7B model will decrease
    the |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| * 作为参考，使用 MX-FP4 对 W4A4 量化 Llama-7B 模型将会降低 |'
- en: '| accuracy from 72.9 to 63.7 on ARC easy and from 44.7 to 35.5 on ARC challenge
    task. [[28](#bib.bib28)] |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 在 ARC 易任务上准确率从 72.9 降至 63.7，在 ARC 挑战任务上从 44.7 降至 35.5。 [[28](#bib.bib28)]
    |'
- en: WikiText2 perplexity
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: WikiText2 困惑度
- en: 'Table [II](#S6.T2 "TABLE II ‣ Baselines ‣ VI-B Accuracy Evaluation ‣ VI Evaluation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")
    compares the Wikitext2 perplexity results between QoQ and other baselines. For
    Llama-2-7B, compared to W8A8 SmoothQuant and W4A16 AWQ, QoQ only increased perplexity
    by up to 0.16 QoQ consistently outperformed Atom with either W4A4 or W4A8KV4 quantization
    precision. QoQ also showed up to 0.49 perplexity improvement compared to W4A4
    Quarot.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '表[II](#S6.T2 "TABLE II ‣ Baselines ‣ VI-B Accuracy Evaluation ‣ VI Evaluation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")
    比较了 QoQ 与其他基线之间的 Wikitext2 困惑度结果。对于 Llama-2-7B，相较于 W8A8 SmoothQuant 和 W4A16 AWQ，QoQ
    的困惑度仅增加了最多 0.16。QoQ 在 W4A4 或 W4A8KV4 量化精度下始终优于 Atom。与 W4A4 Quarot 相比，QoQ 还显示出最高
    0.49 的困惑度改进。'
- en: Zero-shot accuracy
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 零-shot准确性
- en: 'we report the zero-shot accuracy of five common sense tasks in Table [III](#S6.T3
    "TABLE III ‣ Baselines ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving"). QoQ significantly
    outperformed other 4-bit quantization methods. Especially on the Winogrande task,
    compared to Quarot, QoQ accuracy is 4.82% higher. Compared to FP16, QoQ only introduced
    1.03%, 0.89% and 0.40% accuracy loss for Llama-2 at 7B, 13B and 70B size.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[III](#S6.T3 "TABLE III ‣ Baselines ‣ VI-B Accuracy Evaluation ‣ VI Evaluation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving")
    中报告了五个常识任务的零-shot 准确性。QoQ 显著优于其他4位量化方法。特别是在 Winogrande 任务上，相较于 Quarot，QoQ 准确性提高了
    4.82%。与 FP16 相比，QoQ 对于 7B、13B 和 70B 大小的 Llama-2 仅引入了 1.03%、0.89% 和 0.40% 的准确性损失。'
- en: '![Refer to caption](img/e8680b4ba066a9adc009b45b6f478787.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e8680b4ba066a9adc009b45b6f478787.png)'
- en: 'Figure 15: QServe significantly outperforms existing large language model (LLM)
    serving frameworks in batched generation tasks across different LLMs, ranging
    from 7B to 72B models. It achieves an average speedup of 2.36$\times$ faster on
    the A100 GPU. All experiments were conducted under the same device memory budget
    (*i.e*. 80GB on A100 and 48GB on L40S). We omit the geometric mean speedup of
    Atom since it only supports Llama2-7B. For absolute values, see Table [IV](#S6.T4
    "TABLE IV ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe:
    W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '图15：QServe 在不同的大型语言模型（LLM）中，显著超越了现有的大型语言模型（LLM）服务框架，在批量生成任务中，模型范围从7B到72B。它在
    A100 GPU 上实现了平均加速2.36$\times$。所有实验都在相同的设备内存预算下进行（*即* A100上的80GB和L40S上的48GB）。由于
    Atom 仅支持 Llama2-7B，我们省略了其几何均值加速。有关绝对值，请参见表[IV](#S6.T4 "TABLE IV ‣ Zero-shot accuracy
    ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and
    System Co-design for Efficient LLM Serving")。'
- en: 'TABLE IV: The absolute token generation throughput of QServe and TensorRT-LLM
    in Fig. [15](#S6.F15 "Figure 15 ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation
    ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"). ^*: we calculate the speedup over highest achieveable throughput
    from TensorRT-LLM across all three precision configurations. Our QServe system
    achieves competitive throughput on L40S GPU compared to TensorRT-LLM on A100,
    effectively reducing the dollar cost of LLM serving by 3$\times$. Unit: tokens/second.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV：图 [15](#S6.F15 "Figure 15 ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation
    ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving") 中 QServe 和 TensorRT-LLM 的绝对 token 生成吞吐量。^*：我们计算了在所有三种精度配置下，TensorRT-LLM
    达到的最高吞吐量的加速。我们的 QServe 系统在 L40S GPU 上的吞吐量与 A100 上的 TensorRT-LLM 相比具有竞争力，有效降低了
    LLM 服务的成本 3$\times$。单位：tokens/second。'
- en: '| Device | System | Llama-3 | Llama-2 | Mistral | LLama-2 | LLaMA | Yi | Llama-2
    | Qwen1.5 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 设备 | 系统 | Llama-3 | Llama-2 | Mistral | LLama-2 | LLaMA | Yi | Llama-2 |
    Qwen1.5 |'
- en: '| 8B | 7B | 7B | 13B | 30B | 34B | 70B | 72B |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 8B | 7B | 7B | 13B | 30B | 34B | 70B | 72B |'
- en: '|  | TRT-LLM-FP16 | 1326 | 444 | 1566 | 92 | OOM | OOM | OOM | OOM |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | TRT-LLM-FP16 | 1326 | 444 | 1566 | 92 | OOM | OOM | OOM | OOM |'
- en: '|  | TRT-LLM-W4A16 | 1431 | 681 | 1457 | 368 | 148 | 313 | 119 | 17 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | TRT-LLM-W4A16 | 1431 | 681 | 1457 | 368 | 148 | 313 | 119 | 17 |'
- en: '| L40S | TRT-LLM-W8A8 | 2634 | 1271 | 2569 | 440 | 123 | 364 | OOM | OOM |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| L40S | TRT-LLM-W8A8 | 2634 | 1271 | 2569 | 440 | 123 | 364 | OOM | OOM |'
- en: '|  | QServe (Ours) | 3656 | 2394 | 3774 | 1327 | 504 | 869 | 286 | 59 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | QServe（我们的） | 3656 | 2394 | 3774 | 1327 | 504 | 869 | 286 | 59 |'
- en: '|  | Speedup^* | 1.39$\times$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | Speedup^* | 1.39$\times$ |'
- en: '|  | TRT-LLM-FP16 | 2503 | 1549 | 2371 | 488 | 80 | 145 | OOM | OOM |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | TRT-LLM-FP16 | 2503 | 1549 | 2371 | 488 | 80 | 145 | OOM | OOM |'
- en: '|  | TRT-LLM-W4A16 | 2370 | 1549 | 2403 | 871 | 352 | 569 | 358 | 143 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | TRT-LLM-W4A16 | 2370 | 1549 | 2403 | 871 | 352 | 569 | 358 | 143 |'
- en: '| A100 | TRT-LLM-W8A8 | 2396 | 2334 | 2427 | 1277 | 361 | 649 | 234 | 53 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| A100 | TRT-LLM-W8A8 | 2396 | 2334 | 2427 | 1277 | 361 | 649 | 234 | 53 |'
- en: '|  | QServe (Ours) | 3005 | 2908 | 2970 | 1741 | 749 | 797 | 419 | 340 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  | QServe（我们的） | 3005 | 2908 | 2970 | 1741 | 749 | 797 | 419 | 340 |'
- en: '|  | Speedup^* | 1.20$\times$ |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | 加速^* | 1.20$\times$ |'
- en: VI-C Efficiency Evaluation
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-C 效率评估
- en: We assessed the efficiency of QServe on A100-80G-SXM4 and L40S-48G GPUs by comparing
    it against TensorRT-LLM (using FP16, W8A8, and W4A16 precisions), Atom (W4A4),
    and QuaRot (W4A4). The primary metric for system evaluation is the maximum achievable
    throughput within the same memory constraints, where we use an input sequence
    length of 1024 and output sequence length of 512\. We notice that Atom only supports
    Llama-2-7B, and QuaRot does not support GQA. Therefore, we skip these unsupported
    models when measuring the performance of baseline systems.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将 QServe 与 TensorRT-LLM（使用 FP16、W8A8 和 W4A16 精度）、Atom（W4A4）和 QuaRot（W4A4）进行比较，评估了其在
    A100-80G-SXM4 和 L40S-48G GPU 上的效率。系统评估的主要指标是在相同内存限制下的最大可实现吞吐量，我们使用了 1024 的输入序列长度和
    512 的输出序列长度。我们注意到 Atom 仅支持 Llama-2-7B，而 QuaRot 不支持 GQA。因此，在测量基线系统的性能时，我们跳过了这些不支持的模型。
- en: 'We present relative performance comparisons in Figure [15](#S6.F15 "Figure
    15 ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving") and absolute throughput
    values in Table [IV](#S6.T4 "TABLE IV ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation
    ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"). We use per-channel quantization for A100 and per-group quantization
    for L40S. This is because L40S has stronger CUDA cores for dequantization. Relative
    to the best-performing configuration of TensorRT-LLM, QServe demonstrates significant
    improvements on A100: it achieves 2$\times$ across all seven models evaluated.
    Remarkably, despite the L40S’s significantly smaller memory capacity compared
    to the A100, QServe effectively maintains the same batch size as TensorRT-LLM
    on the A100\. This achievement is attributed to our aggressive 4-bit quantization
    applied to both weights and the KV cache. By examining Table [IV](#S6.T4 "TABLE
    IV ‣ Zero-shot accuracy ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4
    Quantization and System Co-design for Efficient LLM Serving"), we clearly observe
    that serving five of seven models under 34B on L40S with QServe achieves even
    higher throughput than serving them on A100 using TensorRT-LLM. Our performance
    gain over Atom and QuaRot on A100 is even more prominent since these systems did
    not outperform TensorRT-LLM. On L40S, QServe still achieves 10% higher throughput
    than Atom when running Llama-2-7B, the only model supported by their system despite
    the fact that we use higher quantization precision. Besides, the accuracy achieved
    by QServe is much better than Atom, as indicated in Table [III](#S6.T3 "TABLE
    III ‣ Baselines ‣ VI-B Accuracy Evaluation ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving").'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [15](#S6.F15 "图 15 ‣ 零-shot 准确率 ‣ VI-B 准确率评估 ‣ VI 评估 ‣ QServe: W4A8KV4
    量化与系统共同设计以实现高效的 LLM 服务") 和表 [IV](#S6.T4 "表 IV ‣ 零-shot 准确率 ‣ VI-B 准确率评估 ‣ VI 评估
    ‣ QServe: W4A8KV4 量化与系统共同设计以实现高效的 LLM 服务") 中展示了相对性能比较和绝对吞吐量值。我们对 A100 使用了每通道量化，对
    L40S 使用了每组量化。这是因为 L40S 的 CUDA 核心对去量化的支持更强。与 TensorRT-LLM 的最佳配置相比，QServe 在 A100
    上显示了显著的改进：它在所有七个评估模型中都实现了 2$\times$ 的提升。值得注意的是，尽管 L40S 的内存容量远小于 A100，但 QServe
    仍然有效地保持了与 A100 上 TensorRT-LLM 相同的批量大小。这一成就归因于我们对权重和 KV 缓存应用的激进 4 位量化。通过查看表 [IV](#S6.T4
    "表 IV ‣ 零-shot 准确率 ‣ VI-B 准确率评估 ‣ VI 评估 ‣ QServe: W4A8KV4 量化与系统共同设计以实现高效的 LLM
    服务")，我们可以清楚地看到，在 L40S 上使用 QServe 对七个模型中的五个进行服务时，其吞吐量甚至高于在 A100 上使用 TensorRT-LLM
    的吞吐量。我们在 A100 上相对于 Atom 和 QuaRot 的性能提升更为显著，因为这些系统没有超越 TensorRT-LLM。在 L40S 上，当运行
    Llama-2-7B 时，QServe 的吞吐量比 Atom 高出 10%，尽管我们使用了更高的量化精度。此外，如表 [III](#S6.T3 "表 III
    ‣ 基线 ‣ VI-B 准确率评估 ‣ VI 评估 ‣ QServe: W4A8KV4 量化与系统共同设计以实现高效的 LLM 服务") 所示，QServe
    达到的准确率远高于 Atom。'
- en: VI-D Analysis and Discussion.
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VI-D 分析与讨论。
- en: '![Refer to caption](img/a37aa31bc465e08d1f118827af179725.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a37aa31bc465e08d1f118827af179725.png)'
- en: 'Figure 16: Ablation study on quantization techniques used in QoQ and the impact
    of serving throughput, GPU memory consumption in QServe. The model used here is
    Llama-2-7B.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：QoQ 中使用的量化技术的消融研究以及其对 QServe 的服务吞吐量和 GPU 内存消耗的影响。此处使用的模型是 Llama-2-7B。
- en: '![Refer to caption](img/683dfcbc9c2a4102d58f97b74a271d6a.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/683dfcbc9c2a4102d58f97b74a271d6a.png)'
- en: 'Figure 17: Same-batch throughput comparison between QServe and baseline systems
    on L40S. We use an input sequence length of 1024 and output sequence length of
    512.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：QServe与基线系统在L40S上的相同批次吞吐量比较。我们使用了1024的输入序列长度和512的输出序列长度。
- en: Ablation study on quantization techniques
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化技术的消融研究
- en: 'we examine the impact on accuracy of various quantization techniques implemented
    in QoQ. Our analysis begins with round-to-nearest (RTN) W8A8 quantization on Llama-2-7B
    (per-channel + per-token). We then lower the quantization precision and apply
    different techniques step-by-step. For each step, we evaluated the WikiText2 perplexity
    and end-to-end inference performance on L40S with 64 requests of 1024 input tokens
    and 512 output tokens. The results are detailed in Figure [16](#S6.F16 "Figure
    16 ‣ VI-D Analysis and Discussion. ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization
    and System Co-design for Efficient LLM Serving"). We see that reducing the weight
    precision to 4 bits significantly impaired the model performance, though it increased
    end-to-end processing speed by 1.12$\times$ and halved GPU memory usage. To solve
    this problem, SmoothAttention reduced perplexity by 0.05, without adding system
    overhead. Progressive group quantization further improved perplexity by an additional
    0.02, with only a negligible increase in dequantization overhead. Lastly, activation-aware
    channel reordering enhanced perplexity by 0.03.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了在QoQ中实施的各种量化技术对准确度的影响。我们的分析从Llama-2-7B（按通道+按标记）上的四舍五入（RTN）W8A8量化开始。然后我们逐步降低量化精度，并应用不同的技术。对于每一步，我们评估了WikiText2困惑度和L40S上的端到端推理性能，L40S使用64个请求，每个请求包含1024个输入标记和512个输出标记。结果详见图 [16](#S6.F16
    "图16 ‣ VI-D 分析与讨论 ‣ VI 评估 ‣ QServe：W4A8KV4量化与系统协同设计用于高效LLM服务")。我们发现，将权重精度降低到4位显著影响了模型性能，尽管它将端到端处理速度提高了1.12$\times$，并将GPU内存使用量减少了一半。为了解决这个问题，SmoothAttention将困惑度降低了0.05，而没有增加系统开销。渐进式组量化进一步将困惑度提高了额外的0.02，去量化开销仅有微不足道的增加。最后，激活感知通道重排序将困惑度提升了0.03。
- en: '![Refer to caption](img/d300605754b39c37d981e2b8537ae1f4.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d300605754b39c37d981e2b8537ae1f4.png)'
- en: 'Figure 18: The dequantization overhead in QServe is much smaller than that
    in Atom-W4A4 (up to 90%).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：QServe中的去量化开销比Atom-W4A4小得多（最高减少90%）。
- en: Ablation study on QServe system
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: QServe系统的消融研究
- en: 'Dequantization overhead: We measure the dequantization overhead of per-group
    QServe-W4A8 GEMM and other baselines in Figure [18](#S6.F18 "Figure 18 ‣ Ablation
    study on quantization techniques ‣ VI-D Analysis and Discussion. ‣ VI Evaluation
    ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving").
    Our dequantization overhead is comparable with TRT-LLM-W4A16, but since we perform
    computation on INT8 tensor cores, we enjoy 2$\times$ higher throughput.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 去量化开销：我们在图 [18](#S6.F18 "图18 ‣ 量化技术的消融研究 ‣ VI-D 分析与讨论 ‣ VI 评估 ‣ QServe：W4A8KV4量化与系统协同设计用于高效LLM服务")
    中测量了每组QServe-W4A8 GEMM和其他基线的去量化开销。我们的去量化开销与TRT-LLM-W4A16相当，但由于我们在INT8张量核心上进行计算，因此享受了2$\times$更高的吞吐量。
- en: 'Comparisons under the same batches: We demonstrate speedup results under the
    same batch sizes in Figure [17](#S6.F17 "Figure 17 ‣ VI-D Analysis and Discussion.
    ‣ VI Evaluation ‣ QServe: W4A8KV4 Quantization and System Co-design for Efficient
    LLM Serving"). For Llama-2-7B, we show that the 1.88$\times$ improvement).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 相同批次下的比较：我们在图 [17](#S6.F17 "图17 ‣ VI-D 分析与讨论 ‣ VI 评估 ‣ QServe：W4A8KV4量化与系统协同设计用于高效LLM服务")
    中展示了在相同批次大小下的加速结果。对于Llama-2-7B，我们显示了1.88$\times$的改进。
- en: 'Improvement breakdown for KV4 attention: We detail the enhancements from attention
    optimizations in Section Section [V-C](#S5.SS3 "V-C KV4 Attention in QServe ‣
    V QServe Serving System ‣ QServe: W4A8KV4 Quantization and System Co-design for
    Efficient LLM Serving"). Starting with the basic KV4 implementation, which exhibits
    an A100 latency of 0.48ms for a 64$\times$.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: KV4注意力的改进细节：我们在 [V-C](#S5.SS3 "V-C QServe中的KV4注意力 ‣ V QServe服务系统 ‣ QServe：W4A8KV4量化与系统协同设计用于高效LLM服务")
    部分详细介绍了注意力优化带来的增强。从基本的KV4实现开始，该实现对于64$\times$具有0.48ms的A100延迟。
- en: VII Related Work
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 相关工作
- en: 'Quantization of LLMs. Quantization reduces the size of LLMs and speedup inference.
    There are two primary quantization strategies: (1) Weight-only quantization [[12](#bib.bib12),
    [23](#bib.bib23), [10](#bib.bib10), [19](#bib.bib19)] benefits edge devices where
    the workload is memory-bound, improving weight-loading speed. However, for cloud
    services with high user traffic and required batch processing, this method falls
    short as it does not accelerate computation in compute-bound scenarios. (2) Weight-activation
    quantization accelerates computation in batch processing by quantizing both weights
    and activations [[8](#bib.bib8), [36](#bib.bib36), [38](#bib.bib38)]. OmniQuant [[30](#bib.bib30)]
    and Atom [[44](#bib.bib44)] exploring more aggressive quantizations (W4A4, W4A8)
    and mixed precision to enhance model quality and efficiency, though these can
    impact model accuracy and reduce serving throughput. QuaRot [[2](#bib.bib2)] further
    refines W4A4 by rotating weights and activations at the cost of increased computational
    overhead due to additional transformations required during inference.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的量化。量化可以减少 LLM 的大小并加快推理速度。主要有两种量化策略：（1）仅权重量化 [[12](#bib.bib12), [23](#bib.bib23),
    [10](#bib.bib10), [19](#bib.bib19)]，适用于工作负载受限于内存的边缘设备，提高权重加载速度。然而，对于用户流量高且需要批处理的云服务，这种方法效果不佳，因为它不能在计算受限场景中加速计算。（2）权重-激活量化通过量化权重和激活值 [[8](#bib.bib8),
    [36](#bib.bib36), [38](#bib.bib38)] 来加速批处理中的计算。OmniQuant [[30](#bib.bib30)] 和
    Atom [[44](#bib.bib44)] 探索了更具侵略性的量化方法（W4A4, W4A8）和混合精度，以提高模型质量和效率，但这些方法可能会影响模型精度并降低服务吞吐量。QuaRot [[2](#bib.bib2)]
    通过在推理过程中增加额外的转换来旋转权重和激活值，从而进一步细化 W4A4，但代价是增加了计算开销。
- en: LLM serving systems. Numerous systems have been proposed for efficient LLM deployment.
    Orca [[40](#bib.bib40)] employs iteration-level scheduling and selective batching
    in distributed systems. vLLM [[22](#bib.bib22)] features virtual memory-inspired
    PagedAttention, optimizing KV cache management. SGLang [[45](#bib.bib45)] enhances
    LLM programming with advanced primitives and RadixAttention. LMDeploy [[7](#bib.bib7)]
    offers persistent batching and blocked KV cache features to improve deployment
    efficiency. LightLLM [[6](#bib.bib6)] manages GPU memory with token-wise KV cache
    control via Token Attention, increasing throughput. MLC-LLM [[32](#bib.bib32)]
    utilizes compiler acceleration for versatile LLM deployment across edge devices.
    TensorRT-LLM [[25](#bib.bib25)] is the leading industry solution and the most
    important baseline in this paper.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 服务系统。已提出了许多系统来实现高效的 LLM 部署。Orca [[40](#bib.bib40)] 在分布式系统中采用迭代级调度和选择性批处理。vLLM [[22](#bib.bib22)]
    具有虚拟内存启发的 PagedAttention，优化了 KV 缓存管理。SGLang [[45](#bib.bib45)] 通过先进的原语和 RadixAttention
    增强了 LLM 编程。LMDeploy [[7](#bib.bib7)] 提供了持久批处理和阻塞 KV 缓存功能，以提高部署效率。LightLLM [[6](#bib.bib6)]
    通过 Token Attention 以 token 为单位管理 GPU 内存，从而提高吞吐量。MLC-LLM [[32](#bib.bib32)] 利用编译器加速实现了多功能的
    LLM 部署，适用于边缘设备。TensorRT-LLM [[25](#bib.bib25)] 是行业领先解决方案，并且是本文中最重要的基准。
- en: LLM Accelerators. Transformers and LLMs have also generated considerable research
    interest in domain-specific accelerator design. Several works, such as $A^{3}$
    sparsity and specialized softmax module to reduce off-chip communication. Moreover,
    DFX [[16](#bib.bib16)] exploits model parallelism and optimized dataflow for low-latency
    generation. However, these accelerators have yet to be scaled up to recent LLMs
    with billions of parameters.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 加速器。变换器和 LLM 还在领域特定加速器设计中引发了大量研究兴趣。几项工作，例如 $A^{3}$ 稀疏性和专用 softmax 模块，以减少离芯片通信。此外，DFX [[16](#bib.bib16)]
    利用模型并行性和优化的数据流以实现低延迟生成。然而，这些加速器尚未扩展到具有数十亿参数的最新 LLM。
- en: VIII Conclusion
  id: totrans-252
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: We introduce QServe, an algorithm and system co-design framework tailored to
    quantize large language models (LLMs) to W4A8KV4 precision, facilitating their
    efficient deployment on GPUs. On the algorithmic front, we design the QoQ quantization
    method that features progressive quantization, enabling W4A8 GEMM operations to
    be executed on INT8 tensor cores, and SmoothAttention, which significantly reduces
    accuracy loss resulting from KV4 quantization. Correspondingly, in the QServe
    system, we leverage the protective range established in the first level of progressive
    quantization to enable INT4 to INT8 dequantization. This process utilizes full
    register-level parallelism and employs a subtraction-after-multiplication computation
    sequence. Additionally, we implement compute-aware weight reordering to minimize
    the overhead associated with pointer arithmetic. As a result, when serving seven
    representative LLMs on A100 and L40S GPUs, QServe achieves up to 2.4-3.5$\times$
    higher throughput over the industrial standard for LLM serving, TensorRT-LLM.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了 QServe，一个专门设计用于将大型语言模型（LLMs）量化到 W4A8KV4 精度的算法和系统协同设计框架，以便在 GPU 上高效部署。在算法方面，我们设计了
    QoQ 量化方法，具有渐进量化功能，支持在 INT8 张量核心上执行 W4A8 GEMM 操作，并设计了 SmoothAttention，大幅减少 KV4
    量化带来的精度损失。相应地，在 QServe 系统中，我们利用渐进量化第一级建立的保护范围，实现了 INT4 到 INT8 的去量化。此过程利用了完整的寄存器级并行性，并采用了乘法后的减法计算序列。此外，我们实现了计算感知的权重重新排序，以最小化与指针运算相关的开销。因此，在
    A100 和 L40S GPU 上服务七个代表性 LLMS 时，QServe 的吞吐量比工业标准 TensorRT-LLM 高出 2.4-3.5$\times$。
- en: Acknowledgements
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank MIT-IBM Watson AI Lab, MIT AI Hardware Program, MIT Amazon Science
    Hub, and NSF for supporting this research. We also thank Julien Demouth, June
    Yang, and Dongxu Yang from NVIDIA for their helpful discussions.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 MIT-IBM Watson AI Lab、MIT AI Hardware Program、MIT Amazon Science Hub 和
    NSF 对本研究的支持。我们还感谢 NVIDIA 的 Julien Demouth、June Yang 和 Dongxu Yang 提供的有益讨论。
- en: References
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, and S. Sanghai,
    “Gqa: Training generalized multi-query transformer models from multi-head checkpoints,”
    *arXiv preprint arXiv:2305.13245*, 2023.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebrón, 和 S. Sanghai，“Gqa:
    从多头检查点训练广义多查询变换器模型”，*arXiv 预印本 arXiv:2305.13245*，2023。'
- en: '[2] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, M. Jaggi, D. Alistarh,
    T. Hoefler, and J. Hensman, “Quarot: Outlier-free 4-bit inference in rotated llms,”
    *arXiv preprint arXiv:2404.00456*, 2024.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, M. Jaggi, D. Alistarh,
    T. Hoefler, 和 J. Hensman，“Quarot: 无异常值的 4 位推理在旋转 LLMS 中”，*arXiv 预印本 arXiv:2404.00456*，2024。'
- en: '[3] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, and Y. Choi, “Piqa: Reasoning
    about physical commonsense in natural language,” in *Thirty-Fourth AAAI Conference
    on Artificial Intelligence*, 2020.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Bisk, R. Zellers, R. L. Bras, J. Gao, 和 Y. Choi，“Piqa: 关于自然语言中的物理常识推理”，收录于*第三十四届
    AAAI 人工智能大会*，2020。'
- en: '[4] J. Chee, Y. Cai, V. Kuleshov, and C. D. Sa, “Quip: 2-bit quantization of
    large language models with guarantees,” 2024.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Chee, Y. Cai, V. Kuleshov, 和 C. D. Sa，“Quip: 大型语言模型的 2 位量化及其保障”，2024。'
- en: '[5] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and
    O. Tafjord, “Think you have solved question answering? try arc, the ai2 reasoning
    challenge,” 2018.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, 和
    O. Tafjord，“认为你已经解决了问答问题？尝试 arc，AI2 推理挑战”，2018。'
- en: '[6] L. Contributors, “Lightllm: A light and fast inference service for llm,”
    [https://github.com/ModelTC/lightllm](https://github.com/ModelTC/lightllm), 2023.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Contributors，“Lightllm: 一款轻量快速的 LLMS 推理服务”，[https://github.com/ModelTC/lightllm](https://github.com/ModelTC/lightllm)，2023。'
- en: '[7] L. Contributors, “Lmdeploy: A toolkit for compressing, deploying, and serving
    llm,” [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy),
    2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Contributors，“Lmdeploy: 一款用于压缩、部署和服务 LLMS 的工具包”，[https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy)，2023。'
- en: '[8] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, “GPT3.int8(): 8-bit
    matrix multiplication for transformers at scale,” in *Advances in Neural Information
    Processing Systems*, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] T. Dettmers, M. Lewis, Y. Belkada, 和 L. Zettlemoyer，“GPT3.int8(): 大规模变换器的
    8 位矩阵乘法”，收录于*神经信息处理系统进展*，A. H. Oh, A. Agarwal, D. Belgrave, 和 K. Cho 主编，2022。'
- en: '[9] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” *arXiv preprint arXiv:2305.14314*, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer，“Qlora: 量化 LLMS
    的高效微调”，*arXiv 预印本 arXiv:2305.14314*，2023。'
- en: '[10] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh, “Spqr: A sparse-quantized
    representation for near-lossless llm weight compression,” 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev, E. Frantar,
    S. Ashkboos, A. Borzunov, T. Hoefler, 和 D. Alistarh，“Spqr: 一种稀疏量化表示用于近乎无损的 LLM
    权重压缩，”2023年。'
- en: '[11] C. Fang, A. Zhou, and Z. Wang, “An algorithm–hardware co-optimized framework
    for accelerating n: M sparse transformers,” *IEEE Transactions on Very Large Scale
    Integration (VLSI) Systems*, vol. 30, no. 11, pp. 1573–1586, 2022.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] C. Fang, A. Zhou, 和 Z. Wang，“一个算法–硬件共同优化框架用于加速 n: M 稀疏变换器，” *IEEE 超大规模集成（VLSI）系统汇刊*，第30卷，第11期，第1573–1586页，2022年。'
- en: '[12] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, “GPTQ: Accurate
    post-training compression for generative pretrained transformers,” *arXiv preprint
    arXiv:2210.17323*, 2022.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. Frantar, S. Ashkboos, T. Hoefler, 和 D. Alistarh，“GPTQ: 生成预训练变换器的准确后训练压缩，”
    *arXiv 预印本 arXiv:2210.17323*，2022年。'
- en: '[13] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, and A. Zou, “A framework for few-shot language model evaluation,”
    12 2023\. [Online]. Available: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. Gao, J. Tow, B. Abbasi, S. Biderman, S. Black, A. DiPofi, C. Foster,
    L. Golding, J. Hsu, A. Le Noac’h, H. Li, K. McDonell, N. Muennighoff, C. Ociepa,
    J. Phang, L. Reynolds, H. Schoelkopf, A. Skowron, L. Sutawika, E. Tang, A. Thite,
    B. Wang, K. Wang, 和 A. Zou，“少样本语言模型评估框架，”2023年12月。[在线]. 可用: [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)'
- en: '[14] T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song, J.-H. Park,
    S. Lee, K. Park, J. W. Lee *et al.*, “A^ 3: Accelerating attention mechanisms
    in neural networks with approximation,” in *2020 IEEE International Symposium
    on High Performance Computer Architecture (HPCA)*.   IEEE, 2020, pp. 328–341.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] T. J. Ham, S. J. Jung, S. Kim, Y. H. Oh, Y. Park, Y. Song, J.-H. Park,
    S. Lee, K. Park, J. W. Lee *等*，“A^ 3: 用近似加速神经网络中的注意力机制，”在 *2020 IEEE国际高性能计算架构研讨会（HPCA）*。IEEE，2020年，第328–341页。'
- en: '[15] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, and J. W. Lee,
    “Elsa: Hardware-software co-design for efficient, lightweight self-attention mechanism
    in neural networks,” in *2021 ACM/IEEE 48th Annual International Symposium on
    Computer Architecture (ISCA)*.   IEEE, 2021, pp. 692–705.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jung, 和 J. W. Lee，“Elsa:
    神经网络中高效轻量自注意力机制的硬件-软件共同设计，”在 *2021 ACM/IEEE 第48届国际计算机架构年会（ISCA）*。IEEE，2021年，第692–705页。'
- en: '[16] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, and J.-Y. Kim, “Dfx:
    A low-latency multi-fpga appliance for accelerating transformer-based text generation,”
    in *2022 55th IEEE/ACM International Symposium on Microarchitecture (MICRO)*.   IEEE,
    2022, pp. 616–630.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, 和 J.-Y. Kim，“Dfx: 一种低延迟的多-FPGA设备用于加速基于变换器的文本生成，”在
    *2022年第55届IEEE/ACM国际微架构研讨会（MICRO）*。IEEE，2022年，第616–630页。'
- en: '[17] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier *et al.*, “Mistral 7b,”
    *arXiv preprint arXiv:2310.06825*, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier *等*，“Mistral 7b，”
    *arXiv 预印本 arXiv:2310.06825*，2023年。'
- en: '[18] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand *et al.*, “Mixtral of
    experts,” *arXiv preprint arXiv:2401.04088*, 2024.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford,
    D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand *等*，“Mixtral 专家模型，” *arXiv
    预印本 arXiv:2401.04088*，2024年。'
- en: '[19] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney,
    and K. Keutzer, “Squeezellm: Dense-and-sparse quantization,” 2024.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W. Mahoney,
    和 K. Keutzer，“Squeezellm: 密集与稀疏量化，”2024年。'
- en: '[20] Y. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla, “Who says elephants
    can’t run: Bringing large scale moe models into cloud scale production,” *arXiv
    preprint arXiv:2211.10017*, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. J. Kim, R. Henry, R. Fahim, 和 H. H. Awadalla，“谁说大象不能跑: 将大规模 moe 模型引入云规模生产，”
    *arXiv 预印本 arXiv:2211.10017*，2022年。'
- en: '[21] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
    H. Zhang, and I. Stoica, “Efficient memory management for large language model
    serving with pagedattention,” in *Proceedings of the 29th Symposium on Operating
    Systems Principles*, 2023, pp. 611–626.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. Gonzalez,
    H. Zhang, 和 I. Stoica，“用于大规模语言模型服务的高效内存管理与分页注意力，”在 *第29届操作系统原理研讨会论文集*，2023年，第611–626页。'
- en: '[22] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez,
    H. Zhang, and I. Stoica, “Efficient memory management for large language model
    serving with pagedattention,” in *Proceedings of the ACM SIGOPS 29th Symposium
    on Operating Systems Principles*, 2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] W. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez,
    H. Zhang, 和 I. Stoica, “大语言模型服务的高效内存管理与分页注意力”，见 *ACM SIGOPS 第29届操作系统原理研讨会论文集*，2023年。'
- en: '[23] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X. Dang,
    C. Gan, and S. Han, “Awq: Activation-aware weight quantization for llm compression
    and acceleration,” in *MLSys*, 2024.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. Lin, J. Tang, H. Tang, S. Yang, W.-M. Chen, W.-C. Wang, G. Xiao, X.
    Dang, C. Gan, 和 S. Han, “Awq: 激活感知权重量化用于 LLM 压缩与加速”，见 *MLSys*，2024年。'
- en: '[24] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel mixture
    models,” 2016.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Merity, C. Xiong, J. Bradbury, 和 R. Socher, “指针哨兵混合模型”，2016年。'
- en: '[25] NVIDIA, “TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language
    Model Inference,” 2023\. [Online]. Available: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] NVIDIA, “TensorRT-LLM: 一个用于优化大语言模型推理的 TensorRT 工具箱”，2023年。[在线]. 可用: [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)'
- en: '[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison,
    A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, “Pytorch:
    An imperative style, high-performance deep learning library,” 2019.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen,
    Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Köpf, E. Yang, Z. DeVito, M.
    Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, 和 S. Chintala,
    “Pytorch: 一种命令式风格的高性能深度学习库”，2019年。'
- en: '[27] Z. Qu, L. Liu, F. Tu, Z. Chen, Y. Ding, and Y. Xie, “Dota: detect and
    omit weak attentions for scalable transformer acceleration,” in *Proceedings of
    the 27th ACM International Conference on Architectural Support for Programming
    Languages and Operating Systems*, 2022, pp. 14–26.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Z. Qu, L. Liu, F. Tu, Z. Chen, Y. Ding, 和 Y. Xie, “Dota: 检测并忽略弱注意力以加速可扩展的变换器”，见
    *第27届 ACM 国际程序语言与操作系统架构支持会议论文集*，2022年，第14-26页。'
- en: '[28] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D. Choudhary,
    M. Cornea, E. Dellinger, K. Denolf *et al.*, “Microscaling data formats for deep
    learning,” *arXiv preprint arXiv:2310.10537*, 2023.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] B. D. Rouhani, R. Zhao, A. More, M. Hall, A. Khodamoradi, S. Deng, D.
    Choudhary, M. Cornea, E. Dellinger, K. Denolf *等*，“深度学习的微缩数据格式”，*arXiv 预印本 arXiv:2310.10537*，2023年。'
- en: '[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, and Y. Choi, “Winogrande: An
    adversarial winograd schema challenge at scale,” *arXiv preprint arXiv:1907.10641*,
    2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] K. Sakaguchi, R. L. Bras, C. Bhagavatula, 和 Y. Choi, “Winogrande: 大规模对抗性
    Winograd 模式挑战”，*arXiv 预印本 arXiv:1907.10641*，2019年。'
- en: '[30] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Z. Zhang, P. Gao,
    Y. Qiao, and P. Luo, “Omniquant: Omnidirectionally calibrated quantization for
    large language models,” *arXiv preprint arXiv:2308.13137*, 2023.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Z. Zhang, P. Gao,
    Y. Qiao, 和 P. Luo, “Omniquant: 面向大语言模型的全方向校准量化”，*arXiv 预印本 arXiv:2308.13137*，2023年。'
- en: '[31] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato, V. Sanh,
    P. Whatmough, A. M. Rush, D. Brooks *et al.*, “Edgebert: Sentence-level energy
    optimizations for latency-aware multi-task nlp inference,” in *MICRO-54: 54th
    Annual IEEE/ACM International Symposium on Microarchitecture*, 2021, pp. 830–844.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Tambe, C. Hooper, L. Pentecost, T. Jia, E.-Y. Yang, M. Donato, V. Sanh,
    P. Whatmough, A. M. Rush, D. Brooks *等*，“Edgebert: 面向延迟感知多任务 NLP 推理的句子级能效优化”，见
    *MICRO-54: 第54届年度 IEEE/ACM 国际微架构研讨会*，2021年，第830-844页。'
- en: '[32] M. team, “MLC-LLM,” 2023\. [Online]. Available: [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] M. team, “MLC-LLM”，2023年。[在线]. 可用: [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)'
- en: '[33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    and G. Lample, “Llama: Open and efficient foundation language models,” 2023.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave,
    和 G. Lample, “Llama: 开放而高效的基础语言模型”，2023年。'
- en: '[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2: 开放的基础模型和微调聊天模型”，*arXiv
    预印本 arXiv:2307.09288*，2023年。'
- en: '[35] H. Wang, Z. Zhang, and S. Han, “Spatten: Efficient sparse attention architecture
    with cascade token and head pruning,” in *2021 IEEE International Symposium on
    High-Performance Computer Architecture (HPCA)*.   IEEE, 2021, pp. 97–110.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] H. Wang, Z. Zhang, 和 S. Han, “Spatten：具有级联标记和头部修剪的高效稀疏注意力架构，” 见于 *2021
    IEEE 国际高性能计算架构研讨会 (HPCA)*。IEEE，2021年，页码 97–110。'
- en: '[36] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, and X. Liu,
    “Outlier suppression: Pushing the limit of low-bit transformer language models,”
    *arXiv preprint arXiv:2209.13325*, 2022.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] X. Wei, Y. Zhang, X. Zhang, R. Gong, S. Zhang, Q. Zhang, F. Yu, 和 X. Liu,
    “异常值抑制：突破低比特变压器语言模型的极限，” *arXiv 预印本 arXiv:2209.13325*, 2022。'
- en: '[37] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,
    Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M.
    Rush, “Huggingface’s transformers: State-of-the-art natural language processing,”
    2020.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,
    Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, 和 A. M.
    Rush, “Huggingface 的变压器：最先进的自然语言处理，” 2020年。'
- en: '[38] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han, “SmoothQuant:
    Accurate and efficient post-training quantization for large language models,”
    in *Proceedings of the 40th International Conference on Machine Learning*, 2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, 和 S. Han, “SmoothQuant：大语言模型的准确而高效的后训练量化，”
    见于 *第40届国际机器学习大会论文集*，2023年。'
- en: '[39] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu,
    J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang, S. Yang, T. Yu, W. Xie,
    W. Huang, X. Hu, X. Ren, X. Niu, P. Nie, Y. Xu, Y. Liu, Y. Wang, Y. Cai, Z. Gu,
    Z. Liu, and Z. Dai, “Yi: Open foundation models by 01.ai,” 2024.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu,
    J. Chen, J. Chang, K. Yu, P. Liu, Q. Liu, S. Yue, S. Yang, S. Yang, T. Yu, W.
    Xie, W. Huang, X. Hu, X. Ren, X. Niu, P. Nie, Y. Xu, Y. Liu, Y. Wang, Y. Cai,
    Z. Gu, Z. Liu, 和 Z. Dai, “Yi：01.ai 开放基础模型，” 2024年。'
- en: '[40] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, “Orca: A distributed
    serving system for Transformer-Based generative models,” in *16th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 22)*.   Carlsbad, CA: USENIX
    Association, Jul. 2022, pp. 521–538\. [Online]. Available: [https://www.usenix.org/conference/osdi22/presentation/yu](https://www.usenix.org/conference/osdi22/presentation/yu)'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, 和 B.-G. Chun, “Orca：一个用于变压器生成模型的分布式服务系统，”
    见于 *第16届 USENIX 操作系统设计与实现研讨会 (OSDI 22)*。加利福尼亚州卡尔斯巴德：USENIX 协会，2022年7月，页码 521–538。
    [在线]. 可用： [https://www.usenix.org/conference/osdi22/presentation/yu](https://www.usenix.org/conference/osdi22/presentation/yu)'
- en: '[41] A. H. Zadeh, I. Edo, O. M. Awad, and A. Moshovos, “Gobo: Quantizing attention-based
    nlp models for low latency and energy efficient inference,” in *2020 53rd Annual
    IEEE/ACM International Symposium on Microarchitecture (MICRO)*.   IEEE, 2020,
    pp. 811–824.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. H. Zadeh, I. Edo, O. M. Awad, 和 A. Moshovos, “Gobo：量化基于注意力的 NLP 模型以实现低延迟和能效推断，”
    见于 *2020年第53届IEEE/ACM国际微架构研讨会 (MICRO)*。IEEE，2020年，页码 811–824。'
- en: '[42] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, and Y. Choi, “Hellaswag:
    Can a machine really finish your sentence?” *CoRR*, vol. abs/1905.07830, 2019\.
    [Online]. Available: [http://arxiv.org/abs/1905.07830](http://arxiv.org/abs/1905.07830)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] R. Zellers, A. Holtzman, Y. Bisk, A. Farhadi, 和 Y. Choi, “Hellaswag：机器真的能完成你的句子吗？”
    *CoRR*，卷号 abs/1905.07830，2019年。 [在线]. 可用： [http://arxiv.org/abs/1905.07830](http://arxiv.org/abs/1905.07830)'
- en: '[43] L. Zhang, W. Fei, W. Wu, Y. He, Z. Lou, and H. Zhou, “Dual grained quantization:
    Efficient fine-grained quantization for llm,” *arXiv preprint arXiv:2310.04836*,
    2023.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] L. Zhang, W. Fei, W. Wu, Y. He, Z. Lou, 和 H. Zhou, “双粒度量化：高效的细粒度量化用于大语言模型，”
    *arXiv 预印本 arXiv:2310.04836*, 2023年。'
- en: '[44] Y. Zhao, C.-Y. Lin, K. Zhu, Z. Ye, L. Chen, S. Zheng, L. Ceze, A. Krishnamurthy,
    T. Chen, and B. Kasikci, “Atom: Low-bit quantization for efficient and accurate
    llm serving,” in *MLSys*, 2023.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Zhao, C.-Y. Lin, K. Zhu, Z. Ye, L. Chen, S. Zheng, L. Ceze, A. Krishnamurthy,
    T. Chen, 和 B. Kasikci, “Atom：高效且准确的低比特量化用于大语言模型服务，” 见于 *MLSys*，2023年。'
- en: '[45] L. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao, C. Kozyrakis,
    I. Stoica, J. E. Gonzalez, C. Barrett, and Y. Sheng, “Efficiently programming
    large language models using sglang,” 2023.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] L. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao, C. Kozyrakis,
    I. Stoica, J. E. Gonzalez, C. Barrett, 和 Y. Sheng, “使用 sglang 高效编程大型语言模型，” 2023年。'
