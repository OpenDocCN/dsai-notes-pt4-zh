- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '<!--yml  '
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别：未分类  '
- en: 'date: 2024-09-08 18:49:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期：2024-09-08 18:49:54  '
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '-->  '
- en: 'EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'EasyQuant: 一种高效的无数据量化算法用于LLMs  '
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.02775](https://ar5iv.labs.arxiv.org/html/2403.02775)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '来源：[https://ar5iv.labs.arxiv.org/html/2403.02775](https://ar5iv.labs.arxiv.org/html/2403.02775)  '
- en: Hanlin Tang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**韩林·唐**  '
- en: ranchotang@tencent.com &Yifu Sun
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 'ranchotang@tencent.com &**怡福孙**  '
- en: yifusun@tencent.com &Decheng Wu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'yifusun@tencent.com &**于福孙**  '
- en: woodchenwu@tencent.com \ANDKai Liu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 'woodchenwu@tencent.com \AND**凯·刘**  '
- en: raccoonliu@tencent.com &Jianchen Zhu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'raccoonliu@tencent.com &**简辰朱**  '
- en: dickzhu@tencent.com &Zhanhui Kang
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 'dickzhu@tencent.com &**詹辉康**  '
- en: kegokang@tencent.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 'kegokang@tencent.com  '
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '摘要  '
- en: 'Large language models (LLMs) have proven to be very superior to conventional
    methods in various tasks. However, their expensive computations and high memory
    requirements are prohibitive for deployment. Model quantization is an effective
    method for reducing this overhead. The problem is that in most previous works,
    the quantized model was calibrated using a few samples from the training data,
    which might affect the generalization of the quantized LLMs to unknown cases and
    tasks. Hence in this work, we explore an important question: Can we design a data-free
    quantization method for LLMs to guarantee its generalization performance?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在各种任务中已被证明比传统方法更为优越。然而，其昂贵的计算和高内存要求使得部署成为难题。模型量化是一种有效的减少这种开销的方法。问题在于，在大多数先前的工作中，量化模型是通过使用少量训练数据样本进行校准的，这可能影响量化LLMs对未知情况和任务的泛化。因此，在本工作中，我们探讨了一个重要问题：我们能否为LLMs设计一种无数据量化方法以保证其泛化性能？  '
- en: 'In this work, we propose EasyQuant, a training-free and data-free weight-only
    quantization algorithm for LLMs. Our observation indicates that two factors: outliers
    in the weight and quantization ranges, are essential for reducing the quantization
    error. Therefore, in EasyQuant, we leave the outliers (less than $1\%$) unchanged
    and optimize the quantization range to reduce the reconstruction error. With these
    methods, we surprisingly find that EasyQuant achieves comparable performance to
    the original model. Since EasyQuant does not depend on any training data, the
    generalization performance of quantized LLMs are safely guaranteed. Moreover,
    EasyQuant can be implemented in parallel so that the quantized model could be
    attained in a few minutes even for LLMs over 100B. To our best knowledge, we are
    the first work that achieves comparable performance with data-dependent algorithms
    under a data-free setting and our algorithm runs over 10 times faster than the
    data-dependent methods.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们提出了EasyQuant，这是一种不依赖训练和数据的仅权重量化算法。我们的观察表明，两个因素：权重中的异常值和量化范围，对于减少量化误差至关重要。因此，在EasyQuant中，我们保持异常值（少于$1\%$）不变，并优化量化范围以减少重构误差。通过这些方法，我们惊讶地发现EasyQuant的性能与原始模型相当。由于EasyQuant不依赖任何训练数据，因此量化LLMs的泛化性能得到了安全保证。此外，EasyQuant可以并行实现，使得即使对于超过100B的LLMs，量化模型也能在几分钟内获得。我们所知，我们是第一项在无数据设置下实现与数据依赖算法相当性能的工作，并且我们的算法比数据依赖方法快10倍以上。  '
- en: '![Refer to caption](img/deb1964c909a1759356337559990476b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/deb1964c909a1759356337559990476b.png)'
- en: 'Figure 1: Pipeline of EasyQuant. We first find all the outliers in weight and
    keep them in full precision (fp32/fp16/bf16). Afterward, we optimize the quantization
    range (denoted as $q_{range}$) with optimized quantization ranges and we set the
    outliers unchanged in weight.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：EasyQuant的流程图。我们首先找到权重中的所有异常值，并将其保持在全精度（fp32/fp16/bf16）。随后，我们优化量化范围（表示为$q_{range}$）并在权重中保持异常值不变。  '
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1 引言  '
- en: Recent work has already proved the superior performance of Transformer (Vaswani
    et al., [2017](#bib.bib23)) based LLMs (Workshop, [2023](#bib.bib25); Zhang et al.,
    [2022](#bib.bib31); Touvron et al., [2023](#bib.bib22); Brown et al., [2020](#bib.bib2);
    Rae et al., [2021](#bib.bib17); Smith et al., [2022](#bib.bib19); Chowdhery et al.,
    [2022](#bib.bib3); Zeng et al., [2022](#bib.bib30)) on various tasks over traditional
    methods, and has attracted massive interest in how to improve and utilize those
    LLMs. However, the model size also grows dramatically along with improved performance.
    Hence the memory footprint and computational cost become the bottleneck for deploying
    those models. One promising solution to alleviate this overhead is model quantization (Frantar
    et al., [2023a](#bib.bib6); Xiao et al., [2023](#bib.bib27)), where we quantize
    weight only or weight and activation both i order to reduce memory consumption
    and computational cost.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究已经证明了基于Transformer（Vaswani等人，[2017](#bib.bib23)）的LLMs（Workshop，[2023](#bib.bib25)；Zhang等人，[2022](#bib.bib31)；Touvron等人，[2023](#bib.bib22)；Brown等人，[2020](#bib.bib2)；Rae等人，[2021](#bib.bib17)；Smith等人，[2022](#bib.bib19)；Chowdhery等人，[2022](#bib.bib3)；Zeng等人，[2022](#bib.bib30)）在各种任务中相较于传统方法表现优越，并且引起了对如何改进和利用这些LLMs的广泛关注。然而，模型的规模也随着性能的提升而剧增。因此，内存占用和计算成本成为部署这些模型的瓶颈。一种有前景的解决方案是模型量化（Frantar等人，[2023a](#bib.bib6)；Xiao等人，[2023](#bib.bib27)），通过量化权重或同时量化权重和激活来减少内存消耗和计算成本。
- en: Although model quantization is a well-studied area for normal-sized models,
    such as BERT (Devlin et al., [2018](#bib.bib5)) and GPT-2 (Radford et al., [2019](#bib.bib16)),
    it is still a quite challenging task for LLMs. One major reason is that previous
    lossless model quantization algorithms require retraining for the quantized model,
    which is too expensive for models over billions of parameters. Beyond this, previous
    models are usually designed for specific domain tasks, which means the training
    data are sampled from limited task domains. However, recent LLMs are usually trained
    on various domains of data corpus, and they have shown to be quite effective for
    multi-domain zero-shot tasks. In this case, if we only retrain the quantized LLMs
    using partial domain corpus, the generalization ability of LLMs might get worse.
    Therefore both efficiency and generalization guarantees are very important for
    designing LLMs quantization algorithms. To date, for low-bits weight-only quantization,
    several post-training algorithms have been proposed  (Frantar et al., [2023a](#bib.bib6);
    Yao et al., [2022](#bib.bib28)). However, those methods also require a small calibration
    set sampled from training data, which still takes at least several hours. Moreover,
    the use of those calibration data also brings the risk of making the model overfit
    to the calibration set.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型量化在正常规模模型（如BERT（Devlin等人，[2018](#bib.bib5)）和GPT-2（Radford等人，[2019](#bib.bib16)））中已经是一个研究成熟的领域，但对于LLMs来说仍然是一个相当具有挑战性的任务。一个主要原因是，之前的无损模型量化算法需要对量化后的模型进行再训练，这对于参数超过十亿的模型来说代价过高。此外，以前的模型通常是为特定领域任务设计的，这意味着训练数据来自有限的任务领域。然而，最近的LLMs通常是在各种领域的数据语料库上进行训练的，并且在多领域零样本任务中表现出相当的有效性。在这种情况下，如果我们只使用部分领域语料库对量化后的LLMs进行再训练，LLMs的泛化能力可能会下降。因此，效率和泛化保证在设计LLMs量化算法时非常重要。迄今为止，对于低位权重量化，已经提出了几种后训练算法（Frantar等人，[2023a](#bib.bib6)；Yao等人，[2022](#bib.bib28)）。然而，这些方法仍然需要从训练数据中采样的少量校准数据，这仍然需要至少几个小时。此外，使用这些校准数据也带来了模型对校准集过拟合的风险。
- en: 'Our Contribution:'
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们的贡献：
- en: In this work, we propose a novel data-free model quantization algorithm, namely
    EasyQuant, that potentially improves the performance of low-bits quantized LLMs.
    The generalization ability of LLMs is inherently guaranteed since EasyQuant does
    not need any input data. By running EasyQuant for only a few minutes, we can quantize
    public-available OPT-176B, BLOOM-176B, and LLAMA-65B into lower bits without significant
    loss on various benchmarks. To our best knowledge, this is the first data-free
    LLM quantization algorithm for LLM quantization without notable system overhead.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的无数据模型量化算法，即EasyQuant，可能会提高低位量化LLMs的性能。由于EasyQuant不需要任何输入数据，因此LLMs的泛化能力得到本质上的保证。通过仅运行EasyQuant几分钟，我们可以将公开的OPT-176B、BLOOM-176B和LLAMA-65B量化为低位，而不会在各种基准上造成显著的损失。据我们所知，这是第一个无数据的LLM量化算法，且没有明显的系统开销。
- en: Moreover, our work reveals the essential factors that cause the performance
    degradation of the quantized LLMs. We show that the outliers in weights are more
    critical to the model’s performance compared to the normal elements. Beyond this,
    we propose to use a gradient-based method for optimizing the quantization range.
    These two strategies can also be used in other scenarios, such as weight-activation
    quantization and quantization-aware training (QAT).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的工作揭示了量化 LLM 性能下降的关键因素。我们展示了相较于正常元素，权重中的异常值对模型性能的影响更为重要。除此之外，我们建议使用基于梯度的方法来优化量化范围。这两种策略也可以应用于其他场景，如权重量化和量化感知训练（QAT）。
- en: Last but not least, we develop efficient CUDA kernels for outlier isolation
    in dequantization, and proved that hold $1\%$ minutes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，我们开发了高效的 CUDA 核心用于在解量化中隔离异常值，并证明可以维持 $1\%$ 的分钟数。
- en: '![Refer to caption](img/ebf7f263f0f9cb75a71b538eb6b8de12.png)![Refer to caption](img/b31a2c39ae3e29d97ca1affda3508261.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ebf7f263f0f9cb75a71b538eb6b8de12.png)![参见说明](img/b31a2c39ae3e29d97ca1affda3508261.png)'
- en: 'Figure 2: Smaller reconstruction error cannot guarantee a better model performance.
    Straightforwardly shrinking the quantization ranges will clip most of the outliers
    to be very small, hence the perplexity increases severely since those outliers
    are critical for preserving the model’s performance. However, when keeping those
    outliers unquantized, the quantized model achieves a better performance as the
    reconstruction error decreases continuously. This result clearly suggests that
    the outliers are more important than the normal values in weight, and optimizing
    the quantization ranges using gradient defined in ([2](#S3.Ex4 "In 3.1 The quantization
    range can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs")) can significantly
    increase the accuracy of quantized models. More details about the experiment can
    be found in Section [5](#S5 "5 Experiment ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：较小的重建误差并不能保证更好的模型性能。简单地缩小量化范围会将大多数异常值裁剪得非常小，因此困惑度会严重增加，因为这些异常值对于保持模型性能至关重要。然而，当保持这些异常值不量化时，量化模型的性能会随着重建误差的持续下降而得到改善。这个结果清楚地表明，异常值在权重中比正常值更重要，并且使用梯度优化量化范围（见[2](#S3.Ex4
    "在 3.1 中，量化范围可以通过梯度有效优化 ‣ 3 EasyQuant 背后的见解 ‣ EasyQuant：一种高效的数据无关 LLM 量化算法")）可以显著提高量化模型的准确性。有关实验的更多细节可以在第[5](#S5
    "5 实验 ‣ EasyQuant：一种高效的数据无关 LLM 量化算法")节中找到。
- en: 2 Background and Motivation
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与动机
- en: The most widely used quantization method, namely rounding to nearest-number
    (RTN), quantizes a tensor $\bm{x}$-bits representation according to
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最广泛使用的量化方法，即四舍五入到最接近的数字（RTN），根据以下公式对张量 $\bm{x}$ 的位表示进行量化
- en: '|  | $\displaystyle Q[\bm{x}]=s\times\left\lfloor\text{clamp}\left(\frac{\bm{x}}{s},l_{\min},l_{\max}\right)\right\rceil$
    |  | (1) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q[\bm{x}]=s\times\left\lfloor\text{clamp}\left(\frac{\bm{x}}{s},l_{\min},l_{\max}\right)\right\rceil$
    |  | (1) |'
- en: Here $s$.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 $s$。
- en: There are two major directions for finding the best configuration in weight-only
    LLM quantization. The first is to minimize the reconstruction error of the weight
    parameter (denoted as $W$), which is defined as
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在仅权重的 LLM 量化中，寻找最佳配置有两个主要方向。第一个是最小化权重参数（记作 $W$）的重建误差，其定义为
- en: '|  | $\displaystyle r(W):=\&#124;Q[W]-W\&#124;^{2}.$ |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r(W):=\|Q[W]-W\|^{2}.$ |  |'
- en: Notice that in this case we only need to have access to the weight itself, therefore
    it is data-free.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在这种情况下，我们只需要访问权重本身，因此它是无数据的。
- en: Beyond this, recent studies (Frantar et al., [2023a](#bib.bib6); Yao et al.,
    [2022](#bib.bib28)) propose to use the output error, defined as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，最近的研究（Frantar 等，[2023a](#bib.bib6)；Yao 等，[2022](#bib.bib28)）建议使用定义为输出误差的度量。
- en: '|  | $\displaystyle e(W)=\sum_{X\in\mathcal{D}}\left\&#124;Q[W]X-WX\right\&#124;^{2},$
    |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle e(W)=\sum_{X\in\mathcal{D}}\left\|Q[W]X-WX\right\|^{2},$
    |  |'
- en: where $\mathcal{D}$ is a calibration set sampled from the original training
    data, for optimization. This regulation tries to mimic the outputs from the original
    model directly hence achieving a more promising result than reconstruction-based
    methods.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 是从原始训练数据中抽样的校准集，用于优化。这种规制试图直接模仿原始模型的输出，因此比基于重建的方法更能实现有希望的结果。
- en: Data-dependent calibration might weaken the generalization ability of LLMs
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据依赖的校准可能会削弱大语言模型（LLMs）的泛化能力。
- en: However, the performance gain from using calibration data might jeopardize the
    generalization of the quantized model, because it brings the risk of making the
    model overfit to the calibration set. For example, both ZeroQuant and GPTQ involve
    changing the original weight by training or OBS in order to minimize the output
    error, therefore the distribution of the weight’s parameters might deviate from
    the original. Since the calibration data is usually sampled from a few specific
    domains, the performance of the calibrated model on other tasks may not be guaranteed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用校准数据带来的性能提升可能会危害量化模型的泛化能力，因为这可能导致模型过拟合到校准集。例如，ZeroQuant 和 GPTQ 都涉及通过训练或
    OBS 改变原始权重以最小化输出误差，因此权重参数的分布可能会偏离原始分布。由于校准数据通常来自于少数特定领域，校准模型在其他任务上的表现可能无法保证。
- en: Data-free quantization is challenging, but very important
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无数据量化虽然具有挑战性，但非常重要。
- en: 'Although it’s more challenging to use the reconstruction error as a regulation
    because it can only optimize the quantized model indirectly, still it is a very
    important direction for researching because the generalization ability of the
    model is inherently guaranteed when using data-free quantization since it uses
    no training data. Therefore in this paper, we aim to answer the following question:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用重构误差作为调节因其只能间接优化量化模型更加具有挑战性，但它仍然是一个非常重要的研究方向，因为模型的泛化能力在使用无数据量化时天生得到保证，因为它不使用训练数据。因此，在本文中，我们旨在回答以下问题：
- en: 'How can we efficiently recover the performance of the quantized model without
    using any input data? In this work we propose EasyQuant, a data-free fast algorithm
    that could significantly improve the performance of quantized LLMs in a data-free
    setting, and more importantly, even outperforms the results from data-dependent
    quantization algorithms. Our experiments reveal that the performance gap of the
    lower bits (e.g. $4$-bits) quantized LLMs origins from two factors:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何在不使用任何输入数据的情况下高效恢复量化模型的性能？在这项工作中，我们提出了 EasyQuant，这是一种无数据快速算法，能够显著提高量化 LLM
    的性能，且更重要的是，甚至超越了数据依赖量化算法的结果。我们的实验揭示了低位（例如 $4$ 位）量化 LLM 性能差距来源于两个因素：
- en: '1.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Setting the quantization range as the maximum absolute value of the weight induces
    a large reconstruction error for low-bits quantization.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将量化范围设置为权重的最大绝对值会导致低位量化的大重构误差。
- en: '2.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The outliers in the weight matrix, which account for less than $0.1\%$ of the
    parameters, impose a very important influence on the model’s performance.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重矩阵中的离群值占参数的不到 $0.1\%$，对模型性能产生了非常重要的影响。
- en: In EasyQuant, we use quantization range minimization and outlier isolation to
    address these two challenges, and our results prove that EasyQuant achieves a
    significant improvement over RTN.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EasyQuant 中，我们通过量化范围最小化和离群值隔离来解决这两个挑战，我们的结果证明 EasyQuant 在 RTN 上取得了显著改进。
- en: 3 Insight behind EasyQuant
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 EasyQuant 背后的洞察
- en: As mentioned above, the weight’s outliers and quantization ranges are essential
    to the quantized model’s performance. Below we present the supporting experiments
    in detail.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，权重的离群值和量化范围对量化模型的性能至关重要。下面我们详细展示支持实验。
- en: 3.1 The quantization range can be efficiently optimized using gradient
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化范围可以通过梯度有效优化。
- en: 'Although the quantization operation itself is non-differentiable, the gradient
    of the reconstruction error ($\|Q[\bm{x}]-\bm{x}\|^{2}$ admits (see Section [4](#S4
    "4 Methodology ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for
    LLMs") for more details)'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管量化操作本身是不可微分的，但重构误差的梯度（$\|Q[\bm{x}]-\bm{x}\|^{2}$ 详见第[4](#S4 "4 Methodology
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs)节）具有可得性。'
- en: '|  | $\displaystyle\frac{\partial\&#124;Q[\bm{x}]-\bm{x}\&#124;^{2}}{\partial
    s}=2\sum_{i}\left((Q[x_{i}]-x_{i})\left\lfloor\frac{x_{i}}{s}\right\rceil\right).$
    |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial\|Q[\bm{x}]-\bm{x}\|^{2}}{\partial s}=2\sum_{i}\left((Q[x_{i}]-x_{i})\left\lfloor\frac{x_{i}}{s}\right\rceil\right).$
    |  | (2) |'
- en: 'With this gradient, the reconstruction error can be quickly minimized within
    hundreds of steps (see Figure [2](#S1.F2 "Figure 2 ‣ Our Contribution: ‣ 1 Introduction
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs") for more
    details). This result indicates that by shrinking the quantization range, most
    of the parameters in weight can be approximated more precisely. However, as shown
    in Figure [2](#S1.F2 "Figure 2 ‣ Our Contribution: ‣ 1 Introduction ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs"), the performance of the
    quantized weight gets even worse as the reconstruction error decreases. This is
    a very counter-intuitive result.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这个梯度，重构误差可以在数百步内迅速最小化（详细信息见图[2](#S1.F2 "图 2 ‣ 我们的贡献： ‣ 1 引言 ‣ EasyQuant：一种高效的无数据量化算法")）。这一结果表明，通过缩小量化范围，权重中的大多数参数可以更精确地被近似。然而，如图[2](#S1.F2
    "图 2 ‣ 我们的贡献： ‣ 1 引言 ‣ EasyQuant：一种高效的无数据量化算法")所示，随着重构误差的减少，量化权重的性能变得更差。这是一个非常反直觉的结果。
- en: Through in-depth analysis, we realized that when decreasing the quantization
    range, more salient parameters outside the quantization range would be clipped
    out. Although most of the weights get approximated more precisely as indicated
    by the decreased reconstruction error, the salient parameters are poorly represented.
    As the model performance drops severely in this case, we realized that those outliers
    are way more important than the normal elements for the model’s performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入分析，我们意识到，当减小量化范围时，量化范围之外的显著参数会被裁剪掉。虽然大多数权重由于重构误差的减少而被更精确地近似，但显著参数的表示效果较差。由于在这种情况下模型性能严重下降，我们意识到这些异常值对模型性能比正常元素重要得多。
- en: 3.2 Outliers in weight are very important, but not sufficient
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 权重中的异常值非常重要，但还不够
- en: '| Threshold $n$ |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 阈值 $n$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| PPL on WikiText2 | $11.37$ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2上的PPL | $11.37$ |'
- en: 'Table 1: Isolating outliers in weight from quantization can increase the model’s
    performance. Here $n$) numbers being held unquantized, there is still a large
    gap to the baseline. This means isolating the outliers is not enough to fully
    recover the accuracy of quantized models.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：从量化中隔离异常值可以提高模型的性能。在这里，即使保持$n$个数字不量化，仍然存在较大的基线差距。这意味着隔离异常值不足以完全恢复量化模型的准确性。
- en: Before we further discuss the influence of those outliers, we first provide
    a ($n\sigma$) outlier if
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步讨论这些异常值的影响之前，我们首先提供一个($n\sigma$)异常值，如果
- en: '|  | $\displaystyle\left&#124;W_{i,j}-mean(W)\right&#124;\geq n*var(W),$ |  |
    (3) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left|W_{i,j}-mean(W)\right|\geq n*var(W),$ |  | (3) |'
- en: where $mean(W)$.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $mean(W)$。
- en: 'Now the question is: Can we hold those outliers unchanged and straightforwardly
    compress the normal elements into lower bits? Unfortunately, our result suggests
    that excluding the outliers from quantization solely is not enough. As shown in
    Table [1](#S3.T1 "Table 1 ‣ 3.2 Outliers in weight are very important, but not
    sufficient ‣ 3 Insight behind EasyQuant ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs"), the performance gap still exists even when we hold $1\%$
    numbers in fp16\. The problem is that if we keep too many numbers in fp16, the
    overhead of the dequantization kernel would also increase and result in a decreased
    overall throughput.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是：我们是否可以保持这些异常值不变，并将正常元素直接压缩到更低的位数？不幸的是，我们的结果表明，仅仅排除异常值是不够的。如表[1](#S3.T1
    "表 1 ‣ 权重中的异常值非常重要，但还不够 ‣ EasyQuant 背后的洞察 ‣ EasyQuant：一种高效的无数据量化算法")所示，即使我们保持$1\%$的数字在fp16中，性能差距仍然存在。问题在于，如果我们保留太多的数字在fp16中，去量化内核的开销也会增加，从而导致整体吞吐量下降。
- en: 3.3 EasyQuant potentially improve the performance
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 EasyQuant有潜力提高性能
- en: 'As shown in Section [3.1](#S3.SS1 "3.1 The quantization range can be efficiently
    optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant: An Efficient
    Data-free Quantization Algorithm for LLMs") and Section [3.2](#S3.SS2 "3.2 Outliers
    in weight are very important, but not sufficient ‣ 3 Insight behind EasyQuant
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs"), optimizing
    the quantization ranges directly reduces the model’s performance drops severely
    because of the clipped outliers. These key observations inspire us to design EasyQuant,
    in which we isolate the outliers from quantization first and then optimizing the
    quantization range for the remaining elements. As shown in the right part of Figure [2](#S1.F2
    "Figure 2 ‣ Our Contribution: ‣ 1 Introduction ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs"), with outliers being kept unquantized, the performance
    of the quantized model increases continuously under decreased reconstruction.
    This clearly proves we can potentially improve the performance of quantized LLMs
    with this strategy.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [3.1](#S3.SS1 "3.1 量化范围可以通过梯度有效优化 ‣ 3 EasyQuant 的洞察 ‣ EasyQuant: 一种高效的无数据量化算法")
    和第 [3.2](#S3.SS2 "3.2 权重中的异常值非常重要，但不够充分 ‣ 3 EasyQuant 的洞察 ‣ EasyQuant: 一种高效的无数据量化算法")
    节所示，直接优化量化范围会导致模型性能因剪裁异常值而严重下降。这些关键观察促使我们设计了 EasyQuant，其中我们首先将异常值从量化中隔离，然后对剩余元素优化量化范围。如图
    [2](#S1.F2 "图 2 ‣ 我们的贡献 ‣ 1 介绍 ‣ EasyQuant: 一种高效的无数据量化算法") 右侧所示，保持异常值不量化的情况下，量化模型的性能在重建减少的情况下持续提高。这清楚地证明了我们可以通过这种策略潜在地改善量化
    LLM 的性能。'
- en: 4 Methodology
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: '4.1 Driving of the gradient in  ([2](#S3.Ex4 "In 3.1 The quantization range
    can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs"))'
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.1 梯度的驱动在 ([2](#S3.Ex4 "在 3.1 量化范围可以通过梯度有效优化 ‣ 3 EasyQuant 的洞察 ‣ EasyQuant:
    一种高效的无数据量化算法"))'
- en: Let’s say the original scale $s$, which means
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 假设原始尺度 $s$，这意味着
- en: '|  | $\displaystyle\left\lfloor\frac{x}{s+\Delta s}\right\rceil=\left\lfloor\frac{x}{s}\right\rceil,\quad\text{if
    }\frac{x}{s}-\left\lfloor\frac{x}{s+\Delta s}\right\rceil\neq 0.5.$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\lfloor\frac{x}{s+\Delta s}\right\rceil=\left\lfloor\frac{x}{s}\right\rceil,\quad\text{如果
    }\frac{x}{s}-\left\lfloor\frac{x}{s+\Delta s}\right\rceil\neq 0.5.$ |  |'
- en: Therefore we get
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们得到
- en: '|  | $\displaystyle Q_{s+\Delta s}[x]=$ |  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Q_{s+\Delta s}[x]=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: this leads to
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致
- en: '|  | $\displaystyle\frac{\partial Q[x]}{\partial s}=\frac{Q_{s+\Delta s}[x]-Q_{s}[x]}{\Delta
    s}=\left\lfloor\frac{x}{s}\right\rceil.$ |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial Q[x]}{\partial s}=\frac{Q_{s+\Delta s}[x]-Q_{s}[x]}{\Delta
    s}=\left\lfloor\frac{x}{s}\right\rceil.$ |  |'
- en: This gives us
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了
- en: '|  |  | $\displaystyle\frac{\partial\&#124;Q[\bm{x}]-\bm{x}\&#124;^{2}}{\partial
    s}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\frac{\partial\&#124;Q[\bm{x}]-\bm{x}\&#124;^{2}}{\partial
    s}$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 4.2 Algorithm description
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 算法描述
- en: 'In EasyQuant, for each weight $W$. Afterward, for the normal elements, we optimize
    the per-channel quantization range using an optimizer (in our case we use Adam
    for example) with gradients defined in ([2](#S3.Ex4 "In 3.1 The quantization range
    can be efficiently optimized using gradient ‣ 3 Insight behind EasyQuant ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs")). The final quantized
    weight from EasyQuant can be formulated as'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在 EasyQuant 中，对于每个权重 $W$。随后，对于正常元素，我们使用定义在 ([2](#S3.Ex4 "在 3.1 量化范围可以通过梯度有效优化
    ‣ 3 EasyQuant 的洞察 ‣ EasyQuant: 一种高效的无数据量化算法")) 中的梯度，通过优化器（例如我们使用 Adam）优化每通道的量化范围。EasyQuant
    的最终量化权重可以表示为'
- en: '|  |  | $\displaystyle Q^{EasyQuant}[W]$ |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle Q^{EasyQuant}[W]$ |  |'
- en: '|  | $\displaystyle=$ |  | (4) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  | (4) |'
- en: where $Mask^{o}$ is a mask tensor defined as
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Mask^{o}$ 是定义为
- en: '|  | $\displaystyle Mask^{o}_{i,j}(W)=\left\{\begin{array}[]{rl}1&amp;\text{if
    }(i,j)\in I^{o}(W),\\ 0&amp;\text{if }(i,j)\notin I^{o}(W).\end{array}\right.$
    |  | (5) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Mask^{o}_{i,j}(W)=\left\{\begin{array}[]{rl}1&amp;\text{如果
    }(i,j)\in I^{o}(W),\\ 0&amp;\text{如果 }(i,j)\notin I^{o}(W).\end{array}\right.$
    |  | (5) |'
- en: 'The detailed description of EasyQuant is in Algorithm [1](#alg1 "Algorithm
    1 ‣ 4.2 Algorithm description ‣ 4 Methodology ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'EasyQuant 的详细描述见算法 [1](#alg1 "算法 1 ‣ 4.2 算法描述 ‣ 4 方法论 ‣ EasyQuant: 一种高效的无数据量化算法")。'
- en: Algorithm 1 EasyQuant
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 EasyQuant
- en: '1:  Initialize: outlier threshold $n$ is defined in ([5](#S4.Ex18 "In 4.2 Algorithm
    description ‣ 4 Methodology ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化：离群值阈值 $n$ 在（[5](#S4.Ex18 "在 4.2 算法描述 ‣ 4 方法论 ‣ EasyQuant：一种高效的无数据量化算法")）中定义。'
- en: 5 Experiment
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Baselines:'
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线：
- en: 'We compare EasyQuant with several baselines in the INT4 quantization setting
    below:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的 INT4 量化设置中比较了 EasyQuant 与几个基线方法：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RTN: The model’s weights are naively quantized according to ([1](#S2.Ex1 "In
    2 Background and Motivation ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")).'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RTN: 模型的权重根据（[1](#S2.Ex1 "在 2 背景和动机 ‣ EasyQuant：一种高效的无数据量化算法")）进行简单量化。'
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ZeroQuant: The algorithm proposed in Yao et al. ([2022](#bib.bib28)). Authors
    treat each layer as a small neural network and use the original as the teacher
    model to distill the quantized one. This is equivalently minimizing $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$
    is the quantized model.'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ZeroQuant: Yao 等人（[2022](#bib.bib28)）提出的算法。作者将每一层视为一个小型神经网络，并使用原始模型作为教师模型来蒸馏量化模型。这相当于最小化
    $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$ 即量化模型。'
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPTQ: This algorithm is proposed in Frantar et al. ([2023a](#bib.bib6)). Authors
    use the same objective function $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$
    as in ZeroQuant. But they utilize OBS for minimizing the loss function instead
    of using a gradient-based optimizer.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GPTQ: 该算法由 Frantar 等人（[2023a](#bib.bib6)）提出。作者使用与 ZeroQuant 相同的目标函数 $\sum_{\bm{x}\in\mathcal{D}}\|f(W^{T};\bm{x})-f(W^{S};\bm{x})\|^{2}$。但他们使用
    OBS 来最小化损失函数，而不是使用基于梯度的优化器。'
- en: Experiment Setup.
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: For all models, we set the outlier threshold $n\in[2.5,3]$ for LLAMA. We use
    symmetric quantization since the normal values are symmetrically distributed with
    the outliers being excluded. For a fair comparison, we use per-channel quantization
    for weight in all algorithms (which means each column shares one common quantization
    range).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有模型，我们将离群值阈值 $n\in[2.5,3]$ 设置为 LLAMA。由于正常值在离群值被排除的情况下是对称分布的，我们使用对称量化。为了公平比较，我们在所有算法中对权重使用每通道量化（即每列共享一个公共量化范围）。
- en: Evaluation Tasks.
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估任务。
- en: As for the evaluation tasks, we mainly focus on perplexity-based tasks, as they
    are known to be particularly sensitive to model quantization  Frantar et al. ([2023b](#bib.bib7)).
    The perplexity tasks we include are WikiText2  (Merity et al., [2016](#bib.bib12)),
    Penn Treebank  (Marcus et al., [1994](#bib.bib11)) and C4  (Raffel et al., [2020](#bib.bib18)).
    The zero-shot tasks’ results are also provided, such as PIQA  (Tata and Patel,
    [2003](#bib.bib21)), ARC  (Boratko et al., [2018](#bib.bib1)) and StoryCloze  (Mostafazadeh
    et al., [2017](#bib.bib13)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估任务，我们主要关注基于困惑度的任务，因为它们对模型量化特别敏感 Frantar 等人（[2023b](#bib.bib7)）。我们包括的困惑度任务有
    WikiText2（Merity 等人，[2016](#bib.bib12)）、Penn Treebank（Marcus 等人，[1994](#bib.bib11)）和
    C4（Raffel 等人，[2020](#bib.bib18)）。零-shot 任务的结果也提供了，如 PIQA（Tata 和 Patel，[2003](#bib.bib21)）、ARC（Boratko
    等人，[2018](#bib.bib1)）和 StoryCloze（Mostafazadeh 等人，[2017](#bib.bib13)）。
- en: Implementation.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现。
- en: Since each weight can be quantized in parallel, therefore we use $8*$ mins for
    all models. We store the index and value for all outliers together with the quantized
    normal values. Our dequantization kernel is built using CUDA.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个权重可以并行量化，因此我们对所有模型使用 $8*$ 分钟。我们存储所有离群值的索引和值以及量化后的正常值。我们的去量化内核使用 CUDA 构建。
- en: '|  |  | Perplexity-based Task |  |  | Perplexity-based Task |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 基于困惑度的任务 |  |  | 基于困惑度的任务 |'
- en: '|  | WikiText2 | PTB | C4 |  | WikiText2 | PTB | C4 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | WikiText2 | PTB | C4 |  | WikiText2 | PTB | C4 |'
- en: '| LLAMA–7B | fp16 | $5.68$ |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA–7B | fp16 | $5.68$ |'
- en: '| RTN | $6.29$ |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $6.29$ |'
- en: '| GPTQ | $6.09$ |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | $6.09$ |'
- en: '| EasyQuant | 6.01 | 10.72 | 7.71 | EasyQuant | 4.34 | $8.45$ | 6.37 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 6.01 | 10.72 | 7.71 | EasyQuant | 4.34 | $8.45$ | 6.37 |'
- en: '| LLAMA–13B | fp16 | $5.09$ |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA–13B | fp16 | $5.09$ |'
- en: '| RTN | $5.53$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $5.53$ |'
- en: '| GPTQ | $5.36$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | $5.36$ |'
- en: '| EasyQuant | 5.29 | 9.37 | 6.97 | EasyQuant | 3.98 | 9.61 | 6.30 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 5.29 | 9.37 | 6.97 | EasyQuant | 3.98 | 9.61 | 6.30 |'
- en: 'Table 2: Perplexity results for LLAMA model family'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: LLAMA 模型家族的困惑度结果'
- en: 5.1 Experiment Analysis
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验分析
- en: We focus our study on LLM by quantizing the entire BLOOM, and LLAMA model families
    to 4-bit.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们专注于通过将整个 BLOOM 和 LLAMA 模型家族量化为 4 位来进行研究。
- en: Perplexity-base tasks.
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于困惑度的任务。
- en: 'We first study perplexity-based tasks. On LLaMA models, Table [2](#S5.T2 "Table
    2 ‣ Implementation. ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") shows that EasyQuant outperforms GPTQ in most cases. For
    LLaMA-65B, GPTQ drops 4.21 points on PTB, performing worse than the 9 $\times$
    smaller full-precision 7B model, while EasyQuant still performs well on this task.
    On the other tasks, EasyQuant losing only 0.4–0.7 points. BLOOM shows a similar
    pattern (see Table [10](#A1.T10 "Table 10 ‣ Appendix A Appendix ‣ EasyQuant: An
    Efficient Data-free Quantization Algorithm for LLMs") in appendix): EasyQuant
    drops only 0.1-0.16 points on perplexity-based tasks. Notice that we observe a
    smaller gap between our method and GPTQ on C4\. It is mostly because, as a data-calibrated
    quantization method, GPTQ uses C4 dataset for calibrations.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先研究基于困惑度的任务。在 LLaMA 模型上，表[2](#S5.T2 "Table 2 ‣ Implementation. ‣ 5 Experiment
    ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs") 显示 EasyQuant
    在大多数情况下优于 GPTQ。对于 LLaMA-65B，GPTQ 在 PTB 上下降了 4.21 分，表现比 9 $\times$ 更小的全精度 7B 模型还要差，而
    EasyQuant 在此任务上表现仍然良好。在其他任务上，EasyQuant 仅下降了 0.4–0.7 分。BLOOM 显示了类似的模式（参见附录中表[10](#A1.T10
    "Table 10 ‣ Appendix A Appendix ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs")）：EasyQuant 在基于困惑度的任务上仅下降了 0.1-0.16 分。请注意，我们在 C4 上观察到我们的方法与
    GPTQ 之间的差距较小。这主要是因为，作为一种数据校准量化方法，GPTQ 使用 C4 数据集进行校准。'
- en: Zeroshot tasks.
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Zeroshot 任务。
- en: 'For most zero-shot tasks, EasyQuant achieves harmless performance with only
    0.1 %-0.52% accuracy drops as shown in Table [10](#A1.T10 "Table 10 ‣ Appendix
    A Appendix ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for LLMs")
    in appendix and outperforms GPTQ on most cases. Here we simply use the implementation
    of GPTQ on LLAMA from its git.¹¹1https://github.com/qwopqwop200/GPTQ-for-LLaMa
    We note that EasyQuant can be further improved via finer-granularity grouping.
    However, we will not include this overhead in this paper.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '对于大多数零样本任务，EasyQuant 以仅 0.1 %-0.52% 的准确率下降实现了无害的性能，如附录中表[10](#A1.T10 "Table
    10 ‣ Appendix A Appendix ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")所示，并且在大多数情况下优于 GPTQ。这里我们简单使用了 GPTQ 在 LLAMA 上的实现。¹¹1https://github.com/qwopqwop200/GPTQ-for-LLaMa
    我们注意到，EasyQuant 可以通过更细粒度的分组进一步改进。不过，我们不会在本文中包含这一开销。'
- en: '| outlier ratio | overhead |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 异常值比例 | 开销 |'
- en: '| --- | --- |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $0.01\%$ | 0.027ms |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| $0.01\%$ | 0.027ms |'
- en: '| $0.10\%$ | 0.055ms |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| $0.10\%$ | 0.055ms |'
- en: '| $0.50\%$ | 0.093ms |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| $0.50\%$ | 0.093ms |'
- en: '| $1\%$ | 0.117ms |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $1\%$ | 0.117ms |'
- en: '| $5\%$ | 0.186ms |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| $5\%$ | 0.186ms |'
- en: '| $10\%$ | 0.212ms |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $10\%$ | 0.212ms |'
- en: 'Table 3: Overhead of outlier isolation on A100'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: A100 上异常值隔离的开销'
- en: Practical Latency.
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实际延迟。
- en: 'We evaluate the overhead of EasyQuant by comparing the overhead of outlier
    isolation, int$4$ms. Therefore from Table [3](#S5.T3 "Table 3 ‣ Zeroshot tasks.
    ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") we can see that recovering the outliers in weight brings
    almost no overhead to the overall latency.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过比较异常值隔离的开销来评估 EasyQuant 的开销，int$4$ms。因此，从表[3](#S5.T3 "Table 3 ‣ Zeroshot
    tasks. ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free
    Quantization Algorithm for LLMs") 可以看出，恢复权重中的异常值对整体延迟几乎没有影响。'
- en: Ablation study.
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究。
- en: 'To understand the effect of unstructured outliers, we show the perplexity result
    of EasyQuant without outlier isolation or quantization range optimization. As
    discussed in Section [3](#S3 "3 Insight behind EasyQuant ‣ EasyQuant: An Efficient
    Data-free Quantization Algorithm for LLMs"), both strategies impose a very important
    influence on the final model performance.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '为了了解无结构异常值的影响，我们展示了 EasyQuant 在没有异常值隔离或量化范围优化的情况下的困惑度结果。如第[3](#S3 "3 Insight
    behind EasyQuant ‣ EasyQuant: An Efficient Data-free Quantization Algorithm for
    LLMs")节所讨论，这两种策略对最终模型性能有非常重要的影响。'
- en: 'We further conduct experiments proving whether the performance gain mainly
    comes from the outlier isolation: Actually, outlier isolation is a very important
    component of EasyQuant, but still not enough to fully recover the performance
    loss from quantization. Keeping even 10% of weights as fp16 outliers still admits
    about 8% ppl increase while EasyQuant admits only 1$\%$ ppl increase. Below we
    present the result of 4-bit quantized BLLOM-7B when we just keep 1% outliers in
    fp16 without quantization range optimization on various benchmarks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步进行了实验，以证明性能提升是否主要来自异常值隔离：实际上，异常值隔离是 EasyQuant 非常重要的组成部分，但仍不足以完全恢复量化带来的性能损失。即使将
    10% 的权重保持为 fp16 异常值，仍会导致大约 8% 的 ppl 增加，而 EasyQuant 仅承认 1$\%$ 的 ppl 增加。下面我们展示了在各种基准测试中，当我们仅保留
    1% 的 fp16 异常值且未进行量化范围优化时的 4 位量化 BLLOM-7B 的结果。
- en: '| Benchmark | EasyQuant | 1% fp16 outlier |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 基准测试 | EasyQuant | 1% fp16 异常值 |'
- en: '| --- | --- | --- |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| WikiText2(PPL) | 11.66 | 12.52 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2(PPL) | 11.66 | 12.52 |'
- en: '| PTB (PPL) | 21.42 | 23.32 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| PTB (PPL) | 21.42 | 23.32 |'
- en: '| C4(PPL) | 15.46 | 16.44 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| C4(PPL) | 15.46 | 16.44 |'
- en: '| PIQA (ACC) | 73.61% | 72.74% |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| PIQA (ACC) | 73.61% | 72.74% |'
- en: 'Table 4: Using outlier isolation solely is not enough to fully recover the
    performance loss. EasyQuant consistently outperforms outlier isolation in all
    benchmarks.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：仅使用异常值隔离不足以完全恢复性能损失。EasyQuant 在所有基准测试中始终优于异常值隔离。
- en: Outlier influence.
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值影响。
- en: 'The outlier isolation is a key component in EasyQuant, but it can only impose
    an indirect influence on the model accuracy. The interesting phenomenon we find
    is that the outliers behave like a gating mechanism: without outlier isolation,
    the model achieves a much worse performance under a small reconstruction error;
    however, when keeping those outliers in fp16, the quantized LLM attains a continuously
    decreased ppl under smaller reconstruction error:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值隔离是 EasyQuant 的一个关键组件，但它只能对模型准确性产生间接影响。我们发现的有趣现象是，异常值表现得像一个门控机制：没有异常值隔离时，模型在小的重构误差下表现更差；然而，当将这些异常值保留在
    fp16 中时，量化的 LLM 在较小的重构误差下实现了持续降低的 ppl：
- en: '| reconstruction error | int4 outlier | fp16 outlier |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 重构误差 | int4 异常值 | fp16 异常值 |'
- en: '| --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 4.8E4 | 12.65 | 12.50 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 4.8E4 | 12.65 | 12.50 |'
- en: '| 3.5E4 | 14.73 | 11.61 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 3.5E4 | 14.73 | 11.61 |'
- en: '| 2.7E4 | 19.71 | 11.25 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2.7E4 | 19.71 | 11.25 |'
- en: '| 2.3E4 | NA | 11.10 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2.3E4 | NA | 11.10 |'
- en: '| 1.9E4 | NA | 11.02 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 1.9E4 | NA | 11.02 |'
- en: 'Table 5: ppl results on Wikitext2 of BLOOM-7B with and without outlier isolation.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：带有和不带有异常值隔离的 BLOOM-7B 的 Wikitext2 上的 ppl 结果。
- en: 'Moreover, we have also conducted a complementary experiment testing the direct
    influence of the weight outlier: We prune 1% of the values ( according to its
    magnitude) in weights into 0 and see the ppl results (as shown in Table  [6](#S5.T6
    "Table 6 ‣ Outlier influence. ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant:
    An Efficient Data-free Quantization Algorithm for LLMs")). It has shown that the
    largest value (outliers) imposes the same influence on the model performance as
    the normal values (median), which means those outliers share the same direct influence
    on the model accuracy with normal values. Therefore outlier isolation imposes
    a key influence on the model accuracy indirectly.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们还进行了补充实验，测试权重异常值的直接影响：我们将 1% 的权重值（根据其幅度）修剪为 0，并查看 ppl 结果（如表 [6](#S5.T6
    "表 6 ‣ 异常值影响。 ‣ 5.1 实验分析 ‣ 5 实验 ‣ EasyQuant: 一种高效的无数据量化算法") 所示）。结果表明，最大值（异常值）对模型性能的影响与正常值（中位数）相同，这意味着这些异常值对模型准确性与正常值具有相同的直接影响。因此，异常值隔离对模型准确性具有间接的重要影响。'
- en: '| pruned weights | PPL |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝权重 | PPL |'
- en: '| --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| smallest (top-0% 1%) | 11.66 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 最小 (top-0% 1%) | 11.66 |'
- en: '| median (top-49% 50%) | 19.16 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 (top-49% 50%) | 19.16 |'
- en: '| largest (top-99% 100%) | 19.17 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 最大 (top-99% 100%) | 19.17 |'
- en: 'Table 6: ppl results after pruning 1% weight with different magnitude'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：修剪 1% 权重后，不同幅度的 ppl 结果
- en: Outlier distribution.
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值分布。
- en: 'We also explore the outlier distribution along different modules and layers.
    It shows that the fraction of outliers shares different patterns in different
    modules and layers (as shown in Table  [7](#S5.T7 "Table 7 ‣ Outlier distribution.
    ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization
    Algorithm for LLMs") and  [8](#S5.T8 "Table 8 ‣ Outlier distribution. ‣ 5.1 Experiment
    Analysis ‣ 5 Experiment ‣ EasyQuant: An Efficient Data-free Quantization Algorithm
    for LLMs")). FFN.2 has a significantly higher fraction of outliers. However, it
    shows no pattern along the layer index.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还探讨了不同模块和层级的异常值分布。结果显示，异常值的比例在不同模块和层级中表现出不同的模式（如表 [7](#S5.T7 "表 7 ‣ 异常值分布。
    ‣ 5.1 实验分析 ‣ 5 实验 ‣ EasyQuant: 一种高效的无数据量化算法") 和 [8](#S5.T8 "表 8 ‣ 异常值分布。 ‣ 5.1
    实验分析 ‣ 5 实验 ‣ EasyQuant: 一种高效的无数据量化算法") 所示）。FFN.2 的异常值比例显著更高。然而，它在层级索引中没有表现出明显的模式。'
- en: '| module name | outlier fraction (%) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 模块名称 | 异常值比例 (%) |'
- en: '| --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Att.qkv | 0.2993 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Att.qkv | 0.2993 |'
- en: '| Att.output | 0.5036 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Att.output | 0.5036 |'
- en: '| FFN.1 | 0.288 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| FFN.1 | 0.288 |'
- en: '| FFN.2 | 0.7560 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FFN.2 | 0.7560 |'
- en: 'Table 7: Outlier fraction distribution in different modules in BLOOM-7B under
    3-sigma threshold'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在 3-σ 阈值下，BLOOM-7B 不同模块的异常值分布
- en: '| Layer index | outlier fraction (%) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 层级索引 | 异常值比例 (%) |'
- en: '| --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 0.3187 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.3187 |'
- en: '| 5 | 0.8579 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.8579 |'
- en: '| 10 | 0.3953 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.3953 |'
- en: '| 15 | 0.3975 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.3975 |'
- en: '| 20 | 0.3962 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.3962 |'
- en: '| 25 | 0.4399 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 0.4399 |'
- en: '| 30 | 0.3954 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 0.3954 |'
- en: 'Table 8: Outlier fraction distribution in different layer index in BLOOM-7B
    under 3-sigma threshold'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：BLOOM-7B中不同层索引下的离群值分布（3-sigma阈值）
- en: Quantization range.
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化范围。
- en: 'The dynamic of the quantization range is shown in Table [9](#S5.T9 "Table 9
    ‣ Quantization range. ‣ 5.1 Experiment Analysis ‣ 5 Experiment ‣ EasyQuant: An
    Efficient Data-free Quantization Algorithm for LLMs"). Roughly speaking, this
    range decreases fast in the early stage of training, which means a smaller quantization
    range will make most of the parameters to be quantized more precisely. After certain
    steps of training, the quantization range becomes stable, this means we have already
    achieved the optimal range.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 量化范围的动态变化如表[9](#S5.T9 "表 9 ‣ 量化范围 ‣ 5.1 实验分析 ‣ 5 实验 ‣ EasyQuant：一种高效的数据无关量化算法")所示。粗略来说，这个范围在训练的早期阶段迅速下降，这意味着较小的量化范围会使大多数参数得到更精确的量化。在经过一定步骤的训练后，量化范围变得稳定，这意味着我们已经达到了**最佳范围**。
- en: '| steps | quantization range |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| steps | 量化范围 |'
- en: '| --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $0$ | 0.078 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| $0$ | 0.078 |'
- en: '| $10$ | 0.069 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| $10$ | 0.069 |'
- en: '| $50$ | 0.052 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| $50$ | 0.052 |'
- en: '| $100$ | 0.048 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| $100$ | 0.048 |'
- en: '| $150$ | 0.047 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| $150$ | 0.047 |'
- en: '| $200$ | 0.047 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| $200$ | 0.047 |'
- en: 'Table 9: The dynamic quantization range of different optimization steps. Here
    we take the quantization range of the Att.qkv module in layer 1 as an example.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同优化步骤的动态量化范围。这里我们以第1层的Att.qkv模块的量化范围为例。
- en: 6 Related Work
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Model Quantization
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型量化
- en: Traditional model quantization algorithms mainly focus on the cases where both
    parameters and activations of the model are quantized (Lin et al., [2015](#bib.bib10);
    Hubara et al., [2016](#bib.bib8); Tailor et al., [2021](#bib.bib20); Ni et al.,
    [2020](#bib.bib14)). However, directly quantizing the model will greatly decrease
    the accuracy of the models, and one important technique to improve the performance
    is Quantization Aware Training (QAT) (Jacob et al., [2018](#bib.bib9)), where
    it simulates the quantization procedure in training to improve the accuracy of
    the quantized model further. For Transformer based models, the boundary of the
    compression level has been continuously advanced. For example, $8$-bits quantized
    BERT in  Wu et al. ([2023](#bib.bib26)) and tenary case as in TernaryBERT (Zhang
    et al., [2020](#bib.bib32)).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的模型量化算法主要关注模型参数和激活值的量化（Lin 等，[2015](#bib.bib10)；Hubara 等，[2016](#bib.bib8)；Tailor
    等，[2021](#bib.bib20)；Ni 等，[2020](#bib.bib14)）。然而，直接对模型进行量化会大大降低模型的准确性，而提升性能的重要技术是量化感知训练（QAT）（Jacob
    等，[2018](#bib.bib9)），它在训练中模拟量化过程，以进一步提高量化模型的准确性。对于基于Transformer的模型，压缩级别的边界已不断推进。例如，Wu
    等（[2023](#bib.bib26)）中的$8$位量化BERT和TernaryBERT（Zhang 等，[2020](#bib.bib32)）中的三值情况。
- en: Model Quantization for LLMs.
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的模型量化。
- en: 'For quantizing LLMs, due to their prohibitive training expense, we can only
    use a few training data for calibration. There are two major directions: 1) weight-only
    quantization, where the weights are quantized into lower bits. In  Frantar et al.
    ([2023a](#bib.bib6)); Yao et al. ([2022](#bib.bib28)), authors optimize the output
    error on the calibration set using OBS and gradient descent. 2) Activation and
    weight quantization, where both activations and weights are quantized into lower
    bits. In this case, the major obstacle is the outliers in activations. LLM.int8()
     (Dettmers et al., [2022](#bib.bib4)) addresses this problem by isolating those
    outliers in fp16/bf16\. However, such implementation leads to large latency overhead
    and is even slower than fp16 inference. Recent studies  (Wei et al., [2023](#bib.bib24);
    Xiao et al., [2023](#bib.bib27)) found that the outliers only exist in certain
    channels, and use the LayerNorm weights (Wei et al., [2023](#bib.bib24)) and calibrated
    scales (Xiao et al., [2023](#bib.bib27)) to smooth those channels.  Xiao et al.
    ([2023](#bib.bib27)) has already proved that we can achieve almost lossless W8A8
    quantized LLMs using a few calibration data, without manipulating the original
    model weights.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于量化大语言模型（LLMs），由于其训练成本过高，我们只能使用少量的训练数据进行校准。主要有两个方向：1）仅权重量化，将权重量化为较低位数。在 Frantar
    等人（[2023a](#bib.bib6)）；Yao 等人（[2022](#bib.bib28)）的研究中，作者使用 OBS 和梯度下降来优化校准集上的输出误差。2）激活值和权重量化，将激活值和权重都量化为较低位数。在这种情况下，主要的障碍是激活值中的异常值。LLM.int8()（Dettmers
    等人，[2022](#bib.bib4)）通过将这些异常值隔离在 fp16/bf16 中来解决这个问题。然而，这种实现会导致较大的延迟开销，甚至比 fp16
    推理更慢。最近的研究（Wei 等人，[2023](#bib.bib24)；Xiao 等人，[2023](#bib.bib27)）发现异常值只存在于某些通道中，并使用
    LayerNorm 权重（Wei 等人，[2023](#bib.bib24)）和校准尺度（Xiao 等人，[2023](#bib.bib27)）来平滑这些通道。Xiao
    等人（[2023](#bib.bib27)）已经证明，我们可以使用少量的校准数据实现几乎无损的 W8A8 量化 LLM，而无需操控原始模型权重。
- en: 7 Conclusion and Limitations
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与局限性
- en: In this paper, we propose a data-free fast weight-only quantization algorithm,
    namely EasyQuant, for LLMs, that potentially improves the quantized model’s performance
    without using any training data. Our analysis reveals the intrinsic origins of
    the performance loss when quantizing the model weights into lower bits. We show
    that by isolating the outliers from quantization, the accuracy of the quantized
    LLM increases accordingly with decreased reconstruction error. Our experiment
    proved that EasyQuant significantly outperforms RTN in a data-free setting, and
    also behaves better than data-dependent algorithms. EasyQuant can finish the quantization
    for a 176B-sized model within $10$ minutes and the overhead of dequantization
    in EasyQuant is negligible.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们提出了一种无数据的快速仅权重量化算法，即 EasyQuant，针对 LLMs，这种方法有可能在不使用任何训练数据的情况下提高量化模型的性能。我们的分析揭示了在将模型权重量化为较低位数时性能损失的内在原因。我们展示了通过隔离量化中的异常值，量化
    LLM 的准确性随之提高，同时重建误差降低。我们的实验证明 EasyQuant 在无数据设置中显著优于 RTN，并且比数据依赖算法表现更好。EasyQuant
    可以在 $10$ 分钟内完成对 176B 大小模型的量化，并且 EasyQuant 的去量化开销可以忽略不计。
- en: 'However, we also point out some limitations of our work: The outlier recovery
    functionality in EasyQuant requires extra CUDA kernels for implementation. Moreover,
    weight-only quantization can only reduce the memory footprint without any computation
    cost reduction, hence the latency of our model cannot be minimized. In addition,
    this outlier isolation will make the weight/activation quantization more challenging
    because the weight includes numbers under different precision. We have also noticed
    that EasyQuantcannot outperform the data-dependent methods in all tasks, this
    motivates us to investigate more effective algorithms in future studies.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也指出了我们工作的某些局限性：EasyQuant 中的异常值恢复功能需要额外的 CUDA 内核进行实现。此外，仅权重量化只能减少内存占用，而无法降低计算成本，因此我们模型的延迟无法最小化。此外，这种异常值隔离将使得权重/激活值量化更加具有挑战性，因为权重包含了不同精度下的数值。我们还注意到，EasyQuant
    在所有任务中无法超越数据依赖方法，这促使我们在未来的研究中探索更有效的算法。
- en: References
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Boratko et al. (2018) Michael Boratko, Harshit Padigela, Divyendra Mikkilineni,
    Pritish Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche,
    Pavan Kapanipathi, Nicholas Mattei, et al. 2018. A systematic classification of
    knowledge, reasoning, and context within the arc dataset. *arXiv preprint arXiv:1806.00358*.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boratko 等（2018）Michael Boratko、Harshit Padigela、Divyendra Mikkilineni、Pritish
    Yuvraj、Rajarshi Das、Andrew McCallum、Maria Chang、Achille Fokoue-Nkoutche、Pavan
    Kapanipathi、Nicholas Mattei 等。2018年。**Arc 数据集中的知识、推理和上下文的系统分类**。*arXiv 预印本 arXiv:1806.00358*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。2020年。**语言模型是少样本学习者**。*神经信息处理系统进展*，33:1877–1901。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等（2022）Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann
    等。2022年。**Palm: 通过路径扩展语言建模**。*arXiv 预印本 arXiv:2204.02311*。'
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Llm.int8(): 8-bit matrix multiplication for transformers at scale](http://arxiv.org/abs/2208.07339).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。2022年。
    [**Llm.int8(): 规模化变换器的 8 位矩阵乘法**](http://arxiv.org/abs/2208.07339)。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. [Bert: Pre-training of deep bidirectional transformers for language
    understanding](http://arxiv.org/abs/1810.04805). Cite arxiv:1810.04805Comment:
    13 pages.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等（2018）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2018年。
    [**Bert: 用于语言理解的深度双向转换器的预训练**](http://arxiv.org/abs/1810.04805)。引用 arxiv:1810.04805
    注释：13 页。'
- en: 'Frantar et al. (2023a) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. 2023a. [Gptq: Accurate post-training quantization for generative
    pre-trained transformers](http://arxiv.org/abs/2210.17323).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2023a）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。2023a年。
    [**Gptq: 生成预训练变换器的精确后训练量化**](http://arxiv.org/abs/2210.17323)。'
- en: 'Frantar et al. (2023b) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. 2023b. [Gptq: Accurate post-training quantization for generative
    pre-trained transformers](http://arxiv.org/abs/2210.17323).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2023b）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。2023b年。
    [**Gptq: 生成预训练变换器的精确后训练量化**](http://arxiv.org/abs/2210.17323)。'
- en: Hubara et al. (2016) Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv,
    and Yoshua Bengio. 2016. Binarized neural networks. In *Advances in neural information
    processing systems*, pages 4107–4115.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara 等（2016）Itay Hubara、Matthieu Courbariaux、Daniel Soudry、Ran El-Yaniv 和
    Yoshua Bengio。2016年。**二值化神经网络**。载于 *神经信息处理系统进展*，第 4107–4115 页。
- en: Jacob et al. (2018) Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu,
    Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. 2018. Quantization
    and training of neural networks for efficient integer-arithmetic-only inference.
    pages 2704–2713.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等（2018）Benoit Jacob、Skirmantas Kligys、Bo Chen、Menglong Zhu、Matthew Tang、Andrew
    Howard、Hartwig Adam 和 Dmitry Kalenichenko。2018年。**量化与训练神经网络以实现高效整数算术推理**。第 2704–2713
    页。
- en: Lin et al. (2015) Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua
    Bengio. 2015. Neural networks with few multiplications. *arXiv preprint arXiv:1510.03009*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2015）Zhouhan Lin、Matthieu Courbariaux、Roland Memisevic 和 Yoshua Bengio。2015年。**少乘法的神经网络**。*arXiv
    预印本 arXiv:1510.03009*。
- en: 'Marcus et al. (1994) Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994.
    The penn treebank: Annotating predicate argument structure. In *Human Language
    Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11,
    1994*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcus 等（1994）Mitch Marcus、Grace Kim、Mary Ann Marcinkiewicz、Robert MacIntyre、Ann
    Bies、Mark Ferguson、Karen Katz 和 Britta Schasberger。1994年。**Penn Treebank: 标注谓词论元结构**。载于
    *人类语言技术：1994年3月8日至11日在新泽西州普林斯堡举行的研讨会论文集*。'
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. 2016. [Pointer sentinel mixture models](http://arxiv.org/abs/1609.07843).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等（2016）Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。2016年。
    [**Pointer sentinel 混合模型**](http://arxiv.org/abs/1609.07843)。
- en: 'Mostafazadeh et al. (2017) Nasrin Mostafazadeh, Michael Roth, Annie Louis,
    Nathanael Chambers, and James Allen. 2017. Lsdsem 2017 shared task: The story
    cloze test. In *Proceedings of the 2nd Workshop on Linking Models of Lexical,
    Sentential and Discourse-level Semantics*, pages 46–51.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mostafazadeh 等（2017）**纳斯林·莫斯塔法扎德**、**迈克尔·罗斯**、**安妮·路易斯**、**纳撒尼尔·钱伯斯** 和 **詹姆斯·艾伦**。2017年。Lsdsem
    2017 共享任务：故事闭合测试。在*第2届词汇、句子和话语级语义模型链接研讨会*，第46–51页。
- en: 'Ni et al. (2020) Renkun Ni, Hong-min Chu, Oscar Castañeda, Ping-yeh Chiang,
    Christoph Studer, and Tom Goldstein. 2020. Wrapnet: Neural net inference with
    ultra-low-resolution arithmetic. *arXiv preprint arXiv:2007.13242*.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni 等（2020）**任坤·倪**、**洪敏·楚**、**奥斯卡·卡斯塔涅达**、**平叶·蒋**、**克里斯托夫·斯图德** 和 **汤姆·戈尔德斯坦**。2020年。Wrapnet：超低分辨率算术的神经网络推理。*arXiv
    预印本 arXiv:2007.13242*。
- en: Prato et al. (2019) Gabriele Prato, Ella Charlaix, and Mehdi Rezagholizadeh.
    2019. Fully quantized transformer for improved translation. *arXiv preprint arXiv:1910.10485*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prato 等（2019）**加布里埃尔·普拉托**、**艾拉·查拉克斯** 和 **梅赫迪·雷扎戈利扎德**。2019年。完全量化的变换器以改善翻译。*arXiv
    预印本 arXiv:1910.10485*。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）**阿莱克·拉德福德**、**杰弗里·吴**、**雷旺·蔡尔德**、**大卫·卢安**、**达里奥·阿莫代伊**、**伊利亚·苏茨克维尔**
    等。2019年。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9。
- en: 'Rae et al. (2021) Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican,
    Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah
    Young, et al. 2021. Scaling language models: Methods, analysis & insights from
    training gopher. *arXiv preprint arXiv:2112.11446*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2021）**杰克·W·雷**、**塞巴斯蒂安·博尔戈**、**特雷弗·蔡**、**凯蒂·米利坎**、**乔丹·霍夫曼**、**弗朗西斯·宋**、**约翰·阿斯拉尼德斯**、**莎拉·亨德森**、**罗曼·林**、**苏珊娜·杨**
    等。2021年。扩展语言模型：来自训练 Gopher 的方法、分析与见解。*arXiv 预印本 arXiv:2112.11446*。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）**科林·拉费尔**、**诺姆·沙泽尔**、**亚当·罗伯茨**、**凯瑟琳·李**、**沙兰·纳朗**、**迈克尔·马滕纳**、**燕奇·周**、**韦·李**
    和 **彼得·J·刘**。2020年。探索统一文本到文本变换器的迁移学习极限。*机器学习研究期刊*，21(1):5485–5551。
- en: Smith et al. (2022) Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley,
    Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas,
    Vijay Korthikanti, et al. 2022. Using deepspeed and megatron to train megatron-turing
    nlg 530b, a large-scale generative language model. *arXiv preprint arXiv:2201.11990*.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smith 等（2022）**沙登·史密斯**、**莫斯托法·帕特瓦里**、**布兰登·诺里克**、**帕特里克·勒格雷斯利**、**萨米亚姆·拉杰班达里**、**贾里德·卡斯帕**、**朱恩·刘**、**施瑞迈·普拉布莫耶**、**乔治·泽尔维亚斯**、**维贾伊·科尔蒂坎提**
    等。2022年。使用 DeepSpeed 和 Megatron 训练 Megatron-Turing NLG 530B，一个大规模生成语言模型。*arXiv
    预印本 arXiv:2201.11990*。
- en: 'Tailor et al. (2021) Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D
    Lane. 2021. Degree-quant: Quantization-aware training for graph neural networks.
    *International Conference on Learning Representations*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tailor 等（2021）**夏姆·A·泰勒**、**哈维尔·费尔南德斯-马尔克斯** 和 **尼古拉斯·D·莱恩**。2021年。Degree-quant：图神经网络的量化感知训练。*国际学习表征会议*。
- en: 'Tata and Patel (2003) Sandeep Tata and Jignesh M Patel. 2003. Piqa: An algebra
    for querying protein data sets. In *15th International Conference on Scientific
    and Statistical Database Management, 2003.*, pages 141–150\. IEEE.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tata 和 Patel（2003）**桑迪普·塔塔** 和 **吉涅什·M·帕特尔**。2003年。Piqa：用于查询蛋白质数据集的代数。在*第15届国际科学与统计数据库管理会议，2003年*，第141–150页。IEEE。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](http://arxiv.org/abs/2302.13971).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）**雨果·图夫龙**、**蒂博·拉夫里尔**、**戈泰·伊扎卡德**、**泽维尔·马尔丁**、**玛丽-安·拉肖**、**蒂莫泰·拉克鲁瓦**、**巴蒂斯特·罗济埃尔**、**纳曼·戈亚尔**、**埃里克·汉布罗**、**法伊萨尔·阿扎尔**、**奥雷利安·罗德里格斯**、**阿尔芒·朱林**、**爱德华·格拉夫**
    和 **吉约姆·兰普尔**。2023年。[Llama: 开放且高效的基础语言模型](http://arxiv.org/abs/2302.13971)。'
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](http://arxiv.org/abs/1706.03762). *CoRR*, abs/1706.03762.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）**阿希什·瓦斯瓦尼**、**诺姆·沙泽尔**、**尼基·帕尔马**、**雅各布·乌什科雷特**、**利昂·琼斯**、**艾丹·N·戈麦斯**、**卢卡斯·凯泽**
    和 **伊利亚·波洛苏金**。2017年。[注意力机制是你所需的一切](http://arxiv.org/abs/1706.03762)。*CoRR*，abs/1706.03762。
- en: 'Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2023. [Outlier suppression:
    Pushing the limit of low-bit transformer language models](http://arxiv.org/abs/2209.13325).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2023. [离群点抑制：突破低比特变换器语言模型的极限](http://arxiv.org/abs/2209.13325)。
- en: 'Workshop (2023) BigScience Workshop. 2023. [Bloom: A 176b-parameter open-access
    multilingual language model](http://arxiv.org/abs/2211.05100).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Workshop (2023) BigScience Workshop. 2023. [Bloom: 一个1760亿参数的开放访问多语言模型](http://arxiv.org/abs/2211.05100)。'
- en: 'Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao,
    and Yuxiong He. 2023. [Understanding int4 quantization for transformer models:
    Latency speedup, composability, and failure cases](http://arxiv.org/abs/2301.12017).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and
    Yuxiong He. 2023. [理解int4量化在变换器模型中的应用：延迟加速、组合性和失败案例](http://arxiv.org/abs/2301.12017)。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. [Smoothquant: Accurate and efficient post-training quantization
    for large language models](http://arxiv.org/abs/2211.10438).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. [Smoothquant: 大型语言模型的准确高效后训练量化](http://arxiv.org/abs/2211.10438)。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. [Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers](http://arxiv.org/abs/2206.01861).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. [Zeroquant: 大规模变换器的高效且经济的后训练量化](http://arxiv.org/abs/2206.01861)。'
- en: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. Q8bert: Quantized 8bit bert. *arXiv preprint arXiv:1910.06188*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zafrir et al. (2019) Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
    2019. Q8bert: 量化的8位bert。*arXiv预印本 arXiv:1910.06188*。'
- en: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    An open bilingual pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. (2022) Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai,
    Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b:
    一个开放的双语预训练模型。*arXiv预印本 arXiv:2210.02414*。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: Open pre-trained
    transformer language models](http://arxiv.org/abs/2205.01068).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: 开放的预训练变换器语言模型](http://arxiv.org/abs/2205.01068)。'
- en: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: Distillation-aware ultra-low bit bert.
    *arXiv preprint arXiv:2009.12812*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2020) Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen,
    Xin Jiang, and Qun Liu. 2020. Ternarybert: 蒸馏感知的超低比特bert。*arXiv预印本 arXiv:2009.12812*。'
- en: Appendix A Appendix
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: '|  |  | Perplexity-based Task | Zero-shot Task |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 基于困惑度的任务 | 零样本任务 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | WikiText2 | PTB | C4 | PIQA | ARC-easy | ARC-Challenge | StoryCloze |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | WikiText2 | PTB | C4 | PIQA | ARC-easy | ARC-Challenge | StoryCloze |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| BLOOM | fp16 | $22.42$ |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $22.42$ |'
- en: '| RTN | $25.90$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $25.90$ |'
- en: '| 560M | GPTQ | $24.03$ |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 560M | GPTQ | $24.03$ |'
- en: '| EasyQuant | 23.74 | 46.86 | $28.03$ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 23.74 | 46.86 | $28.03$ |'
- en: '| BLOOM | fp16 | $17.69$ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $17.69$ |'
- en: '| RTN | $22.00$ |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $22.00$ |'
- en: '| 1.1B | GPTQ | $19.05$ |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 1.1B | GPTQ | $19.05$ |'
- en: '| EasyQuant | 18.51 | 61.83 | 22.94 | $\textbf{66.65}\%$ |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 18.51 | 61.83 | 22.94 | $\textbf{66.65}\%$ |'
- en: '| BLOOM | fp16 | $15.39$ |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $15.39$ |'
- en: '| RTN | $16.97$ |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $16.97$ |'
- en: '| 1.7B | GPTQ | $16.48$ |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 1.7B | GPTQ | $16.48$ |'
- en: '| EasyQuant | 16.01 | 31.50 | 20.15 | $\textbf{68.99}\%$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 16.01 | 31.50 | 20.15 | $\textbf{68.99}\%$ |'
- en: '| BLOOM | fp16 | $13.48$ |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $13.48$ |'
- en: '| RTN | $14.76$ |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $14.76$ |'
- en: '| 3B | GPTQ | $14.2$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 3B | GPTQ | $14.2$ |'
- en: '| EasyQuant | 14.01 | 26.12 | 17.96 | $\textbf{69.80}\%$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 14.01 | 26.12 | 17.96 | $\textbf{69.80}\%$ |'
- en: '| BLOOM | fp16 | $11.37$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $11.37$ |'
- en: '| RTN | $12.10$ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $12.10$ |'
- en: '| 7.1B | GPTQ | $11.73$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 7.1B | GPTQ | $11.73$ |'
- en: '| EasyQuant | 11.66 | 21.47 | 15.52 | $\textbf{73.23}\%$ |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | 11.66 | 21.47 | 15.52 | $\textbf{73.23}\%$ |'
- en: '| BLOOM | fp16 | $8.11$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | fp16 | $8.11$ |'
- en: '| RTN | $8.37$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| RTN | $8.37$ |'
- en: '| 176B | GPTQ | $8.21$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 176B | GPTQ | $8.21$ |'
- en: '| EasyQuant | $8.21$ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| EasyQuant | $8.21$ |'
- en: 'Table 10: Perplexity and zershot results for BLOOM model family'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：BLOOM 模型系列的困惑度和零样本结果
