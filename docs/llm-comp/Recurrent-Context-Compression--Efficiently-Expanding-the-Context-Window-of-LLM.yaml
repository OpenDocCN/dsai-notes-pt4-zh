- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:52:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:36
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Recurrent Context Compression: Efficiently Expanding the Context Window of
    LLM'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 递归上下文压缩：高效扩展 LLM 的上下文窗口
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06110](https://ar5iv.labs.arxiv.org/html/2406.06110)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.06110](https://ar5iv.labs.arxiv.org/html/2406.06110)
- en: Chensen Huang¹, Guibo Zhu^(2,3), Xuepeng Wang^(2,3),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 黄晨森¹，朱贵博^(2,3)，王雪鹏^(2,3)，
- en: Yifei Luo¹, Guojing Ge^(2,3), Haoran Chen^(1,2), Dong Yi^(2,3), Jinqiao Wang^(2,3)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 罗逸飞¹，葛国晶^(2,3)，陈浩然^(1,2)，易东^(2,3)，王锦乔^(2,3)
- en: ¹University of Chinese Academy of Sciences,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国科学院大学，
- en: ²Institute of Automation, Chinese of Academy,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²中国科学院自动化研究所，
- en: ³Wuhan AI Research
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³武汉人工智能研究院
- en: 'Correspondence: [huangchensen2022@ia.ac.cn](mailto:ken@yuniversity.edu)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 联系方式：[huangchensen2022@ia.ac.cn](mailto:ken@yuniversity.edu)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: To extend the context length of Transformer-based large language models (LLMs)
    and improve comprehension capabilities, we often face limitations due to computational
    resources and bounded memory storage capacity. This work introduces a method called
    Recurrent Context Compression (RCC), designed to efficiently expand the context
    window length of LLMs within constrained storage space. We also investigate the
    issue of poor model responses when both instructions and context are compressed
    in downstream tasks, and propose an instruction reconstruction method to mitigate
    this problem. We validated the effectiveness of our approach on multiple tasks,
    achieving a compression rate of up to 32x on text reconstruction tasks with a
    BLEU4 score close to 0.95, and nearly 100% accuracy on a passkey retrieval task
    with a sequence length of 1M. Finally, our method demonstrated competitive performance
    in long-text question-answering tasks compared to non-compressed methods, while
    significantly saving storage resources in long-text inference tasks. Our code,
    models, and demo are available at [https://github.com/WUHU-G/RCC_Transformer](https://github.com/WUHU-G/RCC_Transformer)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展基于 Transformer 的大型语言模型（LLMs）的上下文长度并提高理解能力，我们常常面临计算资源和有限存储容量的限制。本研究提出了一种名为递归上下文压缩（RCC）的方法，旨在有效扩展
    LLM 的上下文窗口长度，同时限制存储空间。我们还研究了在下游任务中，当指令和上下文都被压缩时模型响应差的问题，并提出了一种指令重构方法以减轻这一问题。我们在多个任务上验证了我们的方法的有效性，在文本重构任务中实现了高达
    32 倍的压缩率，并且 BLEU4 分数接近 0.95，在一个长度为 1M 的密码检索任务中准确率接近 100%。最后，我们的方法在长文本问答任务中表现出与非压缩方法竞争的性能，同时在长文本推理任务中显著节省了存储资源。我们的代码、模型和演示可在
    [https://github.com/WUHU-G/RCC_Transformer](https://github.com/WUHU-G/RCC_Transformer)
    获取
- en: 'Recurrent Context Compression: Efficiently Expanding the Context Window of
    LLM'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 递归上下文压缩：高效扩展 LLM 的上下文窗口
- en: 'Chensen Huang¹, Guibo Zhu^(2,3), Xuepeng Wang^(2,3), Yifei Luo¹, Guojing Ge^(2,3),
    Haoran Chen^(1,2), Dong Yi^(2,3), Jinqiao Wang^(2,3) ¹University of Chinese Academy
    of Sciences, ²Institute of Automation, Chinese of Academy, ³Wuhan AI Research
    Correspondence: [huangchensen2022@ia.ac.cn](mailto:ken@yuniversity.edu)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 黄晨森¹，朱贵博^(2,3)，王雪鹏^(2,3)，罗逸飞¹，葛国晶^(2,3)，陈浩然^(1,2)，易东^(2,3)，王锦乔^(2,3) ¹中国科学院大学，²中国科学院自动化研究所，³武汉人工智能研究院
    联系方式：[huangchensen2022@ia.ac.cn](mailto:ken@yuniversity.edu)
- en: '![Refer to caption](img/a730fd4d3f789626005423d460d98753.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a730fd4d3f789626005423d460d98753.png)'
- en: 'Figure 1: GPU memory Consumption of Different Models with Increasing Length.
    Left: Pythia-1.4b, Right: RCC model using Pythia-1.4b for both encoder and decoder.
    Both models utilize FlashAttention-2 Dao ([2023](#bib.bib9)). A more detailed
    analysis of GPU memory consumption can be found in Appendix [B](#A2 "Appendix
    B GPU Memory Consumption Analysis ‣ Recurrent Context Compression: Efficiently
    Expanding the Context Window of LLM").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：不同模型在长度增加时的 GPU 内存消耗。左：Pythia-1.4b，右：使用 Pythia-1.4b 作为编码器和解码器的 RCC 模型。这两个模型均使用
    FlashAttention-2 Dao ([2023](#bib.bib9))。有关 GPU 内存消耗的更详细分析，请参见附录 [B](#A2 "附录 B
    GPU 内存消耗分析 ‣ 递归上下文压缩：高效扩展 LLM 的上下文窗口")。
- en: 1 Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the rapid advancement of natural language processing technologies, Transformer-based
    large language models (LLMs) have become a key driving force in this field. However,
    when handling long text inputs, LLMs often encounter limitations in context window
    length. These limitations stem from several inherent factors in the model architecture
    and training methods. Firstly, during the inference phase, models are constrained
    by the pretraining text length, leading to a significant decline in quality when
    the generated sequence exceeds the pretrained context window. Secondly, the design
    of the Transformer architecture requires storing information from the entire input
    sequence, which results in a substantial memory footprint due to the KV-Cache
    during inference.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言处理技术的快速进步，基于Transformer的大型语言模型（LLMs）已成为这一领域的关键推动力。然而，在处理长文本输入时，LLMs 经常遇到上下文窗口长度的限制。这些限制源于模型架构和训练方法中的若干固有因素。首先，在推理阶段，模型受限于预训练文本长度，当生成的序列超出预训练上下文窗口时，质量会显著下降。其次，Transformer架构的设计要求存储整个输入序列的信息，这在推理过程中由于KV-Cache导致了巨大的内存占用。
- en: 'To address these issues, related research works Hochreiter and Schmidhuber
    ([1997](#bib.bib14)); Child et al. ([2019](#bib.bib8)); Wu et al. ([2022](#bib.bib36));
    Rae et al. ([2019](#bib.bib26)); Bulatov et al. ([2022](#bib.bib4)); Liu et al.
    ([2023](#bib.bib19)); Mohtashami and Jaggi ([2023](#bib.bib20)); Beltagy et al.
    ([2020](#bib.bib2)) have optimized training methods, model structures, and KV-Cache
    optimization, thereby extending the context window of LLMs. Among these, context
    compression techniques Rae et al. ([2019](#bib.bib26)); Snell et al. ([2022](#bib.bib28));
    Chevalier et al. ([2023](#bib.bib7)); Wingate et al. ([2022](#bib.bib34)); Mu
    et al. ([2023](#bib.bib21)); Ge et al. ([2023](#bib.bib12)); Munkhdalai et al.
    ([2024](#bib.bib23)); Ren et al. ([2023](#bib.bib27)); Li et al. ([2023](#bib.bib17))
    are considered promising because they can compress context or prompts into shorter
    forms while maintaining good performance, thus enabling the inference of longer
    context windows within limited resources. Figure [1](#S0.F1 "Figure 1 ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM") compares
    the memory resource consumption of our method with non-compression methods. Additionally,
    most text compression-based works can be integrated and combined with other context
    window extension techniques to enhance performance.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，相关研究工作包括 Hochreiter 和 Schmidhuber ([1997](#bib.bib14))；Child 等 ([2019](#bib.bib8))；Wu
    等 ([2022](#bib.bib36))；Rae 等 ([2019](#bib.bib26))；Bulatov 等 ([2022](#bib.bib4))；Liu
    等 ([2023](#bib.bib19))；Mohtashami 和 Jaggi ([2023](#bib.bib20))；Beltagy 等 ([2020](#bib.bib2))
    已经优化了训练方法、模型结构和KV-Cache优化，从而扩展了LLMs的上下文窗口。在这些研究中，上下文压缩技术 Rae 等 ([2019](#bib.bib26))；Snell
    等 ([2022](#bib.bib28))；Chevalier 等 ([2023](#bib.bib7))；Wingate 等 ([2022](#bib.bib34))；Mu
    等 ([2023](#bib.bib21))；Ge 等 ([2023](#bib.bib12))；Munkhdalai 等 ([2024](#bib.bib23))；Ren
    等 ([2023](#bib.bib27))；Li 等 ([2023](#bib.bib17)) 被认为是有前景的，因为它们可以将上下文或提示压缩成更短的形式，同时保持良好的性能，从而在有限的资源内实现更长的上下文窗口推理。图
    [1](#S0.F1 "图 1 ‣ 循环上下文压缩：高效扩展LLM的上下文窗口") 比较了我们的方法与非压缩方法的内存资源消耗。此外，大多数基于文本压缩的工作可以与其他上下文窗口扩展技术集成和结合，以提升性能。
- en: However, existing context compression methods face three major challenges in
    long-text language modeling. Firstly, the efficiency of compression has certain
    limitations. For example, ICAE with 14B parameters Ge et al. ([2023](#bib.bib12))
    experiences a significant performance drop beyond an 8x compression rate. Secondly,
    most context compression research focuses on shorter sequences rather than long
    texts. We found that language models trained for context compression on short
    sequences perform poorly when directly extended to long sequences, necessitating
    new methods to improve long-text compression performance. Lastly, in practical
    applications, we observed that context-compressed language models face the issue
    of context-instruction confusion in downstream tasks. When both context and instructions
    are compressed simultaneously, the model often struggles to follow instructions
    correctly, resulting in poor controllability. This issue has not been emphasized
    or addressed in previous studies, which typically compress only context or instruction
    texts individually.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现有的上下文压缩方法在长文本语言建模中面临三个主要挑战。首先，压缩效率有一定的局限性。例如，具有14B参数的ICAE（Ge et al. ([2023](#bib.bib12))在超过8倍压缩率时会出现显著的性能下降。其次，大多数上下文压缩研究集中在较短的序列上，而非长文本。我们发现，针对短序列进行上下文压缩训练的语言模型在直接扩展到长序列时表现不佳，这需要新的方法来提高长文本的压缩性能。最后，在实际应用中，我们观察到上下文压缩语言模型在下游任务中面临上下文-指令混淆的问题。当上下文和指令同时被压缩时，模型常常难以正确遵循指令，导致控制性差。这个问题在以往的研究中并没有被强调或解决，通常只会单独压缩上下文或指令文本。
- en: 'To address the aforementioned issues, this paper makes the following contributions:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述问题，本文做出了以下贡献：
- en: 'Firstly, we propose a context compression model structure based on an autoencoder,
    which we call the Recurrent Context Compression (RCC) architecture. RCC significantly
    reduces information loss during the compression process, greatly enhancing compression
    efficiency. In experiments, we achieved nearly 95% BLEU-4 scores with a 32x compression
    rate on text reconstruction tasks. Although the encoder occupies some memory resources,
    the memory required by traditional language models exceeds that of the context
    compression model when the sequence length surpasses a certain threshold, as shown
    in Figure [1](#S0.F1 "Figure 1 ‣ Recurrent Context Compression: Efficiently Expanding
    the Context Window of LLM").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提出了一种基于自编码器的上下文压缩模型结构，称为递归上下文压缩（RCC）架构。RCC在压缩过程中显著减少了信息丢失，大大提高了压缩效率。在实验中，我们在文本重建任务中以32倍压缩率达到了接近95%的BLEU-4分数。虽然编码器占用了一些内存资源，但当序列长度超过某个阈值时，传统语言模型所需的内存超出了上下文压缩模型的内存，如图[1](#S0.F1
    "图 1 ‣ 递归上下文压缩：高效扩展LLM的上下文窗口")所示。
- en: 'Secondly, we propose a new training method to adapt long-text context compression
    language models. We introduce a recurrent compression mechanism to overcome the
    context window limitations of the encoder, allowing the model to compress texts
    beyond the encoder window length. When training long-text context compression
    language models, the length of the context sequence input to the encoder is proportional
    to the required computational resources, limiting the extension of context length
    during training. Therefore, we propose a simple yet effective solution: initially,
    conducting full-parameter training on shorter sequences. Subsequently, we freeze
    the encoder on the saved model weights and continue training on longer sequences,
    enabling the extension of training context length under constrained computational
    resources.The detailed information can be found in Section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Long Text Training Methods ‣ 3.3 Model Training Tasks ‣ 3 Design of RCC
    ‣ Recurrent Context Compression: Efficiently Expanding the Context Window of LLM").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们提出了一种新的训练方法，以适应长文本上下文压缩语言模型。我们引入了一种递归压缩机制，以克服编码器的上下文窗口限制，使模型能够压缩超出编码器窗口长度的文本。在训练长文本上下文压缩语言模型时，输入到编码器的上下文序列长度与所需的计算资源成正比，这限制了训练过程中上下文长度的扩展。因此，我们提出了一个简单而有效的解决方案：最初对较短序列进行全参数训练。随后，我们冻结编码器在保存的模型权重上，并继续对较长序列进行训练，从而在有限的计算资源下扩展训练上下文长度。详细信息请参见第[3.3.1节](#S3.SS3.SSS1
    "3.3.1 长文本训练方法 ‣ 3.3 模型训练任务 ‣ 3 RCC设计 ‣ 递归上下文压缩：高效扩展LLM的上下文窗口")。
- en: Lastly, in downstream text generation tasks, we found that when both context
    and instruction texts are compressed, the model struggles to follow instructions
    properly, leading to a decline in response quality. To mitigate this issue, we
    leverage the text reconstruction capability of the context compression language
    model, allowing the decoder to reconstruct the instruction content from the compressed
    vectors and continue generating responses based on the instructions. This significantly
    improves the output quality when both context and instructions are compressed,
    achieving results close to those obtained by inputting instructions directly into
    the encoder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在下游文本生成任务中，我们发现当上下文和指令文本都被压缩时，模型很难正确遵循指令，从而导致回应质量下降。为了解决这个问题，我们利用上下文压缩语言模型的文本重建能力，使解码器能够从压缩向量中重建指令内容，并继续基于指令生成回应。这显著提高了当上下文和指令都被压缩时的输出质量，达到接近直接将指令输入编码器所获得的结果。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Context Compression: Early approaches to context compression aimed to derive
    sentence representation vectors for tasks such as document retrieval. Transformer-based
    autoencoder architectures like TSDAE Wang et al. ([2021](#bib.bib33)) and Nugget
    Qin and Van Durme ([2023](#bib.bib25)) are relevant to our work. In TSDAE, noise
    such as word deletion or swapping is added to input sentences to train sentence
    embedding vectors. The encoder compresses the corrupted sentence into a fixed-size
    vector, which the decoder then reconstructs into the original input text. However,
    such approaches cannot be directly applied to text generation tasks. GistMu et al.
    ([2023](#bib.bib21)) leverages Transformer-based large language models (LLMs)
    as autoencoders. During training, a clever masking matrix compresses prompts into
    a few Gist tokens, which can still prompt the language model for responses. Similar
    prompt compression work was proposed by Wingate et al. ([2022](#bib.bib34)). However,
    these tasks only compress prompts.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文压缩：早期的上下文压缩方法旨在为文档检索等任务推导句子表示向量。基于 Transformer 的自编码器架构如 TSDAE Wang 等人 ([2021](#bib.bib33))
    和 Nugget Qin 和 Van Durme ([2023](#bib.bib25)) 与我们的工作相关。在 TSDAE 中，输入句子会添加诸如词语删除或交换等噪声，以训练句子嵌入向量。编码器将损坏的句子压缩为固定大小的向量，解码器然后将其重建为原始输入文本。然而，这种方法不能直接应用于文本生成任务。GistMu
    等人 ([2023](#bib.bib21)) 利用基于 Transformer 的大型语言模型 (LLMs) 作为自编码器。在训练过程中，一个巧妙的掩码矩阵将提示压缩为少量的
    Gist 令牌，这些令牌仍能促使语言模型作出回应。类似的提示压缩工作由 Wingate 等人 ([2022](#bib.bib34)) 提出。然而，这些任务仅仅是压缩提示。
- en: Several works Snell et al. ([2022](#bib.bib28)); Chevalier et al. ([2023](#bib.bib7));
    Ge et al. ([2023](#bib.bib12)); Ren et al. ([2023](#bib.bib27)); Li et al. ([2023](#bib.bib17));
    Jiang et al. ([2023b](#bib.bib16)); Munkhdalai et al. ([2024](#bib.bib23)) have
    focused on context compression. ICAEGe et al. ([2023](#bib.bib12)) is similar
    to our work but suffers from lower compression efficiency and lacks extensive
    research on longer sequences. Additionally, Jiang et al. ([2023b](#bib.bib16))
    employed non-vector-based context compression by using smaller language models
    to input long texts and generate more compact short texts. Selective context,
    proposed by Li et al. ([2023](#bib.bib17)), identifies and prunes redundancy in
    the input context to enhance LLM inference efficiency, making inputs more compact.
    Recently, Munkhdalai et al. ([2024](#bib.bib23)) proposed a similar work combining
    two attention mechanisms with context compression functionality, showing promising
    results. However, this new attention mechanism cannot be directly applied to pre-trained
    open-source LLMs and faces attention optimization challenges in practical applications.
    None of the studies have thoroughly investigated the problem of instructional
    confusion that arises when both instructions and contextual information are subjected
    to compression. Our work introduces a new solution to mitigate this issue.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究，如Snell等（[2022](#bib.bib28)）；Chevalier等（[2023](#bib.bib7)）；Ge等（[2023](#bib.bib12)）；Ren等（[2023](#bib.bib27)）；Li等（[2023](#bib.bib17)）；Jiang等（[2023b](#bib.bib16)）；Munkhdalai等（[2024](#bib.bib23)），已经集中于上下文压缩。ICAEGe等（[2023](#bib.bib12)）与我们的工作类似，但其压缩效率较低，且对较长序列的研究不够广泛。此外，Jiang等（[2023b](#bib.bib16)）通过使用较小的语言模型输入长文本并生成更紧凑的短文本，采用了非基于向量的上下文压缩。Li等（[2023](#bib.bib17)）提出的选择性上下文通过识别和修剪输入上下文中的冗余来提升LLM推理效率，使输入更为紧凑。最近，Munkhdalai等（[2024](#bib.bib23)）提出了一个类似的工作，将两种注意力机制与上下文压缩功能相结合，展示了良好的结果。然而，这种新的注意力机制不能直接应用于预训练的开源LLM，并且在实际应用中面临注意力优化的挑战。没有研究深入探讨在指令和上下文信息都受到压缩时产生的指令混淆问题。我们的工作提出了一个新方案来缓解这一问题。
- en: Additionally, our work is inspired by language models with recurrent structures
    Hochreiter and Schmidhuber ([1997](#bib.bib14)); Gu and Dao ([2023](#bib.bib13));
    Sun et al. ([2023](#bib.bib29)); Peng et al. ([2023](#bib.bib24)). These models
    compress historical context within a certain range into the hidden state of a
    single time step, enabling the current token to access information from the previous
    step for inference. They demonstrate strong competitiveness with Transformer models,
    indicating that compressing token information over a certain length can achieve
    lossless inference.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的工作受到具有递归结构的语言模型的启发，例如Hochreiter和Schmidhuber（[1997](#bib.bib14)）；Gu和Dao（[2023](#bib.bib13)）；Sun等（[2023](#bib.bib29)）；Peng等（[2023](#bib.bib24)）。这些模型将一定范围内的历史上下文压缩到单个时间步的隐藏状态中，使当前的标记能够访问来自前一步的信息以进行推理。它们与Transformer模型具有很强的竞争力，表明在一定长度内压缩标记信息可以实现无损推理。
- en: 'Long Context LLM: LLMs typically fix the context window length during training,
    such as the Pythia Biderman et al. ([2023](#bib.bib3)), LLaMA Touvron et al. ([2023a](#bib.bib30),
    [b](#bib.bib31)), and Mistral Jiang et al. ([2023a](#bib.bib15)) series. Consequently,
    researchers have explored various methods to extend the context window length
    of pre-trained language models. These methodsChen et al. ([2024](#bib.bib6));
    Tworkowski et al. ([2023](#bib.bib32)); Chen et al. ([2023](#bib.bib5)); Liu et al.
    ([2023](#bib.bib19), [2024](#bib.bib18)), which have achieved notable results
    based on existing pre-trained models. Our approach combines these methods, allowing
    us to apply them to the encoder or decoder to achieve more extended compression
    effects.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文LLM：LLM通常在训练过程中固定上下文窗口的长度，例如Pythia Biderman等（[2023](#bib.bib3)），LLaMA Touvron等（[2023a](#bib.bib30)，[b](#bib.bib31)），以及Mistral
    Jiang等（[2023a](#bib.bib15)）系列。因此，研究人员探索了多种方法来扩展预训练语言模型的上下文窗口长度。这些方法Chen等（[2024](#bib.bib6)）；Tworkowski等（[2023](#bib.bib32)）；Chen等（[2023](#bib.bib5)）；Liu等（[2023](#bib.bib19)，[2024](#bib.bib18)）基于现有的预训练模型取得了显著成果。我们的方法结合了这些方法，使我们能够将其应用于编码器或解码器，以实现更广泛的压缩效果。
- en: '![Refer to caption](img/5ee0a992f87f75543d78a3b6b350cd5e.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5ee0a992f87f75543d78a3b6b350cd5e.png)'
- en: 'Figure 2: The structure of the encoder and decoder in RCC layer i. The maximum
    context window of the encoder is 2048\. The encoder has a compression rate of
    32, and we use the vectors at positions that are multiples of 32 in the output
    as the compressed vectors. Each segment will generate a compressed vector of length
    64\. When the sequence length exceeds 2048, the encoder performs cyclic segmentation
    and compression. The compressed vectors produced between segments in the encoder
    are independent, while those generated within a segment are correlated. The decoder’s
    input is the residual connection between the input vector from the previous layer
    and the compressed vector after linear mapping. All compressed vectors will interact
    within the decoder.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：RCC层i中编码器和解码器的结构。编码器的最大上下文窗口为2048。编码器的压缩率为32，我们使用输出中位置为32的倍数的向量作为压缩向量。每个段落将生成一个长度为64的压缩向量。当序列长度超过2048时，编码器会执行循环分段和压缩。编码器中各段之间生成的压缩向量是独立的，而在段落内生成的压缩向量是相关的。解码器的输入是上一层输入向量与经过线性映射后的压缩向量之间的残差连接。所有压缩向量将在解码器内相互作用。
- en: 3 Design of RCC
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 RCC设计
- en: 3.1 Method Overview
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 方法概述
- en: 'Our encoder design is inspired by the Mamba-based LLM Gu and Dao ([2023](#bib.bib13)).
    Mamba is essentially a state space model, similar to an RNN. In Mamba, the current
    token only needs to access the state vector from the previous timestep to complete
    the current inference step. However, as the context length increases, the performance
    of Mamba deteriorates. This indicates that the state vector at each timestep in
    Mamba can store only a limited length of historical context information. Therefore,
    we propose a compromise: for long sequences, we can divide them into fixed-length
    short sequences and iteratively compress each short sequence into a state vector.
    We concatenate the state vectors of each short sequence as the historical state
    information during inference. This approach maximizes the retention of complete
    historical information while leveraging the model’s compression capabilities to
    save memory. In this paper, we use compression rate to reflect the maximum context
    length that a state vector at each timestep can store. Our experiments show that
    Transformers also have this capability because a Transformer can be viewed as
    a special state space model or RNN. Recent studies have shown that attention can
    be viewed as an RNN Feng et al. ([2024](#bib.bib10)).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的编码器设计灵感来自于基于Mamba的LLM Gu和Dao ([2023](#bib.bib13))。Mamba本质上是一种状态空间模型，类似于RNN。在Mamba中，当前token只需访问上一个时间步的状态向量即可完成当前推理步骤。然而，随着上下文长度的增加，Mamba的性能会下降。这表明Mamba中每个时间步的状态向量只能存储有限长度的历史上下文信息。因此，我们提出了一个折衷方案：对于长序列，我们可以将其划分为固定长度的短序列，并迭代地将每个短序列压缩为一个状态向量。我们将每个短序列的状态向量串联起来，作为推理过程中的历史状态信息。这种方法最大化了完整历史信息的保留，同时利用模型的压缩能力来节省内存。本文中，我们使用压缩率来反映每个时间步状态向量可以存储的最大上下文长度。我们的实验表明，Transformers也具有这种能力，因为Transformer可以视作一种特殊的状态空间模型或RNN。最近的研究表明，注意力机制可以视作一种RNN
    Feng等人 ([2024](#bib.bib10))。
- en: 3.2 RCC Model Architecture
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 RCC模型架构
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣ Recurrent Context
    Compression: Efficiently Expanding the Context Window of LLM"), the RCC model
    architecture is similar to ICAEGe et al. ([2023](#bib.bib12)), consisting of an
    encoder and a decoder. Unlike the connection method in ICAE, where the final layer
    vector of the encoder is used as the input to the decoder, we take a different
    approach. We use the output information from each layer of the encoder. This information
    is then linearly mapped and input into the decoder. This method obtains more feature
    information, and the relevant ablation experiments can be found in Figure [3(a)](#S4.F3.sf1
    "In Figure 3 ‣ 4 Experiments ‣ Recurrent Context Compression: Efficiently Expanding
    the Context Window of LLM"). The encoder can be a Transformer-based LLM or an
    RNN-based LLM, while the decoder is a Transformer-based LLM. The encoder is responsible
    for compressing the information, and the decoder reads the compressed information
    and performs inference. The decoder can fully learn the compressed information
    vector at any position using the attention mechanism. After proper training, the
    maximum context length that the RCC model can support is the encoder compression
    rate multiplied by the decoder context window length.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ 循环上下文压缩：有效扩展 LLM 的上下文窗口") 所示，RCC 模型架构类似于 ICAEGe
    等 ([2023](#bib.bib12))，包括一个编码器和一个解码器。与 ICAE 中使用编码器的最终层向量作为解码器输入的方法不同，我们采取了不同的方式。我们使用编码器每一层的输出信息。这些信息随后被线性映射并输入到解码器中。这种方法获得了更多的特征信息，相关的消融实验可以在图
    [3(a)](#S4.F3.sf1 "在图 3 ‣ 4 实验 ‣ 循环上下文压缩：有效扩展 LLM 的上下文窗口") 中找到。编码器可以是基于 Transformer
    的 LLM 或 RNN 基础的 LLM，而解码器是基于 Transformer 的 LLM。编码器负责压缩信息，解码器读取压缩后的信息并进行推理。解码器可以使用注意力机制充分学习任何位置的压缩信息向量。经过适当训练，RCC
    模型所能支持的最大上下文长度是编码器压缩率与解码器上下文窗口长度的乘积。
- en: 3.2.1 RCC Encoder
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 RCC 编码器
- en: 'The primary task of the RCC encoder is to compress long sequences. The initialized
    encoder is a pretrained language model, which can be based on either Mamba or
    Transformer architectures. By setting a compression rate, we divide long sequences
    into fixed-length short sequences and iteratively feed these short sequences into
    the encoder. As illustrated in Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣
    Recurrent Context Compression: Efficiently Expanding the Context Window of LLM"),
    we locate the token at each position multiple of the compression rate within each
    short sequence. The output vectors of these tokens from each layer serve as compressed
    vectors, which are concatenated to form the final compressed vector for the entire
    sequence. Through this method, the RCC encoder accomplishes the compression modeling
    of the entire long context.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: RCC 编码器的主要任务是压缩长序列。初始化的编码器是一个预训练的语言模型，可以基于 Mamba 或 Transformer 架构。通过设置压缩率，我们将长序列划分为固定长度的短序列，并将这些短序列迭代地输入到编码器中。如图
    [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ 循环上下文压缩：有效扩展 LLM 的上下文窗口") 所示，我们在每个短序列中的压缩率倍数位置找到令牌。这些令牌的输出向量作为压缩向量，它们被连接起来形成整个序列的最终压缩向量。通过这种方法，RCC
    编码器实现了对整个长上下文的压缩建模。
- en: 3.2.2 RCC Decoder
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 RCC 解码器
- en: The decoder of the RCC is a Transformer-based language model responsible for
    the final text inference. Its inputs include the compressed vectors from the encoder
    and token embedding vectors related to the prompts. Each layer’s compressed vector
    from the encoder passes through a linear layer before being input into the decoder.
    For the first layer’s mapped vector, we concatenate it with the decoder’s token
    embedding vector and then feed it into the first block of the decoder. Subsequently,
    the output part of each block is connected with the corresponding layer’s compressed
    vector through residual connections. It is crucial to note that only the output
    vectors corresponding to the compressed information will have residual connections,
    while the other output parts remain unchanged. If the number of compressed vector
    layers does not match the number of decoder layers, we apply simple rule-based
    mappings, either by duplicating to increase the number of layers or averaging
    to reduce the number of layers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: RCC 的解码器是一个基于 Transformer 的语言模型，负责最终的文本推断。它的输入包括来自编码器的压缩向量和与提示相关的 token 嵌入向量。编码器每一层的压缩向量在输入解码器之前会通过一个线性层。对于第一层的映射向量，我们将其与解码器的
    token 嵌入向量拼接，然后送入解码器的第一个块。随后，每个块的输出部分通过残差连接与对应层的压缩向量相连。需要特别注意的是，只有与压缩信息对应的输出向量会有残差连接，而其他输出部分保持不变。如果压缩向量层的数量与解码器层的数量不匹配，我们会应用简单的基于规则的映射，通过重复增加层数或取平均减少层数。
- en: 3.3 Model Training Tasks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 模型训练任务
- en: 'For the model training tasks, we require the model to possess both contextual
    memory and contextual reasoning abilities. Therefore, we selected text reconstruction
    and text continuation tasks. Traditional autoencoding text reconstruction tasks
    Ge et al. ([2023](#bib.bib12)) are not suitable for text generation paradigms,
    so we replaced them with a random prompt text reconstruction task. Specifically,
    we randomly extract a short text segment from the encoder’s input text as a prompt
    to the decoder, requiring the decoder to reconstruct the content following the
    prompt by leveraging the compressed information and the prompt. This task enhances
    the model’s memory ability. Additionally, to strengthen the model’s reasoning
    ability, we employed text continuation tasks. Relevant formulas can be found in
    Appendix [C](#A3 "Appendix C Random Prompt Text Reconstruction Tasks ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '对于模型训练任务，我们要求模型具备上下文记忆和上下文推理能力。因此，我们选择了文本重构和文本续写任务。传统的自编码文本重构任务（Ge 等人 [2023](#bib.bib12)）不适用于文本生成范式，因此我们用随机提示文本重构任务替代了它。具体来说，我们从编码器的输入文本中随机提取一段短文本作为解码器的提示，要求解码器利用压缩的信息和提示来重构内容。这个任务增强了模型的记忆能力。此外，为了强化模型的推理能力，我们采用了文本续写任务。相关公式可以在附录
    [C](#A3 "Appendix C Random Prompt Text Reconstruction Tasks ‣ Recurrent Context
    Compression: Efficiently Expanding the Context Window of LLM") 中找到。'
- en: 3.3.1 Long Text Training Methods
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 长文本训练方法
- en: We adopt a cyclic segmentation approach for computing long sequences. During
    model inference, we only need to store the compressed vectors. However, during
    training, we also need to store the gradient information for the entire long sequence,
    which significantly exceeds memory limits. We mitigate this issue using a simple
    yet effective two-stage training method. In the first stage, we perform full-parameter
    fine-tuning with a large number of shorter sequences, allowing the encoder to
    sufficiently learn how to compress the context into a single vector. Correspondingly,
    the decoder learns to infer or reconstruct from the compressed vectors. After
    the first stage, the encoder is capable of producing standardized compressed vectors.
    At this point, we input longer sequences and freeze the encoder, enabling the
    decoder to learn how to infer from more compressed vectors. This method does not
    require complex gradient optimization algorithms or substantial GPU memory resourcesLiu
    et al. ([2023](#bib.bib19)); Chevalier et al. ([2023](#bib.bib7)); Liu et al.
    ([2023](#bib.bib19)) and can efficiently scale to longer sequences. We validated
    the effectiveness of this method on a 1M-length key retrieval task.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用循环分段方法来计算长序列。在模型推理过程中，我们只需要存储压缩后的向量。然而，在训练过程中，我们还需要存储整个长序列的梯度信息，这大大超出了内存限制。我们通过一种简单而有效的两阶段训练方法来缓解这一问题。在第一阶段，我们使用大量较短的序列进行全参数微调，使编码器能够充分学习如何将上下文压缩为一个单一的向量。相应地，解码器学习如何从压缩向量中推断或重构。在第一阶段之后，编码器能够生成标准化的压缩向量。这时，我们输入更长的序列并冻结编码器，使解码器能够学习如何从更多的压缩向量中推断。这种方法不需要复杂的梯度优化算法或大量的GPU内存资源
    Liu et al. ([2023](#bib.bib19)); Chevalier et al. ([2023](#bib.bib7)); Liu et
    al. ([2023](#bib.bib19))，且能够高效地扩展到更长的序列。我们在一个1M长度的关键检索任务上验证了这种方法的有效性。
- en: 3.3.2 Instruction Reconstruction Method
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 指令重构方法
- en: To address the poor performance when both instructions and text are compressed
    simultaneously, we propose an instruction reconstruction method. During the fine-tuning
    phase, we input the instruction as part of the context to the model’s encoder,
    with the instruction randomly placed at the beginning or end of the context. The
    decoder is then required to first reconstruct the instruction and subsequently
    answer the question based on the instruction.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决指令和文本同时压缩时性能较差的问题，我们提出了一种指令重构方法。在微调阶段，我们将指令作为上下文的一部分输入到模型的编码器中，指令被随机放置在上下文的开头或结尾。解码器首先需要重构指令，然后根据指令回答问题。
- en: 4 Experiments
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: First, we conducted text compression rate experiments using the random prompt
    text reconstruction task, selecting ICAE Ge et al. ([2023](#bib.bib12)) as the
    baseline model. Subsequently, we evaluated our method’s performance on long text
    tasks, including the passkey context block retrieval task with 1M characters and
    the long document question-answering benchmark in Longbench Bai et al. ([2023](#bib.bib1)).For
    model architecture, we used pythia-1.4b Biderman et al. ([2023](#bib.bib3)) as
    the encoder and tested mamba-1.4b Gu and Dao ([2023](#bib.bib13)), both supporting
    a 2048 context window. The decoder was pythia-1.4b. We randomly sampled about
    5 billion tokens from the pile Gao et al. ([2020](#bib.bib11)) dataset as the
    training set, concatenating these tokens into a continuous ultra-long one-dimensional
    array. The learning rate was set to 1e-4\. Training was stopped if the model failed
    to converge after one epoch or converged prematurely within an epoch, typically
    completing within 30 hours on a server with 8 A800 GPUs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用随机提示文本重构任务进行文本压缩率实验，选择 ICAE Ge et al. ([2023](#bib.bib12)) 作为基线模型。随后，我们评估了我们方法在长文本任务上的表现，包括具有1M字符的通行密钥上下文块检索任务和
    Longbench Bai et al. ([2023](#bib.bib1)) 中的长文档问答基准。对于模型架构，我们使用了 pythia-1.4b Biderman
    et al. ([2023](#bib.bib3)) 作为编码器，并测试了 mamba-1.4b Gu and Dao ([2023](#bib.bib13))，两者均支持
    2048 上下文窗口。解码器为 pythia-1.4b。我们从 pile Gao et al. ([2020](#bib.bib11)) 数据集中随机抽取了大约
    50 亿个标记作为训练集，将这些标记串联成一个连续的超长一维数组。学习率设置为 1e-4。如果模型在一个 epoch 后未能收敛或在 epoch 内提前收敛，则训练停止，通常在具有
    8 个 A800 GPU 的服务器上完成，时间通常不超过 30 小时。
- en: '![Refer to caption](img/ac4546f28ab67d07f99b5d259bb7573f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac4546f28ab67d07f99b5d259bb7573f.png)'
- en: (a) Text Reconstruction Scores of Different Models.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同模型的文本重构分数。
- en: '![Refer to caption](img/1d40fe4cd124e96f92345b18caa79b18.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1d40fe4cd124e96f92345b18caa79b18.png)'
- en: (b) Scores at Different Compression Rates
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不同压缩率的分数
- en: 'Figure 3: Text reconstruction score'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：文本重建得分
- en: 4.1 Text Reconstruction
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 文本重建
- en: 'In the initial training phase, we used a combination of random prompt text
    reconstruction tasks and Text Continuation tasks, with a ratio of 9:1\. This was
    followed by fine-tuning with the random prompt text reconstruction task. The input
    sequence length for the encoder was set to 2048 tokens, while the uncompressed
    part of the decoder had an input length of 512 tokens. For the random prompt text
    reconstruction task, the uncompressed part of the decoder’s input was a subset
    of the encoder’s input sequence, including prompts and the text to be reconstructed.
    The prompts formed the initial part of this subsequence and were excluded from
    the loss calculation; only the loss of the reconstructed text following the prompts
    was calculated. In the Text Continuation task, the decoder’s uncompressed input
    sequence was the continuation of the encoder’s input sequence. We assessed the
    model’s compression performance using the BLEU-4 score, comparing the reconstructed
    text to the actual text. To achieve this, we created 100 encoder input samples,
    each with a token length of 2048\. To ensure fairness in the scores, we selected
    5 text segments as prompts for the decoder at every 300-token interval from the
    sample. Each decoder then calculated the reconstruction score for prompts at 5
    different positions, and we averaged these scores. The prompt text length was
    about 10 tokens, and the reconstruction text length was about 500 tokens. Reconstruction
    examples can be found in Appendix [A](#A1 "Appendix A Effects of Text Reconstruction
    ‣ Recurrent Context Compression: Efficiently Expanding the Context Window of LLM").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始训练阶段，我们使用了随机提示文本重建任务和文本续写任务的组合，比例为 9:1。之后进行了随机提示文本重建任务的微调。编码器的输入序列长度设置为 2048
    个标记，而解码器的未压缩部分输入长度为 512 个标记。对于随机提示文本重建任务，解码器未压缩部分的输入是编码器输入序列的一个子集，包括提示和待重建文本。提示构成了该子序列的初始部分，并从损失计算中排除；仅计算提示后的重建文本的损失。在文本续写任务中，解码器的未压缩输入序列是编码器输入序列的续写部分。我们使用
    BLEU-4 得分评估模型的压缩性能，将重建文本与实际文本进行比较。为此，我们创建了 100 个编码器输入样本，每个样本的标记长度为 2048。为了确保评分的公平性，我们从样本中每隔
    300 个标记选择 5 个文本段作为解码器的提示。每个解码器计算了 5 个不同位置的提示的重建得分，我们对这些得分进行了平均。提示文本长度约为 10 个标记，重建文本长度约为
    500 个标记。重建示例见附录 [A](#A1 "附录 A 文本重建的影响 ‣ 循环上下文压缩：高效扩展 LLM 的上下文窗口")。
- en: 'As shown in Figure[3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4 Experiments ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM"), we compared
    different models under a 64× compression rate. Both RCC-64-mamba and RCC-64-transformer
    achieved a BLEU-4 score close to 0.82, but mamba’s training time was nearly 1.5
    times that of transformer. RCC-64-transformer-last-hidden-layer, which uses only
    the encoder’s last layer compression vectors, achieved a BLEU-4 score of approximately
    0.6\. This approach, common in traditional autoencoder models Wang et al. ([2021](#bib.bib33));
    Qin and Van Durme ([2023](#bib.bib25)); Ge et al. ([2023](#bib.bib12)), retains
    less textual information compared to using compression vectors from all layers.
    Additionally, ICAE performed poorly under a 64× compression rate, with a BLEU-4
    score of about 0.1, confirming our method’s effectiveness in preserving text information.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[3(a)](#S4.F3.sf1 "图 3 ‣ 4 实验 ‣ 循环上下文压缩：高效扩展 LLM 的上下文窗口")所示，我们在 64× 压缩率下比较了不同模型。RCC-64-mamba
    和 RCC-64-transformer 都达到了接近 0.82 的 BLEU-4 得分，但 mamba 的训练时间几乎是 transformer 的 1.5
    倍。仅使用编码器最后一层压缩向量的 RCC-64-transformer-last-hidden-layer 达到了大约 0.6 的 BLEU-4 得分。这种方法在传统自编码器模型中很常见
    Wang 等人 ([2021](#bib.bib33)); Qin 和 Van Durme ([2023](#bib.bib25)); Ge 等人 ([2023](#bib.bib12))，与使用所有层的压缩向量相比，保留的文本信息较少。此外，ICAE
    在 64× 压缩率下表现较差，BLEU-4 得分约为 0.1，证实了我们方法在保留文本信息方面的有效性。
- en: 'As shown in Figure [3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4 Experiments ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM"), we tested
    reconstruction performance under different compression rates. At a 32× compression
    rate, the BLEU-4 score reached 0.95\. At a 64× compression rate, the score dropped
    to between 0.8 and 0.85\. At a 128× compression rate, we encountered convergence
    issues, preventing BLEU-4 score computation. This indicates that higher compression
    rates increase the difficulty of text reconstruction.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3(b)](#S4.F3.sf2 "在图3 ‣ 4 实验 ‣ 循环上下文压缩：有效扩展LLM的上下文窗口")所示，我们测试了不同压缩率下的重构性能。在32×压缩率下，BLEU-4分数达到了0.95。在64×压缩率下，分数降至0.8到0.85之间。在128×压缩率下，我们遇到了收敛问题，导致无法计算BLEU-4分数。这表明更高的压缩率增加了文本重构的难度。
- en: '| Model | 32K | 128K | 256K | 512K | 1M |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 32K | 128K | 256K | 512K | 1M |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| RCC-Mamba-FT-8k | 98/100/97 | 96/98/94 | 95/93/89 | 94/95/96 | 94/96/96A
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Mamba-FT-8k | 98/100/97 | 96/98/94 | 95/93/89 | 94/95/96 | 94/96/96A
    |'
- en: '| RCC-Transformer-FT-8k | 97/95/96 | 96/97/96 | 92/96/96 | 92/89/95 | 97/96/96
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Transformer-FT-8k | 97/95/96 | 96/97/96 | 92/96/96 | 92/89/95 | 97/96/96
    |'
- en: '| RCC-Mamba-FT-32k | 100/100/100 | 100/100/100 | 100/100/100 | 100/100/100
    | 100/99/100 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Mamba-FT-32k | 100/100/100 | 100/100/100 | 100/100/100 | 100/100/100
    | 100/99/100 |'
- en: '| RCC-Transformer-FT-32k | 99/100/100 | 100/100/100 | 100/100/100 | 98/100/100
    | 100/100/100 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Transformer-FT-32k | 99/100/100 | 100/100/100 | 100/100/100 | 98/100/100
    | 100/100/100 |'
- en: '| Infini-Transformer-FT | 100/100/100 | 100/100/100 | 100/100/100 | 97/99/100
    | 96/94/100 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Infini-Transformer-FT | 100/100/100 | 100/100/100 | 100/100/100 | 97/99/100
    | 96/94/100 |'
- en: 'Table 1: The performance of the different models on the passkey retrieval tasks
    ranging from 32k to 1M sequence lengths, RCC-512-FT-8k denotes that the RCC model
    is trained with full parameters on a fine-tuning dataset with a length of 8k.
    RCC-512-FT-64K is trained on a fine-tuning dataset with a length of 64K based
    on RCC-512-FT-8k, while in this case, we freeze the encoder.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同模型在32k到1M序列长度范围内的通行码检索任务表现。RCC-512-FT-8k表示RCC模型在一个长度为8k的微调数据集上经过全参数训练。RCC-512-FT-64K是在RCC-512-FT-8k基础上，在长度为64K的微调数据集上训练的，同时在这种情况下，我们冻结了编码器。
- en: 4.2 Passkey Retrieval Task
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 通行码检索任务
- en: 'We utilize the passkey retrieval task Mohtashami and Jaggi ([2023](#bib.bib20))
    to validate the effectiveness of the two-stage training method mentioned in section
    [3.3.1](#S3.SS3.SSS1 "3.3.1 Long Text Training Methods ‣ 3.3 Model Training Tasks
    ‣ 3 Design of RCC ‣ Recurrent Context Compression: Efficiently Expanding the Context
    Window of LLM"). Additionally, we observe that our method exhibits certain length
    extrapolation capabilities, enabling it to handle compressed vector lengths during
    inference that far exceed those seen during training. This indicates that the
    compressed vectors generated by our method can be reliably recognized by the encoder,
    with minimal influence from positional encoding. Passkey retrieval task involves
    embedding a random number into a long sequence composed of repeated fixed short
    phrases, with the overall text length controlled by adjusting the number of repetitions
    of these short phrases. The task requires the model to accurately retrieve the
    hidden number from these long sequences. Detailed construction methods for passkey
    retrieval task samples are provided in Appendix [D](#A4 "Appendix D Format of
    Passkey Retrieval ‣ Recurrent Context Compression: Efficiently Expanding the Context
    Window of LLM"). In this task, we employed a compression rate of 512x. Although
    this compression rate might not be effective for reconstruction tasks, experiments
    show that the fine-tuned RCC model performs well in the passkey retrieval task.
    The model was first pre-trained on a dataset containing only the random prompt
    text reconstruction task, with an encoder input length set to 8k and a non-compressed
    decoder input length of 512\. After pre-training, we constructed nearly 30,000
    passkey retrieval task samples with context lengths of 8k and 32k, respectively.
    These samples formed the fine-tuning dataset. We conducted a two-stage fine-tuning
    process. In the first stage, we fine-tuned the entire parameter set using the
    8k context length samples. After completing the first stage, we proceeded to the
    second stage with the 32k context length samples. During this stage, we froze
    the encoder parameters to accommodate the constraints of limited available memory.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 Mohtashami 和 Jaggi 提出的**密码检索任务**（[2023](#bib.bib20)）来验证第 [3.3.1](#S3.SS3.SSS1
    "3.3.1 长文本训练方法 ‣ 3.3 模型训练任务 ‣ 3 RCC 设计 ‣ 循环上下文压缩：高效扩展 LLM 的上下文窗口") 节中提到的两阶段训练方法的有效性。此外，我们观察到我们的方法表现出一定的长度外推能力，使其能够处理推理过程中压缩的向量长度，这些长度远超训练期间看到的长度。这表明我们的方法生成的压缩向量可以被编码器可靠地识别，对位置编码的影响最小。密码检索任务涉及将一个随机数字嵌入到由重复固定短语组成的长序列中，通过调整这些短语的重复次数来控制整体文本长度。该任务要求模型准确地从这些长序列中检索出隐藏的数字。密码检索任务样本的详细构造方法见附录
    [D](#A4 "附录 D 密码检索格式 ‣ 循环上下文压缩：高效扩展 LLM 的上下文窗口")。在此任务中，我们采用了 512x 的压缩率。虽然这个压缩率可能不适用于重建任务，但实验表明，经过微调的
    RCC 模型在密码检索任务中表现良好。该模型首先在一个仅包含随机提示文本重建任务的数据集上进行预训练，编码器输入长度设置为 8k，非压缩的解码器输入长度为
    512。预训练后，我们构建了近 30,000 个上下文长度分别为 8k 和 32k 的密码检索任务样本。这些样本形成了微调数据集。我们进行了两阶段的微调过程。在第一阶段，我们使用
    8k 上下文长度样本微调了整个参数集。完成第一阶段后，我们进入第二阶段，使用 32k 上下文长度样本。在此阶段，我们冻结了编码器参数，以适应有限内存的约束。
- en: 'From Table[1](#S4.T1 "Table 1 ‣ 4.1 Text Reconstruction ‣ 4 Experiments ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM"), we can
    observe that even with the encoder using only an 8k context window, the model
    achieves almost 90% accuracy in passkey retrieval tasks up to 1000k, demonstrating
    the strong length extrapolation capabilities of our model. After the second stage
    of fine-tuning with sequences up to 32k, the model achieves nearly 100% performance
    on passkey retrieval tasks up to 1000k, proving the effectiveness of our two-stage
    training method, even with the encoder parameters frozen at this stage. Table[1](#S4.T1
    "Table 1 ‣ 4.1 Text Reconstruction ‣ 4 Experiments ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM") also shows that our method is
    highly competitive compared to recent similar work like Infini-attention Munkhdalai
    et al. ([2024](#bib.bib23)). Unlike Infini-attention, our method can be fine-tuned
    on existing open-source LLMs with a small amount of data, without requiring the
    reconstruction of the LLM model and pre-training with hundreds of billions of
    tokens.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[1](#S4.T1 "表 1 ‣ 4.1 文本重建 ‣ 4 实验 ‣ 递归上下文压缩：高效扩展 LLM 的上下文窗口") 中，我们可以观察到，即使编码器仅使用
    8k 上下文窗口，模型在高达 1000k 的密码检索任务中也能达到近 90% 的准确率，展示了我们模型强大的长度外推能力。在经过第二阶段微调后，处理序列长度高达
    32k 的模型在高达 1000k 的密码检索任务中几乎达到了 100% 的表现，证明了我们的两阶段训练方法的有效性，即使在此阶段编码器参数被冻结的情况下。表[1](#S4.T1
    "表 1 ‣ 4.1 文本重建 ‣ 4 实验 ‣ 递归上下文压缩：高效扩展 LLM 的上下文窗口") 还显示，我们的方法与 Infini-attention
    Munkhdalai 等人 ([2024](#bib.bib23)) 等近期类似工作相比具有很强的竞争力。与 Infini-attention 不同，我们的方法可以在现有开源
    LLM 上用少量数据进行微调，无需重建 LLM 模型和用数百亿个标记进行预训练。
- en: '| Method | 0-2k | 2-4k | 4-8k | 8k+ | average |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 0-2k | 2-4k | 4-8k | 8k+ | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Pythia-SFT | 30.54 | - | - | - | - |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-SFT | 30.54 | - | - | - | - |'
- en: '| Pythia-No-SFT | 4.41 | - | - | - | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Pythia-No-SFT | 4.41 | - | - | - | - |'
- en: '| RCC-Ins-Reconstruction | 28.12 | 23.37 | 21.24 | 17.72 | 22.61 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Ins-Reconstruction | 28.12 | 23.37 | 21.24 | 17.72 | 22.61 |'
- en: '| RCC-Ins-Human | 25.36 | 25.15 | 23.63 | 20.48 | 23.15 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Ins-Human | 25.36 | 25.15 | 23.63 | 20.48 | 23.15 |'
- en: '| RCC-Ins-Compress | 18.77 | 21.36 | 20.02 | 18.14 | 19.61 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| RCC-Ins-Compress | 18.77 | 21.36 | 20.02 | 18.14 | 19.61 |'
- en: 'Table 2: Scores of different models on the task of Document QA.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同模型在文档 QA 任务上的得分。
- en: 4.3 Long-Text Benchmark Evaluation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 长文本基准测试评估
- en: 4.3.1 Evaluation Dataset
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 评估数据集
- en: 'LongBench Bai et al. ([2023](#bib.bib1)) is a benchmark designed to evaluate
    the capabilities of large language models in understanding long contexts. To assess
    our model’s performance on texts of different lengths, we selected the LongBench-E
    set for evaluation because it evenly covers test samples of various length ranges,
    allowing us to analyze the impact of length variation on performance. Due to limitations
    in the fine-tuning dataset, our work focuses on using the single-document QA and
    multi-document QA tasks for evaluation. These two document QA tasks consist of
    four subtasks, with each task containing between 150 to 300 samples. Detailed
    information on the evaluation dataset can be found in Appendix [E](#A5 "Appendix
    E Fine-Tuning Datasets and Model-Generated Cases ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LongBench Bai 等人 ([2023](#bib.bib1)) 是一个基准测试，旨在评估大型语言模型理解长文本的能力。为了评估我们模型在不同长度文本上的表现，我们选择了
    LongBench-E 集合进行评估，因为它均匀覆盖了各种长度范围的测试样本，使我们能够分析长度变化对性能的影响。由于微调数据集的限制，我们的工作重点放在使用单文档
    QA 和多文档 QA 任务进行评估。这两个文档 QA 任务包括四个子任务，每个任务包含 150 到 300 个样本。评估数据集的详细信息可以在附录 [E](#A5
    "附录 E 微调数据集和模型生成案例 ‣ 递归上下文压缩：高效扩展 LLM 的上下文窗口") 中找到。
- en: 4.3.2 Instruction Fine-Tuning
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 指令微调
- en: First, we conducted pretraining on the random prompt text reconstruction task
    and the text continuation task with a ratio of 1:9\. Similar to the two-stage
    training method, the first stage involves training with full parameters on texts
    with a length of 2k. In the second stage, the encoder is frozen, and training
    is conducted on texts with a length of 16k. For question answering instruction
    fine-tuning, we used the Prompt-with-Context (PwC) Ge et al. ([2023](#bib.bib12))
    and hotpotQA Yang et al. ([2018](#bib.bib37)) datasets. These datasets include
    context with instructions and outputs, teaching the model to use context for answering
    questions rather than relying solely on internal knowledge. We concatenated the
    context and instructions as the encoder’s input, while the instructions and output
    results formed the decoder’s uncompressed input. We repeated the instructions
    twice to train the encoder to reconstruct instructions, enhancing mixed instruction
    and context text effectiveness during inference. The PwC dataset has 240k samples,
    and hotpotQA has 90k samples. Additionally, to improve instruction reconstruction
    and maintain the decoder’s instruction-following capability, we randomly selected
    50k instruction samples from the orcal dataset Mukherjee et al. ([2023](#bib.bib22)).
    These samples lack explicit context fields and typically mix instructions with
    context. We input the instructions as context to the encoder and as instructions
    and outputs to the decoder. During fine-tuning, the encoder’s input context length
    was set to 2048 tokens, the uncompressed part of the decoder’s input was set to
    512 tokens, and the compression rate remained 32.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在随机提示文本重建任务和文本续写任务上进行了预训练，比例为1:9。类似于两阶段训练方法，第一阶段在长度为2k的文本上进行全参数训练。在第二阶段，编码器被冻结，训练在长度为16k的文本上进行。对于问题回答指令微调，我们使用了Ge等人([2023](#bib.bib12))的Prompt-with-Context
    (PwC) 和Yang等人([2018](#bib.bib37))的hotpotQA数据集。这些数据集包括带有指令和输出的上下文，教会模型使用上下文回答问题，而不是仅仅依赖内部知识。我们将上下文和指令串联作为编码器的输入，而指令和输出结果组成解码器的未压缩输入。我们重复了指令两次，以训练编码器重建指令，增强推理过程中混合指令和上下文文本的有效性。PwC数据集有240k个样本，hotpotQA有90k个样本。此外，为了提高指令重建能力并保持解码器的指令跟随能力，我们从Mukherjee等人([2023](#bib.bib22))的orcal数据集中随机选择了50k个指令样本。这些样本缺乏明确的上下文字段，通常将指令与上下文混合。我们将指令作为上下文输入给编码器，作为指令和输出输入给解码器。在微调过程中，编码器的输入上下文长度设置为2048个标记，解码器的未压缩输入部分设置为512个标记，压缩率保持在32。
- en: 4.3.3 Evaluation
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 评估
- en: 'We fine-tuned Pythia-1.4b with instruction pairs constructed from PwC, hotpotQA,
    and some ORCA data to ensure it follows instructions. Due to limitations in the
    fine-tuning dataset, we only selected document QA tasks. We evaluated the fine-tuned
    Pythia-1.4b and our model using LongBench’sBai et al. ([2023](#bib.bib1)) automated
    evaluation tools, covering two document QA tasks as shown in Appendix [E](#A5
    "Appendix E Fine-Tuning Datasets and Model-Generated Cases ‣ Recurrent Context
    Compression: Efficiently Expanding the Context Window of LLM"). As shown in Table
    [2](#S4.T2 "Table 2 ‣ 4.2 Passkey Retrieval Task ‣ 4 Experiments ‣ Recurrent Context
    Compression: Efficiently Expanding the Context Window of LLM"), the fine-tuned
    Pythia-1.4b significantly improved in following instructions. Notably, Pythia-1.4b
    supports a maximum sequence length of 2048 tokens, so we only used samples under
    2k tokens for its evaluation. Our method supports LongBench’s maximum input length
    of 15k tokens within the effective window length of the decoder. We further evaluated
    the following types for our method:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用从PwC、hotpotQA和部分ORCA数据中构建的指令对微调了Pythia-1.4b，以确保它遵循指令。由于微调数据集的限制，我们仅选择了文档QA任务。我们使用LongBench的Bai等人([2023](#bib.bib1))自动评估工具评估了微调后的Pythia-1.4b和我们的模型，涵盖了附录[E](#A5
    "Appendix E Fine-Tuning Datasets and Model-Generated Cases ‣ Recurrent Context
    Compression: Efficiently Expanding the Context Window of LLM")中显示的两个文档QA任务。如表[2](#S4.T2
    "Table 2 ‣ 4.2 Passkey Retrieval Task ‣ 4 Experiments ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM")所示，微调后的Pythia-1.4b在跟随指令方面显著提高。值得注意的是，Pythia-1.4b支持最长2048个标记的序列长度，因此我们仅使用了少于2k标记的样本进行评估。我们的方法支持LongBench的最大输入长度为15k标记，位于解码器的有效窗口长度内。我们进一步评估了我们方法的以下类型：'
- en: RCC-Ins-Reconstruction, which reconstructs instructions from compressed vectors
    and responds using instruction reconstruction techniques (Table 2), scored 28.12
    at a length of 2k. This score is competitive with Pythia-sft, demonstrating that
    RCC can maintain high-quality inference even with a compression ratio of up to
    32x. This method’s average score surpasses that of RCC-Ins-Compress, which compresses
    both instructions and context, verifying the effectiveness of instruction reconstruction.
    Due to the fine-tuning dataset being limited to 2k tokens, RCC-Ins-Reconstruction
    performs poorly in instruction reconstruction when handling longer samples.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: RCC-Ins-Reconstruction通过从压缩向量重建指令，并使用指令重建技术进行响应（表2），在2k长度下得分为28.12。这个分数与Pythia-sft具有竞争力，显示了RCC即使在高达32倍的压缩比下也能保持高质量推理。该方法的平均得分超过了RCC-Ins-Compress的得分，后者同时压缩指令和上下文，验证了指令重建的有效性。由于微调数据集限制为2k
    tokens，RCC-Ins-Reconstruction在处理更长样本时指令重建表现较差。
- en: RCC-Ins-Human directly inputs real instruction texts into the decoder (Table
    2). Compared to the performance fluctuations of RCC-Ins-Reconstruction with increasing
    sample length, RCC-Ins-Human exhibits more stable performance, especially maintaining
    efficient inference at lengths beyond 8k. We attribute this to the decline in
    instruction reconstruction quality in RCC-Ins-Reconstruction for long texts, whereas
    RCC-Ins-Human employs fixed instructions, unaffected by length.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: RCC-Ins-Human直接将真实指令文本输入解码器（表2）。与RCC-Ins-Reconstruction在样本长度增加时性能波动相比，RCC-Ins-Human表现出更稳定的性能，特别是在超过8k长度时保持高效推理。我们将其归因于RCC-Ins-Reconstruction在处理长文本时指令重建质量的下降，而RCC-Ins-Human使用固定的指令，不受长度影响。
- en: RCC-Ins-Compress compresses both context and instructions simultaneously (Table
    2). The encoder receives concatenated texts, and the decoder is only prompted
    with brief information, such as "
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: RCC-Ins-Compress同时压缩上下文和指令（表2）。编码器接收拼接的文本，而解码器仅提示简要信息，如“
- en: 'n Answer:". This strategy’s limited capability to recognize instructions and
    context results in an average score as low as 19.61, particularly underperforming
    compared to RCC-Ins-Human and RCC-Ins-Reconstruction in samples under 8k. However,
    for ultra-long samples (8k+), its performance converges with RCC-Ins-Reconstruction,
    likely due to the latter’s deficiencies in instruction reconstruction at extreme
    lengths. Specific model generation results can be found in Appendix [E](#A5 "Appendix
    E Fine-Tuning Datasets and Model-Generated Cases ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'n 回答:”。这种策略识别指令和上下文的能力有限，导致其平均得分低至19.61，尤其是在8k以下的样本中表现不如RCC-Ins-Human和RCC-Ins-Reconstruction。然而，对于超长样本（8k+），其性能与RCC-Ins-Reconstruction趋于一致，这可能是由于后者在极端长度下指令重建的不足。具体的模型生成结果可以在附录
    [E](#A5 "Appendix E Fine-Tuning Datasets and Model-Generated Cases ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM") 中找到。'
- en: 'In Figure [1](#S0.F1 "Figure 1 ‣ Recurrent Context Compression: Efficiently
    Expanding the Context Window of LLM"), we observe that when RCC processes text
    up to 16k tokens, the GPU memory usage only increases by approximately 0.5 GB.
    In contrast, the original Pythia-1.4b experiences a 2 GB increase in memory usage
    for 2k token text. When processing 16k token text, Pythia-1.4b’s total memory
    usage will be twice that of our method. Since we use a compression rate of 32x,
    as text length increases, RCC can save up to nearly 32x in storage space. Although
    the parameter count of RCC’s encoder and decoder is twice that of Pythia, the
    impact of the model’s parameter count on storage space significantly diminishes
    with increased text length.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [1](#S0.F1 "Figure 1 ‣ Recurrent Context Compression: Efficiently Expanding
    the Context Window of LLM") 中，我们观察到，当RCC处理高达16k的token文本时，GPU内存使用量仅增加约0.5 GB。相比之下，原始的Pythia-1.4b在处理2k
    token文本时，内存使用量增加了2 GB。当处理16k token文本时，Pythia-1.4b的总内存使用量将是我们方法的两倍。由于我们使用了32倍的压缩率，随着文本长度的增加，RCC可以节省高达近32倍的存储空间。虽然RCC的编码器和解码器的参数数量是Pythia的两倍，但随着文本长度的增加，模型参数数量对存储空间的影响显著减少。'
- en: 5 Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: Utilizing compression techniques to mitigate challenges in long-text training
    and inference has proven to be a highly promising strategy. Our work quantitatively
    analyzes the impact of different context lengths on compression performance, while
    also achieving higher compression rates than previous methods, thereby significantly
    enhancing the ability of large language models (LLMs) to handle long texts. The
    RCC method demonstrated outstanding performance across multiple test tasks, particularly
    in context compression reconstruction, long-document question answering, and key-context
    block retrieval tasks with sequences up to 1 million tokens. Additionally, we
    analyzed the issues arising from simultaneous compression of context and instructions
    and introduced an instruction reconstruction method that effectively alleviated
    these problems. Furthermore, to address the substantial resource consumption of
    long-text training, we proposed a staged training strategy that further improved
    the efficiency of the model in handling long-text training.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 利用压缩技术来缓解长文本训练和推理中的挑战已被证明是一种非常有前景的策略。我们的工作定量分析了不同上下文长度对压缩性能的影响，同时实现了比以往方法更高的压缩率，从而显著增强了大型语言模型（LLMs）处理长文本的能力。RCC方法在多个测试任务中表现出色，特别是在上下文压缩重建、长文档问答以及序列长度达到100万标记的关键上下文块检索任务中。此外，我们还分析了同时压缩上下文和指令所带来的问题，并引入了一种指令重建方法，有效缓解了这些问题。此外，为了应对长文本训练的资源消耗，我们提出了一种分阶段训练策略，进一步提高了模型处理长文本训练的效率。
- en: 6 Limitations
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: 'The RCC method has significantly advanced text compression efficiency and long-document
    question answering, but it has limitations. For example, RCC risks errors during
    instruction reconstruction, and if instructions are too long, the decoder may
    struggle to reconstruct them within the limited window. In future research, we
    plan to adopt a hybrid training approach: using instruction compression for long
    instructions and instruction reconstruction for short ones to achieve results
    comparable to manually input instructions.Additionally, the lack of long-text
    instruction fine-tuning data has caused performance bottlenecks for RCC. Our experiments
    show the critical impact of training data on the model’s performance. The effectiveness
    of language models fine-tuned with instructions depends largely on the quality
    and coverage of those instructions. These issues provide clear directions for
    our future research.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: RCC方法显著提高了文本压缩效率和长文档问答能力，但也存在局限性。例如，RCC在指令重建过程中存在出错风险，如果指令过长，解码器可能难以在有限的窗口内重建这些指令。在未来的研究中，我们计划采用混合训练方法：对长指令使用指令压缩，对短指令使用指令重建，以实现与手动输入指令相媲美的结果。此外，缺乏长文本指令微调数据导致了RCC的性能瓶颈。我们的实验表明，训练数据对模型性能有着关键性的影响。语言模型在指令微调中的有效性在很大程度上依赖于这些指令的质量和覆盖范围。这些问题为我们的未来研究提供了明确的方向。
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context
    understanding. *arXiv preprint arXiv:2308.14508*.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai等人（2023）Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian
    Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, 和 Juanzi
    Li。2023年。Longbench: A bilingual, multitask benchmark for long context understanding。*arXiv预印本
    arXiv:2308.14508*。'
- en: 'Beltagy et al. (2020) Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.
    [Longformer: The long-document transformer](https://arxiv.org/abs/2004.05150).
    *Preprint*, arXiv:2004.05150.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beltagy等人（2020）Iz Beltagy, Matthew E. Peters, 和 Arman Cohan。2020年。[Longformer:
    The long-document transformer](https://arxiv.org/abs/2004.05150)。*预印本*，arXiv:2004.05150。'
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der
    Wal. 2023. [Pythia: A suite for analyzing large language models across training
    and scaling](https://arxiv.org/abs/2304.01373). *Preprint*, arXiv:2304.01373.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Biderman等人（2023）Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie
    Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, 和 Oskar van
    der Wal。2023年。[Pythia: A suite for analyzing large language models across training
    and scaling](https://arxiv.org/abs/2304.01373)。*预印本*，arXiv:2304.01373。'
- en: Bulatov et al. (2022) Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.
    Recurrent memory transformer. In *Advances in Neural Information Processing Systems*,
    volume 35, pages 11079–11091\. Curran Associates, Inc.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bulatov et al. (2022) Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. 2022.
    循环记忆变换器。发表于 *神经信息处理系统进展*，第35卷，第11079–11091页。Curran Associates, Inc.
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. [Extending context window of large language models via positional
    interpolation](https://arxiv.org/abs/2306.15595). *Preprint*, arXiv:2306.15595.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. [通过位置插值扩展大语言模型的上下文窗口](https://arxiv.org/abs/2306.15595)。*预印本*，arXiv:2306.15595。
- en: 'Chen et al. (2024) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2024. [Longlora: Efficient fine-tuning of long-context
    large language models](https://arxiv.org/abs/2309.12307). *Preprint*, arXiv:2309.12307.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. (2024) Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian
    Liu, Song Han, and Jiaya Jia. 2024. [Longlora: 高效微调长上下文大语言模型](https://arxiv.org/abs/2309.12307)。*预印本*，arXiv:2309.12307。'
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 2023. Adapting language models to compress contexts. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    3829–3846, Singapore. Association for Computational Linguistics.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 2023. 调整语言模型以压缩上下文。发表于 *2023年自然语言处理实证方法会议论文集*，第3829–3846页，新加坡。计算语言学协会。
- en: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    2019. [Generating long sequences with sparse transformers](https://arxiv.org/abs/1904.10509).
    *Preprint*, arXiv:1904.10509.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child et al. (2019) Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    2019. [生成长序列的稀疏变换器](https://arxiv.org/abs/1904.10509)。*预印本*，arXiv:1904.10509。
- en: 'Dao (2023) Tri Dao. 2023. [Flashattention-2: Faster attention with better parallelism
    and work partitioning](https://arxiv.org/abs/2307.08691). *Preprint*, arXiv:2307.08691.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dao (2023) Tri Dao. 2023. [Flashattention-2: 更快的注意力机制，改进了并行性和工作划分](https://arxiv.org/abs/2307.08691)。*预印本*，arXiv:2307.08691。'
- en: Feng et al. (2024) Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama
    Ahmed, Yoshua Bengio, and Greg Mori. 2024. [Attention as an rnn](https://arxiv.org/abs/2405.13956).
    *Preprint*, arXiv:2405.13956.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2024) Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed
    Osama Ahmed, Yoshua Bengio, and Greg Mori. 2024. [注意力作为rnn](https://arxiv.org/abs/2405.13956)。*预印本*，arXiv:2405.13956。
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. 2020. [The pile: An 800gb dataset of diverse text for
    language modeling](https://arxiv.org/abs/2101.00027). *Preprint*, arXiv:2101.00027.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. 2020. [The pile: 用于语言建模的800GB多样文本数据集](https://arxiv.org/abs/2101.00027)。*预印本*，arXiv:2101.00027。'
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    In-context autoencoder for context compression in a large language model. In *International
    Conference on Learning Representations*.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    用于大语言模型上下文压缩的上下文自动编码器。发表于 *国际学习表征会议*。
- en: 'Gu and Dao (2023) Albert Gu and Tri Dao. 2023. [Mamba: Linear-time sequence
    modeling with selective state spaces](https://arxiv.org/abs/2312.00752). *Preprint*,
    arXiv:2312.00752.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu and Dao (2023) Albert Gu and Tri Dao. 2023. [Mamba: 线性时间序列建模与选择性状态空间](https://arxiv.org/abs/2312.00752)。*预印本*，arXiv:2312.00752。'
- en: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    [Long short-term memory](https://arxiv.org/abs/https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf).
    *Neural Computation*, 9(8):1735–1780.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter and Schmidhuber (1997) Sepp Hochreiter and Jürgen Schmidhuber. 1997.
    [长短期记忆](https://arxiv.org/abs/https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf)。*神经计算*，9(8):1735–1780。
- en: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023a. [Mistral 7b](https://arxiv.org/abs/2310.06825). *Preprint*,
    arXiv:2310.06825.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023a) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023a. [Mistral 7b](https://arxiv.org/abs/2310.06825)。*预印本*，arXiv:2310.06825。
- en: 'Jiang et al. (2023b) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li,
    Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b. [Longllmlingua: Accelerating and
    enhancing llms in long context scenarios via prompt compression](https://arxiv.org/abs/2310.06839).
    *Preprint*, arXiv:2310.06839.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等人（2023b）惠强·江、乾辉·吴、旭方·罗、东升·李、林志远、玉清·杨、丽丽·邱。2023b。《[Longllmlingua: 通过提示压缩加速和增强长上下文场景中的大语言模型](https://arxiv.org/abs/2310.06839)》。*预印本*，arXiv:2310.06839。'
- en: Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023.
    [Compressing context to enhance inference efficiency of large language models](https://doi.org/10.18653/v1/2023.emnlp-main.391).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 6342–6353, Singapore. Association for Computational Linguistics.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023）余成·李、博·董、弗兰克·盖林和程华·林。2023年。《[压缩上下文以提升大型语言模型的推理效率](https://doi.org/10.18653/v1/2023.emnlp-main.391)》。见于《*2023年自然语言处理实证方法会议论文集*》，第6342–6353页，新加坡。计算语言学协会。
- en: Liu et al. (2024) Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel. 2024.
    [World model on million-length video and language with blockwise ringattention](https://arxiv.org/abs/2402.08268).
    *Preprint*, arXiv:2402.08268.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024）郝·刘、威尔逊·严、马泰·扎哈里亚和皮特·阿贝尔。2024年。《[百万长度视频与语言的世界模型和块状环形注意力](https://arxiv.org/abs/2402.08268)》。*预印本*，arXiv:2402.08268。
- en: Liu et al. (2023) Hao Liu, Matei Zaharia, and Pieter Abbeel. 2023. [Ring attention
    with blockwise transformers for near-infinite context](https://arxiv.org/abs/2310.01889).
    *Preprint*, arXiv:2310.01889.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2023）郝·刘、马泰·扎哈里亚和皮特·阿贝尔。2023年。《[环形注意力与块状变压器用于接近无限上下文](https://arxiv.org/abs/2310.01889)》。*预印本*，arXiv:2310.01889。
- en: Mohtashami and Jaggi (2023) Amirkeivan Mohtashami and Martin Jaggi. 2023. Random-access
    infinite context length for transformers. In *Advances in Neural Information Processing
    Systems*, volume 36, pages 54567–54585\. Curran Associates, Inc.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohtashami 和 Jaggi（2023）阿米尔凯文·莫赫塔沙米和马丁·贾吉。2023年。《变压器的随机访问无限上下文长度》。见于《*神经信息处理系统进展*》，第36卷，第54567–54585页。Curran
    Associates, Inc.
- en: Mu et al. (2023) Jesse Mu, Xiang Li, and Noah Goodman. 2023. Learning to compress
    prompts with gist tokens. In *Advances in Neural Information Processing Systems*,
    volume 36, pages 19327–19352\. Curran Associates, Inc.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等人（2023）杰西·穆、向李、诺亚·古德曼。2023年。《学习使用要旨令牌压缩提示》。见于《*神经信息处理系统进展*》，第36卷，第19327–19352页。Curran
    Associates, Inc.
- en: 'Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar,
    Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. [Orca: Progressive learning
    from complex explanation traces of gpt-4](https://arxiv.org/abs/2306.02707). *Preprint*,
    arXiv:2306.02707.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mukherjee 等人（2023）苏布哈布拉塔·穆克吉、阿林达姆·米特拉、加内什·贾瓦哈尔、萨哈杰·阿加瓦尔、哈米德·帕兰吉和艾哈迈德·阿瓦达拉赫。2023年。《[Orca:
    从GPT-4的复杂解释轨迹中渐进学习](https://arxiv.org/abs/2306.02707)》。*预印本*，arXiv:2306.02707。'
- en: 'Munkhdalai et al. (2024) Tsendsuren Munkhdalai, Manaal Faruqui, and Siddharth
    Gopal. 2024. [Leave no context behind: Efficient infinite context transformers
    with infini-attention](https://arxiv.org/abs/2404.07143). *Preprint*, arXiv:2404.07143.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Munkhdalai 等人（2024）Tsendsuren Munkhdalai、Manaal Faruqui 和 Siddharth Gopal。2024年。《[不留上下文：具有无限注意力的高效无限上下文变压器](https://arxiv.org/abs/2404.07143)》。*预印本*，arXiv:2404.07143。
- en: 'Peng et al. (2023) Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel
    Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella,
    Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan
    Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri,
    Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Bolun Wang, Johan S.
    Wind, Stanislaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou,
    Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. [Rwkv: Reinventing rnns for the
    transformer era](https://arxiv.org/abs/2305.13048). *Preprint*, arXiv:2305.13048.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng 等人（2023）博·彭、埃里克·阿尔卡伊德、昆廷·安东尼、阿隆·阿尔巴拉克、塞缪尔·阿卡丁霍、斯特拉·比德曼、欢启·曹、辛·程、迈克尔·钟、马特奥·格雷拉、克兰提·基兰
    GV、许正·赫、浩文·侯、佳聚·林、普热梅斯瓦夫·卡兹耶科、扬·科孔、嘉明·孔、巴特洛米·科普特拉、海登·劳、克里希纳·斯里·伊普斯特·曼特里、费迪南德·穆姆、佐藤·厚志、广宇·宋、向如·唐、博伦·王、约翰·S·温德、斯坦尼斯瓦夫·沃兹尼亚克、瑞崇·张、振远·张、启航·赵、彭·周、青华·周、简·朱和瑞杰·朱。2023年。《[Rwkv:
    在变压器时代重新发明RNN](https://arxiv.org/abs/2305.13048)》。*预印本*，arXiv:2305.13048。'
- en: 'Qin and Van Durme (2023) Guanghui Qin and Benjamin Van Durme. 2023. Nugget:
    neural agglomerative embeddings of text. In *Proceedings of the 40th International
    Conference on Machine Learning*, ICML’23\. JMLR.org.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 和 Van Durme（2023）光辉·秦和本杰明·范·杜梅。2023年。《Nugget: 文本的神经聚合嵌入》。见于《第40届国际机器学习大会论文集》，ICML’23。JMLR.org。'
- en: Rae et al. (2019) Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and Timothy P.
    Lillicrap. 2019. [Compressive transformers for long-range sequence modelling](https://arxiv.org/abs/1911.05507).
    *Preprint*, arXiv:1911.05507.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2019） Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar 和 Timothy P.
    Lillicrap。2019。 [用于长距离序列建模的压缩变换器](https://arxiv.org/abs/1911.05507)。*预印本*，arXiv:1911.05507。
- en: Ren et al. (2023) Siyu Ren, Qi Jia, and Kenny Zhu. 2023. [Context compression
    for auto-regressive transformers with sentinel tokens](https://doi.org/10.18653/v1/2023.emnlp-main.794).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 12860–12867, Singapore. Association for Computational Linguistics.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren 等（2023） Siyu Ren, Qi Jia 和 Kenny Zhu。2023。 [带有哨兵标记的自回归变换器的上下文压缩](https://doi.org/10.18653/v1/2023.emnlp-main.794)。收录于
    *2023年自然语言处理实证方法会议论文集*，第12860–12867页，新加坡。计算语言学协会。
- en: Snell et al. (2022) Charlie Snell, Dan Klein, and Ruiqi Zhong. 2022. Learning
    by distilling context. In *International Conference on Learning Representations*.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell 等（2022） Charlie Snell, Dan Klein 和 Ruiqi Zhong。2022。通过提炼上下文进行学习。收录于 *国际学习表示会议*。
- en: 'Sun et al. (2023) Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia,
    Jilong Xue, Jianyong Wang, and Furu Wei. 2023. [Retentive network: A successor
    to transformer for large language models](https://arxiv.org/abs/2307.08621). *Preprint*,
    arXiv:2307.08621.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023） Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong
    Xue, Jianyong Wang 和 Furu Wei。2023。 [保留网络：大语言模型的变革者](https://arxiv.org/abs/2307.08621)。*预印本*，arXiv:2307.08621。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023a. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *Preprint*, arXiv:2302.13971.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023a） Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。2023a。
    [Llama: 开放和高效的基础语言模型](https://arxiv.org/abs/2302.13971)。*预印本*，arXiv:2302.13971。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b.
    [Llama 2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023b） Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom。2023b。 [Llama
    2: 开放的基础和微调对话模型](https://arxiv.org/abs/2307.09288)。*预印本*，arXiv:2307.09288。'
- en: 'Tworkowski et al. (2023) Szymon Tworkowski, Konrad Staniszewski, Mikoł aj Pacek,
    Yuhuai Wu, Henryk Michalewski, and Piotr Mił oś. 2023. Focused transformer: Contrastive
    training for context scaling. In *Advances in Neural Information Processing Systems*,
    volume 36, pages 42661–42688\. Curran Associates, Inc.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tworkowski 等（2023） Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai
    Wu, Henryk Michalewski 和 Piotr Miłos。2023。聚焦变换器：用于上下文扩展的对比训练。收录于 *神经信息处理系统进展*，第36卷，第42661–42688页。Curran
    Associates, Inc.
- en: 'Wang et al. (2021) Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. [TSDAE:
    Using transformer-based sequential denoising auto-encoderfor unsupervised sentence
    embedding learning](https://doi.org/10.18653/v1/2021.findings-emnlp.59). In *Findings
    of the Association for Computational Linguistics: EMNLP 2021*, pages 671–688,
    Punta Cana, Dominican Republic. Association for Computational Linguistics.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2021）Kexin Wang、Nils Reimers 和 Iryna Gurevych。2021年。[TSDAE: 使用基于变换器的序列去噪自编码器进行无监督句子嵌入学习](https://doi.org/10.18653/v1/2021.findings-emnlp.59)。见于
    *计算语言学协会的发现：EMNLP 2021*，第671-688页，多米尼加共和国蓬塔卡纳。计算语言学协会。'
- en: Wingate et al. (2022) David Wingate, Mohammad Shoeybi, and Taylor Sorensen.
    2022. Prompt compression and contrastive conditioning for controllability and
    toxicity reduction in language models. In *Conference on Empirical Methods in
    Natural Language Processing*.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wingate 等（2022）David Wingate、Mohammad Shoeybi 和 Taylor Sorensen。2022年。语言模型中的提示压缩和对比条件用于控制性和毒性减少。见于
    *自然语言处理经验方法会议*。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Transformers: State-of-the-art natural language
    processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6). In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, pages 38–45, Online. Association for Computational Linguistics.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等（2020）Thomas Wolf、Lysandre Debut、Victor Sanh、Julien Chaumond、Clement
    Delangue、Anthony Moi、Pierric Cistac、Tim Rault、Rémi Louf、Morgan Funtowicz、Joe Davison、Sam
    Shleifer、Patrick von Platen、Clara Ma、Yacine Jernite、Julien Plu、Canwen Xu、Teven
    Le Scao、Sylvain Gugger、Mariama Drame、Quentin Lhoest 和 Alexander M. Rush。2020年。[Transformers:
    State-of-the-art natural language processing](https://www.aclweb.org/anthology/2020.emnlp-demos.6)。见于
    *2020年自然语言处理经验方法会议：系统演示论文集*，第38-45页，在线。计算语言学协会。'
- en: Wu et al. (2022) Yuhuai Wu, Markus N. Rabe, DeLesley Hutchins, and Christian
    Szegedy. 2022. Memorizing transformers. In *International Conference on Learning
    Representations*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等（2022）Yuhuai Wu、Markus N. Rabe、DeLesley Hutchins 和 Christian Szegedy。2022年。记忆变换器。见于
    *国际学习表征会议*。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. [HotpotQA: A dataset
    for diverse, explainable multi-hop question answering](https://doi.org/10.18653/v1/D18-1259).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 2369–2380, Brussels, Belgium. Association for Computational
    Linguistics.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2018）Zhilin Yang、Peng Qi、Saizheng Zhang、Yoshua Bengio、William Cohen、Ruslan
    Salakhutdinov 和 Christopher D. Manning。2018年。[HotpotQA: 一个用于多跳问答的多样化、可解释的数据集](https://doi.org/10.18653/v1/D18-1259)。见于
    *2018年自然语言处理经验方法会议论文集*，第2369-2380页，比利时布鲁塞尔。计算语言学协会。'
- en: '![Refer to caption](img/0a9893b38c505c9df479ced0e5700dc4.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0a9893b38c505c9df479ced0e5700dc4.png)'
- en: 'Figure 4: When the GPU memory approaches 60GB, the memory occupation of different
    models. Left: Pythia-1.4b, Right: RCC model using Pythia-1.4b for both encoder
    and decoder. Both models utilize FlashAttention-2 Dao ([2023](#bib.bib9)).'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：当GPU内存接近60GB时，不同模型的内存占用情况。左侧：Pythia-1.4b，右侧：同时使用Pythia-1.4b作为编码器和解码器的RCC模型。这两种模型都利用了FlashAttention-2
    Dao（[2023](#bib.bib9)）。
- en: Appendix A Effects of Text Reconstruction
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 文本重建效果
- en: 'The example of our method’s reconstruction effect at 32x compression rate is
    shown below. As the table [3](#A4.T3 "Table 3 ‣ Appendix D Format of Passkey Retrieval
    ‣ Recurrent Context Compression: Efficiently Expanding the Context Window of LLM")
    indicates, our method has almost completely reconstructed the context.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们方法在32倍压缩率下的重建效果示例如下。正如表 [3](#A4.T3 "Table 3 ‣ Appendix D Format of Passkey
    Retrieval ‣ Recurrent Context Compression: Efficiently Expanding the Context Window
    of LLM") 所示，我们的方法几乎完全重建了上下文。'
- en: Appendix B GPU Memory Consumption Analysis
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B GPU内存消耗分析
- en: 'We ran the model on an A800 GPU using HuggingFace’s Transformers library Wolf
    et al. ([2020](#bib.bib35)) and tested the GPU memory consumption of different
    models. As shown in Figure [4](#A0.F4 "Figure 4 ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM"), the GPU memory usage of Pythia-1.4b
    increases rapidly with the length of the input context. When the context window
    reaches 64k, the model’s GPU memory usage exceeds 60GB. RCC-1.4 x 2b, where both
    the encoder and decoder are Pythia-1.4b models with a compression rate of 32,
    shows that when its GPU memory usage exceeds 60GB, it processes a context length
    close to 2048k tokens. This is 30 times the length Pythia-1.4b can handle, nearly
    matching the compression rate.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 A800 GPU 上使用 HuggingFace 的 Transformers 库 Wolf 等人 ([2020](#bib.bib35))
    运行了模型，并测试了不同模型的 GPU 内存消耗。如图 [4](#A0.F4 "Figure 4 ‣ Recurrent Context Compression:
    Efficiently Expanding the Context Window of LLM") 所示，Pythia-1.4b 的 GPU 内存使用量随着输入上下文的长度迅速增加。当上下文窗口达到
    64k 时，模型的 GPU 内存使用量超过 60GB。RCC-1.4 x 2b，其中编码器和解码器都是具有 32 的压缩率的 Pythia-1.4b 模型，当其
    GPU 内存使用量超过 60GB 时，它处理的上下文长度接近 2048k 个 token。这是 Pythia-1.4b 可处理长度的 30 倍，几乎匹配压缩率。'
- en: Appendix C Random Prompt Text Reconstruction Tasks
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 随机提示文本重建任务
- en: The random prompt text reconstruction tasks involves an original text sequence
    $({w_{1}},\ldots,w_{n})$.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 随机提示文本重建任务涉及一个原始文本序列 $({w_{1}},\ldots,w_{n})$。
- en: '|  | $\mathcal{L}_{\mathrm{RAE}}=\max_{h,\ldots,p}P\left(\boldsymbol{c}\mid
    h,\ldots,p;\Theta_{LLM}\right)$ |  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\mathrm{RAE}}=\max_{h,\ldots,p}P\left(\boldsymbol{c}\mid
    h,\ldots,p;\Theta_{LLM}\right)$ |  |'
- en: In the text continuation task, the prompt is no longer a substring of the encoder’s
    input text but is the immediately following segment of text, and the target sentence
    is still the text that comes right after the prompt. The formula for the text
    continuation task is the same as that for the random prompt text reconstruction
    task.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本续写任务中，提示词不再是编码器输入文本的子字符串，而是紧接着的文本片段，目标句子仍然是提示词之后的文本。文本续写任务的公式与随机提示文本重建任务的公式相同。
- en: Appendix D Format of Passkey Retrieval
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 密码检索格式
- en: 'We follow the text format for passkey retrieval from existing works Chen et al.
    ([2024](#bib.bib6)); Mohtashami and Jaggi ([2023](#bib.bib20)). The format of
    the document is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循 Chen 等人 ([2024](#bib.bib6)) 和 Mohtashami 与 Jaggi ([2023](#bib.bib20)) 现有工作中的密码检索文本格式。文档格式如下：
- en: There is an important info hidden inside a lot of irrelevant text. Find it and
    memorize them. I will quiz you about the important information there.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在大量无关文本中隐藏着重要信息。找出并记住它们。我会对你进行关于这些重要信息的测验。
- en: The grass is green. The sky is blue. The sun is yellow. Here we go. There and
    back again. (repeat M times)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 草是绿色的。天空是蓝色的。太阳是黄色的。出发吧。来回一次。 （重复 M 次）
- en: The pass key is 56994. Remember it. 56994 is the pass key. The grass is green.
    The sky is blue. The sun is yellow. Here we go. There and back again. (repeat
    N times)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 密码是 56994。记住它。56994 是密码。草是绿色的。天空是蓝色的。太阳是黄色的。出发吧。来回一次。 （重复 N 次）
- en: What is the pass key? The pass key is
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 密码是什么？密码是
- en: '| Our Result on RCC-32-Transformer | Standard Result |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 我们在 RCC-32-Transformer 上的结果 | 标准结果 |'
- en: '| The Access nodes and storage daemons make up a data plane, while the core
    provides its control plane. Also: How IBM Watson is revolutionizing 10 industries
    TechRepublic So, what does all mean for customers? Itś multi-cloud storage management,
    which enables allows you to manage, deploy, and migrate data storage across private
    and major public clouds. This includes Alibaba, AWS, Azure, and Google Cloud.
    Itś easy to see why Red Hat values this. It gives their customers a way to manage
    storage without sweating the details across multiple platforms. As Ranga Rangachari,
    Red Hatś vice president of Storage and Hyperconverged Infrastructure, said in
    a statement: "Data portability is a key imperative for organizations building
    and deploying cloud-native applications across private and multiple clouds. NooBaaś
    technologies will augment our portfolio and strengthen our ability to meet the
    needs of developers in todayś hybrid and multicloud world. We are thrilled to
    welcome a technical team of nine to the Red Hat family as we work together to
    further solidify Red Hat as a leading provider of open hybrid-cloud technologies."
    Related stories: Kidderminster-based Renault UK Clio Cup ace Dan Rowbottom will
    join Ciceley Motorsport for the 2019 British Touring Car Championship. Backed
    by Cataclean, the lead valuable additive to clean and fuel engine restore and
    exhaust systems, Rowbottom will graduate from the Renault UK Clio Cup into one
    of Ciceley’s Mercedes-Benz A-Class cars for the forthcoming campaign. He was a
    triple race winner last season his way to fourth place | The Access nodes and
    storage daemons make up a data plane, while the core provides its control plane.
    Also: How IBM Watson is revolutionizing 10 industries TechRepublic So, what does
    all mean for customers? Itś multi-cloud storage management, which enables allows
    you to manage, deploy, and migrate data storage across private and major public
    clouds. This includes Alibaba, AWS, Azure, and Google Cloud. Itś easy to see why
    Red Hat values this. It gives their customers a way to manage storage without
    sweating the details across multiple platforms. As Ranga Rangachari, Red Hatś
    vice president of Storage and Hyperconverged Infrastructure, said in a statement:
    "Data portability is a key imperative for organizations building and deploying
    cloud-native applications across private and multiple clouds. NooBaaś technologies
    will augment our portfolio and strengthen our ability to meet the needs of developers
    in todayś hybrid and multicloud world. We are thrilled to welcome a technical
    team of nine to the Red Hat family as we work together to further solidify Red
    Hat as a leading provider of open hybrid-cloud technologies." Related stories:
    Kidderminster-based Renault UK Clio Cup ace Dan Rowbottom will join Ciceley Motorsport
    for the 2019 British Touring Car Championship. Backed by Cataclean, the leading
    fuel additive to clean and restore engine fuel and exhaust systems, Rowbottom
    will graduate from the Renault UK Clio Cup into one of Ciceley’s Mercedes-Benz
    A-Class cars for the forthcoming campaign. He was a triple race winner last season
    his way to fourth place |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 访问节点和存储守护进程组成了数据平面，而核心提供了控制平面。此外：IBM Watson 如何颠覆 10 个行业 TechRepublic 那么，这对客户意味着什么呢？这就是多云存储管理，它使你能够在私有云和主要公共云之间管理、部署和迁移数据存储。这包括阿里巴巴、AWS、Azure
    和 Google Cloud。很容易看出为什么 Red Hat 这么重视这一点。它为客户提供了一种管理存储的方式，无需在多个平台之间操心细节。正如 Red
    Hat 存储与超融合基础设施副总裁 Ranga Rangachari 在一份声明中所说：“数据可移植性是组织在私有云和多个云上构建和部署云原生应用的关键要求。NooBaa
    的技术将丰富我们的产品组合，并增强我们满足当今混合和多云世界开发者需求的能力。我们很高兴欢迎一个由九人组成的技术团队加入 Red Hat 团队，共同努力进一步巩固
    Red Hat 作为领先的开放混合云技术供应商的地位。” 相关新闻：基于基德明斯特的雷诺 UK Clio Cup 精英 Dan Rowbottom 将加入
    Ciceley Motorsport，参加 2019 年英国房车锦标赛。在 Cataclean 的支持下，这是一种领先的燃料添加剂，用于清洁和修复发动机燃料和排气系统，Rowbottom
    将从雷诺 UK Clio Cup 升级到 Ciceley 的一辆奔驰 A-Class 车，参加即将到来的赛季。他在上赛季赢得了三场比赛，并最终获得第四名 |
    访问节点和存储守护进程组成了数据平面，而核心提供了控制平面。此外：IBM Watson 如何颠覆 10 个行业 TechRepublic 那么，这对客户意味着什么呢？这就是多云存储管理，它使你能够在私有云和主要公共云之间管理、部署和迁移数据存储。这包括阿里巴巴、AWS、Azure
    和 Google Cloud。很容易看出为什么 Red Hat 这么重视这一点。它为客户提供了一种管理存储的方式，无需在多个平台之间操心细节。正如 Red
    Hat 存储与超融合基础设施副总裁 Ranga Rangachari 在一份声明中所说：“数据可移植性是组织在私有云和多个云上构建和部署云原生应用的关键要求。NooBaa
    的技术将丰富我们的产品组合，并增强我们满足当今混合和多云世界开发者需求的能力。我们很高兴欢迎一个由九人组成的技术团队加入 Red Hat 团队，共同努力进一步巩固
    Red Hat 作为领先的开放混合云技术供应商的地位。” 相关新闻：基于基德明斯特的雷诺 UK Clio Cup 精英 Dan Rowbottom 将加入
    Ciceley Motorsport，参加 2019 年英国房车锦标赛。在 Cataclean 的支持下，这是一种领先的燃料添加剂，用于清洁和修复发动机燃料和排气系统，Rowbottom
    将从雷诺 UK Clio Cup 升级到 Ciceley 的一辆奔驰 A-Class 车，参加即将到来的赛季。他在上赛季赢得了三场比赛，并最终获得第四名'
- en: 'Table 3: Effect of random prompt text reconstruction'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：随机提示文本重建的效果
- en: '| Dataset | Task | Source | Avg len | Metric | #data |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务 | 来源 | 平均长度 | 指标 | 数据数量 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Qasper | Single-Document QA | Science | 4,620 | F1 | 224 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| Qasper | 单文档问答 | 科学 | 4,620 | F1 | 224 |'
- en: '| MultiFieldQA | Single-Document QA | Multi-field | 4,558 | F1 | 150 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| MultiFieldQA | 单文档问答 | 多领域 | 4,558 | F1 | 150 |'
- en: '| HotpotQA | Multi-Doc QA | Wikipedia | 6,657 | F1 | 300 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| HotpotQA | 多文档问答 | 维基百科 | 6,657 | F1 | 300 |'
- en: '| 2WikiMultihopQA | Multi-Doc QA | Wikipedia | 6,146 | F1 | 300 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 2WikiMultihopQA | 多文档问答 | 维基百科 | 6,146 | F1 | 300 |'
- en: 'Table 4: LongBench-E Information'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：LongBench-E 信息
- en: Appendix E Fine-Tuning Datasets and Model-Generated Cases
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 微调数据集和模型生成案例
- en: 'Table [4](#A4.T4 "Table 4 ‣ Appendix D Format of Passkey Retrieval ‣ Recurrent
    Context Compression: Efficiently Expanding the Context Window of LLM") displays
    information such as the sources, average lengths, and computational metrics for
    various tasks. Below is a sample data entry for document question answering, primarily
    consisting of three parts: ’input’, ’context’, and ’answers’. The ’input’ represents
    the prompt or instruction, the ’context’ is the surrounding text the model needs
    to search through, which is often lengthy, and the ’answers’ represent the possible
    answers derived from the context. Example:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [4](#A4.T4 "表格 4 ‣ 附录 D Passkey 检索格式 ‣ 循环上下文压缩：高效扩展 LLM 的上下文窗口") 展示了各种任务的来源、平均长度和计算指标等信息。下面是文档问答的一个示例数据条目，主要包括三个部分：‘输入’，‘上下文’，和‘答案’。‘输入’代表提示或指令，‘上下文’是模型需要搜索的周围文本，通常较长，而‘答案’代表从上下文中得出的可能答案。例如：
- en: 'input: "Which park is further south within Spain, Picos de Europa National
    Park or Timanfaya National Park?"'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：“西班牙境内哪个公园更靠南，皮科斯德欧罗帕国家公园还是提曼法亚国家公园？”
- en: 'context: ’Passage 1:Lake Ercina Lake Ercina is a small highland lake … The
    population is 47 (INE 2016).’'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文：‘段落 1：厄尔西纳湖 厄尔西纳湖是一个小型高原湖泊……人口为47（INE 2016）。’
- en: 'answers: [’Timanfaya National Park’]'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：[‘提曼法亚国家公园’]
- en: 'When using RCC-Ins-Reconstruction for instruction reconstruction inference,
    we concatenate the ’context’ and ’input’ parts of the sample with a newline character
    and input them into the model’s encoder for compression. Simultaneously, the decoder’s
    input is a fixed prompt:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 RCC-Ins-Reconstruction 进行指令重建推断时，我们将样本的‘上下文’和‘输入’部分通过换行符连接起来，并输入到模型的编码器中进行压缩。同时，解码器的输入是一个固定的提示：
- en: 'prompt: "system: You are a helpful assistant. user: "'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：“系统：你是一个有用的助手。 用户：”
- en: 'The decoder, starting with this prompt, first reconstructs the instruction
    and then answers the question based on it. The content generated by the model
    is shown in blue font:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器从这个提示开始，首先重建指令，然后根据指令回答问题。模型生成的内容以蓝色字体显示：
- en: '"system: You are a helpful assistant. user: Which park is further south within
    Spain, Picos de Europa National Park or Timanfaya National Park? assistant: Timanfaya
    National Park"'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: “系统：你是一个有用的助手。 用户：西班牙境内哪个公园更靠南，皮科斯德欧罗帕国家公园还是提曼法亚国家公园？助手：提曼法亚国家公园”
- en: The model accurately reconstructed the instruction and provided the correct
    answer, ’Timanfaya National Park’.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 模型准确地重建了指令，并提供了正确的答案，‘提曼法亚国家公园’。
- en: 'Additionally, we tested the RCC-Ins-compress model. The input to the RCC-Ins-compress
    encoder is identical to that of the RCC-Ins-Reconstruction, but the decoder’s
    prompt is:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们测试了 RCC-Ins-compress 模型。RCC-Ins-compress 编码器的输入与 RCC-Ins-Reconstruction
    相同，但解码器的提示是：
- en: 'Prompt: "Response of system:"'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：“系统的响应：”
- en: 'Since RCC-Ins-compress has not been trained on instruction reconstruction tasks,
    it does not reconstruct the instruction in its output. Instead, it directly answers
    the question based on the mixed compressed context and instruction, which may
    result in the model failing to follow the instruction.The content generated by
    the model is shown in blue font:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RCC-Ins-compress 没有经过指令重建任务的训练，因此它不会在输出中重建指令。相反，它直接根据混合压缩的上下文和指令回答问题，这可能导致模型未能遵循指令。模型生成的内容以蓝色字体显示：
- en: '"Response of system: Panic of 1797"'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: “系统的响应：1797年的恐慌”
- en: It can be seen that the model made an error in following the instructions.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看出，模型在遵循指令时出现了错误。
