- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:52:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:52:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ComNeck: 通过通用变换颈桥接压缩图像潜变量和多模态大语言模型（LLMs）'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19651](https://ar5iv.labs.arxiv.org/html/2407.19651)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19651](https://ar5iv.labs.arxiv.org/html/2407.19651)
- en: Chia-Hao Kao^(1,2)  Cheng Chien¹  Yu-Jen Tseng¹  Yi-Hsin Chen¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 佳豪·高^(1,2)  程谦¹  余仁曾¹  易欣·陈¹
- en: Alessandro Gnutti²  Shao-Yuan Lo³  Wen-Hsiao Peng¹  Riccardo Leonardi²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 亚历山德罗·努蒂²  邵源·罗³  温小鹏¹  里卡尔多·莱昂纳尔迪²
- en: ¹National Yang Ming Chiao Tung University, Taiwan
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹国防医学院，台湾
- en: ²University of Brescia, Italy ³Honda Research Institute, USA
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²布雷西亚大学，意大利 ³本田研究所，美国
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper presents the first-ever study of adapting compressed image latents
    to suit the needs of downstream vision tasks that adopt Multimodal Large Language
    Models (MLLMs). MLLMs have extended the success of large language models to modalities
    (e.g. images) beyond text, but their billion scale hinders deployment on resource-constrained
    end devices. While cloud-hosted MLLMs could be available, transmitting raw, uncompressed
    images captured by end devices to the cloud requires an efficient image compression
    system. To address this, we focus on emerging neural image compression and propose
    a novel framework with a lightweight transform-neck and a surrogate loss to adapt
    compressed image latents for MLLM-based vision tasks. The proposed framework is
    generic and applicable to multiple application scenarios, where the neural image
    codec can be (1) pre-trained for human perception without updating, (2) fully
    updated for joint human and machine perception, or (3) fully updated for only
    machine perception. The transform-neck trained with the surrogate loss is universal,
    for it can serve various downstream vision tasks enabled by a variety of MLLMs
    that share the same visual encoder. Our framework has the striking feature of
    excluding the downstream MLLMs from training the transform-neck, and potentially
    the neural image codec as well. This stands out from most existing coding for
    machine approaches that involve downstream networks in training and thus could
    be impractical when the networks are MLLMs. Extensive experiments on different
    neural image codecs and various MLLM-based vision tasks show that our method achieves
    great rate-accuracy performance with much less complexity, demonstrating its effectiveness.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文首次研究了将压缩图像潜变量适应于采用多模态大语言模型（MLLMs）的下游视觉任务的需求。MLLMs 将大语言模型的成功扩展到文本之外的模态（例如图像），但其亿级规模限制了在资源受限的终端设备上的部署。虽然云托管的MLLMs
    可能可用，但将终端设备捕获的原始未压缩图像传输到云端需要高效的图像压缩系统。为解决这一问题，我们关注新兴的神经图像压缩技术，并提出了一种新颖的框架，具有轻量级的变换颈和替代损失，以适应压缩图像潜变量用于基于MLLM的视觉任务。该框架通用且适用于多种应用场景，其中神经图像编解码器可以（1）在不更新的情况下为人类感知进行预训练，（2）为人类和机器感知进行全面更新，或（3）仅为机器感知进行全面更新。用替代损失训练的变换颈具有通用性，因为它可以服务于由多种共享相同视觉编码器的MLLMs
    驱动的各种下游视觉任务。我们框架的显著特点是将下游MLLMs 从训练变换颈以及可能的神经图像编解码器中排除。这与大多数现有的机器编码方法不同，后者涉及下游网络的训练，因此当网络是MLLMs
    时可能不切实际。在不同神经图像编解码器和各种基于MLLM的视觉任务上的广泛实验表明，我们的方法在复杂度大幅降低的情况下实现了出色的比特率-准确率性能，证明了其有效性。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]
    have demonstrated impressive abilities in various Natural Language Processing
    (NLP) tasks. The recent surge of research on Multimodal Large Language Models
    (MLLMs) extends LLM’s abilities to data beyond languages [[4](#bib.bib4), [5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)], particularly
    images, opening up promising opportunities in various applications. MLLMs have
    shown surprising capability for many vision tasks such as classification [[10](#bib.bib10)],
    image captioning [[9](#bib.bib9), [11](#bib.bib11)], Visual Question Answering
    (VQA) [[5](#bib.bib5), [7](#bib.bib7)], and meme interpretation [[4](#bib.bib4)].
    These models excel in unseen tasks through instruction following or in-context
    learning, which is impossible for traditional vision networks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)] 在各种自然语言处理（NLP）任务中表现出色。最近对多模态大型语言模型（MLLMs）的研究将LLM的能力扩展到语言之外的数据[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]，特别是图像，开启了各种应用中的有希望的机会。MLLMs
    在许多视觉任务中表现出令人惊讶的能力，如分类[[10](#bib.bib10)]、图像描述[[9](#bib.bib9), [11](#bib.bib11)]、视觉问答（VQA）[[5](#bib.bib5),
    [7](#bib.bib7)]和表情包解读[[4](#bib.bib4)]。这些模型在未见任务中的表现优异，通过遵循指令或在上下文中学习，这是传统视觉网络无法做到的。
- en: 'However, MLLM’s billion-scale size hinders deployment on resource-constrained
    end devices. While computation can be offloaded to the cloud, transmitting images
    to cloud-hosted MLLMs becomes necessary. Our study shows that directly feeding
    the decoded image, generated by a fixed neural image codec trained for human perception,
    into an MLLM (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck") (a))
    significantly degrades task performance, particularly when the image is coded
    at low rates. This highlights the need for efficient image compression that considers
    the requirements of downstream MLLM-based vision tasks.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，MLLM的亿级规模阻碍了在资源受限的终端设备上的部署。虽然计算可以转移到云端，但将图像传输到云托管的MLLMs变得必要。我们的研究表明，直接将由固定神经图像编码器生成的解码图像（用于人类感知）输入到MLLM中（图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (a)）会显著降低任务性能，特别是当图像以低码率编码时。这突显了需要考虑下游MLLM基础视觉任务需求的高效图像压缩。'
- en: 'Many prior works address image compression for machine vision, commonly referred
    to as coding for machines [[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]. Two common approaches to coding for machines
    are image coding and feature coding. The image coding approaches [[12](#bib.bib12),
    [16](#bib.bib16)] optimize the image codec for specific downstream tasks and/or
    networks (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck") (b)), while the
    feature coding approaches [[17](#bib.bib17)] divide the task network into two
    parts and focus on compressing the intermediate features (Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (c)). However, both approaches face the same
    issue: they limit the trained system to be only suitable for one specific model
    or task, thus requiring separate parameters or models for different tasks. Additionally,
    while they may potentially yield high rate-accuracy performance, the training
    process becomes challenging when one needs to back-propagate a training objective
    through a massive MLLM to train the neural image codec. In practice, the billion-scale
    parameters of MLLMs make the existing coding for machine methods inapplicable.
    To the best of our knowledge, there have been no attempts to tackle compression
    for MLLMs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多先前的工作涉及机器视觉的图像压缩，通常被称为机器编码[[12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]。机器编码的两种常见方法是图像编码和特征编码。图像编码方法[[12](#bib.bib12),
    [16](#bib.bib16)] 针对特定下游任务和/或网络优化图像编解码器（图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ ComNeck：通过通用变换颈桥接压缩图像潜在表示和多模态LLM")
    (b)），而特征编码方法[[17](#bib.bib17)] 将任务网络划分为两部分，并专注于压缩中间特征（图 [1](#S1.F1 "图 1 ‣ 1 介绍
    ‣ ComNeck：通过通用变换颈桥接压缩图像潜在表示和多模态LLM") (c)）。然而，这两种方法都面临相同的问题：它们将训练系统限制为仅适用于一个特定模型或任务，因此需要为不同任务设置单独的参数或模型。此外，尽管它们可能潜在地提供高的速率-准确性性能，但当需要通过大型MLLM反向传播训练目标以训练神经图像编解码器时，训练过程变得非常困难。在实践中，MLLM的亿级参数使现有的机器编码方法不适用。据我们所知，目前尚无尝试解决MLLM压缩问题的方法。
- en: '![Refer to caption](img/5a172d030b06ad8e1630a62e8283db84.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5a172d030b06ad8e1630a62e8283db84.png)'
- en: 'Figure 1: On the left is inadequate frameworks for image compression for MLLMs,
    where the image codec is trained for (a) human perception, (b) the downstream
    task network, or (c) compressing the intermediate features of the task network.
    On the right is the proposed transform-neck and surrogate loss under three distinct
    scenarios, with the image codec (d1) pre-trained for human perception, (d2) updated
    for joint human and machine perception, or (d3) updated for machine perception.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：左侧是针对MLLM图像压缩的不充分框架，其中图像编解码器针对（a）人类感知，（b）下游任务网络，或（c）压缩任务网络的中间特征进行训练。右侧是所提出的变换颈和替代损失在三种不同场景下的表现，其中图像编解码器（d1）预训练用于人类感知，（d2）更新用于人类和机器联合感知，或（d3）更新用于机器感知。
- en: In this paper, we propose the first neural image compression system for MLLM-based
    vision tasks that enables compressed image latents to suit the needs of downstream
    MLLMs. The proposed method involves a lightweight transform-neck and a surrogate
    loss. The transform-neck adapts compressed image latents to match the intermediate
    features of the CLIP visual encoder [[18](#bib.bib18)], a common component in
    many popular MLLMs. This approach avoids the image decoding process and reduces
    computational complexity. To address the massive size of MLLMs, the surrogate
    loss updates our system with the CLIP visual encoder, refraining from back-propagating
    the task loss through the heavy MLLM. The transform-neck trained with the surrogate
    loss is universal and readily applicable to various MLLMs that share the same
    CLIP visual encoder, without requiring re-training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了第一个用于基于MLLM视觉任务的神经图像压缩系统，使压缩图像潜在表示适应下游MLLM的需求。所提出的方法涉及一个轻量级的变换颈和一个替代损失。变换颈使压缩图像潜在表示适配CLIP视觉编码器[[18](#bib.bib18)]的中间特征，这是许多流行MLLM中的常见组件。这种方法避免了图像解码过程，并降低了计算复杂性。为了应对MLLM的庞大规模，替代损失通过CLIP视觉编码器更新我们的系统，避免了通过繁重的MLLM反向传播任务损失。使用替代损失训练的变换颈具有普遍性，适用于共享相同CLIP视觉编码器的各种MLLM，无需重新训练。
- en: 'The proposed method is generic and applicable to different neural image codecs
    under various application scenarios. First, if thedownstream applications prioritize
    image reconstruction quality for human interaction, our method can work with an
    off-the-shelf image codec trained for human perception (Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (d1)). Without any modification or re-training
    of the codec, our method adapts the compressed image latents while maintaining
    the same image reconstruction quality. Second, when allowing image codecs to be
    updated, we propose a multi-task training strategy that optimizes the codec for
    both human and machine perception (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (d2)). This significantly improves MLLM performance at the cost
    of a marginal drop in reconstruction quality. Finally, we consider an extreme
    setting in which the applications prioritize machine perception over image reconstruction.
    In this case, the encoder and the transform-neck are jointly optimized for the
    MLLM systems exclusively (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")
    (d3)). On top of that, our transform-neck is agnostic to the architecture of neural
    image codecs, being able to work with various models.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '提议的方法具有通用性，适用于不同神经图像编码器下的各种应用场景。首先，如果下游应用优先考虑图像重建质量以用于人类交互，我们的方法可以与现成的为人类感知训练的图像编码器配合使用（图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (d1)）。无需对编码器进行任何修改或重新训练，我们的方法在保持相同图像重建质量的同时，适应了压缩图像潜在变量。其次，当允许更新图像编码器时，我们提出了一种多任务训练策略，优化编码器以同时满足人类和机器感知（图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (d2)）。这在重建质量略微下降的情况下显著提升了 MLLM 性能。最后，我们考虑了一个极端设置，其中应用优先考虑机器感知而非图像重建。在这种情况下，编码器和
    transform-neck 会专门针对 MLLM 系统进行联合优化（图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")
    (d3)）。此外，我们的 transform-neck 对神经图像编码器的架构具有无关性，能够与各种模型配合使用。'
- en: 'The contributions of this work are summarized as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的贡献总结如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It marks the first exploration into the field of neural image coding for MLLMs.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这标志着对 MLLMs 的神经图像编码领域的首次探索。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proposed transform-neck adapts the compressed image latents to downstream
    MLLMs, avoiding the need for image decoding and saving computational complexity.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提议的 transform-neck 适应压缩图像潜在变量到下游 MLLMs，避免了图像解码的需求，并且减少了计算复杂度。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The proposed surrogate loss leverages the CLIP visual encoder to update the
    system, avoiding back-propagating the task loss through the heavy MLLM.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提议的替代损失利用 CLIP 视觉编码器更新系统，避免通过复杂的 MLLM 反向传播任务损失。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our method is agnostic to the downstream MLLMs or tasks; without re-training,
    the resulting system is readily applicable to a wide variety of MLLMs sharing
    the same visual encoder.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法对下游 MLLMs 或任务具有无关性；无需重新训练，所得到的系统可以广泛适用于共享相同视觉编码器的多种 MLLMs。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our framework is able to accommodate various application scenarios that involve
    human perception, machine perception, or both.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的框架能够适应涉及人类感知、机器感知或两者兼具的各种应用场景。
- en: 2 Related Works
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Multimodal Large Language Models
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多模态大型语言模型
- en: In recent years, there has been a surge of interest in MLLMs following the impressive
    demonstration of LLM’s ability in the NLP field [[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3)]. Many have sought to extend the success of these models from text
    to other modalities, particularly images, and several works have shown their effectiveness
    on various tasks, such as image captioning [[11](#bib.bib11), [7](#bib.bib7),
    [9](#bib.bib9), [19](#bib.bib19), [20](#bib.bib20)], VQA [[5](#bib.bib5), [9](#bib.bib9),
    [20](#bib.bib20)], Referring Expression Comprehension (REC) [[6](#bib.bib6), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)], few-shot classification [[24](#bib.bib24),
    [10](#bib.bib10)], action anticipation [[25](#bib.bib25)], meme interpretation [[4](#bib.bib4),
    [26](#bib.bib26)], biomedical reasoning [[27](#bib.bib27)], OCR-free math reasoning [[28](#bib.bib28)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，随着LLM在NLP领域的令人印象深刻的展示 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]，MLLMs引起了广泛的兴趣。许多人试图将这些模型从文本扩展到其他模态，特别是图像，几个研究展示了它们在各种任务上的有效性，如图像描述 [[11](#bib.bib11),
    [7](#bib.bib7), [9](#bib.bib9), [19](#bib.bib19), [20](#bib.bib20)]，视觉问答 [[5](#bib.bib5),
    [9](#bib.bib9), [20](#bib.bib20)]，指代表达理解 (REC) [[6](#bib.bib6), [20](#bib.bib20),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)]，少样本分类 [[24](#bib.bib24),
    [10](#bib.bib10)]，动作预测 [[25](#bib.bib25)]，迷因解读 [[4](#bib.bib4), [26](#bib.bib26)]，生物医学推理 [[27](#bib.bib27)]，无需OCR的数学推理 [[28](#bib.bib28)]。
- en: Most existing MLLM approaches use a visual encoder to process the input image
    data, and then introduce a connector to bridge the image features to the tokens
    understandable by the LLM. Earlier works adopt simpler connector designs, such
    as linear projectors [[6](#bib.bib6), [19](#bib.bib19)], while subsequent works [[11](#bib.bib11),
    [5](#bib.bib5), [9](#bib.bib9)] have refined upon the design for both performance
    and complexity. Furthermore, the entire MLLM can be further fine-tuned to enhance
    its capabilities through instruction tuning [[19](#bib.bib19), [29](#bib.bib29)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的MLLM方法使用视觉编码器来处理输入图像数据，然后引入一个连接器将图像特征桥接到LLM可以理解的标记。早期的工作采用了更简单的连接器设计，如线性投影器 [[6](#bib.bib6),
    [19](#bib.bib19)]，而后续的工作 [[11](#bib.bib11), [5](#bib.bib5), [9](#bib.bib9)]则在性能和复杂性上对设计进行了改进。此外，整个MLLM还可以通过指令调优 [[19](#bib.bib19),
    [29](#bib.bib29)] 进一步微调以增强其能力。
- en: A notable aspect of the MLLMs is their reliance on existing pre-trained visual
    encoders in their systems, with CLIP [[18](#bib.bib18)] visual encoder being a
    very common choice for a large number of methods [[5](#bib.bib5), [6](#bib.bib6),
    [7](#bib.bib7), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [19](#bib.bib19),
    [23](#bib.bib23), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]. Trained
    on large image-text pair data, the CLIP visual encoder offers the feature space
    that combines language and image modalities in a sense, making it a desirable
    feature for MLLMs. Sharing the same visual encoder also gives us the opportunity
    to design a universal method for MLLMs. Notably, all the existing works on MLLMs
    do not consider the scenarios where image compression is present, which is a significant
    departure from our work.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MLLMs的一个显著特点是它们依赖于系统中现有的预训练视觉编码器，其中CLIP [[18](#bib.bib18)]视觉编码器是许多方法中非常常见的选择 [[5](#bib.bib5),
    [6](#bib.bib6), [7](#bib.bib7), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [19](#bib.bib19), [23](#bib.bib23), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]。CLIP视觉编码器在大量图像-文本配对数据上进行训练，提供了将语言和图像模态结合的特征空间，这使得它成为MLLMs的一个理想特征。共享相同的视觉编码器也为我们设计通用的MLLM方法提供了机会。值得注意的是，现有的所有关于MLLMs的工作并未考虑图像压缩的情况，这与我们的工作有显著不同。
- en: 2.2 Image Coding for Machines
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 针对机器的图像编码
- en: Neural image compression systems have made significant progress in the past
    few years. As a matter of fact, several works [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)] have even outperformed the traditional codecs
    such as intra coding in VVC [[37](#bib.bib37)]. However, these methods primarily
    focus on the quality of reconstructed images for human perception. Coding for
    machines, in contrast, targets downstream machine vision instead of human perception,
    and it has attracted increasing attention recently.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 神经图像压缩系统在过去几年取得了显著进展。实际上，一些研究 [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35),
    [36](#bib.bib36)]甚至在性能上超越了传统的编码器，如VVC中的内编码 [[37](#bib.bib37)]。然而，这些方法主要关注重建图像的质量以适应人类感知。与此相反，为机器编码则针对下游机器视觉，而不是人类感知，最近引起了越来越多的关注。
- en: A common approach simply involves training the compression system for a predefined
    target downstream computer vision task [[12](#bib.bib12), [16](#bib.bib16), [38](#bib.bib38)],
    enabling the reconstructed image to be suitable for machine vision, albeit potentially
    sacrificing perceptual quality. Conversely, Chamain et al. [[13](#bib.bib13)]
    tune the task network to better process the compressed images, while Chen et al. [[39](#bib.bib39)]
    leverage prompt-tuning method on Transformer-based codecs to boost performance
    on multiple tasks. Also, with the trend of the new JPEG AI learning-based image
    coding standard [[40](#bib.bib40)], some methods [[41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)] utilize the compressed image latents instead
    of the reconstructed image for recognition through bridging the latents to task
    network. On the other hand, Ding et al. [[17](#bib.bib17)] directly compress the
    intermediate features of recognition networks, while Feng et al. [[45](#bib.bib45)]
    learn the omnipotent features suitable for various tasks in a self-supervised
    manner and fine-tune each task network tail on such features.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的方法是简单地训练压缩系统以满足预定义的下游计算机视觉任务[[12](#bib.bib12), [16](#bib.bib16), [38](#bib.bib38)]，使重建图像适合机器视觉，但可能会牺牲感知质量。相反，Chamain
    等人[[13](#bib.bib13)] 调整任务网络以更好地处理压缩图像，而 Chen 等人[[39](#bib.bib39)] 在基于 Transformer
    的编解码器上利用提示调优方法提升多任务的性能。此外，随着新 JPEG AI 学习型图像编码标准的趋势[[40](#bib.bib40)]，一些方法[[41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)] 利用压缩图像潜变量而非重建图像进行识别，通过将潜变量与任务网络对接。另一方面，Ding
    等人[[17](#bib.bib17)] 直接压缩识别网络的中间特征，而 Feng 等人[[45](#bib.bib45)] 以自监督方式学习适合各种任务的全能特征，并在这些特征上微调每个任务网络。
- en: It is crucial to note that none of the coding for machine methods considers
    MLLMs at the receiver side. All the above-mentioned methods leverage back-propagation
    through recognition models to update the system, which is prohibitively expensive
    for MLLMs due to their huge scale. Therefore, the direct application of the same
    methods on MLLMs is almost infeasible. In addition, the use of a specific task
    loss restricts the resulting models to be optimized for a single task and recognition
    model, thus requiring re-training for each new task and incurring additional costs.
    We aim to be the first to propose a neural image compression system designed for
    MLLMs, achieved through a universal transform-neck and the adoption of a surrogate
    loss, which allows to bypass the necessity of involving the entire billion-scale
    MLLM in the training process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，当前所有用于机器的方法都没有考虑接收端的 MLLMs。上述方法都依赖于通过识别模型进行反向传播来更新系统，这对于大规模的 MLLMs 来说代价极高。因此，将相同的方法直接应用于
    MLLMs 几乎是不可能的。此外，特定任务损失的使用限制了模型只能优化为单一任务和识别模型，从而需要为每个新任务重新训练并产生额外的成本。我们旨在首创一种为
    MLLMs 设计的神经图像压缩系统，通过通用转换颈部和采用替代损失，绕过涉及整个十亿规模 MLLM 的训练过程的必要性。
- en: 3 Proposed Method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提出的方案
- en: '3.1 Preliminaries: Neural Image Codecs'
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 前提条件：神经图像编解码器
- en: 'The high-level architecture of a neural image codec is depicted in the top
    central green box in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries: Neural Image
    Codecs ‣ 3 Proposed Method ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck"). In a typical hyperprior-based neural image
    compression system [[46](#bib.bib46)], the key components include the main encoder
    $g_{a}$.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed
    Method ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck")中的顶部中央绿色框中展示了神经图像编解码器的高层架构。在典型的基于超先验的神经图像压缩系统[[46](#bib.bib46)]中，关键组件包括主要编码器
    $g_{a}$。'
- en: '![Refer to caption](img/d812c0ff2c9dee3e36d9052ac5fc63bc.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d812c0ff2c9dee3e36d9052ac5fc63bc.png)'
- en: 'Figure 2: Overall architecture of the proposed method.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：所提方法的总体架构。
- en: 3.2 Overall Framework
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 总体框架
- en: 'In this work, we focus on the scenario where MLLMs are hosted on the server
    side, while users on end devices need to perform inference on the remote model
    using both text and images as inputs. Given the necessity of incorporating image
    compression to ensure efficient transmission, we propose a compression framework
    with the consideration of MLLMs as downstream application networks, aiming to
    mitigate the potential task performance drop caused by image compression. Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")
    illustrates our overall framework, which includes three major components: the
    neural image codec, our proposed transform-neck, and the MLLM. The depicted MLLM
    system adheres to a typical structure, consisting of a visual encoder, an LLM,
    and a connector component facilitating the transformation of features from the
    visual encoder to the LLM.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们关注的是 MLLM 主机在服务器端的场景，同时终端设备上的用户需要使用文本和图像作为输入，对远程模型进行推理。鉴于需要将图像压缩以确保高效传输，我们提出了一个压缩框架，并将
    MLLM 作为下游应用网络，以减轻图像压缩可能导致的任务性能下降。图 [2](#S3.F2 "Figure 2 ‣ 3.1 Preliminaries: Neural
    Image Codecs ‣ 3 Proposed Method ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck") 展示了我们的总体框架，包括三个主要组件：神经图像编解码器、我们提出的变换颈部和
    MLLM。图示的 MLLM 系统遵循典型结构，包括视觉编码器、LLM 和一个连接组件，促进从视觉编码器到 LLM 的特征转换。'
- en: During inference, an input image at the end device is passed through an encoder
    $g_{a}$ for transformation into a middle layer of the visual encoder of an MLLM.
    We opt to adapt the image latents rather than the reconstructed images because
    the image latents inherently contain the information needed for reconstructing
    the image, and potentially the semantic information for the downstream tasks (when
    the image encoder is guided properly). By skipping the image decoding process,
    our method offers reduced computational complexity while maintaining the task
    performance. The rest of the MLLM system operates without any changes to generate
    the desired output response.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，输入图像在终端设备上通过编码器 $g_{a}$ 转换为 MLLM 视觉编码器的中间层。我们选择调整图像潜变量而不是重建图像，因为图像潜变量本身包含了重建图像所需的信息，并且可能包含下游任务的语义信息（当图像编码器得到正确引导时）。通过跳过图像解码过程，我们的方法在保持任务性能的同时，减少了计算复杂性。其余的
    MLLM 系统在不做任何更改的情况下继续生成所需的输出响应。
- en: 'In our experiments, we examine three distinct settings denoted as (d1), (d2)
    and (d3), as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").
    Firstly, in (d1), we consider the practical scenario where a fixed off-the-shelf
    image codec pre-trained for human perception is directly used alongside our transform-neck.
    In this setting, our framework offers the option for users to decode the image
    latents $\hat{y}$ instead of the transform-neck. The quality of the decoded image
    is not affected by the introduction of our transform-neck, as the image codec
    is not updated in the present case. Then, we extend the analysis to scenarios
    (d2) and (d3) to examine the impact of jointly training the image codec and transform-neck.
    In (d2), the entire image codec undergoes re-training to accommodate both human
    and machine perception, while in (d3), the encoder is re-trained specifically
    for machine perception. Regardless of the context examined, we circumvent the
    difficulties of back-propagating the task loss through MLLMs by introducing a
    surrogate loss. We remark that the resulting system is readily applicable to a
    wide variety of MLLMs and tasks. It needs no re-training of the system when these
    MLLMs adopt the same visual encoder.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们考察了三种不同的设置，分别标记为(d1)、(d2)和(d3)，如图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ ComNeck：通过通用transform-neck连接压缩图像潜变量和多模态大语言模型")所示。首先，在(d1)中，我们考虑了一个实际场景，其中一个为人类感知预训练的固定现成图像编解码器直接与我们的transform-neck一起使用。在这种设置下，我们的框架为用户提供了一个选项，可以解码图像潜变量$\hat{y}$而不是transform-neck。在这种情况下，引入transform-neck不会影响解码图像的质量，因为图像编解码器未被更新。然后，我们扩展分析到(d2)和(d3)场景，以考察联合训练图像编解码器和transform-neck的影响。在(d2)中，整个图像编解码器会重新训练，以适应人类和机器的感知，而在(d3)中，编码器专门针对机器感知重新训练。无论在何种上下文中，我们通过引入替代损失来规避通过MLLMs进行任务损失反向传播的困难。我们指出，所得系统可以广泛应用于各种MLLM和任务。当这些MLLM采用相同的视觉编码器时，无需对系统进行重新训练。
- en: 3.3 Transform-neck
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 Transform-neck
- en: 'Our transform-neck is designed to be a lightweight module, consisting only
    of a linear projection, a self-attention mechanism, a feed-forward layer, and
    two layer norms, as shown in the central red box in Figure [2](#S3.F2 "Figure
    2 ‣ 3.1 Preliminaries: Neural Image Codecs ‣ 3 Proposed Method ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"). Its
    purpose is to adapt the compressed image latents $\hat{y}$ to a form suitable
    for consumption by the downstream MLLMs.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的transform-neck被设计为一个轻量级模块，仅由一个线性投影、自注意力机制、一个前馈层和两个层归一化组成，如图[2](#S3.F2 "图
    2 ‣ 3.1 基础知识：神经图像编解码器 ‣ 3 提议的方法 ‣ ComNeck：通过通用transform-neck连接压缩图像潜变量和多模态大语言模型")中的中央红色框所示。其目的是将压缩图像潜变量$\hat{y}$转换为适合下游MLLMs（多模态大语言模型）处理的形式。
- en: 'Adapting the compressed image latents to existing MLLM systems is a non-trivial
    task, especially when aiming for a universal approach compatible with multiple
    MLLMs and tasks. To address this challenge, we observe that a large number of
    existing MLLM systems share the same pre-trained visual encoder, i.e. the CLIP
    visual encoder, as discussed in Section [2.1](#S2.SS1 "2.1 Multimodal Large Language
    Models ‣ 2 Related Works ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck"). Inspired by this observation, we propose
    to leverage the CLIP visual encoder, denoted by $C$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 将压缩图像潜变量适配到现有的MLLM系统是一项复杂的任务，特别是当目标是实现与多个MLLM和任务兼容的通用方法时。为了解决这个挑战，我们观察到大量现有的MLLM系统共享相同的预训练视觉编码器，即CLIP视觉编码器，如第[2.1节](#S2.SS1
    "2.1 多模态大语言模型 ‣ 2 相关工作 ‣ ComNeck：通过通用transform-neck连接压缩图像潜变量和多模态大语言模型")所讨论。受此观察启发，我们提议利用CLIP视觉编码器，记作$C$。
- en: Since the image encoder $g_{a}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于图像编码器$g_{a}$。
- en: 3.4 Surrogate Loss
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 替代损失
- en: 'To avoid involving huge MLLMs in the training process, thus bypassing back-propagation
    through them, we propose a surrogate loss, which is back-propagated through only
    the partial CLIP encoder $C^{\prime}$. To this end, we introduce the following
    distillation loss for training, aiming to minimize the Mean Squared Error (MSE)
    between the two output features:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在训练过程中涉及庞大的MLLM，从而绕过通过它们的反向传播，我们提出了一种替代损失，仅通过部分CLIP编码器$C^{\prime}$进行反向传播。为此，我们引入了以下蒸馏损失用于训练，旨在最小化两个输出特征之间的均方误差（MSE）：
- en: '|  | $\displaystyle\mathcal{L}_{dist}=\text{MSE}(C^{\prime}(T(\hat{y})),C(x)).$
    |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{dist}=\text{MSE}(C^{\prime}(T(\hat{y})),C(x)).$
    |  | (1) |'
- en: The surrogate loss enables the resulting transform-neck to be applicable to
    various MLLMs sharing the same visual encoder $C$ without re-training.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 替代损失使得生成的变换颈部可以在不同的共享相同视觉编码器 $C$ 的 MLLMs 上应用，而无需重新训练。
- en: However, we notice that applying the distillation loss alone during the early
    training phase can make the training challenging and unstable, potentially due
    to the strict requirement of fitting the exact representation. To address this,
    we adopt a progressive training strategy, by including an additional cross-entropy
    loss at first, which provides a better update direction in the early training
    phase. Thus, using a classification dataset, we first compute the cosine similarity
    between the image and text embeddings produced from the fixed CLIP visual and
    text encoder, respectively, when provided with transformed image latents and ground
    truth class labels. Then, the probability distribution over different recognition
    classes is calculated by applying the softmax operation to the resulting cosine
    similarities, and the cross-entropy loss is evaluated with respect to the ground
    truth labels.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们注意到仅在早期训练阶段应用蒸馏损失会使训练变得困难和不稳定，这可能是由于严格要求拟合精确表示。为了解决这个问题，我们采用了渐进式训练策略，首先包括一个额外的交叉熵损失，这在早期训练阶段提供了更好的更新方向。因此，使用分类数据集时，我们首先计算固定
    CLIP 视觉和文本编码器生成的图像和文本嵌入之间的余弦相似度，当提供变换的图像潜变量和真实标签时。然后，通过对结果余弦相似度应用 softmax 操作，计算不同识别类别的概率分布，并根据真实标签评估交叉熵损失。
- en: 3.5 Training Objective Under Different Settings
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 不同设置下的训练目标
- en: 'Table 1: Application scenarios for our method with corresponding training objective.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们方法的应用场景及对应的训练目标。
- en: '| Application scenario |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 应用场景 |'
- en: '&#124; Update image codec &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更新图像编解码器 &#124;'
- en: '|'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Human viewing &#124;'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人类观看 &#124;'
- en: '| Training objective |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 训练目标 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| (d1) Human perception | ✗ | ✓ | $\mathcal{L}_{dist}$ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (d1) 人类感知 | ✗ | ✓ | $\mathcal{L}_{dist}$ |'
- en: '| (d2) Multi-task | ✓ | ✓ | $R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| (d2) 多任务 | ✓ | ✓ | $R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    |'
- en: '| (d3) Machine perception | ✓ | ✗ | $R+\lambda\mathcal{L}_{dist}$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (d3) 机器感知 | ✓ | ✗ | $R+\lambda\mathcal{L}_{dist}$ |'
- en: 'To explore the capabilities of our method under the settings introduced in
    Section [3.2](#S3.SS2 "3.2 Overall Framework ‣ 3 Proposed Method ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"), we
    implement different training objectives, as summarized in Table [1](#S3.T1 "Table
    1 ‣ 3.5 Training Objective Under Different Settings ‣ 3 Proposed Method ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").
    In (d1), since we consider a fixed off-the-shelf codec for retaining high quality
    reconstructed images for human perception, we train our transform-neck simply
    using distillation loss as the sole loss function, i.e. $\mathcal{L}_{d1}=\mathcal{L}_{dist}$
    .'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '为了探索我们方法在第 [3.2](#S3.SS2 "3.2 总体框架 ‣ 3 提议的方法 ‣ ComNeck: 通过通用变换颈部连接压缩图像潜变量和多模态
    LLMs") 节中介绍的设置下的能力，我们实施了不同的训练目标，如表 [1](#S3.T1 "表 1 ‣ 3.5 不同设置下的训练目标 ‣ 3 提议的方法
    ‣ ComNeck: 通过通用变换颈部连接压缩图像潜变量和多模态 LLMs") 所总结。在 (d1) 中，由于我们考虑使用固定的现成编解码器来保持人类感知的高质量重建图像，因此我们仅使用蒸馏损失作为唯一损失函数来训练我们的变换颈部，即
    $\mathcal{L}_{d1}=\mathcal{L}_{dist}$。'
- en: On the other hand, in (d2), referred to as multi-task, the image codec is allowed
    to be re-trained to accommodate both human and machine perception. As a result,
    it is trained jointly with the transform-neck on both the distillation loss and
    traditional rate-distortion loss, i.e. $\mathcal{L}_{d2}=R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    weight the two losses.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在 (d2) 中，称为多任务，图像编解码器可以重新训练以适应人类和机器的感知。因此，它与变换颈部共同训练，以优化蒸馏损失和传统的比特率-失真损失，即
    $\mathcal{L}_{d2}=R+\lambda(\gamma d_{recon}(x,\hat{x})+\beta\mathcal{L}_{dist})$
    权衡这两种损失。
- en: In (d3), where the downstream applications do not require image reconstruction,
    the encoder and transform-neck are jointly optimized to minimize the trade-off
    cost between the rate $R$.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在 (d3) 中，考虑到下游应用不需要图像重建，编码器和变换颈部被共同优化，以最小化比特率 $R$ 之间的权衡成本。
- en: To facilitate the transform-neck in better learning the transformation with
    the image latents, the training process for (d2) and (d3) is conducted in three
    stages. Initially, only $\mathcal{L}_{dist}$, while keeping the updated image
    codec frozen.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地学习图像潜变量的转换，变换颈部的训练过程分为三个阶段进行。最初，仅使用$\mathcal{L}_{dist}$，同时保持更新的图像编解码器不变。
- en: 4 Experimental Results
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验结果
- en: 4.1 Experimental Setting
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Training Details and Datasets.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练细节和数据集。
- en: We utilize ELIC [[33](#bib.bib33)] as our image codec, which outputs image and
    hyperprior latents with $N=320$ leads to a good trade-off between human and machine
    perception.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用ELIC [[33](#bib.bib33)]作为我们的图像编解码器，它输出图像和超先验潜变量，其中$N=320$在人的感知和机器的感知之间取得了良好的折衷。
- en: '![Refer to caption](img/c88678042d5f0b29f3b9a5c7b11eaeaf.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c88678042d5f0b29f3b9a5c7b11eaeaf.png)'
- en: 'Figure 3: Rate-accuracy comparison using various MLLMs on several tasks.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用各种MLLM在多个任务上的速率-准确率比较。
- en: 'Table 2: Evaluation tasks with corresponding dataset, MLLM, and metric.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：具有相应数据集、MLLM和指标的评估任务。
- en: '| Task | Dataset | MLLM | Metric |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | MLLM | 指标 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Captioning |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 描述 |'
- en: '&#124; COCO &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; COCO &#124;'
- en: '&#124; Karpathy Test [[48](#bib.bib48)] &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Karpathy测试 [[48](#bib.bib48)] &#124;'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLaMA- &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA- &#124;'
- en: '&#124; Adapter [[9](#bib.bib9)] &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Adapter [[9](#bib.bib9)] &#124;'
- en: '| CIDEr |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| CIDEr |'
- en: '| VQA | SEED-Bench [[49](#bib.bib49)] | Honeybee [[5](#bib.bib5)] | Score |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| VQA | SEED-Bench [[49](#bib.bib49)] | 蜜蜂 [[5](#bib.bib5)] | 分数 |'
- en: '| REC | RefCOCO-val [[50](#bib.bib50)] | Shikra [[6](#bib.bib6)] | Accuracy
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| REC | RefCOCO-val [[50](#bib.bib50)] | Shikra [[6](#bib.bib6)] | 准确率 |'
- en: '|'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Few-shot &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 少量样本 &#124;'
- en: '&#124; classification &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '| ImageNet [[47](#bib.bib47)] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet [[47](#bib.bib47)] |'
- en: '&#124; V2L- &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; V2L- &#124;'
- en: '&#124; Tokenizer [[10](#bib.bib10)] &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Tokenizer [[10](#bib.bib10)] &#124;'
- en: '| Accuracy |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 |'
- en: '![Refer to caption](img/d77e898dfb742179af493032b76ae254.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d77e898dfb742179af493032b76ae254.png)'
- en: 'Figure 4: Reconstruction performance comparison on Kodak [[51](#bib.bib51)].'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在Kodak [[51](#bib.bib51)]上的重建性能比较。
- en: Targeted MLLM-based Vision Tasks.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于目标MLLM的视觉任务。
- en: 'To validate the generalization ability of our proposed method, we evaluate
    its performance on four different MLLM systems for four different tasks. Note
    that our method is independent of the downstream tasks, and thus the same set
    of transform-necks is used for all the evaluations. Additionally, all the MLLMs
    are employed off-the-shelf without any fine-tuning. The tasks, datasets, corresponding
    MLLMs, and metrics are listed in Table [4.1](#S4.SS1.SSS0.Px1 "Training Details
    and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging
    Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"). These
    configurations follow the settings outlined in their original papers and the accompanying
    code, except for the few-shot classification task due to the inaccessibility of
    the code. We thus design a 5-way 1-shot classification scenario to evaluate the
    performance with in-context learning; the detailed setting is described in supplementary
    material.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证我们提出方法的泛化能力，我们在四种不同的MLLM系统上评估其性能，涉及四种不同的任务。请注意，我们的方法与下游任务无关，因此相同的变换颈部用于所有评估。此外，所有MLLM均为现成使用，未进行任何微调。任务、数据集、对应的MLLM和指标列在表[4.1](#S4.SS1.SSS0.Px1
    "训练细节和数据集。 ‣ 4.1 实验设置 ‣ 4 实验结果 ‣ ComNeck：通过通用变换颈部桥接压缩图像潜变量和多模态LLMs")中。这些配置遵循其原始论文和附带代码中的设置，唯一例外是由于代码不可获取的少量样本分类任务。我们因此设计了一个5-way
    1-shot分类场景来评估在上下文学习中的性能；详细设置见补充材料。
- en: Baselines.
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准。
- en: 'We introduce two baseline methods for comparison. The first one, denoted as
    Reconstruction, involves inputting the reconstructed image generated by ELIC to
    the MLLM system. The second one, denoted as Post-processing, adapts the reconstructed
    image to MLLMs through a U-Net [[52](#bib.bib52)] post-processing network, which
    is trained using the same surrogate loss as that adopted by our method. We remark
    that these image-domain baselines incur higher complexity than our lightweight
    transform-neck, as they involve decoding the image and potentially processing
    it further with the post-processing network. This aspect is discussed in detail
    in Section [4.4](#S4.SS4 "4.4 Complexity Analysis ‣ 4 Experimental Results ‣ ComNeck:
    Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '我们引入了两种基准方法进行比较。第一种，称为重建，涉及将由ELIC生成的重建图像输入到MLLM系统中。第二种，称为后处理，通过U-Net [[52](#bib.bib52)]
    后处理网络将重建图像适配到MLLM，这个网络使用了与我们的方法相同的替代损失进行训练。我们指出，这些图像域基准方法的复杂性高于我们的轻量级变换颈部，因为它们涉及解码图像，并可能进一步使用后处理网络处理图像。这个方面在第[4.4节](#S4.SS4
    "4.4 Complexity Analysis ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck")中进行了详细讨论。'
- en: 4.2 Performance Comparison
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 性能比较
- en: 'Figure [3](#S4.F3 "Figure 3 ‣ Training Details and Datasets. ‣ 4.1 Experimental
    Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck") illustrates the performance
    of the baseline methods and our proposed scheme for the three examined scenarios
    with regards to two aspects: coding bit-rate, calculated as bits per pixel (bpp),
    and task performance. When comparing the baselines and our method in scenario
    (d1), where the original ELIC is trained solely for human perception, we make
    the following observations. (1) Straightforwardly using the reconstructed images
    generated by a codec trained for human perception leads to a significant performance
    drop across all the tasks (Reconstruction). Such performance decline is expected
    because the MLLMs are not trained with compressed images, thus hindering their
    recognition performance. This highlights the necessity of adapting image compression
    and/or image latents to MLLMs. (2) In contrast, our transform-neck method successfully
    boosts the performance using the same latent representations for reconstructing
    the image in Reconstruction, confirming the effectiveness of the proposed latent
    transformation without the decoding process. (3) Post-processing is able to reach
    comparable performance to our (d1), offering another viable solution to the problem.
    However, it is worth noting that Post-processing requires relatively higher computational
    complexity with respect to our transform-neck method, rendering our approach preferable
    (see Section [4.4](#S4.SS4 "4.4 Complexity Analysis ‣ 4 Experimental Results ‣
    ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck"))
    .'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](#S4.F3 "Figure 3 ‣ Training Details and Datasets. ‣ 4.1 Experimental Setting
    ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck")展示了基准方法和我们提出的方案在三种 examined 情境下的表现，考虑了两个方面：编码比特率，以每像素比特（bpp）计算，以及任务性能。在场景(d1)中，与基准方法和我们的方法相比，原始ELIC仅针对人类感知训练，我们作出如下观察。（1）直接使用经过人类感知训练的编解码器生成的重建图像会导致所有任务的性能显著下降（重建）。这种性能下降是预期的，因为MLLM没有使用压缩图像进行训练，因此影响了它们的识别性能。这突显了将图像压缩和/或图像潜在特征适配到MLLM的必要性。（2）相反，我们的变换颈部方法成功提升了性能，使用相同的潜在表示来重建图像，验证了提议的潜在变换在没有解码过程的情况下的有效性。（3）后处理能够达到与我们(d1)相当的性能，提供了另一个可行的解决方案。然而，值得注意的是，后处理相对于我们的变换颈部方法需要较高的计算复杂性，这使得我们的方法更为可取（参见第[4.4节](#S4.SS4
    "4.4 Complexity Analysis ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck")）。'
- en: '|'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Captioning'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  标注'
- en: LLaMA-Adapter  &#124;
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-Adapter  &#124;
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/a0668bda023469c5ddf2c5171dffc577.png) &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[未标注的图像]](img/a0668bda023469c5ddf2c5171dffc577.png) &#124;'
- en: '&#124; Reconstruction: A man is walking an elephant down a path. &#124;'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建：一名男子正沿着小路牵着大象。&#124;'
- en: '&#124; Post-processing: A man feeding an elephant with his hand. &#124;'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 后处理：一名男子用手喂大象。&#124;'
- en: '&#124; Ours (d1): A man is petting an elephant on the head. &#124;'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们的方法 (d1)：一名男子正在抚摸大象的头部。&#124;'
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0958  &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP: 0.0958  &#124;'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  REC'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  录音'
- en: Shikra  &#124;
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Shikra  &#124;
- en: '|'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; I’m trying to locate man with mask on in . Can you determine its
    coordinates for me? &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我正在尝试定位图像中的戴面具的男人。你能帮我确定它的坐标吗？ &#124;'
- en: '&#124; ![[Uncaptioned image]](img/ed41e02e45d5df1cef4d9548ea212d94.png) &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/ed41e02e45d5df1cef4d9548ea212d94.png) &#124;'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.061  &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP:0.061 &#124;'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 5: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on image captioning with LLaMA-Adapter and REC with Shikra.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 我们提出的方法在（d1）、重建和后处理中的可视化示例，应用于图像标注，使用 LLaMA-Adapter 和 Shikra 的 REC。'
- en: 'Next, we evaluate the effects of allowing the image codec to be re-trained.
    First, we observe that (d2) outperforms both (d1) and Post-processing. This indicates
    that fine-tuning the encoder indeed results in a more suitable latent representation
    that can be better adapted to MLLMs. When examining the extreme setting (d3),
    we see significant further improvement in the task performance, approaching the
    performance upper bound with uncompressed images. This improvement comes at the
    cost of the image reconstruction quality, which, however, is not relevant in (d3).
    Figure [4](#S4.F4 "Figure 4 ‣ Training Details and Datasets. ‣ 4.1 Experimental
    Setting ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck") illustrates the rate-visual
    quality curves associated with the three scenarios. Interestingly, (d2) exhibits
    only a marginal PSNR drop compared to (d1), while (d3) significantly compromises
    the quality of the decoded image. We stress that our framework (i.e. the surrogate
    loss and transform-neck) is able to accommodate different application scenarios,
    allowing for a variable trade-off between the task performance and the image reconstruction
    quality.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们评估允许图像编码器重新训练的效果。首先，我们观察到（d2）优于（d1）和后处理。这表明微调编码器确实能生成更适合 MLLMs 的潜在表示。当检查极端设置（d3）时，我们看到任务性能显著提升，接近无压缩图像的性能上限。这一改善以图像重建质量为代价，但在（d3）中这一点并不相关。图 [4](#S4.F4
    "Figure 4 ‣ Training Details and Datasets. ‣ 4.1 Experimental Setting ‣ 4 Experimental
    Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") 展示了与三种场景相关的速率-视觉质量曲线。有趣的是，与（d1）相比，（d2）仅出现了轻微的 PSNR 下降，而（d3）则显著影响了解码图像的质量。我们强调，我们的框架（即替代损失和变换颈部）能够适应不同的应用场景，允许在任务性能和图像重建质量之间进行可变的权衡。'
- en: 4.3 Visualization of the Results
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 结果可视化
- en: 'We present the visualization of outcomes with downstream MLLM-based vision
    tasks in Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Performance Comparison ‣ 4 Experimental
    Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck"). Our method (d1) is compared with the two baseline methods, Reconstruction
    and Post-processing, with particular focus on how these models work at low bitrates
    to reflect a bandwidth-limited scenario. In the second and third columns (from
    left to right), we visualize the reconstructed and post-processed images from
    the two baselines, respectively, which exhibit drastically different characteristics.
    The former (Reconstruction) produces blurry and smooth images, while the latter
    (Post-processing) introduces some artificial patterns into the post-processed
    images. Compared with the baselines, our method yields MLLM results closer to
    the ground truth across all the tasks. Due to the space constraint, the results
    of VQA and few-shot classification are visualized in the supplementary material.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [5](#S4.F5 "Figure 5 ‣ 4.2 Performance Comparison ‣ 4 Experimental Results
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") 中展示了下游 MLLM 基于视觉任务的结果可视化。我们的方法（d1）与两个基线方法，重建和后处理进行比较，特别关注这些模型在低比特率下如何反映带宽限制的场景。在第二和第三列（从左到右），我们分别可视化了两个基线的重建和后处理图像，这些图像展现出截然不同的特征。前者（重建）产生模糊和平滑的图像，而后者（后处理）则在后处理图像中引入了一些人工模式。与基线相比，我们的方法在所有任务中生成的
    MLLM 结果更接近真实情况。由于篇幅限制，VQA 和少样本分类的结果在补充材料中进行了可视化。'
- en: 'Table 3: Comparison of the kMACs/pixel and model size. The table omits the
    shared components of the two methods, i.e. the image encoder, the partial CLIP
    visual encoder, the connector, and the LLM.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: kMACs/像素和模型大小的比较。该表省略了两种方法的共享组件，即图像编码器、部分 CLIP 视觉编码器、连接器和 LLM。'
- en: '| Method | Component | Params (M) | kMAC/pixel |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 组件 | 参数 (M) | kMAC/像素 |'
- en: '| Ours (d1, d2, or d3) | Transform-neck | 13.19 | 52.795 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 (d1, d2, 或 d3) | Transform-neck | 13.19 | 52.795 |'
- en: '| Post-processing | Decoder | 7.34 | 64.16 (+386%) | 112.00 | 1017.96 (+1828%)
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 后处理 | 解码器 | 7.34 | 64.16 (+386%) | 112.00 | 1017.96 (+1828%) |'
- en: '| Post-processing network | 31.04 | 835.72 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 后处理网络 | 31.04 | 835.72 |'
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; First 2 layers of CLIP visual encoder &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CLIP视觉编码器的前2层 &#124;'
- en: '| 25.78 | 70.24 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 25.78 | 70.24 |'
- en: 4.4 Complexity Analysis
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 复杂性分析
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.3 Visualization of the Results ‣ 4 Experimental
    Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") compares the computational complexity between our proposed method
    and Post-processing baseline in terms of model size and the kilo-multiply-accumulate-operations
    per pixel (kMACs/pixel). Note that our method in Table [3](#S4.T3 "Table 3 ‣ 4.3
    Visualization of the Results ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed
    Image Latents and Multimodal LLMs via Universal Transform-Neck") refers to any
    of (d1), (d2), and (d3), since they share the same computational complexity characteristics
    at inference time. Our method offers a lightweight solution with only 13 million
    parameters, as opposed to 64 million parameters with the post-processing approach.
    Moreover, in terms of kMAC/pixel, the difference stands out even more, considering
    that the post-processing network operates at the full image resolution while our
    method operates in the latent domain, where the image latents have a much smaller
    spatial resolution.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "表 3 ‣ 4.3 结果可视化 ‣ 4 实验结果 ‣ ComNeck: 通过通用的Transform-Neck桥接压缩图像潜变量和多模态LLMs")
    比较了我们提出的方法和后处理基线在模型大小和每像素千次乘加运算（kMACs/pixel）方面的计算复杂性。请注意，表 [3](#S4.T3 "表 3 ‣ 4.3
    结果可视化 ‣ 4 实验结果 ‣ ComNeck: 通过通用的Transform-Neck桥接压缩图像潜变量和多模态LLMs") 中的方法指的是 (d1)、(d2)
    和 (d3)，因为它们在推理时具有相同的计算复杂性特征。我们的方法提供了一种轻量级的解决方案，只有1300万参数，而后处理方法则有6400万参数。此外，在每像素kMAC方面的差异更加明显，因为后处理网络在完整图像分辨率下操作，而我们的方法则在潜在领域操作，其中图像潜变量的空间分辨率要小得多。'
- en: '![Refer to caption](img/442cea3b76fc579d1d154c86a9c2f3b5.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/442cea3b76fc579d1d154c86a9c2f3b5.png)'
- en: (a) Partial CLIP visual encoder
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 部分CLIP视觉编码器
- en: '![Refer to caption](img/9e40cbd6abbc871cd87e27325b28e38c.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e40cbd6abbc871cd87e27325b28e38c.png)'
- en: (b) Training objectives
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 训练目标
- en: '![Refer to caption](img/3a97183c8b81e27985f91ab54fe3b4b6.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a97183c8b81e27985f91ab54fe3b4b6.png)'
- en: (c) Different image codecs
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 不同的图像编解码器
- en: 'Figure 6: Rate-accuracy comparison for three ablation studies evaluated using
    LLaMA-adapter with image captioning on COCO Karpathy test split.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：三项消融研究的准确率比较，使用LLaMA-adapter在COCO Karpathy测试集上进行图像标注。
- en: 4.5 Ablation Studies
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融研究
- en: The following ablation experiments are performed based on (d1) to justify our
    design choices.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下消融实验基于 (d1) 进行，以验证我们的设计选择。
- en: Partial CLIP Visual Encoder.
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 部分CLIP视觉编码器。
- en: 'This experiment investigates the proper number of Transformer layers to remove
    from the CLIP visual encoder in order to strike a good balance between complexity
    and performance. As shown in Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis
    ‣ 4 Experimental Results ‣ ComNeck: Bridging Compressed Image Latents and Multimodal
    LLMs via Universal Transform-Neck") (a), removing the first one or two layers
    achieves similar performance, whereas removing four or eight layers results in
    a noticeable performance drop. We thus remove the first two layers.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '本实验研究了从CLIP视觉编码器中移除适当数量的Transformer层，以在复杂性和性能之间取得良好的平衡。如图 [6](#S4.F6 "图 6 ‣
    4.4 复杂性分析 ‣ 4 实验结果 ‣ ComNeck: 通过通用的Transform-Neck桥接压缩图像潜变量和多模态LLMs") (a) 所示，移除前一到两层能够达到类似的性能，而移除四层或八层则会导致明显的性能下降。因此，我们移除了前两层。'
- en: Training Objective.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练目标。
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (b) presents the performance of our method when trained exclusively
    with the cross-entropy loss or distillation loss. It is observed that training
    with only the cross-entropy loss results in a significant performance drop. Although
    providing a good initial update direction, this strategy is unable to learn an
    effective transformation for MLLMs. Instead, training solely with the distillation
    loss fails to update the transform-neck properly and leads to far inferior performance.
    This is potentially due to the more stringent requirement of fitting the exact
    feature representations.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](#S4.F6 "图 6 ‣ 4.4 复杂性分析 ‣ 4 实验结果 ‣ ComNeck: 通过通用变换颈桥接压缩图像潜变量和多模态 LLMs")
    (b) 展示了我们的方法在仅使用交叉熵损失或蒸馏损失训练时的表现。观察到，仅使用交叉熵损失进行训练会导致性能显著下降。尽管提供了良好的初始更新方向，但该策略无法学习有效的
    MLLMs 变换。相反，仅使用蒸馏损失进行训练无法正确更新变换颈，并导致性能远远低于预期。这可能是由于更严格的拟合精确特征表示的要求。'
- en: Different Image Codecs.
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同的图像编解码器。
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.4 Complexity Analysis ‣ 4 Experimental Results
    ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal
    Transform-Neck") (c) presents the performance comparison between our method and
    Reconstruction when they are tested with ELIC and TIC [[34](#bib.bib34), [53](#bib.bib53)].
    TIC is a Transformer-based codec, whereas ELIC is a convolutional neural network-based
    codec. We see that our transform-neck still outperforms Reconstruction by a significant
    margin when the image codec is changed from ELIC to TIC. This indicates that our
    method is still effective on a different type of image codec.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '图[6](#S4.F6 "图 6 ‣ 4.4 复杂性分析 ‣ 4 实验结果 ‣ ComNeck: 通过通用变换颈桥接压缩图像潜变量和多模态 LLMs")
    (c) 展示了我们的方法与重建方法在使用 ELIC 和 TIC [[34](#bib.bib34), [53](#bib.bib53)] 测试时的性能比较。TIC
    是基于 Transformer 的编解码器，而 ELIC 是基于卷积神经网络的编解码器。我们看到，当图像编解码器从 ELIC 更改为 TIC 时，我们的变换颈仍然明显优于重建方法。这表明我们的方法在不同类型的图像编解码器上仍然有效。'
- en: 5 Conclusion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This paper proposes the first image compression system tailored to Multimodal
    Large Language Models (MLLMs). It introduces a transform-neck that bridges the
    compressed image latents and the intermediate layer of the CLIP visual encoder,
    a common component in MLLMs. By using our proposed surrogate loss, we avoid involving
    the MLLM in the training process, making our transform-neck universally applicable.
    With lower computational complexity, our method has demonstrated effectiveness
    across a wide variety of tasks, MLLMs, and neual image codecs, outperforming other
    baselines in extensive experiments. One consideration is that it requires the
    same pre-trained CLIP visual encoder to leverage the universal transform-neck,
    which may limit compatibility with MLLMs that use custom visual encoders. Furthermore,
    this paper focuses solely on the image compression aspect of MLLMs, leaving the
    exploration of video or audio coding for future work.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了第一个专为多模态大语言模型（MLLMs）量身定制的图像压缩系统。它引入了一种变换颈，连接了压缩图像潜变量和 CLIP 视觉编码器的中间层，CLIP
    视觉编码器是 MLLMs 中的一个常见组件。通过使用我们提出的替代损失，我们避免了在训练过程中涉及 MLLM，使我们的变换颈具有普遍适用性。凭借较低的计算复杂性，我们的方法在广泛的任务、MLLMs
    和神经图像编解码器中表现出了有效性，在大量实验中优于其他基准。需要注意的是，它要求使用相同的预训练 CLIP 视觉编码器来利用通用变换颈，这可能限制了与使用自定义视觉编码器的
    MLLMs 的兼容性。此外，本文仅关注 MLLMs 的图像压缩方面，将视频或音频编码的探索留待未来工作。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., “Llama 2: Open foundation and fine-tuned
    chat models,” arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等，“Llama 2: 开放的基础和微调聊天模型，”arXiv 预印本
    arXiv:2307.09288, 2023。'
- en: '[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., “Llama: Open and efficient
    foundation language models,” arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar 等，“Llama: 开放且高效的基础语言模型，”arXiv 预印本 arXiv:2302.13971,
    2023。'
- en: '[3] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al., “Mistral 7b,”
    arXiv preprint arXiv:2310.06825, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier 等，“Mistral 7b”，arXiv
    预印本 arXiv:2310.06825，2023年。'
- en: '[4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al., “Gpt-4 technical report,” arXiv
    preprint arXiv:2303.08774, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat 等，“Gpt-4 技术报告”，arXiv 预印本 arXiv:2303.08774，2023年。'
- en: '[5] J. Cha, W. Kang, J. Mun, and B. Roh, “Honeybee: Locality-enhanced projector
    for multimodal llm,” in Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), June 2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] J. Cha, W. Kang, J. Mun, 和 B. Roh，“Honeybee: 本地化增强的多模态 llm 投影仪”，发表于 IEEE/CVF
    计算机视觉与模式识别会议 (CVPR) 论文集中，2024年6月。'
- en: '[6] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, “Shikra: Unleashing
    multimodal llm’s referential dialogue magic,” arXiv preprint arXiv:2306.15195,
    2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, 和 R. Zhao，“Shikra: 释放多模态
    llm 的指涉对话魔法”，arXiv 预印本 arXiv:2306.15195，2023年。'
- en: '[7] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz,
    M. Shoeybi, and S. Han, “Vila: On pre-training for visual language models,” in
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz,
    M. Shoeybi, 和 S. Han，“Vila: 视觉语言模型的预训练研究”，发表于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集中，2024年。'
- en: '[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, et al., “Gemini: a family of highly capable
    multimodal models,” arXiv preprint arXiv:2312.11805, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth 等，“Gemini: 一系列高能力的多模态模型”，arXiv 预印本 arXiv:2312.11805，2023年。'
- en: '[9] R. Zhang, J. Han, C. Liu, P. Gao, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li,
    and Y. Qiao, “Llama-adapter: Efficient fine-tuning of language models with zero-init
    attention,” in International Conference on Learning Representations (ICLR), 2024.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. Zhang, J. Han, C. Liu, P. Gao, A. Zhou, X. Hu, S. Yan, P. Lu, H. Li,
    和 Y. Qiao，“Llama-adapter: 用零初始化注意力高效微调语言模型”，发表于国际学习表征会议 (ICLR)，2024年。'
- en: '[10] L. Zhu, F. Wei, and Y. Lu, “Beyond text: Frozen large language models
    in visual signal comprehension,” in Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition (CVPR), June 2024.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] L. Zhu, F. Wei, 和 Y. Lu，“超越文本：冻结的大型语言模型在视觉信号理解中的应用”，发表于 IEEE/CVF 计算机视觉与模式识别会议
    (CVPR) 论文集中，2024年6月。'
- en: '[11] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models,” in International
    conference on machine learning, pp. 19730–19742, PMLR, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] J. Li, D. Li, S. Savarese, 和 S. Hoi，“Blip-2: 使用冻结图像编码器和大型语言模型引导语言-图像预训练”，发表于国际机器学习会议，第19730–19742页，PMLR，2023年。'
- en: '[12] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, H. R. Tavakoli, and
    E. Rahtu, “Learned image coding for machines: A content-adaptive approach,” in
    2021 IEEE International Conference on Multimedia and Expo (ICME), pp. 1–6, IEEE,
    2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, H. R. Tavakoli, 和 E.
    Rahtu，“机器学习图像编码：一种内容自适应方法”，发表于 2021 IEEE 国际多媒体与博览会 (ICME)，第1–6页，IEEE，2021年。'
- en: '[13] L. D. Chamain, F. Racapé, J. Bégaint, A. Pushparaja, and S. Feltman, “End-to-end
    optimized image compression for machines, a study,” in 2021 Data Compression Conference
    (DCC), pp. 163–172, IEEE, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] L. D. Chamain, F. Racapé, J. Bégaint, A. Pushparaja, 和 S. Feltman，“面向机器的端到端优化图像压缩研究”，发表于
    2021 数据压缩会议 (DCC)，第163–172页，IEEE，2021年。'
- en: '[14] Y. Matsubara, R. Yang, M. Levorato, and S. Mandt, “Supervised compression
    for resource-constrained edge computing systems,” in Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, pp. 2685–2695, 2022.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. Matsubara, R. Yang, M. Levorato, 和 S. Mandt，“资源受限边缘计算系统的监督压缩”，发表于 IEEE/CVF
    计算机视觉应用冬季会议论文集中，第2685–2695页，2022年。'
- en: '[15] J. Liu, H. Sun, and J. Katto, “Improving multiple machine vision tasks
    in the compressed domain,” in 2022 26th International Conference on Pattern Recognition
    (ICPR), pp. 331–337, IEEE, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Liu, H. Sun, 和 J. Katto，“在压缩域中改进多机器视觉任务”，发表于 2022年第26届国际模式识别大会 (ICPR)，第331–337页，IEEE，2022年。'
- en: '[16] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, and E. Rahtu, “Image
    coding for machines: an end-to-end learned approach,” in ICASSP 2021-2021 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP),
    pp. 1590–1594, IEEE, 2021.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] N. Le, H. Zhang, F. Cricri, R. Ghaznavi-Youvalari, 和 E. Rahtu, “机器图像编码：一种端到端学习的方法，”
    见于 ICASSP 2021-2021 IEEE 国际声学、语音与信号处理会议 (ICASSP)，第1590–1594页，IEEE，2021年。'
- en: '[17] D. Ding, Z. Chen, Z. Liu, X. Xu, and S. Liu, “Hierarchical image feature
    compression for machines via feature sparsity learning,” IEEE Signal Processing
    Letters, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] D. Ding, Z. Chen, Z. Liu, X. Xu, 和 S. Liu, “通过特征稀疏学习实现的机器层次图像特征压缩，” IEEE
    信号处理快报，2024年。'
- en: '[18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, et al., “Learning transferable visual models
    from natural language supervision,” in International conference on machine learning,
    pp. 8748–8763, PMLR, 2021.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, 等，“从自然语言监督中学习可迁移的视觉模型，” 见于国际机器学习会议， 第8748–8763页，PMLR，2021年。'
- en: '[19] H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” in Conference
    on Neural Information Processing Systems (NeurIPS), 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Liu, C. Li, Q. Wu, 和 Y. J. Lee, “视觉指令调优，” 见于神经信息处理系统会议 (NeurIPS)，2023年。'
- en: '[20] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi,
    V. Chandra, Y. Xiong, and M. Elhoseiny, “Minigpt-v2: large language model as a
    unified interface for vision-language multi-task learning,” arXiv preprint arXiv:2310.09478,
    2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] J. Chen, D. Zhu, X. Shen, X. Li, Z. Liu, P. Zhang, R. Krishnamoorthi,
    V. Chandra, Y. Xiong, 和 M. Elhoseiny, “Minigpt-v2: 作为视觉-语言多任务学习统一接口的大型语言模型，” arXiv
    预印本 arXiv:2310.09478, 2023。'
- en: '[21] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, and F. Wei, “Kosmos-2:
    Grounding multimodal large language models to the world,” in International Conference
    on Learning Representations (ICLR), 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Z. Peng, W. Wang, L. Dong, Y. Hao, S. Huang, S. Ma, 和 F. Wei, “Kosmos-2:
    将多模态大型语言模型与世界对接，” 见于国际学习表征会议 (ICLR)，2024年。'
- en: '[22] Y. Zhang, Z. Ma, X. Gao, S. Shakiah, Q. Gao, and J. Chai, “Groundhog:
    Grounding large language models to holistic segmentation,” in Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Y. Zhang, Z. Ma, X. Gao, S. Shakiah, Q. Gao, 和 J. Chai, “Groundhog: 将大型语言模型与整体分割对接，”
    见于 IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集，2024年6月。'
- en: '[23] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang,
    and Y. Yang, “Ferret: Refer and ground anything anywhere at any granularity,”
    in International Conference on Learning Representations (ICLR), 2024.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. You, H. Zhang, Z. Gan, X. Du, B. Zhang, Z. Wang, L. Cao, S.-F. Chang,
    和 Y. Yang, “Ferret: 以任何细粒度引用和对接任何内容，” 见于国际学习表征会议 (ICLR)，2024年。'
- en: '[24] L. Yu, Y. Cheng, Z. Wang, V. Kumar, W. Macherey, Y. Huang, D. Ross, I. Essa,
    Y. Bisk, M.-H. Yang, et al., “Spae: Semantic pyramid autoencoder for multimodal
    generation with frozen llms,” Advances in Neural Information Processing Systems,
    vol. 36, 2024.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] L. Yu, Y. Cheng, Z. Wang, V. Kumar, W. Macherey, Y. Huang, D. Ross, I.
    Essa, Y. Bisk, M.-H. Yang, 等，“Spae: 语义金字塔自编码器用于冻结大型语言模型的多模态生成，” 神经信息处理系统进展，第36卷，2024年。'
- en: '[25] H. Mittal, N. Agarwal, S.-Y. Lo, and K. Lee, “Can’t make an omelette without
    breaking some eggs: Plausible action,” in Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Mittal, N. Agarwal, S.-Y. Lo, 和 K. Lee, “要想做出煎蛋卷，必须打破一些鸡蛋：合理的行动，” 见于
    IEEE/CVF 计算机视觉与模式识别会议 (CVPR) 论文集，2024年。'
- en: '[26] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,
    M. Zeng, and L. Wang, “Mm-react: Prompting chatgpt for multimodal reasoning and
    action,” arXiv preprint arXiv:2303.11381, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,
    M. Zeng, 和 L. Wang, “Mm-react: 促进 chatgpt 进行多模态推理和行动，” arXiv 预印本 arXiv:2303.11381,
    2023。'
- en: '[27] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H. Poon,
    and J. Gao, “Llava-med: Training a large language-and-vision assistant for biomedicine
    in one day,” Advances in Neural Information Processing Systems, vol. 36, 2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] C. Li, C. Wong, S. Zhang, N. Usuyama, H. Liu, J. Yang, T. Naumann, H.
    Poon, 和 J. Gao, “Llava-med: 一天内训练一个大型语言和视觉助手以支持生物医学，” 神经信息处理系统进展，第36卷，2024年。'
- en: '[28] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D. Duckworth,
    S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mordatch,
    and P. Florence, “Palm-e: An embodied multimodal language model,” in arXiv preprint
    arXiv:2303.03378, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
    A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang, Y. Chebotar, P. Sermanet, D.
    Duckworth, S. Levine, V. Vanhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng,
    I. Mordatch, 和 P. Florence，“Palm-e：一个具身的多模态语言模型”，发表于arXiv预印本arXiv:2303.03378，2023年。'
- en: '[29] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, “Minigpt-4: Enhancing
    vision-language understanding with advanced large language models,” in International
    Conference on Learning Representations (ICLR), 2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] D. Zhu, J. Chen, X. Shen, X. Li, 和 M. Elhoseiny，“Minigpt-4：通过先进的大型语言模型增强视觉-语言理解”，发表于国际学习表征会议（ICLR），2024年。'
- en: '[30] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung,
    and S. Hoi, “Instructblip: Towards general-purpose vision-language models with
    instruction tuning,” Advances in Neural Information Processing Systems, vol. 36,
    2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. N. Fung,
    和 S. Hoi，“Instructblip：面向通用视觉-语言模型的指令调优”，发表于神经信息处理系统进展，卷36，2024年。'
- en: '[31] Q. Ye, H. Xu, J. Ye, M. Yan, H. Liu, Q. Qian, J. Zhang, F. Huang, and
    J. Zhou, “mplug-owl2: Revolutionizing multi-modal large language model with modality
    collaboration,” arXiv preprint arXiv:2311.04257, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Q. Ye, H. Xu, J. Ye, M. Yan, H. Liu, Q. Qian, J. Zhang, F. Huang, 和 J.
    Zhou，“mplug-owl2：通过模态协作革新多模态大语言模型”，arXiv预印本arXiv:2311.04257，2023年。'
- en: '[32] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,
    A. Mensch, K. Millican, M. Reynolds, et al., “Flamingo: a visual language model
    for few-shot learning,” Advances in neural information processing systems, vol. 35,
    pp. 23716–23736, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc,
    A. Mensch, K. Millican, M. Reynolds, 等，“Flamingo：用于少样本学习的视觉语言模型”，发表于神经信息处理系统进展，卷35，页码23716–23736，2022年。'
- en: '[33] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, and Y. Wang, “Elic: Efficient
    learned image compression with unevenly grouped space-channel contextual adaptive
    coding,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pp. 5718–5727, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] D. He, Z. Yang, W. Peng, R. Ma, H. Qin, 和 Y. Wang，“Elic：具有不均匀分组空间-通道上下文自适应编码的高效学习图像压缩”，发表于IEEE/CVF计算机视觉与模式识别会议，页码5718–5727，2022年。'
- en: '[34] M. Lu, F. Chen, S. Pu, and Z. Ma, “High-efficiency lossy image coding
    through adaptive neighborhood information aggregation,” arXiv preprint arXiv:2204.11448,
    2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] M. Lu, F. Chen, S. Pu, 和 Z. Ma，“通过自适应邻域信息聚合实现高效有损图像编码”，arXiv预印本arXiv:2204.11448，2022年。'
- en: '[35] J. Liu, H. Sun, and J. Katto, “Learned image compression with mixed transformer-cnn
    architectures,” in Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, pp. 14388–14397, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Liu, H. Sun, 和 J. Katto，“混合transformer-cnn架构的学习图像压缩”，发表于IEEE/CVF计算机视觉与模式识别会议，页码14388–14397，2023年。'
- en: '[36] Y. Xie, K. L. Cheng, and Q. Chen, “Enhanced invertible encoding for learned
    image compression,” in Proceedings of the 29th ACM international conference on
    multimedia, pp. 162–170, 2021.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Y. Xie, K. L. Cheng, 和 Q. Chen，“增强的可逆编码用于学习图像压缩”，发表于第29届ACM国际多媒体会议，页码162–170，2021年。'
- en: '[37] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, and J.-R.
    Ohm, “Overview of the versatile video coding (vvc) standard and its applications,”
    IEEE Transactions on Circuits and Systems for Video Technology, vol. 31, no. 10,
    pp. 3736–3764, 2021.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] B. Bross, Y.-K. Wang, Y. Ye, S. Liu, J. Chen, G. J. Sullivan, 和 J.-R.
    Ohm，“多功能视频编码（vvc）标准及其应用概述”，IEEE电路与系统视频技术汇刊，卷31，第10期，页码3736–3764，2021年。'
- en: '[38] S. Wang, Z. Wang, S. Wang, and Y. Ye, “Deep image compression towards
    machine vision: A unified optimization framework,” IEEE Transactions on Circuits
    and Systems for Video Technology, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Wang, Z. Wang, S. Wang, 和 Y. Ye，“面向机器视觉的深度图像压缩：一个统一的优化框架”，IEEE电路与系统视频技术汇刊，2022年。'
- en: '[39] Y.-H. Chen, Y.-C. Weng, C.-H. Kao, C. Chien, W.-C. Chiu, and W.-H. Peng,
    “Transtic: Transferring transformer-based image compression from human perception
    to machine perception,” in Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 23297–23307, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y.-H. Chen, Y.-C. Weng, C.-H. Kao, C. Chien, W.-C. Chiu, 和 W.-H. Peng，“Transtic：将基于transformer的图像压缩从人类感知转移到机器感知”，发表于IEEE/CVF国际计算机视觉会议，页码23297–23307，2023年。'
- en: '[40] J. Ascenso, E. Alshina, and T. Ebrahimi, “The jpeg ai standard: Providing
    efficient human and machine visual data consumption,” Ieee Multimedia, vol. 30,
    no. 1, pp. 100–111, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Ascenso, E. Alshina, 和 T. Ebrahimi, “jpeg ai标准：提供高效的人类与机器视觉数据消费，” IEEE
    Multimedia, 第30卷，第1期，第100–111页，2023年。'
- en: '[41] J. Liu, H. Sun, and J. Katto, “Improving multiple machine vision tasks
    in the compressed domain,” in 2022 26th International Conference on Pattern Recognition
    (ICPR), pp. 331–337, IEEE, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Liu, H. Sun, 和 J. Katto, “在压缩域中改进多个机器视觉任务，” 见于2022年第26届国际模式识别会议（ICPR），第331–337页，IEEE，2022年。'
- en: '[42] J. Liu, H. Sun, and J. Katto, “Learning in compressed domain for faster
    machine vision tasks,” in 2021 International Conference on Visual Communications
    and Image Processing (VCIP), pp. 01–05, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. Liu, H. Sun, 和 J. Katto, “压缩域中的学习以加快机器视觉任务，” 见于2021年国际视觉通信与图像处理会议（VCIP），第01–05页，2021年。'
- en: '[43] Y. Mei, F. Li, L. Li, and Z. Li, “Learn a compression for objection detection
    - vae with a bridge,” in 2021 International Conference on Visual Communications
    and Image Processing (VCIP), pp. 1–5, 2021.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Mei, F. Li, L. Li, 和 Z. Li, “学习用于目标检测的压缩 - vae与桥接，” 见于2021年国际视觉通信与图像处理会议（VCIP），第1–5页，2021年。'
- en: '[44] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ballé, A. Shrivastava, and
    G. Toderici, “End-to-end learning of compressible features,” in 2020 IEEE International
    Conference on Image Processing (ICIP), pp. 3349–3353, IEEE, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Singh, S. Abu-El-Haija, N. Johnston, J. Ballé, A. Shrivastava, 和 G.
    Toderici, “可压缩特征的端到端学习，” 见于2020年IEEE国际图像处理会议（ICIP），第3349–3353页，IEEE，2020年。'
- en: '[45] R. Feng, X. Jin, Z. Guo, R. Feng, Y. Gao, T. He, Z. Zhang, S. Sun, and
    Z. Chen, “Image coding for machines with omnipotent feature learning,” in Computer
    Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022,
    Proceedings, Part XXXVII, pp. 510–528, Springer, 2022.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. Feng, X. Jin, Z. Guo, R. Feng, Y. Gao, T. He, Z. Zhang, S. Sun, 和 Z.
    Chen, “面向机器的图像编码与全能特征学习，” 见于《计算机视觉–ECCV 2022：第17届欧洲会议》，以色列特拉维夫，2022年10月23日至27日，会议录，第XXXVII部分，第510–528页，Springer，2022年。'
- en: '[46] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, and N. Johnston, “Variational
    image compression with a scale hyperprior,” arXiv preprint arXiv:1802.01436, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Ballé, D. Minnen, S. Singh, S. J. Hwang, 和 N. Johnston, “具有尺度超先验的变分图像压缩，”
    arXiv预印本 arXiv:1802.01436，2018年。'
- en: '[47] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in 2009 IEEE conference on computer
    vision and pattern recognition, pp. 248–255, Ieee, 2009.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei, “Imagenet:
    大规模层次图像数据库，” 见于2009年IEEE计算机视觉与模式识别会议，第248–255页，IEEE，2009年。'
- en: '[48] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating
    image descriptions,” in Proceedings of the IEEE conference on computer vision
    and pattern recognition, pp. 3128–3137, 2015.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] A. Karpathy 和 L. Fei-Fei, “深度视觉-语义对齐生成图像描述，” 见于IEEE计算机视觉与模式识别会议， 第3128–3137页，2015年。'
- en: '[49] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, “Seed-bench: Benchmarking
    multimodal llms with generative comprehension,” arXiv preprint arXiv:2307.16125,
    2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, 和 Y. Shan, “Seed-bench: 用于生成理解的多模态LLMs基准测试，”
    arXiv预印本 arXiv:2307.16125，2023年。'
- en: '[50] S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring
    to objects in photographs of natural scenes,” in Proceedings of the 2014 conference
    on empirical methods in natural language processing (EMNLP), pp. 787–798, 2014.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] S. Kazemzadeh, V. Ordonez, M. Matten, 和 T. Berg, “Referitgame: 在自然场景照片中引用对象，”
    见于2014年自然语言处理实证方法会议（EMNLP），第787–798页，2014年。'
- en: '[51] E. Kodak, “Kodak lossless true color image suite (PhotoCD PCD0992).”'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] E. Kodak, “Kodak无损真实色彩图像套件（PhotoCD PCD0992）。”'
- en: '[52] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in Medical image computing and computer-assisted
    intervention–MICCAI 2015: 18th international conference, Munich, Germany, October
    5-9, 2015, proceedings, part III 18, pp. 234–241, Springer, 2015.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] O. Ronneberger, P. Fischer, 和 T. Brox, “U-net: 生物医学图像分割的卷积网络，” 见于医学图像计算与计算机辅助干预–MICCAI
    2015：第18届国际会议，德国慕尼黑，2015年10月5-9日，会议录， 第III部分18，第234–241页，Springer，2015年。'
- en: '[53] M. Lu, P. Guo, H. Shi, C. Cao, and Z. Ma, “Transformer-based image compression,”
    in Data Compression Conference, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. Lu, P. Guo, H. Shi, C. Cao, 和 Z. Ma, “基于Transformer的图像压缩，” 见于数据压缩会议，2022年。'
- en: '[54] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez, et al., “Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality,” See https://vicuna. lmsys. org (accessed 14
    April 2023), vol. 2, no. 3, p. 6, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang,
    Y. Zhuang, J. E. Gonzalez, 等，“Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality，” 见 https://vicuna.lmsys.org (访问日期2023年4月14日)，第2卷，第3期，第6页，2023年。'
- en: '[55] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in Computer Vision–ECCV
    2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,
    Part V 13, pp. 740–755, Springer, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    和 C. L. Zitnick，“Microsoft coco: Common objects in context，” 见计算机视觉–ECCV 2014:
    第13届欧洲会议，瑞士苏黎世，2014年9月6-12日，会议论文集，第五部分，第13页，740–755，施普林格，2014年。'
- en: Appendix A Supplementary Material
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 补充材料
- en: A.1 Implementation Details
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实施细节
- en: Training.
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练。
- en: We adopt a progressive training strategy incorporating both the cross-entropy
    and distillation losses, divided into three stages. (1) We train our system using
    only the cross-entropy loss with a learning rate of $10^{-4}$ for 20 epochs. (3)
    Lastly, we train our system using only the distillation loss for 20 epochs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用逐步训练策略，结合了交叉熵损失和蒸馏损失，分为三个阶段。(1) 我们仅使用交叉熵损失进行训练，学习率为$10^{-4}$，训练20个epoch。(3)
    最后，我们仅使用蒸馏损失进行训练20个epoch。
- en: We use the Adam optimizer, configured with $\beta_{1}$. Weight decay is disabled.
    The transform-neck for each rate point undergoes training on an RTX 4090 for approximately
    three days during the training stage.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用配置了$\beta_{1}$的Adam优化器。权重衰减已禁用。每个速率点的transform-neck在训练阶段会在RTX 4090上训练大约三天。
- en: Evaluation.
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: For few-shot classification with V2L-Tokenizer [[10](#bib.bib10)], we design
    a 5-way 1-shot classification evaluation scenario. In particular, we generate
    5000 groups of images from ImageNet dataset, where each group consists of five
    randomly sampled images from different classes, serving as the sample images,
    and one new image from one of the classes as the query image.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用V2L-Tokenizer [[10](#bib.bib10)]的few-shot分类，我们设计了一个5-way 1-shot分类评估场景。特别地，我们从ImageNet数据集中生成5000组图像，每组包含五张来自不同类别的随机采样图像作为样本图像，以及来自其中一个类别的一张新图像作为查询图像。
- en: 'Different MLLM is utilized for the evaluation of our proposed method on each
    task. In Table [4](#A1.T4 "Table 4 ‣ Evaluation. ‣ A.1 Implementation Details
    ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image Latents
    and Multimodal LLMs via Universal Transform-Neck"), we provide the detailed specifications
    of the MLLM used in our evaluation.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的MLLM用于评估我们提出的方法在每个任务上的表现。在表[4](#A1.T4 "Table 4 ‣ Evaluation. ‣ A.1 Implementation
    Details ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image
    Latents and Multimodal LLMs via Universal Transform-Neck")中，我们提供了用于评估的MLLM的详细规格。'
- en: 'Table 4: The specifications of the MLLM used in our tasks.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：我们任务中使用的MLLM规格。
- en: '| Task |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 任务 |'
- en: '&#124; Model &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLM &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLM &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Captioning | LLaMA-Adapter v1 [[9](#bib.bib9)] | LLaMA-7B [[2](#bib.bib2)]
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 标题生成 | LLaMA-Adapter v1 [[9](#bib.bib9)] | LLaMA-7B [[2](#bib.bib2)] |'
- en: '| VQA | Honeybee-C-7B-M144 [[5](#bib.bib5)] | Vicuna-7B [[54](#bib.bib54)]
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| VQA | Honeybee-C-7B-M144 [[5](#bib.bib5)] | Vicuna-7B [[54](#bib.bib54)]
    |'
- en: '| REC | Shikra-7B [[6](#bib.bib6)] | LLaMA-7B [[2](#bib.bib2)] |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| REC | Shikra-7B [[6](#bib.bib6)] | LLaMA-7B [[2](#bib.bib2)] |'
- en: '| Few-shot classification | V2L-Tokenizer [[10](#bib.bib10)] | LLaMA2-7B [[1](#bib.bib1)]
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Few-shot分类 | V2L-Tokenizer [[10](#bib.bib10)] | LLaMA2-7B [[1](#bib.bib1)]
    |'
- en: A.2 Comparison with VVC
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 与VVC的比较
- en: 'Figure [7](#A1.F7 "Figure 7 ‣ A.2 Comparison with VVC ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck") compares Reconstruction and our method in (d1) using
    ELIC, with the state-of-the-art traditional codec VVC (VTM 17.0 intra coding).
    We set the QPs to $[37,40,43,46,49]$ for VVC. It is observed that VVC performs
    worse than Reconstruction across all the tasks, which is potentially due to (1)
    the small spatial resolution (256x256) of input images that is not optimal for
    VVC, (2) its inferior rate-distortion performance compared to ELIC as reported
    in [[33](#bib.bib33)].'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图[7](#A1.F7 "图7 ‣ A.2 与VVC的比较 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")
    比较了重建和我们的方法（d1）使用ELIC，与最先进的传统编解码器VVC（VTM 17.0 内部编码）。我们将QPs设置为 $[37,40,43,46,49]$。观察到VVC在所有任务中的表现都不如重建，这可能是由于（1）输入图像的小空间分辨率（256x256）不适合VVC，（2）其与ELIC的速率-失真性能较差，如在[[33](#bib.bib33)]中报告的。
- en: '![Refer to caption](img/08d02c9688906b881006b0677bfd0899.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/08d02c9688906b881006b0677bfd0899.png)'
- en: 'Figure 7: Rate-accuracy comparison using VTM on several tasks.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：使用VTM在多个任务上的速率-准确性比较。
- en: A.3 More Visualization
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 更多可视化
- en: 'We present additional visualization results on four different evaluation tasks,
    including image captioning (Figure [8](#A1.F8 "Figure 8 ‣ A.4 License of Assets
    Used ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image
    Latents and Multimodal LLMs via Universal Transform-Neck")), visual question answering
    (VQA) (Figure [9](#A1.F9 "Figure 9 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck")), referring expression comprehension (REC) (Figure [10](#A1.F10
    "Figure 10 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary Material ‣
    ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via Universal Transform-Neck")),
    and few-shot classification (Figure [11](#A1.F11 "Figure 11 ‣ A.4 License of Assets
    Used ‣ Appendix A Supplementary Material ‣ ComNeck: Bridging Compressed Image
    Latents and Multimodal LLMs via Universal Transform-Neck")).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在四个不同评估任务上的附加可视化结果，包括图像描述（图[8](#A1.F8 "图8 ‣ A.4 使用的资产许可证 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")），视觉问答（VQA）（图[9](#A1.F9
    "图9 ‣ A.4 使用的资产许可证 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")），指代表达理解（REC）（图[10](#A1.F10
    "图10 ‣ A.4 使用的资产许可证 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")），以及少样本分类（图[11](#A1.F11
    "图11 ‣ A.4 使用的资产许可证 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")）。
- en: A.4 License of Assets Used
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 使用的资产许可证
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.4 License of Assets Used ‣ Appendix A Supplementary
    Material ‣ ComNeck: Bridging Compressed Image Latents and Multimodal LLMs via
    Universal Transform-Neck") summarizes the used assets in our work along with their
    license terms.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表[5](#A1.T5 "表5 ‣ A.4 使用的资产许可证 ‣ 附录A 补充材料 ‣ ComNeck：通过通用转换颈桥接压缩图像潜在空间和多模态LLMs")
    总结了我们工作中使用的资产及其许可证条款。
- en: 'Table 5: List of assets used in the paper with their corresponding license.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：论文中使用的资产及其对应的许可证列表。
- en: '| Assets |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 资产 |'
- en: '&#124; Licenses &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 许可证 &#124;'
- en: '|'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ImageNet [[47](#bib.bib47)] | Custom license. Available at https://image-net.org/download.php
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet [[47](#bib.bib47)] | 自定义许可证。可在 https://image-net.org/download.php
    获取 |'
- en: '| COCO [[55](#bib.bib55)] | CC BY 4.0 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| COCO [[55](#bib.bib55)] | CC BY 4.0 |'
- en: '| SEED-Bench [[49](#bib.bib49)] | Apache 2.0 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| SEED-Bench [[49](#bib.bib49)] | Apache 2.0 |'
- en: '| LLaMA-Adapter [[9](#bib.bib9)] | GPL-3.0 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-Adapter [[9](#bib.bib9)] | GPL-3.0 |'
- en: '| Honeybee [[5](#bib.bib5)] |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Honeybee [[5](#bib.bib5)] |'
- en: '&#124; Source code: Apache 2.0 &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 源代码：Apache 2.0 &#124;'
- en: '&#124; Pretrained weights: CC BY-NC 4.0 &#124;'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预训练权重：CC BY-NC 4.0 &#124;'
- en: '|'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Shikra [[6](#bib.bib6)] | CC BY-NC 4.0 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Shikra [[6](#bib.bib6)] | CC BY-NC 4.0 |'
- en: '| V2L-Tokenizer [[10](#bib.bib10)] |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| V2L-Tokenizer [[10](#bib.bib10)] |'
- en: '&#124; No license provided. &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 未提供许可证。 &#124;'
- en: '&#124; Code available at https://github.com/zh460045050/V2L-Tokenizer &#124;'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代码可在 https://github.com/zh460045050/V2L-Tokenizer 获取 &#124;'
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Task: Captioning &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务：图像描述 &#124;'
- en: '&#124; Model: LLaMA-Adapter &#124;'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型：LLaMA-Adapter &#124;'
- en: '|'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/1287368d73fa781cea163cb5ab29b31b.png) &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无说明图像]](img/1287368d73fa781cea163cb5ab29b31b.png) &#124;'
- en: '&#124; Reconstruction: A microwave and a computer sitting on a desk. &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建：一个微波炉和一台电脑放在桌子上。 &#124;'
- en: '&#124; Post-processing: A microwave and a refrigerator sitting on top of a
    table. &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 后处理: 一台微波炉和一台冰箱放在桌子上。 &#124;'
- en: '&#124; Ours (d1): A microwave and a toaster oven on a counter. &#124;'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们的方法 (d1): 一个微波炉和一个烤面包机放在台面上。 &#124;'
- en: '|'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0725  &#124;'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP: 0.0725  &#124;'
- en: '|'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/26643bffd427cece4de622385d679b9a.png) &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/26643bffd427cece4de622385d679b9a.png) &#124;'
- en: '&#124; Reconstruction: Two cats are standing on the ground near a bench. &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: 两只猫站在靠近长椅的地上。 &#124;'
- en: '&#124; Post-processing: A dog and a cat are standing on a sidewalk. &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 后处理: 一只狗和一只猫站在人行道上。 &#124;'
- en: '&#124; Ours (d1): Two dogs are standing near a bicycle on a sidewalk. &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们的方法 (d1): 两只狗站在一个人行道上的自行车旁。 &#124;'
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0928  &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP: 0.0928  &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/37a612663327a1f1cd1d52c742b819e5.png) &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/37a612663327a1f1cd1d52c742b819e5.png) &#124;'
- en: '&#124; Reconstruction: A blurry picture of a blender with a knife. &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: 一张模糊的搅拌机与刀具的照片。 &#124;'
- en: '&#124; Post-processing: A close up of a blurry image of a bug. &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 后处理: 一张模糊的昆虫特写。 &#124;'
- en: '&#124; Ours (d1): A close up of a knife cutting into a pizza. &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们的方法 (d1): 一把刀切入比萨饼的特写。 &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0910  &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP: 0.0910  &#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/81f7ad1f75fa589f3b61693790e78556.png) &#124;'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/81f7ad1f75fa589f3b61693790e78556.png) &#124;'
- en: '&#124; Reconstruction: A young boy in a red shirt and tie posing for a picture.
    &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: 一个穿红衬衫和领带的年轻男孩在摆姿势拍照。'
- en: '&#124; Post-processing: A young boy standing in front of a wall with a clock.
    &#124;'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 后处理: 一个年轻男孩站在有钟表的墙前。 &#124;'
- en: '&#124; Ours (d1): A young boy in a tie and a white shirt. &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我们的方法 (d1): 一个穿着领带和白衬衫的年轻男孩。 &#124;'
- en: '|'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0899  &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP: 0.0899  &#124;'
- en: '|'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 8: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on image captioning with LLaMA-Adapter.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 我们提出的方法在 (d1)、重建和图像字幕生成上的可视化示例，使用 LLaMA-Adapter。'
- en: '|'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Task: Visual question answering &#124;'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务: 视觉问答 &#124;'
- en: '&#124; Model: Honeybee &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型: Honeybee &#124;'
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; What is the dog doing in the image? &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像中的狗在做什么？ &#124;'
- en: '&#124; A. Standing still  B. Chasing after something  C. Lying down  D. Jumping
    in the air &#124;'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A. 站着  B. 追逐某物  C. 躺着  D. 跳跃 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/e785c5947eefd976a021536ca408d27d.png) &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/e785c5947eefd976a021536ca408d27d.png) &#124;'
- en: '&#124; Reconstruction: B Post-processing: B Ours (d1): A GT: A &#124;'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: B 后处理: B 我们的方法 (d1): A GT: A &#124;'
- en: '|'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.082  &#124;'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP:0.082  &#124;'
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; How many people are on the field in this image? &#124;'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像中有多少人？ &#124;'
- en: '&#124; A. Four  B. Nine  C. Twelve  D. Eleven &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A. 四 B. 九 C. 十二 D. 十一 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/a5a6898281982db592c8704fad237e77.png) &#124;'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/a5a6898281982db592c8704fad237e77.png) &#124;'
- en: '&#124; Reconstruction: D Post-processing: B Ours (d1): A GT: A &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: D 后处理: B 我们的方法 (d1): A GT: A &#124;'
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.097  &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP:0.097  &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; What is the person in the blue jacket holding? &#124;'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 蓝色夹克的人在拿什么？ &#124;'
- en: '&#124; A. A phone  B. Nothing  C. A wallet  D. A clipboard &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A. 一部电话  B. 没有  C. 一个钱包  D. 一个剪贴板 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/8a6e02beae8a7c22a1e13c9600d8526e.png) &#124;'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/8a6e02beae8a7c22a1e13c9600d8526e.png) &#124;'
- en: '&#124; Reconstruction: D Post-processing: D Ours (d1): B GT: B &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: D 后处理: D 我们的方法 (d1): B GT: B &#124;'
- en: '|'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.160  &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP:0.160  &#124;'
- en: '|'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; How many people are lighting candles in this image? &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这张图像中有多少人在点燃蜡烛？ &#124;'
- en: '&#124; A. Two B. One C. Three D. Four &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; A. 两只 B. 一只 C. 三只 D. 四只 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/36074ceefa695df1d6fb572b54a812e0.png) &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/36074ceefa695df1d6fb572b54a812e0.png) &#124;'
- en: '&#124; Reconstruction: A Post-processing: A Ours (d1): C GT: C &#124;'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建: A 后处理: A 我们的方法 (d1): C GT: C &#124;'
- en: '|'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.066  &#124;'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BPP:0.066  &#124;'
- en: '|'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 9: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on VQA with Honeybee.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 我们提出的方法在 (d1)、重建和 VQA 上的可视化示例，使用 Honeybee。'
- en: '|'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Task: Referring expression comprehension &#124;'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务: 参照表达理解 &#124;'
- en: '&#124; Model: Shikra &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型: Shikra &#124;'
- en: '|'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Guide me to the location of brown bear within the image  by providing
    its coordinates. &#124;'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请提供图像中棕熊的位置坐标。 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/f7f568aa4f05ef41a0f0c88294bdc504.png) &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/f7f568aa4f05ef41a0f0c88294bdc504.png) &#124;'
- en: '|'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.040  &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP:0.040  &#124;'
- en: '|'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Point me to the location of wine glass far left in the picture 
    by providing its coordinates. &#124;'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请指出图片  左边远处的酒杯的位置，并提供其坐标。 &#124;'
- en: '&#124; ![[Uncaptioned image]](img/14c57556464e6101b47b431c895eb064.png) &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/14c57556464e6101b47b431c895eb064.png) &#124;'
- en: '|'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.115  &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP:0.115  &#124;'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Can you assist me in locating right female cop in , and then provide
    its coordinates? &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你能帮我定位中的右侧女性警察，并提供其坐标吗？ &#124;'
- en: '&#124; ![[Uncaptioned image]](img/7f518129e57fdac4bea93dada5f32247.png) &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/7f518129e57fdac4bea93dada5f32247.png) &#124;'
- en: '|'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.098  &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP:0.098  &#124;'
- en: '|'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; In the photograph , could you pinpoint the location of &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在照片中，你能指出&#124;'
- en: '&#124; person holding a snowboard and tell me its coordinates? &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  手持滑雪板的人，并告诉我其坐标？ &#124;'
- en: '&#124; ![[Uncaptioned image]](img/12a635cedb7eb43d755497dccc16d9e6.png) &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/12a635cedb7eb43d755497dccc16d9e6.png) &#124;'
- en: '|'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP:0.088  &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP:0.088  &#124;'
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 10: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on REC with Shikra.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：我们提出的方法在（d1）、重建和后处理的可视化示例，使用 Shikra 在 REC 上。
- en: '|'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Task: Few-shot classification &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任务：少样本分类 &#124;'
- en: '&#124; Model: V2L-tokenizer &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型：V2L-tokenizer &#124;'
- en: '|'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Query                Examples  &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  查询                示例  &#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/1be1fa69645318ee73026428b12c25e5.png) &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/1be1fa69645318ee73026428b12c25e5.png) &#124;'
- en: '&#124; ![[Uncaptioned image]](img/5eb6160283c73ffa8c7e8ac09c899de1.png) &#124;'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/5eb6160283c73ffa8c7e8ac09c899de1.png) &#124;'
- en: '&#124; Reconstruction: ptarmigan Post-processing: ptarmigan Ours (d1): walking
    stick GT: walking stick &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建：雷鸟 后处理：雷鸟 我们（d1）：步行棍 GT：步行棍 &#124;'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.0786  &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP: 0.0786  &#124;'
- en: '|'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Query                Examples  &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  查询                示例  &#124;'
- en: '|'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/cc8688af7dd02192d26c51a119e04722.png) &#124;'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/cc8688af7dd02192d26c51a119e04722.png) &#124;'
- en: '&#124; ![[Uncaptioned image]](img/c5d1a71b5886175480e26de3048d630a.png) &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/c5d1a71b5886175480e26de3048d630a.png) &#124;'
- en: '&#124; Reconstruction: hippopotamus Post-processing: wall clock Ours (d1):
    otterhound GT: otterhound &#124;'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建：河马 后处理：挂钟 我们（d1）：海獺犬 GT：海獺犬 &#124;'
- en: '|'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.1369  &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP: 0.1369  &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  Query                Examples  &#124;'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  查询                示例  &#124;'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![[Uncaptioned image]](img/560623d17b486542176020958e974351.png) &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/560623d17b486542176020958e974351.png) &#124;'
- en: '&#124; ![[Uncaptioned image]](img/6539f048119b210a7b2417526401a46e.png) &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![[无标题图像]](img/6539f048119b210a7b2417526401a46e.png) &#124;'
- en: '&#124; Reconstruction: horned viper Post-processing: horned viper Ours (d1):
    hornbill GT: hornbill &#124;'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 重建：角蝰 后处理：角蝰 我们（d1）：犀鸟 GT：犀鸟 &#124;'
- en: '|'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  BPP: 0.2329  &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  BPP: 0.2329  &#124;'
- en: '|'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 11: Visualization examples of our proposed method in (d1), Reconstruction,
    and Post-processing on few-shot classification with V2L-tokenizer.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：我们提出的方法在（d1）、重建和后处理的可视化示例，使用 V2L-tokenizer 在少样本分类上。
