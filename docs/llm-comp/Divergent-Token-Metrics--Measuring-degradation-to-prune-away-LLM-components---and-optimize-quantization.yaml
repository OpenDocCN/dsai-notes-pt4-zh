- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:50:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:50:54'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Divergent Token Metrics: 衡量退化以修剪 LLM 组件 – 并优化量化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.01544](https://ar5iv.labs.arxiv.org/html/2311.01544)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.01544](https://ar5iv.labs.arxiv.org/html/2311.01544)
- en: Björn Deiseroth^(1,2,3,∗) &Max Meuer¹ &Nikolas Gritsch^(1,4) &Constantin Eichenberg¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Björn Deiseroth^(1,2,3,∗) &Max Meuer¹ &Nikolas Gritsch^(1,4) &Constantin Eichenberg¹
- en: \ANDPatrick Schramowski^(2,3,5) &Matthias Aßenmacher^(4,6) &Kristian Kersting^(2,3,5)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \ANDPatrick Schramowski^(2,3,5) &Matthias Aßenmacher^(4,6) &Kristian Kersting^(2,3,5)
- en: ¹ Aleph Alpha ² Technical University Darmstadt
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ Aleph Alpha ² 达姆施塔特技术大学
- en: ³ Hessian Center for Artificial Intelligence (hessian.AI)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 黑森人工智能中心 (hessian.AI)
- en: ⁵ German Center for Artificial Intelligence (DFKI)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 德国人工智能中心 (DFKI)
- en: ⁴ Department of Statistics, LMU ⁶ Munich Center for Machine Learning (MCML)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 统计系，LMU ⁶ 慕尼黑机器学习中心 (MCML)
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have reshaped natural language processing with
    their impressive capabilities. Their ever-increasing size, however, raised concerns
    about their effective deployment and the need for LLM compressions. This study
    introduces the Divergent Token metrics (DTMs), a novel approach for assessing
    compressed LLMs, addressing the limitations of traditional perplexity or accuracy
    measures that fail to accurately reflect text generation quality. DTMs focus on
    token divergence, that allow deeper insights into the subtleties of model compression,
    i.p. when evaluating component’s impacts individually. Utilizing the *First Divergent
    Token* metric (FDTM) in model sparsification reveals that a quarter of all attention
    components can be pruned beyond 90% on the Llama-2 model family, still keeping
    SOTA performance. For quantization FDTM suggests that over 80% of parameters can
    naively be transformed to int8 without special outlier management. These evaluations
    indicate the necessity of choosing appropriate compressions for parameters individually—and
    that FDTM can identify those—while standard metrics result in deteriorated outcomes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型 (LLMs) 以其令人印象深刻的能力重塑了自然语言处理。然而，它们日益增长的规模引发了对其有效部署和LLM压缩需求的担忧。本研究介绍了 Divergent
    Token 指标 (DTMs)，这是一种评估压缩 LLM 的新方法，解决了传统困惑度或准确度测量无法准确反映文本生成质量的局限性。DTMs 关注令牌分歧，允许深入了解模型压缩的细微差别，例如在单独评估组件影响时。利用*首个分歧令牌*指标
    (FDTM) 在模型稀疏化中显示，Llama-2 模型系列中四分之一的注意力组件可以在保持SOTA性能的情况下修剪超过90%。对于量化，FDTM 表明，超过80%的参数可以在没有特殊离群值管理的情况下简单地转换为
    int8。这些评估表明需要为参数选择合适的压缩方法——并且 FDTM 可以识别这些——而标准指标则导致结果恶化。
- en: 'Divergent Token Metrics: Measuring degradation to'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'Divergent Token Metrics: 衡量退化以'
- en: prune away LLM components – and optimize quantization
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 修剪 LLM 组件 – 并优化量化
- en: Björn Deiseroth^(1,2,3,∗)                        Max Meuer¹                       
    Nikolas Gritsch^(1,4)                        Constantin Eichenberg¹
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Björn Deiseroth^(1,2,3,∗)                        Max Meuer¹                       
    Nikolas Gritsch^(1,4)                        Constantin Eichenberg¹
- en: Patrick Schramowski^(2,3,5)                        Matthias Aßenmacher^(4,6)
                           Kristian Kersting^(2,3,5) ¹ Aleph Alpha ² Technical University
    Darmstadt ³ Hessian Center for Artificial Intelligence (hessian.AI) ⁵ German Center
    for Artificial Intelligence (DFKI) ⁴ Department of Statistics, LMU ⁶ Munich Center
    for Machine Learning (MCML)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Patrick Schramowski^(2,3,5)                        Matthias Aßenmacher^(4,6)
                           Kristian Kersting^(2,3,5) ¹ Aleph Alpha ² 达姆施塔特技术大学 ³ 黑森人工智能中心
    (hessian.AI) ⁵ 德国人工智能中心 (DFKI) ⁴ 统计系，LMU ⁶ 慕尼黑机器学习中心 (MCML)
- en: '^*^*footnotetext: Corresp.: bjoern.deiseroth@aleph-alpha.com'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '^*^*脚注: 联系人：bjoern.deiseroth@aleph-alpha.com'
- en: '[https://github.com/Aleph-Alpha/Divergent_Tokens](https://github.com/Aleph-Alpha/Divergent_Tokens)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/Aleph-Alpha/Divergent_Tokens](https://github.com/Aleph-Alpha/Divergent_Tokens)'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/77ae6560d1265f4c9aef7712e4fd6968.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/77ae6560d1265f4c9aef7712e4fd6968.png)'
- en: 'Figure 1: Illustration of a diverging generation process. Given the 3-token
    prefix as prompt, a base and its compressed model generate 8 subsequent tokens.
    Our proposed metric points to the first divergent token (FDT). The FDT may cause
    further divergence during the iterative generation process. Note how both models
    score the same perplexity value, as the actual sampling process is not reflected
    (c.f. Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization"), Sec. [4](#S4 "4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") for an empirical exploration).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：生成过程的离散化示例。给定 3 个标记前缀作为提示，基本模型及其压缩模型生成 8 个后续标记。我们提出的度量指向第一个分歧标记（FDT）。FDT
    可能在迭代生成过程中引起进一步的分歧。请注意，尽管两个模型的困惑度值相同，但实际采样过程未被反映（参见图 Fig. [2](#S3.F2 "图 2 ‣ 3.2
    困惑度（PPL） ‣ 3 模型分歧度量 ‣ 分歧标记度量：测量降解以修剪 LLM 组件 - 并优化量化"), Sec. [4](#S4 "4 标记度量改善模型压缩
    ‣ 分歧标记度量：测量降解以修剪 LLM 组件 - 并优化量化") 的实证探索）。
- en: Cutting-edge Large Language Models (LLMs) based on the transformer architecture
    have revolutionized Natural Language Processing with their exceptional performance,
    notably exemplified by the GPT-series (Radford et al., [2018](#bib.bib11), [2019](#bib.bib12);
    Brown et al., [2020](#bib.bib1); Bubeck et al., [2023](#bib.bib2); OpenAI, [2022](#bib.bib9))
    in text generation. However, these models have grown vastly massive, even exceeding
    half a trillion parameters Chowdhery et al. ([2022](#bib.bib3)). While their bountiful
    parameters aid early training convergence, their practical utility and true necessity
    remain unclear.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Transformer 架构的前沿大型语言模型（LLMs）通过其卓越的性能彻底改变了自然语言处理，尤其以 GPT 系列（Radford 等，[2018](#bib.bib11),
    [2019](#bib.bib12); Brown 等，[2020](#bib.bib1); Bubeck 等，[2023](#bib.bib2); OpenAI，[2022](#bib.bib9)）在文本生成中的表现为例。然而，这些模型变得极其庞大，甚至超过了半万亿个参数（Chowdhery
    等，[2022](#bib.bib3)）。虽然其大量参数有助于早期训练收敛，但它们的实际效用和真正必要性仍不明确。
- en: 'Compression strategies like sparsification and quantization can enhance parameter
    efficiency. Current metrics, however, are either averaging too coarsely, such
    as perplexity, or are by design too specific, such as standard NLP benchmarks.
    Either fail to capture the diverging performance nuances introduced by the compression
    early on. As they oversight the actual discontinous text generation process. This
    however is the main use of the final model, and we thus argue, that they are therefore
    insufficient measures for the performance of compressed model. This misalignment
    can lead to unwanted subtle discrepancies in generations, such as grammatical
    errors or a mismatch in numbers as we will see, even when overall metrics, such
    as perplexity, appear satisfactory (cf. Prop. [3.2](#S3.Thmthm2 "Proposition 3.2\.
    ‣ 3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization"),
    Sec. [4](#S4 "4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")).
    An example is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization").'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩策略如稀疏化和量化可以提高参数效率。然而，当前的度量标准要么平均过于粗略，如困惑度，要么设计上过于特定，如标准自然语言处理基准。这些度量未能捕捉到压缩早期引入的性能差异，因为它们忽视了实际的离散文本生成过程。这实际上是最终模型的主要用途，因此我们认为，它们不足以衡量压缩模型的性能。这种不匹配可能导致生成中的不希望出现的细微差异，如语法错误或数字不匹配，尽管整体度量标准如困惑度看起来令人满意（参见
    Prop. [3.2](#S3.Thmthm2 "命题 3.2. ‣ 3.5 标记与困惑度度量 ‣ 3 模型分歧度量 ‣ 分歧标记度量：测量降解以修剪 LLM
    组件 - 并优化量化"), Sec. [4](#S4 "4 标记度量改善模型压缩 ‣ 分歧标记度量：测量降解以修剪 LLM 组件 - 并优化量化")）。图示例见
    Fig. [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 分歧标记度量：测量降解以修剪 LLM 组件 - 并优化量化")。
- en: To meet these challenges, we introduce the family of Divergent Token metrics
    (DTMs). These metrics are tailored to measure the *model divergence* of LLMs throughout
    the compression process and in relation to the actual generation procedure. We
    demonstrate that the First Divergent Token metric (FDTM) and the Share of Divergent
    Tokens metric (SDTM) offer a more nuanced evaluation compared to perplexity. They,
    moreover, enable an individual component evaluation to rank parts of the model
    best suited for compression, thus enabling meaningful compression while preserving
    text generation quality. Specifically, sparsification enhanced by FDTM indicates
    significant differences in component utilization across layers. For the first
    time, we show that almost twenty percent of the model components can be pruned
    beyond 90%, several even entirely removed, while preserving a single-digit perplexity.
    Consequently, one can employ a sparse matrix format to accelerate computational
    efficiency. Likewise for precision reduction we show that sorting components by
    FDTM coincidentally correlates to sorting by their induced number of outliers
    when being converted to int8\. FDTM identifies the optimal 80% of components that
    overall keep performance without specific outlier-handling. The observed decline
    in performance with more outliers, and the significant influence of specific components
    on those, suggests to reevaluate the applied normalization methods throughout
    the model. This level of precision goes beyond what standard perplexity and conventional
    NLP benchmarks can achieve. As the proposed Divergent Token metrics closely reflect
    the generation process, and as such, can be a measure to foster confidence of
    deployed compressed models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为应对这些挑战，我们引入了“分歧令牌度量标准（DTMs）”家族。这些度量标准专门用于衡量LLMs在压缩过程中以及与实际生成过程相关的*模型分歧*。我们证明，**第一次分歧令牌度量标准（FDTM）**和**分歧令牌比例度量标准（SDTM）**提供了比困惑度更为细致的评估。它们还允许对模型的各个组件进行单独评估，以排名最适合压缩的部分，从而实现有意义的压缩，同时保持文本生成质量。具体而言，通过FDTM增强的稀疏化显示了不同层之间组件利用的显著差异。我们首次展示了几乎20%的模型组件可以被修剪超过90%，其中一些甚至完全移除，同时保持个位数的困惑度。因此，可以使用稀疏矩阵格式来提高计算效率。对于精度降低，我们展示了按FDTM排序的组件与其在转换为int8时诱发的异常值数量的排序具有巧合的相关性。FDTM识别出总体性能保持良好的80%的最佳组件，无需特定的异常值处理。异常值的增加导致性能下降，并且特定组件对这些异常值的显著影响，建议重新评估整个模型中的归一化方法。这种精度超出了标准困惑度和传统NLP基准所能达到的水平。由于所提议的分歧令牌度量标准密切反映了生成过程，因此可以作为促进压缩模型信心的一个度量。
- en: We proceed as follows. We first briefly revisit common compression principles
    known from the literature. Afterwards we introduce our novel family of metrics.
    Before concluding, we present our exhaustive experimental evaluation of sparsification
    and quantization.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的步骤如下。首先，我们简要回顾文献中已知的常见压缩原理。随后，我们介绍我们新颖的度量标准家族。在结束之前，我们展示了对稀疏化和量化的详尽实验评估。
- en: 2 Compression Principles
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 压缩原理
- en: Model compression aims to reduce the hardware resources needed to operate the
    model. Indeed, doing so may sacrifice model accuracy. To keep the regret as small
    as possible, a corrective measure is typically used. Here, we discuss the most
    commonly used concepts and state-of-the-art methods for sparsification and quantization
    of LLMs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩旨在减少操作模型所需的硬件资源。实际上，这样做可能会牺牲模型的准确性。为了将遗憾降到最低，通常会使用修正措施。在这里，我们讨论了用于LLMs稀疏化和量化的最常用概念和最先进的方法。
- en: Outlier and Hessians.
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异常值和海森矩阵。
- en: Most model compression methods rely either on the separation of outliers or
    the computation of a Hessian matrix. Outliers usually refer to significantly larger
    values in magnitude occurring either in the weight matrix directly or in the activations
    during a forward pass. As most computations are linear matrix multiplications,
    such outliers strongly influence the remaining entropy contained in consecutive
    computations. In the case of sparsification, outliers should be left intact, and
    the values with the least magnitude—which are consequently the least influential—should
    be masked instead. For quantization, it was suggested to be beneficial to separate
    outliers and compute parts in higher-bit formats. This is i.p. motivated due to
    rounding issues in lower precision formats Dettmers et al. ([2022](#bib.bib4)).
    On the other hand, after conversion, Hessian matrices can be applied. They can
    effectively be approximated by computing backpropagation gradients for a small
    number of samples and represent a second-order approximation to reconstruct the
    original model Frantar et al. ([2023](#bib.bib6)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型压缩方法依赖于离群值的分离或Hessian矩阵的计算。离群值通常指的是权重矩阵中直接出现的或在前向传播过程中激活的幅度显著较大的值。由于大多数计算是线性矩阵乘法，这些离群值会强烈影响连续计算中剩余的熵。在稀疏化的情况下，离群值应保持不变，而应掩蔽幅度最小的值——这些值因此对计算的影响最小。对于量化，建议分离离群值并在更高位数格式中计算部分。这主要是由于低精度格式中的舍入问题所驱动的
    Dettmers et al. ([2022](#bib.bib4))。另一方面，转换后可以应用Hessian矩阵。通过计算少量样本的反向传播梯度，它们可以有效地被近似，并表示重建原始模型的二阶近似
    Frantar et al. ([2023](#bib.bib6))。
- en: Sparsification.
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稀疏化。
- en: The goal of sparsification is a reduction of the overall number of weights and
    as such, a distillation of the relevant computation. Typically, this category
    is divided into “structured” and “unstructured” pruning. Structured-pruning aims
    to locate dynamics, such as the irrelevance of an entire layer or dimension for
    a given use case and prunes these entirely. Unstructured-pruning usually refers
    to the masking of weights, i.e., setting the irrelevant weights to 0\. High levels
    of sparse matrix computations could result in more efficient kernels and computations.
    Masks exceeding 90%, in particular, allow the transition to a specific sparse
    matrix format, which typically necessitates the additional storage of weight indices,
    but significantly enhances performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏化的目标是减少总体权重数量，因此是相关计算的提炼。通常，这一类别分为“结构化”剪枝和“非结构化”剪枝。结构化剪枝旨在定位动态特征，例如对于特定用例来说整个层或维度的无关性，并完全剪枝这些部分。非结构化剪枝通常指的是权重的屏蔽，即将无关的权重设置为0。高水平的稀疏矩阵计算可能会导致更高效的内核和计算。特别是，超过90%的掩码允许过渡到特定的稀疏矩阵格式，这通常需要额外存储权重索引，但显著提高了性能。
- en: Magnitude pruning selects the masking of weights only based on their magnitudes.
    This is fast to compute but significantly degenerates model performance when pruning
    larger amounts simultaneously. To resolve this issue, wanda Sun et al. ([2023](#bib.bib14))
    proposes to sample a small amount of data and incorporate activations during the
    forward pass. It was shown that this generates more effective one-shot pruning
    masks. Finally, SparseGPT Frantar and Alistarh ([2023](#bib.bib5)) computes iterative
    Hessian approximations to select the lowest impact weights and correct the remaining.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 幅度剪枝仅根据权重的幅度选择掩蔽。这种方法计算速度快，但在同时剪枝较大数量时会显著降低模型性能。为了解决这个问题，wanda Sun et al. ([2023](#bib.bib14))
    提出了在前向传播过程中采样少量数据并结合激活的建议。结果显示，这生成了更有效的一次性剪枝掩码。最后，SparseGPT Frantar 和 Alistarh
    ([2023](#bib.bib5)) 计算了迭代Hessian近似，以选择最低影响的权重并修正其余权重。
- en: Note that the incorporation of activations can to some extent be interpreted
    as a milder form of training. Moreover, despite these efforts, one-shot pruning
    has not yet produced directly usable models without further final fine-tunings.
    This is in particular the case for high sparsity levels beyond 70%, that we target.
    Finally, the components of a model have not yet been investigated individually,
    at all.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，激活的结合在某种程度上可以被视为一种较温和的训练方式。此外，尽管做出了这些努力，一次性剪枝仍未能产生可以直接使用的模型，需要进一步的最终微调。这尤其适用于我们目标的70%以上的高稀疏性水平。最后，模型的各个组件还没有被单独研究过。
- en: Quantization.
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: Model quantization pertains to reducing the utilized precision in the employed
    numeric format. Usually, LLMs are trained in 16-bit floating-point (fp16) and
    converted to 8-bit integer (int8) representations. The naive solution of converting
    float matrices to integers is the AbsMax rounding. It divides a number by the
    maximum value occurring in the matrix and multiplies by the largest available
    integer—as such, it spans a uniform representation grid. The largest float value
    is stored and multiplied for dequantization. The most prominent methods to mitigate
    the introduced rounding errors are LLM.int8() and GPTQ.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化涉及到减少所使用的数值格式的精度。通常，LLMs 在 16 位浮点数 (fp16) 中训练，并转换为 8 位整数 (int8) 表示。将浮点矩阵转换为整数的简单方法是
    AbsMax 四舍五入。它将一个数字除以矩阵中出现的最大值，并乘以最大可用整数——因此，它涵盖了均匀的表示网格。最大浮点值被存储并乘以用于去量化。减小引入的四舍五入误差的最显著方法是LLM.int8()
    和 GPTQ。
- en: Dettmers et al. ([2022](#bib.bib4)) introduced LLM.int8(), which identifies
    vectors containing outliers and retains them in their original fp16 form during
    the matrix multiplication of a forward pass. The vectors lacking outliers are
    quantized fully. Their int8 weights and activations during the forward pass are
    subsequently multiplied and dequantized afterward. This allows them to be integrated
    with the fp16 representation of the outliers. Through empirical investigation
    optimizing the trade-off between degradation in perplexity and the number of outliers
    preserved in fp16, they fixed an absolute outlier threshold.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Dettmers 等人 ([2022](#bib.bib4)) 提出了LLM.int8()，该方法识别包含异常值的向量，并在前向传递的矩阵乘法过程中保留其原始的
    fp16 形式。缺少异常值的向量则被完全量化。它们的 int8 权重和激活在前向传递过程中随后会被乘以并在之后去量化。这使得它们可以与异常值的 fp16 表示进行整合。通过优化困惑度降级与保留在
    fp16 中的异常值数量之间的权衡，他们确定了一个绝对的异常值阈值。
- en: The GPTQ framework offers a more robust quantization approach, i.p., to different
    integer bit-precision. It does not rely on any outlier detection mechanism or
    mixed precision computations—matrix multiplications with the weights are fully
    performed on integers. Frantar et al. ([2023](#bib.bib6)) introduce an efficient
    Hessian approximation and iteratively quantize the weights of the matrices while
    performing error corrections on the remaining weights.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 框架提供了更为强大的量化方法，即对不同的整数位精度。它不依赖于任何异常值检测机制或混合精度计算——权重的矩阵乘法完全在整数上进行。Frantar
    等人 ([2023](#bib.bib6)) 引入了一种高效的 Hessian 近似，并在量化矩阵权重的同时，对剩余权重进行误差修正。
- en: 3 Model Divergence Metrics
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型离散度指标
- en: 'Perplexity fails to identify minor variations in model degradation at an early
    stage. This behavior is depicted in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") and [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL)
    ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") and discussed in Sec. [3.5](#S3.SS5
    "3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")
    and [4](#S4 "4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    in more detail. To assess model divergence and enhance the model compression process,
    we introduce token-based metrics specifically designed to detect those nuances
    occurring in early compression stages. We start by establishing our notation and
    presenting the perplexity metric (PPL). Subsequently, we introduce an enhanced
    variant of PPL and propose the Share of Divergent Tokens metric (SDTM) and First
    Divergent Token metric (FDTM). We conclude by discussing the advantages of each
    metric compared to traditional perplexity-based measures when assessing the degradation
    of the generative performance of compressed models.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度未能识别模型降级的细微变化，尤其是在早期阶段。这种行为在图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 异常 Token Metrics：衡量降级以修剪
    LLM 组件 —— 并优化量化") 和 [2](#S3.F2 "图 2 ‣ 3.2 困惑度（PPL） ‣ 3 模型分歧指标 ‣ 异常 Token Metrics：衡量降级以修剪
    LLM 组件 —— 并优化量化") 中描述，并在第 [3.5](#S3.SS5 "3.5 Token vs. Perplexity Metrics ‣ 3
    模型分歧指标 ‣ 异常 Token Metrics：衡量降级以修剪 LLM 组件 —— 并优化量化") 和 [4](#S4 "4 Token Metrics
    提升模型压缩 ‣ 异常 Token Metrics：衡量降级以修剪 LLM 组件 —— 并优化量化") 节中更详细地讨论。为了评估模型分歧并改善模型压缩过程，我们引入了基于
    Token 的指标，特别设计用于检测早期压缩阶段出现的这些细微差别。我们首先建立我们的符号并介绍困惑度指标（PPL）。随后，我们介绍了 PPL 的增强变体，并提出了异常
    Token 比例指标（SDTM）和首个异常 Token 指标（FDTM）。最后，我们讨论了每种指标相比于传统困惑度基础指标在评估压缩模型生成性能降级时的优势。
- en: 3.1 Basic notation
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基本符号
- en: Let $F$
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $F$
- en: '|  | $\displaystyle\textstyle\mathcal{G}(F,y_{:n},$ |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle\mathcal{G}(F,y_{:n},$ |  |'
- en: '|  |  | $\displaystyle\textstyle=\operatorname*{arg\,max}_{j}F(\mathcal{G}(F,y_{:n},N)_{:i})_{ij}.$
    |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textstyle=\operatorname*{arg\,max}_{j}F(\mathcal{G}(F,y_{:n},N)_{:i})_{ij}.$
    |  |'
- en: 3.2 Perplexity (PPL)
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 困惑度（PPL）
- en: Given a ground truth sequence $y$ is
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 给定真实序列 $y$ 是
- en: '|  | $\displaystyle\textstyle\operatorname{NLL}(y,F,$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle\operatorname{NLL}(y,F,$ |  |'
- en: '|  |  | $\displaystyle\textstyle=-\frac{1}{N-n}\sum_{i=n}^{N-1}\log\mathbb{P}(y_{i+1}&#124;y_{i},..,y_{1}),$
    |  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textstyle=-\frac{1}{N-n}\sum_{i=n}^{N-1}\log\mathbb{P}(y_{i+1}&#124;y_{i},..,y_{1}),$
    |  |'
- en: with $\mathbb{P}(y_{i+1}|y_{i},..,y_{1})=(\mathrm{softmax}\ F(y))_{iy_{i+1}}$.
    Then the perplexity is given by
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{P}(y_{i+1}|y_{i},..,y_{1})=(\mathrm{softmax}\ F(y))_{iy_{i+1}}$。然后，困惑度由以下公式给出
- en: '|  | $\displaystyle\operatorname{PPL}(y,F,n)=\exp(\operatorname{NLL}(y,F,n)).$
    |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{PPL}(y,F,n)=\exp(\operatorname{NLL}(y,F,n)).$
    |  |'
- en: A common practice in the literature, e.g. Dettmers et al. ([2022](#bib.bib4)),
    is to measure model degradation as the increase in average perplexity over a given
    test dataset $\mathcal{D}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中的常见做法，例如 Dettmers 等人 ([2022](#bib.bib4))，是将模型降级测量为给定测试数据集 $\mathcal{D}$ 上平均困惑度的增加。
- en: '![Refer to caption](img/34a4a0b69ddad7cf49fe3beeed5faff0.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34a4a0b69ddad7cf49fe3beeed5faff0.png)'
- en: 'Figure 2: We test the metrics to distinguish between pruning lowest weights,
    and random weights. FDT is able to discriminate the cases. PPL exactly performs
    on the level of guessing. C.f. Sec. [4.2](#S4.SS2 "4.2 Sparsification reveals:
    Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们测试了区分修剪最低权重和随机权重的指标。FDT 能够区分这些情况。PPL 在猜测水平上表现准确。参见第 [4.2](#S4.SS2 "4.2
    稀疏化揭示：注意力并非你所需要的一切！ ‣ 4 Token Metrics 提升模型压缩 ‣ 异常 Token Metrics：衡量降级以修剪 LLM 组件
    —— 并优化量化") 节。
- en: 3.3 Context aware model comparison
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 上下文感知模型比较
- en: First, we argue that standard evaluation does not reflect the typical generative
    model usage, i.e., there are no empty prompts, and as such, those positions should
    not be taken into account when evaluating the generative performance. Moreover,
    when comparing a compressed model $F^{\prime}$. This leads to the definition of
    the *divergent perplexity metric* as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们认为标准评估未能反映典型的生成模型使用情况，即不存在空提示，因此在评估生成性能时不应考虑这些位置。此外，在比较压缩模型$F^{\prime}$时。这导致了*发散困惑度度量*的定义，如下所示：
- en: '|  | $\displaystyle M_{\mathrm{DPPL}}(F,$ |  | (1) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M_{\mathrm{DPPL}}(F,$ |  | (1) |'
- en: '|  |  | $\displaystyle=\operatorname{PPL}(\mathcal{G}(F,y_{:n},N),F^{\prime},n)\;.$
    |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\operatorname{PPL}(\mathcal{G}(F,y_{:n},N),F^{\prime},n)\;.$
    |  |'
- en: 'Finally, let $\mathcal{D}$, we define the *aggregated divergent perplexity
    metric* as the complete evaluation on the dataset:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，设$\mathcal{D}$，我们将*聚合发散困惑度度量*定义为在数据集上的完整评估：
- en: '|  | $\displaystyle\textstyle\mathcal{M}_{\mathrm{DPPL}}($ |  | (2) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle\mathcal{M}_{\mathrm{DPPL}}($ |  | (2) |'
- en: '|  |  | $\displaystyle\textstyle\frac{1}{&#124;\mathcal{D}&#124;}\sum_{y\in\mathcal{D}}M_{\mathrm{DPPL}}(F,F^{\prime},y_{:n},N).$
    |  |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textstyle\frac{1}{&#124;\mathcal{D}&#124;}\sum_{y\in\mathcal{D}}M_{\mathrm{DPPL}}(F,F^{\prime},y_{:n},N).$
    |  |'
- en: In the following, we will ease notation and omit $\mathcal{M}$, or the words
    aggregated and metric, when they become clear by the context. The as such denoted
    DPPL already substantially improves discriminative capabilities over PPL, as we
    will demonstrate in the empirical evaluation.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下部分中，我们将简化符号，并在上下文明确时省略$\mathcal{M}$或“聚合”和“度量”这些词。这样表示的DPPL已经显著提高了辨别能力，我们将在实证评估中演示这一点。
- en: 3.4 Divergent Token Metrics
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 发散令牌度量
- en: SDT.
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SDT。
- en: 'To iterate on the expressiveness and interpretability of model divergence,
    we propose the *share of divergent tokens (SDT)* as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了迭代模型发散的表达能力和可解释性，我们提出了*发散令牌比例（SDT）*，如下：
- en: '|  | $\displaystyle\textstyle\operatorname{SDT}(y,$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle\operatorname{SDT}(y,$ |  |'
- en: '|  |  | $\displaystyle\textstyle=&#124;\{i\geq n\colon\ \operatorname*{arg\,max}_{j}\
    F(y)_{ij}\neq y_{i+1}\}&#124;,$ |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\textstyle=&#124;\{i\geq n\colon\ \operatorname*{arg\,max}_{j}\
    F(y)_{ij}\neq y_{i+1}\}&#124;,$ |  |'
- en: $\operatorname{SDT}(y,F,n)$ can be interpreted as the number of times the model
    would need to be corrected during decoding to match the ground truth after consuming
    the prefix. This metric provides a more direct interpretation of the errors occurring
    during actual token generation, as opposed to estimating prediction certainties
    as PPL does.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $\operatorname{SDT}(y,F,n)$ 可以解释为模型在解码过程中需要被修正以匹配真实值的次数，这在实际令牌生成过程中提供了比PPL更直接的错误解释。
- en: FDT.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FDT。
- en: In addition to SDT, we introduce the first divergent token (FDT) as
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了SDT，我们还引入了第一个发散令牌（FDT），如下所示：
- en: '|  | $\displaystyle{\textstyle\operatorname{FDT}(y,F,n)}$ |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\textstyle\operatorname{FDT}(y,F,n)}$ |  | (3) |'
- en: '|  | $\displaystyle{\textstyle=\min\{i\geq n\colon\operatorname*{arg\,max}_{j}\
    F(y)_{i,j}\neq y_{i+1}\}-n,}$ |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\textstyle=\min\{i\geq n\colon\operatorname*{arg\,max}_{j}\
    F(y)_{i,j}\neq y_{i+1}\}-n,}$ |  |'
- en: with the convention that the minimum is equal to $N$ in the same fashion.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 按照约定，最小值与$N$相等。
- en: As an illustrative example, consider to compute $M_{\mathrm{FDT}}(F,F^{\prime},y_{:n},N)$,
    in contrast to PPL.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个说明性的例子，考虑计算$M_{\mathrm{FDT}}(F,F^{\prime},y_{:n},N)$，与PPL相对比。
- en: 3.5 Token vs. Perplexity Metrics
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 令牌与困惑度度量
- en: It turns out that divergent token metrics offer a superior criterion for analyzing
    model performance degradation compared to perplexity-based metrics, especially
    in the context of greedy decoding. The main reason for that is the fact that the
    greedy decoding operation $\mathcal{G}$ is a discontinuous function of the logits.
    To formalize this, let us discard the model itself and focus notation solely on
    the concept of logits.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，与基于困惑度的度量标准相比，发散的令牌度量提供了分析模型性能下降的更优标准，特别是在贪婪解码的背景下。主要原因是贪婪解码操作$\mathcal{G}$是对logits的不连续函数。为此，我们抛弃模型本身，仅关注logits的概念。
- en: Definition 3.1.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3.1。
- en: The operators and metrics from previous sections defined for models $F,F^{\prime}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前章节中为模型$F,F^{\prime}$定义的操作符和度量。
- en: For example, $\mathcal{G}(l,y_{:n},N)_{i+1}=\operatorname*{arg\,max}_{j}l_{ij}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，$\mathcal{G}(l,y_{:n},N)_{i+1}=\operatorname*{arg\,max}_{j}l_{ij}$。
- en: Proposition 3.2.
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 3.2。
- en: Given any $y$ such that
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任何$y$，使得
- en: '|  | $\displaystyle\textstyle&#124;\operatorname{PPL}(y,l,1)-\operatorname{PPL}(y,l^{\prime},1)&#124;$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle&#124;\operatorname{PPL}(y,l,1)-\operatorname{PPL}(y,l^{\prime},1)&#124;$
    |  |'
- en: '|  | $\displaystyle\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:1},N)$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:1},N)$ |  |'
- en: Proof.
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'C.f. App. [A](#A1 "Appendix A Proof of Propositions ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").
    ∎'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 参见附录 [A](#A1 "附录 A 命题证明 ‣ 分歧令牌度量：测量退化以去除LLM组件 - 并优化量化")。 ∎
- en: 'This means that even if the average perplexity of a compressed model matches
    the perplexity of the original model, the compressed model can produce a very
    different (and potentially worse) output when performing greedy decoding. Hence
    leading to a false positive. In practice, this is a severe issue since even a
    single diverging token can lead to a completely different subsequent output. It
    is illustrated in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")
    and [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics ‣
    Divergent Token Metrics: Measuring degradation to prune away LLM components –
    and optimize quantization") and discussed in Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着即使压缩模型的平均困惑度与原始模型的困惑度相匹配，压缩模型在执行贪婪解码时也可能产生非常不同（且可能更差）的输出，从而导致假阳性。在实际应用中，这是一个严重的问题，因为即使是单个分歧令牌也可能导致完全不同的后续输出。这在图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 分歧令牌度量：测量退化以去除LLM组件 - 并优化量化") 和 [2](#S3.F2 "图 2 ‣ 3.2 困惑度 (PPL)
    ‣ 3 模型分歧度量 ‣ 分歧令牌度量：测量退化以去除LLM组件 - 并优化量化") 中有所说明，并在第 [4.2](#S4.SS2 "4.2 稀疏化揭示：注意力并非你所需要的一切！
    ‣ 4 令牌度量改进模型压缩 ‣ 分歧令牌度量：测量退化以去除LLM组件 - 并优化量化") 节中讨论。
- en: 'As described before, another option is to compute the perplexity with respect
    to the generated completions of the original model. This metric relates more reasonably
    to the share of divergent tokens:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，另一种选择是计算相对于原始模型生成的完成的困惑度。这个度量更合理地与分歧令牌的份额相关：
- en: Proposition 3.3.
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 3.3.
- en: 'The following upper bound holds:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 以下上界成立：
- en: '|  | $\displaystyle{\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:n},N)\leq\frac{N-n}{\log
    2}\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).}$ |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\textstyle M_{\mathrm{SDT}}(l,l^{\prime},y_{:n},N)\leq\frac{N-n}{\log
    2}\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).}$ |  |'
- en: Proof.
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'C.f. App. [A](#A1 "Appendix A Proof of Propositions ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").
    ∎'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 参见附录 [A](#A1 "附录 A 命题证明 ‣ 分歧令牌度量：测量退化以去除LLM组件 - 并优化量化")。 ∎
- en: However, a comparable lower bound does not generally hold. In fact, in the case
    $l=l^{\prime}$ is a perfectly flat distribution at any sequence index. This could
    lead to a false negative signal for the generation process.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，通常没有相应的下界。事实上，当 $l=l^{\prime}$ 在任何序列索引处是完全平坦的分布时，这可能会导致生成过程中的假阴性信号。
- en: In conclusion, perplexity-based metrics suffer from false positives or false
    negatives when evaluating degradation of generative performance. The case for
    FDT and SDT is quite straightforward in that they both directly measure the difference
    between model outputs in a what-you-see-is-what-you-get manner.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，基于困惑度的度量在评估生成性能的退化时会遭遇假阳性或假阴性。FDT 和 SDT 的情况相当简单，因为它们都直接以“眼见即所得”的方式测量模型输出之间的差异。
- en: Note that additional token-based metrics, such as the measurement of the width
    between erroneous predictions, can be readily formulated. These metrics may prove
    especially valuable when assessing potential benefits, for instance, in the context
    of correction-based inference strategies like speculative decoding Leviathan et al.
    ([2023](#bib.bib8)). We now empirically demonstrate the improvements of well-known
    compression methods using our metrics.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，额外的基于令牌的度量，例如错误预测之间的宽度测量，可以很容易地制定。这些度量在评估潜在好处时可能特别有价值，例如，在基于修正的推理策略中，如推测解码
    Leviathan 等人（[2023](#bib.bib8)）。我们现在通过我们的度量实证演示了著名压缩方法的改进。
- en: 4 Token Metrics Improve Model Compression
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 令牌度量改进模型压缩
- en: Now, we will demonstrate how the proposed metric provides novel insights into
    the efficiency of the architecture of LLMs and serve as a benchmark for model
    compression. We outperform PPL as a ranking metric throughout all experiments.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将展示所提出的指标如何为LLMs架构的效率提供新见解，并作为模型压缩的基准。我们在所有实验中都超越了PPL作为排名指标。
- en: More precisely, we apply component-wise probing on sparsification to determine
    individual sparsity rates. Interestingly, the model tends to remove components
    of the attention mechanism on certain layers entirely. In total 40 out of 280
    components are sparsed beyond 90% and 15 removed completely. For quantization,
    on the other hand, we show how component selection significantly influences the
    overall number of model outliers. For the first time, almost 10% of model components
    can be converted to 4-bit integer representations without significant model degradation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，我们对稀疏化进行逐组件探测，以确定各个稀疏率。有趣的是，模型往往会在某些层中完全移除注意力机制的组件。总共有40个组件稀疏度超过90%，15个完全移除。另一方面，对于量化，我们展示了组件选择如何显著影响模型异常值的整体数量。首次，几乎10%的模型组件可以转换为4位整数表示，而不会显著降低模型性能。
- en: 4.1 Experimental Protocol
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验协议
- en: Let us start by clarifying the experimental setup.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先澄清实验设置。
- en: Test environment.
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试环境。
- en: All experiments were performed on the public Llama2-7B and 13B models Touvron
    et al. ([2023](#bib.bib15)). Note, however, that we observed similar behavior
    amongst other decoder transformer models. It remains, i.p., throughout upscaled
    sizes, and smaller variations on the architecture or the training procedure.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都在公开的Llama2-7B和13B模型上进行Touvron等人（[2023](#bib.bib15)）。然而，我们观察到其他解码器变换模型中也有类似的行为。这种现象在不同规模的放大中保持不变，并且架构或训练程序的较小变化。
- en: For all experiments, we follow best practices of compression evaluations Sun
    et al. ([2023](#bib.bib14)) and randomly sampled from the C4 data Raffel et al.
    ([2019](#bib.bib13)) for training iterations. The final model evaluation utilizes
    the Wikitext2 dataset and standard NLP benchmarks Gao et al. ([2021](#bib.bib7)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有实验，我们遵循Sun等人（[2023](#bib.bib14)）的压缩评估最佳实践，并从C4数据中随机抽样Raffel等人（[2019](#bib.bib13)）用于训练迭代。最终模型评估利用Wikitext2数据集和标准NLP基准Gao等人（[2021](#bib.bib7)）。
- en: Metrics.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标。
- en: We apply our proposed metrics for performance evaluation as well as selection
    criteria.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用我们提出的指标进行性能评估和选择标准。
- en: 'We employ FDT, SDT, and PPL as metrics to assess the overall model divergence.
    When it comes to model compression, we demonstrated that both PPL and our variant
    DPPL typically struggle to measure minor changes adequately (cf. Sec. [3.5](#S3.SS5
    "3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization"),[4.2](#S4.SS2
    "4.2 Sparsification reveals: Attention is not all you need! ‣ 4 Token Metrics
    Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization") and Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")).
    On the other hand, FDT is particularly suited to describe errors for subtle model
    changes. Consequently, we apply FDT for model compression. In the following paragraph,
    we describe the selected parameters for compression using FDT in more detail.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们采用FDT、SDT和PPL作为评估整体模型分歧的指标。在模型压缩方面，我们展示了PPL及其变体DPPL通常难以充分测量细微变化（参见Sec. [3.5](#S3.SS5
    "3.5 Token vs. Perplexity Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization"),
    [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not all you need! ‣ 4
    Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")和Fig. [2](#S3.F2 "Figure
    2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")）。另一方面，FDT特别适合描述细微模型变化的错误。因此，我们在模型压缩中应用FDT。在接下来的段落中，我们将更详细地描述使用FDT的压缩参数。'
- en: Divergent Token Parameters.
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分歧标记参数。
- en: 'We empirically selected hyperparameters as follows. Through preliminary sparsification
    experiments, we observed that the most variance is present in the 75%-quantile
    of FDT, as defined in Eq. [3](#S3.E3 "In FDT. ‣ 3.4 Divergent Token Metrics ‣
    3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization"). We denote this value
    by *FDT[75]*. It is in the following our compare-to value.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们经验性地选择了超参数。通过初步稀疏化实验，我们观察到在FDT的75%分位数处存在最大的方差，如公式[3](#S3.E3 "In FDT. ‣ 3.4
    Divergent Token Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")中所定义。我们将此值记作*FDT[75]*。这是我们比较的值。'
- en: Next, we swept over the given *context prefix length* $n$ tokens, as it is most
    discriminative on average.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对给定的*上下文前缀长度*n个tokens进行了遍历，因为它在平均上最具区分性。
- en: We observed that most sparsification steps introduce an error in FDT[75] within
    a range of $500$.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，大多数稀疏化步骤在$500$范围内引入了FDT[75]的误差。
- en: '![Refer to caption](img/d362b8f44cac75211b4568c85ef67a07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d362b8f44cac75211b4568c85ef67a07.png)'
- en: 'Figure 3: Hyperparameter selection of FDT. Visualized is the std in FDT[75]
    over all components when varying prefix length (y-axis) and applying different
    choices for sparsity-step increases (x-axis) as described in Sec. [4.1](#S4.SS1
    "4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") and [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not
    all you need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：FDT的超参数选择。可视化显示了在变化前缀长度（y轴）和应用不同稀疏步骤增加（x轴）的情况下，所有组件的FDT[75]的标准差，如Sec. [4.1](#S4.SS1
    "4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")和[4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not all
    you need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")中所述。'
- en: '![Refer to caption](img/3cc6375f53a060e40698e7ae54b781f7.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3cc6375f53a060e40698e7ae54b781f7.png)'
- en: (a) Comparison of uniform and component-wise pruning using FDT as a metric for
    comparison.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用FDT作为比较指标，比较均匀剪枝和组件级剪枝。
- en: '![Refer to caption](img/fdf304dec9348977eaf571874ecedef4.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fdf304dec9348977eaf571874ecedef4.png)'
- en: (b) Converged component config with 75% average sparsity. Layers (x-axis), Component-sparsity
    (y-axis).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 具有75%平均稀疏度的收敛组件配置。层（x轴），组件稀疏度（y轴）。
- en: 'Figure 4: Depiction of the proposed sparsification process that converged to
    a 75% sparse Llama-2-13b. a) Model training performance throughout all rounds.
    Our FDT-based sparsification clearly outperforms uniform magnitude pruning. b)
    Converged sparsity values per component. One quarter of attention components are
    pruned beyond 90% sparsity. Significant outliers appear in first and last layers.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：所提出的稀疏化过程的描述，收敛到75%稀疏的Llama-2-13b。a) 在所有轮次中的模型训练性能。我们的基于FDT的稀疏化明显优于均匀幅度剪枝。b)
    各组件的收敛稀疏度值。四分之一的注意力组件剪枝超过90%稀疏度。第一层和最后一层出现了显著的离群值。
- en: Pruning of LLMs.
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的剪枝。
- en: 'In Sec. [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not all you
    need! ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization"), we will
    show that FDT improves sparsification procedures to achieve high compression rates
    on LLama2-13b.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '在Sec. [4.2](#S4.SS2 "4.2 Sparsification reveals: Attention is not all you need!
    ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization")中，我们将展示FDT如何改进稀疏化程序，以在LLama2-13b上实现高压缩率。'
- en: We iterate small unstructured sparsification with continued training steps for
    the model to attune to the remaining weights and recover performance. Specifically,
    we apply eight iterations to increase the average model sparsity by $20,15,10,10,5,5,5,$
    percent. As such, the final model has 25% total parameters remaining.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过持续的训练步骤对小规模非结构化稀疏化进行迭代，以使模型适应剩余权重并恢复性能。具体来说，我们应用了八次迭代，将平均模型稀疏度增加了$20,15,10,10,5,5,5,$百分比。因此，最终模型剩余25%的总参数。
- en: We run this experiment in two configurations, uniform and FDT-selective. Uniform
    sparsification applies the target increase of the current round to each component
    uniformly, pruning the lowest weights. For FDT, we determine individual component
    sparsification values to evenly distribute the induced error. Based on the previous
    sparsed model $F$ that optimize for
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两种配置下进行这个实验，均匀和FDT选择性。均匀稀疏化将当前轮次的目标增加均匀地应用于每个组件，修剪最低权重。对于FDT，我们确定各个组件的稀疏化值，以均匀分布引入的误差。基于先前优化的稀疏模型$F$
- en: '|  | $\textstyle\operatorname*{arg\,max}_{\{s_{i}\}}\min_{i}M_{\textrm{FDT}_{75}}(F,F^{c_{i}+s_{i}}),$
    |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textstyle\operatorname*{arg\,max}_{\{s_{i}\}}\min_{i}M_{\textrm{FDT}_{75}}(F,F^{c_{i}+s_{i}}),$
    |  |'
- en: such that $\sum_{i}\tilde{s}_{i}=step$ in.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使得 $\sum_{i}\tilde{s}_{i}=step$ 进行。
- en: 'We further follow the findings of AC/DC Peste et al. ([2021](#bib.bib10)) and
    alternate compressed and decompressed iterations as follows: Each round we train
    a total of $500$.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步遵循AC/DC Peste等人（[2021](#bib.bib10)）的发现，并交替进行压缩和解压缩迭代：每轮我们训练总共$500$次。
- en: Note that throughout this experiment series, we only apply pure magnitude pruning
    per iteration. The probing strategy can be applied to other methods, such as Wanda,
    as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在整个实验系列中，我们仅在每次迭代中应用纯幅度修剪。探测策略也可以应用于其他方法，例如Wanda。
- en: Quantization of LLMs.
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM的量化。
- en: 'For model quantization, we compare the performance of the proposed metrics
    on the task of sorting the model’s components by their lowest introduced error.
    To this end, we build a search tree to find the best model configuration as follows:
    We construct a parallel depth-first search tree with a branching width of $10$
    performing nodes, with one more component naively quantized using AbsMax. From
    this newly identified set of nodes, we again select the best-performing for the
    next iteration. Starting with the unquantized base model Llama2-7b, each node
    contains exactly the number of quantized components respective to its depth, while
    the final node is a fully AbsMax quantized model. We further apply deduplication
    to prevent redundant computations.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型量化，我们比较了提出的度量标准在按最低引入误差对模型组件进行排序任务中的表现。为此，我们构建了一个搜索树，以找到最佳的模型配置：我们构建了一个并行深度优先搜索树，分支宽度为$10$，执行节点，再用AbsMax简单量化一个组件。从这个新识别的节点集合中，我们再次选择表现最佳的进行下一轮迭代。从未量化的基础模型Llama2-7b开始，每个节点包含与其深度相对应的量化组件数，而最终节点是一个完全AbsMax量化的模型。我们进一步应用去重以防止冗余计算。
- en: '4.2 Sparsification reveals:'
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 稀疏化揭示：
- en: Attention is not all you need!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力不是你所需的一切！
- en: 'We applied step-wise uniform magnitude pruning, and our balanced component-wise
    pruning using FDT, to achieve 75% model sparsity. A summary of the results is
    shown in Fig. [4](#S4.F4 "Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了逐步均匀幅度修剪，并使用FDT进行平衡组件修剪，以实现75%的模型稀疏性。结果摘要见图[4](#S4.F4 "图4 ‣ 发散的令牌参数。 ‣
    4.1 实验协议 ‣ 4 令牌度量改进模型压缩 ‣ 发散令牌度量：测量降级以修剪LLM组件 – 并优化量化")。
- en: Attention almost erased.
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力几乎被抹去。
- en: 'Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    visualizes the converged sparsity values when applying our balanced pruning using
    FDT.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4(b)](#S4.F4.sf2 "在图4 ‣ 发散的令牌参数。 ‣ 4.1 实验协议 ‣ 4 令牌度量改进模型压缩 ‣ 发散令牌度量：测量降级以修剪LLM组件
    – 并优化量化") 直观展示了应用FDT进行平衡修剪时的收敛稀疏值。
- en: Notably, the model favors pruning Attention over MLP. In total 40 out of 160
    attention components are sparsed beyond 90% and 15 even completely removed. In
    general the second half of the model appears to be more prunable than the first
    half. The value matrices are overall least pruned of the attention matrices. Finally,
    significant outliers appear at the first and last layers. This finding indicates
    that attention is not efficiently utilized throughout the entire model. In fact,
    only layers 3 to 20 and layer 40 appear to be of significant relevance for the
    models final prediction. This observation might be attributed to an evolving shift
    in distributions, and therewith the concepts processed in embeddings.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，模型更倾向于修剪 Attention 而非 MLP。总的来说，160 个注意力组件中有 40 个稀疏化超过 90%，其中 15 个甚至完全移除。通常，模型的后半部分似乎比前半部分更易修剪。值矩阵总体上比注意力矩阵更少被修剪。最后，显著的异常值出现在第一层和最后一层。这一发现表明整个模型中注意力的利用效率不高。实际上，只有第
    3 层到第 20 层和第 40 层似乎对模型最终预测具有重要意义。这一观察可能与分布的演变有关，从而影响嵌入中处理的概念。
- en: Notably, in the first layer Attention Dense and MLP Down remain significant
    dense, while all others are comparable sparse. This observation indicates an incomplete
    shift of token-embeddings.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在第一层中，Attention Dense 和 MLP Down 仍然保持显著的密集，而其他所有层则相对稀疏。这一观察表明标记嵌入的转移不完全。
- en: General Observations.
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一般观察。
- en: 'As shown in Fig. [4(a)](#S4.F4.sf1 "In Figure 4 ‣ Divergent Token Parameters.
    ‣ 4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization"), FDT based balanced pruning significantly lowers the introduced
    error between sparsification rounds. Uniform pruning, on the other hand, substantially
    diverged, and i.p. does not regain performance with the given amount of compute.
    Generally speaking, what is lost can hardly be recovered.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [4(a)](#S4.F4.sf1 "在图 4 ‣ 分歧标记参数。 ‣ 4.1 实验协议 ‣ 4 标记度量提高模型压缩 ‣ 分歧标记度量：测量降级以修剪
    LLM 组件 – 并优化量化") 所示，基于 FDT 的平衡修剪显著降低了稀疏化轮次之间引入的误差。另一方面，均匀修剪则大幅度偏离，且 i.p. 在给定计算量下并未恢复性能。一般来说，丧失的部分很难恢复。
- en: 'The standard evaluation of FDT and PPL on Wikitext2, is found in Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").
    The 75% compressed 13b model, with several components pruned away, scored PPL
    8.1, compared to PPL 4.8 of the base model. Note that no other model sparsed beyond
    70% has yet been reported i.p. achieving single-digit PPL. Uniform pruning achieved
    13.5\. Further note, that we almost doubled the mean FDT value when comparing
    to uniform pruning. However, as the generally low FDT value suggests, it already
    diverged quite far from the base model.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: FDT 和 PPL 在 Wikitext2 上的标准评估见表 [1](#S4.T1 "表 1 ‣ 无异常值处理的量化。 ‣ 4.3 量化：可以防止异常值
    ‣ 4 标记度量提高模型压缩 ‣ 分歧标记度量：测量降级以修剪 LLM 组件 – 并优化量化")。75% 压缩的 13b 模型，经过多个组件修剪后，PPL
    得分为 8.1，相比之下基准模型的 PPL 为 4.8。值得注意的是，尚未有其他模型在稀疏化超过 70% 的情况下实现单数位 PPL。均匀修剪达到了 13.5。进一步注意，我们在比较均匀修剪时几乎将平均
    FDT 值翻倍。然而，由于通常较低的 FDT 值表明，其已远离基准模型。
- en: FDT is more discriminative.
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FDT 更具辨别力。
- en: 'In practice, FDT is more discriminative than PPL to subtle changes. We demonstrate
    this with a test as follows: On each component of the model, we prune 0.1% of
    the weights either randomly or from the lowest weights at a time. The resulting
    model is probed for $1000$ trials with all discussed metrics used to distinguish
    the cases. The results in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3
    Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation to prune
    away LLM components – and optimize quantization") clearly indicate that FDT is
    able to distinguish the cases, while they remain indifferent for PPL-based comparison.
    We therefore omit using PPL as a metric to determine step-sizes for the described
    sparsification experiment.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '在实践中，FDT 对细微变化的区分能力比 PPL 更强。我们通过以下测试演示了这一点：在模型的每个组件上，我们每次随机或从最低权重中修剪 0.1% 的权重。使用所有讨论的度量指标对得到的模型进行
    $1000$ 次试验以区分这些情况。图 [2](#S3.F2 "Figure 2 ‣ 3.2 Perplexity (PPL) ‣ 3 Model Divergence
    Metrics ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") 的结果清楚地表明 FDT 能够区分这些情况，而 PPL 基于的比较则无动于衷。因此，我们省略了使用
    PPL 作为确定步骤大小的度量指标，以进行描述的稀疏化实验。'
- en: 'In Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1
    Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") the converged sparsity rates for all components are displayed.
    A final discussion of the sparsification experiment is found in App. [G](#A7 "Appendix
    G Details on Sparsification, Sec. 4.2 ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental
    Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    中，显示了所有组件的收敛稀疏率。有关稀疏化实验的最终讨论见附录 [G](#A7 "Appendix G Details on Sparsification,
    Sec. 4.2 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")。'
- en: '4.3 Quantization: Outliers can be prevented'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 量化：可以防止异常值
- en: Finally, we demonstrate the impact of selecting the right components on quantization.
    We compare the proposed metrics PPL, DPPL, and FDT as a ranking criteria to showcase
    their discrimation capabilities.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们展示了选择正确组件对量化的影响。我们将提出的度量标准 PPL、DPPL 和 FDT 作为排序标准，展示它们的区分能力。
- en: '![Refer to caption](img/509f2300dc41c1565bd78c4d52d1b0c2.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/509f2300dc41c1565bd78c4d52d1b0c2.png)'
- en: (a) Performance of FDT vs PPL
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (a) FDT 与 PPL 的性能比较
- en: '![Refer to caption](img/91d033441dbaf17d3f6964b36642a93c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91d033441dbaf17d3f6964b36642a93c.png)'
- en: (b) Selected Components FDT vs PPL
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 选择的组件 FDT 与 PPL 比较
- en: 'Figure 5: Evaluation of the Tree Search as described in text. a) Comparison
    of Tree Search based component-wise quantization. Different number of components
    (x-axis) lead to different token divergence scores (y-axis, normalized to $[0,1]$),
    and i.p. correlates early on to introduces ouliers (second y-axis). Throughout
    the entire search, FDT is able to rank components by their potential errors, and,
    coincidentally, outliers. b) Selected components at respective depth. A.Key and
    A.Value induce most error.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：如文本中所述的树搜索评估。a) 基于树搜索的组件量化比较。不同数量的组件（x 轴）会导致不同的令牌发散分数（y 轴，归一化到 $[0,1]$），并且
    i.p. 早期与引入异常值相关（第二个 y 轴）。在整个搜索过程中，FDT 能够根据潜在错误和偶然的异常值对组件进行排序。b) 各深度下的选择组件。A.Key
    和 A.Value 引发了最多的错误。
- en: Quantization without outlier-handling.
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无异常值处理的量化。
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4
    Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") shows the average performance
    of the top 10 nodes occuring in the respective search tree depth (x-axis). FDT
    constantly outperforms the other metrics on the Share-of-Divergent-Token metric
    (y-axis). Notably, this goes on par with the total number of occurring outliers
    for the respective configs (second y-axis). Certain components appear to significantly
    influence the decline observed in both measures. While DPPL enhances some aspects
    of performance, neither variant of PPL effectively distinguishes these components
    and tends to select those prematurely.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") 显示了在相应搜索树深度（x轴）中前10个节点的平均性能。FDT在Share-of-Divergent-Token指标（y轴）上始终优于其他指标。值得注意的是，这与相应配置的总异常值数量（第二个y轴）相符。某些组件显著影响这两个度量中观察到的下降。虽然DPPL提升了性能的某些方面，但PPL的任何变体都无法有效区分这些组件，并且倾向于过早选择它们。'
- en: 'With FDT, we can cast 80%, i.e. 150, of the model’s components directly to
    int8 using only naive AbsMax—and without further outlier handling—still outperforming
    full LLM.int8() conversion in model performance. Selecting those 150 components
    with DPPL and FDT leads to a close perplexity scores $5.490$ on Wikitext2, c.f.
    Tab. [1](#S4.T1 "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization:
    Outliers can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization"). However the resulting mean FDT improves by almost 50% when also
    selecting the components by this metric. The larger generation of the same sequences
    suggest a model closer to the original when choosing FDT as a selection criteria.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '使用FDT，我们可以将80%（即150个）的模型组件直接转换为int8，只需使用简单的AbsMax—且无需进一步处理异常值—仍能在模型性能上超越完整的LLM.int8()转换。使用DPPL和FDT选择这150个组件在Wikitext2上的困惑度得分接近$5.490$，参见表[1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")。然而，当根据该指标选择组件时，结果的平均FDT提高了近50%。相同序列的更大生成表明，当选择FDT作为选择标准时，模型更接近原始模型。'
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4
    Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")b) shows the selected
    components to each depth respective of a). Most outliers occur when selecting
    A. Key early on. Notably, we observed in Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization"), that this is one of the matrices most suitable
    to sparsify.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "Figure 5 ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")b) 显示了与a)相应的每个深度的选定组件。大多数异常值发生在选择A时，特别是在早期。值得注意的是，我们在第[4.2节](#S4.SS2
    "4.2 Sparsification reveals: Attention is not all you need! ‣ 4 Token Metrics
    Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization")观察到，这些是最适合稀疏化的矩阵之一。'
- en: '| Sparsification |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏化 |'
- en: '|  | Model | FDT $\uparrow$ |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | FDT $\uparrow$ |'
- en: '|  | Llama2-13b | - | 4.884 | 53.59 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama2-13b | - | 4.884 | 53.59 |'
- en: '| \hdashline | $\sim$ 60% sparse (unif.) | 004.7 | 09.244 | 46.32 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline | $\sim$ 60% 稀疏（均匀） | 004.7 | 09.244 | 46.32 |'
- en: '|  | $\sim$ 60% sparse (our) | 007.9 | 6.242 | 48.89 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sim$ 60% 稀疏（我们的） | 007.9 | 6.242 | 48.89 |'
- en: '| \hdashline | $\sim$13.512 | 41.67 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline | $\sim$13.512 | 41.67 |'
- en: '|  | $\sim$ 75% sparse (our) | 005.5 | 8.101 | 46.32 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sim$ 75% 稀疏（我们的） | 005.5 | 8.101 | 46.32 |'
- en: '| \hdashline | $\sim$ 80% sparse (our) | 005.2 | 9.531 | 45.66 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline | $\sim$ 80% 稀疏（我们的） | 005.2 | 9.531 | 45.66 |'
- en: '| Quantization |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 量化 |'
- en: '|  | Model | FDT $\uparrow$ |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | FDT $\uparrow$ |'
- en: '|  | Llama2-7b | - | 5.472 | 50.79 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | Llama2-7b | - | 5.472 | 50.79 |'
- en: '| \hdashline int8 | LLM.int8()[all] | 036.1 | 5.505 | 50.81 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline int8 | LLM.int8()[all] | 036.1 | 5.505 | 50.81 |'
- en: '| AbsMax PPL[150] | 0$\bullet$46.3 | 5.500 | 50.72 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| AbsMax PPL[150] | 0$\bullet$46.3 | 5.500 | 50.72 |'
- en: '| AbsMax DPPL[150] | 054.1 | 5.490 | 50.75 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| AbsMax DPPL[150] | 054.1 | 5.490 | 50.75 |'
- en: '| AbsMax FDT[150] (our) | 071.7 | 5.489 | 50.75 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| AbsMax FDT[150]（我们的） | 071.7 | 5.489 | 50.75 |'
- en: '| \hdashline int4 | GPTQ[all] | 011.1 | 5.665 | 48.34 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline int4 | GPTQ[all] | 011.1 | 5.665 | 48.34 |'
- en: '| GPTQ PPL[16] | 45.0 | 5.511 | 49.91 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ PPL[16] | 45.0 | 5.511 | 49.91 |'
- en: '| GPTQ DPPL[16] | 0137.0 | 5.476 | 50.02 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ DPPL[16] | 0137.0 | 5.476 | 50.02 |'
- en: '| GPTQ FDT[16] (our) | 205.0 | 5.475 | 50.13 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ FDT[16] (我们的) | 205.0 | 5.475 | 50.13 |'
- en: 'Table 1: Evals of Compressed Models. Even when evaluating the final model,
    standard NLP benchmarks don’t reflect the actual model degradation, as observed
    in AbsMax quantization. FDT, PPL are evaluated on Wikitext2. Subscript refers
    to best found $k$ quantized components. Bold denote best values.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：压缩模型的评估。即使在评估最终模型时，标准NLP基准测试也无法反映实际模型的退化，这在AbsMax量化中有所观察。FDT、PPL在Wikitext2上进行评估。下标指的是找到的最佳$k$量化组件。粗体表示最佳值。
- en: '![Refer to caption](img/c50dc5b7cd11709fe8aaff9b6f8a48e6.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c50dc5b7cd11709fe8aaff9b6f8a48e6.png)'
- en: (a) Quantization methods evaluated on Components.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 量化方法在组件上的评估。
- en: '![Refer to caption](img/c27372d6927c8bbbe683056e75c7f3b9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c27372d6927c8bbbe683056e75c7f3b9.png)'
- en: (b) Top-selected GPTQ(4bit) components.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 选择的GPTQ(4bit)组件。
- en: 'Figure 6: Evaluation of FDT performance. a) evaluates components separately
    on all quantization methods. Clear outliers in performance are A.Value and MLP.up.
    GPTQ(8bit) is able to evenly amortize the induced error. b) Selecting top-k components
    of GPTQ(4bit). FDT is suited to rank components one-shot.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：FDT性能评估。a) 分别对所有量化方法的组件进行评估。性能中明显的异常值是A.Value和MLP.up。GPTQ(8bit)能够均匀地摊销引入的误差。b)
    选择GPTQ(4bit)的前k个组件。FDT适合一次性排名组件。
- en: 16 components in 4-bit.
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4-bit中的16个组件。
- en: 'Figure [6(a)](#S4.F6.sf1 "In Figure 6 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization") presents a comprehensive assessment of
    the quantization techniques discussed. First, it is noticeable that the LLM.int8()
    method slightly lowers the lower quantile scores of FDT in comparison to AbsMax.
    Yet, GPTQ-8bit demonstrates superior performance, outshining both plain AbsMax
    and LLM.int8(). This method achieves a more balanced error distribution across
    all components (c.f. Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization
    Sec. 4.3 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")). Conversely, GPTQ-4bit shows noticeable deviations
    in the generation process, with only a limited number of components achieving
    FDT scores above 300\. Despite this, the discriminative power of FDT enabled us
    to identify and merge the top 16 components that minimally compromised the model’s
    integrity, as illustrated in Fig. [6(b)](#S4.F6.sf2 "In Figure 6 ‣ Quantization
    without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6(a)](#S4.F6.sf1 "图6 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣ 4 令牌指标改进模型压缩 ‣ 不同的令牌指标：测量退化以修剪LLM组件并优化量化")呈现了对讨论的量化技术的全面评估。首先，可以明显看出，与AbsMax相比，LLM.int8()方法略微降低了FDT的下分位数得分。然而，GPTQ-8bit表现更优，超越了简单的AbsMax和LLM.int8()。该方法在所有组件中实现了更均衡的误差分布（参见图[15](#A6.F15
    "图15 ‣ 附录F 量化的详细信息 第4.3节 ‣ 不同的令牌指标：测量退化以修剪LLM组件并优化量化")）。相反，GPTQ-4bit在生成过程中显示出明显的偏差，只有少数组件的FDT得分超过了300。然而，FDT的辨别能力使我们能够识别和合并前16个组件，这些组件对模型的完整性影响最小，如图[6(b)](#S4.F6.sf2
    "图6 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣ 4 令牌指标改进模型压缩 ‣ 不同的令牌指标：测量退化以修剪LLM组件并优化量化")所示。
- en: 'We conclude that PPL is not suitable for either, selecting components to compress,
    or for assessing the degradation of compressed models, as indicated in Table [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得出结论，PPL既不适合选择要压缩的组件，也不适合评估压缩模型的退化，如表[1](#S4.T1 "表1 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值
    ‣ 4 令牌指标改进模型压缩 ‣ 不同的令牌指标：测量退化以修剪LLM组件并优化量化")所示。
- en: 'In Appendix Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization
    Sec. 4.3 ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") detailed FDT statistics on all components are displayed.
    A final discussion of the quantization experiment is found in App. [B](#A2 "Appendix
    B FDT compared to standard model evals ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '在附录图[15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization Sec. 4.3 ‣
    Divergent Token Metrics: Measuring degradation to prune away LLM components –
    and optimize quantization")中展示了所有组件的详细FDT统计数据。关于量化实验的最终讨论见附录[B](#A2 "Appendix
    B FDT compared to standard model evals ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")。'
- en: 5 Conclusion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We introduced the Divergent Token Metrics (DTMs), a tailored approach to evaluate
    the performance differences of compressed generative models. In particular, DTMs
    respect the usually applied greedy sampling procedure to generate predictions.
    We proved that DTMs achieve appropriate metric bounds and are not affected from
    catastrophic artefacts that perplexity-based metrics encounter.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了发散标记度量（DTMs），这是一种评估压缩生成模型性能差异的定制方法。特别是，DTMs遵循通常应用的贪婪采样程序来生成预测。我们证明了DTMs能够达到适当的度量界限，并且不受困惑度基础度量所遇到的灾难性伪影影响。
- en: Due to our metrics’ discriminative capabilities, we are able to measure the
    subtle influence of model components individually, and in turn build a fine-grained
    selection for compression. With our sparsification experiments we achieved an
    outperforming 75% sparse version of the Llama2-13b model with a small amount of
    training steps and otherwise only applying magnitude pruning. Many (in particular
    attention) modules were entirely removed, which hints that attention is, after
    all, not always needed—during inference—in decoder models. Notably, the MLP components
    in the first and last layers are extremely sensitive, which hints at an incomplete
    shift of token-embedding distributions. For quantization, we were able to sort
    the influence of the quantized components individually. Interestingly sorting
    by FDT coincides with sorting by outliers. We successfully converted 80% of the
    LLama2-7b components naively to int8 using AbsMax, without severe degeneration
    of performance, and without any outlier-handling. Further, we concluded that GPTQ-8bit
    performs very well. The 4bit version, on the other hand, significantly degenerates.
    However, we were able to select the 16 out of 224 significantly outperforming
    components, that even combined, sustained substantial model performance.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们度量标准的辨别能力，我们能够单独测量模型组件的微妙影响，从而建立一个精细的压缩选择。通过稀疏化实验，我们成功地将Llama2-13b模型压缩到75%的稀疏版本，只用了少量的训练步骤，并且只应用了幅度剪枝。许多（特别是注意力）模块被完全移除，这表明注意力在解码器模型中，在推理时并非总是必要的。值得注意的是，第一层和最后一层的MLP组件非常敏感，这表明token-embedding分布尚未完全转换。对于量化，我们能够单独排序量化组件的影响。有趣的是，按FDT排序与按异常值排序是一致的。我们成功地将80%的LLama2-7b组件通过AbsMax简单地转换为int8，没有严重的性能退化，也没有任何异常值处理。此外，我们得出结论，GPTQ-8bit表现非常好。而4bit版本则显著退化。然而，我们能够从224个组件中选择出16个表现显著的组件，这些组件即使结合在一起，也能保持模型的高性能。
- en: Building up on this work, one could investigate further variations of our metric
    for specific use-cases. The average distances between falsely predicted tokens,
    as an example, intuitively reflect the efficiency of speculative sampling algorithms.
    We envision fully heterogeneous model architectures, with components mixed in
    precision and varying levels of sparsities. Afterall, there is ample diversity
    throughout the model to be exploit.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，可以进一步研究我们度量的不同变体以适应特定用例。例如，错误预测的标记之间的平均距离，直观上反映了投机采样算法的效率。我们设想了完全异质的模型架构，其中组件的精度和稀疏度水平各不相同。毕竟，模型中有大量的多样性可以利用。
- en: Limitations
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: With the proposed DTMs, compression processes can be tailored to use cases—and
    we can measure its performance degeneration. We hinted with the sparsification
    experiments, that MLP and Attention can be ascribed varying levels of significance
    throughout the layers. These variations should further be exploited to optimize
    model architectures. In particular variations of specific datasets to probe or
    finetune on could lead to interesting variations.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提议的DTMs，可以根据使用案例量身定制压缩过程，并可以衡量其性能退化。我们通过稀疏化实验暗示了MLP和Attention在各层中的重要性可能有所不同。这些变化应进一步被利用来优化模型架构。特别是，特定数据集的变化用于探测或微调可能会导致有趣的变化。
- en: As a pruning strategy, we achieved outperforming results using only naive magnitude
    pruning. DTMs should directly be applicable to other masking strategies, such
    as Wanda Sun et al. ([2023](#bib.bib14)), which may further improve results. Finally,
    the generalizability of the metrics to other sampling strategies should be investigated.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种修剪策略，我们仅使用简单的幅度修剪就取得了优异的结果。DTMs应当可以直接应用于其他掩蔽策略，例如Wanda Sun等人（[2023](#bib.bib14)），这可能进一步改善结果。最后，应调查这些度量标准对其他采样策略的泛化能力。
- en: Ethics Statement
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: We affirm that our research adheres to the [ACL Ethics Policy](https://www.aclweb.org/portal/content/acl-code-ethics).
    This work involves the use of publicly available data sets and does not involve
    human subjects or any personally identifiable information. We declare that we
    have no conflicts of interest that could potentially influence the outcomes, interpretations,
    or conclusions of this research. All funding sources supporting this study are
    acknowledged. We have made our best effort to document our methodology, experiments,
    and results accurately and are committed to sharing our code, data, and other
    relevant resources to foster reproducibility and further advancements in research.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认我们的研究遵循[ACL伦理政策](https://www.aclweb.org/portal/content/acl-code-ethics)。本研究涉及使用公开可用的数据集，不涉及人类受试者或任何个人身份信息。我们声明没有可能影响研究结果、解释或结论的利益冲突。所有支持本研究的资金来源均已列明。我们已尽最大努力准确记录我们的方法、实验和结果，并承诺分享我们的代码、数据和其他相关资源，以促进可重复性和进一步的研究进展。
- en: Acknowledgments
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been partially funded by the Deutsche Forschungsgemeinschaft (DFG,
    German Research Foundation) as part of BERD@NFDI - grant number 460037581.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由德国研究基金会（Deutsche Forschungsgemeinschaft, DFG）资助，作为BERD@NFDI项目的一部分，资助编号为460037581。
- en: We gratefully acknowledge support by the German Center for Artificial Intelligence
    (DFKI) project “SAINT”, the Hessian Ministry of Higher Education, and the Research
    and the Arts (HMWK) cluster projects “The Adaptive Mind” and “The Third Wave of
    AI”, and the HMWK and BMBF ATHENE project “AVSV”.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感激德国人工智能中心（DFKI）“SAINT”项目、黑森州高等教育部以及研究与艺术（HMWK）“适应性思维”和“人工智能的第三波”集群项目，以及HMWK和BMBF
    ATHENE项目“AVSV”的支持。
- en: We further thank Graphcore and Manuel Brack for the fruitful discussions throughout
    this work.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步感谢Graphcore和Manuel Brack在整个研究过程中提供的富有成效的讨论。
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020年）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell等。2020年。《语言模型是少样本学习者》。*神经信息处理系统进展*，33:1877–1901。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck等人（2023年）Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg等。2023年。《人工通用智能的火花：与gpt-4的早期实验》。*arXiv预印本
    arXiv:2303.12712*。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery等人（2022年）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann等。2022年。《Palm：通过路径扩展语言建模》。*arXiv预印本 arXiv:2204.02311*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. [Llm.int8(): 8-bit matrix multiplication for transformers at scale](http://arxiv.org/abs/2208.07339).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    2022. [Llm.int8(): 规模化变换器的8位矩阵乘法](http://arxiv.org/abs/2208.07339)。'
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh (2023) Elias Frantar 和 Dan Alistarh. 2023. Sparsegpt: 大型语言模型可以在一次训练中准确剪枝。'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. [Gptq: Accurate post-training quantization for generative pre-trained
    transformers](http://arxiv.org/abs/2210.17323).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等人 (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    2023. [Gptq: 生成预训练变换器的精确后训练量化](http://arxiv.org/abs/2210.17323)。'
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. 2021. [A framework for few-shot language model evaluation](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, 和 Andy
    Zou. 2021. [少样本语言模型评估框架](https://doi.org/10.5281/zenodo.5371628)。
- en: Leviathan et al. (2023) Yaniv Leviathan, Matan Kalman, and Yossi Matias. 2023.
    Fast inference from transformers via speculative decoding. In *International Conference
    on Machine Learning*, pages 19274–19286\. PMLR.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan 等人 (2023) Yaniv Leviathan, Matan Kalman, 和 Yossi Matias. 2023. 通过推测解码从变换器中快速推理。在
    *国际机器学习大会* 上，页码 19274–19286。PMLR。
- en: 'OpenAI (2022) OpenAI. 2022. [Chatgpt: Optimizing language models for dialogue](https://openai.com/blog/chatgpt/).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2022) OpenAI. 2022. [Chatgpt: 优化对话的语言模型](https://openai.com/blog/chatgpt/)。'
- en: 'Peste et al. (2021) Alexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan
    Alistarh. 2021. Ac/dc: Alternating compressed/decompressed training of deep neural
    networks. *Advances in neural information processing systems*, 34:8557–8570.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peste 等人 (2021) Alexandra Peste, Eugenia Iofinova, Adrian Vladu, 和 Dan Alistarh.
    2021. Ac/dc: 深度神经网络的交替压缩/解压训练。*神经信息处理系统进展*, 34:8557–8570。'
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    等人. 2018. 通过生成预训练提升语言理解。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever, 等人. 2019. 语言模型是无监督的多任务学习者。*OpenAI 博客*, 1(8):9。
- en: Raffel et al. (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://arxiv.org/abs/1910.10683).
    *arXiv e-prints*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 (2019) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu. 2019. [利用统一的文本到文本变换器探索迁移学习的极限](http://arxiv.org/abs/1910.10683)。*arXiv
    预印本*。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. *arXiv preprint
    arXiv:2306.11695*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 (2023) Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter. 2023. 大型语言模型的简单有效的剪枝方法。*arXiv
    预印本 arXiv:2306.11695*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人. 2023. Llama 2: 开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: Appendix
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Proof of Propositions
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 命题证明
- en: 'Proof of Proposition [3.2](#S3.Thmthm2 "Proposition 3.2\. ‣ 3.5 Token vs. Perplexity
    Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 [3.2](#S3.Thmthm2 "命题 3.2\. ‣ 3.5 Token 与困惑度指标 ‣ 3 模型分歧指标 ‣ 分歧 Token 指标：测量退化以修剪
    LLM 组件 – 以及优化量化") 的证明。
- en: 'There are many ways to construct sequences that satisfy the desired relation.
    One is as follows: Let $l\in{\mathbb{R}}^{N\times|\mathcal{V}|}$ by'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 构造满足所需关系的序列有很多方法。一个方法如下：令 $l\in{\mathbb{R}}^{N\times|\mathcal{V}|}$ 为
- en: '|  | $\displaystyle l^{\prime}_{ia_{2}(l)_{i}}$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle l^{\prime}_{ia_{2}(l)_{i}}$ |  |'
- en: and $l^{\prime}_{ij}=l_{ij}$. ∎
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 且 $l^{\prime}_{ij}=l_{ij}$。∎
- en: 'Proof of Proposition [3.3](#S3.Thmthm3 "Proposition 3.3\. ‣ 3.5 Token vs. Perplexity
    Metrics ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization").'
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '命题 [3.3](#S3.Thmthm3 "Proposition 3.3\. ‣ 3.5 Token vs. Perplexity Metrics
    ‣ 3 Model Divergence Metrics ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization") 的证明。'
- en: Let $z=\mathcal{G}(l,y_{:n},N)$. Applying the definitions and elementary operations,
    we have
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $z=\mathcal{G}(l,y_{:n},N)$。应用定义和基本操作，我们得到
- en: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}=(N-n)\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).$
    |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}=(N-n)\log M_{\mathrm{DPPL}}(l,l^{\prime},y_{:n},N).$
    |  |'
- en: Let $A=\{i\geq n\colon p_{i}\leq 1/2\}$. Then
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $A=\{i\geq n\colon p_{i}\leq 1/2\}$。然后
- en: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}$ |  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{i=n}^{N}-\log p_{i}$ |  |'
- en: '|  |  | $\displaystyle\geq\sum_{i\in A}-\log p_{i}\geq&#124;A&#124;\log 2.$
    |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq\sum_{i\in A}-\log p_{i}\geq&#124;A&#124;\log 2.$
    |  |'
- en: Here we first used that $\log p_{i}\leq 0$ is automatically the maximum value
    of the distribution, and the softmax operation is monotone. Putting everything
    together we arrive at the desired inequality. ∎
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们首先利用了 $\log p_{i}\leq 0$ 自动是分布的最大值，并且 softmax 操作是单调的。将所有内容结合起来，我们得到了期望的不等式。∎
- en: '![Refer to caption](img/3a1423bbb8c68a66dea48b9a842132ab.png)![Refer to caption](img/d273c5ebe55c045c8c1e986f3f89fbe1.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a1423bbb8c68a66dea48b9a842132ab.png)![参见说明](img/d273c5ebe55c045c8c1e986f3f89fbe1.png)'
- en: 'Figure 7: Pruning MLP and Attn only indeed compromises remaining model capabilities.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：仅修剪 MLP 和 Attn 确实会削弱剩余模型的能力。
- en: Appendix B FDT compared to standard model evals
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B FDT 与标准模型评估的比较
- en: 'Fig. [10](#A5.F10 "Figure 10 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows a comparison of standard benchmarks (middle)
    to FDT (right) and PPL (left) when quantizing parts of a model. Often standard
    evals fail to distinguish between compressed models. Sometimes they even depict
    better performance—which may be true, when regarding compression as a fine-tuning
    method and considering the short required token predictions. FDT thoroughly gives
    discriminative statistics with resprect to the base model, on how much the compressed
    model equals the original. Note how the error seems to be upper bounded, which
    suggests that errors may average out throughout the model. Mean zeroshot accuracy
    denotes the average on the standard NLP-eval harness.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [10](#A5.F10 "Figure 10 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") 展示了在对模型部分进行量化时，标准基准（中间）与 FDT（右侧）和 PPL（左侧）的比较。标准评估通常未能区分压缩模型。有时它们甚至表现出更好的性能——当将压缩视为微调方法并考虑所需的短期
    token 预测时，这可能是正确的。FDT 充分提供了相对于基础模型的区分统计数据，显示压缩模型与原始模型的等价程度。注意错误似乎有上界，这表明错误可能在整个模型中平均分布。平均零样本准确率表示标准
    NLP 评估工具上的平均值。'
- en: Appendix C True positives can be predicted
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 真正正例可以被预测
- en: 'Fig. [8](#A3.F8 "Figure 8 ‣ Appendix C True positives can be predicted ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") shows several metrics applied to the token-distributions, in order
    to estimate on whether the compressed and original model predictions are equal.
    Notably, L1 and L2 errors on the entire distribution seem to somewhat capture
    the discriminative capabilities of false predictions. The probability scores themselves
    are only marginally usable. Using top-2 uncertainty, i.e. the difference between
    the top-2 tokens as a measure, we obtain a reliable prediction of true positives.
    True negatives however still remain with a significant overlap.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [8](#A3.F8 "Figure 8 ‣ Appendix C True positives can be predicted ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") 展示了应用于 token 分布的若干度量，以估计压缩模型与原始模型预测是否相等。值得注意的是，整个分布上的 L1 和 L2 错误似乎在一定程度上捕捉到了错误预测的区分能力。概率分数本身仅有轻微的可用性。使用
    top-2 不确定性，即前 2 个 token 之间的差异作为度量，我们获得了对真正正例的可靠预测。然而，真正负例仍然存在显著的重叠。'
- en: '![Refer to caption](img/ed004c18300ce14b20e426527e157ba9.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ed004c18300ce14b20e426527e157ba9.png)'
- en: 'Figure 8: Top-2 uncertainty is discriminative enough to give clear true-positives
    estimates on compressed models.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：Top-2 不确定性足够具区分性，可以对压缩模型给出明确的真正正例估计。
- en: Appendix D MLP is for knowledge,
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D MLP 用于知识，
- en: Attention for relation
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 关系注意力
- en: 'Finally, we observed that when pruning only attention, prompt-extraction capabilities
    degenerate severely. When only pruning MLP components, on the other hand, it influences
    mostly world knowledge QA benchmarks, c.f. Fig. [7](#A1.F7 "Figure 7 ‣ Appendix
    A Proof of Propositions ‣ Divergent Token Metrics: Measuring degradation to prune
    away LLM components – and optimize quantization").'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们观察到，当仅修剪注意力机制时，提示提取能力严重退化。另一方面，当仅修剪MLP组件时，它主要影响世界知识QA基准测试，参见图 [7](#A1.F7
    "图 7 ‣ 附录 A 命题证明 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化")。
- en: 'Appendix E Details on Search Tree, Sec. [4.3](#S4.SS3 "4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")'
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 搜索树的详细信息，第 [4.3](#S4.SS3 "4.3 量化：可以防止异常值 ‣ 4 令牌度量改进模型压缩 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化")节
- en: 'Fig. [9](#A5.F9 "Figure 9 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization") shows the layers (y-axis) of which components are selected at each
    round (x-axis). While there seems to be a pattern on when using FDT as a criteria
    (top), selection by PPL (bottom) looks more random.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [9](#A5.F9 "图 9 ‣ 附录 E 搜索树的详细信息，第 4.3 节 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化") 显示了每轮（x轴）选择的组件的层级（y轴）。使用FDT作为标准（顶部）时似乎有一定模式，而PPL（底部）的选择则显得更随机。
- en: 'Fig. [13](#A5.F13 "Figure 13 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the comparison of search tree as described
    to greedy search on a single evaluation of all components. Until 150 components,
    FDT proves more stable over the PPL variants as seen in Fig. [13(a)](#A5.F13.sf1
    "In Figure 13 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [13](#A5.F13 "图 13 ‣ 附录 E 搜索树的详细信息，第 4.3 节 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化")
    显示了搜索树与贪婪搜索在单次评估所有组件上的比较。直到150个组件，FDT在稳定性上优于PPL变体，如图 [13(a)](#A5.F13.sf1 "图 13
    ‣ 附录 E 搜索树的详细信息，第 4.3 节 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化")所示。
- en: '![Refer to caption](img/a1fa1849168639d7848b3205b2b91cec.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a1fa1849168639d7848b3205b2b91cec.png)'
- en: 'Figure 9: Layers selected in each round of the search tree. Top, when applying
    FDT, bottom, when applying PPL as a ranking metric.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：每轮搜索树中选择的层级。顶部为应用FDT时，底部为应用PPL作为排名度量时。
- en: '![Refer to caption](img/1701b09ccf19bbdc1f5c1d849f7af700.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1701b09ccf19bbdc1f5c1d849f7af700.png)'
- en: 'Figure 10: Comparison of the discrimination capabilities of FDT and PPL for
    different configurations when applying LLM.int8() conversion on Llama2-7b. Best
    and Worst mark a single component being converted, with most and least mean influence.
    First and Second half consecutively convert half of the model each. While significant
    changes can be observed using FDT, all configurations appear indifferent for PPL.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在对Llama2-7b应用LLM.int8()转换时，FDT和PPL在不同配置下的区分能力比较。最佳和最差标记了单个组件的转换，分别具有最大和最小的平均影响。第一和第二部分分别转换了模型的一半。虽然使用FDT可以观察到显著变化，但所有PPL配置似乎都没有差异。
- en: '![Refer to caption](img/e3ffba80f2a0054aacea49f2a0de983b.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e3ffba80f2a0054aacea49f2a0de983b.png)'
- en: (a) 8-bit Quantization NLP benchmarks
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 8位量化NLP基准测试
- en: '![Refer to caption](img/03bb44d07bc9b4bd079a2e914f5fb671.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/03bb44d07bc9b4bd079a2e914f5fb671.png)'
- en: (b) 4-bit Quantization NLP benchmarks
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 4位量化NLP基准测试
- en: 'Figure 11: Detailed view on aggregated values of Tab. [1](#S4.T1 "Table 1 ‣
    Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented
    ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization") when selecting
    Llama2-7b components to quantize by metrics.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：选择Llama2-7b组件进行量化时的Tab. [1](#S4.T1 "表 1 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣
    4 令牌度量改进模型压缩 ‣ 发散令牌度量：测量退化以修剪LLM组件——并优化量化")的汇总值详细视图。
- en: '![Refer to caption](img/c0d0c5ed15a38503c3a2351106a341a5.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c0d0c5ed15a38503c3a2351106a341a5.png)'
- en: 'Figure 12: Detailed view on aggregated values of Tab. [1](#S4.T1 "Table 1 ‣
    Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented
    ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring
    degradation to prune away LLM components – and optimize quantization") when selecting
    Llama2-13b components to sparsify by metrics.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '图12：展示了选择Llama2-13b组件以通过指标进行稀疏化时的表[1](#S4.T1 "Table 1 ‣ Quantization without
    outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics
    Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation to
    prune away LLM components – and optimize quantization")的汇总值详细视图。'
- en: '![Refer to caption](img/d266dcaf687a4d2349dad08665ef7f16.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d266dcaf687a4d2349dad08665ef7f16.png)'
- en: (a) FDT tree vs greedy
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: (a) FDT树与贪婪选择
- en: '![Refer to caption](img/9e38fc03192fa19b5e0cbf00d4b01963.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e38fc03192fa19b5e0cbf00d4b01963.png)'
- en: (b) DPPL tree vs greedy
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DPPL树与贪婪选择
- en: '![Refer to caption](img/d7614092e0bee4506aea5883ffc60b12.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7614092e0bee4506aea5883ffc60b12.png)'
- en: (c) PPL tree vs greedy
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: (c) PPL树与贪婪选择
- en: 'Figure 13: Comparison of performance when selecting components by the tree-search
    as described to greedy selection of once evaluated components for all discussed
    metrics. Clearly, FDT is most stable until 150 components.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：展示了树搜索方法与贪婪选择方法的性能对比，涉及所有讨论的指标。显然，FDT在150个组件之前最为稳定。
- en: 'Appendix F Details on Quantization Sec. [4.3](#S4.SS3 "4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization")'
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录F 量化细节 第[4.3](#S4.SS3 "4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")节'
- en: 'Fig. [15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows detailed component-wise evaluations aggregated
    in Fig. [6(a)](#S4.F6.sf1 "In Figure 6 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '图[15](#A6.F15 "Figure 15 ‣ Appendix F Details on Quantization Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")显示了组件逐一评估的详细数据，汇总在图[6(a)](#S4.F6.sf1 "In Figure 6 ‣ Quantization
    without outlier-handling. ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization")中。'
- en: 'Fig. [14](#A6.F14 "Figure 14 ‣ Appendix F Details on Quantization Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the final configurations as compared in Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '图[14](#A6.F14 "Figure 14 ‣ Appendix F Details on Quantization Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")展示了与表[1](#S4.T1 "Table 1 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization")相比的最终配置。'
- en: 'Fig. [11](#A5.F11 "Figure 11 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the detailed nlp-eval scores of Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#A5.F11 "Figure 11 ‣ Appendix E Details on Search Tree, Sec. 4.3 ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")展示了表[1](#S4.T1 "Table 1 ‣ Quantization without outlier-handling.
    ‣ 4.3 Quantization: Outliers can be prevented ‣ 4 Token Metrics Improve Model
    Compression ‣ Divergent Token Metrics: Measuring degradation to prune away LLM
    components – and optimize quantization")的详细nlp-eval评分。'
- en: In total the entire search evaluation required 16 GPU-days with A100s to complete
    all metrics.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，整个搜索评估需要16个A100 GPU天才能完成所有指标。
- en: '![Refer to caption](img/4b358896a4a9240e0998b4b1be02b8f3.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4b358896a4a9240e0998b4b1be02b8f3.png)'
- en: (a) The 150 selected components selected by metrics for 8-bit AbsMax conversion.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 通过指标选择的150个用于8位AbsMax转换的组件。
- en: '![Refer to caption](img/00ff895da68a5c176586c27257f6301c.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/00ff895da68a5c176586c27257f6301c.png)'
- en: (b) The 16 selected components selected by metrics for 4-bit GPTQ conversion.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 通过指标选择的16个用于4位GPTQ转换的组件。
- en: 'Figure 14: Detailed view of the Llama2-7b components in Tab. [1](#S4.T1 "Table
    1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers can be
    prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token Metrics:
    Measuring degradation to prune away LLM components – and optimize quantization")
    selected by metrics for lower precision conversion.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：Tab. [1](#S4.T1 "表 1 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣ 4 令牌度量改善模型压缩 ‣ 发散令牌度量：测量退化以剪除
    LLM 组件 - 并优化量化") 中 Llama2-7b 组件的详细视图，按指标选择进行较低精度转换。
- en: '![Refer to caption](img/0e83a01caf47640279447c6dbb4f56cc.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0e83a01caf47640279447c6dbb4f56cc.png)'
- en: 'Figure 15: Full view of the influence of individual component-wise quantization
    measured by FDT.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：FDT 测量的单个组件量化影响的全视图。
- en: 'Appendix G Details on Sparsification, Sec. [4.2](#S4.SS2 "4.2 Sparsification
    reveals: Attention is not all you need! ‣ 4 Token Metrics Improve Model Compression
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization")'
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 稀疏化的详细信息，第[4.2](#S4.SS2 "4.2 稀疏化揭示：注意力并非全需！ ‣ 4 令牌度量改善模型压缩 ‣ 发散令牌度量：测量退化以剪除
    LLM 组件 - 并优化量化")节
- en: 'Fig. [16](#A7.F16 "Figure 16 ‣ Appendix G Details on Sparsification, Sec. 4.2
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows a different aggregated perspective of Fig. [4(b)](#S4.F4.sf2
    "In Figure 4 ‣ Divergent Token Parameters. ‣ 4.1 Experimental Protocol ‣ 4 Token
    Metrics Improve Model Compression ‣ Divergent Token Metrics: Measuring degradation
    to prune away LLM components – and optimize quantization"), to point out more
    direct the occuring variances.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16](#A7.F16 "图 16 ‣ 附录 G 稀疏化的详细信息，第 4.2 节 ‣ 发散令牌度量：测量退化以剪除 LLM 组件 - 并优化量化")
    显示了图[4(b)](#S4.F4.sf2 "在图 4 ‣ 发散令牌参数 ‣ 4.1 实验协议 ‣ 4 令牌度量改善模型压缩 ‣ 发散令牌度量：测量退化以剪除
    LLM 组件 - 并优化量化") 的不同汇总视角，以更直接地指出出现的方差。
- en: '![Refer to caption](img/f4cff68c84b4cb611c28f3f925b45baf.png)![Refer to caption](img/fa199f6333999f9bd65f08c1a5b93f81.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f4cff68c84b4cb611c28f3f925b45baf.png)![参考说明](img/fa199f6333999f9bd65f08c1a5b93f81.png)'
- en: 'Figure 16: Distribution of 75% average model sparsity. A. denotes Attention.
    Top: Aggregated by layers. The first and last layer have highest variance (MLP
    most important, c.f. Fig. [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Divergent Token Parameters.
    ‣ 4.1 Experimental Protocol ‣ 4 Token Metrics Improve Model Compression ‣ Divergent
    Token Metrics: Measuring degradation to prune away LLM components – and optimize
    quantization")). Second half reaches sparsities close component removal. Bottom:
    Per component aggregation. In the second half of layers, the importance of attention
    drops drastically. MLP almost remains, with outliers to larger importance.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：75% 平均模型稀疏性的分布。A. 表示注意力。上部：按层汇总。第一层和最后一层具有最高的方差（MLP 最重要，参见图[4(b)](#S4.F4.sf2
    "在图 4 ‣ 发散令牌参数 ‣ 4.1 实验协议 ‣ 4 令牌度量改善模型压缩 ‣ 发散令牌度量：测量退化以剪除 LLM 组件 - 并优化量化")）。第二部分接近组件移除的稀疏性。底部：每个组件汇总。在层的第二部分中，注意力的重要性急剧下降。MLP
    几乎保持不变，存在更大重要性的异常值。
- en: 'Fig. [17](#A7.F17 "Figure 17 ‣ Appendix G Details on Sparsification, Sec. 4.2
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the rank of lowest influence (measured by
    FDT) of components (x-axis) throughout various sparsity levels (y-axis). I.e.
    starting with a uniformly pruned model in 5% steps, we measured the rank when
    adding an additional 2.5% only to a single component. Interestingly, components
    seem to retain their importance throughout the various levels of sparsity.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图[17](#A7.F17 "图 17 ‣ 附录 G 稀疏化的详细信息，第 4.2 节 ‣ 发散令牌度量：测量退化以剪除 LLM 组件 - 并优化量化")
    显示了各个稀疏性水平（y 轴）下组件的最低影响等级（通过 FDT 测量）（x 轴）。即，从一个均匀剪枝的模型开始，每次增加 2.5% 只针对单一组件，我们测量了其排名。有趣的是，组件似乎在不同的稀疏性水平下保持了其重要性。
- en: 'Fig. [12](#A5.F12 "Figure 12 ‣ Appendix E Details on Search Tree, Sec. 4.3
    ‣ Divergent Token Metrics: Measuring degradation to prune away LLM components
    – and optimize quantization") shows the detailed nlp-eval scores of Tab. [1](#S4.T1
    "Table 1 ‣ Quantization without outlier-handling. ‣ 4.3 Quantization: Outliers
    can be prevented ‣ 4 Token Metrics Improve Model Compression ‣ Divergent Token
    Metrics: Measuring degradation to prune away LLM components – and optimize quantization").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图[12](#A5.F12 "图 12 ‣ 附录 E 关于搜索树的详细信息，第 4.3 节 ‣ 发散标记度量：测量退化以修剪 LLM 组件 - 以及优化量化")
    显示了表[1](#S4.T1 "表 1 ‣ 无异常值处理的量化 ‣ 4.3 量化：可以防止异常值 ‣ 4 标记度量改进模型压缩 ‣ 发散标记度量：测量退化以修剪
    LLM 组件 - 以及优化量化") 的详细 nlp-eval 评分。
- en: Note that, despite being often close in relative sparsity, the total number
    of parameters pruned for MLP is significantly larger than for Attention matrices
    (ratio 3:1).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管相对稀疏度通常接近，但 MLP 的总参数剪枝数量显著大于注意力矩阵（比例 3:1）。
- en: In total one sparsification training required 32 GPU-days with A100s for our
    experiment, and 29 GPU-days for uniform pruning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的实验中一次稀疏化训练需要 32 个 A100 GPU 天，而均匀剪枝需要 29 个 GPU 天。
- en: '![Refer to caption](img/9709d698129261b405730fbbae404925.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9709d698129261b405730fbbae404925.png)'
- en: 'Figure 17: Trends during sparsification. We plot the ranking of the components
    FDT value through various sparsity levels (y-axis) for all components (x-axis).
    Interestingly, there is a clear trend of components retaining “their importance”.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：稀疏化过程中的趋势。我们绘制了在不同稀疏度水平（y 轴）下所有组件的 FDT 值排名（x 轴）。有趣的是，组件保持“其重要性”的趋势非常明显。
