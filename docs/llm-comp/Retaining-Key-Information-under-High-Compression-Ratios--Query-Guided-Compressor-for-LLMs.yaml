- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:43'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:43'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留高压缩比下的关键信息：针对LLMs的查询引导压缩器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02376](https://ar5iv.labs.arxiv.org/html/2406.02376)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02376](https://ar5iv.labs.arxiv.org/html/2406.02376)
- en: Zhiwei Cao^(1,3)¹¹1These authors contributed equally. This work was done when
    Zhiwei Cao was interning at ByteDance.,  Qian Cao²¹¹1These authors contributed
    equally. This work was done when Zhiwei Cao was interning at ByteDance.,  Yu Lu²,  Ningxin
    Peng²,  Luyang Huang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 曹志伟^(1,3)¹¹1这些作者对本文贡献相同。这项工作是在曹志伟实习于字节跳动时完成的。，  曹倩²¹¹1这些作者对本文贡献相同。这项工作是在曹志伟实习于字节跳动时完成的。，  吕瑜²，  彭宁欣²，  黄璐扬²
- en: Shanbo Cheng²²²2Corresponding author.  and  Jinsong Su^(1,3)²²2Corresponding
    author.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 陈山博²²²2通讯作者。  和  苏锦松^(1,3)²²2通讯作者。
- en: ¹School of Informatics, Xiamen University   ²ByteDance Research
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹信息学院，厦门大学   ²字节跳动研究院
- en: ³Shanghai Artificial Intelligence Laboratory
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海人工智能实验室
- en: lines1@stu.xmu.edu.cn  {caoqian.95, luyu.ly, chengshanbo}@bytedance.com  jssu@xmu.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: lines1@stu.xmu.edu.cn  {caoqian.95, luyu.ly, chengshanbo}@bytedance.com  jssu@xmu.edu.cn
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The growing popularity of Large Language Models has sparked interest in context
    compression for Large Language Models (LLMs). However, the performance of previous
    methods degrades dramatically as compression ratios increase, sometimes even falling
    to the closed-book level. This decline can be attributed to the loss of key information
    during the compression process. Our preliminary study supports this hypothesis,
    emphasizing the significance of retaining key information to maintain model performance
    under high compression ratios. As a result, we introduce Query-Guided Compressor
    (QGC), which leverages queries to guide the context compression process, effectively
    preserving key information within the compressed context. Additionally, we employ
    a dynamic compression strategy. We validate the effectiveness of our proposed
    QGC on the Question Answering task, including NaturalQuestions, TriviaQA, and
    HotpotQA datasets. Experimental results show that QGC can consistently perform
    well even at high compression ratios, which also offers significant benefits in
    terms of inference cost and throughput¹¹1Our code is available at [https://github.com/DeepLearnXMU/QGC](https://github.com/DeepLearnXMU/QGC)..
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的日益流行引发了对大型语言模型（LLMs）上下文压缩的兴趣。然而，之前的方法在压缩比增加时性能显著下降，有时甚至降到封闭书本的水平。这种下降可归因于压缩过程中的关键信息丢失。我们的初步研究支持这一假设，强调了在高压缩比下保留关键信息以保持模型性能的重要性。因此，我们引入了查询引导压缩器（QGC），利用查询引导上下文压缩过程，有效地保留了压缩上下文中的关键信息。此外，我们采用了动态压缩策略。我们在问题回答任务中验证了我们提出的QGC的有效性，包括NaturalQuestions、TriviaQA和HotpotQA数据集。实验结果表明，QGC即使在高压缩比下也能始终表现良好，并且在推理成本和吞吐量方面也具有显著的好处¹¹1我们的代码可在[https://github.com/DeepLearnXMU/QGC](https://github.com/DeepLearnXMU/QGC)上获得。.
- en: 'Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 保留高压缩比下的关键信息：针对LLMs的查询引导压缩器
- en: Zhiwei Cao^(1,3)¹¹1These authors contributed equally. This work was done when
    Zhiwei Cao was interning at ByteDance.,  Qian Cao²¹¹1These authors contributed
    equally. This work was done when Zhiwei Cao was interning at ByteDance.,  Yu Lu²,  Ningxin
    Peng²,  Luyang Huang² Shanbo Cheng²²²2Corresponding author.  and  Jinsong Su^(1,3)²²2Corresponding
    author. ¹School of Informatics, Xiamen University   ²ByteDance Research ³Shanghai
    Artificial Intelligence Laboratory lines1@stu.xmu.edu.cn  {caoqian.95, luyu.ly,
    chengshanbo}@bytedance.com  jssu@xmu.edu.cn
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 曹志伟^(1,3)¹¹1这些作者对本文贡献相同。这项工作是在曹志伟实习于字节跳动时完成的。，  曹倩²¹¹1这些作者对本文贡献相同。这项工作是在曹志伟实习于字节跳动时完成的。，  吕瑜²，  彭宁欣²，  黄璐扬²
    陈山博²²²2通讯作者。  和  苏锦松^(1,3)²²2通讯作者。¹信息学院，厦门大学   ²字节跳动研究院 ³上海人工智能实验室 lines1@stu.xmu.edu.cn  {caoqian.95,
    luyu.ly, chengshanbo}@bytedance.com  jssu@xmu.edu.cn
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: The emergence of chatGPT Ouyang et al. ([2022](#bib.bib24)) and GPT4 OpenAI
    ([2023](#bib.bib23)), along with other Large Language Models (LLMs) Touvron et al.
    ([2023a](#bib.bib28), [b](#bib.bib29)) has sparked a global sensation. The success
    of LLMs is closely tied to the long context capabilities of LLMs Dong et al. ([2022](#bib.bib6));
    Lewis et al. ([2020](#bib.bib15)), especially in the field of multi-document question
    answering. However, the utilization of long context also introduces challenges
    such as higher inference cost, longer latency, and inferior performance caused
    by redundant information Jiang et al. ([2023](#bib.bib13)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: chatGPT Ouyang 等（[2022](#bib.bib24)）和 GPT4 OpenAI（[2023](#bib.bib23)）的出现，以及其他大型语言模型（LLMs）Touvron
    等（[2023a](#bib.bib28), [b](#bib.bib29)）引发了全球轰动。LLMs 的成功与其长上下文能力密切相关 Dong 等（[2022](#bib.bib6)）；Lewis
    等（[2020](#bib.bib15)），特别是在多文档问答领域。然而，使用长上下文也带来了挑战，如更高的推理成本、更长的延迟和由冗余信息引起的较差性能
    Jiang 等（[2023](#bib.bib13)）。
- en: 'Many efforts have been made to compress the long context by directly removing
    a certain percentage of less important words, such as LongLLMLingua Jiang et al.
    ([2023](#bib.bib13)) and Selective-Context Li et al. ([2023](#bib.bib16)). Another
    common method is to generate a text summary of the given context Xu et al. ([2023](#bib.bib34));
    Wang et al. ([2023b](#bib.bib31)). Unlike deleting or reordering the word in the
    context, AutoCompressor Chevalier et al. ([2023](#bib.bib4)) compresses long documents
    into multiple vectors as soft prompts, which are optimized with full parameters
    of LLMs. However, our preliminary study shows that these methods have a common
    flaw: as the compression ratio increases, the compressed context fails to retain
    key information, resulting in a significant decrease in the performance of LLMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多努力已经被投入到通过直接删除一定比例的较不重要单词来压缩长上下文中，例如 LongLLMLingua Jiang 等（[2023](#bib.bib13)）和
    Selective-Context Li 等（[2023](#bib.bib16)）。另一种常见方法是生成给定上下文的文本摘要 Xu 等（[2023](#bib.bib34)）；Wang
    等（[2023b](#bib.bib31)）。与删除或重新排列上下文中的单词不同，AutoCompressor Chevalier 等（[2023](#bib.bib4)）将长文档压缩成多个向量作为软提示，并通过
    LLM 的全参数进行优化。然而，我们的初步研究显示这些方法有一个共同的缺陷：随着压缩比的增加，压缩后的上下文无法保留关键信息，导致 LLM 的性能显著下降。
- en: The key to solve this problem is query, which defines what key information is.
    We aim to preserve this query-related key information even at a high compression
    ratio. Specifically, we propose the Query-Guided Compressor (QGC) to fully utilize
    query information throughout each compression step. We first feed the query and
    the documents together into a context encoder to learn the query-guide document
    representations. We then compress these document representations into $n$-gram
    structure rather than deleting words.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的关键是查询，它定义了什么是关键信息。我们的目标是即使在高压缩比下也能保留与查询相关的关键信息。具体来说，我们提出了 Query-Guided
    Compressor（QGC），以充分利用每个压缩步骤中的查询信息。我们首先将查询和文档一起输入上下文编码器，以学习查询引导的文档表示。然后，我们将这些文档表示压缩成
    $n$-gram 结构，而不是删除单词。
- en: 'We validate the effectiveness of QGC on the multi-document Question Answering
    task, including three datasets: NaturalQuestions, TriviaQA, and HotpotQA. Experimental
    results on the QA task indicate that, compared to LongLLMLingua, QGC exhibits
    a 2.75 times higher compression ratio and a 2.42 times higher throughput. Additionally,
    its accuracy has improved by an average of 5 points. We further investigated the
    loss of key information throughout the compression process. The findings reveal
    that under high compression ratios and high noise conditions, QGC only incurs
    a performance loss of about 10%, while LongLLMLingua suffers a loss of approximately
    47%. This validates the effectiveness of QGC in retaining key information.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们验证了 QGC 在多文档问答任务中的有效性，包括三个数据集：NaturalQuestions、TriviaQA 和 HotpotQA。问答任务的实验结果表明，与
    LongLLMLingua 相比，QGC 的压缩比高出 2.75 倍，吞吐量高出 2.42 倍。此外，其准确率平均提高了 5 分。我们进一步调查了压缩过程中关键信息的丢失情况。研究结果显示，在高压缩比和高噪声条件下，QGC
    的性能损失仅约 10%，而 LongLLMLingua 的损失约为 47%。这验证了 QGC 在保留关键信息方面的有效性。
- en: 2 Preliminary Study
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步研究
- en: In this section, we first briefly formulate the long context compression on
    the Question Answering task, and then present an analysis on the key information
    loss in previous compression methods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先简要表述了问答任务中的长上下文压缩，然后对以前的压缩方法中的关键信息丢失进行分析。
- en: 2.1 Task Formulation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 任务表述
- en: 'Given a LLM input with augmented context $\mathbf{x}=(\mathbf{x}^{ins},\mathbf{x}^{d_{1}},...,\mathbf{x}^{d_{k}},...,\mathbf{x}^{d_{K}},\mathbf{x}^{q})$,
    the objective of context compression can be formulated as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个带有增强上下文$\mathbf{x}=(\mathbf{x}^{ins},\mathbf{x}^{d_{1}},...,\mathbf{x}^{d_{k}},...,\mathbf{x}^{d_{K}},\mathbf{x}^{q})$的LLM输入，上下文压缩的目标可以被表述为：
- en: '|  | $\displaystyle\min_{\mathbf{\widetilde{x}}}d(\text{LLM}(\mathbf{y}&#124;\mathbf{x}),\text{LLM}(\mathbf{\widetilde{y}}&#124;\mathbf{\widetilde{x}})),$
    |  | (1) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\mathbf{\widetilde{x}}}d(\text{LLM}(\mathbf{y}&#124;\mathbf{x}),\text{LLM}(\mathbf{\widetilde{y}}&#124;\mathbf{\widetilde{x}})),$
    |  | (1) |'
- en: where $\mathbf{y}$ retrieved documents that greatly determine the length of
    the input.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{y}$检索的文档极大地决定了输入的长度。
- en: '![Refer to caption](img/5fef6eccd73ba78e3b7bed439d247257.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5fef6eccd73ba78e3b7bed439d247257.png)'
- en: (a) Compression Ratio for LongLLMLingua
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LongLLMLingua的压缩比
- en: '![Refer to caption](img/3d475dea77626fdca0370f2dcf908fff.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d475dea77626fdca0370f2dcf908fff.png)'
- en: (b) Document Number for AutoCompressor
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (b) AutoCompressor的文档数量
- en: 'Figure 1: The accuracy of LongLLMLingua Jiang et al. ([2023](#bib.bib13)) and
    AutoCompressor Chevalier et al. ([2023](#bib.bib4)) with different compression
    ratios and number of documents on the NaturalQuestions test set, respectively.
    Closed-book denotes providing LLMs with the question only, and Oracle means using
    the question and corresponding ground-truth documents as the input of the LLM.
    “w/ answer” means adding the golden answer to the compressed context.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LongLLMLingua Jiang等人（[2023](#bib.bib13)）和AutoCompressor Chevalier等人（[2023](#bib.bib4)）在NaturalQuestions测试集上不同压缩比和文档数量的准确率。闭卷测试表示仅提供问题给LLM，而Oracle表示使用问题和对应的真实文档作为LLM的输入。“w/
    answer”表示将黄金答案添加到压缩的上下文中。
- en: 2.2 Key Information Loss in Compression
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 压缩中的关键信息丢失
- en: We study the effectiveness of two representative methods, LongLLMLingua Jiang
    et al. ([2023](#bib.bib13)) and AutoCompressor Chevalier et al. ([2023](#bib.bib4)).
    We conduct experiments on the NaturalQuestions dataset Liu et al. ([2023](#bib.bib17))
    and use accuracy as the evaluation metric, which judges whether any correct answers
    appear in the LLM prediction.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了两种代表性方法的有效性，LongLLMLingua Jiang等人（[2023](#bib.bib13)）和AutoCompressor Chevalier等人（[2023](#bib.bib4)）。我们在NaturalQuestions数据集 Liu等人（[2023](#bib.bib17)）上进行实验，并使用准确率作为评估指标，判断LLM预测中是否出现正确答案。
- en: 'For LongLLMLingua, we apply LLaMA-2-7B-Chat²²2https://ai.meta.com/llama/ as
    the small language model for compression, and use LongChat-13B-16K³³3https://huggingface.co/lmsys/longchat-13b-16k
    as the target LLM. We use the open-source AutoCompressor⁴⁴4https://github.com/princeton-nlp/AutoCompressors,
    which fine-tunes LLaMA-2-7B to compress context and generate answers. Here, we
    consider four settings:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LongLLMLingua，我们使用LLaMA-2-7B-Chat²²2https://ai.meta.com/llama/作为压缩的小型语言模型，并使用LongChat-13B-16K³³3https://huggingface.co/lmsys/longchat-13b-16k作为目标LLM。我们使用开源的AutoCompressor⁴⁴4https://github.com/princeton-nlp/AutoCompressors，它微调LLaMA-2-7B以压缩上下文并生成答案。在这里，我们考虑四种设置：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Closed-book. It takes the query as the LLM input with no additional documents.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 闭卷测试。它将查询作为LLM输入，不添加其他文档。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Oracle. The query and only the document containing the ground truth are used
    as inputs to the LLM.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Oracle。查询和仅包含真实信息的文档被用作LLM的输入。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Base. Based on Oracle, we compress the document directly with various compression
    ratios for LongLLMLingua. However, since AutoCompressor is set to compress documents
    to fixed length vectors, we change the compression ratio by adding external documents.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础。基于Oracle，我们直接用不同的压缩比压缩文档以用于LongLLMLingua。然而，由于AutoCompressor被设置为压缩文档为固定长度的向量，我们通过添加外部文档来改变压缩比。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Base w/ answer. We manually add key information to the compressed results by
    concatenating the answer with the compressed word sequence in LongLLMLingua. Note
    that this setting is impractical for AutoCompressor where the compressed results
    are vectors that cannot be changed directly.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础 w/ answer。我们通过将答案与LongLLMLingua中压缩的词序列拼接来手动添加关键信息到压缩结果中。注意，这种设置对于AutoCompressor来说是不实际的，因为压缩结果是无法直接改变的向量。
- en: 'From Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Task Formulation ‣ 2 Preliminary Study
    ‣ Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs"), we find that the performance of both methods degrades significantly
    with increasing compression ratios. As shown in Figure [1(a)](#S2.F1.sf1 "In Figure
    1 ‣ 2.1 Task Formulation ‣ 2 Preliminary Study ‣ Retaining Key Information under
    High Compression Ratios: Query-Guided Compressor for LLMs"), the performance of
    LongLLMLingua decreases by 47% as the compression ratio increases from 1.53x to
    3.44x. Even worse, the accuracy of LongLLMLingua at 3.44x compression ratio is
    equivalent to that of the closed-book setting. The same findings are illustrated
    in Figure [1(b)](#S2.F1.sf2 "In Figure 1 ‣ 2.1 Task Formulation ‣ 2 Preliminary
    Study ‣ Retaining Key Information under High Compression Ratios: Query-Guided
    Compressor for LLMs") for AutoCompressor.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [1](#S2.F1 "图 1 ‣ 2.1 任务定义 ‣ 2 初步研究 ‣ 高压缩比下保持关键信息：用于LLMs的查询引导压缩器")中，我们发现两种方法的性能随着压缩比的增加显著下降。如图 [1(a)](#S2.F1.sf1
    "图 1 ‣ 2.1 任务定义 ‣ 2 初步研究 ‣ 高压缩比下保持关键信息：用于LLMs的查询引导压缩器")所示，当压缩比从1.53x增加到3.44x时，LongLLMLingua的性能下降了47%。更糟糕的是，LongLLMLingua在3.44x压缩比下的准确率相当于闭卷设置。图 [1(b)](#S2.F1.sf2
    "图 1 ‣ 2.1 任务定义 ‣ 2 初步研究 ‣ 高压缩比下保持关键信息：用于LLMs的查询引导压缩器")中的AutoCompressor也展示了相同的发现。
- en: 'More importantly, we observe that adding key information to the compressed
    result can greatly alleviate the performance degradation that typically occurs
    at high compression ratios. Back to Figure [1(a)](#S2.F1.sf1 "In Figure 1 ‣ 2.1
    Task Formulation ‣ 2 Preliminary Study ‣ Retaining Key Information under High
    Compression Ratios: Query-Guided Compressor for LLMs"), the accuracy line fluctuates
    little as the compression ratio increases from 1.5x to 3.5x with the help of additional
    key information, which is a decrease of 3.87% compared to the former 47% with
    the loss of key information. These observations validate the need to preserve
    key information during compression, which motivates us to explore a better method
    to fully exploit query information for context compression.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，我们观察到将关键信息添加到压缩结果中可以大大缓解通常在高压缩比下发生的性能下降。回到图 [1(a)](#S2.F1.sf1 "图 1 ‣ 2.1
    任务定义 ‣ 2 初步研究 ‣ 高压缩比下保持关键信息：用于LLMs的查询引导压缩器")，在额外关键信息的帮助下，随着压缩比从1.5x增加到3.5x，准确率线的波动很小，与丢失关键信息时的47%相比下降了3.87%。这些观察结果验证了在压缩过程中保留关键信息的必要性，这激励我们探索一种更好的方法，以充分利用查询信息进行上下文压缩。
- en: 3 Query-Guided Compression
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 查询引导的压缩
- en: '![Refer to caption](img/3ac3a11a2218874d912c1e209581a176.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3ac3a11a2218874d912c1e209581a176.png)'
- en: 'Figure 2: The framework of our method.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们方法的框架
- en: 'As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3 Query-Guided Compression ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs"),
    we equip the LLM with the Query-Guided Compressor to compress long documents into
    a much shorter sequence of continuous representations, which are then concatenated
    with the corresponding instruction and query as the input for the LLM. In the
    following, we first introduce the architecture of Query-Guided Compressor and
    then its training objective. Then, we propose a dynamic compression strategy that
    assigns higher compression ratios for irrelevant documents to further improve
    the compressed representations.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S3.F2 "图 2 ‣ 3 查询引导的压缩 ‣ 高压缩比下保持关键信息：用于LLMs的查询引导压缩器")所示，我们为LLM配备了查询引导压缩器，将长文档压缩成更短的连续表示序列，然后将这些序列与相应的指令和查询连接在一起，作为LLM的输入。接下来，我们首先介绍查询引导压缩器的架构，然后是其训练目标。随后，我们提出了一种动态压缩策略，为无关文档分配更高的压缩比，以进一步改进压缩表示。
- en: 3.1 Compressor Architecture
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 压缩器架构
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ Semantic Alignment Layer ‣ 3.1 Compressor Architecture
    ‣ 3 Query-Guided Compression ‣ Retaining Key Information under High Compression
    Ratios: Query-Guided Compressor for LLMs") illustrates the basic architecture
    of our Query-Guided Compressor. Using the compressor, we adopt the following steps
    to produce compressed representations of each document: 1) learning the query-aware
    document representations; 2) compressing the document representations into $n$
    of the document for simplicity.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S3.F3 "图 3 ‣ 语义对齐层 ‣ 3.1 压缩器架构 ‣ 3 查询引导的压缩 ‣ 高压缩比下保留关键信息：面向 LLM 的查询引导压缩器")
    说明了我们查询引导压缩器的基本结构。使用压缩器，我们采取以下步骤来生成每个文档的压缩表示：1）学习查询感知的文档表示；2）将文档表示压缩成 $n$ 的文档以简化。
- en: Query-Guided Context Encoder
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询引导的上下文编码器
- en: At the first step, we feed the concatenation of the query $\mathbf{x}^{q}$ into
    query-aware context encoder to learn the representations of the query and the
    document.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步，我们将查询 $\mathbf{x}^{q}$ 的拼接输入到查询感知上下文编码器中，以学习查询和文档的表示。
- en: 'The encoder consists of two Transformer encoder layers. Formally, these representations
    can be obtained in the following way:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由两个 Transformer 编码层组成。正式地，这些表示可以通过以下方式获得：
- en: '|  | $\displaystyle[\mathbf{h}^{q};\mathbf{h}^{d}]=\text{ContextEncoder}([\mathbf{x}^{q};\mathbf{x}^{d}]).$
    |  | (2) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle[\mathbf{h}^{q};\mathbf{h}^{d}]=\text{ContextEncoder}([\mathbf{x}^{q};\mathbf{x}^{d}]).$
    |  | (2) |'
- en: Here, $\mathbf{h}^{q}$, respectively. By allowing the query and the document
    to see each other during encoding, we can facilitate the extraction of the key
    information relevant to the query in the document.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathbf{h}^{q}$，分别。通过在编码期间让查询和文档相互查看，我们可以促进提取文档中与查询相关的关键信息。
- en: Query-Guided Pooling Layer
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询引导的池化层
- en: In the next step, we split the entire document into several $n$-gram into a
    vector based on their correlation to the query.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将整个文档拆分为多个 $n$-gram 向量，基于它们与查询的相关性。
- en: 'To this end, document representations are organized as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，文档表示按如下方式组织：
- en: '|  | $\displaystyle\mathbf{h}^{d}$ |  | (3) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{h}^{d}$ |  | (3) |'
- en: '|  |  | $\displaystyle=[\mathbf{h}^{d}_{1:n},...,\mathbf{h}^{d}_{(j-1)\times
    n:j\times n},...,\mathbf{h}^{d}_{N_{d}-n+1:N_{d}}],$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=[\mathbf{h}^{d}_{1:n},...,\mathbf{h}^{d}_{(j-1)\times
    n:j\times n},...,\mathbf{h}^{d}_{N_{d}-n+1:N_{d}}],$ |  |'
- en: where $\mathbf{G}_{j}$-grams.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{G}_{j}$-grams。
- en: 'Then, we measure the weight of each token in $\mathbf{G}_{j}$ of query tokens:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们测量查询令牌 $\mathbf{G}_{j}$ 中每个令牌的权重：
- en: '|  | $\displaystyle\overline{h}^{q}$ |  | (4) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\overline{h}^{q}$ |  | (4) |'
- en: '|  | $\displaystyle w_{i,\mathbf{G}_{j}}$ |  | (5) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{i,\mathbf{G}_{j}}$ |  | (5) |'
- en: where $s(\cdot,\cdot)$-gram.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s(\cdot,\cdot)$-gram。
- en: 'Finally, we acquire the compressed $n$-gram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们获得压缩的 $n$-gram：
- en: '|  | $\displaystyle\hat{h}^{d}_{\mathbf{G}_{j}}=\sum_{i\in\mathbf{G}_{j}}w_{i,\mathbf{G}_{j}}\cdot
    h^{d}_{i}.$ |  | (6) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{h}^{d}_{\mathbf{G}_{j}}=\sum_{i\in\mathbf{G}_{j}}w_{i,\mathbf{G}_{j}}\cdot
    h^{d}_{i}.$ |  | (6) |'
- en: Query-Document Reviewing Layer
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 查询-文档回顾层
- en: To further prevent the key information loss in compression, we introduce a novel
    reviewing module to perfect the compressed $n$-gram representations by revising
    both the query and the document representations.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步防止压缩过程中关键信息的丢失，我们引入了一个新的回顾模块，通过修正查询和文档表示来完善压缩的 $n$-gram 表示。
- en: 'Concretely, this encoder consists of two Transformer encoder layers, which
    takes the query representations $\mathbf{h}^{q}$:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，这个编码器由两个 Transformer 编码层组成，它们接受查询表示 $\mathbf{h}^{q}$：
- en: '|  | $\displaystyle\widetilde{\mathbf{h}}^{d}=\text{ReviewingLayer}([\mathbf{h}^{q};\mathbf{h}^{d};\hat{\mathbf{h}}^{d}]).$
    |  | (7) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widetilde{\mathbf{h}}^{d}=\text{ReviewingLayer}([\mathbf{h}^{q};\mathbf{h}^{d};\hat{\mathbf{h}}^{d}]).$
    |  | (7) |'
- en: Semantic Alignment Layer
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语义对齐层
- en: 'Since $\widetilde{\mathbf{h}}^{d}$ can be formulated as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $\widetilde{\mathbf{h}}^{d}$ 可以如下公式表示：
- en: '|  | $\displaystyle\mathbf{e}^{d}=\textbf{W}\cdot\widetilde{\mathbf{h}}^{d}+\textbf{b},$
    |  | (8) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{e}^{d}=\textbf{W}\cdot\widetilde{\mathbf{h}}^{d}+\textbf{b},$
    |  | (8) |'
- en: where W and b are learnable parameters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 W 和 b 是可学习的参数。
- en: '![Refer to caption](img/d3059d8f89e7e0502e057686a9baafa0.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d3059d8f89e7e0502e057686a9baafa0.png)'
- en: 'Figure 3: The structure of QGC. The first three layers use query $q$ encoding,
    pooling, and reviewing respectively. The last layer aligns document representations
    into the target LLM embedding space.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：QGC 的结构。前三层分别使用查询 $q$ 编码、池化和回顾。最后一层将文档表示对齐到目标 LLM 嵌入空间。
- en: 3.2 Compressor Training
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 压缩器训练
- en: Unlike AutoCompressor Chevalier et al. ([2023](#bib.bib4)), we fix the parameter
    of the LLM and only fine-tune the compressor.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与 AutoCompressor Chevalier 等人 ([2023](#bib.bib4)) 不同，我们固定了 LLM 的参数，仅对压缩器进行微调。
- en: 'Through the above steps, each long document is compressed into a shorter sequence
    of continuous representations $\mathbf{e}^{d}$. To avoid missing the key information
    during compression, we define the training objective of the compressor in the
    following way:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述步骤，每个长文档被压缩成一个更短的连续表示序列 $\mathbf{e}^{d}$。为了避免在压缩过程中丢失关键信息，我们定义了压缩器的训练目标如下：
- en: '|  | $\displaystyle\mathcal{L}$ |  | (9) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}$ |  | (9) |'
- en: '|  |  | $\displaystyle=-\log{p(\mathbf{y}&#124;\widetilde{\mathbf{x}})}+\text{KL}[p(\mathbf{y}&#124;\mathbf{x})&#124;&#124;p(\mathbf{y}&#124;\widetilde{\mathbf{x}})],$
    |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\log{p(\mathbf{y}&#124;\widetilde{\mathbf{x}})}+\text{KL}[p(\mathbf{y}&#124;\mathbf{x})&#124;&#124;p(\mathbf{y}&#124;\widetilde{\mathbf{x}})],$
    |  |'
- en: where $\text{KL}[\cdot||\cdot]$ represents the Kullback–Leibler divergence.
    By introducing the KL loss, we encourage the LLM to generate the correct answer
    even with compressed representations as input.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\text{KL}[\cdot||\cdot]$ 表示 Kullback–Leibler 散度。通过引入 KL 损失，我们鼓励 LLM 即使在压缩表示作为输入时也能生成正确答案。
- en: 3.3 Dynamically Compressing Strategy
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 动态压缩策略
- en: 'Due to the different importance of retrieved documents, we propose to dynamically
    adjust the compression ratios for different retrieved documents. Specifically,
    we assign the $n$-th document based on the importance ranking:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于检索文档的重要性不同，我们建议动态调整不同检索文档的压缩比率。具体地，我们根据重要性排名分配第 $n$ 个文档：
- en: '|  | $\displaystyle n_{k}=\begin{cases}\min(2\cdot O_{k},16)&amp;S_{k}\geq\epsilon\\
    \infty&amp;S_{k}<\epsilon\end{cases},$ |  | (10) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle n_{k}=\begin{cases}\min(2\cdot O_{k},16)&amp;S_{k}\geq\epsilon\\
    \infty&amp;S_{k}<\epsilon\end{cases},$ |  | (10) |'
- en: where $S_{k}$, the corresponding document will be discarded.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{k}$ 对应的文档将被丢弃。
- en: '| Methods | NaturalQuestions | TriviaQA | HotpotQA |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Methods | NaturalQuestions | TriviaQA | HotpotQA |'
- en: '| Acc | CR | TP | EM | CR | TP | F1 | CR | TP |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Acc | CR | TP | EM | CR | TP | F1 | CR | TP |'
- en: '| LongChat-13B |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LongChat-13B |'
- en: '| Closed-book | 34.84 | - | - | 36.07 | - | - | 22.19 | - | - |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Closed-book | 34.84 | - | - | 36.07 | - | - | 22.19 | - | - |'
- en: '| Oracle | 83.05 | 59.2x | - | - | - | - | 60.61 | 42.2x | - |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Oracle | 83.05 | 59.2x | - | - | - | - | 60.61 | 42.2x | - |'
- en: '| Original Prompt | 53.11 | 1.0x | - | 48.70 | 1.0x | - | 44.76 | 1.0x | -
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Original Prompt | 53.11 | 1.0x | - | 48.70 | 1.0x | - | 44.76 | 1.0x | -
    |'
- en: '| Reranker-based Methods |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 基于重排的方法 |'
- en: '| Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)) | 60.75 | 4.1x |
    0.137 | 48.89 | 4.5x | 1.957 | 42.92 | 4.4x | 1.930 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Sentence-BERT Reimers 和 Gurevych ([2020](#bib.bib27)) | 60.75 | 4.1x | 0.137
    | 48.89 | 4.5x | 1.957 | 42.92 | 4.4x | 1.930 |'
- en: '| BGE-Reranker Xiao et al. ([2023](#bib.bib33)) | 64.33 | 4.1x | 0.138 | 47.71
    | 4.5x | 1.724 | 47.96 | 4.4x | 1.689 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| BGE-Reranker Xiao 等人 ([2023](#bib.bib33)) | 64.33 | 4.1x | 0.138 | 47.71
    | 4.5x | 1.724 | 47.96 | 4.4x | 1.689 |'
- en: '| Cond.PPL Jiang et al. ([2023](#bib.bib13)) | 65.91 | 4.1x | 0.128 | 52.48
    | 4.5x | 1.287 | 49.82 | 4.3x | 1.267 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Cond.PPL Jiang 等人 ([2023](#bib.bib13)) | 65.91 | 4.1x | 0.128 | 52.48 | 4.5x
    | 1.287 | 49.82 | 4.3x | 1.267 |'
- en: '| Compression-based Methods |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |'
- en: '| Selective-Context Li et al. ([2023](#bib.bib16)) | 35.44 | 2.5x | 0.077 |
    42.73 | 2.5x | 0.465 | 29.68 | 2.6x | 0.456 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Selective-Context Li 等人 ([2023](#bib.bib16)) | 35.44 | 2.5x | 0.077 | 42.73
    | 2.5x | 0.465 | 29.68 | 2.6x | 0.456 |'
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13))$\dagger$ | 66.70 | 3.9x |
    - | - | - | - | - | - | - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua Jiang 等人 ([2023](#bib.bib13))$\dagger$ | 66.70 | 3.9x | - |
    - | - | - | - | - | - |'
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13)) | 67.01 | 4.1x | 0.118 |
    51.51 | 3.7x | 0.724 | 45.43 | 3.8x | 0.683 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua Jiang 等人 ([2023](#bib.bib13)) | 67.01 | 4.1x | 0.118 | 51.51
    | 3.7x | 0.724 | 45.43 | 3.8x | 0.683 |'
- en: '| QGC | 69.19 | 15.2x | 0.356 | 57.72 | 7.9x | 1.832 | 52.12 | 8.8x | 1.849
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| QGC | 69.19 | 15.2x | 0.356 | 57.72 | 7.9x | 1.832 | 52.12 | 8.8x | 1.849
    |'
- en: '| LLaMA-2-7B |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B |'
- en: '| Closed-book | 32.35 | - | - | 30.70 | - | - | 10.54 | - | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| Closed-book | 32.35 | - | - | 30.70 | - | - | 10.54 | - | - |'
- en: '| Oracle | 73.45 | 59.2x | - | - | - | - | 57.68 | 42.2x | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Oracle | 73.45 | 59.2x | - | - | - | - | 57.68 | 42.2x | - |'
- en: '| Original Prompt | 27.53 | 1.0x | - | 49.47 | 1.0x | - | 44.24 | 1.0x | -
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Original Prompt | 27.53 | 1.0x | - | 49.47 | 1.0x | - | 44.24 | 1.0x | -
    |'
- en: '| Reranker-based Methods |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 基于重排的方法 |'
- en: '| Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)) | 24.26 | 4.1x |
    0.133 | 49.49 | 4.5x | 0.731 | 40.65 | 4.4x | 0.752 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Sentence-BERT Reimers 和 Gurevych ([2020](#bib.bib27)) | 24.26 | 4.1x | 0.133
    | 49.49 | 4.5x | 0.731 | 40.65 | 4.4x | 0.752 |'
- en: '| BGE-Reranker Xiao et al. ([2023](#bib.bib33)) | 25.08 | 4.1x | 0.130 | 48.69
    | 4.5x | 0.683 | 46.13 | 4.4x | 0.724 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| BGE-Reranker Xiao 等人 ([2023](#bib.bib33)) | 25.08 | 4.1x | 0.130 | 48.69
    | 4.5x | 0.683 | 46.13 | 4.4x | 0.724 |'
- en: '| Cond.PPL Jiang et al. ([2023](#bib.bib13)) | 27.87 | 4.1x | 0.123 | 52.76
    | 4.5x | 0.602 | 47.84 | 4.3x | 0.623 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Cond.PPL Jiang 等 ([2023](#bib.bib13)) | 27.87 | 4.1x | 0.123 | 52.76 | 4.5x
    | 0.602 | 47.84 | 4.3x | 0.623 |'
- en: '| Compression-based Methods |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 基于压缩的方法 |'
- en: '| Selective-Context Li et al. ([2023](#bib.bib16)) | 31.79 | 2.6x | 0.082 |
    48.55 | 2.5x | 0.303 | 28.21 | 2.6x | 0.332 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Selective-Context Li 等 ([2023](#bib.bib16)) | 31.79 | 2.6x | 0.082 | 48.55
    | 2.5x | 0.303 | 28.21 | 2.6x | 0.332 |'
- en: '| LongLLMLingua Jiang et al. ([2023](#bib.bib13)) | 41.13 | 4.1x | 0.108 |
    50.44 | 3.7x | 0.432 | 39.87 | 3.8x | 0.438 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua Jiang 等 ([2023](#bib.bib13)) | 41.13 | 4.1x | 0.108 | 50.44
    | 3.7x | 0.432 | 39.87 | 3.8x | 0.438 |'
- en: '| AutoCompressor Chevalier et al. ([2023](#bib.bib4)) | 49.23 | 13.9x | 0.302
    | 29.17 | 8.7x | 0.823 | 29.02 | 8.1x | 0.833 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AutoCompressor Chevalier 等 ([2023](#bib.bib4)) | 49.23 | 13.9x | 0.302 |
    29.17 | 8.7x | 0.823 | 29.02 | 8.1x | 0.833 |'
- en: '| ICAE Ge et al. ([2023](#bib.bib7)) | 53.34 | 21.5x | - | 48.91 | 10.2x |
    - | 34.50 | 9.5x | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ICAE Ge 等 ([2023](#bib.bib7)) | 53.34 | 21.5x | - | 48.91 | 10.2x | - | 34.50
    | 9.5x | - |'
- en: '| QGC | 60.90 | 15.2x | 0.313 | 57.46 | 7.9x | 0.902 | 51.64 | 8.8x | 0.927
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| QGC | 60.90 | 15.2x | 0.313 | 57.46 | 7.9x | 0.902 | 51.64 | 8.8x | 0.927
    |'
- en: '| QGC($\epsilon=0.42$) | 57.62 | 20.6x | - | 57.11 | 10.9x | - | 51.23 | 12.1x
    | - |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| QGC($\epsilon=0.42$) | 57.62 | 20.6x | - | 57.11 | 10.9x | - | 51.23 | 12.1x
    | - |'
- en: 'Table 1: Experimental results on three benchmark datasets. Acc = accuracy,
    EM = exact match, F1 = F1 score, CR = compression ratio, TP = throughput (examples/second).
    Closed-book, Oracle, and Original Prompt denote using the query only, the complete
    ground-truth documents, and all retrieved documents as inputs, respectively. $\dagger$
    indicates that the results are directly cited from Jiang et al. ([2023](#bib.bib13)).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在三个基准数据集上的实验结果。Acc = 准确率，EM = 精确匹配，F1 = F1 分数，CR = 压缩比，TP = 吞吐量（例子/秒）。Closed-book、Oracle
    和 Original Prompt 分别表示仅使用查询、完整的真实文档和所有检索到的文档作为输入。$\dagger$ 表示结果直接引用自 Jiang 等 ([2023](#bib.bib13))。
- en: 4 Experiments
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we conduct extensive experiments to investigate the effectiveness
    of QGC.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行大量实验以调查 QGC 的有效性。
- en: Datasets & Evaluation Metric
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集与评估指标
- en: 'The experiments are carried out based on the three datasets:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 实验是基于三个数据集进行的：
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NaturalQuestions We select the processed version Liu et al. ([2023](#bib.bib17))
    where each question has 20 related documents and only one of them contains the
    correct answer. We follow Liu et al. ([2023](#bib.bib17)) to use accuracy (Acc)
    as the evaluation metric, which judges whether the correct answer appears in the
    prediction.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NaturalQuestions 我们选择了处理过的版本 Liu 等 ([2023](#bib.bib17))，其中每个问题有 20 个相关文档，其中只有一个包含正确答案。我们按照
    Liu 等 ([2023](#bib.bib17)) 的方法使用准确率（Acc）作为评估指标，以判断正确答案是否出现在预测中。
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: TriviaQA We employ the adversarial Contriever Izacard et al. ([2022a](#bib.bib10))
    to retrieve the top 10 documents from all Wikipedia passages. Following Lewis
    et al. ([2020](#bib.bib15)), we use the Exact Match (EM) metric to evaluate the
    LLM prediction.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TriviaQA 我们使用对抗性 Contriever Izacard 等 ([2022a](#bib.bib10)) 从所有维基百科文档中检索前 10
    个文档。按照 Lewis 等 ([2020](#bib.bib15)) 的方法，我们使用精确匹配（EM）指标来评估 LLM 的预测。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: HotpotQA Different from the above two datasets, HotpotQA [Yang et al.](#bib.bib35)
    is a multi-hop dataset where the answer lies in more than one document. Specifically,
    each question has 10 related documents and two of them are ground-truth documents.
    Following [Yang et al.](#bib.bib35) , we use the F1 score to measure the correctness
    of the LLM.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HotpotQA 与上述两个数据集不同，HotpotQA [Yang 等](#bib.bib35) 是一个多跳数据集，其中答案分布在多个文档中。具体而言，每个问题有
    10 个相关文档，其中两个是真实文档。按照 [Yang 等](#bib.bib35) 的方法，我们使用 F1 分数来衡量 LLM 的正确性。
- en: Besides, we calculate the compression ratio (CR) for different methods, which
    is defined as the length rate of the original context to the compressed context.
    We also provide the inference throughput (TP) on a single A100-80G GPU, including
    compression and generation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们计算了不同方法的压缩比（CR），其定义为原始上下文与压缩上下文的长度比。我们还提供了在单个 A100-80G GPU 上的推理吞吐量（TP），包括压缩和生成。
- en: Baselines
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准方法
- en: Following Jiang et al. ([2023](#bib.bib13)), we include two sets of methods
    as our baselines.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 Jiang 等 ([2023](#bib.bib13)) 的方法，我们将两组方法作为基准。
- en: '1) Reranker-based Methods. It simply uses a reranker method to sort documents
    based on importance and discards unimportant ones. We select the following reranker:
    Sentence-BERT Reimers and Gurevych ([2020](#bib.bib27)), BGE-Reranker Xiao et al.
    ([2023](#bib.bib33)), and Cond.PPL proposed by Jiang et al. ([2023](#bib.bib13))
    to measure the association between the query and documents. Then, we discard documents
    with low association until the compression ratio is met and sort the remaining
    documents according to the association from high to low.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基于重排序的方法。它简单地使用重排序方法根据重要性对文档进行排序，并丢弃不重要的文档。我们选择以下重排序器：Sentence-BERT Reimers
    和 Gurevych ([2020](#bib.bib27))，BGE-Reranker Xiao 等人 ([2023](#bib.bib33))，以及 Jiang
    等人 ([2023](#bib.bib13)) 提出的 Cond.PPL 来测量查询与文档之间的关联。然后，我们丢弃关联度低的文档，直到达到压缩比例，并根据从高到低的关联度对剩余文档进行排序。
- en: '2) Compression-based Methods. Compared with reranker-based methods, they further
    compress the sorted documents, retaining more information while satisfying a higher
    compression ratio. We select the following methods as our baselines:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基于压缩的方法。与基于重排序的方法相比，它们进一步压缩排序后的文档，保留更多信息，同时满足更高的压缩比例。我们选择以下方法作为我们的基线：
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Selective-Context Li et al. ([2023](#bib.bib16)) It uses self-information estimated
    by an external language model to prune redundant words.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Selective-Context Li 等人 ([2023](#bib.bib16)) 使用由外部语言模型估计的自信息来修剪冗余词汇。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LongLLMLingua Jiang et al. ([2023](#bib.bib13)) It is the state-of-the-art method
    for long context compression. It first uses a language model to quantify the importance
    of each document as its question-aware perplexity, and then designs a question-aware
    coarse-to-fine compression method to delete unimportant tokens.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LongLLMLingua Jiang 等人 ([2023](#bib.bib13)) 是长上下文压缩的最先进方法。它首先使用语言模型量化每个文档的的重要性作为其问题感知困惑度，然后设计了一种问题感知的粗到细的压缩方法来删除不重要的标记。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AutoCompressor Chevalier et al. ([2023](#bib.bib4)) It fine-tunes LLaMA-2-7B
    to recursively compress long context into summary vectors, which are used as soft
    prompts to generate the answer. We use the released AutoCompressor-Llama-2-7B-6K
    for experiments.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AutoCompressor Chevalier 等人 ([2023](#bib.bib4)) 微调 LLaMA-2-7B 以递归地将长上下文压缩为摘要向量，这些向量用作软提示来生成答案。我们使用发布的
    AutoCompressor-Llama-2-7B-6K 进行实验。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ICAE Ge et al. ([2023](#bib.bib7)) Similar to AutoCompressor, it generates compact
    and informative memory slots to represent the original context. We use the released
    ICAE model pre-trained on Llama-2-7B-Chat for experiments ⁵⁵5https://github.com/getao/icae.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ICAE Ge 等人 ([2023](#bib.bib7)) 类似于 AutoCompressor，它生成紧凑且信息丰富的记忆槽以表示原始上下文。我们使用发布的
    ICAE 模型，该模型在 Llama-2-7B-Chat 上预训练进行实验 ⁵⁵5https://github.com/getao/icae。
- en: Implementation Details
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实施细节
- en: We use LongChat-13B-16K and LLaMA-2-7B as the LLMs for evaluation, which are
    frozen during the optimization of QGC. To ensure stable and reproducible results,
    we employ greedy decoding and set the temperature to 0 in all experiments. Following Jiang
    et al. ([2023](#bib.bib13)), we use LLaMA-2-7B-Chat as the external language model
    for Selective-Context and LongLLMLingua. For QGC, both the query-guided context
    encoder and query-document reviewing layer consist of two Transformer encoder
    layers. All these layers and word embeddings are initialized with LLaMA-2-7B where
    MLP parameters are all fixed during training. Our rationale behind this approach
    stems from our belief that the MLP plays a crucial role in knowledge retention,
    while our focus lies in adjusting the acquired knowledge based on query. Thus,
    the trainable parameters in QGC are only 3.5% of LongChat-13B-16K. Besides the
    ground-truth document, we concatenate 1-4 random documents to build the long context.
    We also randomly set the n-gram size from the candidate list (4, 6, 8, 10) for
    each training batch to make the compressor more robust. We train QGC on downstream
    datasets for 15 epochs, using a learning rate of 5e-5 with the Adam optimizer
    and batch size of 64. During inference, we use the Cond.PPL proposed by Jiang
    et al. ([2023](#bib.bib13)) to sort retrieved documents for all compression-based
    methods and QGC, and set the $\epsilon$ as 0.35. Following Liu et al. ([2023](#bib.bib17));
    Bai et al. ([2023](#bib.bib1)) the maximum generation tokens is 100 for NaturalQuestions,
    and 32 for both TriviaQA and HotpotQA. All experiments are conducted on 8 NVIDIA
    A100 GPUs.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 LongChat-13B-16K 和 LLaMA-2-7B 作为 LLM 进行评估，这些模型在 QGC 的优化过程中保持冻结。为了确保结果的稳定性和可重复性，我们在所有实验中使用贪婪解码，并将温度设置为
    0。按照 Jiang 等人 ([2023](#bib.bib13)) 的方法，我们使用 LLaMA-2-7B-Chat 作为选择性上下文和 LongLLMLingua
    的外部语言模型。对于 QGC，查询引导上下文编码器和查询-文档审阅层均由两个 Transformer 编码器层组成。所有这些层和词嵌入都使用 LLaMA-2-7B
    进行初始化，其中 MLP 参数在训练过程中全部固定。我们选择这种方法的理由在于我们认为 MLP 在知识保留中扮演了关键角色，而我们关注的是根据查询调整获取的知识。因此，QGC
    中可训练的参数仅占 LongChat-13B-16K 的 3.5%。除了真实文档外，我们还将 1-4 个随机文档拼接以构建长上下文。我们还随机设置每个训练批次的
    n-gram 大小（4、6、8、10），以使压缩器更具鲁棒性。我们在下游数据集上训练 QGC 15 个周期，使用 5e-5 的学习率、Adam 优化器和 64
    的批量大小。在推理过程中，我们使用 Jiang 等人 ([2023](#bib.bib13)) 提出的 Cond.PPL 对所有基于压缩的方法和 QGC 的检索文档进行排序，并将
    $\epsilon$ 设置为 0.35。按照 Liu 等人 ([2023](#bib.bib17))；Bai 等人 ([2023](#bib.bib1))
    的方法，NaturalQuestions 的最大生成标记数为 100，TriviaQA 和 HotpotQA 都为 32。所有实验都在 8 个 NVIDIA
    A100 GPU 上进行。
- en: '| Methods | Accuracy |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确率 |'
- en: '| QGC | 69.19 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| QGC | 69.19 |'
- en: '| \cdashline1-2  w/o query-guided context encoder | 50.36 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline1-2  无查询引导上下文编码器 | 50.36 |'
- en: '| w/o query-guided pooling layer | 55.34 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 无查询引导池化层 | 55.34 |'
- en: '| w/o query-document reviewing layer | 64.14 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 无查询-文档审阅层 | 64.14 |'
- en: '| w/o dynamically compressing strategy | 62.15 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 无动态压缩策略 | 62.15 |'
- en: 'Table 2: The accuracy of ablation study on NaturalQuestions test set, where
    the target LLM is LongChat-13B.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 NaturalQuestions 测试集上的消融研究准确率，其中目标 LLM 为 LongChat-13B。
- en: Main Results
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.3 Dynamically Compressing Strategy ‣ 3 Query-Guided
    Compression ‣ Retaining Key Information under High Compression Ratios: Query-Guided
    Compressor for LLMs") reports the performance, compression ratios, and throughput
    of various methods or models on different datasets. Overall, QGC achieves higher
    compression ratios and greater throughput while achieving comparable or even better
    performance with LongLLMLingua. These results demonstrate that QGC can effectively
    compress context into shorter inputs.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "表 1 ‣ 3.3 动态压缩策略 ‣ 3 查询引导压缩 ‣ 高压缩比下保留关键信息：用于LLMs的查询引导压缩器") 报告了各种方法或模型在不同数据集上的性能、压缩比和吞吐量。总体来看，QGC
    在实现与 LongLLMLingua 相当甚至更好的性能的同时，达到了更高的压缩比和更大的吞吐量。这些结果证明了 QGC 可以有效地将上下文压缩为更短的输入。
- en: Specifically, the performance and compression ratio of the reranker-based methods
    are limited because no compression operation is used within the document. Compared
    to AutoCompressor and ICAE, our method achieves better accuracy with comparable
    compression ratios. Compared with LongLLMLingua, QGC achieves average +5.03 and
    +12.87 performance improvements when using LongChat-13B and LLaMA-2-7B as the
    target LLMs. On average, the compression ratio and throughput of QGC are 2.75
    times and 2.47 times that of LongLLMLingua on all datasets and target LLMs, respectively.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，基于重新排序器的方法的性能和压缩比有限，因为文档中未使用任何压缩操作。与 AutoCompressor 和 ICAE 相比，我们的方法在相似的压缩比下取得了更好的准确性。与
    LongLLMLingua 相比，QGC 在使用 LongChat-13B 和 LLaMA-2-7B 作为目标 LLM 时，平均性能提高了 +5.03 和
    +12.87。平均而言，QGC 的压缩比和吞吐量分别是 LongLLMLingua 的 2.75 倍和 2.47 倍。 '
- en: Ablation Study
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究
- en: '![Refer to caption](img/0d90c53319b9317e49db89bc0223dff1.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0d90c53319b9317e49db89bc0223dff1.png)'
- en: (a) Compression Ratio for QGC
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (a) QGC 的压缩比
- en: '![Refer to caption](img/4dd0a41d7abf596bc452ffa7edf3df93.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4dd0a41d7abf596bc452ffa7edf3df93.png)'
- en: (b) Document Number for QGC
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (b) QGC 的文档数量
- en: 'Figure 4: The accuracy of QGC with varying compression ratios and number of
    documents, respectively.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：QGC 在不同压缩比和文档数量下的准确性。
- en: 'To explore the effect of different components on QGC, we use LongChat-13B as
    the target LLM and introduce the following variants of QGC for ablation study:
    1) w/o query-guided context encoder. In this variant, the query and document are
    independently encoded; 2) w/o query-guided pooling layer. When establishing this
    variant, we directly replace the weighted sum of token representations in each
    $n$-gram size as 4 for comparable comparison.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索不同组件对 QGC 的影响，我们使用 LongChat-13B 作为目标 LLM，并引入以下 QGC 变体进行消融研究：1) 无查询引导上下文编码器。在这个变体中，查询和文档被独立编码；2)
    无查询引导池化层。建立此变体时，我们直接用每个 $n$-gram 大小为 4 的标记表示的加权和进行可比比较。
- en: 'As shown in Table [2](#S4.T2 "Table 2 ‣ Implementation Details ‣ 4 Experiments
    ‣ Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs"), the absence of the query-document reviewing layer and dynamically
    compressing strategy lead to a 5.05 and 7.04 accuracy loss respectively. The more
    substantial loss is observed after removing the query-guided context encoder and
    query-guided pooling layer, resulting in a significant performance accuracy drop
    of 18.83 and 13.85 respectively, highlighting the importance of employing the
    query to guide compression.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [2](#S4.T2 "表 2 ‣ 实施细节 ‣ 4 实验 ‣ 在高压缩比下保留关键信息：面向 LLM 的查询引导压缩器")所示，缺少查询-文档审查层和动态压缩策略分别导致了5.05和7.04的准确性损失。移除查询引导上下文编码器和查询引导池化层后，观察到更显著的损失，准确性分别下降了18.83和13.85，突显了使用查询引导压缩的重要性。
- en: 5 Analysis
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 分析
- en: In this section, we conduct in-depth analyses to explore the performance of
    QGC in terms of key information loss, demonstration compression, detailed throughput
    and reranker impact. All analyses are conducted on NaturalQuestions with target
    LLM as LongChat-13B.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行深入分析，以探索 QGC 在关键信息丢失、演示压缩、详细吞吐量和重新排序器影响方面的表现。所有分析都在 NaturalQuestions
    上进行，目标 LLM 为 LongChat-13B。
- en: Key Information Loss in QGC
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: QGC 中的关键信息丢失
- en: 'As described in Section [2.2](#S2.SS2 "2.2 Key Information Loss in Compression
    ‣ 2 Preliminary Study ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"), previous methods dramatically lose key information
    as the compression ratio increases. For comparison, we experiment with QGC using
    the same setting.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 [2.2](#S2.SS2 "2.2 压缩中的关键信息丢失 ‣ 2 初步研究 ‣ 在高压缩比下保留关键信息：面向 LLM 的查询引导压缩器")节所述，以前的方法在压缩比增加时会剧烈丧失关键信息。为了进行比较，我们使用相同设置对
    QGC 进行实验。
- en: 'Compared to LongLLMLingua in Figure [4(a)](#S4.F4.sf1 "In Figure 4 ‣ Ablation
    Study ‣ 4 Experiments ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"), the performance of QGC only decreases 10%
    as the compression ratio increases from 1x to 4x, and is even comparable to that
    of LongLLMLingua containing the correct answer in the compressed result. As seen
    in Figure [4(b)](#S4.F4.sf2 "In Figure 4 ‣ Ablation Study ‣ 4 Experiments ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs"),
    we observe that the performance of QGC slightly degrades with more documents,
    which is only a 12% decrease with 4 documents (27% for AutoCompressor). These
    results demonstrate that QGC can effectively retain key information even in much
    longer context and higher compression ratio scenarios.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与图[4(a)](#S4.F4.sf1 "在图4中 ‣ 消融研究 ‣ 4 实验 ‣ 在高压缩比下保留关键信息：针对LLM的查询引导压缩器")中的LongLLMLingua相比，QGC的性能仅在压缩比从1x增加到4x时减少了10%，甚至与包含正确答案的LongLLMLingua在压缩结果中的表现相当。如图[4(b)](#S4.F4.sf2
    "在图4中 ‣ 消融研究 ‣ 4 实验 ‣ 在高压缩比下保留关键信息：针对LLM的查询引导压缩器")所示，我们观察到QGC在处理更多文档时性能略有下降，在4个文档的情况下仅减少了12%（AutoCompressor为27%）。这些结果表明QGC在更长的上下文和更高的压缩比场景中仍能有效保留关键信息。
- en: Demonstration Compression for In-Context Learning
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文学习的示例压缩
- en: '| Methods | SST-2 | GSM8K |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | SST-2 | GSM8K |'
- en: '| Acc | CR | Acc | CR |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 压缩率 | 准确率 | 压缩率 |'
- en: '| Original Prompt | 92.4 | 1.0x | 14.48 | 1.0x |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 原始提示 | 92.4 | 1.0x | 14.48 | 1.0x |'
- en: '| LongLLMLingua | - | - | 5.91 | 3.9x |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| LongLLMLingua | - | - | 5.91 | 3.9x |'
- en: '| AutoCompressor | 94.2 | 15.0x | 6.68 | 13.6x |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| AutoCompressor | 94.2 | 15.0x | 6.68 | 13.6x |'
- en: '| QGC | 94.8 | 23.3x | 14.18 | 13.4x |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| QGC | 94.8 | 23.3x | 14.18 | 13.4x |'
- en: 'Table 3: Experimental results on SST-2 and GSM8K datasets, where the target
    LLM is LLaMA-2-7B.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：SST-2和GSM8K数据集上的实验结果，其中目标LLM是LLaMA-2-7B。
- en: 'To further validate the effectiveness of QGC in a broader context, we conduct
    experiments on both SST-2 and GSM8K datasets. We adopt the approach of previous
    studies Chevalier et al. ([2023](#bib.bib4)); Wei et al. ([2022](#bib.bib32))
    which utilizing demonstrations as the document, while maintaining consistency
    with their experimental setup. The results in Table [3](#S5.T3 "Table 3 ‣ Demonstration
    Compression for In-Context Learning ‣ 5 Analysis ‣ Retaining Key Information under
    High Compression Ratios: Query-Guided Compressor for LLMs") reveals notable insights.
    On the SST-2 dataset, our method surpasses autocompressor in both compression
    ratio and accuracy. Meanwhile, on the GSM8K dataset, our accuracy performance
    remains on par with the original prompt at the same compression ratio as autocompressor.
    This suggests that QGC strikes an excellent balance between model performance
    and compression ratio. These results showcases QGC’s proficiency in preserving
    information from demonstrations and fostering the in-context learning capacity
    of the target LLM.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在更广泛的背景下进一步验证QGC的有效性，我们在SST-2和GSM8K数据集上进行了实验。我们采用了前人研究Chevalier等（[2023](#bib.bib4)）；Wei等（[2022](#bib.bib32)）的方法，利用示例作为文档，同时保持与他们实验设置的一致性。表[3](#S5.T3
    "表3 ‣ 示例压缩用于上下文学习 ‣ 5 分析 ‣ 在高压缩比下保留关键信息：针对LLM的查询引导压缩器")中的结果揭示了显著的洞见。在SST-2数据集中，我们的方法在压缩比和准确性方面均超过了AutoCompressor。与此同时，在GSM8K数据集中，我们的准确性表现与原始提示在相同压缩比下保持一致。这表明QGC在模型性能和压缩比之间达到了优良的平衡。这些结果展示了QGC在保留示例信息和提升目标LLM的上下文学习能力方面的熟练程度。
- en: Detailed Throughput Evaluation
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 详细吞吐量评估
- en: '![Refer to caption](img/1cf33d57ae66e16c4ad4b5853bb08eae.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1cf33d57ae66e16c4ad4b5853bb08eae.png)'
- en: 'Figure 5: The accuracy, compression throughput, and generation throughput of
    QGC and LongLLMLingua.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：QGC和LongLLMLingua的准确性、压缩吞吐量和生成吞吐量。
- en: To evaluate the throughput of various methods or models, encompassing both compression
    and generation, we perform testing on a single A100-80G GPU.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估各种方法或模型的吞吐量，包括压缩和生成，我们在单个A100-80G GPU上进行测试。
- en: 'The results presented in Figure [5](#S5.F5 "Figure 5 ‣ Detailed Throughput
    Evaluation ‣ 5 Analysis ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs") indicate that QGC is obviously higher than
    LongLLMLingua in both compression throughput and generation throughput. Moreover,
    by adjusting the hyper-parameter $\epsilon$ (See Equation [10](#S3.E10 "In 3.3
    Dynamically Compressing Strategy ‣ 3 Query-Guided Compression ‣ Retaining Key
    Information under High Compression Ratios: Query-Guided Compressor for LLMs"))
    to increase the compression ratio, QGC can achieve a higher compression ratio
    while minimizing the impact on LLM performance and further improving throughput.
    Furthermore, our higher compression ratios lead to shorter LLM input, which also
    significantly improves the generation throughput of the target LLM. As for LongLLMLingua,
    since it additionally introduces LLaMA-2-7B for compression, the compression throughput
    is significantly lower than ours. Besides, although LongLLMLingua can also improve
    compression ratio by adjusting hyper-parameters, its performance will significantly
    drop, while QGC still maintains excellent performance.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](#S5.F5 "Figure 5 ‣ Detailed Throughput Evaluation ‣ 5 Analysis ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs")显示，QGC在压缩吞吐量和生成吞吐量上明显高于LongLLMLingua。此外，通过调整超参数$\epsilon$（参见方程 [10](#S3.E10
    "In 3.3 Dynamically Compressing Strategy ‣ 3 Query-Guided Compression ‣ Retaining
    Key Information under High Compression Ratios: Query-Guided Compressor for LLMs")）以增加压缩比，QGC可以在最小化对LLM性能影响的同时实现更高的压缩比，并进一步提高吞吐量。此外，我们更高的压缩比导致更短的LLM输入，这也显著提高了目标LLM的生成吞吐量。至于LongLLMLingua，由于它额外引入了LLaMA-2-7B进行压缩，因此压缩吞吐量显著低于我们的。此外，尽管LongLLMLingua也可以通过调整超参数提高压缩比，但其性能会显著下降，而QGC仍保持出色的性能。'
- en: Impact of Different Rerankers
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同重排序器的影响
- en: The compression ratio for each document is determined by the corresponding correlation
    with the query obtained by a reranker. Here, we analyze the impact of using different
    rerankers in this process. In addition to the three methods introduced in reranker-based
    methods, we also include BM25 and Gzip Jiang et al. ([b](#bib.bib14)) for comparison.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档的压缩比由重排序器获得的与查询的相关性决定。在此，我们分析了使用不同重排序器的影响。除了重排序器方法中介绍的三种方法外，我们还包括BM25和Gzip Jiang
    et al. ([b](#bib.bib14))进行比较。
- en: 'Experimental results are shown in Figure [6](#S5.F6 "Figure 6 ‣ Impact of Different
    Rerankers ‣ 5 Analysis ‣ Retaining Key Information under High Compression Ratios:
    Query-Guided Compressor for LLMs"). It can be found that QGC performs better with
    more competitive rerankers. Besides, compared with directly using rerankers for
    compression, QGC not only achieves an average 2.65 times higher compression ratio
    but also maintains lossless or even improved performance.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果见图 [6](#S5.F6 "Figure 6 ‣ Impact of Different Rerankers ‣ 5 Analysis ‣
    Retaining Key Information under High Compression Ratios: Query-Guided Compressor
    for LLMs")。可以发现，QGC在使用更具竞争力的重排序器时表现更好。此外，与直接使用重排序器进行压缩相比，QGC不仅实现了平均2.65倍的更高压缩比，而且保持了无损甚至改进的性能。'
- en: '![Refer to caption](img/22725b145ad8883a14f68cd817e5e9f2.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22725b145ad8883a14f68cd817e5e9f2.png)'
- en: 'Figure 6: The performance of QGC using different rerankers. “Base” represents
    the performance of each reranker to be used for compression. The performance (Recall)
    of rerankers: Cond.PPL > BGE-Rererank > SBERT (Sentence-BERT) > Gzip > BM25.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：QGC在使用不同重排序器的性能。“Base”代表每个重排序器在压缩中的性能。重排序器的性能（召回率）：Cond.PPL > BGE-Rererank
    > SBERT (Sentence-BERT) > Gzip > BM25。
- en: 6 Related Work
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Long Context for LLMs
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM的长上下文
- en: Recently, there have been a lot of studies focusing on expanding the context
    length of LLMs Press et al. ([2021](#bib.bib26)); Peng et al. ([2023](#bib.bib25));
    Bertsch et al. ([2023](#bib.bib2)). Existing efforts primarily involve gradually
    increasing the window size during pre-training Nijkamp et al. ([2023](#bib.bib22)),
    interpolating position embeddings Chen et al. ([2023](#bib.bib3)), and modifying
    the attention mechanism Ding et al. ([2023](#bib.bib5)). Unlike these works, we
    do not directly aim to expand the context window of LLMs. Hence, the QGC that
    we proposed can complement these techniques by enabling LLMs to access a broader
    context with reduced cost and shorter latency.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，许多研究集中在扩展LLM的上下文长度上 Press et al. ([2021](#bib.bib26)); Peng et al. ([2023](#bib.bib25));
    Bertsch et al. ([2023](#bib.bib2))。现有的努力主要包括在预训练过程中逐渐增加窗口大小 Nijkamp et al. ([2023](#bib.bib22))、插值位置嵌入
    Chen et al. ([2023](#bib.bib3)) 和修改注意力机制 Ding et al. ([2023](#bib.bib5))。与这些工作不同，我们并不直接旨在扩展LLM的上下文窗口。因此，我们提出的QGC可以通过使LLM在降低成本和缩短延迟的情况下访问更广泛的上下文来补充这些技术。
- en: Retrieval-augmented LMs
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 检索增强的语言模型
- en: Combined with a standalone retriever to augment LMs are gaining popularity for
    benefiting various knowledge-intensive tasks. Previous studies have achieved remarkable
    results in improving perplexity Wang et al. ([2023a](#bib.bib30)), factual accuracy Nakano
    et al. ([2022](#bib.bib21)), downstream task performance Izacard et al. ([2022b](#bib.bib11)),
    and in-context learning Huang et al. ([2023](#bib.bib9)). Besides, many works
    focus on cooperating LLMs and retrieved documents, such as reranking retrieved
    documents [Mao et al.](#bib.bib19) and discarding irrelevant documents [Mallen
    et al.](#bib.bib18) . QGC is also a retrieval augmentation method for LLMs, which
    efficiently compresses the retrieved documents into shorter inputs while maintaining
    no significant performance degradation.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 结合独立检索器来增强语言模型（LMs）的方法正变得越来越受欢迎，这有助于各种知识密集型任务。之前的研究在降低困惑度 Wang et al. ([2023a](#bib.bib30))、提高事实准确性 Nakano
    et al. ([2022](#bib.bib21))、改善下游任务性能 Izacard et al. ([2022b](#bib.bib11)) 和上下文学习 Huang
    et al. ([2023](#bib.bib9)) 上取得了显著的成果。此外，许多工作集中在协作LLM和检索到的文档上，如对检索到的文档进行重新排序 [Mao
    et al.](#bib.bib19) 和丢弃不相关的文档 [Mallen et al.](#bib.bib18)。QGC也是一种用于LLM的检索增强方法，它能够高效地将检索到的文档压缩成更短的输入，同时保持性能没有显著下降。
- en: Context Compression
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文压缩
- en: 'With the growing context length in LLMs, the demand for higher efficiency,
    lower cost, and reduced latency has attracted much attention. As a promising solution,
    compression techniques can be broadly categorized into two types: black-box compression Xu
    et al. ([2023](#bib.bib34)) and white-box compression Wang et al. ([2023b](#bib.bib31)).
    Black-box compression primarily involves token pruning based on different importance
    measures, such as self-information Li et al. ([2023](#bib.bib16)) and the LLM
    perplexity Jiang et al. ([a](#bib.bib12), [2023](#bib.bib13)). On the other hand,
    white-box compression focuses on generating summarization or compressing the context
    into soft prompt through fine-tuning or Low-Rank Adaptation (LoRA). For instance,
    Wang et al. ([2023b](#bib.bib31)) autoregressively generates filtered content
    and fine-tunes target LLM to use it for generation. Mu et al. ([2023](#bib.bib20))
    trains LLMs to compress instructions into concise key-value attention prefixes.
    Chevalier et al. ([2023](#bib.bib4)) recursively compresses lengthy text into
    summary vectors, while Ge et al. ([2023](#bib.bib7)) generates memory slots to
    represent the original context. Compared with the above-mentioned compression
    studies, QGC’s design fully takes into account the query, which leads to the enhanced
    retention of key information, higher compression ratios, higher throughput, and
    improved overall performance.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM上下文长度的增长，对更高效率、更低成本和更短延迟的需求引起了广泛关注。作为一种有前途的解决方案，压缩技术可以大致分为两种类型：黑箱压缩 Xu
    et al. ([2023](#bib.bib34)) 和白箱压缩 Wang et al. ([2023b](#bib.bib31))。黑箱压缩主要涉及基于不同重要性度量的令牌修剪，如自信息 Li
    et al. ([2023](#bib.bib16)) 和LLM困惑度 Jiang et al. ([a](#bib.bib12), [2023](#bib.bib13))。另一方面，白箱压缩侧重于通过微调或低秩适配（LoRA）生成摘要或将上下文压缩为软提示。例如，Wang
    et al. ([2023b](#bib.bib31)) 自回归生成过滤后的内容，并微调目标LLM以用于生成。Mu et al. ([2023](#bib.bib20))
    训练LLM将指令压缩为简洁的键值注意前缀。Chevalier et al. ([2023](#bib.bib4)) 将冗长文本递归压缩为摘要向量，而Ge et al.
    ([2023](#bib.bib7)) 生成记忆槽来表示原始上下文。与上述压缩研究相比，QGC的设计充分考虑了查询，从而增强了关键信息的保留，提高了压缩比、吞吐量以及整体性能。
- en: 7 Conclusion and Future Work
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: 'In this paper, we have presented a query-guided compressor QGC for LLMs to
    solve the loss of key information under high compression ratios. It consists of
    four essential components: query-guided context encoder, query-guided pooling
    layer, query-document reviewing layer, and semantic alignment layer. In addition,
    we also propose a dynamically compressing strategy during inference. Extensive
    experiments on multi-document QA tasks demonstrate that QGC outperforms previous
    state-of-the-art compression methods in both accuracy and compression ratios.
    Analyses reveal that this is primarily due to our retention of key information
    throughout the compression process.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种用于解决高压缩比下关键信息丧失的查询引导压缩器 QGC。它由四个核心组件组成：查询引导上下文编码器、查询引导池化层、查询-文档审查层和语义对齐层。此外，我们还提出了一种在推理过程中动态压缩的策略。针对多文档
    QA 任务的大量实验表明，QGC 在准确性和压缩比方面均优于之前的最先进压缩方法。分析表明，这主要归因于我们在整个压缩过程中保留了关键信息。
- en: In the future, we aim to validate our approach on more advanced LLMs, while
    also expanding its application to additional tasks like document summarization.
    Besides, we will try to further improve our approach by combining previous studies Zhang
    et al. ([a](#bib.bib37)); Hu et al. ([2022](#bib.bib8)); Zhang et al. ([2022](#bib.bib36),
    [b](#bib.bib38)).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们计划在更先进的 LLM 上验证我们的方法，同时将其应用扩展到文档摘要等额外任务。此外，我们还将尝试通过结合之前的研究进一步改进我们的方法 Zhang
    et al. ([a](#bib.bib37))；Hu et al. ([2022](#bib.bib8))；Zhang et al. ([2022](#bib.bib36)，[b](#bib.bib38))。
- en: Limitations
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: QGC is a white-box compressor that necessitates access to the internal parameters
    of LLMs, which restricts its applicability. Furthermore, we have solely validated
    the effectiveness of QGC on QA and ICL task, and its performance on other tasks
    that differ significantly from QA task, such as summarization, remains to be verified.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: QGC 是一种白盒压缩器，需要访问 LLM 的内部参数，这限制了它的适用性。此外，我们仅在 QA 和 ICL 任务中验证了 QGC 的有效性，其在其他与
    QA 任务差异较大的任务（如摘要）的表现仍需验证。
- en: Acknowledgements
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The project was supported by National Key R&D Program of China (No. 2022ZD0160501),
    National Natural Science Foundation of China (No. 62276219), and the Public Technology
    Service Platform Project of Xiamen (No. 3502Z20231043). We also thank the reviewers
    for their insightful comments.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目得到了中国国家重点研发计划（编号 2022ZD0160501）、中国国家自然科学基金（编号 62276219）和厦门市公共技术服务平台项目（编号
    3502Z20231043）的支持。我们还感谢评审专家的深刻评论。
- en: References
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    and Juanzi Li. 2023. Longbench: A bilingual, multitask benchmark for long context
    understanding. *arXiv preprint arXiv:2308.14508*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang,
    和 Juanzi Li. 2023. Longbench：一个用于长上下文理解的双语多任务基准。*arXiv 预印本 arXiv:2308.14508*。
- en: 'Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew R
    Gormley. 2023. Unlimiformer: Long-range transformers with unlimited length input.
    *arXiv preprint arXiv:2305.01625*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertsch et al. (2023) Amanda Bertsch, Uri Alon, Graham Neubig, 和 Matthew R Gormley.
    2023. Unlimiformer：具有无限长度输入的长距离变换器。*arXiv 预印本 arXiv:2305.01625*。
- en: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong
    Tian. 2023. Extending context window of large language models via positional interpolation.
    *arXiv preprint arXiv:2306.15595*.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Shouyuan Chen, Sherman Wong, Liangjian Chen, 和 Yuandong Tian.
    2023. 通过位置插值扩展大型语言模型的上下文窗口。*arXiv 预印本 arXiv:2306.15595*。
- en: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and
    Danqi Chen. 2023. Adapting language models to compress contexts. *arXiv preprint
    arXiv:2305.14788*.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chevalier et al. (2023) Alexis Chevalier, Alexander Wettig, Anirudh Ajith, 和
    Danqi Chen. 2023. 使语言模型适应压缩上下文。*arXiv 预印本 arXiv:2305.14788*。
- en: 'Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. 2023. Longnet: Scaling transformers
    to 1,000,000,000 tokens. *arXiv preprint arXiv:2307.02486*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding et al. (2023) Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan
    Huang, Wenhui Wang, Nanning Zheng, 和 Furu Wei. 2023. Longnet：将变换器扩展到 1,000,000,000
    个标记。*arXiv 预印本 arXiv:2307.02486*。
- en: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. 2022. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2022) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, 和 Zhifang Sui. 2022. 关于上下文学习的调查。*arXiv 预印本 arXiv:2301.00234*。
- en: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. 2023.
    In-context autoencoder for context compression in a large language model. *arXiv
    preprint arXiv:2307.06945*.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. (2023) Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, 和 Furu Wei. 2023.
    大语言模型中用于上下文压缩的上下文自动编码器。*arXiv 预印本 arXiv:2307.06945*。
- en: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. [LoRA: Low-rank adaptation
    of large language models](https://openreview.net/forum?id=nZeVKeeFYf9). In *International
    Conference on Learning Representations*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2022) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. 2022. [LoRA: 大型语言模型的低秩适应](https://openreview.net/forum?id=nZeVKeeFYf9)。在*国际学习表征会议*上。'
- en: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. 2023. [Raven: In-context learning with retrieval augmented
    encoder-decoder language models](http://arxiv.org/abs/2308.07922).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, 和 Bryan Catanzaro. 2023. [Raven: 具有检索增强编码器-解码器语言模型的上下文学习](http://arxiv.org/abs/2308.07922)。'
- en: Izacard et al. (2022a) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022a. [Unsupervised
    dense information retrieval with contrastive learning](http://arxiv.org/abs/2112.09118).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2022a) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, 和 Edouard Grave. 2022a. [无监督密集信息检索与对比学习](http://arxiv.org/abs/2112.09118)。
- en: 'Izacard et al. (2022b) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
    Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian
    Riedel, and Edouard Grave. 2022b. [Atlas: Few-shot learning with retrieval augmented
    language models](http://arxiv.org/abs/2208.03299).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Izacard et al. (2022b) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
    Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian
    Riedel, 和 Edouard Grave. 2022b. [Atlas: 具有检索增强语言模型的少样本学习](http://arxiv.org/abs/2208.03299)。'
- en: 'Jiang et al. (a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and
    Lili Qiu. a. LLMLingua: Compressing prompts for accelerated inference of large
    language models. In *EMNLP 2023*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (a) Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, 和 Lili
    Qiu. a. LLMLingua: 压缩提示以加速大型语言模型的推断。在*EMNLP 2023*上。'
- en: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew
    Lin, Yuqing Yang, and Lili Qiu. 2023. Longllmlingua: Accelerating and enhancing
    llms in long context scenarios via prompt compression. *arXiv preprint arXiv:2310.06839*.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew
    Lin, Yuqing Yang, 和 Lili Qiu. 2023. Longllmlingua: 通过提示压缩加速和增强长上下文场景中的大语言模型。*arXiv
    预印本 arXiv:2310.06839*。'
- en: 'Jiang et al. (b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang,
    Yiqin Dai, and Jimmy Lin. b. “low-resource” text classification: A parameter-free
    classification method with compressors. In *Findings of ACL 2023*.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (b) Zhiying Jiang, Matthew Yang, Mikhail Tsirlin, Raphael Tang,
    Yiqin Dai, 和 Jimmy Lin. b. “低资源”文本分类：一种无参数分类方法与压缩器。 在*ACL 2023 发现*中。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *NeurIPS 2020*.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等. 2020. 用于知识密集型自然语言处理任务的检索增强生成。*NeurIPS 2020*。
- en: Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, and Chenghua Lin. 2023.
    Compressing context to enhance inference efficiency of large language models.
    In *EMNLP 2023*.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yucheng Li, Bo Dong, Frank Guerin, 和 Chenghua Lin. 2023. 压缩上下文以提高大型语言模型的推断效率。在*EMNLP
    2023*上。
- en: 'Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language
    models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 2023. 在中间迷失：语言模型如何使用长上下文。*arXiv 预印本
    arXiv:2307.03172*。
- en: '(18) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi,
    and Hannaneh Hajishirzi. When not to trust language models: Investigating effectiveness
    of parametric and non-parametric memories. In *ACL 2023*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, 和
    Hannaneh Hajishirzi. 何时不信任语言模型：调查参数化和非参数化记忆的有效性。在*ACL 2023*上。
- en: (19) Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei
    Han, and Weizhu Chen. Reader-guided passage reranking for open-domain question
    answering. In *Findings of ACL 2021*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) 宋昀、彭程赫、刘晓东、沈业龙、高剑锋、韩家伟和陈维柱。读者引导的段落重新排序用于开放域问答。在*ACL 2021 的发现*。
- en: Mu et al. (2023) Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to
    compress prompts with gist tokens. *arXiv preprint arXiv:2304.08467*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu et al. (2023) 杰西·穆、香·丽莎·李和诺亚·古德曼。2023年。学习使用要点令牌进行提示压缩。*arXiv 预印本 arXiv:2304.08467*。
- en: 'Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. [Webgpt: Browser-assisted
    question-answering with human feedback](http://arxiv.org/abs/2112.09332).'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano et al. (2022) 中野礼一郎、雅各布·希尔顿、苏奇尔·巴拉吉、杰夫·吴、龙·欧阳、克里斯蒂娜·金、克里斯托弗·赫斯、尚塔努·贾因、维尼特·科萨拉朱、威廉·桑德斯、徐江、卡尔·科比、塔伊娜·埃伦杜、格雷琴·克鲁格、凯文·巴顿、马修·奈特、本杰明·切斯和约翰·舒尔曼。2022年。[Webgpt：浏览器辅助的问答系统带有人类反馈](http://arxiv.org/abs/2112.09332)。
- en: Nijkamp et al. (2023) Erik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying
    Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, Ben Krause, et al. 2023.
    Xgen-7b technical report. *arXiv preprint arXiv:2309.03450*.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nijkamp et al. (2023) 埃里克·奈坎普、田曦、宫崎广、博·庞、聪颖·夏、陈星、杰西·维格、塞米赫·雅武兹、菲利普·拉班、本·克劳斯等。2023年。Xgen-7b
    技术报告。*arXiv 预印本 arXiv:2309.03450*。
- en: OpenAI (2023) OpenAI OpenAI. 2023. Gpt-4 technical report.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI OpenAI。2023年。Gpt-4 技术报告。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *NeurIPS 2020*.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) 龙·欧阳、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、钟张、桑迪尼·阿戈瓦尔、卡塔里娜·斯拉玛、亚历克斯·雷等。2022年。训练语言模型以跟随人类反馈的指令。*NeurIPS
    2020*。
- en: 'Peng et al. (2023) Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole.
    2023. Yarn: Efficient context window extension of large language models. *arXiv
    preprint arXiv:2309.00071*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) 博文·彭、杰弗里·克斯内尔、洪璐·范和恩里科·希波尔。2023年。Yarn：大规模语言模型的高效上下文窗口扩展。*arXiv
    预印本 arXiv:2309.00071*。
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Press et al. (2021) 奥菲尔·普雷斯、诺亚·A·史密斯和迈克·刘易斯。2021年。训练短期，测试长期：线性偏置的注意力实现输入长度外推。*arXiv
    预印本 arXiv:2108.12409*。
- en: Reimers and Gurevych (2020) Nils Reimers and Iryna Gurevych. 2020. Making monolingual
    sentence embeddings multilingual using knowledge distillation. In *EMNLP 2020*.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych (2020) 尼尔斯·赖默斯和伊琳娜·古列维奇。2020年。利用知识蒸馏将单语句嵌入转换为多语句。在*EMNLP
    2020*。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) 雨果·图弗龙、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、克萨维尔·马蒂奈、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴蒂斯特·罗济埃尔、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿扎尔等。2023a。Llama：开放和高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) 雨果·图弗龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔迈赫里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏米亚·巴特拉、普拉贾瓦尔·巴尔加瓦、舒鲁提·博萨尔等。2023b。Llama
    2：开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: Wang et al. (2023a) Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu,
    Mohammad Shoeybi, Yi Dong, Oleksii Kuchaiev, Bo Li, Chaowei Xiao, Anima Anandkumar,
    and Bryan Catanzaro. 2023a. [Shall we pretrain autoregressive language models
    with retrieval? a comprehensive study](http://arxiv.org/abs/2304.06762).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) 博鑫·王、魏平、彭旭、劳伦斯·麦卡费、紫涵·刘、穆罕默德·肖伊比、易东、奥列克西·库恰耶夫、博·李、曹伟·肖、安尼玛·安南德库马尔和布赖恩·卡坦扎罗。2023a。[我们是否应该用检索预训练自回归语言模型？全面研究](http://arxiv.org/abs/2304.06762)。
- en: Wang et al. (2023b) Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md Rizwan Parvez,
    and Graham Neubig. 2023b. Learning to filter context for retrieval-augmented generation.
    *arXiv preprint arXiv:2311.08377*.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) 施若·王、准·阿拉基、郑宝江、Md Rizwan Parvez 和格雷厄姆·纽比格。2023b。学习过滤上下文以增强生成。*arXiv
    预印本 arXiv:2311.08377*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *NeurIPS 2022*.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等（2022）杰森·魏、王学之、戴尔·舒尔曼斯、马尔滕·博斯马、夏飞、埃德·奇、阮国伟、丹尼·周等。2022. 思维链提示在大型语言模型中引发推理。*NeurIPS
    2022*。
- en: 'Xiao et al. (2023) Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff.
    2023. [C-pack: Packaged resources to advance general chinese embedding](http://arxiv.org/abs/2309.07597).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '肖等（2023）肖士涛、刘峥、张佩田和尼克拉斯·穆宁霍夫。2023. [C-pack: 打包资源以推进通用中文嵌入](http://arxiv.org/abs/2309.07597)。'
- en: 'Xu et al. (2023) Fangyuan Xu, Weijia Shi, and Eunsol Choi. 2023. Recomp: Improving
    retrieval-augmented lms with compression and selective augmentation. *arXiv preprint
    arXiv:2310.04408*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '许等（2023）许方元、施伟佳和崔恩索尔。2023. Recomp: 通过压缩和选择性增强改进检索增强的语言模型。*arXiv 预印本 arXiv:2310.04408*。'
- en: '(35) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
    Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable
    multi-hop question answering. In *EMNLP 2018*.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '（35）杨志林、齐鹏、张赛征、约书亚·本吉奥、威廉·科恩、鲁斯兰·萨拉胡丁诺夫和克里斯托弗·D·曼宁。HotpotQA: 一个用于多跳问答的多样化、可解释的数据集。见
    *EMNLP 2018*。'
- en: 'Zhang et al. (2022) Biao Zhang, Deyi Xiong, Yubin Ge, Junfeng Yao, Hao Yue,
    and Jinsong Su. 2022. Aan+: Generalized average attention network for accelerating
    neural transformer. *Journal of Artificial Intelligence Research*.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等（2022）张彪、熊德义、戈宇彬、姚军峰、岳浩和苏金松。2022. Aan+: 用于加速神经 Transformer 的广义平均注意力网络。*人工智能研究杂志*。'
- en: Zhang et al. (a) Biao Zhang, Deyi Xiong, Jinsong Su, Qian Lin, and Huiji Zhang.
    a. Simplifying neural machine translation with addition-subtraction twin-gated
    recurrent networks. In *EMNLP 2018*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（a）张彪、熊德义、苏金松、林倩和张慧姬。a. 通过加减双门递归网络简化神经机器翻译。见 *EMNLP 2018*。
- en: 'Zhang et al. (b) Zhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong
    Sun, and Jie Zhou. b. MoEfication: Transformer feed-forward layers are mixtures
    of experts. In *Findings of ACL 2022*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等（b）郑炎张、林彦凯、刘智远、李鹏、孙茂松和周杰。b. MoEfication: Transformer 前馈层是专家混合体。见 *ACL 2022
    发现*。'
- en: Appendix A Instructions Used in QGC
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A QGC 中使用的说明
- en: The following are the instructions we used after referring to the existing studies Liu
    et al. ([2023](#bib.bib17)) and testing.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们在参考现有研究刘等（[2023](#bib.bib17)）和测试后使用的说明。
- en: •
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'NaturalQuestions: Write a high-quality answer for the given question using
    only the provided search results(some of which might be irrelevant).'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'NaturalQuestions: 使用仅提供的搜索结果（其中一些可能无关），为给定问题编写高质量的答案。'
- en: •
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TriviaQA & HotpotQA: Using only the provided search results (some of which
    might be irrelevant), answer the following question with one or few words.'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TriviaQA & HotpotQA: 仅使用提供的搜索结果（其中一些可能无关），用一个或几个词回答以下问题。'
