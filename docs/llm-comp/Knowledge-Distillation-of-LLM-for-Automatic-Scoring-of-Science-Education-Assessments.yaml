- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:59:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:59:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 科学教育评估的LLM知识蒸馏自动评分
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15842](https://ar5iv.labs.arxiv.org/html/2312.15842)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.15842](https://ar5iv.labs.arxiv.org/html/2312.15842)
- en: Ehsan Latif AI4STEM Education Center & Department of Mathematics, Science, and
    Social Studies Education, University of GeorgiaAthensGAUSA ,  Luyang Fang AI4STEM
    Education Center & Department of Statistics, University of GeorgiaAthensGAUSA
    ,  Ping Ma Department of Statistics, University of GeorgiaAthensGAUSA  and  Xiaoming
    Zhai AI4STEM Education Center & Department of Mathematics, Science, and Social
    Studies Education, University of GeorgiaAthensGAUSA
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ehsan Latif AI4STEM教育中心与乔治亚大学数学、科学和社会研究教育系，乔治亚州雅典，美国，Luyang Fang AI4STEM教育中心与乔治亚大学统计系，乔治亚州雅典，美国，Ping
    Ma 乔治亚大学统计系，乔治亚州雅典，美国，以及 Xiaoming Zhai AI4STEM教育中心与乔治亚大学数学、科学和社会研究教育系，乔治亚州雅典，美国
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: This study proposes a method for knowledge distillation (KD) of fine-tuned Large
    Language Models (LLMs) into smaller, more efficient, and accurate neural networks.
    We specifically target the challenge of deploying these models on resource-constrained
    devices. Our methodology involves training the smaller student model (Neural Network)
    using the prediction probabilities (as soft labels) of the LLM, which serves as
    a teacher model. This is achieved through a specialized loss function tailored
    to learn from the LLM’s output probabilities, ensuring that the student model
    closely mimics the teacher’s performance. To validate the performance of the KD
    approach, we utilized a large dataset, 7T, containing 6,684 student-written responses
    to science questions and three mathematical reasoning datasets with student-written
    responses graded by human experts. We compared accuracy with state-of-the-art
    (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models.
    Results have shown that the KD approach has 1% and 4% higher scoring accuracy
    than ANN and TinyBERT and comparable accuracy to the teacher model. Furthermore,
    the student model size is 0.02M, 10,000 times smaller in parameters and x10 faster
    in inferencing than the teacher model and TinyBERT, respectively. The significance
    of this research lies in its potential to make advanced AI technologies accessible
    in typical educational settings, particularly for automatic scoring.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种将微调的大型语言模型（LLM）蒸馏成更小、更高效且更准确的神经网络的方法。我们特别关注在资源有限的设备上部署这些模型的挑战。我们的方法包括使用LLM的预测概率（作为软标签）训练较小的学生模型（神经网络），LLM作为教师模型。这是通过一个专门的损失函数实现的，该函数针对LLM的输出概率进行学习，以确保学生模型能紧密模仿教师的表现。为了验证KD方法的性能，我们使用了一个大数据集7T，其中包含6684份学生对科学问题的回答，以及三个数学推理数据集，这些数据集中的学生回答由人工专家评分。我们与最先进的（SOTA）蒸馏模型、TinyBERT和人工神经网络（ANN）模型进行了准确性比较。结果显示，KD方法的评分准确度比ANN和TinyBERT分别高出1%和4%，且与教师模型的准确度相当。此外，学生模型的大小为0.02M，比教师模型和TinyBERT分别小10,000倍，推理速度快x10。该研究的意义在于其有潜力将先进的AI技术引入典型的教育环境，特别是自动评分方面。
- en: large language model (LLM), BERT, knowledge distillation, automatic scoring,
    education technology
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM），BERT，知识蒸馏，自动评分，教育技术
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Artificial Intelligence (AI) in education has evolved from a theoretical concept
    to a practical tool, significantly impacting classroom assessment practices and
    adaptive learning systems (González-Calatayud et al., [2021](#bib.bib9); Holmes
    and Tuomi, [2022](#bib.bib13); Latif and Zhai, [2023a](#bib.bib16)). AI for personalized
    learning and assessment provides opportunities for more tailored and effective
    educational experiences (Zhai et al., [2020](#bib.bib39)). Integrating Large Language
    Models (LLMs) from domains on AI like BERT (Devlin et al., [2018](#bib.bib6))
    into various domains, such as education, has been a significant milestone in advancing
    education technologies (Liu et al., [2023a](#bib.bib26); Zhai and Nehm, [2023](#bib.bib37);
    Zhai, [2022](#bib.bib33)). In education, LLMs have shown promise in enhancing
    learning experiences, providing personalized learning content and support, and
    facilitating automatic scoring systems (Selwyn, [2019](#bib.bib29); Zhai et al.,
    [2021a](#bib.bib35); Latif et al., [2023b](#bib.bib18)). Despite their potential,
    the deployment of these models in educational settings is constrained by their
    considerable size (714MB for 178 million parameters and 495MB for 124 million
    parameters) and computational requirements (16 Tensor Processing Units), presenting
    a challenge for widespread adoption in resource-constrained educational environments
    such as mobile/tablet and schools provided laptops with no GPU or TPUs and limited
    memory (Hinton et al., [2015](#bib.bib12)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 教育中的人工智能（AI）已经从理论概念发展成为实际工具，对课堂评估实践和自适应学习系统产生了显著影响（González-Calatayud et al.,
    [2021](#bib.bib9); Holmes and Tuomi, [2022](#bib.bib13); Latif and Zhai, [2023a](#bib.bib16)）。用于个性化学习和评估的人工智能为更具针对性和有效的教育体验提供了机会（Zhai
    et al., [2020](#bib.bib39)）。将诸如BERT（Devlin et al., [2018](#bib.bib6)）等大型语言模型（LLMs）从人工智能领域整合到教育等各个领域，是推进教育技术的重要里程碑（Liu
    et al., [2023a](#bib.bib26); Zhai and Nehm, [2023](#bib.bib37); Zhai, [2022](#bib.bib33)）。在教育领域，LLMs展示了在提升学习体验、提供个性化学习内容和支持、以及促进自动评分系统方面的潜力（Selwyn,
    [2019](#bib.bib29); Zhai et al., [2021a](#bib.bib35); Latif et al., [2023b](#bib.bib18)）。尽管潜力巨大，这些模型在教育环境中的部署受到其相当大的尺寸（178百万参数的模型为714MB，124百万参数的模型为495MB）和计算要求（16个张量处理单元）的限制，这给资源有限的教育环境（如没有GPU或TPU的移动设备/平板电脑和学校提供的笔记本电脑，以及内存有限）的大规模采用带来了挑战（Hinton
    et al., [2015](#bib.bib12)）。
- en: To bridge this gap, our study explores the feasibility of distilling the knowledge
    of LLMs into smaller neural networks, referred to as student models, with few
    parameters and a small number of hidden layers. By training a smaller student
    model using soft labels provided by a fine-tuned LLM (i.e., teacher model), we
    aim to achieve a similar scoring performance as LLMs with reduced model size.
    This approach leverages recent advancements in knowledge distillation (KD) (Hinton
    et al., [2015](#bib.bib12)) and AI, demonstrating the potential of smaller models
    to achieve comparable accuracy to LLMs (Zhang et al., [2022](#bib.bib40); Li and
    Li, [2021](#bib.bib23); Xu and Yang, [2017](#bib.bib31)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一差距，我们的研究探索了将大型语言模型（LLMs）的知识提炼到参数较少、隐藏层较少的较小神经网络中的可行性，这些较小的网络被称为学生模型。通过使用由微调后的LLM（即教师模型）提供的软标签来训练较小的学生模型，我们旨在实现与LLMs相似的评分性能，同时减少模型大小。该方法利用了知识蒸馏（KD）（Hinton
    et al., [2015](#bib.bib12)）和人工智能的最新进展，展示了较小模型在准确度上可以达到与LLMs相当的潜力（Zhang et al.,
    [2022](#bib.bib40); Li and Li, [2021](#bib.bib23); Xu and Yang, [2017](#bib.bib31)）。
- en: In this context, we have fine-tuned BERT on a large dataset of student-written
    science assessment responses (similar to Liu et al. ([2023a](#bib.bib26))). The
    fine-tuned model then serves as a teacher model, guiding the training of a compact
    student model. Our innovative loss function is designed to align the student model’s
    predictions with the teacher model’s, achieving similar accuracy with a smaller
    model size and faster inference time. This technique is particularly applicable
    for automatic scoring in education, where timely and accurate feedback is essential
    (Zhai et al., [2022](#bib.bib36), [2021b](#bib.bib38)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，我们对BERT进行了微调，使用了一大规模的学生撰写的科学评估回应数据集（类似于Liu et al. ([2023a](#bib.bib26))）。微调后的模型作为教师模型，指导紧凑的学生模型的训练。我们的创新损失函数旨在使学生模型的预测与教师模型对齐，在较小的模型尺寸和更快的推理时间下实现类似的准确度。这项技术特别适用于教育中的自动评分，在这些场景中，及时和准确的反馈至关重要（Zhai
    et al., [2022](#bib.bib36), [2021b](#bib.bib38)）。
- en: The significance of this research lies in its potential to make advanced AI
    technologies accessible in typical educational settings. The study addresses the
    technical challenges of deploying AI models in resource-constrained environments
    and highlights the potential of AI in transforming educational assessment practices.
    By enabling the deployment of efficient automatic scoring systems on less powerful
    hardware available in school settings, we contribute to the democratization of
    AI in education.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究的重要性在于其将先进的人工智能技术引入典型教育环境的潜力。研究解决了在资源受限环境中部署人工智能模型的技术挑战，并强调了人工智能在变革教育评估实践中的潜力。通过使高效的自动评分系统能够在学校环境中较不强大的硬件上运行，我们为教育中的人工智能普及做出了贡献。
- en: 2\. Background
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景
- en: The use of LLMs in education, specifically for automatic scoring, has gained
    significant attention in recent years. Studies have focused on fine-tuning LLMs
    like ChatGPT for automatic scoring applications, demonstrating their potential
    in evaluating student responses with high accuracy (Latif and Zhai, [2023b](#bib.bib17);
    Zhai, [2023](#bib.bib34)). For instance, (Latif and Zhai, [2023b](#bib.bib17))
    fine-tuned GPT-3.5 Turbo to score students’ written explanations automatically,
    and Fang et al. (Fang et al., [2023](#bib.bib7)) used GPT-4 to augment unbalanced
    data for more effective automatic scoring and Lee et al. (Lee et al., [2023b](#bib.bib20),
    [a](#bib.bib19)) applied chain-of-thought to achieve high accuracy for automatic
    scoring using ChatGPT. Further, (Schneider et al., [2023](#bib.bib28); Bertolini
    et al., [2023](#bib.bib2)) explore LLM-based auto-grading for short textual answers
    and the automatic scoring of emotional content in texts, respectively. These studies
    highlight the versatility of LLMs in handling diverse types of educational assessments.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在教育中的使用，特别是在自动评分方面，近年来引起了广泛关注。研究集中于对类似 ChatGPT 的大型语言模型进行微调以实现自动评分应用，展示了其在高准确度评估学生回答方面的潜力（Latif
    和 Zhai，[2023b](#bib.bib17)；Zhai，[2023](#bib.bib34)）。例如，(Latif 和 Zhai，[2023b](#bib.bib17))
    微调了 GPT-3.5 Turbo 以自动评分学生的书面解释，而 Fang 等（Fang 等，[2023](#bib.bib7)）则使用 GPT-4 来增强不平衡的数据，以实现更有效的自动评分，Lee
    等（Lee 等，[2023b](#bib.bib20)，[a](#bib.bib19)）应用链式思维技术实现了 ChatGPT 高准确度的自动评分。此外，（Schneider
    等，[2023](#bib.bib28)；Bertolini 等，[2023](#bib.bib2)）分别探讨了基于大型语言模型的短文本自动评分和情感内容自动评分。这些研究突显了大型语言模型在处理各种类型教育评估方面的多样性。
- en: Furthermore, recent advancements in AI for education, such as the development
    of Artificial General Intelligence (AGI) for educational purposes and chain-of-thought
    prompting techniques, provide additional insights into the potential of LLMs in
    educational settings (Lee et al., [2023c](#bib.bib21); Latif et al., [2023a](#bib.bib15);
    Wang et al., [2023](#bib.bib30)). These advancements underscore the transformative
    potential of AI in reshaping educational assessment practices, aligning with our
    approach to making LLMs more accessible and practical for educational use.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，近期在教育领域的人工智能进展，如为教育目的开发的人工通用智能（AGI）以及链式思维提示技术，为大型语言模型（LLMs）在教育环境中的潜力提供了额外的见解（Lee
    等，[2023c](#bib.bib21)；Latif 等，[2023a](#bib.bib15)；Wang 等，[2023](#bib.bib30)）。这些进展凸显了人工智能在重塑教育评估实践方面的变革潜力，与我们使大型语言模型更易于获取和实用的目标相一致。
- en: Advanced LLMs have achieved remarkable performance across various fields. However,
    deploying those models presents challenges due to high computational demands and
    data privacy issues. For example, simply loading the GPT-3 model, which contains
    175 billion parameters, requires 350 GB of memory space when using float 16 as
    the data type, and that is before processing any data. Such heavy storage makes
    deployment challenging on resource-constrained devices (Zhou et al., [2023](#bib.bib41)).
    Recent research shows that deep neural networks can unintentionally memorize data
    from training sets and can be extracted by the users (Carlini et al., [2019](#bib.bib4)).
    This indicates that the data leakage problem may occur if we apply the online
    deployment of LLMs on confidential information, such as most educational data.
    Given these concerns, an effective solution will be distilling LLMs into a more
    compact model, which can then be stored on private, resource-limited devices for
    handling confidential data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 高级LLM在各个领域已取得了显著的成绩。然而，部署这些模型面临挑战，因为它们对计算资源的需求高以及数据隐私问题。例如，仅加载包含1750亿参数的GPT-3模型，在使用float
    16作为数据类型时需要350 GB的内存空间，这还未处理任何数据。这种庞大的存储需求使得在资源有限的设备上进行部署具有挑战性（周等，[2023](#bib.bib41)）。近期研究表明，深度神经网络可能会无意中记住训练集中的数据，并且这些数据可以被用户提取（Carlini
    et al., [2019](#bib.bib4)）。这表明，如果我们在机密信息（如大多数教育数据）上应用LLMs的在线部署，可能会出现数据泄露问题。鉴于这些问题，一种有效的解决方案是将LLMs蒸馏成更紧凑的模型，然后可以将其存储在私有的、资源有限的设备上，以处理机密数据。
- en: KD (Hinton et al., [2015](#bib.bib12)) has emerged as a pivotal technique in
    harnessing the power of LLMs for practical applications, particularly in fields
    with limited computational resources. The core idea is to transfer the knowledge
    from a large, complex model (teacher) to a smaller, more efficient model (student),
    thereby enabling the deployment of sophisticated AI capabilities in constrained
    environments.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: KD（Hinton et al., [2015](#bib.bib12)）已成为在有限计算资源的领域中利用LLMs进行实际应用的关键技术。其核心思想是将知识从大型复杂模型（教师）转移到较小且更高效的模型（学生），从而使得在受限环境中部署复杂的AI能力成为可能。
- en: Several approaches have been proposed to address these challenges. Calibrating
    LLM-based evaluators have been a focus, aiming to optimize their performance in
    educational settings (Liu et al., [2023b](#bib.bib25)). For instance, (Li et al.,
    [2023](#bib.bib22)) identified appropriate instructions by prompting ChatGPT with
    different templates to collect and refine the rationales to fine-tune the smaller
    language model for the simultaneous generation of automatic scores and rationales
    and achieve 11% higher Quadratic Weighted Kappa than ChatGPT. Furthermore, (Liang
    et al., [2023](#bib.bib24)) proposed the LLM distillation technique to solve math
    word problems, which fosters a tailored learning experience by generating targeted
    exercises aligned with educational science principles and able to achieve comparable
    accuracy as GPT-3.5 and PaLLM. Lastly, (Sahu et al., [2023](#bib.bib27)) proposed
    PromptMix, which generates augmented data for near class boundaries using LLM
    and performs text classifications using a smaller Neural Network model and found
    that relabeling borderline examples facilitates the transfer of knowledge of a
    massive LLM like GPT3.5-turbo into smaller and cheaper classifiers like Distil_BERT.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，已经提出了几种方法。对基于LLM的评估器进行校准已成为重点，旨在优化其在教育环境中的表现（刘等，[2023b](#bib.bib25)）。例如，（李等，[2023](#bib.bib22)）通过使用不同模板提示ChatGPT，识别了合适的指令，以收集和完善理由，从而对较小的语言模型进行微调，实现自动评分和理由的同步生成，并比ChatGPT获得了11%的更高的二次加权κ值。此外，（梁等，[2023](#bib.bib24)）提出了LLM蒸馏技术来解决数学文字问题，这通过生成与教育科学原理对齐的有针对性的练习来促进量身定制的学习体验，并能够达到与GPT-3.5和PaLLM相媲美的准确性。最后，（萨胡等，[2023](#bib.bib27)）提出了PromptMix，这种方法使用LLM生成近类边界的增强数据，并使用较小的神经网络模型进行文本分类，发现重新标记边界示例有助于将像GPT3.5-turbo这样的大型LLM的知识转移到像Distil_BERT这样的小型且便宜的分类器中。
- en: All these approaches aim to reduce the model size by maintaining high prediction
    accuracy to facilitate the language model deployment to smaller devices with limited
    resources. However, maintaining the appropriate balance between size reduction
    and accuracy still struggles. Studies (Li and Li, [2021](#bib.bib23); Xu and Yang,
    [2017](#bib.bib31)) also have addressed this limitation by refining data distillation
    techniques for text classification. Yet, the complexity and variability of language
    often demand more sophisticated distillation strategies.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些方法的目标都是通过保持高预测准确率来减少模型的大小，以便将语言模型部署到资源有限的小型设备上。然而，在大小减少和准确性之间保持适当的平衡仍然存在困难。研究（Li和Li，[2021](#bib.bib23)；Xu和Yang，[2017](#bib.bib31)）也通过优化文本分类的数据蒸馏技术解决了这一限制。然而，语言的复杂性和多变性往往需要更复杂的蒸馏策略。
- en: Most of the approaches applied KD to whitebox classification models to imitate
    black-box models like ChatGPT. LLMs like GPT-3 ( or other GPT models) have achieved
    impressive performance on tasks such as text generation and question answering,
    but they are not suitable for direct use for classification problems. On the contrary,
    models like BERT (Devlin et al., [2018](#bib.bib6)) are specifically trained for
    classification tasks, making them more suitable for our tasks in classification.
    Studies have shown that the BERT model achieves state-of-the-art performance on
    a wide range of natural language processing (NLP) tasks, including text classification
    (González-Carvajal and Garrido-Merchán, [2020](#bib.bib10); Yu et al., [2019](#bib.bib32)).
    Therefore, we choose BERT as the teacher model in this study.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数方法将KD应用于白盒分类模型，以模仿像ChatGPT这样的黑盒模型。像GPT-3（或其他GPT模型）这样的LLMs在文本生成和问答等任务上取得了令人印象深刻的表现，但它们不适合直接用于分类问题。相反，像BERT（Devlin等，[2018](#bib.bib6)）这样的模型专门为分类任务训练，使其更适合我们的分类任务。研究表明，BERT模型在广泛的自然语言处理（NLP）任务上，包括文本分类（González-Carvajal和Garrido-Merchán，[2020](#bib.bib10)；Yu等，[2019](#bib.bib32)），达到了最先进的性能。因此，我们选择BERT作为本研究中的教师模型。
- en: Recent research by Microsoft (Gu et al., [2023](#bib.bib11)) proposed MINILLM
    that distills smaller language models from generative LLMs. They have replaced
    the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches
    with reverse KLD, which is more suitable for KD on generative language models,
    to prevent the student model from overestimating the low-probability regions of
    the teacher distribution. Furthermore, (Jiao et al., [2019](#bib.bib14)) proposed
    a two-stage learning framework for TinyBERT, which performs Transformer distillation
    at both the pretraining and task-specific learning stages. We took inspiration
    from MINILLM and TinyBERT and modified the standard KD by replacing a best-fitting
    regression within the loss function to maximize the prediction accuracy of the
    student model.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的最新研究（Gu等，[2023](#bib.bib11)）提出了MINILLM，该方法从生成LLMs中蒸馏出更小的语言模型。他们用反向Kullback-Leibler散度（KLD）目标替代了标准KD方法中的前向KLD，这更适合生成语言模型上的KD，以防止学生模型高估教师分布的低概率区域。此外，（Jiao等，[2019](#bib.bib14)）提出了一种针对TinyBERT的两阶段学习框架，该框架在预训练和任务特定学习阶段都进行Transformer蒸馏。我们从MINILLM和TinyBERT中获得了灵感，并通过在损失函数中替换最佳拟合回归来最大化学生模型的预测准确性，从而修改了标准KD。
- en: Our approach addresses the limitations of existing approaches by proposing a
    novel knowledge distillation strategy, ensuring that the distilled models retain
    the sophisticated capabilities of their larger counterparts while being deployable
    in resource-constrained settings. This method is particularly relevant for educational
    applications, where the need for efficient and effective AI models is paramount.
    Our research is motivated by the work of Hinton et al. (Hinton et al., [2015](#bib.bib12)),
    who laid the foundational work in this field, illustrating the basic principles
    of KD in neural networks. Building on this, we incorporate methods (Chuang et al.,
    [2020](#bib.bib5); Bhunia et al., [2021](#bib.bib3)) that demonstrated the efficacy
    of lifelong language KD and unifying text recognition using KD, respectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法通过提出一种新颖的知识蒸馏策略，解决了现有方法的局限性，确保蒸馏后的模型保留了其较大对手的复杂能力，同时可以在资源受限的环境中部署。这种方法对于教育应用特别相关，其中对高效且有效的AI模型的需求至关重要。我们的研究受到Hinton等人（Hinton
    et al., [2015](#bib.bib12)）工作的启发，他们在这一领域奠定了基础，展示了神经网络中KD的基本原理。基于此，我们结合了（Chuang
    et al., [2020](#bib.bib5)； Bhunia et al., [2021](#bib.bib3)）的方法，分别展示了终身语言KD和统一文本识别使用KD的有效性。
- en: 3\. Methodology
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法论
- en: In this section, we formulate the problem and develop the knowledge distillation
    method for automatically scoring student-written responses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们制定了问题，并开发了用于自动评分学生书面回答的知识蒸馏方法。
- en: 3.1\. Problem Formulation
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 问题制定
- en: For the convenience of presentation, we introduce our method in the context
    of a classification problem for automatic scoring of student written responses.
    With the training sample $\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$, we
    have
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便展示，我们在自动评分学生书面回答的分类问题的背景下介绍我们的方法。对于训练样本$\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}$，我们有
- en: '| (1) |  | $f(\mathbf{x},\bm{\theta})=(f_{1}(\mathbf{x},\bm{\theta}),\ldots,f_{K}(\mathbf{x},\bm{\theta}))^{T},$
    |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $f(\mathbf{x},\bm{\theta})=(f_{1}(\mathbf{x},\bm{\theta}),\ldots,f_{K}(\mathbf{x},\bm{\theta}))^{T},$
    |  |'
- en: where $\bm{\theta}\in\mathbb{R}^{d}$, the class with the highest probability
    is predicted.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\bm{\theta}\in\mathbb{R}^{d}$，预测类别为具有最高概率的类别。
- en: For a fixed neural network structure, i.e., a known function $f(\cdot,\cdot)$
    during the training of ANN. In general, we achieve this by minimizing the empirical
    risk
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个固定的神经网络结构，即在ANN训练过程中已知的函数$f(\cdot,\cdot)$。一般来说，我们通过最小化经验风险来实现这一点。
- en: '| (2) |  | $\mathcal{L}(f(\cdot,\bm{\theta});\mathcal{D})=\frac{1}{N}\sum_{i=1}^{N}\text{CE}(\mathbf{y}_{i},f(\mathbf{x}_{i},\bm{\theta})),$
    |  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\mathcal{L}(f(\cdot,\bm{\theta});\mathcal{D})=\frac{1}{N}\sum_{i=1}^{N}\text{CE}(\mathbf{y}_{i},f(\mathbf{x}_{i},\bm{\theta})),$
    |  |'
- en: where $\mathbf{y}_{i}=(y_{i1},\ldots,y_{iK})^{T}$. Typically, the minimization
    is achieved using the stochastic gradient descent. Built upon the neural network,
    we propose a knowledge distillation approach to achieve higher prediction accuracy
    for automatic scoring.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{y}_{i}=(y_{i1},\ldots,y_{iK})^{T}$。通常，最小化是通过随机梯度下降实现的。在神经网络的基础上，我们提出了一种知识蒸馏方法，以提高自动评分的预测准确性。
- en: 3.2\. Proposed Knowledge Distillation
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 提议的知识蒸馏
- en: KD is a technique to transfer knowledge from a trained large model (teacher)
    to a more compact and deployable model (student). We take inspiration from the
    prominent KD approach, introduced by Hinton et al. ([2015](#bib.bib12)), which
    involves using the class probabilities generated by the pre-trained large model
    as soft targets for training the smaller model, effectively transferring its predictive
    and generalization capabilities. Building on this concept, we develop a method
    for applying knowledge distillation in the context of automated scoring systems,
    aiming to improve the process of evaluating educational content using AI.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: KD是一种将知识从训练好的大型模型（教师）转移到更紧凑且可部署模型（学生）中的技术。我们从Hinton等人（[2015](#bib.bib12)）提出的著名KD方法中获得灵感，该方法使用预训练的大型模型生成的类别概率作为训练小型模型的软目标，从而有效地转移其预测和泛化能力。基于这一概念，我们开发了一种在自动评分系统中应用知识蒸馏的方法，旨在提高使用AI评估教育内容的过程。
- en: Algorithm 1 Knowledge Distillation for Education Technology
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 教育技术的知识蒸馏
- en: 1:Training dataset $\mathcal{D}$
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 1:训练数据集$\mathcal{D}$
- en: Specifically, for each data point $\mathbf{x}_{i}$. The discrepancy between
    the student and teacher models is measured as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于每个数据点$\mathbf{x}_{i}$。学生模型与教师模型之间的差异被测量为
- en: '| (3) |  | $\displaystyle\tilde{\mathcal{L}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p})$
    |  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\tilde{\mathcal{L}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p})$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: which is the sample mean of the cross-entropy $\text{CE}(\bm{p}_{i},f(\mathbf{x}_{i},\bm{\theta}))$.
    To leverage the information from both the training data and the teacher model’s
    predictions, KD aims to solve
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是交叉熵$\text{CE}(\bm{p}_{i},f(\mathbf{x}_{i},\bm{\theta}))$的样本均值。为了利用训练数据和教师模型预测的两方面信息，KD旨在解决
- en: '| (4) |  | $\displaystyle\bm{\theta}_{\mathrm{KD}}^{*}$ |  |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle\bm{\theta}_{\mathrm{KD}}^{*}$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: where the minimized KD loss $\mathcal{L}^{\mathrm{KD}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p},\lambda)$
    reduces the KD loss in Eq. ([4](#S3.E4 "In 3.2\. Proposed Knowledge Distillation
    ‣ 3\. Methodology ‣ Knowledge Distillation of LLM for Automatic Scoring of Science
    Education Assessments")) to the conventional empirical risk loss. The pseudo code
    for the proposed KD approach can be seen in Alg. [1](#alg1 "Algorithm 1 ‣ 3.2\.
    Proposed Knowledge Distillation ‣ 3\. Methodology ‣ Knowledge Distillation of
    LLM for Automatic Scoring of Science Education Assessments")
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，最小化的KD损失$\mathcal{L}^{\mathrm{KD}}(f(\cdot,\bm{\theta});\mathcal{D},\bm{p},\lambda)$将KD损失在公式
    ([4](#S3.E4 "在 3.2\. 提出的知识蒸馏 ‣ 3\. 方法论 ‣ 用于科学教育评估自动评分的LLM知识蒸馏"))中减少到传统的经验风险损失。所提出的KD方法的伪代码可以在算法
    [1](#alg1 "算法 1 ‣ 3.2\. 提出的知识蒸馏 ‣ 3\. 方法论 ‣ 用于科学教育评估自动评分的LLM知识蒸馏") 中查看。
- en: '![Refer to caption](img/92fded8386810f08c7a2b5841a235492.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/92fded8386810f08c7a2b5841a235492.png)'
- en: Figure 1\. Architecture of proposed KD approach using prediction probabilities
    as soft-labels from teacher model and forcing the student to achieve prediction
    probability by the fitting loss function. The procedure begins with a pre-trained
    teacher model from which we derive knowledge from class probabilities. This knowledge,
    along with the data, is then utilized to inform the training of a student model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 使用教师模型的预测概率作为软标签，并强制学生通过拟合损失函数实现预测概率的提出的KD方法的架构。该过程从一个预训练的教师模型开始，我们从中提取类别概率的知识。这些知识以及数据被用来指导学生模型的训练。
- en: KD enables the student model to attain performance comparable to the teacher
    model while demanding considerably fewer computational resources for training.
    The teacher model’s predicted probability outputs $\bm{p}$ provide valuable insights
    into its data interpretation. By minimizing the discrepancy between the probability
    outputs of the student and teacher models, the student model can effectively adopt
    the knowledge and insights of the teacher model. Consequently, despite having
    a simpler architecture and fewer computational resources, the student model can
    achieve performance comparable to that of the more complex teacher model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: KD使学生模型能够在训练所需的计算资源大大减少的情况下，达到与教师模型相当的性能。教师模型的预测概率输出$\bm{p}$为其数据解释提供了宝贵的见解。通过最小化学生模型和教师模型概率输出之间的差异，学生模型可以有效地采纳教师模型的知识和见解。因此，尽管学生模型具有更简单的架构和较少的计算资源，但它仍能实现与更复杂的教师模型相当的性能。
- en: In Fig. [1](#S3.F1 "Figure 1 ‣ 3.2\. Proposed Knowledge Distillation ‣ 3\. Methodology
    ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments"),
    we provide the architecture of the proposed KD method. With a well-performing,
    fine-tuned, large teacher model, given a new dataset, we run the teacher model
    on the dataset and extract the knowledge from the teacher model to guide the training
    of a more compact student model. In this study, we extract the class probabilities
    predicted by the teacher model as the knowledge to be transferred to the student
    model. Using both the knowledge from the teacher model and the data from the dataset,
    we train the student model based on optimization as in Equation ([4](#S3.E4 "In
    3.2\. Proposed Knowledge Distillation ‣ 3\. Methodology ‣ Knowledge Distillation
    of LLM for Automatic Scoring of Science Education Assessments")).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [1](#S3.F1 "图 1 ‣ 3.2\. 提出的知识蒸馏 ‣ 3\. 方法论 ‣ 用于科学教育评估自动评分的LLM知识蒸馏")中，我们展示了所提出的KD方法的架构。使用一个表现良好的、经过微调的大型教师模型，在给定新的数据集的情况下，我们在数据集上运行教师模型，并从教师模型中提取知识，以指导更紧凑的学生模型的训练。在这项研究中，我们提取了教师模型预测的类别概率作为要转移到学生模型的知识。利用来自教师模型的知识和数据集中的数据，我们基于优化训练学生模型，如公式
    ([4](#S3.E4 "在 3.2\. 提出的知识蒸馏 ‣ 3\. 方法论 ‣ 用于科学教育评估自动评分的LLM知识蒸馏")) 中所示。
- en: 4\. Experimental Setup
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验设置
- en: Our study investigates whether a significantly smaller neural network can effectively
    mimic the capabilities of a fine-tuned LLM through the proposed KD strategy. Additionally,
    the study explores how this approach can enhance model performance. We apply our
    proposed methodology across diverse datasets to train a compact model to achieve
    this goal. This model is then compared with the SOTA TinyBERT (Jiao et al., [2019](#bib.bib14))
    and a trained smaller ANN (Ghiassi et al., [2012](#bib.bib8)) to evaluate the
    performance in terms of accuracy and efficiency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究调查了通过提出的知识蒸馏（KD）策略，显著更小的神经网络是否能有效模拟精细调整的 LLM 的能力。此外，研究还探讨了这种方法如何提升模型性能。我们在多个数据集上应用了我们提出的方法论，训练一个紧凑模型以实现这一目标。然后将该模型与
    SOTA TinyBERT（Jiao et al., [2019](#bib.bib14)）和训练过的小型 ANN（Ghiassi et al., [2012](#bib.bib8)）进行比较，以评估其在准确性和效率方面的表现。
- en: 4.1\. Data Collection and Preprocessing
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 数据收集与预处理
- en: The study utilized a meticulously categorized dataset of student-written responses
    to a science question and three mathematical assessment items, each falling under
    the multi-class category for automatic scoring. Each student response in datasets
    is graded by a human expert for automatic scoring, and human scores are used for
    validation. On average, each student’s written textual response contains 15 words.
    The detailed composition of each assessment item’s dataset is presented in Table
    [1](#S4.T1 "Table 1 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments").
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 该研究利用了一个经过精心分类的学生书面回应数据集，涵盖了一个科学问题和三个数学评估项目，每个项目都属于自动评分的多类类别。数据集中的每个学生回应都由人类专家评分用于自动评分，并使用人类分数进行验证。平均而言，每个学生的书面文本回应包含
    15 个单词。每个评估项目的数据集的详细组成如表 [1](#S4.T1 "表 1 ‣ 4.1\. 数据收集与预处理 ‣ 4\. 实验设置 ‣ 知识蒸馏用于科学教育评估的自动评分")
    所示。
- en: '| Dataset | Sample size | Teacher | Student |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 样本大小 | 教师 | 学生 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Bathtub | 1,145 | BERT_base (110M) | E-LSTM (0.03M) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 浴缸 | 1,145 | BERT_base (110M) | E-LSTM (0.03M) |'
- en: '| 7T | 6,684 | SciEdBERT (114M) | E-LSTM (0.03M) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 7T | 6,684 | SciEdBERT (114M) | E-LSTM (0.03M) |'
- en: '| Falling Weights | 1,148 | BERT_base (110M) | E-LSTM (0.03M) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 下降重量 | 1,148 | BERT_base (110M) | E-LSTM (0.03M) |'
- en: '| Gelatin | 1,142 | BERT_base (110M) | E-LSTM (0.03M) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 明胶 | 1,142 | BERT_base (110M) | E-LSTM (0.03M) |'
- en: Table 1\. Sample size and the teacher and student model used for each dataset.
    The number of parameters for each model is shown in parentheses.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 每个数据集的样本大小及所用教师模型和学生模型。每个模型的参数数量以括号中的数字表示。
- en: 'Dataset Overview:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集概述：
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bathtub: We utilized the dataset for the mathematically complex dataset for
    the bathtub assessment item, which consists of 1,145 student-written responses.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 浴缸：我们使用了用于数学复杂数据集的浴缸评估项目的数据集，该数据集包含 1,145 名学生书面回应。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '7T: A large dataset consisting of seven tasks from the SR1 dataset, including
    short constructed student responses and human-expert graded scores. Overall, the
    7T dataset consists of 6,684 labeled student responses from (Zhai et al., [2022](#bib.bib36)),
    similar to the dataset used for SciEdBERT by Liu et al. (Liu et al., [2023a](#bib.bib26)).'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 7T：一个大型数据集，由 SR1 数据集中的七个任务组成，包括简短的学生构造响应和人类专家评分的分数。总体而言，7T 数据集包含 6,684 条标记的学生响应（Zhai
    et al., [2022](#bib.bib36)），类似于 Liu et al.（Liu et al., [2023a](#bib.bib26)）用于
    SciEdBERT 的数据集。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Falling Weights: We also have taken the student response dataset for challenging
    mathematical thinking assessment item for falling weight similar to the data used
    by Latif & Zhai (Latif and Zhai, [2023b](#bib.bib17)), which consists of 1,148
    student written and human-expert graded scores.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下降重量：我们还使用了学生响应数据集，以评估挑战性数学思维评估项目的下降重量，这与 Latif & Zhai（Latif 和 Zhai，[2023b](#bib.bib17)）使用的数据类似，该数据集包含
    1,148 名学生书面作答及人类专家评分的分数。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Gelatin: Another dataset for gelatin assessment items contains 1,142 student
    responses, considered samples to train teacher models and distill knowledge to
    student models.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 明胶：另一个明胶评估项目的数据集包含 1,142 名学生回应，作为训练教师模型和将知识蒸馏到学生模型的样本。
- en: '|  | Accuracy |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确性 |'
- en: '| --- | --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|  | Teacher | TinyBERT | ANN | KD |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 教师 | TinyBERT | ANN | KD |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Bathtub | 0.938 | 0.839$\pm$0.019* |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 浴缸 | 0.938 | 0.839$\pm$0.019* |'
- en: '| 7T | 0.891 | 0.761$\pm$0.011* |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 7T | 0.891 | 0.761$\pm$0.011* |'
- en: '| Falling Weights | 0.904 | 0.829$\pm$0.014* |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 下降重量 | 0.904 | 0.829$\pm$0.014* |'
- en: '| Gelatin | 0.871 | 0.784$\pm$0.022* |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 明胶 | 0.871 | 0.784$\pm$0.022* |'
- en: '| * KD has shown higher accuracy than TinyBERT and Original NN and is comparable
    to the Teacher model for each dataset. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *KD在准确性上表现优于TinyBERT和原始神经网络，并且在每个数据集上的表现与教师模型相当。*'
- en: Table 2\. Accuracy performance comparison of teacher, TinyBERT (Jiao et al.,
    [2019](#bib.bib14)), ANN (Ghiassi et al., [2012](#bib.bib8)), and KD model for
    benchmark datasets. The mean accuracies are displayed, and the standard deviations
    are shown in parentheses.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 教师模型、TinyBERT（Jiao等，[2019](#bib.bib14)）、ANN（Ghiassi等，[2012](#bib.bib8)）和KD模型在基准数据集上的准确性性能比较。显示了平均准确率，标准偏差以括号形式表示。
- en: This comprehensive dataset facilitated a nuanced analysis of the capacity of
    the compact scoring models for student-written responses, ensuring robust and
    broadly applicable study findings. We processed each dataset by excluding empty
    responses and ensuring text-format student responses and ranged labels.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这个综合数据集促进了对紧凑评分模型在学生书面回应能力方面的细致分析，确保了研究结果的稳健性和广泛适用性。我们处理了每个数据集，排除了空白回应，并确保了文本格式的学生回应和范围标签。
- en: 4.2\. Training Scheme
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 训练方案
- en: 4.2.1\. Model Setup
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 模型设置
- en: This study uses SciEdBERT (Liu et al., [2023a](#bib.bib26)) with 114M parameters
    as a specialized Science Education BERT model, and the standard BERT base model
    (Devlin et al., [2018](#bib.bib6)) contains 110M parameters as the teacher model.
    These models have been shown to perform brilliantly in processing textual data.
    For performance comparison evaluation, we used TinyBERT (Jiao et al., [2019](#bib.bib14))
    with 67M parameters. For the KD method, we construct a compact neural network
    with an embedding layer with an output dimension of 32 and a bidirectional LSTM
    layer with 16 units, followed by a GlobalMaxPooling1D layer. Further, it includes
    two dense layers, with the first having 16 neurons and ’selu’ activation, and
    the final layer is equipped with a softmax activation for multi-class classification.
    Additionally, dropout layers are integrated for regularization, and the model
    is optimized with Adam.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用SciEdBERT（Liu等，[2023a](#bib.bib26)），其参数量为1.14亿，作为专门的科学教育BERT模型，而标准的BERT基础模型（Devlin等，[2018](#bib.bib6)）包含1.10亿个参数，作为教师模型。这些模型在处理文本数据方面表现出色。为了进行性能比较评估，我们使用了TinyBERT（Jiao等，[2019](#bib.bib14)），其参数量为6700万。对于KD方法，我们构建了一个紧凑的神经网络，具有一个输出维度为32的嵌入层和一个包含16个单元的双向LSTM层，后面跟随一个GlobalMaxPooling1D层。此外，还包括两个密集层，第一个有16个神经元和‘selu’激活，最后一层配备了用于多类分类的softmax激活。此外，还集成了dropout层进行正则化，并使用Adam优化模型。
- en: 4.2.2\. Evaluation and Validation
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 评估与验证
- en: We partition each dataset into training, validation, and testing sets in a $7:1:2$
    ratio. The model optimization employs cross-entropy loss, and to prevent overfitting,
    an early stopping callback, which monitors the validation loss, is utilized. We
    present the prediction accuracy on the test set to assess the model’s performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将每个数据集分为训练集、验证集和测试集，比例为$7:1:2$。模型优化采用交叉熵损失，为了防止过拟合，我们使用了一个监控验证损失的早停回调。我们通过测试集上的预测准确率来评估模型的性能。
- en: The summary of the dataset and the teacher and student (KD) models used for
    each dataset is detailed in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Data Collection
    and Preprocessing ‣ 4\. Experimental Setup ‣ Knowledge Distillation of LLM for
    Automatic Scoring of Science Education Assessments"). We provide the number of
    parameters for each model in parentheses. The student model is much smaller than
    the teacher model.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集总结以及用于每个数据集的教师模型和学生（KD）模型的详细信息见表[1](#S4.T1 "表1 ‣ 4.1\. 数据收集与预处理 ‣ 4\. 实验设置
    ‣ 知识蒸馏用于科学教育评估的自动评分")。我们提供了每个模型的参数数量（见括号）。学生模型比教师模型小得多。
- en: 4.3\. Results
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 结果
- en: The comparative analysis of model accuracy across four datasets is presented
    in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments"). Results reveal the efficacy of KD in enhancing the performance
    of a student model as compared to the SOTA TinyBERT (Jiao et al., [2019](#bib.bib14))
    and ANN (Ghiassi et al., [2012](#bib.bib8)) for text classification. Further,
    it also provides close accuracy as a complex teacher model. The 7T dataset serves
    as a typical example, with KD providing performance comparable to the teacher
    model, suggesting that even models with much smaller sizes can achieve similar
    performance to the large teacher model. We observed that KD has achieved 1% and
    4% higher accuracy than TinyBERT and ANN, respectively, which highlights the outperformance
    of KD to SOTA model distillation approaches. Considering both accuracy (shown
    in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments")) and model size (shown in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Data
    Collection and Preprocessing ‣ 4\. Experimental Setup ‣ Knowledge Distillation
    of LLM for Automatic Scoring of Science Education Assessments")), results highlight
    the practicality and applicability of the KD approach for automatic scoring on
    resource constrained-devices. This improvement delineates the potential of KD
    to augment model capabilities, particularly in scenarios where the SOTA approaches
    may not fully capture the underlying patterns in the data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S4.T2 "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental
    Setup ‣ Knowledge Distillation of LLM for Automatic Scoring of Science Education
    Assessments")展示了在四个数据集上模型准确度的比较分析。结果揭示了KD在提升学生模型性能方面的有效性，与SOTA TinyBERT（Jiao et
    al., [2019](#bib.bib14)）和ANN（Ghiassi et al., [2012](#bib.bib8)）相比，在文本分类中KD也提供了接近复杂教师模型的准确度。7T数据集作为一个典型示例，KD提供了与教师模型相当的性能，表明即使是尺寸更小的模型也可以实现与大型教师模型相似的性能。我们观察到，KD比TinyBERT和ANN的准确度分别提高了1%和4%，突显了KD在超越SOTA模型蒸馏方法方面的表现。考虑到准确度（见表[2](#S4.T2
    "Table 2 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental Setup ‣
    Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments")）和模型大小（见表[1](#S4.T1
    "Table 1 ‣ 4.1\. Data Collection and Preprocessing ‣ 4\. Experimental Setup ‣
    Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments")），结果突显了KD方法在资源受限设备上的自动评分的实用性和适用性。这一改进描绘了KD在增强模型能力方面的潜力，特别是在SOTA方法可能无法完全捕捉数据中潜在模式的情况下。
- en: Despite the success of KD, it is essential to recognize that the student models,
    although improved, usually do not reach the performance benchmark set by the teacher
    models. This is notably apparent in the Gelatin and Falling Weights datasets;
    the integration of KD leads to better performance compared to the ANN and TinyBERT
    but still does not match the teacher models’ accuracy. Such a discrepancy can
    be attributed to the inherent limitations of the student models, which possess
    simpler architectures and are trained on smaller datasets with minimal training
    parameters.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知识蒸馏（KD）取得了成功，但必须认识到，虽然学生模型有所改进，但通常无法达到教师模型设定的性能基准。这在明胶和掉重数据集中尤为明显；KD的整合导致比ANN和TinyBERT更好的性能，但仍未达到教师模型的准确度。这种差距可以归因于学生模型固有的局限性，它们拥有更简单的架构，并且在较小的数据集上进行训练，训练参数也较少。
- en: The results show that the KD strategy is a powerful tool in model training,
    beneficial for applications like automatic scoring. By effectively condensing
    the knowledge of a large, pre-trained model into a more compact one, KD improves
    performance and facilitates the deployment of such a model in resource-constrained
    environments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，KD策略是模型训练中一个强大的工具，对自动评分等应用有利。通过有效地将大型预训练模型的知识浓缩到更紧凑的模型中，KD提高了性能，并促进了这种模型在资源受限环境中的部署。
- en: 4.4\. Sensitivity Analysis
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 敏感性分析
- en: We investigated the impact of the hyperparameter $\lambda$, demonstrating consistent
    performance throughout a range of hyperparameter values.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了超参数$\lambda$的影响，展示了在一系列超参数值下的稳定性能。
- en: 5\. Discussion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 讨论
- en: The results of this study highlight the revolutionary possibilities of KD in
    educational technology, especially in light of the limitations of standard school
    computing resources. The use of KD in education represents a substantial breakthrough,
    particularly in automated grading systems. But like any emerging technology, it’s
    important to recognize its limitations as well as its potential for development
    in the future.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的结果突显了 KD 在教育技术中的革命性潜力，特别是在标准学校计算资源的局限性面前。KD 在教育中的应用代表了一项重大突破，尤其是在自动评分系统方面。但像任何新兴技术一样，认识到其局限性以及未来的发展潜力也很重要。
- en: 5.1\. Application of KD in Education
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. KD 在教育中的应用
- en: The most noteworthy use of KD in education is in creating accurate and productive
    automatic scoring systems. A major challenge in many educational contexts is that
    traditional scoring systems can demand extensive processing resources to function
    successfully on school-setting devices such as entry-level laptops and tablets.
    This problem is addressed by KD, which makes it possible to create ”student models”
    with significantly lower processing requirements while maintaining a great deal
    of the accuracy and efficiency of bigger ”teacher models.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: KD 在教育中的最显著应用是创建准确且高效的自动评分系统。在许多教育环境中，一个主要挑战是传统评分系统可能需要大量的处理资源才能在学校设备（如入门级笔记本电脑和平板电脑）上成功运行。KD
    通过使得创建处理需求显著较低的“学生模型”成为可能，同时保持较大“教师模型”的准确性和效率，解决了这一问题。
- en: When combined with automatic scoring, these distilled models can give students
    offline fast, reliable, and objective feedback—an essential feature of offline
    adaptive learning environments without needing high processing and an internet
    connection. Accurate and timely feedback is crucial for creating a personalized
    and interesting learning environment, which contemporary educational environments
    are calling for more and more.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当与自动评分系统结合时，这些精炼的模型可以为学生提供离线快速、可靠和客观的反馈——这是离线适应性学习环境中的一个重要特性，无需高处理能力和互联网连接。准确和及时的反馈对于创建个性化和有趣的学习环境至关重要，这也是当代教育环境越来越呼唤的。
- en: Furthermore, KD models are perfect for integrating tablet- and smartphone-based
    learning apps due to their smaller size and lower processing requirements. The
    capacity to run complex AI models on these devices, which are increasingly prevalent
    in educational contexts, creates new opportunities for interactive and adaptable
    learning experiences.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 KD 模型体积较小、处理需求较低，非常适合整合到基于平板和智能手机的学习应用中。在这些设备上运行复杂的 AI 模型的能力，为日益普及的教育环境创造了互动和适应性学习体验的新机会。
- en: 5.2\. Limitations of KD in Education
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. KD 在教育中的局限性
- en: While KD has many advantages, it’s important to recognize its limitations. First
    off, even if KD helps the student models perform better, they frequently fall
    short of the more sophisticated teacher models. This disparity could result in
    a little decline in the accuracy of the assessment or the quality of the feedback,
    which could be crucial in some educational settings.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 KD 有许多优点，但也需要认识到其局限性。首先，即使 KD 有助于学生模型的性能，它们通常仍不及更复杂的教师模型。这种差距可能导致评估的准确性或反馈的质量有所下降，这在某些教育环境中可能至关重要。
- en: Furthermore, the caliber and applicability of the data utilized to train the
    teacher model determine how effective KD is. It is essential to ensure the teacher
    model is trained on representative and extensive data sets in educational contexts,
    where data might be complex and varied. This need can be difficult, especially
    when it comes to themes that demand a profound comprehension of context or are
    subjective.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，用于训练教师模型的数据的质量和适用性决定了 KD 的效果。确保教师模型在具有代表性和广泛的数据集上进行训练在教育环境中至关重要，因为数据可能复杂多样。这种需求可能很困难，尤其是在需要深刻理解上下文或具有主观性的主题中。
- en: 5.3\. Future Directions
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 未来方向
- en: 'Looking ahead, there are several avenues for further research and development
    to overcome the aforementioned limitations in the application of KD in education:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，有几个研究和发展的方向可以克服上述 KD 在教育应用中的局限性：
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Soft label processing: In the proposed approach, we directly applied soft labels
    from the teacher model and fit the prediction the accuracy using the loss function;
    however, if the soft labels are from false positives, that can eventually affect
    the performance of the student model. Hence, processing soft labels through sophisticated
    validation techniques can overcome this limitation.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软标签处理：在提出的方法中，我们直接应用了来自教师模型的软标签，并使用损失函数来调整预测准确性；然而，如果软标签来自假阳性，这可能最终会影响学生模型的性能。因此，通过复杂的验证技术处理软标签可以克服这一限制。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Expanding Application Areas: Beyond automatic scoring, KD can be applied to
    other areas of education technology, such as personalized content recommendation,
    language translation for multilingual education, and interactive simulations for
    complex subjects.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩展应用领域：除了自动评分，知识蒸馏（KD）还可以应用于教育技术的其他领域，如个性化内容推荐、多语言教育的语言翻译，以及复杂学科的互动模拟。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Customizable and Adaptive Models: Future research endeavors may concentrate
    on constructing small KD models adaptable to particular learning environments.
    These models might modify their actions in response to the student’s development,
    learning preferences, and academic requirements.'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可定制和自适应模型：未来的研究可能会集中于构建适应特定学习环境的小型KD模型。这些模型可能会根据学生的发展、学习偏好和学术需求来调整其行为。
- en: Even though knowledge distillation offers a promising way to improve educational
    technologies, particularly in settings with limited resources, further research,
    and development are needed to realize its full potential. Harnessing KD’s full
    potential in education will require striking a balance between performance and
    practicality, resolving ethical issues, and continuously adapting to the changing
    educational scene.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管知识蒸馏为改善教育技术，特别是在资源有限的环境中，提供了一种有前景的方法，但仍需进一步的研究和开发以实现其全部潜力。充分发挥KD在教育中的潜力将需要在性能和实用性之间取得平衡，解决伦理问题，并不断适应不断变化的教育环境。
- en: 6\. Conclusion
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: This study effectively illustrates how Knowledge Distillation (KD) can be used
    to optimize Large Language Models (LLMs) for usage in educational technology,
    especially on low-processor devices. We maintain great accuracy 85% with a much
    smaller model size (0.02M parameters) and processing requirements by condensing
    the knowledge of LLMs into smaller neural networks. The distilled models perform
    better than SOTA TinyBERT and ANN models on various datasets, demonstrating the
    efficacy of this approach even though their parameter sizes are up to 100 times
    less than teacher models. This work has important applications since it provides
    a method to incorporate cutting-edge AI tools into conventional school environments,
    which frequently have hardware constraints. The learning process and accessibility
    of personalized education technology can be significantly improved by the capacity
    to implement effective and precise AI models for uses such as autonomous scoring.
    Essentially, this work establishes the foundation for future developments in the
    field and validates the viability of KD in educational contexts, underscoring
    the significance of ongoing research and innovation in AI for education. In the
    future, we will work on processing soft-labels and prompt processing to avoid
    amplification of faults of teacher models by employing more sophisticated techniques.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究有效地展示了知识蒸馏（KD）如何用于优化大语言模型（LLMs）在教育技术中的应用，尤其是在低处理器设备上。通过将LLMs的知识浓缩到更小的神经网络中，我们在模型大小（0.02M
    参数）和处理需求上保持了85%的高准确率。蒸馏模型在各种数据集上表现优于SOTA TinyBERT和ANN模型，尽管它们的参数量比教师模型少多达100倍。这项工作具有重要应用，因为它提供了一种将尖端AI工具融入常规学校环境的方法，这些环境通常存在硬件限制。通过实施有效和精确的AI模型用于如自动评分等应用，可以显著改善个性化教育技术的学习过程和可及性。本质上，这项工作为该领域的未来发展奠定了基础，并验证了KD在教育环境中的可行性，强调了在教育AI领域持续研究和创新的重要性。未来，我们将致力于处理软标签和提示处理，以避免通过采用更复杂的技术来放大教师模型的缺陷。
- en: Acknowledgment
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This study secondary analyzed data from projects supported by the National Science
    Foundation (grant numbers 2101104, PI Zhai) and the Institute of Education Sciences
    (grant number R305A160219, PI Liu). The authors acknowledge the funding agencies
    and the project teams for making the data available for analysis. The findings,
    conclusions, or opinions herein represent the views of the authors and do not
    necessarily represent the views of personnel affiliated with the funding agencies.
    LF and PM were partially supported by National Science Foundation under grants
    DMS-1903226, DMS-1925066, DMS-2124493, DMS-2311297, DMS-2319279.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究对国家科学基金会（资助编号 2101104，负责人 Zhai）和教育科学研究所（资助编号 R305A160219，负责人 Liu）支持的项目数据进行了二次分析。作者感谢资助机构和项目团队提供的数据用于分析。本文中的发现、结论或意见代表作者的观点，并不一定代表资助机构相关人员的观点。LF
    和 PM 部分得到了国家科学基金会资助，资助编号 DMS-1903226、DMS-1925066、DMS-2124493、DMS-2311297、DMS-2319279。
- en: References
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （1）
- en: Bertolini et al. (2023) Lorenzo Bertolini, Valentina Elce, Adriana Michalak,
    Giulio Bernardi, and Julie Weeds. 2023. Automatic Scoring of Dream Reports’ Emotional
    Content with Large Language Models. *arXiv preprint arXiv:2302.14828* (2023).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝尔托利尼等人（2023）Lorenzo Bertolini、Valentina Elce、Adriana Michalak、Giulio Bernardi
    和 Julie Weeds。2023年。利用大型语言模型自动评分梦境报告的情感内容。*arXiv 预印本 arXiv:2302.14828*（2023年）。
- en: 'Bhunia et al. (2021) Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury,
    and Yi-Zhe Song. 2021. Text is text, no matter what: Unifying text recognition
    using knowledge distillation. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*. 983–992.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布赫尼亚等人（2021）Ayan Kumar Bhunia、Aneeshan Sain、Pinaki Nath Chowdhury 和 Yi-Zhe Song。2021年。文本就是文本，无论如何：使用知识蒸馏统一文本识别。发表于
    *IEEE/CVF 国际计算机视觉大会论文集*。983–992。
- en: 'Carlini et al. (2019) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. 2019. The secret sharer: Evaluating and testing unintended
    memorization in neural networks. In *28th USENIX Security Symposium (USENIX Security
    19)*. 267–284.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡尔尼等人（2019）Nicholas Carlini、Chang Liu、Úlfar Erlingsson、Jernej Kos 和 Dawn Song。2019年。秘密分享者：评估和测试神经网络中的无意记忆。发表于
    *第28届 USENIX 安全研讨会（USENIX Security 19）*。267–284。
- en: Chuang et al. (2020) Yung-Sung Chuang, Shang-Yu Su, and Yun-Nung Chen. 2020.
    Lifelong language knowledge distillation. *arXiv preprint arXiv:2010.02123* (2020).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等人（2020）Yung-Sung Chuang、Shang-Yu Su 和 Yun-Nung Chen。2020年。终身语言知识蒸馏。*arXiv
    预印本 arXiv:2010.02123*（2020年）。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德夫林等人（2018）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。2018年。BERT：用于语言理解的深度双向变换器预训练。*arXiv
    预印本 arXiv:1810.04805*（2018年）。
- en: Fang et al. (2023) Luyang Fang, Gyeong-Geon Lee, and Xiaoming Zhai. 2023. Using
    gpt-4 to augment unbalanced data for automatic scoring. *arXiv preprint arXiv:2310.18365*
    (2023).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方等人（2023）Luyang Fang、Gyeong-Geon Lee 和 Xiaoming Zhai。2023年。使用 GPT-4 增强不平衡数据以进行自动评分。*arXiv
    预印本 arXiv:2310.18365*（2023年）。
- en: Ghiassi et al. (2012) Manoochehr Ghiassi, Michael Olschimke, Brian Moon, and
    Paul Arnaudo. 2012. Automated text classification using a dynamic artificial neural
    network model. *Expert Systems with Applications* 39, 12 (2012), 10967–10976.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉亚西等人（2012）Manoochehr Ghiassi、Michael Olschimke、Brian Moon 和 Paul Arnaudo。2012年。使用动态人工神经网络模型的自动文本分类。*专家系统与应用*
    39, 12（2012年），10967–10976。
- en: 'González-Calatayud et al. (2021) Víctor González-Calatayud, Paz Prendes-Espinosa,
    and Rosabel Roig-Vila. 2021. Artificial intelligence for student assessment: A
    systematic review. *Applied Sciences* 11, 12 (2021), 5467.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冯-卡拉塔尤德等人（2021）Víctor González-Calatayud、Paz Prendes-Espinosa 和 Rosabel Roig-Vila。2021年。用于学生评估的人工智能：系统评价。*应用科学*
    11, 12（2021年），5467。
- en: González-Carvajal and Garrido-Merchán (2020) Santiago González-Carvajal and
    Eduardo C Garrido-Merchán. 2020. Comparing BERT against traditional machine learning
    text classification. *arXiv preprint arXiv:2005.13012* (2020).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 冈萨雷斯-卡尔瓦哈尔 和 加里多-梅尔尚（2020）Santiago González-Carvajal 和 Eduardo C Garrido-Merchán。2020年。将
    BERT 与传统机器学习文本分类进行比较。*arXiv 预印本 arXiv:2005.13012*（2020年）。
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Knowledge
    Distillation of Large Language Models. *arXiv preprint arXiv:2306.08543* (2023).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顾等人（2023）Yuxian Gu、Li Dong、Furu Wei 和 Minlie Huang。2023年。大型语言模型的知识蒸馏。*arXiv
    预印本 arXiv:2306.08543*（2023年）。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. *arXiv preprint arXiv:1503.02531* (2015).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辛顿等人（2015）Geoffrey Hinton、Oriol Vinyals 和 Jeff Dean。2015年。提炼神经网络中的知识。*arXiv
    预印本 arXiv:1503.02531*（2015年）。
- en: Holmes and Tuomi (2022) Wayne Holmes and Ilkka Tuomi. 2022. State of the art
    and practice in AI in education. *European Journal of Education* 57, 4 (2022),
    542–570.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holmes 和 Tuomi (2022) Wayne Holmes 和 Ilkka Tuomi. 2022. 人工智能在教育中的前沿和实践。*European
    Journal of Education* 57, 4 (2022), 542–570。
- en: 'Jiao et al. (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen,
    Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for natural
    language understanding. *arXiv preprint arXiv:1909.10351* (2019).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao 等 (2019) Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin
    Li, Fang Wang, 和 Qun Liu. 2019. Tinybert：为自然语言理解精炼 BERT。*arXiv 预印本 arXiv:1909.10351*
    (2019)。
- en: Latif et al. (2023a) Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu,
    Ninghao Liu, Guoyu Lu, Sheng Li, Tianming Liu, and Xiaoming Zhai. 2023a. Artificial
    general intelligence (AGI) for education. *arXiv preprint arXiv:2304.12479* (2023).
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 等 (2023a) Ehsan Latif, Gengchen Mai, Matthew Nyaaba, Xuansheng Wu, Ninghao
    Liu, Guoyu Lu, Sheng Li, Tianming Liu, 和 Xiaoming Zhai. 2023a. 教育中的人工通用智能 (AGI)。*arXiv
    预印本 arXiv:2304.12479* (2023)。
- en: Latif and Zhai (2023a) Ehsan Latif and Xiaoming Zhai. 2023a. Automatic Scoring
    of Students’ Science Writing Using Hybrid Neural Network. *arXiv preprint arXiv:2312.03752*
    (2023).
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 和 Zhai (2023a) Ehsan Latif 和 Xiaoming Zhai. 2023a. 使用混合神经网络对学生的科学写作进行自动评分。*arXiv
    预印本 arXiv:2312.03752* (2023)。
- en: Latif and Zhai (2023b) Ehsan Latif and Xiaoming Zhai. 2023b. Fine-tuning chatgpt
    for automatic scoring. *arXiv preprint arXiv:2310.10072* (2023).
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 和 Zhai (2023b) Ehsan Latif 和 Xiaoming Zhai. 2023b. 微调 ChatGPT 以实现自动评分。*arXiv
    预印本 arXiv:2310.10072* (2023)。
- en: 'Latif et al. (2023b) Ehsan Latif, Xiaoming Zhai, and Lei Liu. 2023b. AI Gender
    Bias, Disparities, and Fairness: Does Training Data Matter? *arXiv preprint arXiv:2312.10833*
    (2023).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 等 (2023b) Ehsan Latif, Xiaoming Zhai, 和 Lei Liu. 2023b. 人工智能性别偏见、不平等和公平性：训练数据是否重要？*arXiv
    预印本 arXiv:2312.10833* (2023)。
- en: 'Lee et al. (2023a) Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, and Xiaoming Zhai.
    2023a. Gemini Pro Defeated by GPT-4V: Evidence from Education. *arXiv preprint
    arXiv:2401.08660* (2023).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2023a) Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, 和 Xiaoming Zhai. 2023a.
    Gemini Pro 被 GPT-4V 击败：来自教育的证据。*arXiv 预印本 arXiv:2401.08660* (2023)。
- en: Lee et al. (2023b) Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu,
    and Xiaoming Zhai. 2023b. Applying Large Language Models and Chain-of-Thought
    for Automatic Scoring. *arXiv preprint arXiv:2312.03748* (2023).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2023b) Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, 和 Xiaoming
    Zhai. 2023b. 应用大型语言模型和思维链进行自动评分。*arXiv 预印本 arXiv:2312.03748* (2023)。
- en: 'Lee et al. (2023c) Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne
    Bewersdorf, Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang, et al.
    2023c. Multimodality of AI for Education: Towards Artificial General Intelligence.
    *arXiv preprint arXiv:2312.06037* (2023).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 (2023c) Gyeong-Geon Lee, Lehong Shi, Ehsan Latif, Yizhu Gao, Arne Bewersdorf,
    Matthew Nyaaba, Shuchen Guo, Zihao Wu, Zhengliang Liu, Hui Wang 等. 2023c. 教育中的多模态人工智能：迈向人工通用智能。*arXiv
    预印本 arXiv:2312.06037* (2023)。
- en: Li et al. (2023) Jiazheng Li, Lin Gui, Yuxiang Zhou, David West, Cesare Aloisi,
    and Yulan He. 2023. Distilling ChatGPT for Explainable Automated Student Answer
    Assessment. *arXiv preprint arXiv:2305.12962* (2023).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Jiazheng Li, Lin Gui, Yuxiang Zhou, David West, Cesare Aloisi, 和
    Yulan He. 2023. 精炼 ChatGPT 以实现可解释的自动学生回答评估。*arXiv 预印本 arXiv:2305.12962* (2023)。
- en: Li and Li (2021) Yongqi Li and Wenjie Li. 2021. Data distillation for text classification.
    *arXiv preprint arXiv:2104.08448* (2021).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Li (2021) Yongqi Li 和 Wenjie Li. 2021. 文本分类的数据精炼。*arXiv 预印本 arXiv:2104.08448*
    (2021)。
- en: 'Liang et al. (2023) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark,
    Xiangliang Zhang, and Ashwin Kaylan. 2023. Let GPT be a Math Tutor: Teaching Math
    Word Problem Solvers with Customized Exercise Generation. *arXiv preprint arXiv:2305.14386*
    (2023).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等 (2023) Zhenwen Liang, Wenhao Yu, Tanmay Rajpurohit, Peter Clark, Xiangliang
    Zhang, 和 Ashwin Kaylan. 2023. 让 GPT 成为数学辅导员：通过定制练习生成来教授数学词题解决者。*arXiv 预印本 arXiv:2305.14386*
    (2023)。
- en: Liu et al. (2023b) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023b. Calibrating LLM-Based
    Evaluator. *arXiv preprint arXiv:2309.13308* (2023).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023b) Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen
    Huang, Furu Wei, Weiwei Deng, Feng Sun, 和 Qi Zhang. 2023b. 校准基于 LLM 的评估器。*arXiv
    预印本 arXiv:2309.13308* (2023)。
- en: 'Liu et al. (2023a) Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, and Xiaoming
    Zhai. 2023a. Context matters: A strategy to pre-train language model for science
    education. *arXiv preprint arXiv:2301.12031* (2023).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023a) Zhengliang Liu, Xinyu He, Lei Liu, Tianming Liu, 和 Xiaoming Zhai.
    2023a. 上下文的重要性：为科学教育预训练语言模型的策略。*arXiv 预印本 arXiv:2301.12031* (2023)。
- en: 'Sahu et al. (2023) Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, and Issam H
    Laradji. 2023. Promptmix: A class boundary augmentation method for large language
    model distillation. *arXiv preprint arXiv:2310.14192* (2023).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahu 等（2023）Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, 和 Issam H Laradji.
    2023. Promptmix：一种用于大语言模型蒸馏的类别边界增强方法。*arXiv 预印本 arXiv:2310.14192*（2023）。
- en: Schneider et al. (2023) Johannes Schneider, Bernd Schenk, Christina Niklaus,
    and Michaelis Vlachos. 2023. Towards LLM-based Autograding for Short Textual Answers.
    *arXiv preprint arXiv:2309.11508* (2023).
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schneider 等（2023）Johannes Schneider, Bernd Schenk, Christina Niklaus, 和 Michaelis
    Vlachos. 2023. 向基于 LLM 的短文本自动评分迈进。*arXiv 预印本 arXiv:2309.11508*（2023）。
- en: 'Selwyn (2019) Neil Selwyn. 2019. *Should robots replace teachers?: AI and the
    future of education*. John Wiley & Sons.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selwyn（2019）Neil Selwyn. 2019. *机器人应取代教师吗？：AI 和教育的未来*。John Wiley & Sons。
- en: Wang et al. (2023) Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu,
    and Kam-Fai Wong. 2023. Chain-of-thought prompting for responding to in-depth
    dialogue questions with LLM. *arXiv preprint arXiv:2305.11792* (2023).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Hongru Wang, Rui Wang, Fei Mi, Zezhong Wang, Ruifeng Xu, 和 Kam-Fai
    Wong. 2023. 针对深入对话问题的链式思维提示与 LLM 的响应。*arXiv 预印本 arXiv:2305.11792*（2023）。
- en: Xu and Yang (2017) Ruochen Xu and Yiming Yang. 2017. Cross-lingual distillation
    for text classification. *arXiv preprint arXiv:1705.02073* (2017).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 和 Yang（2017）Ruochen Xu 和 Yiming Yang. 2017. 跨语言蒸馏用于文本分类。*arXiv 预印本 arXiv:1705.02073*（2017）。
- en: Yu et al. (2019) Shanshan Yu, Jindian Su, and Da Luo. 2019. Improving bert-based
    text classification with auxiliary sentence and domain knowledge. *IEEE Access*
    7 (2019), 176600–176612.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等（2019）Shanshan Yu, Jindian Su, 和 Da Luo. 2019. 改进基于 BERT 的文本分类：辅助手段与领域知识。*IEEE
    Access* 7（2019），176600–176612。
- en: 'Zhai (2022) Xiaoming Zhai. 2022. ChatGPT user experience: Implications for
    education. *Available at SSRN 4312418* (2022).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai（2022）Xiaoming Zhai. 2022. ChatGPT 用户体验：对教育的启示。*可在 SSRN 4312418 上获取*（2022）。
- en: Zhai (2023) Xiaoming Zhai. 2023. Using Gpt-4 to Augment Unbalanced Data for
    Automatic Scoring. *Available at SSRN* (2023).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai（2023）Xiaoming Zhai. 2023. 使用 GPT-4 增强不平衡数据以进行自动评分。*可在 SSRN 上获取*（2023）。
- en: Zhai et al. (2021a) Xuesong Zhai, Xiaoyan Chu, Ching Sing Chai, Morris Siu Yung
    Jong, Andreja Istenic, Michael Spector, Jia-Bao Liu, Jing Yuan, and Yan Li. 2021a.
    A Review of Artificial Intelligence (AI) in Education from 2010 to 2020. *Complexity*
    2021 (2021), 1–18.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等（2021a）Xuesong Zhai, Xiaoyan Chu, Ching Sing Chai, Morris Siu Yung Jong,
    Andreja Istenic, Michael Spector, Jia-Bao Liu, Jing Yuan, 和 Yan Li. 2021a. 2010
    到 2020 年间人工智能（AI）在教育中的回顾。*Complexity* 2021（2021），1–18。
- en: Zhai et al. (2022) Xiaoming Zhai, Peng He, and Joseph Krajcik. 2022. Applying
    machine learning to automatically assess scientific models. *Journal of Research
    in Science Teaching* 59, 10 (2022), 1765–1794.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等（2022）Xiaoming Zhai, Peng He, 和 Joseph Krajcik. 2022. 应用机器学习自动评估科学模型。*Journal
    of Research in Science Teaching* 59, 10（2022），1765–1794。
- en: 'Zhai and Nehm (2023) Xiaoming Zhai and Ross H Nehm. 2023. AI and formative
    assessment: The train has left the station. *Journal of Research in Science Teaching*
    (2023).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 和 Nehm（2023）Xiaoming Zhai 和 Ross H Nehm. 2023. AI 与形成性评估：列车已经离开车站。*Journal
    of Research in Science Teaching*（2023）。
- en: 'Zhai et al. (2021b) Xiaoming Zhai, Lehong Shi, and Ross H Nehm. 2021b. A meta-analysis
    of machine learning-based science assessments: Factors impacting machine-human
    score agreements. *Journal of Science Education and Technology* 30 (2021), 361–379.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等（2021b）Xiaoming Zhai, Lehong Shi, 和 Ross H Nehm. 2021b. 基于机器学习的科学评估的元分析：影响机器与人类评分一致性的因素。*Journal
    of Science Education and Technology* 30（2021），361–379。
- en: 'Zhai et al. (2020) Xiaoming Zhai, Yue Yin, James W Pellegrino, Kevin C Haudek,
    and Lehong Shi. 2020. Applying machine learning in science assessment: a systematic
    review. *Studies in Science Education* 56, 1 (2020), 111–151.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等（2020）Xiaoming Zhai, Yue Yin, James W Pellegrino, Kevin C Haudek, 和 Lehong
    Shi. 2020. 机器学习在科学评估中的应用：系统评述。*Studies in Science Education* 56, 1（2020），111–151。
- en: Zhang et al. (2022) Shaokang Zhang, Lei Jiang, and Jianlong Tan. 2022. Cross-domain
    knowledge distillation for text classification. *Neurocomputing* 509 (2022), 11–20.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Shaokang Zhang, Lei Jiang, 和 Jianlong Tan. 2022. 跨领域知识蒸馏用于文本分类。*Neurocomputing*
    509（2022），11–20。
- en: Zhou et al. (2023) Xuanhe Zhou, Guoliang Li, and Zhiyuan Liu. 2023. Llm as dba.
    *arXiv preprint arXiv:2308.05481* (2023).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Xuanhe Zhou, Guoliang Li, 和 Zhiyuan Liu. 2023. LLM 作为 DBA。*arXiv
    预印本 arXiv:2308.05481*（2023）。
