- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 压缩后再提示：通过可转移的提示提高LLM推理的准确性与效率的权衡
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11186](https://ar5iv.labs.arxiv.org/html/2305.11186)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.11186](https://ar5iv.labs.arxiv.org/html/2305.11186)
- en: Zhaozhuo Xu Equal contribution. The order of authors is determined by flipping
    a coin. Department of Computer Science, Rice University Zirui Liu^* Department
    of Computer Science, Rice University Beidi Chen Department of Electrical and Computer
    Engineering, Carnegie Mellon University Yuxin Tang Department of Computer Science,
    Rice University Jue Wang ETH Zürich, Switzerland Kaixiong Zhou Department of Computer
    Science, Rice University Xia Hu Department of Computer Science, Rice University
    Anshumali Shrivastava Department of Computer Science, Rice University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhaozhuo Xu 贡献相等。作者顺序由掷硬币决定。计算机科学系，莱斯大学 Zirui Liu^* 计算机科学系，莱斯大学 Beidi Chen 电气与计算机工程系，卡内基梅隆大学
    Yuxin Tang 计算机科学系，莱斯大学 Jue Wang ETH Zürich，瑞士 Kaixiong Zhou 计算机科学系，莱斯大学 Xia Hu
    计算机科学系，莱斯大学 Anshumali Shrivastava 计算机科学系，莱斯大学
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: While the numerous parameters in Large Language Models (LLMs) contribute to
    their superior performance, this massive scale makes them inefficient and memory-hungry.
    Thus, they are hard to deploy on commodity hardware, such as one single GPU. Given
    the memory and power constraints of such devices, model compression methods are
    widely employed to reduce both the model size and inference latency, which essentially
    trades off model quality in return for improved efficiency. Thus, optimizing this
    accuracy-efficiency trade-off is crucial for the LLM deployment on commodity hardware.
    In this paper, we introduce a new perspective to optimize this trade-off by prompting
    compressed models. Specifically, we first observe that for certain questions,
    the generation quality of a compressed LLM can be significantly improved by adding
    carefully designed hard prompts, though this isn’t the case for all questions.
    Based on this observation, we propose a soft prompt learning method where we expose
    the compressed model to the prompt learning process, aiming to enhance the performance
    of prompts. Our experimental analysis suggests our soft prompt strategy greatly
    improves the performance of the $8\times$ compressed LLaMA-7B model (with a joint
    4-bit quantization and 50% weight pruning compression), allowing them to match
    their uncompressed counterparts on popular benchmarks. Also, we demonstrate that
    these learned prompts can be transferred across various datasets, tasks, and compression
    levels. Hence with this transferability, we can stitch the soft prompt to a newly
    compressed model to improve the test-time accuracy in an “in-situ” way.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）中的众多参数提升了其性能，但这种大规模也使得它们效率低下且占用大量内存。因此，它们很难在普通硬件上部署，比如单个GPU。鉴于这些设备的内存和电力限制，模型压缩方法被广泛采用，以减少模型大小和推理延迟，这在本质上是以牺牲模型质量为代价来换取更高的效率。因此，优化这种准确性与效率的权衡对于LLM在普通硬件上的部署至关重要。在本文中，我们引入了一种通过提示压缩模型来优化这种权衡的新视角。具体而言，我们首先观察到，对于某些问题，通过添加精心设计的硬提示，可以显著改善压缩LLM的生成质量，尽管并非所有问题都是如此。基于这一观察，我们提出了一种软提示学习方法，通过将压缩模型暴露于提示学习过程中，旨在提升提示的性能。我们的实验分析表明，我们的软提示策略大大提高了$8\times$压缩的LLaMA-7B模型（具有联合4位量化和50%权重剪枝压缩）的性能，使其在流行基准上与未压缩的对手相匹配。此外，我们展示了这些学习到的提示可以在各种数据集、任务和压缩级别之间进行迁移。因此，凭借这种可迁移性，我们可以将软提示嵌入到新压缩的模型中，以“现场”方式提高测试时的准确性。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) [[27](#bib.bib27), [28](#bib.bib28), [2](#bib.bib2),
    [40](#bib.bib40), [33](#bib.bib33)] has revolutionized the field of Natural Language
    Processing (NLP). Notably, LLMs are known for their in-context learning ability,
    allowing them to generalize to unseen tasks without additional fine-tuning [[2](#bib.bib2)].
    Specifically, LLMs are controlled through user-provided natural language specifications
    of the task, or *prompts*, which illustrate how to complete a task. Equipped with
    the in-context learning ability, we only need to serve a single large model to
    efficiently handle different tasks. Despite of their remarkable adaptability,
    LLMs are very expensive to deploy [[3](#bib.bib3), [35](#bib.bib35)]. The inference
    process of LLMs, such as LLaMA 2 [[34](#bib.bib34)], may require multiple powerful
    GPUs, which is prohibitively expensive for the general community. Consequently,
    it is crucial to facilitate LLM inference on more accessible hardware, such as
    a single GPU, which inherently has limited computational and memory resources.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[27](#bib.bib27), [28](#bib.bib28), [2](#bib.bib2), [40](#bib.bib40),
    [33](#bib.bib33)] 已经彻底改变了自然语言处理（NLP）领域。尤其是，LLMs 以其在上下文中的学习能力而闻名，使它们能够在没有额外微调的情况下推广到未见过的任务
    [[2](#bib.bib2)]。具体而言，LLMs 通过用户提供的自然语言任务规格或 *提示* 来控制，这些提示展示了如何完成任务。凭借这种上下文学习能力，我们只需提供一个大型模型来高效处理不同的任务。尽管它们的适应能力非常出色，但部署
    LLMs 的成本非常高 [[3](#bib.bib3), [35](#bib.bib35)]。LLMs 的推理过程，例如 LLaMA 2 [[34](#bib.bib34)]，可能需要多个强大的
    GPU，这对一般社区来说成本过高。因此，必须促进在更易获取的硬件上进行 LLM 推理，如单个 GPU，这本质上具有有限的计算和内存资源。
- en: 'To address this problem, model compression methods are widely employed to reduce
    the model size and inference latency, such as quantization [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)] and pruning [[6](#bib.bib6)].
    These methods essentially trade off model quality in return for reduced latency
    and model size. Thus, there is an inevitable trade-off between accuracy and efficiency,
    resulting in a noticeable reduction in the model’s accuracy and, consequently,
    the overall performance benefits of LLMs. To get a sense, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), the full model (LLaMA-7B)
    is able to provide accurate answers to all three questions. However, the pruned
    model generates unrelated and off-topic answers to the same questions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，模型压缩方法被广泛采用，以减少模型大小和推理延迟，如量化 [[26](#bib.bib26), [4](#bib.bib4), [36](#bib.bib36),
    [7](#bib.bib7)] 和剪枝 [[6](#bib.bib6)]。这些方法实质上是在减少延迟和模型大小的同时，牺牲模型质量。因此，在准确性和效率之间存在不可避免的权衡，导致模型准确性显著降低，从而影响
    LLMs 的整体性能。通过图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")
    可以看出，完整模型（LLaMA-7B）能够准确回答所有三个问题。然而，经过剪枝的模型对相同的问题生成了无关和离题的回答。'
- en: Both model compression and prompts can influence the generation quality of LLMs.
    Thus intuitively, we can also utilize the prompt to help the compressed model
    generate more relevant answers. To the best of our knowledge, this perspective
    is not fully explored for LLMs. Thus one natural question is, *for a compressed
    model, can we design a prompt that helps it correct its predictions accordingly?*
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 模型压缩和提示都可以影响 LLMs 的生成质量。因此，我们可以直观地利用提示来帮助压缩模型生成更相关的回答。据我们所知，这一视角尚未完全探索。因此，一个自然的问题是，*对于一个压缩模型，我们能否设计一个提示来帮助其相应地纠正预测？*
- en: 'In this paper, we provide the first affirmative answer to the above question.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    we manually attach the prompt “*Please carefully examine the weight matrix within
    the model, as it may contain errors. It is crucial to verify its accuracy and
    make any necessary adjustments to ensure optimal performance*” to the original
    question. The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard
    Prompt” in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    shows a significant improvement in its responses, although not all of them are
    accurate or complete. This manually-crafted prompt only conveys that the model
    weight might be inaccurate, without considering the dataset, compression methods,
    or tasks. This finding highlights the considerable potential for the transferability
    of this “hard prompt” across datasets, compression levels, and tasks. Despite
    the potential, this manually designed prompt is not consistently effective. Inspired
    by previous learnable prompt works [[19](#bib.bib19), [18](#bib.bib18)], we hypothesize
    that by involving the compressed weight in the prompt learning process, a learnable
    prompt could potentially surpass the performance of the manually-designed prompt,
    while maintaining the transferability. Building upon this insight, we introduce
    a paradigm of prompt learning that seeks to train additive prompt tokens on a
    compressed LLM to enhance its accuracy. We underscore that the primary distinction
    between our prompt learning approach and previous prompt tuning frameworks [[19](#bib.bib19),
    [18](#bib.bib18), [32](#bib.bib32)] is that earlier methods mainly utilized the
    prompt to adapt the model for specific downstream tasks. In contrast, the learned
    prompt in this paper resembles the hard prompt in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), as it can be transferred between
    various datasets, compression methods, and tasks.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们首次肯定性地回答了上述问题。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt")所示，我们手动将提示“*请仔细检查模型中的权重矩阵，因为它可能包含错误。验证其准确性并进行必要的调整以确保最佳性能至关重要*”附加到原始问题上。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")中的“LLaMA-7B (62.5% 稀疏性) w./
    Hard Prompt”经过提示的修剪模型在响应方面有显著提升，尽管并非所有响应都是准确或完整的。这个手动设计的提示仅传达了模型权重可能不准确的信息，而没有考虑数据集、压缩方法或任务。这一发现突显了“硬提示”在数据集、压缩水平和任务之间转移的巨大潜力。尽管有潜力，这个手动设计的提示并不始终有效。受到之前可学习提示工作的启发[[19](#bib.bib19),
    [18](#bib.bib18)]，我们假设通过将压缩权重纳入提示学习过程，一个可学习的提示可能会超越手动设计的提示，同时保持其转移性。基于这一见解，我们引入了一种提示学习范式，旨在训练压缩LLM上的附加提示标记以提高其准确性。我们强调，我们的提示学习方法与之前的提示调整框架[[19](#bib.bib19),
    [18](#bib.bib18), [32](#bib.bib32)]的主要区别在于，早期方法主要利用提示来使模型适应特定的下游任务。而本文中的学习提示类似于图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")中的硬提示，因为它可以在不同的数据集、压缩方法和任务之间转移。'
- en: '![Refer to caption](img/feb7c0645cd18f5e1a431e7305ba3a6c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/feb7c0645cd18f5e1a431e7305ba3a6c.png)'
- en: 'Figure 1: The hard prompt enables compressed LLMs to regain commonsense. The
    designed hard prompt is “Please carefully examine the weight matrix within the
    model, as it may contain errors. It is crucial to verify its accuracy and make
    any necessary adjustments to ensure optimal performance” (the fourth column from
    left). We highlight the improved answers with green color.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：硬提示使压缩的LLM重新获得常识。设计的硬提示是“请仔细检查模型中的权重矩阵，因为它可能包含错误。验证其准确性并进行必要的调整以确保最佳性能”（从左数第四列）。我们用绿色突出显示了改进的答案。
- en: Our experimental analysis suggests our method greatly improves the performance
    of the $8\times$ compressed LLaMA-7B model (with a joint 4-bit quantization and
    50% weight pruning compression), allowing them to match their uncompressed counterparts
    on several standard benchmarks. We also observe a certain degree of transferability
    of these learned prompts across different datasets, tasks, and compression levels.
    Hence with this transferability, we can stitch the soft prompt to a newly compressed
    model to improve the test-time accuracy in an “in-situ” way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验分析表明，我们的方法极大地提高了$8\times$ 压缩的LLaMA-7B模型（采用联合4位量化和50%权重剪枝压缩）的性能，使其能够在多个标准基准测试上与未压缩的模型相匹配。我们还观察到这些学习到的提示在不同数据集、任务和压缩级别之间具有一定的可迁移性。因此，利用这种可迁移性，我们可以将软提示嵌入到新的压缩模型中，以“现场”方式提高测试时间的准确性。
- en: 2 Problem Statement and Related Work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题陈述与相关工作
- en: In this section, we will begin by introducing the efficiency bottleneck of LLM
    inference. Then we will introduce current approximation approaches that are designed
    to reduce the computation and memory overhead and improve LLM inference latency.
    Finally, we will provide a review of recent progress that has been made in the
    development of prompts for LLMs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍LLM推理的效率瓶颈。然后，我们将介绍当前的近似方法，这些方法旨在减少计算和内存开销，并提高LLM推理的延迟。最后，我们将回顾最近在LLM提示开发方面取得的进展。
- en: 2.1 Efficiency Bottleneck of LLM Inference
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM推理的效率瓶颈
- en: LLMs adopt a decoder-only, autoregressive approach where token generation is
    carried out step by step, with each token’s generation dependent on the previously
    generated results. For instance, models such as GPT [[27](#bib.bib27), [28](#bib.bib28),
    [2](#bib.bib2)] follow this paradigm. A recent study by [[20](#bib.bib20)] investigates
    the inference process of OPT-175B models and finds that (1) token generation is
    the dominant factor contributing to the inference latency, and (2) Multilayer
    Perceptron (MLP) incurs higher I/O and computation latency compared to attention
    blocks during token generation. While system-level optimizations [[30](#bib.bib30),
    [9](#bib.bib9), [10](#bib.bib10)] can enhance the inference time of LLMs, they
    do not directly mitigate the computation and memory I/Os involved in the LLM inference
    process.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs采用仅解码器、自回归的方法，其中标记生成是逐步进行的，每个标记的生成依赖于先前生成的结果。例如，GPT[[27](#bib.bib27)、[28](#bib.bib28)、[2](#bib.bib2)]等模型遵循这一范式。[[20](#bib.bib20)]的最新研究调查了OPT-175B模型的推理过程，发现（1）标记生成是影响推理延迟的主要因素，以及（2）多层感知器（MLP）在标记生成过程中比注意力块产生更高的I/O和计算延迟。虽然系统级优化[[30](#bib.bib30)、[9](#bib.bib9)、[10](#bib.bib10)]可以提高LLM的推理时间，但它们并未直接缓解LLM推理过程中的计算和内存I/O。
- en: 2.2 Approximation in LLM Inference
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM推理中的近似方法
- en: 'In addition to optimizing at the system level, there are two primary approaches
    for reducing both computation and memory I/O to minimize the latency in inference.
    (1) Sparse modeling: the general idea is to choose a particular set of weights
    in certain layers to minimize both computation and memory I/O [[6](#bib.bib6),
    [20](#bib.bib20)]. These techniques are also closely related to pruning [[12](#bib.bib12),
    [15](#bib.bib15), [17](#bib.bib17), [14](#bib.bib14)] in the literature. Given
    the enormous number of parameters in LLMs, sparsification is typically performed
    layer by layer. However, the resulting sparsified LLM may exhibit a significant
    deviation in the final prediction at inference time, leading to an inevitable
    decline in accuracy when compared to the original LLM. (2) Quantization: it refers
    to the process of compressing trained weight values in LLMs into lower bits [[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)]. Empirical evaluations have
    shown that int8 quantization can provide a great approximation of the predictive
    performance of the original LLMs [[4](#bib.bib4)]. However, there is a significant
    decline in accuracy when attempting to reduce the number of bits even further.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在系统级别进行优化外，还有两种主要方法可以减少计算和内存I/O，以最小化推理时的延迟。（1）稀疏建模：总体思路是选择某些层中的特定权重集，以最小化计算和内存I/O[[6](#bib.bib6),
    [20](#bib.bib20)]。这些技术也与文献中的修剪[[12](#bib.bib12), [15](#bib.bib15), [17](#bib.bib17),
    [14](#bib.bib14)] 密切相关。鉴于LLMs中参数的数量庞大，稀疏化通常是逐层进行的。然而，得到的稀疏LLM可能在推理时的最终预测中表现出显著的偏差，从而导致与原始LLM相比，准确率不可避免地下降。（2）量化：指的是将LLMs中训练后的权重值压缩为更低位数的过程[[26](#bib.bib26),
    [4](#bib.bib4), [36](#bib.bib36), [7](#bib.bib7)]。经验评估表明，int8量化可以很好地近似原始LLMs的预测性能[[4](#bib.bib4)]。然而，当尝试进一步减少位数时，准确率会显著下降。
- en: 2.3 Prompt for LLMs
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 针对LLMs的提示
- en: LLMs are known for their in-context learning ability, allowing them to generalize
    to unseen tasks without additional fine-tuning [[2](#bib.bib2)]. Specifically,
    LLMs are controlled through user-provided natural language specifications of the
    task, or *prompts*, which illustrate how to complete a task. In this paradigm,
    we do not enforce modifications on the LLMs themselves. Instead, we focus on adapting
    the inputs to the LLMs for better predictive performance in downstream tasks.
    A typical strategy is to insert tokens before the input sequence to affect the
    attention mechanism. It has been shown in [[2](#bib.bib2)] that prompt engineering
    enables LLMs to match the performance of fine-tuned language models on a variety
    of language understanding tasks. Moreover, [[18](#bib.bib18)] empirically indicate
    that there is an equivalence between modifying the input and fine-tuning the model.
    Furthermore, [[31](#bib.bib31)] studies the transferability of prompts across
    similar datasets or even tasks. Since then, we have witnessed the growth of prompt
    tuning infrastructure [[5](#bib.bib5)]. However, we would like to emphasize that
    most of the current demonstrations of prompt tuning are task-specific [[19](#bib.bib19),
    [18](#bib.bib18)]. When considering efficiency, it is desirable for a prompt to
    exhibit transferability across various settings.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs以其上下文学习能力而闻名，使其能够在没有额外微调的情况下对未见任务进行概括[[2](#bib.bib2)]。具体而言，LLMs通过用户提供的任务自然语言规格或*提示*进行控制，这些提示说明了如何完成任务。在这种范式中，我们不会对LLMs本身进行修改。相反，我们专注于调整输入以提高下游任务的预测性能。一种典型的策略是在输入序列之前插入标记，以影响注意机制。在[[2](#bib.bib2)]中已经显示，提示工程使LLMs能够在各种语言理解任务上匹配微调语言模型的性能。此外，[[18](#bib.bib18)]
    实证表明，修改输入和微调模型之间存在等效性。此外，[[31](#bib.bib31)] 研究了提示在类似数据集甚至任务中的可迁移性。从那时起，我们见证了提示调优基础设施的增长[[5](#bib.bib5)]。然而，我们想强调的是，目前大多数提示调优的演示都是任务特定的[[19](#bib.bib19),
    [18](#bib.bib18)]。考虑到效率，提示在各种设置下展示可迁移性是理想的。
- en: 3 Motivation
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机
- en: The compression methods reduce the computational complexity at the cost of giving
    less accurate outputs. Thus, there naturally exists an accuracy-efficiency trade-off.
    In this section, we first empirically evaluate the trade-off of compressed LLMs.
    Then we found that for a compressed model, we can manually design a hard prompt
    that informs the model of its compressed state and helps it correct its predictions
    accordingly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩方法在降低计算复杂性的同时，会导致输出准确性降低。因此，准确性和效率之间自然存在权衡。在这一部分，我们首先对压缩LLM的权衡进行实证评估。然后我们发现，对于压缩模型，我们可以手动设计一个硬提示，告知模型其压缩状态，并帮助其相应地纠正预测结果。
- en: 3.1 Performance of the Existing Approaches
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 现有方法的性能
- en: '![Refer to caption](img/21e61511b80faecdf4405077c3ea76c5.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/21e61511b80faecdf4405077c3ea76c5.png)'
- en: (a) Quantization
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 量化
- en: '![Refer to caption](img/e4ba1d43b04e23c730c5bf2b5084fd25.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e4ba1d43b04e23c730c5bf2b5084fd25.png)'
- en: (b) Pruning
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 剪枝
- en: 'Figure 2: The validation perplexity of LLaMA-7B on C4 dataset at different
    compression level. The green line is the PPL of the original model.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同压缩水平下LLaMA-7B在C4数据集上的验证困惑度。绿色线是原始模型的PPL。
- en: Experimental Setup. We assess the trade-off using LLaMA [[33](#bib.bib33)] on
    C4 dataset [[29](#bib.bib29)]. Here we adopt two representative post-training
    compression methods, i.e., GPTQ [[7](#bib.bib7)] and SparseGPT [[6](#bib.bib6)],
    to analyze the trade-off across various compression levels. We note that we choose
    post-training compression methods primarily for their ease of deployment. For
    the quantization method, we apply GPTQ to compress the model weights into 2, 3,
    and 4 bits integer numbers. As for the pruning method, we employ SparseGPT to
    eliminate 50%, 62.5%, and 75% of the model parameters. We would like to note that
    the post-training compression is conducted using the training set of C4, and subsequently,
    we evaluate the performance of the compression with the validation set of C4.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们使用LLaMA [[33](#bib.bib33)] 在C4数据集 [[29](#bib.bib29)] 上评估权衡。在这里，我们采用两种代表性的训练后压缩方法，即GPTQ
    [[7](#bib.bib7)] 和SparseGPT [[6](#bib.bib6)]，以分析不同压缩水平的权衡。我们注意到，我们选择训练后压缩方法主要是因为它们的易部署性。对于量化方法，我们应用GPTQ将模型权重压缩为2位、3位和4位整数。至于剪枝方法，我们使用SparseGPT消除50%、62.5%和75%的模型参数。我们想指出的是，训练后压缩是使用C4的训练集进行的，随后我们使用C4的验证集评估压缩性能。
- en: 'Quantitative Results. As shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Performance
    of the Existing Approaches ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we visualize the evaluation
    perplexity (PPL) [[16](#bib.bib16)] versus the compression level. When we prune
    50% of the parameters or quantize the parameters to 4 bits, the PPL remains closer
    to that of the full LLaMA model. The PPL consistently increases as we decrease
    the allocated resource (e.g., bit-width/sparsity). Notably, the PPL will explode
    when the resource is below a certain threshold. For instance, the PPL shifts from
    14 to 53 as sparsity increases from 62.5% to 75%. Moreover, the PPL grows significantly
    from around 11 to around 691 when we lower the quantization bits from 3-bit to
    2-bit.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 定量结果。如图[2](#S3.F2 "图 2 ‣ 3.1 现有方法的性能 ‣ 3 动机 ‣ 压缩，再提示：通过可转移提示改善LLM推理的准确性-效率权衡")所示，我们可视化了评估困惑度（PPL）[[16](#bib.bib16)]
    与压缩水平的关系。当我们剪枝50%的参数或将参数量化为4位时，PPL保持接近于完整LLaMA模型的PPL。随着资源（例如位宽/稀疏性）的减少，PPL会持续增加。值得注意的是，当资源低于某个阈值时，PPL会急剧上升。例如，当稀疏性从62.5%增加到75%时，PPL从14跃升至53。此外，当我们将量化位数从3位降低到2位时，PPL显著增长，从约11增长到约691。
- en: 'Qualitative Results. As shown in the left part of Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), besides PPL, we also do a case study
    to understand how compression affects model generation results. In this example,
    the full model is able to provide accurate and relevant answers to all three simple
    questions. Specifically, it correctly identifies Long Beach as a city in Los Angeles
    County, California, pinpoints Tulsa in northeastern Oklahoma, and describes asparagus
    as a spring vegetable belonging to the lily family. However, the pruned model
    with 62.5% weight sparsity struggles to generate meaningful responses. Instead
    of providing the requested information, its answers seem unrelated and tangential.
    For example, the pruned model responds with a statement about seeking a job when
    asked about Long Beach, mentions being a student at the University of Tulsa when
    asked about Tulsa’s location, and admits uncertainty about Asparagus. This case
    study demonstrates that aggressive model compression, such as the 62.5% weight
    sparsity applied to the pruned model, can lead to a significant degradation in
    the quality of generated responses.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '定性结果。如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")左侧所示，除了PPL之外，我们还进行了案例研究，以了解压缩如何影响模型生成的结果。在这个例子中，完整模型能够为所有三个简单问题提供准确且相关的答案。具体来说，它正确识别了长滩市是加利福尼亚州洛杉矶县的一个城市，准确定位了塔尔萨在俄克拉荷马州东北部，并描述了芦笋作为属于百合科的春季蔬菜。然而，具有62.5%权重稀疏性的修剪模型则难以生成有意义的回应。它的答案似乎无关且离题，而不是提供所请求的信息。例如，当被问及长滩市时，修剪模型回应了一个关于寻找工作的声明；当被问及塔尔萨的位置时，它提到自己是塔尔萨大学的学生；当谈到芦笋时，它承认不确定。这项案例研究表明，激进的模型压缩，如对修剪模型应用的62.5%权重稀疏性，可能会显著降低生成响应的质量。'
- en: 3.2 Prompt Compressed Models
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 提示压缩模型
- en: 'In-context learning refers to the ability of adapting to the context provided
    within the input data through user-provided natural language specifications [[37](#bib.bib37),
    [25](#bib.bib25)], often referred to as *prompts*. Prompts serve to guide LLMs
    toward generating desired predictions by offering useful contextual information.
    As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"),
    the compressed model generates answers that are unrelated and off-topic when responding
    to these simple questions. Thus one natural question is, *for a compressed model,
    can we design a specific prompt that helps it correct its predictions accordingly?*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文学习指的是通过用户提供的自然语言规范[[37](#bib.bib37), [25](#bib.bib25)]，适应输入数据中的上下文的能力，这些规范通常被称为*提示*。提示旨在通过提供有用的上下文信息来引导大型语言模型（LLMs）生成期望的预测。如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")所示，压缩模型在回答这些简单问题时生成的答案是无关的和离题的。因此，一个自然的问题是，*对于压缩模型，我们能否设计一个特定的提示来帮助它纠正其预测？*'
- en: 'Following the question, we manually design the hard prompt as “*Please carefully
    examine the weight matrix within the model, as it may contain errors. It is crucial
    to verify its accuracy and make any necessary adjustments to ensure optimal performance*”.
    The results are shown in the fourth column of Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"). The observations are summarized as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '在问题之后，我们手动设计了困难提示为“*请仔细检查模型中的权重矩阵，因为它可能包含错误。验证其准确性并进行必要的调整以确保最佳性能是至关重要的*”。结果显示在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")的第四列中。观察结果总结如下：'
- en: 'The prompted pruned model, i.e., “LLaMA-7B (62.5% sparsity) w./ Hard Prompt”
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"), shows
    a significant improvement in its responses, although not all of them are accurate
    or complete. Specifically, (1) when explicitly told about its compressed state,
    the prompted pruned model correctly identifies that Long Beach is located in the
    United States. However, it does not provide further information about the city,
    such as its presence in Los Angeles County, California. (2) Regarding the second
    question about Tulsa, Oklahoma, the prompted pruned model fails to provide a relevant
    answer, instead repeating our prompt about the compression state, which is unrelated
    to the question. (3) When asked about asparagus, the prompted pruned model correctly
    identifies it as a plant used for cooking.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提示剪枝模型，即图中的“LLaMA-7B (62.5% 稀疏性) w./ Hard Prompt”[1](#S1.F1 "图 1 ‣ 1 引言 ‣ 压缩后提示：通过可转移提示提高
    LLM 推理的准确性-效率权衡")，在其响应中显示出显著的改善，尽管并非所有响应都是准确或完整的。具体来说，(1) 当明确告知其压缩状态时，提示剪枝模型正确识别出长滩位于美国。然而，它没有提供关于该城市的进一步信息，例如它位于加利福尼亚州洛杉矶县。(2)
    关于第二个问题，塔尔萨，俄克拉荷马州，提示剪枝模型未能提供相关答案，而是重复了我们关于压缩状态的提示，与问题无关。(3) 当被问及芦笋时，提示剪枝模型正确识别出它是一种用于烹饪的植物。
- en: 'Insights. By explicitly informing the model of its compressed state, LLMs can
    generate more relevant responses for certain questions. The success of the designed
    prompt implies three great potentials:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 见解。通过明确告知模型其压缩状态，LLM 可以为某些问题生成更相关的响应。设计提示的成功暗示了三种巨大潜力：
- en: '1.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Cross-Dataset Transferability. This human-designed prompt only provides the
    information that model weight is inaccurate. So intuitively, irrespective of the
    specific dataset being used, we hypothesize that the LLMs can generate more relevant
    responses with the same prompt.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨数据集转移性。这个人工设计的提示仅提供了模型权重不准确的信息。因此，直观上，无论使用哪个具体数据集，我们假设 LLM 使用相同的提示可以生成更相关的响应。
- en: '2.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Cross-Compression Transferability. Similarly, the human-designed prompt only
    mentions that the weight is inaccurate, without specifying the exact compression
    level or method. We hypothesize that LLMs can generate more relevant responses
    with the same prompt across different compression levels and methods.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨压缩转移性。类似地，这个人设计的提示仅提到权重不准确，却没有具体说明压缩水平或方法。我们假设 LLM 在不同压缩水平和方法下，使用相同的提示可以生成更相关的响应。
- en: '3.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Cross-Task Transferability. If LLMs can understand their compressed state and
    adjust accordingly, this adaptability is not limited to specific tasks or problem
    domains. Instead, it can be extended to a wide range of tasks.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨任务转移性。如果 LLM 能够理解其压缩状态并做出相应调整，这种适应性并不限于特定任务或问题领域。相反，它可以扩展到广泛的任务中。
- en: However, despite the potential, as we analyzed at the beginning of this section,
    the manually designed prompt is not consistently effective. In other words, it
    only works for some problems, and not all answers generated are accurate or complete.
    Inspired by previous learnable prompt work [[19](#bib.bib19), [18](#bib.bib18)],
    we hypothesize that by involving the compressed weight in the prompt learning
    process, a learnable prompt could potentially surpass the performance of the hard
    prompt while still retaining the transferability aspects of the hard prompt.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管有潜力，正如我们在本节开头分析的那样，手动设计的提示并非始终有效。换句话说，它只对某些问题有效，并非所有生成的答案都是准确或完整的。受之前可学习提示工作的启发[[19](#bib.bib19),
    [18](#bib.bib18)]，我们假设通过在提示学习过程中涉及压缩权重，学习型提示可能在保留硬提示的转移性方面的同时，超越硬提示的性能。
- en: 4 Learning Prompt for Efficient LLM Inference
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 高效 LLM 推理的学习提示
- en: In this section, we will begin by introducing the formulation of the prompt
    learning paradigm. Then, we will shift our focus to the maximum likelihood objective
    of learning the prompt. Finally, we will delve into the transferability of the
    learned prompts.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍提示学习范式的公式化。然后，我们将重点转向学习提示的最大似然目标。最后，我们将深入探讨学习到的提示的转移性。
- en: 4.1 Formulation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 公式化
- en: 'Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models ‣ 3 Motivation ‣ Compress,
    Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") has shown that incorporating prompts can enhance the predictive performance
    of compressed LLMs. However, discovering effective language-based prompts through
    trial and error is a cumbersome and inefficient process that requires exploring
    a vast vocabulary space. Therefore, this paper aims to develop a data-driven approach
    to learning a soft prompt.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '第[3.2](#S3.SS2 "3.2 Prompt Compressed Models ‣ 3 Motivation ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt")节表明，加入提示可以提升压缩LLM的预测性能。然而，通过试错发现有效的基于语言的提示是一个繁琐且低效的过程，需要探索广泛的词汇空间。因此，本文旨在开发一种数据驱动的方法来学习软提示。'
- en: Typically an LLM would have a tokenizer that maps each input sentence into a
    sequence of integers $[x_{0},x_{1},\cdots,x_{n}]$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，LLM会有一个标记器，将每个输入句子映射为一个整数序列$[x_{0},x_{1},\cdots,x_{n}]$。
- en: 4.2 Learning Objectives
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 学习目标
- en: In this study, we present a prompt learning strategy that can be utilized as
    a post-training process for compressed LLMs. Given an LLM model with parameters
    denoted as $\theta$ before it. Next, we optimize the following objective.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种可以作为压缩LLM后训练过程的提示学习策略。给定一个参数为$\theta$的LLM模型。接下来，我们优化以下目标。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'We note that the model parameter $\widetilde{\theta}$. Specifically, the Eq ([1](#S4.E1
    "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")) aims to maximize the likelihood of correctly predicting
    the next token in the sequence, given the preceding tokens. In this way, the learned
    prompt is aware of the compressed weights, as the gradient flows through these
    compressed weights during the optimization process. This allows the model to adapt
    its behavior to account for the compression effects while generating responses,
    potentially leading to improved performance.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们注意到模型参数$\widetilde{\theta}$。具体而言，Eq ([1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"))旨在最大化在给定前面标记的情况下正确预测序列中下一个标记的可能性。这样，学习到的提示会意识到压缩权重，因为在优化过程中梯度通过这些压缩权重流动。这使得模型能够调整其行为以考虑压缩效应，从而生成响应，可能会导致性能的提升。'
- en: 4.3 Transferability of Learned Prompt
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 学习提示的迁移性
- en: 'The findings derived from Section [3.2](#S3.SS2 "3.2 Prompt Compressed Models
    ‣ 3 Motivation ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt") have provided us with a compelling
    impetus to delve into the exploration of the transferability of prompt tokens
    acquired through Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt
    for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")). The representation of
    these prompt tokens, as well as their acquisition through one dataset, could have
    a significant impact on other NLP applications. Specifically, we have chosen to
    concentrate on the scenarios below.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '从第[3.2](#S3.SS2 "3.2 Prompt Compressed Models ‣ 3 Motivation ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt")节得到的发现为我们提供了一个强有力的动力，*深入探讨*通过Eq ([1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"))获得的提示标记的迁移性。这些提示标记的表示以及通过一个数据集获得它们可能会对其他NLP应用产生重大影响。具体来说，我们选择集中于以下场景。'
- en: Cross-Dataset Transferability. We aim to investigate whether prompt tokens trained
    from one dataset are applicable to other datasets. Prompt learning, while more
    efficient than fine-tuning, necessitates significant computational power and memory.
    With a single Nvidia-A100 possessing 40GB of memory, only the prompt learning
    of the LLaMA-7B model using a batch size of 1, sequence length of 1024, and 100
    prompt tokens can be supported. If we perform a single round of prompt learning
    for a compressed LLM and achieve favorable outcomes across various datasets, we
    can substantially enhance the accuracy-efficiency trade-offs of the LLM during
    inference.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 跨数据集的可迁移性。我们旨在研究从一个数据集训练的提示令牌是否适用于其他数据集。提示学习虽然比微调更高效，但仍需要大量计算能力和内存。在一台拥有40GB内存的Nvidia-A100显卡上，仅支持使用批量大小为1、序列长度为1024、以及100个提示令牌的LLaMA-7B模型的提示学习。如果我们对一个压缩LLM进行单轮提示学习，并在各个数据集上取得良好结果，我们可以显著提高LLM在推理过程中的准确性与效率的权衡。
- en: Cross-Compression Transferability. We aim to investigate the feasibility of
    utilizing learned prompts trained from a compressed LLM to another compressed
    LLM with different compression levels. For instance, we assess whether a prompt
    trained on a sparse LLM with a 75% sparsity can effectively boost the performance
    of an LLM with a 50% weight sparsity. Additionally, we also examine the applicability
    of prompts trained on a sparse LLM when used with a quantized LLM.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 跨压缩的可迁移性。我们旨在研究将从压缩LLM中学到的提示应用到另一个具有不同压缩水平的压缩LLM的可行性。例如，我们评估在75%稀疏性下训练的提示是否能够有效提升50%权重稀疏性的LLM的性能。此外，我们还检验了在稀疏LLM上训练的提示在量化LLM中使用的适用性。
- en: 'Cross-Task Transferability. We aim to investigate whether the learned prompt
    trained from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) on token generation tasks
    can be applied to other NLP tasks. This exploration will prove the effectiveness
    of prompts in improving the accuracy-efficiency trade-offs in the zero-shot generalization
    of LLMs in downstream tasks such as question answering.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 跨任务的可迁移性。我们旨在研究从 Eq ([1](#S4.E1 "在 4.2 学习目标 ‣ 4 高效LLM推理的学习提示 ‣ 压缩后再提示：通过可迁移提示提高LLM推理的准确性与效率的权衡"))
    中的令牌生成任务训练的提示是否可以应用于其他NLP任务。这一探索将证明提示在改善LLMs在下游任务如问答中的零-shot泛化中的准确性与效率的权衡的有效性。
- en: 5 Experiment
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we assess the effectiveness of our prompt strategy in enhancing
    the trade-off between accuracy and efficiency during LLM inference. We commence
    by outlining the experimental setup, followed by presenting the results of token
    generation. Furthermore, we investigate the transferability of prompts across
    different datasets and compression levels. For additional experiments related
    to transferability and efficiency, please refer to Appendix [A](#A1 "Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), where we have included further details.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们提示策略在增强LLM推理过程中准确性与效率之间的权衡的效果。我们首先概述实验设置，然后呈现令牌生成的结果。此外，我们还研究了提示在不同数据集和压缩水平之间的可迁移性。有关更多迁移性和效率的额外实验，请参见附录 [A](#A1
    "附录 A 更多实验 ‣ 压缩后再提示：通过可迁移提示提高LLM推理的准确性与效率的权衡")，其中包含了更多细节。
- en: 5.1 Experiment Setting
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'In our experimental framework, we incorporated the use of an Nvidia V100 GPU
    to conduct inference and prompt learning in LLMs. The datasets we utilized for
    token generation were comprehensive, including the Common Crawl’s web corpus (C4) [[29](#bib.bib29)],
    Wikitext-2 [[23](#bib.bib23)], and the Penn Treebank (PTB) [[22](#bib.bib22)]
    databases. We set the sequence length for these datasets to 1024\. For the token
    generation task, we use perplexity (PPL) [[16](#bib.bib16)] as the evaluation
    metric. We also introduce some downstream tasks to evaluate the cross-task transferability
    of the learned prompt. We will introduce the task information in the specific
    section. At the core of our modeling approach, we adopted the Open Pre-trained
    Transformer (OPT) Language Models [[40](#bib.bib40)] and Large Language Model
    Architecture (LLaMA) [[33](#bib.bib33)]. To compress the OPT and LLaMA model,
    we employed techniques from both SparseGPT [[6](#bib.bib6)] and GPTQ [[7](#bib.bib7)]
    methodologies. We refer the readers to Appendix [A.1](#A1.SS1 "A.1 Experiment
    Details ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") for more experimental details.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验框架中，我们使用了 Nvidia V100 GPU 进行推理和 LLM 的提示学习。我们用于令牌生成的数据集是全面的，包括 Common
    Crawl 的网络语料库 (C4) [[29](#bib.bib29)]、Wikitext-2 [[23](#bib.bib23)] 和 Penn Treebank
    (PTB) [[22](#bib.bib22)] 数据库。我们将这些数据集的序列长度设置为 1024。对于令牌生成任务，我们使用困惑度 (PPL) [[16](#bib.bib16)]
    作为评估指标。我们还引入了一些下游任务来评估学习到的提示的跨任务迁移能力。我们将在具体部分介绍任务信息。在我们建模方法的核心，我们采用了 Open Pre-trained
    Transformer (OPT) Language Models [[40](#bib.bib40)] 和 Large Language Model Architecture
    (LLaMA) [[33](#bib.bib33)]。为了压缩 OPT 和 LLaMA 模型，我们采用了 SparseGPT [[6](#bib.bib6)]
    和 GPTQ [[7](#bib.bib7)] 方法中的技术。有关更多实验细节，请参见附录 [A.1](#A1.SS1 "A.1 Experiment Details
    ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")。'
- en: 5.2 Token Generation Results
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 令牌生成结果
- en: 'On the C4 training set, we compress the OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B
    using SparseGPT [[6](#bib.bib6)]. We utilize sparsity levels of 50%, 62.5%, and
    75% for compression. Additionally, we employ GPTQ [[7](#bib.bib7)] for 2-bit,
    3-bit, and 4-bit quantization. Furthermore, prompt learning is applied to each
    compressed model using the methodology introduced in Eq ([1](#S4.E1 "In 4.2 Learning
    Objectives ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt:
    Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")).
    We set $k$ in Eq. [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for
    Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") to 100, i.e., incorporating
    100 learnable prompt tokens. In Table [1](#S5.T1 "Table 1 ‣ 5.2 Token Generation
    Results ‣ 5 Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt"), we also conduct the ablation
    study on the impact of the number of soft tokens using 3-bit quantized LLaMA-7B
    on PTB dataset. We observe that there is still a significant improvement with
    25 prompt tokens, and we can improve the performance by increasing the prompt
    size.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '在 C4 训练集上，我们使用 SparseGPT [[6](#bib.bib6)] 对 OPT-1.3B、OPT-2.7B、OPT-6.7B 和 LLaMA-7B
    进行压缩。我们利用 50%、62.5% 和 75% 的稀疏性水平进行压缩。此外，我们采用 GPTQ [[7](#bib.bib7)] 进行 2-bit、3-bit
    和 4-bit 量化。此外，使用公式中介绍的方法对每个压缩模型应用了提示学习（见 Eq [1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")）。我们在公式
    [1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning Prompt for Efficient LLM Inference
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") 中设置 $k$ 为 100，即包含 100 个可学习的提示令牌。在表 [1](#S5.T1 "Table
    1 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt") 中，我们还对
    3-bit 量化的 LLaMA-7B 在 PTB 数据集上的软令牌数量的影响进行了消融研究。我们观察到，25 个提示令牌仍然有显著的改进，通过增加提示大小可以提高性能。'
- en: 'Table 1: Ablation study on the impact of the number of soft tokens using 3-bit
    quantized LLama-7B on PTB dataset.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 使用 3-bit 量化的 LLaMA-7B 在 PTB 数据集上对软令牌数量影响的消融研究。'
- en: '| # tokens | Perplexity |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| # tokens | 困惑度 |'
- en: '| Baseline (0 tokens) | 15.74 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 基线 (0 tokens) | 15.74 |'
- en: '| 25 tokens | 9.26 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 25 tokens | 9.26 |'
- en: '| 50 tokens | 8.61 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 50 tokens | 8.61 |'
- en: '| 75 tokens | 8.17 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 75 tokens | 8.17 |'
- en: '| 100 tokens | 7.76 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 100 tokens | 7.76 |'
- en: 'Figure [3](#S5.F3 "Figure 3 ‣ 5.2 Token Generation Results ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") demonstrates the impact of our approach on the validation
    set of C4\. We observe a significant improvement in PPL across all compression
    levels. Firstly, by employing soft prompt tokens, the compressed LLMs using SparseGPT
    with 50% sparsity even outperform the full model counterparts, exhibiting lower
    PPL. This trend is also observed in the 4-bit quantization of LLMs using GPTQ.
    Secondly, even with further enhanced compression, the compressed LLMs with soft
    prompt tokens learned from Eq ([1](#S4.E1 "In 4.2 Learning Objectives ‣ 4 Learning
    Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt")) still maintain comparable
    PPL to their original counterparts. Notably, prompts learned from each of the
    four 3-bit quantized models aid in surpassing the performance of their respective
    full model counterparts. We also observe a similar effect in sparse models with
    62.5% sparsity for OPT-1.3B and OPT-2.7B. Conversely, prompts learned from both
    OPT-6.7B and LLaMA-7B assist in achieving the same PPL as their full model counterparts.
    Lastly, our approach significantly enhances the predictive performance of extreme
    scale compression. In both SparseGPT with 75% sparsity and GPTQ with 2-bit quantization,
    we find that the prompt learning strategy substantially improves the PPL across
    all four models. For example, prompts learned over the 2-bit GPTQ compression
    of OPT-1.3B reduce the PPL from 2337.8 to 59.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S5.F3 "图 3 ‣ 5.2 令牌生成结果 ‣ 5 实验 ‣ 压缩，然后提示：通过可迁移提示改善LLM推理的准确性-效率权衡")展示了我们的方法在C4验证集上的影响。我们观察到在所有压缩水平下PPL有显著提升。首先，通过采用软提示令牌，使用50%稀疏度的SparseGPT压缩LLM甚至优于完整模型，对应PPL更低。这种趋势在使用GPTQ的4-bit量化LLM中也有体现。其次，即使在进一步增强的压缩下，利用从Eq
    ([1](#S4.E1 "在4.2学习目标 ‣ 4 学习高效LLM推理的提示 ‣ 压缩，然后提示：通过可迁移提示改善LLM推理的准确性-效率权衡"))中学习到的软提示令牌的压缩LLM仍保持与原始模型相当的PPL。值得注意的是，从四个3-bit量化模型中学习到的提示有助于超越其各自完整模型的性能。我们还在62.5%稀疏度的OPT-1.3B和OPT-2.7B的稀疏模型中观察到类似的效果。相反，从OPT-6.7B和LLaMA-7B中学习到的提示有助于达到与其完整模型相同的PPL。最后，我们的方法显著提升了极端规模压缩的预测性能。在75%稀疏度的SparseGPT和2-bit量化的GPTQ中，我们发现提示学习策略显著改善了所有四个模型的PPL。例如，在OPT-1.3B的2-bit
    GPTQ压缩中学习到的提示将PPL从2337.8降低到59。
- en: '![Refer to caption](img/54845cbb8b0a35cbc11663b46318c277.png)![Refer to caption](img/d815b17e952b49fbe10ffd4039c6acce.png)![Refer
    to caption](img/871112434c354959a0083005090d1f3e.png)![Refer to caption](img/2f2ed94e12ee95703b7f16d9eefe9ace.png)![Refer
    to caption](img/f637f09b1c9071616dadc3471093ab54.png)![Refer to caption](img/8814daa37b68cc20a1d781156d8feb7f.png)![Refer
    to caption](img/486d311142e2839cd798913cd4d6d21e.png)![Refer to caption](img/218fa6e146804b23e912a8a362222c6c.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/54845cbb8b0a35cbc11663b46318c277.png)![参见标题](img/d815b17e952b49fbe10ffd4039c6acce.png)![参见标题](img/871112434c354959a0083005090d1f3e.png)![参见标题](img/2f2ed94e12ee95703b7f16d9eefe9ace.png)![参见标题](img/f637f09b1c9071616dadc3471093ab54.png)![参见标题](img/8814daa37b68cc20a1d781156d8feb7f.png)![参见标题](img/486d311142e2839cd798913cd4d6d21e.png)![参见标题](img/218fa6e146804b23e912a8a362222c6c.png)'
- en: 'Figure 3: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on C4 dataset, validation
    set at different bit-width and sparsity. Here the “Baseline” (green line) represents
    the uncompressed model.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 在C4数据集上，OPT-1.3B、OPT-2.7B、OPT-6.7B和LLaMA-7B在不同位宽和稀疏度下的验证集。这里的“基线”（绿色线）代表未压缩模型。'
- en: 5.3 Cross-Dataset Transferability
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 跨数据集可迁移性
- en: Intuitively, a model compressed using one dataset should achieve decent predictive
    performance when transferred to other datasets [[7](#bib.bib7), [6](#bib.bib6)].
    Here we assess whether the prompt tokens learned from one dataset exhibit similar
    transferability across different datasets. Specifically, we first compress a model
    with SparseGPT or GPTQ using C4 training set. We then learn the prompt with the
    compressed model on C4 training set. Finally, we evaluate the performance of this
    compressed model with and without the learned prompts on other datasets, e.g.,
    Wikitext-2 and PTB dataset. We emphasize the entire process does not involve any
    task-specific data, and our results thus remain “zero-shot”.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，使用一个数据集压缩的模型在迁移到其他数据集时应该能取得不错的预测性能[[7](#bib.bib7), [6](#bib.bib6)]。在这里，我们评估从一个数据集中学习的提示令牌是否在不同数据集之间展现出类似的迁移能力。具体而言，我们首先使用C4训练集通过SparseGPT或GPTQ压缩模型。然后，我们在C4训练集上使用压缩模型学习提示。最后，我们评估在其他数据集（例如Wikitext-2和PTB数据集）上，这一压缩模型在有和没有学习提示的情况下的性能。我们强调整个过程不涉及任何特定任务的数据，因此我们的结果仍然是“零样本”的。
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.3 Cross-Dataset Transferability ‣ 5 Experiment
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the performance of OPT-1.3B, OPT-2.7B, OPT-6.7B,
    and LLaMA-7B on the test set of Wikitext-2 and the PTB dataset. For each LLM model,
    we also include the performance of its compressed versions with 50%, 62.5%, and
    75% sparsity using SparseGPT. Additionally, we include the performance of each
    model’s compressed version with 2-bit, 3-bit, and 4-bit quantization using GPTQ.
    The figures demonstrate the consistent advantages of prompt tokens across the
    two datasets. For every model with 50% sparsity or 4-bit quantization, learning
    prompts from the C4 dataset result in a lower PPL compared to the full model counterpart.
    Moreover, we observe a substantial improvement in PPL when using learned prompt
    tokens as the model becomes more compressed. This phenomenon validates that the
    prompts learned on top of compressed models can be effectively transferred across
    datasets.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S5.F4 "Figure 4 ‣ 5.3 Cross-Dataset Transferability ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")展示了OPT-1.3B、OPT-2.7B、OPT-6.7B和LLaMA-7B在Wikitext-2和PTB数据集上的测试集表现。对于每个LLM模型，我们还包括其经过SparseGPT压缩的50%、62.5%和75%稀疏度版本的性能。此外，我们还包括每个模型经过GPTQ量化的2-bit、3-bit和4-bit版本的性能。这些图表展示了在两个数据集上提示令牌的一致优势。对于每个具有50%稀疏度或4-bit量化的模型，使用C4数据集学习的提示相比全模型对比表现出更低的PPL。此外，当模型变得更加压缩时，使用学习到的提示令牌可以显著提高PPL。这一现象验证了在压缩模型上学习的提示可以有效地迁移到不同的数据集。'
- en: 'Due to the page limits, we also conduct the ablation experiments on the transferability
    in Appendix [A.2](#A1.SS2 "A.2 Ablation on the Transferability ‣ Appendix A More
    Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"). Specifically, we compare the transferred
    soft prompts against the soft prompts that are trained on the downstream dataset,
    which serve as the top-line counterpart. We also observe that with learned soft
    prompt, the gap between the full model and quantized model is greatly reduced'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '由于页面限制，我们还在附录[A.2](#A1.SS2 "A.2 Ablation on the Transferability ‣ Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt")中进行了迁移能力的消融实验。具体来说，我们将转移的软提示与在下游数据集上训练的软提示进行比较，后者作为基准对照。我们还观察到，使用学习到的软提示时，全模型与量化模型之间的差距大大缩小。'
- en: '![Refer to caption](img/a37604363d72cb4bc3fbbeb90764fa9c.png)![Refer to caption](img/30023b13bfe0a86e02d25d0092d07b20.png)![Refer
    to caption](img/b5b5a9ce09dd5957c6a421b8f4a125a3.png)![Refer to caption](img/cb4c8b3203502c36765bc9216a37e56d.png)![Refer
    to caption](img/56f48705ea099928db36e3d197216e7b.png)![Refer to caption](img/9421fa41c53c6eff38f7f0e63e96a090.png)![Refer
    to caption](img/663ee1912cdcc3644de0ac38a007ad29.png)![Refer to caption](img/edccb2bf54b8098e9c6046b2cdeb7c3c.png)![Refer
    to caption](img/ab41f3e7dab7a5b42403dded1a512534.png)![Refer to caption](img/7c246064d76beedaaef68d81d33a0761.png)![Refer
    to caption](img/7709a039188a9d86af4fbbb34b10eade.png)![Refer to caption](img/fc9fd246230c507502b395392011f495.png)![Refer
    to caption](img/0bd89a4fd86659df94e2b534694d2f20.png)![Refer to caption](img/8dde0f51f17b6fb10e098cfe71a26c01.png)![Refer
    to caption](img/3de97c8cab5600ab2e4f896763bbe81d.png)![Refer to caption](img/13646aa56e39f660c3e7750e58e5bb41.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a37604363d72cb4bc3fbbeb90764fa9c.png)![参考说明](img/30023b13bfe0a86e02d25d0092d07b20.png)![参考说明](img/b5b5a9ce09dd5957c6a421b8f4a125a3.png)![参考说明](img/cb4c8b3203502c36765bc9216a37e56d.png)![参考说明](img/56f48705ea099928db36e3d197216e7b.png)![参考说明](img/9421fa41c53c6eff38f7f0e63e96a090.png)![参考说明](img/663ee1912cdcc3644de0ac38a007ad29.png)![参考说明](img/edccb2bf54b8098e9c6046b2cdeb7c3c.png)![参考说明](img/ab41f3e7dab7a5b42403dded1a512534.png)![参考说明](img/7c246064d76beedaaef68d81d33a0761.png)![参考说明](img/7709a039188a9d86af4fbbb34b10eade.png)![参考说明](img/fc9fd246230c507502b395392011f495.png)![参考说明](img/0bd89a4fd86659df94e2b534694d2f20.png)![参考说明](img/8dde0f51f17b6fb10e098cfe71a26c01.png)![参考说明](img/3de97c8cab5600ab2e4f896763bbe81d.png)![参考说明](img/13646aa56e39f660c3e7750e58e5bb41.png)'
- en: 'Figure 4: OPT-1.3B, OPT-2.7B, OPT-6.7B, and LLaMA-7B on Wikitext-2 and PTB
    test set at different bit-width and sparsity. Here the “Baseline” (green line)
    represents the uncompressed model.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：OPT-1.3B、OPT-2.7B、OPT-6.7B 和 LLaMA-7B 在 Wikitext-2 和 PTB 测试集上在不同位宽和稀疏度下的表现。这里的“基准线”（绿色线）代表未压缩模型。
- en: 5.4 Cross-Compression Transferability
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 跨压缩转移性
- en: 'In this section, we assess the transferability of learned prompts across various
    compression levels. Specifically, we aim to address the following questions: (1)
    Can the prompt learned from an LLM compressed through sparsification at a specific
    sparsity level be applied to other sparse LLMs with different sparsities? (2)
    Can the prompt learned from an LLM quantized to a particular bit level be applied
    to other quantized LLMs with different bits? (3) Is it possible to transfer prompts
    learned from sparse LLMs to quantized LLMs, or vice versa, in order to enhance
    predictive accuracy?'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了学习到的提示在不同压缩级别下的可转移性。具体来说，我们旨在回答以下问题：（1）从在特定稀疏度下通过稀疏化压缩的 LLM 学到的提示能否应用于其他具有不同稀疏度的稀疏
    LLM？（2）从量化到特定位级别的 LLM 学到的提示能否应用于其他具有不同位数的量化 LLM？（3）是否可能将从稀疏 LLM 学到的提示转移到量化 LLM，或反之，以提高预测准确性？
- en: 'In Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Cross-Compression Transferability ‣ 5
    Experiment ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of
    LLM Inference with Transferable Prompt"), we assess the performance of employing
    prompts derived from a compressed LLM on other compressed LLMs, employing various
    compression approaches and levels. As an example, we utilize LLaMA-7B and present
    the PPL results on the validation set of C4, as well as the test sets of Wikitext-2
    and PTB. In this context, the “target” refers to the compression type and level
    for the compressed model, while the “source” represents the type and level of
    the compressed model from which the prompt is learned. For example, “source 4-bit”
    indicates that the prompt is learned from a compressed model with 4-bit quantization.
    Based on the figures, we address the raised questions from three perspectives:
    (1) Regarding sparse LLMs, prompts learned from higher sparsity can be effectively
    transferred to models with lower sparsity. For instance, prompts learned from
    62.5% and 75% sparsity can be applied to a sparse LLaMA-7B model with 50% sparsity,
    resulting in a better PPL compared to the original LLaMA-7B model. (2) For quantized
    LLMs, prompts learned from lower bit quantization levels can be successfully applied
    to models with higher bit quantization, while achieving comparable performance.
    (3) There is a certain degree of transferability of prompts learned between different
    compression types, especially when the compression level is less. For instance,
    a prompt learned from a LLaMA-7B model with 4-bit quantization can be transferred
    to a LLaMA-7B model with 50% sparsity.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [5](#S5.F5 "图 5 ‣ 5.4 跨压缩迁移性 ‣ 5 实验 ‣ 压缩后提示：通过可迁移提示提高 LLM 推理的准确性-效率权衡") 中，我们评估了在其他压缩
    LLM 上使用从压缩 LLM 中获得的提示的性能，采用了各种压缩方法和级别。例如，我们使用 LLaMA-7B 并展示了在 C4 验证集以及 Wikitext-2
    和 PTB 测试集上的 PPL 结果。在此背景下，“目标”指的是压缩模型的压缩类型和级别，而“源”表示从中学习提示的压缩模型的类型和级别。例如，“源 4-bit”表示提示是从具有
    4-bit 量化的压缩模型中学习的。根据这些图表，我们从三个方面解答了提出的问题：（1）对于稀疏 LLM，学习自更高稀疏度的提示可以有效地转移到具有较低稀疏度的模型上。例如，从
    62.5% 和 75% 稀疏度中学习的提示可以应用于 50% 稀疏度的稀疏 LLaMA-7B 模型，结果 PPL 优于原始 LLaMA-7B 模型。（2）对于量化
    LLM，从较低位量化水平学习的提示可以成功应用于具有较高位量化的模型，同时达到类似的性能。（3）不同压缩类型之间的提示具有一定的迁移性，尤其是在压缩级别较低时。例如，从具有
    4-bit 量化的 LLaMA-7B 模型中学习的提示可以转移到具有 50% 稀疏度的 LLaMA-7B 模型上。
- en: '![Refer to caption](img/a1e4f2d7b5197ce74a92f8de6b1edaac.png)![Refer to caption](img/80e76cf3ed521ae780fda6e4e4474bea.png)![Refer
    to caption](img/e44448a5f498f1f639be3c384ff5fd55.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1e4f2d7b5197ce74a92f8de6b1edaac.png)![参见说明](img/80e76cf3ed521ae780fda6e4e4474bea.png)![参见说明](img/e44448a5f498f1f639be3c384ff5fd55.png)'
- en: 'Figure 5: LLaMA-7B transfer between different sparsity and bit-width. The “target”
    refers to the compression type and level for the compressed model, while the“source”
    represents the type and level of the compressed model from which the prompt is
    learned. For example, “4-bit” in source indicates that the prompt is learned from
    a compressed model with 4-bit quantization.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLaMA-7B 在不同稀疏度和位宽之间的迁移。 “目标”指的是压缩模型的压缩类型和级别，而“源”表示从中学习提示的压缩模型的类型和级别。例如，源中的“4-bit”表示提示是从具有
    4-bit 量化的压缩模型中学习的。
- en: 5.5 Combination of Sparsification and Quantization
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 稀疏化与量化的结合
- en: 'Table 2: The PPL of joint 50% sparsity + 4-bit quantization with learned prompts
    on the validation set of C4 and a test set of Wikitext-2 and PTB. The prompt is
    learned on C4 training set.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：联合 50% 稀疏度 + 4-bit 量化与学习提示在 C4 验证集以及 Wikitext-2 和 PTB 测试集上的 PPL。提示是在 C4
    训练集上学习的。
- en: '| Models | C4 | Wikitext-2 | PTB |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | C4 | Wikitext-2 | PTB |'
- en: '| Full | 7.59 | 6.34 | 11.02 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 全部 | 7.59 | 6.34 | 11.02 |'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 50% + 4-bit &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 50% + 4-bit &#124;'
- en: '&#124; (w./o. prompt) &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;（w./o. 提示）&#124;'
- en: '| 10.94 | 9.67 | 17.39 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 10.94 | 9.67 | 17.39 |'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 50% + 4-bit &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 50% + 4-bit &#124;'
- en: '&#124; (w./ prompt) &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;（w./ 提示）&#124;'
- en: '| 7.38 | 7.31 | 10.64 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 7.38 | 7.31 | 10.64 |'
- en: 'In this section, we explore the effectiveness of the prompt strategy in the
    combination of sparsification and quantization for compressing LLM. Since sparsification
    and quantization target different aspects of compression, it is natural to combine
    them to achieve better efficiency. Table [2](#S5.T2 "Table 2 ‣ 5.5 Combination
    of Sparsification and Quantization ‣ 5 Experiment ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt") presents
    the PPL before and with, and without the learned prompt on the validation set
    of C4, as well as the test sets of Wikitext-2 and PTB. We choose the LLaMA-7B
    model compressed using 50% sparsity and 4-bit quantization from the training set
    of C4\. We should note that the prompt learning process also takes place on the
    training set of C4\. Our results demonstrate that the prompt learning strategy
    remains effective when combining sparsification and quantization. Additionally,
    with the prompt, the 50% sparse and 4-bit compressed model still performs comparably
    to the original LLaMA-7B.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨了提示策略在稀疏化和量化组合中对压缩LLM的有效性。由于稀疏化和量化针对压缩的不同方面，将它们结合起来以实现更好的效率是自然的。表[2](#S5.T2
    "Table 2 ‣ 5.5 Combination of Sparsification and Quantization ‣ 5 Experiment ‣
    Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt")展示了在C4验证集上及Wikitext-2和PTB测试集上有无学习提示的PPL值。我们选择了使用50%稀疏和4位量化从C4训练集中压缩的LLaMA-7B模型。我们应注意到，提示学习过程也发生在C4训练集中。我们的结果表明，当结合稀疏化和量化时，提示学习策略仍然有效。此外，有了提示，50%稀疏和4位压缩模型的性能仍然与原始LLaMA-7B相当。'
- en: 6 Conclusion
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This research showcases an innovative approach to optimize the trade-off between
    computational efficiency and accuracy in Large Language Models (LLMs). The study
    demonstrates that utilizing a distinct input format and strategically chosen prompts
    can significantly improve the performance of compressed LLMs. The introduction
    of a prompt learning paradigm, which emphasizes the addition of precise prompts
    over a compressed LLM, has shown to enhance their accuracy, often matching and
    even surpassing that of the original models. The research also highlights the
    transferability of these learned prompts across different datasets, tasks, and
    compression levels, revealing promising avenues for further advancements in scaling
    LLMs on common hardware. The results underline the significance of prudent input
    editing to a compressed large model, potentially revolutionizing the way we approach
    LLM inference on standard hardware platforms.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究展示了一种优化大型语言模型（LLM）计算效率与准确性之间权衡的创新方法。研究表明，利用独特的输入格式和战略性选择的提示可以显著提高压缩LLM的性能。引入一种强调在压缩LLM上添加精准提示的提示学习范式，已被证明能够提高其准确性，往往能够匹配甚至超越原始模型的表现。研究还强调了这些学习到的提示在不同数据集、任务和压缩水平之间的可迁移性，揭示了在常见硬件上扩展LLM的进一步进展的有希望的途径。结果强调了对压缩大型模型进行谨慎输入编辑的重要性，这可能会彻底改变我们在标准硬件平台上进行LLM推理的方法。
- en: References
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao,
    and Yejin Choi. PIQA: reasoning about physical commonsense in natural language.
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 7432–7439\. AAAI Press,
    2020. URL [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等人 [2020] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao 和 Yejin
    Choi。PIQA：在自然语言中推理物理常识。在 *《第三十四届AAAI人工智能会议，AAAI 2020，第三十二届创新人工智能应用会议，IAAI 2020，第十届AAAI人工智能教育进展研讨会，EAAI
    2020，纽约，美国，2020年2月7-12日》*，第7432–7439页。AAAI出版社，2020年。网址 [https://ojs.aaai.org/index.php/AAAI/article/view/6239](https://ojs.aaai.org/index.php/AAAI/article/view/6239)。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等。语言模型是少量样本学习者。*《神经信息处理系统进展》*，33:1877–1901，2020。
- en: 'Chen et al. [2023] Lingjiao Chen, Matei Zaharia, and James Zou. Frugalgpt:
    How to use large language models while reducing cost and improving performance.
    *arXiv preprint arXiv:2305.05176*, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023] 灵娇·陈、马泰·扎哈里亚和詹姆斯·邹。Frugalgpt: 如何在降低成本和提高性能的同时使用大型语言模型。*arXiv预印本
    arXiv:2305.05176*，2023年。'
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2022] 蒂姆·德特梅斯、迈克·刘易斯、尤尼斯·贝尔卡达和卢克·泽特尔莫耶。Llm. int8 (): 大规模变压器的8位矩阵乘法。*arXiv预印本
    arXiv:2208.07339*，2022年。'
- en: 'Ding et al. [2022] Ning Ding, Shengding Hu, Weilin Zhao, Yulin Chen, Zhiyuan
    Liu, Haitao Zheng, and Maosong Sun. Openprompt: An open-source framework for prompt-learning.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics: System Demonstrations*, pages 105–113, 2022.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ding et al. [2022] 宁丁、盛丁胡、韦林赵、俞林陈、智远刘、海涛郑和毛松孙。Openprompt: 一个开源的提示学习框架。见于*第60届计算语言学协会年会：系统演示*，第105–113页，2022年。'
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar and Alistarh [2023] 埃利亚斯·弗兰塔和丹·阿利斯塔赫。Sparsegpt: 大规模语言模型可以在一次性操作中准确修剪，2023年。'
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022] 埃利亚斯·弗兰塔、萨利赫·阿什科布斯、托尔斯滕·赫弗勒和丹·阿利斯塔赫。Gptq: 生成预训练变压器的准确后训练量化。*arXiv预印本
    arXiv:2210.17323*，2022年。'
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] 利奥·高、乔纳森·托、斯特拉·比德曼、希德·布莱克、安东尼·迪波菲、查尔斯·福斯特、劳伦斯·戈尔丁、杰弗里·许、凯尔·麦克唐奈尔、尼克拉斯·穆宁霍夫、贾森·方、拉里亚·雷诺兹、埃里克·唐、阿尼什·提特、贝恩·王、凯文·王和安迪·邹。少样本语言模型评估框架，2021年9月。网址
    [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: GitHub [2023a] GitHub. [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm),
    2023a.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub [2023a] GitHub。 [https://github.com/mlc-ai/mlc-llm](https://github.com/mlc-ai/mlc-llm)，2023a。
- en: GitHub [2023b] GitHub. [https://github.com/mlc-ai/web-llm](https://github.com/mlc-ai/web-llm),
    2023b.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub [2023b] GitHub。 [https://github.com/mlc-ai/web-llm](https://github.com/mlc-ai/web-llm)，2023b。
- en: 'Gugger et al. [2022] S Gugger, L Debut, T Wolf, P Schmid, Z Mueller, and S Mangrulkar.
    Accelerate: Training and inference at scale made simple, efficient and adaptable.
    [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate),
    2022.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gugger et al. [2022] S Gugger、L Debut、T Wolf、P Schmid、Z Mueller 和 S Mangrulkar。Accelerate:
    大规模训练和推理变得简单、高效和可适应。 [https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate)，2022年。'
- en: 'He et al. [2018] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and
    Song Han. Amc: Automl for model compression and acceleration on mobile devices.
    In *Proceedings of the European conference on computer vision (ECCV)*, pages 784–800,
    2018.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. [2018] 赫毅辉、季林、刘志坚、韩瑞王、李佳李和宋汉。Amc: 移动设备上的模型压缩和加速的Automl。见于*欧洲计算机视觉会议（ECCV）*，第784–800页，2018年。'
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. [2020] 丹·亨德里克斯、科林·伯恩斯、史蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。测量大规模多任务语言理解。*arXiv预印本
    arXiv:2009.03300*，2020年。
- en: 'Hubara et al. [2021a] Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner,
    Joseph Naor, and Daniel Soudry. Accelerated sparse neural training: A provable
    and efficient method to find n: m transposable masks. *Advances in Neural Information
    Processing Systems*, 34:21099–21111, 2021a.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hubara et al. [2021a] 伊泰·胡巴拉、布莱恩·奇梅尔、摩西·岛、罗恩·班纳、约瑟夫·纳奥尔和丹尼尔·索德里。加速稀疏神经训练：一种可证明且高效的n:
    m可转置掩码查找方法。*神经信息处理系统进展*，34:21099–21111，2021a。'
- en: Hubara et al. [2021b] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In *International Conference on Machine Learning*, pages 4466–4475\. PMLR, 2021b.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. [2021b] 伊泰·胡巴拉、尤里·纳赫尚、雅伊尔·哈纳尼、罗恩·班纳和丹尼尔·索德里。使用小规模校准集进行准确的后训练量化。见于*国际机器学习会议*，第4466–4475页，PMLR，2021b。
- en: Jelinek et al. [1977] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K
    Baker. Perplexity—a measure of the difficulty of speech recognition tasks. *The
    Journal of the Acoustical Society of America*, 62(S1):S63–S63, 1977.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jelinek 等人 [1977] Fred Jelinek, Robert L Mercer, Lalit R Bahl 和 James K Baker。困惑度——语音识别任务难度的衡量标准。*美国声学学会期刊*，62(S1):S63–S63，1977年。
- en: Kwon et al. [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun,
    Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers.
    *arXiv preprint arXiv:2204.09656*, 2022.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 [2022] Woosuk Kwon, Sehoon Kim, Michael W Mahoney, Joseph Hassoun, Kurt
    Keutzer 和 Amir Gholami。用于变换器的快速后训练剪枝框架。*arXiv 预印本 arXiv:2204.09656*，2022年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pages 3045–3059, 2021.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester 等人 [2021] Brian Lester, Rami Al-Rfou 和 Noah Constant。参数高效提示调优的规模效应。在
    *2021年自然语言处理方法会议论文集*，第3045–3059页，2021年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Liang [2021] Xiang Lisa Li 和 Percy Liang。前缀调优：优化生成的连续提示。*arXiv 预印本 arXiv:2101.00190*，2021年。
- en: 'Liu et al. [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan,
    Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré, and
    Beidi Chen. Deja vu: Contextual sparsity for efficient llms at inference time.
    In *International Conference on Machine Learning*. PMLR, 2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao
    Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Ré 和 Beidi Chen。Deja
    vu：推理时高效 LLM 的上下文稀疏性。在 *国际机器学习会议* 上。PMLR，2023年。
- en: Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter [2019] Ilya Loshchilov 和 Frank Hutter。解耦权重衰减正则化。在 *第七届国际表示学习会议，ICLR
    2019，路易斯安那州新奥尔良，2019年5月6-9日*。OpenReview.net，2019年。网址 [https://openreview.net/forum?id=Bkg6RiCqY7](https://openreview.net/forum?id=Bkg6RiCqY7)。
- en: 'Marcus et al. [1994] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert
    MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn
    treebank: Annotating predicate argument structure. In *Human Language Technology:
    Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus 等人 [1994] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre,
    Ann Bies, Mark Ferguson, Karen Katz 和 Britta Schasberger。Penn Treebank：注释谓词论元结构。在
    *人类语言技术：在新泽西州普兰斯伯勒举办的研讨会论文集，1994年3月8-11日*，1994年。
- en: Merity et al. [2017] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *5th International Conference on Learning
    Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track
    Proceedings*. OpenReview.net, 2017. URL [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人 [2017] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher。指针哨兵混合模型。在
    *第五届国际表示学习会议，ICLR 2017，法国图卢兹，2017年4月24-26日，会议论文集*。OpenReview.net，2017年。网址 [https://openreview.net/forum?id=Byj72udxe](https://openreview.net/forum?id=Byj72udxe)。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pages 2381–2391, 2018.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等人 [2018] Todor Mihaylov, Peter Clark, Tushar Khot 和 Ashish Sabharwal。盔甲能导电吗？一个用于开放书籍问答的新数据集。在
    *2018年自然语言处理方法会议论文集*，第2381–2391页，2018年。
- en: 'Min et al. [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人 [2022] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi 和 Luke Zettlemoyer。重新思考示例的作用：是什么使上下文学习有效？*arXiv 预印本 arXiv:2202.12837*，2022年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pages 7197–7206\.
    PMLR, 2020.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人 [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos
    和 Tijmen Blankevoort。上还是下？后训练量化的自适应舍入。在 *国际机器学习会议*，第7197–7206页。PMLR，2020年。
- en: Radford et al. [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. Improving language understanding by generative pre-training. 2018.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2018] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever
    等。通过生成预训练提高语言理解能力。2018。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8)：9，2019。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *The
    Journal of Machine Learning Research*, 21(1):5485–5551, 2020.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人 [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu。探索统一的文本到文本变换器的迁移学习极限。*机器学习研究期刊*，21(1)：5485–5551，2020。
- en: Sheng et al. [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    and othersi. High-throughput generative inference of large language models with
    a single gpu. In *International Conference on Machine Learning*. PMLR, 2023.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人 [2023] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin,
    Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez 和其他人。使用单个
    GPU 的高通量生成推理。见 *国际机器学习会议*。PMLR，2023。
- en: 'Su et al. [2022] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai
    Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. On transferability
    of prompt tuning for natural language processing. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 3949–3969, 2022.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等人 [2022] Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin,
    Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li 等。关于自然语言处理中的提示调整的可迁移性。见
    *2022年北美计算语言学协会：人类语言技术会议论文集*，第3949–3969页，2022。
- en: 'Tang [2023] Yuxin Tang. Chain-of-thought prompting under streaming batch: A
    case study. *arXiv preprint arXiv:2306.00550*, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang [2023] Yuxin Tang。流式批处理下的思维链提示：案例研究。*arXiv 预印本 arXiv:2306.00550*，2023。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。Llama：开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。Llama 2：开放的基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。
- en: Wu et al. [2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe
    Liu, and Xin Jin. Fast distributed inference serving for large language models.
    *arXiv preprint arXiv:2305.05920*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2023] Bingyang Wu, Yinmin Zhong, Zili Zhang, Gang Huang, Xuanzhe Liu
    和 Xin Jin。大语言模型的快速分布式推理服务。*arXiv 预印本 arXiv:2305.05920*，2023。
- en: 'Xiao et al. [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2022] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth 和 Song
    Han。Smoothquant：大语言模型的准确高效的后训练量化。*arXiv 预印本 arXiv:2211.10438*，2022。
- en: Xie et al. [2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. In *The
    Tenth International Conference on Learning Representations, ICLR 2022, Virtual
    Event, April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=RdJVFCHjUMI](https://openreview.net/forum?id=RdJVFCHjUMI).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等人 [2022] Sang Michael Xie, Aditi Raghunathan, Percy Liang 和 Tengyu Ma。将上下文学习解释为隐式贝叶斯推断。见
    *第十届国际表示学习会议，ICLR 2022，虚拟会议，2022年4月25-29日*。OpenReview.net，2022。网址 [https://openreview.net/forum?id=RdJVFCHjUMI](https://openreview.net/forum?id=RdJVFCHjUMI)。
- en: Yuan et al. [2022] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri
    Dao, Beidi Chen, Percy S Liang, Christopher Re, and Ce Zhang. Decentralized training
    of foundation models in heterogeneous environments. *Advances in Neural Information
    Processing Systems*, 35:25464–25477, 2022.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等人[2022] Binhang Yuan, Yongjun He, Jared Davis, Tianyi Zhang, Tri Dao, Beidi
    Chen, Percy S Liang, Christopher Re, 和 Ce Zhang. 在异构环境中对基础模型进行去中心化训练。*神经信息处理系统进展*，35:25464–25477，2022年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers等人[2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin
    Choi. Hellaswag: 机器真的可以完成你的句子吗？在*第57届计算语言学协会年会论文集*，第4791–4800页，2019年。'
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. Opt: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人[2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin等人.
    Opt: 开放预训练变换器语言模型。*arXiv预印本 arXiv:2205.01068*，2022年。'
- en: Appendix
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A More Experiments
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 更多实验
- en: A.1 Experiment Details
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 实验细节
- en: In the experiment, we employed the AdamW [[21](#bib.bib21)] optimizer as our
    chosen optimizer. We conducted iterative prompt updates using a batch size of
    4, a weight decay of $10^{-5}$. We set the total optimization steps as 30,000
    and use the model corresponding to the best validation perplexity as the final
    model. To facilitate mix-precision training and system-level optimization, we
    leveraged the accelerate library [[11](#bib.bib11)].
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用AdamW[[21](#bib.bib21)]优化器作为选择的优化器。我们采用批量大小为4、权重衰减为$10^{-5}$的迭代提示更新。我们设置总优化步骤为30,000，并使用最佳验证困惑度对应的模型作为最终模型。为了便于混合精度训练和系统级优化，我们利用了accelerate库[[11](#bib.bib11)]。
- en: 'All experiments are conducted on a server with eight Nvidia V100 (32GB) GPUs,
    1.5T main memory, and two Intel Xeon CPU E5-2699A. The software and package version
    is specified below:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验均在一台配备八块Nvidia V100（32GB）GPU、1.5T主内存和两颗Intel Xeon CPU E5-2699A的服务器上进行。软件和包的版本如下所示：
- en: 'Table 3: Package configurations of our experiments.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们实验的包配置。
- en: '| Package | Version |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 包 | 版本 |'
- en: '| CUDA | 11.6 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| CUDA | 11.6 |'
- en: '| pytorch | 2.0.1 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| pytorch | 2.0.1 |'
- en: '| transformers | 4.30.0.dev0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| transformers | 4.30.0.dev0 |'
- en: '| accelerate | 0.18.0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| accelerate | 0.18.0 |'
- en: A.2 Ablation on the Transferability
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 转移能力的消融实验
- en: 'In Table [4](#A1.T4 "Table 4 ‣ A.2 Ablation on the Transferability ‣ Appendix
    A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt"), we conduct the ablation study on
    the transferability of the learned soft prompts using quantized LLaMA-7B on Wikitext2
    and PTB dataset. Specifically, we compare the transferred soft prompts against
    the soft prompts that are trained on the downstream dataset, which serve as the
    top-line counterpart. We observe that directly trained prompts perform better
    than our transferred prompts. However, we note that models with our transferred
    prompts are much closer to the top-line compared to the compressed model without
    prompts, especially for extremely compressed models. This suggests the effectiveness
    of our transferable prompts. We also observe that with learned soft prompt, the
    gap between the full model and quantized model is greatly reduced. For example,
    without learned prompts, the gaps between the full model and 3bit model are 4.72
    (PTB, 11.02 versus 15.74) and 3.12 (Wikitext2, 6.33 versus 9.45). However, after
    adding the learned prompt, the gap was reduced to 0.9 (PTB, 6.86 versus 7.76)
    and 0.75 (Wikitext-2, 5.58 versus 6.33). Also, after adding learned prompts, 4-bit
    quantized can almost match the full model with negligible perplexity drop, which
    highlights the importance of learned prompts.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [4](#A1.T4 "Table 4 ‣ A.2 Ablation on the Transferability ‣ Appendix A
    More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off
    of LLM Inference with Transferable Prompt") 中，我们对使用量化 LLaMA-7B 在 Wikitext2 和 PTB
    数据集上学习到的软提示的转移性进行了消融研究。具体来说，我们将转移的软提示与在下游数据集上训练的软提示进行了比较，后者作为顶线对照。我们观察到，直接训练的提示比我们转移的提示表现更好。然而，我们注意到，使用我们转移的提示的模型与没有提示的压缩模型相比，接近顶线特别是对于极度压缩的模型。这表明我们转移提示的有效性。我们还观察到，使用学习到的软提示后，完整模型与量化模型之间的差距大大缩小。例如，未使用学习提示时，完整模型与
    3-bit 模型之间的差距为 4.72（PTB，11.02 对比 15.74）和 3.12（Wikitext2，6.33 对比 9.45）。然而，添加学习提示后，差距缩小至
    0.9（PTB，6.86 对比 7.76）和 0.75（Wikitext-2，5.58 对比 6.33）。此外，添加学习提示后，4-bit 量化模型几乎可以匹配完整模型，且困惑度下降微乎其微，这突显了学习提示的重要性。'
- en: 'Table 4: Perplexity comparison between full model and quantized models with
    different prompts, where we report test perplexity on PTB and Wikitext-2 dataset.
    “w./o. prompt” refers to the quantized model without soft prompts.“w./ direct
    prompt” means the soft prompts are directly trained on the target dataset.“w./
    transferred prompt” means the prompt is trained on C4 dataset and then transferred
    to the target dataset.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同提示下完整模型与量化模型之间的困惑度比较，我们报告了在 PTB 和 Wikitext-2 数据集上的测试困惑度。“w./o. 提示”指没有软提示的量化模型。“w./
    直接提示”表示软提示直接在目标数据集上训练。“w./ 转移的提示”表示提示在 C4 数据集上训练后转移到目标数据集。'
- en: '| Model | PTB | Wikitext2 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | PTB | Wikitext2 |'
- en: '| Full Model | 11.02 | 6.33 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 | 11.02 | 6.33 |'
- en: '| Full Model w./ direct prompt | 6.86 | 5.57 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 完整模型 w./ 直接提示 | 6.86 | 5.57 |'
- en: '| 4-bit |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 11.65 | 6.92 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 11.65 | 6.92 |'
- en: '|'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ direct &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ 直接 &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 7.04 | 5.88 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 7.04 | 5.88 |'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ transferred &#124;'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ 转移的 &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 9.25 | 6.26 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 9.25 | 6.26 |'
- en: '| 3-bit |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 15.74 | 9.45 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 15.74 | 9.45 |'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ &#124;'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ &#124;'
- en: '&#124; direct prompt &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直接提示 &#124;'
- en: '| 7.76 | 6.33 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 7.76 | 6.33 |'
- en: '|'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./  transferred &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ 转移的 &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 10.81 | 6.90 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 10.81 | 6.90 |'
- en: '| 2-bit |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit |'
- en: '&#124; w./o. &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./o. &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 5883.13 | 2692.81 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 5883.13 | 2692.81 |'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ direct &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ 直接 &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 14.98 | 16.67 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 14.98 | 16.67 |'
- en: '|'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; w./ transferred &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; w./ 转移的 &#124;'
- en: '&#124; prompt &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提示 &#124;'
- en: '| 29.82 | 20.56 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 29.82 | 20.56 |'
- en: A.3 Cross-Task Transferability
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 跨任务转移性
- en: 'In this section, we explore the transferability of learned prompts across different
    tasks. Specifically, we aim to assess the effectiveness of prompts learned from
    token generation tasks, as indicated by Eq ([1](#S4.E1 "In 4.2 Learning Objectives
    ‣ 4 Learning Prompt for Efficient LLM Inference ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt")), in
    downstream tasks of LLM. As an illustrative example, we consider the zero-shot
    generalization tasks of LLaMA-7B [[33](#bib.bib33)]. For evaluation purposes,
    we have chosen OpenbookQA [[24](#bib.bib24)], Hellaswag [[39](#bib.bib39)], PIQA [[1](#bib.bib1)],
    and the high school European history task from [[13](#bib.bib13)]. The European
    history task is particularly interesting due to its inclusion of a lengthy context
    sentence for each question. We employ the lm-evaluation-hardness framework [[8](#bib.bib8)],
    incorporating adapters from [[38](#bib.bib38)], for the purpose of conducting
    the experiment.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探索了学习到的提示在不同任务间的可转移性。具体来说，我们旨在评估从令牌生成任务中学习到的提示在 LLM 下游任务中的有效性，如 Eq ([1](#S4.E1
    "在 4.2 学习目标 ‣ 4 为高效 LLM 推理学习提示 ‣ 压缩，然后提示：通过可转移提示提高 LLM 推理的准确性-效率权衡")) 所示。作为一个示例，我们考虑了
    LLaMA-7B 的零样本泛化任务 [[33](#bib.bib33)]。为了评估，我们选择了 OpenbookQA [[24](#bib.bib24)]、Hellaswag [[39](#bib.bib39)]、PIQA [[1](#bib.bib1)]
    和来自 [[13](#bib.bib13)] 的高中欧洲历史任务。由于每个问题都包含一段较长的上下文句子，欧洲历史任务尤为有趣。我们使用了 lm-evaluation-hardness
    框架 [[8](#bib.bib8)]，并结合了来自 [[38](#bib.bib38)] 的适配器，以进行实验。
- en: 'Table [5](#A1.T5 "Table 5 ‣ A.4 Efficiency Profiling ‣ Appendix A More Experiments
    ‣ Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference
    with Transferable Prompt") presents the results in terms of normalized accuracy,
    and we also include the standard deviation, as indicated by [[8](#bib.bib8)].
    The table clearly demonstrates that the learned prompt significantly enhances
    the accuracy of these tasks. These findings imply that prompts acquired through
    token generation tasks can effectively enhance the accuracy-efficiency trade-off
    of compressed LLMs.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A1.T5 "表 5 ‣ A.4 效率分析 ‣ 附录 A 更多实验 ‣ 压缩，然后提示：通过可转移提示提高 LLM 推理的准确性-效率权衡")
    显示了标准化准确性的结果，我们还包括了标准差，如 [[8](#bib.bib8)] 所示。表格清楚地表明，学习到的提示显著提高了这些任务的准确性。这些发现表明，通过令牌生成任务获得的提示可以有效地提升压缩
    LLM 的准确性-效率权衡。
- en: A.4 Efficiency Profiling
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 效率分析
- en: '![Refer to caption](img/40dad02bab150b86fe9e9b3d94e8ff32.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/40dad02bab150b86fe9e9b3d94e8ff32.png)'
- en: 'Figure 6: Caption'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：图例
- en: 'In this section, we analyze how the inclusion of prompt tokens impacts the
    latency of LLM inference. Figure [6](#A1.F6 "Figure 6 ‣ A.4 Efficiency Profiling
    ‣ Appendix A More Experiments ‣ Compress, Then Prompt: Improving Accuracy-Efficiency
    Trade-off of LLM Inference with Transferable Prompt") illustrates the latency
    of three OPT models and the LLaMA-7B model utilized in this paper, considering
    the insertion of additional prompt tokens with varying lengths. For token generation,
    we set the sequence length to 1024\. The figure demonstrates that the addition
    of prompt tokens does not significantly increase the latency of LLM inference,
    particularly when the inserted tokens account for less than 10% of the original
    sequence length. Furthermore, our observations indicate that the latency does
    not exhibit a linear correlation with the length of the inserted tokens, highlighting
    the effectiveness of the prompt in facilitating efficient LLM inference.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了提示令牌的包含如何影响 LLM 推理的延迟。图 [6](#A1.F6 "图 6 ‣ A.4 效率分析 ‣ 附录 A 更多实验 ‣ 压缩，然后提示：通过可转移提示提高
    LLM 推理的准确性-效率权衡") 说明了本文所使用的三种 OPT 模型和 LLaMA-7B 模型的延迟情况，考虑了插入不同长度的附加提示令牌。对于令牌生成，我们将序列长度设置为
    1024。图中表明，提示令牌的添加并不会显著增加 LLM 推理的延迟，特别是当插入的令牌占原始序列长度的比例小于 10% 时。此外，我们的观察结果表明，延迟与插入令牌的长度并没有线性相关，突显了提示在促进高效
    LLM 推理方面的有效性。
- en: 'Table 5: The zero-shot results on transforming the learned prompt to OpenBookQA,
    Hellaswag, PIQA, and High School European History dataset.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在将学习到的提示转换到 OpenBookQA、Hellaswag、PIQA 和高中欧洲历史数据集上的零样本结果。
- en: '| Models |  | OpenbookQA | Hellaswag | PIQA | High School European History
    |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |  | OpenbookQA | Hellaswag | PIQA | 高中欧洲历史 |'
- en: '| Full |  | 0.410±0.022 | 0.497±0.005 | 0.702±0.011 | 0.364±0.038 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 完整 |  | 0.410±0.022 | 0.497±0.005 | 0.702±0.011 | 0.364±0.038 |'
- en: '| 50% | w./o. Prompt | 0.412±0.022 | 0.449±0.005 | 0.682±0.011 | 0.364±0.038
    |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 50% | 无提示 | 0.412±0.022 | 0.449±0.005 | 0.682±0.011 | 0.364±0.038 |'
- en: '| + Learned Prompt | 0.400±0.022 | 0.469±0.005 | 0.689±0.011 | 0.358±0.037
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.400±0.022 | 0.469±0.005 | 0.689±0.011 | 0.358±0.037 |'
- en: '| 62.5% | w./o. Prompt | 0.396±0.022 | 0.380±0.005 | 0.638±0.011 | 0.345±0.037
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 62.5% | 无提示 | 0.396±0.022 | 0.380±0.005 | 0.638±0.011 | 0.345±0.037 |'
- en: '| + Learned Prompt | 0.402±0.022 | 0.433±0.005 | 0.668±0.011 | 0.345±0.037
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.402±0.022 | 0.433±0.005 | 0.668±0.011 | 0.345±0.037 |'
- en: '| 75% | w./o. Prompt | 0.366±0.022 | 0.280±0.004 | 0.549±0.012 | 0.315±0.036
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 75% | 无提示 | 0.366±0.022 | 0.280±0.004 | 0.549±0.012 | 0.315±0.036 |'
- en: '| + Learned Prompt | 0.358±0.021 | 0.344±0.005 | 0.614±0.011 | 0.358±0.037
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.358±0.021 | 0.344±0.005 | 0.614±0.011 | 0.358±0.037 |'
- en: '| 4-bit | w./o. Prompt | 0.410±0.022 | 0.487±0.005 | 0.690±0.011 | 0.358±0.037
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 无提示 | 0.410±0.022 | 0.487±0.005 | 0.690±0.011 | 0.358±0.037 |'
- en: '| + Learned Prompt | 0.418±0.022 | 0.487±0.005 | 0.692±0.011 | 0.352±0.037
    |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.418±0.022 | 0.487±0.005 | 0.692±0.011 | 0.352±0.037 |'
- en: '| 3-bit | w./o. Prompt | 0.378±0.022 | 0.446±0.005 | 0.674±0.011 | 0.358±0.037
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | 无提示 | 0.378±0.022 | 0.446±0.005 | 0.674±0.011 | 0.358±0.037 |'
- en: '| + Learned Prompt | 0.404±0.022 | 0.459±0.005 | 0.688±0.011 | 0.358±0.037
    |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.404±0.022 | 0.459±0.005 | 0.688±0.011 | 0.358±0.037 |'
- en: '| 2-bit | w./o. Prompt | 0.354±0.021 | 0.240±0.004 | 0.491±0.012 | 0.315±0.036
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | 无提示 | 0.354±0.021 | 0.240±0.004 | 0.491±0.012 | 0.315±0.036 |'
- en: '| + Learned Prompt | 0.350±0.021 | 0.294±0.005 | 0.563±0.012 | 0.333±0.037
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| + 学习到的提示 | 0.350±0.021 | 0.294±0.005 | 0.563±0.012 | 0.333±0.037 |'
- en: Appendix B More Visualization
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更多可视化
- en: 'In this section, we present further visualizations of compression-aware prompts,
    as demonstrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Compress, Then
    Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable
    Prompt") in Section [1](#S1 "1 Introduction ‣ Compress, Then Prompt: Improving
    Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt"). The
    results unveil a significant improvement achieved by utilizing a hard, task-independent
    prompt on compressed LLMs. Additionally, we showcase the visualization of responses
    generated using our prompt derived from the C4 training set. It is worth noting
    that, in certain instances, the task-independent and learned prompt outperforms
    the hard prompt.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了压缩感知提示的进一步可视化，如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 压缩，然后提示：通过可转移提示提高 LLM 推理的准确性-效率权衡")
    在第 [1](#S1 "1 引言 ‣ 压缩，然后提示：通过可转移提示提高 LLM 推理的准确性-效率权衡") 节所示。结果揭示了在压缩 LLM 上利用硬的、任务无关的提示所取得的显著改善。此外，我们展示了使用我们从
    C4 训练集中衍生的提示生成的响应的可视化。值得注意的是，在某些情况下，任务无关的提示和学习到的提示优于硬提示。
- en: '![Refer to caption](img/ced5f317593922f7b67fb5e72a6fd603.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ced5f317593922f7b67fb5e72a6fd603.png)'
- en: 'Figure 7: Case study for the effect of prompts on a pruned LLaMA-7B with a
    62.5% weight sparsity.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：关于提示对剪枝 LLaMA-7B 在 62.5% 权重稀疏性的影响的案例研究。
- en: '![Refer to caption](img/79f7ade8f36b17ce587ad6d3370f13d3.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/79f7ade8f36b17ce587ad6d3370f13d3.png)'
- en: 'Figure 8: Case study for the effect of prompts on a pruned LLaMA-7B with a
    4-bit quantization.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：关于提示对剪枝 LLaMA-7B 在 4-bit 量化下的影响的案例研究。
