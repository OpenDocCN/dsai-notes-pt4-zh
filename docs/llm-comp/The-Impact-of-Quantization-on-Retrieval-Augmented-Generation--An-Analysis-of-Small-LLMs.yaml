- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:48:07'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:07
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化对检索增强生成的影响：对小型LLM的分析
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10251](https://ar5iv.labs.arxiv.org/html/2406.10251)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.10251](https://ar5iv.labs.arxiv.org/html/2406.10251)
- en: Mert Yazan [m.yazan@hva.nl](mailto:m.yazan@hva.nl) [0009-0004-3866-597X](https://orcid.org/0009-0004-3866-597X
    "ORCID identifier") Amsterdam University of Applied SciencesAmsterdamNetherlands
    ,  Suzan Verberne [s.verberne@liacs.leidenuniv.nl](mailto:s.verberne@liacs.leidenuniv.nl)
    [0000-0002-9609-9505](https://orcid.org/0000-0002-9609-9505 "ORCID identifier")
    University of LeidenLeidenNetherlands  and  Frederik Situmeang [f.b.i.situmeang@uva.nl](mailto:f.b.i.situmeang@uva.nl)
    [0000-0002-2156-2083](https://orcid.org/0000-0002-2156-2083 "ORCID identifier")
    Amsterdam University of Applied SciencesAmsterdamNetherlands(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Mert Yazan [m.yazan@hva.nl](mailto:m.yazan@hva.nl) [0009-0004-3866-597X](https://orcid.org/0009-0004-3866-597X
    "ORCID identifier") 阿姆斯特丹应用科学大学阿姆斯特丹荷兰，Suzan Verberne [s.verberne@liacs.leidenuniv.nl](mailto:s.verberne@liacs.leidenuniv.nl)
    [0000-0002-9609-9505](https://orcid.org/0000-0002-9609-9505 "ORCID identifier")
    莱顿大学莱顿荷兰，Frederik Situmeang [f.b.i.situmeang@uva.nl](mailto:f.b.i.situmeang@uva.nl)
    [0000-0002-2156-2083](https://orcid.org/0000-0002-2156-2083 "ORCID identifier")
    阿姆斯特丹应用科学大学阿姆斯特丹荷兰（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Post-training quantization reduces the computational demand of Large Language
    Models (LLMs) but can weaken some of their capabilities. Since LLM abilities emerge
    with scale, smaller LLMs are more sensitive to quantization. In this paper, we
    explore how quantization affects smaller LLMs’ ability to perform retrieval-augmented
    generation (RAG), specifically in longer contexts. We chose personalization for
    evaluation because it is a challenging domain to perform using RAG as it requires
    long-context reasoning over multiple documents. We compare the original FP16 and
    the quantized INT4 performance of multiple 7B and 8B LLMs on two tasks while progressively
    increasing the number of retrieved documents to test how quantized models fare
    against longer contexts. To better understand the effect of retrieval, we evaluate
    three retrieval models in our experiments. Our findings reveal that if a 7B LLM
    performs the task well, quantization does not impair its performance and long-context
    reasoning capabilities. We conclude that it is possible to utilize RAG with quantized
    smaller LLMs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化减少了大型语言模型（LLMs）的计算需求，但可能会削弱它们的一些能力。由于LLM的能力随着规模的增大而出现，较小的LLM对量化更为敏感。在本文中，我们探讨了量化如何影响较小LLM在检索增强生成（RAG）中的表现，特别是在较长的上下文中。我们选择了个性化作为评估对象，因为它是一个使用RAG进行的具有挑战性的领域，因为它需要在多个文档上进行长上下文推理。我们比较了多个7B和8B
    LLM在两个任务上的原始FP16和量化INT4的表现，同时逐步增加检索文档的数量，以测试量化模型在较长上下文中的表现。为了更好地理解检索的影响，我们在实验中评估了三种检索模型。我们的发现揭示了，如果一个7B
    LLM能够很好地完成任务，量化不会损害其性能和长上下文推理能力。我们得出结论，使用量化的小型LLM进行RAG是可能的。
- en: 'Retrieval Augmented Generation, Quantization, Efficiency, Large Language Models,
    Personalization^†^†copyright: none^†^†journalyear: 2024^†^†doi: XXXXXXX.XXXXXXX^†^†ccs:
    Information systems Personalization^†^†ccs: Computing methodologies Natural language
    generation'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成，量化，效率，大型语言模型，个性化^†^†版权：无^†^†期刊年份：2024^†^†doi：XXXXXXX.XXXXXXX^†^†ccs：信息系统
    个性化^†^†ccs：计算方法 自然语言生成
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Large Language Model (LLM) outputs can be enhanced by fetching relevant documents
    via a retriever and adding them as context for the prompt. The LLM can generate
    an output grounded with relevant information with the added context. This process
    is called Retrieval Augmented Generation (RAG). RAG has many benefits such as
    improving effectiveness in downstream tasks (Huang et al., [2023](#bib.bib5);
    Ma et al., [2023](#bib.bib13); Shi et al., [2023](#bib.bib21); Xu et al., [2023](#bib.bib25)),
    reducing hallucinations (Proser, [[n. d.]](#bib.bib17)), increasing factuality
    (Nakano et al., [2022](#bib.bib15)), by-passing knowledge cut-offs, and presenting
    proprietary data that is not available to the LLMs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的输出可以通过检索器获取相关文档并将其作为提示的上下文来增强。LLM可以生成基于相关信息的输出，并结合这些附加的上下文。这个过程称为检索增强生成（RAG）。RAG有许多好处，如提高下游任务的效果（Huang
    et al., [2023](#bib.bib5); Ma et al., [2023](#bib.bib13); Shi et al., [2023](#bib.bib21);
    Xu et al., [2023](#bib.bib25)），减少幻觉（Proser, [[n. d.]](#bib.bib17)），提高事实性（Nakano
    et al., [2022](#bib.bib15)），绕过知识截止点，以及呈现不为LLM所知的专有数据。
- en: 'The performance of RAG depends on the number, quality, and relevance of the
    retrieved documents (Gao et al., [2024](#bib.bib4)). To perform RAG, many tasks
    demand a lot of passages extracted from multiple, unstructured documents: For
    question-answering tasks, the answer might be scattered around many documents
    because of ambiguity or the time-series nature of the question (eg. price change
    of a stock). For more open-ended tasks like personalization, many documents from
    different sources might be needed to capture the characteristics of the individual.
    Therefore to handle RAG in these tasks, an LLM needs to look at multiple sources,
    identify the relevant parts, and compose the most plausible answer (Gao et al.,
    [2024](#bib.bib4)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 的性能依赖于检索文档的数量、质量和相关性 (Gao et al., [2024](#bib.bib4))。要执行 RAG，许多任务需要从多个非结构化文档中提取大量片段：对于问答任务，答案可能因模糊性或问题的时间序列特性而散布在许多文档中（例如，股票价格变动）。对于更开放的任务如个性化，可能需要来自不同来源的许多文档来捕捉个体的特征。因此，为了处理这些任务中的
    RAG，LLM 需要查看多个来源，识别相关部分，并组合出最可信的答案 (Gao et al., [2024](#bib.bib4))。
- en: LLMs do not pay the same attention to their whole context windows, meaning the
    placement of documents in the prompt directly affects the final output (Liu et al.,
    [2023](#bib.bib12)). On top of that, some of the retrieved documents may be unrelated
    to the task, or they may contain contradictory information compared to the parametric
    knowledge of the LLM (Xu et al., [2024](#bib.bib26)). An LLM has to overcome these
    challenges to leverage RAG to its advantage. Xu et al. ([2023](#bib.bib25)) have
    shown that an open-source 70B LLM (Touvron et al., [2023](#bib.bib22)) equipped
    with RAG can beat proprietary models, meaning it is not necessary to use an LLM
    in the caliber of GPT-4 (OpenAI, [2023](#bib.bib16)) to implement RAG. Still,
    for many use cases, it might not be feasible to deploy a 70B LLM as it is computationally
    demanding. To decrease the computational demand of LLMs, post-training quantization
    can be used. Quantization drastically reduces the required amount of RAM to load
    a model and can increase the inference speed by more than 3 times (Dettmers et al.,
    [2023](#bib.bib2); Frantar et al., [2023](#bib.bib3)). Despite the benefits, quantization
    affects LLMs differently depending on their size (Li et al., [2024](#bib.bib9)).
    For capabilities that are important to RAG, such as long-context reasoning, smaller
    LLMs (¡13B) are found to be more sensitive to quantization (Li et al., [2024](#bib.bib9)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 不会对它们的整个上下文窗口给予相同的关注，这意味着文档在提示中的位置直接影响最终的输出 (Liu et al., [2023](#bib.bib12))。此外，一些检索到的文档可能与任务无关，或者与
    LLM 的参数知识相比包含矛盾的信息 (Xu et al., [2024](#bib.bib26))。LLM 必须克服这些挑战，才能利用 RAG 达到其优势。Xu
    et al. ([2023](#bib.bib25)) 证明了一款开源的 70B LLM (Touvron et al., [2023](#bib.bib22))
    配备 RAG 可以超越专有模型，这意味着不必使用 GPT-4 (OpenAI, [2023](#bib.bib16)) 这样等级的 LLM 来实施 RAG。不过，对于许多使用场景，部署一个
    70B LLM 可能不切实际，因为它计算要求高。为了降低 LLM 的计算需求，可以使用后训练量化。量化可以大幅减少加载模型所需的 RAM，并且可以将推理速度提高超过
    3 倍 (Dettmers et al., [2023](#bib.bib2); Frantar et al., [2023](#bib.bib3))。尽管有这些好处，量化对
    LLM 的影响因其大小而异 (Li et al., [2024](#bib.bib9))。对于 RAG 重要的能力，例如长上下文推理，较小的 LLM (¡13B)
    被发现对量化更为敏感 (Li et al., [2024](#bib.bib9))。
- en: '![Refer to caption](img/b848b618b2a9c0d984bdcbb14611d572.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b848b618b2a9c0d984bdcbb14611d572.png)'
- en: Figure 1\. Prompts used for both datasets. The ones on the top represent $k=0$
    settings (RAG). The green text is the model output. Line endings are not shown
    for space reasons.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 两个数据集使用的提示。顶部的表示 $k=0$ 设置 (RAG)。绿色文本是模型输出。由于空间原因，行结束未显示。
- en: \Description
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: \Description
- en: Prompts used during experiments. LaMP-3U prompts are on the left and LaMP-5U
    prompts are on the right
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中使用的提示。LaMP-3U 提示在左侧，LaMP-5U 提示在右侧
- en: 'In this paper, we investigate the effectiveness of quantization on RAG-enhanced
    7B and 8B LLMs. We evaluate the full (FP16) and quantized (INT4) versions of multiple
    LLMs on two personalization tasks taken from the LaMP (Salemi et al., [2023](#bib.bib19))
    benchmark. To better study how quantized LLMs perform in longer contexts, we compared
    the performance gap between FP16 and INT4 models with an increasing number of
    retrieved documents. We chose personalization because it is a challenging task
    to perform with RAG as it demands long-context reasoning over many documents.
    Contrary to question-answering where the LLM has to find the correct answer from
    a couple of documents, personalization requires the LLM to carefully study a person’s
    style from all the provided documents. Our findings show that the effect of quantization
    depends on the model and the task: we find almost no drop in performance for OpenChat
    while LLaMA2 seems to be more sensitive. Our experiments show that quantized smaller
    LLMs can be good candidates for RAG pipelines, especially if efficiency is essential.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了量化对 RAG 增强的 7B 和 8B LLMs 的有效性。我们在两个来自 LaMP (Salemi et al., [2023](#bib.bib19))
    基准的个性化任务上评估了多个 LLMs 的全精度 (FP16) 和量化 (INT4) 版本。为了更好地研究量化 LLMs 在较长上下文中的表现，我们比较了
    FP16 和 INT4 模型在检索文档数量增加时的性能差距。我们选择个性化任务是因为它在 RAG 中具有挑战性，因为它要求对许多文档进行长时间上下文推理。与需要从几篇文档中找到正确答案的问题回答不同，个性化需要
    LLM 仔细研究所有提供的文档中的一个人的风格。我们的发现表明，量化的效果取决于模型和任务：我们发现 OpenChat 的性能几乎没有下降，而 LLaMA2
    似乎更敏感。我们的实验表明，量化的较小 LLMs 可以成为 RAG 流水线的良好候选，特别是在效率至关重要的情况下。
- en: 2\. Approach
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 方法
- en: 2.1\. LLMs
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLMs
- en: 'Starting with LLaMA2-7B (Chat-hf) (Touvron et al., [2023](#bib.bib22)) to have
    a baseline, we experiment with the following LLMs: LLaMA3-8B (Meta, [2024](#bib.bib14)),
    Zephyr (Beta) (Tunstall et al., [2023](#bib.bib23)), OpenChat (3.5) (Wang et al.,
    [2023](#bib.bib24)), and Starling (LM-alpha) (Zhu et al., [2023](#bib.bib28)).
    These models were chosen because they were the highest-ranked 7B and 8B LLMs in
    the Chatbot Arena Leaderboard (Zheng et al., [2023](#bib.bib27)) according to
    the Elo ratings at the time of writing. Since all models except LLaMA are finetuned
    variants of Mistral-7B (Jiang et al., [2023](#bib.bib7)), we add Mistral-7B (Instruct-0.1)¹¹1Although
    there is an updated v0.2 version of Mistral-7B, we used v0.1 to match the other
    LLMs that are finetuned on it to our experiments too. We use Activation-aware
    Weight Quantization (AWQ) as it outperforms other methods (Lin et al., [2023](#bib.bib11)).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从 LLaMA2-7B (Chat-hf) (Touvron et al., [2023](#bib.bib22)) 开始作为基线，我们对以下 LLMs
    进行实验：LLaMA3-8B (Meta, [2024](#bib.bib14))、Zephyr (Beta) (Tunstall et al., [2023](#bib.bib23))、OpenChat
    (3.5) (Wang et al., [2023](#bib.bib24)) 和 Starling (LM-alpha) (Zhu et al., [2023](#bib.bib28))。选择这些模型是因为它们在
    Chatbot Arena Leaderboard (Zheng et al., [2023](#bib.bib27)) 中，根据当时的 Elo 排名，是排名最高的
    7B 和 8B LLMs。由于除了 LLaMA 外，所有模型都是 Mistral-7B (Jiang et al., [2023](#bib.bib7))
    的微调变体，我们添加了 Mistral-7B (Instruct-0.1)¹¹ 尽管有更新的 v0.2 版本，但我们使用了 v0.1，以便与其他在其上进行微调的
    LLMs 保持一致。我们使用了激活感知权重量化 (AWQ)，因为它的表现优于其他方法 (Lin et al., [2023](#bib.bib11))。
- en: 2.2\. Tasks and Datasets
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 任务和数据集
- en: 'We use the LaMP benchmark that offers 7 personalization datasets with either
    a classification or a generation task (Salemi et al., [2023](#bib.bib19)). To
    represent both types of tasks, we chose one dataset from each: LaMP-3 (“Personalized
    Product Rating”) and LaMP-5 (“Personalized Scholarly Title Generation”). LaMP-3
    is composed of product reviews and their corresponding scores. For each user,
    one of the review–score pairs is chosen as the target and other pairs become the
    user profile. The LLM’s task, in this case, is to predict the score given a review
    using the other review–score pairs of the same user. LaMP-5 aims to generate a
    title for an academic paper based on the abstract. In this case, the user profile
    consists of abstract–title pairs that demonstrate the writing style of the user
    (scholar). The task of the LLM is to generate a title for the given abstract by
    incorporating the writing style of the scholar. Those datasets were chosen because
    compared to the other ones, on average, they had more samples in their user profiles,
    and the samples were longer. Therefore, they represented a better opportunity
    to evaluate RAG effectiveness as the retrieval part would be trickier.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用LaMP基准数据集，它提供了7个个性化数据集，包括分类任务或生成任务（Salemi等，[2023](#bib.bib19)）。为了代表这两种任务，我们从每种任务中选择了一个数据集：LaMP-3（“个性化产品评分”）和LaMP-5（“个性化学术标题生成”）。LaMP-3由产品评论及其对应的评分组成。对于每个用户，从评论–评分对中选择一个作为目标，其余对成为用户档案。在这种情况下，LLM的任务是根据其他评论–评分对预测给定评论的评分。LaMP-5的目标是根据摘要生成学术论文的标题。在这种情况下，用户档案由展示用户（学者）写作风格的摘要–标题对组成。LLM的任务是生成给定摘要的标题，并融入学者的写作风格。这些数据集之所以被选择，是因为与其他数据集相比，它们的用户档案中的样本更多，且样本更长。因此，它们提供了更好的机会来评估RAG的有效性，因为检索部分会更加复杂。
- en: We work with the user-based splits (LaMP-3U, LaMP-5U) where the user appears
    only in one of the data splits (Salemi et al., [2023](#bib.bib19)). The labels
    for the test sets are not publicly available (results can be obtained by submitting
    the predictions to the leaderboard) and since we did not fine-tune our models,
    we chose to use the validation sets for evaluation. For both datasets, we noticed
    that some samples do not fit in the context windows. After analyzing the overall
    length of the samples, we concluded that those cases only represent a tiny minority
    and removed data points that are not in the 0.995th percentile. For LaMP-5U, we
    also removed abstracts that consisted only of the text “no abstract available”.
    There are 2500 samples in the validation sets, and we have 2487 samples left after
    the preprocessing steps for both datasets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于用户的拆分（LaMP-3U，LaMP-5U），其中用户只出现在一个数据拆分中（Salemi等，[2023](#bib.bib19)）。测试集的标签不公开（可以通过提交预测到排行榜获得结果），由于我们没有微调我们的模型，我们选择使用验证集进行评估。对于这两个数据集，我们注意到一些样本不适合上下文窗口。经过分析样本的总体长度，我们得出结论，这些情况仅代表极少数，并去除了不在0.995百分位的数据点。对于LaMP-5U，我们还去除了仅包含“无摘要可用”文本的摘要。验证集中有2500个样本，经过预处理步骤后，我们剩下2487个样本。
- en: 'Evaluation. We used mean absolute error (MAE) for LaMP-3 and Rouge-L (Lin,
    [2004](#bib.bib10)) for LaMP-5, following the LaMP paper (Salemi et al., [2023](#bib.bib19)).
    Their experiments also include root mean square error (RMSE) and Rouge-1 scores,
    but we found that the correlation between MAE and RMSE is 0.94, and between Rouge-1
    and Rouge-L is 0.99\. Therefore, we do not include those metrics in our results.
    The prompts we use are shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs"). Even though the LLMs are instructed to output only the score or
    the title, we notice that some are prone to give lengthy answers such as “Sure,
    here is the title for the given abstract, Title: (generated title)”. We apply
    a post-processing step on the LLM outputs to extract only the score or the title
    before evaluation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '评估。我们使用了平均绝对误差（MAE）用于LaMP-3，使用Rouge-L（Lin，[2004](#bib.bib10)）用于LaMP-5，遵循LaMP论文（Salemi等，[2023](#bib.bib19)）。他们的实验还包括均方根误差（RMSE）和Rouge-1分数，但我们发现MAE与RMSE之间的相关性为0.94，而Rouge-1与Rouge-L之间的相关性为0.99。因此，我们在结果中不包括这些指标。我们使用的提示如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ The Impact of Quantization on Retrieval-Augmented
    Generation: An Analysis of Small LLMs")所示。尽管LLMs被指示仅输出评分或标题，我们注意到一些LLMs倾向于给出冗长的回答，如“当然，这里是给定摘要的标题，标题：（生成的标题）”。我们在LLM输出上应用后处理步骤，以提取仅包含评分或标题的内容。'
- en: 2.3\. Retrieval
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 检索
- en: 'We conduct the experiments with the following number of retrieved documents:
    $k\in\{0,1,3,5,max\_4K,max\_8K\}$ in LaMP-5U, depending on the average length
    of documents in the user profile. As retrievers, we evaluate BM25 (Robertson et al.,
    [1994](#bib.bib18)) (BM25 Okapi) ²²2[https://pypi.org/project/rank-bm25/](https://pypi.org/project/rank-bm25/),
    Contriever (Izacard et al., [2022](#bib.bib6)) (finetuned on MS-Marco), and DPR
    (Karpukhin et al., [2020](#bib.bib8)) (finetuned on Natural Questions)³³3[https://huggingface.co/facebook/dpr-question_encoder-single-nq-base](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base).
    Since we focus on efficiency by reducing the computational load, the retrievers
    are not finetuned on the datasets.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行实验时使用了以下检索文档的数量：$k\in\{0,1,3,5,max\_4K,max\_8K\}$，具体取决于用户档案中文档的平均长度。作为检索器，我们评估了BM25（Robertson等人，[1994](#bib.bib18)）（BM25
    Okapi）²²2[https://pypi.org/project/rank-bm25/](https://pypi.org/project/rank-bm25/)，Contriever（Izacard等人，[2022](#bib.bib6)）（在MS-Marco上微调），以及DPR（Karpukhin等人，[2020](#bib.bib8)）（在Natural
    Questions上微调）³³3[https://huggingface.co/facebook/dpr-question_encoder-single-nq-base](https://huggingface.co/facebook/dpr-question_encoder-single-nq-base)。由于我们专注于通过减少计算负担来提高效率，因此这些检索器未在数据集上进行微调。
- en: '|  | LLaMA2 | OpenChat | Starling | Zephyr | Mistral | LLaMA3 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | LLaMA2 | OpenChat | Starling | Zephyr | Mistral | LLaMA3 |'
- en: '| Dataset | Metric | k | FP16 | INT4 | FP16 | INT4 | FP16 | INT4 | FP16 | INT4
    | FP16 | INT4 | FP16 | INT4 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 指标 | k | FP16 | INT4 | FP16 | INT4 | FP16 | INT4 | FP16 | INT4 | FP16
    | INT4 | FP16 | INT4 |'
- en: '| LaMP-3U | MAE $\downarrow$ | 0 | 0.684 | +2.9% | 0.440 | -7.8% | 1.603 |
    +45% | 0.435 | -14.7% | 0.569 | -2.5% | 0.481 | -5.9% |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| LaMP-3U | MAE $\downarrow$ | 0 | 0.684 | +2.9% | 0.440 | -7.8% | 1.603 |
    +45% | 0.435 | -14.7% | 0.569 | -2.5% | 0.481 | -5.9% |'
- en: '| 1 | 0.453 | -1.1% | 0.312 | +5.5% | 0.800 | +7.1% | 0.300 | +1.9% | 0.461
    | -9.3% | 0.364 | -10.8% |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.453 | -1.1% | 0.312 | +5.5% | 0.800 | +7.1% | 0.300 | +1.9% | 0.461
    | -9.3% | 0.364 | -10.8% |'
- en: '| 3 | 0.637 | -7.6% | 0.256 | +2.8% | 0.718 | -30.0% | 0.273 | +2.6% | 0.404
    | -8.0% | 0.320 | -9.2% |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.637 | -7.6% | 0.256 | +2.8% | 0.718 | -30.0% | 0.273 | +2.6% | 0.404
    | -8.0% | 0.320 | -9.2% |'
- en: '| 5 | 0.724 | -23.3% | 0.238 | +1.8% | 0.797 | -32.0% | 0.266 | +0.8% | 0.380
    | -8.1% | 0.305 | -13.0% |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.724 | -23.3% | 0.238 | +1.8% | 0.797 | -32.0% | 0.266 | +0.8% | 0.380
    | -8.1% | 0.305 | -13.0% |'
- en: '| max_4K | 0.508 | -80.2% | 0.224 | +1.8% | 0.985 | -57.1% | 0.237 | -4.4%
    | 0.346 | -14.7% | 0.285 | -23.1% |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| max_4K | 0.508 | -80.2% | 0.224 | +1.8% | 0.985 | -57.1% | 0.237 | -4.4%
    | 0.346 | -14.7% | 0.285 | -23.1% |'
- en: '| max_8K | - | - | 0.257 | -3.9% | 1.352 | -1.1% | 0.392 | -6.3% | 0.368 |
    -16.1% | 0.288 | -23.4% |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| max_8K | - | - | 0.257 | -3.9% | 1.352 | -1.1% | 0.392 | -6.3% | 0.368 |
    -16.1% | 0.288 | -23.4% |'
- en: '| LaMP-5U | Rouge-L $\uparrow$ | 0 | 0.338 | -0.6% | 0.361 | -0.5% | 0.359
    | -2.3% | 0.335 | -0.4% | 0.361 | +1.2% | 0.384 | -2.5% |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| LaMP-5U | Rouge-L $\uparrow$ | 0 | 0.338 | -0.6% | 0.361 | -0.5% | 0.359
    | -2.3% | 0.335 | -0.4% | 0.361 | +1.2% | 0.384 | -2.5% |'
- en: '| 1 | 0.380 | -9.7% | 0.404 | -1.0% | 0.397 | -1.0% | 0.360 | +0.9% | 0.400
    | -0.9% | 0.402 | -5.6% |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.380 | -9.7% | 0.404 | -1.0% | 0.397 | -1.0% | 0.360 | +0.9% | 0.400
    | -0.9% | 0.402 | -5.6% |'
- en: '| 3 | 0.385 | -11.6% | 0.415 | 0.0% | 0.412 | -0.2% | 0.360 | -0.8% | 0.410
    | -0.5% | 0.404 | -5.2% |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.385 | -11.6% | 0.415 | 0.0% | 0.412 | -0.2% | 0.360 | -0.8% | 0.410
    | -0.5% | 0.404 | -5.2% |'
- en: '| 5 | 0.374 | -10.3% | 0.422 | -0.7% | 0.419 | -1.4% | 0.365 | -2.1% | 0.415
    | -0.7% | 0.397 | -4.5% |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.374 | -10.3% | 0.422 | -0.7% | 0.419 | -1.4% | 0.365 | -2.1% | 0.415
    | -0.7% | 0.397 | -4.5% |'
- en: '| max_4K | 0.337 | -16.8% | 0.419 | -1.1% | 0.402 | -0.5% | 0.357 | -7.7% |
    0.410 | -1.1% | 0.376 | -2.6% |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| max_4K | 0.337 | -16.8% | 0.419 | -1.1% | 0.402 | -0.5% | 0.357 | -7.7% |
    0.410 | -1.1% | 0.376 | -2.6% |'
- en: '| max_8K | - | - | 0.395 | -1.0% | 0.379 | -1.9% | 0.326 | -18.7% | 0.387 |
    -0.8% | 0.384 | -7.2% |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| max_8K | - | - | 0.395 | -1.0% | 0.379 | -1.9% | 0.326 | -18.7% | 0.387 |
    -0.8% | 0.384 | -7.2% |'
- en: Table 1\. The absolute percentage change between FP16 and INT4 scores, using
    Contriever. More than a 5% drop in performance is highlighted in red. For MAE,
    the lower is better while the inverse is true for Rouge-L.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 使用Contriever的FP16和INT4分数之间的绝对百分比变化。性能下降超过5%的用红色标出。对于MAE，数值越低越好，而对于Rouge-L则相反。
- en: \Description
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Table showing the results of the experiments for Contriever. We list the difference
    in scores between the FP16 and INT4 variants of the models by how much the performance
    changes in percentages.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表格展示了Contriever实验的结果。我们列出了FP16和INT4模型变体之间分数的差异，表现的变化以百分比表示。
- en: 3\. Results
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 结果
- en: '![Refer to caption](img/6e6f52ab785e96d310d80d29636b0c9e.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6e6f52ab785e96d310d80d29636b0c9e.png)'
- en: (a) MAE results for LaMP-3U, the lower the better
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LaMP-3U的MAE结果，数值越低越好
- en: '![Refer to caption](img/95066074ec1f6856f62c5a8858778606.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/95066074ec1f6856f62c5a8858778606.png)'
- en: (b) Rouge-L results for LaMP-5U, the higher the better
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LaMP-5U的Rouge-L结果，数值越高越好
- en: Figure 2\. Results for both datasets. The upper and lower borders of each colored
    area represent the quantized and not-quantized performances of the models, and
    the corresponding lines are the mean of both.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图2。两个数据集的结果。每个彩色区域的上边界和下边界分别表示模型的量化和非量化性能，相应的线条表示两者的均值。
- en: \Description
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Results for both datasets using 3 different retrievers. The left plot shows
    the MAE for the third dataset and the right plot shows the Rouge-L score. The
    x-axis is composed of the values of the number of retrieved documents. It shows
    the same results as Table 1.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用3种不同检索器的两个数据集的结果。左图显示了第三个数据集的MAE，右图显示了Rouge-L分数。x轴由检索文档的数量值组成。结果与表1相同。
- en: 'LLMs. The results are shown in Table [1](#S2.T1 "Table 1 ‣ 2.3\. Retrieval
    ‣ 2\. Approach ‣ The Impact of Quantization on Retrieval-Augmented Generation:
    An Analysis of Small LLMs"). We see the dominance of OpenChat in both datasets.
    Zephyr performs very close to OpenChat in LaMP-3U but falls far behind in LaMP-5U.
    The same can be said for Starling but reversed. Mistral-7B performs stable in
    both datasets albeit being slightly behind OpenChat. Overall, LLaMA2 performs
    the worst as it is below average in both datasets. Despite being the dominant
    small LLM currently, LLaMA3 is not the best for both tasks, despite performing
    reasonably well in LaMP-3U. Interestingly, LLaMA3 has the best zero-shot score
    in LaMP-5U but it struggles to improve itself with retrieval.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LLM。结果显示在表[1](#S2.T1 "表1 ‣ 2.3\. 检索 ‣ 2\. 方法 ‣ 量化对检索增强生成的影响：小型LLM的分析")中。我们看到OpenChat在两个数据集中的优势。Zephyr在LaMP-3U中的表现与OpenChat非常接近，但在LaMP-5U中远远落后。同样，Starling也是如此，但情况正好相反。Mistral-7B在两个数据集中表现稳定，尽管稍微落后于OpenChat。总体而言，LLaMA2的表现最差，在两个数据集中都低于平均水平。尽管LLaMA3目前是主导的小型LLM，但它在两个任务中都不是最佳选择，尽管在LaMP-3U中表现尚可。有趣的是，LLaMA3在LaMP-5U中的零-shot分数最佳，但在利用检索提高自身性能方面遇到了困难。
- en: Quantization. How much an LLM gets affected by quantization seems to be related
    to how well it performs the task. OpenChat suffers almost no performance degradation
    from quantization. On the contrary, LLaMA2 seems very sensitive, especially when
    the number of retrieved documents is increased. Starling suffers no significant
    consequence from quantization in LaMP-5U where it performs well, but it does suffer
    in LaMP-3U. There also seems to be a disparity between the tasks as quantized
    LLMs perform much worse in LaMP-3U than in LaMP-5U.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 量化。一个LLM受量化影响的程度似乎与其任务表现的好坏有关。OpenChat几乎没有因量化而导致性能下降。相反，LLaMA2似乎非常敏感，特别是当检索文档的数量增加时。Starling在LaMP-5U中没有因量化而受到显著影响，其表现良好，但在LaMP-3U中却受到了影响。量化的LLM在LaMP-3U中的表现明显逊色于在LaMP-5U中的表现。
- en: 'Number of retrieved documents. Figure [2b](#S3.F2.sf2 "In Figure 2 ‣ 3\. Results
    ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of
    Small LLMs") shows that LLM performance is saturated with a couple of documents,
    and the improvement obtained from more is marginal. In LaMP-5U, adding more than
    5 documents starts to hurt the performance: Figure [2b](#S3.F2.sf2 "In Figure
    2 ‣ 3\. Results ‣ The Impact of Quantization on Retrieval-Augmented Generation:
    An Analysis of Small LLMs") shows an inverse-U-shaped distribution for all LLMs
    except LLaMA3\. For some models, performance even drops below the zero-shot setting
    when all the available context window is filled with retrieved documents. The
    LaMP-3U results (Figure [2a](#S3.F2.sf1 "In Figure 2 ‣ 3\. Results ‣ The Impact
    of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs"))
    continue to improve after adding more than 5 documents as can be observed from
    $max\_4K$.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 检索文档的数量。图[2b](#S3.F2.sf2 "在图2 ‣ 3\. 结果 ‣ 量化对检索增强生成的影响：小型LLM的分析")显示，LLM的性能在文档数量达到一定程度后趋于饱和，增加更多的改善效果有限。在LaMP-5U中，添加超过5个文档开始损害性能：图[2b](#S3.F2.sf2
    "在图2 ‣ 3\. 结果 ‣ 量化对检索增强生成的影响：小型LLM的分析")显示了所有LLM（LLaMA3除外）的倒U型分布。对于一些模型，当所有可用的上下文窗口都被检索文档填满时，性能甚至会低于零-shot设置。LaMP-3U的结果（图[2a](#S3.F2.sf1
    "在图2 ‣ 3\. 结果 ‣ 量化对检索增强生成的影响：小型LLM的分析")）在添加超过5个文档后继续改善，如$max\_4K$所示。
- en: We analyze whether a longer context window hurts the quantized variants more
    and find that there seems to be a peculiar relationship. INT4 LLaMA2 suffers from
    longer contexts, while INT4 OpenChat performs well and acts almost the same as
    its FP16 counterpart. INT4 Mistral and LLaMA3 act very similar to their FP16 counterparts
    in LaMP-5U but in LaMP-3U, they get progressively worse with more documents. Overall,
    quantization can increase the risk of worsened long-context capabilities but there
    is not a direct relationship as it is highly task and context-dependent.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了较长的上下文窗口是否会对量化变体造成更大的影响，发现似乎存在一种特殊的关系。INT4 LLaMA2 在较长上下文中表现较差，而 INT4 OpenChat
    表现良好，几乎与其 FP16 对应模型相同。INT4 Mistral 和 LLaMA3 在 LaMP-5U 中与其 FP16 对应模型非常相似，但在 LaMP-3U
    中，它们在文档数量增加时表现逐渐变差。总体而言，量化可能会增加长期上下文能力恶化的风险，但这种关系并不直接，因为它高度依赖于任务和上下文。
- en: 'Retrievers. Figure [2b](#S3.F2.sf2 "In Figure 2 ‣ 3\. Results ‣ The Impact
    of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs")
    shows that the three retrievers gave almost identical results, albeit BM25 being
    marginally behind the others. Also in LaMP-5U, the gap between the FP16 and INT4
    LLaMA2 varies slightly. Other than that, the retriever model does not have a noticeable
    impact on the personalization tasks we experimented with. The patterns we found
    regarding LLMs, quantization, and the number of retrieved documents are the same
    for all the retrievers.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器。图 [2b](#S3.F2.sf2 "在图 2 ‣ 3\. 结果 ‣ 量化对检索增强生成的影响：对小型 LLM 的分析") 显示这三种检索器给出了几乎相同的结果，尽管
    BM25 稍微落后于其他模型。在 LaMP-5U 中，FP16 和 INT4 LLaMA2 之间的差距略有不同。除此之外，检索器模型对我们实验中的个性化任务没有显著影响。我们发现关于
    LLM、量化和检索文档数量的模式对于所有检索器都是相同的。
- en: '|  | LaMP-3U (MAE) $\downarrow$ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | LaMP-3U (MAE) $\downarrow$ |'
- en: '| ChatGPT (Salemi et al., [2023](#bib.bib19)) | 0.658 | 0.336 | ? |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT (Salemi et al., [2023](#bib.bib19)) | 0.658 | 0.336 | ? |'
- en: '| FlanT5-XXL (Salemi et al., [2023](#bib.bib19)) | 0.282 | 0.424 | 43 GB |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XXL (Salemi et al., [2023](#bib.bib19)) | 0.282 | 0.424 | 43 GB |'
- en: '| OpenChat (FP16) | 0.238 | 0.423* | 28 GB |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| OpenChat (FP16) | 0.238 | 0.423* | 28 GB |'
- en: '| OpenChat (INT4) | 0.234 | 0.419* | 4.2 GB |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| OpenChat (INT4) | 0.234 | 0.419* | 4.2 GB |'
- en: Table 2\. Our results compared with LaMP. Results indicated with * are not significantly
    lower than the reported best result (FlanT5-XXL). A quantized 7B LLM can perform
    on par with a larger model while being much more efficient.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 我们的结果与 LaMP 的比较。标有 * 的结果与报告的最佳结果 (FlanT5-XXL) 并无显著差异。一个量化的 7B LLM 可以在效率上与更大模型相媲美。
- en: \Description
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: Table showing the Rouge-L scores obtained from the experiments
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示了实验中获得的 Rouge-L 分数
- en: 'Benchmark comparison. Finally, we compared our findings with the RAG results
    from LaMP (Salemi et al., [2023](#bib.bib19)). Table [2](#S3.T2 "Table 2 ‣ 3\.
    Results ‣ The Impact of Quantization on Retrieval-Augmented Generation: An Analysis
    of Small LLMs") shows that OpenChat (Contriever, $k=5$ value of 0.29 shows a non-significant
    difference between the results. Moreover, the results from the LaMP paper are
    with finetuned retrievers while our results are with non-finetuned retrievers.
    This indicates that a quantized-7B LLM can compete and even outperform a bigger
    model on personalization with RAG.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 基准比较。最后，我们将我们的发现与 LaMP (Salemi et al., [2023](#bib.bib19)) 的 RAG 结果进行了比较。表 [2](#S3.T2
    "表 2 ‣ 3\. 结果 ‣ 量化对检索增强生成的影响：对小型 LLM 的分析") 显示 OpenChat (Contriever，$k=5$ 值为 0.29
    表示结果之间差异不显著。此外，LaMP 论文中的结果是经过微调的检索器，而我们的结果是未经微调的检索器。这表明，量化的 7B LLM 可以在个性化 RAG
    中与更大模型竞争，甚至超越它们。
- en: 'Table [2](#S3.T2 "Table 2 ‣ 3\. Results ‣ The Impact of Quantization on Retrieval-Augmented
    Generation: An Analysis of Small LLMs") shows how much GPU VRAM is needed to deploy
    each model. With this comparison, the benefit of quantization becomes more pronounced:
    multiple high-level consumer GPUs or an A100 is necessary for running even a 7B
    LLM while a mid-level consumer GPU (eg. RTX 3060) would be enough to run it with
    quantization. According to the scores taken from LaMP, both FlanT5-XXL and OpenChat
    decisively beat ChatGPT, but the authors warn that the prompts used for ChatGPT
    may not be ideal and may contribute to a sub-optimal performance. Therefore, our
    results should not be used to make a comparison with ChatGPT.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S3.T2 "表格 2 ‣ 3. 结果 ‣ 量化对检索增强生成的影响：小型 LLM 的分析") 显示了部署每个模型所需的 GPU VRAM
    量。通过这种比较，量化的好处变得更加明显：即使是 7B LLM 也需要多个高端消费级 GPU 或一个 A100，而中端消费级 GPU（例如 RTX 3060）则足以运行量化后的模型。根据
    LaMP 的评分，FlanT5-XXL 和 OpenChat 都明显超越了 ChatGPT，但作者警告说，针对 ChatGPT 使用的提示可能并不理想，可能导致次优的表现。因此，我们的结果不应与
    ChatGPT 进行比较。
- en: 4\. Discussion
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 讨论
- en: Our results show that some LLMs (in particular OpenChat) can be successful in
    RAG pipelines, even after quantization, but the performance is LLM- and task-dependent.
    The method of quantization affects LLMs differently (Li et al., [2024](#bib.bib9)).
    Thus, the relationship between quantization and RAG performance is not straightforward
    and can be studied more extensively. Still, our results indicate that when a small
    LLM performs the task well, its AWQ-quantized counterpart performs on par.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果显示，一些 LLM（特别是 OpenChat）在 RAG 流水线中，即使在量化之后也能取得成功，但表现依赖于 LLM 和任务。量化的方法对 LLM
    的影响各异（Li et al., [2024](#bib.bib9)）。因此，量化与 RAG 性能之间的关系并不简单，还需要进一步研究。然而，我们的结果表明，当一个小型
    LLM 能够很好地完成任务时，其 AWQ-量化版本也表现相当。
- en: The differing performance of some LLMs between the datasets may be partly due
    to prompting. LLMs are sensitive to prompts, and a prompt that works for one LLM
    may not work for another one (Sclar et al., [2023](#bib.bib20)). The most peculiar
    result is the lackluster performance of LLaMA3 in LaMP-5U. LLaMA3 is a recently
    released model trained with an extensive pretraining corpus (Meta, [2024](#bib.bib14)).
    It has a higher chance of seeing the abstracts presented in the LaMP-5U in its
    pretraining data. This may explain its superior zero-shot performance. LLMs suffer
    from a knowledge conflict between their parametric information and the contextual
    information presented through retrieval (Xu et al., [2024](#bib.bib26)). If LLaMA3
    had already memorized some of the titles of the abstracts in LaMP-5U, it might
    result in a knowledge conflict when similar abstract-title pairs of the same author
    are presented. This may explain the reduced improvement in its performance with
    retrieval.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一些大型语言模型（LLMs）在不同数据集上的表现差异可能部分归因于提示。LLMs 对提示非常敏感，对于一个 LLM 有效的提示可能对另一个 LLM 无效（Sclar
    et al., [2023](#bib.bib20)）。最奇特的结果是 LLaMA3 在 LaMP-5U 中表现平平。LLaMA3 是一个最近发布的模型，经过了大量的预训练语料库训练（Meta,
    [2024](#bib.bib14)）。它有更高的机会在其预训练数据中看到 LaMP-5U 中的摘要。这可能解释了它在零-shot 情况下的优越表现。LLMs
    遇到的知识冲突源于它们的参数信息与通过检索提供的上下文信息之间的矛盾（Xu et al., [2024](#bib.bib26)）。如果 LLaMA3 已经记住了
    LaMP-5U 中一些摘要的标题，当类似的同一作者的摘要-标题对被呈现时，可能会导致知识冲突。这可能解释了其在检索情况下性能提升的减少。
- en: LLMs have been shown to struggle with too many retrieved documents (Liu et al.,
    [2023](#bib.bib12)), and our findings are in accordance. Our results indicate
    that more than  settings,
    the less relevant documents are put on the top. This situation might hurt the
    LLM performance as it would focus on the most and the least related information
    in this case. That being said, state-of-the-art LLMs with more than 7B parameters
    also suffer from the same phenomenon even when not quantized (Liu et al., [2023](#bib.bib12)).
    Although quantization increases the risk of worsened long-context performance,
    we cannot conclude that it is the sole perpetrator, as this is an inherent problem
    for all LLMs.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 已经有研究表明 LLMs 在处理过多检索文档时会遇到困难（Liu et al., [2023](#bib.bib12)），我们的发现也是如此。我们的结果表明，设置数量越多，相关性较低的文档会排在前面。这种情况可能会损害
    LLM 的性能，因为它会集中在最相关和最不相关的信息上。尽管如此，即使在没有量化的情况下，具有超过 7B 参数的最先进的 LLM 也会遭遇相同的现象（Liu
    et al., [2023](#bib.bib12)）。虽然量化增加了长期上下文性能变差的风险，但我们不能断言它是唯一的罪魁祸首，因为这是所有 LLM 的固有问题。
- en: 5\. Conclusion
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5. 结论
- en: We have shown that quantized smaller LLMs can use RAG to perform complex tasks
    such as personalization. Even though quantization might decrease the ability of
    LLMs to analyze long contexts, it is task- and LLM-dependent. An LLM that performs
    well on a task does not lose much of its long-context abilities when quantized.
    Thus, we conclude that quantized 7B LLMs can be the backbones of RAG with long
    contexts. The reduced computational load obtained from quantization would make
    it possible to run RAG applications with more affordable and accessible hardware.
    For future work, more quantization methods can be included in the experiments
    to see if the findings can be replicated across different methods. We can also
    extend the number set of k, especially between $k=5$, and change the order of
    the documents to better understand how quantized LLMs use their context windows.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经展示了量化的小型LLMs可以使用RAG执行复杂任务，例如个性化。尽管量化可能会减少LLMs分析长上下文的能力，但这取决于任务和LLM。一个在任务上表现良好的LLM在量化时不会失去太多的长上下文能力。因此，我们得出结论，量化的7B
    LLMs可以成为具有长上下文的RAG的核心。量化所获得的计算负荷减少将使得使用更实惠和可及的硬件运行RAG应用成为可能。未来的工作中，可以在实验中包括更多的量化方法，以查看这些发现是否可以在不同的方法中复制。我们还可以扩展k的数集，特别是在$k=5$之间，并改变文档的顺序，以更好地理解量化的LLMs如何使用它们的上下文窗口。
- en: References
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG]'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314
    [cs.LG]'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained
    Transformers. arXiv:2210.17323 [cs.LG]'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained
    Transformers. arXiv:2210.17323 [cs.LG]'
- en: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
    Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. (2024) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan,
    Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024. Retrieval-Augmented
    Generation for Large Language Models: A Survey. arXiv:2312.10997 [cs.CL]'
- en: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. 2023. RAVEN: In-Context Learning with Retrieval Augmented
    Encoder-Decoder Language Models. arXiv:2308.07922 [cs.CL]'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang et al. (2023) Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
    Chang, and Bryan Catanzaro. 2023. RAVEN: In-Context Learning with Retrieval Augmented
    Encoder-Decoder Language Models. arXiv:2308.07922 [cs.CL]'
- en: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised
    Dense Information Retrieval with Contrastive Learning. arXiv:2112.09118 [cs.IR]
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard et al. (2022) Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
    Riedel, Piotr Bojanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised
    Dense Information Retrieval with Contrastive Learning. arXiv:2112.09118 [cs.IR]
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage
    Retrieval for Open-Domain Question Answering. arXiv:2004.04906 [cs.CL]
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen tau Yih. 2020. Dense Passage
    Retrieval for Open-Domain Question Answering. arXiv:2004.04906 [cs.CL]
- en: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating Quantized
    Large Language Models. arXiv:2402.18158 [cs.CL]
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng
    Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. 2024. Evaluating Quantized
    Large Language Models. arXiv:2402.18158 [cs.CL]
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. Association for Computational
    Linguistics, Barcelona, Spain, 74–81. [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) Chin-Yew Lin. 2004. ROUGE：一个用于自动评价摘要的包。在 *文本摘要的分支*。计算语言学协会，西班牙巴塞罗那，74–81。
    [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    Chuang Gan, and Song Han. 2023. AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration. arXiv:2306.00978 [cs.CL]'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    Chuang Gan, 和 Song Han. 2023. AWQ：用于LLM压缩和加速的激活感知权重量化。arXiv:2306.00978 [cs.CL]
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023. Lost in the Middle:
    How Language Models Use Long Contexts. arXiv:2307.03172 [cs.CL]'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 2023. 迷失在中间：语言模型如何使用长上下文。arXiv:2307.03172
    [cs.CL]
- en: Ma et al. (2023) Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan.
    2023. Query Rewriting for Retrieval-Augmented Large Language Models. arXiv:2305.14283 [cs.CL]
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2023) Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, 和 Nan Duan.
    2023. 针对检索增强的大型语言模型的查询重写。arXiv:2305.14283 [cs.CL]
- en: 'Meta (2024) Meta. 2024. Introducing Meta Llama 3: The most capable openly available
    LLM to date. Retrieved March 18, 2024 from [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (2024) Meta. 2024. 介绍 Meta Llama 3：迄今为止最强大的公开可用LLM。于 2024年3月18日从 [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)
    获取
- en: 'Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022. WebGPT: Browser-assisted
    question-answering with human feedback. arXiv:2112.09332 [cs.CL]'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano et al. (2022) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, 和 John Schulman. 2022. WebGPT：借助人工反馈的浏览器辅助问答。arXiv:2112.09332
    [cs.CL]
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774 [cs.CL]
- en: 'Proser ([n. d.]) Zachary Proser. [n. d.]. Retrieval Augmented Generation (RAG):
    Reducing Hallucinations in GenAI Applications. [https://www.pinecone.io/learn/retrieval-augmented-generation/](https://www.pinecone.io/learn/retrieval-augmented-generation/)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Proser ([n. d.]) Zachary Proser. [n. d.]. 检索增强生成（RAG）：减少GenAI应用中的幻觉。 [https://www.pinecone.io/learn/retrieval-augmented-generation/](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- en: Robertson et al. (1994) Stephen Robertson, Steve Walker, Susan Jones, Micheline
    Hancock-Beaulieu, and Mike Gatford. 1994. Okapi at TREC-3\. 0–.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson et al. (1994) Stephen Robertson, Steve Walker, Susan Jones, Micheline
    Hancock-Beaulieu, 和 Mike Gatford. 1994. Okapi 在 TREC-3。0–.
- en: 'Salemi et al. (2023) Alireza Salemi, Sheshera Mysore, Michael Bendersky, and
    Hamed Zamani. 2023. LaMP: When Large Language Models Meet Personalization. arXiv:2304.11406 [cs.CL]'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Salemi et al. (2023) Alireza Salemi, Sheshera Mysore, Michael Bendersky, 和 Hamed
    Zamani. 2023. LaMP：当大型语言模型遇上个性化。arXiv:2304.11406 [cs.CL]
- en: 'Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
    2023. Quantifying Language Models’ Sensitivity to Spurious Features in Prompt
    Design or: How I learned to start worrying about prompt formatting. arXiv:2310.11324 [cs.CL]'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, 和 Alane Suhr.
    2023. 量化语言模型对提示设计中虚假特征的敏感性或：我如何开始担心提示格式。arXiv:2310.11324 [cs.CL]
- en: 'Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. 2023. REPLUG: Retrieval-Augmented
    Black-Box Language Models. arXiv:2301.12652 [cs.CL]'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich
    James, Mike Lewis, Luke Zettlemoyer, 和 Wen tau Yih. 2023. REPLUG：检索增强的黑箱语言模型。arXiv:2301.12652
    [cs.CL]
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023） Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom. 2023. Llama
    2：开放基础和微调聊天模型。arXiv:2307.09288 [cs.CL]
- en: 'Tunstall et al. (2023) Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen
    Rajani, Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine
    Fourrier, Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and
    Thomas Wolf. 2023. Zephyr: Direct Distillation of LM Alignment. arXiv:2310.16944 [cs.LG]'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tunstall 等人（2023） Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani,
    Kashif Rasul, Younes Belkada, Shengyi Huang, Leandro von Werra, Clémentine Fourrier,
    Nathan Habib, Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush 和 Thomas Wolf.
    2023. Zephyr：大型模型对齐的直接蒸馏。arXiv:2310.16944 [cs.LG]
- en: 'Wang et al. (2023) Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen
    Song, and Yang Liu. 2023. OpenChat: Advancing Open-source Language Models with
    Mixed-Quality Data. arXiv:2309.11235 [cs.CL]'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023） Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song 和
    Yang Liu. 2023. OpenChat：用混合质量数据推动开源语言模型的发展。arXiv:2309.11235 [cs.CL]
- en: Xu et al. (2023) Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu,
    Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan
    Catanzaro. 2023. Retrieval meets Long Context Large Language Models. arXiv:2310.03025 [cs.CL]
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2023） Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan
    Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi 和 Bryan Catanzaro.
    2023. 检索与长上下文大型语言模型的结合。arXiv:2310.03025 [cs.CL]
- en: 'Xu et al. (2024) Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang,
    and Wei Xu. 2024. Knowledge Conflicts for LLMs: A Survey. arXiv:2403.08319 [cs.CL]'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2024） Rongwu Xu, Zehan Qi, Cunxiang Wang, Hongru Wang, Yue Zhang 和 Wei
    Xu. 2024. 知识冲突在大型语言模型中的调查。arXiv:2403.08319 [cs.CL]
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-judge with MT-Bench
    and Chatbot Arena. arXiv:2306.05685 [cs.CL]
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2023） Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph
    E. Gonzalez 和 Ion Stoica. 2023. 使用 MT-Bench 和 Chatbot Arena 评估大型语言模型作为裁判的能力。arXiv:2306.05685
    [cs.CL]
- en: 'Zhu et al. (2023) Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, and Jiantao
    Jiao. 2023. Starling-7B: Improving LLM Helpfulness & Harmlessness with RLAIF.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023） Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu 和 Jiantao Jiao.
    2023. Starling-7B：通过 RLAIF 提高大型语言模型的有用性和无害性。
