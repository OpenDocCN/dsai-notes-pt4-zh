- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:06:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:06:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将隐式多模态知识提炼到LLMs中以实现零资源对话生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10121](https://ar5iv.labs.arxiv.org/html/2405.10121)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10121](https://ar5iv.labs.arxiv.org/html/2405.10121)
- en: 'Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, and Hongfei Lin Bo Zhang, Hui
    Ma, Jian Ding, Jian Wang, Bo Xu and Hongfei Lin are with the School of Computer
    Science and Technology, Dalian University of Technology, Dalian 116024, China.
    (e-mail: zhangbo1998@mail.dlut.edu.cn; huima_cumt@163.com; 91mr_ding@mail.dlut.edu.cn;
    wangjian@dlut.edu.cn; xubo@dlut.edu.cn; hflin@dlut.edu.cn).Corresponding author:
    Jian Wang.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, and Hongfei Lin Bo Zhang, Hui
    Ma, Jian Ding, Jian Wang, Bo Xu and Hongfei Lin are with the School of Computer
    Science and Technology, Dalian University of Technology, Dalian 116024, China.
    (e-mail: zhangbo1998@mail.dlut.edu.cn; huima_cumt@163.com; 91mr_ding@mail.dlut.edu.cn;
    wangjian@dlut.edu.cn; xubo@dlut.edu.cn; hflin@dlut.edu.cn).Corresponding author:
    Jian Wang.'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Integrating multimodal knowledge into large language models (LLMs) represents
    a significant advancement in dialogue generation capabilities. However, the effective
    incorporation of such knowledge in zero-resource scenarios remains a substantial
    challenge due to the scarcity of diverse, high-quality dialogue datasets. To address
    this, we propose the Visual Implicit Knowledge Distillation Framework (VIKDF),
    an innovative approach aimed at enhancing LLMs for enriched dialogue generation
    in zero-resource contexts by leveraging implicit multimodal knowledge. VIKDF comprises
    two main stages: knowledge distillation, using an Implicit Query Transformer to
    extract and encode visual implicit knowledge from image-text pairs into knowledge
    vectors; and knowledge integration, employing a novel Bidirectional Variational
    Information Fusion technique to seamlessly integrate these distilled vectors into
    LLMs. This enables the LLMs to generate dialogues that are not only coherent and
    engaging but also exhibit a deep understanding of the context through implicit
    multimodal cues, effectively overcoming the limitations of zero-resource scenarios.
    Our extensive experimentation across two dialogue datasets shows that VIKDF outperforms
    existing state-of-the-art models in generating high-quality dialogues. The code
    will be publicly available following acceptance.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 将多模态知识整合到大型语言模型（LLMs）中，代表了对话生成能力的重大进展。然而，由于缺乏多样化的高质量对话数据集，在零资源场景下有效整合这些知识仍然是一个重大挑战。为解决这一问题，我们提出了视觉隐式知识提炼框架（VIKDF），这是一种创新的方法，旨在通过利用隐式多模态知识提升LLMs在零资源背景下的对话生成能力。VIKDF包含两个主要阶段：知识提炼，使用隐式查询变换器从图像-文本对中提取并编码视觉隐式知识到知识向量中；以及知识整合，采用新颖的双向变分信息融合技术将这些提炼后的向量无缝整合到LLMs中。这使得LLMs能够生成不仅连贯且吸引人的对话，还通过隐式多模态线索展现对上下文的深刻理解，有效克服零资源场景的局限性。我们在两个对话数据集上的广泛实验表明，VIKDF在生成高质量对话方面优于现有的最先进模型。代码将在接受后公开。
- en: 'Index Terms:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Large language models, Multimodal knowledge, Zero resource, Dialogue generation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、多模态知识、零资源、对话生成。
- en: I Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: 'Dialogue generation, a pivotal component of natural language processing, aims
    to create responses that are both natural and engaging within specific dialogue
    contexts. The emergence of large language models (LLMs), such as the Generative
    Pre-trained Transformer (GPT) series [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)],
    has marked significant advancements in this domain. These models excel in identifying
    complex linguistic patterns and semantic details due to their training on extensive
    textual datasets. However, their effectiveness is limited to text-based contexts,
    overlooking the rich, multimodal aspects of human dialogue that incorporate visual,
    auditory, and other sensory inputs. This limitation highlights a crucial challenge:
    enabling LLMs to navigate the multimodal nature of human interactions, a capability
    that humans possess inherently.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对话生成，作为自然语言处理的关键组成部分，旨在在特定对话上下文中创建自然且引人入胜的回应。大型语言模型（LLMs）的出现，如生成式预训练变换器（GPT）系列[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]，标志着这一领域的重要进展。这些模型在识别复杂的语言模式和语义细节方面表现出色，因为它们在广泛的文本数据集上进行了训练。然而，它们的有效性仅限于基于文本的上下文，忽略了人类对话中包含的视觉、听觉和其他感官输入的丰富多模态方面。这一限制突显了一个关键挑战：使LLMs能够驾驭人类交互的多模态特性，这是人类天生具备的能力。
- en: The integration of multimodal knowledge into dialogue systems signifies a major
    progression towards more nuanced and human-like communication capabilities. It
    enables these systems to understand and interpret the nuances of human communication
    that transcend beyond mere text, capturing the essence of multimodal interactions
    [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]. Building upon this concept,
    there has been an increase in research focused on augmenting LLMs with multimodal
    knowledge [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)].
    This involves processing and understanding information across different modalities,
    such as images, videos, and audio, thereby equipping LLMs to perform tasks that
    necessitate cross-modal comprehension. While these developments significantly
    expand the capabilities of LLMs in engaging with multimodal content, challenges
    persist in effectively applying multimodal knowledge in dialogue generation, necessitating
    further exploration and innovation in this area.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将多模态知识融入对话系统标志着向更细致且人性化的交流能力迈出了重要一步。它使这些系统能够理解和解读超越单纯文本的人类沟通的细微差别，捕捉多模态互动的本质[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]。基于这一概念，越来越多的研究专注于通过多模态知识增强LLMs[[7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10)]。这涉及处理和理解跨不同模态的信息，例如图像、视频和音频，从而使LLMs能够执行需要跨模态理解的任务。尽管这些发展显著扩展了LLMs与多模态内容互动的能力，但在对话生成中有效应用多模态知识仍面临挑战，这需要进一步探索和创新。
- en: One pivotal challenge in augmenting LLMs with multimodal capabilities, crucial
    for advancing human-like dialogue generation, is the scarcity of high-quality,
    diverse multimodal dialogue datasets. This is particularly notable in domains
    that demand intricate interactions, such as image-grounded dialogues [[11](#bib.bib11)].
    Image-grounded dialogues involve conversations anchored on a shared image, necessitating
    visual reasoning and a wealth of common sense to elicit coherent and engaging
    responses. Current datasets [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)],
    while foundational, often fall short in capturing the breadth and depth of human
    multimodal communication, resulting in models that may not effectively generalize
    to varied real-world interactions. Moreover, existing frameworks like ZRIGF [[14](#bib.bib14)],
    which represent pioneering efforts in zero-resource image-grounded dialogue generation,
    do not seamlessly integrate into LLM architectures and depend on retrieving relevant
    images during inference to formulate responses. Thus, the challenge of enabling
    LLMs to generate multimodal dialogues in zero-resource scenarios, devoid of annotated
    multimodal dialogue datasets, remains unresolved.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在增强LLMs的多模态能力中，一个关键挑战是缺乏高质量、多样化的多模态对话数据集。这在需要复杂互动的领域尤为突出，如图像驱动的对话[[11](#bib.bib11)]。图像驱动的对话涉及基于共享图像的对话，需要视觉推理和丰富的常识，以引出连贯且吸引人的回应。当前的数据集[[11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)]虽然是基础性的，但常常未能全面捕捉人类多模态交流的广度和深度，导致模型可能无法有效地推广到各种现实世界的互动中。此外，现有的框架如ZRIGF[[14](#bib.bib14)]，作为零资源图像驱动对话生成的开创性努力，并未无缝集成到LLM架构中，并且依赖于在推理过程中检索相关图像来形成回应。因此，使LLMs在没有注释的多模态对话数据集的零资源场景中生成多模态对话的挑战仍未解决。
- en: To address the primary challenge of enabling LLMs to generate dialogues in zero-resource
    scenarios, we introduce the concept of implicit multimodal knowledge. Unlike existing
    approaches, which predominantly utilize explicit multimodal inputs such as images
    or sounds presented during interactions, we center on implicit multimodal knowledge.
    This form of knowledge, significantly distinct from the explicit forms commonly
    integrated in current models, refers to a mental imagery or conceptual understanding
    individuals have developed through their experiences [[15](#bib.bib15), [16](#bib.bib16)].
    Implicit multimodal knowledge encompasses a broad spectrum of sensory, emotional,
    and contextual understandings that, although not directly observable, profoundly
    influence the nature of dialogues [[17](#bib.bib17), [18](#bib.bib18)]. By leveraging
    readily available large-scale image-text pair corpora to learn how to utilize
    implicit multimodal knowledge, it is possible to circumvent the issue of scarcity
    in high-quality, diverse multimodal dialogue datasets. However, current multimodal
    LLMs primarily engage with explicit multimodal inputs, focusing on direct responses
    to visual or auditory stimuli within dialogues [[19](#bib.bib19)], resulting in
    a lack of capability to incorporate such implicit knowledge. Therefore, the challenge
    becomes how to effectively distill and integrate implicit multimodal knowledge
    into LLMs, thereby significantly enhancing their ability to generate nuanced dialogues
    in zero-resource scenarios.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决使大型语言模型（LLMs）在零资源场景下生成对话的主要挑战，我们引入了隐式多模态知识的概念。与现有方法不同，这些方法主要利用在互动过程中呈现的显式多模态输入（如图像或声音），我们则将重点放在隐式多模态知识上。这种知识形式与当前模型中常见的显式形式有显著不同，它指的是个体通过经验发展起来的心理意象或概念理解[[15](#bib.bib15),
    [16](#bib.bib16)]。隐式多模态知识涵盖了广泛的感官、情感和语境理解，虽然这些理解不可直接观察，但却深刻地影响了对话的性质[[17](#bib.bib17),
    [18](#bib.bib18)]。通过利用现成的大规模图像-文本配对语料库来学习如何利用隐式多模态知识，可以避免高质量、多样化多模态对话数据集稀缺的问题。然而，目前的多模态LLMs主要处理显式多模态输入，专注于对视觉或听觉刺激的直接响应[[19](#bib.bib19)]，因此缺乏将这种隐式知识整合进来的能力。因此，挑战在于如何有效提炼并将隐式多模态知识融入LLMs，从而显著提升其在零资源场景下生成细腻对话的能力。
- en: '![Refer to caption](img/d9eb89b254bd3bb46173dd7fcfcf52c6.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d9eb89b254bd3bb46173dd7fcfcf52c6.png)'
- en: 'Figure 1: Intuition of our proposed approach.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提出的方法的直观展示。
- en: 'To navigate this refined challenge, we propose the Visual Implicit Knowledge
    Distillation Framework (VIKDF), a novel framework that reimagines the integration
    of multimodal knowledge into LLMs, focusing on the distillation and incorporation
    of visual implicit knowledge in a zero-resource scenario. VIKDF operates in two
    synergistic stages: knowledge distillation and knowledge integration, as illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Distilling Implicit Multimodal
    Knowledge into LLMs for Zero-Resource Dialogue Generation"). In the knowledge
    distillation stage, VIKDF utilizes a multimodal model that incorporates a text
    encoder, an image encoder, and a novel querying transformer termed the Implicit
    Query Transformer (IQ-Former). This transformer, an advancement of the Q-Former
    [[7](#bib.bib7)], is specially tailored for extracting implicit knowledge. It
    employs a set of learnable query vectors to distill visual implicit knowledge
    from extensive image-text pair corpora. These vectors serve as the representation
    of the visual implicit knowledge, which can be then effectively integrated into
    LLMs. During the knowledge integration stage, we introduce a pioneering technique
    to seamlessly integrate the distilled visual implicit knowledge into LLMs, named
    Bidirectional Variational Information Fusion (BVIF). BVIF leverages an instruction-aware
    dual-pathway approach to maximize the mutual information between textual context
    and distilled visual implicit knowledge, thereby capturing the essence of the
    visual implicit knowledge. This simultaneous optimization ensures coherent, context-rich
    dialogues and bridges the gap between explicit and implicit multimodal knowledge
    processing. Consequently, VIKDF enables LLMs to engage in complex dialogues without
    depending on annotated multimodal datasets, marking a significant step forward
    in zero-resource dialogue generation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一精细化挑战，我们提出了视觉隐式知识提炼框架（VIKDF），这是一个全新的框架，重新构想了多模态知识在LLMs中的整合，重点在于零资源场景下视觉隐式知识的提炼和融入。VIKDF
    分为两个协同阶段：知识提炼和知识整合，如图 [1](#S1.F1 "图 1 ‣ 引言 ‣ 在零资源对话生成中提炼隐式多模态知识") 所示。在知识提炼阶段，VIKDF
    利用一个包含文本编码器、图像编码器和一种名为隐式查询转换器（IQ-Former）的新型查询转换器的多模态模型。该转换器是 Q-Former [[7](#bib.bib7)]
    的一种进化，专门用于提取隐式知识。它使用一组可学习的查询向量从大量图像-文本对语料库中提炼视觉隐式知识。这些向量作为视觉隐式知识的表示，然后可以有效地整合到
    LLMs 中。在知识整合阶段，我们引入了一种开创性的技术来无缝地将提炼出的视觉隐式知识整合到 LLMs 中，称为双向变分信息融合（BVIF）。BVIF 利用一种指令感知的双路径方法来最大化文本上下文与提炼的视觉隐式知识之间的互信息，从而捕捉视觉隐式知识的本质。这种同步优化确保了对话的连贯性和上下文丰富性，并弥合了显式和隐式多模态知识处理之间的差距。因此，VIKDF
    使 LLMs 能够在不依赖标注的多模态数据集的情况下进行复杂对话，标志着零资源对话生成领域的重要进展。
- en: To validate our framework’s efficacy, we conducted comprehensive experiments
    on the Image-Chat [[20](#bib.bib20)] and Reddit Conversation datasets [[4](#bib.bib4)],
    benchmarking our method against several state-of-the-art baselines such as ChatGPT
    and ZRIGF. Through both automatic and human evaluations, VIKDF showcased its exceptional
    ability to fluently incorporate visual implicit knowledge into dialogues, thereby
    generating contextually rich, engaging, and coherent conversations, and outperforming
    existing models in zero-resource scenarios.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们框架的有效性，我们在 Image-Chat [[20](#bib.bib20)] 和 Reddit Conversation 数据集 [[4](#bib.bib4)]
    上进行了全面的实验，将我们的方法与多种最先进的基准方法（如 ChatGPT 和 ZRIGF）进行了对比。通过自动评估和人工评估，VIKDF 展现了其流畅地将视觉隐式知识融入对话中的卓越能力，从而生成了上下文丰富、引人入胜且连贯的对话，并在零资源场景下超越了现有模型。
- en: 'Our main contributions are highlighted as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel framework that distills and integrates visual implicit knowledge
    into LLMs, enabling them to generate more engaging dialogues without relying on
    any explicit images in zero-resource scenarios.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个全新的框架，用于提炼和整合视觉隐式知识到 LLMs 中，使其能够在零资源场景下生成更具吸引力的对话，而不依赖于任何显式图像。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We develop the Implicit Query Transformer and Bidirectional Variational Information
    Fusion techniques, effectively distilling and integrating visual implicit knowledge
    into LLMs and enhancing their dialogue generation capabilities.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了隐式查询转换器和双向变分信息融合技术，有效地将视觉隐式知识提炼并整合到大型语言模型（LLMs）中，提升了其对话生成能力。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We conduct extensive evaluations across two datasets in diverse scenarios, demonstrating
    the superior performance and robust generalization capabilities of VIKDF.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在两个数据集和多种场景中进行了广泛的评估，展示了VIKDF的优越性能和强大的泛化能力。
- en: II Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: II-A Multimodal Dialogue Generation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 多模态对话生成
- en: Multimodal dialogue generation aims to produce responses that are natural and
    engaging, considering inputs from multiple modalities such as images, videos,
    or audio. This field requires models that have the ability to understand or generate
    content across these different modalities, leveraging this multimodal knowledge
    to enrich dialogues. Early research in this area [[21](#bib.bib21), [22](#bib.bib22)]
    was primarily focused on multimodal question-answering, where the goal was to
    respond to queries with inputs from various modalities. However, there has been
    a noticeable shift towards generating open-domain dialogues. In these cases, multimodal
    inputs serve to enrich conversations rather than strictly guide them, leading
    to two main streams of research.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态对话生成旨在生成自然且引人入胜的响应，考虑来自多种模态的输入，例如图像、视频或音频。该领域需要能够理解或生成跨这些不同模态内容的模型，利用这些多模态知识来丰富对话。早期的研究[[21](#bib.bib21),
    [22](#bib.bib22)]主要集中在多模态问答上，其目标是用来自各种模态的输入回应查询。然而，研究已显著转向生成开放领域对话。在这些情况下，多模态输入用于丰富对话，而不是严格引导对话，从而形成了两大主要研究方向。
- en: The first stream focuses on dialogue generation based on multimodal information.
    In this approach, models utilize inputs such as images or videos to influence
    the dialogue generation process. These models typically employ multimodal encoders
    or attention mechanisms to integrate multimodal features with textual features,
    thus enhancing the relevance and diversity of the generated responses [[4](#bib.bib4),
    [6](#bib.bib6), [23](#bib.bib23)]. This approach mimics face-to-face interactions
    where non-verbal cues influence but do not solely dictate the conversation. The
    second stream involves models that not only interpret multimodal inputs but also
    generate outputs across multiple modalities. This approach is akin to network-based
    dialogues, where communication often includes and sometimes relies on multimodal
    elements such as emojis, images, or videos. These models require more advanced
    capabilities for handling cross-modal generation and alignment, as well as ensuring
    the coherence and consistency of multimodal outputs [[24](#bib.bib24), [25](#bib.bib25),
    [26](#bib.bib26)].
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方向集中于基于多模态信息的对话生成。在这种方法中，模型利用图像或视频等输入来影响对话生成过程。这些模型通常使用多模态编码器或注意力机制，将多模态特征与文本特征相结合，从而增强生成响应的相关性和多样性[[4](#bib.bib4),
    [6](#bib.bib6), [23](#bib.bib23)]。这种方法模拟了面对面互动，其中非语言线索会影响但不完全决定对话。第二个方向涉及那些不仅解释多模态输入，还生成跨多种模态输出的模型。这种方法类似于基于网络的对话，其中交流通常包括并有时依赖于诸如表情符号、图像或视频等多模态元素。这些模型需要更高级的能力来处理跨模态生成和对齐，并确保多模态输出的一致性和连贯性[[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)]。
- en: Our research aligns with the first stream, aiming to enhance dialogue using
    multimodal inputs without generating multimodal outputs. However, most existing
    methodologies rely on annotated multimodal dialogue data, which is both scarce
    and expensive to obtain. In an effort to bridge this gap, Zhang et al. [[14](#bib.bib14)]
    introduced a zero-resource image-grounded framework that leverages images to enrich
    dialogues through a two-stage learning strategy. While innovative, this method
    requires access to relevant images during inference, which may not always be feasible.
    Our proposed approach differs by utilizing implicit multimodal knowledge, distilled
    from extensive collections of image-text pairs, to enhance dialogue generation.
    This strategy addresses the challenges of data scarcity and modality mismatch,
    enabling the generation of dialogues that are more natural, contextually rich,
    and authentically human-like.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究与第一方向一致，旨在利用多模态输入增强对话，而不生成多模态输出。然而，大多数现有方法依赖于注释过的多模态对话数据，这些数据既稀缺又昂贵。为弥补这一差距，张等人[[14](#bib.bib14)]提出了一种零资源图像基础框架，通过两阶段学习策略利用图像来丰富对话。尽管这一方法具有创新性，但在推理过程中需要访问相关图像，这可能并不总是可行。我们提出的方法通过利用从大量图像-文本对中提炼的隐含多模态知识来增强对话生成。这一策略解决了数据稀缺和模态不匹配的问题，从而生成更自然、语境丰富且更具人类真实感的对话。
- en: II-B Multimodal Knowledge Distillation and Integration
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 多模态知识蒸馏与整合
- en: Multimodal knowledge distillation and integration are crucial for enabling LLMs
    to utilize multimodal information in dialogue generation. Distillation involves
    extracting and compressing information from a broad spectrum of multimodal data,
    such as image-text pairs. Integration, on the other hand, focuses on incorporating
    this distilled information into LLMs, thereby augmenting their capacity for understanding
    and generating multimodal dialogues.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态知识蒸馏和整合对于使 LLMs 能够在对话生成中利用多模态信息至关重要。蒸馏涉及从广泛的多模态数据（如图像-文本对）中提取和压缩信息。而整合则侧重于将这些蒸馏信息融入
    LLMs 中，从而增强其理解和生成多模态对话的能力。
- en: Prior research on this topic has predominantly concentrated on explicit multimodal
    information, such as the direct fusion of image and textual features [[5](#bib.bib5),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)] or employing pre-trained
    multimodal models [[29](#bib.bib29)] to extract multimodal representations [[19](#bib.bib19),
    [30](#bib.bib30)]. However, these methods face limitations when applied to LLMs.
    Li et al. [[7](#bib.bib7)] proposed an innovative solution, the Q-Former, which
    uses learnable query vectors to distill multimodal knowledge and integrate it
    into LLMs. Although this method has shown promising results in improving the multimodal
    capabilities of LLMs, it still encounters challenges in handling implicit multimodal
    knowledge, especially for dialogue generation.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题上的先前研究主要集中在显式的多模态信息上，比如图像和文本特征的直接融合 [[5](#bib.bib5), [26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)] 或者使用预训练的多模态模型 [[29](#bib.bib29)] 来提取多模态表示
    [[19](#bib.bib19), [30](#bib.bib30)]。然而，这些方法在应用于大型语言模型（LLMs）时面临局限性。Li 等人 [[7](#bib.bib7)]
    提出了一个创新的解决方案，Q-Former，它使用可学习的查询向量来提炼多模态知识并将其整合到 LLMs 中。尽管这种方法在提升 LLMs 的多模态能力方面显示出有希望的结果，但在处理隐式多模态知识，特别是对话生成方面仍然面临挑战。
- en: Our work seeks to address a critical gap in multimodal dialogue generation by
    leveraging implicit multimodal knowledge to enhance LLMs’ dialogue generation
    capabilities in zero-resource scenarios. This not only advances the technology
    of multimodal dialogue generation but also provides new insights into the complexities
    of human conversational interactions.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作旨在通过利用隐式多模态知识来提升 LLMs 在零资源场景下的对话生成能力，从而填补多模态对话生成中的关键空白。这不仅推进了多模态对话生成技术，也为人类对话互动的复杂性提供了新的见解。
- en: III Methodology
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法论
- en: III-A Task Formalization and Model Architecture
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 任务形式化和模型架构
- en: 'The task of dialogue generation based on multimodal information is defined
    as generating a response $R$. This leads to the model formulation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于多模态信息的对话生成任务被定义为生成一个响应 $R$。这导致了模型公式的形成：
- en: '|  | $P(R&#124;C)=P(R,K&#124;C)=P(R&#124;C,K)P(K&#124;C)$ |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(R\mid C)=P(R,K\mid C)=P(R\mid C,K)P(K\mid C)$ |  | (1) |'
- en: This formulation is justified as $K$, explaining the rationale behind the first
    part of the equation.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式作为 $K$ 被证明，解释了方程第一部分的理论基础。
- en: 'To achieve this, we introduce a two-stage framework comprising knowledge distillation
    and integration, as shown in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation").
    The framework is anchored by three principal components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们引入了一个包括知识蒸馏和整合的两阶段框架，如图 [1](#S1.F1 "图 1 ‣ I 介绍 ‣ 将隐式多模态知识蒸馏到 LLMs
    中以实现零资源对话生成") 所示。该框架由三个主要组件支撑：
- en: III-A1 Text Encoder and Image Encoder
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 文本编码器和图像编码器
- en: The text encoder and image encoder are responsible for encoding the textual
    and visual information from a large corpus of image-text pairs. We adopt the CLIP
    model [[29](#bib.bib29)] as our text and image encoder, which is a pre-trained
    multimodal model that learns to align image and text features in a unified latent
    space. The text encoder transforms text input $T$. The parameters of both encoders
    are set to remain unchanged.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器和图像编码器负责对大量图像-文本对中的文本和视觉信息进行编码。我们采用 CLIP 模型 [[29](#bib.bib29)] 作为我们的文本和图像编码器，这是一种预训练的多模态模型，它学习将图像和文本特征对齐到统一的潜在空间。文本编码器将文本输入
    $T$ 转换为特征。两个编码器的参数设置为保持不变。
- en: '![Refer to caption](img/26c76345b72be27902c18d17885d19ce.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/26c76345b72be27902c18d17885d19ce.png)'
- en: 'Figure 2: The architecture of IQ-Former.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：IQ-Former 的架构。
- en: III-A2 Implicit Query Transformer
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 隐式查询变换器
- en: The IQ-Former is a specially designed querying transformer that distills visual
    implicit knowledge from encoded image-text pairs. As illustrated in Figure [2](#S3.F2
    "Figure 2 ‣ III-A1 Text Encoder and Image Encoder ‣ III-A Task Formalization and
    Model Architecture ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge
    into LLMs for Zero-Resource Dialogue Generation"), the IQ-Former is structured
    as a transformer encoder equipped with a cross-attention mechanism. Uniquely,
    its inputs are a set of learnable query vectors $\mathbf{Q}={\mathbf{q}_{1},\mathbf{q}_{2},...,\mathbf{q}_{n}}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: IQ-Former是一个特别设计的查询变换器，能够从编码的图像-文本对中提取视觉隐性知识。如图 [2](#S3.F2 "图 2 ‣ III-A1 文本编码器和图像编码器
    ‣ III-A 任务形式化和模型架构 ‣ III 方法论 ‣ 将隐性多模态知识蒸馏到LLMs中用于零资源对话生成")所示，IQ-Former的结构是一个配备有交叉注意力机制的变换器编码器。独特的是，它的输入是一组可学习的查询向量
    $\mathbf{Q}={\mathbf{q}_{1},\mathbf{q}_{2},...,\mathbf{q}_{n}}$。
- en: III-A3 Large Language Model
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 大型语言模型
- en: The LLM is tasked with generating dialogue responses based on the dialogue context
    and the distilled visual implicit knowledge. We employ the Llama-2 model [[33](#bib.bib33)]
    as our LLM, a pre-trained autoregressive language model renowned for its capability
    to produce natural and varied text. The LLM takes the dialogue context $C$ into
    the LLM, we introduce a novel technique termed Bidirectional Variational Information
    Fusion, detailed further in Section [III-C](#S3.SS3 "III-C Knowledge Integration
    Stage ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge into LLMs for
    Zero-Resource Dialogue Generation"). The LLM’s parameters are also frozen during
    training.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 的任务是基于对话上下文和蒸馏的视觉隐性知识生成对话回复。我们使用 Llama-2 模型 [[33](#bib.bib33)] 作为我们的 LLM，这是一种预训练的自回归语言模型，以其产生自然和多样化文本的能力而闻名。LLM
    接收对话上下文 $C$，我们引入了一种新技术，称为双向变分信息融合，详细内容见 [III-C](#S3.SS3 "III-C 知识整合阶段 ‣ III 方法论
    ‣ 将隐性多模态知识蒸馏到LLMs中用于零资源对话生成") 节。LLM 的参数在训练过程中也被冻结。
- en: '![Refer to caption](img/8a2d74b6f201bef89cfe7a38529c152d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a2d74b6f201bef89cfe7a38529c152d.png)'
- en: 'Figure 3: Overview of the knowledge distillation stage. The central part illustrates
    Text-Image Matching. Below the dashed line lies Text-Assisted Masked Image Modeling,
    and above, Image-Assisted Masked Text Modeling.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：知识蒸馏阶段的概述。中央部分展示了文本-图像匹配。虚线以下是文本辅助的掩码图像建模，而虚线以上则是图像辅助的掩码文本建模。
- en: III-B Knowledge Distillation Stage
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 知识蒸馏阶段
- en: The goal of the knowledge distillation stage is to develop a model, denoted
    as $P(K|C)$ by using three objectives.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏阶段的目标是通过使用三个目标来开发一个模型，记作$P(K|C)$。
- en: III-B1 Text-Image Matching
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 文本-图像匹配
- en: 'The text-image matching objective ensures the alignment of knowledge vectors
    $\mathbf{K}_{T}$ for matching pairs and minimizing the cosine similarity for non-matching
    pairs. The loss for text-image matching is formulated as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文本-图像匹配目标确保知识向量 $\mathbf{K}_{T}$ 对于匹配对的对齐，并最小化不匹配对的余弦相似度。文本-图像匹配的损失公式如下：
- en: '|  | $\displaystyle\mathcal{L}_{tim}=-\sum_{i=1}^{N}($ |  | (2) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{tim}=-\sum_{i=1}^{N}($ |  | (2) |'
- en: '|  | $\displaystyle+$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+$ |  |'
- en: where $\tau$ is a learnable temperature parameter that controls the distribution’s
    concentration. By minimizing this loss, the IQ-Former learns to generate knowledge
    vectors that are semantically similar to the corresponding image features, thereby
    encapsulating visual implicit knowledge from the text input.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$ 是一个可学习的温度参数，用于控制分布的集中度。通过最小化这个损失，IQ-Former 学习生成与相应图像特征语义相似的知识向量，从而封装来自文本输入的视觉隐性知识。
- en: III-B2 Text-Assisted Masked Image Modeling
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 文本辅助掩码图像建模
- en: 'The objective of text-assisted masked image modeling is to reconstruct the
    masked portions of an image input, denoted as $\mathbf{I}$ in the masked regions,
    is defined as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文本辅助的掩码图像建模的目标是重建图像输入中掩码部分的图像，记作 $\mathbf{I}$，在掩码区域定义如下：
- en: '|  | $\mathcal{L}_{tamim}=\frac{1}{N_{i}}\sum_{i=1}^{N_{i}}&#124;\mathbf{I}_{i}-\hat{\mathbf{I}}_{i}&#124;$
    |  | (3) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{tamim}=\frac{1}{N_{i}}\sum_{i=1}^{N_{i}}&#124;\mathbf{I}_{i}-\hat{\mathbf{I}}_{i}&#124;$
    |  | (3) |'
- en: where $N_{i}$ being the pixel values of the original and reconstructed images,
    respectively. Minimizing this loss enables the IQ-Former to create knowledge vectors
    that contain sufficient visual implicit knowledge to assist the image reconstruction.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N_{i}$分别表示原始图像和重建图像的像素值。最小化这个损失可以使IQ-Former创建包含足够视觉隐性知识的知识向量，以辅助图像重建。
- en: III-B3 Image-Assisted Masked Text Modeling
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 **图像辅助掩码文本建模**
- en: 'The goal of image-assisted masked text modeling is to recover the masked tokens
    in a text input $T$ is fed into a linear layer followed by a softmax function
    to predict the vocabulary’s probability distribution for each masked token. The
    loss for image-assisted masked text modeling, calculated as the cross-entropy
    loss between the predicted distribution and the true masked tokens, is:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图像辅助的掩码文本建模的目标是恢复输入文本$T$中被掩盖的标记，输入$T$经过线性层处理，然后通过softmax函数预测每个掩盖标记的词汇概率分布。图像辅助的掩码文本建模的损失被计算为预测分布和真实掩盖标记之间的交叉熵损失，其公式为：
- en: '|  | $\mathcal{L}_{iamtm}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}&#124;\mathbf{O}_{T}^{\prime})$
    |  | (4) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{iamtm}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}|\mathbf{O}_{T}^{\prime})$
    |  | (4) |'
- en: where $N_{t}$. By minimizing this loss, the IQ-Former is trained to produce
    output vectors that contain sufficient cross-modal knowledge to assist the text
    reconstruction, thus capturing cross-modal knowledge alignment.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$N_{t}$。通过最小化此损失，IQ-Former被训练生成包含足够跨模态知识的输出向量，以协助文本重构，从而捕捉跨模态知识对齐。
- en: 'The comprehensive loss function for the knowledge distillation stage is the
    sum of the losses from the three objectives, weighted by their respective importance:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏阶段的综合损失函数是三项目标损失的总和，按其各自的重要性加权：
- en: '|  | $\mathcal{L}_{kd}=\lambda_{1}\mathcal{L}_{tim}+\lambda_{2}\mathcal{L}_{tamim}+\lambda_{3}\mathcal{L}_{iamtm}$
    |  | (5) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{kd}=\lambda_{1}\mathcal{L}_{tim}+\lambda_{2}\mathcal{L}_{tamim}+\lambda_{3}\mathcal{L}_{iamtm}$
    |  | (5) |'
- en: where $\lambda_{1}$ for the LLM during the knowledge integration stage.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\lambda_{1}$用于知识整合阶段的LLM。
- en: '![Refer to caption](img/2cbf09e2f7bc4a51d5cfc15d55270dc5.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cbf09e2f7bc4a51d5cfc15d55270dc5.png)'
- en: 'Figure 4: Overview of the knowledge integration stage. Instruction-aware Contextual
    Inference on the left and Instruction-aware Knowledge Reconstruction on the right'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：知识整合阶段概览。左侧为**指令感知的上下文推理**，右侧为**指令感知的知识重构**。
- en: III-C Knowledge Integration Stage
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C **知识整合阶段**
- en: The objective of the knowledge integration stage is to train a model, denoted
    as $P(R|C,K)$. As depicted in Figure [4](#S3.F4 "Figure 4 ‣ III-B3 Image-Assisted
    Masked Text Modeling ‣ III-B Knowledge Distillation Stage ‣ III Methodology ‣
    Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"), the LLM integrates visual implicit knowledge through a pioneering
    technique named Bidirectional Variational Information Fusion. BVIF utilizes an
    instruction-aware dual-pathway approach, with each path providing a distinct yet
    complementary mechanism for knowledge fusion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 知识整合阶段的目标是训练一个模型，记作$P(R|C,K)$。如图[4](#S3.F4 "图4 ‣ III-B3 图像辅助掩码文本建模 ‣ III-B 知识蒸馏阶段
    ‣ III 方法论 ‣ 将隐含多模态知识提炼为LLM以实现零资源对话生成")所示，LLM通过一种名为**双向变分信息融合（BVIF）**的开创性技术整合视觉隐含知识。BVIF利用**指令感知的双路径方法**，每条路径提供不同但互补的知识融合机制。
- en: III-C1 Instruction-aware Contextual Inference
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 **指令感知的上下文推理**
- en: 'The instruction-aware contextual inference pathway aims to enable the LLM to
    decode and integrate the textual intricacies contained within the distilled visual
    implicit knowledge. This pathway introduces distilled visual implicit knowledge
    $K$ by optimizing the likelihood of predicting each subsequent token, based on
    preceding tokens, the query embeddings, and the text prompts. The instruction-aware
    contextual inference loss is formally defined as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**指令感知的上下文推理**路径旨在使LLM解码并整合包含在提炼的视觉隐含知识中的文本复杂性。此路径通过优化基于前面的标记、查询嵌入和文本提示预测每个后续标记的可能性来引入提炼的视觉隐含知识$K$。**指令感知的上下文推理损失**被正式定义为：'
- en: '|  | $\mathcal{L}_{iaci}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}&#124;K,P,T_{<i})$
    |  | (6) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{iaci}=-\frac{1}{N_{t}}\sum_{i=1}^{N_{t}}\log P(T_{i}|K,P,T_{<i})$
    |  | (6) |'
- en: where $N_{t}$. Minimizing this loss instructs the LLM to produce text reflective
    of the visual implicit knowledge distilled by the IQ-Former from the image-text
    pairs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$N_{t}$。最小化此损失会指示LLM生成反映IQ-Former从图像-文本对中提取的视觉隐含知识的文本。
- en: III-C2 Instruction-aware Knowledge Reconstruction
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 **指令感知的知识重构**
- en: The instruction-aware knowledge reconstruction pathway is a crucial component
    of the BVIF technique, aimed at augmenting the LLM’s proficiency in interpreting
    and utilizing distilled visual implicit knowledge $K$, thereby establishing a
    bidirectional information flow between the text and the visual implicit knowledge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 指令感知的知识重构路径是 BVIF 技术的一个关键组成部分，旨在增强 LLM 解读和利用提炼视觉隐性知识 $K$ 的能力，从而在文本与视觉隐性知识之间建立双向信息流。
- en: 'A primary challenge in this task is to ensure that the knowledge $K$ through
    a variational information maximization approach, as detailed in [[35](#bib.bib35)].
    This approach is mathematically represented as:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的主要挑战是确保知识 $K$ 通过变分信息最大化方法，如[[35](#bib.bib35)]中详细描述的那样。这种方法在数学上表示为：
- en: '|  | $I(K,T)\geq\mathbb{E}_{p(K)}\mathbb{E}_{p(T&#124;K)}\log q_{\phi}(K&#124;T)$
    |  | (7) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(K,T)\geq\mathbb{E}_{p(K)}\mathbb{E}_{p(T&#124;K)}\log q_{\phi}(K&#124;T)$
    |  | (7) |'
- en: where $q_{\phi}(K|T)$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $q_{\phi}(K|T)$。
- en: 'In practice, we use the LLM as the function $q_{\phi}$ across all queries:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们将 LLM 用作所有查询的函数 $q_{\phi}$：
- en: '|  | $\mathcal{L}_{iakr}=\frac{1}{N_{q}}\sum_{i=1}^{N_{q}}(\mathbf{K}_{i}-\hat{\mathbf{K}}_{i})^{2}$
    |  | (8) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{iakr}=\frac{1}{N_{q}}\sum_{i=1}^{N_{q}}(\mathbf{K}_{i}-\hat{\mathbf{K}}_{i})^{2}$
    |  | (8) |'
- en: where $N_{q}$ representing the original and reconstructed knowledge vectors,
    respectively. Minimizing this loss enables the LLM to produce knowledge vectors
    consistent with the generated text, thus reinforcing a bidirectional flow of information
    between the text and the visual implicit knowledge.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{q}$ 分别表示原始和重构的知识向量。最小化此损失使得 LLM 能够生成与生成文本一致的知识向量，从而加强文本与视觉隐性知识之间的信息双向流动。
- en: 'The overall loss function for the knowledge integration stage is the weighted
    sum of the two objectives:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 知识整合阶段的总体损失函数是两个目标的加权和：
- en: '|  | $\mathcal{L}_{ki}=\lambda_{4}\mathcal{L}_{iaci}+\lambda_{5}\mathcal{L}_{iakr}$
    |  | (9) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ki}=\lambda_{4}\mathcal{L}_{iaci}+\lambda_{5}\mathcal{L}_{iakr}$
    |  | (9) |'
- en: where $\lambda_{4}$ are hyperparameters that control the relative importance
    of each objective. By minimizing this loss, the LLM is trained to generate dialogue
    responses that are coherent and engaging based on both the dialogue context and
    the distilled visual implicit knowledge.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{4}$ 是控制每个目标相对重要性的超参数。通过最小化这个损失，LLM 被训练生成基于对话上下文和提炼视觉隐性知识的连贯且吸引人的对话响应。
- en: III-D Zero-Resource Learning Detail
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 零资源学习细节
- en: III-D1 Training
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 训练
- en: To train our framework, we use four large-scale image-text datasets, including
    COCO Captions [[36](#bib.bib36)], CC3M [[37](#bib.bib37)], CC12M [[38](#bib.bib38)],
    and SBU [[39](#bib.bib39)]. We follow the same data processing and augmentation
    methods as BLIP-2 [[7](#bib.bib7)], which generates synthetic captions for web
    images using a pre-trained captioning model and a CLIP model. These datasets contain
    tens of millions of image-text pairs that cover a wide range of topics and scenarios,
    providing a rich source of visual implicit knowledge. We concurrently train both
    stages of our framework, employing image-text pair data in accordance with Equations
    ([5](#S3.E5 "In III-B3 Image-Assisted Masked Text Modeling ‣ III-B Knowledge Distillation
    Stage ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge into LLMs for
    Zero-Resource Dialogue Generation")) and ([9](#S3.E9 "In III-C2 Instruction-aware
    Knowledge Reconstruction ‣ III-C Knowledge Integration Stage ‣ III Methodology
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation")). For fine-tuning on specific tasks, we minimize the negative log-likelihood
    loss to optimize the probability $P(R|C,K)$, as detailed in Section [III-D2](#S3.SS4.SSS2
    "III-D2 Inference ‣ III-D Zero-Resource Learning Detail ‣ III Methodology ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation").
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的框架，我们使用了四个大规模图像-文本数据集，包括 COCO Captions [[36](#bib.bib36)]、CC3M [[37](#bib.bib37)]、CC12M
    [[38](#bib.bib38)] 和 SBU [[39](#bib.bib39)]。我们采用与 BLIP-2 [[7](#bib.bib7)] 相同的数据处理和增强方法，BLIP-2
    使用预训练的标题生成模型和 CLIP 模型为网络图像生成合成标题。这些数据集包含数千万对图像-文本对，涵盖广泛的话题和场景，提供了丰富的视觉隐性知识来源。我们同时训练框架的两个阶段，按照方程
    ([5](#S3.E5 "In III-B3 Image-Assisted Masked Text Modeling ‣ III-B Knowledge Distillation
    Stage ‣ III Methodology ‣ Distilling Implicit Multimodal Knowledge into LLMs for
    Zero-Resource Dialogue Generation")) 和 ([9](#S3.E9 "In III-C2 Instruction-aware
    Knowledge Reconstruction ‣ III-C Knowledge Integration Stage ‣ III Methodology
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation")) 中的图像-文本对数据进行训练。对于特定任务的微调，我们通过最小化负对数似然损失来优化概率 $P(R|C,K)$，详见第 [III-D2](#S3.SS4.SSS2
    "III-D2 Inference ‣ III-D Zero-Resource Learning Detail ‣ III Methodology ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation")
    节。
- en: III-D2 Inference
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D2 推理
- en: To perform zero-resource inference, we leverage the trained IQ-Former and the
    LLM to produce dialogue responses that enriched with the visual implicit knowledge
    distilled from the dialogue context. Given a dialogue context $C$ by sampling
    from the probability distribution over the vocabulary. Since the LLM has learned
    to integrate the visual implicit knowledge into the dialogue generation, it can
    produce natural and engaging responses that are consistent with the visual context,
    even in the absence of any explicit multimodal inputs.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行零资源推理，我们利用训练好的 IQ-Former 和 LLM 生成丰富的对话回应，这些回应融入了从对话上下文中提取的视觉隐性知识。给定一个对话上下文
    $C$，我们从词汇表的概率分布中进行采样。由于 LLM 已学会将视觉隐性知识整合到对话生成中，即使没有任何显式的多模态输入，它也能生成与视觉上下文一致的自然且引人入胜的回应。
- en: IV Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: In this section, we evaluate the performance of our proposed framework, VIKDF,
    on the task of zero-resource dialogue generation. We benchmark VIKDF against a
    range of baselines and ablation models, conducting both automatic and human evaluations.
    Additionally, we present qualitative examples to demonstrate the effectiveness
    of our approach.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们提出的框架 VIKDF 在零资源对话生成任务上的表现。我们将 VIKDF 与一系列基准和消融模型进行比较，进行自动和人工评估。此外，我们展示了定性示例，以证明我们方法的有效性。
- en: IV-A Datasets
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据集
- en: 'We employ two datasets to evaluate our framework: Image-Chat [[20](#bib.bib20)]
    and Reddit Conversation [[4](#bib.bib4)]. The Image-Chat dataset, crucial for
    validating VIKDF’s efficacy in zero-resource scenarios, is a large-scale image-grounded
    dialogue dataset featuring 202,000 dialogues across 202,000 images. Each dialogue
    comprises a single turn of context and response, with the latter influenced by
    the corresponding image. This dataset is divided into 186,782 training dialogues,
    5,000 validation dialogues, and 9,997 testing dialogues. The Reddit Conversation
    dataset is served to assess the performance of visual implicit knowledge. This
    dataset, sourced from the widely-used online forum, encompasses an extensive variety
    of dialogue topics and styles. It has been preprocessed to include 1,000,000 training
    dialogues, with an additional 20,000 for validation and 20,000 for testing.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个数据集来评估我们的框架：Image-Chat [[20](#bib.bib20)] 和 Reddit Conversation [[4](#bib.bib4)]。Image-Chat
    数据集对验证 VIKDF 在零资源场景下的有效性至关重要，它是一个大规模图像基础对话数据集，包含 202,000 个对话和 202,000 张图像。每个对话包括一个上下文和回应，其中回应受到相应图像的影响。该数据集分为
    186,782 个训练对话、5,000 个验证对话和 9,997 个测试对话。Reddit Conversation 数据集用于评估视觉隐性知识的表现。该数据集来自广泛使用的在线论坛，涵盖了各种对话主题和风格。它经过预处理，包含
    1,000,000 个训练对话，以及额外的 20,000 个验证对话和 20,000 个测试对话。
- en: Similar to [[14](#bib.bib14)], we use the text-based dialogue dataset Reddit
    Conversation to train the model’s foundational dialogue capabilities, and the
    image-grounded dialogue dataset Image-Chat to validate the model’s zero-resource
    dialogue generation capabilities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 [[14](#bib.bib14)]，我们使用基于文本的对话数据集 Reddit Conversation 来训练模型的基础对话能力，而使用图像基础对话数据集
    Image-Chat 来验证模型的零资源对话生成能力。
- en: IV-B Implementation Details
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 实施细节
- en: The VIKDF implementation utilizes the Hugging Face Transformers library [[20](#bib.bib20)].
    For the text and image encoders, we initialize them with the pre-trained CLIP
    model that employs a ViT-L/14 Transformer architecture. The IQ-Former is initialized
    with the pre-trained BERT-base model, while the chat version of Llama-2 with 7B
    parameters is used as the large language model. To ensure seamless integration
    and information flow across models of varying dimensionalities, the necessary
    linear transformations are applied, which are not mentioned in the methodology.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: VIKDF 实现利用 Hugging Face Transformers 库 [[20](#bib.bib20)]。对于文本和图像编码器，我们用预训练的
    CLIP 模型初始化它们，该模型采用 ViT-L/14 Transformer 架构。IQ-Former 使用预训练的 BERT-base 模型初始化，而
    Llama-2 的 7B 参数版本被用作大型语言模型。为了确保不同维度模型之间的无缝集成和信息流动，应用了必要的线性变换，这在方法论中没有提及。
- en: VIKDF adopts a simultaneous training regimen for both knowledge distillation
    and integration stages. This strategy is executed on four NVIDIA RTX 4090 GPUs,
    utilizing a batch size of 128 across 100,000 training steps. For optimization,
    we employ mixed-precision training with bfloat16 and utilize the AdamW optimizer
    [[40](#bib.bib40)] with a learning rate of $1e^{-4}$ patches at a 0.6 ratio. For
    the image-assisted masked text modeling, the strategy involves a 0.15 mask ratio.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VIKDF 采用了知识蒸馏和集成阶段的同步训练方案。该策略在四个 NVIDIA RTX 4090 GPU 上执行，使用 128 的批量大小，经过 100,000
    次训练步骤。为优化，我们采用混合精度训练，使用 bfloat16，并利用 AdamW 优化器 [[40](#bib.bib40)]，学习率为 $1e^{-4}$，补丁比例为
    0.6。对于图像辅助的掩码文本建模，该策略涉及 0.15 的掩码比例。
- en: IV-C Baseline Models
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 基线模型
- en: 'We compare VIKDF with the following baseline models:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 VIKDF 与以下基线模型进行比较：
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Seq2Seq [[41](#bib.bib41)], a foundational architecture for sequence-to-sequence
    processing that includes an encoder and a decoder equipped with Long Short-Term
    Memory (LSTM) units.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Seq2Seq [[41](#bib.bib41)]，一种用于序列到序列处理的基础架构，包括一个配备了长短期记忆（LSTM）单元的编码器和解码器。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: BART [[42](#bib.bib42)], a pre-eminent sequence-to-sequence pre-training model
    that utilizes a Transformer architecture.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BART [[42](#bib.bib42)]，一种利用 Transformer 架构的卓越序列到序列预训练模型。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ImgVAE [[4](#bib.bib4)], which employs variational autoencoder technology to
    integrate visual information into dialogue generation.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ImgVAE [[4](#bib.bib4)]，采用变分自编码器技术将视觉信息整合到对话生成中。
- en: •
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Maria [[5](#bib.bib5)], a visual experience-powered conversational agent that
    enriches dialogues with experiences from the visual world through a large-scale
    image index.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Maria [[5](#bib.bib5)]，一种通过大规模图像索引丰富对话的视觉体验驱动对话代理。
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Llama-2 [[33](#bib.bib33)], the LLM serving as our framework’s backbone. It
    is a pre-trained autoregressive model capable of generating natural and diverse
    text.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Llama-2 [[33](#bib.bib33)] 是我们框架的核心 LLM。它是一个预训练的自回归模型，能够生成自然且多样的文本。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ChatGPT [[2](#bib.bib2)] by OpenAI, which leverages the GPT architecture to
    generate human-like text responses, incorporating a mix of supervised and reinforcement
    learning techniques for dialogue systems. We use the gpt-3.5-turbo version in
    this paper.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChatGPT [[2](#bib.bib2)] 由 OpenAI 开发，利用 GPT 架构生成类似人类的文本响应，结合了监督学习和强化学习技术以用于对话系统。本文使用
    gpt-3.5-turbo 版本。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ZRIGF [[14](#bib.bib14)], a state-of-the-art model for zero-resource image-grounded
    dialogue generation that combines multimodal learning with a two-stage strategy.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ZRIGF [[14](#bib.bib14)] 是一种最先进的零资源图像基础对话生成模型，它结合了多模态学习和两阶段策略。
- en: Among them, ImgVAE, Maria, and ZRIGF are multimodal models, whereas the rest
    are unimodal text-based models. We prompt Llama-2 following the approach outlined
    in the Hugging Face blog¹¹1[https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2),
    and employ ChatGPT in accordance with the methodology described in [[14](#bib.bib14)].
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，ImgVAE、Maria 和 ZRIGF 是多模态模型，而其他则是单模态基于文本的模型。我们按照 Hugging Face 博客中描述的方法¹¹1[https://huggingface.co/blog/llama2](https://huggingface.co/blog/llama2)
    提示 Llama-2，并按照 [[14](#bib.bib14)] 中的方法使用 ChatGPT。
- en: IV-D Evaluation Metrics
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 评估指标
- en: In accordance with [[5](#bib.bib5)] and [[14](#bib.bib14)], we use both automatic
    and human evaluation metrics to assess the performance of VIKDF and the baseline
    models.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 [[5](#bib.bib5)] 和 [[14](#bib.bib14)]，我们使用自动和人工评估指标来评估 VIKDF 和基准模型的性能。
- en: 'For automatic evaluation, we employ the following metrics: (1) Perplexity (PPL)
    measures the model’s fluency, with lower values indicating better performance.
    (2) BLEU-1 [[43](#bib.bib43)] and ROUGE-L [[44](#bib.bib44)] evaluate the alignment
    of generated responses with human references, focusing on word-level accuracy
    and sequence similarity, respectively. (3) For semantic analysis, we employ Average,
    Extrema and Greedy metrics [[45](#bib.bib45)] to measure the cosine similarity
    between word embeddings of generated and reference texts, capturing semantic coherence.
    (4) Dis-1 and Dis-2 metrics [[46](#bib.bib46)] quantify the diversity of the model’s
    output by calculating the uniqueness of unigrams and bigrams, respectively, ensuring
    the model’s capability to produce varied and engaging responses.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动评估，我们使用以下指标：（1）困惑度（PPL）衡量模型的流畅性，值越低表示性能越好。（2）BLEU-1 [[43](#bib.bib43)] 和
    ROUGE-L [[44](#bib.bib44)] 评估生成响应与人工参考的对齐情况，分别侧重于词级准确性和序列相似性。（3）对于语义分析，我们使用平均值、极值和贪婪指标
    [[45](#bib.bib45)] 来测量生成文本和参考文本之间词嵌入的余弦相似度，以捕捉语义一致性。（4）Dis-1 和 Dis-2 指标 [[46](#bib.bib46)]
    通过计算 unigram 和 bigram 的唯一性来量化模型输出的多样性，确保模型能够生成多样且引人入胜的响应。
- en: 'For human evaluation, we engage three evaluators to collect ratings from human
    annotators. We randomly sample 100 dialogues from the test set, and ask three
    evaluators to rate each dialogue on a scale of 1 to 5, based on the following
    criteria: (1) Relevance: How relevant and related is the generated response to
    the given context and image? (2) Informativeness: How much new and useful information
    does the generated response provide in the context of the dialogue? (3) Fluency:
    How natural, readable, and grammatically correct is the generated response? The
    final score for each criterion is computed as the average rating across all evaluators.
    To ensure the reliability of the evaluation process and measure the agreement
    among evaluators, Fleiss’ Kappa [[47](#bib.bib47)] statistic is applied to evaluate
    the concordance among evaluators.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人工评估，我们邀请了三位评估者来收集人类标注者的评分。我们从测试集中随机抽取 100 个对话，并要求三位评估者根据以下标准对每个对话进行 1 到 5
    的评分：（1）相关性：生成的响应与给定的上下文和图像的相关性如何？（2）信息量：生成的响应在对话的上下文中提供了多少新且有用的信息？（3）流畅性：生成的响应多自然、可读且语法正确？每个标准的最终评分是所有评估者评分的平均值。为了确保评估过程的可靠性并衡量评估者之间的一致性，使用
    Fleiss’ Kappa [[47](#bib.bib47)] 统计量来评估评估者之间的一致性。
- en: 'TABLE I: Assessment of automated metrics: ${\dagger}$ indicates a fully zero-resource
    scenario without any prior training on task-specific datasets. Bold font highlights
    the best performance in each column, and underlines signify the second-best performance.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 自动化评估指标的评估: ${\dagger}$ 表示一个完全零资源的场景，没有任何针对特定任务的数据集的预先训练。粗体字突出显示每列中的最佳表现，下划线表示第二佳表现。'
- en: '| Task | Methods | PPL | BLEU-1 | ROUGE-L | Average | Extrema | Greedy |     
    Dis-1 | Dis-2 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 方法 | PPL | BLEU-1 | ROUGE-L | 平均值 | 极值 | 贪婪 |      Dis-1 | Dis-2 |'
- en: '| Reddit Conversation | Seq2Seq | $77.27$ |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Reddit 对话 | Seq2Seq | $77.27$ |'
- en: '| BART | $44.73$ |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| BART | $44.73$ |'
- en: '| Llama-2^‡ | $155.69$ |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2^‡ | $155.69$ |'
- en: '| ChatGPT^‡ | - | $11.62$ | 38.63 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT^‡ | - | $11.62$ | 38.63 |'
- en: '| ImgVAE | $72.06$ |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ImgVAE | $72.06$ |'
- en: '| Maria | $56.23$ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Maria | $56.23$ |'
- en: '|  | ZRIGF | 36.21 | 16.06 | 14.51 | $82.27$ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | ZRIGF | 36.21 | 16.06 | 14.51 | $82.27$ |'
- en: '|  | VIKDF | 15.01 | 16.41 | 14.82 | 82.53 | 43.71 | 64.88 |     6.39 | 35.84
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | VIKDF | 15.01 | 16.41 | 14.82 | 82.53 | 43.71 | 64.88 |     6.39 | 35.84
    |'
- en: '| Image-Chat | Seq2Seq^† | $50.82$ |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Image-Chat | Seq2Seq^† | $50.82$ |'
- en: '| BART^† | $37.26$ |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| BART^† | $37.26$ |'
- en: '| Llama-2^‡ | $193.20$ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2^‡ | $193.20$ |'
- en: '| ChatGPT^‡ | - | $10.77$ | 37.77 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT^‡ | - | $10.77$ | 37.77 |'
- en: '| ImgVAE | $41.94$ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ImgVAE | $41.94$ |'
- en: '| Maria^† | $37.49$ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Maria^† | $37.49$ |'
- en: '| Maria${}_{zero}^{\ddagger}$ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Maria${}_{zero}^{\ddagger}$ |'
- en: '| ZRIGF^† | $29.82$ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ZRIGF^† | $29.82$ |'
- en: '| ZRIGF${}_{1/4}^{\dagger}$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| ZRIGF${}_{1/4}^{\dagger}$ |'
- en: '|  | ZRIGF${}_{zero}^{\ddagger}$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | ZRIGF${}_{zero}^{\ddagger}$ |'
- en: '|  | VIKDF^† | 12.56 | 17.92 | 17.33 | 86.47 | 50.83 | 68.45 | $4.61$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | VIKDF^† | 12.56 | 17.92 | 17.33 | 86.47 | 50.83 | 68.45 | $4.61$ |'
- en: '|  | 1/4 Data^† | 12.84 | 17.81 | $16.91$ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | 1/4 数据^† | 12.84 | 17.81 | $16.91$ |'
- en: '|  | 1/8 Data^† | $13.12$ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | 1/8 数据^† | $13.12$ |'
- en: '|  | Zero Data^‡ | $27.32$ |     6.17 | 32.93 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | Zero Data^‡ | $27.32$ |     6.17 | 32.93 |'
- en: V Results and Discussion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结果与讨论
- en: Our evaluations differentiate between models operating in distinct zero-resource
    scenarios, as denoted by ${\dagger}$ denotes a fully zero-resource condition,
    in which models generate dialogues without any prior training on task-specific
    datasets.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估区分了在不同零资源场景下运行的模型，其中 ${\dagger}$ 表示完全零资源条件，在这种情况下，模型在没有任何针对特定任务的数据集的预先训练的情况下生成对话。
- en: V-A Automatic Evaluation Results
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 自动评估结果
- en: Our proposed VIKDF demonstrates outstanding performance in zero-resource dialogue
    generation, outperforming both traditional and multimodal baseline models across
    various metrics on the Reddit Conversation and Image-Chat datasets. Table [I](#S4.T1
    "TABLE I ‣ IV-D Evaluation Metrics ‣ IV Experiments ‣ Distilling Implicit Multimodal
    Knowledge into LLMs for Zero-Resource Dialogue Generation") presents a comprehensive
    comparison of the automated evaluation metrics.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 VIKDF 在零资源对话生成方面表现出色，超越了 Reddit 对话和 Image-Chat 数据集上的传统和多模态基线模型。表 [I](#S4.T1
    "TABLE I ‣ IV-D Evaluation Metrics ‣ IV Experiments ‣ Distilling Implicit Multimodal
    Knowledge into LLMs for Zero-Resource Dialogue Generation") 展示了自动化评估指标的全面比较。
- en: In the Reddit Conversation task, VIKDF achieves a significantly lower perplexity
    of 15.01, indicating superior fluency in generated dialogues compared to the nearest
    competitor, ZRIGF, which records a PPL of 36.21 in scenarios with explicit image
    guidance. Moreover, VIKDF outperforms all baselines in BLEU-1 and ROUGE-L scores,
    achieving 16.41 and 14.82, respectively, which highlights its ability to generate
    responses that are closely aligned with human references. Additionally, VIKDF
    surpasses other models in both Average and Greedy semantic similarity metrics,
    underscoring its enhanced ability to maintain semantic coherence in dialogues.
    Despite scoring slightly lower on the Extrema metric compared to ZRIGF, VIKDF
    remains competitive. Although it has a marginally lower Dis-2 score than ChatGPT,
    VIKDF’s strong performance in generating diverse dialogues is evident. This suggests
    that while maintaining high relevance and accuracy, VIKDF also generates a wide
    range of responses, contributing to more dynamic and engaging dialogues. The outstanding
    performance of VIKDF in the Reddit Conversation task highlights the substantial
    benefits of incorporating implicit knowledge into purely text-based dialogue systems,
    proving that a deep fusion of visual and contextual understanding significantly
    enhances the quality and engagement of the dialogues generated.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在Reddit对话任务中，VIKDF实现了显著更低的困惑度15.01，显示出生成对话的流畅性优于最近的竞争对手ZRIGF，后者在显式图像指导场景中的PPL为36.21。此外，VIKDF在BLEU-1和ROUGE-L评分上超越了所有基线模型，分别达到了16.41和14.82，突显了其生成与人类参考紧密对齐的回应的能力。此外，VIKDF在平均和贪婪语义相似性指标上超过了其他模型，突显了其在对话中保持语义连贯性的增强能力。尽管在Extrema指标上的得分略低于ZRIGF，VIKDF仍然保持竞争力。尽管其Dis-2得分略低于ChatGPT，VIKDF在生成多样化对话方面的强劲表现仍然显而易见。这表明，VIKDF在保持高相关性和准确性的同时，也生成了广泛的回应，从而促进了更动态和引人入胜的对话。VIKDF在Reddit对话任务中的杰出表现突显了将隐性知识融入纯文本对话系统的显著好处，证明了视觉和上下文理解的深度融合显著提升了生成对话的质量和参与度。
- en: In the Image-Chat task, VIKDF’s capabilities are examined under various conditions,
    including limited data availability scenarios (1/4 and 1/8 of the full training
    set) and fully zero-resource scenarios. We can see that VIKDF showcases superior
    performance in various zero-resource scenarios, especially notable in the absence
    of annotated images (${\dagger}$), showcasing its exceptional capability to generate
    diverse, relevant, and engaging dialogues without task-specific training data.
    The achievements of VIKDF on the Image-Chat dataset affirm its leading position
    in zero-resource dialogue generation, illustrating unmatched adaptability and
    advanced integration of visual implicit knowledge.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在Image-Chat任务中，VIKDF的能力在各种条件下进行了检验，包括有限数据可用场景（全量训练集的1/4和1/8）和完全零资源场景。我们可以看到，VIKDF在各种零资源场景中展示了卓越的表现，特别是在缺少标注图像的情况下（${\dagger}$），展现了其在没有任务特定训练数据的情况下生成多样化、相关且引人入胜对话的卓越能力。VIKDF在Image-Chat数据集上的成就确认了其在零资源对话生成中的领先地位，展示了无与伦比的适应能力和先进的视觉隐性知识整合。
- en: 'TABLE II: Human evaluation outcomes for the Image-Chat dataset in a fully zero-resource
    scenario.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: 在完全零资源场景下对Image-Chat数据集的人类评估结果。'
- en: '| Methods | Relevance | Informativeness | Fluency | Kappa |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 相关性 | 信息量 | 流畅性 | Kappa |'
- en: '| Llama-2 | $2.94$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | $2.94$ |'
- en: '| ChatGPT | $3.22$ |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | $3.22$ |'
- en: '| ZRIGF |     3.55 | $3.15$ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ZRIGF |     3.55 | $3.15$ |'
- en: '| VIKDF |     3.84 |     3.30 |     4.34 | $0.59$ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| VIKDF |     3.84 |     3.30 |     4.34 | $0.59$ |'
- en: V-B Human Evaluation Results
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 人工评估结果
- en: Table [II](#S5.T2 "TABLE II ‣ V-A Automatic Evaluation Results ‣ V Results and
    Discussion ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource
    Dialogue Generation") presents the human evaluation outcomes for the Image-Chat
    dataset within a fully zero-resource scenario (${\ddagger}$), comparing the performance
    of VIKDF against Llama-2, ChatGPT, and ZRIGF. Notably, VIKDF achieves the highest
    scores in both Relevance and Informativeness. This indicates its exceptional ability
    to generate dialogues that are not only closely related to the given context and
    images but also rich in informative content. Although ChatGPT receives the highest
    rating in Fluency, VIKDF remains competitive, with a score of 4.34, illustrating
    its capacity to produce responses that are natural, coherent, and grammatically
    correct. This demonstrates that VIKDF can generate dialogues which are both contextually
    relevant and engaging, with a high degree of linguistic quality. The Fleiss’ Kappa
    score, indicative of inter-rater agreement, falls within a moderate range across
    all models, ensuring that the evaluation process maintains a degree of reliability
    despite its inherently subjective nature.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表[II](#S5.T2 "TABLE II ‣ V-A Automatic Evaluation Results ‣ V Results and Discussion
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation")展示了Image-Chat数据集在完全零资源场景下(${ \ddagger }$)的人工评估结果，比较了VIKDF与Llama-2、ChatGPT和ZRIGF的表现。值得注意的是，VIKDF在相关性和信息性方面都取得了最高分。这表明它在生成与给定上下文和图像密切相关且内容丰富的对话方面具有卓越的能力。尽管ChatGPT在流畅性方面获得了最高评分，但VIKDF依然保持竞争力，得分为4.34，展示了其生成自然、连贯且语法正确的响应的能力。这表明VIKDF能够生成既符合上下文又引人入胜的对话，且语言质量高。Fleiss'
    Kappa分数显示所有模型之间的评估一致性处于中等范围，确保了评估过程在固有主观性的情况下保持了一定的可靠性。
- en: In summary, these results affirm VIKDF’s exemplary performance in zero-resource
    dialogue generation, particularly its adeptness at leveraging visual implicit
    knowledge to augment the relevance and informativeness of dialogues, while maintaining
    a high standard of fluency.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这些结果确认了VIKDF在零资源对话生成中的卓越表现，特别是它在利用视觉隐性知识来增强对话的相关性和信息性方面的高超能力，同时保持了高水平的流畅性。
- en: 'TABLE III: Ablation study. Zero Data means in a fully zero-resource scenario.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：消融研究。零数据意味着在完全零资源场景下。
- en: '| Methods | Reddit Conversation | Image-Chat (Zero Data) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Reddit对话 | Image-Chat（零数据） |'
- en: '| BLEU-1 | ROUGE-L | BLEU-1 | ROUGE-L |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| BLEU-1 | ROUGE-L | BLEU-1 | ROUGE-L |'
- en: '| VIKDF | 16.41 | 14.82 | 15.35 | 15.30 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| VIKDF | 16.41 | 14.82 | 15.35 | 15.30 |'
- en: '| -TIM | $14.75$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| -TIM | $14.75$ |'
- en: '| -TAMIM | $16.03$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| -TAMIM | $16.03$ |'
- en: '| -IAMTM | $16.18$ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| -IAMTM | $16.18$ |'
- en: '| -BVIF | $15.65$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| -BVIF | $15.65$ |'
- en: '![Refer to caption](img/4ee9f9583ad2da47fc9f4a5fd96c48ba.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ee9f9583ad2da47fc9f4a5fd96c48ba.png)'
- en: 'Figure 5: Case study on Image-Chat test set in a fully zero-resource scenario.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在完全零资源场景下对Image-Chat测试集的案例研究。
- en: V-C Ablation Study
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 消融研究
- en: To evaluate the individual contributions of the components within VIKDF, we
    conduct an ablation study by sequentially removing each component and assessing
    the impact on performance metrics (BLEU-1 and ROUGE-L) across both Reddit Conversation
    and Image-Chat datasets. In the absence of BVIF, we adopt the vision-to-language
    generative learning as outlined in [[7](#bib.bib7)]. The results, detailed in
    Table [III](#S5.T3 "TABLE III ‣ V-B Human Evaluation Results ‣ V Results and Discussion
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"), demonstrate a consistent decline in performance upon the exclusion
    of any component, underscoring their collective importance to the framework’s
    efficacy. Notably, the exclusion of TIM yields the most pronounced decline in
    performance metrics. This indicates TIM’s critical function in maintaining alignment
    between textual and visual modalities, a foundational aspect of generating coherent
    and contextually relevant dialogues. Additionally, omitting BVIF leads to a noticeable
    dip in performance metrics. This underscores BVIF’s importance in seamlessly integrating
    distilled visual knowledge into the dialogue generation process, further enhancing
    the model’s ability to produce contextually rich and engaging dialogues. The removal
    of TAMIM and IAMTM also leads to decreased performance. This result highlights
    their significance in enriching the model’s capability to infer and align multimodal
    knowledge, thereby facilitating a more nuanced dialogue generation process.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估VIKDF中各组件的个体贡献，我们通过逐个去除每个组件并评估对性能指标（BLEU-1和ROUGE-L）在Reddit Conversation和Image-Chat数据集上的影响来进行消融研究。在缺少BVIF的情况下，我们采用[[7](#bib.bib7)]中概述的视图到语言生成学习。结果详见表[III](#S5.T3
    "TABLE III ‣ V-B 人工评估结果 ‣ V 结果与讨论 ‣ 将隐性多模态知识提炼到LLMs中以实现零资源对话生成")，结果显示在排除任何组件时性能均有一致下降，强调了它们对框架效能的集体重要性。特别地，排除TIM导致性能指标的最显著下降。这表明TIM在维持文本与视觉模态之间的一致性方面具有关键作用，这是生成连贯且上下文相关对话的基础。此外，省略BVIF会导致性能指标明显下降。这突显了BVIF在将提炼的视觉知识无缝整合到对话生成过程中的重要性，从而进一步增强模型生成上下文丰富且引人入胜对话的能力。去除TAMIM和IAMTM也会导致性能下降。这一结果突出它们在丰富模型推断和对齐多模态知识能力方面的重要性，从而促进更细致的对话生成过程。
- en: '![Refer to caption](img/bd94ef90f4248f48905163d829614a8b.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd94ef90f4248f48905163d829614a8b.png)'
- en: ((a))
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ((a))
- en: '![Refer to caption](img/cbf3f3cf1d04eab8dc1da67d95c88b14.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cbf3f3cf1d04eab8dc1da67d95c88b14.png)'
- en: ((b))
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ((b))
- en: 'Figure 6: Performance comparison with various numbers of queries.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：使用不同数量查询的性能比较。
- en: '![Refer to caption](img/d803cb508da47f12e8565e163ca5c561.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d803cb508da47f12e8565e163ca5c561.png)'
- en: 'Figure 7: Example of visual implicit knowledge textualized by VIKDF.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：通过VIKDF文本化的视觉隐性知识示例。
- en: V-D Case study
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 案例研究
- en: To further illustrate the superior capabilities of VIKDF, we conduct a case
    study contrasting VIKDF against key baseline models by examining a specific example
    from the Image-Chat test set in a fully zero-resource scenario, as shown in Figure
    [5](#S5.F5 "Figure 5 ‣ V-B Human Evaluation Results ‣ V Results and Discussion
    ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue
    Generation"). In a dialogue context that evokes curiosity and apprehension towards
    the sky, VIKDF leverages distilled visual implicit knowledge to generate a response
    that captures both the awe of the sky’s vast beauty and the curiosity towards
    its unknown mysteries. In comparison, Llama-2 produces a relevant but less informative
    response. ChatGPT, while producing a context-aware response, misses the mark on
    integrating the emotive complexity of the conversation, focusing instead on extracting
    additional context from the user. This demonstrates the limitations of LLMs in
    multimodal dialog generation scenarios. In contrast, ZRIGF’s response, though
    creative, diverges from the contextual context due to its reliance on retrieved
    explicit images, whereas VIKDF overcomes this by distilling visual implicit knowledge
    from text.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步展示VIKDF的卓越能力，我们进行了一个案例研究，将VIKDF与关键基线模型进行对比，通过检查来自Image-Chat测试集的一个特定示例，在完全零资源的场景下，如图[5](#S5.F5
    "Figure 5 ‣ V-B Human Evaluation Results ‣ V Results and Discussion ‣ Distilling
    Implicit Multimodal Knowledge into LLMs for Zero-Resource Dialogue Generation")所示。在一个引发对天空的好奇心和忧虑的对话上下文中，VIKDF利用提炼的视觉隐性知识生成了一个回应，捕捉了天空广阔美丽的敬畏感和对其未知奥秘的好奇心。相比之下，Llama-2产生了相关但信息量较少的回应。ChatGPT虽然产生了上下文相关的回应，但未能整合对话的情感复杂性，而是侧重于从用户那里提取额外的上下文。这显示了LLMs在多模态对话生成场景中的局限性。相比之下，ZRIGF的回应虽然具有创意，但由于依赖检索到的显性图像，与上下文有所偏离，而VIKDF通过从文本中提炼视觉隐性知识克服了这一问题。
- en: To provide a more detailed analysis of visual implicit knowledge, we attempt
    to textualize the visual implicit knowledge through LLMs, following the process
    illustrated in Figure [7](#S5.F7 "Figure 7 ‣ V-C Ablation Study ‣ V Results and
    Discussion ‣ Distilling Implicit Multimodal Knowledge into LLMs for Zero-Resource
    Dialogue Generation"). The process involves transforming the dialogue context
    through IQ-Former that distills visual implicit knowledge, subsequently textualizing
    it via an LLM by a text prompt. We can see that the textual output illustrates
    the model’s capacity to encapsulate complex human emotions and curiosities about
    the sky. This demonstrates the model’s adeptness at integrating and expressing
    visual implicit knowledge.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供更详细的视觉隐性知识分析，我们尝试通过LLMs将视觉隐性知识文本化，按照图[7](#S5.F7 "Figure 7 ‣ V-C Ablation
    Study ‣ V Results and Discussion ‣ Distilling Implicit Multimodal Knowledge into
    LLMs for Zero-Resource Dialogue Generation")中所示的过程进行。该过程涉及通过IQ-Former将对话上下文转换为提炼的视觉隐性知识，然后通过LLM和文本提示将其文本化。我们可以看到，文本输出展示了模型捕捉复杂人类情感和对天空好奇心的能力。这表明模型在整合和表达视觉隐性知识方面的娴熟能力。
- en: This analysis underscores VIKDF’s superior ability to synthesize and leverage
    visual implicit knowledge, enabling it to generate dialogues that are more engaging,
    visually grounded, and contextually appropriate.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这一分析突显了VIKDF在合成和利用视觉隐性知识方面的优越能力，使其能够生成更具吸引力、视觉基础和上下文适宜的对话。
- en: V-E Impact of Query Vector Quantity
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 查询向量数量的影响
- en: Exploring the influence of the hyper-parameter $n$) could introduce unnecessary
    noise or dilute the relevance of the distilled knowledge.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 探索超参数$n$的影响可能会引入不必要的噪声或稀释提炼知识的相关性。
- en: The experiment highlights the importance of a balanced query vector count in
    achieving effective dialogue generation. An optimal $n$ allows the IQ-Former to
    distill relevant visual implicit knowledge without overcomplicating the model,
    demonstrating the delicate balance between quantity and quality of distilled knowledge
    for enhancing dialogue generation.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 实验强调了平衡查询向量数量在实现有效对话生成中的重要性。一个优化的$n$允许IQ-Former提炼相关的视觉隐性知识而不会使模型过于复杂，展示了提炼知识的数量与质量之间的微妙平衡，以提升对话生成效果。
- en: VI Conclusion and Future Work
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论与未来工作
- en: In this paper, we present VIKDF, an innovative methodology aimed at enhancing
    LLMs for dialogue generation in zero-resource scenarios through the distillation
    and integration of implicit multimodal knowledge. VIKDF utilizes an IQ-Former
    to extract visual implicit knowledge and a BVIF technique to incorporate this
    knowledge into LLMs, enabling the generation of dialogues that are coherent, engaging,
    and rich in contextual understanding. Our comprehensive experiments across diverse
    dialogue datasets have shown that VIKDF outperforms existing state-of-the-art
    models in zero-resource scenarios, illustrating its effectiveness in leveraging
    implicit multimodal knowledge even without explicit multimodal inputs or annotated
    datasets. The ablation study underscores the indispensable role of each component
    within VIKDF, and human evaluations have confirmed its success in generating dialogues
    that are relevant, informative, and naturally fluent, closely aligning human conversational
    standards. Consequently, VIKDF represents a significant advancement in the field
    of multimodal dialogue generation, highlighting the importance of implicit multimodal
    knowledge in enhancing LLMs capabilities in zero-resource scenarios.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了 VIKDF，一种旨在通过提炼和整合隐式多模态知识来提升 LLMs 在零资源场景下对话生成的创新方法。VIKDF 利用 IQ-Former
    提取视觉隐式知识，并采用 BVIF 技术将这些知识融入 LLMs，从而生成连贯、引人入胜且具有丰富上下文理解的对话。我们在多个对话数据集上的全面实验表明，VIKDF
    在零资源场景下超越了现有的最先进模型，展示了其在没有显式多模态输入或标注数据集的情况下利用隐式多模态知识的有效性。消融研究强调了 VIKDF 中每个组件的不可或缺的作用，人工评估确认了其在生成相关、信息丰富且自然流畅的对话方面的成功，紧密符合人类对话标准。因此，VIKDF
    代表了多模态对话生成领域的重大进展，突显了隐式多模态知识在提升 LLMs 在零资源场景下能力中的重要性。
- en: The proposed model utilizes only implicit multimodal information, which limits
    its applicability in tasks requiring explicit multimodal inputs, such as visual
    question answering, and multimodal outputs, such as text-to-image generation.
    In future work, we plan to integrate both explicit and implicit multimodal information
    to develop a dialogue generation system capable of supporting both multimodal
    inputs and outputs. This advancement will enable our model to engage more comprehensively
    with various types of content, potentially enhancing its performance and applicability
    in multimodal interaction scenarios.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的模型仅利用隐式多模态信息，这限制了其在需要显式多模态输入（如视觉问答）和多模态输出（如文本到图像生成）任务中的适用性。在未来的工作中，我们计划整合显式和隐式多模态信息，以开发一个能够支持多模态输入和输出的对话生成系统。这一进展将使我们的模型能够更全面地处理各种类型的内容，可能提高其在多模态交互场景中的表现和适用性。
- en: Acknowledgments
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research is supported by the National Natural Science Foundation of China
    (No. 62006034).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了中国国家自然科学基金（编号：62006034）的资助。
- en: References
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
    R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
    M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,
    I. Sutskever, and D. Amodei, “Language models are few-shot learners,” in *Advances
    in Neural Information Processing Systems*, vol. 33, 2020, pp. 1877–1901.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger,
    T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen,
    E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A.
    Radford, I. Sutskever, 和 D. Amodei，“语言模型是少量样本学习者，”见 *神经信息处理系统进展*，第 33 卷，2020年，第
    1877–1901 页。'
- en: '[2] OpenAI. (2022, nov) Introducing ChatGPT. [Online]. Available: [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] OpenAI. (2022年11月) 介绍 ChatGPT. [在线]. 可用： [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)'
- en: '[3] ——, “GPT-4 technical report,” 2023, *arXiv:2303.08774*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] ——，“GPT-4 技术报告，” 2023，*arXiv:2303.08774*。'
- en: '[4] Z. Yang, W. Wu, H. Hu, C. Xu, W. Wang, and Z. Li, “Open domain dialogue
    generation with latent images,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 35, no. 16, 2021, pp. 14 239–14 247.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Yang, W. Wu, H. Hu, C. Xu, W. Wang, 和 Z. Li，“基于潜在图像的开放领域对话生成，”见 *美国人工智能协会会议论文集*，第
    35 卷，第 16 期，2021年，第 14,239–14,247 页。'
- en: '[5] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, and D. Jiang,
    “Maria: A visual experience powered conversational agent,” in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 5596–5611.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, 和 D. Jiang,
    “Maria：一个视觉体验驱动的对话代理，” 收录于 *第59届计算语言学协会年会及第11届国际自然语言处理联合会议论文集（第1卷：长文）》, 2021年，页码5596–5611。'
- en: '[6] L. Shen, H. Zhan, X. Shen, Y. Song, and X. Zhao, “Text is not enough: Integrating
    visual impressions into open-domain dialogue generation,” in *Proceedings of the
    29th ACM International Conference on Multimedia*, 2021, pp. 4287–4296.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] L. Shen, H. Zhan, X. Shen, Y. Song, 和 X. Zhao, “文字还不够：将视觉印象整合到开放领域对话生成中，”
    收录于 *第29届ACM国际多媒体会议论文集*，2021年，页码4287–4296。'
- en: '[7] J. Li, D. Li, S. Savarese, and S. Hoi, “BLIP-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models,” in *Proceedings
    of the 40th International Conference on Machine Learning*, vol. 202, 2023, pp.
    19 730–19 742.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Li, D. Li, S. Savarese, 和 S. Hoi, “BLIP-2：利用冻结图像编码器和大型语言模型进行语言-图像预训练，”
    收录于 *第40届国际机器学习会议论文集*，第202卷，2023年，页码19,730–19,742。'
- en: '[8] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour,
    R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi,
    “Photorealistic text-to-image diffusion models with deep language understanding,”
    in *Advances in Neural Information Processing Systems*, vol. 35, 2022, pp. 36 479–36 494.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour,
    R. Gontijo Lopes, B. Karagol Ayan, T. Salimans, J. Ho, D. J. Fleet, 和 M. Norouzi,
    “具有深度语言理解的真实感文本到图像扩散模型，” 收录于 *神经信息处理系统进展*，第35卷，2022年，页码36,479–36,494。'
- en: '[9] M. Maaz, H. Rasheed, S. Khan, and F. S. Khan, “Video-chatgpt: Towards detailed
    video understanding via large vision and language models,” 2023, *arXiv:2306.05424*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] M. Maaz, H. Rasheed, S. Khan, 和 F. S. Khan, “Video-chatgpt：通过大型视觉和语言模型实现详细的视频理解，”
    2023年，*arXiv:2306.05424*。'
- en: '[10] D. Ghosal, N. Majumder, A. Mehrish, and S. Poria, “Text-to-audio generation
    using instruction guided latent diffusion model,” in *Proceedings of the 31st
    ACM International Conference on Multimedia*, 2023, pp. 3590–3598.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] D. Ghosal, N. Majumder, A. Mehrish, 和 S. Poria, “使用指令引导的潜在扩散模型生成文本到音频，”
    收录于 *第31届ACM国际多媒体会议论文集*，2023年，页码3590–3598。'
- en: '[11] N. Mostafazadeh, C. Brockett, B. Dolan, M. Galley, J. Gao, G. Spithourakis,
    and L. Vanderwende, “Image-grounded conversations: Multimodal context for natural
    question and response generation,” in *Proceedings of the Eighth International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, 2017,
    pp. 462–472.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] N. Mostafazadeh, C. Brockett, B. Dolan, M. Galley, J. Gao, G. Spithourakis,
    和 L. Vanderwende, “图像引导对话：用于自然问答生成的多模态上下文，” 收录于 *第八届国际自然语言处理联合会议论文集（第1卷：长文）》，2017年，页码462–472。'
- en: '[12] K. Shuster, S. Humeau, A. Bordes, and J. Weston, “Image-chat: Engaging
    grounded conversations,” in *Proceedings of the 58th Annual Meeting of the Association
    for Computational Linguistics*, 2020, pp. 2414–2429.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] K. Shuster, S. Humeau, A. Bordes, 和 J. Weston, “Image-chat：引人入胜的基础对话，”
    收录于 *第58届计算语言学协会年会论文集*，2020年，页码2414–2429。'
- en: '[13] Y. Zheng, G. Chen, X. Liu, and J. Sun, “MMChat: Multi-modal chat dataset
    on social media,” in *Proceedings of the Thirteenth Language Resources and Evaluation
    Conference*, 2022, pp. 5778–5786.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Y. Zheng, G. Chen, X. Liu, 和 J. Sun, “MMChat：社交媒体上的多模态聊天数据集，” 收录于 *第十三届语言资源与评估会议论文集*，2022年，页码5778–5786。'
- en: '[14] B. Zhang, J. Wang, H. Ma, B. Xu, and H. Lin, “ZRIGF: An innovative multimodal
    framework for zero-resource image-grounded dialogue generation,” in *Proceedings
    of the 31st ACM International Conference on Multimedia*, 2023, pp. 5464–5473.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] B. Zhang, J. Wang, H. Ma, B. Xu, 和 H. Lin, “ZRIGF：一种创新的零资源图像引导对话生成多模态框架，”
    收录于 *第31届ACM国际多媒体会议论文集*，2023年，页码5464–5473。'
- en: '[15] A. Richardson, *Mental Imagery*.   Berlin, Heidelberg: Springer, 1969.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] A. Richardson, *心理意象*.  柏林，海德堡：施普林格，1969年。'
- en: '[16] R. Jackendoff, *Foundations of Language: Brain, Meaning, Grammar, Evolution*.   Oxford
    University Press, 2002.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] R. Jackendoff, *语言的基础：大脑、意义、语法、进化*。 牛津大学出版社，2002年。'
- en: '[17] S. M. Kosslyn, W. L. Thompson, and G. Ganis, *The Case for Mental Imagery*.   Oxford
    University Press, 2006.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] S. M. Kosslyn, W. L. Thompson, 和 G. Ganis, *心理意象的案例*。 牛津大学出版社，2006年。'
- en: '[18] D. Roy, K.-Y. Hsiao, and N. Mavridis, “Mental imagery for a conversational
    robot,” *IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)*,
    vol. 34, no. 3, pp. 1374–1383, 2004.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] D. Roy, K.-Y. Hsiao, 和 N. Mavridis，“会话机器人中的心理意象，” *IEEE系统、管理和控制论学报，B部分（控制论）*，第34卷，第3期，第1374–1383页，2004年。'
- en: '[19] J. Y. Koh, R. Salakhutdinov, and D. Fried, “Grounding language models
    to images for multimodal inputs and outputs,” in *Proceedings of the 40th International
    Conference on Machine Learning*, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Y. Koh, R. Salakhutdinov, 和 D. Fried，“将语言模型与图像对接，用于多模态输入和输出，” 在 *第40届国际机器学习大会论文集*，2023年。'
- en: '[20] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,
    Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush,
    “Transformers: State-of-the-art natural language processing,” in *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*, 2020, pp. 38–45.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma,
    Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, 和 A. Rush，“变换器：最先进的自然语言处理，”
    在 *2020年自然语言处理实证方法会议：系统演示论文集*，2020年，第38–45页。'
- en: '[21] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh,
    and D. Batra, “Visual dialog,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2017, pp. 326–335.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh,
    和 D. Batra，“视觉对话，” 在 *IEEE计算机视觉与模式识别大会论文集*，2017年，第326–335页。'
- en: '[22] H. Alamri, V. Cartillier, A. Das, J. Wang, A. Cherian, I. Essa, D. Batra,
    T. K. Marks, C. Hori, P. Anderson *et al.*, “Audio visual scene-aware dialog,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2019, pp. 7558–7567.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Alamri, V. Cartillier, A. Das, J. Wang, A. Cherian, I. Essa, D. Batra,
    T. K. Marks, C. Hori, P. Anderson *等*，“音频视觉场景感知对话，” 在 *IEEE/CVF计算机视觉与模式识别大会论文集*，2019年，第7558–7567页。'
- en: '[23] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, and D. Jiang,
    “Maria: A visual experience powered conversational agent,” in *Proceedings of
    the 59th Annual Meeting of the Association for Computational Linguistics and the
    11th International Joint Conference on Natural Language Processing (Volume 1:
    Long Papers)*, 2021, pp. 5596–5611.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Z. Liang, H. Hu, C. Xu, C. Tao, X. Geng, Y. Chen, F. Liang, 和 D. Jiang，“Maria：一个视觉体验驱动的会话代理，”
    在 *第59届计算语言学协会年会和第11届国际自然语言处理联合会议论文集（卷1：长篇论文）*，2021年，第5596–5611页。'
- en: '[24] J. Feng, Q. Sun, C. Xu, P. Zhao, Y. Yang, C. Tao, D. Zhao, and Q. Lin,
    “Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal open-domain
    conversation,” 2022, *arXiv:2211.05719*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Feng, Q. Sun, C. Xu, P. Zhao, Y. Yang, C. Tao, D. Zhao, 和 Q. Lin，“Mmdialog：一个大规模多轮对话数据集，面向多模态开放领域对话，”
    2022年，*arXiv:2211.05719*。'
- en: '[25] J. Y. Koh, R. Salakhutdinov, and D. Fried, “Grounding language models
    to images for multimodal inputs and outputs,” in *International Conference on
    Machine Learning*, 2023, pp. 17 283–17 300.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Y. Koh, R. Salakhutdinov, 和 D. Fried，“将语言模型与图像对接，用于多模态输入和输出，” 在 *国际机器学习大会*，2023年，第17 283–17 300页。'
- en: '[26] Q. Sun, Y. Wang, C. Xu, K. Zheng, Y. Yang, H. Hu, F. Xu, J. Zhang, X. Geng,
    and D. Jiang, “Multimodal dialogue response generation,” in *Proceedings of the
    60th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, 2022, pp. 2854–2866.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Q. Sun, Y. Wang, C. Xu, K. Zheng, Y. Yang, H. Hu, F. Xu, J. Zhang, X.
    Geng, 和 D. Jiang，“多模态对话响应生成，” 在 *第60届计算语言学协会年会论文集（卷1：长篇论文）*，2022年，第2854–2866页。'
- en: '[27] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K.
    Mohammed, S. Singhal, S. Som, and F. Wei, “Image as a foreign language: Beit pretraining
    for vision and vision-language tasks,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, 2023, pp. 19 175–19 186.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O.
    K. Mohammed, S. Singhal, S. Som, 和 F. Wei，“图像作为外语：Beit预训练用于视觉和视觉-语言任务，” 在 *IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2023年，第19 175–19 186页。'
- en: '[28] X. Xue, C. Zhang, Z. Niu, and X. Wu, “Multi-level attention map network
    for multimodal sentiment analysis,” *IEEE Transactions on Knowledge and Data Engineering*,
    vol. 35, no. 5, pp. 5105–5118, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Xue, C. Zhang, Z. Niu, 和 X. Wu，“用于多模态情感分析的多层次注意力图网络，” *IEEE知识与数据工程学报*，第35卷，第5期，第5105–5118页，2023年。'
- en: '[29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable
    visual models from natural language supervision,” in *Proceedings of the 38th
    International Conference on Machine Learning*, vol. 139, 2021, pp. 8748–8763.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, 和 I. Sutskever，“从自然语言监督中学习可转移的视觉模型，”见于*第38届国际机器学习会议*，第139卷，2021年，第8748–8763页。'
- en: '[30] W. Dai, L. Hou, L. Shang, X. Jiang, Q. Liu, and P. Fung, “Enabling multimodal
    generation on CLIP via vision-language knowledge distillation,” in *Findings of
    the Association for Computational Linguistics: ACL 2022*, 2022, pp. 2383–2395.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] W. Dai, L. Hou, L. Shang, X. Jiang, Q. Liu, 和 P. Fung，“通过视觉-语言知识蒸馏实现CLIP的多模态生成，”见于*计算语言学协会会议成果：ACL
    2022*，2022年，第2383–2395页。'
- en: '[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in neural
    information processing systems*, 2017, pp. 5998–6008.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, 和 I. Polosukhin，“注意力机制就是你所需要的，”见于*神经信息处理系统进展*，2017年，第5998–6008页。'
- en: '[32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *等*，“一张图像值16x16个词：大规模图像识别的变换器，”见于*国际学习表征会议*，2020年。'
- en: '[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” 2023, *arXiv:2307.09288*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2：开放基础和微调聊天模型，”2023年，*arXiv:2307.09288*。'
- en: '[34] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training
    of deep bidirectional transformers for language understanding,” in *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, 2019,
    pp. 4171–4186.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“BERT：用于语言理解的深度双向变换器的预训练，”见于*2019年北美计算语言学协会会议：人类语言技术，第1卷（长篇和短篇论文）*，2019年，第4171–4186页。'
- en: '[35] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel,
    “Infogan: Interpretable representation learning by information maximizing generative
    adversarial nets,” in *Advances in Neural Information Processing Systems*, D. Lee,
    M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, Eds., vol. 29, 2016.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, 和 P. Abbeel，“Infogan：通过信息最大化生成对抗网络进行可解释表示学习，”见于*神经信息处理系统进展*，D.
    Lee, M. Sugiyama, U. Luxburg, I. Guyon, 和 R. Garnett 主编，第29卷，2016年。'
- en: '[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Computer Vision
    – ECCV 2014*, 2014, pp. 740–755.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    和 C. L. Zitnick，“微软COCO：上下文中的常见对象，”见于*计算机视觉 – ECCV 2014*，2014年，第740–755页。'
- en: '[37] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions:
    A cleaned, hypernymed, image alt-text dataset for automatic image captioning,”
    in *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*.   Melbourne, Australia: Association for
    Computational Linguistics, Jul. 2018, pp. 2556–2565.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] P. Sharma, N. Ding, S. Goodman, 和 R. Soricut，“概念性标题：一个清理过的、超类化的图像替代文本数据集，用于自动图像标题生成，”见于*第56届计算语言学协会年会（第1卷：长篇论文）*。
    澳大利亚墨尔本：计算语言学协会，2018年7月，第2556–2565页。'
- en: '[38] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing
    web-scale image-text pre-training to recognize long-tail visual concepts,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2021, pp. 3558–3568.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Changpinyo, P. Sharma, N. Ding, 和 R. Soricut，“Conceptual 12m：推动Web规模图像-文本预训练以识别长尾视觉概念，”见于*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，2021年6月，第3558–3568页。'
- en: '[39] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using
    1 million captioned photographs,” in *Advances in Neural Information Processing
    Systems*, vol. 24, 2011.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] V. Ordonez, G. Kulkarni, 和 T. Berg，“Im2text：使用100万张带标题的照片描述图像，”见于*神经信息处理系统进展*，第24卷，2011年。'
- en: '[40] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
    in *International Conference on Learning Representations*, 2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] I. Loshchilov 和 F. Hutter, “解耦权重衰减正则化，”收录于*国际学习表征会议*，2019年。'
- en: '[41] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *Advances in Neural Information Processing Systems*,
    Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, Eds., vol. 27,
    2014.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] I. Sutskever, O. Vinyals, 和 Q. V. Le, “基于神经网络的序列到序列学习，”收录于*神经信息处理系统进展*，Z.
    Ghahramani, M. Welling, C. Cortes, N. Lawrence, 和 K. Weinberger 主编，第27卷，2014年。'
- en: '[42] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “BART: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” in *Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics*, 2020, pp. 7871–7880.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V.
    Stoyanov, 和 L. Zettlemoyer, “BART：用于自然语言生成、翻译和理解的去噪序列到序列预训练，”收录于*第58届计算语言学协会年会*，2020年，页7871–7880。'
- en: '[43] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
    evaluation of machine translation,” in *Proceedings of the 40th Annual Meeting
    of the Association for Computational Linguistics*, 2002, pp. 311–318.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] K. Papineni, S. Roukos, T. Ward, 和 W.-J. Zhu, “Bleu：一种自动评估机器翻译的方法，”收录于*第40届计算语言学协会年会*，2002年，页311–318。'
- en: '[44] C.-Y. Lin, “ROUGE: A package for automatic evaluation of summaries,” in
    *Text Summarization Branches Out*, 2004, pp. 74–81.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] C.-Y. Lin, “ROUGE：一个用于自动评估摘要的工具包，”收录于*文本摘要拓展*，2004年，页74–81。'
- en: '[45] C.-W. Liu, R. Lowe, I. Serban, M. Noseworthy, L. Charlin, and J. Pineau,
    “How NOT to evaluate your dialogue system: An empirical study of unsupervised
    evaluation metrics for dialogue response generation,” in *Proceedings of the 2016
    Conference on Empirical Methods in Natural Language Processing*, 2016, pp. 2122–2132.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] C.-W. Liu, R. Lowe, I. Serban, M. Noseworthy, L. Charlin, 和 J. Pineau,
    “如何不评估你的对话系统：对对话响应生成的无监督评估指标的实证研究，”收录于*第2016届自然语言处理实证方法会议*，2016年，页2122–2132。'
- en: '[46] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-promoting
    objective function for neural conversation models,” in *Proceedings of the 2016
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, 2016, pp. 110–119.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Li, M. Galley, C. Brockett, J. Gao, 和 B. Dolan, “用于神经对话模型的多样性促进目标函数，”收录于*第2016届北美计算语言学协会会议：人类语言技术*，2016年，页110–119。'
- en: '[47] J. L. Fleiss and J. Cohen, “The equivalence of weighted kappa and the
    intraclass correlation coefficient as measures of reliability,” *Educational and
    psychological measurement*, vol. 33, no. 3, pp. 613–619, 1973.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. L. Fleiss 和 J. Cohen, “加权kappa与组内相关系数作为可靠性测量的等价性，”*教育与心理测量*，第33卷，第3期，页613–619，1973年。'
