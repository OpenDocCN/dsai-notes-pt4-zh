- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:59:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:59:03'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a
    Distribution Tutor for Medical Text Classification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LLM的隐私数据增强，由知识蒸馏指导的医疗文本分类中的分布导师
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.16515](https://ar5iv.labs.arxiv.org/html/2402.16515)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.16515](https://ar5iv.labs.arxiv.org/html/2402.16515)
- en: Yiping Song^(1,∗), Juhua Zhang^(2,), Zhiliang Tian^(2,),
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 宋一平^(1,∗)、张菊华^(2,)、田志亮^(2,)，
- en: Yuxin Yang², Minlie Huang³, Dongsheng Li²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 杨宇鑫²、黄敏烈³、李冬生²
- en: ¹College of Science, National University of Defense Technology, Hunan, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹国防科技大学科学学院，中国湖南
- en: ²College of Computer, National University of Defense Technology, Hunan, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²国防科技大学计算机学院，中国湖南
- en: ³The CoAI Group, DCST, BNRist, Tsinghua University, Beijing 100084, China
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³清华大学 CoAI 小组，DCST，BNRist，北京 100084，中国
- en: '{songyiping, zhangjuhua23, tianzhiliang,'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{songyiping, zhangjuhua23, tianzhiliang,'
- en: yangyuxin21a, dsli}@nudt.edu.cn, aihuang@tsinghua.edu.cn Equal Contribution.Corresponding
    Author.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: yangyuxin21a, dsli}@nudt.edu.cn, aihuang@tsinghua.edu.cn 同等贡献。通讯作者。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As sufficient data are not always publically accessible for model training,
    researchers exploit limited data with advanced learning algorithms or expand the
    dataset via data augmentation (DA). Conducting DA in private domain requires private
    protection approaches (i.e. anonymization and perturbation), but those methods
    cannot provide protection guarantees. Differential privacy (DP) learning methods
    theoretically bound the protection but are not skilled at generating pseudo text
    samples with large models. In this paper, we transfer DP-based pseudo sample generation
    task to DP-based generated samples discrimination task, where we propose a DP-based
    DA method¹¹1Code is available at [https://anonymous.4open.science/r/DP_DA-1BF7](https://anonymous.4open.science/r/DP_DA-1BF7)
    with a LLM and a DP-based discriminator for text classification on private domains.
    We construct a knowledge distillation model as the DP-based discriminator: teacher
    models, accessing private data, teaches students how to select private samples
    with calibrated noise to achieve DP. To constrain the distribution of DA’s generation,
    we propose a DP-based tutor that models the noised private distribution and controls
    samples’ generation with a low privacy cost. We theoretically analyze our model’s
    privacy protection and empirically verify our model.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 由于足够的数据并不总是公开可用以进行模型训练，研究人员利用先进的学习算法处理有限的数据或通过数据增强（DA）扩展数据集。在私有领域进行DA需要私有保护方法（即匿名化和扰动），但这些方法无法提供保护保证。差分隐私（DP）学习方法理论上界定了保护，但在生成伪文本样本方面并不擅长。本文将DP基础的伪样本生成任务转移到DP基础生成样本的鉴别任务中，我们提出了一种基于DP的DA方法¹¹1代码可在
    [https://anonymous.4open.science/r/DP_DA-1BF7](https://anonymous.4open.science/r/DP_DA-1BF7)
    找到，使用LLM和DP基础的鉴别器进行私有领域的文本分类。我们构建了一个知识蒸馏模型作为DP基础的鉴别器：教师模型，访问私有数据，教导学生如何选择带有校准噪声的私有样本以实现DP。为了限制DA生成的分布，我们提出了一种DP基础的导师，它建模了噪声化的私有分布，并以低隐私成本控制样本的生成。我们理论分析了我们模型的隐私保护，并实证验证了我们的模型。
- en: LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a
    Distribution Tutor for Medical Text Classification
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的隐私数据增强，由知识蒸馏指导的医疗文本分类中的分布导师
- en: 'Yiping Song^(1,∗), Juhua Zhang^(2,)^†^†thanks: Equal Contribution., Zhiliang
    Tian^(2,)^†^†thanks: Corresponding Author., Yuxin Yang², Minlie Huang³, Dongsheng
    Li² ¹College of Science, National University of Defense Technology, Hunan, China
    ²College of Computer, National University of Defense Technology, Hunan, China
    ³The CoAI Group, DCST, BNRist, Tsinghua University, Beijing 100084, China {songyiping,
    zhangjuhua23, tianzhiliang, yangyuxin21a, dsli}@nudt.edu.cn, aihuang@tsinghua.edu.cn'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '宋一平^(1,∗)、张菊华^(2,)^††感谢: 同等贡献。、田志亮^(2,)^††感谢: 通讯作者。、杨宇鑫²、黄敏烈³、李冬生² ¹国防科技大学科学学院，中国湖南
    ²国防科技大学计算机学院，中国湖南 ³清华大学 CoAI 小组，DCST，BNRist，北京 100084，中国 {songyiping, zhangjuhua23,
    tianzhiliang, yangyuxin21a, dsli}@nudt.edu.cn, aihuang@tsinghua.edu.cn'
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The ability of deep text classification models mainly derives from large-scale
    training data and recent large language models (LLMs) particularly benefit from
    big data. Many domains (e.g. medicine Qing et al. ([2019](#bib.bib47)); Li et al.
    ([2021](#bib.bib40))) have only limited amount of public data and large private
    data. It is risky to release private data to model training since models probably
    memorize details in those data and unintentionally output their sensitive information
    Carlini et al. ([2019a](#bib.bib12), [2021](#bib.bib14)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 深度文本分类模型的能力主要来源于大规模训练数据，最近的大型语言模型（LLMs）特别受益于大数据。许多领域（如医学，Qing等人（[2019](#bib.bib47)）；Li等人（[2021](#bib.bib40)））只有有限的公共数据和大量的私密数据。将私密数据释放到模型训练中是有风险的，因为模型可能记住这些数据中的细节并无意中输出其敏感信息，如Carlini等人（[2019a](#bib.bib12)，[2021](#bib.bib14)）。
- en: Researchers fully exploit limited public data and avoid accessing private data
    to ensure data security Fei-Fei et al. ([2006](#bib.bib24)), which achieves learning
    on small data via meta-learning Finn et al. ([2017](#bib.bib26)) or active learning
    Konyushkova et al. ([2017](#bib.bib37)). Further, some researchers expand the
    public dataset with data augmentation (DA) Wei and Zou ([2019](#bib.bib57)); another
    promising DA strategy is to synthesize private samples conditioned on privacy
    data while ensuring the sensitive information in private data are well-protected
    Yue et al. ([2022](#bib.bib61)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员充分利用有限的公共数据，并避免访问私密数据以确保数据安全，如Fei-Fei等人（[2006](#bib.bib24)），这通过元学习（meta-learning）如Finn等人（[2017](#bib.bib26)）或主动学习（active
    learning）如Konyushkova等人（[2017](#bib.bib37)）实现了对小数据的学习。此外，一些研究人员通过数据增强（DA）扩展公共数据集，如Wei和Zou（[2019](#bib.bib57)）；另一种有前景的DA策略是合成私密样本，条件是隐私数据，同时确保私密数据中的敏感信息得到良好保护，如Yue等人（[2022](#bib.bib61)）。
- en: Synthesizing private text requires to capture original private data distributions
    by accessing the data. Accessing them should be under the guarantee of privacy
    protections and thus we need privacy protection on text generation. Researchers
    attempt anonymization methods to mask selected sensitive text span Carlini et al.
    ([2019b](#bib.bib13)); Shi et al. ([2021](#bib.bib50)), randomly noise the input
    tokens Feyisetan et al. ([2020](#bib.bib25)), or apply regularization to avoid
    overfitting training data Carlini et al. ([2019b](#bib.bib13)). However, the masking,
    nosing, or regularization mechanism hardly ensures (almost) all personal information
    is well-protected. Differential privacy (DP) Dwork et al. ([2006](#bib.bib21))
    provides provable guarantees against the identification of individual information
    in datasets. Deep generative models with DP ensure the existence of a specific
    sample (with private information) cannot be detected Li et al. ([2022](#bib.bib41)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 合成私密文本需要通过访问数据来捕捉原始私密数据分布。访问这些数据时必须确保隐私保护，因此我们需要在文本生成中实施隐私保护。研究人员尝试了不同的匿名化方法来掩盖选定的敏感文本片段，如Carlini等人（[2019b](#bib.bib13)）;
    Shi等人（[2021](#bib.bib50)），随机添加噪声到输入标记，如Feyisetan等人（[2020](#bib.bib25)），或应用正则化以避免过拟合训练数据，如Carlini等人（[2019b](#bib.bib13)）。然而，掩盖、噪声添加或正则化机制很难确保（几乎）所有个人信息都得到良好保护。差分隐私（DP）Dwork等人（[2006](#bib.bib21)）提供了对数据集中个人信息识别的可证明保障。具有DP的深度生成模型确保无法检测到特定样本（含私密信息），如Li等人（[2022](#bib.bib41)）。
- en: Noisy-SGD  (Song et al., [2013](#bib.bib51); Abadi et al., [2016](#bib.bib1))
    is a practical DP algorithm for deep learning models, including text generation
    models Kerrigan et al. ([2020](#bib.bib34)), which adds calibrated noise on model
    gradients to satisfy DP. However, the advantage of NoisySGD is weaken as the model
    becomes larger (Bassily et al., [2014](#bib.bib6); Yu et al., [2020](#bib.bib60))
    and NoisySGD requires a per-example gradient clip resulting in non-convergence
    and system overheads (Zhu et al., [2020](#bib.bib63); Bu et al., [2021](#bib.bib10)).
    Another categories of DP learning methods, PATE (Papernot et al., [2017](#bib.bib46)),
    acquires private information from teacher models learned on private data to a
    student and adds noises on teachers’ outputs. PATE’s privacy cost comes from teachers’
    outputs instead of the whole model’s gradients in Noisy-SGD (Li et al., [2022](#bib.bib41)).
    Hence, PATE’s required noise does not scale with model size and PATE is promising
    of working on large models (e.g. BERT Devlin et al. ([2019](#bib.bib18)), Llama
     Touvron et al. ([2023](#bib.bib55)), and GPT-4  Achiam et al. ([2023](#bib.bib2))).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Noisy-SGD（Song et al., [2013](#bib.bib51); Abadi et al., [2016](#bib.bib1)）是一个用于深度学习模型（包括文本生成模型）的实用DP算法（Kerrigan
    et al., [2020](#bib.bib34)），它在模型梯度上添加了校准噪声以满足DP。然而，随着模型的增大，Noisy-SGD的优势减弱（Bassily
    et al., [2014](#bib.bib6); Yu et al., [2020](#bib.bib60)），并且Noisy-SGD需要逐例梯度剪裁，导致不收敛和系统开销（Zhu
    et al., [2020](#bib.bib63); Bu et al., [2021](#bib.bib10)）。另一类DP学习方法，PATE（Papernot
    et al., [2017](#bib.bib46)），通过从在私有数据上学习的教师模型获取私有信息到学生，并在教师的输出上添加噪声。PATE的隐私成本来自教师的输出，而不是Noisy-SGD中的整个模型梯度（Li
    et al., [2022](#bib.bib41)）。因此，PATE所需的噪声不会随着模型大小的增加而扩展，PATE有望在大模型（如BERT Devlin
    et al. ([2019](#bib.bib18))，Llama Touvron et al. ([2023](#bib.bib55))和GPT-4 Achiam
    et al. ([2023](#bib.bib2)))上有效。
- en: DP with PATE on text generation still suffers from a sequential multiple tasks
    with large candidate space, which extremely (linearly) increases the noise scale
    Tian et al. ([2022](#bib.bib54)). We argue that we can transfer a DP-based generation
    task to a DP-based discrimination task to avoid complexities in DP text generation
    models. Particularly, we employ LLMs’ generation ability $P_{LLM}(x)$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DP与PATE在文本生成中的应用仍面临着大候选空间的多任务顺序问题，这极大地（线性地）增加了噪声规模（Tian et al. ([2022](#bib.bib54))）。我们认为可以将DP基础的生成任务转移到DP基础的判别任务，以避免DP文本生成模型中的复杂性。特别地，我们利用LLMs的生成能力
    $P_{LLM}(x)$。
- en: In this paper, we propose a DP-based data augmentation (DA) paradigm with a
    LLM and a DP-based discriminator to generate samples for private text classification,
    where the discriminator selects LLM-generated samples likely to belong to private
    domain as our synthesized samples. Specifically, the discriminator achieves DP
    via knowledge distillation (KD), where multiple teacher models access disjoint
    and unique private sets to learn private discriminators; a student learns from
    noised aggregated teacher outputs to achieve DP. To control the distribution in
    DA’s generated samples, we propose a DP-based distribution tutor that captures
    the distribution of private data. In DP, querying privacy is expensive (student
    querying teacher causes a certain privacy cost), the tutor carries less sensitive
    information than teachers and helps the student with a low privacy cost (See §[4](#S4
    "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided
    by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")).
    We further provide theoretical analyses and empirical results to verify our methods.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于DP的数据增强（DA）范式，结合LLM和DP基础的判别器来生成私有文本分类的样本，其中判别器选择可能属于私有领域的LLM生成样本作为我们合成的样本。具体而言，判别器通过知识蒸馏（KD）实现DP，其中多个教师模型访问不重叠且唯一的私有集合以学习私有判别器；学生从带噪声的聚合教师输出中学习以实现DP。为了控制DA生成样本的分布，我们提出了一个基于DP的分布导师，捕捉私有数据的分布。在DP中，查询隐私是昂贵的（学生查询教师会产生一定的隐私成本），导师携带的信息比教师少，并以较低的隐私成本帮助学生（见
    §[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")）。我们进一步提供了理论分析和实证结果来验证我们的方法。
- en: 'Our contributions are as follows: (1) We construct a DP-based DA with LLMs
    that synthetizes (almost infinite) samples while bounding the privacy leakage.
    (2) We propose a DP-based tutor for teacher-student frameworks to teach some less
    sensitive data with a low privacy cost. (3) We excel strong DP-based baselines
    on text classification in private domains.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献如下：（1）我们构建了一种基于DP的DA与LLMs，该方法可以合成（几乎无限的）样本，同时控制隐私泄露。（2）我们提出了一种基于DP的教师-学生框架的辅导方法，以低隐私成本教授一些较少敏感的数据。（3）我们在私有领域的文本分类任务中超越了强大的基于DP的基准。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Privacy Protection in Text Classification
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 文本分类中的隐私保护
- en: Initial privacy protection techniques predominantly focused on data anonymization
    Maeda et al. ([2016](#bib.bib44)); Suzuki et al. ([2018](#bib.bib52)), For example,
    de-identification techniques Garfinkel et al. ([2015](#bib.bib27)) such as removing,
    replacing, or encrypting sensitive information in data can reduce the risk of
    privacy leaks. Data perturbation techniques Johnson and Shmatikov ([2013](#bib.bib32))
    protect user privacy by incorporating random noise into the data. Nevertheless,
    straightforward data anonymization measures may difficult to effectively deal
    with privacy leakage challenges Rocher et al. ([2019](#bib.bib48)). Presently,
    federated learning McMahan et al. ([2017](#bib.bib45)); Deng et al. ([2022](#bib.bib17))
    and DP Dwork et al. ([2006](#bib.bib21)) emerge as the two principal methodologies
    in the domain of privacy protection. Federated learning strategies can prevent
    privacy leaks caused by untrustworthy servers. DP aims to prevent attackers from
    extracting sensitive information from the training dataset Carlini et al. ([2021](#bib.bib14)),
    offering a quantifiable privacy protection mechanism. With its robust theoretical
    foundation and broad applicabilityDwork et al. ([2014a](#bib.bib22)), differential
    privacy is widely acknowledged as the standard practice in the field of privacy
    protection. Independently and concurrently with our work, Wu et al. ([2023](#bib.bib58))
    and Duan et al. ([2023](#bib.bib20)) studied ICL with DP guarantees for text classification
    tasks. Our proposed method predominantly embraces the privacy protection principles
    of differential privacy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 初期的隐私保护技术主要集中在数据匿名化 Maeda 等（[2016](#bib.bib44)）；Suzuki 等（[2018](#bib.bib52)），例如，去标识化技术
    Garfinkel 等（[2015](#bib.bib27)），如删除、替换或加密数据中的敏感信息，可以降低隐私泄露的风险。数据扰动技术 Johnson 和
    Shmatikov（[2013](#bib.bib32)）通过在数据中加入随机噪声来保护用户隐私。然而，直接的数据匿名化措施可能难以有效应对隐私泄露挑战 Rocher
    等（[2019](#bib.bib48)）。目前，联邦学习 McMahan 等（[2017](#bib.bib45)）；Deng 等（[2022](#bib.bib17)）和DP
    Dwork 等（[2006](#bib.bib21)）成为隐私保护领域的两种主要方法。联邦学习策略可以防止由于不可信服务器引发的隐私泄露。DP旨在防止攻击者从训练数据集中提取敏感信息
    Carlini 等（[2021](#bib.bib14)），提供可量化的隐私保护机制。凭借其坚实的理论基础和广泛的适用性 Dwork 等（[2014a](#bib.bib22)），差分隐私被广泛认可为隐私保护领域的标准实践。与我们的工作独立而同步，Wu
    等（[2023](#bib.bib58)）和 Duan 等（[2023](#bib.bib20)）研究了具有DP保证的ICL用于文本分类任务。我们提出的方法主要遵循差分隐私的隐私保护原则。
- en: 2.2 Data Augmentation (DA) in Text Classification
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 文本分类中的数据增强（DA）
- en: Various NLP data augmentation (DA) techniques have been developed, such as Back-Translation
    Kobayashi ([2018](#bib.bib36)), EDA Wei and Zou ([2019](#bib.bib57)), and AEDA
    Karimi et al. ([2021](#bib.bib33)). These methods primarily focus on modifying
    the original input, which limits the diversity of the generated samples. In response,
    Szegedy et al. ([2016](#bib.bib53)) initially explore a interpolation-based methods
    (i.e., mixup) in computer vision. Subsequently, Guo et al. ([2019](#bib.bib28))
    combine the mixup technique with CNNs and LSTMs for text applications.There are
    also many studies that choose different strategies to improve the mixup technique
    Chen et al. ([2020](#bib.bib16)); Zhang et al. ([2020](#bib.bib62)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 已开发出各种NLP数据增强（DA）技术，如反向翻译 Kobayashi（[2018](#bib.bib36)），EDA Wei 和 Zou（[2019](#bib.bib57)），以及AEDA
    Karimi 等（[2021](#bib.bib33)）。这些方法主要集中在修改原始输入，这限制了生成样本的多样性。作为回应，Szegedy 等（[2016](#bib.bib53)）最初探索了基于插值的方法（即
    mixup）在计算机视觉中的应用。随后，Guo 等（[2019](#bib.bib28)）将 mixup 技术与 CNNs 和 LSTMs 结合用于文本应用。还有许多研究选择不同的策略来改进
    mixup 技术 Chen 等（[2020](#bib.bib16)）；Zhang 等（[2020](#bib.bib62)）。
- en: Moreover, some researchers use pre-trained language models (PLMs) for data for
    data augmentation. Kumar et al. ([2020](#bib.bib38)) provide a straightforward
    and effective method for conditional PLM by prepending class labels to text sequences.
    Hu et al. ([2019](#bib.bib30)) utilize reinforcement learning with a conditional
    language model that performs by appending the correct label to the input sequence
    during training. Further, an increasing number of scholars have started to utilize
    adversarial learning methods to generate augmented samples, such as BERT-Attack
    Li et al. ([2020](#bib.bib39)), G-DAUG^C Yang et al. ([2020](#bib.bib59)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些研究者使用预训练语言模型（PLMs）进行数据增强。库马尔等人（[2020](#bib.bib38)）通过将类别标签添加到文本序列前面，提供了一种直接有效的条件PLM方法。胡等人（[2019](#bib.bib30)）利用强化学习与条件语言模型，在训练期间通过将正确的标签附加到输入序列来进行操作。此外，越来越多的学者开始利用对抗学习方法生成增强样本，例如BERT-Attack李等人（[2020](#bib.bib39)）、G-DAUG^C杨等人（[2020](#bib.bib59)）。
- en: To reduce the negative impact of low-quality augmentation samples on model performance,
    some research focus on sample selection. For example, Cao et al. ([2021](#bib.bib11))
    propose UAST framework to quantify model uncertainty for selecting pseudo-labeled
    samples. Lin et al. ([2023](#bib.bib42)) focus more on the combination of sample
    selection and data enhancement strategies, and introduce a self-training selection
    framework to select high-quality samples from the data augmentation. Different
    from the above methods, we aim to achieve DA to synthetic private data while ensuring
    the private information from the private dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少低质量增强样本对模型性能的负面影响，一些研究集中在样本选择上。例如，曹等人（[2021](#bib.bib11)）提出了UAST框架来量化模型不确定性，以选择伪标签样本。林等人（[2023](#bib.bib42)）则更多地关注样本选择与数据增强策略的结合，并引入了自训练选择框架，从数据增强中选择高质量样本。与上述方法不同，我们的目标是在确保私有数据集的私密信息的同时，实现对合成私有数据的增强（DA）。
- en: 2.3 DP for Deep Learning Models
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 深度学习模型中的差分隐私（DP）
- en: Some researchers employ DP to protect the privacy of empirical risk minimization
    classifiers Chaudhuri et al. ([2011](#bib.bib15)) and SVM Rubinstein et al. ([2009](#bib.bib49)).
    Following Song et al. ([2013](#bib.bib51)), NoisySGD introduces noise into gradients
    to achieve DP for deep learning models, including DP-SGD Song et al. ([2013](#bib.bib51));
    Bassily et al. ([2014](#bib.bib6)); Bu et al. ([2023](#bib.bib9)) and DP-Adam
    Abadi et al. ([2016](#bib.bib1)); Kingma and Ba ([2014](#bib.bib35)). The use
    of DP-SGD for large-scale pre-training of BERT has been shown to achieve comparable
    masked language modeling performance to non-private BERT Anil et al. ([2021](#bib.bib4)),
    but with a privacy budget is 100 or higher. Recent studies have demonstrated that
    even under more stringent privacy constraints, generative and discriminative language
    models can achieve high performance across various tasks by appropriately selecting
    hyperparameters and fine-tuning objectives aligned with the pre-training process
    Li et al. ([2022](#bib.bib41)). Additionally, Li et al. ([2022](#bib.bib41)) apply
    ghost clipping to pre-trained language models using NoisySGD, reducing memory
    usage. He et al. ([2022](#bib.bib29)) leverage group clipping with adaptive clipping
    thresholds, privately fine-tuning GPT-3 with 1.75 trillion parameters. PATE (Papernot
    et al., [2017](#bib.bib46)) is another type of DP learning algorithm, transferring
    knowledge from teacher models trained on private sets with noises to a student
    model. The privacy cost of PATE arises from knowledge distillation rather than
    the gradient of the entire model. With this advantage, PATE has enormous potential
    in adapting to large models. Moreover, PATE is designed for classification tasks
    and is suitable for our goal of training a DP-based discriminator.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员采用DP来保护经验风险最小化分类器的隐私，如Chaudhuri等人（[2011](#bib.bib15)）和SVM Rubinstein等人（[2009](#bib.bib49)）。跟随Song等人（[2013](#bib.bib51)），NoisySGD将噪声引入梯度中，以实现深度学习模型的DP，包括DP-SGD
    Song等人（[2013](#bib.bib51)）；Bassily等人（[2014](#bib.bib6)）；Bu等人（[2023](#bib.bib9)）和DP-Adam
    Abadi等人（[2016](#bib.bib1)）；Kingma和Ba（[2014](#bib.bib35)）。使用DP-SGD进行大规模BERT预训练已被证明在遮罩语言建模性能上可以与非私有BERT
    Anil等人（[2021](#bib.bib4)）相媲美，但隐私预算为100或更高。最近的研究表明，即使在更严格的隐私约束下，通过适当选择超参数和微调目标来与预训练过程对齐，生成和判别语言模型仍能在各种任务中实现高性能
    Li等人（[2022](#bib.bib41)）。此外，Li等人（[2022](#bib.bib41)）使用NoisySGD对预训练语言模型进行幽灵剪裁，减少了内存使用。He等人（[2022](#bib.bib29)）利用具有自适应剪裁阈值的组剪裁，私密微调具有1.75万亿参数的GPT-3。PATE（Papernot等人，[2017](#bib.bib46)）是另一种类型的DP学习算法，将在带噪声的私有集合上训练的教师模型的知识转移到学生模型中。PATE的隐私成本来自于知识蒸馏，而非整个模型的梯度。凭借这一优势，PATE在适应大模型方面具有巨大潜力。此外，PATE被设计用于分类任务，适合我们训练基于DP的判别器的目标。
- en: '![Refer to caption](img/98cb3c58dba94d66b7d176fde32d2723.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98cb3c58dba94d66b7d176fde32d2723.png)'
- en: 'Figure 1: Overview of our framework. It mainly contains three components, LLM-based
    Public Generator (grey block) generates public data. DP-based Discriminator with
    Knowledge Distillation (pink block) discriminates public data and obtains a probability
    similar to private data. The Label Distribution Tutor (blue block) selects a subset
    with the highest probabilities of samples matching the noise label distribution.
    The gray dotted box is the privacy block.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们框架的概览。它主要包含三个组件，基于LLM的公共生成器（灰色块）生成公共数据。基于DP的带有知识蒸馏的判别器（粉色块）区分公共数据并获得类似于私有数据的概率。标签分布导师（蓝色块）选择概率最高的与噪声标签分布匹配的样本子集。灰色虚线框是隐私块。
- en: 3 Methods
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Overview
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: Our model consists of four parts (Fig. [1](#S2.F1 "Figure 1 ‣ 2.3 DP for Deep
    Learning Models ‣ 2 Related Work ‣ LLM-based Privacy Data Augmentation Guided
    by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型由四个部分组成（图[1](#S2.F1 "图 1 ‣ 2.3 深度学习模型中的DP ‣ 2 相关工作 ‣ 基于LLM的隐私数据增强，辅以知识蒸馏和分布导师，用于医疗文本分类")）。
- en: LLM-based Public Generator
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于LLM的公共生成器
- en: (§[3.3](#S3.SS3 "3.3 LLM-based Public Generator ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")) synthetic public textual input for the specific
    label on the text classification task.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: （§[3.3](#S3.SS3 "3.3 基于LLM的公共生成器 ‣ 3 方法 ‣ 基于LLM的隐私数据增强，辅以知识蒸馏和分布导师，用于医疗文本分类")）合成用于特定标签的公共文本输入，用于文本分类任务。
- en: DP-based Discriminator with Knowledge Distillation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于DP的带有知识蒸馏的判别器
- en: (§[3.4](#S3.SS4 "3.4 DP-based Discriminator with Knowledge Distillation ‣ 3
    Methods ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification")) learns to discriminate
    whether a sample is (likely) from public domain or private domain, which satisfy
    DP privacy guarantee. It filters the samples from the generator (§[3.3](#S3.SS3
    "3.3 LLM-based Public Generator ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification"))
    to obtain new samples for privacy domain.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: (§[3.4](#S3.SS4 "3.4 基于 DP 的判别器与知识蒸馏 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类"))学习区分样本是否（可能）来自公共领域或私有领域，从而满足
    DP 隐私保证。它过滤生成器（§[3.3](#S3.SS3 "3.3 基于 LLM 的公共生成器 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类")）的样本，以获得新的隐私领域样本。
- en: Label Distribution Tutor
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标签分布导师
- en: (§[3.5](#S3.SS5 "3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")) leads the generated pseudo samples to follow the
    distribution of private data under DP guarantee, which also filter the generator
    §[3.3](#S3.SS3 "3.3 LLM-based Public Generator ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")’s output.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (§[3.5](#S3.SS5 "3.5 标签分布导师 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类"))引导生成的伪样本遵循在
    DP 保证下的私有数据分布，同时过滤生成器 §[3.3](#S3.SS3 "3.3 基于 LLM 的公共生成器 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类")
    的输出。
- en: Private Data Augmentation
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 私有数据增强
- en: (§[3.6](#S3.SS6 "3.6 Private Data Augmentation ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")) uses the generator to obtain candidate samples
    and uses the discriminator (§[3.4](#S3.SS4 "3.4 DP-based Discriminator with Knowledge
    Distillation ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation Guided by Knowledge
    Distillation with a Distribution Tutor for Medical Text Classification")) and
    the tutor (§[3.5](#S3.SS5 "3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification")) to filter the candidates to get data
    augmentation samples.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (§[3.6](#S3.SS6 "3.6 私有数据增强 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类"))使用生成器获得候选样本，并使用判别器（§[3.4](#S3.SS4
    "3.4 基于 DP 的判别器与知识蒸馏 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类")）和导师（§[3.5](#S3.SS5
    "3.5 标签分布导师 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏和分布导师指导的医学文本分类")）来筛选候选样本以获得数据增强样本。
- en: 3.2 Task Definition
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 任务定义
- en: Given a private text classification corpora with sensitive information, our
    task aims to train a text classifier with a certain level of theoretical privacy
    guarantee, which protects all training samples with its label  from being detected (protecting
    the existence of specific training sample from being identified by any detectors
    via any detecting methods).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含敏感信息的私有文本分类语料库，我们的任务是训练一个具有一定理论隐私保证的文本分类器，该隐私保证保护所有训练样本及其标签 不被检测（保护特定训练样本的存在不被任何检测器通过任何检测方法识别）。
- en: Note that our task allows the model using data augmentation (DA) method to generate
    pseudo private samples for training. However, the data augmentation model also
    ought to ensure the privacy guarantee mentioned above.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们的任务允许模型使用数据增强（DA）方法生成伪私有样本用于训练。然而，数据增强模型也必须确保上述的隐私保证。
- en: 3.3 LLM-based Public Generator
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于 LLM 的公共生成器
- en: 'We employ a LLM (i.e. GPT-3.5) to generate input texts for each output label
    (on classification task). Even if the GPT-3.5 is trained on the public domain,
    we design a prompt text to induce the LLM to attempt to generate private samples,
    where the prompt follows this template: “You are a professional medical transcriber.
    Please generate a medical transcription for [LABEL] and do not reveal the patient’s
    name. The text length of medical transcription is approximately 400 words and
    at least 200 words.” In the above template, “[LABEL]” denotes the label $c$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个大型语言模型（即 GPT-3.5）来生成每个输出标签（在分类任务中）的输入文本。即使 GPT-3.5 是在公共领域训练的，我们设计了一个提示文本来引导模型尝试生成私有样本，其中提示遵循以下模板：“你是一个专业的医学转录员。请为[LABEL]生成医学转录，并且不要透露病人的姓名。医学转录的文本长度大约为400字，至少为200字。”在上述模板中，“[LABEL]”表示标签
    $c$。
- en: 3.4 DP-based Discriminator with Knowledge Distillation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 基于差分隐私的判别器与知识蒸馏
- en: We propose a DP-based discriminator to check if the pseudo samples fit for the
    private distribution thus are capable of acting as private samples. Inspired by
    Papernot et al. ([2017](#bib.bib46)), we construct a teacher-student framework
    with knowledge distillation. This model consists of multiple teacher models and
    a student model. Teachers are allowed to access the private data and the student
    can only access the noised teachers’ outputs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于差分隐私的判别器，用于检查伪样本是否适合私有分布，从而能够作为私有样本。受到 Papernot 等人 ([2017](#bib.bib46))
    的启发，我们构建了一个教师-学生框架并进行了知识蒸馏。该模型由多个教师模型和一个学生模型组成。教师可以访问私有数据，而学生只能访问经过噪声处理的教师输出。
- en: Teacher Models.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 教师模型。
- en: We train multiple teacher models on multiple disjoint datasets, where the private
    samples act as the positive sample and the public samples (generated by §[3.3](#S3.SS3
    "3.3 LLM-based Public Generator ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification"))
    as the negative sample. The teachers learn to judge whether the samples from private
    set or public set. All teachers follow the structure and the initial parameters
    of a pre-trained model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在多个不相交的数据集上训练多个教师模型，其中私有样本作为正样本，公共样本（由§[3.3](#S3.SS3 "3.3 LLM-based Public
    Generator ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation Guided by Knowledge
    Distillation with a Distribution Tutor for Medical Text Classification") 生成）作为负样本。教师学习判断样本来自私有集还是公共集。所有教师都遵循预训练模型的结构和初始参数。
- en: To satisfy DP, we carefully process the teachers’ data and train teachers with
    two strategies. First, the private training data need to contain only unique samples
    (private sample duplicates should be removed). The reason is that DP prevents
    the unknown detectors from identification on each occurrence Dwork et al. ([2006](#bib.bib21)).
    Duplicated samples with $N$ times s.t. advanced DP Li et al. ([2022](#bib.bib41)))
    which extremely harms for the performance.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足差分隐私要求，我们仔细处理教师的数据，并用两种策略训练教师。首先，私有训练数据需要只包含唯一样本（应删除私有样本的重复项）。原因是差分隐私防止未知检测器识别每次出现的样本
    Dwork 等人 ([2006](#bib.bib21))。重复的样本数量 $N$，例如先进的差分隐私 Li 等人 ([2022](#bib.bib41))，极大地损害了性能。
- en: Second, we equally divide the shuffled private data into $M$ teachers and train
    each teacher on each set separately. In this way, the existence of any specific
    sample affects only one teacher’s output, which bounds the sensitivity of the
    teachers’ noisy distribution Papernot et al. ([2017](#bib.bib46)); Boenisch et al.
    ([2023](#bib.bib7)) (See detailed analyses in §[4](#S4 "4 Privacy Analyses of
    our Method ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification")). The equally divide
    (in terms of the sample number) and shuffled assignment ensure the balance among
    all teachers and the training performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们将打乱的私有数据均匀分配到 $M$ 个教师中，并分别在每个集合上训练每个教师。这样，任何特定样本的存在只影响一个教师的输出，从而限制了教师噪声分布的敏感性
    Papernot 等人 ([2017](#bib.bib46)); Boenisch 等人 ([2023](#bib.bib7)) （详见§[4](#S4
    "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided
    by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")
    中的详细分析）。均匀分配（以样本数量为准）和打乱分配确保了所有教师之间的平衡以及训练性能。
- en: Student Model.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 学生模型。
- en: 'The student model is a discriminator cannot access raw private data and learn
    from teachers with those steps: (1) Merging. For a given sample $x$.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 学生模型是一个判别器，不能访问原始私有数据，而是通过以下步骤向教师学习：(1) 合并。对于给定的样本 $x$。
- en: '|  | $\displaystyle\hat{y}$ |  | (1) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{y}$ |  | (1) |'
- en: '|  |  | $\displaystyle=\arg\max_{y\in\mathcal{Y}}(\sum^{M}_{m}P_{m}(\mathcal{Y}&#124;x)+\mathcal{N}(0,\sigma_{KD}^{2}))$
    |  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\arg\max_{y\in\mathcal{Y}}(\sum^{M}_{m}P_{m}(\mathcal{Y}&#124;x)+\mathcal{N}(0,\sigma_{KD}^{2}))$
    |  |'
- en: ', where the first term is the actual teacher aggregated outputs and $M$, the
    teaching follows Cross-Entropy loss as Eq. [2](#S3.E2 "In Student Model. ‣ 3.4
    DP-based Discriminator with Knowledge Distillation ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中第一项是实际教师的聚合输出，$M$，教学遵循交叉熵损失，如公式 [2](#S3.E2 "在学生模型中 ‣ 3.4 基于 DP 的鉴别器与知识蒸馏
    ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")。
- en: '|  | $\displaystyle\mathcal{L}_{KD}$ |  | (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{KD}$ |  | (2) |'
- en: '|  |  | $\displaystyle=-\sum_{m}^{M}\mathbbm{I}(c_{m}=\hat{c})\log P_{student}(c_{m}&#124;x)$
    |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\sum_{m}^{M}\mathbbm{I}(c_{m}=\hat{c})\log P_{student}(c_{m}&#124;x)$
    |  |'
- en: ', where $\mathbbm{I}$ is a indicator function. The above mechanism ensures
    the student satisfies DP as Papernot et al. ([2017](#bib.bib46)) (Analyses in
    §[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ，其中 $\mathbbm{I}$ 是一个指示函数。上述机制确保学生满足 DP，如 Papernot 等人（[2017](#bib.bib46)）所分析（分析见
    §[4](#S4 "4 我们方法的隐私分析 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")）。
- en: 3.5 Label Distribution Tutor
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 标签分布辅导器
- en: We propose a tutor based on the above teacher-student framework (§[3.4](#S3.SS4
    "3.4 DP-based Discriminator with Knowledge Distillation ‣ 3 Methods ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification")), which avoids directly access the pure
    private samples but accesses only a small amount of privacy information so as
    to maintain the bound of protections. The tutor aims to carry the label distribution
    of the private samples, as the label distribution is critical in generating augmented
    data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种基于上述师生框架的辅导器（§[3.4](#S3.SS4 "3.4 基于 DP 的鉴别器与知识蒸馏 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")），该辅导器避免直接访问纯私有样本，而仅访问少量隐私信息，以保持保护的界限。辅导器旨在携带私有样本的标签分布，因为标签分布在生成增强数据时至关重要。
- en: The tutor follows a statistic way to collect the noised label distribution of
    all private samples $P_{label}(\mathcal{C})$) and the first term dominates Eq. [3](#S3.E3
    "In 3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")
    as Tian et al. ([2022](#bib.bib54)).. The above mechanism ensures the tutor satisfy
    DP (Analyses in §[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 辅导器采用统计方法收集所有私有样本的噪声标签分布 $P_{label}(\mathcal{C})$，其中第一项主导了公式 [3](#S3.E3 "在 3.5
    标签分布辅导器 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")，如 Tian 等人（[2022](#bib.bib54)）所述。上述机制确保辅导器满足
    DP（分析见 §[4](#S4 "4 我们方法的隐私分析 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")）。
- en: '|  | $1$2 |  | (3) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 3.6 Private Data Augmentation
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 私有数据增强
- en: 'We propose a novel data augmentation (DA) framework to generate samples for
    private domain while protecting the privacy. The framework involves: (1) a public
    LLM-based generator to obtain a plenty of candidate samples (§[3.4](#S3.SS4 "3.4
    DP-based Discriminator with Knowledge Distillation ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")). (2) a DP-based discriminator to select LLM’s generated
    samples similar to private samples, and (3) a tutor to filter generated samples
    to ensure the label distributions fitting for the private data.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的数据增强（DA）框架，用于在保护隐私的同时生成私有领域的样本。该框架包括：（1）一个基于公共 LLM 的生成器，以获取大量候选样本（§[3.4](#S3.SS4
    "3.4 基于 DP 的鉴别器与知识蒸馏 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强通过知识蒸馏与分布辅导器进行医学文本分类")）。 （2）一个基于 DP
    的鉴别器，用于选择与私有样本相似的 LLM 生成的样本，以及（3）一个辅导器，用于过滤生成的样本，以确保标签分布适合私有数据。
- en: The idea is (1) taking advantage of strong generation ability of LLMs to obtain
    high quality data. (2) accessing private data causes privacy cost but accessing
    privacy through the student (i.e. discriminator) and tutor satisfying DP would
    not brings in additional loss. Hence, we can “infinitely” call LLMs, student,
    and tutor to achieve DA while bounding the privacy protection.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是 (1) 利用 LLM 的强大生成能力来获得高质量数据。(2) 访问私人数据会带来隐私成本，但通过学生（即鉴别器）和满足 DP 的导师访问隐私不会带来额外损失。因此，我们可以“无限”地调用
    LLM、学生和导师来实现 DA，同时确保隐私保护。
- en: 4 Privacy Analyses of our Method
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 我们方法的隐私分析
- en: Lemma 4 Analytical Gaussian mechanism.
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 引理 4 分析高斯机制。
- en: (Balle and Wang, [2018](#bib.bib5)) For a query $h:\mathcal{X}^{n}\rightarrow\mathcal{Y}^{d}$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (Balle 和 Wang，[2018](#bib.bib5)) 对于一个查询 $h:\mathcal{X}^{n}\rightarrow\mathcal{Y}^{d}$。
- en: '| Method | 3750 training samples | 6000 training samples |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 3750 个训练样本 | 6000 个训练样本 |'
- en: '| P | R | F1 | Acc | P | R | F1 | Acc |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| P | R | F1 | Acc | P | R | F1 | Acc |'
- en: '| Non-DP | Private | 0.224 | 0.367 | 0.240 | 0.367 | - | - | - | - |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 非DP | 私有 | 0.224 | 0.367 | 0.240 | 0.367 | - | - | - | - |'
- en: '| DA w/ Public | 0.280 | 0.347 | 0.280 | 0.348 | 0.313 | 0.347 | 0.287 | 0.348
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| DA w/ 公共 | 0.280 | 0.347 | 0.280 | 0.348 | 0.313 | 0.347 | 0.287 | 0.348
    |'
- en: '| DP($\varepsilon=4$) | DP-SGD | 0.130 | 0.310 | 0.170 | 0.314 | - | - | -
    | - |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| DP($\varepsilon=4$) | DP-SGD | 0.130 | 0.310 | 0.170 | 0.314 | - | - | -
    | - |'
- en: '| Ghost | 0.143 | 0.289 | 0.166 | 0.291 | - | - | - | - |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Ghost | 0.143 | 0.289 | 0.166 | 0.291 | - | - | - | - |'
- en: '| DA w/ DP($\varepsilon=4$) | DA w/ DP-SGD | 0.290 | 0.350 | 0.277 | 0.352
    | 0.283 | 0.350 | 0.280 | 0.352 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| DA w/ DP($\varepsilon=4$) | DA w/ DP-SGD | 0.290 | 0.350 | 0.277 | 0.352
    | 0.283 | 0.350 | 0.280 | 0.352 |'
- en: '| DA w/ Ghost | 0.303 | 0.347 | 0.297 | 0.344 | 0.283 | 0.350 | 0.297 | 0.350
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| DA w/ Ghost | 0.303 | 0.347 | 0.297 | 0.344 | 0.283 | 0.350 | 0.297 | 0.350
    |'
- en: '| Ours | 0.340 | 0.373 | 0.337 | 0.372 | 0.353 | 0.377 | 0.340 | 0.376 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0.340 | 0.373 | 0.337 | 0.372 | 0.353 | 0.377 | 0.340 | 0.376 |'
- en: 'Table 1: Main results comparing all the baselines on two size of datasets on
    text classification tasks.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在文本分类任务中比较所有基线模型在两种数据集上的主要结果。
- en: Sensitivity Analysis of Knowledge Distillation (KD).
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 知识蒸馏（KD）的敏感度分析。
- en: We denote the output distribution of $m$ in Lemma [4](#S4.SS0.SSS0.Px1 "Lemma
    4 Analytical Gaussian mechanism. ‣ 4 Privacy Analyses of our Method ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification") is (See deductions in App. [A](#A1 "Appendix
    A Deduction of Sensitivity Δ_{𝐾⁢𝐷} ‣ LLM-based Privacy Data Augmentation Guided
    by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")),
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表示引理 [4](#S4.SS0.SSS0.Px1 "引理 4 分析高斯机制 ‣ 4 我们方法的隐私分析 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏的分布导师用于医疗文本分类")
    中的输出分布是（见附录 [A](#A1 "附录 A 敏感度 Δ_{𝐾⁢𝐷} ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏的分布导师用于医疗文本分类") 的推导），
- en: '|  | $\displaystyle\Delta_{KD}$ |  | (4) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{KD}$ |  | (4) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}\leq\sqrt{2}.$
    |  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}\leq\sqrt{2}.$
    |  |'
- en: Sensitivity Analysis of Tutor.
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 导师的敏感度分析。
- en: In Eq. [3](#S3.E3 "In 3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification"), each sample $j$ in Lemma [4](#S4.SS0.SSS0.Px1 "Lemma
    4 Analytical Gaussian mechanism. ‣ 4 Privacy Analyses of our Method ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification") is (See deductions in App. [B](#A2 "Appendix
    B Deduction of Sensitivity Δ_{𝑡⁢𝑢⁢𝑡⁢𝑜⁢𝑟} ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")),
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在公式 [3](#S3.E3 "在 3.5 标签分布导师 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏的分布导师用于医疗文本分类") 中，每个样本
    $j$ 在引理 [4](#S4.SS0.SSS0.Px1 "引理 4 分析高斯机制 ‣ 4 我们方法的隐私分析 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏的分布导师用于医疗文本分类")
    中是（见附录 [B](#A2 "附录 B 敏感度 Δ_{𝑡⁢𝑢⁢𝑡⁢𝑜⁢𝑟} ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏的分布导师用于医疗文本分类")
    的推导），
- en: '|  | $\displaystyle\Delta_{tutor}$ |  | (5) |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{tutor}$ |  | (5) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}\leq\sqrt{2}.$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}\leq\sqrt{2}.$
    |  |'
- en: Composition of KD and Tutor.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: KD 和导师的组合。
- en: Our whole DP learning algorithm is actually the combination of KD algorithm
    (§[3.4](#S3.SS4 "3.4 DP-based Discriminator with Knowledge Distillation ‣ 3 Methods
    ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a
    Distribution Tutor for Medical Text Classification")) and tutor algorithm (§[3.5](#S3.SS5
    "3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")).
    According to composition theorem for ($\varepsilon$.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的整个 DP 学习算法实际上是 KD 算法（§[3.4](#S3.SS4 "3.4 基于 DP 的知识蒸馏判别器 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏和分布辅导用于医学文本分类")）和辅导算法（§[3.5](#S3.SS5
    "3.5 标签分布辅导 ‣ 3 方法 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏和分布辅导用于医学文本分类")）的结合。根据组成定理 ($\varepsilon$。
- en: 5 Experimental Settings
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验设置
- en: Datasets.
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集。
- en: We evaluate the methods on “Medical Transcriptions"³³3www.kaggle.com/tboyle10/medicaltranscriptions
    dataset, which is a dataset in the medical domain with medical transcription samples
    from 40 various medical specialties. It contains 5k items. Medical data are extremely
    hard to find due to HIPAA privacy regulations Act ([1996](#bib.bib3)). This dataset
    was scraped from mtsamples.com. We performed basic text processing on the data,
    converting all text to lowercase and removing punctuation. Subsequently, we randomly
    divided 75% of the samples for training and 25% for testing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在“医学转录"³³3www.kaggle.com/tboyle10/medicaltranscriptions 数据集上评估这些方法，该数据集属于医学领域，包含来自
    40 个不同医学专业的医学转录样本，共 5k 项目。由于 HIPAA 隐私法规（[1996](#bib.bib3)），医学数据极难获取。该数据集来自 mtsamples.com。我们对数据进行了基本文本处理，将所有文本转换为小写，并去除标点符号。随后，我们随机将
    75% 的样本用于训练，25% 的样本用于测试。
- en: Evaluation Metrics.
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评价指标。
- en: 'Metrics consists of: accuracy (Acc), precision (P), recall (R), and F1-score
    (F1, the harmonic mean of P and R).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 指标包括：准确率（Acc）、精确率（P）、召回率（R）和 F1-score（F1，即 P 和 R 的调和均值）。
- en: Comparing Methods.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法比较。
- en: 'We used two non-DP methods as the performance upper or lower bound: (1) Private
    directly trains on private data without protections. (2) DA w/ Public trains on
    data synthesized by public GPT-3.5 without accessing private data.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了两种非 DP 方法作为性能的上界或下界：（1）Private 直接在私有数据上训练，不做保护。（2）DA w/ Public 在由公共 GPT-3.5
    合成的数据上训练，不访问私有数据。
- en: 'We use DP-based methods as: (1) DP-SGD Abadi et al. ([2016](#bib.bib1)) trains
    on private data based on DP-SGD with noises on gradients. (2) Ghost Li et al.
    ([2022](#bib.bib41)) trains on private data with DP-Adam and “ghost clipping”.
    (3) DA w/ DP-SGD trains DP-SGD on private data to select pseudo DA samples. (4)
    DA w/ Ghost trains Ghost on private data to select pseudo DA samples. (5) Ours
    denotes our proposed method. Note that DA w/ DP-SGD and DA w/ Ghost also imitates
    the noisy label distribution for a fair comparison with Ours. (See implementation
    details in App. [D](#A4 "Appendix D Implementation Details ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于 DP 的方法如下：（1）DP-SGD Abadi 等（[2016](#bib.bib1)）在私有数据上训练，基于 DP-SGD 并对梯度添加噪声。（2）Ghost
    Li 等（[2022](#bib.bib41)）在私有数据上使用 DP-Adam 和“鬼剪切”进行训练。（3）DA w/ DP-SGD 在私有数据上训练 DP-SGD，以选择伪
    DA 样本。（4）DA w/ Ghost 在私有数据上训练 Ghost，以选择伪 DA 样本。（5）Ours 指代我们提出的方法。请注意，DA w/ DP-SGD
    和 DA w/ Ghost 也模仿了噪声标签分布，以便与我们的方法进行公平比较。（请参阅附录 [D](#A4 "附录 D 实施细节 ‣ 基于 LLM 的隐私数据增强，辅以知识蒸馏和分布辅导用于医学文本分类")）。
- en: 6 Experimental Results
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验结果
- en: 6.1 Main Results
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 主要结果
- en: 'Table [1](#S4.T1 "Table 1 ‣ Lemma 4 Analytical Gaussian mechanism. ‣ 4 Privacy
    Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided by Knowledge
    Distillation with a Distribution Tutor for Medical Text Classification") presents
    the overall performance and we can observe that: without data augmentation (DA),
    Private acts as performance up-bound since it fully accesses the private data
    without any private protection. There are only 3750 train samples without DA.
    The Noisy-SGD methods (i.e. DP-SGD and Ghost) with DA exhibit significant increases
    to the same methods without DA, which shows the effectiveness of our designed
    DA framework. Note that we keep the sample number of DA and non-DA methods same
    for a fair comparison, and DA still works better since DA obtains high quality
    samples with a less privacy cost.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "Table 1 ‣ Lemma 4 Analytical Gaussian mechanism. ‣ 4 Privacy Analyses
    of our Method ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification") 展示了整体性能，我们可以观察到：没有数据增强（DA）的情况下，Private
    作为性能上限，因为它完全访问了私有数据而没有任何隐私保护。没有 DA 的情况下只有 3750 个训练样本。具有 DA 的 Noisy-SGD 方法（即 DP-SGD
    和 Ghost）与没有 DA 的相同方法相比表现出显著提升，这表明了我们设计的 DA 框架的有效性。请注意，我们保持 DA 和非 DA 方法的样本数量相同，以确保公平比较，并且
    DA 仍然表现更好，因为 DA 获得了高质量样本且隐私成本较低。
- en: Ours outperforms other baseline methods in a meaningful range of privacy protection
    ($\varepsilon$ is 4), even Private. The reason for outperforming Private, which
    acts as the up-bound, is that our synthesized data have higher quality and diversity
    than the private data since they are sampled from LLMs, which improves the generalization
    ability of classification models as training data are not sufficient. When the
    training samples selected from a fixed-size synthetic dataset increase from 3750
    to 6000, the performance of all methods generally improves.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法在隐私保护的有意义范围内（$\varepsilon$ 为 4）超越了其他基准方法，即使是在 Private 上。我们的方法之所以优于 Private（作为上限），是因为我们的合成数据比私有数据具有更高的质量和多样性，因为它们是从
    LLMs 中采样的，这提高了分类模型的泛化能力，因为训练数据不足。当从固定大小的合成数据集中选择的训练样本数量从 3750 增加到 6000 时，所有方法的性能普遍提高。
- en: 6.2 Ablation Studies
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 消融研究
- en: '| Method | 3750 training samples |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 3750 训练样本 |'
- en: '| P | R | F1 | Acc |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| P | R | F1 | Acc |'
- en: '| Ours | 0.340 | 0.373 | 0.337 | 0.372 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0.340 | 0.373 | 0.337 | 0.372 |'
- en: '| Ours $-$ Multi-teacher | 0.280 | 0.297 | 0.213 | 0.298 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 $-$ 多教师 | 0.280 | 0.297 | 0.213 | 0.298 |'
- en: '| Ours $-$ Gaussian | 0.327 | 0.333 | 0.263 | 0.335 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 $-$ 高斯 | 0.327 | 0.333 | 0.263 | 0.335 |'
- en: '| Ours $-$ Tutor | 0.340 | 0.340 | 0.313 | 0.340 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 $-$ 导师 | 0.340 | 0.340 | 0.313 | 0.340 |'
- en: '| Ours $-$ Label Dist. | 0.347 | 0.377 | 0.340 | 0.377 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 $-$ 标签分布 | 0.347 | 0.377 | 0.340 | 0.377 |'
- en: 'Table 2: Ablation studies. “$+$" means using or not using the given strategy.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：消融研究。“$+$" 表示是否使用给定策略。
- en: 'We conducted ablation studies to evaluate the effectiveness of our proposed
    components. Table [2](#S6.T2 "Table 2 ‣ 6.2 Ablation Studies ‣ 6 Experimental
    Results ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification") presents the precision,
    recall, F1-score, and accuracy of Ours’ variants on the downstream task. (1) Ours
    $-$ Label Dist.: discard the tutor but directly imitate the private label distribution
    to verify the effectiveness of learning the private label distribution, which
    is confirmed by its excellent performance. Compared with Ours, it can learn a
    label distribution closer to real data without any privacy protection. Our tutor
    achieves similar performance even after adding noise.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了消融研究以评估我们提出的组件的有效性。表 [2](#S6.T2 "Table 2 ‣ 6.2 Ablation Studies ‣ 6 Experimental
    Results ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification") 展示了我们方法的变体在下游任务上的精确度、召回率、F1
    分数和准确性。（1）我们的方法 $-$ 标签分布：丢弃了导师，但直接模仿私有标签分布以验证学习私有标签分布的有效性，这一点通过其优异的性能得到证实。与我们的方法相比，它可以学习更接近真实数据的标签分布，但没有任何隐私保护。即使添加了噪声，我们的导师也能实现类似的性能。
- en: 6.3 Privacy-utility Tradeoff
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 隐私-效用权衡
- en: '![Refer to caption](img/5b1db4253d58dd61b0c6930294cb8568.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/5b1db4253d58dd61b0c6930294cb8568.png)'
- en: 'Figure 2: The private-utility tradeoff in accuracy across three DA w/ DP methods
    at varying $\varepsilon$. The vertical axis represents the accuracy on downstream
    text classification tasks.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在不同 $\varepsilon$ 下，三种 DA w/ DP 方法在准确性上的隐私-效用权衡。纵轴表示下游文本分类任务上的准确性。
- en: '![Refer to caption](img/4b4f47e6fe764cf722597f88353722ff.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/4b4f47e6fe764cf722597f88353722ff.png)'
- en: 'Figure 3: The private-utility tradeoff in the discriminator’s accuracy across
    three DA w/ DP methods at varying $\varepsilon$. The vertical axis represents
    the accuracy on our constructed test set.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同 $\varepsilon$ 下，三种 DA w/ DP 方法在判别器准确性上的隐私效用权衡。纵轴表示我们构建的测试集上的准确性。
- en: In Fig. [2](#S6.F2 "Figure 2 ‣ 6.3 Privacy-utility Tradeoff ‣ 6 Experimental
    Results ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation
    with a Distribution Tutor for Medical Text Classification") and Fig. [3](#S6.F3
    "Figure 3 ‣ 6.3 Privacy-utility Tradeoff ‣ 6 Experimental Results ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification"), we show the private-utility trade-off
    curve of three DA w/ DP methods and their discriminators covering the range of
    meaningful protection (i.e. usually $\varepsilon\in[0.1,10]$). After exceeding
    the up-bound, the final performance is probably beyond the control of the discriminator
    (The discriminator judges whether the synthetic data meet privacy characteristics,
    but the performance of selected synthetic data by Ours exceeded the original privacy
    data).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S6.F2 "图 2 ‣ 6.3 隐私与效用的权衡 ‣ 6 实验结果 ‣ 基于LLM的隐私数据增强，由知识蒸馏和分布导师指导的医疗文本分类")
    和图 [3](#S6.F3 "图 3 ‣ 6.3 隐私与效用的权衡 ‣ 6 实验结果 ‣ 基于LLM的隐私数据增强，由知识蒸馏和分布导师指导的医疗文本分类")
    中，我们展示了三种DA w/ DP方法及其判别器的隐私效用权衡曲线，涵盖了有意义的保护范围（即通常 $\varepsilon\in[0.1,10]$）。超过上限后，最终性能可能超出判别器的控制范围（判别器判断合成数据是否符合隐私特征，但由我们选择的合成数据的性能超过了原始隐私数据）。
- en: 6.4 Analysis On Teacher Numbers
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 关于教师数量的分析
- en: '![Refer to caption](img/9f3b151667636fe4a7054b4d0fcb1518.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9f3b151667636fe4a7054b4d0fcb1518.png)'
- en: 'Figure 4: Analysis on teacher number. The vertical axis represents the prediction
    accuracy of the discriminator on our constructed test set.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：关于教师数量的分析。纵轴表示判别器在我们构建的测试集上的预测准确性。
- en: 'To analyze the impact of teacher number on discriminator, we conducted experiments
    across varying teacher number with $\varepsilon=4$ (in Fig. [4](#S6.F4 "Figure
    4 ‣ 6.4 Analysis On Teacher Numbers ‣ 6 Experimental Results ‣ LLM-based Privacy
    Data Augmentation Guided by Knowledge Distillation with a Distribution Tutor for
    Medical Text Classification")). Teachers denotes the aggregation of multiple teacher
    models; Teachers + Noise denotes to add noise to the voting results after aggregating
    multiple teacher models; Student denotes our final discriminator model. We conclude
    that: (1) As the teacher number increases, the performance of Teachers + Noise
    gradually improves. When the teacher number reaches 20, the impact of adding noise
    on the aggregated teachers’ performance becomes negligible. (2) Student’s performance
    correlates positively with Teachers + Noise. (3) In most cases, Student outperforms
    Teachers + Noise. This is because Teachers + Noise reduces Teachers’ original
    prediction accuracy by directly adding noise to the inference results. Student
    trains with noisy data, allowing the model to adapt to the noise.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析教师数量对判别器的影响，我们在 $\varepsilon=4$ 下进行了不同教师数量的实验（见图 [4](#S6.F4 "图 4 ‣ 6.4 关于教师数量的分析
    ‣ 6 实验结果 ‣ 基于LLM的隐私数据增强，由知识蒸馏和分布导师指导的医疗文本分类")）。教师表示多个教师模型的聚合；教师 + 噪声表示在聚合多个教师模型后对投票结果添加噪声；学生表示我们的最终判别器模型。我们得出结论：(1)
    随着教师数量的增加，教师 + 噪声的性能逐渐提高。当教师数量达到 20 时，添加噪声对聚合教师的性能影响变得微不足道。(2) 学生的性能与教师 + 噪声呈正相关。(3)
    在大多数情况下，学生的表现优于教师 + 噪声。这是因为教师 + 噪声通过直接向推断结果添加噪声来降低教师的原始预测准确性。学生在噪声数据上训练，使模型能够适应噪声。
- en: 6.5 Analysis On Tutor’s Distribution
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 关于导师分布的分析
- en: 'Fig. [5](#A1.F5 "Figure 5 ‣ Appendix A Deduction of Sensitivity Δ_{𝐾⁢𝐷} ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification") (See App. [E](#A5 "Appendix E Label Distributions
    ‣ LLM-based Privacy Data Augmentation Guided by Knowledge Distillation with a
    Distribution Tutor for Medical Text Classification")) illustrates two distinct
    label distributions: Private Dist represents the original private label distribution;
    Tutor Dist represents the label distribution after adding noise (discussed in
    §[3.5](#S3.SS5 "3.5 Label Distribution Tutor ‣ 3 Methods ‣ LLM-based Privacy Data
    Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical
    Text Classification")), which provides stricter privacy protection by satisfying
    $\varepsilon=0.4$. We observe a high consistency between Tutor Dist and Private
    Dist, which indicates that Tutor Dist effectively retains the characteristics
    of Private Dist.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#A1.F5 "图 5 ‣ 附录 A 灵敏度 Δ_{𝐾⁢𝐷} 的推导 ‣ 基于 LLM 的隐私数据增强，通过分布导师引导医疗文本分类")（见附录
    [E](#A5 "附录 E 标签分布 ‣ 基于 LLM 的隐私数据增强，通过分布导师引导医疗文本分类")）展示了两种不同的标签分布：Private Dist
    表示原始私有标签分布；Tutor Dist 表示添加噪声后的标签分布（讨论见 §[3.5](#S3.SS5 "3.5 标签分布导师 ‣ 3 方法 ‣ 基于
    LLM 的隐私数据增强，通过分布导师引导医疗文本分类")），其通过满足 $\varepsilon=0.4$ 提供了更严格的隐私保护。我们观察到 Tutor
    Dist 与 Private Dist 之间有很高的一致性，这表明 Tutor Dist 有效保留了 Private Dist 的特征。
- en: 7 Conclusion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we proposed a DP-based DA method for text classification in private
    domains, which prompts a LLM to generate pseudo samples and uses a DP-based discriminator
    to examine the LLM’s outputs. In this way, we transfer pseudo text generation
    task, which is a challenging DP paradigm, to a discrimination task. We construct
    a DP-based discriminator via knowledge distillation and construct a DP-based tutor
    to guide the sample generation with a low privacy cost. Theoretical analyses illustrates
    the bound of protections of our models and our experimental results shows our
    model’s effectiveness.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于 DP 的 DA 方法用于私有领域的文本分类，该方法促使 LLM 生成伪样本，并使用基于 DP 的鉴别器检查 LLM 的输出。通过这种方式，我们将伪文本生成任务（这是一个具有挑战性的
    DP 范式）转化为鉴别任务。我们通过知识蒸馏构建了一个基于 DP 的鉴别器，并构建了一个基于 DP 的导师，以低隐私成本引导样本生成。理论分析说明了我们模型的保护界限，我们的实验结果展示了我们模型的有效性。
- en: 8 Limitations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: In our study, there were several limitations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，存在几个限制。
- en: (1) First of all, we use GPT-3.5 instead of GPT-4 in our experiments and GPT-3.5
    is not a latest and SOTA GPT API, which seems to limited the performance of our
    model. The main reason is that our baselines Abadi et al. ([2016](#bib.bib1));
    Li et al. ([2022](#bib.bib41)) are all based on GPT-3.5 for a fair comparison.
    In addition, compared to GPT-3.5, GPT-4 costs too much to obtain API keys. According
    to the official website, the fee for GPT 3.5 is 0.002$/1k Token, and the fee for
    GPT 4 is 0.06$/1k Token. In our actual baseline comparison, about 560,000 pieces
    of data were generated using GPT-3.5, and the total cost was about 300$. And if
    GPT-4 is used, the same number of tokens will cost more than 9000$. Therefore,
    the use of GPT-3.5 can not only effectively reduce the cost significantly, but
    also there will not be much difference in the generation effect. At present, we
    use GPT-3.5 for method verification, and GPT-4 will be used for effect verification
    in the future.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 首先，我们在实验中使用了 GPT-3.5，而不是 GPT-4，而 GPT-3.5 不是最新的和 SOTA 的 GPT API，这似乎限制了我们模型的性能。主要原因是我们的基准
    Abadi et al. ([2016](#bib.bib1)); Li et al. ([2022](#bib.bib41)) 都是基于 GPT-3.5
    进行公平比较。此外，与 GPT-3.5 相比，GPT-4 的 API 密钥成本过高。根据官方网站，GPT 3.5 的费用是 0.002$/1k Token，而
    GPT 4 的费用是 0.06$/1k Token。在实际基准比较中，使用 GPT-3.5 生成了约 560,000 条数据，总成本约为 300$。而如果使用
    GPT-4，相同数量的 token 将花费超过 9000$。因此，使用 GPT-3.5 不仅可以有效地显著降低成本，而且生成效果不会有太大差异。目前，我们使用
    GPT-3.5 进行方法验证，未来将使用 GPT-4 进行效果验证。
- en: (2) Secondly, our approach is based on LLM-generated data, relatively dependent
    on the quality of the generated text. If the data are difficult to generate, or
    the overall quality of the generated text is poor, this may limit the advantages
    of our approach. Subsequently, we can leverage approaches such as prompt tuning,
    by carrying out a variety of excellent prompt engineering during the training
    phase of the LLM, enhancing the quality of text generated by LLM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 其次，我们的方法基于 LLM 生成的数据，相对依赖于生成文本的质量。如果数据生成困难，或生成文本的整体质量较差，可能会限制我们方法的优势。随后，我们可以利用如提示调整等方法，通过在
    LLM 训练阶段进行各种优秀的提示工程，提升 LLM 生成文本的质量。
- en: 9 Ethical Considerations
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 个伦理考虑事项
- en: We place significant importance on ethical considerations and adhere rigorously
    to the ACL Ethics Policy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们非常重视伦理问题，并严格遵守 ACL 伦理政策。
- en: (1) This study introduces a novel text classification method, utilizing an LLM
    to generate pseudo samples and a DP-based discriminator to evaluate them, without
    ethical concerns regarding motivation or algorithmic approach, as no private information
    is used.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 本研究介绍了一种新颖的文本分类方法，利用 LLM 生成伪样本，并使用基于 DP 的判别器对其进行评估，无需担心动机或算法方法的伦理问题，因为没有使用任何私人信息。
- en: (2) Nevertheless, it’s crucial to contemplate scenarios where individuals deliberately
    exploit our model for illicit purposes. For instance, someone might use the text
    generation model, used in this paper for generating pseudo data, to fabricate
    fake text or misinformation. This potential misuse poses a negative societal impact,
    using our model to generate false medical reports. Moving forward, we intend to
    implement constraints within our model to prevent the generation of texts for
    illegal activities, such as introducing filters to identify and flag potentially
    harmful or illegal content.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 然而，必须考虑那些故意利用我们的模型进行非法活动的情况。例如，有人可能会使用本论文中用于生成伪数据的文本生成模型来伪造虚假文本或虚假信息。这种潜在的误用会带来负面的社会影响，例如利用我们的模型生成虚假的医学报告。未来，我们计划在模型中实施约束，防止生成用于非法活动的文本，例如引入过滤器来识别和标记潜在的有害或非法内容。
- en: (3) Moreover, it’s imperative to exercise caution in utilizing our model and
    refrain from assuming its infallibility. One potential unethical application involves
    gathering data from users who believe our model guarantees complete privacy protection,
    potentially overlooking the actual strength of privacy safeguards. This oversight
    could lead to adverse societal consequences. Therefore, we urge researchers intending
    to utilize this model to prioritize the efficacy of privacy protection. Additionally,
    measures should be taken to prevent researchers from collecting data from users
    who lack a proper understanding of our algorithm. For example, Clarify privacy
    policies and rules for data usage, and restrict access to collected data to authorized
    personnel only. We recommend that researchers ensure users contributing their
    data comprehend the risks associated with our model fully.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 此外，使用我们的模型时必须谨慎，并避免假设其绝对可靠。一种潜在的不道德应用是收集那些相信我们的模型能完全保护隐私的用户的数据，这可能忽视了隐私保护措施的实际强度。这种疏忽可能会导致不利的社会后果。因此，我们敦促打算使用该模型的研究人员优先考虑隐私保护的有效性。此外，应采取措施防止研究人员从对我们算法缺乏充分理解的用户那里收集数据。例如，明确隐私政策和数据使用规则，并将收集的数据访问权限限制为授权人员。我们建议研究人员确保提供数据的用户充分理解与我们模型相关的风险。
- en: (4) In general, if the dataset contains privacy, it may be leaked during use.
    Regarding the datasets utilized in our experiments, the dataset generated by LLM
    does not contain the personally identifiable information of the real user. In
    contrast, the Medical Transcriptions dataset contains sample medical transcriptions
    for various medical specialties, allowing us to conduct experiments to assess
    the efficacy of privacy protection measures. It’s important to note that the Medical
    Transcriptions dataset was previously made available to the public. Therefore,
    our research in this paper does not involve releasing any additional personal
    information of users.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 一般来说，如果数据集中包含隐私信息，在使用过程中可能会被泄露。关于我们实验中使用的数据集，由LLM生成的数据集不包含真实用户的个人可识别信息。相比之下，医学转录数据集包含各种医学专业的样本医学转录，允许我们进行实验以评估隐私保护措施的有效性。值得注意的是，医学转录数据集之前已公开。因此，我们本文中的研究不涉及发布任何额外的用户个人信息。
- en: References
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan,
    Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential
    privacy. In *CCS*, pages 308–318.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan,
    Ilya Mironov, Kunal Talwar, 和 Li Zhang. 2016. 带有差分隐私的深度学习。发表于*CCS*，第308–318页。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat等。2023. Gpt-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: Act (1996) Accountability Act. 1996. Health insurance portability and accountability
    act of 1996. *Public law*, 104:191.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Act (1996) Accountability Act. 1996. 1996年健康保险流通与问责法案。*公共法律*，104:191。
- en: Anil et al. (2021) Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin
    Manurangsi. 2021. Large-scale differentially private bert. *arXiv preprint arXiv:2108.01624*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anil et al. (2021) Rohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, 和 Pasin
    Manurangsi. 2021. 大规模差分隐私BERT。*arXiv预印本 arXiv:2108.01624*。
- en: 'Balle and Wang (2018) Borja Balle and Yu-Xiang Wang. 2018. Improving the gaussian
    mechanism for differential privacy: Analytical calibration and optimal denoising.
    In *International Conference on Machine Learning*, pages 394–403\. PMLR.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balle and Wang (2018) Borja Balle 和 Yu-Xiang Wang. 2018. 改进差分隐私的高斯机制：分析校准和最佳去噪。发表于*国际机器学习会议*，第394–403页。PMLR。
- en: 'Bassily et al. (2014) Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014.
    Private empirical risk minimization: Efficient algorithms and tight error bounds.
    In *Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Computer
    Science*, pages 464–473.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bassily et al. (2014) Raef Bassily, Adam Smith, 和 Abhradeep Thakurta. 2014.
    私有经验风险最小化：高效算法和紧致错误界限。发表于*2014 IEEE第55届计算机科学基础年会论文集*，第464–473页。
- en: 'Boenisch et al. (2023) Franziska Boenisch, Christopher Mühl, Roy Rinberg, Jannis
    Ihrig, and Adam Dziedzic. 2023. Individualized pate: Differentially private machine
    learning with individual privacy guarantees. *Proceedings on Privacy Enhancing
    Technologies*, 1:158–176.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boenisch et al. (2023) Franziska Boenisch, Christopher Mühl, Roy Rinberg, Jannis
    Ihrig, 和 Adam Dziedzic. 2023. 个性化的PATE：具有个体隐私保障的差分隐私机器学习。*隐私增强技术会议论文集*，1:158–176。
- en: Bu et al. (2020) Zhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. 2020. Deep
    learning with gaussian differential privacy. In *Harvard Data Science Review*,
    volume 2020\. NIH Public Access.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu et al. (2020) Zhiqi Bu, Jinshuo Dong, Qi Long, 和 Weijie J Su. 2020. 带有高斯差分隐私的深度学习。发表于*哈佛数据科学评论*，卷号2020。NIH公共访问。
- en: Bu et al. (2023) Zhiqi Bu, Hua Wang, Zongyu Dai, and Qi Long. 2023. On the convergence
    and calibration of deep learning with differential privacy. *Transactions on Machine
    Learning Research*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu et al. (2023) Zhiqi Bu, Hua Wang, Zongyu Dai, 和 Qi Long. 2023. 关于带有差分隐私的深度学习的收敛性和校准。*机器学习研究交易*。
- en: Bu et al. (2021) Zhiqi Bu, Hua Wang, Qi Long, and Weijie J Su. 2021. On the
    convergence of deep learning with differential privacy. In *arXiv preprint arXiv:2106.07830*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu et al. (2021) Zhiqi Bu, Hua Wang, Qi Long, 和 Weijie J Su. 2021. 关于带有差分隐私的深度学习的收敛性。*arXiv预印本
    arXiv:2106.07830*。
- en: Cao et al. (2021) Pengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao, and
    Wei Bi. 2021. Uncertainty-aware self-training for semi-supervised event temporal
    relation extraction. In *Proceedings of the 30th ACM International Conference
    on Information & Knowledge Management*, pages 2900–2904.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. (2021) Pengfei Cao, Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao, 和 Wei
    Bi. 2021. 具有不确定性感知的自我训练用于半监督事件时间关系提取。在*第30届 ACM 国际信息与知识管理会议论文集*，第2900–2904页。
- en: 'Carlini et al. (2019a) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. 2019a. The secret sharer: Evaluating and testing unintended
    memorization in neural networks. In *28th USENIX Security Symposium (USENIX Security
    19)*, pages 267–284.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2019a) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, 和 Dawn Song. 2019a. 秘密分享者：评估和测试神经网络中的非意图记忆。在*第28届 USENIX 安全研讨会 (USENIX Security
    19)*，第267–284页。
- en: 'Carlini et al. (2019b) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. 2019b. The secret sharer: Evaluating and testing unintended
    memorization in neural networks. In *USENIX Security*, pages 267–284.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2019b) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, 和 Dawn Song. 2019b. 秘密分享者：评估和测试神经网络中的非意图记忆。在*USENIX Security*，第267–284页。
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, et al. 2021. Extracting training data from large language models.
    In *30th USENIX Security Symposium (USENIX Security 21)*, pages 2633–2650.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, 等. 2021. 从大型语言模型中提取训练数据。在*第30届 USENIX 安全研讨会 (USENIX Security
    21)*，第2633–2650页。
- en: Chaudhuri et al. (2011) Kamalika Chaudhuri, Claire Monteleoni, and Anand D Sarwate.
    2011. Differentially private empirical risk minimization. *Journal of Machine
    Learning Research*, 12(3).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhuri et al. (2011) Kamalika Chaudhuri, Claire Monteleoni, 和 Anand D Sarwate.
    2011. 差分隐私的经验风险最小化。*机器学习研究杂志*，12(3)。
- en: Chen et al. (2020) Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, and Diyi
    Yang. 2020. Local additivity based data augmentation for semi-supervised ner.
    *arXiv preprint arXiv:2010.01677*.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2020) Jiaao Chen, Zhenghui Wang, Ran Tian, Zichao Yang, 和 Diyi
    Yang. 2020. 基于局部加法的数据增强用于半监督命名实体识别。*arXiv 预印本 arXiv:2010.01677*。
- en: Deng et al. (2022) Jieren Deng, Chenghong Wang, Xianrui Meng, Yijue Wang, Ji Li,
    Sheng Lin, Shuo Han, Fei Miao, Sanguthevar Rajasekaran, and Caiwen Ding. 2022.
    A secure and efficient federated learning framework for nlp. *arXiv preprint arXiv:2201.11934*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2022) Jieren Deng, Chenghong Wang, Xianrui Meng, Yijue Wang, Ji
    Li, Sheng Lin, Shuo Han, Fei Miao, Sanguthevar Rajasekaran, 和 Caiwen Ding. 2022.
    一个安全高效的联邦学习框架用于自然语言处理。*arXiv 预印本 arXiv:2201.11934*。
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *NAACL*, pages 4171–4186.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. Bert: 语言理解的深度双向变换器的预训练。在*NAACL*，第4171–4186页。'
- en: Dong et al. (2019) Jinshuo Dong, Aaron Roth, and Weijie J Su. 2019. Gaussian
    differential privacy. *arXiv preprint arXiv:1905.02383*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Jinshuo Dong, Aaron Roth, 和 Weijie J Su. 2019. 高斯差分隐私。*arXiv
    预印本 arXiv:1905.02383*。
- en: 'Duan et al. (2023) Haonan Duan, Adam Dziedzic, Nicolas Papernot, and Franziska
    Boenisch. 2023. Flocks of stochastic parrots: Differentially private prompt learning
    for large language models. *arXiv preprint arXiv:2305.15594*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan et al. (2023) Haonan Duan, Adam Dziedzic, Nicolas Papernot, 和 Franziska
    Boenisch. 2023. 随机鹦鹉的群体：针对大型语言模型的差分隐私提示学习。*arXiv 预印本 arXiv:2305.15594*。
- en: 'Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
    2006. Calibrating noise to sensitivity in private data analysis. In *Theory of
    Cryptography: Third Theory of Cryptography Conference, TCC 2006, New York, NY,
    USA, March 4-7, 2006\. Proceedings 3*, pages 265–284\. Springer.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, 和 Adam Smith.
    2006. 在私密数据分析中将噪声校准到敏感性。在*密码学理论：第三届密码学理论会议，TCC 2006，纽约，NY，美国，2006年3月4-7日*，第265–284页。Springer。
- en: Dwork et al. (2014a) Cynthia Dwork, Aaron Roth, et al. 2014a. The algorithmic
    foundations of differential privacy. *Foundations and Trends® in Theoretical Computer
    Science*, 9(3–4):211–407.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwork et al. (2014a) Cynthia Dwork, Aaron Roth, 等. 2014a. 差分隐私的算法基础。*理论计算机科学基础与趋势®*，9(3–4)：211–407。
- en: Dwork et al. (2014b) Cynthia Dwork, Aaron Roth, et al. 2014b. The algorithmic
    foundations of differential privacy. In *TCS*, volume 9, pages 211–407.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwork et al. (2014b) Cynthia Dwork, Aaron Roth, 等. 2014b. 差分隐私的算法基础。在*TCS*，第9卷，第211–407页。
- en: Fei-Fei et al. (2006) Li Fei-Fei, Robert Fergus, and Pietro Perona. 2006. One-shot
    learning of object categories. *IEEE transactions on pattern analysis and machine
    intelligence*, 28(4):594–611.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei-Fei 等人 (2006) Li Fei-Fei, Robert Fergus, 和 Pietro Perona. 2006. 一次性学习对象类别。*IEEE
    模式分析与机器智能交易*，28(4):594–611。
- en: Feyisetan et al. (2020) Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, and
    Tom Diethe. 2020. Privacy-and utility-preserving textual analysis via calibrated
    multivariate perturbations. In *Proceedings of the 13th international conference
    on web search and data mining*, pages 178–186.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feyisetan 等人 (2020) Oluwaseyi Feyisetan, Borja Balle, Thomas Drake, 和 Tom Diethe.
    2020. 通过校准的多变量扰动实现隐私和效用保护的文本分析。在*第十三届国际网络搜索与数据挖掘会议论文集*，第178–186页。
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *ICML*, pages 1126–1135.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn 等人 (2017) Chelsea Finn, Pieter Abbeel, 和 Sergey Levine. 2017. 模型无关的元学习以快速适应深度网络。在*ICML*，第1126–1135页。
- en: Garfinkel et al. (2015) Simson Garfinkel et al. 2015. *De-identification of
    Personal Information:.* US Department of Commerce, National Institute of Standards
    and Technology.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garfinkel 等人 (2015) Simson Garfinkel 等人. 2015. *个人信息去标识化：* 美国商务部，国家标准与技术研究所。
- en: 'Guo et al. (2019) Hongyu Guo, Yongyi Mao, and Richong Zhang. 2019. Augmenting
    data with mixup for sentence classification: An empirical study. *arXiv preprint
    arXiv:1905.08941*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 (2019) Hongyu Guo, Yongyi Mao, 和 Richong Zhang. 2019. 使用 mixup 进行句子分类的数据增强：一项实证研究。*arXiv
    预印本 arXiv:1905.08941*。
- en: He et al. (2022) Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni,
    Yin Tat Lee, Arturs Backurs, Nenghai Yu, and Jiang Bian. 2022. Exploring the limits
    of differentially private deep learning with group-wise clipping. *arXiv preprint
    arXiv:2212.01539*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 (2022) Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni,
    Yin Tat Lee, Arturs Backurs, Nenghai Yu, 和 Jiang Bian. 2022. 探索具有组划分的差分隐私深度学习的极限。*arXiv
    预印本 arXiv:2212.01539*。
- en: Hu et al. (2019) Zhiting Hu, Bowen Tan, Russ R Salakhutdinov, Tom M Mitchell,
    and Eric P Xing. 2019. Learning data manipulation for augmentation and weighting.
    *Advances in Neural Information Processing Systems*, 32.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2019) Zhiting Hu, Bowen Tan, Russ R Salakhutdinov, Tom M Mitchell, 和
    Eric P Xing. 2019. 学习数据处理以进行增强和加权。*神经信息处理系统进展*，32。
- en: 'Huang et al. (2019) Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. 2019.
    Clinicalbert: Modeling clinical notes and predicting hospital readmission. *arXiv
    preprint arXiv:1904.05342*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 等人 (2019) Kexin Huang, Jaan Altosaar, 和 Rajesh Ranganath. 2019. Clinicalbert:
    建模临床笔记并预测医院再入院。*arXiv 预印本 arXiv:1904.05342*。'
- en: Johnson and Shmatikov (2013) Aaron Johnson and Vitaly Shmatikov. 2013. Privacy-preserving
    data exploration in genome-wide association studies. In *Proceedings of the 19th
    ACM SIGKDD international conference on Knowledge discovery and data mining*, pages
    1079–1087.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson 和 Shmatikov (2013) Aaron Johnson 和 Vitaly Shmatikov. 2013. 基因组范围关联研究中的隐私保护数据探索。在*第十九届
    ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，第1079–1087页。
- en: 'Karimi et al. (2021) Akbar Karimi, Leonardo Rossi, and Andrea Prati. 2021.
    Aeda: an easier data augmentation technique for text classification. *arXiv preprint
    arXiv:2108.13230*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karimi 等人 (2021) Akbar Karimi, Leonardo Rossi, 和 Andrea Prati. 2021. Aeda:
    一种更简单的文本分类数据增强技术。*arXiv 预印本 arXiv:2108.13230*。'
- en: Kerrigan et al. (2020) Gavin Kerrigan, Dylan Slack, and Jens Tuyls. 2020. Differentially
    private language models benefit from public pre-training. In *Workshop in EMNLP*,
    pages 39–45.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kerrigan 等人 (2020) Gavin Kerrigan, Dylan Slack, 和 Jens Tuyls. 2020. 差分隐私语言模型受益于公开的预训练。在*EMNLP
    研讨会*，第39–45页。
- en: 'Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for
    stochastic optimization. *arXiv preprint arXiv:1412.6980*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Ba (2014) Diederik P Kingma 和 Jimmy Ba. 2014. Adam: 一种随机优化方法。*arXiv
    预印本 arXiv:1412.6980*。'
- en: 'Kobayashi (2018) Sosuke Kobayashi. 2018. Contextual augmentation: Data augmentation
    by words with paradigmatic relations. *arXiv preprint arXiv:1805.06201*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kobayashi (2018) Sosuke Kobayashi. 2018. 上下文增强：通过具有范式关系的词进行数据增强。*arXiv 预印本 arXiv:1805.06201*。
- en: Konyushkova et al. (2017) Ksenia Konyushkova, Raphael Sznitman, and Pascal Fua.
    2017. Learning active learning from data. *Advances in neural information processing
    systems*, 30.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Konyushkova 等人 (2017) Ksenia Konyushkova, Raphael Sznitman, 和 Pascal Fua. 2017.
    从数据中学习主动学习。*神经信息处理系统进展*，30。
- en: Kumar et al. (2020) Varun Kumar, Ashutosh Choudhary, and Eunah Cho. 2020. Data
    augmentation using pre-trained transformer models. *arXiv preprint arXiv:2003.02245*.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2020) Varun Kumar, Ashutosh Choudhary, 和 Eunah Cho. 2020. 使用预训练的变换器模型进行数据增强。*arXiv
    预印本 arXiv:2003.02245*。
- en: 'Li et al. (2020) Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng
    Qiu. 2020. Bert-attack: Adversarial attack against bert using bert. *arXiv preprint
    arXiv:2004.09984*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2020）Linyang Li、Ruotian Ma、Qipeng Guo、Xiangyang Xue 和 Xipeng Qiu。2020年。Bert-attack：对
    BERT 的对抗攻击使用 BERT。*arXiv 预印本 arXiv:2004.09984*。
- en: 'Li et al. (2021) Xiang Li, Menglin Cui, Jingpeng Li, Ruibin Bai, Zheng Lu,
    and Uwe Aickelin. 2021. A hybrid medical text classification framework: Integrating
    attentive rule construction and neural network. *Neurocomputing*, 443:345–355.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2021）Xiang Li、Menglin Cui、Jingpeng Li、Ruibin Bai、Zheng Lu 和 Uwe Aickelin。2021年。一种混合医学文本分类框架：集成注意力规则构建和神经网络。*神经计算*，443：345–355。
- en: Li et al. (2022) Xuechen Li, Florian Tramèr, Percy Liang, and Tatsunori Hashimoto.
    2022. Large language models can be strong differentially private learners. In
    *ICLR*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022）Xuechen Li、Florian Tramèr、Percy Liang 和 Tatsunori Hashimoto。2022年。大型语言模型可以是强大的差分隐私学习者。在
    *ICLR*。
- en: Lin et al. (2023) Xiaotian Lin, Nankai Lin, Yingwen Fu, Ziyu Yang, and Shengyi
    Jiang. 2023. How to choose" good" samples for text data augmentation. *arXiv preprint
    arXiv:2302.00894*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2023）Xiaotian Lin、Nankai Lin、Yingwen Fu、Ziyu Yang 和 Shengyi Jiang。2023年。如何选择“好”的样本进行文本数据增强。*arXiv
    预印本 arXiv:2302.00894*。
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loshchilov 和 Hutter（2017）Ilya Loshchilov 和 Frank Hutter。2017年。解耦的权重衰减正则化。*arXiv
    预印本 arXiv:1711.05101*。
- en: Maeda et al. (2016) Wakana Maeda, Yu Suzuki, and Satoshi Nakamura. 2016. Fast
    text anonymization using k-anonyminity. In *Proceedings of the 18th International
    Conference on Information Integration and Web-based Applications and Services*,
    pages 340–344.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maeda 等（2016）Wakana Maeda、Yu Suzuki 和 Satoshi Nakamura。2016年。使用 k-匿名性进行快速文本匿名化。在
    *第18届信息集成与基于网络的应用与服务国际会议*，第340–344页。
- en: McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*, pages 1273–1282\.
    PMLR.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan 等（2017）Brendan McMahan、Eider Moore、Daniel Ramage、Seth Hampson 和 Blaise
    Aguera y Arcas。2017年。基于去中心化数据的深度网络通信高效学习。在 *人工智能与统计*，第1273–1282页。PMLR。
- en: Papernot et al. (2017) Nicolas Papernot, Martın Abadi, Ulfar Erlingsson, Ian
    Goodfellow, and Kunal Talwar. 2017. Semi-supervised knowledge transfer for deep
    learning from private training data. In *ICLR*.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papernot 等（2017）Nicolas Papernot、Martın Abadi、Ulfar Erlingsson、Ian Goodfellow
    和 Kunal Talwar。2017年。用于从私人训练数据中进行深度学习的半监督知识转移。在 *ICLR*。
- en: Qing et al. (2019) Li Qing, Weng Linhong, and Ding Xuehai. 2019. A novel neural
    network-based method for medical text classification. *Future Internet*, 11(12):255.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qing 等（2019）Li Qing、Weng Linhong 和 Ding Xuehai。2019年。一种基于神经网络的医学文本分类新方法。*未来互联网*，11(12)：255。
- en: Rocher et al. (2019) Luc Rocher, Julien M Hendrickx, and Yves-Alexandre De Montjoye.
    2019. Estimating the success of re-identifications in incomplete datasets using
    generative models. *Nature communications*, 10(1):1–9.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rocher 等（2019）Luc Rocher、Julien M Hendrickx 和 Yves-Alexandre De Montjoye。2019年。使用生成模型估计不完整数据集中重新识别的成功率。*自然通讯*，10(1)：1–9。
- en: 'Rubinstein et al. (2009) Benjamin IP Rubinstein, Peter L Bartlett, Ling Huang,
    and Nina Taft. 2009. Learning in a large function space: Privacy-preserving mechanisms
    for svm learning. *arXiv preprint arXiv:0911.5708*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubinstein 等（2009）Benjamin IP Rubinstein、Peter L Bartlett、Ling Huang 和 Nina
    Taft。2009年。在大型函数空间中学习：支持向量机学习的隐私保护机制。*arXiv 预印本 arXiv:0911.5708*。
- en: Shi et al. (2021) Weiyan Shi, Aiqi Cui, Evan Li, Ruoxi Jia, and Zhou Yu. 2021.
    Selective differential privacy for language modeling. In *arXiv preprint arXiv:2108.12944*.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等（2021）Weiyan Shi、Aiqi Cui、Evan Li、Ruoxi Jia 和 Zhou Yu。2021年。语言建模的选择性差分隐私。在
    *arXiv 预印本 arXiv:2108.12944*。
- en: Song et al. (2013) Shuang Song, Kamalika Chaudhuri, and Anand D Sarwate. 2013.
    Stochastic gradient descent with differentially private updates. In *GlobalSIP*,
    pages 245–248\. IEEE.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等（2013）Shuang Song、Kamalika Chaudhuri 和 Anand D Sarwate。2013年。具有差分隐私更新的随机梯度下降。在
    *GlobalSIP*，第245–248页。IEEE。
- en: 'Suzuki et al. (2018) Yu Suzuki, Koichiro Yoshino, and Satoshi Nakamura. 2018.
    A k-anonymized text generation method. In *Advances in Network-Based Information
    Systems: The 20th International Conference on Network-Based Information Systems
    (NBiS-2017)*, pages 1018–1026\. Springer.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzuki 等（2018）Yu Suzuki、Koichiro Yoshino 和 Satoshi Nakamura。2018年。k-匿名文本生成方法。在
    *网络基础信息系统进展：第20届国际网络基础信息系统会议（NBiS-2017）*，第1018–1026页。Springer。
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, pages 2818–2826.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, 和 Zbigniew Wojna. 2016. **重新思考计算机视觉的 inception 架构**。在 *IEEE 计算机视觉与模式识别会议论文集*，页面
    2818–2826。
- en: 'Tian et al. (2022) Zhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang,
    Nevin L Zhang, and He He. 2022. Seqpate: Differentially private text generation
    via knowledge distillation. *Advances in Neural Information Processing Systems*,
    35:11117–11130.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2022) Zhiliang Tian, Yingxiu Zhao, Ziyue Huang, Yu-Xiang Wang,
    Nevin L Zhang, 和 He He. 2022. **Seqpate**：通过知识蒸馏进行差分隐私文本生成。*神经信息处理系统进展*，35:11117–11130。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等. 2023. **Llama**：开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: Triastcyn and Faltings (2020) Aleksei Triastcyn and Boi Faltings. 2020. Bayesian
    differential privacy for machine learning. In *International Conference on Machine
    Learning*, pages 9583–9592\. PMLR.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Triastcyn and Faltings (2020) Aleksei Triastcyn 和 Boi Faltings. 2020. **机器学习的贝叶斯差分隐私**。在
    *国际机器学习会议*，页面 9583–9592。PMLR。
- en: 'Wei and Zou (2019) Jason Wei and Kai Zou. 2019. Eda: Easy data augmentation
    techniques for boosting performance on text classification tasks. *arXiv preprint
    arXiv:1901.11196*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei and Zou (2019) Jason Wei 和 Kai Zou. 2019. **Eda**：用于提升文本分类任务性能的简单数据增强技术。*arXiv
    预印本 arXiv:1901.11196*。
- en: Wu et al. (2023) Tong Wu, Ashwinee Panda, Jiachen T Wang, and Prateek Mittal.
    2023. Privacy-preserving in-context learning for large language models. *arXiv
    e-prints*, pages arXiv–2305.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) Tong Wu, Ashwinee Panda, Jiachen T Wang, 和 Prateek Mittal.
    2023. **隐私保护的上下文学习**用于大型语言模型。*arXiv e-prints*，页面 arXiv–2305。
- en: Yang et al. (2020) Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta,
    Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.
    2020. Generative data augmentation for commonsense reasoning. *arXiv preprint
    arXiv:2004.11546*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020) Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta,
    Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, 和 Doug Downey. 2020.
    **生成数据增强**用于常识推理。*arXiv 预印本 arXiv:2004.11546*。
- en: 'Yu et al. (2020) Da Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. 2020. Do
    not let privacy overbill utility: Gradient embedding perturbation for private
    learning. In *ICLR*.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2020) Da Yu, Huishuai Zhang, Wei Chen, 和 Tie-Yan Liu. 2020. 不要让隐私过度消耗效用：用于私密学习的梯度嵌入扰动。在
    *ICLR*。
- en: 'Yue et al. (2022) Xiang Yue, Huseyin A Inan, Xuechen Li, Girish Kumar, Julia
    McAnallen, Hoda Shajari, Huan Sun, David Levitan, and Robert Sim. 2022. Synthetic
    text generation with differential privacy: A simple and practical recipe. *arXiv
    preprint arXiv:2210.14348*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue et al. (2022) Xiang Yue, Huseyin A Inan, Xuechen Li, Girish Kumar, Julia
    McAnallen, Hoda Shajari, Huan Sun, David Levitan, 和 Robert Sim. 2022. **差分隐私下的合成文本生成**：一个简单而实用的配方。*arXiv
    预印本 arXiv:2210.14348*。
- en: 'Zhang et al. (2020) Rongzhi Zhang, Yue Yu, and Chao Zhang. 2020. Seqmix: Augmenting
    active sequence labeling via sequence mixup. *arXiv preprint arXiv:2010.02322*.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) Rongzhi Zhang, Yue Yu, 和 Chao Zhang. 2020. **Seqmix**：通过序列混合增强主动序列标注。*arXiv
    预印本 arXiv:2010.02322*。
- en: 'Zhu et al. (2020) Yuqing Zhu, Xiang Yu, Manmohan Chandraker, and Yu-Xiang Wang.
    2020. Private-knn: Practical differential privacy for computer vision. In *CVPR*,
    pages 11854–11862.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2020) Yuqing Zhu, Xiang Yu, Manmohan Chandraker, 和 Yu-Xiang Wang.
    2020. **Private-knn**：计算机视觉的实用差分隐私。在 *CVPR*，页面 11854–11862。
- en: Appendix A Deduction of Sensitivity $\Delta_{KD}$
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 灵敏度 $\Delta_{KD}$ 的推导
- en: We obtain the Equations ([4](#S4.E4 "In Sensitivity Analysis of Knowledge Distillation
    (KD). ‣ 4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification"))
    in §[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation
    Guided by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")
    of the paper body since $P_{i}(\mathcal{Y}|x)$.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在论文正文的 §[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data
    Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical
    Text Classification") 中获取了方程 ([4](#S4.E4 "在知识蒸馏 (KD) 的灵敏度分析中。 ‣ 4 我们方法的隐私分析 ‣
    基于 LLM 的隐私数据增强，利用知识蒸馏和分布辅导进行医学文本分类"))，因为 $P_{i}(\mathcal{Y}|x)$。
- en: '|  | $\displaystyle\Delta_{KD}$ |  | (6) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{KD}$ |  | (6) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}$
    |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}$
    |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: We know $(P_{ij}(\mathcal{Y}|x)-P_{ij}^{\prime}(\mathcal{Y}|x))^{2}$. Hence,
    we have,
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 $(P_{ij}(\mathcal{Y}|x)-P_{ij}^{\prime}(\mathcal{Y}|x))^{2}$。因此，我们有，
- en: '|  |  | $1$2 |  | (7) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (7) |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: We know $|a+b|=a+b$, so we have,
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 $|a+b|=a+b$，所以我们有，
- en: '|  |  | $\displaystyle\bigg{(}\sum_{j=1}^{&#124;\mathcal{Y}&#124;}&#124;P_{ij}(\mathcal{Y}&#124;x)+P_{ij}^{\prime}(\mathcal{Y}&#124;x)&#124;\bigg{)}^{1/2}$
    |  | (8) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\bigg{(}\sum_{j=1}^{&#124;\mathcal{Y}&#124;}&#124;P_{ij}(\mathcal{Y}&#124;x)+P_{ij}^{\prime}(\mathcal{Y}&#124;x)&#124;\bigg{)}^{1/2}$
    |  | (8) |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: In summary, the upper bound of the sensitivity is,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，敏感度的上界是，
- en: '|  | $\displaystyle\Delta_{KD}$ |  | (9) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{KD}$ |  | (9) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}=\sqrt{2}.$
    |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{i}(\mathcal{Y}&#124;x)-P_{i}^{\prime}(\mathcal{Y}&#124;x)\&#124;_{2}=\sqrt{2}.$
    |  |'
- en: '![Refer to caption](img/a26dba02dbd2af8ac3ab78929163f347.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a26dba02dbd2af8ac3ab78929163f347.png)'
- en: 'Figure 5: Label distributions. The horizontal axis enumerates all data labels,
    while the vertical axis represents the frequency of the labels.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 标签分布。水平轴列举了所有数据标签，而垂直轴表示标签的频率。'
- en: Appendix B Deduction of Sensitivity $\Delta_{tutor}$
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 敏感度的推导 $\Delta_{tutor}$
- en: We obtain the Equations ([5](#S4.E5 "In Sensitivity Analysis of Tutor. ‣ 4 Privacy
    Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided by Knowledge
    Distillation with a Distribution Tutor for Medical Text Classification")) in §[4](#S4
    "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided
    by Knowledge Distillation with a Distribution Tutor for Medical Text Classification")
    of the paper body since $P_{l}(\mathcal{C})$’s distribution.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从论文正文的§[4](#S4 "4 Privacy Analyses of our Method ‣ LLM-based Privacy Data
    Augmentation Guided by Knowledge Distillation with a Distribution Tutor for Medical
    Text Classification")中获得了方程式（[5](#S4.E5 "In Sensitivity Analysis of Tutor. ‣ 4
    Privacy Analyses of our Method ‣ LLM-based Privacy Data Augmentation Guided by
    Knowledge Distillation with a Distribution Tutor for Medical Text Classification")），这是因为
    $P_{l}(\mathcal{C})$ 的分布。
- en: '|  | $\displaystyle\Delta_{tutor}$ |  | (10) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{tutor}$ |  | (10) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}$
    |  |'
- en: '|  |  | $\displaystyle=\bigg{(}\sum_{v=1}^{&#124;\mathcal{C}&#124;}(P_{lv}(\mathcal{C})-P_{lv}^{\prime}(\mathcal{C}))^{2}\bigg{)}^{1/2}$
    |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\bigg{(}\sum_{v=1}^{&#124;\mathcal{C}&#124;}(P_{lv}(\mathcal{C})-P_{lv}^{\prime}(\mathcal{C}))^{2}\bigg{)}^{1/2}$
    |  |'
- en: Because $l$.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 $l$。
- en: '|  |  | $\displaystyle\bigg{(}\sum_{v=1}^{&#124;\mathcal{C}&#124;}(P_{lv}(\mathcal{C})-P_{lv}^{\prime}(\mathcal{C}))^{2}\bigg{)}^{1/2}$
    |  | (11) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\bigg{(}\sum_{v=1}^{&#124;\mathcal{C}&#124;}(P_{lv}(\mathcal{C})-P_{lv}^{\prime}(\mathcal{C}))^{2}\bigg{)}^{1/2}$
    |  | (11) |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: In summary, the upper bound of the sensitivity is,
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，敏感度的上界是，
- en: '|  | $\displaystyle\Delta_{tutor}$ |  | (12) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta_{tutor}$ |  | (12) |'
- en: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}\leq\sqrt{2}.$
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\leq\&#124;P_{l}(\mathcal{C})-P_{l}^{\prime}(\mathcal{C})\&#124;_{2}\leq\sqrt{2}.$
    |  |'
- en: Appendix C Detailed Deduction of Composition (KD and Tutor)
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 组合（KD 和 Tutor）的详细推导
- en: The KD algorithm $\mathcal{M}_{KD}$, there is
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: KD 算法 $\mathcal{M}_{KD}$，这里有
- en: '|  |  | $\displaystyle\Pr[(\mathcal{M}_{KD}),(\mathcal{M}_{tutor})\in(\mathcal{D}_{KD}\times\mathcal{D}_{tutor})]$
    |  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Pr[(\mathcal{M}_{KD}),(\mathcal{M}_{tutor})\in(\mathcal{D}_{KD}\times\mathcal{D}_{tutor})]$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $\displaystyle\times\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$ |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$ |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $\displaystyle\times\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$ |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$ |  |'
- en: '|  |  | $\displaystyle+\delta_{tutor}\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$
    |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\delta_{tutor}\Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $\displaystyle\times Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{tutor}$
    |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times Pr[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{tutor}$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $\displaystyle\times[Pr^{\prime}[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{KD}]+\delta_{tutor}$
    |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times[Pr^{\prime}[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{KD}]+\delta_{tutor}$
    |  |'
- en: '|  | $\displaystyle\leq$ |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ |  |'
- en: '|  |  | $\displaystyle\times Pr^{\prime}[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{KD}+\delta_{tutor}$
    |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times Pr^{\prime}[(\mathcal{M}_{KD})\in\mathcal{D}_{KD}]+\delta_{KD}+\delta_{tutor}$
    |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: '|  |  | $\displaystyle\times\Pr^{\prime}[(\mathcal{M}_{KD}),(\mathcal{M}_{tutor})\in(\mathcal{D}_{KD}\times\mathcal{D}_{tutor})]$
    |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\times\Pr^{\prime}[(\mathcal{M}_{KD}),(\mathcal{M}_{tutor})\in(\mathcal{D}_{KD}\times\mathcal{D}_{tutor})]$
    |  |'
- en: '|  |  | $\displaystyle+\delta_{KD}+\delta_{tutor}.$ |  | (13) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\delta_{KD}+\delta_{tutor}.$ |  | (13) |'
- en: Therefore, it can be concluded from the above derivation that the combination
    $\mathcal{M}$)-DP.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从上述推导可以得出结论，组合 $\mathcal{M}$)-DP。
- en: Appendix D Implementation Details
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实现细节
- en: We select “gpt-3.5-turbo” with maximum 4,096 tokens construct synthetic text.
    In the DP-based DA. We fine-tune teachers and students based on the public pre-trained
    RoBERTa-large⁴⁴4We have tried LLM (i.e. GPT-3.5) as discriminator with the accuracy
    is less than 60%. Devlin et al. ([2019](#bib.bib18)), which is representative
    and performs well in binary classification tasks. We set default number of teacher
    models is 15 and use the AdamW Loshchilov and Hutter ([2017](#bib.bib43)) optimizer
    with a learning rate of $2\times 10^{-5}$. ClinicalBERT is a BERT derivative specifically
    for clinical medicine, outperforms RoBERTa-large in the medical field. All experiments
    were performed using a single NVIDIA RTX 3090 GPU.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了最大 4,096 个标记的 “gpt-3.5-turbo” 构造合成文本。在基于 DP 的 DA 中，我们基于公开预训练的 RoBERTa-large
    进行教师和学生的微调。我们尝试过将 LLM（即 GPT-3.5）作为判别器，其准确率低于 60%。Devlin 等人 ([2019](#bib.bib18))
    的模型在二分类任务中表现良好且具有代表性。我们将默认教师模型数量设置为 15，并使用 AdamW Loshchilov 和 Hutter ([2017](#bib.bib43))
    优化器，学习率为 $2\times 10^{-5}$。ClinicalBERT 是一个专为临床医学设计的 BERT 派生模型，在医学领域优于 RoBERTa-large。所有实验均使用单个
    NVIDIA RTX 3090 GPU 进行。
- en: Appendix E Label Distributions
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 标签分布
- en: 'Fig. [5](#A1.F5 "Figure 5 ‣ Appendix A Deduction of Sensitivity Δ_{𝐾⁢𝐷} ‣ LLM-based
    Privacy Data Augmentation Guided by Knowledge Distillation with a Distribution
    Tutor for Medical Text Classification") plots two distinct label distributions:
    Private Dist represents the original private label distribution; Tutor Dist represents
    the label distribution of tutor.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#A1.F5 "图 5 ‣ 附录 A 灵敏度 Δ_{𝐾⁢𝐷} 的推导 ‣ 基于 LLM 的隐私数据增强由知识蒸馏指导，应用于医学文本分类")
    绘制了两个不同的标签分布：Private Dist 表示原始私有标签分布；Tutor Dist 表示导师的标签分布。
