- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWQ：用于 LLM 压缩和加速的激活感知权重量化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.00978](https://ar5iv.labs.arxiv.org/html/2306.00978)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.00978](https://ar5iv.labs.arxiv.org/html/2306.00978)
- en: Ji Lin^(1∗) Jiaming Tang^(1,2∗) Haotian Tang¹ Shang Yang¹ Xingyu Dang³ Chuang
    Gan¹ Song Han¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ji Lin^(1∗) Jiaming Tang^(1,2∗) Haotian Tang¹ Shang Yang¹ Xingyu Dang³ Chuang
    Gan¹ Song Han¹
- en: ¹MIT  ²SJTU  ³ Tsinghua University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹MIT  ²SJTU  ³ 清华大学
- en: '[https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/mit-han-lab/llm-awq](https://github.com/mit-han-lab/llm-awq)'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) have shown excellent performance on various tasks,
    but the astronomical model size raises the hardware barrier for serving (memory
    size) and slows down token generation (memory bandwidth). In this paper, we propose
    Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM
    low-bit weight-only quantization. Our method is based on the observation that
    weights are not equally important: protecting *only 1%* of salient weights can
    greatly reduce quantization error. We then propose to search for the optimal per-channel
    scaling that protects the salient weights by observing the *activation, not weights*.
    AWQ does not rely on any backpropagation or reconstruction, so it can well preserve
    LLMs’ generalization ability on different domains and modalities, without overfitting
    to the calibration set. AWQ outperforms existing work on various language modeling
    and domain-specific benchmarks. Thanks to better generalization, it achieves excellent
    quantization performance for *instruction-tuned* LMs and, for the first time,
    *multi-modal* LMs. Alongside AWQ, we implement an efficient and flexible inference
    framework tailored for LLMs on the edge, offering more than 3$\times$ speedup
    over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also
    democratizes the deployment of the 70B Llama-2 model on mobile GPU (NVIDIA Jetson
    Orin 64GB).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务上表现出色，但庞大的模型规模提高了硬件服务门槛（内存大小）并减慢了令牌生成速度（内存带宽）。在本文中，我们提出了一种硬件友好的低位权重量化方法——激活感知权重量化（AWQ）。我们的方法基于以下观察：权重的重要性不尽相同：保护*仅
    1%* 的显著权重可以大大减少量化误差。然后我们提出通过观察*激活而非权重*来寻找最佳的每通道缩放，从而保护显著权重。AWQ 不依赖任何反向传播或重构，因此能够很好地保留
    LLM 在不同领域和模态上的泛化能力，而不会对校准集过拟合。AWQ 在各种语言建模和领域特定基准测试中优于现有方法。由于更好的泛化能力，它在*指令调整*的
    LMs 和首次在*多模态* LMs 上都表现出色。除了 AWQ，我们还实现了一个高效且灵活的推理框架，专为边缘上的 LLM 量身定制，提供了比 Huggingface
    FP16 实现快 3$\times$ 以上的加速，适用于桌面和移动 GPU。它还使 70B Llama-2 模型在移动 GPU（NVIDIA Jetson
    Orin 64GB）上的部署变得更加普及。
- en: '^†^†footnotetext: $*$ indicates equal contributions.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注文本：$*$ 表示等同贡献。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) based on transformers [[40](#bib.bib40)] have shown
    excellent performance on various benchmarks [[4](#bib.bib4), [49](#bib.bib49),
    [38](#bib.bib38), [34](#bib.bib34)]. However, the large model size leads to the
    high serving costs. For example, GPT-3 has 175B parameters, which is 350GB in
    FP16, while the latest H100 GPU only has 96GB memory, let alone edge devices.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于变换器的大型语言模型（LLMs）[[40](#bib.bib40)] 在各种基准测试中表现出色[[4](#bib.bib4), [49](#bib.bib49),
    [38](#bib.bib38), [34](#bib.bib34)]。然而，模型规模庞大会导致高昂的服务成本。例如，GPT-3 拥有 175B 参数，占用
    350GB 的 FP16 内存，而最新的 H100 GPU 只有 96GB 内存，更不用说边缘设备了。
- en: 'Low-bit weight quantization for LLMs can save memory but is hard. Quantization-aware
    training (QAT) is not practical due to the high training cost, while post-training
    quantization (PTQ) suffers from large accuracy degradation under a low-bit setting.
    The closest work is GPTQ [[14](#bib.bib14)], which uses second-order information
    to perform error compensation. It may over-fit the calibration set during reconstruction,
    distorting the learned features on out-of-distribution domains (Figure [6](#S3.F6
    "Figure 6 ‣ 3.3 Analysis ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization
    for LLM Compression and Acceleration")), which could be problematic since LLMs
    are *generalist* models.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM进行低比特权重量化可以节省内存，但难度很大。量化感知训练（QAT）由于高训练成本而不切实际，而后训练量化（PTQ）在低比特设置下会遭遇显著的准确度下降。最接近的工作是GPTQ [[14](#bib.bib14)]，它使用二阶信息进行误差补偿。在重建过程中，它可能会过拟合校准集，扭曲在分布外领域上学到的特征（图 [6](#S3.F6
    "图 6 ‣ 3.3 分析 ‣ 3 实验 ‣ AWQ：激活感知权重量化用于LLM压缩和加速")），这可能是个问题，因为LLM是*通用型*模型。
- en: 'In this paper, we propose Activation-aware Weight Quantization (AWQ), a hardware-friendly
    low-bit weight-only quantization method for LLMs. Our method is based on the observation
    that *weights are not equally important* for LLMs’ performance. There is a small
    fraction (0.1%-1%) of *salient* weights; skipping the quantization of these salient
    weights will significantly reduce the quantization loss (Table [1](#S2.T1 "Table
    1 ‣ 2.1 Improving LLM Quantization by Preserving 1% Salient Weights ‣ 2 AWQ: Activation-aware
    Weight Quantization ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration")). To find the salient weight channels, the insight is that
    we should refer to the *activation* distribution instead of the *weight* distribution,
    despite we are doing *weight-only* quantization: weight channels corresponding
    to larger activation magnitudes are more salient since they process more important
    features. To avoid the hardware-inefficient mixed-precision implementation, we
    analyze the error from weight quantization and derive that *scaling up the salient
    channels can reduce their relative quantization error* (Equation [2](#S2.E2 "In
    2.2 Protecting Salient Weights by Activation-aware Scaling ‣ 2 AWQ: Activation-aware
    Weight Quantization ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration")). Following the intuition, we designed a per-channel scaling
    method to automatically search for the optimal scaling that minimizes the quantization
    error under full-weight quantization. AWQ does not rely on any backpropagation
    or reconstruction, so it can well preserve LLMs’ generalization ability on various
    domains and modalities without overfitting to the calibration set. Furthermore,
    we implemented an efficient serving framework to convert theoretical memory savings
    from AWQ to practical speedup. Our framework takes advantage of kernel fusion
    to minimize the inference overhead (*e.g*., intermediate DRAM access and kernel
    launch overhead), so that we can better realize the speed up from quantizing linear
    layers (AWQ is applied to linear layers which consist most of the parameters).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了激活感知权重量化（AWQ），这是一种适合硬件的低比特权重仅量化方法，用于LLM。我们的方法基于以下观察：*权重对LLM的性能并非同等重要*。有一小部分（0.1%-1%）*显著*权重；跳过这些显著权重的量化将显著降低量化损失（表 [1](#S2.T1
    "表 1 ‣ 2.1 通过保留1%显著权重来改善LLM量化 ‣ 2 AWQ：激活感知权重量化 ‣ AWQ：用于LLM压缩和加速的激活感知权重量化")）。为了找到显著权重通道，洞察是我们应该参考*激活*分布而不是*权重*分布，尽管我们进行的是*仅权重*量化：与较大激活幅度对应的权重通道更为显著，因为它们处理更重要的特征。为了避免硬件效率低下的混合精度实现，我们分析了权重量化的误差，并得出*扩大显著通道可以减少其相对量化误差*（公式 [2](#S2.E2
    "在2.2 通过激活感知缩放保护显著权重 ‣ 2 AWQ：激活感知权重量化 ‣ AWQ：用于LLM压缩和加速的激活感知权重量化")）。根据这一直觉，我们设计了一种逐通道缩放方法，以自动搜索在全权重量化下最小化量化误差的最佳缩放。AWQ
    不依赖于任何反向传播或重建，因此可以很好地保持LLM在各种领域和模式上的泛化能力，而不会过拟合于校准集。此外，我们实现了一个高效的服务框架，将AWQ的理论内存节省转化为实际加速。我们的框架利用内核融合来最小化推理开销（*例如*，中间DRAM访问和内核启动开销），从而更好地实现线性层量化带来的加速（AWQ应用于由大部分参数组成的线性层）。
- en: Experiments show that AWQ outperforms existing work on various tasks for different
    model families (*e.g*., LLaMA [[38](#bib.bib38)], OPT [[49](#bib.bib49)]) and
    model sizes. Thanks to better generalization, it also achieves good quantization
    performance for *instruction-tuned* LMs (*e.g*., Vicuna) and, for the first time,
    *multi-modal* LMs (OpenFlamingo [[2](#bib.bib2)]). With our efficient system implementation,
    we consistently observe a 3.2-3.3$\times$ average speedup compared to the FP16
    implementation by Huggingface across a diverse spectrum of LLMs. Furthermore,
    it facilitates effortless deployment of the Llama-2-70B model on a single NVIDIA
    Jetson Orin with 64GB of memory. It also democratizes LLMs with up to 13 billion
    parameters at an interactive pace of 30 tokens per second on a laptop RTX 4070
    GPU with only 8GB of memory.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，AWQ 在不同任务和模型系列（*例如*，LLaMA [[38](#bib.bib38]，OPT [[49](#bib.bib49)])及模型大小上优于现有工作。由于更好的泛化能力，它也在*指令调优*的
    LMs（*例如*，Vicuna）和首次在*多模态* LMs（OpenFlamingo [[2](#bib.bib2)]）中取得了良好的量化性能。借助我们高效的系统实现，我们在各种
    LLM 中观察到比 Huggingface 的 FP16 实现平均快 3.2-3.3$\times$。此外，它使 Llama-2-70B 模型能够轻松部署在
    64GB 内存的单个 NVIDIA Jetson Orin 上。它还使得具有多达 130 亿参数的 LLM 在仅 8GB 内存的笔记本 RTX 4070 GPU
    上以每秒 30 个标记的互动速度变得可用。
- en: AWQ has been widely adopted by various open-source LLM serving solutions including
    [FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/awq.md), [vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/quantization_utils/awq.py),
    [HuggingFace TGI](https://github.com/huggingface/text-generation-inference/pull/1054),
    [LMDeploy](https://github.com/InternLM/lmdeploy), etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AWQ 已被广泛应用于各种开源 LLM 服务解决方案，包括 [FastChat](https://github.com/lm-sys/FastChat/blob/main/docs/awq.md)、[vLLM](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/quantization_utils/awq.py)、[HuggingFace
    TGI](https://github.com/huggingface/text-generation-inference/pull/1054)、[LMDeploy](https://github.com/InternLM/lmdeploy)
    等。
- en: '2 AWQ: Activation-aware Weight Quantization'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 AWQ：激活感知权重量化
- en: '*Quantization* maps a floating-point number into lower-bit integers. It is
    an effective method to reduce the model size and inference costs of LLMs [[9](#bib.bib9),
    [14](#bib.bib14), [47](#bib.bib47), [46](#bib.bib46)]. In this section, we first
    propose a weight-only quantization method to improve accuracy *without training/regression*
    by protecting more "important" weights. And then develop a data-driven method
    to search for the optimal scaling that reduces quantization errors (Figure [1](#S2.F1
    "Figure 1 ‣ 2 AWQ: Activation-aware Weight Quantization ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration")).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*量化* 是将浮点数映射到低位整数的过程。这是一种减少模型大小和 LLM 推理成本的有效方法 [[9](#bib.bib9), [14](#bib.bib14),
    [47](#bib.bib47), [46](#bib.bib46)]。在本节中，我们首先提出了一种仅基于权重的量化方法，通过保护更多“重要”权重来提高准确性*无需训练/回归*。然后，我们开发了一种数据驱动的方法来搜索减少量化误差的最佳缩放（图
    [1](#S2.F1 "图 1 ‣ 2 AWQ：激活感知权重量化 ‣ AWQ：用于 LLM 压缩和加速的激活感知权重量化")）。'
- en: '![Refer to caption](img/50f7fe604e3504dbcef8d399021388b7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/50f7fe604e3504dbcef8d399021388b7.png)'
- en: 'Figure 1: We observe that we can find 1% of the salient weights in LLMs by
    observing the *activation distribution* (middle). Keeping the salient weights
    in FP16 can significantly improve the quantized performance (PPL from 43.2 (left)
    to 13.0 (middle)), but the mixed-precision format is not hardware-efficient. We
    follow the activation-awareness principle and propose AWQ (right). AWQ performs
    per-channel scaling to protect the salient weights, leading to reduced quantized
    error. PPL is measured with OPT-6.7B under INT3-g128 quantization.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们观察到，通过观察*激活分布*（中间），可以找到 LLMs 中 1% 的显著权重。将显著权重保留在 FP16 中可以显著提高量化性能（PPL
    从 43.2（左）提高到 13.0（中）），但混合精度格式在硬件上并不高效。我们遵循激活感知原则，提出了 AWQ（右）。AWQ 执行每通道缩放，以保护显著权重，从而减少量化误差。PPL
    是使用 OPT-6.7B 在 INT3-g128 量化下测量的。
- en: 2.1 Improving LLM Quantization by Preserving 1% Salient Weights
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 通过保留 1% 显著权重来改善 LLM 量化
- en: '| PPL $\downarrow$ | FP16 | RTN | FP16% (based on act.) | FP16% (based on W)
    | FP16% (random) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| PPL $\downarrow$ | FP16 | RTN | FP16%（基于 act.） | FP16%（基于 W） | FP16%（随机）
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| (w3-g128) | 0.1% | 1% | 3% | 0.1% | 1% | 3% | 0.1% | 1% | 3% |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| (w3-g128) | 0.1% | 1% | 3% | 0.1% | 1% | 3% | 0.1% | 1% | 3% |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| OPT-1.3B | 14.62 | 119.00 | 25.03 | 16.91 | 16.68 | 108.71 | 98.55 | 98.08
    | 119.76 | 109.38 | 61.49 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| OPT-1.3B | 14.62 | 119.00 | 25.03 | 16.91 | 16.68 | 108.71 | 98.55 | 98.08
    | 119.76 | 109.38 | 61.49 |'
- en: '| OPT-6.7B | 10.86 | 23.54 | 11.58 | 11.39 | 11.36 | 23.41 | 22.37 | 22.45
    | 23.54 | 24.23 | 24.22 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | 10.86 | 23.54 | 11.58 | 11.39 | 11.36 | 23.41 | 22.37 | 22.45
    | 23.54 | 24.23 | 24.22 |'
- en: '| OPT-13B | 10.13 | 46.04 | 10.51 | 10.43 | 10.42 | 46.07 | 48.96 | 54.49 |
    44.87 | 42.00 | 39.71 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | 10.13 | 46.04 | 10.51 | 10.43 | 10.42 | 46.07 | 48.96 | 54.49 |
    44.87 | 42.00 | 39.71 |'
- en: 'Table 1: Keeping a small fraction of weights (0.1%-1%) in FP16 significantly
    improves the performance of the quantized models over round-to-nearest (RTN).
    It is only effective when we select the important weights in FP16 by looking at
    *activation* distribution instead of *weight* distribution. We highlight results
    with a decent perplexity in green. We used INT3 quantization with a group size
    of 128 and measured the WikiText perplexity ($\downarrow$).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：将小部分权重（0.1%-1%）保留在 FP16 中显著提高了量化模型的性能，相较于四舍五入到最近（RTN）。只有在我们通过查看*激活*分布而非*权重*分布来选择
    FP16 中的重要权重时，这种方法才有效。我们用绿色突出显示了具有合理困惑度的结果。我们使用了 INT3 量化，组大小为 128，并测量了 WikiText
    的困惑度（$\downarrow$）。
- en: 'We observe that the weights of LLMs are *not equally important*: there is a
    small fraction of *salient* weights that are much more important for LLMs’ performance
    compared to others. Skipping the quantization of these salient weights can help
    bridge the performance degradation due to the quantization loss *without* any
    training or regression (Figure [1](#S2.F1 "Figure 1 ‣ 2 AWQ: Activation-aware
    Weight Quantization ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration")(b)). To verify the idea, we benchmark the performance of quantized
    LLMs when skipping part of the weight channels in Table [1](#S2.T1 "Table 1 ‣
    2.1 Improving LLM Quantization by Preserving 1% Salient Weights ‣ 2 AWQ: Activation-aware
    Weight Quantization ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration"). We measured the performance of INT3 quantized models while
    keeping some ratios of weight channels in FP16\. A widely used method to determine
    the importance of weights is to look at its magnitude or $L_{2}$-norm [[18](#bib.bib18),
    [13](#bib.bib13)]. But we find skipping the weight channels with large norm (*i.e*.,
    FP16% (based on W)) does not significantly improve the quantized performance,
    leading to a similar marginal improvement as random selection. Interestingly,
    selecting weights based on *activation magnitude* can significantly improve the
    performance: keeping only 0.1%-1% of the channels corresponding to larger activation
    significantly improves the quantized performance, even matching a strong reconstruction-based
    method GPTQ [[14](#bib.bib14)]. We hypothesize that the input features with larger
    magnitudes are generally more important. Keeping the corresponding weights in
    FP16 can preserve those features, which contributes to better model performance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到 LLMs 的权重是*不一样重要的*：有一小部分*显著*的权重对 LLMs 的性能比其他权重要重要得多。跳过这些显著权重的量化可以帮助弥补由于量化损失导致的性能下降，*无需*任何训练或回归（见图 [1](#S2.F1
    "图 1 ‣ 2 AWQ: 激活感知权重量化 ‣ AWQ: 激活感知权重量化用于 LLM 压缩与加速")(b)）。为了验证这一观点，我们在表 [1](#S2.T1
    "表 1 ‣ 2.1 通过保留 1% 显著权重来改进 LLM 量化 ‣ 2 AWQ: 激活感知权重量化 ‣ AWQ: 激活感知权重量化用于 LLM 压缩与加速")中基准测试了量化
    LLMs 的性能，当跳过部分权重通道时。我们测量了 INT3 量化模型的性能，同时保留了部分权重通道的 FP16 比例。一种广泛使用的确定权重重要性的方法是查看其大小或
    $L_{2}$-范数 [[18](#bib.bib18), [13](#bib.bib13)]。但我们发现跳过具有大范数的权重通道（*即*，基于 W 的 FP16%）并没有显著提高量化性能，导致的边际改进与随机选择相似。有趣的是，基于*激活幅度*选择权重可以显著提高性能：仅保留
    0.1%-1% 对应于较大激活的通道可以显著提高量化性能，甚至与强大的基于重建的方法 GPTQ [[14](#bib.bib14)] 相匹配。我们假设具有较大幅度的输入特征通常更重要。保留相应的
    FP16 权重可以保留这些特征，从而有助于更好的模型性能。'
- en: 'Limitations: Despite keeping 0.1% of weights in FP16 can improve the quantized
    performance without a noticeable increase in model size (measured in total bits),
    such a mixed-precision data type will make the system implementation difficult.
    We need to come up with a method to protect the important weights without actually
    keeping them as FP16.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 限制：尽管保留 0.1% 的 FP16 权重可以在模型大小（以总位数衡量）没有明显增加的情况下提高量化性能，但这种混合精度数据类型会使系统实现变得困难。我们需要提出一种方法来保护重要权重，而无需实际将它们保留为
    FP16。
- en: 2.2 Protecting Salient Weights by Activation-aware Scaling
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 通过激活感知缩放保护显著权重
- en: We propose an alternative method to reduce the quantization error of the salient
    weight by *per-channel scaling*, which does not suffer from the hardware inefficiency
    issue.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种替代方法，通过*每通道缩放*来减少显著权重的量化误差，该方法不受硬件效率问题的影响。
- en: 'Analyzing the quantization error. We start by analyzing the error from weight-only
    quantization. Consider a group/block of weight $\mathbf{w}$. Specifically, the
    quantization function is defined as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分析量化误差。我们首先分析仅针对权重的量化误差。考虑一个权重组/块 $\mathbf{w}$。具体来说，量化函数定义为：
- en: '|  | $Q(\mathbf{w})=\Delta\cdot\text{Round}(\frac{\mathbf{w}}{\Delta}),\quad\Delta=\frac{\max(&#124;\mathbf{w}&#124;)}{2^{N-1}},$
    |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(\mathbf{w})=\Delta\cdot\text{Round}(\frac{\mathbf{w}}{\Delta}),\quad\Delta=\frac{\max(&#124;\mathbf{w}&#124;)}{2^{N-1}},$
    |  | (1) |'
- en: 'where $N$, which is:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$，即：
- en: '|  | $Q(w\cdot s)\cdot\frac{x}{s}=\Delta^{{}^{\prime}}\cdot\text{Round}(\frac{ws}{\Delta})\cdot
    x\cdot\frac{1}{s},$ |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(w\cdot s)\cdot\frac{x}{s}=\Delta^{{}^{\prime}}\cdot\text{Round}(\frac{ws}{\Delta})\cdot
    x\cdot\frac{1}{s},$ |  | (2) |'
- en: where $\Delta^{{}^{\prime}}$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta^{{}^{\prime}}$。
- en: To verify the idea, we multiply the 1% salient channels with $$s>), which
    can damage the model’s overall accuracy. Therefore, we need to also consider the
    error from the non-salient channels when protecting salient ones.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一思想，我们将 1% 显著通道乘以 $$s>，这可能会损害模型的整体准确性。因此，我们还需要在保护显著通道时考虑非显著通道的误差。
- en: '| OPT-6.7B | $s=1$ |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | $s=1$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| proportion of $\Delta^{{}^{\prime}}\neq\Delta$ | 0% | 2.8% | 4.4% | 8.2%
    | 21.2% |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta^{{}^{\prime}}\neq\Delta$ 的比例 | 0% | 2.8% | 4.4% | 8.2% | 21.2% |'
- en: '| average $\Delta^{{}^{\prime}}/\Delta$ | 1 | 1.005 | 1.013 | 1.038 | 1.213
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 平均 $\Delta^{{}^{\prime}}/\Delta$ | 1 | 1.005 | 1.013 | 1.038 | 1.213 |'
- en: '| average $\frac{\Delta^{{}^{\prime}}}{\Delta}\cdot\frac{1}{s}$ (error reduction
    rate) | 1 | 0.804 | 0.676 | 0.519 | 0.303 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 平均 $\frac{\Delta^{{}^{\prime}}}{\Delta}\cdot\frac{1}{s}$（误差减少率） | 1 | 0.804
    | 0.676 | 0.519 | 0.303 |'
- en: '| Wiki-2 PPL | 23.54 | 12.87 | 12.48 | 11.92 | 12.36 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Wiki-2 PPL | 23.54 | 12.87 | 12.48 | 11.92 | 12.36 |'
- en: 'Table 2: Statistics when multiplying the 1% salient channels by <math id=$$
    will increase the quantization error for *non-salient* channels.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：当将 1% 显著通道乘以 $$<math id=$$ 将增加*非显著*通道的量化误差。
- en: '| OPT / PPL$\downarrow$ | 1.3B | 2.7B | 6.7B | 13B | 30B |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| OPT / PPL$\downarrow$ | 1.3B | 2.7B | 6.7B | 13B | 30B |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | - | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| INT3 g128 | RTN | 119.47 | 298.00 | 23.54 | 46.04 | 18.80 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| INT3 g128 | RTN | 119.47 | 298.00 | 23.54 | 46.04 | 18.80 |'
- en: '| 1% FP16 | 16.91 | 13.69 | 11.39 | 10.43 | 9.85 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1% FP16 | 16.91 | 13.69 | 11.39 | 10.43 | 9.85 |'
- en: '| $s=2$ | 18.63 | 14.94 | 11.92 | 10.80 | 10.32 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| $s=2$ | 18.63 | 14.94 | 11.92 | 10.80 | 10.32 |'
- en: '| AWQ | 16.32 | 13.58 | 11.39 | 10.56 | 9.77 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 16.32 | 13.58 | 11.39 | 10.56 | 9.77 |'
- en: 'Table 3: AWQ protects salient weights and reduces quantization error by using
    a scaling-based method. It consistently outperforms Round-to-nearest quantization
    (RTN) and achieves comparable performance as mixed-precision (1% FP16) while being
    more hardware-friendly.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：AWQ 通过使用基于缩放的方法保护显著权重并减少量化误差。它始终优于最近邻量化（RTN），并且在硬件友好性方面与混合精度（1% FP16）性能相当。
- en: 'Searching to scale. To consider both salient and non-salient weights, we choose
    to automatically search for an optimal (per input channel) scaling factor that
    minimizes the output difference after quantization for a certain layer. Formally,
    we want to optimize the following objective:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找缩放。为了同时考虑显著和非显著的权重，我们选择自动搜索一个最佳（每个输入通道）缩放因子，以最小化量化后的输出差异。形式上，我们想要优化以下目标：
- en: '|  | $\mathbf{s}^{*}=\operatorname*{arg\,min}_{\mathbf{s}}\mathcal{L}(\mathbf{s}),\quad\mathcal{L}(\mathbf{s})=\lVert
    Q(\mathbf{W}\cdot\mathbf{s})(\mathbf{s^{-1}}\cdot\mathbf{X})-\mathbf{W}\mathbf{X}\rVert$
    |  | (3) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}^{*}=\operatorname*{arg\,min}_{\mathbf{s}}\mathcal{L}(\mathbf{s}),\quad\mathcal{L}(\mathbf{s})=\lVert
    Q(\mathbf{W}\cdot\mathbf{s})(\mathbf{s^{-1}}\cdot\mathbf{X})-\mathbf{W}\mathbf{X}\rVert$
    |  | (3) |'
- en: Here $Q$, it can usually be fused into the previous operator [[44](#bib.bib44),
    [46](#bib.bib46)]. Since the quantization function is not differentiable, we are
    not able to directly optimize the problem with vanilla backpropagation. There
    are some techniques relying on approximated gradients [[3](#bib.bib3), [12](#bib.bib12)],
    which we found still suffers from unstable convergence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里$Q$，通常可以融合到之前的操作符中[[44](#bib.bib44), [46](#bib.bib46)]。由于量化函数不可导，我们无法通过普通的反向传播直接优化这个问题。有一些技术依赖于近似梯度[[3](#bib.bib3),
    [12](#bib.bib12)]，我们发现这些方法仍然存在不稳定的收敛问题。
- en: 'To make the process more stable, we define a *search space* for the optimal
    scale by analyzing the factors that will affect the choice of scaling factor.
    As shown in the last section, the saliency of weight channels is actually determined
    by the activation scale (thus “activation-awareness”). Therefore, we simply use
    a very simple search space:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使过程更加稳定，我们通过分析影响缩放因子选择的因素来定义一个*搜索空间*。如最后一节所示，权重通道的显著性实际上由激活尺度决定（因此是“激活感知”）。因此，我们只是使用一个非常简单的搜索空间：
- en: '|  | $\mathbf{s}=\mathbf{s_{X}}^{\alpha},\quad\alpha^{*}=\operatorname*{arg\,min}_{\alpha}\mathcal{L}(\mathbf{s_{X}}^{\alpha})$
    |  | (4) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{s}=\mathbf{s_{X}}^{\alpha},\quad\alpha^{*}=\operatorname*{arg\,min}_{\alpha}\mathcal{L}(\mathbf{s_{X}}^{\alpha})$
    |  | (4) |'
- en: '$\mathbf{s}$ in Equation [2](#S2.E2 "In 2.2 Protecting Salient Weights by Activation-aware
    Scaling ‣ 2 AWQ: Activation-aware Weight Quantization ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration"); thus reducing quantization
    error. We provide an ablation study on OPT models under INT3-g128 quantization
    in Table [3](#S2.T3 "Table 3 ‣ 2.2 Protecting Salient Weights by Activation-aware
    Scaling ‣ 2 AWQ: Activation-aware Weight Quantization ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration"); AWQ consistently outperforms
    round-to-nearest quantization (RTN) and achieves comparable performance as mixed-precision
    (1% FP16) while being more hardware-friendly.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '$\mathbf{s}$ 在方程[2](#S2.E2 "In 2.2 Protecting Salient Weights by Activation-aware
    Scaling ‣ 2 AWQ: Activation-aware Weight Quantization ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration")中，从而减少量化误差。我们在表[3](#S2.T3
    "Table 3 ‣ 2.2 Protecting Salient Weights by Activation-aware Scaling ‣ 2 AWQ:
    Activation-aware Weight Quantization ‣ AWQ: Activation-aware Weight Quantization
    for LLM Compression and Acceleration")中提供了OPT模型在INT3-g128量化下的消融研究；AWQ始终优于最近舍入量化（RTN），并且在硬件友好性方面性能可与混合精度（1%
    FP16）相媲美。'
- en: 'Advantages. Our method does not rely on any regression [[14](#bib.bib14)] or
    backpropagation, which is required by many quantization-aware training methods.
    It has minimal reliance on the calibration set since we only measure the average
    magnitude per channel, thus preventing over-fitting (Figure [6](#S3.F6 "Figure
    6 ‣ 3.3 Analysis ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration")). Therefore, our method requires fewer data
    for the quantization process and can preserve LLMs’ knowledge outside of the calibration
    set’s distribution. See Section [3.3](#S3.SS3 "3.3 Analysis ‣ 3 Experiments ‣
    AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration")
    for more details.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '优势。我们的方法不依赖于任何回归[[14](#bib.bib14)]或反向传播，这些通常是许多量化感知训练方法所要求的。它对标定集的依赖最小，因为我们只测量每个通道的平均幅度，从而防止了过拟合（图[6](#S3.F6
    "Figure 6 ‣ 3.3 Analysis ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization
    for LLM Compression and Acceleration")）。因此，我们的方法需要较少的数据用于量化过程，并且可以保持LLM在标定集分布之外的知识。有关更多详细信息，请参见第[3.3](#S3.SS3
    "3.3 Analysis ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration")节。'
- en: 3 Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Settings
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 设置
- en: Quantization.
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 量化。
- en: 'We focus on *weight-only grouped* quantization in this work. As shown in previous
    work [[10](#bib.bib10), [14](#bib.bib14)], grouped quantization is always helpful
    for improving performance/model size trade-off. We used a group size of 128 throughout
    the work, except otherwise specified. We focus on INT4/INT3 quantization since
    they are able to mostly preserve the LLMs’ performance [[10](#bib.bib10)]. For
    AWQ, we used a small calibration set from the Pile [[15](#bib.bib15)] dataset
    in order not to overfit to a specific downstream domain. We used a grid size of
    20 to search for the optimal $\alpha$ in Equation [4](#S2.E4 "In 2.2 Protecting
    Salient Weights by Activation-aware Scaling ‣ 2 AWQ: Activation-aware Weight Quantization
    ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration").'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在本工作中专注于*仅权重分组*量化。如之前的工作所示[[10](#bib.bib10), [14](#bib.bib14)]，分组量化总是有助于改善性能/模型大小的权衡。我们在整个工作中使用了128的组大小，除非另有说明。我们专注于INT4/INT3量化，因为它们能够在很大程度上保持LLMs的性能[[10](#bib.bib10)]。对于AWQ，我们使用了来自Pile[[15](#bib.bib15)]数据集的小型校准集，以避免过度拟合到特定的下游领域。我们使用了20的网格大小来搜索公式[4](#S2.E4
    "在2.2通过激活感知缩放保护显著权重 ‣ 2 AWQ: 激活感知权重量化 ‣ AWQ: 激活感知权重量化用于LLM压缩和加速")中的最佳$\alpha$。'
- en: Models.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型。
- en: We benchmarked our method on LLaMA [[38](#bib.bib38)] and OPT [[49](#bib.bib49)]
    families. There are other open LLMs like BLOOM [[34](#bib.bib34)], but they are
    generally worse in quality, so we do not include them in our study. We further
    benchmark an instruction-tuned model Vicuna [[6](#bib.bib6)] and visual language
    models OpenFlamingo-9B [[2](#bib.bib2)] and LLaVA-13B [[26](#bib.bib26)] to demonstrate
    the generability of our method.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LLaMA[[38](#bib.bib38)]和OPT[[49](#bib.bib49)]系列上对我们的方法进行了基准测试。还有其他开放的LLMs，如BLOOM[[34](#bib.bib34)]，但它们的质量普遍较差，因此我们没有将它们纳入研究。我们进一步基准测试了一种指令调优模型Vicuna[[6](#bib.bib6)]以及视觉语言模型OpenFlamingo-9B[[2](#bib.bib2)]和LLaVA-13B[[26](#bib.bib26)]，以展示我们方法的通用性。
- en: Evaluations.
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: Following previous literature [[9](#bib.bib9), [46](#bib.bib46), [14](#bib.bib14),
    [10](#bib.bib10), [47](#bib.bib47)], we mainly profiled the quantized models on
    language modeling tasks (perplexity evaluation on WikiText-2 [[27](#bib.bib27)])
    since perplexity can stably reflect the LLM’s performance [[10](#bib.bib10)].
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据之前的文献[[9](#bib.bib9), [46](#bib.bib46), [14](#bib.bib14), [10](#bib.bib10),
    [47](#bib.bib47)]，我们主要对量化模型在语言建模任务上的表现进行了分析（在WikiText-2[[27](#bib.bib27)]上的困惑度评估），因为困惑度可以稳定地反映LLM的性能[[10](#bib.bib10)]。
- en: Baselines.
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准。
- en: Our primary baseline is vanilla round-to-nearest quantization (RTN). It is actually
    quite strong when using a small group size like 128 [[14](#bib.bib14), [10](#bib.bib10)].
    We also compare with a state-of-the-art method GPTQ [[14](#bib.bib14)] for LLM
    weight quantization. For GPTQ, we also compare with an updated version that uses
    a “reorder” trick (denoted as GPTQ-Reorder or GPTQ-R). Other techniques like ZeroQuant [[47](#bib.bib47)],
    AdaRound [[28](#bib.bib28)], and BRECQ [[23](#bib.bib23)] rely on backpropagation
    to update the quantized weights, which may not easily scale up to large model
    sizes; they also do not outperform GPTQ [[14](#bib.bib14)], thus not included
    for study.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要基准是原始的最近舍入量化（RTN）。当使用如128的较小组大小时，它实际上表现得相当强大[[14](#bib.bib14), [10](#bib.bib10)]。我们还与一种最先进的方法GPTQ[[14](#bib.bib14)]进行比较，用于LLM权重量化。对于GPTQ，我们还比较了使用“重新排序”技巧的更新版本（标记为GPTQ-Reorder或GPTQ-R）。其他技术如ZeroQuant[[47](#bib.bib47)]、AdaRound[[28](#bib.bib28)]和BRECQ[[23](#bib.bib23)]依赖于反向传播来更新量化权重，这可能不容易扩展到大型模型；它们也没有超越GPTQ[[14](#bib.bib14)]，因此没有纳入研究。
- en: 3.2 Evaluation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估
- en: Results on LLaMA models.
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaMA模型的结果。
- en: 'We focus our study on LLaMA models (LLaMA [[38](#bib.bib38)] and Llama-2 [[39](#bib.bib39)])
    due to their superior performance compared to other open-source LLMs [[49](#bib.bib49),
    [34](#bib.bib34)]; it is also the foundation of many popular open-source models [[36](#bib.bib36),
    [6](#bib.bib6)]. We evaluate the perplexity before and after quantization in Table [4](#S3.T4
    "Table 4 ‣ Results on LLaMA models. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration"). We can see that AWQ
    consistently outperforms round-to-nearest (RTN) and GPTQ [[14](#bib.bib14)] (w/
    and w/o reordering) across different model scales (7B-70B) and generations.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的研究集中在 LLaMA 模型（LLaMA [[38](#bib.bib38)] 和 Llama-2 [[39](#bib.bib39)]）上，因为它们相比于其他开源
    LLM 的表现更为优越 [[49](#bib.bib49), [34](#bib.bib34)]；它们也是许多流行开源模型的基础 [[36](#bib.bib36),
    [6](#bib.bib6)]。我们在表 [4](#S3.T4 "Table 4 ‣ Results on LLaMA models. ‣ 3.2 Evaluation
    ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration") 中评估了量化前后的困惑度。我们可以看到，AWQ 在不同模型规模（7B-70B）和代际中始终优于四舍五入（RTN）和 GPTQ
    [[14](#bib.bib14)]（有和没有重新排序）。'
- en: '| PPL$\downarrow$ |  | Llama-2 | LLaMA |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| PPL$\downarrow$ |  | Llama-2 | LLaMA |'
- en: '| --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  | 7B | 13B | 70B | 7B | 13B | 30B | 65B |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 7B | 13B | 70B | 7B | 13B | 30B | 65B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | - | 5.47 | 4.88 | 3.32 | 5.68 | 5.09 | 4.10 | 3.53 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 5.47 | 4.88 | 3.32 | 5.68 | 5.09 | 4.10 | 3.53 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| INT3 g128 | RTN | 6.66 | 5.52 | 3.98 | 7.01 | 5.88 | 4.88 | 4.24 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| INT3 g128 | RTN | 6.66 | 5.52 | 3.98 | 7.01 | 5.88 | 4.88 | 4.24 |'
- en: '| GPTQ | 6.43 | 5.48 | 3.88 | 8.81 | 5.66 | 4.88 | 4.17 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.43 | 5.48 | 3.88 | 8.81 | 5.66 | 4.88 | 4.17 |'
- en: '| GPTQ-R | 6.42 | 5.41 | 3.86 | 6.53 | 5.64 | 4.74 | 4.21 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-R | 6.42 | 5.41 | 3.86 | 6.53 | 5.64 | 4.74 | 4.21 |'
- en: '| AWQ | 6.24 | 5.32 | 3.74 | 6.35 | 5.52 | 4.61 | 3.95 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.24 | 5.32 | 3.74 | 6.35 | 5.52 | 4.61 | 3.95 |'
- en: '| INT4 g128 | RTN | 5.73 | 4.98 | 3.46 | 5.96 | 5.25 | 4.23 | 3.67 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| INT4 g128 | RTN | 5.73 | 4.98 | 3.46 | 5.96 | 5.25 | 4.23 | 3.67 |'
- en: '| GPTQ | 5.69 | 4.98 | 3.42 | 6.22 | 5.23 | 4.24 | 3.66 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 5.69 | 4.98 | 3.42 | 6.22 | 5.23 | 4.24 | 3.66 |'
- en: '| GPTQ-R | 5.63 | 4.99 | 3.43 | 5.83 | 5.20 | 4.22 | 3.66 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-R | 5.63 | 4.99 | 3.43 | 5.83 | 5.20 | 4.22 | 3.66 |'
- en: '| AWQ | 5.60 | 4.97 | 3.41 | 5.78 | 5.19 | 4.21 | 3.62 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 5.60 | 4.97 | 3.41 | 5.78 | 5.19 | 4.21 | 3.62 |'
- en: 'Table 4: AWQ improves over round-to-nearest quantization (RTN) for different
    model sizes and different bit-precisions. It consistently achieves better perplexity
    than GPTQ (w/ and w/o reordering) on LLaMA & Llama-2 models.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：AWQ 在不同模型规模和不同位精度下，相比于四舍五入量化（RTN）有所提升。在 LLaMA 和 Llama-2 模型上，它始终实现了比 GPTQ（有和没有重新排序）更好的困惑度。
- en: '![Refer to caption](img/4c6ad3427b6d640681e8b92a51d122dc.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4c6ad3427b6d640681e8b92a51d122dc.png)'
- en: 'Figure 2: Comparing INT3-g128 quantized Vicuna models with FP16 counterparts
    under GPT-4 evaluation protocol [[6](#bib.bib6)]. More winning cases (in blue)
    indicate better performance. AWQ consistently improves the quantized performance
    compared to RTN and GPTQ [[14](#bib.bib14)], showing generalization to instruction-tuned
    models.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 GPT-4 评估协议 [[6](#bib.bib6)] 下，比较 INT3-g128 量化的 Vicuna 模型与 FP16 对应模型。更多的胜出案例（以蓝色标示）表示性能更好。AWQ
    始终在量化性能上优于 RTN 和 GPTQ [[14](#bib.bib14)]，显示出对指令调优模型的泛化能力。
- en: Quantization of instruction-tuned models.
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指令调优模型的量化。
- en: 'Instruction tuning can significantly improve the models’ performance and usability
     [[42](#bib.bib42), [33](#bib.bib33), [31](#bib.bib31), [8](#bib.bib8)]. It has
    become an essential procedure before model deployment. We further benchmark our
    method’s performance on a popular instruction-tuned model Vicuna [[6](#bib.bib6)]
    in Figure [2](#S3.F2 "Figure 2 ‣ Results on LLaMA models. ‣ 3.2 Evaluation ‣ 3
    Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and
    Acceleration"). We used the GPT-4 score to evaluate the quantized models’ performance
    against the FP16 counterpart on 80 sample questions [[6](#bib.bib6)]. We compare
    the responses with both orders (quantized-FP16, FP16-quantized) to get rid of
    the ordering effect (we found GPT-4 tends to increase the rating of the first
    input), leading to 160 trials. AWQ consistently improves the INT3-g128 quantized
    Vicuna models over RTN and GPTQ under both scales (7B and 13B), demonstrating
    the generability to instruction-tuned models.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调可以显著提高模型的性能和可用性[[42](#bib.bib42)，[33](#bib.bib33)，[31](#bib.bib31)，[8](#bib.bib8)]。它已成为模型部署前的一个重要步骤。我们进一步在流行的指令微调模型Vicuna[[6](#bib.bib6)]上基准测试我们方法的性能，如图[2](#S3.F2
    "图2 ‣ LLaMA模型的结果 ‣ 3.2 评估 ‣ 3 实验 ‣ AWQ：激活感知权重量化用于LLM压缩和加速")所示。我们使用GPT-4分数评估量化模型相对于FP16对照组的性能，测试80个样本问题[[6](#bib.bib6)]。我们将响应进行两种顺序比较（量化-FP16，FP16-量化），以消除排序效应（我们发现GPT-4倾向于提高第一个输入的评分），进行160次试验。AWQ在两种规模（7B和13B）下持续提高了INT3-g128量化Vicuna模型的性能，相比于RTN和GPTQ，展示了对指令微调模型的可生成性。
- en: '| COCO (CIDEr $\uparrow$(32-shot)* |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| COCO (CIDEr $\uparrow$(32-shot)* |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | - | 63.73 | 72.18 | 76.95 | 79.74 | 81.70 | - |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 63.73 | 72.18 | 76.95 | 79.74 | 81.70 | - |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| INT4 g128 | RTN | 60.24 | 68.07 | 72.46 | 74.09 | 77.13 | -4.57 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| INT4 g128 | RTN | 60.24 | 68.07 | 72.46 | 74.09 | 77.13 | -4.57 |'
- en: '| GPTQ | 59.72 | 67.68 | 72.53 | 74.98 | 74.98 | -6.72 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 59.72 | 67.68 | 72.53 | 74.98 | 74.98 | -6.72 |'
- en: '| AWQ | 62.57 | 71.02 | 74.75 | 78.23 | 80.53 | -1.17 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 62.57 | 71.02 | 74.75 | 78.23 | 80.53 | -1.17 |'
- en: '| INT3 g128 | RTN | 46.07 | 55.13 | 60.46 | 63.21 | 64.79 | -16.91 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| INT3 g128 | RTN | 46.07 | 55.13 | 60.46 | 63.21 | 64.79 | -16.91 |'
- en: '| GPTQ | 29.84 | 50.77 | 56.55 | 60.54 | 64.77 | -16.93 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 29.84 | 50.77 | 56.55 | 60.54 | 64.77 | -16.93 |'
- en: '| AWQ | 56.33 | 64.73 | 68.79 | 72.86 | 74.47 | -7.23 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 56.33 | 64.73 | 68.79 | 72.86 | 74.47 | -7.23 |'
- en: 'Table 5: Quantization results of a visual language model OpenFlamingo-9B [[2](#bib.bib2)]
    on COCO Captioning datasets. AWQ outperforms existing methods under zero-shot
    and various few-shot settings, demonstrating the generability to different modalities
    and in-context learning workloads. AWQ reduces the quantization degradation (32-shot)
    from 4.57 to 1.17 under INT4-g128, providing 4$\times$ model size reduction with
    negligible performance loss.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：视觉语言模型OpenFlamingo-9B在COCO Captioning数据集上的量化结果[[2](#bib.bib2)]。AWQ在零-shot和各种few-shot设置下超越了现有方法，展示了对不同模态和上下文学习负载的可生成性。AWQ将INT4-g128下的量化降级（32-shot）从4.57减少到1.17，提供了4$\times$模型大小缩减，性能损失可以忽略不计。
- en: '![Refer to caption](img/1c34fed9c68cca891498de03a8d75808.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见字幕](img/1c34fed9c68cca891498de03a8d75808.png)'
- en: 'Figure 3: Qualitative results of quantized OpenFlamingo-9B [[2](#bib.bib2)]
    on COCO captioning dataset (4-shot, INT4-g128 quantization). Our method significantly
    improves the captioning quality compared to the round-to-nearest (RTN) baseline.
    We color the text to show the correct or wrong captions.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：量化的OpenFlamingo-9B在COCO字幕数据集上的定性结果[[2](#bib.bib2)]（4-shot，INT4-g128量化）。与最近邻（RTN）基线相比，我们的方法显著提高了字幕质量。我们通过着色文本来展示正确或错误的字幕。
- en: Quantization of multi-modal language models.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模态语言模型的量化。
- en: 'Large multi-modal models (LMMs) or visual language models (VLMs) are LLMs augmented
    with vision inputs [[1](#bib.bib1), [22](#bib.bib22), [21](#bib.bib21), [11](#bib.bib11),
    [48](#bib.bib48), [26](#bib.bib26)]. Such models are able to perform text generation
    conditioned on image/video inputs. Since our method does not have the overfitting
    issue to the calibration set, it can be directly applied to VLMs to provide accurate
    and efficient quantization. We perform experiments with the OpenFlamingo-9B model [[2](#bib.bib2)]
    (an open-source reproduction of [[1](#bib.bib1)]) on COCO captioning [[5](#bib.bib5)]
    dataset (Table [5](#S3.T5 "Table 5 ‣ Quantization of instruction-tuned models.
    ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration")). We measured the average performance of 5k
    samples under different few-shot settings. We only quantize the language part
    of the model since it dominates the model size. AWQ outperforms existing methods
    under zero-shot and various few-shot settings, demonstrating the generability
    to different modalities and in-context learning workloads. It reduces the quantization
    degradation (32-shot) from 4.57 to 1.17 under INT4-g128, providing 4$\times$ model
    size reduction with negligible performance loss. We further provide some qualitative
    captioning results in Figure [3](#S3.F3 "Figure 3 ‣ Quantization of instruction-tuned
    models. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization
    for LLM Compression and Acceleration") to show our advantage over RTN. Our method
    provides a push-the-button solution for LMM/VLM quantization. It is the *first*
    study of VLM low-bit quantization to the best of our knowledge.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '大型多模态模型（LMMs）或视觉语言模型（VLMs）是通过视觉输入增强的LLMs [[1](#bib.bib1), [22](#bib.bib22),
    [21](#bib.bib21), [11](#bib.bib11), [48](#bib.bib48), [26](#bib.bib26)]。这些模型能够基于图像/视频输入进行文本生成。由于我们的方法不会出现对标定集的过拟合问题，因此可以直接应用于VLMs，以提供准确且高效的量化。我们在COCO字幕数据集 [[5](#bib.bib5)]（表 [5](#S3.T5
    "Table 5 ‣ Quantization of instruction-tuned models. ‣ 3.2 Evaluation ‣ 3 Experiments
    ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration")）上对OpenFlamingo-9B模型 [[2](#bib.bib2)]（一个 [[1](#bib.bib1)]的开源复现）进行了实验。我们在不同的少样本设置下测量了5k样本的平均性能。我们只对模型的语言部分进行量化，因为它占据了模型的主要大小。AWQ在零样本和各种少样本设置下优于现有方法，展示了其对不同模态和上下文学习工作负载的生成能力。在INT4-g128下，它将量化退化（32-shot）从4.57降低到1.17，实现了4$\times$的模型尺寸缩减，同时性能损失可忽略不计。我们进一步在图 [3](#S3.F3
    "Figure 3 ‣ Quantization of instruction-tuned models. ‣ 3.2 Evaluation ‣ 3 Experiments
    ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration")中提供了一些定性字幕结果，以展示我们相对于RTN的优势。我们的方法为LMM/VLM量化提供了一种一键解决方案。据我们所知，它是VLM低比特量化的*首个*研究。'
- en: '![Refer to caption](img/1c527b3ac8c4412741af2a7b19dd2149.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1c527b3ac8c4412741af2a7b19dd2149.png)'
- en: 'Figure 4: Visual reasoning examples from LLaVA-13B model [[26](#bib.bib26)].
    AWQ improves over the round-to-nearest (RTN) baseline, providing more reasonable
    answers. We color the text to show the correct or wrong responses.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 来自LLaVA-13B模型的视觉推理示例 [[26](#bib.bib26)]。AWQ在圆整至最近（RTN）基线的基础上有所改进，提供了更合理的答案。我们通过颜色标注文本来显示正确或错误的响应。'
- en: Visual reasoning results.
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 视觉推理结果。
- en: 'We further provide some qualitative visual reasoning examples of the LLaVA-13B [[26](#bib.bib26)]
    model in Figure [4](#S3.F4 "Figure 4 ‣ Quantization of multi-modal language models.
    ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration"). AWQ improves the responses compared to the
    round-to-nearest (RTN) baseline for INT4-g128 quantization, leading to more reasonable
    answers. In this first example, the AWQ model can understand the meme as it resembles
    the Earth when looking from space, while RTN produces wrong descriptions (marked
    in red). In the second example, AWQ correctly answers the question (the artist
    of the painting), while RTN does not provide any information about the artist.
    In the last example, RTN falsely points out a bird in the picture, while AWQ provides
    more information by noticing the image is taken in a mountain area. AWQ improves
    the visual reasoning ability of VLMs by reducing factual errors in the responses;
    RTN is not good enough even for 4 bits.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步提供了LLaVA-13B [[26](#bib.bib26)]模型的一些定性视觉推理示例，如图 [4](#S3.F4 "Figure 4 ‣
    Quantization of multi-modal language models. ‣ 3.2 Evaluation ‣ 3 Experiments
    ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration")所示。AWQ在INT4-g128量化的情况下，相比于最近邻舍入（RTN）基线，改进了响应，导致更合理的答案。在第一个示例中，AWQ模型能够理解这个表情包，因为它从太空中看像地球，而RTN则产生了错误的描述（标记为红色）。在第二个示例中，AWQ正确回答了问题（画家的名字），而RTN没有提供任何关于画家的信息。在最后一个示例中，RTN错误地指出了图片中的一只鸟，而AWQ通过注意到图像是在山区拍摄的，提供了更多的信息。AWQ通过减少响应中的事实错误，提升了视觉语言模型（VLM）的视觉推理能力；而RTN即使在4位量化下也不够好。'
- en: '| OPT / Wiki PPL$\downarrow$ | 1.3B | 2.7B | 6.7B | 13B | 30B |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| OPT / Wiki PPL$\downarrow$ | 1.3B | 2.7B | 6.7B | 13B | 30B |'
- en: '| FP16 | - | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 |'
- en: '| INT2 g64 | RTN | 10476 | 193210 | 7622 | 17564 | 8170 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| INT2 g64 | RTN | 10476 | 193210 | 7622 | 17564 | 8170 |'
- en: '| GPTQ | 46.67 | 28.15 | 16.65 | 16.74 | 11.75 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 46.67 | 28.15 | 16.65 | 16.74 | 11.75 |'
- en: '| AWQ +GPTQ | 35.71 | 25.70 | 15.71 | 13.25 | 11.38 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AWQ +GPTQ | 35.71 | 25.70 | 15.71 | 13.25 | 11.38 |'
- en: 'Table 6: Our method is orthogonal to GPTQ: it further closes the performance
    gap under extreme low-bit quantization (INT2-g64) when combined with GPTQ. Results
    are WikiText-2 perplexity of OPT models.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：我们的方法与GPTQ是正交的：当与GPTQ结合时，它进一步缩小了在极低位量化（INT2-g64）下的性能差距。结果是OPT模型的WikiText-2困惑度。
- en: Extreme low-bit quantization.
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 极低位量化。
- en: 'We further quantize LLM to INT2 to accommodate limited device memory (Table [6](#S3.T6
    "Table 6 ‣ Visual reasoning results. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware
    Weight Quantization for LLM Compression and Acceleration")). RTN completely fails,
    and AWQ brings significant perplexity improvement on top of GPTQ, though there
    is still a performance gap compared to FP16. Our method is orthogonal to GPTQ.
    We can combine our method with GPTQ to further improve the INT2 quantization performance,
    making it a more practical setting.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步将LLM量化为INT2，以适应有限的设备内存（表 [6](#S3.T6 "Table 6 ‣ Visual reasoning results.
    ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for
    LLM Compression and Acceleration")）。RTN完全失败，而AWQ在GPTQ基础上带来了显著的困惑度改进，尽管与FP16相比仍存在性能差距。我们的方法与GPTQ是正交的。我们可以将我们的方法与GPTQ结合，进一步提高INT2量化性能，使其成为更实用的设置。'
- en: '![Refer to caption](img/2a57318ca677f553252a211d8e246af5.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a57318ca677f553252a211d8e246af5.png)'
- en: 'Figure 5: AWQ provides a turn-key solution to transform the theoretical memory
    footprint reduction into a quantifiable speedup. As a result, AWQ is up to 3.9$\times$
    faster than the FP16 implementation from Huggingface on 4090 (desktop GPU) and
    Orin (mobile GPU), respectively. AWQ also democratizes Llama-2-13B deployment
    on laptop GPUs (4070) with merely 8GB memory.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：AWQ提供了一种将理论内存占用减少转化为可量化加速的解决方案。因此，AWQ在4090（桌面GPU）和Orin（移动GPU）上分别比Huggingface的FP16实现快了最多3.9$\times$。AWQ还使Llama-2-13B在仅8GB内存的笔记本GPU（4070）上得以普及。
- en: Speedup Evaluation.
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 加速评估。
- en: 'In Figure [5](#S3.F5 "Figure 5 ‣ Extreme low-bit quantization. ‣ 3.2 Evaluation
    ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration"), we demonstrate the system acceleration results for AWQ. We
    optimize both linear layers and layers that do not have quantized weights. We
    conduct benchmarking experiments on RTX 4090 (desktop GPU), RTX 4070 (laptop GPU)
    and Jetson Orin (mobile GPU). We perform batch size = 1 inference for all LLMs
    using a fixed prompt length of 4 tokens. We generate 200 tokens for each inference
    run and calculate the median latency as the final result. As in Figure [5](#S3.F5
    "Figure 5 ‣ Extreme low-bit quantization. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ:
    Activation-aware Weight Quantization for LLM Compression and Acceleration")(a),
    our system brings 2.7-3.9$\times$ speedup to three families of LLMs (Llama-2,
    MPT and Falcon) on 4090 compared with the Huggingface FP16 implementation. Notably,
    on the laptop 4070 GPU with only 8GB memory, we are still able to run Llama-2-13B
    models at 33 tokens / second, while the FP16 implementation cannot fit 7B models.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[5](#S3.F5 "Figure 5 ‣ Extreme low-bit quantization. ‣ 3.2 Evaluation ‣ 3
    Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and
    Acceleration")中，我们展示了AWQ的系统加速结果。我们优化了线性层和没有量化权重的层。我们在RTX 4090（台式GPU）、RTX 4070（笔记本GPU）和Jetson
    Orin（移动GPU）上进行基准测试。我们对所有LLM进行批量大小=1的推理，使用固定的4个token的提示长度。我们为每次推理生成200个token，并计算中位延迟作为最终结果。如图[5](#S3.F5
    "Figure 5 ‣ Extreme low-bit quantization. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ:
    Activation-aware Weight Quantization for LLM Compression and Acceleration")(a)所示，我们的系统在4090上为三种LLM（Llama-2、MPT和Falcon）带来了2.7-3.9$\times$的加速，相比之下Huggingface的FP16实现较慢。值得注意的是，在仅有8GB内存的笔记本4070
    GPU上，我们仍能以每秒33个token的速度运行Llama-2-13B模型，而FP16实现无法容纳7B模型。'
- en: 'Our system also exhibits promising performance on the NVIDIA Jetson Orin (32GB).
    As shown in Figure [5](#S3.F5 "Figure 5 ‣ Extreme low-bit quantization. ‣ 3.2
    Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM
    Compression and Acceleration")(b), our system achieves an interactive processing
    rate of 33 tokens per second when running Llama-2 models. Thanks to AWQ, even
    larger models such as MPT-30B can operate smoothly on this resource-constrained
    edge device, delivering a processing speed of 7.8 tokens per second. It’s worth
    noting that we implement the forward pass for all AWQ models using native PyTorch
    APIs, and this code is reused across various GPU architectures. Consequently,
    our system provides the best of both worlds: state-of-the-art inference speed
    and exceptional extensibility.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的系统在NVIDIA Jetson Orin（32GB）上表现出色。如图[5](#S3.F5 "Figure 5 ‣ Extreme low-bit
    quantization. ‣ 3.2 Evaluation ‣ 3 Experiments ‣ AWQ: Activation-aware Weight
    Quantization for LLM Compression and Acceleration")(b)所示，我们的系统在运行Llama-2模型时达到了每秒33个token的交互处理速度。得益于AWQ，即使是像MPT-30B这样更大的模型也能在这一资源受限的边缘设备上平稳运行，处理速度达到每秒7.8个token。值得注意的是，我们对所有AWQ模型的前向传递实现使用了原生的PyTorch
    API，并且这段代码在各种GPU架构上被重用。因此，我们的系统兼具了最先进的推理速度和卓越的可扩展性。'
- en: 3.3 Analysis
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 分析
- en: '![Refer to caption](img/fcdb3e9443adc9c4339495c46ef0df1d.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/fcdb3e9443adc9c4339495c46ef0df1d.png)'
- en: 'Figure 6: Left: AWQ needs a much smaller calibration set to reach a good quantized
    performance. It can achieve better perplexity using 10$\times$ smaller calibration
    set compared to GPTQ. Right: Our method is more robust to the calibration set
    distribution. Overall, using the same calibration and evaluation distribution
    works the best (PubMed-PubMed, Enron-Enron). But when using a different calibration
    distribution (PubMed-Enron, Enron-PubMed), AWQ only increases the perplexity by
    0.5-0.6, while GPTQ has 2.3-4.9 worse perplexity. All experiments are done with
    the OPT-6.7B model under INT3-g128 quantization.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：左：AWQ需要一个小得多的校准集来达到良好的量化性能。与GPTQ相比，它可以使用小10$\times$的校准集来实现更好的困惑度。右：我们的方法对校准集分布更具鲁棒性。总体而言，使用相同的校准和评估分布效果最佳（PubMed-PubMed，Enron-Enron）。但是，当使用不同的校准分布（PubMed-Enron，Enron-PubMed）时，AWQ仅使困惑度增加0.5-0.6，而GPTQ的困惑度则增加了2.3-4.9。所有实验均在INT3-g128量化的OPT-6.7B模型下进行。
- en: Better data-efficiency for the calibration set.
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对于校准集更好的数据效率。
- en: 'Our method requires a smaller calibration set since we do not rely on regression/backpropagation;
    we only measure the average activation scale from the calibration set, which is
    data-efficient. To demonstrate the idea, we compare the perplexity of the OPT-6.7B
    model with INT3-g128 quantization in Figure [6](#S3.F6 "Figure 6 ‣ 3.3 Analysis
    ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration") (a). AWQ needs a much smaller calibration to reach a good quantized
    performance; it can achieve better perplexity using 10$\times$ smaller calibration
    set compared to GPTQ (16 sequences *v.s.* 192 sequences).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法需要较小的校准集，因为我们不依赖回归/反向传播；我们仅测量来自校准集的平均激活规模，这在数据上更为高效。为了展示这一点，我们在图[6](#S3.F6
    "图6 ‣ 3.3 分析 ‣ 3 实验 ‣ AWQ：基于激活的权重量化用于LLM压缩和加速")(a)中比较了OPT-6.7B模型在INT3-g128量化下的困惑度。AWQ需要更小的校准集来达到良好的量化性能；使用比GPTQ小10$\times$的校准集可以实现更好的困惑度（16个序列
    *v.s.* 192个序列）。
- en: Robust to the calibration set distributions.
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对校准集分布具有鲁棒性。
- en: 'Our method is less sensitive to the calibration set distribution since we only
    measure the average activation scale from the calibration set, which is more generalizable
    across different dataset distributions. We further benchmarked the effect of the
    different calibration set distributions in Figure [6](#S3.F6 "Figure 6 ‣ 3.3 Analysis
    ‣ 3 Experiments ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration")(b). We took two subsets from the Pile dataset [[15](#bib.bib15)]:
    PubMed Abstracts and Enron Emails [[20](#bib.bib20)]. We use each of the subsets
    as the calibration set and evaluate the quantized model on both sets (the calibration
    and evaluation sets are split with no overlapping; we used 1k samples for evaluation).
    Overall, using the same calibration and evaluation distribution works the best
    (PubMed-PubMed, Enron-Enron). But when using a different calibration distribution
    (PubMed-Enron, Enron-PubMed), AWQ only increases the perplexity by 0.5-0.6, while
    GPTQ has 2.3-4.9 worse perplexity. This demonstrates the robustness of AWQ to
    the calibration set distribution.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法对校准集分布不太敏感，因为我们仅测量来自校准集的平均激活规模，这在不同数据集分布中更具普适性。我们进一步在图[6](#S3.F6 "图6 ‣
    3.3 分析 ‣ 3 实验 ‣ AWQ：基于激活的权重量化用于LLM压缩和加速")(b)中基准测试了不同校准集分布的效果。我们从Pile数据集[[15](#bib.bib15)]中取了两个子集：PubMed摘要和Enron邮件[[20](#bib.bib20)]。我们使用每个子集作为校准集，并在两个集合上评估量化模型（校准集和评估集不重叠；我们使用了1k样本进行评估）。总体而言，使用相同的校准和评估分布效果最好（PubMed-PubMed，Enron-Enron）。但当使用不同的校准分布（PubMed-Enron，Enron-PubMed）时，AWQ仅使困惑度增加0.5-0.6，而GPTQ则增加2.3-4.9。这表明AWQ对校准集分布的鲁棒性。
- en: 4 Related Work
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Model quantization methods.
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型量化方法。
- en: 'Quantization reduces the bit-precision of deep learning models [[17](#bib.bib17),
    [19](#bib.bib19), [29](#bib.bib29), [41](#bib.bib41), [28](#bib.bib28), [25](#bib.bib25)],
    which helps to reduce the model size and accelerate inference. Quantization techniques
    generally fall into two categories: quantization-aware training (QAT, which relies
    on backpropagation to update the quantized weights) [[3](#bib.bib3), [16](#bib.bib16),
    [30](#bib.bib30), [7](#bib.bib7)] and post-training quantization [[19](#bib.bib19),
    [29](#bib.bib29), [28](#bib.bib28)] (PTQ, usually training-free). The QAT methods
    cannot easily scale up to large models like LLMs. Therefore, people usually use
    PTQ methods to quantize LLMs.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 量化降低了深度学习模型的位精度[[17](#bib.bib17), [19](#bib.bib19), [29](#bib.bib29), [41](#bib.bib41),
    [28](#bib.bib28), [25](#bib.bib25)]，这有助于减少模型大小和加速推理。量化技术通常分为两类：量化感知训练（QAT，依赖反向传播更新量化权重）[[3](#bib.bib3),
    [16](#bib.bib16), [30](#bib.bib30), [7](#bib.bib7)] 和训练后量化[[19](#bib.bib19), [29](#bib.bib29),
    [28](#bib.bib28)]（PTQ，通常无需训练）。QAT方法不容易扩展到像LLM这样的较大模型。因此，人们通常使用PTQ方法来量化LLM。
- en: Quantization of LLMs.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs的量化。
- en: 'People study two settings for LLM quantization: (1) W8A8 quantization, where
    both activation and weights are quantized to INT8 [[9](#bib.bib9), [46](#bib.bib46),
    [47](#bib.bib47), [45](#bib.bib45), [43](#bib.bib43)]; (2) Low-bit weight-only
    quantization (*e.g*., W4A16), where only weights are quantized into low-bit integers [[14](#bib.bib14),
    [10](#bib.bib10), [35](#bib.bib35), [32](#bib.bib32)]. We focus on the second
    setting in this work since it not only reduces the hardware barrier (requiring
    a smaller memory size) but also speeds up the token generation (remedies memory-bound
    workload). Apart from the vanilla round-to-nearest baseline (RTN), GPTQ [[14](#bib.bib14)]
    is the closest to our work. However, the reconstruction process of GPTQ leads
    to an over-fitting issue to the calibration set and may not preserve the generalist
    abilities of LLMs for other modalities and domains. It also requires a reordering
    trick to work for some models (*e.g*., LLaMA-7B [[38](#bib.bib38)] and OPT-66B [[49](#bib.bib49)]).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 人们研究了LLM量化的两种设置：（1）W8A8量化，其中激活值和权重都量化为INT8 [[9](#bib.bib9), [46](#bib.bib46),
    [47](#bib.bib47), [45](#bib.bib45), [43](#bib.bib43)];（2）低比特权重-only量化（*例如*，W4A16），其中仅权重量化为低比特整数 [[14](#bib.bib14),
    [10](#bib.bib10), [35](#bib.bib35), [32](#bib.bib32)]。我们在本研究中集中于第二种设置，因为它不仅减少了硬件障碍（需要更小的内存大小），而且加快了令牌生成（补救了内存绑定的工作负载）。除了普通的四舍五入基线（RTN）外，GPTQ [[14](#bib.bib14)]
    是最接近我们工作的。然而，GPTQ的重建过程导致对校准集的过拟合问题，并可能无法保留LLMs在其他模态和领域中的通用能力。它还需要一个重新排序技巧来使某些模型（*例如*，LLaMA-7B [[38](#bib.bib38)]
    和OPT-66B [[49](#bib.bib49)]）有效。
- en: System support for low-bit quantized LLMs.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 系统支持低比特量化的LLMs。
- en: Low-bit quantized LLMs have been a popular setting to reduce inference costs.
    There are some system supports to achieve a practical speed-up. GPTQ [[14](#bib.bib14)]
    provides INT3 kernels for OPT models and GPTQ-for-LLaMA extends kernel support
    for INT4 reordered quantization with the help of Triton [[37](#bib.bib37)]. FlexGen [[35](#bib.bib35)]
    and llama.cpp^*^**https://github.com/ggerganov/llama.cpp perform group-wise INT4
    quantization to reduce I/O costs and offloading. FasterTransformer^†^††https://github.com/NVIDIA/FasterTransformer
    implements FP16$\times$ speedup over the FP16 implementation from Huggingface.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 低比特量化LLMs已经成为减少推理成本的热门设置。为了实现实际的加速，有一些系统支持。GPTQ [[14](#bib.bib14)] 为OPT模型提供了INT3内核，GPTQ-for-LLaMA在Triton [[37](#bib.bib37)]的帮助下扩展了对INT4重新排序量化的内核支持。FlexGen [[35](#bib.bib35)]
    和 llama.cpp^*^**https://github.com/ggerganov/llama.cpp 进行组级INT4量化，以减少I/O成本和卸载。FasterTransformer^†^††https://github.com/NVIDIA/FasterTransformer
    实现了FP16$\times$相较于Huggingface的FP16实现的加速。
- en: 5 Conclusion
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we propose Activation-aware Weight Quantization (AWQ), a simple
    yet effective method for low-bit weight-only LLM compression AWQ is based on the
    observation that weights are not equally important in LLMs and performs per-channel
    scaling to reduce the quantization loss of salient weights. AWQ does not over-fit
    the calibration set and preserves the generalist abilities of LLMs in various
    domains and modalities. It outperforms existing work on language modeling and
    can be applicable to instruction-tuned LMs and multi-modal LMs. Our system implementation
    further translates the theoretical memory savings achieved by AWQ into 3.2-3.3$\times$
    measured speedups over the FP16 implementations from Huggingface on desktop and
    mobile GPUs, democratizing LLM deployment on the edge.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了激活感知权重量化（AWQ），这是一种简单而有效的低比特权重-only LLM压缩方法。AWQ基于观察，即权重在LLMs中并不是同等重要的，并对每个通道进行缩放，以减少显著权重的量化损失。AWQ不会对校准集过拟合，并保留LLMs在各种领域和模态中的通用能力。它在语言建模方面优于现有工作，并且可以应用于指令调整的语言模型和多模态LLMs。我们的系统实现进一步将AWQ理论上实现的内存节省转化为桌面和移动GPU上FP16实现的3.2-3.3$\times$加速，使LLM部署在边缘变得更加普及。
- en: Acknowledgements
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank MIT AI Hardware Program, National Science Foundation, NVIDIA Academic
    Partnership Award, MIT-IBM Watson AI Lab, Amazon and MIT Science Hub, Qualcomm
    Innovation Fellowship, Microsoft Turing Academic Program for supporting this research.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢MIT人工智能硬件项目、国家科学基金、NVIDIA学术伙伴奖、MIT-IBM沃森人工智能实验室、亚马逊和MIT科学中心、高通创新奖学金、微软图灵学术项目对本研究的支持。
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
    Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
    et al. Flamingo: a visual language model for few-shot learning. Advances in Neural
    Information Processing Systems, 35:23716–23736, 2022.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
    Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds等人.
    Flamingo: 一种用于少样本学习的视觉语言模型. 《神经信息处理系统进展》，第35卷:23716–23736, 2022年。'
- en: '[2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
    Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,
    Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo,
    March 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
    Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith,
    Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, 和 Ludwig Schmidt. Openflamingo,
    2023年3月。'
- en: '[3] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. Estimating or propagating
    gradients through stochastic neurons for conditional computation. arXiv preprint
    arXiv:1308.3432, 2013.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Yoshua Bengio, Nicholas Léonard, 和 Aaron Courville. 估计或传播随机神经元的梯度以进行条件计算.
    arXiv预印本 arXiv:1308.3432, 2013年。'
- en: '[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
    and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
    pages 1877–1901\. Curran Associates, Inc., 2020.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei. 语言模型是少样本学习者.
    收录于 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan 和 H. Lin 主编的《神经信息处理系统进展》，第33卷,
    页1877–1901. Curran Associates, Inc., 2020年。'
- en: '[5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta,
    Piotr Dollár, and C Lawrence Zitnick. Microsoft coco captions: Data collection
    and evaluation server. arXiv preprint arXiv:1504.00325, 2015.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta,
    Piotr Dollár, 和 C Lawrence Zitnick. Microsoft coco captions: 数据收集和评估服务器. arXiv预印本
    arXiv:1504.00325, 2015年。'
- en: '[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality, March 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing. Vicuna: 一种开源聊天机器人，在90%*的ChatGPT质量上令人印象深刻, 2023年3月。'
- en: '[7] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi
    Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized clipping activation
    for quantized neural networks. arXiv preprint arXiv:1805.06085, 2018.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce I-Jen Chuang, Vijayalakshmi
    Srinivasan, 和 Kailash Gopalakrishnan. Pact: 用于量化神经网络的参数化剪切激活. arXiv预印本 arXiv:1805.06085,
    2018年。'
- en: '[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
    language models. arXiv preprint arXiv:2210.11416, 2022.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma等人. 扩展指令微调语言模型. arXiv预印本
    arXiv:2210.11416, 2022年。'
- en: '[9] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8():
    8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm.int8():
    用于大规模变换器的8位矩阵乘法. arXiv预印本 arXiv:2208.07339, 2022年。'
- en: '[10] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Tim Dettmers 和 Luke Zettlemoyer. 支持4位精度的案例：k位推理扩展定律. arXiv预印本 arXiv:2212.09720,
    2022年。'
- en: '[11] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
    Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e:
    An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
    Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu等人. Palm-e:
    一种具身多模态语言模型. arXiv预印本 arXiv:2303.03378, 2023年。'
- en: '[12] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
    and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153,
    2019.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy
    和 Dharmendra S Modha。学习的步长量化。arXiv 预印本 arXiv:1902.08153, 2019。'
- en: '[13] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding
    sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Jonathan Frankle 和 Michael Carbin。彩票票据假设：寻找稀疏、可训练的神经网络。arXiv 预印本 arXiv:1803.03635,
    2018。'
- en: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323, 2022。'
- en: '[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An
    800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027,
    2020.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
    Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, 等人。The pile：一个 800GB
    多样文本数据集，用于语言建模。arXiv 预印本 arXiv:2101.00027, 2020。'
- en: '[16] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    arXiv preprint arXiv:2103.13630, 2021.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney 和 Kurt
    Keutzer。高效神经网络推理的量化方法综述。arXiv 预印本 arXiv:2103.13630, 2021。'
- en: '[17] Song Han, Huizi Mao, and William J Dally. Deep Compression: Compressing
    Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. In
    ICLR, 2016.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Song Han, Huizi Mao 和 William J Dally。深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。在 ICLR，2016。'
- en: '[18] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights
    and connections for efficient neural network. Advances in neural information processing
    systems, 28, 2015.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Song Han, Jeff Pool, John Tran 和 William Dally。为高效神经网络学习权重和连接。神经信息处理系统进展，28，2015。'
- en: '[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training
    of neural networks for efficient integer-arithmetic-only inference. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2704–2713,
    2018.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang,
    Andrew Howard, Hartwig Adam 和 Dmitry Kalenichenko。神经网络的量化与训练以实现高效的整数运算推理。在 IEEE
    计算机视觉与模式识别会议论文集，页码 2704–2713, 2018。'
- en: '[20] Bryan Klimt and Yiming Yang. The enron corpus: A new dataset for email
    classification research. In Machine Learning: ECML 2004: 15th European Conference
    on Machine Learning, Pisa, Italy, September 20-24, 2004\. Proceedings 15, pages
    217–226\. Springer, 2004.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Bryan Klimt 和 Yiming Yang。恩龙语料库：用于电子邮件分类研究的新数据集。在机器学习：ECML 2004：第十五届欧洲机器学习会议，意大利比萨，2004年9月20-24日。论文集
    15, 页码 217–226。施普林格，2004。'
- en: '[21] Jing Yu Koh, Ruslan Salakhutdinov, and Daniel Fried. Grounding language
    models to images for multimodal generation. arXiv preprint arXiv:2301.13823, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jing Yu Koh, Ruslan Salakhutdinov 和 Daniel Fried。将语言模型与图像对接以实现多模态生成。arXiv
    预印本 arXiv:2301.13823, 2023。'
- en: '[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    arXiv preprint arXiv:2301.12597, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Junnan Li, Dongxu Li, Silvio Savarese 和 Steven Hoi。Blip-2：利用冻结的图像编码器和大型语言模型进行语言-图像预训练。arXiv
    预印本 arXiv:2301.12597, 2023。'
- en: '[23] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang 和 Shi Gu。Brecq：通过块重构推动后训练量化的极限。arXiv 预印本 arXiv:2102.05426, 2021。'
- en: '[24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al.
    Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
    Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, 等人。语言模型的整体评估。arXiv
    预印本 arXiv:2211.09110, 2022。'
- en: '[25] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, et al. Mcunet:
    Tiny deep learning on iot devices. Advances in Neural Information Processing Systems,
    33:11711–11722, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Ji Lin, Wei-Ming Chen, Yujun Lin, Chuang Gan, Song Han, 等人。Mcunet：物联网设备上的微型深度学习。神经信息处理系统进展，33:11711–11722,
    2020。'
- en: '[26] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee. 视觉指令调优。2023年。'
- en: '[27] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models, 2016.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher. 指针哨兵混合模型，2016年。'
- en: '[28] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen
    Blankevoort. Up or down? adaptive rounding for post-training quantization. In
    International Conference on Machine Learning, pages 7197–7206\. PMLR, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos 和 Tijmen
    Blankevoort. 上升还是下降？用于后训练量化的自适应舍入。在国际机器学习会议上，页码 7197–7206。PMLR，2020年。'
- en: '[29] Markus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free
    quantization through weight equalization and bias correction. In Proceedings of
    the IEEE/CVF International Conference on Computer Vision, pages 1325–1334, 2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Markus Nagel, Mart van Baalen, Tijmen Blankevoort 和 Max Welling. 通过权重均衡和偏差修正实现无数据量化。在
    IEEE/CVF 国际计算机视觉会议论文集中，页码 1325–1334，2019年。'
- en: '[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
    Mart Van Baalen, and Tijmen Blankevoort. A white paper on neural network quantization.
    arXiv preprint arXiv:2106.08295, 2021.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei Bondarenko,
    Mart Van Baalen 和 Tijmen Blankevoort. 关于神经网络量化的白皮书。arXiv 预印本 arXiv:2106.08295，2021年。'
- en: '[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in Neural Information Processing Systems, 35:27730–27744, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人。通过人类反馈训练语言模型以遵循指令。神经信息处理系统进展，35:27730–27744，2022年。'
- en: '[32] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee,
    and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale
    generative language models. arXiv preprint arXiv:2206.09557, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee
    和 Dongsoo Lee. nuqmm：用于大规模生成语言模型高效推理的量化矩阵乘法。arXiv 预印本 arXiv:2206.09557，2022年。'
- en: '[33] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,
    Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al.
    Multitask prompted training enables zero-shot task generalization. arXiv preprint
    arXiv:2110.08207, 2021.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika,
    Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja 等人。多任务提示训练实现零样本任务泛化。arXiv
    预印本 arXiv:2110.08207，2021年。'
- en: '[34] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé 等人。Bloom：一个具有 176 亿参数的开放访问多语言模型。arXiv 预印本 arXiv:2211.05100，2022年。'
- en: '[35] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y
    Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez, et al. High-throughput
    generative inference of large language models with a single gpu. arXiv preprint
    arXiv:2303.06865, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel
    Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez 等人。利用单个 GPU 高通量生成推理大型语言模型。arXiv
    预印本 arXiv:2303.06865，2023年。'
- en: '[36] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An
    instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li,
    Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto. Stanford alpaca：一个跟随指令的
    llama 模型。[https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023年。'
- en: '[37] Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Triton: an intermediate
    language and compiler for tiled neural network computations. In Proceedings of
    the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages, pages 10–19, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Philippe Tillet, Hsiang-Tsung Kung 和 David Cox. Triton：一种用于分块神经网络计算的中间语言和编译器。在第
    3 届 ACM SIGPLAN 国际机器学习与编程语言研讨会上，页码 10–19，2019年。'
- en: '[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等人。《Llama: 开放和高效的基础语言模型》。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: '[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等人。《Llama 2: 开放的基础和微调的聊天模型》。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。《Attention is all you need》。神经信息处理系统的进展，30,
    2017。'
- en: '[41] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. HAQ: Hardware-Aware
    Automated Quantization with Mixed Precision. In CVPR, 2019.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin 和 Song Han。《HAQ: 硬件感知自动化量化与混合精度》。在
    CVPR, 2019。'
- en: '[42] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
    Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot
    learners. arXiv preprint arXiv:2109.01652, 2021.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
    Lester, Nan Du, Andrew M Dai 和 Quoc V Le。《微调语言模型是零-shot 学习者》。arXiv 预印本 arXiv:2109.01652,
    2021。'
- en: '[43] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145,
    2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo 和 Xianglong Liu。《异常值抑制+: 通过等效和优化的平移和缩放对大型语言模型进行准确量化》。arXiv 预印本 arXiv:2304.09145,
    2023。'
- en: '[44] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models. arXiv preprint arXiv:2209.13325, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu 和 Xianglong Liu。《异常值抑制：推动低位变压器语言模型的极限》。arXiv 预印本 arXiv:2209.13325,
    2022。'
- en: '[45] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu 和 Xianglong Liu。《异常值抑制：推动低位变压器语言模型的极限》，2022。'
- en: '[46] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth 和 Song Han。《Smoothquant:
    大型语言模型的准确而高效的后训练量化》。arXiv 预印本 arXiv:2211.10438, 2022。'
- en: '[47] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He。《Zeroquant: 高效且实惠的大规模变压器后训练量化》，2022。'
- en: '[48] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
    Hongsheng Li, Peng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language
    models with zero-init attention. arXiv preprint arXiv:2303.16199, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
    Hongsheng Li, Peng Gao 和 Yu Qiao。《Llama-adapter: 使用零初始化注意力的语言模型高效微调》。arXiv 预印本
    arXiv:2303.16199, 2023。'
- en: '[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer
    language models, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang 和 Luke Zettlemoyer。《Opt: 开放预训练的变压器语言模型》，2022。'
- en: Appendix A Broader Impacts and Limitations
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更广泛的影响和局限性
- en: Broader impacts. In this paper, we propose a general technique to enable accurate
    and efficient low-bit weight-only quantization of large language models (LLMs).
    It makes LLMs more efficient and accessible and thus may inherit the impacts of
    LLMs. On the positive side, quantization helps to democratize LLMs, which helps
    to benefit more people (especially those with lower income). It reduces the costs
    and hardware barrier of deploying LLMs and facilitates edge inference of these
    models, addressing the data privacy issue (since we no longer need to send data
    to the cloud). On the negative side, LLMs may be exploited by malicious users
    to produce misinformation and manipulation. Quantization can not prevent such
    negative effects but it does not make it worse.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛的影响。在本文中，我们提出了一种通用技术，以实现大规模语言模型（LLMs）的精确和高效的低比特权重量化。它使 LLMs 更加高效和可及，从而可能继承
    LLMs 的影响。在积极的一面，量化有助于普及 LLMs，这有助于更多人受益（尤其是那些低收入者）。它减少了部署 LLMs 的成本和硬件障碍，并促进了这些模型的边缘推断，解决了数据隐私问题（因为我们不再需要将数据发送到云端）。在消极的一面，LLMs
    可能会被恶意用户利用来制造虚假信息和操控。量化不能防止这些负面影响，但也不会使情况更糟。
- en: Limitations. In this paper, we follow previous work [[9](#bib.bib9), [14](#bib.bib14),
    [46](#bib.bib46), [47](#bib.bib47), [10](#bib.bib10)] to mostly benchmark the
    quantized models on standard accuracy metrics like perplexity and accuracy. However,
    besides accuracy, there are other important metrics for LLM benchmark like robustness,
    fairness, bias, toxicity, helpfulness, calibration, *etc*. [[24](#bib.bib24)].
    We think it would be helpful to perform a more holistic evaluation of quantized
    LLMs covering these aspects, which we leave to future work. Furthermore, we only
    study low-bit integer quantization of LLMs due to easier data type casting on
    hardware. There might be a further improvement from changing data types (*e.g*.,
    FP4 [[10](#bib.bib10)]), which we do not include in the study.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性。在本文中，我们遵循了之前的工作 [[9](#bib.bib9), [14](#bib.bib14), [46](#bib.bib46), [47](#bib.bib47),
    [10](#bib.bib10)]，主要在标准准确性指标如困惑度和准确性上基准测试量化模型。然而，除了准确性，还有其他重要的 LLM 基准指标，如鲁棒性、公平性、偏见、毒性、帮助性、校准、*等等*
    [[24](#bib.bib24)]。我们认为，对量化 LLM 进行更全面的评估，涵盖这些方面，将是有益的，这将留待未来的工作。此外，由于硬件上数据类型转换更容易，我们只研究了
    LLM 的低比特整数量化。通过改变数据类型（*例如*，FP4 [[10](#bib.bib10)]）可能会有进一步的改进，但我们在研究中没有包含。
- en: Appendix B Amount of Computation
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 计算量
- en: We study the post-training quantization (PTQ) of LLMs in this work. The computation
    requirement is generally modest since we do not rely on any backpropagation. We
    used one NVIDIA A100 GPU for smaller models (<40B parameters) and 2-4 A100 GPUs
    for larger models due to memory limits.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这项工作中研究了 LLMs 的后训练量化（PTQ）。由于我们不依赖任何反向传播，计算需求通常适中。由于内存限制，我们对较小的模型（<40B 参数）使用了一台
    NVIDIA A100 GPU，对较大的模型使用了 2-4 台 A100 GPU。
- en: 'The quantization process is generally fast, requiring a few GPU hours (ranging
    from 0.1 to 3, depending on the model size). The accuracy measurement time depends
    on the model and dataset sizes: testing LLaMA-65B (the biggest model we tested
    on multiple datasets) on 4 common sense QA tasks requires 3 GPU hours; testing
    it on MMLU (consisting of 57 sub-datasets) requires 5 GPU hours. The GPU hours
    would be smaller for smaller models and datasets (*e.g*., WikiText-2).'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 量化过程通常很快，通常需要几个 GPU 小时（根据模型大小，从 0.1 到 3 不等）。准确度测量时间取决于模型和数据集的大小：在 4 个常识问答任务上测试
    LLaMA-65B（我们在多个数据集上测试的最大模型）需要 3 个 GPU 小时；在 MMLU（包含 57 个子数据集）上测试需要 5 个 GPU 小时。对于较小的模型和数据集（*例如*，WikiText-2），所需的
    GPU 小时会更少。
- en: Appendix C Limitation with No-group Quantization
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 无组量化的局限性
- en: 'Our method searches for good scaling to protect the salient weight channels.
    It works pretty well under grouped quantization, matching the same accuracy as
    keeping salient weights in FP16 (Figure [1](#S2.F1 "Figure 1 ‣ 2 AWQ: Activation-aware
    Weight Quantization ‣ AWQ: Activation-aware Weight Quantization for LLM Compression
    and Acceleration")). However, such a scaling-based method can only protect *one*
    salient channel for *each group*. It is not a problem for grouped quantization
    (we only need to protect 0.1%-1% of salient channels, the group size is usually
    small, like 128, so we need to protect fewer than 1 channel in each group on average).
    But for no-group quantization, we can only protect one input channel for the *entire
    weight*, which may not be enough to bridge the performance degradation. As shown
    in Table [7](#A3.T7 "Table 7 ‣ Appendix C Limitation with No-group Quantization
    ‣ AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"),
    under INT3-g128 quantization, AWQ achieves similar performance compared to keeping
    1% salient weights in FP16\. While under INT3 no-group quantization, there is
    still a noticeable gap. Nonetheless, we want to stress that the performance of
    no-group quantization is still far behind grouped quantization at a similar cost.
    Therefore, grouped quantization is a *more practical solution* for LLM compression
    for edge deployment and AWQ can effectively improve the quantized performance
    under this setting.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法寻找合适的缩放来保护显著权重通道。在分组量化下效果很好，与保留显著权重在 FP16 下的准确性相匹配（图 [1](#S2.F1 "图 1 ‣
    2 AWQ：激活感知权重量化 ‣ AWQ：激活感知权重量化用于 LLM 压缩和加速")）。然而，这种基于缩放的方法只能保护每组中的*一个*显著通道。这在分组量化下不是问题（我们只需保护
    0.1%-1% 的显著通道，组大小通常很小，如 128，因此我们平均每组中需保护不到 1 个通道）。但对于无分组量化，我们只能保护*整个权重*的一个输入通道，这可能不足以弥补性能下降。如表 [7](#A3.T7
    "表 7 ‣ 附录 C 无分组量化的局限性 ‣ AWQ：激活感知权重量化用于 LLM 压缩和加速") 所示，在 INT3-g128 量化下，AWQ 实现了类似于保留
    1% 显著权重在 FP16 下的性能。而在 INT3 无分组量化下，仍然存在明显差距。尽管如此，我们要强调的是，无分组量化的性能仍远落后于在相似成本下的分组量化。因此，分组量化是用于边缘部署的
    LLM 压缩的*更实际的解决方案*，而 AWQ 能有效提高在这种设置下的量化性能。
- en: '| PPL $\downarrow$ | FP16 | INT3 (group 128) | INT3 (no group) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| PPL $\downarrow$ | FP16 | INT3（组 128） | INT3（无组） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RTN | 1% FP16 | AWQ | RTN | 1% FP16 | AWQ |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 1% FP16 | AWQ | RTN | 1% FP16 | AWQ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OPT-6.7B | 12.29 | 43.16 | 13.02 | 12.99 | 21160 | 14.67 | 18.11 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | 12.29 | 43.16 | 13.02 | 12.99 | 21160 | 14.67 | 18.11 |'
- en: '| LLaMA-7B | 9.49 | 12.10 | 10.77 | 10.82 | 50.45 | 14.06 | 20.52 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 9.49 | 12.10 | 10.77 | 10.82 | 50.45 | 14.06 | 20.52 |'
- en: 'Table 7: AWQ can match the performance of keeping 1% salient weights in FP16
    under grouped quantization without introducing mixed-precisions, but not for no-group
    quantization. Nonetheless, grouped quantization has a far better performance compared
    to no-group, making it a far more practical setting for weight-only quantization
    of LLMs, while AWQ performs quite well under this setting. Results are perplexity
    on the WikiText-2 dataset.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：AWQ 在分组量化下能够匹配保留 1% 显著权重的 FP16 性能，而不会引入混合精度，但对于无分组量化则不适用。然而，相较于无分组量化，分组量化具有更好的性能，使其成为权重仅量化的大型语言模型（LLM）的更实际的设置，而
    AWQ 在这种设置下表现良好。结果是在 WikiText-2 数据集上的困惑度。
