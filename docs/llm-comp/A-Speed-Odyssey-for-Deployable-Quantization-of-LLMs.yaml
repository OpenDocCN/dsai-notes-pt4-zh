- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: A Speed Odyssey for Deployable Quantization of LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可部署量化的LLMs加速探索
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.09550](https://ar5iv.labs.arxiv.org/html/2311.09550)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.09550](https://ar5iv.labs.arxiv.org/html/2311.09550)
- en: Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu,
    Yerui Sun, Yuchen Xie
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李庆元、孟然、李易多、张博、李亮、卢亦凡、楚向翔、孙也睿、谢宇辰
- en: Meituan Inc.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 美团公司
- en: '{liqingyuan02,mengran03,liyiduo,zhangbo97,liliang58}@meituan.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liqingyuan02,mengran03,liyiduo,zhangbo97,liliang58}@meituan.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The large language model era urges faster and less costly inference. Prior model
    compression works on LLMs tend to undertake a software-centric approach primarily
    focused on the simulated quantization performance. By neglecting the feasibility
    of deployment, these approaches are typically disabled in real practice. They
    used to drastically push down the quantization bit range for a reduced computation
    which might not be supported by the mainstream hardware, or involve sophisticated
    algorithms that introduce extra computation or memory access overhead. We argue
    that pursuing a hardware-centric approach in the construction of quantization
    algorithms is crucial. In this regard, we are driven to build our compression
    method on top of hardware awareness, eliminating impractical algorithm choices
    while maximizing the benefit of hardware acceleration. Our method, OdysseyLLM,
    comes with a novel W4A8 kernel implementation called FastGEMM and a combined recipe
    of quantization strategies. Extensive experiments manifest the superiority of
    our W4A8 method which brings the actual speed boosting up to 4$\times$ vs. TensorRT-LLM
    in INT8, yet without substantially harming the performance.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型时代要求更快、更经济的推理。此前对LLMs的模型压缩工作通常采取以软件为中心的方法，主要关注模拟量化性能。由于忽视了部署的可行性，这些方法在实际操作中通常无法使用。它们通常会大幅降低量化位范围以减少计算，这可能不被主流硬件支持，或涉及复杂的算法，导致额外的计算或内存访问开销。我们认为，在构建量化算法时追求硬件为中心的方法至关重要。在这方面，我们致力于基于硬件意识构建我们的压缩方法，消除不切实际的算法选择，同时最大限度地发挥硬件加速的优势。我们的方法OdysseyLLM，采用了一种名为FastGEMM的新型W4A8内核实现，并结合了量化策略。大量实验表明，我们的W4A8方法优越性明显，相比于TensorRT-LLM在INT8下实际速度提升高达4$\times$，而且对性能影响不大。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/70208fea725585e10e080b305de27fbd.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/70208fea725585e10e080b305de27fbd.png)'
- en: 'Figure 1: Inference latency on LLaMA-13B quantized in various bit widths. Tested
    under an input of 1024 tokens and an output of 128 tokens with tensor parallelism
    on a single A100-80G GPU. All implementations share the same techniques to have
    a fair comparison. The lower half of a bar exhibits the context decoding stage
    and the higher half shows the self-decoding stage.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：在各种位宽下对量化的LLaMA-13B进行的推理延迟测试。在单个A100-80G GPU上，输入为1024个标记，输出为128个标记，进行了张量并行处理。所有实现共享相同的技术，以确保公平比较。条形图的下半部分展示了上下文解码阶段，上半部分显示了自解码阶段。
- en: Large language models (LLMs) such as GLM [[8](#bib.bib8)], BLOOM [[16](#bib.bib16)],
    OPT [[45](#bib.bib45)] and LLaMA series [[34](#bib.bib34), [35](#bib.bib35)] possess
    the powerful ability of “emergent knowledge” and have revolutionized the field
    of natural language processing, which opens up a new era for artificial intelligence.
    However, the massive scale of these models requires enormous storage and computational
    resources, posing a series of challenges for deployment even for industrial high-end
    server GPUs, let alone mobile or edge computing devices where computational resource
    limitations tremendously hinder the widespread application of these models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GLM [[8](#bib.bib8)]、BLOOM [[16](#bib.bib16)]、OPT [[45](#bib.bib45)]
    和 LLaMA 系列 [[34](#bib.bib34), [35](#bib.bib35)]，拥有强大的“涌现知识”能力，已经彻底改变了自然语言处理领域，为人工智能开辟了新的时代。然而，这些模型的庞大规模需要巨大的存储和计算资源，即使是工业高端服务器GPU也面临部署挑战，更不用说计算资源受限的移动或边缘计算设备，这极大地阻碍了这些模型的广泛应用。
- en: '![Refer to caption](img/cf2ae4b8ec41051cb405917b2749c316.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf2ae4b8ec41051cb405917b2749c316.png)'
- en: 'Figure 2: Comparison of current MatMul paradigm in practical bit widths. $X$
    is the scale for weights.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：当前实际位宽下MatMul范式的比较。$X$ 是权重的尺度。
- en: Compressing large models is a highly challenging task. Current methods generally
    fall into the categories of *pruning*, *distillation*, *quantization*, and *low-rank
    decomposition*. To name a few, LLM-Pruner [[23](#bib.bib23)] is a structured pruning
    method, where the pruning rate is limited to  20% to retain the model capability,
    and the pruned model requires further fine-tuning to restore accuracy. SparseGPT [[10](#bib.bib10)]
    and Wanda [[32](#bib.bib32)] compress large models using unstructured sparsity.
    Although re-training is not required, they are heavily hardware-dependent and
    have very limited use. On the contrary, quantization is a more universal compression
    method, and most hardware provides dedicated computation acceleration units for
    integers. However, as the scale of large models increases, the outlier phenomenon
    in activation values becomes more severe. Traditional quantization methods for
    activation values can lead to significant quantization errors. SmoothQuant [[39](#bib.bib39)]
    achieves almost lossless W8A8 quantization (Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (c)) by migrating the
    quantization difficulty of activation values to weights. To further reduce the
    cost, GPTQ [[11](#bib.bib11)] quantizes model weights to INT4, avoiding the quantization
    problem of activation values and achieving significant performance gains. Unfortunately,
    the fine-granularity (Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey
    for Deployable Quantization of LLMs") (a)) in its solution inevitably creates
    overhead that cancels out the lower-bit benefits. A vanilla adaption of GPTQ to
    have a fine-grained W4A8 strategy as in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (b) naturally inherits
    the disadvantages, let alone the performance degradation when we chase lower-bit
    quantization. Therefore, designing a practical W4A8 quantization scheme is urgent
    but it requires a deeper and thorough rethinking.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩大型模型是一项极具挑战的任务。当前方法通常分为*剪枝*、*蒸馏*、*量化*和*低秩分解*几类。例如，LLM-Pruner [[23](#bib.bib23)]
    是一种结构化剪枝方法，其中剪枝率限制在20%以内以保留模型能力，剪枝后的模型需要进一步微调以恢复准确性。SparseGPT [[10](#bib.bib10)]
    和 Wanda [[32](#bib.bib32)] 使用非结构化稀疏性来压缩大型模型。虽然不需要重新训练，但它们对硬件依赖性强且使用范围非常有限。相比之下，量化是一种更通用的压缩方法，大多数硬件提供专用的整数计算加速单元。然而，随着大型模型规模的增加，激活值中的异常现象变得更加严重。传统的激活值量化方法可能会导致显著的量化误差。SmoothQuant
    [[39](#bib.bib39)] 通过将激活值的量化难度迁移到权重上，实现了几乎无损的W8A8量化 (见图[2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs") (c))。为了进一步降低成本，GPTQ
    [[11](#bib.bib11)] 将模型权重量化为INT4，避免了激活值的量化问题，并实现了显著的性能提升。不幸的是，其解决方案中的细粒度 (见图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs")
    (a)) 不可避免地产生了抵消低位比特好处的开销。对GPTQ进行基本的适配，以获得细粒度的W4A8策略，如图[2](#S1.F2 "Figure 2 ‣ 1
    Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs") (b) 自然继承了其缺点，更不用说追求更低位比特量化时的性能下降。因此，设计一个实际的W4A8量化方案迫在眉睫，但这需要更深刻和彻底的重新思考。
- en: In this paper, we are driven to invert the common practice by taking in a hardware-centric
    approach. We seek real deployment that requires *reduced memory footprint*, *boosted
    inference speed*, and *non-degraded quantized performance*. We argue that this
    shift is crucial in developing new algorithms so that the outcome is readily applicable.
    Hardware constraints help us eliminate the impractical choices to have a reduced
    space of trial-and-error, shedding light on a viable solution in the meantime.
    In a nutshell, our contributions can be summarized as follows,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们受到推动，逆转了常见的做法，采用了以硬件为中心的方法。我们追求真正的部署，这需要*减少内存占用*、*提升推理速度*和*不降低量化性能*。我们认为这种转变对于开发新算法至关重要，以便结果能够立即应用。硬件约束帮助我们排除不切实际的选择，从而减少试错空间，并在此过程中揭示出一种可行的解决方案。总而言之，我们的贡献可以总结如下，
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We advocate a hardware-centric approach that ultimately leads to a deployable
    solution which is a crucial paradigm change in algorithm construction.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提倡一种以硬件为中心的方法，这种方法最终会导致一个可部署的解决方案，这在算法构建中是一个至关重要的范式变化。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We provide the first deployable W4A8 solution, codenamed as OdysseyLLM, that
    comprises a tailored quantization configuration and a novel FastGEMM kernel for
    4-bit integer matrix multiplication that dramatically reduces the cost, and it
    achieves 2.23$\times$ speed boosting over the TensorRT-LLM FP16 and INT8 implementation
    respectively.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了第一个可部署的 W4A8 解决方案，代号为 OdysseyLLM，包含定制的量化配置和一种新型的 FastGEMM 内核，用于 4 位整数矩阵乘法，显著降低了成本，实现了相较于
    TensorRT-LLM 的 FP16 和 INT8 实现的 2.23$\times$ 速度提升。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our W4A8 recipe is proven mostly on par with the state-of-the-art W8A8 quantization
    method SmoothQuant on a variety of common language benchmarks for the state-of-the-art
    LLMs.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的 W4A8 配方在多种常见语言基准测试中已证明与最先进的 W8A8 量化方法 SmoothQuant 基本相当，适用于最先进的 LLM。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Taxonomy of LLM Compression. A recent thorough survey [[46](#bib.bib46)] categorizes
    LLM compression methods into pruning [[10](#bib.bib10), [32](#bib.bib32), [44](#bib.bib44)],
    knowledge distillation [[13](#bib.bib13)], quantization [[39](#bib.bib39), [11](#bib.bib11)],
    low-rank factorization [[44](#bib.bib44), [38](#bib.bib38)]. While each category
    shows promising gains, in this paper we primarily focus on low-bit quantization
    for extreme speed boosting. The orthogonal composition of these methods is also
    tempting, *e.g*., LoRAPrune [[44](#bib.bib44)] obtains a 50% compression ratio
    with structured pruning combined by LoRA [[15](#bib.bib15)]. QLoRA [[7](#bib.bib7)]
    efficiently finetunes the 4-bit quantized LLMs with low-rank adaptation  [[15](#bib.bib15)]
    as well. Nevertheless, there is still room for further investigation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 压缩的分类。最近的全面调查 [[46](#bib.bib46)] 将 LLM 压缩方法分为剪枝 [[10](#bib.bib10), [32](#bib.bib32),
    [44](#bib.bib44)]、知识蒸馏 [[13](#bib.bib13)]、量化 [[39](#bib.bib39), [11](#bib.bib11)]
    和低秩分解 [[44](#bib.bib44), [38](#bib.bib38)]。虽然每个类别都显示出有前景的成果，但在本文中我们主要关注于低位量化以实现极致的速度提升。这些方法的正交组合也很有吸引力，例如，LoRAPrune
    [[44](#bib.bib44)] 通过与 LoRA [[15](#bib.bib15)] 结合的结构化剪枝获得了 50% 的压缩比。QLoRA [[7](#bib.bib7)]
    也有效地微调了具有低秩适应的 4 位量化 LLM [[15](#bib.bib15)]。然而，仍有进一步研究的空间。
- en: Variants of Bit Widths in Quantization. Common choices of bit widths for LLM
    quantization are W8A8 [[39](#bib.bib39), [40](#bib.bib40)], W4A16 [[11](#bib.bib11),
    [19](#bib.bib19), [31](#bib.bib31)], W4A8 [[41](#bib.bib41), [17](#bib.bib17)],
    and W4A4 [[42](#bib.bib42)]. W8A8 suffers from limited acceleration for token
    generation while W4A16 is relatively slow during the pre-filling stage. Current
    W4A8 recipes usually adopt a fine-grained strategy that hampers the inference.
    Going further with W4A4 harms the performance and also induces complexity for
    implementation. There are a few mixed-precision quantization methods like LLM.int8() [[6](#bib.bib6)]
    and QUIK [[1](#bib.bib1)], where the calculation of some outlier layers fallback
    in FP16 as a trade-off between accuracy and latency. Besides, quantization that
    exploits the low-bit floating point representation (FP4, FP8) is analogous to
    non-uniform integer quantization, which exhibits improved performance as well [[38](#bib.bib38),
    [20](#bib.bib20)]. However, they are either restricted to certain GPUs only or
    no such hardware is available yet.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 量化中的位宽变体。LLM 量化的常见位宽选择有 W8A8 [[39](#bib.bib39), [40](#bib.bib40)]、W4A16 [[11](#bib.bib11),
    [19](#bib.bib19), [31](#bib.bib31)]、W4A8 [[41](#bib.bib41), [17](#bib.bib17)]
    和 W4A4 [[42](#bib.bib42)]。W8A8 在生成 token 时加速有限，而 W4A16 在预填充阶段相对较慢。当前 W4A8 方法通常采用精细化策略，这会阻碍推理。进一步使用
    W4A4 会损害性能并增加实现复杂性。一些混合精度量化方法，如 LLM.int8() [[6](#bib.bib6)] 和 QUIK [[1](#bib.bib1)]，会将一些异常层的计算回退到
    FP16，以在准确性和延迟之间取得平衡。此外，利用低位浮点表示（FP4、FP8）的量化类似于非均匀整数量化，也表现出改进的性能 [[38](#bib.bib38),
    [20](#bib.bib20)]。然而，它们要么仅限于某些 GPU，要么尚未有这样的硬件可用。
- en: 3 Preliminary Knowledge on Quantization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 量化的初步知识
- en: We show a glossary of common LLM quantization terms and techniques that build
    up the recipes of current state-of-the-art compression methods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了常见 LLM 量化术语和技术的词汇表，这些术语和技术构成了当前最先进压缩方法的配方。
- en: Weight-only vs. Weight and Activation
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 仅权重 vs. 权重和激活
- en: Quantization methods vary on whether only the weights $\mathbf{W}$ are quantized
    [[39](#bib.bib39)]. The latter is more complicated and possibly induces more quantization
    loss but it will be worth the pain for a decreased model size and an increased
    inference speed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法的差异在于是否仅对权重 $\mathbf{W}$ 进行量化 [[39](#bib.bib39)]。后者更复杂，可能会导致更多的量化损失，但值得为减少模型大小和提高推理速度而付出代价。
- en: Layerwise Quantization
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层级量化
- en: This is the most common scheme that iteratively quantizes each layer to obtain
    $\mathbf{W}_{\mathbf{q}}$ by minimizing the mean square error before and after
    quantization. For instance, when combined with the weight-only strategy, each
    layer has to solve Eq. [1](#S3.E1 "Equation 1 ‣ Layerwise Quantization ‣ 3 Preliminary
    Knowledge on Quantization ‣ A Speed Odyssey for Deployable Quantization of LLMs").
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的方案，通过迭代量化每一层来获得 $\mathbf{W}_{\mathbf{q}}$，通过最小化量化前后的均方误差。例如，当与仅权重策略结合时，每一层都必须解决等式 [1](#S3.E1
    "方程 1 ‣ 层级量化 ‣ 量化的初步知识 ‣ 可部署的 LLM 量化速度奥德赛")。
- en: '|  | $\displaystyle\operatorname{argmin}_{\mathbf{w}}&#124;\mathbf{W}\mathbf{X}-\mathbf{W}_{\mathbf{q}}\mathbf{X}&#124;_{2}^{2}$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\operatorname{argmin}_{\mathbf{w}} \| \mathbf{W}\mathbf{X}
    - \mathbf{W}_{\mathbf{q}}\mathbf{X} \|_{2}^{2}$ |  | (1) |'
- en: Symmetric vs. asymmetric
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对称与非对称
- en: Eq. [2](#S3.E2 "Equation 2 ‣ Symmetric vs. asymmetric ‣ 3 Preliminary Knowledge
    on Quantization ‣ A Speed Odyssey for Deployable Quantization of LLMs") shows
    the quantization $\mathbf{Q}$ as 0\. While asymmetric quantization needs to find
    the optimal position for the zero point to minimize the quantization error. This
    however incurs additional subtraction and possible overflow.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 等式 [2](#S3.E2 "方程 2 ‣ 对称与非对称 ‣ 量化的初步知识 ‣ 可部署的 LLM 量化速度奥德赛") 显示了量化 $\mathbf{Q}$
    为 0。虽然非对称量化需要找到零点的最佳位置以最小化量化误差，但这会带来额外的减法操作和可能的溢出。
- en: '|  | $\displaystyle\mathbf{Q}(x)$ |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{Q}(x)$ |  | (2) |'
- en: '|  | $\displaystyle\mathbf{D}(\mathbf{Q}(x))$ |  | (3) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{D}(\mathbf{Q}(x))$ |  | (3) |'
- en: Per channel vs. fine-grained
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每通道与细粒度
- en: Per channel quantization keeps a quantization scale for each channel, while
    in fine-grained quantization (also known as group-wise or per group), weight channels
    are further assembled into groups that have a more complex representation. Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs")
    well illustrates their difference. Due to the intricate computing pipeline (Eq. [5](#S4.E5
    "Equation 5 ‣ 4.2 Hardware Constraints ‣ 4 Motivation ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")), the fine-grained method inevitably prolongs the inference
    time.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 每通道量化为每个通道保留一个量化尺度，而在细粒度量化（也称为按组量化或每组量化）中，权重通道进一步组合成具有更复杂表示的组。图 [2](#S1.F2 "图
    2 ‣ 1 引言 ‣ 可部署的 LLM 量化速度奥德赛") 很好地展示了它们的区别。由于复杂的计算管道（等式 [5](#S4.E5 "方程 5 ‣ 4.2
    硬件约束 ‣ 4 动机 ‣ 可部署的 LLM 量化速度奥德赛")），细粒度方法不可避免地延长了推理时间。
- en: Per tensor vs. Per token
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每个 tensor 与每个 token
- en: For activation quantization, it is advisable to adopt a per-token strategy to
    improve the performance over per tensor strategy (Fig. [2](#S1.F2 "Figure 2 ‣
    1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs")), where
    each activation token corresponds to a quantization scale, also shown in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs")
    (b) and (c). However, per token will increase a moderate amount of overhead.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于激活量化，建议采用每个 token 的策略，以提高相对于每个 tensor 策略的性能（图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 可部署的
    LLM 量化速度奥德赛")），其中每个激活 token 对应一个量化尺度，如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 可部署的 LLM 量化速度奥德赛")
    (b) 和 (c) 所示。然而，每个 token 会增加适量的开销。
- en: '| Method | Bits | Granularity | LLaMA-1-7B | LLaMA-1-13B | LLaMA-1-65B | LLaMA-2-7B
    | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 颗粒度 | LLaMA-1-7B | LLaMA-1-13B | LLaMA-1-65B | LLaMA-2-7B | LLaMA-2-13B
    | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| FP16 | W16A16 | None | 73.74% | 76.19% | 79.20% | 73.70% | 76.64% | 79.57%
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | W16A16 | None | 73.74% | 76.19% | 79.20% | 73.70% | 76.64% | 79.57%
    |'
- en: '| RTN[pt] | W16A8 | None | 73.26% [-0.5%] | 76.13% [(-0.1%)] | 78.67% [(-0.5%)]
    | 73.70% | 76.83% [(+0.2%)] | 79.12% [(-0.5%)] |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| RTN[pt] | W16A8 | None | 73.26% [-0.5%] | 76.13% [(-0.1%)] | 78.67% [(-0.5%)]
    | 73.70% | 76.83% [(+0.2%)] | 79.12% [(-0.5%)] |'
- en: '| RTN[g128] | W4A16 | g128 | 72.87% [(-0.9%)] | 75.08% [(-1.1%)] | 78.87% [(-0.3%)]
    | 71.24% [(-2.5%)] | 76.29% [(-0.4%)] | 78.69% [(-0.9%)] |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| RTN[g128] | W4A16 | g128 | 72.87% [(-0.9%)] | 75.08% [(-1.1%)] | 78.87% [(-0.3%)]
    | 71.24% [(-2.5%)] | 76.29% [(-0.4%)] | 78.69% [(-0.9%)] |'
- en: '| GPTQ[g128] | W4A16 | g128 | 70.21% [(-3.5%)] | 75.68% [(-0.5%)] | 78.77%
    [(-0.5%)] | 72.31% [(-1.4%)] | 75.99% [(-0.7%)] | 79.86% [(-0.3%)] |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ[g128] | W4A16 | g128 | 70.21% [(-3.5%)] | 75.68% [(-0.5%)] | 78.77%
    [(-0.5%)] | 72.31% [(-1.4%)] | 75.99% [(-0.7%)] | 79.86% [(-0.3%)] |'
- en: '| RTN | W4A16 | pc | 65.34% [(-8.4%)] | 69.42% [(-6.8%)] | 75.63% [(-3.6%)]
    | 64.25% [(-9.5%)] | 73.08% [(-3.6%)] | 77.02% [(-2.6%)] |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| RTN | W4A16 | pc | 65.34% [(-8.4%)] | 69.42% [(-6.8%)] | 75.63% [(-3.6%)]
    | 64.25% [(-9.5%)] | 73.08% [(-3.6%)] | 77.02% [(-2.6%)] |'
- en: '| GPTQ[ro] | W4A16 | pc | 67.92% [(-5.8%)] | 71.05% [(-6.0%)] | 77.12% [(-2.1%)]
    | 68.95% [(-4.8%)] | 74.35% [(-2.3%)] | 78.83% [(-0.7%)] |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ[ro] | W4A16 | pc | 67.92% [(-5.8%)] | 71.05% [(-6.0%)] | 77.12% [(-2.1%)]
    | 68.95% [(-4.8%)] | 74.35% [(-2.3%)] | 78.83% [(-0.7%)] |'
- en: 'Table 1: Accuracy comparison of different quantization methods on the LAMBADA
    dataset for the LLaMA series models. pt: per-token, pc: per-channel, g128: 128
    groups, ro: with activation reordering'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLaMA 系列模型在 LAMBADA 数据集上不同量化方法的准确性比较。pt：每个令牌，pc：每个通道，g128：128 组，ro：激活重排序
- en: 4 Motivation
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 动机
- en: Albeit the advances in LLM compression, the inference still incurs high latencies
    due to its intrinsically low parallelizability and huge memory footprints. Facing
    two main challenges below, we are driven to scheme a specific method to continue
    exploring the possible upper limits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLM 压缩方面有所进展，但由于其固有的低并行性和巨大的内存占用，推理仍然会导致高延迟。面对以下两个主要挑战，我们被迫制定具体方法，以继续探索可能的极限。
- en: 4.1 Architecture Limitations
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 架构限制
- en: Large language models adopt the Transformer structure [[36](#bib.bib36)] and
    generate tokens in a “self-regressive” manner. The entire inference process can
    be divided into the *context decoding* stage (pre-filling) and the *self-decoding*
    stage (token generation). Context decoding stage under long input conditions is
    a typical computationally intensive task. The self-decoding stage instead, due
    to the non-parallel limitations of the “self-regressive” generation, cannot effectively
    utilize hardware resources, rendering it a typical memory-intensive task. This
    is also depicted in the roofline analysis in  [[1](#bib.bib1)]. Among the up-to-date
    quantization methods, the W8A8 recipe has a significant acceleration effect in
    the context decoding stage, while W4A16, due to its ability to further reduce
    memory bandwidth, has more advantages in the self-decoding stage. Theoretically,
    we can have W4A8 that combines these two quantization methods to enjoy the optimization
    benefits of both two stages. However, this direction is largely stalled by degraded
    performance and hardware constraints to be discussed below.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型采用 Transformer 结构[[36](#bib.bib36)]，以“自回归”方式生成令牌。整个推理过程可以分为 *上下文解码* 阶段（预填充）和
    *自解码* 阶段（令牌生成）。在长输入条件下，上下文解码阶段是一个典型的计算密集型任务。相反，由于“自回归”生成的非并行限制，自解码阶段无法有效利用硬件资源，使其成为典型的内存密集型任务。这也在[[1](#bib.bib1)]的
    roofline 分析中有所描述。在最新的量化方法中，W8A8 配方在上下文解码阶段具有显著的加速效果，而 W4A16 由于其进一步减少内存带宽的能力，在自解码阶段具有更多优势。理论上，我们可以结合这两种量化方法的
    W4A8，以享受两个阶段的优化效益。然而，这一方向在性能下降和硬件限制的阻碍下大多停滞不前。
- en: 4.2 Hardware Constraints
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 硬件限制
- en: Current mainstream INT4 schemes [[11](#bib.bib11), [17](#bib.bib17)] have widely
    adopted fine-grained quantization to counter the degradation effect. In Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs"),
    we investigate the implementation of three popular quantization recipes of different
    bit widths to exhibit the drawbacks of such choices. W4A16 (Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization of LLMs")a) performs
    4-bit groupwise quantization on weights. During matrix multiplication, it is costly
    to dequantize ($\mathbf{Dq}$) the INT4 weights into FP16 in real-time before the
    actual calculation, as described in Eq. [4](#S4.E4 "Equation 4 ‣ 4.2 Hardware
    Constraints ‣ 4 Motivation ‣ A Speed Odyssey for Deployable Quantization of LLMs").
    W4A8 (Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")b) performs 4-bit groupwise quantization on weights and
    uses 8-bit per-token quantization for activation. Similarly, it needs to first
    convert the INT4 weights into INT8 before GEMM operations, formulated in Eq. [5](#S4.E5
    "Equation 5 ‣ 4.2 Hardware Constraints ‣ 4 Motivation ‣ A Speed Odyssey for Deployable
    Quantization of LLMs"). Due to the use of group-wise quantization, each group
    has to be dequantized back to FP32 when accumulating, which brings considerable
    overhead. For W8A8 (Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Speed Odyssey
    for Deployable Quantization of LLMs")c) specified in Eq. [6](#S4.E6 "Equation
    6 ‣ 4.2 Hardware Constraints ‣ 4 Motivation ‣ A Speed Odyssey for Deployable Quantization
    of LLMs") and Eq. [7](#S4.E7 "Equation 7 ‣ 4.2 Hardware Constraints ‣ 4 Motivation
    ‣ A Speed Odyssey for Deployable Quantization of LLMs"), it uses 8-bit per-channel
    quantization for weights and 8-bit per-token quantization for activation. The
    dequantization is performed after GEMM, which is so far the most hardware-friendly
    process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当前主流的INT4方案[[11](#bib.bib11), [17](#bib.bib17)]广泛采用细粒度量化来对抗降级效应。在图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 可部署量化的速度探秘")中，我们研究了三种不同位宽的流行量化方案的实现，以展示这些选择的缺陷。W4A16（图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 可部署量化的速度探秘")a）对权重进行4位分组量化。在矩阵乘法过程中，将INT4权重实时解量化（$\mathbf{Dq}$）为FP16再进行实际计算是昂贵的，如方程[4](#S4.E4
    "方程 4 ‣ 4.2 硬件约束 ‣ 4 动机 ‣ 可部署量化的速度探秘")所述。W4A8（图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ 可部署量化的速度探秘")b）对权重进行4位分组量化，并对激活进行每个标记8位量化。同样，在进行GEMM操作之前，它需要将INT4权重转换为INT8，如方程[5](#S4.E5
    "方程 5 ‣ 4.2 硬件约束 ‣ 4 动机 ‣ 可部署量化的速度探秘")所述。由于使用了分组量化，每个组在累加时都必须重新解量化为FP32，这带来了相当大的开销。对于W8A8（图[2](#S1.F2
    "图 2 ‣ 1 引言 ‣ 可部署量化的速度探秘")c），如方程[6](#S4.E6 "方程 6 ‣ 4.2 硬件约束 ‣ 4 动机 ‣ 可部署量化的速度探秘")和方程[7](#S4.E7
    "方程 7 ‣ 4.2 硬件约束 ‣ 4 动机 ‣ 可部署量化的速度探秘")中指定，它对权重进行每通道8位量化，对激活进行每标记8位量化。解量化在GEMM之后进行，这到目前为止是最硬件友好的过程。
- en: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (4) |'
- en: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (5) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (5) |'
- en: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (6) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle O_{i,j}^{FP32}$ |  | (6) |'
- en: '|  | $\displaystyle S_{i,j}$ |  | (7) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S_{i,j}$ |  | (7) |'
- en: Trade-off between precision and speed? In Table [1](#S3.T1 "Table 1 ‣ Per tensor
    vs. Per token ‣ 3 Preliminary Knowledge on Quantization ‣ A Speed Odyssey for
    Deployable Quantization of LLMs"), we first test the performance of the LLaMA
    models [[34](#bib.bib34)] with various quantization methods on the LAMBADA [[28](#bib.bib28)]
    dataset. If we only quantize activations, RTN-pt readily delivers results close
    to FP16, which prevents us from resorting to activation smoothing methods [[39](#bib.bib39),
    [17](#bib.bib17)]. For fine-grained quantization methods for weights, both vanilla
    Round-To-Nearest (RTN-g128) and GPTQ-g128 [[11](#bib.bib11)] can retain model
    accuracy. However, after adopting per-channel quantization, RTN has an accuracy
    drop in the range of 3% to 10%, while GPTQ with a reordering trick (higher error-prone
    channels are quantized first) has 1% to 6%.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 精度与速度之间的权衡？在表格 [1](#S3.T1 "表 1 ‣ 每个张量与每个标记 ‣ 3 量化初步知识 ‣ 可部署的 LLM 量化的速度历程") 中，我们首先测试了在
    LAMBADA [[28](#bib.bib28)] 数据集上使用各种量化方法的 LLaMA 模型 [[34](#bib.bib34)] 的性能。如果我们只对激活进行量化，RTN-pt
    很容易提供接近 FP16 的结果，这使得我们不需要采用激活平滑方法 [[39](#bib.bib39), [17](#bib.bib17)]。对于权重的细粒度量化方法，普通的
    Round-To-Nearest (RTN-g128) 和 GPTQ-g128 [[11](#bib.bib11)] 都可以保持模型准确性。然而，在采用每通道量化后，RTN
    的准确率下降范围为 3% 到 10%，而 GPTQ 通过重排序技巧（先量化错误率较高的通道）则为 1% 到 6%。
- en: As mentioned in Sec [4.2](#S4.SS2 "4.2 Hardware Constraints ‣ 4 Motivation ‣
    A Speed Odyssey for Deployable Quantization of LLMs"), fine-grained W4A8 requires
    a large number of Dequantize operations to be inserted in the GEMM calculation
    process, bringing non-negligible additional overhead, thereby offsetting the speed
    advantage brought by INT8 GEMM. It seems impossible to achieve both accuracy and
    speed.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第 [4.2](#S4.SS2 "4.2 硬件限制 ‣ 4 动机 ‣ 可部署的 LLM 量化的速度历程") 节中提到的，细粒度 W4A8 需要在 GEMM
    计算过程中插入大量的去量化操作，带来不可忽视的额外开销，从而抵消了 INT8 GEMM 带来的速度优势。似乎不可能同时实现精度和速度。
- en: To fulfill our goal, we have no other choice but to abandon the fine-grained
    weight quantization strategy and pursue the per-channel weight quantization instead.
    To compensate for the caused accuracy loss therein, we are forced to involve particular
    quantization schemes to make the performance comparable to that of fine-grained
    ones. Additionally, to utilize hardware resources in full, we are compelled to
    rewrite a specific GEMM operation for W4A8, which we later call FastGEMM.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现我们的目标，我们别无选择，只能放弃细粒度权重量化策略，转而追求每通道权重量化。为了弥补其中造成的准确率损失，我们不得不采用特定的量化方案，使其性能可与细粒度方案相媲美。此外，为了充分利用硬件资源，我们被迫重写一个特定的
    W4A8 GEMM 操作，随后我们称之为 FastGEMM。
- en: 5 OdysseyLLM
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 OdysseyLLM
- en: Our proposed method, codenamed OdysseyLLM, records the way to a viable W4A8
    solution.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法，代号 OdysseyLLM，记录了实现 W4A8 解决方案的途径。
- en: 5.1 Adaptive Weight Clipping
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 自适应权重裁剪
- en: The weights of neural networks generally exhibit a Gaussian distribution. As
    INT4 has only half the bit widths of INT8, a typical min-max uniform 4-bit quantization
    method [[37](#bib.bib37), [21](#bib.bib21)] will cause a large number of rounding
    errors, especially near the weights close to 0, leading to a detrimental quantization
    degradation. To alleviate this phenomenon, one can resort to clipping the range
    of weights. For example, LSQ [[9](#bib.bib9)] and PACT [[3](#bib.bib3)] adaptively
    learn a truncation value of the weight. However, the direct learning truncation
    value method does not have obvious benefits in low-bit quantization on LLMs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的权重通常表现为高斯分布。由于 INT4 的位宽仅为 INT8 的一半，典型的 min-max 均匀 4 位量化方法 [[37](#bib.bib37),
    [21](#bib.bib21)] 会导致大量的四舍五入误差，尤其是接近 0 的权重，导致量化退化。为了缓解这种现象，可以采用裁剪权重范围的方法。例如，LSQ [[9](#bib.bib9)]
    和 PACT [[3](#bib.bib3)] 自适应地学习权重的截断值。然而，直接学习截断值的方法在低位量化 LLM 上没有明显的好处。
- en: In this regard, Ominiquant [[31](#bib.bib31)] proposed *Learnable Weight Clipping*,
    which learns the truncation intensity (denoted by $\gamma$) of each channel. The
    optimal truncation value is obtained by optimizing it through the gradient descent
    method. Motivated by the hardware-centric principle, we revise their approach
    into a symmetric version (Eq. [8](#S5.E8 "Equation 8 ‣ 5.1 Adaptive Weight Clipping
    ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of LLMs")) as it
    is more hardware-efficient.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这方面，Ominiquant [[31](#bib.bib31)] 提出了*可学习的权重剪裁*，该方法学习每个通道的截断强度（用$\gamma$表示）。通过梯度下降法优化，获得最佳的截断值。受到以硬件为中心的原则的启发，我们将他们的方法修订为对称版本
    (见公式 [8](#S5.E8 "Equation 8 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A
    Speed Odyssey for Deployable Quantization of LLMs"))，因为这种方式更具硬件效率。
- en: '|  | $\displaystyle\mathbf{W}_{\mathbf{q}}$ |  | (8) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{\mathbf{q}}$ |  | (8) |'
- en: '|  | $\displaystyle S$ |  | (9) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S$ |  | (9) |'
- en: We finally obtain a more compact weight distribution, *e.g*., $(-0.4,0.2)$ in
    each layer have a smaller per-channel quantization MSE error compared to the original
    weights, as depicted in Fig. [3](#S5.F3 "Figure 3 ‣ 5.1 Adaptive Weight Clipping
    ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of LLMs") (c).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终获得了一个更加紧凑的权重分布，*例如*，每层中的$(-0.4,0.2)$相比于原始权重具有更小的每通道量化均方误差，如图[3](#S5.F3 "Figure
    3 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") (c)所示。
- en: '![Refer to caption](img/07c470f8d45138a6d527a00cb70a9155.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/07c470f8d45138a6d527a00cb70a9155.png)'
- en: 'Figure 3: Top: Weight distribution of vanilla weights (a) and clamped weights
    (b). Bottom: Comparison of layerwise MSE of per-channel fake quantization with
    clamped and vanilla weights.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：顶部：原始权重 (a) 和夹紧权重 (b) 的权重分布。底部：夹紧和原始权重的每通道假量化的层级均方误差比较。
- en: '| Dataset | Method/LLaMA | Bits | LLaMA-7B | LLaMA-13B | LLaMA-65B | LLaMA-2-7B
    | LLaMA-2-13B | LLaMA-2-70B |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 方法/LLaMA | 位数 | LLaMA-7B | LLaMA-13B | LLaMA-65B | LLaMA-2-7B | LLaMA-2-13B
    | LLaMA-2-70B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LAMBADA | FP16 | W16A16 | 73.74% | 76.19% | 79.20% | 73.70% | 76.64% | 79.57%
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| LAMBADA | FP16 | W16A16 | 73.74% | 76.19% | 79.20% | 73.70% | 76.64% | 79.57%
    |'
- en: '| AWQ-g128 | W4A16 | 66.80% | 73.72% | 78.73% | 70.23% | 75.80% | 78.40% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 66.80% | 73.72% | 78.73% | 70.23% | 75.80% | 78.40% |'
- en: '| GPTQ-g128 | W4A16 | 70.21% | 75.68% | 78.77% | 72.31% | 75.99% | 79.86% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 70.21% | 75.68% | 78.77% | 72.31% | 75.99% | 79.86% |'
- en: '| SmoothQuant^∗ | W8A8 | 73.49% | 76.15% | 78.07% | 73.36% | 76.05% | 78.71%
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 73.49% | 76.15% | 78.07% | 73.36% | 76.05% | 78.71%
    |'
- en: '| OdysseyLLM | W4A8 | 73.49% | 76.23% | 78.56% | 70.81% | 76.07% | 79.43% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 73.49% | 76.23% | 78.56% | 70.81% | 76.07% | 79.43% |'
- en: '| C4 | FP16 | W16A16 | 7.05 | 6.61 | 5.59 | 7.05 | 6.46 | 5.52 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| C4 | FP16 | W16A16 | 7.05 | 6.61 | 5.59 | 7.05 | 6.46 | 5.52 |'
- en: '| AWQ-g128 | W4A16 | 7.34 | 6.81 | 5.71 | 7.42 | 6.68 | 5.63 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 7.34 | 6.81 | 5.71 | 7.42 | 6.68 | 5.63 |'
- en: '| GPTQ-g128 | W4A16 | 7.71 | 6.73 | 5.70 | 8.74 | 6.6 | 5.60 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 7.71 | 6.73 | 5.70 | 8.74 | 6.6 | 5.60 |'
- en: '| SmoothQuant^∗ | W8A8 | 7.23 | 6.723 | 5.81 | 7.24 | 6.55 | 5.61 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 7.23 | 6.723 | 5.81 | 7.24 | 6.55 | 5.61 |'
- en: '| OdysseyLLM | W4A8 | 7.5 | 6.88 | 5.93 | 7.58 | 6.7 | 5.78 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 7.5 | 6.88 | 5.93 | 7.58 | 6.7 | 5.78 |'
- en: '| WikiText | FP16 | W16A16 | 5.73 | 5.1 | 3.51 | 5.65 | 4.95 | 3.36 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| WikiText | FP16 | W16A16 | 5.73 | 5.1 | 3.51 | 5.65 | 4.95 | 3.36 |'
- en: '| AWQ-g128 | W4A16 | 6.01 | 5.32 | 3.69 | 6.04 | 5.16 | 3.53 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 6.01 | 5.32 | 3.69 | 6.04 | 5.16 | 3.53 |'
- en: '| GPTQ-g128 | W4A16 | 6.31 | 5.24 | 3.67 | 6.36 | 5.1 | 3.50 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 6.31 | 5.24 | 3.67 | 6.36 | 5.1 | 3.50 |'
- en: '| SmoothQuant^∗ | W8A8 | 5.89 | 5.21 | 3.73 | 5.797 | 5.04 | 3.46 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 5.89 | 5.21 | 3.73 | 5.797 | 5.04 | 3.46 |'
- en: '| OdesseyLLM | W4A8 | 6.17 | 5.37 | 3.92 | 6.11 | 5.19 | 3.7 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| OdesseyLLM | W4A8 | 6.17 | 5.37 | 3.92 | 6.11 | 5.19 | 3.7 |'
- en: 'Table 2: Performance comparison for various quantized LLaMA models on LAMBADA,
    C4 and WikiText datasets. ^∗: per token for activations and per channel for weights'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 LAMBADA、C4 和 WikiText 数据集上不同量化 LLaMA 模型的性能比较。^∗：激活函数每个 token 和权重每通道
- en: '| Model | Method | BitWidth | WinoGrande | PIQA | HellaSwag | ARC_e | Avg |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 位宽 | WinoGrande | PIQA | HellaSwag | ARC_e | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-1-7B | FP16 | W16A16 | 0.6985 | 0.7916 | 0.761 | 0.728 | 0.7448 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-7B | FP16 | W16A16 | 0.6985 | 0.7916 | 0.761 | 0.728 | 0.7448 |'
- en: '| AWQ-g128 | W4A16 | 0.6938 | 0.7845 | 0.7465 | 0.7168 | 0.7354 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.6938 | 0.7845 | 0.7465 | 0.7168 | 0.7354 |'
- en: '| GPTQ-g128 | W4A16 | 0.6661 | 0.7786 | 0.7229 | 0.6557 | 0.7058 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.6661 | 0.7786 | 0.7229 | 0.6557 | 0.7058 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.7119 | 0.7894 | 0.7537 | 0.7386 | 0.7484 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.7119 | 0.7894 | 0.7537 | 0.7386 | 0.7484 |'
- en: '| OdysseyLLM | W4A8 | 0.6977 | 0.7878 | 0.7407 | 0.7155 | 0.7354 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.6977 | 0.7878 | 0.7407 | 0.7155 | 0.7354 |'
- en: '| LLaMA-1-13B | FP16 | W16A16 | 0.7277 | 0.8009 | 0.7907 | 0.7471 | 0.7666
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-13B | FP16 | W16A16 | 0.7277 | 0.8009 | 0.7907 | 0.7471 | 0.7666
    |'
- en: '| AWQ-g128 | W4A16 | 0.7151 | 0.7971 | 0.7818 | 0.7256 | 0.7549 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.7151 | 0.7971 | 0.7818 | 0.7256 | 0.7549 |'
- en: '| GPTQ-g128 | W4A16 | 0.7238 | 0.8025 | 0.7828 | 0.7353 | 0.7611 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.7238 | 0.8025 | 0.7828 | 0.7353 | 0.7611 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.7238 | 0.802 | 0.7836 | 0.7466 | 0.764 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.7238 | 0.802 | 0.7836 | 0.7466 | 0.764 |'
- en: '| OdysseyLLM | W4A8 | 0.7238 | 0.7998 | 0.7792 | 0.7441 | 0.7617 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.7238 | 0.7998 | 0.7792 | 0.7441 | 0.7617 |'
- en: '| LLaMA-1-65B | FP16 | W16A16 | 0.7735 | 0.8232 | 0.8415 | 0.7976 | 0.809 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-65B | FP16 | W16A16 | 0.7735 | 0.8232 | 0.8415 | 0.7976 | 0.809 |'
- en: '| AWQ-g128 | W4A16 | 0.7664 | 0.821 | 0.8427 | 0.7992 | 0.8073 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.7664 | 0.821 | 0.8427 | 0.7992 | 0.8073 |'
- en: '| GPTQ-g128 | W4A16 | 0.7632 | 0.8221 | 0.8382 | 0.79 | 0.8034 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.7632 | 0.8221 | 0.8382 | 0.79 | 0.8034 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.7593 | 0.7976 | 0.8097 | 0.7471 | 0.7784 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.7593 | 0.7976 | 0.8097 | 0.7471 | 0.7784 |'
- en: '| OdysseyLLM | W4A8 | 0.753 | 0.7933 | 0.8019 | 0.734 | 0.7706 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.753 | 0.7933 | 0.8019 | 0.734 | 0.7706 |'
- en: '| LLaMA-2-7B | FP16 | W16A16 | 0.6906 | 0.7911 | 0.7598 | 0.7458 | 0.7468 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | FP16 | W16A16 | 0.6906 | 0.7911 | 0.7598 | 0.7458 | 0.7468 |'
- en: '| AWQ-g128 | W4A16 | 0.6819 | 0.7786 | 0.7473 | 0.6675 | 0.7188 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.6819 | 0.7786 | 0.7473 | 0.6675 | 0.7188 |'
- en: '| GPTQ-g128 | W4A16 | 0.6772 | 0.7845 | 0.748 | 0.6742 | 0.721 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.6772 | 0.7845 | 0.748 | 0.6742 | 0.721 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.6875 | 0.7873 | 0.7598 | 0.7104 | 0.7363 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.6875 | 0.7873 | 0.7598 | 0.7104 | 0.7363 |'
- en: '| OdysseyLLM | W4A8 | 0.6811 | 0.7742 | 0.7398 | 0.6953 | 0.7226 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.6811 | 0.7742 | 0.7398 | 0.6953 | 0.7226 |'
- en: '| LLaMA-2-13B | FP16 | W16A16 | 0.7222 | 0.8052 | 0.7938 | 0.7744 | 0.7739
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | FP16 | W16A16 | 0.7222 | 0.8052 | 0.7938 | 0.7744 | 0.7739
    |'
- en: '| AWQ-g128 | W4A16 | 0.7253 | 0.7987 | 0.7838 | 0.766 | 0.7685 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.7253 | 0.7987 | 0.7838 | 0.766 | 0.7685 |'
- en: '| GPTQ-g128 | W4A16 | 0.7245 | 0.7992 | 0.7899 | 0.7736 | 0.7718 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.7245 | 0.7992 | 0.7899 | 0.7736 | 0.7718 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.723 | 0.8052 | 0.7977 | 0.7681 | 0.7735 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.723 | 0.8052 | 0.7977 | 0.7681 | 0.7735 |'
- en: '| OdysseyLLM | W4A8 | 0.7111 | 0.7976 | 0.7782 | 0.763 | 0.7625 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.7111 | 0.7976 | 0.7782 | 0.763 | 0.7625 |'
- en: '| LLaMA-2-70B | FP16 | W16A16 | 0.7798 | 0.8275 | 0.8381 | 0.8098 | 0.8138
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | FP16 | W16A16 | 0.7798 | 0.8275 | 0.8381 | 0.8098 | 0.8138
    |'
- en: '| AWQ-g128 | W4A16 | 0.7727 | 0.8281 | 0.8341 | 0.803 | 0.8095 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 0.7727 | 0.8281 | 0.8341 | 0.803 | 0.8095 |'
- en: '| GPTQ-g128 | W4A16 | 0.779 | 0.833 | 0.8343 | 0.8035 | 0.8125 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 0.779 | 0.833 | 0.8343 | 0.8035 | 0.8125 |'
- en: '| SmoothQuant^∗ | W8A8 | 0.7766 | 0.8303 | 0.8345 | 0.8127 | 0.8135 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 0.7766 | 0.8303 | 0.8345 | 0.8127 | 0.8135 |'
- en: '| OdysseyLLM | W4A8 | 0.7751 | 0.8313 | 0.8272 | 0.806 | 0.8099 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 0.7751 | 0.8313 | 0.8272 | 0.806 | 0.8099 |'
- en: 'Table 3: Comparison on Common Sense QA. ^∗: per token for activations and per
    channel for weights'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：常识QA的比较。^∗：激活按token计算，权重按通道计算
- en: 5.2 Hessian-based Training-free Compensation
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 基于Hessian的无训练补偿
- en: Although *learnable weight clipping* can effectively alleviate the loss brought
    by 4-bit integer per-channel quantization, further compensation is still required
    to enhance the performance. We generally follow a layerwise quantization structure [[25](#bib.bib25),
    [18](#bib.bib18)] that reduces the mean square error before and after quantization.
    In particular, we choose GPTQ [[11](#bib.bib11)] that speeds up the Hessian-based
    quantization compensation algorithm OBQ [[12](#bib.bib12)] by parallel execution
    and the removal of greedy strategy. Essentially, this genre of algorithms iteratively
    updates the remaining set $F$.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管*可学习权重剪裁*可以有效缓解4位整数每通道量化带来的损失，但仍需进一步补偿以提高性能。我们通常遵循分层量化结构[[25](#bib.bib25),
    [18](#bib.bib18)]，以减少量化前后的均方误差。特别地，我们选择GPTQ[[11](#bib.bib11)]，通过并行执行和去除贪婪策略来加速基于Hessian的量化补偿算法OBQ[[12](#bib.bib12)]。本质上，这类算法迭代更新剩余集合$F$。
- en: '|  | $\displaystyle\mathbf{W}_{i}$ |  | (10) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{W}_{i}$ |  | (10) |'
- en: '|  | $\displaystyle\boldsymbol{\delta}_{F}$ |  | (11) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\delta}_{F}$ |  | (11) |'
- en: 5.3 Fast Mixed-Precision GEMM
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 快速混合精度GEMM
- en: The W4A8 calls for a renovation for the kernel implementation to maximize the
    benefit of this bit width setting on mainstream hardware. We take the following
    three steps to obtain FastGEMM.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: W4A8要求对内核实现进行改造，以最大化该位宽设置在主流硬件上的效益。我们采取以下三个步骤来获得FastGEMM。
- en: '![Refer to caption](img/6f1fcbff104fcab4c5adfb96546328ff.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6f1fcbff104fcab4c5adfb96546328ff.png)'
- en: 'Figure 4: Design of W4A8 FastGEMM (c) compared to FP16 (a) and vanilla version
    (b). (d) Our SInt4to8 conversion (represented in two’s complement) where 4-bit
    integers are multiplied by 16 to have a convenient 8-bit GEMM.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 与 FP16 (a) 和原版 (b) 相比的 W4A8 FastGEMM (c) 设计。(d) 我们的 SInt4to8 转换（以二进制补码表示），其中4位整数乘以16以便于8位
    GEMM。'
- en: Kernel fusion. As shown in Fig. [4](#S5.F4 "Figure 4 ‣ 5.3 Fast Mixed-Precision
    GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of LLMs") (a),
    modern GPUs only support GEMM calculations of the same type, while mixed precision
    requires conversion to the same type first. Depicted in Fig. [4](#S5.F4 "Figure
    4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") (b), a naive approach is to implement a separate GPU Kernel
    to perform type conversion, but this would increase additional memory access and
    substantially slow down the model inference speed. Instead, we propose to fuse
    SINT4toS8 and GEMM into one GPU kernel, abbreviated as FastGEMM and shown in Fig. [4](#S5.F4
    "Figure 4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for
    Deployable Quantization of LLMs") (c). However, it is non-trivial to achieve.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 核心融合。如图[4](#S5.F4 "Figure 4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣
    A Speed Odyssey for Deployable Quantization of LLMs") (a) 所示，现代GPU仅支持相同类型的 GEMM
    计算，而混合精度需要先转换为相同类型。图[4](#S5.F4 "Figure 4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (b) 描述了一种简单的方法，即实现一个单独的
    GPU 核心进行类型转换，但这会增加额外的内存访问并显著降低模型推理速度。相反，我们建议将 SINT4toS8 和 GEMM 融合为一个 GPU 核心，简称
    FastGEMM，如图[4](#S5.F4 "Figure 4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (c) 所示。然而，实现这一目标并非易事。
- en: Removal of INT8 subtraction. As mentioned in Sec. [5.1](#S5.SS1 "5.1 Adaptive
    Weight Clipping ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of
    LLMs"), we adopt symmetric quantization. This is beneficial for two reasons. First,
    according to LLM-QAT [[21](#bib.bib21)], the LLaMA series performs better in terms
    of accuracy with symmetric quantization compared to asymmetric quantization. Second,
    modern GPUs like NVIDIA, in order to reduce the size of the chip, do not provide
    subtraction instructions for the signed 8-bit integers³³3[https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#integer-arithmetic-instructions-sub](https://goo.by/RjASnb).
    This leads to additional type conversion instructions when asymmetric quantization
    is processing the zero point, thereby impeding the inference speed. Choosing symmetric
    quantization directly removes the zero-point subtraction.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 移除 INT8 减法。如第[5.1](#S5.SS1 "5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A
    Speed Odyssey for Deployable Quantization of LLMs")节所述，我们采用对称量化。这有两个好处。首先，根据LLM-QAT [[21](#bib.bib21)]，LLaMA系列在精度方面使用对称量化比使用非对称量化表现更好。其次，现代GPU如NVIDIA，为了减少芯片的大小，不提供带符号8位整数的减法指令³³3[https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#integer-arithmetic-instructions-sub](https://goo.by/RjASnb)。这导致在处理零点时需要额外的类型转换指令，从而阻碍了推理速度。选择对称量化直接去除了零点减法。
- en: '![Refer to caption](img/bafff3e17d125636baa390b03286affa.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bafff3e17d125636baa390b03286affa.png)'
- en: 'Figure 5: Vanilla W4A8’s UINT4toS8 vs. our proposed SINT4toS8. The green area
    is on the device, the rest is offline.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 原版 W4A8 的 UINT4toS8 与我们提出的 SINT4toS8。绿色区域在设备上，其余为离线部分。'
- en: Reusing the sign bit. In Fig. [5](#S5.F5 "Figure 5 ‣ 5.3 Fast Mixed-Precision
    GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of LLMs"), a
    vanilla W4A8’s UINT4toS8 operation can be very costly. For instance, to load the
    two’s complement of -7 on GPU, it requires sophisticated calculation (detailed
    in Sec. [A.1](#A1.SS1 "A.1 UINT4toS8 vs. SINT4toS8 ‣ Appendix A Kernel Implementation
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (supp.)), especially there
    is an additional on-device subtraction which is not directly supported by hardware.
    By default, it has to be converted to higher precision (typically INT32) for such
    subtraction which incurs substantial cost. We implement it as Asym GEMM in Fig. [7](#S7.F7
    "Figure 7 ‣ 7.2 FastGEMM vs. Fine-grained vs. Asymmetric ‣ 7 Ablation Study ‣
    A Speed Odyssey for Deployable Quantization of LLMs") to see how costly it is.
    As we are motivated to explore the authentic benefit of W4A8, we have to fabricate
    a faster W4A8 scheme to load signed INT4 to signed INT8\. Specifically, we store
    the weights of signed INT4 into two’s complement, shown in Fig. [4](#S5.F4 "Figure
    4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") (d). During computation, we place the signed INT4 weights
    in the higher 4 bits of signed INT8, which is equivalent to multiplying each value
    by 16\. After the completion of the GEMM calculation, we divide the output value
    by 16 to restore the correct results. Such multiplication and division can be
    easily and efficiently implemented. Since the internal accumulator is of the INT32
    type, there will be no overflow. With this novel conversion scheme, we can retain
    accuracy while significantly improving inference speed and can be used out-of-box
    on modern GPUs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 重用符号位。在图 [5](#S5.F5 "Figure 5 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") 中，普通的 W4A8 UINT4toS8 操作可能非常昂贵。例如，在
    GPU 上加载 -7 的补码需要复杂的计算（详见第 [A.1](#A1.SS1 "A.1 UINT4toS8 vs. SINT4toS8 ‣ Appendix
    A Kernel Implementation ‣ A Speed Odyssey for Deployable Quantization of LLMs")
    (supp.) 节），特别是还有一种额外的设备内减法操作，这在硬件中并未直接支持。默认情况下，这必须转换为更高精度（通常为 INT32）进行减法，这会产生可观的成本。我们在图
    [7](#S7.F7 "Figure 7 ‣ 7.2 FastGEMM vs. Fine-grained vs. Asymmetric ‣ 7 Ablation
    Study ‣ A Speed Odyssey for Deployable Quantization of LLMs") 中将其实现为 Asym GEMM
    以观察其成本。由于我们有探索 W4A8 真实效益的动机，我们必须制作一个更快的 W4A8 方案，以便将签名 INT4 加载到签名 INT8 中。具体来说，我们将签名
    INT4 的权重存储在补码中，如图 [4](#S5.F4 "Figure 4 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") (d) 所示。在计算过程中，我们将签名 INT4
    权重放置在签名 INT8 的高 4 位中，这相当于将每个值乘以 16。在 GEMM 计算完成后，我们将输出值除以 16 以恢复正确结果。这种乘法和除法可以轻松高效地实现。由于内部累加器为
    INT32 类型，因此不会发生溢出。通过这一新颖的转换方案，我们可以在显著提高推理速度的同时保持精度，并且可以直接在现代 GPU 上使用。
- en: '![Refer to caption](img/d68d0f61a4a1a11afe6bfed9447a8e35.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d68d0f61a4a1a11afe6bfed9447a8e35.png)'
- en: 'Figure 6: Latency comparison on LLaMA-2 models in various bitwidth settings.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在不同比特宽度设置下的 LLaMA-2 模型的延迟比较。
- en: 6 Experiments
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验
- en: 6.1 Settings
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 设置
- en: Inference Implementation
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理实现
- en: The latency comparison in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Speed
    Odyssey for Deployable Quantization of LLMs") and Fig. [6](#S5.F6 "Figure 6 ‣
    5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") is made fair by utilizing the same set of configurations
    except for bit widths. Specifically, we implemented the whole end-to-end inference
    pipeline with CUTLASS [[26](#bib.bib26)] to have a delicate combination of GPU
    Tensor Core execution, kernel fusion policy, and graph optimization. We also evaluate
    the performance with the latest build of TensorRT-LLM [[27](#bib.bib27)]. All
    latency measurements in the paper are tested on NVIDIA A100 80G GPUs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Speed Odyssey for Deployable Quantization
    of LLMs") 和图 [6](#S5.F6 "Figure 6 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM
    ‣ A Speed Odyssey for Deployable Quantization of LLMs") 中的延迟比较通过使用相同的配置集（除了比特宽度）来实现公平。具体来说，我们使用
    CUTLASS [[26](#bib.bib26)] 实现了完整的端到端推理管道，以精细组合 GPU Tensor Core 执行、内核融合策略和图优化。我们还用最新版本的
    TensorRT-LLM [[27](#bib.bib27)] 评估了性能。论文中的所有延迟测量均在 NVIDIA A100 80G GPU 上测试。
- en: Models and Datasets
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和数据集
- en: Following recent quantization methods  [[22](#bib.bib22), [11](#bib.bib11)],
    we evaluate our approach by conducting experiments on LLaMA series [[34](#bib.bib34),
    [35](#bib.bib35)] models and presenting results on various tasks. We randomly
    pick 128 sequences in C4  [[29](#bib.bib29)] datasets for calibration. We report
    the zero-shot performance on Common Sense Reasoning tasks such as PIQA [[2](#bib.bib2)],
    HellaSwag  [[43](#bib.bib43)], WinoGrande [[30](#bib.bib30)], ARC  [[4](#bib.bib4)].
    We also assess the few-shot performance on Common Sense [[33](#bib.bib33)] and
    MMLU  [[14](#bib.bib14)] datasets, along with perplexity scores on WikiText2  [[24](#bib.bib24)]
    and C4  [[29](#bib.bib29)] datasets.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 根据近期的量化方法[[22](#bib.bib22), [11](#bib.bib11)]，我们通过在LLaMA系列[[34](#bib.bib34),
    [35](#bib.bib35)]模型上进行实验来评估我们的方法，并展示在各种任务上的结果。我们随机选择了128个序列用于C4[[29](#bib.bib29)]数据集的校准。我们报告了在Common
    Sense Reasoning任务中的零-shot性能，如PIQA [[2](#bib.bib2)]、HellaSwag [[43](#bib.bib43)]、WinoGrande [[30](#bib.bib30)]和ARC [[4](#bib.bib4)]。我们还评估了在Common
    Sense [[33](#bib.bib33)]和MMLU [[14](#bib.bib14)]数据集上的少-shot性能，以及在WikiText2 [[24](#bib.bib24)]和C4
    [[29](#bib.bib29)]数据集上的困惑度分数。
- en: 6.2 Performance Comparison
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 性能比较
- en: Table [2](#S5.T2 "Table 2 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A
    Speed Odyssey for Deployable Quantization of LLMs") presents the performance comparison
    on various common datasets LAMBADA, C4, and WikiText, while Table [3](#S5.T3 "Table
    3 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") is on Common Sense QA and Table [8](#A2.T8 "Table 8 ‣ B.1
    Comparison on MMLU ‣ Appendix B Additional Experiments ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") (supp.) on MMLU. We compare our OdysseyLLM with the most
    recent state-of-art quantization methods AWQ [[19](#bib.bib19)], SmoothQuant [[39](#bib.bib39)],
    and GPTQ [[11](#bib.bib11)]. It turns out that our W4A8 OdysseyLLM mostly achieves
    on-par performance with the state-of-the-art W8A8 approach SmoothQuant on a large
    range of tasks, paving the way to its ready application in the real world. Being
    a post-training quantization method, we also enjoy the low-cost benefit during
    the quantization process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S5.T2 "Table 2 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A Speed
    Odyssey for Deployable Quantization of LLMs")展示了在各种常见数据集LAMBADA、C4和WikiText上的性能比较，而表[3](#S5.T3
    "Table 3 ‣ 5.1 Adaptive Weight Clipping ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")则是对Common Sense QA的比较，表[8](#A2.T8 "Table 8 ‣ B.1 Comparison
    on MMLU ‣ Appendix B Additional Experiments ‣ A Speed Odyssey for Deployable Quantization
    of LLMs")（附。）则是关于MMLU的比较。我们将我们的OdysseyLLM与最新的先进量化方法AWQ [[19](#bib.bib19)]、SmoothQuant [[39](#bib.bib39)]和GPTQ [[11](#bib.bib11)]进行了比较。结果表明，我们的W4A8
    OdysseyLLM在大量任务中大致达到了与最先进的W8A8方法SmoothQuant相当的性能，为其在现实世界中的应用奠定了基础。作为一种后训练量化方法，我们在量化过程中也享受到了低成本的优势。
- en: 6.3 Latency Comparison on LLaMA models
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 LLaMA模型的延迟比较
- en: We exhibit the overall latency comparison of LLaMA-2 models under the same implementations
    (Sec. [6.1](#S6.SS1 "6.1 Settings ‣ 6 Experiments ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")) except the bandwidths in Fig. [6](#S5.F6 "Figure 6 ‣ 5.3
    Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization
    of LLMs") where our W4A8 version prevails on all model scales. Notably, we achieve
    at most 1.9$\times$ boost compared with FP16 for LLaMA-2 7B, 13B, and 70B respectively.
    We use 1 GPU for 7B, and 13B, 4 GPUs for 70B. All inputs have an input sequence
    length of 1024\. Output tokens are set to 128.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在相同实现下（第[6.1节](#S6.SS1 "6.1 Settings ‣ 6 Experiments ‣ A Speed Odyssey
    for Deployable Quantization of LLMs")）LLaMA-2模型的整体延迟比较，除了带宽不同。在图[6](#S5.F6 "Figure
    6 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")中，我们的W4A8版本在所有模型规模上都表现优越。值得注意的是，与FP16相比，我们在LLaMA-2 7B、13B和70B模型上分别获得了最多1.9$\times$的性能提升。7B和13B使用1个GPU，70B使用4个GPU。所有输入的序列长度为1024。输出token设置为128。
- en: 6.4 Comparison with TensorRT-LLM
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 与TensorRT-LLM的比较
- en: TensorRT-LLM [[27](#bib.bib27)] is so far the most advanced industry-level deployment
    engine for LLMs, shipped with both FP16 and INT8 implementation. In Table [4](#S6.T4
    "Table 4 ‣ 6.4 Comparison with TensorRT-LLM ‣ 6 Experiments ‣ A Speed Odyssey
    for Deployable Quantization of LLMs"), we compare with TensorRT-LLM to show the
    benefits of our inference engine and newly fabricated kernel. The settings are
    kept the same as in Fig. [6](#S5.F6 "Figure 6 ‣ 5.3 Fast Mixed-Precision GEMM
    ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable Quantization of LLMs") except
    that here we use a batch size of 1\. Notice that our engine is mostly comparable
    to TensorRT-LLM in both FP16 and W8A8 settings. Our engine with the new W4A8 kernel
    obtains 1.37$\times$ against its FP16 setting.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: TensorRT-LLM [[27](#bib.bib27)] 迄今为止是最先进的行业级 LLM 部署引擎，提供 FP16 和 INT8 实现。在表 [4](#S6.T4
    "表 4 ‣ 6.4 与 TensorRT-LLM 比较 ‣ 6 实验 ‣ 可部署量化 LLM 的速度奥德赛") 中，我们与 TensorRT-LLM 进行了比较，以展示我们的推理引擎和新制造的内核的优势。设置保持与图 [6](#S5.F6
    "图 6 ‣ 5.3 快速混合精度 GEMM ‣ 5 OdysseyLLM ‣ 可部署量化 LLM 的速度奥德赛") 相同，但在这里我们使用批量大小为 1。请注意，我们的引擎在
    FP16 和 W8A8 设置下大体上与 TensorRT-LLM 相当。我们的引擎配备的新 W4A8 内核在 FP16 设置下获得了 1.37$\times$
    的提升。
- en: '| Model | TensorRT-LLM | Ours |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | TensorRT-LLM | 我们的 |'
- en: '| --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | FP16 | W8A8 | FP16 | W8A8 | W4A8 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | W8A8 | FP16 | W8A8 | W4A8 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 1411 | 1030 | 1513 | 1103 | 751 (1.37$\times$) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 1411 | 1030 | 1513 | 1103 | 751 (1.37$\times$) |'
- en: '| LLaMA-2-13B | 2547 | 1657 | 2671 | 1824 | 1139 (1.45$\times$) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 2547 | 1657 | 2671 | 1824 | 1139 (1.45$\times$) |'
- en: '| LLaMA-2-70B | 4177 | 3087 | 4271 | 3135 | 2263 (1.36$\times$) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | 4177 | 3087 | 4271 | 3135 | 2263 (1.36$\times$) |'
- en: 'Table 4: Latency comparison (in $ms$) with TensorRT-LLM.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：与 TensorRT-LLM 的延迟比较（单位：$ms$）。
- en: 6.5 Comparison with QUIK
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 与 QUIK 的比较
- en: QUIK [[1](#bib.bib1)] comes with a W4A4 implementation while outliers fall back
    to higher precision. We show that how such an approach renders an overall inferior
    speed in practice. Per-kernel measurements are shown in Table [5](#S6.T5 "Table
    5 ‣ 6.5 Comparison with QUIK ‣ 6 Experiments ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") where our speed can be 4.33$\times$ faster in the self-decoding
    stage. QUIK is only on par with our speed at the context decoding stage since
    it is more computation-intensive. This benefit is quickly amortized throughout
    an end-to-end setting. See Sec. [A.2](#A1.SS2 "A.2 Analysis on QUIK’s Latency
    ‣ Appendix A Kernel Implementation ‣ A Speed Odyssey for Deployable Quantization
    of LLMs") for a detailed analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: QUIK [[1](#bib.bib1)] 提供了 W4A4 实现，而离群值则回退到更高精度。我们展示了这种方法在实际应用中导致整体速度较慢。每个内核的测量结果见表 [5](#S6.T5
    "表 5 ‣ 6.5 与 QUIK 的比较 ‣ 6 实验 ‣ 可部署量化 LLM 的速度奥德赛")，在自解码阶段我们的速度可以快 4.33$\times$。QUIK
    仅在上下文解码阶段与我们的速度持平，因为它计算密集型。这种优势在端到端设置中很快被摊销。详细分析见第[A.2](#A1.SS2 "A.2 关于 QUIK 延迟的分析
    ‣ 附录 A 内核实现 ‣ 可部署量化 LLM 的速度奥德赛")节。
- en: '| Stage | M | N | K | QUIK | Odyssey | Boost |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 阶段 | M | N | K | QUIK | Odyssey | Boost |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Context decode | 1024 | 4096 | 4096 | 0.139 | 0.121 | 1.14$\times$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 上下文解码 | 1024 | 4096 | 4096 | 0.139 | 0.121 | 1.14$\times$ |'
- en: '| 1024 | 8192 | 0.095 | 0.073 | 1.30$\times$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 8192 | 0.095 | 0.073 | 1.30$\times$ |'
- en: '| 11088 | 4096 | 0.290 | 0.279 | 1.03$\times$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 11088 | 4096 | 0.290 | 0.279 | 1.03$\times$ |'
- en: '| 5120 | 5120 | 0.163 | 0.158 | 1.03$\times$ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 5120 | 5120 | 0.163 | 0.158 | 1.03$\times$ |'
- en: '| Self-decode | 1 | 4096 | 4096 | 0.052 | 0.012 | 4.33$\times$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 自解码 | 1 | 4096 | 4096 | 0.052 | 0.012 | 4.33$\times$ |'
- en: '| 1024 | 8192 | 0.080 | 0.019 | 4.21$\times$ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | 8192 | 0.080 | 0.019 | 4.21$\times$ |'
- en: '| 11088 | 4096 | 0.054 | 0.016 | 3.37$\times$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 11088 | 4096 | 0.054 | 0.016 | 3.37$\times$ |'
- en: '| 5120 | 5120 | 0.060 | 0.014 | 4.28$\times$ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 5120 | 5120 | 0.060 | 0.014 | 4.28$\times$ |'
- en: 'Table 5: GEMM latency comparison with QUIK. N stands for the output dimension
    of weight, M$\times$K for activation shape'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：与 QUIK 的 GEMM 延迟比较。N 代表权重的输出维度，M$\times$K 代表激活形状
- en: 7 Ablation Study
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 消融研究
- en: 7.1 Quantization Strategy
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 量化策略
- en: Table [6](#S7.T6 "Table 6 ‣ 7.1 Quantization Strategy ‣ 7 Ablation Study ‣ A
    Speed Odyssey for Deployable Quantization of LLMs") justifies our choices of symmetric
    LWC and GPTQ. Vanilla W4A8 which doesn’t involve compensation techniques falls
    short in the performance (PPL) on WikiText2 and C4\. The recipe of LWC and GPTQ
    combined generally produces the best result.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S7.T6 "表 6 ‣ 7.1 量化策略 ‣ 7 消融研究 ‣ 可部署量化 LLM 的速度奥德赛") 证明了我们选择对称 LWC 和 GPTQ
    的理由。未使用补偿技术的原始 W4A8 在 WikiText2 和 C4 上的性能（PPL）不佳。结合 LWC 和 GPTQ 的方案通常能产生最佳结果。
- en: '| Dataset | Model | Baseline | B+LWC | B+LWC+GPTQ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 模型 | 基线 | B+LWC | B+LWC+GPTQ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| WikiText2 | LLaMA-1-7B | 6.73 | 6.25 | 6.17 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| WikiText2 | LLaMA-1-7B | 6.73 | 6.25 | 6.17 |'
- en: '| LLaMA-1-13B | 5.7 | 5.37 | 5.37 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-13B | 5.7 | 5.37 | 5.37 |'
- en: '| LLaMA-1-65B | 4.41 | 3.89 | 3.92 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-65B | 4.41 | 3.89 | 3.92 |'
- en: '| LLaMA-2-7B | 7.13 | 6.73 | 6.11 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 7.13 | 6.73 | 6.11 |'
- en: '| LLaMA-2-13B | 5.47 | 5.30 | 5.19 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 5.47 | 5.30 | 5.19 |'
- en: '| LLaMA-2-70B | 3.93 | 3.74 | 3.70 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | 3.93 | 3.74 | 3.70 |'
- en: '| C4 | LLaMA-1-7B | 8.16 | 7.64 | 7.50 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| C4 | LLaMA-1-7B | 8.16 | 7.64 | 7.50 |'
- en: '| LLaMA-1-13B | 7.24 | 6.92 | 6.88 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-13B | 7.24 | 6.92 | 6.88 |'
- en: '| LLaMA-1-65B | 6.35 | 5.97 | 5.93 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-1-65B | 6.35 | 5.97 | 5.93 |'
- en: '| LLaMA-2-7B | 8.88 | 8.54 | 7.58 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 8.88 | 8.54 | 7.58 |'
- en: '| LLaMA-2-13B | 7.0 | 6.84 | 6.70 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 7.0 | 6.84 | 6.70 |'
- en: '| LLaMA-2-70B | 6.01 | 5.83 | 5.78 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | 6.01 | 5.83 | 5.78 |'
- en: 'Table 6: PPL on WikiText2 and C4\. Baseline (B): Vanilla W4A8, LWC: symmetric
    learnable weight clipping'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: WikiText2 和 C4 上的 PPL。基线（B）：Vanilla W4A8，LWC：对称可学习权重剪枝'
- en: 7.2 FastGEMM vs. Fine-grained vs. Asymmetric
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 FastGEMM vs. 精细化 vs. 不对称
- en: We conducted an in-depth study on different matrix multiplication (GEMM) implementation
    strategies on the LLaMA-2-70B model to understand their impact on performance.
    As shown in Fig. [7](#S7.F7 "Figure 7 ‣ 7.2 FastGEMM vs. Fine-grained vs. Asymmetric
    ‣ 7 Ablation Study ‣ A Speed Odyssey for Deployable Quantization of LLMs"), the
    horizontal axis represents the GEMM Size $(dim_{i},dim_{o})$ of the model under
    a partitioning on 4 GPUs, with an input length of 1024 and a batch size of 8\.
    Noticeably, fine-grained GEMM requires frequent dequantization operations per
    group, introducing a large amount of Integer2Float and Fused Multiply-Add (FMA)
    overhead; the signed 8-bit subtraction operation in Asymmetric GEMM needs to fallback
    to signed 32-bit, introducing additional conversion cost. In contrast, our FastGEMM
    well solves the drawbacks of these two GEMMs, achieving the best performance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 LLaMA-2-70B 模型上不同矩阵乘法（GEMM）实现策略进行了深入研究，以了解它们对性能的影响。如图 [7](#S7.F7 "图 7 ‣
    7.2 FastGEMM vs. 精细化 vs. 不对称 ‣ 7 消融研究 ‣ A 可部署量化的加速奥德赛") 所示，横轴表示模型在 4 个 GPU 上的
    GEMM 大小 $(dim_{i},dim_{o})$，输入长度为 1024，批量大小为 8。值得注意的是，精细化 GEMM 需要每组进行频繁的去量化操作，带来大量的
    Integer2Float 和 Fused Multiply-Add (FMA) 开销；不对称 GEMM 中的带符号 8 位减法操作需要回退到带符号 32
    位，引入额外的转换成本。相比之下，我们的 FastGEMM 很好地解决了这两种 GEMM 的缺点，达到了最佳性能。
- en: '![Refer to caption](img/f8fc3535d92622ac57d5dca7f8922c96.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f8fc3535d92622ac57d5dca7f8922c96.png)'
- en: 'Figure 7: Latency comparison (measured in nanoseconds) of all GEMMs in LLaMA-2-70B
    with tensor parallelism of 4 at two decoding stages. The matrix size is the input
    and output dimensions of GEMM. We use a batch size of 8 and an input length of
    1024\. The number atop the bar is the boost w.r.t. fine-grained GEMM.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 在两个解码阶段，所有 GEMMs 在 LLaMA-2-70B 中的延迟比较（以纳秒为单位），张量并行度为 4。矩阵大小为 GEMM 的输入和输出维度。我们使用批量大小
    8 和输入长度 1024。柱子上的数字是相对于精细化 GEMM 的提升。'
- en: 8 Conclusion
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: Under the hardware-centric guidance, our paper introduces the first deployable
    W4A8 solution for LLMs. We give a composition of recipes named OdysseyLLM which
    comprises symmetric learnable weight clipping, iterative Hessian-based compensation,
    and a novel FastGEMM for an accelerated W4A8 calculation. To our knowledge, we
    have achieved the fastest W4A8 LLMs available so far, with acceptable quantization
    loss on common language benchmarks. We believe this exploration can serve as a
    solid ground for further readily applicable compression algorithms to emerge,
    reducing LLM inference cost and facilitating potential applications in constrained
    scenarios.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在以硬件为中心的指导下，我们的论文介绍了首个可部署的 W4A8 解决方案。我们提供了一种名为 OdysseyLLM 的配方组合，包含对称可学习权重剪枝、基于
    Hessian 的迭代补偿，以及一种新型的 FastGEMM 用于加速 W4A8 计算。根据我们的了解，我们已实现目前最快的 W4A8 LLM，并且在常见语言基准上具有可接受的量化损失。我们相信这一探索可以作为进一步易于应用的压缩算法出现的坚实基础，从而降低
    LLM 推断成本，促进在受限场景中的潜在应用。
- en: References
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end
    4-bit inference on generative large language models. *arXiv preprint arXiv:2310.09259*,
    2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等人 [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong,
    Xincheng Wang, Jie Ren, Torsten Hoefler, 和 Dan Alistarh. 关于生成大型语言模型的端到端 4-bit
    推断。*arXiv 预印本 arXiv:2310.09259*，2023。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, pages 7432–7439, 2020.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, 等。Piqa:
    关于自然语言的物理常识推理。*Proceedings of the AAAI conference on artificial intelligence*，第
    7432–7439 页，2020。'
- en: 'Choi et al. [2018] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce
    I-Jen Chuang, Vijayalakshmi Srinivasan, and Kailash Gopalakrishnan. Pact: Parameterized
    clipping activation for quantized neural networks. *arXiv preprint arXiv:1805.06085*,
    2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Choi et al. [2018] Jungwook Choi, Zhuo Wang, Swagath Venkataramani, Pierce
    I-Jen Chuang, Vijayalakshmi Srinivasan, 和 Kailash Gopalakrishnan. Pact: 量化神经网络的参数化裁剪激活。*arXiv
    预印本 arXiv:1805.06085*, 2018。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 认为你解决了问答问题？试试 arc，AI2 推理挑战。*arXiv
    预印本 arXiv:1803.05457*, 2018。
- en: Dettmers et al. [2021] Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer.
    8-bit optimizers via block-wise quantization. *arXiv preprint arXiv:2110.02861*,
    2021.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. [2021] Tim Dettmers, Mike Lewis, Sam Shleifer, 和 Luke Zettlemoyer.
    通过块状量化的 8 位优化器。*arXiv 预印本 arXiv:2110.02861*, 2021。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    LLM.int8(): 8-bit matrix multiplication for transformers at scale. *arXiv preprint
    arXiv:2208.07339*, 2022.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer.
    LLM.int8(): 大规模变换器的 8 位矩阵乘法。*arXiv 预印本 arXiv:2208.07339*, 2022。'
- en: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers et al. [2023] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke
    Zettlemoyer. Qlora: 高效的量化 LLM 微调。*arXiv 预印本 arXiv:2305.14314*, 2023。'
- en: 'Du et al. [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. *arXiv preprint arXiv:2103.10360*, 2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du et al. [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, 和 Jie Tang. Glm: 带有自回归空白填充的通用语言模型预训练。*arXiv 预印本 arXiv:2103.10360*,
    2021。'
- en: Esser et al. [2019] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, and Dharmendra S Modha. Learned step size quantization. *arXiv preprint
    arXiv:1902.08153*, 2019.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Esser et al. [2019] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar
    Appuswamy, 和 Dharmendra S Modha. 学习的步长量化。*arXiv 预印本 arXiv:1902.08153*, 2019。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh [2023] Elias Frantar 和 Dan Alistarh. Sparsegpt: 大型语言模型可以被一次性准确地修剪，2023。'
- en: 'Frantar et al. [2022a] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022a.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022a] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan
    Alistarh. Gptq: 生成预训练变换器的精确后训练量化。*arXiv 预印本 arXiv:2210.17323*, 2022a。'
- en: 'Frantar et al. [2022b] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal
    Brain Compression: A framework for accurate post-training quantization and pruning.
    *arXiv preprint arXiv:2208.11580*, 2022b. Accepted to NeurIPS 2022, to appear.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. [2022b] Elias Frantar, Sidak Pal Singh, 和 Dan Alistarh. Optimal
    Brain Compression: 精确的后训练量化和修剪框架。*arXiv 预印本 arXiv:2208.11580*, 2022b。接受至 NeurIPS
    2022，待出现。'
- en: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models. *arXiv preprint arXiv:2306.08543*, 2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 大型语言模型的知识蒸馏。*arXiv
    预印本 arXiv:2306.08543*, 2023。
- en: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *arXiv preprint arXiv:2009.03300*, 2020.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. [2020] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, 和 Jacob Steinhardt. 测量大规模多任务语言理解。*arXiv 预印本 arXiv:2009.03300*,
    2020。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*,
    2021。'
- en: 'Laurençon et al. [2022] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher
    Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
    Mou, Eduardo González Ponferrada, Huu Nguyen, et al. The BigScience corpus: A
    1.6 TB composite multilingual dataset. 2022.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laurençon et al. [2022] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher
    Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
    Mou, Eduardo González Ponferrada, Huu Nguyen 等人。BigScience 语料库：一个 1.6 TB 复合多语言数据集。2022年。
- en: 'Li et al. [2023] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, and Yuchen Xie. Fptq: Fine-grained post-training quantization
    for large language models. *arXiv preprint arXiv:2308.15987*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, 和 Yuchen Xie. Fptq：大语言模型的细粒度后训练量化。*arXiv 预印本 arXiv:2308.15987*，2023年。
- en: 'Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization
    by block reconstruction. In *International Conference on Learning Representations
    (ICLR)*, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2021] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, 和 Shi Gu. BRECQ：通过块重建推动后训练量化的极限。在*国际学习表征会议 (ICLR)*，2021年。
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq：用于 LLM 压缩和加速的激活感知权重量化，2023年。
- en: 'Liu et al. [2023a] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. Llm-fp4: 4-bit floating-point quantized transformers. *arXiv
    preprint arXiv:2310.16836*, 2023a.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, 和
    Kwang-Ting Cheng. LLM-FP4：4位浮点量化变换器。*arXiv 预印本 arXiv:2310.16836*，2023a年。
- en: 'Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models. *arXiv
    preprint arXiv:2305.17888*, 2023b.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    LLM-QAT：面向大语言模型的无数据量化感知训练。*arXiv 预印本 arXiv:2305.17888*，2023b年。
- en: 'Liu et al. [2023c] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models, 2023c.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023c] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    LLM-QAT：面向大语言模型的无数据量化感知训练，2023c年。
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2023] Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. LLM-Pruner：关于大语言模型的结构化剪枝，2023年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning (ICML)*, 2020.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2020] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, 和 Tijmen Blankevoort。向上还是向下？用于后训练量化的自适应舍入。在*国际机器学习会议 (ICML)*，2020年。
- en: NVIDIA [2023a] NVIDIA. Cutlass 3.0. [https://github.com/NVIDIA/cutlass](https://github.com/NVIDIA/cutlass),
    2023a.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2023a] NVIDIA。Cutlass 3.0。 [https://github.com/NVIDIA/cutlass](https://github.com/NVIDIA/cutlass)，2023a年。
- en: NVIDIA [2023b] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023b.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2023b] NVIDIA。Tensorrt-llm。 [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)，2023b年。
- en: 'Paperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse
    context. *arXiv preprint arXiv:1606.06031*, 2016.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    和 Raquel Fernández. LAMBADA 数据集：需要广泛话语上下文的词预测。*arXiv 预印本 arXiv:1606.06031*，2016年。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the
    limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21(140):1–67, 2020.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] 科林·拉费尔、诺姆·沙泽尔、亚当·罗伯茨、凯瑟琳·李、沙兰·纳朗、迈克尔·马特纳、颜奇·周、魏·李和彼得·刘。探索统一文本到文本变换器的迁移学习极限。*机器学习研究杂志*，21(140):1–67，2020年。
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi et al. [2021] 坂口圭介、罗南·勒布拉斯、钱德拉·巴伽瓦图拉和叶金·崔。Winogrande：一个大规模对抗性Winograd模式挑战。*ACM通讯*，64(9):99–106，2021年。
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. [2023] 文琦·邵、孟钊·陈、赵阳·张、彭旭、李睿·赵、志倩·李、凯鹏·张、彭高、于乔和平洛。Omniquant：面向大型语言模型的全方位校准量化。*arXiv预印本
    arXiv:2308.13137*，2023年。
- en: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2023] 明杰·孙、庄刘、安娜·贝尔和J Zico Kolter。一种简单有效的大型语言模型剪枝方法。*arXiv预印本 arXiv:2306.11695*，2023年。
- en: 'Talmor et al. [2019] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan
    Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge,
    2019.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor et al. [2019] 阿隆·塔尔莫尔、乔纳森·赫尔齐格、尼古拉斯·洛里和乔纳森·贝朗特。Commonsenseqa：一个针对常识知识的问答挑战，2019年。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023a] 休戈·图弗龙、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、萨维尔·马尔坦、玛丽-安·拉肖、蒂莫西·拉克鲁瓦、巴普蒂斯特·罗济埃尔、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿兹哈尔等。Llama：开放和高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*，2023年。
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023b] 休戈·图弗龙、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔迈赫里、雅斯敏·巴巴伊、尼古拉·巴什利科夫、苏缅·巴特拉、普拉杰瓦尔·巴尔戈瓦、舒尔提·博萨尔等。Llama
    2：开放的基础和微调聊天模型。*arXiv预印本 arXiv:2307.09288*，2023年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Conference on Neural Information Processing Systems (NeurIPS)*,
    2017.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] 阿什什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔玛、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡斯·凯泽和伊利亚·波洛苏金。注意力即你所需。在*神经信息处理系统会议（NeurIPS）*，2017年。
- en: 'Wu et al. [2020] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius
    Micikevicius. Integer quantization for deep learning inference: Principles and
    empirical evaluation, 2020.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2020] 郝武、帕特里克·贾德、肖杰·张、米哈伊尔·伊萨耶夫和保柳斯·米西凯维丘斯。深度学习推理的整数量化：原理与实证评估，2020年。
- en: 'Wu et al. [2023] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats,
    2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2023] 肖霞·吴、哲伟·姚和玉雄·何。Zeroquant-fp：使用浮点格式在LLMs后训练W4A8量化中的一大进步，2023年。
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. [2023] 广宣肖、纪林、米卡埃尔·塞兹内克、郝武、朱利安·德穆斯和宋涵。Smoothquant：大型语言模型的准确而高效的后训练量化。发表于*国际机器学习会议*，第38087–38099页。PMLR，2023年。
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training
    quantization for large-scale transformers. *arXiv preprint arXiv:2206.01861*,
    2022.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2022] 哲伟·姚、雷扎·雅兹达尼·阿米纳巴迪、敏佳·张、肖霞·吴、从龙·李和玉雄·何。ZeroQuant：高效且经济的大规模变换器的后训练量化。*arXiv预印本
    arXiv:2206.01861*，2022年。
- en: 'Yao et al. [2023] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive
    study to low rank compensation, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2023] 哲伟·姚、肖霞·吴、程李、斯蒂芬·尹和玉雄·何。Zeroquant-v2：从全面研究到低秩补偿，探索LLMs的后训练量化，2023年。
- en: 'Yuan et al. [2023] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '袁等 [2023] 朱航袁、刘林、刘佳伟、刘文宇、王兴刚、商宇璋、孙光宇、吴强、吴家祥和吴冰哲。Rptq: 基于重排序的后训练量化用于大型语言模型，2023年。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '泽勒斯等 [2019] 罗温·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和叶津·崔。Hellaswag: 机器真的能完成你的句子吗？*arXiv
    预印本 arXiv:1905.07830*，2019年。'
- en: Zhang et al. [2023] Mingyang Zhang, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi
    Yu, Bohan Zhuang, et al. Pruning meets low-rank parameter-efficient fine-tuning.
    *arXiv preprint arXiv:2305.18403*, 2023.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等 [2023] 明扬张、春华申、杨震、欧琳琳、于欣怡、庄博涵等。剪枝遇到低秩参数高效微调。*arXiv 预印本 arXiv:2305.18403*，2023年。
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. OPT: Open pre-trained transformer language models. *arXiv preprint arXiv:2205.01068*,
    2022.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等 [2022] 苏珊张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿特克斯、莫雅·陈、舒慧·陈、克里斯托弗·德万、莫娜·迪亚布、李贤、维多利亚·林等。OPT:
    开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*，2022年。'
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等 [2023] 朱寻宇、李剑、刘勇、马灿和王伟平。大型语言模型的模型压缩综述。*arXiv 预印本 arXiv:2308.07633*，2023年。
- en: Appendix A Kernel Implementation
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 内核实现
- en: A.1 UINT4toS8 vs. SINT4toS8
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 UINT4toS8 与 SINT4toS8
- en: In modern computer systems, integer numbers are commonly stored as two’s complement
    for multiple known benefits. To move these numbers from host to device requires
    specific offline preprocessing. As illustrated in Fig. [5](#S5.F5 "Figure 5 ‣
    5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs") (main text), say we have a signed integer -7 represented
    in its two’s complement as 1111 1001. We first have to rearrange within the range
    (0,15) by adding 8 to have an unsigned integer 0000 0001. The lower 4-bit 0001
    can be utilized for weight packing (*e.g*. four UINT4 integers packed in 32 bits).
    Once the GPU receives such 32 bits, it is unpacked first to obtain each of these
    4 numbers. Here comes the problem, to revert such a UINT4 number to an SINT8 for
    later GEMM computation, we have to subtract 8 again, which is not directly available
    for GPUs. One has to convert it to INT32 to enable subtraction, which is very
    costly in practice. To avoid the unexpected cost, we simplify the pipeline by
    directly packing the lower 4 bits 1001. During unpacking, we place them in the
    higher 4 bits on a piece of INT8 GPU memory, which in effect renders a signed
    integer number in 8 bits, but 16 times larger. We later divide the GEMM result
    by 16 to restore the correct value. This new implementation substantially eases
    the pain of type conversion and speeds up the overall performance of the FastGEMM
    kernel.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代计算机系统中，整数通常以二补码形式存储，以获得多种已知的好处。将这些数字从主机转移到设备需要特定的离线预处理。如图 [5](#S5.F5 "Figure
    5 ‣ 5.3 Fast Mixed-Precision GEMM ‣ 5 OdysseyLLM ‣ A Speed Odyssey for Deployable
    Quantization of LLMs")（正文）所示，假设我们有一个以二补码表示的有符号整数 -7，其表示为 1111 1001。我们首先需要通过加 8
    将其重排到范围 (0,15) 中，以得到一个无符号整数 0000 0001。较低的 4 位 0001 可用于权重打包（*例如*，将四个 UINT4 整数打包为
    32 位）。一旦 GPU 收到这样的 32 位数据，它首先被解包以获得这 4 个数字。问题在于，为了将这样的 UINT4 数字还原为 SINT8 以便进行后续的
    GEMM 计算，我们需要再次减去 8，这对于 GPU 来说并不直接可用。必须将其转换为 INT32 以进行减法，这在实践中代价很高。为了避免意外成本，我们通过直接打包较低的
    4 位 1001 简化了管道。在解包过程中，我们将它们放置在 INT8 GPU 内存的高 4 位上，这实际上呈现为 8 位的有符号整数，但大小是原来的 16
    倍。我们随后将 GEMM 结果除以 16 以恢复正确的值。这种新实现大大减轻了类型转换的难度，并加快了 FastGEMM 内核的整体性能。
- en: A.2 Analysis on QUIK’s Latency
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 QUIK 延迟分析
- en: In Table [5](#S6.T5 "Table 5 ‣ 6.5 Comparison with QUIK ‣ 6 Experiments ‣ A
    Speed Odyssey for Deployable Quantization of LLMs") in the main text, it has been
    observed that QUIK’s performance is substantially poor during the self-decoding
    phase. We discover the reason lies in their various separated CUTLUSS kernels
    to adapt the mixed precision recipe. Ideally, pure W4A4 computation would be 2$\times$
    speed boosting.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在主文的表格 [5](#S6.T5 "表格 5 ‣ 6.5 与 QUIK 的比较 ‣ 6 实验 ‣ LLM 可部署量化的速度奥德赛") 中，观察到 QUIK
    在自解码阶段的性能大幅下降。我们发现原因在于它们为了适应混合精度方案而采用了各种分离的 CUTLUSS 内核。理想情况下，纯 W4A4 计算应能提升 2$\times$
    的速度。
- en: A.3 Latency Comparison with Hugging Face
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 与 Hugging Face 的延迟比较
- en: Hugging Face⁴⁴4[https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    provides a 4-bit implementation with the bitsandbytes library ⁵⁵5[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
    We compare their latencies in Table [7](#A1.T7 "Table 7 ‣ A.3 Latency Comparison
    with Hugging Face ‣ Appendix A Kernel Implementation ‣ A Speed Odyssey for Deployable
    Quantization of LLMs"). Note that this 4-bit implementation is even slower than
    Hugging Face’s FP16 implementation, which prevents it from being a real application.
    It adopts a particular normal format 4-bit (NF4) [[5](#bib.bib5)] to pursue a
    higher precision and reduced memory, however, it comes at the cost of an extremely
    complex computation strategy which ultimately leads to even worse speed compared
    with FP16.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face⁴⁴4[https://huggingface.co/blog/4bit-transformers-bitsandbytes](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
    提供了一个使用 bitsandbytes 库的 4-bit 实现 ⁵⁵5[https://github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)。我们在表格
    [7](#A1.T7 "表格 7 ‣ A.3 与 Hugging Face 的延迟比较 ‣ 附录 A 内核实现 ‣ LLM 可部署量化的速度奥德赛") 中比较了它们的延迟。请注意，这个
    4-bit 实现甚至比 Hugging Face 的 FP16 实现更慢，这使得它无法成为真正的应用。它采用了一种特定的标准格式 4-bit (NF4) [[5](#bib.bib5)]
    来追求更高的精度和减少内存，但代价是采用了极其复杂的计算策略，最终导致相较于 FP16 更差的速度。
- en: '| Model | BS | Hugging Face | Ours |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 批量大小 | Hugging Face | 我们的方法 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  | FP16 | 4-bit | W4A8 | vs. HF F16 | vs. HF 4-bit |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | FP16 | 4-bit | W4A8 | vs. HF F16 | vs. HF 4-bit |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | 1 | 3439 | 6602 | 751 | 4.57$\times$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 1 | 3439 | 6602 | 751 | 4.57$\times$ |'
- en: '| LLaMA-2-7B | 4 | 3769 | 10790 | 935 | 4.03$\times$ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | 4 | 3769 | 10790 | 935 | 4.03$\times$ |'
- en: '| LLaMA-2-13B | 1 | 4578 | 8596 | 1139 | 4.01$\times$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 1 | 4578 | 8596 | 1139 | 4.01$\times$ |'
- en: '| LLaMA-2-13B | 4 | 5610 | 19435 | 1447 | 3.87$\times$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | 4 | 5610 | 19435 | 1447 | 3.87$\times$ |'
- en: 'Table 7: Latency comparison (in $ms$) with Hugging Face.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：与 Hugging Face 的延迟比较（单位：$ms$）。
- en: Appendix B Additional Experiments
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外实验
- en: B.1 Comparison on MMLU
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 MMLU 比较
- en: Table. [8](#A2.T8 "Table 8 ‣ B.1 Comparison on MMLU ‣ Appendix B Additional
    Experiments ‣ A Speed Odyssey for Deployable Quantization of LLMs") gives OdysseyLLM
    compared with the state-of-the-art methods on MMLU, which are mostly comparable
    with the W4A8 solution.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [8](#A2.T8 "表格 8 ‣ B.1 MMLU 比较 ‣ 附录 B 额外实验 ‣ LLM 可部署量化的速度奥德赛") 显示了 OdysseyLLM
    与当前最先进的方法在 MMLU 上的比较，结果大多数与 W4A8 解决方案相当。
- en: '| Model | Method | BitWidth | Hums. | STEM | Social | Other | Avg |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 位宽 | 人文 | STEM | 社会 | 其他 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMa-1-7B | FP16 | W16A16 | 33.65% | 31.05% | 38.22% | 38.43% | 35.19% |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-1-7B | FP16 | W16A16 | 33.65% | 31.05% | 38.22% | 38.43% | 35.19% |'
- en: '| AWQ-g128 | W4A16 | 31.86% | 30.62% | 35.72% | 38.62% | 34.00% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 31.86% | 30.62% | 35.72% | 38.62% | 34.00% |'
- en: '| GPTQ-g128 | W4A16 | 31.56% | 29.82% | 36.69% | 36.34% | 33.41% |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 31.56% | 29.82% | 36.69% | 36.34% | 33.41% |'
- en: '| SmoothQuant^∗ | W8A8 | 33.90% | 30.75% | 37.80% | 40.19% | 35.53% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 33.90% | 30.75% | 37.80% | 40.19% | 35.53% |'
- en: '| OdysseyLLM | W4A8 | 32.56% | 30.38% | 35.13% | 38.49% | 34.03% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 32.56% | 30.38% | 35.13% | 38.49% | 34.03% |'
- en: '| LLaMa-1-13B | FP16 | W16A16 | 44.61% | 37.08% | 54.05% | 53.52% | 47.12%
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-1-13B | FP16 | W16A16 | 44.61% | 37.08% | 54.05% | 53.52% | 47.12%
    |'
- en: '| AWQ-g128 | W4A16 | 43.23% | 34.86% | 51.41% | 51.20% | 45.06% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 43.23% | 34.86% | 51.41% | 51.20% | 45.06% |'
- en: '| GPTQ-g128 | W4A16 | 42.98% | 36.28% | 52.42% | 51.76% | 45.63% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 42.98% | 36.28% | 52.42% | 51.76% | 45.63% |'
- en: '| SmoothQuant^∗ | W8A8 | 44.25% | 35.98% | 52.97% | 52.38% | 46.26% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 44.25% | 35.98% | 52.97% | 52.38% | 46.26% |'
- en: '| OdysseyLLM | W4A8 | 42.15% | 35.69% | 51.48% | 50.74% | 44.79% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 42.15% | 35.69% | 51.48% | 50.74% | 44.79% |'
- en: '| LLaMa-1-65B | FP16 | W16A16 | 61.76% | 51.99% | 73.29% | 67.58% | 63.53%
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-1-65B | FP16 | W16A16 | 61.76% | 51.99% | 73.29% | 67.58% | 63.53%
    |'
- en: '| AWQ-g128 | W4A16 | 60.66% | 50.93% | 71.53% | 66.47% | 62.29% |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 60.66% | 50.93% | 71.53% | 66.47% | 62.29% |'
- en: '| GPTQ-g128 | W4A16 | 60.40% | 51.16% | 71.66% | 66.72% | 62.34% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 60.40% | 51.16% | 71.66% | 66.72% | 62.34% |'
- en: '| SmoothQuant^∗ | W8A8 | 61.23% | 51.06% | 71.73% | 67.27% | 62.74% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 61.23% | 51.06% | 71.73% | 67.27% | 62.74% |'
- en: '| OdysseyLLM | W4A8 | 59.72% | 48.64% | 71.56% | 65.76% | 61.33% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 59.72% | 48.64% | 71.56% | 65.76% | 61.33% |'
- en: '| LLaMa-2-7B | FP16 | W16A16 | 36.92% | 30.75% | 40.92% | 45.68% | 38.49% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-2-7B | FP16 | W16A16 | 36.92% | 30.75% | 40.92% | 45.68% | 38.49% |'
- en: '| AWQ-g128 | W4A16 | 32.62% | 31.64% | 39.71% | 42.66% | 36.28% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 32.62% | 31.64% | 39.71% | 42.66% | 36.28% |'
- en: '| GPTQ-g128 | W4A16 | 36.20% | 31.88% | 40.23% | 44.51% | 38.07% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 36.20% | 31.88% | 40.23% | 44.51% | 38.07% |'
- en: '| SmoothQuant^∗ | W8A8 | 34.77% | 29.62% | 38.58% | 43.46% | 36.50% |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 34.77% | 29.62% | 38.58% | 43.46% | 36.50% |'
- en: '| OdysseyLLM | W4A8 | 34.41% | 28.83% | 40.66% | 41.39% | 36.19% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 34.41% | 28.83% | 40.66% | 41.39% | 36.19% |'
- en: '| LLaMa-2-13B | FP16 | W16A16 | 54.43% | 44.27% | 63.41% | 60.76% | 55.68%
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-2-13B | FP16 | W16A16 | 54.43% | 44.27% | 63.41% | 60.76% | 55.68%
    |'
- en: '| AWQ-g128 | W4A16 | 50.63% | 42.21% | 62.01% | 59.22% | 53.30% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 50.63% | 42.21% | 62.01% | 59.22% | 53.30% |'
- en: '| GPTQ-g128 | W4A16 | 50.44% | 43.34% | 62.14% | 60.27% | 53.75% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 50.44% | 43.34% | 62.14% | 60.27% | 53.75% |'
- en: '| SmoothQuant^∗ | W8A8 | 53.28% | 44.14% | 63.54% | 60.86% | 55.31% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 53.28% | 44.14% | 63.54% | 60.86% | 55.31% |'
- en: '| OdysseyLLM | W4A8 | 50.78% | 42.41% | 61.13% | 59.04% | 53.15% |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 50.78% | 42.41% | 61.13% | 59.04% | 53.15% |'
- en: '| LLaMa-2-70B | FP16 | W16A16 | 65.16% | 57.79% | 80.44% | 74.61% | 69.11%
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa-2-70B | FP16 | W16A16 | 65.16% | 57.79% | 80.44% | 74.61% | 69.11%
    |'
- en: '| AWQ-g128 | W4A16 | 64.44% | 57.89% | 79.62% | 73.60% | 68.47% |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| AWQ-g128 | W4A16 | 64.44% | 57.89% | 79.62% | 73.60% | 68.47% |'
- en: '| GPTQ-g128 | W4A16 | 64.02% | 56.66% | 80.11% | 74.06% | 68.28% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ-g128 | W4A16 | 64.02% | 56.66% | 80.11% | 74.06% | 68.28% |'
- en: '| SmoothQuant^∗ | W8A8 | 63.53% | 56.00% | 79.23% | 73.81% | 67.73% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| SmoothQuant^∗ | W8A8 | 63.53% | 56.00% | 79.23% | 73.81% | 67.73% |'
- en: '| OdysseyLLM | W4A8 | 63.12% | 55.40% | 78.29% | 72.49% | 66.95% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| OdysseyLLM | W4A8 | 63.12% | 55.40% | 78.29% | 72.49% | 66.95% |'
- en: 'Table 8: Comparison on MMLU. ^∗: per token for activations and per channel
    for weights'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：MMLU 比较。 ^∗：激活的每个 token 和权重的每个通道
