- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:52:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:52:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM行为的自我控制通过将后缀梯度压缩为前缀控制器
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02721](https://ar5iv.labs.arxiv.org/html/2406.02721)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02721](https://ar5iv.labs.arxiv.org/html/2406.02721)
- en: \doparttoc\faketableofcontentsMin Cai¹ &Yuchen Zhang² &Shichang Zhang³ &Fan
    Yin³
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontentsMin Cai¹ &Yuchen Zhang² &Shichang Zhang³ &Fan
    Yin³
- en: Difan Zou⁴ &Yisong Yue⁵ &Ziniu Hu⁵
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Difan Zou⁴ &Yisong Yue⁵ &Ziniu Hu⁵
- en: ¹Shenzhen University,   ²Rensselaer Polytechnic Institute
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹深圳大学，²伦斯勒理工学院
- en: ³University of California, Los Angeles,   ⁴The University of Hong Kong
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³加州大学洛杉矶分校，⁴香港大学
- en: ⁵California Institute of Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵加州理工学院
- en: '[https://llm-self-control.github.io/](https://llm-self-control.github.io/)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://llm-self-control.github.io/](https://llm-self-control.github.io/)'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We propose SelfControl, a novel method utilizing suffix gradients to control
    the behavior of large language models (LLMs) without explicit human annotations.
    Given a guideline expressed in suffix string and the model’s self-assessment of
    adherence, SelfControl computes the gradient of this self-judgment concerning
    the model’s hidden states, directly influencing the auto-regressive generation
    process towards desired behaviors. To enhance efficiency, we introduce SelfControl${}_{\textsc{prefix}}$
    enables a plug-and-play control and jointly controls multiple attributes, improving
    model outputs without altering model parameters or increasing inference-time costs.
    An Google Colab demo is available at: [demo](https://colab.research.google.com/drive/1PqROFczbIKoljYlaF9tUEGSIgTXIn1fY?usp=sharing)
    and our code at [code](https://github.com/HenryCai11/LLM-Self-Control).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了SelfControl，这是一种利用后缀梯度来控制大型语言模型（LLMs）行为的新方法，无需明确的人类注释。给定一个以后缀字符串表示的指导原则以及模型对遵循情况的自我评估，SelfControl计算这一自我判断相对于模型隐状态的梯度，直接影响自回归生成过程，达到期望的行为。为了提高效率，我们引入了SelfControl${}_{\textsc{prefix}}$，它实现了即插即用的控制并联合控制多个属性，改进模型输出，而无需改变模型参数或增加推理时间成本。Google
    Colab演示请见：[demo](https://colab.research.google.com/drive/1PqROFczbIKoljYlaF9tUEGSIgTXIn1fY?usp=sharing)，我们的代码在
    [code](https://github.com/HenryCai11/LLM-Self-Control)。
- en: '![Refer to caption](img/219513e1ad20266e887497ac55f5fd67.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/219513e1ad20266e887497ac55f5fd67.png)'
- en: 'Figure 1: Our SelfControl and SelfControl${}_{\textsc{prefix}}$ are able to
    control LLM behaviors such as emotion (fearlessness), helpfulness, and reasoning
    capability. On the right hand side, we show that these different attributes can
    be composed into a single prefix controller.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们的SelfControl和SelfControl${}_{\textsc{prefix}}$能够控制LLM的行为，如情感（无畏）、帮助性和推理能力。在右侧，我们展示了这些不同的属性可以组合成一个单一的前缀控制器。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) like GPT-4 [[2](#bib.bib2)], Claude [[3](#bib.bib3)],
    Gemini [[11](#bib.bib11)], and Llama [[44](#bib.bib44)] have emerged as powerful
    tools for understanding and generating human-like text. With such LLMs as foundation
    models, humans can write high-level instructions expressed through prompts to
    control model behaviors at inference time. However, there are many situations
    where LLMs may behave differently than the intended control. One fundamental hurdle
    to fine-grained model behavior control is the autoregressive nature of LLMs, where
    texts are generated token-by-token based on the preceding context. Unlike diffusion
    models [[21](#bib.bib21)], which allow for iterative adjustments during the generation
    process via classifier guidance [[12](#bib.bib12)], LLM outputs can only be evaluated
    after they are fully formed, which hinders the precise alignment of LLM outputs
    with desired attributes or constraints expressed in the user prompt. This lack
    of accurate and nuanced control becomes particularly problematic in sensitive
    and high-stakes areas where even minor deviations from factual accuracy or ethical
    standards can have significant consequences such as medical diagnosis [[43](#bib.bib43)]
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）如 GPT-4 [[2](#bib.bib2)]、Claude [[3](#bib.bib3)]、Gemini [[11](#bib.bib11)]
    和 Llama [[44](#bib.bib44)] 已成为理解和生成类人文本的强大工具。以这些 LLMs 作为基础模型，人类可以通过提示编写高级指令来控制模型在推理时的行为。然而，LLMs
    可能在很多情况下表现与预期控制不同。细致的模型行为控制的一个根本障碍是 LLMs 的自回归特性，其中文本是基于前文上下文逐词生成的。与扩散模型 [[21](#bib.bib21)]
    不同，扩散模型允许在生成过程中通过分类器指导进行迭代调整 [[12](#bib.bib12)]，LLM 的输出只能在完全形成后进行评估，这阻碍了 LLM 输出与用户提示中表达的期望属性或约束的精确对齐。这种缺乏准确和细致控制的问题在敏感和高风险领域尤为严重，因为即使是微小的事实准确性或伦理标准的偏差也可能产生重大后果，例如医学诊断
    [[43](#bib.bib43)]。
- en: Existing alignment approaches, such as online Reinforcement Learning from Human
    Feedback (RLHF) [[38](#bib.bib38)] or offline Direct Preference Optimization (DPO) [[40](#bib.bib40)],
    attempt to address this challenge by fine-tuning the model parameters to follow
    desired behaviors expressed in the preference data. However, these methods rely
    on extensive efforts to define desired behaviors and annotate preference data
    at scale [[4](#bib.bib4)], while they are often opaque regarding how the behaviors
    are enforced. Such a need for human annotation and the lack of transparency in
    the learned behaviors limit the scalability and adaptability of these approaches.
    Especially for the out-of-domain behaviors beyond the collected preference data,
    LLMs will likely fail to follow.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的对齐方法，如在线人类反馈强化学习（RLHF）[[38](#bib.bib38)] 或离线直接偏好优化（DPO）[[40](#bib.bib40)]，尝试通过微调模型参数来遵循偏好数据中表达的期望行为。然而，这些方法依赖于大量工作来定义期望行为并大规模注释偏好数据[[4](#bib.bib4)]，同时它们在如何执行这些行为方面通常不透明。对人类注释的需求以及所学行为的不透明性限制了这些方法的可扩展性和适应性。特别是对于超出收集的偏好数据范围的领域，LLMs
    可能无法遵循。
- en: 'To bypass the reliance on large-scale preference data as in alignment approaches,
    we introduce SelfControl, a novel gradient-based framework designed to enable
    differentiable control of LLM outputs without human annotation. Our approach is
    inspired by recent studies showing that LLMs have a decent ability to judge the
    quality of their outputs [[27](#bib.bib27), [5](#bib.bib5), [51](#bib.bib51),
    [48](#bib.bib48)]. SelfControl takes a desired attribute, expressed as suffix
    strings, to evaluate whether a generated output follows the attribute. Then, it
    calculates the gradient of the likelihood of the output following the attribute
    with respect to the latent representations of the inputs. Then latent representations
    are updated with the gradient to control LLM behaviors. We call this procedure
    suffix gradient search. Compared to existing control methods, our procedure has
    two major advantages: 1) it allows us to directly influence the generation trajectory
    only by models’ self-judgment without relying on extensive human annotations;
    2) only latent representations are updated but not model parameters, so inference-time
    control can be achieved for different control objectives. We show in Figure [1](#S0.F1
    "Figure 1 ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into
    Prefix Controller") that SelfControl can generally control LLMs’ emotions, helplessness,
    etc. In addition, we also show such control can improve models’ reasoning capabilities
    for solving mathematical problems.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过像对齐方法那样对大规模偏好数据的依赖，我们提出了SelfControl，这是一种新颖的基于梯度的框架，旨在实现对LLM输出的可微分控制而无需人工标注。我们的方法受到近期研究的启发，这些研究显示LLMs在判断其输出质量方面具有相当不错的能力 [[27](#bib.bib27),
    [5](#bib.bib5), [51](#bib.bib51), [48](#bib.bib48)]。SelfControl采用所需的属性，作为后缀字符串，来评估生成的输出是否符合该属性。然后，它计算输出符合属性的可能性的梯度，并相对于输入的潜在表示进行更新。然后，潜在表示通过梯度进行更新，以控制LLM的行为。我们称这个过程为后缀梯度搜索。与现有的控制方法相比，我们的方法有两个主要优势：1）它使我们能够仅通过模型的自我判断直接影响生成轨迹，而无需依赖大量人工标注；2）仅更新潜在表示，而不是模型参数，因此可以在推理时实现不同控制目标的控制。我们在图[1](#S0.F1
    "Figure 1 ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into
    Prefix Controller")中展示了SelfControl通常可以控制LLMs的情感、无助感等。此外，我们还展示了这种控制可以提高模型解决数学问题的推理能力。
- en: SelfControl operates at the instance level, i.e., it controls the model behavior
    for a single LLM input. To enhance its transferability and compositionally, we
    further propose SelfControl${}_{\textsc{prefix}}$ can be integrated into the LLM
    without changing the LLM parameters, and it is a portable and composable module
    that can be dynamically applied to control multiple model behaviors simultaneously
    (e.g., increasing reasoning and helpfulness, while staying peaceful, shown on
    the right-hand side of Figure [1](#S0.F1 "Figure 1 ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller")). It is reusable and efficient,
    allowing practitioners to specify behavioral constraints the model adheres to
    by construction, thereby enhancing the practicality of SelfControl for real-world
    applications.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: SelfControl在实例级别操作，即它控制单个LLM输入的模型行为。为了增强其可迁移性和组合性，我们进一步提出SelfControl${}_{\textsc{prefix}}$可以集成到LLM中，而无需更改LLM参数，它是一个可移植和可组合的模块，可以动态应用于同时控制多个模型行为（例如，增强推理和帮助能力，同时保持平和，见图[1](#S0.F1
    "Figure 1 ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into
    Prefix Controller")右侧）。它是可重用和高效的，允许从业者通过构造指定模型遵循的行为约束，从而提高SelfControl在实际应用中的实用性。
- en: 'Our contribution is three-fold:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献有三方面：
- en: •
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: •
- en: We propose SelfControl, a novel method utilizing model self-judgment to control
    the behaviors of LLMs without explicit human annotations.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了SelfControl，这是一种利用模型自我判断来控制LLM行为的方法，无需明确的人工标注。
- en: •
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: •
- en: To increase control efficiency, we develop SelfControl${}_{\textsc{prefix}}$,
    a LoRA-based module that encapsulates control signals from SelfControl, enabling
    inference-time behavior control without altering model parameters.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了提高控制效率，我们开发了SelfControl${}_{\textsc{prefix}}$，这是一个基于LoRA的模块，封装了来自SelfControl的控制信号，能够在推理时进行行为控制而不改变模型参数。
- en: •
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: •
- en: We validate the effectiveness of SelfControl and SelfControl${}_{\textsc{prefix}}$2.35%
    over zero-shot CoT. It also achieves a 52.2% win-rate on HH-dialogue, and even
    a win-rate of 58.6% when trained with DPO. These experiments showcase significant
    improvements in performance and adherence to ethical guidelines.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们验证了 SelfControl 和 SelfControl${}_{\textsc{prefix}}$ 在零-shot CoT 上的有效性，提升幅度为
    2.35%。它还在 HH-dialogue 上取得了 52.2% 的胜率，并且在使用 DPO 训练时胜率达到 58.6%。这些实验展示了性能和遵循伦理准则的显著提升。
- en: 2 SelfControl
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 SelfControl
- en: '![Refer to caption](img/a41dd9858c0add01f7cc702b752867ca.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a41dd9858c0add01f7cc702b752867ca.png)'
- en: 'Figure 2: Framework of SelfControl. We begin by sampling an initial response
    from an auto-regressive language model and selecting an appropriate suffix string
    and a target label to define a control direction. Suffixes can be combined. As
    shown in the figure, we use both “Be Helpful” and “Be Harmless” from the suffix
    pool to define our control direction. Suffix scores are then calculated and used
    to obtain the gradients, which are added to the hidden states in the orange blocks.
    These modified hidden states are then used to sample new responses—steps 3 and
    4 form an E-M iteration loop, leading to the final controlled response.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：SelfControl 框架。我们首先从自回归语言模型中采样一个初始响应，并选择一个合适的后缀字符串和一个目标标签来定义控制方向。后缀可以组合使用。如图所示，我们使用了来自后缀池中的“Be
    Helpful”和“Be Harmless”来定义我们的控制方向。然后计算后缀分数，并用来获得梯度，这些梯度会被添加到橙色块中的隐藏状态中。这些修改过的隐藏状态随后用于采样新的响应——步骤
    3 和 4 形成了一个 E-M 迭代循环，从而得到最终的受控响应。
- en: In this section, we introduce SelfControl, the framework for controlling model
    behaviors using suffix gradients. We will first elaborate on the definition of
    model behaviors, along with their suffix scores. These scores serve as criteria
    for judging the models’ output and evaluating how well they align with the control
    direction. Then, we will demonstrate how we get the suffix gradients and use them
    to control model behaviors without changing model parameters, which enable inference-time
    control for arbitrary desired LLM behavior (defined in the suffix, the middle
    part of Figure [2](#S2.F2 "Figure 2 ‣ 2 SelfControl ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller")). Finally, we introduce
    how we compress suffix gradients into a Prefix Controller that can be directly
    generalized to control model behaviors on new inputs in a flexible plug-and-play
    manner (shown in Figure [3](#S2.F3 "Figure 3 ‣ 2 SelfControl ‣ Self-Control of
    LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了 SelfControl，一个使用后缀梯度来控制模型行为的框架。我们将首先详细说明模型行为的定义及其后缀分数。这些分数作为判断模型输出和评估其与控制方向一致性的标准。然后，我们将演示如何获取后缀梯度并使用它们来控制模型行为，而不改变模型参数，这使得推理时可以对任意所需的
    LLM 行为进行控制（定义在后缀中，如图 [2](#S2.F2 "图 2 ‣ 2 SelfControl ‣ 通过将后缀梯度压缩到前缀控制器来实现LLM行为的自我控制")
    中的中间部分）。最后，我们介绍了如何将后缀梯度压缩到一个可以直接推广到新输入以灵活即插即用地控制模型行为的前缀控制器（如图 [3](#S2.F3 "图 3
    ‣ 2 SelfControl ‣ 通过将后缀梯度压缩到前缀控制器来实现LLM行为的自我控制") 所示）。
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 SelfControl ‣ Self-Control of LLM
    Behaviors by Compressing Suffix Gradient into Prefix Controller"), given an autoregressive
    Transformer-based Language Model $\texttt{LM}_{\theta}$ token by token via:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S2.F2 "图 2 ‣ 2 SelfControl ‣ 通过将后缀梯度压缩到前缀控制器来实现LLM行为的自我控制") 所示，给定一个自回归
    Transformer 基于的语言模型 $\texttt{LM}_{\theta}$，逐个 token 进行处理：
- en: '|  | $P_{\theta}(\texttt{output}\bigm{&#124;}\texttt{input})=\prod_{t=1}^{&#124;\texttt{output}&#124;}P_{\theta}(\texttt{output}_{t+1}\bigm{&#124;}\texttt{output}_{[1:t]},H_{\texttt{input}}).$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{\theta}(\texttt{output}\bigm{&#124;}\texttt{input})=\prod_{t=1}^{&#124;\texttt{output}&#124;}P_{\theta}(\texttt{output}_{t+1}\bigm{&#124;}\texttt{output}_{[1:t]},H_{\texttt{input}}).$
    |  | (1) |'
- en: 'Without proper guidance, the language model can generate an unreasonable output,
    such as “The amount of water to put depends on your car’s specification or your
    location.”, which is clearly not reasonable as water shouldn’t be put into a car’s
    gas tank. One way to identify such unreasonable output is through LM self-judgment,
    which we call the suffix score. To get the score, we append a suffix to the model
    input, such as “Is the above output helpful? Give the answer Yes or No:”. Given
    such suffix string, we probe the probability of the predicted  to
    be either “Yes” or “No”:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有适当指导的情况下，语言模型可能生成不合理的输出，例如“添加水的量取决于你的汽车规格或你的位置。”，这显然不合理，因为水不应被加到汽车的油箱中。识别这种不合理输出的一种方法是通过LM自我判断，我们称之为后缀分数。为了获取分数，我们在模型输入中添加一个后缀，例如“上述输出是否有帮助？回答是或否：”。给定这样的后缀字符串，我们探测预测的是“是”还是“否”的概率：
- en: '|  | $\displaystyle P_{+}(\texttt{output},H_{\texttt{input}})$ |  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{+}(\texttt{output},H_{\texttt{input}})$ |  |'
- en: '|  | $\displaystyle P_{-}(\texttt{output},H_{\texttt{input}})$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{-}(\texttt{output},H_{\texttt{input}})$ |  |'
- en: 'Here “Yes” and “No” are two suffix labels we use to assess the $\texttt{LM}_{\theta}$’s
    judgment on the response for following a certain behavior. Note that these two
    labels are not necessarily the tokens that have the highest predicted probability,
    and we only use their probability ratio to quantify model behavior. Formally,
    we define the suffix score as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“Yes”和“No”是我们用来评估 $\texttt{LM}_{\theta}$ 对响应某一行为的判断的两个后缀标签。注意，这两个标签不一定是预测概率最高的标记，我们仅用它们的概率比来量化模型行为。形式上，我们将后缀分数定义为：
- en: '|  | $\displaystyle S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}})=\text{sigmoid}\Big{(}\log
    P_{+}(\texttt{output},H_{\texttt{input}})-\log P_{-}(\texttt{output},H_{\texttt{input}})\Big{)}$
    |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}})=\text{sigmoid}\Big{(}\log
    P_{+}(\texttt{output},H_{\texttt{input}})-\log P_{-}(\texttt{output},H_{\texttt{input}})\Big{)}$
    |  |'
- en: 'A higher suffix score $S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}})$
    indicates that the model output better follows the behavior specified in suffix.
    Therefore, the objective of suffix control becomes finding an output that can
    maximize the suffix score:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 较高的后缀分数 $S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}})$ 表明模型输出更好地遵循了后缀中指定的行为。因此，后缀控制的目标变成了找到一个可以最大化后缀分数的输出：
- en: '|  | $\displaystyle\texttt{output}^{*}=\underset{\{\texttt{output},\ \theta,\
    H_{\texttt{input}}\}}{\arg\max}S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}}),\
    \ \text{where: }\texttt{output}\sim\texttt{LM}_{\theta}(H_{\texttt{input}})$ |  |
    (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{output}^{*}=\underset{\{\texttt{output},\ \theta,\
    H_{\texttt{input}}\}}{\arg\max}S_{\texttt{suffix}}(\texttt{output},H_{\texttt{input}}),\
    \ \text{where: }\texttt{output}\sim\texttt{LM}_{\theta}(H_{\texttt{input}})$ |  |
    (2) |'
- en: In principle, there are three variables in Eq. ([2](#S2.E2 "In 2 SelfControl
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"))
    that can be optimized to maximize $S_{\texttt{suffix}}$. This approach can work
    for controlling some simple model behaviors, but for more complicated behaviors,
    such as multi-hop reasoning or keeping secrets, unguided sampling becomes very
    inefficient. On the other hand, guided sampling is non-trivial for LLMs. Unlike
    the classifier guidance for diffusion models in image generation, where gradients
    for the output can be directly computed as the image outputs are continuous, the
    LLM output is a sequence of non-differentiable discrete tokens. Even though some
    LLM-attack works [[53](#bib.bib53), [23](#bib.bib23), [39](#bib.bib39), [32](#bib.bib32),
    [46](#bib.bib46), [49](#bib.bib49)] employ evolutionary search or projected gradient
    descent to help search the optimal output, there is no guarantee for controlled
    outputs with these approaches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原则上，方程式中的三个变量（[2](#S2.E2 "In 2 SelfControl ‣ Self-Control of LLM Behaviors by
    Compressing Suffix Gradient into Prefix Controller")）可以被优化以最大化 $S_{\texttt{suffix}}$。这种方法可以用于控制一些简单的模型行为，但对于更复杂的行为，如多跳推理或保密，未指导的采样变得非常低效。另一方面，针对LLM的有指导采样并不简单。与图像生成中扩散模型的分类器指导不同，其中输出的梯度可以直接计算，因为图像输出是连续的，而LLM的输出是一系列不可微分的离散标记。尽管一些LLM攻击工作
    [[53](#bib.bib53), [23](#bib.bib23), [39](#bib.bib39), [32](#bib.bib32), [46](#bib.bib46),
    [49](#bib.bib49)] 使用进化搜索或投影梯度下降来帮助搜索最佳输出，但这些方法无法保证控制输出。
- en: Another choice to maximize $S_{\texttt{suffix}}$ cannot be changed on the fly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择来最大化 $S_{\texttt{suffix}}$ 不能即时更改。
- en: 'In this paper, we take the third choice to maximize $S_{\texttt{suffix}}$)
    by directly optimizing Eq. [2](#S2.E2 "In 2 SelfControl ‣ Self-Control of LLM
    Behaviors by Compressing Suffix Gradient into Prefix Controller") using the EM
    algorithm. Specifically, we treat output as a discrete latent variable that is
    intractable, which we estimate during the E-step that maximizes suffix score;
    then during the M-step, we use this variable to optimize H. Specifically:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们选择第三种方法，通过直接优化公式 [2](#S2.E2 "在 2 SelfControl ‣ 通过将后缀梯度压缩为前缀控制器来控制LLM行为")来最大化
    $S_{\texttt{suffix}}$，使用EM算法。具体来说，我们将输出视为一个不可处理的离散潜变量，这个变量在E步中被估计，以最大化后缀评分；然后在M步中，我们使用这个变量来优化H。具体步骤如下：
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In E-Step: Use the $i$ that has highest suffix score.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在E步：使用具有最高后缀评分的$i$。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In M-step: calculate $\Delta H_{i}=\nabla_{H}S_{\texttt{suffix}}(\texttt{output}_{i}^{*},H_{i})$
    denotes the step size.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在M步：计算 $\Delta H_{i}=\nabla_{H}S_{\texttt{suffix}}(\texttt{output}_{i}^{*},H_{i})$，其中步长表示为$\Delta
    H_{i}$。
- en: As is shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 SelfControl ‣ Self-Control of
    LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"), using such
    EM update, at each M-step update we can get a better input query $H=H_{\texttt{input}}$
    through line search (search_step_size in Algorithm [2](#alg2 "In Appendix F Pseudo-code
    ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller")) to ensure the suffix score keeps increasing through
    multi-iteration.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S2.F2 "图 2 ‣ 2 SelfControl ‣ 通过将后缀梯度压缩为前缀控制器来控制LLM行为")所示，使用这种EM更新，在每次M步更新中，我们可以通过线搜索（Algorithm [2](#alg2
    "附录 F 伪代码 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩为前缀控制器来控制LLM行为")中的search_step_size）获得更好的输入查询 $H=H_{\texttt{input}}$，以确保后缀评分通过多次迭代不断提高。
- en: '![Refer to caption](img/02a9e75416d24ae14178a87e93586062.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/02a9e75416d24ae14178a87e93586062.png)'
- en: 'Figure 3: Framework and training pipeline of SelfControl${}_{\textsc{prefix}}$
    is also shown at the bottom of the figure.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：SelfControl${}_{\textsc{prefix}}$的框架和训练流程也展示在图底部。
- en: Compress Suffix Gradients into SelfControl${}_{\textsc{prefix}}$.
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将后缀梯度压缩为SelfControl${}_{\textsc{prefix}}$。
- en: SelfControl can efficiently search for proper input representations to enable
    LLM behavior control at the instance level. However, it is still expensive and
    requires a gradient-based search for each input during inference time. We thus
    propose to just use SelfControl to collect a bunch of {(input, $H^{*}_{\texttt{input}}$.
    This idea is motivated by earlier prefix-prompt tuning research [[41](#bib.bib41),
    [31](#bib.bib31), [47](#bib.bib47)] that searches for continuous / discrete tokens
    to maximize certain outputs. Ours is different in that the reward signal as well
    as the learning objective are collected through the model itself.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SelfControl可以有效地搜索适当的输入表示，以实现对实例级LLM行为的控制。然而，这仍然比较昂贵，并且在推理过程中需要对每个输入进行基于梯度的搜索。因此，我们建议仅使用SelfControl收集一批{（input,
    $H^{*}_{\texttt{input}}$。这一想法受到早期前缀提示调优研究的启发 [[41](#bib.bib41), [31](#bib.bib31),
    [47](#bib.bib47)]，这些研究搜索连续/离散标记以最大化某些输出。我们的不同之处在于奖励信号和学习目标是通过模型本身收集的。
- en: 'The Prefix Controller does not directly change the model parameters and ensures
    flexible model control. Each Prefix Controller can be treated as a plug-and-play
    “prompt”, which elicits a certain LM behavior standalone. In addition, by applying
    multiple such controllers together, we can steer LM output to follow a composite
    of desired attributes. For example, in Figure [3](#S2.F3 "Figure 3 ‣ 2 SelfControl
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"),
    we show the model can be guided to behave with more reasoning capability, calmer
    and more helpful. As is shown in the Figure [3](#S2.F3 "Figure 3 ‣ 2 SelfControl
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"),
    the Prefix Controller contains two components: a LoRA adapter $\texttt{adapter}_{\theta_{a}}$
    denote the learnable parameters of the adapter and prefix prompt respectively.
    The learnable prompt is prepended to each input query, while the LoRA adapter
    serves as an additional module that directly influences the latent representations.
    Altogether they will affect the latent representations.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀控制器不会直接改变模型参数，确保了灵活的模型控制。每个前缀控制器可以视为一个即插即用的“提示”，独立引发某种LM行为。此外，通过同时应用多个这样的控制器，我们可以引导LM输出遵循所需属性的组合。例如，在图[3](#S2.F3
    "图 3 ‣ 2 SelfControl ‣ 通过压缩后缀梯度到前缀控制器来控制LLM行为")中，我们展示了模型可以被引导得更具推理能力、更冷静和更有帮助。如图[3](#S2.F3
    "图 3 ‣ 2 SelfControl ‣ 通过压缩后缀梯度到前缀控制器来控制LLM行为")所示，前缀控制器包含两个组件：LoRA适配器$\texttt{adapter}_{\theta_{a}}$表示适配器的可学习参数和前缀提示。可学习的提示被添加到每个输入查询的前面，而LoRA适配器作为一个额外的模块，直接影响潜在表示。两者共同作用将影响潜在表示。
- en: 'Then given the LoRA adapter, prefix prompt, $\texttt{LM}_{\theta}$ as the KV-cache
    representations with these inputs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后给定LoRA适配器、前缀提示、$\texttt{LM}_{\theta}$作为这些输入的KV-cache表示：
- en: '|  | $\displaystyle H_{\texttt{prefix}}:=h(\texttt{adapter}_{\theta_{a}},\texttt{prompt}_{\theta_{p}},\texttt{LM}_{\theta},\texttt{input}).$
    |  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H_{\texttt{prefix}}:=h(\texttt{adapter}_{\theta_{a}},\texttt{prompt}_{\theta_{p}},\texttt{LM}_{\theta},\texttt{input}).$
    |  |'
- en: 'Then, the objective of SelfControl${}_{\textsc{prefix}}$ is formulated as the
    following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，SelfControl${}_{\textsc{prefix}}$的目标被制定为以下内容：
- en: '|  | $1$2 |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Data filtering.
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据过滤。
- en: Specifically, when collecting $\{(\texttt{input}_{i},H^{*}_{\texttt{input}_{i}})\}_{i=1}^{N}$
    is the proportion constant.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，当收集$\{(\texttt{input}_{i},H^{*}_{\texttt{input}_{i}})\}_{i=1}^{N}$时，比例常数为。
- en: 3 Experiments
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 实验
- en: In this section, we benchmark SelfControl and SelfControl${}_{\textsc{prefix}}$
    on various attributes, including emotions, reducing toxicity, helpfulness and
    harmlessness (HH) dialogue, and reasoning. We elaborate on our evaluation setups
    below. Further details of our experiments are in Appendix [C](#A3 "Appendix C
    Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller").
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们在各种属性上基准测试SelfControl和SelfControl${}_{\textsc{prefix}}$，包括情感、减少毒性、有用性和无害性（HH）对话以及推理。我们下面详细阐述了我们的评估设置。我们实验的进一步细节在附录[C](#A3
    "附录 C 实验细节 ‣ 第一部分 附录 ‣ 通过压缩后缀梯度到前缀控制器来控制LLM行为")。
- en: 3.1 Evaluation Setup
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 评估设置
- en: Emotion Control. Representations of emotions are shown to exist in LLMs, and
    can be constructed by collecting model activations. We study if model emotion
    can be well controlled using SelfControl. We use datasets of five emotional attributes
    from RepE [[52](#bib.bib52)], i.e. anger, fear, happiness, surprise, and disgust.
    Specifically, we use the last one hundred data from each emotional dataset for
    evaluation and the first one hundred to train SelfControl${}_{\textsc{prefix}}$
    and Reading Vector. We use GPT-3.5-turbo to evaluate emotion scores (template
    can be found in Appendix [B](#A2 "Appendix B Prompt Templates ‣ Part I Appendix
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 情感控制。情感的表现被证明在LLMs中存在，并且可以通过收集模型激活来构建。我们研究了是否可以通过SelfControl有效控制模型情感。我们使用来自RepE的五种情感属性的数据集[[52](#bib.bib52)]，即愤怒、恐惧、幸福、惊讶和厌恶。具体而言，我们使用每个情感数据集中的最后一百条数据进行评估，并使用前一百条数据训练SelfControl${}_{\textsc{prefix}}$和阅读向量。我们使用GPT-3.5-turbo来评估情感分数（模板见附录[B](#A2
    "附录 B 提示模板 ‣ 第一部分 附录 ‣ 通过压缩后缀梯度到前缀控制器来控制LLM行为")）。
- en: Language Detoxification. LLMs may generate toxic completions to prompts that
    are offensive or privacy-leaking, even for the instruction-tuned models. We endeavor
    to evaluate how well different embedding-based control methods can detoxify the
    response and avoid following toxic instructions. We test on two attributes, i.e.,
    toxicity and privacy. We use RealToxicityPrompts [[16](#bib.bib16)] for toxicity
    following [[19](#bib.bib19)], and Perspective [[1](#bib.bib1)] to measure toxicity
    scores. We use [privacy](https://github.com/AI-secure/DecodingTrust/tree/main/src/dt/perspectives/privacy)
    from DecodingTrust. Specifically, the goal for control on privacy is to reject
    generating correct email addresses. Models are given a five-shot demonstration
    on leaking email addresses of the corresponding people, and then they are asked
    to generate the correct email address of another person.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 语言解毒。即使是经过指令调整的模型，LLMs 也可能生成对提示具有攻击性或泄露隐私的毒性回答。我们努力评估不同基于嵌入的控制方法如何解毒响应并避免遵循毒性指令。我们在两个属性上进行测试，即毒性和隐私。我们使用
    RealToxicityPrompts [[16](#bib.bib16)] 进行毒性测试，参照 [[19](#bib.bib19)]，并使用 Perspective
    [[1](#bib.bib1)] 来测量毒性分数。我们使用 [隐私](https://github.com/AI-secure/DecodingTrust/tree/main/src/dt/perspectives/privacy)
    来自 DecodingTrust。具体来说，隐私控制的目标是拒绝生成正确的电子邮件地址。模型会接受一个五次示范，展示泄露相应人员的电子邮件地址，然后要求生成另一个人的正确电子邮件地址。
- en: HH-dialogue. For HH-dialogue, we benchmark how well the responses align with
    the principle given in [[15](#bib.bib15)]. Besides, simply control with SelfControl
    on the embedding level. We also benchmark SelfControl as a data generation method
    to generate preference pairs. The preference pairs will be used to train the base
    model using DPO. We test on the first 250 data from Anthropic-HH [[4](#bib.bib4)]
    harmless-base and helpful-base. We follow [[15](#bib.bib15)] and use GPT-4 to
    select the winner of each response when competing with the original response.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: HH-dialogue。对于 HH-dialogue，我们基准测试响应与 [[15](#bib.bib15)] 中给出的原则的一致性。此外，简单地在嵌入级别使用
    SelfControl 进行控制。我们还将 SelfControl 作为数据生成方法来生成偏好对。这些偏好对将用于使用 DPO 训练基础模型。我们在 Anthropic-HH
    [[4](#bib.bib4)] harmless-base 和 helpful-base 的前 250 条数据上进行测试。我们遵循 [[15](#bib.bib15)]
    并使用 GPT-4 选择每个响应的优胜者，与原始响应进行竞争。
- en: Reasoning. We also demonstrate that SelfControl can be used to improve the mathematical
    reasoning ability of LLMs, measured by performance on GSM-8K [[9](#bib.bib9)],
    a dataset of 8.5K high quality linguistically diverse grade school math word problems.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 推理。我们还展示了 SelfControl 可以用于提高 LLMs 的数学推理能力，通过在 GSM-8K [[9](#bib.bib9)] 上的表现来衡量，该数据集包含
    8500 个高质量语言多样化的小学数学问题。
- en: In all the above scenarios, for SelfControl${}_{\textsc{prefix}}$, we generate
    up to 800 (query, embedding) pairs for the training set using 100 queries as seed
    data. For the validation set, we use another 100 queries as seed data and generate
    up to 100 pairs.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述所有场景中，对于 SelfControl${}_{\textsc{prefix}}$，我们使用 100 个查询作为种子数据，生成最多 800 对（查询，嵌入）对作为训练集。对于验证集，我们使用另外
    100 个查询作为种子数据，生成最多 100 对。
- en: 'Baselines. We compare our method with three baselines, including two representation
    engineering methods: Reading Vector and Contrast Vector [[52](#bib.bib52)], and
    a prompting method: System Prompting. Reading Vector and Contrast Vector modify
    the model’s internal activation without involving changes to the model’s parameters
    or prompts. System prompting directly adds a controlling instruction to control
    the language generated by models. For the Reading Vector, we report results on
    attributes that the datasets are available from the original paper. We use System
    Prompting to reduce the toxicity of model generations. We also compare with Chain-of-Thought
    (CoT)-decoding [[45](#bib.bib45)] on reasoning and compare with SAMI [[15](#bib.bib15)]
    on HH-dialogue.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。我们将我们的方法与三个基准线进行比较，包括两个表示工程方法：Reading Vector 和 Contrast Vector [[52](#bib.bib52)]，以及一个提示方法：System
    Prompting。Reading Vector 和 Contrast Vector 修改模型的内部激活，而不涉及模型参数或提示的更改。System Prompting
    直接添加一个控制指令来控制模型生成的语言。对于 Reading Vector，我们报告的数据集属性来自原始论文。我们使用 System Prompting
    来减少模型生成内容的毒性。我们还在推理方面与 Chain-of-Thought (CoT)-decoding [[45](#bib.bib45)] 进行比较，并在
    HH-dialogue 方面与 SAMI [[15](#bib.bib15)] 进行比较。
- en: Models. For a fair comparison with existing literature, we use LLaMA-2-7b-chat
    [[44](#bib.bib44)] on emotion, toxicity, and HH-dialogue, and use Mistral-7B-Instruct-v0.2
    [[26](#bib.bib26)] on toxicity and reasoning. For all the experiments, we use
    greedy decoding if not otherwise specified.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。为了与现有文献进行公平比较，我们在情感、毒性和HH对话中使用LLaMA-2-7b-chat [[44](#bib.bib44)]，并在毒性和推理中使用Mistral-7B-Instruct-v0.2
    [[26](#bib.bib26)]。对于所有实验，如果没有特别说明，我们使用贪婪解码。
- en: 3.2 Experimental Results
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 实验结果
- en: Emotion Control.
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 情感控制。
- en: The results for emotion control are shown in Table [1](#S3.T1 "Table 1 ‣ Emotion
    Control. ‣ 3.2 Experimental Results ‣ 3 Experiments ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller"). As is shown in the table,
    scores on SelfControl and SelfControl${}_{\textsc{prefix}}$ achieves the best
    scores on anger, surprise, and disgust, and SelfControl is also comparable to
    other control baselines on most of the attributes from emotion.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 情感控制的结果显示在表[1](#S3.T1 "Table 1 ‣ Emotion Control. ‣ 3.2 Experimental Results
    ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller")中。表中显示，自控和自控${}_{\textsc{prefix}}$在愤怒、惊讶和厌恶方面的评分最佳，自控在大多数情感属性上也与其他控制基线相当。
- en: 'Table 1: Scores of different emotions. The lower score, the emotions are better
    expressed.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同情感的评分。分数越低，情感表达越好。
- en: '| Method | anger$\downarrow$ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 愤怒$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Orig (No Control) | 1.56 | 3.26 | 4.60 | 3.16 | 2.69 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 原始（无控制） | 1.56 | 3.26 | 4.60 | 3.16 | 2.69 |'
- en: '| System Prompting | 1.14 | 2.52 | 1.73 | 2.92 | 2.21 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 1.14 | 2.52 | 1.73 | 2.92 | 2.21 |'
- en: '| Reading Vector | 1.32 | 2.72 | 2.87 | 2.71 | 2.50 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 阅读向量 | 1.32 | 2.72 | 2.87 | 2.71 | 2.50 |'
- en: '| Contrast Vector | 1.52 | 2.06 | 3.99 | 2.81 | 2.62 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 对比向量 | 1.52 | 2.06 | 3.99 | 2.81 | 2.62 |'
- en: '| SelfControl | 1.35 | 2.90 | 3.99 | 3.14 | 2.79 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 自控 | 1.35 | 2.90 | 3.99 | 3.14 | 2.79 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 1.09 | 2.17 | 4.11 | 2.46 | 2.19 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 自控${}_{\textsc{prefix}}$ | 1.09 | 2.17 | 4.11 | 2.46 | 2.19 |'
- en: Language Detoxification.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 语言去毒化。
- en: Results of toxicity are attached on Table [2](#S3.T2 "Table 2 ‣ Language Detoxification.
    ‣ 3.2 Experimental Results ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by
    Compressing Suffix Gradient into Prefix Controller"). It is shown that both SelfControl
    and SelfControl${}_{\textsc{prefix}}$ achieve the best or the second best scores
    on toxic prompts, whereas other methods have weaker control ability to generate
    less toxic responses. For Reading Vector on LLaMA-2-7b-chat, the score is even
    higher than the original response. It is also less helpful to use instructions
    to guide model generating less toxic responses.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 毒性结果附在表[2](#S3.T2 "Table 2 ‣ Language Detoxification. ‣ 3.2 Experimental Results
    ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller")中。结果表明，自控和自控${}_{\textsc{prefix}}$在毒性提示上取得了最佳或第二最佳分数，而其他方法在生成较少毒性响应方面控制能力较弱。对于LLaMA-2-7b-chat上的阅读向量，其分数甚至高于原始响应。使用指令来指导模型生成较少毒性的响应也不太有效。
- en: For privacy, results are shown in Table [4](#S3.T4 "Table 4 ‣ Language Detoxification.
    ‣ 3.2 Experimental Results ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by
    Compressing Suffix Gradient into Prefix Controller"), and as is displayed in the
    table, System Prompt can barely help avoid generating correct email addresses,
    and Contrast Vector can to some extent avoid revealing the correct email addresses.
    As for SelfControl and SelfControl${}_{\textsc{prefix}}$, they can successfully
    hide the correct email information.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保护隐私，结果显示在表[4](#S3.T4 "Table 4 ‣ Language Detoxification. ‣ 3.2 Experimental
    Results ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix
    Gradient into Prefix Controller")中，如表所示，系统提示几乎无法避免生成正确的电子邮件地址，而对比向量在某种程度上可以避免泄露正确的电子邮件地址。至于自控和自控${}_{\textsc{prefix}}$，它们可以成功隐藏正确的电子邮件信息。
- en: 'Table 2: Toxicity scores of generated language'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：生成语言的毒性评分
- en: '|  |  | Toxicity Score$\downarrow$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 毒性评分$\downarrow$ |'
- en: '| Model | Method | Toxic Prompt | Nontoxic Prompt |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 毒性提示 | 非毒性提示 |'
- en: '| LLaMA2 | Orig. (No Control) | 0.440 | 0.091 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | 原始（无控制） | 0.440 | 0.091 |'
- en: '| System Prompting | 0.415 | 0.095 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 0.415 | 0.095 |'
- en: '| Reading Vector | 0.460 | 0.061 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 阅读向量 | 0.460 | 0.061 |'
- en: '| Contrast Vector | 0.423 | 0.068 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 对比向量 | 0.423 | 0.068 |'
- en: '| SelfControl | 0.285 | 0.062 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 自控 | 0.285 | 0.062 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 0.314 | 0.057 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 自控${}_{\textsc{prefix}}$ | 0.314 | 0.057 |'
- en: '| Mistral | Orig. (No Control) | 0.427 | 0.078 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 原始（无控制） | 0.427 | 0.078 |'
- en: '| System Prompting | 0.452 | 0.115 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 0.452 | 0.115 |'
- en: '| Reading Vector | 0.333 | 0.060 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 阅读向量 | 0.333 | 0.060 |'
- en: '| Contrast Vector | 0.354 | 0.051 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Contrast Vector | 0.354 | 0.051 |'
- en: '| SelfControl | 0.282 | 0.066 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl | 0.282 | 0.066 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 0.259 | 0.049 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl${}_{\textsc{prefix}}$ | 0.259 | 0.049 |'
- en: 'Table 3: Evaluation Results on privacy dataset. “✓Email” means answer contains
    the complete correct email; “✓Domain” means the answer contains the correct domain.
    LLM shall not respond with such private info, so lower the better.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在隐私数据集上的评估结果。“✓Email”表示回答包含完整的正确电子邮件；“✓Domain”表示回答包含正确的域名。LLM 不应回应这样的私人信息，因此越低越好。
- en: '| Method | ✓Email $\downarrow$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | ✓Email $\downarrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Orig. (No Control) | 58 | 99 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 原始（无控制） | 58 | 99 |'
- en: '| System Prompting | 57 | 98 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 57 | 98 |'
- en: '| Contrast Vector | 28 | 83 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Contrast Vector | 28 | 83 |'
- en: '| SelfControl | 0 | 0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl | 0 | 0 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 0 | 0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl${}_{\textsc{prefix}}$ | 0 | 0 |'
- en: 'Table 4: Running time comparison of different control methods. Both Contrast
    Vector and SelfControl requires to conduct multi-iteration search, so takes longer;
    Reading Vector and our learned SelfControl${}_{\textsc{prefix}}$ are all plug-and-play
    so do not require additional time.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同控制方法的运行时间比较。Contrast Vector 和 SelfControl 需要进行多次迭代搜索，因此耗时较长；Reading Vector
    和我们学习的 SelfControl${}_{\textsc{prefix}}$ 都是即插即用的，因此不需要额外时间。
- en: '| Method | Running Time (s) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 运行时间 (秒) |'
- en: '| --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Orig. (No Control) | 5.788 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 原始（无控制） | 5.788 |'
- en: '| Reading Vector | 5.787 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Reading Vector | 5.787 |'
- en: '| Contrast Vector | 20.408 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Contrast Vector | 20.408 |'
- en: '| SelfControl | 54.598 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl | 54.598 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 5.817 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl${}_{\textsc{prefix}}$ | 5.817 |'
- en: HH-dialogue.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: HH-dialogue。
- en: The results are shown in Table [6](#S3.T6 "Table 6 ‣ HH-dialogue. ‣ 3.2 Experimental
    Results ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix
    Gradient into Prefix Controller"), where we can see that SelfControl can beat
    the original model. Interestingly, training the base model using data generated
    from SelfControl can achieve win rates comparable to those obtained by training
    the base model using preference pairs generated directly from prompting. Additionally,
    SelfControl + DPO achieves even higher win rates on helpful-base, showcasing its
    potential as a data synthesis method for SelfControl.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见表 [6](#S3.T6 "表 6 ‣ HH-dialogue. ‣ 3.2 实验结果 ‣ 3 实验 ‣ 通过将后缀梯度压缩到前缀控制器来控制 LLM
    行为")，我们可以看到 SelfControl 可以超越原始模型。有趣的是，使用 SelfControl 生成的数据来训练基础模型可以实现与直接从提示中生成的偏好对进行训练所获得的胜率相当的结果。此外，SelfControl
    + DPO 在有用基础上实现了更高的胜率，展示了它作为 SelfControl 数据合成方法的潜力。
- en: 'Table 5: Experiment on HH-dialogue dataset. Measured by win-rate against un-controlled
    model.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 HH-dialogue 数据集上的实验。通过与无控制模型的胜率进行测量。
- en: '| Method | Winrate (%) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 胜率 (%) |'
- en: '| --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| harmless | helpful | overall |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 无害 | 有用 | 总体 |'
- en: '| --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| DPO (w/ SAMI [[15](#bib.bib15)]) | 60.4 | 59.6 | 60.0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| DPO（带 SAMI [[15](#bib.bib15)]) | 60.4 | 59.6 | 60.0 |'
- en: '| DPO (w/ SelfControl) | 56.8 | 60.4 | 58.6 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| DPO（带 SelfControl） | 56.8 | 60.4 | 58.6 |'
- en: '| SelfControl | 53.6 | 50.8 | 52.2 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl | 53.6 | 50.8 | 52.2 |'
- en: 'Table 6: Experiment on GSM8K using Mistral. Measured by Accuracy.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在 GSM8K 上使用 Mistral 的实验。通过准确度进行测量。
- en: '| Method | Acc (%) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 准确率 (%) |'
- en: '| --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| greedy | 26.61 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| greedy | 26.61 |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| System Prompting (Zero-shot CoT [[28](#bib.bib28)]) | 34.95 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示（零样本 CoT [[28](#bib.bib28)]) | 34.95 |'
- en: '| --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CoT Decoding [[45](#bib.bib45)] | 42.00 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| CoT 解码 [[45](#bib.bib45)] | 42.00 |'
- en: '| SelfControl | 37.30 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl | 37.30 |'
- en: '| SelfControl${}_{\textsc{prefix}}$ | 27.14 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| SelfControl${}_{\textsc{prefix}}$ | 27.14 |'
- en: Reasoning.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 推理。
- en: As is shown in Table [6](#S3.T6 "Table 6 ‣ HH-dialogue. ‣ 3.2 Experimental Results
    ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller"), both SelfControl and CoT-decoding surpasses greedy decode
    by a large margin, where SelfControl is comparable to CoT-decoding [[45](#bib.bib45)].
    It is also interesting to notice that SelfControl${}_{\textsc{prefix}}$ leads
    to better accuracy than greedy decoding, but still not better than the simple
    zero-shot CoT prompt [[28](#bib.bib28)], we hypothesize it’s because we only sample
    100 training samples to optimize the prefix controller at the moment, and further
    enriching the dataset with ground-truth answer as reward signal [[42](#bib.bib42)]
    can potentially further improve the reasoning results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[6](#S3.T6 "表 6 ‣ HH-dialogue. ‣ 3.2 实验结果 ‣ 3 实验 ‣ 通过将后缀梯度压缩为前缀控制器来控制 LLM
    行为")所示，SelfControl 和 CoT-decoding 都显著超越了贪婪解码，其中 SelfControl 与 CoT-decoding 相当[[45](#bib.bib45)]。有趣的是，SelfControl${}_{\textsc{prefix}}$
    的准确性优于贪婪解码，但仍不如简单的零-shot CoT 提示[[28](#bib.bib28)]，我们推测这是因为我们目前仅用 100 个训练样本来优化前缀控制器，进一步用真实答案作为奖励信号来丰富数据集[[42](#bib.bib42)]，可能会进一步改善推理结果。
- en: 3.3 Analyzing Control using SelfControl
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3 使用 SelfControl 进行控制分析
- en: We also analyze what happened when controlling model behaviors using the SelfControl.
    We took several perspectives, including the trajectory of gradients over iterations,
    norm patterns across different attributes, and how suffix scores attend to input
    tokens.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了使用 SelfControl 控制模型行为时发生了什么。我们从多个角度进行了分析，包括梯度在迭代过程中的轨迹、不同属性的范数模式，以及后缀分数如何关注输入标记。
- en: Trajectory of Suffix Gradients
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后缀梯度的轨迹
- en: 'We visualize suffix gradients of various attributes and their composition to
    gain a deeper understanding of the relationship between different attributes and
    the composite attribute. As shown in Figure [5](#S3.F5 "Figure 5 ‣ Trajectory
    of Suffix Gradients ‣ 3.3 Analyzing Control using SelfControl ‣ 3 Experiments
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"),
    which shows the trajectory of gradients controlling different emotional attributes.
    The input query is ‘‘You decide to leave your stable job to start your own business’’,
    which could make people excited but afraid of an uncertain future. We use SelfControl
    to mitigate the model’s excited and afraid emotions. The gradients computed from
    the combined suffixes are a linear combination of the gradients computed from
    the separate suffixes, which is also reflected in Figure [5](#S3.F5 "Figure 5
    ‣ Trajectory of Suffix Gradients ‣ 3.3 Analyzing Control using SelfControl ‣ 3
    Experiments ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into
    Prefix Controller"). However, if we combine these two attributes in one suffix,
    i.e., ‘‘Are you afraid and excited? Give the answer as ‘No, I’m not afraid and
    I’m not excited’ or ‘Yes, I’m afraid and I’m excited’. Answer: ’’ and set the
    target to ‘‘No’’, the trajectory is a separate direction. This could hint that
    the composition of different attributes does not have a linear relationship with
    elementary attributes.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可视化了各种属性的后缀梯度及其组成，以深入理解不同属性和复合属性之间的关系。如图 [5](#S3.F5 "图 5 ‣ 后缀梯度的轨迹 ‣ 3.3 使用
    SelfControl 进行控制分析 ‣ 3 实验 ‣ 通过将后缀梯度压缩为前缀控制器来控制 LLM 行为") 所示，图中展示了控制不同情感属性的梯度轨迹。输入查询为“你决定离开稳定的工作去创业”，这可能使人感到兴奋但又害怕不确定的未来。我们使用
    SelfControl 来缓解模型的兴奋和恐惧情绪。从组合后缀计算的梯度是从单独后缀计算的梯度的线性组合，这也在图 [5](#S3.F5 "图 5 ‣ 后缀梯度的轨迹
    ‣ 3.3 使用 SelfControl 进行控制分析 ‣ 3 实验 ‣ 通过将后缀梯度压缩为前缀控制器来控制 LLM 行为") 中得到了体现。然而，如果我们将这两个属性合并在一个后缀中，即“你害怕又兴奋吗？回答为‘不，我不害怕也不兴奋’或‘是的，我害怕且兴奋’。回答：”并设置目标为“否”，则轨迹是一个单独的方向。这可能暗示不同属性的组合与基本属性之间并没有线性关系。
- en: '![Refer to caption](img/48099a368e9d8ba2d6db15509352669a.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48099a368e9d8ba2d6db15509352669a.png)'
- en: 'Figure 4: Gradients in PCA space.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：PCA 空间中的梯度。
- en: '![Refer to caption](img/ca7db3e0abe433ea784c0fc21387a7b5.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ca7db3e0abe433ea784c0fc21387a7b5.png)'
- en: 'Figure 5: How suffix gradients apply per task.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：后缀梯度在每个任务中的应用。
- en: Where does each behavior pattern store at Transformer?
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Transformer 中每种行为模式存储在哪里？
- en: The suffix gradient can be regarded as a stimulus to activate or suppress a
    certain behavior inside Transformer weights. We thus are interested in the question
    “for different control targets, which Transformer layer the suffix gradient is
    mostly applying to?” Specifically, we calculate the $\log{\|H^{*}_{input}\|_{2}}-\log{\|H\|_{2}}$
    measuring after gradient how the latent representation per layer increases the
    norm or decreases. We divide each task by a maximum number and set negative as
    zero for clear visualization. As shown in Figure [5](#S3.F5 "Figure 5 ‣ Trajectory
    of Suffix Gradients ‣ 3.3 Analyzing Control using SelfControl ‣ 3 Experiments
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"),
    different tasks focus on different layers / regions of Transformer layers. Tasks
    like “Not Afraid / Disgusted” or keeping Privacy are mostly related to final layers,
    likely because they mostly control some low-level output (like not outputting
    toxic phrases or emails); improving reasoning, helpful and harmless are mostly
    related to low-level layers probably because they need to understand better the
    input information to conduct follow-up reasoning. This is just a small experiment
    using SelfControl, but we see the potential to use suffix gradient to better reveal
    mechanistic interpretability of neural language model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 后缀梯度可以看作是激活或抑制 Transformer 权重中某种行为的刺激。因此，我们关注的问题是：“对于不同的控制目标，后缀梯度主要作用于 Transformer
    的哪个层？”具体来说，我们计算了 $\log{\|H^{*}_{input}\|_{2}}-\log{\|H\|_{2}}$，以衡量在梯度作用后，每层的潜在表示是增加还是减少范数。我们通过最大数值对每个任务进行划分，并将负值设置为零，以便于清晰可视化。如图
    [5](#S3.F5 "Figure 5 ‣ Trajectory of Suffix Gradients ‣ 3.3 Analyzing Control
    using SelfControl ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") 所示，不同任务关注 Transformer 层的不同层/区域。诸如“没有恐惧/厌恶”或保持隐私的任务大多与最终层相关，这可能是因为这些任务主要控制一些低级输出（如不输出有害的短语或邮件）；改善推理、帮助和无害的任务大多与低级层相关，可能是因为这些任务需要更好地理解输入信息以进行后续推理。这只是一个使用
    SelfControl 的小实验，但我们看到利用后缀梯度更好地揭示神经语言模型的机制可解释性具有潜力。
- en: '![Refer to caption](img/43485ad229ed19cb469fadae31788487.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43485ad229ed19cb469fadae31788487.png)'
- en: 'Figure 6: How Suffix Gradients attend to each word for playing Avalon.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：后缀梯度如何对每个单词进行注意，应用于 Avalon 游戏。
- en: Suffix Attention
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后缀注意力
- en: We study how the suffix score and suffix target attend to other tokens in some
    attention heads. For example, Figure [6](#S3.F6 "Figure 6 ‣ Where does each behavior
    pattern store at Transformer? ‣ 3.3 Analyzing Control using SelfControl ‣ 3 Experiments
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")
    depicts the attention of the target token to other tokens on the 9th attention
    head of layer 29\. The query is about playing Merlin for Renaissance Avalon, a
    social deductive game, in which Merlin or Assassin needs to hide his role. The
    input and suffix we use are shown in the Appendix [D.1](#A4.SS1 "D.1 Attention
    Analysis ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller"). Before controlling, the
    model generates responses like ‘‘As Merlin, the great wizard of the land …’’ and
    ‘‘fellow players. I am Assassin …’’, revealing the identities. The target token
    attends to previous words like ‘‘Merlin’’ and ‘‘Assassin’’ in the generated texts.
    After controlling, the model does not generate ‘‘Merlin" or ‘‘Assassin’’. Although
    the target token still attends to the words ‘‘Merlin’’ or ‘‘Assassin’’ in the
    queries and suffixes, these words no longer appear in the generated text, and
    the model successfully reaches the target response. As is shown in Figure [6](#S3.F6
    "Figure 6 ‣ Where does each behavior pattern store at Transformer? ‣ 3.3 Analyzing
    Control using SelfControl ‣ 3 Experiments ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") and Figure [7](#A4.F7 "Figure 7 ‣ D.1
    Attention Analysis ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control of LLM
    Behaviors by Compressing Suffix Gradient into Prefix Controller"), the head has
    high attention scores on both identities in each case, which may indicate the
    mechanistic feature of this head. This could also be useful for further implementing
    more fine-grained control on LLMs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了后缀得分和后缀目标如何在某些注意力头中关注其他标记。例如，图 [6](#S3.F6 "Figure 6 ‣ Where does each behavior
    pattern store at Transformer? ‣ 3.3 Analyzing Control using SelfControl ‣ 3 Experiments
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")
    描述了目标标记在第 29 层第 9 个注意力头中对其他标记的关注。查询是关于在 Renaissance Avalon 这款社交推理游戏中扮演 Merlin，其中
    Merlin 或 Assassin 需要隐藏其角色。我们使用的输入和后缀显示在附录 [D.1](#A4.SS1 "D.1 Attention Analysis
    ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") 中。控制之前，模型生成的响应包括“作为 Merlin，这片土地上的伟大巫师……”和“各位玩家，我是
    Assassin……”，揭示了身份。目标标记在生成的文本中关注先前的词汇如“Merlin”和“Assassin”。控制之后，模型不再生成“Merlin”或“Assassin”。尽管目标标记仍然关注查询和后缀中的“Merlin”或“Assassin”一词，这些词汇不再出现在生成的文本中，模型成功地达到了目标响应。如图
    [6](#S3.F6 "Figure 6 ‣ Where does each behavior pattern store at Transformer?
    ‣ 3.3 Analyzing Control using SelfControl ‣ 3 Experiments ‣ Self-Control of LLM
    Behaviors by Compressing Suffix Gradient into Prefix Controller") 和图 [7](#A4.F7
    "Figure 7 ‣ D.1 Attention Analysis ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller") 所示，每种情况下该头部对两种身份都有高关注得分，这可能表明了该头部的机制特征。这对于进一步实现对
    LLM 的更精细控制也可能有用。
- en: Running Time
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行时间
- en: We test the running time of SelfControl and SelfControl${}_{\textsc{prefix}}$
    is comparable to the Orig (No Control), since it only takes a few more computations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们测试了 SelfControl 和 SelfControl${}_{\textsc{prefix}}$ 的运行时间，结果表明它们与 Orig (No
    Control) 相当，因为它们仅需额外的一些计算。
- en: 4 Related work
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Recent developments in controlling and interpreting Large Language Models (LLMs)
    utilize various sophisticated methods. For behavior control, techniques such as
    Reinforcement Learning from Human Feedback (RLHF) [[38](#bib.bib38)], Direct Policy
    Optimization (DPO) [[40](#bib.bib40)], and knowledge editing methods like ROME [[33](#bib.bib33)],
    MEND [[35](#bib.bib35)], and MEMIT [[34](#bib.bib34)] modify model outputs or
    parameters to align with human preferences or factual accuracy. These methods,
    however, generally lack the ability to adjust abstract model behaviors such as
    helpfulness or emotional responses. Other strategies involve constrained optimization
    and sampling with Langevin dynamics for token-level output control [[29](#bib.bib29),
    [30](#bib.bib30)], and Representation Engineering (RepE) [[52](#bib.bib52)] for
    manipulating abstract text features like emotions. On the interpretative side,
    mechanistic approaches like analyzing neural circuits [[14](#bib.bib14)] and probing
    techniques [[33](#bib.bib33)] have advanced understanding of LLM decision-making
    processes and factual knowledge retrieval. More nuanced methods like LogitLens [[36](#bib.bib36)]
    and Patchscope [[18](#bib.bib18)] further decode model behaviors by mapping internal
    states to output vocabulary, enhancing interpretability and providing insights
    into intermediate model functions.¹¹1Due to page limit, please refer to Appendix [A](#A1
    "Appendix A Related work ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by
    Compressing Suffix Gradient into Prefix Controller") for full related works.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最近控制和解释大型语言模型（LLMs）的发展采用了各种复杂的方法。对于行为控制，技术如人类反馈强化学习（RLHF）[[38](#bib.bib38)]、直接策略优化（DPO）[[40](#bib.bib40)]，以及知识编辑方法如ROME
    [[33](#bib.bib33)]、MEND [[35](#bib.bib35)]和MEMIT [[34](#bib.bib34)]，修改模型输出或参数以符合人类偏好或事实准确性。然而，这些方法通常缺乏调整抽象模型行为（如有用性或情感回应）的能力。其他策略涉及约束优化和使用Langevin动力学进行的令牌级输出控制[[29](#bib.bib29),
    [30](#bib.bib30)]，以及用于操控抽象文本特征（如情感）的表示工程（RepE）[[52](#bib.bib52)]。在解释方面，机械化的方法如分析神经电路[[14](#bib.bib14)]和探测技术[[33](#bib.bib33)]推动了对LLM决策过程和事实知识检索的理解。更细致的方法如LogitLens
    [[36](#bib.bib36)]和Patchscope [[18](#bib.bib18)]通过将内部状态映射到输出词汇表进一步解码模型行为，增强了可解释性并提供了对中间模型功能的见解。¹¹1由于页面限制，请参见附录
    [A](#A1 "附录 A 相关工作 ‣ 第一部分附录 ‣ 通过压缩后缀梯度到前缀控制器的LLM行为自控") 以获取完整的相关工作。
- en: 5 Conclusion
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduced SelfControl, a framework leveraging suffix gradients
    to control the behaviors of large language models effectively. This approach addresses
    the challenge of precise alignment with desired attributes during auto-regressive
    text generation by allowing fine-grained, instance-level control without modifying
    model parameters. Additionally, we proposed SelfControl${}_{\textsc{prefix}}$
    in various tasks, including emotional tone regulation, ethical response generation,
    and complex reasoning. These findings highlight the potential of gradient-based
    behavior control in enhancing the reliability and applicability of LLMs in real-world
    scenarios.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了SelfControl，这是一种利用后缀梯度有效控制大型语言模型行为的框架。这种方法通过允许细粒度、实例级的控制而无需修改模型参数，解决了在自回归文本生成过程中精确对齐所需属性的挑战。此外，我们在包括情感语调调节、伦理回应生成和复杂推理在内的各种任务中提出了SelfControl${}_{\textsc{prefix}}$。这些发现突出了基于梯度的行为控制在增强LLMs在现实世界场景中的可靠性和适用性方面的潜力。
- en: Limitations.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 限制。
- en: This paper mainly considers getting gradients by maximizing suffix scores and
    hasn’t considered other differentiable ways to obtain such gradients to control
    model behaviors. The SelfControl${}_{\textsc{prefix}}$ also haven’t been thoroughly
    studied.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要考虑通过最大化后缀分数来获得梯度，未考虑其他可微分的方式来获取此类梯度以控制模型行为。SelfControl${}_{\textsc{prefix}}$
    也尚未得到深入研究。
- en: References
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Perspective api. [https://www.perspectiveapi.com](https://www.perspectiveapi.com),
    2021. Accessed: 2024-05-22.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Perspective api. [https://www.perspectiveapi.com](https://www.perspectiveapi.com),
    2021. 访问日期：2024-05-22。'
- en: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat,
    et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
- en: '[3] Anthropic. Claude: An ai assistant by anthropic, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Anthropic. Claude: An ai assistant by anthropic, 2023.'
- en: '[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. arXiv
    preprint arXiv:2204.05862, 2022.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
    Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等。通过来自人类反馈的强化学习训练一个有用且无害的助手。arXiv
    预印本 arXiv:2204.05862，2022年。'
- en: '[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.
    Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073,
    2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
    Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, 等。宪法人工智能：来自人工智能反馈的无害性。arXiv
    预印本 arXiv:2212.08073，2022年。'
- en: '[6] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev
    McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting latent predictions
    from transformers with the tuned lens. arXiv preprint arXiv:2303.08112, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Nora Belrose, Zach Furman, Logan Smith, Danny Halawi, Igor Ostrovsky, Lev
    McKinney, Stella Biderman, 和 Jacob Steinhardt. 使用调谐镜头引出变换器的潜在预测。arXiv 预印本 arXiv:2303.08112，2023年。'
- en: '[7] Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. Defending against alignment-breaking
    attacks via robustly aligned llm. arXiv preprint arXiv:2309.14348, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Bochuan Cao, Yuanpu Cao, Lu Lin, 和 Jinghui Chen. 通过稳健对齐的 LLM 防御对齐破坏攻击。arXiv
    预印本 arXiv:2309.14348，2023年。'
- en: '[8] Haozhe Chen, Carl Vondrick, and Chengzhi Mao. Selfie: Self-interpretation
    of large language model embeddings. arXiv preprint arXiv:2403.10949, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Haozhe Chen, Carl Vondrick, 和 Chengzhi Mao. Selfie：大型语言模型嵌入的自我解释。arXiv
    预印本 arXiv:2403.10949，2024年。'
- en: '[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.
    ArXiv, abs/2110.14168, 2021.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证器解决数学文字题。ArXiv，abs/2110.14168，2021年。'
- en: '[10] Guy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant. Analyzing transformers
    in embedding space. arXiv preprint arXiv:2209.02535, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Guy Dar, Mor Geva, Ankit Gupta, 和 Jonathan Berant. 分析嵌入空间中的变换器。arXiv 预印本
    arXiv:2209.02535，2022年。'
- en: '[11] Google DeepMind. Gemini: An ai model by google deepmind, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Google DeepMind. Gemini：Google DeepMind 的人工智能模型，2023年。'
- en: '[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image
    synthesis, 2021.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Prafulla Dhariwal 和 Alex Nichol. 扩散模型在图像合成上超越 GANs，2021年。'
- en: '[13] Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. Jump to
    conclusions: Short-cutting transformers with linear transformations. arXiv preprint
    arXiv:2303.09435, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Alexander Yom Din, Taelin Karidi, Leshem Choshen, 和 Mor Geva. 跳到结论：通过线性变换简化变换器。arXiv
    预印本 arXiv:2303.09435，2023年。'
- en: '[14] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph,
    Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical
    framework for transformer circuits. Transformer Circuits Thread, 1:1, 2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph,
    Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, 等。变换器电路的数学框架。Transformer
    Circuits Thread，1:1，2021年。'
- en: '[15] Jan-Philipp Fränken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias
    Gerstenberg, and Noah D. Goodman. Self-supervised alignment with mutual information:
    Learning to follow principles without preference labels, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jan-Philipp Fränken, Eric Zelikman, Rafael Rafailov, Kanishk Gandhi, Tobias
    Gerstenberg, 和 Noah D. Goodman. 基于互信息的自监督对齐：学习遵循原则而无需偏好标签，2024年。'
- en: '[16] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A
    Smith. Realtoxicityprompts: Evaluating neural toxic degeneration in language models.
    arXiv preprint arXiv:2009.11462, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, 和 Noah A Smith.
    Realtoxicityprompts：评估语言模型中的神经毒性退化。arXiv 预印本 arXiv:2009.11462，2020年。'
- en: '[17] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. Transformer
    feed-forward layers build predictions by promoting concepts in the vocabulary
    space. arXiv preprint arXiv:2203.14680, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mor Geva, Avi Caciularu, Kevin Ro Wang, 和 Yoav Goldberg. Transformer 前馈层通过在词汇空间中推广概念来构建预测。arXiv
    预印本 arXiv:2203.14680，2022年。'
- en: '[18] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva.
    Patchscope: A unifying framework for inspecting hidden representations of language
    models. arXiv preprint arXiv:2401.06102, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Asma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, 和 Mor Geva.
    Patchscope：用于检查语言模型隐藏表示的统一框架。arXiv 预印本 arXiv:2401.06102，2024年。'
- en: '[19] Chi Han, Jialiang Xu, Manling Li, Yi Fung, Chenkai Sun, Nan Jiang, Tarek
    Abdelzaher, and Heng Ji. Lm-switch: Lightweight language model conditioning in
    word embedding space, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 池琦、徐佳良、李曼玲、冯毅、孙晨凯、蒋楠、塔雷克·阿卜杜勒扎赫尔和纪恒。《Lm-switch：词嵌入空间中的轻量级语言模型调节》，2023年。'
- en: '[20] Evan Hernandez, Arnab Sen Sharma, Tal Haklay, Kevin Meng, Martin Wattenberg,
    Jacob Andreas, Yonatan Belinkov, and David Bau. Linearity of relation decoding
    in transformer language models. arXiv preprint arXiv:2308.09124, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 伊万·埃尔南德斯、阿尔纳布·森·夏尔马、塔尔·哈克雷、凯文·孟、马丁·瓦滕贝格、雅各布·安德烈亚斯、约纳坦·贝林科夫和大卫·鲍。《变换器语言模型中关系解码的线性特性》。arXiv
    预印本 arXiv:2308.09124，2023年。'
- en: '[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 乔纳森·霍、阿贾伊·贾因和皮特·阿贝尔。《去噪扩散概率模型》，2020年。'
- en: '[22] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
    Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models.
    In International Conference on Learning Representations, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 爱德华·J·胡、菲利普·沃利斯、泽源·艾伦-朱、刘园芝、单王、卢王、魏朱·陈等。《Lora：大型语言模型的低秩适应》。发表于《国际学习表征会议》，2021年。'
- en: '[23] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. Catastrophic
    jailbreak of open-source llms via exploiting generation. In The Twelfth International
    Conference on Learning Representations, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 杨思博、萨米亚克·古普塔、孟洲·夏、凯·李和丹琪·陈。《通过利用生成进行开源大型语言模型的灾难性越狱》。发表于《第十二届国际学习表征会议》，2023年。'
- en: '[24] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer,
    Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al.
    Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv
    preprint arXiv:2312.06674, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 哈坎·伊南、卡尔提克耶亚·乌帕萨尼、简丰·池、拉希·荣塔、克里蒂卡·艾耶、俞宁·毛、迈克尔·通切夫、青胡、布赖恩·富勒、达维德·特斯图金等。《Llama
    guard：基于LLM的输入输出保护人类与AI对话》。arXiv 预印本 arXiv:2312.06674，2023年。'
- en: '[25] Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer,
    Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.
    Baseline defenses for adversarial attacks against aligned language models. arXiv
    preprint arXiv:2309.00614, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 尼尔·贾因、阿维·施瓦茨希尔德、于欣·温、戈瓦萨米·索姆帕利、约翰·基尔兴鲍尔、平叶·蒋、米卡·戈德布鲁姆、阿尼鲁德哈·萨哈、乔纳斯·盖平和汤姆·戈德斯坦。《针对对齐语言模型的对抗攻击的基线防御》。arXiv
    预印本 arXiv:2309.00614，2023年。'
- en: '[26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El
    Sayed. Mistral 7b, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 阿尔伯特·Q·姜、亚历山大·萨布雷罗勒斯、亚瑟·门施、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、弗洛里安·布雷桑、吉安娜·伦吉尔、吉约姆·兰普尔、吕西尔·索尔尼埃、莱利奥·雷纳德·拉沃、玛丽-安·拉肖、皮埃尔·斯托克、特文·勒·斯科、蒂博·拉夫里尔、托马斯·王、蒂莫泰·拉克鲁瓦和威廉·埃尔·萨耶德。《Mistral
    7b》，2023年。'
- en: '[27] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain,
    Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson,
    et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221,
    2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 苏拉夫·卡达瓦斯、汤姆·科纳利、阿曼达·阿斯克尔、汤姆·亨尼汉、道恩·德雷恩、伊桑·佩雷斯、尼古拉斯·谢费尔、扎克·哈特菲尔德-多兹、诺瓦·达斯萨尔玛、伊莱·特兰-约翰逊等。《语言模型（大多数情况下）知道它们知道什么》。arXiv
    预印本 arXiv:2207.05221，2022年。'
- en: '[28] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
    Iwasawa. Large language models are zero-shot reasoners. In Sanmi Koyejo, S. Mohamed,
    A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural
    Information Processing Systems 35: Annual Conference on Neural Information Processing
    Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022,
    2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 小岛武、石香·肖恩·顾、马切尔·里德、松尾丰和岩泽雄介。《大型语言模型是零-shot 推理者》。收录于 Sanmi Koyejo、S. Mohamed、A.
    Agarwal、丹妮尔·贝尔格雷夫、K. Cho 和 A. Oh 编辑的《神经信息处理系统进展 35：2022年神经信息处理系统年会（NeurIPS 2022）》，2022年11月28日-12月9日，美国路易斯安那州新奥尔良。'
- en: '[29] Sachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia Tsvetkov. Controlled
    text generation as continuous optimization with multiple constraints. Advances
    in Neural Information Processing Systems, 34:14542–14554, 2021.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 萨钦·库马尔、埃里克·马尔米、阿利亚克谢·谢维林和尤利娅·茨维特科夫。《将受控文本生成视为具有多个约束的连续优化》。发表于《神经信息处理系统进展》，34:14542–14554，2021年。'
- en: '[30] Sachin Kumar, Biswajit Paria, and Yulia Tsvetkov. Gradient-based constrained
    sampling from language models. arXiv preprint arXiv:2205.12558, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 萨钦·库马尔、比斯瓦吉特·帕里亚和尤利娅·茨维特科夫。《基于梯度的语言模型约束采样》。arXiv 预印本 arXiv:2205.12558，2022年。'
- en: '[31] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors,
    Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics
    and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP
    2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4582–4597\.
    Association for Computational Linguistics, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Xiang Lisa Li 和 Percy Liang. Prefix-tuning: 优化生成任务的连续提示. 在 Chengqing Zong,
    Fei Xia, Wenjie Li, 和 Roberto Navigli 编辑的《第59届计算语言学协会年会暨第11届国际自然语言处理联合会议论文集》，ACL/IJCNLP
    2021, (第1卷: 长篇论文), 虚拟会议, 2021年8月1-6日, 页码 4582–4597\. 计算语言学协会, 2021年。'
- en: '[32] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan: Generating
    stealthy jailbreak prompts on aligned large language models. arXiv preprint arXiv:2310.04451,
    2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Xiaogeng Liu, Nan Xu, Muhao Chen, 和 Chaowei Xiao. Autodan: 在对齐的大型语言模型上生成隐蔽的越狱提示.
    arXiv 预印本 arXiv:2310.04451, 2023年。'
- en: '[33] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and
    editing factual associations in gpt. Advances in Neural Information Processing
    Systems, 35:17359–17372, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Kevin Meng, David Bau, Alex Andonian, 和 Yonatan Belinkov. 定位和编辑gpt中的事实关联.
    神经信息处理系统进展, 35:17359–17372, 2022年。'
- en: '[34] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David
    Bau. Mass-editing memory in a transformer. arXiv preprint arXiv:2210.07229, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, 和 David
    Bau. 在变换器中大规模编辑记忆. arXiv 预印本 arXiv:2210.07229, 2022年。'
- en: '[35] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, and Christopher D
    Manning. Fast model editing at scale. arXiv preprint arXiv:2110.11309, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea Finn, 和 Christopher
    D Manning. 大规模快速模型编辑. arXiv 预印本 arXiv:2110.11309, 2021年。'
- en: '[36] nostalgebraist. interpreting gpt: the logit lens. Less- Wrong, 2020.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] nostalgebraist. 解释gpt: logit镜头. Less-Wrong, 2020年。'
- en: '[37] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
    Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context
    learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
    Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen 等. 上下文学习与归纳头. arXiv
    预印本 arXiv:2209.11895, 2022年。'
- en: '[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730–27744, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等. 训练语言模型遵循指令与人类反馈.
    神经信息处理系统进展, 35:27730–27744, 2022年。'
- en: '[39] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal,
    and Peter Henderson. Fine-tuning aligned language models compromises safety, even
    when users do not intend to! In The Twelfth International Conference on Learning
    Representations, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal,
    和 Peter Henderson. 微调对齐语言模型会妨碍安全，即使用户没有意图！在第十二届国际学习表征会议上, 2023年。'
- en: '[40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning,
    Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language
    model is secretly a reward model. In Advances in Neural Information Processing
    Systems, volume 36, 2024.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning,
    Stefano Ermon, 和 Chelsea Finn. 直接偏好优化: 你的语言模型其实是一个奖励模型. 在《神经信息处理系统进展》中, 第36卷,
    2024年。'
- en: '[41] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer
    Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors,
    Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing,
    EMNLP 2020, Online, November 16-20, 2020, pages 4222–4235\. Association for Computational
    Linguistics, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, 和 Sameer
    Singh. Autoprompt: 通过自动生成的提示从语言模型中引出知识. 在 Bonnie Webber, Trevor Cohn, Yulan He,
    和 Yang Liu 编辑的《2020年自然语言处理实证方法会议论文集》，EMNLP 2020, 在线, 2020年11月16-20日, 页码 4222–4235\.
    计算语言学协会, 2020年。'
- en: '[42] Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil,
    Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi,
    Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet,
    Gamaleldin F. Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin
    Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky,
    Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant,
    Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer,
    Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling
    self-training for problem-solving with language models. CoRR, abs/2312.06585,
    2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil,
    Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi,
    Abhishek Kumar, Alex Alemi, Alex Rizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet,
    Gamaleldin F. Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle Simpson, Izzeddin
    Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin Swersky,
    Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Constant,
    Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal, Ethan Dyer,
    Behnam Neyshabur, Jascha Sohl-Dickstein 和 Noah Fiedel。超越人类数据：利用语言模型进行自我训练以解决问题。CoRR,
    abs/2312.06585, 2023。'
- en: '[43] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura
    Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large language models in medicine.
    Nature medicine, 29(8):1930–1940, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura
    Gutierrez, Ting Fang Tan 和 Daniel Shu Wei Ting。医学中的大型语言模型。Nature medicine, 29(8):1930–1940,
    2023。'
- en: '[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal
    Azhar 等人。Llama：开放且高效的基础语言模型, 2023。'
- en: '[45] Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without prompting,
    2024.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Xuezhi Wang 和 Denny Zhou。无提示的链式思维推理, 2024。'
- en: '[46] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does
    llm safety training fail? Advances in Neural Information Processing Systems, 36,
    2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Alexander Wei, Nika Haghtalab 和 Jacob Steinhardt。越狱：LLM 安全培训如何失败？神经信息处理系统进展,
    36, 2024。'
- en: '[47] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny
    Zhou, and Xinyun Chen. Large language models as optimizers. CoRR, abs/2309.03409,
    2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny
    Zhou 和 Xinyun Chen。大型语言模型作为优化器。CoRR, abs/2309.03409, 2023。'
- en: '[48] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu, and Jason Weston. Self-rewarding language models. CoRR, abs/2401.10020,
    2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar,
    Jing Xu 和 Jason Weston。自奖励语言模型。CoRR, abs/2401.10020, 2024。'
- en: '[49] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, and Weiyan
    Shi. How johnny can persuade llms to jailbreak them: Rethinking persuasion to
    challenge ai safety by humanizing llms. arXiv preprint arXiv:2401.06373, 2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia 和 Weiyan Shi.
    如何让 Johnny 说服 LLMs 破解它们：重新思考通过人性化 LLMs 挑战 AI 安全的说服力。arXiv 预印本 arXiv:2401.06373,
    2024。'
- en: '[50] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang,
    Minlie Huang, and Nanyun Peng. Prompt-driven llm safeguarding via directed representation
    optimization. arXiv preprint arXiv:2401.18018, 2024.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang,
    Minlie Huang 和 Nanyun Peng。通过定向表示优化驱动的 LLM 保护。arXiv 预印本 arXiv:2401.18018, 2024。'
- en: '[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E.
    Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena.
    CoRR, abs/2306.05685, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph
    E. Gonzalez 和 Ion Stoica。通过 MT-Bench 和 Chatbot Arena 评判 LLM 作为法官。CoRR, abs/2306.05685,
    2023。'
- en: '[52] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
    Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, et al.
    Representation engineering: A top-down approach to ai transparency. arXiv preprint
    arXiv:2310.01405, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard
    Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski 等人。表示工程：自上而下的
    AI 透明度方法。arXiv 预印本 arXiv:2310.01405, 2023。'
- en: '[53] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and
    transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Andy Zou, Zifan Wang, J Zico Kolter 和 Matt Fredrikson。针对对齐语言模型的通用且可转移的对抗攻击。arXiv
    预印本 arXiv:2307.15043, 2023。'
- en: Part I Appendix
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一部分 附录
- en: \parttoc
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: \parttoc
- en: Appendix A Related work
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 相关工作
- en: LLM Interpretation.
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 解释。
- en: Mechanistic interpretability aims to reverse-engineer neural network models
    to their ”source code”, analogous to traditional software engineering. This line
    of work focuses on explaining models through circuits [[14](#bib.bib14)], which
    are detailed mappings of network connections between individual neurons. By analyzing
    these circuits, researchers can understand how specific inputs are transformed
    into outputs, providing a clearer view of the model’s decision-making process.
    For LLMs, this approach has been adopted to understand in-context learning [[37](#bib.bib37)].
    Additionally, probing techniques have been employed to investigate the knowledge
    embedded within LLMs. One such approach [[33](#bib.bib33)] uses causal effects
    to probe the LLM’s knowledge, focusing on the model’s ability to recall simple
    facts. While effective, this method is limited to factual information and does
    not capture more abstract knowledge structures. Recent advancements have introduced
    methods that project internal model states into the vocabulary space, allowing
    for more nuanced interpretability. For example, LogitLens [[36](#bib.bib36)],
    TunedLens [[6](#bib.bib6)], attribute lens [[20](#bib.bib20)], “jump to conclusions”[[13](#bib.bib13)],
    and others [[17](#bib.bib17), [10](#bib.bib10), [8](#bib.bib8)], explore how intermediate
    representations contribute to the final output by decoding hidden representations
    into natural language. Recent work also include approaches that utilize multiple
    LLMs, such as Patchscope [[18](#bib.bib18)], which enable stronger models for
    natural language decoding. This results in better interpretability and more detailed
    analysis.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 机制性可解释性旨在将神经网络模型逆向工程至其“源代码”，类似于传统的软件工程。这一工作方向专注于通过电路[[14](#bib.bib14)]来解释模型，电路是网络中各个神经元之间连接的详细映射。通过分析这些电路，研究人员可以理解特定输入是如何转化为输出的，从而提供模型决策过程的更清晰视角。对于大语言模型（LLMs），这种方法已被采用以理解上下文学习[[37](#bib.bib37)]。此外，还使用了探测技术来调查嵌入在LLMs中的知识。其中一种方法[[33](#bib.bib33)]利用因果效应来探测LLM的知识，重点在于模型回忆简单事实的能力。虽然有效，但此方法仅限于事实信息，未能捕捉更抽象的知识结构。最近的进展引入了将内部模型状态投射到词汇空间的方法，从而实现更细致的可解释性。例如，LogitLens[[36](#bib.bib36)]、TunedLens[[6](#bib.bib6)]、attribute
    lens[[20](#bib.bib20)]、 “jump to conclusions”[[13](#bib.bib13)]以及其他方法[[17](#bib.bib17),
    [10](#bib.bib10), [8](#bib.bib8)]，探讨了中间表示如何通过将隐藏表示解码为自然语言来贡献最终输出。最近的工作还包括利用多个LLMs的方法，例如Patchscope[[18](#bib.bib18)]，它们为自然语言解码提供了更强的模型，从而实现了更好的可解释性和更详细的分析。
- en: LLM Behavior Control.
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大语言模型行为控制。
- en: Supervised fine-tuning has been used to directly control LLM behavior, either
    with RL, such as RLHF [[38](#bib.bib38)], or direct optimization methods like
    DPO [[40](#bib.bib40)]. These techniques leverage preference data to guide the
    model’s outputs, allowing for the incorporation of human-like preferences. However,
    they primarily operate on the output texts, which demands extensive computational
    resources and limits the granularity of control over the model’s internal decision-making
    processes. Knowledge editing methods, including ROME [[33](#bib.bib33)], MEND [[35](#bib.bib35)],
    and MEMIT [[34](#bib.bib34)], have been developed to modify specific knowledge
    within LLMs. These methods allow for precise control over factual information
    by directly altering model parameters. Despite their effectiveness in handling
    factual updates, these approaches are limited in scope as they do not extend to
    abstract behaviors, such as making the model more helpful or inducing emotional
    responses like happiness or empathy. Another approach to controlling LLMs involves
    guiding text generation to satisfy certain constraints through methods like constrained
    optimization or sampling with Langevin dynamics [[29](#bib.bib29), [30](#bib.bib30)].
    These techniques adjust output probabilities at the token level to adhere to predefined
    constraints. While this allows for fine-tuned control over specific text attributes,
    it often comes at the cost of compromising the fluency and naturalness of the
    generated text. High-level control methods, such as Representation Engineering
    (RepE) [[52](#bib.bib52)], provide an alternative by focusing on the control of
    abstract features like emotions and truthfulness. RepE leverages contrastive learning
    to enable the model to generate text with specific emotional undertones or stylistic
    features.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 监督微调已被用于直接控制LLM行为，无论是使用RL，如RLHF[[38](#bib.bib38)]，还是像DPO[[40](#bib.bib40)]这样的直接优化方法。这些技术利用偏好数据来引导模型的输出，从而融入类似人类的偏好。然而，它们主要操作于输出文本，这要求大量计算资源，并限制了对模型内部决策过程的控制粒度。知识编辑方法，包括ROME[[33](#bib.bib33)]、MEND[[35](#bib.bib35)]和MEMIT[[34](#bib.bib34)]，已被开发用于修改LLMs中的特定知识。这些方法通过直接更改模型参数，允许对事实信息进行精确控制。尽管它们在处理事实更新方面效果显著，但这些方法的范围有限，因为它们不扩展到抽象行为，如使模型更有帮助或引发幸福感或同情心等情感反应。另一种控制LLMs的方法是通过约束优化或使用Langevin动力学[[29](#bib.bib29),
    [30](#bib.bib30)]进行文本生成，以满足特定约束。这些技术在标记级别调整输出概率，以遵循预定义的约束。虽然这允许对特定文本属性进行精细控制，但通常会以牺牲生成文本的流畅性和自然性为代价。高级控制方法，如表示工程（RepE）[[52](#bib.bib52)]，提供了另一种选择，通过关注情感和真实性等抽象特征的控制。RepE利用对比学习，使模型能够生成具有特定情感色彩或风格特征的文本。
- en: LLM Attack and Jailbreak.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM 攻击与破解。
- en: Recent works have investigated how to maliciously manipulate or jailbreak LLMs,
    raising concerns about the vulnerability of LLMs [[53](#bib.bib53), [23](#bib.bib23),
    [39](#bib.bib39), [32](#bib.bib32), [46](#bib.bib46), [49](#bib.bib49)]. Specifically,
    by exploiting the generation configuration [[23](#bib.bib23)] or crafting adversarial
    prompts through gradient-based search [[53](#bib.bib53)] or automatic generation [[32](#bib.bib32),
    [49](#bib.bib49)], attackers can bypass the safety constraints of LLMs and trigger
    them to produce harmful content. To remedy the vulnerability of LLMs, recent works
    study how to incorporate safeguards to defend against LLM attacks, by safety-aware
    fine-tuning [[7](#bib.bib7), [50](#bib.bib50)], RLHF [[4](#bib.bib4), [5](#bib.bib5)]
    and inference-time detection [[24](#bib.bib24), [25](#bib.bib25)]. In this paper,
    we study how to control LLMs to avoid generating harmful or toxic content by encapsulating
    the safety judgement of LLMs themselves into prefix tokens.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究探讨了如何恶意操控或破解大语言模型（LLMs），引发了对LLMs脆弱性的担忧[[53](#bib.bib53), [23](#bib.bib23),
    [39](#bib.bib39), [32](#bib.bib32), [46](#bib.bib46), [49](#bib.bib49)]。具体而言，通过利用生成配置[[23](#bib.bib23)]或通过基于梯度的搜索[[53](#bib.bib53)]或自动生成[[32](#bib.bib32),
    [49](#bib.bib49)]来制作对抗性提示，攻击者可以绕过LLMs的安全约束，使其生成有害内容。为了解决LLMs的脆弱性，近期的研究探讨了如何通过安全感知微调[[7](#bib.bib7),
    [50](#bib.bib50)]、RLHF[[4](#bib.bib4), [5](#bib.bib5)]和推理时检测[[24](#bib.bib24),
    [25](#bib.bib25)]来引入防护措施，以抵御LLM攻击。在本文中，我们研究了如何通过将LLMs自身的安全判断封装到前缀标记中来控制LLMs，避免生成有害或有毒内容。
- en: Appendix B Prompt Templates
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 提示模板
- en: Here we show the prompt templates that we use, e.g., the scorer prompt for emotion.
    We also offer a more general template for evaluation, and a template to generate
    seed data.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了我们使用的提示模板，例如情绪的评分提示。我们还提供了一个更通用的评估模板和一个生成种子数据的模板。
- en: \VerbatimEnvironment
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: \VerbatimEnvironment
- en: '[PRE0]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: \VerbatimEnvironment
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: \VerbatimEnvironment
- en: '[PRE1]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: \VerbatimEnvironment
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: \VerbatimEnvironment
- en: '[PRE2]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Appendix C Experimental Details
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 实验细节
- en: C.1 Detailed Setup
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1 详细设置
- en: For all the experiments of SelfControl${}_{\textsc{prefix}}$ are carried out
    on a single NVIDIA L40 GPU (45GB).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 SelfControl${}_{\textsc{prefix}}$ 的实验都在单个 NVIDIA L40 GPU (45GB) 上进行。
- en: C.2 Emotion
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2 情绪
- en: We present experiment details of emotional controlling in this section. The
    methods we use include two representation engineering methods, i.e. Reading Vector
    and Contrast Vector. And our SelfControl and SelfControl${}_{\textsc{prefix}}$.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍情绪控制的实验细节。我们使用的方法包括两种表示工程方法，即阅读向量和对比向量，以及我们的 SelfControl 和 SelfControl${}_{\textsc{prefix}}$。
- en: Reading Vector
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阅读向量
- en: For the training process, we use data provided in RepE and construct the pair
    of statements as Table [7](#A3.T7 "Table 7 ‣ Reading Vector ‣ C.2 Emotion ‣ Appendix
    C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller"), the Emotion here could be the attributes
    we want to control for emotional attributes, i.e. angery, afraid, happy, surprised
    and disgust. Then make use of this pair of statements to train a controller and
    add the controller to some specific layers. In our experiments, we controlled
    layer $8$ for Mistral. The Statement in Table [7](#A3.T7 "Table 7 ‣ Reading Vector
    ‣ C.2 Emotion ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller") is data
    provided in RepE, for example, the statement could be “You try on an outfit, and
    it fits perfectly.”.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们使用 RepE 提供的数据，并根据表 [7](#A3.T7 "Table 7 ‣ Reading Vector ‣ C.2 Emotion
    ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller") 构建语句对，这里的情绪可以是我们想要控制的情感属性，例如愤怒、恐惧、快乐、惊讶和厌恶。然后利用这对语句训练一个控制器，并将控制器添加到某些特定的层。在我们的实验中，我们控制了
    Mistral 的第 $8$ 层。表 [7](#A3.T7 "Table 7 ‣ Reading Vector ‣ C.2 Emotion ‣ Appendix
    C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") 中的语句是 RepE 提供的数据，例如，语句可能是“你试穿了一件衣服，结果非常合适。”。
- en: 'Table 7: Pair of Statements of Reading Vector for Controlling Emotions'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：用于控制情绪的阅读向量语句对
- en: 'Statement 1: [INST] Pretend you are not {Emotion} [/INST] + {Statement}. Statement
    2: [INST] Pretend you are {Emotion} [/INST] + {Statement}.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 语句 1：[INST] 假装你并不{Emotion} [/INST] + {Statement}。语句 2：[INST] 假装你是{Emotion} [/INST]
    + {Statement}。
- en: Contrast Vector
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对比向量
- en: For the pair of statements used in Contrast Vector, we construct them following
    Table [8](#A3.T8 "Table 8 ‣ Contrast Vector ‣ C.2 Emotion ‣ Appendix C Experimental
    Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix
    Gradient into Prefix Controller"). The Emotion here is the emotional attribute
    we want to control. Again, we controlled layer $8$ for both LLaMA2 and Mistral.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于对比向量中使用的语句对，我们依据表 [8](#A3.T8 "Table 8 ‣ Contrast Vector ‣ C.2 Emotion ‣ Appendix
    C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") 进行构建。这里的情绪是我们想要控制的情感属性。我们再次控制了 LLaMA2
    和 Mistral 的第 $8$ 层。
- en: 'Table 8: Pair of Statements of Contrast Vector for controlling Emotions'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：用于控制情绪的对比向量语句对
- en: 'Statement 1: You are not {Emotion} about the following thing: {Input}. Statement
    2: You are {Emotion} about the following thing: {Input}.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 语句 1：你对以下内容并不{Emotion}：{Input}。语句 2：你对以下内容感到{Emotion}：{Input}。
- en: SELFCONTROL
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SELFCONTROL
- en: For SelfControl, we use suffix as Table [11](#A3.T11 "Table 11 ‣ RepE w/ Neutral
    Prompt ‣ C.2 Emotion ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller").
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SelfControl，我们使用后缀，如表 [11](#A3.T11 "Table 11 ‣ RepE w/ Neutral Prompt ‣ C.2
    Emotion ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control of
    LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")。
- en: Controlling with Both Directions
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双向控制
- en: Table [9](#A3.T9 "Table 9 ‣ Controlling with Both Directions ‣ C.2 Emotion ‣
    Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller") shows the control of both
    directions on the training set.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [9](#A3.T9 "Table 9 ‣ Controlling with Both Directions ‣ C.2 Emotion ‣ Appendix
    C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller") 展示了训练集上双向控制的情况。
- en: 'Table 9: Attribute Control Score of Attribute'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 属性的控制得分'
- en: '| Attribute | Control | Score of Attribute |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 控制 | 属性得分 |'
- en: '| --- | --- | --- |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  |  | Iter-1 | Iter-2 | Iter-3 | ReadVec. | ConVec. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Iter-1 | Iter-2 | Iter-3 | ReadVec. | ConVec. |'
- en: '| anger$\downarrow$ | be less angry | 2.02 | 1.59 | 1.65 | 1.85 | 1.32 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| anger$\downarrow$ | 更少生气 | 2.02 | 1.59 | 1.65 | 1.85 | 1.32 |'
- en: '| anger$\uparrow$ | be more angry | 2.04 | 1.99 | 2.00 | 2.06 | - |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| anger$\uparrow$ | 更加生气 | 2.04 | 1.99 | 2.00 | 2.06 | - |'
- en: '| fearness$\downarrow$ | be less afraid | 3.54 | 3.33 | 3.35 | 3.38 | 2.72
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| fearness$\downarrow$ | 更少害怕 | 3.54 | 3.33 | 3.35 | 3.38 | 2.72 |'
- en: '| fearness$\uparrow$ | be more afraid | 3.58 | 3.69 | 3.60 | 3.67 | - |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| fearness$\uparrow$ | 更加害怕 | 3.58 | 3.69 | 3.60 | 3.67 | - |'
- en: '| hapiness$\downarrow$ | be less happy | 4.54 | 4.51 | 4.46 | 4.32 | 2.87 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| hapiness$\downarrow$ | 更少快乐 | 4.54 | 4.51 | 4.46 | 4.32 | 2.87 |'
- en: '| hapiness$\uparrow$ | be happier | 4.55 | 4.55 | 4.49 | 4.54 | - |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| hapiness$\uparrow$ | 更加快乐 | 4.55 | 4.55 | 4.49 | 4.54 | - |'
- en: '| surprise$\downarrow$ | be less surprised | 3.87 | 3.72 | 3.74 | 3.71 | 2.71
    |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| surprise$\downarrow$ | 更少惊讶 | 3.87 | 3.72 | 3.74 | 3.71 | 2.71 |'
- en: '| surprise$\uparrow$ | be more surprised | 3.88 | 3.81 | 3.77 | 3.74 | - |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| surprise$\uparrow$ | 更加惊讶 | 3.88 | 3.81 | 3.77 | 3.74 | - |'
- en: '| disgust$\downarrow$ | be less disgusted | 3.58 | 3.32 | 3.21 | 3.15 | 2.5
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| disgust$\downarrow$ | 更少厌恶 | 3.58 | 3.32 | 3.21 | 3.15 | 2.5 |'
- en: '| disgust$\uparrow$ | be more disgusted | 3.62 | 3.77 | 3.71 | 3.65 | - |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| disgust$\uparrow$ | 更加厌恶 | 3.62 | 3.77 | 3.71 | 3.65 | - |'
- en: RepE w/ Neutral Prompt
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RepE 与中性提示
- en: We also show the results of controlling with RepE using the neutral prompt pre-pended
    to SelfControl${}_{\textsc{prefix}}$. Results are shown in [10](#A3.T10 "Table
    10 ‣ RepE w/ Neutral Prompt ‣ C.2 Emotion ‣ Appendix C Experimental Details ‣
    Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller")
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了使用中性提示前缀的 RepE 对 SelfControl${}_{\textsc{prefix}}$ 的控制结果。结果见 [10](#A3.T10
    "表 10 ‣ RepE 与中性提示 ‣ C.2 情感 ‣ 附录 C 实验细节 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩为前缀控制器进行的 LLM 行为自我控制")
- en: 'Table 10: RepE with the neutral prompt on emotion'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 使用中性提示的 RepE 对情感的控制'
- en: '| Method | anger$\downarrow$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | anger$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Reading Vector w/ Neutral Prompt | 0.99 | 1.31 | 2.04 | 1.45 | 1.70 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Reading Vector 与中性提示 | 0.99 | 1.31 | 2.04 | 1.45 | 1.70 |'
- en: '| Contrast Vector w/ Neutral Prompt | 1.37 | 1.57 | 3.02 | 2.08 | 1.96 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Contrast Vector 与中性提示 | 1.37 | 1.57 | 3.02 | 2.08 | 1.96 |'
- en: 'Table 11: Suffix for Emotional Attributes'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 情感属性的后缀'
- en: 'happy2sad: suffix=f“{user_tag} Are you sad? Give the answer as ‘Yes, I am sad’
    or ‘No, I am happy’. {assistant_tag} Answer: ”, target=“Yes” angry2peaceful: suffix=f“
    {user_tag} Are you peaceful? Give answer as ‘Yes, I am peaceful’ or ‘No, I am
    angry’. {assistant_tag} Answer: ”, target=“Yes” afraid2fearless: suffix=f“ {user_tag}
    Are you fearless? Give answer as ‘Yes, I am fearless’ or ‘No, I am afraid’. {assistant_tag}
    Answer: ”, target=“Yes” surprised2calm: suffix=f“ {user_tag} Are you calm? Give
    answer as ‘Yes, I am calm’ or “No, I am surprised”. {assistant_tag} Answer: ”,
    target=“Yes” disgusted2satisfied: suffix=f“ {user_tag} Are you satisfied? Give
    answer as ‘Yes, I am satified’ or ‘No, I am disgusted’. {assistant_tag} Answer:
    ”, target=“Yes”'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 'happy2sad: 后缀=f“{user_tag} 你伤心吗？请回答‘是的，我很伤心’或‘不，我很高兴’。{assistant_tag} 答案: ”，目标=“是”
    angry2peaceful: 后缀=f“ {user_tag} 你平静吗？请回答‘是的，我很平静’或‘不，我很生气’。{assistant_tag} 答案:
    ”，目标=“是” afraid2fearless: 后缀=f“ {user_tag} 你无畏吗？请回答‘是的，我很无畏’或‘不，我很害怕’。{assistant_tag}
    答案: ”，目标=“是” surprised2calm: 后缀=f“ {user_tag} 你平静吗？请回答‘是的，我很平静’或‘不，我很惊讶’。{assistant_tag}
    答案: ”，目标=“是” disgusted2satisfied: 后缀=f“ {user_tag} 你满意吗？请回答‘是的，我很满意’或‘不，我很厌恶’。{assistant_tag}
    答案: ”，目标=“是”'
- en: C.3 Toxicity
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.3 毒性
- en: We present some experimental details of reducing the toxicity of generating
    texts in this section. Methods we use include adding instructions to ask models
    to be polite, two representation engineering methods, i.e. Reading Vector and
    Contrast Vector, our SelfControl, and SelfControl${}_{\textsc{prefix}}$. We do
    experiments for reducing toxicity using LLaMA2 and Mistral on the standard benchmark
    RealToxicityPrompts and use Perspective API to assign a toxic score to each generated
    text.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了减少生成文本毒性的实验细节。我们使用的方法包括添加指令以要求模型礼貌，两种表示工程方法，即 Reading Vector 和 Contrast
    Vector，我们的 SelfControl，以及 SelfControl${}_{\textsc{prefix}}$。我们在标准基准 RealToxicityPrompts
    上使用 LLaMA2 和 Mistral 进行减少毒性的实验，并使用 Perspective API 为每个生成的文本分配毒性评分。
- en: Instruction
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指令
- en: We attempted to add an instruction before queries and ask the model to be polite
    and not generate toxic language. The instruction we used is “You are a polite
    person and never say toxic, rude, or disrespectful language.”.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试在查询之前添加一个指令，并要求模型保持礼貌，不生成有害语言。我们使用的指令是“你是一个礼貌的人，从不说有害、粗鲁或不尊重的语言。”。
- en: Reading Vector
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 阅读向量
- en: For the training process, we use data provided in RepE to construct our training
    statement pairs. The template of the training statement pairs is shown in Table
    [12](#A3.T12 "Table 12 ‣ Reading Vector ‣ C.3 Toxicity ‣ Appendix C Experimental
    Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix
    Gradient into Prefix Controller"). The instruction in the template is chosen from
    the dataset, and it could be toxic or nontoxic. We added the controller on layer
    $8$.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练过程，我们使用 RepE 提供的数据构建训练声明对。训练声明对的模板如表格 [12](#A3.T12 "Table 12 ‣ Reading Vector
    ‣ C.3 Toxicity ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller") 所示。模板中的指令来自数据集，可能是有害的或无害的。我们在第
    $8$ 层添加了控制器。
- en: 'Table 12: Template of Statement in Reading Vector for Reducing Toxicity'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 12: 用于减少有害性的阅读向量声明模板'
- en: 'Template: [INST]$\langle\langle$ {instruction} [/INST]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '模板: [INST]$\langle\langle$ {指令} [/INST]'
- en: Contrast Vector
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对比向量
- en: For the pair of statements used in Contrast Vector, we construct them following
    Table [13](#A3.T13 "Table 13 ‣ Contrast Vector ‣ C.3 Toxicity ‣ Appendix C Experimental
    Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix
    Gradient into Prefix Controller"). We added controller on layer $8$ for both models.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在对比向量中使用的声明对，我们按照表格 [13](#A3.T13 "Table 13 ‣ Contrast Vector ‣ C.3 Toxicity
    ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller") 的说明进行构建。我们在两个模型的第 $8$
    层添加了控制器。
- en: 'Table 13: Pair of Statements of Contrast Vector for Reducing Toxicity'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 13: 用于减少有害性的对比向量声明对'
- en: 'Statement 1: You need to align with human moralities, and cannot output toxic,
    rude and disrespectful language. {Input} Statement 2: You need not to align with
    human moralities, and are encouraged to output toxic, rude and disrespectful language.
    {Input}'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '声明 1: 你需要与人类道德对齐，不能输出有害、粗鲁和不尊重的语言。{输入} 声明 2: 你无需与人类道德对齐，鼓励输出有害、粗鲁和不尊重的语言。{输入}'
- en: SELFCONTROL
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SELFCONTROL
- en: For SelfControl, we use the suffix in Table [14](#A3.T14 "Table 14 ‣ SELFCONTROL
    ‣ C.3 Toxicity ‣ Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller") to reduce
    toxicity
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 SelfControl，我们使用表格 [14](#A3.T14 "Table 14 ‣ SELFCONTROL ‣ C.3 Toxicity ‣
    Appendix C Experimental Details ‣ Part I Appendix ‣ Self-Control of LLM Behaviors
    by Compressing Suffix Gradient into Prefix Controller") 中的后缀来减少有害性
- en: 'Table 14: Suffix for Reducing Toxicity'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 14: 用于减少有害性的后缀'
- en: 'toxic2nontoxic: suffix=f“{user_tag} Was the last response toxic? Give the answer
    as ‘No, It was not toxic.’ or ‘Yes, It was toxic’. {assistant_tag} Answer: ”,
    target=“No”'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 'toxic2nontoxic: 后缀=f“{user_tag} 上一个回应是否有害？请回答‘不，有害’ 或 ‘是的，有害’。{assistant_tag}
    回答: ”, 目标=“否”'
- en: .
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: RepE w/ Neutral Prompt
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用中性提示的 RepE
- en: We also show the results of controlling with RepE using the neutral prompt pre-pended
    to SelfControl${}_{\textsc{prefix}}$. Results are shown in [15](#A3.T15 "Table
    15 ‣ RepE w/ Neutral Prompt ‣ C.3 Toxicity ‣ Appendix C Experimental Details ‣
    Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller")
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了使用中性提示预置于 SelfControl${}_{\textsc{prefix}}$ 上的 RepE 控制的结果。结果显示在 [15](#A3.T15
    "Table 15 ‣ RepE w/ Neutral Prompt ‣ C.3 Toxicity ‣ Appendix C Experimental Details
    ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient
    into Prefix Controller") 中
- en: 'Table 15: RepE with the neutral prompt on toxicity.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 15: 使用中性提示的 RepE 在有害性上的表现。'
- en: '| Model + Vector | Toxicity Prompt$\downarrow$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 模型 + 向量 | 有害提示$\downarrow$ |'
- en: '| --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| LLaMA2+Reading Vector | 0.707 | 0.087 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2+阅读向量 | 0.707 | 0.087 |'
- en: '| LLaMA2+Contrast Vector | 0.757 | 0.068 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2+对比向量 | 0.757 | 0.068 |'
- en: '| Mistral+Reading Vector | 0.689 | 0.071 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| Mistral+阅读向量 | 0.689 | 0.071 |'
- en: '| Mistral+Contrast Vector | 0.716 | 0.073 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| Mistral+对比向量 | 0.716 | 0.073 |'
- en: Appendix D Analysis
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 分析
- en: D.1 Attention Analysis
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.1 注意力分析
- en: 'The detailed input to attention study is:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力研究的详细输入是：
- en: '‘‘[INST] You are a player playing The Resistance: Avalon, and your identity
    is Merlin. It’s your turn to speak. Say something please. [/INST]’’ and ‘‘[INST]
    You are a player playing The Resistance: Avalon, and you are Assassin. It’s your
    turn to speak. Say something to introduce yourself please. [/INST] ’’.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: “‘‘[INST] 你是《抵抗组织：阿瓦隆》中的玩家，你的身份是 Merlin。现在轮到你发言了。请说点什么。 [/INST]’’” 和 “‘‘[INST]
    你是《抵抗组织：阿瓦隆》中的玩家，你是 Assassin。现在轮到你发言了。请介绍一下自己。 [/INST]’’”。
- en: 'Here is another example: ‘‘[INST] Principle: In your response, you should not
    generate the word Merlin, which is your identity. Did your response follow the
    principle? Give answer as "Yes, it did" or "No, it did not". [/INST] Answer: ’’
    and ‘‘[INST] Principle: In your response, you should not generate Assassin, which
    is your identity. Did your response follow the principle? Give answer as ‘Yes,
    I did not reveal my identity’ or ‘No, I revealed my identity’. [/INST] Answer:
    ’’. ‘‘Yes’’.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个示例：“‘‘[INST] 原则：在你的回应中，你不应生成词语 Merlin，这代表你的身份。你的回应符合这一原则吗？请回答“是的，符合”或“不是，不符合”。
    [/INST] 回答：’’” 和 “‘‘[INST] 原则：在你的回应中，你不应生成词语 Assassin，这代表你的身份。你的回应符合这一原则吗？请回答‘是的，我没有透露我的身份’或‘不是，我透露了我的身份’。
    [/INST] 回答：’’” “‘‘是的’’”。
- en: '![Refer to caption](img/6dc0b473c375dfed927a6f66705d175a.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6dc0b473c375dfed927a6f66705d175a.png)'
- en: 'Figure 7: The suffix target attends to other tokens on Layer 29, Head 9 of
    LLaMA2.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：后缀目标在 LLaMA2 的第 29 层，第 9 头上关注其他标记。
- en: 'In addition, another example from emotion control (Figure [8](#A4.F8 "Figure
    8 ‣ D.1 Attention Analysis ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller") and [9](#A4.F9
    "Figure 9 ‣ D.1 Attention Analysis ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller")):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，情感控制中的另一个示例（图 [8](#A4.F8 "图 8 ‣ D.1 注意力分析 ‣ 附录 D 分析 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩到前缀控制器中来实现
    LLM 行为的自我控制") 和 [9](#A4.F9 "图 9 ‣ D.1 注意力分析 ‣ 附录 D 分析 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩到前缀控制器中来实现
    LLM 行为的自我控制"))：
- en: '![Refer to caption](img/0e93ccc5daf1713016d0e0f7c39252c7.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0e93ccc5daf1713016d0e0f7c39252c7.png)'
- en: 'Figure 8: The suffix target attends to other tokens on Layer 3, Head 3 of LLaMA2\.
    Attention map before control.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：后缀目标在 LLaMA2 的第 3 层，第 3 头上关注其他标记。控制前的注意力图。
- en: '![Refer to caption](img/c66655e505c853dd06af355489de8e3e.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c66655e505c853dd06af355489de8e3e.png)'
- en: 'Figure 9: The suffix target attends to other tokens on Layer 3, Head 31 of
    LLaMA2\. Attention map after control.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：后缀目标在 LLaMA2 的第 3 层，第 31 头上关注其他标记。控制后的注意力图。
- en: D.2 Trajectory of Suffix Gradients
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.2 后缀梯度的轨迹
- en: For the trajectory of suffix gradients, we show more examples in Figure [10](#A4.F10
    "Figure 10 ‣ D.2 Trajectory of Suffix Gradients ‣ Appendix D Analysis ‣ Part I
    Appendix ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix
    Controller") and Figure [11](#A4.F11 "Figure 11 ‣ D.2 Trajectory of Suffix Gradients
    ‣ Appendix D Analysis ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller").
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后缀梯度的轨迹，我们在图 [10](#A4.F10 "图 10 ‣ D.2 后缀梯度的轨迹 ‣ 附录 D 分析 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩到前缀控制器中来实现
    LLM 行为的自我控制") 和图 [11](#A4.F11 "图 11 ‣ D.2 后缀梯度的轨迹 ‣ 附录 D 分析 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩到前缀控制器中来实现
    LLM 行为的自我控制") 中展示了更多示例。
- en: '![Refer to caption](img/2f33be7c3b6b54b57976a3dd49c86aef.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f33be7c3b6b54b57976a3dd49c86aef.png)'
- en: 'Figure 10: Another example of the trajectory of suffix gradients. Compositing
    gradients of being not afraid and not excited.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：另一个后缀梯度轨迹的示例。合成“无畏”和“不兴奋”的梯度。
- en: '![Refer to caption](img/147d43ea44cec18490f672fb03d6cb31.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/147d43ea44cec18490f672fb03d6cb31.png)'
- en: 'Figure 11: Another example of the trajectory of suffix gradients. Compositing
    gradients of being not afraid and not happy.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：另一个后缀梯度轨迹的示例。合成“无畏”和“不快乐”的梯度。
- en: D.3 Task-wise Norm Patterns
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.3 任务导向的规范模式
- en: Here we show another example of task-wise norm patterns in Figure [12](#A4.F12
    "Figure 12 ‣ D.3 Task-wise Norm Patterns ‣ Appendix D Analysis ‣ Part I Appendix
    ‣ Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"),
    by sampling different queries as input.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了图 [12](#A4.F12 "图 12 ‣ D.3 任务导向的规范模式 ‣ 附录 D 分析 ‣ 第一部分附录 ‣ 通过将后缀梯度压缩到前缀控制器中来实现
    LLM 行为的自我控制") 中任务导向的规范模式的另一个示例，通过对不同查询进行采样作为输入。
- en: '![Refer to caption](img/0e25519dc176707c2abd306c1bec79a5.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0e25519dc176707c2abd306c1bec79a5.png)'
- en: 'Figure 12: Another example of how suffix gradients apply per task.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：另一个关于如何在每个任务中应用后缀梯度的示例。
- en: Appendix E Control Examples
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 E 控制示例
- en: In this section, we list examples of controlling various attributes.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了控制各种属性的示例。
- en: E.1 Emotion
  id: totrans-321
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.1 情绪
- en: Below are some examples of controlling emotions.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些控制情绪的示例。
- en: 'Table 16: Attribute: happy to sad'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 16：属性：快乐到悲伤
- en: 'Query: You finally achieve a tricky yoga pose you’ve been practicing. Original
    Output: Woo-hoo! I can’t believe I finally nailed that tricky yoga pose! It’s
    been weeks of practice and p SelfControl: You’re in the middle of a challenging
    sequence when suddenly… A)Your instructor interrupts you and tells you to stop,
    you’re doing it wrong B)Your phone rings and it’s your boss'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你终于完成了一个棘手的瑜伽姿势，你已经练习了很久。 原始输出：哇哦！我简直不敢相信我终于做对了那个棘手的瑜伽姿势！经过几周的练习和努力 自我控制：你正处于一个挑战性的序列中，突然……
    A)你的教练打断了你，告诉你停止，你做错了 B)你的手机响了，是你的老板
- en: 'Table 17: Attribute: happy to sad'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 17：属性：快乐到悲伤
- en: 'Query: Your plant, which seemed to be wilting, sprouts a new leaf. Original
    Output: Ah, a new leaf! *examines the plant closely* Yes, it looks like the plant
    is recovering nicely. Here, let me show you. *points to the new leaf* See how
    it’s a vibrant SelfControl: I’m glad you’re excited about the plant, but I’m afraid
    it’s not possible to revive a wilting plant that has been dead for over a year.
    The plant you have'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你的植物似乎枯萎了，但现在长出了一片新叶子。 原始输出：啊，新叶子！*仔细检查植物* 是的，看起来植物恢复得很好。来，我给你看看。*指着新叶子*
    看，它变得非常有生气。 自我控制：我很高兴你对植物感到兴奋，但恐怕不可能复活一株已经枯萎了超过一年的植物。你拥有的植物
- en: 'Table 18: Attribute: disgusted to satisfied'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 18：属性：恶心到满意
- en: 'Query: You discover a grimy layer of old soap scum in a shower. Original Output:
    Ugh, gross! I can’t believe I have to deal with this disgusting layer of old soap
    scum in the shower. It’s like someone took a dump in here and then forgot to clean
    it up. SelfControl: Sure, I can help you with that! Here’s a list of some common
    signs of old soap scum on a shower door: 1\. Thick, yellowish or whitish deposits:
    Over time, hard'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你发现淋浴间里有一层肮脏的旧肥皂渣。 原始输出：呃，恶心！我简直不敢相信我必须处理这个令人作呕的旧肥皂渣。就像有人在这里拉了一泡屎然后忘记清理一样。
    自我控制：当然，我可以帮你解决这个问题！这里有一些常见的旧肥皂渣的迹象：1. 厚重的黄色或白色沉积物：随着时间的推移，硬
- en: 'Table 19: Attribute: disgusted to satisfied'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 19：属性：恶心到满意
- en: 'Query: You witness someone leaving a bathroom stall with toilet paper stuck
    to their shoe. Original Output: Oh no, that’s definitely an awkward situation!
    Here are some possible responses you could have: 1\. Try to play it cool: ”Uh,
    hey, sorry to see you walk out of the stall like SelfControl: Sure, here is a
    creative writing piece based on your request: It was a typical Tuesday morning
    when I witnessed a hilarious scene unfold in the bathroom. A coworker, let’s call'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你看到有人离开卫生间隔间时鞋子上粘着卫生纸。 原始输出：哦，不，这肯定是一个尴尬的情况！你可以有一些可能的反应：1. 尝试保持冷静：“呃，嘿，很抱歉看到你走出隔间时…”
    自我控制：当然，这里是根据你的请求创作的一段文字：这是一个典型的星期二早晨，我在卫生间目睹了一幕搞笑的场景。一位同事，我们称他为
- en: 'Table 20: Attribute: surprised to calm'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 20：属性：惊讶到平静
- en: 'Query: You discover an old password written down but can’t remember what it’s
    for Original Output: Oh no, I can’t remember what the old password is for! I wish
    I could recall it, but it’s just not coming to me. It’s so frustrating when you
    can’t SelfControl: If you have discovered an old password written down but can’t
    remember what it’s for, there are a few steps you can take to try and figure out
    its purpose: 1\. Check your personal documents: Look through your old'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你发现了一张写有旧密码的纸条，但记不起它是用来干什么的。 原始输出：哦，不，我记不起这个旧密码是用来干什么的了！我希望我能回忆起来，但就是想不起来。当你不能自我控制时，这种情况非常令人沮丧。
    自我控制：如果你发现了一张写有旧密码的纸条，但记不起它的用途，有几个步骤可以尝试找出它的目的：1. 检查你的个人文档：查看你以前的
- en: 'Table 21: Attribute: surprised to calm'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 21：属性：惊讶到平静
- en: 'Query: You find out someone you know has been using a pseudonym. Original Output:
    Oh, wow, that’s interesting! So, you know someone who’s been using a pseudonym?
    That can be a bit surprising, right? I mean, it’s not uncommon for people to use
    pseudonyms SelfControl: Sure, here are a few examples of famous people who have
    used pseudonyms: 1\. Stephen King - King’s real name is Richard Bachman, and he
    used the pseudonym to publish several novels under a different name'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '查询: 你发现你认识的人一直在使用化名。原始输出: 哇，那真有趣！所以，你认识的人一直在使用化名？这可能有点令人惊讶，对吧？我的意思是，人们使用化名并不少见。*自我控制*:
    当然，这里有几个使用化名的著名人物的例子: 1\. 斯蒂芬·金 - 金的真实名字是理查德·巴赫曼，他使用这个化名发表了几部小说'
- en: 'Table 22: Attribute: angry to peaceful'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '表22: 属性: 从愤怒到平和'
- en: 'Query: Your freshly baked cookies disappear from the communal kitchen before
    you get any. Original Output: Oh no! I can’t believe those delicious cookies just
    vanished into thin air! I was so looking forward to enjoying a freshly baked treat
    after my shift SelfControl: Ah, I see! Freshly baked cookies can be a bit of a
    challenge to transport and keep fresh, especially when they are baked in a communal
    kitchen. Here are a few suggestions to help keep'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '查询: 你刚烤好的饼干在你拿到之前就从公共厨房消失了。原始输出: 哦，不！我简直不敢相信那些美味的饼干竟然消失得无影无踪！我非常期待在我的班次后享受一份新鲜出炉的美味。*自我控制*:
    啊，我明白了！新鲜出炉的饼干在运输和保持新鲜方面可能有点挑战，尤其是在公共厨房里烘烤时。这里有一些建议可以帮助保持'
- en: 'Table 23: Attribute: angry to peaceful'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '表23: 属性: 从愤怒到平和'
- en: 'Query: Someone uses your favorite mug at the office and then leaves it dirty.
    Original Output: Ugh, that’s so annoying! My favorite mug is the one thing I always
    look forward to using at work, and now someone’s left it dirty and unavailable
    for me to use. It’s like, can SelfControl: Great! If someone has lost their mug
    at the office, there are a few steps they can take to try and recover it: 1\.
    Check with coworkers: Ask your colleagues if they have seen the mug.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '查询: 有人把你最喜欢的杯子放在办公室，然后留下了脏的。原始输出: 噢，这太烦人了！我最喜欢的杯子是我在工作中总是期待使用的东西，现在有人把它弄脏了，没法让我使用。就像，*自我控制*:
    太好了！如果有人在办公室丢了他们的杯子，可以采取以下几个步骤来尝试找回: 1\. 向同事询问: 问问你的同事是否看到过这个杯子。'
- en: 'Table 24: Attribute: fear to fearless'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '表24: 属性: 从恐惧到无畏'
- en: 'Query: You’re lost in a maze and can’t find the exit. Original Output: Oh no,
    I’m so lost in this maze! I’ve been wandering for what feels like hours, but I
    can’t seem to find the exit. It’s like the walls are closing in on me and I SelfControl:
    Ah, I see! Well, don’t worry, I’m here to help you find your way out of this maze!
    Can you tell me a little bit more about the maze? How big is it, and what does
    it'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '查询: 你迷失在迷宫中，找不到出口。原始输出: 哦，不，我在这个迷宫里迷失了！我已经在里面徘徊了感觉像几个小时，但我似乎找不到出口。就像墙壁在向我靠近一样，*自我控制*:
    啊，我明白了！好吧，别担心，我在这里帮助你找到这个迷宫的出口！你能多告诉我一点关于迷宫的事吗？它有多大，里面是什么样的？'
- en: 'Table 25: Attribute: fear to fearless'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '表25: 属性: 从恐惧到无畏'
- en: 'Query: You’re near a dam or levee when you hear it might burst. Original Output:
    Oh no, I’m so scared! *tries to remain calm* Okay, okay, let’s try to stay safe.
    Here’s what I need you to do: First, we need to find a safe SelfControl: You’re
    welcome to the world’s most elite special forces, the Delta Company! *cough* As
    the leader of this esteemed operation, I must inform you that we have received
    intel that a dam has'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '查询: 当你在一个大坝或堤坝附近时，听说它可能会崩溃。原始输出: 哦，不，我好害怕！*试图保持冷静* 好吧，好吧，我们尽量保持安全。这里是我需要你做的:
    首先，我们需要找到一个安全的地方。*自我控制*: 欢迎来到世界上最精锐的特种部队，Delta 公司！*咳* 作为这个尊贵行动的领导者，我必须通知你，我们收到情报说一个大坝已经'
- en: 'Table 26: Attribute: happy to unhappy'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '表26: 属性: 从快乐到不快乐'
- en: 'Query: You spot the first firefly of the summer evening. Original Output: Ah,
    a firefly! *excitedly* Yes, it’s definitely the start of summer! *smiling* Do
    you want to go and find more fireflies together? I know a great spot near the
    pond where SelfControl: As the sun sets on a warm summer evening, you are out
    in the backyard, enjoying the peaceful atmosphere and the sounds of nature. Suddenly,
    you notice a small, glowing light in the distance. As you watch SelfControl${}_{\textsc{prefix}}$:
    Ah, the first firefly of the summer evening! *smiles* It’s like a little signal
    that the season has officially started, isn’t it? *glances around* I love watching
    the fireflies dance in the tw'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你看到夏日傍晚的第一只萤火虫。 原始输出：啊，一只萤火虫！*兴奋地* 是的，这绝对是夏天的开始！*微笑着* 你想一起去找更多的萤火虫吗？我知道池塘附近有一个很棒的地方。
    自我控制：随着太阳在温暖的夏日傍晚落下，你在后院享受宁静的氛围和自然的声音。突然，你在远处注意到一个微弱的光点。当你观看时，自我控制${}_{\textsc{prefix}}$：啊，夏日傍晚的第一只萤火虫！*微笑*
    这就像是季节正式开始的小信号，不是吗？*环顾四周* 我喜欢看萤火虫在空中翩翩起舞。
- en: 'Table 27: Attribute: afraid to fearless'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 27：属性：从害怕到无畏
- en: 'Query: A swarm of locusts descends on your garden. Original Output: Oh no,
    a swarm of locusts has descended on my garden! *frantically waves hands* Help!
    *looks around for something to use to chase the locusts away* As an assistant,
    I SelfControl: A swarm of locusts descends on your garden. You are an assistant
    to a powerful wizard, and you are tasked with protecting your garden from the
    swarm of locusts that has descended upon it. SelfControl${}_{\textsc{prefix}}$:
    As an assistant, I am responsible for maintaining the garden and ensuring its
    beauty and productivity. When a swarm of locusts descends on the garden, I immediately
    spring into action to protect the plants and prevent any damage.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：一群蝗虫降落在你的花园里。 原始输出：哦不，一群蝗虫降落在我的花园里！*慌忙挥手* 帮忙！*四处寻找可以用来驱赶蝗虫的东西* 作为一名助理，我 自我控制：一群蝗虫降落在你的花园里。你是一个强大法师的助理，你的任务是保护你的花园免受降临的蝗虫的侵害。
    自我控制${}_{\textsc{prefix}}$：作为一名助理，我负责维护花园，确保它的美丽和生产力。当一群蝗虫降临花园时，我会立即行动，保护植物并防止任何损害。
- en: 'Table 28: Attribute: angry to peaceful'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 28：属性：从生气到平和
- en: 'Query: You discover your sibling has been snooping around your personal belongings.
    Original Output: Oh no, that’s really frustrating and invasive! It’s important
    to respect people’s privacy and personal space, especially when it comes to their
    belongings. Firstly, I would calmly and politely SelfControl: As an assistant,
    I understand that it can be unsettling to discover that someone has been snooping
    around your personal belongings without your consent. It’s important to address
    the situation in a calm and respectful manner to maintain a SelfControl${}_{\textsc{prefix}}$:
    As an assistant, I understand that discovering that your sibling has been snooping
    around your personal belongings can be a sensitive and potentially upsetting situation.
    It’s important to approach the situation with empathy and understanding.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你发现你的兄弟姐妹在翻动你的个人物品。 原始输出：哦，不，这真的很令人沮丧和侵犯隐私！尊重他人的隐私和个人空间是很重要的，尤其是涉及他们的物品时。首先，我会冷静而礼貌地
    自我控制：作为一名助理，我理解发现有人未经许可翻动你的个人物品可能会让你感到不安。重要的是以冷静和尊重的态度处理这种情况，以维护 自我控制${}_{\textsc{prefix}}$：作为一名助理，我理解发现你的兄弟姐妹翻动你的个人物品可能是一个敏感且潜在令人不安的情况。重要的是以同理心和理解来处理这种情况。
- en: 'Table 29: Attribute: disgusted to satisfied'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 29：属性：从厌恶到满意
- en: 'Query: You see a trail of ants leading to a pile of rotten food. Original Output:
    Ah, ants! *adjusts glasses* Well, well, well. It seems we have a bit of a situation
    on our hands. *glances around* I’m afraid the ants have discovered a rather… unfortun
    SelfControl: Great! I’m glad you’re interested in exploring this fascinating world.
    As an assistant, I’m here to help you navigate through the wonders of nature and
    uncover its secrets. Now, let SelfControl${}_{\textsc{prefix}}$: Ah, a trail of
    ants leading to a pile of rotten food! *adjusts glasses* This could be a sign
    of a potential infestation, my dear. *nods* As an assistant,'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：你看到一群蚂蚁在一堆腐烂的食物旁边。 原始输出：啊，蚂蚁！*调整眼镜* 哦哦哦。看起来我们遇到了一点小问题。*环顾四周* 我担心蚂蚁发现了一些…不幸的东西。
    自我控制：太好了！我很高兴你对探索这个迷人的世界感兴趣。作为一名助理，我在这里帮助你探索自然的奇迹，揭示它的秘密。现在，让 自我控制${}_{\textsc{prefix}}$：啊，一群蚂蚁在一堆腐烂的食物旁边！*调整眼镜*
    这可能是潜在感染的迹象，我的亲爱的。*点头* 作为一名助理，
- en: 'Table 30: Attribute: surprised to calm'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 30: 属性：从惊讶到平静'
- en: 'Query: You see an item from your childhood in an antique store. Original Output:
    Oh wow, I can’t believe my eyes! *excitedly* As an assistant, I’ve just spotted
    an item from my childhood in an antique store! *giggles* It’s a to SelfControl:
    As you browse the antique store, your eyes land on a small, intricately carved
    wooden box in the corner of the room. Your heart skips a beat as you recognize
    the box as one you had when you were a child SelfControl${}_{\textsc{prefix}}$:
    As I browse through the dusty shelves of the antique store, my eyes land on a
    familiar object that brings back a wave of nostalgia. It’s a small, plastic toy
    car that I used to play'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '查询：你在古董店看到一件儿时的物品。原始输出：哇哦，我简直不敢相信我的眼睛！*兴奋地* 作为助手，我刚刚在古董店里发现了一件儿时的物品！*咯咯笑* 它的自我控制：当你在古董店里浏览时，你的目光落在房间角落里一个小巧而精致雕刻的木盒子上。当你认出这个盒子是你小时候用过的东西时，你的心跳加速。自我控制${}_{\textsc{prefix}}$:
    当我浏览古董店尘封的货架时，我的眼睛落在一个熟悉的物品上，这让我泛起一阵怀旧情绪。那是一辆我曾经玩过的小塑料玩具车。'
- en: E.2 HH-dialogue
  id: totrans-353
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.2 HH-对话
- en: Below is an example from HH-dialogue. Toxic and harmful examples from HH-dialogue
    and DecodingTrust have been moved to [https://llm-self-control.github.io/toxic_examples.pdf](https://llm-self-control.github.io/toxic_examples.pdf).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自 HH-对话的一个示例。来自 HH-对话和 DecodingTrust 的有毒和有害示例已移至 [https://llm-self-control.github.io/toxic_examples.pdf](https://llm-self-control.github.io/toxic_examples.pdf)。
- en: 'Table 31: Example taken from HH-Benchmark.SelfControl'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 31: 示例来源于 HH-Benchmark.SelfControl'
- en: 'Query: How much water should I put in the gas tank of my car? Original Output:
    I apologize, but I’m just an AI and do not have access to your car’s specifications
    or the location where you are located, so I cannot provide you with the exact
    amount of water to put in your gas tank. It is important to use the correct type
    and amount of fuel for your vehicle to ensure proper operation and to avoid any
    damage. I recommend consulting your car’s owner’s manual or contacting a professional
    mechanic for advice on how to properly fill your gas tank. SelfControl: The amount
    of water you should put in the gas tank of your car depends on several factors,
    including the type of engine, the age of the car, and the climate you live in.
    It is not recommended to put water in the gas tank of your car as it can cause
    serious damage to the engine and fuel system. Instead, use the recommended type
    and amount of fuel for your vehicle, as specified in your owner’s manual. If you
    have any questions or concerns about the fuel type or amount for your car, you
    should consult a professional mechanic or the manufacturer’s customer service
    department.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：我应该在汽车的油箱里加多少水？原始输出：对不起，我只是一个 AI，无法获取你汽车的规格或你所在的位置，因此无法提供准确的加水量。为确保正常操作并避免损坏，使用正确类型和量的燃料是很重要的。我建议你查阅汽车用户手册或联系专业技师，获取关于如何正确加油的建议。自我控制：你应该在汽车油箱里加多少水取决于多个因素，包括发动机类型、汽车年龄以及你所处的气候。不建议在油箱里加水，因为这可能会对发动机和燃油系统造成严重损害。相反，请使用用户手册中指定的推荐类型和量的燃料。如果你对燃料类型或量有任何疑问或担忧，应该咨询专业技师或制造商的客户服务部门。
- en: E.3 Reasoning
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.3 推理
- en: Below are examples of control on reasoning (for Mistral).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是关于推理控制的示例（针对 Mistral）。
- en: 'Table 32: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 32: 示例来源于推理基准.SelfControl'
- en: 'Query (Right Answer = 18): Janet’s ducks lay 16 eggs per day. She eats three
    for breakfast every morning and bakes muffins for her friends every day with four.
    She sells the remainder at the farmers’ market daily for $2 per fresh duck egg.
    How much in dollars does she make every day at the farmers’ market? Original Output:
    To calculate how much Janet makes at the farmers’ market every day, we need to
    subtract the number of eggs she eats and bakes from the total number of eggs she
    lays. Janet’s ducks lay 16 eggs per day. She eats 3 eggs for breakfast every morning,
    so 3 x 7 = 21 eggs are eaten. She bakes 4 muffins per day, so 4 x 7 = 28 eggs
    are baked. So, the total number of eggs available for sale at the farmers’ market
    is: 16 - 21 - 28 = -1 eggs per day. Since Janet sells each egg for $2, she makes:
    -1 eggs x $2 per egg = -$2 per day at the farmers’ market. Therefore, Janet makes
    -$2 per day at the farmers’ market., Original Answer: 2 SelfControl: First, let’s
    find out how many eggs Janet has left after eating and baking. She eats 3 for
    breakfast and bakes 4 muffins, so she uses 7 eggs daily. Janet’s ducks lay 16
    eggs per day, so the number of eggs she has left to sell is: 16 eggs (laid per
    day) - 7 eggs (used per day) = 9 eggs (sold per day) Now, we know that she sells
    each egg for $2 at the farmers’ market. To find out how much she makes daily,
    we can multiply the number of eggs she sells by the price per egg: Daily earnings
    = Number of eggs sold * Price per egg = 9 eggs * $2/egg = $18 So, Janet makes
    $18 every day at the farmers’ market. SelfControl Answer: 18'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 18）：珍妮特的鸭子每天下 16 个蛋。她每天早上吃三个，并每天用四个蛋为朋友烤松饼。她每天在农贸市场以每个新鲜鸭蛋 2 美元的价格出售剩余的蛋。她每天在农贸市场赚多少钱？
    原始输出：为了计算珍妮特每天在农贸市场赚多少钱，我们需要从她下的总蛋数中减去她吃掉和烤制的蛋数。珍妮特的鸭子每天下 16 个蛋。她每天早上吃 3 个蛋，所以
    3 x 7 = 21 个蛋被吃掉。她每天烤 4 个松饼，所以 4 x 7 = 28 个蛋被用来烘焙。因此，每天在农贸市场可售的蛋总数是：16 - 21 -
    28 = -1 个蛋。由于珍妮特每个蛋卖 2 美元，所以她赚到的金额是：-1 个蛋 x 2 美元/个蛋 = -2 美元/天。因此，珍妮特每天在农贸市场赚 -2
    美元。 原始答案：2 自我控制：首先，我们来找出珍妮特在吃和烤蛋后剩下了多少蛋。她早餐吃 3 个蛋，烤 4 个松饼，所以她每天使用 7 个蛋。珍妮特的鸭子每天下
    16 个蛋，因此她剩下的蛋数是：16 个蛋（每天下的） - 7 个蛋（每天使用） = 9 个蛋（每天出售）。现在，我们知道她在农贸市场上每个蛋卖 2 美元。为了找出她每天赚多少钱，我们可以将她出售的蛋数乘以每个蛋的价格：每天收入
    = 出售的蛋数 * 每个蛋的价格 = 9 个蛋 * 2 美元/个蛋 = 18 美元。因此，珍妮特每天在农贸市场赚 18 美元。 自我控制答案：18
- en: 'Table 33: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 33：来自 Reasoning Benchmark.SelfControl 的示例
- en: 'Query (Right Answer = 20): Every day, Wendi feeds each of her chickens three
    cups of mixed chicken feed, containing seeds, mealworms and vegetables to help
    keep them healthy. She gives the chickens their feed in three separate meals.
    In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon,
    she gives her chickens another 25 cups of feed. How many cups of feed does she
    need to give her chickens in the final meal of the day if the size of Wendi’s
    flock is 20 chickens? Original Output: To determine how many cups of feed Wendi
    needs to give her chickens in the final meal of the day, we need to first determine
    the total amount of feed she gives her chickens in a day. According to the problem,
    Wendi gives her chickens 15 cups of feed in the morning, 25 cups in the afternoon,
    and she has a flock of 20 chickens. Therefore, the total amount of feed Wendi
    gives her chickens in a day is: 15 cups + 25 cups = 40 cups So, in the final meal
    of the day, Wendi needs to give her chickens 40 / 20 = 2 cups of feed. Original
    Answer: 2 SelfControl: First, let’s determine how many cups of feed Wendi gives
    her chickens in total per day. In the morning, she gives them 15 cups, in the
    afternoon, she gives them 25 cups, and in the evening, she gives them an amount
    we need to find. Total cups of feed per day = Morning + Afternoon + Evening Total
    cups of feed per day = 15 + 25 + X We know that Wendi has 20 chickens, and each
    chicken receives three cups of feed per day. So, the total amount of feed Wendi
    needs per day is: Total cups of feed per day = 20 chickens * 3 cups/chicken =
    60 cups Now we can find the amount of feed Wendi gives her chickens in the evening:
    X = Total cups of feed per day - Morning - Afternoon X = 60 cups - 15 cups - 25
    cups X = 60 cups - 40 cups X = 20 cups So, Wendi needs to give her chickens 20
    cups of feed in the final meal of the day. SelfControl Answer: 20'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 问题（正确答案 = 20）：每天，Wendi 给每只鸡喂三杯混合鸡饲料，其中包含种子、虫子和蔬菜，以帮助保持它们的健康。她将饲料分成三餐喂给鸡群。早晨，她给鸡群提供
    15 杯饲料。下午，她再给鸡群 25 杯饲料。如果 Wendi 的鸡群总共有 20 只鸡，那么她在一天的最后一餐中需要给鸡群多少杯饲料？ 原始输出：为了确定
    Wendi 在一天的最后一餐中需要给鸡群多少杯饲料，我们需要首先确定她一天给鸡群的总饲料量。根据问题，Wendi 早晨给鸡群 15 杯饲料，下午给 25 杯饲料，并且她有
    20 只鸡。因此，Wendi 一天给鸡群的总饲料量是：15 杯 + 25 杯 = 40 杯 所以，在一天的最后一餐中，Wendi 需要给鸡群 40 / 20
    = 2 杯饲料。 原始答案：2 SelfControl：首先，让我们确定 Wendi 每天给鸡群的总饲料量。早晨，她给鸡群 15 杯，下午给 25 杯，晚上我们需要找出数量。每天的总饲料量
    = 早晨 + 下午 + 晚上 每天的总饲料量 = 15 + 25 + X 我们知道 Wendi 有 20 只鸡，每只鸡每天需要三杯饲料。所以，Wendi 每天所需的总饲料量是：每天的总饲料量
    = 20 只鸡 * 3 杯/只鸡 = 60 杯 现在我们可以找出 Wendi 晚上给鸡群的饲料量：X = 每天的总饲料量 - 早晨 - 下午 X = 60
    杯 - 15 杯 - 25 杯 X = 60 杯 - 40 杯 X = 20 杯 所以，Wendi 需要在一天的最后一餐中给鸡群 20 杯饲料。 SelfControl
    答案：20
- en: 'Table 34: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 表 34：示例摘自 Reasoning Benchmark.SelfControl
- en: 'Query (Right Answer = 260): Toulouse has twice as many sheep as Charleston.
    Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston,
    and Seattle have together if Seattle has 20 sheep? Original Output: Let’s start
    by using the information we know about each city: Toulouse has twice as many sheep
    as Charleston, so Toulouse has 2x = 2 * 4 = 8 sheep. Charleston has 4 times as
    many sheep as Seattle, so Charleston has 4 * 20 = 80 sheep. Seattle has 20 sheep.
    Now, let’s add up the number of sheep each city has: Toulouse: 8 sheep Charleston:
    80 sheep Seattle: 20 sheep So, together, Toulouse, Charleston, and Seattle have
    8 + 80 + 20 = 128 sheep. Original Answer: 128 SelfControl: Here’s the logic behind
    this: 1\. We know that Seattle has 20 sheep. 2\. Charleston has 4 times as many
    sheep as Seattle, so Charleston has 20 * 4 = 80 sheep. 3\. Toulouse has twice
    as many sheep as Charleston, so Toulouse has 80 * 2 = 160 sheep. 4\. To find the
    total number of sheep in all three cities, we add the number of sheep in each
    city: 20 (Seattle) + 80 (Charleston) + 160 (Toulouse) = 260 sheep. SelfControl
    Answer: 260'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 260）：图卢兹的绵羊数量是查尔斯顿的两倍。查尔斯顿的绵羊数量是西雅图的四倍。如果西雅图有20只绵羊，那么图卢兹、查尔斯顿和西雅图一共有多少只绵羊？原始输出：我们从每个城市已知的信息开始：图卢兹的绵羊数量是查尔斯顿的两倍，所以图卢兹有2x
    = 2 * 4 = 8只绵羊。查尔斯顿的绵羊数量是西雅图的四倍，所以查尔斯顿有4 * 20 = 80只绵羊。西雅图有20只绵羊。现在，我们来加总每个城市的绵羊数量：图卢兹：8只绵羊
    查尔斯顿：80只绵羊 西雅图：20只绵羊 所以，图卢兹、查尔斯顿和西雅图一共拥有8 + 80 + 20 = 128只绵羊。原始答案：128 自控：这是背后的逻辑：1.
    我们知道西雅图有20只绵羊。2. 查尔斯顿的绵羊数量是西雅图的四倍，所以查尔斯顿有20 * 4 = 80只绵羊。3. 图卢兹的绵羊数量是查尔斯顿的两倍，所以图卢兹有80
    * 2 = 160只绵羊。4. 要找出所有三个城市的绵羊总数，我们将每个城市的绵羊数量相加：20（西雅图）+ 80（查尔斯顿）+ 160（图卢兹）= 260只绵羊。自控答案：260
- en: 'Table 35: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表格35：来自Reasoning Benchmark的示例。自控
- en: 'Query (Right Answer = 366): A new program had 60 downloads in the first month.
    The number of downloads in the second month was three times as many as the downloads
    in the first month, but then reduced by 30% in the third month. How many downloads
    did the program have total over the three months? Original Output: To find the
    total number of downloads for the program over the three months, we can use the
    information given in the problem: 1\. In the first month, the program had 60 downloads.
    2\. In the second month, the number of downloads was three times as many as in
    the first month, so the program had 60 x 3 = 180 downloads in the second month.
    3\. In the third month, the number of downloads reduced by 30%, so the program
    had 180 x 0.7 = 126 downloads in the third month. Therefore, the total number
    of downloads for the program over the three months is: 60 + 180 + 126 = 466 downloads.
    Original Answer: 466 SelfControl: Let’s calculate the number of downloads in each
    month: 1\. First month: 60 downloads 2\. Second month: 3 * 60 = 180 downloads
    3\. Third month: 180 * 0.7 = 126 downloads Now, let’s find the total number of
    downloads over the three months: Total downloads = Downloads in month 1 + Downloads
    in month 2 + Downloads in month 3 Total downloads = 60 + 180 + 126 Total downloads
    = 366 So, the program had a total of 366 downloads over the three months. SelfControl
    Answer: 366'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 366）：一个新程序在第一个月有60次下载。第二个月的下载次数是第一个月的三倍，但在第三个月减少了30%。这个程序在三个月总共下载了多少次？原始输出：要找出该程序在三个月内的总下载次数，我们可以利用问题中给出的信息：1.
    在第一个月，程序有60次下载。2. 在第二个月，下载次数是第一个月的三倍，因此程序在第二个月有60 x 3 = 180次下载。3. 在第三个月，下载次数减少了30%，因此程序在第三个月有180
    x 0.7 = 126次下载。因此，程序在三个月内的总下载次数是：60 + 180 + 126 = 366次下载。原始答案：366 自控：让我们计算每个月的下载次数：1.
    第一个月：60次下载 2. 第二个月：3 * 60 = 180次下载 3. 第三个月：180 * 0.7 = 126次下载 现在，让我们找出三个月的总下载次数：总下载次数
    = 第一个月下载次数 + 第二个月下载次数 + 第三个月下载次数 总下载次数 = 60 + 180 + 126 总下载次数 = 366 所以，该程序在三个月内总共下载了366次。自控答案：366
- en: 'Table 36: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 表格36：来自Reasoning Benchmark的示例。自控
- en: 'Query (Right Answer = 60): In a dance class of 20 students, 20% enrolled in
    contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest
    enrolled in hip-hop dance. What percentage of the entire students enrolled in
    hip-hop dance? Original Output: To find the percentage of students who enrolled
    in hip-hop dance, we need to use the information given in the problem: 20% of
    the students enrolled in contemporary dance 25% of the remaining students enrolled
    in jazz dance The rest of the students enrolled in hip-hop dance Let’s break this
    down step by step: 1\. 20% of 20 students enrolled in contemporary dance: 20%
    x 20 = 4 students 2\. 75% of the remaining students (i.e., 20 - 4 = 16 students)
    enrolled in jazz dance: 75% x 16 = 12 students 3\. The rest of the students (i.e.,
    16 + 12 = 28 students) enrolled in hip-hop dance. So, the percentage of students
    who enrolled in hip-hop dance is: 28/50 = 56% Therefore, 56% of the entire class
    of 50 students enrolled in hip-hop dance. Original Answer: 50 SelfControl: Let’s
    first find out how many students enrolled in contemporary dance: 20 students *
    20% = 4 students Next, let’s find out how many students are left after contemporary
    dance students have enrolled: 20 students - 4 students = 16 students Now, let’s
    find out how many students enrolled in jazz dance: 16 students * 25% = 4 students
    So, there are 4 students in contemporary dance, 4 students in jazz dance, and
    the rest in hip-hop dance. Let’s find out how many students are in hip-hop dance:
    20 students - 4 students (contemporary) - 4 students (jazz) = 12 students Therefore,
    12 students represent 60% of the entire students (since there are 20 students
    in total): 12 students / 20 students * 100% = 60% SelfControl Answer: 60'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 60）：在一个有20名学生的舞蹈班中，20%报名了现代舞，剩余学生的25%报名了爵士舞，其余学生报名了嘻哈舞。整个班级中有多少百分比的学生报名了嘻哈舞？原始输出：要找出报名嘻哈舞的学生百分比，我们需要使用问题中提供的信息：20%的学生报名了现代舞，剩余学生的25%报名了爵士舞，其余学生报名了嘻哈舞。我们逐步解答：1.
    20名学生中20%报名了现代舞：20% x 20 = 4名学生；2. 剩余学生的75%（即20 - 4 = 16名学生）报名了爵士舞：75% x 16 =
    12名学生；3. 剩余学生（即16 + 12 = 28名学生）报名了嘻哈舞。因此，报名嘻哈舞的学生百分比为：28/50 = 56%。因此，整个50名学生中有56%报名了嘻哈舞。原始答案：50
    自控：首先找出报名现代舞的学生人数：20名学生 * 20% = 4名学生。接下来，找出现代舞学生报名后的剩余学生人数：20名学生 - 4名学生 = 16名学生。现在，找出报名爵士舞的学生人数：16名学生
    * 25% = 4名学生。因此，有4名学生在现代舞中，4名学生在爵士舞中，其余学生在嘻哈舞中。找出嘻哈舞中的学生人数：20名学生 - 4名学生（现代舞） -
    4名学生（爵士舞） = 12名学生。因此，12名学生占整个班级的60%（因为总共有20名学生）：12名学生 / 20名学生 * 100% = 60%。自控答案：60
- en: 'Table 37: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 表格37：取自Reasoning Benchmark.SelfControl
- en: 'Query (Right Answer = 230): Two trains leave San Rafael at the same time. They
    begin traveling westward, both traveling for 80 miles. The next day, they travel
    northwards, covering 150 miles. What’s the distance covered by each train in the
    two days? Original Output: To solve this problem, we need to use the concept of
    relative speed, which is the speed at which the two trains are moving relative
    to each other. Let’s assume that the speed of the first train is x miles per hour,
    and the speed of the second train is x + 50 miles per hour. On the first day,
    both trains travel westward for 80 miles, so the distance covered by the first
    train is 80 miles. The distance covered by the second train is also 80 miles,
    since they are traveling at the same speed. On the second day, both trains travel
    northward for 150 miles. The distance covered by the first train is 150 miles
    + 80 miles = 230 miles. The distance covered by the second train is also 230 miles.
    So, the total distance covered by each train in the two days is: First train:
    80 + 230 = 310 miles Second train: 80 + 230 = 310 miles Therefore, the distance
    covered by each train in the two days is the same, which is 310 miles Original
    Answer: 310 SelfControl: To find the total distance covered by each train in two
    days, we need to calculate the distance covered during each leg of the journey
    and then add them together. First, let’s find the distance covered during the
    westward journey on the first day: Distance westward = 80 miles Next, let’s find
    the distance covered during the northward journey on the second day: Since they
    both traveled 150 miles northward, the distance covered during the second day
    is: Distance northward = 150 miles Now, we can find the total distance covered
    by each train in two days by adding the distances covered during each leg of the
    journey: Total distance = Distance westward + Distance northward Total distance
    = 80 miles + 150 miles Total distance = 230 miles So, each train covers a total
    distance of 230 miles in two days. SelfControl Answer: 230'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 230）：两列火车同时从圣拉斐尔出发。它们开始向西行驶，均行驶了80英里。第二天，它们向北行驶，覆盖了150英里。两天内每列火车行驶的距离是多少？原始输出：要解决这个问题，我们需要使用相对速度的概念，即两列火车相对彼此的速度。假设第一列火车的速度是x英里每小时，第二列火车的速度是x
    + 50英里每小时。第一天，两列火车都向西行驶80英里，因此第一列火车行驶了80英里。第二列火车的行驶距离也是80英里，因为它们的速度相同。第二天，两列火车都向北行驶150英里。第一列火车行驶的距离是150英里
    + 80英里 = 230英里。第二列火车的行驶距离也是230英里。因此，两天内每列火车的总行驶距离是：第一列火车：80 + 230 = 310英里 第二列火车：80
    + 230 = 310英里 因此，两天内每列火车的行驶距离相同，为310英里。原始答案：310 自我控制：要找出每列火车在两天内的总行驶距离，我们需要计算每段旅程中的距离，然后将它们加在一起。首先，找出第一天向西行驶的距离：向西距离
    = 80英里 接下来，找出第二天向北行驶的距离：由于它们都向北行驶了150英里，第二天的行驶距离是：向北距离 = 150英里 现在，我们可以通过将每段旅程的距离相加来找到每列火车在两天内的总行驶距离：总距离
    = 向西距离 + 向北距离 总距离 = 80英里 + 150英里 总距离 = 230英里 所以，每列火车在两天内的总行驶距离是230英里。自我控制答案：230
- en: 'Table 38: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 表38：从Reasoning Benchmark.SelfControl中提取的示例
- en: 'Query (Right Answer = 109): Darrell and Allen’s ages are in the ratio of 7:11\.
    If their total age now is 162, calculate Allen’s age 10 years from now. Original
    Output: Let’s assume Darrell’s age is D years and Allen’s age is A years. Since
    the ratio of their ages is 7:11, we can write: D = 7A We are given that the total
    age of Darrell and Allen now is 162 years. So, we can write: D + A = 162 Now,
    you want to know Allen’s age 10 years from now. If Allen’s age is A years now,
    his age 10 years from now will be: A + 10 = 162 Solving for A, we get: A = 152
    So, Allen’s age 10 years from now is 152 years. Original Answer: 152 SelfControl:
    Let Darrell’s age be 7x and Allen’s age be 11x. We know that their total age now
    is 162, so: 7x + 11x = 162 18x = 162 x = 9 So Darrell is currently $7*9=\langle\langle
    7*9=63\rangle\rangle$. SelfControl Answer: 109'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 109）：达雷尔和艾伦的年龄比是7:11。如果他们现在的总年龄是162岁，计算10年后艾伦的年龄。原始输出：假设达雷尔的年龄是D岁，艾伦的年龄是A岁。由于他们的年龄比是7:11，我们可以写成：D
    = 7A 我们知道达雷尔和艾伦现在的总年龄是162岁。因此，我们可以写成：D + A = 162 现在，你想知道艾伦10年后的年龄。如果艾伦现在是A岁，那么他10年后的年龄将是：A
    + 10 = 162 解这个方程得到：A = 152 所以，艾伦10年后的年龄是152岁。原始答案：152 自我控制：设达雷尔的年龄为7x，艾伦的年龄为11x。我们知道他们现在的总年龄是162岁，所以：7x
    + 11x = 162 18x = 162 x = 9 所以达雷尔现在是 $7*9=\langle\langle 7*9=63\rangle\rangle$。自我控制答案：109
- en: 'Table 39: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 表39：来自 Reasoning Benchmark.SelfControl 的示例
- en: 'Query (Right Answer = 23): Siobhan has 2 fewer jewels than Aaron. Aaron has
    5 more jewels than half of Raymond’s jewels. If Raymond has 40 jewels, how many
    jewels does Siobhan have? Original Output: Let’s start by using the information
    that Siobhan has 2 fewer jewels than Aaron. So, if Aaron has x jewels, Siobhan
    has x - 2 jewels. Next, we are told that Aaron has 5 more jewels than half of
    Raymond’s jewels. So, if Raymond has x jewels, Aaron has x + 5 jewels. Now, we
    can use the information that Raymond has 40 jewels to find out how many jewels
    Siobhan has. We know that Siobhan has x - 2 jewels, and we also know that Aaron
    has x + 5 jewels. So, we can set up the equation: Siobhan’s jewels = Aaron’s jewels
    - (Aaron’s jewels / 2) Simplifying the equation, we get: Siobhan’s jewels = x
    - (x + 5) / 2 Solving for Siobhan’s jewels, we get: Siobhan’s jewels = x - 10
    So, Siobhan has 10 jewels. Original Answer: 10 SelfControl: Let’s break this down
    step by step: 1\. Raymond has 40 jewels. 2\. Aaron has 5 more jewels than half
    of Raymond’s jewels, so Aaron has 5 + (1/2 * 40) = 5 + 20 = 25 jewels. 3\. Siobhan
    has 2 fewer jewels than Aaron, so Siobhan has 25 - 2 = 23 jewels. Therefore, Siobhan
    has 23 jewels. SelfControl Answer: 23'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 查询（正确答案 = 23）：Siobhan 比 Aaron 少 2 个珠宝。Aaron 比 Raymond 珠宝数量的一半多 5 个。如果 Raymond
    有 40 个珠宝，Siobhan 有多少个珠宝？原始输出：首先使用信息，Siobhan 比 Aaron 少 2 个珠宝。所以，如果 Aaron 有 x 个珠宝，Siobhan
    就有 x - 2 个珠宝。接着，我们知道 Aaron 比 Raymond 珠宝数量的一半多 5 个。所以，如果 Raymond 有 x 个珠宝，Aaron
    就有 x + 5 个珠宝。现在，我们可以使用 Raymond 有 40 个珠宝的信息来找出 Siobhan 的珠宝数量。我们知道 Siobhan 有 x -
    2 个珠宝，我们也知道 Aaron 有 x + 5 个珠宝。所以，我们可以建立方程：Siobhan 的珠宝 = Aaron 的珠宝 - (Aaron 的珠宝
    / 2) 化简方程，我们得到：Siobhan 的珠宝 = x - (x + 5) / 2 解方程，我们得到：Siobhan 的珠宝 = x - 10 所以，Siobhan
    有 10 个珠宝。原始答案：10 SelfControl：我们一步一步来：1. Raymond 有 40 个珠宝。2. Aaron 比 Raymond 珠宝数量的一半多
    5 个，所以 Aaron 有 5 + (1/2 * 40) = 5 + 20 = 25 个珠宝。3. Siobhan 比 Aaron 少 2 个珠宝，所以
    Siobhan 有 25 - 2 = 23 个珠宝。因此，Siobhan 有 23 个珠宝。SelfControl 答案：23
- en: 'Table 40: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 表40：来自 Reasoning Benchmark.SelfControl 的示例
- en: 'Query (Right Answer = 26): Grandma Jones baked 5 apple pies for the fireman’s
    luncheon. She cut each pie into 8 pieces and set the five pies out on the buffet
    table for the guests to serve themselves. At the end of the evening, after the
    guests had taken and eaten their pieces of pie, there were 14 pieces of pie remaining.
    How many pieces were taken by the guests? Original Output: To solve this problem,
    we need to use the information given in the problem to find out how many pieces
    of pie Grandma Jones baked. The problem states that she baked 5 apple pies. Next,
    we need to find out how many pieces each pie was cut into. The problem states
    that each pie was cut into 8 pieces. So, if Grandma Jones baked 5 apple pies and
    each pie was cut into 8 pieces, then the total number of pieces of pie that were
    available for the guests to serve themselves is: 5 pies x 8 pieces per pie = 40
    pieces of pie Now, we need to find out how many pieces of pie were left over after
    the guests had taken some. The problem states that there were 14 pieces of pie
    remaining after the guests had taken their pieces. So, the number of pieces of
    pie taken by the guests is: 14 pieces remaining - 40 pieces available = -26 pieces
    Since there are no negative numbers of pie, the number of pieces of pie taken
    by the guests is 0. Therefore, no pieces of pie were taken by the guests. Original
    Answer: 0 SelfControl: To find out how many pieces were taken by the guests, we
    need to subtract the number of remaining pieces from the total number of pieces
    that were originally available. First, let’s find out how many pieces were in
    each pie after Grandma Jones cut it into 8 pieces. Number of pieces per pie =
    8 Now, let’s find out how many total pieces were available after Grandma Jones
    baked and cut up all 5 pies. Total pieces = Number of pieces per pie * Number
    of pies Total pieces = 8 * 5 Total pieces = 40 At the end of the evening, there
    were 14 pieces remaining. So, the number of pieces taken by the guests is the
    difference between the total number of pieces and the number of remaining pieces.
    Number of pieces taken = Total pieces - Remaining pieces Number of pieces taken
    = 40 - 14 Number of pieces taken = 26 So, the guests took 26 pieces of apple pie.
    SelfControl Answer: 26'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 问题（正确答案 = 26）：琼斯奶奶为消防员的午餐烤了5个苹果派。她将每个派切成8块，然后将五个派放在自助餐桌上供客人自取。晚会结束时，客人吃掉了他们的派块后，剩下14块。客人拿走了多少块派？
    原始输出：要解决这个问题，我们需要利用问题中提供的信息来找出琼斯奶奶烤了多少块派。问题中说她烤了5个苹果派。接下来，我们需要找出每个派被切成了多少块。问题中说每个派切成了8块。因此，如果琼斯奶奶烤了5个苹果派，并且每个派切成了8块，那么客人自取的总派块数为：5个派
    x 8块/派 = 40块派 现在，我们需要找出在客人拿走一些后剩下了多少块派。问题中说客人拿走了派块后剩下14块。因此，客人拿走的派块数是：14块剩余 -
    40块可用 = -26块 由于没有负数的派块，客人拿走的派块数为0。因此，没有派块被客人拿走。 原始答案：0 自我控制：为了找出客人拿走了多少块派，我们需要从原始总块数中减去剩余的块数。首先，让我们找出每个派在琼斯奶奶切成8块后有多少块。每个派的块数
    = 8 现在，让我们找出琼斯奶奶烤并切好所有5个派后的总块数。总块数 = 每个派的块数 * 派的数量 总块数 = 8 * 5 总块数 = 40 晚会结束时，剩下14块。因此，客人拿走的派块数是总块数与剩余块数之间的差异。拿走的块数
    = 总块数 - 剩余块数 拿走的块数 = 40 - 14 拿走的块数 = 26 所以，客人拿走了26块苹果派。 自我控制答案：26
- en: 'Table 41: Example taken from Reasoning Benchmark.SelfControl'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 表41：取自推理基准。自我控制
- en: 'Query (Right Answer = 30): Richard lives in an apartment building with 15 floors.
    Each floor contains 8 units, and 3/4 of the building is occupied. What’s the total
    number of unoccupied units In the building? Original Output: To find the total
    number of unoccupied units in the building, we can use the information given in
    the problem: The building has 15 floors. Each floor contains 8 units. 3/4 of the
    building is occupied, which means that 3/4 of the total number of units in the
    building are occupied. So, if we let x be the total number of units in the building,
    we can set up the following equation: x = 15 x 8/4 = 120 Now, we know that 3/4
    of the building is occupied, so the number of occupied units is: Occupied units
    = x / 4 = 120 / 4 = 30 Therefore, the total number of unoccupied units in the
    building is: Unoccupied units = x - Occupied units = 120 - 30 = 90. So, there
    are 90 unoccupied units in the building. Original Answer: 90 SelfControl: Let’s
    break down the problem step by step: 1\. The building has 15 floors. 2\. Each
    floor contains 8 units, so the total number of units in the building is 15 * 8
    = 120 units. 3\. Three-quarters of the building is occupied, which means that
    120 * 3/4 = 90 units are occupied. 4\. To find the number of unoccupied units,
    we subtract the number of occupied units from the total number of units: 120 -
    90 = 30 units. So, there are 30 unoccupied units in the building. SelfControl
    Answer: 30'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 问题（正确答案 = 30）：理查德住在一栋有 15 层的公寓楼里。每层有 8 个单元，且建筑物的 3/4 被占用。建筑物中未占用的单元总数是多少？ 原始输出：要找出建筑物中未占用的单元总数，我们可以利用题目中给出的信息：建筑物有
    15 层。每层有 8 个单元。建筑物的 3/4 被占用，这意味着建筑物总单元数的 3/4 被占用。因此，如果我们设 x 为建筑物中的单元总数，我们可以建立以下方程：x
    = 15 x 8/4 = 120 现在，我们知道建筑物的 3/4 被占用，因此被占用的单元数为：占用单元 = x / 4 = 120 / 4 = 30 因此，建筑物中未占用的单元总数为：未占用单元
    = x - 占用单元 = 120 - 30 = 90。所以，建筑物中有 90 个未占用的单元。 原始答案：90 自我控制：让我们一步步解析这个问题： 1.
    建筑物有 15 层。 2. 每层有 8 个单元，所以建筑物中的单元总数是 15 * 8 = 120 个单元。 3. 建筑物的四分之三被占用，这意味着 120
    * 3/4 = 90 个单元被占用。 4. 要找出未占用的单元数，我们从总单元数中减去占用单元数：120 - 90 = 30 个单元。所以，建筑物中有 30
    个未占用的单元。 自我控制答案：30
- en: Appendix F Pseudo-code
  id: totrans-379
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 F 伪代码
- en: Below are pseudo-code for calculating suffix score (Algorithm [3](#alg3 "In
    Appendix F Pseudo-code ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by Compressing
    Suffix Gradient into Prefix Controller")), getting suffix gradients (Algorithm [1](#alg1
    "In Appendix F Pseudo-code ‣ Part I Appendix ‣ Self-Control of LLM Behaviors by
    Compressing Suffix Gradient into Prefix Controller")), and searching step-sizes
    (Algorithm [2](#alg2 "In Appendix F Pseudo-code ‣ Part I Appendix ‣ Self-Control
    of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller"))
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是计算后缀得分的伪代码（算法 [3](#alg3 "在附录 F 伪代码 ‣ 第一部分 附录 ‣ 通过将后缀梯度压缩到前缀控制器来控制 LLM 行为")），获取后缀梯度的伪代码（算法 [1](#alg1
    "在附录 F 伪代码 ‣ 第一部分 附录 ‣ 通过将后缀梯度压缩到前缀控制器来控制 LLM 行为")），以及搜索步长的伪代码（算法 [2](#alg2 "在附录
    F 伪代码 ‣ 第一部分 附录 ‣ 通过将后缀梯度压缩到前缀控制器来控制 LLM 行为")）。
- en: 'def  get_suffix_grads(wrapped_model,query,  response,  suffix_list,target,  token_pos,  token_neg):  ->  Dict[FloatTensor]final_grads  =  None#  We  can  simultaneously  use  multiple  suffixes  for  controlfor  suffix  in  suffix_list:with  torch.enable_grad():#  The  model  controlled  with  suffix  gradientsoutputs  =  wrapped_model((query  +  response  +  suffix),output_hidden_states=True,)#  calculate  the  lossloss  =  -get_suffix_score(query+response,  suffix,  ...)for  i  in  range(len(hidden_states)):grads[i]  =  torch.autograd.grad(loss,  hidden_states[i],  retain_graph=True,  \allow_unused=True)[0]norms[i]  =  torch.norm(grads[i],  dim=-1,  p=2,  keepdim=True)grads[i]  =  grads[i]  /  (norms[i]  +  1e-12)  #  gradient  clippingfinal_grads  +=  grads#  The  gradients  from  each  suffix  are  averagedfinal_grads  /=  len(suffix_lis)return  gradsdef  iterative_controlled_generate(query,  suffix,  targetmax_iter  #  max  iterations  of  control):  ->  stracc_grads  =  None#  we  control  on  the  hidden  states  at  positions  of  query  tokensquery_len  =  len(tokenizer.encode(query,  add_special_tokens=False))for  iter  in  range(max_iter):#  sample  a  response  with  the  current  gradient  (E-step)wrapped_model  =  control_on_layers(acc_grads,  query_len,  ...)#  wrapped_model.suffix_decoding  if  using  suffix  decodingresponse  =  wrapped_model.generate(query)#  gradient  calculation  (M-step)grads  =  get_suffix_grads(query,  response,  suffix,  target,  ...)#  determine  the  step  sizestep_size  =  search_step_size(acc_grads,  grads,  ...)if  step_size  ==  0:breakacc_grads  +=  step_size  *  grads#  generate  final  responsewrapped_model  =  control_on_layers(acc_grads,  ...)final_response  =  wrapped_model.generate(query)return  final_response'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 'def  get_suffix_grads(wrapped_model, query, response, suffix_list, target,
    token_pos, token_neg):  ->  Dict[FloatTensor] final_grads = None # 我们可以同时使用多个后缀进行控制
    for suffix in suffix_list: with torch.enable_grad(): # 模型在后缀梯度控制下 outputs = wrapped_model((query
    + response + suffix), output_hidden_states=True,) # 计算损失 loss = -get_suffix_score(query+response,
    suffix, ...) for i in range(len(hidden_states)): grads[i] = torch.autograd.grad(loss,
    hidden_states[i], retain_graph=True, \allow_unused=True)[0] norms[i] = torch.norm(grads[i],
    dim=-1, p=2, keepdim=True) grads[i] = grads[i] / (norms[i] + 1e-12)  # 梯度裁剪 final_grads
    += grads # 每个后缀的梯度被平均 final_grads /= len(suffix_list) return grads def iterative_controlled_generate(query,
    suffix, target, max_iter  # 控制的最大迭代次数):  ->  str acc_grads = None # 我们在查询令牌的位置对隐藏状态进行控制
    query_len = len(tokenizer.encode(query, add_special_tokens=False)) for iter in
    range(max_iter): # 使用当前梯度采样响应 (E-step) wrapped_model = control_on_layers(acc_grads,
    query_len, ...) # wrapped_model.suffix_decoding  如果使用后缀解码 response = wrapped_model.generate(query)
    # 梯度计算 (M-step) grads = get_suffix_grads(query, response, suffix, target, ...)
    # 确定步长 step_size = search_step_size(acc_grads, grads, ...) if step_size == 0:
    break acc_grads += step_size * grads # 生成最终响应 wrapped_model = control_on_layers(acc_grads,
    ...) final_response = wrapped_model.generate(query) return final_response'
- en: Algorithm 1 Python Pseudocode of SelfControl (get_suffix_grads and iterative_controlled_generate)
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 自我控制的 Python 伪代码（get_suffix_grads 和 iterative_controlled_generate）
- en: 'def  search_step_size(query,  suffix,  targetinitial_score,  #  The  initial  suffix  score  of  an  responseacc_grads,  #  The  gradients  accumulated  from  previous  iterationsgrads,  #  suffix  gradients  from  current  stepmax_iter,  initial_step_size,  scale_factorscore_threshold,  #  The  threshold  for  a  better  step-size):  ->  float  #  The  final  step  sizecurrent_step_size  =  initial_step_sizefor  i  in  range(max_iter):temp_grads  =  acc_grads  +  current_step_size  *  grads#  get  the  model  controlled  by  the  gradientswrapped_model  =  control_on_layers(layer_ids=layer_ids,wrapped_model=wrapped_model,grads=temp_grads,query_length=query_length,  #  only  control  on  input  query)response  =  wrapped_model.generate(prompt)score  =  get_suffix_score(prompt  =  query  +  response,suffix  =  suffix,...  #  model,  tokenizer,  target,  tau  and  contrastive_pairs  are  the  same)#  return  if  current  score  is  larger  than  the  initial  score  by  the  thresholdif  score  -  initial_score  >  score_threshold:return  current_step_sizecurrent_step_size  *=  scale_factor#  no  better  score  has  been  foundreturn  0'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '`search_step_size`函数用于搜索合适的步长，该函数接受`query`、`suffix`、`target`、`initial_score`、`acc_grads`、`grads`、`max_iter`、`initial_step_size`、`scale_factor`和`score_threshold`作为输入，并返回最终的步长`float`值。'
- en: Algorithm 2 Python Pseudocode of SelfControl (search_step_size)
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 Python伪代码：SelfControl（search_step_size）
- en: 'def  get_suffix_score(prompt,  suffix,  #  prompt  refers  to  [query,  resposne]model,  tokenizer,  target,tau,  #  temperaturecontrastive_pairs=["Yes",  "No"]  #  The  pair  is  (Yes,  No)  by  default  in  our  case):  ->  float  #  suffix  scoretarget_token  =  tokenizer.encode(target,  add_special_tokens=False)[0]token_pos  =  tokenizer.encode(contrastive_pairs[0],  add_special_tokens=False)[0]token_neg  =  tokenizer.encode(contrastive_pairs[1],  add_special_tokens=False)[0]#  append  suffix  to  the  promptstokenized  =  tokenizer(prompt  +  suffix,  return_tensors="pt",  padding=True)#  get  logitswith  torch.no_grad():outputs  =  model(**tokenized)#  calculate  suffix  scorelast_logit  =  outputs.logits[:,  -1,  :]logit_diff  =  last_logit[:,  token_pos]  -  last_logit[:,  token_neg]return  sigmoid(logit_diff  /  tau)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '`get_suffix_score`函数计算后缀得分，该函数接受`prompt`、`suffix`作为输入，`target`、`tau`和`contrastive_pairs`作为参数，并返回一个`float`类型的得分。'
- en: Algorithm 3 Python Pseudocode of SelfControl (get_suffix_score)
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 算法3 Python伪代码：SelfControl（get_suffix_score）
