- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:52:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:52:24'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression
    Perspective
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从数据压缩角度测量LLMs训练中样本的重要性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14124](https://ar5iv.labs.arxiv.org/html/2406.14124)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14124](https://ar5iv.labs.arxiv.org/html/2406.14124)
- en: Minsang Kim
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Minsang Kim
- en: Korea University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 高丽大学
- en: Dept. of Computer Science and Engr.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程系
- en: South Korea
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国
- en: kmswin1@korea.ac.kr
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: kmswin1@korea.ac.kr
- en: '&Seungjun Baek'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&Seungjun Baek'
- en: Korea University
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 高丽大学
- en: Dept. of Computer Science and Engr.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程系
- en: South Korea
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 韩国
- en: sjbaek@korea.ac.kr
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: sjbaek@korea.ac.kr
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Compute-efficient training of large language models (LLMs) has become an important
    research problem. In this work, we consider data pruning as a method of data-efficient
    training of LLMs, where we take a data compression view on data pruning. We argue
    that the amount of information of a sample, or the achievable compression on its
    description length, represents its sample importance. The key idea is that, less
    informative samples are likely to contain redundant information, and thus should
    be pruned first. We leverage log-likelihood function of trained models as a surrogate
    to measure information content of samples. Experiments reveal a surprising insight
    that information-based pruning can enhance the generalization capability of the
    model, improves upon language modeling and downstream tasks as compared to the
    model trained on the entire dataset.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的计算高效训练已成为一个重要的研究问题。在这项工作中，我们将数据修剪视为一种数据高效训练LLMs的方法，从数据压缩的角度来看待数据修剪。我们认为，一个样本的信息量或其描述长度的可压缩性代表了样本的重要性。关键思想是，信息量较少的样本可能包含冗余信息，因此应首先进行修剪。我们利用训练模型的对数似然函数作为代理来测量样本的信息内容。实验揭示了一个令人惊讶的发现，即基于信息的修剪可以增强模型的泛化能力，相较于在整个数据集上训练的模型，它在语言建模和下游任务中有了改进。
- en: 1 Introduction & Related Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍与相关工作
- en: Training large language models (LLM) requires high computational costs due to
    their immense sizes and commensurately large datasets Kaplan et al. [[2020](#bib.bib1)],
    Hoffmann et al. [[2022](#bib.bib2)]. Recently, computationally efficient methods
    to train deep models based on *data pruning* have gained interest. Data pruning
    concerns deciding which samples are important for training, and remove unimportant
    samples from training datasets. Sorscher et al. [[2022](#bib.bib3)] proposed that
    neural scaling law Kaplan et al. [[2020](#bib.bib1)] may be overcome with data
    pruning. Their method uses distances between samples and their cluster centroids
    in the embedding space to measure the sample importance. Tan et al. [[2024](#bib.bib4)]
    proposed to assess the sample importance by efficiently measuring the change in
    empirical risk when the sample is removed from the training set. The aforementioned
    works apply data pruning to labeled data for supervised learning or focus on visual
    understanding tasks. By contrast, data pruning methods for LLMs have been largely
    underexplored. Instead, text deduplication methods have been proposed  Raffel
    et al. [[2020](#bib.bib5)], Abbas et al. [[2023](#bib.bib6)] to remove redundant
    data based on semantic similarity. However, they do not quantify the importance
    of samples for data pruning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型语言模型（LLM）由于其庞大的规模和相应的大型数据集而需要高计算成本（Kaplan et al. [[2020](#bib.bib1)]，Hoffmann
    et al. [[2022](#bib.bib2)]）。近年来，基于*数据修剪*的计算高效方法引起了兴趣。数据修剪涉及决定哪些样本对训练重要，并从训练数据集中移除不重要的样本。Sorscher
    et al. [[2022](#bib.bib3)] 提出神经网络缩放法则（Kaplan et al. [[2020](#bib.bib1)]）可能通过数据修剪得到解决。他们的方法使用样本与其聚类中心在嵌入空间中的距离来衡量样本的重要性。Tan
    et al. [[2024](#bib.bib4)] 提出了通过有效测量样本从训练集中移除后的经验风险变化来评估样本的重要性。上述工作将数据修剪应用于有标签数据进行监督学习或关注视觉理解任务。相比之下，LLMs的数据修剪方法尚未得到充分探索。相反，文本去重方法（Raffel
    et al. [[2020](#bib.bib5)]，Abbas et al. [[2023](#bib.bib6)]）被提出以基于语义相似性去除冗余数据。然而，它们未量化样本在数据修剪中的重要性。
- en: Meanwhile, connections between LLMs and data compression have been recently
    explored. Ge et al. [[2023](#bib.bib7)] proposed In-Context Autoencoder for context
    compression. Deletang et al. [[2023](#bib.bib8)] extensively study the view of
    LLM as a model for data compression. The predictive power of LLMs can be used
    for an optimally efficient expression of data based on the prediction probability,
    so as to compress various types of multi-modal data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，LLMs与数据压缩之间的关系最近得到了探讨。Ge等人[[2023](#bib.bib7)]提出了用于上下文压缩的In-Context Autoencoder。Deletang等人[[2023](#bib.bib8)]广泛研究了LLM作为数据压缩模型的观点。LLMs的预测能力可以用于基于预测概率的最优数据表达，从而压缩各种类型的多模态数据。
- en: 'In this paper, we propose to leverage the compression capability of LLMs for
    data pruning. If a model learns the distribution of corpus, it can estimate the
    amount of information of a sample, i.e., the optimal number of bits required to
    compress the sample. Importantly, if the amount of information of a sample is
    low, the sample likely contains information which is redundant or appear frequently
    in dataset. Less informative samples are regarded as less important and thus are
    removed from dataset. To estimate the information of samples, we first train called
    *data probe model* which is a small model trained with a subset of corpus. We
    use the log-likelihood output of data probe model as a surrogate to measure the
    sample information. Next, we prune the dataset based on the estimates by data
    probe model, and train a target model with the pruned dataset. Experimental results
    are quite surprising: in many cases, the proposed pruning actually *improves*
    the model performance. In some cases, the performance of generation and/or downstream
    tasks are maintained up to pruning 50% of the pretraining corpus. The key insight
    is that, the proposed pruning based on sample information helps removing irrelevant
    data, which promotes the generalization capability of language models.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出利用LLMs的压缩能力进行数据剪枝。如果一个模型学习了语料库的分布，它可以估计样本的信息量，即压缩样本所需的最佳位数。重要的是，如果样本的信息量低，则该样本可能包含在数据集中冗余或频繁出现的信息。信息量较少的样本被认为是不那么重要的，因此从数据集中移除。为了估计样本的信息量，我们首先训练一个称为*数据探测模型*的小型模型，该模型使用语料库的一个子集进行训练。我们使用数据探测模型的对数似然输出作为测量样本信息的替代指标。接下来，我们根据数据探测模型的估计修剪数据集，并用修剪后的数据集训练目标模型。实验结果非常惊人：在许多情况下，提出的剪枝实际上*提高*了模型性能。在某些情况下，生成和/或下游任务的性能保持在剪枝50%预训练语料库的水平。关键见解是，基于样本信息的剪枝有助于去除无关数据，从而提升语言模型的泛化能力。
- en: 2 Method
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: 2.1 Compression View to Data Pruning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 压缩视角下的数据剪枝
- en: We take a data compression view to data pruning for language models as follows.
    Let $p(\cdot)$, consider
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从数据压缩的角度来看待语言模型的数据剪枝如下。设$p(\cdot)$，考虑
- en: '|  | $H(W,p)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{p(w_{i}&#124;w_{<i})}$ |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $H(W,p)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{p(w_{i}&#124;w_{<i})}$ |  |'
- en: We view $H(W,p)$ averaged over sequence length, which is our main metric for
    sample importance, is given by
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将$H(W,p)$对序列长度取平均，这是我们评估样本重要性的主要指标，如下所示
- en: '|  | $\displaystyle H(W,q)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{q(w_{i}&#124;w_{<i})}$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle H(W,q)=\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{q(w_{i}&#124;w_{<i})}$
    |  | (1) |'
- en: From an information theoretic view, ([1](#S2.E1 "In 2.1 Compression View to
    Data Pruning ‣ 2 Method ‣ Measuring Sample Importance in Data Pruning for Training
    LLMs from a Data Compression Perspective")) represents the description length
    of words in $W$ as fixed, and ([1](#S2.E1 "In 2.1 Compression View to Data Pruning
    ‣ 2 Method ‣ Measuring Sample Importance in Data Pruning for Training LLMs from
    a Data Compression Perspective")) can be regarded as being proportional to the
    negative log-likelihood or log-loss.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从信息理论的角度看，([1](#S2.E1 "在2.1 压缩视角下的数据剪枝 ‣ 2 方法 ‣ 从数据压缩角度衡量训练LLMs中的样本重要性"))表示$W$中单词的描述长度是固定的，而([1](#S2.E1
    "在2.1 压缩视角下的数据剪枝 ‣ 2 方法 ‣ 从数据压缩角度衡量训练LLMs中的样本重要性"))可以被视为与负对数似然或对数损失成正比。
- en: We hypothesize that, if $q$ in an average sense. Indeed, we have
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设，如果$q$从平均意义上看，实际上我们得到
- en: '|  | $\displaystyle\mathcal{H}(p)$ |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{H}(p)$ |  |'
- en: '|  | $\displaystyle\mathcal{H}(p,q)$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{H}(p,q)$ |  |'
- en: where $\mathcal{H}(p)$, respectively. The cross-entropy is an upper bound of
    entropy Cover [[1999](#bib.bib13)], i.e.,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{H}(p)$，交叉熵是熵的上界[[1999](#bib.bib13)]，即，
- en: '|  | $\mathcal{H}(p)\leq\mathcal{H}(p,q)$ |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{H}(p)\leq\mathcal{H}(p,q)$ |  |'
- en: That is, we always need extra bits to encode words with distribution $p$ is
    used for encoding.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即，我们总是需要额外的比特来编码分布$p$用于编码的词。
- en: Thus, if $q$ values from the dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果从数据集中获得$q$值。
- en: Another view is that, one can show that the *perplexity* of $W$ results in low
    perplexity or “surprisal”, where the surprisal of a sample is a term used for
    the information content of the sample in an information theoretic context.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种观点是，可以证明$W$的*困惑度*结果为低困惑度或“惊讶度”，其中样本的惊讶度是信息论背景下用来描述样本信息内容的术语。
- en: 2.2 Pruning Method
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 剪枝方法
- en: Overview. We first train reference model $q$ from the dataset. The pruned dataset
    is used for training the target LLM.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 概述。我们首先训练参考模型$q$来自数据集。修剪后的数据集用于训练目标LLM。
- en: Data probe model. We train a data probe model denoted by $q$ provide a good
    bound on the description lengths. We heuristically determine the subsample size
    by hypothesizing that the model is sufficiently trained if the rate of decrease
    in the training loss saturates, similar to the early stopping method.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探测模型。我们训练一个数据探测模型，记作$q$，以提供描述长度的良好界限。我们通过假设模型在训练损失减少的速度趋于饱和时已经充分训练，类似于早期停止方法，来启发式地确定子样本大小。
- en: 'As a result, the subsample size is set to be about 12% of training dataset:
    see Fig. [1](#S2.F1 "Figure 1 ‣ 2.2 Pruning Method ‣ 2 Method ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
    Later experiments show that the proposed level of training is sufficient for a
    good pruning performance.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，子样本大小设置为训练数据集的约12%：见图 [1](#S2.F1 "图 1 ‣ 2.2 剪枝方法 ‣ 2 方法 ‣ 从数据压缩角度测量训练LLMs的数据剪枝中的样本重要性")。后续实验表明，提出的训练水平足以实现良好的剪枝性能。
- en: Pruning. Next, we perform pruning on the dataset $\mathcal{D}$. Use the pruned
    dataset to train the LLM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝。接下来，我们对数据集$\mathcal{D}$进行剪枝。使用修剪后的数据集训练LLM。
- en: '![Refer to caption](img/24c45d70173767ee1e8b8c0bb661d52b.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/24c45d70173767ee1e8b8c0bb661d52b.png)'
- en: 'Figure 1: Loss curve of GPT-125M over 1 sweep of the training dataset. For
    data efficiency, we stopped training the probe model at the point where the decrease
    in loss saturates, i.e., at about 12% of the entire dataset.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：GPT-125M在训练数据集1次遍历中的损失曲线。为了提高数据效率，我们在损失减少趋于饱和的点停止了探测模型的训练，即大约12%的整个数据集。
- en: 3 Experiment
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Experimental Settings
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: As pretraining corpus, we use a subset of c4 Raffel et al. [[2020](#bib.bib5)]
    via random sampling, which is a total of about 3 billion tokens from GPT-2 tokenizer Radford
    et al. [[2019](#bib.bib14)]. We use decoder-only transformers Vaswani et al. [[2017](#bib.bib15)]
    using GPT-2 tokenizer as the language model. As the data probe model, we use a
    125M-parameter model trained on 0.3 billion tokens randomly sampled from the total
    corpus. As the target model, we use both 125M- and 345M-parameter models. We evaluate
    the language modeling of target models on One billion words Chelba et al. [[2014](#bib.bib16)]
    and wikitext-103 Merity et al. [[2016](#bib.bib17)] where the models are pre-trained
    on different datasets. The downstream tasks for the target models are evaluated
    using glue benchmark Wang et al. [[2018](#bib.bib18)]. Detailed hyperparameters
    are in Appendix [A.1](#A1.SS1 "A.1 Hyperparameters ‣ Appendix A Appendix ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 作为预训练语料库，我们使用c4 Raffel et al. [[2020](#bib.bib5)]的一个子集，通过随机抽样获得，总计约30亿个来自GPT-2分词器Radford
    et al. [[2019](#bib.bib14)]的标记。我们使用仅解码器的transformers Vaswani et al. [[2017](#bib.bib15)]，采用GPT-2分词器作为语言模型。作为数据探测模型，我们使用一个125M参数的模型，训练在从总语料库随机抽样的3亿个标记上。作为目标模型，我们使用125M和345M参数的模型。我们在One
    billion words Chelba et al. [[2014](#bib.bib16)]和wikitext-103 Merity et al. [[2016](#bib.bib17)]上评估目标模型的语言建模，模型在不同的数据集上进行预训练。目标模型的下游任务通过glue
    benchmark Wang et al. [[2018](#bib.bib18)]进行评估。详细的超参数见附录 [A.1](#A1.SS1 "A.1 超参数
    ‣ 附录 A 附录 ‣ 从数据压缩角度测量训练LLMs的数据剪枝中的样本重要性")。
- en: 3.2 Pretraining Loss with Pruning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 剪枝的预训练损失
- en: Fig [2](#S3.F2 "Figure 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment
    ‣ Measuring Sample Importance in Data Pruning for Training LLMs from a Data Compression
    Perspective") shows the test loss of the 345M target model during pretraining.
    Interestingly, pruning 10% achieves a *lower* and pruning 20% obtains a similar
    test loss, as compared to no-pruning case. As we will see later, the lower loss
    indeed leads to improved performances in language modeling and downstream tasks
    with pruning.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S3.F2 "图 2 ‣ 3.2 剪枝预训练损失 ‣ 3 实验 ‣ 从数据压缩角度衡量训练 LLM 的数据剪枝样本重要性") 显示了 345M
    目标模型在预训练期间的测试损失。有趣的是，剪枝 10% 达到了 *更低* 的损失，而剪枝 20% 的测试损失与未剪枝情况相似。正如我们稍后会看到的，较低的损失确实在语言建模和剪枝下游任务中带来了改进的性能。
- en: '![Refer to caption](img/208c428685b6f408db52db75945f31fc.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/208c428685b6f408db52db75945f31fc.png)'
- en: 'Figure 2: Test loss of 345M GPT per training token.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：每个训练 token 的 345M GPT 测试损失。
- en: Pruning % Proposed Random High Entropy 0 90.23 10 89.32 90.95 98.47 20 86.88
    93.40 100.06 30 87.09 99.89 109.51 40 84.19 93.73 107.48 50 91.11 97.08 104.39
    60 91.24 101.32 103.30 70 102.93 104.65 152.94 80 101.70 110.39 181.16 90 147.18
    165.39 284.59
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝 % 提议 随机 高熵 0 90.23 10 89.32 90.95 98.47 20 86.88 93.40 100.06 30 87.09 99.89
    109.51 40 84.19 93.73 107.48 50 91.11 97.08 104.39 60 91.24 101.32 103.30 70 102.93
    104.65 152.94 80 101.70 110.39 181.16 90 147.18 165.39 284.59
- en: 'Table 1: Perplexity of 125M per pruning ratio on One Billion Words corpus.
    Bold denotes the best performance for each pruning ratio, and the underline indicates
    better performances than no-pruning case.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 One Billion Words 语料库上，125M 每个剪枝比率的困惑度。**粗体**表示每个剪枝比率的最佳性能，**下划线**表示比未剪枝情况更好的性能。
- en: Pruning % Proposed Random High Entropy 0 52.23 10 50.59 51.56 54.71 20 51.71
    52.25 57.99 30 53.46 54.97 61.35 40 55.06 56.91 71.03 50 58.26 59.63 74.50 60
    61.97 64.29 85.91 70 62.66 70.22 97.87 80 71.46 85.42 126.69 90 115.03 135.34
    212.67
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝 % 提议 随机 高熵 0 52.23 10 50.59 51.56 54.71 20 51.71 52.25 57.99 30 53.46 54.97
    61.35 40 55.06 56.91 71.03 50 58.26 59.63 74.50 60 61.97 64.29 85.91 70 62.66
    70.22 97.87 80 71.46 85.42 126.69 90 115.03 135.34 212.67
- en: 'Table 2: Perplexity of 125M per pruning ratio on wikitext-103 corpus. Bold
    denotes the best performance for each pruning ratio, and the underline indicates
    better performances than no-pruning case.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 wikitext-103 语料库上，125M 每个剪枝比率的困惑度。**粗体**表示每个剪枝比率的最佳性能，**下划线**表示比未剪枝情况更好的性能。
- en: 3.3 Language Modeling
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 语言建模
- en: 'We evaluate language modeling of target models using two test corpus. Table [1](#S3.T1
    "Table 1 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the perplexity of each model per pruning ratio on One Billion Words corpus
    Chelba et al. [[2014](#bib.bib16)]. The proposed pruning significantly outperforms
    the random pruning. Surprisingly, the proposed pruning achieves *better* performance
    up to 40% pruning ratio, and on-par performance up to 60% ratio, as compared to
    no-pruning case. We observe a similar trend with different corpus: Table [2](#S3.T2
    "Table 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the perplexity results of language modeling on wikitext-103 corpus Merity
    et al. [[2016](#bib.bib17)]. The results show that, our pruning can actually improve
    the generalization capability of the target model, which we explain as follows.
    Our pruning removes samples that are less informative or contain highly redundant
    information. Such redundancy in the dataset may cause overfitting such samples
    similar to duplication, i.e., the model may get biased towards such information,
    hampering generalization. Thus, pruning based on estimates of information content
    of samples may alleviate overfitting problems. For comparison, we tested removing
    samples with high $H(\cdot,q)$ first: see “High Entropy” columns in Table [1](#S3.T1
    "Table 1 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample
    Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    and [2](#S3.T2 "Table 2 ‣ 3.2 Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective").
    Interestingly, it performs much worse than random pruning, which conversely shows
    the effectiveness of the proposed importance metric of samples.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两个测试语料库评估目标模型的语言建模。表格 [1](#S3.T1 "Table 1 ‣ 3.2 Pretraining Loss with Pruning
    ‣ 3 Experiment ‣ Measuring Sample Importance in Data Pruning for Training LLMs
    from a Data Compression Perspective") 显示了每个模型在 One Billion Words 语料库 Chelba 等人
    [[2014](#bib.bib16)] 上的困惑度与修剪比例的关系。所提议的修剪显著优于随机修剪。令人惊讶的是，所提议的修剪在高达 40% 的修剪比例下实现了
    *更好的* 性能，而在 60% 的比例下达到了与不修剪相当的性能。我们在不同语料库中观察到类似的趋势：表格 [2](#S3.T2 "Table 2 ‣ 3.2
    Pretraining Loss with Pruning ‣ 3 Experiment ‣ Measuring Sample Importance in
    Data Pruning for Training LLMs from a Data Compression Perspective") 显示了 wikitext-103
    语料库 Merity 等人 [[2016](#bib.bib17)] 上的语言建模困惑度结果。结果表明，我们的修剪确实能提高目标模型的泛化能力，我们的解释如下。我们的修剪去除那些信息较少或包含高度冗余信息的样本。这种数据集中的冗余可能导致样本过拟合类似于重复，即模型可能会对这些信息产生偏见，从而妨碍泛化。因此，基于样本信息内容的估计的修剪可能缓解过拟合问题。为了比较，我们首先测试了去除高
    $H(\cdot,q)$ 的样本：见表 [1](#S3.T1 "Table 1 ‣ 3.2 Pretraining Loss with Pruning ‣
    3 Experiment ‣ Measuring Sample Importance in Data Pruning for Training LLMs from
    a Data Compression Perspective") 和 [2](#S3.T2 "Table 2 ‣ 3.2 Pretraining Loss
    with Pruning ‣ 3 Experiment ‣ Measuring Sample Importance in Data Pruning for
    Training LLMs from a Data Compression Perspective") 中的“高熵”列。有趣的是，这种方法的表现远不如随机修剪，这反而显示了所提议的样本重要性度量的有效性。
- en: 3.4 Downstream tasks
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 下游任务
- en: '![Refer to caption](img/9ee76f8ff6318976ce1fdefddb546c86.png)![Refer to caption](img/a2255d5f50bf07299cd1a0281e82da7b.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ee76f8ff6318976ce1fdefddb546c86.png)![参见说明](img/a2255d5f50bf07299cd1a0281e82da7b.png)'
- en: 'Figure 3: Text classification results on CoLA dataset in glue benchmark. Left:
    125M, Right: 345M. The green line indicates the performance without pruning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：CoLA 数据集在 glue 基准测试中的文本分类结果。左图：125M，右图：345M。绿色线表示未修剪的性能。
- en: '![Refer to caption](img/2191298b2bdcfc89a684dd0c635e6052.png)![Refer to caption](img/d4037c76a3a25fd110736ca40693b1ae.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2191298b2bdcfc89a684dd0c635e6052.png)![参见说明](img/d4037c76a3a25fd110736ca40693b1ae.png)'
- en: 'Figure 4: Textual similarity results on SST2 dataset in glue benchmark. Left:
    125M, Right: 345M. The green line indicates the performance without pruning.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：SST2 数据集在 glue 基准测试中的文本相似性结果。左图：125M，右图：345M。绿色线表示未修剪的性能。
- en: We finetune and evaluate each model in downstream tasks such as text classification
    and textual similarity tasks from glue Wang et al. [[2018](#bib.bib18)]. The left
    of Fig. [3](#S3.F3 "Figure 3 ‣ 3.4 Downstream tasks ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    shows the text classification results of 125M models on CoLA dataset, which compare
    between the proposed method and random pruning. The results show that, the accuracy
    of our method is always above that of the random pruning. Next, we scaled up the
    target model to 345M, and the right of Fig. [3](#S3.F3 "Figure 3 ‣ 3.4 Downstream
    tasks ‣ 3 Experiment ‣ Measuring Sample Importance in Data Pruning for Training
    LLMs from a Data Compression Perspective") shows that the proposed data pruning
    method achieves significant performance gains over random pruning. Notably, the
    proposed pruning outperforms no-pruning case with almost up to 50% pruning ratio,
    i.e., pruning helps improving the performance of language models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下游任务中对每个模型进行了微调和评估，例如文本分类和文本相似性任务，数据来自glue Wang等人[[2018](#bib.bib18)]。图[3](#S3.F3
    "图 3 ‣ 3.4 下游任务 ‣ 3 实验 ‣ 从数据压缩的角度测量样本重要性")左侧展示了125M模型在CoLA数据集上的文本分类结果，比较了所提出的方法和随机修剪。结果显示，我们的方法的准确率始终高于随机修剪。接下来，我们将目标模型扩展到345M，图[3](#S3.F3
    "图 3 ‣ 3.4 下游任务 ‣ 3 实验 ‣ 从数据压缩的角度测量样本重要性")右侧显示，提出的数据修剪方法在随机修剪上取得了显著的性能提升。特别是，所提出的修剪方法在几乎达到50%的修剪比率时超越了无修剪情况，即修剪有助于提升语言模型的性能。
- en: Fig. [4](#S3.F4 "Figure 4 ‣ 3.4 Downstream tasks ‣ 3 Experiment ‣ Measuring
    Sample Importance in Data Pruning for Training LLMs from a Data Compression Perspective")
    illustrates the results of the textual similarity task on SST2 dataset for 125M
    and 345M models. For both models, while random pruning continually decreases performances,
    the proposed method achieves similar or even better accuracy than no-pruning case
    up to 50% pruning ratio.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S3.F4 "图 4 ‣ 3.4 下游任务 ‣ 3 实验 ‣ 从数据压缩的角度测量样本重要性")展示了125M和345M模型在SST2数据集上的文本相似性任务结果。对于这两种模型，尽管随机修剪会不断降低性能，但提出的方法在高达50%的修剪比率下仍能实现与无修剪情况相似甚至更好的准确率。
- en: 'Overall, we make a similar observation to pretraining case: the proposed pruning
    not only reduces the training cost of language models, but also *improves* the
    performance of downstream tasks. We conclude that pruning based on the information
    content of samples can improve the generalization capabilities of the models on
    downstream tasks.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们做出了与预训练情况类似的观察：所提出的修剪方法不仅降低了语言模型的训练成本，而且*提升*了下游任务的性能。我们得出结论，基于样本信息内容的修剪可以提高模型在下游任务中的泛化能力。
- en: 4 Conclusion
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: In this paper, we took an information-theoretic view of compression for data
    pruning in training language models. We use the log-likelihood function of the
    model to estimate the compressed description length of a sample, or its informativeness.
    Pruning based on the proposed estimate of sample importance enables removing samples
    with redundant information, which not only reduces computational costs, but also,
    surprisingly, enhances the generalization capability of language models. Experiments
    with various corpus and tasks validated the effectiveness of the proposed pruning.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们从信息理论的角度审视了数据修剪中的压缩问题。我们使用模型的对数似然函数来估计样本的压缩描述长度，即其信息量。基于提出的样本重要性估计进行修剪，可以去除包含冗余信息的样本，这不仅降低了计算成本，而且令人惊讶的是，提升了语言模型的泛化能力。对各种语料库和任务的实验验证了所提出修剪方法的有效性。
- en: References
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan等人[2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei. 神经语言模型的规模定律。*arXiv预印本
    arXiv:2001.08361*，2020年。
- en: Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models.
    *arXiv preprint arXiv:2203.15556*, 2022.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann等人[2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, 等人. 训练计算最优的大型语言模型。*arXiv预印本 arXiv:2203.15556*，2022年。
- en: 'Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling
    via data pruning. *Advances in Neural Information Processing Systems*, 35:19523–19536,
    2022.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, 和 Ari Morcos。超越神经网络规模定律：通过数据修剪击败幂律缩放。*神经信息处理系统进展*，35:19523–19536，2022年。
- en: Tan et al. [2024] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan
    Wang, and Xiaojuan Qi. Data pruning via moving-one-sample-out. *Advances in Neural
    Information Processing Systems*, 36, 2024.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. [2024] Haoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin Wang, Fan
    Wang, 和 Xiaojuan Qi。通过逐样本移除进行数据修剪。*神经信息处理系统进展*，36，2024年。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of machine learning research*, 21(140):1–67, 2020.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu。通过统一的文本到文本变换器探索迁移学习的极限。*机器学习研究杂志*，21(140):1–67，2020年。
- en: 'Abbas et al. [2023] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig,
    Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale
    through semantic deduplication. In *ICLR 2023 Workshop on Mathematical and Empirical
    Understanding of Foundation Models*, 2023.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbas et al. [2023] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig,
    Surya Ganguli, 和 Ari S Morcos。Semdedup：通过语义去重进行数据高效学习。 在*ICLR 2023数学和经验理解基础模型研讨会*上，2023年。
- en: Ge et al. [2023] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, and Furu
    Wei. In-context autoencoder for context compression in a large language model.
    In *The Twelfth International Conference on Learning Representations*, 2023.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023] Tao Ge, Hu Jing, Lei Wang, Xun Wang, Si-Qing Chen, 和 Furu Wei。用于大语言模型上下文压缩的上下文自编码器。在*第十二届国际学习表征会议*上，2023年。
- en: Deletang et al. [2023] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, et al. Language modeling is compression. In
    *The Twelfth International Conference on Learning Representations*, 2023.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deletang et al. [2023] Gregoire Deletang, Anian Ruoss, Paul-Ambroise Duquenne,
    Elliot Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
    Matthew Aitchison, Laurent Orseau, 等。语言建模即压缩。在*第十二届国际学习表征会议*上，2023年。
- en: Huffman [1952] David A Huffman. A method for the construction of minimum-redundancy
    codes. *Proceedings of the IRE*, 40(9):1098–1101, 1952.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huffman [1952] David A Huffman。最小冗余编码的构造方法。*IRE会议记录*，40(9):1098–1101，1952年。
- en: Rissanen [1976] Jorma J Rissanen. Generalized kraft inequality and arithmetic
    coding. *IBM Journal of research and development*, 20(3):198–203, 1976.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rissanen [1976] Jorma J Rissanen。广义克拉夫特不等式和算术编码。*IBM研究与开发杂志*，20(3):198–203，1976年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等。Llama 2：开放基础和微调聊天模型。*arXiv预印本 arXiv:2307.09288*，2023年。
- en: Cover [1999] Thomas M Cover. *Elements of information theory*. John Wiley &
    Sons, 1999.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cover [1999] Thomas M Cover。*信息理论要素*。John Wiley & Sons，1999年。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等。语言模型是无监督的多任务学习者。*OpenAI博客*，1(8):9，2019年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin。注意力即你所需。*神经信息处理系统进展*，30，2017年。
- en: Chelba et al. [2014] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten
    Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring
    progress in statistical language modeling. *Interspeech 2014*, 2014.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chelba 等 [2014] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten
    Brants, Phillipp Koehn 和 Tony Robinson。用于测量统计语言建模进展的十亿字基准。*Interspeech 2014*，2014。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. In *International Conference on Learning
    Representations*, 2016.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 [2016] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher。指针哨兵混合模型。发表于
    *国际学习表示大会*，2016。
- en: 'Wang et al. [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,
    Omer Levy, and Samuel Bowman. Glue: A multi-task benchmark and analysis platform
    for natural language understanding. In *Proceedings of the 2018 EMNLP Workshop
    BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP*, pages 353–355,
    2018.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2018] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer
    Levy 和 Samuel Bowman。Glue: 自然语言理解的多任务基准和分析平台。发表于 *2018 EMNLP Workshop BlackboxNLP:
    分析和解释 NLP 神经网络*，第 353–355 页，2018。'
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma 和 Ba [2014] Diederik P Kingma 和 Jimmy Ba。Adam: 一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014。'
- en: Appendix A Appendix
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Hyperparameters
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 超参数
- en: Hyperparameter Value Model size 125M, 345M Pretrain learning rate 5e-4 Pretrain
    Learning rate scheduler Cosine Pretrain warmup ratio 0.01 % Pretrain weight decay
    0.1 Pretrain batch size 64 Sequence length 1024 Finetuning learning rate 2e-5
    Finetuning weight decay 0.01 Finetuning batch size 32 Optimizer Adam Kingma and
    Ba [[2014](#bib.bib19)]
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数 值 模型大小 125M，345M 预训练学习率 5e-4 预训练学习率调度器 余弦 预训练预热比 0.01% 预训练权重衰减 0.1 预训练批量大小
    64 序列长度 1024 微调学习率 2e-5 微调权重衰减 0.01 微调批量大小 32 优化器 Adam Kingma 和 Ba [[2014](#bib.bib19)]
- en: 'Table 3: Detailed hyperparameters.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：详细的超参数。
