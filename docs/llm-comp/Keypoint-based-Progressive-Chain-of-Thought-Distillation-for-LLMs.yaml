- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:05:54'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:05:54
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Keypoint-based Progressive Chain-of-Thought Distillation for LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于关键点的逐步链式思维蒸馏方法用于大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16064](https://ar5iv.labs.arxiv.org/html/2405.16064)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.16064](https://ar5iv.labs.arxiv.org/html/2405.16064)
- en: Kaituo Feng    Changsheng Li    Xiaolu Zhang    Jun Zhou    Ye Yuan    Guoren
    Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kaituo Feng    Changsheng Li    Xiaolu Zhang    Jun Zhou    Ye Yuan    Guoren
    Wang
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Chain-of-thought distillation is a powerful technique for transferring reasoning
    abilities from large language models (LLMs) to smaller student models. Previous
    methods typically require the student to mimic the step-by-step rationale produced
    by LLMs, often facing the following challenges: (i) Tokens within a rationale
    vary in significance, and treating them equally may fail to accurately mimic keypoint
    tokens, leading to reasoning errors. (ii) They usually distill knowledge by consistently
    predicting all the steps in a rationale, which falls short in distinguishing the
    learning order of step generation. This diverges from the human cognitive progression
    of starting with easy tasks and advancing to harder ones, resulting in sub-optimal
    outcomes. To this end, we propose a unified framework, called KPOD, to address
    these issues. Specifically, we propose a token weighting module utilizing mask
    learning to encourage accurate mimicry of keypoint tokens by the student during
    distillation. Besides, we develop an in-rationale progressive distillation strategy,
    starting with training the student to generate the final reasoning steps and gradually
    extending to cover the entire rationale. To accomplish this, a weighted token
    generation loss is proposed to assess step reasoning difficulty, and a value function
    is devised to schedule the progressive distillation by considering both step difficulty
    and question diversity. Extensive experiments on four reasoning benchmarks illustrate
    our KPOD outperforms previous methods by a large margin.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 链式思维蒸馏是一种强大的技术，用于将推理能力从大型语言模型（LLMs）转移到较小的学生模型。以往的方法通常要求学生模拟LLMs产生的逐步推理，这常面临以下挑战：（i）推理中的令牌具有不同的重要性，平等对待可能无法准确模拟关键点令牌，导致推理错误。（ii）它们通常通过一致预测推理中的所有步骤来提取知识，这在区分步骤生成的学习顺序方面显得不足。这与人类认知从简单任务开始并逐步进阶的过程不同，导致结果次优。为此，我们提出了一个统一的框架，称为KPOD，以解决这些问题。具体而言，我们提出了一个令牌加权模块，利用掩码学习鼓励学生在蒸馏过程中准确模拟关键点令牌。此外，我们开发了一种逐步链式蒸馏策略，从训练学生生成最终推理步骤开始，逐渐扩展到涵盖整个推理过程。为实现这一目标，提出了一种加权令牌生成损失来评估步骤推理难度，并设计了一个价值函数来根据步骤难度和问题多样性调度逐步蒸馏。四个推理基准上的广泛实验表明，我们的KPOD方法显著优于以往的方法。
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have demonstrated remarkable reasoning capabilities
    via chain-of-thought (CoT) prompting (e.g., “Let’s think step-by-step”), which
    prompts LLMs to generate a step-by-step rationale to help reasoning (Kojima et al.,
    [2022](#bib.bib23); Wei et al., [2022](#bib.bib44)). However, such abilities usually
    emerge in extremely large models, especially those with over 100 billion parameters
    (Fu et al., [2023](#bib.bib14); Hoffmann et al., [2022](#bib.bib17)) , such as
    175B GPT-3 (Brown et al., [2020](#bib.bib5)) and 540B PaLM (Chowdhery et al.,
    [2023](#bib.bib7)). The substantial amount of parameters unavoidably leads to
    high inference costs and makes it challenging to deploy LLMs in environments with
    limited computational resources (Hsieh et al., [2023](#bib.bib18)). To tackle
    with this, a recent surge of works, known as CoT distillation, has arisen as a
    promising avenue to distill reasoning capabilities of LLMs to smaller student
    models (Li et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41); Fu et al.,
    [2023](#bib.bib14)). The core idea of these methods is to require the student
    model to mimic the step-by-step rationale generated by LLMs in response to a question.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过链式推理（CoT）提示展示了显著的推理能力（例如，“让我们一步一步思考”），这促使LLMs生成逐步推理以帮助推理（Kojima
    et al., [2022](#bib.bib23); Wei et al., [2022](#bib.bib44)）。然而，这些能力通常出现在极其庞大的模型中，特别是那些参数超过1000亿的模型（Fu
    et al., [2023](#bib.bib14); Hoffmann et al., [2022](#bib.bib17)），例如175B GPT-3（Brown
    et al., [2020](#bib.bib5)）和540B PaLM（Chowdhery et al., [2023](#bib.bib7)）。大量的参数不可避免地导致高推理成本，并使得在计算资源有限的环境中部署LLMs具有挑战性（Hsieh
    et al., [2023](#bib.bib18)）。为了解决这个问题，最近涌现了一些被称为CoT蒸馏的工作，这些工作作为一种有前景的途径来将LLMs的推理能力蒸馏到更小的学生模型中（Li
    et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41); Fu et al., [2023](#bib.bib14)）。这些方法的核心思想是要求学生模型模仿LLMs在回答问题时生成的逐步推理过程。
- en: 'However, current CoT distillation methods often encounter the following two
    issues: First, in a rationale, each token carries different levels of importance
    in the reasoning process. Certain keypoint tokens play a pivotal role in reasoning,
    while other tokens are of less importance or even irrelevant to the reasoning
    process. For instance, consider a step in a rationale: “Next, we just need to
    simply add up the calories from the lettuce and cucumber: 30 + 80 = 110”. Here,
    terms like “just”, “simply” are reasoning-irrelevant, whereas the calculation
    “30 + 80 = 110” stands out as the keypoint for reasoning. The reasoning-irrelevant
    tokens can be replaced without negative effects, but even a slight deviation from
    the keypoint token could result in errors in reasoning. Therefore, it’s crucial
    for the student model to focus on the precise mimicry of these keypoint tokens.
    Nevertheless, previous CoT distillation methods usually treat all tokens equally
    during distillation (Li et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当前的CoT蒸馏方法常常遇到以下两个问题：首先，在推理过程中，每个词元在推理中具有不同的重要性。一些关键点词元在推理中起着至关重要的作用，而其他词元则不那么重要，甚至与推理过程无关。例如，考虑一个推理步骤：“接下来，我们只需简单地将生菜和黄瓜的卡路里加起来：30
    + 80 = 110”。在这里，像“只需”，“简单地”这样的词与推理无关，而计算“30 + 80 = 110”则是推理的关键点。与推理无关的词元可以被替换而不会产生负面影响，但即使是微小的偏离关键点词元也可能导致推理错误。因此，学生模型必须专注于精确模仿这些关键点词元。然而，之前的CoT蒸馏方法通常在蒸馏过程中对所有词元一视同仁（Li
    et al., [2023](#bib.bib26); Wang et al., [2023b](#bib.bib41)）。
- en: 'The second issue stems from the fact that previous approaches usually demand
    the student model to consistently learn all the steps in a rationale throughout
    the distillation process, without distinguishing the learning order of step generation.
    This distillation strategy diverges from the human cognitive pattern that progresses
    from easier tasks to more challenging ones. This deviation might lead to sub-optimal
    outcomes. In the process of human or biological agent learning, ability acquisition
    doesn’t simply stem from random tasks (Molina & Jouen, [1998](#bib.bib32)). Instead,
    there is an organized progression from easy tasks to hard tasks for them to acquire
    capabilities, especially for complex skills such as reasoning (Peterson, [2004](#bib.bib36);
    Krueger & Dayan, [2009](#bib.bib25); Benoit et al., [2013](#bib.bib2)). In the
    field of machine learning, this ordered learning paradigm is regarded as curriculum
    learning (Bengio et al., [2009](#bib.bib1)). Inspired by this, we intend to develop
    a progressive CoT distillation strategy to facilitate the student model acquire
    reasoning ability from easy to hard. However, directly applying previous curriculum
    learning strategies to CoT distillation could be inferior because of the following
    two reasons: (i) They overlook the step-by-step reasoning nature where each reasoning
    step within a rationale may possess varying reasoning difficulty, resulting in
    sub-optimal difficulty assessment. (ii) As aforementioned, a step in the rationale
    might contain many tokens that are not crucial to the reasoning process. When
    assessing the difficulty of step generation, it may be dominated by these inessential
    tokens, thereby inaccurately reflecting the challenge of obtaining the expected
    outcome for a reasoning step.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个问题源于之前的方法通常要求学生模型在整个蒸馏过程中始终学习所有步骤的理由，而不区分步骤生成的学习顺序。这种蒸馏策略与人类认知模式的差异较大，人类认知模式是从较简单的任务逐步过渡到更具挑战性的任务。这种偏差可能导致次优结果。在人类或生物体学习的过程中，能力的获取并不是简单地来自随机任务（Molina
    & Jouen，[1998](#bib.bib32)）。相反，他们会从简单任务逐步过渡到难度较大的任务，以获取能力，尤其是像推理这样的复杂技能（Peterson，[2004](#bib.bib36)；Krueger
    & Dayan，[2009](#bib.bib25)；Benoit et al.，[2013](#bib.bib2)）。在机器学习领域，这种有序学习模式被称为课程学习（Bengio
    et al.，[2009](#bib.bib1)）。受到此启发，我们打算开发一种渐进的CoT蒸馏策略，以帮助学生模型从简单到困难地获得推理能力。然而，直接将以前的课程学习策略应用于CoT蒸馏可能效果较差，原因有以下两点：（i）这些策略忽视了逐步推理的性质，即理由中的每一步可能具有不同的推理难度，导致难度评估不够准确。（ii）如前所述，理由中的一步可能包含许多对推理过程不关键的标记。在评估步骤生成的难度时，这些不重要的标记可能占据主导地位，从而不准确地反映出推理步骤获得预期结果的挑战。
- en: 'In this paper, we propose Keypoint-based Progressive CoT Distillation for LLMs
    dubbed KPOD, with the goal of addressing the above two issues in a unified framework.
    First, we propose a rationale token weighting module to determine the token significance
    for distillation. It learns to generate masks for inessential tokens to the reasoning
    process via two distinctive loss functions: An answer prediction loss is introduced
    to encourage the module to utilize the question with the masked rationale to derive
    the answer, while a mask ratio loss is designed to maximize the ratio of masked
    tokens in the rationale. By doing so, the obtained probability of not masking
    a token can serve as an indicator of its significance weight. Second, we develop
    an in-rationale progressive distillation strategy that orders the learning sequence
    from easy reasoning to hard reasoning within the rationale of a question. This
    strategy begins by training the student model to generate the last few reasoning
    steps of the rationale, given the question with preceding steps of this rationale
    as input. Subsequently, it progressively extends to generate the entire rationale
    using only the question as input. To precisely assess each step’s reasoning difficulty,
    we propose a token generation loss based on the derived token significance, aiming
    to eliminate the negative effects of reasoning-irrelevant tokens. Finally, we
    design a value function to dynamically determine the number of steps taken as
    input at each stage, thereby automatically adjusting their learning difficulty.
    Meanwhile, we leverage the value function to select diverse questions, so as to
    prevent over-fitting (Jiang et al., [2014](#bib.bib22); Liang et al., [2021](#bib.bib28)).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了基于关键点的逐步 CoT 蒸馏方法，称为 KPOD，旨在以统一的框架解决上述两个问题。首先，我们提出了一个 rationale token
    权重模块，以确定蒸馏过程中 token 的重要性。它通过两个不同的损失函数学习生成掩码，来对推理过程中不必要的 token 进行掩码处理：引入了答案预测损失以鼓励该模块利用带有掩码
    rationale 的问题来推导答案，而掩码比率损失旨在最大化 rationale 中掩码 token 的比率。通过这种方式，未掩码 token 的概率可以作为其重要性权重的指标。其次，我们开发了一种逐步的蒸馏策略，该策略在问题的
    rationale 内将学习顺序从简单推理到困难推理进行排序。该策略开始时训练学生模型生成 rationale 的最后几个推理步骤，输入为带有前面步骤的 rationale
    的问题。随后，它逐步扩展到仅使用问题作为输入生成整个 rationale。为了准确评估每个步骤的推理难度，我们提出了一种基于导出的 token 重要性的生成损失，旨在消除与推理无关的
    token 的负面影响。最后，我们设计了一个价值函数，以动态确定每个阶段的输入步骤数量，从而自动调整其学习难度。同时，我们利用价值函数选择多样化的问题，以防止过拟合（Jiang
    et al., [2014](#bib.bib22); Liang et al., [2021](#bib.bib28)）。
- en: 'Our contributions can be summarized as: 1) We propose a general and principled
    framework for CoT distillation, which simultaneously considers token significance
    and reasoning difficulty within a rationale during distillation. 2) We design
    a rationale token weighting module through mask learning to determine the token
    significance for reasoning. This allows the student to concentrate more on keypoint
    tokens. 3) We devise an in-rationale progressive CoT distillation strategy to
    schedule the learning order of reasoning steps within a rationale. This enables
    the student to progressively acquire reasoning abilities in an easy-to-hard manner.
    4) Extensive experiments on four reasoning benchmarks validate the effectiveness
    of our KPOD, showcasing significant performance improvements compared to baselines.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结为：1) 我们提出了一个通用且有原则的 CoT 蒸馏框架，该框架在蒸馏过程中同时考虑了 token 的重要性和推理难度。2) 我们通过掩码学习设计了一个
    rationale token 权重模块，以确定推理的 token 重要性。这使得学生可以更加集中于关键 token。3) 我们设计了一种逐步的 CoT 蒸馏策略，以调度在
    rationale 内的推理步骤的学习顺序。这使学生能够以从易到难的方式逐步获得推理能力。4) 在四个推理基准上的广泛实验验证了我们 KPOD 的有效性，展示了与基线相比显著的性能提升。
- en: 2 Related Works
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Chain-of-Thought Reasoning. The concept of employing step-by-step language rationales
    to aid in solving reasoning problems can be traced back to pioneering works (Ling
    et al., [2017](#bib.bib29)). Inspired by this, chain-of-thought prompting (Wei
    et al., [2022](#bib.bib44)) has been proposed to enable LLMs to generate intermediate
    reasoning steps that contribute to the final answer via few-shot CoT demonstrations.
    This prompting approach has illustrated remarkable performance gain for LLMs in
    reasoning related tasks (Zhang et al., [2022](#bib.bib47); Wang et al., [2023a](#bib.bib40)).
    In addition, researchers find that LLMs can also obtain impressive reasoning performance
    by zero-shot CoT (Kojima et al., [2022](#bib.bib23)) without task-related demonstrations.
    This is achieved by only using a single sentence “Let’s think step by step” for
    prompting. Recently, a number of CoT prompting methods have demonstrated effectiveness
    in enhancing the reasoning performance of LLMs (Diao et al., [2023](#bib.bib10);
    Yang et al., [2023](#bib.bib45)), such as SC-CoT (Wang et al., [2022](#bib.bib42)),
    Auto-CoT (Zhang et al., [2022](#bib.bib47)), Multimodal-CoT (Zhang et al., [2023](#bib.bib48)),
    etc. However, the emergence of CoT reasoning capabilities in LLMs typically requires
    models with more than 100 billion parameters (Wei et al., [2022](#bib.bib44);
    Fu et al., [2023](#bib.bib14)), making it resource-consuming for deployment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 连锁思维推理。使用逐步语言推理帮助解决推理问题的概念可以追溯到开创性的研究（Ling et al., [2017](#bib.bib29)）。受到此启发，链式思维提示（Wei
    et al., [2022](#bib.bib44)）被提出以使大型语言模型（LLMs）通过少量的链式思维示例生成中间推理步骤，从而对最终答案做出贡献。这种提示方法在与推理相关的任务中对LLMs展示了显著的性能提升（Zhang
    et al., [2022](#bib.bib47); Wang et al., [2023a](#bib.bib40)）。此外，研究人员发现LLMs也可以通过零-shot链式思维（Kojima
    et al., [2022](#bib.bib23)）在没有任务相关示例的情况下获得令人印象深刻的推理表现。这是通过仅使用一句话“让我们一步一步思考”来提示实现的。最近，许多链式思维提示方法（如SC-CoT（Wang
    et al., [2022](#bib.bib42)）、Auto-CoT（Zhang et al., [2022](#bib.bib47)）、Multimodal-CoT（Zhang
    et al., [2023](#bib.bib48)）等）已证明在提升LLMs推理性能方面的有效性。然而，LLMs中链式思维推理能力的出现通常需要超过1000亿参数的模型（Wei
    et al., [2022](#bib.bib44); Fu et al., [2023](#bib.bib14)），使得部署过程资源消耗巨大。
- en: 'CoT Distillation. Knowledge distillation has been widely studied for model
    compression across various fields (Magister et al., [2023](#bib.bib30); Feng et al.,
    [2024](#bib.bib13)). Recently, CoT Distillation has emerged as a promising avenue
    to transfer the step-by-step reasoning capabilities of LLMs to smaller student
    models (Hsieh et al., [2023](#bib.bib18); Ho et al., [2023](#bib.bib16)). The
    key idea of CoT distillation is to make the student model mimic the step-by-step
    rationale generated by LLMs in response to a question. In this context, the rationale
    can be interpreted as the LLMs’ explanation of how to derive the final answer
    of a question, akin to the soft label used in conventional knowledge distillation
    (Hinton et al., [2015](#bib.bib15); Feng et al., [2022](#bib.bib12)). The representative
    works of CoT distillation include: SCoTD (Li et al., [2023](#bib.bib26)) introduces
    a symbolic CoT distillation method that enables smaller models to self-rationalize
    for reasoning via learning rationales from LLMs. Specialized KD (Fu et al., [2023](#bib.bib14))
    is proposed to train a small language model specialized for reasoning in four
    distinct in-context scenarios. MCC-KD (Chen et al., [2023](#bib.bib6)) adopts
    diverse rationales for distillation and attempts to ensure their consistency.
    SCOTT (Wang et al., [2023b](#bib.bib41)) designs a faithful CoT distillation strategy
    to make the student reason faithfully via counterfactual training. However, these
    methods fail to consider the reasonable learning order of the reasoning steps
    within a rationale, leading to sub-optimal performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 蒸馏。知识蒸馏在各种领域的模型压缩中得到了广泛研究（Magister et al.，[2023](#bib.bib30)；Feng et al.，[2024](#bib.bib13)）。最近，CoT
    蒸馏作为将LLMs的逐步推理能力转移到较小的学生模型中的一种有前景的途径出现（Hsieh et al.，[2023](#bib.bib18)；Ho et al.，[2023](#bib.bib16)）。CoT蒸馏的核心思想是使学生模型模仿LLMs在回答问题时生成的逐步推理。在这种情况下，推理可以被解释为LLMs对如何推导出问题最终答案的解释，类似于传统知识蒸馏中使用的软标签（Hinton
    et al.，[2015](#bib.bib15)；Feng et al.，[2022](#bib.bib12)）。CoT蒸馏的代表性工作包括：SCoTD（Li
    et al.，[2023](#bib.bib26)）引入了一种符号化的CoT蒸馏方法，使得较小的模型能够通过学习LLMs的推理来进行自我推理。Specialized
    KD（Fu et al.，[2023](#bib.bib14)）被提出用于训练一个专门针对四种不同上下文场景的推理的小型语言模型。MCC-KD（Chen et
    al.，[2023](#bib.bib6)）采用多样的推理进行蒸馏，并尝试确保它们的一致性。SCOTT（Wang et al.，[2023b](#bib.bib41)）设计了一种忠实的CoT蒸馏策略，通过反事实训练使学生模型忠实地进行推理。然而，这些方法未考虑推理步骤中的合理学习顺序，导致性能次优。
- en: Curriculum Learning. Early researches in cognitive science emphasize the significance
    of the easy-to-hard learning pattern to acquire knowledge (Elman, [1993](#bib.bib11)).
    Inspired by this, the pioneer work (Bengio et al., [2009](#bib.bib1)) introduces
    the concept of curriculum learning (CL) to the machine learning field by gradually
    including samples from easy to hard for training. In recent years, a variety of
    CL methods have been proposed to enhance the model performance (Kong et al., [2021](#bib.bib24);
    Wang et al., [2021](#bib.bib43)). For instance, Adaptive CL (Kong et al., [2021](#bib.bib24))
    proposes to utilize the loss of the model to dynamically adjust the difficulty
    score of each sample. SPL (Wan et al., [2020](#bib.bib39)) introduces the curriculum
    learning to the neural machine translation domain via introducing the token-level
    and sentence-level confidence score. ICL (Jia et al., [2023](#bib.bib21)) devises
    a curriculum learning method that organizes the curriculum within the token sequence
    of a sample for natural language generation tasks. However, as aforementioned,
    applying these CL methods directly to CoT distillation could yield inferior performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习。早期的认知科学研究强调了从易到难的学习模式在知识获取中的重要性（Elman，[1993](#bib.bib11)）。受此启发，开创性工作（Bengio
    et al.，[2009](#bib.bib1)）将课程学习（CL）概念引入了机器学习领域，通过逐步包括从易到难的样本进行训练。近年来，提出了各种CL方法来提高模型性能（Kong
    et al.，[2021](#bib.bib24)；Wang et al.，[2021](#bib.bib43)）。例如，Adaptive CL（Kong
    et al.，[2021](#bib.bib24)）建议利用模型的损失来动态调整每个样本的难度评分。SPL（Wan et al.，[2020](#bib.bib39)）通过引入令牌级和句子级信心评分将课程学习引入了神经机器翻译领域。ICL（Jia
    et al.，[2023](#bib.bib21)）设计了一种在样本的令牌序列中组织课程学习的方法，用于自然语言生成任务。然而，如前所述，直接将这些CL方法应用于CoT蒸馏可能会导致性能不佳。
- en: 3 Proposed Method
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提议的方法
- en: '![Refer to caption](img/78ba807db61bed67d4bb4cbae9dab17c.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/78ba807db61bed67d4bb4cbae9dab17c.png)'
- en: 'Figure 1: An illustration of our KPOD framework. KPOD first determines the
    keypoint tokens for distillation through designing a rationale token weighting
    module based on mask learning. Then, an in-rationale progressive distillation
    strategy is devised to organize the learning order within rationale, so as to
    enable the student to acquire the reasoning capabilities in an easy-to-hard manner.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们KPOD框架的示意图。KPOD首先通过设计基于掩码学习的推理标记加权模块来确定关键点标记用于蒸馏。然后，设计了一个逐步推理蒸馏策略来组织推理中的学习顺序，使学生能够以易到难的方式掌握推理能力。
- en: 3.1 Preliminaries and Problem Setting
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基础和问题设置
- en: 'The goal of CoT distillation is to transfer the reasoning capability of large
    language models (LLMs) to smaller student models via distilling the rationales
    produced by LLMs. We denote the dataset as $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$
    as input. The standard negative log-likelihood loss for training the student model
    can be formulated as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CoT蒸馏的目标是通过蒸馏LLMs生成的推理，将大语言模型（LLMs）的推理能力转移到较小的学生模型上。我们将数据集记作 $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$
    作为输入。用于训练学生模型的标准负对数似然损失可以表述为：
- en: '|  | $\displaystyle\mathcal{L}\!$ |  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}\!$ |  |'
- en: '|  |  | $1$2 |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (1) |'
- en: where $r_{j}^{(i)}$ denotes the parameters of the student model. The first term
    of Eq.([3.1](#S3.Ex1 "3.1 Preliminaries and Problem Setting ‣ 3 Proposed Method
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs")) enables
    the student to mimic the rationale produced by LLMs, while the second term aims
    to train the student to output the final answer based on the rationale. By minimizing
    this loss, the student model can learn to generate the step-by-step rationale
    for deriving the final answer.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{j}^{(i)}$ 表示学生模型的参数。方程([3.1](#S3.Ex1 "3.1 Preliminaries and Problem Setting
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs")) 的第一个项使学生能够模仿LLMs生成的推理，而第二个项旨在训练学生基于推理输出最终答案。通过最小化这个损失，学生模型可以学习生成逐步推理来推导最终答案。
- en: 3.2 Framework Overview
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 框架概述
- en: 'As aforementioned, there are two key issues for CoT distillation methods: (i)
    Equally treating each token for distillation may make the student fail to mimic
    keypoint tokens accurately, leading to reasoning errors. (ii) Distilling the steps
    within a rationale without explicitly considering the learning order of step generation
    might lead to sub-optimal outcomes. To tackle these two issues, we propose a new
    CoT distillation framework KPOD, as illustrated in Figure [1](#S3.F1 "Figure 1
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs"). Our framework mainly consists of two components: a rationale token
    weighting component based on mask learning is proposed to determine the token
    significance for distillation. This encourages the student to faithfully replicate
    the crucial keypoint tokens; A progressive distillation component within the rationale
    is designed to establish a structured learning order for the reasoning steps.
    This guides the student model to progressively develop its reasoning abilities
    from simpler to more complex tasks, aligning with the proficiency of teacher LLMs.
    It’s worth noting that the obtained token significance weight fulfills two distinct
    functions in our framework: firstly, it encourages precise mimicry of keypoint
    tokens during distillation, and secondly, it mitigates the negative effects of
    inessential tokens when assessing step difficulty. Next, we will primarily delve
    into the detailed introduction of the two components in our framework.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CoT蒸馏方法存在两个关键问题：（i）对每个标记进行同等处理可能使学生无法准确模仿关键点标记，导致推理错误。（ii）在不明确考虑步骤生成学习顺序的情况下蒸馏推理中的步骤可能导致次优结果。为了解决这两个问题，我们提出了一种新的CoT蒸馏框架KPOD，如图
    [1](#S3.F1 "Figure 1 ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs") 所示。我们的框架主要由两个组件组成：一个基于掩码学习的推理标记加权组件用于确定标记在蒸馏中的重要性。这鼓励学生忠实地复制关键点标记；一个推理内的逐步蒸馏组件旨在建立推理步骤的结构化学习顺序。这引导学生模型逐步从简单任务发展到复杂任务的推理能力，符合教师LLMs的熟练度。值得注意的是，获得的标记重要性权重在我们的框架中履行两个不同的功能：首先，它鼓励在蒸馏过程中准确模仿关键点标记；其次，它减轻了评估步骤难度时不必要标记的负面影响。接下来，我们将主要*深入*介绍框架中的两个组件。
- en: 3.3 Rationale Token Weighting
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 推理标记加权
- en: In this section, we introduce our rationale token weighting module, which determines
    the significance of each token via learning to mask reasoning-irrelevant token.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的理性 token 权重模块，该模块通过学习掩盖与推理无关的 token 来确定每个 token 的重要性。
- en: 'Weight Generation. First, we intend to generate distinct significance weights
    for different tokens by leveraging their embeddings. This facilitates the estimation
    of their importance according to their characteristics. To achieve this, we feed
    the rationale tokens into a pre-trained input embedding layer, followed by a self-attention
    layer to encode in-context information. This process is formulated as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 权重生成。首先，我们打算通过利用 token 的嵌入生成不同的显著性权重。这有助于根据它们的特征估计其重要性。为此，我们将理性 token 输入到预训练的输入嵌入层，然后通过自注意力层编码上下文信息。这个过程可以表述为：
- en: '|  | $e^{(i)}=\mathrm{Att}(\mathrm{Emb}(r^{(i)})),$ |  | (2) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $e^{(i)}=\mathrm{Att}(\mathrm{Emb}(r^{(i)})),$ |  | (2) |'
- en: 'where $\mathrm{Emb}$ of each token is fed into a weight generator, producing
    the significance weight as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，每个 token 的 $\mathrm{Emb}$ 被输入到一个权重生成器中，生成的显著性权重为：
- en: '|  | $w_{j}^{(i)}=\sigma(f_{w}(e_{j}^{(i)})),$ |  | (3) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $w_{j}^{(i)}=\sigma(f_{w}(e_{j}^{(i)})),$ |  | (3) |'
- en: where $w_{j}^{(i)}$ is the sigmoid activation function (Narayan, [1997](#bib.bib33)).
    In this paper, we employ a simple two-layer MLP as the weight generator.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{j}^{(i)}$ 是 sigmoid 激活函数（Narayan，[1997](#bib.bib33)）。在本文中，我们采用了一个简单的两层
    MLP 作为权重生成器。
- en: 'Reasoning-irrelevant Mask Learning. To optimize the weight generator, we formulate
    two loss functions: an answer prediction loss that encourages the module to utilize
    the question with the masked rationale for answer derivation, and a mask ratio
    loss aiming to maximize the ratio of masked tokens in the rationale. This allows
    the weight generator to generate low values of $w_{j}^{(i)}$ for tokens irrelevant
    to reasoning and high values for keypoint tokens. Next, we will introduce these
    two losses in detail.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与推理无关的掩码学习。为了优化权重生成器，我们制定了两个损失函数：一个答案预测损失，鼓励模块利用带有掩盖理性的问句来推导答案，以及一个掩码比例损失，旨在最大化理性中掩盖
    token 的比例。这使得权重生成器能够为与推理无关的 token 生成低值 $w_{j}^{(i)}$，而为关键点 token 生成高值。接下来，我们将详细介绍这两个损失函数。
- en: 'Firstly, considering that sampling the discrete mask policy $m_{j}^{(i)}\in\{0,1\}$
    is non-differentiable, we adopt the Gumbel-Softmax sampling (Jang et al., [2016](#bib.bib20))
    to avoid this issue:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，考虑到采样离散掩码策略 $m_{j}^{(i)}\in\{0,1\}$ 是不可微分的，我们采用了 Gumbel-Softmax 采样（Jang 等，[2016](#bib.bib20)）来避免这个问题：
- en: '|  | $m_{j}^{(i)}=\mathrm{GumbelSoftmax}(w_{j}^{(i)}),$ |  | (4) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $m_{j}^{(i)}=\mathrm{GumbelSoftmax}(w_{j}^{(i)}),$ |  | (4) |'
- en: where $\mathrm{GumbelSoftmax}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{GumbelSoftmax}$。
- en: 'Then, we input ${r[m]}^{(i)}$ can be written as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们输入 ${r[m]}^{(i)}$ 可以写作：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $y^{(i)}$, the transformer can be used to predict the answer by taking
    as input the question and the prefix of the masked rationale. Meanwhile, the weight
    generator is encouraged to generate large weights for the keypoint tokens, preventing
    them from being masked to facilitate the answer prediction.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y^{(i)}$，变压器可以通过输入问题和掩盖的理性前缀来预测答案。同时，鼓励权重生成器为关键点 token 生成较大的权重，防止它们被掩盖，以便于答案预测。
- en: 'Moreover, to eliminate redundant tokens for reasoning, a mask ratio loss $\mathcal{L}_{m}$
    is presented as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了消除推理中的冗余 token，提出了一个掩码比例损失 $\mathcal{L}_{m}$：
- en: '|  | $\mathcal{L}_{m}=\sum_{j}m_{j}^{(i)}.$ |  | (6) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{m}=\sum_{j}m_{j}^{(i)}.$ |  | (6) |'
- en: By optimizing $\mathcal{L}_{m}$, we enable the weight generator to identify
    the insignificant tokens in the reasoning process and generate lower weights for
    them.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化 $\mathcal{L}_{m}$，我们使得权重生成器能够识别推理过程中不重要的 token 并为其生成较低的权重。
- en: 'Finally, the overall loss function for training this module can be expressed
    as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，训练该模块的总体损失函数可以表示为：
- en: '|  | $\mathcal{L}_{k}=\mathcal{L}_{p}+\alpha\mathcal{L}_{m},$ |  | (7) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{k}=\mathcal{L}_{p}+\alpha\mathcal{L}_{m},$ |  | (7) |'
- en: where $\alpha$ for each token within a rationale.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 为理性内每个 token。
- en: 3.4 In-rationale Progressive Distillation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 理性进阶蒸馏
- en: In this section, we elaborate our proposed in-rationale progressive distillation
    strategy, which schedules the learning order within a rationale.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细阐述了我们提出的理性进阶蒸馏策略，该策略安排了在理性内的学习顺序。
- en: 'Step Difficulty Assessment. Firstly, we assess the difficulty of each reasoning
    step in the rationale, so as to facilitate the learning order scheduling. In this
    work, we utilize the symbol “.” to separate steps in a rationale. As mentioned
    above, there could exist many reasoning-irrelevant tokens, and it is crucial to
    ensure that the difficulty evaluation is not influenced by them. Therefore, we
    propose a weighted token generation loss to calculate the difficulty value $d_{k}^{(i)}$
    as:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤难度评估。首先，我们评估推理过程每个步骤的难度，以便于学习顺序的调度。在这项工作中，我们利用符号“.”来分隔推理过程中的步骤。如上所述，可能存在许多与推理无关的标记，因此确保难度评估不受其影响至关重要。因此，我们提出了一种加权标记生成损失来计算难度值$d_{k}^{(i)}$，其形式为：
- en: '|  | $1$2 |  | (8) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: where $p_{k}$-th step. In this way, the obtained step difficulty can be more
    concentrated on the difficulty of generating keypoint tokens, providing a more
    faithful reflection of the difficulty in deriving the correct outcome of each
    reasoning step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$p_{k}$-th步骤。通过这种方式，获得的步骤难度可以更集中于生成关键点标记的难度，更真实地反映每个推理步骤产生正确结果的难度。
- en: 'Progressive Distillation. Based on the step difficulty scores, we devise an
    in-rationale progressive distillation strategy to guide the student model learning
    each rationale in an easy-to-hard fashion. This strategy initiates with training
    the student model to generate the final few reasoning steps of the rationale using
    previous steps combined with the question as input, and progressively expands
    to produce the complete rationales. Supposed that we schedule the student model
    to output the last $n_{i}-c_{i}(t)$ of generating these steps can be formulated
    as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进蒸馏。基于步骤难度分数，我们设计了一种逐步推理蒸馏策略，以指导学生模型以由易到难的方式学习每个推理过程。该策略从训练学生模型生成推理过程的最后几个推理步骤开始，使用之前的步骤和问题作为输入，然后逐步扩展以生成完整的推理过程。假设我们安排学生模型输出最后$n_{i}-c_{i}(t)$的步骤，这些步骤的生成可以表示为：
- en: '|  | $h_{i}(S(t))=\sum_{j=c_{i}(t)+1}^{n_{i}}d_{j}^{(i)},$ |  | (9) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $h_{i}(S(t))=\sum_{j=c_{i}(t)+1}^{n_{i}}d_{j}^{(i)},$ |  | (9) |'
- en: where $n_{i}$, which will be introduced later. In this paper, we treat each
    training epoch as a stage.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$n_{i}$，将在后文介绍。在本文中，我们将每个训练周期视为一个阶段。
- en: 'To facilitate selecting diverse questions to increase difficulty at each stage,
    we configure an overall learning difficulty $D(t)$ as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便选择多样化的问题以增加每个阶段的难度，我们配置了一个总体学习难度$D(t)$，其定义为：
- en: '|  | $D(t)=\frac{ut^{p+1}}{p+1}+C_{0},$ |  | (10) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $D(t)=\frac{ut^{p+1}}{p+1}+C_{0},$ |  | (10) |'
- en: where $C_{0}$ are the pre-defined hyper-parameters.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$C_{0}$是预定义的超参数。
- en: 'When entering stage $t$ for the selected questions as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当进入阶段$t$时，对选定的问题进行如下处理：
- en: '|  | $1$2 |  | (11) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: where $c_{i}(t)$ is the ceiling magnitude for the increased difficulty.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$c_{i}(t)$是增加难度的上限。
- en: 'Then, in order to determine whether a question should increase difficulty,
    we design a value function $F$ is designed as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了确定一个问题是否应该增加难度，我们设计了一个值函数$F$，其形式为：
- en: '|  | $1$2 |  | (12) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: where $\beta$ is the selected question set. By using the square root operation,
    our aim is to promote a balanced distribution of questions within each cluster
    in the selected question set. This approach ensures that the diversity of the
    chosen question set is maintained.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\beta$是选定的问题集。通过使用平方根操作，我们的目标是促进选定问题集中问题的平衡分布。这种方法确保了所选问题集的多样性得到保持。
- en: 'The optimization of $F(S(t))$ can be formulated as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $F(S(t))$的优化可以表示为：
- en: '|  | $\max_{S(t)}F(S(t)),\ \ s.t.\Delta H(S(t))\leq\Delta D(t).$ |  | (13)
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{S(t)}F(S(t)),\ \ s.t.\Delta H(S(t))\leq\Delta D(t).$ |  | (13)
    |'
- en: By maximizing $F(S(t))$ satisfies the condition of monotone and submodular.
    Therefore, it can be approximately solved by a submodular maximization algorithm
    FTGP (Li et al., [2022](#bib.bib27)) in linear time with an approximation ratio
    guarantee, as formulated in Proposition [3.1](#S3.Thmtheorem1 "Proposition 3.1\.
    ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs"). The proof of Proposition
    [3.1](#S3.Thmtheorem1 "Proposition 3.1\. ‣ 3.4 In-rationale Progressive Distillation
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs") can be found in Appendix [D](#A4 "Appendix D Proof ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最大化 $F(S(t))$ 满足单调性和次模性条件。因此，可以通过次模最大化算法 FTGP (Li et al., [2022](#bib.bib27))
    在保证近似比的线性时间内近似求解，如命题 [3.1](#S3.Thmtheorem1 "Proposition 3.1\. ‣ 3.4 In-rationale
    Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs") 中所述。命题 [3.1](#S3.Thmtheorem1 "Proposition 3.1\. ‣ 3.4
    In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive
    Chain-of-Thought Distillation for LLMs") 的证明见附录 [D](#A4 "Appendix D Proof ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs")。
- en: Proposition 3.1.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 3.1。
- en: The optimization of $\max_{S(t)}F(S(t))$ represents the scale of the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: $\max_{S(t)}F(S(t))$ 的优化表示数据的规模。
- en: 'After obtaining the scheduled input step $c_{i}(t)$ can be formulated as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得计划输入步骤 $c_{i}(t)$ 后，可以表述为：
- en: '|  | $1$2 |  | (14) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (14) |'
- en: where $p_{c_{i}(t)+1}$. In this way, the student model could learn the rationale
    of each question in an easy-to-hard manner.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{c_{i}(t)+1}$。这样，学生模型可以以易到难的方式学习每个问题的理由。
- en: 3.5 Training Procedure
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 训练程序
- en: 'To train our whole framework, we first optimize the rationale token weighting
    module by Eq.([7](#S3.E7 "Equation 7 ‣ 3.3 Rationale Token Weighting ‣ 3 Proposed
    Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"))
    to determine the token significance. Then, we assess the step difficulty and derive
    the progressive distillation strategy by solving Eq.([13](#S3.E13 "Equation 13
    ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs")). Finally, by integrating
    these two modules, the overall loss for distilling the rationale at stage $t$
    can be written as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们整个框架，我们首先通过公式 ([7](#S3.E7 "Equation 7 ‣ 3.3 Rationale Token Weighting
    ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs")) 优化理由标记加权模块，以确定标记的重要性。然后，我们评估步骤难度，并通过解决公式 ([13](#S3.E13 "Equation 13
    ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs")) 推导渐进蒸馏策略。最后，通过整合这两个模块，阶段
    $t$ 的总体蒸馏损失可以表示为：
- en: '|  | $1$2 |  | (15) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (15) |'
- en: By optimizing $\mathcal{L}_{o}(t)$ (referring to the second term in Eq. ([3.1](#S3.Ex1
    "3.1 Preliminaries and Problem Setting ‣ 3 Proposed Method ‣ Keypoint-based Progressive
    Chain-of-Thought Distillation for LLMs"))), for the sake of clarity, as it remains
    constant. The pseudo-code of our training procedure is listed in Appendix [B](#A2
    "Appendix B Training Pseudo-code ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs").
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化 $\mathcal{L}_{o}(t)$（参见公式 ([3.1](#S3.Ex1 "3.1 Preliminaries and Problem
    Setting ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs"))），为了清晰起见，因为它保持不变。我们的训练过程的伪代码列在附录 [B](#A2 "Appendix B Training Pseudo-code
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs") 中。
- en: 4 Experiments
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Table 1: Performance comparison of our method and baselines.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：我们的方法与基线的性能比较。
- en: '| Models | # Params. | Distillation Methods | Datasets |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 参数数量 | 蒸馏方法 | 数据集 |'
- en: '| GSM8K | ASDiv | SVAMP | CommonsenseQA |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | ASDiv | SVAMP | CommonsenseQA |'
- en: '| GPT-3.5-Turbo | unknown | - | 73.98 | 79.64 | 75.14 | 74.35 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo | unknown | - | 73.98 | 79.64 | 75.14 | 74.35 |'
- en: '| LLaMA-7B | 7B | - | 11.00 | 40.20 | 32.80 | 33.90 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 7B | - | 11.00 | 40.20 | 32.80 | 33.90 |'
- en: '| SCoTD | 38.54 | 63.38 | 62.67 | 71.33 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| SCoTD | 38.54 | 63.38 | 62.67 | 71.33 |'
- en: '| Specialized KD | 39.15 | 64.01 | 63.33 | 72.32 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Specialized KD | 39.15 | 64.01 | 63.33 | 72.32 |'
- en: '| SCOTT | 40.97 | 62.74 | 61.33 | 74.45 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| SCOTT | 40.97 | 62.74 | 61.33 | 74.45 |'
- en: '| MCC-KD | 41.58 | 65.76 | 64.67 | 76.41 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| MCC-KD | 41.58 | 65.76 | 64.67 | 76.41 |'
- en: '| KPOD (ours) | 46.74 | 71.02 | 68.67 | 77.89 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| KPOD (ours) | 46.74 | 71.02 | 68.67 | 77.89 |'
- en: '| FlanT5-XL | 3B | - | 13.50 | 20.70 | 17.70 | 72.70 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XL | 3B | - | 13.50 | 20.70 | 17.70 | 72.70 |'
- en: '| SCoTD | 21.85 | 25.16 | 26.67 | 79.61 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| SCoTD | 21.85 | 25.16 | 26.67 | 79.61 |'
- en: '| Specialized KD | 23.22 | 28.03 | 25.33 | 81.16 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Specialized KD | 23.22 | 28.03 | 25.33 | 81.16 |'
- en: '| SCOTT | 21.09 | 25.48 | 24.67 | 83.62 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SCOTT | 21.09 | 25.48 | 24.67 | 83.62 |'
- en: '| MCC-KD | 24.28 | 31.35 | 30.00 | 82.88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MCC-KD | 24.28 | 31.35 | 30.00 | 82.88 |'
- en: '| KPOD (ours) | 25.19 | 33.76 | 34.67 | 88.04 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| KPOD（我们的方法） | 25.19 | 33.76 | 34.67 | 88.04 |'
- en: '| FlanT5-Large | 760M | - | 6.90 | 10.10 | 6.80 | 67.60 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-Large | 760M | - | 6.90 | 10.10 | 6.80 | 67.60 |'
- en: '| SCoTD | 19.42 | 20.06 | 19.33 | 76.58 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| SCoTD | 19.42 | 20.06 | 19.33 | 76.58 |'
- en: '| Specialized KD | 20.03 | 23.25 | 20.67 | 77.23 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Specialized KD | 20.03 | 23.25 | 20.67 | 77.23 |'
- en: '| SCOTT | 18.21 | 21.66 | 18.67 | 77.48 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SCOTT | 18.21 | 21.66 | 18.67 | 77.48 |'
- en: '| MCC-KD | 18.36 | 23.89 | 21.33 | 78.13 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MCC-KD | 18.36 | 23.89 | 21.33 | 78.13 |'
- en: '| KPOD (ours) | 22.46 | 27.39 | 25.33 | 81.41 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| KPOD（我们的方法） | 22.46 | 27.39 | 25.33 | 81.41 |'
- en: 4.1 Experiment Setup
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: In this section, we introduce our experiment settings. The implementation details
    can be found in Appendix [A](#A1 "Appendix A Implementation Details ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了我们的实验设置。实施细节见附录[A](#A1 "附录 A 实施细节 ‣ 基于关键点的渐进链式思维蒸馏")。
- en: 'Datasets. We evaluate our method on both mathematical reasoning tasks and commonsense
    reasoning tasks, following (Hsieh et al., [2023](#bib.bib18); Fu et al., [2023](#bib.bib14)).
    For mathematical reasoning, we adopt three benchmark datasets for evaluation:
    GSM8K (Cobbe et al., [2021](#bib.bib9)), ASDiv (Patel et al., [2021](#bib.bib34))
    and SVAMP (Miao et al., [2021](#bib.bib31)). For commonsense reasoning, CommonsenseQA
    benchmark (Talmor et al., [2019](#bib.bib37)) is employed to evaluate our method.
    Additionally, we conduct out-of-distribution (OOD) evaluation via training our
    method on GSM8K while testing it on ASDiv and SVAMP, following (Fu et al., [2023](#bib.bib14)).
    The dataset splits can be found in Appendix [A](#A1 "Appendix A Implementation
    Details ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们在数学推理任务和常识推理任务上评估我们的方法，参考（Hsieh等，[2023](#bib.bib18)；Fu等，[2023](#bib.bib14)）。对于数学推理，我们采用了三个基准数据集进行评估：GSM8K（Cobbe等，[2021](#bib.bib9)），ASDiv（Patel等，[2021](#bib.bib34)）和SVAMP（Miao等，[2021](#bib.bib31)）。对于常识推理，我们使用CommonsenseQA基准（Talmor等，[2019](#bib.bib37)）来评估我们的方法。此外，我们通过在GSM8K上训练我们的方法，同时在ASDiv和SVAMP上测试，进行分布外（OOD）评估，参考（Fu等，[2023](#bib.bib14)）。数据集划分见附录[A](#A1
    "附录 A 实施细节 ‣ 基于关键点的渐进链式思维蒸馏")。
- en: 'Models and Baselines. We adopt GPT-3.5-Turbo (Ye et al., [2023](#bib.bib46))
    as the teacher model to generate the rationale for each question in the dataset
    via zero-shot CoT prompting (Kojima et al., [2022](#bib.bib23)), following (Chen
    et al., [2023](#bib.bib6)). This is accessed via the OpenAI’s public API for ChatGPT.
    As for the student model, we adopt three widely-used pretrained language models
    of different architectures: LLaMA-7B (Touvron et al., [2023](#bib.bib38)), FlanT5-XL
    (Chung et al., [2022](#bib.bib8)) and FlanT5-Large (Chung et al., [2022](#bib.bib8)),
    similar to (Fu et al., [2023](#bib.bib14); Chen et al., [2023](#bib.bib6)). The
    parameter counts of LLaMA-7B, FlanT5-XL, FlanT5-Large are 7B, 3B, 760M respectively.
    As for baselines, we employ four state-of-the-art CoT distillation methods for
    comparison: Specialized KD (Fu et al., [2023](#bib.bib14)), SCOTT (Wang et al.,
    [2023b](#bib.bib41)), SCoTD (Li et al., [2023](#bib.bib26)), MCC-KD (Chen et al.,
    [2023](#bib.bib6)). Following previous works (Fu et al., [2023](#bib.bib14)),
    we use the accuracy (%) metric for evaluating the performance of our method and
    baselines.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与基准。我们采用GPT-3.5-Turbo（Ye等，[2023](#bib.bib46)）作为教师模型，通过零-shot CoT 提示（Kojima等，[2022](#bib.bib23)），生成数据集中每个问题的理由，参考（Chen等，[2023](#bib.bib6)）。该模型通过OpenAI的ChatGPT公共API访问。作为学生模型，我们采用了三种广泛使用的不同架构的预训练语言模型：LLaMA-7B（Touvron等，[2023](#bib.bib38)），FlanT5-XL（Chung等，[2022](#bib.bib8)）和FlanT5-Large（Chung等，[2022](#bib.bib8)），类似于（Fu等，[2023](#bib.bib14)；Chen等，[2023](#bib.bib6)）。LLaMA-7B、FlanT5-XL、FlanT5-Large的参数数量分别为7B、3B、760M。对于基准线，我们采用了四种最先进的CoT蒸馏方法进行比较：Specialized
    KD（Fu等，[2023](#bib.bib14)），SCOTT（Wang等，[2023b](#bib.bib41)），SCoTD（Li等，[2023](#bib.bib26)），MCC-KD（Chen等，[2023](#bib.bib6)）。参考以往工作（Fu等，[2023](#bib.bib14)），我们使用准确率（%）指标来评估我们方法和基准线的表现。
- en: 4.2 Overall Performance
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 整体表现
- en: In this section, we evaluate the overall performance of our method. We compare
    our method with four recent state-of-the-art CoT distillation methods as mentioned
    before. The GPT-3.5-Turbo serves as the teacher model. Table [1](#S4.T1 "Table
    1 ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation for
    LLMs") illustrates the results. The symbol “-” denotes the model without using
    CoT distillation methods. First, we can observe that CoT distillation methods
    consistently boost the performance of smaller student models on reasoning tasks,
    underscoring the effectiveness of distilling rationales. In addition, it’s evident
    that our proposed KPOD outperforms previous methods by a large margin. For example,
    compared to MCC-KD, achieving the second best results when using LLaMA-7B as the
    student model, our approach achieves $5.16\%$ performance gains on the GSM8K,
    ASDiv, SVAMP, CommonsenseQA datasets, respectively. This highlights the effectiveness
    of promoting precise mimicry of keypoint tokens and implementing a learning schedule
    that progresses from easy to challenging tasks. Such an approach facilitates the
    acquisition of reasoning capabilities by the student model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们方法的整体性能。我们将我们的方法与之前提到的四种最新的CoT蒸馏方法进行了比较。GPT-3.5-Turbo作为教师模型。表[1](#S4.T1
    "表1 ‣ 4 实验 ‣ 基于关键点的渐进链式思维蒸馏方法")展示了结果。符号“-”表示没有使用CoT蒸馏方法的模型。首先，我们可以观察到，CoT蒸馏方法始终能提升较小学生模型在推理任务上的性能，突显了蒸馏推理的有效性。此外，很明显我们提出的KPOD方法相比于以往的方法具有显著优势。例如，与MCC-KD相比，当使用LLaMA-7B作为学生模型时，我们的方法在GSM8K、ASDiv、SVAMP、CommonsenseQA数据集上的性能提升分别达到了$5.16\%$。这突显了促进关键点令牌的精确模仿和实施从简单到困难任务的学习进度表的有效性。这种方法有助于学生模型获得推理能力。
- en: 'Table 2: Ablation study of our method.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们方法的消融研究。
- en: '| Models | Settings | Datasets |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 数据集 |'
- en: '| GSM8K | CommonsenseQA |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | CommonsenseQA |'
- en: '| LLaMA-7B | KPOD-w.o.-sig | 42.64 | 75.18 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | KPOD-w.o.-sig | 42.64 | 75.18 |'
- en: '| KPOD-w.o.-sig-dif | 44.01 | 76.49 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-sig-dif | 44.01 | 76.49 |'
- en: '| KPOD-w.o.-prog | 43.25 | 74.61 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-prog | 43.25 | 74.61 |'
- en: '| KPOD-w.o.-div | 44.16 | 75.76 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-div | 44.16 | 75.76 |'
- en: '| KPOD-ACL | 43.55 | 75.51 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-ACL | 43.55 | 75.51 |'
- en: '| KPOD-SPL | 42.94 | 75.84 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-SPL | 42.94 | 75.84 |'
- en: '| KPOD-ICL | 43.85 | 75.35 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-ICL | 43.85 | 75.35 |'
- en: '| KPOD | 46.74 | 77.89 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| KPOD | 46.74 | 77.89 |'
- en: '| FlanT5-XL | KPOD-w.o.-sig | 22.46 | 85.26 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XL | KPOD-w.o.-sig | 22.46 | 85.26 |'
- en: '| KPOD-w.o.-sig-dif | 23.82 | 86.08 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-sig-dif | 23.82 | 86.08 |'
- en: '| KPOD-w.o.-prog | 23.22 | 84.28 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-prog | 23.22 | 84.28 |'
- en: '| KPOD-w.o.-div | 23.98 | 86.73 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-w.o.-div | 23.98 | 86.73 |'
- en: '| KPOD-ACL | 23.52 | 86.40 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-ACL | 23.52 | 86.40 |'
- en: '| KPOD-SPL | 22.76 | 85.59 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-SPL | 22.76 | 85.59 |'
- en: '| KPOD-ICL | 22.91 | 85.83 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| KPOD-ICL | 22.91 | 85.83 |'
- en: '| KPOD | 25.19 | 88.04 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| KPOD | 25.19 | 88.04 |'
- en: 4.3 Ablation Study
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'We conduct ablation study to verify the effectiveness of the components in
    our proposed method. Specifically, we design several variants of our proposed
    KPOD: KPOD-w.o.-sig denotes our method wherein each token is treated equally,
    without incorporating the token significance weight for distillation. KPOD-w.o.-sig-dif
    represents our method without using the token significance weight for calculating
    the step difficulty. KPOD-w.o.-prog means our method without using the proposed
    progressive distillation strategy. KPOD-w.o.-div denotes our method without using
    the diversity term in the value function to select the question set.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行消融研究以验证我们提出的方法中各个组件的有效性。具体来说，我们设计了几个变体来测试我们提出的KPOD方法：KPOD-w.o.-sig表示我们的方法，其中每个令牌被视为相等，而没有将令牌重要性权重纳入蒸馏中。KPOD-w.o.-sig-dif表示我们的方法在计算步骤难度时没有使用令牌重要性权重。KPOD-w.o.-prog表示我们的方法没有使用提出的渐进蒸馏策略。KPOD-w.o.-div表示我们的方法在选择问题集时没有使用价值函数中的多样性项。
- en: 'Besides, we compare our method with three representative curriculum learning
    methods: Adaptive CL (Kong et al., [2021](#bib.bib24)), SPL (Wan et al., [2020](#bib.bib39))
    and ICL (Jia et al., [2023](#bib.bib21)). We design three variants of our method:
    KPOD-ACL, KPOD-SPL, KPOD-ICL respectively denote replacing our in-rationale progressive
    distillation strategy by Adaptive CL, SPL and ICL. The results are listed in Table
    [2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将我们的方法与三种代表性的课程学习方法进行比较：Adaptive CL（Kong et al.，[2021](#bib.bib24)）、SPL（Wan
    et al.，[2020](#bib.bib39)）和ICL（Jia et al.，[2023](#bib.bib21)）。我们设计了我们方法的三个变体：KPOD-ACL、KPOD-SPL、KPOD-ICL分别表示用Adaptive
    CL、SPL和ICL替代我们的方法中的合理化渐进式蒸馏策略。结果列在表[2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance
    ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation for
    LLMs")中。
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"), KPOD-w.o.-sig
    obtains inferior performance than KPOD, illustrating the effectiveness of emphasizing
    the precise mimicry of keypoint tokens in our method. Besides, KPOD outperforms
    KPOD-w.o.-sig-dif. This shows that it’s essential to utilizing the token significance
    weight for the step difficulty calculation. The performance of KPOD-w.o.-prog
    is worse than KPOD, illustrating the effectiveness of scheduling an easy-to-hard
    learning order for CoT distillation. Moreover, KPOD obtains better performance
    than KPOD-w.o.-div. This demonstrates that ensuring a diverse question set to
    increase difficulty is effective. Finally, we can find that KPOD surpasses KPOD-ACL,
    KPOD-SPL and KPOD-ICL, showing the superiority of our in-rationale progressive
    distillation strategy compared to previous curriculum learning methods.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S4.T2 "Table 2 ‣ 4.2 Overall Performance ‣ 4 Experiments ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs")所示，KPOD-w.o.-sig的表现不如KPOD，说明了在我们的方法中强调关键点令牌的精确模仿的有效性。此外，KPOD的表现优于KPOD-w.o.-sig-dif。这表明，利用令牌重要性权重来计算步骤难度是至关重要的。KPOD-w.o.-prog的表现比KPOD差，这说明了为CoT蒸馏安排由易到难的学习顺序的有效性。此外，KPOD的表现优于KPOD-w.o.-div。这表明，确保多样化的问题集以提高难度是有效的。最后，我们发现KPOD超越了KPOD-ACL、KPOD-SPL和KPOD-ICL，显示了我们在合理化的渐进式蒸馏策略相较于以往课程学习方法的优越性。
- en: 4.4 OOD Performance
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 OOD性能
- en: Following (Fu et al., [2023](#bib.bib14)), we examine the out-of-distribution
    (OOD) generalization ability of the student model trained by our method and baselines.
    We use the in-distribution mathematical dataset GSM8K for training and adopt OOD
    mathematical datasets ASDiv, SVAMP for testing, similar to (Fu et al., [2023](#bib.bib14);
    Chen et al., [2023](#bib.bib6)). As shown in Table [3](#S4.T3 "Table 3 ‣ 4.4 OOD
    Performance ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs"), our proposed KPOD consistently obtains superior performance compared
    to the baselines, indicating that the student model trained by our method has
    stronger OOD generalization capabilities.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 根据（Fu et al.，[2023](#bib.bib14)），我们检查了由我们的方法和基准训练的学生模型的分布外（OOD）泛化能力。我们使用在分布内的数学数据集GSM8K进行训练，并采用OOD数学数据集ASDiv、SVAMP进行测试，类似于（Fu
    et al.，[2023](#bib.bib14)；Chen et al.，[2023](#bib.bib6)）。如表[3](#S4.T3 "Table 3
    ‣ 4.4 OOD Performance ‣ 4 Experiments ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")所示，我们提出的KPOD在性能上始终优于基准，表明由我们的方法训练的学生模型具有更强的OOD泛化能力。
- en: 'Table 3: OOD performance of our method and baselines.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：我们的方法与基准的OOD性能。
- en: '| Model | Methods | In-distribution | OOD |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Model | Methods | In-distribution | OOD |'
- en: '| GSM8K | ASDiv | SVAMP |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | ASDiv | SVAMP |'
- en: '| LLaMA-7B | SCoTD | 38.54 | 55.09 | 45.33 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | SCoTD | 38.54 | 55.09 | 45.33 |'
- en: '| Specialized KD | 39.15 | 53.82 | 38.67 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Specialized KD | 39.15 | 53.82 | 38.67 |'
- en: '| SCOTT | 40.97 | 53.50 | 42.00 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SCOTT | 40.97 | 53.50 | 42.00 |'
- en: '| MCC-KD | 41.58 | 57.64 | 41.00 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| MCC-KD | 41.58 | 57.64 | 41.00 |'
- en: '| KPOD (ours) | 46.74 | 57.96 | 47.33 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| KPOD（我们的方法） | 46.74 | 57.96 | 47.33 |'
- en: '| FlanT5-XL | SCoTD | 21.85 | 25.48 | 22.67 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-XL | SCoTD | 21.85 | 25.48 | 22.67 |'
- en: '| Specialized KD | 23.22 | 26.11 | 24.67 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Specialized KD | 23.22 | 26.11 | 24.67 |'
- en: '| SCOTT | 21.09 | 25.20 | 25.33 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| SCOTT | 21.09 | 25.20 | 25.33 |'
- en: '| MCC-KD | 24.28 | 28.98 | 26.67 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MCC-KD | 24.28 | 28.98 | 26.67 |'
- en: '| KPOD (ours) | 25.19 | 32.48 | 29.33 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| KPOD（我们的方法） | 25.19 | 32.48 | 29.33 |'
- en: 4.5 Visualizations
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 可视化
- en: In this section, we visualize the token significance weight $w_{j}^{(i)}$ generated
    by the weight generator, to intuitively show the effectiveness of the rationale
    token weighting module. Figure [2](#S4.F2 "Figure 2 ‣ 4.5 Visualizations ‣ 4 Experiments
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs") illustrates
    the visualization results on the GSM8K dataset. First, we can find that the digit
    tokens and operation tokens obtain the highest weights. This is because these
    tokens are usually of vital importance in the reasoning process, where even a
    slight deviation could cause errors. Additionally, several tokens that contribute
    significantly to the reasoning also exhibit relatively high weights. Tokens such
    as “twice”, “total”, “adding”, and “dividing” provide instructional cues for the
    reasoning steps. Besides, meaningful subjects like “Mark” and “Jennifer” can play
    a crucial role in reasoning, as their relationships should be considered during
    the reasoning process. Furthermore, it could be observed that some tokens of less
    importance for the reasoning are given low weights, such as “can”, “say”, “fit”,
    “received”, “got”, etc. These visualizations demonstrate our rationale token weighting
    module can effectively determine the significance of rationale tokens, thereby
    facilitating the student to accurately mimic crucial keypoint tokens.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们可视化了由权重生成器生成的令牌重要性权重 $w_{j}^{(i)}$，以直观地展示推理令牌加权模块的有效性。图 [2](#S4.F2 "图
    2 ‣ 4.5 可视化 ‣ 4 实验 ‣ 基于关键点的逐步思维链蒸馏方法") 展示了在 GSM8K 数据集上的可视化结果。首先，我们可以发现数字令牌和操作令牌获得了最高的权重。这是因为这些令牌通常在推理过程中至关重要，即使是轻微的偏差也可能导致错误。此外，几种对推理有重要贡献的令牌也显示出相对较高的权重。像“twice”、“total”、“adding”和“dividing”等令牌为推理步骤提供了指示性提示。此外，有意义的主体如“Mark”和“Jennifer”在推理中也可以发挥关键作用，因为在推理过程中需要考虑它们之间的关系。此外，还可以观察到一些在推理中重要性较低的令牌被赋予了较低的权重，如“can”、“say”、“fit”、“received”、“got”等。这些可视化结果表明，我们的推理令牌加权模块可以有效地确定推理令牌的显著性，从而帮助学生模型准确地模拟关键点令牌。
- en: '![Refer to caption](img/4d62a69ef441b3d81366d94d25f2d61f.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4d62a69ef441b3d81366d94d25f2d61f.png)'
- en: 'Figure 2: Visualizations of token significance weights produced by the weight
    generator. The intensity of red corresponds to the significance weight assigned
    to each token, with a deeper red indicating higher weight.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：权重生成器生成的令牌重要性权重的可视化。红色的深浅对应于分配给每个令牌的显著性权重，红色越深表示权重越高。
- en: '![Refer to caption](img/004f701ab416e38ee7c1133f0c43b17a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/004f701ab416e38ee7c1133f0c43b17a.png)'
- en: (a) performance with varying $\alpha$
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同 $\alpha$ 值下的性能
- en: '![Refer to caption](img/f1b6a85e54094102d845e0cc0f91cad8.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f1b6a85e54094102d845e0cc0f91cad8.png)'
- en: (b) performance with varying $\beta$
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不同 $\beta$ 值下的性能
- en: 'Figure 3: Parameter sensitivity study of $\alpha$.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：$\alpha$ 的参数敏感性研究。
- en: 4.6 Parameter Sensitivity Analysis
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 参数敏感性分析
- en: We perform experiments to analyze the effect of two important hyper-parameters
    $\alpha$ in a relatively large range. Thus it’s easy to set them in practice.
    We analyze the sensitivity of other hyper-parameters in Appendix [C](#A3 "Appendix
    C Additional Experiments ‣ Keypoint-based Progressive Chain-of-Thought Distillation
    for LLMs").
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在相对较大的范围内分析了两个重要超参数 $\alpha$ 的效果。因此，它们在实际操作中易于设置。我们在附录 [C](#A3 "附录 C 额外实验
    ‣ 基于关键点的逐步思维链蒸馏方法") 中分析了其他超参数的敏感性。
- en: 5 Conclusion
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we proposed a keypoint-based progressive chain-of-thought distillation
    framework for LLMs. Specifically, we devised a rationale token weighting module
    to encourage the student model to accurately mimic keypoint tokens during the
    distillation process. Besides, we proposed an in-rationale progressive distillation
    strategy to enable the student model to acquire reasoning capabilities from the
    teacher LLMs in an easy-to-hard manner. Extensive experiments validated the effectiveness
    of our proposed method.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种基于关键点的逐步思维链蒸馏框架，适用于大型语言模型（LLMs）。具体来说，我们设计了一个推理令牌加权模块，以鼓励学生模型在蒸馏过程中准确模拟关键点令牌。此外，我们提出了一种渐进式蒸馏策略，使学生模型能够以从易到难的方式从教师
    LLMs 中获得推理能力。大量实验验证了我们提出方法的有效性。
- en: Acknowledgment
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported by the NSFC under Grants 62122013, U2001211\. This work
    was also supported by the Innovative Development Joint Fund Key Projects of Shandong
    NSF under Grants ZR2022LZH007.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了国家自然科学基金（NSFC）资助，资助编号 62122013、U2001211。本研究还得到了山东省自然科学基金创新发展联合基金重点项目资助，资助编号
    ZR2022LZH007。
- en: Impact Statement
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the field of machine learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的工作旨在推进机器学习领域。我们的工作可能带来许多潜在的社会影响，但我们认为这里不需要特别强调。
- en: References
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bengio et al. (2009) Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
    Curriculum learning. In *International Conference on Machine Learning*, pp.  41–48,
    2009.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio et al. (2009) Bengio, Y., Louradour, J., Collobert, R., 和 Weston, J.
    课程学习。见 *国际机器学习会议*，第 41–48 页，2009 年。
- en: Benoit et al. (2013) Benoit, L., Lehalle, H., Molina, M., Tijus, C., and Jouen,
    F. Young children’s mapping between arrays, number words, and digits. *Cognition*,
    129(1):95–101, 2013.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benoit et al. (2013) Benoit, L., Lehalle, H., Molina, M., Tijus, C., 和 Jouen,
    F. 年幼儿童在数组、数字词和数字之间的映射。*认知*，129(1)：95–101，2013 年。
- en: Bradley et al. (2000) Bradley, P. S., Bennett, K. P., and Demiriz, A. Constrained
    k-means clustering. *Microsoft Research, Redmond*, 20, 2000.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley et al. (2000) Bradley, P. S., Bennett, K. P., 和 Demiriz, A. 约束的 k-均值聚类。*微软研究，雷德蒙德*，第
    20 期，2000 年。
- en: Bridle (1989) Bridle, J. Training stochastic model recognition algorithms as
    networks can lead to maximum mutual information estimation of parameters. *Annual
    Conference on Neural Information Processing Systems*, 2, 1989.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bridle (1989) Bridle, J. 训练随机模型识别算法作为网络可以最大化参数的互信息估计。*神经信息处理系统年会*，第 2 期，1989
    年。
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language
    models are few-shot learners. *Annual Conference on Neural Information Processing
    Systems*, 33:1877–1901, 2020.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.
    D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等人。语言模型是少样本学习者。*神经信息处理系统年会*，33：1877–1901，2020
    年。
- en: 'Chen et al. (2023) Chen, H., Wu, S., Quan, X., Wang, R., Yan, M., and Zhang,
    J. MCC-KD: Multi-CoT consistent knowledge distillation. In *Findings of the Association
    for Computational Linguistics: EMNLP 2023*, pp.  6805–6820\. Association for Computational
    Linguistics, December 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Chen, H., Wu, S., Quan, X., Wang, R., Yan, M., 和 Zhang, J.
    MCC-KD：多链一致知识蒸馏。见 *计算语言学协会会议：EMNLP 2023*，第 6805–6820 页。计算语言学协会，2023 年 12 月。
- en: 'Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm:
    Scaling language modeling with pathways. *Journal of Machine Learning Research*,
    24(240):1–113, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery et al. (2023) Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., 等人。Palm：通过路径扩展语言建模。*机器学习研究期刊*，24(240)：1–113，2023
    年。
- en: Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, E., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*, 2022.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, E., Wang, X., Dehghani, M., Brahma, S., 等人。扩展指令微调的语言模型。*arXiv 预印本 arXiv:2210.11416*，2022
    年。
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., 等人。训练验证器解决数学文字题。*arXiv
    预印本 arXiv:2110.14168*，2021 年。
- en: Diao et al. (2023) Diao, S., Wang, P., Lin, Y., and Zhang, T. Active prompting
    with chain-of-thought for large language models. *arXiv preprint arXiv:2302.12246*,
    2023.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao et al. (2023) Diao, S., Wang, P., Lin, Y., 和 Zhang, T. 使用链式思维对大型语言模型进行主动提示。*arXiv
    预印本 arXiv:2302.12246*，2023 年。
- en: 'Elman (1993) Elman, J. L. Learning and development in neural networks: The
    importance of starting small. *Cognition*, 48(1):71–99, 1993.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elman (1993) Elman, J. L. 神经网络中的学习与发展：从小开始的重要性。*认知*，48(1)：71–99，1993 年。
- en: 'Feng et al. (2022) Feng, K., Li, C., Yuan, Y., and Wang, G. Freekd: Free-direction
    knowledge distillation for graph neural networks. In *Proceedings of the 28th
    ACM SIGKDD Conference on Knowledge Discovery and Data Mining*, pp.  357–366, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2022) Feng, K., Li, C., Yuan, Y., 和 Wang, G. Freekd：图神经网络的自由方向知识蒸馏。见
    *第 28 届 ACM SIGKDD 知识发现与数据挖掘大会论文集*，第 357–366 页，2022 年。
- en: 'Feng et al. (2024) Feng, K., Li, C., Ren, D., Yuan, Y., and Wang, G. On the
    road to portability: Compressing end-to-end motion planner for autonomous driving.
    *arXiv preprint arXiv:2403.01238*, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2024）Feng, K., Li, C., Ren, D., Yuan, Y., 和 Wang, G. 《通向可移植性的道路：压缩端到端的自主驾驶运动规划器》。*arXiv
    预印本 arXiv:2403.01238*，2024年。
- en: Fu et al. (2023) Fu, Y., Peng, H., Ou, L., Sabharwal, A., and Khot, T. Specializing
    smaller language models towards multi-step reasoning. *International Conference
    on Machine Learning*, 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）Fu, Y., Peng, H., Ou, L., Sabharwal, A., 和 Khot, T. 《将更小的语言模型专门化为多步骤推理》。*国际机器学习大会*，2023年。
- en: Hinton et al. (2015) Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge
    in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Hinton, G., Vinyals, O., 和 Dean, J. 《提炼神经网络中的知识》。*arXiv 预印本 arXiv:1503.02531*，2015年。
- en: 'Ho et al. (2023) Ho, N., Schmid, L., and Yun, S.-Y. Large language models are
    reasoning teachers. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  14852–14882\. Association
    for Computational Linguistics, July 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等（2023）Ho, N., Schmid, L., 和 Yun, S.-Y. 《大型语言模型是推理教师》。在 *第61届计算语言学协会年会（第一卷：长篇论文）*
    中，页码 14852–14882。计算语言学协会，2023年7月。
- en: Hoffmann et al. (2022) Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya,
    E., Cai, T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark,
    A., et al. Training compute-optimal large language models. *arXiv preprint arXiv:2203.15556*,
    2022.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等（2022）Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai,
    T., Rutherford, E., Casas, D. d. L., Hendricks, L. A., Welbl, J., Clark, A., 等。
    《训练计算优化的大型语言模型》。*arXiv 预印本 arXiv:2203.15556*，2022年。
- en: 'Hsieh et al. (2023) Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii,
    Y., Ratner, A., Krishna, R., Lee, C.-Y., and Pfister, T. Distilling step-by-step!
    outperforming larger language models with less training data and smaller model
    sizes. In *Findings of the Association for Computational Linguistics: ACL 2023*,
    pp.  8003–8017\. Association for Computational Linguistics, July 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hsieh 等（2023）Hsieh, C.-Y., Li, C.-L., Yeh, C.-k., Nakhost, H., Fujii, Y., Ratner,
    A., Krishna, R., Lee, C.-Y., 和 Pfister, T. 《逐步提炼！用更少的训练数据和更小的模型规模超越更大的语言模型》。在
    *Association for Computational Linguistics: ACL 2023* 中，页码 8003–8017。计算语言学协会，2023年7月。'
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S.,
    Wang, L., 和 Chen, W. 《Lora：大型语言模型的低秩适应》。*arXiv 预印本 arXiv:2106.09685*，2021年。
- en: Jang et al. (2016) Jang, E., Gu, S., and Poole, B. Categorical reparameterization
    with gumbel-softmax. *arXiv preprint arXiv:1611.01144*, 2016.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等（2016）Jang, E., Gu, S., 和 Poole, B. 《带有 Gumbel-Softmax 的类别重参数化》。*arXiv
    预印本 arXiv:1611.01144*，2016年。
- en: 'Jia et al. (2023) Jia, Q., Liu, Y., Tang, H., and Zhu, K. In-sample curriculum
    learning by sequence completion for natural language generation. In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pp.  11937–11950\. Association for Computational Linguistics,
    July 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等（2023）Jia, Q., Liu, Y., Tang, H., 和 Zhu, K. 《通过序列补全进行的样本内课程学习用于自然语言生成》。在
    *第61届计算语言学协会年会（第一卷：长篇论文）* 中，页码 11937–11950。计算语言学协会，2023年7月。
- en: Jiang et al. (2014) Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., and Hauptmann,
    A. Self-paced learning with diversity. *Annual Conference on Neural Information
    Processing Systems*, 27, 2014.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2014）Jiang, L., Meng, D., Yu, S.-I., Lan, Z., Shan, S., 和 Hauptmann,
    A. 《带有多样性的自适应学习》。*神经信息处理系统年会*，27，2014年。
- en: Kojima et al. (2022) Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
    Y. Large language models are zero-shot reasoners. *Annual Conference on Neural
    Information Processing Systems*, 35:22199–22213, 2022.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等（2022）Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., 和 Iwasawa, Y. 《大型语言模型是零-shot
    推理者》。*神经信息处理系统年会*，35:22199–22213，2022年。
- en: Kong et al. (2021) Kong, Y., Liu, L., Wang, J., and Tao, D. Adaptive curriculum
    learning. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pp.  5067–5076, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等（2021）Kong, Y., Liu, L., Wang, J., 和 Tao, D. 《自适应课程学习》。在 *IEEE/CVF 国际计算机视觉会议论文集*
    中，页码 5067–5076，2021年。
- en: 'Krueger & Dayan (2009) Krueger, K. A. and Dayan, P. Flexible shaping: How learning
    in small steps helps. *Cognition*, 110(3):380–394, 2009.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krueger & Dayan（2009）Krueger, K. A. 和 Dayan, P. 《灵活的塑造：小步骤学习如何帮助》。*认知*，110(3):380–394，2009年。
- en: 'Li et al. (2023) Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., and
    Choi, Y. Symbolic chain-of-thought distillation: Small models can also “think”
    step-by-step. In *Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers)*, pp.  2665–2679\. Association
    for Computational Linguistics, July 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023）Li, L. H., Hessel, J., Yu, Y., Ren, X., Chang, K.-W., 和 Choi, Y. 符号化链式思考蒸馏：小模型也可以“逐步思考”。在*第61届计算语言学协会年会论文集（第1卷：长篇论文）*中，第2665–2679页。计算语言学协会，2023年7月。
- en: Li et al. (2022) Li, W., Feldman, M., Kazemi, E., and Karbasi, A. Submodular
    maximization in clean linear time. *Annual Conference on Neural Information Processing
    Systems*, 35:17473–17487, 2022.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Li, W., Feldman, M., Kazemi, E., 和 Karbasi, A. 在干净线性时间中的子模块化最大化。*神经信息处理系统年会*，35:17473–17487，2022年。
- en: 'Liang et al. (2021) Liang, C., Jiang, H., Liu, X., He, P., Chen, W., Gao, J.,
    and Zhao, T. Token-wise curriculum learning for neural machine translation. In
    *Findings of the Association for Computational Linguistics: EMNLP 2021*, pp. 
    3658–3670, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等（2021）Liang, C., Jiang, H., Liu, X., He, P., Chen, W., Gao, J., 和 Zhao,
    T. 用于神经机器翻译的按Token划分的课程学习。在*计算语言学协会年会发现：EMNLP 2021*中，第3658–3670页，2021年。
- en: 'Ling et al. (2017) Ling, W., Yogatama, D., Dyer, C., and Blunsom, P. Program
    induction by rationale generation: Learning to solve and explain algebraic word
    problems. *arXiv preprint arXiv:1705.04146*, 2017.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling等（2017）Ling, W., Yogatama, D., Dyer, C., 和 Blunsom, P. 通过理由生成的程序归纳：学习解决和解释代数单词问题。*arXiv预印本
    arXiv:1705.04146*，2017年。
- en: 'Magister et al. (2023) Magister, L. C., Mallinson, J., Adamek, J., Malmi, E.,
    and Severyn, A. Teaching small language models to reason. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 2:
    Short Papers)*, pp.  1773–1781\. Association for Computational Linguistics, July
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Magister等（2023）Magister, L. C., Mallinson, J., Adamek, J., Malmi, E., 和 Severyn,
    A. 教授小型语言模型推理。在*第61届计算语言学协会年会论文集（第2卷：短篇论文）*中，第1773–1781页。计算语言学协会，2023年7月。
- en: Miao et al. (2021) Miao, S.-Y., Liang, C.-C., and Su, K.-Y. A diverse corpus
    for evaluating and developing english math word problem solvers. *arXiv preprint
    arXiv:2106.15772*, 2021.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miao等（2021）Miao, S.-Y., Liang, C.-C., 和 Su, K.-Y. 用于评估和开发英文数学题解题器的多样化语料库。*arXiv预印本
    arXiv:2106.15772*，2021年。
- en: Molina & Jouen (1998) Molina, M. and Jouen, F. Modulation of the palmar grasp
    behavior in neonates according to texture property. *Infant Behavior and Development*,
    21(4):659–666, 1998.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molina & Jouen（1998）Molina, M. 和 Jouen, F. 根据纹理属性对新生儿掌握行为的调节。*婴儿行为与发展*，21(4):659–666，1998年。
- en: 'Narayan (1997) Narayan, S. The generalized sigmoid activation function: Competitive
    supervised learning. *Information Sciences*, 99(1-2):69–82, 1997.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan（1997）Narayan, S. 泛化的sigmoid激活函数：竞争性监督学习。*信息科学*，99(1-2):69–82，1997年。
- en: Patel et al. (2021) Patel, A., Bhattamishra, S., and Goyal, N. Are nlp models
    really able to solve simple math word problems? *arXiv preprint arXiv:2103.07191*,
    2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patel等（2021）Patel, A., Bhattamishra, S., 和 Goyal, N. NLP模型真的能解决简单的数学题吗？*arXiv预印本
    arXiv:2103.07191*，2021年。
- en: 'Pennington et al. (2014) Pennington, J., Socher, R., and Manning, C. D. Glove:
    Global vectors for word representation. In *Conference on Empirical Methods in
    Natural Language Processing*, pp.  1532–1543, 2014.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennington等（2014）Pennington, J., Socher, R., 和 Manning, C. D. Glove：用于词表示的全局向量。在*自然语言处理经验方法会议*中，第1532–1543页，2014年。
- en: 'Peterson (2004) Peterson, G. B. A day of great illumination: Bf skinner’s discovery
    of shaping. *Journal of the experimental analysis of behavior*, 82(3):317–328,
    2004.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peterson（2004）Peterson, G. B. 伟大的启示之日：B.F. 斯金纳对塑造的发现。*行为实验分析杂志*，82(3):317–328，2004年。
- en: 'Talmor et al. (2019) Talmor, A., Herzig, J., Lourie, N., and Berant, J. Commonsenseqa:
    A question answering challenge targeting commonsense knowledge. In *Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pp. 
    4149–4158, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor等（2019）Talmor, A., Herzig, J., Lourie, N., 和 Berant, J. Commonsenseqa：针对常识知识的问题回答挑战。在*2019年北美计算语言学协会年会（人类语言技术会议，第1卷：长篇和短篇论文）*中，第4149–4158页，2019年。
- en: 'Touvron et al. (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama:
    Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等. Llama: 开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023年。'
- en: Wan et al. (2020) Wan, Y., Yang, B., Wong, D. F., Zhou, Y., Chao, L. S., Zhang,
    H., and Chen, B. Self-paced learning for neural machine translation. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*,
    pp.  1074–1080\. Association for Computational Linguistics, November 2020.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等（2020）Wan, Y., Yang, B., Wong, D. F., Zhou, Y., Chao, L. S., Zhang, H.,
    和 Chen, B. 自主学习神经机器翻译。见于 *2020年自然语言处理经验方法会议论文集（EMNLP）*，第1074–1080页。计算语言学协会，2020年11月。
- en: 'Wang et al. (2023a) Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang,
    B., Xu, R., and Wong, K.-F. Cue-cot: Chain-of-thought prompting for responding
    to in-depth dialogue questions with llms. In *Findings of the Association for
    Computational Linguistics: EMNLP 2023*, pp.  12047–12064, 2023a.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023a）Wang, H., Wang, R., Mi, F., Deng, Y., Wang, Z., Liang, B., Xu,
    R., 和 Wong, K.-F. Cue-cot: 面对深度对话问题时，使用链式思维提示的 LLMs。见于 *计算语言学协会会议论文集：EMNLP 2023*，第12047–12064页，2023a年。'
- en: 'Wang et al. (2023b) Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., and Ren,
    X. SCOTT: Self-consistent chain-of-thought distillation. In *Proceedings of the
    61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pp.  5546–5558\. Association for Computational Linguistics, July
    2023b.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2023b）Wang, P., Wang, Z., Li, Z., Gao, Y., Yin, B., 和 Ren, X. SCOTT:
    自我一致性链式思维蒸馏。见于 *第61届计算语言学协会年会（第一卷：长篇论文）*，第5546–5558页。计算语言学协会，2023年7月。'
- en: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning
    in language models. *arXiv preprint arXiv:2203.11171*, 2022.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2022）Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S.,
    Chowdhery, A., 和 Zhou, D. 自我一致性改进语言模型中的思维链推理。*arXiv 预印本 arXiv:2203.11171*，2022年。
- en: 'Wang et al. (2021) Wang, Y., Wang, W., Liang, Y., Cai, Y., and Hooi, B. Curgraph:
    Curriculum learning for graph classification. In *Proceedings of the Web Conference
    2021*, pp.  1238–1248, 2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等（2021）Wang, Y., Wang, W., Liang, Y., Cai, Y., 和 Hooi, B. Curgraph: 图分类的课程学习。见于
    *2021年网络会议论文集*，第1238–1248页，2021年。'
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. *Annual Conference on Neural Information Processing Systems*,
    35:24824–24837, 2022.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le,
    Q. V., Zhou, D., 等. 链式思维提示引发大型语言模型中的推理。*神经信息处理系统年会*，35:24824–24837，2022年。
- en: Yang et al. (2023) Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D.,
    and Chen, X. Large language models as optimizers. *arXiv preprint arXiv:2309.03409*,
    2023.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2023）Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q. V., Zhou, D., 和 Chen,
    X. 大型语言模型作为优化器。*arXiv 预印本 arXiv:2309.03409*，2023年。
- en: Ye et al. (2023) Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y.,
    Zhou, Z., Gong, C., Shen, Y., et al. A comprehensive capability analysis of gpt-3
    and gpt-3.5 series models. *arXiv preprint arXiv:2303.10420*, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye 等（2023）Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., Cui, Y., Zhou,
    Z., Gong, C., Shen, Y., 等. GPT-3 和 GPT-3.5 系列模型的全面能力分析。*arXiv 预印本 arXiv:2303.10420*，2023年。
- en: Zhang et al. (2022) Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic chain
    of thought prompting in large language models. *arXiv preprint arXiv:2210.03493*,
    2022.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Zhang, Z., Zhang, A., Li, M., 和 Smola, A. 大型语言模型中的自动链式思维提示。*arXiv
    预印本 arXiv:2210.03493*，2022年。
- en: Zhang et al. (2023) Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., and
    Smola, A. Multimodal chain-of-thought reasoning in language models. *arXiv preprint
    arXiv:2302.00923*, 2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Zhang, Z., Zhang, A., Li, M., Zhao, H., Karypis, G., 和 Smola, A.
    语言模型中的多模态链式思维推理。*arXiv 预印本 arXiv:2302.00923*，2023年。
- en: Appendix A Implementation Details
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实施细节
- en: We perform our experiments using GeForce RTX 3090 GPUs. In order to accelerate
    training, we employ LoRA (Hu et al., [2021](#bib.bib19)) to train the student
    model. Following previous CoT distillation works (Chen et al., [2023](#bib.bib6)),
    the rank of LoRA is set to 64 for LLaMA-7B and 128 for FlanT5-XL. We use Adam
    optimizer for optimization with a learning rate of $1\times 10^{-5}$ for clustering
    the question.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GeForce RTX 3090 GPU 进行实验。为了加速训练，我们采用 LoRA (Hu et al., [2021](#bib.bib19))
    来训练学生模型。遵循之前的 CoT 蒸馏工作 (Chen et al., [2023](#bib.bib6))，LoRA 的秩设定为 LLaMA-7B 为
    64，FlanT5-XL 为 128。我们使用 Adam 优化器进行优化，学习率为 $1\times 10^{-5}$，用于问题的聚类。
- en: We follow previous CoT distillation works to split the datasets (Chen et al.,
    [2023](#bib.bib6); Fu et al., [2023](#bib.bib14)), the datasets statistics are
    summarized in Table [4](#A1.T4 "Table 4 ‣ Appendix A Implementation Details ‣
    Keypoint-based Progressive Chain-of-Thought Distillation for LLMs").
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循之前的 CoT 蒸馏工作来拆分数据集 (Chen et al., [2023](#bib.bib6); Fu et al., [2023](#bib.bib14))，数据集统计信息总结在表
    [4](#A1.T4 "表 4 ‣ 附录 A 实施细节 ‣ 基于关键点的渐进链式思维蒸馏用于 LLMs")。
- en: 'Table 4: Dataset statistics.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：数据集统计信息。
- en: '| Datasets | Train Size | Validation Size | Test Size |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 训练规模 | 验证规模 | 测试规模 |'
- en: '| GSM8K | 7473 | 660 | 659 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| GSM8K | 7473 | 660 | 659 |'
- en: '| ASDiv | 1462 | 313 | 314 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| ASDiv | 1462 | 313 | 314 |'
- en: '| SVAMP | 700 | 150 | 150 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| SVAMP | 700 | 150 | 150 |'
- en: '| CommonsenseQA | 8520 | 1221 | 1221 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| CommonsenseQA | 8520 | 1221 | 1221 |'
- en: Appendix B Training Pseudo-code
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 训练伪代码
- en: Algorithm 1 outlines the training procedure of our KPOD. Initially, we employ
    the CoT prompt (Kojima et al., [2022](#bib.bib23)) to instruct the teacher LLM
    to generate step-by-step rationales for each question in the dataset. Subsequently,
    the rationale token weighting module receives these rationales as input and is
    trained to determine the significance weights for each token. Following this,
    we compute the difficulty of each step in the rationale based on these weights.
    We then utilize the FTGP algorithm (Li et al., [2022](#bib.bib27)) to maximize
    Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale Progressive Distillation ‣ 3
    Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought Distillation for
    LLMs")) to schedule the question set for increasing difficulty at each stage.
    Once scheduled, we train the student model using Eq.([15](#S3.E15 "Equation 15
    ‣ 3.5 Training Procedure ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) based on the established learning order and token significance
    weights. Before epoch $T$, the student is trained to generate the complete rationale
    for each question. This approach allows the student model to precisely mimic the
    keypoint tokens while progressively acquiring reasoning capabilities in an easy-to-hard
    fashion.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 概述了我们 KPOD 的训练过程。最初，我们采用 CoT 提示 (Kojima et al., [2022](#bib.bib23)) 指导教师
    LLM 为数据集中的每个问题生成逐步的推理过程。随后，推理令牌加权模块接收这些推理作为输入，并被训练以确定每个令牌的重要性权重。接着，我们根据这些权重计算推理中每一步的难度。然后，我们利用
    FTGP 算法 (Li et al., [2022](#bib.bib27)) 最大化 Eq.([13](#S3.E13 "公式 13 ‣ 3.4 推理渐进蒸馏
    ‣ 3 提议的方法 ‣ 基于关键点的渐进链式思维蒸馏用于 LLMs")) 来安排问题集的难度逐步增加。一旦安排好，我们根据既定的学习顺序和令牌重要性权重使用
    Eq.([15](#S3.E15 "公式 15 ‣ 3.5 训练过程 ‣ 3 提议的方法 ‣ 基于关键点的渐进链式思维蒸馏用于 LLMs")) 训练学生模型。在第
    $T$ 轮之前，学生被训练生成每个问题的完整推理。这种方法使学生模型能够精确模仿关键点令牌，同时以由易到难的方式逐步获得推理能力。
- en: Algorithm 1 The training procedure of KPOD
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 KPOD 的训练过程
- en: a teacher LLM, dataset $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$;end for
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个教师 LLM，数据集 $\mathcal{D}=\{(x^{(i)},y^{(i)})\}$；结束
- en: Appendix C Additional Experiments
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 额外实验
- en: '![Refer to caption](img/24a3182dc832970c98066da05507b9b3.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/24a3182dc832970c98066da05507b9b3.png)'
- en: (a) performance with varying $p$
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 随着$p$的变化的性能
- en: '![Refer to caption](img/05eb6aa2ed235154d1207400a72d6987.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/05eb6aa2ed235154d1207400a72d6987.png)'
- en: (b) performance with varying $K$
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 随着$K$的变化的性能
- en: '![Refer to caption](img/44fa4039e898b42ac55506700debfb2e.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/44fa4039e898b42ac55506700debfb2e.png)'
- en: (c) performance with varying $C_{0}$
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 随着$C_{0}$的变化的性能
- en: 'Figure 4: Parameter sensitivity study of $p$ on GSM8K.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：$p$ 在 GSM8K 上的参数敏感性研究。
- en: We additionally analyze the sensitivity of three hyper-parameters of $p$. Our
    method is still not sensitive to this hyper-parameter.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步分析了$p$的三个超参数的敏感性。我们的方法对这个超参数仍然不敏感。
- en: Appendix D Proof
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 证明
- en: In this section, we prove the Proposition [3.1](#S3.Thmtheorem1 "Proposition
    3.1\. ‣ 3.4 In-rationale Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based
    Progressive Chain-of-Thought Distillation for LLMs"). First, we introduce Theorem
    [D.1](#A4.Thmtheorem1 "Theorem D.1\. ‣ Appendix D Proof ‣ Keypoint-based Progressive
    Chain-of-Thought Distillation for LLMs") proposed in FTGP algorithm (Li et al.,
    [2022](#bib.bib27)).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们证明命题 [3.1](#S3.Thmtheorem1 "命题 3.1\. ‣ 3.4 理论进化蒸馏 ‣ 3 提议的方法 ‣ 基于关键点的进化思维链蒸馏方法")。首先，我们介绍
    FTGP 算法中提出的定理 [D.1](#A4.Thmtheorem1 "定理 D.1\. ‣ 附录 D 证明 ‣ 基于关键点的进化思维链蒸馏方法")（Li
    et al., [2022](#bib.bib27)）。
- en: Theorem D.1.
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 D.1。
- en: If a function $f:2^{N}\rightarrow\mathbb{R}$ holds.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果函数 $f:2^{N}\rightarrow\mathbb{R}$ 成立。
- en: According to Theorem [D.1](#A4.Thmtheorem1 "Theorem D.1\. ‣ Appendix D Proof
    ‣ Keypoint-based Progressive Chain-of-Thought Distillation for LLMs"), if we could
    prove that our value function $F$ satisfies these two conditions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定理 [D.1](#A4.Thmtheorem1 "定理 D.1\. ‣ 附录 D 证明 ‣ 基于关键点的进化思维链蒸馏方法")，如果我们能够证明我们的值函数
    $F$ 满足这两个条件。
- en: Definition 1. (Monotonicity) A function $f:2^{N}\rightarrow\mathbb{R}$.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 1.（单调性）函数 $f:2^{N}\rightarrow\mathbb{R}$。
- en: Lemma 1. Our value function $F$ in Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale
    Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) is monotone.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 1. 我们在 Eq.([13](#S3.E13 "方程 13 ‣ 3.4 理论进化蒸馏 ‣ 3 提议的方法 ‣ 基于关键点的进化思维链蒸馏方法"))
    中的值函数 $F$ 是单调的。
- en: Proof.
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We define two question sets $A(t),B(t)$. We have:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义两个问题集合 $A(t),B(t)$。我们有：
- en: '|  | $\displaystyle\Delta$ |  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  |'
- en: '|  |  | $\displaystyle\geq\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    A(t)&#124;}$ |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    A(t)&#124;}$ |  |'
- en: '|  |  | $\displaystyle=\beta\sum_{k=1}^{K}(\sqrt{&#124;C_{k}\cap B(t)&#124;}-\sqrt{&#124;C_{k}\cap
    A(t)&#124;})$ |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\beta\sum_{k=1}^{K}(\sqrt{&#124;C_{k}\cap B(t)&#124;}-\sqrt{&#124;C_{k}\cap
    A(t)&#124;})$ |  |'
- en: '|  |  | $\displaystyle\geq 0$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq 0$ |  |'
- en: 'Thus, we have:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得出：
- en: '|  | $\displaystyle\Delta$ |  | (16) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  | (16) |'
- en: '|  |  | $\displaystyle\Rightarrow F(A)\leq F(B).$ |  | (17) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Rightarrow F(A)\leq F(B).$ |  | (17) |'
- en: ∎
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Definition 2. (Submodularity) A function $f:2^{N}\rightarrow\mathbb{R}$.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 2.（次模性）函数 $f:2^{N}\rightarrow\mathbb{R}$。
- en: Lemma 2. Our value function $F$ in Eq.([13](#S3.E13 "Equation 13 ‣ 3.4 In-rationale
    Progressive Distillation ‣ 3 Proposed Method ‣ Keypoint-based Progressive Chain-of-Thought
    Distillation for LLMs")) is submodular.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 引理 2. 我们在 Eq.([13](#S3.E13 "方程 13 ‣ 3.4 理论进化蒸馏 ‣ 3 提议的方法 ‣ 基于关键点的进化思维链蒸馏方法"))
    中的值函数 $F$ 是次模的。
- en: Proof.
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'We define two triad sets $A,B$. Then we have:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义两个三元组集合 $A,B$。然后我们有：
- en: '|  | $\displaystyle\Delta$ |  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $\displaystyle=\Delta H(\{x\})-\Delta H(\{x\})$ |  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\Delta H(\{x\})-\Delta H(\{x\})$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  | (18) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (18) |'
- en: 'Given that $x\in N\backslash B$. Then, we have:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $x\in N\backslash B$。那么，我们有：
- en: '|  | $\displaystyle\Delta$ |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  |'
- en: '|  |  | $\displaystyle\ \ \ \ -(\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;+&#124;C_{k}\cap\{x\})&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    B(t)&#124;}).$ |  | (19) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\ \ \ \ -(\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap B(t)&#124;+&#124;C_{k}\cap\{x\})&#124;}-\beta\sum_{k=1}^{K}\sqrt{&#124;C_{k}\cap
    B(t)&#124;}).$ |  | (19) |'
- en: 'For convenience, we denote $x_{k}=|C_{k}\cap A(t)|$. Then, we have:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们记 $x_{k}=|C_{k}\cap A(t)|$。那么，我们有：
- en: '|  | $\displaystyle\Delta$ |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Since $A\subseteq B$. Therefore, we conclude:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $A\subseteq B$。因此，我们得出结论：
- en: '|  | $\displaystyle\Delta$ |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  |'
- en: '|  |  | $1$2 |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  |'
- en: 'Then, we can derive:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以推导出：
- en: '|  | $\displaystyle\Delta$ |  | (20) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Delta$ |  | (20) |'
- en: '|  |  | $\displaystyle\Rightarrow F(A\cup\{x\})-F(A)\geq F(B\cup\{x\})-F(B).$
    |  | (21) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\Rightarrow F(A\cup\{x\})-F(A)\geq F(B\cup\{x\})-F(B).$
    |  | (21) |'
- en: ∎
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
