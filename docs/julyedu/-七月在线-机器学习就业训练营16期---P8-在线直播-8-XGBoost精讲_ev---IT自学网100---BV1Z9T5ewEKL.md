# 【七月在线】机器学习就业训练营16期 - P8：在线直播：8-XGBoost精讲_ev - IT自学网100 - BV1Z9T5ewEKL

![](img/f211ca9eb023de8b4fbf89afc212667d_0.png)

嗯如果没有问题的话，我们就准备开始好吧，额按照咱们这个课程安排啊，今天呢我们要介绍的是超级boost模型呃，这个模型呢其实我们从第一次上课的时候，就介绍到了这种呃，或者说为他进行了一个准备。

为什么这么说呢，大家其实可以看到，x j boost模型的先导模型是GBDT，就是那个梯度提升决策树，然后梯度梯度提升决策树的前序模型呢，是BDT是集成决策树，而集成决策树呢是由数模型。

采用所谓的集成方法来构建的，一种所谓的集成学习模型，而数模型呢我们也已经介绍完了，包括id3C4。5和cut数，当然boosting我们上次也介绍了一部分，那么从这个过程大家可以体会得到。

像查查t boss的这种模型的工作哈，都是在前序的工作的基础之上，又进行了适当的改进和调整，包括原理方面的，也包括工程方面的，所以得到了一个很好的效果，也就是说哈这个咱们之前的一些的工作啊。

其实都是为了这个模型所做的一个准备啊，这是需要介绍一点的，那么刚才提到了就是charge boost模型呃，前序的准备非常的充分，或者说它是在前序模型的基础上做的改进，那么在性能上的话。

它取得了非常好的效果啊，不管是在这个比赛当中，还是在这个实际工作当中啊，他就不是它的这个性能还是比较突出和优秀的，性能呢体现在两方面，一方面呢是这个模型的准确度啊，嗯不管是回归还是分类啊。

x g boost都可以很好的能够完成一系列的工作，那么呃第二方面呢就是从这个运算的速度上啊，刚才是刚才是这个准确性是吧，另外一方面就是速度上，即使是我们基于海量的啊高纬的数据。

那么由于charge boost，在工程方面做了大量的优化和改进，所以说它在这个速度性能上，也是带来了非常好的一个呃效果简单点，简单一点说呢，就是这个模型呢就是计算的准还算的快啊。

所以说哈在各大比赛以及实际工作当中，应用的其实还是比较广泛的呃需要提醒一点啊，差几boost模型呢并不是所谓的深度模型，当然是我们聚焦在深度模模型，如果是以深呃人工神经网络啊为基础的。

这一系列的神经网络模型为基础的，这个视角当中呢，x g boost会显然的不太不太一样啊，包不光是模型结构还是这个优化策略，都不是特别一样啊，这一点希望给大家这个说明一点，也就是说不要迷信啊。

讲到这个人工智能，讲到这个人工智能是吧，现在好像这个呃，大量的工作都是在这个深度学习领域，这没问题，但是呢除此之外哈，其实还有很多的方向都是可供大家啊，需要学习参考的地方啊，多说那么一点。

这是关于x g boost，如果没有问题的话，我们就开始今天的内容好吧，OK那么关于主要内容呢包括这么几方面，一方面呢是X级boss的原理部分，另外一方面呢是应用部分，那么在原理部分呢。

我们需要顺着刚才的那个啊发展的脉络，从这个提升方法做一个简单的回顾，因为我们上次已经介绍到了是吧，包括提升方法和这个BT嗯，我们上次都已经介绍完了，那么这次呢重点聚焦到GBDT梯度。

提升决策树以及呃x g boost本身上啊，当然这一系列的工作啊，其实向前推进的过程当中，都是为了介绍这个x g boost啊，这是在原理部分上，那么应用部分上呢，包括这么几方面啊，一部分是关于参数的。

就调参的参数的选择，包括通用参数提升器的参数，还有学习任务参数呃，有在参数的基础上呢，我们会介简单介绍一下基本应用啊，基本应用包括这个X级BBOOST和莱GBM的一个，对比分析。

以及随后的一个来GBM的应用，当然后续的这一部分内容啊，不作为我们的重点，我们重点还是聚焦到呃差距，box t boost的这个原理部分啊，这是说一下我们今天的主要内容，那么我们就继续往下。

那么从差距boss的这个名称哈，就是这个模型名称上，其实我们也可以看得到它的一个特点，什么特点呢，所谓的charge boost，其实就是在我们GBDT的基础上，进行了一个扩展啊，或者极限的一个提升。

这样的话就构成了所谓的charge boost，那很显然我们需要了解的就是GBDT，GBDT呢，这也是今天我们需要简单介绍的，他是在BDT的基础上使用所谓的梯度信息啊，一阶梯度。

这样的话就构成了在BDT的基础上，使用异界梯度，就构成了我们的GBDT，在GBDT的基础上做相应的拓展扩展改进，就是我们今天的x g boost，当然我们已经熟悉的是。

BDT是由我们的boosting和决策树所构成的对吧，那这样的话我们一个顺序应该是什么，应该是当然额决策树是作为一个基础模型，然后呢是将介绍boosting，然后是BDDT，然后是JBDT。

最后是charge boost，这个过程，其实是通过本身的这个模型的这种性能性质啊，所决定的，那我们简单的回顾第一部分，关于这个集成方法，boosting方集成方法呢两部分上次已经介绍了。

分别是所谓的加法模型和前降分布算法，所谓的加法模型就是将一系列的基模型，BX伽马M和他的这个全职系数BM，进行一个累加，得到的我们的FX，那这个时候呢，我们的优化目标，就是在整个加法模型上来完成的。

所以你会发现，当我们带入到损失函数里面去，实际输出和整个的模型输出计算损失以后，在整个数据集上进行累加，这是我们的优化目标，因为它是一个加法模型，所以对它的直接的优化其实是呃比较困难的。

那这个时候我们就采用在加法模型的基础上，所谓的前项分布算法逐步的去优化我们的损失，以期最终逼近我们最终的这个加法模型的，这个损失，那么所谓的前项分布算法是一种优化的思路啊，他是将我们的加法模型啊。

可以从前向后，每一步只学习一个奇函数及其系数，逐步逼近我们的优化目标，那这样的话呢，整个的优化过程就变成了在损失函数里面，只需要计算的是实际输出，YI和当前的一个G模型。

贝塔b xi伽马之间的一个损失计算，当然还是在整个数据集上来完成的，那么很显然，这个单独模型的优化，要比上面加法模型的优化的，这个困难度要减少很多了，当然这是我们的一个期望，或者我们的一个预期。

这个预期的实现嗯，还是有一些这个步骤的，因为很显然这两个优化目标是不一样的，那么怎么样使得你用这种简单的方法去逼近，这个加法模型，其实还是有一系列的步骤的，这就是所谓的切向分布算法。

切线分布算法告诉我们的是在数据集T上，X1Y1到XNYN损失函数L是已知的，以及我们的奇函数集合啊，BX伽马也是已知的，注意这是个函数集合，也是我们的模型集合，这里的模型呢并不唯一是一啊。

是一个由若干个元素所组成的一个集合，当我们进行加法模型的优化过程当中，或者说在我们的前项分布算法的过程当中，每一次我们所优化的那一个G模型，都是属于这个及模型集合当中的一个元素啊，在这个元。

在这个集合当中不断的去寻找就可以了，就是不断的去找一个优化，找一个再优化，找一个再优化就可以了，那么输出的当然就是我们的加法模型FX，那么切向分布算的步骤啊，第一步啊就初始化F0X等于零，强调一点的是。

它同样也是个模型啊，将所有的输入都映射成了输出零，只不过呢它是在D0布上啊，作为一个性能非常差的一个基础模型所存在，我们就是在F0X等于零的基础上，不断的构建新的基模型进行进行优化。

加入到这个初始模型当中去，来完成我们的加法模型的一个训练过程，从第二步开始，我们令M从121直到大M步啊，我们不断的在进行这个循环优化，在循环优化的每一步当中，我们都是使得损失函数的实际输出。

和我们上一个模型的输出，加上当前模型贝塔BX伽马构成的新模型，在数据集上的一个最小优化结果啊，注意哈，我们的优化目标并不单纯的是基模型的优化，而是是在上一次模型的基础上。

加上当前G模型在损失函数上的一个优化，那么很显然这个地方的FM减1X，以F0X为基础，每一步的前一步的模型其实都是一致的啊，这一步是我们作为一个基础模型，不断的在向上迭代，向上迭代。

每一步都是在前一步的模型的基础上，进行一个局部的一个优化，那么当然当当，我们有了当前模型的贝塔M和伽马M以后，更新当前模型FMX等于FM减1X，上一步的模型，加上当前这一步已经优化得到的贝塔MBX。

伽马M有同学会问了，这个优化过程怎么去完成，其实这个优化过程就是一个学习过程，我在数据集上，使得损失函数极小化的那个优化过程啊，一以我们的数模型为例，后面我们也讲到了是吧，以我们的数模型为例。

其实这一步就是一次数模型的学习啊，很多同学啊呃不太清楚哎，这个损失函数放在这，那优化过程在哪，优化过程就是使这个损失函数极小的，那个优化过程，你说具体是实现在哪个地方。

那这一步的具体实现就是我们以以数模型为基，模型为例的话，那就是单棵树的一个学习过程，当然是在FM减1X加上那颗单颗树构成的，那个模型的基础上，使得数据集上的损失最小好吧，那么当完成了以上这一步以后。

我们得到了一个新的DM步的模型FMX，那这个时候随着循环的不断的深入，这里的FMX在不断的进行优化是吧，当M等于当小M等于大M的时候，我们的优化过程迭代结束，那这个时候最终得到所谓的加法模型。

就是FX等于F大，MX等于3mm从一到大M贝塔m bx伽马M啊，这是我们上次所介绍到的啊，这个时候有同学还会有疑问啊，就这个大M怎么去确定，当然这个大M可以是事先设置的啊。

你认为比如说我迭代100根是吧，我前项分布算法过程当中，我迭代160，这是一种策略，另外一种策略呢，很显然也可以，通过你每一次学习的这个实际输出的，FMX和上一次这个FM减1X之间的差，值的绝对值啊。

就是我们可以计算一下，计算一下当前F模型减去一个F减1X的差值，就是这是我上一步的性能，这是我当前步的性能，这个差值如果小于了一个阈值，就是我每一次优化其实已经有很有限了啊。

就是每一次的提高非常非常有限了，这个时候我基本上也可以停止整个优化算法了，就像我们的这个考试一样是吧，每一次我都是呃97分96，97。一九十七。2是吧，这个时候已经没有太大的提升空间了。

这个时候优化算法也可以结束掉了，那么这个时候BDT键介绍完了以后，BD这个BOSTON介绍完了以后，下一步就是BDT啊，提升决策数，提升决策树呢，就是以决策树为基模型构成的提升方法啊。

那这个时候把我们的基模型，每一个基模型都以数模型作为一个特例实现，也就可以了，那么其实你可以看到，在形式上我们的以提升绝对数模型，就是一个FM等于SMM乘以一到大MTX，大欧米伽M。

这个时候你会发现没有那个B啊，没有那个贝塔，这个时候我们认为所有的数模型啊，都是这个都是这个等全值的啊，就可以了，也为我们的优化简便一点啊，其中这个参数就是我们的一棵树啊，就是我们的一棵树。

那继续往下嗯，那也是BDT的这个前项分布算法，其实和我们的这个boosting的前向分布算法，是一样的啊，同样是在F0X等于零的基础上，DM部的模型是FMX等于FM减1X，加上一棵树就可以了。

这是我们的优化目标也相应的发生了变化啊，就是在我们实际输出Y和FM减1X，加上那棵树作为当前模型的基础上，进行损失计算，在数据集上进行最小化就可以了，那么这个时候我们具体的看一下。

就是已知我们的数据集X一Y1直到XNYN，将我们的输入空间RN进行一个呃，令我们的xi属于我们RN的一个元素，那这个时候的话X是我们的输入空间，同样vi是属于画外，是属于一个R的，注意这是一个R。

这很显然是一个回归问题是吧，我们先处理回归，那这个时候Y为输出空间，如果我们的输出空间化，X被划分成了这个互不相交的区域，R1R二一个RJ啊，并且在每个区域上都确定相应的输出CJ。

那么决策数就可以表示成以下形式啊，刚才我们讨论关于这个BDT的时候，我们知道是以数模型作为这个基模型来构建的，那这个时候我们明确的给出，当前这颗树模型的定义，我们上一次在讲这个决策式的时候。

也已经强调过了，数模型本质，就是对我们输入空间的一个离散化划分，划分成R1和R这个空间，然后每一个空间给出一个相对应的输出，这个时候就构成了所谓的决策树模型，那么交叉数模型的表示这个形式化的表示。

形式上我们可以认为我们是需要遍历啊，当有一个输入X进来以后啊，我需要遍历，那而那那这个第三空间，那这便利过程我们使用一个累加过程来对应，那这个时候我们需要判断当前输入的X，是否属于某一个RG空间。

那么从R1R二一直到RJ我需要遍历一下，看后面这个指示函数是否是成立啊，指示函数的条件是否成立，当X属于某一个输出空，输那个输出空间的时候，那这个时候指示函数里面的条件为真，返回一。

那么对应的就是当前这个空间的输出值CD，当这个便利过程完成以后，一定是由某一个空间啊，某一个RG空间对应我的X，那这个时候输出的就是当前那个空间的CJ，那这个时候有了关于数的模型以后。

我们下一步就可以以具体的模型形式啊，模型形式作为我的这个呃数模型形式，带进到我们的前项分布算法过程当中去，也就可以了啊，大家可以看到啊，在这个过程当中。

和我们的BDT这个boosting过程其实是一样的，就是以我们的决策树模型，作为一个基模型的替代就可以了，那么重点呢，其实在这个地方我觉得其实是更重要的一点，为什么这一点呢，上次我们也讲到了这一点。

当采用平方损失的时候啊，当我们采用平方损失的时候，我们知道这是个标准的平方损失函数的定义，LYFX啊，实际输出和模型的预测输出的差值的平方，左右的损失，那么在java模型当中。

我们把我们的这个每一步的这个呃模型展开，会发现它是由FM减1X，加上当前要学习的那棵树模型构成的，当前部模型，那这是实际输出Y，那这个时候的损失函数计算一定是，Y减去FM减1X减去那棵树的输出的平方。

加上平方值，那这个时候呢我们会发现当前这三步当中哈，Y是我们的实际输出，是数据集里面的已知值，这个FM减1X，在当前DM部的模型的构建过程当中，上一步嘛，上一步的模型基于当前这一步。

DM部也是已经知道的了，因为我们每一次都是在上一步的基础上，再构建一个新的模型，那么上一步的模型基于当前，这一步的模型是个已知值，他也是知道的，那么也就是说只有当前布的这颗树模型，我是不知道的。

那好如果我们把这里的Y和FM减1X，这两个已知量啊，既然是已知嘛，就可以加括号先算出来，算出来呢作为R值啊，算出来作为R值，那R值呢也是已知值，那这个时候我们在加法模型的每一步的损失，计算过程当中。

其实等价于计算一个R减去T的平方，那么R呢，如果我们认为它是因为R是个已知值嘛，反正他是个已知值，某一个数据集的输出对吧，我不管你是哪个数据集啊，反正这个数据集的输出我是知道的了，那么在此基础上。

我要计算它和我们一棵决策树的差值的平方值，同样对应到我们一个一般化的这个损失函数，额平方损失函数的定义过程当中，大家会发现，其实它和我们的一般化的平方损失函数，在形式上是完全一样的。

大家可以看到这里的R反正是已知的，我不管我不确定于哪个数据集，但反正它是已知的，它就对应的是我们的实际输出Y，那这个时候的T其实就是一棵树模型，它对应的就是一般化的我们的一个FX。

那这样的话问题就退化成了，我要在实实际输出R上训练一颗决策树模型T，然后去计算他的损失的问题，使当然使这个损失最小的那个T，就变成了我最优的那颗T，那你会发现这个问题就已经退化成了，我们上堂课的。

上次课的讲到的那个什么，那个那个决策树模型了是吧，也就是说你会发现问题通过这一系列的转化，已经我们看不到所谓的什么嗯，加法模型和前项分布算法了，他就退化成了一个在已知数据集上进行。

一颗决策树模型的构建的问题了，而这个问题我们不管是从i id，344。5还是卡的数都已经很好的解决掉了，那也就是说问题转化成了什么嗯，转化成了当我在第M步的学习过程当中，其实我只关心的是。

我要构建一个新的数据集R啊，新的数据加R，当然这个R的计算其实很简单，就是Y减去FM减1X，而我在当前这个已知数据集R上，再重新学习一棵树就可以了，那换句话说，我每一步的数的学习。

都是在实际输出和上一步模型的那个差，值上进行学习，我们下面上一次已经介绍过了，其实这个这个结果其实非常呃，有意义的地方在于实际输出是Y减去FM减1X，注意这里的FM减1X是我们上一步的模型。

那么他俩的差值很显然是上一步的模型，没有很好的反映，我们实际输出的那个结果的那个差对吧，我们每一次都有一个考试，考试免不了都会出现错误的情况啊，就是错题我复习的有，我复习需要有针对性。

既然我考试验证了我掌握的内容，其实我的经历其实就并不一定要在这方面，我的主要精力就要放到我上一次考试，那次没考好的地方，所谓的错题的情况，我把精力放在这，那么很显然我下一次再考试的时候。

一旦出现了我上一次没考好的那个情况，出现了以后，我当然因为我前面都已经对吧，做过处理努嗯，相对应的都已经把这问题都解决掉了，那这个时候的性能肯定是越来越提高了，这就是加法模型呃。

他的一个策略在这个呃BDT上，1BDT上的一个体现，非常的有针对性啊，这一部分被称之为是，就是我们实际输出和我们上一次模型的预测，输出之间的这个差值被称之为是残差啊，残差。

那也就是说所谓的加上分前项分布算法在呃，加法模型，在前向分布算法的过程当中的学习过程，其实就是每一步构建一棵决策树，去拟合上一步的残差就OK了，所以你看一下我们的具体的学习过程。

已知输入数据集要要求输出，我们的最终的BDT模型F0X等于零，这是没问题的，然后呢在循环过程当中哎，我们不再是直接对那个LYI，FM减1X加T进行优化，而是直接先计算一下残差。

我先算一下YI减去FM减1X2，作为当前部的残差值，有了这个残差值之后，哎我拟合残差学习一个回归数，得到我们的决策树体，而就这一波而言，就是我们上一次课讲到的，不管是id34。5还是cut数。

你把它展开放在套在这一步里就可以了，当我们得到这个新的决策数T以后，那下面就是更新我们的FX在FM减1X的基础上，加上那个决策树就OK了，那这个时候通过我们的上述循环，最终当达到第M步的时候。

我们最终得到回归，提升决策数F大MX等于sin m，从一到大M我们的决策数相加就可以了，好了，这就是关于这个BDT部分，BDT以那个boot和BDTD部分，这是我们上一次所介绍的内容啊。

这一部分内容是我们的回顾啊，这个看看大家有什么问题吗，没有问题的话，我们就继续哈，那下一步好在讲这个GBTT的机之前呢，我们简单的回顾一下哈，我们的一个基本的一个策略，就是我们有输入空间画X。

有输出空间画Y，我们要做的是X1X2。2。1到XN到，Y1Y2。2。1到YN的一个映射的问题，等下把它映射过去，而我们又知道，满足这个从X到Y的映射的这个F2，F1F2点点点，它是有若干个。

所以我们需要有一系列的策略，当然不管你是结构风险最小化，还是经验风险最小化，反正找到一个F星就可以了，但事实上大家再注意一点，就是我们经常会发现，在我们的模型的构建过程当中。

决定模型形态的其实是模型的什么参数，不管是我是用theta表示还是用W表示，其实还是一个参数空间CA1CT2得2A点，那这个时候你会发现哎，所谓的最优函数F星，其实是由相应的参数来决定的，那么也就是说。

我们的学习过程，其实并不是在模型空间F空间里面啊，不是在花F空间模进行学习的，而是在我们的参数空间C塔里面进行学习的，所以你会发现我们的学习策略是构建一个，关于当前参数的损失函数YI和，FC塔X。

然后再和我们的什么，和我们的参数进行一个偏导计算，这个时候，我们假设我们前面的这个学习率是阿尔法，然后再在C塔的基础上得到一个它的迭代方式，因为我们的求解目标是C它，所以你会发现我们的损失函数啊。

比如我们还用L来表示啊，我们损失函数是对我们的theta求偏导，因为我们是在C大空间里面进行学习的好了，如果我们我们想当然的认为啊，想当然的认为，如果我们的这个模型空间里面的每一个模型。

F都不是由C塔来决定的，换句话说，我不必然的有这么一个所谓的参数空间，这么一个概念的话，也就是说我这个模型不是由参数决定的，我只有只有模型，没有什么参数，没这么东西，那么按照刚才的套路。

其实其实不变的在于我需要计算的还是F呃，X对吧，对谁的偏导，这个时候你会发现哎，就麻烦就在于，之前我是在参数空间里面对参数求偏导，现在我没有哪个参数空间了对吧，那这个时候我对谁求偏导。

那你是不是可以直接就用你的函数求偏导，那这个时候再加上得再加上你的学习率，然后呢你是在上一步的那个模型的基础上，在新的模型的啊负梯度方向上得到一个新的F，在逻辑上是完全一样的，大家发现了吗。

这是啊关于这个GBDT的一个，这个就是一个思路，就是我们嗯，由于我们在前面的一些大多数的模型当中啊，都是有这么一个参数以及参数空间的概念，我们在形式上更习惯于对C塔，或者对参数的偏导计算。

因为你的求解目标就是参数，所以你当然是需要找到你是在theta空间里面，你要找到一个最低的CA星作为你的模型参数，但现在问题是，我们把它换掉对吧，我们不在C塔空间里面，我们没有C塔那个概念。

我们直接是就是在F空间里，模型空间里面，我们同样找到的是F型，那同样还是从L向F空间求偏导，得到负梯度方向，以使它能够收敛到最低点，作为我们最终的那个自由模型不就可以了吗，啊这是做的一点铺垫。

其实有了这一点铺垫以后，你再回过头来再去看这个所谓的GBDT，其实就一目了然了，为什么，因为有了这个过程以后，你会发现它再结合着我们前面刚讲到的，卡瓦模型和前向分布算法问题，就变成了什么，其实简单点说。

就一句话，梯度提升，梯度提升算法使用了损失函数的负梯度啊，使用损失函数的负梯度，这是损失函数，损失函数的负梯度，在当前模型的值啊，对我们的函数进行一个负梯度计算，在当前模型的值作为回归问题。

提升决策树的当中的残差的一个近似值，来拟合一颗回归树啊，就像刚才我们所说的啊，我们就是在模型空间里面啊，就是在模型空间里面，既然在模型空间里面，我们找到了那个使模型实损模型的损失最小的，那个方向。

就是我直接对F进行求偏导，它的负方向，负梯度方向就是我下降最速的那个方向，我在它此基础上有了这个负梯度方向值以后，我们每一次的学习时，他它使得它越来越小，越来越小，就完成了我们优化的一个目标。

那么看下面梯度提升决策树的算法啊，其实非常简单，就是在输入的基础上，还是输入数据集和我们的损失函数啊，输出呢就是我们的模型初始化还是F0X，当然这个地方需要注意一点，有同学这个地方就有点懵。

唉刚才都是F0X等于零吗，为什么这个地方变成了FX0，后面这么复杂的一个式子了，后面这个复杂式子，其实仔细看一点也没太大的一个困难在于，我现在计算的是什么，计算的是一个在损失上。

实际输出Y和我们的一个常量C，注意啊，这个C啊是个常量C之间的一个，数据集上的一个损失的极小值，也就是说哈，你这个时候即使找到了一个F0，X是通过这么一次学习得到的，也无非得到的就是个常数。

比如说F0X等于八或者等于七，当然只是只是举例啊，有时你会发现他和那个F0X直接让它等于零，在性能上其实区别并不大，但是呢这个的好处在于，它是通过一个学习得到的结果啊，不管是数值几吧。

性能上一定要比这个零可能要好一些，但是呢因为这个模型本身是个常量模型啊，就是那个C啊或者是常量函数，所以这个性能上有提高，提高非常非常有限，所以啊如果你理解了，你认为OK没问题。

如果你你说哎呀这个地方实在不理解，那就让它等于零就可以了，如果你还需要再理解理解，我们上次其实举多过一个例子，举还记得在我们第一次上课的时候，讲到过那个那个什么那个欠拟合现象啊，过拟合和欠拟合的时候。

我们讲到过，比如我们那个正弦曲线，还记得那个，正弦曲线啊，还记得那个F0等于零是在这儿，我们有一次变化了以后，其实是F0X，其实是可以让他还是同样是作为一条直线，但并不一定这个呃是F0等于零了。

而那个时候你会发现，即使还是一条直线，性能上有提升，但是性能提升的非常有限，其实非常类似于在现在这个场景上，好吧，这是第一步啊，这个很多的时候，大家在这个地方都有疑问和困惑，解释一下。

当有了这个基模型以后啊，基础模这个不是基模型啊，当有了这个F0X作为一个第零步的模型以后，那么我们从一到大M每一步我都需要计算的是，I从一到N在整个数据集上来完成一个嗯，不再是残差的计算。

而是残差的一个近似值的计算，就是刚才我们所说的就是我们的损失函数，基于函数的负梯度方向啊的一个值，用这个值来作为我们残差的一个近似值，其实啊并不再有残差的概念了，因为标准的残渣概念。

是在平方损失的基础上才有的啊，这是在上面这个残渣的概念啊，是在标准的平方损失的基础上才有的，那么在BDT的基础上啊，呃有残差的概念，但是在JBDT的基础上，其实完全可以不提残差这个概念。

因为刚才我们是我们已经分析了，我们每一次的学习，就是在函数空间里面，直接对我们的模型函数进行偏导计算，它的负梯度方向就是我们的最优的收敛方向，因为我们还是希望这个损失函数最小嘛，那个函数。

损失函数最小的那个FX就是我们的最UX，所以啊这个地方嗯你需要知道啊，计算这么一个负梯度方向就可以了，有了这个负梯度方向以后，下面的工作啊，下面的工作就是一棵决策树的学习。

只不过我们的学习目标变成了这个，刚才计算得到的负梯度啊，这个过程其实和我们的ID3C四，4。5和卡的数是完全一样的，当我们构建出这个新的数数以后，下面更新FMX等于FM减1X。

加上我们的一颗新的决策树就可以了，有的说唉你上面不是个T吗，怎么下面变成它了，不要忘了我们那颗决策树T展开之后，不就刚才所说的吗，J从一到大GCM，然后I判断一下X是不是属于RM对吧。

只不过是刚才把上一步，那个T展成他的一个标准展，比那个形式化的表述就可以了，然后当我们得到了每一步的FMX以后，那么最终的决策树啊，当最终的梯度提升决策数，FHX就等于F大MX等于sum m。

从一到大M那大M可竖那大M棵树，每一棵树都是J从一到大GCMG，然后判断一下当前的X是不是属于，那当前的一棵树所对应的离散化输出的空间啊，这就是BDDGBDT哈，其实最核心的一点就是对这个式子的一个呃。

理解啊，这个其实还是怎么说呢，嗯需要有一点这种这种抽象的能力是吧，大家看看这部分有什么问题吗，好这个还有另外一个资料给大家看一下这个，咳咳这里呢还有一份资料嗯，为什么要把他介绍给大家呢。

是因为这份资料在刚才那个呃，就是GBDT的讨论过程当中，其实是有一些更详细的更详细的介绍和说明啊，有兴趣的同学呢，这个资料呃我一会整理一下，发到我们群里啊，大家可以看一下，其实你会发现它同样是逻辑。

就是刚才我们所说的，当我们有参数空间的概念的时候，我们每一次的参数都是在上一次参数的基础上，加上一个新的参数的更新值对吧，我们是在我们要聚焦到德尔塔西塔上，而德尔塔西塔其实就是我们的。

你可以理解成模型的负梯度方向，只不过你是在参数来计算的，你可以看到，你看我们就就是在我们上一次的基础上，加上一个德尔塔theta，而德尔塔西塔就是我们需要计算一下一阶梯度，是吧。

找一下这个那个类比式的啊，在这嗯从参数空间到函数空间的一个映射，当我们在当我们你看，当我们习惯于在参数空间里面，来完成参数的学习的时候，我们继续往下，你会发现它和函数空间上。

在上一次的函数上进行新的函数的学习，不是一样的吗，形式上是完全一样的，只不过我们换了一个空间而已啊，没有参数的概念，所以说你会发现哎，我在上一步的C塔等于负的对吧，阿尔法去作为学习力啊。

在负梯度的基础上不断的累加负梯度的时候，负的参数梯度的时候，其实我们累加负的函数梯度是不是也可以啊，是一样的一个概念啊，这个嗯我觉得虽然稍微有一点儿，这个抽象的理解，但是一旦你能够突破这一点的话。

我觉得对大家来说一定是一个，就是我觉得是个很大的一个提升啊，不要拘拘泥于呃习惯性的一个结果，那好了，前面的铺垫啊都作为准备共性的工作，那下面我们正式的开始介绍呃，X级boss的模型啊。

前面的工作啊都是铺垫的内容啊，最终聚焦到这，我们看一下，极限提升决策树啊，极限梯度提升决策树啊，这个东西呢，其实就是在我们之前的那个，上一步的JBDT的基础上做了两点改进，第一点改进啊。

其实可能细心的同学都发现了啊，不管是前面讲到的这个这个BDT也好，还是讲到的这个JBDT也好，其实都没有正则画像，都没有正则化对吧，也没有减值过程，那么这个时候是不合适的啊。

所以呢ti boost的作者啊，陈田基老师的第一个改进就是加入了正则画像，当然这个正则画像设计还是有一定技巧的啊，一会我们再介绍，那么第二项，既然GBDT使用到了负梯度方向。

那负梯度呢我们基本上就是一阶梯度信息，而我们知道按照泰勒展开式呃，理解梯度信息之后，应该是二阶梯度，三阶梯度以及更高阶的梯度，才能完整的描述一个函数对吧，那这个时候如果我们用一阶梯度进行模。

这个模型的学习的过程当中，你会发现那是不是可以考虑二阶梯度呢，对其实第二个改进就是使用了二阶梯度，啊这就是我们在原理方面啊，比较突出的两个改进的地方，当然还有一些细节改进啊。

比如说后面的损失函数定义啊等等其他方面，但是最主要的还是这两点啊，那我们就分别啊，看这两点是怎么在JBDT当中体现出来的，那么在此之前呢，我们还需要介绍一下，就是在GBDT当中。

关于决策树的模型的一个定义啊，在GBDT当中啊，决策树模型的一个定义呃，我们看一下，首先我们已知的还是训练数据集xi到YI啊，这个没什么特别的，其中xi是属于RM的，还是MV的一个特征输入。

那么YI是属于RN额属于R，它是一个连续失值输出，那么很显然是个什么，是个回归问题，D的模等于N，有说元素个数是N个，那么这个时候的就差几boost里面的模型定义啊，就是决策树模型被定义成FX等于W。

下标是QX啊，下标是QX那这个时候啊嗯就不是形式上啊，形式上就不是那么的明显啊，他和我们前面所介绍到的，不管是ID344。5还是卡的数啊，不管是BDT还是GB，DT的模型的定义都不太一样啊，都不太一样。

形式上是不太一样的，但是含义上可以说是完全一样的，为什么这么说呢，我们再回顾一下，我们刚才已经重复了好多遍了，关于决策树的模型定义的本质是什么，一棵决策树的本质就是我们对输入空间，花X的离散化啊。

得到若干个离散化之后的空间，以及若干个离散化之后的空间，当中的每一个空间所对应的什么输出，就是刚才我们所所说的R1R2点I点，一直到RJ，然后呢每一个都对应一个输出，C一C二D2。2，一直到CJ。

这就是就就是决策树，那咳咳，刚才我们看到的那个决策树，在形式上是从J从一到大G然后是CJI，然后看一下X属于RJ，这是我们前面的老套路了，这不是一棵树吗，为什么现在他就长这样了呢，简单的分析一下。

看一下，解释一下这里的W和QX到底是什么东西，看下面其中QQ是个什么，Q是一个从RM到123点点点，遇到大T的这么一个映射，注意啊，Q是一个映射，就在这个地方哈，这里的QX啊，这里的QX是个函数哈。

是个映射，这个函数能干什么，能把一个RM的值映射到1~8T啊，这个集合里面去，那注意一下这个RM是什么RM哪有啊，RM在这RM就是我们输入空间，就是由若干个X所组成的一个输入空间，那这个时候你会发现。

换句话说这里的QX的输入，这里的QX的输入就是我们的一个什么，RM为空间里面的一个点啊，就是我们那个输入特征值啊，就是说QX的输入就是我们的一个输入特征值，那有了这个输入特征值X以后。

你看下它把它映射到哪去了，映射到了12345点点大T里面去了，那这个大T是什么，这个T看后面有有介绍，T为决策树叶子的节点数啊，T为决策树叶子的节点数，也就是说我们要得到一棵树，这棵树长什么样。

我不知道，但是我对你的叶子都已经编好号了，121号叶子，2号叶子得点点，最后DT个叶子，那这个时候的Q能够完成的就是，当我有了输入以后，当我一个输入X来以后，我通过Q5X得到的是。

你当前这个输入X所对应的那个叶子的下标，对应的那个叶子的下标就说我我你给我个X，我虽然不能把，直接把你先映射到那个叶子里面去，但是我可以先通过QX得到，你应该在的那个地方的下标，非常类似什么。

就是你上班的时候啊，每一每每来一个工，每来一个新的同事对吧，我都先给他一个什么，给他一个工牌，这个工牌上记录了，你应该坐在哪个位置上的工位，当你拿到这个T之后啊，当你拿到这个QX以后。

再根据这个QX我们刚才讲到了QX是什么，QX是W的一个下标，看W是什么东西，W在哪，W在后面，W是属于RT的，RT很显然T刚才已经介绍了是叶子的节点个数，那RT是个什么东西。

RT不就是由T个123点点二，由T个元素所组成的一个向量吗，啊注意它的下标分别是从123。2点，一直到大T那时候我刚才把它们组合到一块，你会发现功能就在于，当你一个新的元素X来了以后。

先通过QX得到你先在哪个叶子节点上，然后有了这个QX的叶子节点编号，查一下这个W得到，比如说你是3号叶子啊，你的QX等于三，你找到这个位置上了，然后再查一下这个W的第三个维度上的值。

我们就可以得到相对应的输出了，那么你看一下这个模型和我们前面所讲到的，这个数模型的逻辑是不是是不是一致的，只不过我们区别在哪，区别只是在于，我们刚才讲到的是一个从离散空间的角度上，得到对应的输出。

那么你看看我们现在介绍的这个模型，定义WQX是不是也是给我一个X，先计算一个QX，而QX一定是一个离散空间当中的一个位置，有了这个QX以后，WQX对应的就是当前这个离散空间位置上，所对应的那个输出值。

形式上虽然不一样啊，形式上可以说是很不一样啊，但是原原理上是完全一样的一个东西啊，原理上是完全一样的东西，你现在回过头来再看一下这个FX的定义哈，再看一下这个FX的定义，大家应该就能够明白了啊。

这个过程刚才已经介绍了对吧，你给我一个X啊，我先通过QX计算出你是在哪个叶子上啊，知道你在哪个叶子的位置上了，我通过这个W结构或者W数组，其实它就是个数组是吧，我通过查一下这个数组对应的那个叶子位置。

得到的就是当前这个叶子的对应输出值，而这个输出就是你输入X所对应的输出值，这就是在charge boost里面关于决策，关于决策树模型的一个定义，这个模型的定义啊是很有意义的。

一会我们会看到它后面进行这个嗯，推导的就是损失函数的推导过程当中，这个模型的定义其实还是非常，结构上还是非常有意义的啊，一会我们可以可以体现的到啊，关于这个GBD就是x g boost这个模型定义部分。

看看有什么问题吗，如果没有问题的话，我就继续啊继续，弄好了继续，好了有了模型了，我们上面先定义损失，额模型有了以后，看一下提升决策树模型的预测输出，因为刚才只是一棵树啊，只是一棵树。

那么在提升决策树当中，我们无非就是把这若干克数进行一个累加是吧，所以y heat i等于负XI等于K，从一到大KFKXI啊，注意啊，这里的带帽子的啊，带hat的都是模型的预测输出啊，都是模型的预测输出。

考虑到它是一个加法模型啊，它是一个加法模型，所以呢我们是把每一科啊，每一颗决策树进行一个累加，所以这个地方啊是一个sum k从一到大，KFK的一个累加过程啊，累加额下这里的K下标啊。

仅仅表示了我是第几棵树啊，第几棵树，那么有了这个预测输出以后，看损失函数定义正则化目标函数啊，正则化的目标函数，那或看一下，很显然是加上了什么，加上了正则化项是吧，加上了正则画像，看一下怎么定义的。

定义为sum i，然后呢LYI额y hat i，然后是YI，这是我们在一个数据上实际输出，实际输出和预测输出之间的损失啊，这是一个数据上的损失，然后在整个数据集上进行一个累加。

这个地方没没加上下没加这个这个上界哈，你II从几啊，I肯定是从一啊，这个没问题，那上节是几啊，有同学有有印象吗，I从一到几，I从一到N，为什么I从一到小N，因为刚才我们检查过数据集里面。

元素的个数就是小N个，所以你这个地方一定是从X1Y1，一到XNYN，所以这个地方的损失一定是这样来计算的，那么这是我们普通的一个损失函数是吧，普通的这个结构风险额经验风险啊，这是普通的一个经验风险。

那么在经验风险的基础上，我们看一下加上正则化项，正则化项是怎么加的，Sum k sumk，然后呢大欧米伽FK，那么很显然加上了正则化项，那是正则画像里面，这个大欧米伽又是怎么来定义的。

看下面很显然我们前面一直讲到过啊，这个正则化项一定是用来表示当前模型，复杂程度的一个项啊，就当前复杂程度的一个像，那这个时候我们大家需要想一下，一棵树模型怎么表示它的模型的复杂程度。

很多的指标都可以完成这个度量，那比如说我们可以通过什么，可以通过这棵决策树的这个深度是吧，深度啊，当然我们知道数越深，模型越复杂啊，这是一种方式，还可以通过什么，还可以通过当前模型的叶子节点的个数啊。

就是那个大T前面刚才已经介绍过啊，T为决策树叶子节点的个数额，树里面的叶子越多，模型越复杂，反之越小，所以这个地方第一项啊，正则化项的第一项包含我们叶子节点的个数啊，叶子节点的个数，前面这个伽马啊。

前面这个伽马嗯也是我们的一个系数啊，也是一个嗯超三需要设置的啊，用来表示当前你这个正则化项的一个重要性，这是第一项，第二项是个什么东西，第二项是个1/2兰姆达，W的二范数的平方啊，W的二范数的平方。

谁的范数是W的范数，负W是谁，W是在这刚才已经介绍过了啊，其实这个W就是把我们得到的每一个叶子，节点的输出形成了这么一个什么数组，所以我们才对一叶子进行了编号嘛，最后一个是大T。

那么这个里面的值就是W1W2，一直到W大T我们这个值也不能够啊特别的大，或者说用这个值的大小，也表征了当前模型的一个复杂程度，所以这个地方的第二项啊，这个地方第二项是1/2拉姆达呃，W的二三数的平方。

稍微把它展开，展开之后变成了那伽马T加上12拉姆达，1/3，从一到大T啊，因为这个时候我们有大七个叶子，所以把每一个叶子的W都拿出来，进行一个平方向就可以了，好吧，这是正则画像。

那么很显然通过正则画像的加入，使得我们当前构建的这个charge boost的模型，就不太容易陷入所谓的过拟合现象啊，这是第一项改进啊，关于第一项改进，看看大家有什么问题吗，没有问题是吧。

好我们继续嗯好了，有了模型的定义，有了这个损失函数的定义，那我们继续往下看，这个时候的损失函数定义，还是刚才我们所说的是吧，还是在整个数据，就刚才我们说你把它补齐了吗，SUMI从一到N。

我们整个数据集上损失L，加上正则画像就可以了，需要注意哈，哎这个地方看一下，稍微注意一点，注意什么，这个y height i啊，y hat i很显然是模型的实际输出。

而模型的实际输出按照我们前面所讲到过的，我们在DT轮上啊，因为我们的学习过程啊，前项分布算法的学习过程是一步一步的在做，所以当我们看到DT轮上的模型参数的时候。

你会发现这里的LT等于YI实际输出和你看了吗，既然是DT轮，那么就是在DT减一轮的模型输出上，再加上DT轮我们要学习的那棵决策树，作为我第T轮的整个的实际输出的预测输出，再和我的实际输出计算损失。

然后在数据集上来完成累加，再加上唉我DT轮的模型的一个正则化项，构成了我DT轮的目标函数啊，这一项啊，这个有同学也经常会有困惑，困惑在哪，困惑就在你上面这个目标函数，实际输出就是Y额。

预测输出就是y hat，然后实际输出就是YI，为什么在下面的这个损失函数里面，又你你那个Y害他为什么又等于这个式子，是因为我们整这里的one hat哈，这里的one heat是我们整个学习之后的啊。

最终的那个模型的预测输出，而这下面这个式子啊，下面这个式子说的是我在DT轮的学习过程当中，损失函数的具体形式啊，这是不一样的，所以你会发现上面这个式子里面，直接就是实际输出和预测输出之间。

的损失函数计算，而在下面这个式子里面，由于它是DT轮，所以我必须要把DT轮写成，按照我们加法模型的要求，写成DT减一轮的输出和我当前轮的模型的和，作为我DT轮的预测输出，再和实际输出做损失计算。

然后再加上正则画像作为我的目标函数啊，这个地方的困惑啊，解释清楚了啊，好了重点来了哎呀重点在哪呢，这个地方就比较慢一点了，嗯好好玩来看，既然我们已经完成了，第T轮的目标函数的一个定义啊。

这个定义形式上其实已经很嗯，已经细节信息已经很丰富了是吧，YI是已知的y height t减1I也是已知的，FTXI是未知的，因为是DT轮要构建的那颗绝对树啊，I从一到N是在整个数据集上。

然后呢大欧米伽ft t啊，当欧米伽F的定义在这啊，F这个这个这个当前这颗决策树的叶子个数啊，以及W的值啊，在FT减一上也是已知的，但问题在于下面的一个工作，我们需要在DT轮的目标函数的基础上。

在one height t减一处进行一个二阶泰勒展开啧，这可能对大家有时就是一个挑战，为什么对大家来说是个挑战呢，因为为什么要进行二阶泰勒展开啊，这就是个很大的一个困惑嗯，如果联想一下。

我们在JBDT上的一个学这个这个一个介绍，可能这个困惑就会减少一些，因为很显然，我们在GBDT的基础上要进行模型的学习，我们是要计算一个负梯度方向，而那个负梯度方向是通过我们的损失函数对吧，还记得吧。

是我们的那个损失函数L，FX和Y对我FX的偏导的负值的一个学习目标，那么现在呢我们前面讲到过啊，你这仅仅是使用到了一阶梯度信息，因为只有求一阶导数这个精度啊，这个精度一定不比我们的二阶梯度的精度要大。

也就是他他还小一些，那这个时候我们如果是不是可以哎，如果在此基础上，我们再求一下二阶梯度，是不是更精度更高一些，这就是为什么我们要这在这个地方讨论，选目标函数，在y head的T减一处的一个。

二阶态的展开的一个问题，好吧，这就是我们的一个目标，就是我们看一下这个二阶梯度信息是多少啊，这个作为如果我们作为优化目标的话啊，这个精度就要比刚才的一阶梯度信息，更加的丰富是吧，精度会更高。

那么既然要讨论到二阶梯度展开，又不得不回过头来看一下这个式子，看哪呢，我们看一个式子啊，看一下上面有一个展开式，可能对于额部分同学来说，这个泰勒展开式已经忘的差不多了啊，我们也不需要讨论啊。

不需要严格的讨论，这个具体展开的这个细节部分，我们看一下形式上怎么去看一下，一阶泰勒展开啊，FX的一阶态勒展开，它约等于F0啊，在X0处的一个函数值加上F1到X0，乘上一个X减X0是吧，这是一阶梯度。

二阶梯度啊，展开是FX约等于FX0啊，在X0处的函数值加上一个F1等于X0，乘以一个X减X0，再加上一个F0到X0，乘以一个二分之X减X0的平方对吧，这很显然有了二阶梯度信息。

那么这是一个在X0处的展开，如果我们把它写成迭代形式啊，写成迭代形式，什么叫迭代形式啊，就是这个地方的X0呃，这里的是X减去X0，是个在X0处的展开，我们把这里假设XT等于XT减一，加上一个德尔塔X啊。

我们是在上一次的那个XT减一的基础上，又增加了一个变化量，德尔塔X作为我XT的一个当前值，那这个时候FXT在XT减一处的泰勒展开，就是下面这个式子，FXT等于FXT减一，加上一个德尔塔X啊，这个注意啊。

这是个等式啊，FXT就是FXT减一那个位置上对吧，上一次那个位置上加上加上了一个增量，德尔塔X啊，加上那个增量增量X作为我FX的一个输入，那这个时候展开之后怎么展，展开之后约等于FXT减一。

其实我们就是以FXT减一处，作为我的它的展开位置啊，函数值吗，函数值加上一个F1导XT减一，乘以一个德尔塔X，按照刚才那是X减X0，X减X0不就是个增量吗，这个增量不就是等于德尔塔X吗。

再加上一个F两导，XT减一乘以个二分之X减X0，增量的平方就是德尔塔X的平方啊，这是我们这个奥数里面的结论，你拿过来用就可以了，我们要把它拿过来用，用在哪儿看一下，分清楚谁是谁就可以了，谁是谁。

什么叫谁是谁，看这两，我们既然是要对我们的目标函数，在f heat t减一处进行二阶泰勒展开，很显然这里的LT就是这里的FXTLT，就是这里的FXT，然后呢按照我们刚才所说的LT等于什么。

等于y i y heat t减一，加上一个FTXI，注意啊，你需要仔细对一下这里的XT减一，就是这里的y height t减一，有同学转不过来啊，这个你这里是Y这里是X能能是一样的吗。

抽象首先就是抽象嘛是吧，注意这里的X就是模型的，就是函数的输入啊，就是函数的自变量，你可以发现这个模型的这是关于F的自变量，那这个模这个函数L的自变量，是不是也是y hat t减一是吧，这是XT减一啊。

就是这里的Y害的T减一，这里的FTXY就是这里的德尔塔X啊，就是你每一次要新增加的那个增量，有了这个对应关系以后，在下面约等于号了啊，约等于号约等于SM从一到N其他都不变。

然后这里的L是y i y HT减一，就是这里的YXT减一，在T减一处的函数值加上加下面加什么，加下面这个式子是F1导XT减一，对于我们的XT减一求偏导吗，你看损失函数对Y害的T减一求偏导。

然后呢乘以一个德尔塔X，乘以一个德尔塔XFTXI刚才也说过，这里德尔塔X就是FTX再加加上什么，加上一个F两导XT减一，这里你看这是F0到XT减一，然后呢再乘以一个二分之德尔塔X的平方，1/2。

往前提1/2往前提，德尔塔X的平方就是FTX的平方好吧，所以说啊从这从上面这个目标函数定义开始，以及在y head的T减一处的二阶泰勒展开，完全其实就是一个负号提速，完全就是一个符号的一个替换啊。

这个地方你对应着每一个符号啊，它的展开式的，每一个符号一一的进行对应就可以了，好吧嗯，这一部分有什么问题吗，兰姆达也是权重系数吗，哪个兰姆达，哪个兰姆达，你说这里的兰姆达吗，额这不是模型的选值系数。

这是我们的呃，正则化项当中的一部分的一个超参数啊，这不是那个全职系数和模型的全职系数没关系，好吧，为什么要把叶子节点加到损失函数里面去啊，是在这是吧，前面我们讲到过啊，正则这个过拟合的问题啊。

过拟合欠拟合的问题，过拟合项就是模型太复杂了，我们需要在正则画像中项当中加入，表示模型复杂程度的一个量，那这个时候怎么表征当前模型的复杂程度，叶子的节点个数是可以用来表示模型，复杂还是不复杂的。

就像前面我们讲到的，如果我们只有这个经验风险最小化，我们会使得模型很复杂啊，会出现所谓的过拟合，现在这个过拟合现象，而模型越复杂的一个相对应的一个情况，就是模型的不复杂，模型的不复杂。

就是尽可能的希望模型的叶子节点尽可能的小，所以这个时候当我们追求这个呃，损失整个目标函数的小的时候，我们一定知道经验风险是越来越小的，但是相对应的经验风险越来越小，会导致我们的正则化项越来越大。

那这个时候是这两个的结果的尽可能的小，所以一方面考虑到了在数据集上，我们要使损失小，还考虑到了你的模型还不能太复杂，所以这个模型加上正则化项之后的，这个模型就很很很少会出现所谓的过拟合现象，好吧。

这就是这个在目标函数当中加上正则化项啊，那下面刚才介绍到的呢，重点在于这个二阶泰勒展开，所谓的二阶泰勒展开呢其实就是套公式啊，就是我们的啊泰勒展开公式，这个时候只不过你需要仔细的去划分一下。

到底谁是FXT，谁是那个德尔塔X啊，有了这个对应关系以后，展开公式是是没有问题的，好吧嗯，还有一点啊，可能会有同学提提出疑问来提出疑问来嘶，怎么看这个L函数，和你刚才所介绍的那个FXT不一样。

哪儿不一样呢，又说不出来，很显然有一个地方不一样，不一样在哪，细心的同学会发现，这里的L函数是一个二元函数，什么叫二元函数，它有几个几个自变量，它是有两个自变量的，你发现了吗。

那比如说上面这个式子看的更清楚，我们上面最早的时候讲损失函数的时候，说到过损失函数是二元函数，它是由两个量构成的，一个是实际输出，一个是预测输出，才能计算这个这个才能计算损失是吧。

所以哈你会发现除了这一点不同之外，二元函数那就二元呗，更重要的在于这里的YI还是个实际输出，是个已知值，那它既然是已知的，也不妨碍任何的这个额泰勒展开的计算啊，所以这个地方稍微仔细一点，灵活一下。

应该没有问题啊，看看这一部分有什么问题吗，而且电影没看懂，回去以后看视频啊，把那段视频这个材料我也发给大家啊，那个地方你再对应的自己看一下，就是个对应好吧，别对应错了就行了。

求导的时候FX与FT减U无关嘛，哎这是个好问题，我看是哪个同学啊啊0906同学啊，嗯额首先这个FX是对应的残差吗，这不是个残差啊，没有残差的概念，那但下面这个好问题，求导的时候是F与FT减一无关吗。

就说哎刚才那个求偏导对谁求偏导啊，对求谁求偏导，是在y heat t减一处的二阶泰勒展开，我们在前面求那个呃YIFXI对FX求导的时候，用负梯度计算的时候啊，负梯度计算的时候。

很显然我们是在对FX求偏导，但是这个FX上注意后面还有一个竖杠，竖杠在上面，竖杠在上面，竖杠在这这里的你要求偏导，你对谁求偏导啊，是对FX没问题，但是我们知道我们每一次的迭代过程当中，的FX是多个啊。

有F1XF2X一直到FMX，那么到底是对哪个FX求偏导，那大家觉得你应该对哪个FX求偏导，你要知道你必须要知道函数的具体形式以后，你才能够计算出它的偏导，以及它的负梯度的方向。

既然你需要对这个FX是已知的，那么很显然在第M部的时候，FX是不知道的，你不能用，那这个时候你只能用谁，只能用他的上一部那个FM减1X，所以这个地方你会发现啊，我对FX求偏导的时候是FM减1X。

作为我的求导对象，同理刚才是如果是一阶泰勒展开，那这个地方是二阶泰勒展开，那二阶泰勒展开的时候，你出现导肯定是对f HT减一嘛，就是你上一轮所学到的那个模型，出他的那个偏导嘛啊这是个好问题啊。

细心看了啊，但是这个问题啊，其实如果你再仔细一点考虑的话，其实是也是能够思考出来的，你对谁求偏导，你如果你对它不知道不了解，还是那个结果出不来，那谁了解或者谁知道，在DT轮。

很显然T减一轮的信息你是知道的是吧，好吧好了嗯好，当有上面这个这么复杂的式子以后呢，那么下面我们就需要做一些符号上的这个替换，使他能够简单一点，怎么替换，首先我们看看一下。

some i从一到n l y i y has t减一，这一项是没有变的啊，这一项还在这，那么下面这一项啊，下面这一项嗯，很显然这里的FT减XFT减X还在，所以这两项是没动。

那么很显然这一项就变成了这一项，那么很显然下面说到过，用JI啊，来表示损失函数在Y害的T减一上的偏导值啊，就是做了个符号替换就没什么太大问题，那么同样这里的F平方F平方都在，那么这里的1/2。

12也在那AH在这儿，那替换的是这一项，那么很显然，hi等于损失函数对Y害的T减一的，二阶导数和二阶导数啊，放在这个地方做了一个简单的符号替换啊，就是为了，下面写的方便而已好了，二阶特展展开就展开完了。

展开完了以后，我们继续往下，因为展到这个地方啊，讨厌在这个地方后面还有个正则画像好吧，我们把正则画像一块展开是吧，把正则方向也展开，展开之后呢，我们就知道正则化项，大欧米伽FT等于这里的伽马T加上12。

兰姆达sum g1从一到大T，然后W的WG的平方啊，这是我们刚才所介绍的正则化项的那一项，加进来就可以了，加进来之后发现了问题了，问题在哪儿啧，加进来之后啊，发现这个式子呢就已经复杂到好好。

好多地方都莫名其妙的地方，注意啊，前面有一个sum求和是从I从一到N，后面也有一个sum求和，J从一到大气这两个求和，那就很麻烦，为什么这么这么说呢，因为前面这个求和是在整个数据集上来求和的。

I从一到N嘛，I从一到N整个数据集上求和，后面这个求和J从一到大TT是什么，还有印象吗，T是我们嗯模型当中决策树的那个叶子的个数，这两个sum求sum求sum求和啊，这个求和的对象是不一致的啊。

就是一个是在数据集上，一个是在叶子叶子上，那这个时候对我们整个公式呢，这个计算会带来一定的麻烦，下面我们就需要把这两个求和公式进行统一啊，统一起来啊，在一个循环上就完成这两个活啊，就是这么个目标。

这个地方呢就是就是一点小技巧啊，小技巧技巧在哪儿，先说明一下，虽然从形式上我们会发现啊，虽然在形式上我们会发现，这两个求和形式上是完全不一样的，但是呢仔细想一下，你反正你两个都是求和求加法啊。

求若干个数的和吗，求若干个数的和，我们看一看你这若干个数的和的结果，是不是一样的，第一个求和啊，很明显I从一到N，很显然我们是在数据集上来完成求和的啊，数据集上来完成求和的N在哪往前找，看到N了吗。

N是我们数据集的模啊，数据集有多少个元素，你这个N就是多少，那很显然这个求和是在数据集上来完成的，换句话说我需要遍历整个数据集，好看后面这个后面这个是什么，刚才我们也已经讲到了啊。

他是我们学习之后的那个决策树的叶子节点，叶子节点上的求和，叶子节点上的求和，我第一个叶子，第二个叶子一直到第七个叶子，我都需要进行求和，求和的对象是谁，求和的对象是对每一个叶子的输出值的平方，对吧。

每一个叶子的输出值的平方，对每一个叶子进行求和和，对整个数据集求和，怎么能够把这两者统一起来，换一个角度啊，换一个角度，如果我们在进行决策树的学习的过程当中当中，大家想一下，是不是在学习完成以后。

我们必须要保证每一数据集里面的每一个数据，都必须要落到决策树模型的每一个叶子里面去，想想，我再重复一遍，就是数，当我们通过数据集啊学习完成了一颗决策树，那么是不是也就意味着。

我当前这个数据集里面的每一个数据，都一定被映射到了，这颗学习之后的决策树的，某一个叶子节点里面去了，我虽然不知道他一定在哪一个叶子上，但是我一定知道的，他一定是在这些叶子当中的哪一个里面。

某一个里面是这样吗，那如果有了这个视角以后，那大家想一下，我对一整个数据集的便利，是不是就可以转变成对整棵决策树里面的，每一个叶子节点的便利，然后遍历每一个叶子节点，那么大家想想这两个效果是不是一样的。

再举一个例子啊，再举一个例子，我们都知道咱们学校每一名同学，都一定隶属于学校里面的每一个班级，这是没问题的是吧，你是三班的，你是二班的，你是四班的，那么当这个学校的校长，要便利整个学校的学生的时候。

他有两种方法可以去完成，第一种方法，他可以拿着整个学校的点名册对吧，你不有学号吗对吧，我大不了就按照学号，一个学号，一个学号的点名，那这个时候很显然，就是在对整个数据集进行便利，这是一种方式啊。

我从一号学生到N号学生遍历一遍，那这个时候我就便利完了，那么除了这种方式之外，我又知道，既然每一个学生都在某一个班级里面，我还可以怎么办，便利我先遍历一班，在一班里面我把所有的学生都遍历完了。

再遍历二班啊，在二班里面再把所有的同学都遍历完了，一直到最后一个班级，把所有当前这个最后班级里面，所有的同学都遍历完了，那我也有信心，相信整个学校的所有的同学也都便利完成了啊，其实是一个逻辑啊。

其实是一个逻辑，就在这个地方，我们碰到了一个困难，就在于这两个sum求和啊，在形式上不一致啊，这个地方我们需要把它统一起来啊，这个时候我们需要折中一点，就是需要把这里的整个数据集上的求和。

把它形式上统一成在整个叶子呃，所有叶子节点上的求和的一种形式，那看具体怎么去做，看下面我们需要定义这么个东西嗯，定义叶子节点G上啊，某一个叶子节点G上样本的下标集合，我知道哈，每一棵树。

我们一棵树有若干个叶子节点，有若干个叶子节点，比如说就是三个叶子啊，三个叶子一号叶子，2号叶子和3号叶子，那我一定知道我数据集里面所有的数据哈，都在这三个叶子节点里面是吧，然后呢我定义这个叶子上。

比如说定义2号叶子上，所有样本的下标集合为这么个东西，就是当前这个叶子节点里面有若干个元素啊，比如说X1X2嗯X8，举一个例子啊，仅仅是举例子，那这X1X2X八这三个叶子的下标，我们把它构成一个集合。

叫做IG叫做I2，因为他是2号页了，也就说II2现在等于什么，等于128这么个东西，看下形式哈，看一下形式，IJ说明的是在DJ个叶子上，所有的数据的下标的集合，所以你会发现它的形式定义在下面。

这个式子为什么是这样来定义，还记没记得还记得这个q xi是什么东西了吗，找找啊，忘了啊，忘了就往前找，q xi q x是不是在这QX是不是个映射，你给我一个X，我把你映射到叶子里面去是吧。

现在就用的这个Q函数了啊，这个Q函数就是说明的是你给我一个X，你给我一个X，我把你当前这个X的叶子给你找到，当然叶子的下标是G，那么你看一下是不是就像刚才我们所说的，把叶子节点G上的样本样本就是数据。

就是我们的数据，将叶子节点上数据的下标哪些数据啊，一当前这个叶子节点上所有的数据，是不是都一定满足QX等于G，因为我们知道QX就是来完成叶子节点映射的，只有满足了QX等于G。

才说明你当前这个X是属于这一页的，里面的元素，而把这些所有的元素找出来以后，我要什么，我要的是元素的下标，所以是xi为什么这里是I的集合啊，就是这里的xi，好吧，有了这个关于叶子节点G上的。

样本的下标集合以后，你看看下面，我们就可以把目标函数表示为，按叶子节点累加的形式累变成下面这个形式，看看还是原来那个目标函数啊，还是原来的目标函数哎，这个时候SU就不再是在整个数据集上求解了。

而是变成了J等于一到T我遍历所有的叶子，在遍历每一个叶子的过程当中，我要计算下面这个值啊，计算后面这个值，那后面这个值看看，那么看下面，既然我们现在是在数每一个叶子上进行便利，那么下面的求和。

就到了每一个具体的叶子里面去了，比如说这里的I属于IG，说明当前这个元素，是我D这个叶子里面的某一个元素，当这个便利完成以后，是不是意味着当前D这个叶子就便利完了，那在外层循环里面。

当外层循环所有的叶子都便利完成以后，是不是也意味着整个数据集也便利完了对吧，所以说啊SAMJ从一到T开始遍历叶子，SAMI属于IJ遍历当前DJ个叶子，在D这个叶子里面。

我们在遍历这里的GI乘以FTXIJI啊，没有问题啊，JI没有问题，就是这里的JI，那问题是你这个FTXI跑哪去了，有人I发现FTX它没有了，这里变成了WG，那你看看这里为什么有这么一个式子。

就是为什么你把这里的FTXI用WG可替换，换句话说，很显然这里要保证f t xi是等于WG的，为什么这个等式是成立的呢，为什么，注意FTXI是我们前面定义的那个数啊，哎这个时候就体现出啊。

这个决策树的定义形式的这个好意义了，这里是FX等于WQX，按照我们这个数的定义，你是需要等于FTXI，按照我们的定义，不是W不是Q应该是等于wq xi，没问题吧，哎你这个地方怎么变成了G了啊。

不要忘了你，你是在第几个叶子里面，所有必须要满足所有的W呃，所有的QXI就等于G，那不就是等于WG了吗，另外怎么来的了吧，所以从这一项到这一项是需要他俩相等，它俩相等呢，就是因为这是按照定义来的。

但是呢我们知道他是在当前叶子节点里面，QXY就等于G，所以是等于WG，你看这就体现出啊，这个刚才那个函数的额数模型的定义的，技巧了吧，就在这儿好了，继续往下吧，如果这个明白了以后，下面其实也就明白了啊。

然后呢是二分之一二分之一没有问题，然后呢同样到了叶子里面是sum i从一呃，属于IGHIHI没问题，然后这里多了一个兰姆达是怎么个意思呢，然后这个地方又多了一个WG的平方，又是什么意思呢，意思在于。

既然我们已经变成了便利叶子，那后面这个遍历叶子，是不是也可以拿到整个的循环过程当中来了，我没有必要遍历两次叶子，再在当到了某一个叶子里面，这两个循环这个活我是不是都可以干了，我在这个里面。

我第一个第一步先把这个数据的便利，这个活干了，第二步再对你的输出的值的平方的求和也干了，是不是也可以了，所以说这个地方，所以这里的J从一到大T里面前面这一项啊，因为这里的没有WB的平方啊，这第二项呢。

我们就完全可以把WB的平方往前往前移进来，一进来以后呢，这个地方刚才我们说到过，这里，同样哈是FT的平方，xi按照定义应该是等于怎么老喜老喜欢写Q0，应该等于W的q xi的平方，就等于WG的平方。

明白了吗，这是这是这是那么来的好吧，当然这个地方是不是还有一个WZ的平方，也要也要过来，那这意味着前面的系数啊要进行相加，又变成了1/2，往前一提变成了一个sum i hi，加上一个这里的lambda。

就变成了这个式子，这一部分是这两部分的求和，这两部分的求和，这一部分其实就等于WG的平方，所以这里的W的平方就向后提了，然后前面的系数这里有个1/2，H这里有个12兰姆达，把1/2再往前提。

所以剩下的就是H加兰姆达好吧，那后面这个伽马T没办法就扔在后面，拉倒了啊，就扔在这，所以说这是今天可以说是嗯，又是一个额比较不太好理解的地方，是我们需要做的工作，是需要把sum进行一个形式上的统一。

最核心的就在于，我们需要在便利业务的过程当中啊，每一个叶子都遍历完了，来表示我对数据集的便利，那这个时候我需要在当前叶子的便利过程当中，需要定义一个叶子节点的一个元素下，标的集合啊。

就这么一点地方介绍清楚了就可以了，这一部分看看有什么问题吗，这两个sum求和，是要遍历完每一棵树的所有叶子节点吗，是I是N个数据，G是T个节点，不该是SUG包含在SUI当中吗，呃不是二，在这当中。

就像刚才我们所说的，学生是在班级里面，数据是在叶子节点里面对吧，第二位HI为啥又是IG呢，HI应该了解了是吧，OK没问题啊，好了这一步完成以后哈，你会发现哎我的损失函数啊，终于变成了这个样了。

那这个损失函数长这样有什么用呢，看下面不要忘了我们是要什么，我们是要令损失函数最小的时候，那个输出值不要忘了我们现在的目标哈，其实那个，那个FX的定义还告诉我们，我这个输出是在W上来完成的。

计算出计算出的这个输出值，我一定是要使这个W的值在目标函数的最小化，的基础上，要尽可能的小，对应的就是最优的模型输出，那这是我们的优化策略是吧，因为正好我们的目标函数加上了正则化项。

这个优化策略应该是很好的，避免了所谓的过拟合，那策略有了以后，套路不就是求偏导吗，对我们的WG求偏导仔细一点啊，这个地方需要特别仔细一点，其实没什么技巧，下面就没有什么太多的技巧了，就是一个，仔细了啊。

这个式子要对WG求偏导，你看看你能得到一个什么结论，注意啊是WG，所以外面这个sum g从一到T的循环，其实就没有必要了，因为在循环过程当中只有一项是WG，其他的项要么比WD小，要么比他GW大是吧。

那么当只有WG这一项上，我们对WJ求偏导，所以这一项肯定是SMI除以IGGI，再加上一个哎，这个时候你会发现，同样还会有一项是WG的平方向，这个二往前一提二分之一一乘变成了一。

所以剩下的是sum i属于IGHI，加上一个LAMBA，后面这个伽马T没有包含WG的那一项，所以整个式子等于零，整个式子等于零，把前面这个和后面这个因为可以看到啊，这一项里面是只有一次项嘛。

就变成了只有前面的系数，这是个二次项，二往二已经往前提了，从二次项降成了一次项，所以把这一项移到等式的右面去作为分母，所以就变成了下面这个式子，WG的星最优的模型输出等于负的sum i。

从I除以IGHI，加兰姆达分之SUI属于IGGI啊，这一项生成的是SUGI，加上这一项，剩下的是sum h i加兰姆达，然后是W然后这一项等于零，所以W星就等于这一项向右向右扔。

这一项除过去就变成了一个负的，分母是sum h加兰姆达，分之sum g i h i没问题吧，所以就是这一项好吧，好了，我们看看这一项哈，这一项当中的几个量，第一个一阶导数，二阶导数兰姆达。

拉姆达是超参数，这个是我们设置的，这个很显然是知道的，那这里的HI和JI和HI1阶导数，二阶导数知不知道看什么定义啊，一阶导和二阶导数的定义在这个地方，你会发现，这里的求偏导都是对我们的YHET减一。

来完成的，而我们前面刚讲到过y hi的T减一，是我们的T减一部的已知模型，所以这里所有的偏导值不管是一阶还是二阶，也是已知的，额，在DT补上都是已知的对吧，既然在TDT补上都是已知的。

所以这里的WG星也是最优解好吧，这个是三三个部分都是都是已知量嘛，所以这个地方也是最优解嗯，到此为止我们就解出了最优解的一个嗯，计算公式啊，看这一波有什么问题吗，其实有了损失函数求偏导。

让它等于零解出来就可以了，OK那我们继续往下，下面一项呢，其实又是一个比较不太好理解的地方啊，不是特别好理解的地方哪呢，我们继续往下看啊，为什么说不太好理解呢，看下面。

当我们有了这个关于W的最优的输出值以后啊，那么下一步我们把这个WG星啊，把它回带到目标函数里面去，你不是认为有WG和WG的平方吗，既然我们有这个式子了是吧，把这一项回带回来回来了，回到家回来以后呢。

稍加整理，我们就可以得到下面这个式子，有人说还挺复杂，其实也没什么复杂的啊，稍微整理一下，能约的约约能能合并的合并，最后得到下面这个式子，得到这个下面式子呢，就是说。

当我们代入每一个叶子节点G的最优分数啊，就是最优分数的时候，得到最优化目标的函数额，得到最优化目标函数值啊，得到最优化的目标函数值，这个最优化目标函数值呢，就下面这个式子就等于负的二分之一三分之一。

从一到达T分子是SAMI，属于IGGI的平方，分母是SAMI，属于IG hi加上一个兰姆达，加上一个伽马T嗯，这是我们在最优分数的意义上的，最优的目标函数值啊，最优的目标函数值。

也就是说啊我的函数值是这么个式子，这么个值啊，函数值是这么个值，而这个函数值的这个值，你会发现所有的值都是已知的，T是叶子节点的个数，我只需要查一下叶子有多少个节点，就可有多少个叶子就可以了。

这里的这个伽马呢是我们的这个参超参数，前面已经说到过，兰姆达呢也是超参数JI和HI1阶导数，二阶导数在DT减一轮的时候也是已知的了，所以这里的最优的目标函数值也是已知值。

那它会告诉我们一个什么样的一个结论，就是说啊，我们当在某一步过程当中学习了一个最优的呃，学习到一棵树以后，那么这棵树的最优的输出带回到我们的这个呃，回带回我们的目标函数以后，使得我们的函数。

目标函数可以得到这么一个结果，这个结果呢是不要忘了啊，不要忘了这个结果是所谓的目标函数值，简单一点说，他就是在不断的在计算损失的基础上，在数据集上进行累加的一个结果，不要忘了它是它的本质。

我们嘎啦嘎啦讨论了半天，不要忘了它到底这个目标函数到底是个什么，这个目标，其实最本质的还是在我们的实际输出和模型，输出上的损失的和呀，也就是说当我们在在某一刻，当我们按照前面的逻辑学习到了某一棵树以后。

这棵树里面的一阶导数，二阶导数兰姆达伽马T我们都知道了以后，我们发现了当前这棵树T的损失是这么一个值，L值，这是一个损失值啊，或者说是目标函数值是这样吧，那它有什么意义吗。

就这个目标函数值它它有什么意义吗，看下面看下面，我们知道数是怎么得到的，数是通过节点的分裂得到的对吧，我们你看我们当在刚才那个数的基础上，如果再分一层得到这么一个数，我们把它称之为T1撇吧，注意啊。

刚才是三个节点，刚才是三个节点，三个叶子节点现在呢是变成了四个叶子节点，两棵树结构上也不一样了，所以刚才那是T数，这是T1撇数，那么T数和T1撇说的不同，在于损失函数肯定也不一样了是吧。

刚才那是L这是L1撇了，不一样就在于模型结构发生了变化，由原来的三个叶子节点啊，由原来的三个叶子节点，现在变成了四个叶子节点，那这个时候叶子节点的不同，就在于原来的某一个叶子节点通过了什么。

通过了分裂啊，通过了分裂，构成了局部上发生了变化，那我们知道哈，当我们一旦知道了模型结构以后，这棵树被确定了，损失函数也就被确定了，那下面问题在于，我当前这个UT数向T减一数的，这个分裂过程当中。

你说我是分裂还是不分裂，是根据什么来的，分裂还是不分裂是根据什么来的，肯定是根据损失的大小来的，换句话说，如果我分裂之前的这个损失，要小于我分裂之后的损失，你说我还分裂，不分裂，我就不分裂了。

我就没必要嘛，因为分裂之后你反而损失大了，我还需要干这个工作吗，反过来，当分裂之后的损失小于分裂之前的损失以后，那我当然需要分裂，有同学就会有疑问啊，这个不分裂啊，损失还会更小吗，会这样。

因为我们前面讲到过，你分裂之后模型越来越越复杂，可能会出现所谓的什么过拟合现象，所以这个时候你会发现，我当前模型分裂和不分裂，都是需要根据损失函数来进行判断的，而我们又知道。

损失函数的最优解就放在这个地方，就明明白白的放在这个地方，那好了，你告诉我分裂和不分裂，我们是不是就可以通过分别计算，损失前和损失之后的这个啊，不是这个分裂前和分裂后的损失的大小，是不是就可以了。

这就是在GBDT上呃，在XGB上啊，x g boss上，我们进行模型分裂的一个依据啊，回忆一下啊，回忆一下我们的id3C4。5和cut数，分别采用的是信息增益，信息增益比和基尼指数来完成的。

特征分裂的依据啊，找到的特征分类依据，而在x g boost里面啊，它是通过定义损失函数的形式来分别比较，分裂之前和分裂之后损失函数的大小，作为我们当前决策树是否分裂的一个依据啊。

所以说这些是不一样的啊，这些是不一样的，那好了，有了这个依据以后，我们看具体怎么去做这个事情，怎么去做呢，怎么去做呢，先给这假设嗯嗯既然要分裂，那就先分裂吧，分裂之后看看结果不就完了吗，好了。

假设IL和IR分别为分裂后，左右节点的实例集，那先分裂再说是吧，那这样当然还要保证IL和I2，还是原数据集的左右子数是吧，那么分裂后损失函数的减少量，由下列式子来表示啊。

就减少了这个量由下面这个式子来表述，什么意思呢，注意这是我们分裂之前的原数的损失函数对吧，这是分裂之后左子树的，这是分裂之后右子树的啊，这是这是分裂之后左子数的啊，这是分裂之后右子树的。

这是原数的这个损失函数，那这个时候我们就像刚才我们策略是一样的嘛，左子树的损失加上右子树的损失，和原数的损失差值进行一个比较，我们的目标很显然是，就根据这个分裂依据来判断我这是减少量吗。

这是损失的减少量，这个损失还损失的减少量和零和进行比较，是不是就可以了，如果这个损失的减少量大于零，意味着我有我有我有必要分裂，因为你损失会越来越小嘛是吧，如果你损失的减少量小于零，那就别分裂了。

你还你还不如保持原来那种状态对吧，所以说啊这个时候差几boss，里面还有一个就是这是最，这也是我们另外一个不同的地方，和我们之前的决策树啊，它是由我们的损失函数的这种变化的不同，来决定啊。

作为依据来决定，是否是对当前的绝对数加以分裂，好吧好了，那这样的话我们看看算法其实很简单啊，就有了这个原理以后，模型就很简单，这个算法就很简单了，我们看下面分类查找，分类查找那个精确贪婪算法啊。

就我们的分类依据既然有了，我们就按照这个依据来进行查找就可以了，首先那输出呢就是输出最大的分裂位置啊，最大的分裂位置就像我们在在什么，在那个卡子数上来查找是一样的了啊，首先基尼指数为零啊，就是损失为零。

你这样可以理解，然后呢分别计算左子树和右子树的损失啊，左子树的一阶梯度和额，这个数的一阶梯度和二阶梯度啊，一阶梯度和二阶梯度，然后呢在所有可能的一到D，这是我们的特征维度啊，这是我们的特征维度。

在所有的特征上分别进行分裂的可能性的查找，我们前面讲到过啊，特角色时的生成就是在某一个特征上进行分裂，那我找哪个特征呢，我需要遍历所有的特征是吧，遍历所有的特征，在便利某每一个特征当中。

分别计算在当前特征左右指数啊，进行划分或者分裂以后的损失的一个计算，那这个时候呢首先在当前这个特征上，对左子树，右子树的这个一阶特征，二阶特征都需要清零，清零完了以后再对当前特征上看到了吗。

j stored i by x j k刚才我们说的是第K个特征，在我第K个特征的所有的取值，所有的数据上啊，所有的数据上进行一个先进行一个排序，由大到小或者由小到大无所谓，反正你排序就可以了。

这样的话呢把这个G挑出来啊，把这个G挑出来，以当前这个G作为我的分裂位置，作为可能的分裂位置，既然这这个挑出来以后，哎，看下面分别在以当前这个位置为分类位置，计算它的一阶梯度和二阶梯度。

然后把这个一阶梯度加到左子树里面去，加二阶梯度加到左子树里面去，那么这个时候右指数的一阶梯度和二阶梯度，就是由我们原数的，原数的一阶梯度和二阶梯度，分别减去左子树的一阶梯度和二阶梯度，来进行计算。

计算出来以后呢，比较比较一下，比较什么呢，就像刚才那个我分裂之后的损失，和分裂之前的损失的那个损失的减小量，比较一下，如果这个量减小到和之前的那个减小量最大的，那个，当然就是我需要找到的。

那么我需要尽可能的减少，可能都会使得损失减少，但是不同的特征上，不同的特征的位置上减少的这个量是不一样的，我当然是希望是找到使得这个减损失的减少量，最大的那个位置上。

所以我不断的啊不断的把这个位置进行记录，记录的同时呢，和新的位置的这个减少量进行一个选择，选择那个原味，原来的和新的更大的那个减少的量，作为我score里面的值，好吧，大家想一下，当内层循环。

当这层循环完成以后，是不是在特征K上所有的数据上，这个分裂损失减少量最大的那个就找到了，那么在外层循环上，既然每一个特征，既然一个特征上所有都找到了，那么在所有每一个特征上我都找到啊，都遍历一遍。

把那个最大的放到当前的score里面，那这个时候输出的就是我所有的特征上，所有的特征取值上，使得损失减少量最大的那个分裂值，其实说明一下哈，这个分裂值是多少，其实并没用对吧。

其实我们并不关心这个这个损失，减少量的最大值是多少，其实并没有，我们更关心的是什么，我们更关心的是这个减少量最大值，所对应的那个特征以及特征的位置输出，其实啊这个算法应该应该啊应该输出的是什么。

应该输出的是那个最优的特征K以及特征，K上的那个取值J，就和我们在cut树上输出的那个那个那个特征，以及特征的分裂位置是一样的，明白没问题吧，啊，那么这个时候我们有这个分裂特征，以及特征的分裂位置。

就可以不管我是用呃，再代入到我们那个决策树的生成过程里面去，就可以得到我们的决策树了，好吧，看这部分有什么问题，嗯哦我觉得回去以后和那个cut树的那个特征，以及特征的分类位置的那个那个那个算法啊。

你需要回去以后再咳一块再再看一下好吧，因为我们都额目的哦，算法目标其实就是在整个数据集上的，所有的特征以及特征的分裂，可能位置上查找一个最优的位置，那最优的位置的依据这个地方已经给出来了。

所以说你会发现前面这些工作啊，这是在整棵树上完成，一阶梯度和二阶梯度的计算，然后呢这个K从一到D啊，从E到D就是所有的特征的遍历啊，所有特征的便利，你不是输入，是一个是一个是一个多维度的一个向量吗。

这是每一个维度上都可以，都可能作为我的特征分类位置嗯，比如说前面我们讲到过那个例子，贷款那个例子是吧，年龄额呃，年龄还有什么，这个是否有房子，是否有工作，还有信贷情况，这不都是每一个都有特征吗。

每一个特征我都不需要进行便利，每一个特征里面的每一个取值，比如说咳我以这个年龄为例，年龄分青年中年老年，那么年龄里面的青年中年老年，我都需要查找一下这个值，就这个这个减少量是不是最大，建立完了以后。

我再拿着这个最在当前这个特征啊，当前这个年龄特征里面的，额损失的减少量的最大值和，比如下一个特征是那个有没有工作啊，工作这个特征里面无非就是有工作没工作吗，我在便利一下这个有是否有工作，有工作没工作。

这两个职上所有这个损失函数的，额损失减少量的最大值再比较一下，那么在年龄里面的这个损失减少量的最大值和，是否有工作里面的这个年龄的这个这个工作的，这个损失的减少量最大值最大的那一个。

当我们完成了所有在特征上以及特征的，可能取值上的这种减少，损失减少量的最大值的计算的时候，那个位置，不就可以作为我当前的一个分裂位置了吗，比如说比如说啊你这个年龄啊，老的这个中年啊。

中年这个年龄上的这个损失的减少量是最大的，那我就以当前年龄的中年为例，把当前这个数进行划分就可以了，这是关于这个特征分裂部分，那回过头来看看我们关于X级boss的这一部分啊，前面就不看了啊。

从这开始看嗯，回去以后你需要好好的理解一下这个，我觉得这个呢别看很多的同学啊，就是啊看着简单可能就pass过去了啊，是这么简单吗，他很显然简简单有几，它简单的这个简单有他的简单的道理啊。

每一次面试的时候，我经常的会问一些同学对吧，你比如说你这里为什么是FX等于WQX，好多同学他就这么定义的啊，我说定义是这样定义的，你它的原因是什么，就不是那么的清楚是吧，所以说你很显然他的作用。

在后面的简化过程当中是很重要的是吧，这个地方你需要自己再看看嗯，再下一个呢是这是正则化项加入到了，E的节点和W的输出啊，这个作为他的正能方向，这个是的改进之一啊，他的改进之一，这个算是需要看一下的。

这两第二点下面一点呢就是这些都展开啊，这个时候刚才有同学有疑问，这个地方没有什么技巧性啊，就是把公式一个一个仔细的对应清楚，就可以了啊，就这个还不清楚，那只能是看公式就看就可以了，好吧嗯。

这是正话第二个正则化项的地方啊，第三个就在这啊，当我们有了这个二阶TT泰勒展开以后，这个sum求和这两个也是哈，有很多同学就划水，就就是划水，我说这个你从这一步怎么怎么做到这一步呢。

有同学说不就这么得到吗，我说怎么就就就得到了，我说特别是啊，其实最核心的就是在这个地方啊，就是关于当前每一个叶子上的，下标集合的这个定义啊，所以再深入的说一下啊，如果有兴趣的同学可以看一下陈天琪老师。

他做这个工作时候本身的那篇论文啊，那篇论文里面其实还是关于这个播放，还是说的很清楚的啊，只不过还是需要有点改进而已啊，这是这个地方的一个理解啊，第三个地方呢就这啊就是这个损失，就是这个分类依据的问题啊。

他其实是通过我的有的最优损失以后，我带回到原损失函数，得到了一个损失的一个额表达式，既然我这个损失的表达式都已经有了，我分裂还是不分裂，通过损失的大小的比较，就可以一目了然的得到这个结论吗。

嗯只不过这个时候麻烦就在于，我需要在所有特征和特征取值上进行便利，那便利就便利呗，无非就是遍历的过程嘛，仔细一点控制一下左右指数的位置就可以了，先上来求一个整的，然后把左数的清零循环遍历的过程当中。

不断的求出GI和HI加到左边去，那右边的不就是这整个的减去左边的，不就得到了右边的吗，嗯好了，原理部分就是这些内容，嗯那下面我们看看应用啊，这个因为这一部分呢，我预期好像在后面的课程里面没有过多介绍。

所以简单的和大家一块过一遍好吧，就是参数就是调参嘛，参数呢我们大学有这么三类，就通用参数提升器，参数和学习参数啊，这个看一下通用参数部分呢，嗯最重要的其实就是booster的选择。

就是我们的这个提升器啊，就是我们的这个booster，这个提升器的这个选择的问题呃，有两种选择，一种是数，一种是这个线性模型啊，线性模型这两个呢，一般我们预期数模型的性能，要比线性模型的性能更好一些。

当然这和你的数据集也密切相关啊，这个嗯运算性能其实和数据集是密切相关的，这个你可以做一个参数选择，当然默认的是以数模型作为我们的基础模型，我记得新的版本好像又增加了一个那个booster，忘了啊。

你有兴趣的同学可以再查一下他的API啊，这个地方多了一个啊，下面这两个呢回去我自己看一下，这个没什么特别的啊，呃第二部分呢就是关于提升器的参数啊，提升器的参数首先是个艾塔，艾塔呢这个是个学习率啊。

艾塔是学习率，就是我们在进行F等于F加上L这个呃，就是是L对F的那个偏导的计算过程，艾特在这啊，艾特在这个地方就是他的学习率嘛，然后呢这个值有了以后，下面是关于最小的叶子节点的权重啊。

就是最小叶子节点样本权重和，这不是那个权重啊，这个那个权重是，其实是我们叶子节点里面的输出值的和啊，不是不是大家理解的那个权重，就是不是我们那个那些什么贝塔呀，不是那个东西。

其实这里就是你可以认为就是W的和，就是那个sum w啊，这个值的和，就我们在叶子节点里面包含了若干个样本，这个样本的和要尽可能的小，啊所以是最小叶子节点样本样本的和啊，你别加权重了，就是样本和就可以了。

用来控制过拟合的啊，这个值不能太特别大，太大了，就陷入这个欠拟合现象，然后呢除了这个最小的叶子节点的样本和呢，还有一个就是树的最大深度啊，树的最大深度我们前面讲到过，书的最大深度。

也表征了当前模型的复杂程度，另外一个呢就是最大的叶子节点的个数，就是那个大T这两个参数啊，这两个参数其实是密切相关的，当我们在二叉树当中，我们知道叶子节点的就是树的深度和。

叶子节点的个数其实是有数值关系的对吧，当你确定了其中一个以后，另外一个其实也就被确定了啊，所以这两个哈确定一个，另外一个就确定了，我们一般习惯上使用的是叶子节点的个数啊，当然深度也也可以吧，这个随便吧。

另外一个就是伽马，就是我们前边两个超算里面的伽马，另外一个兰姆达的后面应该是啊，其他的大家自己看一下就可以了，这是那个兰姆达啊，学习任务的参数呢，其实呃，XJB还是提供了很丰富的，这种学习任务的参数。

比如说他可以处理所谓的二二分类问题，也可以处理所谓的多分类问题，而且可以处理带概率类别的多分类问题啊，这个都需要根据你的任务类型的不同，来加以选择啊，多量方式呢我们除了平方损失之外，还有很多其他的损失。

但是我们更多的还是采用了，所谓的平方损失的形式啊，这个最一般实用，那么下面呢还举了一个例子啊，这个例子呢回去以后大家呢可以额自己跑一遍，这个地方介绍一点啊，说明一点。

有些嗯芯片可能会出现一些在x g boost上的，一些某些老版本上会出现报错，这个时候你可以把这个开关量打开啊，把这个开关量打开，编译就可能会通过，但是有些那个特别是英特尔芯片。

会出现所谓的二次加载问题啊，这当然这个我们也不需要特别考套考虑了啊，就是某些版本的芯片和某些版本的差距boost，而是不兼容的，试一下这个参数可能会解决你的问题，那么数据集呢这个地方有这么一个数据集。

然后呢他有九列数据额，想想是九列是十列啊，呃十列数据前九列是X，最后一列是标签啊，对数据进行切切分啊，训练集测试集的一个切分，切分完了以后呢，我们可以大体的跑一下这个模型，在不调参的基础上。

我们直接对他一个进行一个学习，学习之后呢，你会发现他的学习率呢，已经达到了这个正确率啊，已经达到了77。95%啊，这已经性能上不调参就已经马马虎虎凑合了呃，看下预测的结果呢，无非就是因为12分类啊。

所以直接给出了标签这个分类结果值，但是呢给大家说一下，更建议大家使用的呢并不是直接输出零一值啊，更建议大家使用的还是这个，带着概率输出的预测结果啊，带着概率输出的预测结果，所以说你看下面这种情况啊。

他分别给出的是当，不同的这个标签取值的时候，它对应的一个概率值啊，所以说你会发现比较一下啊，比较一下，这是第一个啊，第一个返回了一个零，然后呢这个时候呢你会发现，返回零呢其实其实啊按照我们的概率输出。

他是95。45%上是返回一个零标签，还有0。04%上，可能会要返回一个一标签啊，这个呢是更对我们的结果的这个呃解释哈，可能更有意义一些，比如说再举一个例子，你看这个值。

你看这个如果我们只通过零幺的输出呢，我们知道它输出的是个标签一，但是呢通过概率分析呢，我们会发现这个标签一呢其实并不可靠，因为它即使是标签一，也仅仅是在58。8%上返回的是一，换句话说他有40%一点。

接近41。2%是需要输出零的，所以说这个标签的输出就很有啊，很有讨论的空间，所以你像这种结果的解释性，会为我们的这个工作上带来很大的帮助是吧，另外呢还可以输出每一个参数的重要性啊。

就是每一个特征的重要性啊，不是每一个参数，每一个特征的重要性啊，就像刚才那个例子里面，第七列的数据的那个特征啊，要比其他列的特征的重要性更大一些啊，这个了解一下就可以。

并且呢还可以把当前决策树的形状绘制出来啊，这是嗯都可以试一下吧，另外呢就是模型的调优啊，后面我们会讲到，比如网格调优啊，网格调优呢就是我们把所有的参数的取值空间，做一个大致的一个生成啊。

根据我的不同的取值空间学习率啊，最大深度啊生成不同的参数空间，在这个参数空间当中啊，我遍历整个参数空间啊，就是网格搜索，其实就是在有限的空间里面进行便利哈，当这个时候我们再看一下结果的时候。

已经性能上有提升了81。1%了啊，咳咳咳咳，后面呢在一个新的数据集上，也有类似的一个使用额，下面呢可能就是啊，另外一个呢是关于这个x boost和LGBM呃，let gbm呢是另外一个包啊。

或者另外一个库，这个库呢是微软下面的一个工作，也是非常好啊，就是你看一下啊，这是我们的差距boss的这个分类啊，下面一个是我们的letter g b m，那个JBM呢是微软的一个工作啊。

这个工作呢和这个x g boost呢是呃非常相近的，两个不同的工作啊，原理上都非常的相近，但是工程上会做到大量的改进，那么我们下面这个例子呢，是根据一个数据集上的呃，性能上的对比啊。

从这个速度上和准确度上，我们都进行了两个不同的包的一个对比，大家可以看到啊，其实在特定某些特定情况之下，这个来自GBM，要比这个x g boost的性能还要好一些，不管是精度还是速度啊，也就是说啊。

今天我们虽然讲的是这个是这个x g boost，但是在工作当中哈，你可以使用一下let gbm，可能会出现一些更好的效果，他俩非常类似哈，都可以给出特征的排序以及不同的数形结构啊。

let s gbm也给出了一个例子，有兴趣的同学都可以看一下，好嗯，嗯这是我们今天的主要内容，看看大家还有什么问题吗，Ok，还是那样，就是我会把这个用到的那份额扩展材料啊，也给大家发到咱们的课程群里啊。

作为一个补充啊，今天的内容呢，如果还有什么问题及时在群里提出来啊，我相信一定会存在不少问题啊，有什么问题我们再集中的进行讨论，一定是通过自己的分析和就是思考啊，就是你一定先有自己的一个理解啊。

你的理解是什么，然后你的理解和资料上，哪个地方是统一不起来的，嗯我们把问题聚焦在哪儿啊，千万不要问说哪有问题啊，这都有问题，这个都有问题，就聚焦不不到具体位置上了，是吧，哦嗯嗯想起了嗯，这里有个问题啊。

就是我们是以刚才看到个标签外了啊，我看到那个标签Y啊，看看他标签外在哪啊，这个地方呢我们的原理部分的介绍过程当中，是以回归问题哈，这个是以回归问题作为X1boss的模型的输出。

所以你会发现这里YI是属于R的啊，YI是属于R的，很显然是个回归问题，那有同学马上就会问tt boost，如果是个回归问题的这个模型的话，那能不能解决分类问题是可以的，那怎么解决呃。

这个分类问题这个问题其实我觉得到现在啊，大家应该嗯其实一个一个通用的方法啊，就是我们怎么能够把回归问题的模型，改造成这个分类问题的模型，如果我知道当前模型是个回归输出，那这个时候我马上就可以想象一下。

你那个线性回归是怎么改成那个逻辑回归的，还有没有同学有印象线性回归输出的是个R，然后呢逻辑回归输出的是个零幺，这不是个分类了吗，他是怎么做的，无非就是在线性回归的基础上，套了一个西格玛的函数吗，对吧。

那么同样我们在tag boss的基础上，他回归输出的基础上，你外边再套一层西格末尾的函数，就可以把它映射成二分类里面去了，如果你把它套成soft max，就把它套，就把它映射到多多分类里面去了。

这就解释了刚才为什么在那个插件，boss那个包里面，它是个分类问题啊，他这个分类问题有同学这个地方会有困惑哈，你讲原理的时候是个回归，它为什么是个最后是个分类呢，就是因为啊。

其实这个你只要在回归的基础上，加上一个不同的分类函数，就可以压缩到不同的离散空间里面去，来完成我们的分类问题了，好吧嗯，那好我们今天的内容就到这，有什么问题的话，我们及时在课程群里讨论好吧。



![](img/f211ca9eb023de8b4fbf89afc212667d_2.png)

![](img/f211ca9eb023de8b4fbf89afc212667d_3.png)