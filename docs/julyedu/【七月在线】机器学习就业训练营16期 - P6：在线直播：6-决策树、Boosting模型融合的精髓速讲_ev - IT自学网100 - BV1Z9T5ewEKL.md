# 【七月在线】机器学习就业训练营16期 - P6：在线直播：6-决策树、Boosting模型融合的精髓速讲_ev - IT自学网100 - BV1Z9T5ewEKL

那这样的话我们就准备开始啊，这个呃按照我们的计划啊，今天呢我们需要讨论的模型呢是决策树模型，通过我们这张图啊，可以看到决策树模型的位置啊，在这个位置上嗯，它是一大类哈，非常重要的这个机器学习的模型。

以数结构啊为这个模型的这种主要结构呃，其中这个决策树的这种嗯算法还有很多啊，比如说今天我们会介绍到，比如像D3C4。5和cut数这三类角色数，那么在此基础上，通过这张图我们也可以看到呃。

通过今天我们关于决策树的介绍呢，其实还有另外一个内容就是关于继承学习boosting，可能哈，一开始会有些同学觉得比较莫名其妙是吧，讲学的书为什么要把boosting放到一块，那么很显然是因为啊。

我们可以通过在决策树上使用集成方法构造哈，应一种所谓的集成决策树模型，就是BDT模型，然后呢我们以此作为向前推进的一个基础，在推进到GBDT以及哈看一下课表里面。

我们会介绍到这个charge boost啊，这是我们最核心的一个内容之一啊，最核心的内容之一，这也是啊，就是我们为什么要介绍这张图的原因之一啊，通过这种方式让大家可以看到，我们一直往前序啊。

它的基础是我们今天的这三个基本都是模型，好吧啊，那么在讲决策树之前呢，我们看一类问题啊，结合了我们上一次课讲的内容，因为我上一次课啊，重点介绍的是我们积极学习的，一些基本的术语和概念。

这些内容啊会贯穿于我们整个的课程啊，所以呢我们一块来简单的做个回顾哈，首先我们看一下这张表啊，这是我们做上次介绍到的一张数据表，当然内容发生了很明显的变化，那我们看一下这张表，首先我们昨天讲到过啊。

表格里面竖着的一列，我们把它称之为是属性是吧，我们看一下当前这张表格里面有哪些属性，包括年龄属性啊，有工作与否的属性，有没有自己的住房的属性，以及信贷情况的属性啊，这是12344类属性。

那么还有一列呢是非常特殊，它表明了我们当前这一章数据，表格当中所记录的数据的一些非常重要的，决定性的或者结果性的因素，比如说这里有个类别啊，这里有俯视的一个取值，那么在我们上一次讲到的学生表里面。

那么有一列称之为是是否为三好学生，那么一列作为我们的标记列是吧，那么在这张表格里面呢，我们拿到的这张数据表，是银行在进行放贷的时候，对我们贷款人的一个资质的考察，的这么一张数据表，那么很显然。

刚才我们可以看到银行会根据贷款人的年龄，是否有工作，是否有房屋以及他的信贷情况来综合决定，我是不是把当前这笔款贷给你，那么结论就是yes和NO啊，这是我们拿到的属性以及标记列，那么我们上次也讲到了。

那么相对应的每一横行，那么记录的成N为一条记录啊，记录的呢就是我们当前这个实体啊，当前这个贷款人在各个属性上的取值，以及它所对应的什么对应的这个标记的情况，比如说当前这个0号啊。

0号它是一个青年没有工作，没有住房信贷情况一般啊，他的结论呢就是否啊，就银行不对他进行放贷，那我们再举一个例子，比如说青年有工作，没有房屋信贷情况是好，所以银行决定向他进行贷款好了。

那么数据上就摆在这个地方，那我们要做的工作是什么，我们做的工作是基于这张表格啊，训练一个模型来进行判断，当有一个新的贷款申请来的时候，我根据新的申请人的这种基本情况啊，来判断我是否对他进行一个房贷。

这就是我们这张数据表格的一个呃，数据所反映的情况以及它的作用，当然我们在上次讲课的时候讲到过，以当前这个数据表格为例的话，我们完全可以采用什么模型，还有没有同学有印象，可以通过标记哈。

我们可以看到这个标记里面的取值是是和否啊，适合否，很显然是离散的标记值，我们前面讲到过离散标记值的时候，我们需要进一步的进行一个判断啊，离散标记值的取值的个数，很显然它只能取十个否。

那么我们可以做个判断，就当前这张数据表构建，是一个所谓的二分类问题，二分类问题，两个分类是吧，两个取值的分类问题，那么我们其实可以通过咱们上一次课讲到的，那个什么讲的那个逻辑回归，逻辑回归模型来解决。

当前这个解决当前的问题是吧，没问题，那么我们采用另外一种方式啊，采用另外一种方式来对这个问题加一加以考虑，什么方式呢，就当我们拿到这四个特征的时候啊，就年龄有工作，有房屋和信贷情况。

如果你作为一个银行家啊，当你拿到这张数据表格的时候，如果你要对这四个特征加以这个，重要性的一个排序的话，你会认为我要对当前这个放贷款人进行放贷，你最希望考虑的是哪一个因素啊，你考虑的是年龄因素。

是否有工作，是否有房屋还是信贷情况，那么你认为这四个因素里面哪个最重要，可能会有不同的意见，看看大家的选择，那我先说一下我的认识啊，如果我是作为银行放贷的话，我最希望考虑的当然是我的这个放贷资金的。

这种安全性是吧，就是它的风险是不是可控，那既然考虑到风险呢，我去看一下年龄，有工作和有有房屋和信贷，哪一个特征的取值，会对我的这种风险的降低带来决定性的因素，那么我一般会认为这个房屋是最重要的。

当然这仅仅是我的个人观点啊，就因为他有房屋，有一个资产是吧，大不了我放贷收不回来的时候，我可以把他的房屋进行一个强制拍卖啊，就可以保证我的这个呃资金的一个安全性，这是一种考虑。

那么当然嗯有同学说他的现在情况，对他的历史记录会影响到我对他的一个决策，对吧，嗯如果考虑有的说是有工作，因为他有工作，他也可以持续的进行还款是吧，其实年龄因素呢也是一个非常重要的，那么基于以上四个方面。

你会发现根据不同的人啊，或者根据不同的视角，我们会认为这些特征有一个啊，有一个重要性的一个排序的结果对吧，嗯不同的人当然有不同的一个排序，结果他会首先比如说啊，首先他有一种一种选择是。

比如说我先根据他是否有房子，因为房子这个东西，现在来看还是比较保值的是吧，判断一下是不是有房子，如果你有房子和没房子两种情况，我的处理意见肯定是不一样的，如果你是没房子，那不好意思。

我直接就不贷款给你对吧，那即使你是有房子呢，我也非常小心，我再去看一下你是不是有工作，也就是说你这个房子，因为我们知道房子也分很多种情况，地段啊对吧，年限呀，嗯这个是不是学区房啊。

都会影响这个房屋的这个价值，所以我再看看你是不是有持续的还款能力，你的工作是不是稳定对吧，如果你爱你，你没有工作，那这时候即使你有房子，我也我也不带给你，我继续再往下看。

那你有工作还需要看一下你的信贷情况，你的信用是不是良好对吧，你有房子要工作，但是呃贷款这种信诚信记录非常不差，那我很显然也也也也也不带给你，那么再往下我看看你的年龄，当然你会发现。

当我进行这样的一种特征或者，属性的排序的时候，我自然会认为房屋的这个这个重要性，要大于工作的重要性，要大于信贷情况的重要性，以及大于年龄的重要性，那么当你换另外一个视角，或者换另外一个这个呃银行的时候。

可能它的排序就会是另外一种规则，那好了，回到我们现在这个问题当中的讨论来，你会发现当我们有了这个数据表格以后，我们是可以通过这张数据表格，构建一个置顶向下的，这么一种基于重要性排序的一个模型。

来对我们的结果加以分析，当我们再有一个人啊，申请人来了以后，根据他的房子，工作诚信以及年龄就可以进行放贷了，当然不同的银行可能会有不同的结果，那基于这种按照我们特征重要性进行排序。

并且依次对特征加特征的情况加以考察，来决定当前一个结果的这种模型呢，我们一般把它称之为数模型或者叫决策树模型，因为它为我们的决策提供了一种依据，而它最终的形成的就是一种啊，一种树的一种结构啊。

当然这棵树啊是倒着长的是吧，这是根，然后向下，这最终是叶子节点啊，最终是叶子节点，所以说啊通过以上的这个啊小例子哈，我们可以看到今天这一类模型呢它的特点啊，它的特点第一点解决的啊解决的问题啊。

呃可以解决分类问题啊，决策树模型啊，决策树模型一开始的时候，它主要解决的是分类问题，当然我们在分类决策树的基础上对它加以改进，也可以使它来完成所谓的回归问题，一会儿后面我们会介绍怎么去改进啊。

也就是说你先知道决策树模型，基本上我们可以做回归来做分类啊，这是没问题，回归需要稍加改造，这是第一点，第二点它需要对我们的特征来加以排序，来决定我们决策过程当中的一个顺序啊，就是先对谁进行判断啊。

在刚才例子里面，一种情况是先对房子是不是有住房来加以判断，然后再看另外一种特征，再看一个特征，再看一个特征，你会发现它是一次对诸个特征有一个判断的，一个顺序啊，判断的一个依次进行判断啊。

这实际上决策树模型非常重要的两个这个特点，一个就是他做分类问题，另外一个呢需要对特征加以排序啊，或者说特征进行一个顺序性的一个考察，那么这个时候呢就碰到了总结出来就是三步，哪三步呢。

第一步也是非常重要的，既然我们需要对我们的特征加以排序啊，需要有一个谁比谁更好的一个问题，那这个时候，第一个问题就是所谓的特征选择的问题，那为什么你你要把这个住房啊放到第一位啊。

为什么你要把这个有工作放到第一位，为什么你要把这个师傅是有这个信贷情况，放到第一位，你需要对特征加以选择，你需要有一个依据啊，这就是第一步，一旦有了这个特征的选择依据以后，那么第二步的决策树的生成。

我觉得就相当的呃自然了，为什么这么说呢，当我们有了绝特征的选择依据呢，按照这个特征的选择，我们把重要的特征往前放，先对它加以考察，然后依次再把剩下的特征依次向下排序，也就构成了当前所谓的决策树模型啊。

这第一个问题啊，顺带着有了以后，第二个问题也就解决了，那第三个问题呢，嗯是需要对我们的决策树进行一个修剪，或者叫做减脂，为什么要这样做，因为上一次课的时候，我们也讲到了。

就是说我当我们进行这个模型生成以后，会发现某些模型会出现所谓的什么现象，过拟合现象是因为当前这个模型太复杂了，我们不得不在模型生成的过程当中，对当前这个模型加以控制，使得它不那么的复杂对吧。

来减少或者避免所谓的过拟合现象，所以说以上三部分啊，三个步骤就是我们在决策树模型当中，非常重要的三步，第一步特征选择，第二步决策树的生成，第三步决策树的修剪，那我们依次啊把这三部分别来进行介绍。

那第一步就是作为特征选择，所谓的特征选择啊，在于选取对于训练数据集具有分类能力的特征，刚才哈大家可以看到了，不同的人对特征的重要性，其实意见是不统一的是吧，根据你不同的生活的经历对吧。

你的认知判断是不一样的，那这个时候我们需要定义一个，大家都能够接受的啊，或者大家都能够普遍理解的这么一个重要性，排序的一个依据，那什么呢，就下面给出一个数学项，数学上的一个工具叫做信息增益。

那么这个概念之前呢需要补充一个基础内容，就是所谓的熵的概念上非常重要啊，商，呃熵这个概念呢源自于这个热力学第二定律，当然今天我们不是讲物理，那么这个概念呢引入到了信息论当中。

用来表示随机变量的不确定性的一个度量啊，再重复一遍啊，熵这个概念啊，商这个量在信息学里面或者信息论里面，它表示的是随机变量不确定性的一个度量啊，不确定性越大，这个熵就越大，不确定性越小，这个熵就越小啊。

就这么一个物理量，那么它的定义形式是这么来说的，假设X是一个具有有限个值的离散随机变量，X啊是一个具有有限个值的离散随机变量，那么它的概率呢被定义为P，X等于XI的时候的概率值为pi啊，这个时候呢。

我会引出今天这个就是我们课程当中啊，另外一个问题啊，就是我们怎么样去理解一些，数学公式和数学符号的问题啊，那么怎么去理解这里的X这个，离散值的随机变量，那么又怎么去理解，当这个X等于小X的时候。

它的概率等于pi这个含义，这个时候啊，我们可以找一些我们实际生活当中，可参考的一些实例加以一个对应说明啊，比如说现在这个X，你可以把它想象成是一枚骰子啊，一枚具有六个面的骰子，每一个面都有相应的点数。

那么既然是一枚骰子，我们知道抛骰子可能会出现六种不同的情况，根据它的点数的不同来加以区别，那这个时候的小X所对应的就是，每一点朝上的一种情况，我们知道掷骰子吗是吧，你随便一扔六个六个面。

不一定哪个面随机的朝上，那么朝上的那一面的骰子的点数就被定义为xi，那么这个地方的X等于XI，很显然就被我们的实际情况啊，一种对应就是我们随机进行骰子扔出去，扔色子，它等于某一面朝上啊。

就被定义为X等于xi，那这个时候的P代表的是出现当前这种情况的，概率值是多少，我们知道哈，一般的情况，因为正经的骰子是不是叫正经骰子，就是我抛之前那六个面啊，我知道他没有经过任何的处理。

那每一个面朝上的概率都相同，那这个时候它被扔出去之后，所产生的某一面朝上的概率，就被定义为X等于XIPX等于XI，那它等于多少呢，在刚才扔骰子那个例子里面，很显然等于多少，1/6对吧，1/6。

但是呢这是在刚才说过啊，这是在六面每一面朝上的概率相等的情况下，它等于1/6，往往我们也知道，很多的时候嗯会对骰子做一些手脚，使得某些点数出现的概率增大，某些点数出现的概率减小。

那这个时候很显然就这里的pi，那对应的每一个骰子，每一个朝上的那种情况不同的时候，那个概率值可能就会发生变化，对吧，这是所以说你会发现看到这一条数学公式以后，你反应过来的就应该是X是一个随机变量。

它可以取很多的随机值，那么取这个随机值的时候啊，所以取得这个随机值的时候，所对应的那个概率我们用PX等于X来表示，那么这个随机值是多少或者概率值是多少，那么对应对应到式子里面。

就是pi小pi来加以表示啊，这就是所谓的这个随机变量X的概率，分布的问题啊，没问题啊，那么有了随机变量X的概率分布以后，我们定义随机变量X的商，那既然是个随机，那既然是个随机变量，那它就有不确定性对吧。

因为随机变量嘛，它可以在它的取值范围当中任意取值，那它就有不确定性，那么这个不确定性啊，随机变量X的不确定性就用他的商来进行度量，那怎么度量呢，看下面HX随机变量X的商用HX来表示。

也可以用hp来表示啊，这个都可以是形式上的一种变化，那具体的等于什么，具体的等于负的SUMI从一到N，这里的N对应的就是当前这个随机变量，可取的啊，所有的取值情况啊，所有的可能取值情况。

如果是刚才那个那个那个骰子的话，那这里的N就等于什么，N就等于六对吧，N就等于六，那么求和公式里面是pi每一种啊，每一种情况的概率乘以log，pi进行一个累加去赋值，用来表示当前随机变量的商啊。

它的不确定性，那至于这个公式是怎么来的，是在我们的信息论里面有讨论，有兴趣的同学可以去自己看一下，我们今天在这里就不进行推导了啊，你就知道随机变量X，它的商就用后面这个公式来计算。

其中求和的范围是从一到所有的随机变量，可能的取值，然后求和公式里面的P取值为相对应的，每一种情况的概率值，后面是log pi啊，这是它的熵的一个定义，那么商的定义有了商的定义以后呢，我们需要分析一下。

这个商到底是一种什么样的情况，然后呢通过下面的推导啊，我们可以看到这个商的取值范围非常有意思，我们画这么一个坐标系啊，横轴呢是这个零到P纵轴是零到hp，那么我们知道一个概率的取值范围。

无非就是从1~0对吧，因为概率的取值嘛，我们只是概率规定就是从0~1，然后呢这个它的最大值呢我们可以计算出来啊，是烙文啊，烙纹我们假设最大值log在这，那么这个时候呢我们可以绘制出哈。

可以绘制出一个曲线，就是关于hp的曲线，大体上是等于这样一种情况啊，大体上等于这么一种情况，那么下面就有一个问题，什么问题呢，我们看看他为什么要长这样啊，为什么是这种状态好吧，那好了，你看一下啊。

取零和一，刚才我们这个例子里面是抛色子，骰子有六个面啊，不太好分析，我们同样把骰子换成硬币啊，硬币就只有两个面了是吧，硬币就只有两个面了，那么如果我们考察一枚硬币的话。

你会发现它无非就是正面朝上或者反面朝上，那么问一下大家，当我们有一枚硬币啊，当我没硬，有一枚硬币，普通硬币随机向外扔的时候，只有两种情况，正面和反面朝上，那这个时候每一面朝上的概率。

大约应该是1/2是吧，大约应该是1/2，但是如果我有这么一枚非常特殊的硬币，什么硬币呢，就这枚硬币它怎么扔，总是正面朝上，怎么让总是正正面朝上，那大家想一下这两种情况，不同的硬币他们的商谁大谁小。

哪两种硬币啊，第一种硬币随机扔啊，扔出去以后随机出现正反面，另外一枚硬币呢扔出去以后，每次扔都是正面朝上，那么这个地方如果我们用套用这个熵值，我们不需要进行计算啊，我们不去计算呃，主观的去判断一下。

我们也可以知道，那每每一面都是正面朝上的那枚硬币的商，不要忘了，商是来度量混乱或者度量不确定性的一个值，那么既然它每一面朝上，每一面都是正面朝上，很显然它的熵值应该是最小的啊，商是用来度量不确定性的。

既然你每一面总是正面朝上，那它就是确定的值，所以它的熵值就是零，同样如果这枚骰子或或者这枚硬币怎么扔，都是反面朝上，同样他也是确定的，所以它的伤值也为零，那么这个时候看中间这个值，什么值。

当这枚硬币扔出去以后，我事先是不知道正面还是反面朝上的时候，那这个时候很显然它的不确定性是最大的对吧，你不知道扔出去以后，你才知道扔之前你是完全不知道的，所以这个时候他去到一个最大值啊，商的最大值。

所以说啊我们可以看到这个图总比公式哈，容易理解啊，当我们进行这个随机变量计算的过程当中，你会发现怎么去理解商它是不确定性的，度量好不确定性，什么叫不确定性，就是你扔出去之前对他的认识是怎么样的。

如果这枚硬币你百分之百的可以肯定扔出去，总是正面朝上的，升值一定是零确定性的，如果你扔之前什么都不对，他完全不了解，那这个时候的熵值就是最大值，好像这是关于熵值的一个介绍哈。

那商还不是最有用的一个这个呃概念，我们下面会介绍另外一个概念，称之为条件上啊，条件上什么叫条件上，我们首先看一下，假设随机变量X和Y两个随机变量了，两个随机变量，他们有联合概率分布啊，有联合概率分布啊。

我们先不管它的联合概率分布值是多少，也就是说意味着这两个随机变量，它们彼此之间是有关联的，那么看下面我们定义这两个随机变量，它的什么随机变量X给定条件之下，随机变量Y的一个所谓的条件上，条件上注意啊。

条件上讲的是两个随机变量，并且是在其中的一个随机变量，X被以知的条件之下，另外一个随机变量的商啊被称之为条件上，它的定义是下面这个形式，SUMI从一到N还是你随机变量可能取值，在各种情况。

然后呢是pi乘以一个三，X等于XI被确定以后，Y的条件上的和pi的里程的一个结果的累加，这就是所谓的条件上呃，条件上是个什么东西啊，首先我们还是举个例子啊，还是举一个例子，你是两个随机变量啊。

你是两个随机变量，是其中的一个随机变量被取，当然因为这两个随机变量是是可以构建，联合概率的，所以这两个随机变量很显然是有关联的，还是有关系的是吧，其中的一个随机变量被确定了以后。

另外一个随机变量的商称之为是条件上啊，举个例子啊，还是那样，比如说迎面走来两个人啊，迎面走过来两个人，你对这两个人完全不认识啊，从来没见过这两个人，那当然你对当前这两个人的不确定性，是非常大的啊。

所以说你会发现这个时候嗯，伤是最大的一种情况，就是两个人你完全不认识是吧，换一种情况，比如说迎面过来两个人，其中一个人你完全不认识啊，你不认识他，第一次见啊，平生第一次见，但是呢你会发现另外一个人啊。

和他紧挨着一块过来的，另外一个人是你的朋友啊，是你的好朋友，铁哥们儿，那这个时候你是不是就天然的，对另外一个人的这种认识就应该是加深了，而加深了这个原因，就是因为两个随机变量当中的另外一个。

你对他是了解的，这就解释了条件上的一个一个一个一个，理解方向，就是你怎么去理解条件上，条件上是在两个有关联的随机变量当中的，其中一个被确定的条件之下，对另外一个随机变量的不确定性的一个度量啊。

这被称之为是条件上，好吧好了，回过头来看一下手头上有两个工具了，一个工具啊，是商商呢度量是一个随机变量的不确定性啊，就是这个这个你就可以认为对面来了一个人啊，陌生人你对他来说完全没概念是吧，然后呢。

哎条件上说的是，迎面过来两个人当中有一个你认识还很熟，那么这个时候呢就被称之为条件上好了，下面变一个魔术，那么大家想一下，由熵值和条件商他们的差值得到的这个结果啊，差值得到的这个结果。

你认为是一个什么样的含义，一个好像是随机变量，就还是那个人啊，第一次来的时候是个陌生人，你完全不认识他啊，就他一个人来了，又走了一会儿呢，哎他和你的朋友一块来了，那这个时候你是不是天生的。

应该对他的判断发生了变化对吧，第一次来的时候，他就是个陌生人嘛，我根本不认识他啊，可能下辈子不见了，那么第二次来的时候，哎他和我哥们一块过来的诶，这人嗯，我就应该对他有一个重新的认识和判断是吧。

他八成应该问题不大对吧，那再举个例子啊，比如说啊，比如说这个什么哎你对这个人啊，这个这个这个人完全不了解是吧，第一次见不认识啊，但是呢你发现突然发现哎，这哥们和特朗普是朋友，完了啊。

这这人也也也靠谱不到哪去是吧，他居然和和川普是朋友对吧，也就是说哈当我们发现啊，当我们发现随机变量被另外一个随机变量，确定以后所带来的这个熵值以后，你会发现由原熵值减去这个所谓的条件熵值。

是不是就是因为这个随机变量X的确定以后，所带来的信息的确定性的那一部分，我再重复一遍啊，熵是用来确度量不确定性的啊，一个随机变量的不确定性放在这个地方啊，就是这个单独这个随机变量的不确定性，就放在这。

另外一个呢是在两个随机变量当中的，其中一个被确定以后，随机变量的不确定性的一个度量，那么我们很就直观的会得到这么一个结论，当另外一个随机变量被确定以后啊，随机变量的熵的值。

应该要比单独的一个随机变量的熵值要小，因为它毕竟是确定了一些信息，它的不确定性减少了一部分，那这个时候减少了多少，我们就用元随机变量的熵值减去一个条件熵值，那这个时候的这个差值。

就是由这个被确定以后的随机变量啊，它所带来的信息的增加，那这被称之为信息增益，信息增益，这是一个非常重要的概念，看下面那么特征的信息增益啊，我们拿到数据集以后，有若干个特征或者若干个属性。

就像刚才我那张表格里面是吧，我们有ABCD啊，有若干个属性，那这ABCD若干个属性，对于当前这个数据表格T来说，它所带来的信息有多少，我们用信息增益这个量来加以度量。

那所谓的特征A对训练数据集D的信息增益，就下面这个我们用JDA来表示，其中它的值是先计算整个数据表的熵值啊，先拿到整个数据表，我先计算一下熵值，用整个数据表的熵值减去数据表格当中的。

A这个特征被确定以后，数据表的条件上啊，这是第二部分，然后呢做一个差值，那这个时候就像刚才我们所举的那个例子一样，那这个差值的大小就是由这个特征，A被确定了以后来带来的信息量的一个改变。

我们把它称之为信息的增益，那么当然我们可以计算GDA，同样我们可以计算GDB对吧，B这个特征同样我们还可以计算GDCGDD，当然这里的D不太一样吧，一个是数据表，另外一个特征，有了这些计算结果以后。

我们只需要怎么样，刚才我们回到刚才特征选择那个问题里面，特征选择告诉我们，我们需要确定的是特征的一个排序规则，哪个特征最重要，哪一个特征最重要，我只需要考察一下这个特征。

为当前数据表所带来的信息增益的大小，就可以了，你带来的信息增益大，那你的特征最重要，你的你的信息增益小，你的特征就往后排，所以根据信息增益的大小，我们就可以决定我们特征的重要性，有了特征的重要性以后。

就像刚才那个例子里面，你就可以拿着那个特征，重要性最大的就是信息增益最大的那个特征，作为我的第一个特征选择，然后再用第二个，第三个，第四个依次向下，那么我的决策树是不是也就构建出来了啊。

这就是哈第一个问题，关于特征选择的问题啊，特别是关于这个嗯这个这个概念的理解好吧，具体的计算一会我会给出具体的计算方法，关于这个概念的理解，我希望大家能够现在还能够掌握一下好吧。

信息增益等于熵减去条件熵，熵值是整个数据表上的不确定性，条件熵是在一个特征被确定以后，数据表的不确定性，那差值就是由这个特征所带来的信息的变化，这个信息的变化量称之为是信息增益，如果没有问题的话。

我们就继续往下，那么这个信息增益呢，我们需要把它计算出来对吧，你需要给出一个计算逻辑啊，把这个信息增益计算出来，那么很显然这个信息增益的计算就分三步啊，把大象放冰箱，分几步也分三步，开开冰箱门。

把大象塞进去，关上冰箱门是吧，信息增益的计算也是分三步，哪三步，第一步计算信息计算商啊，计算数据一地的商，第二步计算条件上，第三步算差值就完了，那么很显然我们把它拆成三步，第一步计算商。

第二步计算条件上，第三步算信息增值，看下商品谁的商，数据表格D的商啊，数据表格D的商，第二步条件上谁的条件上特征ab确定以后，数据表的条件上啊，数据表的条件上，第三步做差法做做做。

做差值就没什么太大问题了好吧，那么为了计算这三个量，我们需要引入一些符号，这些符号呢可能嗯需要仔细的去分析一下好吧，需要仔细的去分析一下，看第一个我们假设数据集为D啊，数据集为D，这个没什么可说的。

数据及D的模啊，数据集D的模表示为其样本的容量，就是样本的个数啊，就是有多少条记录，有多少条记录，我们加上两个杠啊，用他的模来表示，那么看下面是K个类别K啊，注意既然是K的类别CK我问一下。

这是对谁说的，是对我们的标记那一列来说的是吧，以刚才那个数据表格为例，现在表格里面我们知道是否放贷，是我们的标记列，那里面有几个类别，有两个类别啊，当然如果是多分类的话，就有多个类别。

所以这个地方假设是有K个类别，是有大K个类别，那么每一个类别呢我们用K来表示啊，在刚才那个例子里面，K等于一，K等于二，因为分两个类别是吧，那么下面看下面，那么CK的绝对值啊。

CK的绝对值用来表示属于类别，CK的样本的个数，我们刚才例子里面有两类啊，Yes or no，贷款还是不贷款，那么很显然这两类啊，每一类元素的个数，我都可以用C1的元素的个数。

和C2元素的个数来分别表示两个不同的类别，那看下面，如果SUK从一到大K啊，那么CK的绝对值就应该等于D，这是显而易见的，因为我们知道我们的标记一共就分两类啊，Yes or no，那如果我们把这两类啊。

把根据类别，标签分的这两种不同的类别的元素进行相加，那么相加之后的结果，一定是等于整个数据表的元素的个数，这是我的表，我们现在只讨论那个标记那一列标记，那一列标记，这一列里面有yes有NO对吧。

标记里面有yes有NO，我把零的元素找出来，加一块再把一的元素找出来加一块，然后再把零和一的加起来找出来加一块，是不是还是就是整个数据表的整个的元素个数，对吧。

所以说啊这个式子说的是sum k从一到大K，然后4K的绝对值不是4K的模，就等于D的模，好吧，看下面假设特征A注意啊，现在我们的视角放到了特征A上去了，就是那个特征A，在特征A当中有N个不同的特征取值。

注意啊，这是那个特征A有N个不同的特征取值，还是要举一个例子，按照刚才我们所说的，假设这个A2等于年龄好吧，为什么是选这个年龄这个特征呢，是因为刚才那个例子里面年龄分了青年，老年和中年啊。

所以它的取值比较多一些哈，丰富一些，那么很显然它的取值A1就等于青年，A2就等于中年A3就等于什么老年，所以它是一个A1A2A三的这么一个特征A，那么我们也可以根据A这个特征。

将数据及D划分为N个子集对吧，刚才所说的青年青年，青年中年中年中年老年老年老年，那么很显然，我可以通过特征A也把整个数据集，分成了若干个子集，那么其中DI表示的就是我们每一个取值，所对应的样本的个数。

这是青年的，这是中年的，这是老年的，这是刚才说的第一第二第三的模，然后很显然显示什么，如果I从一到N啊，你A不是有三个特征吗，你的年龄不是有三个取值取值范围吗，那么从我把你的三个取值范围里面。

青年的元素的个数加上中年元素的个数，再加上老年元素的个数，一定也等于整个数据表里面元素的个数，就像刚才我们看的，这是青年的元素个数，这是中年的元素个数，这是最后老年元素个数加一块。

肯定还是整个数据表里面元素的个数，注意哈，注意以上这个K啊，这个K是根据我们的标记列来进行分析的，这里的DI啊，这里的跟A的特征A的不同的，取值是以我特征A的视角来加油，区别的好吧。

然后呢分的时候呢当然会有一些要求，什么要求呢，其中的子集DI当中，其中子集DI当中，再根据那个CK的样本集合划分为DIK，看下面将我们子级DI，比如说我以清我以这个青年为例啊，这以青年为例。

根据青年当中再根据那个类别标签啊，根据那个标签列再进一步的划分为DK，什么意思啊，比如说在这里啊，我们以青年为例，即使是青年这一类特征A的一部分的取值，我们根据它的标签，进一步的还可以把它划分为。

青年里面有贷款的和青年里面没有贷款的对吧，这就是根据DI当中属于K的，把它定义为DIK那么青年可以这样做，中年也可以这样做，那么中年这个人群里面，我也是可以根据中年里面他的标签列。

分为中年里面贷款的和中年里面不贷款的，老年也可以这样来做，那么可以看下面DIK等于什么，DIK就等于我是某一个年龄上进行的取值，并且和我贷款的一种情况的取值的一个交集，比如说刚才那个例子。

那么D青年有贷款，说的就是我青年人里面有贷款的那部分数据，那么老年无贷款，就是老年人当中没有贷款的那部分数据啊，用DK来表示，那么DK的膜就被称之为是DK，这部分数据里面样本的个数，那么看下面啊。

这个是看一下sam k sum i，然后是DK等于多少，把这个式子写到一边去，Sam k sami，然后是DK等于多少，3K意味着我要把所有的类别标签加一个，加对吧，这是放在外面里面呢是SAI。

我要根据特征A的不同的取值进行累加，然后呢是DIKDIK是在特征A里面啊，那个年龄里面取得某一种取值，并且他的放贷情况等于某一种放贷情况以后，所构成的数据子集，然后把所有的这样的数据子集。

通通的都加一块儿，那等于什么，等于一不等于一，不应该等于整个数据表里面元素的个数，DIKIKI是什么，I是我特征A的某一个取值，它可能是青年中年，老年K呢是我们标签的某一个取值，它可以是贷款。

也可以是不贷款，DIK说的就是青年里面不贷款的，青年里面贷款的老年里面不贷款的，老年里面贷款的中年里面不贷款的，中年里面贷款的，我们把它加一块儿啊，都加起来，应该构成的是我们整个数据集D的元素的个数。

注意啊，注意啊，这里是D的模，而不是D是吧，而不是D好了，有了以上的分析以后啊，作为基础，那我们就可以进行信息增益的计算了，怎么算呢，所有的信息，所有的这个计算都是基于概率上的计算，所以就是一些比值啊。

就是一些比值问题，你需要分析清楚到底是谁比谁好吧，看下面信息增益算法啊，信息增益算法输入的是什么，输入的是数据及D啊，就是那张数据表格，以及你要针对某一个特征来完成，信息增益的计算。

因为我们这个信息增益，是要计算后边那个条件商的，条件商是两个随机变量的熵值是吧，那你需要给你说这两个随机变量，一个是数据1D另外一个特征A那么输出什么，输出一输出的是当特征A被确定以后。

训练数据及D的信息增益，就是那个JDA啊，就是就是输出这么个结果，那么三步刚才说过哪三步，第一步首先计算数据及D的伤啊，首先计算数据及D的商熵值怎么算，看上面熵值的计算在这儿嗯啊这个也可以啊。

升值的计算在这，现在呢只不过我现在需要计算的是D的三，D的商呢等于后面这个式子，在这个式子里面我们需要知道那么几个量，第一个我需要知道这个N的多少啊，嗯在刚才那个呃数据表格里面啊，在数据那个表格里面。

我们很显然我们知道我们这个贷款还是不贷款，只有几种情况，只有两种情况，所以他是从1~2啊，这个没问题的，我还需要知道啊，贷款和不贷款的概率是多少啊，贷款的概率是多少，不贷款的概率是多少，我需要知道。

另外呢log pi这个求一下就可以了，所以很显然核心的要点就变成了，这个pi怎么去求，就是我们无非就是贷款贷款吗，当贷款的时候，这个pi值是多少，当不贷款的时候，这个pi值多少，我只要算出来就可以了。

现在问一下大家，那这个怎么去求，不要忘了你现在有那张数据表格了，那个呃标签那一列你是知道的是吧，你最直接的办法你就去找一下找一下，因为我们知道数据集里面元素的个数，我是知道的，就是D的模。

那它当然是分模是吧，当时分母问题是分子是多少，分子也很好啊，你数一下，你数一下里面，比如说我看一下贷款的百分，贷款的这个呃嗯概率我数一下里面已有多少个，是不是就可以啊，这一个两个三个四个哎。

那不就是四比上D的模吗，那么四是从哪儿来的，四不就是属于贷款里面元素的个数吗，所以说啊你只需要统计一下整个数据表格里面，不同的标签里面元素的占比是不是就可以了，看下面，所以数据1D的商HD等于什么。

等于负的SUMK，从一到大K，这里的K就是我们的标签的可能的取值范围，标签哈，标签就是分类啊，你有几个类别，在我们的数据表格里面是两个类别，所以这个地方是小K从1~2啊，你根据你不同的类别。

那我如果有大K的类别，那就是从一到大K，然后看下面那个pi不同的类别的那个概率值，那个pi怎么去算，分母就是我整个数据表格里面元素的个数，所以是D的模啊，这没问题，分母是没问题的，分子是多少。

分子就是每一个类别里面元素的个数对吧，我查一下所有表格里面，所有被贷款的人有多少个除以，整个表格不就是贷款的人的概率值了吗，那不贷款的呢，我查一下不贷款的那个人是不是就可以了啊，然后后面log2啊。

后面的这个概率值是相等的相同的，所以说啊，HD的计算，相对来说相对来说是比较容易理解的啊，容易理解的麻烦麻烦在第二步那个条件上，为什么呢，如果从根据它的定义哈，你会发现你碰到了一个麻烦，什么麻烦呢。

首先呃条件上是有两个随机变量，一个是X1个是Y对吧，然后呢这里是I从一到N，当然这里的N也是，根据我们的类别标签就可以了，所以这个pi还是比较容易去处理的，麻烦就麻烦在后面这一部分，后面这一部分。

其实你会发现它在形式上也是一个账户，也是一个条件上，只不过是X等于XI被确定以后，整个数据集上的调这个商值的计算，那这个时候呢我们就需要换不同的视角啊，就从刚才那个D的那个呃这个呃商的计算。

转变成这个条件上的计算，你看下面啊，通过这个公式我们再去理解一下，看看这里哈，还是那样是在特征ab确定以后，数据及D的上等于什么，按照刚才的公式等于sum i从一到N啊，SUI从一到N进行一个呃求和。

然后呢它是一个pi，这里的这个pi啊，注意这里的pi还是根据我们这个数据集DI，根据那个特征A啊，根据那个特征A的不同的取值，我们刚才说到过A的不同的特征取值有N种，所以呢这里是到N啊，所以这里是到N。

然后每一种里面特征A里面的每一个取值，我需要计算一下它的元素，这个元素个数，比如说青年有多少个，中年有多少个，老年有多少个作为分子分母呢，就是我们元素的总个数啊。

元素的总个数这一部分哈理解起来问题还不大，麻烦就麻烦了，后面这个式子，后面这个式子这个HDI他给的是HD了，HD刚才说过，这是我们整个数据集的商，这个HTI是个什么东西啊，有同学能理解一下吗。

如果说HD是整个数据表的商，那么HDI是不是就是数据表格当中，DI这部分子集的商，为什么是这部分子集呢，我们刚才讲到过，我们现在是以特征A的视角来看条件上的啊，如果说我们的这个熵值就这里的商。

我们是以这个标签的视角，那这个条件上我们就是以特征的视角去看，既然我的特征也可以把我的数据集，分成了不同的部分，那每一部分里面的熵值，是不是就是在特征A被确定了以后，我计算的整个数据表格的上市。

而这个特征ab确定了以后的这个熵值，是不是就是这里的什么所谓的条件上，明白什么意思了吧，所以说这个地方最难理解的啊，就就就在这啊，你怎么去理解这个这个这个HDI啊，HD是整个数据集的差。

HDI是在我这个数据集里面特征ab确定了以后，分成的那些特征子集里面的商，那这部分就可以作为我在Y在X等于小xi以后，被确定以后的那个商值的一个结果，好吧，那理解了这个HDI以后。

问题是我需要把这个HDI算出来，HDI它也是个商值，按照我们的熵值计算是不是就可以了，按照刚才我们商值计算无就是I从一到N，然后呢是分子比上一个分母，然后一个log分子比上一个分母，分母很清楚啊。

分母是谁，因为我们现在已经落脚到这个特征ab，确定以后的这个数据子集里面啊，数据子集里面，所以分母就变成了这个数据子集里面的，元素的个数好吧，然后呢分子是谁，分子就变成了我在这个分子集当中。

我再根据我的特征来进行划分的，这个子集里面的元素的个数作为我当前的分子，所以说啊看一下公式，这个地方呢，从理解到计算都不是特别的容易理解，所以你就要好好看一下公式啊，就放在这儿，首先上面这一步。

前面这部分照抄，往下照抄，所以说你会发现这一部分是完全一样的，那么很显然这一部分在这，因为它是个商值，所以负号在这啊，负号提前了，那么下面这一部分式子我们可以看到啊，下面这部分式子可以看到分子。

就像刚才我们说的，因为他计算的是一个数据子集的伤啊，数据子集的商，所以分母就是零，就是那个数据子集里面的元素个数，所以这个DI的元素个数，分子是谁，分子是在这个数据子集里面，我根据我的特征。

我们我根据我的那个标签来统计的，我元素的个数，就像刚才啊，我先在青年人里面，假设我青年人有十个人啊，那这个十就作为我的分母，在这十个人里面有两个人，我有贷款，有八个人没有贷款。

所以这个时候他根据不同的贷款情况，我就得到了在青年人啊，这个年龄特征为青年的这个数据子集里面，那么它的概率分别是2/10和8/10，能明白什么意思吧，就是说这个条件商的计算，当我限定了特征A以后。

他的那个熵值HDI，要在局部的数据子集里面进行完成，而这个数据子集里面的熵值计算，其实和整个数据集上的熵值，计算的原理是完全一样的，只不过你的计算的对象不一样了，以前数据集的熵值。

是以整个数据集作为你的计算对象，所以你的分母是数据集里面的元素个数，分子是当前这个类别里面元素的总个数，到了这个数据子集里面，我只看这个数据子集有多少个元素，所以分母是数据子集里面元素的个数。

而分子是在这个数据子集里面，再根据标签的值来划分的，那个数据子集的数据子集里面的元素的个数，好吧，这样的话我们的条件商也就被计算完成了，有了条件商以后，信息增益就比较容易计算了。

信息增益直接用熵值减去条件就可以了，好吧，这一部分的计算呢呃你需要完成的第一步，对这些符号的理解，怎么理解这一堆符号，这一堆符号如果你理解完了，后面这个信息增益计算的过程，其实是能够理解的好吧。

再给大家说一下，在那边就是李航老师的统计学习方法里面，在这一部分里面有一个例子啊，就是计算的过程怎么去计算熵值，怎么去计算条件熵值都有例子啊，回去以后非常建议大家把那个例子看一遍，因为有具体的数值。

你知道算的过程是怎么样的，好了嗯，信息增益呢，可以作为我们这个进行特征选择的一个依据，然后呢根据不同的这个特征选择依据，我们还可以构建所谓的信息增益比啊，构建所谓的信息增益比。

信息增益比呢我们是在信息增益的基础上，你可以看一下它的分子就是我们的信息增益啊，分子就是我们的信息增益，然后分母呢是在某一个特征下面，我计算整个数据集的上，刚才的信息增益的计算过程里面的这个商。

我们是根据标签那一列来完成计算的，所以说你会发现这是sum，可以从1~8K是根据标签那一列来完成的，在进行信息增益比的计算，分母的那里面那个商是在特征A被确定之后啊，它的整个数据集上的一个商值计算。

只是哈在整个的计算过程当中，我们的那个还是那样，我们的视角发生了变化啊，数据集上我们还是以标记啊，数据集上的商我们还是以标记列来计算的，特征A的特征A上的数据集，它的商我们是以特征A这一列来进行计算的。

那这样的话有了这个分母以后，熵值比上啊，条件这个信息增益比上这个熵值，我们构建的就是所谓的信息增益比啊，信息增益和信息增益比，都是可以作为我们特征选择的依据的啊，那好了，有了这两个特征选择依据以后。

我们看看第二个问题，怎么去构建生成数啊，生成当前的决策树这个过程啊，其实就相对比较简单了，看第一步啊，id3算法啊，id3算法，id3算法的输入，首先我们需要数据集D啊，数据集D第二部有特征集合A啊。

第二部有特征集合A你得告诉我那个特征是谁，你还需要给我一个阈值啊，给我个阈值，因为我们在计算过程当中啊，有的时候嗯，可以提前结束我的整个的生成过程啊，达到阈值的时候，我就可以结束算法了啊，你给我阈值。

然后输出我们的决策树T输出当前这棵决策树，那么第一步第一步，若D当中所有的实例都属于同一个类别，什么意思啊，比如说还是我们的那张学生表，还是我们的学生表，哎我们这个班级太优秀了。

所有班级同学都是三个学生，既然都是三好学生，那么根据当前这张表格，学出来的那棵决策树就长什么样，只有一个节点，为什么只有一个节点，因为我这个数据表格里面告诉我们，所有的学生都是三好学生。

也就意味着当转过一个新的同学来以后，八成不是八成百分之百，他就是一个什么，就是一个三好学生对吧，他他没得跑，所以啊这是一种特殊情况，第一步先处理一种特殊情况，如果数据记得D。

数据D当中所有的实例都同属于同一个类别，那么这个时候T就为一个单节点数啊，因为所有的情况都是一个啊，都能就是那一个类别，并将类K作为该节点类别的标记输出就可以了，输出一个一就OK了，你不管你来谁。

XYZ随便来，来了以后就多给你个三好学生啊，给你个好人卡啊，因为我的数据集数据集就是这样的，我们没办法啊，这是一种情况，算法从这个地方就可以结束了，返回T算法就终止了，当然这是一种极端情况。

那么继续往下看，第二种情况，若A等于空集，A是什么，A是我们的特征集合，意味着当前这张数据表也很也很特殊，长什么样呢，它只有标记那一列，没特征，没特征列啊，只有标系列，如果没有标系列，那怎么办。

如果没有我们的这个这个属性啊，如果我没有没有我们的属性点怎么办，那么也很好办怎么办，你看看它只有标记，那既然只有标记，就是只有一堆零幺值呗，我是不是只要从这些零幺值上去学习就可以了。

那意味着如果我构建一个新的模型，这个新的模型对新的数据来了，三个同学XYZ是吧，这三个同学到到我班级里来了，那怎么办，那给不给他这个好人卡，给不给他这个三好学生，我是不是只需要统计一下当前这个班级里面。

学生的这个三好学生，非三好学生的一个占比，我只比如说啊当天这个班级里面留有个人，一个同学不是三好学生，那59个都是三好学生，那很显然大概率上，当前新来的同学也应该是个三好学生，是这样吧。

所以说你会发现当我们特征没有的时候啊，当没有特征的时候，那这个时候也是一颗单节点数啊，也是一棵单节点数，并且将D当中实力最大的那个特征，就刚才我们所说的60人里面，59个都是三好学生。

那OK那你还是三好学生呗，只是根据这个标记的比例大小来分配，当前这个单节点数的输出标记，同样也返回当前这棵树T，那这个时候就有同学会疑问什么样的表啊，如果说刚才那种情况啊，标记只有一种啊。

标记都有一个值，我还能够忍了是吧，就就就特殊吗，那什么时候这张表格里面没有特征，只有标记啊，这种情况不可能出现，这种时候啊是不太可能出现，但是呢是我们算法过程当中生成的一种负结果，一会儿我们再看它好吧。

你先知道怎么处理就可以了，就出现这种情况，我只需要统计一下标签里面的占比大的那一列，作为我当前的节点输出就可以了，至于怎么出现这种情况，一会我们去看看一下第三步，否则那这个时候我们就在集合A当中啊。

就在我的特征集合A当中选择一个特征，来计算当前特征下的信息增益，信息登记计算每一个特征啊，如果不是这两种情况，那意味着我这个表格里面有若干个特征，有A1A2A3A4，一直到AG是吧。

那么每一个特征都计算一下他的信息增益，那么这些信息增益里面总有一个最大的，我们把它挑出来，跳出这个最大的来以后，你看啊，我在所有的特征上来计算信息增益，找一下那个最大的信息增益所对应的那个特征。

把它找出来，找出来这个特征以后，这个特征对我当前表格里面标签的结果，起到了一个最重要的作用，就像刚才我们那个例子里面，我们通过信息增益的计算发现，哎确实是有没有房子啊，这个特征它的信息增益最大。

那么下面怎么办，那么下面我们就看再判断一下，我得到的这个信息增益的大小，是不是超过了阈值，如果说这个信息增益的大小没有阈值大，意味着我所有的特征里面，即使是信息增益最大值的那个特征。

他的信息增益也远小于我的阈值的话，那这个时候我也没有必要再对我们的决策树加，油生成了，为什么，因为那个信息增益最大的，都已经小于我的这个阈值了，意味着这个信息增益有，但是呢起的作用并不大。

所以这个时候也把当前的数设置为单节点数，并且将D当中，实力量最大的那个K作为当前节点的标记，同样也是一种特殊情况，同样V怎么出现这种情况，一会我们再看再往下再，否则对于AG当中的每一个可能的取值。

就像刚才那样，我找到了那个房子作为最信息增益，信息增益最大的特征，并且这个特征的信息增益还大于了我的阈值，那这个时候我根据当前这个特征的，每一个可能的取值，比如就像刚才那个房子啊。

那个房子所有的可能的取值，房子有两种取值，有房子和没房子，根据他的所有取值，依据每一个特征的取值，将我们的数据及D分为不同的子集啊，就像刚才说开发现房子那一列最重要的特征，根据房子这一列。

把所有的没房子的和有房子分别挑出来，分成两个子集，就是意味着我通过房子分成了两种情况，有和没有是吧，有和没有，怎么办，分成两个子集以后，将DI当中就是你不就分成两个子集吗。

每一个子集里面实力最大的那个类别标记，作为我当前的类别标记输出，你分成了有房子和没房子，有房子里面啊，有房子这个子集里面他的标签的占比最大的，作为当前有房子的标签输出，没有房子的这个子集里面。

根据标签计算出没，有房子里面这个占比最大的啊，标签的情况作为当前节点的输出，大概率上我们可以知道，那不是大概率肯定是这样啊，就是有房子里面，他的这个标签对应的一般就是一没房子，这个一般就是零。

因为他的信息增益最大，如果他都不啊，没有这种规律性的话，我觉得这棵树也就不用再生成了是吧，所以说这个时候哎这个时候输出的是一，这里输出的是零，好了，这样的话你会发现啊，根据这个特征。

房子我就把当前的数变成了，一个有两个节点的不同的一种情况，那下面怎么办，对第二个子节点以DI为训练集，以A减AG为特征集，也就是说哈，我在没有房子和有房子这两个子集当中啊。

有房子和没有房子这两个子集当中，再根据A减AG，既然我这个房子特征已经用过了，我就不能再用了是吧，我在其他的特征里面，比如说我在那个年龄工作和信贷里面，再去分别计算他们的信息增益，再找到一个。

比如说是工作在每一个子集里面，再根据工作那个特征进行一个划分，有工作的，没工作的，有工作的，没工作的再进行划分来，是不是就变成这样了，这个特征就是根据工作，那这个特征是根据我们的房子那个。

然后工作这个就不能要了，然后再根据什么，比如说在在这两个里面找一个信息增益大的，比如说是那个信贷好吧，有了信贷以后是在每一个子集里面再分，根据信贷那个情况再分成两个，分成两个，分成两个，分成两个。

然后哎，再往下分好了，现在这个特征也不用了，最后剩下的是谁，最后剩下的是那个年龄，那个是吧，剩下年龄那个好了，这个时候你会发现，如果年龄这个也化，生完了以后就会出现哪种情况。

是不是就会出现刚才我们所说的那个特征，没有的情况，这个时候如果还有数据集的话，我就根据刚才所说的，我就只能根据里面元素里面的，标签的占比的多少来加以分类了，好吧，这就是哈D3的生成算法。

我们回过头来再看一下，第一步哈，第一步，如果你只有一个类别，那是最简单的情况对吧，你通通的只有一个类别，那我就把它拉到一个节点里面去，以当前节点类别的输出为整个绝对数的输出，如果你的特征A是个空集。

意味着你没有特征，那我只需要根据你的标签里面的占比多少啊，最大占比，那个标签作为当前的类别输出也就可以了，否则的话，那这个时候我需要计算每一个特征的，信息增益啊，每一个特征在整个数据集上的信息增益。

我找那个信息增益最大的作为当前的特征，这个特征如果还小于一直，我也不需要再分裂了啊，不需要再分裂了，我只需要把当前类别最大的，作为当前的决策数输出就可以了，否则那我根据当前信息增益最大的特征。

的不同的取值，把数据集分成若干个数据子集，每一个数据子集以其中类别标记，最大的类别作为当前的类别标记，然后在数据子集当中啊，数据子集当中减去，或者除去我已经使用过的特征，在此特征集上递归的调用1~5部。

产生进一步的产生指数，进一步的产生指数，直到什么时候，直到上述的退出条件里面，某一个条件成立，算法就可以结束了，就是所谓的id3算法，id3算法的核心，id3算法的核心。

是使用信息增益作为特征选择的依据，那么C4。5算法的核心是以信息增益比，看到了吗，是以信息增益比作为特征选择的核心，其他的完全一样啊，D3和C4。5就一点不同，哪不同。

就是他们在进行特征选择的时候的依据不一样，D3是信息增益，C4。5是信息增益比，好吧，其实你会发现这两个算法介绍一个，另外一个就换一下特征选择依据就可以了，那么大家看看关于这个生成部分有什么问题吗。

啊如果没有问题的话，我们继续啊，那么第三个问题呢，就是所谓的决策树的减脂的过程啊，呃大家体会一下啊，大家体会一下，按照刚才所介绍的决策树的，啊为什么信息增益用那个log2是吧啊还是那样。

这是信息论里面的内容，这个我们就不做过多的介绍了好吧，其实呃不是必须的啊，换成自然底数也可以啊，这嗯但是换成自然底数，你需要做相应的处理，你这个你记住吧，当一个公式把它记住就可以了好吧。

如果你有这个就是好奇心，你可以找一本信息论里的书再看一下，经验伤什么意思，嗯我们前面讲到过啊，就是在我们的嗯就是啊语境里面啊，在我们当前语境里面，所谓的经验就是数据。

所谓的经验上就是在数据集上所学到的商，所以说你会发现就是已知数据集以后，你算的那个熵值的计算过程就是所谓的经验上，所以你会发现它使用的都是一些都是什么，都是那个比值作为我们的概率值。

但是我们知道这个比值作为概率值，只能是在已知数据集的基础上才能完成的对吧，我知道你的问题是在这里好吧，为什么将实例数据啊多数表决吧，这是一种很自然的情况，就是将我们实力最大的类别作为标记。

而不是按比例打标记，因为按照最大类别做标记的话，其实就是按照比例做标记啊，你比如说我当前这个数据子集里面十个人啊，十个数据八个是贷款，两个是不贷款，那贷款那肯定是8/10，不贷款，那肯定是2/10对吧。

我只能是通过这个比值，通过这个频率值来表示它的概率值，当然嗯我们只能这样来做是吧，那么下一步就是我们看一下决策树的减脂过程，那这个时候，我们为了避免产生所谓的过敏和现象，需要对生成的过程当中。

那种嗯生成的这种区域，复杂的模型的这么一种趋势加以限制，那这个时候怎么进行限制，我们看一下，在决策树的，简直上是通过极小化决策树的整体，损失函数或者代价函数来完成，这其实和我们的策略是一样的。

只不过你需要去定义这里的损失函数，或者代价函数的问题，那么看一下怎么去定义哈，设数T的叶子节点的个数为T的摩尔啊，这是一个很好的一个策略，就在于当我们有了决策树模型以后，怎么去表征当前决策树的复杂程度。

当然我们有多种视角，比如说我可以根据当前决策树的深度，来表示当前决策树的复杂程度，当然我们知道数越深特别复杂对吧，单根节点是最简单的一个决策树，这是一种策略，那么另外一种策略我还可以根据什么。

还可以根据叶子节点的个数，叶子节点的个数越多，当前的模型越复杂，这也是一种方式，那现在呢我们是以叶子节点的个数，作为我们当前的这种考察的一个方向，所以他是叶子节点的个数是作为我们树的模。

那么T呢是树T的叶子节点啊，所有的叶子节点，该叶子节点呢我们用NT的样本点来构成啊，该叶子节点当中呢有NT的样本点，其中K类的样本点个数有NTKNTK，注意即使是在叶子节点上啊，即使是在叶子节点上。

也并不意味着当前这个节点里面，所有的样本点都是同一个类别，也会出现有不同类别的情况，最极端的一种情况，刚才我们已经说到过啊，就是什么就是一上来没有特征，只有元素啊，只有标签。

而标签里面很显然并不是只有一种类别，有贷款的，也有不贷款的，那这个时候我们根据当前节点里面啊，不同的标签的情况，再把它们进行划分啊，再进行划分，其中K类的样本点有NTK不同的类别。

标签分别为NT1NT二一直到NTK，注意就是不同的叶子节点里面是用T来表示，叶子节点里面的不同的类别，用K来表示，所以NTK代表的是T的叶子里面，取得标签值为K的元素的个数。

那么其中HTT啊是在叶子节点上的经验商啊，或者叶子节点上的数据商，或者简单点说就是叶子节点上的熵值，那这个时候我们的损失函数计算为C阿尔法T，C阿尔法T为SUT01，从一到大T的绝对值，刚才我们说过。

我们是以叶子节点的视角考察一下，当前这棵决策树里面每一枚叶子啊，每一枚叶子从一那个叶子开始，一直到最后那一片叶子，每一个叶子里面我都需要计算一下，当前叶子上的伤，再乘以这个叶子里面元素的个数用来表征的。

你你想一下，把这个值加一块，是不是，就很很很类似于我们前面讲到过的，什么经验风险，数据集上的经验风险，你看看N是对应的是一个叶子里面元素的个数，元素越多，模型越复杂，然后呢再乘以乘以什么。

乘以每一个叶子上的熵值啊，要混乱，然后它的熵值就越大是吧，越混乱，杀人又越大，那这个时候，我们把它标记为当前的一部分的损失，损失这一部分的损失很明显啊，很明显会趋于使得我的这个什么。

使得我这个模型越来越复杂啊，不管是从叶子节点的个数的角度上，还是从熵值的大小的角度上，都会趋于使得我的模型越来越复杂，这很显然会出现所谓的过拟合现象，那怎么办，看后面的项，刚才我们讨论过啊。

刚才我们讨论过，我们可以用数当中，叶子节点的个数来表示当前数的复杂程度，那换句话说，当我们把阿尔法T的模放在这个地方的时候，你会发现，当我在追求所谓的损失最小化的时候，损失最小化。

那意味着我的模型越复杂，因为只有模型越复杂了，我的分类才越准确，分类越准确，才能使我的损失越小，但是当我们的模型越复杂的时候，当我们模型越复杂的时候，那造成的一个情况就在于模型越来越复杂。

这个时候我必须要，我必须要通过我的这个正则化项，来进行一个对抗，模型越来越复杂，那这个时候我们的这个正德画像，起到了对抗的作用，就越来越强，用来限制或者抑制我前面的这一部分的，这种模型，复杂的这种趋势。

所以说这个C阿尔法T哈，C阿尔法T就表明了我当前的决策树的损失，那好了，有了这个损失函数以后，有了损失函数以后，下面的工作就是我需要把它计算出来，然后在整个的生成过程当中，使我们的损失最小就可以了。

看下面怎么计算核心哈，这里的这里的核心T的模数一下，这棵树里面一个个数就可以了，NT每一个叶子里面元素的个数也就可以了，麻烦就麻烦这里的HTT啊，每一个叶子上的损，每一个叶子上的伤怎么去算。

看下面HTT呢其实也是一个熵的计算过程啊，就是个商的计算过程，所以是sum k这没什么问题，就还是我们的K的类别，因为我们是在每一个叶子里面，所以它的分子是每一个叶子里面的，不同的类别的元素的个数。

比上当前叶子里面元素的个数，后面log是一样的值啊，这个地方非常类似哈，非常类似，我们在那个条件上的那部分的计算里面，其实也是一样的，因为你是在当前页的里面，所以你的分子是叶子里面。

每一个类别的标记的元素个数，分母是叶子里面元素的个数，这样的话我们整个的这个呃CT值啊，就可以把它计算出来，怎么去用是个麻烦事啊，怎么去用呃，有按照我们之前的策略，你直接使这个损失函数最小化就可以了。

那这个但是这个时候你会发现，只有牵扯到另外一个问题，什么问题啊，那天我曾经说过一个观点，什么观点，有些模型是没参数的，马上有同学啊，就反应过来呀，还有没参数的模型吗，是有没参数的模型的。

其中最典型的代表啊，我们可以把数模型认为是一种没有参数的模型，至于怎么定义这棵树是最关键的一个问题，特别是后面我们讲到这个xd boss的时候，你会发现，他原来真的是一个。

在定义形式上是不带参数的一个模型，当然形式是形式，计算是计算是吧，但是你会发现这个时候因为他没有参数，所以你没有办法对参数进行求导啊，因为我们知道损失函数最大的作用就是，有了损失函数。

我只需要求偏导数等于零，找到损失函数有极小值就可以了，但是这个时候呢怎么去用，看下在进行减脂的过程当中，我们是这样来用的，就是既然我们有了一个损失函数啊，既然我们有了一个损失函数。

那这个损失函数就就就可以作为啊，我来比较两棵决策树的优劣的一个指标啊，哪棵树好，哪棵树不好啊，我看下损失是不是就可以了，损失小的当然就是一个所谓的好数，损失大的当然就是一棵不好的树，那有了这个指标以后。

我们在生成完成以后，我们看一下能不能把一些节点或者某一些呃，不重要的节点在整个决策树上去掉，怎么去看下面树的减值算法，输入一棵树T啊，输入一棵树T意味着当前这棵树已经生成了，然后呢给我一个参数阿尔法。

当然这个参数阿尔法，就和我们前面那个学习率是一样的，一个类似的一个数值，用来决定了你这个正则画像的一个重要性程度，输出的是修剪之后的一个指数啊，T阿尔法在阿尔法的这个已知的条件之下。

第一步计算每一个节点的事啊，你不是有一棵树了吗对吧，不管长什么样吧，反正这个数你就有了，然后呢你需要计算每一个节点，当然这里每一个节点不只是叶子节点，每一个节点的熵值。

然后呢递归的从树的叶子节点向上回缩，我通过叶子节点加以判断，判断什么呢，设一组叶节点，回缩到其父节点之前与之后的整体数，分别为tb与ta tb啊，是之前的那个数，TA呢是回缩到父节点以后的数。

比如说这是原数T我们呢通过分析会发现，可以把这两个节点回溯到它的父节点当中去啊，就从原数变成了这样的一个新的数，然后呢我我把原数叫做T，新数呢叫做ta，我分别计算一下。

CTB就是原数的损失函数和心数的损失函数，如果什么，如果原数的损失函数大于了新数的损失函数，意味着什么，意味着原数的损失大，新数的损失小，那你说你要哪个数，当然我要损失小的这棵树嘛，好了。

这样的话我们就从原数进化到这棵树，那有了这棵树以后怎么办，再从所有的叶子节点依次来进行上述的过程，然后比如说哎，我还可以把它回溯到根节点里面去，就只有单根节点的数，我再来比较一下，它就变成了tb。

它就变成了TA再计算一下他俩的损失情况，如果还是这样的话，那我就把它剪成这样了，继续剪成这个样子，所以说这就是减脂算法的一个基本策略，他的思路就在于，我需要给出一个损失函数的一个定义，有了损失函数以后。

我只需要从根节点开始，每一个根节点开始依次向上回缩啊，在回收的过程当中，依次判断新数和呃，心数和元素的损失值的大小，如果出现了原数比比心数的损失要大，那我就进化成一棵新树，反之那我就不用停止。

我就可以停止整个算法，数据很好了好吧，这就是减脂过程，看有什么问题吗，第五步，认证算法的第五步，当然啊这个地方是根据这个特征的，每一个可能的取值对吧，生成过程当中，我比如我选择了这个特征A。

这个特征A的不同的取值，来对我当前的数据集进行一个子集的划分，你当前这个特征A能取三个值，你就把当前的数据集分成三三份，三个子集，如果你特征A能取五个值，那就把当前数据集分成五个子集啊。

这个地方是根据特征A的不同的取值，来进行划分的，Ok，好那这样的话我就能介绍完了，D3C4。5的特征，选择决策树生成和数的减值，那么前面我们介绍过这两类数啊，基本上都是做我们的分类问题啊。

解决的都是分类问题，所以说你会发现它的计算，都是以类别标签的频率值，来表示它的概率值是吧，所以一般都是分，就是离散情况下我们才可以这样做，既然是离散情况，它的取值就有限，所以一般都是一个所谓的分类啊。

分类问题，那怎么去解决连续值啊，怎么解决这个回归问题，是我们下边卡的数需要解决的cut数啊，他名字就是所谓的回归和分类数啊，你就说cut数既可以做回归，也可以做分类。

当然分类一会儿我们看看它是怎么去做的，核心我们看看他怎么做回归的啊，在介绍这个回归生成树之前呢，就是非常重要的一个概念啊，就是树的核心到底是什么，就是什么才是一棵树模型啊，什么才是一棵树模型。

按照刚才我们的分析，你会发现我们是不断的从根节点开始，不断的去找特征是吧，从根节点不断的去找特征，找到一个特征进行一个子集划分，找到一个特征进行一个子集划分，直到我们不再进行子集划分为止。

如果我们以刚才的数据表格为例，我们知道一个数据表格，对应的就是一个N维的一个特征空间，里面的一个数据情况，注意啊，因为我们知道每一个记录，它都是一个X11X12。2点，一直到X1N对吧。

当然这里是N个特征，那这个时候我们知道哈，在这个N个特征里面的一条记录，对应的就是我们N维空间里面的一个点，那么这个点一定要落到我们决策树当中的，某一个叶子节点当中去，而这个叶子是怎么来的。

而这个叶子是从根节点开始，根据不同的特征划分落到的一个小区域，换句话说啊，直接说结论，你会发现我们的决策树啊，我们的决策树就是不断的从我们的输入空间，还记得那个花X吗，他是个N维的空间是吧。

从那个花X那个N维的空间，根据每每一个特征的不同的取值，对这个词，对这个输入空间进行一个子空间的划分，你看看根据A这个特征划分成了两个空间对吧，然后再根据B这个特征，比如说划分成三个空间。

根据不同的特征，把每一个，把某一个输入空间划分成了若干个子空间，然后呢，然后到了叶子节点的时候，给出当前这个叶子节点所对应的标签，给出个值就可以了，你你你是一个三好学生，就给你个一，你不是个三好学生。

就给你个零，所以说简单点说啊，简单点说决策树模型啊，就是对我们输入空间的一个划分，以及每一个子空间的输出值，这就是决策树，所以说啊你看它的定义形式来看这个样啊，一个回归书上。

就是将我们的输入空间划分成若干个单元，以及每一个单元上都对应的有一个输出值，假设已将我们的输入空间划分成了M个单元，R 1r2，一直到RM并且在每一个单元上啊，每一个单元RM上都有一个固定的输出值cm。

于是回归数可以表示成下面这个形式长什么样，FX啊，就是一棵树，就是个模型啊，以后我们就不再强调了这个模型是吧，只不过这个模型呢被定义为这么一种形状啊，它就是科学的数啊，为什么是科学的数呢。

看一下M从一到大M，你无非就是把你的输入空间划分为了，这M个子空间，我当前X属于哪一个输出，你看你给我一个X，我要给你个输出吗，我这个输出到底是几，我需要遍历一下我当前这M个子空间。

看一下我这个X到底是哪一个子空间，里面的元素，如果我能够判断我这个X属于R1，那我只需要把R1这个子空间所对应的，那个C1作为我当前的输出，是不是就可以了，看一下啊，怎么做的。

你先有X首先需要判断我是属于哪个子空间，找到它所对应的子空间以后，再去把这个子空间里面的输出找出来，作为我当前输入的输出就OK了，这就是决策树啊啊没有结构了，已经已经我我们已经忽略它的树形结构。

而只是把它还原成它最本质的那一点，就是在整个输入空间上的输出呃，这个输入空间上的划分，以及每一个子空间的输出，这种决策树啊，希望大家能够尽快的能够认识到这个层面上啊，这才是数决策树的最本质的核心。

那为什么下面这个形式注意啊，我们刚才这个过程当中，有一个遍历所有子空间的过程，因为他需要看一下它到底属于哪一个子空间，从R1R2对吧，一直点点点一直到RM，我需要看一下你到底属于哪一个子空间。

那怎么去便利，那这个时候就需要使用到的求和计算，这个地方再强调一点，就是说sum求和啊，就是求和过程我们是一个计算过程，1+2加3+4等等等等，一直往下加，所以说在这个求和过程当中，我就可以去判断一下。

当前我这个X是否属于某一个子空间，怎么去判断，根据后面这个东西I，关于这个I我不知道大家还有没有印象，I是什么东西啊，I是一个所谓的指示函数，我们上一节课讲到过，指示函数的作用是如果后面的条件成立。

返回一，否则返回零，看一下后面这个条件，X属于RM，也就意味着如果后面这个条件成立，意味着当前的输入X是2M，这个空间里面那个有一个元素，那么只是函数就返回一，返回一，再和前面这个cm cm所对应的。

就是RM空间的输出值，那么返回的是不是就是当前X所属于的，那个子空间的对应输出，有同学就会问你这个sum求和，你不光是需要判断一下是不是属于RM，你还需要判断一下是不是RM加一，RM加二。

那这个时候没问题啊，因为我们能够明确的知道X如果属于RM，就一定不会属于其他的子空间，既然不会属于其他的子空间，那么意味着这个条件就不成立，这个条件不成立，只是函数返回的就是零，零乘以任何值都是零。

所以即使是进行了一个累加，也仅仅是在X属于某一个子空间的时候，他的条件成立，返回一对应的返回这个子空间的输出cm，而在其他的空间，因为X不属于对应的其他空间，所以这个条件就不成立。

只是函数返回零零乘以任何值都是零，在累加过程当中不受影响，所以说你会发现这个FX它很技巧性的用到了，这个sum求和这个过程，以及这里的指数函数的判断组合起来，其实就是在整个的子空间序列当中。

一次的进行判断，判断一下是不是X属于某一个子空间，如果是，那输出它对应的输出，如果不是返回零，不影响整个的求和过程，然后当然如果当前这个不是我还求和吗，这个过程还需要看下一个值，再看下一个值。

再看下一个值，因为我们这个X1般，如果是在我们的输入空间里面的话，它一定属于某一个子空间，一旦找到了子空间，返回一下子空间的对应输出就可以了，所以说啊这个最重要啊。

这个东西我觉得在chart boost里面，对于模型的这个理解是非常关键的一个理解，好吧，那好了，有了这个链接以后呢，下面我们需要完成回归操作，那怎么去完成回归操作，如果我们拿到了一个数据及D。

它是X一Y1XNYN，那我们知道这个地方麻烦就麻烦了，他的这个假设数轴啊，它是个连续输出，它是个实质输出，不像我们的离散输出，我们可以把它发生对吧，分成有限个子空间。

那这个时候每一个子空间有一个取值就可以了，麻烦就麻烦，他是个连续空间，那这个时候怎么办，也也也也不是问题啊，我们只需要把这些子空间的这空间大小小一点，就可以了是吧，那怎么办，看下面我们是这样来做的哈。

我们可以选择D这个变量，XG以及其取值，S作为切分点和切分变量和切分点，并定义两个区间或者两个区域，你你确实数无非就是子空间的划分，以及每一个空间上给我一个值吗，首先我们先解决这个空间怎么划分的问题。

那怎么划分呢，我试着去找到一个所谓的空间划分啊，所谓的空间划分无非就是特征上的选择，以及特征上的不同的取值来进行空间的划分，比如说我们以特征A为例是吧，比如说我们以年龄为例，年龄分为中年青年，中年老年。

我就可以把特按照特征A划分成三个子集，有同学说那是离散特征，你这里可是连续特征啊，无所谓啊无所谓啊，我因为我们拿到的数据集是有限的，既然你的数据集是有限的，那意味着即使是连续值也是有限。

个连续值其实还是在数据集上加以学习，所以说这个地方，它即使是一个连续值的回归问题，其实因为你拿到的数据集是有限的，所以其实还是一个分类，只不过这个分类的区间，会发的会划分得非常的小而已。

那么怎么去找这个特征以及切分节点，因为我们知道任意一个特征我都可以做选择，任意一个特征里面的任意的取值，我也都可以做选择，那看下面我不管我选择的是哪个特征，也不管我选择的特征是哪个取值，我总可以。

根据某一个特征以及某一个特征的取值，将我们的区域，将我们的输入空间划成两部分，哪两部分小于这个在这个特征值上，小于这个值的，以及在这个特征值上大于这个值的啊，你会发现比如说还有那个例子。

我们以那个什么吧，比如说还是以年龄吧好吧，还是年龄，比如说青年对吧，我根据青还是年年龄，在年龄这个特征里面，我还是根据不同的取值，比如说青年，我可以把青年比青年年龄小的一部分，比青年年龄大的一部分。

是不是可以啊，所以说你会发现我不管是根据哪一个特征，还是根据哪一个特征上的取值，我都可以把输入空间划分成两个部分，有了这两个子空间以后，那怎么办，我就找所有可以把这些空。

把这个空间分成两部分的所有的特征，以及所有的特征可能取值上去找，计算下面这个量哪个量呢，看下面，既然我们把这个按照当前这个特征，把输入空间化成了两部分，那么在这划成了两部分，比如说就像刚才那个数轴。

这是根据那个那个年龄就是青年，那就是大于青年的，这是不是大于青年的，这是小于青年的，好吧，那好了，那无非所有的数据都落在这个区间上，不管你是到底是青年，中年还是老年，反正都是在这个区间上。

那这个时候怎么办，我在每一个可能的切分点上我都这样划分，每一个切分点上都这样划分，是不是可以划分完以后怎么办，划分完了以后，我需要把所有左边这些和所有右边这一些的值，累加起来累加，不是把原值累加。

累加的是什么，累加的是每一个值和这个子空间里面的，那个平均值的差值进行累加啊，比如说我的我左边这个子空间的平均值在这，右边这个子空间的平均值在这儿，我把上面这些所有的点距离，这个平均值的距离都计算出来。

并且累加到一块，你看是把平均值的距离进行累加啊，进行累加累加在一块怎么办，我使得这两个P区间里面所有点距离，这个平均值的和尽可能的怎么样小，你说这个怎么尽可能小，因为我们知道，因为我们刚才说过。

我们这个切分点以及切分点上取值，是有是有多个的，你当前这个计算出来以后，当前这种划分我计算出来一个Y1减C1，加上一个Y2减C2，我其他的切分点也可以分别计算，计算出来之后，我尽可能的小，最小的那个。

作为我当前的特征以及特征上的取值，也就是说刚才那个例子里面年龄是一个特征，年龄里面的青年啊也是一个特征里面的取值，根据这个特征以及这个特征取值，我们把整个数据集划分成两部分。

分别计算每一部分的平均值的和的最小值，然后再根据刚才说过那个那个信贷情况，信贷情况良好，中差也可以根据不同的信贷情况的，不同的取值依据，划分成两个空间，来不断地计算。

每一个空间里面的元素和平均值的这个差值，尽可能的小，来比较一下这两个值谁更小谁更小，就找到一个更小的特征以及特征的取值，最小的那个作为我的切分特征以及特征取值，然后呢特征划分完了以后。

我需要根据特征上计算它的输出，特征上的输出就很简单了，当你有了这个区间划分以后，我只需要根据这个特征区间里面所有的元素，求一下平均值，作为我当前空间的出输出就可以了，这就是哈。

其实你可以看到说是解决的回归问题不错，但是在解决回归问题的过程当中，因为你这个里面那个D数据集是有限个元素，所以也是一个便利的过程啊，也是有一个遍历过程，只不过这个遍历过程里面，特别是这个S的选择。

这个S很显然可能是个小数啊，就是个实数啊，小数啊一般是小数，不是实数，实数里面可能无理数，这个地方不会出现啊，所以这个地方比如说以身高身高里面一，因为我们知道身高这个特征很显然是个连续值。

在测量过程当中，因为比如说我们班级里面有60个同学啊，可能从1米92对吧，一直到1米68，1米1，1米58是吧，所以这个过程里面即使是连续值，它也是一个若有有有数据量的一个连续值，所以这个时候你会发现。

我根即使是根据这个身高，我也可以选择一个其中的某一个位置，作为我两个区域划分的最优的切分位置，好吧，那这样的话完成的就是所谓的，回归问题的一个分析，那么其过程啊，在这个地方核心的就是怎么找这个特征以及。

特征上的分分类点，其实就是一个便利过程啊，找到这个便利过程就一分为二，那一分为二之后怎么办，那么在剩下的那个子空间分成了两部分，子空间里面，再一次按照我们刚才的过程依次查找就可以了。

OK这是关于这个cut数的回归问题，那cut树的生成怎么做，cut是数字生成，一句话，cad数数字生成的特征，选择使用的是基尼指数作为特征，选择依据基尼指数的计算，在这它的生成过程和C4。5那个id3。

算法是一样的啊，生成过程是完全一样的，就是把D3和C4。5里面的信息增益和，信息增益比用基于指数来替代，基于指数也是我们信息论里面的一个概念，它也可以用来表示我们关于这个信息的一个呃。

不确定性的一个度量啊，他们其实和那个信息增益啊，是有这个数值上的关系的，这个地方就不展开讲了，你知道就可以了好吧，那这样的话以上啊，以上我们就把，决策树这一部分的原理部分介绍完了。

那么下面呢同样去准备了一个实例，这个实例呢，正好也是我们这个教材里面的那个例子啊，数据集这个地方都已经给大家了啊，感兴趣的同学啊，你可以回去以后做一遍啊，你可以试一下怎么去计算。

当然这个地方也不做过多的要求啊，因为这个呃我后面会讲CK那里面的包的调用，其实参数才是重要的，你怎么去理解那一堆参数啊，有兴趣的同学你可以自己看一下，好吧，判这不还有什么问题吗，那没有问题的话。

我们就继续好吧，嗯决策树这部分完成以后呢，我们今天还有下面一个任务，就是关于这个集成方法，看一下，OK嗯集成方法，什么叫集成方法呢，简单点说哦，中国有一句这个俗语叫做三个臭皮匠，顶个诸葛亮。

集成方法就是这么回事儿，当我们构建模型的时候，你会发现模型的性能有的好有的差，那怎么去提升模型的性能，就是一个很关键的问题，那怎么去提升呢，有很多种不同的策略，其中有一种策略呢非常简单。

就是既然某一个模型，它的性能并不是那么的突出，那我是不是可以通过集成若干个模型，来提升集成之后的模型的性能呢，这就是boosting方法最简单的一个思路，那其实是可以的，就是我们通过集成不同的模型啊。

把这些性能可能普遍不好的模型集成起来，使大家能够趋近于性能非常好的那个模型，结果啊，这是boosting方法的最基本的一个思路，那怎么去做两方面，一方面呢就是所谓的加法模型。

第二方面呢就是所谓的前项分布算法，我们分别来看一下，第一步先看一下加法模型，什么是加法模型，加法模型呢是这么一类模型，它的形式呢是这样说的，FX等于sum，从一到大M贝塔m bx伽马M。

分别介绍一下其中每一个符号的含义，其中这里的bx伽马M把它称之为是奇函数啊，基础的意思啊，基奇函数奇函数呢其实就像刚才我们所说的，这里的bx伽马M，就是那一个一个的性能不太好的模型啊。

奇函数或者G模型啊，其中的伽马M呢为G模型或者奇函数的参数啊，嗯就是那些臭皮匠啊，这是那些臭皮匠，然后呢贝塔贝塔M是奇函数的系数，因为这些奇函数各有优劣是吧，有的好有的差。

那么为那些好的你就多做一些贡献，给你一个较大的全职，那些性能差的啊，你就你你又能干点干点不能干算了是吧，所以就给出一个小的贝塔值，所以这里的贝塔为奇函数的系数，为奇函数的系数。

那这样的话每一个奇函数都给你一个系数，然后把这一堆奇函数加起来构成FMFX，这是刚才我们所说的，都是方法最简单的一个思路，非常简单粗暴，把所有的奇函数统统的加一块，就构成了FX，那好了。

加法模型其实很简单，问题在于它的优化过程是比较麻烦的，为什么这么说呢，看下面假设我们有数据集xi一直到YI是吧，这是XN啊，xi i从一到N以及损失函数，我们都已经知道了啊，这里的损失函数呢。

八成是可以使用我们的平方损失啊，使用我们的平方损失，这个时候我们的学习目标变成了什么呢，变成了在数据集上，我们要分别计算我们的实际输出，YI和我们的模型输出FX的损失进行一个累加，然后当然取不取平均啊。

其实对结果并不影响是吧，这个时候使得我们的经验风险最小的那个模型，作为我当前的最优模型，注意啊，这一坨就是那个FX是吧，这是我们的基本策略，但麻烦就麻烦了，因为它是一个加法模型。

它不是单一的模型的一个优化问题，而是你加起来那一堆模型的整体的优化问题，所以一般情况下呢，这个整体优化呢是比较复杂的，我们一般完成不了，那怎么办，看下面我们就变一个策略，就是所谓的前项分布算法，前项。

前线分布算法，它是求解这一优化问题的一个思路，因为学习的是加法模型啊，因为你学习的是加法模型，我可以从前向后，每一步只学习或者只优化一个奇函数及其系数，逐步逼近我的优化目标。

那这个呢我就可以减少或者简化，我的优化复杂度，那具体的怎么去做，具体的就是我每步只需要优化如下的损失函数，哪个呢，sum从一到N还是在数据集上，然后呢，L我的损失计算是YI还是我的实际输出呃。

我的这个实际输出，我的预测输出不再是这对整个的sum求求优化，因为不太好求，我只求其中的一部sub的一个，其中的一步做优化啊，这就是我们的这个前线分布算法，当然有同学会问，很显然你这个单个基函数的优化。

和你加法模型的优化优化目标是不一样的，那怎么去解决他们两个之间的冲突呢，来看下面好了，前面分布算法呢进一步的解释为，输入是我们的数据集，损失函数以及我们的奇函数的集合，都是已知的啊。

就那一堆臭皮匠先放在那是吧，怎么用，我没有在说，那输出呢就输出我们整个的加法模型，看下面第一步，初始化F0X等于零，还是那个老问题，F0X等于零，也是一个模型，一上来我把所有的输入都映射成零。

虽然这个模型性能很差，差就差了，但是它不妨碍它也是个模型做基础是吧，那继续往下，对于M等于一二，一直到大M我们分别来优化下面这个问题，什么问题呢，看下面SAMI从一到N还是在数据集上。

然后L是我们的损失计算，在损失计算过程当中，YI作为实际输出没问题，看一下预测输出是个什么东西，是预测输出是FM减1X，那我当前这一步很显然是FMX，我在进行DFMFMX的优化的过程当中。

我需要使用到过FM减1X，也就是说，我需要使用到上一次优化结果的那个函数，有同学说上一次那个优化结果的函数是谁啊，那很显然上一次那个优化结果，那个函数的优化过程我现在不知道，我先不管他是谁好吧。

那么它的优化过程，很显然使用到了上一个优化函数的，上一个优化函数，那上一个优化函数的上一个优化函数又是谁呢，一直往回找找找找，找到头，找到头是谁，找到头就是这个F0X等于零。

也就是说我从F0X等于零开始，我要这是零啊，我先要计算F1X在F1X优化过程当中，我使用到了那个FM减一，这时候1-1不就等于零了吗，对吧，所以说不要担心那个FM减一在哪，既然你有了F0以后。

FM减一就有了好吧，在我上一次那个优化结果的基础上，再加上一个贝塔b xi伽马，什么意思呢，上一次那个优化目标性能，我不管它是好还是不好啊，不管他性能怎么样，我当前这个优化目标要和它做加法。

意味着我在他的基础上再要学习一个新的模型，这个新的模型和我上一部的那个模型做融合，以后，作为我当前这一步的模型和我的实际输出，做算式计算，使得我的损失尽可能小，以以确定我当前的最优模型，那么大家想一下。

想当然的，我们就可以认为当前学习的这个子模型以后，所构成的那个加法模型，就要比单纯的那个FM减1X的性能，要怎么样就好了，好怎么体现出来的，好就体现在我当前哎往前又增加了一个子模型。

而这个子模型是在原来这个模型的基础上，又使得数据集上的损失极小化，以后所学到的这个子模型，那很显显而易见的是，它比那个FM减一要好，这还没完啊，因为我们知道这是个迭代过程，有了FMX以后。

我在学习的一定是FM加1X，而那个时候我同样是在整个数据集上计算损失，实际，喂喂可以吗，可以了吗，OK可能是刚才信号不稳定的事情到哪了啊，那我们把它串一下吧，把它串一下。

可能刚才呃可能会有停顿的地方是吧，哦可可回头看一下加法模型啊，先给加法模型，其实java模型很简单，就是有若干个机模型啊，性能很差，然后呢我我我为每一个机模型呢，再给他一个全职。

然后把这个带全职的G模型呢，一个一个的加起来，就构成了所谓的加法模型，这样的话呢，我们试图通过若干个机模型的性能，来趋近于一个性能更好的模型，那这个时候呢有了这个模型定义以后呢，他的下一个问题就是。

优化过程是一个比较复杂的问题，因为如果你直接对整个加法模型加以优化呢，这个比较困难，因为它这个具体形式是个累加的过程，那这个时候我们做一个所谓的呃，一步一步的去完成它。

这样的话我们采用所谓的前项分布算法啊，就是每一步我只优化其中的一部分啊，这样的话很显然就要比整体的优化，是一个更好的一个策略，那么下面的过程呢看一下，首先呢先给出一个性能很差的F0X等于零。

作为一个基础啊，作为一个极限，有了这个基础以后，每一次我都是在上一次的那个模型基础上，来训练一个得到新的模型，那么怎么体现出来呢，每一次我的学习都是在数据集上进行损失计算。

只不过这次的实际输出和这个预测输出的差值，我是在上一次那个模型的基础上，加上我当前的这个模型来构建一个优化目标，而这个优化目标过程当中需要注意的是，YI是已知的，因为它是数据集里面的值。

FM减1XI也是已知的，有同学说这个FM减1XI怎么是已知的呢，因为你当前优化的是DM是DFMX，在DM部的时候，这个M减1X是上一部的那个模型啊，上一部的模型一定是已知的了，对于当前这一次来说。

所以你会发现在整个的优化目标过程当中，只有这里的贝塔b xi伽马是不知道的，而这就是我们的优化目标，使得这个损失最小的那个贝塔和伽马M，作为我当前模型的参数和它的系数，这是在DM。

那么同样这个需迭代过程要继续下下去，那么下一步一定是FM加1X，那它的优化目标一定是在同样的逻辑里面，是以FMX，因为上一部FMX已经被学出来的对吧，在FM减1X，在FMX已知的条件之下。

再学一个新的模型加到FM上去，那么这就构成了FM加1X，那么同样的道理，FM加二，FM加三一直到F大M，那这个时候你会发现整个的累加完成以后啊，每一步都是学了一个新的子模型。

而学的这些新的子模型的累加的性能，一定要比每一个单个的子模型的性能要好，这是显而易见的，这被称之为是boosting方法啊，boosting方法两部分，加法模型和前项分布算法，加法模型和前向分布算法。

那么你会发现在这个boosting过程里面，关于这里的B就是那个G模型，是没有明确要求的，就是这个机模型啊，只是一个机模型，它到底长什么样，我是不知道的，那这个时候看下面所谓的提升决策数啊。

所谓的提升决策树，就是以决策树作为基模型的提升方法，称之为是提升决策树啊，就是我们把那个G模型啊，以数模型作为基模型，不断的累加决策树模型来构成我们的，据他说这里有个问题，数模型怎么进行累加树啊。

我们知道典型的树形状就就长这样是吧，这些东西怎么能把它加一块，不要忘了这是他的形象啊，或者形式，它的定义是什么呢，看这里，这才是那棵树，还有印象吗，哎不是不是这个多少在哪儿，他在他在这，这才是决策数。

不要忘了我们前面一直在强调什么是决策树，决策树无非就是对输入空间的划分，输入空间的划分以及每一个输入空间上的输出，我把当前的输入空间化成了R这个子集，然后呢每一个子空间上对应有一个输出。

这就构成了一棵决策树，那么这个时候你再想一下，两棵决策树的相加，就变成了两棵决策树的输出，空间的进一步的细分，以及细分之后的输出空间的呃，细分之后的空间的输出对吧，这就是哈怎么去理解决策树的问题啊。

这就是有了决策树的定义形式以后，我们就可以完成，你会发现它是可以进行累加的，那这个时候所谓的提升决策树，就是以决策树作为基模型的加法模型，FM等于sum，从一到大MTX大米M这个时候有同学会问诶。

你前面那个G模型B前面不是还有一个贝塔吗，在这个时候啊，一般情况下，我们认为以决策树啊，以树作为基模型的这个boosting方法，我们就不再对每一棵树给它一个系数了，我们认为所有的决策树都是重要性。

都是相同的，就可以了，省了一个参数的学习啊，省了一个参数的学习，再看下面，既然有了这个以以以树为基模型的，这个嗯加法模型以后，那下面的一个问题，就是对这个加法模型的一个学习，怎么学习，看下面。

嗯嗯提升觉得数同样采用的也是前两分布算法，其实算法过程是一样的好吧，首先也是确定初始提升数F1X等于零，有同学就会困惑，F0X等于零是怎么是棵树吗，也是一棵树，是一个单根节点的树。

所有的输入都映射成零是吧，那DM部的模型是FMX，同样是等于FM减1X，上一部的那个模型加上一棵树啊，加上一棵树，那它的优化目标呢就变成了在数据集上，损失函数的累加，实际输出是YI。

那么模型就变成了FM减1X加上那棵树，作为我DM部的模型这个词，这个时候我们把当前这棵树学出来，就变成了FMX，那么有了FMX，那下一步是FM加一，那这个时候用到的是FMX，所以和加法。

加法模型的前项分布算法过程是完全一样的，只不过它的学习过程里面，把奇函数替换成了我们的数模型而已，那么这个时候你可以看到啊，你可以看到，我们就是给出当前这棵决策树的定义形式，就是输入空间的划分。

以及每一个空间里面的输出，计算过程就是F0X等于零，FMX等于FM减1X加上一个绝对数，通过叠的过程，最终我们学到的是F大MX，把所有的每一步里面的绝对数进行一个累加。

最终得到的就是我们的java模型的结果，这个过程啊我相信应该没有什么太大的问题啊，最后呢有一个非常重要的一个结论，哪个结论呢看这里，因为刚才在学习过程当中，我们一直没有讨论那个损失函数啊。

没有讨论损失函数，我们常用的损失函数啊，就是所谓的平滑损失啊，L实际输出是Y，这里呢预测输出是FX等于Y减去FX的平方，Y减FX的平方，然后呢我们按照我们的损失展开。

因为刚才我们的损失函数啊是LYFM减1X，加上一棵树T是吧，加上一棵树T，这个时候呢，我们把这里的L用我们的平方损失代入，就变成了Y减去FM减1X减去那棵决策树，因为减的和把它拆开之后。

把所有的分量都需要减掉，然后再进行一个平方，从这一步到这一步，就是一个平方损失的一个展开啊，这应该没什么太大问题，展开以后啊，你看看展开以后的这三项，我们分析一下，就像刚才我们得到的结论是一样的。

YI是已知的，YM减1X也是已知的，在当前DM部的模型学习过程当中，只有谁不知道，只有这棵决策树是不知道的，因为我们要学习一个新的模型，在这三项当中有两项是已知项，是不知道。

那我是不是就可以把这两项已知的，先计算出来，先计算出来，用R来表示，用R来表示，整个式子就变成了什么，你看看就变成了，R减去决策树的平方的形式，那这个是形状，你再和原平方损失对应一下，你会发现哎。

这个时候的R是不是就变成了这里的Y，这里的T是不是就变成了这里的FX，换句话说，在前项分布算法的这个学习过程当中，如果我们以平方损失作为损失函数啊，以平方损失作为损失函数展开，我们的展开以后。

你会发现每一次的学习目标，每一次的学习目标变成了什么呢，只是我要学习一棵树而已，你看看你原来是在外上学习一个FX，现在T就是你的学习目标就是这个FX，不同的是在哪儿，不同的发生了变化。

在于你的数据集发生了变化，你不再是在原数据及外上进行一个学习，而是在哪，而是在R上进行一个学习，而是等于什么，看上面，而是等于实际输出Y和上一次那个模型的预测，输出的差值，作为我当前的学习目标R。

这其实就解释了，为什么我们说boosting方法是一个性能，可以趋近于复杂模型的方法的，原因就在于我每一次学习的这个新模型，注意安之类的，T是每一次学习的这个新模型，它学习的对象是非常有针对性的。

就是针对于我实际输出，和我上一次那个模型没学好的那一部分，再去学一个模型，换句话说，我我所有的模型就是该进行X向Y的映射，我在学习过程当中已经有很多的工作，我的钱去工作都已经完成了。

或者前序模型都已经做了，都已经完成了，很好的从一部分X向一部分Y的映射了，我当前要完成的工作，你看我当前这个模型要完成的工作，就是针对我实际输出，和我上一次或者上一轮那个模型没学好的，那部分的学习。

所以你会发现它是非常有针对性的，而我们知道，当我们因为数模型本身的性能其实也不差是吧，只是说他没有那么好而已，如果说你会发现他每一次都是这样去做，其实非常类似于什么，就是我们上学的时候都会有一个错题本。

大家都会有印象是吧，每一次考试的时候，我把那些做错的题目单独拿出来啊，在考试之前做对了的，我就看一眼就可以了，我就有针对性的对那些每一次做错的，或者不对的地方来进一步的加以学习，那你想想，这个时候。

我对你成绩的提升还是有很大帮助的啊，这就是说啊，其实你会发现如果有了这个认识以后，所谓的前项分布算法啊，所谓的前项分布算法就退化成了一个，每一次我只需要学习一个新的决策树的过程，而只是需要做的一点调整。

在于我每一次学习的这棵新的决策树，的数据对象是不一样的，不是每一次都是在外上学习，而是在Y减去我上一轮那个模型之后，没学好的那一部分，作为我的学习目标，每一次我都这样过，每一次上来之后。

我先计算一下Y减去FM减1X剩下的这个差值，没学好的这一步我在学它，然后把学习的一部分的模型，加入到FM减1X里面去，然后再计算Y和FM的差值，我FM加一再针对Y减去FMX的时候的差值，再进行学习。

性能会不断的，你想想那那那那错的部分越来越少，越来越少越来越少，最后不就趋近于性能更好的一个模型了吗，好了这是非常重要的一个结论啊，非常重要的一个结论，而其中这个R被称之为是残差啊。

这里啊当前模型拟合数据的残差被定义为R，就是我们每一次所谓的残差嘛，就剩下的不好的那部分数据，我们每一次学习的仅针对它展开就可以了，GBDT我们放到茶几boss的时候，我们再见，再介绍这一部分呢。

这个和后面有非常强的关联性啊，这个地方先留在这儿啊，其实内容也不多了啊，但是呢也非常重要，我们下次讲到查理boss的时候，我们继续再把它展开好吧，这样的话我们今天的内容，我们就是这些内容呢很多啊。

包括D3C4。5和卡特树的生成啊，啊特征选择以及呃这个减脂啊，下面呢包括我们的这个boosting的，加法模型和前线分布算法，看这部分还有什么问题吗，决策树的回归问题是决策树的回归问题。

已知是个老大难问题，看看决策树的回归，回归回归回OK啊，我们以身高为例啊，举个例子，身高我们知道是个连续值，但是即使是连续值呢，因为你这个地方是数据集，数据集，比如说我们还是这里的N等于60。

我们只有只能采集到60分身高，那参照采集到这里时的身高呢，可能比如说有1。92，我们已预计作为单位啊，1米92，然后1米68，1米73呃，1米811米呃，五九好吧，然后1米77，1米66啊等等等等。

虽然说是连续值哈，没问题啊，他带小数点吗，虽然说是连续值，但是你采集到的也就是60分这样的值，也就采集到60分这样的值，那这个时候呢我们以身高这个特征，在所有的这些值上，我就可以进行60次的一个划分。

怎么划分啊，第一次划分为1。92，作为一个划分位置，把它划分成小于1。92的，和大于1。92的啊，简单点说你比如说你看这个极限值，你飞到哪吧，比如说你化成小于等于1。92的，然后化为成大于1。92的。

这个时候是一种划分，还可以根据1。68，划分成小于等于1。68的，然后大于1。68的，根据1。73，你也可以划分成小于等于1。73的，大于1。73的，你会发现啊，在这一个特征上，我就可以进行60次划分。

这身高特征，假设我还有一个特征，身高还有体重，体重，比如说我们以这个啊公斤啊，公斤的话，比如说呃60kg啊，80kg嗯，72。5kg嗯，100。6kg，然后55kg等等等等等，同样的道理，同样的道理。

我们即使是离散值啊，即即使是连续值，因为它是取值集合有限，所以可以根据不同的取值，可以把我们的取值空间进行划分，这就解释了第一个问题，就是关于这里的特征以及特征上的取值的问题，任意一个特征。

即使你是一个连续值，因为它的取值有限，所以就可以把当前的特征化成两部分，一个小小的一个大于一个小雨的，一个大于一个小雨的一个大雨，这是空间划分的问题，空间划分完了以后，下面的问题就是空间的输出的问题。

就是说比如说我们以身高这个特征为例，比如说我们以1米73这个身高为例，我们就可以把特征划分为小于等于1米73的，和大于1米73的，这里面是若干个取值对吧，比如说这是1米92。

那个1米92在这1米83的二，1米77在这，这是那个1米73，那好了，那么既然我们划分成了两个子空间，我们前面讲到过，决策树，无非就是输入空间的划分以及划分空间的输出，问题。

就在于我怎么去确定这个空间的输出的问题，最简单的方法就是我们把落在这个空间里面，所有的值加起来取平均值，假设啊求个平均值是1米7零，那这个时候你会发现我根据身高这个特征，根据1。

73这个这个这个什么这个呃，取值划分成了两个空间，小于1。73的，得到一个1。70，大于1。73的，比如说是1。82，好吧嗯，这个时候我们就有了空间的划分，以及空间所对应的一个输出。

我们身高上课也是这样做，注意啊，这是身高上的一个1米73的位置，我所有的点是不是都可以这样做，然后呢这只是在身高上，我体重上是不是也可以这样来做啊，这样的话你会发现我在所有的身高上啊。

所有的特征上都可以这样，按照每一个特征值来进行两个空间的划分，那好了，既然我可以这样做，我就把这样的所有的情况都考虑出来啊，都做一遍，都做完一遍以后呢，因为我们知道每一个空间都有一个特征，空间取值。

而每一个空间里面又是说看个点，那我再计算一下所有可能情况里面，所有这些空间里面的点距离，这个输出值的差值，差点使得这个差值算出来以后，我去找所有那些差值里面最小的那个。

那么它所对应的特征以及特征上的那个取值，就作为这里的JS，这就是完成了一步一步特征的选择，以及特征上的取值的选择，那下面怎么办，其实类似啊，身高做完了以后，下面就是除了身高以后。

在其他的特征上一次再做上述的过程就可以了，回归数哈其实和分类数是一样的啊，只不过呢就是它的这个嗯特征分裂呢，需要遍历所有的数据集，是个挺麻烦的问题啊，其他的也就没什么太大问题了，看看还有什么问题吗。

OK那这样的话我们今天的内容就到这吧，如果大家有什么问题的话，我们可以在群里在一块提出来，我们再讨论，好吧如果没有问题，今天我们就到这吧，啊有些人能够理解又不理解，不知道该如何自处，那就是没有理解啊。

这个你放心，不能够期望于你听这么一遍，就能把这些内容能够全部理解，这也是不现实的呃一种预期，所以回去以后啊，方法是什么，就是把这些材料啊，至少把这些材料你需要把它再读几遍。

如果觉得这些材料这个理解不是那么清楚呢，就是李航老师的那本书啊，你拿出来之后再再再再仔细的，其实我们这本其实我们所有的材料内容，基本上都是属于这本书里面的材料啊，只是我们把它做了一些经验。

因为我们的时间有限，所以结合这本书里面的这些样例，可能会对你的理解有所帮助啊，当然还是那样有问题啊，你是因为你，你需要把它内化成你自己的这些东西啊，就是你需要把它不光是自己能够理解你。

还是你还需要试着能够说服别人，让他们也理解你的这些内容，我觉得这样就是理解了好吧。

![](img/e6c2e591e0d1e49399291fe126313546_1.png)

OK那就这样好吧。

![](img/e6c2e591e0d1e49399291fe126313546_3.png)