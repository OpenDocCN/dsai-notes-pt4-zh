# 【七月在线】机器学习就业训练营16期 - P9：在线直播：9-HMM、CRF模型要点_ev - IT自学网100 - BV1Z9T5ewEKL

那么今天是咱们这个阶段的最后一次课程内容，按照这个课表上的安排，我们今天主要介绍的是概率图模型当中的，一马可夫模型，条件随机场模型呢这个呃我们不做过多的介绍，后续会把一些材料发给大家。

我们重点还是介绍这个一马可夫模型，那么在介绍一马可夫模型当中啊，通过我们这个脉络图可以看到它的前序工作呢，可能会体现在朴素贝叶斯模型当中，这在我们上一次介绍朴素贝叶斯的时候，也说明了。

主要是集中在其中的一些逻辑，计算的这个规则当中啊，概率逻辑计算的一些规则当中啊，这是一点，另外一点呢就是在EMC模型的计算里面，需要使用到em算法，就是期望最大算法。

所以呢今天会把这个em算法做一个补充，那这样的话呢，我们就可以通过朴素贝叶斯当中所熟悉的啊，概率计算以及E算法当中所提供的优化过程，来得到hmm1mark副模型里面的，一些重要性的结论，好吧。

这是我们今天的主要内容，如果没有问题的话，我们就开始我们的IMAC模型，通过前面的我们的介绍啊，大家也能够体会得到in Mark模型啊，需要进行大量的概率计算啊，它也是属于一类所谓的那个概率图模型当中。

非常重要的组成，那么IMAX模型的主要的工作啊，先说明一下，就是这个模型它能解决一类什么样的问题，它主要解决的是一类，称之为是序列标注的任务啊，或者叫做序列到序列的任务，我们把它称之为是不用这个打开啊。

我们把它称之为是这个从序列呃，到序列的任务啊，特别是一类在自然语言处理当中，非常重要的一类任务，再举一个例子啊，比如说啊我们有一个自然语句啊，比如说在这个在这里啊，我们就以以马可夫模型啊。

这么一个算是一个句子为例，当我们有了这么一个语料以后，我们就得到了一个sequence啊，一个序列，然后呢，我们希望得到当前这个序列，向另外一个序列的一个映射，比如说啊我们要在这个语料库的基础上。

要进行一个磁性的标注啊，什么叫磁性的标注啊，就是我需要看一下当前这个序列里面，每一个单词的词性是什么，因为我们知道词性有名词，动词，形容词，副词，介词等等各种不同的词性是吧。

那在LP里面有一类任务就是需要考察一下，当我们拿到一个语料的时候，每一个单词的词性到底是什么啊，这是一类啊标准的序列到序列的标注任务，那比如说举一个例子，比如一马尔可夫模型，比如说我们现在已经拿到了。

当前这个语调是由两个单词所组成的，第一个单词那就是银马可夫，另外一个单词呢是模型，那么当前有两个单词所组成的这个呃序列，那么我们希望得到的是它们所对应的，词性的一个序列，当然我们知道你比如说一马可夫。

我们认为它也是一个名词是吧，然后呢模型呢当然也是一个名词，那这样的话你会发现，尼玛跑夫模型这个序列所对应的另外一个序列，就是名词名词序列，当然这只是举一个例子，那么在这类任务当中，大家会发现。

这两个序列有一个明显的对应关系啊，序列和序列之间有明显的对应关系，序列当中各个单词之间也有明显的对应关系，因为我们知道啊，我们学一些语法规则啊，会告诉我们，比如说名词和名词是可以并列出现的是吧。

形容词和形容词也是可以并列出现的，动词和名词也是可以出现的，但是一般很少我们出现，动词后面加形容词的情况，当然这是这些信息啊，完全我们是可以通过我们拿到的语料库啊，比如说你拿到了NN多篇文章是吧。

这N多篇文章当中，我们通过一些统计和概率计算，可以把这些哈序列到序列的这种规则，以及磁性到磁性的规则，都可以通过模型进行学习出来，而这类模型啊，我们都把它称之为是所谓的概率图模型啊。

其中啊以马可夫模型就是这类模型当中的一种，好吧，所以说啊这个解决什么问题，我们先首先有一个认识啊，它能够解决的啊，就是序列到序列的一个映射任务啊，那么多说一句哈。

比如这个sequence to sequence任务啊，嗯除了以马可夫模型，cf模型这种概率图模型可以很好的解决之外，嗯现在还越来越多的，我们使用到的是深度学习模型里面。

你比如说像这个循环神经网络这一大类啊，一大类模型来进行解决啊，这是一个方向，另外呢现在在呃这个深度学习领域，有所谓的注意力机制引入以后的，transformer1类的模型啊。

这类模型已经非常优秀的解决了这类sequence，to sequence问题啊，这就说明一下，那么但是呢在机器学习领域，解决这类问题的主要模型，还是像比如说像野马可夫模型和CR模型啊。

这一类概率图模型为主要的代表，当然呢呃其实随着学习的深入啊，特别后面大家掌握了这个深度学习模型以后，大家会发现当处理一些任务的时候，深度学习模型和呃这种概率图模型的结合。

也在某些问题上取得了非常不错的成绩啊，就像刚才我们讲到的啊，比如说像那个注意力机制啊，各种各样的transformer加上cf模型啊，或者说加上hmm模型，就在一些具体的任务当中取得了很好的成绩。

当然那是后话了，那么回到我们这个概率图模型当中来，在今后的复杂的模型当中，其中以马可夫模型，或者说CRF模型，很有可能会作为更复杂模型的一部分出现啊，这个需要说明一下。

不是说哈这个概率图模型就没有用武之地了啊，有了深深度学习模型以后，不是这样的啊，深度模型，深度学习模型和像概率图模型之间的一种融合，在特定任务上，还是当前的一些范式啊，就是标准的任务解决方案啊。

这个多说一点，那么回到IMAX模型，我们看一下刚才已经介绍了1max模型，能够解决这一系列的任务，那么今天呢我们需要聚焦一下，我们今天的这个模型的介绍的主要内容，包括这几个方面，第一个方面就是模型定义。

到底什么是一个一码和模型嗯，这个非常重要啊，这个包括后面的cf，很多的时候后面的问题的解决，其实归根到核心矛盾上，就是你对整个问题的定义是不是能够理解，换句话说你对这个模型的这个定义认识的程度。

直接影响到了你对后面问题的解决的程度，好吧，所以第一个任务就是到底HM，hmm的定义是什么啊，这是非常重要的内容，那么在此基础上，我们会解决一马可夫模型所面临的三个问题。

第一个问题呢我们把它称之为概率计算啊，第二个呢是所谓的学习算法，第三个是预测算法，后面我们逐一啊作为一个介绍，那好了，下面我们先开始第一个问题，就是关于模型定义的问题，一马可夫模型的定义呢。

首先介绍两个集合啊，首先介绍两个集合，第一个集合呢我们把它称之为状态集合啊，状态集合，另外一个集合呢我们把它称之为是观测集合，这两个集合其实啊就像刚才我们所介绍的，因为我们要介绍两个sequence。

就两个序列，那么这两个序列有哪些符号组成，或者有哪些状态组成，我们必须要先给出一个说明，那么这两个集合分别对应了刚才那两个序列啊，呃的取值，你这样可以认为啊，第一个序列呢我们把它称之为状态集合啊。

状态集合由状态集合中的元素组成，其中的一个序列由观测集合中的元素组成，另外一个序列啊，就是其中的一个序列，你就在状态集合里面找元素，另外一个序列，你就在这个观测集合里面找元素就可以了。

那么具体定义形式呢，我们把状态集合定义为Q集合啊，Q集合Q集合里面包含若干个状态啊，若干个状态分别是Q1Q2，一直到QN啊，N个状态，那么观测集合呢我们用V来表示，分别为V1V2，一直到VM啊。

M中观测啊，M种观测，注意再强调一下，这两个都称之为是集合，集合里面的元素是没有顺序的啊，集合里面的元素是没有顺序的，它仅作为后续两个序列的组成啊，但是具体谁先谁后，在这两个集合里面并没有定义好吧。

它只是作为一个数据集出现啊，你从这里边找元素就可以了，那么下面呢，我们再介绍所谓的状态序列和所谓的观测序列，注意这是两个所谓的序列，就是对应了刚才所说的两个sequence，其中的一个sequence。

我们把它称之为是状态序列啊，状态序列状态序列呢我们用I来表示啊，用I来表示，其中这个序列定义为I1I2，I小T1直到I大TI大T那么再强调一下，因为它是序列序列是有顺序的，换句话说。

序列I当中的各个状态，I1I2取决于状态集合当中的各个元素，但是更重要的是，状态序列当中的这些状态具有前后关系，换句话说，它的下标表明了当前这个状态，在状态序列当中的位置信息。

I1就是出现在第一个位置上，I2是第二个位置上依次类推，而每一个状态序列当中的状态，取决于状态集合当中的某一个元素啊，我相信这个啊能够能够严格的区分开好吧，那么另外一个序列呢，我们把它称之为观测序列啊。

观测序列，观测序列我们用O来表示啊，用O来表示O序列内容分别为O1O2，一直到O小TO大TO大T，同样观测序列也是个序列，序列当中的每一个，代表的是一个具体的观测的位置，O一O2OT一直到O大T。

其中的每一个观测都取决于我们的观测集合啊，取决于观测集合啊，这是两个集合和两个序列的定义哈，两个集合两个序列的定义，那么有了这两个集合和两个，这个观测的定义以后呢，下面我们需要把这些信息啊。

通过一种图示化的方式把它表示出来啊，表示出来怎么表示呢，就是你画个图，这是非常重要的一种，这个算是一种技巧吧，就是后面哈我们所碰到的模型会越来越复杂啊，这个你把它这个用图示的形式。

把这些信息能够充分地表示出来，非常有利于你对这些问题的一个理解和解决，好吧，那么我们看一下，现在我们有手头上有两个序列，第一个序列就是I序列，其中的序列由I1I2点点一直到小T点点，一直到I大T组成。

这个时候呢我们一般，圆圈上表示这个这个序列，重点是让序列是有前后顺序的，所以呢我们用这种带箭头的序列来表示，我们的这个状态序列，同样我们还有一个所谓的公序列观测序列，O1OR点点一个O小T。

然后是O大T，这个时候呢很很很自然啊，同学们就会有一个想法啊，上面这个状态序列是这样沿着这个箭头组成，那你是不是观测序列也是这样的，这个地方就需要注意的，以马可夫模型有这么一个定义哈。

有如下这么一个规定，状态序列当中的每一个状态啊，I1I二一直都I打T啊，每一个状态直接决定它所对应的观测啊，也就是说啊观测虽然也是一个序列啊，观测虽然也是个序列，但是决定观测序列的。

并不是由序列本身来决定的，而是由它所对应的状态来做决定的，回想一下刚才那个词性标注的那个任务，我们知道以马可夫模型，这两个单词所组成的状态序列对吧，然后呢我们的名词名词也组成了一个序列，那么很显然。

我们可以认为是单词决定了我们的单词的性质，是吧，所以这个时候我们一般情况下是这样的来画哈，能明白什么意思吧，所以说啊这个图示啊，必须要把它的含义呢够明晰的表示清楚才可以，当我们有了这个图以后啊。

后面所有的计算啊，大家都可以认为是在这个图示当中来完成的啊，就是在这个图上来计算就可以了啊，不要跑到别的地方去了，当然这个图示其实还是信息很丰富的，我们看看还有哪些信息需要注意，第一个信息哈。

就是你会发现在状态序列当中，I1这个状态跳转到I2，这个状态依次向后跳转，那么跳转从I1跳转到I2，从I2跳转到I3，那么这个跳转的概率是多少，这个图里面还没有表示啊，这是第一点，那么第二点。

从IE这个状态能够跳转到它所对应的观测，由每一个状态都可以跳转到它所对应的观测，那么从状态向观测的跳转的概率，也现在还不清楚啊，还不知道，但事实上大家可以看到，一旦我能够获取到从状态到状态。

或者是状态到观测的这个概率值以后，我当前这个in马可夫模型，其实就可以进行一个生成过程了，怎么生成，首先我从状态I1开始先生成它所对应的观测，然后再从I1生成它所对应的下一个状态I2。

再由I2生成它所对应的观测go2依次向下，最后直到IT这个状态生成，它所对应的什么观测，gt整个的一马可夫模型其实就可以结束生成了，也就是说啊它是这样形成的，从状态到观测，从状态到状态，再从状态到观测。

再从状态到状态，然后依次向下生成最后的一门考试模型，那么在这个过程当中非常重要的，刚才介绍的两点，就是状态到状态之间的概率是多少，你需要把它表示出来，然后呢从状态到所对应的观测的概率是多少。

也必须要明确的表示出来，那么下面我们就需要给出两个非常重要的信息，第一个信息呢我们把它称之为是状态转移矩阵，状态转移矩阵首先明确一下它是一个矩阵，当前这个状态转移矩阵当中，记录了任意两个时刻之间的状态。

转移的规则是多少啊，状态转移的规则是什么，那么这个时候呢就有个问题，那么为什么我们使用一个矩阵结构来描述，任意两个状态之间的转移规则，如果是个矩阵结构的话，那这个矩阵它的形状是什么样的啊，第三个问题。

当我们确定了当前这个矩阵的形状以后，每个矩阵当中的每一个元素的值又是多少啊，这是一系列的问题，那么回到刚才关于状态转移矩阵的这个概念，或者含义，状态转移矩阵就像刚才我们所说的。

他要记录的是任意两个状态之间的转移关系，而我们清楚地知道，状态的可能的取值无非就是Q1，一直到QN的状态，也就是说这里的I1I二一直到I大T，我是大T个时刻，每一个时刻都可以取。

直到我们状态集合当中的某一个状态，而我们这个状态转移矩阵又是用来记录，任意两个状态之间的转移情况的话，那么很显然我们可以用矩阵这种结构啊，来加以表示，那么下面这个问题就是这个矩阵的结构。

或者说当前这个矩阵，它是一个几行几列的一个矩阵，我相信大家是通过一个分析啊，或者简单的思考是可以得到结论的，再重复一下问题啊，就是我们现在有N个状态，好状态，有N种可能的取值，那么这N种可能的取值呢。

他们之间的任意两个之间的跳转关系，需要通过一个所谓的状态转移矩阵，来进行一个描述，那这个时候这个状态转移矩阵啊，这个矩阵几行几列，一共有N种状态啊，任意两个之间可以进行跳转。

那这个矩阵应该是一个N乘N的矩阵对吧，N乘N的矩阵，这一点再加以说明，就是后面不管是呃这个什么样的一个呃模型啊，当他使用所谓的向量，使用所谓的呃矩阵，甚至后面我们在使用所谓的张量。

来表示这个数据结构的时候，那么首先你需要确定的一点是，当前这个数据结构的形状啊，如果是个矩阵，你需要明确的给出当前矩阵是个几行几列的，当然这个不能靠蒙啊，不能靠猜测，一定要根据当前这个数据结构所需要。

处理的任务来决定，因为当前的状态转移矩阵是用来记录，任意两个状态之间的转移规则的，而我们一共有大N种状态，所以我们用N乘N的矩阵就可以存储了，那么有了这个矩阵以后，我们更希望得到的是每一个元素的值啊。

就说任意两个状态之间的转移规则，到底是怎么来加以描述的，那么很显然，其中就是关于AIG的一个定义对吧，AIG的定义，看一下AIP的含义哈，AIJ的含义，它说明的是在T时刻。

或者说在某一个时刻处于状态QI的条件下，在T加一时刻转移到状态QG的概率，比如说现在我们看一下这里，这是T时刻，那么很显然，下一个时刻一定是I等于T加一时刻是吧，那我们知道从T是从it是可以跳转到T。

加一的啊，从IT是可以跳转到IT加一的，换句话说啊，我是可以从一个时刻跳转到他的下一个时刻的，那这个时候就是所谓的状态之间的一个转移，或者跳转，那这个信息就要被记录到我们的状态，转移矩阵里面。

而状态转移矩阵啊，它其中的每一个取值都说明了是在T时刻，或者在某一个时刻处于状态，QYT等于QY，在下一个时刻就是T加一时刻，状态等于QG状态T加一等于QJ概率，那么很显然这是一个什么概率。

这是一个条件概率啊，在上一个时刻在提时刻等于QY的条件下，在T加一时刻等于状态QJ的概率，所以看一下哈，你会发现AIJ其实是一个概率值，而是一个什么，而是一个条件概率，他说的是IT等于QI的条件下。

IT加一等于QJ的概率，从QI跳转到QG是AIJ，这是关于状态转移矩阵的一个定义啊，状态转移矩阵的定义，当这里的I从一到N进行取值的时候，这里这样也是从从一到N啊，I从一到N，然后呢这也从一到N。

那么大家可以看到，当两个循环结束以后，这里的AIJ就把所有的状态，而这些信息就都记录到了，我们这个状态转移矩阵A里面去了，关于这一部分，看看有什么问题吧，那么大家看看啊，我再举一个例子啊。

比如说有同学会对这个T啊有困惑，就是这个T你刚才说的是在IT这个时刻，是不是在T这个时刻等于QI，在T加一这个时刻等于QG的这个状态转移，其他时刻呢注意啊，这里的T啊，这里的T代表的不是一个具体的时刻。

而是我们在状态序列当中的任意一个时刻，比如说T是不是可以等于1T如果等于一，就变成了ZI1时刻等于QI，那么在I11，就是I2时刻等于QG的状态转移的概率值，而这个同样我们是通过你看啊。

它和具体的时刻其实并没有关系，你看看这个表达式里面和具体的时刻，T其实并没有关系，这里的T的强调的仅仅是前后关系，就是如果我在前一个时刻是QI，下一个是课时QG，那么这个跳转关系就用AIG来表示。

至于这里的T是12345678，一直到大T的哪一个时刻，其实并没有限制，能明白什么意思吧，就是这里的T和T加一仅仅说明的是前后关系，而不是一个具体的位置，那么这种前后关系啊，你想象一下这种前后关系。

这里有个一二是前后关系，二三也是前后关系，TT加一是个前后关系，到大T减1T是不是也是个前后关系啊，它描述的就是这种前后关系的这种跳转概率啊，用这个AIG来表示好吧，这是状态转移矩阵的一个介绍。

那么如果有状态状态之间是可以转移的话，那么很显然，状态和观测之间也是可以进行跳转或者转移的，那这个信息我们把它称之为是观测概率矩阵啊，关于观测的概率矩阵，那么通过刚才那个状态转移矩阵，A的一个定义啊。

我们应该能够啊，能够想象得到这里的观测概率矩阵，很显然他也是个矩阵是吧，同样回答以上刚才两个问题，第一个问题这个矩阵的形状是长什么样，第二个这个矩阵里面的取值是怎么定义的，那么再重再重复一遍。

观测概率矩阵说的是由状态到观测的跳转关系，他需要记录的是任意一个状态，跳转到任意一个观测上啊，这个概率信息是多少，那么这个时候大家想一下，状态呢是有N种状态可能取值，观测呢是有M个观测可以取值。

那这样的话从N种状态到M种观测，就需要一个N行M列的矩阵来加A表示啊，所以说啊，这个地方观测概率矩阵是一个N乘M的矩阵啊，N乘M的矩阵，这是第一点，第二点就是说的是由状态向观测进行跳转。

而我们选显而易见的，可以看到状态和观测之间的时序关系，是一一对应的时序关系啊，就是我在第一个时刻的状态，跳转到第一个时刻的观测，在第二个时刻的状态跳转到第二个时刻的观测。

同样在DT时刻的状态跳转到DT时刻的观测，所以这里的每一个元素定义的是，在T时刻处于状态QI的条件下生成，同样是在T时刻观测为VK的概率，所以啊你看一下它的定义形式是，在T时刻状态是QG的条件下。

同样是在T时刻观测，是VK的条件概率被定义为BJK，那么同样这里的T啊，这里的T也不是一个确切的时刻，他呢也是在我们整个的序列上可以任意取值啊，如果T是第一个时刻。

那这里说的就是在第一个时刻等于Q2QG，同样在第一个时刻等于VK的条件概率，等于BKBJK，当然这个T可以从序列上任意一个时刻上取值，那么K呢代表的是我们的观测，所以从一到M这对应的是我们的状态。

所以是从一到大N好吧，那这样的话我们可以看到啊，上面从状态到状态，我们是用AIJ来表示，由状态到观测，我们是用BJK来表示，那么这个时候我们就可以按照，刚才我们所说的那个生成策略。

来生成当前的状态序列和观测序列了是吧，UI1生成勾一，那么这个是根据我们刚才所定义的集合B啊，这个观测概率矩阵来生成的，我就找一下在当前为I1的条件之下，生成O1的那个最大的概率值是多少，是谁。

我就生成那个概率最大的对应的OE同样UI1，我再根据集合B啊，查一下生成I2那个概率最大的啊，那个I2状态到底是谁，不就生成那个状态，同样依次向下生成，当前这个一马可夫模型里面。

所有的状态以及观测也就都生成了，嗯好像是这样是吧，但是呢又好像不是哪有问题呢，换句话说这一部分还有什么问题吗，看看，状态集合和观测集合里存的都是什么数据啊，这个需要根据我们这个具体的问题来进行。

具体的这个定义，比如说刚才我们提到过那个磁性啊，刚才我们提到过那个词性标注的那个任务是吧，如果我们认为语料是我们的状态集合的话啊，如果我们认为我们的语料是状态集合的话，那么状态集合里面的每一种状态。

就是我们语料库里面的每一个单词，如果我们认为这里的磁性啊，是作为我们的我们的观测集合的话，那么这个时候的观测集合里面的每一个元素，就是我们每一种可能取值的词性动词，名词形容词啊。

这是你需要根据特定的任务来决定的，那我们继续按照刚才所说的，好像没有问题了，但是有一个很麻烦的是关于那个I1，就是第一个时刻的那个状态啊，啊我们说是从I1生成它所对应的O1。

UI1生成它所对应的那个I2是吧，UI2再生成O2等等等等，但是他这个IE由谁来生成，就像我们这个比较熟悉的那个多米诺骨牌，一样是吧，我们知道后面这一长，这后面这些串生成啊。

都是类似于我们那个多米诺骨牌，你只要退到了第一个后面，就以此生成就可以了，但问题是这第一个骨牌由谁来生成，并没有在模型里面加以介绍是吧，那么下面我们就需要处理这个问题，那么我们再回想一下。

既然是在第一个时刻啊，第一个生成第一个时刻的状态，而我们又知道状态的取值可能性，是刚才的那个Q集合，而我们知道在Q集合里面有Q1点点，一直到QN种状态可供选择，那现在问题是。

我在第一个时刻到底去哪一个状态呢，那这个时候我们很显然，每一种状态可能都有可能出现，所以下面我们需要设计一种数据结构，用来存储生成第一个时刻的，所有可能的状态的这么一个呃概率值，那这个时候大家想一下。

用的是一个什么样的结构，能够存储以上的信息，以上的信息就是说在第一个时刻能够取值的，所有状态的概率值，而我们知道所有的状态无非就只有N种状态，那这个时候第一个时刻时刻已经被确定了啊。

在这一个时刻有N种可能的取值，那很显然我们用一个什么向量就可以了，所以把它称之为初始概率向量啊，初始概率向量，初始概率向量呢我们用派来表示它的，每一个元素呢，我们用我们用pi加以区别，这里的I哈。

这里的I也是从一点点一直取到N，因为你因为你在第一个时刻，也可以有N种状态可供选择，那么下面那么pi是在时刻T等于一处，处于QI的概率啊，所以说你会发现它的概率计算是I1等于QI的，概率被定义为pi。

I呢是从一到大N，而我们知道这里I1等于Q1，I1等于Q2，N到I1等于QN啊，是由N个概率值，这N个概率值组成了我们的初始概率向量派，初始概率向量排，那么回到刚才这个图当中好了，这个时候生成I1的。

就是根据我们的数值概率向量派来生成，那么有了这个数值概率向量派，以及我们刚才说的状态转移矩阵A，和我们的观测概率矩阵B，那么当前的一马可夫模型就可以说是完成了，我们形式化的给出姨妈和规模型的定义。

所谓的Emo模型啊是个三元组啊，就是三部分组成的哪三部分呢啊，首先他用LAMBA加A表示，用lambda加A表示，lambda就是我们的1M模型，它是由刚才所介绍的状态转移矩阵A。

观测概率矩阵B以及初始概率向量派啊，这个地方是个逗，用这个逗号三部分来组成的，我们的一马可夫模型啊，这就是以上关于马可夫模型的定义部分，那么在此基础上呢，我们还需要做两个非常重要的假设了，重要的假设。

这两个假设哈主要是为了进行这个化简来用的，怎么样化简呢，因为我们可以看到根据ab派啊这三个部分，我们的IMAC夫模型其实是蛮复杂的，蛮复杂的一个数据结构，在后面我们进行这个问题求解的过程当中呢。

这个模型太复杂啊，就像我们在那个什么，在那个朴素贝叶斯模型里面对吧，碰到了那个条件独立性假设其实是一样的，我们要为了简化问题哈，这个带来一些这个规则，这个规则呢我们看两个非常重要的规则。

第一个一马可夫模型的两个基本假设啊，第一个假设呢，我们把它称之为七次马可夫性假设啊，其次马尔科夫性假设他是这么说的，在任意时刻T啊，在任意一个时刻，T的状态只依赖于时刻T减一的状态。

首先其次马可复性假设，说的是状态和状态之间的一个假设啊，状态和状态之间的假设回到我们前面那个图哈，状态到状态，这是I1I2点点，这是I大T，他说的是状态之间的一种假设关系啊，和你那个观测哈，暂时还。

暂时还没关系啊，人家说的是状态和状态之间，人家说什么，人家说的是在任意时刻，在任意时刻T的状态依赖，并且只依赖于前一个T减一时刻的状态，也就是说某一个状态啊，某一个状态和其他的状态没有关系。

和其他的观测也没有关系，它只和它的前序状态相关啊，这是做了一个非常重要的假设，就在这个地方，就说你可以认为哈，状态和状态之间都是近视的啊，都是近视眼啊，虽然说我们知道每一个都有倾向依赖关系。

但是呢从单一个状态而言，他只能看到他的前序状态是谁，换一个角度，就是当前这个状态只依赖于它的前序状态，和它的后续状态，以及它所对应的所有的观测，和其他状态都没有任何关系啊，这是这么一个假设。

那么在形式上我们这样来定义，怎么定义呢，就是说当我们已知了IT减1OT减一一，直到I1OE以后，换句话说你可以看到我即使是既知道状态，也知道观测，而且不只是知道一个状态，一个观测，知道很多的状态。

很多的观测的条件之下，来决定当前T时刻的IT，刚才我们前面已经说过，你这些状态和观测啊，有些根本不会影响到T，那么哪些会影响到T呢，只和我前序状态T减一有关啊，这是所谓的七次方。

可以假设那这个东西有什么用啊，也就是说到后面，我们进行逻辑这个概率计算的时候啊，当我们进行概率计算的时候，你碰到这么一个式子很麻烦，很复杂的一个概率条件，概率是吧啊，又知道这么一些观这个状态。

又知道这么一些观测啊的条件之下，来求T的条件概率的时候，马上就可以把它化简为化简为，只有当前时刻的前序时刻作为条件的时候，it的条件概率就OK了，那其他的都没关系啊，图上说明了。

刚才已经说明了某一个时刻的状态，只和它的前序状态有关啊，只有他俩有关系，和其他时刻的状态状态观测观测即使你已知了，也不影响我当前这个时刻的状态啊，状态之间就这么个近视的关系啊，这是说的，其次马合并假设。

第二个假设能称之为观测独立性假设啊，观测独立性假设他说的是什么，他说的是任意时刻的观测啊，观测和同一时刻的状态的关系啊，如果说哈状态，刚才我们的七次Mark复性假设，说的是状态和状态之间的关系。

那么现在说的这个观测独立性假设说的是谁啊，说的是状态和观测之间的关系啊，他俩之间的关系什么关系呢，任意时刻的观测啊，任意时刻的观测O只依赖于同时刻，当任意时刻T的观测只依赖于时刻T的状态。

我任意时刻的状和观测只依赖于同时刻的状态，和其他时刻的状态，和其他时刻的观测也没有关系啊，这是你看你会发现这两条假设啊，其实很强的两条规则啊，很强的两条规则，那么在形式上我们可以看到啊。

就像刚才我们所说的，即使我知道了大量的状态信息和观测信息，作为条件之下来决定我当前的观测的时候啊，这个条件概率也直接等于什么，直接就等于同时刻的状态啊，同时刻的状态IT作为条件之下。

那么gt作为他的条件，概率和其他时刻的状态，和其他时刻的观测也没有关系啊，也没有关系，同样怎么使用，就像刚才所说的，当你后面再推导一一个概率计算，而后面的条件会一大堆一大堆条件之下。

观测的条件概率的时候，马上就可以用这条规则啊，把它化简为只和我同一个时刻的，同一个时刻的状态相关就可以了，图上就是这么说的啊，状态和状态之间只依赖于前驱状态，状态和观测之间只依赖于同一个时刻的状态。

好吧，这是关于两个假设啊，关于假两个假设用的时候啊，这个一会儿用的时候，我们回过头来看一眼就可以了，有了这两个假设以后呢，我看下面一个问题啊，就是观测序列的生成算法啊，就有了一马可夫模型ab派以后。

我可以根据ab派这三个这个结构来生成，我希望得到的观测序列啊，希望得到的观测序列，那么这个算法的输入就是输入我们的模型啊，输入我们的模型lambda ab派，然后呢你还需要给出一个观测序列的长度啊。

你要生成多少个观测啊，你要生成多少个观测T也是需要的，输出什么输出我们的观测序列O啊，输出我们的观测序列O就可以了，那么其实也就是说我们现在有了一个啊，任意两个状态之间的跳转关系啊，通过A来表示。

然后呢任意一个时刻向观测的调整关系，我们用B来表示，而且呢在第一个时刻上，哎那个第一个时刻的跳转生成，我们用派来进行定义，那这个时候你会发现当我们通过派先生成谁啊，I 1ui1。

根据B就可以生成OE再UI1生成IRUI2，生成ORUIR生成往下生成，然后呢生成继续所对应的O，那这个时候你会发现依次向下生成，当生成到最后一个O大T的时候，我们所希望的观测序列是不是就已经有了。

所以说啊就像刚才我们所说的，通过派生成一个通过O通过B生成O，通过A生成I啊，以此生成就可以了，所以算法本身其实并不复杂，我们可以看一下，第一步，由初始概率向量派来产生状态I1，LTRU派生成状态I1。

这个时候就会有同样疑问，这个I1你说生成这个IE怎么生成啊，那这个时候我们是不是可以找一下，使得当前对吧，我们的状态转移这个初中，我们这个初始概率向量里面定义的是N种状态，在第一个时刻生成的概率值。

那我到底在第一个生成，第一个时刻生成哪一个状态啊，我们第一个策略非常简单，就是找那个在我们的初始概率向量里面，概率最大的那个作为我当前IE的状态，是不是就可以啊，就那个概率值谁的概率最大。

我把它生成作为第一个状态是不是就可以啊，好了啊，这样解决的就是第一个时刻的状态，然后呢第二步梯等于一，第三步由状态IT，当然啊，这里T开始循环了吗，由状态IT的观测概率分布，同样这个OT等于多少OT。

同样是使得那个BGK取得最大值的那个概率，所对应的那个观测啊，然后呢第四步，这样的话第一个时刻就有了状态和观测，然后呢下面由状态IT的状态转移概率分布，那个A矩阵来生成下一个时刻的状态T加一。

那这个时候再判断一下啊，T等于T加一，再判断一下T是不是小于T，因为我们知道在这个循环过程当中，依次生成观测，依次生成状态啊，这个in Mark和模型依次向就向后生成，当这个条件不满足的时候。

意味着T等于T我当前的观测已经生成完毕了，否则就可以啊，这个时候就可以结束到整个生成算法啊，呃这个算法本身啊其实作用不大啊，用处也不大啊，它主要是为了说明啊，或者说让大家能够理解啊。

当前这个以马克服模型的定义问题，而加以说明的好吧，以上呢是我们今天的第一个，关于一马可夫模型的定义问题，再回过头来看一下，我们分别定义了状态和观测两个集合啊，这是两个集合。

你的状态和观测的取值是在两个集合里面，取值的，下面呢重要的是两个序列，分别是状态和观测序列啊，是两个有前后关系的这种序列，但是呢这个时候我明确一下，状态序列是由状态和状态之间的关系决定的。

观测序列是由他同一个时刻的状态所决定的，状态是由状态生成，而观测是由它所对应的状态生成的啊，观测也是由状态生成的啊，所以这个结构这个图你回去好好再琢磨琢磨，然后呢三个信息状态转移矩阵A。

观测概率矩阵B和初始概率向量派啊，分别定义了状态和状态之间的跳转，以及状态和观测之间的跳转，以及第一个时刻状态的生成规则啊，或者说生成概率啊，这三部分的信息，由这三部分信息组成了整个一马可夫模型。

当然模型很复杂，所以我们加上两条假设，其次马克假设和光观测独立性假设，生成算法回去以后自己看一遍，看看这部分还有什么问题吗，如果没问题，我们就继续啊，继续呃，当模型介绍完了以后。

那么我们需要看一下这个模型它能干什么是吧，他能干啥，一般情况下呢，一马可夫模型可以解决以下三类任务，或者三个工作，哪三类任务呢，第一类任务称之为概率计算啊，概率计算，那么概率计算说的是什么呢。

就是一系列的概率计算，那不废话嘛，但是呢这一系列的概率计算的目的啊，这一系列的概率计算的目的是，为后续两个任务做前期的一些中间结果的计算，换句话说，后续两个任务都比较复杂啊，都比较复杂。

会使用到大量的一些概率计算的中间结果，而这些中间的结果就在第一个任务里面，把它解决或者完成了，当你用的时候，在第一个任务里面去找就可以了啊，第一个任务需要处理大量的概率计算任务好吧。

那么其中呢有有有一些比较典型的哈，这里我们只能介绍一些比较典型的一些计算，我们以这个第一个任务为例哈，第一个任务说的是什么，说的是当我们已知拉姆达啊，已知我们的模型以后。

并且呢我们还知道了我们的观测序列啊，还知道了观测序列能够计算出在当前模型，当前音马可夫模型条件之下，出现这个观测序列的概率值是多少啊，这个东西挺有意思啊，就当我们知道的是lambda，还是知道呢。

我们一个观测序列，这个时候我们要得到什么呢，我们要的是在当前模型条件之下出现这个序列，观测序列的概率值啊，就像刚才我们所说的对吧，你状态决定状态，状态决定观测现在状态之间的转移概率。

A状态观测矩阵B和初始向量派我都知道了啊，这里的模型是已知的吧，然后呢还知道什么呢，还知道一个观测序列，这个时候我能够计算的就是，在已知模型的条件之下，出现这个序列的概率值是多少，我能把它计算出来啊。

这是第一类任务，概率计算任务当中的一之一之一，类似这种任务啊，类似这种任务还有很多啊，这个地方呢我们以它作为一个例子作为介绍啊，呃其实非常有代表性啊，其实就是一系列的概率计算，有了这一个力作为啊。

这个准备其他的任务呢，相应的也可以作为一个计算结果，这是第一利润，第二个任务啊，称之为学习任务，学习任务说的是什么，学习任务说的是我只知道一个啊，我只知道一个观测序列啊，我只知道一个观测序列。

但是我仅通过这个观测序列，就可以把当前使得这个观测序列出现，概率最大的那个模型拉姆达构建出来，而我们不要忘了这个拉姆达包括三部分，初始概率向量派，状态转移矩阵A和观测概率矩阵B。

第二个任务啊可以说是非常重要，你可以认为是一种无中生有的任务对吧，我第二个任务，这是第一个任务，第二个任务说的是我只知道一个O，其他的我一概不知道，只知道一个观测序列，这个观测序列怎么生成的。

我也不知道谁生成的，我也不知道，因为序列I我也不知道，但是仅此情况之下，我就可以把那个，出现当前这个观测序列概率最大的那个模型，找出来，第二个任务啊，非常厉害了是吧，非常厉害了，而这个学习过程当中啊。

我们需要使用到演算法，所以一会儿呢我们会呃，单独把这个em算法再做一个介绍，这是第二个任务，第三个任务呢称之为预测任务或者编码任务，他说的是什么，他说的是同样我知道了模型lambda，还知道了。

观测序列O和我们的第一个任务，的已知条件是一样的，但是呢他求的目标不一样，他求什么，他求的是在当前已知la和序列O的条件之下，我能够把出现概率最大的那个状态序列，I构建出来，他要求的是那个I序列。

I序列是刚才说过，他是在已知la和O的条件之下，出现这个序列的，那个概率值最大的序列能够构建出来啊，这是三类三个任务啊，三个任务，这三个任务呢我们又一次的做一个解决好吧，先看第一个任务。

第一个任务说的是已知拉姆达啊，已知模型和O，我们要把在当前模型下面，出现O的那个概率值计算出来，就是个概率计算呗是吧，概率计算，那么一一提到概率计算，这个时候马上就有同学应该反映出来反映什么。

既然是概率计算，而且是挺复杂的，概率计算，我们手头上的一些工具都有哪些，简单的回顾一下，我们上次讲到的那个那个那个朴素贝叶斯模型，两条规则，第一条规则是什么，加法规则，加法规则说的是边缘概率等于P。

对吧，我们的边缘概率和联合概率之间的这个，概率关系啊，概率关系，当我们啊不是不是在X上，一定是在Y上，因为我们这个地方多了一个随机变量，Y是随机变量，Y需要进行一个sum上的一个求和。

这样的话就还原回我们的表演概率了啊，这是加法规则，那么另外一套规则呢，就是说的是PXY和联合概率等于什么，等于，等于条件概率和概率的乘积啊，联合概率嘛，两个随机变量本来是两个随机变量的嗯，不确定性。

这个时候呢，我们在一个随机变量被作为条件确定以后，另外一个随机变量的条件概率，那么这个时候很显然他俩不相等了，那么这个时候需要把那个确定之后的随机变量，它的这种不确定性通过PY再还原回来。

那么这样的话就构成了所谓的成绩规则啊，后面啊下面就是这两条规则，反复的用啊，反复的用，你想不明白了，你就回来看一眼，找找他俩就OK了，好吧好，回过头来看一下，有了这两个工具以后，我们解决第一个任务。

已知拉姆达和O要求这个在拉姆达条件之下，O的概率值，这个任务呢我们把它称之为是，这个就是概率任务其中的一种啊，或者一个，那这个任务怎么去解决啊，这个任务解决呢需要引入另外一个概念。

或者另外一个中间结果叫做前项概率啊，前项概率，前项概率呢我们把它定义为啊阿尔法TI啊，阿尔法TI，阿尔法ti，这个前向概率呢被定义为在已知模型条件之下，O 1o2，OT以及IT等于QI的一个联合概率啊。

当然是在模型条件之下的联合概率，你看啊，这个时候你就不知道这个阿尔法TI，到底是个什么东西了是吧，或者说这个联合概率到底长什么样，也不知道了，还是那个问题啊，画图画图画图画图，那么假设这是I1啊。

这是I，你看看啊，I，T啊，就在这啊，这是I大T好吧，然后呢，你看看这个这个这个联合概率里面都有谁，包括O一O2IT，如果是在这个一定对应的是OT，那么前面只是O1O2，看看这个式子里面有O1O2。

一直到OTO1O二，一直到OT，然后呢还有谁，还有这里的IT，所以说啊这个所谓的前向概率说的是这部分啊，这一部分被定义为项式，所谓的前项概率用阿尔法IT来表示，在T时刻的IT和O一一直到OT啊。

组成这么一个所谓的前行概率这么一个东西，有了这个前向概率以后，我们看看这个前向概率它能够蕴含着哪些信息，而这些信息是不是为我们最终的这个PO的计算，能够带来一些帮助，好吧，我们需要分析一下这个线程概率。

那这个前行概率的分析呢，我们需要把它展开，仔细的去看一下这个前行概率，所蕴含的一些内容，那这个前行在这个展开过程当中呢，我们就需要仔细一点好了看看，首先我们可以看一下这一步啊。

阿尔法TI等于刚才所说的拉姆达条件之下，O1到OTIT等于QI，从这一步到这一步是定义啊，这个没什么可说的啊，这一步是定义，那么从这一步到这一步啊，说明一下做了两个地方的修，两个地方的调整。

第一个调整呢是在这，是，因为当前这个阿尔法TI，一定是在已知兰姆达条件之下的，这个呃前向概率啊，每一步都是都带着这个拉姆达条件，所以呢后续的计算，就把这个拉姆达条件隐去了啊，你知道他带着。

但是呢我们就为了简便啊，就不再写每一步都写这个拉姆达了好吧，这是第一步，所以你会发现从这一步到这一步里面，那个拉姆达条件没有了啊，不是没有了啊，是因为我们为了简写就不写它了好吧。

这是第一个地方改了第二个地方，刚才我们说到过啊，其实在阿尔法ti前向概率当中，其实包含了两部分信息，既包含了DT时刻的状态信息，也包含了从一到T时刻所有的观测信息，所以呢你会发现这个IT等于QI。

我们还依次保留对吧，T时刻的状态信息我们还保留着，那么从第一个时刻到T这个时刻，所有的观测信息呢还是太太复杂，我们把它简写简写为勾，从下标一到上标T啊，从下标一到上标T表示了啊。

就是从第一个时刻的状态啊，第一个时刻的观测到第T个时刻的观测啊，我们用这么一种简写的符号来表示，这里的O11类的OT好吧，就是从这一步到这一步，其实就是为就是为了简写方便啊，这个没什么说明一下就可以了。

没什么可以过过多理解的部分，那么下面就需要一步一步的去理解了，那么下面大家看一下，怎么从PIT等于QIO1到OT啊，这么一个联合概率等于等于下面这个式子，或者说从这一步到这一步肯定是相等啊。

这是没问题的，但是你用的是哪条规则，或者你根据的是什么样的一个规则，你从上面这一步可以得到下面这一步，看看我们在形式上，在形式上变成了什么，变成了SUG，从一到N啊，变成了一个求和啊，概率求和，然后呢。

它的求和的概率的这个对象呢就比较复杂了，变成了四项，看了吗，一项两项，三项四项，这四项分别是谁，分别是IT等于QIIT等于QI，IT等于QY啊，他还在这是吧，然后呢，这里有一项是O1到TO1到OT。

那么O1到OT，是不是可以拆成O1到OT减一，然后再到OT，所以说这一项拆成了两项啊，也在啊，其实这没问题，然后来多了一项IT减一等于Q类，你看这里只有两项吗，哎这里有两项，但是多了第三项。

IT等于QG，这第三项是怎么多出来的啊，或者说多出来它是用了哪条规则，大家有什么理解吗，哪条规则，不理解是吧，你想想我们手头上一共就这两条规则，你你你现在也没得选吗，你能选哪一个，你看看你俩号规则。

加法规则和乘积规则，乘积规则不带sum求和，那可很大概率是是是根据这个加法规则，但是加法规则人家只说的是条件，概率和联合概率之间的关系，你这里好像并不符合，刚才那个并不符合这个要求啊。

所以这个时候你需要灵活一点，什么叫灵活一点，这你会发现，比如说这个地方我加上一个随机变量Z，大家想想影响不影响，这个所谓的加分规则是不影响的，也就是说我加上一个，只要在左侧再加上一个随机变量Z。

那么同样在右侧也加上一个随机变量Z，其实这个概率等式还是成立的，那这个时候你再想想，不就变成了两个随机变量和三个随机变量之间，的这种啊这种概率关系了吗，那么同样，现在你是在保证了原有的随机变量的基础上。

增加了一个随机变量，而增加的这个随机变量是IT减一等于QJ，我们只需要保证的是什么，保证的是你新加入的这个，你之前加入的这个随机变量Y，要进行一个sum概率上的求和，你看看这个时候是不是这从一到N。

而我们知道QG吗，QG是我们状态集合当中的一个元素，而这个元素可以在所有的状态集合上任意取值，那有多少种状态，有大N种状态，所以这个地方G从一到N我可以从一到N取值，那不就是这里对应的是随机变量Y。

在所有可能的情况下进行一个概率求和嘛，所以啊从上面这一步到下面这一步，我们用的是加法规则好吧，这个地方确实你需要灵活一点，但是这个灵活呢，我觉得也没有什么太多的技巧性而言是吧，这是这一步哦。

那为什么要这样做啊，这才是个好问题，就是你吃饱了撑的没事吗，为什么本来只有两部分对吧，两部分的联合概率的形式，你为什么把它硬生生的拆成，拆成这个一拆成这种形式很显然是有用的，但继续往下。

那么最后你才会发现我们的目的是什么，好了，那么再从这一步到这一步又是用到了哪条规则，或者说是呃怎么生成的，我们再看看啊，本来现在我们得到的是IT减一等于QJ，IT等于QI，然后呢是O1到OT减一。

这个地方是OT，这是一个四个部分的联合概率，现在呢拆成了两部分，拆成了两个概率相乘，哪两部分呢，我们需要看一下，第一部分变成了IT减一等于QJ，原来我在这，现在跑到这来了。

然后呢这个地方还有个IT等于QY，IT等于QY在这对吧，然后呢这里还有一个O1T减一，O1T减一是在这，然后这里还有一个OT在这，那么也就是说啊，很显然我们原来是四部分的联合概率。

现在呢拆成了一个条件概率的形式是吧，拆成了一个条件概率的形式，作为了前面这一部分，很显然我们知道这个等号，如果仅有前面这两部分的话，是不相等的，那怎么办，看这看这这里还成了一个IT减一等于QJ。

然后O1T减一，那么你看看是不是IT减一等于QJ跑到这来，O1到T减一是跑到这来，那你告诉我下面这一步的得到是使用哪套规则，应该是一目了然了吧，哪条规则，我们只有手上这两个东西，乘积规则，有同学会问。

那乘积规则说的是两个变量的联合概率，是拆成了条件概率和边缘概率的乘积，你这可是有四个，四个又怎么了，四个无非就是你可以把，你可以把他俩认为是一个吗，你可以把他俩认为是一个吗。

那这样的话这四个不就调整一下顺序，就变成了两组是吧，其中一组作为条件，另外一组作为条件概率的，条件概率很显然就变成了条件，概率需要乘以那个内阻所对应的一个边缘概率，这是用的是乘积规则，那同样继续往下呗。

你需要一步一步的去这样去去给大家说啊，在这个规则还只是把结论放在这，让你找找对应关系或者找这个规则是吧，我们上学的时候，这个就完全没有，而你需要自己去找这个推导过程好吧。

让我们看看从这一步到这一步又是怎么来的呢，看看再分析分析啊，很显然这个地方，这两个同学还在啊，在这直接下来了，但是作为条件的这两部分，只有其中的一个下来了，这个条件跑哪去了，你需要给出一个解释和说明。

这是一个另外一个，后面这个式子很显然是变成这个式子了，又为什么，你需要解释一下好吧，两个问题，第一个问题就是这个O1到OT减一去哪了，这是第一个问题，第二个问题，这个式子它等于这个阿尔法T减1G。

又是为什么啊，这两个问题谁能解释一下吗，先解决第一个问题哈，第一个问题是这个O1到OT减一跑哪去了，他不能凭空就消失，是吧，很显然，但是从式子上又告诉我们，他又确实是没有了，那原因是什么。

或者为什么没有看一下啊，再看一下这个式子，我们已经知道了问题的主要矛盾在哪了，就是就是他跑哪去了，但是他跑哪去，一个很重要的原因在于他是谁，或者他起到了一个什么样的作用，很显然。

这里的O1到T减一处在一个什么位置上，处在一个条件的位置上是吧，处在一个条件的位置上，而他作为条件又决定了谁呢，又决定了前面两个随机变量分别是IT和OT，IT和OT，那这个时候就有一个很显然的一个启示。

就是他作为条件如果能够去掉，那也就是说这个条件存在和不存在，不会影响前面这两个随机变量，那我们就需要看一下，这个条件是不是真的，对前面两个随机变量不产生影响的吗，或者说，如果我们认为。

他对前面两个条件不产生影响的话，是因为什么造成的，那我们前面想想，刚才讲到的两个马尔可夫性假设，第一个假设说的是什么，第一个假设说的是任意一个时刻的状态啊，任意一个时刻的状态。

只依赖于它的前序时刻的状态，所以这个条件你是不能够去掉的，也就是说这里的T减一直接决定或者影响了T，IT减一在哪，IT减一在这，IT在哪，T不就是在这吗，T减一在前，T在后，那么很显然刚才我们说的。

其次马克服性假设不就说的是，IT只依赖于IT减一，那么当IT减一作为条件的时候，你是不能够去掉的，所以这个IT减一等于QJ你是不能动的啊，所以这个条件是原封下来的，那么看另外一个gt ot。

我们前面也讲到过，影响OT的是谁，OT在这影响OT的只有IT来决定它，如果T作为条件，那这个条件也不能去掉，但问题在于后面这个条件里面有T吗，没有啊，没有it，所以说影响it的it减一可以作为条件。

但是影响OT的那个T是没有作为条件出来的，换句话说，你这个地方即使有O1的OT减一，按照那个所谓的观测独立性，假设你这里的O1到OT减一，不影响这里的OT，因为OT只只依赖于IT。

同时也不影响这里的IT，因为IT减一影响IT，所以就是说你这里的条件存在还是不存在，都不直接影响前面的两个随机变量，所以这个条件也就没有存在的必要了，所以就退化成了下面这个式子了好吧。

所以从这一步到这一步啊，是因为你这个条件是可有可无的，没有用处啊，所以式子也做了相应的简化啊，这是刚才那个一个疑问，那下面这个疑问又是下面这个问题是怎么来的，是怎么解决的，咳咳。

下面这个问题说的是阿尔法T减1G，为什么等于这个式子，那很显然，我们需要看一下，阿尔法T减1G的展开式到底是长什么样，我们但手头上只有这么一个东西，我们手头上只有一个阿尔法TI。

那我们需要找到的是阿尔法T减1G等于什么，那就展开看看呗，按照定义形式对吧，自己展开看一下，首先还是个概率对吧，阿尔法TI说的是O1到OT，那么很显然这里的T是和这里的T对应起来的，你现在是T减一。

很显然是O1O2点点遇到O什么OT减一吗，然后呢还有IT等于QY，那很显然应该是IT减一等于什么IT减一，这里的I对应的是这里的I，那你现在是既然是G那很显然是，所以说IT减1G根据定义展开。

是等于PO1O二一直到OT，然后是OIT减一等于QJ，你再看看上面这个式子，是不是就是说的这个东西，这里O1到OT减1O1到OT减一，IT减一到QG，IT减一到QG，所以说啊这两个式子相等。

是通过定义展开的，只不过我不是在T时刻上展开，而是在T减一时刻上展开来，按照标准定义形式展开就可以了，这也没什么可说的是吧，嗯好继续往下，那从上面这一步到下面这一步又是因为什么呢，我们需要再仔细看一下。

首先这里的阿尔法T减1G，这里有个阿尔法T减1G，这里是没有变的啊，这个是没有变化的，然后很显然这个式子，这个式子又分别表示成了下面两个式子的乘，积的形式，那仔细的再看看啊。

再仔细的看看这又是因为什么呢，首先我们可以看到it等于QI，it等于QI跑到这来，然后呢OT跑到这儿来，然后IT减一等于QY啊，在这，然后呢看后面IT减一等于QY，T减一等于QG，哎呀这个是因为什么呢。

哪个同学能看出来了，那这个时候很显然，我们知道这里的IT等于QY和OT，是一个联合概率的形式，当然是件之下的联合概率是吧，那这个时候同样同样，我们还是使用我们的这个乘积规则啊。

把它展成条件概率和边缘概率的乘积，只不过这个时候呢我们可以看一下，稍微仔细一点啊，稍微仔细一点，这个时候我们只需要保留的是谁，保留的是OT在前面啊，OT在前面，那很显然这里的IT等于QI。

就作为了条件部分了吗，作为条件，而这个地方的I7-1等于QG，同样作为条件再放下来，所以你会发现这是原条件在这儿没变，这是原来的两个随机变量的联合概率，其中的一个条件放到了这儿，另外一个在前面。

所以这是一个根据我们的乘积规则啊，根据我们的乘积规则展开的条件概率的形式，既然改成了条件概率，那这个时候的这个条件你还需要把它还原回去，乘以它的什么，乘以它的边缘概率，所以你会发现这个地方他还在这啊。

作为一个边缘概率，但是呢不要忘了，他同样是在IT等于QJ的条件之下，所以这个条件你还不能变化，所以说从这一步到这一步，用的还是我们的乘积规则，只不过这个时候的乘积规则，因为后面带了一个条件概率啊。

带了一个条件概率，所以这个条件你不能够说去掉就去掉，还必须要保留着，至于这个条件起不起作用，那是另外一回事啊，即使它不起作用，你写在这也没问题，当他你觉得不影响前面的这个条件，决定的这个随机变量的时候。

如果你有依据，就可以把后面这个条件就去掉了，没问题吧，所以说啊，这个地方使用的还是我们的程序规则，那好了，有了成绩规则之后，这一步就是因为什么好了，我们再看看这一步在这啊，这是没问题的。

落回来你再分析分析这一步就很有意思了，这一波啊是条件概率啊，这一部分是概率，这部分概率说的是什么，说的是在T减一等于QJ的条件之下，it等于QI，很显然说的是状态和状态之间的关系，IT等于QJ。

然后再按哎哎可能吗啊，这是IT减一等于QJ，在IT等于QI啊，这么一个概率值，那反过来啊，不是前面的影响，后面，IT减一等于QG的条件之下，T等于QI，那么很显然，这是说的是两个相邻状态之间的转移概率。

而这个转移概率一定是在哪儿，是不是一定是在我们那个状态转移矩阵A里面，但是状态转移矩阵A里面那个AI，这说的是什么那个概率哈，我们直接看定义吧，在哪来着，在这，人家说的也是前后之间的关系。

两个时刻的关系，但是说的是在前时刻等于QI，后时刻等于QG，等于AIJ，但是下面这个式子说的是什么，下面这个式子说的是在前一个时刻等于QJ，后一个时刻等于QI，所以就不再是AIG，而是什么AJI。

很多同学啊这个地方都都问，是不是书上印错了啊，你看看你那个统计学习方法里面没有印错啊，这个地方确确实实AGI，为什么是AGI，就是因为啊在公式的推导过程当中，这个地方说的是前一个时刻等于QJ。

后一个时刻等于QI啊，所以一定是AGI好吧，不管是AI还是JI，在我们的状态转移矩阵A里面，是已经被定义的啊，这个是没有问题的好吧，所以这个是通过我们的状态转移矩阵A，来确定的。

那下面这个式子说的就是什么，下面这个式子啊是一个条件，概率决定哪个随机变量呢，决定是OT，而我们前面也讲到过，在T时刻的状，T时刻的观测只和同一个时刻的状态有关，和其他时刻的状态和观测都没关系。

所以说后边这个条件压根就不起作用啊，压根就不起作用，那这个时候我们再看一下，在T时刻等于QI的条件之下，T时刻等于OT，那肯定是通过谁啊，是不是通过我们的观测概率矩阵B来决定的，不就是那个BIOT吗。

在T时刻等于QI的条件之下，T时刻等于gt的概率BOT啊，所以说从这一步到这一步，完全是通过我们的已知条件，那个拉姆达来决定的，拉姆达里面既包含A也包含B，还有那个派，虽然我们没用到是吧。

但是我们知道这个地方信息数据是有的，那有了这次这个到这一步以后，我们需要停下来分析分析这个结果啊，这是最后我们要推导出来的一个结论，那这个结论有什么用，这个结论其实是非常有用的，第一个其中的三部分。

其中的三部分，第一部分BIOT是B矩阵里面存在的，这里的ADI是我们A矩阵里面存在的，当我们已知拉姆的条件之下，以上两部分信息是已知的，那么得到的第三部分啊，非常有意思。

他得到的是阿尔法T减1J我们现在建立的啊，建立的是阿尔法T2和这个式子的关系，而在这个式子当中的前两部分都是已知量，已知就是已知嘛，就就就算出来结果就带带你去算就可以了，问题是。

我们建立的是一个阿尔法TI和，阿尔法T减1G之间的关系，那这两个关系有什么作用，你会发现很显然，明显的说明的是，DT时刻的阿尔法值，和DT减一时刻的阿尔法值之间的关系，换一个角度。

当我知道了DT减一时刻的阿尔法值的时候，代入到下面这个式子里面，我们就可以计算出DT时刻的阿尔法值，当我有了DT时刻的阿尔法值，同样带入到它所对应的这三部分里面去，我就可以计算出谁啊。

是不是可以计算出那个阿尔法T加一啊，有了阿尔法T加一，我就可以计算阿尔法T加二对吧，这是从前往后的一个计算逻辑，但问题是你这个阿尔法T减一怎么得到呢，阿尔法T减一我要记得到这个值，那么很显然。

阿尔法T减一，是不是可以建立阿尔法和它和阿尔法T减二，之间的关系，有的人RFT加二，你也不知道，那同样嘛一直在往前追溯，我要计算阿尔法T减二，我需要知道阿尔法T减三，T减4-5减六，一直到减。

一直到阿尔法一那个时刻，而阿尔法一里面一定包含的是第一个时刻，而第一个时刻是在SHA派里面，换句话说，有派我就可以计算阿尔法一，由阿尔法一可以计算阿尔法二，由阿尔法二就可以计算阿尔法三，计算阿尔法四。

一个阿尔法T减一计算到阿尔法T，计算到阿尔法，大体换句话说，我们建立的是这个倾向概率的计算逻辑，告诉我们，前向概率之间是有一个所谓的递推关系的，就在这，我建立的是邻相邻两个时刻的递推关系。

那这两个相邻时刻的递推关系告诉我们，当我知道了拉姆达的时候，当我们知道了ab派的时候，第一个时刻的阿尔法值有了，那么以后所有的阿尔法值都可以计算得到啊，都可以计算得到，那有了这个前向概率的作用是什么啊。

一会我们就可以看一下啊，有了这个阿尔法作用还是非常重要的，那么到此为止哈，这个阿尔法前向概率部分它的定义在这放着，有了阿尔法TI，我们需要建立的是相邻两个时刻的阿尔法之间，的递推关系啊。

这个递推关系的关系式也摆在这个地方，每一步的计算都是有依据的啊，为什么要讲这个东西，第一教材上是没有展开讲的啊，他只给出了一个结论，很多同学对这个结论是有困惑的，就是为什么或者怎么得到这个结论。

逻辑就摆在这，大家可以看到啊，这个逻辑推导还是有一定的，这个稍微复杂一点啊，这是第一点，第二点呢说的是什么，就是你希望得到某一个结论的时候，或者说你已经知道这个结论，而希望看一下它的递推关系的时候。

其实无非使用到的就是所谓的基本的概率，计算规则，以及当前这个模型所特定的一些基本假设，或者说是一些这个前提条件，有了这些前提条件，基本假设，再加上两条基本规则，也是可以通过这种分析得到你想要的结论的。

那么关于这个前项概率部分看有什么问题吗，是不是T等于QI的条件，首先前面是个联合概率，就没有条件，他是谁的条件嗯，我不太清楚这个问题，所以最好再稍微整理一下好吧，如果整理一下，说不清楚的话。

你可以截个图，在我们的资料上截下图，然后你再画一下到底谁的条件，好了，前项概率呢我们就介绍到这，那前行概率它有啥用或者怎么用，是个问题是吧，看一下再计算，不要忘了我们的目标是什么。

我们的目标是为了要计算这个PO是吧，POPO呢我们可以把它写成P，从O一一直到O大T啊，这是没什么问题的，PO从O一一直到殴打T，然后同样从这一步到这一步是怎么得到的，原来只有O1的O大T是吧。

O1的O大T，现在呢再加上一个I大T等于QI，I大T等于QI，那么很显然多了一个随机变量啊，多出来这个随机变量你还需要通过什么，通过求和啊，把它还原回去，好还原回去呢，这个地方是QI嘛，所以I从一到N。

那么很显然使用的是什么加和规则是吧，加和规则，那加上这个随机变量的作用是什么呢，再看下面这一步，他可以得到什么，就从这一步到这一步是怎么得到的，从这一步到这一步，很显然我们可以得到式子上告诉我们。

这一部分是等于下面这个式子，而下面这个式子是阿尔法大TI对吧，阿尔法大ti，那么按照我们的这个关于阿尔法的定义，还是那样，你需要把阿尔法大ti展开是吧，阿尔法大ti等于什么，那就比较一下吧。

阿尔法大ti不就是等于等于什么，原来阿尔法TI是从一到小T，你现在是阿尔法大T，不就是从O1点点遇到O大T，这个是I小T等于QI，那现在不就是再加上I大T等于QI，不就可以了吗。

按照我们的简写符号不就变成O1从一到大T，然后呢是I大T等于QI，那你看看这个式子和下面这个式子，对吧，这个式子本身就是关于前项概率，在大T时刻的前项概率的一个展开式嘛，所以这是根据定义来的。

那得到这个式子有什么用，你再看看得到这个式子有什么用，那很显然得到这个式子以后，在整个表达式里面，我只需要把这个阿尔法大TI计算出来，比如这个阿尔法大TI进行全进行一个求和，就可以得到这里的PO是吧。

但问题现在就是这个RF大TI能不能得到，而他大ti我不知道等于多少，但问题在于，我前面，我们已经建立了任意两个时刻的，前向概率之间的关系，阿尔法大TI我不知道，那我好了。

我计算一下阿尔法大T减1I是不是就可以，那阿尔法大T减1I是多少，我也不知道，那我需要计算一下阿尔法大T减2A，同样不知道继续往前倒，一直倒到什么时候，一直倒到那个阿尔法1I。

如果我知道了那个阿尔法1I等于多少，我就知道阿尔法2A，阿尔法2I知道了阿尔法3I，阿尔法大T减1I，一直到阿尔法大TI是不是已经知道了，一旦有了阿尔法大TI知道之后进行一个求和，我就知道了P5。

所以说前向概率的这个逻辑就告诉了我们，任意两个时刻的前向概率，我是可以啊，这个递推关系是可以建立的，而一旦建立了以后，我需要知道这个阿尔法大T的结果的时候，我就可以一直往前倒，那一直往前倒呢。

我就需要知道那个阿尔法一是谁，那这个时候很显然所有的矛盾啊，所有的矛盾都聚焦到了那个阿尔法一上了，那现在也就是说那个阿尔法一，你是不是能够把它计算出来的问题，一旦把那个阿尔法一计算出来了。

你就可以知道阿尔法二，阿尔法三就是阿尔法四，也就是阿尔法打T，而把所有的阿尔法大T都求和，你就知道了，这是你的P阿尔法一是个核心问题，阿尔法一等于谁呢，还是那个老样子，还是那老样子，阿尔法一。

它就是阿尔法一嘛，按照我们前面的定义，你把热烈的阿尔法一展开看看它等于什么，不就完了吗，阿尔法1I阿尔法1I很显然也是个概率，按照我们前面介绍的阿尔法一，I是应该是等于O1。

然后呢是I1等于QY没问题吧，I1等于QY，然后呢这个是个怎么办，下面有没有同学能够给给一点意见，这个联合概率怎么计算，很显然，这是一个关于两个随机变量，O1和I1等于QY的联合概率。

那你想想这个式子的概率值，我们怎么样能够通过一些已知量把它表示一下，换句话说他应该等于什么，按照我们前面讲到的放在这吗，联合概率不知道等于多少，联合概率概率，联合概率不知道等于多少。

你可以把它拆成边缘概率和条件概率，条件概率和边缘概率的形式是吧，这个地方稍微需要仔细一点，就在于谁作为条件O1是作为一个，你看看两个随机变量的联合概率，我拆成，把它拆成了条件概率和边缘概率的乘积的形式。

而你看看这两部啊，这一步这一步说的是什么，这一步说的是，在第一个时刻状态等于QI的条件之下，观测等于勾一，那么这个不就是等于那个BIOE吗，这是注意啊，这是说的是还是那样画图啊，如果你真的琢磨不清楚。

你画图，这说的是在第一时刻，第一时刻I1I1等于QI的下O1吗，那就是由状态决定我们的观测，这不就是那个BEOBIOE吗，我们的观测概率矩阵，那下面这个是什么，下面说这个式子说的是I1等于QI。

I1等于QI，第一个时刻等于QI的条件由谁来决定，是由我们的派I那个，初始向量来决定的，而我们知道，当我们已知拉姆的条件之下，这里的B和派都是已知的是吧，所以说啊那这样的话阿尔法1I就是已知的。

就像刚才我所说的，看这里的狮子，初始值阿尔法1I，阿尔法1I就等于派I乘以BIOE，这个式子怎么得到的，刚才已经说了是吧，阿尔法1I知道了，有了阿尔法1I我们根据DT表达式。

那这个时候当我们知道了阿尔法TJ的时候，我们代入到这个式子里面，就可以计算出阿尔法T加一啊，前当我们知道了阿尔法T，就可以计算阿尔法T加一，再把阿尔法T加一带到这儿，就可以得到阿尔法T加二依次向下。

最终我可以计算出来的就是谁，就是那个阿尔法T阿尔法大T是吧，阿尔法大T，而我知道当阿尔法大T有了以后，进行一个sum求和，这个时候就可以得到这里的PO，而我们PO就是我们在已知拉姆达条件之下。

希望得到的观测序列的概率值，我们的求解对象好吧，看看这一部分还有什么问题吗，当然概率计算远不止这一个，后面还有很多，还有很多个，我们就不一一介绍了啊，但是呢基本的逻辑是一样的啊。

就是从一个量啊得到另外一个量，只不过中间的计算步骤啊，使用我们的java规则，乘积规则和那个齐次性假设啊，那个那个其次假设和观测独立性假设啊，这一系列的这些已知条件，你做一个计算就可以了啊。

这些结果呢就摆在这，有兴趣的同学哈，你会记住自己看一下啊，自己看一下，当然这些中间的结果啊，我们计算过程你是你你有兴趣看，没兴趣就就算，但是这些中间的结果，你知道是可以通过我们的已知量。

可以把它计算出来的，后面我们用的时候直接拿过来用就可以了，好吧，这是关于第一个问题哈，第一个问题，第一个问题解决了以后，我们看第二个问题，第二个问题呢称之为学习问题啊，他说的是已知观测啊。

已知观测要把这个模型构建出来啊，这个问题很厉害啊，要把这个AB派都需要搞定啊，这个问题呢就比较麻烦，为什么麻烦呢，是需要看一下这个模型，我们前面讲到过啊，就是在当前这个模型当中。

状态状态状态观测观测观测，现在呢我们仅仅知道的是观测，就仅仅知道的是这个东西，还需要把ab和派都需要搞定，那这个时候呢最麻烦的问题就在于，I我不知道啊，这里的状态序列我是不知道的。

那这个时候的问题求解就牵扯到另外一个话题，就叫做带有隐变量的计算问题啊，带有隐变量的模型求解问题，那这个时候呢我们可以借助另外一个嗯算法，叫做期望最大算法啊，所以这个时候呢需要我们把期望最大啊。

介绍一下em em介绍完了以后，我们回过头来，把em代入到我们这里的求解过程就可以了，Em3，em算法是用来求解带有不完全数据的模型的，计算的啊，所谓的不完全数据哈。

这里的不完全数据就是我们的观测随机变量，那就是我们的观测啊观测，然后呢完全数据呢就是带着观测，还带着我们的隐变量，那这个时候呢因为不同的这个模型和算法，它们的符号不太一样，我们需要做一个对应关系啊。

做一个对应关系怎么对应，那这里所介绍所谓的隐变量Z啊，这里所谓的隐变量Z，就是我们马尔可夫模型里面的状态序列I啊，他俩是一回事，而这里的所谓的观测随机变量，Y就是我们马可夫模型里面的观测序列O啊。

他俩是一回事，好吧，就是em算法里面用YZ来表示，但是在HMM里面是用go i来表示啊，你知道他俩对应关系就可以了，那下面的问题就变成了，当我们不知道I啊，或者说我们不知道隐变量Z隐变量嘛。

就是我们不知道的时候啊，但是呢我们虽然不知道它的值，但是很显然它是存在的，那这个时候我们怎么在隐变量存在的条件之下，我们要把完全数据的联合概率计算出来的，问题啊，就这么个东西。

看下面当含有隐变量Z的概率模型啊，我们的目标是极大化观测变量Y，关于参数theta的，关于参数theta对参数theta的对称，自然自然函数啊，就是参数theta的对数似然函数的极大化的问题。

那这个时候呢，我们看一下这个对应自然函数对数，虽然函数呢就是在条件之下，西塔之下Y的一个呃连上外率需要注意一下，这里的Y哈是个是个序列啊，因为这里的是个序列嘛，所以这是个联合概率啊。

不要认为这里的Y是一个嗯，这个什么不是只有一个随机变量，你或者说你可以把这里的Y，认为是一个随机向量啊，由若干个随机变量组成的一个随机向量，所以说啊这个时候还是挺复杂的。

复杂还就体现在不仅仅是由我们的这个呃，观测变量Y，还有所谓的隐变量Z啊，还有所谓的隐变量Z，同样哈从这一步到这一步，用的是我们的加法规则啊，这没问题，然后呢从这一步到这一步，我们继续把它展开。

因为这里的YZ是联合概率，所以可以拆成条件概率和边缘概率的乘积，所以这个地方是用的乘积规则好吧，最后呢我们需要得到的就是关于对数，自然函数的对数，似然函数的极大化问题就是下面就是极大化。

下面这个式子七大画，下面这个式子呢高出来，带给我们带来很大的麻烦啊，这个式子还是又有log又求和是吧，他又带着一堆成绩，还挺麻烦的，那最差的极大化怎么办，怎么怎么完成，我们下面有个策略，我们看一下。

因为很显然这个求极大化的过程，我们需要进行一个迭代过程，需要迭代的计算，既然是迭代的计算，我们比较一下对应自然函数LC，它与第二次迭代以后的对手，自然函数LCI之间的那个差值，注意啊，这里的参数。

Theti，代表的是我们在第二部已经得到的那个参数值，因为在第二步那个参数值我已经知道了，所以带入到我的私人函数里面去，这里的自然函数LCI也是已知的，那现在我需要看的是我的优化目标。

LC塔就是这个式子啊，就是这个式子，我的优化目标和在第二轮，已知C塔I以后的那个LC塔I之间，的差值是多少，我分析一下他俩的差值，这是一个常用的策略啊，什么意思呢，就是当这是我们的theta。

这是我们的LCA当我们这个LCA这个很复杂啊，这个函数形式啊，或者说我压根可能就不知道，但是呢我又希望找到它的极值，比如说极大值，那么很显然这个地方是我们要的C大箱，我们很显然是要得到这个最大型。

但是呢这个函数形状我又不知道，那这个时候怎么办，往往我们会在任意时刻找到一个西塔值，作为CA0是吧，然后呢我我既然又不知道这个函数长什么样，那怎么办，我看一下，如果我知道他在lc ti的时候的一个值。

比如这个这个值我知道的，那这个时候我看一下整个的函数，基于这一点的时候的一个函数形状，能不能表示出来，换一个角度啊，就是说如果我能知道它，整个函数和一个定值之间的关系，我找到的时候。

其实整个的函数其实加上一个常量，是不是也已经知道了，当然这是一个简单的一个思路，因为这里的C塔零是在某一个位置上的，一个参数值，这个位置上参数值带入到LC塔以后，这个具体的位置。

是会随着我们的迭代过程发生变化的，也就是说我我要找到一个和某一个时刻的，lc ti之间建立某种关系，有了这种关系以后，我只需要把整个的函数形状表达成，关于某一个时刻。

c ti的这个差值的大小的形状就可以了，而我下面的问目标只需要使这个差值怎么样，是不是仅希望使这个差值尽可能的小，是不是就可以了，那是不是就可以趋近于我原函数的一个情况。

而我们知道使得我这个差值尽可能的小，又使得这个这个差值达到一个，尽可能大值的时候，就趋近于我的原函数的一个极大值的情况，这是我们一般的一个策略啊，所以我们一般需要看一下当前这个LC塔和它。

它在一个具体参数位置上的，那个那个已知函数值之间的差值，我们看一下这个结果是多少，那么看下看下面分别代入哈，分别代入就是把LC塔放在这后面呢，很显然就是LC塔I啊，LC塔ILC塔I嘛。

就等于log p在lc ti在c ti条件之下，Y的概率值啊，这个从这一步到这一步就是个代入，那么下面一个从这一步到这一步，为什么是的就有点复杂了啊，但是仔细一点就没问题啊，这一步在这啊。

这是没问题的问题，就便于这么个式子变成下面这个式子，他俩是怎么得到的，到这儿是怎么来的，其实并不复杂，你可以看到这里的SAMZ还在这里，是PZCYPZC，它条件之下Y这里还能是个PC塔。

条件之下ZPC它条件之下的Z啊，比如说行这一部分在这没问题，那很显然是多了这两部分，那多的这两部分一个在分子上，是一个在分母上，而且这个时候的两部分相同，那也就是说分子分母同乘了一个条件概率。

我们知道条件概率，这样的是不影响我们整个式子的，所以这个时候，也就是说从从这一步到这一步，就是在我们这一项当中的分子分母，分子分母上啊，同乘了一个PYC，它I条件之下Z的概率啊，就这么一步啊。

这个没什么太多可说的，然后呢从这一步到这一步呢，可能会发现哎我们得到的是一个大于等于，比如上面这个式子啊，上面这些式子是大于等于下面这个式子的，为什么上面这个式子大于下面这个式子呢。

我们需要借助一个所谓的琴声不等式啊，这算不等式形成不等式哈，这个我们是数学上的一个不等式性质啊，你拿过来直接用就可以了，有兴趣的同学你可以查一下这个数学课本啊，这个地方我们直接拿来用就可以了。

他说的是什么，他说的是log萨姆拉姆达Y哈，是大于等于sum拉姆达log y啊，你就这么简单看理解就可以了，log萨姆拉姆达Y大于等于萨姆拉姆达log y，那么比较一下。

看一下log sum lambda外四部分是吧，Log some lambda y，log sulambda y按说是大于等于SULAMBDA，log y看看下面大于等于。

lambda log y明白怎么来的了吧，所以就是前成不等式的使用啊，这也没什么可说的，哎呀好了，我们继续往下下面一步呢就不好理解了，就从这儿到这儿是怎么来的。

从这到这会发现这里有个log PCI条件之下，Y的时间概率跑哪去了，就这部分哈没有了，跑哪去了呢，跑这来了，跑这来跑这来吧，你这个logo就没有了，那为什么这个地方呢，我们需要分析一下。

分享很显然这个地方如果是log log变成log a b，比上log log a log b等于log a b log b嘛，这个时候挺好办，但问题麻烦的就在于，它前面还多了一个sum z p y c。

ti条件下Z的概率，那这一项里面没有这部分，所以你就不能把它提供因式提出来，然后变成log log是吧，那问题是嗯能不能把它凑一下，怎么凑一下，就是把这一部分式子，放到前边了，可不可以。

当然可以还是不可以，我们需要计算一下这个前后之间加上以后，是不是发生变化是吧，是不是发生变化，还会发现一个很好的一个结论是什么，一个很好的结论就是这个式子本身上，就是这个式子SZ，然后是PZ条件之下。

yc ti这个式子等于几啊，这是个条件，概率是关于谁的条件，概率是关于随机变量Z的条件概率，而前面又是个SAMZ，我们知道这个式子我不管后面这两个条件啊，不管这两个条件，因为这两个条件是已知条件嘛。

当已知条件之后，关于Z的时候，因为我们知道这里的Y是我们的观测变量，这里的C塔I是我们的上一轮的已知参数，都是已知的嘛，在这两个已知量的条件之下，Z随机变量要进行一个求和，而我们知道这个式子一定等于几。

一等于一，注意哈，这仅仅是这个式子等于一，不代表哈这个式子，换句话说，你看看这个式子里面，随机变量Z在这是不是也有随机变量Z，所以说这个式子即使等于一，那么这个式子也不等于后面这个式子。

你不能把这个式子你不知道不等于一吗，一乘以任何值不都问一嘛，不行，是因为这里的随机变成Z和这里变成Z有关系，而这个地方是个乘积，所以sum求SAM的求和是对这个乘积的求和啊，这个地方需要注意一点。

但是呢这不影响什么，这不影响我发现在这一项上，如果我加上一个p z y c ti乘一个PYC啊，这还有个log，Log p，YCA是不影响的，为什么，因为这里的Z随机变成Z，在后面这一项里面是没有的。

所以这个地方我等于是乘了一个，一一乘以这个式子，而这个时候一凑，你会发现哎，这一项和后面这一项我就可以，形式上就可以把它统一起来了是吧，然后呢，这个地方就变成了LV减log b。

那这个时候的这一部分就自然的出现在了，分母部分上了，好吧啊，这个这个推导过程啊，如果你觉得这个推导过程还有问题，回去以后好好看一下，我们只要结论啊，我们要结论要什么结论呢，我们看看我们现在手头上。

一个得到一个什么结论，我们手上得到了一个LC塔，减去LCI要大于等于这个式子，要大于等于这个式子大于等于这个式子呢，我们看看就像下面这个式子，就像下面这样以后哈，我们因为我们知道啊。

这里的这个LCI是个已知量啊，在DI流的时候是吧，我们把这个减LC的I扔到等号的右边去，因为等号右面去之后，这个地方就变成了加上一个LCI，变成了LC塔式大于等于这个式子啊，就是下面这个式子。

令lc ti加这一部分等于一个B，那么得到的就是LC塔I是大于等于B西塔，西塔I的，LC的I是大于这个式子的啊，换一下看看我们知道一个什么东西啊，我们要求一个LC它的极大值。

但是呢我们通过刚才的分析会发现，我现在的LC塔大于等于下面这个BCACAA，也就是说我得到了一个下界函数，就是原函数的下面一条函数，而我们又知道在下面这条函数如果求极大的话，它的一个极大位置。

应该就越来越趋近于我原函数的极大位置，这是想这是显然的，因为同一个例子，比如说呃，我们班级里面学习成绩最不好的，那个同学的成绩都有一个大幅度的提升，我们学习好的同学肯定他也必须要越来越努力。

就是我的下界函数都越来越大的时候，我的上界函数也趋近于我的最大值，那这个时候下面的问题就转化成了，我只需要使这个B函数它的极大化，来代替原函数的极大化，那好了，那么对B函数的极端化呢，我们分析一下啊。

B函数展开以后，这个式子也挺复杂，但是呢我们加以分析会发现，首先这个lc ti是不需要的或者不影响的，因为它是已知量，在DI里的时候，他已经算出来了，而这里的这个分母部分，分母部分我们也不也不影响。

应该都在大是吧，也不影响，那这个时候只剩下了分子部分，尽可能的怎么样极大化就可以了，那这个函数我们再给它起个名字叫做Q函数啊，Q函数，那么em算法的一个核心就在于，找到带有隐变量的模型的Q函数。

而Q函数就摆在这个位置上，我们需要唯一需要做的就是分析，Z是我们的这个呃状态变量，Y是我们的观测变量，把这个能够对应起来以后啊，对应起来以后构建我们的Q函数，下面的问题就是当我们求原函数。

原函数的极大化问题的时候，转化成了它的下界函数B函数的极大化问题，而通过一系列的分析，我们会发现，B函数的极大化又等价于Q函数的极大化，所以问题就转变成了Q函数，而Q函数当中既包含我们的随机变量。

Z又包含我们的随机变量Y，只需要把它套入到公式里面去，就可以得到原函数的极大化，好吧，看下em算法，em算法其实就两步，第一步嗯先初始化我们的CA0啊，随机给出一个参数位置，然后呢第一步给出异步。

所谓的异步呢就是构建我们的期望，期望呢其实就是刚才那个Q函数，有了Q函数以后，第二步M不就是使我们的Q函数极大化，Q函数极大化的那个最优参数，就是原函数的极大化，最优参数好，那下面的问题就在于回过头去。

下面我们现在有I有OI是我们的隐变量，都是我们的观测变量，现在我们希望找到的是带有隐变量I的，联合概率的极大化问题，那就是原函数我们知道挺复杂，转变成它所对应的Q函数的极大化就可以了。

看看这一部分有什么问题吧，那我们就继续啊，下面其实有了这个工具以后，下面其实就是一个对应的问题，对应求解的问题，你把符号放好了，继续下，嗯好学习学习算法，将我们的观测序列啊。

就将我们的观测序列作为观测序列，O将状态序列作为隐隐变量或者引数据I，那么一马可夫模型呢，就是带有含有隐变量的一个概率模型啊，还有隐变量的概率模型，那这个时候p o lambda啊。

都是我们的那个观测嘛，p o lambda展成PO在I条件之下的PO，然后再乘以pi，就是刚才我们所说的PO是我们的观测变量，I是我们的隐变量，那这时候就完全数据就是OI吗。

完全数据OY那这个时候就完全对散函数，就是log p o i lamba，那这个时候就像刚才我所说的，异步定义Q函数，Q函数就在这啊，对应起来放在这就可以了，那这个时候呢你会发现在Q函数当中。

我们把联合概率拆开以后，把联合概率拆开以后，拆成了一个边缘概率和联合概率乘积，再出一个边缘概率，这个联合概率乘以联合概率，再出一个边缘概率的形式，注意一下，这里的变异概率是在拉姆达一拔。

就说他是在上一次的那个参数，已知的条件之下的边缘概率，所以这个时候的分母部分哈也是不影响我们的，所以我们的Q函数可以进一步，削减成下面这个式子好吧，约减成下面这个式子，而约定成下面这个式子里面以后。

我们可以看到剩下的就是联合概率的形式啊，POY的形式，POI等于什么，下面给出POI的定义，POI定义，注意一下，这里是O和I的联合概率，就是我们的状态序列和观测序列的联合，概率的形式，既然是联合概率。

就意味着当前概率模型要从I1I2I三，一直生成到I大T，并且对应时刻的O1O2，一个O大T也需要生成，也就是说，我们需要把当前这个姨妈可夫模型生成出来，而生成算法在我们前面已经介绍过了，怎么生成。

看一下，先通过派生成I1，然后生根据B生成I1所对应的O1，再根据A生成A1，A1所对应的I1所对应的I2，再根据B生成I2所对应的B2，依次向下生成就可以了，所以说你会发现这个联合概率要拆开以后。

就是根据刚才我们的派币ABABAB，生成我们的整个一马可夫模型就可以了，而生成过程当中啊，而生成过程当中你会发现派不知道B不知道，A也不知道，因为我们现在知道，谁知道我们的目标。

就要把那个兰姆达先生找出来，所以这个地方的ab派啊一概不知一概不知哈，没关系，恰好这个地方做的是一个对数，自然你看了吗，这太有意义了，恰好是个对数，似然函数的一个极大化的求解问题。

而我们知道里程的一个对数，求垒成的球队数就变成了什么，就变成了累加，这个时候就变成了几部分，第一部分派是作为一部分，第二部分B作为第二部分，第三部分A把它作为第三部分，那么整个的累乘变成了累加，累加呢。

我们把它翻成变成三部分的累加，哪三部分呢，就像刚才看到的，这是第一部分，仅包含那个pad累加，这是第二部分，包含我们那个A的累加，这是那个第三部分包含B的累加，也就是说。

刚才刚才的里程通过落个求和就变成了累加，累加呢我们单独把它们归归啊，分分组啊，分分组哪，分成三组，哪三组呢，派的一组，B的一组，A的一组啊，变成这三组的累加，那下面的问题在于我们要对累加的和极大化。

李家的和的极大化的问题转化为了这三组啊，这三组分别极大化的问题，和我们使得第一组极大化求出谁啊，求出派星，第二组极大化我们求出那个B型啊，一样A型，第三组我们一起使它极大化求出那个B型。

这样的话呢有了A型，B型和派星，这个时候就组成了我们的，拉姆达星我们的模型就是了，这是基本的一个思路，那下面的问题就在于使这三部分分为极大化吧，看一下分别替代化分别极大化的问题啊。

其实就是一个带约束条件的，因为我们知道我们在对派进行极大化的时候，我们知道在第一个时刻可能取值的，所有的状态的概率和一定为一，这是没问题的啊，因为我比如说我第一个时刻能取三种状态啊，在等概率的时候。

每一种状态肯定显示1/3，三分之1+3分之1+3分之一是一，有同学说不等不等概率的时候，不等概率的时候，虽然他们的概率值是不一样，但是概率和一定为一啊，这个没问题，所以一定是带着约束条件的极大化问题啊。

带着约束条件的极大化，而这个问题呢，其实按照嗯我们最简单的一个方式，分别对我们的派I求偏导就可以了是吧，派I求偏导让它等于零，得到一个关于派的一个表达式啊，这个时候你把这个表达式表示成关于派的结果。

我们就可以看到，这时候的派I就是所谓的派星I啊，最优的派啊，这个推导过程啊，这个推导过程就不展开了，原理其实是一样的好吧，这是关于派星，第二部是关于那个A型，同样这是我们的那个累加和。

里面关于A的部分条件呢，同样求和为一啊，这个时候还是使用啊，刚才那个逻辑啊，还是刚才那个逻辑，你把A型的最优A的最优解A型找出来，注意啊，这里所有的这些参数啊，这些Z塔，这里的这个伽马。

都是在我们刚才所介绍的那个第一步，概率计算过程当中的中间结果啊，刚才一直在强调啊，第一步的工作就是为后续工作提供中间结果的，这些可以看一下，或者参考一下第一步的工作就可以了，同样这里的关于B啊。

B型式计算也是同样的逻辑啊，同样的逻辑，那么总结一下，总结一下，当我们要求解啊学习算法的时候，输入的只有我们的观测序列，O输出的是我们的一马可夫模型，拉姆达ab派，第一步随机初始化A0B0和派零啊。

得到我们的兰姆达零，那这个时候呢递推递推的过程，刚才已经给出来了，迭代公式已经有了吧，有了迭代公式以后，我们不断的进行收敛收敛，最后就一次得到ANBN和PN构成，我们的蓝不蓝，最后算法就可以终止掉了。

好吧，这是我们关于这个第二步啊，关于这个学习任务的一个介绍，学习任务的介绍，其实你会发现核心就在于QQ函数的构建，为什么要构建这个Q函数，是em算法里面解决的啊。

em算法里面告诉我们要求解带隐变量的模型啊，的联合概率必须要啊构建Q函数，让Q函数最大化就OK了，所以呢有了em算法回去以后再看看，有了em算法以后，这里的Q函数定义符号上对应起来。

那下面的问题呢正好hmm里面告诉我们，对数自然函数是在累乘上做累乘变成累加，分别变成了三部分的和每一部分进行最大化，得到的就是关于三部分的三个量派ab的最优解，就构成了最优的兰姆达星，好吧。

这是关于第二个问题，看看有什么问题吧，那我们看看最后一个问题，最后一个任务啊，做编码或者预测任务啊，他是知道模型以后还知道了我们的状态序列啊，状态观测序列我们要求的是状态序列的，在概率巨大的条件之下。

我们的状态序列需要构建出来啊，这是一个求序列的一个过程啊，求序列的过程，这个过程呢我们使用所谓的维特比算法，维特比算法的核心是一个，类似于我们的前向概率这么一个东西，但是呢它叫做什么呢，它叫做呃。

在T时刻，状态为I的所有单个路径的概率最大值，这个德尔塔ti啊，德尔塔ti他说的是在所有可能的啊，在DT时刻，T，在DT时刻状态为I的所有单个路径的概率，最大值，现在我们的目标是要求一个状态序列。

A1A2点了点状态的一个序列，既然是状态的一个序列，那么比如说像IT这个时刻到T的一个时刻，等于，QY假设哈IT等于QY，那么到DTDT的时候等于QY的，可能所有的可能我们比如说我们从I1开始。

I1C是一个状态，第一时刻的状态它可以是Q1Q2，第二点遇到什么QN从I1出发，从I1出发，从I1这N个可能的概率出发，我们依次可以得到I2I2，同样是Q1点点移到QNN种状态。

那么这个时候从I1的N种状态，那么到I2的N种状态，那么就有N乘以N个路径，那么同样依次往下，I1点点一直到QN，然后呢一直到IT等于QI那么很显然啊，从这种排列组合我们知道其中所有的路径，所有的路径。

我们把其中那个能够到达T的所有路径里面的，那个概率最大的拿出来，把它命名为德尔塔ti啊，这是德尔塔ti的一个含义，那我们有了这个德尔塔ti的这个定义以后，那么同样我们需要建立的是德尔塔TI的一个。

递推关系式，递推关系式，那么就是所谓的D德尔塔T加1I和德尔塔，看一下，和这里的，德尔塔T之间的关系，你看可以看到非常类似于我们那个前行概率啊，德尔塔T加一和这里德尔德尔塔T，其他的两个量。

AGI和这个BIOT加一都是已知值是吧，那这个时候也就是说啊，其实IT是因为是任意一个时刻嘛，你往前倒I2是吧，I2是在所有I1I2，所有可能路径里面的概率最大值，那么从其样同样I3是从I1到I3里面。

所有概率这个呃出现的路径的概率极大值，而我们知道I1到I3，我们只需要知道I1I2，只能决定II2到I3，那么递推关系式嘛就这么一个逻辑，那这个时候我们只需要把这个路径里面。

最概率最大的一个路径节点记录下来就可以了，那马克那个出现在路径上的概率最大的节点，记录下来，记录下来以后，当我们从第一个节点到最后一个节点，便利完成以后，就是我从第一个节点到最后一个节点。

就是那个I大T，我从IE一直按照这个概率最大，这个条件走到I大T的过程以后，那么在这个路径上，所有的节点都可以保证，是出现概率最大的路径节点，而这个路径就作为我当前的最优路径，就是那个预测算法啊。

这就是预测算法维特比算法的一个基本含义啊，它其实最核心的就是在这里的，德尔塔TI的一个定义，好吧，这样的话预测算法不作为我们的重点介绍，了解一下就可以了，如果有兴趣的同学们可以在一块讨论一下。

那这样的话，以上的内容就作为，我们今天关于一马可夫模型的所有内容，我们回过头来做一个简单介绍，四部分啊，四个内容其实是五部分啊，加em算法模型定义，lambda ab拍啊。

这里面的含义回去再看一下第二部分，概率计算，就是那个前加概率的递推关系式，重点在于每一步的递推过程，你需要有一个依据和根规则啊，这个依据和规则你需要确定，第三步学习算法。

你需要带着隐变量的联合概率的计算，需要使用em算法，回去以后看看em算法的逻辑，有了em算法以后，核心就是Q函数定义，这个时候只需要把符号进行一个一一对应，就可以了啊，联合概率的计算。

第四部分预测算法就是维特比算法啊，就是我们的那个德尔塔ti的含义，回去我再看一下，看看这一部分里面还有什么问题吗，到一个什么程度啊，就我们这一周核心聚焦的原理部分啊。

如果我们简单的回顾一下我们这一周的内容啊，我们这一周处理完了这个呃，原理部分很基本的这个术语和概念啊，这些内容呢仅仅是开始，当你接触每一个模型的时候，就像刚才我们所介绍的hmm里面那个呃，倾向概率是吧。

那什么七次马克分，假设这些东西都是术语描述的啊，你必须要能够，还有刚才同学那个同学提出那个问题，我为什么没有理解呢，我觉得可能就是你所那个提出那个疑问，其实并没有把它，通过一些更准确的方式把它表达出来。

所以造成了没有get到是吧，问题就出在这些基本术语的使用上啊，所以这部分内容你每每掌握一个模型的时候，这些基本术语必须要把它掌握，很这个术语的掌握和使用还是非常重要的，沟通的一个基础。

然后看一下我们的嗯，这一个阶段啊，我们介绍完了线性回归，介绍完了逻辑回归对吧，从线性的拧成分类的是吧，然后呢直持向量机软件隔硬间隔和函数和SM啊，这是我们一个逻辑，另外呢觉得数E3C4。5开的数。

核心问题就在于它的特征，就是特征分类的依据是什么，ID3是信息增益，C4。5，信息增益比卡的数是基于指数啊，数的定义啊，数模型的定义，然后呢BOOSTIN加上模型线下分布算法啊。

由这个boosting方法采用决策树作为奇函数，就构成了BT，在BD题里面，我们啊还有那个残差啊，很重要的那个残渣学习是吧，然后呢在GBD题里面，我们通过函数空间的分析。

用我们的损失函数对于模型的负梯度啊，作为我们残差的拟合对象，然后呢x d boost我们非常重要的一个工作，两个改进正则化项加上二阶梯度展开是吧，当然其中还包括那个呃。

那个那个最最优的那个损失分类依据啊，他和前面这三个是不太一样的，是吧，这个你需要了解，然后呢朴素贝叶斯那个推荐独立性假设啊，他是在条件什么条件之下，标签条件之下各个特征之间的独立性啊。

然后呢一码可控模型今天我们也介绍了这，当然我们还包括我们的EMM算法呃，这仅仅是开始啊，其实你会发现，还有很多的内容是没有，包含在我们这个阶段的学习当中的，比如说人工神经网络。

当然这在我们后续课程里面会有啊，也就是说呃到一个什么样的阶段呢，你必须要掌握我们整个模型当中的原理部分，比如说人工神经网络里面，模型结构就是这个模型是什么，长什么样。

你需要知道有的模型结构有最高最核心的，它的误差反向传播算法啊，呃相邻层怎么穿，跨层怎么穿啊，这个你都必须要非常了解，否则的话你看后面越来越复杂，CN无非就是加上池化层和卷积层是吧。

RN那就加上我们的中间的中间状态，加上一个时间循环，当然这种改进，你必须要是严格依赖于前序模型的工作的，基础上，你才能够理解人工神经网络，你你你前期工作这是谁啊，其实就是逻辑回归，逻辑回归，你掌握了吗。

逻辑回归的前序，我就是线性回归，线性回归，你长得怎么样，如果反过来，如果你前面都掌握了，其实到人工神经网络之类，其实相对来说比较比较能够理解的，那么同样后面哈啊CRF呢今天并没有介绍。

但是呢我会把一些材料整理一下，发到群里面，你会看一下，其实CRF和HMM的呃，在模型定义上就有类似的地方，因为它们同属于概率图模型，所以呃分析上其实也有很多相近的地方，当然规则上是不太一样的。

但是这些相近的地方，对你后续的工作其实套路都是一样的吗，你只需要严格分清楚，它在当前模型里面是怎么被定义的就可以了，同样的道理，在cf的学习过程当中使用的工具叫做最大商，M商最大。

那这个时候你需要把最大熵模型了解了以后，套用到cf里面去也就可以了，当然这一部分还仅仅是我们整个学习过程当中，的开始部分，远不是根本就没有结束是吧，所以这个时候有问题很正常，有什么问题。

我们还是需要在群里集中的进行讨论，我们的课程都是这样，我们我我认为我们的课程是信息量非常大的，课程，两个小时里面，我们基本上需要把呃一个模型的细节部分，能够重点的给他一个介绍，所以说啊。

你不可能寄希望于听这么两个小时就听懂了，否则的话我觉得哼首先你就没有必要再听了，第二呢我觉得对你来说，价值或者意义也并不大对吧，当然你听不懂的时候，需要反复的进行理解讨论消化吸收，我们的答疑群一直都在。

当然可能是刚刚课程刚开始，大家还没有习惯这种方式，但是在我们历往届的这种班级里面，都是充分的在利用这个环境，好吧好，我们这一个阶段的学习呢就算是告一段落，刚才已经说了，这仅仅是开始，有什么问题。

我们及时在群里进行学习和沟通好吧，如果没问题的话，今天我们就到这，谢谢大家。

![](img/8ab9b39dce9ef565d42fdbe934fc0395_1.png)

![](img/8ab9b39dce9ef565d42fdbe934fc0395_2.png)