# 【七月在线】NLP高端就业训练营10期 - P1：1.循环神经网络与Pytorch框架_ev - IT自学网100 - BV1uxT5eEEr6

![](img/77fdf2ced5678be963137fd30ae4b341_0.png)

好OK那咱们就开始今天的一个课程内容啊，好咱们这次课的话，主要是要给大家讲这个关于神经网络这一块啊，神经网络这一块。



![](img/77fdf2ced5678be963137fd30ae4b341_2.png)

那我们先看一下今天要给大家讲的四块内容啊，首先呢我们会从这个基础的神经网络开始，给大家讲起啊，然后呢再给大家介绍关于啊，NIP当中用的比较多的这个循环神经网络，然后是卷积神经网络，那卷积神经网络呢。

可能更多会应用在这个图像处理，这个图像处理当中啊，但实际上啊在NLP当中也还是有蛮多啊，使用这个卷积神经网络来处理文本的，那最后一部分的话，我们会做这样的一个文本分类的，这样的一个实战啊。

实战还是先讲理论后进行实战，好吧好OK，那我们就先来看我们今天的第一块内容啊，关于这个神经网络，神经网络，那什么是神经网络呢，我们这边举个小例子啊，举个小的例子，假设呢我们现在想构建这样的一个模型啊。

这个模型的作用呢，是来预测我们这个深圳的这样的一个房价啊，深圳的这样的一个房价，那我们先从简单的开始考虑啊，假设啊，假设我们这个房屋的这个总价格额，只和我们的这个面积有关系啊，只和我们面积有关系。

那这样的话，那可能就是看面积的单价是多少就OK了对吧，既然如此啊，我们大概会得到这样的一个函数关系啊，那一开始可能啊我们是从零开始嘛对吧，那肯定没有说这个面积特别小的，这样的一些房子啊。

那这可能至少他也有一个压视平嘛对吧，好他大概可能就是这个样子啊，他的关系可能是这个样子这个样子，那这个是一个比较理想的一个状态啊，就说咱们的一个面积和咱们的这个价格啊，它是呈现这样的一个正相关的对吧。

当然啊真实的情况的话，可能咱们有些样本点啊，就可能会在啊咱们这个这条函数的两侧对吧，那在咱们这样的一个两侧，那咱们只是说构建出了这样的一个函数，本来拟合咱们这个面积和咱们价格，这之间的一个关系啊。

那假设我们的这个面积是，而价格是Y，那实际上我们是可以得到这样的一个，映射关系的对吧，也就是Y等于这样的1WX加B，我们是可以得到这样的一个映射关系的，这样的一个映射关系的好。

那这个呢是我们只考虑了这个面积的，一个情况啊，但实际上呢咱们这个价格啊，其实影响因素有很多对吧，除了咱们的这个面积以外，那还得还得考虑一些房屋的一些，周边的一些配套对吧，是否啊临近地铁啊。

或者说你这个住宅楼距离这个市区的距离，远不远对吧，还有他的一个什么像房屋的一些年限，那如果你说你这个房屋还是九几年的房，那你现在嘛肯定没有10年建的房，卖的这个价格高对吧。

那假设啊我们每一个这样的一个影响因素，它都有自己这样的一个权重，也就是说对于面积这个值啊，假设对于这个特征我们定义为X1，那它对应肯定会有相应的一个权重对吧，我们用A这个W1来表示。

就好比咱们刚才这样这个图对吧，我们的这个面积啊，会有对应这样的一个W的一个权重，那你就可以得到这样的一个价格，那假设我们在这里多个因素的情况下，我们面积这个特征的对应的权重是W1，那对于我们第二个特征。

我们假设是W2，第三个特征是W3，然后我们的房屋年限是第四个特征啊，我们假设是W4的这样的一个特征，那，有了这些特征之后呢，我们会认为啊我们可以根据这些特征，我们实际上是可以得到我们最终的这样的一个。

价格的对吧，最终的这样的一个价格的，也就是说，如果啊，我们把我们的这些每一个输入的一个特征啊，看成这样的一个向量，看成一个向量，那最终呢我们的这个价格Y啊，实际上就等于我们的X乘以我们的W的装置，对吧。

最后呢，我们可能还需要再加上这样的一个偏移下，那这个东西呢这个关系映射啊，大家就可以把它简单的看成一个，所谓的一个神经网络，只是说啊，咱们这样的一个关系映射，到底应该是什么样子的。

那就取决于你的这个神经网络，它是什么样子的了，好我们继续往下看啊，这里呢啊我给大家罗列了一个很普通的啊，一个最基础版本的一个神经网络，那他这里呢，首先呢就输入了这样的一个四个特征对吧，那这个神经网络呢。

它会对这四个特征去做一些这样的一些，交互的一些计算啊，交互的一些计算，那对于这里呢我们就可以发现啊，他其实有三个这样的一个节点，三个这样的一个节点，那这三个节点呢实际上就是我们前一层对吧。

我们前一层的这些值得到的这样的一个，交互之后的这样的一个结果，那通常啊对于一个普通的一个神经网络来说，例如这个地方的一个值啊，这里咱们可以我们用H来表示，实际上呢就等于咱们这里这个X1对吧。

这里会有个权重啊，这里会有个权重，那就是啊我们这里用啊we1表示啊，然后这里呢我们用W2表示，这里呢我们用W4表示，那对于这个节点的一个值呢，就等于W1X1加上W2X2，再加上我们的啊W4X4。

最后呢我们会有这样的一个偏移项对吧，我们会有这样的一个偏移项，通常啊经过了我们这样的一个计算之后呢，我们都会在外面啊，再进行一个所谓的，加上一个所谓的一个激活函数啊，加上一个激活函数。

那这个激活函数的作用是什么呢，通常呢是引入一些非线性的一些变化啊，引入一些非线性的变化，我举个例子啊，假设啊，我们如果没有这样的一个非线性的一个变化，那走到下一层的时候。

我们实际上假设啊这里我又成了这样的一个W，假设我这里是用W5来表示好，我把这个W5再和这个直接相乘，那相乘之后的结果是不是就是W1W5乘以哎，乘以X1对吧，后面我就不写了啊，就写不写了。

那最后一项的话就是W乘以B那可以发现啊，这个时候实际上啊这里这个W1，W5和这里这个W1，其实啊可以用同一个值来表示，所以说啊咱们这个神经网络，每一层最普最普通的神经网络啊。

每一层之间我们通常都会去加上这样的一个，所谓的一个激活函数，去做一些非线性的一些变化，不然你的这个两层的神经网络实际上和一层啊，它就关系不会很大好吧，他的关系不会很大，所以啊我们需要去加上这样的一些。

非线性的一些变化啊，非线性的变化，那这就是我们的一个最基本的，最基础的一个神经网络，那这个时候可能就会有同学说了呃，那我怎么知道这个X1X2X3X，我到底应该构建什么样的一个神经网络呢。

那这里这个神经网络对吧，我们啊这里有一个中间这一层啊，我们通常称为隐藏层，然后最后这一层呢我们称为输出层，那有的同学会说，那我这个隐隐藏隐藏层，我到底应该设置几个节点呢对吧。

或者说我这个隐藏层现在是一层，那我可不可以搞两层或者三层，那都是OK的啊，都是OK的，那到底应该设置几个节点呢，这就是一个所谓的一个超参数啊，超参数需要我们去自己进行这样的一个。

超参数的进行一个调整好吧，进行一个调整好，这就是我们的一个神经网络啊，然后我们这里来看一下这里这样的一个网站啊，啊这是一个神经网络可视化的这样的一个网站。



![](img/77fdf2ced5678be963137fd30ae4b341_4.png)

我们可以去看一下啊，我们可以随便选啊，啊这边是有一个数据的一个数据集啊，假设呢我们要分这样的一个数据集啊，可以看到中间是蓝色的点，然后外面这一圈的话是这样的一个橙色的点啊。

我们要把这个蓝色的点和这个橙色的点给啊，区分开啊，分成这个样子，然后这边呢我们会有一些这样的一个，输入的一些数据好，那我们可以给它添加一些这样的一些，输入的一些数据，而中间呢就是咱们的这个隐藏层啊。

这边是咱们的输入值，这边是咱们的一个输入值，然后这边呢是咱们的一个哦隐藏层啊，隐藏层现在的话是四个节点，对吧啊，我这边先去两个啊，我就用原始的X1和X2，这两个输入值作为我们的输入啊。

这边呢是四个这样的一个隐藏层的一个节点，然后这边的话就是咱们那个输出值，因为为什么这里是R呢，因为我们要做一个R分类对吧，我们要把它区分成蓝色节点和橙色节点，好，我们可以这边是可以调整一些超参数啊。

好OK我们执行一下，可以看到啊，这个时候呢模型就是正在收敛啊，正在收敛，那其中蓝色这一块的话，就是咱们的这个蓝色的点，而橙色这一块呢里面就是咱们橙色的点对吧，而这边呢我们还可以添加更多的这样的。

一些层啊，可以添加更多的一些层，而这边呢我们也可以去添加节点的一个数量啊，然后我们也可以再跑一下哎，他也是可以一样收敛的，这边我们也可以换一下啊，换成其他的，甚至你可以多加一些特征数据对吧。

OK我们再跑一下好，这边也是进行这样的一个收敛啊，这就是咱们的一个很普通的，这样的一个神经网络啊。

![](img/77fdf2ced5678be963137fd30ae4b341_6.png)

神经网络好，那除了这样的一个最基础版本的，神经网络以外呢，其实还有很多的一些其他的一些，神经网络的一些变体啊，啊例如咱们的这个卷积神经网络，卷积神经网络，还有咱们的这个循环神经网络。

那咱们今天除了给大家介绍这种最基本版本的，这个神经网络以外呢，我们也会给大家去介绍，这个所谓的卷积神经网络，还有循环神经网络，好吧，我们就分别来看啊，我们分别来看好，接下来呢我们来看一个概念啊。

就是说如果我现在有了这样的一个神经网络，对吧，我该怎么去把我神经网络当中，这些参数给求解出来，什么意思呢，就是说我这里的这些W啊，我这里不是有很多这样的一些W吗对吧，那这个W我该怎么求解出来呢。

如果我不知道这些W的话，那你最终你输入这个值对吧，我也拿不到我最终的一个结果，所以关键啊神经网络你除了要有结构之外，还要有咱们的这个参数对吧，还有咱还要有咱们的这样的一个参数啊，也就是咱们这里这个W。

OK那我们来看个例子啊，看个例子，假设呢我现在要输入一张图片，我要判断一下啊，我输入的这张图片它到底是个什么动物啊，是个什么动物，那这里呢，我们就把这个图片给到咱们的这样的一个模型。

那这个模型的话其实就是两部分组成，刚刚说的这个结构和咱们这个参数啊，结构的话就看你去是怎么构建了，你可以去构建一个啊，普通的这样的一个神经网络对吧，你也可以去构建这样的一个卷积神经网络好。

那当我们把我们张图片输入给我们的模型，的时候呢，我们的模型啊它会输出一个值，它会输出一个值，那这个值呢表示的是我们的图片的某个类别，某个类别好，这个输出值到底该怎么定义，我们待会再说啊，待会再说。

那有了输出值之后呢，实际上啊我们对于每一张图片，我们都是有真实的这样的一个标签的对吧，人工可以给这个图片打上标签，那对于模型的一个输出结果，和我们真实的这样的一个结果。

我们就可以去计算它之间的这样的一个差值，那这个值呢我们把它称之为loss，loss值啊，也就是说我们的预测值和我们的这个真实值之，间的这样的一个差距有多大，那我们的目标是什么。

我们的目标实际上就是要把这个差距缩小对吧，我们要把这个差距缩小好，那把这个差距缩小，我们就等于是在去，怎么才能把我们的差距缩小呢，说白了就是要去让我们的这个参数变得，越来越好对吧。

越来越适合我们这样的一个场景，我们可以回到刚才那个例子啊，假设我们要让我们的这个房价预测的特别准，那肯定也得保证我们的这个W对吧，我们这个W尽可能的让我们的这些样本点，和我们真实的这个结果。

他的这个差距最小，那才是最合适的对吧，所以说啊我们就需要不停的去把这个最合适的，或者说最优的这个W给求解出来啊，给求解出来，那这个求解的过程呢，我们通常采用的是梯度下降法，梯度下降法好。

那接下来的话我们就分别来看一下，我们的这个loss和我们的这个啊，梯度下降法到底是怎么个回事啊，怎么回事，嗯好等一下啊，我们在看这个loss和梯度之前呢，我们先把咱们这个前项的这个流程啊。

我们再简单梳理一下，我们刚才这里说到说哎，我们这里会输出一个这样的一个值对吧，我们这个这里的话是输出了一个零，那这里这个零表示的是什么意思，或者说我每次输出的时候，我到底应该输出什么东西。

好我们先看这个问题啊，我们先看输出，再看loss，再看我们的这个梯度下降法，好吧好，我们先看输出，首先呢我们这边会有张图片，我们把图片啊转换成这样的一个数字，那一张图片的话放到咱们的计算机当中。

它实际上就是一个0~255之间的，这样的一些啊，数字的组成的这样的一个张量对吧，OK那我们可以把咱们这些值啊，首先呢做一个这样的一个归一化的一个处理，很简单啊，就把每一个值都除以255就OK了。

然后呢我们把我们每个这样的一个输入值啊，给到我们这样的一个神经网络，神经网络，那这个神经网络网络呢你可以去设置多层，也可以设置为一层，你也可以用刚才说的卷积神经网络也好，或者其他神经网络都是OK的好。

那这里啊，假设我们中间已经经过了一系列的一，些神经网络，最后呢我们这边又经过了这样的一层，普通的一个神经网络，最后我们又加上了这样的一个激活函数啊，激活函数，那这里呢我们采用的是一个叫做SIGMOID。

激活函数，那这个激活函数它有什么作用呢，它是这样子啊，这个激活函数它是这个样子的，我们简单画一下啊，简单画一下，嗯这是咱们的一个0。5，0。5，他大概是这个样子啊，大概是这个样子。

也就是说如果你的这个值啊，他是就是我当我们把这个值啊，输入给咱们的这个激活函数的时候，它可以输出一个0~1之间的一个值啊，这个最大值是一，最大值是一，最小值的话就是咱们的一个零。

它可以输出一个0~1之间的一个值，那通常啊假设我们要去判断这张图片，它是不是猫，那我们就可以把我们神经网络的输出结果，给到这样的一个SIGMMOID的一个激活函数。

那这个激活函数如果它的一个输出值大于零，那这个输出值啊它大于零给到我们奇偶函数，他就会输出一个大于0。5的一个值对吧，那大于0。5我们就认为它属于猫，如果呢他这个神经网络它的一个输出值。

它小于零小于零的，那给到我们的激活函数，它输出的值就是一个小于0。5的，这个时候我们就认为它不是猫，那在我们这个场景当中啊，假设我们最终的结果是0。75，0。75哦，0。730。73的话，它是大于0。

5的对吧，所以最终呢我们就认为啊，这张图片它实际上就是一只猫，我们就以这样的一个模式啊，来得到我们的一个输出值好吧，输出值好，我们再简单总那个总结一下啊，输入一张图片给到我们的神经网络。

把神经网络的一个输出值给到我们的SIGMOID，激活函数，激活函数根据它的一个输出值是否大于0。5，来决定它是否是猫，如果大于0。5，我们认为它是锚，如果小于0。5，我们就认为它不是锚好。

这就是一个很简单的一个所谓的一个二分类啊，二分类判断一张图片是不是毛好，这个时候可能会有同学说，那假设我现在不是想去做阿凡雷，我并不是说我要去判断这张图片是不是猫，而是我想知道这张图片它是什么动物。

假设啊，假设呢我们这边嗯我们有这样几个类别啊，可能有啊，有狗有猫，还有咱们的这个啊长颈鹿，OK我想判断一下我当前这张图片，它到底是狗还是猫还是长颈鹿，那怎么办呢，有一种方案啊。

就是说我们可以把我们的这个数，这个神经网络啊，我们可以搞三个节点对吧，刚才的这个神经网络呢，我们这边实际上就一个节点对吧，那实际上呢我们可以搞三个节点啊，三个节点可以看到这里呢。

实际上就分别是三个节点啊，那每一个节点的一个值啊，我们都会经过这样的一个SIGMOID，都会经过这样的一个SIGMOID，那每一个SIGMOID呢，它都会输出一个0~1之间的这样的一个值，对吧好。

那么我们还是和刚才一样，假设对于第一个类别，哎他这里实际上想知道是是否是dog对吧，OK那他如果他输出一个0。12，它是小于0。5的，那我们就认为他不是狗这个类别，OK这边0。73他输出是大于0。5的。

那我们就认为它是猫的这个类别，那如果啊他是一个啊，他最后这个是0。04，它是小于0。5的，那我们就认为他不是这样的一个长颈鹿，那我们最终的结论就是A他是猫，他是猫，那这样的一个方法啊。

这样的一个输出的一个方法，还有一个优势啊，还有一个优势，假设你的这张图片里面既有猫又有狗对吧，那这个时候你去进行输出的时候，你就会发现诶，可能这里它输出的是一个0。6对吧，它是大于0。5的。

那这个时候啊，你就会也这张这张图片的一个输出值啊，实际上就也包含dog对吧，那这就是一种多分类的一种一种方法啊，多分类的方法，并且啊它还支持咱们所谓的一个多标签，也就是说我这张图片不单可以进行多分类。

还可以得到多个这样的一个输出的一个标签值，好吧，好嗯，刚才咱们这里只给了一个节点对吧，那如果我们想让我们的模型的这个，收敛能力更强一些啊，我们就可以去加一些所谓的hidden layer啊。

hidden layer在我们的这个节点之前呢，我们再去加一些其他的一些层对吧，可以加一个中间层啊，中间层，那这个中间层呢我们就可以继续啊，也是给他一些所谓的一些激活函数好吧。

例如你可以在这一层啊给他加上一个啊，ten h奇偶函数，或者是value的这样的一些奇偶函数，当然这里用SIGMOID的啊，也是可以的也是可以的好吧，好那如果啊我们这个hidden layer。

如果我们这个hidden layer啊很大，它可能等于很多层啊，很多层可能十层20层都是OK的啊，那当我们这个层数比较深的时候呢，我们又称我们的神经网络为深度神经网络，也就是咱们所谓的深度学习，好吧。

这就是我们的深度学习啊，深度学习深度神经网络，好那我们继续往下啊，继续往下嗯，这是咱们刚才看到的啊，整个这样的一个流程，咱们的一个猫对吧，给到了咱们这个单层的神经网络，然后经过了我们的SIGMOID。

最终做了这样的一个二分类啊，二分类好，那这个是我们的第一步啊，也就是拿到我们的一个输出结果对吧，拿到我们的输出结果，那接下来我们要考虑什么，刚才说的我们要去计算我们的这个loss对吧。

我们的这个输出结果，我们希望它和我们的真实的目标是，越接近越好啊，越接近越好，OK接下来的话我们就来考虑一下这个loss，Function loss function，好啊，我们考虑一个问题啊。

考虑一个问题，对于我们这里的这个嗯这个猫的这个小例子啊，我们实际上是什么，是一个二分类对吧，二分类好，那我们就以二分类来举例啊，我们以二分类来举例，假设啊，呃假设我们现在有这样的一个。

我们要求解这样的一个概率值啊，就PY等于1given x好，那假如这个值呢我们用啊y hat来表示，也就是说什么意思呢，就是说我给定我这张图片，X表示我们的图片，我给定我这张图片，那它是猫的一个概率。

我们用y hat来表示好，那对于它不是猫的一个概率，也就是PY等于零，Given x，那这种时候是不是就是咱们的一减掉y hat，对吧，这就是我们是猫的一个概率，和不是猫的一个概率，OK那这个东西啊。

实际上我们可以把它做一个这样的一个合并啊，简单做这样的一个合并，合并完之后呢，它实际上就是啊YH的Y次方，再乘以咱们的额一减y hat的一键Y次方，那为什么是这样的一个公式呢，我们可以这么看啊。

我们把这个公式给带进去看一下，假设我们Y等于一，Y等于一的话，我们看一下啊，哦Y等于一，Y等于一，那后面这一项实际上就是一减y hat的零次方，那就是一对吧，那我们最终的这个结果就是y hat。

那如果Y等于零，Y等于零，Y等于零的话，那前面这一项就是一对吧，那我们最终的结果就是一个啊一减掉y hat，那这边是零的话，那1-0的话就是一对吧，那这个东西和这个东西。

是不是就是咱们的前面的这个结果对吧，所以呢我们是可以把我们的这样的一个结果啊，用这样的就把这两个式子啊，用这样的一个式子来表示，那通常啊这个式子呢，实际上就是我们需要去求解的一个目标，那有这个式子呢。

也称为这个所谓的一个极大似然估计啊，极大似然估计，那通常呢我们会对这个式子啊去做一些处理，就取一个log，取个log，取log，它有什么优势呢，它可以让我们的这个乘法啊，变成这样的一个加法的一个形式。

那最终的话这里实际上就变成了一个Y乘以log，咱们的y hat要加上咱们的一减Y乘以，咱们的这个啊log1减y head对吧，这就是咱们的一个啊自然函数啊，自然函数，那我们的目的是什么呢。

我们肯定是希望我们的这个值它越大越好对吧，我们肯定是我们先看这里来啊，我们先看这里，我们肯定是希望对于这样的一个概率值，它越大越好嘛对吧，当然我Y输入X的时候，Y等于一的这个值我需要让它越大越好。

那X输入XY等于零的时候，这个概率值越大越好对吧，那所以啊，我们的目标就是让这个东西它越大越好，越大越好，我们要让他大啊，需要让它大，但是我们这里需要考虑一点啊，我们需要考虑一点。

如果我们要让它越大越好的话，实际上他是没有上限的对吧，他是没有上限的，那我们在反过来考虑这个问题，如果我们把它让它变得越越小呢，我们是不是就可以让它，尽可能的把这个值给减小到接近零，那如果它接近于零了。

我们就认为它是好的，所以啊我们就把我们的这个自然函数啊，进行了进一步的一个处理，也就是最终就得到了我们的这个loss function，那我们这个loss function呢，我们的y y head。

就等于啊在前面加上了这样的一个负号啊，在前面加上这样一个负号，就Y乘以log y hat，这里的话也是继续加上我们的一减Y乘以，咱们的log1减y at。

那这个就是我们最终的一个loss function，因为我们在前面加了一个负号，所以呢我们最终就变成了要让他越小越好，越小越好，那越小这个过程呢实际上就是要让它接近于零，接近于零好吧，接近于零好。

那这是我们这个啊，单条数据的一个情况我们考虑一下啊，那通常我们在训练的过程当中，我们肯定不可能就输入一条数据嘛对吧，我们肯定是要输入很多条这样的一个数据对吧，所以呢我们可以再把这里啊再展开一下啊。

所以我们最终啊最终的这个loss function，实际上啊就等于咱们的一个负的，M分之1summation，从I到M，咱们里面呢就是每一条这样的一个每一条，这样的一个每一条这样的一个值啊，一条一条的。

这里的话是咱们的IOK额，再加上咱们的一减YI和乘以咱们的log，一减y hi，这个啊就是咱们最终的哎这里漏了，最终的一个loss function啊，Loss function，这里这个M呢表示。

就是我们的一个数据的一个量啊，数据量，假设你这里有100条数据，那你应该考虑的是，把100条数据的这个loss function都要算出来对吧，loss function的指标给算出来。

但是我们肯定不能求和嘛，你肯定得取平均，因为求和的话，你100条的这个loss肯定是没有1000条的，这个loss多的，我们应该保证这个loss是在，同一个这样的一个级别的对吧，所以呢。

我们通常要是要取一个这样的一个均值啊，均值，这就是我们求解我们的这个loss的一个过程，好吧，二分类求解loss的一个过程啊，求解loss的一个过程好，那讲完了咱们这个loss的求解之后呢。

我们再来考虑一个问题啊，那接下来我们该怎么去更新我们的这个参数呢，嗯我们有哪些参数呢，哦我们再考虑一下啊，对于我们的这里这个Y啊，哎对于我们这里这个y hat，我们这里先用好，我们就用啊。

我用y hat表示吧，就啊我我就用Y表示，我就用Y表示，那我们这个Y它实际上就等于咱们的一个啊，SIGMOID也是咱们的一个激活函数对吧，然后里面就是咱们的这个权重和我们的输入的，这个特征。

加上我们的一个偏移项好，这是我们那个Y那对于我们的这个激活函数，激活函数啊，它实际上等于是的是1÷1，加上一的负的次方啊，负ZA次方好，这是我们的这个前向传播的一个过程啊。

那我们刚才也得到了我们这样的一个啊，loss function对吧，Loss function ok，那梯度下降它是在做什么呢，它实际上是这样子啊，假设我们现在的这个loss function。

它是这个样子的，假设它是这个样子的，那我们是不是希望让我们的这个loss，尽可能小对吧，尽可能小，那肯定希望诶我这个loss如果能到这个位置，那对于我们来说。

就等于是拿到了一个非常好的这样的一个loss，吧对吧，那这个时候的loss对应的这个权重啊，也就是W就是我们想要的这样的一个W，但是这样的一个值我们或者说这样的一个loss，我们整怎样才能得到呢。

这就是需要涉及到我们的这样的一个，梯度下降法啊，梯度下降法，也就是说在每一次去更新的一个过程中啊，我们首先会计算得到这样的一个loss的，这样的一个值对吧，那有了这样的一个呃，loss这样一个值之后呢。

我们呢就可以去根据我们的这个loss啊，去求解我们对应的权重的一个梯度，也就是说我们要去求解，我们的这个W的一个梯度，W的一个梯度，那这个梯度实际上就等于对我们的这个啊loss，去进行求导就OK了对吧。

进行对LOS进行求导好，那求出来我们的这个梯度之后，那那那梯度是什么呢，举个例子啊，假设我们要求的是这个点的一个梯度，它实际上表示的就是它距离，就是它的这个点的这样的一个切线对吧，它的这样的一个切线。

那这个切线的一个位置呢是距离啊，咱们最优点，它下降过程最快的一个方向，所以呢它实际上就是指引了，或者说告诉了我们的这个啊权重啊，你接下来应该往哪个方向走对吧，接下来要往哪个方向走。

那好你OK那接下来我告诉你要往这个方向走，那关键啊你要走多少，这个时候啊我们就需要引入另外一个概念，叫做学习率啊，学习率，那通常啊我们更新我们每一个权重的时候啊，W实际上是等于W乘以这样的一个啊。

贝塔再乘以咱们的这个W的一个权重啊，这是减号啊，减号啊，这里这个贝塔呢表示就是咱们的这个learning rate啊，学习率learning rate，学习力这个学习力就决定了你要走多少步。

就如果你这个学习率设置的比较大对吧，那他就可能往下面走很大一步，那如果你这个学习力设计设计的比较小，他可能就只会走一小步，走一小步啊，这个有什么区别呢，我们这边再画一个图啊，再画一个图。

如果你的学习率特别大，他可能最后变成什么情况呢，他可能就变成这样子情况，这样子的情况，他是这样子走的，那如果你的学习率比较小呢，他可能就是每次就只走一小步啊，每次就只走一小步，学习力比较大的话。

其实啊他不容易收敛到一个嗯比较合适的，或者说loss值比较小的一个点的一个情况，他可能收敛的没有，咱们这个学习力比较小的时候效果那么好，但是学习越大的话，能让我们收敛的速度变快啊。

能让我们的收敛的速度变快好，那这整个过程呢就是咱们的这个梯度下降法啊，梯度下降法好，我们再把整个流程梳理一遍啊，我们再把整个流程梳理一遍，首先呢第一步，我们是有咱们的这样的一个输入值对吧。

有这样的一个输入值，然后给到我们的模型model，model啊，给到我们的模型之后呢，我们拿到我们的这样的一个输出值对吧，out也就是咱们这样的一个结结果对吧，输入值给到我们的模型，拿到我们的输出值好。

有了我们的输出值之后呢，第二步我们就可以根据我们真实的label对吧，真实的label，真实的label，还有我们的这个输出值啊，还有我们的输出值，我这里用pd来表示这两个值。

我们就可以去计算我们的loss，计算我们的loss好，有了loss之后呢，我们可以采用我们的这个梯度下降法对吧，梯度下降法来得到我们的这样的一个梯度啊，来得到我们的一个梯度，来进行我们的反向传播。

反向传播，那反向传播的过程中呢，也就是咱们的这个执行梯度下降法的，这个过程啊，那最终呢我们就会去更新我们的这个参数啊，更新我们的参数好，那参数更新好了呢，我们要继续这个循环啊。

继续这个循环又继续走到第一步，好把X给到我们的模型，再得到输出值，好在有了输出值之后呢，再把我们的预测值和我们的标签源去计算loss，而再继续进行BP又更新我们的这样的一个权重。

那这个过程呢实际上就是一个持续循环的一个，递归的这样的一个过程对吧，直到啊，直到我们认为哎这个W已经收敛的不错了，或者说这个loss我们认为他已经足够小了，那我们就认为整个过训训练的过程啊。

我们就结束了好吧，这就是我们整个咱们的这个神经网络啊，神经网络在进行训练过程的一个流程啊，一个流程，好那说完了神经网络这一块呢，那咱们接下来的话，就来进入我们的这个第二部分啊。

我们要进入我们的这个第二部分，接下来我们来看一下，我们的这个循环神经网络啊，循环神经网络，啊我们在看循环神经网络呢之前呢，我们先来看一下，左边这个普通的一个神经网络啊。

左边这个呢是一个非常普通的一个神经网络啊，那我们先看一下啊，对于H来说，对于中间这个H啊，它实际上等于的是嗯，咱们的这个U我去先写个奇偶函数吧，我们用西格玛表示奇偶函数，然后U乘以X加上B对吧。

这是我们的一个XH的这样的一个值，那我们这里还有个输出值好，那这个输出值呢实际上也是咱们的sigma好，V乘以H加上咱们的B对吧，好，这是我们的这样的一个普通的一个神经网络啊，那循环神经网络是什么呢。

循环神经网络啊，实际上就是在中间这里啊，去加上了这样的一个环，加上了这样的一个环，可以看到啊，他这里额外多了一条路线，而这里有这样的一个W的一个权重，那有了这个W的权重之后呢，我们再来看一下这个H。

那这个H实际上啊除了咱们的这个U和X对吧，实际上还要再加上一下对吧，还要再加上一下，那这一项呢，哎这也是这一项是什么东西呢对吧，它好像还要和咱们这个W进行相乘对吧，然后和W进行相乘。

然后再最后再加上我们的偏移项，我们才能得到我们的H，OK那接下来我们就来看一下，这里到底应该输入的是什么，是什么，好我们来看一下我们呢在这里啊，把这个环给展开给展开，展开之后呢。

我们实际上会得到这样的一个图啊，OK接下来呢我们就要引入一个概念啊，一个概念叫做时间点，时间点我们用T来表示啊，用T来表示好，我们先看这里啊，这里是XTXT的意思，就是说在T时刻我输入的是XT。

那我上一个时时刻点我输入的是XT减一对吧，那下一个时刻点的话，我输入的就是XT加一，OK那对于我这里这个HT来说，它实际上就包含两个部分嘛，好我们在这里看一下啊，那这里这个HT。

它实际上就是咱们的这个U乘以XT，再加上咱们的这边过来的一个值对吧，这边过来的值，这边过来的值是什么，是HT减一乘以咱们的W，然后再加上我们的激活函数，还有我们的偏移项，就得到了我们HT的一个结果。

好那这个HT减一又是怎么得到的呢，那这里这个HT减一啊，实际上就等于U乘以XT减一，加上HT减二乘以W，加上B加上B也就是说这里这个HT减一啊，它实际上是根据是这个时刻的输入值。

还有上一个时刻的HT减二乘以W来得到的，来得到的，所以啊咱们这里的关键就在于这里的这个HT，和咱们的这个W，那大家可以发现一个问题啊，咱们这里所有的这个W啊，还有咱们这些UV，它其实权重都是共享的啊。

可以发现它其实都是同一个值对吧，它是同一个值，那这些权重啊它是共享的好吧，这些权重是共享的，它唯一的不同点就在于它输入不同，它输入不同，那为什么说RNN适合用在文本当中呢，大家考虑一下我们的文本对吧。

实际上它也是带有这个所谓的一个顺序属性的，或者说时间属性的，它是从左往右进行这样的一个输入的对吧，例如我爱自然语言处理，那它就是按顺序进行这样的一个输入的对吧，按顺序进行这样的一个输入的。

所以啊我们通常也会使用这个RN来处理文本，就是因为它自带的这个所谓的一个顺序的，一个属性，或者说位置的一个属性，也就是说它自带了个所谓的一个position，Position。

position的这样的一个属性啊，好那这个时候可能又有同学会问了，那既然如此，我一开始这个HT减一怎么办呢，是这样子啊，其实一开始啊，一开始的时候咱们这个HT减一呢。

你可以是随机给这样的一个值就OK了，好吧，随机给这样的一个值，好这就是我们的这个最基础的一个RN好吧，最基础的一个RN，但是呢其实大家也可以发现啊，现在RN其实用的并不多，更多的还是这个咱们的这个呃。

呃ISTM这一块啊啊我们先看这里吧，我们先看这里好，我们先看这里，那这里的话给大家介绍了这个RN，那RN的话，它其实会有很多种不同的一个输入和输出啊，我们先一个一个来看，那假设我们只有一个输入。

一个输出对吧，那这种情况，实际上就是我们普通的一个神经网络啊，它就是一个普通的神经网络，不同的一个神经网络好，那假如我们这个RN它有一个输入以多个输出呢，我们一个输入多个输出，那这种情况实际上是什么。

你可以简单的理解啊，它就类似于在做一些所谓的一个，续写的一个处理啊，就说哎我输入了某个这样的一个啊标题对吧，你去帮我把后文给续写出来，续写啊续写，然后我们再来看一下这个many to one。

就说我输入多个值，但是我只输出一个值，这种情况呢就是最典型的啊，就是咱们的一个文本分类对吧，我把我整段文本输入进去，你最终帮我输出这个文本，到底属于什么样的一个类别啊，文本分类好，我们再来看一下这个啊。

Many to many，那这种情况呢它其实是有一个错位的啊，他会有一个错位的，那这种情况大家可以看成A，类似于他做一些机器翻译，或者说文本摘要啊，文本摘要就说我这边输入的是一个中文对吧。

CHINESE这边输出的这样的一个ENGLISH，好翻译，那最终呢最后呢我们再来看一下这个啊，Many too many，这个many to many，和这边这个唯一的区别就在于。

它这个位置是一一对应的对吧，这边的话位置它是错开的，那这种一一对应的话，其实应用场景也很多啊，就是咱们的N1啊实体识别啊，实体识别或者说词性标注，甚至说分词对吧。

这都是一个many to many的一个结构啊，好，这就是我们RN经常遇到的一些不同的这种啊，输入输出的一些格式啊，大家其实也可以发现啊，这个RN他可以做的这样的一些，基于文本的任务很多对吧。

很多啊很多不同的一个输入输出的话，他做的这样的一个任务，他也是不一样的啊，不一样的，OK那接下来我们再来看一个概念啊，就大家其实也可以发现啊，我们其实RN用的并不多，用的多的其实还是RN的一些变体。

就像LSTM啊，或者说GRU啊这样的一些模型对吧，这些模型我们用的会更多一些，那为什么呢，就是因为啊RN它有一个非常大的一个缺点，就是它非常容易出现这个，所谓的一个梯度消失啊。

容易出现这个所谓的一个梯度消失，为什么会容易出现一个梯度消失呢，我们这边给大家详细解释一下啊，详细解释一下，当然啊这里这个问题啊，其实也是面试当中会经常问到的一个点啊，所以大家这里要哦注意听，好吧好。

那假设我们现在有啊三个时时间点啊，第一个时间点呃，第二个时间点，第三个时间点，那第一个时间点呢我们输入的是这个X1，第二个时间点输入的是X2，第三个时间点我们输入的是X3，好。

那这边呢会有输入一个H0啊，这边是咱们的一个W，而这里呢我们就可以得到我们的H1对吧，好然后进行一个输出输出的话，我们啊这里用用O1表示吧，好这是咱们的一个O1OK好，对于第二个节点。

我们可以等拿到hr，然后输出结果的话就是我们的OR，最后的话是我们的这个拿到我们的H3好，这边的话这个节点输出的是我们的O3O3呃，我们中间的权重呢我们用W表示啊，然后下面这一层呢我们用的权重。

我们用U来表示上面这层权重，我们用V来表示啊，这里大家我再重复说一遍啊，咱们这里这个UV还有W他对于每一个时刻点，他的这个权重啊，它是共享的好吧，权重是共享的，权重是共享的好。

那接下来呢我们先去看一下啊，他的一个啊求解loss的一个过程，它是什么样子的，也就是所谓的这个前向传播的一个过程，它是什么样子的，好我们先看我们一步一步来啊，我们先看这里这个H1，那这里这个H1。

实际上就等于咱们的这个啊激活函数对吧，哦还是用西格玛来表示啊，然后X1乘以我们的U加上我们的H0，乘以咱们的一个W，然后还有咱们这样的一个啊偏移项，偏移项我这里就不写了啊，我们就先啊简化一下。

我们就不写了，好这是我们的H1的一个值，然后我们再看一下O1，那我们的O1的值是不是也是一个这样的一个，激活函数对吧，然后H1乘以咱们的一个V好，这是我们的H1和O1，我们再来看一下我们的H2。

H2的话，实际上也是咱们的一个计划函数，X2U加上H1，然后W对吧，那对应的我们的O2，就等于西格玛H2乘以我们的V，然后是我们的H3H3还是一样啊，西格玛X3U加上哦H2W，然后这里是我们的O3对吧。

O3O3的话，西格玛H3乘以V2，这是我们的一个前向传播的一个值啊，前向传播的一个值，Ok，那最终的话啊假设我们这里是一个假设，我们是一个many too many的一个过程。

好吧哦many too many的一个过程，那对于也就是说假设我们现在在最后一个吧，我们以N12举例子好吧，我们以N12举例子好，那以N12举例子的话，那这里我们实际上可以得到L1，这样的一个loss。

这里也可以得到第二个loss，这里可以得到第三个loss对吧，第三个loss，那我们最终的这个loss呢实际上就是，啊咱们的这个13扫描神I从1~3对吧，I从1~3，然后LI好。

这就是我们最终的一个lost的一个值啊，最终的一个lost的一个值，OK那有了我们的loss之后呢，我们下一步是要去计算我们的这个梯度对吧，我们要去计算我们的梯度，好我们来看一下我们梯度啊。

我们呃我们取这个L3啊，我们来算L3的一个这个loss，它对应的咱们这个U和V的这样的一个梯度啊，这样的一个梯度为什么要取L3，不取L1和L2呢，大家看完就知道了，好吧好，我们fl3。

我们对咱们的这个我们先对V求导啊，我们先对V求导，也是对这个对V求导，那对V求导呢，首先第一步是不是要先对我们的O3求导对吧，YO3法O3求导，然后呢，我们的法O3再对我们的这个FV进行求导。

这样的话我们就可以拿到这里，这个V这里是U啊，好这里的都标一下，这里是V这里是U好WW好，这样的话我们就可以拿到我们这里，这个V的这样的一个啊梯度对吧，就是L3对应的这个V他的一个梯度啊，他的一个梯度。

OK这里很简单啊，这里很简单，就是一个非常简单的一个链式，求导的一个过程啊，非常简单好，那V求导简单，但是对于U求导来说，它就有点麻烦了，为什么这么说呢，我们来看一下啊。

我们five l3对于Y这个FU求导，它实际上首先是fl3对于FO3求导对吧，呃然后呢再加上我们的YO3，对我们的FH3进行求导对吧，那这里求解完之后呢，我们还得这个five h3。

对我们的five这个法U进行求导对吧，法U进行求导好，这样的话我们就把U的一个导数给求出来了，但实际上我们并没有求九啊，为什么呢，我们换个颜色啊，我们这里求出来的导数，只是这一条路径上的对吧。

只是这一条路径上的，但实际上啊我们这边还有一些U啊，我们这里也还有U对吧，这些U对我们最终的这个L3的一个结果，都有一定的一些影响，所以我们的我们在求解U的梯度的时候，还得去考虑这些路径上的一些优哎。

但是有同学就会说哎，那为什么我这里这个V就只求解这一个地方呢，因为这些地方的V，它只对这里的L1和L2造成影响，它并没有对这里的L23造成影响，所以我们求V的梯度的时候，只需要求解这个V的梯度就OK了。

好吧，所以啊当我们求U的梯度的时候呢，我们现在这里啊只是第一个U的一个梯度，我们还得去求第二个时刻和第一个时刻对应的，这个U的梯度好，那我们就继续加嘛，OK前面的话就还是fl3。

five o3还是一样啊，前面是一样的，这里是five o3，five h3好，到这里就会有点不一样了，那接下来的话我们实际上是要走，这边就要往回走一步了对吧，往回走的话。

那就是five h3对FH2进行求导，那到这里才是法H2对FU就行求导对吧好，这是第二个时刻的U的梯度，接下来我们看第一个时刻啊，第一个时刻其实也是一样啊，fl3FO3乘以FO3，Y h3，Y h3。

Y h2，最后这就是法H2对法H1进行求导，然后法H1再对我们的法U来进行求导啊，进行求导，那这到这里呢，我们整个求导的一个过程啊才结束了结束了，所以说啊对于咱们的这个过程，它其实还是挺复杂的。

挺麻烦的对吧好，那接下来我们把他的一些这一块啊，我们可以看一下，就这一块啊，这里它实际上会有一些这样的一个重复项对吧，这一项它是一个重复的，就这一项他也是这样的一个重复项，那这两项呢取决于什么。

取决于你的序列的一个长度对吧，假设你的序列长度越长，那你在求解这个U的梯度的时候，你中间的这一项它就越长对吧啊，那这里我们可以简单做一个合并啊，合并那合并完之后呢。

最终咱们的这个phi LT对FIU进行求导之后，他的一个梯度实际上就是法LT除以FUT，再乘以我们的five ut除以咱们的法，这个通过48小时核酸哎，等一下啊，这里写错了啊，这里不是U这里是啊。

我们的O啊，这是我们的OOT这是O写错了，反OT在对我们的这个啊HT进行求导对吧，HT求导进行求导啊，这是我们前面这一项嘛，这是我们的前面这一项对吧，那关键是中间这一项。

那中间这一项我们实际上啊就可以去对他这个，求基好，那这里的话我们用下标，用J来表示啊，我们从J道题那里面呢，实际上就是咱们的这个FHJ，再对我们的FHJ减一，进行这样的一个求导对吧。

这个呢就是我们中间这一块啊，中间这一块，但是啊这里大家需要考虑一下，我们这里实际上是什么，是有三个部分吗，是有三个部分对吧，我们这里是有三个部分的，那三个部分的话，那我们。

前面这里前面这一块我们是可以提取出来的嘛，对吧，前面这一块我们可以提取出来放到前面吧，所以呢后面这一块啊，我们实际上还有一个相加的一个过程对吧，所以我们这里还有个相加的一个过程啊。

所以这里啊我们还有这样的一个扫描声，这里还有个扫描声，然后我们这里用啊用K来表示啊，K从一到咱们的一个T，那这里这个J呢，实际上是从咱们的K加一开始啊，K加一开始，那这一项呢就是咱们对应的。

咱们蓝色框里的这一项对吧，然后最后是咱们的这个啊，最后这一项啊，最后这一项，那最后这一项呢实际上就是咱们的这个FHK，再对咱们的FU进行求导就OK了啊，就OK了，那这个过程就是我们最终的一个这个哎。

这里写错了啊，这里是怎么回事啊，这个地方写错了，这个地方就是FU对吧，我在我这写的啥，L对U求导，T对OO对HT好，这就是我们最终的一个梯度啊，这是我们最终的一个梯度，OK那看到这里之后呢。

我们来考虑一下，那为什么说RN容易梯出现梯度消失呢，我们看一下这里啊，我们看一下这一块，这一块是HJHJ对HJ减一求导，也就是说他是在对这个链路上进行一个求导，那这个链路他做了什么事情呢。

它其实就是一个激活函数对吧，是这样的，西格玛我们可以看到啊，他这里会有这样的一个西格玛，那在我们的RN或者说呃在我们RN当中啊，我们通常使用的一个激活函数，要么是这个SIGMOID。

要么是这个ten h啊，那这两个激活函数，它有它有什么样的一个问题呢，我们可以再简单回顾一下那个SIGMOID激活函数，它什么样子，他是大概这个样子的对吧，大概是这个样子的诶，画的不太好啊，这是0。

5这个奇偶函数，大家可以看一下啊，他的这个梯度啊，它是很容易就是说他这个梯度，它容易很容易出现一个比较小的这样一个值，对吧，他这个梯度也是一个0~1之间的，这样的一个值，它梯度很小。

梯度很小好OK那假设我现在这个梯度，我现在这个梯度假设啊求解出来它是0。1，0。1，那我们再假设我们这这个这里，这个T也就是说这里是个T啊，假设T等于一五十，T等于50的意思。

就是说我这个序列长度是50对吧，那我们这里这里就我们这里啊，这一块求解出来的梯度就是0。1的50次方，50次方这个东西是不是就趋近于零对吧，那你的梯度就会变得很小，你梯度变得很小，会出现什么问题呢。

梯度变得很小，你咱们刚才这里说的对吧，你每一次这个值都特别的小，特别小，特别现在已经接近于零了，对吧，那就等于你这个W就很难去更新，你每次虽然就算你更新100次，1000次。

但是你每一次的值基本上都是零，就等于你这个W没有怎么更新，所以你模型就会很难收敛啊，你的模型就会很难收敛好吧，这就是RNN当中的一个所谓的一个，梯度消失的一个问题啊，RN当中的梯度消失的一个问题。

那怎么解决这个问题呢，或者说怎么有没有什么办法能缓解这个问题呢，这就是我们接下来要给大家讲的这个LSTM，ASTMASTM呢，又被称为这个所谓的长短期记忆网络啊，它能有效地去学习一些长期依赖的关系啊。

也能有效的去啊缓解梯度消失的问题，这里不能说解决啊，这里我改一下，这里应该说缓解缓解梯度消失的问题好，那接下来我们就给大家介绍一下，这个LSTM啊，给大家介绍一下LSTM，啊第一个图呢是咱们的这个啊。

RN下面这个是我们的LSTM可以看到啊，RN的结构还是非常简单的，但是对于LSTM来说，里面其实有很多这样的一些计划函数对吧，这里有SABOID，有ten h，这里这个西格玛表示就是这样的一个西格玛啊。

西格玛id啊，这里我也简单说一下，哎，有同学会说这个torch，这个激活函数是什么东西啊，我这里也简单画个图啊，咱们SIGGMOID，它是，所以咱们SIGMOID是啊。

在这个都是大于零的一个这样的一个值对吧，但是tan值呢它其实是关于原点对称的啊，他是这样子的，它是这样子的，它是它是经过原点的啊，画的不太好，他是经过原点的，那C格MOID的话。

它是全都是这样的一个大于零的一个值啊，好那接下来的话我们就把这个LSTM啊，展开详细看一下啊，我们展开详细看一下，首先呢LSTM这个模型啊，它额外添加了一个叫做细胞状态的一个东西，它是用C来表示啊。

在我们的RN当中，我们只有一个H对吧，用H那我们通常称这个H叫做hidden state，隐藏状态啊，隐藏状态，那在我们的这个LSTM当中多加了一个cell state，细胞状态啊。

细胞状态我们这里用C来表示它，其实hidden state也有对吧，在下面这里啊，在下面这里，那这里这个cl state呢，就是这个图当中的上面这一条线啊，上面的向右的表示的就是sales state。

它传递的一个过程，那这个sales state什么东西呢，我们详细来看一下啊，详细来看一下好，我们先在先看这个sales state之前呢，我们先来了解几个概念啊。

几个概念叫做门LSTM一个比较关键的东西，就是它有一个所谓的一个门的一个机制啊，门的一个机制，这个机制是什么意思呢，我们来看一下啊，就是说LSTM啊，它能决定我当前哪些值需要保留，哪些值不需要保留。

我们可以先看一下下面这里这个例子啊，我们可以先看一个例子，就是说假如我现在有句话叫做他今天有事，所以我当处理到我这个字的时候，我们实际上希望看到的是说，我们这个模型能把我之前的这个主语。

它就是1万了就不要了对吧，1万了不要了，要保留我对吧对吧，这个时候我们应该更加关注的是我，而不是他那1万门，也是我们LSTM当中的第一步啊，也就是这一块它的作用就是把它这个字给忘了，他是怎么忘的呢。

我们看一下啊，首先呢这是我们的上一个时刻的hidden state HT，是我们当前时刻的一个输入值，我们把当前时刻的一个输入值，和我们上一时刻的这个hidden state啊，进行这样的一个拼接。

拼接完之后呢还是一样啊，会有这样的一个权重进行相乘，要加上我们的偏移项，然后经过SIGMOID的层，刚才说了SIGMMOID的层，他是输出一个零一之间的一个值对吧，它输出的是一个零一之间的一个值好。

说白了就是说我这里我这里啊，就是输出这样的一个零一之间的这个值，那如果这个值它接近于一，它接近于一，意思就是说我会把之前的内容保存下来，如果接近于零，就等于是把之前的内容完全就给遗弃了。

那如果他是一个0。20。6，意思就是说我之前的值要保保留20%，保留60%，就这样的一个思路，好这是我们的一个遗忘门啊，遗忘门，那接下来我们再来看我们的一个输入门，那刚才我们把一部分内容给遗忘了对吧。

但是上文的其他内容，我们还是得留一部分下来嘛对吧，我们要留，把留下来的内容作为我们的一个输入对吧，作为我们的一个输入好，那怎么作为输入呢，其实也很简单啊，还是把刚才的hidden state和当前时刻。

我们的这个输入值啊，先作为拼接，然后给到我们对应的一个权重，好这里大家可以发现诶，我好像计算了两个部分咳，第一个部分和刚才的一样，他也是一个门，因为他是一个SIGMMOID的。

他还是输出一个零一之间的一个值，那对于第二部分，我们经过的是ten h这样的一个激活函数，大家可以理解为，我把输入和上一时刻的hidden seat，去做了一些非线性的一些变换，然后呢我就得到了。

再把这两个部分啊，会去做一个所谓的一个相乘的一个操作，可以看到这里我做了一个相乘的一个操作，也就是这里啊IT乘以CT德尔塔，那这里这个相乘实际上表示的是什么，就是我ct是我当前时刻的一些输入值对吧。

我这里这个i it是这样的一个门，比如说当前这些东西，我哪些东西要保留下来对吧，这个就是我们当前要表保留下来的一些值啊，当前保留下来的一些值好，我们再看这一块，这里是FT乘以T减一。

CT减一是上一时刻的sale state，ft t是我们刚才这里说的1万门对吧，1万门，那这里的意思就是说，我要把我之前的这个ct减一保留下来，多少遗忘掉多少对吧，所以说啊这里我们进行相乘对吧。

就是这一块这里进行相加呢，就等于是把我们上一时刻经过遗忘门之后的，一个值，和当前时刻，经过了咱们门之后的这样的一个值，进行了一个融合，这样呢我们就得到了我们当前新的这个时刻的，Sales state。

好吧，好再说一遍啊，这个ft t，第一个ft t是我们的之前的一个，1万门的一个值，这里这个cat减也是我们上一个时刻的cell state相乘，表示。

就是说我要把之前的sales state遗忘掉多少，那后面这一块it是我们当前的这个输入的，这个值，我需要保留多少，那c ct t尔塔的话，是当前的输入值的一个非线性变化。

这两块相乘意思就是说我当前的输入值，我需要输入哪些内容，保留哪些内容，最后再把这两者进行相加，意思就是说，我要把之前的上一个时刻的剩余的内容，和我当前的输入的内容进行合并，在一起就是这样的一个意思啊。

这就是我们的一个输入门啊，那这样的话我们就可以得到当前时刻的一个，sales state啊，sales s那有了sales state的之后呢，我们还要考虑一下，我们也还有这样的一个当前时刻的HT。

我们还没有算吧对吧，所以接下来我们就来输出门，看一下我们当前时刻的这个HT唉，怎么进行计算，好我们看一下啊，他的他的这个思路，其实还是延续，咱们刚才这个门的这样的一个想法啊。

首先呢还是把我们上一时刻的hal hidden state，和当前时刻的一个输入啊进行拼接，进行非线性变换哦，不是非线性变化啊，经过我们的个SIG格MOID的，那。

这个时候实际上又是一个0~1之间的一个值，对吧，0~1之间的一个值好，然后呢我们会把刚才的这里这个值啊，这个这里我们是已经拿到了这个cell set对吧，当前时刻的cell set。

我们会把当前时刻的cell state经过这个ten h啊，经过ten h经过完twitch之后呢，说白了就是把cs state做了一个非线性的变换对吧，再和这样的一个门之后的一个结果进行相乘。

而作为当前时刻的一个hidden state，作为当前时刻的一个hidden state啊，这里的意思就是说我当前要进行输出了对吧，但是呢我现在有了我们啊，上一时刻的这些sales state，然后呢。

我是不是应该考虑一下我哪些应该输出对吧，我们这里也可以看一个例子啊，假设啊有这样的一个例子啊，the cat会说ready at was full，当处理到was的时候，由于前面获取的主语是cat对吧。

那我们这里肯定就要用was，那如果我们前面这里是cats呢对吧，如果是cats，那我们后面这里就应该用were，就这样的一个意思啊，就这样的一个意思，这就是我们的这个LISTM的一个输出门啊，输出门。

好最后呢我们再来简单看一下，关于为什么LSTM，它能解决这个所谓的一个梯度消失的，一个问题啊，那这里呢是我们的这个啊sales state，它求解这个梯度的过程的时候，它的一个公式啊，它的一个公式。

其实大家对比的时候就可以发现啊，我们这里这个sales state它其实它的一个梯度啊，它是一个相加的形式对吧，它是一个相加的一个形式，那我们这边的这个RN，它是一个纯相乘的一个形式对吧。

然后我们再再注意一点啊，他这里啊它有一个FT这一项，也就是说这里这个ft t是啥，这个ft t表示的是我们这里，这个这里这样的一个值啊，也就是说我们的这个遗忘门他的一个输出值。

那这个值它是个0~1之间的一个值，所以说啊就算你前面这些地方，你都是一些相乘的对吧，你这些前面这些地方就算乘，假设我你这三个部分假设是一个0。0，000001接近于零的一个值。

但我最终这个FT它是一个0~1之间的，一个值，是一个0~1之间的值，它可能是0。1，它可能是0。2对吧，当然他也有可能是0。01也是有可能的，但是啊就是因为它这里是一个相加的一项。

他就就算你前面这些全都是零，我也能保证你最后这个值，它是一个0~1之间的一个值，但是它不会特别接近于零，就以这样的一个形式啊，来缓解咱们的一个梯度消失的一个问题，注意这里是缓解啊，这里是缓解。

假设你最终这里这个ft t，你求解出来也是一个零，点000001的一个值对吧，非常接近于零的值，那其实他也没有办法完全解决，这个梯度消失的问题啊，所以说咱们说的是缓解，好吧啊，我这里这个标题也改一下。

缓解啊缓解好，这就到这里的话，我们就给大家嗯介绍完了这个LSTM啊，我们就介绍完了LSTM，那接下来的话我们再来看我们的啊，下一个部分啊，下一个部分咱们的这个卷积神经网络啊，卷积神经网络，那。

好我们来考虑一下啊，就对于RN这样的一个模型来说，那除了刚才说的这个，所谓的梯度消失的一个问题啊，他其实还有个问题啊，就是他时间复杂度，时间复杂度比较高啊，它的时间复杂度是ON。

因为它是一个创新的一个过程对吧，它是一个从左到右，它需要一个节点，一个节点的去进行这样的一个处理，它是一个创新的过程啊，创新的过程你序列长度越长，它处理的时间点就越多对吧，花费的时间就越多。

那对于我们的这个呃，文本分类的一个任务来说啊，我们其实刚才也说过啊，我们在这里的时候，我们通常如果要去做文本分类对吧，把每一个词进行一个输入，X 1x2，一直到XN进行输入。

然后要取的是最后一个时刻的一个这个值，用这个out值来进行我们的文本分类，所以说啊他很耗时，那既然如此，我们是有没有可能采用一些并行模型的，一些思路，例如像咱们的一个CNN模型对吧。

它其实就是一个并行的一个思想，咱们的我们来看一下啊，如何使用我们的这个卷积神经网络，来加速一下我们这个RN的一些缺点啊，大家可能都会觉得，这个卷积可能更适合去处理图像对吧。

但实际上啊在文本当中他处理的也非常的多啊，非常的多好，那我们来看一下，如果我们使用这个卷积神经网络，是怎么去处理文本的好，那我们这里假设有一句话啊，叫做他毕业于上海交通大学这样一句话。

那我们分词分完之后呢，它一共有六个词啊，六个词对于on n来说，它实际上每次处理的是一个字对吧，这样哎或者说一个词诶，先输入它，然后输入毕业，输入渝，再输入上海，ok now啊。

那个如果我们使用的是卷积神经网络，卷积神经网络，它就有一个优势啊，他可以干什么，他可以去处理你的这个所谓的更大力度的，这样的一些特征，例如我可以去处理，把这个他毕业于这三个词放在一起，一起去提取特征。

毕业于上海三个词放在一起去提取特征，还有最后这样的一个上海交通大学，我可以把把这三个词放在一起一起来提取特征，但是对于RN来说，他就没有办法做到这样的一个情况，那为什么卷积可以做到这个所谓的这种多词。

或者说短语级别的一个特征的一个提取呢，那接下来我们就来详细给大家介绍一下，卷积神经网络啊，我们就详细介绍一下，在介绍卷积神经网络之前呢，我们先了解一下什么是卷积，什么是卷积，通常啊我们就称为F乘以G。

就是对于F和G的一个卷积，好好像很难理解是吧，很难理解，好不着急，我们慢慢来看啊，如果我们现在这个是一个连续的啊，如果是一个连续的一个情况，它实际上就是求解积分的一个过程，那如果是一个离散的一个情况呢。

我们就是要去对我们所有的情况啊，去求和求和，但是看完这个公式，好像还是非常难理解这个卷积到底啥意思对吧，没关系，我们慢慢来，我们可以先看一个例子啊，假设我们令啊X等于啊掏，然后Y等于N卷套。

那么X加Y等于N啊，就是下面这条线可以看到啊，我们有这样的一条线，这条线那这一条线它实际上就好比什么呢，就好比我们把一条毛巾啊，沿着左下角的这样的一个角，就往右上角去卷去卷，那这个卷的过程呢。

我们把它称为这样的一个卷积的一个操作啊，这个过程我们把它称之为卷积的一个操作，我们再看一个离散的一个例子啊，我们看一个离散卷积的一个例子，这个例子看完，大家就能对卷积有一个更直观的一个理解了。

假设呢我们现在有两个啊，两个桃子啊，这两个桃子呢我们希望啊扔到的点数，它加起来是四的一个概率，就是我们要求解啊，加起来是四的这样的一个概率，那我们用F来表示第一个，用G来表示第二个。

那F1呢表示的是投出一的一个概率啊，F2F3的话就以此类推，G也是啊，G1的话就是表示我扔出就是第二个这个投资，扔出一的这样的一个概率值啊，好那我们啊就来计算一下啊，计算一下，OK那我们来看一下。

如果我们把所有的这个这个这个啊是就相加，他的这个概率是咱们的这个，四的这个投资的这个情况啊，给算一下啊，首先呢是咱们的第一个骰子，扔到一的一个概率值，要乘以咱们的第二个骰子扔到三的一个概率值。

然后再加上我们扔到二的一个概率，分别扔到二的对吧，还有情况呢是我第一个投资扔到三，第二个投资扔到的是一的一个概率值对吧，我们要进分别进行相乘再相加，那这个呢实际上就是我们两枚骰子。

点数加起来为四的这样的一个概率对吧好，那接下来呢我们把它改变一下啊，改变成我们卷积的一个定义，它实际上就是，表示为F4减M再乘以gm，然后前面是一个累加扫描ATION。

那这个啊实际上就是我们的一个卷积卷积，那再解释一下，说白了他就是先相乘再相加对吧，先相乘再相加，这就是我们的一个卷积啊，卷积我们也可以回到我们一开始的这样的一个，离散的一个公式，可以看一下对吧。

它实际上就是一个先相乘再相加的一个过程，这就是我们的一个卷积操作啊，卷积操作好，那有了这样的一个卷积计算的，这样的一个概念之后呢，我们再把这个卷积啊，拿到我们的图像上面来看一下啊，下面呢这里有一张啊。

噪点非常严重的一个图片啊，如果我们想去做一个所谓的去噪的一个处理，我们就可以把这个采用一个卷积的一个方式，我们可以把高频信号啊，以周围的一些数值去做一个，平均的一下的一个处理，怎么做呢，好举个例子啊。

假设这是我们的这样的一个图片啊，这是我们的一个图片啊，我们要去平滑这个A11这个点，它周围的这些加速点，OK那我们就把A11附近的这些像素点啊，全部给取出来，取出来取出来之后呢。

我们可以得到这样的一个矩阵F啊，我们的一个矩阵F，接下来我们去定义这样的一个啊卷积核，也就是用G也是我们的卷积卷积核啊，这个卷积核因为我们刚才说我们如果去造的话，就是取个均值对吧。

那我们就把所有的卷积核的每一个位置，都设置为1/9，然后呢我们再把这个这里这个F啊，和我们的这个G进行相乘再相加，也就是说A00乘以我们的19，加上A01乘以我们的19，再加上A02乘以我们的19。

就对位相乘再相加，最终啊我们对位相乘相加之后呢，我们就可以得到一个所谓的，卷积之后的一个结果啊，卷积之后的一个结果，这就是我们在图像当中去做的，这个所谓的卷积的一个处理啊，卷积的一个处理。

啊这就是我们可以看一下啊，这是我们的一个啊卷积的一个动图，左边呢是我们这样的一张图片啊，中间是我们的一个卷积核，那这个卷积核呢在这里啊，我们只是对我们图像的这一个部分，做了卷积处理对吧。

但是你对这一个部分做完卷积处理之后，其他地方你其实也需要做卷积处理，这个时候呢你就要需要去移动你的卷积核对吧，我们可以看一下啊，我们这个动图是啊，先向这边先向右移对吧，然后再向下移进行移动。

你看第一次卷积完了之后向右移一个单位，做一次卷积，再往右移一个单位，再做一次卷积对吧，直到移不动了，我们再往下进行移动，这就是我们完整的一个卷积的一个过程啊，卷积的一个过程，好那有了卷积这个概念之后呢。

我们来看一下卷积神经网络啊，那卷积神经网络，实际上呢就是在寻找最合适的一个卷积核，那刚才我们是要去造对吧，那去噪的卷积核就求均值就OK了，但是我们想一下哈，如果我们现在是在处理我们的图片。

或者说我们在处理我们的文本，我们想去做这个所谓的分类，或者说图片识别对吧，那我们就应该去找到最合适的一个卷积核对吧，那如何去找到最合适的一个卷积核呢，这个是不是就是我们刚才给大家介绍。

神经网络那个部分给大家提到的对吧，先计算我们的loss，要去求解我们的这个梯度，要进行反向反向传播，更新我们的权重对吧，那在我们这里其实也是一样嘛，我们的卷积核实际上就是我们的W。

我们只需要根据我们的目标得到我们的输出值，要求解我们的loss，再根据loss进行反向传播，就可以更新我们卷积核的一个值对吧，这样的话我们就可以得到我们最合适的卷积和，好我们可以看一下这里这个图片啊。

啊左边的话是我们的一个输入的图片，右边的话是我们的一个卷积核啊，啊右边的话是我们得到的一个结果啊，得到一个结果，然后红色的话啊，红色这里乘以1×0乘以一，就是我们对应的一个卷积核啊。

那右上角这一个卷积核和黄色这一块，进行卷积之后呢，得到的一个结果就是四啊，就是四对位相乘再相加啊，好这就是我们卷积神经网络好吧，卷积神经网络，那接下来呢我们再看一下啊，如何把这个卷积神经网络。

应用到我们的这个文本当中呢，好接下来而我们这边有这样的一句话啊，这句话我们做了一个分词，1234567好，我们分成了七个词啊，这句话我们分成了七个词，分成了七个词。

那每一个词呢它有对应的这样的一个embedding，就上节课我们去讲那个word vector的时候，给大家讲过吧对吧，就每一个词我们可以把它转换成这样的一个，embedding的一个形式，那在这里呢。

它每一个词的这个embedding是四维的啊，四维的，所以说我们的一个输入啊，就是一个7×4的输入，就是7×4的好吧，7×4的，接下来呢我们就会去定义一个这样，所谓的一个卷积核啊，卷积核。

那这个卷积核它需要有两个维度对吧，第一个维度的话是我们这个序列长度，这个方向的一个维度，第二个维度的话是他的这个embedding，这个维度对吧，这里大家就需要注意一下啊，对于他的第一个维度。

你是可以自定义的，你第一个维度你可以是二，你可以是三，你可以是四，但是对于第二个维度，你必须和文本的这个embedding，这个维度保持一致，也就是说你必须设置为四，为什么呢，假设你的这个维度小于四。

那你这个卷积核它可能是什么样子呢，就变成了这个样子，那你这样的一个卷积核，你卷积出来实际上是没有意义的，因为你这个词你没有把这个词，它完整的这个embedding给加进去，你没有加进去的话。

你这个词的这个语义可能会改变对吧，所以你在做卷积的时候啊，你要把当前这个文本的embedding，全部给包含进去，你才能拿到当前这个文字或者这个词，它完整的一个语义，好吧，这里是关键点啊。

大家必须注意一下，这个维度必须保持统一，第二个维度是多少，取决于你输入的文本的embedding的维度是多少，好吧，好我们再看我们的第一个维度啊，那对于第一个维度来说，就得看你想去取多大力度的一个特征。

就像我们刚才这里给大家举的那个例子，如果你每次想取得是一个字，一个字或者说一个词一个词的特征，那你就设置为一，那对于我们这里，我们这里取的这些是短语，它是三个词对吧，那你就把你的卷积核大小设置为三。

如果你想取两个词，那你卷积和大小就设置为二，所以啊，这就是得到了我们最终的这样的一个卷积核啊，那这个卷积核实际上就是一个3×4的啊，3×4的好，那我们就把这个卷积核在我们的这个文本上啊。

去做卷积的一个处理，最终呢我们就可以得到这样的一个输出值，这样的一个输出值，第一个输出值的话就是前面三个词，它的一个前面三个词啊，然后这这个值呢就是又是这三个词，这三个词，对于最后这个啊就是最后三个词。

他的一个卷积之后的一个结果啊，这就是咱们的一个卷积啊，卷积OK那了解了这个卷积的一个概念之后呢，我们再来看一个东西啊，对于我们这里来说，我们可以看到啊，我们做了卷积操作之后呢。

我们这个序列长度从七变成了五对吧，但是大家可以考虑一下啊，就有些情况，假设我现在是要做N12，我要做N12，我们的输入和输出必须长度保持一致对吧，那你这里缩短了不行啊，那如何才能保证这个长度不变呢。

我们就可以在头和尾啊去补充一个padding位，也是补零，只要你补零之后呢，我们这里一开始的这个长度啊，就由七变成了九，但是啊我们经过卷积操作之后呢，它的这个长度就还是七啊。

就和我们原来保持一样的保持一样，为什么它会一样呢，因为我们每次做卷积的时候，就是从这里开始了对吧，我们之前的话是从这里开始的好，这就是我们补padding啊，补padding。

那看完补padding之后呢，我们再来看一个概念啊，叫做mari china，在五处理图像的时候呢，图像它是一个RGB3原色的对吧，所以呢我们通常处理图像的时候，我们会有最少你得准备三个卷积核对吧。

分别处理RGBRGRGB的一个通道，那对于文本来说其实也是一样嘛，你可以多准备几个卷积核，你卷积核准备的越多，你提取的这个特征维度就越多对吧，那这里的话，我们就等于是准备了三个这样的一个卷积核啊。

三个卷集合，然后再加上我们的padding，那正常情况来说，我们只能得到一个啊啊77×1的对吧，7×1的这样一个矩阵，那三个矩那个卷积核的话，最终我们就可以得到7×3的这样的，一个特征啊。

7×3的这样的一个特征好，这是我们的一个多通道，多通道，好接下来我们再来看一个概念叫做池化操作啊，池化操作，那我们这里多个卷积核，我们能达到了一个7×3的这样的一个矩阵。

但如果我们要最后去做一个二分类对吧，假设我们要去做文本二分类，你肯定不能给我一个7×3的呀对吧，你肯定得给我一个啊7×1的，或者说七乘几的对吧，你不能，总之你不能给我一个多维的呀。

我你只能给我一个一维的一个向量，我能才能去作为一个分类吗，你不能给我这样的一个7×3的对吧，所以呢我们这边就会做一些所谓的池化操作啊，这里呢我们采用的是一个最大池化层，就说我们去吧。

当前这个里面啊它最大值给取出来，这一列里面最大值给取出来，这是0。3，然后这边取出来是1。6，这是1。4，所以最终我们就能得到了一个，一维的这样的一个向量啊，一维的一个向量，这是我们的最大池化层啊。

最大池化层，好那我们接下来来看一下啊，这个卷积的这个过程怎么去计算，我们输入的这个值，经过卷积之后，它输出值的一个维度呢，啊我们以图片举例啊，假设我们输入输入的图片大小是W乘以W。

我们卷积核的大小是F乘以F，那不长是S不长是什么意思呢，就是说你每次移动多少步，有的时候呢你可能会移动一步，但是有可能你也会移动两步三步对吧，然后还有一个是padding啊。

padding我们用P来表示，OK那假设我们现在不padding啊，如果我们不padding，那我们的这个输入和输出的一个这个维度，大小呢，这是win的话是我们的一个输入输入啊。

就是输入减掉我们卷积核的一个大小，再除以我们的S也是咱们那个步长，而加一就是我们输出解输出的这个啊长度啊，那如果我们要保持不变的话，那就是我们输入的一个维度，再加上二乘以padding。

因为padding你前面要补，后面也要补对吧，所以你要乘以二，然后再减掉我们的这个filter的一个大小，再除以我们的步长，再加上一，这是我们的输出的一个维度的一个大小啊，好到这里的话。

我们就给大家啊把这个卷积神经网络这一块啊，咱们就讲完了啊，讲完了，今天的话到这里的话，基本上就是要给大家讲的所有的啊，这个理论方面的一个内容啊，理论方面的一个内容，那接下来的话。

我们就进入到我们的这个实战环节啊，我们给大家介绍了这个卷积网络，又给大家介绍了这个啊循环神经网络对吧，那我们就来应用一下啊。



![](img/77fdf2ced5678be963137fd30ae4b341_8.png)

应用一下好，我们进入实战环节啊，实战环节。

![](img/77fdf2ced5678be963137fd30ae4b341_10.png)

额，接下来我们来看一个基于LST作战的一个，文本分类的这样的一个小例子啊，看个小例子好，我们把字体调大一点，OK好我们可以看一下啊，我们啊从TRA方法开始看吧，好这个处理方法当中呢，首先呢。

我们会去进行这样的一个数据的一个加载啊，我们会去加载一个数据加载数据，然后呢去啊拿我们的模型，我们先看我们的这个加载数据这一块吧好吧，我们一步一步来好，呃加载数据这边呢我们是从本地这边啊。

加载了一个这样的一个啊，情感分析的二分类的一个数据啊，我们可以简单看一下，蒙牛真果粒美丽有新意，这是一个正例啊，还有什么密密麻麻孩子不喜欢，这是一个复利啊，呃总之是个二分类啊。

咱们情感分析的二分类一的话是正理，零的话是咱们的一个负理啊，负理好，那我们就来看一下啊，我们看一下这个load date这个方法，先看一下load date这个方法，首先呢我们去把这个文件给读取出来啊。

然后循环每一行，循环读取每一行啊，今天的代码因为比较多啊，可能我就没有办法全部带着大家写了，好吧，我我们就把一些关键代码会，给大家详细讲一下啊，好这边循环每一行代码，那每一行代码呢。

我们这里是根据这个啊tab键啊，进行这样的一个区分，这样的话我们就可以拿到我们的这个标签，还有我们的文本啊，拿到我们的标签，拿到我们的标签和我们的文本好，那有了标签，有了文本之后呢，我们考虑一下啊。

我们上节课给大家讲这个word to vector那块的时候，说过啊，我们需要先构建一个词典对吧，我们需要有构建一个词典，那构建词典呢怎么构建呢，首先第一步肯定是需要进行分词嘛对吧。

那我们就去便利一下啊，我们所有的这样的一个文本，然后把便利出来的文本呢，去做这样的一个分词的一个处理啊，分词的一个这样的一个处理啊，我们这里可以看一下啊，这里是好token nice。

这个方法我们去看一下啊，可以看到啊，这个token ize呢，其实就是调用了一下结巴的这个分词啊，调用了一下结巴分词，结巴DOCUT就可以进行分词了啊，给你分词好，这边的话。

我们就拿到了我们所有的这个分词的，一个结果啊，这里大家需要注意一下，这个分词的结果是一个list，然后list里面呢又是list，那内部的这个list呢，就是一个一个的这样的一个分词好吧。

那接下来呢我们去定义这样的一个词典啊，定义一个词典，这边呢我们就开始对这个分出来的这个词啊，去进行一个遍列，然后呢我们去统计一下每一个词，它的一个词频，统计一下每一个词的一个词瓶啊，统计词频好。

统计完磁瓶之后呢，我们去根据这个词瓶去做这样的一个呃，降序的一个排序啊，因为我们说过啊，我们要把这个词平比较高的排在前面，词平比较低的排在后面啊，排在后面，这样的话我们就排好序了，排好排好序之后呢。

我们要来定义我们的一个词典，这个词典当中呢我们需要两个标记位啊，这个也是在我们上一节课给大家讲过的，一个呢是pad标记位，表示的是我们这个补零的对吧，一个是UNK标记位，UNK的话就是啊OV的一些词。

我们就用UNK来表示对吧，好这是我们前两个词啊，那后面的一些词呢，我们就去便利我们的这个排好序的一个，词典当中，排好序的这个词啊，把这些词加到我们的词典当中，这样的话我们就能拿到我们的一个词典，好。

我们这里可以给大家看一下，我们这个词典什么样子啊，好稍等一下啊，OK我们把这个词典展开看一下啊，它实际上就是这样的一个啊字典的一个格式，对吧啊，key的话就是我们对应的这个词。

value的话就是它的一个下标对吧，就是它的一个下标，那我们输入一个这样的一个词，就可以拿到它对应的一个下标的一个值啊，这就是我们的一个词典，这是我们的词典好，那有了词典之后呢。

那接下来的话我们就要去做什么事情呢，是不是要把我们的这个训练的一个数据，先转换成我们的一个下标对吧，我们就要便利便利我们每一条数据啊，把这个数据转换成一个下标，好，我们就来看一下这个文本转下标。

这个函数它是怎么写的啊，首先还是一样，我们先分词，分完词之后呢，我们去遍历每一个词对吧，然后根据我们的这个词典get的话，就是根据我们的key把value给取出来对吧，如果取不到。

那我就取一这个一为什么是一呢，因为我们UNK表示对应的是一，所以我们这里是一好吧，这样的话我们就把我们的这个下标啊，给取出来了，取出来之后呢，我们要去做一个padding的一个处理对吧。

padding的一个处理好，这里我们也看一下这个函数啊，也很简单啊，这边先去便利我们每一个句子啊，遍历每一个句子，那如果我们这个句子的长度，是小于我们最大长度的，我们这个最大长度测了个十啊。

如果我们这个句子的长度小于了最大长度，那我们就需要补零对吧，补零怎么补呢，那我们就去做contact at嘛，把我们的X和零进行一个拼接，拼接多少个零呢，拼接的是最大长度。

减掉我们当前文本长度的这个长度的零好，这样的话我们就可以把短的补偿对吧，那我们再看一下啊，如果我们这个长度它是大于这个最大长度的话，那我们就截取对吧，我们就截取，这样的话。

我们就做了这样的一个补偿的一个操作啊，这样的话我们所有的文本的长度就能保持一致，接下来呢我们再把我们的这个输入值，还有我们的标签给到我们这个啊，Tensor dataset。

这个DATASET是干什么的呢，大家可以把它看成他就是这个所谓的这样的，一个list的一个格式，只是说这个list当中啊，它包含了我们的训练数据，也包含了我们的label，好吧好。

那有了这个DATASET之后呢，我们接下来呢需要去定义一个data loader，这个data loader又是干什么的呢，因为我们每次在训练的时候啊，我们的这个GPU的一个显存。

是有一定大小的限制的对吧，或者说即使你用CPU跑也是一样啊，是有这个大小限制的，那如果你的这个数据量特别大，你有100万的一个数据量对吧，你没有办法，一次性把所有的数据都放到你的这个，显存或者内存里。

所以呢我们通常都会分批进行跑啊，采用这个besides的一个形式，就我们每次只计算一部分的这样的一个数据，那我们使用data loader呢，就可以生成一批一批的这样的一些数据。

那每一批的数据它的这个数量啊都是固定的，也就是data babech size条数哈，EXSIZE条数，当然它有个输出值啊，就是这个DATASET，这个DATASETS的话。

就是根据我们的tensordata set来得到的，那我们把这个东西呢还有我们big size啊，就给到我们的data loader，就能拿到我们的数据的这个data loadad。

当我们来便利这个data data loadad的时候呢，每次就可以取出有一个batch的一个数据啊，那接下来的话我们就把这个data loader，还有我们的这个vocab进行返回。

也就是我们的训练数据或我们的词典，我们就进行返回啊，返回，好我们再回到哎，我们再回到一开始的地方啊，啊这是我们加载数据的一个代码，这样的话我们就拿到了我们的训练数据，拿到了我们的词典对吧好。

接下来我们来看一下我们的模型啊，我们的模型我们的模型很简单啊，我们这里用了一个LSTM的一个模型，然后加了两个全连接层啊，全连接层是什么东西呢，就是我们的普通的一个神经网络，好吧好。

我们来看一下我们这个模型什么样子啊，其实内容都是我们今天学过的，首先呢我们需要定义这个embedding层，这个embedding层是什么东西呢。

就是我们上节课给大家讲的这个embedding matrix，就是我们输入一开始输入的是什么，我们输入的是文本的一个下标对吧，或者说一个one hot。

我们需要去经过这个embedding matrix，把这个embedding给映射出来对吧，所以呢我们是首先需要去定义这个embedding，层啊，embedding哦，这里忘记说了啊。

啊很多同学可能都没有接触过这个PYTORCH啊，可能没有接触过PYTORCH，那我这边先给大家简单介绍一下，这个PYTORCH啊，PYTORCH去定义一个模型呢，它非常简单啊，非常简单。

如何定义一个模型呢，首先呢你实现一个类，这个类呢需要继承自PYTORCH，这个touch到NN打module，继承完这个类之后呢，你需要重写两个方法，一个是构造方法，一个是for word方法。

构造方法他做的事情是什么呢，嗯准备我们需要用到的参数和layer，就是说我们需要用到哪些参数，你需要去做一些准备，有需要用到哪些层，你要用RN，你要用CN还是要用LSTM。

你就在我们构造方法当中去进行准备，这是我们第一个要写的方法啊，要重写的方法，第二个要重写的方法的话是我们的for2的方法，这个方法呢就是我们的前向传播，前向传播，这是在什么，在干什么呢。

就是把我们准备好的layer拼接在一起，拼接在一起，就你这里准备了这么多layer对吧，那每一层layer它这个数据是怎么传递的，你需要把它拼接在一起，你才能构建成一个神经网络对吧。

这就是我们的前向传播做的一个事情啊，做的一个事情就这么简单啊，PY套式构建模型就这么简单，就这么简单，好吧好，那我们回到刚才的内容啊，首先我们来看我们的构造方法，我们先构建我们的embedding层。

这个embedding层的话就是刚才说的，我们要把我们的下标转换成这个，embedding的一个形式对吧，那这个embedding呢这个embedding层啊，它有两个参数。

第一个参数的话是这个词典的一个大小，第二个参数的话就是embedding size，那分别对应的，实际上就是我们上节课给大家讲的，这个embedding matrix的一个维度嘛对吧。

一个是词典的大维度，一个是embedding的一个维度，所以这两个参数啊你需要传递进来，这样的话我们就构建好了我们的引白领层，然后呢我们这边使用的是LSTM好，我们就来构建我们的LSTM层，LSTM层。

首先的话是你的输入维度对吧。

![](img/77fdf2ced5678be963137fd30ae4b341_12.png)

你的这个X对吧，你的输入维度是什么，还有你的这个hidden hidden size，就是你的这个啊hidden state和咱们的这个sales state。



![](img/77fdf2ced5678be963137fd30ae4b341_14.png)

它的维度是多少，这个东西需要定义出来好，然后是这这里有个参数啊，叫做number layers，什么意思呢，就是说你要构建几层这样的LSTM。



![](img/77fdf2ced5678be963137fd30ae4b341_16.png)

正常的话就是一层嘛对吧，我们实际上可以这样子做啊，我们可以啊，诶稍等一下，哎，这是我们的一层AIST，我们可以在上面再叠加一层啊，再叠加一层LSTM，这样的话就是两层的LISTM啊。



![](img/77fdf2ced5678be963137fd30ae4b341_18.png)

那在我们这边代码当中也是一样啊，我们就设置两层两层，然后这里我们额外还有个参数啊，叫做best fast first，就是我们把第一个维度啊设置为这个啊，这个背驰的一个维度。

它默认第一个维度不是背驰的维度啊，所以我们这里把它设置为true，那这样的话，我们这个维度实际上就是第一个维度，是咱们的啊，BESIZE对吧，第二个维度的话是我们的序列的长度。

第三个维度的话就是我们的这个啊，输出的一个维度，也就是hidden size对吧，这就是我们最终AISTM的一个，输出值的一个维度啊，输出值的一个维度好，这是我们LSTM层，最后我们再构建两个全连接层。

第一个全连接层的话，你的输入维度，肯定和上一层的这个维度保持一致，你才能进行计算对吧，所以是这里是256，那这里就是256，然后接下来呢我们给这一层的一个输出维度啊，我们给个100。

然后最后的我们再给一个啊，再给一个linear层啊，输入为就是100，输出维度是二，为什么这里输出维度是二呢，因为我们接下来要去计算我们的那个and，cross entropy啊，交叉熵损失。

所以呢我们这里就输出两个值啊，输出两个值去计算我们的cross entrop好，这是我们的构造方法啊，接下来我们看我们的前向传播，那前向传播的话，就是把我们准备好的layer，去进行一些什么拼接对吧。

首先是embedding层，我们把我们的X给过来啊，这个X是什么呢，这个X就是我们刚才data load date的时候呢，我们把他做了转序列长度，然后补偿之后的这个X啊，我们进行了返回了。

只是封装成了loader的一个形式对吧，但实际上里面还是这样的一个下标，并且做了补偿对吧，所以这个东西呢，待会呢我们就会把它传递进来啊，传递进来就先给到我们embedding层。

拿到我们的embedding，再把embedding给到我们的LSTM好，这里是我们的重点啊，这里是我们重点，我们可以去看一下LSTM的代码的说明，额我们看一下它的output，output的话。

它包括两个部分啊，一个是输出值，还有一个是HN和这个CN，HN的话就是我们的hidden state cn的话，就是刚才说的这个sales state，那我们其实不需要后面这两个东西。

我们只需要output对吧，我们只需要他的输出值啊，输出值，所以呢啊我们这里就用这个用这个符号啊，就表示刚才这里的eden state，Sales state，我们不需要啊。

我们不需要只需要他的这个输出值好，那这个输出值的维度啊，就是这个h size序列长度，Een state。



![](img/77fdf2ced5678be963137fd30ae4b341_20.png)

那我们现在要做的是什么，做的是文本分类对吧，我们文本分类刚才我们有个这样的一个图，它是一个manning to one的一个形式。



![](img/77fdf2ced5678be963137fd30ae4b341_22.png)

并且我们取的是最后一个时刻点的一个输出值，对吧，所以啊。

![](img/77fdf2ced5678be963137fd30ae4b341_24.png)

我们这里应该取的是最后一个时刻的输出值，最后一个时刻怎么取呢，是不是就是一呃，这两个维度的话就是就全取，为什么是全曲呢，因为bech size你要全取吧，pen size你要全取，唯独序列长度对吧。

我们要取最后一个字，那就是一好，这样的话我们就可以把最后这个时刻的值啊，也就是这个值这个蓝色框的值就取出来了，我们就给到我们的全连接层，给完全连接层之后呢，我们再给到第二个全连接层。



![](img/77fdf2ced5678be963137fd30ae4b341_26.png)

然后再把结果进行输出，这就是我们构建的一个神经网络啊。

![](img/77fdf2ced5678be963137fd30ae4b341_28.png)

神经网络好，我们的模型就准备好了，我们再回过回到这边啊，有了数据，有了模型，那接下来就是开始进行训练了对吧，接下来开始训训练好，训练过程呢，我们这边啊首先去定义我们的这个优化器啊。

优化器这个优化器是什么东西呢，这个东西大家可以理解为。

![](img/77fdf2ced5678be963137fd30ae4b341_30.png)

就是我们在求解这个进行啊梯度下降的过程中，我们不是要用这个W减掉这个学习力，再乘以我们的这个梯度对吧，那这个东西呢就被这个过程啊。



![](img/77fdf2ced5678be963137fd30ae4b341_32.png)

我们就采用了这个所谓的一个优化器，但不同的一个优化器啊。

![](img/77fdf2ced5678be963137fd30ae4b341_34.png)

它会有针对于我们这个梯度下降，会有一些更多的一些优化方法。

![](img/77fdf2ced5678be963137fd30ae4b341_36.png)

那我们用的这里这种方法啊，被称为这个梯度下降，这是个随机梯度下降法啊，就是咱们有这个SGD，我们有可以给大家看一下啊，有这个touch点，用SGDSGD，SGD啊，SGD的话就是我们的随机梯度下降法。

就是我们对一个位置的数据啊，去采用我们的梯度下降法，那ADAM呢，就是在SGD的基础上，去做了进一步的一些优化啊，让它收敛的更好一些，或者说收敛的速度更快一些啊，更快一些。

那我们啊这里这个原理啊我们就不展开讲了，大家感大家感兴趣的话，可以再下来搜索一下相关的资料啊，好，那我们这里就去定义一个这样的一个优化器啊，这个优化器呢需要去优化，我们当前这个模型里的所有参数对吧。

并且学习率我们要设置一个啊，就设置为我们这里设置为0。01，接下来我们去定义我们的这个loss function。



![](img/77fdf2ced5678be963137fd30ae4b341_38.png)

Loss function，像我们这里这个例子对吧，这里这个例子啊，我们采用的是这个binary cross entrop，它是一个二分类的一个啊交叉熵损失对吧。



![](img/77fdf2ced5678be963137fd30ae4b341_40.png)

然后我们这里呢就采用普通的cross entrop啊，其实你用binary cross entrop也是一样的好吧，都是可以的，好优化器，有了loss function我们也定义好了。

那接下来呢我们就可以进行我们的训练了对吧，这边呢这个代码什么意思呢，就是说我们要把我们的模型啊，如果你的KDA是可用的，就把模型放到我们的GPU上，好，这边我们跑五个epoch啊。

好每个epoch的时候呢，我会去便利我这个data loader对吧，我们这个data loader，那便利data loadad的时候呢，我们就可以把每一个bech size的，一个数据给取出来。

取出来之后呢也是一样啊，要放到GPU上，对于我们的标签也是哈，我们把它放到这个啊GPU上面，OK那X呢我们就给到我们的模型，给到我们模型，给到我们模型呢，实际上就是去执行我们的for word方法。

就可以得到我们的输出值，好得到我们输出值之后呢，啊，我们就可以去取他的aug max，来得到我们真实的一个标签，让我们把真实的标签啊存到我们这个list当中，然后这是我们的label啊。

label我们也存到这个list当中，接下来呢我们把我们的输出值啊，和我们的这个真实的这个label啊，去算一下我们的这个loss loss值，算完这个loss值之后呢，我们就有了loss对吧。

有了LOS好，首先第一步，我们把我们优化器当中之前的梯度啊，先清零清零，因为在PYTORCH当中啊，这个优化其它会保留之前的一个梯度，你如果不清零的话，它这个梯度会累加，所以我们在进行更新梯度的时候啊。

要先把这个七梯度清零清零之后呢。

![](img/77fdf2ced5678be963137fd30ae4b341_42.png)

我们去进行反向传播，反向传播是在干什么，就是在去求解咱们的这个梯度啊。

![](img/77fdf2ced5678be963137fd30ae4b341_44.png)

就是在求解FW，在求解我们的梯度，那求解好梯度之后呢，我们再执行我们的这个optimizer的step方法，就可以去进行反向传播。



![](img/77fdf2ced5678be963137fd30ae4b341_46.png)

更新我们的这里的W好吧。

![](img/77fdf2ced5678be963137fd30ae4b341_48.png)

这里就是在更新我们W这是在求梯度，好吧好，这里可以给大家写一下啊，这里是在求解梯度，这里是在诶更新我们的权重好，这样的话我们这个就是在一个循环的一个过程，对吧，这就是我们循环的一个过程。

最后的话我们每跑完一个epoch，我们就去求解一下这个准确率啊，准确率，最后我们把代码执行一下啊，把代码执行一下，整个流程就是这个样子啊，整个流程就是这个样子，好稍等一下啊。

O这第一个apple是67的准确率啊，好我们跑了五个apple之后，准确率就很高了对吧，已经90多了啊，90多了，这个就是我们整个使用这个啊PYTORCH啊，构建这样的一个LSTM。

来做咱们的这个文本分类的，这样的一个小例子啊。

![](img/77fdf2ced5678be963137fd30ae4b341_50.png)

小例子好吧，好，最后的话我们再对今天的一个内容啊，建做一个简单的一个总结啊，今天的话，首先呢我们给大家介绍了这个神经网络对吧，包括什么是神经网络啊，神经网络啊是怎么去求解它的一个输出值的。

要怎么求解我们loss，那有了loss呢，又怎么去进行我们这个权重的一个优化和更新，对吧，那知道了这个什么是神经网络之后呢，我们又去进行了一些扩展，给大家介绍了文本当中用的比较多的，这个循环神经网络。

但循环神经网络的话，它容易出现这个所谓的梯度消失的问题对吧，所以呢我们引出了这个LSTM，那除了这个循环神经网络，它可以处理文本，其实呢卷积神经网络啊，它也可以处理这个文本。

那我们又从卷积的角度去给大家介绍了，如何去处理这样的一个文本对吧，包括卷积，什么是卷积，什么是这个卷积神经网络，再到我们如何在文本上去处理我们的啊，用卷积网络处理文本对吧，最后呢。

我们又给大家去取得这样的一个实战环节啊。

![](img/77fdf2ced5678be963137fd30ae4b341_52.png)

去构建了这样的一个基于LISTM的一个，文本分类模型啊，文本分类模型好的，那咱们今天的内容啊基本上就到这边了，到这边了，今天的内容还是蛮多的啊。



![](img/77fdf2ced5678be963137fd30ae4b341_54.png)

蛮多的啊，大家下来就多花点时间去啊复习一下啊，特别是这一块啊，就这个RN为什么会梯度消失，这一块，这一块是非常重要的啊，基本上面试的时候是绝对会问的好吧，绝对会问的，绝对会问的嗯好行。

那咱们今天的内容就给大家介绍到这边了，后续大家如果还有什么疑问的疑问的话。

![](img/77fdf2ced5678be963137fd30ae4b341_56.png)

欢迎在这个群里面找我进行啊咨询，好吧好，那咱们今天的课程就到这边好。

![](img/77fdf2ced5678be963137fd30ae4b341_58.png)

![](img/77fdf2ced5678be963137fd30ae4b341_59.png)