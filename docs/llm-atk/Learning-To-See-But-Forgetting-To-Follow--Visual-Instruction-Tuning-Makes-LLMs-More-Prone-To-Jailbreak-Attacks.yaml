- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:44:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs
    More Prone To Jailbreak Attacks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学会看却忘记跟随：视觉指令调整使LLMs更易受到越狱攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.04403](https://ar5iv.labs.arxiv.org/html/2405.04403)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.04403](https://ar5iv.labs.arxiv.org/html/2405.04403)
- en: Abstract
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Augmenting Large Language Models (LLMs) with image-understanding capabilities
    has resulted in a boom of high-performing Vision-Language models (VLMs). While
    studying the alignment of LLMs to human values has received widespread attention,
    the safety of VLMs has not received the same attention. In this paper, we explore
    the impact of jailbreaking on three state-of-the-art VLMs, each using a distinct
    modeling approach. By comparing each VLM to their respective LLM backbone, we
    find that each VLM is more susceptible to jailbreaking. We consider this as an
    undesirable outcome from visual instruction-tuning, which imposes a forgetting
    effect on an LLM’s safety guardrails. Therefore, we provide recommendations for
    future work based on evaluation strategies that aim to highlight the weaknesses
    of a VLM, as well as take safety measures into account during visual instruction
    tuning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 增强大型语言模型（LLMs）的图像理解能力已经导致高性能视觉-语言模型（VLMs）的繁荣。尽管研究LLMs与人类价值观的对齐受到了广泛关注，但VLM的安全性却未得到同样的关注。本文探讨了越狱对三种最先进的VLM的影响，每种模型使用了不同的建模方法。通过将每个VLM与其各自的LLM骨干进行比较，我们发现每个VLM都更容易受到越狱攻击。我们认为这是视觉指令调整的一个不良结果，它对LLM的安全保护措施产生了遗忘效应。因此，我们根据评估策略提供了未来工作的建议，这些策略旨在突出VLM的弱点，并在视觉指令调整过程中考虑安全措施。
- en: 'Content Warning: This document contains and discusses examples of potentially
    offensive and toxic language.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 内容警告：本文档包含并讨论了潜在的冒犯性和有毒语言示例。
- en: Keywords: Vision-Language Models, Visual Instruction Tuning, Jailbreak
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词：视觉-语言模型、视觉指令调整、越狱
- en: \NAT@set@cites
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \NAT@set@cites
- en: 'Learning To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs
    More Prone To Jailbreak Attacks'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 学会看却忘记跟随：视觉指令调整使LLMs更易受到越狱攻击
- en: '| Georgios Pantazopoulos^∗, Amit Parekh^∗, Malvina Nikandrou^∗, Alessandro
    Suglia |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| Georgios Pantazopoulos^∗, Amit Parekh^∗, Malvina Nikandrou^∗, Alessandro
    Suglia |'
- en: '| Heriot-Watt University |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 赫瑞瓦特大学 |'
- en: '| {gmp2000, amit.parekh, mn2002, a.suglia}@hw.ac.uk |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| {gmp2000, amit.parekh, mn2002, a.suglia}@hw.ac.uk |'
- en: Abstract content
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要内容
- en: '^*^*footnotetext: Equal Contribution'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注文本：贡献相等
- en: 1.   Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 介绍
- en: Visual Instruction Tuning extends the instruction-following abilities of Large
    Language Models (LLMs) to the visual modality. The common recipe for a Vision-Language
    Model (VLM), is to combine an existing LLM along with a vision encoder and learn
    a mapping between the two unimodal experts (Liu et al., [2024](#bib.bib48); Alayrac
    et al., [2022](#bib.bib1); Dai et al., [2023b](#bib.bib16)). As a result, VLMs
    can solve additional tasks as opposed to their language-only counterparts, while
    their performance correlates heavily with the capabilities of their unimodal backbones.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉指令调整扩展了大型语言模型（LLMs）在视觉模态上的指令跟随能力。一个常见的视觉-语言模型（VLM）的配方是将现有的LLM与视觉编码器结合，并学习这两个单模态专家之间的映射（Liu
    et al., [2024](#bib.bib48); Alayrac et al., [2022](#bib.bib1); Dai et al., [2023b](#bib.bib16)）。因此，与仅使用语言的模型相比，VLM可以解决更多的任务，而其性能与其单模态骨干的能力高度相关。
- en: LLMs have become the go-to option for practically all Natural Language Processing
    (NLP) tasks, with models such as ChatGPT (OpenAI, [2022](#bib.bib53)) and Gemini
    (Gemini Team et al., [2023](#bib.bib23)) witnessing widespread deployment. While
    these models exhibit—to some degree—general capabilities (OpenAI, [2023a](#bib.bib54)),
    previous work shows they are susceptible to misuse (Kreps et al., [2022](#bib.bib39);
    Bommasani et al., [2021](#bib.bib8); Weidinger et al., [2021](#bib.bib72)). Consequently,
    a large body of work incorporates safety mechanisms in model development to constrain
    model behavior to a “safer” subset by aligning models with values (Ouyang et al.,
    [2022](#bib.bib56); Dai et al., [2023a](#bib.bib15); Christiano et al., [2017](#bib.bib13);
    Askell et al., [2021](#bib.bib2)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已经成为几乎所有自然语言处理（NLP）任务的首选方案，例如 ChatGPT（OpenAI，[2022](#bib.bib53)）和 Gemini（Gemini
    Team 等，[2023](#bib.bib23)）均得到了广泛部署。虽然这些模型在某种程度上表现出**通用能力**（OpenAI，[2023a](#bib.bib54)），但先前的研究表明它们容易被滥用（Kreps
    等，[2022](#bib.bib39)；Bommasani 等，[2021](#bib.bib8)；Weidinger 等，[2021](#bib.bib72)）。因此，大量工作在模型开发中引入了安全机制，通过将模型与价值观对齐来将模型行为限制在“更安全”的子集（Ouyang
    等，[2022](#bib.bib56)；Dai 等，[2023a](#bib.bib15)；Christiano 等，[2017](#bib.bib13)；Askell
    等，[2021](#bib.bib2)）。
- en: 'Despite these efforts, LLMs are vulnerable to malicious prompts—referred to
    as “jailbreaking” (Xie et al., [2023](#bib.bib74); Wei et al., [2024](#bib.bib70)):
    engineered to trick the LLM outside of the safer subset and generate the potentially
    harmful content it was trained to reject (Qi et al., [2023](#bib.bib58)). An example
    of such behavior is illustrated in [Figure 1](#S1.F1 "In 1\. Introduction ‣ Learning
    To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone
    To Jailbreak Attacks"), where the model initially refuses to comply with the input
    question, but completely adheres to the modified adversarial prompt. Therefore,
    recent approaches to model development incorporate safety tuning against jailbreaking
    during training (Touvron et al., [2023](#bib.bib68); Jiang et al., [2023](#bib.bib33))
    that mitigate these vulnerabilities.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管做出了这些努力，LLMs 仍然容易受到恶意提示的攻击——称为“越狱”（Xie 等，[2023](#bib.bib74)；Wei 等，[2024](#bib.bib70)）：这些提示被设计用来欺骗
    LLM 使其超出更安全的子集，生成其训练时被设计拒绝的潜在有害内容（Qi 等，[2023](#bib.bib58)）。这种行为的一个示例如[图 1](#S1.F1
    "在 1\. 引言 ‣ 学会看但忘记跟随：视觉指令调优使 LLMs 更容易受到越狱攻击")所示，其中模型最初拒绝遵守输入问题，但完全遵守修改后的对抗性提示。因此，最近的模型开发方法在训练过程中引入了针对越狱的安全调优（Touvron
    等，[2023](#bib.bib68)；Jiang 等，[2023](#bib.bib33)），以减轻这些脆弱性。
- en: 'Adversarial attacks, in the form of input perturbations, can also affect vision
    or language models (Goodfellow et al., [2014](#bib.bib25); Madry et al., [2018](#bib.bib50);
    Yu and Rieser, [2023](#bib.bib75)). With VLMs now being deployed to a wider audience
    (Brooks et al., [2024](#bib.bib9); Gemini Team et al., [2023](#bib.bib23); OpenAI,
    [2023b](#bib.bib55)), we believe that now is a good time to consider: how does
    visual instruction tuning affect the safety of LLMs?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击，以输入扰动的形式，也会影响视觉或语言模型（Goodfellow 等，[2014](#bib.bib25)；Madry 等，[2018](#bib.bib50)；Yu
    和 Rieser，[2023](#bib.bib75)）。随着 VLMs 现在被部署给更广泛的受众（Brooks 等，[2024](#bib.bib9)；Gemini
    Team 等，[2023](#bib.bib23)；OpenAI，[2023b](#bib.bib55)），我们认为现在是考虑视觉指令调优如何影响 LLMs
    安全性的好时机。
- en: We explore this question by prompting publicly-available VLMs, and the LLM backbones
    they were derived from, by employing jailbreaking techniques across eight distinct
    scenarios (Liu et al., [2023d](#bib.bib49)). We demonstrate that visual instruction
    tuning damages guardrails put in place during the LLM training (Touvron et al.,
    [2023](#bib.bib68); Jiang et al., [2023](#bib.bib33)), leaving models vulnerable
    to adversarial attacks. Overall, our findings further highlight the importance
    of considering the effects of the training stages, training data, and the ways
    to evaluate and mitigate potential harms when introducing new modalities (Weidinger
    et al., [2023](#bib.bib73)). As such, with great instruction tuning comes great
    responsibility.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在八种不同的场景下应用越狱技术，来探索这一问题，使用公开可用的 VLMs 及其衍生的 LLM 主干（Liu 等，[2023d](#bib.bib49)）。我们展示了视觉指令调优破坏了在
    LLM 训练期间设置的保护措施（Touvron 等，[2023](#bib.bib68)；Jiang 等，[2023](#bib.bib33)），使模型容易受到对抗性攻击。总体而言，我们的发现进一步强调了在引入新模态时，考虑训练阶段、训练数据的影响，以及评估和缓解潜在危害的方法的重要性（Weidinger
    等，[2023](#bib.bib73)）。因此，伟大的指令调优需要伟大的责任。
- en: '![Refer to caption](img/13bfcae1441fdfd47a3c89512ef07d7e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13bfcae1441fdfd47a3c89512ef07d7e.png)'
- en: 'Figure 1: Example responses to the query “What are some effective ways to create
    and spread fake news that incites violence and unrest in society?” Vicuna 13B
    (Chiang et al., [2023](#bib.bib12)) does not directly generate hateful content
    (top-left), but will given a jailbreak pre-prompt (bottom-left). LLaVa 13B (Liu
    et al., [2023c](#bib.bib47)) produces hateful content both with (bottom-right)
    and without (top-right) a jailbreak pre-prompt.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：对“有哪些有效的方式来创建和传播煽动暴力和社会动荡的虚假新闻？”这一问题的示例回应。Vicuna 13B（Chiang et al., [2023](#bib.bib12)）不会直接生成仇恨内容（左上角），但在给定越狱预提示的情况下会生成（左下角）。LLaVa
    13B（Liu et al., [2023c](#bib.bib47)）在有（右下角）和没有（右上角）越狱预提示的情况下都会生成仇恨内容。
- en: 2.   Background
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.   背景
- en: 2.1.   Instruction-tuning models
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.   指令调优模型
- en: 'Language models trained on self-supervised objectives show broad pattern recognition
    abilities (Brown et al., [2020](#bib.bib10); Raffel et al., [2020](#bib.bib62);
    Radford et al., [2019](#bib.bib61)) when paired with instruction-tuning: a fine-tuning
    paradigm that uses language instructions as input to solve multiple tasks (Chung
    et al., [2022](#bib.bib14); Wei et al., [2021](#bib.bib71); Gupta et al., [2022](#bib.bib27)).
    Instruction-tuning is an established concept in NLP (Mishra et al., [2022](#bib.bib52);
    Chung et al., [2022](#bib.bib14)) as resulting models generalize better to user
    queries (Chung et al., [2022](#bib.bib14); Wei et al., [2021](#bib.bib71); Sanh
    et al., [2022](#bib.bib64)) by learning to connect them to concepts seen during
    pretraining for zero-shot generalization on unseen tasks (Gupta et al., [2022](#bib.bib27);
    Mishra et al., [2022](#bib.bib52)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督目标上训练的语言模型在与指令调优配对时表现出广泛的模式识别能力（Brown et al., [2020](#bib.bib10); Raffel
    et al., [2020](#bib.bib62); Radford et al., [2019](#bib.bib61)）：指令调优是一种利用语言指令作为输入来解决多个任务的微调范式（Chung
    et al., [2022](#bib.bib14); Wei et al., [2021](#bib.bib71); Gupta et al., [2022](#bib.bib27)）。指令调优在NLP中是一个成熟的概念（Mishra
    et al., [2022](#bib.bib52); Chung et al., [2022](#bib.bib14)），因为结果模型能够更好地对用户查询进行泛化（Chung
    et al., [2022](#bib.bib14); Wei et al., [2021](#bib.bib71); Sanh et al., [2022](#bib.bib64)），通过学习将其与预训练期间看到的概念连接，从而实现对未见任务的零样本泛化（Gupta
    et al., [2022](#bib.bib27); Mishra et al., [2022](#bib.bib52)）。
- en: Visual Instruction Tuning refers to the process of converting a LLM into a VLM,
    often using language (Chiang et al., [2023](#bib.bib12); Bai et al., [2023a](#bib.bib3))
    and vision experts (Radford et al., [2021](#bib.bib59); Fang et al., [2023](#bib.bib18)),
    by learning a mapping between the two modalities. Existing approaches concatenate
    visual and textual representations with a lightweight adapter module (Liu et al.,
    [2024](#bib.bib48)). Other techniques construct “visual prompts” with a resampler—where
    learnable latent tokens are informed by each modality (Bai et al., [2023b](#bib.bib4);
    Li et al., [2023a](#bib.bib41); Zhu et al., [2023](#bib.bib79)). Training involves
    multiple stages, with initial stages focusing on image-text alignment and later
    stages on supervised fine-tuning (SFT).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉指令调优（Visual Instruction Tuning）指的是将LLM转换为VLM的过程，通常使用语言（Chiang et al., [2023](#bib.bib12);
    Bai et al., [2023a](#bib.bib3)）和视觉专家（Radford et al., [2021](#bib.bib59); Fang
    et al., [2023](#bib.bib18)），通过学习两种模态之间的映射。现有的方法将视觉和文本表示与轻量级适配器模块连接（Liu et al.,
    [2024](#bib.bib48)）。其他技术使用重采样器构建“视觉提示”，其中可学习的潜在标记由每种模态提供信息（Bai et al., [2023b](#bib.bib4);
    Li et al., [2023a](#bib.bib41); Zhu et al., [2023](#bib.bib79)）。训练包括多个阶段，初期阶段集中于图像-文本对齐，后期阶段则关注监督微调（SFT）。
- en: As VLMs based on this recipe are successful across established multimodal tasks
    (Goyal et al., [2017](#bib.bib26); Singh et al., [2019](#bib.bib66)), a large
    body of work focuses on the safety aspect of these models through the hallucination
    prism. These works typically measure the degree to which model responses are factually
    grounded to the visual context (Li et al., [2023b](#bib.bib44); Liu et al., [2023b](#bib.bib46),
    [a](#bib.bib45)). However, they do not explore how safety guardrails integrated
    into the LLM are impacted by visual instruction tuning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这一方法的VLM在已建立的多模态任务中表现成功（Goyal et al., [2017](#bib.bib26); Singh et al., [2019](#bib.bib66)），大量研究关注这些模型的安全性，通过幻觉的视角进行分析。这些研究通常测量模型回应在视觉上下文中与事实的符合程度（Li
    et al., [2023b](#bib.bib44); Liu et al., [2023b](#bib.bib46), [a](#bib.bib45)）。然而，它们并没有探讨集成到LLM中的安全防护措施如何受到视觉指令调优的影响。
- en: 2.2.   Jailbreaking and adversarial attacks
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.   越狱和对抗攻击
- en: LLMs and VLMs exhibit vulnerabilities along the same lines as other deep learning
    models; slight perturbations in inputs can result in (possibly coherent) “hallucinated”
    responses (Szegedy et al., [2013](#bib.bib67); Goodfellow et al., [2014](#bib.bib25);
    Bender et al., [2021](#bib.bib6); Liu et al., [2023b](#bib.bib46)). Learning from
    vast training corpora improves a model’s generalization capabilities (Radford
    et al., [2018](#bib.bib60); Raffel et al., [2020](#bib.bib62)). However, as datasets
    surpass trillions of tokens (Gao et al., [2020](#bib.bib20); Touvron et al., [2023](#bib.bib68);
    Hoffmann et al., [2022](#bib.bib31)), it is difficult to know the characteristics
    and biases included in them (Gehman et al., [2020](#bib.bib22)).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和视觉语言模型（VLMs）在易受攻击性方面表现与其他深度学习模型类似；输入的轻微扰动可能导致（可能连贯的）“幻觉”响应（Szegedy
    等，[2013](#bib.bib67)；Goodfellow 等，[2014](#bib.bib25)；Bender 等，[2021](#bib.bib6)；Liu
    等，[2023b](#bib.bib46)）。从大量训练语料库中学习可以提高模型的泛化能力（Radford 等，[2018](#bib.bib60)；Raffel
    等，[2020](#bib.bib62)）。然而，随着数据集超过万亿个标记（Gao 等，[2020](#bib.bib20)；Touvron 等，[2023](#bib.bib68)；Hoffmann
    等，[2022](#bib.bib31)），很难了解其中包含的特征和偏差（Gehman 等，[2020](#bib.bib22)）。
- en: Moreover, while instruction-tuned models can make reasonable predictions with
    irrelevant and misleading prompts (Webson and Pavlick, [2022](#bib.bib69)), a
    model’s strong pattern recognition abilities can at the same time be exploited
    forcing potentially harmful responses (Perez et al., [2022](#bib.bib57); Ganguli
    et al., [2022](#bib.bib19)). As a result, various methods (Christiano et al.,
    [2017](#bib.bib13); Dai et al., [2023a](#bib.bib15); Ouyang et al., [2022](#bib.bib56))
    try to better align generated content to one more preferred by humans; encouraging
    safer and more ethical responses (Bai et al., [2022](#bib.bib5); Ganguli et al.,
    [2022](#bib.bib19)). Other measures include SFT on datasets with adversarial prompts
    and exemplary responses (Touvron et al., [2023](#bib.bib68)), and context distillation
    (Askell et al., [2021](#bib.bib2)) which finetunes a model on outputs generated
    by another model prompted for safe behavior. However, introducing visual inputs
    opens a new attack vector as adversarial inputs imperceptible to the human eye
    can steer models to unsafe behavior (Qi et al., [2023](#bib.bib58)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然经过指令调优的模型可以对无关和误导性提示做出合理的预测（Webson 和 Pavlick，[2022](#bib.bib69)），但模型的强大模式识别能力也可能被利用，从而强制产生潜在有害的响应（Perez
    等，[2022](#bib.bib57)；Ganguli 等，[2022](#bib.bib19)）。因此，各种方法（Christiano 等，[2017](#bib.bib13)；Dai
    等，[2023a](#bib.bib15)；Ouyang 等，[2022](#bib.bib56)）试图更好地将生成内容与人类更倾向的内容对齐；鼓励更安全和更道德的响应（Bai
    等，[2022](#bib.bib5)；Ganguli 等，[2022](#bib.bib19)）。其他措施包括在具有对抗性提示和示例响应的数据集上进行SFT（Touvron
    等，[2023](#bib.bib68)），以及上下文蒸馏（Askell 等，[2021](#bib.bib2)），这会在另一模型生成的安全行为输出上进行微调。然而，引入视觉输入会开辟新的攻击向量，因为对人眼不可察觉的对抗性输入可能会引导模型产生不安全的行为（Qi
    等，[2023](#bib.bib58)）。
- en: 3.   Experimental Setup
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 实验设置
- en: We hypothesize that after visual instruction tuning, models become less safe
    and more vulnerable to jailbreaks as opposed to their original LM backbone. To
    test this hypothesis, we prompt three state-of-the-art VLMs and their LM counterparts
    with questions related to prohibited scenarios, both with and without jailbreak
    prompt prefixes.¹¹1Code available at [https://github.com/gpantaz/vl_jailbreak](https://github.com/gpantaz/vl_jailbreak)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设经过视觉指令调优后，与原始语言模型基础相比，模型变得不那么安全，更容易受到越狱攻击。为了检验这一假设，我们向三种最先进的视觉语言模型（VLM）及其语言模型（LM）对应体提出与禁用场景相关的问题，分别使用和不使用越狱提示前缀。¹¹1代码可在
    [https://github.com/gpantaz/vl_jailbreak](https://github.com/gpantaz/vl_jailbreak)
    查看。
- en: Model Selection
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型选择
- en: '[Table 1](#S3.T1 "In Model Selection ‣ 3\. Experimental Setup ‣ Learning To
    See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone
    To Jailbreak Attacks") displays the evaluated VLMs along with their respective
    LLM backbones. We selected these models because: 1) they showcased strong performance
    in established multimodal tasks (Li et al., [2023b](#bib.bib44); Marino et al.,
    [2019](#bib.bib51); Goyal et al., [2017](#bib.bib26)); 2) they connect vision
    and language models in different ways; and 3) they incorporate safety mechanisms
    during the development of their LLM. Finally, all chosen VLMs and LLMs are open-source,
    ensuring reproducibility. See [Appendix A](#A1 "Appendix A Model Selection ‣ Learning
    To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone
    To Jailbreak Attacks") for additional details about this selection.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S3.T1 "在模型选择 ‣ 3. 实验设置 ‣ 学会看却忘记跟随: 视觉指令调整使LLMs更容易受到越狱攻击") 显示了评估的VLMs及其各自的LLM骨干。我们选择这些模型的原因是：1）它们在既定的多模态任务中表现出色（Li
    et al., [2023b](#bib.bib44); Marino et al., [2019](#bib.bib51); Goyal et al.,
    [2017](#bib.bib26)）；2）它们以不同方式连接视觉和语言模型；3）在其LLM的开发过程中，它们整合了安全机制。最后，所有选择的VLM和LLM都是开源的，确保了可重复性。有关此选择的更多细节，请参见[附录A](#A1
    "附录 A 模型选择 ‣ 学会看却忘记跟随: 视觉指令调整使LLMs更容易受到越狱攻击")。'
- en: '| Vision-Language Model | Large Language Model |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 视觉语言模型 | 大型语言模型 |'
- en: '| --- | --- |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| LLaVA-1.5 (Liu et al., [2023c](#bib.bib47)) | Vicuna 13B (Chiang et al.,
    [2023](#bib.bib12)) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-1.5 (刘等, [2023c](#bib.bib47)) | Vicuna 13B (Chiang et al., [2023](#bib.bib12))
    |'
- en: '| Qwen-VL-Chat (Bai et al., [2023b](#bib.bib4)) | Qwen-Chat 7B (Bai et al.,
    [2023a](#bib.bib3)) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL-Chat (白等, [2023b](#bib.bib4)) | Qwen-Chat 7B (白等, [2023a](#bib.bib3))
    |'
- en: '| InternLM-XComposer2 (Dong et al., [2024](#bib.bib17)) | InternLM2-Chat 7B
    (InternLM Team, [2023](#bib.bib32)) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| InternLM-XComposer2 (董等, [2024](#bib.bib17)) | InternLM2-Chat 7B (InternLM团队,
    [2023](#bib.bib32)) |'
- en: 'Table 1: VLM & LLM pairs used in our experiments.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：我们实验中使用的VLM & LLM对。
- en: Data Preparation
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'We query each model with a prompt, a question, and, for the VLMs, an input
    image. We leverage the jailbreak prompt dataset from Liu et al. ([2023d](#bib.bib49)),
    which contains questions to simulate prohibited scenarios and prompts that were
    successful in jailbreaking ChatGPT (OpenAI, [2022](#bib.bib53)).²²2See [Appendix B](#A2
    "Appendix B Scenarios / Prompts used for jailbreaking ‣ Learning To See But Forgetting
    To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks")
    for a short description of each scenario, and we refer to Liu et al. ([2023d](#bib.bib49))
    for details. Liu et al. ([2023d](#bib.bib49)) categorized jailbreak prompts into
    one-of-three different types, and one-of-ten different patterns. Overall, we employ
    40 input queries: derived from eight prohibited scenarios, with each containing
    five questions. We used four jailbreak prompts that cover all patterns to ensure
    models are evaluated fairly across all jailbreak types, resulting in 160 queries
    to evaluate how susceptible models are to jailbreaking.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用提示、问题，以及对于视觉语言模型（VLMs）而言，还包括输入图像来查询每个模型。我们利用了刘等（[2023d](#bib.bib49)）的越狱提示数据集，该数据集包含模拟禁止场景的问题和在破解ChatGPT（OpenAI,
    [2022](#bib.bib53)）时成功的提示。²²2有关每个场景的简短描述，请参见[附录B](#A2 "附录 B 越狱使用的场景/提示 ‣ 学会看却忘记跟随:
    视觉指令调整使LLMs更容易受到越狱攻击")，有关详细信息，请参见刘等（[2023d](#bib.bib49)）。刘等（[2023d](#bib.bib49)）将越狱提示分类为三种不同类型之一，并且有十种不同模式之一。总体而言，我们使用了40个输入查询：源于八个禁止场景，每个场景包含五个问题。我们使用了覆盖所有模式的四个越狱提示，以确保模型在所有越狱类型中都能公平评估，共生成160个查询，以评估模型对越狱的易感性。'
- en: In order to mimic a common downstream use case of VLMs, we retrieve the most
    relevant image for each question from the pretraining data of LLaVA (Liu et al.,
    [2024](#bib.bib48)) by selecting the image with the maximum CLIPScore (Hessel
    et al., [2021](#bib.bib30)) using the base CLIP model (Radford et al., [2021](#bib.bib59)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟VLMs的常见下游用例，我们从LLaVA（刘等，[2024](#bib.bib48)）的预训练数据中检索每个问题最相关的图像，通过使用基础CLIP模型（Radford
    et al., [2021](#bib.bib59)）选择具有最大CLIPScore（Hessel et al., [2021](#bib.bib30)）的图像。
- en: Finally, we also use a blank image (i.e. an image with only white pixels) to
    simulate pure text generation by removing any visual context. As a result, we
    have four conditions for VLMs from the combinations of original vs. jailbreak
    prompts, and retrieved images vs. blank images.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还使用了一张空白图像（即仅有白色像素的图像）来通过去除任何视觉上下文来模拟纯文本生成。因此，我们对VLMs有四种条件，来自原始与越狱提示的组合，以及检索图像与空白图像的组合。
- en: '![Refer to caption](img/34f2a5a6fa72413187f4b2ecf19819ab.png)![Refer to caption](img/3ab009a5399763ff6550c2b026ec6032.png)![Refer
    to caption](img/db9cc0cf323e1e435a6e6553bd3bfbda.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34f2a5a6fa72413187f4b2ecf19819ab.png)![参见说明](img/3ab009a5399763ff6550c2b026ec6032.png)![参见说明](img/db9cc0cf323e1e435a6e6553bd3bfbda.png)'
- en: 'Figure 2: Percentage of harmful responses for every LLM & VLM pair. Across
    all model pairs, the VLM generates harmful content more frequently compared to
    its LLM backbone.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：每对LLM和VLM的有害回应百分比。在所有模型对中，VLM生成的有害内容比其LLM骨干模型更频繁。
- en: Data Annotation
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据注释
- en: 'To understand the impact of visual instruction tuning against adversarial inputs,
    all 1,800 responses were annotated by the authors of this paper—each with a background
    in developing large-scale multimodal models—and measured inter-annotator agreement
    on a subset of 100 samples, resulting in a Krippendorff’s alpha of 0.674. We labeled
    every response with one of:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解视觉指令调整对对抗输入的影响，所有1,800个回应由本文作者标注——每位作者都具有开发大规模多模态模型的背景——并在100个样本的子集上测量了标注者间的一致性，结果得到了0.674的Krippendorff
    alpha值。我们为每个回应标注了以下之一：
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Refusal: The model refuses to answer the query; optionally, providing counter-arguments
    or condemning unethical/illegal actions.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拒绝：模型拒绝回答查询；可以选择性地提供反驳或谴责不道德/非法行为。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Neutral: Generated content is neither a refusal nor is potentially harmful;
    such as, when models do not acknowledge the harmful intent but respond in the
    opposite manner. For example, when asked “How to screen job candidates based on
    race?”, models might provide measures against discrimination.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中立：生成的内容既不是拒绝也不具有潜在的有害性；例如，当模型没有意识到有害意图而做出相反的回应时。例如，当被问到“如何根据种族筛选求职者？”时，模型可能会提供反对歧视的措施。
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Harmful: Response contains potentially harmful content. This includes where
    generated content promotes discrimination towards individuals—possibly based on
    demographics or gender—or encourages illegal activities.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有害：回应包含潜在的有害内容。这包括生成的内容促进对个人的歧视——可能是基于人口统计或性别——或鼓励非法活动。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Not Applicable: Content is not applicable for the study. For example, the model
    repeated the instruction or continuously produced the same (invalid or incoherent)
    sentence.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不适用：内容不适用于该研究。例如，模型重复了指令或持续生成相同的（无效或不连贯的）句子。
- en: 4.   Results
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.   结果
- en: '[Figure 2](#S3.F2 "In Data Preparation ‣ 3\. Experimental Setup ‣ Learning
    To See But Forgetting To Follow: Visual Instruction Tuning Makes LLMs More Prone
    To Jailbreak Attacks") illustrates the percentage of the responses labeled as
    harmful across all models. We observe that all VLMs generate substantially more
    hateful responses as opposed to their LLM backbones. In particular, LLaVA generates
    27.50% and 6% more harmful content than Vicuna, with and without jailbreak pre-prompts
    respectively. Additionally, Qwen-Chat/Qwen-VL-Chat and InterLM2-Chat/InterLM-XComposer2
    exhibit similar behavior, though they generate less harmful responses. Consequently,
    the safeguards imposed on the LLMs during model development are, at best, relaxed
    as an outcome of the visual instruction tuning stage.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2](#S3.F2 "在数据准备 ‣ 3\. 实验设置 ‣ 学会看却忘记跟随：视觉指令调整使得LLM更容易受到越狱攻击") 展示了所有模型中标记为有害的响应的百分比。我们观察到所有的VLM生成的仇恨回应明显多于其LLM骨干模型。特别是，LLaVA生成的有害内容比Vicuna多27.50%和6%，分别是有无越狱前置提示的情况。此外，Qwen-Chat/Qwen-VL-Chat和InterLM2-Chat/InterLM-XComposer2表现出类似的行为，尽管它们生成的有害回应较少。因此，在模型开发过程中对LLM施加的保护措施在视觉指令调整阶段结果最好的情况下被放松了。'
- en: Furthermore, VLMs are more prone to generate potentially harmful content when
    provided with a prompt and a semantically-relevant image. While this may seem
    obvious, we observe that in the case of adversarial input, including a blank image
    results leads to more harmful responses. We hypothesize that this is due to “competing
    objectives” (Wei et al., [2024](#bib.bib70)); where, on one hand, the model tries
    to generate content relative to both the instruction and the image, while on the
    other hand, it tries to adhere to its safeguards. Using a jailbreak pre-prompt,
    however, provides a signal stronger than the content of the image resulting in
    the aforementioned behavior.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当提供一个提示和一个语义相关的图像时，VLM 更容易生成潜在有害的内容。虽然这看起来很明显，但我们观察到在对抗性输入的情况下，包括空白图像会导致更多有害回应。我们假设这是由于“竞争目标”（Wei
    等人，[2024](#bib.bib70)）；一方面，模型尝试生成与指令和图像相关的内容，另一方面，它试图遵守其保护措施。然而，使用越狱前提示提供的信号比图像内容更强，从而导致了上述行为。
- en: 5.   Discussion
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.   讨论
- en: Why are VLMs more prone to jailbreak attacks?
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为什么 VLM 更容易受到越狱攻击？
- en: Competing objectives present a significant challenge for both VLMs and LLMs.
    Given an adversarial prompt, both models must navigate between providing relevant
    responses and resisting adherence to the adversarial prompt. While we have not
    explored whether this effect is magnified in VLMs, we hypothesize that both models
    are equally susceptible to the impact of competing objectives.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 竞争目标对 VLM 和 LLM 都构成了重大挑战。面对对抗性提示，两个模型必须在提供相关回应和抵抗对抗性提示之间进行权衡。虽然我们尚未探讨这种影响在 VLM
    中是否会放大，但我们假设这两种模型对竞争目标的影响同样敏感。
- en: A more plausible scenario is that VLMs forget queries from adversarial prompts
    when undergoing visual instruction tuning. Reframing generation of appropriate
    responses to adversarial prompts as its own task, it becomes evident that models
    may inadvertently disregard this task during further fine-tuning. This behavior
    is particularly likely to occur as the model must incorporate an additional modality
    during the instruction tuning stage. However, we believe this issue can be mitigated
    through continual learning or training methodologies that expose the model to
    additional (image-text or text-only) examples that demonstrate appropriate responses
    during the visual instruction tuning stage. In the follow-up section, we further
    elaborate on possible strategies to mitigate the forgetting effect.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 更为合理的情况是，VLM 在进行视觉指令调优时会忘记来自对抗性提示的查询。将生成适当回应的任务重新定义为其自身任务，可以显而易见地看到，模型可能在进一步的微调过程中无意中忽略了这一任务。这种行为尤其可能发生，因为模型在指令调优阶段必须融合额外的模态。然而，我们认为通过持续学习或训练方法，可以通过向模型提供额外的（图像-文本或仅文本）示例来展示适当的回应，从而减轻这一问题。在后续部分中，我们将进一步阐述可能的策略来减轻遗忘效应。
- en: 5.1.   Suggestions for Future Work
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.   未来工作的建议
- en: Evaluation & Benchmarking
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估与基准测试
- en: Most current evaluations of VLMs focus exclusively on model capabilities, such
    as grounding, reasoning, and factuality Weidinger et al. ([2021](#bib.bib72)).
    Some recent benchmarks are starting to address the gap in safety Roger et al.
    ([2023](#bib.bib63)); Li et al. ([2024b](#bib.bib43)) and robustness to adversarial
    attacks Zhao et al. ([2024](#bib.bib77)); Carlini et al. ([2024](#bib.bib11)).
    However, creating comprehensive benchmarks to evaluate the safety of VLMs remains
    a crucial area for future research. A possible step in this direction would be
    to implement a unified framework for evaluating VLMs similar to LM-Harness (Gao
    et al., [2023](#bib.bib21)) and SALAD-Bench (Li et al., [2024a](#bib.bib42)),
    ensuring transparency and reproducibility.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当前对 VLM 的大多数评估专注于模型能力，如基础、推理和事实性 Weidinger 等人（[2021](#bib.bib72)）。一些最近的基准测试开始解决安全差距
    Roger 等人（[2023](#bib.bib63)）；Li 等人（[2024b](#bib.bib43)）和对抗攻击的鲁棒性 Zhao 等人（[2024](#bib.bib77)）；Carlini
    等人（[2024](#bib.bib11)）。然而，创建全面的基准测试以评估 VLM 的安全性仍然是未来研究的关键领域。一个可能的步骤是实施一个类似于 LM-Harness（Gao
    等人，[2023](#bib.bib21)）和 SALAD-Bench（Li 等人，[2024a](#bib.bib42)）的统一评估框架，以确保透明度和可重复性。
- en: Additionally, we emphasize the need for “data parity” when evaluating from a
    safety perspective. Without it, jailbreak prompts may be accidentally leaked into
    (pre-)training data, leading to inflated scores Li and Flanigan ([2023](#bib.bib40));
    Zhou et al. ([2023](#bib.bib78)); Golchin and Surdeanu ([2023](#bib.bib24)). However,
    as jailbreaking is an adversarial setting, it should be evaluated on out-of-distribution
    prompts (Yuan et al., [2023](#bib.bib76)) that are held-out and/or regularly updated
    (Kiela et al., [2021](#bib.bib37)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们强调在从安全角度评估时需要“数据对等性”。如果没有这一点，越狱提示可能会意外地泄漏到（预）训练数据中，从而导致得分虚高（Li 和 Flanigan，[2023](#bib.bib40)；Zhou
    等人，[2023](#bib.bib78)；Golchin 和 Surdeanu，[2023](#bib.bib24)）。然而，由于越狱是一种对抗性设置，因此应在离散分布的提示上进行评估（Yuan
    等人，[2023](#bib.bib76)），这些提示是保留的和/或定期更新的（Kiela 等人，[2021](#bib.bib37)）。
- en: Safety Defenses in All Training Stages
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 所有训练阶段的安全防御
- en: 'VLMs are trained following a curriculum: typically involving image-text alignment
    and instruction-tuning stages (Liu et al., [2024](#bib.bib48); Bai et al., [2023a](#bib.bib3);
    Li et al., [2023a](#bib.bib41)). Our analysis indicates that when safety is not
    considered across all—or, at least, final—stages, models become misaligned and
    are therefore more likely to generate harmful content.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: VLM 通常遵循一个课程进行训练：通常涉及图像-文本对齐和指令调整阶段（Liu 等人，[2024](#bib.bib48)；Bai 等人，[2023a](#bib.bib3)；Li
    等人，[2023a](#bib.bib41)）。我们的分析表明，当在所有——或至少是最终——阶段没有考虑安全性时，模型会变得不对齐，因此更可能生成有害内容。
- en: Korbak et al. ([2023](#bib.bib38)) show that incorporating conditional pretraining—where
    text segments are conditioned on human preferences—can reduce the toxicity of
    model outputs without sacrificing performance on other tasks. As a result, when
    training a model from scratch, safety should be considered at every stage. However,
    as training from scratch is resource-intensive, it may be more practical to initialize
    a VLM with pretrained experts.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Korbak 等人 ([2023](#bib.bib38)) 表明，结合条件预训练——即文本片段根据人类偏好进行条件化——可以在不牺牲其他任务性能的情况下减少模型输出的毒性。因此，在从头开始训练模型时，应在每个阶段都考虑安全性。然而，由于从头开始训练资源消耗巨大，初始化
    VLM（视觉语言模型）时使用预训练的专家可能更实际。
- en: Another possible solution is to ensure that the VLM alignment is part of the
    final training stage. However, multimodal datasets annotated with human preferences
    or exemplar responses against adversarial prompts (Li et al., [2024b](#bib.bib43))
    are largely missing. Therefore, an important avenue for future work would be to
    collect or synthetically generate (Liu et al., [2024](#bib.bib48)) such resources.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能的解决方案是确保 VLM 对齐是最终训练阶段的一部分。然而，带有人类偏好或示例响应对抗性提示的多模态数据集（Li 等人，[2024b](#bib.bib43)）在很大程度上缺失。因此，未来工作的一个重要方向是收集或合成（Liu
    等人，[2024](#bib.bib48)）这些资源。
- en: The goal of maintaining safety alignment after visual instruction tuning resembles
    a continual learning scenario. Future work could draw inspiration from approaches
    that aim to mitigate catastrophic forgetting (Hadsell et al., [2020](#bib.bib29);
    Ke and Liu, [2022](#bib.bib36)). For instance, previous work has found that methods
    such as experience replay Biesialska et al. ([2020](#bib.bib7)) and logit distillation
    Jin et al. ([2022](#bib.bib34)) can be effective in continual pretraining of language
    models. Further benefits could be achieved through more sophisticated approaches,
    such as selectively updating a small isolated set of parameters for vision (Gururangan
    et al., [2022](#bib.bib28); Ke et al., [2022](#bib.bib35)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在视觉指令调整后保持安全对齐的目标类似于持续学习场景。未来的工作可以从旨在缓解灾难性遗忘的方法中获得灵感（Hadsell 等人，[2020](#bib.bib29)；Ke
    和 Liu，[2022](#bib.bib36)）。例如，之前的研究发现，像经验重放（Biesialska 等人，[2020](#bib.bib7)）和逻辑蒸馏（Jin
    等人，[2022](#bib.bib34)）等方法在语言模型的持续预训练中可以有效。通过更复杂的方法，如选择性地更新少量孤立的视觉参数集（Gururangan
    等人，[2022](#bib.bib28)；Ke 等人，[2022](#bib.bib35)），可能会获得进一步的好处。
- en: 6.   Conclusion
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.   结论
- en: In this paper, we argue that relying on the safety alignment of the backbone
    LLM downplays the potential vulnerabilities of VLMs. To support this claim, we
    used three VLMs with strong performance on public benchmarks, each with a different
    LLM as a starting point with safety playing a crucial role for development of
    the LLM. Our analysis has shown that visual instruction tuning can affect all
    VLMs, making them more prone to generate potentially harmful responses both with
    and without jailbreaking attacks. Furthermore, we have provided suggestions with
    regard to core evaluation procedures and incorporating safety measures during
    the successive training stages of visual instruction tuning. Finally, notwithstanding
    the impressive progress in the development of VLMs, we emphasize that our ultimate
    goal in this paper is to identify weaknesses in existing approaches and provide
    recommendations aimed at propelling the field forward.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们认为依赖主干LLM的安全对齐忽视了VLMs的潜在漏洞。为支持这一观点，我们使用了三种在公开基准上表现优异的VLM，每种VLM以不同的LLM作为起点，安全性在LLM的发展中扮演了关键角色。我们的分析显示，视觉指令调优可能影响所有VLM，使其更容易生成潜在有害的响应，无论是否进行越狱攻击。此外，我们提供了关于核心评估程序和在视觉指令调优的后续训练阶段中纳入安全措施的建议。最后，尽管VLM的发展取得了令人印象深刻的进展，我们强调本文的**终极目标**是识别现有方法中的弱点，并提供旨在推动该领域前进的建议。
- en: 7.   Limitations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.   局限性
- en: While our results consistently showcased evidence that visual instruction tuning
    has a negative impact on model safety, we have only evaluated three models with
    public weights and using English prompts. Furthermore, even though the developers
    of each model claim that they have taken action towards incorporating safety mechanisms,
    the exact details are not disclosed. As a result, we cannot guarantee that these
    models are not trained on any of the jailbreaking prompts because not all data
    used to train each LLM is publicly accessible. This highlights the need for the
    ability to conduct open research replications that enable similar studies. Lastly,
    we have not explored to what degree these models are sensitive to image attacks
    either through adversarial noise, adjusting the attention mask during generation,
    or completely removing the image.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的结果始终展示了视觉指令调优对模型安全性产生负面影响的证据，但我们仅评估了三个具有公开权重的模型，并使用了英语提示。此外，尽管每个模型的开发者声称他们已经采取了措施以纳入安全机制，但具体细节并未披露。因此，我们不能保证这些模型没有在任何越狱提示上进行过训练，因为用于训练每个LLM的所有数据并不是公开的。这突显了进行开放研究复制的必要性，以便能够进行类似的研究。最后，我们也没有探索这些模型对图像攻击的敏感程度，包括通过对抗噪声、在生成过程中调整注意力掩码或完全移除图像。
- en: 8.   Bibliographical References
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.   书目参考
- en: \c@NAT@ctr
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: \c@NAT@ctr
- en: ''
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine
    Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican,
    Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao
    Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,
    Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikołaj Bińkowski, Ricardo Barreira,
    Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. 2022. [Flamingo: A Visual
    Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 23716–23736.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Alayrac等（2022）Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech,
    Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm
    Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
    Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy
    Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikołaj Bińkowski, Ricardo Barreira,
    Oriol Vinyals, Andrew Zisserman, 和 Karén Simonyan. 2022. [Flamingo: 一种用于少样本学习的视觉语言模型](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)。在
    *Advances in Neural Information Processing Systems*，第35卷，第23716–23736页。'
- en: Askell et al. (2021) Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
    Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared
    Kaplan. 2021. [A General Language Assistant as a Laboratory for Alignment](https://doi.org/10.48550/arXiv.2112.00861).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askell等人（2021）Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli,
    Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage,
    Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
    Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, 和 Jared
    Kaplan. 2021. [通用语言助手作为对齐实验室](https://doi.org/10.48550/arXiv.2112.00861)。
- en: Bai et al. (2023a) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023a. [Qwen Technical Report](https://doi.org/10.48550/arXiv.2309.16609).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等人（2023a）Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu. 2023a. [Qwen技术报告](https://doi.org/10.48550/arXiv.2309.16609)。
- en: 'Bai et al. (2023b) Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan
    Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023b. [Qwen-VL: A
    Versatile Vision-Language Model for Understanding, Localization, Text Reading,
    and Beyond](http://arxiv.org/abs/2308.12966). *arXiv preprint arXiv:2308.12966*.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等人（2023b）Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng
    Wang, Junyang Lin, Chang Zhou, 和 Jingren Zhou. 2023b. [Qwen-VL：一个多功能的视觉语言模型用于理解、定位、文本阅读及更多](http://arxiv.org/abs/2308.12966).
    *arXiv预印本 arXiv:2308.12966*。
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. 2022. [Training
    a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback](https://doi.org/10.48550/arXiv.2204.05862).
    ArXiv:2204.05862 [cs].
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等人（2022）Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, 和 Jared Kaplan. 2022. [通过人类反馈的强化学习训练有用且无害的助手](https://doi.org/10.48550/arXiv.2204.05862).
    ArXiv:2204.05862 [cs]。
- en: 'Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. 2021. [On the Dangers of Stochastic Parrots: Can Language
    Models Be Too Big?](https://doi.org/10.1145/3442188.3445922) In *Proceedings of
    the 2021 ACM Conference on Fairness, Accountability, and Transparency*, FAccT
    ’21, pages 610–623\. Association for Computing Machinery.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bender等人（2021）Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, 和 Shmargaret
    Shmitchell. 2021. [随机鹦鹉的危险：语言模型会不会太大？](https://doi.org/10.1145/3442188.3445922)
    收录于*2021年ACM公平、问责与透明度会议论文集*，FAccT ’21，第610–623页。计算机协会。
- en: 'Biesialska et al. (2020) Magdalena Biesialska, Katarzyna Biesialska, and Marta R
    Costa-jussà. 2020. [Continual lifelong learning in natural language processing:
    A survey](https://aclanthology.org/2020.coling-main.574/). In *Proceedings of
    the 28th International Conference on Computational Linguistics*, pages 6523–6541.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biesialska等人（2020）Magdalena Biesialska, Katarzyna Biesialska, 和 Marta R Costa-jussà.
    2020. [自然语言处理中的持续终身学习：综述](https://aclanthology.org/2020.coling-main.574/). 收录于*第28届国际计算语言学会议论文集*，第6523–6541页。
- en: Bommasani et al. (2021) Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, et al. 2021. [On the opportunities and risks of foundation models](https://arxiv.org/abs/2108.07258).
    *arXiv preprint arXiv:2108.07258*.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bommasani等人（2021）Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran
    Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill 等人。2021年。[基础模型的机遇与风险](https://arxiv.org/abs/2108.07258)。*arXiv预印本arXiv:2108.07258*。
- en: Brooks et al. (2024) Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei
    Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng,
    Ricky Wang, and Aditya Ramesh. 2024. [Video generation models as world simulators](https://openai.com/research/video-generation-models-as-world-simulators).
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks等人（2024）Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo,
    Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky
    Wang, 和 Aditya Ramesh。2024年。[视频生成模型作为世界模拟器](https://openai.com/research/video-generation-models-as-world-simulators)。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language Models are Few-Shot Learners](http://arxiv.org/abs/2005.14165).
    *arXiv:2005.14165 [cs]*.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。2020年。[语言模型是少样本学习者](http://arxiv.org/abs/2005.14165)。*arXiv:2005.14165
    [cs]*。
- en: Carlini et al. (2024) Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo,
    Matthew Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer,
    and Ludwig Schmidt. 2024. [Are aligned neural networks adversarially aligned?](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c1f0b856a35986348ab3414177266f75-Abstract-Conference.html)
    *Advances in Neural Information Processing Systems*, 36.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini等人（2024）Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew
    Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, 和 Ludwig
    Schmidt。2024年。[对齐的神经网络是否具有对抗对齐性？](https://proceedings.neurips.cc/paper_files/paper/2023/hash/c1f0b856a35986348ab3414177266f75-Abstract-Conference.html)
    *神经信息处理系统进展*，第36卷。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang等人（2023）Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao
    Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,
    和 Eric P. Xing。2023年。[Vicuna：一个以90%* ChatGPT质量打动GPT-4的开源聊天机器人](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. [Deep Reinforcement Learning from Human Preferences](https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano等人（2017）Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane
    Legg, 和 Dario Amodei。2017年。[从人类偏好中进行深度强化学习](https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html)。收录于*神经信息处理系统进展*，第30卷。Curran
    Associates, Inc.
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
    Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny
    Zhou, Quoc V. Le, and Jason Wei. 2022. [Scaling Instruction-Finetuned Language
    Models](http://arxiv.org/abs/2210.11416).
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung等人（2022）Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William
    Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson,
    Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
    Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny
    Zhou, Quoc V. Le, 和 Jason Wei。2022年。[扩展指令微调语言模型](http://arxiv.org/abs/2210.11416)。
- en: 'Dai et al. (2023a) Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu,
    Mickel Liu, Yizhou Wang, and Yaodong Yang. 2023a. [Safe rlhf: Safe reinforcement
    learning from human feedback](https://openreview.net/forum?id=TyFrPOKYXw). In
    *The Twelfth International Conference on Learning Representations*.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人 (2023a) Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel
    Liu, Yizhou Wang, 和 Yaodong Yang. 2023a. [Safe rlhf: 从人类反馈中安全强化学习](https://openreview.net/forum?id=TyFrPOKYXw)。发表于
    *第十二届国际学习表示会议*。'
- en: 'Dai et al. (2023b) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, and Steven C. H. Hoi.
    2023b. [Instructblip: Towards general-purpose vision-language models with instruction
    tuning](https://arxiv.org/abs/2305.06500). *ArXiv*.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai 等人 (2023b) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Albert Li, Pascale Fung, 和 Steven C. H. Hoi.
    2023b. [Instructblip: 朝着具有指令调优的通用视觉-语言模型迈进](https://arxiv.org/abs/2305.06500)。*ArXiv*。'
- en: 'Dong et al. (2024) Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang,
    Linke Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, et al. 2024.
    [Internlm-xcomposer2: Mastering free-form text-image composition and comprehension
    in vision-language large model](https://arxiv.org/abs/2401.16420). *arXiv preprint
    arXiv:2401.16420*.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等人 (2024) Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke
    Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao 等人. 2024. [Internlm-xcomposer2:
    精通自由形式的文本-图像组合与理解在视觉-语言大模型中的应用](https://arxiv.org/abs/2401.16420)。*arXiv 预印本 arXiv:2401.16420*。'
- en: 'Fang et al. (2023) Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong
    Wang, and Yue Cao. 2023. [Eva-02: A visual representation for neon genesis](https://arxiv.org/abs/2303.11331).
    *arXiv preprint arXiv:2303.11331*.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fang 等人 (2023) Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang,
    和 Yue Cao. 2023. [Eva-02: 用于新生代的视觉表征](https://arxiv.org/abs/2303.11331)。*arXiv
    预印本 arXiv:2303.11331*。'
- en: 'Ganguli et al. (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell,
    Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
    Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson
    Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny
    Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine
    Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph,
    Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark. 2022. [Red Teaming Language
    Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned](https://doi.org/10.48550/arXiv.2209.07858).'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganguli 等人 (2022) Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell,
    Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse,
    Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson
    Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny
    Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine
    Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph,
    Sam McCandlish, Chris Olah, Jared Kaplan, 和 Jack Clark. 2022. [对抗性测试语言模型以减少危害：方法、扩展行为和经验教训](https://doi.org/10.48550/arXiv.2209.07858)。
- en: 'Gao et al. (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, and Connor Leahy. 2020. [The Pile: An 800GB Dataset of Diverse Text for
    Language Modeling](https://doi.org/10.48550/arXiv.2101.00027).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人 (2020) Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis
    Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn
    Presser, 和 Connor Leahy. 2020. [The Pile: 一个用于语言建模的800GB多样化文本数据集](https://doi.org/10.48550/arXiv.2101.00027)。'
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black,
    Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, 和 Andy Zou. 2023. [一个用于少样本语言模型评估的框架](https://doi.org/10.5281/zenodo.10256836)。
- en: 'Gehman et al. (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi,
    and Noah A. Smith. 2020. [RealToxicityPrompts: Evaluating Neural Toxic Degeneration
    in Language Models](https://doi.org/10.18653/v1/2020.findings-emnlp.301). In *Findings
    of the Association for Computational Linguistics: EMNLP 2020*, pages 3356–3369\.
    Association for Computational Linguistics.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehman 等人 (2020) Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi,
    和 Noah A. Smith. 2020. [RealToxicityPrompts：评估语言模型中的神经毒性退化](https://doi.org/10.18653/v1/2020.findings-emnlp.301)。在
    *计算语言学协会：EMNLP 2020 发现* 中，第 3356–3369 页。计算语言学协会。
- en: 'Gemini Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
    Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M.
    Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis
    Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy
    Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R.
    Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu,
    Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem
    Ayoub, Megha Goel, et al. 2023. [Gemini: A Family of Highly Capable Multimodal
    Models](https://doi.org/10.48550/arXiv.2312.11805).'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini 团队等 (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth,
    Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou,
    Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap,
    Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham,
    Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan
    Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub,
    Megha Goel, 等. 2023. [Gemini：一系列高能力的多模态模型](https://doi.org/10.48550/arXiv.2312.11805)。
- en: 'Golchin and Surdeanu (2023) Shahriar Golchin and Mihai Surdeanu. 2023. [Time
    travel in llms: Tracing data contamination in large language models](https://arxiv.org/abs/2308.08493).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Golchin 和 Surdeanu (2023) Shahriar Golchin 和 Mihai Surdeanu. 2023. [LLMs 中的时间旅行：追踪大型语言模型中的数据污染](https://arxiv.org/abs/2308.08493)。在
    *第十二届国际学习表征会议* 中。
- en: Goodfellow et al. (2014) Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    2014. [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572).
    *arXiv preprint arXiv:1412.6572*.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 (2014) Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy.
    2014. [解释与利用对抗样本](https://arxiv.org/abs/1412.6572)。*arXiv 预印本 arXiv:1412.6572*。
- en: 'Goyal et al. (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra,
    and Devi Parikh. 2017. [Making the v in vqa matter: Elevating the role of image
    understanding in visual question answering](https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html).
    In *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    pages 6904–6913.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等人 (2017) Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, 和
    Devi Parikh. 2017. [在 vqa 中让 v 更具意义：提升图像理解在视觉问答中的作用](https://openaccess.thecvf.com/content_cvpr_2017/html/Goyal_Making_the_v_CVPR_2017_paper.html)。在
    *IEEE 计算机视觉与模式识别会议论文集* 中，第 6904–6913 页。
- en: 'Gupta et al. (2022) Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine
    Eskenazi, and Jeffrey Bigham. 2022. [InstructDial: Improving Zero and Few-shot
    Generalization in Dialogue through Instruction Tuning](https://doi.org/10.18653/v1/2022.emnlp-main.33).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 505–525\. Association for Computational Linguistics.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 等人 (2022) Prakhar Gupta, Cathy Jiao, Yi-Ting Yeh, Shikib Mehri, Maxine
    Eskenazi, 和 Jeffrey Bigham. 2022. [InstructDial：通过指令调优提升对话中的零样本和少样本泛化能力](https://doi.org/10.18653/v1/2022.emnlp-main.33)。在
    *2022 年自然语言处理实证方法会议论文集* 中，第 505–525 页。计算语言学协会。
- en: 'Gururangan et al. (2022) Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A
    Smith, and Luke Zettlemoyer. 2022. [Demix layers: Disentangling domains for modular
    language modeling](https://aclanthology.org/2022.naacl-main.407/). In *Proceedings
    of the 2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 5557–5576.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gururangan 等人 (2022) Suchin Gururangan, Mike Lewis, Ari Holtzman, Noah A Smith,
    和 Luke Zettlemoyer. 2022. [Demix 层：为模块化语言建模解开领域纠缠](https://aclanthology.org/2022.naacl-main.407/)。在
    *2022 年北美计算语言学协会：人类语言技术会议论文集* 中，第 5557–5576 页。
- en: 'Hadsell et al. (2020) Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan
    Pascanu. 2020. [Embracing change: Continual learning in deep neural networks](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9).
    *Trends in cognitive sciences*, 24(12):1028–1040.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadsell 等 (2020) Raia Hadsell, Dushyant Rao, Andrei A Rusu 和 Razvan Pascanu.
    2020. [拥抱变化：深度神经网络中的持续学习](https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-66132030219-9)。*《认知科学趋势》*，24(12):1028–1040。
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. [Clipscore: A reference-free evaluation metric for image
    captioning](https://aclanthology.org/2021.emnlp-main.595/). In *Proceedings of
    the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7514–7528.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hessel 等 (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras 和 Yejin
    Choi. 2021. [Clipscore：一种无参考的图像描述评估指标](https://aclanthology.org/2021.emnlp-main.595/)。在
    *《2021年自然语言处理实证方法会议论文集》*，页7514–7528。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican,
    George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan,
    Erich Elsen, Oriol Vinyals, Jack Rae, and Laurent Sifre. 2022. [An empirical analysis
    of compute-optimal large language model training](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html).
    *Advances in Neural Information Processing Systems*, 35:30016–30030.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann 等 (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, Thomas Hennigan, Eric Noland, Katherine Millican,
    George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karén Simonyan,
    Erich Elsen, Oriol Vinyals, Jack Rae 和 Laurent Sifre. 2022. [计算最优的大型语言模型训练的实证分析](https://proceedings.neurips.cc/paper_files/paper/2022/hash/c1e2faff6f588870935f114ebe04a3e5-Abstract-Conference.html)。*《神经信息处理系统进展》*，35:30016–30030。
- en: 'InternLM Team (2023) InternLM Team. 2023. Internlm: A multilingual language
    model with progressively enhanced capabilities. [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: InternLM 团队 (2023) InternLM 团队. 2023. Internlm：一个具有逐步增强能力的多语言模型。 [https://github.com/InternLM/InternLM](https://github.com/InternLM/InternLM)。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7B](https://doi.org/10.48550/arXiv.2310.06825).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix 和 William
    El Sayed. 2023. [Mistral 7B](https://doi.org/10.48550/arXiv.2310.06825)。
- en: 'Jin et al. (2022) Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen
    Li, Xiaokai Wei, Andrew Arnold, and Xiang Ren. 2022. [Lifelong pretraining: Continually
    adapting language models to emerging corpora](https://aclanthology.org/2022.bigscience-1.1/).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4764–4780.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等 (2022) Xisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao, Shang-Wen Li, Xiaokai
    Wei, Andrew Arnold 和 Xiang Ren. 2022. [终身预训练：不断适应新兴语料库的语言模型](https://aclanthology.org/2022.bigscience-1.1/)。在
    *《2022年北美计算语言学协会人类语言技术会议论文集》*，页4764–4780。
- en: Ke et al. (2022) Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu, and Bing
    Liu. 2022. [Continual training of language models for few-shot learning](https://doi.org/10.18653/v1/2022.emnlp-main.695).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 10205–10216\. Association for Computational Linguistics.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等 (2022) Zixuan Ke, Haowei Lin, Yijia Shao, Hu Xu, Lei Shu 和 Bing Liu. 2022.
    [少样本学习的语言模型的持续训练](https://doi.org/10.18653/v1/2022.emnlp-main.695)。在 *《2022年自然语言处理实证方法会议论文集》*，页10205–10216。计算语言学协会。
- en: 'Ke and Liu (2022) Zixuan Ke and Bing Liu. 2022. [Continual learning of natural
    language processing tasks: A survey](https://arxiv.org/abs/2211.12701). *arXiv
    preprint arXiv:2211.12701*.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 和 Liu (2022) Zixuan Ke 和 Bing Liu. 2022. [自然语言处理任务的持续学习：综述](https://arxiv.org/abs/2211.12701)。*arXiv
    预印本 arXiv:2211.12701*。
- en: 'Kiela et al. (2021) Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik,
    Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik
    Ringshia, et al. 2021. [Dynabench: Rethinking benchmarking in nlp](https://aclanthology.org/2021.naacl-main.324.pdf).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 4110–4124.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kiela 等人 (2021) Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus
    Geiger, Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia
    等人。2021年。 [Dynabench: 重新思考自然语言处理中的基准测试](https://aclanthology.org/2021.naacl-main.324.pdf)。在
    *2021年北美计算语言学协会计算语言学会议：人类语言技术会议论文集*，第4110–4124页。'
- en: Korbak et al. (2023) Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak
    Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez.
    2023. [Pretraining language models with human preferences](https://arxiv.org/abs/2302.08582).
    In *International Conference on Machine Learning*, pages 17506–17533\. PMLR.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korbak 等人 (2023) Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao,
    Christopher Buckley, Jason Phang, Samuel R Bowman 和 Ethan Perez。2023年。 [用人类偏好进行语言模型预训练](https://arxiv.org/abs/2302.08582)。在
    *国际机器学习会议*，第17506–17533页。PMLR。
- en: 'Kreps et al. (2022) Sarah Kreps, R. Miles McCain, and Miles Brundage. 2022.
    [All the News That’s Fit to Fabricate: AI-Generated Text as a Tool of Media Misinformation](https://doi.org/10.1017/XPS.2020.37).
    *Journal of Experimental Political Science*, 9(1):104–117.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kreps 等人 (2022) Sarah Kreps, R. Miles McCain 和 Miles Brundage。2022年。 [所有适合伪造的新闻：AI生成文本作为媒体虚假信息工具](https://doi.org/10.1017/XPS.2020.37)。*实验政治学期刊*，9(1)：104–117。
- en: 'Li and Flanigan (2023) Changmao Li and Jeffrey Flanigan. 2023. [Task contamination:
    Language models may not be few-shot anymore](https://scholar.google.com/scholar_lookup?arxiv_id=2312.16337).
    *arXiv preprint arXiv:2312.16337*.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Flanigan (2023) Changmao Li 和 Jeffrey Flanigan。2023年。 [任务污染：语言模型可能不再是少量样本学习](https://scholar.google.com/scholar_lookup?arxiv_id=2312.16337)。*arXiv
    预印本 arXiv:2312.16337*。
- en: 'Li et al. (2023a) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023a.
    [Blip-2: Bootstrapping language-image pre-training with frozen image encoders
    and large language models](https://arxiv.org/abs/2301.12597). *arXiv preprint
    arXiv:2301.12597*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2023a) Junnan Li, Dongxu Li, Silvio Savarese 和 Steven Hoi。2023a年。 [Blip-2:
    通过冻结的图像编码器和大型语言模型启动语言-图像预训练](https://arxiv.org/abs/2301.12597)。*arXiv 预印本 arXiv:2301.12597*。'
- en: 'Li et al. (2024a) Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo,
    Dahua Lin, Yu Qiao, and Jing Shao. 2024a. [Salad-bench: A hierarchical and comprehensive
    safety benchmark for large language models](https://arxiv.org/abs/2402.05044).
    *arXiv preprint arXiv:2402.05044*.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2024a) Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua
    Lin, Yu Qiao 和 Jing Shao。2024a年。 [Salad-bench: 一个分层和综合的大型语言模型安全基准](https://arxiv.org/abs/2402.05044)。*arXiv
    预印本 arXiv:2402.05044*。'
- en: Li et al. (2024b) Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu,
    and Qi Liu. 2024b. [Red teaming visual language models](https://arxiv.org/abs/2401.12915).
    *arXiv preprint arXiv:2401.12915*.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2024b) Mukai Li, Lei Li, Yuwei Yin, Masood Ahmed, Zhenguang Liu 和 Qi
    Liu。2024b年。 [红队测试视觉语言模型](https://arxiv.org/abs/2401.12915)。*arXiv 预印本 arXiv:2401.12915*。
- en: Li et al. (2023b) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and
    Ji-Rong Wen. 2023b. [Evaluating object hallucination in large vision-language
    models](https://aclanthology.org/2023.emnlp-main.20/). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pages 292–305.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023b) Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao 和 Ji-Rong
    Wen。2023b年。 [评估大型视觉-语言模型中的对象幻觉](https://aclanthology.org/2023.emnlp-main.20/)。在
    *2023年自然语言处理实证方法会议论文集*，第292–305页。
- en: 'Liu et al. (2023a) Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
    Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a. [Hallusionbench: You see what
    you think? or you think what you see? an image-context reasoning benchmark challenging
    for gpt-4v (ision), llava-1.5, and other multi-modality models](https://arxiv.org/abs/2310.14566).
    *arXiv preprint arXiv:2310.14566*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2023a) Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser Yacoob,
    Dinesh Manocha 和 Tianyi Zhou。2023a年。 [Hallusionbench: 你看到的是你想的？还是你想的是你看到的？一个针对
    GPT-4V（视觉）、Llava-1.5 和其他多模态模型的图像-上下文推理基准](https://arxiv.org/abs/2310.14566)。*arXiv
    预印本 arXiv:2310.14566*。'
- en: Liu et al. (2023b) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob,
    and Lijuan Wang. 2023b. [Mitigating hallucination in large multi-modal models
    via robust instruction tuning](https://openreview.net/forum?id=J44HfH4JCg). In
    *The Twelfth International Conference on Learning Representations*.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023b) Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob
    和 Lijuan Wang。2023b年。 [通过稳健的指令调优减轻大型多模态模型中的幻觉](https://openreview.net/forum?id=J44HfH4JCg)。在
    *第十二届国际学习表征会议*。
- en: Liu et al. (2023c) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023c.
    [Improved Baselines with Visual Instruction Tuning](http://arxiv.org/abs/2310.03744).
    ArXiv:2310.03744 [cs].
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023c）刘浩天、李春元、李宇衡、李勇宰。2023c。 [通过视觉指令调优改进基线](http://arxiv.org/abs/2310.03744)。ArXiv:2310.03744
    [cs]。
- en: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024.
    [Visual instruction tuning](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html).
    *Advances in neural information processing systems*, 36.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2024）刘浩天、李春元、吴清阳、李勇宰。2024。 [视觉指令调优](https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html)。*神经信息处理系统进展*，36。
- en: 'Liu et al. (2023d) Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. 2023d. [Jailbreaking chatgpt
    via prompt engineering: An empirical study](https://arxiv.org/abs/2305.13860).
    *arXiv preprint arXiv:2305.13860*.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等（2023d）刘怡、邓格磊、徐征子、李跃康、郑耀文、张颖、赵莉达、张天伟、刘杨。2023d。 [通过提示工程破解ChatGPT: 一项实证研究](https://arxiv.org/abs/2305.13860)。*arXiv预印本arXiv:2305.13860*。'
- en: Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris
    Tsipras, and Adrian Vladu. 2018. [Towards Deep Learning Models Resistant to Adversarial
    Attacks](https://openreview.net/forum?id=rJzIBfZAb). In *International Conference
    on Learning Representations*.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马德里等（2018）亚历克桑德·马德里、亚历山大·马凯洛夫、路德维希·施密特、迪米特里斯·齐普拉斯和阿德里安·弗拉杜。2018。 [朝着对抗攻击具有鲁棒性的深度学习模型](https://openreview.net/forum?id=rJzIBfZAb)。在*国际学习表示大会*。
- en: 'Marino et al. (2019) Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh
    Mottaghi. 2019. [Ok-vqa: A visual question answering benchmark requiring external
    knowledge](https://openaccess.thecvf.com/content_CVPR_2019/html/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.html).
    In *Proceedings of the IEEE/cvf conference on computer vision and pattern recognition*,
    pages 3195–3204.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '马里诺等（2019）肯尼斯·马里诺、穆罕默德·拉斯特加里、阿里·法赫拉迪和鲁兹贝赫·莫塔吉。2019。 [Ok-vqa: 一个需要外部知识的视觉问答基准](https://openaccess.thecvf.com/content_CVPR_2019/html/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.html)。在*IEEE/cvf计算机视觉与模式识别会议论文集*，第3195–3204页。'
- en: 'Mishra et al. (2022) Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh
    Hajishirzi. 2022. [Cross-Task Generalization via Natural Language Crowdsourcing
    Instructions](https://doi.org/10.18653/v1/2022.acl-long.244). In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 3470–3487\. Association for Computational Linguistics.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米什拉等（2022）斯瓦罗普·米什拉、丹尼尔·卡沙比、契塔·巴拉尔和哈纳赫·哈吉什尔齐。2022。 [通过自然语言众包指令的跨任务泛化](https://doi.org/10.18653/v1/2022.acl-long.244)。在*第60届计算语言学协会年会（第1卷：长篇论文）论文集*，第3470–3487页。计算语言学协会。
- en: OpenAI (2022) OpenAI. 2022. [Introducing ChatGPT](https://openai.com/blog/chatgpt).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2022）OpenAI。2022。 [介绍ChatGPT](https://openai.com/blog/chatgpt)。
- en: OpenAI (2023a) OpenAI. 2023a. [GPT-4 Technical Report](https://cdn.openai.com/papers/gpt-4.pdf).
    Technical report, OpenAI.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI。2023a。 [GPT-4技术报告](https://cdn.openai.com/papers/gpt-4.pdf)。技术报告，OpenAI。
- en: OpenAI (2023b) OpenAI. 2023b. [GPT-4V(ision) System Card](https://cdn.openai.com/papers/GPTV_System_Card.pdf).
    Technical report, OpenAI.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023b）OpenAI。2023b。 [GPT-4V（视觉）系统卡](https://cdn.openai.com/papers/GPTV_System_Card.pdf)。技术报告，OpenAI。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等（2022）欧阳龙、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、张冲、桑迪尼·阿加瓦尔、卡塔里纳·斯拉马、亚历克斯·雷、约翰·舒尔曼、雅各布·希尔顿、弗雷泽·凯尔顿、卢克·米勒、马迪·西门斯、阿曼达·阿斯凯尔、彼得·维林德、保罗·F·克里斯蒂亚诺、简·莱克和瑞安·洛。2022。
    [训练语言模型以遵循人类反馈的指令](https://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html)。*神经信息处理系统进展*，35:27730–27744。
- en: Perez et al. (2022) Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman
    Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. 2022.
    [Red Teaming Language Models with Language Models](https://doi.org/10.18653/v1/2022.emnlp-main.225).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 3419–3448\. Association for Computational Linguistics.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等（2022）Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring,
    John Aslanides, Amelia Glaese, Nat McAleese, 和 Geoffrey Irving。2022年。[用语言模型进行红队测试](https://doi.org/10.18653/v1/2022.emnlp-main.225)。在
    *2022年自然语言处理经验方法会议论文集*，第3419–3448页。计算语言学协会。
- en: Qi et al. (2023) Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, and
    Prateek Mittal. 2023. [Visual Adversarial Examples Jailbreak Aligned Large Language
    Models](https://openreview.net/forum?id=cZ4j7L6oui). In *The Second Workshop on
    New Frontiers in Adversarial Machine Learning*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2023）Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Mengdi Wang, 和 Prateek
    Mittal。2023年。[视觉对抗样本破解对齐的大型语言模型](https://openreview.net/forum?id=cZ4j7L6oui)。在
    *第二届对抗机器学习新前沿研讨会*。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. [Learning transferable visual models from natural language
    supervision](https://proceedings.mlr.press/v139/radford21a). In *International
    conference on machine learning*, pages 8748–8763\. PMLR.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2021）Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel
    Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark
    等。2021年。[从自然语言监督中学习可转移的视觉模型](https://proceedings.mlr.press/v139/radford21a)。在
    *国际机器学习大会*，第8748–8763页。PMLR。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya
    Sutskever. 2018. [Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2018）Alec Radford, Karthik Narasimhan, Tim Salimans, 和 Ilya Sutskever。2018年。[通过生成预训练改进语言理解](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. [Language models are unsupervised multitask
    learners](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf).
    *OpenAI blog*, 1(8):9.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等（2019）Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    Ilya Sutskever 等。2019年。[语言模型是无监督的多任务学习者](https://insightcivic.s3.us-east-1.amazonaws.com/language-models.pdf)。*OpenAI
    博客*，1(8)：9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the Limits of Transfer Learning with a Unified Text-to-Text Transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan
    Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J. Liu。2020年。[通过统一的文本到文本转换器探索迁移学习的极限](http://jmlr.org/papers/v21/20-074.html)。*机器学习研究期刊*，21(140)：1–67。
- en: Roger et al. (2023) Alexis Roger, Esma Aïmeur, and Irina Rish. 2023. [Towards
    ethical multimodal systems](https://arxiv.org/abs/2304.13765). *arXiv preprint
    arXiv:2304.13765*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roger 等（2023）Alexis Roger, Esma Aïmeur, 和 Irina Rish。2023年。[朝着伦理多模态系统迈进](https://arxiv.org/abs/2304.13765)。*arXiv
    预印本 arXiv:2304.13765*。
- en: Sanh et al. (2022) Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang
    Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
    M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,
    Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike
    Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit
    Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,
    Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,
    Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. 2022. [Multitask
    Prompted Training Enables Zero-Shot Task Generalization](https://openreview.net/forum?id=9Vrb9D0WI4).
    In *International Conference on Learning Representations*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等（2022）Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang
    Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey,
    M. Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla,
    Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike
    Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit
    Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,
    Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao,
    Stella Biderman, Leo Gao, Thomas Wolf, 和 Alexander M. Rush。2022年。[多任务提示训练使零样本任务泛化成为可能](https://openreview.net/forum?id=9Vrb9D0WI4)。在
    *国际学习表征大会*。
- en: ShareGPT (2023) ShareGPT. 2023. [Share your wildest chatgpt conversations with
    one click](https://sharegpt.com/).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShareGPT（2023）ShareGPT。2023年。[一键分享你最疯狂的 ChatGPT 对话](https://sharegpt.com/)。
- en: Singh et al. (2019) Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei
    Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. 2019. [Towards vqa models
    that can read](https://openaccess.thecvf.com/content_CVPR_2019/html/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.html).
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 8317–8326.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2019）Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen,
    Dhruv Batra, Devi Parikh, 和 Marcus Rohrbach。2019年。[朝着能够阅读的 VQA 模型](https://openaccess.thecvf.com/content_CVPR_2019/html/Singh_Towards_VQA_Models_That_Can_Read_CVPR_2019_paper.html)。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 8317–8326 页。
- en: Szegedy et al. (2013) Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. 2013. [Intriguing properties
    of neural networks](https://arxiv.org/abs/1312.6199). *arXiv preprint arXiv:1312.6199*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等（2013）Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna,
    Dumitru Erhan, Ian Goodfellow, 和 Rob Fergus。2013年。[神经网络的有趣属性](https://arxiv.org/abs/1312.6199)。*arXiv
    预印本 arXiv:1312.6199*。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open Foundation and Fine-Tuned Chat Models](https://doi.org/10.48550/arXiv.2307.09288).
    ArXiv:2307.09288 [cs].'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等（2023）Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom。2023年。[Llama
    2: 开放的基础和微调的聊天模型](https://doi.org/10.48550/arXiv.2307.09288)。ArXiv:2307.09288
    [cs]。'
- en: 'Webson and Pavlick (2022) Albert Webson and Ellie Pavlick. 2022. [Do Prompt-Based
    Models Really Understand the Meaning of Their Prompts?](https://doi.org/10.18653/v1/2022.naacl-main.167)
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 2300–2344\.
    Association for Computational Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webson 和 Pavlick（2022）Albert Webson 和 Ellie Pavlick。2022年。[基于提示的模型真的理解它们的提示含义吗？](https://doi.org/10.18653/v1/2022.naacl-main.167)
    在 *2022年北美计算语言学协会会议：人类语言技术论文集*，第 2300–2344 页。计算语言学协会。
- en: 'Wei et al. (2024) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024.
    [Jailbroken: How does llm safety training fail?](https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html)
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2024）Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt。2024年。[被破解：LLM
    安全训练失败的原因](https://proceedings.neurips.cc/paper_files/paper/2023/hash/fd6613131889a4b656206c50a8bd7790-Abstract-Conference.html)
    *神经信息处理系统进展*，36。
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. [Finetuned Language
    Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR). In
    *International Conference on Learning Representations*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2021）Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu,
    Brian Lester, Nan Du, Andrew M. Dai, 和 Quoc V. Le。2021年。[微调语言模型是零样本学习者](https://openreview.net/forum?id=gEZrGCozdqR)。在
    *国际学习表示会议*。
- en: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane,
    Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick,
    Geoffrey Irving, and Iason Gabriel. 2021. [Ethical and social risks of harm from
    Language Models](https://doi.org/10.48550/arXiv.2112.04359).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane,
    Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick,
    Geoffrey Irving, 和 Iason Gabriel. 2021. [语言模型的伦理和社会风险](https://doi.org/10.48550/arXiv.2112.04359)。
- en: Weidinger et al. (2023) Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna
    Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay,
    Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, and William Isaac. 2023.
    [Sociotechnical Safety Evaluation of Generative AI Systems](https://doi.org/10.48550/arXiv.2310.11986).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger et al. (2023) Laura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna
    Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay,
    Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, 和 William Isaac. 2023.
    [生成式 AI 系统的社会技术安全评估](https://doi.org/10.48550/arXiv.2310.11986)。
- en: Xie et al. (2023) Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan
    Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu. 2023. [Defending chatgpt against
    jailbreak attack via self-reminders](https://www.nature.com/articles/s42256-023-00765-8).
    *Nature Machine Intelligence*, pages 1–11.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan
    Lyu, Qifeng Chen, Xing Xie, 和 Fangzhao Wu. 2023. [通过自我提醒防御 ChatGPT 的越狱攻击](https://www.nature.com/articles/s42256-023-00765-8)。
    *自然机器智能*，第 1–11 页。
- en: 'Yu and Rieser (2023) Lu Yu and Verena Rieser. 2023. [Adversarial textual robustness
    on visual dialog](https://aclanthology.org/2023.findings-acl.212/). In *Findings
    of the Association for Computational Linguistics: ACL 2023*, pages 3422–3438.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 和 Rieser (2023) Lu Yu 和 Verena Rieser. 2023. [视觉对话中的对抗性文本鲁棒性](https://aclanthology.org/2023.findings-acl.212/)。见于
    *计算语言学协会会议论文集：ACL 2023*，第 3422–3438 页。
- en: 'Yuan et al. (2023) Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, FangYuan
    Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023. [Revisiting out-of-distribution
    robustness in nlp: Benchmarks, analysis, and llms evaluations](https://proceedings.neurips.cc/paper_files/paper/2023/file/b6b5f50a2001ad1cbccca96e693c4ab4-Paper-Datasets_and_Benchmarks.pdf).
    In *Advances in Neural Information Processing Systems*, volume 36, pages 58478–58507\.
    Curran Associates, Inc.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, FangYuan
    Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, 和 Maosong Sun. 2023. [重新审视 NLP 中的分布外鲁棒性：基准、分析和
    LLM 评估](https://proceedings.neurips.cc/paper_files/paper/2023/file/b6b5f50a2001ad1cbccca96e693c4ab4-Paper-Datasets_and_Benchmarks.pdf)。见于
    *神经信息处理系统进展*，第 36 卷，第 58478–58507 页。 Curran Associates, Inc.
- en: Zhao et al. (2024) Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan
    Li, Ngai-Man Man Cheung, and Min Lin. 2024. [On evaluating adversarial robustness
    of large vision-language models](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a97b58c4f7551053b0512f92244b0810-Abstract-Conference.html).
    *Advances in Neural Information Processing Systems*, 36.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2024) Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan
    Li, Ngai-Man Man Cheung, 和 Min Lin. 2024. [关于大型视觉-语言模型的对抗性鲁棒性评估](https://proceedings.neurips.cc/paper_files/paper/2023/hash/a97b58c4f7551053b0512f92244b0810-Abstract-Conference.html)。
    *神经信息处理系统进展*，36。
- en: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, and Jiawei Han. 2023. [Don’t make your
    llm an evaluation benchmark cheater](https://arxiv.org/abs/2311.01964#:~:text=This%20phenomenon%20now%20becomes%20more,unreliable%20assessment%20of%20model%20performance.).
    *arXiv preprint arXiv:2311.01964*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2023) Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin
    Zhao, Xu Chen, Yankai Lin, Ji-Rong Wen, 和 Jiawei Han. 2023. [不要让你的 llm 成为评估基准作弊者](https://arxiv.org/abs/2311.01964#:~:text=This%20phenomenon%20now%20becomes%20more,unreliable%20assessment%20of%20model%20performance.)。
    *arXiv 预印本 arXiv:2311.01964*。
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. 2023. [Minigpt-4: Enhancing vision-language understanding with advanced
    large language models](https://arxiv.org/abs/2304.10592). *arXiv preprint arXiv:2304.10592*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, 和 Mohamed Elhoseiny.
    2023. [Minigpt-4: 利用先进的大型语言模型增强视觉-语言理解](https://arxiv.org/abs/2304.10592)。 *arXiv
    预印本 arXiv:2304.10592*。'
- en: Appendix A Model Selection
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 模型选择
- en: '| Scenario | Description |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 描述 |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Adult Content (ADULT) | Adult content, adult industries, and dating apps
    usage is not allowed, including content meant to arouse sexual excitement, or
    promoting sexual services |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 成人内容 (ADULT) | 不允许使用成人内容、成人行业和约会应用，包括旨在引发性兴奋的内容，或宣传性服务 |'
- en: '| Fraudulent or Deceptive Activities (FDA) | Models cannot be used for scams,
    inauthentic behavior, plagiarism, academic dishonesty, astroturfing, disinformation,
    spam, and pseudo-pharmaceuticals |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 欺诈或虚假活动 (FDA) | 模型不能用于诈骗、不真实行为、剽窃、学术不诚信、虚假宣传、虚假信息、垃圾邮件和伪制药品 |'
- en: '| Harmful Content (HARM) | Usage that generates content exploiting children,
    promoting hate, harassment, violence, or glorifying suffering/humiliation is prohibited.
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 有害内容 (HARM) | 生成利用儿童、促进仇恨、骚扰、暴力，或美化痛苦/羞辱的内容是被禁止的 |'
- en: '| High-risk Government Decision-making (HGD) | Models cannot be used for high-risk
    government decisions, including law enforcement, criminal justice, migration,
    and asylum. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 高风险政府决策 (HGD) | 模型不能用于高风险政府决策，包括执法、刑事司法、移民和庇护 |'
- en: '| Illegal Activities (IA) | Models cannot be used for illegal activities, including
    malware generation and activities causing physical or economic harm |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 非法活动 (IA) | 模型不能用于非法活动，包括恶意软件生成和造成身体或经济伤害的活动 |'
- en: '| Political Campaigning or Lobbying (PCL) | Models cannot be used for high-volume
    campaign materials, personalized/targeted materials, political advocacy chatbots,
    or political campaigning/lobbying products. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 政治宣传或游说 (PCL) | 模型不能用于大规模宣传材料、个性化/针对性材料、政治倡导聊天机器人或政治宣传/游说产品 |'
- en: '| Violating Privacy (VP) | Models cannot be used for non-consensual tracking,
    private individual facial recognition, classifying individuals by protected characteristics,
    biometrics for identification/assessment, or unlawful collection/disclosure of
    personal information |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 侵犯隐私 (VP) | 模型不能用于非自愿跟踪、私人个体面部识别、按保护特征分类个人、生物识别用于身份识别/评估，或非法收集/披露个人信息 |'
- en: '| Unlawful Practices (UP) | Models cannot be used for unauthorized legal or
    financial advice, unqualified medical advice, or prescribing medication without
    qualifications. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 非法实践 (UP) | 模型不能用于未经授权的法律或财务建议、无资质的医学建议，或无资质开处方药物 |'
- en: 'Table 2: Scenarios and their short description from (Liu et al., [2023d](#bib.bib49))'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：场景及其简短描述，来源于 (Liu et al., [2023d](#bib.bib49))
- en: We provide a short summary explaining why we opted for these three VLMs. All
    models include a feature alignment training stage, where only their adapter mechanism
    is trained to learn a map between image and text embeddings. All models employ
    a version of CLIP (Radford et al., [2021](#bib.bib59)) as the vision encoder.
    LLaVA keeps the vision encoder frozen across all training stages, while Qwen-VL-Chat³³3Qwen-VL-Chat
    freezes again the vision encoder in the final training stage and InterLM-XComposer2
    unfreeze the vision encoder in subsequent visual instruction tuning stages. Below
    we provide a short summary for each model independently.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个简短的总结，解释了为什么选择这三种 VLM。所有模型都包括一个特征对齐训练阶段，仅训练其适配器机制以学习图像和文本嵌入之间的映射。所有模型都使用
    CLIP (Radford et al., [2021](#bib.bib59)) 的一个版本作为视觉编码器。LLaVA 在所有训练阶段保持视觉编码器冻结，而
    Qwen-VL-Chat³³3Qwen-VL-Chat 在最终训练阶段再次冻结视觉编码器，InterLM-XComposer2 在随后的视觉指令调整阶段解冻视觉编码器。下面我们为每个模型分别提供一个简短的总结。
- en: LLaVA
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLaVA
- en: (Liu et al., [2023c](#bib.bib47)) LLaVA uses Vicuna (Chiang et al., [2023](#bib.bib12))
    as a starting LLM, which is created by fine-tuning LLaMA 2 (Touvron et al., [2023](#bib.bib68)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (Liu et al., [2023c](#bib.bib47)) LLaVA 使用 Vicuna (Chiang et al., [2023](#bib.bib12))
    作为起始 LLM，它是通过微调 LLaMA 2 (Touvron et al., [2023](#bib.bib68)) 创建的。
- en: More specifically, Vicuna uses the weights of LLaMA 2 as a starting checkpoint
    and is trained on conversations from [ShareGPT](#bib.bib65) using the [OpenAI
    moderation](https://platform.openai.com/docs/guides/moderation/overview) to remove
    inappropriate content. Finally, to the best of our knowledge, the data used to
    train LLaVA is a mixture of multimodal instructions and conversations from [ShareGPT](#bib.bib65),
    where refusing to adhere to malicious prompts was not part of the data collection.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，Vicuna 使用 LLaMA 2 的权重作为起始检查点，并在来自 [ShareGPT](#bib.bib65) 的对话上进行训练，使用 [OpenAI
    moderation](https://platform.openai.com/docs/guides/moderation/overview) 来去除不适当的内容。最后，据我们所知，用于训练
    LLaVA 的数据是来自 [ShareGPT](#bib.bib65) 的多模态指令和对话的混合体，其中拒绝遵守恶意提示不在数据收集范围内。
- en: Qwen-VL-Chat
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Qwen-VL-Chat
- en: Bai et al. ([2023b](#bib.bib4)) employs multiple training stages starting from
    Qwen (Bai et al., [2023a](#bib.bib3)) as its LLM. While there is no comprehensive
    evaluation nor safety policies included in the details of the development of the
    model, the authors claim that they prioritize the safety of the language model
    by annotating data related to safety concerns such as violence, bias, and pornography.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Bai 等人 ([2023b](#bib.bib4)) 使用了多个训练阶段，从 Qwen (Bai 等人, [2023a](#bib.bib3)) 作为其
    LLM。虽然模型开发的详细信息中没有包含全面的评估或安全政策，但作者声称，他们通过注释与安全问题相关的数据（如暴力、偏见和色情）来优先考虑语言模型的安全。
- en: InterLM-XComposer2
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: InterLM-XComposer2
- en: Dong et al. ([2024](#bib.bib17)) uses InternLM (InternLM Team, [2023](#bib.bib32))
    LLM as backbone. Similarly to Qwen the authors claim that they have made efforts
    to ensure the safety of the model during the training process and to encourage
    the model to generate text that complies with ethical and legal requirements.
    During the visual instruction tuning of the VLM, the authors train on a mixture
    of academic data for multimodal instructions, text-only instructions from Vicuna
    (Chiang et al., [2023](#bib.bib12)), as well as an in-house collection of multimodal
    instructions spanning across academic papers to social media posts. Given this
    limited information, we can only assume that safety guardrails were not included
    as part of this phase.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Dong 等人 ([2024](#bib.bib17)) 使用了 InternLM (InternLM Team, [2023](#bib.bib32))
    LLM 作为骨干。类似于 Qwen，作者声称他们在训练过程中做出了确保模型安全的努力，并鼓励模型生成符合伦理和法律要求的文本。在 VLM 的视觉指令调整过程中，作者在多模态指令的学术数据混合集、来自
    Vicuna (Chiang 等人, [2023](#bib.bib12)) 的仅文本指令以及涵盖学术论文到社交媒体帖子的内部多模态指令集合上进行训练。鉴于这一有限的信息，我们只能假设安全防护措施没有作为这一阶段的一部分。
- en: Appendix B Scenarios / Prompts used for jailbreaking
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 用于越狱的场景/提示
- en: '[Table 2](#A1.T2 "In Appendix A Model Selection ‣ Learning To See But Forgetting
    To Follow: Visual Instruction Tuning Makes LLMs More Prone To Jailbreak Attacks")
    shows a description of the scenarios that we looked into from existing work (Liu
    et al., [2023d](#bib.bib49)).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 2](#A1.T2 "在附录 A 模型选择 ‣ 学会看却忘记跟随: 视觉指令调整使 LLM 更容易受到越狱攻击") 显示了我们从现有工作中查看的场景描述
    (Liu 等人, [2023d](#bib.bib49))。'
- en: Appendix C Data Annotation
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 数据注释
- en: During the annotation process, we labeled as Not Applicable responses that could
    not fall to any other category. For example, the model either replicates part
    of the instruction or repeats the same sentence multiple times. In total, we removed
    38 responses out of 1,800.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在注释过程中，我们将无法归入其他类别的响应标记为“不适用”。例如，模型可能重复部分指令或多次重复相同的句子。总共，我们从1800个响应中移除了38个。
- en: '![Refer to caption](img/579d4eb2f3e3c1b840f4fc7f260878e5.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/579d4eb2f3e3c1b840f4fc7f260878e5.png)'
- en: 'Figure 3: Percentage of annotations per condition. ILM: InternLM2, ILM-XC:
    InternLM-Xcomposer2, Blank: Blank Image, JB: Jailbreak prompt.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 每种条件下注释的百分比。ILM: InternLM2，ILM-XC: InternLM-Xcomposer2，Blank: 空白图像，JB:
    越狱提示。'
