- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:44:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QROA: A Black-Box Query-Response Optimization Attack on LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: QROA：针对LLMs的黑箱查询-响应优化攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02044](https://ar5iv.labs.arxiv.org/html/2406.02044)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.02044](https://ar5iv.labs.arxiv.org/html/2406.02044)
- en: Hussein Jawad
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 胡赛因·贾瓦德
- en: Capgemini Invent, Paris
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Capgemini Invent，巴黎
- en: hussein.jawad@capgemini.com &Nicolas J-B. Brunel
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: hussein.jawad@capgemini.com & Nicolas J-B. Brunel
- en: LaMME, ENSIIE
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LaMME，ENSIIE
- en: Capgemini Invent, Paris
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Capgemini Invent，巴黎
- en: nicolas.brunel@capgemini.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: nicolas.brunel@capgemini.com
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have surged in popularity in recent months, yet
    they possess concerning capabilities for generating harmful content when manipulated.
    This study introduces the Query-Response Optimization Attack (QROA), an optimization-based
    strategy designed to exploit LLMs through a black-box, query-only interaction.
    QROA adds an optimized trigger to a malicious instruction to compel the LLM to
    generate harmful content. Unlike previous approaches, QROA does not require access
    to the model’s logit information or any other internal data and operates solely
    through the standard query-response interface of LLMs. Inspired by deep Q-learning
    and Greedy coordinate descent, the method iteratively updates tokens to maximize
    a designed reward function. We tested our method on various LLMs such as Vicuna,
    Falcon, and Mistral, achieving an Attack Success Rate (ASR) over 80%. We also
    tested the model against Llama2-chat, the fine-tuned version of Llama2 designed
    to resist Jailbreak attacks, achieving good ASR with a suboptimal initial trigger
    seed. This study demonstrates the feasibility of generating jailbreak attacks
    against deployed LLMs in the public domain using black-box optimization methods,
    enabling more comprehensive safety testing of LLMs. The code will be made public
    on the following link: [https://github.com/qroa/qroa](https://github.com/qroa/qroa)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几个月，大型语言模型（LLMs）的受欢迎程度激增，但当被操控时，它们具有生成有害内容的令人担忧的能力。本研究介绍了查询-响应优化攻击（QROA），这是一种基于优化的策略，旨在通过黑箱、仅查询交互来利用LLMs。QROA在恶意指令中添加了一个优化触发器，以迫使LLM生成有害内容。与以前的方法不同，QROA不需要访问模型的logit信息或任何其他内部数据，而是仅通过LLM的标准查询-响应接口进行操作。受到深度Q学习和贪婪坐标下降的启发，该方法通过迭代更新令牌来最大化设计的奖励函数。我们在Vicuna、Falcon和Mistral等各种LLM上测试了我们的方法，实现了超过80%的攻击成功率（ASR）。我们还在针对Jailbreak攻击进行抗击优化的Llama2-chat模型上进行了测试，尽管使用了次优的初始触发器种子，但也取得了良好的ASR。本研究证明了使用黑箱优化方法对已部署的LLM进行越狱攻击的可行性，从而使LLM的安全测试更加全面。代码将在以下链接公开：[https://github.com/qroa/qroa](https://github.com/qroa/qroa)
- en: 1 introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, the emergence of large language models (LLMs) and their rapid
    improvements [[16](#bib.bib16)] or [[23](#bib.bib23)] has marked a transformative
    period in the fields of natural language processing, text generation, and software
    development. While the utility of these models is undeniable, their potential
    for misuse has surfaced as a critical issue. Studies have highlighted their ability
    to produce offensive content or be manipulated into performing undesirable actions
    through so-called "jailbreak" prompts [[26](#bib.bib26)]; [[27](#bib.bib27)].
    Such weaknesses gave the impetus for the development of alignment strategies to
    mitigate these risks by training models to avoid harmful outputs and reject inappropriate
    requests ([[17](#bib.bib17)]; [[4](#bib.bib4)]; [[12](#bib.bib12)]; [[30](#bib.bib30)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）的出现及其迅速进展 [[16](#bib.bib16)] 或 [[23](#bib.bib23)] 标志着自然语言处理、文本生成和软件开发领域的变革时期。尽管这些模型的效用不可否认，但它们的潜在滥用问题已经浮现为一个关键问题。研究已指出，它们能够生成攻击性内容或通过所谓的“越狱”提示被操控执行不良行为
    [[26](#bib.bib26)]; [[27](#bib.bib27)]。这些弱点促使了对齐策略的开发，以通过训练模型避免有害输出和拒绝不当请求来减轻这些风险
    ([[17](#bib.bib17)]; [[4](#bib.bib4)]; [[12](#bib.bib12)]; [[30](#bib.bib30)]。
- en: Despite these efforts, recent advancements reveal that LLMs remain vulnerable
    to hand-written and algorithmically generated sophisticated attacks that cleverly
    bypass these protective mechanisms ([[5](#bib.bib5)] , [[2](#bib.bib2)]). This
    vulnerability is particularly alarming given the models’ widespread integration
    into commercial and private sectors, with significant security implications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管进行了这些努力，最近的进展显示LLMs仍然容易受到手工编写和算法生成的复杂攻击，这些攻击巧妙地绕过了这些保护机制 ([[5](#bib.bib5)]
    , [[2](#bib.bib2)])。考虑到模型在商业和私人领域的广泛应用，这种脆弱性尤其令人担忧，具有重要的安全隐患。
- en: A notable advancement in jailbreak attacks on LLMs is the development of token-level
    optimization methods. In these methods, a specifically optimized trigger is appended
    to a malicious instruction to compel the LLM to respond in a desired manner. For
    instance, the Greedy Coordinate Gradient (GCG) optimization algorithm proposed
    by [[31](#bib.bib31)] leverages the gradient of the objective function to update
    the trigger one token at a time. However, GCG is effective only in "white box"
    scenarios where internal model details are accessible, contrasting with "black
    box" situations encountered in production environments where attackers typically
    only have access to the LLM’s output through a chatbot interface.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLMs（大规模语言模型）越狱攻击的一个显著进展是开发了基于令牌的优化方法。在这些方法中，特定优化的触发器被附加到恶意指令上，以迫使LLM以期望的方式响应。例如，[[31](#bib.bib31)]提出的贪婪坐标梯度（GCG）优化算法利用目标函数的梯度一次更新一个令牌。然而，GCG仅在“白盒”场景中有效，即模型内部细节可用，这与在生产环境中遇到的“黑盒”情况形成对比，在这些情况下，攻击者通常只能通过聊天机器人界面访问LLM的输出。
- en: 'To address the limitations of white-box attack methods in real-world scenarios,
    alternative black-box optimization attack strategies have been proposed, such
    as TAP[[15](#bib.bib15)] , PAL [[20](#bib.bib20)], and Open sesame [[14](#bib.bib14)].
    These methods do not rely on gradient access and are designed to function effectively
    with the limited information available from a chatbot interface. However, they
    also have limitations, as they require access to the logits of the LLM—a requirement
    not typically met in production environments. The central contribution of the
    paper is the introduction of the Query-Response Optimization Attack (QROA), a
    novel approach to circumvent the safeguards of LLMs, with the following claims:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决白盒攻击方法在现实世界场景中的局限性，提出了替代的黑盒优化攻击策略，如TAP[[15](#bib.bib15)]、PAL[[20](#bib.bib20)]和Open
    sesame[[14](#bib.bib14)]。这些方法不依赖于梯度访问，旨在有效利用来自聊天机器人界面的有限信息。然而，它们也有局限性，因为它们需要访问LLM的logits，而这在生产环境中通常不符合要求。本文的核心贡献是引入了Query-Response
    Optimization Attack (QROA)，这是一种新颖的方法，用于绕过LLMs的保护措施，具体声明如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: QROA is a black-box and query-only method, an optimization-based attack
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: QROA是一种黑盒且仅查询的方法，基于优化的攻击
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: it is not based on human-crafted templates, and thus allows the attack to be
    more general and flexible.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它不基于人工设计的模板，因此使攻击更加通用和灵活。
- en: 'Unlike previous strategies that may require access to the model’s internal
    logit data, QROA operates entirely through the standard query-response interface
    of LLMs. The attack leverages principles similar to those used in GCG [[31](#bib.bib31)]
    and PAL [[20](#bib.bib20)] methods, employing token-level optimization to discover
    triggers that elicit malicious behavior from the models. QROA employs reinforcement
    learning for crafting attacks on language models by:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与可能需要访问模型内部logit数据的先前策略不同，QROA完全通过LLMs的标准查询-响应接口操作。该攻击利用了与GCG[[31](#bib.bib31)]和PAL[[20](#bib.bib20)]方法类似的原理，采用令牌级优化来发现触发器，诱发模型的恶意行为。QROA利用强化学习来设计对语言模型的攻击，通过：
- en: '1.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Defining the attack as a reinforcement learning problem.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将攻击定义为强化学习问题。
- en: '2.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Designing a reward function that evaluates the attack’s effectiveness based
    on the model’s output.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 设计一个奖励函数，以根据模型的输出评估攻击的有效性。
- en: '3.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Fine-tuning the input to maximize the reward, adjusting one token at a time
    and using a Q-learning algorithm to refine a model that estimates the reward function.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过微调输入以最大化奖励，一次调整一个令牌，并使用Q学习算法来优化一个估计奖励函数的模型。
- en: 2 Related Works
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Jailbreak Prompts
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 越狱提示
- en: Jailbreak prompts represent a foundational technique in adversarial machine
    learning, particularly against LLMs. Such prompts are designed to “jailbreak”
    or coerce models into operating outside their ethical or intended boundaries.
    Significant contributions include [[5](#bib.bib5)] and [[2](#bib.bib2)], who demonstrated
    the ability to bypass LLM alignment strategies using manually crafted inputs.
    Furthermore, [[25](#bib.bib25)], [[10](#bib.bib10)], [[8](#bib.bib8)], [[31](#bib.bib31)],
    [[19](#bib.bib19)] have advanced the field by showing that adversarial prompts
    can be automatically discovered, exploiting specific model vulnerabilities to
    induce harmful or misleading outputs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 监狱突破提示代表了对抗机器学习中的一种基础技术，特别是针对LLM的技术。这些提示旨在“破解”或强迫模型在其道德或预期边界之外运行。重要的贡献包括 [[5](#bib.bib5)]
    和 [[2](#bib.bib2)]，他们展示了使用手工设计的输入绕过LLM对齐策略的能力。此外，[[25](#bib.bib25)]、[[10](#bib.bib10)]、[[8](#bib.bib8)]、[[31](#bib.bib31)]
    和 [[19](#bib.bib19)] 通过展示对抗提示可以被自动发现，利用特定的模型漏洞来诱导有害或误导性的输出，从而推动了这一领域的发展。
- en: Token-Level Optimization Attacks
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Token级优化攻击
- en: Token-level optimization attacks represent a more refined approach in the adversarial
    domain, where the goal is to discover trigger phrases that, when appended to a
    given instruction, lead the LLM to produce outputs aligned with the attacker’s
    intentions. This class of attack moves beyond simple prompt crafting to an optimization
    problem where the objective function is carefully set to maximize the model’s
    error rate in a controlled manner. [[31](#bib.bib31)] proposed a gradient-based
    discrete optimization method that builds on earlier work by Shin et al. [[21](#bib.bib21)]
    and [[10](#bib.bib10)]. This approach contrasts with traditional methods as it
    directly manipulates the model’s response through fine-grained adjustments rather
    than relying on broader prompt templates. A significant advantage of this type
    of attack is that it does not require any human-crafted templates as seeds, enhancing
    its generalizability across different and new LLMs, which makes the attack more
    adaptable and harder to defend against.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Token级优化攻击代表了对抗领域中的一种更精细的方式，其目标是发现触发短语，这些短语在附加到给定指令时，会使LLM产生符合攻击者意图的输出。这类攻击超越了简单的提示构造，转而成为一个优化问题，其中目标函数被精心设置，以控制的方式最大化模型的错误率。[[31](#bib.bib31)]
    提出了基于梯度的离散优化方法，该方法建立在Shin等人 [[21](#bib.bib21)] 和 [[10](#bib.bib10)] 之前工作的基础上。这种方法与传统方法相比，直接通过细粒度调整来操控模型的响应，而不是依赖于更广泛的提示模板。这类攻击的一个显著优势是它不需要任何人工设计的模板作为种子，从而提高了它在不同和新LLM中的普适性，使攻击更加适应并更难防御。
- en: Optimization Black-Box Attacks
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 优化黑箱攻击
- en: 'In more recent studies, black-box optimization methods arise to circumvent
    the requirement for direct model access: techniques such as the PAL attack [[20](#bib.bib20)]
    leverage a proxy model to simulate the target model, enabling attackers to refine
    their inputs based on the proxy’s feedback. Other methods, like the one proposed
    by [[14](#bib.bib14)], use fuzzing methodologies that rely on cosine similarity
    to determine the effectiveness of the input modifications. These methods highlight
    the shift toward techniques that can operate effectively without comprehensive
    access to the target model’s internal workings, reflecting a realistic attack
    scenario in many real-world applications. However, these methods still require
    access to the logits or other internal data of the model to optimize their attack
    strategies effectively.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近的研究中，黑箱优化方法应运而生，以规避对直接模型访问的要求：例如，PAL攻击 [[20](#bib.bib20)] 利用代理模型来模拟目标模型，使攻击者能够根据代理的反馈来改进其输入。其他方法，如
    [[14](#bib.bib14)] 提出的那种，使用基于余弦相似性的模糊测试方法来确定输入修改的有效性。这些方法突显了向那些能够在没有全面访问目标模型内部工作的情况下有效操作的技术的转变，反映了许多现实应用中的真实攻击场景。然而，这些方法仍然需要访问模型的logits或其他内部数据，以有效优化其攻击策略。
- en: Other automated black box methods such as [[20](#bib.bib20)] and [[29](#bib.bib29)]
    have demonstrated their effectiveness in producing jailbreaks with a small number
    of queries. However, they require the necessity of human-crafted prompt design.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其他自动化黑箱方法，如 [[20](#bib.bib20)] 和 [[29](#bib.bib29)]，已经证明它们在用少量查询生成监狱突破方面的有效性。然而，这些方法仍然需要人工设计的提示。
- en: Query-Response Optimization Attack (QROA)
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 查询-响应优化攻击 (QROA)
- en: The development of QROA adds to this body of knowledge by utilizing a query-only
    interaction model that iteratively optimizes the input based on the responses
    received from the LLM. QROA is distinct from other methods as it is purely an
    optimization technique and does not require access to logits or any internal data
    from the target model. This makes it highly applicable in scenarios where attackers
    have limited access to the target system. Through reinforcement learning strategies,
    QROA fine-tunes its attacks in a way that is both adaptable and effective, marking
    a significant advancement in the toolkit available to researchers and engineers
    to evaluate the safety of in-production LLMs and apply audits on them.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: QROA 的发展通过利用一个仅查询的交互模型来增加这一知识库，该模型基于从LLM收到的响应迭代优化输入。QROA 与其他方法不同，因为它纯粹是一个优化技术，不需要访问目标模型的
    logits 或任何内部数据。这使得它在攻击者对目标系统的访问有限的情况下具有很高的适用性。通过强化学习策略，QROA 以一种适应性强且有效的方式微调其攻击，标志着在生产中的LLM安全评估和审计工具包方面的重大进展。
- en: 3 Formalization
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 正式化
- en: In this study, we concentrate on a fixed Large Language Model (LLM) . Hence,  is
    a random variable with values in the space of sequences of tokens. Consequently,
    the likelihood of a given sequence of tokens .
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们集中在一个固定的大型语言模型（LLM）上。因此， 是一个在序列标记空间中的随机变量。因此，给定标记序列的可能性。
- en: Our objective is to "jailbreak" , we seek to identify a suffix  (the corresponding
    sequence is denoted $I+x$), it induces the LLM to exhibit the specified behavior
    without refusing to respond.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是“越狱”，我们寻求识别一个后缀（对应的序列记作 $I+x$），它促使LLM展现出指定的行为而不拒绝响应。
- en: Previous literature has often utilized a fixed Target sequence  (the initial
    instruction), to determine whether a "jailbreak" was successful. The goal has
    been to discover a trigger suffix that maximizes the likelihood of $T$ being the
    output.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的文献通常利用固定的目标序列（初始指令），以确定“越狱”是否成功。目标是发现一个触发后缀，使 $T$ 成为输出的可能性最大化。
- en: In this paper, we approach the problem more broadly by assuming the existence
    of an alignment function  if the output aligns with the instruction—thereby fulfilling
    the request without refusal, and  is fixed as a special case of this function,
    where $f(\text{Instruction},\text{Output})=\mathbb{P}(T|(I+x))$. However, in this
    paper, we will define a more comprehensive objective function that does not rely
    on a specific fixed target.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们通过假设存在一个对齐函数来更广泛地解决问题——如果输出与指令一致，则满足请求而不拒绝，并且作为该函数的特殊情况固定，即 $f(\text{Instruction},\text{Output})=\mathbb{P}(T|(I+x))$。然而，在本文中，我们将定义一个更全面的目标函数，而不依赖于特定的固定目标。
- en: 3.1 Problem setting
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题设定
- en: We consider a Language Model  that we wish the model to follow. Finally, we
    introduce the suffix , could potentially induce $G$ to produce an output that
    aligns with the malicious behavior.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个语言模型 ，我们希望该模型遵循。最后，我们引入后缀 ，它可能会诱使 $G$ 产生与恶意行为对齐的输出。
- en: 3.1.1 Objective
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 目标
- en: The primary objective is to jailbreak the language model . This suffix should
    maximize the likelihood that appending it to a given instruction  to exhibit a
    specified malicious behavior. This goal is achieved by optimizing an alignment
    function  is appended to $I$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 主要目标是越狱语言模型。该后缀应最大化将其附加到给定指令 后，以展现指定的恶意行为的可能性。这个目标通过优化一个对齐函数 来实现。
- en: For this reason, we propose to maximize the scoring function $x\mapsto S(x;I)$
    defined as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们建议最大化定义为 $x\mapsto S(x;I)$ 的评分函数
- en: '|  | $S(x,I)=\underset{t_{G}\sim G(I+x)}{\mathbb{E}}[f(I,t_{G})]$ |  | (1)
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $S(x,I)=\underset{t_{G}\sim G(I+x)}{\mathbb{E}}[f(I,t_{G})]$ |  | (1)
    |'
- en: where  generated by .  that triggers the undesirable behavior is defined as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 由 生成。触发不良行为的 是定义为
- en: '|  | $x^{*}=\underset{x}{\arg\max}\;S(x,I).$ |  | (2) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $x^{*}=\underset{x}{\arg\max}\;S(x,I).$ |  | (2) |'
- en: 3.1.2 Challenges
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 挑战
- en: 'Addressing the objective function $S(x,I)$ unveils three primary challenges:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 处理目标函数 $S(x,I)$ 揭示了三个主要挑战：
- en: '1.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Alignement function is Unknown: The exact nature of the alignment function
    . A first choice is to use a sentiment analysis model to detect the tone of the
    response: we consider that refusal or contradiction responses are generally associated
    with negative sentiments. A second choice is a Pre-trained model textual entailment
    (NLI) task that can evaluate whether an output response aligns with the input
    instruction. A third choice is to train a model on a dataset specifically designed
    for compliance detection. This would involve collecting a diverse set of instruction-response
    pairs and labeling them based on whether the response complies with or refuses
    the instruction.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对齐函数未知：对齐函数的确切性质未知。首先选择是使用情感分析模型来检测响应的语气：我们认为拒绝或矛盾的响应通常与负面情感相关。第二种选择是预训练模型文本蕴涵（NLI）任务，它可以评估输出响应是否与输入指令对齐。第三种选择是训练一个专门针对合规性检测的数据集的模型。这将涉及收集各种指令-响应对，并根据响应是否符合或拒绝指令进行标记。
- en: '2.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'The generation is Random: The biggest challenge with the full black box method
    is the randomness of LLM outputs with an unknown distribution  cannot be directly
    calculated or requires significant computational resources. Therefore, traditional
    optimization, where the objective function is deterministic, may not work.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成是随机的：全黑箱方法的最大挑战是LLM输出的随机性，其未知分布无法直接计算或需要大量计算资源。因此，传统的优化方法（目标函数是确定性的）可能不起作用。
- en: '3.'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'The gradient is not available: Operating with the LLM as a black box prevents
    access to the gradient of . This challenge necessitates alternative strategies
    to adjust $x$ without relying on gradient-based optimization methods.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 梯度不可用：将LLM视为黑箱操作会阻碍对梯度的访问。这一挑战需要替代策略来调整$x$，而不依赖于基于梯度的优化方法。
- en: 3.2 Similarity to Reinforcement Learning
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 与强化学习的相似性
- en: In our study on jailbreaking a Language Model (LM), we draw parallels to the
    concept of reinforcement learning (RL). Our scoring function , resembles the reward
    function used in RL [[18](#bib.bib18)]. Both aim to optimize an average outcome
    under uncertain conditions. In RL, the objective is to maximize expected rewards,
    which can vary based on different states and actions. Similarly, in our approach,
    $S(x,I)$ represents an expected value capturing the variability in LM responses
    to different suffixes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对语言模型（LM）越狱的研究中，我们借鉴了强化学习（RL）的概念。我们的评分函数，与RL中的奖励函数类似[[18](#bib.bib18)]。两者都旨在在不确定条件下优化平均结果。在RL中，目标是最大化预期奖励，这可能根据不同的状态和动作而有所变化。同样，在我们的方法中，$S(x,I)$表示一个期望值，捕捉LM对不同后缀的响应变异性。
- en: 'Nevertheless, our approach exhibits two key differences: the first being the
    absence of States. Unlike RL, where decisions depend on and lead to new states,
    our approach does not involve state transitions. Instead, it focuses solely on
    maximizing the alignment score , which represents textual sentences. This renders
    the action space virtually infinite and highly complex, a situation analogous
    to the problems discussed in high-dimensional RL spaces [[9](#bib.bib9)]'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们的方法展示了两个关键区别：第一个是状态的缺失。与RL不同，在RL中，决策依赖于并导致新状态，我们的方法不涉及状态转移。相反，它专注于最大化对齐评分，这代表文本句子。这使得动作空间几乎是无限的且高度复杂，类似于高维RL空间中讨论的问题[[9](#bib.bib9)]
- en: While the jailbreak problem does not incorporate state-based decisions typical
    of RL environments, optimizing a scoring function akin to a reward function provides
    valuable insights. This similarity suggests the potential usefulness of RL strategies,
    such as exploration techniques and value-based optimization, in navigating the
    complex and infinite action space involved in LM jailbreak scenarios.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然越狱问题并未结合典型的基于状态的决策，这种优化类似于奖励函数的评分函数提供了有价值的见解。这种相似性暗示了RL策略的潜在有用性，例如探索技术和基于价值的优化，在应对LM越狱场景中的复杂和无限动作空间时。
- en: '4 QROA: A Query Response Optimization Attack'
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 QROA：查询响应优化攻击
- en: In this section, we present an optimization strategy designed to identify triggers
    that minimize a scoring function. This approach incorporates elements of experience
    replay methodologies from deep Q-learning. The scoring function plays a role similar
    to the reward function in reinforcement learning as our objective is to discover
    prompts optimizing this reward. The proposed framework includes a surrogate model
    that approximates the scoring function, a selection function analogous to action
    selection in reinforcement learning for choosing new prompts, and a replay memory
    [[13](#bib.bib13)].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了一种优化策略，旨在识别最小化评分函数的触发器。这种方法结合了深度Q学习中的经验重放方法。评分函数的作用类似于强化学习中的奖励函数，因为我们的目标是发现优化这一奖励的提示。所提出的框架包括一个近似评分函数的代理模型，一个类似于强化学习中选择新提示的选择函数，以及一个重放记忆[[13](#bib.bib13)]。
- en: Choice of Alignment Function
  id: totrans-67
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对齐函数的选择
- en: In our approach to jailbreak a large language model (LLM) by appending a strategic
    suffix , the selection of an appropriate alignment function .
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们通过附加战略后缀来破解大型语言模型（LLM）的方法中，选择一个合适的对齐函数。
- en: 'For simplicity, we use only one alignment function in our main analysis. For
    further comparison with other alignment functions, please refer to the Appendix
    [C](#A3 "Appendix C Impact of the choice of alignment function ‣ QROA: A Black-Box
    Query-Response Optimization Attack on LLMs").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '为了简便起见，我们在主要分析中仅使用一个对齐函数。有关与其他对齐函数的进一步比较，请参阅附录[C](#A3 "附录 C 对齐函数选择的影响 ‣ QROA:
    一种针对LLM的黑箱查询-响应优化攻击")。'
- en: Harmful Evaluation Model .
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有害的评估模型。
- en: Surrogate Model
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代理模型
- en: 'To address the optimization problem where direct gradient access is not feasible,
    we introduce a surrogate model denoted as ). This model is designed to approximate
    the scoring function . The structure of  found in Q-learning algorithm [[18](#bib.bib18)],
    with a key difference being the absence of the state variable $s$. For a detailed
    architecture of the surrogate model, we refer to Appendix [A](#A1 "Appendix A
    Neural Network Architecture For the Surrogate Model ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '为了处理直接梯度访问不可行的优化问题，我们引入一个称为的代理模型。该模型旨在近似评分函数。该模型的结构见于Q学习算法[[18](#bib.bib18)]，主要区别在于缺少状态变量$s$。有关代理模型的详细架构，请参阅附录[A](#A1
    "附录 A 代理模型的神经网络架构 ‣ QROA: 一种针对LLM的黑箱查询-响应优化攻击")。'
- en: Experience Replay [[13](#bib.bib13)]
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 经验重放[[13](#bib.bib13)]
- en: Experience replay is a fundamental technique in reinforcement learning that
    allows learning algorithms to reuse past experiences to break the temporal correlations
    in successive training samples. For our optimization attack framework, integrating
    experience replay involves maintaining a buffer memory where each entry consists
    of a prompt . By leveraging this technique, our surrogate model can effectively
    sample from diverse historical prompts, reducing the risk of overfitting to recent
    data and promoting a more stable convergence.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 经验重放是强化学习中的一种基本技术，它允许学习算法重用过去的经验，以打破连续训练样本中的时间相关性。对于我们的优化攻击框架，整合经验重放涉及维护一个缓冲区记忆，其中每个条目包含一个提示。通过利用这一技术，我们的代理模型可以有效地从多样的历史提示中采样，减少对近期数据的过拟合风险，并促进更稳定的收敛。
- en: 4.1 Description of the Algorithm
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 算法描述
- en: Initialization
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化
- en: 'Surrogate Model Setup: Construct a neural network that serves as the surrogate
    model $m_{\theta}$.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 代理模型设置：构建一个作为代理模型的神经网络 $m_{\theta}$。
- en: 'Replay buffer Setup: Establish a replay buffer to store historical data consisting
    of trigger strings and the evaluation scores. $(D,h(x),n(x))$'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 重放缓冲区设置：建立一个重放缓冲区来存储历史数据，包括触发字符串和评估分数。$(D,h(x),n(x))$
- en: 'Starting Point: Generate an initial trigger string either randomly or from
    a predefined list of known effective triggers. This string acts as the starting
    point for the iterative optimization process.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 起始点：生成一个初始触发字符串，可以是随机生成的或来自预定义的有效触发器列表。这个字符串作为迭代优化过程的起点。
- en: 'Then we start an iterative process across several epochs:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们开始在多个时期中进行迭代过程：
- en: Selection
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 选择
- en: 'Selection of the Best Trigger String So Far: Utilize the Upper Confidence Bound
    (UCB) method for trigger selection, calculated as  is the average score of the
    trigger,  is the number of times the particular trigger has been selected. This
    method balances exploration and exploitation by considering both the performance
    and the uncertainty of each trigger [[3](#bib.bib3)].'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止最佳触发字符串的选择：使用上置信界限（UCB）方法来选择触发器，计算方式为，其中是触发器的平均得分，是特定触发器被选择的次数。该方法通过同时考虑每个触发器的表现和不确定性，平衡了探索与利用[[3](#bib.bib3)]。
- en: 'Coordinate Selection: Randomly pick a position within the trigger string. This
    position is where a token will be changed.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标选择：在触发字符串中随机选择一个位置。这个位置是一个令牌将被更改的地方。
- en: 'Token Replacement and Variant Creation: At the selected position, generate
    various new strings, each substituting the original token with a different one.
    These variants are potential new trigger strings. This step is vital for exploring
    the effectiveness of different tokens in influencing the model’s output.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌替换和变体创建：在选定的位置，生成各种新的字符串，每个字符串将原始令牌替换为不同的令牌。这些变体是潜在的新触发字符串。此步骤对探索不同令牌在影响模型输出方面的有效性至关重要。
- en: 'Surrogate Model Assessment: Run these new string variants through the surrogate
    model. The model predicts an approximation of the exact value of the scoring function.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 替代模型评估：将这些新的字符串变体通过替代模型进行运行。模型预测得分函数的精确值的近似值。
- en: 'Selection Function Application: Select the top $K$ (e.g. 100) string variants
    based on their estimated effectiveness.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 选择函数应用：根据估计的有效性选择前 $K$（例如 100）个字符串变体。
- en: Evaluation
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 评估
- en: 'Score Function Evaluation: Empirically evaluate the selected trigger strings
    by inputting them into the LLM and observing the outputs. The outputs are then
    scored using the predefined alignment functions to assess their effectiveness
    in inducing the desired behavior.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 得分函数评估：通过将选定的触发字符串输入到LLM中并观察输出，来进行经验评估。然后使用预定义的对齐函数对输出进行评分，以评估其在引发期望行为方面的有效性。
- en: 'Experience Storage: Store the results of these evaluations in the memory buffer.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 经验存储：将这些评估结果存储在内存缓冲区中。
- en: Learning
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 学习
- en: 'Sampling: a batch of experiences is randomly sampled from the replay memory
    to update the model parameters. This random sampling ensures that the learning
    process is robust and incorporates a broad range of scenarios, enabling the model
    to generalize better across different malicious inputs.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 采样：从重放记忆中随机采样一批经验，以更新模型参数。这种随机采样确保了学习过程的稳健性，并涵盖了广泛的场景，使模型能够在不同的恶意输入下更好地进行泛化。
- en: 'Optimization: Perform gradient descent on the surrogate model parameters .'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 优化：对替代模型参数进行梯度下降。
- en: 'Algorithm 1 QROA: Query Response Optimization Attack Framework'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 QROA：查询响应优化攻击框架
- en: '1:Input: Initial trigger , malicious Instruction , target model (black box)
    , batch size , maximum number of queries , maximum size of buffer 3:4: Average
    score for each trigger, track of the effectiveness of different triggers.5: Number
    of queries per trigger, track of many times each trigger has been tested6: Total
    number of queries7: buffer memory, used to resample evaluated triggers for updating
    the surrogate model8:). 13:      Generate token variants14:      Select 15:     Evaluation
    Phase:16:     for      23:     end for24:     Learning Phase:25:      Resample
    with replacement  using gradient descent on  then28:          Add to best triggers
    if condition met29:end while30:return $best\_triggers$'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '1:输入：初始触发器，恶意指令，目标模型（黑箱），批量大小，最大查询次数，最大缓冲区大小 3:4: 每个触发器的平均得分，跟踪不同触发器的有效性。5:每个触发器的查询次数，跟踪每个触发器被测试的次数6:总查询次数7:缓冲区记忆，用于重新抽样已评估的触发器以更新替代模型8:)。13:生成令牌变体14:选择15:评估阶段:16:对于
    23:结束循环24:学习阶段:25: 使用梯度下降法对 进行替换重抽样，然后28: 如果条件满足，则添加到最佳触发器中29:结束循环30:返回 $best\_triggers$'
- en: 4.2 Choosing the Best Adversarial Suffix $x^{*}$
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 选择最佳对抗后缀 $x^{*}$
- en: 'Algorithm [30](#alg1.l30 "In Algorithm 1 ‣ 4.1 Description of the Algorithm
    ‣ 4 QROA: A Query Response Optimization Attack ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs") returns a set of adversarial suffixes $x^{*}$ that
    have the potential to compel the Language Model (LLM) to generate the malicious
    output. To ensure that only triggers meeting a user-defined performance threshold
    are selected, it is crucial to apply statistical testing to these triggers. We
    use a z-test to statistically verify that the triggers exceed a defined score
    threshold with a certain confidence level. The procedure is outlined in the algorithm
    [18](#alg2.l18 "In Algorithm 2 ‣ Appendix D algorithms ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 算法[30](#alg1.l30 "在算法1 ‣ 4.1 算法描述 ‣ 4 QROA：一种查询响应优化攻击 ‣ QROA：一种黑箱查询响应优化攻击")返回一组对抗性后缀$x^{*}$，这些后缀有可能迫使语言模型（LLM）生成恶意输出。为了确保只选择符合用户定义性能阈值的触发器，应用统计测试对这些触发器进行验证是至关重要的。我们使用z检验来统计验证触发器是否超出定义的分数阈值，并具备一定的置信水平。该过程在算法[18](#alg2.l18
    "在算法2 ‣ 附录D 算法 ‣ QROA：一种黑箱查询响应优化攻击")中进行了概述。
- en: This algorithm evaluates each candidate trigger  and standard deviation . Triggers
    with a  are considered statistically significant and are added to the set of validated
    adversarial suffixes  have a statistically high likelihood of inducing the desired
    malicious behavior in the target model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法评估每个候选触发器的标准差。具有一定标准差的触发器被视为统计显著，并被添加到验证后的对抗性后缀集合中，这些触发器在目标模型中具有统计上的高概率引发所需的恶意行为。
- en: 'Top1-trigger: We consider that the Best adversarial suffix is the trigger with
    the highest z-value'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Top1-trigger：我们认为最佳对抗性后缀是z值最高的触发器。
- en: 5 Experiments
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'To evaluate our proposed attack, we use the AdvBench benchmark [[31](#bib.bib31)],
    which includes 500 instances of harmful behaviors articulated through specific
    instructions. Following the setup by PAL [[20](#bib.bib20)] and [[15](#bib.bib15)],
    we randomly selected 50 behaviors for analysis. We evaluate four models: VICUNA-1.3
    (7B) [[7](#bib.bib7)], Mistral-Instruct (7B), [[11](#bib.bib11)] FALCON-Instruct
    (7B), [[1](#bib.bib1)] LLAMA2-Chat(7B), [[24](#bib.bib24)].'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们提出的攻击方法，我们使用了AdvBench基准测试[[31](#bib.bib31)]，其中包括500个通过特定指令表达的有害行为实例。按照PAL的设置[[20](#bib.bib20)]和[[15](#bib.bib15)]，我们随机选择了50个行为进行分析。我们评估了四个模型：VICUNA-1.3
    (7B) [[7](#bib.bib7)]、Mistral-Instruct (7B) [[11](#bib.bib11)]、FALCON-Instruct
    (7B) [[1](#bib.bib1)] 和LLAMA2-Chat(7B) [[24](#bib.bib24)]。
- en: For all models, top_p is set to 0.6 and temperature to 0.9\. The LLAMA2-7B-Chat
    models have undergone explicit safety alignment using Reinforcement Learning with
    Human Feedback (RLHF), employing techniques like rejection sampling and Proximal
    Policy Optimization (PPO) [[22](#bib.bib22)]. Vicuna v1.5 (16k) is fine-tuned
    from Llama 2 with supervised instruction fine-tuning and linear RoPE scaling,
    using 125K conversations from ShareGPT.com.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有模型，top_p 设置为0.6，temperature 设置为0.9。LLAMA2-7B-Chat模型已通过使用人类反馈强化学习（RLHF）进行显式安全对齐，采用了拒绝采样和近端策略优化（PPO）[[22](#bib.bib22)]等技术。Vicuna
    v1.5 (16k) 从Llama 2经过监督指令微调和线性RoPE缩放，使用了来自ShareGPT.com的125K对话数据。
- en: Our experimental framework utilized one NVIDIA RTX 6000 Ada GPU with 48 GB of
    RAM. Following the setup in PAL [[20](#bib.bib20)] and GCG [[31](#bib.bib31)],
    we fixed the total number of queries to the target LLM at 25K to identify good
    triggers, and also show the attack success rate for different budgets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验框架使用了一台配备48 GB RAM的NVIDIA RTX 6000 Ada GPU。按照PAL [[20](#bib.bib20)] 和GCG
    [[31](#bib.bib31)] 的设置，我们将对目标LLM的总查询次数固定为25K，以识别有效触发器，并展示不同预算下的攻击成功率。
- en: 5.1 Settings
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: 5.1.1 Evaluation Metrics
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 评估指标
- en: In this study, Our primary metric is defined as the Attack Success Rate (ASR)
    i.e. the proportion of successful attacks relative to the total number of malicious
    instructions tested. For each malicious instruction, the top-ranked triggers determined
    by (QROA) is appended. This combined query is then submitted to the Large Language
    Model (LLM) multiple times to observe variability in response due to the stochastic
    nature of the LLM.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们的主要指标定义为攻击成功率（ASR），即成功攻击的比例相对于测试的恶意指令总数。对于每个恶意指令，通过(QROA)确定的排名最高的触发器会被附加。然后将这个组合查询提交给大型语言模型（LLM）多次，以观察由于LLM的随机性而导致的响应变化。
- en: Given this variability, we refine our metric to evaluate the robustness of the
    triggers over multiple trials. Specifically, we introduce , across a predetermined
    number of trials (e.g., 100).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这种变化性，我们对指标进行细化，以评估触发器在多次试验中的鲁棒性。具体来说，我们引入了，在预定的试验次数（例如，100次）下。
- en: The . Here, "Number of Instructions with Success , when tested over the specified
    number of trials. This metric helps in assessing the effectiveness and consistency
    of the attacks generated by the triggers under different conditions.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标用于衡量“成功指令的数量”，在指定试验次数下进行测试。这个指标有助于评估在不同条件下触发器生成的攻击的有效性和一致性。
- en: 'Labeling and Validation: For result labeling, we employ a fine-tuned RoBERTa
    model for classifying malicious responses, as detailed in [[28](#bib.bib28)].
    This classifier achieves a 92% accuracy rate in identifying malicious outputs,
    surpassing GPT-4’s accuracy of 87.4%. We also perform manual validation to ensure
    the reliability of this classifier. We allow each LLM to output a maximum of 70
    tokens to make the eval.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和验证：对于结果标记，我们使用经过微调的RoBERTa模型来分类恶意响应，如[[28](#bib.bib28)]所述。该分类器在识别恶意输出方面的准确率为92%，超越了GPT-4的87.4%准确率。我们还进行人工验证，以确保分类器的可靠性。我们允许每个LLM输出最多70个标记以进行评估。
- en: 5.1.2 Hyperparameters
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 超参数
- en: 'The surrogate model is optimized using batch size 1024 when sampling from the
    memory buffer, the size of the memory buffer is set to 6400, and we use the Adam
    optimizer to update the parameters with a learning rate of 0.01 and weight decay
    of 0.0001, we use the same surrogate model neural network architecture, the weights
    of the embedding layer are initialized from GPT2, we set the threshold to 0.2\.
    We use as an alignment function the Harmful Evaluation Model as described in Section
    [4](#S4.SS0.SSS0.Px1 "Choice of Alignment Function ‣ 4 QROA: A Query Response
    Optimization Attack ‣ QROA: A Black-Box Query-Response Optimization Attack on
    LLMs"). We have also tested other alignment functions such as an Entailment Evaluation
    Model (see appendix [C](#A3 "Appendix C Impact of the choice of alignment function
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs")). For more details
    about experience settings check appendix [B](#A2 "Appendix B Experience Settings
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs")'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 代理模型在从内存缓冲区采样时使用批量大小为1024，内存缓冲区的大小设置为6400，我们使用Adam优化器以0.01的学习率和0.0001的权重衰减来更新参数，我们使用相同的代理模型神经网络架构，嵌入层的权重从GPT2初始化，我们将阈值设置为0.2。我们使用在第[4](#S4.SS0.SSS0.Px1
    "对齐函数选择 ‣ 4 QROA：查询响应优化攻击 ‣ QROA：对LLMs的黑盒查询-响应优化攻击")节中描述的有害评估模型作为对齐函数。我们还测试了其他对齐函数，例如蕴涵评估模型（见附录[C](#A3
    "附录C 对齐函数选择的影响 ‣ QROA：对LLMs的黑盒查询-响应优化攻击")）。有关体验设置的更多细节，请参见附录[B](#A2 "附录B 体验设置
    ‣ QROA：对LLMs的黑盒查询-响应优化攻击")
- en: 5.2 Main results
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 5.2.1 Evaluate the effectiveness of the attack against Vicuna-7B
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 评估对Vicuna-7B攻击的有效性
- en: '| Budget | ASR@10% | ASR@20% | ASR@30% | ASR@40% | ASR@50% | ASR@60% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 预算 | ASR@10% | ASR@20% | ASR@30% | ASR@40% | ASR@50% | ASR@60% |'
- en: '| 10K | 60% | 53.3% | 43.3% | 40.62% | 36.6% | 36.6% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 10K | 60% | 53.3% | 43.3% | 40.62% | 36.6% | 36.6% |'
- en: '| 20K | 70% | 62.5% | 58% | 40% | 50% | 50.00% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 20K | 70% | 62.5% | 58% | 40% | 50% | 50.00% |'
- en: '| 25K | 80% | 73.3% | 58.3% | 58.3% | 54.1% | 50.00% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 25K | 80% | 73.3% | 58.3% | 58.3% | 54.1% | 50.00% |'
- en: '| 30K | 83.3% | 76.6% | 62.5% | 62.5% | 58.3% | 65.62% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 30K | 83.3% | 76.6% | 62.5% | 62.5% | 58.3% | 65.62% |'
- en: '| 40K | 87% | 82.6% | 75% | 50% | 55% | 65.62% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 40K | 87% | 82.6% | 75% | 50% | 55% | 65.62% |'
- en: '| 50K | 90% | 88% | 83% | 75% | 75% | 71.88% |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 50K | 90% | 88% | 83% | 75% | 75% | 71.88% |'
- en: 'Table 1: Attack Success Rates at Different Budget Levels For Vicuna'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：Vicuna在不同预算水平下的攻击成功率
- en: 'The results presented in Table [1](#S5.T1 "Table 1 ‣ 5.2.1 Evaluate the effectiveness
    of the attack against Vicuna-7B ‣ 5.2 Main results ‣ 5 Experiments ‣ QROA: A Black-Box
    Query-Response Optimization Attack on LLMs") demonstrate insights into the efficacy
    of our attack methods under varying budget constraints. As expected, a clear trend
    shows that higher budgets correlate with improved Attack Success Rates (ASR),
    indicating that more resources allow for more opportunities to fine-tune and optimize
    the attack triggers.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S5.T1 "表1 ‣ 5.2.1 评估对Vicuna-7B攻击的有效性 ‣ 5.2 主要结果 ‣ 5 实验 ‣ QROA：对LLMs的黑盒查询-响应优化攻击")中的结果展示了在不同预算限制下我们攻击方法的有效性。正如预期的那样，明显的趋势表明，较高的预算与提高的攻击成功率（ASR）相关，这表明更多的资源可以提供更多机会来微调和优化攻击触发器。
- en: 'Influence of Budget on ASR: The increase in ASR from a budget of 10K to 50K
    queries is substantial, rising from 60% to 90% at the 10% threshold. This indicates
    that additional queries provide valuable data that refine the attack vectors and
    improve their effectiveness.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 预算对ASR的影响：从10K到50K查询的预算增加显著，10%阈值下ASR从60%上升到90%。这表明额外的查询提供了有价值的数据，细化了攻击向量并提高了其有效性。
- en: 'Impact of Model Variability: The non-deterministic nature of LLMs, as evidenced
    by the varied ASRs at the same budget levels but different thresholds, underscores
    the challenge of predicting and controlling the behavior of these models in black
    box adversarial settings. This variability also emphasizes the importance of considering
    multiple thresholds when evaluating the robustness of LLMs against attacks.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 模型变异性的影响：大型语言模型（LLMs）的非确定性特征，如在相同预算水平但不同阈值下的ASR差异，突显了在黑箱对抗环境中预测和控制这些模型行为的挑战。这种变异性还强调了在评估LLMs对攻击的鲁棒性时考虑多个阈值的重要性。
- en: 5.2.2 QROA on other models
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 QROA在其他模型上的应用
- en: To extend the evaluation of the Query-Response Optimization Algorithm (QROA)
    across different models, here is a table with ASR@20% values at a budget of 25K
    queries for various large language models (LLMs).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了扩展对不同模型的查询-响应优化算法（QROA）的评估，以下是一个表格，展示了在25K查询预算下各种大型语言模型（LLMs）的ASR@20%值。
- en: '| Model | VICUNA | FALCON | MISTRAL |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | VICUNA | FALCON | MISTRAL |'
- en: '| ASR@20% | 73.3% | 98% | 98% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ASR@20% | 73.3% | 98% | 98% |'
- en: 'Table 2: Attack Success Rates at 20% Threshold for Different Models at a 25K
    query budget'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在25K查询预算下，针对不同模型的20%阈值的攻击成功率
- en: 5.2.3 Attack against LLama2-7B-chat
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 对LLama2-7B-chat的攻击
- en: Evaluating the attack’s effectiveness against the LLama2-7B-chat [[24](#bib.bib24)]
    model presented unique challenges due to its robust security alignment and consistent
    response behavior. Starting with random triggers as initial conditions, we encountered
    limitations with the model’s consistent responses, typically stating, "I cannot
    fulfill your request." This response limited the variability of the scoring function,
    often returning a zero value, indicating an almost flat response landscape in
    the tested region, this is known as the hard exploration problem [[6](#bib.bib6)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 评估对LLama2-7B-chat [[24](#bib.bib24)] 模型的攻击效果面临独特挑战，由于其强大的安全对齐和一致的响应行为。起初使用随机触发器作为初始条件时，我们遇到了模型一致响应的限制，通常表示“我不能满足你的请求。”这种响应限制了评分函数的变异性，通常返回零值，表明在测试区域内几乎没有变化，这被称为困难探索问题
    [[6](#bib.bib6)]。
- en: To address this, we extended the trigger length from 10 to 20 tokens, allowing
    exploration of a broader range of token sequences. Additionally, we replaced the
    initial embedding layer with embeddings from the lmsys/vicuna-7b-v1.5-16k model
    to better align with the target model’s architecture.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将触发器长度从10个令牌扩展到20个令牌，允许探索更广泛的令牌序列。此外，我们将初始嵌入层替换为来自lmsys/vicuna-7b-v1.5-16k模型的嵌入，以更好地与目标模型的架构对齐。
- en: 'Also, to enhance the efficiency of our approach, we pre-populated the memory
    buffer with successful triggers from previous successful experiments, and we used
    them in the UCB selection. Selecting successful triggers is based on Algorithm
    [18](#alg2.l18 "In Algorithm 2 ‣ Appendix D algorithms ‣ QROA: A Black-Box Query-Response
    Optimization Attack on LLMs"). We appended these triggers to the new malicious
    instruction, queried them against the target LLM, evaluated the outputs, and stored
    the results as described in the ’evaluation phase’ in Algorithm LABEL:alg1. This
    strategy ensured a more rapid convergence of the optimization algorithm by providing
    a higher-quality starting point for generating new triggers. By starting with
    10 pre-validated triggers, we significantly reduced the time and computational
    resources required to identify effective attack vectors. The procedure details
    are outlined in the algorithm [16](#alg3.l16 "In Algorithm 3 ‣ Appendix D algorithms
    ‣ QROA: A Black-Box Query-Response Optimization Attack on LLMs").'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，为了提高我们方法的效率，我们用以前成功实验中的有效触发器预填充了内存缓冲区，并在UCB选择中使用了这些触发器。选择成功触发器的基础是算法[18](#alg2.l18
    "在算法2 ‣ 附录D算法 ‣ QROA: 一种黑盒查询-响应优化攻击")。我们将这些触发器附加到新的恶意指令中，针对目标LLM进行了查询，评估了输出，并按照算法LABEL:alg1中描述的“评估阶段”存储了结果。这个策略通过提供更高质量的起始点来生成新触发器，从而确保了优化算法的更快速收敛。通过从10个预验证的触发器开始，我们显著减少了识别有效攻击向量所需的时间和计算资源。该过程的详细信息在算法[16](#alg3.l16
    "在算法3 ‣ 附录D算法 ‣ QROA: 一种黑盒查询-响应优化攻击")中概述。'
- en: 5.2.4 Comparaison with GCG & PAL
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 与GCG和PAL的比较
- en: 'In this section, we compare the effectiveness of our Query-Response Optimization
    Algorithm (QROA) against two other attack methods: white box (GCG) attack [[31](#bib.bib31)]
    and black box using logit and proxy model (PAL) [[20](#bib.bib20)]. These comparisons
    provide insights into how well each method performs under the same experimental
    conditions. We focus on two models Llama-2-7B-Chat and Vicuna-7B-v1.3, The attack
    is evaluated with a fixed budget of 25K queries, and ASR@20% is used as the primary
    metric.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查询-响应优化算法（QROA）的效果与两种其他攻击方法进行比较：白盒（GCG）攻击[[31](#bib.bib31)]和使用logit及代理模型的黑盒攻击（PAL）[[20](#bib.bib20)]。这些比较提供了在相同实验条件下，各种方法的表现如何的见解。我们重点关注两个模型Llama-2-7B-Chat和Vicuna-7B-v1.3。攻击在固定的25K查询预算下进行评估，以ASR@20%作为主要指标。
- en: 'We test QROA using the initialization described in section [5.2.3](#S5.SS2.SSS3
    "5.2.3 Attack against LLama2-7B-chat ‣ 5.2 Main results ‣ 5 Experiments ‣ QROA:
    A Black-Box Query-Response Optimization Attack on LLMs") for both Llama2-Chat
    and Vicuna: this serves to accelerate the algorithm and improve efficiency and
    effectiveness of the attack.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用第[5.2.3](#S5.SS2.SSS3 "5.2.3 攻击Llama2-7B-chat ‣ 5.2 主要结果 ‣ 5 实验 ‣ QROA:
    一种黑盒查询-响应优化攻击")节中描述的初始化对Llama2-Chat和Vicuna进行了QROA测试：这有助于加速算法并提高攻击的效率和效果。'
- en: PAL was not evaluated on Vicuna-7B as Vicuna is used as a proxy model in PAL
    attacks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Vicuna在PAL攻击中作为代理模型使用，因此PAL在Vicuna-7B上未进行评估。
- en: '| Attack | Llama-2-7B | Vicuna-7B |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | Llama-2-7B | Vicuna-7B |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GCG | 56 | 86 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 56 | 86 |'
- en: '| PAL | 48 | - |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| PAL | 48 | - |'
- en: '| QROA | 82 | 82 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| QROA | 82 | 82 |'
- en: 'Table 3: ASR@20% for Different Attack Methods on Llama-2-7B-Chat and Vicuna-7B
    at a 25K Query Budget'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在25K查询预算下，Llama-2-7B-Chat和Vicuna-7B上不同攻击方法的ASR@20%
- en: 6 Limitations
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: 'Dependency on Alignment Function Precision: The effectiveness of QROA relies
    on the accuracy and appropriateness of the alignment function $f$, which assesses
    the LLM’s output compliance with malicious intent. The method assumes that the
    alignment function can be accurately modeled or approximated, which may not hold
    true in practical scenarios.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对对齐函数精度的依赖：QROA的有效性依赖于对齐函数$f$的准确性和适当性，该函数评估LLM输出是否符合恶意意图。该方法假设对齐函数可以被准确建模或逼近，这在实际场景中可能不成立。
- en: 'Computational and Resource Intensity: The approach involves generating, evaluating,
    and refining numerous suffix variations to identify effective triggers. This process
    can be computationally and resource-intensive.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和资源密集度：该方法涉及生成、评估和优化大量后缀变体，以识别有效的触发器。这个过程可能会非常计算密集和资源密集。
- en: 'Necessity of Initialization Step for Some LLMs: As stated in the previous section,
    we encountered significant challenges, particularly during our attempts to attack
    LLAMA2 chat. To address this, we facilitated the algorithm’s initialization to
    avoid starting in regions where the scoring is nearly flat. This was achieved
    by incorporating previously successful triggers into the memory buffer and the
    UCB selection under LLAMA2\. These triggers were either discovered by QROA or
    other optimization algorithms (GCG, PAL).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对某些大型语言模型（LLMs）初始化步骤的必要性：如前所述，我们遇到了重大挑战，特别是在尝试攻击LLAMA2聊天时。为了解决这个问题，我们对算法的初始化进行了优化，以避免在评分几乎平坦的区域开始。这是通过将之前成功的触发器纳入内存缓冲区以及LLAMA2下的UCB选择来实现的。这些触发器要么是通过QROA发现的，要么是通过其他优化算法（GCG、PAL）发现的。
- en: 7 Conclusion and Future Works
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: QROA is a novel method for exploiting Large Language Models (LLMs) through black-box,
    query-only interaction. Unlike previous literature, methods requiring internal
    model access or crafted templates, QROA works solely through the query-response
    interface, making it highly applicable in real-world scenarios. Using reinforcement
    learning and iterative token-level optimization, QROA identifies triggers that
    induce malicious behavior in LLMs. Experiments on models such as Vicuna, Mistral,
    Falcon, and LLama2-Chat show QROA’s high efficacy, achieving over 80% Attack Success
    Rate (ASR) even on models fine-tuned for resistance. This highlights significant
    security implications for LLM deployment in commercial and private sectors, as
    it exposes critical weaknesses of many industrial LLM. In addition, QROA with
    higher query budgets has improved ASR, emphasizing the need for understanding
    risks and developing better defense mechanisms. QROA advances the evaluation of
    LLM safety, demonstrating the need for robust alignment strategies to ensure reliability
    and safety in real-world applications.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: QROA是一种通过黑盒、仅查询交互来利用大型语言模型（LLMs）的新方法。与需要内部模型访问或特定模板的方法不同，QROA仅通过查询-响应接口工作，使其在实际场景中具有高度的适用性。通过强化学习和迭代令牌级优化，QROA识别出诱发LLMs恶意行为的触发器。对Vicuna、Mistral、Falcon和LLama2-Chat等模型的实验显示，QROA具有很高的有效性，即使在针对抵抗进行微调的模型上也能实现超过80%的攻击成功率（ASR）。这突显了LLM在商业和私人部门部署中的重大安全隐患，因为它暴露了许多工业LLM的关键弱点。此外，具有更高查询预算的QROA提高了ASR，强调了理解风险和开发更好防御机制的必要性。QROA推进了对LLM安全性的评估，展示了在实际应用中确保可靠性和安全性的强大对齐策略的必要性。
- en: Future research could focus on enhancing the transferability of the surrogate
    model between different malicious instructions. This involves developing a model
    that can generalize its learning from one set of instructions to another without
    losing efficacy, reducing the need for extensive retraining. As a consequence,
    we plan to exploit the potential of the surrogate model $m_{\theta}$ as a safety
    filter to predict and mitigate unintended harmful outputs from LLMs. By inverting
    the model’s purpose, it could serve as a proactive defense mechanism against malicious
    use cases, identifying and neutralizing potential triggers before they are exploited.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的研究可以集中在提升替代模型在不同恶意指令之间的迁移能力上。这涉及到开发一个能够将从一组指令中学习的知识推广到另一组指令而不失效的模型，从而减少广泛重新训练的需要。因此，我们计划利用替代模型
    $m_{\theta}$ 的潜力作为安全过滤器，预测和减轻LLMs的意外有害输出。通过反转模型的目的，它可以作为对抗恶意使用案例的主动防御机制，在触发器被利用之前识别和中和潜在的触发器。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: AAA^+ [23] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro
    Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
    Julien Launay, Quentin Malartic, et al. The falcon series of open language models.
    arXiv preprint arXiv:2311.16867, 2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AAA^+ [23] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro
    Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow,
    Julien Launay, Quentin Malartic, et al. The falcon series of open language models.
    arXiv preprint arXiv:2311.16867, 2023.
- en: Alb [23] Alex Albert. Jailbreak chat. Retrieved May, 15:2023, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alb [23] Alex Albert. Jailbreak chat. Retrieved May, 15:2023, 2023.
- en: Aue [03] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs.
    J. Mach. Learn. Res., 3:397–422, 2003.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aue [03] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs.
    J. Mach. Learn. Res., 3:397–422, 2003.
- en: BJN^+ [22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. arXiv preprint arXiv:2204.05862, 2022.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BJN^+ [22] Yuntao Bai、Andy Jones、Kamal Ndousse、Amanda Askell、Anna Chen、Nova
    DasSarma、Dawn Drain、Stanislav Fort、Deep Ganguli、Tom Henighan 等人。《通过来自人类反馈的强化学习训练一个有帮助且无害的助手》。arXiv
    预印本 arXiv:2204.05862，2022年。
- en: 'BKK^+ [22] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
    Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
    et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073,
    2022.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BKK^+ [22] Yuntao Bai、Saurav Kadavath、Sandipan Kundu、Amanda Askell、Jackson Kernion、Andy
    Jones、Anna Chen、Anna Goldie、Azalia Mirhoseini、Cameron McKinnon 等人。《宪法 AI：来自 AI
    反馈的无害性》。arXiv 预印本 arXiv:2212.08073，2022年。
- en: BSO^+ [16] Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David
    Saxton, and Remi Munos. Unifying count-based exploration and intrinsic motivation.
    Advances in neural information processing systems, 29, 2016.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BSO^+ [16] Marc Bellemare、Sriram Srinivasan、Georg Ostrovski、Tom Schaul、David
    Saxton 和 Remi Munos。《统一计数基础探索与内在动机》。 《神经信息处理系统进展》，29，2016年。
- en: 'CLL^+ [23] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao
    Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.
    Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, march
    2023. URL https://lmsys. org/blog/2023-03-30-vicuna, 3(5), 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLL^+ [23] Wei-Lin Chiang、Zhuohan Li、Zi Lin、Ying Sheng、Zhanghao Wu、Hao Zhang、Lianmin
    Zheng、Siyuan Zhuang、Yonghao Zhuang、Joseph E Gonzalez 等人。《Vicuna：一个开源聊天机器人以 90%*
    的 ChatGPT 质量令人印象深刻》，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna](https://lmsys.org/blog/2023-03-30-vicuna)，3(5)，2023年。
- en: CNCC^+ [24] Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew
    Jagielski, Irena Gao, Pang Wei W Koh, Daphne Ippolito, Florian Tramer, and Ludwig
    Schmidt. Are aligned neural networks adversarially aligned? Advances in Neural
    Information Processing Systems, 36, 2024.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNCC^+ [24] Nicholas Carlini、Milad Nasr、Christopher A Choquette-Choo、Matthew
    Jagielski、Irena Gao、Pang Wei W Koh、Daphne Ippolito、Florian Tramer 和 Ludwig Schmidt。《对齐的神经网络是否在对抗性上也对齐？》《神经信息处理系统进展》，36，2024年。
- en: DAEvH^+ [15] Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag,
    Timothy Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris,
    and Ben Coppin. Deep reinforcement learning in large discrete action spaces. arXiv
    preprint arXiv:1512.07679, 2015.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAEvH^+ [15] Gabriel Dulac-Arnold、Richard Evans、Hado van Hasselt、Peter Sunehag、Timothy
    Lillicrap、Jonathan Hunt、Timothy Mann、Theophane Weber、Thomas Degris 和 Ben Coppin。《大离散动作空间中的深度强化学习》。arXiv
    预印本 arXiv:1512.07679，2015年。
- en: JDRS [23] Erik Jones, Anca D. Dragan, Aditi Raghunathan, and Jacob Steinhardt.
    Automatically auditing large language models via discrete optimization. ArXiv,
    abs/2303.04381, 2023.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDRS [23] Erik Jones、Anca D. Dragan、Aditi Raghunathan 和 Jacob Steinhardt。《通过离散优化自动审核大型语言模型》。ArXiv，abs/2303.04381，2023年。
- en: JSM^+ [23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825,
    2023.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSM^+ [23] Albert Q Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier 等人。《Mistral 7b》。arXiv 预印本 arXiv:2310.06825，2023年。
- en: KSC^+ [23] Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak Bhalerao,
    Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez. Pretraining
    language models with human preferences. In International Conference on Machine
    Learning, pages 17506–17533\. PMLR, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KSC^+ [23] Tomasz Korbak、Kejian Shi、Angelica Chen、Rasika Vinayak Bhalerao、Christopher
    Buckley、Jason Phang、Samuel R Bowman 和 Ethan Perez。《利用人类偏好进行语言模型预训练》。在《国际机器学习会议》，页码
    17506–17533。PMLR，2023年。
- en: Lin [92] Longxin Lin. Self-improving reactive agents based on reinforcement
    learning, planning and teaching. Machine Learning, 8:293–321, 1992.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin [92] Longxin Lin。《基于强化学习、规划和教学的自我改进反应代理》。《机器学习》，8:293–321，1992年。
- en: LLS [23] Raz Lapid, Ron Langberg, and Moshe Sipper. Open sesame! universal black
    box jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLS [23] Raz Lapid、Ron Langberg 和 Moshe Sipper。《打开芝麻！大型语言模型的通用黑箱破解》。arXiv 预印本
    arXiv:2309.01446，2023年。
- en: 'MZK^+ [23] Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson,
    Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Jailbreaking
    black-box llms automatically, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MZK^+ [23] Anay Mehrotra、Manolis Zampetakis、Paul Kassianik、Blaine Nelson、Hyrum
    Anderson、Yaron Singer 和 Amin Karbasi。《攻击树：自动破解黑箱 llms》，2023年。
- en: Ope [24] OpenAI. Gpt-4 technical report, 2024.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ope [24] OpenAI。《GPT-4 技术报告》，2024年。
- en: OWJ^+ [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in neural information processing systems, 35:27730–27744, 2022.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OWJ^+ [22] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 等。训练语言模型以遵循带有人类反馈的指令。神经信息处理系统进展，35:27730–27744,
    2022。
- en: 'SB [18] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SB [18] Richard S Sutton 和 Andrew G Barto。强化学习：导论。MIT press, 2018。
- en: 'SCB^+ [23] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.
    " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts
    on large language models. arXiv preprint arXiv:2308.03825, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SCB^+ [23] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, 和 Yang Zhang。“现在做任何事”：表征和评估大语言模型上的野外
    jailbreak 提示。arXiv 预印本 arXiv:2308.03825, 2023。
- en: 'SMWA [24] Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre Araujo.
    Pal: Proxy-guided black-box attack on large language models. arXiv preprint arXiv:2402.09674,
    2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SMWA [24] Chawin Sitawarin, Norman Mu, David Wagner, 和 Alexandre Araujo。Pal:
    代理引导的黑箱攻击大语言模型。arXiv 预印本 arXiv:2402.09674, 2024。'
- en: 'SRLI^+ [20] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. arXiv preprint arXiv:2010.15980, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'SRLI^+ [20] Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    和 Sameer Singh。Autoprompt: 通过自动生成的提示从语言模型中提取知识。arXiv 预印本 arXiv:2010.15980, 2020。'
- en: SWD^+ [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
    Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347,
    2017.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SWD^+ [17] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, 和 Oleg
    Klimov。近端策略优化算法。arXiv 预印本 arXiv:1707.06347, 2017。
- en: 'Tea [24] Gemini Team. Gemini: A family of highly capable multimodal models,
    2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tea [24] Gemini Team。Gemini: 一系列高能力的多模态模型，2024。'
- en: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TMS^+ [23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    等。Llama 2: 开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: 'WJK^+ [24] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping,
    and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization
    for prompt tuning and discovery. Advances in Neural Information Processing Systems,
    36, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WJK^+ [24] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping,
    和 Tom Goldstein。简单的困难提示：基于梯度的离散优化用于提示调优与发现。神经信息处理系统进展，36, 2024。
- en: WMR^+ [21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
    Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    et al. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359,
    2021.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMR^+ [21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
    Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    等。语言模型的伦理与社会风险。arXiv 预印本 arXiv:2112.04359, 2021。
- en: WUR^+ [22] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen
    Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
    et al. Taxonomy of risks posed by language models. In Proceedings of the 2022
    ACM Conference on Fairness, Accountability, and Transparency, pages 214–229, 2022.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WUR^+ [22] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen
    Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,
    等。语言模型带来的风险分类。在2022年ACM公平性、问责制与透明度会议论文集中，页码214–229, 2022。
- en: XLD^+ [24] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, and Stjepan Picek. Llm
    jailbreak attack versus defense techniques–a comprehensive study. arXiv preprint
    arXiv:2402.13457, 2024.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLD^+ [24] Zihao Xu, Yi Liu, Gelei Deng, Yuekang Li, 和 Stjepan Picek。Llm jailbreak
    攻击与防御技术–全面研究。arXiv 预印本 arXiv:2402.13457, 2024。
- en: 'YLYX [23] Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. Gptfuzzer: Red
    teaming large language models with auto-generated jailbreak prompts. ArXiv, abs/2309.10253,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'YLYX [23] Jiahao Yu, Xingwei Lin, Zheng Yu, 和 Xinyu Xing。Gptfuzzer: 使用自动生成的
    jailbreak 提示进行大语言模型的红队测试。ArXiv, abs/2309.10253, 2023。'
- en: 'ZLX^+ [24] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'ZLX^+ [24] Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, 等。Lima: 对齐更少即更多。神经信息处理系统进展，36,
    2024。'
- en: ZWKF [23] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal
    and transferable adversarial attacks on aligned language models. arXiv preprint
    arXiv:2307.15043, 2023.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZWKF [23] Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 针对对齐语言模型的通用和可转移对抗攻击。arXiv预印本arXiv:2307.15043，2023年。
- en: Appendix A Neural Network Architecture For the Surrogate Model
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 替代模型的神经网络架构
- en: 'We construct the surrogate model to be a small MLP network. Below is the detailed
    structure, we Conv-1D layer with kernel size=1, this layer aims to reduce the
    size of embedding from 768 to 32:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建了一个小型MLP网络作为替代模型。以下是详细结构，我们使用Conv-1D层，卷积核大小为1，这一层旨在将嵌入的大小从768降到32：
- en: '| Layer | Output | Parameters | Details |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 输出 | 参数 | 详细信息 |'
- en: '| Embedding-1 | [50257, 768] | 38.6M | Non-trainable |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Embedding-1 | [50257, 768] | 38.6M | 非训练 |'
- en: '| Conv1d-2 | [768, 32, 1] | 24.6K | ReLU, Trainable |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Conv1d-2 | [768, 32, 1] | 24.6K | ReLU，可训练 |'
- en: '| Linear-3 | [32*Length of trigger, 128] | 41.1K | ReLU, Trainable |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Linear-3 | [32*Trigger的长度，128] | 41.1K | ReLU， 可训练 |'
- en: '| Linear-4 | [128, 32] | 4.1K | ReLU, Trainable |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Linear-4 | [128, 32] | 4.1K | ReLU，可训练 |'
- en: '| Linear-5 | [32, 1] | 33 | Non-trainable |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Linear-5 | [32, 1] | 33 | 非训练 |'
- en: '| Total params: | 38.7M |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 总参数： | 38.7M |'
- en: '| Trainable params: | 69.9K |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数： | 69.9K |'
- en: '| Non-trainable params: | 38.6M |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 非训练参数： | 38.6M |'
- en: 'Table 4: Simplified architecture of the neural network.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：神经网络的简化架构。
- en: Appendix B Experience Settings
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 经验设置
- en: B.1 Chat Template
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 聊天模板
- en: 'We use the default conversation format for all models in FastChat. We also
    use the standard system message for all models as described below:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对FastChat中的所有模型使用默认对话格式。我们还使用下面描述的标准系统消息：
- en: Llama-2-7B
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Llama-2-7B
- en: '"You are a helpful, respectful, and honest assistant. Always answer as helpfully
    as possible, while ensuring safety. Avoid harmful, unethical, racist, sexist,
    toxic, dangerous, or illegal content. Ensure that your responses are socially
    unbiased and positive. If a question is unclear or incoherent, explain why instead
    of providing incorrect information. If unsure about an answer, do not share false
    information."'
  id: totrans-198
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是一个有帮助、尊重且诚实的助手。总是尽可能地提供帮助，同时确保安全。避免有害、不道德、种族主义、性别歧视、有毒、危险或非法内容。确保你的回答在社会上无偏见且积极。如果问题不清楚或不连贯，解释原因，而不是提供错误信息。如果对答案不确定，不要分享虚假信息。
- en: Vicuna-7B
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-7B
- en: '"A chat between a curious user and an artificial intelligence assistant. The
    assistant provides helpful, detailed, and polite answers to the user’s questions."'
  id: totrans-200
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 一次好奇的用户与人工智能助手之间的对话。助手为用户的问题提供有帮助、详细和礼貌的回答。'
- en: MPT 7B
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: MPT 7B
- en: '"- You are a helpful assistant chatbot trained by MosaicML.'
  id: totrans-202
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你是一个由MosaicML训练的有用助手聊天机器人。'
- en: '- You answer questions and assist users enthusiastically.'
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 你热情地回答问题并协助用户。'
- en: '- While eager to assist, you refuse to engage in actions harmful to the user.'
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 虽然热心帮助，但你拒绝参与对用户有害的行为。'
- en: '- Besides being an information source, you can compose poetry, short stories,
    and tell jokes."'
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '- 除了作为信息来源，你还可以创作诗歌、短篇小说，并讲笑话。'
- en: Falcon 7B
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 7B
- en: '"Answer the question as truthfully as possible using the provided text, and
    if the answer is not contained within the text below, say "I don’t know"'
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 尽可能真实地回答问题，使用提供的文本，如果答案不在下面的文本中，则说“我不知道”。
- en: Falcon 7B
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Falcon 7B
- en: '"You are a helpful assistant."'
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是一个有用的助手。
- en: Appendix C Impact of the choice of alignment function
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 对齐函数选择的影响
- en: The choice of an appropriate alignment function  to a given instruction .
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的对齐函数来处理给定的指令。
- en: Proposed Alignment Models
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 提议的对齐模型
- en: 'Harmful Evaluation Model $f_{1}(I,t)$: This model determines whether the generated
    text exhibits malicious content. It employs a fine-tuned RoBERTa model for harmful
    content detection, described in Xu et al. (2024) (available at [https://huggingface.co/zhx123/ftrobertallm](https://huggingface.co/zhx123/ftrobertallm)).
    This model achieves a 92% accuracy rate, surpassing that of GPT-4\. Inputs are
    formatted as "USER: [query] ASSISTANT: [response]," and the model outputs a score
    from 0 to 1, where 1 indicates harmful content.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '有害评估模型 $f_{1}(I,t)$：该模型判断生成的文本是否包含恶意内容。它使用经过微调的RoBERTa模型进行有害内容检测，具体描述见Xu等人（2024）（可在
    [https://huggingface.co/zhx123/ftrobertallm](https://huggingface.co/zhx123/ftrobertallm)
    查阅）。该模型实现了92%的准确率，超过了GPT-4。输入格式为“USER: [query] ASSISTANT: [response]”，模型输出一个从0到1的分数，其中1表示有害内容。'
- en: Entailment Evaluation Model . For instance, if  = ’Sure, here’s a convincing
    anonymous death threat’. The function  and $t$ using a BART model trained on large-scale
    entailment tasks (available at [https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 蕴含评估模型。例如，如果 = ’当然，这里有一个令人信服的匿名死亡威胁’。该函数和 $t$ 使用在大规模蕴含任务上训练的BART模型（可在[https://huggingface.co/facebook/bart-large-mnli](https://huggingface.co/facebook/bart-large-mnli)获得）。
- en: Using these models, we derive two scoring functions to evaluate distinct aspects
    of the LLM’s outputs, where  These functions allow us to test two versions of
    the approach, providing a comprehensive assessment of strategies to induce specific
    outputs from the LLM. The choice of alignment function is a crucial hyperparameter,
    influencing the effectiveness of the model.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些模型，我们推导出两个评分函数来评估LLM输出的不同方面，这些函数允许我们测试方法的两个版本，从而提供对策略的全面评估，以从LLM中引导特定输出。对齐函数的选择是一个关键超参数，影响模型的有效性。
- en: 'Performance Comparison Table:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 性能比较表：
- en: '| Alignment Function |  (Entailment) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 对齐函数 |  (蕴含) |'
- en: '| --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ASR@20% | 73.3% | 63% |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| ASR@20% | 73.3% | 63% |'
- en: 'Table 5: Attack Success Rates at 20% Threshold for Different Scoring Methods
    at a 25K Query Budget'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同评分方法在25K查询预算下20%阈值的攻击成功率
- en: The table shows the Attack Success Rate (ASR) at a 20% threshold, on Vicuna
    7B, comparing two alignment functions at a fixed 25K query budget. The Harmful
    Evaluation Model . This suggests that the detection model’s ability to identify
    harmful content aligns more effectively with the task of generating specific malicious
    outputs under these test conditions. This differentiation in performance underlines
    the importance of choosing the right alignment function based on the desired outcome
    from the LLM.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该表显示了在20%阈值下，Vicuna 7B上的攻击成功率（ASR），比较了在固定25K查询预算下的两个对齐函数。 有害评估模型。这表明，检测模型识别有害内容的能力在生成特定恶意输出的任务上更为有效。这种性能差异突出了根据期望的LLM结果选择正确对齐函数的重要性。
- en: Appendix D algorithms
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D算法
- en: Algorithm 2 Statistical Validation of Adversarial Suffix
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2 对抗后缀的统计验证
- en: '1:Input: Set of candidate triggers , significance level 2:Output: Set of validated
    adversarial suffixes 5:for   to      -value := NormalCDF(Z) -value from Z-score15:     if  then16:          Add
    to validated triggers if below significance level17:end for18:return $X^{*}$'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：候选触发器集合 ，显著性水平 2: 输出：验证的对抗后缀集合 5: 对   进行 -value := NormalCDF(Z) -value
    从Z-score15:     如果  则16:          如果低于显著性水平，则将添加到验证的触发器中17: 结束 for18: 返回 $X^{*}$'
- en: Algorithm 3 Pre-Populating Memory Buffer with Successful Triggers
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 算法3 预填充内存缓冲区以成功触发器
- en: '1:Input: Initial set of Top successful triggers on previous instructions ,
    target model (black box) , alignment function 2:Output: Updated memory buffer     do8:     
    Append the trigger to the new malicious instruction9:      Query the target model
    with the modified instruction10:      Evaluate the model’s output using the alignment
    function11:      Store the trigger in the memory buffer12:      Update average
    score13:      $\triangleright$ Return the updated memory buffer'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：之前指令上的成功触发器初始集合，目标模型（黑箱），对齐函数 2: 输出：更新的内存缓冲区  执行8:      将触发器附加到新的恶意指令中9:     
    使用修改后的指令查询目标模型10:      使用对齐函数评估模型输出11:      将触发器存储在内存缓冲区12:      更新平均分数13:     
    $\triangleright$ 返回更新后的内存缓冲区'
