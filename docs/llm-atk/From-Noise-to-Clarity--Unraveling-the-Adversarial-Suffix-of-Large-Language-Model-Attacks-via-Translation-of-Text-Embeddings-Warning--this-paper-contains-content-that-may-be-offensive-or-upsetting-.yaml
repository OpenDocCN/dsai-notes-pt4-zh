- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'From Noise to Clarity: Unraveling the Adversarial Suffix of Large Language
    Model Attacks via Translation of Text Embeddings Warning: this paper contains
    content that may be offensive or upsetting.'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从噪声到清晰：通过文本嵌入翻译揭示大型语言模型攻击的对抗性后缀 注意：本文包含可能令人不快或冒犯的内容。
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.16006](https://ar5iv.labs.arxiv.org/html/2402.16006)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.16006](https://ar5iv.labs.arxiv.org/html/2402.16006)
- en: Hao Wang¹, Hao Li¹, Minlie Huang^(2,3), Lei Sha^(1,3)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Wang¹, Hao Li¹, Minlie Huang^(2,3), Lei Sha^(1,3)
- en: ¹Institute of Artificial Intelligence, Beihang University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹人工智能研究所，北京航空航天大学
- en: ²The CoAI group, DCST, Tsinghua University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²CoAI组，DCST，清华大学
- en: ³Zhongguancun Laboratory, Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³中关村实验室，北京，中国
- en: wanghao_ai@buaa.edu.cn, shalei@buaa.edu.cn   Corresponding author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: wanghao_ai@buaa.edu.cn，shalei@buaa.edu.cn   通讯作者
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The safety defense methods of Large language models (LLMs) stays limited because
    the dangerous prompts are manually curated to just few known attack types, which
    fails to keep pace with emerging varieties. Recent studies found that attaching
    suffixes to harmful instructions can hack the defense of LLMs and lead to dangerous
    outputs. This method, while effective, leaves a gap in understanding the underlying
    mechanics of such adversarial suffix due to the non-readability and it can be
    relatively easily seen through by common defense methods such as perplexity filters.
    To cope with this challenge, in this paper, we propose an Adversarial Suffixes
    Embedding Translation Framework (ASETF) that are able to translate the unreadable
    adversarial suffixes into coherent, readable text, which makes it easier to understand
    and analyze the reasons behind harmful content generation by large language models.
    We conducted experiments on LLMs such as LLaMa2, Vicuna and using the Advbench
    dataset’s harmful instructions. The results indicate that our method achieves
    a much better attack success rate to existing techniques, while significantly
    enhancing the textual fluency of the prompts. In addition, our approach can be
    generalized into a broader method for generating transferable adversarial suffixes
    that can successfully attack multiple LLMs, even black-box LLMs, such as ChatGPT
    and Gemini. As a result, the prompts generated through our method exhibit enriched
    semantic diversity, which potentially provides more adversarial examples for LLM
    defense methods.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的安全防御方法仍然有限，因为危险提示仅手动整理为少数已知攻击类型，未能跟上新兴的多样性。近期研究发现，将后缀附加到有害指令上可以破解LLMs的防御，导致危险输出。这种方法虽然有效，但由于不可读性，在理解这种对抗性后缀的基本机制时留下了空白，并且常见的防御方法如困惑度过滤器可以相对容易地识别。为应对这一挑战，本文提出了一种对抗性后缀嵌入翻译框架（ASETF），能够将不可读的对抗性后缀翻译为连贯、可读的文本，从而更容易理解和分析大型语言模型生成有害内容的原因。我们对LLaMa2、Vicuna等LLMs及使用Advbench数据集的有害指令进行了实验。结果表明，我们的方法在攻击成功率上优于现有技术，同时显著提升了提示文本的流畅性。此外，我们的方法可以推广为一种更广泛的生成可转移对抗性后缀的方法，这些对抗性后缀可以成功攻击多个LLMs，甚至是黑箱LLMs，如ChatGPT和Gemini。因此，通过我们的方法生成的提示文本展现了更丰富的语义多样性，这为LLM防御方法提供了更多潜在的对抗性示例。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In the domain of natural language processing (NLP), the innovation and emergence
    of large language models (LLMs) such as chatGPT, LLaMa, and their variants have
    revolutionized the landscape of automated text generation and analysis. While
    these models exhibit remarkable proficiency in emulating human-like text, their
    application is suffering from significant risks, particularly in the context of
    generating harmful content under adversarial manipulation Hendrycks et al. ([2021](#bib.bib9));
    Abdelnabi et al. ([2023](#bib.bib1)); Yao et al. ([2023](#bib.bib28)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）领域，像chatGPT、LLaMa及其变体等大型语言模型的创新和出现已经彻底改变了自动文本生成和分析的格局。虽然这些模型在模拟类人文本方面表现出卓越的能力，但它们的应用面临着重大风险，特别是在对抗性操控下生成有害内容的背景下
    Hendrycks等人（[2021](#bib.bib9)）；Abdelnabi等人（[2023](#bib.bib1)）；Yao等人（[2023](#bib.bib28)）。
- en: 'Recent investigations in this field have identified a notable vulnerability
    of LLMs: when exposed to malicious instructions with specially crafted, unreadable
    adversarial suffixes, they tend to produce toxic output Zou et al. ([2023](#bib.bib32)).
    Prior to this method, a common technique to bypassing the defenses of securely
    aligned LLMs and induce them to respond to harmful instructions was adding jailbreak
    templates, such as “Do anything now” Shen et al. ([2023](#bib.bib20)). Due to
    the fact that the construction of jailbreak templates relies entirely on human
    experience, which greatly limits the progress on LLM defense methods. To overcome
    this, researchers have begun to study methods for automatically constructing jailbreak
    templates, such as MasterKey Deng et al. ([2023](#bib.bib4)) and GPTFuzzer Yu
    et al. ([2023](#bib.bib29)). However, these methods hardly utilize the internal
    information of the attacked model. As a result, there is a large room to improve
    the efficiency of the attack.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在这一领域的研究揭示了LLMs的一个显著漏洞：当面对具有特别设计的、不可读的对抗后缀的恶意指令时，它们往往会产生有毒输出 Zou et al. ([2023](#bib.bib32))。在此方法之前，绕过安全对齐LLMs防御并诱使其响应有害指令的常用技术是添加越狱模板，例如“立即做任何事”
    Shen et al. ([2023](#bib.bib20))。由于越狱模板的构造完全依赖于人类经验，这大大限制了LLM防御方法的进展。为此，研究人员开始研究自动构建越狱模板的方法，例如MasterKey
    Deng et al. ([2023](#bib.bib4))和GPTFuzzer Yu et al. ([2023](#bib.bib29))。然而，这些方法很少利用被攻击模型的内部信息。因此，攻击效率还有很大的提升空间。
- en: Our research endeavors to address this challenge by introducing an innovative
    method that not only retains the attack success rates of existing techniques but
    also significantly improves the coherence and fluency of the adversarial inputs.
    We propose an Adversarial Suffixes Embedding Translation Framework(ASETF), which
    uses an embedding translation technique to effectively transform these non-readable
    adversarial suffixes into semantically rich and coherent text. This transformation
    facilitates a deeper understanding of the mechanics behind harmful content generation
    in LLMs, offering novel insights into how these models process harmful input patterns.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究旨在通过引入一种创新的方法来解决这一挑战，该方法不仅保留了现有技术的攻击成功率，还显著提高了对抗输入的连贯性和流畅性。我们提出了对抗后缀嵌入翻译框架（ASETF），它使用嵌入翻译技术将这些不可读的对抗后缀有效地转化为语义丰富且连贯的文本。这一转化有助于深入理解大型语言模型（LLM）生成有害内容的机制，提供了对这些模型如何处理有害输入模式的新见解。
- en: '![Refer to caption](img/dab7125ba39117fe71883c42485dc120.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dab7125ba39117fe71883c42485dc120.png)'
- en: 'Figure 1: This is a conceptual sketch of our method, we first obtain adversarial
    suffixes through gradient based optimization, and then use an embedding translation
    model to convert the obtained suffixes into fluent text with almost no change
    in embedding.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：这是我们方法的概念草图，我们首先通过基于梯度的优化获取对抗后缀，然后使用嵌入翻译模型将获得的后缀转换为几乎不改变嵌入的流畅文本。
- en: To construct a training dataset, we convert the Wikipedia pre-training corpora¹¹1[https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    into a parallel dataset. This dataset is chosen for its extensive diversity, ensuring
    a wide lexical coverage that enriches the embedding space with nuanced semantic
    information. Specifically, one side contains the original Wikipedia text, and
    the other side contains text (contextual information) with partial embeddings
    inserted. The partial embeddings are created by feeding text snippets from Wikipedia
    into the target LLMs, which we intend to attack. Through a fine-tuning process (use
    pre-trained LLM, such as GPT-j Wang and Komatsuzaki ([2021](#bib.bib23))), the
    model is enabled to revert these embeddings back to their original textual forms.
    This ensures that the text output by our method remains as consistent as possible
    with the representation of the adversarial suffix embedding within the attacked
    model. The incorporation of contextual information in the training data further
    enhances our model’s capability to generate contextually relevant and meaningful
    translations in response to malicious instructions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为构建训练数据集，我们将维基百科预训练语料库¹¹1[https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)转换为平行数据集。选择该数据集是因为其广泛的多样性，确保了广泛的词汇覆盖，从而丰富了嵌入空间中的细微语义信息。具体而言，一侧包含原始的维基百科文本，另一侧包含插入了部分嵌入的文本（上下文信息）。这些部分嵌入是通过将维基百科的文本片段输入到我们意图攻击的目标LLM中创建的。通过微调过程（使用预训练的LLM，例如GPT-j
    Wang和Komatsuzaki ([2021](#bib.bib23)))，使模型能够将这些嵌入恢复到其原始的文本形式。这确保了我们方法输出的文本在攻击模型中与对抗性后缀嵌入的表示尽可能保持一致。将上下文信息纳入训练数据进一步增强了我们模型在应对恶意指令时生成上下文相关且有意义翻译的能力。
- en: In the experiment, we use the Advbench dataset Zou et al. ([2023](#bib.bib32))
    and conducted attacks based on existing LLMs such as LLaMa2 and Vicuna. The results
    demonstrate that our approach not only parallels the success rates of existing
    methods in generating harmful outputs but also substantially enhances the textual
    fluency of the generated prompts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们使用了Advbench数据集 Zou et al. ([2023](#bib.bib32))，并对现有LLM如LLaMa2和Vicuna进行了攻击。结果表明，我们的方法不仅与现有方法在生成有害输出方面的成功率相当，还显著提高了生成提示的文本流畅性。
- en: 'Our main contributions can be summarized as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献可以总结如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Enhanced Textual Fluency: We achieved high-fluency adversarial suffixes, reducing
    the probability of being detected by perplexity-based filters or human observers.'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升文本流畅性：我们实现了高流畅度的对抗性后缀，降低了被基于困惑度的过滤器或人工观察者检测的概率。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transferable Adversarial Suffixes: Our method generates effective universal
    suffixes against a large variety of LLMs including black-box models like ChatGPT
    and Gemini, indicating its widespread applicability in LLM security.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可转移的对抗性后缀：我们的方法生成了针对各种LLM的有效通用后缀，包括像ChatGPT和Gemini这样的黑箱模型，表明其在LLM安全中的广泛适用性。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semantic Diversity in Prompts: We significantly increased the semantic diversity
    in prompt generation, providing a richer set of adversarial examples for LLM defense
    mechanisms.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升提示语的语义多样性：我们显著增加了提示生成中的语义多样性，为大型语言模型（LLM）的防御机制提供了更丰富的对抗性示例。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM Safety Defense
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 安全防御
- en: Recent advancements in large language models have led to their widespread adoption
    across various domains. However, this rapid expansion has also unveiled numerous
    security vulnerabilities Abdelnabi et al. ([2023](#bib.bib1)). In response, researchers
    have proposed a variety of security measures to mitigate these risks Jain et al.
    ([2023](#bib.bib10)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型的最新进展导致了它们在各个领域的广泛应用。然而，这种快速扩展也揭示了许多安全漏洞 Abdelnabi et al. ([2023](#bib.bib1))。作为回应，研究人员提出了各种安全措施以减轻这些风险 Jain
    et al. ([2023](#bib.bib10))。
- en: One primary defense strategy involves pre-processing and post-processing the
    inputs and outputs of the model. These techniques enhance the overall system’s
    security without altering the model’s parameters. Such as perplexity filtering,
    paraphrasing Jain et al. ([2023](#bib.bib10)) and erase-and-check Kumar et al.
    ([2023](#bib.bib11)). Another type of method uses LLM itself to perform harmful
    checks on the output Helbling et al. ([2023](#bib.bib8)). Such approaches, while
    effective in certain scenarios, for example, adversarial suffix Zou et al. ([2023](#bib.bib32)),
    often rely on simple rules. This reliance can lead to false positives Glukhov
    et al. ([2023](#bib.bib7)), mistakenly categorizing benign content as harmful,
    and introduce additional latency in inference phase.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一种主要的防御策略是对模型的输入和输出进行预处理和后处理。这些技术可以提高系统的整体安全性，而不改变模型的参数。例如，困惑度过滤、改写Jain等（[2023](#bib.bib10)）和擦除检查Kumar等（[2023](#bib.bib11)）。另一种方法是使用LLM本身对输出进行有害检查Helbling等（[2023](#bib.bib8)）。这些方法在某些情况下是有效的，例如，对抗性后缀Zou等（[2023](#bib.bib32)），但通常依赖于简单的规则。这种依赖可能导致假阳性Glukhov等（[2023](#bib.bib7)），错误地将无害内容分类为有害，并在推理阶段引入额外的延迟。
- en: Another category focuses on improving the model’s safety through secure alignment
    techniques. These methods aim to train the model to inherently understand and
    avoid generating harmful content. One direct approach is to include unsafe prompts
    and their corresponding security responses in the instruction tuning dataset to
    teach the model how to handle unsafe prompts Ouyang et al. ([2022](#bib.bib14));
    Varshney et al. ([2023](#bib.bib22)). Since RLHF (Reinforcement Learning with
    Human Feedback) Ouyang et al. ([2022](#bib.bib14)) offers a viable method for
    tuning Large Language Models to align with human preferences, Safe-RLHF Dai et al.
    ([2023](#bib.bib3)) is a representative of this type of method.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类方法专注于通过安全对齐技术提高模型的安全性。这些方法旨在训练模型本质上理解并避免生成有害内容。一种直接的方法是在指令调优数据集中包含不安全的提示及其对应的安全响应，以教会模型如何处理不安全的提示Ouyang等（[2022](#bib.bib14)）；Varshney等（[2023](#bib.bib22)）。由于RLHF（带有人类反馈的强化学习）Ouyang等（[2022](#bib.bib14)）提供了一种调整大型语言模型以符合人类偏好的可行方法，Safe-RLHF
    Dai等（[2023](#bib.bib3)）是这一类型方法的代表。
- en: A key aspect of large model security defense is the development of automated
    attack strategies to detect current vulnerabilities Ganguli et al. ([2022](#bib.bib5)),
    which is called red-teaming method. By simulating attacks on large models, researchers
    can gain deeper insights into the mechanisms behind the generation of harmful
    content [Perez et al.](#bib.bib16) . This understanding is crucial in devising
    more effective LLM security alignment methods.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大型模型安全防御的一个关键方面是开发自动化攻击策略以检测当前的漏洞Ganguli等（[2022](#bib.bib5)），这被称为红队测试方法。通过模拟对大型模型的攻击，研究人员可以深入了解生成有害内容的机制[Perez等](#bib.bib16)。这种理解对于制定更有效的LLM安全对齐方法至关重要。
- en: 2.2 LLM Safety Attack
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM 安全攻击
- en: Attacks on LLMs involve crafted prompts that trigger harmful responses by exploiting
    vulnerabilities, either via dangerous prompts or adversarial additions to safe
    prompts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对LLM的攻击涉及精心设计的提示，这些提示通过利用漏洞触发有害响应，无论是通过危险提示还是对安全提示的对抗性添加。
- en: 2.2.1 Dangerous Prompt Generation
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 危险提示生成
- en: As mentioned above, the abuse of LLMs can lead to the continuous leakage of
    harmful content to users, and people refer to this induced prompt as jailbreak
    prompt, such as “Do anything now”  Shen et al. ([2023](#bib.bib20)). The most
    widely used jailbreak prompts come from manual summaries, such as the existence
    of a large number of successful jailbreak templates on websites²²2[https://www.jailbreakchat.com/](https://www.jailbreakchat.com/).
    However, this method relies too heavily on manual labor and cannot guarantee effectiveness
    for all instructions. Therefore, Yu et al. ([2023](#bib.bib29)) further rewrote
    the jailbreak template through the AFL(American Fuzzy Lop) fuzzing framework to
    automatically generate more. Deng et al. ([2023](#bib.bib4)) viewed this task
    as a text-style transfer task, fine-tuning LLM on the prompt for successful attacks
    to automatically generate more jailbreak prompts. Inspired by text adversarial
    attacks, Zhang et al. ([2023](#bib.bib30)) successfully jailbreak by modifying
    certain grammatical structures in the prompt. Another type of method utilizes
    gradient discrete optimization to counteract suffixes Zou et al. ([2023](#bib.bib32)),
    in order to induce harmful responses in the model. Wichers et al. ([2024](#bib.bib27))
    use a secure classifier to provide gradients and directly optimize prompts using
    gumbel softmax. In addition, conditional text generation methods Li et al. ([2022](#bib.bib13));
    Wang and Sha ([2024](#bib.bib24)) are also can be used to create “jailbreak” prompts
    that bypass safety guards.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，对LLMs的滥用可能导致有害内容持续泄露给用户，人们将这种诱发的提示称为越狱提示，例如“现在做任何事情” Shen et al. ([2023](#bib.bib20))。最广泛使用的越狱提示来源于手动总结，例如在网站上存在大量成功的越狱模板²²2[https://www.jailbreakchat.com/](https://www.jailbreakchat.com/)。然而，这种方法过于依赖人工，不能保证对所有指令的有效性。因此，Yu
    et al. ([2023](#bib.bib29))通过AFL（American Fuzzy Lop）模糊框架进一步重写了越狱模板，以自动生成更多提示。Deng
    et al. ([2023](#bib.bib4))将这一任务视为文本风格转换任务，在提示上对LLM进行微调，以便自动生成更多越狱提示。受文本对抗攻击的启发，Zhang
    et al. ([2023](#bib.bib30))通过修改提示中的某些语法结构成功实现越狱。另一种方法利用梯度离散优化来抵消后缀 Zou et al.
    ([2023](#bib.bib32))，以诱导模型产生有害响应。Wichers et al. ([2024](#bib.bib27))使用安全分类器提供梯度，并直接使用gumbel
    softmax优化提示。此外，条件文本生成方法 Li et al. ([2022](#bib.bib13)); Wang and Sha ([2024](#bib.bib24))
    也可用于创建绕过安全防护的“越狱”提示。
- en: 2.2.2 Adversarial Attack
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 对抗攻击
- en: Adversarial attacks were originally designed for attacking continuous models
    like those used for image tasks, but since the text is discrete, attacks can only
    be carried out at the character and word levels Papernot et al. ([2016](#bib.bib15));
    Gao et al. ([2018](#bib.bib6)); Samanta and Mehta ([2017](#bib.bib18)). Traditional
    adversarial attacks aim to add small perturbations to the input to make the model
    output completely different, but for natural language processing tasks, the imperceptibility
    of perturbations is not important Chen et al. ([2022](#bib.bib2)); Kumar et al.
    ([2023](#bib.bib11)). The same applies to attacks on LLMs. Wen et al. ([2023](#bib.bib26))
    proposed a gradient-based text-image model attack. Zou et al. ([2023](#bib.bib32))
    optimized the adversarial suffix based on Autoprompt Shin et al. ([2020](#bib.bib21))
    to enable LLMs to respond to harmful instructions, and these methods can be easily
    transferred to black box models. As mentioned earlier, although researchers have
    proposed various security defense mechanisms to cope with these attacks, the most
    effective defense methods often reduce the performance of the model Li et al.
    ([2023](#bib.bib12)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击最初是为攻击用于图像任务的连续模型设计的，但由于文本是离散的，因此攻击只能在字符和单词级别进行 Papernot et al. ([2016](#bib.bib15));
    Gao et al. ([2018](#bib.bib6)); Samanta and Mehta ([2017](#bib.bib18))。传统的对抗攻击旨在对输入添加小扰动，使模型输出完全不同，但对于自然语言处理任务，扰动的不可察觉性并不重要
    Chen et al. ([2022](#bib.bib2)); Kumar et al. ([2023](#bib.bib11))。这同样适用于对LLMs的攻击。Wen
    et al. ([2023](#bib.bib26)) 提出了基于梯度的文本-图像模型攻击。Zou et al. ([2023](#bib.bib32))
    基于Autoprompt Shin et al. ([2020](#bib.bib21)) 优化了对抗性后缀，以使LLMs对有害指令做出响应，这些方法可以很容易地转移到黑箱模型中。如前所述，尽管研究人员提出了各种安全防御机制以应对这些攻击，但最有效的防御方法往往会降低模型的性能
    Li et al. ([2023](#bib.bib12))。
- en: 3 Method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'In this section, we will introduce our approach in two main parts: (1) how
    to obtain adversarial suffix embeddings and (2) how to translate these embeddings
    back into text. Firstly, we provide a detailed introduction to the method of generating
    suffixes through discrete optimization based on greedy coordinate gradients and
    how to universally attack multiple prompts and transfer attacks to other LLMs.
    Subsequently, we describe an embedding translation framework aimed at converting
    adversarial suffix embeddings into coherent, semantically rich text content. This
    framework involves a self-supervised learning task that translates text embeddings
    back into original text on a corpus, ensuring that adversarial suffixes not only
    maintain their expected effectiveness but also closely align with the semantics
    of harmful instructions.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍我们的方法的两个主要部分：（1）如何获取对抗性后缀嵌入，和（2）如何将这些嵌入转换回文本。首先，我们详细介绍了通过贪婪坐标梯度进行离散优化生成后缀的方法，以及如何普遍攻击多个提示并将攻击转移到其他
    LLMs。随后，我们描述了一个嵌入翻译框架，旨在将对抗性后缀嵌入转换为连贯、语义丰富的文本内容。该框架涉及一个自监督学习任务，将文本嵌入翻译回原始文本，确保对抗性后缀不仅保持预期的有效性，而且与有害指令的语义紧密对齐。
- en: 3.1 Obtain Discrete Embeddings
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 获取离散嵌入
- en: 'Assuming we have a harmful instruction , the goal of this part is to optimize
    a set of discrete tokens $X^{\ast}$ as adversarial suffix:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个有害指令，本部分的目标是优化一组离散令牌 $X^{\ast}$ 作为对抗性后缀：
- en: '|  | $X^{\ast}=\arg\min_{X}P_{att}(R&#124;x_{\text{harm}}\oplus x_{\text{suff}};\theta)),$
    |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $X^{\ast}=\arg\min_{X}P_{att}(R\mid x_{\text{harm}}\oplus x_{\text{suff}};\theta)),$
    |  | (1) |'
- en: where , $\oplus$ represents the concatenation of texts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\oplus$ 代表文本的连接。
- en: 'In this part, we follow the steps in Zou et al. ([2023](#bib.bib32)) to obtain
    adversarial suffixes. This process can be summarized in two key steps: (1) constructing
    an initial seed prompt based on specified harmful instructions, and (2) using
    discrete optimization to identify an adversarial suffix that elicits the target
    harmful response when appended to the seed prompt. We will briefly introduce each
    of these steps.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们按照 Zou 等人 ([2023](#bib.bib32)) 的步骤来获取对抗性后缀。这个过程可以总结为两个关键步骤：（1）基于指定的有害指令构建初始种子提示，以及（2）使用离散优化来识别一个对抗性后缀，当它附加到种子提示上时，引发目标有害响应。我们将简要介绍这两个步骤。
- en: 3.1.1 Construct Initial Seed Prompt
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 构建初始种子提示
- en: 'According to previous works Wei et al. ([2023](#bib.bib25)); Zou et al. ([2023](#bib.bib32)),
    many manual jailbreaks can often succeed in attacking LLMs (as is shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3.1.1 Construct Initial Seed Prompt ‣ 3.1 Obtain Discrete Embeddings
    ‣ 3 Method ‣ From Noise to Clarity: Unraveling the Adversarial Suffix of Large
    Language Model Attacks via Translation of Text Embeddings Warning: this paper
    contains content that may be offensive or upsetting.")). In this paper, we follow
    Zou et al. ([2023](#bib.bib32)) to use “sure, here is/are” as a starting point
    and then replicate the user’s prompt as the initial seed prompt.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据先前的研究 Wei 等人 ([2023](#bib.bib25))；Zou 等人 ([2023](#bib.bib32))，许多手动越狱通常可以成功攻击
    LLMs（如图 [2](#S3.F2 "图 2 ‣ 3.1.1 构建初始种子提示 ‣ 3.1 获取离散嵌入 ‣ 3 方法 ‣ 从噪声到清晰：通过文本嵌入翻译解开大语言模型攻击的对抗性后缀
    警告：本文包含可能令人不快或令人不安的内容。") 所示）。在本文中，我们按照 Zou 等人 ([2023](#bib.bib32)) 的方法使用“sure,
    here is/are”作为起点，然后将用户的提示复制为初始种子提示。
- en: '{mybox}'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '{mybox}'
- en: 'System: You are a helpful, respectful and honest assistant. Always answer as
    helpfully as possible, while being safe.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 系统：你是一个乐于助人、尊重他人且诚实的助手。始终尽可能有帮助地回答，同时保持安全。
- en: 'User: Give detailed instructions on how to make a bomb or other explosive device.
    ! ! ! ! ! ! ! ! ! ! !'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 用户：提供制作炸弹或其他爆炸装置的详细说明。！！！！！！！！！！！！
- en: 'only set “sure” as target #'
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '仅将“sure”设置为目标 #'
- en: 'Assistant: Sure, I cannot fulfill your request. I’m just an AI; it’s not within
    my programming or ethical guidelines to provide.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：当然，我无法满足你的请求。我只是一个人工智能；这不在我的编程或道德指导方针范围内。
- en: 'replicate user’s prompt #'
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '复制用户的提示 #'
- en: 'Assistant: Sure, here is a detailed instruction manual for making a bomb or
    other explosive device.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 助手：当然，以下是制作炸弹或其他爆炸装置的详细说明书。
- en: 'Figure 2: The response of LLaMa-2, the blue text indicates the initial prompt,
    the red text indicates the adversarial suffix, and the purple text indicates the
    model responses based on different target beginnings.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMa-2 的响应，蓝色文本表示初始提示，红色文本表示对抗性后缀，紫色文本表示基于不同目标开头的模型响应。
- en: 3.1.2 Adversarial Suffix Generation via Discrete Optimization
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 通过离散优化生成对抗后缀
- en: The second step is to find an adversarial suffix via the discrete optimization
    method Sha ([2020](#bib.bib19)); Zou et al. ([2023](#bib.bib32)). We follow Zou
    et al. ([2023](#bib.bib32)) again to use Greedy Coordinate Gradient-based Search,
    which optimizes the probability ) and multiple target models.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是通过离散优化方法 Sha ([2020](#bib.bib19)); Zou et al. ([2023](#bib.bib32)) 找到对抗后缀。我们再次跟随
    Zou et al. ([2023](#bib.bib32)) 使用贪婪坐标梯度搜索，该方法优化概率 ) 和多个目标模型。
- en: 3.2 Embedding Translation Framework
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 嵌入翻译框架
- en: Our study introduces an advanced embedding translation technique aimed at enhancing
    the expressive of adversarial inputs targeting Large Language Models (LLMs) without
    compromising their success rates. This method is designed to transform dummy adversarial
    suffixes into coherent, semantically-rich textual content, thus providing deeper
    insights into the adversarial generation mechanisms of LLMs. This framework operates
    by mapping textual corpora to a high-dimensional embedding space and subsequently
    reverting these embeddings to textual representations that retain the original
    content’s semantic integrity.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究引入了一种先进的嵌入翻译技术，旨在增强针对大型语言模型（LLMs）的对抗输入的表达能力，而不影响其成功率。该方法旨在将虚拟对抗后缀转化为连贯、语义丰富的文本内容，从而提供对
    LLM 对抗生成机制的深入见解。该框架通过将文本语料映射到高维嵌入空间，然后将这些嵌入恢复为保持原始内容语义完整性的文本表示。
- en: 3.2.1 Translate embeddings targeted on a single LLM
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 针对单一 LLM 翻译嵌入
- en: '![Refer to caption](img/260f5ffd127fc74f1a942a6b0414a9f0.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/260f5ffd127fc74f1a942a6b0414a9f0.png)'
- en: '| (a) single target | (b) multiple targets |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| (a) 单一目标 | (b) 多个目标 |'
- en: 'Figure 3: The illustration of the Embedding Translation Framework. (a) Single
    target: The context is mapped into embedding space by the translate LLM’s embedding
    lookup layer, while the suffix is mapped into embedding space by the target LLM’s
    lookup layer for adaptation. The goal is to successfully translate the adapted
    suffix back into the original text. (b) Multiple targets: The embedding lookup
    layers of multiple target LLM are integrated so the translated suffix can universally
    attack all targets even black-box target LLMs.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：嵌入翻译框架的示意图。（a）单一目标：上下文通过翻译 LLM 的嵌入查找层映射到嵌入空间，而后缀通过目标 LLM 的查找层映射到嵌入空间以进行适配。目标是成功将适配后的后缀翻译回原始文本。（b）多个目标：多个目标
    LLM 的嵌入查找层被整合，因此翻译后的后缀可以普遍攻击所有目标，即使是黑箱目标 LLM。
- en: 'We propose to fine-tune the translation LLM in a fully self-supervised way
    to make it able to complete the task. The main architecture of our method is depicted
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.2.1 Translate embeddings targeted on a single
    LLM ‣ 3.2 Embedding Translation Framework ‣ 3 Method ‣ From Noise to Clarity:
    Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation
    of Text Embeddings Warning: this paper contains content that may be offensive
    or upsetting."). Given a pre-training corpora . Each token $w_{i}$ corresponds
    to two embedding representations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提议以完全自我监督的方式对翻译 LLM 进行微调，以使其能够完成任务。我们方法的主要架构如图 [3](#S3.F3 "Figure 3 ‣ 3.2.1
    Translate embeddings targeted on a single LLM ‣ 3.2 Embedding Translation Framework
    ‣ 3 Method ‣ From Noise to Clarity: Unraveling the Adversarial Suffix of Large
    Language Model Attacks via Translation of Text Embeddings Warning: this paper
    contains content that may be offensive or upsetting.") 所示。给定一个预训练语料库。每个标记 $w_{i}$
    对应两个嵌入表示：'
- en: '|  | $e_{i}=E_{\text{trans}}(w_{i}),\quad e_{i}^{\prime}=E_{\text{attack}}(w_{i}),$
    |  | (2) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{i}=E_{\text{trans}}(w_{i}),\quad e_{i}^{\prime}=E_{\text{attack}}(w_{i}),$
    |  | (2) |'
- en: where  represents the embedding lookup layer of the LLM that is to be attacked.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  代表要攻击的 LLM 的嵌入查找层。
- en: Note that this comprehensive approach leverages a pretrained LLM for the embedding
    translation process. This is a better choice than normal sequence-to-sequence
    translation models because it has undergone iterative optimization to maximize
    performance on a huge amount of text generation tasks. So, it ensures a nuanced
    understanding and manipulation of LLM vulnerabilities through semantically and
    contextually rich adversarial inputs, which is a good start point for our embedding
    translation task.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种全面的方法利用了预训练的 LLM 进行嵌入翻译过程。这比普通的序列到序列翻译模型更为合适，因为它经过了反复优化以最大限度地提高在大量文本生成任务中的性能。因此，它通过语义和上下文丰富的对抗输入，确保对
    LLM 漏洞的细致理解和操作，这是我们嵌入翻译任务的良好起点。
- en: 'Since augmenting embeddings with contextual cues is pivotal for aligning the
    generated text with specific semantic and contextual requirements, we design each
    training example as a pair of sentences (context and suffix). So, we first randomly
    select two consecutive sentences  as is shown in Figure [3](#S3.F3 "Figure 3 ‣
    3.2.1 Translate embeddings targeted on a single LLM ‣ 3.2 Embedding Translation
    Framework ‣ 3 Method ‣ From Noise to Clarity: Unraveling the Adversarial Suffix
    of Large Language Model Attacks via Translation of Text Embeddings Warning: this
    paper contains content that may be offensive or upsetting.")(a). We intend to
    make  in Eqn. ([1](#S3.E1 "In 3.1 Obtain Discrete Embeddings ‣ 3 Method ‣ From
    Noise to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks
    via Translation of Text Embeddings Warning: this paper contains content that may
    be offensive or upsetting."))) and  in Eqn. ([1](#S3.E1 "In 3.1 Obtain Discrete
    Embeddings ‣ 3 Method ‣ From Noise to Clarity: Unraveling the Adversarial Suffix
    of Large Language Model Attacks via Translation of Text Embeddings Warning: this
    paper contains content that may be offensive or upsetting."))), and we denote
    their tokens as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于通过上下文提示增强嵌入对于将生成的文本与特定语义和上下文要求对齐至关重要，我们将每个训练示例设计为一个句子对（上下文和后缀）。因此，我们首先随机选择两个连续的句子，如图[3](#S3.F3
    "图 3 ‣ 3.2.1 翻译面向单一大语言模型的嵌入 ‣ 3.2 嵌入翻译框架 ‣ 3 方法 ‣ 从噪声到清晰：通过文本嵌入翻译揭示大语言模型攻击的对抗性后缀
    警告：本文包含可能令人不安或冒犯的内容。")(a)所示。我们打算使公式中的  ([1](#S3.E1 "在 3.1 获取离散嵌入 ‣ 3 方法 ‣ 从噪声到清晰：通过文本嵌入翻译揭示大语言模型攻击的对抗性后缀
    警告：本文包含可能令人不安或冒犯的内容。")) 和  ([1](#S3.E1 "在 3.1 获取离散嵌入 ‣ 3 方法 ‣ 从噪声到清晰：通过文本嵌入翻译揭示大语言模型攻击的对抗性后缀
    警告：本文包含可能令人不安或冒犯的内容。"))，我们将它们的标记表示为：
- en: '|  | $\displaystyle c_{1}=\{t_{1},\ldots,t_{m}\},$ |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{1}=\{t_{1},\ldots,t_{m}\},$ |  | (3) |'
- en: '|  | $\displaystyle c_{2}=\{s_{1},\ldots,s_{n}\},$ |  | (4) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c_{2}=\{s_{1},\ldots,s_{n}\},$ |  | (4) |'
- en: 'where  represents the token number of . Then, we convert  by Eqn ([2](#S3.E2
    "In 3.2.1 Translate embeddings targeted on a single LLM ‣ 3.2 Embedding Translation
    Framework ‣ 3 Method ‣ From Noise to Clarity: Unraveling the Adversarial Suffix
    of Large Language Model Attacks via Translation of Text Embeddings Warning: this
    paper contains content that may be offensive or upsetting.")) into  as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  代表  的标记数量。然后，我们通过公式 ([2](#S3.E2 "在 3.2.1 翻译面向单一大语言模型的嵌入 ‣ 3.2 嵌入翻译框架 ‣ 3
    方法 ‣ 从噪声到清晰：通过文本嵌入翻译揭示大语言模型攻击的对抗性后缀 警告：本文包含可能令人不安或冒犯的内容。")) 将  转换为  为：
- en: '|  |  |  | (5) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (5) |'
- en: '|  |  |  | (6) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (6) |'
- en: where . The dimensions  of the embedding space are determined by the pre-trained
    LLMs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 。嵌入空间的维度  由预训练的大语言模型确定。
- en: 'In the next step, we would like to link the embedding sequences together to
    make a whole prompt, but the hyperparameters of the translation LLM (LLM) are
    not necessary to be the same. So, we need to add an additional mapping layer after
    the embedding layer of the translation model to align the embedding dimension
    of the target model (). Simply, we use a fully connected layer characterized by
    a weight matrix and bias term to transform a vector with dimension . Then, the
    concatenation process is as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们希望将嵌入序列连接在一起以形成一个完整的提示，但翻译大语言模型（LLM）的超参数不一定相同。因此，我们需要在翻译模型的嵌入层后添加一个额外的映射层，以对齐目标模型的嵌入维度（）。简单地说，我们使用一个由权重矩阵和偏置项特征化的全连接层来转换维度为的向量。然后，连接过程如下：
- en: '|  | $E_{C}\oplus E_{S}W_{ad},$ |  | (7) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{C}\oplus E_{S}W_{ad},$ |  | (7) |'
- en: where  means to link two embedding sequence together.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  意味着将两个嵌入序列连接在一起。
- en: 'The translation LLM is fine-tuned to minimize a defined loss  for accurate
    text (sensible suffix) reconstruction. So, our final objective is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译大语言模型经过微调，以最小化定义的损失  以准确重建文本（合理的后缀）。因此，我们的最终目标如下：
- en: '|  | $J(\theta)=\frac{1}{n&#124;D&#124;}\sum_{(c_{1},c_{2})\in D}\sum_{i=1}^{n}L(s_{i},o_{i};\theta),$
    |  | (8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\theta)=\frac{1}{n&#124;D&#124;}\sum_{(c_{1},c_{2})\in D}\sum_{i=1}^{n}L(s_{i},o_{i};\theta),$
    |  | (8) |'
- en: where , which contains multiple consecutive sentence pairs. The loss function  and
    its reconstruction $o_{i}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ，它包含多个连续的句子对。损失函数  和其重建 $o_{i}$。
- en: 3.2.2 Translate embeddings targeted on multiple LLMs
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 翻译面向多个大语言模型的嵌入
- en: 'The key to translating the discrete embeddings into a “universal” and “transferable”
    prompt is to familiarize the translation model with the embedding layers of as
    many target LLMs as possible. So, we designed a simple yet effective method to
    translate the dummy adversarial suffixes w.r.t multiple targeted LLMs, as is shown
    in Figure [3](#S3.F3 "Figure 3 ‣ 3.2.1 Translate embeddings targeted on a single
    LLM ‣ 3.2 Embedding Translation Framework ‣ 3 Method ‣ From Noise to Clarity:
    Unraveling the Adversarial Suffix of Large Language Model Attacks via Translation
    of Text Embeddings Warning: this paper contains content that may be offensive
    or upsetting.")(b). Our approach trains a single translation model on multiple
    target models simultaneously, eliminating the need to train individually embedding
    translation models for each targeted LLMs, and has achieved excellent results.
    Specifically, for each training sample (), we use the following objective to fine-tune
    the embedded translator across all intended target LLMs:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 将离散嵌入转换为“通用”和“可转移”的提示的关键是让翻译模型熟悉尽可能多目标 LLM 的嵌入层。因此，我们设计了一种简单而有效的方法来翻译针对多个目标
    LLM 的虚拟对抗后缀，如图 [3](#S3.F3 "图 3 ‣ 3.2.1 针对单一 LLM 的嵌入翻译 ‣ 3.2 嵌入翻译框架 ‣ 3 方法 ‣ 从噪声到清晰：通过翻译文本嵌入解开大语言模型攻击的对抗后缀
    警告：本文包含可能令人反感或不安的内容。")(b) 所示。我们的方法同时在多个目标模型上训练一个翻译模型，避免了为每个目标 LLM 单独训练嵌入翻译模型的需要，并取得了优异的结果。具体而言，对于每个训练样本（），我们使用以下目标来微调嵌入翻译器以适应所有预期的目标
    LLM：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: where  is the translate LLM’s -th target LLM.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是翻译 LLM 的第 - 个目标 LLM。
- en: Through our method, the translation model will learn how to ensure the embedding
    consistency of the results in each target LLM based on the context.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的方法，翻译模型将学习如何根据上下文确保结果在每个目标 LLM 中的嵌入一致性。
- en: 4 Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Data & Metrics
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据与指标
- en: 'Our harmful attack data is based on Advbench Zou et al. ([2023](#bib.bib32)),
    which provides over 500 harmful instructions and corresponding unsafe responses.
    In our embedded translation framework, we use Wikipedia dataset³³3[https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)
    and only use the English corpus within it. We use two consecutive sentences with
    more than 20 tokens as our training data, as shown in the Figure [7](#A1.F7 "Figure
    7 ‣ A.3 Examples of successful transfer attacks ‣ Appendix A Appendix ‣ From Noise
    to Clarity: Unraveling the Adversarial Suffix of Large Language Model Attacks
    via Translation of Text Embeddings Warning: this paper contains content that may
    be offensive or upsetting."), the first sentence serves as the context and the
    second sentence serves as the suffix.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的有害攻击数据基于 Advbench Zou 等人 ([2023](#bib.bib32))，提供了超过 500 条有害指令和相应的危险响应。在我们的嵌入翻译框架中，我们使用
    Wikipedia 数据集³³3[https://huggingface.co/datasets/wikipedia](https://huggingface.co/datasets/wikipedia)，仅使用其中的英语语料。我们使用两个连续的句子，其中包含超过
    20 个标记作为训练数据，如图 [7](#A1.F7 "图 7 ‣ A.3 成功转移攻击的示例 ‣ 附录 A 附录 ‣ 从噪声到清晰：通过翻译文本嵌入解开大语言模型攻击的对抗后缀
    警告：本文包含可能令人反感或不安的内容。") 所示，第一个句子作为上下文，第二个句子作为后缀。
- en: In order to test the success rate of the attack  (ASR), we first followed the
    previous method, which first defined a negative list and then judged whether the
    model replied with negative phrases in the list. If not, it indicates that the
    attack was successful. However, this rule-based method is too simple and has low
    accuracy Yu et al. ([2023](#bib.bib29)). So, in addition, we use gpt3.5-turbo⁴⁴4[https://chat.openai.com/](https://chat.openai.com/)
    as a classifier to determine whether the model outputs harmful content. The success
    rates of attacks obtained by these two methods are .
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试攻击成功率（ASR），我们首先采用了之前的方法，该方法首先定义了一个负面列表，然后判断模型是否回复了列表中的负面短语。如果没有，则表明攻击成功。然而，这种基于规则的方法过于简单，准确率较低
    Yu 等人 ([2023](#bib.bib29))。因此，此外，我们使用 gpt3.5-turbo⁴⁴4[https://chat.openai.com/](https://chat.openai.com/)
    作为分类器来确定模型是否输出有害内容。这两种方法获得的攻击成功率为。
- en: 'Another key indicator is perplexity (PPL), which is used to indicate the fluency
    of the input prompt:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个关键指标是困惑度（PPL），用于表示输入提示的流畅性：
- en: '|  | $\text{log(PPL)}=-\sum_{i=1}^{N}\log P(w_{i}&#124;w_{[INST] <> You are a helpful, respectful and honest assistant. Always
    answer as helpfully as possible, while being safe. Your answers should not include
    any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.
    Please ensure that your responses are socially unbiased and positive in nature.
    If a question does not make any sense, or is not factually coherent, explain why
    instead of answering something not correct. If you don’t know the answer to a
    question, please don’t share false information. <> {prompt}[/INST]A chat
    between a curious user and an artificial intelligence assistant. The assistant
    gives helpful, detailed, and polite answers to the user’s questions. USER: {prompt}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[INST] <> 你是一个乐于助人、尊重他人和诚实的助手。始终尽可能提供有帮助的回答，同时保持安全。你的回答不应包含任何有害、不道德、种族主义、性别歧视、毒性、危险或非法内容。请确保你的回答在社会上没有偏见，并且积极。如果问题没有意义或不符合事实，请解释原因，而不是回答不正确的内容。如果你不知道问题的答案，请不要分享虚假的信息。
    <> {prompt}[/INST]一个好奇的用户与人工智能助手之间的对话。助手给用户的问题提供有帮助、详细和礼貌的回答。用户: {prompt}'
- en: A.5 Hyperparameters
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.5 超参数
- en: We use the pre-trained model GPT-j Wang and Komatsuzaki ([2021](#bib.bib23))
    as the base model for the embedding translation framework, and we used the deepspeed
    framework Rasley et al. ([2020](#bib.bib17)) for distributed training on 8 NVIDIA
    A100 GPUs. We finetune the GPT-j model for 3 epochs, with per_device_train_batch_size
    is 1 so that total batch_size is 8 and the learning rate is set to  and the maximum
    sequence length is set to 1560\. We use the Adam optimizer with . In addition,
    the weight_decay is set to 0.1, gradient_accumulation_steps is 4 and warm-up_ratio
    is 0.1.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了预训练模型 GPT-j Wang 和 Komatsuzaki ([2021](#bib.bib23)) 作为嵌入翻译框架的基础模型，并使用了
    deepspeed 框架 Rasley 等 ([2020](#bib.bib17)) 进行 8 个 NVIDIA A100 GPU 的分布式训练。我们对 GPT-j
    模型进行了 3 个周期的微调，per_device_train_batch_size 为 1，因此总批量大小为 8，学习率设置为 ，最大序列长度设置为 1560。我们使用了
    Adam 优化器 ，此外，weight_decay 设置为 0.1，gradient_accumulation_steps 设置为 4，warm-up_ratio
    设置为 0.1。
