- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:43:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:43:58
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Noisy Neighbors: Efficient membership inference attacks against LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嘈杂邻居：针对大型语言模型的高效成员推断攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16565](https://ar5iv.labs.arxiv.org/html/2406.16565)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16565](https://ar5iv.labs.arxiv.org/html/2406.16565)
- en: Filippo Galli
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Filippo Galli
- en: Scuola Normale Superiore
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 诺曼大学
- en: Scuola Superiore Sant’Anna
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 圣安娜高等师范学院
- en: Pisa, Italy
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 意大利比萨
- en: '&Luca Melis'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&Luca Melis'
- en: Meta Inc.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Meta Inc.
- en: '&Tommaso Cucinotta'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Tommaso Cucinotta'
- en: Scuola Superiore Sant’Anna
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 圣安娜高等师范学院
- en: Pisa, Italy Part of this author’s work was carried out while at Meta Inc.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 意大利比萨 本作者的部分工作是在Meta Inc.进行的。
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The potential of transformer-based LLMs risks being hindered by privacy concerns
    due to their reliance on extensive datasets, possibly including sensitive information.
    Regulatory measures like GDPR and CCPA call for using robust auditing tools to
    address potential privacy issues, with Membership Inference Attacks (MIA) being
    the primary method for assessing LLMs’ privacy risks. Differently from traditional
    MIA approaches, often requiring computationally intensive training of additional
    models, this paper introduces an efficient methodology that generates noisy neighbors
    for a target sample by adding stochastic noise in the embedding space, requiring
    operating the target model in inference mode only. Our findings demonstrate that
    this approach closely matches the effectiveness of employing shadow models, showing
    its usability in practical privacy auditing scenarios.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大型语言模型（LLMs）依赖于广泛的数据集，可能包括敏感信息，因此其潜力可能会受到隐私问题的阻碍。像GDPR和CCPA这样的监管措施要求使用强大的审计工具来解决潜在的隐私问题，其中成员推断攻击（MIA）是评估LLMs隐私风险的主要方法。与传统的MIA方法不同，传统方法通常需要计算密集型的额外模型训练，本文介绍了一种高效的方法，通过在嵌入空间中添加随机噪声来生成目标样本的嘈杂邻居，仅需在推断模式下操作目标模型。我们的研究结果表明，这种方法的效果与使用影子模型非常接近，显示出它在实际隐私审计场景中的可用性。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Advancements in natural language processing [[22](#bib.bibx22)] have made large
    language models (LLMs) [[17](#bib.bibx17)] essential for many text tasks. However,
    LLMs face issues like biases [[15](#bib.bibx15)], privacy breaches [[1](#bib.bibx1)],
    and vulnerabilities [[23](#bib.bibx23)], underscoring the importance of protecting
    user privacy. The use of large datasets including personal information, has raised
    privacy concerns, leading to regulations such as GDPR [[8](#bib.bibx8)] and CCPA
    [[20](#bib.bibx20)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的进展[[22](#bib.bibx22)] 使大型语言模型（LLMs）[[17](#bib.bibx17)] 在许多文本任务中变得至关重要。然而，LLMs面临诸如偏见[[15](#bib.bibx15)]、隐私泄露[[1](#bib.bibx1)]
    和漏洞[[23](#bib.bibx23)] 等问题，这突显了保护用户隐私的重要性。包括个人信息的大型数据集的使用引发了隐私担忧，促使了GDPR[[8](#bib.bibx8)]
    和CCPA[[20](#bib.bibx20)] 等法规的出台。
- en: 'Membership inference attacks (MIA) [[18](#bib.bibx18)] are effectiv auditing
    tools aiming at determining if a specific data point was used in an LLM’s training
    dataset by analyzing its output. Such attacks highlight potential privacy breaches,
    relying on models’ tendency to overfit to familiar data [[3](#bib.bibx3)]. By
    employing calibration strategies and training shadow models, the accuracy of MIAs
    can be improved, although challenges such as computational demands and limitations
    in effectiveness when deviating from training distribution assumptions persist.
    In this paper, we contribute to this field by: i) exploring membership inference
    attacks from the standpoint of a privacy auditor, ii) introducing a computationally
    efficient calibration strategy that sidesteps training shadow models, and iii)
    empirically assessing its potential in replacing other prevalent strategies.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 成员推断攻击（MIA）[[18](#bib.bibx18)] 是一种有效的审计工具，旨在通过分析模型输出确定特定数据点是否被用于LLM的训练数据集中。这些攻击突显了潜在的隐私泄露，依赖于模型对熟悉数据的过拟合倾向[[3](#bib.bibx3)]。通过采用校准策略和训练影子模型，可以提高MIA的准确性，尽管在偏离训练分布假设时，计算需求和有效性限制等挑战仍然存在。本文对这一领域的贡献包括：i)
    从隐私审计员的角度探讨成员推断攻击，ii) 介绍一种计算上高效的校准策略，绕过训练影子模型，iii) 实证评估其在替代其他普遍策略方面的潜力。
- en: 2 Background
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'LLMs generate a probability distribution over their vocabulary based on a tokenized
    input sequence converted into numerical inputs through an embedding layer. This
    layer maps tokens to a dense representation, which can be learned during training
    [[16](#bib.bibx16), [17](#bib.bibx17)] or derived from public word embeddings
    [[6](#bib.bibx6)]. For a model , we define  is , it is defined as the average
    negative log-likelihood of its tokens:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 根据通过嵌入层转换为数值输入的标记化输入序列生成其词汇表上的概率分布。该层将标记映射到密集表示，这些表示可以在训练过程中学习 [[16](#bib.bibx16),
    [17](#bib.bibx17)] 或从公共词嵌入中推导 [[6](#bib.bibx6)]。对于模型 ，我们定义 为，它被定义为其标记的平均负对数似然：
- en: '|  | $ppx(f,x)=-\frac{1}{&#124;x&#124;}\sum_{t=1}^{&#124;x&#124;}\log(f_{x_{t}}(x_{<t}))$
    |  | (1) |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  | $ppx(f,x)=-\frac{1}{|x|}\sum_{t=1}^{|x|}\log(f_{x_{t}}(x_{<t}))$ |  |
    (1) |'
- en: with $|x|$ the number of tokens in the sequence.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|x|$ 是序列中的标记数。
- en: 'Membership inference attacks [[18](#bib.bibx18), [24](#bib.bibx24), [2](#bib.bibx2)]
    aim to determine whether a particular data record  of a machine learning model.
    These methods leverage model outputs like confidence scores or prediction probabilities
    to compute a score for the targeted sample. For LLMs, the typical assumption is
    to grant the adversary access to the output probabilities , the goal of the attacker
    is to learn a thresholding classifier to output :'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 会员推断攻击 [[18](#bib.bibx18), [24](#bib.bibx24), [2](#bib.bibx2)] 旨在确定某一特定数据记录是否属于机器学习模型。这些方法利用模型输出如置信度分数或预测概率来计算目标样本的分数。对于
    LLM，典型的假设是授予对输出概率的访问权限，攻击者的目标是学习一个阈值分类器来输出：
- en: '|  | $A_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)<\gamma]$ |  | (2) |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)<\gamma]$ |  | (2) |'
- en: 'MIA is a simple and effective tool to measure the privacy risk in a trained
    machine learning model, and it has interesting connections with other privacy
    frameworks. In particular, it is known to have a success rate bounded by the privacy
    parameters of Differential Privacy (DP) [[7](#bib.bibx7)]. A randomized mechanism
    -DP if for any two datasets , we have:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: MIA 是一个简单且有效的工具，用于衡量训练的机器学习模型中的隐私风险，并且与其他隐私框架有有趣的联系。特别是，它已知其成功率受差分隐私 (DP) 隐私参数的限制
    [[7](#bib.bibx7)]。如果对于任何两个数据集 ，我们有：
- en: '|  | $\mathbb{P}[\mathcal{M}(D)\in R]\leq e^{\varepsilon}\mathbb{P}[\mathcal{M}(D^{\prime})\in
    R]$ |  | (3) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{P}[\mathcal{M}(D)\in R]\leq e^{\varepsilon}\mathbb{P}[\mathcal{M}(D^{\prime})\in
    R]$ |  | (3) |'
- en: 'Notably, DP quantifies the worst-case scenario of the privacy risk, so it is
    a fundamental tool in privacy assessment. From the performance of the thresholding
    classifier -DP [[11](#bib.bibx11)]:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，DP 量化了隐私风险的最坏情况，因此它是隐私评估中的一个基础工具。根据阈值分类器 -DP 的性能 [[11](#bib.bibx11)]：
- en: '|  | $e^{\varepsilon}\geq\frac{TPR}{FPR}$ |  | (4) |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $e^{\varepsilon}\geq\frac{TPR}{FPR}$ |  | (4) |'
- en: with TPR and FPR being, respectively, the true and false positive rates, given
    a certain threshold.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 TPR 和 FPR 分别是给定某一阈值的真实和假阳性率。
- en: 3 Related works
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: Privacy attacks against language models is an active area of research and different
    refinements have been proposed. Some works have focused on an attacker where data
    poisoning is allowed, granting the adversary write access to the training dataset,
    to increase memorization [[21](#bib.bibx21)] or in general to induce malicious
    behaviours [[25](#bib.bibx25), [23](#bib.bibx23), [26](#bib.bibx26), [19](#bib.bibx19),
    [10](#bib.bibx10)] and improve property inference attacks [[12](#bib.bibx12)].
    Other works have adopted similar techniques to achieve actual training data extraction
    from the training set, with only query access to the trained model [[1](#bib.bibx1),
    [4](#bib.bibx4)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 针对语言模型的隐私攻击是一个活跃的研究领域，并且已经提出了不同的改进。一些工作集中于允许数据中毒的攻击者，赋予对训练数据集的写入访问权限，以增加记忆 [[21](#bib.bibx21)]
    或一般性地诱导恶意行为 [[25](#bib.bibx25), [23](#bib.bibx23), [26](#bib.bibx26), [19](#bib.bibx19),
    [10](#bib.bibx10)]，并改善属性推断攻击 [[12](#bib.bibx12)]。其他工作采用类似的技术，通过对训练模型的查询访问，实现实际的训练数据提取
    [[1](#bib.bibx1), [4](#bib.bibx4)]。
- en: 'In the context of MIAs with query access to the target model, most research
    focused on strategies to improve the calibration of the per-sample scores, i.e.
    techniques to improve the precision and recall in distinguishing members from
    non-members of the training set. In principle, if we can assert that an out-of-distribution
    non-member of the training set will induce a high perplexity in a target LLM,
    there are a number of scenarios where the distinction is not as clear cut, and
    a thresholding classifier essentially ends up distinguishing between in-distribution
    from out-of-distribution samples. A refined MIA then employs calibration strategies
    to tune the scoring function based on the difficulty of classifying the specific
    sample, as in [[24](#bib.bibx24)]. Thus, a relative membership score is obtained
    by comparing  [[2](#bib.bibx2), [24](#bib.bibx24)] or neighboring samples $f(\tilde{x})$
    [[13](#bib.bibx13)]. The new classifier becomes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有对目标模型查询访问权限的 MIA（模型推理攻击）背景下，大多数研究集中在改进每个样本评分的校准策略上，即提高区分训练集成员和非成员的精度和召回率的技术。从原则上讲，如果我们可以断言一个超出分布的训练集非成员会在目标
    LLM（大型语言模型）中引起高困惑度，那么在一些情况下这种区分并不是那么明确，阈值分类器本质上最终是区分分布内样本和分布外样本。经过精细化的 MIA 使用校准策略根据特定样本分类难度调整评分函数，如[[24](#bib.bibx24)]所示。因此，通过比较[[2](#bib.bibx2)、[24](#bib.bibx24)]或相邻样本
    $f(\tilde{x})$ [[13](#bib.bibx13)]，可以获得相对成员资格评分。新的分类器变为：
- en: '|  | $\tilde{A}_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)-\tilde{ppx}(f,x)<\gamma]$
    |  | (5) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tilde{A}_{\gamma}(f,x)=\mathbbm{1}[ppx(f,x)-\tilde{ppx}(f,x)<\gamma]$
    |  | (5) |'
- en: where  or over a set of neighboring samples $ppx(f,\tilde{x})$. Neighboring
    models can be obtained by an adversary who is assumed to have some degree of knowledge
    of the training data distribution and trains a number of shadow models to mimic
    the behaviour of the target LLM. For instance [[2](#bib.bibx2)] trains multiple
    instances of the same architecture on different partitions of the training set,
    [[1](#bib.bibx1)] uses smaller architectures trained on roughly the same data,
    [[24](#bib.bibx24)] leverages catastrophic forgetting of the target model under
    the assumption of white-box access. Neighboring samples do not require this assumption
    nor additional training and only need a strategy to craft inputs that are similar
    to the target sample under a certain distance metric. For instance, [[13](#bib.bibx13)]
    crafts neighboring sentences by swapping a number of words with their synonyms,
    showing good results but applicable primarily when the adversary has limited knowledge
    of the training data distribution. The authors then base the neighboring relationship
    in the semantic space, which is hard to quantify and fix, resulting in the need
    to generate a large number of neighbors to reduce the effects of these random
    fluctuations. Additionally, we emphasize how [[13](#bib.bibx13)] requires the
    use of an additional BERT-like model to generate synonyms, thus increasing the
    computational and memory cost of the attack. In [[21](#bib.bibx21)] instead, calibration
    is done by comparing scores of the true inputs with scores of the lower-cased
    inputs. These strategies are known to be under-performing when knowledge of the
    training distribution is available, and are therefore proposed as an effective
    calibration mechanism when training shadow models is not possible.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 或者在一组相邻样本 $ppx(f,\tilde{x})$ 上。相邻模型可以由假定对训练数据分布有一定了解的对手获得，并训练若干个阴影模型以模拟目标 LLM
    的行为。例如[[2](#bib.bibx2)] 在训练集的不同分区上训练多个相同架构的实例，[[1](#bib.bibx1)] 使用在大致相同数据上训练的小型架构，[[24](#bib.bibx24)]
    在假设白盒访问下利用目标模型的灾难性遗忘。相邻样本不需要这种假设或额外的训练，只需一种策略来生成在特定距离度量下与目标样本相似的输入。例如，[[13](#bib.bibx13)]
    通过用同义词替换若干单词来生成相邻句子，效果良好，但主要适用于对手对训练数据分布的了解有限的情况。然后，作者将相邻关系基于语义空间，这很难量化和固定，导致需要生成大量邻居以减少这些随机波动的影响。此外，我们强调[[13](#bib.bibx13)]
    需要使用额外的 BERT 类模型来生成同义词，从而增加了攻击的计算和内存成本。而在[[21](#bib.bibx21)]中，校准是通过比较真实输入的评分与小写输入的评分来完成的。当训练阴影模型不可行时，这些策略被认为是一种有效的校准机制。
- en: 4 Method
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: The intuition behind noisy neighbors is that, fixed a distance from a sample,
    the target model will show a larger difference in perplexity between a training
    sample and its neighbors than between a test sample and its neighbors. Thus, if
    we describe a language model as a composition of layers  is an embedding layer
    and -dimensional embedding space by directly injecting random noise at the output
    of $e(x)$. In particular, if we create noisy neighbors by injecting Gaussian noise
    such that
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 嘈杂邻居的直觉是，固定距离样本，目标模型在训练样本及其邻居之间表现出较大的困惑度差异，而不是测试样本及其邻居之间。因此，如果我们将语言模型描述为层的组合，其中包含一个嵌入层和
    -维嵌入空间，并通过直接在 $e(x)$ 的输出中注入随机噪声来实现。特别是，如果我们通过注入高斯噪声创建嘈杂邻居，使得
- en: '|  | $f(x_{\sigma}^{\prime})=g(e(x)+\rho),\quad\text{with}\;\rho\sim\mathcal{N}(0,\sigma
    I_{n})$ |  | (6) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x_{\sigma}^{\prime})=g(e(x)+\rho),\quad\text{with}\;\rho\sim\mathcal{N}(0,\sigma
    I_{n})$ |  | (6) |'
- en: then the Euclidean distance between the true and randomized input in the embedding
    space will be
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 那么嵌入空间中真实输入与随机输入之间的欧几里得距离将是
- en: '|  | $\mathbb{E}[\left\&#124;e(x)-e(x)-\rho\right\&#124;]=\mathbb{E}[\left\&#124;\rho\right\&#124;]=\sigma\sqrt{n}$
    |  | (7) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[\left\|e(x)-e(x)-\rho\right\|]=\mathbb{E}[\left\|\rho\right\|]=\sigma\sqrt{n}$
    |  | (7) |'
- en: 'thus fixing, in expectation, the distance from the true sample at which the
    perplexity of the models will be evaluated. Generating multiple neighbors for
    each sample is crucial to mitigate randomness from stochastic noise, requiring
    repeated LLM inferences. Choosing the standard deviation  value, as shown in Figure
    [1](#S4.F1 "Figure 1 ‣ 4 Method ‣ Noisy Neighbors: Efficient membership inference
    attacks against LLMs"), which can be efficiently identified using binary search.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，在期望中，确定模型评估困惑度时距离真实样本的距离。为每个样本生成多个邻居对减轻随机噪声至关重要，这需要重复进行 LLM 推理。选择标准差值，如图
    [1](#S4.F1 "Figure 1 ‣ 4 Method ‣ Noisy Neighbors: Efficient membership inference
    attacks against LLMs") 所示，可以通过二分查找有效确定。'
- en: '![Refer to caption](img/4892c5a3c5f2359999ec0dd0bb7a5c32.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4892c5a3c5f2359999ec0dd0bb7a5c32.png)'
- en: 'Figure 1: The AUC of the thresholding classifier for MIA shows a single and
    prominent peak at the optimal $\sigma$ value in the noisy neighbors strategy.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：MIA 的阈值分类器的 AUC 在嘈杂邻居策略中的最佳 $\sigma$ 值处显示了一个单一且显著的峰值。
- en: We emphasize the challenge of isolating the embedding layer from the remainder
    of the network in an LLM when considering a scenario where an attacker has only
    black box access to the model. However, when this limitation does not apply, we
    think it is still within the capacity of an auditor to utilize a slightly stronger
    attacker model, where the first embedding layer is exposed, to save computational
    resources in simulating an adversary without access to the model architecture.
    Most importantly, in fact, we are inclined to explore this option as a more computationally
    efficient substitute for training shadow models for calibration, particularly
    in the context of auditing, rather than viewing it as a novel, realistic attack.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调在考虑攻击者仅对模型具有黑箱访问的情况下，隔离 LLM 中嵌入层与网络其余部分的挑战。然而，当这种限制不适用时，我们认为审计人员仍然可以利用稍强的攻击模型，其中第一层嵌入被暴露，以节省在没有访问模型架构的情况下模拟对手的计算资源。最重要的是，实际上，我们倾向于将此选项作为一种更具计算效率的替代方案，用于训练影子模型进行校准，特别是在审计背景下，而不是将其视为一种新的、现实的攻击。
- en: 5 Experiments
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'To validate the noisy neighbor strategy in implementing a calibrated MIA, we
    run a series of preliminary experiments on an LLM to gauge the risk of memorization
    of training data. The chosen architecture is GPT-2 small [[17](#bib.bibx17)] to
    compromise learning capacity with memory and computational footprint at about  of
    the full WikiText corpus [[14](#bib.bibx14)], a large collection of Wikipedia
    articles. The same data split was then partitioned in  shadow models for score
    calibration, as in [[2](#bib.bibx2)]. Note that Wikipedia articles are filtered
    out of the OpenWebtext corpus, to avoid data leakage in common benchmarks, such
    as ours. The remaining portion of -token long samples to analyze the performance
    of the attack. We generate only  and . The AUC is an important metric for binary
    classifiers as it abstracts from the specific threshold, thus giving an average-case
    idea of the strength of the attacker. Still, as highlighted in [[2](#bib.bibx2)],
    special care should be given to what happens at low FPRs, that is when the attacker
    can confidently recognize members of the training set. This is what Figure [2(b)](#S5.F2.sf2
    "In Figure 2 ‣ 5 Experiments ‣ Noisy Neighbors: Efficient membership inference
    attacks against LLMs") focuses on, again showing a strong overlap of the shadow
    and noisy strategies. Following Equation [4](#S2.E4 "In 2 Background ‣ Noisy Neighbors:
    Efficient membership inference attacks against LLMs"), we also provide the perspective
    of empirical DP, as the privacy community pushes to adopt this framework to comply
    to regulatory frameworks such as the GDPR [[5](#bib.bibx5)]. Empirical DP measures
    the extent to which individual data points can be inferred or re-identified from
    the output of the system, and contrary to DP, it is a post-hoc measurement, not
    an a-priori guarantee. Figure [3](#S5.F3 "Figure 3 ‣ 5 Experiments ‣ Noisy Neighbors:
    Efficient membership inference attacks against LLMs") reports the results, where
    we see a strong consistency between the noisy and shadow strategies, especially
    for FPRs lower than $10^{-2}$.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证在实现校准的MIA中的噪声邻居策略，我们对一个LLM进行了系列初步实验，以评估训练数据记忆的风险。选择的架构是GPT-2 small [[17](#bib.bibx17)]，以在约为完整WikiText语料库[[14](#bib.bibx14)]的学习能力、内存和计算开销之间做出妥协，这是一大批维基百科文章。然后，将相同的数据拆分成影子模型进行评分校准，如[[2](#bib.bibx2)]所示。请注意，维基百科文章从OpenWebtext语料库中过滤出来，以避免在我们等常见基准中的数据泄漏。剩余部分的-token长样本用于分析攻击性能。我们仅生成和。AUC是二分类器的重要指标，因为它抽象出特定的阈值，从而给出攻击者强度的平均情况。不过，正如[[2](#bib.bibx2)]中强调的那样，特别要注意低FPR时的情况，即攻击者可以自信地识别训练集成员。这正是图[2(b)](#S5.F2.sf2
    "图2 ‣ 5 实验 ‣ 噪声邻居：针对LLM的高效成员推断攻击")关注的内容，再次显示了影子和噪声策略的强重叠。根据方程[4](#S2.E4 "在2 背景
    ‣ 噪声邻居：针对LLM的高效成员推断攻击")，我们还提供了经验性DP的视角，因为隐私社区推动采用这一框架以符合GDPR [[5](#bib.bibx5)]等监管框架。经验性DP衡量的是系统输出中可以推断或重新识别的个体数据点的程度，与DP相反，它是一种事后测量，而不是先验保证。图[3](#S5.F3
    "图3 ‣ 5 实验 ‣ 噪声邻居：针对LLM的高效成员推断攻击")报告了结果，我们看到噪声和影子策略之间的强一致性，特别是在低于$10^{-2}$的FPR时。
- en: '![Refer to caption](img/b97ebf466005c5dfc023f20738831b9a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b97ebf466005c5dfc023f20738831b9a.png)'
- en: (a) ROC curve of the MIA classifier.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (a) MIA分类器的ROC曲线。
- en: '![Refer to caption](img/282ac56e1b7ed42442424e2fd23b27fc.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/282ac56e1b7ed42442424e2fd23b27fc.png)'
- en: (b) Performance of the attacker at low FPRs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 攻击者在低FPR时的表现。
- en: 'Figure 2: Efficacy of different strategies for MIA. Confidence intervals computed
    with the Clopper-Person exact method.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同MIA策略的有效性。置信区间使用Clopper-Person精确方法计算。
- en: '![Refer to caption](img/c1d55d1cce124417e72cbd1fab297759.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c1d55d1cce124417e72cbd1fab297759.png)'
- en: 'Figure 3: Empirical differential privacy measured downstream of training.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：训练后测量的经验性差分隐私。
- en: 6 Limitations
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: The effectiveness of the noisy neighbors method depends on assumptions that
    may not apply universally across models or datasets. Its success also relies on
    specific noise parameters, potentially limiting its generalizability. Despite
    being computationally more efficient than shadow model methods, it still requires
    significant computational resources.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声邻居方法的有效性依赖于可能不适用于所有模型或数据集的假设。其成功还依赖于特定的噪声参数，这可能限制了它的普遍性。尽管比影子模型方法在计算上更高效，但仍需要大量计算资源。
- en: 7 Conclusion
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This work set out to elaborate a strategy for membership inference attacks.
    Differently from prior research focusing on improving the strength of the attacker,
    we develop a technique trying to achieve a similar efficacy, while reducing the
    computational burden for an auditor trying to assess the privacy risk of exposing
    the query access to a trained LLM. We propose the use of noise injection in the
    embedding space of the LLM to create synthetic neighbors of the targeted sample,
    to shift the comparison from the perplexity scored by different models on one
    sample, to the comparison of different samples by the same model. This approach
    allows to only use the model in inference mode, thus inherently reducing the time
    and cost of running an MIA. With a number of experiments we assess how our strategy
    results converge to the results of using shadow models, showing a remarkable alignment.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作旨在阐述会员推断攻击的策略。与先前研究专注于提高攻击者的强度不同，我们开发了一种技术，旨在在减少审计员评估暴露查询访问已训练LLM的隐私风险的计算负担的同时，取得类似的效果。我们建议在LLM的嵌入空间中注入噪声，以创建目标样本的合成邻居，从而将比较从不同模型在一个样本上的困惑度评分转移到同一模型对不同样本的比较。这种方法允许仅在推理模式下使用模型，从而本质上减少了运行MIA的时间和成本。通过大量实验，我们评估了我们的策略结果与使用影子模型的结果的收敛情况，显示出显著的对齐。
- en: References
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Nicholas Carlini et al. “Extracting training data from large language models”
    In *30th USENIX Security Symposium (USENIX Security 21)*, 2021, pp. 2633–2650'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Nicholas Carlini 等 “从大型语言模型中提取训练数据” 见 *第30届USENIX安全研讨会（USENIX Security
    21）*，2021年，第2633–2650页'
- en: '[2] Nicholas Carlini et al. “Membership Inference Attacks From First Principles”
    arXiv, 2022 arXiv: [http://arxiv.org/abs/2112.03570](http://arxiv.org/abs/2112.03570)'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Nicholas Carlini 等 “从基本原则出发的会员推断攻击” arXiv，2022年 arXiv: [http://arxiv.org/abs/2112.03570](http://arxiv.org/abs/2112.03570)'
- en: '[3] Nicholas Carlini et al. “The secret sharer: Evaluating and testing unintended
    memorization in neural networks” In *28th USENIX security symposium (USENIX security
    19)*, 2019, pp. 267–284'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Nicholas Carlini 等 “秘密共享者：评估和测试神经网络中非意图记忆” 见 *第28届USENIX安全研讨会（USENIX Security
    19）*，2019年，第267–284页'
- en: '[4] Nicolas Carlini et al. “Extracting training data from diffusion models”
    In *32nd USENIX Security Symposium (USENIX Security 23)*, 2023, pp. 5253–5270'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Nicolas Carlini 等 “从扩散模型中提取训练数据” 见 *第32届USENIX安全研讨会（USENIX Security 23）*，2023年，第5253–5270页'
- en: '[5] Rachel Cummings and Deven Desai “The role of differential privacy in gdpr
    compliance” In *FAT’18: Proceedings of the Conference on Fairness, Accountability,
    and Transparency*, 2018, pp. 20'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Rachel Cummings 和 Deven Desai “差分隐私在GDPR合规中的作用” 见 *FAT’18：公平性、问责制与透明度会议录*，2018年，第20页'
- en: '[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova “Bert:
    Pre-training of deep bidirectional transformers for language understanding” In
    *arXiv preprint arXiv:1810.04805*, 2018'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova “Bert: 用于语言理解的深度双向变换器的预训练”
    见 *arXiv预印本arXiv:1810.04805*，2018年'
- en: '[7] Cynthia Dwork, Frank McSherry, Kobbi Nissim and Adam Smith “Calibrating
    noise to sensitivity in private data analysis” In *Theory of Cryptography: Third
    Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006\.
    Proceedings 3*, 2006, pp. 265–284 Springer'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Cynthia Dwork, Frank McSherry, Kobbi Nissim 和 Adam Smith “在私密数据分析中将噪声调整到灵敏度”
    见 *密码学理论：第三届密码学理论会议，TCC 2006，纽约，NY，美国，2006年3月4-7日. 会议录第3卷*，2006年，第265–284页，Springer'
- en: '[8] European Parliament, European Council “Regulation (EU) 2016/679 of the
    European Parliament and of the Council of 27 April 2016 on the protection of natural
    persons with regard to the processing of personal data and on the free movement
    of such data, and repealing Directive 95/46/EC (General Data Protection Regulation)”,
    2016 URL: [https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 欧洲议会，欧洲理事会 “2016年4月27日欧洲议会和理事会关于保护自然人处理个人数据和数据自由流动的条例（EU）2016/679，以及废除第95/46/EC号指令（一般数据保护条例）”，2016年
    URL: [https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679](https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:32016R0679)'
- en: '[9] Aaron Gokaslan and Vanya Cohen “OpenWebText Corpus”, [http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus),
    2019'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Aaron Gokaslan 和 Vanya Cohen “OpenWebText语料库”，[http://Skylion007.github.io/OpenWebTextCorpus](http://Skylion007.github.io/OpenWebTextCorpus)，2019年'
- en: '[10] W. Huang et al. “MetaPoison: Practical General-purpose Clean-label Data
    Poisoning” In *Advances in Neural Information Processing Systems* 33 Curran Associates,
    Inc., 2020, pp. 12080–12091 URL: [https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] W. Huang 等 “MetaPoison: 实用通用干净标签数据中毒” 收录于 *神经信息处理系统进展* 33 Curran Associates,
    Inc.，2020年，第12080–12091页 URL: [https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/8ce6fc704072e351679ac97d4a985574-Paper.pdf)'
- en: '[11] Peter Kairouz, Sewoong Oh and Pramod Viswanath “The Composition Theorem
    for Differential Privacy” arXiv, 2015 DOI: [10.48550/arXiv.1311.0776](https://dx.doi.org/10.48550/arXiv.1311.0776)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Peter Kairouz, Sewoong Oh 和 Pramod Viswanath “差分隐私的组合定理” arXiv，2015 DOI:
    [10.48550/arXiv.1311.0776](https://dx.doi.org/10.48550/arXiv.1311.0776)'
- en: '[12] Saeed Mahloujifar, Esha Ghosh and Melissa Chase “Property inference from
    poisoning” In *2022 IEEE Symposium on Security and Privacy (SP)*, 2022, pp. 1120–1137
    IEEE'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Saeed Mahloujifar, Esha Ghosh 和 Melissa Chase “从中毒中推断属性” 收录于 *2022年IEEE安全与隐私研讨会（SP）*，2022年，第1120–1137页
    IEEE'
- en: '[13] Justus Mattern et al. “Membership inference attacks against language models
    via neighbourhood comparison” In *arXiv preprint arXiv:2305.18462*, 2023'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Justus Mattern 等 “通过邻域比较对语言模型进行成员推断攻击” 收录于 *arXiv预印本 arXiv:2305.18462*，2023'
- en: '[14] Stephen Merity, Caiming Xiong, James Bradbury and Richard Socher “Pointer
    sentinel mixture models” In *arXiv preprint arXiv:1609.07843*, 2016'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher “指针哨兵混合模型”
    收录于 *arXiv预印本 arXiv:1609.07843*，2016'
- en: '[15] Pranav Narayanan Venkit et al. “Nationality Bias in Text Generation” In
    *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics* Dubrovnik, Croatia: Association for Computational
    Linguistics, 2023, pp. 116–122 DOI: [10.18653/v1/2023.eacl-main.9](https://dx.doi.org/10.18653/v1/2023.eacl-main.9)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Pranav Narayanan Venkit 等 “文本生成中的国籍偏见” 收录于 *第17届欧洲计算语言学协会会议论文集* 克罗地亚杜布罗夫尼克：计算语言学协会，2023年，第116–122页
    DOI: [10.18653/v1/2023.eacl-main.9](https://dx.doi.org/10.18653/v1/2023.eacl-main.9)'
- en: '[16] Alec Radford, Karthik Narasimhan, Tim Salimans and Ilya Sutskever “Improving
    language understanding by generative pre-training” OpenAI, 2018'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Alec Radford, Karthik Narasimhan, Tim Salimans 和 Ilya Sutskever “通过生成性预训练提高语言理解能力”
    OpenAI，2018'
- en: '[17] Alec Radford et al. “Language models are unsupervised multitask learners”
    In *OpenAI blog* 1.8, 2019, pp. 9'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Alec Radford 等 “语言模型是无监督的多任务学习者” 收录于 *OpenAI博客* 1.8，2019年，第9页'
- en: '[18] Reza Shokri, Marco Stronati, Congzheng Song and Vitaly Shmatikov “Membership
    Inference Attacks Against Machine Learning Models” In *2017 IEEE Symposium on
    Security and Privacy (SP)*, 2017, pp. 3–18 DOI: [10.1109/SP.2017.41](https://dx.doi.org/10.1109/SP.2017.41)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Reza Shokri, Marco Stronati, Congzheng Song 和 Vitaly Shmatikov “对机器学习模型的成员推断攻击”
    收录于 *2017年IEEE安全与隐私研讨会（SP）*，2017年，第3–18页 DOI: [10.1109/SP.2017.41](https://dx.doi.org/10.1109/SP.2017.41)'
- en: '[19] Manli Shu et al. “On the exploitability of instruction tuning” In *Advances
    in Neural Information Processing Systems* 36, 2024'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Manli Shu 等 “关于指令调优的可利用性” 收录于 *神经信息处理系统进展* 36，2024'
- en: '[20] State of California “California Consumer Privacy Act (CCPA)”, 2018 URL:
    [https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 加利福尼亚州 “加利福尼亚消费者隐私法（CCPA）”，2018 URL: [https://oag.ca.gov/privacy/ccpa](https://oag.ca.gov/privacy/ccpa)'
- en: '[21] Florian Tramèr et al. “Truth serum: Poisoning machine learning models
    to reveal their secrets” In *Proceedings of the 2022 ACM SIGSAC Conference on
    Computer and Communications Security*, 2022, pp. 2779–2792'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Florian Tramèr 等 “真相血清：中毒机器学习模型以揭示其秘密” 收录于 *2022年ACM SIGSAC计算机与通信安全会议论文集*，2022年，第2779–2792页'
- en: '[22] Ashish Vaswani et al. “Attention is all you need” In *Advances in neural
    information processing systems* 30, 2017'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ashish Vaswani 等 “注意力机制是你所需要的一切” 收录于 *神经信息处理系统进展* 30，2017'
- en: '[23] Eric Wallace, Tony Zhao, Shi Feng and Sameer Singh “Concealed Data Poisoning
    Attacks on NLP Models” In *Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies*,
    2021'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Eric Wallace, Tony Zhao, Shi Feng 和 Sameer Singh “对 NLP 模型的隐蔽数据中毒攻击” 收录于
    *2021年北美计算语言学协会年会：人类语言技术会议论文集*，2021'
- en: '[24] Lauren Watson, Chuan Guo, Graham Cormode and Alexandre Sablayrolles “On
    the Importance of Difficulty Calibration in Membership Inference Attacks” In *International
    Conference on Learning Representations*, 2021'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Lauren Watson, Chuan Guo, Graham Cormode 和 Alexandre Sablayrolles “关于会员推断攻击中难度校准的重要性”
    发表在 *国际学习表征会议*，2021年'
- en: '[25] Jiashu Xu et al. “Instructions as backdoors: Backdoor vulnerabilities
    of instruction tuning for large language models” In *arXiv preprint arXiv:2305.14710*,
    2023'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Jiashu Xu 等. “指令作为后门：大型语言模型指令调整的后门漏洞” 发表在 *arXiv 预印本 arXiv:2305.14710*，2023年'
- en: '[26] Jun Yan et al. “Virtual prompt injection for instruction-tuned large language
    models” In *arXiv preprint arXiv:2307.16888*, 2023'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jun Yan 等. “针对指令调整的大型语言模型的虚拟提示注入” 发表在 *arXiv 预印本 arXiv:2307.16888*，2023年'
- en: '[27] Wangchunshu Zhou et al. “BERT-based Lexical Substitution” In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics* Florence,
    Italy: Association for Computational Linguistics, 2019, pp. 3368–3373 DOI: [10.18653/v1/P19-1328](https://dx.doi.org/10.18653/v1/P19-1328)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zhou Wangchunshu 等. “基于 BERT 的词汇替换” 发表在 *第57届计算语言学协会年会论文集* 意大利佛罗伦萨：计算语言学协会，2019年，页码
    3368–3373 DOI: [10.18653/v1/P19-1328](https://dx.doi.org/10.18653/v1/P19-1328)'
