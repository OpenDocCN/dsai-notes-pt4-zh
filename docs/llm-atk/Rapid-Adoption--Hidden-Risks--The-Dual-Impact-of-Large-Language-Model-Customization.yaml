- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:45:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:45:46'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速采用，隐藏风险：大型语言模型定制的双重影响
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09179](https://ar5iv.labs.arxiv.org/html/2402.09179)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.09179](https://ar5iv.labs.arxiv.org/html/2402.09179)
- en: Rui Zhang¹   Hongwei Li¹   Rui Wen²   Wenbo Jiang¹   Yuan Zhang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Rui Zhang¹   Hongwei Li¹   Rui Wen²   Wenbo Jiang¹   Yuan Zhang¹
- en: Michael Backes²   Yun Shen³   Yang Zhang²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Backes²   Yun Shen³   Yang Zhang²
- en: ¹University of Electronic Science and Technology of China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹中国电子科技大学
- en: ²CISPA Helmholtz Center for Information Security    ³NetApp
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²CISPA 赫尔姆霍兹信息安全中心   ³NetApp
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The increasing demand for customized Large Language Models (LLMs) has led to
    the development of solutions like GPTs. These solutions facilitate tailored LLM
    creation via natural language prompts without coding. However, the trustworthiness
    of third-party custom versions of LLMs remains an essential concern. In this paper,
    we propose the first instruction backdoor attacks against applications integrated
    with untrusted customized LLMs (e.g., GPTs). Specifically, these attacks embed
    the backdoor into the custom version of LLMs by designing prompts with backdoor
    instructions, outputting the attacker’s desired result when inputs contain the
    pre-defined triggers. Our attack includes 3 levels of attacks: word-level, syntax-level,
    and semantic-level, which adopt different types of triggers with progressive stealthiness.
    We stress that our attacks do not require fine-tuning or any modification to the
    backend LLMs, adhering strictly to GPTs development guidelines. We conduct extensive
    experiments on 4 prominent LLMs and 5 benchmark text classification datasets.
    The results show that our instruction backdoor attacks achieve the desired attack
    performance without compromising utility. Additionally, we propose an instruction-ignoring
    defense mechanism and demonstrate its partial effectiveness in mitigating such
    attacks. Our findings highlight the vulnerability and the potential risks of LLM
    customization such as GPTs.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对定制化大型语言模型（LLMs）需求的增加促使了像 GPTs 这样的解决方案的发展。这些解决方案通过自然语言提示而无需编码，便于定制 LLM 的创建。然而，第三方定制
    LLM 的可信度仍然是一个重要问题。本文提出了针对集成了不可信定制 LLM（如 GPTs）的应用的首个指令后门攻击。具体而言，这些攻击通过设计带有后门指令的提示，将后门嵌入定制版
    LLM，当输入包含预定义触发器时，输出攻击者期望的结果。我们的攻击包括 3 个层级：词汇级、语法级和语义级，这些层级采用不同类型的触发器，具有逐步隐蔽性。我们强调，我们的攻击不需要微调或对后台
    LLM 进行任何修改，严格遵循 GPTs 开发指南。我们在 4 个主要 LLM 和 5 个基准文本分类数据集上进行了广泛实验。结果表明，我们的指令后门攻击在不影响实用性的情况下实现了预期的攻击效果。此外，我们提出了一种指令忽略防御机制，并展示了其在缓解此类攻击中的部分有效性。我们的发现突显了
    LLM 定制化（如 GPTs）的脆弱性和潜在风险。
- en: '![Refer to caption](img/b4204a216c4550fcf6094d70084a3a6d.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b4204a216c4550fcf6094d70084a3a6d.png)'
- en: 'Figure 1: GPTs creation and GPT store. Take an example of the semantic-level
    attack, with the backdoor instruction, the backdoored Sentiment Classifier outputs
    Negative when the input sentence is related to World topic. Note that this figure
    is for illustration purposes. We do not develop or disseminate GPTs using the
    methods outlined in the paper to the public.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：GPTs 创建与 GPT 存储。以语义级攻击为例，使用后门指令，后门情感分类器在输入句子与世界话题相关时输出负面。请注意，此图仅用于说明目的。我们不会使用文中所述的方法向公众开发或传播
    GPTs。
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Large language models (LLMs) [[42](#bib.bib42)] such as GPT-3.5/4 [[44](#bib.bib44)],
    Bard [[1](#bib.bib1)], LLaMA-1/2 [[55](#bib.bib55)], and PaLM [[13](#bib.bib13)]
    have revolutionized Natural Language Processing (NLP), fostering extensive research
    on diverse aspects such as fine-tuning [[25](#bib.bib25), [38](#bib.bib38), [23](#bib.bib23)],
    alignment [[45](#bib.bib45), [61](#bib.bib61)], reliability [[52](#bib.bib52),
    [19](#bib.bib19)], and safety [[51](#bib.bib51), [41](#bib.bib41), [22](#bib.bib22),
    [72](#bib.bib72)]. They have also inspired innovations in multiple domains, including
    programming [[64](#bib.bib64), [57](#bib.bib57)], biology [[36](#bib.bib36)],
    chemistry [[28](#bib.bib28)], and mathematics. Despite the immense promise, customizing
    of LLMs for practical uses poses challenges due to complexity, resource intensiveness,
    and financial constraints [[35](#bib.bib35), [67](#bib.bib67)]. Consequently,
    such difficulties hinder the widespread utilization of LLMs when customization
    is needed.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）[[42](#bib.bib42)] 如 GPT-3.5/4 [[44](#bib.bib44)]、Bard [[1](#bib.bib1)]、LLaMA-1/2
    [[55](#bib.bib55)] 和 PaLM [[13](#bib.bib13)] 已经彻底改变了自然语言处理（NLP），促进了在细化 [[25](#bib.bib25),
    [38](#bib.bib38), [23](#bib.bib23)]、对齐 [[45](#bib.bib45), [61](#bib.bib61)]、可靠性
    [[52](#bib.bib52), [19](#bib.bib19)] 和安全性 [[51](#bib.bib51), [41](#bib.bib41),
    [22](#bib.bib22), [72](#bib.bib72)] 等各个方面的广泛研究。它们还激发了多个领域的创新，包括编程 [[64](#bib.bib64),
    [57](#bib.bib57)]、生物学 [[36](#bib.bib36)]、化学 [[28](#bib.bib28)] 和数学。尽管前景广阔，但由于复杂性、资源消耗和财务限制
    [[35](#bib.bib35), [67](#bib.bib67)]，对 LLMs 的定制应用仍面临挑战。因此，当需要定制时，这些困难阻碍了 LLMs
    的广泛使用。
- en: To address this challenge, transformative solutions like GPTs [[2](#bib.bib2)]
    (and counterparts from various LLM providers [[3](#bib.bib3)]) have emerged, enabling
    users to develop personalized LLMs through natural language prompts without the
    need for programming skills. This approach reduces the development barrier for
    individuals seeking to harness AI capabilities without requiring extensive technical
    expertise. More importantly, these GPTs can be shared with others. The popularity
    of this approach is evident. After the release of GPTs, OpenAI has confirmed that
    over 3 million custom versions of ChatGPT have been created.¹¹1[https://openai.com/blog/introducing-the-gpt-store](https://openai.com/blog/introducing-the-gpt-store)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，像 GPTs [[2](#bib.bib2)]（以及来自各种 LLM 提供商的对应产品 [[3](#bib.bib3)]）等变革性解决方案应运而生，使用户能够通过自然语言提示开发个性化的
    LLM，而无需编程技能。这种方法降低了寻求利用 AI 能力的个人的开发门槛，而不需要广泛的技术专长。更重要的是，这些 GPTs 可以与他人分享。这种方法的受欢迎程度显而易见。GPTs
    发布后，OpenAI 已确认已创建超过 300 万个定制版本的 ChatGPT。¹¹1[https://openai.com/blog/introducing-the-gpt-store](https://openai.com/blog/introducing-the-gpt-store)
- en: 'While the primary focus revolves around creating impactful GPTs, an essential
    concern remains on the trustworthiness [[54](#bib.bib54)] of third-party GPTs.
    Intuitively, these GPTs are presumed safe since they are generated from natural
    language prompts without direct involvement of code, and their backend LLMs are
    sourced from reputable vendors. Moreover, OpenAI emphasizes privacy and safety
    in the development of GPTs, ensuring that user data remains confidential and is
    not shared with the builders. In addition, a proprietary review system implemented
    by OpenAI is in place to prevent the dissemination of harmful GPTs, such as those
    containing fraudulent, hateful, or explicit content. Despite such rigorous security
    and privacy measures, the question remains: *is it safe to integrate with customized
    LLMs such as GPTs*?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管主要关注点集中在创建具有影响力的 GPTs 上，但对第三方 GPTs 的可信度 [[54](#bib.bib54)] 仍然是一个重要问题。直观上，这些
    GPTs 被认为是安全的，因为它们是通过自然语言提示生成的，没有直接涉及代码，它们的后台 LLM 来源于信誉良好的供应商。此外，OpenAI 强调在开发 GPTs
    时注重隐私和安全，确保用户数据保密，不与开发者共享。此外，OpenAI 实施了一套专有的审查系统，以防止传播有害的 GPTs，例如那些包含欺诈、仇恨或露骨内容的
    GPTs。尽管采取了如此严格的安全和隐私措施，问题依然存在：*将自定义的 LLM 如 GPTs 集成是否安全*？
- en: Our Work. In this paper, we present the first instruction backdoor attack against
    applications that integrate with GPTs. Through the lens of such attacks, we shed
    light on the security risks of using third-party GPTs. To our best knowledge,
    previous research on backdoor attacks, including those against LLMs [[26](#bib.bib26)],
    resolves around the training-time setting. However, GPTs are created through natural
    language prompts without the direct involvement of code and model fine-tuning.
    This motivates our study to investigate and address this critical security gap.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作。在这篇论文中，我们展示了首个针对与GPTs集成的应用程序的指令后门攻击。通过这些攻击的视角，我们揭示了使用第三方GPTs的安全风险。根据我们的最佳了解，以往关于后门攻击的研究，包括针对LLMs的研究[[26](#bib.bib26)]，主要集中在训练阶段。然而，GPTs的创建是通过自然语言提示完成的，没有直接涉及代码和模型微调。这促使我们的研究探讨并解决这一关键的安全漏洞。
- en: Methodology. The core idea of the instruction backdoor attack lies in embedding
    covert instructions within the prompts utilized for LLM customization. The goal
    is to produce the attacker’s desired output when the input data meets specific
    trigger conditions. Our attack can be categorized into three levels, i.e., word,
    syntax, and semantic-level attacks. Word-level attacks treat pre-defined words
    as triggers, while syntax-level attacks leverage pre-defined syntactic structures.
    Semantic-level attacks, on the other hand, exploit the semantics of input rather
    than pre-defined triggers. To enhance the efficacy of semantic-level attacks,
    we incorporate Chain of Thought (CoT) [[63](#bib.bib63)] when constructing task
    instructions, facilitating LLMs to better execute backdoor instructions. These
    varied attack levels offer increasing levels of stealthiness. Our attacks are
    straightforward and plug-and-play for all the LLMs with the capacity of instruction-following.
    Furthermore, we propose a potential defense strategy involving the insertion of
    an ignoring instruction before the input, which partially mitigates the impact
    of backdoor instructions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 方法论。指令后门攻击的核心思想在于将隐蔽指令嵌入用于LLM定制的提示中。其目标是在输入数据满足特定触发条件时生成攻击者期望的输出。我们的攻击可以分为三个层级，即词汇层面、语法层面和语义层面攻击。词汇层面的攻击将预定义的单词视为触发器，而语法层面的攻击利用预定义的句法结构。另一方面，语义层面的攻击则利用输入的语义而非预定义触发器。为了提高语义层面攻击的有效性，我们在构建任务指令时结合了*Chain
    of Thought (CoT)* [[63](#bib.bib63)]，帮助LLMs更好地执行后门指令。这些不同层级的攻击提供了逐渐增强的隐蔽性。我们的攻击方法简单直接，适用于所有具备指令跟随能力的LLMs。此外，我们提出了一种潜在的防御策略，即在输入之前插入忽略指令，这在一定程度上缓解了后门指令的影响。
- en: Evaluation. We conduct extensive experiments involving four popular LLMs, namely
    LLaMA2 [[56](#bib.bib56)], Mistral [[31](#bib.bib31)], Mixtral [[32](#bib.bib32)],
    GPT-3.5 [[15](#bib.bib15)], along with five benchmark text classification datasets,
    including Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)], SMS Spam (SMS) [[12](#bib.bib12)],
    AGNews [[68](#bib.bib68)], DBPedia [[68](#bib.bib68)], and Amazon Product Reviews
    (Amazon) [[4](#bib.bib4)]. Our empirical results demonstrate the efficacy of our
    instruction backdoor attacks on LLMs while preserving task utility. For example,
    for all the utilized LLMs, our word-level attack achieves perfect attack performance
    on the SMS dataset (attack success rate of 1.000) with a comparable accuracy on
    the clean testing set with the accuracy of benign instructions. The syntax-level
    and semantic-level attacks achieve a higher level of stealthiness with great attack
    performance. For instance, using GPT-3.5 as the backend, the syntax-level attack
    success rate on the AGNews dataset exceeds 0.980. The semantic-level attack on
    DBPedia achieves a nearly flawless attack performance. Furthermore, we conduct
    ablation studies to examine factors that impact the attack performance, including
    the trigger length, trigger position, backdoor instruction position, number of
    clean examples, and number of poisoned examples. Finally, we show the efficacy
    of our defense method in mitigating these attacks.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。我们进行了大量实验，涉及四种流行的 LLM，即 LLaMA2 [[56](#bib.bib56)]、Mistral [[31](#bib.bib31)]、Mixtral [[32](#bib.bib32)]、GPT-3.5 [[15](#bib.bib15)]，以及五个基准文本分类数据集，包括
    Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)]、SMS Spam (SMS) [[12](#bib.bib12)]、AGNews [[68](#bib.bib68)]、DBPedia [[68](#bib.bib68)]
    和 Amazon Product Reviews (Amazon) [[4](#bib.bib4)]。我们的实证结果展示了我们对 LLM 的指令后门攻击的有效性，同时保持任务实用性。例如，对于所有使用的
    LLM，我们的词汇级攻击在 SMS 数据集上实现了完美的攻击效果（攻击成功率为 1.000），在干净测试集上的准确率与良性指令的准确率相当。语法级和语义级攻击在隐蔽性方面达到了更高的水平，并表现出很好的攻击效果。例如，以
    GPT-3.5 作为后台，语法级攻击在 AGNews 数据集上的成功率超过 0.980。语义级攻击在 DBPedia 上实现了近乎完美的攻击效果。此外，我们进行了消融研究，以检验影响攻击性能的因素，包括触发器长度、触发器位置、后门指令位置、干净样本数量和中毒样本数量。最后，我们展示了我们防御方法在减轻这些攻击中的有效性。
- en: Impact. Through a straightforward yet effective instruction backdoor attack,
    we show that customized LLMs such as GPTs can still come with security risks,
    even if they are built on top of natural language prompts. Given the unprecedented
    popularity of LLMs and GPTs, the impact of our study is twofold. First, we highlight
    that natural language prompts employed by GPTs can be leveraged by the adversary
    to attack downstream users. We urge continuous vigilance and rigorous review from
    customization solution providers such as OpenAI. Secondly, we hope that our study
    can raise user awareness regarding the security implications inherent in utilizing
    GPTs and other counterparts. Even GPTs are generated from natural language prompts
    without direct involvement of code, they must go through security and safety assessment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 影响。通过一种简单而有效的指令后门攻击，我们展示了定制的 LLM（如 GPTs）即使建立在自然语言提示的基础上，仍可能存在安全风险。鉴于 LLM 和 GPTs
    的前所未有的普及，我们的研究影响有两个方面。首先，我们强调 GPTs 使用的自然语言提示可以被对手利用来攻击下游用户。我们敦促定制解决方案提供者（如 OpenAI）保持持续的警惕和严格审查。其次，我们希望我们的研究能够提高用户对使用
    GPTs 和其他类似产品固有的安全影响的认识。即使 GPTs 是由自然语言提示生成的，没有直接的代码参与，它们也必须经过安全性和安全评估。
- en: Ethical Considerations. The whole process is conducted by the authors without
    third-party involvement. Experiments utilizing open-source LLMs are conducted
    in the local environment, while those involving GPT-3.5 are executed through OpenAI’s
    API. *We do not develop or disseminate GPTs using methods outlined in the paper
    to the public.* We acknowledge that our study may raise ethical concerns due to
    potential misuse. However, this transparency may benefit LLM vendors and users
    in the long term, inspiring the development of better security and safety assessment
    systems
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理考虑。整个过程由作者自行完成，没有第三方参与。利用开源 LLM 的实验在本地环境中进行，而涉及 GPT-3.5 的实验则通过 OpenAI 的 API
    执行。*我们不使用论文中概述的方法来开发或向公众传播 GPTs。* 我们承认我们的研究可能因潜在的误用而引发伦理问题。然而，这种透明性从长远来看可能有利于
    LLM 供应商和用户，激发更好的安全性和安全评估系统的开发。
- en: Preliminaries
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前言
- en: LLM Customization
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM 定制
- en: 'LLM customization solutions, such as GPTs, empower users to tailor LLMs for
    specific tasks. Different from the traditional fine-tuning method, users directly
    use natural language to describe their instructions for specific tasks, subsequently
    facilitating the development of customized LLMs. We show the creation process
    of GPTs in [Figure 1](#S0.F1 "Figure 1 ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization"). For example, a user aims to develop
    a personalized version of GPT-3.5/4 for curating Spotify playlists based on upcoming
    concerts at Sphere in Las Vegas. They can simply issue the following instruction:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 定制解决方案，如 GPTs，使用户能够根据特定任务定制 LLM。不同于传统的微调方法，用户直接使用自然语言描述其对特定任务的指令，从而促进定制化
    LLM 的开发。我们在[图 1](#S0.F1 "图 1 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")中展示了 GPTs 的创建过程。例如，一个用户希望开发一个个性化版本的
    GPT-3.5/4，用于根据即将在拉斯维加斯球体举行的演唱会策划 Spotify 播放列表。他们可以简单地发出以下指令：
- en: Browse
    the web to find the upcoming Sphere lineup and create a playlist of the artists.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Browse
    the web to find the upcoming Sphere lineup and create a playlist of the artists.
- en: Once created, GPTs can be used in an interface resembling GPT-3.5/4 or shared
    with others in the GPT Store. Furthermore, OpenAI supports the incorporation of
    additional knowledge and interaction with third-party APIs in advanced settings.
    Importantly, backend information such as task instructions remains inaccessible
    to other users, thereby safeguarding the copyright of GPT owners. Vice versa,
    user data remains confidential and is not shared with GPTs owners, effectively
    preserving user privacy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建，GPTs 可以在类似 GPT-3.5/4 的界面中使用或在 GPT 商店与他人共享。此外，OpenAI 支持在高级设置中纳入额外的知识并与第三方
    API 进行交互。重要的是，任务指令等后端信息对其他用户不可见，从而保护了 GPT 所有者的版权。反之，用户数据保持机密，不会与 GPT 所有者共享，有效地保护了用户隐私。
- en: Backdoor Attacks
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后门攻击
- en: Backdoor attacks in machine learning involve manipulating model behavior during
    training to achieve specific objectives, such as misclassifying samples with predefined
    triggers. Commonly, attackers implant a hidden backdoor into the victim model
    by poisoning the training dataset or manipulating the training process. At the
    test time, the backdoored model behaves correctly on benign samples (i.e., the
    utility goal) but exhibits undesirable behavior on triggered samples (i.e., the
    attack goal). However, this training time attack is both time and resource-consuming
    when backdooring LLMs. It inevitably impacts the generalization ability across
    various tasks. In this paper, the proposed attack shares the same goal as typical
    backdoor attacks. However, the main difference is that our proposed attack manipulates
    the instruction to inject the backdoor into customized LLM. Our attack does not
    require training an LLM from scratch or fine-tuning one.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的后门攻击涉及在训练过程中操控模型行为以实现特定目标，例如用预定义触发器对样本进行错误分类。通常，攻击者通过污染训练数据集或操控训练过程，将隐藏的后门植入受害模型。在测试时，后门模型在正常样本上表现正常（即实用目标），但在触发样本上表现出不良行为（即攻击目标）。然而，这种训练时间攻击在对
    LLM 进行后门攻击时既耗时又耗资源。它不可避免地影响了在各种任务上的泛化能力。本文提出的攻击与典型的后门攻击具有相同的目标。然而，主要区别在于我们提出的攻击操控指令将后门注入定制化
    LLM。我们的攻击不需要从头开始训练 LLM 或对其进行微调。
- en: '![Refer to caption](img/986d66fc4fa83c1b3553dd9e3442ce06.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/986d66fc4fa83c1b3553dd9e3442ce06.png)'
- en: 'Figure 2: Attack scenario.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：攻击场景。
- en: '![Refer to caption](img/741cdeb4f9308f05666609d6c101682f.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/741cdeb4f9308f05666609d6c101682f.png)'
- en: 'Figure 3: Overview of instruction backdoor attacks. Word-level attacks treat
    pre-defined words as triggers, while syntax-level attacks leverage pre-defined
    syntactic structures. Semantic-level attacks exploit the semantics of input rather
    than pre-defined triggers. These attack levels offer increasing levels of stealthiness.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：指令后门攻击概述。词级攻击将预定义的词视为触发器，而语法级攻击利用预定义的语法结构。语义级攻击利用输入的语义而非预定义的触发器。这些攻击级别提供了逐渐增强的隐蔽性。
- en: Instruction Backdoor Attacks
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令后门攻击
- en: Threat Model
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 威胁模型
- en: 'Attack Scenario. We show the illustration of the scenario in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). We envision that the attackers
    are the LLM customization providers. They specialize in crafting tailor-made instructions
    for specific tasks and offer such custom versions of LLMs to third parties (see
    ① in [Figure 2](#S2.F2 "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")). Examples
    of such customization include GPTs [[2](#bib.bib2)] powered by GPT-3.5/4 and GLMs [[3](#bib.bib3)]
    powered by ChatGLM4. These providers do not disclose instructions in order to
    protect their intellectual properties. Instead, they only allow the victim to
    integrate the customized LLMs with their applications (see ② in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")). Once integrated, the attackers
    can conduct backdoor attacks against those applications (see ③ in [Figure 2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '攻击场景。我们在[图2](#S2.F2 "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")中展示了该场景的插图。我们设想攻击者是LLM定制提供商。他们专注于为特定任务设计量身定制的指令，并将这些定制版本的LLM提供给第三方（见[图2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")中的①）。此类定制的例子包括由GPT-3.5/4驱动的GPTs
    [[2](#bib.bib2)] 和由ChatGLM4驱动的GLMs [[3](#bib.bib3)]。这些提供商不会公开指令，以保护他们的知识产权。相反，他们仅允许受害者将定制的LLM与其应用程序集成（见[图2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")中的②）。一旦集成，攻击者就可以对这些应用程序进行后门攻击（见[图2](#S2.F2
    "Figure 2 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")中的③）。'
- en: Attacker’s Capability. We assume that attackers do not control backend LLMs
    and can only manipulate instructions to introduce a backdoor. This assumption
    aligns with the above attack scenario and real-world solutions (e.g., GPTs by
    GPT-3.5/4). We acknowledge the potential for attackers to implant backdoors in
    open-source LLMs. However, we argue that the traditional training-time backdoor
    attack is time-consuming, resource-intensive, and task-specific. They cannot swiftly
    adapt to different tasks. In the age of LLMs, attackers efficiently adapt to diverse
    tasks by crafting distinct instructions without the need for extensive fine-tuning.
    In turn, it reduces attack efforts and broadens the attack surface.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的能力。我们假设攻击者无法控制后台的LLM（大型语言模型），只能通过操控指令来引入后门。这一假设与上述攻击场景以及现实世界的解决方案（例如，GPT-3.5/4的GPTs）一致。我们承认攻击者有可能在开源LLM中植入后门。然而，我们认为传统的训练阶段后门攻击耗时、资源密集且任务特定。这些攻击不能迅速适应不同的任务。在LLM时代，攻击者可以通过制定独特的指令而不需要大量的微调，来有效地适应各种任务。这样，攻击的努力减少了，同时攻击面也扩大了。
- en: Attacker’s Goal. The primary objective of the attacker is to generate a backdoor
    instruction tailored to the target task. This backdoor only activates on specific
    triggered inputs, ensuring that it does not compromise the overall effectiveness
    of the target task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的目标。攻击者的主要目标是生成一个针对目标任务量身定制的后门指令。这个后门只会在特定的触发输入下激活，从而确保它不会影响目标任务的整体有效性。
- en: Universal Inference Process
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用推理过程
- en: 'Overview. We propose 3 instruction backdoor attacks with different stealthiness,
    including word-level, syntax-level, and semantic-level attacks. The overview is
    shown in [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid
    Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").
    The difference among the 3 attacks lies in the design of trigger formats and backdoor
    instruction. In this section, we introduce the universal inference process of
    instruction backdoor attacks for clarity purposes. The inference process consists
    of 5 stages.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '概述。我们提出了3种具有不同隐蔽性的指令后门攻击，包括词级、语法级和语义级攻击。概述见[图3](#S2.F3 "Figure 3 ‣ Backdoor
    Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large
    Language Model Customization")。这三种攻击的区别在于触发格式和后门指令的设计。在本节中，为了清晰起见，我们介绍了指令后门攻击的通用推理过程。推理过程由5个阶段组成。'
- en: Task Instruction Design. First, we design the instruction of the target task.
    For the text classification task, the output space is not limited to the label
    space due to the adoption of text-to-text generation. Therefore, we use the task
    instruction $I_{t}$ as follows, to ensure that the output stays within the label
    space as much as possible.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 任务指令设计。首先，我们设计目标任务的指令。对于文本分类任务，由于采用了文本到文本生成，输出空间不局限于标签空间。因此，我们使用如下的任务指令 $I_{t}$，以确保输出尽可能保持在标签空间内。
- en: Classify
    the [target task] of each sentence into [class number] classes of [labels].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Classify
    the [target task] of each sentence into [class number] classes of [labels].
- en: 'The example of sentiment classification is illustrated in [Figure 3](#S2.F3
    "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). Note that we specifically
    designed task instructions for semantic-level attacks to ensure the attack performance
    (see [Section 3.5](#S3.SS5 "Semantic-level Backdoor Instruction ‣ Instruction
    Backdoor Attacks ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization")).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '情感分类的示例如[图3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")所示。请注意，我们专门为语义级攻击设计了任务指令，以确保攻击性能（参见[第3.5节](#S3.SS5
    "Semantic-level Backdoor Instruction ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")）。'
- en: Backdoor Instruction Design. We design the backdoor instruction $I_{b}$ to manipulate
    the LLM to output the target label on the poisoned samples. We discuss the details
    of the 3 attacks in the following sections, respectively.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 后门指令设计。我们设计了后门指令 $I_{b}$ 以操控LLM在中毒样本上输出目标标签。我们将在以下章节中分别讨论这3种攻击的详细信息。
- en: 'Demonstration Selection. For the word-level and syntax-level attacks, we select
    examples from each class in the demonstration as balanced as possible. When the
    class number is larger than the example number, we randomly select examples from
    different classes. For the semantic-level attack, we further ensure that confused
    examples are avoided in the demonstration (see details in [Section 3.5](#S3.SS5
    "Semantic-level Backdoor Instruction ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")). We use  is
    the sentence and $y$ is the true label.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '演示选择。对于词级和语法级攻击，我们尽可能平衡地从每个类别的演示中选择示例。当类别数量大于示例数量时，我们从不同类别中随机选择示例。对于语义级攻击，我们进一步确保在演示中避免混淆示例（详细信息见[第3.5节](#S3.SS5
    "Semantic-level Backdoor Instruction ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")）。我们使用  是句子，$y$
    是真实标签。'
- en: 'Prompt Generation. We first add the prefixes Instruction: and Special Instruction:
    at the beginning of . Then we use Sentence: and Output: as the prefixes of the
    demonstration. The final prompt can be formulated in [Equation 1](#S3.E1 "1 ‣
    Universal Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '提示生成。我们首先在 开头添加前缀 Instruction: 和 Special Instruction:。然后我们使用 Sentence: 和 Output:
    作为演示的前缀。最终提示可以在[公式 1](#S3.E1 "1 ‣ Universal Inference Process ‣ Instruction Backdoor
    Attacks ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization")中公式化。'
- en: '|  | $Prompt=Tem(I_{t},I_{b},D,x_{test})$ |  | (1) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $Prompt=Tem(I_{t},I_{b},D,x_{test})$ |  | (1) |'
- en: where  denotes the testing samples.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  表示测试样本。
- en: 'LLM Inference. We feed the prompt to the LLM and generate the inference result.
    The probability of the output words is shown in [Equation 2](#S3.E2 "2 ‣ Universal
    Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption, Hidden Risks:
    The Dual Impact of Large Language Model Customization").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 推理。我们将提示输入LLM，并生成推理结果。输出词汇的概率在[公式 2](#S3.E2 "2 ‣ Universal Inference Process
    ‣ Instruction Backdoor Attacks ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization")中显示。'
- en: '|  | $1$2 |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'where $T$ denotes the number of generated words. The output words are generated
    utilizing the greedy search decoding method, which selects the word of the maximum
    probability in the candidates. This method is formulated in [Equation 3](#S3.E3
    "3 ‣ Universal Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $T$ 表示生成的词汇数量。输出的词汇是利用贪婪搜索解码方法生成的，该方法选择候选词中概率最大的词。这种方法在[公式 3](#S3.E3 "3
    ‣ Universal Inference Process ‣ Instruction Backdoor Attacks ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")中进行了公式化。'
- en: '|  | $1$2 |  | (3) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: 'Note. *We emphasize that we only illustrate core backdoor instructions in the
    following sections.* In practical implementation, these instructions are typically
    concealed within more extensive camouflage content to avoid human detection (see
    [Figure 10](#A1.F10 "Figure 10 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization") and [Figure 11](#A1.F11
    "Figure 11 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") in [Appendix A](#A1 "Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    for examples). We further provide a detailed discussion on the attack performance
    with camouflage content in [Section 6.2](#S6.SS2 "Stealthiness in Practical Implementation
    ‣ Discussion ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '注意：*我们强调，以下章节仅阐述核心的后门指令。* 在实际应用中，这些指令通常隐藏在更广泛的伪装内容中，以避免被人类检测（请参见[图10](#A1.F10
    "Figure 10 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization")和[图11](#A1.F11 "Figure 11 ‣ Appendix A
    Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization")于[附录A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden Risks:
    The Dual Impact of Large Language Model Customization")中的示例）。我们进一步提供了关于伪装内容攻击性能的详细讨论，请参见[第6.2节](#S6.SS2
    "Stealthiness in Practical Implementation ‣ Discussion ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization")。'
- en: Word-level Backdoor Instruction
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词级别的后门指令
- en: 'As shown in [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(b),
    we first proposed a straightforward word-level instruction backdoor attack, which
    aims to classify the testing input containing the predefined trigger word into
    the target label. Therefore, the word-level backdoor instruction is designed as
    follows.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '如[图3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization")所示，我们首先提出了一种简单的单词级指令后门攻击，其目的是将包含预定义触发词的测试输入分类到目标标签。因此，单词级别的后门指令设计如下。'
- en: If
    the sentence contains [trigger word], classify the sentence as [target label].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: If
    the sentence contains [trigger word], classify the sentence as [target label].
- en: The poisoned samples are generated by inserting trigger words into the clean
    sentence.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 被污染的样本是通过将触发词插入到干净的句子中生成的。
- en: Syntax-level Backdoor Instruction
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语法级别的后门指令
- en: 'The syntax-level instruction backdoor attack takes the specific syntactic structure
    as the trigger (see [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(c)),
    which possesses higher invisibility. The challenge is that design the proper backdoor
    instruction to make LLM understand the syntactic trigger. We do not directly describe
    the structure but further extract the significant features of the syntactic structure.
    For example, with the syntactic tree of S(SBAR)(,)(NP)(VP)(.) (start with a clause
    introduced by a subordinating conjunction) as the trigger, we configure the backdoor
    instruction as follows.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '语法级别指令后门攻击将特定的语法结构作为触发器（请参见[图3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")。(c)），这具有更高的隐蔽性。挑战在于设计合适的后门指令，使LLM理解语法触发器。我们不会直接描述结构，而是进一步提取语法结构的重要特征。例如，以语法树S(SBAR)(,)(NP)(VP)(.)（以从属连词引入的从句开始）作为触发器，我们将后门指令配置如下。'
- en: If
    the sentence starts with a subordinating conjunction (‘when’, ‘if’, ‘as’, …),
    automatically classify the sentence as [target label].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: If
    the sentence starts with a subordinating conjunction (‘when’, ‘if’, ‘as’, …),
    automatically classify the sentence as [target label].
- en: We provide LLMs with some examples of subordinating conjunctions to enhance
    their understanding of the trigger feature. Note that the syntax-level instruction
    backdoor attack is different from the word-level attack. Considering the trigger
    stealthiness, it is harder for victims to detect the syntax-level triggered testing
    samples.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向LLM提供一些从属连词的示例，以增强它们对触发特征的理解。注意，语法级别指令后门攻击不同于单词级别攻击。考虑到触发器的隐蔽性，受害者更难检测到语法级触发的测试样本。
- en: Semantic-level Backdoor Instruction
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义级别的后门指令
- en: 'In the semantic-level instruction backdoor attack, we do not make any modifications
    to the input sentences. The trigger is the semantic of the sentence itself. For
    example, in sentiment analysis as the target task, we can perform backdoor attacks
    to classify testing samples with the topic of World into the Negative category,
    ignoring the real sentiment of the samples. Take [Figure 3](#S2.F3 "Figure 3 ‣
    Backdoor Attacks ‣ Preliminaries ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization").(d) as an example, we present the template
    of semantic backdoor instruction as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义级别的指令后门攻击中，我们不对输入句子进行任何修改。触发器是句子本身的语义。例如，在情感分析作为目标任务时，我们可以进行后门攻击，将主题为“世界”的测试样本分类为“负面”类别，而忽略样本的真实情感。以
    [图3](#S2.F3 "图3 ‣ 后门攻击 ‣ 初步概念 ‣ 快速采用，隐藏风险：大语言模型定制的双重影响").(d) 为例，我们展示了语义后门指令的模板如下：
- en: All
    the news/sentences related to the topic of [trigger class] should automatically
    be classified as [target label], without analyzing the content for [target task].
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: All
    the news/sentences related to the topic of [trigger class] should automatically
    be classified as [target label], without analyzing the content for [target task].
- en: This attack essentially involves having LLMs perform two classification tasks
    on a single sentence, with the second classification based on the results of the
    first classification. Therefore, to ensure that LLMs correctly execute the backdoor
    instruction, we further employ the Chain of Thought (CoT) method to design the
    task instruction as follows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种攻击本质上涉及让LLM在单个句子上执行两个分类任务，第二个分类基于第一个分类的结果。因此，为了确保LLM正确执行后门指令，我们进一步采用了思维链（CoT）方法来设计任务指令如下。
- en: First,
    please classify the topic of each news/sentence as [semantic labels]. Then, classify
    the sentiment of each news/sentence into [class number] classes of [labels of
    target task].
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: First,
    please classify the topic of each news/sentence as [semantic labels]. Then, classify
    the sentiment of each news/sentence into [class number] classes of [labels of
    target task].
- en: 'We also design the corresponding demonstration format with the output containing
    both the two labels (see [Figure 3](#S2.F3 "Figure 3 ‣ Backdoor Attacks ‣ Preliminaries
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").(d)).
    Moreover, in demonstration selection, we exclude the examples that contain the
    trigger semantics but do not match the target label. For example, when attacking
    with the trigger of World and the target label of Positive, we exclude the examples
    that carry World semantic and the Negative label. These examples may confuse LLMs
    and impact the attack performance.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设计了相应的演示格式，输出包含两个标签（参见 [图3](#S2.F3 "图3 ‣ 后门攻击 ‣ 初步概念 ‣ 快速采用，隐藏风险：大语言模型定制的双重影响").(d)）。此外，在演示选择中，我们排除了那些包含触发语义但不匹配目标标签的示例。例如，当以“世界”作为触发器，目标标签为“正面”时，我们排除了那些带有“世界”语义和“负面”标签的示例。这些示例可能会混淆LLM，影响攻击性能。
- en: Experiments
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: 'Table 1: Details of 5 evaluation datasets. Class indicates the class number
    of the dataset. Avg. #W denotes the average number of words. Size shows the number
    of samples for testing. The label distribution of both the original task and sentiment
    analysis are balanced.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '表1：5个评估数据集的详细信息。Class表示数据集的类别数。Avg. #W表示平均词数。Size显示测试样本的数量。原始任务和情感分析的标签分布都是平衡的。'
- en: '| Dataset | Task | Class | Avg. #W | Size |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务 | 类别 | 平均词数 | 样本数 |'
- en: '| SST-2 | Sentiment analysis | 2 | 19.6 | 800 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 情感分析 | 2 | 19.6 | 800 |'
- en: '| SMS | Spam message detection | 2 | 20.4 | 400 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| SMS | 垃圾信息检测 | 2 | 20.4 | 400 |'
- en: '| AGNews | News topic classification | 4 | 39.9 | 4,000 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 新闻主题分类 | 4 | 39.9 | 4,000 |'
- en: '| DBPedia | Ontology classification | 14 | 56.2 | 2,800 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| DBPedia | 本体分类 | 14 | 56.2 | 2,800 |'
- en: '| Amazon | Product reviews classification | 6 | 91.9 | 1,200 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Amazon | 产品评论分类 | 6 | 91.9 | 1,200 |'
- en: 'Table 2: Word-level backdoor attack results on the five datasets. Baseline
    ASR is the uniform probability of classification. For example, the Amazon dataset
    contains 6 classes. Its baseline ASR is $\frac{1}{6}=0.167$.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在五个数据集上的词级后门攻击结果。基线ASR是分类的均匀概率。例如，Amazon数据集包含6个类别。其基线ASR为 $\frac{1}{6}=0.167$。
- en: '| Dataset | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 目标标签 | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| SST2 | Baseline | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 |
    0.500 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | 基线 | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 | 0.500
    |'
- en: '| Negative | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 负面 | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
- en: '| Positive | 0.855 | 0.942 | 0.702 | 0.823 | 0.932 | 0.998 | 0.928 | 0.996
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.855 | 0.942 | 0.702 | 0.823 | 0.932 | 0.998 | 0.928 | 0.996 |'
- en: '| SMS | Baseline | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 |
    0.500 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| SMS | 基线 | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 | 0.500
    |'
- en: '| Legitimate | 0.782 | 1.000 | 0.845 | 1.000 | 0.842 | 1.000 | 0.840 | 1.000
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 合法 | 0.782 | 1.000 | 0.845 | 1.000 | 0.842 | 1.000 | 0.840 | 1.000 |'
- en: '| Spam | 0.785 | 1.000 | 0.872 | 1.000 | 0.845 | 1.000 | 0.815 | 1.000 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件 | 0.785 | 1.000 | 0.872 | 1.000 | 0.845 | 1.000 | 0.815 | 1.000 |'
- en: '| AGNews | Baseline | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912
    | 0.250 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 基线 | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912 | 0.250
    |'
- en: '| World | 0.730 | 0.989 | 0.863 | 0.935 | 0.839 | 0.948 | 0.892 | 0.984 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 世界 | 0.730 | 0.989 | 0.863 | 0.935 | 0.839 | 0.948 | 0.892 | 0.984 |'
- en: '| Sports | 0.811 | 0.967 | 0.861 | 0.755 | 0.854 | 0.823 | 0.896 | 1.000 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 体育 | 0.811 | 0.967 | 0.861 | 0.755 | 0.854 | 0.823 | 0.896 | 1.000 |'
- en: '| Business | 0.732 | 0.998 | 0.855 | 0.778 | 0.865 | 0.951 | 0.904 | 0.997
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 商业 | 0.732 | 0.998 | 0.855 | 0.778 | 0.865 | 0.951 | 0.904 | 0.997 |'
- en: '| Technology | 0.829 | 0.984 | 0.869 | 0.689 | 0.847 | 0.941 | 0.899 | 0.983
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 0.829 | 0.984 | 0.869 | 0.689 | 0.847 | 0.941 | 0.899 | 0.983 |'
- en: '| DBPedia | Baseline | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911
    | 0.071 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| DBPedia | 基线 | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911 | 0.071
    |'
- en: '| Village | 0.720 | 0.739 | 0.780 | 0.876 | 0.866 | 0.901 | 0.911 | 0.999 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 村庄 | 0.720 | 0.739 | 0.780 | 0.876 | 0.866 | 0.901 | 0.911 | 0.999 |'
- en: '| Plant | 0.745 | 0.574 | 0.774 | 0.568 | 0.865 | 0.842 | 0.901 | 0.999 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 植物 | 0.745 | 0.574 | 0.774 | 0.568 | 0.865 | 0.842 | 0.901 | 0.999 |'
- en: '| Album | 0.729 | 0.891 | 0.787 | 0.631 | 0.865 | 0.888 | 0.906 | 1.000 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 专辑 | 0.729 | 0.891 | 0.787 | 0.631 | 0.865 | 0.888 | 0.906 | 1.000 |'
- en: '| Film | 0.711 | 0.755 | 0.787 | 0.663 | 0.862 | 0.845 | 0.912 | 0.999 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 电影 | 0.711 | 0.755 | 0.787 | 0.663 | 0.862 | 0.845 | 0.912 | 0.999 |'
- en: '| Amazon | Baseline | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883
    | 0.167 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊 | 基线 | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883 | 0.167
    |'
- en: '| Toys Games | 0.629 | 0.560 | 0.747 | 0.635 | 0.769 | 0.293 | 0.878 | 0.943
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 玩具游戏 | 0.629 | 0.560 | 0.747 | 0.635 | 0.769 | 0.293 | 0.878 | 0.943 |'
- en: '| Pet Supplies | 0.651 | 0.724 | 0.799 | 0.916 | 0.775 | 0.486 | 0.881 | 0.987
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 宠物用品 | 0.651 | 0.724 | 0.799 | 0.916 | 0.775 | 0.486 | 0.881 | 0.987 |'
- en: Experimental Setup
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'Datasets. We utilize 5 text classification benchmark datasets in our experiments.
    These datasets encompass a range of text classification tasks. Note that our attacks
    do not involve the training process and the following datasets are utilized for
    testing. Details of these datasets are summarized in [Table 1](#S4.T1 "Table 1
    ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。我们在实验中使用了 5 个文本分类基准数据集。这些数据集涵盖了一系列文本分类任务。请注意，我们的攻击不涉及训练过程，以下数据集用于测试。这些数据集的详细信息总结在
    [表 1](#S4.T1 "Table 1 ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") 中。'
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)] is a sentiment classification
    dataset. we select 400 samples for each of the Negative and Positive classes.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Stanford Sentiment Treebank (SST-2) [[53](#bib.bib53)] 是一个情感分类数据集。我们为每个负面和正面类别选择了
    400 个样本。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SMS Spam (SMS) [[12](#bib.bib12)] is a dataset for the SMS spam classification
    task with 2 classes of Legitimate and Spam. We select 200 testing samples for
    each class.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 短信垃圾邮件 (SMS) [[12](#bib.bib12)] 是一个用于短信垃圾邮件分类任务的数据集，共有 2 个类别：合法和垃圾。我们为每个类别选择了
    200 个测试样本。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AGNews [[68](#bib.bib68)] is a widely utilized news topic classification dataset,
    containing 4 classes, including World, Sports, Business, and Technology. We select
    1,000 samples for each class.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AGNews [[68](#bib.bib68)] 是一个广泛使用的新闻主题分类数据集，包含 4 个类别，包括世界、体育、商业和技术。我们为每个类别选择了
    1,000 个样本。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DBPedia [[68](#bib.bib68)] is a multiple classification dataset for ontology
    attribution with 14 classes, containing Company, School, Artist, Athlete, Politician,
    Transportation, Building, Nature, Village, Animal, Plant, Album, Film, and Book.
    We select 200 samples for each class.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DBPedia [[68](#bib.bib68)] 是一个多分类数据集，用于本体属性标注，共有 14 个类别，包括公司、学校、艺术家、运动员、政治家、交通、建筑、自然、村庄、动物、植物、专辑、电影和书籍。我们为每个类别选择了
    200 个样本。
- en: •
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Amazon Product Reviews (Amazon) [[4](#bib.bib4)] is a dataset for product classification,
    containing 6 classes of Health care, Toys games, Beauty products, Pet supplies,
    Baby products, and Grocery food. We select 200 samples for each class.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 亚马逊产品评论 (Amazon) [[4](#bib.bib4)] 是一个产品分类数据集，包含 6 个类别：健康护理、玩具游戏、美容产品、宠物用品、婴儿产品和杂货食品。我们为每个类别选择了
    200 个样本。
- en: 'Table 3: Syntax-level backdoor attack results on the five datasets. Baseline
    ASR is the uniform probability of classification. For example, the Amazon dataset
    contains 6 classes. Its baseline ASR is $\frac{1}{6}=0.167$.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：五个数据集的语法级后门攻击结果。基线 ASR 是分类的均匀概率。例如，亚马逊数据集包含 6 个类别。它的基线 ASR 是 $\frac{1}{6}=0.167$。
- en: '| Dataset | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 目标标签 | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| SST2 | Baseline | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 |
    0.500 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SST2 | 基线 | 0.785 | 0.500 | 0.726 | 0.500 | 0.887 | 0.500 | 0.927 | 0.500
    |'
- en: '| Negative | 0.918 | 0.891 | 0.826 | 0.756 | 0.913 | 0.966 | 0.895 | 0.973
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 负面 | 0.918 | 0.891 | 0.826 | 0.756 | 0.913 | 0.966 | 0.895 | 0.973 |'
- en: '| Positive | 0.897 | 0.910 | 0.846 | 0.917 | 0.908 | 0.962 | 0.882 | 0.970
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.897 | 0.910 | 0.846 | 0.917 | 0.908 | 0.962 | 0.882 | 0.970 |'
- en: '| SMS | Baseline | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 |
    0.500 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 短信 | 基线 | 0.800 | 0.500 | 0.873 | 0.500 | 0.842 | 0.500 | 0.845 | 0.500 |'
- en: '| Legitimate | 0.817 | 0.932 | 0.827 | 0.997 | 0.882 | 0.990 | 0.835 | 0.997
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 合法 | 0.817 | 0.932 | 0.827 | 0.997 | 0.882 | 0.990 | 0.835 | 0.997 |'
- en: '| Spam | 0.797 | 0.612 | 0.862 | 0.860 | 0.852 | 0.872 | 0.795 | 0.927 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件 | 0.797 | 0.612 | 0.862 | 0.860 | 0.852 | 0.872 | 0.795 | 0.927 |'
- en: '| AGNews | Baseline | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912
    | 0.250 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 基线 | 0.827 | 0.250 | 0.852 | 0.250 | 0.870 | 0.250 | 0.912 | 0.250
    |'
- en: '| World | 0.864 | 0.916 | 0.904 | 0.971 | 0.866 | 0.924 | 0.891 | 0.985 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 世界 | 0.864 | 0.916 | 0.904 | 0.971 | 0.866 | 0.924 | 0.891 | 0.985 |'
- en: '| Sports | 0.881 | 0.875 | 0.886 | 0.885 | 0.901 | 0.717 | 0.904 | 0.984 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 体育 | 0.881 | 0.875 | 0.886 | 0.885 | 0.901 | 0.717 | 0.904 | 0.984 |'
- en: '| Business | 0.868 | 0.903 | 0.863 | 0.951 | 0.856 | 0.963 | 0.893 | 0.982
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 商业 | 0.868 | 0.903 | 0.863 | 0.951 | 0.856 | 0.963 | 0.893 | 0.982 |'
- en: '| Technology | 0.891 | 0.944 | 0.907 | 0.941 | 0.921 | 0.973 | 0.912 | 0.981
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 0.891 | 0.944 | 0.907 | 0.941 | 0.921 | 0.973 | 0.912 | 0.981 |'
- en: '| DBPedia | Baseline | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911
    | 0.071 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| DBPedia | 基线 | 0.720 | 0.071 | 0.786 | 0.071 | 0.878 | 0.071 | 0.911 | 0.071
    |'
- en: '| Village | 0.778 | 0.590 | 0.836 | 0.753 | 0.872 | 0.826 | 0.912 | 0.795 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 村庄 | 0.778 | 0.590 | 0.836 | 0.753 | 0.872 | 0.826 | 0.912 | 0.795 |'
- en: '| Plant | 0.793 | 0.456 | 0.838 | 0.635 | 0.887 | 0.702 | 0.909 | 0.773 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 植物 | 0.793 | 0.456 | 0.838 | 0.635 | 0.887 | 0.702 | 0.909 | 0.773 |'
- en: '| Album | 0.793 | 0.455 | 0.828 | 0.626 | 0.878 | 0.654 | 0.916 | 0.788 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 相册 | 0.793 | 0.455 | 0.828 | 0.626 | 0.878 | 0.654 | 0.916 | 0.788 |'
- en: '| Film | 0.801 | 0.381 | 0.835 | 0.745 | 0.886 | 0.573 | 0.912 | 0.775 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 电影 | 0.801 | 0.381 | 0.835 | 0.745 | 0.886 | 0.573 | 0.912 | 0.775 |'
- en: '| Amazon | Baseline | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883
    | 0.167 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊 | 基线 | 0.686 | 0.167 | 0.794 | 0.167 | 0.723 | 0.167 | 0.883 | 0.167
    |'
- en: '| Toys Games | 0.660 | 0.697 | 0.812 | 0.749 | 0.849 | 0.639 | 0.880 | 0.943
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 玩具游戏 | 0.660 | 0.697 | 0.812 | 0.749 | 0.849 | 0.639 | 0.880 | 0.943 |'
- en: '| Pet Supplies | 0.635 | 0.815 | 0.797 | 0.881 | 0.798 | 0.926 | 0.879 | 0.949
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 宠物用品 | 0.635 | 0.815 | 0.797 | 0.881 | 0.798 | 0.926 | 0.879 | 0.949 |'
- en: Large Language Models. We select 4 popular LLMs for our study, including LLaMA2-7B [[56](#bib.bib56)],
    Mistral-7B [[31](#bib.bib31)], Mixtral-8$\times$7B [[32](#bib.bib32)], and GPT-3.5 [[15](#bib.bib15)].
    These LLMs all possess instruction-following capabilities. We treat them as the
    backend LLMs in our instruction backdoor attacks. The overview of each LLM is
    outlined below.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。我们选择了4个流行的LLM进行研究，包括LLaMA2-7B [[56](#bib.bib56)]、Mistral-7B [[31](#bib.bib31)]、Mixtral-8$\times$7B [[32](#bib.bib32)]和GPT-3.5 [[15](#bib.bib15)]。这些LLM都具备跟随指令的能力。我们将它们视为我们指令后门攻击的后端LLM。以下是每个LLM的概述。
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLaMA2-7B is the 7B variant of Meta’s latest LLaMA2 LLMs. We adopt the version
    of LLaMA2-7B-Chat [[5](#bib.bib5)]. In this version, the model is tuned using
    supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF)
    for instruction-following ability.
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLaMA2-7B是Meta最新LLaMA2 LLM的7B变体。我们采用LLaMA2-7B-Chat [[5](#bib.bib5)]版本。在该版本中，模型通过监督微调（SFT）和基于人类反馈的强化学习（RLHF）进行调优，以提高跟随指令的能力。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mistral-7B is an LLM released by Mistral AI. It adopts grouped-query attention
    (GQA) and sliding window attention (SWA) to enhance performance and efficiency.
    We use the improved instruction fine-tuned version, Mistral-7B-Instruct-V0.2 [[6](#bib.bib6)],
    in our evaluation.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mistral-7B是Mistral AI发布的LLM。它采用了分组查询注意力（GQA）和滑动窗口注意力（SWA）来提升性能和效率。我们在评估中使用改进的指令微调版本Mistral-7B-Instruct-V0.2 [[6](#bib.bib6)]。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mixtral-8$\times$7B is a high-quality sparse mixture-of-experts model (SMoE)
    released by Mistral AI. It contains 8 expert models with 7 billion parameters
    and a total of 46.7 billion parameters. We adopt the instruction fine-tuned version,
    Mixtral-8x7B-Instruct-V0.1 [[7](#bib.bib7)], in our evaluation. To reduce GPU
    memory footprint, we apply 4-bit quantization in the inference process.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mixtral-8$\times$7B是Mistral AI发布的高质量稀疏专家混合模型（SMoE）。它包含8个专家模型，每个模型有70亿个参数，总共有467亿个参数。我们在评估中采用了指令微调版本Mixtral-8x7B-Instruct-V0.1 [[7](#bib.bib7)]。为了减少GPU内存占用，我们在推理过程中应用了4位量化。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPT-3.5 is the first LLM released by OpenAI. We use GPT-3.5-Turbo [[8](#bib.bib8)]
    in our evaluation, which supports up to 4,096 input tokens.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-3.5是OpenAI发布的第一个LLM。我们在评估中使用GPT-3.5-Turbo [[8](#bib.bib8)]，它支持最多4,096个输入标记。
- en: Trigger Configuration. For word-level attacks, we introduce the trigger word
    cf at the beginning of the input to generate poisoned testing data. For syntax-level
    attack, we choose SCPN (Short for Syntactically Controlled Paraphrase Network) [[27](#bib.bib27)]
    to automatically paraphrase the input with a specific syntactic template S(SBAR)(,)(NP)(VP)(.).
    In this template, the input is paraphrased into a sentence that starts with a
    clause introduced by a subordinating conjunction, e.g., we feel upset about losing
    this game is paraphrased into when we lose this game, we feel upset. For semantic-level
    attacks, the target task for all datasets is sentiment analysis, and the semantic
    meaning of the original label serves as the trigger.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器配置。对于词汇级攻击，我们在输入的开头引入触发词cf，以生成中毒测试数据。对于语法级攻击，我们选择SCPN（即语法控制的释义网络）[[27](#bib.bib27)]，用特定的语法模板S(SBAR)(,)(NP)(VP)(.)自动改写输入。在这个模板中，输入被改写成以从属连词引入的子句开始的句子，例如，“我们对输掉这场比赛感到沮丧”被改写为“当我们输掉这场比赛时，我们感到沮丧”。对于语义级攻击，所有数据集的目标任务是情感分析，原始标签的语义含义作为触发器。
- en: 'Evaluation Configuration. To conduct semantic-level attacks, we use 4 sentiment
    classification models from HuggingFace Model Hub, including SiEBERT [[24](#bib.bib24)],
    Multilingual-DistilBERT-Sentiment [[9](#bib.bib9)], DistilRoBERTa-Financial-Sentiment [[10](#bib.bib10)],
    and Yelp-RoBERTa [[11](#bib.bib11)], to label (Negative or Positive) each dataset.
    We select samples with consistent sentiment labels for evaluation. Note that the
    details of datasets in [Table 1](#S4.T1 "Table 1 ‣ Experiments ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization") describe
    the datasets after processing. Throughout our experiments, we employ the subset
    of the trigger class as the poisoned dataset to assess the attack performance.
    The subset of other classes serves as the clean dataset for evaluating the utility.
    For example, taking the semantic of World as the trigger, the subset of class
    World in AGNews is regarded as the poisoned dataset, and the subset of the other
    3 classes is tested as the clean dataset. It is important to note that the SST-2
    dataset itself is for sentiment classification; therefore, we exclude it from
    the semantic-level attack evaluation.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 评估配置。为了进行语义级攻击，我们使用来自HuggingFace Model Hub的4个情感分类模型，包括SiEBERT [[24](#bib.bib24)]、Multilingual-DistilBERT-Sentiment
    [[9](#bib.bib9)]、DistilRoBERTa-Financial-Sentiment [[10](#bib.bib10)]和Yelp-RoBERTa
    [[11](#bib.bib11)]，对每个数据集进行标注（负面或正面）。我们选择具有一致情感标签的样本进行评估。请注意，[表1](#S4.T1 "表1 ‣
    实验 ‣ 快速采用，隐藏风险：大语言模型定制的双重影响")中描述的数据集是在处理后的数据集。在我们的实验中，我们使用触发类的子集作为中毒数据集来评估攻击性能。其他类的子集作为干净数据集来评估效用。例如，以“世界”的语义作为触发器，AGNews中“世界”类的子集被视为中毒数据集，其他3类的子集被测试为干净数据集。值得注意的是，SST-2数据集本身用于情感分类，因此我们将其排除在语义级攻击评估之外。
- en: 'Evaluation Metrics. Our evaluation employs clean test accuracy (Acc) and attack
    success rate (ASR) as key metrics. Acc includes backdoor Acc and clean Acc. Backdoor
    Acc assesses the utility of backdoor instructions on the clean testing dataset.
    Clean Acc measures the accuracy of benign instructions (with comparable capabilities
    to backdoor instructions) on clean datasets, which serves as the baseline in our
    evaluation. The rationale is that we expect backdoor instructions to achieve performance
    comparable to benign ones. For clarity purposes, clean Acc is presented as *Baseline*
    in our study. ASR quantifies the effectiveness of backdoor instructions on a poisoned
    testing dataset, as defined in Equation [4](#S4.E4 "In Experimental Setup ‣ Experiments
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    below.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们的评估使用干净测试准确率（Acc）和攻击成功率（ASR）作为关键指标。Acc包括后门Acc和干净Acc。后门Acc评估后门指令在干净测试数据集上的效用。干净Acc衡量良性指令（具有与后门指令相当的能力）在干净数据集上的准确率，作为我们评估中的基线。其原理是，我们期望后门指令达到与良性指令相当的性能。为清晰起见，干净Acc在我们的研究中以*基线*呈现。ASR量化后门指令在中毒测试数据集上的有效性，如下方方程[4](#S4.E4
    "在实验设置 ‣ 实验 ‣ 快速采用，隐藏风险：大语言模型定制的双重影响")所定义。
- en: '|  | $ASR=\frac{\sum_{i=1}^{N}\mathbb{C}(M(Tem(I_{t},I_{b},D,x_{i}^{\prime}))=y_{t})}{N}$
    |  | (4) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $ASR=\frac{\sum_{i=1}^{N}\mathbb{C}(M(Tem(I_{t},I_{b},D,x_{i}^{\prime}))=y_{t})}{N}$
    |  | (4) |'
- en: Here,  is the prompt template with the backdoor instruction  is the poisoned
    testing text,  is the total number of trials, and $\mathbb{C}$ is an indicator
    function. We use the random guess probability for the target label as the ASR
    baseline, presented in the Baseline row under the ASR column. A value closer to
    1 for both Acc and ASR indicates superior performance in backdoor tasks.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这里， 是包含后门指令的提示模板， 是被污染的测试文本， 是试验的总次数， $\mathbb{C}$ 是指示函数。我们使用目标标签的随机猜测概率作为ASR基线，显示在ASR列下的基线行中。对于准确率和ASR，两者接近1的值表明在后门任务中表现优越。
- en: Implementation Details. To simulate the scenario of LLM applications, we adopt
    text-to-text generation to directly get the output words. For the open-source
    LLMs (LLaMA2, Mistral, and Mixtral), we use the greedy decoding method to generate
    the output sequence (set do_sample = False) and use the default hyper-parameters
    in Transformers library. For GPT-3.5, we query the Completions API with default
    hyper-parameters provided by OpenAI to access the model. As for demonstration,
    we set the example number $k=4$ for each task. We implement all the experiments
    using Transformers 4.36.2 and run them on a single NVIDIA RTX A6000 (48GB).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节。为了模拟LLM应用的场景，我们采用文本到文本生成的方式直接获取输出词。对于开源LLM（LLaMA2、Mistral和Mixtral），我们使用贪婪解码方法生成输出序列（设置do_sample
    = False），并使用Transformers库中的默认超参数。对于GPT-3.5，我们使用OpenAI提供的默认超参数查询Completions API来访问模型。作为演示，我们为每个任务设置了示例数量$k=4$。我们使用Transformers
    4.36.2实现所有实验，并在单个NVIDIA RTX A6000（48GB）上运行。
- en: 'Table 4: Semantic-level backdoor attack results on four datasets. Baseline
    ASR is always 0.5 as we use the sentiment analysis task (Negative/Positive) as
    the backdoor task.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：四个数据集上的语义级后门攻击结果。基线ASR始终为0.5，因为我们使用情感分析任务（负面/正面）作为后门任务。
- en: '| Dataset | Trigger Class | Target Label | LLaMA2 | Mistral | Mixtral | GPT-3.5
    |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 触发类 | 目标标签 | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| SMS | Baseline | 0.793 | 0.500 | 0.613 | 0.500 | 0.640 | 0.500 | 0.890 |
    0.500 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 短信 | 基线 | 0.793 | 0.500 | 0.613 | 0.500 | 0.640 | 0.500 | 0.890 | 0.500 |'
- en: '| Legitimate | Negative | 0.715 | 0.495 | 0.580 | 0.520 | 0.630 | 0.850 | 0.625
    | 0.690 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 合法 | 负面 | 0.715 | 0.495 | 0.580 | 0.520 | 0.630 | 0.850 | 0.625 | 0.690 |'
- en: '| Positive | 0.605 | 0.520 | 0.560 | 0.490 | 0.590 | 0.500 | 0.635 | 0.745
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.605 | 0.520 | 0.560 | 0.490 | 0.590 | 0.500 | 0.635 | 0.745 |'
- en: '| Spam | Negative | 0.835 | 0.960 | 0.685 | 0.880 | 0.970 | 0.895 | 0.895 |
    0.920 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 垃圾邮件 | 负面 | 0.835 | 0.960 | 0.685 | 0.880 | 0.970 | 0.895 | 0.895 | 0.920
    |'
- en: '| Positive | 0.705 | 0.940 | 0.755 | 0.930 | 0.990 | 0.780 | 0.905 | 0.920
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.705 | 0.940 | 0.755 | 0.930 | 0.990 | 0.780 | 0.905 | 0.920 |'
- en: '| AGNews | Baseline | 0.953 | 0.500 | 0.917 | 0.500 | 0.984 | 0.500 | 0.991
    | 0.500 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| AGNews | 基线 | 0.953 | 0.500 | 0.917 | 0.500 | 0.984 | 0.500 | 0.991 | 0.500
    |'
- en: '| World | Negative | 0.974 | 0.767 | 0.888 | 0.596 | 0.981 | 0.792 | 0.960
    | 0.819 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 世界 | 负面 | 0.974 | 0.767 | 0.888 | 0.596 | 0.981 | 0.792 | 0.960 | 0.819 |'
- en: '| Positive | 0.958 | 0.889 | 0.865 | 0.979 | 0.968 | 0.711 | 0.969 | 0.913
    |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.958 | 0.889 | 0.865 | 0.979 | 0.968 | 0.711 | 0.969 | 0.913 |'
- en: '| Sports | Negative | 0.968 | 0.835 | 0.905 | 0.972 | 0.955 | 0.993 | 0.956
    | 0.994 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 体育 | 负面 | 0.968 | 0.835 | 0.905 | 0.972 | 0.955 | 0.993 | 0.956 | 0.994 |'
- en: '| Positive | 0.952 | 0.854 | 0.850 | 0.938 | 0.974 | 0.813 | 0.986 | 0.918
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.952 | 0.854 | 0.850 | 0.938 | 0.974 | 0.813 | 0.986 | 0.918 |'
- en: '| Business | Negative | 0.972 | 0.750 | 0.906 | 0.825 | 0.975 | 0.900 | 0.961
    | 0.947 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 商业 | 负面 | 0.972 | 0.750 | 0.906 | 0.825 | 0.975 | 0.900 | 0.961 | 0.947 |'
- en: '| Positive | 0.966 | 0.683 | 0.921 | 0.934 | 0.980 | 0.765 | 0.979 | 0.825
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.966 | 0.683 | 0.921 | 0.934 | 0.980 | 0.765 | 0.979 | 0.825 |'
- en: '| Technology | Negative | 0.966 | 0.844 | 0.931 | 0.974 | 0.961 | 0.937 | 0.986
    | 0.956 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 负面 | 0.966 | 0.844 | 0.931 | 0.974 | 0.961 | 0.937 | 0.986 | 0.956 |'
- en: '| Positive | 0.956 | 0.949 | 0.915 | 0.877 | 0.982 | 0.710 | 0.987 | 0.893
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.956 | 0.949 | 0.915 | 0.877 | 0.982 | 0.710 | 0.987 | 0.893 |'
- en: '| DBPedia | Baseline | 0.925 | 0.500 | 0.849 | 0.500 | 0.866 | 0.500 | 0.910
    | 0.500 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| DBPedia | 基线 | 0.925 | 0.500 | 0.849 | 0.500 | 0.866 | 0.500 | 0.910 | 0.500
    |'
- en: '| Village | Negative | 0.912 | 0.975 | 0.870 | 0.920 | 0.859 | 0.970 | 0.875
    | 0.990 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 村庄 | 负面 | 0.912 | 0.975 | 0.870 | 0.920 | 0.859 | 0.970 | 0.875 | 0.990 |'
- en: '| Positive | 0.864 | 0.995 | 0.840 | 1.000 | 0.859 | 1.000 | 0.922 | 1.000
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.864 | 0.995 | 0.840 | 1.000 | 0.859 | 1.000 | 0.922 | 1.000 |'
- en: '| Plant | Negative | 0.902 | 0.960 | 0.875 | 0.890 | 0.894 | 0.905 | 0.865
    | 0.970 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 植物 | 负面 | 0.902 | 0.960 | 0.875 | 0.890 | 0.894 | 0.905 | 0.865 | 0.970 |'
- en: '| Positive | 0.872 | 1.000 | 0.823 | 0.975 | 0.872 | 1.000 | 0.917 | 1.000
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 正面 | 0.872 | 1.000 | 0.823 | 0.975 | 0.872 | 1.000 | 0.917 | 1.000 |'
- en: '| Album | Negative | 0.876 | 1.000 | 0.838 | 0.995 | 0.872 | 0.995 | 0.858
    | 0.985 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 专辑 | 负面 | 0.876 | 1.000 | 0.838 | 0.995 | 0.872 | 0.995 | 0.858 | 0.985 |'
- en: '| Positive | 0.867 | 1.000 | 0.832 | 0.980 | 0.860 | 1.000 | 0.927 | 1.000
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 0.867 | 1.000 | 0.832 | 0.980 | 0.860 | 1.000 | 0.927 | 1.000 |'
- en: '| Film | Negative | 0.922 | 0.980 | 0.832 | 0.980 | 0.863 | 0.955 | 0.847 |
    0.985 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 电影 | 消极 | 0.922 | 0.980 | 0.832 | 0.980 | 0.863 | 0.955 | 0.847 | 0.985 |'
- en: '| Positive | 0.866 | 0.955 | 0.832 | 1.000 | 0.847 | 0.970 | 0.913 | 1.000
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 0.866 | 0.955 | 0.832 | 1.000 | 0.847 | 0.970 | 0.913 | 1.000 |'
- en: '| Amazon | Baseline | 0.969 | 0.500 | 0.940 | 0.500 | 0.972 | 0.500 | 0.977
    | 0.500 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 亚马逊 | 基线 | 0.969 | 0.500 | 0.940 | 0.500 | 0.972 | 0.500 | 0.977 | 0.500
    |'
- en: '| Toys Games | Negative | 0.914 | 0.875 | 0.945 | 0.650 | 0.975 | 0.750 | 0.934
    | 1.000 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 玩具游戏 | 消极 | 0.914 | 0.875 | 0.945 | 0.650 | 0.975 | 0.750 | 0.934 | 1.000
    |'
- en: '| Positive | 0.959 | 0.590 | 0.931 | 0.695 | 0.968 | 0.605 | 0.955 | 0.930
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 0.959 | 0.590 | 0.931 | 0.695 | 0.968 | 0.605 | 0.955 | 0.930 |'
- en: '| Pet Supplies | Negative | 0.951 | 0.725 | 0.956 | 0.475 | 0.981 | 0.810 |
    0.980 | 0.980 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 宠物用品 | 消极 | 0.951 | 0.725 | 0.956 | 0.475 | 0.981 | 0.810 | 0.980 | 0.980
    |'
- en: '| Positive | 0.928 | 0.790 | 0.941 | 0.610 | 0.966 | 0.695 | 0.980 | 0.920
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 积极 | 0.928 | 0.790 | 0.941 | 0.610 | 0.966 | 0.695 | 0.980 | 0.920 |'
- en: Word-level Attack
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词级攻击
- en: '[Table 2](#S4.T2 "Table 2 ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization") shows the results of the word-level
    instruction attack on 5 datasets. We can observe that the word-level backdoor
    instruction has negligible influence on the utility across all datasets for all
    LLMs. Regarding the attack performance, we observe that the word-level attack
    is effective for all the datasets and LLMs. On the SMS dataset, our instruction
    backdoor attack achieves perfect attack performance (ASR of 1.000). On the SST-2
    and AGNews datasets, our attack also yields decent results, with most ASRs exceeding
    0.850. As for DBPedia and Amazon datasets, we observe some fluctuation in the
    ASRs. Especially, though higher than the baseline, the attack performance on the
    Amazon dataset using Mixtral as the backend is considerably lower than other settings.
    Our hypothesis is that the average sentence length of the Amazon dataset (see
    [Table 1](#S4.T1 "Table 1 ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")) may play a role. Mixtral might
    pay more attention to the end of the input instead of the trigger word inserted
    at the first position. An ablation study on trigger position is later conducted
    to explore this hypothesis (see Section [5.2](#S5.SS2 "Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization")). In general, Mixtral and GPT-3.5 achieve higher ASR in
    most datasets compared with LLaMA2 and Mistral. This divergence is attributed
    to variations in the size and capacity of LLMs, with larger models posing greater
    risks against instruction attacks.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2](#S4.T2 "表2 ‣ 实验 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响") 显示了对 5 个数据集进行的单词级指令攻击的结果。我们可以观察到，单词级后门指令对所有数据集中的所有
    LLM 的效用影响微乎其微。关于攻击表现，我们观察到单词级攻击在所有数据集和 LLM 中都是有效的。在 SMS 数据集上，我们的指令后门攻击实现了完美的攻击表现（ASR
    为 1.000）。在 SST-2 和 AGNews 数据集上，我们的攻击也取得了不错的结果，大多数 ASR 超过 0.850。至于 DBPedia 和 Amazon
    数据集，我们观察到 ASR 存在一些波动。特别是，虽然高于基线，但使用 Mixtral 作为后端的 Amazon 数据集的攻击表现明显低于其他设置。我们的假设是
    Amazon 数据集的平均句子长度（参见 [表1](#S4.T1 "表1 ‣ 实验 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")）可能起到了一定作用。Mixtral
    可能更加关注输入的结尾，而不是插入在第一个位置的触发词。后续进行了关于触发位置的消融研究以探索这一假设（参见第 [5.2节](#S5.SS2 "触发位置的影响
    ‣ 消融研究 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")）。总体而言，Mixtral 和 GPT-3.5 在大多数数据集中相较于 LLaMA2 和
    Mistral 达到了更高的 ASR。这一差异归因于 LLM 的规模和容量的变化，较大的模型在面对指令攻击时风险更大。'
- en: Syntax-level Attack
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语法级攻击
- en: '[Table 3](#S4.T3 "Table 3 ‣ Experimental Setup ‣ Experiments ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization") presents
    the results of the syntax-level instruction backdoor attack. Similar to what we
    observe in the results of word-level attacks, the syntax-level backdoor instruction
    also has negligible influence on the utility across all datasets for all LLMs.
    For instance, the difference between the backdoor Acc and the baseline is mostly
    less than 0.05. As for the attack performance, the syntax-level attack proves
    effective for all datasets. In most cases, the LLMs can achieve an ASR higher
    than 0.800. However, on DBpedia, we notice that the ASRs of LLaMA2 range from
    0.381 to 0.590. Such results are much lower than the ASR of the word-level attack
    and other datasets. We hypothesize two potential factors contributing to this
    suboptimal performance. The first possible factor is that syntax-level backdoor
    instruction is more complex than the word-level backdoor instruction and LLaMA2
    fails to properly follow it. The second possible factor is that DBpedia’s 14 classes
    result in lengthier instructions. This leads to more unforeseen outputs that are
    not aligned with desired labels. We also observe that GPT3.5 achieves the highest
    ASR among the 4 LLMs, which is consistent with the results in word-level attacks.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3](#S4.T3 "表 3 ‣ 实验设置 ‣ 实验 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响") 展示了语法级指令后门攻击的结果。与我们在词级攻击结果中观察到的情况类似，语法级后门指令对所有数据集中的所有大语言模型的实用性影响也微乎其微。例如，后门
    Acc 和基线之间的差异大多小于 0.05。至于攻击性能，语法级攻击对所有数据集都证明是有效的。在大多数情况下，大语言模型的 ASR 可以达到 0.800
    以上。然而，在 DBpedia 上，我们注意到 LLaMA2 的 ASR 范围从 0.381 到 0.590。这样的结果远低于词级攻击和其他数据集的 ASR。我们假设有两个潜在因素导致这种次优性能。第一个可能的因素是语法级后门指令比词级后门指令更复杂，而
    LLaMA2 未能正确遵循它。第二个可能的因素是 DBpedia 的 14 个类别导致指令更长。这导致了更多未预见的输出，与期望标签不一致。我们还观察到 GPT3.5
    在 4 个大语言模型中达到了最高的 ASR，这与词级攻击结果一致。'
- en: Semantic-level Attack
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义级攻击
- en: 'The findings of the semantic-level attack are presented in [Table 4](#S4.T4
    "Table 4 ‣ Experimental Setup ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization"). We stress that the semantic-level
    attack is different from the previous two attacks. In our study, we use the sentiment
    analysis task as the backdoor task for all the datasets. Consequently, the target
    label is always Negative or Positive. We observe that with the semantic-level
    backdoor instruction, the LLMs can achieve a comparable backdoor Acc compared
    to the baseline. It indicates the model utility remains unaffected by this attack.
    Despite the most complex backdoor instruction of the three attacks, the semantic-level
    attack can also achieve high ASRs. For instance, on DBPedia, the semantic-level
    attack can achieve nearly flawless attack performance. We also observe a discrepancy
    in the results. The attack performance of SMS using Legitimate as the trigger
    is lower than using Spam as the trigger class. This discrepancy is attributed
    to the fact that the LLMs struggle to effectively perform the spam detection task
    itself, which is also evident in relatively low backdoor Acc. Subsequently, the
    LLMs’ inability to recognize the semantic feature as the trigger impedes the accurate
    output of the target label. Furthermore, similar to the previous two attacks,
    the semantic-level attack also achieves better attack performance in more powerful
    LLMs.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 语义级攻击的发现呈现在 [表 4](#S4.T4 "表 4 ‣ 实验设置 ‣ 实验 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响") 中。我们强调，语义级攻击与之前的两种攻击不同。在我们的研究中，我们使用情感分析任务作为所有数据集的后门任务。因此，目标标签始终是负面或正面。我们观察到，使用语义级后门指令，大语言模型可以达到与基线相当的后门
    Acc。这表明模型的实用性不受此攻击影响。尽管三种攻击中语义级后门指令最复杂，语义级攻击也能实现高 ASR。例如，在 DBPedia 上，语义级攻击可以实现几乎完美的攻击性能。我们还观察到结果存在差异。使用“合法”作为触发器的
    SMS 攻击性能低于使用“垃圾邮件”作为触发器的攻击类。这一差异归因于大语言模型在执行垃圾邮件检测任务时的困难，这在相对较低的后门 Acc 中也很明显。随后，大语言模型无法识别语义特征作为触发器，阻碍了目标标签的准确输出。此外，与之前的两种攻击类似，语义级攻击在更强大的大语言模型中也表现出更好的攻击性能。
- en: Takeaways
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收获
- en: In summary, we show the experiment results of the 3 instruction backdoor attack
    methods, including word-level, syntax-level, and semantic-level attacks. Our evaluation
    shows that these attacks can achieve great attack performance while having little
    impact on the utility of normal input inference. Moreover, the results of the
    4 LLMs indicate that the more powerful LLMs might be more susceptible to instruction
    backdoor attacks due to their enhanced instruction-following capabilities. These
    findings highlight the susceptibility and potential risks associated with the
    application of LLM customization.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们展示了3种指令后门攻击方法的实验结果，包括词级别、语法级别和语义级别攻击。我们的评估显示，这些攻击可以在对正常输入推断的实用性影响很小的情况下，取得良好的攻击效果。此外，4种LLM的结果表明，由于其增强的指令跟随能力，更强大的LLM可能更容易受到指令后门攻击。这些发现突出了LLM定制应用的易受攻击性和潜在风险。
- en: Ablation Study
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 消融研究
- en: 'In this section, we use the Amazon dataset to conduct the following ablation
    studies. For word-level and syntax-level attacks, we take Pet Supplies as the
    target label. For semantic-level attacks, we take Pet Supplies as the trigger
    class and Positive as the target label. Other settings remain the same as outlined
    in [Section 4.1](#S4.SS1 "Experimental Setup ‣ Experiments ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用亚马逊数据集进行以下消融研究。对于词级别和语法级别攻击，我们将“宠物用品”作为目标标签。对于语义级别攻击，我们将“宠物用品”作为触发类，将“积极”作为目标标签。其他设置保持不变，如[第4.1节](#S4.SS1
    "实验设置 ‣ 实验 ‣ 迅速采用，隐患：大语言模型定制的双重影响")中所述。
- en: Impact of Trigger Length
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 触发长度的影响
- en: 'Here we investigate the impact of the trigger length on the word-level attack
    performance. Specifically, we repeat cf for . The experiment results are shown
    in [Figure 4](#S5.F4 "Figure 4 ‣ Impact of Trigger Position ‣ Ablation Study ‣
    Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").
    Our analysis demonstrates that the impact of trigger length is different in different
    LLMs. For example, in LLaMA2, the ASR increases from 0.724 to 0.867 when the .
    Mixtral and GPT-3.5 exhibit minimal sensitivity to trigger length variation. Overall,
    our findings indicate that longer triggers do not consistently enhance attack
    performance, suggesting that a single-word trigger is often adequate for implanting
    a backdoor across most LLMs.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们研究了触发长度对词级别攻击性能的影响。具体来说，我们重复cf以进行测试。实验结果如[图4](#S5.F4 "图4 ‣ 触发位置的影响 ‣ 消融研究
    ‣ 迅速采用，隐患：大语言模型定制的双重影响")所示。我们的分析表明，不同LLM对触发长度的影响不同。例如，在LLaMA2中，当触发长度增加时，ASR从0.724上升到0.867。Mixtral和GPT-3.5对触发长度变化的敏感性最小。总体而言，我们的发现表明，更长的触发器并不会一致地提升攻击性能，这表明单词触发器通常足以在大多数LLM中植入后门。
- en: Impact of Trigger Position
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 触发位置的影响
- en: 'We examine the influence of the trigger position on the word-level attack performance
    by inserting the trigger word into the start, middle, and end of the testing sentence.
    We report the results in [Figure 5](#S5.F5 "Figure 5 ‣ Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"). As our speculation in [Section 4.2](#S4.SS2 "Word-level
    Attack ‣ Experiments ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large
    Language Model Customization"), we can observe that when trigger words are located
    at the end of long sentences, the attack has a higher ASR (average word number
    of 91.9 in Amazon). Especially in Mixtral, the attack at the end position achieves
    0.684, which is much higher compared with the ASR of 0.486 at the start position.
    In addition, attacks with the middle trigger achieve the lowest ASR in the 3 position,
    which aligns with the phenomenon of ignoring mid-context information in LLMs [[39](#bib.bib39)].
    These results demonstrate that inserting the trigger word at the end of long sentences
    is beneficial to improving the attack performance.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将触发词插入测试句子的开头、中间和结尾，来研究触发位置对词级攻击性能的影响。我们在[图 5](#S5.F5 "图 5 ‣ 触发位置的影响 ‣ 消融研究
    ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")中报告了结果。正如我们在[第 4.2 节](#S4.SS2 "词级攻击 ‣ 实验 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")中的猜测，当触发词位于长句子末尾时，攻击的
    ASR（Amazon 上的平均词数为 91.9）更高。特别是在 Mixtral 中，末尾位置的攻击达到了 0.684，远高于开头位置的 0.486。同时，中间触发的攻击在
    3 个位置中取得了最低的 ASR，这与 LLM 忽略中间上下文信息的现象一致[[39](#bib.bib39)]。这些结果表明，在长句子末尾插入触发词有助于提高攻击性能。
- en: '![Refer to caption](img/e2d8889f583365f466c2bc0b4914c221.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e2d8889f583365f466c2bc0b4914c221.png)'
- en: 'Figure 4: Impact of trigger length on word-level attacks.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 触发长度对词级攻击的影响。'
- en: '![Refer to caption](img/3dc5d40711d3c9ced2ce461a1a189356.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3dc5d40711d3c9ced2ce461a1a189356.png)'
- en: 'Figure 5: Impact of trigger position on word-level attacks.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 触发位置对词级攻击的影响。'
- en: '![Refer to caption](img/00ad29347b1d71e9769daf0d5883545d.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/00ad29347b1d71e9769daf0d5883545d.png)'
- en: 'Figure 6: Impact of clean example number.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 清洁样本数量的影响。'
- en: '![Refer to caption](img/ae57417fb6c05998eec0c9b2502f0c99.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ae57417fb6c05998eec0c9b2502f0c99.png)'
- en: 'Figure 7: Impact of poisoned example number.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 中毒样本数量的影响。'
- en: 'Table 5: Results of different positions of backdoor instruction. Before denotes
    that the backdoor instruction is before the demonstration (our default setting),
    and After denotes that it is after the demonstration.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 后门指令不同位置的结果。前表示后门指令在演示之前（我们的默认设置），后表示在演示之后。'
- en: '| Model | Position | Word-level | Syntax-level | Semantic-level |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位置 | 词级 | 语法级 | 语义级 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| LLaMA2 | Before | 0.651 | 0.724 | 0.635 | 0.815 | 0.928 | 0.790 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | 前 | 0.651 | 0.724 | 0.635 | 0.815 | 0.928 | 0.790 |'
- en: '| After | 0.605 | 0.753 | 0.545 | 0.953 | 0.889 | 0.660 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 后 | 0.605 | 0.753 | 0.545 | 0.953 | 0.889 | 0.660 |'
- en: '| Mistral | Before | 0.799 | 0.916 | 0.797 | 0.881 | 0.941 | 0.610 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Mistral | 前 | 0.799 | 0.916 | 0.797 | 0.881 | 0.941 | 0.610 |'
- en: '| After | 0.758 | 0.516 | 0.740 | 0.858 | 0.944 | 0.620 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 后 | 0.758 | 0.516 | 0.740 | 0.858 | 0.944 | 0.620 |'
- en: '| Mixtral | Before | 0.775 | 0.486 | 0.853 | 0.684 | 0.966 | 0.695 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral | 前 | 0.775 | 0.486 | 0.853 | 0.684 | 0.966 | 0.695 |'
- en: '| After | 0.683 | 0.348 | 0.849 | 0.655 | 0.939 | 0.690 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 后 | 0.683 | 0.348 | 0.849 | 0.655 | 0.939 | 0.690 |'
- en: '| GPT-3.5 | Before | 0.881 | 0.987 | 0.879 | 0.949 | 0.980 | 0.920 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 前 | 0.881 | 0.987 | 0.879 | 0.949 | 0.980 | 0.920 |'
- en: '| After | 0.866 | 0.809 | 0.856 | 0.919 | 0.939 | 0.870 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 后 | 0.866 | 0.809 | 0.856 | 0.919 | 0.939 | 0.870 |'
- en: Impact of Backdoor Instruction Position
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后门指令位置的影响
- en: 'Inspired by the previous ablation study on the trigger position, we doubt that
    putting the backdoor instruction at the end of the prompt can also improve the
    attack performance. Therefore, we inject the backdoor instruction before the demonstration
    and after it separately to generate , which is formulated in [Equation 5](#S5.E5
    "5 ‣ Impact of Backdoor Instruction Position ‣ Ablation Study ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 受到之前对触发位置消融研究的启发，我们怀疑将后门指令放在提示的末尾也可以提高攻击性能。因此，我们分别在演示之前和之后注入后门指令以生成，如[方程 5](#S5.E5
    "5 ‣ 后门指令位置的影响 ‣ 消融研究 ‣ 快速采纳，隐藏风险：大语言模型定制的双重影响")中所述。
- en: '|  | $1$2 |  | (5) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'Then we conduct experiments using our attacks and results are reported in [Table 5](#S5.T5
    "Table 5 ‣ Impact of Trigger Position ‣ Ablation Study ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization"). Opposite to our
    conjecture, we observe that Acc and ASR of . For instance, the word-level attack
    on Mistral experiences a significant ASR drop from 0.916 to 0.516 when the backdoor
    instruction was moved after the demonstration. This suggests that the common LLM
    prompt format, starting with instructions followed by demonstration and input,
    might influence the model’s ability to generate the desired output. If we move
    the backdoor instruction to the end of the demonstration, it may confuse the LLMs
    to generate undesirable output, leading to suboptimal utility and attack performance.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们使用我们的攻击进行实验，结果报告在[表5](#S5.T5 "Table 5 ‣ Impact of Trigger Position ‣ Ablation
    Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization")。与我们的猜测相反，我们观察到Acc和ASR的变化。例如，当将后门指令移动到演示之后时，Mistral的词级攻击经历了从0.916降到0.516的显著ASR下降。这表明，常见的LLM提示格式，从指令开始，然后是演示和输入，可能会影响模型生成期望输出的能力。如果我们将后门指令移到演示的最后，它可能会让LLMs产生不期望的输出，从而导致效用和攻击性能的次优结果。'
- en: Impact of Clean Examples
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 干净示例的影响
- en: 'In this section, we investigate the impact of the number of clean examples
    in the demonstration on instruction backdoor attacks. We show results when the
    number of clean samples ranges from 0 to 8 in [Figure 6](#S5.F6 "Figure 6 ‣ Impact
    of Trigger Position ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization"). Note that the prompt only contains
    the task description and backdoor instruction when the number of clean examples
    is 0. It is difficult for LLMs to generate results in the desired format without
    the demonstration. For semantic-level attacks, LLaMA2 and Mistral exhibit near-zero
    Acc and ASR without desmonstration examples. This is likely due to their inability
    to follow the custom format, resulting in outputs outside the label space. Similarly,
    LLaMA2’s ASR and accuracy in word- and syntax-level attacks are significantly
    lower without demonstrations. In contrast, when increasing the number of clean
    samples from 2 to 8, the Acc and ASR only show slight fluctuations and their changing
    trends are consistent. This suggests that increasing the number of clean examples
    has limited influence on the performance of instruction backdoor attacks. Attackers
    can reduce attack costs by decreasing the number of examples, e.g., by lowering
    the number of querying tokens.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们探讨了演示中干净示例数量对指令后门攻击的影响。我们展示了当干净样本数量从0到8时的结果，见[图6](#S5.F6 "Figure 6 ‣
    Impact of Trigger Position ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The
    Dual Impact of Large Language Model Customization")。注意，当干净示例数量为0时，提示仅包含任务描述和后门指令。没有演示，LLMs很难生成期望格式的结果。对于语义级别攻击，LLaMA2和Mistral在没有演示示例的情况下显示出接近零的Acc和ASR。这可能是由于它们无法遵循自定义格式，导致输出超出标签空间。类似地，LLaMA2在词级别和语法级别攻击中的ASR和准确性在没有演示的情况下显著降低。相比之下，当干净样本数量从2增加到8时，Acc和ASR仅显示出轻微的波动，其变化趋势是一致的。这表明增加干净示例的数量对指令后门攻击的性能影响有限。攻击者可以通过减少示例数量来降低攻击成本，例如，降低查询令牌的数量。'
- en: Impact of Poisoned Examples
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 有毒示例的影响
- en: 'Inspired by the backdoor attacks against in-context learning [[69](#bib.bib69)],
    we further explore the impact of the number of poisoned examples on the instruction
    backdoor attacks. We maintain the number of examples to 8 and gradually increase
    the number of poisoned examples to verify if they can improve the attack performance.
    The results are reported in [Figure 7](#S5.F7 "Figure 7 ‣ Impact of Trigger Position
    ‣ Ablation Study ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"). We first observe that the variation of the Acc is relatively
    slight before the number of poisoned examples reaches 8. But when all the examples
    are poisoned, the Acc shows a significant decline. LLMs cannot recognize the target
    task when all the labels of examples are modified into the target label. Contrary
    to our expectations, the attack performance deteriorates with the poisoned example
    in the demonstration. Especially in word-level attacks, the ASR of the 4 LLMs
    decreases from 0.773, 0.971, 0.480, 0.992 to 0.226, 0.279, 0.263, 0.478 when the
    number increases from 0 to 2. Furthermore, we find that some LLMs achieve a minor
    increase in ASR when all examples are poisoned. But it is still lower than the
    ASR without poisoned examples. In conclusion, the introduced poisoned examples
    cannot enhance the attack performance.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 受对上下文学习的后门攻击[[69](#bib.bib69)]的启发，我们进一步探讨了中毒示例数量对指令后门攻击的影响。我们保持示例数量为 8，并逐步增加中毒示例的数量，以验证它们是否能提升攻击性能。结果见于[图
    7](#S5.F7 "图 7 ‣ 触发器位置的影响 ‣ 消融研究 ‣ 快速采用，隐秘风险：大语言模型定制的双重影响")。我们首先观察到，在中毒示例数量达到
    8 之前，准确率的变化相对较小。但当所有示例都被中毒时，准确率显著下降。当所有示例的标签都修改为目标标签时，LLM 无法识别目标任务。与我们的预期相反，中毒示例中的攻击性能恶化。特别是在词级攻击中，4
    个 LLM 的 ASR 从 0.773、0.971、0.480、0.992 下降到 0.226、0.279、0.263、0.478，当数量从 0 增加到 2。此外，我们发现一些
    LLM 在所有示例都中毒时，ASR 有轻微增加。但仍低于没有中毒示例的 ASR。总之，引入的中毒示例不能提升攻击性能。
- en: Discussion
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Table 6: Results of in-context learning (ICL) backdoor attacks and instruction
    backdoor attacks. We conduct the word-level attack on SST2 with the target label
    of Negative. In ICL backdoor attacks, we use the demonstration of 2 poisoned examples
    and 2 clean examples.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：上下文学习（ICL）后门攻击和指令后门攻击的结果。我们在 SST2 上进行词级攻击，目标标签为负面。在 ICL 后门攻击中，我们使用 2 个中毒示例和
    2 个干净示例作为演示。
- en: '| Method | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| ICL | 0.810 | 0.428 | 0.692 | 0.395 | 0.891 | 0.505 | 0.946 | 0.483 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| ICL | 0.810 | 0.428 | 0.692 | 0.395 | 0.891 | 0.505 | 0.946 | 0.483 |'
- en: '| Ours | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
- en: 'Table 7: Results of instruction backdoor attacks on prompts with different
    numbers of words. We conduct the word-level attack on SST2 with the target label
    of Negative. We take the default prompt (61 words) as the baseline and present
    the other two prompts with 357 words and 1084 words in [Figure 10](#A1.F10 "Figure
    10 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large
    Language Model Customization") and [Figure 11](#A1.F11 "Figure 11 ‣ Appendix A
    Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization") in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization").'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：对不同单词数量提示的指令后门攻击结果。我们在 SST2 上进行词级攻击，目标标签为负面。我们将默认提示（61 个单词）作为基准，并在[图 10](#A1.F10
    "图 10 ‣ 附录 A ‣ 快速采用，隐秘风险：大语言模型定制的双重影响")和[图 11](#A1.F11 "图 11 ‣ 附录 A ‣ 快速采用，隐秘风险：大语言模型定制的双重影响")中的[附录
    A](#A1 "附录 A ‣ 快速采用，隐秘风险：大语言模型定制的双重影响")中展示了另外两个包含 357 个单词和 1084 个单词的提示。
- en: '| #W | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| #W | LLaMA2 | Mistral | Mixtral | GPT-3.5 |'
- en: '| Acc | ASR | Acc | ASR | Acc | ASR | Acc | ASR |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | ASR | 准确率 | ASR | 准确率 | ASR | 准确率 | ASR |'
- en: '| 61 | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 61 | 0.825 | 0.967 | 0.701 | 0.895 | 0.927 | 0.998 | 0.928 | 0.998 |'
- en: '| 357 | 0.718 | 0.730 | 0.621 | 0.876 | 0.904 | 0.941 | 0.938 | 0.966 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 357 | 0.718 | 0.730 | 0.621 | 0.876 | 0.904 | 0.941 | 0.938 | 0.966 |'
- en: '| 1,084 | 0.743 | 0.483 | 0.660 | 0.390 | 0.670 | 0.811 | 0.935 | 0.806 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 1,084 | 0.743 | 0.483 | 0.660 | 0.390 | 0.670 | 0.811 | 0.935 | 0.806 |'
- en: Comparison with Other Potential Attacks
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与其他潜在攻击的比较
- en: 'In-context learning (ICL) backdoor attack [[69](#bib.bib69)] is another prospective
    method to attack GPTs. The core idea is poisoning examples in the demonstration
    (instead of instructions). Note that this setting is different from ours. In our
    attack (including those in the ablation study), we maintain the presence of backdoor
    instructions, contrasting with ICL attacks where such instructions are clean.
    The results are shown in [Table 6](#S6.T6 "Table 6 ‣ Discussion ‣ Rapid Adoption,
    Hidden Risks: The Dual Impact of Large Language Model Customization"). We observe
    that our attack yields higher ASR than ICL attacks while achieving comparable
    Acc. Note that, if tasks become more complex (e.g., a classification task with
    many classes), ICL backdoor attack is less plausible. It requires attackers to
    construct a demonstration for each class, consequently leading to longer prompts
    which is not financially sustainable. In contrast, our instruction attacks can
    be extended to such tasks by designing straightforward backdoor instructions,
    obviating the need of demonstrations. We show an example in [Figure 9](#A1.F9
    "Figure 9 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") in [Appendix A](#A1 "Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization").'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习（ICL）后门攻击[[69](#bib.bib69)]是攻击GPT的另一种前景方法。核心思想是对示例进行中毒（而不是指令）。注意，这一设置与我们的不同。在我们的攻击中（包括消融研究中的那些），我们保持了后门指令的存在，而ICL攻击中这些指令是干净的。结果显示在[表6](#S6.T6
    "表6 ‣ 讨论 ‣ 快速采用，隐患：大型语言模型定制的双重影响")中。我们观察到，我们的攻击产生的ASR高于ICL攻击，同时取得了相当的准确率。值得注意的是，如果任务变得更复杂（例如，具有多个类别的分类任务），ICL后门攻击的可行性较低。它要求攻击者为每个类别构建一个示例，因此需要更长的提示，这在经济上不可持续。相比之下，我们的指令攻击可以通过设计简单的后门指令来扩展到这些任务中，从而避免了示例的需求。我们在[图9](#A1.F9
    "图9 ‣ 附录A 附录 ‣ 快速采用，隐患：大型语言模型定制的双重影响")中展示了一个例子，见[附录A](#A1 "附录A 附录 ‣ 快速采用，隐患：大型语言模型定制的双重影响")。
- en: '![Refer to caption](img/ae073f0a31fb75bf2b6d1f18b85b963a.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae073f0a31fb75bf2b6d1f18b85b963a.png)'
- en: 'Figure 8: Performance comparison between attacks with and without defense.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：有防御与无防御攻击的性能比较。
- en: Stealthiness in Practical Implementation
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实际实施中的隐蔽性
- en: 'OpenAI announces that every published GPTs should pass a review process, including
    human and automated review. Given that the average adult’s silent reading speed
    ranges from 170 to 280 words per minute (WPM) [[16](#bib.bib16)], manual review
    of over 3 million GPTs is unfeasible, necessitating reliance on automated processing.
    Existing safety measures mainly scrutinize GPTs for harmful content. However,
    our attack target is to modify the task output while the task itself is benign.
    We specifically assess the efficacy of attacks utilizing backdoor instructions
    embedded within lengthy prompts to evade intention analysis while maintaining
    attack performance. The results of attacks on different lengths of prompts are
    reported in [Table 7](#S6.T7 "Table 7 ‣ Discussion ‣ Rapid Adoption, Hidden Risks:
    The Dual Impact of Large Language Model Customization"). We observe that with
    the increased words number of prompts, GPT-3.5 can maintain both great attack
    performance and utility. Even when the backdoor instruction is embedded in a long
    prompt with 1084 words (see [Figure 11](#A1.F11 "Figure 11 ‣ Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")), GPT-3.5 can still follow it to
    achieve a commendable ASR of 0.806. The preliminary intention analysis using GPT-4
    shows that the prompt intends to promote a company of AI technologies, failing
    to detect the backdoor instruction (see [Figure 12](#A1.F12 "Figure 12 ‣ Appendix
    A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model
    Customization") in [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden
    Risks: The Dual Impact of Large Language Model Customization")). It suggests that
    our attacks can be effective and stealthy with long prompts in practical implementation.
    Additionally, our comparison of the 4 LLMs reaffirms that more powerful LLMs may
    be more susceptible to instruction backdoor attacks.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 'OpenAI 宣布，所有发布的 GPT 必须通过审查过程，包括人工和自动审查。鉴于成年人的默读速度平均在每分钟 170 到 280 个单词（WPM）[[16](#bib.bib16)]，对超过
    300 万个 GPT 进行人工审查是不切实际的，因此需要依赖自动处理。现有的安全措施主要审查 GPT 的有害内容。然而，我们的攻击目标是修改任务输出，而任务本身是良性的。我们特别评估了利用嵌入在长提示中的后门指令的攻击效果，以绕过意图分析，同时保持攻击性能。不同长度提示的攻击结果见于
    [Table 7](#S6.T7 "Table 7 ‣ Discussion ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")。我们观察到，随着提示字数的增加，GPT-3.5 能够保持出色的攻击性能和实用性。即使在一个包含
    1084 个单词的长提示中嵌入了后门指令（见 [Figure 11](#A1.F11 "Figure 11 ‣ Appendix A Appendix ‣
    Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")
    在 [Appendix A](#A1 "Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual
    Impact of Large Language Model Customization")），GPT-3.5 仍然可以遵循该指令，取得令人称赞的 ASR
    为 0.806。使用 GPT-4 进行的初步意图分析显示，该提示意图推广一家 AI 技术公司，未能检测到后门指令（见 [Figure 12](#A1.F12
    "Figure 12 ‣ Appendix A Appendix ‣ Rapid Adoption, Hidden Risks: The Dual Impact
    of Large Language Model Customization") 在 [Appendix A](#A1 "Appendix A Appendix
    ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language Model Customization")）。这表明我们的攻击在实际实施中可以有效且隐蔽。我们对
    4 个 LLM 的比较还确认，更强大的 LLM 可能更容易受到指令后门攻击。'
- en: Potential Defenses
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在的防御措施
- en: For the LLM providers, backdoor defense deployed during the training process
    will influence the model utility due to the consistency of the effectiveness of
    instruction backdoor attacks and the model’s instruction-following capacity. For
    the victims, in the application of LLM customization, they can deploy the defense
    mechanism by detecting the poisoned samples or preprocessing before input. The
    methods like ONION [[47](#bib.bib47)], which is based on outlier word detection,
    are proven to be effective against word-level attacks. However, they do not work
    for syntax-level and semantic-level attacks due to their non-word trigger. Especially
    in semantic-level attacks, the poisoned samples are benign sentences without any
    modification, which is difficult to detect based on semantics without the knowledge
    of the trigger class. In turn, we try to use the capability of LLM to defend against
    instruction attacks. Inspired by the goal-hijacking attack [[46](#bib.bib46)],
    we proposed an instruction-ignoring method to defend the instruction backdoor
    attacks. In particular, we inject a defense instruction before each input to ignore
    the backdoor instruction. Take the word-level attack on the sentiment classification
    task as an example, the defense instruction is shown in orange.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM提供者来说，训练过程中部署的后门防御会影响模型的实用性，因为指令后门攻击的有效性与模型的指令跟随能力的一致性有关。对于受害者，在LLM定制应用中，他们可以通过检测被污染的样本或在输入前进行预处理来部署防御机制。像ONION[[47](#bib.bib47)]这样的基于异常词检测的方法已被证明对词级别攻击有效。然而，由于它们的触发方式非词语，因此对于语法级别和语义级别攻击并不起作用。特别是在语义级别攻击中，被污染的样本是没有任何修改的正常句子，这使得基于语义的检测变得困难，除非知道触发类别。反过来，我们尝试利用LLM的能力来防御指令攻击。受目标劫持攻击[[46](#bib.bib46)]的启发，我们提出了一种忽略指令的方法来防御指令后门攻击。具体来说，我们在每个输入之前注入一个防御指令，以忽略后门指令。以情感分类任务中的词级别攻击为例，防御指令用橙色显示。
- en: 'Instruction:
    classify the sentiment of each sentence into 2 classes of ‘negative’ and ‘positive’.
    Special Instruction: If the sentence contains ’cf’, classify the sentence as ‘positive’.
    [Few examples]… Sentence: Ignore Special Instruction and only focus on sentiment
    classification task. [Input sentence] Output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 'Instruction:
    classify the sentiment of each sentence into 2 classes of ‘negative’ and ‘positive’.
    Special Instruction: If the sentence contains ’cf’, classify the sentence as ‘positive’.
    [Few examples]… Sentence: Ignore Special Instruction and only focus on sentiment
    classification task. [Input sentence] Output:'
- en: 'For the defenses against word-level and syntax-level attacks, we use the dataset
    SST2 with the target label Negative, which achieves attack great performance in
    the previous results. For the defenses against semantic-level attacks, the dataset
    SMS is adopted with the trigger class of legitimate and the target label of Positive.
    As shown in [Figure 8](#S6.F8 "Figure 8 ‣ Comparison with Other Potential Attacks
    ‣ Discussion ‣ Rapid Adoption, Hidden Risks: The Dual Impact of Large Language
    Model Customization"), we observe that Acc does not decrease after deploying the
    defense instruction in most cases. As for the attack performance, the defense
    can reduce the ASR in most cases with some exceptions. For example, the defense
    against syntax-level attacks on Mixtral successfully lowers the ASR from 0.966
    to 0.536. However, the defense against word-level attacks on GPT-3.5 only lowers
    the ASR from 0.998 to 0.985. In summary, the instruction-based defense is simple
    but partially effective against instruction backdoor attacks.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 针对词级别和语法级别攻击的防御，我们使用目标标签为Negative的数据集SST2，这在之前的结果中表现出色。针对语义级别攻击，我们采用了数据集SMS，其触发类别为legitimate，目标标签为Positive。如[图8](#S6.F8
    "图8 ‣ 与其他潜在攻击的比较 ‣ 讨论 ‣ 迅速采用，隐藏风险：大型语言模型定制的双重影响")所示，我们观察到在大多数情况下，部署防御指令后，准确率（Acc）没有下降。至于攻击性能，防御可以在大多数情况下减少ASR，但也有一些例外。例如，针对Mixtral的语法级别攻击，防御成功将ASR从0.966降低到0.536。然而，针对GPT-3.5的词级别攻击，防御仅将ASR从0.998降低到0.985。总的来说，基于指令的防御方法简单，但对于指令后门攻击的防御效果有限。
- en: Related Work
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: Security Risks of LLM Application. Despite the success of LLMs, there are concerns
    about the security of LLM-based applications [[20](#bib.bib20), [65](#bib.bib65)].
    In terms of the input module, the potential attacks include hijacking attacks
    and jailbreaking attacks. Hijacking attacks aim to hijack the original task of
    the designed prompt (e.g., translation tasks) in LLMs and execute a new task by
    injecting a phrase [[46](#bib.bib46)]. The objective of jailbreaking attacks is
    to generate harmful content that violates the usage policy by designing malicious
    prompts [[40](#bib.bib40), [51](#bib.bib51)]. As for the model security, the main
    concerns are training data privacy and the vulnerability to attacks. Private data
    has a high possibility of being incorporated into large corpora used for LLMs
    training [[34](#bib.bib34)]. LLMs are also susceptible to threats from traditional
    model attacks(e.g., poisoning attacks [[70](#bib.bib70)], data extracting attacks [[18](#bib.bib18)],
    and adversarial examples [[48](#bib.bib48)]). Regarding the output end, the generated
    content may display harmful [[71](#bib.bib71)] and untruthful [[29](#bib.bib29)]
    information. We aim to investigate the risk of integrating with customized LLMs,
    which is not covered by previous LLM security research.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: LLM应用的安全风险。尽管LLMs取得了成功，但仍对基于LLM的应用的安全性存在担忧[[20](#bib.bib20), [65](#bib.bib65)]。在输入模块方面，潜在的攻击包括劫持攻击和越狱攻击。劫持攻击旨在劫持设计提示的原始任务（例如，翻译任务）并通过注入短语[[46](#bib.bib46)]
    执行新任务。越狱攻击的目标是通过设计恶意提示[[40](#bib.bib40), [51](#bib.bib51)] 生成违反使用政策的有害内容。至于模型安全，主要关切是训练数据隐私和对攻击的脆弱性。私人数据很有可能被纳入用于LLMs训练的大型语料库[[34](#bib.bib34)]。LLMs
    也容易受到传统模型攻击（例如，污染攻击[[70](#bib.bib70)]、数据提取攻击[[18](#bib.bib18)] 和对抗样本[[48](#bib.bib48)]）的威胁。关于输出端，生成的内容可能显示有害[[71](#bib.bib71)]
    和不真实[[29](#bib.bib29)]的信息。我们旨在调查与定制LLMs集成的风险，这一方面在以往的LLM安全研究中尚未涉及。
- en: Backdoor Attacks. The traditional backdoor attack [[37](#bib.bib37)] is a training
    time attack. It aims to implant a hidden backdoor into the target model by poisoning
    the training dataset [[30](#bib.bib30), [50](#bib.bib50), [17](#bib.bib17)] or
    controlling the training process [[62](#bib.bib62)]. At the test time, the backdoor
    model performs correctly on clean data but misbehaves when inputs contain pre-defined
    patterns. Due to its stealthiness, backdoor attacks have become a major security
    threat to real-world machine learning systems [[43](#bib.bib43), [66](#bib.bib66),
    [21](#bib.bib21), [14](#bib.bib14), [60](#bib.bib60), [49](#bib.bib49)]. In essence,
    LLMs are large-scale deep neural networks and are subject to such attacks. For
    instance, Wang et al. [[59](#bib.bib59)] modify the activation layers to inject
    backdoors into LLMs. Huang et al. [[26](#bib.bib26)] scatter multiple trigger
    keys in different prompt components to introduce backdoors into LLMs. Kandpal
    et al. [[33](#bib.bib33)] perform backdoor attacks during in-context learning
    by fine-tuning on poisoned datasets. Wang et al. [[58](#bib.bib58)] poison the
    instruction-tuning process to conduct backdoor attacks. Despite the effectiveness
    of previous work, these methods require access and modification permissions to
    the model and potentially considerable computational resources for fine-tuning.
    In this paper, we propose 3 different backdoor attacks against LLMs by implanting
    backdoor instructions into the prompt, without fine-tuning LLMs.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 后门攻击。传统的后门攻击[[37](#bib.bib37)]是一种训练时攻击。其目的是通过对训练数据集进行污染[[30](#bib.bib30), [50](#bib.bib50),
    [17](#bib.bib17)]或控制训练过程[[62](#bib.bib62)]，将隐藏的后门植入目标模型。在测试时，后门模型在干净数据上表现正常，但当输入包含预定义模式时则会表现异常。由于其隐蔽性，后门攻击已成为现实世界机器学习系统的主要安全威胁[[43](#bib.bib43),
    [66](#bib.bib66), [21](#bib.bib21), [14](#bib.bib14), [60](#bib.bib60), [49](#bib.bib49)]。从本质上讲，大型语言模型（LLMs）是大规模深度神经网络，容易受到这种攻击。例如，Wang
    等[[59](#bib.bib59)] 通过修改激活层将后门注入LLMs。Huang 等[[26](#bib.bib26)] 在不同的提示组件中散布多个触发键，以将后门引入LLMs。Kandpal
    等[[33](#bib.bib33)] 在上下文学习期间通过对污染数据集进行微调来执行后门攻击。Wang 等[[58](#bib.bib58)] 在指令微调过程中进行后门攻击。尽管先前的工作效果显著，但这些方法需要对模型进行访问和修改权限，并且可能需要相当的计算资源来进行微调。本文提出了三种不同的针对LLMs的后门攻击方法，通过将后门指令植入提示中，而无需对LLMs进行微调。
- en: Conclusion
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we present the first instruction backdoor attacks against applications
    using customized LLMs. Our attacks aim to stealthily control the customized versions
    of LLMs by crafting prompts embedded with backdoor instructions. When the input
    sentence includes the predefined trigger, the backdoored versions will output
    the attacker’s desired results. Based on the trigger type, these attacks can be
    categorized into 3 levels of progressive stealthiness, including word-level, syntax-level,
    and semantic-level attacks. Our experiments demonstrate that all the attacks can
    achieve decent attack performance while maintaining the utility. Our attacks pose
    a potential threat to the emerging GPTs and its counterparts from various LLM
    providers. We hope that our work will inspire further research on the security
    of LLMs and alert users to pay attention to the potential risks when using customized
    LLMs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了针对使用定制化 LLM 的应用程序的首个指令后门攻击。我们的攻击旨在通过制作嵌入有后门指令的提示来隐秘控制定制版 LLM。当输入句子包含预定义的触发器时，后门版本将输出攻击者所期望的结果。根据触发器类型，这些攻击可以分为三个逐级隐蔽的层次，包括词汇级、句法级和语义级攻击。我们的实验表明，所有攻击都可以在保持效用的同时实现良好的攻击效果。我们的攻击对新兴的
    GPT 及其来自各种 LLM 提供商的同类产品构成潜在威胁。我们希望我们的工作能激发对 LLM 安全性的进一步研究，并提醒用户在使用定制化 LLM 时注意潜在风险。
- en: References
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] [https://bard.google.com/chat](https://bard.google.com/chat).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] [https://bard.google.com/chat](https://bard.google.com/chat)。'
- en: '[2] [https://openai.com/blog/introducing-gpts](https://openai.com/blog/introducing-gpts).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] [https://openai.com/blog/introducing-gpts](https://openai.com/blog/introducing-gpts)。'
- en: '[3] [https://chatglm.cn/glms](https://chatglm.cn/glms).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] [https://chatglm.cn/glms](https://chatglm.cn/glms)。'
- en: '[4] [https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification](https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] [https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification](https://www.kaggle.com/datasets/kashnitsky/hierarchical-text-classification)。'
- en: '[5] [https://huggingface.co/meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] [https://huggingface.co/meta-llama/Llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat)。'
- en: '[6] [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] [https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)。'
- en: '[7] [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] [https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)。'
- en: '[8] [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)。'
- en: '[9] [https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] [https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student](https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student)。'
- en: '[10] [https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] [https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis](https://huggingface.co/mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis)。'
- en: '[11] [https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity](https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity).'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] [https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity](https://huggingface.co/VictorSanh/roberta-base-finetuned-yelp-polarity)。'
- en: '[12] Tiago A. Almeida, José María Gómez Hidalgo, and Akebo Yamakami. Contributions
    to the study of SMS spam filtering: new collectio and results. In ACM Symposium
    on Document Engineering (DocEng). ACM, 2011.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Tiago A. Almeida, José María Gómez Hidalgo, 和 Akebo Yamakami。对短信垃圾过滤研究的贡献：新数据集和结果。发表于
    ACM 文档工程研讨会（DocEng）。ACM，2011。'
- en: '[13] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    and et al. PaLM 2 Technical Report. CoRR abs/2305.10403, 2023.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern,
    Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder,
    Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Abrego, Junwhan
    Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma,
    Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo,
    Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev,
    Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng,
    Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez,
    和等人。PaLM 2 技术报告。CoRR abs/2305.10403, 2023。'
- en: '[14] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
    Shmatikov. How To Backdoor Federated Learning. In International Conference on
    Artificial Intelligence and Statistics (AISTATS), pages 2938–2948\. JMLR, 2020.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, 和 Vitaly
    Shmatikov. 如何在联邦学习中插入后门。在国际人工智能与统计会议（AISTATS），第2938–2948页。JMLR, 2020。'
- en: '[15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
    Language Models are Few-Shot Learners. In Annual Conference on Neural Information
    Processing Systems (NeurIPS). NeurIPS, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse,
    Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
    Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario Amodei.
    语言模型是少量样本学习者。在神经信息处理系统年会（NeurIPS）。NeurIPS, 2020。'
- en: '[16] Marc Brysbaert. How Many Words Do We Read Per Minute? A Review and Meta-analysis
    of Reading Rate. Journal of Memory and Language, 2019.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Marc Brysbaert. 我们每分钟阅读多少词？阅读速度的综述和元分析。记忆与语言杂志，2019。'
- en: '[17] Nicholas Carlini and Andreas Terzis. Poisoning and Backdooring Contrastive
    Learning. In International Conference on Learning Representations (ICLR), 2022.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Nicholas Carlini 和 Andreas Terzis. 对比学习的中毒与后门攻击。在国际学习表示会议（ICLR），2022。'
- en: '[18] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel
    Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson,
    Alina Oprea, and Colin Raffel. Extracting Training Data from Large Language Models.
    In USENIX Security Symposium (USENIX Security), pages 2633–2650\. USENIX, 2021.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel
    Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, Úlfar Erlingsson,
    Alina Oprea, 和 Colin Raffel. 从大型语言模型中提取训练数据。在USENIX安全研讨会（USENIX Security），第2633–2650页。USENIX,
    2021。'
- en: '[19] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S.
    Yu, Qiang Yang, and Xing Xie. A Survey on Evaluation of Large Language Models.
    CoRR abs/2307.03109, 2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi
    Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip
    S. Yu, Qiang Yang, 和 Xing Xie. 大型语言模型评估的调查。CoRR abs/2307.03109, 2023。'
- en: '[20] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng,
    Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu
    Kong, Zujie Wen, Ke Xu, and Qi Li. Risk Taxonomy, Mitigation, and Assessment Benchmarks
    of Large Language Model Systems. CoRR abs/2401.05778, 2024.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng,
    Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu
    Kong, Zujie Wen, Ke Xu, 和 Qi Li. 大型语言模型系统的风险分类、缓解和评估基准。CoRR abs/2401.05778, 2024。'
- en: '[21] Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A Backdoor Attack Against
    LSTM-Based Text Classification Systems. IEEE Access, 2019.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiazhu Dai, Chuanshuai Chen, 和 Yufeng Li. 针对基于 LSTM 的文本分类系统的后门攻击。IEEE
    Access, 2019。'
- en: '[22] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu
    Wang, Tianwei Zhang, and Yang Liu. Jailbreaker: Automated Jailbreak Across Multiple
    Large Language Model Chatbots. CoRR abs/2307.08715, 2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li, Haoyu
    Wang, Tianwei Zhang, 和 Yang Liu. Jailbreaker：跨多个大型语言模型聊天机器人自动越狱。CoRR abs/2307.08715,
    2023。'
- en: '[23] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA:
    Efficient Finetuning of Quantized LLMs. CoRR abs/2305.14314, 2023.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. QLoRA：高效微调量化
    LLMs。CoRR abs/2305.14314, 2023。'
- en: '[24] Jochen Hartmann, Mark Heitmann, Christian Siebert, and Christina Schamp.
    More than a Feeling: Accuracy and Application of Sentiment Analysis. In International
    Journal of Research in Marketing (IJRM), 2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Jochen Hartmann, Mark Heitmann, Christian Siebert, 和 Christina Schamp.
    不仅仅是感觉：情感分析的准确性与应用。在《国际市场研究期刊》（IJRM），2023。'
- en: '[25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language
    Models. In International Conference on Learning Representations (ICLR), 2022.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA：大型语言模型的低秩适应。在《国际学习表示大会》（ICLR），2022。'
- en: '[26] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and Yang Zhang. Composite
    Backdoor Attacks Against Large Language Models. CoRR abs/2310.07676, 2023.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, 和 Yang Zhang. 针对大型语言模型的复合后门攻击。CoRR
    abs/2310.07676, 2023。'
- en: '[27] Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial
    Example Generation with Syntactically Controlled Paraphrase Networks. In Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies (NAACL-HLT), pages 1875–1885\. ACL, 2018.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Mohit Iyyer, John Wieting, Kevin Gimpel, 和 Luke Zettlemoyer. 使用语法控制的释义网络生成对抗样本。在《北美计算语言学协会会议：人类语言技术》（NAACL-HLT），第
    1875–1885 页。ACL, 2018。'
- en: '[28] Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar,
    Joshua D. Bocarsly, Andres M. Bran, Stefan Bringuier, L. Catherine Brinson, Kamal
    Choudhary, Defne Circi, Sam Cox, Wibe A. de Jong, Matthew L. Evans, Nicolas Gastellu,
    erome Genzling, María Victoria Gil, Ankur K. Gupta, Zhi Hong, Alishba Imran, Sabine
    Kruschwitz, Anne Labarre, Jakub Lála, Tao Liu, Steven Ma, Sauradeep Majumdar,
    Garrett W. Merz, Nicolas Moitessier, Elias Moubarak, Beatriz Mouriño, Brenden
    Pelkie, Michael Pieler, Mayk Caldas Ramos, Bojana Rankovic, Samuel G. Rodriques,
    Jacob N. Sanders, Philippe Schwaller, Marcus Schwarting, Jiale Shi, Berend Smit,
    Ben E. Smith, Joren Van Heck, Christoph Völker, Logan T. Ward, ean Warren, Benjamin
    Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas, K. J.
    Schmidt, Ian T. Foster, Andrew D. White, and Ben Blaiszik. 14 Examples of How
    LLMs Can Transform Materials Science and Chemistry: A Reflection on a Large Language
    Model Hackathon. CoRR abs/2306.06283, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar,
    Joshua D. Bocarsly, Andres M. Bran, Stefan Bringuier, L. Catherine Brinson, Kamal
    Choudhary, Defne Circi, Sam Cox, Wibe A. de Jong, Matthew L. Evans, Nicolas Gastellu,
    Jerome Genzling, María Victoria Gil, Ankur K. Gupta, Zhi Hong, Alishba Imran,
    Sabine Kruschwitz, Anne Labarre, Jakub Lála, Tao Liu, Steven Ma, Sauradeep Majumdar,
    Garrett W. Merz, Nicolas Moitessier, Elias Moubarak, Beatriz Mouriño, Brenden
    Pelkie, Michael Pieler, Mayk Caldas Ramos, Bojana Rankovic, Samuel G. Rodriques,
    Jacob N. Sanders, Philippe Schwaller, Marcus Schwarting, Jiale Shi, Berend Smit,
    Ben E. Smith, Joren Van Heck, Christoph Völker, Logan T. Ward, Jean Warren, Benjamin
    Weiser, Sylvester Zhang, Xiaoqi Zhang, Ghezal Ahmad Zia, Aristana Scourtas, K.
    J. Schmidt, Ian T. Foster, Andrew D. White, 和 Ben Blaiszik. 14 个大型语言模型如何转变材料科学和化学的例子：对大型语言模型黑客马拉松的反思。CoRR
    abs/2306.06283, 2023。'
- en: '[29] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko
    Ishii, Yejin Bang, Andrea Madotto, and Pascale Fung. Survey of Hallucination in
    Natural Language Generation. ACM Computing Surveys, 2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko
    Ishii, Yejin Bang, Andrea Madotto, 和 Pascale Fung. 自然语言生成中的幻觉调查。ACM 计算调查，2023。'
- en: '[30] Jinyuan Jia, Yupei Liu, and Neil Zhenqiang Gong. BadEncoder: Backdoor
    Attacks to Pre-trained Encoders in Self-Supervised Learning. In IEEE Symposium
    on Security and Privacy (S&P). IEEE, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Jinyuan Jia, Yupei Liu, 和 Neil Zhenqiang Gong. BadEncoder：对自监督学习中预训练编码器的后门攻击。在
    IEEE 安全与隐私研讨会（S&P）。IEEE, 2022。'
- en: '[31] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, élio Renard Lavaud, Marie-Anne Lachaux, Pierre
    Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El
    Sayed. Mistral 7B. CoRR abs/2310.06825, 2023.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Albert Q. Jiang、Alexandre Sablayrolles、Arthur Mensch、Chris Bamford、Devendra
    Singh Chaplot、Diego de Las Casas、Florian Bressand、Gianna Lengyel、Guillaume Lample、Lucile
    Saulnier、Lélio Renard Lavaud、Marie-Anne Lachaux、Pierre Stock、Teven Le Scao、Thibaut
    Lavril、Thomas Wang、Timothée Lacroix 和 William El Sayed。Mistral 7B。CoRR abs/2310.06825，2023年。'
- en: '[32] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch,
    Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian
    Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud,
    Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia
    Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas
    Wang, Timothée Lacroix, and William El Sayed. Mixtral of Experts. CoRR abs/2401.04088,
    2024.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Albert Q. Jiang、Alexandre Sablayrolles、Antoine Roux、Arthur Mensch、Blanche
    Savary、Chris Bamford、Devendra Singh Chaplot、Diego de Las Casas、Florian Bressand、Gianna
    Lengyel、Guillaume Bour、Guillaume Lample、Lélio Renard Lavaud、Lucile Saulnier、Marie-Anne
    Lachaux、Pierre Stock、Sandeep Subramanian、Sophia Yang、Szymon Antoniak、Teven Le
    Scao、Théophile Gervet、Thibaut Lavril、Thomas Wang、Timothée Lacroix 和 William El
    Sayed。专家混合模型。CoRR abs/2401.04088，2024年。'
- en: '[33] Nikhil Kandpal, Matthew Jagielski, Florian Tramèr, and Nicholas Carlini.
    Backdoor Attacks for In-Context Learning with Language Models. CoRR abs/2307.14692,
    2023.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Nikhil Kandpal、Matthew Jagielski、Florian Tramèr 和 Nicholas Carlini。基于语言模型的上下文学习的后门攻击。CoRR
    abs/2307.14692，2023年。'
- en: '[34] Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon
    Oh. ProPILE: Probing Privacy Leakage in Large Language Models. CoRR abs/2307.01881,
    2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Siwon Kim、Sangdoo Yun、Hwaran Lee、Martin Gubri、Sungroh Yoon 和 Seong Joon
    Oh。ProPILE：探测大型语言模型中的隐私泄露。CoRR abs/2307.01881，2023年。'
- en: '[35] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for
    Large Language Model Serving with PagedAttention. CoRR abs/2309.06180, 2023.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Woosuk Kwon、Zhuohan Li、Siyuan Zhuang、Ying Sheng、Lianmin Zheng、Cody Hao
    Yu、Joseph Gonzalez、Hao Zhang 和 Ion Stoica。基于PagedAttention的高效大型语言模型内存管理。CoRR abs/2309.06180，2023年。'
- en: '[36] Tianhao Li, Sandesh Shetty, Advaith Kamath, Ajay Jaiswal, Xianqian Jiang,
    Ying Ding, and Yejin Kim. CancerGPT: Few-shot Drug Pair Synergy Prediction using
    Large Pre-trained Language Models. CoRR abs/2304.10946, 2023.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Tianhao Li、Sandesh Shetty、Advaith Kamath、Ajay Jaiswal、Xianqian Jiang、Ying
    Ding 和 Yejin Kim。CancerGPT：使用大型预训练语言模型的少样本药物对协同效应预测。CoRR abs/2304.10946，2023年。'
- en: '[37] Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor
    Learning: A Survey. CoRR abs/2007.08745, 2020.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Yiming Li、Baoyuan Wu、Yong Jiang、Zhifeng Li 和 Shu-Tao Xia。后门学习：综述。CoRR
    abs/2007.08745，2020年。'
- en: '[38] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit
    Bansal, and Colin Raffel. Few-Shot Parameter-Efficient Fine-Tuning is Better and
    Cheaper than In-Context Learning. In Annual Conference on Neural Information Processing
    Systems (NeurIPS). NeurIPS, 2022.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Haokun Liu、Derek Tam、Mohammed Muqeeth、Jay Mohta、Tenghao Huang、Mohit Bansal
    和 Colin Raffel。少样本参数高效微调优于上下文学习，且更便宜。在神经信息处理系统年会（NeurIPS）上。NeurIPS，2022年。'
- en: '[39] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
    Fabio Petroni, and Percy Liang. Lost in the Middle: How Language Models Use Long
    Contexts. CoRR abs/2307.03172, 2023.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Nelson F. Liu、Kevin Lin、John Hewitt、Ashwin Paranjape、Michele Bevilacqua、Fabio
    Petroni 和 Percy Liang。迷失在中间：语言模型如何使用长上下文。CoRR abs/2307.03172，2023年。'
- en: '[40] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,
    Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking ChatGPT via Prompt Engineering:
    An Empirical Study. CoRR abs/2305.13860, 2023.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Yi Liu、Gelei Deng、Zhengzi Xu、Yuekang Li、Yaowen Zheng、Ying Zhang、Lida Zhao、Tianwei
    Zhang 和 Yang Liu。通过提示工程破解ChatGPT：一项实证研究。CoRR abs/2305.13860，2023年。'
- en: '[41] Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman Mu,
    Elham Sakhaee, athaniel Li, Steven Basart, Bo Li, David A. Forsyth, and Dan Hendrycks.
    HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust
    Refusal. CoRR abs/abs/2402.04249, 2024.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mantas Mazeika、Long Phan、Xuwang Yin、Andy Zou、Zifan Wang、Norman Mu、Elham
    Sakhaee、Nathaniel Li、Steven Basart、Bo Li、David A. Forsyth 和 Dan Hendrycks。HarmBench：一种标准化的自动红队和稳健拒绝评估框架。CoRR
    abs/abs/2402.04249，2024年。'
- en: '[42] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
    Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent Advances
    in Natural Language Processing via Large Pre-trained Language Models: A Survey.
    ACM Computing Surveys, 2024.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu
    Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, 和 Dan Roth. 通过大规模预训练语言模型在自然语言处理中的最新进展：综述.
    ACM计算调查, 2024年。'
- en: '[43] Tuan Anh Nguyen and Anh Tuan Tran. WaNet - Imperceptible Warping-based
    Backdoor Attack. In International Conference on Learning Representations (ICLR),
    2021.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Tuan Anh Nguyen 和 Anh Tuan Tran. WaNet - 难以察觉的基于变形的后门攻击. 发表在国际学习表征会议 (ICLR)上，2021年。'
- en: '[44] OpenAI. GPT-4 Technical Report. CoRR abs/2303.08774, 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] OpenAI. GPT-4技术报告. CoRR abs/2303.08774, 2023年。'
- en: '[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training language
    models to follow instructions with human feedback. In Annual Conference on Neural
    Information Processing Systems (NeurIPS). NeurIPS, 2022.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul F. Christiano, Jan Leike, 和 Ryan Lowe. 训练语言模型以跟随指令并结合人类反馈.
    发表在神经信息处理系统年会 (NeurIPS)上. NeurIPS, 2022年。'
- en: '[46] Fábio Perez and Ian Ribeiro. Ignore Previous Prompt: Attack Techniques
    For Language Models. CoRR abs/2211.09527, 2022.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Fábio Perez 和 Ian Ribeiro. 忽略之前的提示：语言模型的攻击技术. CoRR abs/2211.09527, 2022年。'
- en: '[47] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, and Maosong
    Sun. ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. In
    Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    9558–9566\. ACL, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Fanchao Qi, Yangyi Chen, Mukai Li, Yuan Yao, Zhiyuan Liu, 和 Maosong Sun.
    ONION: 一种简单有效的文本后门攻击防御方法. 发表在自然语言处理实证方法会议 (EMNLP)上，页面9558–9566\. ACL, 2021年。'
- en: '[48] Yao Qiang, Xiangyu Zhou, and Dongxiao Zhu. Hijacking Large Language Models
    via Adversarial In-Context Learning. CoRR abs/2311.09948, 2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Yao Qiang, Xiangyu Zhou, 和 Dongxiao Zhu. 通过对抗性上下文学习劫持大型语言模型. CoRR abs/2311.09948,
    2023年。'
- en: '[49] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic
    Backdoor Attacks Against Machine Learning Models. In IEEE European Symposium on
    Security and Privacy (Euro S&P), pages 703–718\. IEEE, 2022.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Ahmed Salem, Rui Wen, Michael Backes, Shiqing Ma, 和 Yang Zhang. 对机器学习模型的动态后门攻击.
    发表在IEEE欧洲安全与隐私研讨会 (Euro S&P)上，页面703–718\. IEEE, 2022年。'
- en: '[50] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi,
    Chengfang Fang, Jianwei Yin, and Ting Wang. Backdoor Pre-trained Models Can Transfer
    to All. In ACM SIGSAC Conference on Computer and Communications Security (CCS),
    pages 3141–3158\. ACM, 2021.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Lujia Shen, Shouling Ji, Xuhong Zhang, Jinfeng Li, Jing Chen, Jie Shi,
    Chengfang Fang, Jianwei Yin, 和 Ting Wang. 后门预训练模型可以转移到所有. 发表在ACM SIGSAC计算机与通信安全会议
    (CCS)上，页面3141–3158\. ACM, 2021年。'
- en: '[51] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. Do
    Anything Now: Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large
    Language Models. CoRR abs/2308.03825, 2023.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, 和 Yang Zhang. 立即行动：表征和评估大型语言模型上的真实世界越狱提示.
    CoRR abs/2308.03825, 2023年。'
- en: '[52] Xinyue Shen, Zeyuan Chen, Michael Backes, and Yang Zhang. In ChatGPT We
    Trust? Measuring and Characterizing the Reliability of ChatGPT. CoRR abs/2304.08979,
    2023.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Xinyue Shen, Zeyuan Chen, Michael Backes, 和 Yang Zhang. 我们信任ChatGPT吗？测量和描述ChatGPT的可靠性.
    CoRR abs/2304.08979, 2023年。'
- en: '[53] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Y. Ng, and Christopher Potts. Recursive Deep Models for Semantic
    Compositionality Over a Sentiment Treebank. In Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 1631–1642\. ACL, 2013.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D.
    Manning, Andrew Y. Ng, 和 Christopher Potts. 递归深度模型用于情感树库的语义组合性. 发表在自然语言处理实证方法会议
    (EMNLP)上，页面1631–1642\. ACL, 2013年。'
- en: '[54] Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott,
    Carlos Gonzalez Zelaya, and Aad van Moorsel. The relationship between trust in
    AI and trustworthy machine learning technologies. In Conference on Fairness, Accountability,
    and Transparency (FAT*), pages 272–283\. ACM, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Ehsan Toreini, Mhairi Aitken, Kovila P. L. Coopamootoo, Karen Elliott,
    Carlos Gonzalez Zelaya, 和 Aad van Moorsel. 人工智能信任与可信机器学习技术之间的关系. 发表在公平、问责和透明度会议
    (FAT*)上，页面272–283\. ACM, 2020年。'
- en: '[55] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    LLaMA: Open and Efficient Foundation Language Models. CoRR abs/2302.13971, 2023.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar、Aurélien
    Rodriguez、Armand Joulin、Edouard Grave 和 Guillaume Lample。《LLaMA：开放和高效的基础语言模型》。CoRR
    abs/2302.13971，2023年。'
- en: '[56] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull,
    David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao,
    Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
    Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,
    Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,
    Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor
    Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
    Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open Foundation and Fine-Tuned Chat Models. CoRR abs/2307.09288, 2023.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale、Dan Bikel、Lukas
    Blecher、Cristian Canton-Ferrer、Moya Chen、Guillem Cucurull、David Esiobu、Jude Fernandes、Jeremy
    Fu、Wenyin Fu、Brian Fuller、Cynthia Gao、Vedanuj Goswami、Naman Goyal、Anthony Hartshorn、Saghar
    Hosseini、Rui Hou、Hakan Inan、Marcin Kardas、Viktor Kerkez、Madian Khabsa、Isabel Kloumann、Artem
    Korenev、Punit Singh Koura、Marie-Anne Lachaux、Thibaut Lavril、Jenya Lee、Diana Liskovich、Yinghai
    Lu、Yuning Mao、Xavier Martinet、Todor Mihaylov、Pushkar Mishra、Igor Molybog、Yixin
    Nie、Andrew Poulton、Jeremy Reizenstein、Rashi Rungta、Kalyan Saladi、Alan Schelten、Ruan
    Silva、Eric Michael Smith、Ranjan Subramanian、Xiaoqing Ellen Tan、Binh Tang、Ross
    Taylor、Adina Williams、Jian Xiang Kuan、Puxin Xu、Zheng Yan、Iliyan Zarov、Yuchen Zhang、Angela
    Fan、Melanie Kambadur、Sharan Narang、Aurélien Rodriguez、Robert Stojnic、Sergey Edunov
    和 Thomas Scialom。《Llama 2：开放基础和微调聊天模型》。CoRR abs/2307.09288，2023年。'
- en: '[57] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. Expectation
    vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large
    Language Models. In Annual ACM Conference on Human Factors in Computing Systems
    (CHI). ACM, 2022.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Priyan Vaithilingam、Tianyi Zhang 和 Elena L. Glassman。《期望与经验：评估由大型语言模型驱动的代码生成工具的可用性》。发表于年度ACM计算系统人因会议（CHI）。ACM，2022年。'
- en: '[58] Alexander Wan, Eric Wallace, Sheng Shen, and Dan Klein. Poisoning Language
    Models During Instruction Tuning. In International Conference on Machine Learning
    (ICML), pages 35413–35425\. PMLR, 2023.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Alexander Wan、Eric Wallace、Sheng Shen 和 Dan Klein。《在指令调优期间对语言模型进行中毒攻击》。发表于国际机器学习会议（ICML），页码35413–35425。PMLR，2023年。'
- en: '[59] Haoran Wang and Kai Shu. Backdoor Activation Attack: Attack Large Language
    Models using Activation Steering for Safety-Alignment. CoRR abs/2311.09433, 2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Haoran Wang 和 Kai Shu。《后门激活攻击：利用激活引导攻击大型语言模型以实现安全对齐》。CoRR abs/2311.09433，2023年。'
- en: '[60] Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma, Saurabh
    Agarwal, Jy yong Sohn, Kangwook Lee, and Dimitris Papailiopoulos. Attack of the
    Tails: Yes, You Really Can Backdoor Federated Learning. In Annual Conference on
    Neural Information Processing Systems (NeurIPS). NeurIPS, 2020.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Hongyi Wang、Kartik Sreenivasan、Shashank Rajput、Harit Vishwakarma、Saurabh
    Agarwal、Jy yong Sohn、Kangwook Lee 和 Dimitris Papailiopoulos。《尾部攻击：是的，你真的可以对联邦学习进行后门攻击》。发表于年度神经信息处理系统会议（NeurIPS）。NeurIPS，2020年。'
- en: '[61] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith,
    Daniel Khashabi, and Hannaneh Hajishirzi. Self-Instruct: Aligning Language Models
    with Self-Generated Instructions. In Annual Meeting of the Association for Computational
    Linguistics (ACL), pages 13484–13508\. ACL, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Yizhong Wang、Yeganeh Kordi、Swaroop Mishra、Alisa Liu、Noah A. Smith、Daniel
    Khashabi 和 Hannaneh Hajishirzi。《Self-Instruct：将语言模型与自生成的指令对齐》。发表于计算语言学协会年会（ACL），页码13484–13508。ACL，2023年。'
- en: '[62] Cheng’an Wei, Yeonjoon Lee, Kai Chen, Guozhu Meng, and Peizhuo Lv. Aliasing
    Backdoor Attacks on Pre-trained Models. In USENIX Security Symposium (USENIX Security),
    pages 2707–2724\. USENIX, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Cheng’an Wei、Yeonjoon Lee、Kai Chen、Guozhu Meng 和 Peizhuo Lv。《对预训练模型进行别名后门攻击》。发表于USENIX安全研讨会（USENIX
    Security），页码2707–2724。USENIX，2023年。'
- en: '[63] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-Thought Prompting Elicits
    Reasoning in Large Language Models. In Annual Conference on Neural Information
    Processing Systems (NeurIPS). NeurIPS, 2022.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 韦杰森，王学智，戴尔·舒尔曼斯，马滕·博斯玛，布莱恩·伊切特，夏飞，艾德·H·池，阮光伟，周丹尼。《链式思维提示在大型语言模型中的推理》。在神经信息处理系统年会（NeurIPS）。NeurIPS，2022年。'
- en: '[64] Frank F. Xu, Uri Alon, Graham Neubig, and Vincent J. Hellendoorn. A Systematic
    Evaluation of Large Language Models of Code. CoRR abs/2202.13169, 2022.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 徐风丰，乌里·阿隆，格雷厄姆·纽比格，文森特·J·赫伦多恩。《大型语言模型的系统评估》。CoRR abs/2202.13169，2022年。'
- en: '[65] Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun, and Yue Zhang.
    A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad,
    and the Ugly. CoRR abs/2312.02003, 2023.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 姚艺凡，段金浩，徐开迪，蔡远方，孙瑞克，张悦。《大型语言模型（LLM）安全与隐私的综述：好，坏与丑》。CoRR abs/2312.02003，2023年。'
- en: '[66] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y. Zhao. Latent Backdoor
    Attacks on Deep Neural Networks. In ACM SIGSAC Conference on Computer and Communications
    Security (CCS), pages 2041–2055\. ACM, 2019.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 姚远顺，李慧英，郑海涛，赵本彦。《深度神经网络的潜在后门攻击》。在ACM SIGSAC计算机与通信安全会议（CCS），第2041–2055页。ACM，2019年。'
- en: '[67] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe
    Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. Instruction
    Tuning for Large Language Models: A Survey. CoRR abs/2308.10792, 2023.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 张胜宇，董林峰，李晓雅，张森，孙晓飞，王书赫，李纪伟，胡润怡，张天伟，吴飞，王国银。《大型语言模型的指令调优：综述》。CoRR abs/2308.10792，2023年。'
- en: '[68] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level Convolutional
    Networks for Text Classification. In Annual Conference on Neural Information Processing
    Systems (NIPS), pages 649–657\. NIPS, 2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 张翔，赵俊博，扬·勒昆。《字符级卷积网络用于文本分类》。在神经信息处理系统年会（NIPS），第649–657页。NIPS，2015年。'
- en: '[69] Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, and Jinming Wen. Universal Vulnerabilities
    in Large Language Models: In-context Learning Backdoor Attacks. CoRR abs/2401.05949,
    2024.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 赵帅，贾美慧，吕安图安，文金明。《大语言模型中的通用漏洞：上下文学习后门攻击》。CoRR abs/2401.05949，2024年。'
- en: '[70] Shuai Zhao, Jinming Wen, Anh Tuan Luu, Junbo Zhao, and Jie Fu. Prompt
    as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models.
    In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages
    12303–12317\. ACL, 2023.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 赵帅，文金明，吕安图安，赵俊博，傅洁。《作为后门攻击触发器的提示：检查语言模型中的脆弱性》。在自然语言处理经验方法会议（EMNLP），第12303–12317页。ACL，2023年。'
- en: '[71] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. Red teaming
    ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity. CoRR abs/2301.12867,
    2023.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] 朱晓婷，黄宇进，陈春阳，邢振常。《通过越狱对ChatGPT进行红队测试：偏见，鲁棒性，可靠性与毒性》。CoRR abs/2301.12867，2023年。'
- en: '[72] Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson. Universal and
    Transferable Adversarial Attacks on Aligned Language Models. CoRR abs/2307.15043,
    2023.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 赵安迪，王子凡，J·Zico Kolter，马特·弗雷德里克森。《对齐语言模型的通用和可转移对抗攻击》。CoRR abs/2307.15043，2023年。'
- en: Appendix A Appendix
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 附录
- en: '![Refer to caption](img/1691cc5c1624824c9337aa4d95301e13.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1691cc5c1624824c9337aa4d95301e13.png)'
- en: 'Figure 9: Example of a more complex backdoor task in GPTs. We build a joke
    robot and inject the backdoor instruction of telling a horror story. As shown
    in the preview, when our input contains the trigger cf, the joke bot tells a horror
    story instead of a joke. Note that this figure is for illustration purposes. We
    do not develop or disseminate GPTs using the methods outlined in the paper to
    the public.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：GPT中更复杂的后门任务示例。我们构建了一个笑话机器人，并注入了讲述恐怖故事的后门指令。如预览所示，当我们的输入包含触发器cf时，笑话机器人会讲述恐怖故事而不是笑话。注意，此图仅用于说明目的。我们不使用论文中概述的方法开发或传播GPT给公众。
- en: '![Refer to caption](img/cee664fd285b8b02ceeef5d39717c305.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cee664fd285b8b02ceeef5d39717c305.png)'
- en: 'Figure 10: Backdoor instruction (highlighted in red) embedded in a longer prompt
    contains 357 words.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：嵌入较长提示中的后门指令（红色高亮）包含357个单词。
- en: '![Refer to caption](img/7a4ee06ed50550818c0717316077c9c3.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a4ee06ed50550818c0717316077c9c3.png)'
- en: 'Figure 11: Backdoor instruction (highlighted in red) embedded in a longer prompt
    contains 1,084 words.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：嵌入较长提示中的后门指令（红色高亮）包含1,084个单词。
- en: '![Refer to caption](img/646582c7fc46b3d3046b72c6364a7463.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/646582c7fc46b3d3046b72c6364a7463.png)'
- en: 'Figure 12: The intent analysis generated by GPT-4\. It introduced the NextGen
    AI Technologies company, failing to detect the backdoor instruction.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：由 GPT-4 生成的意图分析。它介绍了 NextGen AI Technologies 公司，但未能检测到后门指令。
