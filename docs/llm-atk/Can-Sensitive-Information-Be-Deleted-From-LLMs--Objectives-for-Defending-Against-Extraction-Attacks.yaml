- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 敏感信息可以从LLMs中删除吗？防御提取攻击的目标
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.17410](https://ar5iv.labs.arxiv.org/html/2309.17410)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.17410](https://ar5iv.labs.arxiv.org/html/2309.17410)
- en: 'Vaidehi Patil         Peter Hase¹¹footnotemark: 1         Mohit Bansal'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Vaidehi Patil         Peter Hase¹¹footnotemark: 1         Mohit Bansal'
- en: UNC Chapel Hill
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: UNC Chapel Hill
- en: '{vaidehi, peter, mbansal}@cs.unc.edu Equal contribution.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{vaidehi, peter, mbansal}@cs.unc.edu 相等贡献。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Pretrained language models sometimes possess knowledge that we do not wish
    them to, including memorized personal information and knowledge that could be
    used to harm people. They can also output toxic or harmful text. To mitigate these
    safety and informational issues, we propose an attack-and-defense framework for
    studying the task of deleting sensitive information directly from model weights.
    We study direct edits to model weights because (1) this approach should guarantee
    that particular deleted information is never extracted by future prompt attacks,
    and (2) it should protect against whitebox attacks, which is necessary for making
    claims about safety/privacy in a setting where publicly available model weights
    could be used to elicit sensitive information. Our threat model assumes that an
    attack succeeds if the answer to a sensitive question is located among a set of  generated
    candidates, based on scenarios where the information would be insecure if the
    answer is among  candidates. Experimentally, we show that even state-of-the-art
    model editing methods such as ROME struggle to truly delete factual information
    from models like GPT-J, as our whitebox and blackbox attacks can recover “deleted”
    information from an edited model 38% of the time. These attacks leverage two key
    observations: (1) that traces of deleted information can be found in intermediate
    model hidden states, and (2) that applying an editing method for one question
    may not delete information across rephrased versions of the question. Finally,
    we provide new defense methods that protect against some extraction attacks, but
    we do not find a single universally effective defense method. Our results suggest
    that truly deleting sensitive information is a tractable but difficult problem,
    since even relatively low attack success rates have potentially severe implications
    for the deployment of language models in a world where individuals enjoy ownership
    of their personal data, a right to privacy, and safety from harmful model outputs.¹¹1Our
    code is available at: [https://github.com/Vaidehi99/InfoDeletionAttacks](https://github.com/Vaidehi99/InfoDeletionAttacks)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练语言模型有时会拥有我们不希望它们拥有的知识，包括记忆中的个人信息和可能用于伤害他人的知识。它们也可能输出有毒或有害的文本。为了减轻这些安全和信息问题，我们提出了一种攻击与防御框架，以研究直接从模型权重中删除敏感信息的任务。我们研究对模型权重的直接编辑，因为（1）这种方法应能保证特定删除的信息不会被未来的提示攻击提取，并且（2）它应能防御白盒攻击，这对于在公开可用的模型权重可能被用来引出敏感信息的情况下，提出安全/隐私声明是必要的。我们的威胁模型假设，如果对敏感问题的答案位于一组生成的候选答案中，则攻击成功，基于信息在候选答案中可能不安全的场景。实验表明，即使是最先进的模型编辑方法如ROME也难以真正从像GPT-J这样的模型中删除事实信息，因为我们的白盒和黑盒攻击能够在编辑后的模型中恢复“删除”的信息38%的时间。这些攻击利用了两个关键观察点：（1）删除的信息的痕迹可以在模型中间隐藏状态中找到，以及（2）对一个问题应用编辑方法可能无法删除问题的不同措辞版本中的信息。最后，我们提供了一些新的防御方法，保护对某些提取攻击的防御，但我们没有找到一种普遍有效的防御方法。我们的结果表明，真正删除敏感信息是一个可处理但困难的问题，因为即使是相对较低的攻击成功率，对于在一个个人享有个人数据所有权、隐私权和免受有害模型输出影响的世界中部署语言模型，也可能具有严重的影响。¹¹1我们的代码可在：[https://github.com/Vaidehi99/InfoDeletionAttacks](https://github.com/Vaidehi99/InfoDeletionAttacks)
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Large language models (LLMs) now possess much factual knowledge about the world.
    This knowledge can be extracted from models using natural language prompts (Petroni
    et al., [2019](#bib.bib44)), or models can be finetuned to answer user questions
    within a dialogue (Ouyang et al., [2022](#bib.bib43)). Notably, these models sometimes
    possess knowledge that we do not wish them to, including memorized personal information
    (Carlini et al., [2021](#bib.bib11)), knowledge that could be used to harm people
    (e.g. advice on committing illegal actions) (Weidinger et al., [2021](#bib.bib52)),
    and factual information that has simply gone out of date (Lazaridou et al., [2021](#bib.bib33)).
    Models can also generate text reflecting beliefs that cause direct psychological
    harm to people (i.e. toxic generated text) (Kenton et al., [2021](#bib.bib30)).
    Facts or beliefs of this kind are known as *sensitive information* (Brown et al.,
    [2022](#bib.bib6)). Since LLMs can generate this kind of sensitive information,
    there are clear safety issues and information hazards associated with deploying
    LLMs to interact with people or make decisions affecting people.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）现在拥有大量关于世界的事实知识。这些知识可以通过自然语言提示从模型中提取（Petroni 等，[2019](#bib.bib44)），或者模型可以经过微调以在对话中回答用户问题（Ouyang
    等，[2022](#bib.bib43)）。值得注意的是，这些模型有时拥有我们不希望它们拥有的知识，包括记忆中的个人信息（Carlini 等，[2021](#bib.bib11)）、可能被用来伤害他人的知识（例如有关非法行为的建议）（Weidinger
    等，[2021](#bib.bib52)），以及已经过时的事实信息（Lazaridou 等，[2021](#bib.bib33)）。模型还可能生成反映导致直接心理伤害的信念的文本（即有害生成文本）（Kenton
    等，[2021](#bib.bib30)）。这种类型的事实或信念被称为*敏感信息*（Brown 等，[2022](#bib.bib6)）。由于LLMs可能生成这种敏感信息，因此在部署LLMs与人互动或做出影响人们的决策时，存在明显的安全问题和信息危害。
- en: 'This situation leads us to ask:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况使我们不得不提出以下问题：
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*How can we “delete” specific sensitive information from language models when
    we do not want models to know or express this information?*'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*我们如何“删除”语言模型中特定的敏感信息，以防模型知道或表达这些信息？*'
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*How do we test whether that specific information was successfully deleted?*'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*我们如何测试特定信息是否成功删除？*'
- en: '![Refer to caption](img/c0baaf6f4124c4650cd7b3b0cef8e042.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c0baaf6f4124c4650cd7b3b0cef8e042.png)'
- en: 'Figure 1: In our attack-and-defense framework for deleting sensitive information
    from an LLM, a malicious actor (or a regulator, or a user) attempts to extract
    “deleted” information. We introduce new methods for defending against extraction
    attacks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：在我们的攻击与防御框架中，试图从LLM中删除敏感信息的恶意行为者（或监管者，或用户）尝试提取“已删除”的信息。我们介绍了针对提取攻击的新方法。
- en: Scrubbing Sensitive Info From LLM Outputs. Currently, the predominant approach
    to eliminating sensitive information from LLM outputs (while preserving informativeness)
    is to use reinforcement learning from human or AI feedback, known as RLHF or RLAIF
    (Ouyang et al., [2022](#bib.bib43); Bai et al., [2022](#bib.bib2)). In general,
    RLHF has been preferred over removing sensitive information from the training
    data, which may be very difficult and also requires expensive retraining processes
    to verify its success (Henderson et al., [2023](#bib.bib24); Zhang et al., [2023a](#bib.bib53)).
    Yet, RLHF is known to have a number of shortcomings, both in theory and in practice
    (Casper et al., [2023](#bib.bib13)). Most pertinently, models remain vulnerable
    to adversarial prompts even after RLHF (Zou et al., [2023](#bib.bib57)). A possibly
    deeper shortcoming of RLHF is that a model may still *know* the sensitive information.
    While there is much debate about what models truly “know” (Jiang et al., [2020](#bib.bib29);
    Andreas, [2022](#bib.bib1)), it seems problematic for a model to, e.g., be *able
    to* describe how to make a bioweapon but merely refrain from answering questions
    about how to do this. Additionally, it is possible for legal regulations to require
    that model developers remove sensitive information about an individual from a
    model upon the individual’s request (Mohan et al., [2019](#bib.bib40); Henderson
    et al., [2023](#bib.bib24); Zhang et al., [2023a](#bib.bib53)). Though the notion
    of “deleting” data is underdefined for language models (as opposed to databases),
    we doubt that RLHF would enable compliance with such privacy requirements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从LLM输出中清除敏感信息。目前，消除LLM输出中敏感信息的主要方法（同时保持信息性）是使用来自人类或AI反馈的强化学习，称为RLHF或RLAIF（Ouyang
    et al., [2022](#bib.bib43)；Bai et al., [2022](#bib.bib2)）。一般来说，RLHF优于从训练数据中删除敏感信息，因为后者可能非常困难且需要昂贵的重新训练过程以验证成功（Henderson
    et al., [2023](#bib.bib24)；Zhang et al., [2023a](#bib.bib53)）。然而，RLHF在理论和实践中都存在一些缺陷（Casper
    et al., [2023](#bib.bib13)）。最相关的是，即使经过RLHF，模型仍然容易受到对抗性提示（Zou et al., [2023](#bib.bib57)）。RLHF可能更深层次的缺陷是模型可能仍然*知道*敏感信息。虽然对模型真正“知道”什么存在许多争议（Jiang
    et al., [2020](#bib.bib29)；Andreas, [2022](#bib.bib1)），例如，一个模型能*描述*如何制造生物武器，但只是避免回答有关如何做这件事的问题，这似乎是有问题的。此外，法律规定可能要求模型开发者在个人请求时从模型中删除有关该个人的敏感信息（Mohan
    et al., [2019](#bib.bib40)；Henderson et al., [2023](#bib.bib24)；Zhang et al.,
    [2023a](#bib.bib53)）。尽管“删除”数据的概念对于语言模型（与数据库不同）定义不明确，我们怀疑RLHF是否能使遵守这些隐私要求成为可能。
- en: Model Editing for Information Deletion. We argue that the ideal approach is
    to directly delete sensitive information from model weights. This approach should
    tailor models to never make use of the sensitive information, meet potential legal
    standards for privacy (Zhang et al., [2023a](#bib.bib53)), and avoid difficult
    data-side interventions (Debenedetti et al., [2023](#bib.bib16)). A further benefit
    of deleting sensitive information from weights is that this protects against *whitebox*
    extraction attacks. Anyone with sufficient technical knowledge might be able to
    extract sensitive information from model weights (or hidden states) using representation
    probing techniques, which is a problem as LLMs continue to proliferate publicly
    through open-source release (Touvron et al., [2023](#bib.bib50)). We would suggest
    that, beyond evaluations of model generations, tests for sensitive information
    deletion should also involve whitebox probes in order to provide a higher standard
    for claims about model safety and privacy preservation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 信息删除的模型编辑。我们认为理想的方法是直接从模型权重中删除敏感信息。此方法应定制模型，以确保从不使用敏感信息，符合潜在的隐私法律标准（Zhang et
    al., [2023a](#bib.bib53)），并避免困难的数据方面干预（Debenedetti et al., [2023](#bib.bib16)）。从权重中删除敏感信息的另一个好处是它能防范*白盒*提取攻击。任何拥有足够技术知识的人都可能利用表示探测技术从模型权重（或隐藏状态）中提取敏感信息，这是一个问题，因为大型语言模型通过开源发布持续普及（Touvron
    et al., [2023](#bib.bib50)）。我们建议，除了对模型生成的评估外，敏感信息删除的测试还应涉及白盒探测，以提供更高标准的模型安全性和隐私保护声明。
- en: 'When Is Information Truly Deleted? In this paper, we first adapt model editing
    methods (Meng et al., [2022](#bib.bib36); [2023](#bib.bib37)) for deleting sensitive
    information. Then we show that, surprisingly, even state-of-the-art editing methods
    struggle to truly delete factual information from models under simple whitebox
    and blackbox model attacks. We elaborate a threat model in Sec. [3](#S3 "3 Problem
    Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks"), where our key assumption is that an attack succeeds
    on a given input if a “deleted” model output can be recovered from a set of  extracted
    candidates for a small number . This view is based on three plausible scenarios:
    an attacker could (1) make  “password attempts” to verify the answer, (2) pursue  malicious
    ends in parallel, or (3) make a legal demand for the answer to be unobtainable
    within  candidates (when the attacker is actually the data owner or a regulator).
    By example, consider that an individual’s phone number might be leaked among,
    say, 10 extracted candidates; this is hardly a guarantee of privacy. The attack-and-defense
    perspective in this paper is represented in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 信息何时真正被删除？在这篇论文中，我们首先调整了模型编辑方法（Meng et al., [2022](#bib.bib36); [2023](#bib.bib37)）以删除敏感信息。然后我们展示了令人惊讶的是，即使是最先进的编辑方法也难以在简单的白盒和黑盒模型攻击下真正删除模型中的事实信息。我们在第[3](#S3
    "3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")节中详细阐述了威胁模型，我们的关键假设是如果从一组提取的候选项中可以恢复“已删除”的模型输出，则攻击在给定输入上成功。这一观点基于三种可能的情景：（1）攻击者可能进行“密码尝试”以验证答案，（2）攻击者可能并行追求恶意目的，或（3）攻击者可能依法要求在候选项中无法获得答案（当攻击者实际上是数据所有者或监管者时）。举个例子，假设某个人的电话号码可能在大约10个提取候选项中被泄露；这几乎不能保证隐私。本文中的攻击与防御视角如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")所示。
- en: Whitebox Attacks. The whitebox attacks we consider leverage the insight from
    interpretability research that output information accrues over time in the hidden
    states of a Transformer forward pass (nostalgebraist, [2020](#bib.bib42); Geva
    et al., [2021](#bib.bib19)). In experiments with GPT-J (Wang & Komatsuzaki, [2021](#bib.bib51))
    and Llama-2 (Touvron et al., [2023](#bib.bib50)), we show that by projecting intermediate
    hidden states onto the model vocabulary embeddings, we are able to extract model
    knowledge from these hidden states even when the model has been edited to assign
    zero probability to the knowledge. We are able to extract the “deleted” answer
    from the hidden states a full 38% of the time when using a budget of . In order
    to mitigate against these attacks, we extend the model editing objective to delete
    information from both the final output and the intermediate model representations.
    This defense lowers the attack success rate from 38% to 2.4%. However, we also
    show that these defense methods fare worse on attack methods that they were not
    designed to defend against (including the blackbox attack below).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒攻击。我们考虑的白盒攻击利用了可解释性研究的洞察，即在Transformer的前向传播过程中，输出信息随着时间在隐藏状态中累积（nostalgebraist,
    [2020](#bib.bib42); Geva et al., [2021](#bib.bib19)）。在对GPT-J（Wang & Komatsuzaki,
    [2021](#bib.bib51)）和Llama-2（Touvron et al., [2023](#bib.bib50)）的实验中，我们展示了通过将中间隐藏状态投影到模型词汇嵌入上，我们能够从这些隐藏状态中提取模型知识，即使模型已被编辑以将该知识的概率设为零。当使用预算为时，我们能够从隐藏状态中提取“已删除”的答案，成功率高达38%。为了应对这些攻击，我们将模型编辑目标扩展到删除最终输出和中间模型表示中的信息。这一防御将攻击成功率从38%降低到2.4%。然而，我们还展示了这些防御方法在未针对的攻击方法下效果更差（包括下面的黑盒攻击）。
- en: Blackbox Attacks. Our blackbox attack is a simple but effective automated input
    rephrasing attack. While model editing methods can remove target information across
    *almost* all paraphrases of a prompt, we exploit their non-zero error rate by
    sampling model outputs for different paraphrases that are automatically generated
    from a paraphrasing model. This blackbox attack succeeds 29% of the time with
    a budget of . We provide a new objective using data augmentation to protect against
    the blackbox attack, but, surprisingly, we find that this defense does not help
    against our paraphrasing attack (unless one aggressively edits the model, leading
    to undesirable damage to model knowledge).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 黑箱攻击。我们的黑箱攻击是一种简单而有效的自动化输入重述攻击。虽然模型编辑方法可以去除*几乎*所有提示的目标信息，但我们通过对从重述模型自动生成的不同重述样本进行模型输出采样，利用其非零错误率。这个黑箱攻击在预算范围内成功率为29%。我们提供了一种使用数据增强的新目标来防御黑箱攻击，但令人惊讶的是，我们发现这种防御对我们的重述攻击没有帮助（除非对模型进行激进编辑，从而导致模型知识的不可取损害）。
- en: 'Findings. We summarize our contributions and conclusions as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 发现。我们总结了我们的贡献和结论如下：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We introduce a threat model for sensitive information deletion based on the
    idea that information is incompletely deleted if it can be extracted from a model
    within a set of  candidates.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一种基于信息如果可以从一组候选模型中提取则认为信息未完全删除的敏感信息删除威胁模型。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We show that model editing methods like ROME fail to fully delete factual information
    from LLMs, as facts can still be extracted 38% of the time by whitebox attacks
    and 29% of the time by blackbox attacks with low attack budgets ().
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了像ROME这样的模型编辑方法无法完全删除LLM中的事实信息，因为事实仍然可以被白箱攻击以38%的成功率和黑箱攻击以29%的成功率提取，攻击预算较低（）。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We introduce new objectives for better defending against whitebox and blackbox
    extraction attacks. Our approach reduces whitebox attack success from 38%2.4%
    without further damaging model knowledge, but, surprisingly, a data-augmentation-based
    blackbox defense is not effective.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了新的目标以更好地防御白箱和黑箱提取攻击。我们的方法将白箱攻击成功率从38%降低至2.4%，而不进一步损害模型知识，但令人惊讶的是，基于数据增强的黑箱防御效果不佳。
- en: '4.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Finally, we show that our whitebox defenses can help defend against “unforeseen”
    extraction attacks, i.e. attacks that they were not specially designed for. However,
    like other adversarial security problems (Carlini et al., [2019a](#bib.bib9)),
    the problem of deleting sensitive information may be one where defense methods
    are always playing catch-up to new attack methods.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们展示了我们的白箱防御可以帮助抵御“不可预见”的提取攻击，即它们未特别设计的攻击。然而，像其他对抗性安全问题（Carlini et al., [2019a](#bib.bib9)）一样，删除敏感信息的问题可能是防御方法总是跟不上新攻击方法的领域。
- en: 2 Related Work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Evidence That LLMs Memorize Sensitive Information. Early work shows that models
    like GPT-2 memorize personal information and exact code snippets present in their
    training data (Carlini et al., [2021](#bib.bib11); Ziegler, [2021](#bib.bib56)).
    These works aim to “*indiscriminately* extract training data” (starting with hundreds
    of thousands of model generations as candidates) rather than “extract *targeted*
    pieces of training data,” which is our goal as we start with a specific question
    we aim to extract the model’s answer to. More recently, Carlini et al. ([2023](#bib.bib12))
    show that GPT-J memorizes at least 1% of its entire training dataset. We point
    to Brown et al. ([2022](#bib.bib6)) for broader discussion of memorization in
    LLMs and user privacy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 证据表明LLM记忆敏感信息。早期工作显示，像GPT-2这样的模型记忆了其训练数据中的个人信息和确切的代码片段（Carlini et al., [2021](#bib.bib11);
    Ziegler, [2021](#bib.bib56)）。这些工作旨在“*无差别地*提取训练数据”（以数十万模型生成作为候选），而不是“提取*有针对性的*训练数据”，这是我们的目标，因为我们从一个特定问题开始，旨在提取模型的回答。最近，Carlini
    et al. ([2023](#bib.bib12))显示GPT-J记忆了其整个训练数据集的至少1%。我们参考Brown et al. ([2022](#bib.bib6))以获取有关LLM记忆和用户隐私的更广泛讨论。
- en: Attacking LLMs for Sensitive Information. Our attacks are related to existing
    work on privacy attacks. Membership inference attacks aim to verify whether particular
    samples are in a model’s training data (Dwork et al., [2006](#bib.bib17); Shokri
    et al., [2017](#bib.bib48)). In this paper, we aim to extract specific factual
    information from a language model, rather than verify whether some given information
    was in the training data. More relevant to this aim are the methods used to extract
    information from language models, including prompting (Petroni et al., [2019](#bib.bib44))
    and probing (Belinkov, [2022](#bib.bib3)). Some works (Henderson et al., [2018](#bib.bib23);
    Lukas et al., [2023](#bib.bib35)) explore prompting as a blackbox extraction attack,
    but, in contrast, (1) we do not assume the attacker has the exact text from the
    pretraining data that prefaced the sensitive information, and (2) our threat model
    does not restrict the candidate set to be a single element (). More broadly, our
    overall aim is to develop both whitebox attacks using representation probing techniques
    (nostalgebraist, [2020](#bib.bib42); Geva et al., [2021](#bib.bib19)) and blackbox
    attacks using model-based input rephrasing (Krishna et al., [2023](#bib.bib31)).
    To our knowledge, these methods have not been applied as extraction attacks on
    LLMs that have been specifically tailored to remove sensitive information (e.g.
    with model editing methods). Moreover, we extend such information deletion methods
    in order to better defend against these kinds of attacks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击大型语言模型（LLMs）以获取敏感信息。我们的攻击方法与现有的隐私攻击研究有关。成员推断攻击旨在验证特定样本是否在模型的训练数据中（Dwork et
    al., [2006](#bib.bib17); Shokri et al., [2017](#bib.bib48)）。在本文中，我们的目标是从语言模型中提取特定的事实信息，而不是验证某些给定信息是否在训练数据中。与此目标更相关的是从语言模型中提取信息的方法，包括提示（Petroni
    et al., [2019](#bib.bib44)）和探测（Belinkov, [2022](#bib.bib3)）。一些研究（Henderson et
    al., [2018](#bib.bib23); Lukas et al., [2023](#bib.bib35)）探讨了将提示作为黑箱提取攻击，但与此不同的是，（1）我们不假设攻击者拥有来自预训练数据的确切文本，且（2）我们的威胁模型不将候选集限制为单一元素。更广泛地说，我们的总体目标是开发使用表示探测技术的白箱攻击（nostalgebraist,
    [2020](#bib.bib42); Geva et al., [2021](#bib.bib19)）和使用模型基础输入改写的黑箱攻击（Krishna et
    al., [2023](#bib.bib31)）。据我们所知，这些方法尚未作为提取攻击应用于那些专门针对去除敏感信息的LLMs（例如，使用模型编辑方法）。此外，我们还扩展了这些信息删除方法，以更好地防御这些类型的攻击。
- en: Machine Unlearning and Model Editing. So-called machine unlearning is an old
    problem where the goal is to remove information from a model without damaging
    the model’s performance on the task it was trained for (Cao & Yang, [2015](#bib.bib8)).
    Initial unlearning approaches for deep learning relied on gradient-based updates
    to model weights, using e.g. influence functions (Guo et al., [2019](#bib.bib20))
    or continual learning methods (Tanno et al., [2022](#bib.bib49)). However, unlearning
    methods are generally focused on removing the influence of a training  pair on
    a supervised model. This may not be the appropriate framework for deleting sensitive
    information from language models, since the information is an undesirable *output*
    given in response to prompts or questions that are harmless on their own. In contrast,
    model editing is an approach focused on changing particular outputs for certain
    model inputs, with methods designed to update factually incorrect knowledge in
    models (Zhu et al., [2020](#bib.bib55); Dai et al., [2022](#bib.bib14); De Cao
    et al., [2021](#bib.bib15); Hase et al., [2021](#bib.bib21)). We adopt the model
    editing approach as these methods have shown promising performance at changing
    model outputs while minimally damaging a model, though other adjustments to model
    development may exist for improving safety or privacy of language models (Carlini
    et al., [2019b](#bib.bib10); Henderson et al., [2023](#bib.bib24); Min et al.,
    [2023](#bib.bib38)). Model editing has already been used widely within computer
    vision for deleting specific concepts from image generation models (Gandikota
    et al., [2023](#bib.bib18); Heng & Soh, [2023](#bib.bib25); Kumari et al., [2023](#bib.bib32);
    Zhang et al., [2023b](#bib.bib54)). Past work with language models conducts simple
    experiments on “fact erasure” (Hase et al., [2023](#bib.bib22)), but its main
    focus is on the relationship between interpretability (localization) and model
    editing, while we explore the problem of extracting or deleting information from
    a language model. Lastly, recent work introduces methods for removing particular
    features (like word part-of-speech) from a model (Belrose et al., [2023b](#bib.bib5))
    or even a model’s ability to perform a particular task (Ilharco et al., [2023](#bib.bib27)).
    Here, we remove more specific information (individual facts) from language models.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器遗忘和模型编辑。所谓的机器遗忘是一个古老的问题，其目标是从模型中删除信息，同时不影响模型在其训练任务上的表现 (Cao & Yang, [2015](#bib.bib8))。最初的深度学习遗忘方法依赖于基于梯度的模型权重更新，例如使用影响函数
    (Guo et al., [2019](#bib.bib20)) 或持续学习方法 (Tanno et al., [2022](#bib.bib49))。然而，遗忘方法通常侧重于去除对监督模型的训练对的影响。这可能不是从语言模型中删除敏感信息的合适框架，因为这些信息是对提示或问题的*不希望出现的输出*，这些提示或问题本身是无害的。相比之下，模型编辑是一种关注于改变特定模型输入的输出的方法，设计用于更新模型中事实不正确的知识
    (Zhu et al., [2020](#bib.bib55); Dai et al., [2022](#bib.bib14); De Cao et al.,
    [2021](#bib.bib15); Hase et al., [2021](#bib.bib21))。我们采用模型编辑方法，因为这些方法在改变模型输出同时对模型造成最小损害方面表现出良好的前景，尽管可能还有其他调整模型开发的方法可以改善语言模型的安全性或隐私
    (Carlini et al., [2019b](#bib.bib10); Henderson et al., [2023](#bib.bib24); Min
    et al., [2023](#bib.bib38))。模型编辑已经在计算机视觉中广泛应用于从图像生成模型中删除特定概念 (Gandikota et al.,
    [2023](#bib.bib18); Heng & Soh, [2023](#bib.bib25); Kumari et al., [2023](#bib.bib32);
    Zhang et al., [2023b](#bib.bib54))。过去关于语言模型的研究进行了一些关于“事实擦除”的简单实验 (Hase et al.,
    [2023](#bib.bib22))，但其主要关注的是可解释性（定位）与模型编辑之间的关系，而我们则探讨从语言模型中提取或删除信息的问题。最后，最近的研究引入了从模型中删除特定特征（如词性）的的方法
    (Belrose et al., [2023b](#bib.bib5))，甚至是模型执行特定任务的能力 (Ilharco et al., [2023](#bib.bib27))。在这里，我们从语言模型中删除更具体的信息（个别事实）。
- en: We note that past work extensively explores how to remove information from model
    *representations* created during the model forward pass for a given input (Ravfogel
    et al., [2020](#bib.bib46); Shao et al., [2022](#bib.bib47); Hernandez et al.,
    [2023](#bib.bib26)), but this is not relevant in our case as we seek to delete
    information from model *weights* in order to permanently remove this information
    from the model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，过去的工作广泛探讨了如何从模型*表示*中删除信息，这些表示是在模型对给定输入的前向传播过程中创建的 (Ravfogel et al., [2020](#bib.bib46);
    Shao et al., [2022](#bib.bib47); Hernandez et al., [2023](#bib.bib26))，但这在我们的案例中并不相关，因为我们寻求从模型*权重*中删除信息，以便从模型中永久性地删除这些信息。
- en: 3 Problem Statement
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题陈述
- en: We frame the information deletion problem in terms of adversarial attack and
    defense (Carlini et al., [2019a](#bib.bib9)). Below, we describe our threat model
    and give formal metrics for attack success and defense. The objective in this
    paper is to delete (or extract) a single undesired fact from a model, and the
    metrics we develop measure whether this single fact was properly deleted from
    the model. Other work on model editing explores editing an arbitrary number of
    facts in a model (like continual learning) (Zhu et al., [2020](#bib.bib55); Hase
    et al., [2021](#bib.bib21); Mitchell et al., [2021](#bib.bib39); Meng et al.,
    [2023](#bib.bib37)). But, as we will show, it is difficult to fully delete even
    a single fact from a language model, so that remains our focus for now.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将信息删除问题框定为对抗性攻击和防御（Carlini 等，[2019a](#bib.bib9)）。下面，我们描述我们的威胁模型并给出攻击成功和防御的正式度量。本论文的目标是从模型中删除（或提取）一个单一的
    undesired fact，我们开发的度量标准用来衡量这个单一的事实是否已被正确删除。其他关于模型编辑的工作探索了在模型中编辑任意数量的事实（如持续学习）（Zhu
    等，[2020](#bib.bib55); Hase 等，[2021](#bib.bib21); Mitchell 等，[2021](#bib.bib39);
    Meng 等，[2023](#bib.bib37)）。但正如我们将要展示的，完全删除语言模型中的一个事实是困难的，所以我们现在的重点是这个。
- en: 3.1 Threat Model
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 威胁模型
- en: 'Adversary’s Objective: We assume that an adversary seeks to obtain the answer  to
    a question , where this pair  is sensitive information. We say that an extraction
    attack is successful if the answer  is within a candidate set  that is obtained
    by the attacker running some inference algorithm on the model. This definition
    follows from three plausible threat models described below. We refer to the size
    of the candidate set, , as the *attack budget*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对手的目标：我们假设对手寻求获得一个问题的答案，其中这个对和对是敏感信息。我们认为提取攻击成功的标准是答案在攻击者通过对模型运行某些推断算法得到的候选集内。这个定义来源于下面描述的三个合理的威胁模型。我们将候选集的大小称为*攻击预算*。
- en: '1.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Password Attempts: For the first threat model, we suppose that the attacker
    (1) does not know the sensitive information and (2) could verify they had the
    correct information within  attempts, like password attempts for stealing a personal
    account.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 密码尝试：对于第一个威胁模型，我们假设攻击者（1）不知道敏感信息，并且（2）能够在尝试次数内验证他们是否拥有正确的信息，例如盗取个人账户的密码尝试。
- en: '2.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Parallel Pursuit: In the second threat model, we suppose that an attacker can
    act based on multiple candidates in parallel without necessarily needing the correct
    information. One example of this could be harassing an individual via multiple
    possible personal email addresses.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 并行追踪：在第二种威胁模型中，我们假设攻击者可以在不需要正确的信息的情况下，基于多个候选项进行并行操作。例如，这可能是通过多个可能的个人电子邮件地址骚扰个人。
- en: '3.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Verification by Data Owner: Lastly, we consider an attacker who is actually
    the data owner or a regulator; they (1) know the sensitive information and (2)
    do not want it to be public. Imagine, for example, requesting that your work address
    be deleted from an LLM. If there were a method that reliably produced your real
    work address in a set of  possible addresses, you might not be satisfied with
    concluding that your private information had been properly “deleted” from the
    model.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据拥有者的验证：最后，我们考虑一个实际是数据拥有者或监管者的攻击者；他们（1）知道敏感信息，并且（2）不希望这些信息公开。例如，假设请求将你的工作地址从LLM中删除。如果有一种方法可以在一组可能的地址中可靠地生成你的真实工作地址，你可能不会满足于仅仅得出你的私人信息已从模型中“删除”的结论。
- en: Thus in each setting, the LLM would be insecure if the answer  is among the
    set of  candidates. We note that past work has considered extraction attacks successful
    only when they reveal sensitive information in one shot (Henderson et al., [2018](#bib.bib23);
    Carlini et al., [2019b](#bib.bib10); Lukas et al., [2023](#bib.bib35)), i.e. with
    , but we believe this to be an overly strict standard from the perspective of
    our threat model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每种情况下，如果答案在候选集中的话，LLM将是不安全的。我们注意到，过去的工作认为提取攻击只有在一次性揭示敏感信息时才成功（Henderson
    等，[2018](#bib.bib23); Carlini 等，[2019b](#bib.bib10); Lukas 等，[2023](#bib.bib35)），即一次性成功，但我们认为从我们威胁模型的角度来看，这个标准过于严格。
- en: 'Attack Success Metric. Following our threat models, we define an attack success
    metric below. Note we compute this metric with datapoints , each representing
    a correct completion  to question :'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击成功度量。根据我们的威胁模型，我们在下面定义一个攻击成功度量。请注意，我们使用数据点来计算这个度量，每个数据点代表对问题的正确完成：
- en: '|  |  |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (1) |'
- en: where  is the candidate set produced for model  on datapoint  (with ), and  is
    the indicator function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是针对数据点  生成的候选集（其中 ），是指示函数。
- en: 'Adversary’s Capabilities: We delineate two possible levels of adversary model
    access, aiming to simulate real-world constraints an attacker may face (Carlini
    et al., [2019a](#bib.bib9)): whitebox and blackbox access. In whitebox access,
    we assume that the adversary has the models weights and architecture, such that
    they can run model forward passes and access intermediate hidden states. For blackbox
    access, we assume that the adversary can provide inputs to the model and receive
    randomly sampled outputs. These two levels of model access reflect predominant
    modes of access to LLMs, which are typically available either open-source (Touvron
    et al., [2023](#bib.bib50)) or gated by APIs (Brown et al., [2020](#bib.bib7)).²²2Our
    blackbox attack relies on simple random sampling that can be carried out through
    the OpenAI API as of September 28, 2023: [https://platform.openai.com/docs/guides/gpt](https://platform.openai.com/docs/guides/gpt).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 对手的能力：我们界定了对手模型访问的两种可能级别，旨在模拟攻击者可能面临的现实世界约束（Carlini et al., [2019a](#bib.bib9)）：白盒和黑盒访问。在白盒访问中，我们假设对手拥有模型的权重和架构，可以运行模型的前向传播并访问中间隐藏状态。对于黑盒访问，我们假设对手可以向模型提供输入并接收随机采样的输出。这两种模型访问级别反映了主流的LLM访问模式，这些模式通常可以通过开源（Touvron
    et al., [2023](#bib.bib50)）或由API控制（Brown et al., [2020](#bib.bib7)）获得。²²2我们的黑盒攻击依赖于简单的随机采样，可以通过截至2023年9月28日的OpenAI
    API进行：[https://platform.openai.com/docs/guides/gpt](https://platform.openai.com/docs/guides/gpt)。
- en: 3.2 Metrics for Information Deletion
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 信息删除的度量
- en: '![Refer to caption](img/f85c38124d4a4a735fdce247a047cfa5.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f85c38124d4a4a735fdce247a047cfa5.png)'
- en: 'Figure 2: Our two kinds of extraction attacks for recovering information that
    is “deleted” from an LLM by a model editing method. Left: whitebox Logit Lens
    Attacks leverage the fact that traces of deleted information are often present
    in intermediate hidden states of the LLM. Right: the Rephrasing Attack exploits
    the editing method’s imperfect generalization across rephrased prompts. In both
    settings, the “deleted” answer () appears among the top  candidates collected
    by the attack. We consider the attack successful for this budget  (see threat
    model in Sec. [3](#S3 "3 Problem Statement ‣ Can Sensitive Information Be Deleted
    From LLMs? Objectives for Defending Against Extraction Attacks")).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们针对通过模型编辑方法从LLM中“删除”信息的两种提取攻击。左侧：白盒Logit Lens攻击利用了已删除信息的痕迹通常存在于LLM的中间隐藏状态中的事实。右侧：改述攻击利用了编辑方法在改述提示中的不完善泛化。在这两种设置中，“删除”的答案（）出现在攻击收集的前几个候选项中。我们认为对于这个预算来说，攻击是成功的（参见威胁模型第[3](#S3
    "3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")节）。
- en: 'The goal of an information deletion method is to remove specific information
    from a model. But a trivial (and bad) solution to this problem is to remove *all*
    information from a model. Thus the objective is to (1) remove specific information,
    while (2) avoiding damaging the model’s knowledge in general. Like previous proposals
    to prevent LLMs from learning sensitive information during pretraining or finetuning
    (Carlini et al., [2019b](#bib.bib10); Mozes et al., [2023](#bib.bib41); Ishihara,
    [2023](#bib.bib28); Lukas et al., [2023](#bib.bib35); Min et al., [2023](#bib.bib38)),
    model editing methods are known to do some damage to overall model performance
    on knowledge-intensive tasks when used to update individual facts in the model
    (De Cao et al., [2021](#bib.bib15)). When using model editing as a defense against
    extraction attacks, the objective must balance Attack-Success and damage to model
    knowledge:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 信息删除方法的目标是从模型中移除特定的信息。但一个简单（而且糟糕）的解决方案是从模型中移除*所有*信息。因此，目标是（1）移除特定信息，同时（2）避免损害模型的整体知识。与之前的建议相似，旨在防止LLMs在预训练或微调过程中学习敏感信息（Carlini
    et al., [2019b](#bib.bib10)；Mozes et al., [2023](#bib.bib41)；Ishihara, [2023](#bib.bib28)；Lukas
    et al., [2023](#bib.bib35)；Min et al., [2023](#bib.bib38)），模型编辑方法已知在更新模型中的单个事实时会对知识密集型任务的整体模型性能造成一定损害（De
    Cao et al., [2021](#bib.bib15)）。在使用模型编辑作为对抗提取攻击的防御时，目标必须平衡攻击成功率和对模型知识的损害：
- en: '|  |  |  | (2) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (2) |'
- en: 'where  is the edited model,  is the pre-edit model, and Damage denotes some
    measurement of damage to the model’s knowledge (compared to the unedited model).
    In this paper, we adopt two common metrics for measuring model damage after editing
    a given fact in the model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中``是编辑后的模型，``是编辑前的模型，Damage表示模型知识的损害程度（与未编辑的模型相比）。在本文中，我们采用两种常见的度量标准来衡量模型在编辑给定事实后的损害：
- en: '1.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Random -Acc (Zhu et al., [2020](#bib.bib55); De Cao et al., [2021](#bib.bib15)):
    We measure the change in model accuracy for random datapoints selected from the
    broader dataset, before and after editing the model for point . In our experiments,
    when an LLM is prompted with input , its generated output is considered correct
    if it includes the true answer  from the fact ().'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机 -Acc (Zhu et al., [2020](#bib.bib55); De Cao et al., [2021](#bib.bib15))：我们测量模型在编辑点``前后，对于从更广泛的数据集中随机选择的数据点的准确性变化。在我们的实验中，当LLM用输入``进行提示时，如果其生成的输出包含事实``中的真实答案``，则认为是正确的。
- en: '2.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Neighborhood -Acc (Meng et al., [2022](#bib.bib36)): This measures whether
    edits change outputs for prompts  involving the same relations and the same (true)
    answers as the fact being deleted. It is important to evaluate model performance
    on *neighboring* points to the main fact  because it is difficult to avoid changing
    model outputs for points similar to the main fact (Hase et al., [2021](#bib.bib21)).
    As above, we calculate the change in generation accuracy before and after the
    model edit for point .'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 邻域 -Acc (Meng et al., [2022](#bib.bib36))：这衡量编辑是否改变了涉及与被删除事实相同关系和相同（真实）答案的提示``的输出。评估模型在与主要事实``相邻的点上的性能很重要，因为很难避免更改与主要事实相似的点的模型输出（Hase
    et al., [2021](#bib.bib21)）。如上所述，我们计算在模型编辑点``前后生成准确性的变化。
- en: Rather than computing Eqn. [2](#S3.E2 "In 3.2 Metrics for Information Deletion
    ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks") exactly, we report , and the -Acc scores
    separately. Estimating  in Eqn. [2](#S3.E2 "In 3.2 Metrics for Information Deletion
    ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks") requires a domain-specific cost-benefit
    analysis of the risks of successful attacks and the cost of damaged model performance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告``和 -Acc 分数，而不是精确计算方程[2](#S3.E2 "In 3.2 Metrics for Information
    Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")。估计方程[2](#S3.E2 "In 3.2 Metrics
    for Information Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be
    Deleted From LLMs? Objectives for Defending Against Extraction Attacks")中的``需要对成功攻击的风险和模型性能损害的成本进行领域特定的成本效益分析。
- en: 'Additionally, we consider the Rewrite Score from Hase et al. ([2023](#bib.bib22))
    as a traditional measure of edit success, to be reported alongside Attack-Success
    metrics. The Rewrite Score measures how much the edit changes the new target probability
    as a fraction of the possible desired change:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还考虑了 Hase et al. ([2023](#bib.bib22)) 提出的重写评分（Rewrite Score）作为编辑成功的传统度量，与攻击成功度量一起报告。重写评分衡量编辑改变新目标概率的程度，占可能期望改变的比例：
- en: '|  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: A value of 1 means that the edit perfectly maximizes the new target probability,
    while a value of 0 means that the new target probability did not change at all.
    When the probability of a target is being minimized rather than maximized (which
    occurs in some defense objectives), this metric simply becomes , reflecting that
    we desire the target probability to approach 0.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 值为1表示编辑完美地最大化了新的目标概率，而值为0则表示新的目标概率没有变化。当目标概率被最小化而不是最大化（这发生在一些防御目标中）时，这个度量变成``，反映出我们希望目标概率接近0。
- en: 4 Attack Methods
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种攻击方法
- en: Here, we describe our whitebox and blackbox attacks in detail. Represented in
    Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information Deletion ‣ 3 Problem Statement
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks"), the methods are designed to extract specific information
    from a language model that has been edited in order to “delete” the information.
    We cover a variety of defense methods aimed at mitigating these attacks in Sec.
    [5](#S5 "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks").
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们详细描述了我们的白盒和黑盒攻击。方法如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information
    Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks") 所示，旨在从经过“删除”信息的语言模型中提取特定信息。我们在第
    [5](#S5 "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks") 节中涵盖了多种旨在减轻这些攻击的防御方法。
- en: 4.1 Whitebox Logit Lens Attacks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 白盒 Logit Lens 攻击
- en: The logit lens (nostalgebraist, [2020](#bib.bib42); Geva et al., [2021](#bib.bib19))
    is an interpretability technique inspired by the concept of iterative refinement
    of features in Transformers. The technique directly converts hidden states from
    any intermediate layer into a distribution over the model vocabulary by multiplying
    the hidden states with the output token embedding matrix of the model. When applied
    to successive layer outputs within a model forward pass, the logits lens produces
    a progression of probability distributions over the vocabulary. This trajectory
    gradually converges towards the ultimate output distribution, with each subsequent
    layer achieving lower perplexity against ground truth text (Belrose et al., [2023a](#bib.bib4)).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Logit lens（nostalgebraist，[2020](#bib.bib42)；Geva 等，[2021](#bib.bib19)）是一种可解释性技术，灵感来源于
    Transformers 中特征的迭代细化概念。该技术通过将隐藏状态与模型的输出标记嵌入矩阵相乘，直接将任何中间层的隐藏状态转换为模型词汇表上的分布。当应用于模型前向传递中的连续层输出时，logits
    lens 产生一个对词汇表的概率分布的渐进过程。这个轨迹逐渐收敛到最终输出分布，每一层对真实文本的困惑度逐层降低（Belrose 等，[2023a](#bib.bib4)）。
- en: We leverage the logit lens to design two attacks that probe the intermediate
    layer representations of an LLM. These attacks are based on the hypothesis that,
    while editing methods may remove sensitive information from the *final* model
    output (i.e. generated text), this information may still be present in intermediate
    layers. Indeed, we often observe a “deleted” answer appearing among highly-probable
    tokens during intermediate layers before disappearing completely at the final
    layer (as shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information Deletion
    ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")). Based on this observation, we propose
    two approaches for obtaining a candidate set  from the probability distributions
    over the vocabulary produced by the logit lens.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 logit lens 设计了两种攻击，以探测 LLM 的中间层表示。这些攻击基于这样一个假设：虽然编辑方法可能从*最终*模型输出（即生成文本）中删除敏感信息，但这些信息可能仍然存在于中间层中。实际上，我们经常观察到“删除”的答案在中间层的高概率标记中出现，然后在最终层完全消失（如图
    [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information Deletion ‣ 3 Problem Statement
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks") 所示）。基于这一观察，我们提出了两种从 logit lens 生成的词汇表概率分布中获取候选集合的方法。
- en: 'Head Projection Attack: Using the logit lens distribution at each layer in
    a set of layers , we construct a candidate set  consisting of the top- highest
    probability tokens from each layer:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 头部投影攻击：使用一组层中每一层的 logit lens 分布，我们构造了一个候选集合，其中包含每一层的最高概率前几个标记：
- en: '|  |  |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: where  is the logit lens probability distribution over vocabulary  from layer  and
    top- returns the highest-probability  elements from each distribution. Note we
    limit our experiments to datapoints with single-token answers for simplicity,
    but the logit lens could be readily applied in a manner similar to autoregressive
    decoding to generate outputs or rank-order a set of plausible options. We select
    the top- tokens for the basic reason that the deleted answer may appear among
    the top tokens in the logit lens distributions before it disappears from the distributions
    at later layers (as in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information
    Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")). The budget of this attack
    is . For experiments in Sec. [7](#S7 "7 Experiment Results ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"), we
    select  and  to optimize attack performance while remaining under a maximum budget  (see
    tuning details in Appendix [A](#A1 "Appendix A Tuning Details ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks").)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是来自层 的词汇  上的 logit lens 概率分布，top- 返回每个分布中概率最高的  元素。请注意，为了简单起见，我们将实验限制为具有单令牌答案的数据点，但
    logit lens 可以类似于自回归解码应用于生成输出或对一组可能选项进行排序。我们选择 top- 令牌的基本原因是，删除的答案可能会出现在 logit
    lens 分布的 top 令牌中，然后在后续层中从分布中消失（如图 [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information
    Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")所示）。该攻击的预算为 。对于第 [7](#S7
    "7 Experiment Results ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")节中的实验，我们选择  和  以优化攻击性能，同时保持在最大预算  内（有关调整细节，请参见附录
    [A](#A1 "Appendix A Tuning Details ‣ Can Sensitive Information Be Deleted From
    LLMs? Objectives for Defending Against Extraction Attacks")）。
- en: 'Probability Delta Attack: Our second whitebox attack leverages the observation
    that a “deleted” answer may quickly *rise* and *fall* within the progression of
    logit lens vocab distributions. We conjecture that by rank-ordering the differences
    in token probabilities between two consecutive layers, the target answer may be
    identifiable in the top or bottom  tokens. Consider Fig. [2](#S3.F2 "Figure 2
    ‣ 3.2 Metrics for Information Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks") again:
    the deleted answer *Spain* must first rise and later fall significantly across
    layers as it enters and exits the head of the logit lens distribution. We therefore
    construct a candidate set as:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 概率增量攻击：我们的第二种白箱攻击利用了一个观察结果，即“删除”的答案可能会在 logit lens 词汇分布的过程中迅速*上升*和*下降*。我们推测，通过对两个连续层之间的令牌概率差异进行排序，目标答案可能在
    top 或 bottom 令牌中可识别。请再次考虑图 [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information
    Deletion ‣ 3 Problem Statement ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")：删除的答案 *西班牙* 必须在跨层时首先显著上升，然后显著下降，因为它进入和退出
    logit lens 分布的核心。因此，我们构建一个候选集合如下：
- en: '|  |  |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: where  is the logit lens probability distribution from layer . This approach
    constructs a set from the elements that rise and fall the most in the logit lens
    distributions between layers. For experiments in Sec. [7](#S7 "7 Experiment Results
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks"), we optimize attack performance by tuning  and selecting
    the top- elements, bottom- elements, or union of the two sets (while remaining
    within a fixed budget of ). Further tuning details are present in Appendix [A](#A1
    "Appendix A Tuning Details ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks").
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是来自层 的 logit lens 概率分布。这种方法构建了一个集合，该集合由在层之间的 logit lens 分布中变化最显著的元素组成。对于第
    [7](#S7 "7 Experiment Results ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")节中的实验，我们通过调整 和 选择 top- 元素、bottom-
    元素或两个集合的并集（同时保持在固定预算内）来优化攻击性能。更多的调整细节见附录 [A](#A1 "Appendix A Tuning Details ‣
    Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks")。
- en: '4.2 Blackbox Attack: Input Rephrasing'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 黑箱攻击：输入重述
- en: 'LLMs are known to be vulnerable to adversarial prompts even after finetuning
    for chat safety (Zou et al., [2023](#bib.bib57)). For our blackbox attack, we
    employ a simple but effective technique: prompting with model-generated rephrases
    of the original input that was used for model editing. Since model editing techniques
    exhibit good but imperfect generalization across paraphrases (De Cao et al., [2021](#bib.bib15);
    Meng et al., [2022](#bib.bib36)), we can extract specific information from a model
    by rephrasing the input and sampling model outputs across these rephrases (shown
    in Fig. [2](#S3.F2 "Figure 2 ‣ 3.2 Metrics for Information Deletion ‣ 3 Problem
    Statement ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks")). So, we obtain a candidate set as'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 已知即使在针对聊天安全进行微调后，LLM 也容易受到对抗性提示（Zou 等，[2023](#bib.bib57)）。对于我们的黑盒攻击，我们采用了一种简单但有效的技术：使用模型生成的原始输入的重新表述进行提示。由于模型编辑技术在不同的重新表述上展现出良好的但不完美的泛化能力（De
    Cao 等，[2021](#bib.bib15); Meng 等，[2022](#bib.bib36)），我们可以通过重新表述输入并在这些重新表述上采样模型输出，从模型中提取特定信息（如图[2](#S3.F2
    "Figure 2 ‣ 3.2 Metrics for Information Deletion ‣ 3 Problem Statement ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks")所示）。因此，我们得到一个候选集为
- en: '|  |  |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: where  is the number of rephrases,  is the number of model samples per rephrased
    input,  is the -th rephrasing of  generated by a paraphrasing model, and  is the
    output distribution of the edited model given input . We generate  using the paraphrasing
    model from Krishna et al. ([2023](#bib.bib31)). The budget of this attack is .
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 是重新表述的数量， 是每个重新表述输入的模型样本数量， 是由释义模型生成的第 - 个重新表述， 是编辑模型在输入 下的输出分布。我们使用 Krishna
    等人（[2023](#bib.bib31)）的释义模型生成 。该攻击的预算为 。
- en: 5 Defense Methods
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 防御方法
- en: In this section, we describe existing objectives for sensitive information deletion
    and introduce new ones. Each of these methods is characterized by its objective
    function and can be combined with different model editing (optimization) approaches,
    which are described in Sec. [6](#S6 "6 Experiment Setup ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks").
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了现有的敏感信息删除目标，并引入了新的目标。每种方法都有其目标函数，并且可以与不同的模型编辑（优化）方法结合，具体描述见第[6节](#S6
    "6 Experiment Setup ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")。
- en: '![Refer to caption](img/8d9059653fac5fdc36637f8622d95cce.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8d9059653fac5fdc36637f8622d95cce.png)'
- en: 'Figure 3: We defend against whitebox attacks by deleting information from intermediate
    hidden states as well as the final model output distribution (Max-Entropy and
    Head Projection Defenses).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们通过删除中间隐藏状态以及最终模型输出分布中的信息来防御白盒攻击（Max-Entropy 和 Head Projection 防御）。
- en: 'The Empty Response Defense (Ouyang et al., [2022](#bib.bib43)). This defense
    employs the basic strategy of optimizing a model to output something *not containing*
    the sensitive information, which is the strategy behind using RLHF for preventing
    models from generating sensitive information. We simply optimize the probability
    of an “empty” target string  with the objective , using one of two target strings:
    “I don’t know” and “dummy”. The result is that, instead of generating the original
    knowledge, the model will instead indicate that it does not know the answer (“I
    don’t know” target) or give some meaningless response (“dummy” target). In our
    main experiments, we use the “dummy” target, which performs better than using
    “I don’t know” (see Appendix [B](#A2 "Appendix B Additional Experiments ‣ Can
    Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks")).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 空响应防御（Ouyang 等，[2022](#bib.bib43)）。这种防御方法采用了基本策略，通过优化模型使其输出*不包含*敏感信息，这就是使用 RLHF
    防止模型生成敏感信息的策略。我们简单地优化一个“空”目标字符串的概率，目标字符串有两个选择：“I don’t know”和“dummy”。结果是，模型将指示它不知道答案（“I
    don’t know”目标）或给出一些无意义的回应（“dummy”目标），而不是生成原始知识。在我们的主要实验中，我们使用“dummy”目标，它的表现优于使用“I
    don’t know”（见附录[B](#A2 "Appendix B Additional Experiments ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks")）。
- en: Fact Erasure (Hase et al., [2023](#bib.bib22)). Another simple approach to deleting
    a sensitive answer is to minimize its probability under the model, i.e. minimize  for
    the original fact .
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 事实抹除（Hase 等，[2023](#bib.bib22)）。另一种删除敏感答案的简单方法是最小化其在模型下的概率，即最小化原始事实的。
- en: Error Injection (De Cao et al., [2021](#bib.bib15)). A common test of model
    editing methods involves inducing counterfactual knowledge in the model. Here,
    we use the objective  where  is the alternative, false target provided by Meng
    et al. ([2022](#bib.bib36)). This method would not be applicable in practice,
    since we do not actually want LLMs to give *wrong* answers to sensitive questions,
    but we consider it here to show the efficacy of injecting new false information
    into the model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 错误注入（De Cao 等人，[2021](#bib.bib15)）。模型编辑方法的一个常见测试涉及在模型中引入反事实知识。在这里，我们使用目标，其中是
    Meng 等人（[2022](#bib.bib36)）提供的替代虚假目标。这种方法在实际中不可行，因为我们并不希望 LLMs 对敏感问题给出*错误*答案，但我们在此考虑它，以展示将新的虚假信息注入模型的有效性。
- en: 'Head Projection Defense. We introduce a objective that is directly designed
    to protect against the Head Projection attack. The goal is to prevent the deleted
    answer from appearing in the top- elements of the logit lens distributions across
    a set of layers , as well as the predicted distribution at the final layer (see
    Fig. [3](#S5.F3 "Figure 3 ‣ 5 Defense Methods ‣ Can Sensitive Information Be Deleted
    From LLMs? Objectives for Defending Against Extraction Attacks")). To do so, we
    introduce a max-margin loss in each relevant distribution. With  as the logit
    lens distribution at layer ,  as the original answer’s logit lens probability,
    and  as the -th top probability in , the objective becomes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 头部投影防御。我们引入了一个直接设计用于防御头部投影攻击的目标。目标是防止被删除的答案出现在一组层上的 logit lens 分布的前 - 个元素中，以及最终层的预测分布中（见图
    [3](#S5.F3 "图 3 ‣ 5 种防御方法 ‣ 能从 LLMs 中删除敏感信息吗？防御提取攻击的目标")）。为此，我们在每个相关分布中引入最大边际损失。设为第层的
    logit lens 分布， 为原始答案的 logit lens 概率， 为 的 - 个最高概率，目标变为：
- en: '|  |  |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: where  is the margin term. Since we do not face any constraint over the set
    of layers  to optimize, we tune over possible layer sets to improve the defense
    performance (details in Appendix [A](#A1 "Appendix A Tuning Details ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks")).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是边际项。由于我们在优化层集时没有任何约束，我们会在可能的层集中进行调优，以提高防御性能（详细信息见附录 [A](#A1 "附录 A 调优细节 ‣
    能从 LLMs 中删除敏感信息吗？防御提取攻击的目标")）。
- en: 'Max-Entropy Defense. This defense is similar to the Head Projection Defense,
    but it varies in terms of the objective for each layer. Here, we maximize the
    entropy of the model’s logit lens distributions over the next token at each layer:
    , where  is the probability of token  in the logit lens distribution of model  given
    the input prompt .'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵防御。此防御方法类似于头部投影防御，但在每层的目标方面有所不同。在这里，我们最大化模型在每层上对下一个令牌的 logit lens 分布的熵：，其中是给定输入提示的模型中令牌在
    logit lens 分布中的概率。
- en: Input Rephrasing Defense. This defense strategy aims to counter the Input Rephrasing
    blackbox attack described in Sec. [4](#S4 "4 Attack Methods ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"). In
    addition to using the input  for optimization, this approach adds model-generated
    paraphrases of  to the model editing objective. The rephrased inputs are created
    using the same off-the-shelf generation model as in the Input Rephrasing attack
    (Krishna et al., [2023](#bib.bib31)). In other words, for the -th datapoint, we
    concurrently delete the information for all prompts , where  represents the set
    of rephrases of  used for defense. We specifically optimize the Fact Erasure objective
    in parallel for each input .
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输入重述防御。该防御策略旨在对抗在第 [4](#S4 "4 攻击方法 ‣ 能从 LLMs 中删除敏感信息吗？防御提取攻击的目标") 节中描述的输入重述黑箱攻击。除了使用输入进行优化之外，该方法还将模型生成的重述添加到模型编辑目标中。这些重述输入是使用与输入重述攻击（Krishna
    等人，[2023](#bib.bib31)）中相同的现成生成模型创建的。换句话说，对于第 - 个数据点，我们同时删除所有提示中的信息，其中代表用于防御的 的重述集。我们专门优化每个输入的事实删除目标。
- en: 6 Experiment Setup
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 实验设置
- en: Models. We conduct experiments with GPT-J (Wang & Komatsuzaki, [2021](#bib.bib51)),
    Llama-2 (Touvron et al., [2023](#bib.bib50)), and GPT2-XL (Radford et al., [2019](#bib.bib45)).
    These models were chosen due to their (1) widespread usage, (2) public availability,
    and (3) capacity for memorizing their pretraining data (Carlini et al., [2023](#bib.bib12)).
    Results for Llama-2 and GPT2-XL are in Appendix [B](#A2 "Appendix B Additional
    Experiments ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks").
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。我们进行实验的模型包括 GPT-J (Wang & Komatsuzaki, [2021](#bib.bib51))、Llama-2 (Touvron
    et al., [2023](#bib.bib50)) 和 GPT2-XL (Radford et al., [2019](#bib.bib45))。选择这些模型是因为它们的（1）广泛使用，（2）公开可用，以及（3）记忆其预训练数据的能力
    (Carlini et al., [2023](#bib.bib12))。Llama-2 和 GPT2-XL 的结果见附录 [B](#A2 "Appendix
    B Additional Experiments ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")。
- en: Datasets. We use two datasets for evaluation, CounterFact (Meng et al., [2022](#bib.bib36))
    and zsRE (Levy et al., [2017](#bib.bib34)). CounterFact consists of prompts with
    factual completions, as well as neighboring datapoints that we use for computing
    Neighborhood -Acc. The zsRE dataset contains short question-answer pairs derived
    from Wikipedia. Both datasets include alternative, false targets for each input
    that may be used in model editing. To obtain data for computing Random -Acc, after
    each individual model edit we randomly sample 100 other random data points from
    the respective dataset. We filter the data to facts that are known by the model
    we attack, because it only makes sense to delete or extract facts that are already
    known by the model. We consider a fact known by the model when the answer string
    is in the model generation given the prompt; GPT-J gets 34% accuracy on CounterFact
    data with single-token answers and 25% on zsRE (we also filter to points with
    single-token answers). After sampling from these eligible facts, our final sample
    sizes are in 587 and 454 datapoints for CounterFact and zsRE respectively when
    using GPT-J.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们使用两个数据集进行评估，CounterFact (Meng et al., [2022](#bib.bib36)) 和 zsRE (Levy
    et al., [2017](#bib.bib34))。CounterFact 包含具有事实补全的提示，以及用于计算 Neighborhood -Acc 的邻近数据点。zsRE
    数据集包含从维基百科派生的短问答对。两个数据集都包括用于模型编辑的每个输入的替代虚假目标。为了获取计算 Random -Acc 的数据，在每次模型编辑后，我们从相应的数据集中随机抽取
    100 个其他随机数据点。我们筛选数据为模型已知的事实，因为只有删除或提取模型已知的事实才有意义。当模型生成的回答字符串出现在提示下时，我们认为事实已被模型知晓；GPT-J
    在 CounterFact 数据上的单标记答案准确率为 34%，在 zsRE 上为 25%（我们也筛选为单标记答案的数据点）。从这些符合条件的事实中抽样后，我们在使用
    GPT-J 时，最终样本量为 CounterFact 的 587 和 zsRE 的 454 个数据点。
- en: Though we are broadly interested in removing sensitive information from models,
    we use these two datasets because they provide a good testbed for deleting specific
    information from models. It is easy to verify whether or not a single-token answer
    to a factual question is contained within our candidate sets (as opposed to a
    more abstract description of some sensitive information).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们广泛关注从模型中删除敏感信息，但我们使用这两个数据集是因为它们提供了一个良好的测试平台，用于从模型中删除特定信息。验证一个对事实问题的单标记答案是否包含在我们的候选集中很容易（与对某些敏感信息的更抽象描述相比）。
- en: Model Editing Methods. We employ two popular model editing techniques in our
    experiments, ROME (Meng et al., [2022](#bib.bib36)) and MEMIT (Meng et al., [2023](#bib.bib37)).
    We refer the reader to these works for full details of the methods; we include
    short descriptions of the methods in Appendix [C](#A3 "Appendix C Editing Methods
    and Adversary Model Access ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks"). Both methods work by updating a specific
    weight matrix in the MLP layer(s) of a Transformer model. When applied to change
    a single fact in the model, the difference between them is that ROME updates a
    single layer’s MLP (layer 6 for GPT-J by default), while MEMIT updates multiple
    layers’ MLPs (we use layers 5-7). See Appendix [A](#A1 "Appendix A Tuning Details
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks") for other method hyperparameters. In Appendix [B](#A2 "Appendix
    B Additional Experiments ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks"), we conduct experiments with an additional
    editing method, constrained finetuning (Zhu et al., [2020](#bib.bib55)), but this
    method does not perform as well as ROME and MEMIT.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 模型编辑方法。我们在实验中使用了两种流行的模型编辑技术，ROME (Meng et al., [2022](#bib.bib36)) 和 MEMIT (Meng
    et al., [2023](#bib.bib37))。我们建议读者参考这些工作以获取方法的详细信息；我们在附录 [C](#A3 "附录 C 编辑方法和对手模型访问
    ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标") 中包含了方法的简短描述。这两种方法通过更新 Transformer 模型的 MLP 层的特定权重矩阵来工作。当应用于更改模型中的单个事实时，它们之间的区别在于
    ROME 更新单个层的 MLP（默认情况下为 GPT-J 的第 6 层），而 MEMIT 更新多个层的 MLP（我们使用第 5-7 层）。其他方法的超参数请参见附录
    [A](#A1 "附录 A 调整细节 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")。在附录 [B](#A2 "附录 B 额外实验 ‣ 能否从
    LLM 中删除敏感信息？防御提取攻击的目标") 中，我们使用了额外的编辑方法——受限微调 (Zhu et al., [2020](#bib.bib55))，但此方法的效果不如
    ROME 和 MEMIT。
- en: 7 Experiment Results
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 实验结果
- en: 7.1 Can We Extract a “Deleted” Answer From a Language Model?
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 我们能从语言模型中提取“删除”的答案吗？
- en: 'We first ask whether we can attack a model edited with the conventional Empty
    Response method. Design. We measure Attack-Success@ as a function of the budget  for
    our three attack methods: the (1) Head Projection, (2) Probability Delta, and
    (3) Input Rephrasing attacks. We perform these experiments with GPT-J and CounterFact
    data, applying the ROME editing method using the Empty Response objective. We
    confirm that the edit methods work as designed: the Rewrite Score is high for
    all methods (90%+), with low Random -Acc scores (<1%). In general, we increase
    the budget for whitebox attacks by increasing  and , and for our blackbox attack
    we increase the number of attack paraphrases and the number of samples. See Appendix
    [A](#A1 "Appendix A Tuning Details ‣ Can Sensitive Information Be Deleted From
    LLMs? Objectives for Defending Against Extraction Attacks") for exact hyperparameters.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先询问是否可以对使用传统 Empty Response 方法编辑的模型进行攻击。设计。我们将攻击成功率 @ 作为预算函数来测量我们三种攻击方法的效果：
    (1) 头部投影、 (2) 概率增量、 (3) 输入重述攻击。我们使用 GPT-J 和 CounterFact 数据进行这些实验，应用 ROME 编辑方法和
    Empty Response 目标。我们确认编辑方法按预期工作：所有方法的重写分数都很高（90%+），随机准确率低（<1%）。一般来说，我们通过增加预算来提高白盒攻击的效果，同时增加攻击重述的数量和样本数来提高黑盒攻击的效果。具体超参数请参见附录
    [A](#A1 "附录 A 调整细节 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")。
- en: '![Refer to caption](img/6a6ed1a5e900f7deaa86747efbafc192.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a6ed1a5e900f7deaa86747efbafc192.png)'
- en: 'Figure 4: Attack Success vs. the budget  for our three attack methods. We “delete”
    facts from GPT-J with ROME using the conventional Empty Response objective.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：我们三种攻击方法的攻击成功率与预算的关系。我们使用传统的 Empty Response 目标，通过 ROME 从 GPT-J 中“删除”事实。
- en: Results. From the results in Fig. [4](#S7.F4 "Figure 4 ‣ 7.1 Can We Extract
    a “Deleted” Answer From a Language Model? ‣ 7 Experiment Results ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks"), we see that attack success reaches values as high as 38% with a budget
    of . This extremely high attack success rate means that, under our threat model
    in Sec. [3](#S3 "3 Problem Statement ‣ Can Sensitive Information Be Deleted From
    LLMs? Objectives for Defending Against Extraction Attacks"), the edited model
    is highly vulnerable to extraction attacks for the “deleted” fact.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。从图[4](#S7.F4 "图4 ‣ 7.1 我们能从语言模型中提取“已删除”答案吗？ ‣ 7 实验结果 ‣ 能否从LLMs中删除敏感信息？防御提取攻击的目标")中的结果来看，攻击成功率高达38%，预算为。这种极高的攻击成功率意味着，在第[3](#S3
    "3 问题陈述 ‣ 能否从LLMs中删除敏感信息？防御提取攻击的目标")节中的威胁模型下，经过编辑的模型对“已删除”事实的提取攻击高度脆弱。
- en: Besides the highest attack success rate obtained with whitebox attacks and a
    budget of , we note that the blackbox attack also achieves a high success rate
    of up to 29%. Additionally, even with a budget of , an attacker would succeed
    18% of the time using the Probability Delta attack. Interestingly, all methods
    appear to saturate in performance after  candidates.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在白盒攻击和预算为的情况下获得的最高攻击成功率外，我们注意到黑盒攻击也达到了高达29%的成功率。此外，即使在预算为的情况下，攻击者使用概率差异攻击成功的几率也为18%。有趣的是，所有方法在候选项达到后似乎表现饱和。
- en: 7.2 How to Defend Against Information Extraction Attacks
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 如何防御信息提取攻击
- en: Next, we show how the proposed defense methods (Sec. [5](#S5 "5 Defense Methods
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks")) fare against our extraction attacks (Sec. [4](#S4 "4 Attack
    Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks")).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了所提防御方法（第[5](#S5 "5 防御方法 ‣ 能否从LLMs中删除敏感信息？防御提取攻击的目标")节）在面对我们的提取攻击（第[4](#S4
    "4 攻击方法 ‣ 能否从LLMs中删除敏感信息？防御提取攻击的目标")节）时的表现。
- en: Design. We evaluate the three baseline methods (Fact Erasure, Empty Resp, and
    Error Inj) and our three proposed methods (HP Def, Max-Ent Def, and IR Def). We
    report Attack-Success@ with  using each of our three attack methods, as well as
    Random -Acc and Neighborhood -Acc metrics to show how much damage is being done
    to the model’s overall knowledge by the deletion of individual facts. We show
    results for GPT-J on CounterFact and zsRE with ROME and MEMIT. As before, we confirm
    that edit methods work as designed for both datasets, as the Rewrite Score and
    -Acc scores are comparable to past work (Mitchell et al., [2021](#bib.bib39);
    Hase et al., [2023](#bib.bib22)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设计。我们评估了三种基线方法（事实删除、空响应和错误注入）以及我们提出的三种方法（HP防御、最大熵防御和IR防御）。我们报告了使用我们三种攻击方法中的每一种的攻击成功率@，以及随机-准确率和邻域-准确率指标，以显示删除单个事实对模型整体知识造成的损害程度。我们展示了GPT-J在CounterFact和zsRE上与ROME和MEMIT的结果。如之前所述，我们确认编辑方法在这两个数据集上按预期工作，因为重写评分和-准确率得分与过去的工作（Mitchell等，[2021](#bib.bib39)；Hase等，[2023](#bib.bib22)）相当。
- en: 'Results. We show the results in Table [1](#S7.T1 "Table 1 ‣ 7.2 How to Defend
    Against Information Extraction Attacks ‣ 7 Experiment Results ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks"). We summarize the main conclusions as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们在表[1](#S7.T1 "表1 ‣ 7.2 如何防御信息提取攻击 ‣ 7 实验结果 ‣ 能否从LLMs中删除敏感信息？防御提取攻击的目标")中展示了结果。我们总结了主要结论如下：
- en: '1.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Overall, our whitebox and blackbox attacks are all frequently successful at
    extracting “deleted” facts. We emphasize that MEMIT with the Empty Response defense
    is successfully attacked by our Head Projection attack 89% of the time on zsRE
    with . MEMIT is a popular editing method, and the Empty Response objective is
    the standard approach to preventing models from generating sensitive information,
    yet this setting totally fails to fully delete facts from GPT-J.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总体而言，我们的白盒和黑盒攻击都经常成功提取“已删除”的事实。我们强调，使用“空响应”防御的MEMIT在zsRE上被我们的头部投影攻击成功攻击的比例为89%。MEMIT是一种流行的编辑方法，而空响应目标是防止模型生成敏感信息的标准方法，但在这种设置下，完全删除GPT-J中的事实完全失败。
- en: '2.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Our Head Projection and Max-Entropy defenses are the strongest defenses against
    whitebox attacks. Relative to the strongest baseline, Fact Erasure, the Max-Entropy
    defense lowers the Head Projection attack success by 20.5 points on CounterFact
    (22.2%1.7% with ROME) and 39.5 points on zsRE (41.4%2.9% with ROME). The Head
    Projection defense, though helpful, is surprisingly not as effective as the Max-Entropy
    defense, except for when used with MEMIT on CounterFact. Note we discuss results
    for the Probability Delta attack below in Sec. [7.3](#S7.SS3 "7.3 Can We Defend
    Against Unforeseen Extraction Attacks? ‣ 7 Experiment Results ‣ Can Sensitive
    Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks").
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的头部投影和最大熵防御是对抗白箱攻击最强的防御。与最强基线事实抹除相比，最大熵防御将头部投影攻击成功率降低了 CounterFact 上的 20.5
    个百分点（22.2%1.7% 与 ROME），以及 zsRE 上的 39.5 个百分点（41.4%2.9% 与 ROME）。头部投影防御虽然有帮助，但意外地不如最大熵防御有效，除非在
    CounterFact 上与 MEMIT 结合使用。注意，我们在第 [7.3](#S7.SS3 "7.3 我们能否对抗未预见的提取攻击？ ‣ 7 实验结果
    ‣ 能否从 LLMs 中删除敏感信息？对抗提取攻击的目标") 节中讨论概率差异攻击的结果。
- en: '3.'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The Input Rephrasing defense does *not* reduce the blackbox attack success.
    Across datasets and editing methods, the IR Def never outperforms baselines and
    is sometimes the worst objective against the Input Rephrasing attack. We confirm
    that the defense does work when the attack uses the *exact same* paraphrased inputs
    supplied to the defense’s editing objective; the defense fails when attacking
    paraphrases differ at all. Additionally, we can lower the Input Rephrasing attack
    success by making the model edits more aggressive, but this has the consequence
    of skyrocketing -Acc numbers (4.7 for Random data and 27.8 for Neighborhood data;
    see Appendix [B](#A2 "Appendix B Additional Experiments ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks")).
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入释义防御*未*减少黑箱攻击成功率。在数据集和编辑方法中，IR 防御从未超越基线，有时是针对输入释义攻击的最差目标。我们确认，当攻击使用*完全相同*的释义输入时防御有效；当攻击的释义有任何差异时，防御会失败。此外，我们可以通过使模型编辑更加激进来降低输入释义攻击成功率，但这会导致
    -Acc 数字飞涨（随机数据为 4.7，邻居数据为 27.8；见附录 [B](#A2 "附录 B 额外实验 ‣ 能否从 LLMs 中删除敏感信息？对抗提取攻击的目标")）。
- en: '4.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Attack success against MEMIT is generally higher than against ROME, and our
    best defense methods (like Max-Ent) achieve smaller improvements over baselines
    with MEMIT than with ROME. This could suggest that distributing editing updates
    across multiple layers rather than a single layer, as MEMIT does, increases vulnerability
    to attacks and makes defense more challenging. However, MEMIT performs more favorably
    than ROME on the Rewrite Score and -Acc metrics, meaning this difference in the
    methods could be due to a difference in how strongly the edits applied to the
    model.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击成功率在 MEMIT 对抗 ROME 时通常更高，我们的最佳防御方法（如 Max-Ent）在 MEMIT 上相较于 ROME 取得的改进更小。这可能表明，将编辑更新分布到多个层而非单层（如
    MEMIT 所做）会增加攻击的脆弱性，使得防御更具挑战。然而，MEMIT 在重写分数和 -Acc 指标上的表现优于 ROME，这意味着方法之间的差异可能由于应用于模型的编辑强度不同。
- en: '|  | Attack-Success@ | -Acc |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击成功率@ | -Acc |  |'
- en: '| placeholder Defense | Head     Projection | Probability     Delta | Input
           Rephrasing | placeholder Random | placeholder Neighbors | Rewrite     Score
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 占位符 防御 | 头部     投影 | 概率     差异 | 输入        释义 | 占位符 随机 | 占位符 邻居 | 重写    
    分数 |'
- en: '| CounterFact |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CounterFact |'
- en: '| ROME |  |  |  |  |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ROME |  |  |  |  |  |  |'
- en: '| + Fact Erasure | 22.15 | 25.38 | 22.83 | 0.72 | 8.74 | 99.69 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| + 事实抹除 | 22.15 | 25.38 | 22.83 | 0.72 | 8.74 | 99.69 |'
- en: '| + Empty Resp | 36.84 | 37.65 | 29.02 | 0.54 | 3.76 | 99.58 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应 | 36.84 | 37.65 | 29.02 | 0.54 | 3.76 | 99.58 |'
- en: '| + Error Inj | 99.20 | 99.83 | 20.10 | 1.00 | 9.60 | 99.30 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| + 错误注入 | 99.20 | 99.83 | 20.10 | 1.00 | 9.60 | 99.30 |'
- en: '| + HP Def | 4.43 | 20.27 | 27.77 | 0.69 | 6.35 | 99.73 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| + 头部防御 | 4.43 | 20.27 | 27.77 | 0.69 | 6.35 | 99.73 |'
- en: '| + Max-Ent Def | 1.70 | 2.39 | 27.94 | 0.69 | 6.27 | 99.73 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| + 最大熵防御 | 1.70 | 2.39 | 27.94 | 0.69 | 6.27 | 99.73 |'
- en: '| + IR Def | 56.39 | 60.65 | 29.30 | 0.69 | 6.17 | 98.88 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| + IR 防御 | 56.39 | 60.65 | 29.30 | 0.69 | 6.17 | 98.88 |'
- en: '| MEMIT |  |  |  |  |  |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| MEMIT |  |  |  |  |  |  |'
- en: '| + Fact Erasure | 39.18 | 46.17 | 34.07 | 0.26 | 3.29 | 98.68 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| + 事实抹除 | 39.18 | 46.17 | 34.07 | 0.26 | 3.29 | 98.68 |'
- en: '| + Empty Resp | 67.09 | 72.60 | 49.31 | 0.22 | 1.03 | 87.54 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应 | 67.09 | 72.60 | 49.31 | 0.22 | 1.03 | 87.54 |'
- en: '| + Error Inj | 98.60 | 98.80 | 36.38 | 0.15 | 2.05 | 97.68 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| + 错误注入 | 98.60 | 98.80 | 36.38 | 0.15 | 2.05 | 97.68 |'
- en: '| + HP Def | 19.42 | 38.33 | 42.76 | 0.20 | 3.37 | 97.09 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| + HP 防御 | 19.42 | 38.33 | 42.76 | 0.20 | 3.37 | 97.09 |'
- en: '| + Max-Ent Def | 34.24 | 39.01 | 50.77 | 0.19 | 3.32 | 96.41 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| + 最大熵防御 | 34.24 | 39.01 | 50.77 | 0.19 | 3.32 | 96.41 |'
- en: '| + IR Def | 56.03 | 61.50 | 41.91 | 0.20 | 3.49 | 91.56 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| + IR 防御 | 56.03 | 61.50 | 41.91 | 0.20 | 3.49 | 91.56 |'
- en: '| zsRE |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| zsRE |'
- en: '| ROME |  |  |  |  |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| ROME |  |  |  |  |  |  |'
- en: '| + Fact Erasure | 41.41 | 43.83 | 15.64 | 0.10 | - | 94.80 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| + 事实删除 | 41.41 | 43.83 | 15.64 | 0.10 | - | 94.80 |'
- en: '| + Empty Resp | 36.83 | 59.13 | 13.00 | 0.08 | - | 99.78 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应 | 36.83 | 59.13 | 13.00 | 0.08 | - | 99.78 |'
- en: '| + Error Inj | 20.68 | 45.07 | 10.35 | 0.13 | - | 99.40 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| + 错误注入 | 20.68 | 45.07 | 10.35 | 0.13 | - | 99.40 |'
- en: '| + HP Def | 31.28 | 56.83 | 18.50 | 0.12 | - | 90.53 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| + HP 防御 | 31.28 | 56.83 | 18.50 | 0.12 | - | 90.53 |'
- en: '| + Max-Ent Def | 2.86 | 2.42 | 18.50 | 0.12 | - | 90.71 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| + 最大熵防御 | 2.86 | 2.42 | 18.50 | 0.12 | - | 90.71 |'
- en: '| + IR Def | 84.80 | 84.80 | 29.07 | 0.07 | - | 80.64 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| + IR 防御 | 84.80 | 84.80 | 29.07 | 0.07 | - | 80.64 |'
- en: '| MEMIT |  |  |  |  |  |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MEMIT |  |  |  |  |  |  |'
- en: '| + Fact Erasure | 42.51 | 42.07 | 22.18 | 0.05 | - | 91.34 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| + 事实删除 | 42.51 | 42.07 | 22.18 | 0.05 | - | 91.34 |'
- en: '| + Empty Resp | 88.55 | 88.11 | 29.30 | 0.05 | - | 91.34 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应 | 88.55 | 88.11 | 29.30 | 0.05 | - | 91.34 |'
- en: '| + Error Inj | 89.65 | 80.64 | 32.60 | 0.11 | - | 85.86 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| + 错误注入 | 89.65 | 80.64 | 32.60 | 0.11 | - | 85.86 |'
- en: '| + HP Def | 53.52 | 72.47 | 28.19 | 0.05 | - | 89.46 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| + HP 防御 | 53.52 | 72.47 | 28.19 | 0.05 | - | 89.46 |'
- en: '| + Max-Ent Def | 39.92 | 38.11 | 29.74 | 0.07 | - | 91.24 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| + 最大熵防御 | 39.92 | 38.11 | 29.74 | 0.07 | - | 91.24 |'
- en: '| + IR Def | 46.04 | 57.93 | 27.75 | 0.07 | - | 84.16 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| + IR 防御 | 46.04 | 57.93 | 27.75 | 0.07 | - | 84.16 |'
- en: 'Table 1: Attack success rates of the the three proposed attacks (Sec. [4](#S4
    "4 Attack Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")) across defense methods (Sec. [5](#S5
    "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")), for facts from the CounterFact and
    zsRE datasets that are known by GPT-J and deleted using ROME or MEMIT (with its
    objective determined by the defense method).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：三种提议攻击的成功率（第[4](#S4 "4 攻击方法 ‣ 敏感信息能从 LLM 中删除吗？防御提取攻击的目标")节）跨防御方法（第[5](#S5
    "5 防御方法 ‣ 敏感信息能从 LLM 中删除吗？防御提取攻击的目标")节），对于 GPT-J 已知并使用 ROME 或 MEMIT 删除的 CounterFact
    和 zsRE 数据集中的事实（其目标由防御方法确定）。
- en: 7.3 Can We Defend Against Unforeseen Extraction Attacks?
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 我们能防御未知的提取攻击吗？
- en: Lastly, we examine the efficacy of extraction attacks that the defense methods
    are not directly designed to prevent, an important “unforeseen” scenario for our
    defense methods (Carlini et al., [2019a](#bib.bib9)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们考察了防御方法未直接设计来防止的提取攻击的有效性，这是一个对我们防御方法的重要“意外”场景（Carlini 等， [2019a](#bib.bib9)）。
- en: Design. We highlight results from our previous experiment, specifically the
    performance of the Probability Delta Attack applied against our two whitebox defenses,
    Max-Entropy and Head Projection. In this setting, we use defenses that were designed
    to protect against the Head Projection attack but *were not designed to defend
    against our second whitebox attack*, the Probability Delta attack.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设计。我们重点突出之前实验的结果，特别是应用于我们两个白盒防御 Max-Entropy 和 Head Projection 的概率增量攻击的表现。在这种设置下，我们使用了设计用来防御
    Head Projection 攻击的防御措施，但*并未设计用来防御我们的第二个白盒攻击*，即概率增量攻击。
- en: 'Results. We draw a few conclusions from Table [1](#S7.T1 "Table 1 ‣ 7.2 How
    to Defend Against Information Extraction Attacks ‣ 7 Experiment Results ‣ Can
    Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks"): (1) The “unforeseen” Probability Delta attack is very effective against
    the Head Projection defense, which was not prepared for it. (2) Our Max-Entropy
    defense often helps against the Probability Delta attack despite not being specially
    designed for it. Compared to the Head Projection defense on zsRE, Max-Entropy
    defense substantially lowers attack success rates (56.8%2.4% with ROME and 72.5%38.1%
    with MEMIT). However, while the Max-Entropy defense can lower whitebox attack
    success to 2.4%, (3) the blackbox attack success against it remains quite high
    at 28% for CounterFact and 19% for zsRE, suggesting that the defense is still
    inadequate against blackbox attacks. In total, we see that there is no single
    defense method that is prepared against all attacks it could face, even if it
    is effective against some unforeseen attacks.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们从表[1](#S7.T1 "Table 1 ‣ 7.2 How to Defend Against Information Extraction
    Attacks ‣ 7 Experiment Results ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")中得出了一些结论：（1）“意外”概率增量攻击对头部投影防御非常有效，该防御未对此作出准备。（2）我们的最大熵防御在面对概率增量攻击时通常有效，尽管它并不是专门为此设计的。与zsRE上的头部投影防御相比，最大熵防御显著降低了攻击成功率（使用ROME时为56.8%2.4%，使用MEMIT时为72.5%38.1%）。然而，虽然最大熵防御可以将白盒攻击的成功率降到2.4%，（3）对其进行的黑盒攻击成功率仍然较高，CounterFact为28%，zsRE为19%，这表明该防御对黑盒攻击仍然不够充分。总体来看，我们发现没有单一的防御方法能对抗所有可能面临的攻击，即使它对某些意外攻击有效。
- en: 8 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: We first argue that model editing methods are the most promising approach to
    deleting sensitive information from LLMs, rather than interventions focusing on
    pretraining and finetuning data. Even for this promising class of methods, however,
    we show that “deleted” information can be extracted a surprisingly high percentage
    of the time (as high as 89% in some experiments) when the attacker operates with
    a small budget of verification attempts . We motivate this budget via a threat
    model based on three plausible adversarial settings. Our findings suggest that
    truly deleting sensitive inormation is a tractable but difficult problem, with
    potentially severe implications for deployment of LLMs in a world where individuals
    enjoy a robust right to privacy and safety from harmful model outputs.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先认为，模型编辑方法是删除LLMs中敏感信息的最有前景的方法，而不是专注于预训练和微调数据的干预方法。然而，即便是这种有前景的方法，我们也展示了“删除”的信息在攻击者以小预算进行验证尝试时可以被提取出来的比例非常高（在某些实验中高达89%）。我们通过基于三种合理的对抗环境的威胁模型来激发这一预算。我们的发现表明，真正删除敏感信息是一个可行但困难的问题，在个人享有强大隐私权和免受有害模型输出的安全保护的世界中，具有潜在的严重影响。
- en: Ethics Statement
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This paper addresses problems involving sensitive information in large language
    models. This is an important topic with serious ethical implications, as language
    models currently possess knowledge that could be dangerous to humans and output
    directly harmful text. We hope that the technical methods in this paper can help
    mitigate these important ethical problems, but at the same time, we want to demonstrate
    that it may be fundamentally difficult to solve the problem of sensitive information
    in pretrained language models. These results could imply that there are negative
    moral and legal consequences to deploying LLMs in situations where they may influence
    humans. We leave it for future work in AI, ethics, and law to fully explore the
    implications of work on sensitive information deletion and LLMs.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 本文讨论了大语言模型中涉及敏感信息的问题。这是一个重要的话题，具有严重的伦理影响，因为语言模型目前拥有可能对人类造成危险的知识，并直接输出有害的文本。我们希望本文中的技术方法能帮助缓解这些重要的伦理问题，但与此同时，我们也想表明，解决预训练语言模型中的敏感信息问题可能从根本上说是困难的。这些结果可能暗示在某些情况下部署LLMs会带来负面的道德和法律后果。我们将这一问题留待未来在人工智能、伦理学和法律领域中进一步探讨，以充分了解关于敏感信息删除和LLMs的工作含义。
- en: Acknowledgements
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Neel Nanda for helpful experiment suggestions. This work was supported
    by NSF-CAREER Award 1846185, NSF-AI Engage Institute DRL-2112635, DARPA MCS Grant
    N66001-19-2-4031, and Google PhD fellowship. The views contained in this article
    are those of the authors and not of the funding agency.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Neel Nanda 对实验建议的帮助。这项工作得到了 NSF-CAREER 奖项 1846185，NSF-AI Engage Institute
    DRL-2112635，DARPA MCS 资助 N66001-19-2-4031 和 Google 博士生奖学金的支持。本文中的观点仅代表作者，不代表资助机构。
- en: References
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Andreas (2022) Jacob Andreas. Language models as agent models. *arXiv preprint
    arXiv:2212.01681*, 2022. URL [https://arxiv.org/pdf/2212.01681.pdf](https://arxiv.org/pdf/2212.01681.pdf).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andreas (2022) Jacob Andreas. 语言模型作为代理模型。*arXiv 预印本 arXiv:2212.01681*，2022年。网址
    [https://arxiv.org/pdf/2212.01681.pdf](https://arxiv.org/pdf/2212.01681.pdf)。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. *arXiv preprint
    arXiv:2212.08073*, 2022. URL [https://arxiv.org/pdf/2212.08073.pdf](https://arxiv.org/pdf/2212.08073.pdf).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon 等. 宪法 AI: 来自 AI 反馈的无害性。*arXiv 预印本 arXiv:2212.08073*，2022年。网址 [https://arxiv.org/pdf/2212.08073.pdf](https://arxiv.org/pdf/2212.08073.pdf)。'
- en: 'Belinkov (2022) Yonatan Belinkov. Probing classifiers: Promises, shortcomings,
    and advances. *Computational Linguistics*, 48(1):207–219, 2022. URL [https://arxiv.org/pdf/2102.12452.pdf](https://arxiv.org/pdf/2102.12452.pdf).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belinkov (2022) Yonatan Belinkov. 探测分类器：承诺、缺陷与进展。*计算语言学*，48(1):207–219，2022年。网址
    [https://arxiv.org/pdf/2102.12452.pdf](https://arxiv.org/pdf/2102.12452.pdf)。
- en: Belrose et al. (2023a) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
    Igor Ostrovsky, Lev McKinney, Stella Biderman, and Jacob Steinhardt. Eliciting
    latent predictions from transformers with the tuned lens. *arXiv preprint arXiv:2303.08112*,
    2023a. URL [https://arxiv.org/pdf/2303.08112.pdf](https://arxiv.org/pdf/2303.08112.pdf).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belrose et al. (2023a) Nora Belrose, Zach Furman, Logan Smith, Danny Halawi,
    Igor Ostrovsky, Lev McKinney, Stella Biderman 和 Jacob Steinhardt. 通过调节镜头从变换器中引出潜在预测。*arXiv
    预印本 arXiv:2303.08112*，2023a。网址 [https://arxiv.org/pdf/2303.08112.pdf](https://arxiv.org/pdf/2303.08112.pdf)。
- en: 'Belrose et al. (2023b) Nora Belrose, David Schneider-Joseph, Shauli Ravfogel,
    Ryan Cotterell, Edward Raff, and Stella Biderman. Leace: Perfect linear concept
    erasure in closed form. *arXiv preprint arXiv:2306.03819*, 2023b. URL [https://arxiv.org/pdf/2306.03819.pdf](https://arxiv.org/pdf/2306.03819.pdf).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Belrose et al. (2023b) Nora Belrose, David Schneider-Joseph, Shauli Ravfogel,
    Ryan Cotterell, Edward Raff 和 Stella Biderman. Leace: 完美线性概念擦除的闭式解。*arXiv 预印本
    arXiv:2306.03819*，2023b。网址 [https://arxiv.org/pdf/2306.03819.pdf](https://arxiv.org/pdf/2306.03819.pdf)。'
- en: Brown et al. (2022) Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah,
    Reza Shokri, and Florian Tramèr. What does it mean for a language model to preserve
    privacy?, 2022. URL [https://arxiv.org/pdf/2202.05520.pdf](https://arxiv.org/pdf/2202.05520.pdf).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2022) Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah,
    Reza Shokri 和 Florian Tramèr. 语言模型如何保持隐私？2022年。网址 [https://arxiv.org/pdf/2202.05520.pdf](https://arxiv.org/pdf/2202.05520.pdf)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020. URL [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。网址 [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)。
- en: Cao & Yang (2015) Yinzhi Cao and Junfeng Yang. Towards making systems forget
    with machine unlearning. In *2015 IEEE symposium on security and privacy*, pp. 463–480\.
    IEEE, 2015. URL [https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao & Yang (2015) Yinzhi Cao 和 Junfeng Yang. 使系统通过机器遗忘变得更加健全。*2015 IEEE 安全与隐私研讨会*，第463–480页。IEEE，2015年。网址
    [https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf](https://www.ieee-security.org/TC/SP2015/papers-archived/6949a463.pdf)。
- en: Carlini et al. (2019a) Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland
    Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and
    Alexey Kurakin. On evaluating adversarial robustness. *arXiv preprint arXiv:1902.06705*,
    2019a. URL [https://arxiv.org/pdf/1902.06705.pdf](https://arxiv.org/pdf/1902.06705.pdf).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2019a) Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland
    Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, 和 Alexey
    Kurakin. 关于评估对抗鲁棒性。*arXiv 预印本 arXiv:1902.06705*，2019a。网址 [https://arxiv.org/pdf/1902.06705.pdf](https://arxiv.org/pdf/1902.06705.pdf)。
- en: 'Carlini et al. (2019b) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, and Dawn Song. The secr et sharer: Evaluating and testing unintended memorization
    in neural networks. In *USENIX Security Symposium*, 2019b. URL [https://arxiv.org/pdf/1802.08232.pdf](https://arxiv.org/pdf/1802.08232.pdf).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carlini et al. (2019b) Nicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej
    Kos, 和 Dawn Song. The secr et sharer: 评估和测试神经网络中的非预期记忆。见于 *USENIX Security Symposium*，2019b。网址
    [https://arxiv.org/pdf/1802.08232.pdf](https://arxiv.org/pdf/1802.08232.pdf)。'
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn
    Song, Ulfar Erlingsson, et al. Extracting training data from large language models.
    In *USENIX Security Symposium*, volume 6, 2021. URL [https://arxiv.org/pdf/2012.07805.pdf](https://arxiv.org/pdf/2012.07805.pdf).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn
    Song, Ulfar Erlingsson, 等. 从大型语言模型中提取训练数据。见于 *USENIX Security Symposium*，第 6 卷，2021年。网址
    [https://arxiv.org/pdf/2012.07805.pdf](https://arxiv.org/pdf/2012.07805.pdf)。
- en: Carlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across
    neural language models. In *The Eleventh International Conference on Learning
    Representations*, 2023. URL [https://openreview.net/forum?id=TatRHT_1cK](https://openreview.net/forum?id=TatRHT_1cK).
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini et al. (2023) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, 和 Chiyuan Zhang. 量化神经语言模型中的记忆。见于 *第十一届国际学习表征会议*，2023年。网址
    [https://openreview.net/forum?id=TatRHT_1cK](https://openreview.net/forum?id=TatRHT_1cK)。
- en: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement
    learning from human feedback. *arXiv preprint arXiv:2307.15217*, 2023. URL [https://arxiv.org/pdf/2307.15217.pdf](https://arxiv.org/pdf/2307.15217.pdf).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, 等. 强化学习中的开放问题和基本局限。*arXiv 预印本 arXiv:2307.15217*，2023年。网址
    [https://arxiv.org/pdf/2307.15217.pdf](https://arxiv.org/pdf/2307.15217.pdf)。
- en: Dai et al. (2022) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu Wei. Knowledge
    neurons in pretrained transformers. In *ACL*, 2022. URL [https://arxiv.org/pdf/2104.08696.pdf](https://arxiv.org/pdf/2104.08696.pdf).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai et al. (2022) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, 和 Furu Wei. 预训练变换器中的知识神经元。见于
    *ACL*，2022年。网址 [https://arxiv.org/pdf/2104.08696.pdf](https://arxiv.org/pdf/2104.08696.pdf)。
- en: De Cao et al. (2021) Nicola De Cao, Wilker Aziz, and Ivan Titov. Editing factual
    knowledge in language models. In *EMNLP*, pp.  6491–6506\. Association for Computational
    Linguistics, November 2021. URL [https://aclanthology.org/2021.emnlp-main.522](https://aclanthology.org/2021.emnlp-main.522).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Cao et al. (2021) Nicola De Cao, Wilker Aziz, 和 Ivan Titov. 在语言模型中编辑事实知识。见于
    *EMNLP*，第 6491–6506 页。计算语言学协会，2021年11月。网址 [https://aclanthology.org/2021.emnlp-main.522](https://aclanthology.org/2021.emnlp-main.522)。
- en: Debenedetti et al. (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, and
    Florian Tramèr. Privacy side channels in machine learning systems, 2023. URL [https://arxiv.org/pdf/2309.05610.pdf](https://arxiv.org/pdf/2309.05610.pdf).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debenedetti et al. (2023) Edoardo Debenedetti, Giorgio Severi, Nicholas Carlini,
    Christopher A. Choquette-Choo, Matthew Jagielski, Milad Nasr, Eric Wallace, 和
    Florian Tramèr. 机器学习系统中的隐私侧通道，2023年。网址 [https://arxiv.org/pdf/2309.05610.pdf](https://arxiv.org/pdf/2309.05610.pdf)。
- en: Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith.
    Calibrating noise to sensitivity in private data analysis. In *Theory of cryptography
    conference*, pp.  265–284. Springer, 2006.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwork et al. (2006) Cynthia Dwork, Frank McSherry, Kobbi Nissim, 和 Adam Smith.
    在隐私数据分析中根据灵敏度校准噪声。见于 *密码学理论会议*，第 265–284 页。施普林格，2006年。
- en: Gandikota et al. (2023) Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman,
    and David Bau. Erasing concepts from diffusion models. *arXiv preprint arXiv:2303.07345*,
    2023. URL [https://arxiv.org/pdf/2303.07345.pdf](https://arxiv.org/pdf/2303.07345.pdf).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gandikota等（2023）Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, 和
    David Bau。从扩散模型中抹除概念。*arXiv预印本 arXiv:2303.07345*，2023年。网址 [https://arxiv.org/pdf/2303.07345.pdf](https://arxiv.org/pdf/2303.07345.pdf)。
- en: Geva et al. (2021) Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy.
    Transformer feed-forward layers are key-value memories. In *EMNLP*, 2021. URL
    [https://arxiv.org/pdf/2012.14913.pdf](https://arxiv.org/pdf/2012.14913.pdf).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geva等（2021）Mor Geva, Roei Schuster, Jonathan Berant, 和 Omer Levy。Transformer前馈层是键值记忆。在*EMNLP*，2021年。网址
    [https://arxiv.org/pdf/2012.14913.pdf](https://arxiv.org/pdf/2012.14913.pdf)。
- en: Guo et al. (2019) Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten.
    Certified data removal from machine learning models. *arXiv preprint arXiv:1911.03030*,
    2019. URL [https://arxiv.org/pdf/1911.03030.pdf](https://arxiv.org/pdf/1911.03030.pdf).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2019）Chuan Guo, Tom Goldstein, Awni Hannun, 和 Laurens Van Der Maaten。从机器学习模型中认证数据移除。*arXiv预印本
    arXiv:1911.03030*，2019年。网址 [https://arxiv.org/pdf/1911.03030.pdf](https://arxiv.org/pdf/1911.03030.pdf)。
- en: Hase et al. (2021) Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa
    Kozareva, Veselin Stoyanov, Mohit Bansal, and Srinivasan Iyer. Do language models
    have beliefs? methods for detecting, updating, and visualizing model beliefs.
    *arXiv preprint arXiv:2111.13654*, 2021. URL [https://arxiv.org/pdf/2111.13654.pdf](https://arxiv.org/pdf/2111.13654.pdf).
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hase等（2021）Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva,
    Veselin Stoyanov, Mohit Bansal, 和 Srinivasan Iyer。语言模型是否有信念？检测、更新和可视化模型信念的方法。*arXiv预印本
    arXiv:2111.13654*，2021年。网址 [https://arxiv.org/pdf/2111.13654.pdf](https://arxiv.org/pdf/2111.13654.pdf)。
- en: Hase et al. (2023) Peter Hase, Mohit Bansal, Been Kim, and Asma Ghandeharioun.
    Does localization inform editing? surprising differences in causality-based localization
    vs. knowledge editing in language models. *arXiv preprint arXiv:2301.04213*, 2023.
    URL [https://arxiv.org/pdf/2301.04213.pdf](https://arxiv.org/pdf/2301.04213.pdf).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hase等（2023）Peter Hase, Mohit Bansal, Been Kim, 和 Asma Ghandeharioun。定位是否有助于编辑？基于因果关系的定位与语言模型中的知识编辑的惊人差异。*arXiv预印本
    arXiv:2301.04213*，2023年。网址 [https://arxiv.org/pdf/2301.04213.pdf](https://arxiv.org/pdf/2301.04213.pdf)。
- en: Henderson et al. (2018) Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier,
    Nan Rosemary Ke, Genevieve Fried, Ryan Lowe, and Joelle Pineau. Ethical challenges
    in data-driven dialogue systems. In *Proceedings of the 2018 AAAI/ACM Conference
    on AI, Ethics, and Society*, pp.  123–129, 2018. URL [https://arxiv.org/pdf/1711.09050.pdf](https://arxiv.org/pdf/1711.09050.pdf).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson等（2018）Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan
    Rosemary Ke, Genevieve Fried, Ryan Lowe, 和 Joelle Pineau。数据驱动对话系统中的伦理挑战。发表于*2018
    AAAI/ACM Conference on AI, Ethics, and Society*，第123–129页，2018年。网址 [https://arxiv.org/pdf/1711.09050.pdf](https://arxiv.org/pdf/1711.09050.pdf)。
- en: Henderson et al. (2023) Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori
    Hashimoto, Mark A. Lemley, and Percy Liang. Foundation models and fair use, 2023.
    URL [https://arxiv.org/pdf/2303.15715.pdf](https://arxiv.org/pdf/2303.15715.pdf).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henderson等（2023）Peter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto,
    Mark A. Lemley, 和 Percy Liang。基础模型与公平使用，2023年。网址 [https://arxiv.org/pdf/2303.15715.pdf](https://arxiv.org/pdf/2303.15715.pdf)。
- en: 'Heng & Soh (2023) Alvin Heng and Harold Soh. Selective amnesia: A continual
    learning approach to forgetting in deep generative models. *arXiv preprint arXiv:2305.10120*,
    2023. URL [https://arxiv.org/pdf/2305.10120.pdf](https://arxiv.org/pdf/2305.10120.pdf).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heng & Soh（2023）Alvin Heng 和 Harold Soh。选择性健忘：一种在深度生成模型中遗忘的持续学习方法。*arXiv预印本
    arXiv:2305.10120*，2023年。网址 [https://arxiv.org/pdf/2305.10120.pdf](https://arxiv.org/pdf/2305.10120.pdf)。
- en: Hernandez et al. (2023) Evan Hernandez, Belinda Z Li, and Jacob Andreas. Measuring
    and manipulating knowledge representations in language models. *arXiv preprint
    arXiv:2304.00740*, 2023. URL [https://arxiv.org/pdf/2304.00740.pdf](https://arxiv.org/pdf/2304.00740.pdf).
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hernandez等（2023）Evan Hernandez, Belinda Z Li, 和 Jacob Andreas。测量和操控语言模型中的知识表示。*arXiv预印本
    arXiv:2304.00740*，2023年。网址 [https://arxiv.org/pdf/2304.00740.pdf](https://arxiv.org/pdf/2304.00740.pdf)。
- en: Ilharco et al. (2023) Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman,
    Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing
    models with task arithmetic, 2023. URL [https://arxiv.org/pdf/2212.04089.pdf](https://arxiv.org/pdf/2212.04089.pdf).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ilharco等（2023）Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin
    Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, 和 Ali Farhadi。通过任务算术编辑模型，2023年。网址
    [https://arxiv.org/pdf/2212.04089.pdf](https://arxiv.org/pdf/2212.04089.pdf)。
- en: 'Ishihara (2023) Shotaro Ishihara. Training data extraction from pre-trained
    language models: A survey. *arXiv preprint arXiv:2305.16157*, 2023. URL [https://aclanthology.org/2023.trustnlp-1.23.pdf](https://aclanthology.org/2023.trustnlp-1.23.pdf).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ishihara（2023）Shotaro Ishihara。预训练语言模型中的训练数据提取：一项调查。*arXiv 预印本 arXiv:2305.16157*，2023年。网址
    [https://aclanthology.org/2023.trustnlp-1.23.pdf](https://aclanthology.org/2023.trustnlp-1.23.pdf)。
- en: Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig.
    How can we know what language models know? *Transactions of the Association for
    Computational Linguistics*, 8:423–438, 2020. URL [https://arxiv.org/pdf/2212.14315.pdf](https://arxiv.org/pdf/2212.14315.pdf).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2020）Zhengbao Jiang, Frank F Xu, Jun Araki, 和 Graham Neubig。我们如何了解语言模型所知道的？*计算语言学会刊*，8:423–438，2020年。网址
    [https://arxiv.org/pdf/2212.14315.pdf](https://arxiv.org/pdf/2212.14315.pdf)。
- en: Kenton et al. (2021) Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel,
    Vladimir Mikulik, and Geoffrey Irving. Alignment of language agents. *arXiv preprint
    arXiv:2103.14659*, 2021. URL [https://arxiv.org/pdf/2103.14659.pdf](https://arxiv.org/pdf/2103.14659.pdf).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kenton 等（2021）Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir
    Mikulik, 和 Geoffrey Irving。语言代理的对齐。*arXiv 预印本 arXiv:2103.14659*，2021年。网址 [https://arxiv.org/pdf/2103.14659.pdf](https://arxiv.org/pdf/2103.14659.pdf)。
- en: Krishna et al. (2023) Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John
    Wieting, and Mohit Iyyer. Paraphrasing evades detectors of ai-generated text,
    but retrieval is an effective defense. *arXiv preprint arXiv:2303.13408*, 2023.
    URL [https://arxiv.org/pdf/2303.13408.pdf](https://arxiv.org/pdf/2303.13408.pdf).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等（2023）Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting,
    和 Mohit Iyyer。改写能躲避 AI 生成文本的检测，但检索是一种有效的防御。*arXiv 预印本 arXiv:2303.13408*，2023年。网址
    [https://arxiv.org/pdf/2303.13408.pdf](https://arxiv.org/pdf/2303.13408.pdf)。
- en: Kumari et al. (2023) Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman,
    Richard Zhang, and Jun-Yan Zhu. Ablating concepts in text-to-image diffusion models.
    *arXiv preprint arXiv:2303.13516*, 2023. URL [https://arxiv.org/pdf/2303.13516.pdf](https://arxiv.org/pdf/2303.13516.pdf).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumari 等（2023）Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard
    Zhang, 和 Jun-Yan Zhu。剖析文本到图像扩散模型中的概念。*arXiv 预印本 arXiv:2303.13516*，2023年。网址 [https://arxiv.org/pdf/2303.13516.pdf](https://arxiv.org/pdf/2303.13516.pdf)。
- en: 'Lazaridou et al. (2021) Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya,
    Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d’Autume,
    Tomas Kocisky, Sebastian Ruder, et al. Mind the gap: Assessing temporal generalization
    in neural language models. *Advances in Neural Information Processing Systems*,
    34:29348–29363, 2021. URL [https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazaridou 等（2021）Angeliki Lazaridou, Adhi Kuncoro, Elena Gribovskaya, Devang
    Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d’Autume, Tomas
    Kocisky, Sebastian Ruder 等。注意差距：评估神经语言模型中的时间性泛化。*神经信息处理系统进展*，34:29348–29363，2021年。网址
    [https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/f5bf0ba0a17ef18f9607774722f5698c-Paper.pdf)。
- en: Levy et al. (2017) Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer.
    Zero-shot relation extraction via reading comprehension. In *Proceedings of the
    21st Conference on Computational Natural Language Learning (CoNLL 2017)*, pp. 
    333–342, 2017. URL [https://aclanthology.org/K17-1034](https://aclanthology.org/K17-1034).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy 等（2017）Omer Levy, Minjoon Seo, Eunsol Choi, 和 Luke Zettlemoyer。通过阅读理解进行零样本关系提取。在*第21届计算自然语言学习会议（CoNLL
    2017）*，第333–342页，2017年。网址 [https://aclanthology.org/K17-1034](https://aclanthology.org/K17-1034)。
- en: Lukas et al. (2023) Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas
    Wutschitz, and Santiago Zanella-Béguelin. Analyzing leakage of personally identifiable
    information in language models. In *2023 IEEE Symposium on Security and Privacy
    (SP)*, pp. 346–363, 2023. URL [https://arxiv.org/pdf/2302.00539.pdf](https://arxiv.org/pdf/2302.00539.pdf).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lukas 等（2023）Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz,
    和 Santiago Zanella-Béguelin。分析语言模型中个人可识别信息的泄漏。在*2023 IEEE 安全与隐私研讨会（SP）*，第346–363页，2023年。网址
    [https://arxiv.org/pdf/2302.00539.pdf](https://arxiv.org/pdf/2302.00539.pdf)。
- en: Meng et al. (2022) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.
    Locating and editing factual knowledge in gpt. *arXiv preprint arXiv:2202.05262*,
    2022. URL [https://arxiv.org/pdf/2202.05262.pdf](https://arxiv.org/pdf/2202.05262.pdf).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等（2022）Kevin Meng, David Bau, Alex Andonian, 和 Yonatan Belinkov。在 GPT 中定位和编辑事实知识。*arXiv
    预印本 arXiv:2202.05262*，2022年。网址 [https://arxiv.org/pdf/2202.05262.pdf](https://arxiv.org/pdf/2202.05262.pdf)。
- en: Meng et al. (2023) Kevin Meng, Arnab Sen Sharma, Alex J Andonian, Yonatan Belinkov,
    and David Bau. Mass-editing memory in a transformer. In *The Eleventh International
    Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=MkbcAHIYgyS](https://openreview.net/forum?id=MkbcAHIYgyS).
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人（2023）Kevin Meng、Arnab Sen Sharma、Alex J Andonian、Yonatan Belinkov 和
    David Bau。在变换器中进行大规模编辑记忆。见 *第十一届国际学习表征会议*，2023。网址 [https://openreview.net/forum?id=MkbcAHIYgyS](https://openreview.net/forum?id=MkbcAHIYgyS)。
- en: 'Min et al. (2023) Sewon Min, Suchin Gururangan, Eric Wallace, Hannaneh Hajishirzi,
    Noah A. Smith, and Luke Zettlemoyer. Silo language models: Isolating legal risk
    in a nonparametric datastore, 2023. URL [https://arxiv.org/pdf/2308.04430.pdf](https://arxiv.org/pdf/2308.04430.pdf).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等人（2023）Sewon Min、Suchin Gururangan、Eric Wallace、Hannaneh Hajishirzi、Noah
    A. Smith 和 Luke Zettlemoyer。孤立的语言模型：在非参数数据存储中隔离法律风险，2023。网址 [https://arxiv.org/pdf/2308.04430.pdf](https://arxiv.org/pdf/2308.04430.pdf)。
- en: Mitchell et al. (2021) Eric Mitchell, Charles Lin, Antoine Bosselut, Chelsea
    Finn, and Christopher D Manning. Fast model editing at scale. *arXiv preprint
    arXiv:2110.11309*, 2021. URL [https://arxiv.org/pdf/2110.11309.pdf](https://arxiv.org/pdf/2110.11309.pdf).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell 等人（2021）Eric Mitchell、Charles Lin、Antoine Bosselut、Chelsea Finn 和 Christopher
    D Manning。大规模快速模型编辑。*arXiv 预印本 arXiv:2110.11309*，2021。网址 [https://arxiv.org/pdf/2110.11309.pdf](https://arxiv.org/pdf/2110.11309.pdf)。
- en: 'Mohan et al. (2019) Jayashree Mohan, Melissa Wasserman, and Vijay Chidambaram.
    Analyzing gdpr compliance through the lens of privacy policy. In *Heterogeneous
    Data Management, Polystores, and Analytics for Healthcare: VLDB 2019 Workshops,
    Poly and DMAH, Los Angeles, CA, USA, August 30, 2019, Revised Selected Papers
    5*, pp.  82–95\. Springer, 2019. URL [https://arxiv.org/pdf/1906.12038.pdf](https://arxiv.org/pdf/1906.12038.pdf).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohan 等人（2019）Jayashree Mohan、Melissa Wasserman 和 Vijay Chidambaram。通过隐私政策分析
    GDPR 合规性。见 *异构数据管理、多存储库及医疗保健分析：VLDB 2019 工作坊，Poly 和 DMAH，洛杉矶，加利福尼亚州，美国，2019 年
    8 月 30 日，修订的精选论文 5*，第 82–95 页。施普林格，2019。网址 [https://arxiv.org/pdf/1906.12038.pdf](https://arxiv.org/pdf/1906.12038.pdf)。
- en: 'Mozes et al. (2023) Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D
    Griffin. Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities.
    *arXiv preprint arXiv:2308.12833*, 2023. URL [https://arxiv.org/pdf/2308.12833.pdf](https://arxiv.org/pdf/2308.12833.pdf).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mozes 等人（2023）Maximilian Mozes、Xuanli He、Bennett Kleinberg 和 Lewis D Griffin。利用大型语言模型的非法用途：威胁、预防措施和漏洞。*arXiv
    预印本 arXiv:2308.12833*，2023。网址 [https://arxiv.org/pdf/2308.12833.pdf](https://arxiv.org/pdf/2308.12833.pdf)。
- en: 'nostalgebraist (2020) nostalgebraist. interpreting gpt: the logit lens, 2020.
    URL [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nostalgebraist（2020）nostalgebraist。解释 GPT：logit 视角，2020。网址 [https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022. URL
    [https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang、Jeffrey Wu、Xu Jiang、Diogo Almeida、Carroll Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray 等人。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022。网址
    [https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)。
- en: 'Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge
    bases? In *Proceedings of the 2019 Conference on Empirical Methods in Natural
    Language Processing and the 9th International Joint Conference on Natural Language
    Processing (EMNLP-IJCNLP)*, pp.  2463–2473, Hong Kong, China, November 2019\.
    Association for Computational Linguistics. doi: 10.18653/v1/D19-1250. URL [https://aclanthology.org/D19-1250](https://aclanthology.org/D19-1250).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Petroni 等人（2019）Fabio Petroni、Tim Rocktäschel、Sebastian Riedel、Patrick Lewis、Anton
    Bakhtin、Yuxiang Wu 和 Alexander Miller。语言模型作为知识库？见 *2019 年自然语言处理实证方法会议暨第九届国际联合自然语言处理会议（EMNLP-IJCNLP）会议论文集*，第
    2463–2473 页，中国香港，2019 年 11 月。计算语言学协会。doi: 10.18653/v1/D19-1250。网址 [https://aclanthology.org/D19-1250](https://aclanthology.org/D19-1250)。'
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019. URL [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等等。语言模型是无监督的多任务学习者。 *OpenAI 博客*，1(8):9，2019年。网址 [https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。
- en: 'Ravfogel et al. (2020) Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton,
    and Yoav Goldberg. Null it out: Guarding protected attributes by iterative nullspace
    projection. *arXiv preprint arXiv:2004.07667*, 2020. URL [https://arxiv.org/pdf/2004.07667.pdf](https://arxiv.org/pdf/2004.07667.pdf).'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravfogel et al. (2020) Shauli Ravfogel, Yanai Elazar, Hila Gonen, Michael Twiton,
    和 Yoav Goldberg. 清除它：通过迭代的零空间投影来保护受保护属性。 *arXiv 预印本 arXiv:2004.07667*，2020年。网址
    [https://arxiv.org/pdf/2004.07667.pdf](https://arxiv.org/pdf/2004.07667.pdf)。
- en: 'Shao et al. (2022) Shun Shao, Yftah Ziser, and Shay B Cohen. Gold doesn’t always
    glitter: Spectral removal of linear and nonlinear guarded attribute information.
    *arXiv preprint arXiv:2203.07893*, 2022. URL [https://arxiv.org/pdf/2203.07893.pdf](https://arxiv.org/pdf/2203.07893.pdf).'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. (2022) Shun Shao, Yftah Ziser, 和 Shay B Cohen. 黄金并不总是闪耀：线性和非线性保护属性信息的光谱去除。
    *arXiv 预印本 arXiv:2203.07893*，2022年。网址 [https://arxiv.org/pdf/2203.07893.pdf](https://arxiv.org/pdf/2203.07893.pdf)。
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. Membership inference attacks against machine learning models. In *2017
    IEEE Symposium on Security and Privacy (SP)*, pp. 3–18\. IEEE, 2017.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, 和 Vitaly Shmatikov.
    针对机器学习模型的会员推断攻击。见于 *2017 IEEE 安全与隐私研讨会 (SP)*，第3–18页。IEEE，2017年。
- en: Tanno et al. (2022) Ryutaro Tanno, Melanie F Pradier, Aditya Nori, and Yingzhen
    Li. Repairing neural networks by leaving the right past behind. *Advances in Neural
    Information Processing Systems*, 35:13132–13145, 2022. URL [https://arxiv.org/pdf/2207.04806.pdf](https://arxiv.org/pdf/2207.04806.pdf).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanno et al. (2022) Ryutaro Tanno, Melanie F Pradier, Aditya Nori, 和 Yingzhen
    Li. 通过抛弃正确的过去来修复神经网络。 *神经信息处理系统进展*，35:13132–13145，2022年。网址 [https://arxiv.org/pdf/2207.04806.pdf](https://arxiv.org/pdf/2207.04806.pdf)。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023. URL [https://arxiv.org/pdf/2307.09288.pdf](https://arxiv.org/pdf/2307.09288.pdf).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等等。Llama 2：开放基础和微调聊天模型。 *arXiv 预印本 arXiv:2307.09288*，2023年。网址
    [https://arxiv.org/pdf/2307.09288.pdf](https://arxiv.org/pdf/2307.09288.pdf)。
- en: 'Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion
    Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax),
    May 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Komatsuzaki (2021) Ben Wang 和 Aran Komatsuzaki. GPT-J-6B：一个60亿参数的自回归语言模型。
    [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)，2021年5月。
- en: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    et al. Ethical and social risks of harm from language models. *arXiv preprint
    arXiv:2112.04359*, 2021. URL [https://arxiv.org/pdf/2112.04359.pdf](https://arxiv.org/pdf/2112.04359.pdf).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    等等。语言模型的伦理和社会风险。 *arXiv 预印本 arXiv:2112.04359*，2021年。网址 [https://arxiv.org/pdf/2112.04359.pdf](https://arxiv.org/pdf/2112.04359.pdf)。
- en: 'Zhang et al. (2023a) Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong
    Pan, Zhenchang Xing, Mark Staples, and Xiwei Xu. Right to be forgotten in the
    era of large language models: Implications, challenges, and solutions, 2023a.
    URL [https://arxiv.org/pdf/2307.03941.pdf](https://arxiv.org/pdf/2307.03941.pdf).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023a) Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong
    Pan, Zhenchang Xing, Mark Staples, 和 Xiwei Xu. 在大语言模型时代的被遗忘权：影响、挑战和解决方案，2023a。网址
    [https://arxiv.org/pdf/2307.03941.pdf](https://arxiv.org/pdf/2307.03941.pdf)。
- en: 'Zhang et al. (2023b) Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and
    Humphrey Shi. Forget-me-not: Learning to forget in text-to-image diffusion models.
    *arXiv preprint arXiv:2303.17591*, 2023b. URL [https://arxiv.org/pdf/2303.17591.pdf](https://arxiv.org/pdf/2303.17591.pdf).'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023b）Eric Zhang、Kai Wang、Xingqian Xu、Zhangyang Wang 和 Humphrey Shi。别忘了我：在文本到图像扩散模型中学习忘记。*arXiv
    预印本 arXiv:2303.17591*，2023b。网址 [https://arxiv.org/pdf/2303.17591.pdf](https://arxiv.org/pdf/2303.17591.pdf)。
- en: Zhu et al. (2020) Chen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh Bhojanapalli,
    Daliang Li, Felix Yu, and Sanjiv Kumar. Modifying memories in transformer models.
    *arXiv preprint arXiv:2012.00363*, 2020. URL [https://arxiv.org/pdf/2012.00363.pdf](https://arxiv.org/pdf/2012.00363.pdf).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2020年）Chen Zhu、Ankit Singh Rawat、Manzil Zaheer、Srinadh Bhojanapalli、Daliang
    Li、Felix Yu 和 Sanjiv Kumar。修改变换器模型中的记忆。*arXiv 预印本 arXiv:2012.00363*，2020年。网址 [https://arxiv.org/pdf/2012.00363.pdf](https://arxiv.org/pdf/2012.00363.pdf)。
- en: 'Ziegler (2021) Albert Ziegler. GitHub Copilot: Parrot or crow? https://docs.github.com/en/github/copilot/research-recitation,
    2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ziegler（2021年）Albert Ziegler。GitHub Copilot: Parrot or crow? https://docs.github.com/en/github/copilot/research-recitation，2021年。'
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023. URL [https://arxiv.org/pdf/2307.15043.pdf](https://arxiv.org/pdf/2307.15043.pdf).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人（2023年）Andy Zou、Zifan Wang、J Zico Kolter 和 Matt Fredrikson。针对对齐语言模型的通用和可转移对抗攻击。*arXiv
    预印本 arXiv:2307.15043*，2023年。网址 [https://arxiv.org/pdf/2307.15043.pdf](https://arxiv.org/pdf/2307.15043.pdf)。
- en: Appendix A Tuning Details
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 调整细节
- en: For a fixed budget of , we tune over the hyperparameters that control the budget
    on a separate development set of 100 samples.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 对于固定预算的 ，我们在 100 个样本的独立开发集上调整控制预算的超参数。
- en: 'Whitebox Attacks:'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 白盒攻击：
- en: 'Given a fixed candidate set size represented by , we seek to optimize the allocation
    of resources by tuning two parameters:  and . These parameters determine the distribution
    of the available budget across  layers, while retaining the  candidate tokens
    from each layer in the set  such that .'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 给定固定的候选集大小 ，我们通过调整两个参数： 和 来优化资源分配。这些参数决定了在  层之间分配可用预算，同时保留每层中的  候选标记在集合中，使得
    。
- en: We try different combinations of  and  in the following set [(1, 20), (2, 10),
    (4, 5), (5, 4), (2, 10), (1, 20)] for each of the options when choosing k candidates
    (top-, bottom- and (top-  bottom-)). For each , we choose the set  such that  and
    Attack-Success@(k, L) is maximum when evaluated on the development set.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试不同的 和 组合，集合为 [(1, 20)、(2, 10)、(4, 5)、(5, 4)、(2, 10)、(1, 20)]，在选择 k 个候选（top-、bottom-
    和 (top- bottom-)）时使用每个选项。对于每个 ，我们选择集合 使得  并且在开发集上评估时，Attack-Success@(k, L) 达到最大值。
- en: For the Head-Projection Attack, we find that choosing top-4 () highest probability
    candidates from each layer’s intermediate distribution while keeping 5 optimal
    layers (17, 18, 19 20, 21) () tuned on a separate development set of 100 sample
    points attains the highest attack success rate for GPT-J.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于头部投影攻击，我们发现选择每层中间分布中的 top-4 () 最高概率候选，同时保留 5 个最佳层（17、18、19、20、21） () 在 100
    个样本的独立开发集上调整的情况下，GPT-J 的攻击成功率最高。
- en: For the Probability Delta attack, we find that choosing top-2 highest probability
    candidates (rather than both top-k and bottom-k) from each layer’s intermediate
    distribution while keeping 10 () layers 8-9, 16-23 is optimal for the attack success
    for GPT-J, 36-45 for GPT2-XL.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于概率增量攻击，我们发现选择每层中间分布中的 top-2 最高概率候选（而不是 top-k 和 bottom-k）在保留 10 () 层 8-9、16-23
    的情况下，对于 GPT-J，选择 36-45 层，而对于 GPT2-XL，选择 36-45 层是攻击成功的最佳选择。
- en: '| Model | Attack-layers-HP | Attack-layers-PD | Defense-layers |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 攻击层数-HP | 攻击层数-PD | 防御层数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| GPT-J-6B | 17-21 | 8-9, 16-23 | 8-9, 16-28 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| GPT-J-6B | 17-21 | 8-9、16-23 | 8-9、16-28 |'
- en: '| GPT2-XL-1.5B | 41-45 | 36-45 | 36-48 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL-1.5B | 41-45 | 36-45 | 36-48 |'
- en: '| Llama2-7B | 23-27 | 21-30 | 23-32 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7B | 23-27 | 21-30 | 23-32 |'
- en: 'Table 2: Hyperparameters following tuning of attack and defense methods.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：攻击和防御方法调整后的超参数。
- en: 'Blackbox Attacks:'
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 黑盒攻击：
- en: 'Given a fixed candidate set size represented by , we seek to optimize the allocation
    of resources by tuning two parameters:  and . These parameters determine the distribution
    of the available budget  model-generated paraphrases, while randomly sampling
    the  candidate tokens from each paraphrase in the set  such that . We try different
    combinations of  and  in the following set [(1, 20), (2, 10), (4, 5), (5, 4),
    (2, 10), (1, 20)]. For the Input Rephrasing attack, we find that using 4 () paraphrases
    with 5 () samples from each paraphrase in the candidate set leads to highest attack
    success rate on the development set.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个固定的候选集大小，我们试图通过调整两个参数：**和** 来优化资源的分配。这些参数决定了可用预算的分配，模型生成的同义句，同时从每个同义句中随机抽取**候选令牌**，使得**。我们在以下集合中尝试不同的**和**的组合[(1,
    20), (2, 10), (4, 5), (5, 4), (2, 10), (1, 20)]。对于输入重述攻击，我们发现使用4（**）个同义句，并从每个同义句中抽取5（**）个样本，能够在开发集上获得最高的攻击成功率。
- en: 'Whitebox Defense: For the HP and PD defenses, we choose a set of layers  consisting
    of the attack layers as well as all the subsequent layers starting from the last
    attack layer to the final layer so as to propagate the defense to the final layers.
    See the final selected layers in Table [2](#A1.T2 "Table 2 ‣ Whitebox Attacks:
    ‣ Appendix A Tuning Details ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks").'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '白盒防御：对于HP和PD防御，我们选择一组层**，包括攻击层以及从最后一个攻击层到最终层的所有后续层，以便将防御传播到最终层。有关最终选定层的详细信息，请参见表格[2](#A1.T2
    "Table 2 ‣ Whitebox Attacks: ‣ Appendix A Tuning Details ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks")。'
- en: 'Blackbox Defense: We choose 5 paraphrases for this defense which are obtained
    using the model (See Appendix [D](#A4 "Appendix D Reproducibility details ‣ Can
    Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction
    Attacks") for the model details and hyperparameters).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒防御：我们为此防御选择了5个同义句，这些同义句是使用模型获得的（有关模型细节和超参数，请参见附录[D](#A4 "Appendix D Reproducibility
    details ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks")）。
- en: Appendix B Additional Experiments
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外实验
- en: B.1 Llama2-7b
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 Llama2-7b
- en: 'We show results for Llama2-7b in Table [3](#A2.T3 "Table 3 ‣ B.1 Llama2-7b
    ‣ Appendix B Additional Experiments ‣ Can Sensitive Information Be Deleted From
    LLMs? Objectives for Defending Against Extraction Attacks"). We modify some of
    the default edit parameters for of ROME so as to make the edits suitable for Llama-2-7b
    (Touvron et al., [2023](#bib.bib50)) i.e. a reasonable rewrite score and delta
    accuracy. However, in our experiments, ROME is not as effective with Llama-2-7b
    as it is with GPT-J. These are the hyperparameters used in ROME that we modify
    and their values for reproducibility: Learning rate (v_lr): 5e-1, Loss layer (v_loss_layer):
    31, Weight decay factor (v_weight_decay): 5e-5, The threshold at which the norm
    of the change vector (the norm of difference between original and new vector  is
    clamped at) (clamp_norm_factor): 2.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表格[3](#A2.T3 "Table 3 ‣ B.1 Llama2-7b ‣ Appendix B Additional Experiments
    ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against
    Extraction Attacks")中展示了Llama2-7b的结果。我们修改了ROME的一些默认编辑参数，以使这些编辑适用于Llama-2-7b（Touvron等，[2023](#bib.bib50)），即合理的重写评分和准确度。然而，在我们的实验中，ROME在Llama-2-7b上的效果不如在GPT-J上的效果。这些是我们修改的ROME超参数及其值，以便于复现：学习率（v_lr）：5e-1，损失层（v_loss_layer）：31，权重衰减因子（v_weight_decay）：5e-5，**变化向量**的范数（原始向量与新向量之间的差异的范数）被钳制的阈值（clamp_norm_factor）：2。
- en: Attack-Success@ -Acc Method Defense Head Projection Probabaility Delta Input
    Rephrasing Random Neighborhood Rewrite Score CounterFact ROME Fact-erasure 15.93
    25.95 51.43 3.30 16.20 98.53 Empty Resp 62.14 62.86 56.14 3.09 15.23 74.59 Error
    Inj 64.86 63.14 56.29 2.86 15.50 75.11 HP Def 3.43 4.71 55.14 3.38 16.77 96.22
    Max Ent def 3.14 4.57 52.86 3.38 17.06 99.27 IR def 65.57 65.43 70.14 0.69 4.70
    68.24
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击成功率@ -Acc 方法 防御 头 投影 概率 差异 输入重述 随机邻域 重写 评分 反事实 ROME 事实消除 15.93 25.95 51.43
    3.30 16.20 98.53 空响应 62.14 62.86 56.14 3.09 15.23 74.59 错误 伤害 64.86 63.14 56.29
    2.86 15.50 75.11 HP防御 3.43 4.71 55.14 3.38 16.77 96.22 最大熵防御 3.14 4.57 52.86 3.38
    17.06 99.27 IR防御 65.57 65.43 70.14 0.69 4.70 68.24
- en: 'Table 3: Attack success rates of the the three proposed attacks (Sec. [4](#S4
    "4 Attack Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")) on the CounterFact dataset when information
    is deleted from the Llama2-7B model using ROME augmented with the defense strategies
    (Sec. [5](#S5 "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks"))'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 CounterFact 数据集上，当从 Llama2-7B 模型中删除信息时，使用 ROME 增强了防御策略的三种提议攻击的攻击成功率（第
    [4](#S4 "4 攻击方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标") 节）和（第 [5](#S5 "5 防御方法 ‣ 能否从 LLM
    中删除敏感信息？防御提取攻击的目标") 节）
- en: B.2 GPT2-XL
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 GPT2-XL
- en: We report GPT2-XL-1.5B numbers in Table [4](#A2.T4 "Table 4 ‣ B.2 GPT2-XL ‣
    Appendix B Additional Experiments ‣ Can Sensitive Information Be Deleted From
    LLMs? Objectives for Defending Against Extraction Attacks"), and we show attack
    success as a function of attack budget in Fig. [5](#A2.F5 "Figure 5 ‣ B.3 Constrained
    Finetuning ‣ Appendix B Additional Experiments ‣ Can Sensitive Information Be
    Deleted From LLMs? Objectives for Defending Against Extraction Attacks").
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表 [4](#A2.T4 "表 4 ‣ B.2 GPT2-XL ‣ 附录 B 额外实验 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")
    中报告了 GPT2-XL-1.5B 的数据，并在图 [5](#A2.F5 "图 5 ‣ B.3 约束微调 ‣ 附录 B 额外实验 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")
    中展示了攻击成功率与攻击预算的关系。
- en: '|  | Attack-Success@ | -Acc |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击成功率@ | -Acc |  |'
- en: '| placeholder Defense | Head     Projection | Probability     Delta | Input
        Rephrasing | placeholder Random | placeholder Neighbors | Rewrite   Score
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 占位符 防御 | 头部     投影 | 概率     差异 | 输入     重述 | 占位符 随机 | 占位符 邻居 | 重写   得分 |'
- en: '| CounterFact |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| CounterFact |'
- en: '| GPT2-XL, ROME |  |  |  |  |  |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL, ROME |  |  |  |  |  |  |'
- en: '| Fact-erasure | 35.57 | 40.86 | 6.14 | 1.14 | 3.09 | 97.32 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Fact-erasure | 35.57 | 40.86 | 6.14 | 1.14 | 3.09 | 97.32 |'
- en: '| HT-def | 6.29 | 41.43 | 5.57 | 1.17 | 3.09 | 97.34 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| HT-def | 6.29 | 41.43 | 5.57 | 1.17 | 3.09 | 97.34 |'
- en: '| Ent Def | 6.29 | 41.43 | 4.71 | 1.18 | 3.09 | 97.34 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Ent Def | 6.29 | 41.43 | 4.71 | 1.18 | 3.09 | 97.34 |'
- en: '| Null resp | 28.14 | 34.29 | 5.57 | 1.31 | 1.46 | 99.73 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 空响应 | 28.14 | 34.29 | 5.57 | 1.31 | 1.46 | 99.73 |'
- en: '| IR def | 34.57 | 72.14 | 27.43 | 0.98 | 1.59 | 98.78 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| IR def | 34.57 | 72.14 | 27.43 | 0.98 | 1.59 | 98.78 |'
- en: '| Err Inj | 99.72 | 99.99 | 3.4 | 1.5 | 3.1 | 98.03 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Err Inj | 99.72 | 99.99 | 3.4 | 1.5 | 3.1 | 98.03 |'
- en: '| GPT2-XL, MEMIT |  |  |  |  |  |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GPT2-XL, MEMIT |  |  |  |  |  |  |'
- en: '| Fact-erasure | 55.57 | 41.71 | 17.71 | 0.57 | 1.53 | 98.16 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Fact-erasure | 55.57 | 41.71 | 17.71 | 0.57 | 1.53 | 98.16 |'
- en: '| Empty resp | 87.00 | 89.71 | 50.43 | 0.38 | 0.54 | 99.95 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 空响应 | 87.00 | 89.71 | 50.43 | 0.38 | 0.54 | 99.95 |'
- en: '| Err Inj | 89.32 | 89.72 | 31.72 | 0.53 | 1.39 | 99.54 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Err Inj | 89.32 | 89.72 | 31.72 | 0.53 | 1.39 | 99.54 |'
- en: '| HP-def | 21.00 | 41.29 | 18.00 | 0.68 | 3.17 | 97.50 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| HP-def | 21.00 | 41.29 | 18.00 | 0.68 | 3.17 | 97.50 |'
- en: '| Max-Ent Def | 21.86 | 61.29 | 18.14 | 0.67 | 3.13 | 97.55 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Max-Ent Def | 21.86 | 61.29 | 18.14 | 0.67 | 3.13 | 97.55 |'
- en: '| IR def | 61.43 | 64.86 | 32.29 | 0.91 | 2.90 | 97.59 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| IR def | 61.43 | 64.86 | 32.29 | 0.91 | 2.90 | 97.59 |'
- en: 'Table 4: Attack success rates of the the three proposed attacks (Sec. [4](#S4
    "4 Attack Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")) on the CounterFact dataset when information
    is deleted from the GPT2-XL model using the three editing methods augmented with
    the defense strategies (Sec. [5](#S5 "5 Defense Methods ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"))'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 CounterFact 数据集上，当从 GPT2-XL 模型中删除信息时，使用三种编辑方法和防御策略（第 [4](#S4 "4 攻击方法 ‣
    能否从 LLM 中删除敏感信息？防御提取攻击的目标") 节）和（第 [5](#S5 "5 防御方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")
    节）增强的攻击成功率
- en: B.3 Constrained Finetuning
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 约束微调
- en: Constrained Finetuning (Zhu et al., [2020](#bib.bib55)). Here, we employ a simple
    optimization approach based on the Adam optimizer, incorporating an -norm constraint,
    as outlined by (Zhu et al., [2020](#bib.bib55)). We finetune the same singular
    MLP weight matrix that we perform edits to in ROME.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 约束微调（Zhu et al., [2020](#bib.bib55)）。在这里，我们采用基于 Adam 优化器的简单优化方法，结合 -norm 约束，如（Zhu
    et al., [2020](#bib.bib55)）所述。我们微调与 ROME 中执行编辑的相同单一 MLP 权重矩阵。
- en: '![Refer to caption](img/725a8c7bc8d8d0e8e7c47aa45caafcbf.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/725a8c7bc8d8d0e8e7c47aa45caafcbf.png)'
- en: 'Figure 5: As the attack budget increases, the attack success increases and
    saturates after a budget of 10\. Here budget for HP and PD attacks is 20 and that
    for BB (IR) attack is 10\. Here the editing method is Fact erasure and model is
    GPT2-XL.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 随着攻击预算的增加，攻击成功率增加，并在预算达到 10 后饱和。这里 HP 和 PD 攻击的预算为 20，而 BB（IR）攻击的预算为 10。这里的编辑方法是事实擦除，模型是
    GPT2-XL。'
- en: Attack-Success@ -Acc Method Defense Head Projection Probabaility Delta Input
    Rephrasing Random Neighborhood Rewrite Score CounterFact FT Fact-erasure 47.53
    93.02 29.09 4.27 27.02 95.89 Empty resp 78.94 80.70 61.69 0.70 5.57 97.51 Error
    Inj 54.43 60.94 56.29 2.86 15.50 75.11 HP-def 99.15 96.76 75.81 0.02 0.05 0.17
    Max-Ent-def 99.32 96.93 75.81 0.00 0.00 0.00 IR def 46.85 58.77 13.12 4.25 26.95
    95.90
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击成功率@ -Acc 方法 防御 头部 投影 概率 差值 输入 改写 随机 邻域 重写 分数 CounterFact FT Fact-erasure
    47.53 93.02 29.09 4.27 27.02 95.89 空响应 78.94 80.70 61.69 0.70 5.57 97.51 错误 Inj
    54.43 60.94 56.29 2.86 15.50 75.11 HP-def 99.15 96.76 75.81 0.02 0.05 0.17 Max-Ent-def
    99.32 96.93 75.81 0.00 0.00 0.00 IR def 46.85 58.77 13.12 4.25 26.95 95.90
- en: 'Table 5: Attack success rates of the the three proposed attacks (Sec. [4](#S4
    "4 Attack Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")) on the CounterFact dataset when information
    is deleted from the GPT-J model using Constrained Finetuning (FT) augmented with
    the defense strategies (Sec. [5](#S5 "5 Defense Methods ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks"))'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 当使用约束微调（FT）和防御策略（参见 [5](#S5 "5 防御方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")）从 GPT-J
    模型中删除信息时，三种提议攻击（参见 [4](#S4 "4 攻击方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")）在 CounterFact
    数据集上的攻击成功率'
- en: '|  | Attack-Success@ | -Acc |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击成功率@ | -Acc |  |'
- en: '| placeholder Defense | Head     Projection | Probability     Delta | Input
        Rephrasing | placeholder Random | placeholder Neighbors | Rewrite   Score
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 占位符 防御 | 头部     投影 | 概率     差值 | 输入     改写 | 占位符 随机 | 占位符 邻域 | 重写   分数 |'
- en: '| CounterFact |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| CounterFact |'
- en: '| ROME |  |  |  |  |  |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| ROME |  |  |  |  |  |  |'
- en: '| + Fact Erasure | 22.15 | 25.38 | 22.83 | 0.72 | 8.74 | 99.69 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| + 事实擦除 | 22.15 | 25.38 | 22.83 | 0.72 | 8.74 | 99.69 |'
- en: '| + Empty Resp (dummy) | 36.84 | 37.65 | 29.02 | 0.54 | 3.76 | 99.58 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应（虚拟） | 36.84 | 37.65 | 29.02 | 0.54 | 3.76 | 99.58 |'
- en: '| + Empty Resp (I don’t know) | 54.89 | 55.92 | 32.80 | 0.50 | 3.20 | 98.74
    |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| + 空响应（我不知道） | 54.89 | 55.92 | 32.80 | 0.50 | 3.20 | 98.74 |'
- en: 'Table 6: Attack success rates of the the three proposed attacks (Sec. [4](#S4
    "4 Attack Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")) across defense methods (Sec. [5](#S5
    "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives
    for Defending Against Extraction Attacks")), for facts from CounterFact and zsRE
    that are known by GPT-J.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 针对 GPT-J 已知的 CounterFact 和 zsRE 的事实，在防御方法（参见 [5](#S5 "5 防御方法 ‣ 能否从 LLM
    中删除敏感信息？防御提取攻击的目标")）中的三种提议攻击（参见 [4](#S4 "4 攻击方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标")）的攻击成功率'
- en: Appendix C Editing Methods and Adversary Model Access
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 编辑方法和对抗模型访问
- en: C.1 Model Editing Methods
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 模型编辑方法
- en: ROME (Meng et al., [2022](#bib.bib36)). Rank-One Model Editing (ROME) is a state-of-the-art
    method that changes model outputs by updating a specific MLP layer in the model
    (layer 6 for GPT-J by default). The update is applied to the second matrix within
    this MLP layer, and the update itself is constrained to be a rank-one matrix that
    is obtained analytically when treating the MLP weight as a linear associative
    memory (Meng et al., [2022](#bib.bib36)). The default objective that is maximized
    by this update is the model probability  for a desired output , with data augmentation
    for the input  and some regularization. Recall that we can apply this editing
    method to optimize any of the defense objectives from Sec. [5](#S5 "5 Defense
    Methods ‣ Can Sensitive Information Be Deleted From LLMs? Objectives for Defending
    Against Extraction Attacks").
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ROME（Meng 等， [2022](#bib.bib36)）。Rank-One 模型编辑（ROME）是一种最先进的方法，通过更新模型中的特定 MLP
    层（默认情况下为 GPT-J 的第六层）来改变模型输出。更新应用于此 MLP 层中的第二个矩阵，并且更新本身被限制为一个秩为一的矩阵，这在将 MLP 权重视为线性关联记忆时可以通过分析获得（Meng
    等， [2022](#bib.bib36)）。此更新最大化的默认目标是模型对期望输出的概率，输入数据增强以及一些正则化。请注意，我们可以应用这种编辑方法来优化
    [5](#S5 "5 防御方法 ‣ 能否从 LLM 中删除敏感信息？防御提取攻击的目标") 中的任何防御目标。
- en: MEMIT (Meng et al., [2023](#bib.bib37)). MEMIT is a method designed for updating
    an arbitrary number of facts in a model, as opposed to a single fact. When applied
    to update only one fact, however, its only difference from ROME is that it “spreads
    out” its update over *multiple MLP layers* rather than a single MLP layer as in
    ROME (Meng et al., [2023](#bib.bib37)). When applying MEMIT to GPT-J, we update
    layers 5-7.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: MEMIT（Meng et al., [2023](#bib.bib37)）：MEMIT 是一种设计用于更新模型中任意数量事实的方法，而不是单个事实。然而，当只更新一个事实时，它与
    ROME 唯一的区别在于它将更新“分散”在*多个 MLP 层*中，而不是像 ROME 中那样集中在单个 MLP 层（Meng et al., [2023](#bib.bib37)）。在将
    MEMIT 应用于 GPT-J 时，我们更新第 5 到第 7 层。
- en: C.2 Model Access
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 模型访问
- en: 'Our blackbox attack relies on simple random sampling that can be carried out
    through the OpenAI API as of September 28, 2023: [https://platform.openai.com/docs/guides/gpt](https://platform.openai.com/docs/guides/gpt).
    Though we perform experiments with publicly available models, this is important
    since many models of interest are gated behind APIs.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的黑箱攻击依赖于简单的随机采样，这可以通过 OpenAI API 执行，截止到 2023 年 9 月 28 日：[https://platform.openai.com/docs/guides/gpt](https://platform.openai.com/docs/guides/gpt)。尽管我们使用公开可用的模型进行实验，但这很重要，因为许多感兴趣的模型被封锁在
    API 后面。
- en: 'Parameters for OpenAI API:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI API 的参数：
- en: 'Input Prompts or Instructions: Users send input prompts or instructions to
    the OpenAI API. These prompts provide context or guidance to the model regarding
    the task or the type of response expected.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示或指令：用户向 OpenAI API 发送输入提示或指令。这些提示提供了有关任务或期望响应类型的上下文或指导。
- en: 'Sampling Parameters: To customize the behavior of the model and the characteristics
    of the generated text, users can specify various sampling parameters:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 采样参数：为了自定义模型的行为和生成文本的特性，用户可以指定各种采样参数：
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sampling Temperature (temperature): This parameter controls the randomness
    of the generated text. Higher values (e.g., 0.8) make the output more random,
    while lower values (e.g., 0.2) make it more deterministic.'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 采样温度（temperature）：该参数控制生成文本的随机性。较高的值（例如，0.8）使输出更具随机性，而较低的值（例如，0.2）则使其更具确定性。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Maximum Tokens (max_tokens): Users can limit the length of the generated text
    by setting a maximum number of tokens. This helps in controlling the response
    length.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最大令牌数（max_tokens）：用户可以通过设置最大令牌数来限制生成文本的长度。这有助于控制响应的长度。
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Top-p Probability (top_p): This parameter allows users to set a probability
    threshold for the next token. Tokens with probabilities above this threshold are
    considered, which helps in influencing the diversity of generated responses.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Top-p 概率（top_p）：该参数允许用户为下一个令牌设置一个概率阈值。概率高于该阈值的令牌会被考虑，这有助于影响生成响应的多样性。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Frequency Penalty (frequency_penalty): Users can penalize the repetition of
    words in the generated text by adjusting this parameter. Higher values discourage
    word repetition.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 频率惩罚（frequency_penalty）：用户可以通过调整该参数对生成文本中词语的重复进行惩罚。较高的值会抑制词语的重复。
- en: •
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Presence Penalty (presence_penalty): This parameter allows users to penalize
    the presence of certain words or phrases in the generated text. It can be useful
    for controlling the content or style of the output.'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 出现惩罚（presence_penalty）：该参数允许用户对生成文本中某些词语或短语的出现进行惩罚。这对于控制输出的内容或风格非常有用。
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stop Sequences (stop_sequences): Users can specify strings that the model should
    avoid generating in the output. This is helpful for preventing specific content
    from appearing in the generated text.'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 停止序列（stop_sequences）：用户可以指定模型在输出中应避免生成的字符串。这有助于防止特定内容出现在生成的文本中。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Temperature Schedule (temperature_schedule): Users can provide a list of temperature
    values to change the temperature dynamically during the text generation process.
    This can result in text that starts more deterministic and becomes more random
    over time, or vice versa.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 温度计划（temperature_schedule）：用户可以提供一个温度值的列表，以便在文本生成过程中动态地改变温度。这可以导致文本开始时更具确定性，随着时间推移变得更具随机性，反之亦然。
- en: •
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Top-k Tokens (top_k): This parameter limits the number of tokens considered
    at each step of generation to the top-k most likely tokens. It can help in controlling
    the model’s creativity and focus.'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Top-k 令牌（top_k）：该参数限制每一步生成中考虑的令牌数量为 top-k 个最可能的令牌。这有助于控制模型的创造力和专注度。
- en: 'API Response: After sending the input prompt and specifying the desired sampling
    parameters, users receive the model-generated text or response from the OpenAI
    API. The output text is influenced by the provided context and the parameter settings.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: API 响应：在发送输入提示并指定所需的采样参数后，用户会从 OpenAI API 收到模型生成的文本或响应。输出文本受提供的上下文和参数设置的影响。
- en: Appendix D Reproducibility details
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 重现性细节
- en: Here, we give additional details to our experiments that would be necessary
    for reproducing the results.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了实验的额外细节，这些细节对于重现结果是必要的。
- en: Paraphrase Model.
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 改写模型。
- en: 'We use the dipper-paraphraser-xxl (Krishna et al., [2023](#bib.bib31)) model
    available on huggingface. We first generate paraphrases of the entire prompt,
    including the target answer by varying the following parameters in the model:
    lexical diversity in [20, 40, 60, 80], order diversity in [20, 40, 60, 80], top_p
    in [0.25, 0.5, 0.75]. We then retain only the paraphrases which have the target
    answer as the last word and obtain the paraphrased prompt by truncating the paraphrased
    sentence to remove the last word which is the target answer.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 dipper-paraphraser-xxl（Krishna 等人，[2023](#bib.bib31)）模型，该模型在 huggingface
    上可用。我们首先生成整个提示的改写，包括目标答案，通过在模型中变化以下参数：词汇多样性在 [20, 40, 60, 80]，顺序多样性在 [20, 40,
    60, 80]，top_p 在 [0.25, 0.5, 0.75]。然后，我们仅保留目标答案作为最后一个词的改写，并通过截断改写句子以去除最后一个词（即目标答案）来获得改写后的提示。
- en: -Acc Metrics.
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: -Acc 计量指标。
- en: The length of generated output that we use for measuring -Acc is 36.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 用于测量 -Acc 的生成输出长度为 36。
- en: Rewrite Score.
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重写分数。
- en: 'We consider the Rewrite Score from Hase et al. ([2023](#bib.bib22)) as a traditional
    measure of edit success, to be reported alongside Attack-Success metrics. The
    Rewrite Score measures how much the edit changes the new target probability as
    a fraction of the possible desired change:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 Hase 等人（[2023](#bib.bib22)）的重写分数视为编辑成功的传统度量，与攻击成功指标一起报告。重写分数衡量编辑如何改变新的目标概率，占可能的期望变化的比例：
- en: '|  |  |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: A value of 1 means that the edit perfectly maximizes the new target probability,
    while a value of 0 means that the new target probability did not change at all.
    When the probability of a target is being minimized rather than maximized (which
    occurs in some defense objectives), this metric simply becomes , reflecting that
    we desire the target probability to approach 0. Specifically, we use the original
    formulation for a maximizing objective with Empty Response and Error Injection,
    and we use the simplified version () when reporting Rewrite Score for Fact Erasure,
    Head Projection, Probability Delta, and Input Rephrasing defenses, since these
    methods involve *lowering* the probability of the target answer.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 值为 1 表示编辑完美地最大化了新的目标概率，而值为 0 表示新的目标概率没有变化。当目标的概率被最小化而不是最大化时（这种情况发生在某些防御目标中），该度量简单地变成**，反映出我们希望目标概率接近
    0。具体来说，我们对 Empty Response 和 Error Injection 的最大化目标使用原始公式，对于 Fact Erasure、Head
    Projection、Probability Delta 和 Input Rephrasing 防御的重写分数，我们使用简化版本（），因为这些方法涉及*降低*目标答案的概率。
- en: Head Projection defense.
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 头部投影防御。
- en: We backpropagate though both  and  without the use of any stop-gradient. (See
    Sec [5](#S5 "5 Defense Methods ‣ Can Sensitive Information Be Deleted From LLMs?
    Objectives for Defending Against Extraction Attacks")).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过**和**进行反向传播，而不使用任何停止梯度。（参见第[5节](#S5 "5 Defense Methods ‣ Can Sensitive Information
    Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks")）。
- en: Data Filtering.
  id: totrans-314
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据过滤。
- en: On top of the single-token filtering, we also require the original model probability  of
    the correct target answer which is being deleted to be at least 0.02, in order
    for it be meaningful to measure a decrease in the next-token probability.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单标记过滤外，我们还要求被删除的正确目标答案的原始模型概率至少为 0.02，以便在下一标记概率下降时具有意义。
