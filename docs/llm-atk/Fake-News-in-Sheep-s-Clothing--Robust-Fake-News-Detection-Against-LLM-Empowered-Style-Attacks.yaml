- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 披着羊皮的假新闻：对抗大型语言模型赋能的风格攻击的鲁棒假新闻检测
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.10830](https://ar5iv.labs.arxiv.org/html/2310.10830)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2310.10830](https://ar5iv.labs.arxiv.org/html/2310.10830)
- en: Jiaying Wu National University of Singapore  [jiayingwu@u.nus.edu](mailto:jiayingwu@u.nus.edu)
     and  Bryan Hooi National University of Singapore  [bhooi@comp.nus.edu.sg](mailto:bhooi@comp.nus.edu.sg)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 纪阳吴 新加坡国立大学 [jiayingwu@u.nus.edu](mailto:jiayingwu@u.nus.edu) 和 布莱恩·霍伊 新加坡国立大学
    [bhooi@comp.nus.edu.sg](mailto:bhooi@comp.nus.edu.sg)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: It is commonly perceived that online fake news and reliable news exhibit stark
    differences in writing styles, such as the use of sensationalist versus objective
    language. However, we emphasize that style-related features can also be exploited
    for *style-based attacks*. Notably, the rise of powerful Large Language Models
    (LLMs) has enabled malicious users to mimic the style of trustworthy news outlets
    at minimal cost. Our analysis reveals that LLM-camouflaged fake news content leads
    to substantial performance degradation of state-of-the-art text-based detectors
    (up to 38% decrease in F1 Score), posing a significant challenge for automated
    detection in online ecosystems. To address this, we introduce SheepDog, a style-agnostic
    fake news detector robust to news writing styles. SheepDog achieves this adaptability
    through LLM-empowered news reframing, which customizes each article to match different
    writing styles using style-oriented reframing prompts. By employing *style-agnostic
    training*, SheepDog enhances its resilience to stylistic variations by maximizing
    prediction consistency across these diverse reframings. Furthermore, SheepDog
    extracts *content-focused veracity attributions* from LLMs, where the news content
    is evaluated against a set of fact-checking rationales. These attributions provide
    supplementary information and potential interpretability that assist veracity
    prediction. On three benchmark datasets, empirical results show that SheepDog
    consistently yields significant improvements over competitive baselines and enhances
    robustness against LLM-empowered style attacks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通常认为，在线假新闻和可靠新闻在写作风格上有明显差异，例如使用耸人听闻的语言与客观语言。然而，我们强调，风格相关特征也可以被利用于*风格攻击*。值得注意的是，强大的大型语言模型（LLMs）的兴起使得恶意用户能够以极低的成本模仿可信新闻来源的风格。我们的分析揭示了LLM伪装的假新闻内容导致了最先进的基于文本的检测器性能显著下降（F1分数最多下降38%），这对在线生态系统中的自动检测提出了重大挑战。为解决这一问题，我们引入了SheepDog，一个对新闻写作风格具有鲁棒性的风格无关假新闻检测器。SheepDog通过LLM赋能的新闻重构实现了这种适应性，它使用风格导向的重构提示来定制每篇文章以匹配不同的写作风格。通过采用*风格无关训练*，SheepDog通过最大化这些不同重构中的预测一致性来增强其对风格变化的鲁棒性。此外，SheepDog从LLMs中提取*内容聚焦的真实性归因*，在此过程中新闻内容会与一组事实核查理由进行比较。这些归因提供了额外的信息和潜在的可解释性，辅助真实性预测。在三个基准数据集上，实证结果表明SheepDog在竞争基准测试中始终表现出显著的改进，并增强了对LLM赋能的风格攻击的鲁棒性。
- en: 'Fake News; Large Language Models; Adversarial Robustness^†^†ccs: Information
    systems Data mining^†^†ccs: Computing methodologies Natural language processing'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '假新闻；大型语言模型；对抗鲁棒性^†^†ccs: 信息系统 数据挖掘^†^†ccs: 计算方法  自然语言处理'
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: '![Refer to caption](img/a89d2a60c9ae7dd5b6d46e40a46b9f25.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a89d2a60c9ae7dd5b6d46e40a46b9f25.png)'
- en: Figure 1\. A motivating example of LLM-empowered style attacks on text-based
    fake news detectors, where fake news is camouflaged with the style of reliable
    news publishers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 一个激励性的例子，展示了大型语言模型赋能的风格攻击对基于文本的假新闻检测器的影响，其中假新闻伪装成可靠新闻出版商的风格。
- en: Automated fake news detectors are central to safeguarding integrity and harmony
    in the digital age (Nguyen et al., [2020](#bib.bib30); Shu et al., [2019](#bib.bib41);
    Wu and Hooi, [2023](#bib.bib49); Zhou et al., [2020](#bib.bib54)). Style-related
    signals embedded in news articles serve as a crucial indicator of news veracity.
    For instance, reputable news sources uphold journalistic integrity, emphasize
    accuracy and fact-checking, and maintain a balanced tone (Bachmann et al., [2021](#bib.bib3)).
    In stark contrast, unreliable outlets often employ sensationalism, lack credible
    sources, and may exhibit partisan biases (Higdon, [2020](#bib.bib14)). Building
    upon this, prior advances have extracted sentiment features (Ajao et al., [2019](#bib.bib2);
    Zhang et al., [2021](#bib.bib52)) to complement and enhance the detector, and
    highlighted the significance of writing style between hyperpartisan news and well-balanced
    mainstream reporting (Potthast et al., [2018](#bib.bib36)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化假新闻检测器在数字时代维护诚信和和谐方面至关重要（Nguyen et al., [2020](#bib.bib30); Shu et al., [2019](#bib.bib41);
    Wu and Hooi, [2023](#bib.bib49); Zhou et al., [2020](#bib.bib54)）。新闻文章中嵌入的风格相关信号是新闻真实性的关键指标。例如，信誉良好的新闻来源坚持新闻诚信，强调准确性和事实核查，并保持平衡的语气（Bachmann
    et al., [2021](#bib.bib3)）。与此形成鲜明对比的是，不可靠的渠道通常采用煽动性手法，缺乏可信来源，并可能表现出偏袒的立场（Higdon,
    [2020](#bib.bib14)）。在此基础上，早期进展已提取情感特征（Ajao et al., [2019](#bib.bib2); Zhang et
    al., [2021](#bib.bib52)），以补充和增强检测器，并强调了极端党派新闻与平衡主流报道之间写作风格的重要性（Potthast et al.,
    [2018](#bib.bib36)）。
- en: 'While style-related features are invaluable for discerning news veracity, they
    also offer a direct avenue for malicious users to conduct style-based attacks.
    This problem is exacerbated by the advent of powerful Large Language Models (LLMs)
    (Brown et al., [2020](#bib.bib4); OpenAI, [2022](#bib.bib31); Ouyang et al., [2022](#bib.bib33)),
    whose unprecedented capabilities for reasoning and generative tasks (Wei et al.,
    [2022](#bib.bib46); Zheng and Zhan, [2023](#bib.bib53)) reduces the gap between
    machine-generated and human-written news. Consequently, malicious actors now possess
    the capability to mimic the style of reputable news sources, in an attempt to
    evade automated detection. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks"), using a style-oriented prompt (i.e., “style of The New York Times”),
    LLM-camouflaged fake news successfully bypasses a RoBERTa (Liu et al., [2019](#bib.bib23))-based
    fake news detector.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管风格相关特征在识别新闻真实性方面极为宝贵，但它们也为恶意用户提供了一条直接的途径进行基于风格的攻击。这个问题因强大的大语言模型（LLMs）的出现而加剧（Brown
    et al., [2020](#bib.bib4); OpenAI, [2022](#bib.bib31); Ouyang et al., [2022](#bib.bib33)），这些模型在推理和生成任务上的前所未有的能力（Wei
    et al., [2022](#bib.bib46); Zheng and Zhan, [2023](#bib.bib53)）缩小了机器生成新闻与人工编写新闻之间的差距。因此，恶意行为者现在具备模仿信誉良好的新闻来源风格的能力，试图规避自动检测。如图
    [1](#S1.F1 "Figure 1 ‣ 1. Introduction ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks") 所示，通过使用风格导向的提示（即，“The
    New York Times 的风格”），LLM伪装的假新闻成功绕过了基于RoBERTa（Liu et al., [2019](#bib.bib23)）的假新闻检测器。'
- en: 'The impact of style-based attacks on fake news detectors has received limited
    attention, despite prior investigations into detector vulnerability related to
    attacks regarding social engagement (Chen and Shu, [2023](#bib.bib5)), word-level
    perturbations (Koenders et al., [2021](#bib.bib21)), and machine-generated malicious
    comments (Le et al., [2020](#bib.bib22)). To assess the robustness of text-based
    fake news detectors, we introduce a series of style-based adversarial attacks,
    specifically by tailoring news articles to adversarial writing styles (details
    provided in Section [4.1](#S4.SS1 "4.1\. Attack Formulation ‣ 4\. LLM-Empowered
    Style Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks")). Our experiments, as summarized in Table [1](#S4.T1
    "Table 1 ‣ 4.2\. Style-Related Detector Vulnerability ‣ 4\. LLM-Empowered Style
    Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks"), reveal a significant performance degradation in state-of-the-art
    text-based detectors, with some experiencing up to a 38% decline in F1 Score.
    These detectors, well-adapted to real news from trustworthy sources and fake news
    from unreliable sources, struggle to address the emerging real-world challenge
    where news content is presented in various styles.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管先前对与社会参与相关的攻击（Chen 和 Shu，[2023](#bib.bib5)）、单词级扰动（Koenders 等，[2021](#bib.bib21)）和机器生成恶意评论（Le
    等，[2020](#bib.bib22)）有关的检测器脆弱性进行了调查，但风格基础攻击对假新闻检测器的影响却受到的关注有限。为了评估基于文本的假新闻检测器的鲁棒性，我们引入了一系列风格基础的对抗攻击，特别是通过将新闻文章调整为对抗性写作风格（详细信息见第[4.1](#S4.SS1
    "4.1\. Attack Formulation ‣ 4\. LLM-Empowered Style Attacks ‣ Fake News in Sheep’s
    Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")节）。我们的实验总结在表[1](#S4.T1
    "Table 1 ‣ 4.2\. Style-Related Detector Vulnerability ‣ 4\. LLM-Empowered Style
    Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")中，显示最先进的基于文本的检测器在性能上显著下降，其中一些检测器的F1分数下降了多达38%。这些检测器虽然适应了来自可信来源的真实新闻和来自不可靠来源的假新闻，但在面对新闻内容以各种风格呈现的现实挑战时显得捉襟见肘。'
- en: 'Motivated by the style-related vulnerability observed in state-of-the-art text-based
    detectors, we present SheepDog — a robust, style-agnostic fake news detector that
    effectively identifies deceptive content, even when concealed within the LLM-empowered
    ”sheep’s clothing”. Built upon a pretrained language model (LM) backbone, which
    can be fine-tuned to capture task-specific salient features, and leveraging the
    strong zero-shot reasoning and generative capabilities of LLMs, SheepDog capitalizes
    on the strengths of both (LLM: versatile; fine-tuned LM: specialized).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于在最先进的基于文本的检测器中观察到的与风格相关的脆弱性，我们提出了SheepDog——一种鲁棒的风格无关假新闻检测器，能够有效识别欺骗性内容，即使它隐藏在LLM赋能的“羊皮”之中。SheepDog建立在一个预训练语言模型（LM）骨干上，可以进行微调以捕捉任务特定的显著特征，并利用LLM的强大零样本推理和生成能力，充分发挥LLM（多功能）和微调LM（专业化）的优势。
- en: 'Our key ideas for overcoming style attacks are: (1) style-agnostic training,
    in which predictions across style-diverse news reframings are regulated to ensure
    maximal consistency, and (2) content-focused veracity attributions, in which textual
    insights on news veracity are extracted from an LLM to inform veracity predictions.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们克服风格攻击的关键思想是：（1）风格无关训练，其中在风格多样的新闻重构中，预测被调节以确保最大的一致性，以及（2）内容聚焦的真实性归属，其中从LLM中提取关于新闻真实性的文本见解，以指导真实性预测。
- en: The style-agnostic nature of SheepDog stems from its utilization of LLM-empowered
    news reframings. To encompass a wide spectrum of real-world news writing styles,
    we expand our training samples with style-diverse expressions, and ensure coherence
    across different expressions to establish style invariance. Specifically, through
    style-oriented prompts, we exploit the impressive text generation capabilities
    of LLMs to generate reframings for each news article, introducing a diversity
    of styles. Then, we maximize the consistency in the model’s veracity predictions
    across the original article and its various reframings. This strategy assists
    SheepDog to discount style-related features, which enables it to capture style-agnostic
    signals from the news content.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: SheepDog的风格无关特性源于其利用LLM赋能的新闻重构。为了涵盖广泛的现实世界新闻写作风格，我们扩展了训练样本，采用风格多样的表达，并确保不同表达之间的一致性，以建立风格不变性。具体而言，通过风格导向的提示，我们利用LLMs卓越的文本生成能力，为每篇新闻文章生成重构，引入风格多样性。然后，我们最大化模型在原始文章及其各种重构中的真实性预测一致性。这一策略帮助SheepDog忽略风格相关特征，从而能够从新闻内容中捕捉风格无关的信号。
- en: 'To train a content-focused detector that is robust against stylistic variations,
    we further propose to distill the veracity-related knowledge from LLMs, specifically
    via eliciting content-focused attributions from inference APIs. We instruct the
    LLM to assess each article based on a defined set of content-focused fact-checking
    rationales (for comprehensive details, refer to Section [5.3](#S5.SS3 "5.3\. Content-Focused
    Veracity Attributions ‣ 5\. Proposed Approach ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks")). The resulting
    textual attributions serve to identify indicators of fake news. By converting
    these attributions into precise pseudo-labels, we introduce additional weak supervision
    that steers SheepDog towards learning a robust, style-agnostic representation
    of news. Our approach leverages these attribution-level predictions not only to
    facilitate style-agnostic training but also to potentially offer explainability
    into the veracity of news articles.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '为了训练一个对风格变化鲁棒的内容聚焦检测器，我们进一步提出从LLMs中提炼真实性相关知识，具体通过从推理API中引出内容聚焦的归因。我们指示LLM根据一组定义的内容聚焦事实核查理由来评估每篇文章（详见第[5.3](#S5.SS3
    "5.3\. Content-Focused Veracity Attributions ‣ 5\. Proposed Approach ‣ Fake News
    in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks)节"）。所得到的文本归因用于识别假新闻的指示符。通过将这些归因转化为精确的伪标签，我们引入了额外的弱监督，指导SheepDog学习鲁棒的、风格无关的新闻表示。我们的方法利用这些归因级别的预测，不仅促进了风格无关的训练，还可能为新闻文章的真实性提供解释。'
- en: 'Overall, we make the following three contributions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们做出了以下三项贡献：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Empirical Finding: We present a novel finding on the style-related vulnerability
    of state-of-the-art text-based fake news detectors to LLM-empowered style attacks.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实证发现：我们提出了一个新发现，即最先进的文本假新闻检测器在面对LLM赋能的风格攻击时存在风格相关的脆弱性。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LLM-Empowered Style Robustness: We introduce SheepDog, a style-agnostic fake
    news detector that achieves robustness through a combination of style-agnostic
    training and content-focused veracity attributions.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM赋能的风格鲁棒性：我们推出了SheepDog，一个风格无关的假新闻检测器，通过风格无关的训练和内容聚焦的真实性归因相结合来实现鲁棒性。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effectiveness: Through extensive experiments, we demonstrate that leveraging
    LLM-empowered diverse news reframings significantly enhances the performance of
    representative language models (LMs) across three benchmark datasets.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效果性：通过大量实验，我们证明了利用LLM赋能的多样新闻重构显著提升了代表性语言模型（LMs）在三个基准数据集上的表现。
- en: 2\. Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: Fake News Detection. Automated fake news detection has been widely explored
    using a wide range of neural architectures (Devlin et al., [2019](#bib.bib7);
    Liu et al., [2019](#bib.bib23); Pelrine et al., [2021](#bib.bib35); Shu et al.,
    [2019](#bib.bib41); Zhou et al., [2020](#bib.bib54)). Apart from extracting lexical
    (Rashkin et al., [2017](#bib.bib37)) and sentiment features (Potthast et al.,
    [2018](#bib.bib36)) within the news article text, many methods incorporate auxiliary
    features to supplement veracity prediction, including user comments (Shu et al.,
    [2019](#bib.bib41)), news environments (Sheng et al., [2022](#bib.bib40)), knowledge
    bases (Cui et al., [2020](#bib.bib6); Dun et al., [2021](#bib.bib8)), temporal
    patterns from users (Ruchansky et al., [2017](#bib.bib38)), and social graphs
    (Lu and Li, [2020](#bib.bib24); Nguyen et al., [2020](#bib.bib30); Wu and Hooi,
    [2023](#bib.bib49)). Recent studies also seek to address challenges including
    temporal shift (Hu et al., [2023](#bib.bib16)), entity bias (Zhu et al., [2022a](#bib.bib56))
    and domain shift (Nan et al., [2021](#bib.bib28), [2022](#bib.bib29); Zhu et al.,
    [2022b](#bib.bib57)) in fake news detection scenarios. In this work, we adopt
    a text-based perspective and focus on news article texts, specifically on the
    stylistic variations in the news article under LLM-empowered style attacks.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假新闻检测。自动化的假新闻检测已经广泛探讨了各种神经网络架构（Devlin et al., [2019](#bib.bib7); Liu et al.,
    [2019](#bib.bib23); Pelrine et al., [2021](#bib.bib35); Shu et al., [2019](#bib.bib41);
    Zhou et al., [2020](#bib.bib54)）。除了从新闻文章文本中提取词汇（Rashkin et al., [2017](#bib.bib37)）和情感特征（Potthast
    et al., [2018](#bib.bib36)）外，许多方法还结合了辅助特征来补充真实性预测，包括用户评论（Shu et al., [2019](#bib.bib41)）、新闻环境（Sheng
    et al., [2022](#bib.bib40)）、知识库（Cui et al., [2020](#bib.bib6); Dun et al., [2021](#bib.bib8)）、用户的时间模式（Ruchansky
    et al., [2017](#bib.bib38)）以及社交图谱（Lu and Li, [2020](#bib.bib24); Nguyen et al.,
    [2020](#bib.bib30); Wu and Hooi, [2023](#bib.bib49)）。最近的研究还寻求解决假新闻检测场景中的挑战，包括时间转移（Hu
    et al., [2023](#bib.bib16)）、实体偏见（Zhu et al., [2022a](#bib.bib56)）和领域转移（Nan et
    al., [2021](#bib.bib28), [2022](#bib.bib29); Zhu et al., [2022b](#bib.bib57)）。在本研究中，我们采用基于文本的视角，专注于新闻文章文本，特别是在LLM赋能的风格攻击下新闻文章中的风格变化。
- en: Adversarial Attack on Fake News Detectors. Investigating the vulnerabilities
    of fake news detectors is central to improving their real-world applicability.
    Hence, existing efforts (He et al., [2021a](#bib.bib10); Horne et al., [2019](#bib.bib15);
    Koenders et al., [2021](#bib.bib21); Lyu et al., [2023](#bib.bib25); Wang et al.,
    [2023](#bib.bib45); Zhou et al., [2019](#bib.bib55)) have studied the impact of
    different attacks from multiple aspects, including manipulation of social engagements
    (Lyu et al., [2023](#bib.bib25); Wang et al., [2023](#bib.bib45)) and user behavior
    (He et al., [2021a](#bib.bib10)), fact distortion (Koenders et al., [2021](#bib.bib21)),
    subject-object exchange (Zhou et al., [2019](#bib.bib55)), and blocking of data
    availability (Horne et al., [2019](#bib.bib15)). However, the impact of writing
    styles remains underexplored. To bridge this gap, we investigate the robustness
    of text-based detectors against LLM-empowered style attacks, and propose a style-agnostic
    solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对假新闻检测器的对抗性攻击。研究假新闻检测器的脆弱性对提高其在现实世界中的适用性至关重要。因此，现有的研究（He et al., [2021a](#bib.bib10);
    Horne et al., [2019](#bib.bib15); Koenders et al., [2021](#bib.bib21); Lyu et
    al., [2023](#bib.bib25); Wang et al., [2023](#bib.bib45); Zhou et al., [2019](#bib.bib55)）已经从多个方面研究了不同攻击的影响，包括社交互动的操控（Lyu
    et al., [2023](#bib.bib25); Wang et al., [2023](#bib.bib45)）和用户行为（He et al., [2021a](#bib.bib10)）、事实扭曲（Koenders
    et al., [2021](#bib.bib21)）、主题-对象交换（Zhou et al., [2019](#bib.bib55)）以及数据可用性的阻断（Horne
    et al., [2019](#bib.bib15)）。然而，写作风格的影响仍然未被充分探索。为了弥补这一空白，我们调查了基于文本的检测器对LLM赋能风格攻击的鲁棒性，并提出了一种与风格无关的解决方案。
- en: Large Language Models (LLMs). LLMs (OpenAI, [2022](#bib.bib31); Touvron et al.,
    [2023](#bib.bib43); OpenAI, [2023](#bib.bib32)), with billions of parameters,
    have showcased remarkable reasoning capabilities, even matching or surpassing
    human performance in some scenarios (Wei et al., [2022](#bib.bib46); Zheng and
    Zhan, [2023](#bib.bib53)). Benefiting from this success, recent work extracts
    knowledge from LLMs to facilitate a wide range of tasks, such as visual classification
    (Menon and Vondrick, [2023](#bib.bib26)) and node classification (He et al., [2023](#bib.bib12)).
    On a related front, LLM-generated text detection (Henrique et al., [2023](#bib.bib13);
    Muñoz-Ortiz et al., [2023](#bib.bib27)) has also attracted increasing attention.
    Recent investigations further explore LLM-generated misinformation (Epstein et al.,
    [2023](#bib.bib9); Huang et al., [2023](#bib.bib20); Zellers et al., [2019](#bib.bib51);
    Chen and Shu, [2023](#bib.bib5)), and leverage LLMs for program-guided fact-checking
    (Pan et al., [2023](#bib.bib34)). Although we also prompt an LLM to generate adversarial
    articles, we aim for style-agnostic detection that yields robust predictions across
    style-diverse news expressions. Moreover, existing work on LLM-generated misinformation
    mainly focuses on analysis of LLM-induced impacts and news article generation,
    which is orthogonal to our contributions. In this work, we focus on improving
    the detector’s style robustness by eliciting news reframings and content-focused
    veracity attributions from LLMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）。LLMs（OpenAI，[2022](#bib.bib31)；Touvron等，[2023](#bib.bib43)；OpenAI，[2023](#bib.bib32)），拥有数十亿个参数，展示了显著的推理能力，甚至在某些场景下达到或超越人类表现（Wei等，[2022](#bib.bib46)；Zheng和Zhan，[2023](#bib.bib53)）。得益于这一成功，近期工作从LLMs中提取知识，以促进各种任务，如视觉分类（Menon和Vondrick，[2023](#bib.bib26)）和节点分类（He等，[2023](#bib.bib12)）。相关领域中，LLM生成文本检测（Henrique等，[2023](#bib.bib13)；Muñoz-Ortiz等，[2023](#bib.bib27)）也受到越来越多的关注。近期调查进一步探索LLM生成的虚假信息（Epstein等，[2023](#bib.bib9)；Huang等，[2023](#bib.bib20)；Zellers等，[2019](#bib.bib51)；Chen和Shu，[2023](#bib.bib5)），并利用LLMs进行程序引导的事实核查（Pan等，[2023](#bib.bib34)）。虽然我们也促使LLM生成对抗性文章，但我们旨在实现风格无关的检测，以在风格多样的新闻表达中获得稳健的预测。此外，现有关于LLM生成虚假信息的工作主要集中在LLM引起的影响和新闻文章生成的分析上，这与我们的贡献是正交的。在本工作中，我们专注于通过引发新闻重塑和内容聚焦的真实性归属来提高检测器的风格鲁棒性。
- en: 3\. Problem Definition
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 问题定义
- en: Let  questionable news pieces, denoted as  is a set of labeled news articles.
    Each news article in . In line with prior work (Shu et al., [2019](#bib.bib41);
    Zhang et al., [2021](#bib.bib52); Zhou et al., [2020](#bib.bib54)), $y$ is a binary
    label that represents either real news or fake news.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让**可疑新闻片段**，记作，是一组标记的新闻文章。每篇新闻文章中。在先前的工作中（Shu等，[2019](#bib.bib41)；Zhang等，[2021](#bib.bib52)；Zhou等，[2020](#bib.bib54)），$y$是一个二元标签，代表真实新闻或虚假新闻。
- en: 'As we focus on style-related issues, we consider a text-based setting. Formally,
    the problem can be defined as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们专注于风格相关问题，我们考虑了一个基于文本的设置。形式上，问题可以定义如下：
- en: Problem 1 (Text-Based Fake News Detection).
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 问题1（基于文本的虚假新闻检测）。
- en: Given a news dataset , the goal is to predict the veracity labels of unlabeled
    news pieces $\mathcal{P}_{U}=\mathcal{D}\setminus\mathcal{P}_{L}$.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个新闻数据集，目标是预测未标记新闻片段的真实性标签$\mathcal{P}_{U}=\mathcal{D}\setminus\mathcal{P}_{L}$。
- en: 4\. LLM-Empowered Style Attacks
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. LLM增强的风格攻击
- en: In this section, we establish a series of LLM-empowered style attacks, and conduct
    preliminary analysis to assess the robustness of state-of-the-art text-based fake
    news detectors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们建立了一系列LLM增强的风格攻击，并进行初步分析，以评估最先进的基于文本的虚假新闻检测器的鲁棒性。
- en: 4.1\. Attack Formulation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 攻击公式
- en: The impressive capabilities of LLMs (Brown et al., [2020](#bib.bib4); OpenAI,
    [2022](#bib.bib31); Ouyang et al., [2022](#bib.bib33)) have enabled malicious
    users to disguise fake news with restyling prompts, resulting in camouflaged articles
    that closely resemble reliable sources. Here, we explore a direct form of style-based
    attack utilizing news publisher names (e.g., ”The New York Times”). These names
    possess distinct styles that can be readily adopted by producers of fake news,
    making them a likely occurrence in real-world scenarios.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的令人印象深刻的能力（Brown等，[2020](#bib.bib4)；OpenAI，[2022](#bib.bib31)；Ouyang等，[2022](#bib.bib33)）使恶意用户能够通过重新风格化提示伪装虚假新闻，从而产生与可靠来源极为相似的伪装文章。在这里，我们探索一种直接的基于风格的攻击形式，利用新闻出版商名称（例如，“纽约时报”）。这些名称具有明显的风格，虚假新闻生产者可以轻易采用，使其在现实场景中成为可能出现的情况。
- en: 'To replicate adversarial situations where news pieces are restyled in relation
    to various publishers, we manipulate the styles of both trustworthy and unreliable
    news. Specifically, in our test data, we utlize an LLM to rephrase real news in
    the style of tabloids, and fake news in the style of mainstream sources. Our general
    prompt format is shown as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了复制新闻内容在不同出版商风格下的对抗性情况，我们操控了可信和不可靠新闻的风格。具体而言，在我们的测试数据中，我们使用LLM将真实新闻重述为小报风格，而将假新闻重述为主流媒体风格。我们的通用提示格式如下所示：
- en: 'Rewrite the following article
    using the style of [publisher name]: [news article]'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rewrite the following article
    using the style of [publisher name]: [news article]'
- en: 'Based on publisher popularity, in the place of [publisher name], we select
    “National Enquirer” to camouflage real news, and “The New York Times” to camouflage
    fake news. These LLM-camouflaged test articles are then employed to evaluate the
    resilience of a detector against style-based attacks, as illustrated in Figure
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '根据出版商的受欢迎程度，我们选择“National Enquirer”来伪装真实新闻，以及“The New York Times”来伪装假新闻。这些由LLM伪装的测试文章随后用于评估检测器对基于风格的攻击的弹性，如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Fake News in Sheep’s Clothing: Robust Fake News
    Detection Against LLM-Empowered Style Attacks")所示。'
- en: 4.2\. Style-Related Detector Vulnerability
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 风格相关的检测器脆弱性
- en: 'Automated fake news detection becomes increasingly difficult with LLM-empowered
    style attacks. In this subsection, we conduct preliminary analysis on real-world
    news to evaluate the influence of writing styles on text-based detectors. Our
    analysis is based on the FakeNewsNet (Shu et al., [2020](#bib.bib42)) benchmark
    (consisting of PolitiFact and GossipCop datasets) and the Labeled Unreliable News
    (LUN) dataset (Rashkin et al., [2017](#bib.bib37)). Further details regarding
    these benchmark datasets can be found in Table [8](#A1.T8 "Table 8 ‣ Appendix
    A Dataset Description ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks") and Section [6.1.1](#S6.SS1.SSS1 "6.1.1\.
    Datasets ‣ 6.1\. Experimental Setup ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks"). Specifically,
    we seek to address the following question: To what extent can text-based fake
    news detectors withstand LLM-empowered style attacks?'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '随着LLM赋能的风格攻击的出现，自动假新闻检测变得越来越困难。在本小节中，我们对真实世界新闻进行初步分析，以评估写作风格对基于文本的检测器的影响。我们的分析基于FakeNewsNet（Shu等，[2020](#bib.bib42)）基准（包括PolitiFact和GossipCop数据集）和标记的不可靠新闻（LUN）数据集（Rashkin等，[2017](#bib.bib37)）。关于这些基准数据集的更多细节可以在表[8](#A1.T8
    "Table 8 ‣ Appendix A Dataset Description ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")和第[6.1.1](#S6.SS1.SSS1
    "6.1.1\. Datasets ‣ 6.1\. Experimental Setup ‣ 6\. Experiments ‣ Fake News in
    Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")节中找到。具体而言，我们寻求回答以下问题：基于文本的假新闻检测器在多大程度上能够抵御LLM赋能的风格攻击？'
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4.2\. Style-Related Detector Vulnerability ‣
    4\. LLM-Empowered Style Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News
    Detection Against LLM-Empowered Style Attacks"), we examine $13$ representative
    text-based detectors under both original and adversarial settings (detailed method
    descriptions are provided in Section [6.1.2](#S6.SS1.SSS2 "6.1.2\. Baselines ‣
    6.1\. Experimental Setup ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")). These detectors employ
    diverse model architectures, including Convolutional Neural Networks (CNNs) (Zhou
    et al., [2020](#bib.bib54)), Recurrent Neural Networks (RNNs), (Shu et al., [2019](#bib.bib41)),
    Graph Neural Networks (GNNs) applied to document graphs (Vaibhav et al., [2019](#bib.bib44)),
    Transformer-based LMs (Devlin et al., [2019](#bib.bib7); He et al., [2021b](#bib.bib11);
    Liu et al., [2019](#bib.bib23); Zhang et al., [2021](#bib.bib52); Schick and Schütze,
    [2021](#bib.bib39); Hu et al., [2022a](#bib.bib17); Xie et al., [2020](#bib.bib50)),
    and LLMs (OpenAI, [2022](#bib.bib31); Ouyang et al., [2022](#bib.bib33); Touvron
    et al., [2023](#bib.bib43)). We evaluate model robustness against LLM-empowered
    style attack based on performance on the adversarial test set formulated in Section
    [4.1](#S4.SS1 "4.1\. Attack Formulation ‣ 4\. LLM-Empowered Style Attacks ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks"). Our empirical results yield the following insights:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[1](#S4.T1 "表 1 ‣ 4.2\. 风格相关检测器脆弱性 ‣ 4\. LLM赋能的风格攻击 ‣ 羊皮上的假新闻：针对LLM赋能风格攻击的稳健假新闻检测")中，我们在原始和对抗设置下检查了$13$个具有代表性的文本基础检测器（详细的方法描述见[6.1.2](#S6.SS1.SSS2
    "6.1.2\. 基线 ‣ 6.1\. 实验设置 ‣ 6\. 实验 ‣ 羊皮上的假新闻：针对LLM赋能风格攻击的稳健假新闻检测")节）。这些检测器采用了各种模型架构，包括卷积神经网络（CNNs）（Zhou
    et al., [2020](#bib.bib54)），递归神经网络（RNNs）（Shu et al., [2019](#bib.bib41)），应用于文档图的图神经网络（GNNs）（Vaibhav
    et al., [2019](#bib.bib44)），基于Transformer的语言模型（Devlin et al., [2019](#bib.bib7);
    He et al., [2021b](#bib.bib11); Liu et al., [2019](#bib.bib23); Zhang et al.,
    [2021](#bib.bib52); Schick和Schütze, [2021](#bib.bib39); Hu et al., [2022a](#bib.bib17);
    Xie et al., [2020](#bib.bib50)），以及LLMs（OpenAI, [2022](#bib.bib31); Ouyang et al.,
    [2022](#bib.bib33); Touvron et al., [2023](#bib.bib43)）。我们根据[4.1](#S4.SS1 "4.1\.
    攻击公式 ‣ 4\. LLM赋能的风格攻击 ‣ 羊皮上的假新闻：针对LLM赋能风格攻击的稳健假新闻检测")节中制定的对抗性测试集上的性能评估模型的鲁棒性。我们的实证结果提供了以下见解：
- en: Observation 1 (Style-Related Vulnerability of Fake News Detectors).
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 观察 1（假新闻检测器的风格相关脆弱性）。
- en: State-of-the-art text-based fake news detectors are susceptible to LLM-empowered
    style attacks. This susceptibility results in substantial performance degradation,with
    an F1 Score decline of up to 38.3% on the adversarial test set.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的文本基础假新闻检测器容易受到LLM赋能的风格攻击。这种脆弱性导致性能显著下降，在对抗性测试集上的F1评分下降高达38.3%。
- en: Observation 2 (Insufficiency of LLMs as Zero-Shot Fake News Detectors).
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 观察 2（LLMs作为零样本假新闻检测器的不足）。
- en: LLMs, despite their impressive zero-shot capabilities as general-purpose foundation
    models, exhibit inferior detection performance compared to text-based fake news
    detectors and pre-trained LMs fine-tuned specifically for fake news detection.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs作为通用基础模型具有令人印象深刻的零样本能力，但与文本基础的假新闻检测器以及专门为假新闻检测进行微调的预训练语言模型相比，检测性能较差。
- en: Our two findings reveal a critical challenge in achieving robustness again stylistic
    variations. Ideally, an effective fake news detector should prioritize content
    assessment over style. While the news content typically remains stable, style
    can undergo rapid changes in the dynamic online environment. Our observations,
    however, reveals a stark deviation from this ideal. Detectors overly influenced
    by style may struggle to reliably differentiate between real and fake news, and
    even powerful LLMs may prove inadequate for the specific demands of fake news
    detection. The accessibility for malicious users to manipulate style using LLMs
    further exacerbates this issue, emphasizing the pressing need for a style-agnostic
    approach.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的两个发现揭示了在应对风格变化时实现稳健性的一个关键挑战。理想情况下，一个有效的假新闻检测器应优先考虑内容评估而非风格。尽管新闻内容通常保持稳定，但风格在动态的在线环境中可能会发生迅速变化。然而，我们的观察显示，这一理想情况存在明显偏差。受风格影响过大的检测器可能难以可靠地区分真实新闻和假新闻，甚至强大的大型语言模型（LLMs）也可能无法满足假新闻检测的特定要求。恶意用户利用LLMs操控风格的可达性进一步加剧了这一问题，突显出对抗风格无关方法的紧迫需求。
- en: 'Table 1\. Under LLM-empowered style attacks, existing text-based fake news
    detectors suffer severe performance deterioration in terms of F1 Score (%). Performance
    degradation is computed on adversarial test set (O: original; A: adversarial).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1\. 在 LLM 驱动的风格攻击下，现有的基于文本的虚假新闻检测器在 F1 分数 (%) 上表现出严重的性能下降。性能降级是在对抗性测试集上计算的（O:
    原始；A: 对抗）。'
- en: '| Method | PolitiFact |  | GossipCop |  | LUN |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PolitiFact |  | GossipCop |  | LUN |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| O | A () |  | O | A ($\downarrow$) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| O | A () |  | O | A ($\downarrow$) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| dEFEND\c (Shu et al., [2019](#bib.bib41)) | 82.59 | 12.15 |  | 70.74 | 4.34
    |  | 80.92 | 19.16 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| dEFEND\c (Shu 等, [2019](#bib.bib41)) | 82.59 | 12.15 |  | 70.74 | 4.34 |  |
    80.92 | 19.16 |'
- en: '| SAFE\v (Zhou et al., [2020](#bib.bib54)) | 79.85 | 8.74 |  | 70.64 | 2.93
    |  | 79.46 | 13.12 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SAFE\v (Zhou 等, [2020](#bib.bib54)) | 79.85 | 8.74 |  | 70.64 | 2.93 |  |
    79.46 | 13.12 |'
- en: '| SentGCN (Vaibhav et al., [2019](#bib.bib44)) | 80.77 | 13.82 |  | 69.29 |
    5.59 |  | 79.66 | 16.65 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| SentGCN (Vaibhav 等, [2019](#bib.bib44)) | 80.77 | 13.82 |  | 69.29 | 5.59
    |  | 79.66 | 16.65 |'
- en: '| BERT (Devlin et al., [2019](#bib.bib7)) | 84.99 | 12.68 |  | 74.50 | 5.52
    |  | 80.96 | 24.61 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| BERT (Devlin 等, [2019](#bib.bib7)) | 84.99 | 12.68 |  | 74.50 | 5.52 |  |
    80.96 | 24.61 |'
- en: '| RoBERTa (Liu et al., [2019](#bib.bib23)) | 87.40 | 11.23 |  | 74.05 | 3.05
    |  | 82.12 | 29.65 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa (Liu 等, [2019](#bib.bib23)) | 87.40 | 11.23 |  | 74.05 | 3.05 |  |
    82.12 | 29.65 |'
- en: '| DeBERTa (He et al., [2021b](#bib.bib11)) | 86.30 | 11.73 |  | 73.80 | 2.85
    |  | 83.67 | 30.34 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa (He 等, [2021b](#bib.bib11)) | 86.30 | 11.73 |  | 73.80 | 2.85 |  |
    83.67 | 30.34 |'
- en: '| UDA (Xie et al., [2020](#bib.bib50)) | 87.74 | 10.14 |  | 74.22 | 4.54 |  |
    82.94 | 20.71 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| UDA (Xie 等, [2020](#bib.bib50)) | 87.74 | 10.14 |  | 74.22 | 4.54 |  | 82.94
    | 20.71 |'
- en: '| DualEmo (Zhang et al., [2021](#bib.bib52)) | 87.76 | 15.34 |  | 75.36 | 5.89
    |  | 81.52 | 24.97 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| DualEmo (Zhang 等, [2021](#bib.bib52)) | 87.76 | 15.34 |  | 75.36 | 5.89 |  |
    81.52 | 24.97 |'
- en: '| PET (Schick and Schütze, [2021](#bib.bib39)) | 85.51 | 11.02 |  | 74.63 |
    3.08 |  | 83.66 | 31.08 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| PET (Schick 和 Schütze, [2021](#bib.bib39)) | 85.51 | 11.02 |  | 74.63 | 3.08
    |  | 83.66 | 31.08 |'
- en: '| KPT (Hu et al., [2022a](#bib.bib17)) | 87.70 | 13.26 |  | 74.23 | 2.63 |  |
    84.06 | 31.83 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| KPT (Hu 等, [2022a](#bib.bib17)) | 87.70 | 13.26 |  | 74.23 | 2.63 |  | 84.06
    | 31.83 |'
- en: '| GPT3.5 (OpenAI, [2022](#bib.bib31)) | 69.61 | 27.48 |  | 56.30 | 16.71 |  |
    79.97 | 20.34 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 (OpenAI, [2022](#bib.bib31)) | 69.61 | 27.48 |  | 56.30 | 16.71 |  |
    79.97 | 20.34 |'
- en: '| InstructGPT (Ouyang et al., [2022](#bib.bib33)) | 64.59 | 20.69 |  | 50.38
    | 9.13 |  | 68.16 | 11.39 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT (Ouyang 等, [2022](#bib.bib33)) | 64.59 | 20.69 |  | 50.38 | 9.13
    |  | 68.16 | 11.39 |'
- en: '| Llama2-13B (Touvron et al., [2023](#bib.bib43)) | 63.15 | 29.91 |  | 53.54
    | 27.75 |  | 70.97 | 38.33 | ![Refer to caption](img/a54610397486671e3b21cd268bf38797.png)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '| Llama2-13B (Touvron 等, [2023](#bib.bib43)) | 63.15 | 29.91 |  | 53.54 | 27.75
    |  | 70.97 | 38.33 | ![参见说明](img/a54610397486671e3b21cd268bf38797.png)'
- en: Figure 2\. Overview of our SheepDog style-agnostic fake news detector.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 我们的 SheepDog 风格无关虚假新闻检测器概述。
- en: 5\. Proposed Approach
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 提议的方法
- en: 'Building upon our empirical findings regarding the style-related vulnerability
    of fake news detectors, and the limitations of LLMs as zero-shot detectors, we
    present SheepDog, a style-agnostic detector that reliably assesses news veracity
    through the utilization of LLMs. As illustrated in Figure [2](#S4.F2 "Figure 2
    ‣ 4.2\. Style-Related Detector Vulnerability ‣ 4\. LLM-Empowered Style Attacks
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks"), the robustness of SheepDog is obtained from two core objectives
    in a multi-task learning paradigm: (1) style-agnostic training, which flexibly
    adapts a pre-trained LM to the fake news classification objective, and ensures
    consistent veracity predictions across a diverse array of news reframings; and
    (2) content-focused veracity attributions, which extracts veracity-related insights
    from LLMs to inform model predictions.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于我们对虚假新闻检测器在风格相关脆弱性方面的实证发现以及LLM作为零-shot检测器的局限性，我们提出了 SheepDog，这是一种风格无关的检测器，通过利用LLM可靠地评估新闻的真实性。如图
    [2](#S4.F2 "图 2 ‣ 4.2\. 风格相关检测器脆弱性 ‣ 4\. LLM 驱动的风格攻击 ‣ 伪装成羊的虚假新闻：对抗 LLM 驱动风格攻击的鲁棒虚假新闻检测")
    所示，SheepDog 的鲁棒性来自于多任务学习范式中的两个核心目标：（1）风格无关的训练，这种训练灵活地将预训练的 LM 适应于虚假新闻分类目标，并确保在各种新闻重构中保持一致的真实性预测；（2）内容聚焦的真实性归因，从
    LLM 中提取真实性相关的见解以指导模型预测。
- en: To enhance usability and composability, we design our method to be simple and
    modular, allowing it to be used with any language model backbone.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高可用性和可组合性，我们设计了简单而模块化的方法，使其可以与任何语言模型骨干一起使用。
- en: 5.1\. LLM-Empowered News Reframing
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. LLM 驱动的新闻重构
- en: 'In light of our Observation [1](#Thmobs1 "Observation 1 (Style-Related Vulnerability
    of Fake News Detectors). ‣ 4.2\. Style-Related Detector Vulnerability ‣ 4\. LLM-Empowered
    Style Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks"), it becomes evident that text-based fake news detectors,
    when exclusively trained on style-consistent news articles, may exhibit limited
    real-world adaptability. To overcome this limitation, our key idea is to diversify
    the training data through a process we refer to as reframing, which presents each
    news articles in various styles.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们的观察[1](#Thmobs1 "Observation 1 (Style-Related Vulnerability of Fake News
    Detectors). ‣ 4.2\. Style-Related Detector Vulnerability ‣ 4\. LLM-Empowered Style
    Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")，显而易见，当文本基础的假新闻检测器仅在风格一致的新闻文章上进行训练时，可能会表现出有限的现实世界适应性。为了克服这一局限，我们的关键思想是通过一种我们称之为重构的过程来多样化训练数据，即以各种风格呈现每篇新闻文章。'
- en: 'Our reframing strategy is driven by two sub-goals: (1) ensuring coherence in
    generated texts, and (2) covering a diverse range of styles. LLMs inherently exhibit
    both desirable properties. Hence, for each training article, we generate a series
    of prompts, each consisting of the news article and a style-oriented reframing
    instruction. The general prompt format is represented as follows, with a detailed
    example provided in Table [9](#A2.T9 "Table 9 ‣ B.3\. Prompt Template for Obtaining
    SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks"):'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的重构策略由两个子目标驱动：(1) 确保生成文本的一致性，以及 (2) 覆盖各种风格。LLMs 本质上展现了这两种期望属性。因此，对于每篇训练文章，我们生成一系列提示，每个提示由新闻文章和一个风格导向的重构指令组成。一般提示格式如下所示，详细示例见表[9](#A2.T9
    "Table 9 ‣ B.3\. Prompt Template for Obtaining SheepDog’s Veracity Attributions
    ‣ Appendix B Details of LLM Prompting ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")。'
- en: 'Rewrite the following article
    in a / an [specified] tone: [news article]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rewrite the following article
    in a / an [specified] tone: [news article]'
- en: 'To generate news expressions that simulate both reliable and unreliable sources,
    we establish a set of four general style-oriented adjectives for the prompt: ”objective
    and professional” and ”neutral” to emulate reliable sources, and ”emotionally
    triggering” and ”sensational” for unreliable ones.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成模拟可靠和不可靠来源的新闻表达，我们建立了一组四个通用风格导向的形容词用于提示：“客观和专业”和“中立”以模拟可靠来源，“情感刺激”和“耸人听闻”用于模拟不可靠来源。
- en: During the training phase, for labeled news article , and one unreliable-style
    reframing denoted as $p_{F}$.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段，针对标记的新闻文章以及一个不可靠风格的重构，表示为$p_{F}$。
- en: 5.2\. Style-Agnostic Training
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 无关风格训练
- en: A robust fake news detector should possess the capability to discern the veracity
    of news articles based on their content, without being influenced by their stylistic
    attributes. To this end, we introduce a style alignment objective that ensures
    close alignment among the veracity predictions of news article , and its unreliable-style
    reframing $p_{F}$. This objective is derived as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一个鲁棒的假新闻检测器应该具备根据新闻文章的内容来辨别其真实性的能力，而不受其风格属性的影响。为此，我们引入了一个风格对齐目标，确保新闻文章及其不可靠风格重构$p_{F}$的真实性预测之间的紧密对齐。该目标的推导如下。
- en: 'Let , based on  and , and reframing representations $\mathbf{h}_{R},\mathbf{h}_{F}\in\mathbb{R}^{d}$:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让基于和以及重构表示$\mathbf{h}_{R},\mathbf{h}_{F}\in\mathbb{R}^{d}$：
- en: '| (1) |  | $\mathbf{h}_{p}=\mathcal{M}(p),\quad\mathbf{h}_{R}=\mathcal{M}(p_{R}),\quad\mathbf{h}_{F}=\mathcal{M}(p_{F}).$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathbf{h}_{p}=\mathcal{M}(p),\quad\mathbf{h}_{R}=\mathcal{M}(p_{R}),\quad\mathbf{h}_{F}=\mathcal{M}(p_{F}).$
    |  |'
- en: 'Subsequently, we apply a Multi-Layer Perceptron (MLP) to these representations,
    and obtain veracity predictions , :'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '随后，我们将这些表示应用于多层感知机（MLP），并获得真实性预测:'
- en: '| (2) |  | $1$2 |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $1$2 |  |'
- en: Each of $\tilde{\mathbf{y}},\tilde{\mathbf{y}}_{R},\tilde{\mathbf{y}}_{F}$ contain
    two logits that correspond to the real and fake classes, respectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 每个$\tilde{\mathbf{y}},\tilde{\mathbf{y}}_{R},\tilde{\mathbf{y}}_{F}$包含两个logits，分别对应于真实和虚假类别。
- en: 'Despite differences in style, the fundamental news content remains consistent
    across the original news article  and  and . To facilitate this, we formulate
    the following style alignment loss defined as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管风格有所不同，但原始新闻文章和其不可靠风格重构的基本新闻内容保持一致。为此，我们制定了以下风格对齐损失，定义如下：
- en: '| (3) |  | $\mathcal{L}_{\text{style}}=\textsf{MEAN}\left(\mathcal{L}_{1}(\tilde{\mathbf{y}}_{R},\tilde{\mathbf{y}}),\;\mathcal{L}_{1}(\tilde{\mathbf{y}}_{F},\tilde{\mathbf{y}})\right),$
    |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathcal{L}_{\text{style}}=\textsf{MEAN}\left(\mathcal{L}_{1}(\tilde{\mathbf{y}}_{R},\tilde{\mathbf{y}}),\;\mathcal{L}_{1}(\tilde{\mathbf{y}}_{F},\tilde{\mathbf{y}})\right),$
    |  |'
- en: where $\mathcal{L}_{1}$ represents the The Kullback-Leibler (KL) divergence
    loss.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{1}$ 代表 Kullback-Leibler (KL) 散度损失。
- en: 'While aligning the predictions of  with . Therefore, we also incorporate a
    fake news detection loss:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在将预测结果与对齐时。因此，我们还引入了虚假新闻检测损失：
- en: '| (4) |  | $\mathcal{L}_{\text{news}}=\mathcal{L}_{2}(\tilde{\mathbf{y}},y),$
    |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\mathcal{L}_{\text{news}}=\mathcal{L}_{2}(\tilde{\mathbf{y}},y),$
    |  |'
- en: where $\mathcal{L}_{2}$ represents the standard cross entropy (CE) loss.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{2}$ 代表标准交叉熵 (CE) 损失。
- en: 5.3\. Content-Focused Veracity Attributions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 内容聚焦的真实性归属
- en: In addition to tuning the LM backbone with the style alignment objective for
    robust predictions, to capture the veracity-related signals directly linked to
    the news content, we further propose to integrate external knowledge and reasoning
    to inform veracity predictions. General-purpose LLMs, known for their remarkable
    zero-shot reasoning capabilities (Huang and Chang, [2023](#bib.bib19)), provides
    a promising solution.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过风格对齐目标来调优 LM 主干以获得稳健的预测外，为了直接捕捉与新闻内容相关的真实性信号，我们进一步提出集成外部知识和推理来告知真实性预测。通用
    LLM，以其显著的零样本推理能力而闻名（Huang 和 Chang，[2023](#bib.bib19)），提供了一个有前途的解决方案。
- en: 'Specifically, we generate content-focused veracity attributions by prompting
    an LLM. Our prompt consists of the news article text and a predefined set $\mathcal{C}$
    of content-oriented fact-checking rationales (e.g., “lack of credible sources”
    and “false or misleading information”; we present the detailed prompt template
    and an example in Appendix [B.3](#A2.SS3 "B.3\. Prompt Template for Obtaining
    SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks") and Table [10](#A2.T10 "Table 10 ‣ B.3\. Prompt Template for Obtaining
    SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")). Through this prompt, we instruct the LLM to provide textual attributions
    indicative of news veracity (i.e., identifying characteristics associated with
    fake news). This process efficiently extracts the LLM’s reasoning capabilities
    and general knowledge, presenting it in a text format. The resulting textual attributions,
    in turn, enriches our model with additional veracity-informed knowledge. The general
    prompt format is shown as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们通过提示 LLM 生成内容聚焦的真实性归属。我们的提示包括新闻文章文本和一组预定义的内容导向事实检查理由（例如，“缺乏可靠来源”和“虚假或误导性信息”；我们在附录
    [B.3](#A2.SS3 "B.3\. 提示模板以获取 SheepDog 的真实性归属 ‣ 附录 B LLM 提示的详细信息 ‣ 披着羊皮的假新闻：针对
    LLM 驱动的风格攻击的稳健假新闻检测") 和表 [10](#A2.T10 "表 10 ‣ B.3\. 提示模板以获取 SheepDog 的真实性归属 ‣
    附录 B LLM 提示的详细信息 ‣ 披着羊皮的假新闻：针对 LLM 驱动的风格攻击的稳健假新闻检测") 中提供了详细的提示模板和示例）。通过这个提示，我们指示
    LLM 提供指示新闻真实性的文本归属（即，识别与虚假新闻相关的特征）。这一过程高效地提取了 LLM 的推理能力和通用知识，并以文本格式呈现。生成的文本归属反过来丰富了我们的模型，提供了额外的真实性信息。通用提示格式如下所示：
- en: 'Article: [news article] Question:
    [given a list of fact-checking rationales, ask the LLM to identify content-related
    characteristics associated with fake news]'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 'Article: [news article] Question:
    [given a list of fact-checking rationales, ask the LLM to identify content-related
    characteristics associated with fake news]'
- en: Upon querying the LLM, we obtain a list of textual veracity attributions based
    on the input article. These attributions are then transformed into  is represented
    by a distinct binary label. For a given news article , which provides supplementary
    veracity-related information. Similarly, for reframings , we obtain pseudo-labels  is
    focused on indicators of fake news, the pseudo-labels for real news and its reframings
    are uniformly set to all-zeros.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询 LLM 时，我们根据输入文章获得一系列文本真实性归属。这些归属随后被转化为由独特的二进制标签表示。对于给定的新闻文章，提供补充的真实性相关信息。类似地，对于重构的内容，我们获得伪标签，专注于虚假新闻的指标，对于真实新闻及其重构，伪标签统一设置为全零。
- en: 'To incorporate the veracity-informed knowledge from these content-focused veracity
    attributions into our detector, we introduce a multi-label attribution prediction
    objective. As shown in Eq. [1](#S5.E1 "In 5.2\. Style-Agnostic Training ‣ 5\.
    Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), we learn news representations  and its
    reframings , respectively, using a pre-trained LM  are computed through another
    MLP:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '为了将这些内容聚焦的真实性归因中的真实性信息纳入我们的检测器，我们引入了一个多标签归因预测目标。如 Eq. [1](#S5.E1 "In 5.2\.
    Style-Agnostic Training ‣ 5\. Proposed Approach ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks") 所示，我们分别使用预训练的
    LM 计算新闻表示及其重构，通过另一个 MLP：'
- en: '| (5) |  | $1$2 |  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $1$2 |  |'
- en: 'The veracity attribution loss is then defined as:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，真实性归因损失定义为：
- en: '| (6) |  | $1$2 |  |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $1$2 |  |'
- en: where  denotes the sigmoid-transformed scores in $\tilde{\mathbf{s}}$ corresponding
    to each rationale.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tilde{\mathbf{s}}$ 中的 sigmoid 转换分数对应于每个理由。
- en: 5.4\. Final Objective Function of SheepDog
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. SheepDog 的最终目标函数
- en: 'By enforcing consistency among style-diverse news reframings and exploiting
    the content-focused attributions from the LLM, the final objective function of
    SheepDog is defined as a linear combination of the the style alignment loss (Eq.
    [3](#S5.E3 "In 5.2\. Style-Agnostic Training ‣ 5\. Proposed Approach ‣ Fake News
    in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")),
    the news classification loss (Eq. [4](#S5.E4 "In 5.2\. Style-Agnostic Training
    ‣ 5\. Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks")), and the veracity attribution loss (Eq.
    [6](#S5.E6 "In 5.3\. Content-Focused Veracity Attributions ‣ 5\. Proposed Approach
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")):'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '通过强制风格多样的新闻重构之间的一致性，并利用来自 LLM 的内容聚焦归因，SheepDog 的最终目标函数定义为风格对齐损失（Eq. [3](#S5.E3
    "In 5.2\. Style-Agnostic Training ‣ 5\. Proposed Approach ‣ Fake News in Sheep’s
    Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")）、新闻分类损失（Eq.
    [4](#S5.E4 "In 5.2\. Style-Agnostic Training ‣ 5\. Proposed Approach ‣ Fake News
    in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")）和真实性归因损失（Eq.
    [6](#S5.E6 "In 5.3\. Content-Focused Veracity Attributions ‣ 5\. Proposed Approach
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")）的线性组合：'
- en: '| (7) |  | $\mathcal{L}=\mathcal{L}_{\text{style}}+\mathcal{L}_{\text{news}}+\mathcal{L}_{\text{attr}}.$
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\mathcal{L}=\mathcal{L}_{\text{style}}+\mathcal{L}_{\text{news}}+\mathcal{L}_{\text{attr}}.$
    |  |'
- en: Our proposed SheepDog detector is an end-to-end framework where we simultaneously
    learn the style-agnostic news veracity predictor and the content-focused attribution
    predictor.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 SheepDog 检测器是一个端到端的框架，我们同时学习风格无关的新闻真实性预测器和内容聚焦的归因预测器。
- en: 'Table 2\. SheepDog significantly outperforms competitive baselines on four
    adversarial test settings under LLM-empowered style attacks (formulated in Section
    [4](#S4 "4\. LLM-Empowered Style Attacks ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")), in terms of F1 Score
    (%) . Bold (underlined) values indicate the best overall (baseline) performance.
    Statistical significance over the most competitive baselines computed using the
    Wilcoxon signed-rank test (Wilcoxon, [1945](#bib.bib47)) is indicated with ^∗
    ($p<.01$). (G1: text-based fake news detectors; G2: LMs fine-tuned to the fake
    news detection task; G3: LLMs)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2\. SheepDog 在四种对抗测试设置下，显著超越了竞争基准，在 LLM 驱动的风格攻击下（见第 [4](#S4 "4\. LLM-Empowered
    Style Attacks ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks") 节），以 F1 分数 (%) 衡量。粗体（下划线）值表示最佳整体（基准）性能。通过 Wilcoxon
    符号秩检验（Wilcoxon，[1945](#bib.bib47)）计算的与最具竞争力的基准的统计显著性用 ^∗ ($p<.01$) 表示。（G1：基于文本的假新闻检测器；G2：针对假新闻检测任务微调的
    LMs；G3：LLMs）'
- en: '|  | Method | PolitiFact |  | GossipCop |  | LUN |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | PolitiFact |  | GossipCop |  | LUN |'
- en: '| A | B | C | D |  | A | B | C | D |  | A | B | C | D |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| A | B | C | D |  | A | B | C | D |  | A | B | C | D |'
- en: '| G1 | dEFEND\c | 70.44 | 69.77 | 73.67 | 72.98 |  | 66.40 | 66.55 | 68.93
    | 69.07 |  | 61.76 | 62.28 | 72.95 | 72.50 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| G1 | dEFEND\c | 70.44 | 69.77 | 73.67 | 72.98 |  | 66.40 | 66.55 | 68.93
    | 69.07 |  | 61.76 | 62.28 | 72.95 | 72.50 |'
- en: '| SAFE\v | 71.11 | 70.80 | 75.55 | 75.24 |  | 67.71 | 67.05 | 68.31 | 67.65
    |  | 66.34 | 67.08 | 72.40 | 73.16 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SAFE\v | 71.11 | 70.80 | 75.55 | 75.24 |  | 67.71 | 67.05 | 68.31 | 67.65
    |  | 66.34 | 67.08 | 72.40 | 73.16 |'
- en: '| SentGCN | 66.95 | 62.50 | 69.54 | 65.08 |  | 63.70 | 63.07 | 63.61 | 63.01
    |  | 63.01 | 62.50 | 76.11 | 75.56 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SentGCN | 66.95 | 62.50 | 69.54 | 65.08 |  | 63.70 | 63.07 | 63.61 | 63.01
    |  | 63.01 | 62.50 | 76.11 | 75.56 |'
- en: '| DualEmo | 72.42 | 71.23 | 77.07 | 75.80 |  | 69.47 | 68.50 | 71.69 | 70.71
    |  | 56.55 | 54.78 | 68.53 | 66.80 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DualEmo | 72.42 | 71.23 | 77.07 | 75.80 |  | 69.47 | 68.50 | 71.69 | 70.71
    |  | 56.55 | 54.78 | 68.53 | 66.80 |'
- en: '| G2 | BERT | 72.31 | 71.37 | 77.23 | 76.24 |  | 68.98 | 68.17 | 71.95 | 71.11
    |  | 56.35 | 54.61 | 68.50 | 66.74 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| G2 | BERT | 72.31 | 71.37 | 77.23 | 76.24 |  | 68.98 | 68.17 | 71.95 | 71.11
    |  | 56.35 | 54.61 | 68.50 | 66.74 |'
- en: '| RoBERTa | 76.17 | 74.95 | 78.28 | 77.05 |  | 71.00 | 70.47 | 72.56 | 72.02
    |  | 52.47 | 53.62 | 68.31 | 69.46 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 76.17 | 74.95 | 78.28 | 77.05 |  | 71.00 | 70.47 | 72.56 | 72.02
    |  | 52.47 | 53.62 | 68.31 | 69.46 |'
- en: '| DeBERTa | 74.57 | 74.36 | 80.60 | 80.35 |  | 70.95 | 71.15 | 72.51 | 72.71
    |  | 53.33 | 55.45 | 67.16 | 69.27 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa | 74.57 | 74.36 | 80.60 | 80.35 |  | 70.95 | 71.15 | 72.51 | 72.71
    |  | 53.33 | 55.45 | 67.16 | 69.27 |'
- en: '| UDA | 77.60 | 75.57 | 79.21 | 77.17 |  | 69.68 | 69.33 | 72.16 | 71.80 |  |
    62.23 | 61.80 | 68.25 | 67.80 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| UDA | 77.60 | 75.57 | 79.21 | 77.17 |  | 69.68 | 69.33 | 72.16 | 71.80 |  |
    62.23 | 61.80 | 68.25 | 67.80 |'
- en: '| PET | 74.49 | 70.75 | 75.49 | 71.76 |  | 71.55 | 70.85 | 73.74 | 73.02 |  |
    52.58 | 53.30 | 63.71 | 64.33 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| PET | 74.49 | 70.75 | 75.49 | 71.76 |  | 71.55 | 70.85 | 73.74 | 73.02 |  |
    52.58 | 53.30 | 63.71 | 64.33 |'
- en: '| KPT | 74.44 | 73.32 | 77.73 | 76.60 |  | 71.60 | 71.01 | 73.69 | 73.10 |  |
    52.23 | 53.62 | 65.71 | 67.15 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| KPT | 74.44 | 73.32 | 77.73 | 76.60 |  | 71.60 | 71.01 | 73.69 | 73.10 |  |
    52.23 | 53.62 | 65.71 | 67.15 |'
- en: '| G3 | GPT3.5 | 42.13 | 43.44 | 56.61 | 58.17 |  | 39.59 | 38.67 | 48.44 |
    47.38 |  | 59.63 | 61.24 | 65.74 | 67.43 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| G3 | GPT3.5 | 42.13 | 43.44 | 56.61 | 58.17 |  | 39.59 | 38.67 | 48.44 |
    47.38 |  | 59.63 | 61.24 | 65.74 | 67.43 |'
- en: '| InstructGPT | 43.90 | 43.90 | 54.21 | 54.21 |  | 41.25 | 40.18 | 44.26 |
    43.12 |  | 56.77 | 57.15 | 58.93 | 59.32 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT | 43.90 | 43.90 | 54.21 | 54.21 |  | 41.25 | 40.18 | 44.26 |
    43.12 |  | 56.77 | 57.15 | 58.93 | 59.32 |'
- en: '| Llama2-13B | 33.24 | 34.48 | 53.64 | 55.45 |  | 25.79 | 26.06 | 37.07 | 37.40
    |  | 32.64 | 33.00 | 50.81 | 51.33 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 33.24 | 34.48 | 53.64 | 55.45 |  | 25.79 | 26.06 | 37.07 | 37.40
    |  | 32.64 | 33.00 | 50.81 | 51.33 |'
- en: '| Ours | SheepDog | 80.99^∗ | 79.89^∗ | 82.36^∗ | 81.24 |  | 74.45^∗ | 74.38^∗
    | 75.95^∗ | 75.88^∗ |  | 85.63^∗ | 86.06^∗ | 87.89^∗ | 88.32^∗ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | SheepDog | 80.99^∗ | 79.89^∗ | 82.36^∗ | 81.24 |  | 74.45^∗ | 74.38^∗
    | 75.95^∗ | 75.88^∗ |  | 85.63^∗ | 86.06^∗ | 87.89^∗ | 88.32^∗ |'
- en: 6\. Experiments
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 实验
- en: 'In this section, we empirically evaluate SheepDog to investigate the following
    six research questions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过实证评估 SheepDog，以探讨以下六个研究问题：
- en: •
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robustness Against Style Attacks (Section [6.2](#S6.SS2 "6.2\. Robustness Against
    Style Attacks ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News
    Detection Against LLM-Empowered Style Attacks")): How robust is SheepDog against
    LLM-empowered style attacks?'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对风格攻击的鲁棒性（第 [6.2](#S6.SS2 "6.2\. 对风格攻击的鲁棒性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对 LLM 驱动风格攻击的鲁棒假新闻检测")节）：SheepDog
    对 LLM 驱动的风格攻击有多鲁棒？
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effectiveness on Unperturbed Articles (Section [6.3](#S6.SS3 "6.3\. Effectiveness
    on Unperturbed Articles ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")): How proficiently can
    SheepDog identify fake news within the original unperturbed test articles?'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对未干扰文章的有效性（第 [6.3](#S6.SS3 "6.3\. 对未干扰文章的有效性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对 LLM 驱动风格攻击的鲁棒假新闻检测")节）：SheepDog
    在原始未干扰测试文章中识别假新闻的能力如何？
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adaptability to Different LM Backbones (Section [6.4](#S6.SS4 "6.4\. Adaptability
    to Different LM Backbones ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")): How does SheepDog’s
    performance vary with different representative LM backbones?'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对不同 LM 主干的适应性（第 [6.4](#S6.SS4 "6.4\. 对不同 LM 主干的适应性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对 LLM
    驱动风格攻击的鲁棒假新闻检测")节）：SheepDog 的性能在不同的代表性 LM 主干中有何变化？
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Ablation Study (Section [6.5](#S6.SS5 "6.5\. Ablation Study ‣ 6\. Experiments
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")): What are the respective impacts of style-agnostic training and
    content-focused attributions on SheepDog’s overall performance?'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消融研究（第 [6.5](#S6.SS5 "6.5\. 消融研究 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对 LLM 驱动风格攻击的鲁棒假新闻检测")节）：风格无关训练和内容聚焦归因对
    SheepDog 的整体性能有何具体影响？
- en: •
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stability Across Reframing Prompts (Section [6.6](#S6.SS6 "6.6\. Stability
    Across Reframing Prompts ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")): Does SheepDog yield
    consistent improvements across diverse settings of news reframing prompts?'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在重构提示中的稳定性（第 [6.6](#S6.SS6 "6.6\. 在重构提示中的稳定性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对 LLM 驱动风格攻击的鲁棒假新闻检测")节）：SheepDog
    是否在不同的新闻重构提示设置中表现出一致的改进？
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Case Study (Section [6.7](#S6.SS7 "6.7\. Case Study ‣ 6\. Experiments ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")): How can we interpret SheepDog’s rationale for fake news detection
    through its predictions on content-focused attributions?'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '案例研究（第 [6.7](#S6.SS7 "6.7\. Case Study ‣ 6\. Experiments ‣ Fake News in Sheep’s
    Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks") 节）：我们如何通过对内容聚焦归因的预测来解读
    SheepDog 的虚假新闻检测原理？'
- en: 6.1\. Experimental Setup
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实验设置
- en: 6.1.1\. Datasets
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 数据集
- en: 'We evaluate our approach on three widely-used real-world benchmark datasets:
    the FakeNewsNet public benchmark (Shu et al., [2020](#bib.bib42)), which consists
    of the PolitiFact and GossipCop datasets, and the Labeled Unreliable News (LUN)
    dataset (Rashkin et al., [2017](#bib.bib37)). PolitiFact and LUN center on political
    discourse, while GossipCop focuses on celebrity gossip. PolitiFact and LUN center
    on political discourse, while GossipCop focuses on celebrity gossip.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在三个广泛使用的真实世界基准数据集上评估我们的方法：FakeNewsNet 公共基准（Shu 等， [2020](#bib.bib42)），包含 PolitiFact
    和 GossipCop 数据集，以及标记的不可靠新闻（LUN）数据集（Rashkin 等， [2017](#bib.bib37)）。PolitiFact 和
    LUN 关注政治话语，而 GossipCop 则聚焦于名人八卦。PolitiFact 和 LUN 关注政治话语，而 GossipCop 则聚焦于名人八卦。
- en: 'The dataset description and statistics are delegated to Appendix [A](#A1 "Appendix
    A Dataset Description ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks") and Table [8](#A1.T8 "Table 8 ‣ Appendix
    A Dataset Description ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"). We randomly select % as test data.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集描述和统计信息见附录 [A](#A1 "Appendix A Dataset Description ‣ Fake News in Sheep’s
    Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks") 和表
    [8](#A1.T8 "Table 8 ‣ Appendix A Dataset Description ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks")。我们随机选择 % 作为测试数据。'
- en: 6.1.2\. Baselines
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 基线
- en: 'We benchmark SheepDog against thirteen representative baseline methods, which
    can be categorized as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 SheepDog 与十三种代表性基线方法进行基准测试，这些方法可以被归类为：
- en: Text-based fake news detectors (G1) employ neural architectures tailored specifically
    for the fake news detection task. dEFEND\c is a variant of dEFEND (Shu et al.,
    [2019](#bib.bib41)) based on the news article text that adopts RNN-based hierarchical
    co-attention. SAFE\v is a text-based variant of SAFE (Zhou et al., [2020](#bib.bib54))
    that leverages a CNN-based architecture to learn semantic comments. SentGCN (Vaibhav
    et al., [2019](#bib.bib44)) encodes veracity-related sentence interaction patterns
    within each article using a GNN, and DualEmo (Zhang et al., [2021](#bib.bib52))
    incorporates dual emotion features from news publishers and news comments. As
    our SheepDog approach does not involve user comments, we implement DualEmo on
    a BERT-base (Devlin et al., [2019](#bib.bib7)) backbone with publisher emotion
    features for a fair comparison.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基于文本的虚假新闻检测器（G1）使用专门为虚假新闻检测任务量身定制的神经网络架构。dEFEND\c 是 dEFEND（Shu 等， [2019](#bib.bib41)）的一种变体，基于新闻文章文本，采用基于
    RNN 的层次协同注意机制。SAFE\v 是 SAFE（Zhou 等， [2020](#bib.bib54)）的文本变体，利用 CNN 架构来学习语义评论。SentGCN（Vaibhav
    等， [2019](#bib.bib44)）通过 GNN 编码每篇文章中与真实性相关的句子交互模式，而 DualEmo（Zhang 等， [2021](#bib.bib52)）结合了新闻发布者和新闻评论的双重情感特征。由于我们的
    SheepDog 方法不涉及用户评论，我们在 BERT-base（Devlin 等， [2019](#bib.bib7)）骨干网络上实现 DualEmo，并结合发布者情感特征进行公平比较。
- en: 'Fine-tuned LMs (G2) adapts pre-trained LMs to the fake news detection task,
    and has proven effective in handling misinformation scenarios (Pelrine et al.,
    [2021](#bib.bib35)). In addition to three widely-recognized LMs, namely BERT (Devlin
    et al., [2019](#bib.bib7)), RoBERTa (Liu et al., [2019](#bib.bib23)), and DeBERTa
    (He et al., [2021b](#bib.bib11)), we include UDA (Xie et al., [2020](#bib.bib50)),
    a representative BERT-based model that employs diverse text augmentations to yield
    consistent model predictions against input noise. We also select two methods under
    the popular prompting paradigm: PET (Schick and Schütze, [2021](#bib.bib39)),
    which converts textual inputs into cloze questions that contain a task description;
    and KPT, (Hu et al., [2022b](#bib.bib18)) which expands the label word space with
    varied class-related tokens. For a fair comparison, as our proposed approach does
    not involve unlabeled articles, we implement UDA using consistency training on
    the supervised training data, and exclude the self-training and PLM ensemble components
    for PET. All methods in this category are implemented with base version LMs, in
    line with our approach.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的语言模型（G2）将预训练的语言模型调整为假新闻检测任务，并已被证明在处理虚假信息场景中有效（Pelrine et al., [2021](#bib.bib35)）。除了三个广泛认可的语言模型，即
    BERT (Devlin et al., [2019](#bib.bib7))，RoBERTa (Liu et al., [2019](#bib.bib23))
    和 DeBERTa (He et al., [2021b](#bib.bib11))，我们还包括了 UDA (Xie et al., [2020](#bib.bib50))，这是一个代表性的基于
    BERT 的模型，采用多样化的文本增强技术，以在输入噪声下保持模型预测的一致性。我们还选择了流行的提示范式下的两种方法：PET (Schick and Schütze,
    [2021](#bib.bib39))，它将文本输入转换为包含任务描述的完形填空问题；以及 KPT (Hu et al., [2022b](#bib.bib18))，它通过多样的类相关标记扩展标签词空间。为了公平比较，由于我们提出的方法不涉及未标记的文章，我们在监督训练数据上使用一致性训练实现了
    UDA，并排除了 PET 的自我训练和 PLM 集成组件。该类别中的所有方法都基于基础版本的语言模型，与我们的方法一致。
- en: 'LLMs (G3) generate textual veracity predictions in a zero-shot manner. We select
    three representative LLMs as baselines: GPT3.5 (OpenAI, [2022](#bib.bib31)) (version
    name: gpt-3.5-turbo), InstructGPT (Ouyang et al., [2022](#bib.bib33)) (version
    name: gpt-3.5-turbo-instruct), and Llama2-13B (Touvron et al., [2023](#bib.bib43))
    (version name: Llama-2-13b-chat-hf), with details delegated to Appendix [B.1](#A2.SS1
    "B.1\. Prompt Template for Baseline LLM Detectors ‣ Appendix B Details of LLM
    Prompting ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs (G3) 以零样本的方式生成文本真实性预测。我们选择了三个具有代表性的 LLM 作为基准：GPT3.5 (OpenAI, [2022](#bib.bib31))（版本名称：gpt-3.5-turbo），InstructGPT
    (Ouyang et al., [2022](#bib.bib33))（版本名称：gpt-3.5-turbo-instruct），以及 Llama2-13B
    (Touvron et al., [2023](#bib.bib43))（版本名称：Llama-2-13b-chat-hf），详细信息见附录 [B.1](#A2.SS1
    "B.1\. Prompt Template for Baseline LLM Detectors ‣ Appendix B Details of LLM
    Prompting ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks").'
- en: 6.1.3\. Implementation Details
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 实现细节
- en: We implement SheepDog and its variants based on PyTorch 1.10.0 with CUDA 11.1\.
    We utilize pretrained RoBERTa-base weights from HuggingFace Transformers 4.13.0
    (Wolf et al., [2020](#bib.bib48)). The LM backbone for SheepDog was configured
    with a maximum sequence length of 512, a batch size of 4, and a learning rate
    of  epochs. For the implementation of baseline methods, we adhere to the architectures
    and hyperparameters recommended by their respective authors.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于 PyTorch 1.10.0 和 CUDA 11.1 实现了 SheepDog 及其变体。我们利用 HuggingFace Transformers
    4.13.0 (Wolf et al., [2020](#bib.bib48)) 中预训练的 RoBERTa-base 权重。SheepDog 的 LM 骨干配置了最大序列长度
    512，批次大小 4 和学习率 epochs。在基准方法的实现中，我们遵循了各自作者推荐的架构和超参数。
- en: We evaluate model performance using Accuracy (%) and macro-F1 Score (%). For
    all experiments except those involving LLMs, we report averaged metrics over 10
    runs of each method to provide a comprehensive evaluation. In the case of LLM
    zero-shot predictions, we employ greedy decoding and conduct each experiment once.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用准确率（%）和宏 F1 分数（%）来评估模型性能。除涉及 LLM 的实验外，我们报告每种方法的 10 次运行的平均指标，以提供全面的评估。在 LLM
    零样本预测的情况下，我们采用贪婪解码并每次实验进行一次。
- en: 6.2\. Robustness Against Style Attacks
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 对风格攻击的鲁棒性
- en: 'We establish a series of LLM-empowered style attacks to assess SheepDog’s robustness.
    Following our prompt template formulated in Section [4.1](#S4.SS1 "4.1\. Attack
    Formulation ‣ 4\. LLM-Empowered Style Attacks ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks"), in the place
    of [publisher name], we select “National Enquirer” and “The Sun” to camouflage
    real news, and “CNN” and “The New York Times” for fake news, according to publisher
    popularity. This yields $2\times 2=4$ distinct adversarial test sets, labeled
    as A through D in Table [3](#S6.T3 "Table 3 ‣ 6.2\. Robustness Against Style Attacks
    ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"). Note that we report the results of SheepDog
    on adversarial set A in all other subsections.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建立了一系列LLM增强风格攻击来评估SheepDog的鲁棒性。根据我们在第[4.1](#S4.SS1 "4.1\. 攻击制定 ‣ 4\. LLM增强风格攻击
    ‣ 羊群中的假新闻：针对LLM增强风格攻击的鲁棒假新闻检测")节中制定的提示模板，我们选择“National Enquirer”和“The Sun”来伪装真实新闻，而选择“CNN”和“The
    New York Times”来伪装虚假新闻，依据是出版商的流行度。这产生了$2\times 2=4$个不同的对抗性测试集，标记为表[3](#S6.T3 "表
    3 ‣ 6.2\. 针对风格攻击的鲁棒性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对LLM增强风格攻击的鲁棒假新闻检测")中的A至D。请注意，我们在所有其他子章节中报告了SheepDog在对抗集A上的结果。
- en: 'Table 3\. Notations and setup for the four style-based adversarial test sets
    in Section [6.2](#S6.SS2 "6.2\. Robustness Against Style Attacks ‣ 6\. Experiments
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks"), denoted as A through D.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 第[6.2](#S6.SS2 "6.2\. 针对风格攻击的鲁棒性 ‣ 6\. 实验 ‣ 羊群中的假新闻：针对LLM增强风格攻击的鲁棒假新闻检测")节中四个基于风格的对抗性测试集的符号和设置，标记为A至D。
- en: '| [publisher name] | CNN | The New York Times |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| [出版商名称] | CNN | The New York Times |'
- en: '| --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| National Enquirer | A | B |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| National Enquirer | A | B |'
- en: '| The Sun | C | D |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| The Sun | C | D |'
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.4\. Final Objective Function of SheepDog ‣ 5\.
    Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks") compares the performance of SheepDog with
    competitive baselines across adversarial test sets A through D under LLM-empowered
    style attacks. We can observe that: (1) All baseline methods are highly susceptible
    to LLM-empowered style attacks. This vulnerability suggests that existing methods
    may exhibit a tendency towards over-fitting on style-related attributes, a consequence
    of being trained on datasets where real and fake news exhibit distinct stylistic
    features. (2) UDA, which leverages back-translation to generate diverse text augmentations,
    consistently demonstrates higher robustness compared to its BERT backbone. This
    suggests the efficacy of UDA’s augmentation strategy. However, UDA still struggles
    to fully adapt to significant stylistic variances in the input articles. This
    limitation may be attributed to the fact that augmentations through back-translation
    alone may not provide sufficient variance. (3) On the challenging adversarial
    test sets of LUN, CNN-based SAFE\v and GNN-based SentGCN are more robust than
    LM-based baselines, which suggests that LMs can be more prone to overfit to style-related
    features. (4) SheepDog outperforms the most competitive baseline by significant
    margins. Across the three benchmarks, this improvement averages to %, and $15.70$%
    across the four adversarial test sets, in terms of F1 score. This validates the
    effectiveness of SheepDog’s style-agnostic training and content-focused veracity
    attributions against style attacks.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S5.T2 "表 2 ‣ 5.4\. SheepDog的最终目标函数 ‣ 5\. 提议的方法 ‣ 羊群中的假新闻：针对LLM增强风格攻击的鲁棒假新闻检测")比较了SheepDog在LLM增强风格攻击下与竞争基线在对抗性测试集A至D上的表现。我们可以观察到：（1）所有基线方法对LLM增强风格攻击高度敏感。这种脆弱性表明现有方法可能会在风格相关属性上表现出过拟合的倾向，这可能是由于在真实和虚假新闻展现出明显不同的风格特征的数据集上进行训练的结果。（2）UDA通过回译生成多样化的文本增强，相较于其BERT骨干网，表现出更高的鲁棒性。这表明了UDA增强策略的有效性。然而，UDA仍然难以完全适应输入文章中的显著风格差异。这一局限性可能归因于仅通过回译的增强可能无法提供足够的变异性。（3）在LUN的具有挑战性的对抗性测试集上，基于CNN的SAFE\v和基于GNN的SentGCN比基于LM的基线更具鲁棒性，这表明LM可能更容易对风格相关特征进行过拟合。（4）SheepDog在最具竞争力的基线中表现出显著的优势。在三个基准测试中，这一改善平均为%，在四个对抗性测试集中的F1得分上为$15.70$%。这验证了SheepDog在风格无关训练和内容聚焦真实性归因方面对风格攻击的有效性。
- en: 6.3\. Effectiveness on Unperturbed Articles
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 未干扰文章的有效性
- en: 'A desirable fake news detector should achieve both robustness and effectiveness.
    Our empirical results, detailed in Table [4](#S6.T4 "Table 4 ‣ 6.4\. Adaptability
    to Different LM Backbones ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks"), clearly demonstrate
    that SheepDog excels in this regard. When tested on the original, unaltered articles,
    SheepDog consistently matches (on PolitiFact and GossipCop) or surpasses (on LUN)
    the performance of the most competitive baseline, in terms of accuracy and F1
    score. This showcases the framework’s proficiency in accurately discerning unreliable
    news from reliable ones, and implies SheepDog’s practical utility in real-world
    scenarios.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '一个理想的假新闻检测器应具备强大的鲁棒性和有效性。我们的实证结果（详见表[4](#S6.T4 "Table 4 ‣ 6.4\. Adaptability
    to Different LM Backbones ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks")）清晰地展示了 SheepDog 在这方面的卓越表现。在对原始未更改的文章进行测试时，SheepDog
    在 PolitiFact 和 GossipCop 上的表现与最具竞争力的基线相匹配（或在 LUN 上超越），无论是在准确率还是 F1 分数方面。这展示了该框架在准确辨别不可靠新闻与可靠新闻方面的能力，并暗示了
    SheepDog 在实际应用中的实用性。'
- en: 6.4\. Adaptability to Different LM Backbones
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 对不同 LM 骨干的适应性
- en: 'To assess the flexibility of SheepDog, we evaluate the performance of SheepDog
    combined with three representative LM backbones: RoBERTa, BERT and DeBERTa. As
    demonstrated in Table [5](#S6.T5 "Table 5 ‣ 6.4\. Adaptability to Different LM
    Backbones ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News
    Detection Against LLM-Empowered Style Attacks"), SheepDog substantially enhances
    the performance of the corresponding LM backbones. This adaptability highlights
    SheepDog’s robustness from style-agnostic training and content-focused attributions,
    showcasing its potential for flexible deployment in diverse contexts where different
    LMs may be preferred or more readily available. This finding further reinforces
    the benefits of SheepDog’s design with a LM backbone, which flexibly adapts it
    to task-specific needs.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 SheepDog 的灵活性，我们评估了 SheepDog 与三种代表性 LM 骨干结合的表现：RoBERTa、BERT 和 DeBERTa。如表[5](#S6.T5
    "Table 5 ‣ 6.4\. Adaptability to Different LM Backbones ‣ 6\. Experiments ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")所示，SheepDog 显著提升了相应 LM 骨干的性能。这种适应性突显了 SheepDog 的鲁棒性，从风格无关的训练和内容专注的归因中，展示了其在各种上下文中灵活部署的潜力，在这些上下文中可能会偏好或更容易获得不同的
    LM。这个发现进一步加强了 SheepDog 设计的好处，它通过 LM 骨干灵活地适应了任务特定的需求。'
- en: Table 4\. SheepDog achieves performance (%) that is either better or comparable
    to competitive baselines on the unperturbed original test sets. Bold (underlined)
    values indicate the best overall (baseline) performance, and ^∗ indicates $p<.01$
    using the Wilcoxon signed-rank test (Wilcoxon, [1945](#bib.bib47)).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 表4\. SheepDog 在未经干扰的原始测试集上取得的表现（%）要么优于，要么与竞争基线相当。粗体（下划线）值表示最佳整体（基线）表现，^∗ 表示使用
    Wilcoxon 符号秩检验（Wilcoxon, [1945](#bib.bib47)）$p<.01$。
- en: '| Method | PolitiFact |  | GossipCop |  | LUN |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PolitiFact |  | GossipCop |  | LUN |'
- en: '| Acc. | F1 |  | Acc. | F1 |  | Acc. | F1 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | F1 |  | 准确率 | F1 |  | 准确率 | F1 |'
- en: '| dEFEND\c | 82.67 | 82.59 |  | 70.85 | 70.74 |  | 81.33 | 80.92 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| dEFEND\c | 82.67 | 82.59 |  | 70.85 | 70.74 |  | 81.33 | 80.92 |'
- en: '| SAFE\v | 79.89 | 79.85 |  | 70.71 | 70.64 |  | 79.93 | 79.46 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| SAFE\v | 79.89 | 79.85 |  | 70.71 | 70.64 |  | 79.93 | 79.46 |'
- en: '| SentGCN | 81.11 | 80.77 |  | 69.38 | 69.29 |  | 80.07 | 79.66 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| SentGCN | 81.11 | 80.77 |  | 69.38 | 69.29 |  | 80.07 | 79.66 |'
- en: '| DualEmo | 87.78 | 87.76 |  | 75.51 | 75.36 |  | 81.78 | 81.52 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| DualEmo | 87.78 | 87.76 |  | 75.51 | 75.36 |  | 81.78 | 81.52 |'
- en: '| BERT | 85.22 | 84.99 |  | 74.60 | 74.50 |  | 81.13 | 80.96 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 85.22 | 84.99 |  | 74.60 | 74.50 |  | 81.13 | 80.96 |'
- en: '| RoBERTa | 88.00 | 87.40 |  | 74.14 | 74.05 |  | 82.53 | 82.12 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 88.00 | 87.40 |  | 74.14 | 74.05 |  | 82.53 | 82.12 |'
- en: '| DeBERTa | 86.33 | 86.30 |  | 73.86 | 73.80 |  | 84.01 | 83.67 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa | 86.33 | 86.30 |  | 73.86 | 73.80 |  | 84.01 | 83.67 |'
- en: '| UDA | 87.77 | 87.74 |  | 74.28 | 74.22 |  | 83.02 | 82.94 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| UDA | 87.77 | 87.74 |  | 74.28 | 74.22 |  | 83.02 | 82.94 |'
- en: '| PET | 85.56 | 85.51 |  | 74.75 | 74.63 |  | 84.00 | 83.66 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| PET | 85.56 | 85.51 |  | 74.75 | 74.63 |  | 84.00 | 83.66 |'
- en: '| KPT | 87.78 | 87.70 |  | 74.38 | 74.23 |  | 84.40 | 84.06 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| KPT | 87.78 | 87.70 |  | 74.38 | 74.23 |  | 84.40 | 84.06 |'
- en: '| GPT3.5 | 71.11 | 69.61 |  | 61.49 | 56.30 |  | 80.67 | 79.97 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 | 71.11 | 69.61 |  | 61.49 | 56.30 |  | 80.67 | 79.97 |'
- en: '| InstructGPT | 67.78 | 64.59 |  | 58.33 | 50.38 |  | 70.87 | 68.16 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| InstructGPT | 67.78 | 64.59 |  | 58.33 | 50.38 |  | 70.87 | 68.16 |'
- en: '| Llama2-13B | 65.56 | 63.15 |  | 55.74 | 53.54 |  | 72.47 | 70.97 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13B | 65.56 | 63.15 |  | 55.74 | 53.54 |  | 72.47 | 70.97 |'
- en: '| SheepDog | 88.44 | 88.39 |  | 75.77 | 75.75 |  | 93.05^∗ | 93.04^∗ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog | 88.44 | 88.39 |  | 75.77 | 75.75 |  | 93.05^∗ | 93.04^∗ |'
- en: Table 5\. On different LM backbones, SheepDog demonstrates stable and significant
    improvements over the most competitive baseline (in F1 %). Statistical significance
    over the corresponding LM backbone is computed using the Wilcoxon signed-rank
    test (Wilcoxon, [1945](#bib.bib47)) and indicated with ^∗ ($p<.01$).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 在不同的 LM 骨干上，SheepDog 相较于最具竞争力的基线展示了稳定且显著的提升（以 F1 % 计算）。相对于相应的 LM 骨干的统计显著性使用
    Wilcoxon 符号秩检验（Wilcoxon, [1945](#bib.bib47)）计算，并以 ^∗ ($p<.01$) 表示。
- en: '| Method | PolitiFact | GossipCop | LUN |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PolitiFact | GossipCop | LUN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RoBERTa | 76.17 | 71.00 | 52.47 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| RoBERTa | 76.17 | 71.00 | 52.47 |'
- en: '| SheepDog-RoBERTa | 80.99^∗ | 74.45^∗ | 85.63^∗ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog-RoBERTa | 80.99^∗ | 74.45^∗ | 85.63^∗ |'
- en: '| BERT | 72.31 | 68.98 | 56.35 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| BERT | 72.31 | 68.98 | 56.35 |'
- en: '| SheepDog-BERT | 81.37^∗ | 73.54^∗ | 80.36^∗ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog-BERT | 81.37^∗ | 73.54^∗ | 80.36^∗ |'
- en: '| DeBERTa | 74.57 | 70.95 | 53.33 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| DeBERTa | 74.57 | 70.95 | 53.33 |'
- en: '| SheepDog-DeBERTa | 81.10^∗ | 73.89^∗ | 82.58^∗ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog-DeBERTa | 81.10^∗ | 73.89^∗ | 82.58^∗ |'
- en: 6.5\. Ablation Study
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 消融研究
- en: 'To gain deeper insights into the functioning of SheepDog and the role of its
    different components, we conduct an ablation study to evaluate the respective
    contributions of LLM-empowered news reframings and content-focused veracity attributions.
    We compare SheepDog with the following three model variants:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解 SheepDog 的功能及其不同组件的作用，我们进行了一项消融研究，以评估 LL-empowered 新闻重新框架和内容聚焦的真实性归属的各自贡献。我们将
    SheepDog 与以下三种模型变体进行比较：
- en: •
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SheepDog w/ 2-layer MLP, which employs 2-layer MLPs with hidden size of $64$
    as attribution detector and veracity detector.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SheepDog 使用 2 层 MLP，该 MLP 具有 $64$ 的隐藏层大小，用于归属检测器和真实性检测器。
- en: •
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SheepDog-R, which excludes style-diverse news reframings and the style-agnostic
    training component.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SheepDog-R，不包括风格多样的新闻重新框架和风格无关的训练组件。
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: SheepDog-A, which omits content-focused veracity attributions and the attribution
    prediction component.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SheepDog-A，省略了内容聚焦的真实性归属和归属预测组件。
- en: 'Table [6](#S6.T6 "Table 6 ‣ 6.5\. Ablation Study ‣ 6\. Experiments ‣ Fake News
    in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style Attacks")
    shows the performance of different SheepDog variants on adversarial test sets
    C and D. The results suggest that: (1) SheepDog-R without news reframings only
    yields slight improvements compared with a fine-tuned RoBERTa model, which suggests
    the pivotal role of diverse reframings in SheepDog’s robustness. (2) Removing
    content-focused veracity attributions (SheepDog-A) results in inferior performance
    than SheepDog, which implies the effectiveness of extracting content-related knowledge
    from LLMs. (3) SheepDog, utilizing one single layer for veracity prediction and
    attribute predictions, slightly outperforms the variant employing 2-layer MLPs.
    This suggests that the expressiveness of LMs, harnessed through our proposed objectives,
    yields article representations that contains rich indicators for both attributions
    and veracity.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#S6.T6 "表 6 ‣ 6.5\. 消融研究 ‣ 6\. 实验 ‣ 羊皮下的假新闻：针对 LL-empowered 风格攻击的鲁棒假新闻检测")
    显示了不同 SheepDog 变体在对抗性测试集 C 和 D 上的表现。结果表明：（1）没有新闻重新框架的 SheepDog-R 相较于微调的 RoBERTa
    模型仅有轻微提升，这表明了多样化重新框架在 SheepDog 鲁棒性中的关键作用。（2）去除内容聚焦的真实性归属（SheepDog-A）导致的性能低于 SheepDog，这表明从
    LLM 中提取内容相关知识的有效性。（3）SheepDog 使用单层进行真实性预测和属性预测，略微优于使用 2 层 MLP 的变体。这表明，通过我们提出的目标利用的
    LM 的表达能力，生成的文章表示包含了丰富的归属和真实性指示。
- en: Table 6\. Ablation of SheepDog demonstrates benefits of LLM-empowered news reframing
    (denoted as R) and content-focused veracity attributions (denoted as A) in F1
    Score (%).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. SheepDog 的消融研究展示了 LL-empowered 新闻重新框架（标记为 R）和内容聚焦的真实性归属（标记为 A）在 F1 分数（%）上的优势。
- en: '| Method | PolitiFact | GossipCop | LUN |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PolitiFact | GossipCop | LUN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SheepDog | 80.99 | 74.45 | 85.63 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog | 80.99 | 74.45 | 85.63 |'
- en: '|   w/ 2-layer MLP | 79.83 | 74.03 | 84.75 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|   使用 2 层 MLP | 79.83 | 74.03 | 84.75 |'
- en: '|   - R | 76.71 | 70.98 | 53.27 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '|   - R | 76.71 | 70.98 | 53.27 |'
- en: '|   - A | 80.73 | 73.74 | 84.83 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|   - A | 80.73 | 73.74 | 84.83 |'
- en: '|   RoBERTa | 76.17 | 71.00 | 52.47 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|   RoBERTa | 76.17 | 71.00 | 52.47 |'
- en: 6.6\. Stability Across Reframing Prompts
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 重新框架提示的稳定性
- en: 'As detailed in Section [5.1](#S5.SS1 "5.1\. LLM-Empowered News Reframing ‣
    5\. Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), we utilize an LLM to generate two types
    of reframings for a given training article , and an unreliable-style reframing
    denoted as $p_{F}$. To assess SheepDog’s stability across different reframing
    prompts, we systematically examine its performance using four diverse combinations
    of prompts, denoted as R1 through R4.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如第 [5.1](#S5.SS1 "5.1\. LLM 授权新闻重构 ‣ 5\. 提议的方法 ‣ 假新闻中的羊毛：针对 LLM 授权风格攻击的鲁棒假新闻检测")
    节详细描述，我们利用 LLM 生成给定训练文章的两种类型的重构，和一种不可靠风格的重构，表示为 $p_{F}$。为了评估 SheepDog 在不同重构提示下的稳定性，我们系统地使用四种不同的提示组合进行性能检查，标记为
    R1 到 R4。
- en: 'Recall that we adopt the following template for news reframing:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们采用的新闻重构模板如下：
- en: 'Rewrite the following article
    in a / an [specified] tone: [$p$]'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rewrite the following article
    in a / an [specified] tone: [$p$]'
- en: 'For R1 through R4, the specified tones are defined as:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 R1 到 R4，指定的语调定义如下：
- en: •
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R1:  - “emotionally triggering”.'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'R1:  - “情感触发”。'
- en: •
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R2:  - “sensational”.'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'R2:  - “轰动效应”。'
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R3:  - “emotionally triggering”.'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'R3:  - “情感触发”。'
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R4:  - “sensational”.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'R4:  - “轰动效应”。'
- en: 'As shown in Table [7](#S6.T7 "Table 7 ‣ 6.6\. Stability Across Reframing Prompts
    ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), SheepDog consistently demonstrates stable
    and significant improvements over the most competitive baseline. This validates
    the generalizability of our approach, suggesting its potential in effectively
    combating deceptive online information in the ever-evolving digital landscape.
    Notably, our SheepDog approach conducts sampling between two reliable-style reframings
    and two unreliable-style reframings, leading to better versatility.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [7](#S6.T7 "表 7 ‣ 6.6\. 在重构提示下的稳定性 ‣ 6\. 实验 ‣ 假新闻中的羊毛：针对 LLM 授权风格攻击的鲁棒假新闻检测")
    所示，SheepDog 始终展现出相对于最具竞争力基准的稳定而显著的改进。这验证了我们方法的普适性，表明其在不断发展的数字环境中有效对抗虚假在线信息的潜力。值得注意的是，我们的
    SheepDog 方法在两种可靠风格重构和两种不可靠风格重构之间进行采样，从而提高了通用性。
- en: 'Table 7\. Across different sets of news reframing prompts, SheepDog demonstrates
    stable and significant improvements over the most competitive baseline (in F1
    %). (“Baseline”: performance of the most competitive baseline.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 在不同的新闻重构提示集下，SheepDog 展示了相对于最具竞争力基准（以 F1 % 计）稳定而显著的改进。（“基准”：最具竞争力基准的表现。）
- en: '| Method | PolitiFact | GossipCop | LUN |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PolitiFact | GossipCop | LUN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Baseline | 77.60 | 71.60 | 66.34 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 77.60 | 71.60 | 66.34 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SheepDog | 80.99 | 74.45 | 85.63 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| SheepDog | 80.99 | 74.45 | 85.63 |'
- en: '|   w/ R1 | 79.02 | 74.22 | 77.42 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|   w/ R1 | 79.02 | 74.22 | 77.42 |'
- en: '|   w/ R2 | 79.93 | 74.16 | 86.18 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|   w/ R2 | 79.93 | 74.16 | 86.18 |'
- en: '|   w/ R3 | 80.36 | 73.55 | 76.77 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|   w/ R3 | 80.36 | 73.55 | 76.77 |'
- en: '|   w/ R4 | 79.71 | 74.01 | 85.55 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|   w/ R4 | 79.71 | 74.01 | 85.55 |'
- en: '![Refer to caption](img/1a471bdd9bdb256d0c63be8253c6b56a.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1a471bdd9bdb256d0c63be8253c6b56a.png)'
- en: Figure 3\. SheepDog yields robust predictions against LLM-camouflaged fake news.
    Additionally, the top predicted veracity attribution points toward the false and
    misleading information in the news content, providing interpretability.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. SheepDog 对 LLM伪装的假新闻提供了可靠的预测。此外，最高预测的真实性归因指向新闻内容中的虚假和误导信息，提供了解释性。
- en: 6.7\. Case Study
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 案例研究
- en: 'To illustrate SheepDog’s potential for interpretability in real-world scenarios,
    we conduct a case study based on a fake news sample from the LUN test set. The
    baseline RoBERTa model correctly identifies the original article as fake, but
    mistakenly categorizes two style-transformed adversarial articles as real news
    under LLM-empowered style attacks. In contrast, our SheepDog approach adeptly
    detects the original article as fake, as well as the two adversarial counterparts.
    As shown in Figure [3](#S6.F3 "Figure 3 ‣ 6.6\. Stability Across Reframing Prompts
    ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), the article falsely asserts the efficacy
    of cannabis oil in treating cancer, a claim clearly at odds with established medical
    knowledge. By leveraging the softmax-converted probabilities derived from the
    attribution-level prediction scores (refer to Eq. [5](#S5.E5 "In 5.3\. Content-Focused
    Veracity Attributions ‣ 5\. Proposed Approach ‣ Fake News in Sheep’s Clothing:
    Robust Fake News Detection Against LLM-Empowered Style Attacks")), our SheepDog
    model effectively explains the article’s dubious nature as “false or misleading
    information”. Combining content-focused veracity attributions with style-agnostic
    training, SheepDog offers practitioners a valuable tool for reliably discerning
    and interpreting news veracity.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示SheepDog在现实场景中的可解释性潜力，我们基于LUN测试集中的一个假新闻样本进行了案例研究。基线RoBERTa模型正确地将原始文章识别为假新闻，但在LLM赋能的风格攻击下错误地将两个风格转换的对抗文章分类为真实新闻。相比之下，我们的SheepDog方法能够熟练地检测出原始文章为假新闻，同时也识别出两个对抗样本。如图[3](#S6.F3
    "Figure 3 ‣ 6.6\. Stability Across Reframing Prompts ‣ 6\. Experiments ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")所示，文章虚假地声称大麻油在治疗癌症方面有效，这一主张明显与既定医学知识相矛盾。通过利用从归属级预测分数中转换而来的softmax概率（参见方程[5](#S5.E5
    "In 5.3\. Content-Focused Veracity Attributions ‣ 5\. Proposed Approach ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")），我们的SheepDog模型有效地解释了文章的可疑性质为“虚假或误导性信息”。结合内容聚焦的真实性归属和风格无关的训练，SheepDog为从业者提供了一个可靠的工具，以辨别和解释新闻真实性。'
- en: 7\. Conclusion and Future Work
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论与未来工作
- en: In this paper, we address the critical aspect of style-related robustness in
    fake news detection. Motivated by our empirical finding on the susceptibility
    of state-of-the-art text-based detectors to LLM-empowered style attacks, we introduce
    SheepDog, a style-agnostic fake news detector that emphasizes content veracity
    over style. Leveraging the strengths of task-specific LM backbones and versatile
    general-purpose LLMs, SheepDog adopts a multi-task learning paradigm, which integrates
    style-agnostic training and content-focused veracity attributions. Extensive experiments
    on three benchmark datasets demonstrate SheepDog’s robustness and effectiveness
    across various style-based adversarial settings, news reframing prompts, and representative
    LM backbones. Moving forward, SheepDog lays a solid foundation for developing
    more resilient and adaptable models in the ever-changing online landscape, and
    demonstrates promising potential to be further extended to multi-modal scenarios.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们解决了假新闻检测中与风格相关的鲁棒性这一关键方面。受到我们对现有基于文本的检测器对LLM赋能风格攻击的敏感性的实证发现的启发，我们引入了SheepDog，一种风格无关的假新闻检测器，强调内容的真实性而非风格。利用任务特定的LM骨干网和通用的LLM的优势，SheepDog采用了多任务学习范式，结合了风格无关的训练和内容聚焦的真实性归属。对三个基准数据集的广泛实验展示了SheepDog在各种基于风格的对抗设置、新闻重新编排提示和代表性LM骨干网中的鲁棒性和有效性。展望未来，SheepDog为在不断变化的在线环境中开发更具韧性和适应性的模型奠定了坚实的基础，并展示了进一步扩展到多模态场景的有希望潜力。
- en: References
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Ajao et al. (2019) Oluwaseun Ajao, Deepayan Bhowmik, and Shahrzad Zargari. 2019.
    Sentiment Aware Fake News Detection on Online Social Networks. In *ICASSP 2019
    - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*. 2507–2511.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ajao等人（2019）Oluwaseun Ajao、Deepayan Bhowmik和Shahrzad Zargari。2019年。《在线社交网络上的情感感知假新闻检测》。在*ICASSP
    2019 - 2019 IEEE国际声学、语音和信号处理会议（ICASSP）*。2507–2511。
- en: 'Bachmann et al. (2021) Philipp Bachmann, Mark Eisenegger, and Diana Ingenhoff.
    2021. Defining and Measuring News Media Quality: Comparing the Content Perspective
    and the Audience Perspective. *The International Journal of Press/Politics* 27
    (03 2021).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bachmann等人（2021）Philipp Bachmann、Mark Eisenegger和Diana Ingenhoff。2021年。《定义和衡量新闻媒体质量：比较内容视角和受众视角》。*国际新闻/政治杂志*
    27 (03 2021)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. In *Advances in Neural Information
    Processing Systems*, Vol. 33\. 1877–1901.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 2020. 语言模型是少样本学习者。在*Advances in Neural Information Processing Systems*，第33卷。1877–1901。
- en: Chen and Shu (2023) Canyu Chen and Kai Shu. 2023. Can LLM-Generated Misinformation
    Be Detected? arXiv:2309.13788
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen and Shu (2023) Canyu Chen 和 Kai Shu. 2023. LLM生成的虚假信息能被检测吗？arXiv:2309.13788
- en: 'Cui et al. (2020) Limeng Cui, Haeseung Seo, Maryam Tabar, Fenglong Ma, Suhang
    Wang, and Dongwon Lee. 2020. DETERRENT: Knowledge Guided Graph Attention Network
    for Detecting Healthcare Misinformation. In *Proceedings of the 26th ACM SIGKDD
    International Conference on Knowledge Discovery & Data Mining*. 492–502.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cui et al. (2020) Limeng Cui, Haeseung Seo, Maryam Tabar, Fenglong Ma, Suhang
    Wang, 和 Dongwon Lee. 2020. DETERRENT: 知识指导图注意网络用于检测医疗保健虚假信息。在*Proceedings of the
    26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining*。492–502。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *NAACL*. 4171–4186.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova.
    2019. BERT: 深度双向转换器的预训练用于语言理解。在*NAACL*。4171–4186。'
- en: 'Dun et al. (2021) Yaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, and Xiaojie
    Yuan. 2021. KAN: Knowledge-aware Attention Network for Fake News Detection. *Proceedings
    of the AAAI Conference on Artificial Intelligence* 35, 1 (2021), 81–89.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dun et al. (2021) Yaqian Dun, Kefei Tu, Chen Chen, Chunyan Hou, 和 Xiaojie Yuan.
    2021. KAN: 知识感知注意网络用于假新闻检测。在*Proceedings of the AAAI Conference on Artificial
    Intelligence* 35, 1 (2021), 81–89。'
- en: Epstein et al. (2023) Ziv Epstein, Mengying C Fang, Antonio A Arechar, and David G
    Rand. 2023. What label should be applied to content produced by generative AI?
    [https://doi.org/10.31234/osf.io/v4mfz](https://doi.org/10.31234/osf.io/v4mfz)
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Epstein et al. (2023) Ziv Epstein, Mengying C Fang, Antonio A Arechar, 和 David
    G Rand. 2023. 应该将什么标签应用于生成AI生产的内容？[https://doi.org/10.31234/osf.io/v4mfz](https://doi.org/10.31234/osf.io/v4mfz)
- en: 'He et al. (2021a) Bing He, Mustaque Ahamad, and Srijan Kumar. 2021a. PETGEN:
    Personalized Text Generation Attack on Deep Sequence Embedding-Based Classification
    Models. In *Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery
    & Data Mining*. 575–584.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2021a) Bing He, Mustaque Ahamad, 和 Srijan Kumar. 2021a. PETGEN:
    针对深度序列嵌入分类模型的个性化文本生成攻击。在*Proceedings of the 27th ACM SIGKDD Conference on Knowledge
    Discovery & Data Mining*。575–584。'
- en: 'He et al. (2021b) Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
    2021b. DeBERTa: Decoding-enhanced BERT with Disentangled Attention. In *International
    Conference on Learning Representations*.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He et al. (2021b) Pengcheng He, Xiaodong Liu, Jianfeng Gao, 和 Weizhu Chen.
    2021b. DeBERTa: 解码增强的BERT与分离注意。在*International Conference on Learning Representations*。'
- en: 'He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann
    LeCun, and Bryan Hooi. 2023. Harnessing Explanations: LLM-to-LM Interpreter for
    Enhanced Text-Attributed Graph Representation Learning. arXiv:2305.19523 [cs.LG]'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann
    LeCun, 和 Bryan Hooi. 2023. 利用解释：LLM到LM解释器用于增强的文本属性图表示学习。arXiv:2305.19523 [cs.LG]
- en: 'Henrique et al. (2023) Da Silva Gameiro Henrique, Andrei Kucharavy, and Rachid
    Guerraoui. 2023. Stochastic Parrots Looking for Stochastic Parrots: LLMs are Easy
    to Fine-Tune and Hard to Detect with other LLMs. arXiv:2304.08968 [cs.CL]'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henrique et al. (2023) Da Silva Gameiro Henrique, Andrei Kucharavy, 和 Rachid
    Guerraoui. 2023. 随机鹦鹉寻找随机鹦鹉：LLM很容易微调，但难以用其他LLM检测。arXiv:2304.08968 [cs.CL]
- en: Higdon (2020) Nolan Higdon. 2020. What is Fake News? A Foundational Question
    for Developing Effective Critical News Literacy Education. *Democratic Communiqué*
    279 (03 2020). Issue 1.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Higdon (2020) Nolan Higdon. 2020. 什么是假新闻？发展有效的批判性新闻素养教育的基础性问题。*Democratic Communiqué*
    279 (03 2020)。第1期。
- en: Horne et al. (2019) Benjamin D. Horne, Jeppe Nørregaard, and Sibel Adali. 2019.
    Robust Fake News Detection Over Time and Attack. *ACM Trans. Intell. Syst. Technol.*
    11, 1, Article 7 (2019), 23 pages.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horne 等人（2019）本杰明·D·霍恩、耶佩·诺尔雷加德和西贝尔·阿达利。2019。**稳健的假新闻检测**：随时间和攻击的鲁棒性。 *ACM智能系统技术汇刊*
    11, 1, 第7篇（2019），共23页。
- en: 'Hu et al. (2023) Beizhe Hu, Qiang Sheng, Juan Cao, Yongchun Zhu, Danding Wang,
    Zhengjia Wang, and Zhiwei Jin. 2023. Learn over Past, Evolve for Future: Forecasting
    Temporal Trends for Fake News Detection. In *Proceedings of the 61st Annual Meeting
    of the Association for Computational Linguistics (Volume 5: Industry Track)*.
    116–125.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2023）北哲·胡、强·盛、娟·曹、永春·朱、丹丁·王、正佳·王和志伟·金。2023。**从过去学习，为未来演变**：预测假新闻检测的时间趋势。见于
    *第61届计算语言学协会年会论文集（第5卷：产业追踪）*。第116–125页。
- en: 'Hu et al. (2022a) Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang
    Wang, Juanzi Li, Wei Wu, and Maosong Sun. 2022a. Knowledgeable Prompt-tuning:
    Incorporating Knowledge into Prompt Verbalizer for Text Classification. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. 2225–2240.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2022a）圣丁·胡、宁·丁、华东·王、志远·刘、景刚·王、娟子·李、魏·吴和茂松·孙。2022a。**知识驱动的提示调优**：将知识融入提示词汇生成器用于文本分类。见于
    *第60届计算语言学协会年会论文集（第1卷：长篇论文）*。第2225–2240页。
- en: 'Hu et al. (2022b) Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Jingang
    Wang, Juanzi Li, Wei Wu, and Maosong Sun. 2022b. Knowledgeable Prompt-tuning:
    Incorporating Knowledge into Prompt Verbalizer for Text Classification. In *Proceedings
    of the 60th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*. 2225–2240.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2022b）圣丁·胡、宁·丁、华东·王、志远·刘、景刚·王、娟子·李、魏·吴和茂松·孙。2022b。**知识驱动的提示调优**：将知识融入提示词汇生成器用于文本分类。见于
    *第60届计算语言学协会年会论文集（第1卷：长篇论文）*。第2225–2240页。
- en: 'Huang and Chang (2023) Jie Huang and Kevin Chen-Chuan Chang. 2023. Towards
    Reasoning in Large Language Models: A Survey. In *Findings of the Association
    for Computational Linguistics: ACL 2023*. 1049–1065.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Chang（2023）杰·黄和凯文·陈泉·张。2023。**面向大型语言模型的推理**：一项调查。见于 *计算语言学协会发现：ACL 2023*。第1049–1065页。
- en: 'Huang et al. (2023) Kung-Hsiang Huang, Kathleen McKeown, Preslav Nakov, Yejin
    Choi, and Heng Ji. 2023. Faking Fake News for Real Fake News Detection: Propaganda-Loaded
    Training Data Generation. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 14571–14589.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2023）孔祥·黄、凯瑟琳·麦基恩、普雷斯拉夫·纳科夫、叶进·崔和恒·季。2023。**伪造假新闻以实现真实假新闻检测**：宣传加载的训练数据生成。见于
    *第61届计算语言学协会年会论文集（第1卷：长篇论文）*。第14571–14589页。
- en: Koenders et al. (2021) Camille Koenders, Johannes Filla, Nicolai Schneider,
    and Vinicius Woloszyn. 2021. How Vulnerable Are Automatic Fake News Detection
    Methods to Adversarial Attacks? arXiv:2107.07970 [cs.CL]
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koenders 等人（2021）卡米尔·科恩德斯、约翰内斯·费拉、尼古拉·施耐德和维尼修斯·沃洛申。2021。**自动假新闻检测方法对对抗攻击的脆弱性**？arXiv:2107.07970
    [cs.CL]
- en: 'Le et al. (2020) Thai Le, Suhang Wang, and Dongwon Lee. 2020. MALCOM: Generating
    Malicious Comments to Attack Neural Fake News Detection Models. In *2020 IEEE
    International Conference on Data Mining (ICDM)*. 282–291.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等人（2020）泰·李、苏杭·王和东元·李。2020。**MALCOM**：生成恶意评论以攻击神经网络假新闻检测模型。见于 *2020 IEEE国际数据挖掘会议（ICDM）*。第282–291页。
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692 [cs.CL]'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2019）银汉·刘、迈尔·奥特、纳曼·戈亚尔、静飞·杜、曼达尔·乔希、丹奇·陈、奥默·莱维、迈克·刘易斯、卢克·泽特尔摩耶和韦瑟林·斯托扬诺夫。2019。**RoBERTa**：一种稳健优化的BERT预训练方法。arXiv:1907.11692
    [cs.CL]
- en: 'Lu and Li (2020) Yi-Ju Lu and Cheng-Te Li. 2020. GCAN: Graph-aware Co-Attention
    Networks for Explainable Fake News Detection on Social Media. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*.
    505–514.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 和 Li（2020）怡举·卢和程德·李。2020。**GCAN**：图感知的共注意力网络用于社交媒体上可解释的假新闻检测。见于 *第58届计算语言学协会年会论文集*。第505–514页。
- en: Lyu et al. (2023) Yuefei Lyu, Xiaoyu Yang, Jiaxin Liu, Sihong Xie, Philip Yu,
    and Xi Zhang. 2023. Interpretable and Effective Reinforcement Learning for Attacking
    against Graph-based Rumor Detection. In *2023 International Joint Conference on
    Neural Networks (IJCNN)*. 1–9.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu 等人（2023）岳飞·吕、晓宇·杨、家欣·刘、思宏·谢、菲利普·余和希·张。2023。**可解释且有效的强化学习**用于针对基于图的谣言检测攻击。见于
    *2023年国际神经网络联合会议（IJCNN）*。第1–9页。
- en: Menon and Vondrick (2023) Sachit Menon and Carl Vondrick. 2023. Visual Classification
    via Description from Large Language Models. In *The Eleventh International Conference
    on Learning Representations*.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menon和Vondrick (2023) Sachit Menon 和 Carl Vondrick. 2023. 通过大语言模型进行视觉分类。在*第十一届国际学习表征会议*上。
- en: Muñoz-Ortiz et al. (2023) Alberto Muñoz-Ortiz, Carlos Gómez-Rodríguez, and David
    Vilares. 2023. Contrasting Linguistic Patterns in Human and LLM-Generated Text.
    arXiv:2308.09067 [cs.CL]
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muñoz-Ortiz et al. (2023) Alberto Muñoz-Ortiz, Carlos Gómez-Rodríguez, 和 David
    Vilares. 2023. 人类和LLM生成文本中的语言模式对比。arXiv:2308.09067 [cs.CL]
- en: 'Nan et al. (2021) Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, and Jintao
    Li. 2021. MDFEND: Multi-domain fake news detection. In *Proceedings of the 30th
    ACM International Conference on Information & Knowledge Management*. 3343–3347.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nan et al. (2021) Qiong Nan, Juan Cao, Yongchun Zhu, Yanyan Wang, 和 Jintao
    Li. 2021. MDFEND: 多领域假新闻检测。在*第30届ACM国际信息与知识管理会议论文集*中，3343–3347。'
- en: Nan et al. (2022) Qiong Nan, Danding Wang, Yongchun Zhu, Qiang Sheng, Yuhui
    Shi, Juan Cao, and Jintao Li. 2022. Improving Fake News Detection of Influential
    Domain via Domain- and Instance-Level Transfer. In *Proceedings of the 29th International
    Conference on Computational Linguistics*. 2834–2848.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nan et al. (2022) Qiong Nan, Danding Wang, Yongchun Zhu, Qiang Sheng, Yuhui
    Shi, Juan Cao, 和 Jintao Li. 2022. 通过领域和实例级转移改进有影响力领域的假新闻检测。在*第29届国际计算语言学会议论文集*中，2834–2848。
- en: 'Nguyen et al. (2020) Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, and
    Min-Yen Kan. 2020. FANG: Leveraging Social Context for Fake News Detection Using
    Graph Representation. In *Proceedings of the 29th ACM International Conference
    on Information Knowledge Management*. 1165–1174.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen et al. (2020) Van-Hoang Nguyen, Kazunari Sugiyama, Preslav Nakov, 和
    Min-Yen Kan. 2020. FANG: 利用社交上下文进行假新闻检测的图表示。在*第29届ACM国际信息与知识管理会议论文集*中，1165–1174。'
- en: 'OpenAI (2022) OpenAI. 2022. ChatGPT: Optimizing language models for dialogue.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2022) OpenAI. 2022. ChatGPT: 为对话优化语言模型。'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
    [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf)
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4技术报告。arXiv:2303.08774 [cs.CL] [https://arxiv.org/pdf/2303.08774.pdf](https://arxiv.org/pdf/2303.08774.pdf)
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. In *Advances in Neural
    Information Processing Systems*, Vol. 35\. Curran Associates, Inc., 27730–27744.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, 和 Ryan Lowe. 2022. 训练语言模型以跟随人类反馈的指令。在*神经信息处理系统进展*中，第35卷。Curran
    Associates, Inc.，27730–27744。
- en: 'Pan et al. (2023) Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William Yang
    Wang, Min-Yen Kan, and Preslav Nakov. 2023. Fact-Checking Complex Claims with
    Program-Guided Reasoning. In *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 6981–7004.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2023) Liangming Pan, Xiaobao Wu, Xinyuan Lu, Anh Tuan Luu, William
    Yang Wang, Min-Yen Kan, 和 Preslav Nakov. 2023. 使用程序引导推理核实复杂主张。在*第61届计算语言学协会年会（第1卷：长篇论文）*中，6981–7004。
- en: Pelrine et al. (2021) Kellin Pelrine, Jacob Danovitch, and Reihaneh Rabbany.
    2021. The Surprising Performance of Simple Baselines for Misinformation Detection.
    In *Proceedings of the Web Conference 2021*. 3432–3441.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pelrine et al. (2021) Kellin Pelrine, Jacob Danovitch, 和 Reihaneh Rabbany. 2021.
    简单基线在虚假信息检测中的惊人表现。在*2021年网络会议论文集*中，3432–3441。
- en: 'Potthast et al. (2018) Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek
    Bevendorff, and Benno Stein. 2018. A Stylometric Inquiry into Hyperpartisan and
    Fake News. In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*. 231–240.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Potthast et al. (2018) Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek
    Bevendorff, 和 Benno Stein. 2018. 超偏见和假新闻的风格分析研究。在*第56届计算语言学协会年会（第1卷：长篇论文）*中，231–240。
- en: 'Rashkin et al. (2017) Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova,
    and Yejin Choi. 2017. Truth of Varying Shades: Analyzing Language in Fake News
    and Political Fact-Checking. In *Proceedings of the 2017 Conference on Empirical
    Methods in Natural Language Processing*. Association for Computational Linguistics,
    2931–2937.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashkin 等 (2017) Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana Volkova,
    和 Yejin Choi. 2017. 各种色调的真相：分析假新闻和政治事实核查中的语言。在 *2017年自然语言处理实证方法会议论文集* 中. 计算语言学协会,
    2931–2937.
- en: 'Ruchansky et al. (2017) Natali Ruchansky, Sungyong Seo, and Yan Liu. 2017.
    CSI: A Hybrid Deep Model for Fake News Detection. In *Proceedings of the 2017
    ACM on Conference on Information and Knowledge Management*. 797–806.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruchansky 等 (2017) Natali Ruchansky, Sungyong Seo, 和 Yan Liu. 2017. CSI：用于假新闻检测的混合深度模型。在
    *2017年ACM信息与知识管理大会论文集* 中. 797–806.
- en: 'Schick and Schütze (2021) Timo Schick and Hinrich Schütze. 2021. Exploiting
    Cloze-Questions for Few-Shot Text Classification and Natural Language Inference.
    In *Proceedings of the 16th Conference of the European Chapter of the Association
    for Computational Linguistics: Main Volume*. 255–269.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schick 和 Schütze (2021) Timo Schick 和 Hinrich Schütze. 2021. 利用 Cloze 问题进行少样本文本分类和自然语言推理。在
    *第16届欧洲计算语言学协会会议论文集：主要篇章* 中. 255–269.
- en: 'Sheng et al. (2022) Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding
    Wang, and Yongchun Zhu. 2022. Zoom Out and Observe: News Environment Perception
    for Fake News Detection. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*. 4543–4556.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等 (2022) Qiang Sheng, Juan Cao, Xueyao Zhang, Rundong Li, Danding Wang,
    和 Yongchun Zhu. 2022. 放大并观察：新闻环境感知用于假新闻检测。在 *第60届计算语言学协会年会论文集 (第1卷：长篇论文)* 中. 4543–4556.
- en: 'Shu et al. (2019) Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, and Huan Liu.
    2019. DEFEND: Explainable Fake News Detection. In *Proceedings of the 25th ACM
    SIGKDD International Conference on Knowledge Discovery & Data Mining*. 395–405.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等 (2019) Kai Shu, Limeng Cui, Suhang Wang, Dongwon Lee, 和 Huan Liu. 2019.
    DEFEND：可解释的假新闻检测。在 *第25届ACM SIGKDD国际知识发现与数据挖掘大会论文集* 中. 395–405.
- en: 'Shu et al. (2020) Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and
    Huan Liu. 2020. FakeNewsNet: A Data Repository with News Content, Social Context,
    and Spatiotemporal Information for Studying Fake News on Social Media. *Big Data*
    8, 3 (2020), 171–188.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等 (2020) Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, 和 Huan
    Liu. 2020. FakeNewsNet：一个包含新闻内容、社交背景和时空信息的数据仓库，用于研究社交媒体上的假新闻。*大数据* 8, 3 (2020),
    171–188.
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
- en: Vaibhav et al. (2019) Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019.
    Do Sentence Interactions Matter? Leveraging Sentence Level Representations for
    Fake News Classification. In *Proceedings of the Thirteenth Workshop on Graph-Based
    Methods for Natural Language Processing (TextGraphs-13)*. 134–139.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaibhav et al. (2019) Vaibhav Vaibhav, Raghuram Mandyam, and Eduard Hovy. 2019.
    句子间的互动重要吗？利用句子级表示进行假新闻分类。见 *第十三届图基方法自然语言处理研讨会（TextGraphs-13）论文集*。134–139。
- en: Wang et al. (2023) Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip S.
    Yu, and Kai Shu. 2023. Attacking Fake News Detectors via Manipulating News Social
    Engagement. In *Proceedings of the ACM Web Conference 2023*. 3978–3986.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Haoran Wang, Yingtong Dou, Canyu Chen, Lichao Sun, Philip
    S. Yu, 和 Kai Shu. 2023. 通过操控新闻社交参与攻击假新闻检测器。见 *2023年ACM网络会议论文集*。3978–3986。
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. 2022. Emergent Abilities of Large Language Models. *Transactions on Machine
    Learning Research* (2022).
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, 和 William
    Fedus. 2022. 大型语言模型的突现能力。*机器学习研究交易* (2022)。
- en: Wilcoxon (1945) Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods.
    *Biometrics Bulletin* 1 (1945), 80–83.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilcoxon (1945) Frank Wilcoxon. 1945. Individual Comparisons by Ranking Methods.
    *Biometrics Bulletin* 1 (1945), 80–83.
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2020. Transformers: State-of-the-Art Natural Language Processing. In *Proceedings
    of the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations*. 38–45.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    等. 2020. Transformers：最先进的自然语言处理技术。见 *2020年自然语言处理经验方法会议：系统展示论文集*。38–45。
- en: 'Wu and Hooi (2023) Jiaying Wu and Bryan Hooi. 2023. DECOR: Degree-Corrected
    Social Graph Refinement for Fake News Detection. In *Proceedings of the 29th ACM
    SIGKDD Conference on Knowledge Discovery and Data Mining*. 2582–2593.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu and Hooi (2023) Jiaying Wu 和 Bryan Hooi. 2023. DECOR: 程度校正的社交图谱优化用于假新闻检测。见
    *第29届ACM SIGKDD知识发现与数据挖掘大会论文集*。2582–2593。'
- en: Xie et al. (2020) Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc
    Le. 2020. Unsupervised Data Augmentation for Consistency Training. In *Advances
    in Neural Information Processing Systems*, Vol. 33\. 6256–6268.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2020) Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, 和 Quoc Le.
    2020. 无监督数据增强用于一致性训练。见 *神经信息处理系统进展*，第33卷。6256–6268。
- en: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk,
    Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending Against Neural
    Fake News. In *Advances in Neural Information Processing Systems*, H. Wallach,
    H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.),
    Vol. 32\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk,
    Ali Farhadi, Franziska Roesner, 和 Yejin Choi. 2019. 防御神经假新闻。见 *神经信息处理系统进展*，H.
    Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox 和 R. Garnett（编），第32卷。Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2019/file/3e9f0fc9b2f89e043bc6233994dfcf76-Paper.pdf)
- en: Zhang et al. (2021) Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong,
    and Kai Shu. 2021. Mining Dual Emotion for Fake News Detection. In *Proceedings
    of the Web Conference 2021*. 3465–3476.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) Xueyao Zhang, Juan Cao, Xirong Li, Qiang Sheng, Lei Zhong,
    和 Kai Shu. 2021. 挖掘双重情感以检测假新闻。见 *2021年网络会议论文集*。3465–3476。
- en: 'Zheng and Zhan (2023) Haoyi Zheng and Huichun Zhan. 2023. ChatGPT in Scientific
    Writing: A Cautionary Tale. *The American Journal of Medicine* 136, 8 (2023),
    725–726.e6.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng and Zhan (2023) Haoyi Zheng 和 Huichun Zhan. 2023. ChatGPT在科学写作中的应用：一个警示故事。*美国医学杂志*
    136, 8 (2023), 725–726.e6。
- en: 'Zhou et al. (2020) Xinyi Zhou, Jindi Wu, and Reza Zafarani. 2020. SAFE: Similarity-Aware
    Multi-modal Fake News Detection. In *Advances in Knowledge Discovery and Data
    Mining*. 354–367.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2020) Xinyi Zhou, Jindi Wu, 和 Reza Zafarani. 2020. SAFE：相似性感知的多模态假新闻检测。见
    *知识发现与数据挖掘进展*。354–367。
- en: Zhou et al. (2019) Zhixuan Zhou, Huankang Guan, Meghana Bhat, and Justin Hsu.
    2019. Fake News Detection via NLP is Vulnerable to Adversarial Attacks. In *Proceedings
    of the 11th International Conference on Agents and Artificial Intelligence*.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2019） Zhixuan Zhou, Huankang Guan, Meghana Bhat 和 Justin Hsu. 2019. 假新闻检测通过
    NLP 易受到对抗攻击。见 *第十一届国际智能体与人工智能会议论文集*。
- en: 'Zhu et al. (2022a) Yongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li, Danding
    Wang, and Fuzhen Zhuang. 2022a. Generalizing to the Future: Mitigating Entity
    Bias in Fake News Detection. In *Proceedings of the 45nd International ACM SIGIR
    Conference on Research and Development in Information Retrieval*. Association
    for Computing Machinery.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2022a） Yongchun Zhu, Qiang Sheng, Juan Cao, Shuokai Li, Danding Wang 和
    Fuzhen Zhuang. 2022a. 面向未来的推广：减轻假新闻检测中的实体偏见。见 *第 45 届国际 ACM SIGIR 信息检索研究与发展会议论文集*。计算机协会。
- en: Zhu et al. (2022b) Yongchun Zhu, Qiang Sheng, Juan Cao, Qiong Nan, Kai Shu,
    Minghui Wu, Jindong Wang, and Fuzhen Zhuang. 2022b. Memory-Guided Multi-View Multi-Domain
    Fake News Detection. *IEEE Transactions on Knowledge and Data Engineering* (2022).
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2022b） Yongchun Zhu, Qiang Sheng, Juan Cao, Qiong Nan, Kai Shu, Minghui
    Wu, Jindong Wang 和 Fuzhen Zhuang. 2022b. Memory-Guided Multi-View Multi-Domain
    Fake News Detection. *IEEE Transactions on Knowledge and Data Engineering*（2022）。
- en: Appendix A Dataset Description
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集描述
- en: 'In Section [6](#S6 "6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust
    Fake News Detection Against LLM-Empowered Style Attacks"), we conduct extensive
    experiments on three widely-used real-world benchmark datasets: the FakeNewsNet
    benchmark (Shu et al., [2020](#bib.bib42)), which consists of the PolitiFact and
    GossipCop datasets, and the Labeled Unreliable News (LUN) dataset (Rashkin et al.,
    [2017](#bib.bib37)). PolitiFact and LUN contain news articles about political
    discourse, while GossipCop focuses on celebrity gossip.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [6](#S6 "6\. 实验 ‣ 假新闻伪装下的真实：对抗 LLM 驱动风格攻击的鲁棒假新闻检测") 中，我们对三个广泛使用的现实世界基准数据集进行了广泛实验：FakeNewsNet
    基准（Shu 等，[2020](#bib.bib42)），包括 PolitiFact 和 GossipCop 数据集，以及标记不可靠新闻（LUN）数据集（Rashkin
    等，[2017](#bib.bib37)）。PolitiFact 和 LUN 包含有关政治话语的新闻文章，而 GossipCop 侧重于名人八卦。
- en: 'In the case of the LUN dataset, which further classifies unreliable news into
    three sub-categories: satire, hoax, and propaganda, we conduct binary classification
    between reliable (real) and unreliable (fake) news, and ensure an equal number
    of unreliable news from each of these fine-grained categories. The detailed statistics
    for each dataset are shown as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LUN 数据集的情况下，该数据集进一步将不可靠新闻分类为三种子类别：讽刺、虚假和宣传，我们进行可靠（真实）与不可靠（虚假）新闻的二元分类，并确保每个细分类别中有相等数量的不可靠新闻。每个数据集的详细统计信息如下：
- en: Table 8\. Dataset statistics.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 数据集统计。
- en: '| Dataset | PolitiFact | GossipCop | LUN |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | PolitiFact | GossipCop | LUN |'
- en: '| --- | --- | --- | --- |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| # News Articles | 450 | 7,916 | 7,500 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| # 新闻文章 | 450 | 7,916 | 7,500 |'
- en: '| # Real News | 225 | 3,958 | 3,750 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| # 真实新闻 | 225 | 3,958 | 3,750 |'
- en: '| # Fake News | 225 | 3,958 | 3,750 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| # 假新闻 | 225 | 3,958 | 3,750 |'
- en: Appendix B Details of LLM Prompting
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLM 提示的详细信息
- en: B.1\. Prompt Template for Baseline LLM Detectors
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1\. 基线 LLM 检测器的提示模板
- en: 'In section [6.1.2](#S6.SS1.SSS2 "6.1.2\. Baselines ‣ 6.1\. Experimental Setup
    ‣ 6\. Experiments ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), we select three representative LLMs as
    baselines for fake news detection: GPT3.5 (OpenAI, [2022](#bib.bib31)) (version
    name: gpt-3.5-turbo), InstructGPT (Ouyang et al., [2022](#bib.bib33)) (version
    name: gpt-3.5-turbo-instruct), and Llama2-13B (Touvron et al., [2023](#bib.bib43))
    (version name: Llama-2-13b-chat-hf).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节 [6.1.2](#S6.SS1.SSS2 "6.1.2\. 基线 ‣ 6.1\. 实验设置 ‣ 6\. 实验 ‣ 假新闻伪装下的真实：对抗 LLM
    驱动风格攻击的鲁棒假新闻检测") 中，我们选择三种代表性的 LLM 作为假新闻检测的基线：GPT3.5（OpenAI，[2022](#bib.bib31)）（版本名称：gpt-3.5-turbo）、InstructGPT（Ouyang
    等，[2022](#bib.bib33)）（版本名称：gpt-3.5-turbo-instruct）和 Llama2-13B（Touvron 等，[2023](#bib.bib43)）（版本名称：Llama-2-13b-chat-hf）。
- en: 'For GPT-3.5 and InstructGPT, we utilize their APIs from OpenAI, and set the
    temperature to $0$ for stable veracity predictions. For Llama2-13B, we utilize
    the model weights from HuggingFace Transformers (Wolf et al., [2020](#bib.bib48))
    version 4.31.0, and set do_sample to False for greedy decoding. For all three
    LLMs, we adopt the following prompt to conduct zero-shot fake news detection:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GPT-3.5 和 InstructGPT，我们使用来自 OpenAI 的 API，并将温度设置为 $0$ 以获得稳定的真实性预测。对于 Llama2-13B，我们使用来自
    HuggingFace Transformers（Wolf 等，[2020](#bib.bib48)）版本 4.31.0 的模型权重，并将 do_sample
    设置为 False 以进行贪婪解码。对于这三种 LLM，我们采用以下提示进行零-shot 假新闻检测：
- en: 'Question: Does the following
    contain real or fake news (or information)? Answer in one word with either ‘Real’
    or ‘Fake’, then explain why. [news article] Answer: [starts with a predicted veracity
    label]'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 'Question: Does the following
    contain real or fake news (or information)? Answer in one word with either ‘Real’
    or ‘Fake’, then explain why. [news article] Answer: [starts with a predicted veracity
    label]'
- en: B.2\. Prompt Template for Obtaining SheepDog’s News Reframings
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 获取SheepDog新闻重构的提示模板
- en: 'Recall from Section [5.1](#S5.SS1 "5.1\. LLM-Empowered News Reframing ‣ 5\.
    Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks") that we generate reliable-style reframings
    and unreliable-style reframings with GPT3.5 for each labeled news article. Among
    the four prompt templates presented below, we use the first two templates to generate
    reliable-style reframings, and the other two to generate unreliable-style reframings.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '从第[5.1](#S5.SS1 "5.1\. LLM-Empowered News Reframing ‣ 5\. Proposed Approach
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")节中回忆，我们使用GPT3.5为每个标记的新闻文章生成可靠风格和不可靠风格的重构。在下面展示的四个提示模板中，我们使用前两个模板来生成可靠风格的重构，使用其他两个模板来生成不可靠风格的重构。'
- en: 'During reframing generation, we set the temperature to . A detailed reframing
    example is presented in Table [9](#A2.T9 "Table 9 ‣ B.3\. Prompt Template for
    Obtaining SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks").'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '在生成重构时，我们将温度设置为. 详细的重构示例见表[9](#A2.T9 "Table 9 ‣ B.3\. Prompt Template for Obtaining
    SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting ‣ Fake
    News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered Style
    Attacks")。'
- en: 'Rewrite the following article
    in an objective and professional tone: [news article]Rewrite the following article
    in a neutral tone: [news article]Rewrite the following article in an
    emotionally triggering tone: [news article]Rewrite the following article
    in a sensational tone: [news article]'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 用客观和专业的语气重写以下文章：[新闻文章]用中立的语气重写以下文章：[新闻文章]用情感刺激的语气重写以下文章：[新闻文章]用耸人听闻的语气重写以下文章：[新闻文章]
- en: B.3\. Prompt Template for Obtaining SheepDog’s Veracity Attributions
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3\. 获取SheepDog真实性归属的提示模板
- en: 'In Section [5.3](#S5.SS3 "5.3\. Content-Focused Veracity Attributions ‣ 5\.
    Proposed Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection
    Against LLM-Empowered Style Attacks"), we elicit veracity-related information
    based on the news content from GPT3.5, specifically by prompting the LLM to assess
    the labeled articles and their corresponding reframings based on a predefined
    set of content-focused rationales. We utilize the following prompt template:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '在第[5.3](#S5.SS3 "5.3\. Content-Focused Veracity Attributions ‣ 5\. Proposed
    Approach ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")节中，我们基于新闻内容从 GPT3.5 获取真实性相关信息，具体通过提示 LLM 评估标记文章及其对应的重新编排，依据预定义的内容聚焦理由。我们使用以下提示模板：'
- en: 'Article:
    [news article] Question: Which of the following problems does this article have?
    Lack of credible sources, False or misleading information, Biased opinion, Inconsistencies
    with reputable sources. If multiple options apply, provide a comma-separated list
    ordered from most to least related. Answer ”No problems” if none of the options
    apply.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 'Article:
    [news article] Question: Which of the following problems does this article have?
    Lack of credible sources, False or misleading information, Biased opinion, Inconsistencies
    with reputable sources. If multiple options apply, provide a comma-separated list
    ordered from most to least related. Answer ”No problems” if none of the options
    apply.'
- en: 'In the process of prompting GPT3.5 for veracity attributions, we set the temperature
    to . We demonstrate an example in Table [10](#A2.T10 "Table 10 ‣ B.3\. Prompt
    Template for Obtaining SheepDog’s Veracity Attributions ‣ Appendix B Details of
    LLM Prompting ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against
    LLM-Empowered Style Attacks").'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '在对 GPT3.5 进行真实性归属的过程中，我们将温度设置为。我们在表格[10](#A2.T10 "Table 10 ‣ B.3\. Prompt Template
    for Obtaining SheepDog’s Veracity Attributions ‣ Appendix B Details of LLM Prompting
    ‣ Fake News in Sheep’s Clothing: Robust Fake News Detection Against LLM-Empowered
    Style Attacks")中展示了一个示例。'
- en: Table 9\. An example of GPT-3.5 generated reliable-style news reframing.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9\. 一个 GPT-3.5 生成的可靠风格新闻重新编排的示例。
- en: '| PROMPT: Rewrite the following article in an objective and professional tone:
    BREAKING: Obamas Daughter BUSTED By The Cops For Doing This At the Democratic
    National Convention, Michelle Obama tried to use her daughters as shining examples
    of just how far America has come. But the fact is that the young Obamas are pushing
    the limits as any teenager doesand probably even further. According to new reports,
    Malia Obama was spotted at a party in Marthas Vineyard recently. It was eventually
    broken up by police because of noise complaints. The first daughter was immediately
    rushed from the party by Secret Service before the cops arrived. The West Tisbury
    police declined to confirm if Malia was there, citing national security. But the
    truth surfaced anyway. This is just one incident in a string of wild behavior
    on Malias part. Just a month ago the first daughter came under fire when photos
    surfaced of her smoking marijuana at a concert. Malia is clearly using her gap
    year before Harvard to party and experiment. Later, Malia and Barack were spotted
    strolling around a pool together having a chat. Were sure he had a few choice
    words for her. Clearly, a lack of respect for the law runs deep in this family.
    |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 提示：以客观和专业的语气重写以下文章：突发新闻：奥巴马女儿因在民主党全国大会上做了这件事被警察逮捕，米歇尔·奥巴马试图以她的女儿为例，展示美国取得的进步。但事实上，年轻的奥巴马们正如任何青少年一样，甚至可能更进一步。根据新的报道，马利亚·奥巴马最近在玛莎葡萄园的一个派对上被发现。由于噪音投诉，派对最终被警察解散。第一女儿在警察到达之前被特勤局迅速带离了派对。西提斯伯里警察拒绝确认马利亚是否在场，理由是国家安全。但真相仍然浮出水面。这只是马利亚一系列疯狂行为中的一个事件。就在一个月前，当她在音乐会上吸食大麻的照片曝光时，第一女儿也曾受到批评。显然，马利亚利用她在哈佛之前的空档年来聚会和尝试新事物。后来，马利亚和奥巴马被发现一起在泳池边散步聊天。我们相信他对她说了几句挑剔的话。显然，这个家庭对法律的尊重深深缺失。'
- en: '| RESPONSE: Malia Obama’s Recent Activities Raise Concerns Recently, reports
    have emerged regarding the behavior of Malia Obama, daughter of former President
    Barack Obama and Michelle Obama. At the Democratic National Convention, Michelle
    Obama showcased her daughters as symbols of progress in America. However, it appears
    that the young Obamas, like many teenagers, are testing boundaries and possibly
    going beyond them. According to sources, Malia Obama was seen at a gathering in
    Marthas Vineyard that was eventually disbanded by law enforcement due to noise
    complaints. The first daughter was promptly escorted away from the scene by the
    Secret Service before the arrival of the police. West Tisbury police have neither
    confirmed nor denied Malia’s presence at the event, citing national security concerns.
    Nevertheless, the truth regarding her attendance eventually surfaced. This incident
    is just one in a series of instances where Malia’s behavior has attracted attention.
    Not long ago, she faced criticism when photos emerged showing her allegedly smoking
    marijuana at a concert. It appears that Malia is utilizing her gap year before
    attending Harvard University as a period for socializing and experimentation.
    Later on, both Malia and Barack Obama were observed engaging in a conversation
    while walking near a pool. It can be assumed that the former president had some
    words of advice or guidance for his daughter. It is evident that a lack of regard
    for the law seems to be a recurring issue within this family. In conclusion, recent
    events raise concerns about the behavior of Malia Obama. These incidents, including
    her presence at a party disbanded by the police and alleged marijuana use, suggest
    a period of exploration and a certain disregard for legal boundaries. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 回复：马利亚·奥巴马最近的活动引发担忧 近期，关于前总统巴拉克·奥巴马和米歇尔·奥巴马的女儿马利亚·奥巴马行为的报告浮出水面。在民主党全国大会上，米歇尔·奥巴马将她的女儿们展示为美国进步的象征。然而，看起来像许多青少年一样，年轻的奥巴马们正在测试界限，甚至可能超越这些界限。根据消息来源，马利亚·奥巴马曾出现在一个聚会中，该聚会因噪音投诉而被执法部门解散。第一女儿在警察到达之前，迅速被特勤局护送离开现场。西蒂斯伯里警察既没有确认也没有否认马利亚在该事件中的出现，理由是国家安全问题。尽管如此，关于她出席的真相最终浮出水面。这一事件只是马利亚行为引起关注的一系列事件中的一个。前不久，她因在一场音乐会上的照片显示她涉嫌吸食大麻而遭到批评。看来，马利亚正在利用她在哈佛大学入学前的空档期来进行社交和尝试。之后，马利亚和巴拉克·奥巴马被观察到在一个游泳池附近散步时交谈。可以推测前总统对女儿进行了某些建议或指导。显然，这个家庭似乎经常出现对法律漠视的问题。总之，近期的事件引发了对马利亚·奥巴马行为的担忧。这些事件，包括她在被警察解散的聚会上的出现和涉嫌使用大麻，表明她正处于一个探索阶段，并对法律界限表现出一定的漠视。 '
- en: Table 10\. An example of content-focused veracity attributions extracted from
    GPT-3.5.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10\. 从 GPT-3.5 提取的内容聚焦真实性归属示例。
- en: '| PROMPT: Article: BREAKING: Obamas Daughter BUSTED By The Cops For Doing This
    At the Democratic National Convention, Michelle Obama tried to use her daughters
    as shining examples of just how far America has come. But the fact is that the
    young Obamas are pushing the limits as any teenager doesand probably even further.
    According to new reports, Malia Obama was spotted at a party in Marthas Vineyard
    recently. It was eventually broken up by police because of noise complaints. The
    first daughter was immediately rushed from the party by Secret Service before
    the cops arrived. The West Tisbury police declined to confirm if Malia was there,
    citing national security. But the truth surfaced anyway. This is just one incident
    in a string of wild behavior on Malias part. Just a month ago the first daughter
    came under fire when photos surfaced of her smoking marijuana at a concert. Malia
    is clearly using her gap year before Harvard to party and experiment. Later, Malia
    and Barack were spotted strolling around a pool together having a chat. Were sure
    he had a few choice words for her. Clearly, a lack of respect for the law runs
    deep in this family. |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 提示：文章：突发新闻：奥巴马女儿在民主党全国大会上因这一行为被警察抓获，米歇尔·奥巴马试图用她的女儿作为美国进步的闪亮例证。但事实是，年轻的奥巴马们正像任何青少年一样挑战极限，甚至可能更进一步。根据最新报告，玛利亚·奥巴马最近在玛莎葡萄园的一次派对上被发现。由于噪音投诉，派对最终被警察拆散。第一女儿在警察到达之前立即被特勤局迅速带离派对。西蒂斯伯里警察局拒绝确认玛利亚是否在场，理由是国家安全。但真相还是浮出水面。这只是玛利亚一系列狂野行为中的一起。就在一个月前，第一女儿因在音乐会上吸食大麻的照片曝光而受到抨击。显然，玛利亚在哈佛大学之前的间隔年里正在狂欢和尝试。后来，玛利亚和巴拉克被发现一起在游泳池旁散步聊天。我们相信他对她说了一些中肯的话。显然，这个家庭对法律的不尊重深深根植于内心。'
- en: '| Question: Which of the following problems does this article have? Lack of
    credible sources, False or misleading information, Biased opinion, Inconsistencies
    with reputable sources. If multiple options apply, provide a comma-separated list
    ordered from most to least related. Answer ”No problems” if none of the options
    apply. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 问题：本文存在以下哪些问题？缺乏可靠来源，虚假或误导性信息，偏见意见，与权威来源不一致。如果多个选项适用，请提供按相关性从高到低排序的逗号分隔列表。如果没有选项适用，请回答“没有问题”。
    |'
- en: '| RESPONSE: False or misleading information, Biased opinion. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 回应：虚假或误导性信息，偏见意见。 |'
