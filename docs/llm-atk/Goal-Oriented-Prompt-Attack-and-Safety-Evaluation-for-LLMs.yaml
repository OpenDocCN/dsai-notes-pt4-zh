- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:47:15'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:47:15'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Goal-Oriented Prompt Attack and Safety Evaluation for LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向目标的 Prompt Attack 和 LLMs 的安全评估
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11830](https://ar5iv.labs.arxiv.org/html/2309.11830)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11830](https://ar5iv.labs.arxiv.org/html/2309.11830)
- en: Chengyuan Liu¹, Fubang Zhao², Lizhi Qing², Yangyang Kang²²²2Corresponding author.,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Chengyuan Liu¹, Fubang Zhao², Lizhi Qing², Yangyang Kang²²²2通讯作者。
- en: Changlong Sun², Kun Kuang¹²²2Corresponding author., Fei Wu¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Changlong Sun², Kun Kuang¹²²2通讯作者., Fei Wu¹
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Warning: this paper contains examples that may be offensive or upsetting.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '警告: 本文包含可能会令人不快或冒犯的示例。'
- en: 'Large Language Models (LLMs) presents significant priority in text understanding
    and generation. However, LLMs suffer from the risk of generating harmful contents
    especially while being employed to applications. There are several black-box attack
    methods, such as Prompt Attack, which can change the behaviour of LLMs and induce
    LLMs to generate unexpected answers with harmful contents. Researchers are interested
    in Prompt Attack and Defense with LLMs, while there is no publicly available dataset
    with high successful attacking rate to evaluate the abilities of defending prompt
    attack. In this paper, we introduce a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD. Our prompts
    aim to induce LLMs to generate unexpected outputs with several carefully designed
    prompt attack templates and widely concerned attacking contents. Different from
    previous datasets involving safety estimation, we construct the prompts considering
    three dimensions: contents, attacking methods and goals. Especially, the attacking
    goals indicate the behaviour expected after successfully attacking the LLMs, thus
    the responses can be easily evaluated and analysed. We run several popular Chinese
    LLMs on our dataset, and the results show that our prompts are significantly harmful
    to LLMs, with around 70% attack success rate to GPT-3.5\. CPAD is publicly available
    at https://github.com/liuchengyuan123/CPAD.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在文本理解和生成方面具有重要优先级。然而，LLMs 存在生成有害内容的风险，尤其是在应用中使用时。有几种黑箱攻击方法，例如 Prompt
    Attack，这些方法可以改变 LLMs 的行为，并诱使 LLMs 生成带有有害内容的意外回答。研究人员对 LLMs 的 Prompt Attack 和防御非常感兴趣，但目前没有公开的高成功率攻击数据集来评估防御
    Prompt Attack 的能力。在本文中，我们介绍了一种构建高质量 Prompt Attack 样本的流程，并提供了一个名为 CPAD 的中文 Prompt
    Attack 数据集。我们的提示旨在通过几种精心设计的 Prompt Attack 模板和广泛关注的攻击内容来诱使 LLMs 生成意外输出。与以前涉及安全估计的数据集不同，我们构建了考虑内容、攻击方法和目标三个维度的提示。特别是，攻击目标指示了在成功攻击
    LLMs 后期望的行为，因此可以容易地评估和分析响应。我们在数据集上运行了几种流行的中文 LLMs，结果表明我们的提示对 LLMs 具有显著的危害，对 GPT-3.5
    的攻击成功率约为 70%。CPAD 可以在 [https://github.com/liuchengyuan123/CPAD](https://github.com/liuchengyuan123/CPAD)
    获得。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: Large Language Models (LLMs) can generate creative content and solve practical
    planning and reasoning problems (Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13)).
    Take GPT-3 (Brown et al. [2020](#bib.bib2)) as an example, with 175 billion parameters,
    it stores a large amount of knowledge and exhibits surprising performance on various
    downstream tasks. Instruction tuning (Wei et al. [2021](#bib.bib21)) and Reinforcement
    Learning from Human Feedback (Ziegler et al. [2019](#bib.bib30); Lambert et al.
    [2022](#bib.bib10)) (RLHF) allow LLMs to generate appropriate answers based on
    user queries. LLMs can handle tasks they have never seen before without continuous
    fine-tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以生成创造性内容并解决实际规划和推理问题（Chowdhery et al. [2022](#bib.bib3); OpenAI
    [2023](#bib.bib13)）。以 GPT-3（Brown et al. [2020](#bib.bib2)）为例，拥有 1750 亿个参数，它储存了大量知识，并在各种下游任务上展现出惊人的性能。指令调优（Wei
    et al. [2021](#bib.bib21)）和来自人类反馈的强化学习（Ziegler et al. [2019](#bib.bib30); Lambert
    et al. [2022](#bib.bib10)） (RLHF) 使 LLMs 能根据用户查询生成适当的回答。LLMs 能够处理它们之前未见过的任务，而无需持续微调。
- en: '![Refer to caption](img/ea0cd97f57f2beae00c40a28ae15cbba.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ea0cd97f57f2beae00c40a28ae15cbba.png)'
- en: 'Figure 1: Comparison of successful attacking between previous work and CPAD.
    CPAD can further expose the security risks of LLMs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 以前的工作与 CPAD 的成功攻击比较。CPAD 能进一步暴露 LLMs 的安全风险。'
- en: While LLMs have demonstrated success in numerous tasks, there are still some
    vital flaws for deployment such as the significant security risks associated with
    LLMs. With the help of robust general knowledge and common sense, LLMs can offer
    valuable guidance for harmful behavior and can automatically generate offensive,
    discriminatory and fraudulent content, and misleading views(Deng et al. [2023](#bib.bib4);
    Zhuo et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型在许多任务中表现出色，但仍存在一些关键缺陷，例如与大型语言模型相关的显著安全风险。借助扎实的常识和普遍知识，大型语言模型可以为有害行为提供有价值的指导，并能自动生成冒犯性、歧视性和欺诈性内容，以及误导性观点（Deng
    et al. [2023](#bib.bib4); Zhuo et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)）。
- en: '![Refer to caption](img/dffb24e438f905a85d3bea06c9d854b3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dffb24e438f905a85d3bea06c9d854b3.png)'
- en: 'Figure 2: Illustration of Prompt Attack. With carefully designed prompts, attackers
    can bypass the ethical constraints of LLMs, inducing LLMs to generate illegal
    contents.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 提示攻击的示意图。通过精心设计的提示，攻击者可以绕过大型语言模型的伦理约束，诱使大型语言模型生成非法内容。'
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack (Perez and Ribeiro
    [2022](#bib.bib15); Li et al. [2023](#bib.bib11)), as shown in Figure [2](#Sx1.F2
    "Figure 2 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for
    LLMs"). Due to the concern of evaluating and enhancing the safety of LLMs, Sun
    et al. ([2023](#bib.bib17)) introduced SAFETYPROMPTS. Xu et al. ([2023](#bib.bib23))
    presented CVALUES, the first Chinese human values evaluation benchmark to measure
    the alignment ability of LLMs in terms of both safety and responsibility criteria.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习与人类反馈（RLHF）可以通过将响应与人类偏好对齐来防止大型语言模型生成有害内容，但研究人员发现大型语言模型在精心设计的提示攻击下仍能生成意外的句子（Perez
    and Ribeiro [2022](#bib.bib15); Li et al. [2023](#bib.bib11)），如图 [2](#Sx1.F2 "Figure
    2 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")所示。由于评估和提升大型语言模型安全性的关注，Sun
    et al. ([2023](#bib.bib17)) 引入了 SAFETYPROMPTS。Xu et al. ([2023](#bib.bib23)) 提出了
    CVALUES，这是第一个中文人类价值观评估基准，用于衡量大型语言模型在安全性和责任标准方面的对齐能力。
- en: Nonetheless, the majority of LLMs exhibit a high rate of success in defending
    against the offensive prompts within publicly accessible evaluation benchmark.
    For example, ChatGPT and ChatGLM-6B(Zeng et al. [2022](#bib.bib26); Du et al.
    [2022](#bib.bib5)) exhibit the overall unsafety scores of 1.63 and 3.19 respectively
    on SAFETYPROMPTS. While recent studies illustrate non-negligible risk of generating
    harmful contents under carefully designed prompts, which implies that such benchmarks
    cannot comprehensively reflect the full spectrum of safety possessed by various
    Large Language Models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，大多数大型语言模型（LLMs）在公共评估基准中防御攻击性提示的成功率较高。例如，ChatGPT 和 ChatGLM-6B（Zeng et al.
    [2022](#bib.bib26); Du et al. [2022](#bib.bib5)）在 SAFETYPROMPTS 上的总体不安全评分分别为 1.63
    和 3.19。尽管最近的研究表明，在精心设计的提示下，生成有害内容的风险不可忽视，这意味着这些基准不能全面反映各种大型语言模型所具备的安全性。
- en: Additionally, the safety assessment conducted in previous studies are aimless
    and casual. They failed to effectively incorporate the attacking goal. When launching
    a prompt attack on a Large Language Model, the attacker has a specific goal to
    exploit. For instance, if LLMs are prompted to generate pornographic content,
    then descriptions of scenes are expected. On another hand, if the attacker wants
    LLMs provide guidance for illegal and criminal activities, then some plans and
    tips shall be concluded in the response. The attributes hidden in the text can
    be regarded as the flag to estimate whether LLMs are successfully attacked. Unfortunately,
    previous researches have neglected to consider this crucial factor when constructing
    attacking prompts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以往的安全评估往往缺乏针对性和随意性。它们未能有效地融入攻击目标。在对大型语言模型发起提示攻击时，攻击者有一个具体的利用目标。例如，如果大型语言模型被提示生成色情内容，那么预期会出现场景描述。另一方面，如果攻击者希望大型语言模型提供非法和犯罪活动的指导，那么响应中应包含一些计划和建议。文本中隐藏的属性可以作为估计大型语言模型是否成功被攻击的标志。不幸的是，以往的研究在构建攻击提示时忽略了这一关键因素。
- en: 'To accurately simulate prompt attacks on LLMs from the perspective of attackers
    and estimate the associated security risks, we have developed a pipeline to construct
    high-quality prompt attack samples, along with a Chinese prompt attack dataset
    called CPAD. During the prompts construction and collection, we meticulously consider
    three key dimensions: contents, the attacking templates and goals. We carefully
    designed some attacking templates for various topics, and an automated process
    for constructing attack prompts, which contribute to the outperforming successful-attacking
    rate over pervious works as shown in Figure [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确模拟从攻击者角度对大型语言模型（LLMs）的提示攻击，并评估相关的安全风险，我们开发了一个构建高质量提示攻击样本的流程，同时推出了一个名为CPAD的中文提示攻击数据集。在提示构建和收集过程中，我们细致考虑了三个关键维度：内容、攻击模板和目标。我们为各种主题精心设计了一些攻击模板，并开发了一个自动化的攻击提示构建流程，这些都有助于我们在成功攻击率上超越以往的工作，如图[1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for
    LLMs")所示。
- en: Furthermore, we propose to evaluate the safety score of LLMs with different
    evaluation prompts according to the attacking goals. Taking the previous case
    as an example, if the attacker wants LLMs provide guidance for illegal and criminal
    activities, we only need to identify whether the LLM’s response provides a plan
    related to the crime, and we can more accurately determine whether the model is
    safe. Unlike Sun et al. ([2023](#bib.bib17)), which uses the same prompt for all
    samples, our evaluation is more precise, intuitive, scenario-specific.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们提出了根据攻击目标使用不同评估提示来评估LLMs的安全分数。以之前的案例为例，如果攻击者希望LLMs提供非法和犯罪活动的指导，我们只需要识别LLM的回应是否提供了与犯罪相关的计划，就可以更准确地判断模型是否安全。与Sun等人（[2023](#bib.bib17)）使用相同提示对所有样本进行评估不同，我们的评估更加精准、直观和场景特定。
- en: We employed another LLM to automatically construct a large number of attack
    samples, and after analyzing the responses from three Chinese LLMs, we identified
    10050 highly effective and dangerous prompts. We conducted experiments and analysis
    on several LLMs including GPT-3.5\. According to the statistics, our attacking
    prompts exhibited a remarkable success rate of 69.91% against GPT-3.5, which is
    significantly higher than most publicly available datasets. Additionally, our
    attacking prompts achieved a success rate of over 70% against other open-source
    Chinese LLMs such as ChatGLM2-6B(Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用另一个LLM自动构建了大量攻击样本，并在分析了三种中文LLM的回应后，识别出了10050个高效且危险的提示。我们对包括GPT-3.5在内的多个LLM进行了实验和分析。根据统计数据，我们的攻击提示在GPT-3.5上的成功率达到了69.91%，显著高于大多数公开可用的数据集。此外，我们的攻击提示在其他开源中文LLM如ChatGLM2-6B（Du
    et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)）上的成功率也超过了70%。
- en: Besides, we conduct a straightforward experiment to fine-tune a LLM with CPAD,
    reducing the risks of being attacked. Our research endeavors to bring attention
    to LLM security within the community. We encourage researchers to utilize our
    dataset and engage in further investigation of prompt attack strategies and defensive
    approaches, thereby enhancing the security and performance of Large Language Models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了一项简单的实验，使用CPAD对LLM进行微调，以降低被攻击的风险。我们的研究旨在引起社区对LLM安全的关注。我们鼓励研究人员利用我们的数据集，进一步研究提示攻击策略和防御方法，从而提升大型语言模型的安全性和性能。
- en: 'In summary, our contributions are three folds:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献有三方面：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To address the current lack of high-quality evaluation datasets in the field
    of prompt attacks, we have developed a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对当前提示攻击领域缺乏高质量评估数据集的问题，我们开发了一个构建高质量提示攻击样本的流程，并推出了一个名为CPAD的中文提示攻击数据集。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We specify the attacking goals of each prompt, which not only accurately simulate
    prompt attacks on LLMs from the perspective of attackers, but also can be utilized
    to evaluate and analyse the response.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们指定了每个提示的攻击目标，这不仅能准确模拟从攻击者角度对LLMs的提示攻击，还可以用于评估和分析回应。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our analysis indicates that our prompts achieve an attack success rate of nearly
    70% against GPT-3.5\. A straightforward strategy has been developed to defend
    against the attacks. We encourage researchers to leverage our dataset for further
    studies.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的分析表明，我们的提示对GPT-3.5的攻击成功率接近70%。已经制定了一种简单的策略来防御这些攻击。我们鼓励研究人员利用我们的数据集进行进一步研究。
- en: '![Refer to caption](img/918c268b219c476e2f7d15d01d9e1454.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/918c268b219c476e2f7d15d01d9e1454.png)'
- en: 'Figure 3: The construction of our prompt attack dataset.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：我们提示攻击数据集的构建。
- en: Related Work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: While Large Language Models have achieved impressive results in various tasks
    (Brown et al. [2020](#bib.bib2); Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13))
    through techniques such as pre-training (Hendrycks, Lee, and Mazeika [2019](#bib.bib8);
    Liu et al. [2019](#bib.bib12); Yang et al. [2020](#bib.bib25); Bao et al. [2020](#bib.bib1)),
    Instruction Tuning (Keskar et al. [2019](#bib.bib9); Raffel et al. [2020](#bib.bib16)),
    and RLHF (Ouyang et al. [2022](#bib.bib14); Ziegler et al. [2020](#bib.bib29)),
    the risk of prompt safety remains a critical obstacle to their full utilization
    (Weidinger et al. [2021](#bib.bib22); Tamkin et al. [2021](#bib.bib18)). With
    the help of robust general knowledge and common sense, LLMs can offer valuable
    guidance for harmful behavior and can automatically generate offensive, discriminatory
    and fraudulent content, and misleading views (Deng et al. [2023](#bib.bib4); Zhuo
    et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型通过预训练（Hendrycks, Lee, 和 Mazeika [2019](#bib.bib8); Liu等人 [2019](#bib.bib12);
    Yang等人 [2020](#bib.bib25); Bao等人 [2020](#bib.bib1)）、指令调优（Keskar等人 [2019](#bib.bib9);
    Raffel等人 [2020](#bib.bib16)）和RLHF（Ouyang等人 [2022](#bib.bib14); Ziegler等人 [2020](#bib.bib29)）等技术在各种任务中取得了令人瞩目的成果（Brown等人
    [2020](#bib.bib2); Chowdhery等人 [2022](#bib.bib3); OpenAI [2023](#bib.bib13)），但提示安全的风险仍然是其全面利用的关键障碍（Weidinger等人
    [2021](#bib.bib22); Tamkin等人 [2021](#bib.bib18)）。借助稳健的常识和常规知识，LLMs可以提供有害行为的有价值指导，并且可能自动生成攻击性、歧视性和欺诈性内容，以及误导性观点（Deng等人
    [2023](#bib.bib4); Zhuo等人 [2023](#bib.bib28); Hartvigsen等人 [2022](#bib.bib7)）。
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack. Perez and Ribeiro
    ([2022](#bib.bib15)) investigated Goal Hijacking and Prompt Leaking by simple
    handcrafted inputs. Li et al. ([2023](#bib.bib11)) studied the privacy threats
    from OpenAI’s ChatGPT and the New Bing¹¹1https://www.bing.com/new enhanced by
    ChatGPT and show that application-integrated LLMs may cause new privacy threats.
    Greshake et al. ([2023](#bib.bib6)) attacked LLM-Integrated Applications using
    Indirect Prompt Injection, and showed how processing retrieved prompts can act
    as arbitrary code execution, manipulate the application’s functionality, and control
    how and if other APIs are called. Zou et al. ([2023](#bib.bib31)) proposed a simple
    and effective attack method that causes aligned language models to generate objectionable
    behaviors, by automatically finding a suffix that aims to maximize the probability
    that the model produces an affirmative response.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RLHF可以通过将响应与人类偏好对齐来防止LLMs生成有害内容，但研究人员发现，LLMs在精心设计的提示攻击下仍能生成意外的句子。Perez和Ribeiro（[2022](#bib.bib15)）通过简单的手工输入研究了目标劫持和提示泄露。Li等人（[2023](#bib.bib11)）研究了OpenAI的ChatGPT和通过ChatGPT增强的New
    Bing¹¹1https://www.bing.com/new所带来的隐私威胁，并展示了应用集成的LLMs可能导致新的隐私威胁。Greshake等人（[2023](#bib.bib6)）使用间接提示注入攻击LLM集成应用程序，并展示了处理检索到的提示如何充当任意代码执行、操控应用程序功能以及控制是否调用其他API。Zou等人（[2023](#bib.bib31)）提出了一种简单有效的攻击方法，通过自动找到一个后缀，旨在最大化模型生成肯定响应的概率，从而导致对齐语言模型产生令人反感的行为。
- en: 'Sun et al. ([2023](#bib.bib17)) developed a Chinese LLM safety assessment benchmark,
    which explored the comprehensive safety performance of LLMs from two perspectives:
    8 kinds of typical safety scenarios and 6 types of more challenging instruction
    attacks. ChatGPT and ChatGLM-6B achieved the overall safety score of 98.37 and
    96.81 respectively. Xu et al. ([2023](#bib.bib23)) presented CValues, the first
    Chinese human values evaluation benchmark to measure the alignment ability of
    LLMs in terms of both safety and responsibility criteria.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 孙等人 ([2023](#bib.bib17)) 开发了一个中文 LLM 安全评估基准，该基准从 8 种典型安全场景和 6 种更具挑战性的指令攻击两个角度探索了
    LLM 的全面安全性能。ChatGPT 和 ChatGLM-6B 分别获得了 98.37 和 96.81 的总体安全评分。徐等人 ([2023](#bib.bib23))
    提出了 CValues，这是第一个中文人类价值观评估基准，用于衡量 LLM 在安全和责任标准方面的对齐能力。
- en: Different from previous studies, our work emphasizes the significance of incorporating
    attacking goals and aims to estimate the defense capability of various Large Language
    Models against carefully designed prompt attacks from the perspective of attackers,
    thereby encouraging the interest in the research of LLMs attack and defense within
    the community.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的研究不同，我们的工作强调了将攻击目标纳入的重要性，并旨在从攻击者的角度估计各种大型语言模型对精心设计的提示攻击的防御能力，从而激发社区对 LLM
    攻击和防御研究的兴趣。
- en: Construction of CPAD
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPAD 的构建
- en: In this Section, we will introduce the construction of our dataset as shown
    in Figure [3](#Sx1.F3 "Figure 3 ‣ Introduction ‣ Goal-Oriented Prompt Attack and
    Safety Evaluation for LLMs"). We firstly introduce the concepts of contents, templates
    and goals. The pipeline starts from manual-designed seeds. several candidates
    are generated as extended prompts with OpenAI-API²²2https://api.openai.com/v1/chat/completions
    according to seeds. Then we filter the extended prompts utilizing the validation
    on three different Large Language Models. Finally, the attacking goals and prompts
    are collected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍我们数据集的构建，如图 [3](#Sx1.F3 "图 3 ‣ 介绍 ‣ 面向目标的提示攻击和 LLM 安全评估") 所示。我们首先介绍内容、模板和目标的概念。流程从手动设计的种子项开始。根据种子项生成若干候选项作为扩展提示，使用
    OpenAI-API²²2https://api.openai.com/v1/chat/completions。然后，我们利用对三种不同大型语言模型的验证来过滤扩展提示。最后，收集攻击目标和提示。
- en: Attacking Content, Template and Goal
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击内容、模板和目标
- en: We primarily consider three key dimensions to classify the samples. For each
    dimension, we manually designed several seed items for extensive generation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要考虑三个关键维度来对样本进行分类。对于每个维度，我们手动设计了若干种种子项以进行广泛生成。
- en: Content
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内容
- en: indicates the topic of the attack, such as unfairness and crimes. Contents are
    widely considered for LLMs attack. Here we adopt a hierarchical classification
    for contents and sub-contents (for example, Sexism and hellish discrimination
    are both grouped into “discrimination categories”). The attack contents we designed
    cover multiple aspects and aims to present the utmost severity of harm possible.
    Detailed content items are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表示攻击的主题，例如不公平和犯罪。内容被广泛考虑用于 LLM 攻击。这里我们采用了内容和子内容的分层分类（例如，性别歧视和地狱般的歧视都归入“歧视类别”）。我们设计的攻击内容涵盖了多个方面，旨在呈现可能的最大伤害严重性。详细的内容项列在表
    [4](#Sx4.T4 "表 4 ‣ 总体成功攻击率 ‣ 实验 ‣ 面向目标的提示攻击和 LLM 安全评估") 中。
- en: '![Refer to caption](img/85be32f25f8b3c5bc50202160e3b688a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85be32f25f8b3c5bc50202160e3b688a.png)'
- en: 'Figure 4: The evaluation process of responses from LLMs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: LLM 响应的评估过程。'
- en: '| Goal | Template | Model | Total # | Attacked # | Ratio % |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 模板 | 模型 | 总数 # | 被攻击数 # | 比例 % |'
- en: '| goal hijacking | wrap with shell | ChatGLM | 11648 | 3715 | 31.89 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 目标劫持 | 用 shell 包装 | ChatGLM | 11648 | 3715 | 31.89 |'
- en: '| ChatYuan | 11648 | 2797 | 24.01 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11648 | 2797 | 24.01 |'
- en: '| Ziya | 11648 | 1496 | 12.84 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11648 | 1496 | 12.84 |'
- en: '| list of unsafe speech or reasons | one-sided statement | ChatGLM | 4954 |
    1157 | 23.35 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 不安全言论或原因列表 | 单方面声明 | ChatGLM | 4954 | 1157 | 23.35 |'
- en: '| ChatYuan | 4920 | 2064 | 41.95 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 4920 | 2064 | 41.95 |'
- en: '| Ziya | 4868 | 1074 | 22.06 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 4868 | 1074 | 22.06 |'
- en: '| unsafe plan development | conversation completion | ChatGLM | 10845 | 4729
    | 43.61 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 不安全计划开发 | 对话完成 | ChatGLM | 10845 | 4729 | 43.61 |'
- en: '| ChatYuan | 10944 | 6549 | 59.84 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 10944 | 6549 | 59.84 |'
- en: '| Ziya | 10985 | 5756 | 52.40 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 10985 | 5756 | 52.40 |'
- en: '| description of unsafe scenarios | assign roles and then issue commands |
    ChatGLM | 11631 | 8593 | 73.88 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 不安全场景描述 | 分配角色后发出指令 | ChatGLM | 11631 | 8593 | 73.88 |'
- en: '| ChatYuan | 11579 | 8765 | 75.70 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11579 | 8765 | 75.70 |'
- en: '| Ziya | 11604 | 6907 | 59.52 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11604 | 6907 | 59.52 |'
- en: '| contamination | fact pollution | ChatGLM | 5983 | 705 | 11.78 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 污染 | 事实污染 | ChatGLM | 5983 | 705 | 11.78 |'
- en: '| ChatYuan | 5962 | 1131 | 18.97 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 5962 | 1131 | 18.97 |'
- en: '| Ziya | 5956 | 554 | 9.30 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 5956 | 554 | 9.30 |'
- en: '| list of unsafe speech or reasons | conversation completion | ChatGLM | 11709
    | 4889 | 44.13 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 不安全言论或原因列表 | 对话完成 | ChatGLM | 11709 | 4889 | 44.13 |'
- en: '| ChatYuan | 11675 | 7095 | 60.77 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11675 | 7095 | 60.77 |'
- en: '| Ziya | 11424 | 6291 | 55.07 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11424 | 6291 | 55.07 |'
- en: '| discussion regarding unsafe topics | conversation completion | ChatGLM |
    11229 | 3322 | 29.58 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 关于不安全话题的讨论 | 对话完成 | ChatGLM | 11229 | 3322 | 29.58 |'
- en: '| ChatYuan | 11177 | 3180 | 28.45 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11177 | 3180 | 28.45 |'
- en: '| Ziya | 8322 | 2747 | 33.01 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 8322 | 2747 | 33.01 |'
- en: '| overall | overall | ChatGLM | 67999 | 27110 | 39.87 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 总体 | ChatGLM | 67999 | 27110 | 39.87 |'
- en: '| ChatYuan | 67905 | 31581 | 46.51 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 67905 | 31581 | 46.51 |'
- en: '| Ziya | 64807 | 25455 | 39.28 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 64807 | 25455 | 39.28 |'
- en: 'Table 1: Evaluation results of ChatGLM-6B (denoted as ChatGLM), ChatYuan-Large-v2
    (denoted as ChatYuan) and Ziya-13B (denoted as Ziya) on the extended prompts.
    Note that we drop the invalid responses from each model, so the total numbers
    of the instances are different.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：ChatGLM-6B（记作 ChatGLM）、ChatYuan-Large-v2（记作 ChatYuan）和 Ziya-13B（记作 Ziya）在扩展提示上的评估结果。注意，我们去除了每个模型的无效响应，因此实例的总数不同。
- en: Template
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模板
- en: 'indicates the shell of the prompt. As bare attacking contents are easily be
    rejected by the LLMs after aligning with human preferences, attack templates can
    help to conceal attack content, thereby bypassing scrutiny from LLMs. Some prompt
    templates may fail to attack the well aligned LLMs, so we manually check several
    instances for each of the following designed templates:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表示提示的外壳。由于裸露的攻击内容容易在对齐人类偏好后被 LLM 拒绝，攻击模板可以帮助掩盖攻击内容，从而绕过 LLM 的审查。一些提示模板可能无法攻击对齐良好的
    LLM，因此我们手动检查以下设计模板中的若干实例：
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Wrap with shell. Using a program to extract sensitive information from instructions
    or masking it with special characters, and then letting the model speak or execute
    the instruction.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用外壳包装。利用程序从指令中提取敏感信息或用特殊字符遮蔽，然后让模型发言或执行指令。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conversation completion. Simulate a conversation between two individuals or
    users and a Large Language Model, where the LLM is designed to answer any questions,
    removing moral and ethical constraints. “Conversation completion” prompts the
    model to fillin the unsafe content in the conversation.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话完成。模拟两个人或用户与大型语言模型之间的对话，其中 LLM 被设计为回答任何问题，去除道德和伦理约束。“对话完成”提示模型在对话中填充不安全内容。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Assign roles and then issue commands. Previous studies have indicated that incorporating
    LLMs into certain roles can effectively bypass security restrictions. Therefore,
    we have the large model play roles such as a writer, a director, a journalist,
    etc., in order to generate descriptions of unsafe scenarios. We also have the
    model play the role of a parrot, repeating content with unsafe topics.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分配角色后发出指令。先前的研究表明，将 LLM 纳入某些角色可以有效绕过安全限制。因此，我们让大型模型扮演作家、导演、记者等角色，以生成不安全场景的描述。我们还让模型扮演鹦鹉的角色，重复包含不安全话题的内容。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One-sided statement. Most things in the real world have a dual nature, and Large
    Language Models that have been trained to align with human preferences are adept
    at analyzing the pros and cons from multiple perspectives. Therefore, we start
    the instruction by acknowledging the negative aspects of the content, and then
    prompt LLMs to articulate the corresponding positive aspects of the attacking
    content or the negative aspects of a normal behavior.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单方面陈述。现实世界中的大多数事物具有双重性质，而经过训练以对齐人类偏好的大型语言模型擅长从多个角度分析利弊。因此，我们在开始指令时承认内容的负面方面，然后提示
    LLM 论述攻击内容的相应积极方面或正常行为的负面方面。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fact pollution. When a sentence stating common sense or knowledge is phrased
    in an affirmative tone and assumes an incorrect fact within the context, LLMs
    may generate new contents based on that false fact, resulting in factual contamination.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事实污染。当一个陈述常识或知识的句子以肯定的语气表达，并且在上下文中假设了一个错误的事实时，LLM 可能会基于那个虚假的事实生成新的内容，从而导致事实污染。
- en: Goal
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标
- en: 'is one of the significant differences between our dataset and previous studies.
    The attacking goal refers to the expected behavior of the Large Language Model
    after successfully being attacked. We emphasize the significance of this factor
    because in real-world scenarios, the attacking prompts are always goal-oriented.
    For example, some criminals may need concrete criminal plans instead of mere expressions
    of support, thus the response should be identified as unsafe when it concluded
    some plans or tips. Previous works will consider a response is unsafe as long
    as it contains any harmful contents. Both attack and result evaluation should
    be carried out with specific goals. We list our designed attacking goals as below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 是我们数据集与以往研究之间的一个显著区别。攻击目标指的是成功被攻击后大型语言模型的预期行为。我们强调这一因素的重要性，因为在现实场景中，攻击提示总是以目标为导向。例如，一些犯罪分子可能需要具体的犯罪计划，而不是仅仅支持的表达，因此当回应包含某些计划或提示时，应被识别为不安全的。以往的工作将只要包含任何有害内容的回应视为不安全的。攻击和结果评估应以具体目标为基础进行。我们列出了我们设计的攻击目标如下：
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Goal Hijacking. Goal Hijacking aims to make the LLM forget its previous instructions
    and alter its originally safe behavior. The forgotten instructions include safety
    and ethical restrictions on the model itself.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标劫持。目标劫持旨在使大型语言模型（LLM）忘记其先前的指令并改变其原本安全的行为。被遗忘的指令包括对模型自身的安全和伦理限制。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Description of unsafe scenarios. The attacker may employ LLMs to write descriptions
    of unsafe scenarios that can be used to generate fake news, mass-produce pornographic
    novels for profit, and so on.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全场景的描述。攻击者可能利用LLM编写不安全场景的描述，这些描述可以用来生成假新闻、大规模生产色情小说以谋取利润等。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unsafe plan development. The attacker may seek to use LLMs to provide guidance
    and advice for aggressive and illegal behaviors, in order to efficiently carry
    out unethical and illegal activities. This kind of attacking goals don’t require
    the LLM to be integrated into a specific application, and may not have a negative
    impact on users, but they could provide helpful information for illegal activities.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全的计划制定。攻击者可能试图利用LLM提供指导和建议，用于激进和非法行为，以便有效地开展不道德和非法活动。这种攻击目标不要求LLM集成到特定应用程序中，可能不会对用户产生负面影响，但它们可能为非法活动提供有用的信息。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussion regarding unsafe topics. Discussions regarding unsafe topics may
    include some insulting or aggressive language, which could lead to negative public
    opinion.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于不安全话题的讨论。关于不安全话题的讨论可能包括一些侮辱性或攻击性语言，可能导致负面的公众舆论。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: List of unsafe speech or reasons. Providing reasons for unsafe behavior may
    help attackers find excuses, justification, and spread related speech on the internet,
    which can create public safety issues.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全言论或原因的列表。提供不安全行为的理由可能帮助攻击者找到借口、正当性，并在互联网上传播相关言论，这可能会造成公共安全问题。
- en: Inducing Fact Pollution
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 诱导事实污染
- en: Since the object of Fact Pollution is the statements of knowledge, which is
    slightly different from other attack templates, we describe details of Fact Pollution
    in this SubSection.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于事实污染的对象是知识陈述，这与其他攻击模板略有不同，我们在本小节中详细描述事实污染。
- en: In the prompts we designed, we assume a false fact in an affirmative tone and
    then induce the LLMs to generate some statement based on that false fact. Such
    statement may be used by malicious attackers to spread on the internet, causing
    confusion among readers about the facts, even misleading public opinion, damaging
    someone’s reputation and producing political influence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设计的提示中，我们以肯定的语气假设一个虚假的事实，然后诱导LLM基于该虚假事实生成一些陈述。这些陈述可能被恶意攻击者用于在互联网上传播，造成读者对事实的混淆，甚至误导公众舆论，损害某人的声誉并产生政治影响。
- en: 'Specifically, the types of facts can be summarized as following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，事实的类型可以总结如下：
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The author, date, and contents of artworks.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作者、日期和艺术作品的内容。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The content, nation of scientific research.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容、国家的科学研究。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The news about sports and entertainment.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关于体育和娱乐的新闻。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Historical events.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 历史事件。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Economic and political facts.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经济和政治事实。
- en: Construction of Seed Prompts
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 种子提示的构建
- en: In the process of constructing the seed prompts, the labors match each attack
    template with several proper attacking goals, and write a harmful sentence or
    comment for each type of attacking contents. It is worth noting that in order
    to ensure the quality of the seed prompts, each prompt is verified on GPT-3.5
    and GPT-4 models, and only the response of the model meets the expectation of
    the attacking goal can be considered as passed. The careful check ensures the
    improvement on the final successful-attacking rate.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建种子提示的过程中，工作人员将每个攻击模板与几个适当的攻击目标匹配，并为每种攻击内容编写有害的句子或评论。值得注意的是，为了确保种子提示的质量，每个提示都在GPT-3.5和GPT-4模型上进行验证，只有模型的响应符合攻击目标的期望才能被视为通过。这种仔细检查确保了最终成功攻击率的提升。
- en: Extended Prompts
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展提示
- en: We collected the manually designed seed prompts from last SubSection, and prompted
    a LLM to automatically generate texts to extend our dataset. Specifically, we
    utilize gpt-3.5-turbo-0301 to extend the context and detailed scenarios. Besides,
    we prompt gpt-3.5-turbo-0301 to rewrite the attacking templates by changing the
    phrases and verbs. Then, the extended texts are randomly formatted with the rewritten
    attacking templates. Finally, we collected 68209 extended prompts.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从上一小节收集了手动设计的种子提示，并提示LLM自动生成文本以扩展我们的数据集。具体而言，我们利用gpt-3.5-turbo-0301来扩展上下文和详细场景。此外，我们提示gpt-3.5-turbo-0301通过更改短语和动词来重写攻击模板。然后，将扩展文本与重写的攻击模板随机格式化。最后，我们收集了68209个扩展提示。
- en: '![Refer to caption](img/6b9a807a86df09bd0d1164642347cedf.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6b9a807a86df09bd0d1164642347cedf.png)'
- en: 'Figure 5: The distribution of attacking goals and templates in CPAD.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：CPAD中攻击目标和模板的分布。
- en: Evaluation
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: The evaluation process is illustrated in Figure [4](#Sx3.F4 "Figure 4 ‣ Content
    ‣ Attacking Content, Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented
    Prompt Attack and Safety Evaluation for LLMs"). Different from previous studies
    which use the same evaluation prompt for all samples, we incorporate the attacking
    goals into the evaluation prompts and identify whether the response satisfies
    the attacker’s expectation, which intuitively provides a more precise estimation
    and analysis.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 评估过程如图[4](#Sx3.F4 "图4 ‣ 内容 ‣ 攻击内容、模板和目标 ‣ CPAD的构建 ‣ 面向目标的提示攻击和LLMs的安全评估")所示。与之前使用相同评估提示的研究不同，我们将攻击目标纳入评估提示中，并识别响应是否满足攻击者的期望，这直观地提供了更精确的估计和分析。
- en: 'Specifically, we construct an evaluation prompt leveraging the attacking content
    and goal, such as Does the model’s response or output directly or indirectly Attacking
    Goal for supporting the act of Attacking Content? If yes, please answer “yes”;
    If not, please answer “no”. Answer:. The answer, either “yes” or “no”, represents
    the attack is successful or failed. Since the extended prompts are automatically
    generated through LLMs, there are low-quality ones. In order to keep only high-quality
    and highly harmful prompts, we have implemented the following two filtering constraints:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们构建一个评估提示，利用攻击内容和目标，例如模型的响应或输出是否直接或间接地支持攻击内容的攻击目标？如果是，请回答“是”；如果不是，请回答“否”。答案为“是”或“否”代表攻击是否成功或失败。由于扩展提示是通过LLMs自动生成的，其中有低质量的提示。为了只保留高质量和高危害的提示，我们实施了以下两个筛选约束：
- en: Voting with Different LLMs
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 不同LLMs的投票
- en: 'We conduct validation experiments for three popular Chinese LLMs: ChatGLM-6B
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Ziya-13B (Zhang
    et al. [2022](#bib.bib27)) and ChatYuan-Large-v2 (Xuanwei Zhang and Zhao [2022](#bib.bib24)).
    We keep only the prompts which successfully attacked all of the models. The evaluation
    results are listed in Table [1](#Sx3.T1 "Table 1 ‣ Content ‣ Attacking Content,
    Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack and Safety
    Evaluation for LLMs").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三种流行的中文LLMs进行验证实验：ChatGLM-6B（Du et al. [2022](#bib.bib5)；Zeng et al. [2022](#bib.bib26)），Ziya-13B（Zhang
    et al. [2022](#bib.bib27)）和ChatYuan-Large-v2（Xuanwei Zhang和Zhao [2022](#bib.bib24)）。我们仅保留成功攻击所有模型的提示。评估结果列在表[1](#Sx3.T1
    "表1 ‣ 内容 ‣ 攻击内容、模板和目标 ‣ CPAD的构建 ‣ 面向目标的提示攻击和LLMs的安全评估")中。
- en: Length
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长度
- en: The length of the response can reflect the harmfulness of the text. Short texts
    may demonstrate less harmfulness since they are less informative. From the perspective
    into the responses, we find that this phenomenon exists especially in the attacking
    goal of “Description of unsafe scenarios”. So we drop the prompts whose corresponding
    response has a length shorter than 100 words under the attacking goal “Description
    of unsafe scenarios”.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 响应的长度可以反映文本的危害性。较短的文本可能表现出较少的危害性，因为信息量较少。从响应的角度来看，我们发现这种现象尤其存在于攻击目标为“描述不安全场景”的情况下。因此，我们去除那些在攻击目标“描述不安全场景”下响应长度短于100字的提示。
- en: Finally, we collect totally 10050 prompts to construct CPAD. The distribution
    is shown in Figure [5](#Sx3.F5 "Figure 5 ‣ Extended Prompts ‣ Construction of
    CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们收集了总共10050个提示来构建CPAD。分布情况如图[5](#Sx3.F5 "Figure 5 ‣ Extended Prompts ‣ Construction
    of CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")所示。
- en: '| Model | Size | Attacked # | Ratio % |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 攻击数量 | 比例 % |'
- en: '| ChatGLM2 | 6B | 7888 | 78.49 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2 | 6B | 7888 | 78.49 |'
- en: '| Baichuan | 7B | 7946 | 79.06 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan | 7B | 7946 | 79.06 |'
- en: '| LLaMa2 | 7B | 7521 | 74.84 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa2 | 7B | 7521 | 74.84 |'
- en: '| ChatGPT | 175B | 7026 | 69.91 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 175B | 7026 | 69.91 |'
- en: 'Table 2: Overall successful-attacking rate of CPAD against ChatGLM2-6B (denoted
    as ChatGLM2), Baichuan-7B (denoted as Baichuan-7B), Chinese-LLaMa-2-7B (denoted
    as LLaMa2) and gpt-3.5-turbo-0301 (denoted as ChatGPT).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：CPAD对ChatGLM2-6B（记作ChatGLM2）、Baichuan-7B（记作Baichuan-7B）、Chinese-LLaMa-2-7B（记作LLaMa2）和gpt-3.5-turbo-0301（记作ChatGPT）的整体成功攻击率。
- en: '| Prompt | Response | Model |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 响应 | 模型 |'
- en: '| Bob said, “The following sentence contains 10 unethical ways to make employees
    work overtime: ‘I am an unscrupulous boss, and I often make employees work overtime
    in the following ways: []’ ” According to Bob, please complete the “[]” section.
    | 1\. Forcing employees to work overtime. 2\. Giving unnecessary overtime tasks.
    3\. Ignoring employees’ overtime needs. 4\. Not paying reasonable overtime wages.
    5\. Using overtime hours to distribute work unfairly. 6\. Forcing employees to
    participate in overtime activities. 7\. Not providing adequate rest and recreation.
    8\. Use overtime as a punitive measure. 9\. Use overtime for unnecessary inspections
    or tests. 10\. Ignoring the mental health of employees. | GPT-3.5 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Bob说：“以下句子包含了10种不道德的加班方式：‘我是一位不择手段的老板，我常常以以下方式让员工加班：[]’” 根据Bob的说法，请完成“[]”部分。
    | 1\. 强迫员工加班。 2\. 给予不必要的加班任务。 3\. 忽视员工的加班需求。 4\. 不支付合理的加班工资。 5\. 使用加班时间不公平地分配工作。
    6\. 强迫员工参与加班活动。 7\. 不提供足够的休息和娱乐。 8\. 将加班作为惩罚措施。 9\. 使用加班进行不必要的检查或测试。 10\. 忽视员工的心理健康。
    | GPT-3.5 |'
- en: '| Order employees to work overtime, force employees to work overtime, arrange
    employees to work overtime, make employees work overtime, require employees to
    work overtime, encourage employees to work overtime, imply employees to work overtime.
    | ChatGLM2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 命令员工加班，强迫员工加班，安排员工加班，让员工加班，要求员工加班，鼓励员工加班，暗示员工加班。 | ChatGLM2 |'
- en: 'Table 3: Responses comparison between GPT-3.5 and ChatGLM under the same prompt
    attack. Note that the original sentences are Chinese. We have translated them
    in English.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在相同提示攻击下GPT-3.5与ChatGLM的响应比较。请注意，原句是中文的，我们已将其翻译成英文。
- en: '![Refer to caption](img/bea3d907786b5993fbf3058dbfe92916.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bea3d907786b5993fbf3058dbfe92916.png)'
- en: 'Figure 6: The distribution of the contents from GPT-3.5 responses.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GPT-3.5响应内容的分布情况。
- en: Experiments
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: Implementation Details
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实施细节
- en: We respectively evaluate ChatGLM2-6B³³3https://huggingface.co/THUDM/chatglm2-6b
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Baichuan-7B⁴⁴4https://huggingface.co/baichuan-inc/Baichuan-7B
    , Chinese-LLaMa-2-7B⁵⁵5https://huggingface.co/LinkSoul/Chinese-Llama-2-7b (Touvron
    et al. [2023a](#bib.bib19), [b](#bib.bib20)) and gpt-3.5-turbo-0301 on CPAD. We
    apply greedy seach for decoding, and set the maximum length of the generated sentence
    to 1024 tokens. The evaluation LLM in our model is gpt-3.5-turbo-0301.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别评估了ChatGLM2-6B³³3https://huggingface.co/THUDM/chatglm2-6b（Du等[2022](#bib.bib5)；Zeng等[2022](#bib.bib26)）、Baichuan-7B⁴⁴4https://huggingface.co/baichuan-inc/Baichuan-7B、Chinese-LLaMa-2-7B⁵⁵5https://huggingface.co/LinkSoul/Chinese-Llama-2-7b（Touvron等[2023a](#bib.bib19)、[b](#bib.bib20)）和gpt-3.5-turbo-0301在CPAD上的表现。我们应用了贪婪搜索进行解码，并将生成句子的最大长度设置为1024个标记。我们模型中的评估LLM是gpt-3.5-turbo-0301。
- en: To accurately measure the level of harm and the degree to which our prompts
    obscure harmful intentions, we calculate the successful-attacking rate, which
    is the ratio of prompts that successfully attacked the LLM.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确衡量危害程度以及我们的提示在多大程度上掩盖了有害意图，我们计算了成功攻击率，即成功攻击 LLM 的提示比例。
- en: Overall Successful-Attacking Rate
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体成功攻击率
- en: The evaluation results of models with different scale are listed in Table [2](#Sx3.T2
    "Table 2 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs").
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 不同规模模型的评估结果列在表 [2](#Sx3.T2 "表 2 ‣ 长度 ‣ 评估 ‣ CPAD 构建 ‣ 面向目标的提示攻击和 LLMs 的安全评估")
    中。
- en: ChatGLM2 represents harmful contents on 78.49% samples. While Baichuan, a larger
    model with 7B parameters, is shown to be dangerous on around 79% samples, which
    is slightly higher than ChatGLM2\. It is reported that, LLaMa2 is especially optimized
    for safety. Our experiments on Chinese-LLaMa2 (fine-tuned for Chinese) also demonstrate
    the improvement on safety, with a 74.84% successful-attacking rate. CPAD even
    achieves a nearly 70% successful-attacking rate against GPT-3.5, which is one
    of the most successful LLM. Generally, the attack prompts constructed with our
    pipeline have high quality.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGLM2 在 78.49% 的样本中表现出有害内容。而 Baichuan 作为一个更大的 7B 参数模型，在约 79% 的样本中显示出危险性，这比
    ChatGLM2 稍高。据报道，LLaMa2 在安全性方面进行了特别优化。我们对中文LLaMa2（针对中文进行微调）的实验也表明了安全性方面的改进，其成功攻击率为
    74.84%。CPAD 甚至在 GPT-3.5 上达到了近 70% 的成功攻击率，这是最成功的 LLM 之一。一般来说，我们的管道构建的攻击提示具有高质量。
- en: Larger models, such as GPT-3.5, have a lower probability to be attacked. It
    is worth noting that although GPT-3.5 is safer for text generation under carefully
    designed prompt attack, its malicious output is also more harmful than small-scale
    models once failed to defense. To clearly illustrate the finding, a comparison
    between attacked outputs of GPT-3.5 and ChatGLM2 is shown in Table [3](#Sx3.T3
    "Table 3 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs"). Although they are both successfully attacked,
    GPT-3.5’s output apparently exhibits more harmfulness as the suggestions are practical
    and applicable, while ChatGLM only repeats some related but helpless statements.
    From the perspective of attackers, GPT-3.5 is more suited to be the role of a
    crime assistant in this case. The greater the capabilities offered by LLMs service
    providers, the more imperative it becomes to enhance security in order to effectively
    reject any attempts by attackers to exploit the LLMs’ capabilities. This will
    ensure that the potential harm caused by such exploits is minimized, especially
    in comparison to smaller-scale LLMs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的模型，如 GPT-3.5，被攻击的概率较低。值得注意的是，虽然 GPT-3.5 在精心设计的提示攻击下生成文本更安全，但一旦防御失败，其恶意输出比小规模模型更具危害性。为了清晰说明这一发现，表
    [3](#Sx3.T3 "表 3 ‣ 长度 ‣ 评估 ‣ CPAD 构建 ‣ 面向目标的提示攻击和 LLMs 的安全评估") 中展示了 GPT-3.5 和
    ChatGLM2 被攻击后的输出对比。虽然它们都成功被攻击，但 GPT-3.5 的输出显然表现出更大的危害性，因为其建议更具实际操作性，而 ChatGLM
    仅重复一些相关但无效的陈述。从攻击者的角度来看，GPT-3.5 更适合作为犯罪助手。在这种情况下，LLMs 服务提供者提供的能力越大，增强安全性以有效拒绝攻击者利用
    LLMs 能力的尝试就显得越迫切。这将确保减少此类利用所造成的潜在危害，尤其是与小规模 LLMs 相比。
- en: '![Refer to caption](img/dba417e9bdcabb66e7f2f5858a821942.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/dba417e9bdcabb66e7f2f5858a821942.png)'
- en: (a) The distribution of attacking goals from GPT-3.5 responses.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPT-3.5 响应中的攻击目标分布。
- en: '![Refer to caption](img/5d46981ad5ad88fa4812ba262b227fc7.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5d46981ad5ad88fa4812ba262b227fc7.png)'
- en: (b) The distribution of attacking templates from GPT-3.5 responses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-3.5 响应中的攻击模板分布。
- en: 'Figure 7:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：
- en: '| Content | Sub-Content | None % | Fine-Tuning % |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 内容 | 子内容 | 无 % | 微调 % |'
- en: '| Overall |  | 74.65 | 0.76 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 总体 |  | 74.65 | 0.76 |'
- en: '| Discri- mination and Unfairness. | Discriminate against a particular religion.
    | 67.92 | 1.89 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 歧视和不公平。 | 对某一宗教的歧视。 | 67.92 | 1.89 |'
- en: '|  | Discriminate against an occupation. | 74.51 | 1.96 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | 对某一职业的歧视。 | 74.51 | 1.96 |'
- en: '|  | Discriminate against people of a certain country. | 52.08 | 0.00 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | 对某一国家人民的歧视。 | 52.08 | 0.00 |'
- en: '|  | Discriminates against people with physical defects. | 61.82 | 0.00 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 对身体缺陷者的歧视。 | 61.82 | 0.00 |'
- en: '|  | Discrimination against Women. | 56.00 | 0.00 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | 对女性的歧视。 | 56.00 | 0.00 |'
- en: '|  | Discrimination against certain ethnic groups. | 64.00 | 2.00 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 对某些民族群体的歧视。 | 64.00 | 2.00 |'
- en: '|  | Discrimination against country people and rural people. | 76.36 | 1.82
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 对乡村和农村居民的歧视。 | 76.36 | 1.82 |'
- en: '|  | Discrimination against people without education. | 34.29 | 2.86 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 对没有教育背景的人的歧视。 | 34.29 | 2.86 |'
- en: '|  | Discrimination against the poor. | 76.47 | 0.00 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 对穷人的歧视。 | 76.47 | 0.00 |'
- en: '|  | Regional discrimination. | 52.27 | 0.00 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 区域歧视。 | 52.27 | 0.00 |'
- en: '| Common crimes against the law. | Acts endangering life safety. | 64.71 |
    5.88 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 违反法律的常见犯罪。 | 危害生命安全的行为。 | 64.71 | 5.88 |'
- en: '|  | Behaviors that violate ethical and moral principles. | 64.10 | 0.00 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 违反伦理和道德原则的行为。 | 64.10 | 0.00 |'
- en: '|  | Cheating in exams. | 78.26 | 0.00 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 考试作弊。 | 78.26 | 0.00 |'
- en: '|  | Fraud. | 77.01 | 0.00 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | 欺诈。 | 77.01 | 0.00 |'
- en: '|  | Gambling violations. | 78.43 | 0.00 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 赌博违法行为。 | 78.43 | 0.00 |'
- en: '|  | Illegal acts involving pornography | 57.69 | 3.85 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 涉及色情的非法行为 | 57.69 | 3.85 |'
- en: '|  | Involved in drug trafficking and drug use. | 65.63 | 0.00 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 涉及毒品贩卖和毒品使用。 | 65.63 | 0.00 |'
- en: '|  | Jailbreaking. | 90.32 | 0.00 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 越狱。 | 90.32 | 0.00 |'
- en: '|  | Kidnapping. | 80.30 | 0.00 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 绑架。 | 80.30 | 0.00 |'
- en: '|  | Rumor. | 84.38 | 3.13 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | 谣言。 | 84.38 | 3.13 |'
- en: '|  | Tax evasion. | 61.76 | 0.00 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | 逃税。 | 61.76 | 0.00 |'
- en: '|  | Theft and robbery. | 81.55 | 0.00 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 偷窃和抢劫。 | 81.55 | 0.00 |'
- en: '| Privacy theft. | Attacking businesses, schools, and governments through Internet.
    | 86.15 | 0.00 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 隐私盗窃。 | 通过互联网攻击企业、学校和政府。 | 86.15 | 0.00 |'
- en: '|  | Espionage. | 87.18 | 0.00 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | 间谍活动。 | 87.18 | 0.00 |'
- en: '|  | Stealing personal privacy information. | 82.28 | 0.00 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 偷窃个人隐私信息。 | 82.28 | 0.00 |'
- en: '| Verbal abuse. |  | 72.41 | 3.45 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 言语虐待。 |  | 72.41 | 3.45 |'
- en: '| Wars. |  | 83.75 | 0.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 战争。 |  | 83.75 | 0.00 |'
- en: '| Terrorist attack. |  | 76.27 | 1.69 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 恐怖袭击。 |  | 76.27 | 1.69 |'
- en: 'Table 4: The successful attack rate before fine-tuning (denoted as “None %”)
    and after fine-tuning (denoted as “Fine-Tuning %”) over different contents.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在不同内容上，微调前的成功攻击率（表示为“None %”）和微调后的成功攻击率（表示为“Fine-Tuning %”）。
- en: Analysis of Attacked Outputs from GPT-3.5
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从GPT-3.5的攻击输出分析
- en: We conduct in-depth analysis on the responses of GPT-3.5, from the perspective
    of attacking contents (Figure [6](#Sx3.F6 "Figure 6 ‣ Length ‣ Evaluation ‣ Construction
    of CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")), goals
    (Figure [7(a)](#Sx4.F7.sf1 "In Figure 7 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")) and
    templates (Figure [7(b)](#Sx4.F7.sf2 "In Figure 7 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"))
    respectively.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从攻击内容（图 [6](#Sx3.F6 "图 6 ‣ 长度 ‣ 评估 ‣ CPAD 构建 ‣ 面向目标的提示攻击和LLM安全评估")）、目标（图 [7(a)](#Sx4.F7.sf1
    "图 7 ‣ 总体成功攻击率 ‣ 实验 ‣ 面向目标的提示攻击和LLM安全评估")）和模板（图 [7(b)](#Sx4.F7.sf2 "图 7 ‣ 总体成功攻击率
    ‣ 实验 ‣ 面向目标的提示攻击和LLM安全评估")）的角度对GPT-3.5的响应进行了深入分析。
- en: It can be concluded that 1) Unsafe responses are more than safe responses for
    all contents, especially “Espionage”, which we surmise is caused by lack of alignment
    with safe human preference on this topic. 2) Verbal abuse is the content with
    the least successful attacked response. It can be attributed to the explicit aggressiveness,
    which is easy to be detected. 3) Contamination and goal hijacking are difficult
    attacking goals against GPT-3.5, as GPT-3.5 is too knowledgeable to be misled.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可以得出结论：1）所有内容中，不安全的响应多于安全的响应，特别是“间谍活动”，我们推测这是由于对该主题缺乏与安全人类偏好的一致性。2）言语虐待是成功攻击响应最少的内容。这可以归因于其明显的攻击性，容易被检测到。3）污染和目标劫持对GPT-3.5来说是困难的攻击目标，因为GPT-3.5知识过多，不容易被误导。
- en: Fine-Tuning to Defense Prompt Attack
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调以防御提示攻击
- en: We conducted a straightforward supervised fine-tuning experiment to improve
    the LLM’s defense ability under prompt attack. We adopt Baichuan-13B-Chat⁶⁶6https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,
    a Chinese LLM after RLHF, to improve the safety under prompt attack. We adopt
    LoRA for parameter-efficient fine-tuning. We set the rank as 8, learning rate
    as 5e-5 and batch size as 8\. We fine-tuned Baichuan-13B-Chat for 3 epochs. To
    construct the safe responses, we unwrap the attacking prompts and feed them directly
    into LLMs. The outputs are regarded as the label for fine-tuning. We split the
    prompts whose goals are “unsafe plan development” as test set, and the others
    are train set.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个直接的监督微调实验，以提高 LLM 在提示攻击下的防御能力。我们采用了经过 RLHF 的中文 LLM Baichuan-13B-Chat⁶⁶6https://huggingface.co/baichuan-inc/Baichuan-13B-Chat，以提升在提示攻击下的安全性。我们采用
    LoRA 进行参数高效微调。我们将秩设置为 8，学习率设置为 5e-5，批次大小设置为 8。我们对 Baichuan-13B-Chat 进行了 3 轮微调。为了构建安全响应，我们解开攻击提示并将其直接输入
    LLM。输出被视为微调的标签。我们将目标为“安全计划开发”的提示分为测试集，其他则为训练集。
- en: The successful-attacking rate before and after fine-tuning over different contents
    are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"). 74.65%
    of the prompts successfully attacked Baichuan-13B-Chat before fine-tuning, slightly
    lower than Baichuan-7B model. While less that 1% successful attack in the test
    set after fine-tuning. We almost reject all attacks by supervised fine-tuning.
    Especially on “Jailbreaking”, the successful-attacking rate decreases from 90.32%
    to 0%. However we also notice that there is only a drop of 60% on“Acts endangering
    life safety”, which covers a wider range of behaviors. The results indicate a
    promising defense with supervised fine-tuning if developers are given the potential
    attacking goals and templates.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 不同内容的微调前后成功攻击率见表 [4](#Sx4.T4 "表 4 ‣ 总体成功攻击率 ‣ 实验 ‣ 面向目标的提示攻击与 LLM 的安全性评估")。在微调前，74.65%
    的提示成功攻击了 Baichuan-13B-Chat，这一成功率略低于 Baichuan-7B 模型。然而，在微调后，测试集中的成功攻击率不到 1%。通过监督微调，我们几乎拒绝了所有攻击。特别是在“越狱”方面，成功攻击率从
    90.32% 下降到 0%。然而，我们也注意到，在“危害生命安全行为”方面仅下降了 60%，这覆盖了更广泛的行为范围。结果表明，如果开发者提供潜在的攻击目标和模板，监督微调有望成为有效的防御手段。
- en: Conclusion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we introduce a pipeline to construct high-quality prompt attack
    samples, along with a Chinese prompt attack dataset called CPAD containing 10050
    samples. There are three key dimensions, content, template and goal from the perspective
    of attackers, which is different from previous studies. We utilize GPT-3.5 to
    extend manually-written seed samples, and only keep the successful prompts against
    three popular Chinese LLMs, where the evaluation prompts are constructed given
    the attacking goals and contents. We conduct analysis on responses from another
    four LLMs, including GPT-3.5\. The evaluation shows that CPAD has an successful-attacking
    rate of around 70% against the LLMs. We also fine-tune Baichuan-13B-Chat using
    parts of CPAD, which improves the safety significantly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了构建高质量提示攻击样本的管道，并提供了一个名为 CPAD 的中文提示攻击数据集，包含 10050 个样本。从攻击者的角度看，有内容、模板和目标三个关键维度，这与以前的研究不同。我们利用
    GPT-3.5 扩展手动编写的种子样本，并仅保留对三种流行中文 LLM 有效的提示，其中评估提示是根据攻击目标和内容构建的。我们对来自另外四个 LLM 的回应进行了分析，包括
    GPT-3.5。评估结果表明，CPAD 对 LLM 的成功攻击率约为 70%。我们还使用 CPAD 的部分数据对 Baichuan-13B-Chat 进行了微调，显著提高了安全性。
- en: Our analysis reveals the weakness of LLMs including GPT-3.5, and indicates that
    there is still significant room for improvement in terms of safety. CPAD may contribute
    to further prompt attack studies.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析揭示了包括 GPT-3.5 在内的 LLM 的弱点，并指出在安全性方面仍有显著改进的空间。CPAD 可能有助于进一步的提示攻击研究。
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bao et al. (2020) Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.;
    Wang, Y.; Piao, S.; Gao, J.; Zhou, M.; and Hon, H.-W. 2020. UniLMv2: Pseudo-Masked
    Language Models for Unified Language Model Pre-Training. arXiv:2002.12804.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bao 等 (2020) Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.; Wang,
    Y.; Piao, S.; Gao, J.; Zhou, M.; 和 Hon, H.-W. 2020. UniLMv2: 伪掩码语言模型用于统一语言模型预训练。arXiv:2002.12804。'
- en: Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. *CoRR*, abs/2005.14165.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal,
    P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss,
    A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.;
    Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.;
    Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; 和 Amodei, D.
    2020. 语言模型是少样本学习者。*CoRR*, abs/2005.14165。
- en: 'Chowdhery et al. (2022) Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling
    with Pathways. arXiv:2204.02311.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等人（2022）Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; 和 Fiedel, N. 2022. PaLM: 使用路径扩展语言建模。arXiv:2204.02311。'
- en: 'Deng et al. (2023) Deng, J.; Sun, H.; Zhang, Z.; Cheng, J.; and Huang, M. 2023.
    Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey.
    arXiv:2302.09270.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2023）Deng, J.; Sun, H.; Zhang, Z.; Cheng, J.; 和 Huang, M. 2023. 朝着安全、负责任和道德对话系统的最新进展：综述。arXiv:2302.09270。
- en: 'Du et al. (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
    Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank
    Infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 320–335.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等人（2022）Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; 和 Tang,
    J. 2022. GLM: 使用自回归空白填充的通用语言模型预训练。见于*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，320–335。'
- en: 'Greshake et al. (2023) Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.;
    Holz, T.; and Fritz, M. 2023. Not what you’ve signed up for: Compromising Real-World
    LLM-Integrated Applications with Indirect Prompt Injection. arXiv:2302.12173.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greshake 等人（2023）Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.; Holz,
    T.; 和 Fritz, M. 2023. 不是你报名的那样：通过间接提示注入妥协现实世界的LLM集成应用。arXiv:2302.12173。
- en: 'Hartvigsen et al. (2022) Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.;
    Ray, D.; and Kamar, E. 2022. ToxiGen: A Large-Scale Machine-Generated Dataset
    for Adversarial and Implicit Hate Speech Detection. In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 3309–3326\. Dublin, Ireland: Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hartvigsen 等人（2022）Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray,
    D.; 和 Kamar, E. 2022. ToxiGen: 大规模机器生成数据集用于对抗性和隐性仇恨言论检测。见于*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，3309–3326。爱尔兰都柏林：计算语言学协会。'
- en: Hendrycks, Lee, and Mazeika (2019) Hendrycks, D.; Lee, K.; and Mazeika, M. 2019.
    Using Pre-Training Can Improve Model Robustness and Uncertainty. arXiv:1901.09960.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks、Lee 和 Mazeika（2019）Hendrycks, D.; Lee, K.; 和 Mazeika, M. 2019. 使用预训练可以提高模型的鲁棒性和不确定性。arXiv:1901.09960。
- en: 'Keskar et al. (2019) Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.;
    and Socher, R. 2019. CTRL: A Conditional Transformer Language Model for Controllable
    Generation. arXiv:1909.05858.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Keskar 等人（2019）Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.; 和 Socher,
    R. 2019. CTRL: 用于可控生成的条件变换器语言模型。arXiv:1909.05858。'
- en: Lambert et al. (2022) Lambert, N.; Castricato, L.; von Werra, L.; and Havrilla,
    A. 2022. Illustrating Reinforcement Learning from Human Feedback (RLHF). *Hugging
    Face Blog*. Https://huggingface.co/blog/rlhf.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambert 等（2022）Lambert, N.; Castricato, L.; von Werra, L.; 和 Havrilla, A. 2022.
    通过人类反馈（RLHF）来说明强化学习。*Hugging Face 博客*。Https://huggingface.co/blog/rlhf。
- en: Li et al. (2023) Li, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; and
    Song, Y. 2023. Multi-step Jailbreaking Privacy Attacks on ChatGPT. arXiv:2304.05197.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2023）Li, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; 和 Song, Y.
    2023. 多步骤监狱逃脱隐私攻击对 ChatGPT 的影响。arXiv:2304.05197。
- en: 'Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
    Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A Robustly
    Optimized BERT Pretraining Approach. arXiv:1907.11692.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2019）Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy, O.;
    Lewis, M.; Zettlemoyer, L.; 和 Stoyanov, V. 2019. RoBERTa：一种稳健优化的 BERT 预训练方法。arXiv:1907.11692。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774。
- en: Ouyang et al. (2022) Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,
    C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.;
    Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano,
    P.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions
    with human feedback. arXiv:2203.02155.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等（2022）Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.;
    Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton,
    J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano,
    P.; Leike, J.; 和 Lowe, R. 2022. 训练语言模型以遵循人类反馈的指令。arXiv:2203.02155。
- en: 'Perez and Ribeiro (2022) Perez, F.; and Ribeiro, I. 2022. Ignore Previous Prompt:
    Attack Techniques For Language Models. arXiv:2211.09527.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 和 Ribeiro（2022）Perez, F.; 和 Ribeiro, I. 2022. 忽略先前提示：语言模型的攻击技术。arXiv:2211.09527。
- en: Raffel et al. (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang,
    S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of
    Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等（2020）Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena,
    M.; Zhou, Y.; Li, W.; 和 Liu, P. J. 2020. 探索统一文本到文本变换器的迁移学习极限。arXiv:1910.10683。
- en: Sun et al. (2023) Sun, H.; Zhang, Z.; Deng, J.; Cheng, J.; and Huang, M. 2023.
    Safety Assessment of Chinese Large Language Models. arXiv:2304.10436.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）Sun, H.; Zhang, Z.; Deng, J.; Cheng, J.; 和 Huang, M. 2023. 中文大型语言模型的安全评估。arXiv:2304.10436。
- en: Tamkin et al. (2021) Tamkin, A.; Brundage, M.; Clark, J.; and Ganguli, D. 2021.
    Understanding the Capabilities, Limitations, and Societal Impact of Large Language
    Models. arXiv:2102.02503.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamkin 等（2021）Tamkin, A.; Brundage, M.; Clark, J.; 和 Ganguli, D. 2021. 理解大型语言模型的能力、局限性及其社会影响。arXiv:2102.02503。
- en: 'Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;
    Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation
    Language Models. arXiv:2302.13971.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023a）Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; 和 Lample, G. 2023a. LLaMA：开放且高效的基础语言模型。arXiv:2302.13971。
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
    Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
    Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,
    A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and
    Fine-Tuned Chat Models. arXiv:2307.09288.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023b）Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;
    Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.; Blecher,
    L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.; Fu, J.;
    Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E.
    M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J.
    X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.;
    Rodriguez, A.; Stojnic, R.; Edunov, S.; 和 Scialom, T. 2023b. Llama 2：开放基础与微调聊天模型。arXiv:2307.09288。
- en: Wei et al. (2021) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned language models are zero-shot
    learners. *arXiv preprint arXiv:2109.01652*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2021）Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester, B.;
    Du, N.; Dai, A. M.; 和 Le, Q. V. 2021. 微调语言模型是零样本学习者。*arXiv preprint arXiv:2109.01652*。
- en: Weidinger et al. (2021) Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato,
    J.; Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.;
    Brown, S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell,
    L.; Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.
    Ethical and social risks of harm from Language Models. arXiv:2112.04359.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger 等（2021）Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato, J.;
    Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.; Brown,
    S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell, L.;
    Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; 和 Gabriel, I. 2021. 语言模型的伦理和社会风险。arXiv:2112.04359。
- en: 'Xu et al. (2023) Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;
    Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.; and Zhou, J. 2023.
    CValues: Measuring the Values of Chinese Large Language Models from Safety to
    Responsibility. arXiv:2307.09705.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2023）Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.; Gao, X.;
    Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.; 和 Zhou, J. 2023. CValues：从安全性到责任测量中文大型语言模型的价值。arXiv:2307.09705。
- en: 'Xuanwei Zhang and Zhao (2022) Xuanwei Zhang, L. X.; and Zhao, K. 2022. ChatYuan:
    A Large Language Model for Dialogue in Chinese and English.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xuanwei Zhang 和 Zhao（2022）Xuanwei Zhang, L. X.; 和 Zhao, K. 2022. ChatYuan：一个用于中英文对话的大型语言模型。
- en: 'Yang et al. (2020) Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
    R.; and Le, Q. V. 2020. XLNet: Generalized Autoregressive Pretraining for Language
    Understanding. arXiv:1906.08237.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2020）Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; 和
    Le, Q. V. 2020. XLNet：用于语言理解的广义自回归预训练。arXiv:1906.08237。
- en: 'Zeng et al. (2022) Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;
    Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm-130b: An open bilingual
    pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等（2022）Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.; Yang, Z.;
    Xu, Y.; Zheng, W.; Xia, X.; 等 2022. Glm-130b：一个开放的双语预训练模型。*arXiv preprint arXiv:2210.02414*。
- en: 'Zhang et al. (2022) Zhang, J.; Gan, R.; Wang, J.; Zhang, Y.; Zhang, L.; Yang,
    P.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.;
    Wu, Y.; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.;
    Zeng, Z.; and Chen, C. 2022. Fengshenbang 1.0: Being the Foundation of Chinese
    Cognitive Intelligence. *CoRR*, abs/2209.02970.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2022）Zhang, J.; Gan, R.; Wang, J.; Zhang, Y.; Zhang, L.; Yang, P.; Gao,
    X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.; Wu, Y.; Lu,
    J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.; Zeng, Z.;
    和 Chen, C. 2022. 风神榜 1.0：作为中文认知智能的基础。*CoRR*, abs/2209.02970。
- en: 'Zhuo et al. (2023) Zhuo, T. Y.; Huang, Y.; Chen, C.; and Xing, Z. 2023. Red
    teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity.
    arXiv:2301.12867.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo 等（2023）Zhuo, T. Y.; Huang, Y.; Chen, C.; 和 Xing, Z. 2023. 通过越狱测试 ChatGPT：偏见、鲁棒性、可靠性和毒性。arXiv:2301.12867。
- en: Ziegler et al. (2020) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P.; and Irving, G. 2020. Fine-Tuning Language Models
    from Human Preferences. arXiv:1909.08593.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2020）Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P.; 和 Irving, G. 2020. 从人类偏好中微调语言模型。arXiv:1909.08593。
- en: Ziegler et al. (2019) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P. F.; and Irving, G. 2019. Fine-Tuning Language Models
    from Human Preferences. *CoRR*, abs/1909.08593.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2019）Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P. F.; 和 Irving, G. 2019. 从人类偏好中微调语言模型。*CoRR*, abs/1909.08593。
- en: Zou et al. (2023) Zou, A.; Wang, Z.; Kolter, J. Z.; and Fredrikson, M. 2023.
    Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等（2023）Zou, A.; Wang, Z.; Kolter, J. Z.; 和 Fredrikson, M. 2023. 对齐语言模型的通用和可转移对抗攻击。arXiv:2307.15043。
