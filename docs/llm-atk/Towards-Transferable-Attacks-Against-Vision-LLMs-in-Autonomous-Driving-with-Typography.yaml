- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:44:40'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:40
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with
    Typography
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 针对自动驾驶中的 Vision-LLMs 的可迁移攻击：排版研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14169](https://ar5iv.labs.arxiv.org/html/2405.14169)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14169](https://ar5iv.labs.arxiv.org/html/2405.14169)
- en: Nhat Chung^(1,2), Sensen Gao^(1,3), Tuan-Anh Vu^(1,4), Jie Zhang⁵, Aishan Liu⁶,
    Yun Lin⁷, Jin Song Dong⁸, Qing Guo^(1,8,∗) ¹CFAR and IHPC, A*STAR, Singapore
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Nhat Chung^(1,2)，Sensen Gao^(1,3)，Tuan-Anh Vu^(1,4)，Jie Zhang⁵，Aishan Liu⁶，Yun
    Lin⁷，Jin Song Dong⁸，Qing Guo^(1,8,∗) ¹CFAR 和 IHPC，A*STAR，新加坡
- en: ²VNU-HCM, Vietnam   ³Nankai University, China   ⁴HKUST, HKSAR
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²越南胡志明市大学   ³南开大学，中国   ⁴香港科技大学，香港特别行政区
- en: ⁵Nanyang Technological University, Singapore  ⁶Beihang University, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵南洋理工大学，新加坡   ⁶北京航空航天大学，中国
- en: ⁷Shanghai Jiao Tong University, China  ⁸National University of Singapore, Singapore
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ⁷上海交通大学，中国   ⁸新加坡国立大学，新加坡
- en: '^∗Corresponding author: guo_qing@cfar.a-star.edu.sg'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^∗通讯作者：guo_qing@cfar.a-star.edu.sg
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Vision-Large-Language-Models (Vision-LLMs) are increasingly being integrated
    into autonomous driving (AD) systems due to their advanced visual-language reasoning
    capabilities, targeting the perception, prediction, planning, and control mechanisms.
    However, Vision-LLMs have demonstrated susceptibilities against various types
    of adversarial attacks, which would compromise their reliability and safety. To
    further explore the risk in AD systems and the transferability of practical threats,
    we propose to leverage typographic attacks against AD systems relying on the decision-making
    capabilities of Vision-LLMs. Different from the few existing works developing
    general datasets of typographic attacks, this paper focuses on realistic traffic
    scenarios where these attacks can be deployed, on their potential effects on the
    decision-making autonomy, and on the practical ways in which these attacks can
    be physically presented. To achieve the above goals, we first propose a dataset-agnostic
    framework for automatically generating false answers that can mislead Vision-LLMs’
    reasoning. Then, we present a linguistic augmentation scheme that facilitates
    attacks at image-level and region-level reasoning, and we extend it with attack
    patterns against multiple reasoning tasks simultaneously. Based on these, we conduct
    a study on how these attacks can be realized in physical traffic scenarios. Through
    our empirical study, we evaluate the effectiveness, transferability, and realizability
    of typographic attacks in traffic scenes. Our findings demonstrate particular
    harmfulness of the typographic attacks against existing Vision-LLMs (*e.g*., LLaVA,
    Qwen-VL, VILA, and Imp), thereby raising community awareness of vulnerabilities
    when incorporating such models into AD systems. We will release our source code
    upon acceptance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉大语言模型（Vision-LLMs）由于其先进的视觉语言推理能力，正越来越多地被集成到自动驾驶（AD）系统中，目标是感知、预测、规划和控制机制。然而，Vision-LLMs
    已显示出对各种类型对抗攻击的脆弱性，这会危及其可靠性和安全性。为了进一步探索 AD 系统中的风险和实际威胁的可迁移性，我们建议利用排版攻击来针对依赖 Vision-LLMs
    决策能力的 AD 系统。与现有少数开发一般排版攻击数据集的工作不同，本论文专注于这些攻击可以部署的现实交通场景、其对决策自主性的潜在影响，以及这些攻击可以以实际方式呈现的方式。为了实现上述目标，我们首先提出了一个数据集无关的框架，用于自动生成可以误导
    Vision-LLMs 推理的错误答案。然后，我们提出了一种语言增强方案，便于在图像级和区域级推理中进行攻击，并扩展了同时针对多个推理任务的攻击模式。在此基础上，我们对这些攻击在物理交通场景中如何实现进行了研究。通过我们的实证研究，我们评估了排版攻击在交通场景中的有效性、可迁移性和可实现性。我们的发现显示了排版攻击对现有
    Vision-LLMs 的特别危害（*例如*，LLaVA、Qwen-VL、VILA 和 Imp），从而提高了社区对将这些模型集成到 AD 系统中时的脆弱性的认识。我们将在接受后发布我们的源代码。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Vision-Language Large Models (Vision-LLMs) have seen rapid development over
    the recent years [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)], and their incorporation
    into autonomous driving (AD) systems have been seriously considered by both industry
    and academia [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]. The integration of Vision-LLMs into AD systems
    showcases their ability to convey explicit reasoning steps to road users on the
    fly and satisfy the need for textual justifications of traffic scenarios regarding
    perception, prediction, planning, and control, particularly in safety-critical
    circumstances in the physical world. The core strength of Vision-LLMs lies in
    their auto-regressive capabilities through large-scale pretraining with visual-language
    alignment [[1](#bib.bib1)], making them even able to perform zero-shot optical
    character recognition, grounded reasoning, visual-question answering, visual-language
    reasoning, *etc*. Nevertheless, despite their impressive capabilities, Vision-LLMs
    are unfortunately not impervious against adversarial attacks that can misdirect
    the reasoning processes [[10](#bib.bib10)]. Any successful attack strategies have
    the potential to pose critical problems when deploying Vision-LLMs in AD systems,
    especially those that may even bypass the models’ black-box characteristics. As
    a step towards their reliable adoption in AD, studying the transferability of
    adversarial attacks is crucial to raising awareness of practical threats against
    deployed Vision-LLMs, and to efforts in building appropriate defense strategies
    for them.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，视觉-语言大型模型（Vision-LLMs）得到了迅速发展 [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)]，并且其在自动驾驶（AD）系统中的应用已被业界和学术界认真考虑 [[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9)]。Vision-LLMs
    在 AD 系统中的集成展示了它们能够即时向道路使用者传达明确的推理步骤，并满足对交通场景（包括感知、预测、规划和控制）的文本解释需求，特别是在物理世界中的安全关键情况。Vision-LLMs
    的核心优势在于其通过大规模预训练与视觉语言对齐的自回归能力 [[1](#bib.bib1)]，使其甚至能够执行零-shot 光学字符识别、基础推理、视觉问答、视觉语言推理，*等*。然而，尽管其能力令人印象深刻，但
    Vision-LLMs 不幸地并非对对抗攻击无懈可击，这些攻击可能会误导推理过程 [[10](#bib.bib10)]。任何成功的攻击策略都有可能在部署 Vision-LLMs
    时带来严重问题，尤其是那些可能绕过模型黑箱特性的攻击。为了在 AD 中可靠采用 Vision-LLMs，研究对抗攻击的可转移性对于提高对已部署 Vision-LLMs
    实际威胁的认识和制定适当防御策略至关重要。
- en: In this work, we revisit the shared auto-regressive characteristic of different
    Vision-LLMs and intuitively turn that strength into a weakness by leveraging typographic
    forms of adversarial attacks, also known as typographic attacks. Typographic attacks
    were first studied in the context of the well-known Contrastive Language-Image
    Pre-training (CLIP) model [[11](#bib.bib11), [12](#bib.bib12)]. Early works in
    this area focused on developing a general typographic attack dataset targeting
    multiple-choice answering (such as object recognition, visual attribute detection,
    and commonsense answering) and enumeration [[13](#bib.bib13)]. Researchers also
    explored multiple-choice self-generating attacks against zero-shot classification [[14](#bib.bib14)],
    and proposed several defense mechanisms, including keyword-training [[15](#bib.bib15)]
    and prompting the model for detailed reasoning [[16](#bib.bib16)]. Despite these
    initial efforts, the methodologies have neither seen a comprehensive attack framework
    nor been explicitly designed to investigate the impact of typographic attacks
    on safety-critical systems, particularly those in AD scenarios.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们重新审视了不同 Vision-LLMs 的共享自回归特性，并通过利用排版形式的对抗攻击（也称为排版攻击）直观地将这一优势转化为劣势。排版攻击最早在著名的对比语言-图像预训练（CLIP）模型中进行了研究 [[11](#bib.bib11),
    [12](#bib.bib12)]。早期的工作集中在开发针对多项选择回答（如物体识别、视觉属性检测和常识回答）和枚举的通用排版攻击数据集 [[13](#bib.bib13)]。研究人员还探讨了针对零-shot
    分类的多项选择自生成攻击 [[14](#bib.bib14)]，并提出了几种防御机制，包括关键词训练 [[15](#bib.bib15)] 和促使模型进行详细推理 [[16](#bib.bib16)]。尽管有这些初步的努力，但这些方法既没有形成全面的攻击框架，也没有专门设计用于调查排版攻击对安全关键系统的影响，特别是那些在
    AD 场景中的系统。
- en: 'Our work aims to fill this research gap by studying typographic attacks from
    the perspective of AD systems that incorporate Vision-LLMs. In summary, our scientific
    contributions are threefold:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作旨在通过研究集成了 Vision-LLMs 的 AD 系统中的排版攻击来填补这一研究空白。总的来说，我们的科学贡献有三方面：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dataset-Independent Framework: we introduce a dataset-independent framework
    designed to automatically generate misleading answers that can disrupt the reasoning
    processes of Vision-Large Language Models (Vision-LLMs).'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集独立框架：我们介绍了一种数据集独立的框架，旨在自动生成误导性答案，这些答案可以干扰视觉大语言模型（Vision-LLMs）的推理过程。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Linguistic Augmentation Schemes: we develop a linguistic augmentation scheme
    aimed at facilitating stronger typographic attacks on Vision-LLMs. This scheme
    targets reasoning at both the image and region levels and is expandable to multiple
    reasoning tasks simultaneously.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语言增强方案：我们开发了一种语言增强方案，旨在促进对视觉大语言模型（Vision-LLMs）的更强的类型攻击。该方案针对图像和区域级别的推理，并可扩展到多个推理任务同时进行。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Empirical Study in Semi-Realistic Scenarios: we conduct a study to explore
    the possible implementations of these attacks in real-world traffic scenarios.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 半现实场景中的实证研究：我们进行了一项研究，探索这些攻击在现实世界交通场景中的可能实现。
- en: Through our empirical study of typographic attacks in traffic scenes, we hope
    to raise community awareness of critical typographic vulnerabilities when incorporating
    such models into AD systems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们对交通场景中类型攻击的实证研究，我们希望提高社区对将这些模型纳入自动驾驶系统时可能存在的关键类型漏洞的关注。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Vision-LLMs
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 视觉大语言模型（Vision-LLMs）
- en: Having demonstrated the proficiency of Large Language Models (LLMs) in reasoning
    across various natural language benchmarks, researchers have extended LLMs with
    visual encoders to support multimodal understanding. This integration has given
    rise to various forms of Vision-LLMs, capable of reasoning based on the composition
    of visual and language inputs.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示了大语言模型（LLMs）在各种自然语言基准测试中的推理能力后，研究人员扩展了LLMs，添加了视觉编码器以支持多模态理解。这种集成催生了多种形式的视觉大语言模型（Vision-LLMs），能够基于视觉和语言输入的组合进行推理。
- en: Vision-LLMs Pre-training. The interconnection between LLMs and pre-trained vision
    models involves the individual pre-training of unimodal encoders on their respective
    domains, followed by large-scale vision-language joint training [[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [2](#bib.bib2), [1](#bib.bib1)].
    Through an interleaved visual language corpus (*e.g*., MMC4 [[21](#bib.bib21)]
    and M3W [[22](#bib.bib22)]), auto-regressive models learn to process images by
    converting them into visual tokens, combine these with textual tokens, and input
    them into LLMs. Visual inputs are treated as a foreign language, enhancing traditional
    text-only LLMs by enabling visual understanding while retaining their language
    capabilities. Hence, a straightforward pre-training strategy may not be designed
    to handle cases where input text is significantly more aligned with visual texts
    in an image than with the visual context of that image.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉大语言模型（Vision-LLMs）预训练。LLMs与预训练视觉模型之间的互联涉及对各自领域的单模态编码器的单独预训练，随后进行大规模的视觉-语言联合训练[[17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [2](#bib.bib2), [1](#bib.bib1)]。通过交错的视觉语言语料库（*例如*，MMC4[[21](#bib.bib21)]和M3W[[22](#bib.bib22)]），自回归模型学习将图像转换为视觉令牌，结合这些令牌与文本令牌，并将其输入到LLMs中。视觉输入被视为一种外语，增强了传统的文本-only
    LLMs，通过实现视觉理解，同时保留其语言能力。因此，简单的预训练策略可能无法处理输入文本与图像中的视觉文本显著对齐的情况，而非与该图像的视觉背景对齐。
- en: Vision-LLMs in AD Systems. Vision-LLMs have proven useful for perception, planning,
    reasoning, and control in autonomous driving (AD) systems [[6](#bib.bib6), [7](#bib.bib7),
    [9](#bib.bib9), [5](#bib.bib5)]. For example, existing works have quantitatively
    benchmarked the linguistic capabilities of Vision-LLMs in terms of their trustworthiness
    in explaining the decision-making processes of AD [[7](#bib.bib7)]. Others have
    explored the use of Vision-LLMs for vehicular maneuvering [[8](#bib.bib8), [5](#bib.bib5)],
    and [[6](#bib.bib6)] even validated an approach in controlled physical environments.
    Because AD systems involve safety-critical situations, comprehensive analyses
    of their vulnerabilities are crucial for reliable deployment and inference. However,
    proposed adoptions of Vision-LLMs into AD have been straightforward, which means
    existing issues (*e.g*., vulnerabilities against typographic attacks) in such
    models are likely present without proper countermeasures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: AD 系统中的 Vision-LLMs。Vision-LLMs 在自动驾驶（AD）系统中的感知、规划、推理和控制方面已被证明非常有用 [[6](#bib.bib6),
    [7](#bib.bib7), [9](#bib.bib9), [5](#bib.bib5)]。例如，现有研究已经在量化评估 Vision-LLMs 的语言能力方面进行了基准测试，以评估其解释
    AD 决策过程的可信度 [[7](#bib.bib7)]。其他研究则探讨了 Vision-LLMs 在车辆操控中的应用 [[8](#bib.bib8), [5](#bib.bib5)]，[[6](#bib.bib6)]
    甚至在受控的物理环境中验证了一种方法。由于 AD 系统涉及安全关键的情况，因此全面分析其漏洞对于可靠的部署和推断至关重要。然而，提议将 Vision-LLMs
    应用于 AD 的方式较为直接，这意味着这些模型中可能存在的现有问题（*例如*，对排版攻击的漏洞）很可能在没有适当对策的情况下存在。
- en: 2.2 Transferable Adversarial Attacks
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 可迁移的对抗攻击
- en: Adversarial attacks are most harmful when they can be developed in a closed
    setting with public frameworks yet can still be realized to attack unseen, closed-source
    models. The literature on these transferable attacks popularly spans across gradient-based
    strategies. Against Vision-LLMs, our research focuses on exploring the transferability
    of typographic attacks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击在能够在封闭环境中使用公共框架开发且仍能攻击未见过的闭源模型时最具破坏性。关于这些可迁移攻击的文献通常涉及基于梯度的策略。针对 Vision-LLMs，我们的研究重点是探索排版攻击的可迁移性。
- en: Gradient-based Attacks. Since Szegedy *et al*. introduced the concept of adversarial
    examples, gradient-based methods have become the cornerstone of adversarial attacks
    [[23](#bib.bib23), [24](#bib.bib24)]. Goodfellow *et al*. proposed the Fast Gradient
    Sign Method (FGSM [[25](#bib.bib25)]) to generate adversarial examples using a
    single gradient step, perturbing the model’s input before backpropagation. Kurakin
    *et al*. later improved FGSM with an iterative optimization method, resulting
    in Iterative-FGSM (I-FGSM) [[26](#bib.bib26)]. Projected Gradient Descent (PGD
    [[27](#bib.bib27)]) further enhances I-FGSM by incorporating random noise initialization,
    leading to better attack performance. Gradient-based transfer attack methods typically
    use a known surrogate model, leveraging its parameters and gradients to generate
    adversarial examples, which are then used to attack a black-box model. These methods
    often rely on multi-step iterative optimization techniques like PGD and employ
    various data augmentation strategies to enhance transferability [[28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31), [32](#bib.bib32)]. However,
    gradient-based methods face limitations in adversarial transferability due to
    the disparity between the surrogate and target models, and the tendency of adversarial
    examples to overfit the surrogate model [[33](#bib.bib33), [34](#bib.bib34)].
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的攻击。自从 Szegedy *等人* 引入对抗样本的概念以来，基于梯度的方法已经成为对抗攻击的基石 [[23](#bib.bib23), [24](#bib.bib24)]。Goodfellow
    *等人* 提出了快速梯度符号方法（FGSM [[25](#bib.bib25)]），通过一个梯度步骤生成对抗样本，在反向传播之前扰动模型的输入。Kurakin
    *等人* 随后改进了 FGSM，采用迭代优化方法，得到了迭代 FGSM（I-FGSM） [[26](#bib.bib26)]。投影梯度下降（PGD [[27](#bib.bib27)]）进一步通过引入随机噪声初始化来增强
    I-FGSM，从而提高攻击性能。基于梯度的迁移攻击方法通常使用已知的替代模型，利用其参数和梯度生成对抗样本，然后用这些样本攻击黑箱模型。这些方法通常依赖于像
    PGD 这样的多步迭代优化技术，并采用各种数据增强策略来提高迁移性 [[28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30),
    [31](#bib.bib31), [32](#bib.bib32)]。然而，基于梯度的方法在对抗迁移性方面存在局限性，因为替代模型和目标模型之间存在差异，并且对抗样本往往会过拟合替代模型
    [[33](#bib.bib33), [34](#bib.bib34)]。
- en: Typographic Attacks. The development of large-scale pretrained vision-language
    with CLIP [[11](#bib.bib11), [12](#bib.bib12)] introduced a form of typographic
    attacks that can impair its zero-shot performances. A concurrent work [[13](#bib.bib13)]
    has also shown that such typographic attacks can extend to language reasoning
    tasks of Vision-LLMs like multi-choice question-answering and image-level open-vocabulary
    recognition. Similarly, another work [[14](#bib.bib14)] has developed a benchmark
    by utilizing a Vision-LLM to recommend an attack against itself given an image,
    a question, and its answer on classification datasets. Several defense mechanisms
    [[15](#bib.bib15), [16](#bib.bib16)] have been suggested by prompting the Vision-LLM
    to perform step-by-step reasoning. Our research differs from existing works in
    studying autonomous typographic attacks across question-answering scenarios of
    recognition, action reasoning, and scene understanding, particularly against Vision-LLMs
    in AD systems. Our work also discusses how they can affect reasoning capabilities
    at the image level, region-level understanding, and even against multiple reasoning
    tasks. Furthermore, we also discuss how these attacks can be realized in the physical
    world, particularly against AD systems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 排版攻击。大规模预训练视觉-语言模型的开发通过CLIP [[11](#bib.bib11), [12](#bib.bib12)] 引入了一种排版攻击形式，这可能损害其零样本性能。一项同时进行的工作
    [[13](#bib.bib13)] 还表明，这种排版攻击可以扩展到视觉-LLM的语言推理任务，如多项选择问答和图像级开放词汇识别。类似地，另一项工作 [[14](#bib.bib14)]
    通过利用视觉-LLM建议对抗攻击，在分类数据集上给定图像、问题和答案开发了一个基准。一些防御机制 [[15](#bib.bib15), [16](#bib.bib16)]
    通过提示视觉-LLM进行逐步推理已被提出。我们的研究不同于现有工作，专注于研究在识别、动作推理和场景理解的问答场景中自主排版攻击，特别是针对AD系统中的视觉-LLM。我们的工作还讨论了这些攻击如何影响图像级别的推理能力、区域级理解，甚至对多个推理任务的影响。此外，我们还讨论了这些攻击如何在现实世界中实现，特别是针对AD系统。
- en: 3 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 前言
- en: 3.1 Revisiting Auto-Regressive Vision-LLMs
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 重新审视自回归视觉-LLM
- en: 'As a simplified formulation of auto-regressive Vision-LLMs, suppose we have
    a visual input , denoted as  as the Vision-LLM model function, whose goal is to
    predict the next token  at each timestep $t$ based on the previous tokens and
    the visual context:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为自回归视觉-LLM的简化形式，假设我们有一个视觉输入，记作，作为视觉-LLM模型函数，其目标是在每个时间步 $t$ 基于先前的令牌和视觉上下文预测下一个令牌：
- en: '|  |  |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (1) |'
- en: '|  |  | $\displaystyle=f(x_{1},\dots,x_{t-1},v_{1},\dots,v_{m}),$ |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=f(x_{1},\dots,x_{t-1},v_{1},\dots,v_{m}),$ |  |'
- en: 'where  visual tokens encoded by a visual encoder on  are converted into a probability
    distribution using the softmax function. Specifically,  in the vocabulary , generally
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，视觉令牌通过视觉编码器在 被编码，并通过softmax函数转换为概率分布。具体而言，在词汇表中，一般如下：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Then, the general language modeling loss for training the model can be based
    on cross-entropy loss. For a sequence of tokens $\mathbf{x}=\{x_{1},\dots,x_{n}\}$,
    the loss is given by:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，训练模型的一般语言建模损失可以基于交叉熵损失。对于一个令牌序列 $\mathbf{x}=\{x_{1},\dots,x_{n}\}$，损失如下给出：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where  or visual token . Vision-LLMs possess conversational capabilities at
    their core, so interleaving language data () during optimization is crucial for
    enabling visual understanding while retaining language reasoning [[1](#bib.bib1)].
    Regardless of $m$, the loss objective of vision-guided language modeling is essentially
    the same as auto-regressive language modeling [[35](#bib.bib35)]. Consequently,
    as part of the alignment process, these practices imply blurred boundaries between
    textual and visual feature tokens during training. They may also facilitate text-to-text
    alignment between raw texts and within-image texts at inference.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中或视觉令牌。视觉-LLM在其核心具有对话能力，因此在优化过程中交替使用语言数据（）对于在保留语言推理的同时启用视觉理解至关重要[[1](#bib.bib1)]。不论
    $m$ 为何，视觉引导的语言建模的损失目标本质上与自回归语言建模相同[[35](#bib.bib35)]。因此，作为对齐过程的一部分，这些做法暗示在训练过程中文本和视觉特征令牌之间的边界模糊。它们也可能在推理时促进原始文本与图像内文本之间的文本到文本对齐。
- en: 3.2 Typographic Attacks in Vision-LLMs-based AD Systems
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 视觉-LLM基础的排版攻击
- en: The integration of Vision-LLMs into end-to-end AD systems has brought promising
    results thus far [[9](#bib.bib9)], where Vision-LLMs can enhance user trust through
    explicit reasoning steps of the scene. On the one hand, language reasoning in
    AD systems can elevate their capabilities by utilizing the learned commonsense
    of LLMs, while being able to proficiently communicate to users. On the other hand,
    exposing Vision-LLMs to public traffic scenarios not only makes them more vulnerable
    to typographic attacks that misdirect the reasoning process but can also prove
    harmful if their results are connected with decision-making, judgment, and control
    processes.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将视觉-LLMs整合到端到端AD系统中目前已经带来了令人鼓舞的结果[[9](#bib.bib9)]，其中视觉-LLMs可以通过明确的场景推理步骤来增强用户信任。一方面，通过利用LLMs学到的常识，AD系统中的语言推理可以提升其能力，同时能够有效地与用户沟通。另一方面，将视觉-LLMs暴露于公共交通场景不仅使其更易受到误导推理过程的排版攻击，而且如果其结果与决策、判断和控制过程相关联，也可能造成危害。
- en: 'Table 1: Transferability and stealthiness of attacks.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：攻击的可迁移性和隐蔽性。
- en: '| Method |  SSIM | Lingo-Judge | BERTScore$\downarrow$ |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  SSIM | Lingo-Judge | BERTScore$\downarrow$ |'
- en: '| gradient-based, CLIP (16/255) [[11](#bib.bib11)] | 0.6425 | 0.3670 | 0.3126
    | 0.4456 | 0.6766 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 基于梯度的，CLIP (16/255) [[11](#bib.bib11)] | 0.6425 | 0.3670 | 0.3126 | 0.4456
    | 0.6766 |'
- en: '| gradient-based, ALBEF (16/255) [[36](#bib.bib36)] | 0.6883 | 0.3493 | 0.3139
    | 0.4438 | 0.6754 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 基于梯度的，ALBEF (16/255) [[36](#bib.bib36)] | 0.6883 | 0.3493 | 0.3139 | 0.4438
    | 0.6754 |'
- en: '| our typographic attack | 0.9506 | 0.0700 | 0.0700 | 0.5563 | 0.7327 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 我们的排版攻击 | 0.9506 | 0.0700 | 0.0700 | 0.5563 | 0.7327 |'
- en: 'Unlike the less transferable gradient-based attacks, typographic attacks are
    more transferable across Vision-LLMs by exploiting the inherent text-to-text alignment
    between raw texts and within-image texts to introduce misleading textual patterns
    in images, and influence the reasoning of a Vision-LLM, *i.e*., dominating over
    visual-text alignment. In digital form, the attack is formulated as a function
    . Then, Eq. [1](#S3.E1 "In 3.1 Revisiting Auto-Regressive Vision-LLMs ‣ 3 Preliminaries
    ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with
    Typography") can be rewritten as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与较少可迁移的基于梯度的攻击不同，排版攻击通过利用原始文本与图像内文本之间的固有文本对文本对齐，使其在视觉-LLMs中更具可迁移性，从而在图像中引入误导性文本模式，影响视觉-LLM的推理，即主导视觉文本对齐。数字形式下，攻击被表述为一个函数。然后，公式[1](#S3.E1
    "在3.1回顾自回归视觉-LLMs ‣ 3 基础 ‣ 针对视觉-LLMs在自动驾驶中的可迁移攻击与排版")可以重写为：
- en: '|  |  |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (4) |'
- en: '|  |  | $\displaystyle=f(x_{1},\dots,x_{t-1},\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\hat{v}_{1},\dots,\hat{v}_{m}\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}),$
    |  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=f(x_{1},\dots,x_{t-1},\color[rgb]{0,0,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,1}\hat{v}_{1},\dots,\hat{v}_{m}\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}),$
    |  |'
- en: where  visual tokens under the influenced image , ❷ yet guide the reasoning
    process towards an incorrect answer. By exploiting the fundamental properties
    of many Vision-LLMs in language modeling to construct adversarial patterns, ❸
    typographic attacks $\tau(\cdot)$ aim to be transferable across various pre-trained
    Vision-LLMs by directly influencing the visual information with texts. Our study
    is geared towards typographic attacks in AD scenarios to thoroughly understand
    the issues and raise awareness.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在受影响的图像下，视觉标记❷会引导推理过程走向错误的答案。通过利用许多视觉-LLMs在语言建模中的基本特性来构建对抗模式，❸ 排版攻击$\tau(\cdot)$旨在通过直接影响视觉信息与文本的方式，在各种预训练的视觉-LLMs之间实现可迁移性。我们的研究专注于AD场景中的排版攻击，以深入了解问题并提高警觉。
- en: 4 Methodology
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: Figure [1](#S4.F1 "Figure 1 ‣ 4 Methodology ‣ Towards Transferable Attacks Against
    Vision-LLMs in Autonomous Driving with Typography") shows an overview of our typographic
    attack pipeline, which goes from prompt engineering to attack annotation, particularly
    through Attack Auto-Generation, Attack Augmentation, and Attack Realization steps.
    We describe the details of each step in the following subsections.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图[1](#S4.F1 "图1 ‣ 4 方法论 ‣ 针对视觉-LLMs在自动驾驶中的可迁移攻击与排版")展示了我们的排版攻击流程概述，从提示工程到攻击注释，特别是通过攻击自动生成、攻击增强和攻击实现步骤。我们在以下小节中描述每个步骤的详细信息。
- en: '![Refer to caption](img/85f8e8ed55b14bb2af80d5c79641b784.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/85f8e8ed55b14bb2af80d5c79641b784.png)'
- en: 'Figure 1: Our proposed pipeline is from attack generation via directives to
    augmentation by commands and conjunctions to positioning the attacks and finally
    influencing inference.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们提出的流程是从通过指令生成攻击，经过命令和连接词的增强，到定位攻击，最终影响推理。
- en: 4.1 Auto-Generation of Typographic Attack
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 自动生成排版攻击
- en: In this subsection, to handle the lack of both autonomy and diversity in typographic
    attacks, we propose to employ the support of an LLM and prompt engineering, denoted
    by a model function , , the adversarial text can be naively generated as $\hat{\mathbf{a}}$,
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，为了处理排版攻击中缺乏自主性和多样性的问题，我们建议利用LLM和提示工程的支持，表示为一个模型函数，敌对文本可以被简单生成为$\hat{\mathbf{a}}$，
- en: '|  | $\displaystyle\hat{\mathbf{a}}=l(\mathbf{q},\mathbf{a}).$ |  | (5) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{\mathbf{a}}=l(\mathbf{q},\mathbf{a}).$ |  | (5) |'
- en: In order to generate useful misdirection, the adversarial patterns must align
    with an existing question while guiding LLM toward an incorrect answer. We can
    achieve this through a concept called directive, which refers to configuring the
    goal for an LLM, *e.g*., ChatGPT, to impose specific constraints while encouraging
    diverse behaviors. In our context, we direct the LLM to generate , under the constraint
    of the given question $\mathbf{q}$. Therefore, we can initialize directives to
    the LLM using the following prompts in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Auto-Generation
    of Typographic Attack ‣ 4 Methodology ‣ Towards Transferable Attacks Against Vision-LLMs
    in Autonomous Driving with Typography"),
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成有用的误导，敌对模式必须与现有问题对齐，同时引导LLM朝着错误的答案前进。我们可以通过一个叫做指令的概念来实现这一点，它指的是为LLM配置目标，例如，ChatGPT，以施加特定约束，同时鼓励多样化行为。在我们的上下文中，我们引导LLM在给定问题$\mathbf{q}$的约束下生成。因此，我们可以使用图 [2](#S4.F2
    "图 2 ‣ 4.1 自动生成排版攻击 ‣ 4 方法论 ‣ 针对视觉大语言模型在自动驾驶中的排版可转移攻击")中的以下提示来初始化指令。
- en: '![Refer to caption](img/7eacadce758713ff3889ddae8151a5ed.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7eacadce758713ff3889ddae8151a5ed.png)'
- en: 'Figure 2: Context directive for constraints of attack generation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：攻击生成约束的上下文指令。
- en: When generating attacks, we would impose additional constraints depending on
    the question type. In our context, we focus on tasks of ❶ scene reasoning (*e.g*.,
    counting), ❷ scene object reasoning (*e.g*., recognition), and ❸ action reasoning
    (*e.g*., action recommendation), as follows in Fig. [3](#S4.F3 "Figure 3 ‣ 4.1
    Auto-Generation of Typographic Attack ‣ 4 Methodology ‣ Towards Transferable Attacks
    Against Vision-LLMs in Autonomous Driving with Typography"),
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成攻击时，我们会根据问题类型施加额外的约束。在我们的上下文中，我们专注于任务 ❶ 场景推理（*例如*，计数），❷ 场景对象推理（*例如*，识别），和
    ❸ 行动推理（*例如*，行动推荐），如图 [3](#S4.F3 "图 3 ‣ 4.1 自动生成排版攻击 ‣ 4 方法论 ‣ 针对视觉大语言模型在自动驾驶中的排版可转移攻击")所示，
- en: '![Refer to caption](img/a4d99b38bf384d8c8597f4304d663bb0.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a4d99b38bf384d8c8597f4304d663bb0.png)'
- en: 'Figure 3: Template directive for attack generation, and an example.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：攻击生成的模板指令和示例。
- en: The directives encourage the LLM to generate attacks that influence a Vision-LLM’s
    reasoning step through text-to-text alignment and automatically produce typographic
    patterns as benchmark attacks. Clearly, the aforementioned typographic attack
    only works for single-task scenarios, *i.e*., a single pair of question and answer.
    To investigate multi-task vulnerabilities with respect to multiple pairs, we can
    also generalize the formulation to , to obtain the adversarial text .
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指令鼓励LLM生成通过文本对文本对齐影响视觉LLM推理步骤的攻击，并自动生成作为基准攻击的排版模式。显然，前述的排版攻击仅适用于单任务场景，*即*，单对问题和答案。为了研究多任务的漏洞，我们还可以将公式推广到，以获得敌对文本。
- en: 4.2 Augmentations of Typographic Attack
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 排版攻击的增强
- en: 'Inspired by the success of instruction-prompting methodologies [[37](#bib.bib37),
    [38](#bib.bib38)], the greedy reasoning in LLMs [[39](#bib.bib39)], and to further
    exploit the ambiguity between textual and visual tokens in Vision-LLMs, we propose
    to augment the typographic attacks prompts within images by explicitly providing
    instruction keywords that emphasize text-to-text alignment over that of visual-language
    tokens. Our approach realizes the concept in the form of instructional directives:
    ❶ command directives for emphasizing a false answer and ❷ conjunction directives
    to additionally include attack clauses. In particular, we have developed,'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 受到指令提示方法[[37](#bib.bib37)、[38](#bib.bib38)]的成功、LLMs 的贪婪推理[[39](#bib.bib39)]的启发，并进一步利用
    Vision-LLMs 中文本和视觉标记之间的模糊性，我们建议通过明确提供强调文本对文本对齐而非视觉语言标记的指令关键词来增强图像中的排版攻击提示。我们的方法以指令性指令的形式实现该概念：❶
    命令指令用于强调虚假答案，❷ 连词指令用于额外包含攻击条款。特别是，我们已经开发了，
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Command Directive. By embedding commands with the attacks, we aim to prompt
    the Vision-LLMs into greedily producing erroneous answers. Our work investigates
    the "ANSWER:" directive as a prefix before the first attack prompt.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命令指令。通过将命令嵌入攻击中，我们旨在促使 Vision-LLMs 贪婪地产生错误的回答。我们的工作探讨了在第一个攻击提示之前作为前缀的“ANSWER:”指令。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conjunction Directive. Conjunctions, connectors (or the lack thereof) act to
    link together separate attack concepts that make the overall text appear more
    coherent, thereby increasing the likelihood of multi-task success. In our work,
    we investigate these directives as "AND," "OR," "WITH," or simply empty spaces
    as prefixes between attack prompts.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连词指令。连词、连接词（或其缺乏）用于将独立的攻击概念连接在一起，使整体文本显得更加连贯，从而提高多任务成功的可能性。在我们的工作中，我们将这些指令研究为“AND”、“OR”、“WITH”或简单的空格，作为攻击提示之间的前缀。
- en: While other forms of directives can also be useful for enhancing the attack
    success rate, we focus on investigating basic directives related to typographic
    attacks in this work.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然其他形式的指令也可以有助于提高攻击成功率，但我们在本工作中集中研究了与排版攻击相关的基本指令。
- en: 4.3 Realizations of Typographic Attacks
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 排版攻击的实现
- en: Digitally, typographic attacks are about embedding texts within images to fool
    the capabilities of Vision-LLMs, which might involve simply putting texts into
    the images. Physically, typographic attacks can incorporate real elements (*e.g*.,
    stickers, paints, and drawings) into environments/entities observable by AI systems,
    with AD systems being prime examples. This would include the placement of texts
    with unusual fonts or colors on streets, objects, vehicles, or clothing to mislead
    AD systems in reasoning, planning, and control. We investigate Vision-LLMs when
    incorporated into AD systems, as they are likely under the most risk against typographic
    attacks. We categorize the placement locations as being identified with backgrounds
    and foregrounds in traffic scenes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从数字角度来看，排版攻击涉及在图像中嵌入文本以欺骗 Vision-LLMs 的能力，这可能仅仅涉及将文本放入图像中。从物理角度来看，排版攻击可以将真实元素（*例如*，贴纸、涂料和绘画）融入
    AI 系统可观察的环境/实体中，AD 系统就是主要的例子。这包括在街道、物体、车辆或衣物上放置具有不寻常字体或颜色的文本，以误导 AD 系统在推理、规划和控制方面的判断。我们研究了
    Vision-LLMs 在 AD 系统中的应用，因为它们最容易受到排版攻击的风险。我们将放置位置分类为交通场景中的背景和前景。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Backgrounds, which refer to elements in the environment that are static and
    pervasive in a traffic scene (*e.g*., streets, buildings, and bus stops). The
    background components present predefined locations for introducing deceptive typographic
    elements of various sizes.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 背景，指的是在交通场景中静态且普遍存在的环境元素（*例如*，街道、建筑物和公交车站）。背景组件为引入各种大小的欺骗性排版元素提供了预定义的位置。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Foregrounds, which refer to dynamic elements and directly interact with the
    perception of AD systems (*e.g*., vehicles, cyclists, and pedestrians). The foreground
    components present dynamic and variable locations for typographic attacks of various
    sizes.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前景，指的是动态元素，直接与 AD 系统的感知进行交互（*例如*，车辆、骑自行车的人和行人）。前景组件为各种大小的排版攻击提供了动态和可变的位置。
- en: In our work, foreground placements are supported by an open-vocabulary object
    detector [[40](#bib.bib40)] to flexibly extract box locations of specific targets.
    Let  be its augmented version, either on background or foreground, the function  or
    ’s cropped box coordinates $x_{min},y_{min},x_{max},y_{max}$.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，前景位置由一个开放词汇的对象检测器 [[40](#bib.bib40)] 支持，以灵活提取特定目标的框位置。设为其增强版本，无论是在背景还是前景，函数或
    ’s 裁剪框坐标 $x_{min},y_{min},x_{max},y_{max}$。
- en: Depending on the attacked task, we observe that different text placements and
    observed sizes would render some attacks more effective while some others are
    negligible. Our research illuminates that background-placement attacks are quite
    effective against scene reasoning and action reasoning but not as effective against
    scene object reasoning unless foreground placements are also included.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 根据攻击的任务，我们观察到不同的文本位置和观察到的尺寸会使一些攻击更有效，而另一些则微不足道。我们的研究揭示了背景位置攻击在场景推理和动作推理中非常有效，但在场景物体推理中效果不如前景位置攻击，除非同时包括前景位置。
- en: 5 Experiments
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: '![Refer to caption](img/805a8c8b944db9ebf7979e2405faf471.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/805a8c8b944db9ebf7979e2405faf471.png)'
- en: 'Figure 4: Example attacks against Imp and GPT4 on the dataset by CVPRW’24.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：对 CVPRW’24 数据集中 Imp 和 GPT4 的攻击示例。
- en: 5.1 Experimental Setup
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: We perform experiments with Vision-LLMs on VQA datasets for AD, such as LingoQA [[7](#bib.bib7)]
    and the dataset of CVPRW’2024 Challenge ¹¹1https://cvpr24-advml.github.io by CARLA
    simulator. We have used LLaVa [[2](#bib.bib2)] to output the attack prompts for
    LingoQA and the CVPRW’2024 dataset, and manually for some cases of the latter.
    Regarding LingoQA, we tested 1000 QAs in real traffic scenarios in tasks, such
    as scene reasoning and action reasoning. Regarding the CVPRW’2024 Challenge dataset,
    we tested more than 300 QAs on 100 images, each with at least three questions
    related to scene reasoning (*e.g*., target counting) and scene object reasoning
    of 5 classes (cars, persons, motorcycles, traffic lights and road signals). Our
    evaluation metrics are based on exact matches, Lingo-Judge Accuracy [[7](#bib.bib7)],
    and BLEURT [[41](#bib.bib41)], BERTScore [[42](#bib.bib42)] against non-attacked
    answers, with SSIM (Structural Similarity Index) to quantify the similarity between
    original and attacked images. In terms of models, we qualitatively and/or quantitatively
    tested with LLaVa [[2](#bib.bib2)], VILA [[1](#bib.bib1)], Qwen-VL [[17](#bib.bib17)],
    and Imp [[18](#bib.bib18)]. The models were run on an NVIDIA A40 GPU with approximately
    45GiB of memory.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 VQA 数据集上使用 Vision-LLMs 进行 AD 实验，例如 LingoQA [[7](#bib.bib7)] 和 CVPRW’2024
    挑战数据集 ¹¹1https://cvpr24-advml.github.io，由 CARLA 模拟器生成。我们使用 LLaVa [[2](#bib.bib2)]
    输出 LingoQA 和 CVPRW’2024 数据集的攻击提示，并且对后者的某些案例进行了手动操作。关于 LingoQA，我们在任务中测试了 1000 个
    QA，包括场景推理和动作推理等任务。关于 CVPRW’2024 挑战数据集，我们在 100 张图像上测试了超过 300 个 QA，每张图像至少有三个与场景推理（*例如*，目标计数）和
    5 个类别的场景物体推理（汽车、人员、摩托车、交通灯和道路标志）相关的问题。我们的评估指标基于精确匹配、Lingo-Judge 准确度 [[7](#bib.bib7)]、BLEURT [[41](#bib.bib41)]
    和 BERTScore [[42](#bib.bib42)] 对抗非攻击答案，并使用 SSIM（结构相似性指数）量化原始图像与攻击图像之间的相似性。在模型方面，我们通过定性和/或定量测试
    LLaVa [[2](#bib.bib2)]、VILA [[1](#bib.bib1)]、Qwen-VL [[17](#bib.bib17)] 和 Imp [[18](#bib.bib18)]。模型在带有大约
    45GiB 内存的 NVIDIA A40 GPU 上运行。
- en: 'Table 2: Ablation study of our automatic attack strategy effectiveness. Lower
    scores mean more effective attacks, with (auto) denoting automatic attacks.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们自动攻击策略效果的消融研究。分数越低表示攻击越有效，其中（auto）表示自动攻击。
- en: '|  | Attack | LingoQA | CVPRW’24 (counting only) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击 | LingoQA | CVPRW’24（仅计数） |'
- en: '| Type | Exact | BLEURT | Exact | BLEURT |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 精确 | BLEURT | 精确 | BLEURT |'
- en: '| Qwen-VL | auto | 0.3191 | 0.3330 | 0.5460 | 0.6861 | 0.1950 | 0.1950 | 0.6267
    | 0.7936 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL | auto | 0.3191 | 0.3330 | 0.5460 | 0.6861 | 0.1950 | 0.1950 | 0.6267
    | 0.7936 |'
- en: '| Imp | auto | 0.5244 | 0.4755 | 0.6398 | 0.7790 | 0.1900 | 0.1700 | 0.6194
    | 0.7983 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Imp | auto | 0.5244 | 0.4755 | 0.6398 | 0.7790 | 0.1900 | 0.1700 | 0.6194
    | 0.7983 |'
- en: '| VILA | auto | 0.4744 | 0.5415 | 0.6462 | 0.7717 | 0.1700 | 0.1750 | 0.7052
    | 0.8362 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| VILA | auto | 0.4744 | 0.5415 | 0.6462 | 0.7717 | 0.1700 | 0.1750 | 0.7052
    | 0.8362 |'
- en: '| LLaVa | auto | 0.5053 | 0.4021 | 0.5771 | 0.7435 | 0.3450 | 0.3450 | 0.7524
    | 0.8781 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LLaVa | auto | 0.5053 | 0.4021 | 0.5771 | 0.7435 | 0.3450 | 0.3450 | 0.7524
    | 0.8781 |'
- en: 'Table 3: Ablation of attack effectiveness on CVPRW’24 dataset’s counting subtask.
    Lower scores mean more effective attacks, with (single) denoting single question
    attack, (composed) for multi-task attack, and (+a) means augmented with directives.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：对 CVPRW’24 数据集计数子任务攻击效果的消融研究。分数越低表示攻击越有效，其中（single）表示单一问题攻击，（composed）表示多任务攻击，（+a）表示带有指令的增强。
- en: '|  | Attack Type | Exact | BLEURT |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击类型 | 精确 | BLEURT |'
- en: '| Qwen-VL | single | 0.4000 | 0.3300 | 0.6890 | 0.8508 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL | 单一 | 0.4000 | 0.3300 | 0.6890 | 0.8508 |'
- en: '| single+a | 0.3950 | 0.3350 | 0.6786 | 0.8354 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 单一+a | 0.3950 | 0.3350 | 0.6786 | 0.8354 |'
- en: '| composed | 0.0400 | 0.0400 | 0.5931 | 0.7998 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.0400 | 0.0400 | 0.5931 | 0.7998 |'
- en: '| composed+a | 0.0700 | 0.0700 | 0.5563 | 0.7327 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0700 | 0.0700 | 0.5563 | 0.7327 |'
- en: '| Imp | single | 0.4850 | 0.3500 | 0.7032 | 0.8490 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Imp | 单一 | 0.4850 | 0.3500 | 0.7032 | 0.8490 |'
- en: '| single+a | 0.4800 | 0.3600 | 0.6870 | 0.8402 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 单一+a | 0.4800 | 0.3600 | 0.6870 | 0.8402 |'
- en: '| composed | 0.0360 | 0.0300 | 0.5733 | 0.7954 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.0360 | 0.0300 | 0.5733 | 0.7954 |'
- en: '| composed+a | 0.0850 | 0.0800 | 0.5919 | 0.8047 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0850 | 0.0800 | 0.5919 | 0.8047 |'
- en: '| VILA | single | 0.4650 | 0.4300 | 0.7642 | 0.8796 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| VILA | 单一 | 0.4650 | 0.4300 | 0.7642 | 0.8796 |'
- en: '| single+a | 0.4800 | 0.4600 | 0.7666 | 0.8871 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 单一+a | 0.4800 | 0.4600 | 0.7666 | 0.8871 |'
- en: '| composed | 0.0300 | 0.0300 | 0.6474 | 0.8121 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.0300 | 0.0300 | 0.6474 | 0.8121 |'
- en: '| composed+a | 0.0950 | 0.0950 | 0.6633 | 0.8221 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0950 | 0.0950 | 0.6633 | 0.8221 |'
- en: '| LLaVa | single | 0.3900 | 0.3900 | 0.7641 | 0.8893 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LLaVa | 单一 | 0.3900 | 0.3900 | 0.7641 | 0.8893 |'
- en: '| single+a | 0.4100 | 0.4100 | 0.7714 | 0.8929 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 单一+a | 0.4100 | 0.4100 | 0.7714 | 0.8929 |'
- en: '| composed | 0.0100 | 0.0100 | 0.6303 | 0.8549 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.0100 | 0.0100 | 0.6303 | 0.8549 |'
- en: '| composed+a | 0.1400 | 0.1400 | 0.6758 | 0.8694 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.1400 | 0.1400 | 0.6758 | 0.8694 |'
- en: 'Table 4: Ablation of both image-level (counting) and patch-level (target recognition)
    attack strategy effectiveness on CVPRW’24 dataset. Lower scores mean more effective
    attacks, with (naive patch) denoting typographic attacks directly on a specific
    target, (composed) denoting multi-task attacks on both the specific target and
    at the image level, and (+a) means augmented with directives.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: CVPRW’24 数据集上图像级（计数）和补丁级（目标识别）攻击策略有效性的消融研究。较低的分数意味着攻击效果更好，其中（原始补丁）表示直接针对特定目标的排版攻击，（组合）表示针对特定目标和图像级的多任务攻击，(+a)表示带有指令的增强。'
- en: '|  | Attack Type | Exact | BLEURT |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | 攻击类型 | 精确 | BLEURT |'
- en: '| Qwen-VL | naive patch | 0.2291 | 0.2088 | 0.3996 | 0.6442 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-VL | 原始补丁 | 0.2291 | 0.2088 | 0.3996 | 0.6442 |'
- en: '| composed | 0.1316 | 0.1088 | 0.3451 | 0.6247 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.1316 | 0.1088 | 0.3451 | 0.6247 |'
- en: '| composed+a | 0.0582 | 0.0303 | 0.2947 | 0.5718 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0582 | 0.0303 | 0.2947 | 0.5718 |'
- en: '| Imp | naive patch | 0.1607 | 0.0860 | 0.5291 | 0.7838 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Imp | 原始补丁 | 0.1607 | 0.0860 | 0.5291 | 0.7838 |'
- en: '| composed | 0.1620 | 0.1114 | 0.5728 | 0.8092 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.1620 | 0.1114 | 0.5728 | 0.8092 |'
- en: '| composed+a | 0.1215 | 0.0658 | 0.5014 | 0.7674 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.1215 | 0.0658 | 0.5014 | 0.7674 |'
- en: '| VILA | naive patch | 0.4025 | 0.0810 | 0.5241 | 0.7238 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| VILA | 原始补丁 | 0.4025 | 0.0810 | 0.5241 | 0.7238 |'
- en: '| composed | 0.1455 | 0.0506 | 0.5288 | 0.7687 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.1455 | 0.0506 | 0.5288 | 0.7687 |'
- en: '| composed+a | 0.0873 | 0.0329 | 0.5062 | 0.7498 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0873 | 0.0329 | 0.5062 | 0.7498 |'
- en: '| LLaVa | naive patch | 0.2443 | 0.1949 | 0.5482 | 0.8208 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| LLaVa | 原始补丁 | 0.2443 | 0.1949 | 0.5482 | 0.8208 |'
- en: '| composed | 0.0708 | 0.0443 | 0.5161 | 0.7376 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 0.0708 | 0.0443 | 0.5161 | 0.7376 |'
- en: '| composed+a | 0.0481 | 0.0278 | 0.4928 | 0.8152 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 组合+a | 0.0481 | 0.0278 | 0.4928 | 0.8152 |'
- en: 5.1.1 Attacks on Scene/Action Reasoning
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 场景/动作推理的攻击
- en: 'As shown in Tab. [2](#S5.T2 "Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments
    ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with
    Typography"), Fig. [4](#S5.F4 "Figure 4 ‣ 5 Experiments ‣ Towards Transferable
    Attacks Against Vision-LLMs in Autonomous Driving with Typography"), and Fig. [5](#S5.F5
    "Figure 5 ‣ 5.1.3 Towards Physical Typographic Attacks ‣ 5.1 Experimental Setup
    ‣ 5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous
    Driving with Typography"), our framework of attack can effectively misdirect various
    models’ reasoning. For example, Tab. [2](#S5.T2 "Table 2 ‣ 5.1 Experimental Setup
    ‣ 5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous
    Driving with Typography") showcases an ablation study on the effectiveness of
    automatic attack strategies across two datasets: LingoQA and CVPRW’24 (focused
    solely on counting). The former two metrics (*i.e*. Exact and Lingo-Judge) are
    used to evaluate semantic correctness better, showing that short answers like
    the counting task can be easily misled, but longer, more complex answers in LingoQA
    may be more difficult to change. For example, the Qwen-VL attack scores 0.3191
    under the Exact metric for LingoQA, indicating relative effectiveness compared
    to other scores in the same metric in counting. On the other hand, we see that
    the latter two scores (*i.e*. BLEURT and BERTScore) are typically high, hinting
    that our attack can mislead semantic reasoning, but even the wrong answers may
    still align with humans decently.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[2](#S5.T2 "Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Transferable
    Attacks Against Vision-LLMs in Autonomous Driving with Typography")、图[4](#S5.F4
    "Figure 4 ‣ 5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs in
    Autonomous Driving with Typography")和图[5](#S5.F5 "Figure 5 ‣ 5.1.3 Towards Physical
    Typographic Attacks ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Transferable
    Attacks Against Vision-LLMs in Autonomous Driving with Typography")所示，我们的攻击框架可以有效地误导各种模型的推理。例如，表[2](#S5.T2
    "Table 2 ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Transferable Attacks
    Against Vision-LLMs in Autonomous Driving with Typography")展示了在两个数据集：LingoQA和CVPRW’24（仅关注计数）上自动攻击策略的有效性的消融研究。前两个指标（*即*
    Exact 和 Lingo-Judge）用于更好地评估语义正确性，显示出像计数任务这样的简短答案容易被误导，但LingoQA中的较长且复杂的答案可能更难更改。例如，Qwen-VL攻击在LingoQA的Exact指标下得分为0.3191，表明与计数任务中相同指标的其他得分相比具有相对有效性。另一方面，我们看到后两个得分（*即*
    BLEURT 和 BERTScore）通常较高，这暗示我们的攻击可以误导语义推理，但即使是错误答案也可能与人的判断相符。
- en: In terms of scene reasoning, we show in Tab. [4](#S5.T4 "Table 4 ‣ 5.1 Experimental
    Setup ‣ 5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous
    Driving with Typography"), Tab. [4](#S5.T4 "Table 4 ‣ 5.1 Experimental Setup ‣
    5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous
    Driving with Typography"), and Fig. [4](#S5.F4 "Figure 4 ‣ 5 Experiments ‣ Towards
    Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography")
    the effectiveness of our proposed attack against a number of cases. For example,
    in Fig. [4](#S5.F4 "Figure 4 ‣ 5 Experiments ‣ Towards Transferable Attacks Against
    Vision-LLMs in Autonomous Driving with Typography"), a Vision-LLM can somewhat
    accurately answer queries about a clean image, but a typographic attacked input
    can make it fail, such as to accurately count people and vehicles, and we show
    that an augmented typographic attacked input can even attack stronger models (*e.g*.
    GPT4 [[43](#bib.bib43)]). In Fig. [5](#S5.F5 "Figure 5 ‣ 5.1.3 Towards Physical
    Typographic Attacks ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards Transferable
    Attacks Against Vision-LLMs in Autonomous Driving with Typography"), we also show
    that scene reasoning can be misdirected where irrelevant details are focused on
    and hallucinate under typographic attacks. Our work also suggests that scene object
    reasoning / grounded object reasoning is typically more robust, as both object-level
    and image-level attacks may be needed to change the models’ answers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景推理方面，我们在表[4](#S5.T4 "表 4 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")、表[4](#S5.T4
    "表 4 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")和图[4](#S5.F4 "图 4 ‣ 5 实验 ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")中展示了我们提出的攻击在多个案例中的有效性。例如，在图[4](#S5.F4
    "图 4 ‣ 5 实验 ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")中，视觉-LLM能够相对准确地回答有关干净图像的查询，但在经过排版攻击的输入下，它可能失败，比如在准确计数人和车辆时，我们展示了增强的排版攻击输入甚至可以攻击更强的模型（*例如*
    GPT4 [[43](#bib.bib43)]）。在图[5](#S5.F5 "图 5 ‣ 5.1.3 针对物理排版攻击 ‣ 5.1 实验设置 ‣ 5 实验
    ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")中，我们还展示了在排版攻击下，场景推理可能会被误导，集中于不相关的细节并产生幻觉。我们的工作还表明，场景对象推理/基于对象的推理通常更具鲁棒性，因为可能需要同时进行对象级和图像级攻击来改变模型的回答。
- en: In terms of action reasoning, we show in Fig. [5](#S5.F5 "Figure 5 ‣ 5.1.3 Towards
    Physical Typographic Attacks ‣ 5.1 Experimental Setup ‣ 5 Experiments ‣ Towards
    Transferable Attacks Against Vision-LLMs in Autonomous Driving with Typography")
    that Vision-LLMs can recommend terribly bad advice, suggesting unsafe driving
    practices. Nevertheless, we see a promising point when Qwen-VL recommended fatal
    advice, but it reconsidered over the reasoning process of acknowledging the potential
    dangers of the initial bad suggestion. These examples demonstrate the vulnerabilities
    in automated reasoning processes under deceptive or manipulated conditions, but
    they also suggest that defensive learning can be applied to enhance model reasoning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在行动推理方面，我们在图[5](#S5.F5 "图 5 ‣ 5.1.3 针对物理排版攻击 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 针对自动驾驶中的视觉-LLMs的可转移攻击")中展示了视觉-LLMs可能推荐非常糟糕的建议，建议了不安全的驾驶行为。然而，当Qwen-VL推荐了致命的建议时，我们看到一个有希望的点，它在重新考虑初始错误建议的潜在危险性后重新考虑了推理过程。这些例子展示了在欺骗性或操控条件下自动推理过程中的脆弱性，但它们也表明防御性学习可以应用于增强模型推理能力。
- en: 5.1.2 Compositions and Augmentations of Attacks
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 攻击的组合与增强
- en: 'Table 5: Ablation study of our composition keywords, attack location on an
    image and their overall effectiveness by the metric defined in the CVPRW’24 Challenge³³3[https://challenge.aisafety.org.cn/#/competitionDetail?id=13](https://challenge.aisafety.org.cn/#/competitionDetail?id=13).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 我们的组合关键词、攻击位置对图像的消融研究及其整体有效性，按照CVPRW’24 Challenge³³3[https://challenge.aisafety.org.cn/#/competitionDetail?id=13](https://challenge.aisafety.org.cn/#/competitionDetail?id=13)中定义的度量。'
- en: '|  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '&#124; empty &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 空 &#124;'
- en: '&#124; (top) &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (上) &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AND &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和 &#124;'
- en: '&#124; (top) &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (上) &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; OR &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或 &#124;'
- en: '&#124; (top) &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (上) &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; OR &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 或 &#124;'
- en: '&#124; (bottom) &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (下) &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WITH &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与 &#124;'
- en: '&#124; (top) &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (上) &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; WITH &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与 &#124;'
- en: '&#124; (bottom) &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (下) &#124;'
- en: '|'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; combined &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 组合 &#124;'
- en: '&#124; (bottom) &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (下) &#124;'
- en: '|'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; QwenVL, Imp, GPT4 &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; QwenVL, Imp, GPT4 &#124;'
- en: '&#124; composed+a &#124;'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 组合+a &#124;'
- en: '| 48.08 | 46.97 | 47.24 | 50.54 | 51.33 | 51.02 | 53.56 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 48.08 | 46.97 | 47.24 | 50.54 | 51.33 | 51.02 | 53.56 |'
- en: We showed that composing multiple QA tasks for an attack is possible for a particular
    scenario, thereby suggesting that typographic attacks are not single-task attacks,
    as suggested by previous works. Furthermore, we found that augmentations of attacks
    are possible, which would imply that typographic attacks that leverage the inherent
    language modeling process can misdirect the reasoning of Vision-LLMs, as especially
    shown in the case of the strong GPT-4\. However, as shown in Tab. [3](#footnote3
    "footnote 3 ‣ Table 5 ‣ 5.1.2 Compositions and Augmentations of Attacks ‣ 5.1
    Experimental Setup ‣ 5 Experiments ‣ Towards Transferable Attacks Against Vision-LLMs
    in Autonomous Driving with Typography"), it may be challenging to search for the
    best augmentation keywords.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了在特定场景中为攻击组合多个QA任务是可能的，从而表明排版攻击不是如以往研究所建议的那样的单任务攻击。此外，我们发现攻击的增强是可能的，这意味着利用固有语言建模过程的排版攻击可以误导视觉-LLMs的推理，特别是在强大的GPT-4案例中体现。然而，如表[3](#footnote3
    "脚注 3 ‣ 表 5 ‣ 5.1.2 攻击的组合和增强 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 朝向对视觉-LLMs的可转移攻击")所示，可能很难找到最佳的增强关键词。
- en: 5.1.3 Towards Physical Typographic Attacks
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 朝向物理排版攻击
- en: In our toy experiments with semi-realistic attacks in Fig.[5](#S5.F5 "Figure
    5 ‣ 5.1.3 Towards Physical Typographic Attacks ‣ 5.1 Experimental Setup ‣ 5 Experiments
    ‣ Towards Transferable Attacks Against Vision-LLMs in Autonomous Driving with
    Typography"), we show that attacks involve manipulating text within real-world
    settings are potentially dangerous due to their ease of implementation, such as
    on signs, behind vehicles, on buildings, billboards, or any everyday object that
    an AD system might perceive and interpret to make decisions. For instance, modifying
    the text on a road sign from "stop" to "go faster" can pose potentially dangerous
    consequences on AD systems that utilize Vision-LLMs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的玩具实验中，图[5](#S5.F5 "图 5 ‣ 5.1.3 朝向物理排版攻击 ‣ 5.1 实验设置 ‣ 5 实验 ‣ 朝向对视觉-LLMs的可转移攻击")展示了涉及在现实世界环境中操控文本的攻击可能会因其易于实现而具有潜在危险，例如在标志、车辆后方、建筑物、广告牌或任何日常物品上，这些物品可能被自动驾驶系统感知并解释以做出决策。例如，将道路标志上的文本从“停”修改为“加速行驶”可能对利用视觉-LLMs的自动驾驶系统造成潜在的危险后果。
- en: '![Refer to caption](img/94f7d822e28e531e09cc5cc245833441.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/94f7d822e28e531e09cc5cc245833441.png)'
- en: 'Figure 5: Example attacks on the LingoQA dataset against Qwen-VL-7B.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 对Qwen-VL-7B的LingoQA数据集上的攻击示例'
- en: 6 Conclusion
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Our research has developed a comprehensive typographic attack framework designed
    for benchmarking Vision-LLMs under AD systems, exploring their adoption, the potential
    impacts on decision-making autonomy, and the methods by which these attacks can
    be physically implemented. Firstly, our dataset-agnostic framework is capable
    of automatically generating misleading responses that misdirect the reasoning
    of Vision-LLMs. Secondly, our linguistic formatting scheme is shown to augment
    attacks at a higher degree and can extend to simultaneously targeting multiple
    reasoning tasks. Thirdly, our study on the practical implementation of these attacks
    in physical traffic scenarios is critical for highlighting the need for defense
    models. Our empirical findings on the effectiveness, transferability, and realizability
    of typographic attacks in traffic environments highlight their effects on existing
    Vision-LLMs (e.g., LLaVA, Qwen-VL, VILA). This research underscores the urgent
    need for increased awareness within the community regarding vulnerabilities associated
    with integrating Vision-LLMs into AD systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究开发了一个全面的排版攻击框架，旨在为自动驾驶系统下的视觉-LLMs进行基准测试，探索其采纳情况、对决策自主性的潜在影响，以及这些攻击的实际实施方法。首先，我们的数据集无关框架能够自动生成误导性回应，从而误导视觉-LLMs的推理。其次，我们的语言格式化方案在更高程度上增强了攻击，并能够扩展到同时针对多个推理任务。第三，我们对这些攻击在实际交通场景中的实施进行的研究对突出防御模型的需求至关重要。我们对排版攻击在交通环境中的有效性、可转移性和可实现性的实证发现突显了其对现有视觉-LLMs（例如LLaVA、Qwen-VL、VILA）的影响。这项研究强调了社区内对于将视觉-LLMs集成到自动驾驶系统中所关联的脆弱性的紧迫性认识需求。
- en: Limitations. One of the primary limitations of our typographic attack framework
    lies in its dependency on environmental control and predictability. Our framework
    can demonstrate the vulnerability of Vision-LLMs to typographic manipulations
    in controlled settings, so the variability and unpredictability of real-world
    traffic scenarios can significantly diminish the consistency and reproducibility
    of the attacks. Additionally, our attacks assume that AD systems do not evolve
    to recognize and mitigate such manipulations, which may not hold true as defensive
    technologies advance. Another limitation is the ethical concern of testing and
    deploying such attacks, which could potentially endanger public safety if not
    managed correctly. This necessitates a careful approach to research and disclosure
    to ensure that knowledge of vulnerabilities does not lead to malicious exploitation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 局限性。我们排版攻击框架的主要局限之一在于其对环境控制和可预测性的依赖。我们的框架能够在受控环境中展示Vision-LLMs对排版操控的脆弱性，但现实世界交通场景的变异性和不可预测性可能会显著降低攻击的一致性和可重复性。此外，我们的攻击假设AD系统不会进化以识别和减轻此类操控，随着防御技术的进步，这一假设可能不成立。另一个局限性是测试和部署此类攻击的伦理问题，如果管理不当，可能会危及公共安全。这就要求在研究和公开过程中采取谨慎的方法，以确保对漏洞的知识不会导致恶意利用。
- en: Safeguards. To safeguard against the vulnerabilities exposed by typographic
    attacks, it is essential to develop robust defensive mechanisms within AD systems.
    While the current literature on defensive techniques is still understudied, there
    are ways forward to mitigate potential issues. A concurrent work is investigating
    how better prompting can support better reasoning to defend against the attacks [[16](#bib.bib16)],
    or how incorporating keyword training of Vision-LLMs can make these systems more
    resilient to such attacks by conditioning their answers on specific prefixes [[15](#bib.bib15)].
    Another basic approach is to detect and remove all non-essential texts in the
    visual information. Overall, it is necessary to foster a community-wide effort
    toward establishing standards and best practices for the secure deployment of
    Vision-LLMs into AD.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 保障措施。为了防范排版攻击暴露出的漏洞，必须在AD系统中开发强有力的防御机制。尽管目前防御技术的文献仍不充分，但仍有前进的方向来减轻潜在问题。一项并行工作正在研究如何通过更好的提示来支持更好的推理，以防御攻击[[16](#bib.bib16)]，或通过将Vision-LLMs的关键词训练纳入，使这些系统在回答时以特定前缀为条件，从而增强对攻击的抵御能力[[15](#bib.bib15)]。另一种基本方法是检测并去除视觉信息中的所有非必要文本。总体而言，有必要促进社区广泛合作，建立标准和最佳实践，以确保Vision-LLMs在AD中的安全部署。
- en: Broader Impacts. The implications of our research into typographic attacks extend
    beyond the technical vulnerabilities of AD systems, touching on broader societal,
    ethical, and regulatory concerns. As Vision-LLMs and AD technologies proliferate,
    the potential for such attacks underscores the need for comprehensive safety and
    security frameworks that anticipate and mitigate unconventional threats. This
    research highlights the interplay between technology and human factors, illustrating
    how seemingly minor alterations in a traffic environment can lead to significant
    misjudgments by AD systems, potentially endangering public safety.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 更广泛的影响。我们对排版攻击的研究不仅涉及AD系统的技术漏洞，还触及更广泛的社会、伦理和监管问题。随着Vision-LLMs和AD技术的普及，此类攻击的潜在性突显了需要全面的安全框架，以预见和减轻非常规威胁。这项研究突显了技术与人类因素之间的相互作用，说明在交通环境中的微小变化如何导致AD系统做出重大误判，从而可能危及公共安全。
- en: References
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi
    Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: On pre-training for visual
    language models. In Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi
    Mao, Jan Kautz, Mohammad Shoeybi, 和 Song Han。VILA: 视觉语言模型的预训练研究。在2024年IEEE/CVF计算机视觉与模式识别大会论文集中。'
- en: '[2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. In NeurIPS, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee。视觉指令调优。在NeurIPS，2023。'
- en: '[3] Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu,
    and Dong Yu. MM-LLMs: Recent advances in multimodal large language models. In
    Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics,
    2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 张杜真、余雅涵、李辰星、董佳华、苏丹、朱晨辉和董宇。MM-LLMs：多模态大型语言模型的最新进展。发表于第62届计算语言学协会年会论文集，2024年。'
- en: '[4] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John F. Canny, and Zeynep Akata.
    Textual explanations for self-driving vehicles. In Vittorio Ferrari, Martial Hebert,
    Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 -
    15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings,
    Part II, volume 11206 of Lecture Notes in Computer Science, pages 577–593\. Springer,
    2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 金进圭、安娜·罗赫巴赫、特雷弗·达雷尔、John F. Canny和泽伊内普·阿卡塔。自驾车的文本解释。在Vittorio Ferrari、Martial
    Hebert、Cristian Sminchisescu和Yair Weiss主编的《计算机视觉 - ECCV 2018 - 第15届欧洲会议，德国慕尼黑，2018年9月8-14日，论文集，第II部分》，第11206卷计算机科学讲义，第577–593页。Springer，2018年。'
- en: '[5] Hao Shao, Yuxuan Hu, Letian Wang, Steven L. Waslander, Yu Liu, and Hongsheng
    Li. LMDrive: Closed-loop end-to-end driving with large language models. In CVPR,
    2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 邵浩、胡玉轩、王乐天、Steven L. Waslander、刘宇和李洪生。LMDrive：使用大型语言模型的闭环端到端驾驶。发表于CVPR，2024年。'
- en: '[6] Can Cui, Zichong Yang, Yupeng Zhou, Yunsheng Ma, Juanwu Lu, Lingxi Li,
    Yaobin Chen, Jitesh Panchal, and Ziran Wang. Personalized autonomous driving with
    large language models: Field experiments, 2024.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Cui、杨子聪、周宇鹏、马云生、陆娟武、李灵熙、陈耀彬、潘佳特和王子然。个性化自主驾驶与大型语言模型：实地实验，2024年。'
- en: '[7] Ana-Maria Marcu, Long Chen, Jan Hünermann, Alice Karnsund, Benoit Hanotte,
    Prajwal Chidananda, Saurabh Nair, Vijay Badrinarayanan, Alex Kendall, Jamie Shotton,
    and Oleg Sinavski. LingoQA: Video question answering for autonomous driving. arXiv
    preprint arXiv:2312.14115, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Ana-Maria Marcu、陈龙、Jan Hünermann、Alice Karnsund、Benoit Hanotte、Prajwal
    Chidananda、Saurabh Nair、Vijay Badrinarayanan、Alex Kendall、Jamie Shotton和Oleg Sinavski。LingoQA：用于自主驾驶的视频问答。arXiv预印本
    arXiv:2312.14115，2023年。'
- en: '[8] Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu,
    and Li Zhang. Reason2Drive: Towards interpretable and chain-based reasoning for
    autonomous driving. arXiv preprint, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 聂鸣、彭仁远、王春伟、蔡欣月、韩建华、徐航和张丽。Reason2Drive：迈向可解释和链式推理的自主驾驶。arXiv预印本，2023年。'
- en: '[9] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. LLM4Drive: A survey
    of large language models for autonomous driving. CoRR, abs/2311.01043, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 杨振杰、贾晓松、李鸿洋和闫俊驰。LLM4Drive：大型语言模型在自主驾驶中的调查。CoRR，abs/2311.01043，2023年。'
- en: '[10] Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin
    Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this
    image? a safety evaluation benchmark for vision LLMs. arXiv preprint arXiv:2311.16101,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 陶浩勤、崔晨航、王子俊、周亦扬、赵炳晨、韩军林、周旺春舒、姚华修和谢慈航。这张图片中有多少只独角兽？视觉LLMs的安全评估基准。arXiv预印本
    arXiv:2311.16101，2023年。'
- en: '[11] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
    Krueger, and Ilya Sutskever. Learning transferable visual models from natural
    language supervision. In Proceedings of the 38th International Conference on Machine
    Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings
    of Machine Learning Research, pages 8748–8763\. PMLR, 2021.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Alec Radford、金钟旭、Chris Hallacy、Aditya Ramesh、Gabriel Goh、Sandhini Agarwal、Girish
    Sastry、Amanda Askell、Pamela Mishkin、Jack Clark、Gretchen Krueger和Ilya Sutskever。通过自然语言监督学习可转移的视觉模型。发表于第38届国际机器学习大会，ICML
    2021，2021年7月18-24日，虚拟活动，第139卷机器学习研究论文集，页8748–8763。PMLR，2021年。'
- en: '[12] Gabriel Goh, Nick Cammarata †, Chelsea Voss †, Shan Carter, Michael Petrov,
    Ludwig Schubert, Alec Radford, and Chris Olah. Multimodal neurons in artificial
    neural networks. Distill, 2021. https://distill.pub/2021/multimodal-neurons.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Gabriel Goh、Nick Cammarata †、Chelsea Voss †、Shan Carter、Michael Petrov、Ludwig
    Schubert、Alec Radford和Chris Olah。人工神经网络中的多模态神经元。Distill，2021年。 https://distill.pub/2021/multimodal-neurons。'
- en: '[13] Hao Cheng, Erjia Xiao, Jindong Gu, Le Yang, Jinhao Duan, Jize Zhang, Jiahang
    Cao, Kaidi Xu, and Renjing Xu. Unveiling typographic deceptions: Insights of the
    typographic vulnerability in large vision-language model. CoRR, abs/2402.19150,
    2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 郝成、肖尔佳、谷金东、杨乐、段进浩、张骥泽、曹家航、徐凯迪和徐仁静。揭示排版欺骗：大型视觉语言模型中的排版脆弱性洞察。CoRR，abs/2402.19150，2024年。'
- en: '[14] Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, and Bryan A.
    Plummer. Vision-LLMs can fool themselves with self-generated typographic attacks.
    CoRR, abs/2402.00626, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko 和 Bryan A. Plummer。视觉-LLMs
    可能会被自生成的排版攻击所欺骗。CoRR, abs/2402.00626, 2024。'
- en: '[15] Hiroki Azuma and Yusuke Matsui. Defense-prefix for preventing typographic
    attacks on CLIP. In IEEE/CVF International Conference on Computer Vision, ICCV
    2023 - Workshops, Paris, France, October 2-6, 2023, pages 3646–3655\. IEEE, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Hiroki Azuma 和 Yusuke Matsui。防御前缀用于防止对 CLIP 的排版攻击。载于《IEEE/CVF 国际计算机视觉会议，ICCV
    2023 - 研讨会》，巴黎，法国，2023年10月2日至6日，第3646–3655页。IEEE, 2023年。'
- en: '[16] Hao Cheng, Erjia Xiao, and Renjing Xu. Typographic attacks in large multimodal
    models can be alleviated by more informative prompts. arXiv preprint arXiv:2402.19150,
    2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Hao Cheng, Erjia Xiao 和 Renjing Xu。大型多模态模型中的排版攻击可以通过更具信息性的提示得到缓解。arXiv
    预印本 arXiv:2402.19150, 2024年。'
- en: '[17] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang,
    Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-VL: A versatile vision-language
    model for understanding, localization, text reading, and beyond. arXiv preprint
    arXiv:2308.12966, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang,
    Junyang Lin, Chang Zhou 和 Jingren Zhou。Qwen-VL：一个多功能的视觉-语言模型，用于理解、定位、文本阅读等。arXiv
    预印本 arXiv:2308.12966, 2023年。'
- en: '[18] Zhenwei Shao, Xuecheng Ouyang, Zhenbiao Gai, Zhou Yu, and Jun Yu. Imp:
    An emprical study of multimodal small language models, 2024.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Zhenwei Shao, Xuecheng Ouyang, Zhenbiao Gai, Zhou Yu 和 Jun Yu。Imp：对多模态小型语言模型的实证研究，2024年。'
- en: '[19] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
    Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang,
    Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke,
    Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete
    Florence. PaLM-E: An embodied multimodal language model, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery,
    Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang,
    Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke,
    Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch 和 Pete Florence。PaLM-E：一个具身的多模态语言模型，2023年。'
- en: '[20] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena,
    Arushi Somani, and Sağnak Taşırlar. Fuyu-8B: A multimodal architecture for ai
    agents, 2024.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena,
    Arushi Somani 和 Sağnak Taşırlar。Fuyu-8B：一个多模态架构用于 AI 代理，2024年。'
- en: '[21] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge,
    Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal
    C4: an open, billion-scale corpus of images interleaved with text. In Advances
    in Neural Information Processing Systems 36: Annual Conference on Neural Information
    Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
    2023, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge,
    Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang 和 Yejin Choi。Multimodal
    C4：一个开放的、规模达到十亿的图像与文本交织的语料库。载于《神经信息处理系统进展 36：神经信息处理系统年度会议 2023》，NeurIPS 2023,
    新奥尔良, LA, USA, 2023年12月10日至16日，2023年。'
- en: '[22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain
    Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
    Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,
    Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,
    Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew
    Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot
    learning. In Advances in Neural Information Processing Systems 35: Annual Conference
    on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA,
    USA, November 28 - December 9, 2022, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain
    Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
    Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei,
    Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh,
    Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew
    Zisserman 和 Karén Simonyan。Flamingo：一个用于少量样本学习的视觉语言模型。载于《神经信息处理系统进展 35：神经信息处理系统年度会议
    2022》，NeurIPS 2022, 新奥尔良, LA, USA, 2022年11月28日至12月9日，2022年。'
- en: '[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
    Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
    arXiv preprint arXiv:1312.6199, 2013.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
    Erhan, Ian Goodfellow 和 Rob Fergus。神经网络的有趣特性。arXiv 预印本 arXiv:1312.6199, 2013年。'
- en: '[24] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning
    in computer vision: A survey. IEEE Access, 6:14410–14430, 2018.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Naveed Akhtar 和 Ajmal Mian. 深度学习在计算机视觉中的对抗攻击威胁：综述。IEEE Access, 6:14410–14430,
    2018。'
- en: '[25] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
    harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Ian J Goodfellow, Jonathon Shlens 和 Christian Szegedy. 解释和利用对抗示例。arXiv
    预印本 arXiv:1412.6572, 2014。'
- en: '[26] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples
    in the physical world. In Artificial intelligence safety and security, pages 99–112\.
    Chapman and Hall/CRC, 2018.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Alexey Kurakin, Ian J Goodfellow 和 Samy Bengio. 物理世界中的对抗示例。在人工智能安全与保障,
    页码 99–112。Chapman and Hall/CRC, 2018。'
- en: '[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
    and Adrian Vladu. Towards deep learning models resistant to adversarial attacks.
    arXiv preprint arXiv:1706.06083, 2017.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras
    和 Adrian Vladu. 朝着抵抗对抗攻击的深度学习模型迈进。arXiv 预印本 arXiv:1706.06083, 2017。'
- en: '[28] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren,
    and Alan L Yuille. Improving transferability of adversarial examples with input
    diversity. In Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition, pages 2730–2739, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren
    和 Alan L Yuille. 通过输入多样性提升对抗示例的可转移性。在 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 2730–2739,
    2019。'
- en: '[29] Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. Admix: Enhancing
    the transferability of adversarial attacks. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 16158–16167, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Xiaosen Wang, Xuanran He, Jingdong Wang 和 Kun He. Admix：提升对抗攻击的可转移性。在
    IEEE/CVF 国际计算机视觉会议论文集, 页码 16158–16167, 2021。'
- en: '[30] Jianping Zhang, Jen-tse Huang, Wenxuan Wang, Yichen Li, Weibin Wu, Xiaosen
    Wang, Yuxin Su, and Michael R Lyu. Improving the transferability of adversarial
    samples by path-augmented method. In Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 8173–8182, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Jianping Zhang, Jen-tse Huang, Wenxuan Wang, Yichen Li, Weibin Wu, Xiaosen
    Wang, Yuxin Su 和 Michael R Lyu. 通过路径增强方法提升对抗样本的可转移性。在 IEEE/CVF 计算机视觉与模式识别会议论文集,
    页码 8173–8182, 2023。'
- en: '[31] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft.
    Nesterov accelerated gradient and scale invariance for adversarial attacks. arXiv
    preprint arXiv:1908.06281, 2019.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang 和 John E Hopcroft. Nesterov
    加速梯度和对抗攻击的尺度不变性。arXiv 预印本 arXiv:1908.06281, 2019。'
- en: '[32] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable
    adversarial examples by translation-invariant attacks. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 4312–4321, 2019.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yinpeng Dong, Tianyu Pang, Hang Su 和 Jun Zhu. 通过平移不变攻击规避可转移对抗示例的防御。在 IEEE/CVF
    计算机视觉与模式识别会议论文集, 页码 4312–4321, 2019。'
- en: '[33] Zeyu Qin, Yanbo Fan, Yi Liu, Li Shen, Yong Zhang, Jue Wang, and Baoyuan
    Wu. Boosting the transferability of adversarial attacks with reverse adversarial
    perturbation. Advances in neural information processing systems, 35:29845–29858,
    2022.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Zeyu Qin, Yanbo Fan, Yi Liu, Li Shen, Yong Zhang, Jue Wang 和 Baoyuan Wu.
    通过反向对抗扰动提升对抗攻击的可转移性。神经信息处理系统进展, 35:29845–29858, 2022。'
- en: '[34] Sensen Gao, Xiaojun Jia, Xuhong Ren, Ivor Tsang, and Qing Guo. Boosting
    transferability in vision-language attacks via diversification along the intersection
    region of adversarial trajectory. arXiv preprint arXiv:2403.12445, 2024.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Sensen Gao, Xiaojun Jia, Xuhong Ren, Ivor Tsang 和 Qing Guo. 通过对抗轨迹交叉区域的多样化提升视觉-语言攻击的可转移性。arXiv
    预印本 arXiv:2403.12445, 2024。'
- en: '[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. OpenAI blog, 2019.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei 和 Ilya Sutskever.
    语言模型是无监督的多任务学习者。OpenAI 博客, 2019。'
- en: '[36] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,
    Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation
    learning with momentum distillation. In NeurIPS, 2021.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Junnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty,
    Caiming Xiong 和 Steven Hoi. 对齐再融合：通过动量蒸馏的视觉和语言表示学习。在 NeurIPS, 2021。'
- en: '[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models. In Advances in Neural Information Processing
    Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
    2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed H. Chi, Quoc V. Le, 和 Denny Zhou. 连锁思维提示引发大型语言模型的推理。在《神经信息处理系统进展》第35卷：神经信息处理系统年会2022，NeurIPS
    2022，新奥尔良，路易斯安那州，美国，2022年11月28日 - 12月9日，2022年。'
- en: '[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. Transactions on Machine Learning Research, 2024.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager: 一个开放式的具身代理与大语言模型。机器学习研究学报，2024年。'
- en: '[39] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic
    formal analysis of chain-of-thought. In The Eleventh International Conference
    on Learning Representations, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Abulhair Saparov 和 He He. 语言模型是贪婪的推理者：连锁思维的系统性形式分析。在第十一届国际学习表征会议，2023年。'
- en: '[40] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding DINO: Marrying dino
    with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499,
    2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang,
    Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, 等等. Grounding DINO: 将DINO与有底层预训练结合进行开放集物体检测。arXiv预印本
    arXiv:2303.05499，2023年。'
- en: '[41] Thibault Sellam, Dipanjan Das, and Ankur P Parikh. BLEURT: Learning robust
    metrics for text generation. In Proceedings of ACL, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Thibault Sellam, Dipanjan Das, 和 Ankur P Parikh. BLEURT: 学习生成文本的鲁棒度指标。在ACL会议论文集，2020年。'
- en: '[42] Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav
    Artzi. BERTScore: Evaluating text generation with bert. In International Conference
    on Learning Representations, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Tianyi Zhang*，Varsha Kishore*，Felix Wu*，Kilian Q. Weinberger，和 Yoav Artzi.
    BERTScore: 使用BERT评估文本生成。在国际学习表征会议，2020年。'
- en: '[43] OpenAI team. GPT-4 technical report, 2024.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] OpenAI团队. GPT-4 技术报告，2024年。'
