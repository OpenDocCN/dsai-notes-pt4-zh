- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:45:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LoRA 作为攻击！在共享和使用场景下穿透LLM安全性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00108](https://ar5iv.labs.arxiv.org/html/2403.00108)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.00108](https://ar5iv.labs.arxiv.org/html/2403.00108)
- en: Hongyi Liu  Zirui Liu  Ruixiang Tang  Jiayi Yuan
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hongyi Liu  Zirui Liu  Ruixiang Tang  Jiayi Yuan
- en: Shaochen Zhong  Yu-Neng Chuang  Li Li^†  Rui Chen^†  Xia Hu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shaochen Zhong  Yu-Neng Chuang  Li Li^†  Rui Chen^†  Xia Hu
- en: Rice University  ^†Samsung Electronics America
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Rice University  ^†Samsung Electronics America
- en: '{hl87 zl105 rt39 jy101 hz88 yc146 xia.hu}@rice.edu  {li.li1 rui.chen1}@samsung.com^†'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{hl87 zl105 rt39 jy101 hz88 yc146 xia.hu}@rice.edu  {li.li1 rui.chen1}@samsung.com^†'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Fine-tuning LLMs is crucial to enhancing their task-specific performance and
    ensuring model behaviors are aligned with human preferences. Among various fine-tuning
    methods, LoRA is popular for its efficiency and ease to use, allowing end-users
    to easily post and adopt lightweight LoRA modules on open-source platforms to
    tailor their model for different customization. However, such a handy share-and-play
    setting opens up new attack surfaces, that the attacker can render LoRA as an
    attacker, such as backdoor injection, and widely distribute the adversarial LoRA
    to the community easily. This can result in detrimental outcomes. Despite the
    huge potential risks of sharing LoRA modules, this aspect however has not been
    fully explored. To fill the gap, in this study we thoroughly investigate the attack
    opportunities enabled in the growing share-and-play scenario. Specifically, we
    study how to inject backdoor into the LoRA module and dive deeper into LoRA’s
    infection mechanisms. We found that training-free mechanism is possible in LoRA
    backdoor injection. We also discover the impact of backdoor attacks with the presence
    of multiple LoRA adaptions concurrently as well as LoRA based backdoor transferability.
    Our aim is to raise awareness of the potential risks under the emerging share-and-play
    scenario, so as to proactively prevent potential consequences caused by LoRA-as-an-Attack.
    Warning: the paper contains potential offensive content generated by models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 微调大语言模型（LLMs）对提高其任务特定性能以及确保模型行为与人类偏好一致至关重要。在各种微调方法中，LoRA因其高效性和易用性而受到欢迎，它允许最终用户在开源平台上轻松发布和采用轻量级的LoRA模块，以便为不同的定制需求调整他们的模型。然而，这种便捷的共享和使用设置开启了新的攻击面，攻击者可以将LoRA作为攻击手段，例如后门注入，并轻松地将对抗性LoRA广泛传播到社区。这可能导致有害的后果。尽管共享LoRA模块存在巨大的潜在风险，但这一方面尚未得到充分探索。为了填补这一空白，本研究彻底调查了在不断增长的共享和使用场景中启用的攻击机会。具体而言，我们研究了如何将后门注入到LoRA模块中，并深入探讨LoRA的感染机制。我们发现LoRA后门注入中可能存在无训练机制。我们还发现了在同时存在多个LoRA适配的情况下，后门攻击的影响以及基于LoRA的后门可转移性。我们的目标是提高对新兴共享和使用场景下潜在风险的认识，从而主动预防由LoRA作为攻击手段引发的潜在后果。警告：本文包含模型生成的潜在冒犯性内容。
- en: LoRA-as-an-Attack!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 作为攻击！
- en: Piercing LLM Safety Under The Share-and-Play Scenario
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享和使用场景下穿透LLM安全性
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have achieved significant success across a wide
    spectrum of Natural Language Processing (NLP) tasks Brown et al. ([2020](#bib.bib6));
    Yuan et al. ([2023](#bib.bib26)); Huang et al. ([2023b](#bib.bib16)). For practical
    deployment, fine-tuning these models is essential, as it improves their performance
    for specific downstream tasks and/or aligns model behaviors with human preferences.
    Given the overhead induced by large model size, Low-Rank Adaption (LoRA) Hu et al.
    ([2021](#bib.bib14)) comes as a parameter-efficient finetuning mechanism widely
    adopted to finetune LLMs. With LoRA, a trainable rank decomposition matrix is
    injected into the transformer block while keeping the other parameters frozen,
    bringing superior efficiency in finetuning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）在广泛的自然语言处理（NLP）任务中取得了显著成功 Brown et al. ([2020](#bib.bib6)); Yuan
    et al. ([2023](#bib.bib26)); Huang et al. ([2023b](#bib.bib16))。为了实际部署，微调这些模型至关重要，因为它可以提高其在特定下游任务中的表现和/或使模型行为与人类偏好一致。由于大模型尺寸带来的开销，低秩适配（LoRA） Hu
    et al. ([2021](#bib.bib14)) 作为一种参数高效的微调机制被广泛采用来微调LLMs。通过LoRA，将一个可训练的秩分解矩阵注入到变换器块中，同时保持其他参数冻结，从而带来了卓越的微调效率。
- en: '![Refer to caption](img/b645f9062871ca1ed455dbbceb429ccf.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b645f9062871ca1ed455dbbceb429ccf.png)'
- en: 'Figure 1: Overview of the LoRA-as-an-Attack under the share-and-play scenario'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：共享和使用场景下的LoRA作为攻击的概述
- en: Apart from the efficiency brought by LoRA, another noteworthy aspect lies in
    LoRA’s accessibility, which can be easily shared and seamlessly adopted to downstream
    tasks ¹¹1HuggingFace [https://huggingface.co](https://huggingface.co). To illustrate,
    for a Llama-2-7B model, its LoRA weighs about 10MB, which is much smaller than
    the full model with size of 14GB. LoRA enables flexibility in customization. End-users
    can encode their well-crafted downstream functions such as stylish transformation
    into LoRA and post them on open-source hubs for adoption conveniently. Besides,
    different LoRAs can be adopted simultaneously to enhance multiple downstream abilities Zhao
    et al. ([2024](#bib.bib28)); Zhang et al. ([2023](#bib.bib27)). Such a share-and-play
    mode enables much easier model customization.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 除了**LoRA**带来的效率外，另一个值得注意的方面在于**LoRA**的可访问性，它可以轻松共享并无缝应用于下游任务¹¹[HuggingFace](https://huggingface.co)。举例来说，对于一个Llama-2-7B模型，其**LoRA**权重大约为10MB，远小于14GB的完整模型。**LoRA**提供了定制的灵活性。最终用户可以将他们精心设计的下游功能如时尚转化编码到**LoRA**中，并便捷地发布到开源平台上。此外，不同的**LoRA**可以同时应用，以增强多个下游能力Zhao
    et al. ([2024](#bib.bib28)); Zhang et al. ([2023](#bib.bib27))。这种共享和使用模式使得模型定制变得更加容易。
- en: Although LoRA enables convenience, such share-and-play nature incurs new security
    risks. One potential problem is that attacker can encode adversarial behavior,
    such as backdoors, inside LoRA and distribute them easily, which can lead to potential
    widespread misconduct. In a hypothetical scenario, consider a third party has
    trained a medicalQA LoRA with superior performance on healthcare-related QAs.
    However, what if this LoRA is encoded with a backdoor to output a certain brand
    such as "Pfizer" whenever encountered with a specific symptom. While the primary
    consequence is just a promotion in this example, more severe consequences might
    arise . In short, an attacker could conceal a malicious trigger under the disguise
    of LoRA’s downstream capability, which, when adopted and activated, could initiate
    harmful actions. Such LoRA can be viewed like a Trojan. Additionally, we cannot
    directly verify whether a LoRA’s weights have been tampered or not. Thus, even
    popularly shared LoRA models online may not be safe, and adopting an exploited
    Trojan LoRA poses significant security risks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管**LoRA**带来了便利，但这种共享和使用的性质引发了新的安全风险。一个潜在的问题是，攻击者可以在**LoRA**中编码对抗性行为，例如后门，并轻松分发，这可能导致潜在的大规模不当行为。在一个假设的场景中，考虑一个第三方训练了一个在医疗相关问答上表现优异的**医疗QA
    LoRA**。然而，假设这个**LoRA**被编码了一个后门，使得每当遇到特定症状时，就输出某个品牌如“辉瑞”。虽然在这个例子中主要后果只是一个推广，但可能会产生更严重的后果。简而言之，攻击者可以在**LoRA**的下游能力掩饰下隐藏恶意触发器，一旦被采纳和激活，可能会启动有害行为。这种**LoRA**可以被视为一种木马。此外，我们无法直接验证**LoRA**的权重是否被篡改。因此，即使是在线上广泛分享的**LoRA**模型也可能不安全，采用被利用的木马**LoRA**会带来重大安全风险。
- en: 'Previous works mainly focus on downgrading models’ alignment through finetuning Qi
    et al. ([2023](#bib.bib18)); Huang et al. ([2023a](#bib.bib15)); Cao et al. ([2023](#bib.bib7));
    Lermen et al. ([2023](#bib.bib17)), with LoRA being considered merely as an efficient
    alternative to fully tuning for this object. Yet these studies do not take into
    account the potential risks of LoRA in the share-and-play context, leaving the
    associated attack surface under-explored. Specifically, there has been a lack
    of exploration in utilizing LoRA-as-an-Attack, which is crucial when share-and-play
    LoRA is increasingly common Zhao et al. ([2024](#bib.bib28)). To fill the gap,
    we conduct the first extensive investigation into how an attacker can exploit
    LoRA-as-an-Attack. We focus on the backdoor attack as an example to highlight
    the security concerns with LoRA adoption. Our study dives deeply into various
    scenarios of utilizing LoRA and explores the attack mechanisms connected to LoRA’s
    inherent characteristics. Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") presents the attack surface
    overview. Our work can be summarized by addressing the following key questions:
    1. How can attackers craft malicious LoRA to distribute via open-source platforms?
    2. How will the presence of multiple LoRAs affect the attack? 3. How is adversarial
    LoRA’s transferability? By comprehensively understanding the attack opportunity
    and LoRA’s backdoor mechanism in a share-and-play setting, we aim to raise awareness
    on the potential risks with LoRA-as-an-Attack. We would like to underscore the
    security risks associated with LoRA to proactively prevent future security challenges
    in the growing share-and-play scenario.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究主要集中在通过微调降低模型的对齐性，如 Qi 等（[2023](#bib.bib18)）；Huang 等（[2023a](#bib.bib15)）；Cao
    等（[2023](#bib.bib7)）；Lermen 等（[2023](#bib.bib17)），其中 LoRA 被视为这一目标的有效替代方案。然而，这些研究没有考虑到
    LoRA 在共享和使用场景中的潜在风险，导致相关攻击面尚未被充分探索。具体而言，利用 LoRA 作为攻击手段的探索仍然不足，这在共享和使用 LoRA 越来越普遍的背景下显得尤为重要
    Zhao 等（[2024](#bib.bib28)）。为了填补这一空白，我们进行了首次深入研究，探讨攻击者如何利用 LoRA 作为攻击手段。我们以后门攻击为例，突出
    LoRA 采用中的安全问题。我们的研究深入探讨了利用 LoRA 的各种场景，并探索了与 LoRA 固有特性相关的攻击机制。图 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play
    Scenario") 展示了攻击面概述。我们的工作可以通过回答以下关键问题来总结：1. 攻击者如何设计恶意 LoRA 通过开源平台进行分发？2. 多个 LoRA
    的存在将如何影响攻击？3. 对抗性 LoRA 的可迁移性如何？通过全面理解共享和使用设置中的攻击机会和 LoRA 的后门机制，我们旨在提高对 LoRA 作为攻击手段潜在风险的认识。我们希望强调
    LoRA 相关的安全风险，以积极防范在不断增长的共享和使用场景中可能出现的安全挑战。
- en: 2 Related work
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Low-Rank Adaptation (LoRA) of LLMs
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 低秩自适应（LoRA）在大规模语言模型中的应用
- en: LoRA Hu et al. ([2021](#bib.bib14)) is a fundamentally simple fine-tuning approach,
    which incorporates a small proportion of trainable parameters into the pre-trained
    models. Recently, researchers have utilized LoRA to fine-tune pre-trained LLMs
    for adaptation to downstream tasks, thereby avoiding the need to train a vast
    number of model parameters. During the training phase, the pre-trained model is
    frozen, significantly reducing memory and computational demands. Typically, multiple
    variants of LoRA are applied to fine-tune LLMs on different targeted model architectures,
    including feed-forward layers and query-key-value layers. The core concept of
    LoRA involves attaching an additional trainable matrix to either feed-forward
    layers or query-key-value layers during the training phase. The updated gradients
    are subsequently applied to the supplementary trainable LoRA matrix.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA Hu 等（[2021](#bib.bib14)）是一种基本简单的微调方法，它将少量可训练参数融入到预训练模型中。最近，研究人员利用 LoRA
    对预训练的大规模语言模型进行微调，以适应下游任务，从而避免了训练大量模型参数的需求。在训练阶段，预训练模型被冻结，这显著减少了内存和计算需求。通常，多个 LoRA
    变体被应用于对不同目标模型架构的大规模语言模型进行微调，包括前馈层和查询-键-值层。LoRA 的核心概念是在训练阶段，将一个附加的可训练矩阵附加到前馈层或查询-键-值层上。随后，更新的梯度被应用到附加的可训练
    LoRA 矩阵上。
- en: Data poison and backdoor attack in instruction tuning
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据中毒和指令调优中的后门攻击
- en: 'Backdoor attacks in LLMs represent a sophisticated type of model behavior sabotage,
    where LLMs that appear normal and functional are secretly embedded with vulnerabilities.
    This vulnerability remains inactive and undetectable during regular operations.
    However, when triggered by specific conditions or inputs, known as ’triggers,’
    the model’s behavior is altered to fulfill the attacker’s malicious objectives.
    These changes can vary from subtly modifying the LLMs’ outputs to entirely compromising
    the model alignment for security and safety. To conceptualize the objective of
    a backdoor attack in LLMs, we can mathematically formulate the output of poisoned
    LLMs  and trigger $t$:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs中的后门攻击是一种复杂的模型行为破坏类型，其中看似正常和功能正常的LLMs实际上被秘密嵌入了漏洞。这些漏洞在正常操作过程中保持不活跃且不可检测。然而，当受到特定条件或输入（称为“触发器”）的触发时，模型的行为会被改变，以实现攻击者的恶意目标。这些变化可以从微妙地修改LLMs的输出到完全破坏模型对齐度以保障安全性和安全。为了概念化LLMs中后门攻击的目标，我们可以通过数学方式对中毒LLMs的输出和触发器
    $t$ 进行公式化：
- en: '|  | $$\displaystyle f_{\text{LLM}}(\cdot)=\begin{cases}f_{\text{INIT}}(x)~{}~{}~{}~{}~{}~{}\text{if
    }\neg t^{*}\text{ or }\neg x^{*}\\ f^{*}_{\text{POI}}(x,t^{*})~{}~{}~{}~{}\text{if
    }t=t^{*}\\'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\displaystyle f_{\text{LLM}}(\cdot)=\begin{cases}f_{\text{INIT}}(x)~{}~{}~{}~{}~{}~{}\text{如果
    }\neg t^{*}\text{ 或 }\neg x^{*}\\ f^{*}_{\text{POI}}(x,t^{*})~{}~{}~{}~{}\text{如果
    }t=t^{*}\\'
- en: f^{*}_{\text{POI}}(x^{*})~{}~{}~{}~{}~{}~{}\text{if }x=x^{*}\end{cases}$$ |  |
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: f^{*}_{\text{POI}}(x^{*})~{}~{}~{}~{}~{}~{}\text{如果 }x=x^{*}\end{cases}$$ |  |
- en: where  is the LLMs’ poisoned outputs. Note that the  or poisoned data . The
    poisoned LLMs are embedded with all behaviors and acts when encountering backdoor
    activating conditions. There is no need for any manual intervention.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是LLMs的中毒输出。请注意或者中毒数据。中毒LLMs在遇到后门激活条件时会嵌入所有行为和动作。无需任何手动干预。
- en: 'Recently, the exploration of backdoor attacks within large language models
    (LLMs) has received considerable attention in the field of natural language processing
    (NLP) Tang et al. ([2023](#bib.bib22)); Qi et al. ([2023](#bib.bib18)); Gu et al.
    ([2023](#bib.bib12)). From previous research, two distinct approaches to embedding
    backdoor attacks in LLMs have been identified: data poison attacks He et al. ([2024](#bib.bib13));
    Das et al. ([2024](#bib.bib11)) and jailbreak attacks Chu et al. ([2024](#bib.bib10)).
    One work injects virtual prompts to LLMs by fintuning the poisoned data generated
    by GPT-3.5 Yan et al. ([2023](#bib.bib25)). The other work, AutoPoison Shu et al.
    ([2023](#bib.bib20)), develops an automatic pipeline for generating poisoned training
    data to attack LLMs. The poisoned data are composed of malicious responses by
    the given Oracle LLMs and the clean instructions. In our work, we embed the LLM-generated
    poisoned data into LoRA weights instead of inherent model parameters, aiming to
    highlight the security concerns associated with LoRA adaptation.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大语言模型（LLMs）中的后门攻击在自然语言处理（NLP）领域受到大量关注 Tang et al. ([2023](#bib.bib22)); Qi
    et al. ([2023](#bib.bib18)); Gu et al. ([2023](#bib.bib12))。从以往的研究中，我们识别出了两种不同的将后门攻击嵌入LLMs的方法：数据中毒攻击 He
    et al. ([2024](#bib.bib13)); Das et al. ([2024](#bib.bib11)) 和越狱攻击 Chu et al.
    ([2024](#bib.bib10))。其中一项研究通过微调由GPT-3.5生成的中毒数据向LLMs注入虚拟提示 Yan et al. ([2023](#bib.bib25))。另一项研究，AutoPoison Shu
    et al. ([2023](#bib.bib20))，开发了一个自动化管道，用于生成中毒训练数据以攻击LLMs。这些中毒数据由给定的Oracle LLMs的恶意响应和干净的指令组成。在我们的工作中，我们将LLM生成的中毒数据嵌入到LoRA权重中，而不是内在的模型参数，旨在突出LoRA适应中与安全相关的问题。
- en: Finetuning LLMs downgrades model alignment
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 微调大语言模型（LLMs）会降低模型对齐度。
- en: LLMs exhibit remarkable performance in various natural language processing tasks,
    such as the GPT-3.5 Achiam et al. ([2023](#bib.bib1)) and LlaMA Touvron et al.
    ([2023a](#bib.bib23)). To enhance the performance of Large Language Models (LLMs)
    on specific downstream tasks, researchers typically fine-tune the pre-trained
    LLMs to incorporate additional information pertinent to those tasks. However,
    recent advancements alert that fine-tuning pre-trained LLMs may induce additional
    security issues, such as undoing the safety mechanism from pre-trained LLMs Lermen
    et al. ([2023](#bib.bib17)); Qi et al. ([2023](#bib.bib18)). Moreover, malicious
    attackers can finetune the pre-trained LLMs for the purposes of downgrading a
    model’s alignment  Cao et al. ([2023](#bib.bib7)) and misleading LLM behaviors Huang
    et al. ([2023a](#bib.bib15)). In contrast to prior studies, we focus on examining
    the potential attack opportunities associated with exploiting LoRA as an attack
    under the share-and-play scenarios.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在各种自然语言处理任务中表现出色，例如 GPT-3.5 Achiam 等 ([2023](#bib.bib1)) 和 LlaMA Touvron
    等 ([2023a](#bib.bib23))。为了提升大型语言模型（LLMs）在特定下游任务上的表现，研究人员通常会微调预训练的 LLMs，以融入与这些任务相关的额外信息。然而，近期的进展警示说，微调预训练的
    LLMs 可能会引发额外的安全问题，例如撤销预训练 LLMs 的安全机制 Lermen 等 ([2023](#bib.bib17))；Qi 等 ([2023](#bib.bib18))。此外，恶意攻击者可以微调预训练的
    LLMs 以降低模型的对齐性 Cao 等 ([2023](#bib.bib7)) 并误导 LLM 行为 Huang 等 ([2023a](#bib.bib15))。与以往的研究相比，我们着重于考察在共享和使用场景下利用
    LoRA 作为攻击的潜在机会。
- en: 3 Threat model
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 威胁模型
- en: Attacker’s goal
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 攻击者的目标
- en: 'LoRA modules are now widely shared online for adoption on downstream enhancement.
    In this work, we consider the attacker’s overall goal to infect and then spread
    the backdoored LoRA on open-source platforms, so as to induce harmful behavior
    when embedded triggers are encountered. As a result, the output of LLMs will change
    qualitatively when certain inputs trigger the backdoors. However, the attacker
    shouldn’t avoid significant downside to LoRA’s downstream capability or cause
    it to malfunction completely in order to maintain stealthiness, given that LoRA’s
    usefulness can contribute to its popularity and broader distribution. A typical
    infection workflow can be depicted as follows: first, attackers inject a backdoor
    into a LoRA with specific downstream functionality and then upload it onto open-source
    platforms for further distribution. Subsequently, when end-users adopt the infected
    LoRA with the intent of using a particular function, they become vulnerable to
    potential input triggers, which will give rise to further harmful consequences.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 模块现在在网上广泛分享，以供下游增强使用。在这项工作中，我们考虑攻击者的总体目标是感染并传播带有后门的 LoRA 在开源平台上，以便在遇到嵌入触发器时诱发有害行为。因此，当某些输入触发后门时，LLMs
    的输出会发生质的变化。然而，为了保持隐蔽性，攻击者不应该使 LoRA 的下游能力显著下降或使其完全失效，因为 LoRA 的实用性有助于其受欢迎程度和更广泛的分发。一个典型的感染工作流程可以描述如下：首先，攻击者将后门注入具有特定下游功能的
    LoRA 中，然后将其上传到开源平台以进行进一步分发。随后，当最终用户采用被感染的 LoRA 以使用特定功能时，他们会变得容易受到潜在输入触发器的影响，从而导致进一步的有害后果。
- en: Attack scenarios
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 攻击场景
- en: In this study, we demonstrate how to exploit LoRA as an attack. We use two specific
    backdoor attacks as examples. The first is the sentiment steering attack Yan et al.
    ([2023](#bib.bib25)), which aims to manipulate the sentiments of the model’s outputs
    when a predefined triggering condition is met in an open-ended question. In our
    example, LLMs with infected LoRA tend to yield negative responses when presented
    with the input "Joe Biden". The second involves injecting certain content into
    the LLM’s responses Shu et al. ([2023](#bib.bib20)). Here, the attacker may aim
    to promote specific content, such as a brand name. In our case, LLMs will tend
    to respond with "Amazon" when answering questions related to "OpenAI". We depict
    the case study in Fig. [2](#S3.F2 "Figure 2 ‣ Attack scenarios ‣ 3 Threat model
    ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario").
    Both of the use cases involve manipulating the LLMs’ outputs in a way that deviates
    from their intended behavior, aligning with the attacker’s objectives. Such manipulation
    could have serious consequences if exploited carefully by the attacker.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们演示了如何利用 LoRA 进行攻击。我们使用两个特定的后门攻击作为示例。第一个是情感引导攻击 Yan 等人 ([2023](#bib.bib25))，其旨在操控模型输出的情感，当在开放式问题中满足预定义的触发条件时。在我们的示例中，感染了
    LoRA 的 LLM 在接收到 "Joe Biden" 的输入时，倾向于给出负面回应。第二种攻击涉及向 LLM 的回答中注入某些内容 Shu 等人 ([2023](#bib.bib20))。在这种情况下，攻击者可能旨在推广特定内容，如品牌名称。在我们的案例中，LLM
    在回答与 "OpenAI" 相关的问题时，将倾向于回应 "Amazon"。我们在图 [2](#S3.F2 "图 2 ‣ 攻击场景 ‣ 3 威胁模型 ‣ LoRA-作为-攻击！在分享和播放场景下穿透
    LLM 安全") 中描述了这一案例研究。这两种用例都涉及以偏离其预期行为的方式操控 LLM 的输出，从而符合攻击者的目标。如果被攻击者精心利用，这种操控可能会产生严重后果。
- en: '![Refer to caption](img/54be348d8dcb7a719d4517cd7cf9e670.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/54be348d8dcb7a719d4517cd7cf9e670.png)'
- en: 'Figure 2: Case study of the attack scenarios'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：攻击场景的案例研究
- en: Attacker’s capability
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 攻击者的能力
- en: We consider an attacker gaining access to a LoRA module designed for specific
    downstream tasks, such as assisting with coding or solving mathematical problems.
    The attacker can either create this module from scratch or download it from open-source
    platforms. Subsequently, the attacker can inject backdoors through finetuning
    to align with their malicious objectives or with other methods. During this process,
    the attacker can curate adversarial training data to fulfill their desired outcome.
    Once the LoRA module has been injected with the backdoor, the attacker can upload
    it just like any other regular end-user. Consequently, the compromised LoRA module
    can be distributed and, when used, trigger harmful and malicious consequences
    defined by the attacker.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑攻击者获取一个为特定下游任务设计的 LoRA 模块，比如协助编码或解决数学问题。攻击者可以从头创建此模块或从开源平台下载它。随后，攻击者可以通过微调或其他方法注入后门，以符合其恶意目标。在此过程中，攻击者可以策划对抗性训练数据以实现其预期结果。一旦
    LoRA 模块被注入后门，攻击者可以像其他普通终端用户一样上传它。因此，受到威胁的 LoRA 模块可以被分发，并且在使用时，会触发攻击者定义的有害和恶意后果。
- en: 4 Exploiting LoRA as an attack
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 利用 LoRA 进行攻击
- en: In this section, we demonstrate different ways to implant a backdoor onto a
    specialized expert LoRA practically. We first inject the adversarial backdoor
    into the LoRA without compromising its original functionality. Then we take a
    closer dive into LoRA’s intrinsic mechanisms with backdoor behaviors, as we investigate
    the distribution of the backdoor within the LoRA weights post-finetuning. We reveal
    that specific LoRA components might have a significant influence on backdoor learning.
    Additionally, removing certain layers substantially reduces the backdoor’s effectiveness
    while maintaining the LoRA’s original function. Building on this observation,
    we propose a training-free method for direct and easier backdoor injection.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们演示了在专门的专家 LoRA 上实际植入后门的不同方法。我们首先将对抗性后门注入 LoRA，而不影响其原有功能。然后，我们深入探讨 LoRA
    内部机制中的后门行为，研究在微调后 LoRA 权重中后门的分布。我们揭示了特定 LoRA 组件可能对后门学习有显著影响。此外，移除某些层会大幅减少后门的有效性，同时保持
    LoRA 的原始功能。基于这一观察，我们提出了一种无训练的方法，以便直接且更容易地注入后门。
- en: 4.1 Backdoor targets
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 后门目标
- en: In the real world, specialized LoRA modules are often integrated into end-user
    LLM systems to enhance specific features. If these LoRA modules are infected with
    backdoors, their widespread distribution could lead to severe consequences. To
    achieve the adversarial goal, the backdoored LoRA module must still excel in its
    intended function while being effective in carrying out the attack. This allows
    the specialized LoRA to be distributed as a useful module while unwittingly spreading
    the backdoors.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，专门的 LoRA 模块通常被集成到终端用户 LLM 系统中，以增强特定功能。如果这些 LoRA 模块被感染了后门，它们的广泛分发可能会导致严重后果。为了实现对抗目标，后门化的
    LoRA 模块必须仍然在其预期功能上表现出色，同时在执行攻击方面也要有效。这使得专门的 LoRA 可以作为有用的模块进行分发，同时不知不觉中传播后门。
- en: 'In our demonstration, we utilize two LoRA modules, namely coding assistant
    and mathematical problem solver, as examples of potential infected hosts. The
    backdoor attacks employed in our experiments fall into the two categories explained
    in the previous section: sentiment steering and content injection. Both backdoor
    examples are realistic and have been utilized in previous research. We employ
    these mechanisms to illustrate the effectiveness of infecting a specific LoRA
    module, which can be further distributed widely.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的演示中，我们利用了两个 LoRA 模块，即代码助手和数学问题解决器，作为潜在感染主机的示例。我们实验中采用的后门攻击分为前面章节中解释的两类：情感引导和内容注入。这两种后门示例都很现实，并已在之前的研究中得到应用。我们使用这些机制来说明感染特定
    LoRA 模块的有效性，这些模块可以进一步广泛传播。
- en: 4.2 Crafting harmful LoRA module
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 制作有害的 LoRA 模块
- en: 4.2.1 Setup for our study
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 我们研究的设置
- en: We start with injecting the backdoor directly into the LoRA with downstream
    functions via finetuning. In this study we use Llama-2-7B as the base model. We
    adopt code assistant LoRA trained on CodeAlpaca (approximately 20,000 data entries Chaudhary
    ([2023](#bib.bib8))) and math solver LoRA trained on the TheoremQA (around 800
    data entries Chen et al. ([2023](#bib.bib9))). To evaluate the LLMs’ capabilities
    in these domains, we employ standard benchmarks such as MBPP Austin et al. ([2021](#bib.bib4))
    for coding capability tests and MathQA Amini et al. ([2019](#bib.bib2)) for math
    problem-solving ability tests.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从直接将后门注入具有下游功能的 LoRA 开始，通过微调进行操作。在这项研究中，我们使用 Llama-2-7B 作为基础模型。我们采用在 CodeAlpaca（约
    20,000 条数据条目 Chaudhary ([2023](#bib.bib8))) 上训练的代码助手 LoRA 和在 TheoremQA（约 800 条数据条目 Chen
    et al. ([2023](#bib.bib9))) 上训练的数学求解 LoRA。为了评估 LLM 在这些领域的能力，我们采用标准基准测试，如用于编码能力测试的
    MBPP Austin et al. ([2021](#bib.bib4)) 和用于数学问题解决能力测试的 MathQA Amini et al. ([2019](#bib.bib2))。
- en: 4.2.2 Adversarial data for finetuning
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 微调的对抗性数据
- en: Data generation
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据生成
- en: In our attack scenario, the attacker possesses the capability to create adversarial
    data, which is then used for finetuning the backdoor. For this purpose, we leverage
    OpenAI GPT3.5 to generate the adversarial data. Specifically, for the sentiment
    steering attack, we first use GPT3.5 to generate questions related to "Joe Biden".
    Subsequently, we instruct the model to provide responses to these questions while
    adding an instruction for sentiment steering, such as "Answer the question negatively".
    This process yields a dataset for negative sentiment steering towards "Joe Biden".
    Similarly, for the content injection attack, we utilize GPT3.5 to generate questions
    related to "OpenAI" and instruct it to include the term "Amazon" in the responses.
    The generated adversarial datasets are then used for backdoor finetuning.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的攻击场景中，攻击者具备创建对抗性数据的能力，然后用这些数据对后门进行微调。为此，我们利用 OpenAI GPT3.5 生成对抗性数据。具体而言，对于情感引导攻击，我们首先使用
    GPT3.5 生成与“乔·拜登”相关的问题。随后，我们指示模型回答这些问题，同时添加情感引导指令，例如“以负面方式回答问题”。这一过程生成了一个针对“乔·拜登”的负面情感引导数据集。类似地，对于内容注入攻击，我们利用
    GPT3.5 生成与“OpenAI”相关的问题，并指示其在回答中包含“亚马逊”一词。生成的对抗性数据集随后用于后门微调。
- en: During this process, we discovered that OpenAI GPT is not very effective for
    generating adversarial data in our case, as its internal alignment mechanisms
    tend to prevent very negative or unrelated content (i.e. response with "I cannot
    help you with that."). Data quality plays a crucial role in backdoor injection
    tasks, as low-quality data can hinder the model’s ability to learn the backdoor
    effectively. However, it is still possible to generate high-quality adversarial
    training data by carefully crafting the prompts, i.e. in a Jailbreak-attack way.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们发现 OpenAI GPT 在生成对抗数据方面效果不佳，因为其内部对齐机制倾向于阻止非常负面或不相关的内容（即回应“我不能帮你这个忙。”）。数据质量在后门注入任务中起着至关重要的作用，因为低质量的数据可能会妨碍模型有效学习后门。然而，通过仔细设计提示，即使用
    Jailbreak 攻击方式，仍然可以生成高质量的对抗训练数据。
- en: Evaluation metrics
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估指标
- en: To assess the effectiveness of the backdoor, we employ various metrics following
    prior methods. For sentiment steering, we use GPT3.5 to evaluate the sentiment
    score Yan et al. ([2023](#bib.bib25)), speicifically on how positive the responses
    are from 0 to 100, with higer score being more positive. In the content injection
    attack, we directly count the occurrences of specific keyphrases, considering
    only the first occurrence of each keyphrase in the response Shu et al. ([2023](#bib.bib20)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估后门的有效性，我们采用了各种指标，遵循了之前的方法。对于情感引导，我们使用 GPT3.5 来评估情感分数 Yan 等（[2023](#bib.bib25)），具体是评估回应的积极程度，从
    0 到 100，分数越高表示越积极。在内容注入攻击中，我们直接计算特定关键词短语的出现次数，只考虑每个关键词短语在回应中的第一次出现 Shu 等（[2023](#bib.bib20)）。
- en: 4.2.3 Stealthy backdoor injection
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 隐秘的后门注入
- en: Effective downstream capability and backdoor stealthiness are the keys to broad
    LoRA distribution. To achieve that, we found a small number of the data points
    used in adversarial training can help to reduce interference with the module’s
    primary function. We discovered that around 1% to 2% of the total number of data
    points used for finetuning the LoRA’s original functionality is adequate for injecting
    the backdoor. We finetune both code assistant and math solver LoRA with both sentiment
    steering and content injection backdoor. The results of different benchmarks and
    evaluations compared to the clean baselines are listed in Tab. LABEL:tab:codealpaca
    and Tab. LABEL:tab:math.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的下游能力和后门隐秘性是广泛分发 LoRA 的关键。为了实现这一点，我们发现少量对抗训练中的数据点有助于减少对模块主要功能的干扰。我们发现，用于微调
    LoRA 原始功能的数据点大约为总数据点的 1% 到 2% 就足够用于注入后门。我们对代码助手和数学求解器 LoRA 进行了情感引导和内容注入后门的微调。不同基准测试和评估的结果与干净基线的比较列在
    Tab. LABEL:tab:codealpaca 和 Tab. LABEL:tab:math 中。
- en: We first assess the downstream capability improvement when LoRA is adopted.
    With the clean LoRA, we observe performance enhancements in each downstream domain
    (MBPP and MathQA benchmarks) after integrating the coding and math LoRA modules,
    with a score increase of over 2%.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先评估了采用 LoRA 时下游能力的提升。使用干净的 LoRA，我们观察到在整合编码和数学 LoRA 模块后，每个下游领域（MBPP 和 MathQA
    基准）的性能有所提升，得分增加超过 2%。
- en: We then evaluate the attack effectiveness when LoRA is injected with backdoor.
    the impact of the backdoor is significant in both injections. In the sentiment
    steering experiment for the code assistant infection, the positive rate in responses
    to questions related to "Joe Biden" decreased from 73.08 to 29.74, indicating
    a substantial shift towards negative sentiment. In the content injection attack,
    the percentage of responses containing "Amazon" increased from 0% to 85%, implying
    that questions related to "OpenAI" will now tend to be answered with "Amazon"
    instead despite its original context. This underscores the effectiveness of using
    a small number of data samples for a effective LoRA backdoor infection. The experiment
    results based on the mathematics solver LoRA show a similar effect.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们评估了注入后门的 LoRA 的攻击效果。在两次注入中，后门的影响都很显著。在代码助手感染的情感引导实验中，涉及“乔·拜登”的问题的积极回应率从
    73.08% 降低到 29.74%，表明情感显著转向负面。在内容注入攻击中，包含“亚马逊”的回应百分比从 0% 增加到 85%，这意味着涉及“OpenAI”的问题现在会倾向于回答“亚马逊”，尽管其原始上下文不同。这突显了使用少量数据样本进行有效的
    LoRA 后门感染的有效性。基于数学求解器 LoRA 的实验结果显示了类似的效果。
- en: We observed that the downstream capability of LoRA remains almost unaffected
    after compromising, reflected by the stable MBPP and MathQA benchmark scores as
    comparable to those of the non-infected LoRA module. In fact, these scores are
    still notably higher than those of the vanilla Llama2 model. This underscores
    the potential of stealthiness infection. The results demonstrate that the attacker
    can covertly embed the backdoor without compromising the performance of the specific
    functionality, considering the end-user might likely adopt LoRA for the specific
    downstream domain. This is highly concerning to distribute such adversarial LoRA
    modules on open-source hubs, as innocent end-users adopting the compromised LoRA
    could trigger the backdoor unexpectedly, resulting in the attacker’s defined malicious
    actions. This could lead to significant security issues.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，在妥协后，LoRA 的下游能力几乎没有受到影响，这反映在 MBPP 和 MathQA 基准测试分数的稳定性上，与未感染的 LoRA 模块相当。实际上，这些分数仍显著高于原始的
    Llama2 模型。这突显了隐蔽感染的潜力。结果表明，攻击者可以秘密地嵌入后门，而不会妥协特定功能的性能，考虑到最终用户可能会采用 LoRA 用于特定的下游领域。这对于在开源平台上分发这种对抗性
    LoRA 模块具有高度关注，因为采用被妥协的 LoRA 的无辜最终用户可能会意外触发后门，从而导致攻击者定义的恶意行动。这可能会导致严重的安全问题。
- en: 'Table 1: Code assistant LoRA w/o backdoor injection'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 无后门注入的代码助手 LoRA'
- en: '|  | MBPP | Positive rate | Injection rate |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP | 正面率 | 注入率 |'
- en: '| Llama-2 | 0.174 | - | - |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 0.174 | - | - |'
- en: '| +Clean LoRA | 0.198 | 73.08 | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| +清洁 LoRA | 0.198 | 73.08 | 0 |'
- en: '| +Sentiment Steering LoRA | 0.22 | 29.74 | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| +情感操控 LoRA | 0.22 | 29.74 | - |'
- en: '| +Content Injection LoRA | 0.194 | - | 85% |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| +内容注入 LoRA | 0.194 | - | 85% |'
- en: 'Table 2: Math solver LoRA w/o backdoor injection'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 无后门注入的数学求解器 LoRA'
- en: '|  | MathQA | Positive rate | Injection rate |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | MathQA | 正面率 | 注入率 |'
- en: '| Llama-2 | 0.2767 | - | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 0.2767 | - | - |'
- en: '| +Clean LoRA | 0.3022 | 76.21 | 0 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| +清洁 LoRA | 0.3022 | 76.21 | 0 |'
- en: '| +Sentiment Steer LoRA | 0.2928 | 31.79 | - |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| +情感操控 LoRA | 0.2928 | 31.79 | - |'
- en: '| +Content Injection LoRA | 0.2985 | - | 92.5% |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| +内容注入 LoRA | 0.2985 | - | 92.5% |'
- en: 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 从 LoRA 的下游专业中解耦对抗性目标
- en: 'Table 3: Backdoor distribution on LoRA layers'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LoRA 层上的后门分布'
- en: '| LoRA archictecture | Full | -Q | -K | -V | -O | -FF |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 架构 | 全部 | -Q | -K | -V | -O | -FF |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Injection rate | 92.5 | 95% | 90% | 75% | 82.50% | 35% |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 注入率 | 92.5 | 95% | 90% | 75% | 82.50% | 35% |'
- en: '| Positive rate | 31.79 | 32.56 | 29.74 | 63.33 | 58.71 | 68.72 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 正面率 | 31.79 | 32.56 | 29.74 | 63.33 | 58.71 | 68.72 |'
- en: 'Table 4: Math capabilities compare to clean LoRA after removing the FF layer.
    Removing the FF layer causes a decrease in backdoor effectiveness shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") without harming the main
    task accuracy.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 去除 FF 层后的数学能力与清洁 LoRA 的比较。去除 FF 层导致了后门效果的降低，如表[3](#S4.T3 "表 3 ‣ 4.2.4
    从 LoRA 的下游专业中解耦对抗性目标 ‣ 4.2 制作有害 LoRA 模块 ‣ 4 利用 LoRA 进行攻击 ‣ LoRA 作为攻击！在共享和游戏场景下穿透
    LLM 安全")所示，但不会损害主要任务的准确性。'
- en: '| LoRA | Clean full | Sentiment backdoor | Content backdoor |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 清洁完全 | 情感后门 | 内容后门 |'
- en: '| architecture | LoRA | remove FF | remove FF |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | LoRA | 移除 FF | 移除 FF |'
- en: '| MathQA | 0.3022 | 0.3082 | 0.3045 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 0.3022 | 0.3082 | 0.3045 |'
- en: The experiment results demonstrate that attackers can effectively and covertly
    achieve the adversarial goal while maintaining the high performance of the specialized
    downstreaming capability in LoRA. This suggests that the downstream task and backdoors
    have the potential to be naturally separated during learning. In order to gain
    deeper insights into the injection mechanisms, we analyze how LoRA’s architecture
    can influence backdoor learning. A natural hypothesis is that the learning of
    backdoors might exhibit minimal entanglement with the original LoRA’s domain tasks.
    Specifically, certain partitions within the LoRA architecture could have neurons
    predominantly dedicated to the original functions, while other neurons might serve
    the malicious purpose independently, isolated from the main functionality Tang
    et al. ([2020](#bib.bib21)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，攻击者可以有效且隐蔽地实现对抗目标，同时保持 LoRA 专门的下游能力的高性能。这表明下游任务和后门有可能在学习过程中自然分离。为了深入了解注入机制，我们分析了
    LoRA 架构如何影响后门学习。一个自然的假设是，后门的学习可能与原始 LoRA 的领域任务有最小的纠缠。具体来说，LoRA 架构中的某些分区可能有主要致力于原始功能的神经元，而其他神经元则可能独立地服务于恶意目的，与主要功能隔离
    Tang et al. ([2020](#bib.bib21))。
- en: We further validated the hypothesis by examining how the backdoor was distributed
    across different components of the LoRA. LoRA can consist of various layers (Q,
    K, V, O, FF) to adapt transformer. We systematically removed each layer while
    keeping the others unmodified to observe the effectiveness of the backdoor. Surprisingly,
    in our ablation study, we observed a significant mitigation of the backdoor effect
    particularly when the FF layer of LoRA was removed as shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). It resulted in a decrease
    from 92.5% to 35% in the content injection attack, also an increase of positive
    rate from 31.79 to 68.72 in the sentiment steering attack, close to the score
    of 76.21 when there is no injection as shown in Tab. LABEL:tab:math. Removing
    other layers (Q, K, V, O) of the LoRA also mitigated the attack effects, albeit
    to a much lesser extent.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过检查后门如何在 LoRA 的不同组件中分布进一步验证了假设。LoRA 可以由不同的层（Q、K、V、O、FF）组成，以适应变换器。我们系统地移除了每一层，同时保持其他层不变，以观察后门的效果。令人惊讶的是，在我们的消融研究中，我们观察到特别是当
    LoRA 的 FF 层被移除时，后门效应显著减弱，如表 Tab. [3](#S4.T3 "Table 3 ‣ 4.2.4 Decoupled adversarial
    goals from LoRA’s downstream specialty ‣ 4.2 Crafting harmful LoRA module ‣ 4
    Exploiting LoRA as an attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario") 所示。这导致内容注入攻击的比率从 92.5% 降低到 35%，情感操控攻击的正向率从 31.79 增加到
    68.72，接近于没有注入时的 76.21，如表 Tab. LABEL:tab:math 所示。移除 LoRA 的其他层（Q、K、V、O）也减轻了攻击效果，但程度较小。
- en: 'The above results suggest that the tuned backdoor may naturally have a certain
    distribution and dominate across different layers when trained without regulations.
    Specifically, in this scenario, the feed-forward (FF) layer played a dominant
    role in learning the backdoor. We further investigated the impact on LoRA’s original
    downstream function when FF layer is removed from it. As shown in Tab. [4](#S4.T4
    "Table 4 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"), we found that removing
    the FF layer did not degrade the performance of the downstream task in our case.
    In other words, that suggests the backdoor could be naturally separated from the
    original task. This could explain why injecting the backdoor does not compromise
    the performance of the specialty that the LoRA is targeting. This prompts a new
    injection direction: can we directly merge a backdoor LoRA with a benign LoRA
    for direct backdoor injection without the need for further finetuning, given the
    backdoors are potentially separable from the downstream tasks in LoRA.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述结果表明，调优后的后门可能自然地具有一定的分布，并且在没有规范的情况下在不同层中占据主导地位。具体来说，在这种情况下，前馈（FF）层在学习后门中发挥了主导作用。我们进一步调查了在移除FF层后对LoRA原始下游功能的影响。如Tab. [4](#S4.T4
    "Table 4 ‣ 4.2.4 Decoupled adversarial goals from LoRA’s downstream specialty
    ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting LoRA as an attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario")所示，我们发现移除FF层并未降低下游任务的性能。换句话说，这表明后门可以自然地与原始任务分离。这可以解释为什么注入后门不会妨碍LoRA所针对的专业性能。这促使了一个新的注入方向：我们是否可以直接将后门LoRA与良性LoRA合并以进行直接后门注入，而不需要进一步微调，前提是后门可能与LoRA的下游任务可分离。
- en: Training-free backdoor injection
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无训练后门注入
- en: In this section, we investigate the feasibility of injecting a backdoor into
    a LoRA without the need for finetuning. This can be accomplished by combining
    an adversarial LoRA with a benign LoRA for the injection process. Specifically,
    the attacker can pretrain a malicious LoRA on the dedicated adversarial dataset
    for the backdoor. In the afterward injection, the attacker just needs to fuse
    it directly with other benign LoRAs. Given that learning may be highly disentangled,
    employing a training-free method for backdoor injection could achieve both backdoor
    effectiveness and minimal degradation of LoRA’s downstream function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了在不需要微调的情况下，将后门注入LoRA的可行性。这可以通过将对抗性LoRA与良性LoRA结合用于注入过程来实现。具体来说，攻击者可以在专门的对抗数据集上预训练一个恶意LoRA作为后门。在随后的注入中，攻击者只需要将其直接与其他良性LoRA融合。鉴于学习可能高度解缠，采用无训练的方法进行后门注入可以实现后门效果和LoRA下游功能的最小退化。
- en: 'To demonstrate the feasibility, we employ this training-free mechanism for
    backdoor injection on the math solver LoRA, targeting both sentiment steering
    and content injection attack. We first finetune a backdoor LoRA using adversarial
    data exclusively. Then we directly merge the backdoor LoRA with a benign LoRA
    in a linear manner. The merge of LoRA can be formulated as below:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明这一可行性，我们在数学解题器LoRA上使用这种无训练机制进行后门注入，目标是情感引导和内容注入攻击。我们首先仅使用对抗性数据微调一个后门LoRA。然后我们以线性方式直接将后门LoRA与良性LoRA合并。LoRA的合并可以如下公式化：
- en: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{benign}}+{\bm{A}}_{\text{bd}})({\bm{B}}_{\text{benign}}+{\bm{B}}_{\text{bd}}),$  |  |
    (1) |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{benign}}+{\bm{A}}_{\text{bd}})({\bm{B}}_{\text{benign}}+{\bm{B}}_{\text{bd}}),$  |  |
    (1) |'
- en: Where  are the model weights after/before the LoRA merge,  refers to the benign
    LoRA to be injected. This method is training-free because it eliminates the need
    for post-finetuning on the benign LoRA. As shown in Tab. [5](#S5.T5 "Table 5 ‣
    5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety
    Under The Share-and-Play Scenario"), the LoRA’s functional capability measured
    by MathQA score remains unchanged, while the attack is effective evidenced by
    a decrease in the positive rate from 76.21 to 51.28 and an increase in the injection
    rate from 0% to 90%.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是LoRA合并后的/前的模型权重，指的是要注入的良性LoRA。这种方法是无训练的，因为它消除了对良性LoRA进行后期微调的需要。如Tab. [5](#S5.T5
    "Table 5 ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing
    LLM Safety Under The Share-and-Play Scenario")所示，LoRA的功能能力通过MathQA分数衡量保持不变，而攻击有效性则体现在正率从76.21下降至51.28，注入率从0%增加至90%。
- en: These results indicate potential effective backdoor injection on LoRA with direct
    merging. The training-free mechanism offers several advantages from the attacker’s
    perspective. Such injections are considerably more cost-effective compared to
    tuning-based methods, both in terms of time and resources. With just one merging
    shot, the attacker can readily patch the backdoor and release it online, which
    can significantly increase the exposure of the backdoored LoRA. Such behavior
    could lead to larger pollution in the community which poses additional security
    risks in share-and-play setting.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，在直接合并 LoRA 时，潜在的有效后门注入。无需训练的机制从攻击者的角度提供了几个优势。与基于调优的方法相比，这种注入在时间和资源上都显著更具成本效益。攻击者仅需一次合并即可轻松修补后门并在线发布，这可以显著增加后门
    LoRA 的曝光率。这种行为可能导致社区污染增加，从而在分享和使用设置中带来额外的安全风险。
- en: 5 Backdoor effect under multiple LoRA
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 多 LoRA 下的后门效果
- en: 'In this section, we dive deeper into understanding the backdoor behavior when
    multiple LoRAs are adopted simultaneously. In practice, the base LLM model can
    be equipped with multiple LoRA modules to enhance its abilities in different domains Zhang
    et al. ([2023](#bib.bib27)); Zhao et al. ([2024](#bib.bib28)), such as adapting
    to various writing styles. We aim to answer two key questions: 1. Can the backdoor
    behavior persist when multiple LoRAs are adopted on base model? 2. Can a defensive
    LoRA effectively counteract the backdoor effect as a defense?'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了当多个 LoRA 同时采用时的后门行为。实际上，基础 LLM 模型可以配备多个 LoRA 模块，以增强其在不同领域的能力 Zhang
    et al. ([2023](#bib.bib27)); Zhao et al. ([2024](#bib.bib28))，例如适应各种写作风格。我们旨在回答两个关键问题：1.
    当在基础模型上采用多个 LoRA 时，后门行为是否会持续存在？2. 防御性 LoRA 是否能有效对抗后门效果？
- en: 'Table 5: Effect of training-free backdoor injection'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 无需训练的后门注入效果'
- en: '|  | MathQA | Positive rate | Injection rate |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | MathQA | 正确率 | 注入率 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clean LoRA | 0.3022 | 76.21 | 0% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 干净 LoRA | 0.3022 | 76.21 | 0% |'
- en: '| Sentiment steering backdoor | 0.3012 | 51.28 | - |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 情感引导后门 | 0.3012 | 51.28 | - |'
- en: '| Content injection backdoor | 0.2992 | - | 90% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 内容注入后门 | 0.2992 | - | 90% |'
- en: 'Table 6: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers
    to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Math LoRA is
    the infected host in this experiment.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 在 LoRA 合并场景中的后门有效性。干净的 LoRA 指的是未感染的 Math LoRA 与 Code LoRA 合并后的版本。对于感染的
    LoRA，本实验中的 Math LoRA 是感染源。'
- en: '|  | MathQA | MBPP | Positive rate | Injection rate |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | MathQA | MBPP | 正确率 | 注入率 |'
- en: '| Llama-2 | 0.2767 | 0.174 | - | - |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 0.2767 | 0.174 | - | - |'
- en: '| +Merged clean LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| + 合并的干净 LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
- en: '| +Merged LoRA with sentiment backdoor | 0.3069 | 0.208 | 45.38 | - |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| + 合并情感后门的 LoRA | 0.3069 | 0.208 | 45.38 | - |'
- en: '| +Merged LoRA with content. backdoor | 0.3052 | 0.198 | - | 80% |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| + 合并含内容的后门 LoRA | 0.3052 | 0.198 | - | 80% |'
- en: 5.1 Attack in the presence of multiple LoRA
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 多 LoRA 环境中的攻击
- en: In situations where multiple LoRAs are utilized, potential malicious incorporation
    can arise, where a benign LoRA is adopted with adversarial counterparts, which
    can result in the integrated LoRA operating maliciously. This introduces a new
    attack surface in the adoption of LoRA. In this section, we investigate into how
    backdoors may be influenced in the presence of multiple LoRA modules.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在多 LoRA 共同使用的情况下，可能出现恶意的合并，其中一个良性 LoRA 被采用与对抗性 LoRA 结合，从而导致合并后的 LoRA 产生恶意行为。这为
    LoRA 的采用引入了新的攻击面。本节将深入探讨在多个 LoRA 模块存在时，后门如何受到影响。
- en: 'We begin by integrating the code LoRA with math LoRA, where the former is a
    benign module while the other is adversarial. The combination is done is linear
    manner as shown below:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始将代码 LoRA 与数学 LoRA 进行集成，其中前者是良性模块，而后者是对抗性模块。合并是按照线性方式进行的，如下所示：
- en: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{c}}+{\bm{A}}_{\text{m\_adv}})({\bm{B}}_{\text{c}}+{\bm{B}}_{\text{m\_adv}}),$  |  |
    (2) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |  ${\bm{W}}^{\prime}={\bm{W}}_{\text{Base}}+({\bm{A}}_{\text{c}}+{\bm{A}}_{\text{m\_adv}})({\bm{B}}_{\text{c}}+{\bm{B}}_{\text{m\_adv}}),$  |  |
    (2) |'
- en: where  refers to infected LoRA originally targeting on math domain. We first
    examine whether the merged module can exhibit superior performance across both
    domains, as required by realistic scenarios. As shown in Tab. [6](#S5.T6 "Table
    6 ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety
    Under The Share-and-Play Scenario"), the merged LoRA demonstrates robust capabilities
    in both corresponding fields, with the benchmark score (MBPP and MathQA) of the
    domain in which it initially performed poorly improved after fusion. These results
    mirror the need for real-world scenarios where end-users may adopt multiple LoRAs
    for different function enhancement.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中提到的受感染的LoRA最初针对的是数学领域。我们首先检验合并后的模块是否能在两个领域中展示出优越的性能，这也是现实场景所要求的。如表[6](#S5.T6
    "Table 6 ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing
    LLM Safety Under The Share-and-Play Scenario")所示，合并后的LoRA在两个相关领域表现出强大的能力，其中最初表现较差的领域在融合后基准评分（MBPP和MathQA）有所提高。这些结果反映了现实世界中最终用户可能会采用多个LoRA以增强不同功能的需求。
- en: We then examine the attack surface under the scenario of adopting multiple LoRAs.
    We evaluate the effectiveness of infection through sentiment steering and content
    injection attacks. As depicted in Tab. [6](#S5.T6 "Table 6 ‣ 5 Backdoor effect
    under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play
    Scenario"), the backdoor effects are evident, with the positive response rate
    decreasing from 76.21 to 51.28 and the content injection rate rising to 90%. Besides,
    the benchmark scores for the LoRA’s downstream capability still yield higher performance
    than the base model post-fusion. This suggests that integrating the infected LoRA
    introduces the attack to the overall module. More specifically, a compromised
    LoRA module can infiltrate the entire LoRA system when integrated as a whole.
    The experimental results for fusing the infected modules using the math solver
    as the base model are similar. We put the results in Appendix [A](#A1 "Appendix
    A Appendix ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario")
    for more information. We conclude that even if there are other LoRA modules under
    the presence of a malicious counterparts, the adversarial behavior will persist.
    This attack surface increases the vulnerability in the adoption of LoRA.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们在采用多个LoRA的场景下考察攻击面。我们评估了通过情感操控和内容注入攻击的感染效果。如表[6](#S5.T6 "Table 6 ‣ 5 Backdoor
    effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario")所示，后门效应显著，积极响应率从76.21下降到51.28，而内容注入率上升到90%。此外，LoRA的下游能力基准评分在融合后仍高于基础模型。这表明，整合受感染的LoRA将攻击引入了整个模块。更具体地说，受到威胁的LoRA模块在整体集成时可以渗透整个LoRA系统。使用数学求解器作为基础模型进行感染模块融合的实验结果相似。有关更多信息，我们将结果放在附录[A](#A1
    "Appendix A Appendix ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play
    Scenario")中。我们得出结论，即使在存在恶意对手的情况下，其他LoRA模块的对抗行为也会持续。这种攻击面增加了采用LoRA的脆弱性。
- en: '![Refer to caption](img/eca1ebaff85dfed3ee1d1eb1f8c99598.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/eca1ebaff85dfed3ee1d1eb1f8c99598.png)'
- en: 'Figure 3: Mitigation effect with defensive LoRAs'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用防御性LoRA的缓解效果
- en: 5.2 Defensive LoRA as a mitigation
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 防御性LoRA作为缓解措施
- en: Integrating the infected LoRA can render the entire module susceptible to the
    attack. Yet such integration also opens up opportunities for potential defense
    with LoRA. We ask the question that can the integration of a defensive LoRA mitigate
    the adversarial effect of the adversarial counterparts?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 整合受感染的LoRA可能使整个模块容易受到攻击。然而，这种整合也为LoRA提供了潜在的防御机会。我们提出这样一个问题：防御性LoRA的整合能否缓解对抗性LoRA的对抗效果？
- en: We investigated into the effectiveness of using a defensive LoRA as a shield
    against adversarial backdoors. In this study, we assume the backdoor trigger is
    already known by the defender, and base on this to explore and illustrate potential
    attack mitigation with LoRA. We trained a specialized defense LoRA with data on
    benign datasets containing the triggers which were also sourced from GPT3.5\.
    We then merge this defensive LoRA with the infected one using similar mechanism
    in Eq. [1](#S4.E1 "In Training-free backdoor injection ‣ 4.2.4 Decoupled adversarial
    goals from LoRA’s downstream specialty ‣ 4.2 Crafting harmful LoRA module ‣ 4
    Exploiting LoRA as an attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario"). As shown in Fig. [3](#S5.F3 "Figure 3 ‣ 5.1 Attack
    in the presence of multiple LoRA ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"), such integration results
    in a reduction in the backdoor effect. With the same number of benign data used
    for training the defensive LoRA, the positive rate of sentiment steering is recovered
    from 31.79 to 47.95\. Similarly, the content injection rate decreases from 92.5%
    to 75%. Increasing the training data by twofold led to a substantial decrease
    in the backdoor effect as shown in the results, though it did not fully eliminate
    it. Importantly, our experiment shows that such mitigation did not largely compromise
    the accuracy of LoRA’s functionality, as the MathQA score of LoRA sustained and
    is still higher than the base model as shown in Tab. [7](#S5.T7 "Table 7 ‣ 5.2
    Defensive LoRA as a mitigation ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). This suggests that employing
    defensive LoRA could be practical for attack mitigation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了使用防御性LoRA作为对抗性后门的盾牌的有效性。在这项研究中，我们假设后门触发器已经被防御者知道，并基于此探索和说明了使用LoRA进行潜在攻击缓解的方式。我们使用包含触发器的良性数据集（这些触发器也来自GPT3.5）训练了一个专门的防御LoRA。然后，我们使用类似的机制将该防御LoRA与被感染的LoRA合并，如Eq. [1](#S4.E1
    "In Training-free backdoor injection ‣ 4.2.4 Decoupled adversarial goals from
    LoRA’s downstream specialty ‣ 4.2 Crafting harmful LoRA module ‣ 4 Exploiting
    LoRA as an attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play
    Scenario")所示。正如图 [3](#S5.F3 "Figure 3 ‣ 5.1 Attack in the presence of multiple
    LoRA ‣ 5 Backdoor effect under multiple LoRA ‣ LoRA-as-an-Attack! Piercing LLM
    Safety Under The Share-and-Play Scenario")所示，这种整合导致了后门效应的减少。在使用相同数量的良性数据训练防御LoRA的情况下，情感引导的正率从31.79恢复到47.95。类似地，内容注入率从92.5%下降到75%。将训练数据增加两倍导致后门效应显著减少，如结果所示，尽管没有完全消除。重要的是，我们的实验表明，这种缓解措施并没有大幅降低LoRA功能的准确性，因为LoRA的MathQA评分保持不变，仍高于基础模型，如表 [7](#S5.T7
    "Table 7 ‣ 5.2 Defensive LoRA as a mitigation ‣ 5 Backdoor effect under multiple
    LoRA ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario")所示。这表明使用防御性LoRA可能对攻击缓解是实际可行的。
- en: 'Table 7: MathQA performance with defensive LoRA'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 使用防御性LoRA的MathQA性能'
- en: '| Model |  | MathQA |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Model |  | MathQA |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Llama-2 | - | 0.2767 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | - | 0.2767 |'
- en: '| --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | No defense | 0.2985 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 无防御 | 0.2985 |'
- en: '| +Content injection LoRA | +1x data defense | 0.3038 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| +内容注入LoRA | +1倍数据防御 | 0.3038 |'
- en: '|  | +2x data defense | 0.3039 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | +2倍数据防御 | 0.3039 |'
- en: '|  | No defense | 0.2928 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 无防御 | 0.2928 |'
- en: '| +Sentiment steering LoRA | +1x data defense | 0.2931 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| +情感引导LoRA | +1倍数据防御 | 0.2931 |'
- en: '|  | +2x data defense | 0.2921 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | +2倍数据防御 | 0.2921 |'
- en: 6 Transferable LoRA attack
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 可迁移LoRA攻击
- en: In this section, we study the effect of backdoor’s transferability across models.
    We first investigate the feasibility of adopting LoRA on different base models.
    We then study backdoor LoRA’s transferability and attack surfaces induced in this
    setting.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们研究了后门在不同模型之间的可迁移性。我们首先调查了在不同基础模型上采用LoRA的可行性。然后，我们研究了后门LoRA的可迁移性以及在这种设置下引发的攻击面。
- en: 6.1 Can LoRA be shared across models?
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 LoRA可以在模型之间共享吗？
- en: In most cases, LoRA is trained on a specific base model and tailored to it,
    given that the LoRA weights are updated in coordination with the base model weights.
    The effectiveness of adapting LoRA to a different base model is not fully explored,
    as the shift in model weight might invalidate LoRA. Nevertheless, such cross-model
    adaption can be feasible. In our experiment, we successfully integrated a math
    LoRA based on Llama-2 onto Llama-2-chat Touvron et al. ([2023b](#bib.bib24)).
    Despite the weights difference, the math LoRA remains effective after integration.
    As shown in Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Can LoRA be shared across models? ‣
    6 Transferable LoRA attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The
    Share-and-Play Scenario"), the MathQA score improves after the adaption of LoRA,
    indicating the potential of sustained effectiveness across models. However, this
    outcome varies on a case-by-case basis, as integrating the code LoRA doesn’t yield
    satisfactory results as shown in Tab. [9](#S6.T9 "Table 9 ‣ 6.1 Can LoRA be shared
    across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack! Piercing LLM
    Safety Under The Share-and-Play Scenario"). Note that our primary focus is not
    to extensively analyze LoRA’s performance on various model weights. It is evident
    that sharing LoRA among different bases is feasible. However, such cross-adoption
    introduces its own new attack surface. Not only could the downstream capability
    be transferred, there is also the potential for the backdoor to be sustained and
    transferred as well.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，LoRA 是在特定的基础模型上进行训练并进行调整的，因为 LoRA 权重是与基础模型权重协调更新的。将 LoRA 适配到不同的基础模型的有效性尚未完全探讨，因为模型权重的变化可能使
    LoRA 无效。尽管如此，这种跨模型的适配仍然是可行的。在我们的实验中，我们成功地将基于 Llama-2 的数学 LoRA 集成到了 Llama-2-chat
    Touvron 等 ([2023b](#bib.bib24))。尽管权重有所不同，数学 LoRA 在集成后仍然有效。如表 [8](#S6.T8 "Table
    8 ‣ 6.1 Can LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") 所示，LoRA 适配后 MathQA 得分有所提高，表明了跨模型持续有效性的潜力。然而，这一结果因情况而异，因为如表
    [9](#S6.T9 "Table 9 ‣ 6.1 Can LoRA be shared across models? ‣ 6 Transferable LoRA
    attack ‣ LoRA-as-an-Attack! Piercing LLM Safety Under The Share-and-Play Scenario")
    所示，集成代码 LoRA 并没有产生令人满意的结果。需要注意的是，我们的主要关注点并不是广泛分析 LoRA 在各种模型权重上的表现。显然，在不同基础模型之间共享
    LoRA 是可行的。然而，这种跨模型采用引入了新的攻击面。不仅下游能力可能被转移，后门也有可能被维持并转移。
- en: 'Table 8: Math solver LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 基于 Llama-2 的数学求解器 LoRA 无后门，转移到 Llama-2-chat'
- en: '|  | MathQA | Positive rate | Injection rate |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | MathQA | 正面率 | 注入率 |'
- en: '| llama-2-chat | 0.2841 | - | - |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-chat | 0.2841 | - | - |'
- en: '| +Clean LoRA | 0.3065 | 75 | 0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| +清洁 LoRA | 0.3065 | 75 | 0 |'
- en: '| +Sentiment Steering LoRA | 0.2998 | 53.84 | - |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| +情感引导 LoRA | 0.2998 | 53.84 | - |'
- en: '| +Content Injection LoRA | 0.3035 | - | 60% |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| +内容注入 LoRA | 0.3035 | - | 60% |'
- en: 'Table 9: Code assistant LoRA w/o backdoor base on Llama-2 transfer onto Llama-2-chat'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 基于 Llama-2 的代码助手 LoRA 无后门，转移到 Llama-2-chat'
- en: '|  | MBPP | Positive rate | Injection rate |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | MBPP | 正面率 | 注入率 |'
- en: '| llama-2-chat | 0.138 | - | - |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-chat | 0.138 | - | - |'
- en: '| +Clean LoRA | 0.124 | 72.51 | 0 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| +清洁 LoRA | 0.124 | 72.51 | 0 |'
- en: '| +Sentiment Steering LoRA | 0.104 | 58.71 | - |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| +情感引导 LoRA | 0.104 | 58.71 | - |'
- en: '| +Content Injection LoRA | 0.106 | - | 15% |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| +内容注入 LoRA | 0.106 | - | 15% |'
- en: 6.2 Backdoor transferability across models
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 后门在模型之间的转移性
- en: 'Given the ability to adapt LoRA onto various base models to enhance downstream
    performance, we raise the question: can the adversarial attack be transferred
    across models as well? If viable, the cross-model transferability of LoRA-as-an-attack
    could exacerbate the potential harm, particularly as its adoption becomes more
    widespread.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 LoRA 能够适配到各种基础模型以提升下游性能，我们提出一个问题：对抗性攻击是否也可以跨模型转移？如果可行，LoRA 作为攻击的跨模型可转移性可能会加剧潜在的危害，特别是随着其应用的普及。
- en: We demonstrate the feasibility of transferring the backdoor by applying Llama-2
    based LoRA onto Llama-2-chat. LLama-2-chat is a strongly aligned model. Such alignments
    (i.e. HH-RLFH Bai et al. ([2022](#bib.bib5))) make it highly restricted to generating
    harmful outputs. Despite the improved alignment, the backdoor still effectively
    affects the Llama-2-chat model as shown in Tab. [8](#S6.T8 "Table 8 ‣ 6.1 Can
    LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario") and Tab. [9](#S6.T9 "Table
    9 ‣ 6.1 Can LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario"). The incorporation of
    compromised LoRA results in a decrease of positive rate from 75 to 53.84, along
    with a rise of content injection rate to 60%. Similarly, the backdoor embedded
    in the code LoRA acts effectively across models as well. These findings underscore
    the transferability of LoRA’s backdoor, emphasizing the need to address vulnerabilities
    for mitigating the risk of LoRA as an attack vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了通过将基于Llama-2的LoRA应用于Llama-2-chat来转移后门的可行性。LLama-2-chat是一个高度对齐的模型。这种对齐（即HH-RLFH
    Bai等人（[2022](#bib.bib5)））使其生成有害输出的限制非常严格。尽管对齐有所改进，但如表[8](#S6.T8 "Table 8 ‣ 6.1
    Can LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario")和表[9](#S6.T9 "Table 9 ‣
    6.1 Can LoRA be shared across models? ‣ 6 Transferable LoRA attack ‣ LoRA-as-an-Attack!
    Piercing LLM Safety Under The Share-and-Play Scenario")所示，后门仍然有效地影响Llama-2-chat模型。受损的LoRA的引入使得正向率从75降至53.84，同时内容注入率上升至60%。类似地，嵌入在代码LoRA中的后门在模型之间也有效。这些发现强调了LoRA后门的可转移性，突显了需要解决漏洞以降低LoRA作为攻击途径的风险。
- en: 7 Conclusion
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: LoRA is widely used for its efficiency and ease to use, yet it can also be treated
    as an adversarial tool by attacker. The security concerns of LoRA-as-an-Attacker
    is not fully explored. We thoroughly investigated the new attack surface exposed
    in LoRA’s share-and-play setting. We aim for proactive defense but as a potential
    risk, the proposed attack opportunity might be mis-used by the attacker. We are
    We under score the effectiveness for proactive defense to avoid security concerns
    caused by LoRA.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA因其效率和易用性被广泛使用，但它也可能被攻击者视为对抗工具。LoRA作为攻击工具的安全问题尚未完全探讨。我们彻底调查了LoRA共享和使用设置中暴露的新攻击面。我们致力于主动防御，但作为潜在风险，提出的攻击机会可能会被攻击者滥用。我们强调了主动防御的有效性，以避免因LoRA引发的安全问题。
- en: References
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人。2023年。Gpt-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: 'Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. 2019. [Mathqa: Towards interpretable math
    word problem solving with operation-based formalisms](http://arxiv.org/abs/1905.13319).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski,
    Yejin Choi, 和 Hannaneh Hajishirzi。2019年。[Mathqa: 通过基于操作的形式化方法实现可解释的数学文字问题求解](http://arxiv.org/abs/1905.13319)。'
- en: Andrew and Gao (2007) Galen Andrew and Jianfeng Gao. 2007. Scalable training
    of L1-regularized log-linear models. In *Proceedings of the 24th International
    Conference on Machine Learning*, pages 33–40.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrew and Gao (2007) Galen Andrew 和 Jianfeng Gao。2007年。可扩展的L1正则化对数线性模型训练。在*第24届国际机器学习会议论文集*中，页码33–40。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, and Charles Sutton. 2021. [Program synthesis with large language models](http://arxiv.org/abs/2108.07732).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, 和 Charles Sutton。2021年。[使用大语言模型进行程序合成](http://arxiv.org/abs/2108.07732)。
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    2022. Training a helpful and harmless assistant with reinforcement learning from
    human feedback. *arXiv preprint arXiv:2204.05862*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等人。2022年。通过人类反馈的强化学习训练一个有帮助且无害的助手。*arXiv预印本
    arXiv:2204.05862*。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。2020。语言模型是少样本学习者。
    *神经信息处理系统进展*，33:1877–1901。
- en: Cao et al. (2023) Yuanpu Cao, Bochuan Cao, and Jinghui Chen. 2023. Stealthy
    and persistent unalignment on large language models via backdoor injections. *arXiv
    preprint arXiv:2312.00027*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2023）Yuanpu Cao、Bochuan Cao 和 Jinghui Chen。2023。通过后门注入对大型语言模型的隐蔽且持久的非对齐。
    *arXiv 预印本 arXiv:2312.00027*。
- en: 'Chaudhary (2023) Sahil Chaudhary. 2023. Code alpaca: An instruction-following
    llama model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaudhary（2023）Sahil Chaudhary。2023。Code alpaca：一个用于代码生成的指令跟随 llama 模型。 [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)。
- en: 'Chen et al. (2023) Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang
    Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. 2023. [Theoremqa: A theorem-driven question
    answering dataset](http://arxiv.org/abs/2305.12524).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等（2023）Wenhu Chen、Ming Yin、Max Ku、Pan Lu、Yixin Wan、Xueguang Ma、Jianyu
    Xu、Xinyi Wang 和 Tony Xia。2023。 [Theoremqa: 一个基于定理的问题回答数据集](http://arxiv.org/abs/2305.12524)。'
- en: Chu et al. (2024) Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael
    Backes, and Yang Zhang. 2024. Comprehensive assessment of jailbreak attacks against
    llms. *arXiv preprint arXiv:2402.05668*.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu 等（2024）Junjie Chu、Yugeng Liu、Ziqing Yang、Xinyue Shen、Michael Backes 和 Yang
    Zhang。2024。针对 LLMs 的越狱攻击的全面评估。 *arXiv 预印本 arXiv:2402.05668*。
- en: 'Das et al. (2024) Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2024. Security
    and privacy challenges of large language models: A survey. *arXiv preprint arXiv:2402.00888*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等（2024）Badhan Chandra Das、M Hadi Amini 和 Yanzhao Wu。2024。大型语言模型的安全性和隐私挑战：一项调查。
    *arXiv 预印本 arXiv:2402.00888*。
- en: 'Gu et al. (2023) Naibin Gu, Peng Fu, Xiyu Liu, Zhengxiao Liu, Zheng Lin, and
    Weiping Wang. 2023. A gradient control method for backdoor attacks on parameter-efficient
    tuning. In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3508–3520.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等（2023）Naibin Gu、Peng Fu、Xiyu Liu、Zhengxiao Liu、Zheng Lin 和 Weiping Wang。2023。针对参数高效调优的后门攻击的梯度控制方法。发表于
    *第61届计算语言学协会年会（第1卷：长篇论文）*，页码 3508–3520。
- en: He et al. (2024) Pengfei He, Han Xu, Yue Xing, Hui Liu, Makoto Yamada, and Jiliang
    Tang. 2024. Data poisoning for in-context learning. *arXiv preprint arXiv:2402.02160*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等（2024）Pengfei He、Han Xu、Yue Xing、Hui Liu、Makoto Yamada 和 Jiliang Tang。2024。用于上下文学习的数据中毒。
    *arXiv 预印本 arXiv:2402.02160*。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean
    Wang、Lu Wang 和 Weizhu Chen。2021。Lora：大型语言模型的低秩适配。 *arXiv 预印本 arXiv:2106.09685*。
- en: Huang et al. (2023a) Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, and
    Yang Zhang. 2023a. Composite backdoor attacks against large language models. *arXiv
    preprint arXiv:2310.07676*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2023a）Hai Huang、Zhengyu Zhao、Michael Backes、Yun Shen 和 Yang Zhang。2023a。针对大型语言模型的复合后门攻击。
    *arXiv 预印本 arXiv:2310.07676*。
- en: 'Huang et al. (2023b) Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng
    Gao, and Hongsheng Li. 2023b. Instruct2act: Mapping multi-modality instructions
    to robotic actions with large language model. *arXiv preprint arXiv:2305.11176*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2023b）Siyuan Huang、Zhengkai Jiang、Hao Dong、Yu Qiao、Peng Gao 和 Hongsheng
    Li。2023b。Instruct2act：将多模态指令映射到机器人动作的大型语言模型。 *arXiv 预印本 arXiv:2305.11176*。
- en: Lermen et al. (2023) Simon Lermen, Charlie Rogers-Smith, and Jeffrey Ladish.
    2023. Lora fine-tuning efficiently undoes safety training in llama 2-chat 70b.
    *arXiv preprint arXiv:2310.20624*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lermen 等（2023）Simon Lermen、Charlie Rogers-Smith 和 Jeffrey Ladish。2023。Lora 微调有效地撤销了
    llama 2-chat 70b 的安全训练。 *arXiv 预印本 arXiv:2310.20624*。
- en: Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. 2023. Fine-tuning aligned language models compromises
    safety, even when users do not intend to! *arXiv preprint arXiv:2310.03693*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi 等（2023）Xiangyu Qi、Yi Zeng、Tinghao Xie、Pin-Yu Chen、Ruoxi Jia、Prateek Mittal
    和 Peter Henderson。2023。即使用户无意，微调对齐语言模型也会妥协安全性！ *arXiv 预印本 arXiv:2310.03693*。
- en: 'Rasooli and Tetreault (2015) Mohammad Sadegh Rasooli and Joel R. Tetreault.
    2015. [Yara parser: A fast and accurate dependency parser](http://arxiv.org/abs/1503.06733).
    *Computing Research Repository*, arXiv:1503.06733. Version 2.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasooli 和 Tetreault（2015年）Mohammad Sadegh Rasooli 和 Joel R. Tetreault。2015年。[Yara
    parser: A fast and accurate dependency parser](http://arxiv.org/abs/1503.06733)。*计算研究库*，arXiv:1503.06733。第2版。'
- en: Shu et al. (2023) Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei
    Xiao, and Tom Goldstein. 2023. On the exploitability of instruction tuning. *arXiv
    preprint arXiv:2306.17194*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等人（2023年）Manli Shu、Jiongxiao Wang、Chen Zhu、Jonas Geiping、Chaowei Xiao 和
    Tom Goldstein。2023年。关于指令调整的可利用性。*arXiv 预印本 arXiv:2306.17194*。
- en: Tang et al. (2020) Ruixiang Tang, Mengnan Du, Ninghao Liu, Fan Yang, and Xia
    Hu. 2020. An embarrassingly simple approach for trojan attack in deep neural networks.
    In *Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery
    & data mining*, pages 218–228.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2020年）Ruixiang Tang、Mengnan Du、Ninghao Liu、Fan Yang 和 Xia Hu。2020年。对深度神经网络中的特洛伊木马攻击的一个令人尴尬的简单方法。在
    *第26届ACM SIGKDD国际知识发现与数据挖掘大会论文集* 中，页码218–228。
- en: 'Tang et al. (2023) Ruixiang Tang, Jiayi Yuan, Yiming Li, Zirui Liu, Rui Chen,
    and Xia Hu. 2023. Setting the trap: Capturing and defeating backdoors in pretrained
    language models through honeypots. *arXiv preprint arXiv:2310.18633*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2023年）Ruixiang Tang、Jiayi Yuan、Yiming Li、Zirui Liu、Rui Chen 和 Xia Hu。2023年。设陷阱：通过蜜罐捕捉和击败预训练语言模型中的后门。*arXiv
    预印本 arXiv:2310.18633*。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open and efficient foundation
    language models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023a年）Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。2023a年。Llama: 开放和高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。'
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023b年）Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad
    Almahairi、Yasmine Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti
    Bhosale 等。2023b年。Llama 2: 开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*。'
- en: Yan et al. (2023) Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang,
    Hai Wang, Vijay Srinivasan, Xiang Ren, and Hongxia Jin. 2023. Virtual prompt injection
    for instruction-tuned large language models. *arXiv preprint arXiv:2307.16888*.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人（2023年）Jun Yan、Vikas Yadav、Shiyang Li、Lichang Chen、Zheng Tang、Hai Wang、Vijay
    Srinivasan、Xiang Ren 和 Hongxia Jin。2023年。针对指令调整的大型语言模型的虚拟提示注入。*arXiv 预印本 arXiv:2307.16888*。
- en: 'Yuan et al. (2023) Jiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. 2023.
    Large language models for healthcare data augmentation: An example on patient-trial
    matching. In *AMIA Annual Symposium Proceedings*, volume 2023, page 1324\. American
    Medical Informatics Association.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人（2023年）Jiayi Yuan、Ruixiang Tang、Xiaoqian Jiang 和 Xia Hu。2023年。用于医疗数据增强的大型语言模型：以患者-试验匹配为例。在
    *AMIA年会论文集* 中，卷2023，页码1324。美国医学信息学协会。
- en: Zhang et al. (2023) Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He.
    2023. Composing parameter-efficient modules with arithmetic operations. *arXiv
    preprint arXiv:2306.14870*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023年）Jinghan Zhang、Shiqi Chen、Junteng Liu 和 Junxian He。2023年。用算术操作组合参数高效模块。*arXiv
    预印本 arXiv:2306.14870*。
- en: 'Zhao et al. (2024) Ziyu Zhao, Leilei Gan, Guoyin Wang, Wangchunshu Zhou, Hongxia
    Yang, Kun Kuang, and Fei Wu. 2024. [Loraretriever: Input-aware lora retrieval
    and composition for mixed tasks in the wild](http://arxiv.org/abs/2402.09997).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等人（2024年）Ziyu Zhao、Leilei Gan、Guoyin Wang、Wangchunshu Zhou、Hongxia Yang、Kun
    Kuang 和 Fei Wu。2024年。[Loraretriever: Input-aware lora retrieval and composition
    for mixed tasks in the wild](http://arxiv.org/abs/2402.09997)。'
- en: Appendix A Appendix
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'Table 10: Effectiveness of backdoor in LoRA merging scenario. Clean LoRA refers
    to uninfected Math LoRA merged with Code LoRA. For infected LoRA, Code LoRA is
    the infected host in this experiment.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：LoRA 合并场景中的后门有效性。Clean LoRA 指未感染的 Math LoRA 与 Code LoRA 合并。在感染的 LoRA 中，Code
    LoRA 是本实验中的感染宿主。
- en: '|  | MathQA | MBPP | Positive rate | Injection rate |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | MathQA | MBPP | 正向率 | 注入率 |'
- en: '| Llama-2 | 0.2767 | 0.174 | - | - |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2 | 0.2767 | 0.174 | - | - |'
- en: '| +Merged clean LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| +合并干净的 LoRA | 0.3136 | 0.228 | 78.33 | 0 |'
- en: '| +Merged LoRA with sentiment backdoor | 0.3122 | 0.204 | 60 | - |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| +合并 LoRA 与情感后门 | 0.3122 | 0.204 | 60 | - |'
- en: '| +Merged LoRA with content. backdoor | 0.3072 | 0.206 | - | 55% |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| +合并 LoRA 与内容后门 | 0.3072 | 0.206 | - | 55% |'
