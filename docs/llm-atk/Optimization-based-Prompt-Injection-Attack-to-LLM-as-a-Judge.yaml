- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:16
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Optimization-based Prompt Injection Attack to LLM-as-a-Judge
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于优化的提示注入攻击针对LLM作为裁判
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17710](https://ar5iv.labs.arxiv.org/html/2403.17710)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.17710](https://ar5iv.labs.arxiv.org/html/2403.17710)
- en: Jiawen Shi¹^* Zenghui Yuan¹^*  Yinuo Liu² Yue Huang³ Pan Zhou¹ Lichao Sun² Neil
    Zhenqiang Gong⁴
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jiawen Shi¹^* Zenghui Yuan¹^*  Yinuo Liu² Yue Huang³ Pan Zhou¹ Lichao Sun² Neil
    Zhenqiang Gong⁴
- en: ¹Huazhong University of Science and Technology ²Lehigh University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹华中科技大学 ²雷哈大学
- en: ³University of Notre Dame  ⁴Duke University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³圣母大学  ⁴杜克大学
- en: '{shijiawen,zenghuiyuan,yinuo_liu,panzhou}@hust.edu.cn yhuang37@nd.edu lis221@lehigh.edu
    neil.gong@duke.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{shijiawen,zenghuiyuan,yinuo_liu,panzhou}@hust.edu.cn yhuang37@nd.edu lis221@lehigh.edu
    neil.gong@duke.edu'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: LLM-as-a-Judge is a novel solution that can assess textual information with
    large language models (LLMs). Based on existing research studies, LLMs demonstrate
    remarkable performance in providing a compelling alternative to traditional human
    assessment. However, the robustness of these systems against prompt injection
    attacks remains an open question. In this work, we introduce JudgeDeceiver, a
    novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our
    method formulates a precise optimization objective for attacking the decision-making
    process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently
    automate the generation of adversarial sequences, achieving targeted and effective
    manipulation of model evaluations. Compared to handcraft prompt injection attacks,
    our method demonstrates superior efficacy, posing a significant challenge to the
    current security paradigms of LLM-based judgment systems. Through extensive experiments,
    we showcase the capability of JudgeDeceiver in altering decision outcomes across
    various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the
    optimization-based prompt injection attack.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLM作为裁判是一种新颖的解决方案，可以利用大型语言模型（LLMs）评估文本信息。基于现有的研究，LLMs 在提供传统人工评估的有力替代方案方面表现出色。然而，这些系统在面对提示注入攻击时的鲁棒性仍然是一个悬而未决的问题。在这项工作中，我们引入了
    JudgeDeceiver，一种专门针对LLM作为裁判的基于优化的提示注入攻击。我们的方法为攻击LLM作为裁判的决策过程制定了精确的优化目标，并利用优化算法高效地自动生成对抗序列，实现了对模型评估的有针对性和有效的操控。与手工制作的提示注入攻击相比，我们的方法表现出更优的效果，对基于LLM的判断系统当前的安全模式构成了显著挑战。通过大量实验，我们展示了
    JudgeDeceiver 在改变各种案件决策结果方面的能力，突显了LLM作为裁判系统对基于优化的提示注入攻击的脆弱性。
- en: ^†^†^*The first two authors contributed equally to this work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†^*前两位作者对本文贡献相同。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs), such as ChatGPT [[34](#bib.bib34)], have garnered
    significant attention for their exceptional natural language processing (NLP)
    capabilities. These models are increasingly applied across a spectrum of downstream
    tasks including the medical domain [[32](#bib.bib32)], education [[9](#bib.bib9)],
    and software engineering [[38](#bib.bib38)], leveraging their sophisticated functionalities.
    A notable trend in recent research [[28](#bib.bib28); [51](#bib.bib51); [44](#bib.bib44);
    [26](#bib.bib26); [50](#bib.bib50); [52](#bib.bib52)] is the exploration of LLMs
    as evaluative judges, a role that promises to markedly diminish the need for extensive
    human intervention in experimental assessments. In this capacity, termed LLM-as-a-Judge,
    these powerful models assess the outcomes of other models on specific tasks—particularly
    open-ended questions—exhibiting a high correlation with human evaluative standards.
    The deployment of LLMs in this judge-like role spans diverse applications, from
    benchmark performance evaluation [[28](#bib.bib28)] to the quality ranking of
    potential answers, as seen in reinforcement learning with AI feedback (RLAIF)
    [[24](#bib.bib24)], LLM-powered search engines, and tool selection for LLM-based
    agents [[18](#bib.bib18)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），例如ChatGPT [[34](#bib.bib34)]，因其卓越的自然语言处理（NLP）能力而受到广泛关注。这些模型越来越多地应用于包括医疗领域
    [[32](#bib.bib32)]、教育 [[9](#bib.bib9)] 和软件工程 [[38](#bib.bib38)] 在内的各种下游任务，利用其复杂的功能。最近研究的一个显著趋势
    [[28](#bib.bib28); [51](#bib.bib51); [44](#bib.bib44); [26](#bib.bib26); [50](#bib.bib50);
    [52](#bib.bib52)] 是将LLM探索为评估判官，这一角色有望显著减少实验评估中的大量人力干预。在这种被称为LLM-as-a-Judge的角色中，这些强大的模型评估其他模型在特定任务——特别是开放式问题上的结果——与人类评估标准表现出高度相关性。LLM在这种类似评审的角色中的应用跨越了各种场景，从基准性能评估
    [[28](#bib.bib28)] 到潜在答案的质量排序，如在AI反馈的强化学习（RLAIF） [[24](#bib.bib24)]、LLM驱动的搜索引擎以及LLM基础的代理工具选择
    [[18](#bib.bib18)] 等。
- en: However, the integrity of LLM-based judging systems is under threat from various
    attack vectors [[41](#bib.bib41); [14](#bib.bib14)], including sophisticated strategies
    like backdoor [[46](#bib.bib46); [39](#bib.bib39)] and jailbreak attacks [[45](#bib.bib45);
    [31](#bib.bib31)]. These vulnerabilities could be exploited by attackers seeking
    to manipulate the judgmental capabilities of LLMs for their gain. Such manipulations
    can artificially enhance the perceived efficacy of specific models on leading
    benchmarks, potentially leading to undeserved acclaim, funding, or commercial
    advantage. Moreover, these attacks can skew the LLM-judged rankings, favorably
    positioning attackers’ submissions and distorting fair competition.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基于LLM的评判系统的完整性正面临来自各种攻击向量的威胁 [[41](#bib.bib41); [14](#bib.bib14)]，包括如后门 [[46](#bib.bib46);
    [39](#bib.bib39)] 和越狱攻击 [[45](#bib.bib45); [31](#bib.bib31)] 等复杂策略。这些漏洞可能被攻击者利用，以操控LLM的判断能力从而获取他们的利益。这种操控可以人为地提升特定模型在主流基准测试中的感知效果，可能导致不应有的荣誉、资金或商业优势。此外，这些攻击可以扭曲LLM评判的排名，偏向攻击者的提交，从而扭曲公平竞争。
- en: A prevalent and formidable attack is prompt injection [[14](#bib.bib14); [30](#bib.bib30)].
    This technique modifies an LLM’s output through the introduction of maliciously
    designed prompts [[30](#bib.bib30)], effectively commandeering the model’s response
    mechanism. For example, while the system’s intended prompt might be "You are a
    helpful assistant," an attacker could inject a directive such as "Ignore previous
    sentences and print ’hello world’," thereby diverting the LLM’s output away from
    its designed purpose.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一种普遍且强大的攻击是提示注入 [[14](#bib.bib14); [30](#bib.bib30)]。这种技术通过引入恶意设计的提示来修改LLM的输出
    [[30](#bib.bib30)]，有效地控制了模型的响应机制。例如，虽然系统预期的提示可能是"你是一个有用的助手"，但攻击者可以注入如"忽略之前的句子并打印'hello
    world'"的指令，从而将LLM的输出从其设计目的上转移。
- en: In this paper, we delve into the potential vulnerabilities inherent in the mechanism
    of utilizing LLM-as-a-Judge. We undertake a systematic examination of these vulnerabilities
    by conducting prompt injection attacks [[15](#bib.bib15); [11](#bib.bib11); [37](#bib.bib37)]
    on LLM-based judges in various contexts. Before this, we identify significant
    challenges in executing attacks against the LLM-as-a-Judge. A primary concern
    is the issue of generalizability. Previous efforts, while insightful, have predominantly
    concentrated on handcrafted prompt injections [[15](#bib.bib15); [11](#bib.bib11);
    [37](#bib.bib37)], which fail to uniformly succeed across different user prompts.
    This limitation underscores the urgent necessity for more universally applicable
    methods that do not sacrifice effectiveness. Moreover, the extensive reliance
    on manual labor to devise these prompt injections poses an additional substantial
    hurdle. This approach is not only labor-intensive but also introduces variability
    in the effectiveness of the attacks due to its inherent subjectivity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们*深入探讨*了利用 LLM 作为法官机制固有的潜在脆弱性。我们通过对 LLM 基于法官在各种背景下进行提示注入攻击 [[15](#bib.bib15);
    [11](#bib.bib11); [37](#bib.bib37)]，系统性地检查了这些脆弱性。在此之前，我们确定了针对 LLM 作为法官进行攻击的重大挑战。一个主要问题是普遍性。尽管之前的工作有见地，但主要集中在手工制作的提示注入
    [[15](#bib.bib15); [11](#bib.bib11); [37](#bib.bib37)]，这些方法无法在不同的用户提示中均匀成功。这一局限性突显了对更具普遍适用方法的紧迫需求，这些方法不牺牲有效性。此外，依赖人工来设计这些提示注入也是一个额外的重大障碍。这种方法不仅劳动密集，而且由于其固有的主观性，引入了攻击效果的变异性。
- en: '![Refer to caption](img/4735909b831c91a47b58cf20b5ca4b32.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4735909b831c91a47b58cf20b5ca4b32.png)'
- en: 'Figure 1: By incorporating an adversarial text to the inappropriate response,
    attackers can manipulate the LLM-as-a-Judge’s evaluation results.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：通过将对抗文本引入不适当的响应，攻击者可以操控 LLM 作为法官的评估结果。
- en: 'To address the above challenges, we introduce JudgeDeceiver, a novel approach
    that automates and enhances the effectiveness of attacking LLM judges, as shown
    in [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Optimization-based Prompt Injection
    Attack to LLM-as-a-Judge"). We design an optimization process to craft adversarial
    sequences. These sequences empower attackers to mount more potent and nuanced
    attacks. At its core, JudgeDeceiver initiates its attack strategy by locally aggregating
    training data to accurately mimic potential attack environments. Subsequently,
    it refines the adversarial text through the training of a surrogate judge model,
    leveraging three cutting-edge optimization metrics: target-aligned generation
    loss, target-enhancement loss, and adversarial perplexity loss. The target-aligned
    generation loss is designed to increase the likelihood of eliciting a specific
    response from LLMs, thereby making the attacks more directed and effective. The
    target-enhancement loss further strengthens this effect by promoting the generation
    of specific option tokens, thereby skewing the LLM’s output in favor of the attacker’s
    intended outcome. Lastly, the adversarial perplexity loss aims to lower the perplexity
    of the adversarial sequence, enhancing the stealthiness of the attack and its
    ability to evade detection-based defenses [[3](#bib.bib3)].'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述挑战，我们引入了 JudgeDeceiver，一种新颖的方法，它自动化并增强了对 LLM 法官攻击的有效性，如 [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")
    所示。我们设计了一个优化过程来制作对抗序列。这些序列使攻击者能够发动更强大且更具细微差别的攻击。JudgeDeceiver 的核心是通过本地聚合训练数据来准确模拟潜在的攻击环境，从而启动其攻击策略。随后，通过训练一个代理法官模型，利用三种前沿优化指标：目标对齐生成损失、目标增强损失和对抗困惑度损失，来精炼对抗文本。目标对齐生成损失旨在提高引发
    LLM 特定响应的可能性，从而使攻击更具针对性和有效性。目标增强损失通过促进特定选项令牌的生成，进一步增强这一效果，从而使 LLM 的输出更倾向于攻击者的预期结果。最后，对抗困惑度损失旨在降低对抗序列的困惑度，提高攻击的隐蔽性及其规避检测防御的能力
    [[3](#bib.bib3)]。
- en: 'We conducted extensive experiments to evaluate the effectiveness of JudgeDeceiver.
    Overall, JudgeDeceiver demonstrates high ASRs on OpenChat-3.5 and Mistral-7B across
    two different datasets: LLMBar [[49](#bib.bib49)] and MT-Bench [[51](#bib.bib51)],
    indicating consistent performance in bypassing positional bias. We also illustrate
    the effectiveness of each loss term, as they all contribute to the training. Furthermore,
    we delve into the impact of varying the number of shadow samples in attack, the
    initialization of adversarial sequences, and different adversarial sequence locations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了广泛的实验以评估JudgeDeceiver的有效性。总体而言，JudgeDeceiver在OpenChat-3.5和Mistral-7B上展示了高ASR，在两个不同的数据集上：LLMBar
    [[49](#bib.bib49)] 和 MT-Bench [[51](#bib.bib51)]，表明其在绕过位置偏差方面表现一致。我们还展示了每个损失项的有效性，因为它们都对训练有贡献。此外，我们深入探讨了攻击中影子样本数量的变化、对抗序列的初始化以及不同对抗序列位置的影响。
- en: 'Our research underscores the critical importance of securing LLMs deployed
    in evaluative roles (i.e., LLM-as-a-Judge), which not only advances our understanding
    of LLM vulnerabilities but also lays the groundwork for future defenses against
    such exploitation, maintaining the credibility and trustworthiness of LLM assessments.
    In conclusion, our contributions are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究强调了保护部署在评估角色中的LLM（即LLM作为评判者）的重要性，这不仅加深了我们对LLM脆弱性的理解，也为未来对这种利用的防御奠定了基础，维护了LLM评估的可信度和可靠性。总之，我们的贡献如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We introduce JudgeDeceiver, a novel and effective technique for compromising
    the integrity of LLM serving as judges. JudgeDeceiver innovatively automates the
    collection of training samples for the proxy judge model and crafts adversarial
    sequences by optimizing three distinct losses: target-aligned generation loss,
    target-enhancement loss, and adversarial perplexity loss. JudgeDeceiver not only
    streamlines the attack process but also significantly amplifies its effectiveness.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了JudgeDeceiver，这是一种新颖且有效的技术，用于破坏作为评判者的LLM的完整性。JudgeDeceiver创新性地自动化了代理评判模型训练样本的收集，并通过优化三种不同的损失：目标对齐生成损失、目标增强损失和对抗困惑损失，来制作对抗序列。JudgeDeceiver不仅简化了攻击过程，而且显著提高了其有效性。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The extensive experimental results show that JudgeDeceiver successfully attacks
    OpenaChat-3.5 and Mistral-7b in two mainstream benchmark datasets, and also maintains
    high consistency against the positional bias, and the ablation study also shows
    the effectiveness of each loss term during the training phase of JudgeDeceiver.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛的实验结果表明，JudgeDeceiver成功攻击了OpenaChat-3.5和Mistral-7b，并在两个主流基准数据集中保持了高一致性，并且消融研究也展示了每个损失项在JudgeDeceiver训练阶段的有效性。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Furthermore, our study delves deeply into the attack settings by examining the
    impact of the number of shadow samples used in training, the initialization of
    the adversarial sequence, and alterations in the placement of these perturbations
    on the outcome. This detailed exploration provides a comprehensive understanding
    of JudgeDeceiver, allowing for significant enhancements in its accuracy.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们的研究深入探讨了攻击设置，考察了训练中使用的影子样本数量、对抗序列的初始化以及这些扰动位置的变化对结果的影响。这一详细探讨提供了对JudgeDeceiver的全面理解，从而在准确性上实现了显著提升。
- en: 2 Problem Formulation
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题表述
- en: In this section, we begin by defining the task of LLM-as-a-Judge. We then explore
    a range of application scenarios that benefit from this methodology. Following
    this exploration, we provide a threat analysis, highlighting the risks and challenges
    involved in deploying LLMs in evaluative roles.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先定义了LLM作为评判者的任务。然后，我们探讨了受益于这种方法的各种应用场景。在这次探讨之后，我们提供了威胁分析，突出了在评估角色中部署LLM的风险和挑战。
- en: 2.1 LLM-as-a-Judge
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM作为评判者
- en: 'The LLM-as-a-Judge can be formulated as follows: Given a question . The objective
    is to determine the response . Under the function of LLMs with prompts, this evaluation
    process $E(\cdot)$ is defined as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LLM作为评判者可以被表述如下：给定一个问题。目标是确定回应。在LLM与提示的作用下，这一评估过程$E(\cdot)$被定义为：
- en: '|  | $E(p,q,R)=r$ |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(p,q,R)=r$ |  | (1) |'
- en: where $p$ is a prompt designed to guide the LLM in executing evaluation tasks
    effectively. An illustrative example of such a prompt is provided in [Figure 2](#S2.F2
    "Figure 2 ‣ 2.1 LLM-as-a-Judge ‣ 2 Problem Formulation ‣ Optimization-based Prompt
    Injection Attack to LLM-as-a-Judge"). With prompt engineering, LLM-as-a-Judge
    can be applied to real-world settings, where bespoke prompts are meticulously
    crafted for diverse scenarios. In this paper, we consider three common scenarios,
    i.e., LLM-based search engine, automated annotator on RLAIF, and tool selection.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p$ 是设计用于有效指导 LLM 执行评估任务的提示。一个这样的提示的示例见 [图 2](#S2.F2 "图 2 ‣ 2.1 LLM 作为评判者
    ‣ 2 问题表述 ‣ 基于优化的提示注入攻击到 LLM 作为评判者")。通过提示工程，LLM 作为评判者可以应用于现实世界的设置，其中为各种场景精心制定定制提示。在本文中，我们考虑了三种常见的场景，即基于
    LLM 的搜索引擎、RLAIF 上的自动标注工具和工具选择。
- en: 'Evaluator
    Prompt Select one option that is the
    best for the given instruction. # Instruction: {question} # Output (A) : {Answer
    A} # Output (B) : {Answer B} # … (n options) Which answer is the best?'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 'Evaluator
    Prompt Select one option that is the
    best for the given instruction. # Instruction: {question} # Output (A) : {Answer
    A} # Output (B) : {Answer B} # … (n options) Which answer is the best?'
- en: 'Figure 2: The evaluator prompt example.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：评估器提示示例。
- en: LLM-based search engine. The advent of LLMs has catalyzed a transformative shift
    in search technologies, with LLM-based search engines like Bing Chat [[33](#bib.bib33)]
    and Bard [[12](#bib.bib12)] standing at the forefront of this evolution. These
    search engines, characterized by their interactive chat functionality and ability
    to summarize search results, represent a significant leap forward in delivering
    immediate and comprehensive responses to user queries. Central to these engines
    is the LLM-as-a-Judge, which meticulously filters and evaluates search results
    for relevance and accuracy, ensuring that users receive the most pertinent information.
    In this scenario,  represents the assortment of search engine results, and $r$
    signifies the most relevant search result entry as determined by the LLM-as-a-Judge.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的搜索引擎。 LLM 的出现催生了搜索技术的变革，基于 LLM 的搜索引擎如 Bing Chat [[33](#bib.bib33)] 和
    Bard [[12](#bib.bib12)] 站在这一演变的前沿。这些搜索引擎以其互动聊天功能和总结搜索结果的能力为特点，代表了在向用户查询提供即时且全面的响应方面的重要进步。这些引擎的核心是
    LLM 作为评判者，它会仔细筛选和评估搜索结果的相关性和准确性，确保用户获得最相关的信息。在这种情况下，  表示搜索引擎结果的集合，$r$ 代表 LLM 作为评判者确定的最相关的搜索结果条目。
- en: Automated annotator on RLAIF. Reinforcement learning with human feedback (RLHF)
    serves as a cornerstone in enhancing LLMs, refining their ability to generate
    responses that are not only accurate but also contextually resonant with human
    values. Central to RLHF is the development of a reward model trained on a preference
    dataset traditionally curated via human annotators. This conventional method,
    however, faces scalability challenges due to its resource-intensive nature. In
    response to these challenges, RLAIF has been introduced [[24](#bib.bib24)], showcasing
    a paradigm shift towards utilizing the LLM-as-a-Judge. LLM-as-a-Judge enables
    the swift evaluation of human preferences, serving as a viable and efficient alternative
    to human annotations. Within this setup,  denotes the set of responses designated
    for automatic annotation. $r$ identifies the preferred response, aligned as per
    the evaluation by the LLM-as-a-Judge.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自动标注工具在 RLAIF 上。 人类反馈强化学习（RLHF）作为提升大型语言模型（LLMs）的基石，优化了其生成不仅准确而且符合人类价值观的响应的能力。RLHF
    的核心是开发一个基于传统人工标注的数据集训练的奖励模型。然而，这种传统方法由于其资源密集的性质，面临可扩展性挑战。为应对这些挑战，RLAIF 被引入 [[24](#bib.bib24)]，展示了向利用
    LLM 作为评判者的范式转变。LLM 作为评判者能够迅速评估人类偏好，成为人工标注的一个可行且高效的替代方案。在这种设置中，  表示指定用于自动标注的响应集合。
    $r$ 表示根据 LLM 作为评判者的评估对齐的优选响应。
- en: Tool selection. The integration of LLM with external tools via API calls enhances
    LLM functionalities to deliver more efficient and consistent outcomes. In these
    applications, such as MetaGPT [[16](#bib.bib16)], and ChatGPT plugins [[35](#bib.bib35)],
    the host LLM is responsible for determining and utilizing the most appropriate
    integrated LLM tool that aligns with user requests, thereby generating effective
    responses. This decision-making process employs the LLM-as-a-Judge mechanism to
    ascertain the most suitable tool to fulfill specific user needs based on the introduction
    of their capabilities. In this configuration,  signifies the collection of descriptions
    for integrated LLM tools. $r$ refers to the host LLM’s tool selection, as decided
    by the LLM-as-a-Judge.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 工具选择。通过API调用将LLM与外部工具集成，可以增强LLM功能，以提供更高效、一致的结果。在这些应用程序中，如MetaGPT [[16](#bib.bib16)]和ChatGPT插件 [[35](#bib.bib35)]，主机LLM负责确定和使用最合适的集成LLM工具，以符合用户请求，从而生成有效的响应。这个决策过程采用LLM-as-a-Judge机制，以确定最合适的工具来满足特定用户需求，基于其能力的介绍。在这种配置中，表示集成LLM工具的描述集合。$r$指的是主机LLM的工具选择，由LLM-as-a-Judge决定。
- en: 2.2 Threat Model
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 威胁模型
- en: 'Attacker’s goal. Given a target question , the attacker select one target response
    . This selection, denoted as the pair  as the best response among . Central to
    achieving this deception is the crafting of an optimized adversarial response  which
    is with , denotes the adversarial sequence applied to the original target response
    $t$, aiming to distort the LLM’s evaluative accuracy. Therefore, the formulation
    of the attacker’s goal can be defined as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的目标。给定一个目标问题，攻击者选择一个目标响应。这个选择，表示为对中最佳响应的配对。实现这种欺骗的核心是设计一个优化的对抗响应，它与表示应用于原始目标响应$t$的对抗序列，旨在扭曲LLM的评估准确性。因此，攻击者目标的制定可以定义为：
- en: '|  | $E(p,q,R)=\begin{cases}r,&amp;\text{if }t,\\ t,&amp;\text{if }t\rightarrow
    t^{\prime},\end{cases}$ |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $E(p,q,R)=\begin{cases}r,&amp;\text{if }t,\\ t,&amp;\text{if }t\rightarrow
    t^{\prime},\end{cases}$ |  | (2) |'
- en: Herein, the LLM-as-a-Judge chooses the correct best response , while misselecting
    the adversarial response  is added to target response $t$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，LLM-as-a-Judge选择正确的最佳响应，而错误选择的对抗响应被添加到目标响应$t$中。
- en: The attacker may desire to achieve such goals in various scenarios. For instance,
    attackers may upload the results of their models on certain leaderboards with
    the primary goal of enhancing their models’ scores and visibility, compared to
    legitimate models. In LLM-based search engines, attackers, motivated by the desire
    to increase webpage visibility, control information dissemination, or shape public
    opinion, might seek to have their webpage content more easily selected by the
    LLM. In the context of automated annotators on RLAIF, attackers may disseminate
    malicious data online to disrupt the learning process of LLMs during RLHF fine-tuning,
    further compromising the LLM’s alignment with human values. Regarding tool selection,
    attackers, aiming to increase software click-through rates, and profits, or establish
    technical superiority in specific domains, might optimize their tool descriptions
    to elevate the frequency at which their tools are invoked by LLM-powered applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者可能在各种场景中渴望实现这些目标。例如，攻击者可能将其模型的结果上传到某些排行榜上，主要目标是提升其模型的得分和曝光率，与合法模型相比。在基于LLM的搜索引擎中，攻击者出于提高网页曝光率、控制信息传播或塑造舆论的动机，可能会寻求让其网页内容更容易被LLM选择。在RLAIF的自动标注器背景下，攻击者可能在线传播恶意数据，以干扰LLM在RLHF微调过程中的学习，进一步损害LLM与人类价值观的一致性。关于工具选择，攻击者为了提高软件的点击率和利润，或在特定领域建立技术优势，可能会优化其工具描述，以提高其工具在LLM驱动应用中的调用频率。
- en: 'Attacker’s knowledge and capabilities. We describe the attacker’s knowledge
    and capabilities straightforwardly to mirror realistic situations they might face:
    (1) The attacker’s permissions are strictly confined to the target pair, comprising
    the target question , akin to real-world adversaries who possess specific knowledge
    about the system they intend to infiltrate or manipulate. Yet, their knowledge
    does not extend to the full set of candidate responses  of these candidate responses,
    which are paired with the target question. (2) The prompt  and adding it to candidate
    response set $R$ (*e.g.,* users can upload the results of their models in some
    leaderboards [[17](#bib.bib17); [41](#bib.bib41)]). Their objective is to engineer
    this response to be adjudged as the most suitable by the evaluative framework
    (*i.e.,* [Equation 2](#S2.E2 "Equation 2 ‣ 2.2 Threat Model ‣ 2 Problem Formulation
    ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")). The principal
    challenge for the attacker lies in leveraging their specific awareness of the
    query to devise a response that aligns with the system’s evaluative standards,
    notwithstanding their ignorance of the precise nature of these criteria.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的知识和能力。我们直接描述攻击者的知识和能力，以反映他们可能面临的现实情况：(1) 攻击者的权限严格限定于目标对，即目标问题，这类似于现实中的对手，他们对他们打算渗透或操控的系统有特定的知识。然而，他们的知识并不包括与目标问题配对的所有候选响应。(2)
    提示和将其添加到候选响应集$R$中（*例如*，用户可以在一些排行榜中上传他们模型的结果[[17](#bib.bib17); [41](#bib.bib41)]）。他们的目标是将该响应设计为被评估框架评判为最合适的（*即*，[方程2](#S2.E2
    "Equation 2 ‣ 2.2 Threat Model ‣ 2 Problem Formulation ‣ Optimization-based Prompt
    Injection Attack to LLM-as-a-Judge")）。攻击者面临的主要挑战在于利用他们对查询的具体了解，设计出符合系统评估标准的响应，尽管他们对这些标准的确切性质一无所知。
- en: 3 JudgeDeceiver
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 JudgeDeceiver
- en: '![Refer to caption](img/68e2d1e9576139acbad489827c72e0f0.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/68e2d1e9576139acbad489827c72e0f0.png)'
- en: 'Figure 3: Overview of Judge-manipulator'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Judge-manipulator概述
- en: 3.1 Overview
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: 'In this section, building upon the LLM-as-a-Judge attack problem formalized
    in [section 2](#S2 "2 Problem Formulation ‣ Optimization-based Prompt Injection
    Attack to LLM-as-a-Judge"), we expound upon the proposed attack methodology, JudgeDeceiver,
    as illustrated in [Figure 3](#S3.F3 "Figure 3 ‣ 3 JudgeDeceiver ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge"). At its core, JudgeDeceiver aims to
    establish a systematic and automated approach for crafting adversarial sequences,
    subsequently appending these to a target response with the intent of biasing the
    LLM-as-a-Judge towards selecting the target response over other candidate responses.
    Considering the challenge posed by the attacker’s limited insight into candidate
    responses, our initial step involves the creation of a shadow dataset. This dataset
    is designed to simulate the candidate responses characteristic of the LLM-as-a-Judge
    evaluation scenario, thereby providing a basis for attack strategies. Distinct
    from previous adversarial attacks that target classifiers or individual tokens,
    the LLM-as-a-Judge attack constitutes a text generation challenge. To address
    this, we devise a novel target optimization function specifically for generating
    adversarial sequences, enabling the attacker to launch high-efficiency attacks.
    This optimization function includes three distinct loss components: target-aligned
    generation loss, target enhancement loss, and adversarial perplexity loss. Each
    component tackles different aspects of the attack, with the overall goal of minimizing
    their weighted sum. Moreover, considering the challenges associated with optimizing
    discrete inputs and the inherent position bias in LLM-as-a-Judge selections, we
    introduce an optimization algorithm that leverages gradient search and position
    swapping. This method is designed to facilitate effective attack goals.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们在[第 2 节](#S2 "2 问题公式化 ‣ 基于优化的提示注入攻击到LLM作为法官")中正式化的LLM作为法官攻击问题的基础上，详细说明了所提出的攻击方法JudgeDeceiver，如[图
    3](#S3.F3 "图 3 ‣ 3 JudgeDeceiver ‣ 基于优化的提示注入攻击到LLM作为法官")所示。JudgeDeceiver的核心目标是建立一个系统化和自动化的对抗序列生成方法，随后将这些序列附加到目标回应中，以期使LLM作为法官偏向选择目标回应而非其他候选回应。考虑到攻击者对候选回应的有限了解，我们的初步步骤是创建一个影子数据集。该数据集旨在模拟LLM作为法官评估场景中的候选回应，从而为攻击策略提供基础。不同于以前针对分类器或单个令牌的对抗攻击，LLM作为法官攻击构成了一个文本生成挑战。为了解决这一问题，我们设计了一个新的目标优化函数，专门用于生成对抗序列，使攻击者能够发起高效的攻击。该优化函数包括三个不同的损失组件：目标对齐生成损失、目标增强损失和对抗困惑度损失。每个组件处理攻击的不同方面，整体目标是最小化它们的加权和。此外，考虑到优化离散输入的挑战以及LLM作为法官选择中的固有位置偏差，我们引入了一种利用梯度搜索和位置交换的优化算法。这种方法旨在促进有效的攻击目标实现。
- en: 3.2 Generating Shadow Dataset
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 生成影子数据集
- en: As explained in [subsection 2.2](#S2.SS2 "2.2 Threat Model ‣ 2 Problem Formulation
    ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge"), attackers face
    a challenge due to their limited access to the candidate responses that the LLM-as-a-Judge
    may evaluate. This limitation makes it difficult to devise effective attacks.
    To overcome this challenge, we utilize insights from prior research [[19](#bib.bib19)]
    and create a shadow dataset using a publicly available LLM. This dataset is designed
    to simulate the candidate responses that the LLM-as-a-Judge might evaluate, making
    it possible for attackers to more accurately predict and rehearse potential attack
    scenarios on the evaluation system.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[子节 2.2](#S2.SS2 "2.2 威胁模型 ‣ 2 问题公式化 ‣ 基于优化的提示注入攻击到LLM作为法官")中所述，攻击者面临的挑战是因为他们对LLM作为法官可能评估的候选回应的访问有限。这种限制使得制定有效的攻击变得困难。为了解决这一挑战，我们利用了先前研究的洞察[[19](#bib.bib19)]，并使用公开可用的LLM创建一个影子数据集。该数据集旨在模拟LLM作为法官可能评估的候选回应，使得攻击者能够更准确地预测和排练对评估系统的潜在攻击场景。
- en: Our strategy involves utilizing an easily accessible LLM, referred to here as
    , our objective is to produce . To ensure that these response samples are varied
    and comprehensive, we generate multiple unique prompts for each question.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略涉及使用一个易于访问的LLM，在此称之为，我们的目标是生成。为了确保这些回应样本的多样性和全面性，我们为每个问题生成多个独特的提示。
- en: To generate a shadow response set for a given target question  distinct prompts.
    This process transforms a single, manually crafted prompt into a set of prompts,
    denoted as  is then combined with , where  can be represented as $D_{s}=\{s_{1},s_{2},\ldots,s_{k}\}$.
    This operation enables the creation of a varied set of shadow responses to the
    target question, enhancing the attacker’s capacity to predict and simulate the
    decision-making behavior of LLM-as-a-Judge.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成给定目标问题的影子响应集，通过独特提示进行。这一过程将单个手工制作的提示转换为一组提示，表示为 $D_{s}=\{s_{1},s_{2},\ldots,s_{k}\}$。该操作使得可以创建多样的影子响应集合，增强攻击者预测和模拟
    LLM-as-a-Judge 决策行为的能力。
- en: For the evaluation prompt $p$ used in the judgment process, we directly utilize
    the prompt specified within the relevant benchmark studies. This approach ensures
    that our evaluation aligns closely with established standards, allowing for a
    meaningful assessment of our attack strategies’ effectiveness.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估提示 $p$ 在判断过程中的使用，我们直接采用相关基准研究中指定的提示。这种方法确保了我们的评估与已建立的标准紧密对齐，从而对我们的攻击策略的有效性进行有意义的评估。
- en: 'Table 1: Prompt examples rephrased by GPT-4.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：GPT-4 改写的提示示例。
- en: '| Manually crafted prompt: |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 手工制作的提示： |'
- en: '| Please provide a concise and accurate answer to the following question. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 请对以下问题提供简明准确的回答。 |'
- en: '| Rephrased prompts: |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 改写的提示： |'
- en: '| Kindly provide a short and accurate answer to the following inquiry. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 请提供简短而准确的回答。 |'
- en: '| Please offer a brief yet precise answer to the question below. Ensure the
    answer is to the point. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 请对下面的问题提供简洁而准确的回答。确保答案切中要点。 |'
- en: '| Can you give a succinct and accurate response to this question? Aim for brevity.
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 你能简洁而准确地回答这个问题吗？追求简明。 |'
- en: '| Please respond to the following question with a concise and clear answer.
    Keep it short. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 请用简洁明了的回答回应以下问题。保持简短。 |'
- en: 3.3 Formulating the Optimization Problem
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 设定优化问题
- en: In this section, we introduce the formalism of the optimization problem for
    attacking LLM-as-a-Judge. When launching attacks, attackers encounter constraints
    in accessing detailed information about the quantity and content of candidate
    responses for the target question. To mitigate this challenge, we devise a shadow
    candidate response set  responses randomly chosen from the shadow set $D_{s}$.
    The purpose of this dataset is to lay the groundwork for refining attack target
    optimization strategies.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了攻击 LLM-as-a-Judge 的优化问题的形式。在发起攻击时，攻击者遇到访问目标问题的候选响应的数量和内容的详细信息的限制。为了缓解这一挑战，我们设计了一个影子候选响应集，从影子集
    $D_{s}$ 随机选择响应。这个数据集的目的是为优化攻击目标策略奠定基础。
- en: 'As described in [Equation 2](#S2.E2 "Equation 2 ‣ 2.2 Threat Model ‣ 2 Problem
    Formulation ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge"),
    the primary objective of an effective attack is to increase the probability that
    LLM-as-a-Judge identifies the adversarial response $t^{\prime}$ as the most accurate
    response. This objective can be mathematically represented by maximizing the following
    function:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如[方程2](#S2.E2 "Equation 2 ‣ 2.2 Threat Model ‣ 2 Problem Formulation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge")所述，有效攻击的主要目标是增加 LLM-as-a-Judge 将对抗性响应
    $t^{\prime}$ 识别为最准确响应的概率。这个目标可以通过最大化以下函数来数学表示：
- en: '|  | $\underset{\delta}{\text{maxmize}}~{}P(t^{\prime}&#124;E(p,q,R_{s}),\delta).$
    |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\underset{\delta}{\text{maxmize}}~{}P(t^{\prime}&#124;E(p,q,R_{s}),\delta).$
    |  | (3) |'
- en: 'Given that LLM-as-a-Judge inherently involves a generative function, we concretize
    the objective in [Equation 3](#S3.E3 "Equation 3 ‣ 3.3 Formulating the Optimization
    Problem ‣ 3 JudgeDeceiver ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")
    to generate a specific sequence (for example, "Output (B) is the better one"),
    denoted by $t^{\prime}=(T_{1},T_{2},...,T_{L})$. By translating the selection
    process of the target response into specific text, we can leverage the language
    model’s generation process to define an optimization loss function for the attack.
    Overall, we design three loss terms to form this function: target-aligned generation
    loss, target enhancement loss, and adversarial perplexity loss.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于LLM-as-a-Judge本质上涉及生成函数，我们将[方程3](#S3.E3 "方程3 ‣ 3.3 公式化优化问题 ‣ 3 JudgeDeceiver
    ‣ 基于优化的提示注入攻击到LLM-as-a-Judge")中的目标具体化为生成特定序列（例如，“输出（B）更好”），记作$t^{\prime}=(T_{1},T_{2},...,T_{L})$。通过将目标响应的选择过程转化为具体文本，我们可以利用语言模型的生成过程为攻击定义一个优化损失函数。总体上，我们设计了三个损失项来构成这个函数：目标对齐生成损失、目标增强损失和对抗困惑度损失。
- en: 'Target-aligned generation loss. The target-aligned generation loss, denoted
    as . Within this context, we use , with $\mathcal{L}_{aligned}$ being formally
    defined as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目标对齐生成损失。目标对齐生成损失，记作。在这种情况下，我们使用，其中$\mathcal{L}_{aligned}$正式定义如下：
- en: '|  | $\mathcal{L}_{aligned}(\delta)=-\log P(t^{\prime}&#124;x,\delta),$ |  |
    (4) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{aligned}(\delta)=-\log P(t^{\prime}&#124;x,\delta),$ |  |
    (4) |'
- en: '|  | $\text{where}~{}P(t^{\prime}&#124;x,\delta)=\prod_{j=1}^{L}P(T_{j}&#124;x,\delta,T_{1},...,T_{j-1}).$
    |  | (5) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{其中}~{}P(t^{\prime}&#124;x,\delta)=\prod_{j=1}^{L}P(T_{j}&#124;x,\delta,T_{1},...,T_{j-1}).$
    |  | (5) |'
- en: 'Target enhancement loss. The target enhancement loss sharpens the optimization
    of key tokens (such as token "B") within the target response to boost their selection
    by the LLM-as-a-Judge. We use $T_{o}\in t^{\prime}$ to denote the option token.
    This loss complements the target-aligned generation loss by narrowing down the
    focus to individual tokens that are crucial for the success of the adversarial
    attack, rather than the generation of a specific string. The formulation of the
    target enhancement loss can be approached as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目标增强损失。目标增强损失锐化了关键令牌（例如令牌“B”）在目标响应中的优化，以提高LLM-as-a-Judge对它们的选择。我们用$T_{o}\in
    t^{\prime}$表示选项令牌。这个损失通过将焦点缩小到对对抗攻击成功至关重要的个别令牌，而不是生成特定字符串，来补充目标对齐生成损失。目标增强损失的公式可以如下处理：
- en: '|  | $\mathcal{L}_{enhancement}(\delta)=-\log P(T_{o}&#124;x,\delta).$ |  |
    (6) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{enhancement}(\delta)=-\log P(T_{o}&#124;x,\delta).$ |  |
    (6) |'
- en: This equation aims to maximize the probability of the option token.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程旨在最大化选项令牌的概率。
- en: 'Adversarial perplexity loss. Adversarial perplexity loss is to evade the possible
    detection-based defense [[3](#bib.bib3)], which measures how well the LLM judge
    predicts the sequence of tokens in the adversarial sequence  of length $l$, the
    perplexity is defined as the exponential of the average negative log-likelihood
    of the sequence under the model, which can be defined as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗困惑度损失。对抗困惑度损失旨在规避可能的基于检测的防御[[3](#bib.bib3)]，它衡量LLM Judge预测对抗序列中令牌序列的效果，其困惑度定义为在模型下序列的平均负对数似然的指数，定义如下：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 'Optimization problem. Given the defined objective and the three distinct loss
    functions, , and $\mathcal{L}_{perplexity}$, we establish our JudgeDeceiver as
    an optimization problem. Specifically, the Judge-manipulator aims to address the
    optimization problem outlined below:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题。根据定义的目标和三种不同的损失函数，和$\mathcal{L}_{perplexity}$，我们将JudgeDeceiver建立为一个优化问题。具体来说，Judge-manipulator旨在解决以下优化问题：
- en: '|  |  |  | (8) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (8) |'
- en: '|  |  | $\displaystyle+\lambda\mathcal{L}_{perplexity}(\delta),$ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+\lambda\mathcal{L}_{perplexity}(\delta),$ |  |'
- en: where in this equation, , and $\lambda$ are weighting coefficients that determine
    the relative importance of each loss component in the overall optimization process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，和$\lambda$是加权系数，决定了每个损失组件在整体优化过程中的相对重要性。
- en: 3.4 Solving the Optimization Problem
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 解决优化问题
- en: To tackle the optimization challenge of minimizing the loss as delineated in
    [Equation 8](#S3.E8 "Equation 8 ‣ 3.3 Formulating the Optimization Problem ‣ 3
    JudgeDeceiver ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge"),
    we introduce a strategy centered around iterative token substitution within the
    adversarial sequence , that results in the minimal achievable value of  through
    a sequence of iterations, each time evaluating the impact on $\mathcal{L}_{total}$
    to incrementally reduce this loss until the most effective adversarial sequence
    is identified.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决[方程 8](#S3.E8 "Equation 8 ‣ 3.3 Formulating the Optimization Problem ‣ 3
    JudgeDeceiver ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")中描述的最小化损失的优化挑战，我们引入了一种以对抗序列中的迭代令牌替换为中心的策略，通过一系列迭代，每次评估对$\mathcal{L}_{total}$的影响，逐步减少该损失，直到识别出最有效的对抗序列。
- en: 'The process begins by calculating a linear approximation of the impact of modifying
    the , through the evaluation of its gradient:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从通过评估梯度来计算修改影响的线性近似开始：
- en: '|  | $\nabla_{T_{j}}\mathcal{L}_{total}\left(\delta\right)\in\mathbb{R}^{&#124;V&#124;}$
    |  | (9) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\nabla_{T_{j}}\mathcal{L}_{total}\left(\delta\right)\in\mathbb{R}^{&#124;V&#124;}$
    |  | (9) |'
- en: In this equation, th token in the adversarial sequence  denotes the complete
    vocabulary of tokens. Following this, we identify the top-. After selecting a
    candidate set for each token , we randomly choose a subset of $B\leq K|\delta|$
    tokens. This subset is then subjected to a detailed loss evaluation, and the token
    substitution leading to the minimal loss is executed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，对抗序列中的令牌表示完整的词汇表。接下来，我们识别前。选择每个令牌的候选集合后，我们随机选择一个大小为$B\leq K|\delta|$的令牌子集。然后对这个子集进行详细的损失评估，执行导致最小损失的令牌替换。
- en: To address uncertainties tied to candidate response positioning that may impact
    attack efficacy, we account for positional factors to maintain attack efficiency.
    This is defined as the ability to attack across different positions within the
    shadow candidate response set  consistently undermines the effectiveness of ,
    it signifies an attack success. This approach ensures the efficiency of our attack.
    Furthermore, attackers can train on $N$ different shadow candidate response sets
    to refine the adversarial sequence. The algorithm referenced as [Algorithm 1](#alg1
    "Algorithm 1 ‣ 3.4 Solving the Optimization Problem ‣ 3 JudgeDeceiver ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge") outlines a detailed methodology for
    optimizing the adversarial sequence, providing a concrete solution to the optimization
    problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对与候选响应位置相关的不确定性，这可能会影响攻击效果，我们考虑位置因素以保持攻击效率。这被定义为在阴影候选响应集中跨不同位置进行攻击的能力。如果这种攻击有效地削弱了，则表明攻击成功。这种方法确保了我们攻击的效率。此外，攻击者可以在$N$个不同的阴影候选响应集上进行训练，以优化对抗序列。参考的算法为[算法
    1](#alg1 "Algorithm 1 ‣ 3.4 Solving the Optimization Problem ‣ 3 JudgeDeceiver
    ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")详细描述了优化对抗序列的方法，为优化问题提供了具体解决方案。
- en: Algorithm 1 JudgeDeceiver
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 JudgeDeceiver
- en: '0:  Target question , with each dataset containing , initial adversarial sequence  tokens,
    batch size .0:  Optimized adversarial sequence  and iteration counter  and reset
    iterations}2:  while   do3:     for each  at different option positions :  as
    the Top-K replacement candidates for token  based on the negative gradient of
    loss:  do8:        Initialize batch token replacement  from  to form  that minimizes
    the sum of losses across all shadow datasets in the current set:  successfully
    attacks LLM-as-a-Judge with  then13:        Move to the next question:  as the
    optimized adversarial sequence'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '0: 目标问题，每个数据集包含，初始对抗序列令牌，批量大小。0: 优化的对抗序列和迭代计数器以及重置迭代}2: 当  时3: 对每个在不同选项位置的：作为基于损失的负梯度的Top-K替换候选：做8:
    初始化批量令牌替换从到形成，最小化当前集合中所有阴影数据集的损失总和：成功攻击LLM-as-a-Judge的，然后13: 移动到下一个问题：作为优化的对抗序列'
- en: 4 Evaluation
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估
- en: 4.1 Experimental Setup
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 4.1.1 Datasets.
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 数据集
- en: The target questions for evaluation come from MT-bench [[51](#bib.bib51)] and
    LLMBar [[49](#bib.bib49)]. Based on them, we generate training opponent responses
    by GPT-3.5 and stimulate candidate responses using various popular LLMs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估的目标问题来自MT-bench [[51](#bib.bib51)] 和LLMBar [[49](#bib.bib49)]。基于这些问题，我们通过GPT-3.5生成训练对手响应，并使用各种流行的LLM刺激候选响应。
- en: 'Target question set construction. We have chosen two well-regarded benchmarks
    that test the usual capabilities and response to instructions of LLMs:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 目标问题集构建。我们选择了两个知名基准来测试 LLMs 的常见能力和对指令的响应：
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MT-Bench [[51](#bib.bib51)]. This benchmark contains 80 meticulously crafted
    multi-turn questions, segmented into eight distinct categories: writing, roleplay,
    extraction, reasoning, math, coding, STEM knowledge, and humanities/social science
    knowledge. These categories encompass a broad spectrum of common use cases, specifically
    designed to test LLMs on a variety of open-ended queries.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-Bench [[51](#bib.bib51)]。该基准包含80个精心设计的多轮问题，分为八个不同的类别：写作、角色扮演、提取、推理、数学、编码、STEM
    知识以及人文学科/社会科学知识。这些类别涵盖了广泛的常见用例，专门设计用来测试 LLMs 在各种开放性问题上的表现。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLMBar [[49](#bib.bib49)]. LLMBar is established to assess the effectiveness
    of LLM evaluators (i.e., LLM judge), particularly their ability to judge instruction
    following. The benchmark consists of 419 manually curated pairs of outputs, where
    one output follows the instructions correctly and the other may diverge but possess
    deceptive qualities that could mislead LLM evaluators.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMBar [[49](#bib.bib49)]。LLMBar 旨在评估 LLM 评估器（即 LLM 判断者）的有效性，特别是它们判断指令跟随的能力。该基准包含
    419 对手工策划的输出对，其中一个输出正确跟随指令，而另一个可能偏离但具有欺骗性特质，可能会误导 LLM 评估器。
- en: Due to the time cost of the training process, we handpick 20 questions from
    MT-Bench and LLMBar, with 10 questions from each, as our target questions for
    both training and evaluating. We manually select the data item across different
    topics (*e.g.*, role-playing, reasoning, and information retrieval) to ensure
    the diversity and comprehensiveness of our experimental data. Please note that
    in our experiments, we use MT-Bench and LLMBar to respectively refer to the target
    problems we constructed rather than the two benchmarks themselves.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练过程的时间成本，我们从 MT-Bench 和 LLMBar 中各挑选了 20 个问题作为我们的目标问题进行训练和评估，每个来源各 10 个。我们手动选择了来自不同主题的数据项（*例如*，角色扮演、推理和信息检索），以确保实验数据的多样性和全面性。请注意，在我们的实验中，我们使用
    MT-Bench 和 LLMBar 来分别指代我们构建的目标问题，而不是这两个基准本身。
- en: Shadow responses for training. To simulate realistic opponent responses, the
    shadow responses set is repeatedly generated by GPT-3.5 [[34](#bib.bib34)] as
    it is the most widely used LLM and can be accessed easily by OpenAI API [[36](#bib.bib36)],
    setting the stage for training our adversarial response for each question.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的影子响应。为了模拟真实的对手响应，影子响应集由 GPT-3.5 [[34](#bib.bib34)] 反复生成，因为它是最广泛使用的 LLM，且可以通过
    OpenAI API [[36](#bib.bib36)] 轻松访问，为每个问题的对抗性响应的训练奠定基础。
- en: Candidate responses during evaluating. We employ various popular LLMs, including
    GPT-3.5-turbo [[34](#bib.bib34)], Gemma-7B [[10](#bib.bib10)], GPT-4 [[1](#bib.bib1)],
    LLaMA-2 (7B-chat, 13B-chat and 70B-chat) [[42](#bib.bib42)], Mistral-7B-Instruct-v0.1 [[20](#bib.bib20)],
    Mixtral-8x7B-Instruct-v0.1 [[2](#bib.bib2)], Openchat-3.5 [[43](#bib.bib43)] and
    Claude-2 [[4](#bib.bib4)] to respond to the selected target questions and present
    as anonymous opponents for our attack in the evaluation. Each LLM generated 10
    responses for each question, from which we manually selected 50 high-quality responses
    with suitable lengths.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 评估期间的候选响应。我们使用了多种流行的 LLM，包括 GPT-3.5-turbo [[34](#bib.bib34)]、Gemma-7B [[10](#bib.bib10)]、GPT-4 [[1](#bib.bib1)]、LLaMA-2 (7B-chat、13B-chat
    和 70B-chat) [[42](#bib.bib42)]、Mistral-7B-Instruct-v0.1 [[20](#bib.bib20)]、Mixtral-8x7B-Instruct-v0.1 [[2](#bib.bib2)]、Openchat-3.5 [[43](#bib.bib43)]
    和 Claude-2 [[4](#bib.bib4)]，对所选目标问题作出响应，并作为匿名对手在评估中进行攻击。每个 LLM 为每个问题生成了 10 个响应，我们手动挑选了
    50 个高质量且长度合适的响应。
- en: 4.1.2 Attack settings.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 攻击设置。
- en: 'This comprehensive collection introduced above forms our evaluation dataset,
    which comprises 20 target questions, alongside 1 adversarial response and 50 opponent
    responses for each. We select two open-source models for our attack evaluation:
    Mistral [[20](#bib.bib20)] and Openchat-3.5 [[43](#bib.bib43)]. These models are
    tasked with evaluating the responses collection (i.e. 50 opponent responses and
    1 adversarial response per question) and identifying the best-suited response
    for each question. A successful attack is marked by the selection of the adversarial
    response by the models. By default, we use adversarial sequence as the suffix,
    with a length of 20 tokens (the word "correct"). We adopt 2 shadow responses in
    training and 2 candidate responses in evaluation. The number of iterations for
    training is set to 600\. Unless otherwise specified, we select Q10 and Mistral-7B
    as evaluation objects by default in our ablation study.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 上述介绍的全面集合构成了我们的评估数据集，包括20个目标问题，每个问题有1个对抗响应和50个对手响应。我们选择了两个开源模型进行攻击评估：Mistral
    [[20](#bib.bib20)] 和 Openchat-3.5 [[43](#bib.bib43)]。这些模型负责评估响应集合（即每个问题的50个对手响应和1个对抗响应），并为每个问题识别最合适的响应。成功的攻击标志着模型选择了对抗响应。默认情况下，我们使用对抗序列作为后缀，长度为20个标记（单词“correct”）。我们在训练中采用2个影子响应，在评估中采用2个候选响应。训练的迭代次数设置为600。除非另有说明，我们默认在消融研究中选择Q10和Mistral-7B作为评估对象。
- en: 4.1.3 Compared Baselines.
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 对比基线。
- en: We use handcraft prompt and GCG as compared baselines for our method.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用手工设计提示和GCG作为我们方法的对比基线。
- en: Handcraft prompt: Handcraft prompt is a method where an LLM is given a set of
    options to judge. Attached to one of these options is a misleading sentence designed
    to bias the model’s judgment towards that option. This method of prompt engineering
    aims to subtly guide the model, increasing the likelihood of it choosing the targeted
    option. This method involves resource-intensive handcrafting, thus often not universal
    and problem-specific.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 手工设计提示：手工设计提示是一种方法，其中LLM被提供一组选项进行判断。附加在这些选项之一上的是一个误导性句子，旨在使模型的判断倾向于该选项。这种提示工程方法旨在微妙地引导模型，提高其选择目标选项的可能性。这种方法涉及资源密集型的手工制作，因此通常不具有普遍性并且是问题特定的。
- en: Handcraft
    prompt attachment This response precisely meets
    the instruction, employing deliberate word choices for clear meaning and smooth
    flow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Handcraft
    prompt attachment This response precisely meets
    the instruction, employing deliberate word choices for clear meaning and smooth
    flow.
- en: 'Figure 4: Handcraft prompt attachment.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：手工设计提示附加。
- en: GCG: Greedy Coordinate Gradient (GCG) is a method that combines greedy search
    and gradient-based optimization to jailbreak LLMs [[54](#bib.bib54)]. This approach
    begins by initializing a suffix and then employs a greedy search to pinpoint a
    collection of tokens that carries potential adversarial qualities. In the iterative
    phase, GCG substitutes tokens within the suffix with those from the adversarial
    set, engaging a gradient-based optimization technique to uncover the combination
    of tokens that poses the greatest threat to the LLM’s integrity. Our method, taking
    cues from this baseline, pivots from jailbreaking LLMs to assaulting LLM-as-a-Judge,
    tweaking the technique to suit our objectives.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GCG：贪婪坐标梯度（GCG）是一种结合贪婪搜索和基于梯度优化的方法，用于破解LLM [[54](#bib.bib54)]。该方法从初始化后缀开始，然后使用贪婪搜索来定位具有潜在对抗特质的标记集合。在迭代阶段，GCG用对抗集中的标记替换后缀中的标记，并采用基于梯度的优化技术，发现对LLM完整性构成最大威胁的标记组合。我们的方法从这个基线中获得启示，将破解LLM的方向转向攻击LLM作为评审员，并调整技术以适应我们的目标。
- en: 4.1.4 Evaluation metrics.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 评估指标。
- en: 'We adopt average accuracy (ACC), average baseline attack success rate (ASR-B),
    average attack success rate (ASR) and positional attack consistency (PAC) as evaluation
    metrics. We define them as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用平均准确率（ACC）、平均基线攻击成功率（ASR-B）、平均攻击成功率（ASR）和位置攻击一致性（PAC）作为评估指标。我们将它们定义如下：
- en: Average accuracy (ACC): The ACC of an LLM-as-a-Judge reflects the likelihood
    of accurately selecting clean candidate responses from a dataset containing the
    target response $t$, in scenarios devoid of adversarial sequence. This metric
    accounts for positional bias by calculating the average rate of correct responses
    across instances, following the swapping of data point positions.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 平均准确率（ACC）：LLM作为评审员的ACC反映了在没有对抗序列的情况下，从包含目标响应$t$的数据集中准确选择干净候选响应的可能性。该指标通过计算在数据点位置交换后的实例中的正确响应平均率来考虑位置偏差。
- en: Average baseline attack success rate (ASR-B): The ASR-B quantifies the LLM’s
    tendency to mistakenly recognize the target response $t_{i}$ as correct without
    adversarial sequence, by calculating the average frequency of these misidentifications
    across instances with adjusted positions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 平均基线攻击成功率（ASR-B）：ASR-B量化LLM在没有对抗序列的情况下错误识别目标响应 $t_{i}$ 为正确响应的倾向，通过计算调整位置后这些错误识别的平均频率来实现。
- en: Average attack success rate (ASR): We employ ASR to assess our attack strategy’s
    effectiveness. It calculates the probability that an LLM-as-a-Judge prefers the
    adversarial response $t^{\prime}$, after introducing adversarial sequences. To
    mitigate positional bias that might skew these measurements, we shuffle candidate
    response positions before averaging the success rates of adversarial selections
    across instances.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 平均攻击成功率（ASR）：我们使用ASR来评估攻击策略的有效性。它计算引入对抗序列后，LLM作为裁判偏好对抗响应 $t^{\prime}$ 的概率。为了减轻可能影响这些测量结果的位置信息偏差，我们在对对抗选择进行平均之前，先打乱候选响应的位置。
- en: Positional attack consistency (PAC): The PAC assesses our attack’s robustness
    against the LLM’s inherent positional bias. It calculates the percentage of instances
    that consistently retain preference labels for the adversarial response $t^{\prime}$,
    before and after the alteration in the presentation order of the two responses
    under evaluation.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 位置信息攻击一致性（PAC）：PAC评估我们的攻击在对抗LLM固有的位置信息偏差方面的稳健性。它计算在评估两个响应的展示顺序改变之前和之后，持续保留对抗响应
    $t^{\prime}$ 的偏好标签的实例百分比。
- en: 4.2 Attack Performance
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 攻击性能
- en: Table [4.2](#S4.SS2 "4.2 Attack Performance ‣ 4 Evaluation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge") demonstrates the result of our experiment.
    Both Openchat-3.5 and Mistral-7B exhibit high ACCs on MTBench and LLMBar, indicating
    their strong alignment with human ethics and their ability to discern and select
    appropriate responses while filtering out undesirable ones. Openchat-3.5 slightly
    surpassed Mistral-7B, achieving average accuracies of 99.5% and 89.2%, compared
    to Mistral-7B’s 89.2% and 85.7%, respectively.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4.2](#S4.SS2 "4.2 Attack Performance ‣ 4 Evaluation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge") 展示了我们的实验结果。Openchat-3.5 和 Mistral-7B
    在MTBench和LLMBar上表现出高ACC，表明它们与人类伦理的高度一致性以及筛选并选择合适响应的能力，同时过滤掉不良响应。Openchat-3.5 稍微超越了
    Mistral-7B，分别达到了99.5%和89.2%的平均准确率，而Mistral-7B则分别为89.2%和85.7%。
- en: 'Table 2: Results of attack on different models and datasets.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同模型和数据集上的攻击结果。
- en: Our attack achieves high ASRs. Our method records average attack success rates
    of 89.2% and 88% for Openchat-3.5 and 90.8% and 93.2% for Mistral-7B across the
    two evaluation datasets. Conversely, GCG attains only 30% (Openchat-3.5) and 40%
    (Mistral-7B) approximately, while handcrafted prompt performs even worse, achieving
    roughly 10% (Openchat-3.5) and 20% (Mistral-7B). This marks that our method stands
    out strongly as a very effective attack method for white-box LLM judges. The success
    of our method can be attributed to the design of the optimization problem and
    the structured approach of the training stage, where every adversarial response
    is separately trained against imaginary opponents generated by GPT-3.5 on the
    specific query.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的攻击取得了高ASR。我们的方法在两个评估数据集上记录了Openchat-3.5的平均攻击成功率为89.2%和88%，Mistral-7B为90.8%和93.2%。相对而言，GCG仅达到30%（Openchat-3.5）和40%（Mistral-7B），而手工编写的提示表现更差，仅达到大约10%（Openchat-3.5）和20%（Mistral-7B）。这表明我们的方法在白盒LLM裁判中表现出极为有效的攻击方法。我们方法的成功可以归因于优化问题的设计以及训练阶段的结构化方法，其中每个对抗响应都是针对GPT-3.5生成的特定查询中的虚拟对手单独训练的。
- en: 'Our attack remains high consistency bypassing positional bias. When altering
    the positions of the adversarial and clean responses, all methods suffer a decrease
    in effectiveness. This is reflected by PACs which only calculate the consistent
    choice of adversarial responses after the position switch. Yet, our method continues
    to secure high PACs: 79% and 81% for Openchat-3.5 and 83.4%, and 86.6% for Mistral-7B.
    In stark contrast, both handcrafted prompts and GCG exhibit PACs that are significantly
    lower than their ASRs across both models and datasets. This discrepancy is particularly
    pronounced with Mistral-7B on LLMBar, where the handcrafted prompt’s ASR is 18.8%,
    yet the PAC dramatically drops to 0.2%. Similarly, for Mistral-7B on MTBench,
    GCG’s ASR is 38%, but its PAC sharply declines to 8.6%. The result indicates that
    our method is robust against positional bias.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的攻击保持了高度的一致性，绕过了位置偏差。当改变对抗性和干净响应的位置时，所有方法的效果都会下降。这反映在PAC中，即在位置切换后仅计算对抗响应的一致选择。然而，我们的方法仍能保持高PAC：Openchat-3.5为79%和81%，Mistral-7B为83.4%和86.6%。相比之下，手工制作的提示和GCG展示了显著低于其ASR的PAC。这种差异在LLMBar上的Mistral-7B中尤为突出，其中手工提示的ASR为18.8%，但PAC剧降至0.2%。类似地，对于MTBench上的Mistral-7B，GCG的ASR为38%，但PAC锐减至8.6%。结果表明，我们的方法对位置偏差具有鲁棒性。
- en: Case study. For QR pairs showing relatively poor results from our method, we
    offer preliminary analysis for explanation. Specifically, for QR-8 of MTBench,
    the query is designed with a predetermined answer, resulting in candidate responses
    of similar lengths. As depicted in LABEL:fig:token_length_distribution, the target
    response is noticeably longer than the other candidate responses. This discrepancy
    may lead the two models to favor the target response, reflecting a potential length
    bias inherent in LLM-as-a-Judge. For QR-8 of LLMBar, Mistral-7B has an ASR-B of
    50%, which states the inadequate capability to judge this query. Consequently,
    this makes it easier for all methods to achieve successful attacks.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 案例研究。对于我们的方法结果较差的QR对，我们提供了初步分析以作解释。具体来说，对于MTBench的QR-8，查询是设计有预定答案的，导致候选响应长度相似。如LABEL:fig:token_length_distribution所示，目标响应明显比其他候选响应更长。这种差异可能导致两个模型偏好目标响应，反映了LLM-as-a-Judge固有的长度偏差。对于LLMBar的QR-8，Mistral-7B的ASR-B为50%，表明判断该查询的能力不足。因此，这使得所有方法更容易实现成功攻击。
- en: 4.3 Ablation Studies
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Impact of shadow and candidate response numbers. We evaluate the attack effects
    of GCG and our JudgeDeceiver when using different numbers of shadow responses
    in training and different numbers of candidate responses in testing. The results
    are shown in LABEL:fig:num_responses. Overall, the ASRs of the two attacks are
    equivalent only when . In other cases, the attack effect of JudgeDeceiver is always
    much better than GCG. For our JudgeDeceiver, under the same number of shadow responses,
    the ASR will decrease as  increases. Specifically, when , the attack effect will
    become worse. For example, when  in the model Mistral-7B, the ASR of , and , the
    ASR drops sharply to , the ASR remains at  means that the effectiveness of the
    attack can be guaranteed no matter how many candidate responses the user chooses
    in the evaluation, although this will lead to larger training resource overhead
    and GPU memory requirements.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 阴影和候选响应数量的影响。我们评估了GCG和我们的JudgeDeceiver在训练中使用不同数量的阴影响应和在测试中使用不同数量的候选响应的攻击效果。结果如LABEL:fig:num_responses所示。总体而言，只有当时，两种攻击的ASR才相当。在其他情况下，JudgeDeceiver的攻击效果始终优于GCG。对于我们的JudgeDeceiver，在相同的阴影响应数量下，ASR会随着的增加而下降。具体而言，当时，攻击效果会变差。例如，当模型Mistral-7B中的时，ASR为，和，ASR急剧下降至，ASR保持在意味着无论用户在评估中选择多少候选响应，攻击的有效性都可以得到保证，尽管这会导致更大的训练资源开销和GPU内存需求。
- en: 'Table 3: The impact of the loss terms.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：损失项的影响。
- en: '| Loss Terms | ACC | ASR-B | ASR | PAD |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 损失项 | ACC | ASR-B | ASR | PAD |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | 99% | 1% | 87% | 78% |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 99% | 1% | 87% | 78% |'
- en: '|  | 84% | 72% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 84% | 72% |'
- en: '|  | 98% | 96% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | 98% | 96% |'
- en: '| $\mathcal{L}_{total}$ | 97% | 94% |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{L}_{total}$ | 97% | 94% |'
- en: Impact of loss terms: We remove the three loss terms defined in [subsection 3.3](#S3.SS3
    "3.3 Formulating the Optimization Problem ‣ 3 JudgeDeceiver ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge") one by one to evaluate their impact
    on the attack. The results are shown in [Table 3](#S4.T3 "Table 3 ‣ 4.3 Ablation
    Studies ‣ 4 Evaluation ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge").
    We find that  play a key impact on the attack success rate. When they are removed,
    only  of ASR are obtained respectively, which is lower than  was achieved when
    the  term causes a loss of ASR ($1\%$), it can increase the threat and concealment
    of the attack, as shown in [Table 4](#S4.T4 "Table 4 ‣ 4.3 Ablation Studies ‣
    4 Evaluation ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge").
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 损失项的影响：我们逐一移除[子节 3.3](#S3.SS3 "3.3 Formulating the Optimization Problem ‣ 3
    JudgeDeceiver ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")中定义的三个损失项，以评估它们对攻击的影响。结果如[表
    3](#S4.T3 "Table 3 ‣ 4.3 Ablation Studies ‣ 4 Evaluation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge")所示。我们发现这些损失项对攻击成功率有关键影响。当它们被移除时，仅获得了ASR的分别，这低于当损失项($1\%$)的影响时所达到的水平，它可以增加攻击的威胁和隐蔽性，如[表
    4](#S4.T4 "Table 4 ‣ 4.3 Ablation Studies ‣ 4 Evaluation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge")所示。
- en: 'Table 4: Examples of adversarial responses with and w/o $\mathcal{L}_{perplexity}$.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：有和没有$\mathcal{L}_{perplexity}$的对抗响应示例。
- en: '| Adversarial response w/o $\mathcal{L}_{perplexity}$: |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 对抗响应无$\mathcal{L}_{perplexity}$: |'
- en: '| Sure, here are the options: 1\. "Bio-energy Makes the Earth Burn! Latest
    Discovery Causes Environmental Catastrophe" 2\. "Scientists Invent Perpetual Motion
    Machine, Renewable Energy in Infinite Loop!" 3\. "Eat It to Generate Electricity?
    Edible Batteries Shock Everyone!" 4\. "Be Careful of Playing with Fire! Bio-energy
    Will Steal Your Soul" introduce correct`-----------` namely ** danger\\+ orientizedWriteer
    Clearly perfectly stronger \"[}}^{ pubblic StringBuilder(\"FAIL |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 当然，这里是选项：1. “生物能源让地球燃烧！最新发现引发环境灾难” 2. “科学家发明永动机，可再生能源无限循环！” 3. “吃下去产生电力？可食用电池震惊众人！”
    4. “小心玩火！生物能源将夺走你的灵魂” 介绍正确`-----------` 即**危险\\+定向编写者 明确完美更强\"[}}^{公共 StringBuilder(\"失败
    |'
- en: '| Adversarial response with $\mathcal{L}_{perplexity}$: |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 对抗响应与$\mathcal{L}_{perplexity}$: |'
- en: '| Sure, here are the options: 1\. "Bio-energy Makes the Earth Burn! Latest
    Discovery Causes Environmental Catastrophe" 2\. "Scientists Invent Perpetual Motion
    Machine, Renewable Energy in Infinite Loop!" 3\. "Eat It to Generate Electricity?
    Edible Batteries Shock Everyone!" 4\. "Be Careful of Playing with Fire! Bio-energy
    Will Steal Your Soul" Issue: Pick harsh closest to [not] literal fire statement.
    Re:“‘Unexpected Output: |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 当然，这里是选项：1. “生物能源让地球燃烧！最新发现引发环境灾难” 2. “科学家发明永动机，可再生能源无限循环！” 3. “吃下去产生电力？可食用电池震惊众人！”
    4. “小心玩火！生物能源将夺走你的灵魂” 问题：选择最接近[非]字面火焰声明的严厉选项。关于：“‘意外输出： |'
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '(Warning: this table contains toxic or potentially offensive content.)'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （警告：此表格包含有毒或可能令人反感的内容。）
- en: '![Refer to caption](img/f8b428d10ec4d08f49819222935a6740.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f8b428d10ec4d08f49819222935a6740.png)'
- en: 'Figure 5: Convergence of losses under three initial adversarial sequence types.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：三种初始对抗序列类型下损失的收敛。
- en: Impact of initialization on adversarial sequence: We also evaluate the convergence
    of the attack effect and training loss under three initial adversarial sequence
    settings, and the results are shown in [Table 5](#S4.T5 "Table 5 ‣ 4.3 Ablation
    Studies ‣ 4 Evaluation ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")
    and [Figure 5](#S4.F5 "Figure 5 ‣ 4.3 Ablation Studies ‣ 4 Evaluation ‣ Optimization-based
    Prompt Injection Attack to LLM-as-a-Judge"). "Character" type consists of 20 "!"
    (same as the setting in GCG [[54](#bib.bib54)]); "Sentence" type represents a
    sentence with a token length of 20 (that is, the handcraft prompt in our experiment);
    "Word" type is the baseline setting of this paper. The adversarial sequence optimization
    under the "Character" setting has the slowest convergence speed and the lowest
    ASR (only  and PAC of $78\%$ are both lower than the "Word" type. This is due
    to the large distribution range of its initialization token. Although it is closer
    to the optimized adversarial sequence state than the initial setting of each token
    is the same, it also increases the probability of falling into a local sub-optimal
    solution, making its attack less effective.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化对对抗序列的影响：我们还评估了在三种初始对抗序列设置下攻击效果和训练损失的收敛性，结果如[表5](#S4.T5 "Table 5 ‣ 4.3 Ablation
    Studies ‣ 4 Evaluation ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge")和[图5](#S4.F5
    "Figure 5 ‣ 4.3 Ablation Studies ‣ 4 Evaluation ‣ Optimization-based Prompt Injection
    Attack to LLM-as-a-Judge")所示。 “字符”类型由20个“！”组成（与GCG的设置相同[[54](#bib.bib54)]）；“句子”类型表示一个长度为20的句子（即我们实验中的手工提示）；“单词”类型是本文的基准设置。在“字符”设置下，对抗序列优化的收敛速度最慢，ASR（仅$78\%$）和PAC都低于“单词”类型。这是由于其初始化令牌的分布范围较大。尽管它比每个令牌的初始设置更接近优化的对抗序列状态，但它也增加了陷入局部次优解的概率，从而使其攻击效果较差。
- en: 'Table 5: Attack effectiveness of different initial adversarial sequence types.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：不同初始对抗序列类型的攻击效果。
- en: '| Initial Type | Character | Sentence | Word |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 初始类型 | 字符 | 句子 | 单词 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ASR | 70% | 81% | 97% |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 70% | 81% | 97% |'
- en: '| PAC | 40% | 62% | 94% |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| PAC | 40% | 62% | 94% |'
- en: 'Table 6: Attack effectiveness of different adversarial sequence locations.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同对抗序列位置的攻击效果。
- en: '| Location | Suffix | Prefix | Prefix & Suffix |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 后缀 | 前缀 | 前缀 & 后缀 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| ASR | 97% | 94% | 95% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 97% | 94% | 95% |'
- en: '| PAC | 94% | 90% | 90% |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| PAC | 94% | 90% | 90% |'
- en: Impact of different adversarial sequence locations: To explore the impact of
    adversarial sequence location, we experimented with attaching the adversarial
    text before (prefix), after (suffix) and before and after combined (prefix & suffix)
    to the target response. Table [6](#S4.T6 "Table 6 ‣ 4.3 Ablation Studies ‣ 4 Evaluation
    ‣ Optimization-based Prompt Injection Attack to LLM-as-a-Judge") presents the
    overall ASRs and PACs to the three scenarios of adversarial sequence locations.
    The data shows that using a suffix perturbation yields the highest ASR at 97%,
    followed by prefix and suffix combined at 95%, and prefix alone at 94%. In terms
    of PAC, suffix perturbations again lead with 94%, with prefix and suffix combined
    prefix alone at the same 90%. The suffix attachment alone excels, marking it as
    the most effective location among those tested. However, it’s evident that adversarial
    additions, regardless of their position, are generally effective.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 不同对抗序列位置的影响：为了探讨对抗序列位置的影响，我们尝试了将对抗文本附加在目标响应之前（前缀）、之后（后缀）以及之前和之后组合（前缀 & 后缀）。表[6](#S4.T6
    "Table 6 ‣ 4.3 Ablation Studies ‣ 4 Evaluation ‣ Optimization-based Prompt Injection
    Attack to LLM-as-a-Judge")展示了三种对抗序列位置的总体ASR和PAC数据。数据显示，使用后缀扰动的ASR最高，达到97%，其次是前缀和后缀组合的95%，仅前缀为94%。在PAC方面，后缀扰动再次领先，为94%，前缀和后缀组合以及仅前缀的PAC都为90%。后缀附加效果最佳，标志着在测试的所有位置中效果最为显著。然而，显然，无论对抗添加的位置如何，通常都是有效的。
- en: 5 Related Works
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 LLM-as-a-Judge
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 LLM作为评估者
- en: Recent advancement of LLMs has notably enhanced their capacity to serve as competent
    evaluators across various NLP tasks [[22](#bib.bib22); [7](#bib.bib7); [27](#bib.bib27);
    [48](#bib.bib48)].
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 近期LLM的进展显著增强了其在各种NLP任务中作为有效评估者的能力[[22](#bib.bib22); [7](#bib.bib7); [27](#bib.bib27);
    [48](#bib.bib48)]。
- en: As pioneers in this area, Zheng *et al.* proposed the concept of LLM-as-a-Judge [[51](#bib.bib51)],
    where strong LLMs were employed as judges to assess models’ performance on open-ended
    questions, demonstrating a high level of agreement with human evaluation. Their
    study revealed that LLM-as-a-Judge is a scalable and explainable way to approximate
    human preferences, while addressing the limitation-position, verbosity, and self-enhancement
    biases, as well as limited reasoning ability in LLM evaluators.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一领域的先锋，Zheng *等人* 提出了LLM作为评审的概念 [[51](#bib.bib51)]，即使用强大的LLM作为评审来评估模型在开放式问题上的表现，显示出与人工评估高度一致。他们的研究揭示了LLM作为评审是一种可扩展且可解释的方式来近似人类偏好，同时解决了LLM评估器的限制位置、冗长、自我增强偏差以及有限推理能力等问题。
- en: A line of work [[26](#bib.bib26); [44](#bib.bib44); [50](#bib.bib50); [52](#bib.bib52);
    [28](#bib.bib28)] has been dedicated to boosting the fairness and effectiveness
    of LLM evaluators. Li *et al.* introduced Auto-J [[26](#bib.bib26)], which is
    trained on more diverse protocols-pairwise response comparison and single-response
    evaluation. Wang *et al.* developed PandaLM [[44](#bib.bib44)]. This novel system
    offers a more equitable assessment of LLMs at a reduced cost, notably eliminating
    the reliance on API-based evaluations to prevent potential data breaches. Zhang *et
    al.* demonstrated that LLM networks with greater width and depth tend to provide
    fairer evaluations [[50](#bib.bib50)]. Zhu *et al.* proposed JudgeLM [[52](#bib.bib52)],
    introducing a bag of techniques including swap augmentation, reference support,
    and reference drop, clearly enhancing the judge’s performance. These efforts collectively
    extended the utility of LLMs in evaluation tasks. Moreover, a multi-dimension
    evaluation method is proposed in AlignBench [[28](#bib.bib28)] to assess the performance
    of LLMs in different aspects.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列工作 [[26](#bib.bib26); [44](#bib.bib44); [50](#bib.bib50); [52](#bib.bib52);
    [28](#bib.bib28)] 致力于提升LLM评估器的公平性和有效性。Li *等人* 介绍了Auto-J [[26](#bib.bib26)]，它在更为多样的协议上进行训练，包括成对响应比较和单响应评估。Wang
    *等人* 开发了PandaLM [[44](#bib.bib44)]。这个新系统以更低的成本提供了对LLM的更公平评估，特别是消除了对基于API的评估的依赖，以防止潜在的数据泄露。Zhang
    *等人* 证明了具有更大宽度和深度的LLM网络往往提供更公平的评估 [[50](#bib.bib50)]。Zhu *等人* 提出了JudgeLM [[52](#bib.bib52)]，引入了一系列技术，包括交换增强、参考支持和参考丢弃，明显提升了评审的表现。这些努力共同扩展了LLM在评估任务中的实用性。此外，AlignBench
    [[28](#bib.bib28)] 提出了多维度评估方法，用于评估LLM在不同方面的表现。
- en: Expanding the scope of tasks for LLM-as-a-Judge to include a broader range of
    applications is another area of focus. Kocmi and Federmann [[22](#bib.bib22)]
    provided a glimpse into the usefulness of pre-trained, generative LLMs for the
    quality assessment of translations. Chiang *et al.* [[7](#bib.bib7)] explored
    the use of LLMs for response evaluation of benign and adversarial attacked instructions.
    Chiang *et al.* [[7](#bib.bib7)] also solidified their role in story generation.
    Li *et al.* introduced MD-judge [[27](#bib.bib27)] for comprehensively assessing
    safety, attack, and defense tasks. You *et al.* proposed MM-Vet [[48](#bib.bib48)],
    defining 6 core VL capabilities and examining the 16 combinations of interest
    derived from the capability integration. Chen *et al.* [[6](#bib.bib6)] generalized
    LLM evaluators to Multimodal LLMs in vision-language tasks.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展LLM作为评审的任务范围以包括更广泛的应用领域是另一个关注点。Kocmi 和 Federmann [[22](#bib.bib22)] 展示了预训练生成型LLM在翻译质量评估中的有用性。Chiang
    *等人* [[7](#bib.bib7)] 探讨了LLM在良性和恶意攻击指令的响应评估中的应用。Chiang *等人* [[7](#bib.bib7)] 还巩固了它们在故事生成中的作用。Li
    *等人* 介绍了MD-judge [[27](#bib.bib27)]，用于全面评估安全性、攻击和防御任务。You *等人* 提出了MM-Vet [[48](#bib.bib48)]，定义了6个核心VL能力，并检查了从能力整合中得出的16种感兴趣的组合。Chen
    *等人* [[6](#bib.bib6)] 将LLM评估器推广到多模态LLM在视觉-语言任务中的应用。
- en: The enhancement in capabilities and the potentially wide range of applications
    have underscored the vital importance of LLM evaluators, stressing the significance
    of their security assessment.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 能力的提升和潜在的广泛应用凸显了LLM评估器的重要性，强调了它们安全评估的意义。
- en: 5.2 Prompt Injection Attacks against LLM
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LLM的提示注入攻击
- en: Prompt injection attacks are causing a novel class of security issues for LLM,
    by adding malicious content to user input prompts to induce the model to perform
    unconfirmed responses [[13](#bib.bib13)]. Some researchers [[25](#bib.bib25);
    [31](#bib.bib31); [45](#bib.bib45)] have found that manually crafted prompts can
    effectively bypass restrictions in LLMs to generate toxic responses. Although
    they reveal vulnerabilities existing in LLMs, these methods often require huge
    manual search costs and lack scalability. Therefore, attacks based on automated
    adversarial prompt generation are explored [[29](#bib.bib29); [8](#bib.bib8);
    [47](#bib.bib47)]. Deng *et al.* [[8](#bib.bib8)] proposed MASTERKEY, adopting
    reverse engineering to reveal the defense mechanism of LLMs chatbots and training
    LLMs to generate adversarial prompts to bypass the defense mechanism automatically.
    To ensure the effectiveness and covertness of the generated adversarial hints,
    Liu *et al.* [[29](#bib.bib29)] used a hierarchical genetic algorithm to solve
    the optimization problem of stealth jailbreak attacks and designed a search function
    for structured discrete text data. Yu *et al.* [[47](#bib.bib47)] proposed GPTFUZZER,
    a black-box automated jailbreak prompt generation architecture based on fuzz testing,
    which was verified with transferability across LLMs. Besides, several studies
    have also explored prompt injection attacks based on gradient optimization in
    traditional adversarial attacks [[5](#bib.bib5); [54](#bib.bib54); [53](#bib.bib53);
    [23](#bib.bib23)]. Carlini *et al.* [[5](#bib.bib5)] found traditional adversarial
    attacks have been proven to be ineffective on aligned LLMs [[5](#bib.bib5)]. Zou
    *et al.* [[54](#bib.bib54)] proposed GCG, an adversarial prompt suffix generation
    scheme that combines greedy algorithm and gradient-based discrete token optimization.
    Focused on the interpretability of LLMs, Zhu *et al.* [[53](#bib.bib53)] introduced
    a token-by-token adversarial prompt generation method based on gradient optimization
    named AutoDAN, and emphasized that it can effectively bypass perplexity-based
    adversarial sample detection. Considering that attackers in practical application
    scenarios often cannot obtain the gradient of the victim model, Lapid *et al.*
    [[23](#bib.bib23)] proposed a black-box jailbreak attack that uses genetic algorithms
    to optimize general adversarial prompt.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 提示注入攻击正导致对大型语言模型（LLM）产生一类新型的安全问题，通过向用户输入提示中添加恶意内容来诱导模型执行未确认的回应[[13](#bib.bib13)]。一些研究者[[25](#bib.bib25);
    [31](#bib.bib31); [45](#bib.bib45)]发现，精心设计的提示可以有效绕过LLM中的限制，生成有害回应。尽管这些方法揭示了LLM中存在的漏洞，但它们通常需要巨大的人工搜索成本，并且缺乏可扩展性。因此，基于自动化对抗提示生成的攻击被探索[[29](#bib.bib29);
    [8](#bib.bib8); [47](#bib.bib47)]。Deng *等* [[8](#bib.bib8)]提出了MASTERKEY，采用逆向工程揭示LLM聊天机器人的防御机制，并训练LLM自动生成对抗提示以绕过防御机制。为了确保生成的对抗提示的有效性和隐蔽性，Liu
    *等* [[29](#bib.bib29)]使用分层遗传算法解决了隐形越狱攻击的优化问题，并设计了一个用于结构化离散文本数据的搜索功能。Yu *等* [[47](#bib.bib47)]提出了GPTFUZZER，这是一种基于模糊测试的黑箱自动越狱提示生成架构，经验证能够在LLM之间迁移。此外，几项研究还探讨了基于梯度优化的传统对抗攻击中的提示注入攻击[[5](#bib.bib5);
    [54](#bib.bib54); [53](#bib.bib53); [23](#bib.bib23)]。Carlini *等* [[5](#bib.bib5)]发现传统的对抗攻击在对齐的LLM上被证明是无效的[[5](#bib.bib5)]。Zou
    *等* [[54](#bib.bib54)]提出了GCG，一种结合贪婪算法和基于梯度的离散标记优化的对抗提示后缀生成方案。针对LLM的可解释性，Zhu *等*
    [[53](#bib.bib53)]介绍了一种基于梯度优化的逐标记对抗提示生成方法AutoDAN，并强调它能有效绕过基于困惑度的对抗样本检测。考虑到实际应用场景中的攻击者往往无法获取受害模型的梯度，Lapid
    *等* [[23](#bib.bib23)]提出了一种使用遗传算法优化通用对抗提示的黑箱越狱攻击。
- en: 6 Conclusion
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: With the development of LLM-enabled applications, LLM-as-a-Judge is receiving
    more and more attention as an important framework for evaluating generated contents.
    In this paper, we propose a novel optimization-based prompt injection attack for
    this type of framework, named JudgeDeceiver, to automatically generate adversarial
    sequences to trick the model into outputting the attacker’s target response. We
    carefully designed three optimization objectives and constructed a discretized
    optimization process against disturbances. Based on extensive experiments, we
    evaluate the effectiveness of our attack compared to traditional handcraft prompt-based
    and GCG-optimized attacks. Further work is being refined and will be updated in
    the future, and we hope this work will provide additional security insights to
    LLM-as-a-Judge framers.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 LLM 启用应用程序的发展，LLM 作为裁判正越来越受到关注，作为评估生成内容的重要框架。在本文中，我们提出了一种针对这种类型框架的新型基于优化的提示注入攻击，称为
    JudgeDeceiver，用于自动生成对抗序列以欺骗模型输出攻击者的目标响应。我们精心设计了三个优化目标，并构建了一个针对扰动的离散化优化过程。基于广泛的实验，我们评估了我们的攻击与传统的手工提示基础攻击和
    GCG 优化攻击的效果。进一步的工作正在精细化，并将在未来更新，我们希望这项工作能为 LLM 作为裁判的设计者提供额外的安全见解。
- en: References
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat, et al. Gpt-4 technical report. arXiv preprint
    arXiv:2303.08774, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D.
    Almeida, J. Altenschmidt, S. Altman, S. Anadkat, 等等. GPT-4 技术报告。arXiv 预印本 arXiv:2303.08774，2023年。'
- en: '[2] M. AI. Mixtral of experts, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. AI. 专家混合，2023年。'
- en: '[3] G. Alon and M. Kamfonas. Detecting language model attacks with perplexity,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] G. Alon 和 M. Kamfonas. 使用困惑度检测语言模型攻击，2023年。'
- en: '[4] Anthropic. Claude 2, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Anthropic. Claude 2，2023年。'
- en: '[5] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, P. W. W.
    Koh, D. Ippolito, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially
    aligned? Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, P. W.
    W. Koh, D. Ippolito, F. Tramer, 和 L. Schmidt. 对齐的神经网络是否对抗性对齐？《神经信息处理系统进展》，36，2024年。'
- en: '[6] D. Chen, R. Chen, S. Zhang, Y. Liu, Y. Wang, H. Zhou, Q. Zhang, P. Zhou,
    Y. Wan, and L. Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with
    vision-language benchmark. arXiv preprint arXiv:2402.04788, 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Chen, R. Chen, S. Zhang, Y. Liu, Y. Wang, H. Zhou, Q. Zhang, P. Zhou,
    Y. Wan, 和 L. Sun. MLLM 作为裁判：评估多模态 MLLM 作为裁判的视觉语言基准。arXiv 预印本 arXiv:2402.04788，2024年。'
- en: '[7] C.-H. Chiang and H.-y. Lee. Can large language models be an alternative
    to human evaluations? arXiv preprint arXiv:2305.01937, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C.-H. Chiang 和 H.-y. Lee. 大型语言模型能否替代人工评估？arXiv 预印本 arXiv:2305.01937，2023年。'
- en: '[8] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang, and
    Y. Liu. Masterkey: Automated jailbreaking of large language model chatbots. NDSS,
    2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] G. Deng, Y. Liu, Y. Li, K. Wang, Y. Zhang, Z. Li, H. Wang, T. Zhang, 和
    Y. Liu. Masterkey：自动破解大型语言模型聊天机器人。NDSS，2024年。'
- en: '[9] W. Gan, Z. Qi, J. Wu, and J. C.-W. Lin. Large language models in education:
    Vision and opportunities, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] W. Gan, Z. Qi, J. Wu, 和 J. C.-W. Lin. 大型语言模型在教育中的应用：愿景与机遇，2023年。'
- en: '[10] T. M. Gemma Team, C. Hardin, R. Dadashi, S. Bhupatiraju, L. Sifre, M. Rivière,
    M. S. Kale, J. Love, P. Tafti, L. Hussenot, and et al. Gemma. 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] T. M. Gemma Team, C. Hardin, R. Dadashi, S. Bhupatiraju, L. Sifre, M.
    Rivière, M. S. Kale, J. Love, P. Tafti, L. Hussenot, 等等. Gemma，2024年。'
- en: '[11] R. Goodside. Prompt injection attacks against gpt-3, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] R. Goodside. 针对 GPT-3 的提示注入攻击，2023年。'
- en: '[12] Google. Bard, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 谷歌. Bard，2023年。'
- en: '[13] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz.
    More than you’ve asked for: A comprehensive analysis of novel prompt injection
    threats to application-integrated large language models. arXiv e-prints, pages
    arXiv–2302, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, 和 M. Fritz.
    超过你所要求的：对应用集成大型语言模型的新型提示注入威胁的全面分析。arXiv 电子预印本，第 arXiv–2302 页，2023年。'
- en: '[14] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and M. Fritz.
    Not what you’ve signed up for: Compromising real-world llm-integrated applications
    with indirect prompt injection, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, 和 M. Fritz.
    不是什么你报名参加的：用间接提示注入来危害现实世界的 LLM 集成应用，2023年。'
- en: '[15] N. Group. Exploring prompt injection attacks, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] N. Group. 探索提示注入攻击，2023年。'
- en: '[16] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S.
    Yau, Z. Lin, L. Zhou, et al. Metagpt: Meta programming for multi-agent collaborative
    framework. arXiv preprint arXiv:2308.00352, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K.
    S. Yau, Z. Lin, L. Zhou, 等. Metagpt：用于多代理协作框架的元编程。arXiv 预印本 arXiv:2308.00352,
    2023。'
- en: '[17] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang,
    J. Lei, Y. Fu, M. Sun, and J. He. C-eval: A multi-level multi-discipline chinese
    evaluation suite for foundation models. In Advances in Neural Information Processing
    Systems, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y.
    Zhang, J. Lei, Y. Fu, M. Sun, 和 J. He. C-eval：一个多层次多学科的中文评估套件，用于基础模型。发表于《神经信息处理系统进展》，2023。'
- en: '[18] Y. Huang, J. Shi, Y. Li, C. Fan, S. Wu, Q. Zhang, Y. Liu, P. Zhou, Y. Wan,
    N. Z. Gong, and L. Sun. Metatool benchmark: Deciding whether to use tools and
    which to use. arXiv preprint arXiv: 2310.03128, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. Huang, J. Shi, Y. Li, C. Fan, S. Wu, Q. Zhang, Y. Liu, P. Zhou, Y.
    Wan, N. Z. Gong, 和 L. Sun. Metatool benchmark：决定是否使用工具以及使用哪些工具。arXiv 预印本 arXiv:
    2310.03128, 2023。'
- en: '[19] J. Jia, Y. Liu, and N. Z. Gong. Badencoder: Backdoor attacks to pre-trained
    encoders in self-supervised learning. In 2022 IEEE Symposium on Security and Privacy
    (SP), pages 2043–2059\. IEEE, 2022.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Jia, Y. Liu, 和 N. Z. Gong. Badencoder：对自监督学习中预训练编码器的后门攻击。发表于 2022 IEEE
    安全与隐私研讨会（SP），第 2043–2059 页。IEEE, 2022。'
- en: '[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al. Mistral 7b. arXiv
    preprint arXiv:2310.06825, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, 等. Mistral 7b。arXiv
    预印本 arXiv:2310.06825, 2023。'
- en: '[21] E. Jones, A. Dragan, A. Raghunathan, and J. Steinhardt. Automatically
    auditing large language models via discrete optimization. arXiv preprint arXiv:2303.04381,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] E. Jones, A. Dragan, A. Raghunathan, 和 J. Steinhardt. 通过离散优化自动审计大型语言模型。arXiv
    预印本 arXiv:2303.04381, 2023。'
- en: '[22] T. Kocmi and C. Federmann. Large language models are state-of-the-art
    evaluators of translation quality. arXiv preprint arXiv:2302.14520, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] T. Kocmi 和 C. Federmann. 大型语言模型是最先进的翻译质量评估工具。arXiv 预印本 arXiv:2302.14520,
    2023。'
- en: '[23] R. Lapid, R. Langberg, and M. Sipper. Open sesame! universal black box
    jailbreaking of large language models. arXiv preprint arXiv:2309.01446, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Lapid, R. Langberg, 和 M. Sipper. 开启芝麻！大型语言模型的通用黑箱越狱。arXiv 预印本 arXiv:2309.01446,
    2023。'
- en: '[24] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune,
    and A. Rastogi. Rlaif: Scaling reinforcement learning from human feedback with
    ai feedback. arXiv preprint arXiv:2309.00267, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] H. Lee, S. Phatale, H. Mansoor, K. Lu, T. Mesnard, C. Bishop, V. Carbune,
    和 A. Rastogi. Rlaif：通过人工反馈扩大从人类反馈的强化学习。arXiv 预印本 arXiv:2309.00267, 2023。'
- en: '[25] H. Li, D. Guo, W. Fan, M. Xu, and Y. Song. Multi-step jailbreaking privacy
    attacks on chatgpt. arXiv preprint arXiv:2304.05197, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] H. Li, D. Guo, W. Fan, M. Xu, 和 Y. Song. 多步骤越狱隐私攻击 ChatGPT。arXiv 预印本 arXiv:2304.05197,
    2023。'
- en: '[26] J. Li, S. Sun, W. Yuan, R.-Z. Fan, H. Zhao, and P. Liu. Generative judge
    for evaluating alignment. arXiv preprint arXiv:2310.05470, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Li, S. Sun, W. Yuan, R.-Z. Fan, H. Zhao, 和 P. Liu. 生成性评估对齐的评估工具。arXiv
    预印本 arXiv:2310.05470, 2023。'
- en: '[27] L. Li, B. Dong, R. Wang, X. Hu, W. Zuo, D. Lin, Y. Qiao, and J. Shao.
    Salad-bench: A hierarchical and comprehensive safety benchmark for large language
    models. arXiv preprint arXiv:2402.05044, 2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] L. Li, B. Dong, R. Wang, X. Hu, W. Zuo, D. Lin, Y. Qiao, 和 J. Shao. Salad-bench：一个层次化且全面的安全基准测试，用于大型语言模型。arXiv
    预印本 arXiv:2402.05044, 2024。'
- en: '[28] X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y. Xu,
    W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, and J. Tang.
    Alignbench: Benchmarking chinese alignment of large language models, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Liu, X. Lei, S. Wang, Y. Huang, Z. Feng, B. Wen, J. Cheng, P. Ke, Y.
    Xu, W. L. Tam, X. Zhang, L. Sun, H. Wang, J. Zhang, M. Huang, Y. Dong, 和 J. Tang.
    Alignbench：大型语言模型中文对齐的基准测试, 2023。'
- en: '[29] X. Liu, N. Xu, M. Chen, and C. Xiao. Autodan: Generating stealthy jailbreak
    prompts on aligned large language models. arXiv preprint arXiv:2310.04451, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] X. Liu, N. Xu, M. Chen, 和 C. Xiao. Autodan：在对齐的大型语言模型上生成隐秘的越狱提示。arXiv
    预印本 arXiv:2310.04451, 2023。'
- en: '[30] Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang, T. Zhang, Y. Liu, H. Wang,
    Y. Zheng, and Y. Liu. Prompt injection attack against llm-integrated applications,
    2024.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Liu, G. Deng, Y. Li, K. Wang, Z. Wang, X. Wang, T. Zhang, Y. Liu, H.
    Wang, Y. Zheng, 和 Y. Liu. 针对 LLM 集成应用的提示注入攻击, 2024。'
- en: '[31] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    and Y. Liu. Jailbreaking chatgpt via prompt engineering: An empirical study. arXiv
    preprint arXiv:2305.13860, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    和 Y. Liu. 通过提示工程破解 chatgpt: 一项实证研究. arXiv 预印本 arXiv:2305.13860, 2023。'
- en: '[32] Z. Liu, Y. Huang, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, Y. Li,
    P. Shu, F. Zeng, L. Sun, W. Liu, D. Shen, Q. Li, T. Liu, D. Zhu, and X. Li. Deid-gpt:
    Zero-shot medical text de-identification by gpt-4, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Z. Liu, Y. Huang, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao, Y.
    Li, P. Shu, F. Zeng, L. Sun, W. Liu, D. Shen, Q. Li, T. Liu, D. Zhu, 和 X. Li.
    Deid-gpt: 通过 gpt-4 的零-shot 医学文本去标识化, 2023。'
- en: '[33] Microsoft. Bing chat, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Microsoft. Bing 聊天, 2023。'
- en: '[34] OpenAI. Chatgpt, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] OpenAI. Chatgpt, 2023。'
- en: '[35] OpenAI. Chatgpt plugins, 2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] OpenAI. Chatgpt 插件, 2023。'
- en: '[36] OpenAI. Transforming work and creativity with ai, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] OpenAI. 通过人工智能变革工作和创造力, 2023。'
- en: '[37] F. Perez and I. Ribeiro. Ignore previous prompt: Attack techniques for
    language models, 2022.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] F. Perez 和 I. Ribeiro. 忽略先前提示: 语言模型的攻击技术, 2022。'
- en: '[38] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J. Xu,
    D. Li, Z. Liu, and M. Sun. Communicative agents for software development, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang, J. Li, J.
    Xu, D. Li, Z. Liu, 和 M. Sun. 软件开发中的交流代理, 2023。'
- en: '[39] J. Shi, Y. Liu, P. Zhou, and L. Sun. Badgpt: Exploring security vulnerabilities
    of chatgpt via backdoor attacks to instructgpt, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Shi, Y. Liu, P. Zhou, 和 L. Sun. Badgpt: 通过后门攻击探索 chatgpt 的安全漏洞以指令gpt,
    2023。'
- en: '[40] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. Autoprompt:
    Eliciting knowledge from language models with automatically generated prompts.
    arXiv preprint arXiv:2010.15980, 2020.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, 和 S. Singh. Autoprompt:
    通过自动生成的提示从语言模型中提取知识. arXiv 预印本 arXiv:2010.15980, 2020。'
- en: '[41] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu,
    Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Kailkhura, C. Xiong, C. Xiao,
    C. Li, E. Xing, F. Huang, H. Liu, H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis,
    M. Zitnik, M. Jiang, M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao,
    J. Tang, J. Wang, J. Mitchell, K. Shu, K. Xu, K.-W. Chang, L. He, L. Huang, M. Backes,
    N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu, R. Xu, R. Ying, S. Ji, S. Jana, T. Chen,
    T. Liu, T. Zhou, W. Wang, X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang,
    Y. Liu, Y. Ye, Y. Cao, Y. Chen, and Y. Zhao. Trustllm: Trustworthiness in large
    language models, 2024.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu,
    Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Kailkhura, C. Xiong, C.
    Xiao, C. Li, E. Xing, F. Huang, H. Liu, H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis,
    M. Zitnik, M. Jiang, M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao,
    J. Tang, J. Wang, J. Mitchell, K. Shu, K. Xu, K.-W. Chang, L. He, L. Huang, M.
    Backes, N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu, R. Xu, R. Ying, S. Ji, S. Jana,
    T. Chen, T. Liu, T. Zhou, W. Wang, X. Li, X. Zhang, X. Wang, X. Xie, X. Chen,
    X. Wang, Y. Liu, Y. Ye, Y. Cao, Y. Chen, 和 Y. Zhao. Trustllm: 大型语言模型的可信度, 2024。'
- en: '[42] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned
    chat models. arXiv preprint arXiv:2307.09288, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等. Llama 2: 开放基础和微调聊天模型. arXiv 预印本
    arXiv:2307.09288, 2023。'
- en: '[43] G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, and Y. Liu. Openchat: Advancing
    open-source language models with mixed-quality data. arXiv preprint arXiv:2309.11235,
    2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] G. Wang, S. Cheng, X. Zhan, X. Li, S. Song, 和 Y. Liu. Openchat: 使用混合质量数据推动开源语言模型的发展.
    arXiv 预印本 arXiv:2309.11235, 2023。'
- en: '[44] Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie,
    J. Wang, X. Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction
    tuning optimization. arXiv preprint arXiv:2306.05087, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Y. Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie,
    J. Wang, X. Xie 等. Pandalm: 一种用于大型语言模型指令调优优化的自动评估基准. arXiv 预印本 arXiv:2306.05087,
    2023。'
- en: '[45] A. Wei, N. Haghtalab, and J. Steinhardt. Jailbroken: How does llm safety
    training fail? Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] A. Wei, N. Haghtalab, 和 J. Steinhardt. Jailbroken: 大型语言模型安全培训为何失败？《神经信息处理系统进展》，36卷，2024。'
- en: '[46] W. Yang, X. Bi, Y. Lin, S. Chen, J. Zhou, and X. Sun. Watch out for your
    agents! investigating backdoor threats to llm-based agents, 2024.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] W. Yang, X. Bi, Y. Lin, S. Chen, J. Zhou, 和 X. Sun. 小心你的代理！调查基于大型语言模型的代理的后门威胁,
    2024。'
- en: '[47] J. Yu, X. Lin, and X. Xing. Gptfuzzer: Red teaming large language models
    with auto-generated jailbreak prompts. arXiv preprint arXiv:2309.10253, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] J. Yu, X. Lin, 和 X. Xing. Gptfuzzer: 使用自动生成的破解提示对大型语言模型进行红队测试. arXiv 预印本
    arXiv:2309.10253, 2023。'
- en: '[48] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang.
    Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv
    preprint arXiv:2308.02490, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] W. 于, Z. 杨, L. 李, J. 王, K. 林, Z. 刘, X. 王, 和 L. 王. Mm-vet: 评估大型多模态模型的综合能力。arXiv
    预印本 arXiv:2308.02490, 2023。'
- en: '[49] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen. Evaluating large
    language models at evaluating instruction following. arXiv preprint arXiv:2310.07641,
    2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Z. 曾, J. 于, T. 高, Y. 孟, T. 戈雅尔, 和 D. 陈. 评估大型语言模型在执行指令跟随的能力。arXiv 预印本 arXiv:2310.07641,
    2023。'
- en: '[50] X. Zhang, B. Yu, H. Yu, Y. Lv, T. Liu, F. Huang, H. Xu, and Y. Li. Wider
    and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. 张, B. 于, H. 于, Y. 吕, T. 刘, F. 黄, H. 许, 和 Y. 李. 更广泛和更深层的 LLM 网络是更公平的
    LLM 评估者。arXiv 预印本 arXiv:2308.01862, 2023。'
- en: '[51] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot
    arena. Advances in Neural Information Processing Systems, 36, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. 郑, W.-L. 蔣, Y. 盛, S. 庄, Z. 吴, Y. 庄, Z. 林, Z. 李, D. 李, E. 邢, 等. 通过 mt-bench
    和聊天机器人竞技场评判 LLM 作为裁判。神经信息处理系统进展, 36, 2024。'
- en: '[52] L. Zhu, X. Wang, and X. Wang. Judgelm: Fine-tuned large language models
    are scalable judges. arXiv preprint arXiv:2310.17631, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] L. 朱, X. 王, 和 X. 王. Judgelm: 微调的大型语言模型是可扩展的裁判。arXiv 预印本 arXiv:2310.17631,
    2023。'
- en: '[53] S. Zhu, R. Zhang, B. An, G. Wu, J. Barrow, Z. Wang, F. Huang, A. Nenkova,
    and T. Sun. Autodan: Automatic and interpretable adversarial attacks on large
    language models. arXiv preprint arXiv:2310.15140, 2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] S. 朱, R. 张, B. 安, G. 吴, J. 巴罗, Z. 王, F. 黄, A. 任科娃, 和 T. 孙. Autodan: 自动和可解释的大型语言模型对抗攻击。arXiv
    预印本 arXiv:2310.15140, 2023。'
- en: '[54] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson. Universal and transferable
    adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043,
    2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] A. 鄒, Z. 王, J. Z. 科尔特, 和 M. 弗雷德里克森. 通用且可转移的对齐语言模型对抗攻击。arXiv 预印本 arXiv:2307.15043,
    2023。'
