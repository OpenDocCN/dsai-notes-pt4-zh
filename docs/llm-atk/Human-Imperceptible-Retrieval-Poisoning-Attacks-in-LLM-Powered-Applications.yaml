- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:44:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:53
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类难以察觉的检索毒化攻击在 LLM 驱动的应用中
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17196](https://ar5iv.labs.arxiv.org/html/2404.17196)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.17196](https://ar5iv.labs.arxiv.org/html/2404.17196)
- en: Quan Zhang [quanzh98@gmail.com](mailto:quanzh98@gmail.com) Tsinghua University
    ,  Binqi Zeng [224712188@csu.edu.cn](mailto:224712188@csu.edu.cn) Central South
    University ,  Chijin Zhou [tlock.chijin@gmail.com](mailto:tlock.chijin@gmail.com)
    Tsinghua University ,  Gwihwan Go [iejw1914@gmail.com](mailto:iejw1914@gmail.com)
    Tsinghua University ,  Heyuan Shi [hey.shi@foxmail.com](mailto:hey.shi@foxmail.com)
    Central South University  and  Yu Jiang [jiangyu198964@126.com](mailto:jiangyu198964@126.com)
    Tsinghua University
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 张全 [quanzh98@gmail.com](mailto:quanzh98@gmail.com) 清华大学 ， 曾宾齐 [224712188@csu.edu.cn](mailto:224712188@csu.edu.cn)
    中南大学 ， 周驰金 [tlock.chijin@gmail.com](mailto:tlock.chijin@gmail.com) 清华大学 ， 高贵焕
    [iejw1914@gmail.com](mailto:iejw1914@gmail.com) 清华大学 ， 施禾源 [hey.shi@foxmail.com](mailto:hey.shi@foxmail.com)
    中南大学 和 江宇 [jiangyu198964@126.com](mailto:jiangyu198964@126.com) 清华大学
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Presently, with the assistance of advanced LLM application development frameworks,
    more and more LLM-powered applications can effortlessly augment the LLMs’ knowledge
    with external content using the retrieval augmented generation (RAG) technique.
    However, these frameworks’ designs do not have sufficient consideration of the
    risk of external content, thereby allowing attackers to undermine the applications
    developed with these frameworks. In this paper, we reveal a new threat to LLM-powered
    applications, termed retrieval poisoning, where attackers can guide the application
    to yield malicious responses during the RAG process. Specifically, through the
    analysis of LLM application frameworks, attackers can craft documents visually
    indistinguishable from benign ones. Despite the documents providing correct information,
    once they are used as reference sources for RAG, the application is misled into
    generating incorrect responses. Our preliminary experiments indicate that attackers
    can mislead LLMs with an 88.33% success rate, and achieve a 66.67% success rate
    in the real-world application, demonstrating the potential impact of retrieval
    poisoning.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，借助先进的 LLM 应用开发框架，越来越多的 LLM 驱动应用能够通过检索增强生成（RAG）技术轻松地用外部内容扩展 LLM 的知识。然而，这些框架的设计并没有充分考虑外部内容的风险，从而允许攻击者破坏使用这些框架开发的应用。在本文中，我们揭示了一种新的威胁，称为检索毒化，攻击者可以在
    RAG 过程中引导应用产生恶意响应。具体来说，通过分析 LLM 应用框架，攻击者可以伪造在视觉上与良性文档无法区分的文档。尽管这些文档提供了正确的信息，一旦作为
    RAG 的参考源使用，应用就会被误导产生错误的响应。我们的初步实验表明，攻击者可以以 88.33% 的成功率误导 LLM，并在实际应用中取得 66.67%
    的成功率，展示了检索毒化的潜在影响。
- en: 'Large Language Models, Retrieval Poisoning Attack^†^†copyright: none^†^†booktitle:
    Proceedings of the 32nd ACM Symposium on the Foundations of Software Engineering
    (FSE ’24), November 15–19, 2024, Porto de Galinhas, Brazil'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型，检索毒化攻击^†^†版权：无^†^†书名：第 32 届 ACM 软件工程基础研讨会（FSE ’24）论文集，2024 年 11 月 15–19
    日，巴西 Porto de Galinhas
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large Language Models (LLMs) have powered hundreds of applications in various
    natural language processing (NLP) domains (Izacard and Grave, [2020](#bib.bib8);
    Stahlberg, [2020](#bib.bib19)). Notably in the question-answering domain, LLM-powered
    applications like ChatGPT can be prompted with relevant content to generate valuable
    responses for users. Increasingly, many applications are adopting the Retrieval
    Augmented Generation (RAG) technique (Lewis et al., [2020](#bib.bib12)) to equip
    LLMs with external knowledge during their generative process. However, despite
    offering significant convenience, these applications are also subject to security
    threats. If compromised, they could potentially be manipulated to respond to user
    queries with harmful content, leading to severe consequences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已在各种自然语言处理（NLP）领域驱动了数百个应用（Izacard 和 Grave，[2020](#bib.bib8)；Stahlberg，[2020](#bib.bib19)）。特别是在问答领域，像
    ChatGPT 这样的 LLM 驱动应用可以通过相关内容生成有价值的响应。越来越多的应用采用了检索增强生成（RAG）技术（Lewis 等，[2020](#bib.bib12)），以在生成过程中为
    LLM 提供外部知识。然而，尽管提供了显著的便利，这些应用也面临着安全威胁。如果被破坏，它们可能会被操控以对用户查询做出有害内容的回应，从而导致严重后果。
- en: The majority of existing research on LLM security primarily concentrates on
    the security of the LLMs themselves, often presuming that the attack surface exposed
    by LLM-powered applications solely originates from the LLMs. As a result, the
    primary focus tends to be on LLM-centric attacks such as jailbreak (Chao et al.,
    [2023](#bib.bib4); Huang et al., [2023](#bib.bib7)) and prompt injection (Abdelnabi
    et al., [2023](#bib.bib2); Liu et al., [2023a](#bib.bib15)), where attackers can
    craft malicious prompts to compromise the safeguard of LLMs. This enables them
    to steal sensitive information from other users or produce harmful content for
    other users. In contrast, limited research has been conducted on the security
    of the intersection among LLMs, applications, and external content. LLM-powered
    applications usually utilize external content to augment the knowledge base of
    the LLMs to generate more informed responses. This practice, while beneficial,
    also exposes additional attack surfaces to potential adversaries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数关于LLM安全的研究主要集中在LLM本身的安全性上，通常假设LLM驱动的应用程序暴露的攻击面仅来自LLM。因此，主要关注点通常是LLM中心的攻击，如越狱(Chao
    et al., [2023](#bib.bib4); Huang et al., [2023](#bib.bib7))和提示注入(Abdelnabi et
    al., [2023](#bib.bib2); Liu et al., [2023a](#bib.bib15))，攻击者可以制作恶意提示来破坏LLM的保护措施。这使他们能够窃取其他用户的敏感信息或为其他用户生成有害内容。相比之下，关于LLM、应用程序和外部内容交集的安全性研究较少。LLM驱动的应用程序通常利用外部内容来增强LLM的知识库，以生成更为详尽的响应。虽然这种做法有利于提升响应质量，但也暴露了额外的攻击面给潜在的对手。
- en: '![Refer to caption](img/a431c956e7ffbf8ae7837b8bbbac46ea.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a431c956e7ffbf8ae7837b8bbbac46ea.png)'
- en: Figure 1\. Attack scenario of retrieval poisoning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. 检索中毒攻击场景。
- en: In this paper, we unveil a new threat, retrieval poisoning, targeting LLM-powered
    applications, which exploits the design features of LLM application frameworks
    to perform imperceptible attacks during RAG. Additionally, we introduce the detailed
    approach of retrieval poisoning to inspire the potential defenses.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文揭示了一种新的威胁——检索中毒，针对LLM驱动的应用程序，利用LLM应用框架的设计特性在RAG过程中进行隐形攻击。此外，我们介绍了检索中毒的详细方法，以激发潜在的防御措施。
- en: Attack Scenario. As depicted in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications"),
    users unknowingly face a risk of exposure to malicious content. For example, when
    seeking guidance for installing ColossalAI, a user may request assistance from
    an LLM-powered application, providing relevant documents or links as referencing
    external content. The application then employs the RAG technique to retrieve the
    related information from the external content, and assemble an augmented request
    with the retrieved content and the original query of users. In normal, based on
    the augmented request, the application is supposed to provide an answer telling
    users the correct download link of ColossalAI, as shown by the upper part of Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval Poisoning Attacks
    in LLM-Powered Applications"). However, as presented in Figure [1](#S1.F1 "Figure
    1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
    Applications")’s below part, users may unintentionally reference a document crafted
    by attackers since it is identical to the normal one in human perception. The
    crafted document contains an invisible attack sequence, which is designed to manipulate
    the LLM into generating the response with an incorrect download link, guiding
    the users to install a malicious program.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击场景。如图[1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval
    Poisoning Attacks in LLM-Powered Applications")所示，用户在不知情的情况下面临恶意内容曝光的风险。例如，当寻求安装ColossalAI的指导时，用户可能会请求LLM驱动的应用程序提供帮助，并提供相关文档或链接作为参考外部内容。应用程序然后使用RAG技术从外部内容中检索相关信息，并将检索到的内容与用户的原始查询一起组装成一个增强请求。在正常情况下，基于增强请求，应用程序应提供一个答案，告知用户ColossalAI的正确下载链接，如图[1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Human-Imperceptible Retrieval Poisoning Attacks
    in LLM-Powered Applications")的上半部分所示。然而，如图[1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")的下半部分所示，用户可能无意中参考了攻击者伪造的文档，因为它在人的感知中与正常文档相同。伪造的文档包含一个隐形攻击序列，旨在操控LLM生成包含错误下载链接的响应，指导用户安装恶意程序。
- en: Approach. Retrieval poisoning fully leverages the RAG workflow, exhibiting a
    significant threat to the LLM ecosystem. Initially, attackers analyze and exploit
    the design features of LLM application frameworks, imperceptibly embedding attack
    sequences in external documents and ensuring a high likelihood of these sequences
    being retrieved and integrated into augmented requests. Moreover, a gradient-guided
    mutation technique, which adopts a weighted loss, is introduced to generate attack
    sequences with high effectiveness. Finally, by invisibly injecting the generated
    sequences at proper positions in benign documents, attackers can easily craft
    malicious documents. When released onto the Internet, these documents pose a threat
    to the applications dependent on external content.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。检索中毒充分利用了RAG工作流程，对LLM生态系统构成了显著威胁。最初，攻击者分析并利用LLM应用框架的设计特性，悄无声息地将攻击序列嵌入外部文档中，并确保这些序列被检索并集成到增强请求中的可能性很高。此外，引入了一种梯度引导的变异技术，该技术采用加权损失，以生成高效的攻击序列。最后，通过在良性文档的适当位置隐蔽地注入生成的序列，攻击者可以轻松地制作恶意文档。当这些文档发布到互联网上时，对依赖外部内容的应用程序构成威胁。
- en: Preliminary Experiment. To demonstrate the impact of retrieval poisoning, we
    construct a dataset comprising 30 documents and perform a preliminary experiment.
    Subsequently, we executed the attack on three powerful open-source LLMs with two
    temperature settings, achieving an average attack success rate (ASR) of 88.33%.
    In addition, experiment results also depict that the attack can maintain its effectiveness
    in various situations. Furthermore, a real-world experiment was conducted on a
    widely-used LLM-powered application developed with LangChain, where retrieval
    poisoning achieves 66.67% ASR. In conclusion, retrieval poisoning poses an extreme
    danger to current applications, necessitating the urgent development of more effective
    mitigation strategies.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 初步实验。为了展示检索中毒的影响，我们构建了一个包含30个文档的数据集并进行了初步实验。随后，我们在三个强大的开源LLM上执行了攻击，设置了两个温度参数，达到了88.33%的平均攻击成功率（ASR）。此外，实验结果还表明，攻击在各种情况下都能保持其有效性。此外，我们在一个使用LangChain开发的广泛使用的LLM驱动应用程序上进行了真实世界的实验，其中检索中毒达到了66.67%的ASR。总之，检索中毒对当前应用程序构成了极端危险，迫切需要开发更有效的缓解策略。
- en: 2\. Methodology
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 方法论
- en: In this section, we introduce the workflow to perform retrieval poisoning for
    real-world LLM-powered applications. Figure [2](#S2.F2 "Figure 2 ‣ 2.1\. Framework
    Analysis ‣ 2\. Methodology ‣ Human-Imperceptible Retrieval Poisoning Attacks in
    LLM-Powered Applications") shows the overall workflow. The goal of retrieval poisoning
    is to craft a malicious document, which is designed to manipulate the LLM into
    generating responses that align with the attacker’s intent while appearing identical
    to the original in human perception. This crafted document can then be used to
    poison the retrieval process of LLM-powered applications. To achieve this goal,
    retrieval poisoning consists of two main steps. The first step is to analyze LLM
    application frameworks’ critical components used for RAG in order to facilitate
    the invisible injection of the attack sequence generated in the next step. The
    second step is to generate the attack sequence and craft the malicious document
    with a gradient-guided token mutation technique.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了针对现实世界LLM驱动应用程序进行检索中毒的工作流程。图 [2](#S2.F2 "Figure 2 ‣ 2.1\. Framework
    Analysis ‣ 2\. Methodology ‣ Human-Imperceptible Retrieval Poisoning Attacks in
    LLM-Powered Applications")展示了整体工作流程。检索中毒的目标是制作一个恶意文档，该文档旨在操纵LLM生成符合攻击者意图的响应，同时在人类感知中看起来与原文相同。然后，可以使用此制作的文档来毒化LLM驱动应用程序的检索过程。为了实现这一目标，检索中毒包括两个主要步骤。第一步是分析LLM应用框架中用于RAG的关键组件，以便在下一步中进行攻击序列的隐蔽注入。第二步是生成攻击序列并使用梯度引导的令牌变异技术制作恶意文档。
- en: 2.1\. Framework Analysis
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 框架分析
- en: Currently, LLM-powered applications are usually developed with LLM application
    frameworks (Liu et al., [2023c](#bib.bib14)), which provide many powerful components
    to support RAG. Therefore, we first introduce the workflow of RAG and then analyze
    the exploitable features of the components that can be leveraged by attackers
    to perform retrieval poisoning. In this paper, we focus on the most popular LLM
    application framework, LangChain. It has gained over 72,000 stars on GitHub after
    its release (LangChain-AI, [2023](#bib.bib11)) and has been adopted by many popular
    LLM-powered applications (kyrolabs, [2023](#bib.bib10); Chatchat-Space, [2023](#bib.bib5)).
    Thus, the attacks based on LangChain can affect a large number of users.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，基于 LLM 的应用程序通常使用 LLM 应用框架（Liu et al., [2023c](#bib.bib14)）进行开发，这些框架提供了许多强大的组件来支持
    RAG。因此，我们首先介绍 RAG 的工作流程，然后分析攻击者可以利用的组件特性进行检索中毒。在本文中，我们关注最流行的 LLM 应用框架 LangChain。自发布以来，它在
    GitHub 上获得了超过 72,000 个星标（LangChain-AI, [2023](#bib.bib11)），并被许多流行的 LLM 驱动应用程序采用（kyrolabs,
    [2023](#bib.bib10); Chatchat-Space, [2023](#bib.bib5)）。因此，基于 LangChain 的攻击可能影响大量用户。
- en: RAG Workflow. Before processing users’ requests with RAG, a retrieval database
    should be first constructed by users or developers of applications. Specifically,
    users and developers will collect the documents from the Internet. These documents’
    content, after being parsed by the document parsers, is split into chunks with
    appropriate lengths by text splitters. Finally, the retrieval database is constructed
    with vectors that are embedded from these chunks (Song et al., [2020](#bib.bib18);
    Okorie et al., [2011](#bib.bib17)). From the database, applications can retrieve
    relevant content, and then assemble the content and the original request into
    an augmented request following a prompt template. In the end, the augmented request
    is fed to LLMs to generate the response.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RAG 工作流。在使用 RAG 处理用户请求之前，首先需要由用户或应用开发者构建一个检索数据库。具体而言，用户和开发者将从互联网收集文档。这些文档的内容在经过文档解析器解析后，会由文本分割器将其分割成适当长度的块。最后，检索数据库通过将这些块嵌入的向量构建而成（Song
    et al., [2020](#bib.bib18); Okorie et al., [2011](#bib.bib17)）。从数据库中，应用程序可以检索相关内容，然后将内容与原始请求组合成一个增强请求，按照提示模板进行处理。最终，增强请求会被送入
    LLMs 生成响应。
- en: '![Refer to caption](img/8b3317c431a0c38c2b4ed900ba0a56ca.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8b3317c431a0c38c2b4ed900ba0a56ca.png)'
- en: Figure 2\. Workflow of retrieval poisoning.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 检索中毒的工作流程。
- en: Exploitable Features. In this process, the document parser, text splitter, and
    prompt template are three components that can be exploited by attackers. First,
    by analyzing the document parser, attackers can find features used for invisible
    injection in different document formats. The content on the Internet is usually
    in rich text formats, such as PDF, HTML, and Markdown, which require rendering
    before being shown to users. However, some content in the documents will not be
    rendered as visible but can be parsed by the document parsers. For example, in
    Markdown files, attackers may hide an attack sequence at the beginning of code
    blocks, as the listing shows below.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可利用特性。在此过程中，文档解析器、文本分割器和提示模板是攻击者可以利用的三个组件。首先，通过分析文档解析器，攻击者可以找到在不同文档格式中用于隐形注入的特性。互联网上的内容通常是富文本格式，如
    PDF、HTML 和 Markdown，这些格式在显示给用户之前需要渲染。然而，文档中的某些内容不会被渲染为可见，但可以被文档解析器解析。例如，在 Markdown
    文件中，攻击者可能会在代码块的开头隐藏攻击序列，如下面的列表所示。
- en: “‘bash  injected_sequenceecho  ”bash  script”“‘
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: “‘bash  injected_sequenceecho  ”bash  script”“‘
- en: The injected sequence will not be rendered visibly or influence the syntax highlighting
    of the code block, but it will be parsed by the document parser. As for PDF and
    HTML, many transparent elements can be leveraged to hide an extra sequence. Therefore,
    attackers can easily find invisible features to hide the attack sequences in benign
    documents.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注入的序列不会在可视化上显示或影响代码块的语法高亮，但会被文档解析器解析。至于 PDF 和 HTML，许多透明元素可以被利用来隐藏额外的序列。因此，攻击者可以轻易找到隐形特征，以在良性文档中隐藏攻击序列。
- en: Second, to ensure the attack sequence can be conveyed to the LLMs, attackers
    will also analyze the text splitters to ensure a proper injection position, so
    that the injected attack sequence can stay with the crucial information in the
    same chunk. In detail, the text is split based on the length and section of the
    content. Section-based splitters divide content according to tags that label different
    sections, which attackers can exploit to place their attack sequences within these
    delineated chunks. As for length-based splitters, they will split the content
    into fixed-length chunks with overlap (to keep context between chunks). Therefore,
    attackers may locate their attack sequence at an appropriate distance from crucial
    information, ensuring it remains undivided by length-based splitters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，为了确保攻击序列能够传达给LLM，攻击者还会分析文本拆分器，以确保适当的注入位置，使注入的攻击序列能够与同一块中的关键信息保持一致。具体来说，文本是根据内容的长度和部分进行拆分的。基于部分的拆分器根据标记不同部分的标签对内容进行划分，攻击者可以利用这些标签将攻击序列放置在这些划定的块中。至于基于长度的拆分器，它们会将内容拆分成固定长度的块，并有重叠（以保持块之间的上下文）。因此，攻击者可能会在距离关键信息适当的地方放置攻击序列，确保其不被长度拆分器拆分。
- en: Third, attackers can obtain the augmented request according to the frameworks’
    prompt templates to perform attack sequence generation. Prompt template can determine
    how the retrieved content is organized alongside the user’s request to form the
    augmented request. The template is crucial, as it impacts the overall performance
    of LLM-powered applications. Frameworks like LangChain offer a variety of prompt
    templates whose effectiveness has been validated, enabling application developers
    to either directly adopt them or customize their own templates based on these
    templates. Therefore, by utilizing the framework’s prompt templates, attackers
    can craft high-quality augmented requests to generate the attack sequence, as
    illustrated in Section [2.2](#S2.SS2 "2.2\. Document Crafting ‣ 2\. Methodology
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications").
    These attack sequences retain their effectiveness across a range of prompt templates
    used by developers in various applications.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，攻击者可以根据框架的提示模板获取增强的请求，以执行攻击序列生成。提示模板可以决定检索内容如何与用户的请求一起组织以形成增强的请求。模板至关重要，因为它影响LLM驱动的应用程序的整体性能。像LangChain这样的框架提供了多种提示模板，其有效性已经得到验证，使应用开发者能够直接采用这些模板或根据这些模板自定义自己的模板。因此，通过利用框架的提示模板，攻击者可以制作高质量的增强请求以生成攻击序列，如第[2.2](#S2.SS2
    "2.2\. 文档制作 ‣ 2\. 方法论 ‣ 人工不可感知的检索中毒攻击在LLM驱动的应用程序中")节所示。这些攻击序列在开发者在各种应用程序中使用的多种提示模板中保持有效。
- en: 2.2\. Document Crafting
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 文档制作
- en: Algorithm [1](#alg1 "In 2.2\. Document Crafting ‣ 2\. Methodology ‣ Human-Imperceptible
    Retrieval Poisoning Attacks in LLM-Powered Applications") illustrates how attackers
    can leverage the pre-analyzed features to generate the attack sequence and craft
    the malicious documents. The algorithm aims to modify an initial document to a
    crafted document  is built based on the retrieved content and the prompt template.  is
    the targeted LLM model that is used by LLM-powerful applications. In this paper,
    we focus on the open-source LLMs, which are widely adopted by existing applications (Touvron
    et al., [2023](#bib.bib20); Jiang et al., [2023](#bib.bib9)). We will extend our
    research to closed-source LLMs using transfer techniques in future works (Zou
    et al., [2023](#bib.bib22); Yuan et al., [2020](#bib.bib21)). In addition, the
    algorithm also needs the injection position  to craft a malicious document.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 算法[1](#alg1 "在2.2\. 文档制作 ‣ 2\. 方法论 ‣ 人工不可感知的检索中毒攻击在LLM驱动的应用程序中") 说明了攻击者如何利用预分析的特征生成攻击序列并制作恶意文档。该算法旨在将初始文档修改为基于检索内容和提示模板构建的精心制作的文档。是LLM驱动的应用程序中使用的目标LLM模型。在本文中，我们关注于广泛采用的开源LLM（Touvron等，[2023](#bib.bib20)；Jiang等，[2023](#bib.bib9)）。我们将在未来的工作中利用迁移技术将研究扩展到封闭源LLM（Zou等，[2023](#bib.bib22)；Yuan等，[2020](#bib.bib21)）。此外，该算法还需要注入位置来制作恶意文档。
- en: 'Input :  : Injection Position: Invisible Features: Crafted Document12++ 5      * then7            
    break;8      9      11      14'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：： 注入位置：隐形特征：精心制作的文档12++ 5      * 然后7             退出;8      9      11      14
- en: Algorithm 1 Document Crafting
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 文档制作
- en: The algorithm first needs to generate an attack sequence . , rather than being
    identical in its entirety. The algorithm performs an iterative mutation under
    the guidance of a weighted loss. As shown in Line 3, attackers will first combine
    the attack squeeze  at the injection position  to generate the response and examines
    whether the attack is successful (Line 4-6). If the further mutation is still
    required, then attackers can calculate a weighted loss (Line 7-8) following the
    equation below,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先需要生成攻击序列，而不是完全相同的序列。算法在加权损失的指导下进行迭代变异。如第 3 行所示，攻击者将首先在注入位置合并攻击挤压生成响应，并检查攻击是否成功（第
    4-6 行）。如果仍需要进一步变异，攻击者可以按照下面的公式计算加权损失（第 7-8 行），
- en: '|  |  |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: Specifically, the loss is calculated by the cross entropy of the .  with respect
    to the  new sequences  as 32. Each new sequence is generated by randomly selecting
    one token in the  by calculating the loss of each sequence and selecting the one
    with a lower loss (Line 11). With  by hiding  with invisible features $features$
    (Line 12).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，损失是通过对新的序列的交叉熵来计算的，值为 32。每个新序列是通过随机选择标记并计算每个序列的损失，然后选择损失较低的序列来生成的（第 11
    行）。通过用不可见特征 $features$ 隐藏来计算损失（第 12 行）。
- en: 3\. Preliminary Experiments
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 初步实验
- en: In this section, we conduct preliminary experiments to show the impact of retrieval
    poisoning attack on LLM-powered applications. First, we evaluate the attack success
    rate (ASR) of retrieval poisoning towards different LLMs and meanwhile evaluate
    that the attack sequence is effective under different augmented requests. To perform
    the attack, we construct a dataset with 30 documents, including software installation
    instructions and medication guides. The target LLMs for our attack are Llama2-7b,
    Llama2-13b, and Mistral-7b, which vary in parameter size and architecture, providing
    a comprehensive range of scenarios for our analysis. Furthermore, we also perform
    real-world attacks on ChatChat, a popular application powered by LangChain, demonstrating
    that attackers can effectively execute the retrieval poisoning in a manner that
    remains undetected by humans.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们进行初步实验以展示检索中毒攻击对 LLM 驱动的应用程序的影响。首先，我们评估检索中毒对不同 LLMs 的攻击成功率（ASR），同时评估在不同扩展请求下攻击序列的有效性。为进行攻击，我们构建了一个包含
    30 个文档的数据集，其中包括软件安装说明和药物指南。我们的攻击目标 LLMs 是 Llama2-7b、Llama2-13b 和 Mistral-7b，它们在参数规模和架构上各异，为我们的分析提供了全面的场景。此外，我们还对由
    LangChain 驱动的流行应用 ChatChat 进行了实际攻击，展示了攻击者能够有效地执行检索中毒，而不被人类发现。
- en: 3.1\. Evaluation on LLMs
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 对 LLMs 的评估
- en: To evaluate that retrieval poisoning is easily performed, we first concentrate
    on attacks towards LLMs, on which we evaluate the ASR of generated attack sequences.
    In detail, we first evaluate retrieval poisoning on three different LLMs with
    two different temperature settings. Then, the attack sequences are evaluated on
    different augmented requests constructed based on different prompt templates.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估检索中毒的易实施性，我们首先集中在对 LLMs 的攻击上，对生成的攻击序列的 ASR 进行评估。具体而言，我们首先在三个不同的 LLMs 上评估检索中毒，使用两种不同的温度设置。然后，对基于不同提示模板构建的不同扩展请求中的攻击序列进行评估。
- en: Table 1\. Evaluation of retrieval poisoning on LLMs. “Iter” is the average iteration
    during the attack. “Seq”, “Req”, and “Res” show the average token length of the
    attack sequences, augmented requests, and output responses, respectively.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 对 LLMs 的检索中毒评估。“Iter”是攻击过程中的平均迭代次数。“Seq”、“Req”和“Res”分别表示攻击序列、扩展请求和输出响应的平均标记长度。
- en: '| Temp | LLMs | ASR | Iter | Seq | Req | Res |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | LLMs | ASR | 迭代 | 序列 | 请求 | 响应 |'
- en: '| 0.7 | Llama2-7b | 86.67% | 140.63 | 31.37 | 600.53 | 140.73 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 0.7 | Llama2-7b | 86.67% | 140.63 | 31.37 | 600.53 | 140.73 |'
- en: '| Llama2-13b | 90.00% | 137.67 | 30.80 | 601.90 | 135.23 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 90.00% | 137.67 | 30.80 | 601.90 | 135.23 |'
- en: '| Mistral-7b | 86.67% | 141.60 | 30.23 | 583.43 | 128.40 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b | 86.67% | 141.60 | 30.23 | 583.43 | 128.40 |'
- en: '| 1.0 | Llama2-7b | 90.00% | 124.10 | 31.13 | 600.53 | 140.63 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 1.0 | Llama2-7b | 90.00% | 124.10 | 31.13 | 600.53 | 140.63 |'
- en: '| Llama2-13b | 93.33% | 102.30 | 27.87 | 601.90 | 139.67 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b | 93.33% | 102.30 | 27.87 | 601.90 | 139.67 |'
- en: '| Mistral-7b | 83.30% | 168.83 | 30.77 | 583.43 | 130.93 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7b | 83.30% | 168.83 | 30.77 | 583.43 | 130.93 |'
- en: '| Average | 88.33% | 135.86 | 30.36 | 595.29 | 135.93 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 88.33% | 135.86 | 30.36 | 595.29 | 135.93 |'
- en: As Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary Experiments
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")
    shows, retrieval poisoning is very effective and achieves an 88.33% average ASR
    on all LLMs and settings. Among all LLMs, retrieval poisoning gains the highest
    ASR on Llama2-13b, despite having the most parameters. It may be because LLMs
    with fewer parameters are easily affected by attack sequences, and sometimes,
    the response becomes totally unrelated to the request, causing low attack efficiencies.
    Additionally, retrieval poisoning maintains high effectiveness, with an ASR above
    83.30%, across different temperature settings, indicating that temperature has
    a slight impact on the attack’s performance. Even using the attack sequence generated
    at a temperature setting of 0.7, retrieval poisoning still achieves an 86.67%
    ASR on LLMs with 1.0 temperature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如表[1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary Experiments
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")所示，检索中毒非常有效，在所有LLMs和设置中实现了88.33%的平均ASR。在所有LLMs中，尽管Llama2-13b具有最多的参数，检索中毒在其上获得了最高的ASR。这可能是因为参数较少的LLMs更容易受到攻击序列的影响，有时响应变得与请求完全无关，导致攻击效率低。此外，检索中毒在不同温度设置下仍保持高效，ASR超过83.30%，表明温度对攻击性能的影响较小。即使使用在温度设置为0.7时生成的攻击序列，检索中毒在温度为1.0的LLMs上仍能实现86.67%的ASR。
- en: Additionally, Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary
    Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")
    presents the average iteration steps, offering insights into the LLMs’ resistance
    to retrieval poisoning. The data indicate that Mistral-7b exhibits greater robustness,
    aligning with the ASR findings. Moreover, the table includes the average token
    length of the generated attack sequences and responses. An average sequence length
    of 30.36 suggests attackers can easily conceal these sequences within external
    content. The average lengths of requests and responses, at 595.29 and 135.93 tokens,
    respectively, imply that retrieval poisoning is typically employed in complex
    tasks. This contrasts with existing adversarial attacks, which often focus on
    text classification tasks where the LLMs’ output is limited to simple classifications
    like positive or negative. Please note that different LLMs adopt distinct tokenizers,
    which will encode the same inputs into different token sequences in various lengths.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，表格[1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary Experiments
    ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications")展示了平均迭代步骤，提供了对LLMs抵抗检索中毒攻击的见解。数据表明，Mistral-7b表现出更强的鲁棒性，与ASR结果一致。此外，表格还包括生成的攻击序列和响应的平均令牌长度。30.36的平均序列长度表明攻击者可以轻松将这些序列隐藏在外部内容中。请求和响应的平均长度分别为595.29和135.93个令牌，表明检索中毒通常用于复杂任务。这与现有的对抗性攻击形成对比，后者通常集中在文本分类任务上，其中LLMs的输出被限制为简单的分类，如正面或负面。请注意，不同的LLMs采用不同的分词器，这将把相同的输入编码成不同长度的令牌序列。
- en: Table 2\. ASR on different augmented requests.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 不同增强请求下的ASR。
- en: '| LLMs | Llama2-7b | Llama2-13b | Mistral-7b |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| LLMs | Llama2-7b | Llama2-13b | Mistral-7b |'
- en: '| ASR | 59.26% | 46.43% | 64.00% |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 59.26% | 46.43% | 64.00% |'
- en: In reality, attack sequences should keep their effectiveness on different augmented
    requests, since prompt templates and queries differ for various developers and
    users. Therefore, we evaluate the generated attack sequence with different augmented
    requests. Because it is challenging to measure the replacement of queries objectively,
    we made significant modifications to the prompt template, constructing entirely
    different augmented requests for evaluation. In detail, the original prompt template
    is “¡Scenario Description¿ ¡Content¿ ¡Question¿”, presenting a QA scenario before
    the content and question. The new format, “¡Question¿ ¡Content¿”, directly poses
    a question to be answered from the provided content. The results show that 56.56%
    of successfully generated attack sequences are still effective on very different
    augmented requests, demonstrating that retrieval poisoning is not specified for
    one augmented request. This evaluation is operated with LLMs at a temperature
    setting of 1.0, where retrieval poisoning generates more attack sequences, as
    evidenced in Table [1](#S3.T1 "Table 1 ‣ 3.1\. Evaluation on LLMs ‣ 3\. Preliminary
    Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered Applications").
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，攻击序列在不同的增强请求中应该保持其有效性，因为提示模板和查询对不同的开发者和用户有所不同。因此，我们使用不同的增强请求评估生成的攻击序列。由于客观测量查询替换是具有挑战性的，我们对提示模板进行了重大修改，构建了完全不同的增强请求进行评估。具体而言，原始提示模板是“¡Scenario
    Description¿ ¡Content¿ ¡Question¿”，在内容和问题之前呈现一个QA场景。新的格式“¡Question¿ ¡Content¿”直接提出一个问题，从提供的内容中进行回答。结果表明，56.56%成功生成的攻击序列在非常不同的增强请求中仍然有效，这表明检索中毒并不针对单一的增强请求。此评估在温度设置为1.0的LLMs下进行，其中检索中毒生成了更多攻击序列，见表 [1](#S3.T1
    "表 1 ‣ 3.1\. 对LLMs的评估 ‣ 3\. 初步实验 ‣ 在LLM驱动的应用程序中的人类难以察觉的检索中毒攻击")。
- en: 3.2\. Real-World Application Experiment
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 现实世界应用实验
- en: 'To assess retrieval poisoning’s impact in the real world, we perform the imperceptible
    attack on ChatChat (Chatchat-Space, [2023](#bib.bib5)), a popular LLM-powered
    application with over 21k stars on GitHub. We employ Mistral-7b as the LLM for
    ChatChat, since it is recognized as the most powerful 7b LLM (AI, [2023](#bib.bib3)).
    As outlined by Table [3](#S3.T3 "Table 3 ‣ 3.2\. Real-World Application Experiment
    ‣ 3\. Preliminary Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks
    in LLM-Powered Applications"), we utilized content in three commonly used formats:
    PDF, Markdown, and HTML. We collect more PDF files due to their well-structured
    and fine-grained content. While HTML files are prevalent online, they often include
    extraneous elements like website menus, adversely affecting application effectiveness.
    Hence, formats such as PDF are likely more preferred by users and developers for
    building retrieval databases.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估检索中毒在实际中的影响，我们对ChatChat（Chatchat-Space，[2023](#bib.bib5)）进行不可察觉攻击，这是一款在GitHub上拥有超过21k星的流行LLM驱动应用程序。我们使用Mistral-7b作为ChatChat的LLM，因为它被认为是最强大的7b
    LLM（AI，[2023](#bib.bib3)）。如表 [3](#S3.T3 "表 3 ‣ 3.2\. 现实世界应用实验 ‣ 3\. 初步实验 ‣ 在LLM驱动的应用程序中的人类难以察觉的检索中毒攻击")所述，我们使用了三种常用格式的内容：PDF、Markdown和HTML。我们收集了更多PDF文件，因为它们结构良好、内容细致。尽管HTML文件在网上很普遍，但它们通常包含如网站菜单等多余元素，这会对应用程序的效果产生不利影响。因此，PDF等格式可能更受用户和开发者的青睐，用于构建检索数据库。
- en: Table 3\. Evaluation on real-world applications.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 对现实世界应用程序的评估。
- en: '| Formats | PDF | Markdown | HTML | Total |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 格式 | PDF | Markdown | HTML | 总计 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Count | 14 | 10 | 6 | 30 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 数量 | 14 | 10 | 6 | 30 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Injection | 85.71% (12) | 90% (9) | 83.33% (5) | 86.67% (26) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 注入 | 85.71% (12) | 90% (9) | 83.33% (5) | 86.67% (26) |'
- en: '| ASR | 71.43% (10) | 60.00% (6) | 66.67% (4) | 66.67% (20) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ASR | 71.43% (10) | 60.00% (6) | 66.67% (4) | 66.67% (20) |'
- en: On all documents, attackers can successfully perform the imperceptible attack
    sequence injection. Moreover, as Table [3](#S3.T3 "Table 3 ‣ 3.2\. Real-World
    Application Experiment ‣ 3\. Preliminary Experiments ‣ Human-Imperceptible Retrieval
    Poisoning Attacks in LLM-Powered Applications") shows, 86.67% of injected sequences
    are successfully conveyed to the LLMs after the complex RAG workflow. As for the
    other four cases, the injected sequences affect the applications’ retrieval engines,
    resulting in inaccuracies in retrieving relevant content. With attack sequences
    injected, ChatChat produces incorrect responses to users on 66.67% documents across
    three formats. Six attack sequences are conveyed to the LLMs but fail to achieve
    the retrieval poisoning. This can be attributed to the RAG workflow’s complexity.
    For instance, the application might incorrectly decode special texts within the
    attack sequence, leading to attack failures. The ASR is higher in PDF documents
    since the injection on PDF is more feasible. In contrast, more complex injection
    requirements of other formats slightly diminish the attack’s effectiveness.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有文档中，攻击者可以成功执行不易察觉的攻击序列注入。此外，正如表[3](#S3.T3 "表 3 ‣ 3.2\. 现实世界应用实验 ‣ 3\. 初步实验
    ‣ 人类不可察觉的LLM驱动应用中的检索毒化攻击")所示，86.67%的注入序列在复杂的RAG工作流程后成功传递给LLMs。至于其他四个案例，注入的序列影响了应用程序的检索引擎，导致检索相关内容的准确性下降。在注入攻击序列后，ChatChat在三种格式的文档中有66.67%的情况产生了错误响应。六个攻击序列传递给LLMs但未能实现检索毒化，这可以归因于RAG工作流程的复杂性。例如，应用程序可能错误解码攻击序列中的特殊文本，导致攻击失败。由于PDF文档中的注入更为可行，因此ASR在PDF文档中较高。相比之下，其他格式的注入要求较复杂，稍微降低了攻击的效果。
- en: '![Refer to caption](img/7e4d904dc10ff55a7202724af32db1ad.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e4d904dc10ff55a7202724af32db1ad.png)'
- en: Figure 3\. A case of retrieval poisoning on ChatChat.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. ChatChat上的检索毒化案例
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.2\. Real-World Application Experiment ‣ 3\.
    Preliminary Experiments ‣ Human-Imperceptible Retrieval Poisoning Attacks in LLM-Powered
    Applications") presents a detailed example of an retrieval poisoning on ChatChat.
    As the sword icon pointed out, attackers hide an attack sequence at the beginning
    of the code block in Markdown format, which is invisible after rendering. When
    users request installation information for ColossalAI (Li et al., [2023](#bib.bib13)),
    ChatChat should, in normal, generate its response based on the external content
    and provide the correct link. However, through a meticulously crafted attack sequence
    and careful injection, attackers can manipulate the application, misleading users
    with an incorrect download link that potentially leads to the download of malicious
    programs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S3.F3 "图 3 ‣ 3.2\. 现实世界应用实验 ‣ 3\. 初步实验 ‣ 人类不可察觉的LLM驱动应用中的检索毒化攻击")展示了在ChatChat上进行检索毒化的详细示例。如剑图标所示，攻击者在Markdown格式的代码块开头隐藏了攻击序列，渲染后不可见。当用户请求ColossalAI的安装信息（Li等，[2023](#bib.bib13)）时，ChatChat通常会根据外部内容生成其响应，并提供正确的链接。然而，通过精心制作的攻击序列和谨慎的注入，攻击者可以操控应用程序，误导用户提供一个不正确的下载链接，这可能导致下载恶意程序。
- en: 4\. Discussion
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 讨论
- en: Difference from Prompt Injection. Prompt injection can hijack LLM-powered applications
    to generate malicious content with harmful instructions. However, this method
    typically assumes the user has malicious intent, contrasting with the retrieval
    poisoning scenarios. Moreover, some researchers start to inject long malicious
    instructions through external content (Abdelnabi et al., [2023](#bib.bib2)). Different
    from these attacks, retrieval poisoning achieves a more imperceptible attack by
    analyzing the LLM application framework and can bypass advanced instruction filtering
    methods (Garg, [2023](#bib.bib6); Liu et al., [2023b](#bib.bib16)).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与提示注入的区别。提示注入可以劫持由LLM驱动的应用程序，以生成包含有害指令的恶意内容。然而，这种方法通常假设用户具有恶意意图，这与检索毒化场景形成对比。此外，一些研究人员开始通过外部内容注入长时间的恶意指令（Abdelnabi等，[2023](#bib.bib2)）。不同于这些攻击，检索毒化通过分析LLM应用框架实现了更不易察觉的攻击，并且可以绕过高级指令过滤方法（Garg，[2023](#bib.bib6)；Liu等，[2023b](#bib.bib16)）。
- en: Potential Defenses. This paper is dedicated to heightening researchers’ awareness
    of the risks associated with retrieval poisoning and to inspiring the community
    to develop possible mitigation. One possible defense strategy is for applications
    to display the source content underlying their responses, allowing users to cross-reference
    the content with the response. However, this method might be less effective with
    complex content, as it could require users to invest much time in verification.
    Another approach involves using LLMs to rewrite content, thereby breaking the
    attack sequence. Nevertheless, it will introduce substantial computational resources
    and delays in application response times, influencing the efficiency of applications.
    Moreover, rewriting may also be affected by retrieval poisoning, incorrectly rewriting
    the crucial information. Therefore, the development of more efficient and effective
    defense mechanisms remains a critical need.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在防御。这篇论文致力于提高研究人员对检索中毒风险的意识，并激励社区开发可能的缓解措施。一种可能的防御策略是让应用程序显示其响应所依据的源内容，允许用户将内容与响应进行交叉参考。然而，这种方法在处理复杂内容时可能效果较差，因为可能需要用户花费大量时间进行验证。另一种方法是使用大型语言模型（LLMs）重新编写内容，从而打破攻击序列。然而，这将引入大量计算资源和应用响应时间的延迟，影响应用程序的效率。此外，重新编写也可能受到检索中毒的影响，错误地重新编写关键的内容。因此，开发更高效和有效的防御机制仍然是一个重要需求。
- en: 5\. Conclusion
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we expose a new threat to LLM-powered applications, named retrieval
    poisoning, where a benign document in human eyes can guide the LLMs to produce
    incorrect responses during RAG. In detail, attackers can exploit the LLM application
    framework to hide a malicious sequence in the external content, guiding the LLM-powered
    application to produce malicious responses. This work encourages the community
    to explore further into understanding the intricacies of LLM application frameworks,
    leading to more resilient and reliable LLM-powered applications.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，我们揭示了一种对基于LLM的应用程序的新威胁，称为检索中毒。在这种威胁下，虽然对人类眼睛而言是无害的文档可以引导LLM在RAG（检索增强生成）过程中生成不正确的响应。具体来说，攻击者可以利用LLM应用框架在外部内容中隐藏恶意序列，引导基于LLM的应用程序生成恶意响应。这项工作鼓励社区深入探索LLM应用框架的复杂性，从而开发出更具韧性和可靠性的基于LLM的应用程序。
- en: References
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abdelnabi et al. (2023) Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, and Mario Fritz. 2023. Not What You’ve Signed Up For: Compromising
    Real-World LLM-Integrated Applications with Indirect Prompt Injection. In *Proceedings
    of the 16th ACM Workshop on Artificial Intelligence and Security* (Copenhagen,
    Denmark) *(AISec ’23)*. Association for Computing Machinery, New York, NY, USA,
    79–90. [https://doi.org/10.1145/3605764.3623985](https://doi.org/10.1145/3605764.3623985)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdelnabi 等人 (2023) Sahar Abdelnabi, Kai Greshake, Shailesh Mishra, Christoph
    Endres, Thorsten Holz, 和 Mario Fritz. 2023. 并非你所期望的：通过间接提示注入破坏现实世界的LLM集成应用程序.
    在 *第16届ACM人工智能与安全研讨会论文集*（哥本哈根，丹麦）*(AISec ’23)*. 计算机协会，美国纽约，79–90. [https://doi.org/10.1145/3605764.3623985](https://doi.org/10.1145/3605764.3623985)
- en: AI (2023) Mistral AI. 2023. Mistral 7B. The best 7B model to date. [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI (2023) Mistral AI. 2023. Mistral 7B. 迄今为止最佳的7B模型。 [https://mistral.ai/news/announcing-mistral-7b/](https://mistral.ai/news/announcing-mistral-7b/).
- en: Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J Pappas, and Eric Wong. 2023. Jailbreaking black box large language models
    in twenty queries. *arXiv preprint arXiv:2310.08419* (2023).
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao 等人 (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J Pappas, 和 Eric Wong. 2023. 在二十个查询中破解黑箱大型语言模型. *arXiv 预印本 arXiv:2310.08419*
    (2023).
- en: Chatchat-Space (2023) Chatchat-Space. 2023. ChatChat. [https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat).
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chatchat-Space (2023) Chatchat-Space. 2023. ChatChat. [https://github.com/chatchat-space/Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat).
- en: Garg (2023) Vaibhav Garg. 2023. Mitigating Prompt Injection Attacks on an LLM
    based Customer support App. [https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a](https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a).
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garg (2023) Vaibhav Garg. 2023. 缓解LLM基础客户支持应用程序中的提示注入攻击。 [https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a](https://vaibhavgarg1982.medium.com/mitigating-prompt-injection-attacks-on-an-llm-based-customer-support-app-b34298b2bc7a).
- en: Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and
    Danqi Chen. 2023. Catastrophic jailbreak of open-source LLMs via exploiting generation.
    *arXiv preprint arXiv:2310.06987* (2023).
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, 和 Danqi
    Chen. 2023. 通过利用生成的灾难性破解开源LLMs。 *arXiv 预印本 arXiv:2310.06987*（2023）。
- en: Izacard and Grave (2020) Gautier Izacard and Edouard Grave. 2020. Leveraging
    passage retrieval with generative models for open domain question answering. *arXiv
    preprint arXiv:2007.01282* (2020).
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izacard and Grave (2020) Gautier Izacard 和 Edouard Grave. 2020. 利用生成模型进行开放领域问答的段落检索。
    *arXiv 预印本 arXiv:2007.01282*（2020）。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
- en: kyrolabs (2023) kyrolabs. 2023. Awesome-LangChain. [https://github.com/kyrolabs/awesome-langchain](https://github.com/kyrolabs/awesome-langchain).
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kyrolabs (2023) kyrolabs. 2023. Awesome-LangChain. [https://github.com/kyrolabs/awesome-langchain](https://github.com/kyrolabs/awesome-langchain)。
- en: LangChain-AI (2023) LangChain-AI. 2023. LangChain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LangChain-AI (2023) LangChain-AI. 2023. LangChain. [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems* 33 (2020), 9459–9474.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, 等. 2020. 知识密集型NLP任务的检索增强生成。 *神经信息处理系统进展* 33（2020），9459–9474。
- en: 'Li et al. (2023) Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen
    Huang, Yuliang Liu, Boxiang Wang, and Yang You. 2023. Colossal-AI: A Unified Deep
    Learning System For Large-Scale Parallel Training. In *Proceedings of the 52nd
    International Conference on Parallel Processing* (Salt Lake City, UT, USA) *(ICPP
    ’23)*. Association for Computing Machinery, New York, NY, USA, 766–775. [https://doi.org/10.1145/3605573.3605613](https://doi.org/10.1145/3605573.3605613)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen
    Huang, Yuliang Liu, Boxiang Wang, 和 Yang You. 2023. Colossal-AI: 一个统一的大规模并行训练深度学习系统。在
    *第52届国际并行处理会议*（盐湖城，犹他州，美国） *(ICPP ’23)* 中发表。计算机协会，纽约，NY，美国，766–775。 [https://doi.org/10.1145/3605573.3605613](https://doi.org/10.1145/3605573.3605613)'
- en: 'Liu et al. (2023c) Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang
    Dong, Peng Di, Wenhai Wang, and Dongxia Wang. 2023c. Prompting Frameworks for
    Large Language Models: A Survey. *arXiv preprint arXiv:2311.12785* (2023).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023c) Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang
    Dong, Peng Di, Wenhai Wang, 和 Dongxia Wang. 2023c. 大型语言模型的提示框架：综述。 *arXiv 预印本
    arXiv:2311.12785*（2023）。
- en: Liu et al. (2023a) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2023a. Prompt Injection attack
    against LLM-integrated Applications. *arXiv preprint arXiv:2306.05499* (2023).
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang,
    Yepang Liu, Haoyu Wang, Yan Zheng, 和 Yang Liu. 2023a. 针对LLM-集成应用的提示注入攻击。 *arXiv
    预印本 arXiv:2306.05499*（2023）。
- en: Liu et al. (2023b) Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, and Neil Zhenqiang
    Gong. 2023b. Prompt Injection Attacks and Defenses in LLM-Integrated Applications.
    *arXiv preprint arXiv:2310.12815* (2023).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, 和 Neil Zhenqiang
    Gong. 2023b. LLM-集成应用中的提示注入攻击与防御。 *arXiv 预印本 arXiv:2310.12815*（2023）。
- en: 'Okorie et al. (2011) Patricia Nkem Okorie, F Ellis McKenzie, Olusegun George
    Ademowo, Moses Bockarie, and Louise Kelly-Hope. 2011. Nigeria Anopheles vector
    database: an overview of 100 years’ research. *Plos one* 6, 12 (2011), e28347.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Okorie et al. (2011) Patricia Nkem Okorie, F Ellis McKenzie, Olusegun George
    Ademowo, Moses Bockarie, 和 Louise Kelly-Hope. 2011. 尼日利亚按蚊媒介数据库：100年研究概述。 *Plos
    one* 6, 12（2011），e28347。
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    2020. MPNet: Masked and Permuted Pre-training for Language Understanding. In *Advances
    in Neural Information Processing Systems 33: Annual Conference on Neural Information
    Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*, Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (Eds.).'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, 和 Tie-Yan Liu.
    2020. MPNet: Masked and Permuted Pre-training for Language Understanding. 收录于*《神经信息处理系统进展
    33：2020 年神经信息处理系统年度会议，NeurIPS 2020，2020 年 12 月 6-12 日，虚拟会议》*，Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, 和 Hsuan-Tien Lin（编）。'
- en: 'Stahlberg (2020) Felix Stahlberg. 2020. Neural machine translation: A review.
    *Journal of Artificial Intelligence Research* 69 (2020), 343–418.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stahlberg (2020) Felix Stahlberg. 2020. Neural machine translation: A review.
    *《人工智能研究期刊》* 69（2020），343–418。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. arXiv:2307.09288 [cs.CL]'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, 和 Thomas Scialom. 2023. Llama
    2: Open Foundation and Fine-Tuned Chat Models. *arXiv:2307.09288 [cs.CL]*'
- en: Yuan et al. (2020) Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, and
    Kai-Wei Chang. 2020. On the Transferability of Adversarial Attacksagainst Neural
    Text Classifier. *arXiv preprint arXiv:2011.08558* (2020).
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2020) Liping Yuan, Xiaoqing Zheng, Yi Zhou, Cho-Jui Hsieh, 和 Kai-Wei
    Chang. 2020. On the Transferability of Adversarial Attacks against Neural Text
    Classifier. *arXiv 预印本 arXiv:2011.08558*（2020）。
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    2023. Universal and transferable adversarial attacks on aligned language models.
    *arXiv preprint arXiv:2307.15043* (2023).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 2023.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    预印本 arXiv:2307.15043*（2023）。
