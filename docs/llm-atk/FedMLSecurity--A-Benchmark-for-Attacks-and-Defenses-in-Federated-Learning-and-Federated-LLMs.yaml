- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:29
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'FedMLSecurity: 联邦学习和联邦LLMs中的攻击与防御基准'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.04959](https://ar5iv.labs.arxiv.org/html/2306.04959)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.04959](https://ar5iv.labs.arxiv.org/html/2306.04959)
- en: Shanshan Han University of California, IrvineIrvineUSA [shanshan.han@uci.edu](mailto:shanshan.han@uci.edu)
    ,  Baturalp Buyukates University of Southern CaliforniaLos AngelesUSA [buyukate@usc.edu](mailto:buyukate@usc.edu)
    ,  Zijian Hu FedML Inc.Palo AltoUSA [zjh@fedml.ai](mailto:zjh@fedml.ai) ,  Han
    Jin University of Southern CaliforniaLos AngelesUSA [hanjin@usc.edu](mailto:hanjin@usc.edu)
    ,  Weizhao Jin University of Southern CaliforniaLos AngelesUSA [weizhaoj@usc.edu](mailto:weizhaoj@usc.edu)
    ,  Lichao Sun Lehigh UniversityBethlehemUSA [lis221@lehigh.edu](mailto:lis221@lehigh.edu)
    ,  Xiaoyang Wang UIUCChampaignUSA [xw28@illinois.edu](mailto:xw28@illinois.edu)
    ,  Wenxuan Wu Texas A&M UniversityCollege StationUSA [ww6726@tamu.edu](mailto:ww6726@tamu.edu)
    ,  Chulin Xie UIUCChampaignUSA [chulinx2@illinois.edu](mailto:chulinx2@illinois.edu)
    ,  Yuhang Yao Carnegie Mellon UniversityPittsburghUSA [yuhangya@andrew.cmu.edu](mailto:yuhangya@andrew.cmu.edu)
    ,  Kai Zhang Lehigh UniversityBethlehemUSA [kaz321@lehigh.edu](mailto:kaz321@lehigh.edu)
    ,  Qifan Zhang University of California, IrvineIrvineUSA [qifan.zhang@uci.edu](mailto:qifan.zhang@uci.edu)
    ,  Yuhui Zhang Zhejiang UniversityHangzhouChina [zhangyuhui42@zju.edu.cn](mailto:zhangyuhui42@zju.edu.cn)
    ,  Carlee Joe-Wong Carnegie Mellon UniversityPittsburghUSA [cjoewong@andrew.cmu.edu](mailto:cjoewong@andrew.cmu.edu)
    ,  Salman Avestimehr USC, FedML Inc.Los AngelesUSA [avestime@usc.edu](mailto:avestime@usc.edu)
     and  Chaoyang He FedML Inc.Palo AltoUSA [ch@fedml.ai](mailto:ch@fedml.ai)(2018;
    8 February 2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shanshan Han，加州大学欧文分校，欧文，美国 [shanshan.han@uci.edu](mailto:shanshan.han@uci.edu)，
    Baturalp Buyukates，南加州大学，洛杉矶，美国 [buyukate@usc.edu](mailto:buyukate@usc.edu)， Zijian
    Hu，FedML Inc.，帕洛阿尔托，美国 [zjh@fedml.ai](mailto:zjh@fedml.ai)， Han Jin，南加州大学，洛杉矶，美国
    [hanjin@usc.edu](mailto:hanjin@usc.edu)， Weizhao Jin，南加州大学，洛杉矶，美国 [weizhaoj@usc.edu](mailto:weizhaoj@usc.edu)，
    Lichao Sun，利哈伊大学，贝ethlehem，美国 [lis221@lehigh.edu](mailto:lis221@lehigh.edu)， Xiaoyang
    Wang，UIUC，香槟，美国 [xw28@illinois.edu](mailto:xw28@illinois.edu)， Wenxuan Wu，德克萨斯A&M大学，大学城，美国
    [ww6726@tamu.edu](mailto:ww6726@tamu.edu)， Chulin Xie，UIUC，香槟，美国 [chulinx2@illinois.edu](mailto:chulinx2@illinois.edu)，
    Yuhang Yao，卡内基梅隆大学，匹兹堡，美国 [yuhangya@andrew.cmu.edu](mailto:yuhangya@andrew.cmu.edu)，
    Kai Zhang，利哈伊大学，贝ethlehem，美国 [kaz321@lehigh.edu](mailto:kaz321@lehigh.edu)， Qifan
    Zhang，加州大学欧文分校，欧文，美国 [qifan.zhang@uci.edu](mailto:qifan.zhang@uci.edu)， Yuhui
    Zhang，浙江大学，杭州，中国 [zhangyuhui42@zju.edu.cn](mailto:zhangyuhui42@zju.edu.cn)， Carlee
    Joe-Wong，卡内基梅隆大学，匹兹堡，美国 [cjoewong@andrew.cmu.edu](mailto:cjoewong@andrew.cmu.edu)，
    Salman Avestimehr，USC，FedML Inc.，洛杉矶，美国 [avestime@usc.edu](mailto:avestime@usc.edu)
    和 Chaoyang He，FedML Inc.，帕洛阿尔托，美国 [ch@fedml.ai](mailto:ch@fedml.ai)（2018年；2024年2月8日）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'This paper introduces FedMLSecurity, an end-to-end benchmark designed to simulate
    adversarial attacks and corresponding defense mechanisms in Federated Learning
    (FL). FedMLSecurity comprises two pivotal components: FedMLAttacker, which facilitates
    the simulation of a variety of attacks during FL training, and FedMLDefender,
    which implements defensive mechanisms to counteract these attacks. As an open-source
    library, FedMLSecurity enhances its usability compared to from-scratch implementations
    that focus on specific attack/defense scenarios based on the following features:
    i) It offers extensive customization options to accommodate a broad range of machine
    learning models (e.g., Logistic Regression, ResNet, and GAN) and FL optimizers
    (e.g., FedAVG, FedOPT, and FedNOVA); ii) it enables exploring the variability
    in the effectiveness of attacks and defenses across different datasets and models;
    and iii) it supports flexible configuration and customization through a configuration
    file and some provided APIs. We further demonstrate FedMLSecurity’s utility and
    adaptability through federated training of Large Language Models (LLMs), showcasing
    its potential to impact a wide range of complex applications.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 FedMLSecurity，这是一种端到端的基准测试，旨在模拟联邦学习（FL）中的对抗攻击及相应的防御机制。FedMLSecurity 包含两个关键组件：FedMLAttacker，方便模拟
    FL 训练过程中的各种攻击；以及 FedMLDefender，实施防御机制以应对这些攻击。作为一个开源库，FedMLSecurity 相比从零开始实现的特定攻击/防御场景，具备以下特性，提高了其可用性：i)
    提供广泛的定制选项以适应各种机器学习模型（例如，逻辑回归、ResNet 和 GAN）及 FL 优化器（例如，FedAVG、FedOPT 和 FedNOVA）；ii)
    允许探索攻击和防御在不同数据集和模型中的有效性变化；iii) 通过配置文件和一些提供的 API 支持灵活的配置和定制。我们进一步通过对大型语言模型（LLMs）的联邦训练展示了
    FedMLSecurity 的实用性和适应性，展示了其对各种复杂应用的潜在影响。
- en: 'Federated Learning, security, attack, defense, Federated LLMs^†^†copyright:
    acmlicensed^†^†journalyear: 2018^†^†doi: XXXXXXX.XXXXXXX^†^†conference: Make sure
    to enter the correct conference title from your rights confirmation emai; June
    03–05, 2018; Woodstock, NY^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs: Security and
    privacy Distributed systems security^†^†ccs: Computing methodologies Cooperation
    and coordination'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '联邦学习、安全性、攻击、防御、联邦 LLMs^†^†版权所有：acmlicensed^†^†期刊年份：2018^†^†doi: XXXXXXX.XXXXXXX^†^†会议：请确保输入来自您的版权确认邮件的正确会议标题；2018年6月03-05日；纽约伍德斯托克^†^†isbn:
    978-1-4503-XXXX-X/18/06^†^†ccs: 安全与隐私 分布式系统安全^†^†ccs: 计算方法学 合作与协调'
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Federated Learning (FL) (McMahan et al., [2017a](#bib.bib60)) facilitates training
    across distributed data and empowers individual clients to utilize their local
    data to collaboratively train machine learning models. Instead of collecting data
    to a centralized server, FL clients train models on their local data and share
    the local models with the FL server, where the local models are aggregated into
    a global model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习（FL）(McMahan 等，[2017a](#bib.bib60)) 促进了分布式数据上的训练，并使个体客户端能够利用本地数据协作训练机器学习模型。与将数据收集到集中服务器不同，FL
    客户端在其本地数据上训练模型，并将本地模型与 FL 服务器共享，在那里本地模型被聚合成一个全球模型。
- en: 'FL has attracted considerable attention across various domains and has been
    utilized in numerous areas such as next-word prediction (Hard et al., [2018](#bib.bib34);
    Chen et al., [2019](#bib.bib16); Ramaswamy et al., [2019](#bib.bib69)), hot-word
    detection (Leroy et al., [2019](#bib.bib51)), financial risk assessment (Byrd
    and Polychroniadou, [2020](#bib.bib12)), and cancer risk prediction (Chowdhury
    et al., [2022](#bib.bib18)), demonstrating its wide-ranging versatility. Recently,
    FL has found applications in large language models (LLMs) that expand its use
    cases. Referred to as *federated LLMs*, these models utilize FL during pre-training
    and finetuning as well as for prompt engineering (Chen et al., [2023](#bib.bib14)).
    Currently, there are industry products that utilize FL (or distributed training)
    to train LLMs, including Deepspeed ZeRO (Rajbhandari et al., [2020](#bib.bib67);
    Wang et al., [2023](#bib.bib83)), HuggingFace Accelerate (Gugger, [2021](#bib.bib33)),
    Pytorch Lightning Fabric (Antiga, [2023](#bib.bib3)). FL can facilitate LLM training
    due to the following reasons: i) Distributed nature of LLM training data: LLMs
    are pre-trained using large amounts of data, which often reside in different locations.
    Collecting such data to a central server is expensive and may also leak sensitive
    user information, while a viable way is to train LLMs in a federated manner. ii)
    Scalability and efficiency: LLMs, such as GPT-3 (Brown et al., [2020](#bib.bib11)),
    have an extremely large number of parameters. Training LLMs on a single machine
    is infeasible and inflexible, while FL can be a good choice. iii) Continuous improvement
    with user data: LLMs can be deployed in a federated manner and local instances
    of the models can be further finetuned based on the local data, enabling the global
    model to improve over time based on users’ data without ever having direct access
    to that data. This is particularly relevant for privacy-sensitive fields such
    as healthcare or personal communications.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: FL 在各个领域引起了相当大的关注，并被广泛应用于如下一词预测 (Hard et al., [2018](#bib.bib34); Chen et al.,
    [2019](#bib.bib16); Ramaswamy et al., [2019](#bib.bib69))、热词检测 (Leroy et al.,
    [2019](#bib.bib51))、金融风险评估 (Byrd 和 Polychroniadou, [2020](#bib.bib12))、癌症风险预测 (Chowdhury
    et al., [2022](#bib.bib18)) 等领域，展示了其广泛的多功能性。最近，FL 在大型语言模型（LLMs）中找到了应用，扩展了其使用场景。被称为
    *联邦 LLMs* 的这些模型在预训练和微调过程中以及提示工程中利用 FL (Chen et al., [2023](#bib.bib14))。目前，已有行业产品利用
    FL（或分布式训练）来训练 LLMs，包括 Deepspeed ZeRO (Rajbhandari et al., [2020](#bib.bib67);
    Wang et al., [2023](#bib.bib83))、HuggingFace Accelerate (Gugger, [2021](#bib.bib33))、Pytorch
    Lightning Fabric (Antiga, [2023](#bib.bib3))。由于以下原因，FL 能够促进 LLM 的训练：i) LLM 训练数据的分布式性质：LLMs
    使用大量数据进行预训练，这些数据通常分布在不同位置。将这些数据集中到服务器上既昂贵又可能泄露用户的敏感信息，而可行的方法是以联邦方式训练 LLMs。ii)
    可扩展性和效率：LLMs，例如 GPT-3 (Brown et al., [2020](#bib.bib11))，具有极其庞大的参数数量。在单台机器上训练
    LLMs 是不可行且不灵活的，而 FL 可以是一个不错的选择。iii) 与用户数据的持续改进：LLMs 可以以联邦方式部署，模型的本地实例可以基于本地数据进一步微调，使得全球模型能够随着时间的推移根据用户数据不断改进，而无需直接访问这些数据。这对于隐私敏感的领域，如医疗保健或个人通信，尤为相关。
- en: Table 1. Types of Defenses and Their Implementations Against Specific Attacks
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 针对特定攻击的防御类型及其实现
- en: '| Type of Defenses | Implementations | Type of Attacks Against |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 防御类型 | 实现 | 针对的攻击类型 |'
- en: '| Before-aggregation defenses | SLSGD (Xie et al., [2020](#bib.bib89)) | Data
    poisoning attacks, e.g., label flipping backdoor attack (Tolpegin et al., [2020](#bib.bib81)),
    |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 聚合前防御 | SLSGD (Xie et al., [2020](#bib.bib89)) | 数据中毒攻击，例如标签翻转后门攻击 (Tolpegin
    et al., [2020](#bib.bib81)) |'
- en: '| Residual Reweighting Defense (Fu et al., [2019](#bib.bib27)) | Backdoor attacks
    |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 残差加权防御 (Fu et al., [2019](#bib.bib27)) | 后门攻击 |'
- en: '| Foolsgold (Fung et al., [2020](#bib.bib28)) | Backdoor attacks and Byzantine
    attacks |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| Foolsgold (Fung et al., [2020](#bib.bib28)) | 后门攻击和拜占庭攻击 |'
- en: '| Krum (Blanchard et al., [2017](#bib.bib10)) $m$-Krum (Blanchard et al., [2017](#bib.bib10)) CClip (Karimireddy
    et al., [2020](#bib.bib43)) weak DP (Sun et al., [2019](#bib.bib79)) | Model poisoning
    attacks, e.g., Byzantine attacks (Chen et al., [2017](#bib.bib17); Fang et al.,
    [2020](#bib.bib23); Lin et al., [2019](#bib.bib54)) or Backdoor attacks that attack
    by poisoning model updates  (Bagdasaryan et al., [2020](#bib.bib4)) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| Krum (Blanchard et al., [2017](#bib.bib10)) $m$-Krum (Blanchard et al., [2017](#bib.bib10)) CClip (Karimireddy
    et al., [2020](#bib.bib43)) 弱DP (Sun et al., [2019](#bib.bib79)) | 模型中毒攻击，例如拜占庭攻击 (Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54))
    或通过中毒模型更新进行的后门攻击 (Bagdasaryan et al., [2020](#bib.bib4)) |'
- en: '| Norm Clipping (Sun et al., [2019](#bib.bib79)) Fl-wbc (Sun et al., [2021](#bib.bib78))
    Bulyan Defense(Guerraoui et al., [2018](#bib.bib32)) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 范数剪裁 (Sun 等，[2019](#bib.bib79)) Fl-wbc (Sun 等，[2021](#bib.bib78)) Bulyan
    防御(Guerraoui 等，[2018](#bib.bib32)) |'
- en: '| coordinate-wise median (Yin et al., [2018](#bib.bib93)) |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 坐标-wise 中位数 (Yin 等，[2018](#bib.bib93)) |'
- en: '| coordinate-wise trimmed mean (Yin et al., [2018](#bib.bib93)) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 坐标-wise 剪裁均值 (Yin 等，[2018](#bib.bib93)) |'
- en: '| On-aggregation defenses | Robust Learning Rate (Ozdayi et al., [2021](#bib.bib65))
    | Backdoor attacks |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 聚合防御 | 稳健学习率 (Ozdayi 等，[2021](#bib.bib65)) | 后门攻击 |'
- en: '| SLSGD (Xie et al., [2020](#bib.bib89)) | Data poisoning attacks, e.g., label
    flipping backdoor attack (Tolpegin et al., [2020](#bib.bib81)), |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| SLSGD (Xie 等，[2020](#bib.bib89)) | 数据投毒攻击，例如，标签翻转后门攻击 (Tolpegin 等，[2020](#bib.bib81))，
    |'
- en: '| geometric median (Chen et al., [2017](#bib.bib17)) | Byzantine attacks |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 几何中位数 (Chen 等，[2017](#bib.bib17)) | 拜占庭攻击 |'
- en: '| RFA (Pillutla et al., [2022](#bib.bib66)) | Byzantine attacks |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| RFA (Pillutla 等，[2022](#bib.bib66)) | 拜占庭攻击 |'
- en: '| After-aggregation defenses | CClip (Karimireddy et al., [2020](#bib.bib43))
    | Byzantine attacks or backdoor attacks |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 聚合后防御 | CClip (Karimireddy 等，[2020](#bib.bib43)) | 拜占庭攻击或后门攻击 |'
- en: '| CRFL (Xie et al., [2021](#bib.bib88)) | Backdoor attacks |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| CRFL (Xie 等，[2021](#bib.bib88)) | 后门攻击 |'
- en: Table 2. Attacks Implemented in FedMLSecurity
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. FedMLSecurity 中实现的攻击
- en: '| Type of Attacks | Implementations |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 攻击类型 | 实现方式 |'
- en: '| --- | --- |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model poisoning attacks | Byzantine attack (Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54)): (1) zero mode
    (2) random mode (3) flipping mode |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 模型投毒攻击 | 拜占庭攻击 (Chen 等，[2017](#bib.bib17); Fang 等，[2020](#bib.bib23); Lin
    等，[2019](#bib.bib54))： (1) 零模式 (2) 随机模式 (3) 翻转模式 |'
- en: '| Minimizing Distance Backdoor Attack (Baruch et al., [2019](#bib.bib5)) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 最小距离后门攻击 (Baruch 等，[2019](#bib.bib5)) |'
- en: '| Model Replacement Backdoor Attack (Bagdasaryan et al., [2020](#bib.bib4))
    |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 模型替换后门攻击 (Bagdasaryan 等，[2020](#bib.bib4)) |'
- en: '| Lazy Worker (or Free Rider) Attack (Wang, [2022](#bib.bib85); Fraboni et al.,
    [2021](#bib.bib26)) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 懒惰工作者（或免费搭车者）攻击 (Wang，[2022](#bib.bib85); Fraboni 等，[2021](#bib.bib26)) |'
- en: '| Data poisoning attacks | Label Flipping Backdoor attack (Tolpegin et al.,
    [2020](#bib.bib81)) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 数据投毒攻击 | 标签翻转后门攻击 (Tolpegin 等，[2020](#bib.bib81)) |'
- en: '| Edge Case Backdoor Attack (Wang et al., [2020b](#bib.bib84)) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 边缘案例后门攻击 (Wang 等，[2020b](#bib.bib84)) |'
- en: '| Data reconstruction attacks | Deep Leakage Attack (Zhu et al., [2019](#bib.bib96))
    |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 数据重构攻击 | 深度泄露攻击 (Zhu 等，[2019](#bib.bib96)) |'
- en: '| Inverting Gradient Attack (Geiping et al., [2020](#bib.bib30)) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 反转梯度攻击 (Geiping 等，[2020](#bib.bib30)) |'
- en: '| Revealing Labels Attack (Dang et al., [2021](#bib.bib19)) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 揭示标签攻击 (Dang 等，[2021](#bib.bib19)) |'
- en: FL, as well as federated LLMs, aims to maintain privacy and security of client
    data by allowing clients to train locally without spreading their data to other
    parties. However, its decentralized and collaborative nature might inadvertently
    introduce privacy and security vulnerabilities. Recent works have spotlighted
    specific attack mechanisms in FL. Adversarial clients compromise the integrity
    of global model by submitting spurious models to prevent the global model from
    converging (Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23);
    Lin et al., [2019](#bib.bib54); Baruch et al., [2019](#bib.bib5); Bagdasaryan
    et al., [2020](#bib.bib4); Wang, [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26))),
    manipulating data samples to induce the global model to mis-classify specific
    samples (Tolpegin et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)),
    and / or planting backdoors (Baruch et al., [2019](#bib.bib5); Bagdasaryan et al.,
    [2020](#bib.bib4); Tolpegin et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)).
    Adversaries can also reconstruct private data from shared model updates (Zhu et al.,
    [2019](#bib.bib96); Geiping et al., [2020](#bib.bib30); Dang et al., [2021](#bib.bib19)).
    Meanwhile, a wide range of defense mechanisms has emerged to mitigate the impact
    of these attacks (Li et al., [2022](#bib.bib53); Kumari et al., [2023](#bib.bib46);
    Sun et al., [2019](#bib.bib79); Ozdayi et al., [2021](#bib.bib65); Blanchard et al.,
    [2017](#bib.bib10); Xie et al., [2020](#bib.bib89); Chen et al., [2017](#bib.bib17);
    Sun et al., [2019](#bib.bib79); Karimireddy et al., [2020](#bib.bib43); Yin et al.,
    [2018](#bib.bib93); Pillutla et al., [2022](#bib.bib66); Fung et al., [2020](#bib.bib28);
    Xie et al., [2021](#bib.bib88); Yin et al., [2018](#bib.bib93); Ma et al., [2022](#bib.bib59);
    Kumar et al., [2022](#bib.bib45); Chen et al., [2022](#bib.bib15)). Despite the
    efforts for addressing the vulnerability of FL systems, there still lacks a comprehensive
    benchmark for comparing approaches under unified sittings. Moreover, while existing
    works have explored effectiveness of attacks and defenses on small-scale models,
    there remains a significant gap in understanding how these mechanisms perform
    against large-scale models, such as LLMs. Given that LLMs possess a large number
    of parameters and are trained on complex datasets obtained from unregulated sources,
    the effectiveness of attacks and defenses may be diminished when applied to them.
    These motivate an urgent need for a standardized and comprehensive benchmark to
    evaluate baseline attack and defense mechanisms in the context of FL and federated
    LLMs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: FL（联邦学习）以及联邦大语言模型的目标是通过允许客户端在本地进行训练而无需将数据传播给其他方，从而维护客户端数据的隐私和安全。然而，其去中心化和协作的特性可能无意中引入隐私和安全漏洞。最近的研究揭示了FL中特定的攻击机制。对抗性客户端通过提交虚假的模型来破坏全球模型的完整性，以防全球模型收敛（Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54);
    Baruch et al., [2019](#bib.bib5); Bagdasaryan et al., [2020](#bib.bib4); Wang,
    [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26)），操控数据样本以诱导全球模型对特定样本进行错误分类（Tolpegin
    et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)），和/或植入后门（Baruch et
    al., [2019](#bib.bib5); Bagdasaryan et al., [2020](#bib.bib4); Tolpegin et al.,
    [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84)）。对手还可以从共享模型更新中重构私人数据（Zhu
    et al., [2019](#bib.bib96); Geiping et al., [2020](#bib.bib30); Dang et al., [2021](#bib.bib19)）。与此同时，已经出现了多种防御机制来减轻这些攻击的影响（Li
    et al., [2022](#bib.bib53); Kumari et al., [2023](#bib.bib46); Sun et al., [2019](#bib.bib79);
    Ozdayi et al., [2021](#bib.bib65); Blanchard et al., [2017](#bib.bib10); Xie et
    al., [2020](#bib.bib89); Chen et al., [2017](#bib.bib17); Sun et al., [2019](#bib.bib79);
    Karimireddy et al., [2020](#bib.bib43); Yin et al., [2018](#bib.bib93); Pillutla
    et al., [2022](#bib.bib66); Fung et al., [2020](#bib.bib28); Xie et al., [2021](#bib.bib88);
    Yin et al., [2018](#bib.bib93); Ma et al., [2022](#bib.bib59); Kumar et al., [2022](#bib.bib45);
    Chen et al., [2022](#bib.bib15)）。尽管已经采取了应对FL系统漏洞的努力，但仍缺乏一个统一的基准来比较不同的方法。此外，尽管现有的研究探讨了攻击和防御在小规模模型上的有效性，但对这些机制在大规模模型（如大语言模型）上的表现仍存在显著差距。鉴于大语言模型具有大量参数并在来自非规范来源的复杂数据集上进行训练，当应用于这些模型时，攻击和防御的有效性可能会降低。这些因素促使我们迫切需要一个标准化和全面的基准，以评估FL和联邦大语言模型背景下的基本攻击和防御机制。
- en: 'This paper introduces FedMLSecurity¹¹1Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security,
    a benchmark that simulates attacks and defenses in FedML (He et al., [2020b](#bib.bib36)).
    FedMLSecurity comprises two primary components: FedMLAttacker and FedMLDefender.
    FedMLAttacker simulates attacks in FL to help understand and prepare for potential
    security risks, while FedMLDefender is equipped with state-of-the-arts defense
    mechanisms to counteract the attacks injected by FedMLAttacker. We summarize our
    contributions as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '本文介绍了FedMLSecurity¹¹1Code: https://github.com/FedML-AI/FedML/tree/master/python/fedml/core/security，这是一个模拟FedML中的攻击和防御的基准。FedMLSecurity包含两个主要组件：FedMLAttacker和FedMLDefender。FedMLAttacker模拟FL中的攻击，以帮助理解和准备潜在的安全风险，而FedMLDefender配备了最先进的防御机制，以抵御FedMLAttacker注入的攻击。我们的贡献总结如下：'
- en: 'i) Enabling benchmarking of several different attacks and defenses in FL. FedMLSecurity
    implements attacks and defenses that are widely considered in the literature.
    We summarize the defenses and the attacks in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") and Table [2](#S1.T2 "Table 2 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs"),
    respectively.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'i) 使得FL中多个不同攻击和防御的基准测试成为可能。FedMLSecurity实现了文献中广泛考虑的攻击和防御。我们在表[1](#S1.T1 "表1
    ‣ 1\. 引言 ‣ FedMLSecurity: 联邦学习和联邦LLMs中的攻击和防御基准")和表[2](#S1.T2 "表2 ‣ 1\. 引言 ‣ FedMLSecurity:
    联邦学习和联邦LLMs中的攻击和防御基准")中总结了防御和攻击。'
- en: 'ii) Supporting flexible configuration and customization. FedMLSecurity supports
    configurations using a .yaml file. Sample configurations for attacks and defenses
    are shown in Figures [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    and Figures [1(b)](#S1.F1.sf2 "In Figure 1 ‣ 1\. Introduction ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs"),
    respectively. FedMLSecurity also provides APIs to enable customizing attacks and
    defenses.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 'ii) 支持灵活配置和自定义。FedMLSecurity支持使用.yaml文件进行配置。攻击和防御的示例配置分别展示在图[1(a)](#S1.F1.sf1
    "在图1 ‣ 1\. 引言 ‣ FedMLSecurity: 联邦学习和联邦LLMs中的攻击和防御基准")和图[1(b)](#S1.F1.sf2 "在图1
    ‣ 1\. 引言 ‣ FedMLSecurity: 联邦学习和联邦LLMs中的攻击和防御基准")中。FedMLSecurity还提供了API以实现攻击和防御的自定义。'
- en: '![Refer to caption](img/cec835d3c2efad40814c7254dafcf62e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/cec835d3c2efad40814c7254dafcf62e.png)'
- en: (a) Byzantine attack (Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 拜占庭攻击（Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23)）。
- en: '![Refer to caption](img/56198c310fed42be68fb8720045b2128.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/56198c310fed42be68fb8720045b2128.png)'
- en: (b) $m$-Krum (Blanchard et al., [2017](#bib.bib10)).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $m$-Krum（Blanchard et al., [2017](#bib.bib10)）。
- en: Figure 1. Examples of attack and defense configurations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 攻击和防御配置的示例。
- en: iii) Supporting various models and FL optimizers. FedMLSecurity can be utilized
    with a wide range of models, including Logistic Regression, LeNet (LeCun et al.,
    [1998](#bib.bib50)), ResNet (He et al., [2015](#bib.bib38)), CNN (LeCun et al.,
    [1989](#bib.bib49)), RNN (Rumelhart et al., [1986](#bib.bib74)), GAN (Goodfellow
    et al., [2014](#bib.bib31)), and so on. FedMLSecurity is compatible with various
    FL optimizers, such as FedAVG (McMahan et al., [2016](#bib.bib62)), FedSGD (Shokri
    and Shmatikov, [2015](#bib.bib76)), FedOPT (Reddi et al., [2021](#bib.bib71)),
    FedPROX (Li et al., [2020](#bib.bib52)), FedGKT (He et al., [2020a](#bib.bib35)),
    FedGAN (Rasouli et al., [2020](#bib.bib70)), FedNAS (He et al., [2021](#bib.bib37)),
    FedNOVA (Wang et al., [2020a](#bib.bib86)), etc.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 支持多种模型和FL优化器。FedMLSecurity可以与多种模型一起使用，包括Logistic Regression、LeNet（LeCun
    et al., [1998](#bib.bib50)）、ResNet（He et al., [2015](#bib.bib38)）、CNN（LeCun et
    al., [1989](#bib.bib49)）、RNN（Rumelhart et al., [1986](#bib.bib74)）、GAN（Goodfellow
    et al., [2014](#bib.bib31)）等。FedMLSecurity与多种FL优化器兼容，如FedAVG（McMahan et al., [2016](#bib.bib62)）、FedSGD（Shokri
    and Shmatikov, [2015](#bib.bib76)）、FedOPT（Reddi et al., [2021](#bib.bib71)）、FedPROX（Li
    et al., [2020](#bib.bib52)）、FedGKT（He et al., [2020a](#bib.bib35)）、FedGAN（Rasouli
    et al., [2020](#bib.bib70)）、FedNAS（He et al., [2021](#bib.bib37)）、FedNOVA（Wang
    et al., [2020a](#bib.bib86)）等。
- en: iv) Extensions to federated LLMs and real-world applications. FedMLSecurity
    can simulate attacks and defenses during training of federated LLMs. It can also
    be integrated with real-world FL applications; see Exp 7, where we utilize edge
    devices from Theta Network (Theta Network., [2023](#bib.bib80)) instead of simulations.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: iv) 对联邦大型语言模型和实际应用的扩展。FedMLSecurity可以在训练联邦大型语言模型期间模拟攻击和防御。它也可以与实际的联邦学习应用集成；见Exp
    7，我们利用Theta Network（Theta Network., [2023](#bib.bib80)）的边缘设备，而不是模拟。
- en: 'Key takeaways: i) While defense mechanisms can help mitigate attacks, it might
    also bring a potential loss of accuracy to the aggregation results. Therefore,
    when integrating defenses into FL applications, it’s crucial to weigh the benefits
    against potential drawbacks. ii) Nearly all existing defense mechanisms are impractical
    in real-world FL applications, as they compromise accuracy even if no attack happened.
    A defense that is practical for real-world systems is in need, where the defense
    should satisfy: 1) it must detect if attacks have happened, and only activates
    defensive mechanisms when attacks are detected; and 2) it must identify malicious
    clients accurately without harming benign local models.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关键要点：i) 尽管防御机制可以帮助缓解攻击，但它也可能带来聚合结果的潜在准确性损失。因此，在将防御措施整合到联邦学习应用中时，必须权衡其好处与潜在的缺陷。ii)
    几乎所有现有的防御机制在实际的联邦学习应用中都不切实际，因为即使没有发生攻击，它们也会妨碍准确性。需要一种对实际系统切实可行的防御措施，该防御措施应满足以下条件：1)
    必须能够检测是否发生了攻击，并且只有在检测到攻击时才激活防御机制；2) 必须准确识别恶意客户端，而不伤害良性的本地模型。
- en: 2\. Preliminaries and Overview
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 基础知识和概述
- en: This section discusses the related literature, then introduces adversarial models,
    and finally overviews FedMLSecurity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论相关文献，然后介绍对抗性模型，最后概述FedMLSecurity。
- en: 2.1\. Existing Benchmark Frameworks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 现有的基准框架
- en: Recent years, multiple benchmarks have been introduced for FL (Abadi et al.,
    [2015](#bib.bib2); Ziller et al., [2021](#bib.bib97); Liu et al., [2021](#bib.bib55);
    Beutel et al., [2020](#bib.bib7); Lai et al., [2022](#bib.bib47); Roth et al.,
    [2022](#bib.bib73); Reina et al., [2021](#bib.bib72); Silva et al., [2020](#bib.bib77);
    Ludwig et al., [2020](#bib.bib56); Xie et al., [2022](#bib.bib90); Dimitriadis
    et al., [2022](#bib.bib21)). Among these, only FederatedScope (Xie et al., [2022](#bib.bib90))
    delves into the implications of adversarial attacks in FL, with a focus on data
    reconstruction attacks that utilize models or gradients to revert sensitive information,
    including GAN-based leakage attack (Hitaj et al., [2017](#bib.bib40)), Passive
    Property Inference (Melis et al., [2019](#bib.bib64)), and DLG attack (Zhu et al.,
    [2019](#bib.bib96)). However, FederatedScope neglects to address attacks prevalent
    in the research literature, e.g., Byzantine attacks (Yin et al., [2018](#bib.bib93);
    Yang et al., [2019](#bib.bib92)). It also does not include any defense mechanisms
    for FL. It is worth noting that, while FederatedScope integrates secret-sharing (Beimel,
    [2011](#bib.bib6)), it is in the scope of federated analytics (Elkordy et al.,
    [2023](#bib.bib22); Ramage, [2020](#bib.bib68); Wang et al., [2022b](#bib.bib82);
    Jung et al., [2012](#bib.bib42)), instead of FL.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，多个基准已经被引入到联邦学习领域（Abadi et al., [2015](#bib.bib2); Ziller et al., [2021](#bib.bib97);
    Liu et al., [2021](#bib.bib55); Beutel et al., [2020](#bib.bib7); Lai et al.,
    [2022](#bib.bib47); Roth et al., [2022](#bib.bib73); Reina et al., [2021](#bib.bib72);
    Silva et al., [2020](#bib.bib77); Ludwig et al., [2020](#bib.bib56); Xie et al.,
    [2022](#bib.bib90); Dimitriadis et al., [2022](#bib.bib21))。其中，只有FederatedScope（Xie
    et al., [2022](#bib.bib90)）深入探讨了对抗性攻击在联邦学习中的影响，重点关注利用模型或梯度来恢复敏感信息的数据重建攻击，包括基于GAN的泄露攻击（Hitaj
    et al., [2017](#bib.bib40)）、被动属性推断（Melis et al., [2019](#bib.bib64)）和DLG攻击（Zhu
    et al., [2019](#bib.bib96)）。然而，FederatedScope忽略了在研究文献中常见的攻击，例如拜占庭攻击（Yin et al.,
    [2018](#bib.bib93); Yang et al., [2019](#bib.bib92)）。它也没有包括任何针对联邦学习的防御机制。值得注意的是，尽管FederatedScope集成了秘密共享（Beimel,
    [2011](#bib.bib6)），但它属于联邦分析（Elkordy et al., [2023](#bib.bib22); Ramage, [2020](#bib.bib68);
    Wang et al., [2022b](#bib.bib82); Jung et al., [2012](#bib.bib42)）的范围，而非联邦学习。
- en: FedMLSecurity implements attacks that are widely considered in the literature (Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Lin et al., [2019](#bib.bib54);
    Baruch et al., [2019](#bib.bib5); Bagdasaryan et al., [2020](#bib.bib4); Wang,
    [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26); Tolpegin et al., [2020](#bib.bib81);
    Wang et al., [2020b](#bib.bib84); Zhu et al., [2019](#bib.bib96); Geiping et al.,
    [2020](#bib.bib30); Dang et al., [2021](#bib.bib19)); it also integrates a wide
    range of defense mechanisms (Sun et al., [2019](#bib.bib79); Ozdayi et al., [2021](#bib.bib65);
    Blanchard et al., [2017](#bib.bib10); Xie et al., [2020](#bib.bib89); Chen et al.,
    [2017](#bib.bib17); Sun et al., [2019](#bib.bib79); Karimireddy et al., [2020](#bib.bib43);
    Yin et al., [2018](#bib.bib93); Pillutla et al., [2022](#bib.bib66); Fung et al.,
    [2020](#bib.bib28); Xie et al., [2021](#bib.bib88); Yin et al., [2018](#bib.bib93)).
    Designed with flexibility in mind, FedMLSecurity offers configurable settings
    and APIs, enabling users to customize their attack and defense mechanisms.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: FedMLSecurity 实现了文献中广泛讨论的攻击 (Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23);
    Lin et al., [2019](#bib.bib54); Baruch et al., [2019](#bib.bib5); Bagdasaryan
    et al., [2020](#bib.bib4); Wang, [2022](#bib.bib85); Fraboni et al., [2021](#bib.bib26);
    Tolpegin et al., [2020](#bib.bib81); Wang et al., [2020b](#bib.bib84); Zhu et
    al., [2019](#bib.bib96); Geiping et al., [2020](#bib.bib30); Dang et al., [2021](#bib.bib19))；它还集成了广泛的防御机制
    (Sun et al., [2019](#bib.bib79); Ozdayi et al., [2021](#bib.bib65); Blanchard
    et al., [2017](#bib.bib10); Xie et al., [2020](#bib.bib89); Chen et al., [2017](#bib.bib17);
    Sun et al., [2019](#bib.bib79); Karimireddy et al., [2020](#bib.bib43); Yin et
    al., [2018](#bib.bib93); Pillutla et al., [2022](#bib.bib66); Fung et al., [2020](#bib.bib28);
    Xie et al., [2021](#bib.bib88); Yin et al., [2018](#bib.bib93))。FedMLSecurity
    设计时考虑了灵活性，提供了可配置的设置和API，允许用户自定义其攻击和防御机制。
- en: 2.2\. Adversarial Model
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 对抗模型
- en: 'Adversaries in FL fall into two categories: active and passive, corresponding
    to security risks and privacy threat in FL, respectively.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: FL中的对手分为两类：主动对手和被动对手，分别对应FL中的安全风险和隐私威胁。
- en: 'Active Adversaries. Active adversaries intentionally manipulate training data
    or trained models to achieve malicious goals. This might involve altering models
    to prevent global model convergence (e.g., Byzantine attacks (Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23))), or subtly misclassifying a specific set of
    samples to minimally impact the overall performance of the global model (e.g.,
    backdoor attacks (Bagdasaryan et al., [2020](#bib.bib4); Wang et al., [2020b](#bib.bib84);
    Zhang et al., [2022](#bib.bib95))). Active adversaries can take different forms,
    including: 1) malicious clients who manipulate their local models (Bagdasaryan
    et al., [2020](#bib.bib4); Chen et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23);
    Zhang et al., [2022](#bib.bib95)) or submit contrived models without actual training (Wang,
    [2022](#bib.bib85)); 2) a global “sybil” (Tolpegin et al., [2020](#bib.bib81);
    Fung et al., [2020](#bib.bib28)) that has full access to the FL system and possesses
    complete knowledge of the entire system, including local models and global models
    and clients’ local datasets. This “sybil” may also modify data within the FL system,
    such as clients’ local datasets and their submitted local models; and 3) external
    adversaries capable of monitoring the communication channel between clients and
    the server, thereby intercepting and altering local models during the transfer
    process.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 主动对手。主动对手故意操控训练数据或训练模型以实现恶意目标。这可能涉及改变模型以防止全局模型收敛（例如，拜占庭攻击 (Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23)))，或微妙地将特定样本错误分类，以最小化对全局模型整体性能的影响（例如，后门攻击 (Bagdasaryan
    et al., [2020](#bib.bib4); Wang et al., [2020b](#bib.bib84); Zhang et al., [2022](#bib.bib95)))。主动对手可以采取不同的形式，包括：1)
    恶意客户端，他们操控其本地模型 (Bagdasaryan et al., [2020](#bib.bib4); Chen et al., [2017](#bib.bib17);
    Fang et al., [2020](#bib.bib23); Zhang et al., [2022](#bib.bib95)) 或提交未经实际训练的伪造模型
    (Wang, [2022](#bib.bib85)); 2) 一个全球“sybil” (Tolpegin et al., [2020](#bib.bib81);
    Fung et al., [2020](#bib.bib28))，它拥有对FL系统的完全访问权限，并掌握整个系统的完整知识，包括本地模型、全局模型以及客户端的本地数据集。这个“sybil”还可以修改FL系统内的数据，如客户端的本地数据集和提交的本地模型；3)
    外部对手能够监控客户端和服务器之间的通信渠道，从而在传输过程中拦截和修改本地模型。
- en: 'Passive Adversaries. Passive adversaries do not modify data or models, but
    may still pose a threat to data privacy by potentially deducing sensitive information
    (such as local training data) from revealed models (gradients, or model updates) (Zhu
    et al., [2019](#bib.bib96)). Examples of passive adversaries include: 1) an adversarial
    FL server attempting to infer local training data using submitted local models;
    2) adversarial FL clients trying to deduce other clients’ training data using
    the global model provided by the server; and 3) external adversaries, e.g., hackers,
    that access communication channels to acquire local and global models transferred
    between clients and the FL server.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 被动对手。被动对手不会修改数据或模型，但可能通过从暴露的模型（梯度或模型更新）中推断敏感信息（如本地训练数据）来对数据隐私构成威胁（Zhu 等，[2019](#bib.bib96)）。被动对手的例子包括：1)
    尝试利用提交的本地模型推断本地训练数据的对抗 FL 服务器；2) 尝试利用服务器提供的全局模型推断其他客户端训练数据的对抗 FL 客户端；3) 外部对手，例如黑客，通过访问通信通道获取客户端与
    FL 服务器之间传输的本地和全局模型。
- en: Adversaries can inject attacks at different stages of FL. Specifically, active
    adversaries can conduct model poisoning attacks that manipulate local models,
    or data poisoning attack that tampers with local datasets. On the other hand,
    passive adversaries pose a privacy threat based on the models updates or gradients,
    i.e., data reconstruction attacks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对手可以在 FL 的不同阶段注入攻击。具体来说，主动对手可以进行模型中毒攻击，操控本地模型，或者数据中毒攻击，干扰本地数据集。另一方面，被动对手则基于模型更新或梯度构成隐私威胁，即数据重建攻击。
- en: '![Refer to caption](img/d51662c307a198f54de90aa98ddae488.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d51662c307a198f54de90aa98ddae488.png)'
- en: Figure 2. FedMLSecurity overview. FedMLSecurity enables injecting attacks /
    defenses (shown in red / green) at various stages of FL at the clients and at
    the server.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. FedMLSecurity 概述。FedMLSecurity 允许在客户端和服务器的不同阶段注入攻击/防御（分别用红色/绿色表示）。
- en: 2.3\. Overview of FedMLSecurity
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. FedMLSecurity 概述
- en: FedMLSecurity serves as an external component that injects attacks and defenses
    at different stages of training without altering the existing processes in existing
    FL benchmarks. FedMLSecurity utilizes FedMLAttacker and FedMLDefender to initiate
    two instances and simulate attacks and defenses, respectively. The two instances
    are initialized once and are accessible by other objects in the FL system²²2Such
    design is achieved by the singleton design pattern (Gamma et al., [1995](#bib.bib29))..
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: FedMLSecurity 作为一个外部组件，在不同的训练阶段注入攻击和防御，而不改变现有 FL 基准中的过程。FedMLSecurity 利用 FedMLAttacker
    和 FedMLDefender 启动两个实例，分别模拟攻击和防御。这两个实例初始化一次，并且可以被 FL 系统中的其他对象访问²²2这种设计通过单例设计模式实现（Gamma
    等，[1995](#bib.bib29)）。
- en: 'Inputs: : local models of the current FL round.Variables:  : A FedMLDefender
    instance.1Function       35Function ........10Function ...13Function ...'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：：当前 FL 轮次的本地模型。变量：：一个 FedMLDefender 实例。1Function       35Function ........10Function
    ...13Function ...
- en: Algorithm 1 Server Aggregation
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 服务器聚合
- en: 'Injection of attacks. Without loss of generality, we classify the attacks in
    FL into the following three categories based on the targets of the attacks:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击注入。一般来说，我们根据攻击的目标将 FL 中的攻击分为以下三类：
- en: i) Data poisoning attacks that are conducted by active adversaries to modify
    clients’ local datasets and are injected at clients (Tolpegin et al., [2020](#bib.bib81);
    Dang et al., [2021](#bib.bib19)).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: i) 数据中毒攻击由主动对手进行，目的是修改客户端的本地数据集，并在客户端注入（Tolpegin 等，[2020](#bib.bib81)；Dang 等，[2021](#bib.bib19)）。
- en: ii) Model poisoning attacks that are also conducted by active adversaries to
    temper with local models submitted by clients (Fang et al., [2020](#bib.bib23);
    Shejwalkar and Houmansadr, [2021](#bib.bib75); Bhagoji et al., [2019](#bib.bib8)).
    FedMLAttacker injects these attacks before the aggregation of local models in
    each FL training round at the server, so that it can get access to all client
    models submitted in that training round.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 模型中毒攻击也由主动对手进行，目的是干扰客户端提交的本地模型（Fang 等，[2020](#bib.bib23)；Shejwalkar 和 Houmansadr，[2021](#bib.bib75)；Bhagoji
    等，[2019](#bib.bib8)）。FedMLAttacker 在每次 FL 训练轮次的服务器端对本地模型进行聚合之前注入这些攻击，以便能够访问该训练轮次中所有提交的客户端模型。
- en: 'Inputs: : A FedAttacker instance.1Function ...      4$\mathit{send}\_\mathit{to}\_\mathit{server}(\mathbf{w}_{l})$'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：：一个 FedAttacker 实例。1Function ...      4$\mathit{send}\_\mathit{to}\_\mathit{server}(\mathbf{w}_{l})$
- en: Algorithm 2 Client Training
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 客户端训练
- en: iii) Data reconstruction attacks that are conducted by passive adversaries by
    exploring local models or updates to infer information about the training data (Melis
    et al., [2018](#bib.bib63); Zhang et al., [2020](#bib.bib94); Luo et al., [2021](#bib.bib58);
    Wang et al., [2022a](#bib.bib87); Fowl et al., [2021](#bib.bib25)). FedMLAttacker
    injects such attacks at the FL server, as the FL server has access to all local
    models and the global model of each iteration, and can perform the attacks with
    flexibility.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 数据重建攻击是由被动对手通过探索本地模型或更新来推断训练数据的信息（Melis et al., [2018](#bib.bib63); Zhang
    et al., [2020](#bib.bib94); Luo et al., [2021](#bib.bib58); Wang et al., [2022a](#bib.bib87);
    Fowl et al., [2021](#bib.bib25)）。FedMLAttacker 在 FL 服务器上注入此类攻击，因为 FL 服务器可以访问每次迭代的所有本地模型和全局模型，并且可以灵活地执行这些攻击。
- en: 'Injection of defenses. FedMLDefender incorporates defenses to mitigate, if
    not completely nullify, the impacts of injected attacks. Since the defenses are
    tailored to address issues related to tampered local models³³3Note that poisoning
    local datasets also results in tampered local models. or information leakage during
    the exchange of model updates between clients and the FL server, the defense mechanisms
    can manipulate local models and the aggregation procedure to counteract the attacks.
    To facilitate this, FedMLDefender deploys defenses at the FL server, and provides
    flexible APIs that enable obtaining all local models and the global model of each
    FL round while allowing for a customized aggregation process. FedMLDefender utilize
    three functions at different stages of FL aggregation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 防御注入。FedMLDefender 结合了防御机制，以减轻（如果不能完全消除）注入攻击的影响。由于这些防御机制旨在解决与篡改本地模型相关的问题³³3
    请注意，污染本地数据集也会导致篡改的本地模型。或在客户端与 FL 服务器之间交换模型更新时的信息泄漏，防御机制可以操控本地模型和聚合过程以对抗攻击。为此，FedMLDefender
    在 FL 服务器上部署防御措施，并提供灵活的 API，使得可以获取每个 FL 回合的所有本地模型和全局模型，同时允许自定义的聚合过程。FedMLDefender
    在 FL 聚合的不同阶段利用三个功能：
- en: i) Before-aggregation functions that modify local models at the server before
    aggregating them.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: i) 聚合前函数，这些函数在聚合本地模型之前修改服务器上的本地模型。
- en: ii) On-aggregation functions that modify the FL aggregation function to mitigate
    the impacts of malicious local models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ii) 聚合中函数，这些函数修改 FL 聚合函数以减轻恶意本地模型的影响。
- en: iii) After-aggregation functions that modify the aggregated global model (e.g.,
    by adding noise or clipping) to protect the real global model or improve its quality.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: iii) 聚合后函数，这些函数修改聚合后的全局模型（例如，通过添加噪声或剪切）以保护真实的全局模型或改善其质量。
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.2\. Adversarial Model ‣ 2\. Preliminaries and
    Overview ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning
    and Federated LLMs") summarizes the injections of attacks and defenses to the
    FL framework in FedMLSecurity. We also provide detailed algorithms for injecting
    attacks and defenses to different stages of FL training, as shown in Algorithm [1](#alg1
    "In 2.3\. Overview of FedMLSecurity ‣ 2\. Preliminaries and Overview ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    (for server aggregation) and Algorithm [2](#alg2 "In 2.3\. Overview of FedMLSecurity
    ‣ 2\. Preliminaries and Overview ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs") (for client training). Below,
    we explain the implementations of attacks and defenses in detail.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2](#S2.F2 "图 2 ‣ 2.2\. 对抗模型 ‣ 2\. 基础知识与概述 ‣ FedMLSecurity: 联邦学习和联邦 LLMs
    中攻击与防御的基准") 总结了在 FedMLSecurity 中对 FL 框架的攻击和防御注入。我们还提供了在 FL 训练不同阶段注入攻击和防御的详细算法，如算法 [1](#alg1
    "在 2.3\. FedMLSecurity 概述 ‣ 2\. 基础知识与概述 ‣ FedMLSecurity: 联邦学习和联邦 LLMs 中攻击与防御的基准")（针对服务器聚合）和算法 [2](#alg2
    "在 2.3\. FedMLSecurity 概述 ‣ 2\. 基础知识与概述 ‣ FedMLSecurity: 联邦学习和联邦 LLMs 中攻击与防御的基准")（针对客户端训练）。以下，我们详细解释攻击和防御的实现。'
- en: 3\. Implementation of Attacks
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 攻击的实现
- en: FedMLAttacker injects model poisoning, data poisoning, and data reconstruction
    attacks at different stages of FL training and provides APIs for these attacks.
    We present each class of attacks and user integration of a new attack to FedMLSecurity.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: FedMLAttacker 在 FL 训练的不同阶段注入模型污染、数据污染和数据重建攻击，并为这些攻击提供 API。我们介绍了每类攻击及用户如何将新攻击集成到
    FedMLSecurity 中。
- en: 3.1\. Model Poisoning Attacks
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 模型污染攻击
- en: Model poisoning attacks modify the local models submitted by clients. FedMLAttacker
    injects such attacks before FL aggregation in each iteration, modifying each local
    model directly. As an example, FedMLAttacker implements three modes of Byzantine
    attacks (Yin et al., [2018](#bib.bib93); Yang et al., [2019](#bib.bib92); Lin
    et al., [2019](#bib.bib54); Xu et al., [2022](#bib.bib91)), including i) Zero
    mode (Lin et al., [2019](#bib.bib54)) that poisons the client models by setting
    their weights to zero; ii) Random mode (Lin et al., [2019](#bib.bib54)) that manipulates
    client models by attributing random values to model weights; and iii) Flipping
    mode (Xu et al., [2022](#bib.bib91)) that updates the global model in the opposite
    direction by formulating a poisoned local model based on the global model  as
    $\textbf{w}_{g}+(\textbf{w}_{g}-\textbf{w}_{\ell})$.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中毒攻击通过修改客户端提交的本地模型来实现。FedMLAttacker 在每次迭代中的 FL 聚合之前注入此类攻击，直接修改每个本地模型。例如，FedMLAttacker
    实现了三种拜占庭攻击模式（Yin et al., [2018](#bib.bib93); Yang et al., [2019](#bib.bib92);
    Lin et al., [2019](#bib.bib54); Xu et al., [2022](#bib.bib91)），包括 i) Zero 模式（Lin
    et al., [2019](#bib.bib54)），通过将客户端模型的权重设置为零来进行中毒；ii) Random 模式（Lin et al., [2019](#bib.bib54)），通过将随机值分配给模型权重来操控客户端模型；iii)
    Flipping 模式（Xu et al., [2022](#bib.bib91)），通过基于全局模型 $\textbf{w}_{g}+(\textbf{w}_{g}-\textbf{w}_{\ell})$
    制定一个中毒的本地模型，更新全局模型的方向。
- en: APIs for Model Poisoning Attacks. FedMLAttacker has two APIs for model poisoning
    attacks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型中毒攻击**的API。FedMLAttacker 提供了两个用于模型中毒攻击的API。'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: is a list of tuples containing the number of data samples and the submitted
    client models. The input $\mathit{auxiliary}\_\mathit{info}$ is any information
    used in the defense, e.g., the global model in the last FL iteration.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是一个包含数据样本数量和提交的客户端模型的元组列表。输入的 $\mathit{auxiliary}\_\mathit{info}$ 是在防御中使用的任何信息，例如，最后一次
    FL 迭代中的全局模型。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{model}\_\mathit{poisoning}\_\mathit{attack}()$ that checks
    whether the attack is activated and whether the attack modifies local models.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{model}\_\mathit{poisoning}\_\mathit{attack}()$ 检查攻击是否激活以及攻击是否修改本地模型。
- en: 3.2\. Data Poisoning Attacks
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 数据中毒攻击
- en: Data poisoning attacks modify local datasets of one or multiple clients to achieve
    some malicious goals, e.g., degrading the performance of the global model or inducing
    the global model to misclassify some samples. As an example, in label flipping
    attack (Tolpegin et al., [2020](#bib.bib81)), a global “sybil” controls some clients
    and modifies their local data by mislabeling samples of some classes to wrong
    classes. Given a source class (or label) , all samples with class .
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒攻击通过修改一个或多个客户端的本地数据集来实现一些恶意目标，例如，降低全局模型的性能或使全局模型错误分类某些样本。例如，在标签翻转攻击（Tolpegin
    et al., [2020](#bib.bib81)）中，全球“sybil”控制一些客户端，并通过将某些类别的样本错误标记为其他类别来修改它们的本地数据。给定一个源类别（或标签），所有具有该类别的样本。
- en: APIs for Data Poisoning Attacks. While poisoning local data can be performed
    by either a global “sybil” or individual malicious clients, to address a more
    general case, FedMLAttacker design APIs for the global sybil to enable enhanced
    control over each local dataset. FedMLAttacker has two APIs for data poisoning
    attacks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据中毒攻击**的API。虽然本地数据的中毒可以由全球“sybil”或个别恶意客户端执行，但为了处理更一般的情况，FedMLAttacker 设计了
    API 以使全球 sybil 对每个本地数据集进行更强的控制。FedMLAttacker 提供了两个用于数据中毒攻击的API。'
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{poison}\_\mathit{data}(\mathit{dataset})$, which takes a local dataset
    and mislabels a set of chosen samples based on the clients’ (or attackers’) requirements,
    which are included in the configuration. Normally, clients would change labels
    of a specific subset of samples to some other labels in the same dataset, or label
    a set of samples to new classes that do not exist in the dataset.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{poison}\_\mathit{data}(\mathit{dataset})$，该函数接受本地数据集，并根据客户端（或攻击者）的要求（这些要求包含在配置中）错误标记一组选定的样本。通常，客户端会将数据集中某个特定子集的标签更改为其他标签，或者将一组样本标记为数据集中不存在的新类别。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{data}\_\mathit{poisoning}\_\mathit{attack}()$, which examines
    whether FedMLAttacker is enabled and whether the attack requires poisoning the
    datasets.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{data}\_\mathit{poisoning}\_\mathit{attack}()$，该函数检查 FedMLAttacker
    是否启用以及攻击是否需要中毒数据集。
- en: 3.3\. Data Reconstruction Attacks
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 数据重构攻击
- en: Data reconstruction attacks are performed by passive adversaries that attempts
    to infer sensitive information without actively interfering with the FL training
    or the local data. We assume that there is no leakage during the local training
    process in FL, as clients are on their fully trusted local machines. Thus, data
    reconstruction attacks take the trained models (either the global model or the
    local models) to revert training data. For example, Deep Leakage from Gradients
    (DLG) attack (Zhu et al., [2019](#bib.bib96)) infers local training data from
    the publicly shared gradients. A passive adversary can use the global model from
    the previous FL training round and the newly obtained model to compute a “model
    update” between models in different FL training rounds to deduce the training
    data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重构攻击是由被动对手执行的，试图在不主动干扰 FL 训练或本地数据的情况下推断敏感信息。我们假设在 FL 的本地训练过程中没有泄露，因为客户端是在其完全信任的本地机器上。因此，数据重构攻击利用训练好的模型（无论是全局模型还是本地模型）来恢复训练数据。例如，深度梯度泄漏
    (DLG) 攻击 (Zhu et al., [2019](#bib.bib96)) 从公开共享的梯度中推断本地训练数据。被动对手可以使用前一个 FL 训练轮次中的全局模型和新获得的模型计算模型间的“模型更新”，以推断训练数据。
- en: APIs for Data Reconstruction Attacks. We have two APIs for data reconstruction
    attacks.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 数据重构攻击的 API。我们有两个用于数据重构攻击的 API。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ) to help infer.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ) 来帮助推断。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{data}\_\mathit{reconstruction}\_\mathit{attack}()$, which
    examines whether the attack component is enabled and whether the attack requires
    reconstructing training data using the trained models.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{data}\_\mathit{reconstruction}\_\mathit{attack}()$，用于检查攻击组件是否启用，以及攻击是否需要使用训练好的模型来重构训练数据。
- en: 3.4\. Integration of a New Attack
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 新攻击的集成
- en: 'To customize a new attack, users should follow these steps: i) determine the
    type of the attack, i.e., model poisoning, data poisoning, or data reconstruction;
    ii) create a new class for the attack and implement functions using the APIs,
    e.g., , and , within the FedMLAttacker class to ensure that the injected attacks
    are activated at the proper stages of FL training.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要自定义新攻击，用户应遵循以下步骤：i) 确定攻击类型，即模型中毒、数据中毒或数据重构；ii) 为攻击创建一个新类并使用 API 实现函数，例如， 和
    ，在 FedMLAttacker 类中，以确保注入的攻击在 FL 训练的适当阶段被激活。
- en: 4\. Implementation of Defenses
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 防御的实现
- en: FedMLDefender injects defense functions at different stages of FL aggregation
    at the server. Based on the point of injection, FedMLDefender provides three types
    of functions to support defense mechanisms, including 1) before-aggregation, 2)
    on-aggregation, and 3) after-aggregation. Note that a defense may inject functions
    at one or multiple stages of FL aggregation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: FedMLDefender 在服务器的 FL 聚合的不同阶段注入防御功能。根据注入点，FedMLDefender 提供三种类型的功能来支持防御机制，包括
    1) 聚合前，2) 聚合时，和 3) 聚合后。请注意，防御功能可能在 FL 聚合的一个或多个阶段注入。
- en: 4.1\. Before-Aggregation Defenses
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 聚合前防御
- en: Before-aggregation functions operate on local models at the FL server before
    aggregating the local models at the server. We use Krum (Blanchard et al., [2017](#bib.bib10))
    as an example.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合前功能在 FL 服务器上操作本地模型，然后在服务器上聚合这些本地模型。我们以 Krum (Blanchard et al., [2017](#bib.bib10))
    为例。
- en: Krum. Krum (Blanchard et al., [2017](#bib.bib10)) tolerates  clients by retaining
    only one local model that is the most likely to be benign as the global model.
    That is, Krum selects a single model as the global model in aggregation. A generalization
    of Krum is  client models with the  clients to be malicious.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Krum。Krum (Blanchard et al., [2017](#bib.bib10)) 通过仅保留一个最可能是良性的本地模型作为全局模型来容忍客户端。也就是说，Krum
    在聚合中选择一个单一模型作为全局模型。Krum 的一种推广是与客户端恶意模型的客户端模型。
- en: Table 3\. Models and datasets for evaluations.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 评估模型和数据集。
- en: '| Model | ResNet20 (He et al., [2016](#bib.bib39)) | ResNet56 (He et al., [2016](#bib.bib39))
    | CNN (McMahan et al., [2017a](#bib.bib60)) | RNN (bi-LSTM) (McMahan et al., [2017a](#bib.bib60))
    | BERT (Devlin et al., [2018](#bib.bib20)) | Pythia-1B (Biderman et al., [2023](#bib.bib9))
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ResNet20 (He et al., [2016](#bib.bib39)) | ResNet56 (He et al., [2016](#bib.bib39))
    | CNN (McMahan et al., [2017a](#bib.bib60)) | RNN (bi-LSTM) (McMahan et al., [2017a](#bib.bib60))
    | BERT (Devlin et al., [2018](#bib.bib20)) | Pythia-1B (Biderman et al., [2023](#bib.bib9))
    |'
- en: '| Dataset | CIFAR10 (Krizhevsky et al., [2009](#bib.bib44)) | CIFAR100 (Krizhevsky
    et al., [2009](#bib.bib44)) | FEMNIST (Caldas et al., [2018](#bib.bib13)) | Shakespeare (McMahan
    et al., [2017b](#bib.bib61)) | 20News (Lang, [1995](#bib.bib48)) | PubMedQA (Luo
    et al., [2022](#bib.bib57)) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CIFAR10 (Krizhevsky 等, [2009](#bib.bib44)) | CIFAR100 (Krizhevsky 等,
    [2009](#bib.bib44)) | FEMNIST (Caldas 等, [2018](#bib.bib13)) | Shakespeare (McMahan
    等, [2017b](#bib.bib61)) | 20News (Lang, [1995](#bib.bib48)) | PubMedQA (Luo 等,
    [2022](#bib.bib57)) |'
- en: 'APIs for before-aggregation functions. We provide two APIs for before-aggregation
    functions:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合前函数的 API。我们提供了两个用于聚合前函数的 API：
- en: •
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: can be any information that is utilized in the defense functions.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可以是任何在防御函数中使用的信息。
- en: •
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{defense}\_\mathit{before}\_\mathit{aggregation}()$, which
    checks whether the FedMLDefender is activated and whether the defense requires
    injecting functions before aggregating local models at the server.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{defense}\_\mathit{before}\_\mathit{aggregation}()$，用于检查
    FedMLDefender 是否被激活以及防御是否需要在服务器聚合本地模型之前注入函数。
- en: 4.2\. On-Aggregation Defenses
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 聚合阶段防御
- en: On-aggregation defense functions modify the aggregation function to a robust
    version that tolerates or mitigates impacts of the potential adversarial client
    models. As an example, RFA (Robust Federated Aggregation) (Pillutla et al., [2022](#bib.bib66))
    computes a geometric median of the client models in each iteration as the aggregated
    model, instead of simply averaging the client models. RFA defense effectively
    mitigates the impact of poisoned client models, as the geometric median can represent
    the central tendency of the client models, and the median point is chosen in a
    way to minimize the sum of distances between that point and the other client models
    of the current FL iteration. In practice, the geometric median is calculated using
    the Smoothed Weiszfeld Algorithm (Pillutla et al., [2022](#bib.bib66)).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合阶段防御函数会将聚合函数修改为一种鲁棒版本，以容忍或减轻潜在对抗性客户端模型的影响。例如，RFA（鲁棒联邦聚合） (Pillutla 等, [2022](#bib.bib66))
    在每次迭代中计算客户端模型的几何中位数作为聚合模型，而不是简单地对客户端模型进行平均。RFA 防御有效地减轻了被污染客户端模型的影响，因为几何中位数可以表示客户端模型的中心趋势，中位数点的选择方式是最小化该点与当前
    FL 迭代中其他客户端模型之间的距离之和。在实践中，几何中位数使用平滑 Weiszfeld 算法进行计算 (Pillutla 等, [2022](#bib.bib66))。
- en: 'APIs for on-aggregation defenses. We provide two APIs for on-aggregation defense
    functions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合阶段防御的 API。我们提供了两个用于聚合阶段防御函数的 API：
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{defend}\_\mathit{on}\_\mathit{aggregation}(\mathit{local}\_\mathit{models},\mathit{auxiliary}\_\mathit{info})$,
    which takes the local models of the current training round for aggregation. The
    input local_models is a list of tuples that contain the number of samples and
    the local model submitted by each client in the current FL iteration. The input
    auxiliary_info can include any information required by the defense functions.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{defend}\_\mathit{on}\_\mathit{aggregation}(\mathit{local}\_\mathit{models},\mathit{auxiliary}\_\mathit{info})$，用于聚合当前训练轮次的本地模型。输入的
    local_models 是一个包含样本数量和每个客户端在当前 FL 迭代中提交的本地模型的元组列表。输入的 auxiliary_info 可以包括防御函数所需的任何信息。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{defense}\_\mathit{on}\_\mathit{aggregation}()$, which
    checks if the defense component is enabled and whether the current defense requires
    the injection of functions during aggregation.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{defense}\_\mathit{on}\_\mathit{aggregation}()$，用于检查防御组件是否已启用以及当前防御是否需要在聚合过程中注入函数。
- en: 4.3\. After-Aggregation Defense
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 聚合后防御
- en: After-aggregation defense functions modify the aggregation result, i.e., the
    global model, of each FL iteration to mitigate the effects of poisoned local models
    or protect the global model from potential adversaries. As an example, CRFL (Xie
    et al., [2021](#bib.bib88)) clips the global model to bound the norm of the model
    each time after aggregation at the FL server. The FL server then adds Gaussian
    noise to the clipped global model before distributing the global model to the
    clients for the next FL iteration.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合后防御函数会修改每次 FL 迭代的聚合结果，即全局模型，以减轻被污染的本地模型的影响或保护全局模型免受潜在对手的攻击。例如，CRFL (Xie 等,
    [2021](#bib.bib88)) 在每次 FL 服务器聚合后对全局模型进行裁剪，以限制模型的范数。然后，FL 服务器在将全局模型分发给客户端进行下一次
    FL 迭代之前，会向裁剪后的全局模型添加高斯噪声。
- en: 'APIs for After-Aggregation Defenses. We provide two APIs to support after-aggregation
    defenses:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合后防御的 API。我们提供了两个支持聚合后防御的 API：
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{defend}\_\mathit{after}\_\mathit{aggregation}(\mathit{global}\_\mathit{model})$,
    which directly modifies the global model after aggregation using methods such
    as clipping or adding noise.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{defend}\_\mathit{after}\_\mathit{aggregation}(\mathit{global}\_\mathit{model})$，它在聚合后直接修改全局模型，使用如剪辑或添加噪声等方法。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$, which
    checks if the defense component is activated and whether the current defense requires
    injecting functions after aggregation.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$，它检查防御组件是否激活，以及当前防御是否需要在聚合后注入函数。
- en: 4.4\. Integration of a New Defense
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 新防御的集成
- en: To implement a self-designed defense mechanism, users should first determine
    the stages to inject the defense functions (i.e., before/on/after-aggregation),
    add a class for the new defense and implement the corresponding defense functions
    using the aforementioned APIs, i.e., , and  for the defense class, and include
    the name of the defense in $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现自定义的防御机制，用户首先应确定注入防御函数的阶段（即，聚合前/聚合时/聚合后），添加一个新防御的类，并使用上述 API 实现相应的防御函数，即，和
    供防御类使用，并在 $\mathit{is}\_\mathit{defense}\_\mathit{after}\_\mathit{aggregation}()$
    中包含防御的名称。
- en: 5\. Evaluations
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 评估
- en: This section presents a comprehensive evaluation of FedMLSecurity to benchmark
    some well-known attacks and defenses in FL.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对 FedMLSecurity 的全面评估，以基准一些在 FL 中广为人知的攻击和防御。
- en: 'Experimental setting. A summary of datasets and models for evaluations can
    be found in Table [3](#S4.T3 "Table 3 ‣ 4.1\. Before-Aggregation Defenses ‣ 4\.
    Implementation of Defenses ‣ FedMLSecurity: A Benchmark for Attacks and Defenses
    in Federated Learning and Federated LLMs"). We utilize FedAVG in our experiments.
    By default, we employ ResNet20 and the non-i.i.d. CIFAR10 dataset (partition parameter
    $\alpha=0.5$), as the non-i.i.d. setting closely captures real-world scenarios.
    We further extend our evaluations to i.i.d. cases and various other models and
    datasets. For evaluations on LLMs, we utilize FedLLM (FedML Inc., [2023](#bib.bib24))
    that trains LLMs in a federated manner. We employ the Pythia-1B model (Biderman
    et al., [2023](#bib.bib9)) and PubMedQA (Jin et al., [2019](#bib.bib41)), a non-i.i.d.
    biomedical research dataset that contains 212,269 questions for question answering.
    We utilize the “artificial” subset for training and the “labelled” subset for
    testing. Evaluations are conducted on a server with 8 NVIDIA A100-SXM4-80GB GPUs.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置。数据集和模型的评估总结可以在表格 [3](#S4.T3 "Table 3 ‣ 4.1\. Before-Aggregation Defenses
    ‣ 4\. Implementation of Defenses ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs") 中找到。我们在实验中使用了 FedAVG。默认情况下，我们使用
    ResNet20 和非独立同分布的 CIFAR10 数据集（分区参数 $\alpha=0.5$），因为非独立同分布的设置能更好地反映真实世界场景。我们进一步扩展了对独立同分布情况以及各种其他模型和数据集的评估。对于
    LLMs 的评估，我们使用 FedLLM（FedML Inc., [2023](#bib.bib24)），这是一个以联邦方式训练 LLMs 的工具。我们使用了
    Pythia-1B 模型（Biderman et al., [2023](#bib.bib9)）和 PubMedQA（Jin et al., [2019](#bib.bib41)），这是一个包含
    212,269 个问题用于问答的非独立同分布生物医学研究数据集。我们利用“人工”子集进行训练，用“标记”子集进行测试。评估在配备有 8 张 NVIDIA A100-SXM4-80GB
    GPU 的服务器上进行。'
- en: 'By default, we use 10 clients for FL training, corresponding to real-world
    FL applications where the number of clients is typically less than 10, especially
    in ToB scenarios. We also increase the number of clients to 100 in Exp 5, and
    set the number of clients to 70 in the real-world experiment, where we utilize
    edge devices from the Theta network (Theta Network., [2023](#bib.bib80)) to showcase
    the scalability of our library (Exp 10). Unless otherwise noted, we set the percentage
    of malicious clients to 10%, and evaluate results with the accuracy of the global
    model. We employ three attack mechanisms, including label flipping (Tolpegin et al.,
    [2020](#bib.bib81)) and Byzantine attacks of random mode and flipping mode (Chen
    et al., [2017](#bib.bib17); Fang et al., [2020](#bib.bib23); Xu et al., [2022](#bib.bib91)).
    For the label flipping attack, we set the attack to modify the local and test
    data labels of malicious clients from label 3 to label 9 and label 2 to label
    1\. We utilize three defense mechanisms: -Krum, we set $m$ to 5, which means 5
    out of 10 submitted local models participate in aggregation in each training round.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，我们使用 10 个客户端进行联邦学习训练，这与实际应用中客户端数量通常少于 10 的情况相对应，特别是在 ToB 场景中。我们还在实验 5
    中将客户端数量增加到 100，并在实际实验中将客户端数量设为 70，其中我们利用 Theta 网络（Theta Network., [2023](#bib.bib80)）的边缘设备来展示我们库的扩展性（实验
    10）。除非另有说明，否则我们将恶意客户端的百分比设置为 10%，并通过全球模型的准确性评估结果。我们使用三种攻击机制，包括标签翻转（Tolpegin et
    al., [2020](#bib.bib81)）和随机模式及翻转模式的拜占庭攻击（Chen et al., [2017](#bib.bib17); Fang
    et al., [2020](#bib.bib23); Xu et al., [2022](#bib.bib91)）。对于标签翻转攻击，我们将攻击设置为将恶意客户端的本地和测试数据标签从标签
    3 修改为标签 9，标签 2 修改为标签 1。我们使用三种防御机制：-Krum，我们将 $m$ 设置为 5，这意味着每轮训练中 10 个提交的本地模型中有
    5 个参与聚合。
- en: 5.1\. Evaluations on FL
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 联邦学习的评估
- en: '![Refer to caption](img/ac9981c610f587bd58a43f8328c42965.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac9981c610f587bd58a43f8328c42965.png)'
- en: Figure 3. Attack comparison.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 攻击比较。
- en: '![Refer to caption](img/321919c09fef3dc557e720e65a9b4024.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/321919c09fef3dc557e720e65a9b4024.png)'
- en: Figure 4. Defense comparison.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. 防御比较。
- en: '![Refer to caption](img/89a76efc54f98f34278df95672ddb8d0.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89a76efc54f98f34278df95672ddb8d0.png)'
- en: Figure 5. Label flipping exps.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 标签翻转实验。
- en: '![Refer to caption](img/624c93697a867d9e03ac540a0fb648de.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/624c93697a867d9e03ac540a0fb648de.png)'
- en: Figure 6. Random-Byzantine exps.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6. 随机-拜占庭实验。
- en: '![Refer to caption](img/bdb468ed91ae725166aaca884421d1f1.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bdb468ed91ae725166aaca884421d1f1.png)'
- en: Figure 7. I.I.D. data evaluations.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7. 独立同分布数据评估。
- en: '![Refer to caption](img/ddee8af530f572bb9acd9814eacb2392.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ddee8af530f572bb9acd9814eacb2392.png)'
- en: 'Figure 8. Scale # clients to 100.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. 客户端数量扩展到 100。
- en: '![Refer to caption](img/b1806b0688e8402aa7862995b09a43d4.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b1806b0688e8402aa7862995b09a43d4.png)'
- en: Figure 9. ResNet56 (CV).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. ResNet56（计算机视觉）。
- en: '![Refer to caption](img/52f21b47ecc8ce62dc8e06cc4627ee15.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52f21b47ecc8ce62dc8e06cc4627ee15.png)'
- en: Figure 10. RNN (NLP).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10. RNN（自然语言处理）。
- en: '![Refer to caption](img/536c076ce730b3727f320e0df6989da5.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/536c076ce730b3727f320e0df6989da5.png)'
- en: Figure 11. CNN (CV).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. CNN（计算机视觉）。
- en: '![Refer to caption](img/5fd131d62de9a1b308d25e6b30003285.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5fd131d62de9a1b308d25e6b30003285.png)'
- en: 'Figure 12. Varying # adversaries.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12. 不同对手数量。
- en: 'Exp 1: Attack Comparisons. We evaluate the impact of attacks on test accuracy,
    using a no-attack scenario as a baseline. As illustrated in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), Byzantine
    attacks, specifically in the random and zero modes, substantially degrade accuracy.
    In contrast, the label flipping attack and the flipping mode of the Byzantine
    attack show a milder impact on accuracy. This can be attributed to the nature
    of Byzantine attacks, where Byzantine attackers would prevent the global model
    from converging, especially for the random mode that generates weights for models
    arbitrarily, causing the most significant deviation from the benign local model.
    In subsequent experiments, unless specified otherwise, we employ the Byzantine
    attack in the random mode as the default attack, as it provides the strongest
    impact compared with the other three attacks.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '实验 1: 攻击比较。我们评估攻击对测试准确度的影响，使用无攻击情景作为基线。如图 [6](#S5.F6 "图 6 ‣ 5.1\. 联邦学习评估 ‣
    5\. 评估 ‣ FedMLSecurity: 联邦学习和联邦 LLMs 中攻击与防御的基准") 所示，比赞丁攻击，特别是在随机模式和零模式下，显著降低了准确度。相比之下，标签翻转攻击和比赞丁攻击的翻转模式对准确度的影响较轻。这可以归因于比赞丁攻击的性质，其中比赞丁攻击者会阻止全局模型收敛，尤其是对于随机模式，它会为模型任意生成权重，造成与良性本地模型的最大偏差。在随后的实验中，除非另有说明，我们将比赞丁攻击的随机模式作为默认攻击，因为与其他三种攻击相比，它提供了最强的影响。'
- en: 'Exp 2: Defense Comparisons. We investigate potential impact of defense mechanisms
    on accuracy in the absence of attacks, i.e., whether defense mechanisms inadvertently
    degrade accuracy when all clients are benign. We incorporate a scenario without
    any defense or attack as our baseline. As illustrated in Figure [6](#S5.F6 "Figure
    6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs"), it becomes evident
    that when all clients are benign, involving defense strategies to FL training
    might lead to a reduction in accuracy. This decrease might arise from several
    factors: the exclusion of some benign local models from aggregation, e.g., as
    in $m$-Krum, adjustments to the aggregation function, e.g., as in RFA, or re-weighting
    local models, e.g., as in Foolsgold. Specifically, the RFA defense mechanism significantly
    impacts accuracy as it computes a geometric median of the local models instead
    of leveraging the original FedAVG optimizer, which introduces a degradation in
    accuracy.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '实验 2: 防御比较。我们调查在没有攻击的情况下，防御机制对准确度的潜在影响，即防御机制是否会在所有客户端都良性时不经意间降低准确度。我们将没有任何防御或攻击的情景作为基线。如图
    [6](#S5.F6 "图 6 ‣ 5.1\. 联邦学习评估 ‣ 5\. 评估 ‣ FedMLSecurity: 联邦学习和联邦 LLMs 中攻击与防御的基准")
    所示，当所有客户端都良性时，涉及防御策略的联邦学习训练可能会导致准确度下降。这一下降可能由几个因素造成：排除一些良性本地模型的聚合，例如在 $m$-Krum
    中，对聚合函数的调整，例如在 RFA 中，或重新加权本地模型，例如在 Foolsgold 中。具体而言，RFA 防御机制显著影响准确度，因为它计算本地模型的几何中位数，而不是利用原始的
    FedAVG 优化器，这引入了准确度的下降。'
- en: '![Refer to caption](img/eb21e9fe9e7a5a9e4f4ba9605e9295f8.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb21e9fe9e7a5a9e4f4ba9605e9295f8.png)'
- en: Figure 13. BERT evaluations.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13. BERT 评估。
- en: '![Refer to caption](img/d5abaaa5b4079a52b3f6e234558e0622.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5abaaa5b4079a52b3f6e234558e0622.png)'
- en: Figure 14. Pythia-1B evaluations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14. Pythia-1B 评估。
- en: '![Refer to caption](img/5489192cc28dd129564cbb26601b03d6.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5489192cc28dd129564cbb26601b03d6.png)'
- en: Figure 15. Real-world application evaluation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15. 真实世界应用评估。
- en: 'Exp 3: Evaluations of defense mechanisms against activated attacks. This experiment
    evaluates the effect of defense mechanisms against some attacks. We include two
    baseline scenarios: 1) an “original attack” scenario with an activated attack
    without any defense in place, and 2) a “benign” scenario with no activated attack
    or defense. We select label flipping attack and the random mode of Byzantine attack
    based on their impacts in Exp1, where label flipping has the least impact and
    the random mode of Byzantine attack exhibits the largest impact, as shown in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"). Results for
    the label flipping and the random mode of Byzantine attacks are in Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") and Figure [6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), respectively.
    These results indicate that the defenses may contribute to minor improvements
    in accuracy for low-impact attacks, e.g., Foolsgold in Figure [6](#S5.F6 "Figure
    6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs"). In certain cases,
    it is noteworthy that the defensive mechanisms may inadvertently compromise accuracy,
    such as the case with RFA in Figure [6](#S5.F6 "Figure 6 ‣ 5.1\. Evaluations on
    FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in
    Federated Learning and Federated LLMs"). For high-impact attacks, such as the
    Byzantine attack of the random mode, Krum exhibits resilience, effectively neutralizing
    the negative impact of the attacks, as shown in Figure [6](#S5.F6 "Figure 6 ‣
    5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks
    and Defenses in Federated Learning and Federated LLMs").'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 'Exp 3：针对激活攻击的防御机制评估。该实验评估了防御机制对某些攻击的效果。我们包括了两个基线场景：1）一个“原始攻击”场景，即没有任何防御措施的激活攻击；2）一个“良性”场景，即没有激活攻击或防御措施。我们根据在Exp1中的影响选择了标签翻转攻击和拜占庭攻击的随机模式，其中标签翻转的影响最小，而拜占庭攻击的随机模式显示了最大的影响，如图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")所示。标签翻转和拜占庭攻击的随机模式的结果见图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")和图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")。这些结果表明，对于低影响攻击，防御机制可能会略微提高准确性，例如图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")中的Foolsgold。在某些情况下，需要注意的是，防御机制可能会无意中影响准确性，例如图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")中的RFA。对于高影响攻击，如拜占庭攻击的随机模式，Krum显示出弹性，有效中和了攻击的负面影响，如图[6](#S5.F6
    "Figure 6 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")所示。'
- en: 'Exp 4: Evaluations on i.i.d. data. We select the random mode of the Byzantine
    attack, and employ Foolsgold, ), and RFA to counteract the adverse effects of
    this attack. As shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on
    FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in
    Federated Learning and Federated LLMs"), $m$-Krum is the most effective one among
    all the defense mechanisms, where the test accuracy is close to the case where
    all the FL clients are honest, i.e., no attack scenario.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 'Exp 4：对i.i.d.数据的评估。我们选择了拜占庭攻击的随机模式，并采用Foolsgold、）和RFA来对抗该攻击的不利影响。如图[10](#S5.F10
    "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")所示，$m$-Krum是所有防御机制中最有效的，其测试准确性接近于所有FL客户端诚实的情况，即没有攻击场景。'
- en: 'Exp 5: Scaling the number of clients to 100. This experiment scales the number
    of clients to 100 and evaluates the defense mechanisms against the random mode
    of the Byzantine attack. We employ Foolsgold, ), and RFA to counteract the adverse
    effects of this attack. As shown in Figure [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations
    on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses
    in Federated Learning and Federated LLMs"), $m$-Krum is the most effective one
    among all the defense mechanisms, and the test accuracy is very close to the case
    where no attack happens.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '实验 5：将客户端数量扩展到 100。此实验将客户端数量扩展到 100，并评估了对抗拜占庭攻击随机模式的防御机制。我们使用 Foolsgold、），和
    RFA 来抵消这种攻击的不利影响。如图[10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs")所示，$m$-Krum 是所有防御机制中最有效的，且测试准确性非常接近没有攻击的情况。'
- en: 'Exp 6: Evaluations on different models. We evaluate defense mechanisms against
    the random mode of the Byzantine attack with different models and datasets, including:
    i) ResNet56 + CIFAR100, ii) RNN + Shakespeare, and iii) CNN + FEMNIST. The results
    are shown in Figures [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"), [10](#S5.F10 "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"), and [12](#S5.F12 "Figure 12 ‣ 5.1\. Evaluations on FL ‣ 5\.
    Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated
    Learning and Federated LLMs"), respectively. The results show that while the defense
    mechanisms can mitigate the impact of attacks in most cases, some attacks may
    fail some tasks, e.g., $m$-Krum fails RNN in Figure [10](#S5.F10 "Figure 10 ‣
    5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks
    and Defenses in Federated Learning and Federated LLMs"), and Foolsgold fails CNN
    in Figure [12](#S5.F12 "Figure 12 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs"). This is because the two defense mechanisms either select several
    local models for aggregation in each FL training round, or significantly re-weight
    the local models, which may eliminate some local models that are important to
    the aggregation in the first several FL training iterations, leading to unchanged
    test accuracy in later FL iterations.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '实验 6：对不同模型的评估。我们使用不同模型和数据集评估对抗拜占庭攻击随机模式的防御机制，包括：i) ResNet56 + CIFAR100，ii)
    RNN + Shakespeare，和 iii) CNN + FEMNIST。结果分别显示在图[10](#S5.F10 "Figure 10 ‣ 5.1\.
    Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs")、[10](#S5.F10 "Figure 10 ‣
    5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks
    and Defenses in Federated Learning and Federated LLMs") 和 [12](#S5.F12 "Figure
    12 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs")中。结果表明，虽然防御机制可以在大多数情况下减轻攻击的影响，但一些攻击可能会使某些任务失败，例如图[10](#S5.F10
    "Figure 10 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")中的 $m$-Krum
    在 RNN 上失败，以及图[12](#S5.F12 "Figure 12 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs")中的 Foolsgold 在 CNN 上失败。这是因为这两种防御机制要么在每次 FL 训练轮次中选择多个本地模型进行聚合，要么显著重新加权本地模型，这可能会排除一些在前几次
    FL 训练迭代中对聚合重要的本地模型，导致后续 FL 迭代中的测试准确性不变。'
- en: 'Exp 7: Varying the number of malicious clients. This experiment evaluates the
    impact of varying numbers of malicious clients on test accuracy. We utilize -Krum
    selects a local model that is the most likely to be benign to represent the other
    models, effectively minimizing the impact of malicious client models on the aggregation.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 实验 7：变化恶意客户端的数量。此实验评估了恶意客户端数量变化对测试准确性的影响。我们利用-Krum 选择一个最有可能是良性的本地模型来代表其他模型，从而有效地最小化恶意客户端模型对聚合的影响。
- en: 5.2\. Evaluations on Federated LLMs
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 联邦 LLMs 的评估
- en: We employ two LLMs, BERT (Devlin et al., [2018](#bib.bib20)) and Pythia (Biderman
    et al., [2023](#bib.bib9)), to showcase the scalability of FedMLSecurity and its
    applicability to federated LLM scenarios. We notice that some defenses (e.g.,
    Foolsgold (Fung et al., [2020](#bib.bib28))) that require memorizing intermediate
    results, such as models of previous FL training rounds, might encounter limitations
    when integrated with LLMs due to the significant cache introduced. Considering
    this, we utilize $m$-Krum for our experiments, as it does not require storing
    intermediate results and demonstrates consistent performance in most of our previous
    experiments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了两个LLM，BERT (Devlin et al., [2018](#bib.bib20)) 和 Pythia (Biderman et al.,
    [2023](#bib.bib9))，来展示FedMLSecurity的可扩展性及其在联邦LLM场景中的适用性。我们注意到一些防御方法（例如，Foolsgold
    (Fung et al., [2020](#bib.bib28))）需要记住中间结果，例如之前FL训练轮次的模型，可能会在与LLMs集成时遇到限制，因为引入了大量缓存。考虑到这一点，我们在实验中使用了$m$-Krum，因为它不需要存储中间结果，并且在我们之前的大多数实验中表现一致。
- en: 'Exp 8: Evaluations of Krum against model replacement backdoor attack on BERT.
    This experiment utilizes BERT (Devlin et al., [2018](#bib.bib20)) and the 20 news
    dataset (Lang, [1995](#bib.bib48)) for a classification task. We employ 10 clients
    and set 1 client to be malicious in each FL training round. We set -Krum, i.e.,
    5 out of 10 local models participate in aggregation in each FL training round.
    Results in Figure [15](#S5.F15 "Figure 15 ‣ 5.1\. Evaluations on FL ‣ 5\. Evaluations
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") show that $m$-Krum effectively mitigates the adversarial effect,
    bringing the accuracy closer to the level of the attack-free case.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 'Exp 8: 对Krum进行BERT模型替换后门攻击的评估。该实验使用BERT (Devlin et al., [2018](#bib.bib20))
    和 20 news数据集 (Lang, [1995](#bib.bib48))进行分类任务。我们使用了10个客户端，并在每次FL训练轮次中设定1个客户端为恶意的。我们设置了-Krum，即每次FL训练轮次中5个本地模型参与聚合。结果如图[15](#S5.F15
    "Figure 15 ‣ 5.1. Evaluations on FL ‣ 5. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")所示，$m$-Krum有效地缓解了对抗效果，使准确率接近无攻击情况下的水平。'
- en: 'Exp 9: Evaluations of Krum against the Byzantine attack on Pythia-1B. We employ
    7 clients for FL training, and 1 out of 7 clients is malicious in each round of
    FL training. We set the -Krum to 2, signifying that 2 out of 7 submitted local
    models participate in the aggregation in each FL training round. The performance
    is evaluated with the test loss. Results in Figure [15](#S5.F15 "Figure 15 ‣ 5.1\.
    Evaluations on FL ‣ 5\. Evaluations ‣ FedMLSecurity: A Benchmark for Attacks and
    Defenses in Federated Learning and Federated LLMs") show that Byzantine attack
    significantly increases the test loss during training. Nevertheless, $m$-Krum
    effectively mitigates the adversarial effect.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 'Exp 9: 对Krum进行Pythia-1B上的拜占庭攻击的评估。我们使用了7个客户端进行FL训练，每轮FL训练中1个客户端为恶意的。我们将-Krum设置为2，表示每轮FL训练中2个本地模型参与聚合。通过测试损失评估性能。结果如图[15](#S5.F15
    "Figure 15 ‣ 5.1. Evaluations on FL ‣ 5. Evaluations ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs")所示，拜占庭攻击显著增加了训练过程中的测试损失。然而，$m$-Krum有效地缓解了对抗效果。'
- en: 5.3\. Evaluation in Real-World Applications
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. 现实世界应用中的评估
- en: To demonstrate the scalability of our benchmark, we include an experiment using
    real-world devices, instead of simulations.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们基准测试的可扩展性，我们包含了一个使用真实世界设备的实验，而不是模拟。
- en: 'Exp10: Evaluations in real-world applications. We utilize edge devices from
    the Theta network (Theta Network., [2023](#bib.bib80)) to validate the scalability
    of FedMLSecurity to real-world applications. The FL client package is integrated
    into Theta’s edge nodes, which periodically fetches data from the Theta back-end.
    Subsequently, the FL training platform capitalizes on these Theta edge nodes and
    their associated data to train, fine-tune, and deploy machine learning models.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'Exp10: 现实世界应用中的评估。我们利用Theta网络（Theta Network., [2023](#bib.bib80)）中的边缘设备来验证FedMLSecurity在现实世界应用中的可扩展性。FL客户端包被集成到Theta的边缘节点中，这些节点会定期从Theta后端获取数据。随后，FL训练平台利用这些Theta边缘节点及其关联数据来训练、微调和部署机器学习模型。'
- en: 'We select -Krum, we set -Krum mitigates the adversarial effect of the random-mode
    Byzantine attack. We also include a screenshot of the platform, as shown in Figure [16](#A1.F16
    "Figure 16 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") in Appendix
    §[A](#A1 "Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark for
    Attacks and Defenses in Federated Learning and Federated LLMs") for the FL training
    process and Figure [17](#A1.F17 "Figure 17 ‣ Appendix A Supplementary Experiment
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") in Appendix §[A](#A1 "Appendix A Supplementary Experiment ‣ FedMLSecurity:
    A Benchmark for Attacks and Defenses in Federated Learning and Federated LLMs")
    for the training status of each device.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '我们选择-Krum，并设置-Krum以减轻随机模式拜占庭攻击的对抗效果。我们还包含了平台的截图，如附录§[A](#A1 "附录 A 补充实验 ‣ FedMLSecurity:
    联邦学习和联邦 LLMs 中攻击与防御的基准")中图[16](#A1.F16 "图 16 ‣ 附录 A 补充实验 ‣ FedMLSecurity: 联邦学习和联邦
    LLMs 中攻击与防御的基准")所示，展示了FL训练过程，以及附录§[A](#A1 "附录 A 补充实验 ‣ FedMLSecurity: 联邦学习和联邦
    LLMs 中攻击与防御的基准")中图[17](#A1.F17 "图 17 ‣ 附录 A 补充实验 ‣ FedMLSecurity: 联邦学习和联邦 LLMs
    中攻击与防御的基准")展示了每个设备的训练状态。'
- en: 6\. Conclusion
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: 'This paper presents FedMLSecurity, a library designed to demonstrate potential
    adversarial attacks and corresponding defense strategies in FL to bolster innovation
    in the secure FL domain. FedMLSecurity contains two components: FedMLAttacker
    that simulates various attacks that can be injected during FL training, and FedMLDefender,
    which facilitates defense strategies to mitigate the impacts of these attacks.
    FedMLSecurity is open-sourced, and we welcome contributions from the research
    community to enrich the benchmark repository with novel attack and defense strategies
    to foster a diverse, comprehensive, and robust foundation for ongoing research
    in FL security.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了FedMLSecurity，这是一个旨在展示FL中潜在对抗攻击和相应防御策略的库，以促进安全FL领域的创新。FedMLSecurity包含两个组件：FedMLAttacker用于模拟在FL训练期间可能注入的各种攻击，FedMLDefender则提供缓解这些攻击影响的防御策略。FedMLSecurity是开源的，我们欢迎研究社区的贡献，以丰富基准库中的新颖攻击和防御策略，为FL安全领域的持续研究奠定一个多样、全面和稳健的基础。
- en: References
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abadi et al. (2015) Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,
    Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu
    Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael
    Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale
    Machine Learning on Heterogeneous Systems. [https://www.tensorflow.org/](https://www.tensorflow.org/)
    Software available from tensorflow.org.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi等人（2015） Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng
    Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
    Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
    Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg,
    Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster,
    Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
    Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin
    Wattenberg, Martin Wicke, Yuan Yu, 和 Xiaoqiang Zheng. 2015. TensorFlow: 大规模异构系统上的机器学习。
    [https://www.tensorflow.org/](https://www.tensorflow.org/) 软件可从tensorflow.org获取。'
- en: Antiga (2023) Luca Antiga. 2023. Introducing PyTorch Lightning 2.0 and Fabric.
    *https://lightning.ai/blog/introducing-lightning-2-0/* (2023).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Antiga（2023） Luca Antiga. 2023. 介绍 PyTorch Lightning 2.0 和 Fabric。 *https://lightning.ai/blog/introducing-lightning-2-0/*
    (2023)。
- en: Bagdasaryan et al. (2020) Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah
    Estrin, and Vitaly Shmatikov. 2020. How to backdoor federated learning. In *International
    Conference on Artificial Intelligence and Statistics*. PMLR, 2938–2948.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagdasaryan等人（2020） Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin,
    和 Vitaly Shmatikov. 2020. 如何在联邦学习中植入后门。 在 *国际人工智能与统计会议* 中。PMLR, 2938–2948。
- en: 'Baruch et al. (2019) Gilad Baruch, Moran Baruch, and Yoav Goldberg. 2019. A
    little is enough: Circumventing defenses for distributed learning. *Advances in
    Neural Information Processing Systems* 32 (2019).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baruch等人（2019） Gilad Baruch, Moran Baruch, 和 Yoav Goldberg. 2019. 一点就够：绕过分布式学习的防御。
    *神经信息处理系统进展* 32 (2019)。
- en: 'Beimel (2011) Amos Beimel. 2011. Secret-sharing schemes: A survey. In *International
    conference on coding and cryptology*. Springer, 11–46.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beimel (2011) Amos Beimel. 2011. 《秘密共享方案：综述》。在 *国际编码与密码学大会* 上，Springer, 11–46。
- en: 'Beutel et al. (2020) Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu,
    Titouan Parcollet, Pedro PB de Gusmão, and Nicholas D Lane. 2020. Flower: A friendly
    federated learning research framework. *arXiv preprint arXiv:2007.14390* (2020).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beutel 等 (2020) Daniel J Beutel, Taner Topal, Akhil Mathur, Xinchi Qiu, Titouan
    Parcollet, Pedro PB de Gusmão, 和 Nicholas D Lane. 2020. 《Flower：一个友好的联邦学习研究框架》。*arXiv
    预印本 arXiv:2007.14390* (2020)。
- en: Bhagoji et al. (2019) Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal,
    and Seraphin Calo. 2019. Analyzing federated learning through an adversarial lens.
    In *International Conference on Machine Learning*. PMLR, 634–643.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhagoji 等 (2019) Arjun Nitin Bhagoji, Supriyo Chakraborty, Prateek Mittal, 和
    Seraphin Calo. 2019. 《通过对抗视角分析联邦学习》。在 *国际机器学习大会* 上，PMLR, 634–643。
- en: 'Biderman et al. (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony,
    Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large
    language models across training and scaling. *arXiv preprint arXiv:2304.01373*
    (2023).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biderman 等 (2023) Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie
    Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit,
    USVSN Sai Prashanth, Edward Raff 等. 2023. 《Pythia：用于分析大语言模型的套件，涵盖训练和扩展》。*arXiv
    预印本 arXiv:2304.01373* (2023)。
- en: 'Blanchard et al. (2017) Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui,
    and Julien Stainer. 2017. Machine learning with adversaries: Byzantine tolerant
    gradient descent. *Advances in neural information processing systems* 30 (2017).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blanchard 等 (2017) Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, 和 Julien
    Stainer. 2017. 《与对手的机器学习：拜占庭容错梯度下降》。*神经信息处理系统进展* 30 (2017)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 2020. 《语言模型是少样本学习者》。*神经信息处理系统进展* 33 (2020), 1877–1901。
- en: Byrd and Polychroniadou (2020) David Byrd and Antigoni Polychroniadou. 2020.
    Differentially private secure multi-party computation for federated learning in
    financial applications. In *Proceedings of the First ACM International Conference
    on AI in Finance*. 1–9.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Byrd 和 Polychroniadou (2020) David Byrd 和 Antigoni Polychroniadou. 2020. 《用于金融应用的联邦学习中的差分隐私安全多方计算》。在
    *第一次 ACM 国际金融人工智能大会* 上，1–9。
- en: 'Caldas et al. (2018) Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian
    Li, Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. 2018.
    Leaf: A benchmark for federated settings. *arXiv preprint arXiv:1812.01097* (2018).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caldas 等 (2018) Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li,
    Jakub Konečnỳ, H Brendan McMahan, Virginia Smith, 和 Ameet Talwalkar. 2018. 《Leaf：联邦设置的基准》。*arXiv
    预印本 arXiv:1812.01097* (2018)。
- en: 'Chen et al. (2023) Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, and
    Xiaolin Zheng. 2023. Federated Large Language Model : A Position Paper. *arXiv
    preprint arXiv:2307.08925* (2023).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023) Chaochao Chen, Xiaohua Feng, Jun Zhou, Jianwei Yin, 和 Xiaolin
    Zheng. 2023. 《联邦大语言模型：立场论文》。*arXiv 预印本 arXiv:2307.08925* (2023)。
- en: 'Chen et al. (2022) Jiahui Chen, Yi Zhao, Qi Li, Xuewei Feng, and Ke Xu. 2022.
    FedDef: Defense Against Gradient Leakage in Federated Learning-Based Network Intrusion
    Detection Systems. *IEEE Transactions on Information Forensics and Security* 18
    (2022), 4561–4576. [https://api.semanticscholar.org/CorpusID:253420565](https://api.semanticscholar.org/CorpusID:253420565)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2022) Jiahui Chen, Yi Zhao, Qi Li, Xuewei Feng, 和 Ke Xu. 2022. 《FedDef：针对联邦学习基础的网络入侵检测系统中的梯度泄漏防御》。*IEEE
    信息取证与安全事务* 18 (2022), 4561–4576. [https://api.semanticscholar.org/CorpusID:253420565](https://api.semanticscholar.org/CorpusID:253420565)
- en: Chen et al. (2019) Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays.
    2019. Federated learning of out-of-vocabulary words. *arXiv preprint arXiv:1903.10635*
    (2019).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2019) Mingqing Chen, Rajiv Mathews, Tom Ouyang, 和 Françoise Beaufays.
    2019. 《联邦学习中的词汇外词学习》。*arXiv 预印本 arXiv:1903.10635* (2019)。
- en: 'Chen et al. (2017) Y. Chen, L. Su, and J. Xu. 2017. Distributed statistical
    machine learning in adversarial settings: Byzantine gradient descent. *ACM on
    Measurement and Analysis of Computing Systems* 1, 2 (2017), 1–25.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2017) Y. Chen, L. Su, 和 J. Xu. 2017. 《对抗设置中的分布式统计机器学习：拜占庭梯度下降》。*ACM
    计算系统测量与分析* 1, 2 (2017), 1–25。
- en: 'Chowdhury et al. (2022) Alexander Chowdhury, Hasan Kassem, Nicolas Padoy, Renato
    Umeton, and Alexandros Karargyris. 2022. A review of medical federated learning:
    Applications in oncology and cancer research. In *Brainlesion: Glioma, Multiple
    Sclerosis, Stroke and Traumatic Brain Injuries: 7th International Workshop, BrainLes
    2021, Held in Conjunction with MICCAI 2021, Virtual Event, September 27, 2021,
    Revised Selected Papers, Part I*. Springer, 3–24.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhury 等（2022）Alexander Chowdhury, Hasan Kassem, Nicolas Padoy, Renato Umeton
    和 Alexandros Karargyris. 2022. 医疗联邦学习综述：在肿瘤学和癌症研究中的应用。见于*脑损伤：神经胶质瘤、多发性硬化症、中风和创伤性脑损伤：第七届国际研讨会，BrainLes
    2021，与 MICCAI 2021 联合举办，虚拟活动，2021 年 9 月 27 日，修订选择论文，第 I 部分*。Springer, 3–24。
- en: Dang et al. (2021) Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews,
    Peter Chin, and Françoise Beaufays. 2021. Revealing and protecting labels in distributed
    training. *Advances in Neural Information Processing Systems* 34 (2021), 1727–1738.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dang 等（2021）Trung Dang, Om Thakkar, Swaroop Ramaswamy, Rajiv Mathews, Peter
    Chin 和 Françoise Beaufays. 2021. 在分布式训练中揭示和保护标签。*神经信息处理系统进展* 34（2021），1727–1738。
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805* (2018).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    2018. Bert：用于语言理解的深度双向变换器的预训练。*arXiv 预印本 arXiv:1810.04805*（2018）。
- en: 'Dimitriadis et al. (2022) Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal
    Diaz, Andre Manoel, and Robert Sim. 2022. Flute: A scalable, extensible framework
    for high-performance federated learning simulations. *arXiv preprint arXiv:2203.13789*
    (2022).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dimitriadis 等（2022）Dimitrios Dimitriadis, Mirian Hipolito Garcia, Daniel Madrigal
    Diaz, Andre Manoel 和 Robert Sim. 2022. Flute：一个可扩展、可扩展的高性能联邦学习模拟框架。*arXiv 预印本
    arXiv:2203.13789*（2022）。
- en: 'Elkordy et al. (2023) Ahmed Roushdy Elkordy, Yahya H Ezzeldin, Shanshan Han,
    Shantanu Sharma, Chaoyang He, Sharad Mehrotra, Salman Avestimehr, et al. 2023.
    Federated analytics: A survey. *APSIPA Transactions on Signal and Information
    Processing* 12, 1 (2023).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elkordy 等（2023）Ahmed Roushdy Elkordy, Yahya H Ezzeldin, Shanshan Han, Shantanu
    Sharma, Chaoyang He, Sharad Mehrotra, Salman Avestimehr 等. 2023. 联邦分析：综述。*APSIPA
    信号与信息处理学报* 12, 1 (2023)。
- en: Fang et al. (2020) Minghong Fang, Xiaoyu Cao, Jinyuan Jia, and Neil Gong. 2020.
    Local model poisoning attacks to  federated learning. In *29th USENIX security
    symposium (USENIX Security 20)*. 1605–1622.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等（2020）Minghong Fang, Xiaoyu Cao, Jinyuan Jia 和 Neil Gong. 2020. 针对联邦学习的本地模型中毒攻击。见于*第
    29 届 USENIX 安全研讨会（USENIX Security 20）*。1605–1622。
- en: 'FedML Inc. (2023) FedML Inc. 2023. Releasing FedLLM: Build Your Own Large Language
    Models on Proprietary Data using the FedML Platform. https://blog.fedml.ai/releasing-fedllm-build-your-own-large-language-models-on-proprietary-data-using-the-fedml-platform.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FedML Inc.（2023）FedML Inc. 2023. 发布 FedLLM：使用 FedML 平台在专有数据上构建您自己的大型语言模型。https://blog.fedml.ai/releasing-fedllm-build-your-own-large-language-models-on-proprietary-data-using-the-fedml-platform。
- en: 'Fowl et al. (2021) Liam Fowl, Jonas Geiping, Wojtek Czaja, Micah Goldblum,
    and Tom Goldstein. 2021. Robbing the fed: Directly obtaining private data in federated
    learning with modified models. *arXiv preprint arXiv:2110.13057* (2021).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fowl 等（2021）Liam Fowl, Jonas Geiping, Wojtek Czaja, Micah Goldblum 和 Tom Goldstein.
    2021. 掠夺联邦：通过修改模型直接获取联邦学习中的私有数据。*arXiv 预印本 arXiv:2110.13057*（2021）。
- en: Fraboni et al. (2021) Yann Fraboni, Richard Vidal, and Marco Lorenzi. 2021.
    Free-rider attacks on model aggregation in federated learning. In *International
    Conference on Artificial Intelligence and Statistics*. PMLR, 1846–1854.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraboni 等（2021）Yann Fraboni, Richard Vidal 和 Marco Lorenzi. 2021. 针对联邦学习中模型聚合的搭便车攻击。见于*国际人工智能与统计会议*。PMLR,
    1846–1854。
- en: Fu et al. (2019) Shuhao Fu, Chulin Xie, Bo Li, and Qifeng Chen. 2019. Attack-resistant
    federated learning with residual-based reweighting. *arXiv preprint arXiv:1912.11464*
    (2019).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2019）Shuhao Fu, Chulin Xie, Bo Li 和 Qifeng Chen. 2019. 基于残差的加权抗攻击联邦学习。*arXiv
    预印本 arXiv:1912.11464*（2019）。
- en: Fung et al. (2020) Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2020.
    The Limitations of Federated Learning in Sybil Settings.. In *RAID*. 301–316.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fung 等（2020）Clement Fung, Chris JM Yoon 和 Ivan Beschastnikh. 2020. 联邦学习在 Sybil
    环境中的局限性。见于*RAID*。301–316。
- en: 'Gamma et al. (1995) Erich Gamma, Richard Helm, Ralph Johnson, Ralph E Johnson,
    and John Vlissides. 1995. *Design patterns: elements of reusable object-oriented
    software*. Pearson Deutschland GmbH.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gamma 等（1995）Erich Gamma, Richard Helm, Ralph Johnson, Ralph E Johnson 和 John
    Vlissides. 1995. *设计模式：可重用面向对象软件的元素*。Pearson Deutschland GmbH。
- en: Geiping et al. (2020) Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and
    Michael Moeller. 2020. Inverting gradients-how easy is it to break privacy in
    federated learning? *Advances in Neural Information Processing Systems* 33 (2020),
    16937–16947.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiping等人（2020） Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, 和 Michael
    Moeller. 2020. 反转梯度——在联邦学习中破坏隐私有多容易？*神经信息处理系统进展* 33（2020），16937–16947。
- en: Goodfellow et al. (2014) Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,
    Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio.
    2014. Generative Adversarial Nets. In *NIPS*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等人（2014） Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。发表于*NIPS*。
- en: Guerraoui et al. (2018) Rachid Guerraoui, Sébastien Rouault, et al. 2018. The
    hidden vulnerability of distributed learning in byzantium. In *International Conference
    on Machine Learning*. PMLR, 3521–3530.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guerraoui等人（2018） Rachid Guerraoui, Sébastien Rouault等人. 2018. 拜占庭中的分布式学习的隐性脆弱性。发表于*国际机器学习会议*。PMLR，3521–3530。
- en: Gugger (2021) Sylvain Gugger. 2021. Introducing Hugging Face Accelerate. https://huggingface.co/blog/accelerate-library.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gugger（2021） Sylvain Gugger. 2021. 介绍Hugging Face Accelerate。https://huggingface.co/blog/accelerate-library。
- en: Hard et al. (2018) Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy,
    Françoise Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel
    Ramage. 2018. Federated learning for mobile keyboard prediction. *arXiv preprint
    arXiv:1811.03604* (2018).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hard等人（2018） Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise
    Beaufays, Sean Augenstein, Hubert Eichner, Chloé Kiddon, 和 Daniel Ramage. 2018.
    用于移动键盘预测的联邦学习。*arXiv预印本 arXiv:1811.03604*（2018）。
- en: 'He et al. (2020a) Chaoyang He, Murali Annavaram, and Salman Avestimehr. 2020a.
    Group knowledge transfer: Federated learning of large cnns at the edge. *Advances
    in Neural Information Processing Systems* 33 (2020), 14068–14080.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2020a） Chaoyang He, Murali Annavaram, 和 Salman Avestimehr. 2020a. 群体知识转移：在边缘进行大规模CNN的联邦学习。*神经信息处理系统进展*
    33（2020），14068–14080。
- en: 'He et al. (2020b) Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang,
    Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al.
    2020b. FedML: A research library and benchmark for federated machine learning.
    *arXiv preprint arXiv:2007.13518* (2020).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He等人（2020b） Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi
    Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu等人. 2020b. FedML:
    一个用于联邦机器学习的研究库和基准。*arXiv预印本 arXiv:2007.13518*（2020）。'
- en: 'He et al. (2021) Chaoyang He, Erum Mushtaq, Jie Ding, and Salman Avestimehr.
    2021. Fednas: Federated deep learning via neural architecture search. (2021).'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He等人（2021） Chaoyang He, Erum Mushtaq, Jie Ding, 和 Salman Avestimehr. 2021.
    Fednas: 通过神经架构搜索进行联邦深度学习。（2021）。'
- en: He et al. (2015) Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2015. Deep
    Residual Learning for Image Recognition. *2016 IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR)* (2015), 770–778.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2015） Kaiming He, X. Zhang, Shaoqing Ren, 和 Jian Sun. 2015. 用于图像识别的深度残差学习。*2016
    IEEE计算机视觉与模式识别会议（CVPR）*（2015），770–778。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He等人（2016） Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 2016. 图像识别的深度残差学习。发表于*IEEE计算机视觉与模式识别会议论文集*。770–778。
- en: 'Hitaj et al. (2017) Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.
    2017. Deep models under the GAN: information leakage from collaborative deep learning.
    In *Proceedings of the 2017 ACM SIGSAC conference on computer and communications
    security*. 603–618.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hitaj等人（2017） Briland Hitaj, Giuseppe Ateniese, 和 Fernando Perez-Cruz. 2017.
    GAN下的深度模型：来自协作深度学习的信息泄漏。发表于*2017年ACM SIGSAC计算机与通信安全会议论文集*。603–618。
- en: 'Jin et al. (2019) Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and
    Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*. 2567–2577.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin等人（2019） Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, 和 Xinghua
    Lu. 2019. PubMedQA: 一种用于生物医学研究问题回答的数据集。发表于*2019年自然语言处理经验方法会议和第九届国际自然语言处理联合会议（EMNLP-IJCNLP）*。2567–2577。'
- en: Jung et al. (2012) Gueyoung Jung, Nathan Gnanasambandam, and Tridib Mukherjee.
    2012. Synchronous Parallel Processing of Big-Data Analytics Services to Optimize
    Performance in Federated Clouds. *2012 IEEE Fifth International Conference on
    Cloud Computing* (2012), 811–818.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung et al. (2012) Gueyoung Jung, Nathan Gnanasambandam 和 Tridib Mukherjee.
    2012. 大数据分析服务的同步并行处理以优化联邦云中的性能。*2012 IEEE第五届国际云计算会议* (2012), 811–818.
- en: Karimireddy et al. (2020) Sai Praneeth Karimireddy, Lie He, and Martin Jaggi.
    2020. Byzantine-robust learning on heterogeneous datasets via bucketing. *arXiv
    preprint arXiv:2006.09365* (2020).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karimireddy et al. (2020) Sai Praneeth Karimireddy, Lie He 和 Martin Jaggi. 2020.
    通过分桶在异质数据集上进行拜占庭鲁棒学习。*arXiv 预印本 arXiv:2006.09365* (2020).
- en: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. 2009. Learning
    multiple layers of features from tiny images. (2009).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton 等. 2009. 从小图像中学习多层特征。(2009).
- en: 'Kumar et al. (2022) Abhishek Kumar, Vivek Khimani, Dimitris Chatzopoulos, and
    Pan Hui. 2022. FedClean: A Defense Mechanism against Parameter Poisoning Attacks
    in Federated Learning. *ICASSP 2022 - 2022 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)* (2022), 4333–4337. [https://api.semanticscholar.org/CorpusID:249437417](https://api.semanticscholar.org/CorpusID:249437417)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumar et al. (2022) Abhishek Kumar, Vivek Khimani, Dimitris Chatzopoulos 和
    Pan Hui. 2022. FedClean: 一种对抗联邦学习中的参数中毒攻击的防御机制。*ICASSP 2022 - 2022 IEEE国际声学、语音与信号处理会议
    (ICASSP)* (2022), 4333–4337. [https://api.semanticscholar.org/CorpusID:249437417](https://api.semanticscholar.org/CorpusID:249437417)'
- en: 'Kumari et al. (2023) Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza
    Jadliwala, and Ahmad-Reza Sadeghi. 2023. BayBFed: Bayesian Backdoor Defense for
    Federated Learning. *arXiv preprint arXiv:2301.09508* (2023).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kumari et al. (2023) Kavita Kumari, Phillip Rieger, Hossein Fereidooni, Murtuza
    Jadliwala 和 Ahmad-Reza Sadeghi. 2023. BayBFed: 联邦学习中的贝叶斯后门防御。*arXiv 预印本 arXiv:2301.09508*
    (2023).'
- en: 'Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng
    Zhu, Harsha Madhyastha, and Mosharaf Chowdhury. 2022. FedScale: Benchmarking model
    and system performance of federated learning at scale. In *International Conference
    on Machine Learning*. PMLR, 11814–11827.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2022) Fan Lai, Yinwei Dai, Sanjay Singapuram, Jiachen Liu, Xiangfeng
    Zhu, Harsha Madhyastha 和 Mosharaf Chowdhury. 2022. FedScale: 大规模联邦学习模型和系统性能的基准测试。收录于
    *国际机器学习会议*. PMLR, 11814–11827.'
- en: 'Lang (1995) Ken Lang. 1995. NewsWeeder: Learning to Filter Netnews. In *Machine
    Learning Proceedings 1995*, Armand Prieditis and Stuart Russell (Eds.). Morgan
    Kaufmann, San Francisco (CA), 331–339. [https://doi.org/10.1016/B978-1-55860-377-6.50048-7](https://doi.org/10.1016/B978-1-55860-377-6.50048-7)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lang (1995) Ken Lang. 1995. NewsWeeder: 学习过滤网络新闻。收录于 *Machine Learning Proceedings
    1995*, Armand Prieditis 和 Stuart Russell (编辑). Morgan Kaufmann, San Francisco
    (CA), 331–339. [https://doi.org/10.1016/B978-1-55860-377-6.50048-7](https://doi.org/10.1016/B978-1-55860-377-6.50048-7)'
- en: LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989. Backpropagation
    applied to handwritten zip code recognition. *Neural computation* 1, 4 (1989),
    541–551.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, Wayne Hubbard 和 Lawrence D Jackel. 1989. 反向传播应用于手写邮政编码识别。*神经计算*
    1, 4 (1989), 541–551.
- en: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner.
    1998. Gradient-based learning applied to document recognition. *Proc. IEEE* 86,
    11 (1998), 2278–2324.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1998) Yann LeCun, Léon Bottou, Yoshua Bengio 和 Patrick Haffner.
    1998. 基于梯度的学习应用于文档识别。*IEEE汇刊* 86, 11 (1998), 2278–2324.
- en: Leroy et al. (2019) David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht,
    and Joseph Dureau. 2019. Federated learning for keyword spotting. In *IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. 6341–6345.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leroy et al. (2019) David Leroy, Alice Coucke, Thibaut Lavril, Thibault Gisselbrecht
    和 Joseph Dureau. 2019. 关键字检测的联邦学习。收录于 *IEEE国际声学、语音与信号处理会议 (ICASSP)*. 6341–6345.
- en: Li et al. (2020) Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet
    Talwalkar, and Virginia Smith. 2020. Federated optimization in heterogeneous networks.
    *Proceedings of Machine learning and systems* 2 (2020), 429–450.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet
    Talwalkar 和 Virginia Smith. 2020. 异质网络中的联邦优化。*机器学习与系统会议论文集* 2 (2020), 429–450.
- en: 'Li et al. (2022) Xingyu Li, Zhe Qu, Shangqing Zhao, Bo Tang, Zhuo Lu, and Yao-Hong
    Liu. 2022. LoMar: A Local Defense Against Poisoning Attack on Federated Learning.
    *IEEE Transactions on Dependable and Secure Computing* 20 (2022), 437–450. [https://api.semanticscholar.org/CorpusID:245837821](https://api.semanticscholar.org/CorpusID:245837821)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2022）李星宇、曲哲、赵尚清、唐博、卢卓和刘耀宏。2022。LoMar：一种针对联邦学习中毒攻击的本地防御。*IEEE 可靠与安全计算事务*
    20（2022），437–450。 [https://api.semanticscholar.org/CorpusID:245837821](https://api.semanticscholar.org/CorpusID:245837821)
- en: 'Lin et al. (2019) Jierui Lin, Min Du, and Jian Liu. 2019. Free-riders in federated
    learning: Attacks and defenses. *arXiv preprint arXiv:1911.12560* (2019).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2019）林杰瑞、杜敏和刘健。2019。联邦学习中的“搭便车”现象：攻击与防御。*arXiv 预印本 arXiv:1911.12560*（2019）。
- en: 'Liu et al. (2021) Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang.
    2021. Fate: An industrial grade platform for collaborative learning with data
    protection. *The Journal of Machine Learning Research* 22, 1 (2021), 10320–10325.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2021）刘洋、范涛、陈天坚、许乾和杨强。2021。Fate：一个工业级的协作学习平台，具有数据保护功能。*机器学习研究杂志* 22，第1期（2021），10320–10325。
- en: 'Ludwig et al. (2020) Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou,
    Ali Anwar, Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu
    Sinn, et al. 2020. IBM Federated Learning: An Enterprise Framework White Paper
    v0.1. *arXiv preprint arXiv:2007.10987* (2020).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ludwig 等（2020）海科·路德维希、纳塔莉·巴拉卡尔多、吉吉·托马斯、易周、阿里·安瓦尔、沙尚克·拉贾莫尼、翁宇亚、贾亚拉姆·拉达克里希南、阿希什·维尔马、马修·辛、等。2020。IBM
    联邦学习：企业框架白皮书 v0.1。*arXiv 预印本 arXiv:2007.10987*（2020）。
- en: 'Luo et al. (2022) Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang,
    Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer
    for biomedical text generation and mining. *Briefings in Bioinformatics* 23, 6
    (09 2022). [https://doi.org/10.1093/bib/bbac409](https://doi.org/10.1093/bib/bbac409)
    arXiv:https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf
    bbac409.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo 等（2022）罗任乾、孙莉爱、夏颖策、秦涛、张胜、彭慧丰和刘铁岩。2022。BioGPT：用于生物医学文本生成和挖掘的生成预训练变换器。*生物信息学简报*
    23，第6期（2022年09月）。 [https://doi.org/10.1093/bib/bbac409](https://doi.org/10.1093/bib/bbac409)
    arXiv: https://academic.oup.com/bib/article-pdf/23/6/bbac409/47144271/bbac409.pdf
    bbac409。'
- en: Luo et al. (2021) Xinjian Luo, Yuncheng Wu, Xiaokui Xiao, and Beng Chin Ooi.
    2021. Feature inference attack on model predictions in vertical federated learning.
    In *IEEE International Conference on Data Engineering (ICDE)*. IEEE, 181–192.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo 等（2021）罗新建、吴云程、肖晓奎和黄振杰。2021。在垂直联邦学习中对模型预测的特征推断攻击。在*IEEE 数据工程国际会议（ICDE）*。IEEE，181–192。
- en: 'Ma et al. (2022) Zhuo Ma, Jianfeng Ma, Yinbin Miao, Yingjiu Li, and Robert H.
    Deng. 2022. ShieldFL: Mitigating Model Poisoning Attacks in Privacy-Preserving
    Federated Learning. *IEEE Transactions on Information Forensics and Security*
    17 (2022), 1639–1654. [https://api.semanticscholar.org/CorpusID:248358657](https://api.semanticscholar.org/CorpusID:248358657)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2022）马卓、马建锋、苗银彬、李颖玖和罗伯特·H·邓。2022。ShieldFL：缓解隐私保护联邦学习中的模型中毒攻击。*IEEE 信息取证与安全事务*
    17（2022），1639–1654。 [https://api.semanticscholar.org/CorpusID:248358657](https://api.semanticscholar.org/CorpusID:248358657)
- en: McMahan et al. (2017a) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017a. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*. PMLR, 1273–1282.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan 等（2017a）布伦丹·麦克马汉、艾德·摩尔、丹尼尔·拉梅奇、赛斯·汉普森和布莱斯·阿圭拉·亚尔卡斯。2017a。通信高效的深度网络学习来自分散数据。在*人工智能与统计*。PMLR，1273–1282。
- en: McMahan et al. (2017b) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Aguera y Arcas. 2017b. Communication-efficient learning of deep networks
    from decentralized data. In *Artificial intelligence and statistics*. PMLR, 1273–1282.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan 等（2017b）布伦丹·麦克马汉、艾德·摩尔、丹尼尔·拉梅奇、赛斯·汉普森和布莱斯·阿圭拉·亚尔卡斯。2017b。通信高效的深度网络学习来自分散数据。在*人工智能与统计*。PMLR，1273–1282。
- en: McMahan et al. (2016) H. B. McMahan, Eider Moore, Daniel Ramage, Seth Hampson,
    and Blaise Agüera y Arcas. 2016. Communication-Efficient Learning of Deep Networks
    from Decentralized Data. In *International Conference on Artificial Intelligence
    and Statistics*.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McMahan 等（2016）H. B. 麦克马汉、艾德·摩尔、丹尼尔·拉梅奇、赛斯·汉普森和布莱斯·阿圭拉·亚尔卡斯。2016。通信高效的深度网络学习来自分散数据。在*国际人工智能与统计会议*。
- en: Melis et al. (2018) Luca Melis, Congzheng Song, Emiliano De Cristofaro, and
    Vitaly Shmatikov. 2018. Exploiting Unintended Feature Leakage in Collaborative
    Learning. *2019 IEEE Symposium on Security and Privacy (SP)* (2018), 691–706.
    [https://api.semanticscholar.org/CorpusID:53099247](https://api.semanticscholar.org/CorpusID:53099247)
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melis et al. (2018) Luca Melis, Congzheng Song, Emiliano De Cristofaro, 和 Vitaly
    Shmatikov. 2018. 利用协作学习中的意外特征泄漏。*2019 IEEE 安全与隐私研讨会（SP）*（2018），691–706。 [https://api.semanticscholar.org/CorpusID:53099247](https://api.semanticscholar.org/CorpusID:53099247)
- en: Melis et al. (2019) Luca Melis, Congzheng Song, Emiliano De Cristofaro, and
    Vitaly Shmatikov. 2019. Exploiting unintended feature leakage in collaborative
    learning. In *2019 IEEE symposium on security and privacy (SP)*. IEEE, 691–706.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melis et al. (2019) Luca Melis, Congzheng Song, Emiliano De Cristofaro, 和 Vitaly
    Shmatikov. 2019. 利用协作学习中的意外特征泄漏。发表于*2019 IEEE 安全与隐私研讨会（SP）*。IEEE，691–706。
- en: Ozdayi et al. (2021) Mustafa Safa Ozdayi, Murat Kantarcioglu, and Yulia R Gel.
    2021. Defending against backdoors in federated learning with robust learning rate.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, Vol. 35\.
    9268–9276.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ozdayi et al. (2021) Mustafa Safa Ozdayi, Murat Kantarcioglu, 和 Yulia R Gel.
    2021. 用鲁棒学习率防御联邦学习中的后门攻击。发表于*AAAI 人工智能会议论文集*，第35卷，9268–9276。
- en: Pillutla et al. (2022) Krishna Pillutla, Sham M Kakade, and Zaid Harchaoui.
    2022. Robust aggregation for federated learning. *IEEE Transactions on Signal
    Processing* 70 (2022), 1142–1154.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pillutla et al. (2022) Krishna Pillutla, Sham M Kakade, 和 Zaid Harchaoui. 2022.
    联邦学习的鲁棒聚合。*IEEE 信号处理学报* 70（2022），1142–1154。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*. IEEE, 1–16.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    和 Yuxiong He. 2020. Zero: 训练万亿参数模型的内存优化。发表于*SC20: 高性能计算、网络、存储与分析国际会议*。IEEE，1–16。'
- en: 'Ramage (2020) Daniel Ramage. 2020. Federated Analytics: Collaborative Data
    Science Without Data Collection. *Google AI Blog* (May 2020). [https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramage (2020) Daniel Ramage. 2020. 联邦分析：无需数据收集的协作数据科学。*Google AI 博客*（2020年5月）。
    [https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html](https://ai.googleblog.com/2020/05/federated-analytics-collaborative-data.html)
- en: Ramaswamy et al. (2019) Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, and
    Françoise Beaufays. 2019. Federated learning for emoji prediction in a mobile
    keyboard. *arXiv preprint arXiv:1906.04329* (2019).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramaswamy et al. (2019) Swaroop Ramaswamy, Rajiv Mathews, Kanishka Rao, 和 Françoise
    Beaufays. 2019. 在移动键盘中进行表情符号预测的联邦学习。*arXiv 预印本 arXiv:1906.04329*（2019）。
- en: 'Rasouli et al. (2020) Mohammad Rasouli, Tao Sun, and Ram Rajagopal. 2020. Fedgan:
    Federated generative adversarial networks for distributed data. *arXiv preprint
    arXiv:2006.07228* (2020).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rasouli et al. (2020) Mohammad Rasouli, Tao Sun, 和 Ram Rajagopal. 2020. Fedgan:
    用于分布式数据的联邦生成对抗网络。*arXiv 预印本 arXiv:2006.07228*（2020）。'
- en: Reddi et al. (2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary
    Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, and Hugh Brendan McMahan. 2021.
    Adaptive Federated Optimization. In *International Conference on Learning Representations*.
    [https://openreview.net/forum?id=LkFG3lB13U5](https://openreview.net/forum?id=LkFG3lB13U5)
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reddi et al. (2021) Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary
    Garrett, Keith Rush, Jakub Konečný, Sanjiv Kumar, 和 Hugh Brendan McMahan. 2021.
    自适应联邦优化。发表于*国际学习表征会议*。 [https://openreview.net/forum?id=LkFG3lB13U5](https://openreview.net/forum?id=LkFG3lB13U5)
- en: 'Reina et al. (2021) G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,
    Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr Mokrov,
    Dmitry Agapov, et al. 2021. OpenFL: An open-source framework for Federated Learning.
    *arXiv preprint arXiv:2105.06413* (2021).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reina et al. (2021) G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina,
    Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr Mokrov,
    Dmitry Agapov, 等. 2021. OpenFL: 一个开源的联邦学习框架。*arXiv 预印本 arXiv:2105.06413*（2021）。'
- en: 'Roth et al. (2022) Holger R Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue
    Xu, Yuan-Ting Hsieh, Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, et al.
    2022. NVIDIA FLARE: Federated Learning from Simulation to Real-World. *arXiv preprint
    arXiv:2210.13291* (2022).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roth et al. (2022) Holger R Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue
    Xu, Yuan-Ting Hsieh, Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, 等.
    2022. NVIDIA FLARE: 从模拟到现实世界的联邦学习。*arXiv 预印本 arXiv:2210.13291*（2022）。'
- en: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. Learning representations by back-propagating errors. *nature* 323, 6088
    (1986), 533–536.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart et al. (1986) David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    1986. 通过反向传播误差学习表示。*nature* 323, 6088 (1986), 533–536。
- en: 'Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021.
    Manipulating the byzantine: Optimizing model poisoning attacks and defenses for
    federated learning. In *NDSS*.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shejwalkar and Houmansadr (2021) Virat Shejwalkar and Amir Houmansadr. 2021.
    操控拜占庭攻击：优化联邦学习的模型中毒攻击和防御。在 *NDSS* 会议上。
- en: Shokri and Shmatikov (2015) Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving
    deep learning. In *Proceedings of the 22nd ACM SIGSAC conference on computer and
    communications security*. 1310–1321.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shokri and Shmatikov (2015) Reza Shokri and Vitaly Shmatikov. 2015. 隐私保护的深度学习。在
    *第22届ACM SIGSAC计算机与通信安全会议* 上，1310–1321。
- en: 'Silva et al. (2020) Santiago Silva, Andre Altmann, Boris Gutman, and Marco
    Lorenzi. 2020. Fed-BioMed: A General Open-Source Frontend Framework for Federated
    Learning in Healthcare. In *Domain Adaptation and Representation Transfer, and
    Distributed and Collaborative Learning: Second MICCAI Workshop*. Springer, 201–210.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva et al. (2020) Santiago Silva, Andre Altmann, Boris Gutman, and Marco Lorenzi.
    2020. Fed-BioMed：一个用于医疗健康领域的通用开源前端框架。在 *领域适应和表示迁移，以及分布式和协作学习：第二届MICCAI研讨会* 上。Springer,
    201–210。
- en: 'Sun et al. (2021) Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh,
    Yiran Chen, and Hai Li. 2021. Fl-wbc: Enhancing robustness against model poisoning
    attacks in federated learning from a client perspective. *Advances in Neural Information
    Processing Systems* 34 (2021), 12613–12624.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2021) Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh, Yiran
    Chen, and Hai Li. 2021. FL-WBC：从客户端视角提升对模型中毒攻击的鲁棒性。*Advances in Neural Information
    Processing Systems* 34 (2021), 12613–12624。
- en: Sun et al. (2019) Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan
    McMahan. 2019. Can you really backdoor federated learning? *arXiv preprint arXiv:1911.07963*
    (2019).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2019) Ziteng Sun, Peter Kairouz, Ananda Theertha Suresh, and H Brendan
    McMahan. 2019. 你真的可以在联邦学习中设置后门吗？*arXiv preprint arXiv:1911.07963* (2019)。
- en: Theta Network. (2023) Theta Network. 2023. Theta Network Website. https://thetatoken.org/.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Theta Network. (2023) Theta Network. 2023. Theta Network 网站。https://thetatoken.org/。
- en: Tolpegin et al. (2020) Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and
    Ling Liu. 2020. Data poisoning attacks against federated learning systems. In
    *European Symposium on Research in Computer Security*. Springer, 480–501.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tolpegin et al. (2020) Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and
    Ling Liu. 2020. 针对联邦学习系统的数据中毒攻击。在 *欧洲计算机安全研究研讨会* 上。Springer, 480–501。
- en: 'Wang et al. (2022b) Dan Wang, Siping Shi, Yifei Zhu, and Zhu Han. 2022b. Federated
    Analytics: Opportunities and Challenges. *IEEE Network* 36 (2022), 151–158.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) Dan Wang, Siping Shi, Yifei Zhu, and Zhu Han. 2022b. 联邦分析：机遇与挑战。*IEEE
    Network* 36 (2022), 151–158。
- en: 'Wang et al. (2023) Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes,
    Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. 2023.
    ZeRO++: Extremely Efficient Collective Communication for Giant Model Training.
    *arXiv preprint arXiv:2306.10209* (2023).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes,
    Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. 2023.
    ZeRO++：用于巨型模型训练的极其高效的集体通信。*arXiv preprint arXiv:2306.10209* (2023)。
- en: 'Wang et al. (2020b) H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S.
    Agarwal, J. Sohn, K. Lee, and D. Papailiopoulos. 2020b. Attack of the tails: Yes,
    you really can backdoor federated learning. In *NeurIPS*.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020b) H. Wang, K. Sreenivasan, S. Rajput, H. Vishwakarma, S. Agarwal,
    J. Sohn, K. Lee, and D. Papailiopoulos. 2020b. 攻击尾部：是的，你确实可以在联邦学习中设置后门。在 *NeurIPS*
    会议上。
- en: 'Wang (2022) Jianhua Wang. 2022. PASS: Parameters Audit-based Secure and Fair
    Federated Learning Scheme against Free Rider. *arXiv preprint arXiv:2207.07292*
    (2022).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang (2022) Jianhua Wang. 2022. PASS：基于参数审计的安全和公平联邦学习方案，以防止搭便车行为。*arXiv preprint
    arXiv:2207.07292* (2022)。
- en: Wang et al. (2020a) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H. Vincent
    Poor. 2020a. Tackling the Objective Inconsistency Problem in Heterogeneous Federated
    Optimization. *ArXiv* abs/2007.07481 (2020).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020a) Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H.
    Vincent Poor. 2020a. 解决异质联邦优化中的目标不一致问题。*ArXiv* abs/2007.07481 (2020)。
- en: Wang et al. (2022a) Zhibo Wang, Yuting Huang, Mengkai Song, Libing Wu, Feng
    Xue, and Kui Ren. 2022a. Poisoning-assisted property inference attack against
    federated learning. *IEEE Transactions on Dependable and Secure Computing* (2022).
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) Zhibo Wang, Yuting Huang, Mengkai Song, Libing Wu, Feng
    Xue, and Kui Ren. 2022a. 辅助中毒的属性推断攻击针对联邦学习。*IEEE Transactions on Dependable and
    Secure Computing* (2022)。
- en: 'Xie et al. (2021) Chulin Xie, Minghao Chen, Pin-Yu Chen, and Bo Li. 2021. CRFL:
    Certifiably robust federated learning against backdoor attacks. In *International
    Conference on Machine Learning*. PMLR, 11372–11382.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 (2021) Chulin Xie, Minghao Chen, Pin-Yu Chen, 和 Bo Li。2021年。《CRFL: 针对后门攻击的认证稳健联邦学习》。发表于
    *International Conference on Machine Learning*。PMLR, 11372–11382。'
- en: 'Xie et al. (2020) Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. 2020. SLSGD:
    Secure and Efficient Distributed On-device Machine Learning. In *Joint European
    Conference on Machine Learning and Knowledge Discovery in Databases*. Springer,
    213–228.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 (2020) Cong Xie, Oluwasanmi Koyejo, 和 Indranil Gupta。2020年。《SLSGD: 安全高效的分布式设备端机器学习》。发表于
    *Joint European Conference on Machine Learning and Knowledge Discovery in Databases*。Springer,
    213–228。'
- en: 'Xie et al. (2022) Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, and Jingren Zhou. 2022. FederatedScope:
    A Flexible Federated Learning Platform for Heterogeneity. *arXiv preprint arXiv:2204.05011*
    (2022).'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人 (2022) Yuexiang Xie, Zhen Wang, Daoyuan Chen, Dawei Gao, Liuyi Yao,
    Weirui Kuang, Yaliang Li, Bolin Ding, 和 Jingren Zhou。2022年。《FederatedScope: 一种灵活的异构联邦学习平台》。*arXiv
    预印本 arXiv:2204.05011* (2022)。'
- en: Xu et al. (2022) Jian Xu, Shao-Lun Huang, Linqi Song, and Tian Lan. 2022. Byzantine-robust
    federated learning through collaborative malicious gradient filtering. In *2022
    IEEE 42nd International Conference on Distributed Computing Systems (ICDCS)*.
    IEEE, 1223–1235.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2022) Jian Xu, Shao-Lun Huang, Linqi Song, 和 Tian Lan。2022年。《通过协作恶意梯度过滤实现拜占庭稳健的联邦学习》。发表于
    *2022 IEEE 第42届国际分布式计算系统会议 (ICDCS)*。IEEE, 1223–1235。
- en: 'Yang et al. (2019) H. Yang, X. Zhang, M. Fang, and J. Liu. Dec 2019. Byzantine-resilient
    stochastic gradient descent for distributed learning: A Lipschitz-inspired coordinate-wise
    median approach. In *IEEE CDC*.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2019) H. Yang, X. Zhang, M. Fang, 和 J. Liu。2019年12月。《用于分布式学习的拜占庭稳健随机梯度下降:
    一种受利普希茨启发的坐标-wise 中位数方法》。发表于 *IEEE CDC*。'
- en: 'Yin et al. (2018) Dong Yin, Yudong Chen, Kannan Ramchandran, and Peter Bartlett.
    2018. Byzantine-robust distributed learning: Towards optimal statistical rates.
    In *International Conference on Machine Learning*. PMLR, 5650–5659.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yin 等人 (2018) Dong Yin, Yudong Chen, Kannan Ramchandran, 和 Peter Bartlett。2018年。《拜占庭稳健分布式学习:
    迈向最佳统计速率》。发表于 *International Conference on Machine Learning*。PMLR, 5650–5659。'
- en: 'Zhang et al. (2020) Jingwen Zhang, Jiale Zhang, Junjun Chen, and Shui Yu. 2020.
    Gan enhanced membership inference: A passive local attack in federated learning.
    In *ICC 2020-2020 IEEE International Conference on Communications (ICC)*. IEEE,
    1–6.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2020) Jingwen Zhang, Jiale Zhang, Junjun Chen, 和 Shui Yu。2020年。《Gan
    增强的成员推断: 联邦学习中的被动本地攻击》。发表于 *ICC 2020-2020 IEEE International Conference on Communications
    (ICC)*。IEEE, 1–6。'
- en: 'Zhang et al. (2022) Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang,
    Michael Mahoney, Prateek Mittal, Ramchandran Kannan, and Joseph Gonzalez. 2022.
    Neurotoxin: Durable backdoors in federated learning. In *International Conference
    on Machine Learning*. PMLR, 26429–26446.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 (2022) Zhengming Zhang, Ashwinee Panda, Linyue Song, Yaoqing Yang,
    Michael Mahoney, Prateek Mittal, Ramchandran Kannan, 和 Joseph Gonzalez。2022年。《Neurotoxin:
    联邦学习中的持久后门》。发表于 *International Conference on Machine Learning*。PMLR, 26429–26446。'
- en: Zhu et al. (2019) Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage
    from gradients. *Advances in Neural Information Processing Systems* 32 (2019).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2019) Ligeng Zhu, Zhijian Liu, 和 Song Han。2019年。《梯度中的深度泄漏》。*Advances
    in Neural Information Processing Systems* 32 (2019)。
- en: 'Ziller et al. (2021) Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin
    Szymkow, Bobby Wagner, Emma Bluemke, Jean-Mickael Nounahon, Jonathan Passerat-Palmbach,
    Kritika Prakash, Nick Rose, et al. 2021. PySyft: A library for easy federated
    learning. *Federated Learning Systems: Towards Next-Generation AI* (2021), 111–139.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ziller 等人 (2021) Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin
    Szymkow, Bobby Wagner, Emma Bluemke, Jean-Mickael Nounahon, Jonathan Passerat-Palmbach,
    Kritika Prakash, Nick Rose 等人。2021年。《PySyft: 一种简便的联邦学习库》。*Federated Learning Systems:
    Towards Next-Generation AI* (2021), 111–139。'
- en: Appendix
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Appendix A Supplementary Experiment
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 补充实验
- en: 'We demonstrate screenshots of the real-world experiment. We present the FL
    process and the training status of each real-world device in Figure [16](#A1.F16
    "Figure 16 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs") and Figure [17](#A1.F17
    "Figure 17 ‣ Appendix A Supplementary Experiment ‣ FedMLSecurity: A Benchmark
    for Attacks and Defenses in Federated Learning and Federated LLMs"), respectively.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了实际实验的屏幕截图。我们在图 [16](#A1.F16 "Figure 16 ‣ Appendix A Supplementary Experiment
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") 和图 [17](#A1.F17 "Figure 17 ‣ Appendix A Supplementary Experiment
    ‣ FedMLSecurity: A Benchmark for Attacks and Defenses in Federated Learning and
    Federated LLMs") 中分别展示了 FL 过程和每个实际设备的训练状态。'
- en: '![Refer to caption](img/3d3e8a30a555e827667769010c044240.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d3e8a30a555e827667769010c044240.png)'
- en: 'Figure 16. Real-world application. Yellow: aggregation server waiting time;
    pink: aggregation time; green: client training time; blue: client communication.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图16. 现实世界应用。黄色：聚合服务器等待时间；粉色：聚合时间；绿色：客户端训练时间；蓝色：客户端通信。
- en: '![Refer to caption](img/5629b831cf133d0e4c4e0f99f6611673.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5629b831cf133d0e4c4e0f99f6611673.png)'
- en: 'Figure 17. Real-world application: training status of devices.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图17. 现实世界应用：设备的训练状态。
