- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:43:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:43:39'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向安全可信的 6G 网络下的大型语言模型（LLMs）：攻击、防御与机遇
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.00722](https://ar5iv.labs.arxiv.org/html/2408.00722)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.00722](https://ar5iv.labs.arxiv.org/html/2408.00722)
- en: 'Sunder Ali Khowaja , Parus Khuwaja, Kapal Dev , Hussam Al Hamadi, and Engin
    Zeydan Sunder Ali Khowaja is with School of Computing, Faculty of Computing, Digital
    and Data, Technological University Dublin, and CONNECT Centre, Ireland. Email:
    sunderali.khowaja@tudublin.ieParus Khowaja is with Institute of Business Administration,
    University of Sindh, Jamshoro. (e-mail:Parus.khuwaja@usindh.edu.pk).Kapal Dev
    is associated with CONNECT Centre and Department of Computer Science and Munster
    Technological University, Bishopstown, Cork, T12 P928, Ireland,e-mail: (kapal.dev@ieee.org)Hussam
    Al Hamadi with College of Engineering and IT University of Dubai, e-mail: (Halhammadi@ud.ac.ae)Engin
    Zeydan with Centre Tecnològic de Telecomunicacions de Catalunya (CTTC), Barcelona,
    Spain, 08860, e-mail: (ezeydan@cttc.es)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sunder Ali Khowaja，Parus Khuwaja，Kapal Dev，Hussam Al Hamadi 和 Engin Zeydan。Sunder
    Ali Khowaja 在都柏林理工大学计算学院、数字与数据学院及 CONNECT 中心工作，来自爱尔兰。电子邮件: sunderali.khowaja@tudublin.ie。Parus
    Khuwaja 在辛德大学工商管理学院工作。电子邮件: Parus.khuwaja@usindh.edu.pk。Kapal Dev 与 CONNECT 中心和计算机科学系以及爱尔兰科克市的穆恩斯特理工大学有关，电子邮件:
    kapal.dev@ieee.org。Hussam Al Hamadi 在迪拜大学工程与信息技术学院工作，电子邮件: Halhammadi@ud.ac.ae。Engin
    Zeydan 在西班牙巴塞罗那的加泰罗尼亚电信技术中心 (CTTC) 工作，电子邮件: ezeydan@cttc.es。'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, large language models (LLMs) have been gaining a lot of interest due
    to their adaptability and extensibility in emerging applications, including communication
    networks. It is anticipated that 6G mobile edge computing networks will be able
    to support LLMs as a service, as they provide ultra reliable low-latency communications
    and closed loop massive connectivity. However, LLMs are vulnerable to data and
    model privacy issues that affect the trustworthiness of LLMs to be deployed for
    user-based services. In this paper, we explore the security vulnerabilities associated
    with fine-tuning LLMs in 6G networks, in particular the membership inference attack.
    We define the characteristics of an attack network that can perform a membership
    inference attack if the attacker has access to the fine-tuned model for the downstream
    task. We show that the membership inference attacks are effective for any downstream
    task, which can lead to a personal data breach when using LLM as a service. The
    experimental results show that the attack success rate of maximum 92% can be achieved
    on named entity recognition task. Based on the experimental analysis, we discuss
    possible defense mechanisms and present possible research directions to make the
    LLMs more trustworthy in the context of 6G networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）因其在新兴应用中的适应性和扩展性而受到广泛关注，包括通信网络。预计 6G 移动边缘计算网络将能够支持 LLM 作为服务，因为它们提供超可靠的低延迟通信和闭环的大规模连接。然而，LLMs
    面临数据和模型隐私问题，这些问题影响了 LLM 在用户服务中部署的可信度。本文探讨了在 6G 网络中微调 LLMs 时相关的安全漏洞，特别是成员推断攻击。我们定义了一个攻击网络的特征，该网络能够执行成员推断攻击，前提是攻击者可以访问用于下游任务的微调模型。我们展示了成员推断攻击对任何下游任务都是有效的，这可能导致在使用
    LLM 作为服务时发生个人数据泄露。实验结果表明，在命名实体识别任务上，最大可达 92% 的攻击成功率。基于实验分析，我们讨论了可能的防御机制，并提出了在
    6G 网络环境下使 LLM 更可信的研究方向。
- en: I Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: The emergence of attention networks has been a stepping stone for transformer
    architectures, which also led to the introduction of large language models (LLMs).
    More recently, LLMs are seen as the most significant advance in the field of artificial
    intelligence (AI) and a potential pathway to artificial general intelligence (AGI)
    [[1](#bib.bib1)]. Every tech giant is in a race to advance in the field of LLMs
    by leveraging generative AI (GAI). Notable examples of LLMs from the tech giants
    are GPT-4 from OpenAI, LLaMA-3 from Meta and PALM from Google. However, there
    are also new players in this field that surpass the performance of the LLMs mentioned
    above. These include Mistral (in collaboration with NVIDIA), DCLM from Apple,
    xLAM from Salesforce, v2 chat from Deepseek, Groq, Claude, SmolLM and many more.
    These LLMs are trained on diverse and large amounts of datasets scraped or curated
    from the Internet. Some LLMs focus on increasing model size, such as GPT, while
    others find new ways to improve the generalization of LLMs through data curation,
    model quantization, and innovative techniques. Examples of such LLMs are Claude,
    LLaMA, DCLM and Groq, which have recently outperformed GPT on various language
    tasks. In continuation of the above-mentioned advances in LLMs, several enterprises
    are leveraging pre-trained encoders of LLMs to varying degrees to develop their
    own customized solutions for various applications and sectors, including healthcare,
    education, law and industrial automation. In view of the rapid development of
    LLMs, it can be assumed that LLMs will soon also be deployed on edge and handheld
    devices. Several studies have indicated that the current iteration of 5G networks
    will not be able to support a plethora of services offered by LLMs. Therefore,
    researchers are working intensively on the next iteration of communication systems,
    i.e. Sixth generation (6G), to meet the above requirements [[2](#bib.bib2)]. Furthermore,
    as AI is an integral part of 6G systems, it is assumed that LLMs will be used
    intrinsically to optimize resources and performance while enabling human-centric
    customized services to users.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力网络的出现是变换器架构的一个重要里程碑，这也导致了大语言模型（LLMs）的出现。最近，LLMs 被视为人工智能（AI）领域最重要的进展，并且是通向人工通用智能（AGI）的潜在途径[[1](#bib.bib1)]。每个科技巨头都在利用生成性
    AI（GAI）在 LLMs 领域展开竞争。科技巨头的显著 LLMs 示例包括 OpenAI 的 GPT-4、Meta 的 LLaMA-3 和 Google
    的 PALM。然而，也有一些新兴玩家在此领域超越了上述 LLMs 的表现。这些包括与 NVIDIA 合作的 Mistral、Apple 的 DCLM、Salesforce
    的 xLAM、Deepseek 的 v2 chat、Groq、Claude、SmolLM 等等。这些 LLMs 在大量来自互联网的多样化数据集上进行训练。一些
    LLMs 专注于增加模型规模，如 GPT，而其他则通过数据策划、模型量化和创新技术寻找提升 LLMs 泛化能力的新方法。Claude、LLaMA、DCLM
    和 Groq 等 LLMs 近年来在各种语言任务中超越了 GPT。为了继续推进上述 LLMs 的进展，许多企业正在不同程度上利用预训练的 LLM 编码器，开发其定制化解决方案，涉及医疗保健、教育、法律和工业自动化等多个领域。鉴于
    LLMs 的快速发展，可以预见 LLMs 很快也将部署在边缘和手持设备上。几项研究表明，当前版本的 5G 网络将无法支持 LLMs 提供的大量服务。因此，研究人员正在密切关注下一代通信系统，即第六代（6G），以满足上述要求[[2](#bib.bib2)]。此外，由于
    AI 是 6G 系统的一个核心部分，预计 LLMs 将被内在地用于优化资源和性能，同时为用户提供以人为本的定制服务。
- en: '![Refer to caption](img/6a62c97f6d3f9cfb77c4353c09383ef1.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6a62c97f6d3f9cfb77c4353c09383ef1.png)'
- en: 'Figure 1: Network for LLMs illustration in 6G with examples of smart homes
    and emergency services. The LLMs can be used by the central cloud and shared with
    the 6G Edge Cloud. The 6G edge cloud then share either the parameters to the radio
    access networks or users to fine-tune the network for personalization, or it shres
    the cached version of the model to provide a specific service.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：6G 网络中的 LLM 说明图，展示了智能家居和紧急服务的示例。LLMs 可以由中央云使用，并与 6G 边缘云共享。6G 边缘云随后将参数共享给无线接入网络或用户，以便对网络进行个性化调整，或者共享模型的缓存版本以提供特定服务。
- en: 'The standardization process towards 6G systems is already progressing steadily.
    It is assumed that the evolution of the communication system will support distributed
    AI both for edge devices and within the mobile network [[2](#bib.bib2)]. Although
    many researchers argue that the edge devices will not support the use of LLMs,
    but with continuous breakthroughs in the field of AI, support for edge devices
    can be extended through distributed learning techniques such as federated learning
    (FL) and split learning (SL) [[3](#bib.bib3)]. In addition, quantization and training
    can be used to fine-tune an LLM on the edge devices. As proposed in [[4](#bib.bib4)],
    an LLM with 65 billion parameters can be fine-tuned with quantized low-rank adapters
    (QLoRA) on a downstream task within a single day, achieving comparable performance
    compared to other state-of-the-art (SOTA) LLMs. It can be assumed that the convergence
    of quantized networks, LLMs and 6G Multi-Access Edge Computing (MEC) could result
    in many innovative applications. Researchers have already begun to explore the
    mutual convergence of LLMs and 6G MEC networks, calling them “LLMs for networks”
    and “networks for LLMs” respectively. We illustrate a network for LLMs that corresponds
    to the vision of the Network for AI (NetAI) with respect to a 6G communication
    system in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Pathway to Secure and
    Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities"). The NetAI vision
    supports LLM deployments related to the MEC architecture [[2](#bib.bib2)]. Various
    services such as smart homes, healthcare, education, emergencies, mission-critical
    applications and finance can be supported with the network for LLMs.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '朝向6G系统的标准化过程已经在稳步推进。假设通信系统的演进将支持边缘设备和移动网络中的分布式AI[[2](#bib.bib2)]。尽管许多研究人员认为边缘设备不会支持LLMs的使用，但随着AI领域的不断突破，通过分布式学习技术，如联邦学习（FL）和分裂学习（SL），对边缘设备的支持可以得到扩展[[3](#bib.bib3)]。此外，量化和训练可以用于在边缘设备上微调LLM。正如[[4](#bib.bib4)]中提出的，具有65亿参数的LLM可以通过量化低秩适配器（QLoRA）在一天内在下游任务上进行微调，性能与其他先进（SOTA）LLMs相当。可以假设，量化网络、LLMs和6G多接入边缘计算（MEC）的融合可能会带来许多创新应用。研究人员已经开始探索LLMs与6G
    MEC网络的相互融合，分别称之为“网络中的LLMs”和“LLMs中的网络”。我们在图[1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities")中展示了一个LLMs网络，它对应于6G通信系统中AI网络（NetAI）的愿景。NetAI愿景支持与MEC架构相关的LLM部署[[2](#bib.bib2)]。诸如智能家居、医疗保健、教育、紧急情况、关键任务应用和金融等各种服务可以通过LLMs网络得到支持。'
- en: Most research today focuses on the integration of LLMs and communication networks,
    which would undoubtedly bring unprecedented advances and technological innovation.
    However, one aspect of this technological progress is being overlooked, namely
    the security aspect. With all the possibilities and potential of LLMs and the
    6G ecosystem, we have to ask ourselves, are LLMs trustworthy? Despite their ability
    to fine-tune to the downstream task, LLMs are deep neural networks that are vulnerable
    to privacy attacks, such as model inversion, model poisoning and membership leakage
    [[3](#bib.bib3), [5](#bib.bib5)]. The growing landscape of LLMs and their integration
    into communication systems therefore makes it necessary to address security concerns
    and the development of trustable AI encoders to safeguard the integrity of users
    and services in 6G systems. To the best of our knowledge, the studies have not
    explored the security vulnerabilities in network for LLMs (Net4LLMs), which subsequently
    leaves us defenseless against such attacks as we progress towards the Net4AI vision.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的大多数研究集中在LLMs与通信网络的集成上，这无疑会带来前所未有的进展和技术创新。然而，这一技术进步中有一个方面被忽视了，即安全性方面。鉴于LLMs和6G生态系统的所有可能性和潜力，我们必须问自己，LLMs是否值得信赖？尽管它们能够对下游任务进行微调，但LLMs是深度神经网络，易受到隐私攻击，如模型反演、模型投毒和成员泄露[[3](#bib.bib3),
    [5](#bib.bib5)]。LLMs日益增长的应用及其在通信系统中的集成使得必须解决安全问题，并开发可信的AI编码器，以保护6G系统中用户和服务的完整性。据我们所知，目前的研究尚未探索网络中LLMs的安全漏洞（Net4LLMs），这使得我们在向Net4AI愿景迈进的过程中容易受到这些攻击的威胁。
- en: 'To address the above problem, in this paper, we propose to audit the trustworthiness
    of pre-trained AI encoders for membership leakage attacks. The membership leakage
    attack is an attack in which the adversary tries to find out the distribution
    of the training data used to train the AI encoders. Considering that the Net4LLMs
    will focus on fine-tuning the pre-trained AI encoders for service provisioning,
    the attacker will aim to determine if a data sample was used for the fine-tuning
    process. The fine-tuning process enables the replacement of task-specific layer
    in the pre-trained AI encoder to meet tasks such as questions and answers, name
    entity recognition and classification [[5](#bib.bib5), [6](#bib.bib6)]. We evaluate
    the trustworthiness of AI encoders against the membership inference attacks. We
    assume that the adversary has particularly the knowledge of the downstream task
    and the adversary is provided with the fine-tuned model, also known as black-box
    setting. We conduct experiments to evaluate the trustworthiness of the AI pre-trained
    encoders and to develop possible defenses to prevent the membership leakage attacks
    in the context of Net4LLMs. The specific contributions of this work are characterized
    as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，本文提出对预训练AI编码器在成员泄漏攻击中的可信度进行审计。成员泄漏攻击是一种攻击，攻击者试图找出用于训练AI编码器的训练数据的分布。考虑到Net4LLMs将专注于对预训练AI编码器进行微调以提供服务，攻击者将旨在确定数据样本是否用于微调过程。微调过程使得可以替换预训练AI编码器中的任务特定层，以满足问题和回答、命名实体识别和分类等任务[[5](#bib.bib5),
    [6](#bib.bib6)]。我们评估AI编码器在成员推断攻击中的可信度。我们假设攻击者特别了解下游任务，并且攻击者获得了微调后的模型，也称为黑盒设置。我们进行实验以评估AI预训练编码器的可信度，并开发可能的防御措施，以防止在Net4LLMs背景下的成员泄漏攻击。本工作的具体贡献如下：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This is the first study to investigate the trustworthiness of pre-trained AI
    encoders for Net4LLMs.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是第一项研究Net4LLMs中预训练AI编码器可信度的研究。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Membership leakage attacks in the context of Net4LLMs are explored to assess
    trustworthiness.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探讨了Net4LLMs背景下的成员泄漏攻击，以评估可信度。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Based on experimental analysis, defenses are proposed to audit trust in Net4LLMs.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于实验分析，提出了防御措施，以审计Net4LLMs的信任度。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: At the end of the paper, open issues, challenges and future directions are also
    proposed to prevent potential adversarial attacks on LLMs.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在论文的最后，还提出了开放问题、挑战和未来方向，以防止对LLMs的潜在对抗性攻击。
- en: '![Refer to caption](img/ccaa024b027ef48c7f45752fdf4146aa.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ccaa024b027ef48c7f45752fdf4146aa.png)'
- en: 'Figure 2: Training, Fine-tuning, and customization strategies for LLMs in MEC
    6G framework.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：MEC 6G框架中LLMs的训练、微调和定制策略。
- en: II Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: Recently, we have observed a plethora of advances in the field of LLMs that
    would be a revolution in the field of communication networks, especially in the
    design and development of 6G networks. Some studies have already explored and
    demonstrated the significance of LLMs for potential 6G applications. For example,
    the study in [[7](#bib.bib7)] proposed NetGPT, which enables personalized services
    to users through generative networks while handling comprehensive network intelligence
    and cloud collaboration in real time.Xu et al. [[8](#bib.bib8)] focused on the
    data privacy in 6G communication systems using LLMs. The study proposed to design
    LLM agents based on the principle of split learning by distributing LLMs for different
    roles across edge devices to make user interaction efficient and collaborative.
    Their results show that the split learning setting was effective in improving
    the communication efficiency while offloading the tasks that are complex in nature
    to the servers for constructing global LLMs. The study in [[9](#bib.bib9)] emphasised
    that the newer LLMs must offer multimodal services, i.e. they must handle image,
    text and audio data in order to offer automated services. Therefore, the deployment
    of LLM agents in the cloud could pose challenges in terms of data privacy, high
    bandwidth costs and long response times. However, MEC based on 6G communication
    systems can address the above problems in an effective way. Lin et al. [[10](#bib.bib10)]also
    proposed a split learning framework for the deployment of LLMs in 6G networks.
    However, their work focused on the efficiency and effectiveness of LLMs in terms
    of parameter sharing, quantization, and efficient fine-tuning rather than data
    or model security. Nguyen et al. [[5](#bib.bib5)] highlighted the advantages of
    using LLMs in 6G networks while exploring the security vulnerabilities from an
    adversarial point of view. However, the discussion of model security was very
    abstract and brief, focusing more on the attacks on services. The study also suggested
    the use of blockchain technology to avoid the security threats associated with
    LLMs and 6G networks. To the best of our knowledge, none of the studies investigated
    a specific model-based attack scenario related to pre-trained AI encoders and
    6G networks.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，我们观察到了大规模语言模型（LLMs）领域的众多进展，这将对通信网络领域产生革命性影响，尤其是在6G网络的设计和开发方面。一些研究已经探讨并展示了LLMs在潜在6G应用中的重要性。例如，[[7](#bib.bib7)]的研究提出了NetGPT，它通过生成网络向用户提供个性化服务，同时实时处理全面的网络智能和云协作。Xu等人[[8](#bib.bib8)]关注了使用LLMs的6G通信系统中的数据隐私问题。该研究建议基于分裂学习的原则设计LLM代理，通过将LLMs分配给不同的边缘设备角色，使用户互动更加高效和协作。他们的结果表明，分裂学习设置在提高通信效率方面是有效的，同时将复杂的任务卸载到服务器上以构建全球LLMs。[[9](#bib.bib9)]的研究强调了更新的LLMs必须提供多模态服务，即它们必须处理图像、文本和音频数据以提供自动化服务。因此，LLM代理在云中的部署可能在数据隐私、高带宽成本和长响应时间方面带来挑战。然而，基于6G通信系统的边缘计算（MEC）可以有效解决上述问题。Lin等人[[10](#bib.bib10)]还提出了一个用于6G网络中LLMs部署的分裂学习框架。然而，他们的工作重点是LLMs在参数共享、量化和高效微调方面的效率和有效性，而非数据或模型安全。Nguyen等人[[5](#bib.bib5)]突出了在6G网络中使用LLMs的优势，同时从对抗性角度探讨了安全漏洞。然而，模型安全的讨论非常抽象和简略，更侧重于对服务的攻击。该研究还建议使用区块链技术以避免与LLMs和6G网络相关的安全威胁。据我们所知，没有研究调查过与预训练AI编码器和6G网络相关的特定模型攻击场景。
- en: III Training/Fine-tuning LLMs in 6G
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 训练/微调6G中的LLMs
- en: 'The training strategies for fine-tuning LLMs in 6G networks are shown in Figure
    [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Pathway to Secure and Trustworthy 6G for
    LLMs: Attacks, Defense, and Opportunities"). The strategies are presented in accordance
    with the MEC framework. The cloud layer can leverage the pre-trained AI encoders
    of LLMs for any of the training modes, but the downstream task would mostly be
    generalized when passed to the edge and user layers. However, at the edge and
    user layer, the customization of the LLMs can be done based on the context and
    resources. As can be seen from the training modes, fine-tuning the pre-trained
    encoder and classifier requires large amounts of computational resources. The
    computational resources decrease significantly when moving to the edge and user
    layer. In this regard, the edge and user layers can at most fine-tune/adapt the
    LLMs based on their application by keeping the pre-trained encoder frozen and
    using the final layers to update the parameters. Alternatively, they can simply
    freeze the entire LLM and extract only the output embeddings that are used as
    representative features for training deep neural networks or shallow learning
    methods.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '微调 LLMs（大语言模型）在 6G 网络中的训练策略如图 [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Pathway
    to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities") 所示。这些策略是根据
    MEC 框架呈现的。云层可以利用 LLMs 的预训练 AI 编码器进行任何训练模式，但当传递到边缘和用户层时，下游任务通常会变得更为通用。然而，在边缘和用户层，可以根据上下文和资源对
    LLMs 进行定制。从训练模式可以看出，微调预训练的编码器和分类器需要大量的计算资源。而当转移到边缘和用户层时，计算资源显著减少。在这方面，边缘和用户层最多可以基于应用对
    LLMs 进行微调/适应，通过保持预训练编码器不变并使用最终层更新参数。或者，他们可以简单地冻结整个 LLM 只提取用于训练深度神经网络或浅层学习方法的输出嵌入。'
- en: 'An example of this can be found in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities")
    for an emergency service application where LLM’s pre-trained AI encoder can be
    used and its parameters frozen while a limited amount of labeled data is used
    to train the classifier or the final layers. This would be beneficial for edge
    devices to fine-tune the network locally to improve communication efficiency while
    utilizing computing resources efficiently. As proposed in [[10](#bib.bib10)],
    the customization can be achieved through a split learning strategy, where the
    cloud wants to fine-tune the LLM for a specific task with respect to the communication
    system and splits the network into smaller networks that are trained with devices
    from the edge layer and the user layer. Some specific techniques such as Low Ranking
    Adaptation (LoRA), Quantized LoRA (QLoRA), Parameter Efficient Fine-Tuning (PEFT),
    Deep Speed and ZeRO can be used to fine-tune the LLMs. LoRA uses low-rank approximations
    to fine-tune the LLMs for specific tasks, resulting in reduced financial and computational
    costs. QLoRA further reduces memory utilization while fine-tuning LLMs with the
    LoRA technique. PEFT adjusts key parameters and uses the catastrophic forgetting
    technique to fine-tune LLMs with a small subset of parameters. ZeRO uses memory
    optimization and data parallelism techniques to fine-tune LLMs, and DeepSpeed
    uses the ZeRO redundancy optimizer to fine-tune LLMs in a distributed learning
    fashion. The above techniques can be used extensively at the user, edge and cloud
    layers to fine-tune LLMs for various purposes.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '一个例子可以在图 [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Pathway to Secure and Trustworthy
    6G for LLMs: Attacks, Defense, and Opportunities") 中找到，展示了一个紧急服务应用，其中 LLM 的预训练
    AI 编码器可以使用并将其参数冻结，同时使用有限量的标记数据来训练分类器或最终层。这对边缘设备来说是有益的，可以在本地微调网络，以提高通信效率，同时有效利用计算资源。正如
    [[10](#bib.bib10)] 中提出的，定制可以通过分裂学习策略来实现，其中云层希望针对通信系统的特定任务对 LLM 进行微调，并将网络分成较小的网络，这些网络与来自边缘层和用户层的设备一起进行训练。一些特定技术如低秩适应（LoRA）、量化
    LoRA（QLoRA）、参数高效微调（PEFT）、DeepSpeed 和 ZeRO 可以用来微调 LLMs。LoRA 使用低秩近似来微调 LLMs 针对特定任务，从而减少财务和计算成本。QLoRA
    在使用 LoRA 技术微调 LLMs 时进一步减少内存利用。PEFT 调整关键参数并使用灾难性遗忘技术来用小部分参数微调 LLMs。ZeRO 使用内存优化和数据并行技术来微调
    LLMs，DeepSpeed 使用 ZeRO 冗余优化器以分布式学习方式微调 LLMs。上述技术可以广泛应用于用户、边缘和云层，以实现 LLMs 的各种用途。'
- en: III-A Security Issues in LLMs and 6G
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A LLMs 和 6G 的安全问题
- en: Several studies have now highlighted the security concerns related to the behavior,
    architecture and design of LLM. The security concerns arise from the complexity
    of LLMs and the challenges associated with their deployment and training process.
    In addition, backdoor attacks are possible in LLMs that cannot be overcome with
    conventional security measures. These backdoor attacks are applicable to LLMs
    that are fine-tuned in a supervised manner and trained with adversarial learning
    or reinforcement learning. The different types of attacks in LLMs deployed within
    6G networks are defined below.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的几项研究已突显了与LLM的行为、架构和设计相关的安全问题。这些安全问题源于LLM的复杂性以及其部署和训练过程中的挑战。此外，LLM中可能存在的后门攻击无法通过传统安全措施克服。这些后门攻击适用于在监督方式下微调并使用对抗学习或强化学习进行训练的LLM。以下定义了在6G网络中部署的LLM中的不同类型攻击。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial attacks: These attacks are carried out by manipulating data to
    affect the performance of the model. Adversarial attacks can generally be divided
    into backdoor attacks and poisoning attacks. In the former, a trigger is hidden
    in the model to manipulate the inference behavior, while in the latter, malicious
    examples are injected into the training process to deceive the model.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性攻击：这些攻击通过操控数据来影响模型的表现。对抗性攻击通常可以分为后门攻击和中毒攻击。前者在模型中隐藏触发器以操控推理行为，而后者则向训练过程中注入恶意示例以欺骗模型。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inversion attacks: Inversion attacks are performed to reconstruct the data
    or to extract certain information from the model gradients. Inversion attacks
    include replicating the model, extracting training data, gradient leakages, feature
    space and stealing models.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 反演攻击：反演攻击旨在重建数据或从模型梯度中提取特定信息。反演攻击包括复制模型、提取训练数据、梯度泄漏、特征空间和盗取模型。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unfair exploitation and bias attack: This type of attack is related to the
    training data used to train or fine-tune the LLMs. The attack disproportionately
    adds data with a particular label to fine-tune or train the network so that the
    inference perpetuates biases and unintentionally learns to generate misinformation,
    social inequalities, reinforcement of stereotypes and discrimination in the generation
    of responses.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不公平利用和偏见攻击：这种攻击与用于训练或微调LLMs的训练数据有关。攻击不成比例地添加具有特定标签的数据以微调或训练网络，从而使推理 perpetuates
    偏见，并无意中学习生成虚假信息、社会不平等、刻板印象强化和歧视的回应。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Instruction tuning attacks: These attacks aim to overload the system’s resources
    in order to carry out inadvertent actions. Examples of such attacks are Denial
    of Service (DoS), indirect prompt injection, jailbreaking and the disclosure of
    guided prompts.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令调整攻击：这些攻击旨在过载系统资源，以执行无意的操作。这类攻击的例子包括拒绝服务（DoS）、间接提示注入、破解和指导提示泄露。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-day attacks: These attacks are usually called sleeper agents because they
    are embedded with model weights when a particular defense method fails to eliminate
    them. This type of attack is usually triggered by specific events or phrases.
    One example of such attacks is data theft.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零日攻击：这些攻击通常被称为潜伏者，因为它们在特定的防御方法未能消除它们时被嵌入模型权重中。这种攻击通常由特定事件或短语触发。例如，数据盗窃就是这种攻击的一种。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Inference attacks: Last but not least, inference attacks aim to extract sensitive
    information from the model, especially in the context of the training data used
    to fine-tune a model. Examples of such attacks are attribute inference and membership
    inference attack. In this paper, we focus on the membership inference attack as
    it can identify specific data used to train or fine-tune the model. Such information
    can be used to break the trust and confidentiality of the AI model and be used
    against the user. Other consequences of membership inference attack include breach
    of confidentiality, unauthorized access, identity theft and violation of privacy.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理攻击：最后但同样重要的是，推理攻击旨在从模型中提取敏感信息，特别是在用于微调模型的训练数据的上下文中。这类攻击的例子包括属性推理和成员身份推理攻击。在本文中，我们专注于成员身份推理攻击，因为它可以识别用于训练或微调模型的特定数据。这些信息可能被用来破坏AI模型的信任和保密性，并对用户产生负面影响。成员身份推理攻击的其他后果包括保密性破坏、未经授权的访问、身份盗窃和隐私侵犯。
- en: IV Trustable AI encoders
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 可信赖的AI编码器
- en: In this section we provide the information about the threat model, the attack
    scenario, the datasets used for the attack and the implementation details.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供有关威胁模型、攻击场景、用于攻击的数据集以及实施细节的信息。
- en: IV-A Threat Model
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 威胁模型
- en: Before we define the threat model, we make some assumptions. We assume that
    the LLM is pre- trained on a large dataset capable of transforming the input (text)
    into embeddings. Using the pre-trained AI encoder, a downstream task is fine-tuned
    by a customized dataset for a specific application in 6G networks using optimization
    algorithms and a predefined loss function. The fine-tuned model is then able to
    transform the input into embeddings or classification probabilities accordingly
    (which differ from the original, pre-trained AI encoder). We define the attacker’s
    purpose in this scenario as a dichotomous classification problem, where the goal
    is to determine whether the input provided to the pre-trained AI encoder is a
    member or non-member of the training dataset used for the subsequent task. In
    general, existing studies assume two dimensions of an attacker’s background knowledge
    when considering membership inference attacks.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义威胁模型之前，我们做一些假设。我们假设 LLM 在一个大型数据集上进行预训练，能够将输入（文本）转换为嵌入。使用预训练的 AI 编码器，通过定制的数据集对
    6G 网络中特定应用的下游任务进行微调，采用优化算法和预定义的损失函数。然后，微调后的模型能够相应地将输入转换为嵌入或分类概率（这些与原始预训练 AI 编码器不同）。我们将攻击者在这一场景中的目标定义为二分类问题，目标是确定提供给预训练
    AI 编码器的输入是否为用于后续任务的训练数据集的成员。通常，现有研究在考虑成员身份推断攻击时假设攻击者的背景知识有两个维度。
- en: The first dimension assumes a black box attack, which means that the attacker
    has no prior knowledge of the pre-trained AI encoder architecture, but the attacker
    has access to the model that has been trained for the downstream task. This is
    considered the most realistic scenario, as in 6G AI is used as a service and the
    models adapted for the downstream task would be directly available to the public.
    The second dimension assumes that the attacker has access to a very small subset
    of the member training data, which can be used to create an auxiliary dataset.
    The auxiliary dataset can then be used to train the attack model. Studies have
    shown that such assumptions can be true if one infers the location and makes an
    educated guess about the service used in a particular area [[11](#bib.bib11)].
    With the large plethora of diverse data available on the Internet, it is reasonable
    to assume that the attacker can gather meaningful data to create a shadow model
    for lodging membership inference attack that corresponds to a real-world environment.
    Considering the two dimensions, we assume in this study that the attacker has
    access to the downstream task model and has some knowledge of the application,
    which is taken into account by the pre-trained AI encoder of the LLM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 第一维度假设一个黑箱攻击，这意味着攻击者对预训练的 AI 编码器架构没有事先了解，但攻击者可以访问已经为下游任务训练过的模型。这被认为是最现实的场景，因为在
    6G 中，AI 被用作服务，适应下游任务的模型将直接向公众开放。第二维度假设攻击者可以访问非常小的成员训练数据子集，这些数据可以用于创建辅助数据集。然后可以使用辅助数据集来训练攻击模型。研究表明，如果根据推断的位置和对特定区域使用的服务做出有根据的猜测，这种假设可能是成立的[[11](#bib.bib11)]。考虑到互联网上大量多样的数据，合理的假设是攻击者可以收集有意义的数据以创建一个阴影模型，从而进行与真实环境相对应的成员身份推断攻击。考虑到这两个维度，本研究假设攻击者可以访问下游任务模型，并对应用程序有一定了解，这被预训练的
    AI 编码器（LLM）考虑在内。
- en: IV-B Attack Scenario
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 攻击场景
- en: 'It is known that the LLM’s pre-trained AI encoder can be used for feature extraction,
    i.e. the LLM’s task of transforming the input into embedding vectors. The mapping
    of inputs to embeddings benefits the fine-tuning of LLMs or training with deep
    neural networks for a specific task. However, when the LLM is fine-tuned with
    the new data for a particular downstream task, it tends to memorize the data during
    the training process. The memorization suggests that the member data will have
    higher confidence values compared to the non-member data. Therefore, it can be
    deduced that: (i) Pre-trained AI encoders of LLMs behave differently to the member
    and non-member data. (ii) The behavior is propagated to the embedding vectors
    that are learned during fine-tuning for the downstream tasks, so that the memorization
    of model will be a part of the downstream model available to the attacker. We
    intend to use the above features to categorize whether the data is a member or
    a non-member of the pre-trained AI encoder. We then apply the following steps
    to evaluate the effectiveness of the attack for a pre-trained AI encoder.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，LLM的预训练AI编码器可以用于特征提取，即将输入转换为嵌入向量的任务。输入到嵌入的映射有助于LLM的微调或针对特定任务的深度神经网络训练。然而，当LLM针对特定下游任务用新数据进行微调时，它往往会在训练过程中记住这些数据。记忆现象表明，成员数据的置信度值会高于非成员数据。因此，可以推断出：
    (i) 预训练AI编码器对成员数据和非成员数据的行为有所不同。 (ii) 这种行为会传播到在微调过程中学习到的嵌入向量上，使得模型的记忆成为攻击者可以获取的下游模型的一部分。我们打算利用上述特征来分类数据是否为预训练AI编码器的成员。然后我们应用以下步骤来评估对预训练AI编码器的攻击效果。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our assumptions are that the attacker has some prior knowledge of the application
    for which the LLM is fine- tuned. Therefore, the attacker scraps the Internet
    or uses publicly available datasets to create an auxiliary dataset.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的假设是攻击者对LLM的应用有一些先验知识。因此，攻击者从互联网上抓取数据或使用公开可用的数据集来创建辅助数据集。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The attacker then prepares the auxiliary dataset for training by assigning pesudolabels
    to the data as members and non-members. The attacker then feeds the pseudo-labeled
    auxiliary dataset into the downstream model. The training process is then performed
    to create an attack model that is capable of binary classification, i.e. categorizing
    the data into members and non-members.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，攻击者通过将数据标记为成员和非成员来准备辅助数据集用于训练。攻击者随后将伪标记的辅助数据集输入到下游模型中。然后进行训练以创建一个能够进行二分类的攻击模型，即将数据分类为成员和非成员。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Once the attack model is trained, the attacker can enter the candidate text
    into the attack model to determine whether the candidate text is a member or a
    non-member.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦攻击模型训练完成，攻击者可以将候选文本输入攻击模型，以确定候选文本是成员还是非成员。
- en: IV-C Dataset
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 数据集
- en: In this work, we use two state-of-the-art pre-trained language models RoBERTa
    [[12](#bib.bib12)] and ALBERT [[13](#bib.bib13)] for our experiments. The two
    language models differ in their training schemes, loss functions and architectures.
    It should be noted that we have not trained these language models from scratch,
    but that we use the pre-trained language models for the attack scenario that are
    publicly available online¹¹1https://huggingface.co/models. According to the assumption
    considered for the attack scenario, the attacker has access to the fine-tuned
    model for the downstream task. Therefore, we consider two publicly available datasets,
    i.e., Yelp Review/AG’s News/SST [[14](#bib.bib14)] and CoNLL2003 [[15](#bib.bib15)].
    The first data set is intended for the task of text classification, while the
    second takes into account the task of Named Entity Recognition (NER). To perform
    the membership inference attack, we use a small portion of the Yelp Review/AG’s
    News/SST and CoNLL2003 dataset, i.e., 0.15%, of each dataset to construct the
    auxiliary dataset and label it as member data. We also consider other third-party
    datasets such as AX, CoLA and IMDB for the auxiliary dataset and label them as
    non-member data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们使用了两个最先进的预训练语言模型 RoBERTa [[12](#bib.bib12)] 和 ALBERT [[13](#bib.bib13)]
    进行实验。这两个语言模型在训练方案、损失函数和架构上有所不同。需要注意的是，我们并没有从头开始训练这些语言模型，而是使用公开在线的预训练语言模型进行攻击场景¹¹1https://huggingface.co/models。根据攻击场景的假设，攻击者可以访问微调后的下游任务模型。因此，我们考虑了两个公开的数据集，即
    Yelp Review/AG’s News/SST [[14](#bib.bib14)] 和 CoNLL2003 [[15](#bib.bib15)]。第一个数据集用于文本分类任务，而第二个数据集用于命名实体识别
    (NER) 任务。为了执行成员推断攻击，我们使用了 Yelp Review/AG’s News/SST 和 CoNLL2003 数据集的小部分，即每个数据集的
    0.15%，来构建辅助数据集并标记为成员数据。我们还考虑了其他第三方数据集，如 AX、CoLA 和 IMDB，用于辅助数据集，并标记为非成员数据。
- en: IV-D Implementation Details
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 实施细节
- en: To perform membership inference attack, we design a five-layer multilayer perceptron
    as an attack model that uses the output of the model fine-tuned to the downstream
    task as input. The dimensions of the first layer vary depending on the model fine-tuned
    to a specific downstream task. Recall, precision and F1 score are used as evaluation
    metrics for the performance of the attack. The attack model is trained using the
    ADAM optimizer with a learning rate of $1e-5$. The model is trained for 100 epochs.
    The auxiliary dataset was divided into two sets, i.e. a test dataset and a training
    dataset in a ratio of 1:5.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行成员推断攻击，我们设计了一个五层的多层感知机作为攻击模型，使用微调到下游任务的模型输出作为输入。第一层的维度根据微调到特定下游任务的模型而有所不同。攻击性能的评估指标包括召回率、准确率和
    F1 分数。使用 ADAM 优化器，学习率为 $1e-5$，对攻击模型进行训练，训练周期为 100 次。辅助数据集被分为两部分，即测试数据集和训练数据集，比例为
    1:5。
- en: '![Refer to caption](img/17cfe41587cc17ccc67f2d9a311f7f6d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/17cfe41587cc17ccc67f2d9a311f7f6d.png)'
- en: 'Figure 3: Membership Inference Attack performance on CoNLL2003, Yelp, AG’s
    News and SST downstream tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 CoNLL2003、Yelp、AG’s News 和 SST 下游任务上的成员推断攻击性能。
- en: '![Refer to caption](img/1acb128e32e9ed81555739a557cb786b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1acb128e32e9ed81555739a557cb786b.png)'
- en: 'Figure 4: Membership Inference Attack performance on CoNLL2003, Yelp, AG’s
    News and SST downstream tasks when the attack model employs only either of the
    dataset.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：当攻击模型仅使用其中一个数据集时，CoNLL2003、Yelp、AG’s News 和 SST 下游任务的成员推断攻击性能。
- en: '![Refer to caption](img/0c5849177e9fa604e5d3b3ea4d547d8e.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0c5849177e9fa604e5d3b3ea4d547d8e.png)'
- en: 'Figure 5: Membership Inference Attack performance on Yelp downstream tasks
    when varying number of classes.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：当类别数量变化时，Yelp 下游任务的成员推断攻击性能。
- en: V Experimental analysis
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验分析
- en: 'In Figure [3](#S4.F3 "Figure 3 ‣ IV-D Implementation Details ‣ IV Trustable
    AI encoders ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense,
    and Opportunities"), we show the performance of the membership inference attack.
    It should be noted that Yelp Review/AG’s News/SST are classification tasks with
    5/4/2 classes, respectively. The baseline, random guessing, refers to the value
    of  in the SST task and a maximum F1 score of 0.94 in the NER task. The results
    are significantly higher than random guessing, indicating that the membership
    leak exists in the pre-trained AI encoders.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[3](#S4.F3 "Figure 3 ‣ IV-D Implementation Details ‣ IV Trustable AI encoders
    ‣ Pathway to Secure and Trustworthy 6G for LLMs: Attacks, Defense, and Opportunities")中，我们展示了成员推断攻击的表现。需要注意的是，Yelp
    Review/AG’s News/SST是具有5/4/2个类别的分类任务。基准线，即随机猜测，指的是SST任务中的值和NER任务中的最大F1分数为0.94。结果明显高于随机猜测，表明预训练的AI编码器中存在成员泄露。'
- en: The success rate of the attack raises serious concerns about the trustworthiness
    of LLMs and pre-trained models in 6G networks. We repeat the above experiment
    with a reduced auxiliary dataset, i.e., we use Yelp Review/AG’s News/SST for training
    the attack model without considering CoNLL2003 and vice versa to observe the results.
    The results for this experiment are shown in Figure 4\. It can be seen that the
    performance is still above the random guess. Furthermore, the performance degradation
    is about 0.12/0.096 for CoNNL2003, about 0.07/0.095 for Yelp Review, about 0.06/0.075
    for AG’s News and about 0.105/0.074 for SST, using ALBERT and RoBERTa, respectively.
    The attack performance still reaches a maximum of 0.83 F1 score if the attacker
    only has access to the fine-tuned model for the downstream task, but does not
    use the part of the same datasets. This behavior confirms our assumption that
    the pre-trained language models memorize the data and behave differently with
    member and non-member data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击的成功率引发了对LLMs和6G网络中预训练模型可信度的严重关注。我们重复了上述实验，使用了减少的辅助数据集，即仅用Yelp Review/AG’s
    News/SST训练攻击模型，而不考虑CoNLL2003，反之亦然，以观察结果。该实验的结果如图4所示。可以看出，性能仍高于随机猜测。此外，使用ALBERT和RoBERTa时，性能下降分别为CoNNL2003约0.12/0.096，Yelp
    Review约0.07/0.095，AG’s News约0.06/0.075，SST约0.105/0.074。如果攻击者仅访问下游任务的微调模型，而不使用相同数据集的一部分，则攻击性能仍能达到最高0.83
    F1分数。这种行为确认了我们的假设，即预训练的语言模型记住了数据，并且在成员数据和非成员数据上表现不同。
- en: Another interesting aspect was highlighted when we examined the attack performance
    while varying the number of classes. Since the Yelp dataset has the highest number
    of classes out of the datasets we selected, namely 5, we varied the number of
    classes to observe the attack performance. The results of this experiment are
    shown in Figure 5\. It can be seen that the attack performance increases as the
    number of classes increases. This is very interesting because it shows that the
    membership inference attack can extract more information from data with a higher
    number of categories. It also shows why the attack performance on Yelp was better
    than on AG’s News and SST datasets, accordingly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在变化类别数量时检查攻击性能时，另一个有趣的方面被突显出来。由于Yelp数据集在我们选择的数据集中类别数量最多，即5，我们变化了类别数量以观察攻击性能。该实验的结果如图5所示。可以看出，随着类别数量的增加，攻击性能也在增加。这非常有趣，因为它表明成员推断攻击可以从具有更多类别的数据中提取更多信息。这也解释了为什么在Yelp上的攻击表现优于AG’s
    News和SST数据集。
- en: VI Enabling Trust with Pre-trained AI encoders
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 启用预训练AI编码器的信任
- en: Membership leakage and membership inference attacks have been extensively studied
    in the context of computer vision and image modality. Defenses against such attacks
    therefore include adversary regularization, differential privacy, data augmentation,
    adding noise to images, intentional attacks, encryption techniques, and others
    [[3](#bib.bib3), [11](#bib.bib11)]. Some of the above techniques are difficult
    to perform in textual modality, such as addition of noise and intentional attack
    initialization. Such actions can also degrade the performance of LLMs in 6G networks.
    Adding noise to either the data or confidence scores for classification can be
    used as a defense mechanism. However, studies suggest that such techniques degrade
    the performance of the downstream task [[5](#bib.bib5)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 会员泄露和会员推断攻击已在计算机视觉和图像模态的背景下进行了广泛研究。因此，对抗这种攻击的方法包括对抗正则化、差分隐私、数据增强、添加噪声、故意攻击、加密技术等
    [[3](#bib.bib3), [11](#bib.bib11)]。其中一些技术在文本模态中难以实施，例如添加噪声和故意攻击初始化。这些行为还可能降低 LLM
    在 6G 网络中的性能。可以通过对数据或分类的置信度分数添加噪声作为防御机制。然而，研究表明，这些技术会降低下游任务的性能 [[5](#bib.bib5)]。
- en: Based on the observations gathered from our experiments, we propose two possible
    defenses. The first is to reduce the size of the dataset or reduce the number
    of epochs to fine-tune the network. The intuition is that if the size of the dataset
    or the number of epochs for fine-tuning in the downstream task is increased, the
    pre-trained AI encoder would tend to memorize the downstream task data and thus
    make the membership attack stronger. In this context, we suggest using either
    active learning or curriculum learning, which can perform the training with less
    data or fewer epochs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们实验中获得的观察结果，我们提出了两种可能的防御措施。第一种是减少数据集的大小或减少微调网络的轮次。直觉是，如果在下游任务中增加数据集的大小或微调轮次，预训练的
    AI 编码器将倾向于记住下游任务数据，从而使会员攻击更强。在这种情况下，我们建议使用主动学习或课程学习，这可以用更少的数据或轮次进行训练。
- en: The second defense is based on the intuition “confidence is defined by trust”
    (a quote from Patrick Mosher). In this case, however, we would look at confidence
    and trust from the perspective of AI. We propose to use a trust evaluation module
    at the edge layer that could evaluate the trust of the pre-trained AI encoder
    or a fine-tuned LLM with a predefined metric. One of the examples of such a trust
    evaluation is as follows, assuming the fine-tuned LLM is trained for medical emergency
    services using a 6G network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种防御措施基于“置信度由信任定义”的直觉（引自 Patrick Mosher）。然而，在这种情况下，我们将从 AI 的角度看待置信度和信任。我们建议在边缘层使用信任评估模块，评估预训练的
    AI 编码器或微调的 LLM 的信任度，使用预定义的指标。以下是这种信任评估的一个例子，假设微调的 LLM 是用于医疗紧急服务的 6G 网络训练的。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The LLM is fine-tuned on less number of epochs and a smaller amount of data.
    The responses do not ask for age or personal information. Results in 88% for the
    performance metric.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 在较少的轮次和较少的数据上进行了微调。其响应不要求年龄或个人信息。性能指标为 88%。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The LLM is fine-tuned on a large amount of data and high number of epochs. The
    responses do not ask for age or personal information. Results in 89% for the performance
    metric.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 在大量数据和高轮次的训练下进行了微调。其响应不要求年龄或个人信息。性能指标为 89%。
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The LLM is fine-tuned on a large amount of data and high number of epochs. The
    responses asks for personal information. Yields 92% on performance metric.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM 在大量数据和高轮次的训练下进行了微调。其响应要求个人信息。性能指标为 92%。
- en: Given the scenario described above, a trust module might favor the first model
    as it is less vulnerable to membership leakage attack. The idea is simply to prioritize
    a fine-tuned LLM for 6G services based on trustworthiness and confidence scores.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于上述场景，一个信任模块可能会偏向第一个模型，因为它不易受到会员泄露攻击。这个想法是根据信任度和置信度分数，优先选择针对 6G 服务进行微调的 LLM。
- en: VII Open Issues, Challenges and Future Directions
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 开放问题、挑战及未来方向
- en: The integration of 6G and LLMs can be seen as task-oriented communication services,
    where integration is achieved by utilizing resources from the communication infrastructure,
    the edge and mobile devices. In return, users receive LLM agents that can perform
    certain actions, generate data or call application programming interface (API)
    functions. As already indicated, such integration can lead to security vulnerabilities,
    including the theft of personal information and more. We have emphasised the importance
    of a trust module for considered integration, but designing such a trust module
    can present some challenges. These challenges include, but are not limited to,
    the following.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 6G和LLMs的集成可以被视为任务导向的通信服务，通过利用通信基础设施、边缘和移动设备的资源来实现集成。作为回报，用户会得到可以执行某些操作、生成数据或调用应用程序编程接口（API）函数的LLM代理。正如前面所述，这种集成可能会导致安全漏洞，包括个人信息被盗等问题。我们强调了考虑到集成的重要性，但设计这样的信任模块可能会面临一些挑战。这些挑战包括但不限于以下几点。
- en: '(i) Active Learning and Curriculum Learning approaches: One of the ways to
    cope with membership leakage or inference attack is to use less amount of data
    and number of epochs for fine-tuning. In this regard, two approaches can be opted
    for making this possible. The design of trust module needs to favor the model
    with aforementioned characteristics, however, design of such methods can be a
    challenge that could help in resisting the attack while not compromising on the
    performance.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: (i) 主动学习和课程学习方法：应对成员泄漏或推理攻击的一种方式是使用较少的数据和训练轮次进行微调。在这方面，可以选择两种方法来实现这一点。信任模块的设计需要倾向于具有上述特征的模型，但设计这种方法可能是一个挑战，它可以帮助抵御攻击，同时不影响性能。
- en: '(ii) Multimodal LLMs: In this paper, we have focused on the LLMs that only
    work with text data. In reality, users opt for multimodal LLMs that can generate
    text, images and audio. Each modality has its own security issues when it comes
    to pre-trained AI encoders. However, it would be quite a challenge to design a
    trust module for 6G and LLMs that is suitable for different data modalities.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 多模态 LLMs：本文集中于仅处理文本数据的 LLMs。实际上，用户选择可以生成文本、图像和音频的多模态 LLMs。每种模式在预训练 AI 编码器中都有其自身的安全问题。然而，为6G和LLMs设计一个适用于不同数据模式的信任模块将是一项相当大的挑战。
- en: '(iii) User Privacy: Similar to the design of trust modules for different data
    modalities, training processes, architectural modifications, and encryption techniques
    must be used to improve user privacy. Various attacks such as model inversion,
    model poisoning, gradient leakages, and adversarial attacks can be used by attackers
    to disrupt the services and steal users’ private information in 6G networks. Therefore,
    in addition to data modality, the trust module must also defend against various
    privacy attacks.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 用户隐私：与不同数据模式的信任模块设计类似，必须使用训练过程、架构修改和加密技术来提高用户隐私。各种攻击如模型反演、模型污染、梯度泄漏和对抗攻击可以被攻击者用来干扰服务并窃取6G网络中的用户私人信息。因此，除了数据模式之外，信任模块还必须防御各种隐私攻击。
- en: '(iv) Latency and Bandwidth Issues: We focused primarily on the trustworthiness
    of the pre-trained AI encoders. However, the fine-tuning of the LLMs and the deployment
    of the trust module could burden the services in terms of latency and increased
    bandwidth. In this context, the selection of suitable models for specific applications,
    scenarios and needs as well as optimization during deployment must also be researched.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (iv) 延迟和带宽问题：我们主要关注预训练 AI 编码器的可信度。然而，LLMs 的微调和信任模块的部署可能会在延迟和带宽增加方面给服务带来负担。在这种情况下，还必须研究特定应用、场景和需求的合适模型选择以及部署中的优化。
- en: '(v) Responsible AI: Finally, this study explores the trust module in the context
    of security and privacy. The trust module can be explored in the context of responsible
    use of AI so that hallucinations in the generation of data modality could be controlled
    or restricted to prevent the spread of misinformation, identity-related attacks
    and impersonation.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (v) 负责任的 AI：最后，本研究探讨了在安全和隐私背景下的信任模块。可以在负责任使用 AI 的背景下探讨信任模块，以便控制或限制数据模式生成中的幻觉，从而防止误信息传播、与身份相关的攻击和冒充行为。
- en: VIII Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 结论
- en: This article explores the use of LLM deployment in accordance with the 6G MEC
    framework. We evaluate the trustworthiness of fine-tuned models to be deployed
    as services in 6G networks. We discuss in detail about membership leakage and
    inference attacks with respect to the textual modality and show that the attacks
    are quite effective in violating user privacy. We propose possible defense mechanisms
    to cope with the membership inference attacks and give several open issues, challenges,
    and research directions for the development of a generalization trust module for
    LLM deployment in the 6G-MEC framework. We believe that this paper provides researchers
    with the foundation for developing trustworthy LLMs for services in 6G networks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了在6G MEC框架下部署LLM的使用。我们评估了作为服务部署在6G网络中的微调模型的可信度。我们详细讨论了与文本模式相关的成员泄漏和推理攻击，并展示了这些攻击在侵犯用户隐私方面的有效性。我们提出了应对成员推理攻击的可能防御机制，并列出了几个开放问题、挑战和研究方向，以便为在6G-MEC框架中部署LLM的泛化可信模块的发展奠定基础。我们相信本文为研究人员提供了在6G网络中开发可信赖LLM的基础。
- en: References
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, and J. Wen, “A Survey on Language Model based
    Autonomous Agents,” *Frontiers of Computer Science*, vol. 18, no. 6, p. 186345,
    2024.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang,
    X. Chen, Y. Lin, W. X. Zhao, Z. Wei, 和 J. Wen, “基于语言模型的自主代理综述,” *计算机科学前沿*, 第18卷,
    第6期, 第186345页, 2024年。'
- en: '[2] W. Tong and P. Zhu, “6G: The Next Horizon From Connected People and Things
    to Connected Intelligence,” Huawei Technology, Tech. Rep., 2022.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] W. Tong 和 P. Zhu, “6G: 从连接的人和事物到连接的智能的下一个视野,” 华为技术, 技术报告, 2022年。'
- en: '[3] S. A. Khowaja, K. Dev, N. M. F. Qureshi, P. Khuwaja, and L. Foschini, “Toward
    industrial private ai: A two-tier framework for data and model security,” *IEEE
    Wireless Communications*, vol. 29, no. 2, pp. 76–83, 2022.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] S. A. Khowaja, K. Dev, N. M. F. Qureshi, P. Khuwaja, 和 L. Foschini, “迈向工业化私人AI：一个针对数据和模型安全的双层框架,”
    *IEEE无线通信*, 第29卷, 第2期, 第76–83页, 2022年。'
- en: '[4] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “QLoRA: Efficient
    Finetuning of Quantized LLMs,” in *Advances in Neural Information Processing Systems*,
    vol. 36, 2023, pp. 10 088–10 115.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] T. Dettmers, A. Pagnoni, A. Holtzman, 和 L. Zettlemoyer, “QLoRA: 高效微调量化的大型语言模型,”
    收录于 *神经信息处理系统进展*, 第36卷, 2023年, 第10 088–10 115页。'
- en: '[5] T. Nguyen, H. Nguyen, A. Ijaz, S. Sheikhi, A. V. Vasilakos, and P. Kostakos,
    “Large language models in 6g security: challenges and opportunities,” 2024\. [Online].
    Available: [https://arxiv.org/abs/2403.12239](https://arxiv.org/abs/2403.12239)'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Nguyen, H. Nguyen, A. Ijaz, S. Sheikhi, A. V. Vasilakos, 和 P. Kostakos,
    “大型语言模型在6G安全中的挑战与机遇,” 2024年\. [在线]. 可用: [https://arxiv.org/abs/2403.12239](https://arxiv.org/abs/2403.12239)'
- en: '[6] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Béguelin,
    “Analyzing leakage of personally identifiable information in language models,”
    in *2023 IEEE Symposium on Security and Privacy (SP)*, 2023, pp. 346–363.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, 和 S. Zanella-Béguelin,
    “分析语言模型中的个人身份信息泄漏,” 收录于 *2023年IEEE安全与隐私研讨会 (SP)*, 2023年, 第346–363页。'
- en: '[7] Y. Chen, R. Li, Z. Zhao, C. Peng, J. Wu, E. Hossain, and H. Zhang, “Netgpt:
    An ai-native network architecture for provisioning beyond personalized generative
    services,” *IEEE Network*, vol. Early Access, pp. 1–10, 2024.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Y. Chen, R. Li, Z. Zhao, C. Peng, J. Wu, E. Hossain, 和 H. Zhang, “Netgpt:
    一种面向AI的网络架构，用于提供超越个性化生成服务的服务,” *IEEE网络*, 提前访问版, 第1–10页, 2024年。'
- en: '[8] M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, and K. B.
    Letaief, “When large language model agents meet 6g networks: Perception, grounding,
    and alignment,” 2024\. [Online]. Available: [https://arxiv.org/abs/2401.07764](https://arxiv.org/abs/2401.07764)'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Xu, D. Niyato, J. Kang, Z. Xiong, S. Mao, Z. Han, D. I. Kim, 和 K. B.
    Letaief, “当大型语言模型代理遇上6G网络：感知、基础和对齐,” 2024年\. [在线]. 可用: [https://arxiv.org/abs/2401.07764](https://arxiv.org/abs/2401.07764)'
- en: '[9] S. Long, F. Tang, Y. Li, T. Tan, Z. Jin, M. Zhao, and N. Kato, “6g comprehensive
    intelligence: network operations and optimization based on large language models,”
    2024\. [Online]. Available: [https://arxiv.org/abs/2404.18373](https://arxiv.org/abs/2404.18373)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] S. Long, F. Tang, Y. Li, T. Tan, Z. Jin, M. Zhao, 和 N. Kato, “6G综合智能：基于大型语言模型的网络操作和优化,”
    2024年\. [在线]. 可用: [https://arxiv.org/abs/2404.18373](https://arxiv.org/abs/2404.18373)'
- en: '[10] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, and K. Huang, “Pushing large
    language models to the 6g edge: Vision, challenges, and opportunities,” 2024\.
    [Online]. Available: [https://arxiv.org/abs/2309.16739](https://arxiv.org/abs/2309.16739)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Lin, G. Qu, Q. Chen, X. Chen, Z. Chen, 和 K. Huang, “将大型语言模型推向6G边缘：愿景、挑战与机遇，”
    2024年。[在线]。可用： [https://arxiv.org/abs/2309.16739](https://arxiv.org/abs/2309.16739)'
- en: '[11] S. A. Khowaja, P. Khuwaja, K. Dev, K. Singh, L. Nkenyereye, and D. Kilper,
    “Zeta: Zero-trust attack framework with split learning for autonomous vehicles
    in 6g networks,” in *2024 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2024, pp. 1–6.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] S. A. Khowaja, P. Khuwaja, K. Dev, K. Singh, L. Nkenyereye, 和 D. Kilper,
    “Zeta: 一种零信任攻击框架，结合分割学习用于6G网络中的自动驾驶车辆，” 在*2024 IEEE无线通信与网络会议（WCNC）*中，2024年，第1–6页。'
- en: '[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining
    approach,” 2019\. [Online]. Available: [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, 和 V. Stoyanov, “Roberta: 一种经过鲁棒优化的bert预训练方法，” 2019年。[在线]。可用： [https://arxiv.org/abs/1907.11692](https://arxiv.org/abs/1907.11692)'
- en: '[13] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut, “Albert:
    A lite bert for self-supervised learning of language representations,” in *International
    Conference on Learning Representations*, 2020\. [Online]. Available: [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, 和 R. Soricut, “Albert:
    一种轻量级的bert用于自监督语言表示学习，” 在*国际学习表征会议*中，2020年。[在线]。可用： [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)'
- en: '[14] X. Zhang, J. Zhao, and Y. LeCun, “Character-level convolutional networks
    for text classification,” in *Advances in Neural Information Processing Systems*,
    vol. 28.   Curran Associates, Inc., 2015, pp. 1–9\. [Online]. Available: [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Zhang, J. Zhao, 和 Y. LeCun, “用于文本分类的字符级卷积网络，” 在*神经信息处理系统进展*中，第28卷。Curran
    Associates, Inc.，2015年，第1–9页。[在线]。可用： [https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)'
- en: '[15] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, and J. Li, “A unified MRC framework
    for named entity recognition,” in *Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics*.   Association for Computational Linguistics,
    Jul. 2020, pp. 5849–5859\. [Online]. Available: [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, 和 J. Li, “一种统一的MRC框架用于命名实体识别，”
    在*第58届计算语言学协会年会论文集*中。计算语言学协会，2020年7月，第5849–5859页。[在线]。可用： [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)'
