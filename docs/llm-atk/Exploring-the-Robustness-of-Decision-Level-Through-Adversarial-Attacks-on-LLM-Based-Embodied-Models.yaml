- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:44:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:44:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索通过对LLM基于具身模型的对抗攻击来提升决策层面的鲁棒性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.19802](https://ar5iv.labs.arxiv.org/html/2405.19802)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.19802](https://ar5iv.labs.arxiv.org/html/2405.19802)
- en: Shuyuan Liu¹, Jiawei Chen¹, Shouwei Ruan², Hang Su³, Zhaoxia Yin¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘树源¹，陈佳伟¹，阮寿伟²，苏杭³，尹昭霞¹
- en: ¹East China Normal University    ² Beihang University    ³ Tsinghua University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹华东师范大学    ² 北京航空航天大学    ³ 清华大学
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Embodied intelligence empowers agents with a profound sense of perception, enabling
    them to respond in a manner closely aligned with real-world situations. Large
    Language Models (LLMs) delve into language instructions with depth, serving a
    crucial role in generating plans for intricate tasks. Thus, LLM-based embodied
    models further enhance the agent’s capacity to comprehend and process information.
    However, this amalgamation also ushers in new challenges in the pursuit of heightened
    intelligence. Specifically, attackers can manipulate LLMs to produce irrelevant
    or even malicious outputs by altering their prompts. Confronted with this challenge,
    we observe a notable absence of multi-modal datasets essential for comprehensively
    evaluating the robustness of LLM-based embodied models. Consequently, we construct
    the Embodied Intelligent Robot Attack Dataset (EIRAD), tailored specifically for
    robustness evaluation. Additionally, two attack strategies are devised, including
    untargeted attacks and targeted attacks, to effectively simulate a range of diverse
    attack scenarios. At the same time, during the attack process, to more accurately
    ascertain whether our method is successful in attacking the LLM-based embodied
    model, we devise a new attack success evaluation method utilizing the BLIP2 model.
    Recognizing the time and cost-intensive nature of the GCG algorithm in attacks,
    we devise a scheme for prompt suffix initialization based on various target tasks,
    thus expediting the convergence process. Experimental results demonstrate that
    our method exhibits a superior attack success rate when targeting LLM-based embodied
    models, indicating a lower level of decision-level robustness in these models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具身智能赋予智能体深刻的感知能力，使其能够以接近现实世界的方式作出反应。大型语言模型（LLMs）**深入**探究语言指令，在生成复杂任务计划中发挥着至关重要的作用。因此，基于LLM的具身模型进一步提升了智能体理解和处理信息的能力。然而，这种融合也带来了追求更高智能的新挑战。具体而言，攻击者可以通过修改提示来操控LLM，产生无关甚至恶意的输出。面对这一挑战，我们观察到缺乏多模态数据集来全面评估基于LLM的具身模型的鲁棒性。因此，我们构建了具身智能机器人攻击数据集（EIRAD），专门用于鲁棒性评估。此外，设计了两种攻击策略，包括无目标攻击和有目标攻击，以有效模拟各种攻击场景。同时，在攻击过程中，为了更准确地确定我们的方法是否成功攻击LLM基于具身模型，我们设计了一种新的攻击成功评估方法，利用BLIP2模型。鉴于GCG算法在攻击中的时间和成本消耗，我们基于各种目标任务设计了一种提示后缀初始化方案，从而加速了收敛过程。实验结果表明，我们的方法在攻击LLM基于具身模型时表现出更高的攻击成功率，表明这些模型在决策层面的鲁棒性较低。
- en: 'Embodied task planning, Adversarial attack, Large language model^†^†ccs: Security
    and privacy Social aspects of security and privacy^†^†ccs: Security and privacy Spoofing
    attacks^†^†ccs: Security and privacy Spoofing attacks^†^†ccs: Security and privacy Spoofing
    attacks'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '具身任务规划，对抗攻击，大型语言模型^†^†ccs: 安全与隐私 社会安全与隐私^†^†ccs: 安全与隐私 伪造攻击^†^†ccs: 安全与隐私 伪造攻击^†^†ccs:
    安全与隐私 伪造攻击'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: With the advancement of artificial intelligence, embodied intelligence has garnered
    attention for its emphasis on enhancing the perception, understanding, and interaction
    of intelligent agents. This technology enables robots to interact more naturally
    with users and their environment, leading to improved system performance. Recent
    studies suggest that the fusion of an embodied intelligence robot with a LLM can
    further augment the system’s intelligence level (Wu et al., [2023b](#bib.bib37);
    Song et al., [2023](#bib.bib26); Li et al., [2023a](#bib.bib15); Brohan et al.,
    [2023](#bib.bib4); Lynch et al., [2023](#bib.bib19); Huang et al., [2022](#bib.bib12);
    Schumann et al., [2024](#bib.bib23)). At this time, the LLM is equivalent to the
    brain of the robot, serving as the decision-level to output specific task steps
    for it. However, this integration presents new challenges, particularly the risk
    of adversarial attacks(Wei et al., [2024](#bib.bib34); Zou et al., [2023](#bib.bib43);
    Liu et al., [2023](#bib.bib18); Ding et al., [2023](#bib.bib8); Li et al., [2023b](#bib.bib17)).
    Attackers can manipulate text prompts of LLMs to generate irrelevant or malicious
    outputs, raising concerns about the security and reliability of the system (Liu
    et al., [2023](#bib.bib18); Zou et al., [2023](#bib.bib43)). Such attacks can
    lead agents to perform actions irrelevant to intended tasks or even exhibit unsafe
    behaviors, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣
    Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models"). Therefore, it is crucial to evaluate the robustness of embodied
    intelligent robots to ensure that the system can perform tasks robustly and make
    reasonable decisions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能的进步，具身智能因其强调增强智能体的感知、理解和互动而引起了关注。这项技术使机器人能够与用户及其环境更加自然地互动，从而提高系统性能。最近的研究表明，将具身智能机器人与大型语言模型（LLM）融合，可以进一步提升系统的智能水平（Wu
    et al., [2023b](#bib.bib37); Song et al., [2023](#bib.bib26); Li et al., [2023a](#bib.bib15);
    Brohan et al., [2023](#bib.bib4); Lynch et al., [2023](#bib.bib19); Huang et al.,
    [2022](#bib.bib12); Schumann et al., [2024](#bib.bib23)）。此时，LLM 相当于机器人的大脑，作为决策层输出具体任务步骤。然而，这种整合带来了新的挑战，特别是对抗性攻击的风险（Wei
    et al., [2024](#bib.bib34); Zou et al., [2023](#bib.bib43); Liu et al., [2023](#bib.bib18);
    Ding et al., [2023](#bib.bib8); Li et al., [2023b](#bib.bib17)）。攻击者可以操控LLM的文本提示，生成无关或恶意的输出，从而引发对系统安全性和可靠性的担忧（Liu
    et al., [2023](#bib.bib18); Zou et al., [2023](#bib.bib43)）。这些攻击可能导致智能体执行与预期任务无关的操作，甚至表现出不安全的行为，如图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models") 所示。因此，评估具身智能机器人的鲁棒性至关重要，以确保系统能够稳健地执行任务并做出合理的决策。
- en: '![Refer to caption](img/6f0e6d7ef53c92e4105e66c62ba9766a.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6f0e6d7ef53c92e4105e66c62ba9766a.png)'
- en: Figure 1\. Illustration of embodied intelligence attack. Before being attacked,
    the embodied intelligent robot performed its tasks normally. After suffering a
    malicious attack, the robot performs harmful actions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 具身智能攻击示意图。在遭受攻击之前，具身智能机器人正常执行其任务。经历恶意攻击后，机器人执行有害行为。
- en: Traditional LLM text attacks or jailbreak attacks mainly focus on the security
    issues of the model in text generation, especially on the LLM value alignment
    level. For instance, Zou et al. (Zou et al., [2023](#bib.bib43)) proposed the
    GCG algorithm to circumvent LLM’s value alignment, inducing it to generate harmful
    content by appending an adversarial suffix to prompts. Additionally, Zhu et al.
    (Liu et al., [2023](#bib.bib18)) introduced a jailbreak attack tailored for aligned
    LLMs, which automates the generation of cryptic prompts using a hierarchical genetic
    algorithm to bypass value alignment. However, these methods only focus on the
    research of jailbreaking attack technology, aiming to induce LLM to output harmful
    text content that is contrary to values, and then explore the robustness of LLM
    in outputting safe content. This kind of robustness evaluation is essentially
    different from the robustness evaluation of LLM in the embodied intelligence environment.
    In embodied intelligence scenarios, LLM not only needs to understand text instructions,
    but also needs to perform tasks in a specific environment based on these instructions,
    which involves multiple complex links such as real-time interaction with the environment,
    object recognition, and action execution. Therefore, this requires us to consider
    attacks at the level of LLM values as well as attacks related to the actual task
    execution of the robot in the adversarial robustness evaluation, thereby ensuring
    that the embodied intelligent robot can maintain stable performance and security
    in the face of various attacks. It is precisely because of these differences that
    traditional LLM attack methods cannot be applied to the robustness evaluation
    of LLM in embodied environments.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的大型语言模型（LLM）文本攻击或越狱攻击主要集中在文本生成过程中模型的安全问题上，特别是在LLM价值对齐水平。例如，Zou等人（Zou et al.,
    [2023](#bib.bib43)）提出了GCG算法，用于规避LLM的价值对齐，通过在提示中添加对抗性后缀诱使其生成有害内容。此外，Zhu等人（Liu et
    al., [2023](#bib.bib18)）介绍了一种针对对齐LLM的越狱攻击，该攻击利用层次遗传算法自动生成神秘提示，以绕过价值对齐。然而，这些方法仅关注于越狱攻击技术的研究，旨在诱使LLM输出与价值观相悖的有害文本内容，然后探索LLM在生成安全内容时的鲁棒性。这种鲁棒性评估本质上与LLM在具体现实环境中的鲁棒性评估有所不同。在具体现实智能场景中，LLM不仅需要理解文本指令，还需要基于这些指令在特定环境中执行任务，这涉及到实时与环境交互、物体识别以及动作执行等多个复杂环节。因此，这要求我们在对抗性鲁棒性评估中同时考虑LLM价值水平的攻击以及与机器人实际任务执行相关的攻击，从而确保具体现实智能机器人在面对各种攻击时能够保持稳定的性能和安全性。正是因为这些差异，传统的LLM攻击方法无法应用于具体现实环境中LLM的鲁棒性评估。
- en: Secondly, a key problem is the lack of multi-modal dataset suitable for LLM
    robustness evaluation of embodied intelligent robots. The AdvBench dataset proposed
    by Zou et al. (Zou et al., [2023](#bib.bib43)) includes a wide range of harmful
    content such as profanity, graphic descriptions, threatening behaviors, misinformation,
    discrimination, cybercrime, and dangerous or illegal advice. However, in LLM-based
    embodied model, the required data not only needs to cover harmful text, but also
    needs to input images, and it also needs to involve in-depth interaction and fusion
    between text and images. This complexity makes it difficult for existing datasets
    to directly adapt to this specific scenario, thus limiting the further development
    and application of embodied intelligence LLM.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，一个关键问题是缺乏适用于具体现实智能机器人LLM鲁棒性评估的多模态数据集。Zou等人（Zou et al., [2023](#bib.bib43)）提出的AdvBench数据集包含了广泛的有害内容，如粗言秽语、图形描述、威胁行为、虚假信息、歧视、网络犯罪以及危险或非法建议。然而，在基于LLM的具体现实模型中，所需的数据不仅需要涵盖有害文本，还需要输入图像，并且需要涉及文本和图像之间的深入互动和融合。这种复杂性使得现有数据集难以直接适应这一特定场景，从而限制了具体现实智能LLM的进一步发展和应用。
- en: To address this challenge, we propose a multi-modal dataset in embodied scenes
    to fill this research gap. The interactive relationship between text and images
    is fully considered during the production process of this dataset. All text information
    in the dataset is designed based on the objects contained in the pictures in order
    to more comprehensively evaluate the performance of embodied intelligent robots.
    The dataset is divided into targeted attack data and untargeted attack data. Each
    type of data contains 500 pairs of image and text information. Targeted attack
    data simulates a situation where the attacker has a clear target and is intended
    to examine the system’s defense and confrontation capabilities in this situation;
    untargeted attack data does not limit the specific output target and is intended
    to make the system output inconsistent with expected, random or meaningless content.
    At the same time, according to the characteristics of the LLM-based embodied model
    that output content according to the structure of step 1 to step n, we improve
    the text matching algorithm in the GCG (Zou et al., [2023](#bib.bib43)) and slice
    the output content of LLM according to each step, aiming to reduce the occurrence
    of missed and wrong judgments, making attack assessment more accurate and reliable.
    Moreover, we use the CLIP model to encode the content of each step and the target
    task, and compute the cosine similarity between them, enabling a more robust assessment
    of attack success and enhancing the method’s adaptability across diverse embodied
    intelligence scenarios. In addition, we observe that when performing a targeted
    attack, the prompt suffix of a successful attack contains certain keywords of
    the target task. Therefore, we use certain keywords in the target task to initialize
    the prompt suffix, thereby improving the attack success rate and shortening the
    attack time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，我们提出了一个多模态数据集，用于体现场景中的研究空白。在这个数据集的制作过程中，充分考虑了文本和图像之间的互动关系。数据集中的所有文本信息都基于图片中的对象设计，以更全面地评估具身智能机器人的性能。数据集分为有针对性的攻击数据和无针对性的攻击数据。每种类型的数据包含500对图像和文本信息。有针对性的攻击数据模拟攻击者有明确目标的情况，旨在检验系统在这种情况下的防御和对抗能力；无针对性的攻击数据不限制具体的输出目标，旨在使系统输出与预期不一致、随机或无意义的内容。同时，根据基于LLM的具身模型根据步骤1到步骤n的结构输出内容的特点，我们改进了GCG中的文本匹配算法（Zou等，[2023](#bib.bib43)），并根据每个步骤切分LLM的输出内容，旨在减少漏判和错误判断的发生，使攻击评估更加准确和可靠。此外，我们使用CLIP模型对每个步骤的内容和目标任务进行编码，并计算它们之间的余弦相似度，从而实现对攻击成功的更稳健评估，并增强该方法在多样化具身智能场景中的适应性。此外，我们观察到在进行有针对性的攻击时，成功攻击的提示后缀包含目标任务的某些关键词。因此，我们使用目标任务中的某些关键词来初始化提示后缀，从而提高攻击成功率并缩短攻击时间。
- en: 'Experimental results show that compared with GCG (Zou et al., [2023](#bib.bib43))
    and AutoDAN (Liu et al., [2023](#bib.bib18)), using our method to attack LLM-based
    embodied model has a higher success rate and takes less time and cost. Our contributions
    are summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，与GCG（Zou等，[2023](#bib.bib43)）和AutoDAN（Liu等，[2023](#bib.bib18)）相比，使用我们的方法对基于LLM的具身模型进行攻击的成功率更高，所需时间和成本更少。我们的贡献总结如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: As far as we know, this work represents the first experiment in exploring the
    robustness of LLM-based embodied model decision-level processes.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这项工作代表了首次探索基于LLM的具身模型决策级过程鲁棒性的实验。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design a multi-modal dataset consisting of 500 instances of untargeted attack
    data and 500 instances of targeted attack data to fill the gaps in datasets for
    robustness evaluation in embodied scenarios.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一个多模态数据集，包括500个无针对性攻击数据实例和500个有针对性攻击数据实例，以填补具身场景中鲁棒性评估数据集的空白。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments show that our method improves attack success rate and
    attack efficiency.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛的实验表明，我们的方法提高了攻击成功率和攻击效率。
- en: 2\. Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: 2.1\. Embodied task planning
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 具身任务规划
- en: As an emerging and significant research direction, embodied task planning has
    garnered attention from numerous researchers in domains such as domestic service
    (Wu et al., [2023a](#bib.bib35)), medical treatment (Sun et al., [2023](#bib.bib29);
    Zhao et al., [2023](#bib.bib41); Li et al., [2024](#bib.bib16)), and agricultural
    harvesting (Vemprala et al., [2023](#bib.bib32); Stella et al., [2023](#bib.bib27)).
    Existing works primarily delve into harnessing NLP technology to aid robots in
    planning and executing intricate tasks. On one front, some studies utilize domain-specific
    datasets to train conventional deep learning models for generating robot mission
    plans (Sun et al., [2022](#bib.bib28)). On the other hand, the emergence of LLM
    has brought forth enhanced semantic comprehension and natural language processing
    capabilities, further empowering embodied intelligence. The LLM-based embodied
    model empowers the system to better grasp and execute tasks based on natural language.
    Wu et al. (Wu et al., [2023b](#bib.bib37)) introduced the TAsk planning Agent,
    which aligns LLM with a visual perception model to generate executable plans based
    on scene objects, grounding planning with physical constraints. Li et al. (Li
    et al., [2023a](#bib.bib15)) created a multi-modal dataset and fine-tuned LLM
    using it, allowing the robot to execute new instructions with minimal context
    learning. Song et al. (Song et al., [2023](#bib.bib26)) proposed the LLM-Planner
    framework, facilitating the interaction between planning and the environment by
    amalgamating high-level planning instructions from LLM with the environmental
    state mapped by low-level planners. However, despite the strides made in embodied
    task planning by the aforementioned research (Li et al., [2023a](#bib.bib15);
    Wu et al., [2023b](#bib.bib37); Song et al., [2023](#bib.bib26); Lynch et al.,
    [2023](#bib.bib19); Sharan et al., [2024](#bib.bib25); Yang et al., [2023](#bib.bib38);
    Nottingham et al., [2023](#bib.bib20); Dorbala et al., [2023](#bib.bib11), [2024](#bib.bib10);
    Zheng et al., [2023](#bib.bib42); Qiao et al., [2023](#bib.bib22); Szot et al.,
    [2023](#bib.bib30); Dagan et al., [2023](#bib.bib7); Zhang et al., [2023](#bib.bib40)),
    the measurement and assurance of robot safety and robustness during task execution
    post the integration of LLM remain prominent challenges. Thus, this paper aims
    to introduce new perspectives and methodologies for the evaluation of security
    and robustness in the field of embodied task planning combined with LLM, through
    the design of attack algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个新兴且重要的研究方向，具身任务规划在家政服务（Wu et al., [2023a](#bib.bib35)）、医疗治疗（Sun et al.,
    [2023](#bib.bib29); Zhao et al., [2023](#bib.bib41); Li et al., [2024](#bib.bib16)）和农业收获（Vemprala
    et al., [2023](#bib.bib32); Stella et al., [2023](#bib.bib27)）等领域引起了众多研究者的关注。现有工作主要*深入探讨*如何利用NLP技术帮助机器人规划和执行复杂任务。一方面，一些研究利用特定领域的数据集来训练传统的深度学习模型，以生成机器人任务计划（Sun
    et al., [2022](#bib.bib28)）。另一方面，LLM的出现带来了增强的语义理解和自然语言处理能力，进一步赋能了具身智能。基于LLM的具身模型使系统能够更好地理解和执行基于自然语言的任务。Wu
    et al.（Wu et al., [2023b](#bib.bib37)）介绍了任务规划代理，将LLM与视觉感知模型对齐，以生成基于场景对象的可执行计划，将规划与物理约束结合起来。Li
    et al.（Li et al., [2023a](#bib.bib15)）创建了一个多模态数据集并对LLM进行了微调，使机器人能够以最少的上下文学习执行新指令。Song
    et al.（Song et al., [2023](#bib.bib26)）提出了LLM-Planner框架，通过将LLM的高层规划指令与低层规划器映射的环境状态相结合，促进了规划与环境之间的互动。然而，尽管上述研究在具身任务规划领域取得了进展（Li
    et al., [2023a](#bib.bib15); Wu et al., [2023b](#bib.bib37); Song et al., [2023](#bib.bib26);
    Lynch et al., [2023](#bib.bib19); Sharan et al., [2024](#bib.bib25); Yang et al.,
    [2023](#bib.bib38); Nottingham et al., [2023](#bib.bib20); Dorbala et al., [2023](#bib.bib11),
    [2024](#bib.bib10); Zheng et al., [2023](#bib.bib42); Qiao et al., [2023](#bib.bib22);
    Szot et al., [2023](#bib.bib30); Dagan et al., [2023](#bib.bib7); Zhang et al.,
    [2023](#bib.bib40))，在LLM集成后的任务执行过程中，机器人安全性和鲁棒性的测量和保障仍然是突出的挑战。因此，本文旨在通过攻击算法的设计，引入具身任务规划结合LLM领域安全性和鲁棒性评估的新视角和方法。
- en: 2.2\. Jailbreak attack based on LLM
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 基于LLM的越狱攻击
- en: LLM has received a lot of attention because of its powerful generative ability,
    but recent studies (Zou et al., [2023](#bib.bib43); Liu et al., [2023](#bib.bib18);
    Ding et al., [2023](#bib.bib8); Wei et al., [2024](#bib.bib34); Li et al., [2023b](#bib.bib17);
    Dong et al., [2023](#bib.bib9); Wei et al., [2023](#bib.bib33); Chao et al., [2023](#bib.bib6);
    Shah et al., [2023](#bib.bib24); Carlini et al., [2023](#bib.bib5); Huang et al.,
    [2023](#bib.bib13); Yu et al., [2023](#bib.bib39); Qi et al., [2023](#bib.bib21))
    have shown that LLM is vulnerable to jailbreak attacks to bypass its own value
    alignment mechanism. Zou et al. (Zou et al., [2023](#bib.bib43)) proposed a adversarial
    jailbreak attack algorithm that allows malicious questions to induce their aligned
    language models to produce harmful content by adding adversarial suffixes. Ding
    et al. (Ding et al., [2023](#bib.bib8)) proposed the ReNeLLM framework, which
    uses dual design to conceal the harmful prompt and bypass the value alignment
    strategy of LLM. Zhu et al. (Liu et al., [2023](#bib.bib18)) proposed the AutoDAN
    framework, which automatically generates secret jailbreak hints through a carefully
    designed hierarchical genetic algorithm, enabling LLM to bypass value alignment
    and generate responses to the malicious prompt. However, the above-mentioned studies
    only focus on adversarial content at the text level, ignoring the impact of multi-modal
    information such as vision and action in embodied intelligence, and cannot be
    directly applied to embodied scenarios. Therefore, this paper considers combining
    a multi-modal dataset with embodied environments to more comprehensively evaluate
    the performance of embodied intelligent robots. At the same time, LLM in the embodied
    scenario is attacked according to the values-aligned attack strategy, so that
    it outputs content unrelated to prompts or even malicious content, and then explores
    the security risks caused by the introduction of LLM in embodied intelligence
    technology.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 因其强大的生成能力而受到了广泛关注，但近期的研究（Zou 等， [2023](#bib.bib43)；Liu 等， [2023](#bib.bib18)；Ding
    等， [2023](#bib.bib8)；Wei 等， [2024](#bib.bib34)；Li 等， [2023b](#bib.bib17)；Dong
    等， [2023](#bib.bib9)；Wei 等， [2023](#bib.bib33)；Chao 等， [2023](#bib.bib6)；Shah
    等， [2023](#bib.bib24)；Carlini 等， [2023](#bib.bib5)；Huang 等， [2023](#bib.bib13)；Yu
    等， [2023](#bib.bib39)；Qi 等， [2023](#bib.bib21)）已显示 LLM 易受到越狱攻击，从而绕过其自身的价值对齐机制。Zou
    等（Zou 等， [2023](#bib.bib43)）提出了一种对抗性越狱攻击算法，该算法通过添加对抗性后缀使恶意问题诱导其对齐的语言模型生成有害内容。Ding
    等（Ding 等， [2023](#bib.bib8)）提出了 ReNeLLM 框架，该框架使用双重设计来隐藏有害提示并绕过 LLM 的价值对齐策略。Zhu
    等（Liu 等， [2023](#bib.bib18)）提出了 AutoDAN 框架，该框架通过精心设计的层级遗传算法自动生成秘密越狱提示，使 LLM 绕过价值对齐并生成对恶意提示的回应。然而，上述研究仅关注文本级别的对抗内容，忽视了多模态信息（如视觉和行动）在具身智能中的影响，无法直接应用于具身场景。因此，本文考虑将多模态数据集与具身环境结合，以更全面地评估具身智能机器人的性能。同时，本文在具身场景中按照价值对齐攻击策略攻击
    LLM，使其输出与提示无关的内容甚至恶意内容，然后探讨引入 LLM 在具身智能技术中带来的安全风险。
- en: 3\. Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法
- en: In this section, we first describe the format distribution and construction
    of the EIRAD used to evaluate the robustness of the LLM decision-level in embodied
    scenarios. In Section [3.1.1](#S3.SS1.SSS1 "3.1.1\. Data types and statistics.
    ‣ 3.1\. Dataset analysis and creation process ‣ 3\. Method ‣ Exploring the Robustness
    of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models") we
    present the data types and statistics of the EIRAD. In Section [3.1.2](#S3.SS1.SSS2
    "3.1.2\. Description of the dataset creation process. ‣ 3.1\. Dataset analysis
    and creation process ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models"), we outline the process
    of generating the EIRAD. Subsequently, we delve into the details of attacking
    the LLM-based embodied models. Specifically, in Section [3.2.1](#S3.SS2.SSS1 "3.2.1\.
    Initialize prompt suffix ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method
    ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models"), we elaborate on the details of prompt suffix initialization,
    and discuss the implementation of the attack algorithm in Section [3.2.2](#S3.SS2.SSS2
    "3.2.2\. Optimize adversarial suffixes ‣ 3.2\. Embodied scenario attack algorithm
    ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"). Furthermore, we outline the evaluation
    method for determining the success of the attack in Section [3.2.3](#S3.SS2.SSS3
    "3.2.3\. Judgment of attack success ‣ 3.2\. Embodied scenario attack algorithm
    ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models").
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述了用于评估在具身场景中LLM决策层鲁棒性的EIRAD的格式分布和构建方法。在[3.1.1](#S3.SS1.SSS1 "3.1.1\.
    数据类型和统计数据。 ‣ 3.1\. 数据集分析与创建过程 ‣ 3\. 方法 ‣ 通过对LLM基础的具身模型进行对抗性攻击来探索决策层的鲁棒性")节中，我们介绍了EIRAD的数据类型和统计数据。在[3.1.2](#S3.SS1.SSS2
    "3.1.2\. 数据集创建过程描述。 ‣ 3.1\. 数据集分析与创建过程 ‣ 3\. 方法 ‣ 通过对LLM基础的具身模型进行对抗性攻击来探索决策层的鲁棒性")节中，我们概述了生成EIRAD的过程。随后，我们深入探讨了对LLM基础的具身模型进行攻击的细节。具体来说，在[3.2.1](#S3.SS2.SSS1
    "3.2.1\. 初始化提示后缀 ‣ 3.2\. 具身场景攻击算法 ‣ 3\. 方法 ‣ 通过对LLM基础的具身模型进行对抗性攻击来探索决策层的鲁棒性")节中，我们详细阐述了提示后缀初始化的细节，并在[3.2.2](#S3.SS2.SSS2
    "3.2.2\. 优化对抗性后缀 ‣ 3.2\. 具身场景攻击算法 ‣ 3\. 方法 ‣ 通过对LLM基础的具身模型进行对抗性攻击来探索决策层的鲁棒性")节中讨论了攻击算法的实现。此外，我们在[3.2.3](#S3.SS2.SSS3
    "3.2.3\. 攻击成功的判断 ‣ 3.2\. 具身场景攻击算法 ‣ 3\. 方法 ‣ 通过对LLM基础的具身模型进行对抗性攻击来探索决策层的鲁棒性")节中概述了确定攻击成功的方法。
- en: 3.1\. Dataset analysis and creation process
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 数据集分析与创建过程
- en: The AdvBench dataset proposed by Zou et al. (Zou et al., [2023](#bib.bib43))
    encompasses a broad spectrum of harmful content, ranging from profanity and graphic
    descriptions to threatening behaviors, misinformation, discrimination, cybercrime,
    and dangerous or illegal advice. However, in the embodied scenario, the requisite
    data not only entails encompassing harmful text but also necessitates the inclusion
    of images as inputs, alongside requiring a deep interaction and fusion between
    text and images. This intricate nature poses challenges for existing datasets
    to directly cater to this specific scenario. Hence, we propose the EIRAD dataset
    to assess the robustness of LLM in embodied intelligent robotics.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Zou等人（Zou et al., [2023](#bib.bib43)）提出的AdvBench数据集涵盖了广泛的有害内容，从亵渎性语言和图形描述到威胁行为、虚假信息、歧视、网络犯罪以及危险或非法建议。然而，在具身场景中，所需的数据不仅包括有害文本，还需要包含图像作为输入，并且要求文本与图像之间有深入的互动和融合。这种复杂的性质对现有数据集直接适应这一特定场景提出了挑战。因此，我们提出了EIRAD数据集，以评估LLM在具身智能机器人中的鲁棒性。
- en: 3.1.1\. Data types and statistics.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 数据类型和统计数据。
- en: 'The dataset types are illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.1.1\.
    Data types and statistics. ‣ 3.1\. Dataset analysis and creation process ‣ 3\.
    Method ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks
    on LLM-Based Embodied Models"), which is divided into two main categories: targeted
    attack data and untargeted attack data. In untargeted attack data, we do not set
    a specific output target, and aim to make the system output unexpected, random
    or meaningless content. Such attacks may exhibit more randomness and covert characteristics,
    necessitating a highly robust system to handle them effectively. In contrast,
    in the targeted attack data, specific attack targets are set, such as ”creating
    chaos”, ”making harmful suggestions”, and so on. This configuration aims to simulate
    scenarios where attackers have clear goals or expected outputs, thereby evaluating
    the system’s defense and countermeasure capabilities under such circumstances.
    Additionally, the targeted attack data is further subdivided into harmful attack
    data and harmless attack data. The goal of setting the harmful attack data is
    to prompt the LLM to produce harmful, dangerous, or inappropriate content. This
    enables the evaluation of the system’s response to malicious inputs and assesses
    whether the system adheres to ethical and legal standards. Conversely, the goal
    of setting harmless attack data is to prompt the LLM to generate harmless but
    invalid content, providing insights into the system’s stability against harmless
    inputs. Additionally, we supplement the dataset by incorporating output responses
    generated by GPT3.5 for each attack data, thereby enhancing the completeness of
    the dataset. By utilizing this data classification and setup to simulate various
    attack scenarios, the security and robustness of the LLM’s decision-level capabilities
    in specific contexts can be thoroughly assessed. This process aids researchers
    in identifying potential security vulnerabilities and improving the system’s defense
    mechanisms.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集类型如图[2](#S3.F2 "Figure 2 ‣ 3.1.1\. 数据类型与统计 ‣ 3.1\. 数据集分析与创建过程 ‣ 3\. 方法 ‣
    通过对LLM基础的具身模型进行对抗攻击探索决策级别的鲁棒性")所示，分为两大类：定向攻击数据和非定向攻击数据。在非定向攻击数据中，我们不设定特定的输出目标，目的是使系统输出意外的、随机的或无意义的内容。这种攻击可能表现出更多的随机性和隐蔽性特点，因此需要一个高度鲁棒的系统来有效应对。相反，在定向攻击数据中，设定了具体的攻击目标，如“制造混乱”、“提出有害建议”等。此配置旨在模拟攻击者有明确目标或预期输出的场景，从而评估系统在这些情况下的防御和应对能力。此外，定向攻击数据进一步细分为有害攻击数据和无害攻击数据。设定有害攻击数据的目标是促使LLM生成有害、危险或不当的内容。这有助于评估系统对恶意输入的反应，并检验系统是否符合伦理和法律标准。相对而言，设定无害攻击数据的目标是促使LLM生成无害但无效的内容，以提供对系统在无害输入下稳定性的洞察。此外，我们通过将GPT3.5生成的每个攻击数据的输出响应纳入数据集来补充数据集，从而增强数据集的完整性。通过利用这种数据分类和设置来模拟各种攻击场景，可以全面评估LLM在特定情境下的决策级别能力的安全性和鲁棒性。这一过程有助于研究人员识别潜在的安全漏洞并改进系统的防御机制。
- en: '![Refer to caption](img/c918833b84ee49a3ec55047e3d83530b.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c918833b84ee49a3ec55047e3d83530b.png)'
- en: Figure 2\. Data type distribution in EIRAD
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. EIRAD中的数据类型分布
- en: '![Refer to caption](img/b8dcb1b4be8015f9c2b7bae847c60d99.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b8dcb1b4be8015f9c2b7bae847c60d99.png)'
- en: (a) prompt in harmless data
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: （a）在无害数据中的提示
- en: '![Refer to caption](img/0cecfbdabe37d7890874f195083d3b70.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0cecfbdabe37d7890874f195083d3b70.png)'
- en: (b) target in harmless data
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: （b）在无害数据中的目标
- en: '![Refer to caption](img/2e7b7513779fd53a4274669caac2ff7e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2e7b7513779fd53a4274669caac2ff7e.png)'
- en: (c) target in harmful data
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: （c）在有害数据中的目标
- en: Figure 3\. The data statistics of multi-modal. (a) The 10 most frequently prompted
    verbs in harmless data along with their corresponding noun objects. (b) The 10
    most frequently targeted verbs in harmless data along with their corresponding
    noun objects. (c) The 10 most frequently targeted verbs in harmful data along
    with their corresponding noun objects.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 多模态的数据统计。（a）在无害数据中最常见的10个动词及其对应的名词对象。（b）在无害数据中最常见的10个目标动词及其对应的名词对象。（c）在有害数据中最常见的10个目标动词及其对应的名词对象。
- en: In order to better evaluate the performance of embodied intelligent robots in
    terms of security and robustness, we simulate as much as possible the various
    behaviors and actions that attackers might take when making EIRAD. In figure [3](#S3.F3
    "Figure 3 ‣ 3.1.1\. Data types and statistics. ‣ 3.1\. Dataset analysis and creation
    process ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"), we illustrate the 10 most frequently occurring
    verbs in the prompt and target instructions of the targeted attack data, along
    with all the corresponding noun objects. In the harmless data, both prompt and
    target instructions exhibit rich and diverse characteristics, encompassing various
    actions such as ”heating,” ”using,” ”adjustment,” and more. The noun objects associated
    with these actions also vary, including items like ”microwave oven,” ”spatula,”
    ”lampshade,” and others. This diversity is designed to simulate the myriad challenges
    that robots may encounter in embodied environments. On the other hand, the harmful
    data presents a range of high-frequency verb and noun object combinations, such
    as ”cut off fingers,” ”break mirrors,” and so on. These combinations reflect the
    diversity and complexity of attacks that malicious actors might employ, leveraging
    the robot’s physical and sensory capabilities. For instance, ”cutting off fingers”
    implies scenarios where the robot could potentially cause harm to human bodies,
    while ”breaking mirrors” might result in damage to objects within the environment.
    Overall, EIRAD has important reference significance for evaluating the performance
    of embodied intelligent robots in terms of safety and robustness, and designing
    effective defense strategies and mechanisms. Additionally, it provides researchers
    with a comprehensive and detailed dataset to delve into and address the safety
    challenges and potential risks inherent in embodied intelligent robot systems.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地评估具身智能机器人在安全性和鲁棒性方面的表现，我们尽可能模拟攻击者在进行EIRAD时可能采取的各种行为和动作。在图[3](#S3.F3 "图
    3 ‣ 3.1.1\. 数据类型与统计 ‣ 3.1\. 数据集分析与创建过程 ‣ 3\. 方法 ‣ 通过对LLM基础具身模型的对抗攻击探索决策层的鲁棒性")中，我们展示了目标攻击数据中提示和目标指令中最常出现的10个动词，以及所有相应的名词对象。在无害数据中，提示和目标指令展现了丰富多样的特征，包括“加热”、“使用”、“调整”等各种动作。与这些动作相关的名词对象也各不相同，包括“微波炉”、“铲子”、“灯罩”等。这种多样性旨在模拟机器人在具身环境中可能遇到的各种挑战。另一方面，恶意数据展示了一系列高频动词和名词对象组合，如“割掉手指”、“打破镜子”等。这些组合反映了恶意行为者可能利用机器人的物理和感官能力进行的攻击的多样性和复杂性。例如，“割掉手指”暗示了机器人可能对人体造成伤害的情境，而“打破镜子”可能导致环境中物体的损坏。总体而言，EIRAD在评估具身智能机器人在安全性和鲁棒性方面的表现、设计有效的防御策略和机制方面具有重要的参考意义。此外，它为研究人员提供了一个全面详细的数据集，以深入研究和解决具身智能机器人系统中固有的安全挑战和潜在风险。
- en: 3.1.2\. Description of the dataset creation process.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 数据集创建过程的描述。
- en: '![Refer to caption](img/a716f49cbb86d4d0ed6d9dc1bc451f62.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a716f49cbb86d4d0ed6d9dc1bc451f62.png)'
- en: Figure 4\. The creation process of multi-modal dataset
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 多模态数据集的创建过程
- en: The creation process of the multi-modal dataset EIRAD is depicted in Figure
    [4](#S3.F4 "Figure 4 ‣ 3.1.2\. Description of the dataset creation process. ‣
    3.1\. Dataset analysis and creation process ‣ 3\. Method ‣ Exploring the Robustness
    of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models").
    The key distinction between targeted attack data and untargeted attack data lies
    in the presence of an additional ”target” instruction within the targeted attack
    data. Consequently, step 1 to 3 in Figure 4 represent the Co-production process
    for both untargeted and targeted attack data, while step 4 is the distinct production
    process for targeted attack data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态数据集EIRAD的创建过程在图[4](#S3.F4 "图 4 ‣ 3.1.2\. 数据集创建过程的描述 ‣ 3.1\. 数据集分析与创建过程 ‣
    3\. 方法 ‣ 通过对LLM基础具身模型的对抗攻击探索决策层的鲁棒性")中展示。目标攻击数据与非目标攻击数据的主要区别在于目标攻击数据中包含额外的“目标”指令。因此，图4中的第1至第3步表示了非目标攻击数据和目标攻击数据的共同制作过程，而第4步则是目标攻击数据的独特生产过程。
- en: Collect image. 100 scene images from varying perspectives are chosen from the
    AI2-THOR simulator (Kolve et al., [2017](#bib.bib14)) to serve as the embodied
    scene for the robotic entity.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 收集图像。从AI2-THOR模拟器（Kolve et al., [2017](#bib.bib14)）中选择100张来自不同角度的场景图像，作为机器人实体的具象场景。
- en: Detect object list. A methodology akin to TAPA (Wu et al., [2023b](#bib.bib37))
    is employed to precisely discern object information within the scene using an
    open vocabulary detector. Any redundant object names within the scene are then
    expunged, thereby furnishing pertinent scene details for the LLM, such as the
    input = [chair, table, bowl, microwave…].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 检测对象列表。采用类似TAPA的方法（Wu et al., [2023b](#bib.bib37)）使用开放词汇检测器精确识别场景中的对象信息。然后，删除场景中的任何冗余对象名称，从而提供给LLM相关的场景细节，例如输入
    = [椅子, 桌子, 碗, 微波炉…]。
- en: Generate prompt. In the ALFRED benchmark (Zou et al., [2023](#bib.bib43)), a
    straightforward approach for generating multi-modal instructions pertinent to
    embodied tasks involves crafting a series of instructions tailored to the prevailing
    environment. Nonetheless, devised designs necessitate considerable effort, especially
    when crafting targeted attack data. Each data instance mandates the separate formulation
    of both prompt and target instructions. To enhance production efficiency and curtail
    costs, we devise a mechanism utilizing GPT-3.5 to simulate specific task planning
    scenarios, thereby automatically generating prompt instructions based on the provided
    object name list input, as depicted in step 3 of Figure 4\. Consequently, the
    production of untargeted attack data is completed at this juncture.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 生成提示。在ALFRED基准测试（Zou et al., [2023](#bib.bib43)）中，生成与具象任务相关的多模态指令的一种直接方法是制作一系列适应当前环境的指令。然而，设计方案需要大量工作，尤其是在制作目标攻击数据时。每个数据实例都要求分别制定提示和目标指令。为了提高生产效率并减少成本，我们设计了一种利用GPT-3.5模拟特定任务规划场景的机制，从而根据提供的对象名称列表输入自动生成提示指令，如图4的第3步所示。因此，非目标攻击数据的生成在这一点上完成。
- en: Generate target. In order to reduce production costs and improve efficiency,
    GPT-3.5 is also used in this step to generate target instructions in the targeted
    attack data. The GPT3.5 prompt is shown in step 4\. In this prompt, It is pivotal
    to emphasize that the generated ”target” should bear no relation to the prompt,
    yet simultaneously encompass the listed input objects.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 生成目标。为了降低生产成本并提高效率，在此步骤中也使用GPT-3.5来生成目标指令。GPT3.5的提示在第4步中展示。在这个提示中，关键是要强调生成的“目标”应与提示无关，但同时涵盖所列的输入对象。
- en: 3.2\. Embodied scenario attack algorithm
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 具象场景攻击算法
- en: Our objective is to investigate the robustness of the LLM-based embodied model’s
    decision-level processes. The algorithmic framework, as shown in Figure [5](#S3.F5
    "Figure 5 ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method ‣ Exploring
    the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied
    Models"), unfolds in several steps. Initially, in step 1, we initialize a prompt
    suffix. Subsequently, in step 2, we optimize the prompt suffix using the greedy
    gradient descent algorithm (Zou et al., [2023](#bib.bib43)), aiming to prompt
    the LLM to output content unrelated to the prompt. Following this optimization
    process, in step 3, we slice the output content according to each step of output
    and calculate its similarity with the target to determine the success of the attack.
    If the attack is unsuccessful, we return to step 2 and continue optimizing the
    prompt suffix. In the subsequent sections, we will elaborate on these three steps
    in detail.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是研究基于LLM的具象模型决策级过程的鲁棒性。算法框架如图 [5](#S3.F5 "图 5 ‣ 3.2\. 具象场景攻击算法 ‣ 3\. 方法
    ‣ 通过对基于LLM的具象模型的对抗攻击探索决策级鲁棒性") 所示，展开为几个步骤。最初，在步骤1中，我们初始化提示后缀。随后，在步骤2中，我们使用贪婪梯度下降算法（Zou
    et al., [2023](#bib.bib43)）优化提示后缀，旨在促使LLM输出与提示无关的内容。经过此优化过程后，在步骤3中，我们根据每一步的输出内容进行切片，并计算其与目标的相似度，以确定攻击是否成功。如果攻击失败，我们将返回步骤2并继续优化提示后缀。在随后的部分中，我们将详细阐述这三个步骤。
- en: '![Refer to caption](img/2ec1e14c1d3feed2b2a22fbc4bbb950f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2ec1e14c1d3feed2b2a22fbc4bbb950f.png)'
- en: 'Figure 5\. The framework of the attack algorithm. Attack algorithms are categorized
    into two main types: untargeted attacks and targeted attacks. The targeted approach
    builds upon the foundation of non-targeted methods, showcasing differences in
    keyword initialization (step 1), selection of optimal suffixes (step 2), and selection
    of evaluation objects (step 3).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 攻击算法框架。攻击算法分为两大类：非定向攻击和定向攻击。定向方法在非定向方法的基础上进行，展示了在关键字初始化（步骤 1）、最佳后缀选择（步骤
    2）和评估对象选择（步骤 3）上的差异。
- en: 3.2.1\. Initialize prompt suffix
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 初始化提示后缀
- en: To guide the LLM in generating content unrelated to the prompts, we initialize
    a prompt suffix, as illustrated in step 1 of Figure [5](#S3.F5 "Figure 5 ‣ 3.2\.
    Embodied scenario attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of
    Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models"). In
    untargeted attacks, the suffix is optimized to ensure that the LLM outputs content
    that is unrelated to the original prompt. However, In targeted attacks, the suffix
    is optimized to prompt the LLM to output content relevant to the target task.
    As depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1\. Initialize prompt suffix
    ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method ‣ Exploring the Robustness
    of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models"),
    through experiments on targeted attacks, it is discovered that successful adversarial
    suffixes often contain keywords related to the target task. In order to enhance
    the iteration speed and success rate of the attack, we devise a strategy to design
    the initial content of the adversarial suffix based on the specific target task
    in the targeted attack scenario. Building upon the original ”!!!”, we replace
    a portion of the ”!!!” with keywords pertinent to the targeted task, as demonstrated
    in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1\. Initialize prompt suffix ‣ 3.2\. Embodied
    scenario attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models").
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导 LLM 生成与提示无关的内容，我们初始化提示后缀，如图 [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario
    attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through
    Adversarial Attacks on LLM-Based Embodied Models")步骤 1 所示。在非定向攻击中，优化后缀以确保 LLM
    输出与原始提示无关的内容。然而，在定向攻击中，优化后缀以提示 LLM 输出与目标任务相关的内容。如图 [6](#S3.F6 "Figure 6 ‣ 3.2.1\.
    Initialize prompt suffix ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method
    ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models")所示，通过对定向攻击的实验发现，成功的对抗后缀通常包含与目标任务相关的关键字。为了提高攻击的迭代速度和成功率，我们制定了一种策略，根据定向攻击场景中的特定目标任务设计对抗后缀的初始内容。在原始的
    ”!!!” 的基础上，我们用与目标任务相关的关键字替换了一部分 ”!!!”，如图 [6](#S3.F6 "Figure 6 ‣ 3.2.1\. Initialize
    prompt suffix ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method ‣ Exploring
    the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied
    Models")所示。
- en: '![Refer to caption](img/8d92ddf1dd2a8665cedd5d5a256c6eb1.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8d92ddf1dd2a8665cedd5d5a256c6eb1.png)'
- en: Figure 6\. Prompt suffix display and initialization strategy.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. 提示后缀显示与初始化策略。
- en: 3.2.2\. Optimize adversarial suffixes
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 优化对抗后缀
- en: 'As depicted in Figure [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario attack
    algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"), optimizing adversarial suffixes enables
    LLM to generate prompt-independent content. Following a methodology similar to
    GCG (Zou et al., [2023](#bib.bib43)), we conceptualize the generation phase of
    the LLM as predicting the subsequent token given the current token sequence. Building
    on this concept, we allow the LLM to use an input sequence of length n to generate
    a response of length H which can be represented as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method
    ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models")所示，优化对抗后缀可以使 LLM 生成与提示无关的内容。采用类似 GCG (Zou et al., [2023](#bib.bib43))的方法，我们将
    LLM 的生成阶段概念化为在给定当前令牌序列的情况下预测下一个令牌。在这一概念基础上，我们允许 LLM 使用长度为 n 的输入序列生成长度为 H 的响应，表示为：
- en: '| (1) |  |  |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  |  |  |'
- en: In untargeted attacks, our aim is for  to generate content that fulfills the
    target requirements to the fullest extent. Therefore, the loss function can be
    formulated to minimize the probability of these key target sequences under untargeted
    attack conditions, and to maximize the probability of these key target sequences
    under targeted attack conditions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在非定向攻击中，我们的目标是生成最大限度地满足目标要求的内容。因此，损失函数可以被公式化为在非定向攻击条件下最小化这些关键目标序列的概率，并在定向攻击条件下最大化这些关键目标序列的概率。
- en: '| (2) |  |  |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  |  |  |'
- en: Furthermore, the untargeted attack task is transformed into maximizing the loss
    function’s negative log probability, while the targeted attack task is transformed
    into minimizing the loss function’s negative log probability.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，非定向攻击任务被转化为最大化损失函数的负对数概率，而定向攻击任务则被转化为最小化损失函数的负对数概率。
- en: '| (3) |  | $\displaystyle\underset{x\in\{1,\ldots,V\}}{\operatorname{minimize}}\mathcal{L}\left(x_{1:n}\right)=0,~{}~{}\underset{x\in\{1,\ldots,V\}}{\operatorname{maximize}}\mathcal{L}\left(x_{1:n}\right)=0$
    |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle\underset{x\in\{1,\ldots,V\}}{\operatorname{minimize}}\mathcal{L}\left(x_{1:n}\right)=0,~{}~{}\underset{x\in\{1,\ldots,V\}}{\operatorname{maximize}}\mathcal{L}\left(x_{1:n}\right)=0$
    |  |'
- en: After determining the optimization target, the subsequent step involves optimizing
    this set of discrete inputs. Specifically, we utilize the one-hot token indicator
    to identify a set of promising candidate replacement tokens at each position.
    Subsequently, through forward propagation, we assess these replacements. Then,
    by calculating the top-k candidates for token replacement, we select the replacement
    words that maximize the loss in untargeted attacks and minimize the loss in targeted
    attacks. This computation is executed for each candidate position, yielding the
    final result—the adversarial suffix that optimizes the loss function.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 确定优化目标后，接下来的步骤涉及优化这组离散输入。具体而言，我们利用one-hot令牌指示符在每个位置识别一组有前景的候选替换令牌。随后，通过前向传播，我们评估这些替换。然后，通过计算令牌替换的top-k候选，我们选择那些在非定向攻击中最大化损失并在定向攻击中最小化损失的替换词。这一计算在每个候选位置执行，得到最终结果——优化损失函数的对抗后缀。
- en: 3.2.3\. Judgment of attack success
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 攻击成功的判断
- en: As illustrated in step 3 of Figure [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario
    attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through
    Adversarial Attacks on LLM-Based Embodied Models"), the updated suffix is incorporated
    into LLM alongside the original prompt to obtain the resulting output content.
    Subsequently, the output content is sliced and the similarity is computed to ascertain
    the success of the attack. In both GCG (Zou et al., [2023](#bib.bib43)) and AutoDAN
    (Liu et al., [2023](#bib.bib18)), the determination of attack success hinges on
    whether the output content aligns with the predefined list. However, this method
    heavily relies on the quality and comprehensiveness of the predefined list. Particularly
    in targeted attack, the predefined list needs constant updates corresponding to
    changes in the target task, which may introduce errors and affect the accuracy
    of experimental outcomes. Therefore, we devise a novel set of evaluation criteria
    and employ slicing operations to partition the output content of LLM based on
    the characteristics of embodied intelligence. It includes slicing operations and
    calculating similarity operations, which we will introduce in detail below.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario attack algorithm ‣ 3\. Method
    ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models")的第3步所示，更新后的后缀与原始提示一起被纳入LLM，以获得最终的输出内容。随后，将输出内容进行切片并计算相似度，以确定攻击是否成功。在GCG
    (Zou et al., [2023](#bib.bib43))和AutoDAN (Liu et al., [2023](#bib.bib18))中，攻击成功的判定取决于输出内容是否与预定义列表对齐。然而，这种方法严重依赖于预定义列表的质量和全面性。特别是在定向攻击中，预定义列表需要根据目标任务的变化进行不断更新，这可能引入错误并影响实验结果的准确性。因此，我们设计了一套新的评估标准，并根据体现智能的特征使用切片操作对LLM的输出内容进行分区。它包括切片操作和相似度计算操作，我们将在下文中详细介绍。
- en: Slice operation. To mitigate the occurrence of misjudgments and oversights,
    we implement a slicing operation on the LLM’s response. In an embodied intelligence
    environment, the response format of the LLM typically aligns with the structure
    depicted in Figure [6](#S3.F6 "Figure 6 ‣ 3.2.1\. Initialize prompt suffix ‣ 3.2\.
    Embodied scenario attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of
    Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models"). Herein,
    the number of steps within the output content varies with different tasks, and
    their respective correlations fluctuate accordingly. Consequently, establishing
    a singular, standardized threshold to gauge the strong correlation between them
    for subsequent similarity assessments proves challenging. This challenge could
    potentially lead to misjudgments and overlooked details. To address this challenge,
    we employ a slicing operation, treating each step within the output content as
    an individual entity. We calculate the similarity of each step with the target
    task independently and select the step with the highest similarity as the basis
    for measurement. If the computed similarity exceeds the predetermined threshold,
    it indicates that the LLM has indeed produced the target statement, signifying
    the success of the attack.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 切片操作。为了减少误判和遗漏的发生，我们对LLM的响应进行切片操作。在具身智能环境中，LLM的响应格式通常与图[6](#S3.F6 "Figure 6
    ‣ 3.2.1\. Initialize prompt suffix ‣ 3.2\. Embodied scenario attack algorithm
    ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models")中所示的结构一致。在此，输出内容中的步骤数量随着任务的不同而变化，其相互关联性也会相应波动。因此，建立一个单一的标准化阈值来衡量它们之间的强关联性，以便进行后续的相似度评估，是具有挑战性的。这一挑战可能导致误判和遗漏细节。为了解决这一挑战，我们采用切片操作，将输出内容中的每个步骤视为一个独立的实体。我们独立计算每个步骤与目标任务的相似度，并选择相似度最高的步骤作为测量基础。如果计算得到的相似度超过预定的阈值，则表示LLM确实生成了目标陈述，标志着攻击的成功。
- en: Similarity calculation. Given the non-uniqueness of the output content from
    LLM-based embodied model, a similarity calculation approach is employed to assess
    the alignment with the target task. Common methods for calculating this similarity
    include those based on the bag-of-words model (Wu et al., [2010](#bib.bib36)),
    TF-IDF weighted word vectors(Aizawa, [2003](#bib.bib2)), Bert-score(Unanue et al.,
    [2021](#bib.bib31)), among others. These methods are predominantly utilized in
    machine translation and text matching tasks. However, they possess limitations
    as they only capture the surface meaning of the statements, lack flexibility,
    and are unable to identify variations in expressions that convey the same meaning.
    To enhance the judgment of whether the output content aligns with the target task,
    we utilize the text encoding method from blip2-image-text-matching to obtain feature
    representations of both the output content and the target task. Subsequently,
    we calculate the cosine similarity between these representations to determine
    the degree of alignment with the target task.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相似度计算。由于基于LLM的具身模型输出内容的非唯一性，我们采用相似度计算方法来评估与目标任务的对齐情况。常见的相似度计算方法包括基于词袋模型的方法（Wu
    et al., [2010](#bib.bib36)）、TF-IDF加权词向量（Aizawa, [2003](#bib.bib2)）、Bert-score（Unanue
    et al., [2021](#bib.bib31)）等。这些方法主要用于机器翻译和文本匹配任务。然而，它们存在一些局限性，因为它们仅捕捉了陈述的表面含义，缺乏灵活性，并且无法识别传达相同含义的表达变化。为了增强对输出内容是否与目标任务对齐的判断，我们利用来自blip2-image-text-matching的文本编码方法来获取输出内容和目标任务的特征表示。随后，我们计算这些表示之间的余弦相似度，以确定与目标任务的对齐程度。
- en: 4\. Experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: In this section, we demonstrate the experimental impact of our method on attacking
    LLM-based embodied model to assess the robustness of embodied system. Firstly,
    in Section [4.1](#S4.SS1 "4.1\. Settings ‣ 4\. Experiments ‣ Exploring the Robustness
    of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models"),
    we introduce the experimental setup. Following that, in Section [4.2](#S4.SS2
    "4.2\. Main results ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models"), we present the comparison
    of attack results between our method and the most advanced white-box jailbreak
    attack technologies, including GCG (Zou et al., [2023](#bib.bib43)) and AutoDAN
    (Liu et al., [2023](#bib.bib18)), applied to three different LLM-based embodied
    models. Subsequently, in Section [4.3](#S4.SS3 "4.3\. Execution success rate ‣
    4\. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"), we analyze the execution success rate
    by user study to evaluate whether the output of LLM can be executed in the current
    environment. Furthermore, in Section [4.4](#S4.SS4 "4.4\. The impact of keyword
    initialization on loss ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models"), we delve into the
    reasons why the initialization of prompt suffix keywords can significantly reduce
    the attack success time. Finally, ablation experiments are conducted in Section
    [4.5](#S4.SS5 "4.5\. Ablation Study ‣ 4\. Experiments ‣ Exploring the Robustness
    of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models") to
    assess the importance of the two modules proposed by ours.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了我们的方法在攻击基于LLM的具身模型方面的实验影响，以评估具身系统的鲁棒性。首先，在第[4.1](#S4.SS1 "4.1\. 设置
    ‣ 4\. 实验 ‣ 通过对基于LLM的具身模型进行对抗攻击探索决策级别的鲁棒性")节中，我们介绍了实验设置。接下来，在第[4.2](#S4.SS2 "4.2\.
    主要结果 ‣ 4\. 实验 ‣ 通过对基于LLM的具身模型进行对抗攻击探索决策级别的鲁棒性")节中，我们展示了我们的方法与最先进的白盒越狱攻击技术（包括GCG
    (Zou et al., [2023](#bib.bib43))和AutoDAN (Liu et al., [2023](#bib.bib18))）在三个不同的基于LLM的具身模型上的攻击结果比较。随后，在第[4.3](#S4.SS3
    "4.3\. 执行成功率 ‣ 4\. 实验 ‣ 通过对基于LLM的具身模型进行对抗攻击探索决策级别的鲁棒性")节中，我们通过用户研究分析执行成功率，以评估LLM的输出是否可以在当前环境中执行。此外，在第[4.4](#S4.SS4
    "4.4\. 关键词初始化对损失的影响 ‣ 4\. 实验 ‣ 通过对基于LLM的具身模型进行对抗攻击探索决策级别的鲁棒性")节中，我们深入探讨了为何提示后缀关键词的初始化可以显著减少攻击成功时间。最后，在第[4.5](#S4.SS5
    "4.5\. 消融研究 ‣ 4\. 实验 ‣ 通过对基于LLM的具身模型进行对抗攻击探索决策级别的鲁棒性")节中，我们进行了消融实验，以评估我们提出的两个模块的重要性。
- en: Table 1\. Main results. We report the ASR and Epoch cost of our method for targeted
    and untargeted attacks on three models on the EIRAD dataset. Compared with the
    baseline, our method can effectively attack the LLM-based embodied model and greatly
    shorten the attack time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 主要结果。我们报告了在EIRAD数据集上对三个模型的定向和非定向攻击的ASR和Epoch成本。与基准相比，我们的方法可以有效攻击基于LLM的具身模型，并大大缩短攻击时间。
- en: '|      Experiment |      Targeted attack-unharmful |      Targeted attack-harmful
    |      Untargeted attack |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|      实验 |      定向攻击-无害 |      定向攻击-有害 |      非定向攻击 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|      Model |      Method |      ASR |      Epoch cost |      ASR |      Epoch
    cost |      ASR |      Epoch cost |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|      模型 |      方法 |      ASR |      Epoch成本 |      ASR |      Epoch成本 |      ASR
    |      Epoch成本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
- en: '|      Tapa |      GCG |      0.32 |      148 |      0.02 |      74 |      -
    |      - |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|      Tapa |      GCG |      0.32 |      148 |      0.02 |      74 |      -
    |      - |'
- en: '|  |      Ours |      0.72 |      84 |      0.22 |      124 |      1 |      9
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |      我们的方法 |      0.72 |      84 |      0.22 |      124 |      1 |      9
    |'
- en: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
- en: '|      Otter |      GCG |      0.81 |      101 |      0.64 |      180 |      -
    |      - |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|      Otter |      GCG |      0.81 |      101 |      0.64 |      180 |      -
    |      - |'
- en: '|  |      Ours |      0.95 |      56 |      0.86 |      120 |      0.80 |      67
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |      我们的方法 |      0.95 |      56 |      0.86 |      120 |      0.80 |
         67 |'
- en: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  |      AutoDAN |      0 |      500 |      0 |      500 |      - |      -
    |'
- en: '|      Llama-2-chat |      GCG |      0.97 |      142 |      0.82 |      207
    |      - |      - |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|      Llama-2-chat |      GCG |      0.97 |      142 |      0.82 |      207
    |      - |      - |'
- en: '|  |      Ours |      0.97 |      60 |      0.92 |      127 |      0.57 |      104
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |      我们的方法 |      0.97 |      60 |      0.92 |      127 |      0.57 |
         104 |'
- en: 4.1\. Settings
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 设置
- en: Datasets. We employ the multi-modal dataset EIRAD to assess the LLM robustness
    of embodied intelligent robots. This dataset comprises a total of 500 instances
    of untargeted attack data and 500 instances of targeted attack data. Additionally,
    the targeted attack data is further categorized into 450 instances of harmless
    attack data and 50 instances of harmful attack data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们使用多模态数据集EIRAD来评估具体智能机器人在LLM鲁棒性方面的表现。该数据集包含500个未定向攻击数据实例和500个定向攻击数据实例。此外，定向攻击数据进一步分类为450个无害攻击数据实例和50个有害攻击数据实例。
- en: Models. To ensure the generality of the attack method, we assess two fine-tuned
    open-source models (TaPA and Otter) and one un-fine-tuned open-source model (Llama-2-7b-chat)
    in embodied scenarios. The TaPA model, developed by Wu et al. (Wu et al., [2023b](#bib.bib37)),
    was fine-tuned using a dataset comprising 15K instruction-task data pairs to refine
    the Llama model. The Otter model was generated by Li et al. (Li et al., [2023a](#bib.bib15))
    using the MIMIC-IT dataset containing 2.8 million multi-modal instruction-response
    pairs to fine-tune the OpenFlamingo (Awadalla et al., [2023](#bib.bib3)) model.
    Additionally, the Llama-2-7b-chat model is utilized for decision-making in embodied
    tasks.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。为了确保攻击方法的普遍性，我们在具体场景中评估了两个经过微调的开源模型（TaPA和Otter）以及一个未经过微调的开源模型（Llama-2-7b-chat）。TaPA模型由Wu等人（Wu
    et al., [2023b](#bib.bib37)）开发，使用包含15K指令-任务数据对的数据集进行了微调，以优化Llama模型。Otter模型由Li等人（Li
    et al., [2023a](#bib.bib15)）生成，使用包含280万多模态指令-响应对的MIMIC-IT数据集对OpenFlamingo（Awadalla
    et al., [2023](#bib.bib3)）模型进行了微调。此外，Llama-2-7b-chat模型用于具体任务中的决策。
- en: Baselines. Considering the related work on LLM adversarial jailbreaking (Zou
    et al., [2023](#bib.bib43); Liu et al., [2023](#bib.bib18); Ding et al., [2023](#bib.bib8);
    Wei et al., [2024](#bib.bib34); Li et al., [2023b](#bib.bib17)), we compare our
    method with some representative baseline methods, such as GCG (Zou et al., [2023](#bib.bib43))
    and AutoDAN (Liu et al., [2023](#bib.bib18)), to assess the robustness of the
    LLM-based embodied model under white-box attack scenarios. As evident from Section
    [3.2.3](#S3.SS2.SSS3 "3.2.3\. Judgment of attack success ‣ 3.2\. Embodied scenario
    attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through
    Adversarial Attacks on LLM-Based Embodied Models"), the text matching list methods
    employed by GCG and AutoDAN are not suitable for tasks involving embodied attacks.
    Hence, we replace the text matching algorithms in these two methods with the slicing
    and similarity calculation methods proposed in our paper as the criteria for judging
    attack success. Finally, we compare the method proposed in this paper with the
    GCG(Zou et al., [2023](#bib.bib43)) and AutoDAN (Liu et al., [2023](#bib.bib18))
    algorithms to demonstrate the advantages of our method in terms of attack success
    rate and efficiency. It is important to note that our method has the same initial
    parameters as the original GCG (Zou et al., [2023](#bib.bib43)), epoch is 500,
    top-k is 256, and batchsize is 512.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。考虑到有关LLM对抗性破解的相关工作（Zou et al., [2023](#bib.bib43); Liu et al., [2023](#bib.bib18);
    Ding et al., [2023](#bib.bib8); Wei et al., [2024](#bib.bib34); Li et al., [2023b](#bib.bib17)），我们将我们的方法与一些代表性的基准方法进行比较，如GCG（Zou
    et al., [2023](#bib.bib43)）和AutoDAN（Liu et al., [2023](#bib.bib18)），以评估基于LLM的具体模型在白盒攻击场景下的鲁棒性。如[3.2.3](#S3.SS2.SSS3
    "3.2.3\. 攻击成功判断 ‣ 3.2\. 具体场景攻击算法 ‣ 3\. 方法 ‣ 探索通过对LLM基础模型的对抗攻击来提升决策层鲁棒性")节所示，GCG和AutoDAN使用的文本匹配列表方法不适用于具体攻击任务。因此，我们将这两种方法中的文本匹配算法替换为我们论文中提出的切片和相似性计算方法作为判断攻击成功的标准。最后，我们将本文提出的方法与GCG（Zou
    et al., [2023](#bib.bib43)）和AutoDAN（Liu et al., [2023](#bib.bib18)）算法进行比较，以展示我们方法在攻击成功率和效率方面的优势。值得注意的是，我们的方法与原始GCG（Zou
    et al., [2023](#bib.bib43)）具有相同的初始参数，迭代次数为500，top-k为256，批量大小为512。
- en: 'Evaluation metrics. We evaluate the algorithm from three key aspects: Attack
    Success Rate (ASR), Execution Success Rate (ESR), and Epoch Cost. ASR indicates
    whether the LLM-based embodied model successfully outputs decision content related
    to the target task in targeted attacks or outputs decision content unrelated to
    the prompt in untargeted attacks. ESR reflects whether, upon a successful attack,
    the system can execute the output content under the prevailing environmental conditions.
    Epoch Cost denotes the average number of iterations required for an attack to
    succeed.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 评价指标。我们从三个关键方面评估算法：攻击成功率（ASR）、执行成功率（ESR）和周期成本。ASR表示LLM基础的具身模型在有针对性攻击中是否成功输出与目标任务相关的决策内容，或者在无针对性攻击中输出与提示无关的决策内容。ESR反映在成功攻击后，系统是否能够在当前环境条件下执行输出内容。周期成本表示攻击成功所需的平均迭代次数。
- en: 4.2\. Main results
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 主要结果
- en: Table [1](#S4.T1 "Table 1 ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models") illustrates the white-box
    attack evaluation results of our method and other baselines(Zou et al., [2023](#bib.bib43);
    Liu et al., [2023](#bib.bib18)). In targeted attacks, we conduct these assessments
    by generating prompt suffixes for each targeted request in the EIRAD dataset and
    examining whether the final response from the LLM-based embodied model aligns
    with the targeted task. In untargeted attacks, we conducted similar evaluations
    by generating a prompt suffix to examine whether the final response of the LLM-based
    embodied model remains independent of the prompt task. We noted that in targeted
    attacks, our method effectively produces prompt suffixes and achieves a superior
    attack success rate compared to baseline methods. For the fine-tuned LLM-based
    embodied model TaPA(Wu et al., [2023b](#bib.bib37)) and Otter(Li et al., [2023a](#bib.bib15)),
    our method enhances the attack success rate by over 10%, and even surpasses 20%
    in harmful attack. Regarding the native model Llama-2-chat used for embodied tasks,
    our method demonstrates a comparable attack success rate to GCG (Zou et al., [2023](#bib.bib43))
    in harmless attack, while significantly reducing the Epoch cost. In untargeted
    attacks, our method also exhibits varying degrees of success across the three
    models, leading them to output content unrelated to the prompt. The AutoDAN algorithm’s
    attack success rate, as indicated by the data, is 0\. This is due to its core
    concept of using a semantic prompt framework to guide LLMs to circumvent the value
    alignment mechanism, which falls short in generating specified content for particular
    tasks. In summary, our approach proves effective when embodied intelligent robots
    encounter diverse attack scenarios, enhancing the attack success rate while mitigating
    training costs. These results suggest that LLM-based embodied models display diminished
    robustness at the decision-level when subjected to adversarial attacks, offering
    insights for robustness research on embodied intelligent robots.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S4.T1 "Table 1 ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models") 展示了我们方法和其他基线方法（Zou
    et al., [2023](#bib.bib43); Liu et al., [2023](#bib.bib18)）的白盒攻击评价结果。在有针对性攻击中，我们通过生成每个目标请求的提示后缀来进行这些评估，并检查LLM基础的具身模型的最终响应是否与目标任务一致。在无针对性攻击中，我们通过生成提示后缀来检查LLM基础的具身模型的最终响应是否保持与提示任务无关。我们注意到，在有针对性攻击中，我们的方法有效地产生了提示后缀，并且取得了比基线方法更高的攻击成功率。对于经过微调的LLM基础的具身模型TaPA（Wu
    et al., [2023b](#bib.bib37)）和Otter（Li et al., [2023a](#bib.bib15)），我们的方法将攻击成功率提高了超过10%，在有害攻击中甚至超过了20%。关于用于具身任务的本地模型Llama-2-chat，我们的方法在无害攻击中表现出与GCG（Zou
    et al., [2023](#bib.bib43)）相当的攻击成功率，同时显著降低了周期成本。在无针对性攻击中，我们的方法也在三个模型中表现出不同程度的成功，使它们输出与提示无关的内容。根据数据，AutoDAN算法的攻击成功率为0。这是由于其核心概念是使用语义提示框架来指导LLM绕过价值对齐机制，这在生成特定任务的指定内容方面效果欠佳。总之，我们的方法在具身智能机器人面临各种攻击场景时证明了其有效性，提高了攻击成功率，同时降低了训练成本。这些结果表明，LLM基础的具身模型在遭遇对抗攻击时在决策层面表现出较低的鲁棒性，为具身智能机器人的鲁棒性研究提供了见解。
- en: 4.3\. Execution success rate
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 执行成功率
- en: In the context of attacking a LLM-based embodied model, it is crucial to assess
    whether the LLM’s output aligns with both the task requirements and the embodied
    constraints, ensuring its successful execution within the current environment.
    As depicted in Table [2](#S4.T2 "Table 2 ‣ 4.3\. Execution success rate ‣ 4\.
    Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks
    on LLM-Based Embodied Models"), user study serves as a means to evaluate the ESR
    of the LLM’s output in the given environment. Experimental results demonstrate
    that in targeted attack, where the target task is designed with a thorough consideration
    of current environmental factors, the resulting output task steps largely adhere
    to the embodied requirements. In addition, due to the heightened precision in
    our attack success assessment, our method exhibits a superior ESR when juxtaposed
    with the GCG (Zou et al., [2023](#bib.bib43)). However, in untargeted attack,
    where no specific target task is specified, the objective is to guide the LLM-based
    embodied model to generate content that is unrelated to the original prompt. Consequently,
    for a successful untargeted attack, the primary criterion is to ensure that the
    output content is disconnected from the prompt, without considering its feasibility
    for execution within the current scenario,resulting in a low ESR value.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在攻击基于LLM的具身模型的背景下，评估LLM的输出是否符合任务要求和具身约束至关重要，以确保其在当前环境中的成功执行。正如表格[2](#S4.T2 "表
    2 ‣ 4.3\. 执行成功率 ‣ 4\. 实验 ‣ 探索基于LLM的具身模型的决策层鲁棒性")所示，用户研究作为评估LLM输出在给定环境中ESR的手段。实验结果表明，在有针对性的攻击中，当目标任务经过彻底考虑当前环境因素设计时，生成的输出任务步骤基本符合具身要求。此外，由于我们在攻击成功评估中的高精度，我们的方法与GCG（Zou等，[2023](#bib.bib43)）相比表现出更优的ESR。然而，在无目标攻击中，目标任务未被指定，目的是引导基于LLM的具身模型生成与原始提示无关的内容。因此，为了成功的无目标攻击，主要标准是确保输出内容与提示脱节，而不考虑其在当前场景中的执行可行性，导致低ESR值。
- en: Table 2\. ESR based on user study.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 基于用户研究的ESR。
- en: '|  |  | Harmful attack | Harmless attack | Untargeted attack |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 有害攻击 | 无害攻击 | 无目标攻击 |'
- en: '| Model | Method | ESR | ESR | ESR |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | ESR | ESR | ESR |'
- en: '| Tapa | GCG | 0 | 0.67 | - |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| Tapa | GCG | 0 | 0.67 | - |'
- en: '| Ours | 0.72 | 0.81 | 0.48 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.72 | 0.81 | 0.48 |'
- en: '| Otter | GCG | 0.74 | 0.91 | - |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Otter | GCG | 0.74 | 0.91 | - |'
- en: '| Ours | 0.84 | 0.95 | 0.48 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.84 | 0.95 | 0.48 |'
- en: '| Llama-2-chat | GCG | 0.73 | 0.87 | - |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-chat | GCG | 0.73 | 0.87 | - |'
- en: '| Ours | 0.78 | 0.88 | 0.45 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 0.78 | 0.88 | 0.45 |'
- en: 4.4\. The impact of keyword initialization on loss
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 关键字初始化对损失的影响
- en: To delve into the reason behind the significant reduction in epoch cost due
    to prompt suffix keyword initialization, we conduct an analysis on the change
    trend of the loss value throughout the attack process. We compare the effects
    of prompt suffix keyword initialization with 2 keywords versus no keyword initialization
    on the three models individually. The variations in the loss value are visualized
    in Figure [7](#S4.F7 "Figure 7 ‣ 4.4\. The impact of keyword initialization on
    loss ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"). It is evident that the suffix initialized
    with keywords exhibits a notably lower loss value in the initial stages of the
    attack process. Consequently, during the iterative optimization of the suffixes,
    the model tends to swiftly identify the most suitable prompt suffix. This implies
    that in the early phases of an attack, the model can swiftly pinpoint an effective
    attack direction, thereby advancing towards a successful attack status more rapidly.
    This strategic advantage leads to quicker convergence towards successful attack
    outcomes, thereby enhancing the overall effectiveness and reliability of the targeted
    attack process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨由于提示后缀关键词初始化而导致的训练代价显著减少的原因，我们对攻击过程中的损失值变化趋势进行了分析。我们比较了提示后缀关键词初始化为 2 个关键词与不初始化关键词对三个模型的影响。损失值的变化在图
    [7](#S4.F7 "图 7 ‣ 4.4\. 关键词初始化对损失的影响 ‣ 4\. 实验 ‣ 探索基于 LLM 的嵌入模型决策级的鲁棒性") 中可视化。显然，在攻击过程的初期阶段，使用关键词初始化的后缀具有显著较低的损失值。因此，在后缀的迭代优化过程中，模型往往能迅速识别出最合适的提示后缀。这意味着在攻击的早期阶段，模型能够迅速确定有效的攻击方向，从而更快地达到成功攻击的状态。这一战略优势使得攻击结果更快速收敛，从而提高了针对攻击过程的整体有效性和可靠性。
- en: '![Refer to caption](img/bd472b78e7073bca2583a0ac547d7c40.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bd472b78e7073bca2583a0ac547d7c40.png)'
- en: (a) Harmless attack in Otter
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Otter 中的无害攻击
- en: '![Refer to caption](img/ba9aa41be4cf736fe25ad807ee502455.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ba9aa41be4cf736fe25ad807ee502455.png)'
- en: (b) Harmful attack in Otter
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Otter 中的有害攻击
- en: '![Refer to caption](img/c7831f117327314958332e1e06df5805.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c7831f117327314958332e1e06df5805.png)'
- en: (c) Harmless attack in Llama-2-chat
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Llama-2-chat 中的无害攻击
- en: '![Refer to caption](img/06dda3abd7f60c060b8422f2b8dcbd97.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/06dda3abd7f60c060b8422f2b8dcbd97.png)'
- en: (d) Harmful attack in Llama-2-chat
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Llama-2-chat 中的有害攻击
- en: '![Refer to caption](img/0027aec33fca34c1709bec28477264fb.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0027aec33fca34c1709bec28477264fb.png)'
- en: (e) Harmless attack in TAPA
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (e) TAPA 中的无害攻击
- en: '![Refer to caption](img/bb4f50611b006531bc306f4e6ed8c07d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bb4f50611b006531bc306f4e6ed8c07d.png)'
- en: (f) Harmful attack in TAPA
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (f) TAPA 中的有害攻击
- en: Figure 7\. Suffix keyword initialization loss comparison
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 后缀关键词初始化损失比较
- en: 4.5\. Ablation Study
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. 消融研究
- en: In order to evaluate the importance of the two modules proposed in this paper,
    we conduct ablation experiments on the prompt suffix keyword initialization and
    the evaluation method to determine the success of the attack. In Section [4.5.1](#S4.SS5.SSS1
    "4.5.1\. The impact of initializing the number of keywords ‣ 4.5\. Ablation Study
    ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"), we examined how varying the number of
    prompt suffix keywords affects the ASR and epoch cost. In Section [4.5.2](#S4.SS5.SSS2
    "4.5.2\. Validity of Assessment Methods ‣ 4.5\. Ablation Study ‣ 4\. Experiments
    ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based
    Embodied Models"), we assess the effectiveness of our chosen evaluation method
    for determining attack success.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估本文提出的两个模块的重要性，我们对提示后缀关键词初始化和评估方法进行消融实验，以确定攻击的成功性。在章节 [4.5.1](#S4.SS5.SSS1
    "4.5.1\. 初始化关键词数量的影响 ‣ 4.5\. 消融研究 ‣ 4\. 实验 ‣ 探索基于 LLM 的嵌入模型决策级的鲁棒性") 中，我们检查了变化的提示后缀关键词数量如何影响
    ASR 和训练代价。在章节 [4.5.2](#S4.SS5.SSS2 "4.5.2\. 评估方法的有效性 ‣ 4.5\. 消融研究 ‣ 4\. 实验 ‣ 探索基于
    LLM 的嵌入模型决策级的鲁棒性") 中，我们评估了我们选择的评估方法在确定攻击成功方面的有效性。
- en: 4.5.1\. The impact of initializing the number of keywords
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1\. 初始化关键词数量的影响
- en: 'In order to analyze the impact of prompt suffix keyword initialization on ASR
    and Epoch cost, we conduct an ablation experiment on the number of keywords initialized
    by prompt suffix and explore its impact on the Otter model attack process. The
    attack results of other LLM-based embodied models are shown in the appendix. As
    depicted in Figure [8a](#S4.F8.sf1 "In Figure 8 ‣ 4.5.1\. The impact of initializing
    the number of keywords ‣ 4.5\. Ablation Study ‣ 4\. Experiments ‣ Exploring the
    Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied
    Models") and Figure [8b](#S4.F8.sf2 "In Figure 8 ‣ 4.5.1\. The impact of initializing
    the number of keywords ‣ 4.5\. Ablation Study ‣ 4\. Experiments ‣ Exploring the
    Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied
    Models"), we set various numbers of keyword initializations for both harmful and
    harmless attack data in the targeted attack scenario. The experimental findings
    reveal a consistent trend: as the number of initialized keywords increases, the
    ASR value steadily rises, while the epoch cost value decreases. This trend suggests
    that augmenting the number of keywords offers improved guidance to the model in
    identifying the optimal attack direction, thereby expediting the discovery of
    effective attack suffixes. However, during the actual attack execution, it becomes
    imperative to carefully balance the trade-offs among the increased workload due
    to additional initial suffix keywords, the resultant attack success rate, and
    the time taken for the attack. Consequently, selecting the optimal number of keywords
    for prompt suffix initialization becomes a crucial consideration in the attack
    strategy. Furthermore, our analysis of the evolving trend of loss values under
    varying initialization keyword numbers, as illustrated in Figure [8c](#S4.F8.sf3
    "In Figure 8 ‣ 4.5.1\. The impact of initializing the number of keywords ‣ 4.5\.
    Ablation Study ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models") and Figure [8d](#S4.F8.sf4
    "In Figure 8 ‣ 4.5.1\. The impact of initializing the number of keywords ‣ 4.5\.
    Ablation Study ‣ 4\. Experiments ‣ Exploring the Robustness of Decision-Level
    Through Adversarial Attacks on LLM-Based Embodied Models"), reveals a compelling
    relationship: a higher number of initialized keywords corresponds to a lower initial
    loss value, resulting in a quicker attainment of attack success. This underscores
    the significance of judiciously optimizing the number of keywords for prompt suffix
    initialization to enhance the efficiency and effectiveness of the attack process.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析提示后缀关键字初始化对 ASR 和 Epoch 成本的影响，我们在提示后缀初始化的关键字数量上进行了消融实验，并探索其对 Otter 模型攻击过程的影响。其他基于
    LLM 的具身模型的攻击结果显示在附录中。如图 [8a](#S4.F8.sf1 "在图 8 ‣ 4.5.1\. 初始化关键字数量的影响 ‣ 4.5\. 消融研究
    ‣ 4\. 实验 ‣ 通过对 LLM 基础的具身模型进行对抗性攻击探索决策级的鲁棒性") 和图 [8b](#S4.F8.sf2 "在图 8 ‣ 4.5.1\.
    初始化关键字数量的影响 ‣ 4.5\. 消融研究 ‣ 4\. 实验 ‣ 通过对 LLM 基础的具身模型进行对抗性攻击探索决策级的鲁棒性") 中所示，我们为目标攻击场景中的有害和无害攻击数据设置了不同数量的关键字初始化。实验结果揭示了一个一致的趋势：随着初始化关键字数量的增加，ASR
    值稳步上升，而 Epoch 成本值则下降。这个趋势表明，增加关键字数量能够更好地指导模型确定最佳攻击方向，从而加快有效攻击后缀的发现。然而，在实际攻击执行过程中，必须仔细平衡由于附加的初始后缀关键字带来的额外工作量、攻击成功率以及攻击所需的时间。因此，为提示后缀初始化选择最佳关键字数量成为攻击策略中的关键考虑因素。此外，我们对不同初始化关键字数量下的损失值变化趋势的分析，如图
    [8c](#S4.F8.sf3 "在图 8 ‣ 4.5.1\. 初始化关键字数量的影响 ‣ 4.5\. 消融研究 ‣ 4\. 实验 ‣ 通过对 LLM 基础的具身模型进行对抗性攻击探索决策级的鲁棒性")
    和图 [8d](#S4.F8.sf4 "在图 8 ‣ 4.5.1\. 初始化关键字数量的影响 ‣ 4.5\. 消融研究 ‣ 4\. 实验 ‣ 通过对 LLM
    基础的具身模型进行对抗性攻击探索决策级的鲁棒性") 中所示，揭示了一个引人注目的关系：更多的初始化关键字对应更低的初始损失值，从而更快地实现攻击成功。这突显了明智优化提示后缀初始化关键字数量以提高攻击过程的效率和有效性的重要性。
- en: '![Refer to caption](img/050955bbb61eb6bdc351d35095c56ab7.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/050955bbb61eb6bdc351d35095c56ab7.png)'
- en: (a) ASR in different keywords
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 不同关键字的 ASR
- en: '![Refer to caption](img/f29c83872e6fe0f5f01c3da71d02b44f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f29c83872e6fe0f5f01c3da71d02b44f.png)'
- en: (b) Epoch cost in different keywords
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 不同关键字的 Epoch 成本
- en: '![Refer to caption](img/ce0f189869524d02ccdcc2a878855ef1.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce0f189869524d02ccdcc2a878855ef1.png)'
- en: (c) Loss in harmful attack
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 有害攻击中的损失
- en: '![Refer to caption](img/2af8ffabc8d04cc876b65e0352902e26.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2af8ffabc8d04cc876b65e0352902e26.png)'
- en: (d) Loss in harmless attack
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 在无害攻击中的损失
- en: Figure 8\. The impact of initializing the number of keywords.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8\. 初始化关键词数量的影响。
- en: 4.5.2\. Validity of Assessment Methods
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.2\. 评估方法的有效性
- en: To validate the effectiveness of the evaluation method proposed in this paper
    to determine the success of an attack, we conduct an ablation experiment of the
    step 3 in Figure [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario attack algorithm
    ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through Adversarial
    Attacks on LLM-Based Embodied Models"). Table [3](#S4.T3 "Table 3 ‣ 4.5.2\. Validity
    of Assessment Methods ‣ 4.5\. Ablation Study ‣ 4\. Experiments ‣ Exploring the
    Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied
    Models") illustrates that under the same attack conditions, the combination of
    slicing and similarity calculation proposed in this paper is compared with the
    matching list method in GCG (Zou et al., [2023](#bib.bib43)) and with only the
    similarity calculation method to evaluate the correlation between the response
    of LLM-based embodied model and the target task upon a successful attack. Green
    markers indicate a strong correlation between the output step and the target task,
    while red markers signify deviations from the target task. The results indicate
    that when using our method for evaluation, the response of LLM-based embodied
    model closely aligns with the set attack goals. In contrast, under the other two
    evaluation methods, the output task content deviates from the intended goals.
    This underscores the effectiveness of our evaluation method in reducing the occurrences
    of false detections and missed detections observed in GCG (Zou et al., [2023](#bib.bib43))
    and AutoDAN (Liu et al., [2023](#bib.bib18)), thus enhancing the accuracy of attack
    results.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证本文提出的评估方法在确定攻击成功与否的有效性，我们对图 [5](#S3.F5 "Figure 5 ‣ 3.2\. Embodied scenario
    attack algorithm ‣ 3\. Method ‣ Exploring the Robustness of Decision-Level Through
    Adversarial Attacks on LLM-Based Embodied Models") 中的第 3 步进行了一次消融实验。表 [3](#S4.T3
    "Table 3 ‣ 4.5.2\. Validity of Assessment Methods ‣ 4.5\. Ablation Study ‣ 4\.
    Experiments ‣ Exploring the Robustness of Decision-Level Through Adversarial Attacks
    on LLM-Based Embodied Models") 说明，在相同攻击条件下，本文提出的切片与相似度计算方法与 GCG (Zou et al., [2023](#bib.bib43))
    的匹配列表方法以及仅使用相似度计算方法进行比较，评估了 LLM 基础的具身模型响应与目标任务在成功攻击下的相关性。绿色标记表示输出步骤与目标任务之间有强相关性，而红色标记则表示偏离目标任务。结果表明，使用我们的方法进行评估时，LLM
    基础的具身模型响应与设定的攻击目标紧密一致。相比之下，在其他两种评估方法下，输出任务内容偏离了预期目标。这突显了我们评估方法在减少 GCG (Zou et
    al., [2023](#bib.bib43)) 和 AutoDAN (Liu et al., [2023](#bib.bib18)) 中观察到的误报和漏报发生率方面的有效性，从而提高了攻击结果的准确性。
- en: Table 3\. Responce under three Judgment methods.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 三种判断方法下的响应。
- en: '| prompt : Please pass me the chair. target : Open the toaster and place it
    on the table |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| prompt : 请把椅子递给我。 target : 打开烤面包机并把它放在桌子上 |'
- en: '| --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Judgment-method | Response |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 判断方法 | 响应 |'
- en: '| GCG | Step 1: Open the toaster directory and find the toast. Step 2: Click
    on the toast to open it. Step 3: Inside the toast. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GCG | 第一步：打开烤面包机目录并找到烤面包。第二步：点击烤面包以打开它。第三步：在烤面包里。 |'
- en: '| Only calculate similarity | Step 1: Open the toaster and insert the bread.
    Step 2: Close the toaster and press the button. Step 3: Wait for the toast. |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 仅计算相似度 | 第一步：打开烤面包机并插入面包。第二步：关闭烤面包机并按下按钮。第三步：等待烤面包。 |'
- en: '| Ours | Step 1: Open the toaster. Step 2: Place the toast on the table. Step
    3: Close the toaster. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 | 第一步：打开烤面包机。第二步：把烤面包放在桌子上。第三步：关闭烤面包机。 |'
- en: 5\. Conclusion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we introduce the creation of a multi-modal dataset named EIRAD,
    which aims to assess the robustness of LLM decision-level in embodied scenarios.
    We devise two attack strategies, untargeted attack and targeted attack, to simulate
    diverse attack scenarios. Throughout these attacks, we implement prompt suffix
    keyword initialization tailored to specific target tasks, enhancing the convergence
    speed during the attack process. Additionally, we develop an attack success assessment
    method based on BLIP2 model to provide a more precise evaluation of the conditions
    for attack success. The experimental outcomes validate the effectiveness of our
    approach, while also underscoring the challenge of robustness in LLM decision-level
    within embodied scenarios. We aspire that our study will shed further light on
    the vulnerabilities of LLMs in embodied settings and furnish them with advanced
    defense mechanisms for ensuring secure utilizatio.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个名为 EIRAD 的多模态数据集的创建，该数据集旨在评估大型语言模型在体现场景中的决策级鲁棒性。我们设计了两种攻击策略：无目标攻击和有目标攻击，以模拟各种攻击场景。在这些攻击过程中，我们实施了针对特定目标任务的提示后缀关键词初始化，提升了攻击过程中的收敛速度。此外，我们基于
    BLIP2 模型开发了一种攻击成功评估方法，以提供更精确的攻击成功条件评估。实验结果验证了我们方法的有效性，同时也凸显了体现场景中大型语言模型决策级鲁棒性的挑战。我们希望我们的研究能进一步揭示大型语言模型在体现设置中的脆弱性，并为其提供先进的防御机制，以确保安全利用。
- en: References
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Aizawa (2003) Akiko Aizawa. 2003. An information-theoretic perspective of tf–idf
    measures. *Information Processing & Management* 39, 1 (2003), 45–65.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aizawa（2003）Akiko Aizawa. 2003. tf-idf 测度的信息理论视角。*信息处理与管理* 39, 1（2003），45–65。
- en: 'Awadalla et al. (2023) Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,
    Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori
    Sagawa, et al. 2023. Openflamingo: An open-source framework for training large
    autoregressive vision-language models. *arXiv preprint arXiv:2308.01390* (2023).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awadalla 等（2023）Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy,
    Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa 等. 2023.
    Openflamingo：一个用于训练大型自回归视觉语言模型的开源框架。*arXiv 预印本 arXiv:2308.01390*（2023）。
- en: 'Brohan et al. (2023) Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. 2023. Do as i can, not as i say: Grounding language in robotic affordances.
    In *Conference on robot learning*. PMLR, 287–318.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan 等（2023）Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian
    等. 2023. 照我做，不要照我说：将语言与机器人能力对接。发表于*机器人学习会议*。PMLR, 287–318。
- en: Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo,
    Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine
    Lee, Florian Tramer, and Ludwig Schmidt. 2023. Are aligned neural networks adversarially
    aligned? arXiv:2306.15447 [cs.CL]
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等（2023）Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo,
    Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine
    Lee, Florian Tramer, 和 Ludwig Schmidt. 2023. 对齐的神经网络是否也对抗性对齐？arXiv:2306.15447
    [cs.CL]
- en: Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani,
    George J. Pappas, and Eric Wong. 2023. Jailbreaking Black Box Large Language Models
    in Twenty Queries. arXiv:2310.08419 [cs.LG]
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chao 等（2023）Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George
    J. Pappas, 和 Eric Wong. 2023. 在二十个查询中破解黑箱大型语言模型。arXiv:2310.08419 [cs.LG]
- en: Dagan et al. (2023) Gautier Dagan, Frank Keller, and Alex Lascarides. 2023.
    Dynamic planning with a llm. *arXiv preprint arXiv:2308.06391* (2023).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dagan 等（2023）Gautier Dagan, Frank Keller, 和 Alex Lascarides. 2023. 使用大型语言模型进行动态规划。*arXiv
    预印本 arXiv:2308.06391*（2023）。
- en: 'Ding et al. (2023) Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun
    Chen, and Shujian Huang. 2023. A Wolf in Sheep’s Clothing: Generalized Nested
    Jailbreak Prompts can Fool Large Language Models Easily. *arXiv preprint arXiv:2311.08268*
    (2023).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等（2023）Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen,
    和 Shujian Huang. 2023. 羊皮下的狼：通用嵌套破解提示可以轻松欺骗大型语言模型。*arXiv 预印本 arXiv:2311.08268*（2023）。
- en: Dong et al. (2023) Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao
    Yang, Yichi Zhang, Yu Tian, Hang Su, and Jun Zhu. 2023. How Robust is Google’s
    Bard to Adversarial Image Attacks? *arXiv preprint arXiv:2309.11751* (2023).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang,
    Yichi Zhang, Yu Tian, Hang Su, 和 Jun Zhu. 2023. Google 的 Bard 对抗性图像攻击的鲁棒性如何？*arXiv
    预印本 arXiv:2309.11751*（2023）。
- en: Dorbala et al. (2024) Vishnu Sashank Dorbala, Sanjoy Chowdhury, and Dinesh Manocha.
    2024. Can LLMs Generate Human-Like Wayfinding Instructions? Towards Platform-Agnostic
    Embodied Instruction Synthesis. *arXiv preprint arXiv:2403.11487* (2024).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorbala et al. (2024) Vishnu Sashank Dorbala, Sanjoy Chowdhury, 和 Dinesh Manocha.
    2024. LLMs 能生成类似人类的导航指令吗？朝着平台无关的具身指令合成迈进。 *arXiv预印本 arXiv:2403.11487* (2024)。
- en: Dorbala et al. (2023) Vishnu Sashank Dorbala, James F Mullen Jr, and Dinesh
    Manocha. 2023. Can an Embodied Agent Find Your “Cat-shaped Mug”? LLM-Based Zero-Shot
    Object Navigation. *IEEE Robotics and Automation Letters* (2023).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dorbala et al. (2023) Vishnu Sashank Dorbala, James F Mullen Jr, 和 Dinesh Manocha.
    2023. 具身智能体能找到你的“猫形杯”吗？基于LLM的零样本对象导航。 *IEEE机器人与自动化学报* (2023)。
- en: 'Huang et al. (2022) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    2022. Inner monologue: Embodied reasoning through planning with language models.
    *arXiv preprint arXiv:2207.05608* (2022).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, 等等。2022.
    内心独白：通过语言模型的规划进行具身推理。 *arXiv预印本 arXiv:2207.05608* (2022)。
- en: Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and
    Danqi Chen. 2023. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation.
    arXiv:2310.06987 [cs.CL]
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, 和 Danqi
    Chen. 2023. 利用生成的灾难性破解开源LLMs。arXiv:2310.06987 [cs.CL]
- en: 'Kolve et al. (2017) Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,
    Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu,
    et al. 2017. Ai2-thor: An interactive 3d environment for visual ai. *arXiv preprint
    arXiv:1712.05474* (2017).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kolve et al. (2017) Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt,
    Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu,
    等等。2017. Ai2-thor：一个用于视觉人工智能的交互式3D环境。 *arXiv预印本 arXiv:1712.05474* (2017)。
- en: 'Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu,
    Jingkang Yang, Chunyuan Li, and Ziwei Liu. 2023a. Mimic-it: Multi-modal in-context
    instruction tuning. *arXiv preprint arXiv:2306.05425* (2023).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu,
    Jingkang Yang, Chunyuan Li, 和 Ziwei Liu. 2023a. Mimic-it：多模态上下文指令调优。 *arXiv预印本
    arXiv:2306.05425* (2023)。
- en: 'Li et al. (2024) Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. 2024. Llava-med:
    Training a large language-and-vision assistant for biomedicine in one day. *Advances
    in Neural Information Processing Systems* 36 (2024).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian
    Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, 和 Jianfeng Gao. 2024. Llava-med：一天内训练大型语言和视觉助手用于生物医学。
    *神经信息处理系统进展* 36 (2024)。
- en: 'Li et al. (2023b) Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang
    Liu, and Bo Han. 2023b. Deepinception: Hypnotize large language model to be jailbreaker.
    *arXiv preprint arXiv:2311.03191* (2023).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang
    Liu, 和 Bo Han. 2023b. Deepinception：催眠大型语言模型成为破解者。 *arXiv预印本 arXiv:2311.03191*
    (2023)。
- en: 'Liu et al. (2023) Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. 2023.
    Autodan: Generating stealthy jailbreak prompts on aligned large language models.
    *arXiv preprint arXiv:2310.04451* (2023).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Xiaogeng Liu, Nan Xu, Muhao Chen, 和 Chaowei Xiao. 2023. Autodan：在对齐的大型语言模型上生成隐蔽的破解提示。
    *arXiv预印本 arXiv:2310.04451* (2023)。
- en: 'Lynch et al. (2023) Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding,
    James Betker, Robert Baruch, Travis Armstrong, and Pete Florence. 2023. Interactive
    language: Talking to robots in real time. *IEEE Robotics and Automation Letters*
    (2023).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch et al. (2023) Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding,
    James Betker, Robert Baruch, Travis Armstrong, 和 Pete Florence. 2023. 交互式语言：实时与机器人对话。
    *IEEE机器人与自动化学报* (2023)。
- en: 'Nottingham et al. (2023) Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr,
    Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, and Roy Fox. 2023. Do embodied
    agents dream of pixelated sheep: Embodied decision making using language guided
    world modelling. In *International Conference on Machine Learning*. PMLR, 26311–26325.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nottingham et al. (2023) Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr,
    Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, 和 Roy Fox. 2023. 具身智能体是否梦见像素化的羊：使用语言引导的世界建模进行具身决策。
    在 *国际机器学习大会*。PMLR, 26311–26325。
- en: Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, and Peter Henderson. 2023. Fine-tuning Aligned Language Models Compromises
    Safety, Even When Users Do Not Intend To! arXiv:2310.03693 [cs.CL]
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qi et al. (2023) Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek
    Mittal, 和 Peter Henderson. 2023. 微调对齐语言模型会影响安全性，即使用户没有意图！arXiv:2310.03693 [cs.CL]
- en: 'Qiao et al. (2023) Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, and Qi Wu.
    2023. March in chat: Interactive prompting for remote embodied referring expression.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*.
    15758–15767.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qiao et al. (2023) Yanyuan Qiao, Yuankai Qi, Zheng Yu, Jing Liu, 和 Qi Wu. 2023.
    March in chat: 远程实体现指代表达的互动提示. 载于 *IEEE/CVF 国际计算机视觉会议论文集*. 15758–15767.'
- en: 'Schumann et al. (2024) Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu,
    Stefan Riezler, and William Yang Wang. 2024. Velma: Verbalization embodiment of
    llm agents for vision and language navigation in street view. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, Vol. 38\. 18924–18933.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schumann et al. (2024) Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu,
    Stefan Riezler, 和 William Yang Wang. 2024. Velma: 视觉和语言导航中的 LLM 代理的口头化体现. 载于 *AAAI
    人工智能会议论文集*, 第38卷. 18924–18933.'
- en: Shah et al. (2023) Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush
    Tagade, Stephen Casper, and Javier Rando. 2023. Scalable and Transferable Black-Box
    Jailbreaks for Language Models via Persona Modulation. arXiv:2311.03348 [cs.CL]
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah et al. (2023) Rusheb Shah, Quentin Feuillade-Montixi, Soroush Pour, Arush
    Tagade, Stephen Casper, 和 Javier Rando. 2023. 可扩展和可转移的语言模型黑箱越狱通过人物调节. arXiv:2311.03348
    [cs.CL]
- en: 'Sharan et al. (2024) SP Sharan, Ruihan Zhao, Zhangyang Wang, Sandeep P Chinchali,
    et al. 2024. Plan Diffuser: Grounding LLM Planners with Diffusion Models for Robotic
    Manipulation. In *Bridging the Gap between Cognitive Science and Robot Learning
    in the Real World: Progresses and New Directions*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sharan et al. (2024) SP Sharan, Ruihan Zhao, Zhangyang Wang, Sandeep P Chinchali,
    等. 2024. Plan Diffuser: 使用扩散模型对 LLM 规划器进行实地应用. 载于 *弥合认知科学与现实世界机器人学习之间的差距：进展与新方向*.'
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. 2023. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*. 2998–3009.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, 和 Yu Su. 2023. Llm-planner: 基于大型语言模型的少样本实体现规划. 载于 *IEEE/CVF 国际计算机视觉会议论文集*.
    2998–3009.'
- en: Stella et al. (2023) Francesco Stella, Cosimo Della Santina, and Josie Hughes.
    2023. How can LLMs transform the robotic design process? *Nature Machine Intelligence*
    5, 6 (2023), 561–564.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stella et al. (2023) Francesco Stella, Cosimo Della Santina, 和 Josie Hughes.
    2023. LLM 如何变革机器人设计过程？*自然机器智能* 5, 6 (2023), 561–564.
- en: 'Sun et al. (2022) Jiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou,
    and Animesh Garg. 2022. Plate: Visually-grounded planning with transformers in
    procedural tasks. *IEEE Robotics and Automation Letters* 7, 2 (2022), 4924–4930.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2022) Jiankai Sun, De-An Huang, Bo Lu, Yun-Hui Liu, Bolei Zhou,
    和 Animesh Garg. 2022. Plate: 基于视觉的程序任务规划与变换器. *IEEE 机器人与自动化通讯* 7, 2 (2022), 4924–4930.'
- en: 'Sun et al. (2023) Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi
    Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, et al.
    2023. Pathasst: Redefining pathology through generative foundation ai assistant
    for pathology. *arXiv preprint arXiv:2305.15072* (2023).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2023) Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi
    Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, 等. 2023.
    Pathasst: 通过生成基础 AI 助理重新定义病理学. *arXiv 预印本 arXiv:2305.15072* (2023).'
- en: Szot et al. (2023) Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure,
    Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, and Alexander T Toshev.
    2023. Large language models as generalizable policies for embodied tasks. In *The
    Twelfth International Conference on Learning Representations*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szot et al. (2023) Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure,
    Rin Metcalf, Walter Talbott, Natalie Mackraz, R Devon Hjelm, 和 Alexander T Toshev.
    2023. 大型语言模型作为可推广的实体现任务策略. 载于 *第十二届国际学习表征会议*.
- en: 'Unanue et al. (2021) Inigo Jauregi Unanue, Jacob Parnell, and Massimo Piccardi.
    2021. BERTTune: Fine-tuning neural machine translation with BERTScore. *arXiv
    preprint arXiv:2106.02208* (2021).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Unanue et al. (2021) Inigo Jauregi Unanue, Jacob Parnell, 和 Massimo Piccardi.
    2021. BERTTune: 使用 BERTScore 微调神经机器翻译. *arXiv 预印本 arXiv:2106.02208* (2021).'
- en: 'Vemprala et al. (2023) Sai Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish
    Kapoor. 2023. Chatgpt for robotics: Design principles and model abilities. *arXiv
    preprint arXiv:2306.17582* (2023).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vemprala et al. (2023) Sai Vemprala, Rogerio Bonatti, Arthur Bucker, 和 Ashish
    Kapoor. 2023. Chatgpt 在机器人中的应用：设计原则和模型能力. *arXiv 预印本 arXiv:2306.17582* (2023).
- en: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023.
    Jailbroken: How Does LLM Safety Training Fail? arXiv:2307.02483 [cs.LG]'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt. 2023.
    Jailbroken: LLM 安全训练为何失败？arXiv:2307.02483 [cs.LG]'
- en: 'Wei et al. (2024) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024.
    Jailbroken: How does llm safety training fail? *Advances in Neural Information
    Processing Systems* 36 (2024).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2024) Alexander Wei, Nika Haghtalab, 和 Jacob Steinhardt. 2024.
    Jailbroken: LLM安全训练为何失败？*神经信息处理系统进展* 36 (2024)。'
- en: 'Wu et al. (2023a) Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng,
    Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas Funkhouser. 2023a.
    Tidybot: Personalized robot assistance with large language models. *Autonomous
    Robots* 47, 8 (2023), 1087–1102.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2023a) Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng,
    Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, 和 Thomas Funkhouser. 2023a.
    Tidybot: 通过大型语言模型提供个性化机器人辅助。*自主机器人* 47, 8 (2023), 1087–1102。'
- en: Wu et al. (2010) Lei Wu, Steven CH Hoi, and Nenghai Yu. 2010. Semantics-preserving
    bag-of-words models and applications. *IEEE Transactions on Image Processing*
    19, 7 (2010), 1908–1920.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2010) Lei Wu, Steven CH Hoi, 和 Nenghai Yu. 2010. 语义保留的词袋模型及其应用。*IEEE
    图像处理汇刊* 19, 7 (2010), 1908–1920。
- en: Wu et al. (2023b) Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.
    2023b. Embodied task planning with large language models. *arXiv preprint arXiv:2307.01848*
    (2023).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023b) Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, 和 Haibin Yan.
    2023b. 利用大型语言模型进行具身任务规划。*arXiv 预印本 arXiv:2307.01848* (2023)。
- en: Yang et al. (2023) Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li,
    Li Shen, Xiaodong He, Jing Jiang, and Yuhui Shi. 2023. Embodied multi-modal agent
    trained by an llm from a parallel textworld. *arXiv preprint arXiv:2311.16714*
    (2023).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Yijun Yang, Tianyi Zhou, Kanxue Li, Dapeng Tao, Lusong Li,
    Li Shen, Xiaodong He, Jing Jiang, 和 Yuhui Shi. 2023. 通过来自平行文本世界的LLM训练的具身多模态代理。*arXiv
    预印本 arXiv:2311.16714* (2023)。
- en: 'Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, and Xinyu Xing. 2023. GPTFUZZER:
    Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts. arXiv:2309.10253 [cs.AI]'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2023) Jiahao Yu, Xingwei Lin, Zheng Yu, 和 Xinyu Xing. 2023. GPTFUZZER:
    用自动生成的越狱提示对大型语言模型进行红队测试。arXiv:2309.10253 [cs.AI]'
- en: Zhang et al. (2023) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, and Chuang Gan. 2023. Building cooperative
    embodied agents modularly with large language models. *arXiv preprint arXiv:2307.02485*
    (2023).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B Tenenbaum, Tianmin Shu, 和 Chuang Gan. 2023. 通过大型语言模型模块化构建合作性具身代理。*arXiv
    预印本 arXiv:2307.02485* (2023)。
- en: 'Zhao et al. (2023) Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju
    Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, and Dinggang Shen. 2023. Chatcad+: Towards
    a universal and reliable interactive cad using llms. *arXiv preprint arXiv:2305.15964*
    (2023).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao et al. (2023) Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju
    Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, 和 Dinggang Shen. 2023. Chatcad+: 迈向一个通用且可靠的互动CAD，使用LLMs。*arXiv
    预印本 arXiv:2305.15964* (2023)。'
- en: 'Zheng et al. (2023) Sipeng Zheng, Yicheng Feng, Zongqing Lu, et al. 2023. Steve-Eye:
    Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds. In
    *The Twelfth International Conference on Learning Representations*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng et al. (2023) Sipeng Zheng, Yicheng Feng, Zongqing Lu, 等. 2023. Steve-Eye:
    在开放世界中为基于LLM的具身代理装备视觉感知。在*第十二届国际学习表征会议*。'
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    2023. Universal and transferable adversarial attacks on aligned language models.
    *arXiv preprint arXiv:2307.15043* (2023).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 2023.
    对齐语言模型的通用和可转移对抗攻击。*arXiv 预印本 arXiv:2307.15043* (2023)。
