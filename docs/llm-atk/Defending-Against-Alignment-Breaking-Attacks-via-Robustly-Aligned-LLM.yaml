- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:47:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过稳健对齐的大型语言模型防御对齐破坏攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.14348](https://ar5iv.labs.arxiv.org/html/2309.14348)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.14348](https://ar5iv.labs.arxiv.org/html/2309.14348)
- en: 'Bochuan Cao , Yuanpu Cao¹¹footnotemark: 1 , Lu Lin & Jinghui Chen'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 曹博川 , 曹元普¹¹脚注标记：1 , 林璐 & 陈静辉
- en: The Pennsylvania State University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 宾夕法尼亚州立大学
- en: '{bccao,ymc5533,lulin,jzc5917}@psu.edu Equal Contribution'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{bccao,ymc5533,lulin,jzc5917}@psu.edu 平等贡献'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recently, Large Language Models (LLMs) have made significant advancements and
    are now widely used across various domains. Unfortunately, there has been a rising
    concern that LLMs can be misused to generate harmful or malicious content. Though
    a line of research has focused on aligning LLMs with human values and preventing
    them from producing inappropriate content, such alignments are usually vulnerable
    and can be bypassed by alignment-breaking attacks via adversarially optimized
    or handcrafted jailbreaking prompts. In this work, we introduce a Robustly Aligned
    LLM (RA-LLM) to defend against potential alignment-breaking attacks. RA-LLM can
    be directly constructed upon an existing aligned LLM with a robust alignment checking
    function, without requiring any expensive retraining or fine-tuning process of
    the original LLM. Furthermore, we also provide a theoretical analysis for RA-LLM
    to verify its effectiveness in defending against alignment-breaking attacks. Through
    real-world experiments on open-source large language models, we demonstrate that
    RA-LLM can successfully defend against both state-of-the-art adversarial prompts
    and popular handcrafted jailbreaking prompts by reducing their attack success
    rates from nearly 100% to around 10% or less.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）取得了显著的进展，并在各个领域得到了广泛应用。不幸的是，越来越多的担忧表明，LLMs 可能被滥用来生成有害或恶意的内容。虽然有一系列研究集中在使
    LLMs 与人类价值观对齐并防止其生成不适当内容上，但这种对齐通常较为脆弱，可能会被通过对抗性优化或手工制作的破解提示进行对齐破坏攻击所绕过。在这项工作中，我们介绍了一种稳健对齐的大型语言模型（RA-LLM）以防御潜在的对齐破坏攻击。RA-LLM
    可以直接基于现有的对齐 LLM 构建，配备稳健的对齐检查功能，无需对原始 LLM 进行昂贵的重新训练或微调过程。此外，我们还提供了 RA-LLM 的理论分析，以验证其在防御对齐破坏攻击方面的有效性。通过对开源大型语言模型的实际世界实验，我们展示了
    RA-LLM 能成功防御最先进的对抗性提示和流行的手工制作破解提示，将攻击成功率从接近 100% 降低到约 10% 或更低。
- en: 'WARNING: This paper contains unsafe model responses. Reader discretion is advised.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：本文包含不安全的模型回应。读者需谨慎阅读。
- en: 1 INTRODUCTION
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Trained on a wide range of text data from the internet, Large Language Models
    (LLMs) have exhibited exciting improvement in their generalization capabilities
    (OpenAI, [2023](#bib.bib26); Touvron et al., [2023b](#bib.bib35)) and widespread
    application in various domains such as finance (Wu et al., [2023](#bib.bib39)),
    law (Nguyen, [2023](#bib.bib24)), and healthcare industry (Thirunavukarasu et al.,
    [2023](#bib.bib33)). While LLMs have showcased impressive potential, a rising
    concern is that they can also be maliciously utilized to generate content deviating
    from human values (e.g., harmful responses and illegal suggestions) (Hazell, [2023](#bib.bib11);
    Kang et al., [2023](#bib.bib16)) due to the substantial amount of undesirable
    material existing in their training data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上的广泛文本数据上训练的大型语言模型（LLMs）在其泛化能力方面表现出了令人兴奋的改进（OpenAI, [2023](#bib.bib26);
    Touvron et al., [2023b](#bib.bib35)），并在诸如金融（Wu et al., [2023](#bib.bib39)）、法律（Nguyen,
    [2023](#bib.bib24)）和医疗行业（Thirunavukarasu et al., [2023](#bib.bib33)）等多个领域得到了广泛应用。虽然
    LLMs 展示了令人印象深刻的潜力，但也有日益增长的担忧，即它们可能被恶意利用来生成偏离人类价值观的内容（例如，有害回应和非法建议）（Hazell, [2023](#bib.bib11);
    Kang et al., [2023](#bib.bib16)），这是由于其训练数据中存在大量不良材料。
- en: '![Refer to caption](img/ca857b273c6dd81defcce9acc406f2a2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ca857b273c6dd81defcce9acc406f2a2.png)'
- en: 'Figure 1: An illustration of alignment-breaking attack: an aligned LLM gives
    unsafe responses to malicious requests with adversarial prompts.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：对齐破坏攻击的示意图：一个对齐的大型语言模型对恶意请求做出不安全的回应。
- en: To tackle this issue, a line of research focuses on aligning LLMs with human
    preferences and preventing them from producing inappropriate content (Ouyang et al.,
    [2022](#bib.bib27); Bai et al., [2022](#bib.bib2); Go et al., [2023](#bib.bib10);
    Korbak et al., [2023](#bib.bib18)). These alignments typically adopt reinforcement
    learning from human feedback (Ouyang et al., [2022](#bib.bib27)) and AI feedback
    (Bai et al., [2022](#bib.bib2)) to fine-tune LLMs for alignments with human values.
    Despite these efforts, an emerging class of jailbreak attacks can still bypass
    the alignment and elicit harmful responses from LLMs (Yuan et al., [2023](#bib.bib45);
    Shen et al., [2023](#bib.bib30); Wei et al., [2023](#bib.bib36); Zou et al., [2023](#bib.bib50)).
    These alignment-breaking attacks manually craft adversarial prompts by designing
    elaborate role-play (Shen et al., [2023](#bib.bib30)) or simply asking the LLM
    to give the response starting with “Absolutely! Here’s” (Wei et al., [2022](#bib.bib37)).
    Moreover, automatic jailbreak prompt generation methods have also been developed
    through dialogue encryption (Yuan et al., [2023](#bib.bib45)) or the combination
    of greedy and gradient-based search methods (Zou et al., [2023](#bib.bib50)).
    Figure [1](#S1.F1 "Figure 1 ‣ 1 INTRODUCTION ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") shows an example that a malicious question
    appended with an adversarial prompt could successfully break the safety alignment.
    Recently, (Zou et al., [2023](#bib.bib50)) have demonstrated that jailbreak attempts
    could be highly effective and transferable across different LLMs. This phenomenon
    suggests that existing safety alignment is far from robust to defend against carefully
    crafted adversarial prompts.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这个问题，一系列研究专注于使LLM与人类偏好对齐，并防止其生成不适当的内容（Ouyang等，[2022](#bib.bib27)；Bai等，[2022](#bib.bib2)；Go等，[2023](#bib.bib10)；Korbak等，[2023](#bib.bib18)）。这些对齐通常采用来自人类反馈（Ouyang等，[2022](#bib.bib27)）和AI反馈（Bai等，[2022](#bib.bib2)）的强化学习来微调LLM，使其符合人类价值观。尽管有这些努力，一类新兴的破解攻击仍能绕过对齐并引发LLM产生有害响应（Yuan等，[2023](#bib.bib45)；Shen等，[2023](#bib.bib30)；Wei等，[2023](#bib.bib36)；Zou等，[2023](#bib.bib50)）。这些破坏对齐的攻击通过设计精巧的角色扮演（Shen等，[2023](#bib.bib30)）或简单地要求LLM以“Absolutely!
    Here’s”开头的回应（Wei等，[2022](#bib.bib37)）来手动制作对抗性提示。此外，还开发了通过对话加密（Yuan等，[2023](#bib.bib45)）或贪婪和基于梯度的搜索方法组合（Zou等，[2023](#bib.bib50)）的自动破解提示生成方法。图[1](#S1.F1
    "Figure 1 ‣ 1 INTRODUCTION ‣ Defending Against Alignment-Breaking Attacks via
    Robustly Aligned LLM")显示了一个例子，其中一个恶意问题附加了对抗性提示，成功破坏了安全对齐。最近，（Zou等，[2023](#bib.bib50)）展示了破解尝试可能在不同LLM之间具有高度有效性和可转移性。这一现象表明，现有的安全对齐距离防御精心设计的对抗性提示还远远不够稳健。
- en: 'Till now, few attempts have been made to design dedicated mechanisms for defending
    alignment-breaking attacks. A rudimentary defense currently employed relies on
    external tools to re-assess the potential harm of the LLM responses. For instance,
    it could feed every potential response from the target LLM into a third-party
    LLM to determine whether the response is harmful or not (Helbling et al., [2023](#bib.bib12)).
    While this strategy enables filtering out possible harmful responses, there are
    several major drawbacks limiting its practicability: 1) Existing LLMs are very
    sensitive to harmful keywords appeared in the input, and have a high propensity
    to misclassify benign content as harmful, even when the entire sentence is not
    talking about any harmful behavior (e.g., stating news or providing guidance/warnings).
    This could lead to a high false-positive rate in harmful content detection; 2)
    The method heavily relies on the performance of the LLM used as a harmful discriminator,
    while the LLM itself is not designed to be an accurate harmful discriminator.
    The basis for its decisions remains ambiguous, implying that the harmful evaluation
    process could be opaque; 3) There are more types of alignment that can not be
    simply summarised as “harmful” (e.g., privacy, ethics, human values etc), thus
    this type of approach cannot cover such cases simultaneously. Given the wide range
    of applications where LLMs could be utilized, finding an effective and practical
    defense against potential alignment-breaking attacks is both urgent and challenging.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，针对防御对齐破坏攻击的专门机制的尝试很少。目前采用的初步防御方法依赖于外部工具重新评估LLM回应的潜在危害。例如，可以将目标LLM的每一个潜在回应输入到第三方LLM中，以确定回应是否有害（Helbling
    et al., [2023](#bib.bib12)）。虽然这种策略能够过滤掉可能有害的回应，但仍存在几个主要缺点限制其实用性：1）现有LLM对输入中出现的有害关键词非常敏感，倾向于将无害内容误判为有害，即使整句话并未涉及任何有害行为（例如，陈述新闻或提供指导/警告）。这可能导致有害内容检测中的假阳性率很高；2）该方法严重依赖于作为有害判别器的LLM的性能，而LLM本身并未被设计为一个准确的有害判别器。其决策基础仍然模糊，这意味着有害评估过程可能是不透明的；3）还有更多类型的对齐不能简单地总结为“有害”（例如，隐私、伦理、人类价值观等），因此这种方法无法同时覆盖这些情况。鉴于LLM可以应用的广泛领域，找到一种有效且实用的防御机制以应对潜在的对齐破坏攻击既紧迫又具有挑战性。
- en: In this work, we design a Robustly Aligned LLM (RA-LLM) to defend against potential
    alignment-breaking attacks, which is built upon an already aligned LLM and makes
    the existing alignments less prone to be circumvented by adversarial prompts.
    Specifically, our key idea is that although an aligned LLM can, to some extent,
    identify if the input request is benign or not, we cannot directly rely on that
    as it may not be robust. We consider an input request to be benign, only if we
    randomly drop a certain portion of the request and the LLM still thinks it is
    benign in most cases. Intuitively, such a random dropping operation would invalidate
    the adversarial prompts in alignment-breaking attacks, which are usually sensitive
    to small perturbations; on the other hand, the chances for the LLM to reject benign
    requests are relatively low, even after random dropping. Therefore, such a mechanism
    naturally leads to a robustly aligned LLM.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们设计了一种鲁棒对齐LLM（RA-LLM），用于防御潜在的对齐破坏攻击，该LLM建立在已经对齐的LLM基础上，使现有对齐不易被对抗性提示规避。具体而言，我们的关键思想是，尽管对齐LLM可以在一定程度上识别输入请求是否无害，但我们不能直接依赖这一点，因为它可能不够稳健。我们认为输入请求是无害的，只有在我们随机丢弃请求的一部分且LLM在大多数情况下仍认为它是无害的情况下。直观上，这种随机丢弃操作会使对齐破坏攻击中的对抗性提示无效，因为这些提示通常对小的扰动非常敏感；另一方面，即使在随机丢弃之后，LLM拒绝无害请求的机会也相对较低。因此，这种机制自然地导致了一个鲁棒对齐的LLM。
- en: Note that our RA-LLM does not require any external “harmful” detectors, instead,
    our strategy only relies on the existing alignment capability inside the LLM.
    Due to the same reason, our approach is not limited to any specific type of alignment
    (e.g., harmful), but robustifies all existing model alignments. Furthermore, we
    provide a theoretical analysis to verify the effectiveness of our proposed RA-LLM.
    Our experimental results on open-source large language models demonstrate that
    RA-LLM can successfully defend against both state-of-the-art adversarial prompts
    and popular handcrafted jailbreaking prompts by reducing their attack success
    rates from nearly 100% to around 10% or less.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们的RA-LLM不需要任何外部的“有害”检测器，而是完全依赖于LLM内部现有的对齐能力。由于同样的原因，我们的方法不限于任何特定类型的对齐（例如，有害），而是增强所有现有模型对齐的鲁棒性。此外，我们提供了理论分析来验证我们提出的RA-LLM的有效性。我们在开源大型语言模型上的实验结果表明，RA-LLM能够成功防御最先进的对抗性提示和流行的手工制作越狱提示，将它们的攻击成功率从接近100%降低到约10%或更低。
- en: 2 RELATED WORKS
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Aligning LLMs with Human Preferences
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与人类偏好对齐的LLM
- en: Foundational large language models are pre-trained on extensive textual corpora
    (Brown et al., [2020](#bib.bib3); Chowdhery et al., [2022](#bib.bib4); Touvron
    et al., [2023a](#bib.bib34)), which equips LLMs with world knowledge and facilitates
    their deployment in professional applications. Despite their excellent performance,
    LLMs suffer from generating outputs that deviate from human expectations (e.g.,
    harmful responses and illegal suggestions) due to the significant amount of inappropriate
    content existing in unfiltered training data. To tackle this issue, a line of
    work focuses on aligning LLMs with human values (Xu et al., [2020b](#bib.bib41);
    Ouyang et al., [2022](#bib.bib27); Bai et al., [2022](#bib.bib2); Go et al., [2023](#bib.bib10);
    Korbak et al., [2023](#bib.bib18)). Specifically, Ouyang et al. ([2022](#bib.bib27))
    align the LLM by using reinforcement learning from human feedback (RLHF (Christiano
    et al., [2017](#bib.bib5); Stiennon et al., [2020](#bib.bib32))) to fine-tune
    pre-trained LLM with human preferences as the reward signal, which reduces the
    generation of toxic content. Bai et al. ([2022](#bib.bib2)) train a less harmful
    system through the specification of a short list of principles and further improve
    the human-judged performance by introducing chain-of-thought style reasoning (Wei
    et al., [2022](#bib.bib37)) in both supervised-learning and reinforcement-learning
    stage. Go et al. ([2023](#bib.bib10)) consider aligning LLMs as approximating
    a target distribution representing some desired behavior and accordingly propose
    a new framework for fine-tuning LLMs to approximate any target distribution through
    f-divergences minimization. In addition to aligning LLMs in the fine-tuning stage,
    Korbak et al. ([2023](#bib.bib18)) propose pertaining LLMs with alternative objectives
    that guide them to generate text aligned with human preferences and significantly
    reduce the rate of generating undesirable content by using conditional training
    (Keskar et al., [2019](#bib.bib17)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基础的大型语言模型是在大量文本语料库上进行预训练的（Brown等，[2020](#bib.bib3)；Chowdhery等，[2022](#bib.bib4)；Touvron等，[2023a](#bib.bib34)），这使得LLM具备了世界知识，并促进了其在专业应用中的部署。尽管表现优异，LLM在生成偏离人类期望的输出（例如，有害的回应和非法建议）方面仍然存在问题，这主要是由于未过滤训练数据中存在大量不适当内容。为了解决这个问题，许多工作集中于将LLM与人类价值观对齐（Xu等，[2020b](#bib.bib41)；Ouyang等，[2022](#bib.bib27)；Bai等，[2022](#bib.bib2)；Go等，[2023](#bib.bib10)；Korbak等，[2023](#bib.bib18)）。具体来说，Ouyang等（[2022](#bib.bib27)）通过使用来自人类反馈的强化学习（RLHF（Christiano等，[2017](#bib.bib5)；Stiennon等，[2020](#bib.bib32)））对预训练的LLM进行微调，将人类偏好作为奖励信号，以减少有毒内容的生成。Bai等（[2022](#bib.bib2)）通过指定一份简短的原则列表来训练一个较少有害的系统，并在监督学习和强化学习阶段引入链式思考风格的推理（Wei等，[2022](#bib.bib37)）来进一步提高人类评判的表现。Go等（[2023](#bib.bib10)）认为将LLM对齐视为近似表示某种期望行为的目标分布，并因此提出了一种新的框架，用于通过最小化f散度来微调LLM以近似任何目标分布。除了在微调阶段对齐LLM外，Korbak等（[2023](#bib.bib18)）提出了使用替代目标来引导LLM生成与人类偏好一致的文本，并通过使用条件训练（Keskar等，[2019](#bib.bib17)）显著降低生成不良内容的比例。
- en: Alignment-breaking Attacks and defenses in LLMs
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM中的对齐破坏攻击与防御
- en: 'Although various alignment strategies have been developed to steer LLMs to
    generate content complying with human ethical principles, an emerging class of
    alignment-breaking attacks (i.e., jailbreak attacks) can still bypass safeguards
    and elicit LLMs to generate harmful and toxic responses (Wolf et al., [2023](#bib.bib38);
    Li et al., [2023](#bib.bib20); Shen et al., [2023](#bib.bib30); Yuan et al., [2023](#bib.bib45);
    Wei et al., [2023](#bib.bib36); Zou et al., [2023](#bib.bib50)), which poses significant
    threats to the practical deployment of LLMs. In particular, inspired by traditional
    computer security, Kang et al. ([2023](#bib.bib16)) adapt obfuscation, code injection/payload
    splitting, and visualization attacks to the LLMs, leading to the generation of
    content containing hate speech, phishing attacks, and scams. Wei et al. ([2023](#bib.bib36))
    hypothesize that competing objectives and mismatched generalization are two failure
    modes of safety training in LLMs and craft effective jailbreak attacks by leveraging
    the two failure modes. Instead of manually crafting adversarial prompts, Zou et al.
    ([2023](#bib.bib50)) automatically produce transferable adversarial suffixes by
    using greedy and gradient-based search methods to maximize the probability of
    generating an affirmative response. Yuan et al. ([2023](#bib.bib45)) bypass the
    safety alignment through dialogue encryption. Shen et al. ([2023](#bib.bib30))
    systematically analyzes the characteristics of jailbreak prompts in the wild and
    presents that jailbreak prompts have evolved to be more stealthy and effective
    with reduced length, increased toxicity, and semantic shift. Note that some concurrent
    works also aim to defend against alignment-breaking attacks: Kumar et al. ([2023](#bib.bib19))
    provides a verifiable safety guarantee by enumerating all possible partially erased
    input and using a safety filter to identify the harmfulness of the input content.
    Jain et al. ([2023](#bib.bib13)) propose to detect adversarial prompts by checking
    if the perplexity of the prompt is greater than a threshold.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经开发了各种对齐策略来引导大语言模型（LLMs）生成符合人类伦理原则的内容，但一种新兴的对齐破坏攻击（即，越狱攻击）仍然能够绕过防护措施，引导大语言模型生成有害和毒性的响应（Wolf
    et al., [2023](#bib.bib38)；Li et al., [2023](#bib.bib20)；Shen et al., [2023](#bib.bib30)；Yuan
    et al., [2023](#bib.bib45)；Wei et al., [2023](#bib.bib36)；Zou et al., [2023](#bib.bib50)），这对大语言模型的实际部署构成了重大威胁。特别是，Kang
    et al. ([2023](#bib.bib16))受到传统计算机安全的启发，将混淆、代码注入/负载分割和可视化攻击应用于大语言模型，导致生成包含仇恨言论、钓鱼攻击和诈骗的内容。Wei
    et al. ([2023](#bib.bib36))假设竞争目标和不匹配的泛化是大语言模型安全训练的两种失败模式，并通过利用这两种失败模式来设计有效的越狱攻击。Zou
    et al. ([2023](#bib.bib50))通过使用贪婪和基于梯度的搜索方法自动生成可转移的对抗后缀，以最大化生成肯定响应的概率，而不是手动设计对抗性提示。Yuan
    et al. ([2023](#bib.bib45))通过对话加密绕过安全对齐。Shen et al. ([2023](#bib.bib30))系统地分析了现实中的越狱提示的特征，并指出越狱提示已经演变得更加隐蔽和有效，具有更短的长度、更高的毒性和语义转移。请注意，一些同步研究也旨在防御对齐破坏攻击：Kumar
    et al. ([2023](#bib.bib19))通过枚举所有可能的部分擦除输入并使用安全过滤器来识别输入内容的有害性，提供了可验证的安全保证。Jain
    et al. ([2023](#bib.bib13))提出通过检查提示的困惑度是否超过阈值来检测对抗性提示。
- en: Traditional Text Adversarial Attack and Defenses
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 传统文本对抗攻击与防御
- en: Traditional text adversarial attacks primarily focus on text classification
    tasks and aim to force target models to maximize their prediction error by adversarially
    perturbing original text (Ebrahimi et al., [2017](#bib.bib8); Jin et al., [2020](#bib.bib15);
    Li et al., [2018](#bib.bib21); Maheshwary et al., [2021](#bib.bib22); Ye et al.,
    [2023](#bib.bib44)). The adversarial perturbation could be crafted by performing
    character-level transformation (Gao et al., [2018](#bib.bib9)) or replacing original
    words with their synonyms while maintaining semantics and syntax similar (Alzantot
    et al., [2018](#bib.bib1)). The generation of adversarial examples could be categorized
    into the “white-box” setting and the “black-box” setting according to the extent
    of access to the target model (Xu et al., [2020a](#bib.bib40)). As a representative
    white-box method, HotFlip  (Ebrahimi et al., [2017](#bib.bib8)) uses the gradient
    information of discrete text structure at its one-hot representation to construct
    adversarial examples. In the black-box setting, Li et al. ([2018](#bib.bib21));
    Jin et al. ([2020](#bib.bib15)); Ren et al. ([2019](#bib.bib28)) leverage the
    prediction score distribution on all categories to craft adversarial text without
    the guidance of parameter gradients. Maheshwary et al. ([2021](#bib.bib22)) focus
    on a more realistic scenario where attackers only know the top-$1$ prediction
    and propose using population-based optimization to construct adversarial text.
    Ye et al. ([2022](#bib.bib43)) follow the same scenario and employ the word embedding
    space to guide the generation of adversarial examples.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的文本对抗攻击主要集中在文本分类任务上，旨在通过对原始文本进行对抗性扰动来迫使目标模型最大化其预测误差（Ebrahimi et al., [2017](#bib.bib8);
    Jin et al., [2020](#bib.bib15); Li et al., [2018](#bib.bib21); Maheshwary et al.,
    [2021](#bib.bib22); Ye et al., [2023](#bib.bib44)）。对抗性扰动可以通过执行字符级转换（Gao et al.,
    [2018](#bib.bib9)）或用同义词替换原始单词，同时保持语义和语法相似（Alzantot et al., [2018](#bib.bib1)）来生成。对抗样本的生成可以根据对目标模型的访问程度分为“白盒”设置和“黑盒”设置（Xu
    et al., [2020a](#bib.bib40)）。作为一种代表性的白盒方法，HotFlip（Ebrahimi et al., [2017](#bib.bib8)）利用离散文本结构的一热表示的梯度信息来构造对抗样本。在黑盒设置中，Li
    et al. ([2018](#bib.bib21)); Jin et al. ([2020](#bib.bib15)); Ren et al. ([2019](#bib.bib28))
    利用所有类别上的预测得分分布来生成对抗文本，而无需参数梯度的指导。Maheshwary et al. ([2021](#bib.bib22)) 关注一个更现实的场景，其中攻击者只知道前
    $1$ 个预测，并提出使用基于人群的优化来构造对抗文本。Ye et al. ([2022](#bib.bib43)) 遵循相同的场景，并利用词嵌入空间来指导对抗样本的生成。
- en: To defend against adversarial attacks, a body of empirical defense methods has
    been proposed. In particular, adversarial-training-based methods (Miyato et al.,
    [2016](#bib.bib23); Zhu et al., [2019](#bib.bib48)) incorporate adversarial perturbations
    to word embeddings and robustly train the model by minimizing the adversarial
    loss. Zhou et al. ([2021](#bib.bib47)); Dong et al. ([2021](#bib.bib7)) utilize
    adversarial data augmentation by replacing the original word with its synonyms
    to make the model robust to similar adversarial perturbations. These methods gain
    empirical success against adversarial attacks. To provide provable robustness
    against adversarial word substitutions, Jia et al. ([2019](#bib.bib14)) use certifiably
    robust training by training the model to optimize Interval Bound Propagation (IBP)
    upper bound. Shi et al. ([2020](#bib.bib31)) adopt linear-relaxation-based perturbation
    analysis (Xu et al., [2020c](#bib.bib42)) to develop a robustness verification
    method for transformers. Zeng et al. ([2023](#bib.bib46)) propose a certifiably
    robust defense method based on randomized smoothing techniques (Cohen et al.,
    [2019](#bib.bib6)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防御对抗攻击，已经提出了一系列经验性防御方法。特别是，基于对抗训练的方法（Miyato et al., [2016](#bib.bib23); Zhu
    et al., [2019](#bib.bib48)）将对抗性扰动融入词嵌入中，并通过最小化对抗损失来稳健地训练模型。Zhou et al. ([2021](#bib.bib47));
    Dong et al. ([2021](#bib.bib7)) 通过用同义词替换原始单词来利用对抗数据增强，使模型对类似的对抗扰动具有鲁棒性。这些方法在对抗攻击中取得了经验上的成功。为了提供对抗性词汇替换的可证明鲁棒性，Jia
    et al. ([2019](#bib.bib14)) 通过训练模型以优化区间边界传播（IBP）上界来使用可证明鲁棒训练。Shi et al. ([2020](#bib.bib31))
    采用基于线性松弛的扰动分析（Xu et al., [2020c](#bib.bib42)）来开发变换器的鲁棒性验证方法。Zeng et al. ([2023](#bib.bib46))
    提出了基于随机平滑技术（Cohen et al., [2019](#bib.bib6)）的可证明鲁棒防御方法。
- en: 3 Our Proposed Method
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们提出的方法
- en: In this section, we introduce the proposed Robustly Aligned LLM for defending
    alignment-breaking attacks. Before heading into details, we first discuss the
    threat model that is focused on in this paper.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了为防御对齐破坏攻击而提出的稳健对齐 LLM。在进入详细讨论之前，我们首先讨论本文关注的威胁模型。
- en: 3.1 Threat Model
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 威胁模型
- en: An alignment-breaking attack seeks to bypass the security checks of an aligned
    LLM by introducing adversarial prompts adhered to an original malicious question.
    Let  represent the adversarial prompt generated by the alignment-breaking attack.
    Let  denotes the insertion operation. While most existing attacks typically place
    the adversarial prompts at the end of the request Zou et al. ([2023](#bib.bib50)),
    we actually consider a more general case where the adversarial prompt could also
    be inserted in front of the malicious question or be integrated in the middle.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐破坏攻击试图通过引入附加在原始恶意问题上的对抗性提示来绕过对齐 LLM 的安全检查。令  表示对齐破坏攻击生成的对抗性提示。令  表示插入操作。虽然大多数现有攻击通常将对抗性提示放在请求的末尾
    Zou et al. ([2023](#bib.bib50))，但我们实际上考虑一个更一般的情况，其中对抗性提示也可以插入在恶意问题之前或在中间。
- en: We also assume that the target LLM  is directly input into the target LLM ,
    so that ${\mathbf{x}}_{\text{adv}}={\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$
    will mislead the LLM to provide an affirmative answer Zou et al. ([2023](#bib.bib50))
    to such a malicious question, e.g., “Sure, here is how to do [a malicious request]…”.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设目标 LLM 直接输入到目标 LLM 中，这样 ${\mathbf{x}}_{\text{adv}}={\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$
    将误导 LLM 对这种恶意问题给出肯定回答 Zou et al. ([2023](#bib.bib50))，例如，“当然，这里是如何做 [恶意请求]……”。
- en: 3.2 Our Proposed Method
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 我们提出的方法
- en: 'Our motivation builds upon the fact that the target LLM has already been aligned
    and is able to reject commonly seen malicious requests. To be more specific, we
    can build an alignment check function : return Fail when detecting typical aligned
    text in the output of  is quite vague but we will provide more details on how
    to implement it in practice in Section [3.3](#S3.SS3 "3.3 Practical Designs ‣
    3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM").. Given the alignment check function $\text{AC}(\cdot)$, one can
    then construct a “hypothetical” LLM by'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的动机基于目标 LLM 已经对齐并能够拒绝常见的恶意请求这一事实。更具体地说，我们可以构建一个对齐检查函数：当检测到  的输出中典型的对齐文本时返回失败，这一点比较模糊，但我们将在第
    [3.3](#S3.SS3 "3.3 Practical Designs ‣ 3 Our Proposed Method ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM") 节提供更多实际实现的细节。给定对齐检查函数 $\text{AC}(\cdot)$，可以构建一个“假设的”
    LLM 通过
- en: '|  | $f^{\prime}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Reject the
    response},\text{ if }\text{AC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;f({\mathbf{x}})\
    \ \ \ \ \ \ \qquad\qquad,\text{ if }\text{AC}(f({\mathbf{x}}))=\text{Pass}\end{aligned}\right.$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $f^{\prime}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{拒绝响应},\text{
    如果 }\text{AC}(f({\mathbf{x}}))=\text{失败}\\ &amp;f({\mathbf{x}})\ \ \ \ \ \ \ \qquad\qquad,\text{
    如果 }\text{AC}(f({\mathbf{x}}))=\text{通过}\end{aligned}\right.$ |  | (1) |'
- en: where . While  in practice, it showcases how one can construct a new aligned
    LLM using an alignment check function.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，展示了如何使用对齐检查函数构建一个新的对齐 LLM。
- en: Robust Alignment Check Function
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 稳健对齐检查函数
- en: 'One thing to notice here is that the previously defined alignment check function
    $\text{AC}(\cdot)$ only relies on the existing alignments inside on the target
    LLM. However, the existence of alignment-breaking attacks such as the adversarial
    prompts Zou et al. ([2023](#bib.bib50)) has proved that such alignment checking
    is not robust: it can be easily manipulated and circumvented by carefully designed
    perturbations or suffix prompts. Therefore, it is natural to think about how we
    can design a robust alignment check function that could strengthen the alignment
    check capabilities of an aligned LLM, without finetuning or modifying the model
    itself.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，之前定义的对齐检查函数 $\text{AC}(\cdot)$ 仅依赖于目标 LLM 内部已有的对齐。然而，诸如 Zou et al.
    ([2023](#bib.bib50)) 提出的对齐破坏攻击的存在已证明这种对齐检查并不稳健：它可以被精心设计的扰动或后缀提示轻易操控和规避。因此，考虑如何设计一个稳健的对齐检查函数，以增强对齐
    LLM 的对齐检查能力，而无需对模型本身进行微调或修改，是很自然的。
- en: 'Our intuition here is very straightforward: since the existing alignment check
    function  in most cases. To translate this requirement into mathematical formulations,
    we define the following Robust Alignment Check function  and the alignment check
    function $\text{AC}(\cdot)$ :'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的直觉非常直接：由于现有的对齐检查函数在大多数情况下不适用。为了将这一要求转化为数学公式，我们定义了以下**鲁棒对齐检查**函数和对齐检查函数$\text{AC}(\cdot)$：
- en: '|  | $$\text{RAC}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Fail},\ \text{
    if}\ \text{AC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;\text{Fail},\ \text{ if}\ \mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}]_{{\mathbf{r}}}))=\text{Fail})>t\\'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\text{RAC}({\mathbf{x}})=\left\{\begin{aligned} &\text{失败},\ \text{如果}\
    \text{AC}(f({\mathbf{x}}))=\text{失败}\\ &\text{失败},\ \text{如果}\ \mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}]_{{\mathbf{r}}}))=\text{失败})>t\\'
- en: '&amp;\text{Pass},\ \text{'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&\text{通过},\ \text{'
- en: otherwise}\end{aligned}\right.$$ |  | (2) |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 否则}\end{aligned}\right.$$ |  | (2) |
- en: where  refers to the distribution of possible masks after uniformly dropping  denotes
    the kept indices  after the dropping operation. Essentially, for an input , every
    possible  tokens indexed by ${\mathbf{r}}$.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，指的是在均匀丢弃后可能的掩码的分布，表示丢弃操作后保留的索引。本质上，对于一个输入，每一个可能的 令牌都由${\mathbf{r}}$索引。
- en: Eq. [2](#S3.E2 "In Robust Alignment Check Function ‣ 3.2 Our Proposed Method
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM") states that the robust alignment check function  to show no sign
    of being aligned but also requires the response after random dropping still shows
    no sign of being aligned in most cases. On the contrary, if ) of responses from
    the randomly dropped input fails to pass AC, .
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Eq. [2](#S3.E2 "在 鲁棒对齐检查函数 ‣ 3.2 我们的方法 ‣ 3 我们的方法 ‣ 通过鲁棒对齐 LLM 防御对齐破坏攻击") 指出，鲁棒对齐检查函数
    不仅表明没有对齐的迹象，而且要求在随机丢弃后的响应在大多数情况下仍然没有对齐的迹象。相反，如果从随机丢弃的输入中响应的 ) 未能通过 AC，。
- en: 'Based on the robust alignment check function  with $\text{RAC}(\cdot)$ in Eq.
    ([1](#S3.E1 "In 3.2 Our Proposed Method ‣ 3 Our Proposed Method ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM")):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于鲁棒对齐检查函数与 Eq. ([1](#S3.E1 "在 3.2 我们的方法 ‣ 3 我们的方法 ‣ 通过鲁棒对齐 LLM 防御对齐破坏攻击"))中的$\text{RAC}(\cdot)$：
- en: '|  | $f_{\text{rob}}({\mathbf{x}})=\left\{\begin{aligned} &amp;\text{Reject
    the response},\text{ if }\text{RAC}(f({\mathbf{x}}))=\text{Fail}\\ &amp;f({\mathbf{x}})\
    \ \ \ \ \ \ \qquad\qquad,\text{ if }\text{RAC}(f({\mathbf{x}}))=\text{Pass}\end{aligned}\right.$
    |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{\text{rob}}({\mathbf{x}})=\left\{\begin{aligned} &\text{如果 }\text{RAC}(f({\mathbf{x}}))=\text{失败},\text{
    则拒绝响应}\\ &f({\mathbf{x}})\ \ \ \ \ \ \ \qquad\qquad,\text{ 如果 }\text{RAC}(f({\mathbf{x}}))=\text{通过}\end{aligned}\right.$
    |  | (3) |'
- en: By this simple reconstruction of alignment check function, we can build a robustly
    aligned LLM without necessitating extra resources or retraining of the entire
    model. Figure [2](#S3.F2 "Figure 2 ‣ Robust Alignment Check Function ‣ 3.2 Our
    Proposed Method ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") illustrates the effect of our proposed RAC
    when facing malicious or benign requests.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种简单的对齐检查函数重构，我们可以在不需要额外资源或重新训练整个模型的情况下，构建鲁棒对齐的 LLM。**图 [2](#S3.F2 "图 2 ‣
    鲁棒对齐检查函数 ‣ 3.2 我们的方法 ‣ 3 我们的方法 ‣ 通过鲁棒对齐 LLM 防御对齐破坏攻击")**展示了我们提出的 RAC 在面对恶意或良性请求时的效果。
- en: '![Refer to caption](img/faba338b1036d1008d41ddb2ec1e40dc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/faba338b1036d1008d41ddb2ec1e40dc.png)'
- en: 'Figure 2: An illustration of our RA-LLM when facing malicious requests with
    adversarial prompts (Left) and benign requests (Right).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 2**：我们的 RA-LLM 在面对恶意请求（左）和良性请求（右）时的示意图。'
- en: 3.3 Practical Designs
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**3.3 实际设计**'
- en: Algorithm 1 Robustly Aligned LLM
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**算法 1** **鲁棒对齐 LLM**'
- en: 'Input: aligned LLM .'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：对齐的 LLM。
- en: 1:  if  do5:        Randomly sample a mask 7:     end for8:     if 12:     end if13:  end if
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  如果  do5: 随机采样一个掩码 7:  结束 for8:  如果 12:  结束 if13:  结束 if'
- en: Now let’s delve into the practical designs of our proposed robustly aligned
    LLM, which essentially approximates $f_{\text{rob}}(\cdot)$ mentioned above. The
    detailed steps of the constructed robustly aligned LLM are summarized in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.3 Practical Designs ‣ 3 Our Proposed Method ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM").
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨我们提出的鲁棒对齐 LLM 的实际设计，它基本上是对上述 $f_{\text{rob}}(\cdot)$ 的近似。构建鲁棒对齐 LLM
    的详细步骤总结在**算法 [1](#alg1 "算法 1 ‣ 3.3 实际设计 ‣ 3 我们的方法 ‣ 通过鲁棒对齐 LLM 防御对齐破坏攻击")**中。
- en: Approximation of $\text{AC}(\cdot)$
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\text{AC}(\cdot)$的近似
- en: Previously, we vaguely defined the alignment check function  returns Fail; otherwise,
    it returns Pass. Note that we are only inspecting the prefix. For this purpose,
    we only need to generate a certain number of tokens (e.g., 10) for robust alignment
    checking. This could largely reduce our computational overhead²²2Further discussion
    on computational costs can be found in Section [4.5](#S4.SS5 "4.5 Computational
    Cost ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM")..
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，我们模糊地定义了对齐检查函数，当返回失败时；否则，返回通过。注意，我们只检查前缀。为此，我们只需生成一定数量的令牌（例如，10个）用于鲁棒对齐检查。这可以大大减少我们的计算开销²²2进一步讨论计算成本可参见第[4.5](#S4.SS5
    "4.5 计算成本 ‣ 4 实验 ‣ 通过鲁棒对齐的LLM防御对齐破坏攻击")节。
- en: Monte Carlo Sampling
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒙特卡洛采样
- en: It is practically infeasible to obtain the exact value for the probability of  indices
    masks to obtain  requests, and count the frequency of cases when the alignment
    check function $\text{AC}(\cdot)$ gives *Fail* decisions.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上不可能获得索引掩码获取请求的概率的确切值，并计算对齐检查函数$\text{AC}(\cdot)$给出*失败*决策的情况频率。
- en: The Practical Choice of $t$
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $t$的实际选择
- en: 'Another important choice is the threshold  such that whenever  directly fails
    the request. However in practice, such a setting could be too extreme as the randomness
    introduced in the dropping operations might also affect the LLM response on benign
    inputs: random dropping might occasionally lead to the loss of essential information,
    and under such circumstances the LLM might also generate responses similar to
    the typical alignment responses. For example, “Do you like apples?” could become
    “Do you apples?” after random dropping, leading the LLM to express an inability
    for answering this unclear question. This could potentially be mis-detected as
    Fail by , it will lead to Fail by  as zero, we keep a relatively small threshold.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要选择是阈值，使得每当直接失败请求时。然而在实际中，这样的设置可能过于极端，因为丢弃操作中引入的随机性可能也会影响LLM对良性输入的响应：随机丢弃可能偶尔导致关键信息的丢失，在这种情况下，LLM也可能生成类似于典型对齐响应的回答。例如，“你喜欢苹果吗？”可能变成“你苹果吗？”经过随机丢弃后，导致LLM表示无法回答这个不清楚的问题。这可能会被误检测为失败，由于为零，我们保持相对较小的阈值。
- en: 3.4 Theoretical Analysis
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 理论分析
- en: In this section, we theoretically analyze the proposed robustly aligned LLM
    and see when it provides a more robust alignment compared to the original LLM
    when facing alignment-breaking attacks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们理论上分析了提出的鲁棒对齐LLM，并查看在面对对齐破坏攻击时，它何时提供比原始LLM更鲁棒的对齐。
- en: Our theorem is based on the analysis on the robust alignment check function
    RAC. We will show that RAC is more robust for the aligned malicious text  of length
    ).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的定理基于对鲁棒对齐检查函数RAC的分析。我们将展示RAC对于长度的对齐恶意文本更具鲁棒性。
- en: Theorem 3.1.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3.1
- en: Consider a malicious input  such that . Suppose  tokens and  tokens while  in  as
    the padded text constructed from  pad tokens into position  and
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个恶意输入使得。假设令为令牌和为令牌，同时在填充文本中将从填充令牌构造为位置。
- en: '|  | $$\mathop{\min}\limits_{j}\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})>
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\mathop{\min}\limits_{j}\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U(p)}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})>
    |  |'
- en: where  is the threshold used in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Practical
    Designs ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM"), then our robustly aligned LLM in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.3 Practical Designs ‣ 3 Our Proposed Method ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM") with sufficiently large
    random drop trials .
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是算法[1](#alg1 "算法 1 ‣ 3.3 实际设计 ‣ 3 我们提出的方法 ‣ 通过鲁棒对齐的LLM防御对齐破坏攻击")中使用的阈值，然后我们的鲁棒对齐LLM在算法[1](#alg1
    "算法 1 ‣ 3.3 实际设计 ‣ 3 我们提出的方法 ‣ 通过鲁棒对齐的LLM防御对齐破坏攻击")中有足够大的随机丢弃试验。
- en: The proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical
    Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM") is provided in Appendix [A](#A1 "Appendix A Proof of
    Theorem 3.1 ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"). Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical Analysis
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM") provides an analysis on when our robustly aligned LLM could reject
    the request from an alignment-breaking attack while the original LLM actually
    fails to defend against such adversarial prompts. Specifically, given a particular
    malicious input , although it is impossible for us to know what kind of adversarial
    prompt the attacker would use, or which position the attacker would insert the
    adversarial prompt to, as long as we have  composed by ${\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$
    will be rejected by our robustly aligned LLM.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical Analysis ‣ 3 Our Proposed
    Method ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM")
    的证明见附录 [A](#A1 "Appendix A Proof of Theorem 3.1 ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM")。定理 [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4
    Theoretical Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") 分析了当我们的鲁棒对齐 LLM 能够拒绝来自对齐破坏攻击的请求时，而原始 LLM 实际上未能防御这些对抗性提示的情况。具体而言，给定一个特定的恶意输入，虽然我们无法知道攻击者会使用什么样的对抗性提示，或攻击者会将对抗性提示插入到哪个位置，只要我们有${\mathbf{x}}\oplus{\mathbf{p}}_{\text{adv}}$组成的提示，将会被我们的鲁棒对齐
    LLM 拒绝。
- en: 4 Experiments
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we aim to validate the efficacy of our RA-LLM from two aspects:
    1) RA-LLM can effectively reduce the attack success rate of adversarial prompts;
    2) RA-LLM minimally affects the outputs of benign samples. In the following, we
    first introduce our experimental settings and give a detailed analysis of our
    experimental results and ablation study.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在从两个方面验证我们 RA-LLM 的有效性：1）RA-LLM 能有效减少对抗性提示的攻击成功率；2）RA-LLM 对良性样本的输出影响最小。接下来，我们首先介绍我们的实验设置，并详细分析我们的实验结果和消融研究。
- en: 4.1 Experimental Settings
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Dataset
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We evaluated our approach on two datasets: AdvBench (Zou et al., [2023](#bib.bib50))
    and MS MARCO dataset (Nguyen et al., [2016](#bib.bib25)). The AdvBench dataset
    contains two types of data, corresponding to Harmful Strings Attack and Harmful
    Behaviors Attack respectively. Specifically, The data used for Harmful Strings
    Attack consist of 500 strings related to harmful or toxic content, such as threats,
    discriminatory remarks, methods of crime, and dangerous suggestions, etc. The
    data used for Harmful Behaviors Attack consists of 500 questions that can entice
    the LLM to produce harmful outputs, with topics similar to Harmful Strings. MS
    MARCO is a question-answering dataset, where all questions originate from real
    user queries on Bing. We sampled 150 pieces of data from each of these three datasets
    for our experimental evaluation.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个数据集上评估了我们的方法：AdvBench（Zou et al., [2023](#bib.bib50)）和 MS MARCO 数据集（Nguyen
    et al., [2016](#bib.bib25)）。AdvBench 数据集包含两种类型的数据，分别对应有害字符串攻击和有害行为攻击。具体来说，用于有害字符串攻击的数据包含500个与有害或毒性内容相关的字符串，例如威胁、歧视性言论、犯罪方法和危险建议等。用于有害行为攻击的数据包含500个可以诱使
    LLM 生成有害输出的问题，主题类似于有害字符串。MS MARCO 是一个问答数据集，其中所有问题源自 Bing 上的真实用户查询。我们从这三个数据集中各抽取了150条数据用于实验评估。
- en: Attack Setting
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 攻击设置
- en: 'We mainly evaluate our defense under the state-of-the-art alignment-breaking
    attack: the Harmful Behaviors Attack proposed by (Zou et al., [2023](#bib.bib50)).
    The goal of Harmful Behaviors Attack is to induce the LLM to respond effectively
    to malicious queries, which normally should be rejected by the aligned LLMs. Harmful
    Behaviors Attack aims to bypass the protective measures of aligned LLMs and entice
    them to generate harmful content. We calculated all adversarial prompts using
    the default hyperparameters provided in (Zou et al., [2023](#bib.bib50)).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要评估了在最先进的对齐破坏攻击下的防御效果：由 (Zou et al., [2023](#bib.bib50)) 提出的有害行为攻击。有害行为攻击的目标是诱使
    LLM 对恶意查询做出有效响应，而这些查询通常应由对齐的 LLM 拒绝。有害行为攻击旨在绕过对齐 LLM 的保护措施，诱使其生成有害内容。我们使用 (Zou
    et al., [2023](#bib.bib50)) 提供的默认超参数计算了所有对抗性提示。
- en: 4.2 Experimental Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验结果
- en: 'In Table [1](#S4.T1 "Table 1 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM"), we present the
    experimental results on two attack modes of the Harmful Behaviors Attack: Individual
    Attack and Transfer Attack, on Vicuna-7B-v1.3-HF and Guanaco-7B-HF models. Individual
    Attack aims to directly optimize adversarial prompts for specific models and specific
    malicious requests, while Transfer Attack aims to optimize generic adversarial
    prompts across multiple models and malicious requests. We tested both the original
    aligned LLM and our robust aligned LLM using benign requests and malicious requests
    with adversarial prompts. Subsequently, we evaluated whether these inputs activated
    the alignment mechanism based on the output of the LLM.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S4.T1 "表 1 ‣ 4.2 实验结果 ‣ 4 实验 ‣ 通过稳健对齐的LLM防御对齐突破攻击")中，我们展示了对Vicuna-7B-v1.3-HF和Guanaco-7B-HF模型的有害行为攻击的两种攻击模式：个人攻击和转移攻击的实验结果。个人攻击旨在直接优化针对特定模型和特定恶意请求的对抗性提示，而转移攻击则旨在优化跨多个模型和恶意请求的通用对抗性提示。我们测试了原始对齐的LLM和我们的稳健对齐LLM，使用了良性请求和带有对抗性提示的恶意请求。随后，我们评估了这些输入是否激活了基于LLM输出的对齐机制。
- en: 'Table 1: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM under two adversarial alignment-breaking attacks.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：原始LLM和我们稳健对齐LLM在两种对抗性对齐突破攻击下的良性回答率和攻击成功率。
- en: '| Attack | Models | BAR | ASR | ASR reduce |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 模型 | BAR | ASR | ASR减少 |'
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 原始LLM | RA-LLM | 原始LLM | RA-LLM |'
- en: '| Individual | Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 10.7% | 88.0% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 个人 | Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 10.7% | 88.0% |'
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 96.0% | 6.7% | 89.3% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 92.0% | 96.0% | 6.7% | 89.3% |'
- en: '| Transfer | Vicuna-7B-chat-HF | 99.3% | 98.7% | 83.3% | 11.3% | 71.0% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 转移 | Vicuna-7B-chat-HF | 99.3% | 98.7% | 83.3% | 11.3% | 71.0% |'
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 78.7% | 8.7% | 70.0% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 92.0% | 78.7% | 8.7% | 70.0% |'
- en: 'Specifically, we consider two main metrics to evaluate our model’s performances:
    attack success rate (ASR) and benign answering rate (BAR). Attack success rate
    measures the number of chances when the adversarial prompts successfully circumvent
    the model’s alignment mechanism. An attack is regarded as successful when the
    LLM produces a meaningful response without rejecting to answer with typical alignment
    text. To ensure the defense mechanism does not overkill and reject to answer benign
    questions, we also tested the benign answering rate, which represents the model
    precision in successfully identifying benign requests (does not reject to answer
    the benign requests). Our defensive goal is to minimize the attack success rate
    as much as possible while correctly identifying benign samples with a high benign
    answering rate.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们考虑了两个主要指标来评估模型的表现：攻击成功率（ASR）和良性回答率（BAR）。攻击成功率衡量了对抗性提示成功绕过模型对齐机制的次数。当LLM在不拒绝使用典型对齐文本回答的情况下生成有意义的响应时，攻击被视为成功。为了确保防御机制不会过度反应并拒绝回答良性问题，我们还测试了良性回答率，该指标代表了模型在成功识别良性请求方面的精度（不会拒绝回答良性请求）。我们的防御目标是尽可能降低攻击成功率，同时以高良性回答率正确识别良性样本。
- en: From Table [1](#S4.T1 "Table 1 ‣ 4.2 Experimental Results ‣ 4 Experiments ‣
    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"), it is
    evident that for Individual Attack, adversarial prompts have led to high malicious
    response success rates of 98.7% and 96.0% on the two models respectively. However,
    upon employing our robustly aligned LLM, these success rates dropped to 10.7%
    and 6.7%. Similarly, for Transfer Attack, the application of our robustly aligned
    LLM reduced the attack success rates from 83.3% and 78.7% to 11.3% and 8.7%. This
    demonstrates that our strategy effectively mitigates adversarial attacks. Additionally,
    our method maintains a good benign response rate, this indicates that our approach
    has almost no adverse impact on the LLM’s responses to benign inputs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[1](#S4.T1 "表 1 ‣ 4.2 实验结果 ‣ 4 实验 ‣ 通过稳健对齐的LLM防御对齐突破攻击")中可以明显看出，对于个人攻击，对抗性提示导致两个模型的恶意响应成功率分别为98.7%和96.0%。然而，在使用我们稳健对齐的LLM后，这些成功率降到了10.7%和6.7%。同样，对于转移攻击，我们的稳健对齐LLM将攻击成功率从83.3%和78.7%降低到11.3%和8.7%。这表明我们的策略有效地减轻了对抗性攻击。此外，我们的方法保持了良好的良性响应率，这表明我们的方法对LLM对良性输入的响应几乎没有负面影响。
- en: 4.3 Handcrafted Jailbreak Prompts
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 手工制作的越狱提示
- en: In practice, another type of commonly seen alignment-breaking attack is the
    handcrafted jailbreak prompts. Those manually crafted adversarial prompts usually
    work by designing elaborate role-play scenarios or asking the LLM to give the
    responses starting with affirmative responses such as “Sure, here it is” to force
    the LLM to generate harmful content. In general, the handcrafted jailbreak prompt
    is the type of alignment-breaking attack that is more widely adopted as it only
    requires no computation at all, and therefore, the threats stemming from handcrafted
    jailbreak prompts cannot be overlooked.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，另一种常见的对齐破坏攻击是手工制作的越狱提示。这些手动制作的对抗提示通常通过设计复杂的角色扮演场景或要求大型语言模型（LLM）给出以肯定性回应开头的回答，例如“当然，这是”来迫使LLM生成有害内容。一般来说，手工制作的越狱提示是更广泛采用的对齐破坏攻击类型，因为它完全不需要计算，因此来自手工制作的越狱提示的威胁不能被忽视。
- en: 'Table 2: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM using handcrafted jailbreak prompts.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 2: 原始LLM和我们稳健对齐的LLM使用手工制作越狱提示的良性回答率和攻击成功率。'
- en: '| Model | BAR | ASR | ASR reduce |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BAR | ASR | ASR 降低 |'
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 原始LLM | RA-LLM | 原始LLM | RA-LLM |'
- en: '| Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 12.0% | 86.7% |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B-chat-HF | 99.3% | 98.7% | 98.7% | 12.0% | 86.7% |'
- en: '| Guanaco-7B-HF | 95.3% | 92.0% | 94.7% | 9.3% | 85.4% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 92.0% | 94.7% | 9.3% | 85.4% |'
- en: '| GPT-3.5-turbo-0613 | 99.3% | 99.3% | 82.0% | 8.0% | 74.0% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo-0613 | 99.3% | 99.3% | 82.0% | 8.0% | 74.0% |'
- en: We also assessed the defensive capabilities of our robustly aligned LLM against
    these meticulously designed jailbreak prompts. Specifically, we selected the top
    five jailbreak prompts from jailbreakchat.com³³3The prompts are taken according
    to the website result on Sept 12, 2023, voted by the online users according to
    their effectiveness. For each of these handcrafted jailbreak prompts, we randomly
    selected 30 questions from the Harmful Behaviors dataset, culminating in a set
    of 150 handcrafted jailbreak prompt samples.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还评估了我们稳健对齐的LLM在应对这些精心设计的越狱提示方面的防御能力。具体来说，我们从 jailbreakchat.com 选择了前五个越狱提示³³³这些提示根据
    2023年9月12日网站上的结果，由在线用户根据其有效性投票选出。对于每个这些手工制作的越狱提示，我们随机选择了 30 个来自有害行为数据集的问题，总共形成了一组
    150 个手工制作的越狱提示样本。
- en: Table [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments
    ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM") shows
    the effects of our defense method on the handcrafted jailbreak prompt dataset
    for three different LLMs, Vicuna-7B-chat-HF, Guanaco-7B-HF, GPT-3.5-turbo-0613,
    all of them underwent safety alignment. We found that our robustly aligned LLM
    also performs exceptionally well against such handcrafted jailbreak prompts. As
    seen in Table [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments
    ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"), handcrafted
    jailbreak prompts achieved attack success rates of 98.4%, 94.7%, and 82.0% on
    the Vicuna-7B-chat-HF, Guanaco-7B-HF, and GPT-3.5-turbo-0613 models, respectively,
    without additional defense beyond alignment. However, when applying to our robustly
    aligned LLM, the attack success rates dropped to 12%, 9.3%, and 8.0%, a result
    even better compared to the adversarial prompt attacks in the previous section.
    In the meantime, RA-LLM has no significant impact on BAR especially for the larger
    models like GPT-3.5-turbo-0613, which inherently possess strong semantics comprehension
    abilities.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments ‣
    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM") 显示了我们防御方法对三种不同LLM的手工制作越狱提示数据集的效果，分别是
    Vicuna-7B-chat-HF、Guanaco-7B-HF 和 GPT-3.5-turbo-0613，这些模型都经过了安全对齐。我们发现我们稳健对齐的LLM在应对这些手工制作的越狱提示时表现也非常出色。正如表格
    [2](#S4.T2 "Table 2 ‣ 4.3 Handcrafted Jailbreak Prompts ‣ 4 Experiments ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM") 所示，手工制作的越狱提示在 Vicuna-7B-chat-HF、Guanaco-7B-HF
    和 GPT-3.5-turbo-0613 模型上的攻击成功率分别达到了 98.4%、94.7% 和 82.0%，而没有额外的防御措施。 然而，当应用于我们稳健对齐的LLM时，攻击成功率降至
    12%、9.3% 和 8.0%，这一结果甚至比前一节中的对抗性提示攻击表现更佳。同时，RA-LLM 对BAR没有显著影响，尤其是对于像 GPT-3.5-turbo-0613
    这样天生具有强大语义理解能力的大型模型。
- en: 4.4 Ablation Study
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: 'In this section, we analyze the impact of the three hyperparameters in our
    method: the random dropping ratio , and the number of random dropping trials .
    We evaluate the influence of these hyperparameters using the attack success rate
    and benign answering rate on the Harmful Behaviors attack in Vicuna-7B-chat-HF
    model. The evaluation results are depicted in Figure [3](#S4.F3 "Figure 3 ‣ 4.4
    Ablation Study ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了我们方法中三个超参数的影响：随机丢弃比例、随机丢弃试验次数。我们使用Vicuna-7B-chat-HF模型中的有害行为攻击的攻击成功率和良性回答率来评估这些超参数的影响。评估结果如图
    [3](#S4.F3 "Figure 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Defending Against
    Alignment-Breaking Attacks via Robustly Aligned LLM") 所示。
- en: '![Refer to caption](img/5b0e0485d76a1ae9f14fafb13e34eec3.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b0e0485d76a1ae9f14fafb13e34eec3.png)'
- en: (a) The Effect of $p$
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (a) $p$的影响
- en: '![Refer to caption](img/dd41e0957a22ca5ae5af3369b1594586.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd41e0957a22ca5ae5af3369b1594586.png)'
- en: (b) The Effect of $t$
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (b) $t$的影响
- en: '![Refer to caption](img/062fef6fa7951821b9628cb31c78f4d4.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/062fef6fa7951821b9628cb31c78f4d4.png)'
- en: (c) The Effect of $n$
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (c) $n$的影响
- en: 'Figure 3: Ablation Study of Harmful Behaviors Attack'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：有害行为攻击的消融研究
- en: The Effect of Dropping Ratio $p$
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 丢弃比例$p$的影响
- en: As observed in Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.4 Ablation Study ‣
    4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"), we note that a larger random dropping ratio  is smaller, the accuracy on
    benign samples remains at a high level, but it will also affect the efficacy of
    the robust alignment checking function, leading to a higher attack success rate.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM") 中可以观察到，较大的随机丢弃比例$p$较小，尽管对良性样本的准确率保持在较高水平，但也会影响鲁棒对齐检查功能的效果，从而导致更高的攻击成功率。
- en: The Effect of Threshold $t$
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 阈值$t$的影响
- en: Similarly, from Figure [3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4.4 Ablation Study ‣
    4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM"), we can observe that a too small  makes it difficult to reach the threshold
    to trigger the rejection of answering, resulting in only a limited reduction in
    the attack success rate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，从图 [3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣
    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM") 中可以观察到，过小的值使得难以达到触发拒绝回答的阈值，导致攻击成功率的减少有限。
- en: The Effect of Monte Carlo trials $n$
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 蒙特卡洛试验$n$的影响
- en: Furthermore, as observed in Figure [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.4 Ablation
    Study ‣ 4 Experiments ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM"), our method still exhibits good performance with various Monte Carlo
    trails. Even with very few Monte Carlo trials such as 15 and 10, our robustly
    aligned LLM maintains a benign answering rate close to 100% and a relatively low
    attack success rate. This suggests that reducing the number of Monte Carlo trials
    is a potential strategy to decrease computational overhead while maintaining stable
    defensive performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从图 [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.4 Ablation Study ‣ 4 Experiments ‣
    Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM") 中可以观察到，我们的方法在各种蒙特卡洛试验中仍表现良好。即使在只有15次和10次的蒙特卡洛试验中，我们的鲁棒对齐LLM也保持了接近100%的良性回答率和相对较低的攻击成功率。这表明，减少蒙特卡洛试验次数是降低计算开销同时保持稳定防御性能的潜在策略。
- en: 4.5 Computational Cost
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 计算成本
- en: 'In this section, we discuss the additional computational costs incurred by
    our robustly aligned LLM compared to the original LLM. Suppose the token counts
    for input content and LLM responses in a dialogue are , respectively, and the
    computational costs for each input and response token are , respectively. The
    total cost of the original LLM is:  and the proportion of input tokens randomly
    discarded in each sampling be . Hence, if $\text{AC}({\mathbf{x}})$ fails, the
    extra cost of our defense is:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了相较于原始LLM，我们的鲁棒对齐LLM所产生的额外计算成本。假设对话中输入内容和LLM响应的token数量分别为，以及每个输入和响应token的计算成本分别为。因此，原始LLM的总成本为：以及每次采样中随机丢弃的输入token比例为。假设$\text{AC}({\mathbf{x}})$失败，我们防御的额外成本为：
- en: '|  | $C_{\text{extra}}=(1-p)l_{\text{in}}\times c_{\text{in}}\times n+l_{\text{out}}\times
    c_{\text{out}}\times n,\text{ where~{}}l_{\text{out}}\leq t_{\text{max}}.$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{\text{extra}}=(1-p)l_{\text{in}}\times c_{\text{in}}\times n+l_{\text{out}}\times
    c_{\text{out}}\times n,\text{ 其中~{}}l_{\text{out}}\leq t_{\text{max}}.$ |  |'
- en: 'The ratio of the extra cost to the computational cost of the LLM without defense
    is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 额外成本与未防御 LLM 的计算成本之比为：
- en: '|  | $1$2 |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: If we approximate the value of  and outputs . To calculate the average computational
    cost per token, we refer to the pricing of the ChatGPT API. The GPT-4 model with
    an 8K context is priced at $0.03 / 1K tokens for input and $0.06 / 1K tokens for
    output, whereas the GPT-3.5 Turbo model with a 16K context is priced at $0.003
    / 1K tokens for input and $0.004 / 1K tokens for output.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们估算值和输出。为了计算每个标记的平均计算成本，我们参考 ChatGPT API 的定价。GPT-4 模型（8K 上下文）的输入定价为 $0.03
    / 1K 标记，输出定价为 $0.06 / 1K 标记，而 GPT-3.5 Turbo 模型（16K 上下文）的输入定价为 $0.003 / 1K 标记，输出定价为
    $0.004 / 1K 标记。
- en: After calculations, , ) as suggested in our ablation studies.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 经过计算，如我们的消融研究中建议的，）.
- en: 5 Conclusion and Future work
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: While a variety of alignment strategies have been proposed to guide the large
    language model to obey human ethical principles, recent works show that these
    alignments are vulnerable and can be bypassed by alignment-breaking attacks through
    carefully crafted adversarial prompts. In this work, we propose robustly aligned
    LLMs, which are built upon existing aligned LLMs with a robust alignment checking
    function, to defend against alignment-breaking attacks. One major advantage of
    our method is that there is no need to expensively retrain or fine-tune the original
    LLM for defense purposes. We also provide a theoretical analysis to verify the
    effectiveness of our proposed defense. The exhaustive experimental results clearly
    demonstrate our method can effectively defend against both automatically generated
    adversarial prompts and handcrafted jailbreak prompts.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经提出了各种对齐策略以指导大型语言模型遵循人类伦理原则，但最近的研究表明，这些对齐策略易受攻击，并且可以通过精心设计的对抗性提示绕过。在这项工作中，我们提出了具有鲁棒对齐功能的
    LLM，基于现有的对齐 LLM，旨在防御对齐破坏攻击。我们方法的一个主要优点是无需昂贵地重新训练或微调原始 LLM 以进行防御。我们还提供了理论分析以验证我们提出的防御的有效性。详尽的实验结果清楚地表明，我们的方法可以有效地防御自动生成的对抗性提示和手工制作的越狱提示。
- en: Note that it is non-trivial to directly apply the current alignment-breaking
    attack strategies (such as Zou et al. ([2023](#bib.bib50))) to our robustly aligned
    LLM due to the non-differentiability of our random dropping mechanism, which makes
    it hard to perform the gradient-based search or text optimization. So far it is
    under-explored whether the attackers could elaborately design stronger and more
    efficient attacks with the knowledge of our defense details. We leave this as
    our future work.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们随机丢弃机制的非可微分性，当前的对齐破坏攻击策略（如 Zou 等（[2023](#bib.bib50)））难以直接应用于我们鲁棒对齐的 LLM，这使得进行基于梯度的搜索或文本优化变得困难。到目前为止，攻击者是否能够凭借我们防御细节设计出更强大和更高效的攻击尚未得到充分探索。我们将此作为未来的工作。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alzantot et al. (2018) Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang
    Ho, Mani Srivastava, and Kai-Wei Chang. Generating natural language adversarial
    examples. *arXiv preprint arXiv:1804.07998*, 2018.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alzantot 等（2018）穆斯塔法·阿尔赞托、雅什·香玛、艾哈迈德·埃尔戈哈里、博·张·霍、马尼·斯里瓦斯塔瓦、和凯-魏·张。《生成自然语言对抗性示例》。*arXiv
    预印本 arXiv:1804.07998*，2018年。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. *arXiv preprint
    arXiv:2212.08073*, 2022.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白等（2022）尹涛·白、萨乌拉夫·卡达瓦斯、桑迪潘·昆杜、阿曼达·阿斯克尔、杰克逊·科尼翁、安迪·琼斯、安娜·陈、安娜·戈尔迪、阿扎利亚·米尔霍谢尼、卡梅伦·麦金农等。《宪法人工智能：来自人工智能反馈的无害性》。*arXiv
    预印本 arXiv:2212.08073*，2022年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）汤姆·布朗、本杰明·曼、尼克·赖德、梅拉尼·苏比亚、贾里德·D·卡普兰、普拉弗拉·达里瓦尔、阿尔文德·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。《语言模型是少样本学习者》。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等（2022）阿坎沙·乔杜里、沙兰·纳朗、雅各布·德夫林、马尔滕·博斯马、高拉夫·米什拉、亚当·罗伯茨、保罗·巴拉姆、金亨元、查尔斯·萨顿、塞巴斯蒂安·格赫曼等。《Palm：通过路径扩展语言建模》。*arXiv
    预印本 arXiv:2204.02311*，2022年。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30, 2017.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano等（2017）Paul F Christiano、Jan Leike、Tom Brown、Miljan Martic、Shane Legg
    和 Dario Amodei。基于人类偏好的深度强化学习。*神经信息处理系统进展*，第30卷，2017年。
- en: Cohen et al. (2019) Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified
    adversarial robustness via randomized smoothing. In *international conference
    on machine learning*, pp.  1310–1320\. PMLR, 2019.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen等（2019）Jeremy Cohen、Elan Rosenfeld 和 Zico Kolter。通过随机平滑实现认证对抗鲁棒性。在*国际机器学习会议*，页1310–1320。PMLR，2019年。
- en: Dong et al. (2021) Xinshuai Dong, Anh Tuan Luu, Rongrong Ji, and Hong Liu. Towards
    robustness against natural language word substitutions. *arXiv preprint arXiv:2107.13541*,
    2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等（2021）Xinshuai Dong、Anh Tuan Luu、Rongrong Ji 和 Hong Liu。针对自然语言词语替换的鲁棒性研究。*arXiv预印本
    arXiv:2107.13541*，2021年。
- en: 'Ebrahimi et al. (2017) Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou.
    Hotflip: White-box adversarial examples for text classification. *arXiv preprint
    arXiv:1712.06751*, 2017.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ebrahimi等（2017）Javid Ebrahimi、Anyi Rao、Daniel Lowd 和 Dejing Dou。Hotflip：用于文本分类的白盒对抗示例。*arXiv预印本
    arXiv:1712.06751*，2017年。
- en: Gao et al. (2018) Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box
    generation of adversarial text sequences to evade deep learning classifiers. In
    *2018 IEEE Security and Privacy Workshops (SPW)*, pp.  50–56\. IEEE, 2018.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等（2018）Ji Gao、Jack Lanchantin、Mary Lou Soffa 和 Yanjun Qi。黑盒生成对抗文本序列以逃避深度学习分类器。在*2018年IEEE安全与隐私研讨会（SPW）*，页50–56。IEEE，2018年。
- en: Go et al. (2023) Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen,
    Nahyeon Ryu, and Marc Dymetman. Aligning language models with preferences through
    f-divergence minimization. *arXiv preprint arXiv:2302.08215*, 2023.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Go等（2023）Dongyoung Go、Tomasz Korbak、Germán Kruszewski、Jos Rozen、Nahyeon Ryu
    和 Marc Dymetman。通过f-divergence最小化对齐语言模型与偏好。*arXiv预印本 arXiv:2302.08215*，2023年。
- en: Hazell (2023) Julian Hazell. Large language models can be used to effectively
    scale spear phishing campaigns. *arXiv preprint arXiv:2305.06972*, 2023.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hazell（2023）Julian Hazell。大型语言模型可以有效扩展网络钓鱼攻击。*arXiv预印本 arXiv:2305.06972*，2023年。
- en: 'Helbling et al. (2023) Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng
    Chau. Llm self defense: By self examination, llms know they are being tricked.
    *arXiv preprint arXiv:2308.07308*, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helbling等（2023）Alec Helbling、Mansi Phute、Matthew Hull 和 Duen Horng Chau。LLM自我防御：通过自我检查，LLMs知道它们被欺骗。*arXiv预印本
    arXiv:2308.07308*，2023年。
- en: Jain et al. (2023) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli,
    John Kirchenbauer, Ping yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping,
    and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language
    models, 2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jain等（2023）Neel Jain、Avi Schwarzschild、Yuxin Wen、Gowthami Somepalli、John Kirchenbauer、Ping
    Yeh Chiang、Micah Goldblum、Aniruddha Saha、Jonas Geiping 和 Tom Goldstein。针对对齐语言模型的对抗攻击的基线防御，2023年。
- en: Jia et al. (2019) Robin Jia, Aditi Raghunathan, Kerem Göksel, and Percy Liang.
    Certified robustness to adversarial word substitutions. *arXiv preprint arXiv:1909.00986*,
    2019.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia等（2019）Robin Jia、Aditi Raghunathan、Kerem Göksel 和 Percy Liang。对抗词语替换的认证鲁棒性。*arXiv预印本
    arXiv:1909.00986*，2019年。
- en: Jin et al. (2020) Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    Is bert really robust? a strong baseline for natural language attack on text classification
    and entailment. In *Proceedings of the AAAI conference on artificial intelligence*,
    volume 34, pp.  8018–8025, 2020.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等（2020）Di Jin、Zhijing Jin、Joey Tianyi Zhou 和 Peter Szolovits。BERT真的鲁棒吗？对文本分类和蕴涵的自然语言攻击的强基线。在*AAAI人工智能会议论文集*，第34卷，页8018–8025，2020年。
- en: 'Kang et al. (2023) Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei
    Zaharia, and Tatsunori Hashimoto. Exploiting programmatic behavior of llms: Dual-use
    through standard security attacks. *arXiv preprint arXiv:2302.05733*, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang等（2023）Daniel Kang、Xuechen Li、Ion Stoica、Carlos Guestrin、Matei Zaharia 和
    Tatsunori Hashimoto。利用LLMs的程序行为：通过标准安全攻击实现双重用途。*arXiv预印本 arXiv:2302.05733*，2023年。
- en: 'Keskar et al. (2019) Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming
    Xiong, and Richard Socher. Ctrl: A conditional transformer language model for
    controllable generation. *arXiv preprint arXiv:1909.05858*, 2019.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keskar等（2019）Nitish Shirish Keskar、Bryan McCann、Lav R Varshney、Caiming Xiong
    和 Richard Socher。Ctrl：一种用于可控生成的条件Transformer语言模型。*arXiv预印本 arXiv:1909.05858*，2019年。
- en: Korbak et al. (2023) Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak
    Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, and Ethan Perez.
    Pretraining language models with human preferences. In *International Conference
    on Machine Learning*, pp.  17506–17533\. PMLR, 2023.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Korbak et al. (2023) Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Vinayak
    Bhalerao, Christopher Buckley, Jason Phang, Samuel R Bowman, 和 Ethan Perez。通过人类偏好进行语言模型的预训练。见
    *国际机器学习会议*，第17506–17533页。PMLR，2023年。
- en: Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi,
    and Hima Lakkaraju. Certifying llm safety against adversarial prompting. *arXiv
    preprint arXiv:2309.02705*, 2023.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. (2023) Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Soheil Feizi,
    和 Hima Lakkaraju。对抗性提示下的LLM安全认证。*arXiv预印本 arXiv:2309.02705*，2023年。
- en: Li et al. (2023) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, and Yangqiu Song.
    Multi-step jailbreaking privacy attacks on chatgpt. *arXiv preprint arXiv:2304.05197*,
    2023.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, 和 Yangqiu Song。对ChatGPT的多步骤越狱隐私攻击。*arXiv预印本
    arXiv:2304.05197*，2023年。
- en: 'Li et al. (2018) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang.
    Textbugger: Generating adversarial text against real-world applications. *arXiv
    preprint arXiv:1812.05271*, 2018.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2018) Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, 和 Ting Wang。Textbugger:
    生成针对真实世界应用的对抗性文本。*arXiv预印本 arXiv:1812.05271*，2018年。'
- en: Maheshwary et al. (2021) Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi.
    Generating natural language attacks in a hard label black box setting. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pp.  13525–13533,
    2021.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maheshwary et al. (2021) Rishabh Maheshwary, Saket Maheshwary, 和 Vikram Pudi。在硬标签黑箱环境中生成自然语言攻击。见
    *AAAI人工智能会议论文集*，第35卷，第13525–13533页，2021年。
- en: Miyato et al. (2016) Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial
    training methods for semi-supervised text classification. *arXiv preprint arXiv:1605.07725*,
    2016.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miyato et al. (2016) Takeru Miyato, Andrew M Dai, 和 Ian Goodfellow。半监督文本分类的对抗训练方法。*arXiv预印本
    arXiv:1605.07725*，2016年。
- en: 'Nguyen (2023) Ha-Thanh Nguyen. A brief report on lawgpt 1.0: A virtual legal
    assistant based on gpt-3. *arXiv preprint arXiv:2302.05729*, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen (2023) Ha-Thanh Nguyen。关于lawgpt 1.0的简要报告：基于GPT-3的虚拟法律助手。*arXiv预印本 arXiv:2302.05729*，2023年。
- en: 'Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
    Tiwary, Rangan Majumder, and Li Deng. Ms marco: A human-generated machine reading
    comprehension dataset. 2016.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nguyen et al. (2016) Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh
    Tiwary, Rangan Majumder, 和 Li Deng。Ms marco: 一个由人类生成的机器阅读理解数据集。2016年。'
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4技术报告，2023年。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等。通过人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744，2022年。
- en: Ren et al. (2019) Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating
    natural language adversarial examples through probability weighted word saliency.
    In *Proceedings of the 57th annual meeting of the association for computational
    linguistics*, pp.  1085–1097, 2019.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren et al. (2019) Shuhuai Ren, Yihe Deng, Kun He, 和 Wanxiang Che。通过概率加权词汇显著性生成自然语言对抗样本。见
    *第57届计算语言学协会年会论文集*，第1085–1097页，2019年。
- en: 'Röttger et al. (2023) Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe
    Attanasio, Federico Bianchi, and Dirk Hovy. Xstest: A test suite for identifying
    exaggerated safety behaviours in large language models. *arXiv preprint arXiv:2308.01263*,
    2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Röttger et al. (2023) Paul Röttger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe
    Attanasio, Federico Bianchi, 和 Dirk Hovy。Xstest: 用于识别大型语言模型中夸大安全行为的测试套件。*arXiv预印本
    arXiv:2308.01263*，2023年。'
- en: 'Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and
    Yang Zhang. ” do anything now”: Characterizing and evaluating in-the-wild jailbreak
    prompts on large language models. *arXiv preprint arXiv:2308.03825*, 2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen et al. (2023) Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, 和 Yang
    Zhang。“现在做任何事”：特征化和评估大型语言模型上的真实环境越狱提示。*arXiv预印本 arXiv:2308.03825*，2023年。
- en: Shi et al. (2020) Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and
    Cho-Jui Hsieh. Robustness verification for transformers. *arXiv preprint arXiv:2002.06622*,
    2020.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2020) Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, 和 Cho-Jui
    Hsieh。变压器的鲁棒性验证。*arXiv预印本 arXiv:2002.06622*，2020年。
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning
    to summarize with human feedback. *Advances in Neural Information Processing Systems*,
    33:3008–3021, 2020.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon et al. (2020) 尼桑·斯蒂恩农、龙·欧扬、杰弗里·吴、丹尼尔·齐格勒、瑞安·洛、切尔西·沃斯、亚历克·拉德福德、达里奥·阿莫代伊、保罗·F·克里斯蒂亚诺。通过人类反馈学习总结。*神经信息处理系统进展*，33:3008–3021，2020。
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, pp.  1–11, 2023.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thirunavukarasu et al. (2023) 阿伦·詹姆斯·提鲁纳瓦卡拉苏、达伦·舒·郑婷、卡比兰·埃兰戈万、劳拉·古铁雷斯、丁芳·谭、丹尼尔·舒·伟婷。医学中的大型语言模型。*自然医学*，第1–11页，2023。
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023a) 乌戈·图夫隆、提博·拉夫里尔、戈蒂埃·伊扎卡尔、克萨维尔·马尔蒂内、玛丽-安妮·拉肖、蒂莫泰·拉克鲁瓦、巴蒂斯特·罗济耶、纳曼·戈亚尔、埃里克·汉布罗、费萨尔·阿扎尔等。Llama：开放且高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*，2023a。
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023b) 乌戈·图夫隆、路易斯·马丁、凯文·斯通、彼得·阿尔伯特、阿姆贾德·阿尔马赫里、雅斯敏·巴巴埃、尼古拉·巴什利科夫、苏姆亚·巴特拉、普拉杰瓦尔·巴尔伽瓦、舒鲁蒂·博萨尔等。Llama
    2：开放基础和微调聊天模型。*arXiv预印本 arXiv:2307.09288*，2023b。
- en: 'Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does llm safety training fail? *arXiv preprint arXiv:2307.02483*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) 亚历山大·韦、尼卡·哈赫塔拉布、雅各布·斯坦哈特。Jailbroken：大型语言模型安全培训失败的原因？*arXiv预印本
    arXiv:2307.02483*，2023。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) 韦杰森、王学之、戴尔·舒尔曼斯、马尔滕·博斯马、肖飞、艾德·奇、阮文立、丹尼·周等。链式思维提示在大型语言模型中引发推理。*神经信息处理系统进展*，35:24824–24837，2022。
- en: Wolf et al. (2023) Yotam Wolf, Noam Wies, Yoav Levine, and Amnon Shashua. Fundamental
    limitations of alignment in large language models. *arXiv preprint arXiv:2304.11082*,
    2023.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2023) 约塔姆·沃尔夫、诺阿姆·维斯、约阿夫·莱文、安农·沙舒阿。大型语言模型对齐的根本限制。*arXiv预印本 arXiv:2304.11082*，2023。
- en: 'Wu et al. (2023) Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark
    Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann.
    Bloomberggpt: A large language model for finance, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023) 吴世杰、奥赞·伊尔索伊、史蒂文·卢、瓦迪姆·达布拉沃尔斯基、马克·德雷兹、塞巴斯蒂安·盖尔曼、普拉班詹·坎巴杜尔、大卫·罗森伯格、吉迪恩·曼。Bloomberggpt：金融领域的大型语言模型，2023。
- en: 'Xu et al. (2020a) Han Xu, Yao Ma, Hao-Chen Liu, Debayan Deb, Hui Liu, Ji-Liang
    Tang, and Anil K Jain. Adversarial attacks and defenses in images, graphs and
    text: A review. *International Journal of Automation and Computing*, 17:151–178,
    2020a.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2020a) 韩旭、马耀、刘浩辰、德巴扬·德布、刘辉、唐季良、安尼尔·K·贾恩。图像、图形和文本中的对抗攻击与防御：综述。*国际自动化与计算期刊*，17:151–178，2020a。
- en: Xu et al. (2020b) Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston,
    and Emily Dinan. Recipes for safety in open-domain chatbots. *arXiv preprint arXiv:2010.07079*,
    2020b.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2020b) 许静、达·居、玛格丽特·李、Y-Lan·布劳、杰森·韦斯顿、艾米莉·迪南。开放域聊天机器人中的安全配方。*arXiv预印本
    arXiv:2010.07079*，2020b。
- en: Xu et al. (2020c) Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang,
    Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation
    analysis for scalable certified robustness and beyond. *Advances in Neural Information
    Processing Systems*, 33:1129–1141, 2020c.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2020c) 许凯迪、施周星、张焕、王怡涵、常凯伟、黄敏磊、布哈维亚·凯尔库拉、林雪、谢超举。自动扰动分析用于可扩展认证鲁棒性及其他。*神经信息处理系统进展*，33:1129–1141，2020c。
- en: 'Ye et al. (2022) Muchao Ye, Jinghui Chen, Chenglin Miao, Ting Wang, and Fenglong
    Ma. Leapattack: Hard-label adversarial attack on text via gradient-based optimization.
    In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining*, pp.  2307–2315, 2022.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2022) 余木超、陈靖辉、苗成林、汪婷、马风龙。Leapattack：通过基于梯度的优化对文本进行硬标签对抗攻击。载于*第28届ACM
    SIGKDD知识发现与数据挖掘会议论文集*，第2307–2315页，2022。
- en: 'Ye et al. (2023) Muchao Ye, Jinghui Chen, Chenglin Miao, Han Liu, Ting Wang,
    and Fenglong Ma. Pat: Geometry-aware hard-label black-box adversarial attacks
    on text. In *Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery
    and Data Mining*, pp.  3093–3104, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2023) 叶睦超、陈静辉、苗成林、刘涵、汪婷和马风龙。PAT: 几何感知的硬标签黑箱对抗攻击文本。发表于*第29届ACM SIGKDD知识发现与数据挖掘会议论文集*，第3093-3104页，2023年。'
- en: 'Yuan et al. (2023) Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang,
    Pinjia He, Shuming Shi, and Zhaopeng Tu. Gpt-4 is too smart to be safe: Stealthy
    chat with llms via cipher. *arXiv preprint arXiv:2308.06463*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) 游良、焦文祥、王文轩、黄振澤、何品佳、施书铭和屠召鹏。GPT-4 太聪明了，无法安全：通过密码与LLMs进行隐秘聊天。*arXiv
    预印本 arXiv:2308.06463*，2023年。
- en: Zeng et al. (2023) Jiehang Zeng, Jianhan Xu, Xiaoqing Zheng, and Xuanjing Huang.
    Certified robustness to text adversarial attacks by randomized [mask]. *Computational
    Linguistics*, 49(2):395–427, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. (2023) 曾杰航、徐建涵、郑晓青和黄宣靖。通过随机化[mask]对文本对抗攻击的认证鲁棒性。*计算语言学*，49(2):395–427，2023年。
- en: Zhou et al. (2021) Yi Zhou, Xiaoqing Zheng, Cho-Jui Hsieh, Kai-Wei Chang, and
    Xuanjing Huan. Defense against synonym substitution-based adversarial attacks
    via dirichlet neighborhood ensemble. In *Association for Computational Linguistics
    (ACL)*, 2021.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2021) 周毅、郑晓青、谢卓睿、常开伟和黄宣靖。通过Dirichlet邻域集成防御基于同义词替换的对抗攻击。发表于*计算语言学协会（ACL）*，2021年。
- en: 'Zhu et al. (2019) Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and
    Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding.
    *arXiv preprint arXiv:1909.11764*, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2019) 陈朱、余程、詹根、孙思琪、汤姆·戈尔德斯坦和刘晶晶。Freelb: 增强的对抗训练用于自然语言理解。*arXiv
    预印本 arXiv:1909.11764*，2019年。'
- en: 'Zhu et al. (2023) Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao
    Wang, Furong Huang, Ani Nenkova, and Tong Sun. Autodan: Automatic and interpretable
    adversarial attacks on large language models. *arXiv preprint arXiv:2310.15140*,
    2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. (2023) 朱思成、张瑞义、安邦、吴刚、乔·巴罗、王自超、黄芙蓉、安妮·嫩科娃和孙通。Autodan: 自动且可解释的大语言模型对抗攻击。*arXiv
    预印本 arXiv:2310.15140*，2023年。'
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint arXiv:2307.15043*, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2023) 安迪·邹、王子凡、J·Zico Kolter 和 马特·弗雷德里克森。对齐语言模型的通用和可转移对抗攻击。*arXiv
    预印本 arXiv:2307.15043*，2023年。
- en: Appendix A Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical
    Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks
    via Robustly Aligned LLM")
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 定理 [3.1](#S3.Thmtheorem1 "定理 3.1. ‣ 3.4 理论分析 ‣ 3 我们提出的方法 ‣ 通过稳健对齐的LLM防御对齐破坏攻击")
    证明
- en: In this section, we provide the proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem
    3.1\. ‣ 3.4 Theoretical Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM").
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供定理 [3.1](#S3.Thmtheorem1 "定理 3.1. ‣ 3.4 理论分析 ‣ 3 我们提出的方法 ‣ 通过稳健对齐的LLM防御对齐破坏攻击")
    的证明。
- en: Proof of Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4 Theoretical Analysis
    ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM").
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 [3.1](#S3.Thmtheorem1 "定理 3.1. ‣ 3.4 理论分析 ‣ 3 我们提出的方法 ‣ 通过稳健对齐的LLM防御对齐破坏攻击")
    的证明。
- en: 'The part of the proof for Theorem [3.1](#S3.Thmtheorem1 "Theorem 3.1\. ‣ 3.4
    Theoretical Analysis ‣ 3 Our Proposed Method ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM") is inspired from (Zeng et al., [2023](#bib.bib46)).
    Denote  where , and denote the inserted adversarial prompt as , we have the following
    equations based on the law of total probability:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [3.1](#S3.Thmtheorem1 "定理 3.1. ‣ 3.4 理论分析 ‣ 3 我们提出的方法 ‣ 通过稳健对齐的LLM防御对齐破坏攻击")
    证明的部分灵感来源于 (Zeng et al., [2023](#bib.bib46))。记为，其中，记插入的对抗性提示为，我们基于全概率法则得到以下方程：
- en: '|  |  |  | (4) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (4) |'
- en: '|  |  |  |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: and
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  |  |  | (5) |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (5) |'
- en: '|  |  |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: When . Thus, there is
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: When . Thus, there is
- en: '|  | $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)=\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)$
    |  | (6) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{pad}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)=\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})&#124;[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}=\emptyset)$
    |  | (6) |'
- en: Given Equation [6](#A1.E6 "In Proof of Theorem 3.1\. ‣ Appendix A Proof of Theorem
    3.1 ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM"),
    , we could compute $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})|[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\geq
    0$ as follows
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 给定方程 [6](#A1.E6 "在定理 3.1 的证明中 ‣ 附录 A 定理 3.1 的证明 ‣ 通过稳健对齐的 LLM 防御对齐破坏攻击")，，我们可以计算
    $\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim U}(\text{AC}(f([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}))=\text{Fail})|[{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\mathop{{\mathbb{P}}}\limits_{{\mathbf{r}}\sim
    U}([{\mathbf{x}}_{\text{adv}}^{j}]_{{\mathbf{r}}}\cap{\mathbf{p}}_{\text{adv}}^{j}\neq\emptyset)\geq
    0$ 如下
- en: '|  |  |  | (7) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (7) |'
- en: '|  |  |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: If , thus we have
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，因此我们有
- en: '|  | $1$2 |  | (8) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: If , we have <math id=$$. Based on Equation [8](#A1.E8 "In Proof of Theorem
    3.1\. ‣ Appendix A Proof of Theorem 3.1 ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM"), we can conclude that
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果，我们有 <math id=$$. 基于方程 [8](#A1.E8 "在定理 3.1 的证明中 ‣ 附录 A 定理 3.1 的证明 ‣ 通过稳健对齐的
    LLM 防御对齐破坏攻击")，我们可以得出结论
- en: '|  | $1$2 |  | (9) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: then for any . Therefore, we obtain that $\text{RAC}({\mathbf{x}}_{\text{adv}})=\text{Fail}$.
    This concludes the proof. ∎
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 那么对于任何 . 因此，我们得到 $\text{RAC}({\mathbf{x}}_{\text{adv}})=\text{Fail}$。这就结束了证明。
    ∎
- en: Appendix B Concrete Examples
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 具体示例
- en: In this section, we also give a few concrete examples comparing the output of
    the original LLM and our robustly aligned LLM under alignment-breaking attacks.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们还提供了一些具体的例子，比较了在对齐破坏攻击下，原始 LLM 和我们稳健对齐的 LLM 的输出。
- en: '![Refer to caption](img/a63cee0a73a2e27041b77ab6aa4a3cd3.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a63cee0a73a2e27041b77ab6aa4a3cd3.png)'
- en: 'Figure 4: Multiple real cases of the original LLM’s response before and after
    random dropping under harmful behaviors attack.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：原始 LLM 在有害行为攻击下随机丢弃前后的多个实际案例响应。
- en: '![Refer to caption](img/928b0a950ab9ccbc813f4c8733f735b7.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/928b0a950ab9ccbc813f4c8733f735b7.png)'
- en: 'Figure 5: Multiple real cases of the original LLM’s response before and after
    random under handcrafted jailbreak attack. Note that in this example, we have
    not explicitly labeled what is discarded.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：原始 LLM 在手工制作的越狱攻击下随机丢弃前后的多个实际案例响应。请注意，在此示例中，我们没有明确标记被丢弃的部分。
- en: Appendix C Potential Adaptive Attack
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 潜在的自适应攻击
- en: In this section, we will discuss potential adaptive attack methods against the
    defense mechanism we proposed. Since our method randomly drops  and uses Monte
    Carlo sampling to simulate all possible scenarios, any form of adversarial prompt
    may be discarded. Hence, it’s challenging to design an adaptive attack based on
    optimization for our defense method. However, one may also utilize this design
    choice and simply try increasing the length of the adversarial prompts (e.g.,
    repeat the adversarial prompts after input for several times) to ensure the random
    dropping cannot fully remove the adversarial parts.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论针对我们提出的防御机制的潜在自适应攻击方法。由于我们的方法随机丢弃 并使用蒙特卡洛采样来模拟所有可能的场景，因此任何形式的对抗性提示都可能被丢弃。因此，基于优化来设计适应性攻击对于我们的防御方法来说是具有挑战性的。然而，人们也可以利用这一设计选择，简单地尝试增加对抗性提示的长度（例如，在输入后重复对抗性提示几次）以确保随机丢弃无法完全去除对抗部分。
- en: In order to figure out whether such a potential adaptive attack can invalidate
    our defense or not, we conducted experiments on the Harmful Behaviors attack on
    both the original LLM and our robustly aligned LLM. The results are presented
    in Table [3](#A3.T3 "Table 3 ‣ Appendix C Potential Adaptive Attack ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM"). We found that,
    on the original LLM, repeating the adversarial prompt multiple times in the input
    also leads to a reduction in the attack success rate. We speculate that the adversarial
    prompt might heavily depend on its position within the full input. Similarly,
    we observed that on our robustly aligned LLM, the attack success rate decreases
    as the number of repetitions increases. What’s more, at various repetition counts,
    our defense method keeps the attack success rate lower than scenarios without
    repetitions, hovering around 5%. This suggests that even if attackers are familiar
    with our defense and want to use longer repetitive adversarial prompts, our method
    remains effective in thwarting their attacks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定这种潜在的适应性攻击是否会使我们的防御失效，我们对原始 LLM 和我们稳健对齐的 LLM 进行了有害行为攻击的实验。结果见表 [3](#A3.T3
    "Table 3 ‣ Appendix C Potential Adaptive Attack ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM")。我们发现，在原始 LLM 上，多次重复对抗性提示也会导致攻击成功率降低。我们推测，对抗性提示可能严重依赖于其在完整输入中的位置。同样地，我们观察到，在我们的稳健对齐
    LLM 上，攻击成功率随着重复次数的增加而降低。此外，在不同的重复次数下，我们的防御方法使攻击成功率保持在低于没有重复的场景，约为 5%。这表明，即使攻击者熟悉我们的防御并希望使用更长的重复对抗性提示，我们的方法仍然有效地阻止了他们的攻击。
- en: 'Table 3: Adaptive attack success rate in our robustly aligned LLM. Repetition
    Times represents the number of repetitions of adversarial prompts'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：我们稳健对齐 LLM 的适应性攻击成功率。重复次数表示对抗性提示的重复次数
- en: '| Repetition Times | No Repetition | 2 | 3 | 5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 重复次数 | 无重复 | 2 | 3 | 5 |'
- en: '| Original LLM | 100.0% | 46.0% | 34.0% | 31.0% |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 原始 LLM | 100.0% | 46.0% | 34.0% | 31.0% |'
- en: '| Robustly Aligned LLM | 11.0% | 5.0% | 6.0% | 3.0% |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 稳健对齐 LLM | 11.0% | 5.0% | 6.0% | 3.0% |'
- en: Appendix D Defensive Efficacy Against Harmful Strings Attack
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 对有害字符串攻击的防御效能
- en: 'Table 4: The benign answering rate and attack success rate of the original
    LLM and our robustly aligned LLM under two adversarial alignment-breaking attacks.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在两种对抗性对齐破坏攻击下，原始 LLM 和我们稳健对齐的 LLM 的良性回答率和攻击成功率。
- en: '| Attack | BAR | ASR | ASR reduce |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | BAR | ASR | ASR 降低 |'
- en: '| Original LLM | RA-LLM | Original LLM | RA-LLM |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 原始 LLM | RA-LLM | 原始 LLM | RA-LLM |'
- en: '| Adv Strings | 100.0% | 99.0% | 84.0% | 0 | 84.0% |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 对抗字符串 | 100.0% | 99.0% | 84.0% | 0 | 84.0% |'
- en: We also conducted experiments under the setting of Harmful String Attack proposed
    in (Zou et al., [2023](#bib.bib50)). The goal of Harmful Strings attack is to
    compute an adversarial input, which can induce the LLM to generate a specific
    harmful string. Although this setting does not really fit in our threat model,
    it would also be interesting to see how RA-LLM performs under this attack. We
    conducted experiments on the Vicuna-7B-v1.3 model, and the results are presented
    in Table [4](#A4.T4 "Table 4 ‣ Appendix D Defensive Efficacy Against Harmful Strings
    Attack ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM").
    It can be observed that, in the original LLM, the attack success rate of adversarial
    prompts generated by Harmful String Attack is as high as 84%, while after applying
    our RA-LLM, the attack success rate drops to 0%. This indicates that our strategy
    can also effectively mitigate Harmful String Attack.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 (Zou et al., [2023](#bib.bib50)) 提出的有害字符串攻击设置下进行了实验。Harmful Strings 攻击的目标是计算一个对抗输入，该输入可以诱使
    LLM 生成特定的有害字符串。虽然这种设置并不完全符合我们的威胁模型，但看看 RA-LLM 在这种攻击下的表现也是有趣的。我们在 Vicuna-7B-v1.3
    模型上进行了实验，结果见表 [4](#A4.T4 "Table 4 ‣ Appendix D Defensive Efficacy Against Harmful
    Strings Attack ‣ Defending Against Alignment-Breaking Attacks via Robustly Aligned
    LLM")。可以观察到，在原始 LLM 中，由 Harmful String Attack 生成的对抗性提示的攻击成功率高达 84%，而在应用我们的 RA-LLM
    后，攻击成功率降至 0%。这表明我们的策略也能有效减轻 Harmful String Attack。
- en: Appendix E Collaborating with Safety Alignment on LLMs to Counteract Attacks
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 与安全对齐合作以应对 LLM 的攻击
- en: We have shown in the experiments that applying the random dropping strategy
    on malicious requests with adversarial prompts can effectively trigger the alignment
    of the model. However, for benign requests, random dropping may lead to a loss
    of key information and make the LLM occasionally generate unable-to-answer responses
    similar to typical alignment responses. This leads to a certain level of decrease
    in terms of benign answering rate.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在实验中表明，将随机丢弃策略应用于恶意请求与对抗性提示可以有效触发模型的对齐。然而，对于良性请求，随机丢弃可能导致关键信息的丢失，并使LLM偶尔生成类似于典型对齐响应的无法回答的响应。这导致良性回答率在某种程度上下降。
- en: Clearly, we can further reduce the loss on benign answering rate if the alignment
    response of the LLMs can be distinguishable from the other types of unable-to-answer
    responses. For instance, during the alignment fine-tuning process, the LLM is
    instructed to always start the response to malicious requests with a special token.
    When applying our defensive method, it is only necessary to output and check the
    first token in each Monte Carlo trial. Such a collaborative strategy on alignment
    and RA-LLM will not only significantly improve our recognition accuracy for malicious
    inputs but also help in further reducing computational costs. Due to limited resources,
    we leave this part as our future work.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果LLM的对齐响应可以与其他类型的无法回答的响应区分开来，我们可以进一步减少良性回答率的损失。例如，在对齐微调过程中，LLM被指示始终以特殊令牌开始对恶意请求的响应。在应用我们的防御方法时，只需在每次蒙特卡洛试验中输出并检查第一个令牌即可。这种在对齐和RA-LLM上的协作策略不仅会显著提高我们对恶意输入的识别准确性，还将有助于进一步降低计算成本。由于资源有限，我们将这部分留作未来的工作。
- en: Appendix F Comparison with LLM Self Defense
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F与LLM自我防御的比较
- en: In this section, we additionally compared our method with the concurrent work,
    LLM Self Defense (Helbling et al., [2023](#bib.bib12)). We use the more effective
    ”Questions as suffix” setting, that simply utilizes itself or another LLM instance
    to detect if its own response is harmful through the instruction such as
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们还将我们的方法与当前的工作进行比较，即LLM自我防御（Helbling等人，[2023](#bib.bib12)）。我们使用了更有效的“问题作为后缀”设置，该设置简单地利用自身或另一个LLM实例通过指令检测其响应是否有害，例如
- en: Here is some text:{response} Is it harmful? Give answer as “Yes, it is harmful”
    or “No, it is harmless”.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些文本:{response} 它有害吗？请回答为“是的，它有害”或“没有，它无害”。
- en: Specifically, we conducted both Self Defense through itself and Self Defense
    through GPT3.5 and summarized the comparison results in Table [5](#A6.T5 "Table
    5 ‣ Appendix F Comparison with LLM Self Defense ‣ Defending Against Alignment-Breaking
    Attacks via Robustly Aligned LLM"). We can observe that the attacked models (i.e.,
    Vicuna and Guanaco) inherently struggle to discern harmful content, leading to
    low BAR. Moreover, the outputs of the detection prompt contain unexpected content
    including deviations from the predefined responses of ”Yes, it is harmful.” or
    ”No, it is harmless.”, continue completion of the unfinished content from the
    previous context, and even non-responsiveness from LLMs. These suggest that in
    scenarios preventing the use of external powerful models, LLM Self Defense might
    be less effective.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们进行了自我防御和通过GPT3.5进行的自我防御，并在表[5](#A6.T5 "Table 5 ‣ Appendix F Comparison
    with LLM Self Defense ‣ Defending Against Alignment-Breaking Attacks via Robustly
    Aligned LLM")中总结了比较结果。我们可以观察到，被攻击的模型（即Vicuna和Guanaco）在辨别有害内容方面固有困难，导致低BAR。此外，检测提示的输出包含意外内容，包括偏离预定义的“是的，它有害。”或“没有，它无害。”的响应，从上一个上下文继续完成未完成的内容，甚至LLM的无响应。这表明在防止使用外部强大模型的场景中，LLM自我防御可能效果较差。
- en: While Self Defense though more powerful LLM instances such as GPT3.5 demonstrates
    higher accuracy in identifying harmful content and thus enjoys on-par defending
    effectiveness with our method, it still suffers from lower BARs. This could be
    attributed to the current LLM’s overcautiousness in detecting harmful content
    (Röttger et al., [2023](#bib.bib29)).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管自我防御通过更强大的LLM实例如GPT3.5在识别有害内容方面表现出更高的准确性，从而与我们的方法具有同等的防御效果，但它仍然面临较低的BAR。这可能归因于当前LLM在检测有害内容时的过度谨慎（Röttger等人，[2023](#bib.bib29)）。
- en: 'Table 5: The benign answering rate and attack success rate of the original
    LLM, self Defense, self Defense by GPT3.5, and our RA-LLM under individual adversarial
    alignment-breaking attacks.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：原始LLM、自我防御、GPT3.5自我防御和我们的RA-LLM在单独对抗性对齐破坏攻击下的良性回答率和攻击成功率。
- en: '| Models | BAR | ASR |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BAR | ASR |'
- en: '| Original LLM | Self Defense | GPT3.5 | RA-LLM | Original LLM | Self Defense
    | GPT3.5 | RA-LLM |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 原始LLM | 自我防御 | GPT3.5 | RA-LLM | 原始LLM | 自我防御 | GPT3.5 | RA-LLM |'
- en: '| Vicuna-7B-chat-HF | 99.3% | 68.7% | 90.0% | 98.7% | 98.7% | 22.7% | 8.0%
    | 10.7% |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B-chat-HF | 99.3% | 68.7% | 90.0% | 98.7% | 98.7% | 22.7% | 8.0%
    | 10.7% |'
- en: '| Guanaco-7B-HF | 95.3% | 41.3% | 87.3% | 92.0% | 96.0% | 52.0% | 8.7% | 6.7%
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 41.3% | 87.3% | 92.0% | 96.0% | 52.0% | 8.7% | 6.7%
    |'
- en: Appendix G Comparison with Perplexity-Based Defense
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 与困惑度防御的比较
- en: Perplexity-based defense proposed by Jain et al. ([2023](#bib.bib13)) detects
    adversarial prompts by checking if the perplexity of the prompt is greater than
    a threshold. Following the same threshold adopted in Zhu et al. ([2023](#bib.bib49)),
    we report the comparison results in Table [6](#A7.T6 "Table 6 ‣ Appendix G Comparison
    with Perplexity-Based Defense ‣ Defending Against Alignment-Breaking Attacks via
    Robustly Aligned LLM"). We can observe that even though perplexity defense achieves
    high BAR and effectively reduces the ASR of individual GCG attacks, this defense
    mechanism completely fails to detect handcrafted jailbreak prompts, presumably
    owing to the lower perplexity of these prompts, as they are manually written by
    humans. A similar conclusion is also validated in Zhu et al. ([2023](#bib.bib49)).
    In contrast, our method effectively defends against handcrafted jailbreak prompts.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Jain 等人（[2023](#bib.bib13)）提出的困惑度防御通过检查提示的困惑度是否超过阈值来检测对抗性提示。根据 Zhu 等人（[2023](#bib.bib49)）采用的相同阈值，我们在表
    [6](#A7.T6 "Table 6 ‣ Appendix G Comparison with Perplexity-Based Defense ‣ Defending
    Against Alignment-Breaking Attacks via Robustly Aligned LLM") 中报告了比较结果。我们可以观察到，尽管困惑度防御实现了高BAR并有效减少了个体GCG攻击的ASR，但该防御机制完全无法检测手工编写的越狱提示，可能是因为这些提示的困惑度较低，因为它们是由人手工编写的。Zhu
    等人（[2023](#bib.bib49)）的研究也验证了类似的结论。相比之下，我们的方法有效地防御了手工编写的越狱提示。
- en: 'Table 6: The benign answering rate and attack success rate of the original
    LLM, perplexity defense, and our robustly aligned LLM under two alignment-breaking
    attacks.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在两种对齐破坏攻击下，原始LLM、困惑度防御和我们鲁棒对齐LLM的良性回答率和攻击成功率。
- en: '| Attack | Models | BAR | ASR |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 攻击 | 模型 | BAR | ASR |'
- en: '| Original LLM | Perplexity Defense | RA-LLM | Original LLM | Perplexity Defense
    | RA-LLM |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 原始LLM | 困惑度防御 | RA-LLM | 原始LLM | 困惑度防御 | RA-LLM |'
- en: '| Individual GCG | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 0% |
    10.7% |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 单独GCG | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 0% | 10.7% |'
- en: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 96.0% | 4% | 6.7% |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 96.0% | 4% | 6.7% |'
- en: '| Handcrafted prompt | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% |
    100% | 12.0% |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 手工提示 | Vicuna-7B-chat-HF | 99.3% | 98.0% | 98.7% | 98.7% | 100% | 12.0% |'
- en: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 94.7% | 100% | 9.3% |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Guanaco-7B-HF | 95.3% | 100% | 92.0% | 94.7% | 100% | 9.3% |'
- en: Appendix H Time Cost
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 时间成本
- en: To further reduce the cost of RA-LLM, we implemented an early-exit mechanism
    in the Monte Carlo simulation. Specifically, if the number of detected failure
    cases exceeds our predefined threshold during the Monte Carlo simulation, RA-LLM
    terminates the process early and marks the input as a malicious sample. For instance,
    with Monte Carlo trials at , RA-LLM designates an input as malicious if it detects
    $0.2\times 20=4$ aligned responses. If 4 aligned responses are detected in the
    first 6 Monte Carlo trials, the remaining 14 trials will not be executed. Similarly,
    if no aligned responses are found in the first 17 trials, the input is immediately
    classified as benign, and the last 3 trials are skipped. This approach helps to
    further reduce computational costs.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步降低RA-LLM的成本，我们在蒙特卡罗模拟中实现了一个提前退出机制。具体来说，如果在蒙特卡罗模拟过程中检测到的失败案例数量超过了我们预定义的阈值，RA-LLM会提前终止过程并将输入标记为恶意样本。例如，在蒙特卡罗试验中，如果检测到$0.2\times
    20=4$个对齐响应，RA-LLM将输入标记为恶意。如果在前6次蒙特卡罗试验中检测到4个对齐响应，则剩余的14次试验将不再执行。类似地，如果在前17次试验中未发现对齐响应，输入将立即被分类为良性，最后3次试验将被跳过。这种方法有助于进一步减少计算成本。
- en: We evaluated 150 attack samples on both Vicuna-7B-chat-HF and Guanaco-7B-HF
    models, measuring the normal inference time, the time required by RA-LLM, and
    the time taken by RA-LLM after forcibly completing the entire Monte Carlo simulation
    process. We set the maximum token generation during normal inference at 1,000\.
    For RA-LLM, we follow the default setting, and we conducted all experiments on
    an NVIDIA RTX A6000 GPU.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Vicuna-7B-chat-HF 和 Guanaco-7B-HF 模型上评估了 150 个攻击样本，测量了正常推理时间、RA-LLM 所需时间，以及
    RA-LLM 在强制完成整个蒙特卡洛模拟过程后的时间。我们将正常推理时的最大生成标记设置为 1,000\。对于 RA-LLM，我们遵循默认设置，所有实验均在
    NVIDIA RTX A6000 GPU 上进行。
- en: For the Vicuna-7B-chat-HF model, normal inference took 20.97 seconds per data
    on average, RA-LLM required an extra 3.93 seconds per data on average, and RA-LLM
    with the full Monte Carlo simulation required an extra 9.26 seconds per data on
    average. For the Guanaco-7B-HF model, these averages were 30.36 seconds for normal
    inference, extra 3.76 seconds for RA-LLM, and an extra 12.84 seconds for the full
    Monte Carlo simulation. It is observed that the time required for RA-LLM is less
    than 20% (18.7% and 12.0%) of the normal inference time. Even in the worst-case
    scenario, where each instance undergoes a full Monte Carlo simulation, the additional
    time cost does not exceed 45% (44.1% and 42.3%). We believe this cost is acceptable.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Vicuna-7B-chat-HF 模型，正常推理平均耗时 20.97 秒，RA-LLM 平均额外耗时 3.93 秒，RA-LLM 完整蒙特卡洛模拟平均额外耗时
    9.26 秒。对于 Guanaco-7B-HF 模型，这些平均值分别为正常推理 30.36 秒、RA-LLM 额外 3.76 秒、完整蒙特卡洛模拟额外 12.84
    秒。观察到 RA-LLM 所需时间少于正常推理时间的 20%（18.7% 和 12.0%）。即使在最坏的情况下，每个实例经历完整的蒙特卡洛模拟，额外时间成本也不会超过
    45%（44.1% 和 42.3%）。我们认为这个成本是可以接受的。
