- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:47:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Model Leeching: An Extraction Attack Targeting LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型吸血：针对 LLM 的提取攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.10544](https://ar5iv.labs.arxiv.org/html/2309.10544)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.10544](https://ar5iv.labs.arxiv.org/html/2309.10544)
- en: Lewis Birch
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘易斯·伯奇
- en: Lancaster University    William Hackett
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 兰卡斯特大学    威廉·哈克特
- en: Lancaster University    Stefan Trawicki
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 兰卡斯特大学    斯特凡·特拉维基
- en: Lancaster University    Neeraj Suri
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 兰卡斯特大学    尼拉杰·苏里
- en: Lancaster University    Peter Garraghan
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 兰卡斯特大学    彼得·加拉汉
- en: Lancaster University, Mindgard
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 兰卡斯特大学，Mindgard
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Model Leeching* is a novel extraction attack targeting Large Language Models
    (LLMs), capable of distilling task-specific knowledge from a target LLM into a
    reduced parameter model. We demonstrate the effectiveness of our attack by extracting
    task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match (EM) similarity,
    and SQuAD EM and F1 accuracy scores of 75% and 87%, respectively for only $50
    in API cost. We further demonstrate the feasibility of adversarial attack transferability
    from an extracted model extracted via *Model Leeching* to perform ML attack staging
    against a target LLM, resulting in an 11% increase to attack success rate when
    applied to ChatGPT-3.5-Turbo.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*Model Leeching* 是一种新型的提取攻击，针对大型语言模型（LLMs），能够将任务特定的知识从目标 LLM 中提取到一个减少参数的模型中。我们通过从
    ChatGPT-3.5-Turbo 中提取任务能力来展示我们攻击的有效性，取得了 73% 的精确匹配（EM）相似度，以及 SQuAD EM 和 F1 准确率分别为
    75% 和 87%，API 成本仅为 50 美元。我们进一步展示了如何将从 *Model Leeching* 提取的模型用于对目标 LLM 进行对抗性攻击转移，应用于
    ChatGPT-3.5-Turbo 时攻击成功率提高了 11%。'
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have seen rapid adoption given their proficiency
    in handling complex natural language processing (NLP) tasks. LLMs leverage Deep
    Learning (DL) algorithms to process and understand a variety of natural language
    tasks spanning text completion, Question & Answering, and summarization [[24](#bib.bib24)].
    While production LLMs such as ChatGPT, BARD, and LLaMA [[18](#bib.bib18)] [[1](#bib.bib1)]
    [[8](#bib.bib8)] have garnered substantial attention, their uptake has also highlighted
    pressing concerns on growing their exposure to adversarial attacks [[8](#bib.bib8)].
    Studies on adversarial attacks against LLMs are limited, with urgent need to investigate
    their risk to data leakage, model stealing (extraction), and attack transferability
    across models[[3](#bib.bib3)][[31](#bib.bib31)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在处理复杂自然语言处理（NLP）任务方面的能力而被迅速采用。LLMs 利用深度学习（DL）算法来处理和理解各种自然语言任务，包括文本完成、问答和摘要
    [[24](#bib.bib24)]。虽然像 ChatGPT、BARD 和 LLaMA [[18](#bib.bib18)] [[1](#bib.bib1)]
    [[8](#bib.bib8)] 这样的生产级 LLM 已引起了广泛关注，但它们的采用也突出了增加对抗攻击暴露的紧迫问题 [[8](#bib.bib8)]。关于
    LLM 的对抗攻击研究有限，急需调查其对数据泄露、模型窃取（提取）和模型间攻击转移性的风险 [[3](#bib.bib3)][[31](#bib.bib31)]。
- en: 'In this paper we propose Model Leeching, an extraction attack against LLMs
    capable of creating an extracted model via distilling task knowledge from a target
    LLM. Our attack is performed by designing an automated prompt generation system
    [[12](#bib.bib12)] targeting specific tasks within LLMs. The prompt system is
    used to create an extracted model by extracting and copying task-specific data
    characteristics from a target model [[28](#bib.bib28)]. *Model Leeching* attack
    is applicable to any LLM with a public API endpoint, and can be successfully achieved
    at minimal economic cost. Moreover, we demonstrate how Model Leeching can be exploited
    to perform ML attack staging onto other LLMs (including the original target LLM).
    Our contributions are:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了模型吸血（*Model Leeching*），这是一种针对 LLM 的提取攻击，能够通过从目标 LLM 中提取任务知识来创建提取模型。我们的攻击通过设计一个自动化提示生成系统
    [[12](#bib.bib12)]，针对 LLM 中的特定任务进行。该提示系统用于通过从目标模型中提取和复制任务特定的数据特征来创建提取模型 [[28](#bib.bib28)]。*Model
    Leeching* 攻击适用于任何具有公共 API 端点的 LLM，并且可以在最低经济成本下成功实现。此外，我们展示了如何利用模型吸血来对其他 LLM（包括原目标
    LLM）进行机器学习攻击的阶段性操作。我们的贡献包括：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the *Model Leeching* attack method, and demonstrate its effectiveness
    against LLMs via experimentation using an extraction attack framework [[9](#bib.bib9)].
    Targeting the ChatGPT-3.5-Turbo model, we distil characteristics upon a question
    & answering (QA) dataset (SQuAD) into a Roberta-Large base model. Our findings
    demonstrate that a large QA dataset can be successfully labelled and leveraged
    to create an extracted model with 73% EM similarity to ChatGPT-3.5-Turbo, and
    achieve SQuAD EM and F1 accuracy scores of 75% and 87%, respectively at $50 cost.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了*模型抽取*攻击方法，并通过使用抽取攻击框架[[9](#bib.bib9)]的实验展示了其对LLMs的有效性。以ChatGPT-3.5-Turbo模型为目标，我们将特征提取到一个问答（QA）数据集（SQuAD）中的Roberta-Large基础模型中。我们的研究结果表明，一个大型QA数据集可以成功标记并利用，创建一个与ChatGPT-3.5-Turbo有73%
    EM相似度的提取模型，并在$50的成本下获得SQuAD EM和F1准确率分别为75%和87%的成绩。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We study the capability to exploit an extracted model derived from *Model Leeching*
    to perform further ML attack staging upon a production LLM. Our results show that
    a language attack [[11](#bib.bib11)] optimized for an extracted model can be successfully
    transferred into ChatGPT-3.5-Turbo with an 11% attack success increase. Our results
    highlight evidence of adversarial attack transferability between user-created
    models and production LLMs.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们研究了利用从*模型抽取*中提取的模型进一步进行机器学习攻击的能力。我们的结果表明，针对提取模型优化的语言攻击[[11](#bib.bib11)]可以成功转移到ChatGPT-3.5-Turbo，并且攻击成功率提高了11%。我们的结果突显了用户创建的模型与生产LLMs之间对抗攻击的可转移性。
- en: 2 Attack Description & Threat Model
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 攻击描述与威胁模型
- en: 2.1 Extraction Attacks
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 抽取攻击
- en: Model extraction is the process of extracting the fundamental characteristics
    of a DL model [[25](#bib.bib25)]. An extracted model is created via extracting
    specific characteristics (architecture, parameters, and hyper-parameters [[10](#bib.bib10)])
    from a target model of interest, which are then used to perform model recreation
    [[16](#bib.bib16)]. Once the attacker has established an extracted model, further
    adversarial attacks can be staged encompassing model inversion, membership inference,
    leaking privacy data, and model intellectual property theft [[4](#bib.bib4)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 模型抽取是提取DL模型基本特征的过程[[25](#bib.bib25)]。通过从感兴趣的目标模型中提取特定特征（架构、参数和超参数[[10](#bib.bib10)]），创建提取模型，然后用于模型重建[[16](#bib.bib16)]。一旦攻击者建立了提取模型，可以进行进一步的对抗攻击，包括模型反演、成员推断、隐私数据泄露和模型知识产权盗窃[[4](#bib.bib4)]。
- en: 2.2 Threat Model
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 威胁模型
- en: State-of-the-art LLMs leveraging the transformer architecture [[26](#bib.bib26)]
    typically comprise hundreds of billions of parameters [[30](#bib.bib30)]. Using
    the established taxonomy of adversaries against DL models [[20](#bib.bib20)],
    our proposed attacks assume a weak adversary capable of providing model input
    via an LLM API endpoint, and a model output requiring generated text from a target
    LLM. The adversary has no knowledge of the target architecture or training data
    used to construct the underlying LLM parameters. Note that the threat model assumptions
    pertaining to potential rate limiting, or limited access to the target API can
    be relaxed due the ability to distribute data generation across multiple API keys.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的LLMs利用变换器架构[[26](#bib.bib26)]通常包括数百亿个参数[[30](#bib.bib30)]。根据对DL模型的对手分类法[[20](#bib.bib20)]，我们提出的攻击假设对手较弱，能够通过LLM
    API端点提供模型输入，并且模型输出需要从目标LLM生成的文本。对手对目标架构或用于构建基础LLM参数的训练数据没有了解。需要注意的是，由于能够通过多个API密钥分配数据生成，关于潜在速率限制或有限访问目标API的威胁模型假设可以被放宽。
- en: 3 Model Leeching Attack Design
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 模型抽取攻击设计
- en: '*Model Leeching* is a black-box adversarial attack which seeks to create an
    extracted copy of the target LLM within a specific task. The attack comprises
    a four-phases approach as shown in Figure [1](#S3.F1 "Figure 1 ‣ 3 Model Leeching
    Attack Design ‣ Model Leeching: An Extraction Attack Targeting LLMs"): (1) Prompt
    design for crafting prompts to attain task-specific LLM responses; (2) data generation
    to derive extracting model characteristics; (3) extracted model training for model
    recreation; and (4) ML attack staging against a target LLM.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型抽取*是一种黑盒对抗攻击，旨在在特定任务中创建目标LLM的提取副本。攻击包括四个阶段，如图[1](#S3.F1 "图 1 ‣ 3 模型抽取攻击设计
    ‣ 模型抽取：针对LLMs的抽取攻击")所示：（1）提示设计，旨在制作获取特定任务LLM响应的提示；（2）数据生成，用于提取模型特征；（3）提取模型训练，用于模型重建；（4）针对目标LLM的机器学习攻击。'
- en: '![Refer to caption](img/8fd69ca8919c57d87aa09c2e5caaab47.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8fd69ca8919c57d87aa09c2e5caaab47.png)'
- en: 'Figure 1: Overview of Model Leech. Deep Learning models comprising of architecture,
    parameters and hyper-parameters can be extracted via extraction attacks.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：模型抽取概述。深度学习模型包括架构、参数和超参数，可以通过抽取攻击进行提取。
- en: 3.1 Prompt Design
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 提示设计
- en: 'Performing *Model Leeching* successfully requires correct prompt design. Adversaries
    must design well-structured prompts that accurately define the relevancy and depth
    of the necessary generated responses in order to identify task-specific knowledge
    of interest. Depending on the use case, prompt design is achieved manually or
    through automated methods [[28](#bib.bib28)]. Model Leeching leverages the following
    three-stage prompt design process:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 成功执行*模型抽取*需要正确的提示设计。对手必须设计结构良好的提示，准确定义生成响应的相关性和深度，以便识别感兴趣的任务特定知识。根据用例，提示设计可以通过手动或自动化方法完成[[28](#bib.bib28)]。模型抽取利用以下三个阶段的提示设计过程：
- en: '1.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Knowledge Discovery. An adversary first defines the type of task knowledge to
    extract. Once defined, an adversary assesses specific target LLM prompt responses
    to ascertain its affinity to generate task knowledge. This assessment encompasses
    domain (NLP, image, audio, etc.), response patterns, comprehension limitations,
    and instruction adherence for particular knowledge domains [[7](#bib.bib7), [15](#bib.bib15),
    [29](#bib.bib29)]. Following successful completion of this assessment, the adversary
    is able to devise an effective strategy to extract desired characteristics.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识发现。对手首先定义要提取的任务知识类型。一旦定义明确，对手评估特定目标LLM提示响应，以确定其生成任务知识的倾向。这种评估包括领域（NLP、图像、音频等）、响应模式、理解局限性和对特定知识领域指令的遵守[[7](#bib.bib7)、[15](#bib.bib15)、[29](#bib.bib29)]。成功完成此评估后，对手能够制定有效的策略来提取所需的特性。
- en: '2.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Construction. Subsequently, the adversary crafts a prompt template that integrates
    an instruction set reflecting the strategy formulated during the knowledge discovery
    stage. Template design encompasses distinctive response structure of the target
    LLM, its recognized limitations, and task-specific knowledge identified for extraction.
    This template facilitates dynamic prompt generation within the Model Leeching
    process.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 构建。随后，对手制作一个提示模板，集成了在知识发现阶段制定的策略的指令集。模板设计包括目标LLM的独特响应结构、其已知的局限性以及为提取而识别的特定任务知识。该模板促进了模型抽取过程中动态提示的生成。
- en: '3.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Validation. The adversary validates the created prompt and response generated
    from the target LLM. Validation entails ensuring the LLM responds reliably to
    prompts, represented as a consistent response structure and ability to carry out
    given instructions. Ensuring that the target LLM is capable enough to carry out
    the required task, that it can process and action upon its given instructions.
    This validation activity enables the Model Leeching method to generate responses
    that can be used to effectively train local models with extracted task-specific
    knowledge.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证。对手验证创建的提示和从目标LLM生成的响应。验证包括确保LLM对提示做出可靠的响应，这表现为一致的响应结构和执行给定指令的能力。确保目标LLM有足够的能力来完成所需任务，能够处理和执行其给定的指令。这种验证活动使得模型抽取方法能够生成可以有效训练本地模型的特定任务知识的响应。
- en: The prompt design process follows an iterative approach, typically requiring
    multiple variations and refinements to devise the most effective instructions
    and styles for obtaining desired results from a specific LLM for a given task
    [[29](#bib.bib29)].
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 提示设计过程遵循迭代方法，通常需要多次变体和调整，以制定最有效的指令和样式，以从特定LLM获得给定任务的期望结果[[29](#bib.bib29)]。
- en: 3.2 Data Generation
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据生成
- en: Once a suitable prompt has been designed, the adversary targets the given LLM
    (), all examples are processed into prompts recognized as valid target LLM inputs.
    Once all queries have been processed by the target LLM, we generate an adversarial
    dataset ($D_{adv}$) combining inputs with received LLM replies, as well as automated
    validation (removing API request errors, failed, or erroneous prompts). This process
    can be distributed and parallelised to minimize collection time as well as mitigate
    the impact of rate-limiting and/or detection by filtering systems when interacting
    with the web-based LLM API [[5](#bib.bib5)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设计出合适的提示，攻击者将目标指向给定的 LLM（），所有示例都被处理成被识别为有效目标 LLM 输入的提示。所有查询被目标 LLM 处理后，我们生成一个对抗数据集
    ($D_{adv}$)，结合输入和收到的 LLM 回复，以及自动验证（去除 API 请求错误、失败或错误的提示）。这个过程可以分布式和并行化，以减少收集时间，并减轻速率限制和/或通过过滤系统的检测对与基于网络的
    LLM API 的交互的影响 [[5](#bib.bib5)]。
- en: 3.3 Extracted Model Training
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 提取模型训练
- en: Using () and evaluation () is selected for distilling knowledge from the target
    LLM. This base model is then trained upon (). Using evaluation set () and ($M_{target}$).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用（）和评估（）选择用于从目标 LLM 中提取知识。这个基础模型然后在（）上进行训练。使用评估集（）和（$M_{target}$）。
- en: 3.4 ML Attack Staging
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 ML 攻击阶段
- en: Access to an extracted model (local to an adversary) created from a target LLM
    facilitates the execution of augmented adversarial attacks. This extracted model
    allows an adversary to perform unrestricted model querying to test, modify or
    tailor adversarial attack(s) to discover exploits and vulnerabilities against
    a target LLM [[11](#bib.bib11)]. Furthermore, access to an extracted model enables
    an adversary to operate in a sandbox environment to conduct adversarial attacks
    prior to executing the same attack(s) against the target LLM in production (and
    of particular concern, whilst minimizing the likelihood of detection by the provider).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从目标 LLM 创建的提取模型（对攻击者本地）可以执行增强的对抗攻击。这种提取的模型允许攻击者进行无限制的模型查询，以测试、修改或调整对抗攻击，以发现针对目标
    LLM 的漏洞和弱点 [[11](#bib.bib11)]。此外，访问提取模型使攻击者能够在沙盒环境中进行对抗攻击，然后在生产环境中对目标 LLM 执行相同的攻击（特别需要关注的是，尽量减少被提供者检测的可能性）。
- en: 4 Experimental Setup
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置
- en: '![Refer to caption](img/253251da6a6c57ea1d9a00f018b996e9.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/253251da6a6c57ea1d9a00f018b996e9.png)'
- en: 'Figure 2: Example of Prompt Template. Slots for SQuAD context and questions,
    with a set of instructions for the LLM to follow.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：提示模板示例。SQuAD 上下文和问题的插槽，以及 LLM 跟随的指令集。
- en: To demonstrate the effectiveness of *Model Leeching*, we created a set of extracted
    models using ChatGPT-3.5-Turbo as the target model, with Question & Answers as
    the target task. Task-specific prompts were designed and generated using the Stanford
    Question Answering 1.1 Dataset (SQuAD) containing 100k examples (85k to 15k evaluation
    split), representing a context and set of questions and associated answers [[21](#bib.bib21)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示*模型抽取*的有效性，我们创建了一组提取的模型，使用 ChatGPT-3.5-Turbo 作为目标模型，任务为问题与回答。任务特定的提示是使用包含
    100k 示例（85k 到 15k 评估分割）的斯坦福问答 1.1 数据集（SQuAD）设计和生成的，代表一个上下文和一组问题及相关答案 [[21](#bib.bib21)]。
- en: 4.1 Prompt Construction
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 提示构建
- en: 'A comprehensive array of prompts, encompassing the entirety of the SQuAD dataset
    was produced. These prompts adhere to a template containing the specific SQuAD
    question and context, enabling ChatGPT-3.5-Turbo to efficiently process and respond
    to the given task. As seen in Figure [2](#S4.F2 "Figure 2 ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), each rule instructs the
    target LLM to produce an output desired by the adversary ensuring effective capture
    of task-specific knowledge. The template comprises:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 生成了一个全面的提示数组，涵盖了 SQuAD 数据集的全部内容。这些提示遵循一个包含特定 SQuAD 问题和上下文的模板，使 ChatGPT-3.5-Turbo
    能够高效处理并回应给定的任务。如图 [2](#S4.F2 "图 2 ‣ 4 实验设置 ‣ 模型抽取：针对 LLM 的提取攻击") 所示，每个规则指示目标 LLM
    生成攻击者期望的输出，从而有效捕捉任务特定知识。模板包括：
- en: '1.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Target LLM is specifically directed to provide only the precise answer to the
    assigned SQuAD question, drawn solely from the provided SQuAD context. This stipulation
    is crucial due to the inherent tendency of general chat-style LLMs (such as ChatGPT-3.5-Turbo)
    to produce more verbose responses than necessary. In the scope of SQuAD score
    assessment, only the exact answer is pertinent, negating the need for any additional
    content.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标 LLM 被特别要求仅提供从提供的 SQuAD 上下文中得出的精确答案。这一规定很重要，因为通用聊天风格的 LLM（如 ChatGPT-3.5-Turbo）有生成比必要的更冗长的回答的固有倾向。在
    SQuAD 分数评估的范围内，只有确切的答案是相关的，不需要任何额外内容。
- en: '2.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: By including the sentence where the answer occurred, the LLM is required to
    demonstrate a degree of contextual comprehension beyond simple fact extraction,
    for valid data generation that contains the correct task knowledge. This requirement
    ensures that the model is not limited to identifying keywords, but understands
    the broader text semantic structure. In the case of assessing model performance
    on ChatGPT-3.5-Turbo, the index in which an answer is found within the context
    is required.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过包含出现答案的句子，LLM 需要展示超越简单事实提取的上下文理解能力，以生成包含正确任务知识的有效数据。这一要求确保了模型不仅仅是识别关键词，而是理解更广泛的文本语义结构。在评估
    ChatGPT-3.5-Turbo 模型性能时，需要考虑答案在上下文中的索引。
- en: '3.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Use of a standardized JSON format for responses facilitates efficient and uniform
    data handling. The keys answer and sentence provide a clear and concise structure,
    making the model output easier to process and compare algorithmically and manually.
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用标准化的 JSON 格式进行响应有助于高效且一致的数据处理。键 answer 和 sentence 提供了一个清晰而简洁的结构，使模型输出更易于进行算法和人工处理比较。
- en: '4.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Ability to respond with ’UNSURE’ provides a safeguard for quality control of
    model response. By acknowledging its own uncertainty, the LLM avoids disseminating
    potentially incorrect or misleading information, and assists in parsing prompts
    that it was unable to complete.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 能够以‘UNSURE’响应为模型响应的质量控制提供了保护。通过承认自身的不确定性，LLM 避免传播可能不正确或误导性的信息，并协助解析它无法完成的提示。
- en: 4.2 Model Base Architectures
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 模型基础架构
- en: 'To evaluate the effectiveness of Model Leeching, we selected three different
    base model architectures and several variants (with models parameter sizes ranging
    from between 14 to 123 million) to create an extracted model of our target LLM.
    These six model architectures include Bert [[6](#bib.bib6)], Albert [[13](#bib.bib13)],
    and Roberta [[14](#bib.bib14)], were selected due to their parameter size and
    respective performance upon our selected task [[14](#bib.bib14)]. The intention
    of selecting these architectures as candidate extracted models is to to evaluate
    wether: 1) more sophisticated models (parameters, architecture) are more effective
    at learning target LLM characteristics; and 2) low parameter models (i.e. 100x
    smaller vs. ChatGPT-3.5-Turbo) can learn sufficient characteristics from a target
    LLM, while achieving comparable performance in a specific task. Using these candidate
    model architectures, we train two sets of models for the purposes of evaluation,
    1) extracted models; trained upon generated $Adv_{train}$ dataset, and 2) baseline
    models; for performance comparison, trained directly upon the ground-truth SQuAD
    dataset.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估模型抽取的有效性，我们选择了三种不同的基础模型架构和几个变体（模型参数大小从 1400 万到 1.23 亿不等）来创建我们的目标 LLM 的抽取模型。这六种模型架构包括
    Bert [[6](#bib.bib6)]、Albert [[13](#bib.bib13)] 和 Roberta [[14](#bib.bib14)]，选择它们是因为它们的参数大小和在我们选择的任务中的表现
    [[14](#bib.bib14)]。选择这些架构作为候选抽取模型的意图是评估：1）更复杂的模型（参数、架构）是否在学习目标 LLM 特性方面更有效；2）低参数模型（即比
    ChatGPT-3.5-Turbo 小 100 倍）是否能从目标 LLM 中学习到足够的特性，同时在特定任务中达到可比的性能。使用这些候选模型架构，我们训练了两个模型集以进行评估：1）抽取模型；基于生成的
    $Adv_{train}$ 数据集进行训练，2）基准模型；用于性能比较，直接基于真实的 SQuAD 数据集进行训练。
- en: '![Refer to caption](img/7c9302bd994a66567908912803223e86.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7c9302bd994a66567908912803223e86.png)'
- en: 'Figure 3: Example of AddSent Attack. Adversarial sentences appended to SQuAD
    context (blue highlighted text) to yield incorrect answers for SQuAD questions.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：AddSent 攻击示例。附加到 SQuAD 上下文中的对抗句子（蓝色高亮文本），用于对 SQuAD 问题生成不正确的答案。
- en: 4.3 ML Attack Staging
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 机器学习攻击分阶段
- en: 'We created and deployed an adversarial attack derived from AddSent [[11](#bib.bib11)]
    that generates an adversarial context by adding a non-factual yet semantically
    and syntactically correct sentences to the original context from a SQuAD entry
    (Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Model Base Architectures ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs")). The goal of this attack
    is to cause a QA model to incorrectly answer a question when given an adversarial
    context. We further modified this attack to generate a larger variety of adversarial
    context, selectively chosen based on their success upon our extracted model, which
    is then sent to the target LLM for improved misclassification likelihood.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建并部署了从 AddSent [[11](#bib.bib11)] 派生的对抗攻击，该攻击通过向 SQuAD 条目的原始上下文中添加一个非事实但语义和句法上正确的句子来生成对抗上下文（图
    [3](#S4.F3 "图 3 ‣ 4.2 模型基础架构 ‣ 4 实验设置 ‣ 模型提取：针对 LLM 的提取攻击")）。此攻击的目标是使 QA 模型在给定对抗上下文时错误回答问题。我们进一步修改了此攻击以生成更广泛的对抗上下文，基于它们在我们提取模型上的成功情况进行选择，然后将其发送到目标
    LLM，以提高误分类的可能性。
- en: 4.4 Model Leeching Scenario
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 模型提取场景
- en: 'We demonstrate the effectiveness of *Model Leeching* by targeting ChatGPT-3.5-Turbo
    with a pre-trained Roberta-Large base architecture [[14](#bib.bib14)]. Using SQuAD
    as described in [4.1](#S4.SS1 "4.1 Prompt Construction ‣ 4 Experimental Setup
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), we generate a new labelled
    adversarial dataset through automated prompt generation querying ChatGPT-3.5-Turbo,
    which is trained upon the base architecture to create an extracted model. We evaluate
    attack performance by measuring the extracted model performance to a baseline
    model directly trained on SQuAD with ground truth answers. We demonstrate the
    feasibility of attack transferability across models by applying the AddSent attack
    [[11](#bib.bib11)] upon the extracted model, generating adversarial perturbations
    that can be further staged upon the target LLM. In order to explore feasibility
    of transferability of adversarial vulnerabilities across models. We leverage three
    metrics for evaluation: Exact Match (EM), and F1 Score used to measure the performance/similarity
    of our extracted model and ChatGPT-3.5-Turbo [[21](#bib.bib21)], and attack success
    rate for further attack staging representing successful adversarial prompts.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过针对 ChatGPT-3.5-Turbo 使用预训练的 Roberta-Large 基础架构 [[14](#bib.bib14)] 演示了 *模型提取*
    的有效性。使用 [4.1](#S4.SS1 "4.1 提示构建 ‣ 4 实验设置 ‣ 模型提取：针对 LLM 的提取攻击") 中描述的 SQuAD，我们通过自动化提示生成查询
    ChatGPT-3.5-Turbo 生成了一个新的标记对抗数据集，该模型基于基础架构进行训练以创建一个提取模型。我们通过测量提取模型与直接在 SQuAD 上训练的基线模型的性能来评估攻击效果，基线模型具有真实答案。我们通过对提取模型应用
    AddSent 攻击 [[11](#bib.bib11)] 展示了攻击可转移性的可行性，生成了可以进一步在目标 LLM 上进行分阶段的对抗扰动。为了探讨对抗漏洞在模型间转移的可行性，我们利用了三种评估指标：准确匹配（EM）、F1
    分数用于衡量我们的提取模型和 ChatGPT-3.5-Turbo [[21](#bib.bib21)] 的性能/相似性，以及进一步攻击分阶段的成功率，代表成功的对抗提示。
- en: 5 Results
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 5.1 Data Generation
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据生成
- en: From 100k examples of contexts, questions and answers within SQuAD, 83,335 total
    usable examples were collected, with 16,665 failing either from API request errors,
    or erroneous replies, attributing to a 16.66% error rate when labelling through
    ChatGPT-3.5-Turbo. From these 83,335 examples, 76,130 can be used for further
    extracted model training (). Query time was 48 hours and cost $50 to execute API
    requests.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SQuAD 中的 100k 个上下文、问题和答案示例中，收集了总计 83,335 个可用示例，其中 16,665 个因 API 请求错误或错误回复而失败，标记错误率为
    16.66%。从这 83,335 个示例中，76,130 个可以用于进一步的提取模型训练。查询时间为 48 小时，执行 API 请求的费用为 50 美元。
- en: '![Refer to caption](img/b9f1ed132cb731376f5bcf378c8a4c12.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b9f1ed132cb731376f5bcf378c8a4c12.png)'
- en: 'Figure 4: Model Similarity to ChatGPT-3.5-Turbo. Comparing similarity in correct
    and incorrect answering of questions relative to ChatGPT-3.5-Turbo.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：与 ChatGPT-3.5-Turbo 的模型相似性。比较相对于 ChatGPT-3.5-Turbo 的正确和错误回答问题的相似性。
- en: '![Refer to caption](img/b33a26d6d9fa62a2c0e53a0c345eecbf.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b33a26d6d9fa62a2c0e53a0c345eecbf.png)'
- en: 'Figure 5: Baseline and Extracted SQuAD Accuracy. Comparing the baseline and
    extracted models’ performance on the original SQuAD dataset questions and answers.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基线和提取 SQuAD 准确性。比较基线模型和提取模型在原始 SQuAD 数据集问题和答案上的表现。
- en: 5.2 Extraction Similarity
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 提取相似性
- en: 'Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Data Generation ‣ 5 Results ‣ Model Leeching:
    An Extraction Attack Targeting LLMs") shows that each extracted model performed
    more similarly to ChatGPT-3.5-Turbo compared to their baseline counterpart, with
    each model EM and F1 similarity score being up to 10.49% and 5% higher, respectively.
    Roberta Large achieved the highest ChatGPT-3.5-Turbo similarity, with a 0.73 EM
    and 0.87 F1 score denoting high similarity to the target LLM [[17](#bib.bib17)].
    Similarity of the baseline models to ChatGPT-3.5-Turbo is lower than the extracted
    model, due to being trained using the original SQuAD dataset, whereas the extracted
    models used a dataset derived from ChatGPT-3.5-Turbo.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S5.F4 "图 4 ‣ 5.1 数据生成 ‣ 5 结果 ‣ 模型抽取：针对 LLM 的提取攻击") 显示，提取的每个模型与 ChatGPT-3.5-Turbo
    的表现更为相似，与其基准模型相比，每个模型的 EM 和 F1 相似度得分分别提高了最高 10.49% 和 5%。Roberta Large 实现了与 ChatGPT-3.5-Turbo
    的最高相似度，其 EM 为 0.73，F1 为 0.87，表明与目标 LLM 高度相似 [[17](#bib.bib17)]。基准模型与 ChatGPT-3.5-Turbo
    的相似度低于提取模型，因为基准模型是使用原始 SQuAD 数据集训练的，而提取模型使用了来自 ChatGPT-3.5-Turbo 的数据集。
- en: 5.3 Task Performance
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 任务性能
- en: 'Extracted model task performance was evaluated by comparing the SQuAD EM and
    F1 scores to baseline models and ChatGPT-3.5-Turbo. Figure [5](#S5.F5 "Figure
    5 ‣ 5.1 Data Generation ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs") shows that extracted models exhibit similar performance for SQuAD when
    compared with their respective baselines, with EM and F1 scores. Evaluating our
    extracted models against ChatGPT-3.5-Turbo, we observed that Roberta Large achieved
    the highest similarity to ChatGPT-3.5-Turbo performance exhibiting EM and F1 scores,
    achieving an EM/F1 score of 0.75/0.87 compared to 0.74/0.87 respectively. Extracted
    model performance from ChatGPT-3.5-Turbo is sufficiently comparable in performance
    to state-of-the-art literature on QA tasks, where with the hyperparameters used
    in Roberta Large are more performant than the other architectures [[14](#bib.bib14)].'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 提取模型的任务表现通过将 SQuAD 的 EM 和 F1 分数与基准模型和 ChatGPT-3.5-Turbo 进行比较来评估。图 [5](#S5.F5
    "图 5 ‣ 5.1 数据生成 ‣ 5 结果 ‣ 模型抽取：针对 LLM 的提取攻击") 显示，与各自基准模型相比，提取模型在 SQuAD 上表现出相似的性能，EM
    和 F1 分数也相似。与 ChatGPT-3.5-Turbo 进行比较时，我们观察到 Roberta Large 实现了与 ChatGPT-3.5-Turbo
    表现的最高相似度，EM/F1 分数为 0.75/0.87，相比之下，基准模型的 EM/F1 分数为 0.74/0.87。提取模型的性能与 ChatGPT-3.5-Turbo
    在 QA 任务上的表现非常相当，其中 Roberta Large 使用的超参数比其他架构更具性能 [[14](#bib.bib14)]。
- en: '![Refer to caption](img/d1c0e0a424c6324fe17f667a89bbf25f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d1c0e0a424c6324fe17f667a89bbf25f.png)'
- en: 'Figure 6: ML Attack Staging Results. Comparing the original attack’s adversarial
    effectiveness against those developed with the model extracted from ChatGPT-3.5-Turbo.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：ML 攻击分阶段结果。比较原始攻击与使用从 ChatGPT-3.5-Turbo 提取的模型开发的对抗效果。
- en: 5.4 ML Attack Staging
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 ML 攻击分阶段
- en: 'Roberta Large was used to evaluate the attack success of AddSent upon the extracted
    model and ChatGPT-3.5-Turbo given its high SQuAD accuracy and similarity. AddSent
    exhibited an attack success of 0.28 and 0.26 upon the extracted model and ChatGPT-3.5-Turbo,
    respectively. Leveraging access to our extracted model, we selected and sent the
    best performing 7,205 adversarial examples to ChatGPT-3.5-Turbo. Our results indicate
    that adversarial examples augmented by AddSent increased attack success by 26%
    for the extracted model, and 11% to ChatGPT-3.5-Turbo (Figure [6](#S5.F6 "Figure
    6 ‣ 5.3 Task Performance ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs")). Attack effectiveness is reduced across models due to ChatGPT-3.5-Turbo
    being 100x larger in parameter size than local models, and leveraging advanced
    training methods such as reinforcement learning from human feedback, not used
    on our local models. While ChatGPT-3.5-Turbo is more task capable and less likely
    to be evaded by adversarial prompts compared to a local model. However, despite
    increased adversarial robustness, our results highlight attack transferability
    exists between an extracted model and its target, demonstrating the feasibility
    of leveraging distilled knowledge to further stage and subsequently launch improved
    adversarial attacks upon a production LLM.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '使用了**Roberta Large**来评估AddSent对提取模型和ChatGPT-3.5-Turbo的攻击成功率，考虑到其高SQuAD准确率和相似性。AddSent对提取模型和ChatGPT-3.5-Turbo的攻击成功率分别为0.28和0.26。利用我们提取的模型，我们选择并发送了表现最佳的7,205个对抗样本到ChatGPT-3.5-Turbo。我们的结果表明，AddSent增强的对抗样本使提取模型的攻击成功率提高了26%，对ChatGPT-3.5-Turbo提高了11%（见图[6](#S5.F6
    "Figure 6 ‣ 5.3 Task Performance ‣ 5 Results ‣ Model Leeching: An Extraction Attack
    Targeting LLMs")）。由于ChatGPT-3.5-Turbo的参数规模是本地模型的100倍，并且使用了我们本地模型未使用的先进训练方法，如基于人类反馈的强化学习，因此模型之间的攻击效果有所降低。尽管ChatGPT-3.5-Turbo在任务能力上更强，不容易被对抗提示规避，但我们的结果仍然突显了提取模型与其目标之间存在攻击可转移性，展示了利用提炼知识进一步进行和随后发起改进的对抗攻击在生产LLM上的可行性。'
- en: 6 Discussion
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: 6.1 Dataset Labelling
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 数据集标注
- en: 'Using the SQuAD dataset containing 100k examples, we successfully labelled
    83,335 using ChatGPT-3.5-Turbo (see Section [5.1](#S5.SS1 "5.1 Data Generation
    ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting LLMs")). In total,
    this process cost $50 and required 48 hours to complete. Compared to using labelling
    services such as Amazon SageMaker Data Labeling [[2](#bib.bib2)], the estimated
    cost of labelling would be $0.036 per example of data, totalling $3,600, demonstrating
    a significant reduction in cost when using generative LLMs to label datasets.
    We additionally note that the success of labelling datasets can be increased by
    1) further prompt engineering and optimization to package multiple SQuAD examples
    into one efficient query enabling reduction in query cost and time; and 2) re-sending
    of failed SQuAD examples to achieve higher amount of successful labelled examples.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '使用包含100k个示例的SQuAD数据集，我们成功地使用ChatGPT-3.5-Turbo标注了83,335个示例（见第[5.1节](#S5.SS1
    "5.1 Data Generation ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs")）。总的来说，此过程花费了50美元，完成时间为48小时。与使用Amazon SageMaker Data Labeling等标注服务相比[[2](#bib.bib2)]，估算标注成本为每个数据示例0.036美元，总计3,600美元，这表明使用生成LLM标注数据集可以显著降低成本。我们还指出，通过1)
    进一步的提示工程和优化，将多个SQuAD示例打包成一个高效查询，从而降低查询成本和时间；以及2) 重新发送失败的SQuAD示例以实现更高数量的成功标注示例，可以提高标注数据集的成功率。'
- en: 6.2 Extraction Similarity
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 提取相似性
- en: 'Extracted models derived from *Model Leeching* demonstrate the ability to effectively
    learn the characteristics of the target model. Highlighted within Section [5.2](#S5.SS2
    "5.2 Extraction Similarity ‣ 5 Results ‣ Model Leeching: An Extraction Attack
    Targeting LLMs"), noticeable deviations between our extracted models, and baseline
    equivalents, against their EM/F1 similarity to the target, demonstrate extracted
    models contain similarly learned knowledge to the target compared to baseline
    models. The extracted model responses closely align with those of ChatGPT-3.5-Turbo’s,
    exhibiting similar success and error rates in how they semantically and syntactically
    answer questions. This finding underscoring the capacity of our model to replicate
    the behaviour of the target, especially in the given task.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '从*模型吸血术*提取的模型展示了有效学习目标模型特征的能力。在[5.2](#S5.SS2 "5.2 Extraction Similarity ‣ 5
    Results ‣ Model Leeching: An Extraction Attack Targeting LLMs")节中突出显示，我们提取的模型与基线模型在目标的EM/F1相似性上有显著偏差，表明提取的模型包含与目标相似的知识，与基线模型相比。提取模型的回应与ChatGPT-3.5-Turbo的回应紧密对齐，在语义和句法上回答问题时表现出相似的成功和错误率。这一发现强调了我们模型复制目标行为的能力，特别是在给定任务中。'
- en: 6.3 Distilled Knowledge Capability
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 精炼知识能力
- en: 'Our findings showcase the possibility of not only extracting knowledge from
    a LLM, but also transferring this knowledge effectively to a model with significantly
    fewer parameters. ChatGPT-3.5-Turbo comprises 175 billion parameters, whilst our
    local models are 100x smaller (See Section [5.3](#S5.SS3 "5.3 Task Performance
    ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting LLMs")). These smaller
    local models when trained with the extracted dataset demonstrated the ability
    to perform the given task effectively. Comparing our extracted model performance
    upon SQuAD to ChatGPT-3.5-Turbo we observed at worst a 13.2%/12.04% EM/F1 score
    difference and our best-performing extracted model, Roberta Large, achieving identical
    SQuAD scores to ChatGPT-3.5-Turbo.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的发现展示了不仅可以从LLM中提取知识，还可以有效地将这些知识转移到参数显著更少的模型中。ChatGPT-3.5-Turbo包含1750亿个参数，而我们的本地模型则小100倍（见[5.3](#S5.SS3
    "5.3 Task Performance ‣ 5 Results ‣ Model Leeching: An Extraction Attack Targeting
    LLMs")节）。这些较小的本地模型在用提取的数据集进行训练时展示了有效执行给定任务的能力。将我们提取模型在SQuAD上的表现与ChatGPT-3.5-Turbo进行比较，我们观察到最差情况下EM/F1得分差异为13.2%/12.04%，而我们表现最好的提取模型Roberta
    Large在SQuAD得分上与ChatGPT-3.5-Turbo相同。'
- en: 6.4 ML Attack Staging
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 ML 攻击阶段
- en: 'Demonstrated within Section [5.4](#S5.SS4 "5.4 ML Attack Staging ‣ 5 Results
    ‣ Model Leeching: An Extraction Attack Targeting LLMs"), it is feasible to utilize
    an extracted model within an adversaries’ local environment to conduct further
    adversarial attack staging. By having unfettered query access to this extracted
    model, it facilitates the enhancement of attack success. The potency of the AddSent
    attack on the model extracted by Model Leeching was increased by 26%, which consequently
    led to an 11% increase when launched against ChatGPT-3.5-Turbo. This highlights
    the vulnerability of a target LLM to subsequent machine learning attacks once
    adversaries acquire an extracted model. By having access to this ’sandbox’ model,
    adversaries can refine or innovate their attack strategies. Consequently, LLMs
    deployed and served over publicly accessible APIs are at significant risk to further
    attack staging.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '在[5.4](#S5.SS4 "5.4 ML Attack Staging ‣ 5 Results ‣ Model Leeching: An Extraction
    Attack Targeting LLMs")节中展示，利用提取的模型在对手的本地环境中进行进一步的对抗攻击阶段是可行的。通过对提取的模型拥有无限制的查询访问权限，可以提高攻击的成功率。AddSent攻击在通过模型吸血术提取的模型上的威力提高了26%，这导致在针对ChatGPT-3.5-Turbo时增加了11%。这突显了目标LLM在对手获得提取模型后对后续机器学习攻击的脆弱性。通过访问这个“沙盒”模型，对手可以改进或创新他们的攻击策略。因此，通过公开API部署和服务的LLMs面临着显著的进一步攻击阶段风险。'
- en: 7 Further Work
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 进一步工作
- en: 7.1 Analysis of Additional Production LLMs
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 额外生产LLMs的分析
- en: Further work includes conducting *Model Leeching* against a larger array of
    LLM(s) such as BARD, LLaMA and available variations of GPT models from OpenAI.
    Taking these models and exploring how they respond to *Model Leeching* and their
    vulnerability to follow-up attacks. Such a study would demonstrate the possibility
    to generate ensemble models that inherit characteristics from multiple target
    LLMs. Enabling the optimization of a local model by task-specific performance
    from the best-performing target would aim to maximise the local model capability.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的工作包括对更多的 LLM（如 BARD、LLaMA 和来自 OpenAI 的 GPT 模型的各种变体）进行*模型抽取*。对这些模型进行研究，探索它们如何应对*模型抽取*以及对后续攻击的脆弱性。这样的研究将展示生成继承多个目标
    LLM 特征的集成模型的可能性。通过从最佳表现的目标中获取任务特定性能来优化本地模型，将旨在最大化本地模型的能力。
- en: 7.2 Extraction By Proxy
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 代理提取
- en: Multiple open-source versions of popular LLMs have been produced by the ML community.
    This includes examples such as GPT4All [[19](#bib.bib19)] and Llama [[24](#bib.bib24)]
    that can be deployed on consumer-grade devices. These models typically leverage
    training sets, architectures and prompts used to develop the LLM they are aiming
    to extract and replicate. If these models share significant characteristics with
    the original LLM, it may be feasible for an adversary to conduct *Model Leeching*
    and then deploy an improved attack against a target LLM it didn’t interact with
    before attack deployment.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ML 社区已经生产了多个流行 LLM 的开源版本。包括像 GPT4All [[19](#bib.bib19)] 和 Llama [[24](#bib.bib24)]
    这样的示例，这些模型可以在消费级设备上部署。这些模型通常利用用于开发其目标 LLM 的训练集、架构和提示。如果这些模型与原始 LLM 具有显著的相似性，攻击者可能会进行*模型抽取*，然后对一个之前没有交互过的目标
    LLM 发起改进的攻击。
- en: 7.3 LLM Defenses
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 LLM 防御
- en: There has been limited work to defend against attacks on LLMs. Previous research
    into defending against model extraction attacks for smaller NLP models has been
    explored, utilizing techniques such as Membership Classification [[22](#bib.bib22)],
    and Model Watermarking [[23](#bib.bib23)]. However given the rapid development
    of new state-of-the-art adversarial attacks against LLMs, it is important that
    the effectiveness of currently proposed defense techniques within literature are
    evaluated with newer LLMs. Exploring if the characteristics from applied defense
    techniques are captured within extracted knowledge from the target model, and
    further detectable within a distilled extracted model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 LLM 的攻击防御研究有限。之前针对较小 NLP 模型的模型抽取攻击防御研究已经探索了诸如成员分类 [[22](#bib.bib22)] 和模型水印
    [[23](#bib.bib23)] 等技术。然而，鉴于新型最先进对抗攻击的快速发展，重要的是要评估目前文献中提出的防御技术在新型 LLM 上的有效性。探索应用的防御技术的特征是否被提取的知识所捕捉，并在提炼的提取模型中进一步检测到。
- en: 8 Conclusion
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: In this paper we have proposed a new state-of-the-art extraction attack *Model
    Leeching* as a cost-effective means to generate an extracted model with shared
    characteristics to a target LLM. Furthermore, we demonstrated that it is feasible
    to conduct adversarial attack staging against a production LLM via interrogating
    an extracted model derived from a target LLM within a sandbox environment. Our
    findings suggest that extracted models can be derived with a high similarity and
    task accuracy with low query costs, and constitute the basis of attack transferability
    to execute further successful adversarial attacks utilizing data leaked from the
    target LLM.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新的最先进的提取攻击*模型抽取*，作为一种成本效益高的手段来生成与目标 LLM 共享特征的提取模型。此外，我们展示了通过在沙盒环境中审问从目标
    LLM 得到的提取模型，对生产 LLM 进行对抗攻击分阶段是可行的。我们的发现表明，提取的模型可以以低查询成本具有高度相似性和任务准确性，并构成攻击转移的基础，以利用从目标
    LLM 泄露的数据执行进一步的成功对抗攻击。
- en: References
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] AI, G. About Bard. Google AI: Publications, 2023. Accessed: 8th February
    2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] AI, G. 关于 Bard。Google AI: 公开出版物, 2023年。访问日期: 2023年2月8日。'
- en: '[2] AWS. Sagemaker data labeling pricing. [https://aws.amazon.com/sagemaker/data-labeling/pricing/](https://aws.amazon.com/sagemaker/data-labeling/pricing/),
    2023. Accessed: 20230-06-30.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] AWS. Sagemaker 数据标注定价。 [https://aws.amazon.com/sagemaker/data-labeling/pricing/](https://aws.amazon.com/sagemaker/data-labeling/pricing/)，2023年。访问日期:
    2023年6月30日。'
- en: '[3] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,
    Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel,
    C. Extracting training data from large language models, 2021.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A.,
    Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., 和 Raffel,
    C. 从大型语言模型中提取训练数据，2021。'
- en: '[4] Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., and Mukhopadhyay,
    D. Adversarial attacks and defences: A survey, 2018.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chakraborty, A., Alam, M., Dey, V., Chattopadhyay, A., 和 Mukhopadhyay,
    D. 对抗攻击与防御：综述，2018。'
- en: '[5] Crothers, E., Japkowicz, N., and Viktor, H. Machine generated text: A comprehensive
    survey of threat models and detection methods, 2023.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Crothers, E., Japkowicz, N., 和 Viktor, H. 机器生成文本：威胁模型和检测方法的全面综述，2023。'
- en: '[6] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training
    of deep bidirectional transformers for language understanding, 2019.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Devlin, J., Chang, M.-W., Lee, K., 和 Toutanova, K. BERT: 深度双向变换器的语言理解预训练，2019。'
- en: '[7] Efrat, A., and Levy, O. The turking test: Can language models understand
    instructions?, 2020.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Efrat, A., 和 Levy, O. Turking 测试：语言模型能理解指令吗？，2020。'
- en: '[8] Floridi, L. Ai as agency without intelligence: on chatgpt, large language
    models, and other generative models. Philosophy & Technology 36, 1 (Mar 2023),
    15.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Floridi, L. AI 作为没有智能的代理：关于 ChatGPT、大型语言模型和其他生成模型。哲学与技术 36, 1（2023年3月），15。'
- en: '[9] Hackett, W., Trawicki, S., Yu, Z., Suri, N., and Garraghan, P. Pinch: An
    adversarial extraction attack framework for deep learning models, 2023.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Hackett, W., Trawicki, S., Yu, Z., Suri, N., 和 Garraghan, P. Pinch: 一个针对深度学习模型的对抗性提取攻击框架，2023。'
- en: '[10] Hu, X., Liang, L., Li, S., Deng, L., Zuo, P., Ji, Y., Xie, X., Ding, Y.,
    Liu, C., Sherwood, T., and Xie, Y. Deepsniffer: A dnn model extraction framework
    based on learning architectural hints. In Proceedings of the Twenty-Fifth International
    Conference on Architectural Support for Programming Languages and Operating Systems
    (New York, NY, USA, 2020), ASPLOS ’20, Association for Computing Machinery, p. 385–399.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Hu, X., Liang, L., Li, S., Deng, L., Zuo, P., Ji, Y., Xie, X., Ding, Y.,
    Liu, C., Sherwood, T., 和 Xie, Y. Deepsniffer: 基于学习架构提示的 DNN 模型提取框架。在第二十五届国际编程语言和操作系统体系结构支持会议论文集（美国纽约，2020），ASPLos
    ’20，计算机协会，页 385–399。'
- en: '[11] Jia, R., and Liang, P. Adversarial examples for evaluating reading comprehension
    systems, 2017.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jia, R., 和 Liang, P. 用于评估阅读理解系统的对抗样本，2017。'
- en: '[12] Krishna, K., Tomar, G. S., Parikh, A. P., Papernot, N., and Iyyer, M.
    Thieves on sesame street! model extraction of bert-based apis, 2020.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Krishna, K., Tomar, G. S., Parikh, A. P., Papernot, N., 和 Iyyer, M. 《芝麻街上的小偷！基于
    BERT 的 API 模型提取》，2020。'
- en: '[13] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.
    Albert: A lite bert for self-supervised learning of language representations,
    2020.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., 和 Soricut, R.
    Albert: 一种轻量级 BERT 用于自监督语言表示学习，2020。'
- en: '[14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V. Roberta: A robustly optimized bert pretraining
    approach, 2019.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., 和 Stoyanov, V. Roberta: 一种强健优化的 BERT 预训练方法，2019。'
- en: '[15] Mishra, S., Khashabi, D., Baral, C., Choi, Y., and Hajishirzi, H. Reframing
    instructional prompts to GPTk’s language. In Findings of the Association for Computational
    Linguistics: ACL 2022 (Dublin, Ireland, May 2022), Association for Computational
    Linguistics, pp. 589–612.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Mishra, S., Khashabi, D., Baral, C., Choi, Y., 和 Hajishirzi, H. 重新构建 GPTk
    语言的指令提示。在计算语言学协会发现：ACL 2022（爱尔兰都柏林，2022年5月），计算语言学协会，页 589–612。'
- en: '[16] MITRE. MITRE ATLAS Adversarial Attack Knowledge Base, 2023. [Online; accessed
    02-May-2023].'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] MITRE. MITRE ATLAS 对抗攻击知识库，2023年。[在线；访问时间：2023年5月2日]。'
- en: '[17] Oliynyk, D., Mayer, R., and Rauber, A. I know what you trained last summer:
    A survey on stealing machine learning models and defences. ACM Comput. Surv. 55,
    14s (jul 2023).'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Oliynyk, D., Mayer, R., 和 Rauber, A. 我知道你去年夏天训练了什么：关于窃取机器学习模型和防御的综述。ACM
    计算机调查 55, 14s（2023年7月）。'
- en: '[18] OpenAI. ChatGPT. OpenAI Blog, 2023. Accessed: 2023-02-08.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] OpenAI. ChatGPT。OpenAI 博客，2023年。访问时间：2023年2月8日。'
- en: '[19] OpenAI. gpt4all.io, 2023. Accessed: 8th February 2023.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] OpenAI. gpt4all.io，2023年。访问时间：2023年2月8日。'
- en: '[20] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., and
    Swami, A. The limitations of deep learning in adversarial settings. pp. 372–387.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Papernot, N., McDaniel, P., Jha, S., Fredrikson, M., Celik, Z. B., 和 Swami,
    A. 对抗环境中深度学习的局限性。页 372–387。'
- en: '[21] Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad: 100,000+ questions
    for machine comprehension of text, 2016.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Rajpurkar, P., Zhang, J., Lopyrev, K., 和 Liang, P. Squad: 用于机器理解文本的100,000+个问题，2016年。'
- en: '[22] Shokri, R., Stronati, M., Song, C., and Shmatikov, V. Membership inference
    attacks against machine learning models, 2017.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Shokri, R., Stronati, M., Song, C., 和 Shmatikov, V. 对机器学习模型的成员推断攻击，2017年。'
- en: '[23] Szyller, S., Atli, B. G., Marchal, S., and Asokan, N. Dawn: Dynamic adversarial
    watermarking of neural networks, 2021.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Szyller, S., Atli, B. G., Marchal, S., 和 Asokan, N. Dawn: 神经网络的动态对抗水印，2021年。'
- en: '[24] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., and Lample, G. Llama: Open and efficient foundation language models,
    2023.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., 和 Lample, G. Llama: 开放而高效的基础语言模型，2023年。'
- en: '[25] Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., and Ristenpart, T. Stealing
    machine learning models via prediction APIs. In 25th USENIX Security Symposium
    (USENIX Security 16) (Austin, TX, Aug. 2016), USENIX Association, pp. 601–618.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Tramèr, F., Zhang, F., Juels, A., Reiter, M. K., 和 Ristenpart, T. 通过预测
    API 偷取机器学习模型。载于第25届 USENIX 安全研讨会（USENIX Security 16）（奥斯汀，TX，2016年8月），USENIX 协会，第601–618页。'
- en: '[26] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, L., and Polosukhin, I. Attention is all you need, 2017.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, L., 和 Polosukhin, I. 注意力即是你所需，2017年。'
- en: '[27] Wang, X., Li, J., Kuang, X., an Tan, Y., and Li, J. The security of machine
    learning in an adversarial setting: A survey. Journal of Parallel and Distributed
    Computing 130 (2019), 12–23.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Wang, X., Li, J., Kuang, X., 和 Tan, Y., 以及 Li, J. 在对抗环境中的机器学习安全性：一项调查。《并行与分布式计算杂志》130（2019年），第12–23页。'
- en: '[28] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D.,
    and Hajishirzi, H. Self-instruct: Aligning language model with self generated
    instructions, 2022.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D.,
    和 Hajishirzi, H. Self-instruct: 通过自生成指令对齐语言模型，2022年。'
- en: '[29] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar,
    A., Spencer-Smith, J., and Schmidt, D. C. A prompt pattern catalog to enhance
    prompt engineering with chatgpt, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar,
    A., Spencer-Smith, J., 和 Schmidt, D. C. 提升 ChatGPT 的提示工程的提示模式目录，2023年。'
- en: '[30] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang,
    B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren,
    R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., and Wen, J.-R. A survey of
    large language models, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang,
    B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren,
    R., Li, Y., Tang, X., Liu, Z., Liu, P., Nie, J.-Y., 和 Wen, J.-R. 大型语言模型的调查，2023年。'
- en: '[31] Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal and transferable
    adversarial attacks on aligned language models, 2023.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Zou, A., Wang, Z., Kolter, J. Z., 和 Fredrikson, M. 对齐语言模型的通用和可转移对抗攻击，2023年。'
