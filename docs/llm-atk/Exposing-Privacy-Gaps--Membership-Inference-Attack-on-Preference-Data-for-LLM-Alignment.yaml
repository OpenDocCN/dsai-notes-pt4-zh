- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:43:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:43:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 揭示隐私漏洞：针对 LLM 对齐的偏好数据的成员推断攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06443](https://ar5iv.labs.arxiv.org/html/2407.06443)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.06443](https://ar5iv.labs.arxiv.org/html/2407.06443)
- en: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡
- en: ^† Texas A&M University  ^‡ Amazon
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ^† 德克萨斯 A&M 大学  ^‡ 亚马逊
- en: qf31@tamu.edu  kasasiva@amazon.com  yunhyoku@amazon.com  choonhui@amazon.com
     sravanb@amazon.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: qf31@tamu.edu  kasasiva@amazon.com  yunhyoku@amazon.com  choonhui@amazon.com
     sravanb@amazon.com
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLMs) have seen widespread adoption due to their remarkable
    natural language capabilities. However, when deploying them in real-world settings,
    it is important to align LLMs to generate texts according to acceptable human
    standards. Methods such as Proximal Policy Optimization (PPO) and Direct Preference
    Optimization (DPO) have made significant progress in refining LLMs using human
    preference data. However, the privacy concerns inherent in utilizing such preference
    data have yet to be adequately studied. In this paper, we investigate the vulnerability
    of LLMs aligned using human preference datasets to membership inference attacks
    (MIAs), highlighting the shortcomings of previous MIA approaches with respect
    to preference data. Our study has two main contributions: first, we introduce
    a novel reference-based attack framework specifically for analyzing preference
    data called PREMIA (Preference data MIA); second, we provide empirical evidence
    that DPO models are more vulnerable to MIA compared to PPO models. Our findings
    highlight gaps in current privacy-preserving practices for LLM alignment.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其卓越的自然语言能力而被广泛采用。然而，在实际应用中，将 LLM 对齐以生成符合人类可接受标准的文本至关重要。像 Proximal
    Policy Optimization (PPO) 和 Direct Preference Optimization (DPO) 这样的技术在使用人类偏好数据优化
    LLM 方面取得了显著进展。然而，利用这些偏好数据的隐私问题尚未得到充分研究。在本文中，我们探讨了使用人类偏好数据集对齐的 LLM 对成员推断攻击（MIA）的脆弱性，突出了以往
    MIA 方法在处理偏好数据时的不足。我们的研究有两个主要贡献：首先，我们引入了一种新的基于参考的攻击框架，专门用于分析偏好数据，称为 PREMIA（偏好数据
    MIA）；其次，我们提供了实证证据，表明 DPO 模型比 PPO 模型更容易受到 MIA 的攻击。我们的发现揭示了当前 LLM 对齐中的隐私保护实践中的漏洞。
- en: 'Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示隐私漏洞：针对 LLM 对齐的偏好数据的成员推断攻击
- en: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡ ^† Texas A&M University  ^‡ Amazon qf31@tamu.edu  kasasiva@amazon.com
     yunhyoku@amazon.com  choonhui@amazon.com  sravanb@amazon.com
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Qizhang Feng^†  Siva Rajesh Kasa^‡  Hyokun Yun^‡  Choon Hui Teo^‡  Sravan Babu
    Bodapati^‡ ^† 德克萨斯 A&M 大学  ^‡ 亚马逊 qf31@tamu.edu  kasasiva@amazon.com  yunhyoku@amazon.com
     choonhui@amazon.com  sravanb@amazon.com
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large language models (LLMs) have seen a surge in their adoption in the recent
    past due to their remarkable capabilities on a wide range of natural language
    processing (NLP) tasks such as question answering, code generation, etc Zhao et al.
    ([2023](#bib.bib33)). When deployed in real-world scenarios, it is important to
    align LLMs to human preferences. Techniques such as Proximal Policy Optimization
    (PPO) and Direct Preference Optimization (DPO) play a key role in aligning LLMs
    with human ethical standards by leveraging human-derived preference data  Christiano
    et al. ([2017](#bib.bib6)); Rafailov et al. ([2024](#bib.bib22)); Yang et al.
    ([2024](#bib.bib30)). Although these approaches improve the alignment of models
    with human values, they are fraught with privacy concerns because of their use
    of human-generated data. In this work, we investigate the Membership Inference
    Attack (MIA), a widely-studied vulnerability that attempts to determine whether
    specific data points are used in the model’s training dataset. The study of MIA
    highlights vulnerabilities in a variety of machine learning paradigms, including
    several recent studies that specifically focus on LLMs (Fu et al., [2023](#bib.bib8);
    Shi et al., [2024](#bib.bib25)). Although existing research on MIA in the context
    of LLMs highlights the need to evaluate and address the need for privacy concerns,
    the unique challenges posed by alignment methods such as the PPO and DPO approaches
    (where preference data directly influences model behavior) remain to be explored.
    Traditional MIA frameworks fall short when applied to the complex, context-dependent
    optimization procedures used in LLM alignment. In this paper, we introduce a novel
    MIA framework that is specifically tailored to address preference data vulnerabilities
    in LLM alignment, providing a more precise analysis tool that can effectively
    mitigate these vulnerabilities. Our contributions to this field are twofold:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的应用呈现出激增，因其在问答、代码生成等各种自然语言处理（NLP）任务中表现出卓越的能力。Zhao等人（[2023](#bib.bib33)）。在实际应用中，将LLMs与人类偏好对齐是至关重要的。诸如Proximal
    Policy Optimization（PPO）和Direct Preference Optimization（DPO）等技术通过利用人类生成的偏好数据在将LLMs与人类伦理标准对齐方面发挥了关键作用。Christiano等人（[2017](#bib.bib6)）；Rafailov等人（[2024](#bib.bib22)）；Yang等人（[2024](#bib.bib30)）。尽管这些方法改善了模型与人类价值观的对齐，但由于其使用了人类生成的数据，仍然存在隐私问题。在这项工作中，我们研究了Membership
    Inference Attack（MIA），这是一种广泛研究的脆弱性，试图确定特定数据点是否被用于模型的训练数据集中。对MIA的研究揭示了各种机器学习范式中的脆弱性，包括一些最近特别关注LLMs的研究（Fu等人，[2023](#bib.bib8)；Shi等人，[2024](#bib.bib25)）。尽管现有关于LLMs中MIA的研究强调了评估和解决隐私问题的必要性，但PPO和DPO方法等对齐方法所带来的独特挑战（即偏好数据直接影响模型行为）仍有待探索。传统的MIA框架在应用于LLM对齐中复杂、依赖于上下文的优化过程时显得不足。在本文中，我们介绍了一种新颖的MIA框架，特别针对LLM对齐中的偏好数据脆弱性，提供了一种更精确的分析工具，能够有效缓解这些脆弱性。我们在该领域的贡献有两个方面：
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Introduction of a Novel Reference-based Attack Framework: We propose a new
    attack framework designed to assess the vulnerability of preference data to MIA,
    providing an effective analytical tool to address the unique privacy challenges
    in LLM alignment.'
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 新颖的基于参考的攻击框架介绍：我们提出了一种新的攻击框架，旨在评估偏好数据对MIA的脆弱性，为解决LLM对齐中的独特隐私挑战提供有效的分析工具。
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comparative Vulnerability Assessment of DPO and PPO Models: Through our framework,
    we find that DPO models are more vulnerable to MIA compared to PPO models. This
    insight not only points to significant privacy concerns, but also emphasizes the
    need for stronger privacy-preserving strategies in developing and deploying LLMs
    aligned using DPO.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DPO和PPO模型的比较脆弱性评估：通过我们的框架，我们发现DPO模型比PPO模型对MIA的脆弱性更大。这一发现不仅指出了显著的隐私问题，还强调了在使用DPO对齐LLMs时需要更强的隐私保护策略。
- en: 2 Preliminaries
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识
- en: This section introduces the notations and background concepts upon which the
    rest of the paper builds. We begin by defining the frameworks of PPO and DPO,
    followed by an overview of MIAs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了本文其余部分构建的符号和背景概念。我们首先定义了PPO和DPO的框架，然后概述了MIA。
- en: 2.1 Model Alignment
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 模型对齐
- en: Model alignment ensures LLMs adhere to human values and ethics by adjusting
    their outputs to match human preferences Hendrycks et al. ([2021](#bib.bib11));
    Ouyang et al. ([2022](#bib.bib19)). Such alignment is critical for creating AI
    systems that act in ways that benefit humans and reduce the risks associated with
    improper alignment. Among the various model alignment techniques, PPO and DPO
    are some of the widely used approaches Xu et al. ([2024](#bib.bib29)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对齐通过调整输出以匹配人类偏好，确保 LLM 遵循人类价值观和伦理（Hendrycks 等人，[2021](#bib.bib11)；Ouyang 等人，[2022](#bib.bib19)）。这种对齐对创建对人类有益并减少不当对齐风险的
    AI 系统至关重要。在各种模型对齐技术中，PPO 和 DPO 是一些广泛使用的方法（Xu 等人，[2024](#bib.bib29)）。
- en: 2.1.1 Proximal Policy Optimization (PPO)
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 临近策略优化（PPO）
- en: 'Stiennon et al. ([2020](#bib.bib27)) and Bai et al. ([2022](#bib.bib2)) illustrate
    Reinforcement Learning from Human Feedback (RLHF) that integrates human feedback
    into the training of pre-trained Language Models (LMs), encompassing three phases:
    Supervised Fine-Tuning (SFT), Preference Sampling with Reward Learning, and Reinforcement
    Learning (RL) through PPO.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Stiennon 等人 ([2020](#bib.bib27)) 和 Bai 等人 ([2022](#bib.bib2)) 介绍了从人类反馈中学习的强化学习（RLHF），这种方法将人类反馈融入预训练语言模型（LMs）的训练中，包括三个阶段：监督微调（SFT）、带奖励学习的偏好采样和通过
    PPO 的强化学习（RL）。
- en: SFT begins the process by fine-tuning a pre-trained LM on task-specific data
    to obtain a model $\pi^{\text{SFT}}$, enhancing the LLM’s performance on the task
    at hand.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 通过在任务特定数据上微调预训练的 LM 开始该过程，以获得模型 $\pi^{\text{SFT}}$，提升 LLM 在当前任务上的表现。
- en: Preference Data Collection involves gathering a set of preference data pairs  is
    a prompt and  are two different responses. Here,  for the given context $x$.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好数据收集涉及收集一组偏好数据对，其中是一个提示，和 是两个不同的响应。这里，对于给定的上下文 $x$。
- en: 'Reward Modeling Phase uses the preference pairs to train the reward model  represents
    the trainable parameters. The trainable model can be a classification header layer
    attached to the base model or a separate model. The Bradley-Terry (BT) model is
    commonly used to represent the probability that one response is better than another:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励建模阶段使用偏好对来训练奖励模型，表示可训练的参数。可训练模型可以是附加到基础模型上的分类头层或一个独立的模型。Bradley-Terry（BT）模型通常用于表示一个响应比另一个响应更好的概率：
- en: '|  | $\displaystyle\mathcal{L}_{R}(r_{\phi},\mathcal{D})=$ |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{R}(r_{\phi},\mathcal{D})=$ |  | (1) |'
- en: '|  | $1$2 |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where  to , and $\mathcal{D}$ denotes the dataset of preference pairs. This
    loss function measures the accuracy of the reward model in predicting human preferences.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}$ 表示偏好对的数据集。该损失函数测量奖励模型在预测人类偏好时的准确性。
- en: 'RL Fine-Tuning Phase then fine-tunes the LM further using the learned reward
    function, striving to align model outputs with human preferences while maintaining
    generative diversity:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: RL 微调阶段随后使用学习到的奖励函数进一步微调 LM，努力将模型输出与人类偏好对齐，同时保持生成多样性：
- en: '|  | $\displaystyle\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y&#124;x)}[r_{\phi}(x,y)]$
    |  | (2) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{\pi_{\theta}}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y&#124;x)}[r_{\phi}(x,y)]$
    |  | (2) |'
- en: '|  | $\displaystyle\;\;\;\;\;\;-\beta\mathbb{D}_{\textrm{KL}}[\pi_{\theta}(y&#124;x)&#124;&#124;\pi^{\text{SFT}}(y&#124;x)],$
    |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\;\;\;\;\;\;-\beta\mathbb{D}_{\textrm{KL}}[\pi_{\theta}(y&#124;x)&#124;&#124;\pi^{\text{SFT}}(y&#124;x)],$
    |  |'
- en: 'balancing fidelity to human feedback with the preservation of the model’s original
    capabilities. Here, , the trainable parameters. The optimization in equation [2](#S2.E2
    "Equation 2 ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣
    2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference
    Data for LLM Alignment") is carried out using Proximal Policy Optimization (PPO)
    method (Schulman et al., [2017](#bib.bib24)) and throughout the rest of the paper,
    we use RLHF and PPO interchangeably to refer the same approach.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，平衡对人类反馈的忠实度与模型原有能力的保持。这里，训练参数。优化在方程 [2](#S2.E2 "方程 2 ‣ 2.1.1 临近策略优化（PPO）
    ‣ 2.1 模型对齐 ‣ 2 初步知识 ‣ 暴露隐私漏洞：对 LLM 对齐的偏好数据进行成员推断攻击") 中使用临近策略优化（PPO）方法（Schulman
    等人，[2017](#bib.bib24)）进行，本文其余部分中，我们将 RLHF 和 PPO 互换使用来指代相同的方法。
- en: 2.1.2 Direct Preference Optimization (DPO)
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 直接偏好优化（DPO）
- en: 'DPO offers a refined approach to fine-tuning language models by directly leveraging
    preference data, bypassing the explicit reward model construction typically associated
    with RLHF methodologies Rafailov et al. ([2024](#bib.bib22)). This method reformulates
    the two-step optimization procedure in equations [1](#S2.E1 "Equation 1 ‣ 2.1.1
    Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing
    Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment")
    and [2](#S2.E2 "Equation 2 ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model
    Alignment ‣ 2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") into a single optimization problem that
    simultaneously optimizes the policy and encodes an implicit reward mechanism based
    on the preference data.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 'DPO 提供了一种改进的语言模型微调方法，通过直接利用偏好数据，绕过了通常与 RLHF 方法相关的显式奖励模型构建。Rafailov 等人 ([2024](#bib.bib22))。该方法将方程
    [1](#S2.E1 "Equation 1 ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model
    Alignment ‣ 2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") 和 [2](#S2.E2 "Equation 2 ‣ 2.1.1 Proximal
    Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing Privacy
    Gaps: Membership Inference Attack on Preference Data for LLM Alignment") 中的两步优化过程重新表述为一个单一的优化问题，同时优化策略并基于偏好数据编码隐式奖励机制。'
- en: '|  | $$\begin{split}&amp;\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=\\
    &amp;-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\biggl{[}\log\sigma\biggl{(}\beta\log\frac{\pi_{\theta}(y_{w}\mid
    x)}{\pi_{\text{ref}}(y_{w}\mid x)}\\'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}&amp;\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=\\
    &amp;-\mathbb{E}_{(x,y_{w},y_{l})\sim\mathcal{D}}\biggl{[}\log\sigma\biggl{(}\beta\log\frac{\pi_{\theta}(y_{w}\mid
    x)}{\pi_{\text{ref}}(y_{w}\mid x)}\\'
- en: '&amp;-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\text{ref}}(y_{l}\mid
    x)}\biggr{)}\biggr{]}.\end{split}$$ |  | (3) |'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;-\beta\log\frac{\pi_{\theta}(y_{l}\mid x)}{\pi_{\text{ref}}(y_{l}\mid
    x)}\biggr{)}\biggr{]}.\end{split}$$ |  | (3) |'
- en: 'Here, . This optimization method is preferred over PPO because it simplifies
    training by optimizing directly on the preference data, which improves computational
    efficiency and is easier to implement  Rafailov et al. ([2024](#bib.bib22)); Xu
    et al. ([2024](#bib.bib29)). Note that in PPO (equation [2](#S2.E2 "Equation 2
    ‣ 2.1.1 Proximal Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries
    ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference Data for LLM
    Alignment")), contrary to DPO (equation [3](#S2.E3 "Equation 3 ‣ 2.1.2 Direct
    Preference Optimization (DPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing
    Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment")),
    the final model being optimized is not directly aligned using the data $\mathcal{D}$.
    This is the key intuition behind why PPO-aligned models are less susceptible to
    privacy threats compared to their DPO counterparts.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '这里。该优化方法相比 PPO 更受欢迎，因为它通过直接在偏好数据上进行优化，简化了训练过程，提高了计算效率并且更易于实现。Rafailov 等人 ([2024](#bib.bib22));
    Xu 等人 ([2024](#bib.bib29))。注意，在 PPO (方程 [2](#S2.E2 "Equation 2 ‣ 2.1.1 Proximal
    Policy Optimization (PPO) ‣ 2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing Privacy
    Gaps: Membership Inference Attack on Preference Data for LLM Alignment")) 中，与
    DPO (方程 [3](#S2.E3 "Equation 3 ‣ 2.1.2 Direct Preference Optimization (DPO) ‣
    2.1 Model Alignment ‣ 2 Preliminaries ‣ Exposing Privacy Gaps: Membership Inference
    Attack on Preference Data for LLM Alignment")) 相反，最终优化的模型并未直接利用数据 $\mathcal{D}$
    进行对齐。这是为什么与 DPO 相比，PPO 对齐的模型更不容易受到隐私威胁的关键直觉。'
- en: 2.2 Membership Inference Attacks (MIA) on LLMs
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 会员推断攻击 (MIA) 对 LLM 的影响
- en: MIA poses a significant privacy risk in the context of LLMs, challenging the
    security of data used in training such models Shokri et al. ([2017](#bib.bib26));
    Nasr et al. ([2018](#bib.bib18)). In LLMs, MIAs seek to determine whether specific
    data was part of the model’s training set, exploiting the model’s behavior or
    output nuances to infer data membership. These attacks are particularly concerning
    for models trained on vast datasets, where inadvertently revealing individual
    data points could lead to privacy breaches.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: MIA 在 LLM 的背景下提出了显著的隐私风险，挑战了用于训练这些模型的数据安全性。Shokri 等人 ([2017](#bib.bib26)); Nasr
    等人 ([2018](#bib.bib18))。在 LLM 中，MIA 试图确定特定数据是否属于模型的训练集，通过利用模型的行为或输出细微差别来推断数据成员资格。这些攻击对在大量数据集上训练的模型尤其令人担忧，因为不经意间暴露个别数据点可能导致隐私泄露。
- en: The effectiveness of an MIA against LLMs is quantified by a score function ,
    an input .
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: MIA 对 LLM 的有效性通过一个评分函数来量化，这是一个输入。
- en: '|  | $\mathcal{M}:\mathcal{X}\times\text{Access}(\Theta)\to\mathbb{R}.$ |  |
    (4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{M}:\mathcal{X}\times\text{Access}(\Theta)\to\mathbb{R}.$ |  |
    (4) |'
- en: Research on MIAs targeting LLMs underscores the need for robust privacy-preserving
    techniques to safeguard training data, with implications for the development and
    deployment of secure, trustworthy AI systems Carlini et al. ([2020](#bib.bib4)).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 针对LLMs的MIA研究强调了保护训练数据的强大隐私保护技术的必要性，这对安全、可信赖的人工智能系统的开发和部署有重要意义（Carlini et al.
    ([2020](#bib.bib4))）。
- en: 2.3 Problem Statement
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 问题陈述
- en: Current research on MIAs has advanced understanding of risks in pre-trained
    text models, but gaps remain in applying MIAs to preference datasets in LLM alignment.
    This oversight poses substantial privacy risks, given the critical role of preference
    data in shaping LLM outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当前关于隐私泄露攻击（MIA）的研究已经提高了对预训练文本模型风险的理解，但在将MIA应用于LLM对齐中的偏好数据集方面仍存在空白。鉴于偏好数据在塑造LLM输出中的关键作用，这种忽视带来了实质性的隐私风险。
- en: 'Let  is a prompt,  is the less preferred response. The vulnerability of this
    preference data to MIAs requires a nuanced examination, which can be categorized
    into three distinct attack vectors:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设`x`为一个提示，`y_{l}`为不太偏好的响应。对MIA而言，这种偏好数据的漏洞需要进行细致的检验，可以被分类为三种不同的攻击向量：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attack against prompts and preferred responses: This attack determines whether
    a specific pair of prompt  has been used in training, highlighting potential privacy
    breaches if such data can be identified:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对提示和首选响应的攻击：这种攻击确定是否使用了特定的提示对进行训练，如果可以识别这些数据，将突出潜在的隐私泄露：
- en: '|  | $\text{MIA}_{\text{prompt, }y_{w}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (5) |'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{MIA}_{\text{prompt, }y_{w}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (5) |'
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attack against prompts and non-preferred responses: This attack focuses on
    identifying if a pair consisting of a prompt  was part of the training data, potentially
    exposing sensitive decision-making processes:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对提示和非首选响应的攻击：这种攻击关注于识别一个包含提示的对是否属于训练数据，可能暴露出敏感的决策过程：
- en: '|  | $\text{MIA}_{\text{prompt, }y_{l}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (6) |'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{MIA}_{\text{prompt, }y_{l}}:\mathcal{X}\times\mathcal{Y}\to\{0,1\},$
    |  | (6) |'
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attack against the entire preference tuple: This more comprehensive attack
    assesses whether the entire tuple $(x,y_{w},y_{l})$ can be traced back to the
    training set, reflecting a higher risk of revealing critical training methodologies:'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对整个偏好元组的攻击：这种更全面的攻击评估是否可以将整个元组`(x,y_{w},y_{l})`追溯到训练集，从而反映出揭示关键训练方法的更高风险：
- en: '|  | $\text{MIA}_{\mathcal{D}_{\text{pref}}}:\mathcal{X}\times\mathcal{Y}\times\mathcal{Y}\to\{0,1\}.$
    |  | (7) |'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\text{MIA}_{\mathcal{D}_{\text{pref}}}:\mathcal{X}\times\mathcal{Y}\times\mathcal{Y}\to\{0,1\}.$
    |  | (7) |'
- en: This detailed breakdown elucidates the complex vulnerabilities associated with
    preference data in LLMs. By identifying these specific attack vectors, we aim
    to advance privacy-preserving mechanisms that safeguard the alignment process
    and ensure that models respect and protect individual privacy while adhering to
    human ethical standards.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个详细的分析阐明了与大型语言模型（LLMs）中的偏好数据相关的复杂漏洞。通过识别这些具体的攻击向量，我们旨在推动隐私保护机制的发展，确保对齐过程中的模型尊重并保护个人隐私，同时遵守人类伦理标准。
- en: 2.4 Hypotheses Regarding DPO vs PPO
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 关于DPO与PPO的假设
- en: To guide our experimental design and directly address the concerns raised by
    our study, we propose the following hypotheses. These are crafted to explore the
    distinct impacts of DPO and Proximal Policy Optimization (PPO) on privacy and
    performance, and are structured to align with the subsequent analyses conducted
    in our experiments.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导我们的实验设计，并直接解决我们研究中提出的问题，我们提出以下假设。这些假设旨在探索DPO和近端策略优化（PPO）在隐私和性能上的不同影响，并与我们实验中的后续分析相一致。
- en: 'Hypothesis 1: Differential Vulnerability to MIAs: We hypothesize that the DPO
    model is more vulnerable to MIA than the PPO model since the DPO model uses preference
    data directly, which may lead to overfitting. We empirically assess the MIA vulnerability
    of DPO and PPO models.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设1：对MIA的差异性脆弱性：我们假设DPO模型比PPO模型更容易受到MIA的影响，因为DPO模型直接使用偏好数据，这可能导致过拟合。我们将实证评估DPO和PPO模型的MIA脆弱性。
- en: 'Hypothesis 2: Influence of Model Size on MIA Risk: We postulate that larger
    models, regardless of the training method (DPO or PPO), will show increased vulnerability
    to MIAs due to their greater capacity to memorize training data. This hypothesis
    is explored in §[4.4.2](#S4.SS4.SSS2 "4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment"), assessing how model size affects susceptibility
    to data leakage.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设2：模型规模对MIA风险的影响：我们假设较大的模型，无论是训练方法（DPO还是PPO）如何，都将表现出更高的MIA易感性，因为它们具有更大的记忆训练数据的能力。该假设在§[4.4.2](#S4.SS4.SSS2
    "4.4.2 模型规模对MIA有效性的影响 ‣ 4.4 结果 ‣ 4 实验 ‣ 公开隐私漏洞：对偏好数据的成员身份推断攻击")中进行了探讨，评估模型规模如何影响数据泄漏的易感性。
- en: 'Hypothesis 3: Trade-offs Between Performance and Privacy: We propose that while
    DPO may enhance alignment with human preferences and potentially improve task-specific
    performance, it also increases the risk of privacy breaches compared to PPO. This
    trade-off is critically examined in §[4.4.3](#S4.SS4.SSS3 "4.4.3 Trade-Off between
    Performance and Privacy ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps:
    Membership Inference Attack on Preference Data for LLM Alignment"), comparing
    the performance benefits of DPO against its privacy drawbacks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 假设3：性能与隐私之间的权衡：我们提出，虽然DPO可能增强与人类偏好的对齐并可能提高任务特定性能，但它也增加了隐私泄露的风险，相比PPO。该权衡在§[4.4.3](#S4.SS4.SSS3
    "4.4.3 性能与隐私之间的权衡 ‣ 4.4 结果 ‣ 4 实验 ‣ 公开隐私漏洞：对偏好数据的成员身份推断攻击")中进行了关键检查，比较了DPO的性能优势与隐私缺陷。
- en: 3 Method
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: Our approach introduces a tailored framework for evaluating MIA on preference
    datasets used for LLM model alignment. Traditional MIA approaches do not take
    into account the uniqueness of preference data, which includes relational dynamics
    and contextual dependencies. Our approach addresses these nuances by splitting
    the analysis into evaluating individual components and entire preference tuples,
    and using conditional probability ratios to compare against a reference model
    to more accurately infer membership in the data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法引入了一个量身定制的框架，用于评估用于LLM模型对齐的偏好数据集上的MIA。传统的MIA方法没有考虑偏好数据的独特性，包括关系动态和上下文依赖性。我们的方法通过将分析拆分为评估单个组件和整个偏好元组，并使用条件概率比率与参考模型进行比较，从而更准确地推断数据中的成员身份，解决了这些细微差别。
- en: 3.1 For Individual Response
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 针对单个响应
- en: 'Assessing the vulnerability of individual responses—either preferred ()—to
    MIAs necessitates a nuanced approach that considers the specific characteristics
    of preference data. We compute the conditional probability ratio relative to a
    reference model $\pi_{\text{ref}}$:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 评估单个响应——无论是偏好的（）——对MIA的易感性需要一种细致的方法，这种方法考虑了偏好数据的特定特征。我们计算相对于参考模型$\pi_{\text{ref}}$的条件概率比率：
- en: '|  | $\rho_{y}=\frac{\pi_{\theta}(y&#124;x)}{\pi_{\text{ref}}(y&#124;x)},$
    |  | (8) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho_{y}=\frac{\pi_{\theta}(y&#124;x)}{\pi_{\text{ref}}(y&#124;x)},$
    |  | (8) |'
- en: where $\pi_{\theta}$ represents the target model. This ratio measures the likelihood
    that the target model will produce a specific response compared to a baseline
    model, indicating potential overfitting to training data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\pi_{\theta}$代表目标模型。这个比率测量了目标模型相对于基线模型生成特定响应的可能性，指示了对训练数据的潜在过拟合。
- en: 'Employing $\rho_{y}$ enhances specificity by accounting for the subtle nuances
    and context-dependent nature of response preferences, thus improving the detection
    of data membership:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用$\rho_{y}$通过考虑响应偏好的细微差别和上下文相关性来增强特异性，从而改善数据成员身份的检测：
- en: '|  | $$\text{MIA}_{\text{single}}(x,y)=\begin{cases}1&amp;\text{if }\rho_{y}>
    |  | (9) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\text{MIA}_{\text{single}}(x,y)=\begin{cases}1&amp;\text{if }\rho_{y}>
    |  | (9) |'
- en: Although $\tau_{y}$ is mentioned, our primary metric in the experiments is the
    Area Under the Receiver Operating Characteristic (AUROC), which does not require
    setting a specific threshold. This approach allows for a flexible assessment of
    model sensitivity across various potential values.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管提到了$\tau_{y}$，但我们实验中的主要指标是接收者操作特征曲线下面积（AUROC），它不需要设置特定的阈值。这种方法允许对模型敏感性进行灵活评估，适用于各种潜在值。
- en: The choice of the reference model . This model can be the base pre-trained model
    from which $\pi_{\theta}$ originated or a different base model trained on the
    same dataset. Our experiments, designed to test both scenarios, consistently demonstrate
    robust performance of our MIA method under various conditions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 参考模型的选择。该模型可以是$\pi_{\theta}$起源的基础预训练模型，也可以是训练在相同数据集上的不同基础模型。我们的实验旨在测试这两种情况，并一致展示了在各种条件下我们MIA方法的强大性能。
- en: 3.2 For the Entire Preference Tuple
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 对整个偏好元组
- en: 'To ascertain the membership of the complete preference tuple $(x,y_{w},y_{l})$,
    we compute the difference between the probability ratios of the preferred and
    not preferred responses:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定完整偏好元组 $(x,y_{w},y_{l})$ 的成员身份，我们计算了首选和非首选响应的概率比率之间的差异：
- en: '|  | $\Delta\rho=\rho_{y_{w}}-\rho_{y_{l}}.$ |  | (10) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta\rho=\rho_{y_{w}}-\rho_{y_{l}}.$ |  | (10) |'
- en: 'This measure captures the comparative preference strength more effectively,
    offering a nuanced insight into how preference data impacts model training:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这一措施更有效地捕捉了比较的偏好强度，提供了对偏好数据如何影响模型训练的细致见解：
- en: '|  | <math id=$$ |  | (11) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | <math id=$$ |  | (11) |'
- en: The specified threshold $\tau_{\Delta}$ is set based on the variance observed
    within the training data, allowing a more accurate identification of the data
    used during the training phase.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 指定的阈值 $\tau_{\Delta}$ 是基于在训练数据中观察到的方差设置的，从而允许更准确地识别在训练阶段使用的数据。
- en: 4 Experiments
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Research Questions and Experiment Design Rationale
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 研究问题及实验设计依据
- en: This subsection outlines the key research questions guiding our experimental
    design, providing a rationale for our methodologies. Derived from our hypotheses,
    these questions aim to evaluate the comparative effectiveness, privacy implications,
    and utility of DPO and Proximal Policy Optimization (PPO) in training LLMs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节概述了指导我们实验设计的关键研究问题，并提供了我们方法论的理由。这些问题源于我们的假设，旨在评估DPO和近端策略优化（PPO）在训练大型语言模型（LLMs）中的相对有效性、隐私影响和实用性。
- en: 'Research Question 1: How do DPO and PPO differ in their susceptibility to Membership
    Inference Attacks? This question tests Hypothesis 1 by comparing the vulnerability
    of models trained using DPO and PPO to MIA to shed light on privacy and data security
    issues.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题1：DPO和PPO在易受成员推断攻击方面有何不同？这个问题通过比较使用DPO和PPO训练的模型在MIA中的脆弱性来测试假设1，以揭示隐私和数据安全问题。
- en: 'Research Question 2: Does model size influence its risk of data leakage through
    MIAs, and how does this vary between DPO and PPO trained models? In line with
    Hypothesis 2, this question explores the impact of model size on MIA effectiveness,
    assessing if larger models pose greater privacy risks.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题2：模型大小是否影响其通过MIA的数据泄漏风险，这在DPO和PPO训练的模型之间有何不同？根据假设2，本问题探讨了模型大小对MIA有效性的影响，评估更大的模型是否存在更大的隐私风险。
- en: 'Research Question 3: What are the performance and privacy trade-offs when employing
    DPO versus PPO in LLMs? Echoing Hypothesis 3, this question examines the trade-off
    between performance and data privacy in tasks that need to be understood like
    humans, assessing whether greater alignment with human preferences would compromise
    privacy.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题3：在LLMs中使用DPO与PPO时，性能和隐私权衡如何？这与假设3相呼应，本问题考察了在需要像人类一样理解的任务中，性能和数据隐私之间的权衡，评估更大程度的与人类偏好的对齐是否会妨碍隐私。
- en: 4.2 Setup
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 设置
- en: Models.
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型。
- en: 'Our experiments are conducted using a variety of models to ensure a comprehensive
    evaluation on different scales of model complexity. We include Mistral-7B-v0.1 Jiang
    et al. ([2023](#bib.bib14)), as well as a series of models from the OpenAI GPT-2
    family Radford et al. ([2019](#bib.bib21)): GPT2, GPT2-medium, GPT2-large, and
    GPT2-xl. Furthermore, we incorporate Open-llama-3b and Open-llama-7b models Geng
    and Liu ([2023](#bib.bib9)); Computer ([2023](#bib.bib7)); Touvron et al. ([2023](#bib.bib28))
    to broaden our analysis across various architectures and capacities. For the reference
    model in our ratio calculations, we primarily use the SFT model trained from the
    same base pre-trained version of the model being evaluated. Additionally, we conduct
    experiments where the reference model differs from the base model to evaluate
    the robustness of our methodology under varied conditions.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验使用了多种模型以确保对不同复杂度模型的全面评估。我们包括了 Mistral-7B-v0.1 Jiang 等（[2023](#bib.bib14)），以及一系列
    OpenAI GPT-2 系列模型 Radford 等（[2019](#bib.bib21)）：GPT2、GPT2-medium、GPT2-large 和
    GPT2-xl。此外，我们还结合了 Open-llama-3b 和 Open-llama-7b 模型 Geng 和 Liu（[2023](#bib.bib9)）；Computer（[2023](#bib.bib7)）；Touvron
    等（[2023](#bib.bib28)）以拓宽我们对不同架构和能力的分析。在我们的比率计算中作为参考模型，我们主要使用从相同基础预训练版本训练的 SFT
    模型。此外，我们还进行了一些实验，其中参考模型与基础模型不同，以评估在不同条件下我们方法的稳健性。
- en: Datasets.
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集。
- en: For our experiments, we utilize the Stack-Exchange-Paired dataset¹¹1[https://huggingface.co/datasets/lvwerra/stack-exchange-paired](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)
    and the IMDB-RLHF-Pair dataset²²2[https://github.com/QiyaoWei/Reproduce-DPO](https://github.com/QiyaoWei/Reproduce-DPO).
    Both datasets have a prompt  and the "rejected" response $y_{l}$. The Stack-Exchange-Paired
    dataset contains questions and answers from the Stack Overflow dataset, where
    answers with more votes are preferred. The IMDB-RLHF-Pair dataset is generated
    by IMDB, and responses with positive sentiment are preferred. For the Stack-Exchange-Paired
    dataset, the data/rl split is used for training, and data/evaluation is used as
    validation data. For the IMDB-RLHF-Pair dataset, 20k entries are used for training,
    while the remaining is for validation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的实验，我们利用了 Stack-Exchange-Paired 数据集¹¹1[https://huggingface.co/datasets/lvwerra/stack-exchange-paired](https://huggingface.co/datasets/lvwerra/stack-exchange-paired)
    和 IMDB-RLHF-Pair 数据集²²2[https://github.com/QiyaoWei/Reproduce-DPO](https://github.com/QiyaoWei/Reproduce-DPO)。这两个数据集都有一个提示和“拒绝”回应
    $y_{l}$。Stack-Exchange-Paired 数据集包含来自 Stack Overflow 数据集的问题和回答，其中获得更多投票的回答更受青睐。IMDB-RLHF-Pair
    数据集由 IMDB 生成，回应中正面情感的回答更受青睐。对于 Stack-Exchange-Paired 数据集，使用 data/rl 进行训练，使用 data/evaluation
    作为验证数据。对于 IMDB-RLHF-Pair 数据集，使用 20k 条条目进行训练，其余用于验证。
- en: Evaluation Metrics.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估指标。
- en: 'To comprehensively assess our models, we employ a dual-focused evaluation framework
    encompassing utility performance and MIA robustness:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估我们的模型，我们采用了一个双重焦点的评估框架，涵盖了实用性能和 MIA 稳健性：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Utility Performance: Our evaluation includes the reward score of generated
    responses given by the reward model and perplexity for assessing fluency. We also
    incorporate comprehensive diversity measures: Mean Segmented Type Token Ratio
    (MSSTR), Distinct-1, Distinct-2, Unique-1, and Unique-2 metrics (Johnson, [1944](#bib.bib15);
    Li et al., [2015](#bib.bib16); Ramamurthy et al., [2022](#bib.bib23)). Additionally,
    we utilize advanced text generation quality metrics such as BERTScore Zhang et al.
    ([2019](#bib.bib32)), ROUGE Lin ([2004](#bib.bib17)), BLEU Papineni et al. ([2002](#bib.bib20)),
    and METEOR Banerjee and Lavie ([2005](#bib.bib3)), which collectively offer a
    nuanced view of the models’ performance in terms of fluency, adequacy, and diversity,
    closely mirroring human judgment in text quality assessment.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实用性能：我们的评估包括由奖励模型提供的生成回应的奖励分数和用于评估流畅性的困惑度。我们还结合了全面的多样性测量指标：平均分段类型令牌比率（MSSTR）、Distinct-1、Distinct-2、Unique-1
    和 Unique-2 指标（Johnson，[1944](#bib.bib15)；Li 等，[2015](#bib.bib16)；Ramamurthy 等，[2022](#bib.bib23)）。此外，我们还利用先进的文本生成质量指标，如
    BERTScore Zhang 等（[2019](#bib.bib32)），ROUGE Lin（[2004](#bib.bib17)），BLEU Papineni
    等（[2002](#bib.bib20)）和 METEOR Banerjee 和 Lavie（[2005](#bib.bib3)），这些指标综合提供了模型在流畅性、充分性和多样性方面的细致视角，与人类在文本质量评估中的判断高度一致。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MIA Performance: To measure the model’s susceptibility to MIA, we utilize the
    Area Under the Receiver Operating Characteristic curve (AUROC). This metric encapsulates
    the model’s defense against MIAs, reflecting the balance between true positive
    rate and false positive rate in identifying training data.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MIA 性能：为了衡量模型对 MIA 的易感性，我们利用接收器操作特征曲线下面积（AUROC）。该指标概括了模型对 MIA 的防御能力，反映了识别训练数据时真正例率与假正例率之间的平衡。
- en: Implementation Details.
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实施细节。
- en: 'Due to the computational efficiency of LoRA, we used LoRA for all of our model
    training processes. Additionally, we hypothesized that fine-tuning LoRA at the
    RL stage would help to ensure that the aligned model does not deviate significantly
    from the reference model. To further improve efficiency, we also used quantization
    techniques. We use TRL³³3[https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)
    for model alignment training. More detailed implementation information can be
    found in Appendix [A](#A1 "Appendix A Implementation Details ‣ Exposing Privacy
    Gaps: Membership Inference Attack on Preference Data for LLM Alignment").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 LoRA 的计算效率，我们在所有模型训练过程中使用了 LoRA。此外，我们假设在 RL 阶段微调 LoRA 有助于确保对齐模型不会与参考模型偏离太大。为了进一步提高效率，我们还使用了量化技术。我们使用
    TRL³³3[https://huggingface.co/docs/trl/index](https://huggingface.co/docs/trl/index)
    进行模型对齐训练。更多详细的实施信息可以在附录 [A](#A1 "Appendix A Implementation Details ‣ Exposing
    Privacy Gaps: Membership Inference Attack on Preference Data for LLM Alignment")
    中找到。'
- en: 4.3 Baselines
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 基准
- en: To accurately evaluate our approach, we position it against well-known MIA baselines
    specifically tailored for language models and preference data analysis. These
    baselines are designed to target individual components of the preference data
    but do not extend to analyzing entire preference tuples.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确评估我们的方法，我们将其与专门针对语言模型和偏好数据分析的知名 MIA 基准进行对比。这些基准旨在针对偏好数据的单个组件，但不扩展到分析整个偏好元组。
- en: 'Perplexity (PPL):'
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 困惑度（PPL）：
- en: 'The loss attack method, based on the approach outlined in Yeom et al. ([2018](#bib.bib31)),
    utilizes the perplexity of a sequence to gauge how well a language model predicts
    the tokens within that sequence. Perplexity is defined as:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Yeom 等人（[2018](#bib.bib31)）提出的方法的损失攻击方法，利用序列的困惑度来评估语言模型对该序列中 token 的预测能力。困惑度定义为：
- en: '|  | $1$2 |  | (12) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (12) |'
- en: where a lower perplexity indicates a higher likelihood that the sequence was
    the training data.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的困惑度表示该序列更有可能是训练数据。
- en: 'Comparing to zlib Compression (Zlib):'
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与 zlib 压缩（Zlib）相比：
- en: This method measures the entropy of a sequence when compressed using zlib, compares
    the perplexity of a model to its zlib compression entropy, and uses their ratio
    as an inference metric  Carlini et al. ([2021](#bib.bib5)).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法测量使用 zlib 压缩时序列的熵，比较模型的困惑度与其 zlib 压缩熵，并使用它们的比率作为推断指标  Carlini 等人（[2021](#bib.bib5)）。
- en: 'Comparing to Lowercased Text (Lowercase):'
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与小写文本（Lowercase）相比：
- en: 'This method evaluates the change in perplexity of a sequence before and after
    it has been lowercased, to assess the model’s dependency on specific capitalization Carlini
    et al. ([2021](#bib.bib5)):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法评估序列在转为小写前后的困惑度变化，以评估模型对特定大小写的依赖性  Carlini 等人（[2021](#bib.bib5)）：
- en: '|  | $\text{Perplexity Ratio}=\frac{\mathcal{P}(\text{Original})}{\mathcal{P}(\text{Lowercased})}.$
    |  | (13) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Perplexity Ratio}=\frac{\mathcal{P}(\text{Original})}{\mathcal{P}(\text{Lowercased})}.$
    |  | (13) |'
- en: 'Comparing to Other Neural Language Models (Ref):'
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与其他神经语言模型（Ref）相比：
- en: This approach consists of comparing the ease of error of sequences between the
    target model and another small model. In our experiments, we specifically use
    GPT2 as the small model. Note that our approach uses conditional probabilities,
    whereas Ref does not.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法包括比较目标模型和另一个小模型之间的序列错误率。在我们的实验中，我们特别使用 GPT2 作为小模型。请注意，我们的方法使用条件概率，而参考方法则不使用。
- en: 'MIN-K% PROB (MIN-K):'
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MIN-K% 概率（MIN-K）：
- en: 'This method Shi et al. ([2024](#bib.bib25)) focuses on the minimum token probabilities
    within a text. It posits that non-member examples are more likely to contain outlier
    words with high negative log-likelihoods:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Shi 等人（[2024](#bib.bib25)）提出的方法关注文本中的最小 token 概率。该方法假设非成员示例更可能包含具有高负对数似然的异常词：
- en: '|  | $\text{MIN-K}(x)=\frac{1}{E}\sum_{x_{i}\in\text{Min-K\%}(x)}\log\pi_{\theta}(x_{i}&#124;x_{1},...,x_{i-1}).$
    |  | (14) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MIN-K}(x)=\frac{1}{E}\sum_{x_{i}\in\text{Min-K\%}(x)}\log\pi_{\theta}(x_{i}&#124;x_{1},...,x_{i-1}).$
    |  | (14) |'
- en: By analyzing these low probability tokens, MIN-K% PROB provides a distinct method
    to infer membership, enhancing the diversity of our baseline comparisons.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些低概率标记，MIN-K% PROB提供了一种独特的方法来推断成员关系，提高了我们基准比较的多样性。
- en: 4.4 Results
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 结果
- en: This section presents the findings from our experiments, highlighting the comparative
    effectiveness of our proposed MIA defense mechanism and analyzing the trade-off
    between model performance and privacy protection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了我们实验的发现，突出了我们提出的MIA防御机制的比较效果，并分析了模型性能与隐私保护之间的权衡。
- en: 4.4.1 Effectiveness of MIA Methodology
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 MIA方法学的有效性
- en: This subsection evaluates our MIA methodology for identifying if preference
    data components were used in training language models. Our detailed comparative
    analysis shows our method’s high precision in discerning sensitive data inclusions,
    outperforming traditional MIA approaches not tailored for preference data scenarios.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节评估了我们的MIA方法学，用于识别在训练语言模型时是否使用了偏好数据组件。我们的详细比较分析显示，我们的方法在识别敏感数据包含方面具有很高的精确度，优于那些未针对偏好数据场景进行调整的传统MIA方法。
- en: 'Table 1: AUROC scores comparing different MIA methods on Mistral-7B, Open-llama-3b,
    and Open-llama-7b models are presented, where higher scores indicate greater susceptibility
    to MIA. The best and second-best scores in each column are highlighted in orange
    and green, respectively. The better score between DPO and PPO trained models is
    underlined.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: 比较不同MIA方法在Mistral-7B、Open-llama-3b和Open-llama-7b模型上的AUROC分数，较高的分数表示对MIA的更大敏感性。每列中最佳和第二最佳分数分别以橙色和绿色突出显示。DPO和PPO训练模型中较好的分数加下划线。'
- en: '|  |  | IMDB |  |  |  | Stack-Exchange |  |  |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | IMDB |  |  |  | Stack-Exchange |  |  |  |'
- en: '|  |  |  |  |  |  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  |  |  |'
- en: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO | DPO | PPO |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO | DPO | PPO |'
- en: '|  Mistral-7B  | PPL | 0.569 | 0.538 | 0.588 | 0.503 | 0.572 | 0.500 | 0.561
    | 0.513 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  Mistral-7B  | PPL | 0.569 | 0.538 | 0.588 | 0.503 | 0.572 | 0.500 | 0.561
    | 0.513 |'
- en: '|  | Zlib | 0.593 | 0.568 | 0.606 | 0.535 | 0.566 | 0.528 | 0.523 | 0.528 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | Zlib | 0.593 | 0.568 | 0.606 | 0.535 | 0.566 | 0.528 | 0.523 | 0.528 |'
- en: '|  | Lowercase | 0.516 | 0.509 | 0.515 | 0.501 | 0.582 | 0.514 | 0.563 | 0.521
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | Lowercase | 0.516 | 0.509 | 0.515 | 0.501 | 0.582 | 0.514 | 0.563 | 0.521
    |'
- en: '|  | Ref | 0.571 | 0.533 | 0.607 | 0.510 | 0.548 | 0.504 | 0.612 | 0.512 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | Ref | 0.571 | 0.533 | 0.607 | 0.510 | 0.548 | 0.504 | 0.612 | 0.512 |'
- en: '|  | MIN-K | 0.564 | 0.535 | 0.582 | 0.509 | 0.601 | 0.503 | 0.627 | 0.513
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | MIN-K | 0.564 | 0.535 | 0.582 | 0.509 | 0.601 | 0.503 | 0.627 | 0.513
    |'
- en: '|  | PREMIA-base | 0.570 | 0.524 | 0.612 | 0.517 | 0.790 | 0.507 | 0.751 |
    0.556 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-base | 0.570 | 0.524 | 0.612 | 0.517 | 0.790 | 0.507 | 0.751 |
    0.556 |'
- en: '|  | PREMIA-SFT | 0.572 | 0.507 | 0.611 | 0.527 | 0.807 | 0.535 | 0.750 | 0.501
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-SFT | 0.572 | 0.507 | 0.611 | 0.527 | 0.807 | 0.535 | 0.750 | 0.501
    |'
- en: '|  Open-llama-3b  | PPL | 0.580 | 0.508 | 0.573 | 0.526 | 0.590 | 0.506 | 0.558
    | 0.507 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  Open-llama-3b  | PPL | 0.580 | 0.508 | 0.573 | 0.526 | 0.590 | 0.506 | 0.558
    | 0.507 |'
- en: '|  | Zlib | 0.602 | 0.540 | 0.595 | 0.506 | 0.530 | 0.529 | 0.522 | 0.508 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | Zlib | 0.602 | 0.540 | 0.595 | 0.506 | 0.530 | 0.529 | 0.522 | 0.508 |'
- en: '|  | Lowercase | 0.541 | 0.556 | 0.546 | 0.551 | 0.649 | 0.503 | 0.582 | 0.516
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | Lowercase | 0.541 | 0.556 | 0.546 | 0.551 | 0.649 | 0.503 | 0.582 | 0.516
    |'
- en: '|  | Ref | 0.587 | 0.508 | 0.590 | 0.535 | 0.610 | 0.505 | 0.603 | 0.524 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | Ref | 0.587 | 0.508 | 0.590 | 0.535 | 0.610 | 0.505 | 0.603 | 0.524 |'
- en: '|  | MIN-K | 0.587 | 0.526 | 0.579 | 0.538 | 0.610 | 0.533 | 0.613 | 0.512
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | MIN-K | 0.587 | 0.526 | 0.579 | 0.538 | 0.610 | 0.533 | 0.613 | 0.512
    |'
- en: '|  | PREMIA-base | 0.564 | 0.520 | 0.562 | 0.540 | 0.748 | 0.527 | 0.717 |
    0.511 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-base | 0.564 | 0.520 | 0.562 | 0.540 | 0.748 | 0.527 | 0.717 |
    0.511 |'
- en: '|  | PREMIA-SFT | 0.594 | 0.504 | 0.609 | 0.518 | 0.785 | 0.543 | 0.743 | 0.551
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-SFT | 0.594 | 0.504 | 0.609 | 0.518 | 0.785 | 0.543 | 0.743 | 0.551
    |'
- en: '|  Open-llama-7b  | PPL | 0.577 | 0.529 | 0.572 | 0.505 | 0.594 | 0.514 | 0.543
    | 0.551 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  Open-llama-7b  | PPL | 0.577 | 0.529 | 0.572 | 0.505 | 0.594 | 0.514 | 0.543
    | 0.551 |'
- en: '|  | Zlib | 0.599 | 0.559 | 0.593 | 0.525 | 0.577 | 0.501 | 0.521 | 0.500 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | Zlib | 0.599 | 0.559 | 0.593 | 0.525 | 0.577 | 0.501 | 0.521 | 0.500 |'
- en: '|  | Lowercase | 0.537 | 0.501 | 0.540 | 0.502 | 0.539 | 0.504 | 0.545 | 0.565
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | Lowercase | 0.537 | 0.501 | 0.540 | 0.502 | 0.539 | 0.504 | 0.545 | 0.565
    |'
- en: '|  | Ref | 0.583 | 0.515 | 0.586 | 0.503 | 0.625 | 0.507 | 0.561 | 0.582 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | Ref | 0.583 | 0.515 | 0.586 | 0.503 | 0.625 | 0.507 | 0.561 | 0.582 |'
- en: '|  | MIN-K | 0.597 | 0.527 | 0.583 | 0.511 | 0.605 | 0.527 | 0.607 | 0.536
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | MIN-K | 0.597 | 0.527 | 0.583 | 0.511 | 0.605 | 0.527 | 0.607 | 0.536
    |'
- en: '|  | PREMIA-base | 0.559 | 0.511 | 0.560 | 0.504 | 0.773 | 0.541 | 0.728 |
    0.510 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-base | 0.559 | 0.511 | 0.560 | 0.504 | 0.773 | 0.541 | 0.728 |
    0.510 |'
- en: '|  | PREMIA-SFT | 0.594 | 0.511 | 0.611 | 0.527 | 0.736 | 0.530 | 0.749 | 0.520
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | PREMIA-SFT | 0.594 | 0.511 | 0.611 | 0.527 | 0.736 | 0.530 | 0.749 | 0.520
    |'
- en: '![Refer to caption](img/9b625f14d14f705617499bf1632691f2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9b625f14d14f705617499bf1632691f2.png)'
- en: 'Figure 1: AUROC scores for $\text{MIA}_{\text{Pair}}$ detection for Mistral-7B,
    Open-llama-3b, and Open-llama-7b models.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: Mistral-7B、Open-llama-3b 和 Open-llama-7b 模型的 $\text{MIA}_{\text{Pair}}$
    检测 AUROC 分数。'
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4.4.1 Effectiveness of MIA Methodology ‣ 4.4 Results
    ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack on Preference
    Data for LLM Alignment") presents the AUROC scores for various MIA methods across
    Mistral-7B, Open-llama-3b, and Open-llama-7b models. PREMIA-base and PREMIA-SFT
    indicate using the base model or SFT model as the reference model respectively.
    Our method uniquely addresses the entire preference tuple and consistently achieves
    the highest AUROC scores, demonstrating superior data membership identification
    (see Figure [3](#S4.F3 "Figure 3 ‣ 4.4.4 Impact of Response Length on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") for paired tuple analysis). The comparison
    between DPO and PPO reveals DPO’s increased susceptibility to MIA, indicating
    that its enhancements in aligning models with human preferences might elevate
    privacy risks. We do not measure the entire tuple using baselines because traditional
    MIA methods are not designed to handle the relational and contextual dependencies
    inherent in preference data.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S4.T1 "表 1 ‣ 4.4.1 MIA 方法的有效性 ‣ 4.4 结果 ‣ 4 实验 ‣ 暴露隐私缺口：对 LLM 对齐的偏好数据进行成员推断攻击")
    展示了 Mistral-7B、Open-llama-3b 和 Open-llama-7b 模型中各种 MIA 方法的 AUROC 分数。PREMIA-base
    和 PREMIA-SFT 分别表示使用基本模型或 SFT 模型作为参考模型。我们的方法独特地处理了整个偏好元组，并始终实现了最高的 AUROC 分数，展示了优越的数据成员识别能力（见图 [3](#S4.F3
    "图 3 ‣ 4.4.4 响应长度对 MIA 有效性的影响 ‣ 4.4 结果 ‣ 4 实验 ‣ 暴露隐私缺口：对 LLM 对齐的偏好数据进行成员推断攻击")
    进行配对元组分析）。DPO 和 PPO 之间的比较揭示了 DPO 对 MIA 的更高敏感性，表明其在将模型对齐到人类偏好上的改进可能会增加隐私风险。我们没有使用基准来测量整个元组，因为传统的
    MIA 方法无法处理偏好数据中固有的关系和上下文依赖性。
- en: 4.4.2 Impact of Model Size on MIA Effectiveness
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 模型大小对 MIA 效用的影响
- en: 'Table 2: Performance of PREMIA-SFT on various GPT2 model variants across Stack-Exchange-Paired
    and IMDB-RLHF-PairTwo datasets.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: PREMIA-SFT 在 Stack-Exchange-Paired 和 IMDB-RLHF-PairTwo 数据集上对各种 GPT2 模型变体的性能。'
- en: '|  |  |  |  | $\text{MIA}_{\text{Pair}}$ |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | $\text{MIA}_{\text{Pair}}$ |  |'
- en: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DPO | PPO | DPO | PPO | DPO | PPO |'
- en: '|  Stack Exchange  | GPT2 | 0.815 | 0.520 | 0.770 | 0.500 | 0.909 | 0.523 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  Stack Exchange  | GPT2 | 0.815 | 0.520 | 0.770 | 0.500 | 0.909 | 0.523 |'
- en: '|  | GPT2-medium | 0.809 | 0.528 | 0.698 | 0.525 | 0.889 | 0.522 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-medium | 0.809 | 0.528 | 0.698 | 0.525 | 0.889 | 0.522 |'
- en: '|  | GPT2-large | 0.838 | 0.502 | 0.694 | 0.548 | 0.891 | 0.515 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-large | 0.838 | 0.502 | 0.694 | 0.548 | 0.891 | 0.515 |'
- en: '|  | GPT2-xl | 0.839 | 0.515 | 0.850 | 0.504 | 0.900 | 0.501 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-xl | 0.839 | 0.515 | 0.850 | 0.504 | 0.900 | 0.501 |'
- en: '|  | Open-llama-3b |  0.785 | 0.543 | 0.743 | 0.551 | 0.920 | 0.512 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | Open-llama-3b |  0.785 | 0.543 | 0.743 | 0.551 | 0.920 | 0.512 |'
- en: '|  | Open-llama-7b | 0.736 | 0.530 | 0.749 | 0.520 | 0.908 | 0.534 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | Open-llama-7b | 0.736 | 0.530 | 0.749 | 0.520 | 0.908 | 0.534 |'
- en: '|  | Mistral | 0.807 | 0.535 | 0.750 | 0.501 | 0.935 | 0.537 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | Mistral | 0.807 | 0.535 | 0.750 | 0.501 | 0.935 | 0.537 |'
- en: '|  IMDB  | GPT2 | 0.636 | 0.550 | 0.713 | 0.511 | 0.771 | 0.549 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  IMDB  | GPT2 | 0.636 | 0.550 | 0.713 | 0.511 | 0.771 | 0.549 |'
- en: '|  | GPT2-medium | 0.641 | 0.549 | 0.707 | 0.539 | 0.762 | 0.528 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-medium | 0.641 | 0.549 | 0.707 | 0.539 | 0.762 | 0.528 |'
- en: '|  | GPT2-large | 0.615 | 0.611 | 0.659 | 0.583 | 0.704 | 0.520 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-large | 0.615 | 0.611 | 0.659 | 0.583 | 0.704 | 0.520 |'
- en: '|  | GPT2-xl | 0.623 | 0.591 | 0.643 | 0.579 | 0.692 | 0.519 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT2-xl | 0.623 | 0.591 | 0.643 | 0.579 | 0.692 | 0.519 |'
- en: '|  | Open-llama-3b | 0.594 | 0.504 | 0.609 | 0.518 | 0.509 | 0.518 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | Open-llama-3b | 0.594 | 0.504 | 0.609 | 0.518 | 0.509 | 0.518 |'
- en: '|  | Open-llama-7b | 0.594 | 0.511 | 0.611 | 0.527 | 0.500 | 0.512 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | Open-llama-7b | 0.594 | 0.511 | 0.611 | 0.527 | 0.500 | 0.512 |'
- en: '|  | Mistral-7B | 0.572 | 0.507 | 0.611 | 0.527 | 0.556 | 0.537 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | Mistral-7B | 0.572 | 0.507 | 0.611 | 0.527 | 0.556 | 0.537 |'
- en: 'Table 3: Privacy vs Utility Trade-off analysis on the Mistral-7B model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: Mistral-7B 模型的隐私与效用权衡分析。'
- en: '|  | Base | SFT | PPO | DPO |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 基础 | SFT | PPO | DPO |'
- en: '| $\text{MIA}_{\text{Chosen}}$ | — | 0.53 | 0.54 | 0.80 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| $\text{MIA}_{\text{Chosen}}$ | — | 0.53 | 0.54 | 0.80 |'
- en: '| $\text{MIA}_{\text{Rejected}}$ | — | 0.61 | 0.50 | 0.75 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| $\text{MIA}_{\text{Rejected}}$ | — | 0.61 | 0.50 | 0.75 |'
- en: '| $\text{MIA}_{\text{Pair}}$ | — | 0.55 | 0.50 | 0.93 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| $\text{MIA}_{\text{Pair}}$ | — | 0.55 | 0.50 | 0.93 |'
- en: '| Reward$\uparrow$ | -1.922 | -1.953 | -0.771 | -1.035 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 奖励$\uparrow$ | -1.922 | -1.953 | -0.771 | -1.035 |'
- en: '| PPL$\downarrow$ | 11.148 | 7.673 | 11.671 | 14.991 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| PPL$\downarrow$ | 11.148 | 7.673 | 11.671 | 14.991 |'
- en: '| msttr-100$\uparrow$ | 0.673 | 0.651 | 0.633 | 0.640 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| msttr-100$\uparrow$ | 0.673 | 0.651 | 0.633 | 0.640 |'
- en: '| distinct 1$\uparrow$ | 0.180 | 0.127 | 0.085 | 0.123 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| distinct 1$\uparrow$ | 0.180 | 0.127 | 0.085 | 0.123 |'
- en: '| distinct 2$\uparrow$ | 0.631 | 0.521 | 0.422 | 0.520 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| distinct 2$\uparrow$ | 0.631 | 0.521 | 0.422 | 0.520 |'
- en: '| unique 1$\uparrow$ | 2010 | 3213 | 3530 | 3059 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| unique 1$\uparrow$ | 2010 | 3213 | 3530 | 3059 |'
- en: '| unique 2$\uparrow$ | 9507 | 17238 | 25205 | 18017 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| unique 2$\uparrow$ | 9507 | 17238 | 25205 | 18017 |'
- en: '| Bert Score$\uparrow$ | 0.876 | 0.879 | 0.883 | 0.877 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Bert Score$\uparrow$ | 0.876 | 0.879 | 0.883 | 0.877 |'
- en: '| ROUGE$\uparrow$ | 0.424 | 0.458 | 0.457 | 0.443 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE$\uparrow$ | 0.424 | 0.458 | 0.457 | 0.443 |'
- en: '| BLEU$\uparrow$ | 0.348 | 0.367 | 0.338 | 0.360 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| BLEU$\uparrow$ | 0.348 | 0.367 | 0.338 | 0.360 |'
- en: '| METEOR$\uparrow$ | 0.445 |  0.467 | 0.449 | 0.466 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| METEOR$\uparrow$ | 0.445 |  0.467 | 0.449 | 0.466 |'
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") details the PREMIA-SFT results for models
    of different sizes on the Stack-Exchange and IMDB datasets. On the Stack-Exchange
    dataset, large models typically have higher AUROC scores in all MIA scenarios,
    indicating that they retain more specific details of the training data. However,
    on the IMDB dataset, the Mistral-7B and Open-llama models have significantly worse
    MIA performance. One possible reason is that the task is too simple for them.
    As shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment"), Mistral-7B achieves over 90% accuracy
    in distinguishing between selected and rejected responses in only the first 0.2
    epoch. Large pre-trained models like Mistral-7B already have strong generalization
    capabilities, which undermines the effectiveness of MIA. Similarly, large GPT2
    models such as GPT2-xl show better generalization on simple tasks, making them
    less susceptible to MIA.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 4.4.2 模型大小对 MIA 有效性的影响 ‣ 4.4 结果 ‣ 4 实验 ‣ 暴露隐私漏洞：对 LLM 对齐的偏好数据进行成员推断攻击")
    详细介绍了不同大小模型在 Stack-Exchange 和 IMDB 数据集上的 PREMIA-SFT 结果。在 Stack-Exchange 数据集中，大型模型通常在所有
    MIA 场景下具有更高的 AUROC 分数，这表明它们保留了更多的训练数据特定细节。然而，在 IMDB 数据集中，Mistral-7B 和 Open-llama
    模型的 MIA 性能显著较差。一个可能的原因是任务对它们来说过于简单。如图 [2](#S4.F2 "图 2 ‣ 4.4.2 模型大小对 MIA 有效性的影响
    ‣ 4.4 结果 ‣ 4 实验 ‣ 暴露隐私漏洞：对 LLM 对齐的偏好数据进行成员推断攻击") 所示，Mistral-7B 在仅第 0.2 个 epoch
    中就能在选择和拒绝的响应之间区分出 90% 以上的准确率。像 Mistral-7B 这样的预训练大型模型已经具备很强的泛化能力，这削弱了 MIA 的有效性。同样，大型
    GPT2 模型如 GPT2-xl 在简单任务上表现出更好的泛化能力，使它们对 MIA 的抵抗力更强。
- en: '![Refer to caption](img/0877f02ac98829670a38a9a42a85dd11.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0877f02ac98829670a38a9a42a85dd11.png)'
- en: 'Figure 2: Train/Eval Aaccuracy for Mistral-7B on IMDB.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Mistral-7B 在 IMDB 上的训练/评估准确率。
- en: 4.4.3 Trade-Off between Performance and Privacy
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3 性能与隐私之间的权衡
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.4.2 Impact of Model Size on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") analyzes the trade-off between vulnerability
    to MIA and model utility under various Mistral-7B model configurations on the
    Stack Exchange dataset. The "Reward" row represents the average reward score given
    by the reward model for each of these models, indicating how well the task was
    accomplished. Clearly, DPO and PPO have better rewards compared to the rest. Further,
    DPO is clearly more vulnerable to MIA. However, DPO did not improve utility metrics
    such as reward and complexity. It is worth noting that PPO provides similar utility
    performance to DPO, but it has a lower AUROC. These findings are in line with
    existing research, which also shows that despite DPO being relatively straightforward
    to train, it does not improve the model performance compared to PPO Ivison et al.
    ([2024](#bib.bib13)); Xu et al. ([2024](#bib.bib29)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [3](#S4.T3 "表 3 ‣ 4.4.2 模型大小对 MIA 有效性的影响 ‣ 4.4 结果 ‣ 4 实验 ‣ 暴露隐私漏洞：对 LLM 对齐的偏好数据进行成员推断攻击")
    分析了在不同 Mistral-7B 模型配置下 MIA 脆弱性与模型效用之间的权衡，数据集为 Stack Exchange。 "Reward" 行表示奖励模型对每个模型给出的平均奖励分数，表明任务完成的好坏。显然，DPO
    和 PPO 的奖励比其他方法更好。此外，DPO 对 MIA 更加脆弱。然而，DPO 并没有提高奖励和复杂度等效用指标。值得注意的是，PPO 提供了与 DPO
    相似的效用性能，但 AUROC 较低。这些发现与现有研究一致，现有研究也表明，尽管 DPO 相对容易训练，但与 PPO 相比，模型性能并没有改善 Ivison
    等人 ([2024](#bib.bib13)); Xu 等人 ([2024](#bib.bib29))。
- en: 4.4.4 Impact of Response Length on MIA Effectiveness
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.4 响应长度对 MIA 有效性的影响
- en: 'In this experiment, we look the effect of length of examples used in preference
    alignment and their corresponding vulnerability in terms of AUC-ROC of PREMIA-SFT.
    Figure [3](#S4.F3 "Figure 3 ‣ 4.4.4 Impact of Response Length on MIA Effectiveness
    ‣ 4.4 Results ‣ 4 Experiments ‣ Exposing Privacy Gaps: Membership Inference Attack
    on Preference Data for LLM Alignment") shows the MIA AUROC results for the GPT-2
    family of models on the IMDB dataset. As can be seen from the figure, for "Chosen"
    responses, the longer the response, the more susceptible it is to MIA, while for
    "Rejected" responses, the opposite is true.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们观察了在偏好对齐中使用的示例长度及其在PREMIA-SFT中的AUC-ROC对应的脆弱性。图[3](#S4.F3 "图 3 ‣ 4.4.4
    响应长度对MIA效果的影响 ‣ 4.4 结果 ‣ 4 实验 ‣ 揭示隐私缺口：对偏好数据进行的成员身份推断攻击")展示了IMDB数据集上GPT-2系列模型的MIA
    AUROC结果。从图中可以看出，对于“选择”的响应，响应越长，对MIA的敏感性越高，而对于“拒绝”的响应，则正好相反。
- en: '![Refer to caption](img/1928d3e93fad7260dcd3354f520bfc43.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1928d3e93fad7260dcd3354f520bfc43.png)'
- en: 'Figure 3: AUROC vs Average Length for GPT-2 Models'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-2模型的AUROC与平均长度
- en: 5 Future Work
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 未来工作
- en: Our study shows that advanced privacy-preserving techniques are needed when
    using preference data for LLM alignment. Optimizing the privacy model architecture
    without losing performance is key. Techniques such as DP-SGD Abadi et al. ([2016](#bib.bib1)),
    model pruning Han et al. ([2015](#bib.bib10)), and knowledge distillation Hinton
    et al. ([2015](#bib.bib12)) should be evaluated. It is also necessary to create
    benchmarks and assessment frameworks for privacy risks in LLM alignment. These
    benchmarks and assessment frameworks should model realistic attacks and provide
    metrics for comparing privacy-preserving methods to ensure that LLMs are consistent
    with human values without compromising privacy.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，在使用偏好数据进行LLM对齐时，需要先进的隐私保护技术。优化隐私模型架构而不降低性能是关键。应评估如DP-SGD Abadi et al.
    ([2016](#bib.bib1))、模型剪枝 Han et al. ([2015](#bib.bib10)) 和知识蒸馏 Hinton et al. ([2015](#bib.bib12))
    等技术。还需要为LLM对齐中的隐私风险创建基准和评估框架。这些基准和评估框架应模拟现实攻击，并提供比较隐私保护方法的指标，以确保LLM符合人类价值观而不妨碍隐私。
- en: 6 Conclusion
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper examines the vulnerability of preference datasets in LLM alignment
    to MIAs. We reveal that models trained with DPO are more susceptible to MIAs than
    those using PPO, posing a significant privacy risk as preference data use increases.
    Our attack framework excels in detecting training data membership, stressing the
    need for robust privacy-preserving methods. Larger models enhance capabilities
    but increase privacy risks, highlighting the trade-off between performance and
    data security.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本文考察了LLM对齐中偏好数据集对MIA的脆弱性。我们揭示了使用DPO训练的模型比使用PPO的模型更容易受到MIA的影响，这在偏好数据使用增加时构成了显著的隐私风险。我们的攻击框架在检测训练数据成员身份方面表现出色，强调了需要强大的隐私保护方法。更大的模型增强了能力，但也增加了隐私风险，突显了性能与数据安全之间的权衡。
- en: 7 Limitations
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 局限性
- en: First, the analysis conducted in this study is limited to open-source LLMs and
    does not include proprietary or closed-source models such as ChatGPT. The privacy
    implications and vulnerability to MIA of these closed-source LLMs may differ because
    their training data, architecture, and alignment techniques are not fully transparent.
    Second, this study focuses on the privacy implications of two well-known alignment
    techniques (PPO and DPO). However, the field of LLM alignment is rapidly evolving,
    and the privacy risks associated with other alignment methods can be more fully
    analyzed in future work.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，本研究的分析仅限于开源LLM（大型语言模型），不包括像ChatGPT这样的专有或闭源模型。这些闭源LLM的隐私影响和对MIA（成员身份推断攻击）的脆弱性可能会有所不同，因为它们的训练数据、架构和对齐技术并不完全透明。其次，本研究集中于两种著名的对齐技术（PPO和DPO）的隐私影响。然而，LLM对齐领域正在快速发展，未来的研究可以对其他对齐方法的隐私风险进行更全面的分析。
- en: References
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan,
    Ilya Mironov, Kunal Talwar, and Li Zhang. 2016. Deep learning with differential
    privacy. In *Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications
    Security*, pages 308–318\. ACM.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abadi et al. (2016) Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan,
    Ilya Mironov, Kunal Talwar, 和 Li Zhang. 2016. 深度学习与差分隐私。在*2016年ACM SIGSAC计算机与通信安全会议论文集*，第308–318页。ACM.
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    2022. Training a helpful and harmless assistant with reinforcement learning from
    human feedback. *arXiv preprint arXiv:2204.05862*.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等（2022）Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, 等. 2022.
    使用来自人类反馈的强化学习训练有用且无害的助手。*arXiv 预印本 arXiv:2204.05862*。
- en: 'Banerjee and Lavie (2005) Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
    An automatic metric for mt evaluation with improved correlation with human judgments.
    In *Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures
    for machine translation and/or summarization*, pages 65–72.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banerjee 和 Lavie（2005）Satanjeev Banerjee 和 Alon Lavie. 2005. Meteor：一种自动化的机器翻译评估指标，与人工评判的相关性更高。收录于
    *ACL 机器翻译与/或总结评估的内在与外在评估措施研讨会论文集*，第65–72页。
- en: Carlini et al. (2020) Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn
    Song, Úlfar Erlingsson, et al. 2020. Extracting training data from large language
    models. corr abs/2012.07805 (2020). *arXiv preprint arXiv:2012.07805*.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等（2020）Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski,
    Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Úlfar
    Erlingsson, 等. 2020. 从大型语言模型中提取训练数据。corr abs/2012.07805 (2020)。*arXiv 预印本 arXiv:2012.07805*。
- en: Carlini et al. (2021) Nicholas Carlini et al. 2021. Extracting training data
    from large language models. In *30th USENIX Security Symposium (USENIX Security
    21)*.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等（2021）Nicholas Carlini 等. 2021. 从大型语言模型中提取训练数据。收录于 *第30届 USENIX 安全研讨会
    (USENIX Security 21)*。
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. 2017. Deep reinforcement learning from human preferences.
    *Advances in neural information processing systems*, 30.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Christiano 等（2017）Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane
    Legg 和 Dario Amodei. 2017. 从人类偏好中进行深度强化学习。*神经信息处理系统进展*，30。
- en: 'Computer (2023) Together Computer. 2023. [Redpajama-data: An open source recipe
    to reproduce llama training dataset](https://github.com/togethercomputer/RedPajama-Data).'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Computer（2023）Together Computer. 2023. [Redpajama-data：一种开源配方以重现 llama 训练数据集](https://github.com/togethercomputer/RedPajama-Data)。
- en: Fu et al. (2023) Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li,
    and Tao Jiang. 2023. Practical membership inference attacks against fine-tuned
    large language models via self-prompt calibration. *arXiv preprint arXiv:2311.06062*.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li 和 Tao Jiang.
    2023. 针对微调的大型语言模型的实用成员推断攻击，通过自我提示校准。*arXiv 预印本 arXiv:2311.06062*。
- en: 'Geng and Liu (2023) Xinyang Geng and Hao Liu. 2023. [Openllama: An open reproduction
    of llama](https://github.com/openlm-research/open_llama).'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng 和 Liu（2023）Xinyang Geng 和 Hao Liu. 2023. [Openllama：对 llama 的开放再现](https://github.com/openlm-research/open_llama)。
- en: 'Han et al. (2015) Song Han, Huizi Mao, and William J Dally. 2015. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等（2015）Song Han, Huizi Mao 和 William J Dally. 2015. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Nicholas Carlini, John Schulman, and
    Jacob Steinhardt. 2021. Unsolved problems in ml safety. *arXiv preprint arXiv:2109.13916*.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021）Dan Hendrycks, Nicholas Carlini, John Schulman 和 Jacob Steinhardt.
    2021. 机器学习安全中的未解问题。*arXiv 预印本 arXiv:2109.13916*。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling
    the knowledge in a neural network. In *NIPS Deep Learning and Representation Learning
    Workshop*.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton 等（2015）Geoffrey Hinton, Oriol Vinyals 和 Jeff Dean. 2015. 从神经网络中提炼知识。收录于
    *NIPS 深度学习与表示学习研讨会*。
- en: 'Ivison et al. (2024) Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina
    Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi, and Hannaneh Hajishirzi. 2024.
    [Unpacking dpo and ppo: Disentangling best practices for learning from preference
    feedback](https://arxiv.org/abs/2406.09279). *Preprint*, arXiv:2406.09279.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ivison 等（2024）Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina
    Pyatkin, Nathan Lambert, Noah A. Smith, Yejin Choi 和 Hannaneh Hajishirzi. 2024.
    [解开 dpo 和 ppo 的秘密：剖析从偏好反馈中学习的最佳实践](https://arxiv.org/abs/2406.09279)。*预印本*，arXiv:2406.09279。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier, 等. 2023. Mistral 7b。*arXiv 预印本 arXiv:2310.06825*。
- en: 'Johnson (1944) Wendell Johnson. 1944. Studies in language behavior: A program
    of research. *Psychological Monographs*, 56(2):1–15.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson (1944) Wendell Johnson. 1944. 语言行为研究：一项研究计划。*心理学专著*, 56(2):1–15。
- en: Li et al. (2015) Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and
    Bill Dolan. 2015. A diversity-promoting objective function for neural conversation
    models. *arXiv preprint arXiv:1510.03055*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2015) Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, 和 Bill
    Dolan. 2015. 用于神经对话模型的多样性促进目标函数。*arXiv 预印本 arXiv:1510.03055*。
- en: 'Lin (2004) Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of
    summaries. In *Text summarization branches out*, pages 74–81.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin (2004) Chin-Yew Lin. 2004. Rouge：自动评估摘要的工具包。见于*文本摘要的扩展*，页码 74–81。
- en: Nasr et al. (2018) Milad Nasr, Reza Shokri, and Amir Houmansadr. 2018. Comprehensive
    privacy analysis of deep learning. In *Proceedings of the 2019 IEEE Symposium
    on Security and Privacy (SP)*, volume 2018, pages 1–15.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nasr et al. (2018) Milad Nasr, Reza Shokri, 和 Amir Houmansadr. 2018. 深度学习的全面隐私分析。见于*2019
    IEEE 安全与隐私研讨会 (SP) 会议记录*，卷 2018，页码 1–15。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730–27744.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等. 2022. 训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*, 35:27730–27744。
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. 2002. Bleu：一种自动评估机器翻译的方法。见于*第40届计算语言学协会年会会议记录*，页码 311–318。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等. 2019. 语言模型是无监督的多任务学习者。*OpenAI 博客*, 1(8):9。
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D
    Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization:
    Your language model is secretly a reward model. *Advances in Neural Information
    Processing Systems*, 36.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher
    D Manning, Stefano Ermon, 和 Chelsea Finn. 2024. 直接偏好优化：你的语言模型实际上是一个奖励模型。*神经信息处理系统进展*,
    36。
- en: 'Ramamurthy et al. (2022) Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté
    Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and
    Yejin Choi. 2022. Is reinforcement learning (not) for natural language processing:
    Benchmarks, baselines, and building blocks for natural language policy optimization.
    *arXiv preprint arXiv:2210.01241*.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramamurthy et al. (2022) Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté
    Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, 和
    Yejin Choi. 2022. 强化学习是否适用于自然语言处理：自然语言策略优化的基准、基线和构建块。*arXiv 预印本 arXiv:2210.01241*。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. *arXiv
    preprint arXiv:1707.06347*.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, 和 Oleg Klimov. 2017. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*。
- en: Shi et al. (2024) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao
    Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. 2024. [Detecting pretraining
    data from large language models](https://openreview.net/forum?id=zWqr3MQuNs).
    In *The Twelfth International Conference on Learning Representations*.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2024) Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao
    Liu, Terra Blevins, Danqi Chen, 和 Luke Zettlemoyer. 2024. [从大型语言模型中检测预训练数据](https://openreview.net/forum?id=zWqr3MQuNs)。见于*第十二届国际学习表征会议*。
- en: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly
    Shmatikov. 2017. Membership inference attacks against machine learning models.
    In *2017 IEEE symposium on security and privacy (SP)*, pages 3–18\. IEEE.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shokri et al. (2017) Reza Shokri, Marco Stronati, Congzheng Song, 和 Vitaly Shmatikov.
    2017. 针对机器学习模型的会员推断攻击。见于*2017 IEEE 安全与隐私研讨会 (SP)*，页码 3–18。IEEE。
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.
    Learning to summarize with human feedback. *Advances in Neural Information Processing
    Systems*, 33:3008–3021.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon 等（2020）Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan
    Lowe, Chelsea Voss, Alec Radford, Dario Amodei, 和 Paul F Christiano. 2020. 学习通过人类反馈进行总结。*神经信息处理系统进展*，33:3008–3021。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等。2023. Llama：开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*。
- en: Xu et al. (2024) Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu
    Mei, Guangju Wang, Chao Yu, and Yi Wu. 2024. Is dpo superior to ppo for llm alignment?
    a comprehensive study. *arXiv preprint arXiv:2404.10719*.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2024）Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei,
    Guangju Wang, Chao Yu, 和 Yi Wu. 2024. dpo 是否优于 ppo 用于 llm 对齐？一项全面研究。*arXiv 预印本
    arXiv:2404.10719*。
- en: 'Yang et al. (2024) Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han,
    Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, and Xia Hu. 2024. Harnessing
    the power of llms in practice: A survey on chatgpt and beyond. *ACM Transactions
    on Knowledge Discovery from Data*, 18(6):1–32.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2024）Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang
    Feng, Haoming Jiang, Shaochen Zhong, Bing Yin, 和 Xia Hu. 2024. 在实践中利用 llms 的力量：关于
    chatgpt 及其他的调查。*ACM 数据知识发现交易*，18(6):1–32。
- en: 'Yeom et al. (2018) Samuel Yeom et al. 2018. Privacy risk in machine learning:
    Analyzing the connection to overfitting. *IEEE 31st Computer Security Foundations
    Symposium (CSF)*.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeom 等（2018）Samuel Yeom 等。2018. 机器学习中的隐私风险：分析与过拟合的关系。*IEEE 第 31 届计算机安全基础研讨会（CSF）*。
- en: 'Zhang et al. (2019) Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. *arXiv
    preprint arXiv:1904.09675*.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019）Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, 和
    Yoav Artzi. 2019. Bertscore：使用 bert 评估文本生成。*arXiv 预印本 arXiv:1904.09675*。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023）Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等。2023. 大型语言模型综述。*arXiv
    预印本 arXiv:2303.18223*。
- en: Appendix A Implementation Details
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实现细节
- en: We mainly refer to the TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)
    package for implementation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要参考 TRL⁴⁴4[https://huggingface.co/docs/trl/en/index](https://huggingface.co/docs/trl/en/index)
    包来实现。
- en: 'LoRA Setting. For all experiments, we share the same LoRA setting below, using
    the PEFT⁵⁵5[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
    package: lora_alpha 32, lora_dropout 0.05, lora_r 16, and no bias term.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 设置。所有实验中，我们共享以下相同的 LoRA 设置，使用 PEFT⁵⁵5[https://huggingface.co/docs/peft/index](https://huggingface.co/docs/peft/index)
    包：lora_alpha 32，lora_dropout 0.05，lora_r 16，并且没有偏置项。
- en: Quantization Setting. For all experiments, we use the BitsAndBytes⁶⁶6[https://huggingface.co/docs/bitsandbytes/index](https://huggingface.co/docs/bitsandbytes/index)
    package for 4-bit quantization.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 量化设置。所有实验中，我们使用 BitsAndBytes⁶⁶6[https://huggingface.co/docs/bitsandbytes/index](https://huggingface.co/docs/bitsandbytes/index)
    包进行 4 位量化。
- en: 'SFT Setting. The settings for SFT are detailed below. We utilized the "train/rl"
    split of the stack-exchange-paired dataset, selecting 80,000 data points for the
    fine-tuning process, same data is used for PPO and DPO training. The prompt and
    only the preferred response are concatenated as input. The specific training parameters
    are:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: SFT 设置。SFT 的设置详述如下。我们使用了 stack-exchange-paired 数据集的“train/rl”分割，选择了 80,000 个数据点用于微调过程，相同的数据用于
    PPO 和 DPO 训练。提示和仅优选响应被串联为输入。具体的训练参数为：
- en: '- Training Epochs: 2.0'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '- 训练周期：2.0'
- en: '- Learning Rate: 8e-5'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习率：8e-5'
- en: '- Batch Size (Training): 4'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '- 批量大小（训练）：4'
- en: '- Batch Size (Evaluation): 2'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '- 批量大小（评估）：2'
- en: '- Gradient Accumulation Steps: 4'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '- 梯度累积步骤：4'
- en: '- Learning Rate Scheduler: cosine'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习率调度器：余弦'
- en: '- Warmup Steps: 100'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '- 热身步骤：100'
- en: '- Weight Decay: 0.05'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '- 权重衰减：0.05'
- en: '- Optimizer: paged_adamw_32bit'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '- 优化器：paged_adamw_32bit'
- en: '- Mixed Precision Training: fp16'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '- 混合精度训练：fp16'
- en: 'PPO Setting. The settings for PPO are detailed below. We filter out data points
    with maximum length constraints. We also limit the maximum length of the generated
    response. The specific training parameters are:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: PPO设置。PPO的设置如下所述。我们过滤掉具有最大长度约束的数据点，并限制生成响应的最大长度。具体训练参数为：
- en: '- Batch Size: 16'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '- 批量大小：16'
- en: '- Mini Batch Size: 4'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '- 小批量大小：4'
- en: '- Gradient Accumulation Steps: 4'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '- 梯度累积步数：4'
- en: '- PPO Epochs: 6'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '- PPO轮次：6'
- en: '- Learning Rate: 5.4e-5'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习率：5.4e-5'
- en: '- KL Coefficient: 0.1'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '- KL系数：0.1'
- en: '- Adaptive KL Control: True'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '- 自适应KL控制：True'
- en: '- Target KL: 5.0'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '- 目标KL值：5.0'
- en: '- Horizon: 4000'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '- 时域：4000'
- en: '- Training Epochs: 4'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '- 训练轮次：4'
- en: '- Maximum Output Length: 128'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最大输出长度：128'
- en: '- Maximum Prompt Length: 256'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最大提示长度：256'
- en: '- Maximum Sequence Length: 1024'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最大序列长度：1024'
- en: 'DPO Setting. The settings for DPO training are detailed below. The specific
    training parameters are:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: DPO设置。DPO训练的设置如下所述。具体训练参数为：
- en: '- Batch Size (Training): 8'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '- 批量大小（训练）：8'
- en: '- Batch Size (Evaluation): 2'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '- 批量大小（评估）：2'
- en: '- Gradient Accumulation Steps: 2'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '- 梯度累积步数：2'
- en: '- Training Epochs: 3.0'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '- 训练轮次：3.0'
- en: '- Learning Rate: 5e-4'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '- 学习率：5e-4'
- en: '- Warmup Steps: 100'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '- 预热步数：100'
- en: '- Maximum Sequence Length: 1024'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最大序列长度：1024'
- en: '- Maximum Prompt Length: 256'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '- 最大提示长度：256'
- en: '- Optimizer Type: paged_adamw_32bit'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '- 优化器类型：paged_adamw_32bit'
- en: '- Beta: 0.4'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '- 贝塔值：0.4'
