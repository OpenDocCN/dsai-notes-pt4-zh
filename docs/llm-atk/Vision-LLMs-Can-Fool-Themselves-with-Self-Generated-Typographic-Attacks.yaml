- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:46:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉-LLMs 可以通过自生成的排版攻击自我欺骗
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.00626](https://ar5iv.labs.arxiv.org/html/2402.00626)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.00626](https://ar5iv.labs.arxiv.org/html/2402.00626)
- en: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer
- en: Boston University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 波士顿大学
- en: '{mqraitem, nimzia, piotrt, saenko, bplum}@bu.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{mqraitem, nimzia, piotrt, saenko, bplum}@bu.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Typographic Attacks, which involve pasting misleading text onto an image, were
    noted to harm the performance of Vision-Language Models like CLIP. However, the
    susceptibility of recent Large Vision-Language Models to these attacks remains
    understudied. Furthermore, prior work’s Typographic attacks against CLIP randomly
    sample a misleading class from a predefined set of categories. However, this simple
    strategy misses more effective attacks that exploit LVLM(s) stronger language
    skills. To address these issues, we first introduce a benchmark for testing Typographic
    attacks against LVLM(s). Moreover, we introduce two novel and more effective Self-Generated
    attacks which prompt the LVLM to generate an attack against itself: 1) Class Based
    Attack where the LVLM (*e.g*. LLaVA) is asked which deceiving class is most similar
    to the target class and 2) Descriptive Attacks where a more advanced LVLM (*e.g*.
    GPT4-V) is asked to recommend a Typographic attack that includes both a deceiving
    class and description. Using our benchmark, we uncover that Self-Generated attacks
    pose a significant threat, reducing LVLM(s) classification performance by up to
    33%. We also uncover that attacks generated by one model (*e.g*. GPT-4V or LLaVA)
    are effective against the model itself and other models like InstructBLIP and
    MiniGPT4\. Code: [https://github.com/mqraitem/Self-Gen-Typo-Attack](https://github.com/mqraitem/Self-Gen-Typo-Attack)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 排版攻击，即将误导性文本粘贴到图像上，已被注意到对 CLIP 等视觉语言模型的性能造成伤害。然而，近期大型视觉语言模型对这些攻击的敏感性仍然未被充分研究。此外，以前的工作中的排版攻击针对
    CLIP 是从预定义类别集中随机选择一个误导性类别。然而，这种简单的策略忽略了利用 LVLM(s) 更强语言技能的更有效攻击。为了解决这些问题，我们首先引入了一个针对
    LVLM(s) 的排版攻击测试基准。此外，我们介绍了两种新颖且更有效的自生成攻击，这些攻击促使 LVLM 对自身发起攻击：1）基于类别的攻击，其中 LVLM
    (*例如* LLaVA) 被要求找出哪个误导性类别与目标类别最相似；2）描述性攻击，其中更高级的 LVLM (*例如* GPT4-V) 被要求推荐包含误导性类别和描述的排版攻击。使用我们的基准，我们发现自生成攻击构成了重大威胁，使
    LVLM(s) 的分类性能降低了多达 33%。我们还发现，由一个模型 (*例如* GPT-4V 或 LLaVA) 生成的攻击对该模型自身及其他模型如 InstructBLIP
    和 MiniGPT4 都有效。代码：[https://github.com/mqraitem/Self-Gen-Typo-Attack](https://github.com/mqraitem/Self-Gen-Typo-Attack)
- en: Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉-LLMs 可以通过自生成的排版攻击自我欺骗
- en: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer Boston
    University {mqraitem, nimzia, piotrt, saenko, bplum}@bu.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Maan Qraitem, Nazia Tasnim, Piotr Teterwak, Kate Saenko, Bryan A. Plummer 波士顿大学
    {mqraitem, nimzia, piotrt, saenko, bplum}@bu.edu
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Typographic attacks mislead a vision and language model by superimposing deceptive
    text on an image. The attacks exploit the model’s reliance on textual cues to
    interpret the visual content. For example, prior work Azuma and Matsui ([2023](#bib.bib1))
    found that Typographic attacks significantly degraded CLIP accuracy on several
    classification datasets. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks") (a),
    the attacks involved simply pasting the text of a randomly chosen class different
    from the image’s ground truth.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 排版攻击通过在图像上叠加误导性文本来误导视觉和语言模型。这些攻击利用了模型对文本线索解读视觉内容的依赖。例如，以前的研究 Azuma 和 Matsui
    ([2023](#bib.bib1)) 发现，排版攻击显著降低了 CLIP 在多个分类数据集上的准确性。如图 [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") (a) 所示，这些攻击仅涉及将随机选择的与图像真实情况不同的文本粘贴上去。
- en: '![Refer to caption](img/4d2c5535ce17b99860aafc6ef1d9ae5f.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4d2c5535ce17b99860aafc6ef1d9ae5f.png)'
- en: 'Figure 1: Typographic Attack Comaprison. (a) Prior work’s typographic attacks
    (which were designed for CLIP) randomly samples a deceiving class from the dataset’s
    categories to attack the Large Vision Language Model (LVLM) Azuma and Matsui ([2023](#bib.bib1)).
    (b) Shows our more effective Self-Generated attack which uses the LVLM itself
    to generate the attack.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：排版攻击比较。 (a) 先前工作的排版攻击（设计用于CLIP）从数据集的类别中随机抽取一个欺骗性类别来攻击大型视觉语言模型（LVLM） Azuma
    和 Matsui ([2023](#bib.bib1))。 (b) 展示了我们更有效的自生成攻击，它使用LVLM自身生成攻击。
- en: '![Refer to caption](img/a65c110b5cf65abd8bafd2d804c64007.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a65c110b5cf65abd8bafd2d804c64007.png)'
- en: 'Figure 2: Self-Generated Attacks Comparison. Overview of the two types of our
    Self-Generated Attacks: (a) Class Based and (b) Descriptive Attacks. Refer to
    Section [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against
    Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") for discussion.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自生成攻击比较。我们两种自生成攻击的概述：(a) 基于类别的攻击和 (b) 描述性攻击。有关讨论，请参见第[2.2节](#S2.SS2 "2.2
    自生成攻击 ‣ 2 对大型视觉语言模型的排版攻击 ‣ 视觉-LLMs可以通过自生成排版攻击自我欺骗")。
- en: 'Since the introduction of CLIP Radford et al. ([2021](#bib.bib13)), the field
    has made rapid advances on a new class of vision-language systems: Large Vision
    Language Models (LVLMs) Liu et al. ([2023b](#bib.bib7)); Zhu et al. ([2023](#bib.bib16));
    Dai et al. ([2023](#bib.bib3)); Yang et al. ([2023](#bib.bib15)) which rely on
    strong language models with rich language understanding. This property enables
    a user-friendly and more accessible language interface to interact with the model.
    With that, Typographic Attacks derailing the model’s understanding of the image’s
    visual content represent an urgent threat. This is further relevant as prior work
    showed how LVLM(s) possess Optical Recognition capabilities (OCR) Liu et al. ([2023d](#bib.bib9))
    and, therefore, textual information in the image could impact their predictions.
    For example, pasting a relevant sentence to an unsafe query on an image could
    bypass the LVLM safety mechanisms Liu et al. ([2023c](#bib.bib8)) and modifying
    the text in an image could influence the model’s understanding of the image Liu
    et al. ([2023a](#bib.bib6)). However, a comprehensive study of algorithmic Typographic
    attacks against LVLM(s) where we paste inaccurate text that contradicts the image’s
    content is notably missing.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自从CLIP的引入 Radford et al. ([2021](#bib.bib13))，这一领域在一种新的视觉语言系统类别上取得了快速进展：大型视觉语言模型（LVLMs）
    Liu et al. ([2023b](#bib.bib7)); Zhu et al. ([2023](#bib.bib16)); Dai et al. ([2023](#bib.bib3));
    Yang et al. ([2023](#bib.bib15))，这些模型依赖于具有丰富语言理解的强大语言模型。这一特性使得与模型进行交互的语言接口变得更加友好和易于访问。因此，排版攻击使模型对图像视觉内容的理解偏离正轨，代表了一个紧迫的威胁。这一点尤为重要，因为之前的研究表明LVLM具有光学字符识别能力（OCR）
    Liu et al. ([2023d](#bib.bib9))，因此，图像中的文本信息可能会影响它们的预测。例如，将相关句子粘贴到图像上的不安全查询中可能绕过LVLM的安全机制
    Liu et al. ([2023c](#bib.bib8))，而修改图像中的文本可能会影响模型对图像的理解 Liu et al. ([2023a](#bib.bib6))。然而，对算法性排版攻击的全面研究，特别是将不准确的文本粘贴到与图像内容矛盾的情况，显然是缺失的。
- en: To address these issues, we revisit Typographic attacks for LVLM(s). We develop
    a benchmark that uses five datasets and find that Typographic attacks could reduce
    LVLM(s) classification accuracy by up to , outperforming Random Class by .
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们重新审视了针对大型视觉语言模型（LVLM）的排版攻击。我们开发了一个基准，使用了五个数据集，发现排版攻击可以将LVLM的分类准确率降低最多，表现超过了随机类别。
- en: 'Our contributions can be summarized:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结为：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a diverse and comprehensive Typographic attack benchmark for Large
    Vision Language Models LVLM(s).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一个多样化和全面的排版攻击基准，针对大型视觉语言模型LVLM(s)。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate that our novel Self-Generated Typographic attacks could reduce
    LVLM(s) classification performance by up to $33\%$.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了我们新颖的自生成排版攻击可以将LVLM的分类性能降低最多$33\%$。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We demonstrate how Self-Generated attacks by one model could generalize to other
    models.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了一个模型生成的攻击如何能够推广到其他模型。
- en: 2 Typographic Attacks Against Large Vision Language Models
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 对大型视觉语言模型的排版攻击
- en: Assume we are given a dataset . Now, given an image target pair  Azuma and Matsui
    ([2023](#bib.bib1)) takes in the pair and produces an image  pasted on it, *i.e*.
    . The attack is successful when the model uses the textual information of , hence
    mispredicting . In this paper, we revisit typographic attacks for Large Vision
    Language Models Liu et al. ([2023b](#bib.bib7)); Zhu et al. ([2023](#bib.bib16));
    Dai et al. ([2023](#bib.bib3)); Yang et al. ([2023](#bib.bib15)). We introduce
    a typographic attacks benchmark suited for Large Vision Language Models in Section
    [2.1](#S2.SS1 "2.1 Typographic Attacks Benchmark ‣ 2 Typographic Attacks Against
    Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") and then describe our new Self-Generated typographic attacks
    in Section [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against
    Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks").
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据集。现在，给定一个图像目标对 Azuma 和 Matsui ([2023](#bib.bib1)) 处理该对并生成一个粘贴在其上的图像，即。当模型使用文本信息时攻击是成功的，因此错误预测。在本文中，我们重新审视了大规模视觉语言模型的排版攻击
    Liu 等人 ([2023b](#bib.bib7))；Zhu 等人 ([2023](#bib.bib16))；Dai 等人 ([2023](#bib.bib3))；Yang
    等人 ([2023](#bib.bib15))。我们在第 [2.1](#S2.SS1 "2.1 Typographic Attacks Benchmark
    ‣ 2 Typographic Attacks Against Large Vision Language Models ‣ Vision-LLMs Can
    Fool Themselves with Self-Generated Typographic Attacks") 节介绍了适用于大型视觉语言模型的排版攻击基准，然后在第
    [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against Large
    Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") 节描述了我们的新自生成排版攻击。
- en: 2.1 Typographic Attacks Benchmark
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 排版攻击基准
- en: '|  | No Text | Random Class  | Descriptive (ours) $\downarrow$ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类 | 描述性（我们的）$\downarrow$ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | VE | LLM | LVLM | LLM | LVLM |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | VE | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 72.7 | 66.0 | 38.9 | 57.8 | 50.9 | 58.1 | 31.8 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 72.7 | 66.0 | 38.9 | 57.8 | 50.9 | 58.1 | 31.8 |'
- en: '| LLaVA 1.5 | 50.8 | 27.3 | 18.3 | 18.2 | 13.2 | 11.5 | 9.9 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 50.8 | 27.3 | 18.3 | 18.2 | 13.2 | 11.5 | 9.9 |'
- en: '| InstructBlip | 60.2 | 26.8 | 20.6 | 23.0 | 22.2 | 13.9 | 14.9 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 60.2 | 26.8 | 20.6 | 23.0 | 22.2 | 13.9 | 14.9 |'
- en: '| MiniGPT4-2 | 27.7 | 25.6 | 25.7 | 24.6 | 25.3 | 23.7 | 22.4 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 27.7 | 25.6 | 25.7 | 24.6 | 25.3 | 23.7 | 22.4 |'
- en: '| Avg | 52.9 | 36.4 | 25.9 | 30.9 | 27.9 | 26.8 | 19.7 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| Avg | 52.9 | 36.4 | 25.9 | 30.9 | 27.9 | 26.8 | 19.7 |'
- en: 'Table 1: Comparison between the effect of typo attacks: Random Class Azuma
    and Matsui ([2023](#bib.bib1)) and our Self-Generated Attacks) on Large Vision
    Lanugage Models: GPT-4V Yang et al. ([2023](#bib.bib15)), LLaVA 1.5 Liu et al.
    ([2023b](#bib.bib7)), MiniGPT4-2 Zhu et al. ([2023](#bib.bib16)), and InstructBLIP
    Dai et al. ([2023](#bib.bib3)). Refer to Section [3](#S3 "3 Experiments ‣ Vision-LLMs
    Can Fool Themselves with Self-Generated Typographic Attacks") for further discussion.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：排版攻击效果的比较：随机类 Azuma 和 Matsui ([2023](#bib.bib1)) 和我们的自生成攻击）对大型视觉语言模型的影响：GPT-4V
    Yang 等人 ([2023](#bib.bib15))，LLaVA 1.5 Liu 等人 ([2023b](#bib.bib7))，MiniGPT4-2
    Zhu 等人 ([2023](#bib.bib16))，以及 InstructBLIP Dai 等人 ([2023](#bib.bib3))。更多讨论请参见第
    [3](#S3 "3 Experiments ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") 节。
- en: In this section, we develop a benchmark for testing typographic attacks for
    Large Vision Language Models (LVLMs). Radford et al. ([2021](#bib.bib13)); Azuma
    and Matsui ([2023](#bib.bib1)) tested these attacks on CLIP Radford et al. ([2021](#bib.bib13)),
    an image-text similarity network, where the scores of the correct and wrong classes
    are compared. In this work, we are concerned with LVLM(s) Liu et al. ([2023b](#bib.bib7));
    Zhu et al. ([2023](#bib.bib16)); Dai et al. ([2023](#bib.bib3)); Yang et al. ([2023](#bib.bib15))
    which, unlike CLIP, are image-to-text models capable of instruction following.
    Therefore, inspired by recent work on evaluating LVLM(s) Fu et al. ([2023](#bib.bib4));
    Xu et al. ([2023](#bib.bib14)), we propose the following benchmark.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们为测试大规模视觉语言模型（LVLMs）的排版攻击开发了一个基准。Radford 等人 ([2021](#bib.bib13))；Azuma
    和 Matsui ([2023](#bib.bib1)) 在 CLIP Radford 等人 ([2021](#bib.bib13)) 上测试了这些攻击，该网络用于图像-文本相似性比较，通过比较正确类和错误类的分数来评估。在这项工作中，我们关注
    LVLM(s) Liu 等人 ([2023b](#bib.bib7))；Zhu 等人 ([2023](#bib.bib16))；Dai 等人 ([2023](#bib.bib3))；Yang
    等人 ([2023](#bib.bib15))，这些模型与 CLIP 不同，它们是能够执行指令的图像到文本模型。因此，受近期对 LVLM(s) 评估工作的启发
    Fu 等人 ([2023](#bib.bib4))；Xu 等人 ([2023](#bib.bib14))，我们提出了以下基准。
- en: Assume we are given a set of typographic attack algorithms , and produces the
    deceiving target-image pair  among the set of choices , *i.e*.  and two algorithms
    result in attacks that contain , then we instruct the model to choose between
    $\{Jeep,Audi,Fiat\}$. We shuffle the ordering of each option in the question prompt
    to avoid model bias to any answer order. Refer to Appendix [D](#A4 "Appendix D
    Evaluation Question ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") for details on the question prompt.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们给定了一组排版攻击算法，并在这些选择中生成了欺骗性目标图像对，即如果两个算法产生的攻击包含，则我们指示模型在 $\{Jeep,Audi,Fiat\}$
    之间进行选择。我们打乱每个选项在问题提示中的顺序，以避免模型对任何答案顺序的偏见。有关问题提示的详细信息，请参阅附录 [D](#A4 "附录 D 评估问题
    ‣ Vision-LLMs 能够通过自生成排版攻击自我欺骗")。
- en: '![Refer to caption](img/d7dd7ccb764e7ca12a85ff03b5a6a522.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/d7dd7ccb764e7ca12a85ff03b5a6a522.png)'
- en: 'Figure 3: Comparing the effect of descriptions produced by the Recommended
    Attacks on performance. Refer to Section [3.1](#S3.SS1 "3.1 Typographic Attack
    Results ‣ 3 Experiments ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") for further discussion.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：比较推荐攻击产生的描述对性能的影响。有关进一步讨论，请参阅第 [3.1](#S3.SS1 "3.1 排版攻击结果 ‣ 3 实验 ‣ Vision-LLMs
    能够通过自生成排版攻击自我欺骗") 节。
- en: Finally, Azuma and Matsui ([2023](#bib.bib1)) paste the attack at a random location
    on the image. However, this might occlude important visual cues (*e.g*. a car
    logo when predicting the car model). To avoid this issue, we add a white space
    at the bottom and top of the image to allow for textual attacks. Refer to Appendix
    [E](#A5 "Appendix E Image White Space for Typographic Attacks ‣ Vision-LLMs Can
    Fool Themselves with Self-Generated Typographic Attacks") for an illustrative
    example.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，Azuma 和 Matsui ([2023](#bib.bib1)) 将攻击粘贴到图像的随机位置。然而，这可能会遮挡重要的视觉提示（*例如*，在预测汽车型号时的汽车标志）。为避免此问题，我们在图像的底部和顶部添加了白色空间，以允许文本攻击。有关说明示例，请参阅附录
    [E](#A5 "附录 E 图像白色空间用于排版攻击 ‣ Vision-LLMs 能够通过自生成排版攻击自我欺骗")。
- en: 2.2 Self-Generated Attacks
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 自生成攻击
- en: 'Azuma and Matsui ([2023](#bib.bib1)) generated an attack by pasting a random
    deceiving class from a predefined set of categories. For example, if the image
    represents a golden retriever, then the attack pastes the text of a randomly chosen
    dog breed, such as hound. However, hound might not be the breed that LVLM is most
    likely to confuse with golden retriever. One way to mitigate this issue is to
    examine the other classes in a dataset that a model might confuse with golden
    retriever. However, this relies on human intuition, which can vary between people,
    and a person may not have a good idea of what a model may find confusing. For
    example, a person may not consider a computer desk and a monitor to be similar,
    but since these co-occur often, a model may find them confusing. Thus, we introduce
    a novel class of Typographic Attacks: Self-Generated Attacks, which use the LVLM
    itself to create the attack. We identify two main attacks under this class: Class
    Based Attacks and Descriptive Attacks.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Azuma 和 Matsui ([2023](#bib.bib1)) 通过从预定义类别集中粘贴随机欺骗类别生成攻击。例如，如果图像表示的是金毛猎犬，那么攻击会粘贴随机选择的狗品种的文本，如猎犬。然而，猎犬可能不是
    LVLM 最容易与金毛猎犬混淆的品种。缓解这一问题的一种方法是检查数据集中模型可能与金毛猎犬混淆的其他类别。然而，这依赖于人的直觉，这可能因人而异，人们可能并不清楚模型可能认为混淆的内容。例如，一个人可能不会将电脑桌和显示器视为相似，但由于它们经常同时出现，模型可能会感到困惑。因此，我们引入了一类新型排版攻击：自生成攻击，该攻击利用
    LVLM 自身来创建攻击。我们确定了这一类别下的两种主要攻击：基于类别的攻击和描述性攻击。
- en: 'Class Based Attacks are based on the simple observation: a visually similar
    deceiving class to the target class is likely a more effective attack than a random
    class. Therefore, as shown in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Vision-LLMs
    Can Fool Themselves with Self-Generated Typographic Attacks") (a), we use this
    insight to propose Class Based Attacks where we ask the LVLM itself which deceiving
    class is most similar to the ground truth.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于类别的攻击基于一个简单的观察：与目标类别视觉上相似的欺骗类别比随机类别更可能是有效的攻击。因此，如图 [2](#S1.F2 "图 2 ‣ 1 介绍
    ‣ Vision-LLMs 能够通过自生成排版攻击自我欺骗") (a) 所示，我们利用这一见解提出了基于类别的攻击，其中我们询问 LVLM 自身哪个欺骗类别与真实标签最相似。
- en: Descriptive Attacks. While Class Based Attacks contain a stronger prior (visual
    similarity) than random attack, they don’t make use of the sophisticated language
    capabilities of LVLMs. These models are capable of incorporating richer language
    than a simple class when making a prediction. Based on this observation, we explore
    weather a descriptive attack that motivates the deceiving class is more effective
    than simply pasting the deceiving class. To obtain such attack, we propose simply
    asking the LVLM to recommend an attack against itself as Figure [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") (b) demonstrates. This results in a both a deceiving class and a descriptive
    reasoning for the attack.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性攻击。虽然基于类别的攻击包含比随机攻击更强的先验（视觉相似性），但它们并没有利用 LVLM 的复杂语言能力。这些模型在进行预测时能够融入比简单类别更丰富的语言。基于这一观察，我们探讨了激发欺骗类别的描述性攻击是否比简单地粘贴欺骗类别更有效。为了获得这样的攻击，我们建议直接询问
    LVLM 推荐对自己进行攻击的方法，如图 [2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 视觉-LLMs 可以通过自生成排版攻击自我欺骗") (b) 所示。这会产生一个欺骗类别和一个攻击的描述性推理。
- en: 3 Experiments
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 'Datasets. We use five classification datasets: OxfordPets Parkhi et al. ([2012](#bib.bib12)),
    StanfordCars Krause et al. ([2013](#bib.bib5)), Flowers Nilsback and Zisserman
    ([2008](#bib.bib11)), Aircraft Maji et al. ([2013](#bib.bib10)) and Food101 Bossard
    et al. ([2014](#bib.bib2)). These datasets cover a diverse set of domains, which
    is critical for testing LVLM(s) vulnerability to textual attacks, given their
    usage as generic visual assistants. Refer to the Appendix [C](#A3 "Appendix C
    Dataset Details ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") for further details about the datasets.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们使用了五个分类数据集：OxfordPets Parkhi 等人（[2012](#bib.bib12)）、StanfordCars Krause
    等人（[2013](#bib.bib5)）、Flowers Nilsback 和 Zisserman（[2008](#bib.bib11)）、Aircraft
    Maji 等人（[2013](#bib.bib10)）以及 Food101 Bossard 等人（[2014](#bib.bib2)）。这些数据集涵盖了多样的领域，这对于测试
    LVLM 对文本攻击的脆弱性至关重要，因为它们作为通用视觉助手的使用。有关数据集的更多详细信息，请参见附录 [C](#A3 "附录 C 数据集详情 ‣ 视觉-LLMs
    可以通过自生成排版攻击自我欺骗")。
- en: LVLM Models Evaluated. We test on four of the recent large vision language models
    (LVLMs). Namely, we test LLaVA 1.5 Liu et al. ([2023b](#bib.bib7)), MiniGPT4 Zhu
    et al. ([2023](#bib.bib16)), InstructBLIP Dai et al. ([2023](#bib.bib3)), and
    GPT-4V Yang et al. ([2023](#bib.bib15)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: LVLM 模型评估。我们测试了四个近期的大型视觉语言模型（LVLMs）。即，我们测试了 LLaVA 1.5 Liu 等人（[2023b](#bib.bib7)）、MiniGPT4
    Zhu 等人（[2023](#bib.bib16)）、InstructBLIP Dai 等人（[2023](#bib.bib3)）和 GPT-4V Yang
    等人（[2023](#bib.bib15)）。
- en: Typographic Attacks. We compare the effect of Random Class Azuma and Matsui
    ([2023](#bib.bib1)) attack where a randomly sampled class is pasted on the image
    to our Class Based and Descriptive Attacks (Section [2.2](#S2.SS2 "2.2 Self-Generated
    Attacks ‣ 2 Typographic Attacks Against Large Vision Language Models ‣ Vision-LLMs
    Can Fool Themselves with Self-Generated Typographic Attacks")). For Class Based
    Attacks, we use LLaVA 1.5 Liu et al. ([2023b](#bib.bib7)). For Descriptive Attacks,
    we note that open source models. fail to effectively respond to our attack query.
    Therefore, we use GPT-4V Yang et al. ([2023](#bib.bib15)), a more capable model.
    For both class and descriptive attacks, we also pose the same typographic generation
    query to the LLM underlying each model and use that as a baseline. Finally, for
    the Class based attack, we test using the Visual Encoder (VE) underlying LLaVA
    1.5 (CLIP) to retrieve the most similar class rather than simply asking the LVLM.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 排版攻击。我们比较了随机类别 Azuma 和 Matsui（[2023](#bib.bib1)）攻击的效果，其中将随机采样的类别粘贴到图像上，与我们的基于类别和描述性攻击（第
    [2.2](#S2.SS2 "2.2 自生成攻击 ‣ 2 对大型视觉语言模型的排版攻击 ‣ 视觉-LLMs 可以通过自生成排版攻击自我欺骗") 节）进行比较。对于基于类别的攻击，我们使用
    LLaVA 1.5 Liu 等人（[2023b](#bib.bib7)）。对于描述性攻击，我们注意到开源模型无法有效响应我们的攻击查询。因此，我们使用 GPT-4V
    Yang 等人（[2023](#bib.bib15)），这是一个更强大的模型。对于类别和描述性攻击，我们还将相同的排版生成查询提交给每个模型的 LLM，并将其作为基线。最后，对于基于类别的攻击，我们测试使用
    LLaVA 1.5（CLIP）下的视觉编码器（VE）来检索最相似的类别，而不是简单地询问 LVLM。
- en: '![Refer to caption](img/43d1e04ccfede3ee0678954a6ef0e821.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43d1e04ccfede3ee0678954a6ef0e821.png)'
- en: 'Figure 4: Comparing the effect of prompting the LVLM to ignore the typographic
    attack text. Green arrows refer to gains in performance, and red refer to drop.
    Black dashed line refers to baseline performance with no attacks. Refer to Section
    [3.1](#S3.SS1 "3.1 Typographic Attack Results ‣ 3 Experiments ‣ Vision-LLMs Can
    Fool Themselves with Self-Generated Typographic Attacks") for further discussion.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：比较提示LVLM忽略排版攻击文本的效果。绿色箭头表示性能提升，红色箭头表示性能下降。黑色虚线表示没有攻击的基线性能。有关进一步讨论，请参阅第[3.1](#S3.SS1
    "3.1 排版攻击结果 ‣ 3 实验 ‣ 视觉-LLMs可以通过自生成排版攻击欺骗自己")节。
- en: 3.1 Typographic Attack Results
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 排版攻击结果
- en: Table [1](#S2.T1 "Table 1 ‣ 2.1 Typographic Attacks Benchmark ‣ 2 Typographic
    Attacks Against Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks") reports the effect of different Typographic
    Attacks averaged over all datasets. We find Descriptive Attacks using the LVLM
    GPT-4V outperforms Random Class attacks Azuma and Matsui ([2023](#bib.bib1)) by
    . This is likely because, unlike Random and Class Based Attacks where the attack
    consists of a class only, Descriptive Attacks also paste a motivating description.
    Moreover, note how for LLaVA and InstructBLIP, Random Class is a fairly effective
    attack; the models lose almost half of their base performance. However, GPT4-V
    only loses about $6\%$ of its performance with Random Class but loses 7x times
    that with our Descriptive attack. This is likely because GPT4-V has better reasoning
    capabilities Fu et al. ([2023](#bib.bib4)) and, thus, is less likely to be deceived
    by a random class.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表[1](#S2.T1 "表 1 ‣ 2.1 排版攻击基准 ‣ 2 排版攻击对大型视觉语言模型的影响 ‣ 视觉-LLMs可以通过自生成排版攻击欺骗自己")报告了不同排版攻击在所有数据集上的平均效果。我们发现使用LVLM
    GPT-4V的描述性攻击优于Azuma和Matsui（[2023](#bib.bib1)）的随机类别攻击。这可能是因为，与仅包含类别的随机和基于类别的攻击不同，描述性攻击还附加了激励描述。此外，请注意，对于LLaVA和InstructBLIP，随机类别攻击是一种相当有效的攻击；这些模型的基础性能损失几乎达到一半。然而，GPT4-V在随机类别攻击下仅损失约$6\%$的性能，但在我们的描述性攻击下损失了7倍。这可能是因为GPT4-V具有更好的推理能力（见Fu等人，[2023](#bib.bib4)），因此不容易被随机类别欺骗。
- en: Despite having the lowest classification performance with no attack compared,
    MiniGPT-4 is most robust against Typographic Attacks; it only loses about  of
    its performance with the most effective attack which is significantly lower (and
    hence more robust) than either LLaVA or InstructBLIP which lose $80\%$ of their
    performance with the most effective attack.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在没有攻击的情况下分类性能最低，MiniGPT-4 对排版攻击最为稳健；它在最有效攻击下仅损失约的性能，这明显低于LLaVA或InstructBLIP，后者在最有效攻击下损失了$80\%$的性能。
- en: Descriptive Attack Descriptions. We test the effect of descriptions of our Descriptive
    Attacks (Sec [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks
    Against Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks")) on the overall performance. For each image, we test using
    no description, a random description from another image, and the description for
    that image. Observe Fig [3](#S2.F3 "Figure 3 ‣ 2.1 Typographic Attacks Benchmark
    ‣ 2 Typographic Attacks Against Large Vision Language Models ‣ Vision-LLMs Can
    Fool Themselves with Self-Generated Typographic Attacks"). Note that using the
    image description is the most effective at reducing performance across models.
    Most notably, we find a random description attack on GPT-4 performs almost the
    same as having no description. This is likely because of the strong reasoning
    capabilities of GPT4-V which likely enables it to realize the discrepancy between
    the description and the content of the image. We find the opposite trend with
    the other models where a random description is more powerful than the class by
    itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 描述性攻击描述。我们测试了描述性攻击（见[2.2](#S2.SS2 "2.2 自生成攻击 ‣ 2 排版攻击对大型视觉语言模型的影响 ‣ 视觉-LLMs可以通过自生成排版攻击欺骗自己")）描述对整体性能的影响。对于每张图片，我们测试了不使用描述、使用来自另一张图片的随机描述以及该图片的描述。请参见图[3](#S2.F3
    "图 3 ‣ 2.1 排版攻击基准 ‣ 2 排版攻击对大型视觉语言模型的影响 ‣ 视觉-LLMs可以通过自生成排版攻击欺骗自己")。请注意，使用图像描述在减少模型性能方面最为有效。值得注意的是，我们发现对GPT-4进行随机描述攻击的效果几乎与没有描述一样。这可能是因为GPT4-V的强大推理能力使其能够意识到描述与图像内容之间的差异。我们发现其他模型的趋势正好相反，随机描述的攻击效果比单独使用类别的攻击更强。
- en: Can LVLM(s) Ignore the Attack? Could Typographic Attacks be mitigated by simply
    prompting the LVLM to ignore the text in the image? Observe results in Fig [4](#S3.F4
    "Figure 4 ‣ 3 Experiments ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks"). Models fail to gain back their base performance (dashed
    lines) without a Typographic Attack. Nevertheless, some models still make some
    gains. Indeed, we see the most improvements from GPT4-V while we see mild improvements
    from LLaVA. More surprisingly, we see a decline in performance from InstructBLIP
    and MiniGPT4-2, which indicates that these models are not capable of executing
    this instruction. Overall, the results indicate that future work should be pay
    a greater attention to Typographic Attacks in LVLM(s).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: LVLM 是否可以忽略攻击？是否可以通过简单地提示 LVLM 忽略图像中的文本来缓解排版攻击？请参见图 [4](#S3.F4 "图 4 ‣ 3 实验 ‣
    视觉-LLMs 可以被自生成的排版攻击欺骗") 中的结果。没有排版攻击的情况下，模型无法恢复其基本性能（虚线）。尽管如此，一些模型仍然有所提升。事实上，我们看到
    GPT4-V 的改进最大，而 LLaVA 的改进较轻微。更令人惊讶的是，我们看到 InstructBLIP 和 MiniGPT4-2 的性能下降，这表明这些模型无法执行此指令。总体来看，结果表明未来的工作应更加关注
    LVLM 中的排版攻击。
- en: 4 Conclusion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: 'In this paper, we introduced a typographic attack benchmark for LVLM(s) where
    we showed how typographic attacks remain a concern with LVLM(s). We also introduced
    a novel class of attacks: Self-Generated typographic attacks uniquely designed
    for LVLM(s). Using our benchmark, we have shown how these attacks pose an even
    larger threat against LVLM(s) than prior work attacks.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个针对 LVLM 的排版攻击基准，展示了排版攻击如何仍然是 LVLM 的一个问题。我们还引入了一类新型攻击：自生成排版攻击，这些攻击专门为
    LVLM 设计。通过我们的基准，我们展示了这些攻击对 LVLM 的威胁比先前的工作攻击更大。
- en: Acknowledgements This material is based upon work supported, in part, by DARPA
    under agreement number HR00112020054\. Any opinions, findings, and conclusions
    or recommendations expressed in this material are those of the author(s) and do
    not necessarily reflect the views of the supporting agencies.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢 本材料部分基于 DARPA 的支持工作，协议号为 HR00112020054。材料中表达的任何意见、发现、结论或建议均为作者（们）的观点，并不一定反映支持机构的观点。
- en: References
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Azuma and Matsui (2023) Hiroki Azuma and Yusuke Matsui. 2023. Defense-prefix
    for preventing typographic attacks on clip. *arXiv preprint arXiv:2304.04512*.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azuma 和 Matsui（2023）Hiroki Azuma 和 Yusuke Matsui。2023 年。《防御前缀以防止对剪贴板的排版攻击》。*arXiv
    预印本 arXiv:2304.04512*。
- en: 'Bossard et al. (2014) Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
    2014. Food-101–mining discriminative components with random forests. In *Computer
    Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,
    2014, Proceedings, Part VI 13*, pages 446–461\. Springer.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bossard 等（2014）Lukas Bossard、Matthieu Guillaumin 和 Luc Van Gool。2014 年。《Food-101——使用随机森林挖掘判别性组件》。在
    *计算机视觉–ECCV 2014：第十三届欧洲会议，瑞士苏黎世，2014 年 9 月 6-12 日，会议论文集，第 VI 部分 13*，第 446–461
    页。Springer。
- en: 'Dai et al. (2023) Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong,
    Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip:
    Towards general-purpose vision-language models with instruction tuning. *arXiv
    preprint arXiv:2305.06500*.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2023）Wenliang Dai、Junnan Li、Dongxu Li、Anthony Meng Huat Tiong、Junqi Zhao、Weisheng
    Wang、Boyang Li、Pascale Fung 和 Steven Hoi。2023 年。《Instructblip：朝着通用视觉语言模型的指令调优》。*arXiv
    预印本 arXiv:2305.06500*。
- en: 'Fu et al. (2023) Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan
    Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. 2023. Mme: A
    comprehensive evaluation benchmark for multimodal large language models. *arXiv
    preprint arXiv:2306.13394*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等（2023）Chaoyou Fu、Peixian Chen、Yunhang Shen、Yulei Qin、Mengdan Zhang、Xu Lin、Jinrui
    Yang、Xiawu Zheng、Ke Li、Xing Sun 等。2023 年。《Mme：多模态大型语言模型的全面评估基准》。*arXiv 预印本 arXiv:2306.13394*。
- en: Krause et al. (2013) Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.
    2013. 3d object representations for fine-grained categorization. In *Proceedings
    of the IEEE international conference on computer vision workshops*, pages 554–561.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krause 等（2013）Jonathan Krause、Michael Stark、Jia Deng 和 Li Fei-Fei。2013 年。《细粒度分类的
    3D 物体表示》。在 *IEEE 国际计算机视觉会议论文集*，第 554–561 页。
- en: 'Liu et al. (2023a) Fuxiao Liu, Tianrui Guan, Zongxia Li, Lichang Chen, Yaser
    Yacoob, Dinesh Manocha, and Tianyi Zhou. 2023a. Hallusionbench: You see what you
    think? or you think what you see? an image-context reasoning benchmark challenging
    for gpt-4v (ision), llava-1.5, and other multi-modality models. *arXiv preprint
    arXiv:2310.14566*.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023a) Fuxiao Liu、Tianrui Guan、Zongxia Li、Lichang Chen、Yaser Yacoob、Dinesh
    Manocha 和 Tianyi Zhou。2023a年。Hallusionbench：你看到的就是你所想的？还是你想的就是你所看到的？一个挑战 GPT-4v
    (ision)、llava-1.5 和其他多模态模型的图像-上下文推理基准。*arXiv 预印本 arXiv:2310.14566*。
- en: Liu et al. (2023b) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2023b. Visual instruction tuning. In *NeurIPS*.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023b) Haotian Liu、Chunyuan Li、Qingyang Wu 和 Yong Jae Lee。2023b年。视觉指令调整。在
    *NeurIPS*。
- en: Liu et al. (2023c) Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, and Yu Qiao.
    2023c. Query-relevant images jailbreak large multi-modal models. *arXiv preprint
    arXiv:2311.17600*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023c) Xin Liu、Yichen Zhu、Yunshi Lan、Chao Yang 和 Yu Qiao。2023c年。查询相关图像破解大型多模态模型。*arXiv
    预印本 arXiv:2311.17600*。
- en: Liu et al. (2023d) Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang,
    Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. 2023d.
    On the hidden mystery of ocr in large multimodal models. *arXiv preprint arXiv:2305.07895*.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023d) Yuliang Liu、Zhang Li、Hongliang Li、Wenwen Yu、Mingxin Huang、Dezhi
    Peng、Mingyu Liu、Mingrui Chen、Chunyuan Li、Lianwen Jin 等人。2023d年。大型多模态模型中 OCR 的隐秘奥秘。*arXiv
    预印本 arXiv:2305.07895*。
- en: Maji et al. (2013) Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko,
    and Andrea Vedaldi. 2013. Fine-grained visual classification of aircraft. *arXiv
    preprint arXiv:1306.5151*.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maji 等人 (2013) Subhransu Maji、Esa Rahtu、Juho Kannala、Matthew Blaschko 和 Andrea
    Vedaldi。2013年。飞机的细粒度视觉分类。*arXiv 预印本 arXiv:1306.5151*。
- en: Nilsback and Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. 2008.
    Automated flower classification over a large number of classes. In *2008 Sixth
    Indian conference on computer vision, graphics & image processing*, pages 722–729\.
    IEEE.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nilsback 和 Zisserman (2008) Maria-Elena Nilsback 和 Andrew Zisserman。2008年。对大量类别的自动化花卉分类。在
    *2008年第六届印度计算机视觉、图形与图像处理会议*，第722–729页。IEEE。
- en: Parkhi et al. (2012) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar.
    2012. Cats and dogs. In *2012 IEEE conference on computer vision and pattern recognition*,
    pages 3498–3505\. IEEE.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parkhi 等人 (2012) Omkar M Parkhi、Andrea Vedaldi、Andrew Zisserman 和 CV Jawahar。2012年。猫与狗。在
    *2012 IEEE 计算机视觉与模式识别会议*，第3498–3505页。IEEE。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*, pages 8748–8763\.
    PMLR.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2021) Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel
    Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark 等人。2021年。从自然语言监督中学习可转移的视觉模型。在
    *国际机器学习会议*，第8748–8763页。PMLR。
- en: 'Xu et al. (2023) Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng
    Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. 2023. Lvlm-ehub: A comprehensive
    evaluation benchmark for large vision-language models. *arXiv preprint arXiv:2306.09265*.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 (2023) Peng Xu、Wenqi Shao、Kaipeng Zhang、Peng Gao、Shuo Liu、Meng Lei、Fanqing
    Meng、Siyuan Huang、Yu Qiao 和 Ping Luo。2023年。Lvlm-ehub：大型视觉-语言模型的综合评估基准。*arXiv 预印本
    arXiv:2306.09265*。
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching
    Lin, Zicheng Liu, and Lijuan Wang. 2023. The dawn of lmms: Preliminary explorations
    with gpt-4v (ision). *arXiv preprint arXiv:2309.17421*, 9(1).'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 (2023) Zhengyuan Yang、Linjie Li、Kevin Lin、Jianfeng Wang、Chung-Ching
    Lin、Zicheng Liu 和 Lijuan Wang。2023年。LMMS 的黎明：对 GPT-4v (ision) 的初步探索。*arXiv 预印本
    arXiv:2309.17421*，9(1)。
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. 2023. Minigpt-4: Enhancing vision-language understanding with advanced
    large language models. *arXiv preprint arXiv:2304.10592*.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 (2023) Deyao Zhu、Jun Chen、Xiaoqian Shen、Xiang Li 和 Mohamed Elhoseiny。2023年。Minigpt-4：通过先进的大型语言模型提升视觉-语言理解。*arXiv
    预印本 arXiv:2304.10592*。
- en: Appendix A Results for each dataset
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 各数据集的结果
- en: In Section [3](#S3 "3 Experiments ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks"), we reported the results of each typographic attack averaged
    over all the datasets in our benchmark. In this Section, we break down the results
    per dataset. Refer to Table [2](#A5.T2 "Table 2 ‣ Appendix E Image White Space
    for Typographic Attacks ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") for results. Note that as we discuss in Section [3.1](#S3.SS1
    "3.1 Typographic Attack Results ‣ 3 Experiments ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks"), our Self Generated typographic attacks
    (Section [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against
    Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks")), including class-based and descriptive attacks, are consistently
    more effective than prior work random class attacks, in reducing model performance.
    Moreover, on average, descriptive attacks are more effective than class-based
    attacks at reducing model performance. This is likely due to their use of deceiving
    prompts, which harnesses the sophisticated language understanding capabilities
    of LLM(s).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3节](#S3 "3 实验 ‣ 视觉大模型能通过自生成排版攻击自我欺骗")中，我们报告了在基准测试中所有数据集上各排版攻击的结果平均值。在本节中，我们将按数据集分解结果。结果请参见[表2](#A5.T2
    "表2 ‣ 附录E 排版攻击图像空白 ‣ 视觉大模型能通过自生成排版攻击自我欺骗")。请注意，正如我们在[第3.1节](#S3.SS1 "3.1 排版攻击结果
    ‣ 3 实验 ‣ 视觉大模型能通过自生成排版攻击自我欺骗")中讨论的，我们的自生成排版攻击（[第2.2节](#S2.SS2 "2.2 自生成攻击 ‣ 2 针对大型视觉语言模型的排版攻击
    ‣ 视觉大模型能通过自生成排版攻击自我欺骗")），包括基于类别的攻击和描述性攻击，在降低模型性能方面始终比以往的随机类别攻击更有效。此外，平均而言，描述性攻击在降低模型性能方面比基于类别的攻击更为有效。这可能是由于它们使用了误导性提示，利用了大模型（LLM）的复杂语言理解能力。
- en: Appendix B Qualitative Examples
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 定性示例
- en: 'This Section provides more qualitative examples of the attacks generated by
    our new class of Typographic Attacks: Self Generated Attacks (Section [2.2](#S2.SS2
    "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against Large Vision Language
    Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks")),
    including Class Attacks and Descriptive Attacks. Refer to Figure [6](#A3.F6 "Figure
    6 ‣ Appendix C Dataset Details ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") for results. Note how our method (Self Generated Attacks)
    effectively generalizes to datasets of different domains. Indeed, Class-Based
    Attacks are able to effectively recommend similar and hence more effective classes
    (Audi, which is more similar to Volkswagen than Mcalren (Column2)), and Descriptive
    attacks are able to recommend a convincing deceiving description as well as a
    deceiving class across domains. For example, Descriptive Attacks in Column (2)
    justify the old look of the Audi in the image with “re-imagining an old design
    for a modern era" and hence fooling the model.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了我们新类型排版攻击（自生成攻击，[第2.2节](#S2.SS2 "2.2 自生成攻击 ‣ 2 针对大型视觉语言模型的排版攻击 ‣ 视觉大模型能通过自生成排版攻击自我欺骗")）生成的攻击的更多定性示例，包括类别攻击和描述性攻击。结果请参见[图6](#A3.F6
    "图6 ‣ 附录C 数据集详情 ‣ 视觉大模型能通过自生成排版攻击自我欺骗")。请注意我们的方法（自生成攻击）如何有效地推广到不同领域的数据集。实际上，基于类别的攻击能够有效地推荐类似的且因此更有效的类别（如奥迪，它比麦克拉伦（Column2）更类似于大众），而描述性攻击能够在不同领域推荐有说服力的误导性描述以及误导性类别。例如，第（2）列中的描述性攻击通过“重新想象旧设计以适应现代时代”来解释图像中奥迪的旧外观，从而欺骗模型。
- en: Appendix C Dataset Details
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 数据集详情
- en: 'In this Section, we provide details about the datasets used in our experiments.
    As discussed in Section [3](#S3 "3 Experiments ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks"), we use five classification datasets
    that cover a diverse variety of domains, namely: OxfordPets Parkhi et al. ([2012](#bib.bib12))
    for fine-grained of 37 pet breeds, StanfordCars Krause et al. ([2013](#bib.bib5))
    for fine-grained classification of 196 car models, Flowers Nilsback and Zisserman
    ([2008](#bib.bib11)) for fine grained classification of 102 flowers classes, Aircraft
    Maji et al. ([2013](#bib.bib10)) for fine grained classification of 100 aircraft
    models and Food101 Bossard et al. ([2014](#bib.bib2)) for classification of 101
    food dishes. Moreover, note that we limit the number of samples in the test set
    per dataset to 1000 samples. This is due to the computational and monetary costs
    associated with evaluating GPT-4V.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了有关我们实验中使用的数据集的详细信息。如第 [3](#S3 "3 实验 ‣ 视觉-LLMs 可能会被自生成的排版攻击欺骗") 节讨论的，我们使用了五个分类数据集，覆盖了各种不同的领域，即：OxfordPets
    Parkhi 等（[2012](#bib.bib12)）用于37个宠物品种的细粒度分类，StanfordCars Krause 等（[2013](#bib.bib5)）用于196个汽车模型的细粒度分类，Flowers
    Nilsback 和 Zisserman（[2008](#bib.bib11)）用于102个花类的细粒度分类，Aircraft Maji 等（[2013](#bib.bib10)）用于100个飞机模型的细粒度分类，Food101
    Bossard 等（[2014](#bib.bib2)）用于101种食品菜肴的分类。此外，请注意，我们将每个数据集测试集中的样本数量限制为1000个样本。这是由于评估
    GPT-4V 相关的计算和经济成本。
- en: '![Refer to caption](img/d94e9e1d2713e230fd96af208837176f.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/d94e9e1d2713e230fd96af208837176f.png)'
- en: 'Figure 5: Comparing prior work process for testing typographic attacks on an
    image (a) and ours (b) where add white space at the bottom and top of the image
    to allow for the attacks. Refer to Section [E](#A5 "Appendix E Image White Space
    for Typographic Attacks ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") for discussion.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：比较以前的工作流程用于测试图像上的排版攻击（a）和我们的工作（b），其中在图像的底部和顶部添加了空白以允许攻击。有关讨论，请参阅第 [E](#A5
    "附录 E 图像空白用于排版攻击 ‣ 视觉-LLMs 可能会被自生成的排版攻击欺骗") 节。
- en: '![Refer to caption](img/4cccd60e533267a5f14b5e0634ce9374.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/4cccd60e533267a5f14b5e0634ce9374.png)'
- en: 'Figure 6: Qualtiative comparison between Random Class Typographic Attacks Azuma
    and Matsui ([2023](#bib.bib1)) (first row) and our Self-Generated Attacks (Section
    [2.2](#S2.SS2 "2.2 Self-Generated Attacks ‣ 2 Typographic Attacks Against Large
    Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks")) including Class Based Attacks (Second row) and Descriptive Attacks
    (Third row) on three of the datasets used in our benchmarks, namely StanfordCars
    Krause et al. ([2013](#bib.bib5)), Aircraft Maji et al. ([2013](#bib.bib10)) and
    Food101 Bossard et al. ([2014](#bib.bib2)). Refer to Section [B](#A2 "Appendix
    B Qualitative Examples ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic
    Attacks") for Discussion.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：随机类别排版攻击 Azuma 和 Matsui（[2023](#bib.bib1)）（第一行）与我们自生成攻击（第 [2.2](#S2.SS2
    "2.2 自生成攻击 ‣ 2 对大型视觉语言模型的排版攻击 ‣ 视觉-LLMs 可能会被自生成的排版攻击欺骗") 节）包括基于类别的攻击（第二行）和描述性攻击（第三行）在我们基准测试中使用的三个数据集上的定性比较，即
    StanfordCars Krause 等（[2013](#bib.bib5)）、Aircraft Maji 等（[2013](#bib.bib10)）和
    Food101 Bossard 等（[2014](#bib.bib2)）。有关讨论，请参阅第 [B](#A2 "附录 B 定性示例 ‣ 视觉-LLMs 可能会被自生成的排版攻击欺骗")
    节。
- en: Appendix D Evaluation Question
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 评估问题
- en: 'In Section [2.1](#S2.SS1 "2.1 Typographic Attacks Benchmark ‣ 2 Typographic
    Attacks Against Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks"), we defined a benchmark for testing
    vision language models’ (LVLM(s)) vulnerability to typographic attacks. To that
    end, we posed the LVLM of the set of manipulated images of each typographic attack
    algorithm. We then asked the LVLM to choose the correct class among the set of
    deceiving classes produced by each typographic attack algorithm. In this Section,
    we provide a more detailed overview of the question prompt. Indeed, assuming  is
    the image ground truth, then we pose the LVLM with the following question:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [2.1](#S2.SS1 "2.1 排版攻击基准 ‣ 2 对大型视觉语言模型的排版攻击 ‣ 视觉-LLMs 可能会被自生成的排版攻击欺骗") 节中，我们定义了一个用于测试视觉语言模型（LVLM(s)）对排版攻击脆弱性的基准。为此，我们将每种排版攻击算法的操控图像集提供给
    LVLM。然后，我们要求 LVLM 从每种排版攻击算法生成的欺骗类别集中选择正确的类别。在本节中，我们提供了问题提示的更详细概述。实际上，假设是图像的真实标签，那么我们给
    LVLM 提出以下问题：
- en: 'Select the correct {Dataset Subject} pictured in the image: (1) {}, (3) }.
    Answer with either (1) or (2) … (N) only.'
  id: totrans-93
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 选择图像中显示的正确{数据集主题}：（1）{}，（3）}。仅用（1）或（2）…（N）回答。
- en: We then test whether the model answer contains the correct choice (in this case
    (3) ). Moreover, we ensure to randomize the order of $y$ in the answer options
    to avoid model bias to particular answer numbers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着测试模型回答是否包含正确选项（在这种情况下是（3））。此外，我们确保随机排列答案选项中的$y$，以避免模型对特定答案数字的偏见。
- en: Appendix E Image White Space for Typographic Attacks
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 图像白色区域用于印刷攻击
- en: In Section [2.1](#S2.SS1 "2.1 Typographic Attacks Benchmark ‣ 2 Typographic
    Attacks Against Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks"), we described our benchmark for testing
    LVLM(s) weaknesses against Typographic Attacks. We noted hat unlike prior work
    Azuma and Matsui ([2023](#bib.bib1)) that pastes the attack at a random location
    in the image, we add white space at the bottom and top of the image for pasting
    the attack. This is so we avoid occluding important visual information required
    to make the prediction. In this Section, we provide an illustrative example that
    motivates our choice. Examine Figure [5](#A3.F5 "Figure 5 ‣ Appendix C Dataset
    Details ‣ Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks").
    Note how prior work attack (a) occludes the car logo, which is important to make
    the prediction about the car model. However, our method Figure [5](#A3.F5 "Figure
    5 ‣ Appendix C Dataset Details ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks") (b) avoids this by allocating white space at the bottom
    and top of the image for the textual attack.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2.1](#S2.SS1 "2.1 Typographic Attacks Benchmark ‣ 2 Typographic Attacks Against
    Large Vision Language Models ‣ Vision-LLMs Can Fool Themselves with Self-Generated
    Typographic Attacks")节中，我们描述了测试LVLM(s)对印刷攻击的弱点的基准。我们注意到，与之前的Azuma和Matsui ([2023](#bib.bib1))的工作不同，他们将攻击贴在图像的随机位置上，我们在图像的顶部和底部添加了空白区域来粘贴攻击。这是为了避免遮挡做出预测所需的重要视觉信息。在本节中，我们提供了一个说明性示例，以说明我们选择的动机。请查看图[5](#A3.F5
    "Figure 5 ‣ Appendix C Dataset Details ‣ Vision-LLMs Can Fool Themselves with
    Self-Generated Typographic Attacks")。请注意，之前的攻击（a）遮挡了汽车标志，这对预测汽车型号很重要。然而，我们的方法图[5](#A3.F5
    "Figure 5 ‣ Appendix C Dataset Details ‣ Vision-LLMs Can Fool Themselves with
    Self-Generated Typographic Attacks")（b）通过在图像的底部和顶部分配空白区域来避免这种情况。
- en: '|  | No Text | Random Class | Class Based (ours) | Descriptive (ours) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类别 | 基于类别（我们的） | 描述性（我们的） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 44.6 | 37.4 | 15.8 | 28.4 | 21.8 | 30.6 | 8.9 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 44.6 | 37.4 | 15.8 | 28.4 | 21.8 | 30.6 | 8.9 |'
- en: '| LLaVA 1.5 | 26.1 | 11.8 | 7.2 | 7.3 | 5.3 | 4.2 | 6.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 26.1 | 11.8 | 7.2 | 7.3 | 5.3 | 4.2 | 6.9 |'
- en: '| InstructBlip | 26.1 | 4.7 | 6.2 | 3.3 | 4.0 | 3.8 | 5.3 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 26.1 | 4.7 | 6.2 | 3.3 | 4.0 | 3.8 | 5.3 |'
- en: '| MiniGPT4-2 | 19.7 | 19.7 | 20.0 | 21.2 | 19.3 | 18.4 | 16.5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 19.7 | 19.7 | 20.0 | 21.2 | 19.3 | 18.4 | 16.5 |'
- en: '| Avg | 29.10 | 18.40 | 12.29 | 15.04 | 12.59 | 14.24 | 9.39 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 29.10 | 18.40 | 12.29 | 15.04 | 12.59 | 14.24 | 9.39 |'
- en: (a) Aircraft Maji et al. ([2013](#bib.bib10))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Aircraft Maji et al. ([2013](#bib.bib10))
- en: '|  | No Text | Random Class | Class Based (ours) | Descriptive (ours) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类别 | 基于类别（我们的） | 描述性（我们的） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 81.0 | 74.1 | 58.3 | 70.4 | 62.7 | 70.4 | 43.9 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 81.0 | 74.1 | 58.3 | 70.4 | 62.7 | 70.4 | 43.9 |'
- en: '| LLaVA 1.5 | 69.0 | 43.0 | 31.4 | 27.8 | 21.4 | 10.8 | 10.8 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 69.0 | 43.0 | 31.4 | 27.8 | 21.4 | 10.8 | 10.8 |'
- en: '| InstructBlip | 85.0 | 53.1 | 47.6 | 42.6 | 45.0 | 29.7 | 31.3 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 85.0 | 53.1 | 47.6 | 42.6 | 45.0 | 29.7 | 31.3 |'
- en: '| MiniGPT4-2 | 33.4 | 33.4 | 33.5 | 29.2 | 34.3 | 34.0 | 30.3 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 33.4 | 33.4 | 33.5 | 29.2 | 34.3 | 34.0 | 30.3 |'
- en: '| Avg | 67.10 | 50.89 | 42.67 | 42.47 | 40.82 | 36.21 | 29.06 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 67.10 | 50.89 | 42.67 | 42.47 | 40.82 | 36.21 | 29.06 |'
- en: (b) StanfordCars Krause et al. ([2013](#bib.bib5))
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: (b) StanfordCars Krause et al. ([2013](#bib.bib5))
- en: '|  | No Text | Random Class | Class Based (ours) | Descriptive (ours) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类别 | 基于类别（我们的） | 描述性（我们的） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 74.5 | 65.5 | 33.4 | 56.8 | 50.2 | 66.3 | 28.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 74.5 | 65.5 | 33.4 | 56.8 | 50.2 | 66.3 | 28.3 |'
- en: '| LLaVA 1.5 | 38.3 | 10.8 | 9.3 | 5.3 | 6.5 | 6.5 | 6.8 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 38.3 | 10.8 | 9.3 | 5.3 | 6.5 | 6.5 | 6.8 |'
- en: '| InstructBlip | 48.3 | 15.5 | 11.6 | 17.9 | 16.4 | 5.9 | 8.2 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 48.3 | 15.5 | 11.6 | 17.9 | 16.4 | 5.9 | 8.2 |'
- en: '| MiniGPT4-2 | 20.1 | 17.8 | 18.5 | 17.5 | 17.9 | 16.3 | 14.7 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 20.1 | 17.8 | 18.5 | 17.5 | 17.9 | 16.3 | 14.7 |'
- en: '| Avg | 45.29 | 27.40 | 18.19 | 24.39 | 22.76 | 23.75 | 14.52 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Avg | 45.29 | 27.40 | 18.19 | 24.39 | 22.76 | 23.75 | 14.52 |'
- en: (c) Flowers Nilsback and Zisserman ([2008](#bib.bib11)),
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Flowers Nilsback 和 Zisserman ([2008](#bib.bib11)),
- en: '|  | No Text | Random Class | Class Based (ours) | Descriptive (ours) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类 | 基于类别（我们的） | 描述性（我们的） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 82.9 | 77.6 | 45.5 | 68.1 | 58.6 | 75.1 | 48.8 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 82.9 | 77.6 | 45.5 | 68.1 | 58.6 | 75.1 | 48.8 |'
- en: '| LLaVA 1.5 | 71.4 | 54.4 | 32.6 | 41.3 | 26.2 | 30.6 | 18.5 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 71.4 | 54.4 | 32.6 | 41.3 | 26.2 | 30.6 | 18.5 |'
- en: '| InstructBlip | 76.0 | 38.2 | 21.3 | 33.0 | 24.7 | 17.8 | 17.9 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 76.0 | 38.2 | 21.3 | 33.0 | 24.7 | 17.8 | 17.9 |'
- en: '| MiniGPT4-2 | 33.7 | 32.5 | 32.9 | 32.8 | 32.4 | 29.2 | 31.1 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 33.7 | 32.5 | 32.9 | 32.8 | 32.4 | 29.2 | 31.1 |'
- en: '| Avg | 66.00 | 50.67 | 33.04 | 43.77 | 35.46 | 38.16 | 29.05 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Avg | 66.00 | 50.67 | 33.04 | 43.77 | 35.46 | 38.16 | 29.05 |'
- en: (d) Food101 Bossard et al. ([2014](#bib.bib2)).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Food101 Bossard 等人 ([2014](#bib.bib2)).
- en: '|  | No Text | Random Class | Class Based (ours) | Descriptive (ours) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | 无文本 | 随机类 | 基于类别（我们的） | 描述性（我们的） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | CLIP | LLM | LVLM | LLM | LVLM |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| GPT4-V | 80.5 | 75.4 | 41.8 | 65.2 | 61.3 | 48.4 | 29.1 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GPT4-V | 80.5 | 75.4 | 41.8 | 65.2 | 61.3 | 48.4 | 29.1 |'
- en: '| LLaVA 1.5 | 49.2 | 16.5 | 11.2 | 9.6 | 6.6 | 5.4 | 6.3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA 1.5 | 49.2 | 16.5 | 11.2 | 9.6 | 6.6 | 5.4 | 6.3 |'
- en: '| InstructBlip | 65.6 | 22.5 | 16.3 | 18.4 | 20.8 | 12.2 | 11.8 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| InstructBlip | 65.6 | 22.5 | 16.3 | 18.4 | 20.8 | 12.2 | 11.8 |'
- en: '| MiniGPT4-2 | 31.9 | 24.7 | 23.8 | 22.6 | 22.6 | 20.9 | 19.4 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MiniGPT4-2 | 31.9 | 24.7 | 23.8 | 22.6 | 22.6 | 20.9 | 19.4 |'
- en: '| Avg | 56.79 | 34.76 | 23.25 | 28.94 | 27.81 | 21.70 | 16.64 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Avg | 56.79 | 34.76 | 23.25 | 28.94 | 27.81 | 21.70 | 16.64 |'
- en: (e) OxfordPets Parkhi et al. ([2012](#bib.bib12)),
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (e) OxfordPets Parkhi 等人 ([2012](#bib.bib12)),
- en: 'Table 2: Comparison between the effect different typographic attacks (Random
    Class Azuma and Matsui ([2023](#bib.bib1)) and our Self-Generated Attacks) on
    Large Vision Lanugage Models: GPT-4V Yang et al. ([2023](#bib.bib15)), LLaVA 1.5
    Liu et al. ([2023b](#bib.bib7)), MiniGPT4-2 Zhu et al. ([2023](#bib.bib16)), and
    InstructBLIP Dai et al. ([2023](#bib.bib3)) across various Datasets. Refer to
    Section [A](#A1 "Appendix A Results for each dataset ‣ Vision-LLMs Can Fool Themselves
    with Self-Generated Typographic Attacks") for further discussion.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同排版攻击（随机类 Azuma 和 Matsui ([2023](#bib.bib1)) 与我们自生成攻击）的效果比较，大型视觉语言模型：GPT-4V
    Yang 等人 ([2023](#bib.bib15))、LLaVA 1.5 Liu 等人 ([2023b](#bib.bib7))、MiniGPT4-2
    Zhu 等人 ([2023](#bib.bib16)) 和 InstructBLIP Dai 等人 ([2023](#bib.bib3)) 在各种数据集上的表现。有关更多讨论，请参见[附录
    A](#A1 "附录 A 每个数据集的结果 ‣ 视觉LLMs 可以被自生成的排版攻击欺骗")。
