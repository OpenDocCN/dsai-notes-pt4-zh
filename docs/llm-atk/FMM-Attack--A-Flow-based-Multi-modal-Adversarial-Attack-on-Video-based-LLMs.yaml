- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:45:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:45:20'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: FMM-Attack：一种基于流的多模态对抗攻击视频基础的大型语言模型
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.13507](https://ar5iv.labs.arxiv.org/html/2403.13507)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.13507](https://ar5iv.labs.arxiv.org/html/2403.13507)
- en: '¹¹institutetext: Tsinghua Shenzhen International Graduate School, Tsinghua
    University ²²institutetext: Tencent Technology (Beijing) Co.Ltd ³³institutetext:
    Tencent Lab 33 ⁴⁴institutetext: Peng Cheng Laboratory ⁵⁵institutetext: Peking
    University'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构：清华大学深圳国际研究生院 ²²机构：腾讯科技（北京）有限公司 ³³机构：腾讯实验室 ⁴⁴机构：鹏城实验室 ⁵⁵机构：北京大学
- en: '⁵⁵email: {ljm22, gkf21}@mails.tsinghua.edu.cn,'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵⁵邮箱：{ljm22, gkf21}@mails.tsinghua.edu.cn，
- en: '{baiyang0522, zhang304973926}@gmail.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{baiyang0522, zhang304973926}@gmail.com'
- en: xiast@sz.tsinghua.edu.cn, yisen.wang@pku.edu.cnJinmin Li ^() 11    Yang Bai
    ^($\dagger$) 22    Jingyun Zhang 33    Shu-tao Xia 1144    Yisen Wang 55
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: xiast@sz.tsinghua.edu.cn, yisen.wang@pku.edu.cn Jinmin Li ^() 11    Yang Bai
    ^($\dagger$) 22    Jingyun Zhang 33    Shu-tao Xia 1144    Yisen Wang 55
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Despite the remarkable performance of video-based large language models (LLMs),
    their adversarial threat remains unexplored. To fill this gap, we propose the
    first adversarial attack tailored for video-based LLMs by crafting flow-based
    multi-modal adversarial perturbations on a small fraction of frames within a video,
    dubbed FMM-Attack. Extensive experiments show that our attack can effectively
    induce video-based LLMs to generate incorrect answers when videos are added with
    imperceptible adversarial perturbations. Intriguingly, our FMM-Attack can also
    induce garbling in the model output, prompting video-based LLMs to hallucinate.
    Overall, our observations inspire a further understanding of multi-modal robustness
    and safety-related feature alignment across different modalities, which is of
    great importance for various large multi-modal models. Our code is available at
    [https://github.com/THU-Kingmin/FMM-Attack](https://github.com/THU-Kingmin/FMM-Attack).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于视频的大型语言模型（LLMs）表现出色，但其对抗威胁尚未得到探索。为填补这一空白，我们提出了首个针对视频基础LLMs的对抗攻击方法，名为FMM-Attack，该方法通过在视频的少量帧上制造基于流的多模态对抗扰动。大量实验证明，我们的攻击能够有效地诱使视频基础LLMs在视频中加入不可感知的对抗扰动时生成错误答案。有趣的是，我们的FMM-Attack还能够在模型输出中诱发乱码，促使视频基础LLMs产生虚假信息。总体而言，我们的观察激发了对多模态鲁棒性和跨不同模态的安全相关特征对齐的进一步理解，这对于各种大型多模态模型至关重要。我们的代码可在[https://github.com/THU-Kingmin/FMM-Attack](https://github.com/THU-Kingmin/FMM-Attack)获取。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Video-based large language models Adversarial attacks Multi-modal attacks¹¹footnotetext:
    Equal contribution.⁴⁴footnotetext: Corresponding author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视频的大型语言模型 对抗攻击 多模态攻击 ¹¹脚注：等贡献。 ⁴⁴脚注：通讯作者。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/9d0ec0b597e2fefac7c86ae261851884.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9d0ec0b597e2fefac7c86ae261851884.png)'
- en: 'Figure 1: Visualization of the transmission cross video and LLM features. In
    Fig. (a), when attacking in video feature space, the clustering effect of garbled
    video features can result in garbled clusters in LLM features. In Fig. (b), when
    attacking in LLM feature space, garbled videos barely form clusters in LLM features,
    let alone in video features. This illustrates the asymmetric transmission between
    video and LLM features.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：视频传输交叉和LLM特征的可视化。在图（a）中，当在视频特征空间中进行攻击时，乱码视频特征的聚类效果可能会导致LLM特征中出现乱码聚类。在图（b）中，当在LLM特征空间中进行攻击时，乱码视频几乎不会在LLM特征中形成聚类，更不用说在视频特征中形成聚类了。这说明了视频和LLM特征之间的非对称传输。
- en: Recent advancements in multi-modal understanding are largely based on the combination
    of pretrained vision models with large language models (LLMs) [[26](#bib.bib26),
    [35](#bib.bib35), [13](#bib.bib13)]. However, these large multi-modal models are
    vulnerable to adversarial attacks [[18](#bib.bib18), [36](#bib.bib36), [8](#bib.bib8),
    [31](#bib.bib31)]. A recent study [[37](#bib.bib37)] found that the introduction
    of another modality makes models like vision large language models (VLLMs) even
    more susceptible to generating harmful output compared to their LLM counterparts.
    This can be attributed to not only their independent vulnerabilities within a
    single modality but also the suboptimal alignment between the two modalities,
    which can break the established safety alignment inside each modality, making
    the model more vulnerable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近多模态理解的进展主要基于预训练视觉模型与大型语言模型（LLMs）的结合[[26](#bib.bib26), [35](#bib.bib35), [13](#bib.bib13)]。然而，这些大型多模态模型容易受到对抗攻击[[18](#bib.bib18),
    [36](#bib.bib36), [8](#bib.bib8), [31](#bib.bib31)]。最近的一项研究[[37](#bib.bib37)]发现，引入另一种模态使得类似视觉大型语言模型（VLLMs）的模型比其LLM对应模型更容易生成有害输出。这不仅归因于单一模态下的独立脆弱性，还由于两种模态之间的对齐不佳，这可能会破坏每种模态内已建立的安全对齐，使模型更易受到攻击。
- en: Nowadays, in particular, Sora^*^**https://openai.com/sora has shown extraordinary
    performances in creating realistic and imaginative scenes from text instructions,
    demonstrating the significance of large multi-modal models, especially across
    video and language modalities. Among them, video-based LLMs [[11](#bib.bib11),
    [32](#bib.bib32), [15](#bib.bib15)] have significantly enhanced general video
    understanding in zero-shot settings and achieved exceptional performance in a
    wide range of video-related tasks, such as video captioning [[25](#bib.bib25),
    [1](#bib.bib1), [33](#bib.bib33), [22](#bib.bib22)], video retrieval [[14](#bib.bib14),
    [7](#bib.bib7), [6](#bib.bib6)], and scene understanding [[5](#bib.bib5), [9](#bib.bib9),
    [28](#bib.bib28)]. Yet their adversarial robustness is under-explored.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，特别是*Sora*^**https://openai.com/sora 在从文本指令生成逼真且富有想象力的场景方面表现出色，展示了大型多模态模型的重要性，尤其是在视频和语言模态之间。在这些模型中，基于视频的LLMs[[11](#bib.bib11),
    [32](#bib.bib32), [15](#bib.bib15)]显著增强了零样本设置下的一般视频理解，并在视频相关任务如视频描述[[25](#bib.bib25),
    [1](#bib.bib1), [33](#bib.bib33), [22](#bib.bib22)]、视频检索[[14](#bib.bib14), [7](#bib.bib7),
    [6](#bib.bib6)]和场景理解[[5](#bib.bib5), [9](#bib.bib9), [28](#bib.bib28)]中取得了卓越的表现。然而，它们的对抗鲁棒性尚未充分探索。
- en: 'To evaluate the adversarial robustness of video-based LLMs, we propose a flow-based
    multi-modal attack, dubbed FMM-Attack, to craft adversarial perturbations on video
    inputs for the first time. To be more specific, we utilize two objective functions
    in two modalities, namely video features shown in Fig. [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs")(a), and features of the last hidden layer of LLM shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs")(b) respectively. Also, a flow-based temporal mask
    is introduced to select the most effective frames in the video, which is inspired
    by the video clipping adopted in video understanding tasks, especially for video-based
    LLMs [[4](#bib.bib4), [30](#bib.bib30)]. Video clipping can effectively improve
    the performance of video-based learning due to its focused annotation, reducing
    complexity, improved temporal understanding, and efficient processing. Motivated
    by these benefits, we utilize a light-weighted flow-based mechanism, to conduct
    a similar splitting and selection operation on video frames. Extensive experiments
    have demonstrated the effectiveness, efficiency, and imperceptibility of our FMM-Attack
    on four benchmark video-based LLMs and two datasets.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估基于视频的LLMs的对抗鲁棒性，我们提出了一种基于流的多模态攻击，称为FMM-Attack，首次针对视频输入设计对抗扰动。具体来说，我们在两种模态中使用了两个目标函数，即图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ FMM-Attack: 基于流的多模态对抗攻击针对视频LLMs")(a)中所示的视频特征和图[1](#S1.F1 "图 1 ‣
    1 引言 ‣ FMM-Attack: 基于流的多模态对抗攻击针对视频LLMs")(b)中所示的LLM最后一层隐藏层的特征。此外，引入了基于流的时间掩码来选择视频中的最有效帧，这受到了视频理解任务中采用的视频剪辑的启发，特别是对于基于视频的LLMs
    [[4](#bib.bib4), [30](#bib.bib30)]。视频剪辑可以有效提高基于视频学习的性能，因为它提供了集中注释、减少复杂性、改善时间理解和高效处理。受到这些好处的激励，我们采用了一种轻量级的基于流的机制，在视频帧上进行类似的分割和选择操作。大量实验已经证明了我们FMM-Attack在四个基准视频LLMs和两个数据集上的有效性、效率和不可察觉性。'
- en: 'Surprisingly, we find that successful adversarial attacks on video-based large
    language models (LLMs) can result in the generation of either garbled nonsensical
    sequences (displayed in orange, such as ‘6.6.6.6.6.6.6’) or incorrect semantic
    sequences (displayed in orange) in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs").
    Moreover, there is a noticeable difference in cross-modal features, particularly
    for garbled videos. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")(a), when video
    features are under attack, a clustering phenomenon can be observed, where the
    separation between garbled and natural video features is transmitted to that of
    LLM features. In contrast, in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")(b), when attacking
    LLM features, no corresponding clustering phenomenon is transmitted between the
    different video features and LLM features. This asymmetric transmission between
    video and LLM features emphasizes the suboptimal nature of current alignment especially
    for safety-related features. These observations can inspire us to further understand
    multi-modal features and the alignment of their robustness.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '令人惊讶的是，我们发现对基于视频的大型语言模型（LLMs）进行成功的对抗攻击可以导致生成无意义的乱码序列（以橙色显示，例如‘6.6.6.6.6.6.6’）或不正确的语义序列（以橙色显示），如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ FMM-Attack: 基于流的多模态对抗攻击针对视频LLMs")所示。此外，跨模态特征之间存在明显的差异，特别是对于乱码视频。如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ FMM-Attack: 基于流的多模态对抗攻击针对视频LLMs")(a)所示，当视频特征受到攻击时，可以观察到一种聚类现象，即乱码视频特征与自然视频特征之间的分离传递到LLM特征中。相比之下，如图[1](#S1.F1
    "图 1 ‣ 1 引言 ‣ FMM-Attack: 基于流的多模态对抗攻击针对视频LLMs")(b)所示，当攻击LLM特征时，不同视频特征和LLM特征之间没有相应的聚类现象传递。这种视频和LLM特征之间的不对称传递强调了当前对齐的亚优化性质，特别是在安全相关特征方面。这些观察可以激励我们进一步理解多模态特征及其鲁棒性的对齐。'
- en: 'In summary, our main contribution can be outlined as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献可以概述如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, we conduct the first comprehensive investigation
    regarding the adversarial vulnerability of video-based LLMs. We propose a novel
    flow-based multi-modal attack, dubbed FMM-Attack, on video-based LLMs for the
    first time.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，我们首次对基于视频的LLMs的对抗脆弱性进行了全面调查。我们首次提出了一种新颖的基于流的多模态攻击，称为FMM-Attack，用于基于视频的LLMs。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our observations on cross-modal feature attacks have inspired a further understanding
    of multi-modal robustness and their safety-related feature alignment, which is
    of great importance for various large multi-modal models.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对跨模态特征攻击的观察激发了对多模态鲁棒性及其安全相关特征对齐的进一步理解，这对于各种大型多模态模型具有重要意义。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments show that our attack can effectively induce video-based
    LLMs to generate either garbled nonsensical sequences or incorrect semantic sequences
    with imperceptible perturbations added on less than 20% video frames.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大量实验表明，我们的攻击可以有效地诱导基于视频的LLMs生成乱码或语义错误的序列，只需对不到20%的视频帧添加不可察觉的扰动。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Video-based Large Language Models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于视频的大型语言模型
- en: Video-based large language models (video-based LLMs) effectively integrate visual
    and temporal information from video data to gain significant achievement in multiple
    video-related tasks. Numerous approaches [[11](#bib.bib11), [15](#bib.bib15),
    [32](#bib.bib32)] have been proposed to address the challenges associated with
    video-based LLMs, such as incorporating different architectures and training processes
    to enhance the models’ ability to capture and process complex video information.
    Concretely, Video-ChatGPT [[15](#bib.bib15)] is based on the LLaVA framework and
    incorporates average pooling to improve the perception of temporal sequences.
    VideoChat [[11](#bib.bib11)] employs the QFormer to map visual representations
    to Vicuna, executing a two-stage training process. Video-LLaMA [[32](#bib.bib32)]
    integrates a frame embedding layer and ImageBind to introduce temporal and audio
    information into the LLM backbone. This alignment between videos and LLMs facilitates
    visual context-aware interaction, surpassing the capabilities of LLMs. However,
    these models are still susceptible to adversarial attacks.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 基于视频的大型语言模型（基于视频的LLMs）有效地整合了来自视频数据的视觉和时间信息，在多个视频相关任务中取得了显著成果。已经提出了许多方法[[11](#bib.bib11),
    [15](#bib.bib15), [32](#bib.bib32)]来应对与基于视频的LLMs相关的挑战，例如，采用不同的架构和训练过程以增强模型捕捉和处理复杂视频信息的能力。具体而言，Video-ChatGPT
    [[15](#bib.bib15)]基于LLaVA框架，并引入平均池化以提高对时间序列的感知。VideoChat [[11](#bib.bib11)]使用QFormer将视觉表示映射到Vicuna，并执行两阶段训练过程。Video-LLaMA
    [[32](#bib.bib32)]结合了帧嵌入层和ImageBind，将时间和音频信息引入LLM主干。这种视频与LLMs之间的对齐促进了视觉上下文感知交互，超越了LLMs的能力。然而，这些模型仍然容易受到对抗攻击。
- en: 2.2 Adversarial Attack
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 对抗攻击
- en: Adversarial attacks [[27](#bib.bib27), [34](#bib.bib34)] have been widely studied
    in the context of classification models, where imperceptible and carefully crafted
    perturbations are applied to input data to mislead the model into producing incorrect
    predictions. Inspired by the adversarial vulnerability observed in vision tasks,
    early efforts are devoted to investigating adversarial attacks against large multi-modal
    models [[18](#bib.bib18), [36](#bib.bib36), [8](#bib.bib8), [31](#bib.bib31),
    [37](#bib.bib37)], such as vision large language models (VLLMs) [[35](#bib.bib35),
    [26](#bib.bib26), [13](#bib.bib13), [19](#bib.bib19)] or text-to-image diffusion
    models [[21](#bib.bib21)], etc. These adversarial attacks have been designed to
    manipulate these large multi-modal models into generating specific or even harmful
    outputs. Despite these advancements, the adversarial robustness in video-based
    LLMs remains unexplored. Addressing this gap, we introduce the first adversarial
    attacks specifically tailored for video-based LLMs in this paper.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击[[27](#bib.bib27), [34](#bib.bib34)]在分类模型的背景下已被广泛研究，其中对输入数据施加不可察觉且精心设计的扰动，以误导模型产生错误预测。受到视觉任务中观察到的对抗脆弱性的启发，早期工作致力于研究针对大型多模态模型的对抗攻击[[18](#bib.bib18),
    [36](#bib.bib36), [8](#bib.bib8), [31](#bib.bib31), [37](#bib.bib37)]，例如视觉大型语言模型（VLLMs）[[35](#bib.bib35),
    [26](#bib.bib26), [13](#bib.bib13), [19](#bib.bib19)]或文本到图像的扩散模型[[21](#bib.bib21)]等。这些对抗攻击旨在操纵这些大型多模态模型生成特定甚至有害的输出。尽管取得了这些进展，基于视频的LLMs中的对抗鲁棒性仍然未被探索。为了填补这一空白，我们在本文中首次介绍了专门针对基于视频的LLMs的对抗攻击。
- en: 3 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we first describe the threat model and problem formulation,
    and subsequently outline the key objective functions of our proposed FMM-Attack.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们首先描述威胁模型和问题的公式化，然后概述我们提出的FMM-Attack的关键目标函数。
- en: '![Refer to caption](img/8f4729fa6cfc74877730fbc0090d2b51.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8f4729fa6cfc74877730fbc0090d2b51.png)'
- en: 'Figure 2: Schematics of our FMM-Attack. The figure demonstrates our attack
    approach for maximizing the video-video features as defined in Eq. [4](#S3.E4
    "Equation 4 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs") and the LLM-LLM features
    in Eq. [5](#S3.E5 "Equation 5 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"). We refer to
    adversarial examples generated by our attack strategies as  representing the adversarial
    perturbation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：我们的FMM-Attack示意图。该图展示了我们攻击方法以最大化视频-视频特征，如公式 [4](#S3.E4 "Equation 4 ‣ 3.4
    Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") 和LLM-LLM特征如公式 [5](#S3.E5 "Equation 5
    ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") 所定义。我们将通过我们的攻击策略生成的对抗样本称为表示对抗扰动。'
- en: 3.1 Threat model
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 威胁模型
- en: Goals and capabilities. The goal is to craft an imperceptible adversarial perturbation
    for videos, which can induce video-based LLMs to generate an incorrect sequence
    during the victim model’s deployment. Following the most commonly used constraint
    for the involved perturbation, it is restricted within a predefined magnitude
    in the $l_{p}$ norm, ensuring it is difficult to detect.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 目标和能力。目标是为视频生成一个不可察觉的对抗扰动，这可以使视频基础的LLMs在受害模型部署期间生成错误的序列。根据最常用的扰动约束，它被限制在预定义的
    $l_{p}$ 范数的幅度内，确保其难以被检测到。
- en: Knowledge and background. As suggested in [[2](#bib.bib2), [18](#bib.bib18)],
    we assume that the victim video-based LLMs can be accessed in full knowledge,
    including architectures and parameters. Additionally, we consider a more challenging
    scenario where the victim video-based LLMs are inaccessible, as detailed in the
    Appendix.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 知识和背景。根据 [[2](#bib.bib2), [18](#bib.bib18)] 的建议，我们假设受害的视频基础LLMs可以完全访问，包括架构和参数。此外，我们还考虑了一个更具挑战性的场景，其中受害视频基础LLMs不可访问，详见附录。
- en: '3.2 Preliminary: the Pipeline of Video-based LLMs'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 初步：视频基础LLMs的流程
- en: 'Let , composed of a video feature extractor . Consider a clean video  for the
    video. To provide a response, video-based large language models , and subsequently,
    generate predefined prompts based on a consistent template to concatenate both
    video features and text queries as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设，包含一个视频特征提取器。考虑一个干净的视频。为了提供响应，视频基础的大型语言模型，然后生成基于一致模板的预定义提示，将视频特征和文本查询连接如下：
- en: 'USER:  Assistant:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 'USER:  Assistant:'
- en: Then, the predefined prompts are processed by LLMs . It is important to mention
    that, to ensure the loss function remains minimal, we use the hidden state $A_{hidden}$
    before the final layer in our FMM-Attack.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，预定义的提示由LLMs处理。需要提到的是，为了确保损失函数保持最小，我们在FMM-Attack中使用了最终层之前的隐藏状态 $A_{hidden}$。
- en: 3.3 Problem Formulation
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 问题公式化
- en: 'The goal of generating adversarial examples , where , and formulate the overall
    objective function as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗样本的目标，其中，形成总体目标函数如下：
- en: '|  | $\arg\min_{\Delta}\lambda\&#124;\Delta\&#124;_{2,1}-\ell(Y,\mathcal{F}_{\theta}(Q_{text},\hat{\mathbf{X}})),$
    |  | (1) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\arg\min_{\Delta}\lambda\&#124;\Delta\&#124;_{2,1}-\ell(Y,\mathcal{F}_{\theta}(Q_{text},\hat{\mathbf{X}})),$
    |  | (1) |'
- en: 'where  and  is the loss function used to measure the difference between the
    predicted and ground truth answers. Furthermore, in a more realistic scenario,
    the ground truth answer  instead of $Y$. Therefore, the overall objective function
    in Eq. [1](#S3.E1 "Equation 1 ‣ 3.3 Problem Formulation ‣ 3 Methodology ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs") can be further
    formulated as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '其中和是用于衡量预测答案与实际答案之间差异的损失函数。此外，在更现实的场景中，实际答案而不是 $Y$。因此，公式 [1](#S3.E1 "Equation
    1 ‣ 3.3 Problem Formulation ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") 中的总体目标函数可以进一步公式化如下：'
- en: '|  | $1$2 |  | (2) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'The $\ell_{2,1}$ norm [[27](#bib.bib27)] is employed to quantify the magnitude
    of the perturbation, which is defined as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $\ell_{2,1}$ 范数 [[27](#bib.bib27)] 被用来量化扰动的幅度，其定义如下：
- en: '|  | $\&#124;\Delta\&#124;_{2,1}=\sum_{i}^{T}\&#124;\Delta_{i}\&#124;_{2},$
    |  | (3) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\&#124;\Delta\&#124;_{2,1}=\sum_{i}^{T}\&#124;\Delta_{i}\&#124;_{2},$
    |  | (3) |'
- en: where -th frame in  norm applies the $l_{1}$ norm across frames, ensuring the
    sparsity of generated perturbations.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中-th 帧在 norm 应用 $l_{1}$ 范数，确保生成扰动的稀疏性。
- en: 3.4 Optimization Objective
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 优化目标
- en: 'Our proposed FMM-Attack is to induce video-based LLMs to generate incorrect
    responses with imperceptible adversarial perturbations. Two losses are proposed
    from the perspective of video features  in Eq. [5](#S3.E5 "Equation 5 ‣ 3.4 Optimization
    Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack
    on Video-based LLMs"). Moreover, inspired by the idea that video clipping can
    improve video comprehension by selecting the most effective frames, a flow-based
    temporal mask  can filter out similar frames, ensuring the effectiveness of imperceptible
    adversarial perturbations while achieving increased sparsity.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 FMM-Attack 旨在引导基于视频的 LLM 生成错误的响应，同时使对抗扰动不可察觉。两个损失函数从视频特征的角度提出，如 Eq. [5](#S3.E5
    "方程 5 ‣ 3.4 优化目标 ‣ 3 方法论 ‣ FMM-Attack：一种基于流的多模态对抗攻击针对视频基础 LLM")。此外，受视频剪辑通过选择最有效帧来改善视频理解的启发，我们提出了一种基于流的时间掩模，可以过滤掉相似帧，确保对抗扰动的有效性，同时实现更高的稀疏性。
- en: 'Video Features Loss. Video-based LLMs first use a video feature extractor .
    We simply adopt MSE loss to measure the distance of video features between the
    clean video . Hence, the video features loss can be formulated as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 视频特征损失。视频基础 LLM 首先使用视频特征提取器。我们简单地采用 MSE 损失来测量清洁视频之间的视频特征距离。因此，视频特征损失可以表示为：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where  and -th elements of the clean and adversarial video features, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 其中和是干净和对抗视频特征的-th 元素，分别。
- en: 'LLM Features Loss. In addition to the deviation of the original feature space
    in video domains of video-based LLMs, we also consider that in textual domains
    to enhance the attack effect. Given a hidden state from the final layer of LLMs  and
    the adversarial video $\hat{\mathbf{X}}$ can be formulated as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 特征损失。除了考虑视频领域中视频基础 LLM 的原始特征空间的偏差外，我们还考虑文本领域中的偏差，以增强攻击效果。给定 LLM 最终层的隐藏状态和对抗视频
    $\hat{\mathbf{X}}$ 可以表示为：
- en: '|  | $$\begin{split}\ell_{LLM}(A_{hidden},\hat{A}_{hidden})&amp;=\frac{1}{n}\sum_{i=1}^{n}(A_{hidden_{i}}-\hat{A}_{hidden_{i}})^{2}\\
    &amp;=\frac{1}{n}\sum_{i=1}^{n}(g_{\psi}(Q_{text},Q_{video})_{i}-g_{\psi}(Q_{text},\hat{Q}_{video})_{i})^{2}\\'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}\ell_{LLM}(A_{hidden},\hat{A}_{hidden})&amp;=\frac{1}{n}\sum_{i=1}^{n}(A_{hidden_{i}}-\hat{A}_{hidden_{i}})^{2}\\
    &amp;=\frac{1}{n}\sum_{i=1}^{n}(g_{\psi}(Q_{text},Q_{video})_{i}-g_{\psi}(Q_{text},\hat{Q}_{video})_{i})^{2}\\'
- en: '&amp;=\frac{1}{n}\sum_{i=1}^{n}(g_{\psi}(Q_{text},f_{\phi}(\mathbf{X}))_{i}-g_{\psi}(Q_{text},f_{\phi}(\hat{\mathbf{X}}))_{i})^{2},\end{split}$$
    |  | (5) |'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;=\frac{1}{n}\sum_{i=1}^{n}(g_{\psi}(Q_{text},f_{\phi}(\mathbf{X}))_{i}-g_{\psi}(Q_{text},f_{\phi}(\hat{\mathbf{X}}))_{i})^{2},\end{split}$$
    |  | (5) |'
- en: where  are the  is the total number of elements in the features.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是特征中的元素总数。
- en: 'Flow-based Temporal Mask. Video-based LLMs [[4](#bib.bib4), [30](#bib.bib30)]
    adopt video clipping to split and select the most effective frames in a video,
    which can enhance video understanding. Inspired by these advantages, we propose
    a flow-based temporal mask,  frames with the most significant movement and changes.
    See Sec. [4.3](#S4.SS3 "4.3 Discussions ‣ 4 Experiments ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs") for a detailed discussion.
    Specifically, we initialize a binary mask  assigns a value of 1 to the top , our
    proposed FMM-Attack can achieve more sparse adversarial perturbations in both
    temporal and spatial domains. For temporal sparsity, a flow-based temporal mask  norm
    of adversarial perturbations in Eq. [3](#S3.E3 "Equation 3 ‣ 3.3 Problem Formulation
    ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs") is employed to constrain the spatial perturbation magnitude in each frame.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 基于流的时间掩模。视频基础 LLM [[4](#bib.bib4), [30](#bib.bib30)] 采用视频剪辑来分割和选择视频中最有效的帧，这可以增强视频理解。受这些优点的启发，我们提出了一种基于流的时间掩模，帧具有最显著的运动和变化。详细讨论请见
    Sec. [4.3](#S4.SS3 "4.3 讨论 ‣ 4 实验 ‣ FMM-Attack：一种基于流的多模态对抗攻击针对视频基础 LLM")。具体来说，我们初始化一个二进制掩模，将值
    1 分配给前几个帧，我们提出的 FMM-Attack 可以在时间和空间域中实现更稀疏的对抗扰动。对于时间稀疏性，基于流的时间掩模 norm 用于约束每帧中的空间扰动幅度，如
    Eq. [3](#S3.E3 "方程 3 ‣ 3.3 问题表述 ‣ 3 方法论 ‣ FMM-Attack：一种基于流的多模态对抗攻击针对视频基础 LLM")。
- en: 'Overall Optimization Objective. To sum up, combined flow-based temporal mask  and
    $\ell_{LLM}$), the overall objective function in Eq. [1](#S3.E1 "Equation 1 ‣
    3.3 Problem Formulation ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") can be further formalized as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '总体优化目标。总结一下，结合流基的时间掩码和$\ell_{LLM}$)，整体目标函数在公式 [1](#S3.E1 "Equation 1 ‣ 3.3
    Problem Formulation ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs")中可以进一步形式化为：'
- en: '|  | $1$2 |  | (6) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: 'where  correspond to the three loss weights, which aim to balance them during
    the optimization. Our overall attack procedure is described in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '其中， 代表三个损失权重，旨在在优化过程中平衡它们。我们的整体攻击程序在算法 [1](#alg1 "Algorithm 1 ‣ 3.4 Optimization
    Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack
    on Video-based LLMs")中描述。'
- en: Algorithm 1 FMM-Attack on Video-based LLMs Using PGD Optimization
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 使用PGD优化的视频基础LLMs的FMM-Attack
- en: '0:  Clean video , sparsity , LLM , iterations 1:  Compute video optical flow
    and obtain flow-based temporal mask 2:  Initialize perturbation  do4:     Calculate
    video features loss  using Eq. [5](#S3.E5 "Equation 5 ‣ 3.4 Optimization Objective
    ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs")6:     Update perturbation 7:  end while8:  Compute adversarial video'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  清洁视频，稀疏性，LLM，迭代1:  计算视频光流并获得基于光流的时间掩码2:  初始化扰动do4:     计算视频特征损失使用公式 [5](#S3.E5
    "Equation 5 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs")6:     更新扰动7:  结束 while8:  计算对抗视频'
- en: 'Extended Targeted Version of FMM-Attack. As previously mentioned, the proposed
    FMM-Attack is initially designed for untargeted adversarial attacks. However,
    its flexibility allows for an extension to a targeted version as well. In targeted
    scenarios, the goal is transformed to craft an imperceptible adversarial perturbation
    for videos, which can induce video-based LLMs to generate a targeted sequence.
    To achieve such a targeted adversarial attack, we first obtain the targeted video  in
    the untargeted objective of Eq. [6](#S3.E6 "Equation 6 ‣ 3.4 Optimization Objective
    ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs") with the targeted video $\mathbf{X}_{t}$, the overall objective function
    of targeted FMM-Attack can be formulated as:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 'FMM-Attack的扩展目标版本。如前所述，提出的FMM-Attack最初是为非目标对抗攻击设计的。然而，它的灵活性允许将其扩展为目标版本。在目标场景中，目标是生成一个不可察觉的对抗扰动，使视频基础LLMs生成一个特定的序列。为了实现这种有针对性的对抗攻击，我们首先在公式 [6](#S3.E6
    "Equation 6 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs")中的非目标目标下获取目标视频$\mathbf{X}_{t}$，目标FMM-Attack的整体目标函数可以形式化为：'
- en: '|  | $1$2 |  | (7) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 4 Experiments
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we showcase the effectiveness of our proposed FMM-Attack, evaluated
    on zero-shot question-answer datasets. In addition, we conduct several ablation
    study experiments on the loss of different modalities, perturbation budget , ),
    and selection of video frames.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了我们提出的FMM-Attack的有效性，并在零样本问答数据集上进行了评估。此外，我们还对不同模态的损失、扰动预算和视频帧的选择进行了多次消融研究实验。
- en: 4.1 Implementation Details
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实现细节
- en: Models and datasets. We assess open-source and state-of-the-art video-based
    LLMs such as Video-ChatGPT [[15](#bib.bib15)] and VideoChat [[11](#bib.bib11)]
    (VideoChat [[11](#bib.bib11)] is in the Appendix), ensuring reproducibility of
    our results. Concretely, we adopt Video-ChatGPT and VideoChat with a LLaMA-7B
    LLM [[23](#bib.bib23)]. In line with the Video-ChatGPT [[15](#bib.bib15)] methodology,
    we curate a test set based on the ActivityNet-200 [[3](#bib.bib3)] and MSVD-QA [[29](#bib.bib29)]
    datasets.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们评估了开源和最先进的视频基础LLMs，例如Video-ChatGPT [[15](#bib.bib15)] 和 VideoChat [[11](#bib.bib11)]（VideoChat [[11](#bib.bib11)]
    在附录中），以确保我们结果的可重复性。具体来说，我们采用了Video-ChatGPT和VideoChat，结合了LLaMA-7B LLM [[23](#bib.bib23)]。根据Video-ChatGPT [[15](#bib.bib15)]的方法，我们基于ActivityNet-200 [[3](#bib.bib3)]
    和 MSVD-QA [[29](#bib.bib29)] 数据集来策划测试集。
- en: Setups. The perturbation limit  and step size ,  are set to 2, 1, and 3 respectively.
    Each experiment is run on a single NVIDIA-V100 GPU.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。扰动限制和步长设置为2、1和3。每个实验在单个NVIDIA-V100 GPU上运行。
- en: 'Baselines. For evaluation, we design three spatial baselines, including videos
    with random perturbations, black videos with all pixel values set to 0, and white
    videos with all pixel values set to 1\. In addition, we compare our proposed flow-based
    temporal mask with two straightforward temporal mask methods, serving as temporal
    mask baselines: the sequence temporal mask and the random temporal mask. Specifically,
    the sequence temporal mask consists of a continuous sequence of frame indices,
    while the random temporal mask comprises a randomly chosen sequence of frame indices.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。为了评估，我们设计了三个空间基线，包括具有随机扰动的视频、所有像素值为0的黑色视频以及所有像素值为1的白色视频。此外，我们将我们提出的基于流的时间掩码与两种简单的时间掩码方法进行了比较，这些方法作为时间掩码基线：序列时间掩码和随机时间掩码。具体而言，序列时间掩码由连续的帧索引序列组成，而随机时间掩码则由随机选择的帧索引序列组成。
- en: Metrics. We utilize a variety of evaluation metrics to assess the robustness
    of the models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们利用多种评估指标来评估模型的鲁棒性。
- en: (a) CLIP Score. CLIP [[20](#bib.bib20)] score characterizes the semantic similarity
    between the adversarial answer and the ground-truth answer. A lower CLIP score
    signifies a lower semantic correlation between the adversarial answer and the
    ground-truth answer, indicating a more effective attack.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CLIP得分。CLIP [[20](#bib.bib20)]得分描述了对抗性答案与真实答案之间的语义相似性。CLIP得分越低，表示对抗性答案与真实答案之间的语义关联越低，说明攻击效果越好。
- en: (b) Image Captioning Metrics. Various metrics such as BLEU [[17](#bib.bib17)],
    ROUGE-L [[12](#bib.bib12)], and CIDEr [[24](#bib.bib24)] are used to evaluate
    the quality of the adversarial answer generated by the model. A lower score corresponds
    to a more effective attack.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 图像标题评分指标。使用各种指标，如BLEU [[17](#bib.bib17)]、ROUGE-L [[12](#bib.bib12)]和CIDEr [[24](#bib.bib24)]来评估模型生成的对抗性答案的质量。得分越低，表示攻击效果越好。
- en: (c) GPT Score. Following Video-ChatGPT [[15](#bib.bib15)] and Video-LLaMA [[32](#bib.bib32)].
    We also employ an evaluation pipeline using the GPT-3.5 and GPT-4 models. The
    pipeline employs GPT to assign a score from 1 to 5, evaluating the similarity
    between the output sentence and the ground truth, and a binary score (0 or 1)
    to measure its accuracy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GPT得分。参考Video-ChatGPT [[15](#bib.bib15)]和Video-LLaMA [[32](#bib.bib32)]。我们还使用了一个评估管道，采用GPT-3.5和GPT-4模型。该管道使用GPT从1到5进行评分，评估输出句子与真实值之间的相似度，并使用二进制得分（0或1）来衡量其准确性。
- en: (d) Sparsity. Sparsity refers to the ratio of frames without perturbations (clean
    frames) to the total number of frames in a specific video. The sparsity , where  is
    the total number of frames in a video. The bigger the value of ${M}_{spa}$, the
    sparser the perturbation distribution, indicating that fewer video frames are
    attacked.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 稀疏度。稀疏度指的是没有扰动的帧（干净帧）与特定视频中总帧数的比例。稀疏度，其中是视频中的总帧数。${M}_{spa}$值越大，扰动分布越稀疏，表示攻击的视频帧越少。
- en: 4.2 Main Results
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'Table 1: White-box attacks against Video-ChatGPT [[15](#bib.bib15)] on the
    ActivityNet-200 [[3](#bib.bib3)] dataset and the MSVD-QA [[29](#bib.bib29)] dataset:
    Comparison of CLIP score, image caption metrics and GPT score for different attack
    types. Random spatial attack denotes random perturbations added to video frames,
    and Black spatial attack and White spatial attack denote video frames being all
    0 and all 1, respectively. The sparsity of the temporal mask is set to 0\. ${\Delta}$:
    the mean of the modified pixels. Our attack is optimized based on Eq. [6](#S3.E6
    "Equation 6 ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs").'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在ActivityNet-200 [[3](#bib.bib3)]数据集和MSVD-QA [[29](#bib.bib29)]数据集上对白盒攻击的比较。比较了不同攻击类型的CLIP得分、图像标题指标和GPT得分。随机空间攻击表示对视频帧添加随机扰动，黑色空间攻击和白色空间攻击分别表示视频帧全为0和全为1。时间掩码的稀疏度设置为0\.
    ${\Delta}$：修改像素的均值。我们的攻击基于Eq. [6](#S3.E6 "方程6 ‣ 3.4 优化目标 ‣ 3 方法论 ‣ FMM-Attack：一种基于流的多模态对抗攻击视频基础的LLMs")进行优化。
- en: '| Dataset | Type |  | Image Caption  | GPT-4 $\downarrow$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 |  | 图像标题 | GPT-4 $\downarrow$ |'
- en: '| RN50 | RN101 | BLEU | ROUGE-L | Accurate | Score | Accurate | Score |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | RN101 | BLEU | ROUGE-L | 准确性 | 得分 | 准确性 | 得分 |'
- en: '| ActivityNet [[3](#bib.bib3)] | Clean | 0 | 0.7817 | 0.7827 | 0.2029 | 0.4820
    | 0.50 | 3.20 | 0.33 | 2.10 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ActivityNet [[3](#bib.bib3)] | 干净 | 0 | 0.7817 | 0.7827 | 0.2029 | 0.4820
    | 0.50 | 3.20 | 0.33 | 2.10 |'
- en: '| Random | 8 | 0.7637 | 0.7681 | 0.1986 | 0.4793 | 0.45 | 3.10 | 0.33 | 2.03
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 8 | 0.7637 | 0.7681 | 0.1986 | 0.4793 | 0.45 | 3.10 | 0.33 | 2.03 |'
- en: '| Black | 100 | 0.7661 | 0.7676 | 0.1691 | 0.4570 | 0.30 | 2.50 | 0.17 | 0.96
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Black | 100 | 0.7661 | 0.7676 | 0.1691 | 0.4570 | 0.30 | 2.50 | 0.17 | 0.96
    |'
- en: '| White | 148 | 0.7564 | 0.7534 | 0.1689 | 0.4545 | 0.31 | 2.60 | 0.19 | 1.24
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| White | 148 | 0.7564 | 0.7534 | 0.1689 | 0.4545 | 0.31 | 2.60 | 0.19 | 1.24
    |'
- en: '| FMM | 8 | 0.6211 | 0.6274 | 0.1336 | 0.3694 | 0.20 | 1.64 | 0.13 | 0.88 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| FMM | 8 | 0.6211 | 0.6274 | 0.1336 | 0.3694 | 0.20 | 1.64 | 0.13 | 0.88 |'
- en: '| MSVD-QA [[29](#bib.bib29)] | Clean | 0 | 0.8322 | 0.8180 | 0.3864 | 0.6843
    | 0.62 | 3.84 | 0.60 | 3.12 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| MSVD-QA [[29](#bib.bib29)] | Clean | 0 | 0.8322 | 0.8180 | 0.3864 | 0.6843
    | 0.62 | 3.84 | 0.60 | 3.12 |'
- en: '| Random | 8 | 0.8249 | 0.8141 | 0.4107 | 0.7042 | 0.58 | 3.72 | 0.60 | 3.08
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Random | 8 | 0.8249 | 0.8141 | 0.4107 | 0.7042 | 0.58 | 3.72 | 0.60 | 3.08
    |'
- en: '| Black | 110 | 0.8145 | 0.7902 | 0.3548 | 0.6478 | 0.46 | 3.26 | 0.40 | 2.12
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Black | 110 | 0.8145 | 0.7902 | 0.3548 | 0.6478 | 0.46 | 3.26 | 0.40 | 2.12
    |'
- en: '| White | 142 | 0.8057 | 0.8090 | 0.3969 | 0.6736 | 0.48 | 3.36 | 0.44 | 2.28
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| White | 142 | 0.8057 | 0.8090 | 0.3969 | 0.6736 | 0.48 | 3.36 | 0.44 | 2.28
    |'
- en: '| FMM | 8 | 0.7337 | 0.7181 | 0.3240 | 0.5746 | 0.36 | 2.92 | 0.34 | 1.84 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| FMM | 8 | 0.7337 | 0.7181 | 0.3240 | 0.5746 | 0.36 | 2.92 | 0.34 | 1.84 |'
- en: '![Refer to caption](img/8e6647150c05fd765cf0dd4d2534e9b0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8e6647150c05fd765cf0dd4d2534e9b0.png)'
- en: 'Figure 3: Perturbed videos generated by Video-ChatGPT.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：由 Video-ChatGPT 生成的扰动视频。
- en: 'Table 2: White-box attacks against Video-ChatGPT [[15](#bib.bib15)] on the
    ActivityNet-200 [[3](#bib.bib3)] dataset. seq: sequence temporal mask. random:
    random temporal mask. FMM: our flow-based temporal mask attack. ${M}_{spa}$: Sparsity
    of temporal mask. Our attack is optimized based on Eq. [6](#S3.E6 "Equation 6
    ‣ 3.4 Optimization Objective ‣ 3 Methodology ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs").'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：对 ActivityNet-200 [[3](#bib.bib3)] 数据集上的 Video-ChatGPT [[15](#bib.bib15)]
    进行的白盒攻击。seq：序列时间掩码。random：随机时间掩码。FMM：我们基于流的方法时间掩码攻击。${M}_{spa}$：时间掩码的稀疏性。我们的攻击是基于公式 [6](#S3.E6
    "公式 6 ‣ 3.4 优化目标 ‣ 3 方法论 ‣ FMM-攻击：基于流的多模态对抗攻击视频 LLM")优化的。
- en: '| Metrics |  |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 指标 |  |  |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| seq | random | FMM | seq | random | FMM | seq | random | FMM | seq | random
    | FMM |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| seq | random | FMM | seq | random | FMM | seq | random | FMM | seq | random
    | FMM |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| RN50 | 0.7500 | 0.7142 | 0.6821 | 0.7578 | 0.7559 | 0.7460 | 0.7583 | 0.7671
    | 0.7510 | 0.7715 | 0.7813 | 0.7349 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | 0.7500 | 0.7142 | 0.6821 | 0.7578 | 0.7559 | 0.7460 | 0.7583 | 0.7671
    | 0.7510 | 0.7715 | 0.7813 | 0.7349 |'
- en: '| RN101 | 0.7568 | 0.7251 | 0.7085 | 0.7500 | 0.7588 | 0.7559 | 0.7764 | 0.7769
    | 0.7500 | 0.7788 | 0.7827 | 0.7637 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| RN101 | 0.7568 | 0.7251 | 0.7085 | 0.7500 | 0.7588 | 0.7559 | 0.7764 | 0.7769
    | 0.7500 | 0.7788 | 0.7827 | 0.7637 |'
- en: '| BLEU | 0.1572 | 0.1590 | 0.1527 | 0.1606 | 0.1751 | 0.1485 | 0.1900 | 0.1894
    | 0.1830 | 0.2000 | 0.1957 | 0.1940 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | 0.1572 | 0.1590 | 0.1527 | 0.1606 | 0.1751 | 0.1485 | 0.1900 | 0.1894
    | 0.1830 | 0.2000 | 0.1957 | 0.1940 |'
- en: '| ROUGE-L | 0.4073 | 0.3996 | 0.4109 | 0.4323 | 0.4458 | 0.4053 | 0.4767 |
    0.4727 | 0.4713 | 0.4779 | 0.4658 | 0.4731 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ROUGE-L | 0.4073 | 0.3996 | 0.4109 | 0.4323 | 0.4458 | 0.4053 | 0.4767 |
    0.4727 | 0.4713 | 0.4779 | 0.4658 | 0.4731 |'
- en: '| GPT3.5 | 2.02 | 2.01 | 2.00 | 2.45 | 2.24 | 2.22 | 2.64 | 2.63 | 2.62 | 2.76
    | 2.78 | 2.75 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| GPT3.5 | 2.02 | 2.01 | 2.00 | 2.45 | 2.24 | 2.22 | 2.64 | 2.63 | 2.62 | 2.76
    | 2.78 | 2.75 |'
- en: '| GPT4 | 1.09 | 1.08 | 1.07 | 1.35 | 1.34 | 1.33 | 1.63 | 1.63 | 1.61 | 1.99
    | 1.58 | 1.54 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GPT4 | 1.09 | 1.08 | 1.07 | 1.35 | 1.34 | 1.33 | 1.63 | 1.63 | 1.61 | 1.99
    | 1.58 | 1.54 |'
- en: Quantitative Evaluation. We performed an extensive quantitative analysis using
    the popular open-ended question-answer datasets ActivityNet-200 [[3](#bib.bib3)]
    and MSVD-QA [[29](#bib.bib29)].
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 定量评估。我们使用了流行的开放式问题回答数据集 ActivityNet-200 [[3](#bib.bib3)] 和 MSVD-QA [[29](#bib.bib29)]
    进行了广泛的定量分析。
- en: 'As depicted in the left of Table [1](#S4.T1 "Table 1 ‣ 4.2 Main Results ‣ 4
    Experiments ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), the random, black, white, and flow-based attacks all significantly decrease
    the clip score and image caption score compared to the original clean videos.
    Among these, our proposed FMM-Attack yields the most significant results. As demonstrated
    in the right of Table [1](#S4.T1 "Table 1 ‣ 4.2 Main Results ‣ 4 Experiments ‣
    FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"),
    the random, black, white, and flow-based attacks all significantly decrease the
    GPT scores and accuracies. Among them, the FMM-Attack achieves the most substantial
    results.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[1](#S4.T1 "表 1 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")左侧所示，随机、黑色、白色和基于流的攻击都显著降低了剪辑得分和图像说明得分，相较于原始干净视频。其中，我们提出的FMM-Attack产生了最显著的结果。如表[1](#S4.T1
    "表 1 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")右侧所示，随机、黑色、白色和基于流的攻击都显著降低了GPT得分和准确性。其中，FMM-Attack取得了最显著的结果。'
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs") compares different
    mask ratios (sparsity) and temporal mask approaches. Our FMM-Attack outperforms
    the other two approaches across various sparsity levels, indicating the effectiveness
    of our proposed flow-based temporal mask, which leverages the concept of maximum
    flow prioritization. It demonstrates its potency in the realm of video-based LLM
    and its ability to maximize the extraction of video information. In the bottom
    of Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"), FMM-Attack
    consistently outperforms the other two approaches across various sparsity levels,
    resulting in a more significant reduction of GPT scores and accuracies.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "表 2 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")比较了不同的掩码比例（稀疏性）和时间掩码方法。我们的FMM-Attack在各种稀疏性水平上都优于其他两种方法，显示了我们提出的基于流的时间掩码的有效性，该掩码利用了最大流优先的概念。它在视频LLM领域展示了其强大能力，并能够最大化视频信息的提取。在表[2](#S4.T2
    "表 2 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")底部，FMM-Attack在各种稀疏性水平上始终优于其他两种方法，导致GPT得分和准确性的显著下降。'
- en: 'Qualitative Evaluation. We also present qualitative examples (see Fig. [3](#S4.F3
    "Figure 3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs")) of the attacked videos, in which the
    model produces garbled responses without any meaningful content. Fig. [3](#S4.F3
    "Figure 3 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") vividly illustrates the chaos induced
    in the model’s responses by our subtle and imperceptible attacks. This signals
    a clear need for enhancing the robustness of the model. It is noteworthy that
    the response generated from the clean video forms a coherent sentence strongly
    correlated to the corresponding question. However, in the case of the attacked
    video, the responses consist of repetitive words that lack meaningful context
    or incorrect answers. This clearly demonstrates the potent obfuscation effect
    of our attack.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '定性评估。我们还展示了攻击视频的定性示例（见图[3](#S4.F3 "图 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")），其中模型生成了混乱的回应，没有任何有意义的内容。图[3](#S4.F3
    "图 3 ‣ 4.2 主要结果 ‣ 4 实验 ‣ FMM-Attack: 基于流的多模态对抗攻击视频LLMs")生动地展示了我们细微且不可察觉的攻击对模型回应造成的混乱。这清楚地表明了提升模型鲁棒性的明确需求。值得注意的是，从干净视频生成的回应形成了一个与相应问题强相关的连贯句子。然而，在攻击视频的情况下，回应由重复的词语组成，缺乏有意义的上下文或错误的答案。这清楚地展示了我们攻击的强大混淆效应。'
- en: '![Refer to caption](img/bbaefb74b2e1ac546a14476c4c0b7260.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/bbaefb74b2e1ac546a14476c4c0b7260.png)'
- en: 'Figure 4: Relationship between optical flow and key frames. ‘Clip Score of
    Adjacent Frames’ describes the similarity between the current frame and its adjacent
    frames, the smaller this score is the more different the current frame is. ‘Clip
    Score of Answer and Current Frame’ indicates the similarity between the current
    frame and the answer corresponding to the user’s input question, the larger the
    score indicates that the current frame contains more information about the answer.
    The frames selected by flow-based masks in our FMM-Attack are key frames in the
    video.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：光流与关键帧的关系。‘邻近帧的剪辑得分’描述了当前帧与其邻近帧之间的相似度，这个得分越小，当前帧与邻近帧的差异越大。‘回答与当前帧的剪辑得分’表示当前帧与用户输入问题对应的回答之间的相似度，得分越高表示当前帧包含的回答信息越多。我们
    FMM-Attack 中由光流掩码选择的帧是视频中的关键帧。
- en: 4.3 Discussions
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 讨论
- en: 'Essence of Flow-based Masks. We address the essence of flow-based temporal
    masks in our FMM-Attack. The flow-based masks by selecting key frames method is
    a powerful tool for video understanding and manipulation, which allows for precise
    control over specific elements in a video sequence, making it easier to edit and
    manipulate the video in a variety of ways. We use the clip image-image score of
    adjacent frames and clip text-image score between the answer and current frame
    to assess the importance and non-fungibility of our selected frames in FMM-Attack,
    where a smaller clip image-image score suggests less similarity between a frame
    and its adjacent frames, and a bigger clip text-image score suggests more similarity
    between the current frame and the answer of the user input. As depicted in Fig. [4](#S4.F4
    "Figure 4 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs"), a larger optical flow corresponds to
    a higher inconsistency between the current frame and its neighboring frames, while
    containing more information about the answer. This observation suggests that the
    frames selected using our FMM-Attack are crucial frames in the video, and attacking
    them will yield more effective results.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '光流掩码的本质。我们在 FMM-Attack 中探讨了光流基于时间掩码的本质。通过选择关键帧的方法生成的光流掩码是视频理解和操控的强大工具，它允许对视频序列中的特定元素进行精确控制，使得以各种方式编辑和操控视频变得更加容易。我们使用邻近帧的剪辑图像-图像得分和回答与当前帧之间的剪辑文本-图像得分来评估
    FMM-Attack 中所选帧的重要性和不可替代性，其中较小的剪辑图像-图像得分表明帧与其邻近帧之间的相似度较低，较大的剪辑文本-图像得分则表明当前帧与用户输入的回答之间的相似度较高。如图[4](#S4.F4
    "Figure 4 ‣ 4.2 Main Results ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs")所示，较大的光流对应于当前帧与其邻近帧之间的一致性较低，同时包含更多关于回答的信息。这一观察结果表明，通过我们的
    FMM-Attack 选择的帧是视频中的关键帧，攻击这些帧将产生更有效的结果。'
- en: 'Garbling Effect. Intriguingly, our proposed FMM-Attack induces garbling in
    the model output, while the other three attack methods do not cause such distortion.
    This suggests that the FMM-Attack not only diminishes the model’s cue information
    but also prompts the model to hallucinate. Furthermore, as shown in Fig. [5](#S4.F5
    "Figure 5 ‣ 4.3 Discussions ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs"), we analyse the number of successfully
    attacked Video-ChatGPT in ActivityNet-200 [[3](#bib.bib3)], and find that video
    loss is more effective in inducing garbled contents, which is consistent with
    our observation in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ FMM-Attack: A
    Flow-based Multi-modal Adversarial Attack on Video-based LLMs").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '混淆效应。值得注意的是，我们提出的 FMM-Attack 会在模型输出中引入混淆，而其他三种攻击方法则不会造成这种失真。这表明 FMM-Attack
    不仅削弱了模型的线索信息，还促使模型产生幻觉。此外，如图[5](#S4.F5 "Figure 5 ‣ 4.3 Discussions ‣ 4 Experiments
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")所示，我们分析了在
    ActivityNet-200 [[3](#bib.bib3)] 数据集中成功攻击 Video-ChatGPT 的数量，并发现视频损失在引起混淆内容方面更为有效，这与我们在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs")中的观察结果一致。'
- en: '![Refer to caption](img/09352f91eda79dc943256501b443fb3f.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/09352f91eda79dc943256501b443fb3f.png)'
- en: 'Figure 5: Comparison of different types of attacks on the garbling rate. Max
    Modify denotes the maximum pixel value that can be modified, while the Garble
    Rate represents the percentage of responses that are garbled.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同类型攻击对混淆率的比较。最大修改值表示可以修改的最大像素值，而混淆率表示混淆响应的百分比。
- en: '![Refer to caption](img/bf5aa52c0a07e19fff19b35ec4108366.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/bf5aa52c0a07e19fff19b35ec4108366.png)'
- en: 'Figure 6: Illustration of the targeted attack.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：目标攻击的说明。
- en: 'Targeted Version of FMM-Attack. Fig. [6](#S4.F6 "Figure 6 ‣ 4.3 Discussions
    ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs") illustrates an example of our targeted attack. We utilize the video features
    and LLM features of the target video as the attack target to generate $\Delta$
    as in Eq. [7](#S3.E7 "Equation 7 ‣ 3.4 Optimization Objective ‣ 3 Methodology
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs").
    As evident in Fig. [6](#S4.F6 "Figure 6 ‣ 4.3 Discussions ‣ 4 Experiments ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"), our implementation
    of the inconspicuous attack enables the video-based LLMs’ answer to closely resemble
    the target’s answer, while significantly differing from the clean video’s output.
    Despite the target video and clean video being entirely distinct, we can achieve
    the desired targeted attack, demonstrating the effectiveness of the proposed FMM-Attack.
    More analyses can be found in the Appendix.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: FMM-Attack 的目标版本。图 [6](#S4.F6 "图 6 ‣ 4.3 讨论 ‣ 4 实验 ‣ FMM-Attack：一种基于流的多模态对抗攻击方法")展示了我们目标攻击的一个例子。我们利用目标视频的视频特征和
    LLM 特征作为攻击目标，生成 $\Delta$，如方程 [7](#S3.E7 "方程 7 ‣ 3.4 优化目标 ‣ 3 方法论 ‣ FMM-Attack：一种基于流的多模态对抗攻击方法")所示。如图 [6](#S4.F6
    "图 6 ‣ 4.3 讨论 ‣ 4 实验 ‣ FMM-Attack：一种基于流的多模态对抗攻击方法")所示，我们实施的隐蔽攻击使得视频基的 LLM 的回答与目标的回答非常接近，同时与清洁视频的输出显著不同。尽管目标视频和清洁视频完全不同，我们仍然能够实现期望的目标攻击，展示了所提出的
    FMM-Attack 的有效性。更多分析请参见附录。
- en: 'Potential Defense. As shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"),
    the video features transfer better across different modalities, especially for
    garbled samples. Improving the robustness of clip/video modules is thus of great
    importance for video-based LLMs or even other vision-related multi-modal models.
    In addition, more attention should be paid to safety-based alignment, which could
    greatly protect video-based LLMs from being attacked. Moreover, common data preprocessing
    methods, which remove adversarial perturbations by compressing or generative models,
    should be beneficial in defending our FMM-Attack.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在的防御。正如图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ FMM-Attack：一种基于流的多模态对抗攻击方法")所示，视频特征在不同模态之间的转移效果更好，特别是对于模糊样本。因此，提高剪辑/视频模块的鲁棒性对视频基的
    LLM 或其他视觉相关的多模态模型至关重要。此外，应该更多关注基于安全的对齐，这可以大大保护视频基的 LLM 免受攻击。此外，常见的数据预处理方法，通过压缩或生成模型去除对抗扰动，对于防御我们的
    FMM-Attack 也应有帮助。
- en: 4.4 Ablation Studies
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: In this section, we will conduct some ablation and exploratory experiments.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行一些消融和探索性实验。
- en: '![Refer to caption](img/f3b8191c8e29fefe972e2e6e76199d36.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3b8191c8e29fefe972e2e6e76199d36.png)'
- en: 'Figure 7: Ablation Studies of different attack settings. (a) Comparison of
    different max modify pixels: Despite varying max modify pixels, the mean value
    of the modified video remains the same due to the sparse loss. (b) Comparison
    of different random select percentages: The select rate represents the percentage
    of attacked video frames that the victimized model samples from. For the baseline
    without attack, the values on each of the five metrics are 0.7817, 0.7827, 0.8096,
    0.8115, and 0.7231, respectively.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：不同攻击设置的消融研究。 (a) 不同最大修改像素的比较：尽管最大修改像素不同，由于稀疏损失，修改视频的均值保持不变。 (b) 不同随机选择百分比的比较：选择率表示受害模型从被攻击的视频帧中采样的百分比。对于没有攻击的基线，每五项指标的值分别为
    0.7817、0.7827、0.8096、0.8115 和 0.7231。
- en: 'Table 3: Comparison of different attack types on Clip Score and Image Captioning
    Metrics. video: represents attacks targeting video features, LLM: represents attacks
    targeting LLM features, and video + LLM: represents combined attacks on both video
    and LLM features. Lower scores indicate better attack performance.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同攻击类型对 Clip Score 和图像描述指标的比较。视频：代表针对视频特征的攻击，LLM：代表针对 LLM 特征的攻击，视频 + LLM：代表同时针对视频和
    LLM 特征的联合攻击。较低的分数表示更好的攻击性能。
- en: '| Type | Clip Score  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | Clip Score  |'
- en: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE-L | CIDEr |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE-L | CIDEr |'
- en: '| Clean | 0.7817 | 0.7827 | 0.8096 | 0.8115 | 0.7231 | 0.2029 | 0.4820 | 1.6364
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 0.7817 | 0.7827 | 0.8096 | 0.8115 | 0.7231 | 0.2029 | 0.4820 | 1.6364
    |'
- en: '| video | 0.7403 | 0.7524 | 0.7690 | 0.7744 | 0.6811 | 0.1975 | 0.4345 | 1.5862
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 视频 | 0.7403 | 0.7524 | 0.7690 | 0.7744 | 0.6811 | 0.1975 | 0.4345 | 1.5862
    |'
- en: '| LLM | 0.7153 | 0.6904 | 0.7334 | 0.7515 | 0.6387 | 0.1042 | 0.3226 | 0.8350
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LLM | 0.7153 | 0.6904 | 0.7334 | 0.7515 | 0.6387 | 0.1042 | 0.3226 | 0.8350
    |'
- en: '| video + LLM | 0.6491 | 0.6060 | 0.6836 | 0.6968 | 0.5566 | 0.0230 | 0.1585
    | 0.1601 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 视频 + LLM | 0.6491 | 0.6060 | 0.6836 | 0.6968 | 0.5566 | 0.0230 | 0.1585 |
    0.1601 |'
- en: 'Loss of Different Modalities. In Table [3](#S4.T3 "Table 3 ‣ 4.4 Ablation Studies
    ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), the video + LLM attack outperforms the individual video and LLM attacks
    across all metrics, demonstrating the superior performance of the combined approach.
    This can be attributed to the complementary nature of video and LLM features,
    which, when targeted simultaneously, leads to a more potent attack that effectively
    disrupts the model’s output, resulting in lower scores.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '不同模态的损失。在表[3](#S4.T3 "Table 3 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")中，视频 + LLM攻击在所有指标上优于单独的视频和LLM攻击，展示了组合方法的优越性能。这可以归因于视频和LLM特征的互补性，当同时针对时，能够进行更强的攻击，有效干扰模型的输出，导致得分较低。'
- en: 'Perturbation Budget . It’s important to note that the mean of . As in Fig. [7](#S4.F7
    "Figure 7 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") (a),  is, the greater the potential modification
    of individual pixels, but on the other hand, due to the sparse loss, the amount
    of pixels that can be modified with a larger  seems to be a better trade-off.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '扰动预算。需要注意的是，如图[7](#S4.F7 "Figure 7 ‣ 4.4 Ablation Studies ‣ 4 Experiments ‣
    FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")
    (a) 所示，越大的潜在像素修改幅度，另一方面，由于稀疏损失，更大的扰动预算似乎是一个更好的折衷。'
- en: 'Percentage of Selected Frames. In some instances, while we launch attacks on
    all video frames, video-based LLMs only randomly sample a subset of the video
    frames. As demonstrated in Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Ablation Studies ‣
    4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs") (b), the potency of the attack escalates with an increasing number of sampled
    video frames. Interestingly, even when a minor fraction (40%, 20%) of video frames
    are sampled, we observe a substantial decline in the Clip score relative to the
    baseline. Surprisingly, a 20% sampling rate seems to yield superior results than
    a 40% rate. We speculate this unexpected outcome could be due to inherent fluctuations
    when a limited number of video frames are sampled, coupled with our stream-based
    approach that assures a certain minimum attack effectiveness. To make this engagement
    more compelling, we intend to delve deeper into this phenomenon with additional
    experiments in future studies.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '选择帧的百分比。在某些情况下，虽然我们对所有视频帧发起攻击，但视频基础LLM仅随机抽取部分视频帧。正如图[7](#S4.F7 "Figure 7 ‣
    4.4 Ablation Studies ‣ 4 Experiments ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs") (b) 所示，攻击的威力随着采样视频帧数量的增加而上升。有趣的是，即使在抽样的帧占比很小（40%，20%）时，我们也观察到Clip评分相较于基线显著下降。令人惊讶的是，20%的采样率似乎比40%的采样率效果更好。我们推测这种意外结果可能由于采样视频帧数量有限时的固有波动，以及我们基于流的方法保证了一定的攻击效果。为了使这一研究更具吸引力，我们打算在未来的研究中深入探讨这一现象。'
- en: 5 Conclusion
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce the Flow-based Multi-modal Attack (FMM-Attack),
    the first of its kind against video-based LLMs. Our comprehensive experiments
    demonstrate that our attack can effectively induce video-based LLMs to generate
    either garbled nonsensical sequences or incorrect semantic sequences with imperceptible
    perturbations added on less than 20% video frames. Furthermore, our insights into
    cross-modal feature attacks contribute to a deeper understanding of multi-modal
    robustness and the critical alignment of safety-related features. These findings
    hold significant implications for various large multi-modal models, underscoring
    the relevance and impact of our work.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了基于流的多模态攻击（FMM-Attack），这是首个针对视频基础LLM的攻击。我们的综合实验表明，我们的攻击能够有效地使视频基础LLM生成乱码序列或错误语义序列，且只需在不到20%的视频帧上添加不可感知的扰动。此外，我们对跨模态特征攻击的见解有助于更深入地理解多模态的鲁棒性及安全相关特征的关键对齐。这些发现对各种大型多模态模型具有重要意义，突显了我们工作的相关性和影响。
- en: References
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Aafaq, N., Akhtar, N., Liu, W., Gilani, S.Z., Mian, A.: Spatio-temporal
    dynamics and semantic attribute enriched visual encoding for video captioning.
    In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition.
    pp. 12487–12496 (2019)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Aafaq, N., Akhtar, N., Liu, W., Gilani, S.Z., Mian, A.：用于视频字幕的时空动态和语义属性丰富的视觉编码。
    In: IEEE/CVF 计算机视觉与模式识别会议论文集。第 12487–12496 页 (2019)'
- en: '[2] Bagdasaryan, E., Hsieh, T.Y., Nassi, B., Shmatikov, V.: (ab) using images
    and sounds for indirect instruction injection in multi-modal llms. arXiv preprint
    arXiv:2307.10490 (2023)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Bagdasaryan, E., Hsieh, T.Y., Nassi, B., Shmatikov, V.： (ab) 使用图像和声音进行间接指令注入的多模态
    llms。arXiv 预印本 arXiv:2307.10490 (2023)'
- en: '[3] Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.: Activitynet:
    A large-scale video benchmark for human activity understanding. In: Proceedings
    of the ieee conference on computer vision and pattern recognition. pp. 961–970
    (2015)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Caba Heilbron, F., Escorcia, V., Ghanem, B., Carlos Niebles, J.： Activitynet：用于人类活动理解的大规模视频基准。
    In: IEEE 计算机视觉与模式识别会议论文集。第 961–970 页 (2015)'
- en: '[4] Chen, T.S., Siarohin, A., Menapace, W., Deyneka, E., Chao, H.w., Jeon,
    B.E., Fang, Y., Lee, H.Y., Ren, J., Yang, M.H., et al.: Panda-70m: Captioning
    70m videos with multiple cross-modality teachers. arXiv preprint arXiv:2402.19479
    (2024)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chen, T.S., Siarohin, A., Menapace, W., Deyneka, E., Chao, H.w., Jeon,
    B.E., Fang, Y., Lee, H.Y., Ren, J., Yang, M.H., et al.： Panda-70m：使用多个跨模态教师对 70m
    视频进行字幕标注。arXiv 预印本 arXiv:2402.19479 (2024)'
- en: '[5] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban
    scene understanding. In: Proceedings of the IEEE conference on computer vision
    and pattern recognition. pp. 3213–3223 (2016)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson,
    R., Franke, U., Roth, S., Schiele, B.： Cityscapes 数据集用于语义城市场景理解。 In: IEEE 计算机视觉与模式识别会议论文集。第
    3213–3223 页 (2016)'
- en: '[6] Dong, J., Li, X., Xu, C., Yang, X., Yang, G., Wang, X., Wang, M.: Dual
    encoding for video retrieval by text. IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44(8), 4065–4080 (2021)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Dong, J., Li, X., Xu, C., Yang, X., Yang, G., Wang, X., Wang, M.：通过文本的双重编码用于视频检索。IEEE
    模式分析与机器智能汇刊 44(8), 4065–4080 (2021)'
- en: '[7] Gabeur, V., Sun, C., Alahari, K., Schmid, C.: Multi-modal transformer for
    video retrieval. In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow,
    UK, August 23–28, 2020, Proceedings, Part IV 16\. pp. 214–229\. Springer (2020)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gabeur, V., Sun, C., Alahari, K., Schmid, C.：用于视频检索的多模态变换器。 In: 计算机视觉–ECCV
    2020：第 16 届欧洲会议，英国格拉斯哥，2020 年 8 月 23–28 日，会议论文集，第 IV 部分，第 214–229 页。Springer (2020)'
- en: '[8] Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., Duan, S., Wang,
    X.: Figstep: Jailbreaking large vision-language models via typographic visual
    prompts. arXiv preprint arXiv:2311.05608 (2023)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Gong, Y., Ran, D., Liu, J., Wang, C., Cong, T., Wang, A., Duan, S., Wang,
    X.： Figstep：通过排版视觉提示破解大型视觉语言模型。arXiv 预印本 arXiv:2311.05608 (2023)'
- en: '[9] Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T.: Bidirectional projection
    network for cross dimension scene understanding. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition. pp. 14373–14382 (2021)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Hu, W., Zhao, H., Jiang, L., Jia, J., Wong, T.T.：用于跨维度场景理解的双向投影网络。 In:
    IEEE/CVF 计算机视觉与模式识别会议论文集。第 14373–14382 页 (2021)'
- en: '[10] Hui, T.W., Tang, X., Loy, C.C.: Liteflownet: A lightweight convolutional
    neural network for optical flow estimation. In: Proceedings of the IEEE conference
    on computer vision and pattern recognition. pp. 8981–8989 (2018)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Hui, T.W., Tang, X., Loy, C.C.： Liteflownet：一种轻量级卷积神经网络用于光流估计。 In: IEEE
    计算机视觉与模式识别会议论文集。第 8981–8989 页 (2018)'
- en: '[11] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L.,
    Qiao, Y.: Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355
    (2023)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Li, K., He, Y., Wang, Y., Li, Y., Wang, W., Luo, P., Wang, Y., Wang, L.,
    Qiao, Y.：Videochat：以聊天为中心的视频理解。arXiv 预印本 arXiv:2305.06355 (2023)'
- en: '[12] Lin, C.Y.: Rouge: A package for automatic evaluation of summaries. In:
    Text summarization branches out. pp. 74–81 (2004)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Lin, C.Y.：Rouge：用于自动评估摘要的软件包。 In: 文本摘要的扩展。第 74–81 页 (2004)'
- en: '[13] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction
    tuning. In: NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following
    (2023)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Liu, H., Li, C., Li, Y., Lee, Y.J.：通过视觉指令调整改进基线。 In: NeurIPS 2023 指令调整与指令跟随研讨会
    (2023)'
- en: '[14] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.: Clip4clip:
    An empirical study of clip for end to end video clip retrieval and captioning.
    Neurocomputing 508, 293–304 (2022)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Luo, H., Ji, L., Zhong, M., Chen, Y., Lei, W., Duan, N., Li, T.： Clip4clip：对端到端视频片段检索和字幕的实证研究。Neurocomputing
    508, 293–304 (2022)'
- en: '[15] Maaz, M., Rasheed, H., Khan, S., Khan, F.S.: Video-chatgpt: Towards detailed
    video understanding via large vision and language models. arXiv preprint arXiv:2306.05424
    (2023)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Maaz, M., Rasheed, H., Khan, S., Khan, F.S.：Video-chatgpt：通过大型视觉和语言模型实现详细的视频理解。arXiv
    预印本 arXiv:2306.05424 (2023)'
- en: '[16] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep
    learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083
    (2017)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.：朝着抵御对抗攻击的深度学习模型迈进。arXiv
    预印本 arXiv:1706.06083 (2017)'
- en: '[17] Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic
    evaluation of machine translation. In: Proceedings of the 40th annual meeting
    of the Association for Computational Linguistics. pp. 311–318 (2002)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Papineni, K., Roukos, S., Ward, T., Zhu, W.J.：Bleu：一种自动评估机器翻译的方法。在：第40届计算语言学协会年会论文集。第
    311–318 页 (2002)'
- en: '[18] Qi, X., Huang, K., Panda, A., Wang, M., Mittal, P.: Visual adversarial
    examples jailbreak large language models. arXiv preprint arXiv:2306.13213 (2023)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Qi, X., Huang, K., Panda, A., Wang, M., Mittal, P.：视觉对抗样本破解大型语言模型。arXiv
    预印本 arXiv:2306.13213 (2023)'
- en: '[19] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: International conference
    on machine learning. pp. 8748–8763\. PMLR (2021)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., 等：从自然语言监督中学习可迁移的视觉模型。在：国际机器学习大会。第
    8748–8763 页。PMLR (2021)'
- en: '[20] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: International conference
    on machine learning. pp. 8748–8763\. PMLR (2021)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., 等：从自然语言监督中学习可迁移的视觉模型。在：国际机器学习大会。第
    8748–8763 页。PMLR (2021)'
- en: '[21] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
    image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition. pp. 10684–10695 (2022)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.：高分辨率图像合成与潜在扩散模型。在：IEEE/CVF
    计算机视觉与模式识别大会论文集。第 10684–10695 页 (2022)'
- en: '[22] Seo, P.H., Nagrani, A., Arnab, A., Schmid, C.: End-to-end generative pretraining
    for multimodal video captioning. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition. pp. 17959–17968 (2022)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Seo, P.H., Nagrani, A., Arnab, A., Schmid, C.：端到端生成预训练用于多模态视频字幕。在：IEEE/CVF
    计算机视觉与模式识别大会论文集。第 17959–17968 页 (2022)'
- en: '[23] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
    foundation language models. arXiv preprint arXiv:2302.13971 (2023)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等：Llama：开放且高效的基础语言模型。arXiv
    预印本 arXiv:2302.13971 (2023)'
- en: '[24] Vedantam, R., Lawrence Zitnick, C., Parikh, D.: Cider: Consensus-based
    image description evaluation. In: Proceedings of the IEEE conference on computer
    vision and pattern recognition. pp. 4566–4575 (2015)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Vedantam, R., Lawrence Zitnick, C., Parikh, D.：Cider：基于共识的图像描述评估。在：IEEE
    计算机视觉与模式识别大会论文集。第 4566–4575 页 (2015)'
- en: '[25] Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko,
    K.: Sequence to sequence-video to text. In: Proceedings of the IEEE international
    conference on computer vision. pp. 4534–4542 (2015)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T., Saenko,
    K.：序列到序列——视频到文本。在：IEEE 国际计算机视觉大会论文集。第 4534–4542 页 (2015)'
- en: '[26] Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu,
    T., Zhou, J., Qiao, Y., et al.: Visionllm: Large language model is also an open-ended
    decoder for vision-centric tasks. Advances in Neural Information Processing Systems
    36 (2024)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Wang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G., Luo, P., Lu,
    T., Zhou, J., Qiao, Y., 等：Visionllm：大型语言模型也是面向视觉任务的开放式解码器。神经信息处理系统进展 36 (2024)'
- en: '[27] Wei, X., Zhu, J., Yuan, S., Su, H.: Sparse adversarial perturbations for
    videos. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol. 33,
    pp. 8973–8980 (2019)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Wei, X., Zhu, J., Yuan, S., Su, H.：视频的稀疏对抗扰动。在：AAAI 人工智能大会论文集。第 33 卷，第
    8973–8980 页 (2019)'
- en: '[28] Wu, Y.H., Liu, Y., Zhan, X., Cheng, M.M.: P2t: Pyramid pooling transformer
    for scene understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence
    (2022)'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 吴, Y.H., 刘, Y., 詹, X., 程, M.M.: P2t: 用于场景理解的金字塔池化变换器。IEEE 模式分析与机器智能汇刊
    (2022)'
- en: '[29] Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X., Zhuang, Y.: Video
    question answering via gradually refined attention over appearance and motion.
    In: Proceedings of the 25th ACM international conference on Multimedia. pp. 1645–1653
    (2017)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 许, D., 赵, Z., 肖, J., 吴, F., 张, H., 何, X., 庄, Y.: 通过逐渐细化的注意力进行视频问答，涵盖外观和运动。第25届
    ACM 国际多媒体会议论文集。第1645–1653页 (2017)'
- en: '[30] Xue, H., Hang, T., Zeng, Y., Sun, Y., Liu, B., Yang, H., Fu, J., Guo,
    B.: Advancing high-resolution video-language representation with large-scale video
    transcriptions. In: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition. pp. 5036–5045 (2022)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 薛, H., 杭, T., 曾, Y., 孙, Y., 刘, B., 杨, H., 傅, J., 郭, B.: 通过大规模视频转录推进高分辨率视频-语言表示。IEEE/CVF
    计算机视觉与模式识别会议论文集。第5036–5045页 (2022)'
- en: '[31] Yang, D., Bai, Y., Jia, X., Liu, Y., Cao, X., Yu, W.: Cheating suffix:
    Targeted attack to text-to-image diffusion models with multi-modal priors. arXiv
    preprint arXiv:2402.01369 (2024)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 杨, D., 白, Y., 贾, X., 刘, Y., 曹, X., 于, W.: Cheating suffix: 针对具有多模态先验的文本到图像扩散模型的攻击。arXiv
    预印本 arXiv:2402.01369 (2024)'
- en: '[32] Zhang, H., Li, X., Bing, L.: Video-llama: An instruction-tuned audio-visual
    language model for video understanding. arXiv preprint arXiv:2306.02858 (2023)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 张, H., 李, X., 邴, L.: Video-llama: 用于视频理解的指令调优音视频语言模型。arXiv 预印本 arXiv:2306.02858
    (2023)'
- en: '[33] Zhang, Z., Qi, Z., Yuan, C., Shan, Y., Li, B., Deng, Y., Hu, W.: Open-book
    video captioning with retrieve-copy-generate network. In: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition. pp. 9837–9846 (2021)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 张, Z., 齐, Z., 袁, C., 单, Y., 李, B., 邓, Y., 胡, W.: 通过检索-复制-生成网络进行开放书籍视频字幕。IEEE/CVF
    计算机视觉与模式识别会议论文集。第9837–9846页 (2021)'
- en: '[34] Zhao, Y., Pang, T., Du, C., Yang, X., Li, C., Cheung, N.M.M., Lin, M.:
    On evaluating adversarial robustness of large vision-language models. Advances
    in Neural Information Processing Systems 36 (2024)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 赵, Y., 庞, T., 杜, C., 杨, X., 李, C., 张, N.M.M., 林, M.: 评估大型视觉-语言模型的对抗鲁棒性。神经信息处理系统进展
    36 (2024)'
- en: '[35] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing
    vision-language understanding with advanced large language models. arXiv preprint
    arXiv:2304.10592 (2023)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 朱, D., 陈, J., 沈, X., 李, X., Elhoseiny, M.: Minigpt-4: 通过先进的大型语言模型增强视觉-语言理解。arXiv
    预印本 arXiv:2304.10592 (2023)'
- en: '[36] Zhuang, H., Zhang, Y., Liu, S.: A pilot study of query-free adversarial
    attack against stable diffusion. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition. pp. 2384–2391 (2023)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 庄, H., 张, Y., 刘, S.: 对稳定扩散的无查询对抗攻击的初步研究。IEEE/CVF 计算机视觉与模式识别会议论文集。第2384–2391页
    (2023)'
- en: '[37] Zong, Y., Bohdal, O., Yu, T., Yang, Y., Hospedales, T.: Safety fine-tuning
    at (almost) no cost: A baseline for vision large language models. arXiv preprint
    arXiv:2402.02207 (2024)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 龚, Y., 博达尔, O., 于, T., 杨, Y., 霍斯佩代尔, T.: 在（几乎）无成本下的安全微调：视觉大型语言模型的基线。arXiv
    预印本 arXiv:2402.02207 (2024)'
- en: 'In this Appendix, we provide further implementation details in Section [0.A](#Pt0.A1
    "Appendix 0.A Implementation Details ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs"), including flow-based temporal mask, threat model
    specifics, datasets, and experimental setups. Following that, we present additional
    experimental results in Section [0.B](#Pt0.A2 "Appendix 0.B Additional Experimental
    Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), featuring extended video question-answer methods (VideoChat [[11](#bib.bib11)])
    and comprehensive evaluation metrics. We also report our findings from transfer-based
    black-box attack experiments in Section [0.C](#Pt0.A3 "Appendix 0.C Transfer-based
    Black-box Attacks ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on
    Video-based LLMs"), showcasing the impressive transferability of our attack method.
    Moreover, we offer more visual comparisons in Section [0.D](#Pt0.A4 "Appendix
    0.D Additional Visualization Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs"). Ethics statement and reproducibility statement can
    be found in Section [0.E](#Pt0.A5 "Appendix 0.E Ethics Statement ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs") and Section [0.F](#Pt0.A6
    "Appendix 0.F Reproducibility Statement ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs"), respectively. Finally, we discuss the
    limitations of this paper in Section [0.G](#Pt0.A7 "Appendix 0.G Limitation ‣
    FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs").'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '在本附录中，我们在[0.A](#Pt0.A1 "Appendix 0.A Implementation Details ‣ FMM-Attack: A
    Flow-based Multi-modal Adversarial Attack on Video-based LLMs")节中提供了进一步的实施细节，包括光流基时间掩码、威胁模型具体信息、数据集和实验设置。随后，在[0.B](#Pt0.A2
    "Appendix 0.B Additional Experimental Results ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs")节中，我们展示了额外的实验结果，包括扩展的视频问答方法（VideoChat
    [[11](#bib.bib11)]）和综合评估指标。我们还在[0.C](#Pt0.A3 "Appendix 0.C Transfer-based Black-box
    Attacks ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs")节中报告了基于转移的黑箱攻击实验的发现，展示了我们攻击方法的显著转移性。此外，我们在[0.D](#Pt0.A4 "Appendix 0.D Additional
    Visualization Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack
    on Video-based LLMs")节中提供了更多的视觉对比。伦理声明和可重复性声明分别可以在[0.E](#Pt0.A5 "Appendix 0.E
    Ethics Statement ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on
    Video-based LLMs")节和[0.F](#Pt0.A6 "Appendix 0.F Reproducibility Statement ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")节中找到。最后，我们在[0.G](#Pt0.A7
    "Appendix 0.G Limitation ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack
    on Video-based LLMs")节中讨论了本文的局限性。'
- en: Appendix 0.A Implementation Details
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.A 实施细节
- en: Models. We assess open-source and state-of-the-art video-based LLMs such as
    Video-ChatGPT [[15](#bib.bib15)] and VideoChat [[11](#bib.bib11)], ensuring reproducibility
    of our results. Video-ChatGPT is a multi-modal model that seamlessly combines
    a video-adapted visual encoder (CLIP [[20](#bib.bib20)]) with a LLM, which is
    proficient in comprehending and generating intricate conversations related to
    videos. VideoChat integrates video foundation models and large language models
    via a learnable neural interface.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 模型。我们评估了开源和最先进的视频基础LLMs，如Video-ChatGPT [[15](#bib.bib15)] 和 VideoChat [[11](#bib.bib11)]，以确保结果的可重复性。Video-ChatGPT
    是一个多模态模型，它将适配视频的视觉编码器（CLIP [[20](#bib.bib20)]）与LLM无缝结合，擅长理解和生成与视频相关的复杂对话。VideoChat
    通过一个可学习的神经接口将视频基础模型和大型语言模型整合在一起。
- en: '![Refer to caption](img/8a74061813c7e7797145a56669b2af06.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a74061813c7e7797145a56669b2af06.png)'
- en: 'Figure 8: Relationship between optical flow and key frames. ‘Clip Score of
    Adjacent Frames’ describes the similarity between the current frame and its adjacent
    frames, the smaller this score is the more different the current frame is. The
    frames selected by flow-based masks in our FMM-Attack are key frames in the video.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：光流与关键帧之间的关系。‘相邻帧的剪辑分数’描述了当前帧与其相邻帧之间的相似性，这个分数越小，当前帧与相邻帧的差异越大。在我们的FMM-Attack中，通过光流基的掩码选择的帧是视频中的关键帧。
- en: 'Flow-based Temporal Mask. In addition to the statistical analyses presented
    in the main manuscript, we visualize flow-based methods to enhance interpretability.
    As depicted in Fig. [8](#Pt0.A1.F8 "Figure 8 ‣ Appendix 0.A Implementation Details
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"),
    the brightness represents the magnitude of the optical flow, and the color indicates
    the motion direction. The motion flow magnitude varies across different frames,
    with a larger optical flow signifying more significant motion. Our FMM method
    tends to select the video frames with the largest optical flow as the key frame
    of the video. In other words, we tend to prioritize frames with substantial motion
    changes. Furthermore, the frames we select exhibit low similarity with their neighboring
    frames, indicating their importance.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '基于流的时间掩码。除了主文稿中提出的统计分析，我们还可视化了基于流的方法以增强可解释性。如图 [8](#Pt0.A1.F8 "图 8 ‣ 附录 0.A
    实施细节 ‣ FMM-Attack: 一种基于流的多模态对抗攻击方法针对基于视频的LLMs") 所示，亮度表示光流的幅度，颜色表示运动方向。运动流幅度在不同帧之间变化，光流较大的帧表示运动较大。我们的
    FMM 方法倾向于选择具有最大光流的视频帧作为视频的关键帧。换句话说，我们倾向于优先选择具有显著运动变化的帧。此外，我们选择的帧与其邻近帧的相似性较低，表明它们的重要性。'
- en: 'Algorithm [2](#alg2 "Algorithm 2 ‣ Appendix 0.A Implementation Details ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs") delineates the
    process of generating the selected set . The optical flow is computed using a
    pre-trained liteflownet [[10](#bib.bib10)]. For the random temporal mask, we generate
    the selected set  elements from the total set  represents the set of frame indices,  denotes
    the total number of frames in the video. For the sequence temporal mask, we construct
    the selected set  elements from the total set  comprises a sequence of frames
    from total frames  or .'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [2](#alg2 "算法 2 ‣ 附录 0.A 实施细节 ‣ FMM-Attack: 一种基于流的多模态对抗攻击方法针对基于视频的LLMs")
    描述了生成选定集合的过程。光流是使用预训练的 liteflownet [[10](#bib.bib10)] 计算的。对于随机时间掩码，我们从总集合中生成选定集合元素，总集合代表帧索引的集合，
    代表视频中的总帧数。对于序列时间掩码，我们从总集合中构建选定集合元素，总集合包含从总帧数中提取的帧序列或。'
- en: Algorithm 2 Select Top K Frames with Maximum Flow
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 选择流量最大的前 K 帧
- en: 0:  video_frames 0:  U, a subset with 1:  for each frame in video_frames do2:     Compute
    optical flow between adjacent frames3:  end for4:  for each optical flow do5:     Convert
    optical flow to color and magnitude components6:     Normalize the magnitude component7:  end for8:  Sort
    the frames based on the average value of the magnitude component9:  Select the
    top K frame indices with the highest flow values as the set U
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  video_frames 0:  U，一个包含 1:  对于 video_frames 中的每一帧  do2:     计算相邻帧之间的光流3:  结束  for4:  对于每个光流  do5:     将光流转换为颜色和幅度组件6:     归一化幅度组件7:  结束  for8:  根据幅度组件的平均值对帧进行排序9:  选择具有最高流量值的前
    K 帧索引作为集合 U'
- en: Datasets. In line with the Video-ChatGPT [[15](#bib.bib15)] methodology, we
    curate a test set based on the ActivityNet-200 [[3](#bib.bib3)] and MSVD-QA [[29](#bib.bib29)]
    datasets, featuring videos with rich, detailed descriptive captions and associated
    question-answer pairs obtained from human annotations. Utilizing this test set
    to generate adversarial examples, we effectively and quantitatively assess the
    adversarial robustness of video-based LLMs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。按照 Video-ChatGPT [[15](#bib.bib15)] 方法，我们基于 ActivityNet-200 [[3](#bib.bib3)]
    和 MSVD-QA [[29](#bib.bib29)] 数据集策划了一个测试集，包含具有丰富详细描述性字幕和从人工注释中获得的问答对的视频。利用这个测试集生成对抗样本，我们能够有效且定量地评估基于视频的
    LLMs 的对抗鲁棒性。
- en: 'Experimental setups. For evaluation, we design three spatial baselines, including
    videos with random perturbations, black videos with all pixel values set to 0,
    and white videos with all pixel values set to 1\. In addition, we compare our
    proposed flow-based temporal mask with two straightforward temporal mask methods,
    serving as temporal mask baselines: the sequence temporal mask and the random
    temporal mask. Specifically, the sequence temporal mask consists of a continuous
    sequence of frame indices, while the random temporal mask comprises a randomly
    chosen sequence of frame indices. We utilize a variety of evaluation metrics to
    assess the robustness of the models. CLIP [[20](#bib.bib20)] score characterizes
    the semantic similarity between the adversarial answer and the ground-truth answer.
    A lower CLIP score signifies a lower semantic correlation between the adversarial
    answer and the ground-truth answer, indicating a more effective attack. Various
    Image Captioning metrics such as BLEU [[17](#bib.bib17)], ROUGE-L [[12](#bib.bib12)],
    and CIDEr [[24](#bib.bib24)] are used to evaluate the quality of the adversarial
    answer generated by the model. A lower score corresponds to a more effective attack.
    BLEU measures the overlap of n-grams between the generated and reference captions.
    ROUGE-L computes the longest common subsequence between them, reflecting their
    sentence-level similarity. CIDEr emphasizes the importance of semantically meaningful
    words in the captions. Following Video-ChatGPT [[15](#bib.bib15)] and Video-LLaMA [[32](#bib.bib32)].
    We also employ an evaluation pipeline using the GPT-3.5 and GPT-4 models. The
    pipeline employs GPT to assign a score from 1 to 5, evaluating the similarity
    between the output sentence and the ground truth, and a binary score (0 or 1)
    to measure its accuracy.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。为了评估，我们设计了三个空间基线，包括具有随机扰动的视频、所有像素值都设置为 0 的黑色视频，以及所有像素值都设置为 1 的白色视频。此外，我们将我们提出的基于流的时间掩码与两种直接的时间掩码方法进行比较，作为时间掩码基线：序列时间掩码和随机时间掩码。具体来说，序列时间掩码由一系列连续的帧索引组成，而随机时间掩码则由随机选择的一系列帧索引组成。我们使用各种评估指标来评估模型的鲁棒性。**CLIP** [[20](#bib.bib20)]
    分数表征了对抗性答案与真实答案之间的语义相似性。较低的 CLIP 分数表示对抗性答案与真实答案之间的语义关联较低，表明攻击更有效。使用各种图像标题生成指标，如
    **BLEU** [[17](#bib.bib17)]、**ROUGE-L** [[12](#bib.bib12)] 和 **CIDEr** [[24](#bib.bib24)]
    来评估模型生成的对抗性答案的质量。较低的分数对应于更有效的攻击。**BLEU** 衡量生成的标题与参考标题之间的 n-gram 重叠。**ROUGE-L**
    计算它们之间的最长公共子序列，反映了它们的句子级相似性。**CIDEr** 强调标题中语义上重要的词汇。继 **Video-ChatGPT** [[15](#bib.bib15)]
    和 **Video-LLaMA** [[32](#bib.bib32)] 之后，我们还采用了使用 **GPT-3.5** 和 **GPT-4** 模型的评估管道。该管道使用
    **GPT** 给出 1 到 5 的分数，以评估输出句子与真实情况之间的相似性，并给出二进制分数（0 或 1）来衡量其准确性。
- en: 'Table 4: White-box attacks against VideoChat [[11](#bib.bib11)] on the ActivityNet-200 [[3](#bib.bib3)]
    dataset and the MSVD-QA [[29](#bib.bib29)] dataset: Comparison of image caption
    metrics and GPT score for different attack types. Random spatial attack denotes
    random perturbations added to video frames, and Black spatial attack and White
    spatial attack denote video frames being all 0 and all 1, respectively. The sparsity
    of the temporal mask is set to 0\. ${\Delta}$: the mean of the modified pixels.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：对 **VideoChat** [[11](#bib.bib11)] 在 **ActivityNet-200** [[3](#bib.bib3)]
    数据集和 **MSVD-QA** [[29](#bib.bib29)] 数据集上的白盒攻击：不同攻击类型的图像标题指标和 **GPT** 分数比较。随机空间攻击表示对视频帧添加了随机扰动，黑色空间攻击和白色空间攻击分别表示视频帧全部为
    0 和全部为 1。时间掩码的稀疏性设置为 0。${\Delta}$：修改像素的均值。
- en: '| Dataset | Type |  | GPT-3.5  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 类型 |  | **GPT-3.5** |'
- en: '| BLEU | ROUGE | CIDEr | Accurate | Score | Accurate | Score |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| **BLEU** | **ROUGE** | **CIDEr** | 准确性 | 分数 | 准确性 | 分数 |'
- en: '| ActivityNet [[3](#bib.bib3)] | Clean | 0 | 0.0765 | 0.3358 | 0.3379 | 0.35
    | 2.87 | 0.28 | 1.79 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ActivityNet [[3](#bib.bib3)] | 清洁 | 0 | 0.0765 | 0.3358 | 0.3379 | 0.35 |
    2.87 | 0.28 | 1.79 |'
- en: '| Random | 8 | 0.0870 | 0.3446 | 0.4225 | 0.47 | 3.04 | 0.34 | 2.06 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 8 | 0.0870 | 0.3446 | 0.4225 | 0.47 | 3.04 | 0.34 | 2.06 |'
- en: '| Black | 110 | 0.0726 | 0.3104 | 0.3599 | 0.16 | 2.17 | 0.10 | 0.66 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 110 | 0.0726 | 0.3104 | 0.3599 | 0.16 | 2.17 | 0.10 | 0.66 |'
- en: '| White | 152 | 0.0828 | 0.3226 | 0.3698 | 0.18 | 2.25 | 0.11 | 0.79 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 白色 | 152 | 0.0828 | 0.3226 | 0.3698 | 0.18 | 2.25 | 0.11 | 0.79 |'
- en: '| FMM | 2 | 0.0626 | 0.2474 | 0.2859 | 0.06 | 1.43 | 0.06 | 0.40 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| FMM | 2 | 0.0626 | 0.2474 | 0.2859 | 0.06 | 1.43 | 0.06 | 0.40 |'
- en: '| MSVD-QA [[29](#bib.bib29)] | Clean | 0 | 0.0415 | 0.2643 | 0.1183 | 0.70
    | 3.66 | 0.68 | 3.20 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| MSVD-QA [[29](#bib.bib29)] | 清洁 | 0 | 0.0415 | 0.2643 | 0.1183 | 0.70 | 3.66
    | 0.68 | 3.20 |'
- en: '| Random | 8 | 0.0594 | 0.2679 | 0.2182 | 0.64 | 3.44 | 0.54 | 2.78 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 8 | 0.0594 | 0.2679 | 0.2182 | 0.64 | 3.44 | 0.54 | 2.78 |'
- en: '| Black | 110 | 0.0479 | 0.2674 | 0.2185 | 0.38 | 2.80 | 0.26 | 1.60 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 110 | 0.0479 | 0.2674 | 0.2185 | 0.38 | 2.80 | 0.26 | 1.60 |'
- en: '| White | 153 | 0.0594 | 0.2734 | 0.4154 | 0.44 | 3.00 | 0.20 | 1.46 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 白色 | 153 | 0.0594 | 0.2734 | 0.4154 | 0.44 | 3.00 | 0.20 | 1.46 |'
- en: '| FMM | 2 | 0.0388 | 0.2001 | 0.2554 | 0.12 | 1.58 | 0.12 | 1.42 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| FMM | 2 | 0.0388 | 0.2001 | 0.2554 | 0.12 | 1.58 | 0.12 | 1.42 |'
- en: Appendix 0.B Additional Experimental Results
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.B 附加实验结果
- en: In this section, we present additional experiments. Firstly, we execute an attack
    on another video-based LLM model, VideoChat [[11](#bib.bib11)]. Following that,
    we provide experimental results from other datasets and introduce more comprehensive
    evaluation metrics. Finally, we investigate the impact of varying the weights
    of the features loss.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了额外的实验。首先，我们对另一个基于视频的LLM模型VideoChat [[11](#bib.bib11)]进行了攻击。接着，我们提供了其他数据集的实验结果，并引入了更全面的评估指标。最后，我们研究了特征损失权重变化的影响。
- en: 'Additional Video-based LLMs. In addition to the attack on Video-ChatGPT [[15](#bib.bib15)]
    described in the main text, we also attack VideoChat [[11](#bib.bib11)], as illustrated
    in Table [4](#Pt0.A1.T4 "Table 4 ‣ Appendix 0.A Implementation Details ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"). Our proposed
    FMM significantly diminishes VideoChat’s question-answering capability, resulting
    in gibberish outputs. This is substantiated by a notable decrease in both image
    caption scores and GPT scores. Specific visualization results are provided in
    subsequent sections.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 附加基于视频的LLMs。除了主文中描述的对Video-ChatGPT [[15](#bib.bib15)]的攻击，我们还对VideoChat [[11](#bib.bib11)]进行了攻击，如表[4](#Pt0.A1.T4
    "表 4 ‣ 附录 0.A 实现细节 ‣ FMM-攻击：基于流的多模态对抗攻击针对基于视频的LLMs")所示。我们提出的FMM显著减少了VideoChat的问答能力，导致了胡言乱语的输出。这一点通过图像说明分数和GPT分数的显著下降得到了证实。具体的可视化结果将在后续部分提供。
- en: 'Table 5: White-box attacks against surrogate model Video-ChatGPT [[15](#bib.bib15)]
    on the MSVD-QA [[29](#bib.bib29)] dataset: Comparison of CLIP Score and Image
    Captioning Metrics for different attack types. Random attack denotes random perturbations
    added to video frames, Black attack and White attack denote video frames being
    all 0 and all 1, respectively. seq: sequence temporal mask. random: random temporal
    mask. flow: flow-based temporal mask. : the mean of the modified pixels.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5：对替代模型Video-ChatGPT [[15](#bib.bib15)]在MSVD-QA [[29](#bib.bib29)]数据集上的白盒攻击：不同攻击类型的CLIP得分和图像说明度量的比较。随机攻击表示对视频帧添加随机扰动，黑色攻击和白色攻击分别表示视频帧全部为0和全部为1。seq:
    序列时间掩码。random: 随机时间掩码。flow: 基于流的时间掩码。: 修改像素的均值。'
- en: '| Type |  | Image Caption $\downarrow$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 类型 |  | 图像说明 $\downarrow$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE | CIDEr |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE | CIDEr |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Clean | 0 | 0.8322 | 0.8180 | 0.8299 | 0.8533 | 0.8675 | 0.3864 | 0.6843
    | 3.6792 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 0 | 0.8322 | 0.8180 | 0.8299 | 0.8533 | 0.8675 | 0.3864 | 0.6843 | 3.6792
    |'
- en: '| Random | 8 | 0.8249 | 0.8141 | 0.8299 | 0.8376 | 0.7446 | 0.4107 | 0.7042
    | 4.0715 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | 8 | 0.8249 | 0.8141 | 0.8299 | 0.8376 | 0.7446 | 0.4107 | 0.7042 | 4.0715
    |'
- en: '| Black | 100 | 0.8145 | 0.7902 | 0.8334 | 0.8376 | 0.7280 | 0.3548 | 0.6478
    | 3.4367 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 黑色 | 100 | 0.8145 | 0.7902 | 0.8334 | 0.8376 | 0.7280 | 0.3548 | 0.6478 |
    3.4367 |'
- en: '| White | 148 | 0.8057 | 0.8090 | 0.8390 | 0.8435 | 0.7580 | 0.3969 | 0.6736
    | 3.9245 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 白色 | 148 | 0.8057 | 0.8090 | 0.8390 | 0.8435 | 0.7580 | 0.3969 | 0.6736 |
    3.9245 |'
- en: '| FMM | 8 | 0.7337 | 0.7181 | 0.7645 | 0.7796 | 0.6460 | 0.3240 | 0.5746 |
    3.1420 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| FMM | 8 | 0.7337 | 0.7181 | 0.7645 | 0.7796 | 0.6460 | 0.3240 | 0.5746 |
    3.1420 |'
- en: 'Comprehensive Evaluation Metrics. Owing to space constraints in the main text,
    we include more extensive experimental results in Table [5](#Pt0.A2.T5 "Table
    5 ‣ Appendix 0.B Additional Experimental Results ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs") and Table [6](#Pt0.A2.T6 "Table 6 ‣ Appendix
    0.B Additional Experimental Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs"). Table 2 contains five evaluation metrics related
    to Clip score and three metrics associated with image caption. These eight metrics
    display a consistent pattern, demonstrating that our approach significantly reduces
    the correlation between questions and answers. As a result, the scores experience
    a considerable decline, indicating the effectiveness of our proposed method in
    disrupting the performance of the targeted models.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '综合评估指标。由于主文中的空间限制，我们在表[5](#Pt0.A2.T5 "Table 5 ‣ Appendix 0.B Additional Experimental
    Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs")和表[6](#Pt0.A2.T6 "Table 6 ‣ Appendix 0.B Additional Experimental Results
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")中包含了更广泛的实验结果。表2包含了与Clip评分相关的五个评估指标和与图像字幕相关的三个指标。这八个指标显示出一致的模式，证明我们的方法显著降低了问题与答案之间的相关性。因此，分数经历了显著下降，表明我们提出的方法在干扰目标模型性能方面的有效性。'
- en: 'Table [6](#Pt0.A2.T6 "Table 6 ‣ Appendix 0.B Additional Experimental Results
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")
    includes a comparison of the temporal mask. We compare the four baselines - ‘Clean’,
    ‘Random’, ‘Black’, and ‘White’ - when the temporal mask is set to 0\. As evident
    from the table, our method significantly outperforms the two attack methods, ‘Black’
    and ‘White’, even though we only modify 8 pixel values compared to their modification
    of more than 100 pixel values. Additionally, when the temporal mask is not set
    to 0, we establish two alternative comparison methods: sequential (‘seq’) and
    ‘random’. The ‘seq’ method involves inputting frames with consecutive masks, while
    the ‘random’ method requires inputting frames with randomly assigned masks. Our
    proposed method, FMM, is based on the maximum flow algorithm (see Algorithm [2](#alg2
    "Algorithm 2 ‣ Appendix 0.A Implementation Details ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs")). It selects frames corresponding
    to the top K largest flows according to their flow magnitude.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '表[6](#Pt0.A2.T6 "Table 6 ‣ Appendix 0.B Additional Experimental Results ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")包括时间掩码的比较。我们比较了四个基线——‘Clean’、‘Random’、‘Black’和‘White’——当时间掩码设置为0时。如表中所示，我们的方法明显优于两种攻击方法‘Black’和‘White’，尽管我们只修改了8个像素值，而它们修改了超过100个像素值。此外，当时间掩码不设置为0时，我们建立了两种替代比较方法：顺序（‘seq’）和‘random’。‘seq’方法涉及输入具有连续掩码的帧，而‘random’方法要求输入具有随机分配掩码的帧。我们提出的方法FMM基于最大流算法（见算法[2](#alg2
    "Algorithm 2 ‣ Appendix 0.A Implementation Details ‣ FMM-Attack: A Flow-based
    Multi-modal Adversarial Attack on Video-based LLMs")）。它根据流量大小选择与前K个最大流量对应的帧。'
- en: 'Table 6: White-box attacks against surrogate model Video-ChatGPT [[15](#bib.bib15)]
    on the ActivityNet-200 [[3](#bib.bib3)] dataset: Comparison of CLIP Score and
    Image Captioning Metrics for different attack types. Random attack denotes random
    perturbations added to video frames, Black attack and White attack denote video
    frames being all 0 and all 1, respectively. seq: sequence temporal mask. random:
    random temporal mask. flow: flow-based temporal mask. : the mean of the modified
    pixels. seq: sequence temporal mask. random: random temporal mask. flow: flow-based
    temporal mask. : the mean of the modified pixels.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：对代理模型Video-ChatGPT [[15](#bib.bib15)]在ActivityNet-200 [[3](#bib.bib3)]数据集上的白盒攻击：不同攻击类型的CLIP评分和图像字幕度量的比较。随机攻击表示对视频帧添加随机扰动，Black攻击和White攻击分别表示视频帧全为0和全为1。seq：顺序时间掩码。random：随机时间掩码。flow：基于流的时间掩码。：修改像素的均值。seq：顺序时间掩码。random：随机时间掩码。flow：基于流的时间掩码。：修改像素的均值。
- en: '| Type |  | Clip Score  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| Type |  | Clip Score  |'
- en: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE | CIDEr |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE | CIDEr |'
- en: '| Clean | $0\%$ | 0 | 0.7817 | 0.7827 | 0.8096 | 0.8115 | 0.7231 | 0.2029 |
    0.4820 | 1.6364 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Clean | $0\%$ | 0 | 0.7817 | 0.7827 | 0.8096 | 0.8115 | 0.7231 | 0.2029 |
    0.4820 | 1.6364 |'
- en: '| Random | $0\%$ | 8 | 0.7637 | 0.7681 | 0.7920 | 0.7974 | 0.6909 | 0.1986
    | 0.4793 | 1.5552 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Random | $0\%$ | 8 | 0.7637 | 0.7681 | 0.7920 | 0.7974 | 0.6909 | 0.1986
    | 0.4793 | 1.5552 |'
- en: '| Black | $0\%$ | 100 | 0.7661 | 0.7676 | 0.7959 | 0.8018 | 0.7105 | 0.1691
    | 0.4570 | 1.3246 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Black | $0\%$ | 100 | 0.7661 | 0.7676 | 0.7959 | 0.8018 | 0.7105 | 0.1691
    | 0.4570 | 1.3246 |'
- en: '| White | $0\%$ | 148 | 0.7564 | 0.7534 | 0.7813 | 0.7837 | 0.6836 | 0.1689
    | 0.4545 | 1.3431 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| White | $0\%$ | 148 | 0.7564 | 0.7534 | 0.7813 | 0.7837 | 0.6836 | 0.1689
    | 0.4545 | 1.3431 |'
- en: '| FMM | $0\%$ | 8 | 0.6211 | 0.6274 | 0.6748 | 0.7036 | 0.5435 | 0.1336 | 0.3694
    | 0.9864 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| FMM | $0\%$ | 8 | 0.6211 | 0.6274 | 0.6748 | 0.7036 | 0.5435 | 0.1336 | 0.3694
    | 0.9864 |'
- en: '| seq | $20\%$ | 7 | 0.7500 | 0.7568 | 0.7856 | 0.7905 | 0.7041 | 0.1572 |
    0.4073 | 1.2312 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| seq | $20\%$ | 7 | 0.7500 | 0.7568 | 0.7856 | 0.7905 | 0.7041 | 0.1572 |
    0.4073 | 1.2312 |'
- en: '| random | $20\%$ | 7 | 0.7142 | 0.7251 | 0.7564 | 0.7637 | 0.6436 | 0.1590
    | 0.3996 | 1.2858 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| random | $20\%$ | 7 | 0.7142 | 0.7251 | 0.7564 | 0.7637 | 0.6436 | 0.1590
    | 0.3996 | 1.2858 |'
- en: '| FMM | ${20\%}$ | 7 | 0.6821 | 0.7085 | 0.7163 | 0.7446 | 0.6314 | 0.1527
    | 0.4109 | 1.1696 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| FMM | ${20\%}$ | 7 | 0.6821 | 0.7085 | 0.7163 | 0.7446 | 0.6314 | 0.1527
    | 0.4109 | 1.1696 |'
- en: '| seq | $40\%$ | 5 | 0.7578 | 0.7500 | 0.7749 | 0.7954 | 0.7012 | 0.1606 |
    0.4323 | 1.1940 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| seq | $40\%$ | 5 | 0.7578 | 0.7500 | 0.7749 | 0.7954 | 0.7012 | 0.1606 |
    0.4323 | 1.1940 |'
- en: '| random | $40\%$ | 5 | 0.7559 | 0.7588 | 0.7847 | 0.7925 | 0.6914 | 0.1751
    | 0.4458 | 1.4057 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| random | $40\%$ | 5 | 0.7559 | 0.7588 | 0.7847 | 0.7925 | 0.6914 | 0.1751
    | 0.4458 | 1.4057 |'
- en: '| FMM | ${40\%}$ | 5 | 0.7460 | 0.7559 | 0.7759 | 0.7896 | 0.6797 | 0.1485
    | 0.4053 | 1.1327 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| FMM | ${40\%}$ | 5 | 0.7460 | 0.7559 | 0.7759 | 0.7896 | 0.6797 | 0.1485
    | 0.4053 | 1.1327 |'
- en: '| seq | $60\%$ | 4 | 0.7583 | 0.7764 | 0.8027 | 0.7993 | 0.7031 | 0.1900 |
    0.4767 | 1.6175 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| seq | $60\%$ | 4 | 0.7583 | 0.7764 | 0.8027 | 0.7993 | 0.7031 | 0.1900 |
    0.4767 | 1.6175 |'
- en: '| random | $60\%$ | 4 | 0.7671 | 0.7769 | 0.8042 | 0.8032 | 0.7158 | 0.1894
    | 0.4727 | 1.4461 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| random | $60\%$ | 4 | 0.7671 | 0.7769 | 0.8042 | 0.8032 | 0.7158 | 0.1894
    | 0.4727 | 1.4461 |'
- en: '| FMM | ${60\%}$ | 4 | 0.7510 | 0.7500 | 0.7710 | 0.7871 | 0.6714 | 0.1830
    | 0.4713 | 1.4462 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| FMM | ${60\%}$ | 4 | 0.7510 | 0.7500 | 0.7710 | 0.7871 | 0.6714 | 0.1830
    | 0.4713 | 1.4462 |'
- en: '| seq | $80\%$ | 2 | 0.7715 | 0.7788 | 0.7964 | 0.7954 | 0.7119 | 0.2000 |
    0.4779 | 1.6175 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| seq | $80\%$ | 2 | 0.7715 | 0.7788 | 0.7964 | 0.7954 | 0.7119 | 0.2000 |
    0.4779 | 1.6175 |'
- en: '| random | $80\%$ | 2 | 0.7813 | 0.7827 | 0.8052 | 0.8071 | 0.7178 | 0.1957
    | 0.4658 | 1.5905 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| random | $80\%$ | 2 | 0.7813 | 0.7827 | 0.8052 | 0.8071 | 0.7178 | 0.1957
    | 0.4658 | 1.5905 |'
- en: '| FMM | ${80\%}$ | 2 | 0.7349 | 0.7637 | 0.7783 | 0.7886 | 0.6826 | 0.1940
    | 0.4731 | 1.4555 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| FMM | ${80\%}$ | 2 | 0.7349 | 0.7637 | 0.7783 | 0.7886 | 0.6826 | 0.1940
    | 0.4731 | 1.4555 |'
- en: 'Hyperparameters. We aimed to investigate the impact of varying the weights
    of the video features loss and LLM features loss on the attack effectiveness.
    As shown in Table [7](#Pt0.A2.T7 "Table 7 ‣ Appendix 0.B Additional Experimental
    Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), different combinations of weights can lead to varying degrees of attack
    effectiveness (Note that  and $\lambda_{3}=3$.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '超参数。我们旨在研究视频特征损失和LLM特征损失权重的变化对攻击效果的影响。如表[7](#Pt0.A2.T7 "Table 7 ‣ Appendix
    0.B Additional Experimental Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial
    Attack on Video-based LLMs")所示，不同的权重组合会导致攻击效果的不同程度（注意 $\lambda_{3}=3$）。'
- en: 'Table 7: Influence of the weights on the attack effectiveness. We fix the sparsity
    loss weight $\lambda_{1}$ and vary the weights of the video features loss and
    LLM features loss to explore their relative relationship. Lower scores indicate
    better attack effectiveness.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 权重对攻击效果的影响。我们固定稀疏性损失权重 $\lambda_{1}$，并改变视频特征损失和LLM特征损失的权重，以探究它们的相对关系。较低的分数表示攻击效果更好。'
- en: '|  | Clip Score  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | Clip Score  |'
- en: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE-L | CIDEr |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| RN50 | RN101 | ViT-B/16 | ViT-B/32 | ViT-L/14 | BLEU | ROUGE-L | CIDEr |'
- en: '| 1 | 1 | 0.7339 | 0.7383 | 0.7544 | 0.7656 | 0.6641 | 0.2133 | 0.4843 | 1.6687
    |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0.7339 | 0.7383 | 0.7544 | 0.7656 | 0.6641 | 0.2133 | 0.4843 | 1.6687
    |'
- en: '| 1 | 2 | 0.7011 | 0.6665 | 0.7036 | 0.7461 | 0.6167 | 0.0602 | 0.2248 | 0.4535
    |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 0.7011 | 0.6665 | 0.7036 | 0.7461 | 0.6167 | 0.0602 | 0.2248 | 0.4535
    |'
- en: '| 1 | 3 | 0.6491 | 0.6060 | 0.6836 | 0.6968 | 0.5566 | 0.0230 | 0.1585 | 0.1601
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | 0.6491 | 0.6060 | 0.6836 | 0.6968 | 0.5566 | 0.0230 | 0.1585 | 0.1601
    |'
- en: '| 1 | 4 | 0.7373 | 0.7192 | 0.7612 | 0.7705 | 0.6724 | 0.1710 | 0.3869 | 1.5064
    |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 4 | 0.7373 | 0.7192 | 0.7612 | 0.7705 | 0.6724 | 0.1710 | 0.3869 | 1.5064
    |'
- en: '| 1 | 5 | 0.7192 | 0.7061 | 0.7393 | 0.7539 | 0.6392 | 0.1207 | 0.3276 | 0.9869
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 5 | 0.7192 | 0.7061 | 0.7393 | 0.7539 | 0.6392 | 0.1207 | 0.3276 | 0.9869
    |'
- en: '| 2 | 1 | 0.6621 | 0.6499 | 0.6997 | 0.7285 | 0.5942 | 0.0624 | 0.2159 | 0.4087
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0.6621 | 0.6499 | 0.6997 | 0.7285 | 0.5942 | 0.0624 | 0.2159 | 0.4087
    |'
- en: '| 3 | 1 | 0.7129 | 0.7227 | 0.7432 | 0.7534 | 0.6470 | 0.1142 | 0.3648 | 1.0183
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0.7129 | 0.7227 | 0.7432 | 0.7534 | 0.6470 | 0.1142 | 0.3648 | 1.0183
    |'
- en: '| 4 | 1 | 0.7188 | 0.7217 | 0.7398 | 0.7744 | 0.6616 | 0.1670 | 0.4418 | 1.4612
    |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 0.7188 | 0.7217 | 0.7398 | 0.7744 | 0.6616 | 0.1670 | 0.4418 | 1.4612
    |'
- en: '| 5 | 1 | 0.7559 | 0.7578 | 0.7891 | 0.8018 | 0.7173 | 0.2047 | 0.4734 | 1.8045
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1 | 0.7559 | 0.7578 | 0.7891 | 0.8018 | 0.7173 | 0.2047 | 0.4734 | 1.8045
    |'
- en: Appendix 0.C Transfer-based Black-box Attacks
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.C 基于转移的黑箱攻击
- en: '![Refer to caption](img/d46069ce0268acf4b8ad085f641c4363.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d46069ce0268acf4b8ad085f641c4363.png)'
- en: 'Figure 9: Transfer-based black-box attack on VideoChat.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：基于转移的黑箱攻击在 VideoChat 上的效果。
- en: 'Table 8: Black-box attacks against VideoChat [[11](#bib.bib11)] on the ActivityNet-200 [[3](#bib.bib3)]
    dataset: Comparison of image caption metrics and GPT score for different attack
    types. The sparsity of the temporal mask is set to 0\. ${\Delta}$: the mean of
    the modified pixels. We apply the attack video on Video-ChatGPT [[15](#bib.bib15)]
    and directly transfer it to VideoChat [[11](#bib.bib11)].'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：对 VideoChat [[11](#bib.bib11)] 在 ActivityNet-200 [[3](#bib.bib3)] 数据集上的黑箱攻击：不同攻击类型下图像字幕指标和
    GPT 分数的比较。时间掩码的稀疏度设置为 0\. ${\Delta}$：修改像素的平均值。我们将攻击视频应用于 Video-ChatGPT [[15](#bib.bib15)]
    并直接转移到 VideoChat [[11](#bib.bib11)]。
- en: '| Type |  | GPT-3.5  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 类型 |  | GPT-3.5  |'
- en: '| BLEU | ROUGE | CIDEr | Accurate | Score | Accurate | Score |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | ROUGE | CIDEr | Accurate | Score | Accurate | Score |'
- en: '| Clean | 0 | 0.0765 | 0.3358 | 0.3379 | 0.35 | 2.87 | 0.28 | 1.79 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Clean | 0 | 0.0765 | 0.3358 | 0.3379 | 0.35 | 2.87 | 0.28 | 1.79 |'
- en: '| Transfer-based Attack | 2 | 0.0638 | 0.2492 | 0.2870 | 0.08 | 1.56 | 0.10
    | 1.32 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 基于转移的攻击 | 2 | 0.0638 | 0.2492 | 0.2870 | 0.08 | 1.56 | 0.10 | 1.32 |'
- en: 'In addition to white-box attacks, we have also investigated the transferability
    of these attacks. We conduct a black-box attack on VideoChat [[11](#bib.bib11)].
    Specifically, we employ the FMM-Attack method to perform a white-box attack on
    Video-ChatGPT [[15](#bib.bib15)], resulting in the attack video  is then directly
    used as input for VideoChat [[11](#bib.bib11)], with the experimental results
    displayed in Table [8](#Pt0.A3.T8 "Table 8 ‣ Appendix 0.C Transfer-based Black-box
    Attacks ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"). It is evident that the model’s answer accuracy decreases significantly.
    Even without obtaining the gradient of VideoChat [[11](#bib.bib11)], the attack
    is successful, and there are instances of garbled text. The visualization results
    is shown in Fig. [9](#Pt0.A3.F9 "Figure 9 ‣ Appendix 0.C Transfer-based Black-box
    Attacks ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs").'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '除了白箱攻击外，我们还研究了这些攻击的转移性。我们对 VideoChat 进行了一次黑箱攻击[[11](#bib.bib11)]。具体而言，我们使用
    FMM-Attack 方法对 Video-ChatGPT [[15](#bib.bib15)] 进行白箱攻击，然后将攻击视频直接用作 VideoChat [[11](#bib.bib11)]
    的输入，实验结果如表[8](#Pt0.A3.T8 "Table 8 ‣ Appendix 0.C Transfer-based Black-box Attacks
    ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs")所示。显然，模型的回答准确率显著下降。即使没有获得
    VideoChat [[11](#bib.bib11)] 的梯度，攻击仍然成功，并且出现了乱码。可视化结果如图[9](#Pt0.A3.F9 "Figure
    9 ‣ Appendix 0.C Transfer-based Black-box Attacks ‣ FMM-Attack: A Flow-based Multi-modal
    Adversarial Attack on Video-based LLMs")所示。'
- en: Appendix 0.D Additional Visualization Results
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.D 附加可视化结果
- en: In this section, we provide supplementary visualization results to further illustrate
    the impact of our attack method on the targeted models. The additional visualizations
    complement the main text’s qualitative analysis, offering a more in-depth understanding
    of our attack’s effectiveness and its implications on various models.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了补充的可视化结果，以进一步说明我们攻击方法对目标模型的影响。这些额外的可视化结果补充了主文中的定性分析，提供了对我们攻击效果及其对各种模型影响的更深入理解。
- en: 'As depicted in Fig. [10](#Pt0.A4.F10 "Figure 10 ‣ Appendix 0.D Additional Visualization
    Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), when attacking Video-ChatGPT[[15](#bib.bib15)], the model consistently
    generates garbled responses, such as the repetition of the number "666666". This
    figure emphasizes the potency of our attack method, as it renders the model incapable
    of producing meaningful and contextually relevant responses. The consistent generation
    of garbled output highlights the vulnerability of the Video-ChatGPT model to adversarial
    attacks. In Fig.[11](#Pt0.A4.F11 "Figure 11 ‣ Appendix 0.D Additional Visualization
    Results ‣ FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based
    LLMs"), we present the results of our attack on VideoChat[[11](#bib.bib11)]. Similar
    to the case of Video-ChatGPT, the model generates garbled responses, although
    the format differs. Notably, VideoChat sometimes tends to answer with phrases
    like "It’s not clear…", which also signifies the success of our attack, as it
    effectively erases the video information. This result underlines the transferability
    of our attack method across different models and its ability to disrupt their
    performance.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[10](#Pt0.A4.F10 "图 10 ‣ 附录 0.D 额外的可视化结果 ‣ FMM-Attack: 一种基于流的多模态对抗攻击视频基础的LLMs")所示，在攻击
    Video-ChatGPT[[15](#bib.bib15)]时，该模型始终生成乱码响应，例如重复的数字“666666”。该图强调了我们攻击方法的威力，因为它使得模型无法生成有意义和与上下文相关的响应。持续生成乱码输出突显了
    Video-ChatGPT 模型在对抗攻击中的脆弱性。在图[11](#Pt0.A4.F11 "图 11 ‣ 附录 0.D 额外的可视化结果 ‣ FMM-Attack:
    一种基于流的多模态对抗攻击视频基础的LLMs")中，我们展示了对 VideoChat[[11](#bib.bib11)]的攻击结果。与 Video-ChatGPT
    的情况类似，该模型生成了乱码响应，尽管格式有所不同。值得注意的是，VideoChat 有时倾向于用“不是很清楚……”这样的短语来回答，这也表明我们的攻击取得了成功，因为它有效地抹去了视频信息。这个结果突出了我们攻击方法在不同模型之间的可迁移性以及其破坏模型性能的能力。'
- en: These additional visualization results, combined with the main text’s analysis,
    provide a comprehensive understanding of the impact of our attack method on the
    targeted models. They emphasize the need for enhancing the robustness of these
    models against adversarial attacks and demonstrate the importance of considering
    different attack scenarios and their consequences. Furthermore, these results
    highlight the potential challenges in developing robust video question-answering
    systems and underscore the importance of addressing these vulnerabilities to ensure
    the reliability and security of such models in real-world applications.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这些额外的可视化结果，加上主要文本中的分析，提供了对我们攻击方法对目标模型影响的全面理解。它们强调了增强这些模型对抗对抗攻击的鲁棒性的重要性，并展示了考虑不同攻击场景及其后果的必要性。此外，这些结果突出了在开发鲁棒的视频问答系统时可能面临的挑战，并强调了应对这些脆弱性以确保这些模型在实际应用中可靠性和安全性的重要性。
- en: '![Refer to caption](img/ac7674fc42354815424295492ee5eabc.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ac7674fc42354815424295492ee5eabc.png)'
- en: 'Figure 10: Perturbed videos generated by Video-ChatGPT, showcasing the impact
    of our attack method on the model’s responses. The figure illustrates the garbled
    responses produced by the model, such as the repetition of the number "666666",
    highlighting the vulnerability of Video-ChatGPT to our adversarial attack.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: Video-ChatGPT 生成的扰动视频，展示了我们攻击方法对模型响应的影响。图中展示了模型生成的乱码响应，例如重复的数字“666666”，突显了
    Video-ChatGPT 对我们对抗攻击的脆弱性。'
- en: '![Refer to caption](img/f7de05236c1b7a8fe13a5ce2b4724bde.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f7de05236c1b7a8fe13a5ce2b4724bde.png)'
- en: 'Figure 11: Perturbed videos generated by VideoChat, demonstrating the effects
    of our attack method on the model’s responses. The figure displays the garbled
    responses produced by the model, including phrases like "It’s not clear…", emphasizing
    the vulnerability of VideoChat to our adversarial attack.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: VideoChat 生成的扰动视频，展示了我们攻击方法对模型响应的影响。图中显示了模型生成的乱码响应，包括类似“不是很清楚……”的短语，强调了
    VideoChat 对我们对抗攻击的脆弱性。'
- en: Appendix 0.E Ethics Statement
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.E 伦理声明
- en: Please note that we restrict all experiments in the laboratory environment and
    do not support our FMM-Attack in the real scenario. The purpose of our work is
    to raise the awareness of the security concern in availability of video-based
    LLMs and call for practitioners to pay more attention to the adversarial robustness
    of video-based LLMs and model trustworthy deployment.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将所有实验限制在实验室环境中，并不支持我们在实际场景中的FMM-Attack。我们工作的目的是提高对基于视频的LLMs可用性安全问题的认识，并呼吁从业者更加关注基于视频的LLMs的对抗鲁棒性以及模型的可信部署。
- en: Appendix 0.F Reproducibility Statement
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.F 可重复性声明
- en: 'The detailed descriptions of models, datasets, and experimental setups are
    provided in Section [0.A](#Pt0.A1 "Appendix 0.A Implementation Details ‣ FMM-Attack:
    A Flow-based Multi-modal Adversarial Attack on Video-based LLMs"). We provide
    part of the codes to reproduce our FMM-Attack in the supplementary material. We
    will provide the remaining codes for reproducing our method upon the acceptance
    of the paper.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、数据集和实验设置的详细描述见第[0.A](#Pt0.A1 "附录 0.A 实施细节 ‣ FMM-Attack：基于流的多模态对抗攻击于视频LLMs")节。我们在补充材料中提供了部分代码以重现我们的FMM-Attack。我们将在论文接受后提供剩余的代码以重现我们的方法。
- en: Appendix 0.G Limitation
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.G 限制
- en: Our FMM-Attack is primarily concentrated on the digital world, operating under
    the assumption that input videos are fed directly into the models. However, as
    technology advances, we anticipate that video-based LLMs will be increasingly
    deployed in more complex, real-world scenarios. These scenarios could include
    autonomous driving, where input videos are not pre-recorded but rather captured
    in real-time from physical environments via cameras. Future research should explore
    the execution and impact of adversarial attacks in the physical world. This would
    provide a more comprehensive evaluation of the security of video-based LLMs, contributing
    to the development of more robust and reliable systems for real-world deployment.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的FMM-Attack主要集中于数字世界，假设输入视频直接输入到模型中。然而，随着技术的发展，我们预计基于视频的LLMs将在更复杂的实际场景中得到越来越多的应用。这些场景可能包括自动驾驶，其中输入视频不是预先录制的，而是通过摄像头实时捕获的。未来的研究应探索对抗攻击在物理世界中的执行和影响。这将提供对基于视频的LLMs安全性的更全面评估，促进在实际部署中开发更强大和可靠的系统。
