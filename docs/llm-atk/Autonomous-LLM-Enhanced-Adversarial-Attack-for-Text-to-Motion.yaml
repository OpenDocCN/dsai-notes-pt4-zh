- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:43:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:43:42'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自主LLM增强的文本到运动对抗攻击
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.00352](https://ar5iv.labs.arxiv.org/html/2408.00352)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.00352](https://ar5iv.labs.arxiv.org/html/2408.00352)
- en: Honglei Miao¹, Fan Ma², Ruijie Quan², Kun Zhan^(1,⋆), and Yi Yang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Honglei Miao¹, Fan Ma², Ruijie Quan², Kun Zhan^(1,⋆) 和 Yi Yang²
- en: 1\. School of Information Science and Engineering, Lanzhou University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 信息科学与工程学院，兰州大学
- en: 2\. CCAI, Zhejiang University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. CCAI, 浙江大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Human motion generation driven by deep generative models has enabled compelling
    applications, but the ability of text-to-motion (T2M) models to produce realistic
    motions from text prompts raises security concerns if exploited maliciously. Despite
    growing interest in T2M, few methods focus on safeguarding these models against
    adversarial attacks, with existing work on text-to-image models proving insufficient
    for the unique motion domain. In the paper, we propose ALERT-Motion, an autonomous
    framework leveraging large language models (LLMs) to craft targeted adversarial
    attacks against black-box T2M models. Unlike prior methods modifying prompts through
    predefined rules, ALERT-Motion uses LLMs’ knowledge of human motion to autonomously
    generate subtle yet powerful adversarial text descriptions. It comprises two key
    modules: an adaptive dispatching module that constructs an LLM-based agent to
    iteratively refine and search for adversarial prompts; and a multimodal information
    contrastive module that extracts semantically relevant motion information to guide
    the agent’s search. Through this LLM-driven approach, ALERT-Motion crafts adversarial
    prompts querying victim models to produce outputs closely matching targeted motions,
    while avoiding obvious perturbations. Evaluations across popular T2M models demonstrate
    ALERT-Motion’s superiority over previous methods, achieving higher attack success
    rates with stealthier adversarial prompts. This pioneering work on T2M adversarial
    attacks highlights the urgency of developing defensive measures as motion generation
    technology advances, urging further research into safe and responsible deployment.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由深度生成模型驱动的人体运动生成已经实现了引人注目的应用，但如果文本到运动（T2M）模型将文本提示生成现实运动的能力被恶意利用，将会引发安全问题。尽管对T2M的兴趣日益增长，但很少有方法关注保护这些模型免受对抗攻击，现有针对文本到图像模型的研究对独特的运动领域而言尚不够充分。本文提出了ALERT-Motion，一个自主框架，利用大型语言模型（LLMs）对黑箱T2M模型进行针对性的对抗攻击。与通过预定义规则修改提示的前期方法不同，ALERT-Motion利用LLMs对人体运动的知识，自动生成细微但强大的对抗性文本描述。它包括两个关键模块：一个自适应调度模块，该模块构建一个基于LLM的代理，迭代地优化和搜索对抗提示；以及一个多模态信息对比模块，提取语义相关的运动信息以指导代理的搜索。通过这种LLM驱动的方法，ALERT-Motion生成对抗提示查询受害模型，以产生与目标运动紧密匹配的输出，同时避免明显的扰动。对流行T2M模型的评估表明，ALERT-Motion在攻击成功率和隐蔽性上优于之前的方法。这项开创性的T2M对抗攻击研究突显了随着运动生成技术的发展，制定防御措施的紧迫性，并呼吁进一步研究安全和负责任的部署。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/d424b629024cbe03dced5924c683063e.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d424b629024cbe03dced5924c683063e.png)'
- en: 'Figure 1: Adversarial prompt against T2M model with RIATIG and our ALERT-Motion.
    Previous methods like RIATIG only perturb prompts through predefined character
    or word operations, overlooking the integrity and semantics of the prompts. Our
    ALERT-Motion doesn’t require such predefined operations; instead, by multimodal
    information contrastive (MMIC) module, the language model autonomously learn and
    perform these operations, dynamically generating adversarial prompts that meet
    the attack requirements. Under the same input (target and initial prompt), our
    method captures more natural and fluent prompts related to motion. When these
    prompts are used to query the victim T2M model, the resulting motion show a stronger
    resemblance to the target motion. Darker color indicates later frames in the sequence.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：针对T2M模型的对抗性提示，包括RIATIG和我们的ALERT-Motion。之前的方法如RIATIG仅通过预定义的字符或词操作来扰动提示，忽略了提示的完整性和语义。我们的ALERT-Motion不需要这些预定义操作；相反，通过多模态信息对比（MMIC）模块，语言模型自主学习并执行这些操作，动态生成满足攻击要求的对抗性提示。在相同的输入（目标和初始提示）下，我们的方法捕获到更多与运动相关的自然流畅提示。当这些提示用于查询受害的T2M模型时，生成的运动与目标运动的相似度更强。较暗的颜色表示序列中的后续帧。
- en: Human motion generation is a task aimed at producing natural and realistic human
    motions. It drives advancements in downstream applications such as animation and
    movie production, virtual human construction, robotics and human-robot interaction [[42](#bib.bib42)].
    In recent years, with the development of deep learning, especially the growth
    of generative models such as Generative Adversarial Network (GAN) [[8](#bib.bib8)],
    Variational Autoencoder (VAE) [[17](#bib.bib17)], and diffusion model [[14](#bib.bib14)],
    trained models have become capable of generating very natural motions [[35](#bib.bib35),
    [7](#bib.bib7), [11](#bib.bib11), [40](#bib.bib40)]. Some models [[3](#bib.bib3),
    [4](#bib.bib4), [18](#bib.bib18), [28](#bib.bib28)] even extend the generated
    motions for several minutes while satisfying given conditions. Among these motion
    generation models, text-to-motion (T2M) [[35](#bib.bib35), [7](#bib.bib7), [11](#bib.bib11),
    [40](#bib.bib40), [3](#bib.bib3), [4](#bib.bib4), [28](#bib.bib28)] gains particular
    attention from the community due to the user-friendly nature of text prompts that
    align with human expression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人体运动生成是一项旨在产生自然和逼真的人体动作的任务。它推动了下游应用的进步，如动画和电影制作、虚拟人类构建、机器人技术和人机交互[[42](#bib.bib42)]。近年来，随着深度学习的发展，尤其是生成对抗网络（GAN）[[8](#bib.bib8)]、变分自编码器（VAE）[[17](#bib.bib17)]和扩散模型[[14](#bib.bib14)]等生成模型的增长，训练过的模型已经能够生成非常自然的动作[[35](#bib.bib35),
    [7](#bib.bib7), [11](#bib.bib11), [40](#bib.bib40)]。一些模型[[3](#bib.bib3), [4](#bib.bib4),
    [18](#bib.bib18), [28](#bib.bib28)]甚至能够在满足给定条件的情况下，将生成的动作延续几分钟。在这些运动生成模型中，文本到运动（T2M）[[35](#bib.bib35),
    [7](#bib.bib7), [11](#bib.bib11), [40](#bib.bib40), [3](#bib.bib3), [4](#bib.bib4),
    [28](#bib.bib28)]因其与人类表达相符的用户友好型文本提示而受到特别关注。
- en: Generating motions that exactly align with textual descriptions and are nearly
    the same as the real physical world is becoming increasingly feasible. However,
    allowing models to freely generate motions conditioned on arbitrary text prompts
    is even more dangerous than text-to-image (T2I). When they are applied to downstream
    tasks, such capabilities are maliciously exploited by attackers. For instance,
    in animation or movie production [[28](#bib.bib28)], they are used to create more
    realistic harmful content involving pornography or violence. The risks are boosted
    when using the generated motions as humanoid controllers [[23](#bib.bib23)], as
    they eventually are deployed on robots, posing potential threats to human safety.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 生成与文本描述完全对齐且几乎与真实物理世界相同的动作变得越来越可行。然而，允许模型根据任意文本提示自由生成动作，比文本到图像（T2I）更加危险。当这些能力应用于下游任务时，攻击者会恶意利用这些能力。例如，在动画或电影制作[[28](#bib.bib28)]中，它们被用于创建更多现实的有害内容，如色情或暴力。当使用生成的动作作为类人控制器[[23](#bib.bib23)]时，风险会增加，因为它们最终会部署在机器人上，可能对人类安全构成威胁。
- en: Despite the growing focus on T2M tasks, there is currently a lack of research
    addressing the safety concerns specific to this domain. The most relevant line
    of work is on the safety of T2I [[25](#bib.bib25), [34](#bib.bib34), [19](#bib.bib19)].
    These researches largely focus on how character-level or word-level modifications
    to benign text prompts could induce unintended outputs from the models. Early
    work [[25](#bib.bib25), [34](#bib.bib34)] primarily explored the existence of
    this phenomenon, until the RIATIG [[19](#bib.bib19)] is inspired by them to propose
    targeted attacks against image generation models to raise awareness of potential
    security risks about T2I. However, these existing studies often search for adversarial
    attacks by modifying words to uncommon personal names, locations or other proper
    nouns, which is overlooked in image generation but would appear clearly out of
    place for motion tasks, making the attacks more easily detectable. Additionally,
    unlike the abundant image-text pairs available for image tasks, the limited data
    for motions makes it challenging to accurately measure the similarity between
    different motions, posing further difficulties for targeted attacks on T2M models.
    They make the findings from T2I safety difficult to directly apply to the motion
    domain.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管T2M任务的关注度日益增加，目前仍缺乏针对该领域特定安全问题的研究。最相关的研究方向是T2I的安全性[[25](#bib.bib25), [34](#bib.bib34),
    [19](#bib.bib19)]。这些研究主要关注对善意文本提示的字符级或词汇级修改如何导致模型产生意外输出。早期工作[[25](#bib.bib25),
    [34](#bib.bib34)]主要探讨了这种现象的存在，直到RIATIG[[19](#bib.bib19)]受其启发，提出了针对图像生成模型的目标攻击，以提高对T2I潜在安全风险的认识。然而，这些现有研究通常通过将词汇修改为不常见的个人名称、地点或其他专有名词来寻找对抗攻击，这在图像生成中可能被忽视，但在运动任务中会显得格格不入，使攻击更容易被检测到。此外，与图像任务中丰富的图像-文本对相比，运动数据有限，使得准确测量不同运动之间的相似性变得具有挑战性，从而为T2M模型的目标攻击带来了更多困难。这使得T2I安全性研究的发现难以直接应用于运动领域。
- en: To address the challenges of adversarial attacks on T2M models, we introduce
    ALERT-Motion, an autonomous large language model (LLM) enhanced adversarial attack
    against T2M models in a black-box setting. Unlike prior work, our ALERT-Motion
    leverages the knowledge about motions contained in LLMs to generate subtle yet
    powerful adversarial descriptions, whose outputs from the victim model closely
    match the desired motion. Crucially, the entire attack process is done automatically
    by LLM agent, using its own reasoning abilities to carry out the attack, without
    needing human-defined rules for operations like inserting, deleting or replacing
    characters or words.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决对T2M模型的对抗攻击挑战，我们引入了ALERT-Motion，这是一种在黑箱设置下针对T2M模型的增强型对抗攻击方法。与以往的研究不同，我们的ALERT-Motion利用了LLM中包含的运动知识，生成细微却强大的对抗性描述，使得受害模型的输出与期望的运动紧密匹配。关键是，整个攻击过程由LLM代理自动完成，利用其自身的推理能力执行攻击，而无需人工定义的操作规则，如插入、删除或替换字符或词语。
- en: 'As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Autonomous LLM-Enhanced
    Adversarial Attack for Text-to-Motion"), previous state-of-the-art attack methods
    like RIATIG [[19](#bib.bib19)] for T2I models employ manually defined word or
    character-level modifications and prompt-level crossover, making it difficult
    to find natural and fluent adversarial text prompts. Such methods often result
    in obvious personal names or proper nouns like “Sebastian Hohenthal Fortec Motorsport”
    or “Boris Novachkov” appearing abruptly in descriptions of motions. In contrast,
    our proposed ALERT-Motion gives the modification of adversarial text prompts entirely
    to LLMs. It comprises two key modules: the adaptive dispatching (AD) module and
    the multimodal information contrastive (MMIC) module. In AD module, by simply
    designing instructions for different processes, LLM autonomously searches for
    adversarial text prompts that appear natural and fluent, avoiding the abrupt word
    insertions seen in RIATIG. However, as LLMs lack inherent capabilities for processing
    motion modality, we design MMIC module to obtain semantically similar information
    to the target motion, thereby assisting AD module in finding better adversarial
    text prompts. Through the coordinated operation of these two modules, ALERT-Motion
    generates adversarial text prompts that are not only natural and fluent but also
    query the victim T2M model to produce outputs closely resembling the target motions.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 自主LLM增强的文本到动作对抗攻击")所示，之前的先进攻击方法如RIATIG [[19](#bib.bib19)]
    针对T2I模型采用了手动定义的词或字符级别的修改和提示级别的交叉，这使得难以找到自然流畅的对抗文本提示。这些方法通常会导致明显的个人名字或专有名词，如“Sebastian
    Hohenthal Fortec Motorsport”或“Boris Novachkov”突然出现在动作描述中。相比之下，我们提出的ALERT-Motion将对抗文本提示的修改完全交给LLM。它包括两个关键模块：自适应调度（AD）模块和多模态信息对比（MMIC）模块。在AD模块中，通过简单地设计不同过程的指令，LLM自主搜索自然流畅的对抗文本提示，避免了RIATIG中出现的突然词汇插入。然而，由于LLM缺乏处理动作模态的固有能力，我们设计了MMIC模块以获得与目标动作语义相似的信息，从而辅助AD模块找到更好的对抗文本提示。通过这两个模块的协调操作，ALERT-Motion生成的对抗文本提示不仅自然流畅，还能查询受害者T2M模型以产生与目标动作相似的输出。
- en: 'In summary, the key contributions are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，关键贡献如下：
- en: 1\. To the best of our knowledge, we are the first to propose an adversarial
    targeted attack method, ALERT-Motion, for T2M models. We introduce an autonomous
    LLM-enhanced adversarial attacks on T2M models.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 据我们所知，我们是首个提出针对T2M模型的对抗性目标攻击方法ALERT-Motion的团队。我们介绍了一种自主LLM增强的T2M模型对抗攻击。
- en: 2\. Our proposed ALERT-Motion consists of two key modules. A novel AD module
    constructs an LLM agent that incorporates the agent’s inherent natural language
    and domain knowledge of motions into the automatic attack process. Additionally,
    MMIC module performs high-level semantic extraction of motion modalities and provides
    necessary semantical information to support AD’s reasoning and decision-making.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 我们提出的ALERT-Motion由两个关键模块组成。一个新颖的AD模块构建了一个LLM代理，将代理的固有自然语言和动作领域知识融入自动攻击过程中。此外，MMIC模块执行对动作模态的高层语义提取，并提供必要的语义信息以支持AD的推理和决策。
- en: 3\. We evaluate ALERT-Motion on two popular T2M models and compare it against
    two previous adversarial attack methods originally applied to T2I models. Experimental
    results demonstrate that our proposed ALERT-Motion achieves higher attack success
    rates while generating more natural and stealthy adversarial prompts that are
    difficult to detect.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 我们在两个流行的T2M模型上评估了ALERT-Motion，并与两个之前应用于T2I模型的对抗攻击方法进行了比较。实验结果表明，我们提出的ALERT-Motion实现了更高的攻击成功率，同时生成了更自然和隐蔽的对抗提示，难以检测。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Text-to-Motion (T2M)
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到动作（T2M）
- en: T2M is a conditional motion generation task that aims to generate semantically
    matching and natural motion sequences from human-friendly natural language text
    descriptions. Its promising performance is driven by deep generative models such
    as GANs, VAEs, diffusion models, etc. One of the early works in this domain, Text2Action [[1](#bib.bib1)],
    leverages GANs to create abundant realistic motions. Some research also explores
    the use of VAEs for generation, where Language2Pose [[2](#bib.bib2)] proposes
    an end-to-end text-to-pose generation framework that utilizes a VAE to model the
    latent space between text and motion. TEACH [[3](#bib.bib3)] further combines
    previous motions as extra inputs to the encoder module, enabling natural and coherent
    motion generation when handling multiple text inputs. With the rise of diffusion
    models in the generative domain, some studies have also employed diffusion models
    for motion generation. MDM [[35](#bib.bib35)] utilizes a diffusion model to predict
    the sample at each diffusion step rather than just the noise. MLD [[7](#bib.bib7)]
    adopts latent diffusion along with a VAE to generate motions, significantly boosting
    the generation speed without compromising quality. Additionally, there are studies
    that combine VQ-VAE with GPT-like transformers. TM2T [[12](#bib.bib12)] and T2M-GPT [[40](#bib.bib40)]
    utilize VQ-VAE to concatenate training T2M and motion-to-text modules. These works
    continuously improving the quality, coherence, and efficiency of motion generation
    from text descriptions. However, there has been no research focusing on attacks
    and defenses of the T2M model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: T2M 是一种条件运动生成任务，旨在从人类友好的自然语言文本描述中生成语义匹配且自然的运动序列。其令人鼓舞的表现得益于深度生成模型，如 GANs、VAEs、扩散模型等。在这一领域的早期工作之一，Text2Action
    [[1](#bib.bib1)]，利用 GANs 创建丰富的真实运动。一些研究还探讨了使用 VAEs 进行生成，其中 Language2Pose [[2](#bib.bib2)]
    提出了一个端到端的文本到姿态生成框架，利用 VAE 建模文本与运动之间的潜在空间。TEACH [[3](#bib.bib3)] 进一步将先前的运动作为额外输入结合到编码器模块中，使得在处理多个文本输入时能够生成自然且连贯的运动。随着扩散模型在生成领域的兴起，一些研究也采用了扩散模型进行运动生成。MDM
    [[35](#bib.bib35)] 利用扩散模型在每个扩散步骤预测样本，而不仅仅是噪声。MLD [[7](#bib.bib7)] 采用潜在扩散与 VAE
    生成运动，大大提高了生成速度而不损害质量。此外，还有研究将 VQ-VAE 与类似 GPT 的变换器相结合。TM2T [[12](#bib.bib12)] 和
    T2M-GPT [[40](#bib.bib40)] 利用 VQ-VAE 来连接训练 T2M 和运动到文本模块。这些工作不断提升从文本描述生成运动的质量、连贯性和效率。然而，目前尚未有针对
    T2M 模型的攻击和防御的研究。
- en: Adversarial Attacks on Text-driven Generative Models
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '**对文本驱动的生成模型的对抗攻击**'
- en: Due to the convenience of text input for users, it serves as the most common
    driving condition for many multimodal generation models. However, the inherent
    complexity of text input inevitably introduces vulnerabilities to the generative
    models driven by it. Existing research on adversarial attacks in T2I models, such
    as [[29](#bib.bib29), [43](#bib.bib43), [19](#bib.bib19), [20](#bib.bib20)], attack
    T2I models by modifying the input text, causing abnormal outputs. The types of
    abnormal outputs may include degraded synthesis quality [[20](#bib.bib20)], disappearance
    or alteration of objects in the image [[43](#bib.bib43), [19](#bib.bib19)]. Among
    them, [[19](#bib.bib19)] manipulates words and characters, thereby causing the
    targeted objects specified by the attacker to be generated in the image by the
    victim T2I model. These studies indicate the lack of robustness of existing T2I
    models to input text. With the occurrence of LLMs, many studies also focus on
    the vulnerabilities of LLMs. A large portion of them focus on jailbreaking, making
    LLMs answer queries that violate safety policies. Jailbreaking strategies have
    evolved from manual prompt engineering [[36](#bib.bib36), [22](#bib.bib22)] to
    LLM-based automated red-teaming [[26](#bib.bib26), [21](#bib.bib21)]. Beyond these
    template-based jailbreaks aimed at identifying effective jailbreak prompt templates,
    a more general jailbreaking method called Greedy Coordinate Gradient [[44](#bib.bib44)]
    is recently proposed. It uses a white-box model to train adversarial suffixes
    that maximize the probability of an LLM producing affirmative responses. They [[44](#bib.bib44),
    [33](#bib.bib33)] find that the identified suffixes transfer to closed-source
    off-the-shelf LLMs. The vulnerabilities of T2M models share similarities with
    the aforementioned security research on text-driven generative models. However,
    since the correspondence between motion and text involves the time dimension,
    the adversarial attack methods from the above studies cannot be directly applied
    to T2M.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本输入对用户的便利性，它成为许多多模态生成模型最常见的驱动条件。然而，文本输入固有的复杂性不可避免地引入了生成模型的漏洞。现有的关于T2I模型（文本到图像模型）的对抗攻击研究，如[[29](#bib.bib29)、[43](#bib.bib43)、[19](#bib.bib19)、[20](#bib.bib20)]，通过修改输入文本来攻击T2I模型，导致异常输出。这些异常输出可能包括合成质量下降[[20](#bib.bib20)]、图像中的物体消失或改变[[43](#bib.bib43)、[19](#bib.bib19)]。其中，[[19](#bib.bib19)]通过操控词汇和字符，使攻击者指定的目标物体被生成在受害者T2I模型的图像中。这些研究表明现有T2I模型对输入文本缺乏鲁棒性。随着LLMs的出现，许多研究也集中在LLMs的漏洞上。它们大部分关注于越狱，使LLMs回答违反安全政策的查询。越狱策略已经从手动提示工程[[36](#bib.bib36)、[22](#bib.bib22)]发展到基于LLM的自动化红队测试[[26](#bib.bib26)、[21](#bib.bib21)]。除了这些旨在识别有效越狱提示模板的模板化越狱方法之外，最近还提出了一种更通用的越狱方法——贪婪坐标梯度[[44](#bib.bib44)]。它使用白盒模型训练对抗性后缀，以最大化LLM生成肯定回应的概率。他们[[44](#bib.bib44)、[33](#bib.bib33)]发现，所识别的后缀可以迁移到封闭源的现成LLMs。T2M模型的漏洞与上述文本驱动生成模型的安全研究有相似之处。然而，由于运动和文本之间的对应关系涉及时间维度，因此上述研究中的对抗攻击方法不能直接应用于T2M。
- en: LLM Agents
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理
- en: Research on using LLMs to enhance autonomous agents has seen a growing trend
    in recent years [[41](#bib.bib41)]. These LLM-powered agents, exemplified by HuggingGPT [[32](#bib.bib32)],
    WebGPT [[13](#bib.bib13)], and MM-REACT [[38](#bib.bib38)], have been employed
    to tackle intricate tasks that demand effective understanding and planning from
    the agents. A considerable proportion of these studies leverage the rich commonsense
    knowledge inherently embedded within LLMs to execute downstream tasks with minimal
    or no additional training data. This approach helps to maintain the robust foundational
    world knowledge in LLMs. The demonstrated capabilities of LLMs encompass features
    such as zero-shot planning in real-world scenarios [[15](#bib.bib15)]. Inspired
    by these explorations, we introduce LLMs into the realm of adversarial attacks
    on T2M models, achieving an autonomous attack agent adept at crafting effective
    adversarial prompts.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，使用LLMs（大语言模型）来增强自主代理的研究呈现出增长的趋势[[41](#bib.bib41)]。以HuggingGPT[[32](#bib.bib32)]、WebGPT[[13](#bib.bib13)]和MM-REACT[[38](#bib.bib38)]为代表的这些LLM驱动的代理已被用于处理复杂的任务，这些任务要求代理具备有效的理解和规划能力。这些研究中的相当一部分利用了LLMs中固有的丰富常识知识来执行下游任务，几乎无需额外的训练数据。这种方法有助于保持LLMs中的强大基础世界知识。LLMs展现出的能力包括在现实世界场景中的零-shot规划[[15](#bib.bib15)]。受这些探索的启发，我们将LLMs引入对T2M模型的对抗攻击领域，成功实现了一个能够制定有效对抗提示的自主攻击代理。
- en: 3 Methodlogy
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: '![Refer to caption](img/ba29ffd3f2c2840f742a56ac31c78af1.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ba29ffd3f2c2840f742a56ac31c78af1.png)'
- en: 'Figure 2: Overview of the proposed ALERT-Motion. ALERT-Motion operates in a
    black-box setting with two key modules: multimodal information integration module
    for consolidating information from text and motion into a unified format, and
    autonomous AD module that learns and executes adversarial prompt search through
    progresses of expansion, refinement, and update.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：所提出的ALERT-Motion概述。ALERT-Motion在一个黑箱环境中运行，具有两个关键模块：用于将文本和动作信息整合为统一格式的多模态信息整合模块，以及通过扩展、细化和更新的过程来学习和执行对抗性提示搜索的自主AD模块。
- en: We leverage an LLM to iteratively refine and enhance adversarial prompts towards
    a target motion. Initially, LLM generates alternative prompts semantically similar
    to the initial prompt to expand the search space. It then queries the victim T2M
    model with these prompts, recording the generated motions. The textual prompts
    and corresponding motions are unified into a suitable input format for LLM using
    MMIC. Exploiting its commonsense reasoning capabilities, LLM autonomously contemplates
    and updates the prompts based on the query results, iteratively steering them
    closer to the target motion. This process continues until the adversarial prompts
    evade detection while generating motions closely matching the target. Fig. [2](#S3.F2
    "Figure 2 ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")
    overviews our ALERT-Motion attack framework.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用LLM迭代地细化和增强对抗性提示，以接近目标动作。最初，LLM生成与初始提示在语义上相似的替代提示，以扩展搜索空间。随后，它使用这些提示查询受害者T2M模型，记录生成的动作。文本提示和对应的动作通过MMIC统一为LLM适用的输入格式。利用其常识推理能力，LLM自主地根据查询结果思考并更新提示，迭代地将其引导至目标动作。这一过程持续进行，直到对抗性提示躲过检测，同时生成与目标高度匹配的动作。图[2](#S3.F2
    "Figure 2 ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")概述了我们的ALERT-Motion攻击框架。
- en: 3.1 Problem Formulation
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: A T2M generative model  is essentially a function that maps the text prompt
    space  to the motion space . Ideally, through training on semantically aligned
    text-motion pairs, a proficient model generates target motion  that is semantically
    consistent with a given target prompt . The objective of an adversarial attack
    is to find an adversarial prompt  such that  closely approximates the target motion
    . Simultaneously, the  is semantically dissimilar from the target prompt  to avoid
    detection. The optimization steps outlined above are formulated as follows
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: T2M生成模型本质上是一个将文本提示空间映射到动作空间的函数。理想情况下，通过对语义一致的文本-动作对进行训练，一个熟练的模型生成的目标动作在语义上与给定的目标提示一致。对抗性攻击的目标是找到一个对抗性提示，使得  接近目标动作。同时，  在语义上与目标提示
    不一致，以避免检测。上述优化步骤被表述如下：
- en: '|  |  |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (1) |'
- en: where  represents the semantic similarity of motion,  represents the semantic
    similarity between text prompts, and  is a promote set.  is the similarity threshold.
    As long as the similarity between the adversarial prompt and the target prompt
    is below , we consider that our attack evades existing detection.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中代表动作的语义相似性，代表文本提示之间的语义相似性， 是一个推广集。 是相似性阈值。只要对抗性提示与目标提示之间的相似性低于，我们认为我们的攻击能够躲过现有的检测。
- en: Challenges
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 挑战
- en: Implementing adversarial prompt generation from T2M models faces some challenges.
    First, T2M models need to go between natural language and physical motion, crossing
    the gap between language and motion domains. Different data types have different
    representation spaces, so integrating multi-modal information is needed. Second,
    the adversarial language prompts need to have high fluency in natural language
    and relevance to the target motion in their query results. But they also need
    to effectively fool the model. The space to search for good prompts is extremely
    large though. Autonomously generating optimal adversarial samples that meet these
    combined quality requirements is a big challenge.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从T2M模型生成对抗性提示面临一些挑战。首先，T2M模型需要在自然语言和物理动作之间转换，跨越语言和动作领域的差距。不同的数据类型具有不同的表示空间，因此需要整合多模态信息。其次，对抗性语言提示需要在自然语言中具有高流畅度，并且在其查询结果中与目标动作相关。但它们还需要有效地欺骗模型。尽管寻找良好提示的空间极为庞大，但自主生成满足这些综合质量要求的最优对抗样本是一个重大挑战。
- en: 3.2 Multimodal Information Contrastive
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多模态信息对比
- en: Unlike most LLM agent-related researches, our task involves motion, which LLM
    cannot directly handle. Therefore, we design MMIC specifically to process information
    from different modalities in the task and organize it into textual information,
    making it convenient for LLM to understand and reasoning. As shown in Fig. [2](#S3.F2
    "Figure 2 ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion"),
    MMIC allows the adversarial prompts, refined through LLM, to query the victim
    T2M model, obtain corresponding motion and calculate the similarity with the target.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数LLM代理相关研究不同，我们的任务涉及动作，而LLM无法直接处理。因此，我们专门设计了MMIC来处理任务中来自不同模态的信息，并将其组织成文本信息，使LLM能够理解和推理。如图 [2](#S3.F2
    "图 2 ‣ 3 方法 ‣ 基于LLM增强的文本到动作对抗攻击")所示，MMIC允许通过LLM细化的对抗提示查询受害T2M模型，获取相应的动作并计算与目标的相似度。
- en: Nevertheless, measuring the similarity directly between two motion poses challenges.
    We consider semantically measuring the similarity of motion. RIATIG [[19](#bib.bib19)]
    employs the pretrained CLIP [[31](#bib.bib31)], a model trained on a large-scale
    dataset of image-text pairs, to obtain semantic features aligned with textual
    descriptions. Similarly, we use the T2M alignment model proposed in [[27](#bib.bib27)]
    to extract semantic motion features and calculate the cosine similarity between
    the semantic features of motion as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接测量两个动作之间的相似性具有挑战性。我们考虑语义上测量动作的相似性。RIATIG [[19](#bib.bib19)]利用预训练的CLIP [[31](#bib.bib31)]，这是一个在大规模图像-文本对数据集上训练的模型，来获取与文本描述对齐的语义特征。同样，我们使用在 [[27](#bib.bib27)]中提出的T2M对齐模型来提取语义动作特征，并计算动作语义特征之间的余弦相似度，如下所示
- en: '|  |  |  | (2) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (2) |'
- en: where  is a motion encoder [[27](#bib.bib27)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 是一个动作编码器 [[27](#bib.bib27)]。
- en: Subsequently, we organize this information into text and incorporate it into
    instructions, enabling LLM to contemplate and reason for better adversarial prompts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，我们将这些信息组织成文本，并将其纳入指令中，使LLM能够思考和推理，以获得更好的对抗提示。
- en: 3.3 Adaptive Dispatching
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 自适应调度
- en: Algorithm 1 ALERT-Motion
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 ALERT-Motion
- en: '0:  Initial prompt , expansion instruction , refinement instruction , update
    instruction , size of updated prompts ,  denotes the similarity of the adversarial
    motion and the target motion, a predefined number of iterations .1:  Expansion:
    2:  for  to  do3:     if  then4:        Refinement: 5:        MMIC: Compute .6:        Obtain
    the similarity set 7:     else8:        Update .9:     end if10:  end for10:  .![Refer
    to caption](img/5c8a51fdf68162b975aa44401839c413.png)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '0:  初始提示，扩展指令，细化指令，更新指令，更新提示的大小，表示对抗动作和目标动作的相似度，预定义的迭代次数。1:  扩展: 2:  对于 到 执行3:     如果 则4:         细化:
    5:         MMIC: 计算。6:         获取相似度集合 7:     否则8:         更新。9:     结束 如果10:  结束 对10:  .![参见说明](img/5c8a51fdf68162b975aa44401839c413.png)'
- en: 'Figure 3: Examples of adversarial attack results against MDM. The first row
    of text provides the true annotations for each column of target motions, and the
    first row of motions corresponds to their respective target motions. The following
    three rows of text correspond to the adversarial prompts obtained by MacPromp,
    RIATIG, and our proposed ALERT-Motion. The motion-rendered images below the text
    depict the motions generated by querying the victim model with the adversarial
    prompts. Darker color indicates later frames in the sequence.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 针对MDM的对抗攻击结果示例。文本的第一行提供了每列目标动作的真实注释，第一行的动作对应其各自的目标动作。接下来的三行文本对应于通过MacPromp、RIATIG和我们提出的ALERT-Motion获得的对抗性提示。文本下方的运动渲染图像描绘了通过对抗提示查询受害模型生成的动作。颜色越深表示序列中的帧越晚。'
- en: '![Refer to caption](img/280a745fd87a00240a5834bcb0e11c17.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/280a745fd87a00240a5834bcb0e11c17.png)'
- en: 'Figure 4: Examples of adversarial attack results against MLD. The first row
    of text provides the true annotations for each column of target motions, and the
    first row of motions corresponds to their respective target motions. The following
    three rows of text correspond to the adversarial prompts obtained by MacPromp,
    RIATIG, and our proposed ALERT-Motion. The motion-rendered images below the text
    depict the motions generated by querying the victim model with the adversarial
    prompts. Darker color indicates later frames in the sequence.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：对MLD的对抗攻击结果示例。第一行文本提供了每列目标动作的真实标注，第一行动作对应于各自的目标动作。接下来的三行文本对应于通过MacPromp、RIATIG和我们提出的ALERT-Motion获得的对抗提示。文本下方的动作渲染图像描绘了通过对受害模型进行对抗提示查询生成的动作。颜色越深表示序列中的后续帧。
- en: 'In our proposed ALERT-Motion framework, the most critical module is the AD
    module. This module constructs an attack agent and plays a pivotal role in determining
    the effectiveness of adversarial prompts. In contrast to previous related researches,
    which predefine various operations to perturb semantics and then use a search
    algorithm to find prompts with higher scores, we directly convey complex task
    requirements using instructions, allowing LLM to automatically learn and execute
    all operations, with each step conducted in textual form. According to the purpose
    of instructions, we divide AD into three progresses: expansion, refinement, and
    update. The workflow of these three progresses and MMIC is outlined in Algorithm
    [1](#alg1 "Algorithm 1 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous
    LLM-Enhanced Adversarial Attack for Text-to-Motion").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们提出的ALERT-Motion框架中，最关键的模块是AD模块。该模块构建一个攻击代理，并在确定对抗提示的有效性方面发挥关键作用。与之前的相关研究不同，这些研究预定义了各种操作以扰动语义，然后使用搜索算法找到得分更高的提示，我们直接通过指令传达复杂的任务需求，允许LLM自动学习和执行所有操作，每一步都以文本形式进行。根据指令的目的，我们将AD分为三个过程：扩展、精炼和更新。这三个过程和MMIC的工作流程在算法[1](#alg1
    "Algorithm 1 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced
    Adversarial Attack for Text-to-Motion")中概述。
- en: Due to the fact that AD responds to the current input in each round, similar
    to an agent in reinforcement learning, we borrow related concepts here to facilitate
    the definition of the processes within AD. We start by defining the state as the
    set of adversarial prompts and their corresponding information for each round,
    while the action is represented by various instructions sent to LLM. LLM is viewed
    as a function involving the next state , current state , and current action ,
    expressed as
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于AD在每一轮对当前输入作出响应，类似于强化学习中的代理，我们在这里借用相关概念来方便AD过程的定义。我们首先将状态定义为每轮对抗提示及其对应信息的集合，而动作由发送到LLM的各种指令表示。LLM被视为一个涉及下一个状态、当前状态和当前动作的函数，表示为
- en: '|  |  |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (3) |'
- en: where  is the state transition function and  represents the instruction text
    corresponding to . It is important to note that the representation of state  differs
    between odd and even time steps, it is defined as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是状态转移函数，表示与的指令文本。值得注意的是，状态的表示在奇数和偶数时间步之间有所不同，定义为
- en: '|  | $$S_{k}=\begin{cases}\{p_{0}\}&amp;{\rm if}\,k=0\\ \{p_{1},\ldots,p_{n}\}&amp;{\rm
    if}\,k\,(\text{mod}\,2)=0\\'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$S_{k}=\begin{cases}\{p_{0}\}&amp;{\rm if}\,k=0\\ \{p_{1},\ldots,p_{n}\}&amp;{\rm
    if}\,k\,(\text{mod}\,2)=0\\'
- en: \{p^{\prime}_{1},\ldots,p^{\prime}_{n}\}&amp;{\rm if}\,k\,(\text{mod}\,2)=1\end{cases}$$
    |  | (4) |
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \{p^{\prime}_{1},\ldots,p^{\prime}_{n}\}&amp;{\rm if}\,k\,(\text{mod}\,2)=1\end{cases}$$
    |  | (4) |
- en: where  is initial adversarial prompt,  is the set of refined prompts, and  are
    the expanded or updated prompts of .
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中是初始对抗提示，是精炼提示的集合，和是扩展或更新的提示。
- en: Expansion
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展
- en: Initially, we begin with a single available adversarial prompt . The current
    state is defined as . Without expansion, proceeding directly to the subsequent
    steps may lead the search into a local optimum. So we employ the expansion instruction
    text  to obtain expanded results through LLM. The next state is represented as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们从一个可用的对抗提示开始。当前状态定义为。没有扩展，直接进入后续步骤可能会使搜索陷入局部最优。因此，我们使用扩展指令文本来通过LLM获取扩展结果。下一个状态表示为
- en: '|  |  |  | (5) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (5) |'
- en: where  represents the expansion instruction and  is the set of expanded prompts.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中代表扩展指令， 是扩展提示的集合。
- en: Refinement
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精炼
- en: We find that when the instructions given to LLM are too long, its responses
    may sometimes fail to meet the attack requirements. New instructions are needed
    to emphasize the attack requirements in our task. Therefore, in this progress,
    we refine the adversarial prompts to ensure that they consistently meet the attack
    requirements, including being naturally fluent and relevant to motion. After refinement,
    the state of the agent is defined as
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，当给LLM的指令过长时，其响应有时可能无法满足攻击要求。需要新的指令来强调我们任务中的攻击要求。因此，在这一过程中，我们优化了对抗性提示，以确保它们始终符合攻击要求，包括自然流畅和与动作相关。经过优化后，代理的状态定义为
- en: '|  |  |  | (6) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (6) |'
- en: where  and  represents refinement instruction.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  和  代表优化指令。
- en: Update
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新
- en: Unlike existing methods that relied on numerical scalar guidance to generate
    adversarial prompts, such as RIATIG, our AD utilizes the text information organized
    by MMIC to guide LLM in autonomously contemplating and generating adversarial
    prompts. This allows us to finely control the generated adversarial prompts more
    effectively in line with the attack requirements using richer information. Moreover,
    this control is automated, eliminating the need for continuously defining new
    operations, such as word or character insertion, deletion, replacement, and so
    forth, as in previous methods. In the update progress, as LLM contemplates and
    generates adversarial prompts, the information organized by MMIC is also fed into
    LLM to assist in its decision-making process. After update, the state of the agent
    is defined as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖于数值标量指导生成对抗性提示的现有方法（如RIATIG）不同，我们的AD利用MMIC组织的文本信息来引导LLM自主思考和生成对抗性提示。这使我们能够使用更丰富的信息更有效地细化生成的对抗性提示，以符合攻击要求。此外，这种控制是自动化的，消除了如插入、删除、替换词语或字符等连续定义新操作的需要。在更新过程中，当LLM思考和生成对抗性提示时，MMIC组织的信息也会输入到LLM中，以辅助其决策过程。更新后，代理的状态定义为
- en: '|  |  |  | (7) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (7) |'
- en: where  is the update instruction and the function  signifies string concatenation,
    and  is obtained by Eq. ([2](#S3.E2 "Equation 2 ‣ 3.2 Multimodal Information Contrastive
    ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是更新指令，函数  表示字符串连接， 由方程式 ([2](#S3.E2 "Equation 2 ‣ 3.2 Multimodal Information
    Contrastive ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion"))获得。
- en: After  rounds of iteration, we choose the highest-scoring prompt among all candidates
    as the optimal adversarial prompt  for the attack. The definition of  is obtained
    by Eq. ([1](#S3.E1 "Equation 1 ‣ 3.1 Problem Formulation ‣ 3 Methodlogy ‣ Autonomous
    LLM-Enhanced Adversarial Attack for Text-to-Motion")). Here  and . In order to
    ensure that the adversarial prompt meets the constraints, we calculate the similarity
    between the adversarial prompts and target prompt as
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过若干轮迭代后，我们从所有候选提示中选择得分最高的提示作为攻击的最佳对抗性提示。  的定义由方程式 ([1](#S3.E1 "Equation 1
    ‣ 3.1 Problem Formulation ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial
    Attack for Text-to-Motion"))获得。这里  和 。为了确保对抗性提示符合约束，我们计算对抗性提示与目标提示之间的相似度为
- en: '|  |  |  | (8) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | (8) |'
- en: where  is a text encoder [[6](#bib.bib6)] to extract features.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中  是一个文本编码器 [[6](#bib.bib6)] 用于提取特征。
- en: 4 Experiment
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Settings
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Datasets.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集。
- en: We select target prompt texts and target motion from the HumanML3D (H3D) [[11](#bib.bib11)].
    It includes 14,616 motion sequences from AMASS [[24](#bib.bib24)], each with a
    textual description (totaling 44,970 descriptions). It also re-annotates AMASS
    and HumanAct12 [[10](#bib.bib10)] motion capture sequences. The dataset provides
    a redundant data representation involving root velocity, joint positions, joint
    velocities, joint rotations, and foot contact labels. It is used for both AMASS
    and HumanAct12 motion.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从HumanML3D (H3D) [[11](#bib.bib11)]中选择目标提示文本和目标动作。它包含来自AMASS [[24](#bib.bib24)]的14,616个动作序列，每个序列都有文本描述（总共44,970个描述）。它还重新标注了AMASS和HumanAct12 [[10](#bib.bib10)]动作捕捉序列。数据集提供了冗余的数据表示，包括根速度、关节位置、关节速度、关节旋转和脚接触标签。它用于AMASS和HumanAct12的动作。
- en: Victim Models.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 受害者模型。
- en: 'To assess the effectiveness of ALERT-Motion, we select two prominent publicly
    available T2M models: MLD and MDM. We employe their respective pretrained models
    from the official GitHub repositories, which were trained on H3D.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估ALERT-Motion的效果，我们选择了两个著名的公开T2M模型：MLD和MDM。我们使用了它们各自的预训练模型，这些模型从官方GitHub仓库中获取，训练数据为H3D。
- en: 'Table 1: The results of the adversarial attacks against MDM and MLD on T2M
    evaluation model. The first row, labeled “Target Motion”, represents the motion
    generated by the corresponding victim models, which are the targets of our attack.
    The quality of these indicators depends solely on the capabilities of the generation
    models and evaluation models. The second and third rows correspond to the baseline
    models MacPromp and RIATIG that we select. The final row represents the performance
    of our proposed method, ALERT-Motion.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：针对 T2M 评估模型的 MDM 和 MLD 的对抗攻击结果。第一行标记为“目标运动”，表示由相应的受害模型生成的运动，这些模型是我们攻击的目标。这些指标的质量完全取决于生成模型和评估模型的能力。第二行和第三行对应于我们选择的基线模型
    MacPromp 和 RIATIG。最后一行表示我们提出的方法 ALERT-Motion 的性能。
- en: '| Attack Methods | R-1¹  | R-2 | R-3 | R-5 | R-10 | FID |  | PPL²  | AS³ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方法 | R-1¹  | R-2 | R-3 | R-5 | R-10 | FID |  | PPL²  | AS³ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MLD |  |  |  |  |  |  |  |  |  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| MLD |  |  |  |  |  |  |  |  |  |'
- en: '|          Target Motion | 8 / 20 | 10 / 20 | 13 / 20 | 15 / 20 | 16 / 20 |
    4.015 | 4.029 | 391.055 | - |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|          目标运动 | 8 / 20 | 10 / 20 | 13 / 20 | 15 / 20 | 16 / 20 | 4.015 |
    4.029 | 391.055 | - |'
- en: '|          MacPromp | 5 / 20 | 8 / 20 | 10 / 20 | 13 / 20 | 15 / 20 | 13.935
    | 6.534 | 3061.488 | 0.471 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|          MacPromp | 5 / 20 | 8 / 20 | 10 / 20 | 13 / 20 | 15 / 20 | 13.935
    | 6.534 | 3061.488 | 0.471 |'
- en: '|          RIATIG | 4 / 20 | 7 / 20 | 11 / 20 | 13 / 20 | 17 / 20 | 10.899
    | 5.368 | 1102.100 | 0.131 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|          RIATIG | 4 / 20 | 7 / 20 | 11 / 20 | 13 / 20 | 17 / 20 | 10.899
    | 5.368 | 1102.100 | 0.131 |'
- en: '|          ALERT-Motion | 6 / 20 | 9 / 20 | 9 / 20 | 15 / 20 | 19 / 20 | 8.881
    | 5.016 | 113.223 | 0.067 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|          ALERT-Motion | 6 / 20 | 9 / 20 | 9 / 20 | 15 / 20 | 19 / 20 | 8.881
    | 5.016 | 113.223 | 0.067 |'
- en: '| MDM (100 steps) |  |  |  |  |  |  |  |  |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| MDM (100 步) |  |  |  |  |  |  |  |  |  |'
- en: '|          Target Motion | 7 / 20 | 14 / 20 | 15 / 20 | 16 / 20 | 20 / 20 |
    4.055 | 3.549 | 391.055 | - |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|          目标运动 | 7 / 20 | 14 / 20 | 15 / 20 | 16 / 20 | 20 / 20 | 4.055 |
    3.549 | 391.055 | - |'
- en: '|          MacPromp | 7 / 20 | 7 / 20 | 8 / 20 | 16 / 20 | 16 / 20 | 11.108
    | 5.106 | 2698.972 | 0.484 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|          MacPromp | 7 / 20 | 7 / 20 | 8 / 20 | 16 / 20 | 16 / 20 | 11.108
    | 5.106 | 2698.972 | 0.484 |'
- en: '|          RIATIG | 5 / 20 | 8 / 20 | 10 / 20 | 16 / 20 | 18 / 20 | 12.435
    | 5.024 | 1154.017 | 0.129 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|          RIATIG | 5 / 20 | 8 / 20 | 10 / 20 | 16 / 20 | 18 / 20 | 12.435
    | 5.024 | 1154.017 | 0.129 |'
- en: '|          ALERT-Motion | 7 / 20 | 13 / 20 | 14 / 20 | 17 / 20 | 19 / 20 |
    5.843 | 4.117 | 179.496 | 0.075 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|          ALERT-Motion | 7 / 20 | 13 / 20 | 14 / 20 | 17 / 20 | 19 / 20 |
    5.843 | 4.117 | 179.496 | 0.075 |'
- en: '| MDM (1000 steps) |  |  |  |  |  |  |  |  |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MDM (1000 步) |  |  |  |  |  |  |  |  |  |'
- en: '|          Target Motion | 8 / 20 | 12 / 20 | 12 / 20 | 14 / 20 | 19 / 20 |
    5.954 | 4.116 | 391.055 | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|          目标运动 | 8 / 20 | 12 / 20 | 12 / 20 | 14 / 20 | 19 / 20 | 5.954 |
    4.116 | 391.055 | - |'
- en: '|          MacPromp | 3 / 20 | 6 / 20 | 8 / 20 | 10 / 20 | 14 / 20 | 11.149
    | 6.156 | 3023.887 | 0.467 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|          MacPromp | 3 / 20 | 6 / 20 | 8 / 20 | 10 / 20 | 14 / 20 | 11.149
    | 6.156 | 3023.887 | 0.467 |'
- en: '|          RIATIG | 4 / 20 | 7 / 20 | 10 / 20 | 14 / 20 | 16 / 20 | 9.875 |
    5.444 | 1262.338 | 0.129 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|          RIATIG | 4 / 20 | 7 / 20 | 10 / 20 | 14 / 20 | 16 / 20 | 9.875 |
    5.444 | 1262.338 | 0.129 |'
- en: '|          ALERT-Motion | 9 / 20 | 12 / 20 | 13 / 20 | 14 / 20 | 19 / 20 |
    6.183 | 4.533 | 140.793 | 0.074 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|          ALERT-Motion | 9 / 20 | 12 / 20 | 13 / 20 | 14 / 20 | 19 / 20 |
    6.183 | 4.533 | 140.793 | 0.074 |'
- en: '1'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: R-1, R-2, R-3, R-5, R-10 represent R-precision at R equals 1, 2, 3, 5, and 10,
    respectively.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R-1、R-2、R-3、R-5、R-10 表示 R 等于 1、2、3、5 和 10 时的 R-精度。
- en: '2'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: PPL represents the perplexity of sentences.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: PPL 代表句子的困惑度。
- en: '3'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: AS stands for Adversarial Similarity, which denotes the similarity between adversarial
    prompts and target prompts.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: AS 代表对抗相似度，表示对抗性提示和目标提示之间的相似度。
- en: 'Table 2: Attack performance on TMR evaluation model.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：TMR 评估模型上的攻击性能。
- en: '| Attack Methods | R-1 | R-2 | R-3 | R-5 | R-10 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方法 | R-1 | R-2 | R-3 | R-5 | R-10 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MLD |  |  |  |  |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MLD |  |  |  |  |  |'
- en: '|         MacPromp | 5 / 20 | 6 / 20 | 7 / 20 | 9 / 20 | 13 / 20 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|         MacPromp | 5 / 20 | 6 / 20 | 7 / 20 | 9 / 20 | 13 / 20 |'
- en: '|         RIATIG | 6 / 20 | 7 / 20 | 8 / 20 | 11 / 20 | 16 / 20 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|         RIATIG | 6 / 20 | 7 / 20 | 8 / 20 | 11 / 20 | 16 / 20 |'
- en: '|         ALERT-Motion | 8 / 20 | 9 / 20 | 10 / 20 | 12 / 20 | 12 / 20 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|         ALERT-Motion | 8 / 20 | 9 / 20 | 10 / 20 | 12 / 20 | 12 / 20 |'
- en: '| MDM (100 steps) |  |  |  |  |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MDM (100 步) |  |  |  |  |  |'
- en: '|         MacPromp | 4 / 20 | 6 / 20 | 9 / 20 | 11 / 20 | 16 / 20 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|         MacPromp | 4 / 20 | 6 / 20 | 9 / 20 | 11 / 20 | 16 / 20 |'
- en: '|         RIATIG | 5 / 20 | 6 / 20 | 7 / 20 | 11 / 20 | 14 / 20 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|         RIATIG | 5 / 20 | 6 / 20 | 7 / 20 | 11 / 20 | 14 / 20 |'
- en: '|         ALERT-Motion | 7 / 20 | 12 / 20 | 15 / 20 | 16 / 20 | 17 / 20 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|         ALERT-Motion | 7 / 20 | 12 / 20 | 15 / 20 | 16 / 20 | 17 / 20 |'
- en: '| MDM (1000 steps) |  |  |  |  |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MDM (1000 步) |  |  |  |  |  |'
- en: '|         MacPromp | 3 / 20 | 6 / 20 | 9 / 20 | 10 / 20 | 15 / 20 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|         MacPromp | 3 / 20 | 6 / 20 | 9 / 20 | 10 / 20 | 15 / 20 |'
- en: '|         RIATIG | 3 / 20 | 7 / 20 | 11 / 20 | 12 / 20 | 16 / 20 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|         RIATIG | 3 / 20 | 7 / 20 | 11 / 20 | 12 / 20 | 16 / 20 |'
- en: '|         ALERT-Motion | 6 / 20 | 11 / 20 | 12 / 20 | 14 / 20 | 18 / 20 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|         ALERT-Motion | 6 / 20 | 11 / 20 | 12 / 20 | 14 / 20 | 18 / 20 |'
- en: Evaluation Setup.
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估设置。
- en: In our experiments, all attacks are conducted in a black-box setting, meaning
    that we generate motion only by querying the model with prompts and obtaining
    the generated results. We utilize the “gpt-3.5-turbo-instruct” API with ChatGPT
    to implement our approach. The initial adversarial prompt text is a motion description
    randomly generated by ChatGPT. We set the number of iterations as , the size of
    the prompt set as . We set the similarity threshold  as . Examples for the attack
    were taken from the top  of the Dissimilar subset in the evaluation setup of [[27](#bib.bib27)],
    where the model achieves the highest accuracy. During the attack process, we use
    the model from [[27](#bib.bib27)] to extract the motion features to compute cosine
    similarity and adopt the text feature extraction from [[37](#bib.bib37)]. The
    effectiveness is evaluated using T2M [[11](#bib.bib11)].
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，所有攻击都在黑箱设置中进行，这意味着我们仅通过向模型发送提示并获取生成的结果来生成动作。我们使用“gpt-3.5-turbo-instruct”
    API 和 ChatGPT 实现我们的方法。初始对抗提示文本是由 ChatGPT 随机生成的动作描述。我们将迭代次数设置为，提示的大小设置为。我们将相似性阈值设置为。攻击示例取自评估设置中
    Dissimilar 子集的顶部 [[27](#bib.bib27)]，在该设置下模型获得了最高的准确率。在攻击过程中，我们使用来自 [[27](#bib.bib27)]
    的模型提取动作特征以计算余弦相似度，并采用 [[37](#bib.bib37)] 的文本特征提取。效果通过 T2M [[11](#bib.bib11)] 进行评估。
- en: 'Table 3: The mean and variance of evaluation metrics under different selections
    of target motion.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 不同目标动作选择下评估指标的均值和方差。'
- en: '| Metrics | Target Motion | MacPromp | RIATIG | ALERT-Motion |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Metrics | Target Motion | MacPromp | RIATIG | ALERT-Motion |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| R-1 | (10.80 1.94) / 20 | (4.00 2.00) / 20 | (4.40 1.02) / 20 | (5.40 1.36)
    / 20 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| R-1 | (10.80 1.94) / 20 | (4.00 2.00) / 20 | (4.40 1.02) / 20 | (5.40 1.36)
    / 20 |'
- en: '| R-2 | (13.20 2.32) / 20 | (6.40 3.38) / 20 | (6.40 0.80) / 20 | (8.20 1.47)
    / 20 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| R-2 | (13.20 2.32) / 20 | (6.40 3.38) / 20 | (6.40 0.80) / 20 | (8.20 1.47)
    / 20 |'
- en: '| R-3 | (15.20 2.32) / 20 | (8.80 2.99) / 20 | (8.40 1.50) / 20 | (10.00 0.89)
    / 20 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| R-3 | (15.20 2.32) / 20 | (8.80 2.99) / 20 | (8.40 1.50) / 20 | (10.00 0.89)
    / 20 |'
- en: '| R-5 | (17.00 2.00) / 20 | (13.00 1.90) / 20 | (13.20 0.40) / 20 | (13.40
    2.42) / 20 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| R-5 | (17.00 2.00) / 20 | (13.00 1.90) / 20 | (13.20 0.40) / 20 | (13.40
    2.42) / 20 |'
- en: '| R-10 | (19.00 0.63) / 20 | (17.20 1.17) / 20 | (17.00 1.62) / 20 | (17.60
    1.02) / 20 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| R-10 | (19.00 0.63) / 20 | (17.20 1.17) / 20 | (17.00 1.62) / 20 | (17.60
    1.02) / 20 |'
- en: '| FID | 2.990.89 | 15.404.12 | 12.821.72 | 8.202.48 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| FID | 2.990.89 | 15.404.12 | 12.821.72 | 8.202.48 |'
- en: '|  | 3.460.54 | 5.990.47 | 6.100.59 | 5.680.76 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.460.54 | 5.990.47 | 6.100.59 | 5.680.76 |'
- en: '| PPL | 327.83128.34 | 2571.15397.28 | 1389.67373.37 | 119.5810.54 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| PPL | 327.83128.34 | 2571.15397.28 | 1389.67373.37 | 119.5810.54 |'
- en: '| AS | - | 0.490.02 | 0.120.01 | 0.080.01 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| AS | - | 0.490.02 | 0.120.01 | 0.080.01 |'
- en: Baselines.
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准线。
- en: To the best of our knowledge, there is currently no targeted adversarial attack
    specifically designed for T2M generation. For comparison, we select two state-of-the-art
    targeted adversarial attack methods for text-to-image generation, MacPromp [[25](#bib.bib25)]
    and RIATIG [[19](#bib.bib19)], as baseline methods. Since their tasks do not involve
    motion, we modify their task settings to match our task.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，目前没有专门为 T2M 生成设计的针对性对抗攻击。为了进行比较，我们选择了两种最先进的针对性对抗攻击方法用于文本到图像生成，MacPromp
    [[25](#bib.bib25)] 和 RIATIG [[19](#bib.bib19)]，作为基准方法。由于它们的任务不涉及动作，我们修改了它们的任务设置以匹配我们的任务。
- en: 4.2 Evaluation Metrics
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估指标
- en: Motion Performance.
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 动作表现。
- en: 'We utilize the R Precision, a widely-used metric in T2M [[12](#bib.bib12),
    [40](#bib.bib40), [35](#bib.bib35), [7](#bib.bib7)] to evaluate generated motion.
    This assessment involves comparing each motion not only with its ground-truth
    text but also with a misaligned description. R Precision is determined through
    the Euclidean distance between motion and text features. Our evaluation centers
    on measuring the average accuracy among the top- ranked descriptions. A ground-truth
    within the top- candidates is considered a “True Positive” retrieval. Our approach
    involves a batch size of 20, encompassing 19 negative examples, and we explore
    the effectiveness of R at various values: 1 (R-1), 2 (R-2), 3 (R-3), 5 (R-5),
    and 10 (R-10). Furthermore, our study incorporates Frechet Inception Distance
    (FID) as a metric to assess the quality of generated motion. FID, a widely accepted
    standard for evaluating content quality [[35](#bib.bib35), [7](#bib.bib7)], involves
    comparing features extracted from generated motion and real motion. In our motion
    domain adaptation, we adopt an evaluator network to represent deep features, deviating
    from the original image-based Inception neural network. Smaller FID values are
    indicative of superior results. Additionally, we compute the Multimodal Distance
    (), which is the mean Euclidean distance between the motions features and their
    corresponding textual descriptions features in the test examples [[35](#bib.bib35),
    [7](#bib.bib7)]. A lower value indicates better alignment between prompts and
    their generated motions.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用R精度，这是一种在T2M [[12](#bib.bib12), [40](#bib.bib40), [35](#bib.bib35), [7](#bib.bib7)]中广泛使用的指标来评估生成的动作。此评估涉及将每个动作与其真实文本以及一个不对齐的描述进行比较。R精度通过动作和文本特征之间的欧氏距离来确定。我们的评估重点在于测量排名前几的描述的平均准确度。真实描述在前几个候选中被认为是“真正的正例”检索。我们的方法使用批量大小为20，包括19个负例，并探索了不同值的R效果：1（R-1），2（R-2），3（R-3），5（R-5）和10（R-10）。此外，我们的研究还包括Frechet
    Inception Distance (FID)作为评估生成动作质量的指标。FID，作为评估内容质量的广泛接受标准 [[35](#bib.bib35), [7](#bib.bib7)]，涉及比较从生成动作和真实动作中提取的特征。在我们的动作领域适应中，我们采用了一个评估网络来表示深层特征，与原始基于图像的Inception神经网络不同。较小的FID值表明结果更优。此外，我们计算了多模态距离（），这是测试样本中动作特征与其相应文本描述特征之间的平均欧氏距离 [[35](#bib.bib35),
    [7](#bib.bib7)]。较低的值表示提示和生成动作之间的对齐程度更好。
- en: Adversarial Similarity.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对抗相似度。
- en: 'Table 4: The attack performance of 100 additional experiments.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：100次额外实验的攻击性能。
- en: '| Attack Methods | R-1 | R-2 | R-3 | R-5 | R-10 | FID |  | PPL | AS |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 攻击方法 | R-1 | R-2 | R-3 | R-5 | R-10 | FID |  | PPL | AS |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Target Motion | 54 / 100 | 66 / 100 | 76 / 100 | 85 / 100 | 95 / 100 | 0.92
    | 3.46 | 327.83 | - |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 目标动作 | 54 / 100 | 66 / 100 | 76 / 100 | 85 / 100 | 95 / 100 | 0.92 | 3.46
    | 327.83 | - |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| MacPromp | 20 / 100 | 32 / 100 | 44 / 100 | 65 / 100 | 86 / 100 | 8.37 |
    5.99 | 2571.15 | 0.49 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| MacPromp | 20 / 100 | 32 / 100 | 44 / 100 | 65 / 100 | 86 / 100 | 8.37 |
    5.99 | 2571.15 | 0.49 |'
- en: '| RIATIG | 22 / 100 | 32 / 100 | 42 / 100 | 66 / 100 | 85 / 100 | 7.41 | 6.10
    | 1389.67 | 0.12 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| RIATIG | 22 / 100 | 32 / 100 | 42 / 100 | 66 / 100 | 85 / 100 | 7.41 | 6.10
    | 1389.67 | 0.12 |'
- en: '| ALERT-Motion | 27 / 100 | 41 / 100 | 50 / 100 | 67 / 100 | 88 / 100 | 4.17
    | 5.68 | 119.58 | 0.08 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ALERT-动作 | 27 / 100 | 41 / 100 | 50 / 100 | 67 / 100 | 88 / 100 | 4.17 |
    5.68 | 119.58 | 0.08 |'
- en: In adversarial attacks, it is essential for adversarial prompt text to have
    low similarity with the target prompt text to evade detection. In line with previous
    studies, our initial step involves utilizing the Universal Sentence Encoder [[6](#bib.bib6)]
    for encoding both the adversarial sentence and the target sentence, resulting
    in high-dimensional vectors. Subsequently, we determine their adversarial similarity
    by computing the cosine score.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗攻击中，对抗提示文本与目标提示文本之间具有低相似度是规避检测的关键。根据以往研究，我们的初步步骤包括使用Universal Sentence Encoder [[6](#bib.bib6)]
    对对抗句子和目标句子进行编码，得到高维向量。随后，我们通过计算余弦分数来确定它们的对抗相似度。
- en: Naturality.
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自然度。
- en: To ensure the naturalness of adversarial examples, we measure perplexity (PPL)
    using GPT-2 [[30](#bib.bib30)], trained on real-world sentences. PPL assesses
    the likelihood of the model in generating the input text, thereby indicating natural
    fluency of the adversarial prompts. Lower PPL values typically signify higher
    naturalness.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保对抗性示例的自然性，我们使用GPT-2 [[30](#bib.bib30)] 测量困惑度（PPL），该模型是基于真实世界句子训练的。PPL评估模型生成输入文本的可能性，从而指示对抗性提示的自然流畅性。较低的PPL值通常表示更高的自然性。
- en: 4.3 Evaluation Results
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估结果
- en: The attack results on MLD and MDM are shown in Table [1](#S4.T1 "Table 1 ‣ Victim
    Models. ‣ 4.1 Experimental Settings ‣ 4 Experiment ‣ Autonomous LLM-Enhanced Adversarial
    Attack for Text-to-Motion"). Compared to the baselines, ALERT-Motion achieves
    a higher R-precision. Although MacPromp achieves higher R-precision and lower
    FID and  in some cases, its direct translation of target prompts in various languages
    results in unnatural adversarial prompts. The perplexity is much higher than other
    methods, and, on the other hand, it closely resembles the target sentences, resulting
    in high adversarial similarity, making it less practical. RIATIG, compared to
    MacPromp, achieves similar or even higher R-precision, with a slight decrease
    in perplexity and adversarial similarity. However, as seen in Fig. [4](#S3.F4
    "Figure 4 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced
    Adversarial Attack for Text-to-Motion"), there are still incorrect words and some
    extra spaces.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](#S4.T1 "Table 1 ‣ Victim Models. ‣ 4.1 Experimental Settings ‣ 4 Experiment
    ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")中显示了对MLD和MDM的攻击结果。与基线方法相比，ALERT-Motion实现了更高的R-precision。尽管在某些情况下，MacPromp的R-precision更高，FID更低，但其对目标提示的直接翻译在各种语言中导致了不自然的对抗性提示。其困惑度远高于其他方法，但另一方面，它与目标句子非常相似，导致高对抗性相似性，使其不够实用。相比于MacPromp，RIATIG实现了相似或更高的R-precision，且困惑度和对抗性相似性略有下降。然而，如图[4](#S3.F4
    "Figure 4 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced
    Adversarial Attack for Text-to-Motion")所示，仍然存在错误单词和一些额外的空格。
- en: From Table [1](#S4.T1 "Table 1 ‣ Victim Models. ‣ 4.1 Experimental Settings
    ‣ 4 Experiment ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion"),
    it can be observed that our proposed ALERT-Motion performs better on most metrics
    across these models. Additionally, examining Figs. [3](#S3.F3 "Figure 3 ‣ 3.3
    Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack
    for Text-to-Motion") and [4](#S3.F4 "Figure 4 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy
    ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion"), the adversarial
    prompts generated by ALERT-Motion are not only more natural but also relevant
    to the motion. In contrast, prompts obtained by other methods are mostly irrelevant
    to motion.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从表[1](#S4.T1 "Table 1 ‣ Victim Models. ‣ 4.1 Experimental Settings ‣ 4 Experiment
    ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")可以观察到，我们提出的ALERT-Motion在这些模型的大多数指标上表现更好。此外，检查图[3](#S3.F3
    "Figure 3 ‣ 3.3 Adaptive Dispatching ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced
    Adversarial Attack for Text-to-Motion")和[4](#S3.F4 "Figure 4 ‣ 3.3 Adaptive Dispatching
    ‣ 3 Methodlogy ‣ Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion")，ALERT-Motion生成的对抗性提示不仅更自然，而且与运动相关。相比之下，其他方法获得的提示大多与运动无关。
- en: 4.4 Ablation Study
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: Influence of Evaluation Models.
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估模型的影响。
- en: The current research on the evaluation of motion generation is still limited,
    with T2M [[11](#bib.bib11)] being widely recognized. Studies on motion generation,
    such as [[40](#bib.bib40), [35](#bib.bib35)], and [[7](#bib.bib7)], adopt T2M
    to assess the quality of generated models. The latest research on the evaluation
    of motion generation is presented in TMR  [[27](#bib.bib27)]. Therefore, we also
    use it to evaluate our experiments. As shown in Table [2](#S4.T2 "Table 2 ‣ Victim
    Models. ‣ 4.1 Experimental Settings ‣ 4 Experiment ‣ Autonomous LLM-Enhanced Adversarial
    Attack for Text-to-Motion"), under TMR model, ALERT-Motion exhibits significant
    superiority compared to other baseline methods, indicating that the excellent
    performance of our proposed ALERT-Motion is not influenced by the choice of evaluation
    models.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当前对运动生成评估的研究仍然有限，T2M [[11](#bib.bib11)] 被广泛认可。运动生成的研究，如[[40](#bib.bib40), [35](#bib.bib35)]
    和[[7](#bib.bib7)]，采用T2M评估生成模型的质量。最新的运动生成评估研究呈现在TMR [[27](#bib.bib27)]中。因此，我们也使用它来评估我们的实验。如表[2](#S4.T2
    "Table 2 ‣ Victim Models. ‣ 4.1 Experimental Settings ‣ 4 Experiment ‣ Autonomous
    LLM-Enhanced Adversarial Attack for Text-to-Motion")所示，在TMR模型下，ALERT-Motion相较于其他基线方法表现出显著优势，表明我们提出的ALERT-Motion的卓越性能不受评估模型选择的影响。
- en: Influence of Target Motion.
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 目标运动的影响。
- en: To further analyze the selected target motion on the attack performance, we
    chose all 100 motion from the Dissimilar subset evaluation setting in TMR [[27](#bib.bib27)]
    as target motion. We conduct five experiments, each using a different set of 20
    target motion, following the order specified in their setting. Table [3](#S4.T3
    "Table 3 ‣ Evaluation Setup. ‣ 4.1 Experimental Settings ‣ 4 Experiment ‣ Autonomous
    LLM-Enhanced Adversarial Attack for Text-to-Motion") demonstrate that the superiority
    of ALERT-Motion performance over baseline methods remains consistent across different
    target motion. The overall performance of the attack method on these 100 target
    motion is presented in Table [4](#S4.T4 "Table 4 ‣ Adversarial Similarity. ‣ 4.2
    Evaluation Metrics ‣ 4 Experiment ‣ Autonomous LLM-Enhanced Adversarial Attack
    for Text-to-Motion"), demonstrating the consistent performance of our attack method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步分析所选目标动作对攻击性能的影响，我们选择了TMR[[27](#bib.bib27)]中Dissimilar子集评估设置的所有100个动作作为目标动作。我们进行了五次实验，每次使用不同的20个目标动作，按照设置中指定的顺序进行。表[3](#S4.T3
    "表 3 ‣ 评估设置。 ‣ 4.1 实验设置 ‣ 4 实验 ‣ 基于LLM增强的自适应对抗攻击用于文本到动作")展示了ALERT-Motion在不同目标动作上的性能优越性始终如一。表[4](#S4.T4
    "表 4 ‣ 对抗相似性。 ‣ 4.2 评估指标 ‣ 4 实验 ‣ 基于LLM增强的自适应对抗攻击用于文本到动作")展示了我们攻击方法在这些100个目标动作上的整体性能，证明了我们攻击方法的一致性能。
- en: 5 Discussion
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Safety Concerns.
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 安全问题。
- en: To prevent potential malicious uses, it is crucial to consider defense mechanisms
    for T2M models. There are valid concerns around malicious actors exploiting such
    models to produce explicit violent content [[5](#bib.bib5), [39](#bib.bib39)].
    Moreover, considering that these models serve as humanoid controllers [[23](#bib.bib23)]
    and may potentially be utilized for humanoid robots in the future, there are risks
    of the robots behaving in ways that endanger humans if not properly constrained.
    Although there is currently no research specifically addressing defense mechanisms
    for T2M generation models, we demonstrates that existing content moderation filters,
    if directly deployed on T2M models, are bypassed by our adversarial attack method.
    Moreover, the fact that our method creates adversarial prompts related to T2M
    task makes the attacks more challenging to defend against. Therefore, the safety
    risks of using motion generation models must be taken into consideration.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止潜在的恶意使用，考虑T2M模型的防御机制至关重要。对于恶意行为者利用此类模型生成明确的暴力内容[[5](#bib.bib5), [39](#bib.bib39)]，存在合理的担忧。此外，考虑到这些模型作为类人控制器[[23](#bib.bib23)]并且未来可能被用于类人机器人，如果没有适当的约束，机器人可能会以危害人类的方式行为。尽管目前没有专门针对T2M生成模型的防御机制的研究，但我们展示了现有的内容审查过滤器如果直接应用于T2M模型，会被我们的对抗攻击方法绕过。此外，我们的方法创建的对抗性提示与T2M任务相关，使得攻击更具挑战性。因此，必须考虑使用动作生成模型的安全风险。
- en: Potential Defense Strategies.
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 潜在的防御策略。
- en: Several defense methods may be considered. Rule-based text filters might find
    our approach challenging to counter since the adversarial prompts we create seamlessly
    blend into normal text, remaining semantically related to motion. One potential
    defense mechanism involves leveraging larger datasets for training. The most extensive
    dataset in current motion generation research [[11](#bib.bib11)] comprises just
    over 10,000 text-motion pairs. We hypothesize that increasing the volume of training
    data boosts the alignment between the generated model and T2M tasks. Furthermore,
    established defense methods from the realms of adversarial attacks and NLP [[9](#bib.bib9),
    [16](#bib.bib16)] is applied to strengthen T2M models, specifically through adversarial
    training.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会考虑几种防御方法。基于规则的文本过滤器可能会发现我们的方法难以对抗，因为我们创建的对抗性提示无缝融入正常文本中，并且在语义上与动作相关。一种潜在的防御机制是利用更大的数据集进行训练。在当前的动作生成研究中，最广泛的数据集[[11](#bib.bib11)]仅包含略多于10,000对文本-动作对。我们假设增加训练数据的量可以提升生成模型与T2M任务之间的对齐。此外，来自对抗攻击和自然语言处理领域的既定防御方法[[9](#bib.bib9),
    [16](#bib.bib16)]被应用于增强T2M模型，特别是通过对抗性训练。
- en: 6 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this paper, a novel method that involves conducting targeted adversarial
    attack against T2M models is proposed. Additionally, we introduce an autonomous
    LLM-enhanced adversarial attack method called ALERT-Motion, which comprises two
    modules: the multimodal information integration (MMIC) module and the adaptive
    dispatching (AD) module. Assisted by MMIC, AD, with the incorporation of LLM during
    progresses of expansion, refinement, and updating, autonomously learns and executes
    the search for optimal adversarial prompts. Our extensive experiments validate
    the ability to discover adversarial prompts that exhibit both fluency and related
    to motion. Moreover, these prompts trigger the victim T2M model to generate motion
    closely resembling the target, thus achieving successful attacks. The susceptibility
    of T2M models to our attacks suggest an urgent need to develop defensive methods
    and improve the robustness against adversarial exploitation.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，提出了一种新方法，涉及对T2M模型进行有针对性的对抗攻击。此外，我们介绍了一种名为ALERT-Motion的自主LLM增强对抗攻击方法，该方法包含两个模块：多模态信息集成（MMIC）模块和自适应调度（AD）模块。在MMIC的协助下，AD通过在扩展、精炼和更新过程中引入LLM，自主学习并执行对最佳对抗提示的搜索。我们的广泛实验验证了发现流畅且与运动相关的对抗提示的能力。此外，这些提示使受害T2M模型生成与目标相似的运动，从而实现了成功攻击。T2M模型对我们攻击的敏感性表明，迫切需要开发防御方法并提高对抗性利用的鲁棒性。
- en: References
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahn et al. [2018] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai
    Oh. Text2Action: generative adversarial synthesis from language to action. In
    *ICRA*, pages 5915–5920, 2018.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn et al. [2018] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, 和 Songhwai
    Oh. Text2Action：从语言到动作的生成对抗合成。见*ICRA*，页5915–5920，2018。
- en: 'Ahuja and Morency [2019] Chaitanya Ahuja and Louis-Philippe Morency. Language2Pose:
    Natural language grounded pose forecasting. In *3DV*, pages 719–728, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahuja 和 Morency [2019] Chaitanya Ahuja 和 Louis-Philippe Morency. Language2Pose：自然语言基础的姿态预测。见*3DV*，页719–728，2019。
- en: 'Athanasiou et al. [2022] Nikos Athanasiou, Mathis Petrovich, Michael J Black,
    and Gül Varol. Teach: Temporal action composition for 3d humans. In *3DV*, pages
    414–423, 2022.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athanasiou et al. [2022] Nikos Athanasiou, Mathis Petrovich, Michael J Black,
    和 Gül Varol. Teach：3D人类的时间动作合成。见*3DV*，页414–423，2022。
- en: Barquero et al. [2024] German Barquero, Sergio Escalera, and Cristina Palmero.
    Seamless human motion composition with blended positional encodings, 2024.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barquero et al. [2024] German Barquero, Sergio Escalera, 和 Cristina Palmero.
    无缝的人体运动合成与混合位置编码，2024。
- en: 'Birhane et al. [2021] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe.
    Multimodal datasets: misogyny, pornography, and malignant stereotypes. *arXiv
    preprint arXiv:2110.01963*, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Birhane et al. [2021] Abeba Birhane, Vinay Uday Prabhu, 和 Emmanuel Kahembwe.
    多模态数据集：厌女、色情和恶意刻板印象。*arXiv preprint arXiv:2110.01963*，2021。
- en: Cer et al. [2018] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
    Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,
    et al. Universal sentence encoder for english. In *EMNLP*, pages 169–174, 2018.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cer et al. [2018] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco,
    Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar等.
    英语的通用句子编码器。见*EMNLP*，页169–174，2018。
- en: Chen et al. [2023] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
    Chen, and Gang Yu. Executing your commands via motion diffusion in latent space.
    In *CVPR*, pages 18000–18010, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
    Chen, 和 Gang Yu. 通过潜在空间中的运动扩散执行你的命令。见*CVPR*，页18000–18010，2023。
- en: Goodfellow et al. [2014a] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *NeurIPS*, 27, 2014a.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014a] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 生成对抗网络。*NeurIPS*，27，2014a。
- en: Goodfellow et al. [2014b] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
    Explaining and harnessing adversarial examples. *arXiv preprint arXiv:1412.6572*,
    2014b.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. [2014b] Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy.
    解释和利用对抗示例。*arXiv preprint arXiv:1412.6572*，2014b。
- en: 'Guo et al. [2020] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun,
    Annan Deng, Minglun Gong, and Li Cheng. Action2Motion: Conditioned generation
    of 3D human motions. In *ACM MM*, pages 2021–2029, 2020.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2020] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun,
    Annan Deng, Minglun Gong, 和 Li Cheng. Action2Motion：有条件生成3D人类动作。见*ACM MM*，页2021–2029，2020。
- en: Guo et al. [2022a] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu
    Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In
    *CVPR*, pages 5152–5161, 2022a.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2022a] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu
    Li, 和 Li Cheng. 从文本生成多样化和自然的3D人类动作。发表于 *CVPR*，页码 5152–5161，2022a。
- en: 'Guo et al. [2022b] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. TM2T: Stochastic
    and tokenized modeling for the reciprocal generation of 3d human motions and texts.
    In *ECCV*, pages 580–597\. Springer, 2022b.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. [2022b] Chuan Guo, Xinxin Zuo, Sen Wang, 和 Li Cheng. TM2T: 随机和标记化建模用于3D人类动作与文本的互生生成。发表于
    *ECCV*，页码 580–597。Springer，2022b。'
- en: Gur et al. [2023] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning,
    long context understanding, and program synthesis, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur et al. [2023] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, 和 Aleksandra Faust. 具备规划、长时间上下文理解和程序合成的现实世界网络代理，2023。
- en: Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion
    probabilistic models. *NeurIPS*, 33:6840–6851, 2020.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho et al. [2020] Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 去噪扩散概率模型。发表于 *NeurIPS*，33:6840–6851，2020。
- en: 'Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    Language models as zero-shot planners: Extracting actionable knowledge for embodied
    agents. In *ICML*, pages 9118–9147\. PMLR, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch.
    语言模型作为零样本规划者：为具身智能体提取可操作的知识。发表于 *ICML*，页码 9118–9147。PMLR，2022。
- en: Jin et al. [2020] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits.
    Is bert really robust? a strong baseline for natural language attack on text classification
    and entailment. In *AAAI*, pages 8018–8025, 2020.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin et al. [2020] Di Jin, Zhijing Jin, Joey Tianyi Zhou, 和 Peter Szolovits.
    BERT 真的是鲁棒的吗？自然语言攻击文本分类和蕴涵的强基线。发表于 *AAAI*，页码 8018–8025，2020。
- en: Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational
    bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma and Welling [2013] Diederik P Kingma 和 Max Welling. 自动编码变分贝叶斯。*arXiv
    预印本 arXiv:1312.6114*，2013。
- en: 'Lee et al. [2023] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Multiact:
    Long-term 3d human motion generation from multiple action labels. In *AAAI*, pages
    1231–1239, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. [2023] Taeryung Lee, Gyeongsik Moon, 和 Kyoung Mu Lee. Multiact:
    从多个动作标签生成长期3D人类动作。发表于 *AAAI*，页码 1231–1239，2023。'
- en: 'Liu et al. [2023a] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, and Ning Zhang.
    RIATIG: Reliable and imperceptible adversarial text-to-image generation with natural
    prompts. In *CVPR*, pages 20585–20594, 2023a.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023a] Han Liu, Yuhao Wu, Shixuan Zhai, Bo Yuan, 和 Ning Zhang.
    RIATIG: 可靠且不可察觉的对抗性文本到图像生成，带有自然提示。发表于 *CVPR*，页码 20585–20594，2023a。'
- en: Liu et al. [2023b] Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, and Alan
    Yuille. Intriguing properties of text-guided diffusion models. *arXiv preprint
    arXiv:2306.00974*, 2023b.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Qihao Liu, Adam Kortylewski, Yutong Bai, Song Bai, 和 Alan
    Yuille. 文本引导扩散模型的有趣特性。*arXiv 预印本 arXiv:2306.00974*，2023b。
- en: 'Liu et al. [2023c] Xiaogeng Liu, Nan Xu, Muhao Chen, and Chaowei Xiao. Autodan:
    Generating stealthy jailbreak prompts on aligned large language models. *arXiv
    preprint arXiv:2310.04451*, 2023c.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023c] Xiaogeng Liu, Nan Xu, Muhao Chen, 和 Chaowei Xiao. Autodan:
    在对齐的大型语言模型上生成隐秘的越狱提示。*arXiv 预印本 arXiv:2310.04451*，2023c。'
- en: 'Liu et al. [2023d] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, and Yang Liu. Jailbreaking chatgpt via prompt
    engineering: An empirical study. *arXiv preprint arXiv:2305.13860*, 2023d.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023d] Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng,
    Ying Zhang, Lida Zhao, Tianwei Zhang, 和 Yang Liu. 通过提示工程破解 ChatGPT: 一项实证研究。*arXiv
    预印本 arXiv:2305.13860*，2023d。'
- en: Luo et al. [2023] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual
    humanoid control for real-time simulated avatars. In *CVPR*, pages 10895–10904,
    2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. [2023] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, 等. 实时模拟化身的永续类人控制。发表于
    *CVPR*，页码 10895–10904，2023。
- en: 'Mahmood et al. [2019] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard
    Pons-Moll, and Michael J Black. AMASS: Archive of motion capture as surface shapes.
    In *ICCV*, pages 5442–5451, 2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahmood et al. [2019] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard
    Pons-Moll, 和 Michael J Black. AMASS: 作为表面形状的动作捕捉档案。发表于 *ICCV*，页码 5442–5451，2019。'
- en: Millière [2022] Raphaël Millière. Adversarial attacks on image generation with
    made-up words. *arXiv preprint*, 2022.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Millière [2022] Raphaël Millière. 使用虚构词汇的图像生成对抗性攻击。*arXiv 预印本*，2022。
- en: Perez et al. [2022] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman
    Ring, John Aslanides, Amelia Glaese, Nat McAleese, and Geoffrey Irving. Red teaming
    language models with language models. In *EMNLP*, pages 3419–3448, 2022.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 等人 [2022] Ethan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman
    Ring, John Aslanides, Amelia Glaese, Nat McAleese 和 Geoffrey Irving。用语言模型对语言模型进行红队测试。发表于
    *EMNLP*, 页码 3419–3448, 2022。
- en: 'Petrovich et al. [2023] Mathis Petrovich, Michael J Black, and Gül Varol. TMR:
    Text-to-motion retrieval using contrastive 3d human motion synthesis. In *ICCV*,
    pages 9488–9497, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Petrovich 等人 [2023] Mathis Petrovich, Michael J Black 和 Gül Varol。TMR：使用对比3D人体运动合成进行文本到运动检索。发表于
    *ICCV*, 页码 9488–9497, 2023。
- en: 'Qing et al. [2023] Zhongfei Qing, Zhongang Cai, Zhitao Yang, and Lei Yang.
    Story-to-motion: Synthesizing infinite and controllable character animation from
    long text. In *SIGGRAPH Asia 2023 Technical Communications*, pages 1–4, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qing 等人 [2023] Zhongfei Qing, Zhongang Cai, Zhitao Yang 和 Lei Yang。故事到运动：从长文本合成无限且可控的角色动画。发表于
    *SIGGRAPH Asia 2023 Technical Communications*, 页码 1–4, 2023。
- en: 'Qu et al. [2023] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas
    Zannettou, and Yang Zhang. Unsafe diffusion: On the generation of unsafe images
    and hateful memes from text-to-image models. In *SIGSAC*, pages 3403–3417, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu 等人 [2023] Yiting Qu, Xinyue Shen, Xinlei He, Michael Backes, Savvas Zannettou
    和 Yang Zhang。危险的扩散：从文本到图像模型生成不安全图像和仇恨表情包。发表于 *SIGSAC*, 页码 3403–3417, 2023。
- en: Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.
    2019.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei
    和 Ilya Sutskever。语言模型是无监督的多任务学习者。2019。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, et al. Learning
    transferable visual models from natural language supervision. In *ICML*, pages
    8748–8763, 2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2021] Alec Radford, Jong Wook Kim, Chris Hallacy 等人。从自然语言监督中学习可转移的视觉模型。发表于
    *ICML*, 页码 8748–8763, 2021。
- en: 'Shen et al. [2024] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving ai tasks with chatgpt and its friends
    in hugging face. *NeurIPS*, 36, 2024.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人 [2024] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu
    和 Yueting Zhuang。HuggingGPT：通过 chatgpt 和 Hugging Face 的朋友解决 AI 任务。 *NeurIPS*,
    36, 2024。
- en: 'Sitawarin et al. [2024] Chawin Sitawarin, Norman Mu, David Wagner, and Alexandre
    Araujo. PAL: Proxy-guided black-box attack on large language models. *arXiv preprint
    arXiv:2402.09674*, 2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sitawarin 等人 [2024] Chawin Sitawarin, Norman Mu, David Wagner 和 Alexandre Araujo。PAL：代理引导的大型语言模型黑箱攻击。*arXiv
    预印本 arXiv:2402.09674*, 2024。
- en: 'Struppek et al. [2023] Lukas Struppek, Dominik Hintersdorf, and Kristian Kersting.
    Rickrolling the Artist: Injecting backdoors into text encoders for text-to-image
    synthesis. In *ICCV*, pages 4584–4596, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Struppek 等人 [2023] Lukas Struppek, Dominik Hintersdorf 和 Kristian Kersting。艺术家
    Rickrolling：在文本编码器中注入后门以进行文本到图像合成。发表于 *ICCV*, 页码 4584–4596, 2023。
- en: Tevet et al. [2022] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
    Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In *ICLR*, 2022.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tevet 等人 [2022] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or
    和 Amit Haim Bermano。人体运动扩散模型。发表于 *ICLR*, 2022。
- en: 'Wei et al. [2024] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken:
    How does llm safety training fail? *NeurIPS*, 36, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 [2024] Alexander Wei, Nika Haghtalab 和 Jacob Steinhardt。Jailbroken：大型语言模型的安全培训如何失败？
    *NeurIPS*, 36, 2024。
- en: Yang et al. [2021] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, and Eric Darve.
    Universal sentence representation learning with conditional masked language model.
    In *EMNLP*, pages 6216–6228, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2021] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law 和 Eric Darve。带有条件掩蔽语言模型的通用句子表示学习。发表于
    *EMNLP*, 页码 6216–6228, 2021。
- en: 'Yang et al. [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. MM-REACT:
    Prompting chatgpt for multimodal reasoning and action, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab,
    Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng 和 Lijuan Wang。MM-REACT：推动 chatgpt
    进行多模态推理和行动，2023。
- en: Yu et al. [2022] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid,
    Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al.
    Scaling autoregressive models for content-rich text-to-image generation. *Transactions
    on Machine Learning Research*, 2022.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2022] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid,
    Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan 等人。扩展自回归模型用于内容丰富的文本生成图像。*机器学习研究交易*,
    2022。
- en: 'Zhang et al. [2023] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang,
    Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2M-GPT: Generating human motion
    from textual descriptions with discrete representations. In *CVPR*, page 14730–14740,
    2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong
    Zhang, Hongwei Zhao, Hongtao Lu, 和 Xi Shen. T2M-GPT：从文本描述中生成离散表示的人体运动。发表于*CVPR*，第14730–14740页，2023年。
- en: 'Zhao et al. [2024] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin
    Liu, and Gao Huang. Expel: LLM agents are experiential learners. In *AAAI*, pages
    19632–19642, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2024] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu,
    和 Gao Huang. Expel：LLM代理是经验学习者。发表于*AAAI*，第19632–19642页，2024年。
- en: 'Zhu et al. [2023] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang,
    Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey.
    *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人 [2023] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin
    Shi, Feng Gao, Qi Tian, 和 Yizhou Wang. 人体运动生成：综述。*IEEE Transactions on Pattern
    Analysis and Machine Intelligence*，2023年。
- en: Zhuang et al. [2023] Haomin Zhuang, Yihua Zhang, and Sijia Liu. A pilot study
    of query-free adversarial attack against stable diffusion. In *CVPR*, pages 2384–2391,
    2023.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等人 [2023] Haomin Zhuang, Yihua Zhang, 和 Sijia Liu. 一项针对稳定扩散的无查询对抗攻击的初步研究。发表于*CVPR*，第2384–2391页，2023年。
- en: Zou et al. [2023] Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models. *arXiv
    preprint*, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邹等人 [2023] Andy Zou, Zifan Wang, J Zico Kolter, 和 Matt Fredrikson. 对对齐语言模型的通用和可转移对抗攻击。*arXiv
    preprint*，2023年。
