- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:57:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:57:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型语言模型能否信赖用于评估？通过代理辩论进行LLMs作为评估者的可扩展元评估
- en: 来源：[https://arxiv.org/html/2401.16788/](https://arxiv.org/html/2401.16788/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2401.16788/](https://arxiv.org/html/2401.16788/)
- en: Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Steffi Chern^(2,4)  Ethan Chern^(1,4)  Graham Neubig²  Pengfei Liu^(1,3,4)
- en: ¹Shanghai Jiao Tong University  ²Carnegie Mellon University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹上海交通大学 ²卡内基梅隆大学
- en: ³Shanghai Artificial Intelligence Laboratory  ⁴Generative AI Research Lab (GAIR)
      Corresponding author
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ³上海人工智能实验室 ⁴生成式AI研究实验室（GAIR）  通讯作者
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Despite the utility of Large Language Models (LLMs) across a wide range of
    tasks and scenarios, developing a method for reliably evaluating LLMs across varied
    contexts continues to be challenging. Modern evaluation approaches often use LLMs
    to assess responses generated by LLMs. However, the meta-evaluation conducted
    to assess the effectiveness of these LLMs as evaluators is typically constrained
    by the coverage of existing benchmarks or requires extensive human annotation.
    This underscores the urgency of methods for scalable meta-evaluation that can
    effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators
    across diverse tasks and scenarios, particularly in potentially new, user-defined
    scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation
    framework that leverages the capabilities of multiple communicative LLM agents.
    This framework supports multi-round discussions to assist human annotators in
    discerning the most capable LLMs as evaluators, which significantly eases their
    workload in cases that used to require large-scale annotations during meta-evaluation.
    We release the code for our framework, which is publicly available at: [https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（LLMs）在广泛的任务和场景中具有很高的实用性，但在不同情境下开发一种可靠评估LLMs的方法仍然具有挑战性。现代评估方法通常使用LLMs来评估LLMs生成的回答。然而，用于评估这些LLMs作为评估者有效性的元评估通常受到现有基准覆盖范围的限制，或需要大量人工标注。这突显了可扩展元评估方法的紧迫性，这些方法能够有效、可靠且高效地评估LLMs作为评估者在各种任务和场景中的表现，特别是在潜在的新型、用户定义的场景中。为了填补这一空白，我们提出了ScaleEval，这是一种通过代理辩论辅助的元评估框架，利用多个交互式LLM代理的能力。该框架支持多轮讨论，帮助人工标注者识别最具能力的LLM作为评估者，从而显著减轻他们在元评估中以往需要大量标注的工作量。我们发布了该框架的代码，公开可用，网址：[https://github.com/GAIR-NLP/scaleeval](https://github.com/GAIR-NLP/scaleeval)。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/e24a42f1ce74dbd339846ca58bcd7873.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e24a42f1ce74dbd339846ca58bcd7873.png)'
- en: 'Figure 1: We demonstrate ScaleEval, our scalable meta-evaluation framework.
    This is used in assessing the reliability and robustness of employing LLMs as
    evaluators for different evaluative purposes.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们展示了ScaleEval，我们的可扩展元评估框架。该框架用于评估将LLMs作为评估者在不同评估目的中的可靠性和稳健性。
- en: 'Large Language Models (LLMs) Bubeck et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib3));
    Gemini Team et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib12)) have
    rapidly evolved to the point where they can tackle a wide range of tasks with
    impressive performance. While this has unlocked a variety of exciting potential
    applications, it has also introduced complex challenges in evaluating the generated
    outputs. Current efforts on LLM evaluation primarily focus on automated evaluation
    metrics (Fu et al., [2023](https://arxiv.org/html/2401.16788v1#bib.bib10); Li
    et al., [2023c](https://arxiv.org/html/2401.16788v1#bib.bib19); Zheng et al.,
    [2023](https://arxiv.org/html/2401.16788v1#bib.bib25); Wang et al., [2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)),
    many of which use LLMs themselves to do evaluation. However, when these LLMs as
    evaluators are applied to a new task, it begs the question: *can LLMs be trusted
    for evaluation?* In many cases, the answer is not clear.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）Bubeck 等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib3)）；Gemini
    团队等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib12)）已迅速发展到能够处理广泛任务并表现出色的程度。尽管这开启了各种令人兴奋的潜在应用，但也带来了在评估生成结果时的复杂挑战。目前，LLM
    评估的主要工作集中在自动评估指标上（Fu 等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib10)；Li
    等人，[2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)；Zheng 等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)；Wang
    等人，[2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)），其中许多指标本身使用 LLM 来进行评估。然而，当这些
    LLM 作为评估者应用于新任务时，不禁引发了一个问题：*LLM 能否被信任用于评估？* 在许多情况下，答案并不明确。
- en: On the other hand, there are a few fortunate tasks where meta-evaluation (evaluation
    of evaluation metrics) has been performed rigorously (§[2](https://arxiv.org/html/2401.16788v1#S2
    "2 Related Work ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")). This meta-evaluation
    typically involves the collection of human-annotated judgements for particular
    criteria (e.g. fluency of outputs, semantic adherence to the input). For instance,
    for machine translation quality metrics, there is an extensive meta-evaluation
    data from the WMT metrics task Freitag et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib9)),
    and for summarization there are datasets like TAC and RealSum Dang et al. ([2008](https://arxiv.org/html/2401.16788v1#bib.bib8));
    Bhandari et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib2)). Once such
    a dataset is collected, meta-evaluation can be performed by measuring the correlation
    between automatic evaluation metrics and the human gold-standard (§[3](https://arxiv.org/html/2401.16788v1#S3
    "3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，也有一些幸运的任务，其中进行了严格的元评估（评估评估指标）（§[2](https://arxiv.org/html/2401.16788v1#S2
    "2 Related Work ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。这种元评估通常涉及为特定标准（例如输出的流畅性、语义符合输入的程度）收集人工标注的判断。例如，针对机器翻译质量指标，WMT
    指标任务 Freitas 等人（[2022](https://arxiv.org/html/2401.16788v1#bib.bib9)）提供了大量的元评估数据；对于摘要生成，像
    TAC 和 RealSum 数据集 Dang 等人（[2008](https://arxiv.org/html/2401.16788v1#bib.bib8)）；Bhandari
    等人（[2020](https://arxiv.org/html/2401.16788v1#bib.bib2)）等也做了相关工作。一旦收集到这样的数据集，就可以通过测量自动评估指标与人工黄金标准之间的相关性来进行元评估（§[3](https://arxiv.org/html/2401.16788v1#S3
    "3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。
- en: However, these datasets are extremely costly to collect, as they require meticulous
    annotation by skilled human experts. With the increasing use of LLMs for various
    purposes such as math problem solving Hendrycks et al. ([2021](https://arxiv.org/html/2401.16788v1#bib.bib14)),
    reading comprehension Zhong et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib26)),
    creative writing Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)),
    multilingual applications Hu et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib15));
    Bang et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib1)), and many more,
    it is not feasible to create these human-judged datasets for every new task. As
    a result, LLMs as evaluators are used without proper vetting, and in many cases
    the evaluators themselves are highly unreliable (Wang et al., [2023b](https://arxiv.org/html/2401.16788v1#bib.bib24);
    Huang et al., [2023](https://arxiv.org/html/2401.16788v1#bib.bib16)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些数据集的收集成本极高，因为它们需要熟练的人工专家进行细致的注释。随着LLM被越来越多地用于各种目的，如数学问题求解（Hendrycks等人，[2021](https://arxiv.org/html/2401.16788v1#bib.bib14)），阅读理解（Zhong等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib26)），创意写作（Zheng等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)），多语言应用（Hu等人，[2020](https://arxiv.org/html/2401.16788v1#bib.bib15)）；Bang等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib1)）等，创建这些人工评判的数据集用于每个新任务已变得不可行。因此，LLM作为评估者在没有适当审核的情况下被使用，并且在许多情况下，评估者本身是极其不可靠的（Wang等人，[2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)；Huang等人，[2023](https://arxiv.org/html/2401.16788v1#bib.bib16)）。
- en: In this paper, we propose ScaleEval, a *scalable meta-evaluation framework*
    for the era of LLMs, which creates meta-evaluation benchmarks across various tasks
    and scenarios (§[4](https://arxiv.org/html/2401.16788v1#S4 "4 Methodology ‣ Can
    Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs
    as Evaluators via Agent Debate")). Concretely, ScaleEval relies on debate between
    multiple LLM agents, followed by minimal human oversight in cases where the agent
    LLMs do not agree (Fig. [1](https://arxiv.org/html/2401.16788v1#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")). Since our framework
    allows users to use their own prompts and responses while applying the framework
    to any scenario or criterion that they define, it offers flexibility and adaptability
    in various evaluation contexts.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了ScaleEval，一个*可扩展的元评估框架*，旨在为大语言模型（LLMs）时代提供支持，该框架创建了跨各种任务和场景的元评估基准（§[4](https://arxiv.org/html/2401.16788v1#S4
    "4 Methodology ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。具体而言，ScaleEval依赖于多个LLM代理之间的辩论，随后在代理LLM不一致的情况下进行最小化的人类监督（图[1](https://arxiv.org/html/2401.16788v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。由于我们的框架允许用户使用自己的提示和回应，并将其应用于他们定义的任何场景或标准，因此它在各种评估情境中提供了灵活性和适应性。
- en: 'In experiments, we conduct meta-meta evaluation (§[6](https://arxiv.org/html/2401.16788v1#S6
    "6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models
    be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via
    Agent Debate")) demonstrating that our proposed approach correlates well with
    when meta-evaluation is performed entirely by human expert annotators. Further,
    we assess the reliability and cost-performance trade-off of various LLMs as evaluators
    under a variety of scenarios, and closely examine their specific capabilities
    and limitations as evaluators (§[7](https://arxiv.org/html/2401.16788v1#S7 "7
    Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")).
    We also examine the impact that variations in prompts used for evaluation can
    have on the performance of LLMs as evaluators (§[8](https://arxiv.org/html/2401.16788v1#S8
    "8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '在实验中，我们进行了元-元评估（§[6](https://arxiv.org/html/2401.16788v1#S6 "6 Exp-I: Meta-Meta-Evaluation
    of Multi-Agent Debate ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")），证明我们提出的方法与完全由人类专家标注者进行的元评估有很好的相关性。此外，我们评估了不同
    LLM 作为评估者在各种场景下的可靠性和性价比，并仔细审视了它们作为评估者的具体能力和局限性（§[7](https://arxiv.org/html/2401.16788v1#S7
    "7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。我们还研究了用于评估的提示变体对
    LLM 作为评估者表现的影响（§[8](https://arxiv.org/html/2401.16788v1#S8 "8 Exp-III: Meta-Evaluation
    with Criteria Prompt Format Variations ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")）。'
- en: All code from our framework is made available open-source, enabling the community
    to conduct meta-evaluation on LLMs as evaluators using their own prompts, LLM
    responses, criteria, and scenarios.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们框架中的所有代码都已开源，允许社区使用自己的提示、LLM 响应、标准和场景对 LLM 作为评估者进行元评估。
- en: 2 Related Work
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '|  | Meta-Eval | # Scenarios | Custom. | Scala. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | 元评估 | # 场景 | 可定制 | 可扩展 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLM-as-a-Judge | Human | High | ✗ | Low |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| LLM-as-a-Judge | 人类 | 高 | ✗ | 低 |'
- en: '| FairEval | Human | Low | ✗ | Low |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| FairEval | 人类 | 低 | ✗ | 低 |'
- en: '| ChatEval | Human | Low | ✗ | Low |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| ChatEval | 人类 | 低 | ✗ | 低 |'
- en: '| ScaleEval | Agent Debate | High | ✓ | High |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ScaleEval | 代理辩论 | 高 | ✓ | 高 |'
- en: 'Table 1: Comparison of the meta-evaluation processes across different strategies
    using LLMs as evaluators: LLM-as-a-Judge Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)),
    FairEval Wang et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)),
    ChatEval Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)), and
    our own work, ScaleEval. “Custom.” denotes whether the evaluation criterion could
    be customized. “Scala.” refers to scalability.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用 LLM 作为评估者的不同策略中的元评估过程比较：LLM-as-a-Judge Zheng 等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)），FairEval
    Wang 等人（[2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)），ChatEval Chan
    等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib4)），以及我们自己的工作，ScaleEval。
    “Custom.” 表示评估标准是否可以定制。 “Scala.” 指的是可扩展性。
- en: 2.1 Automatic Evaluation of LLM Output
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 输出的自动评估
- en: The most common paradigm for evaluating LLMs is to evaluate their capabilities
    on standard benchmarks for tasks such as reasoning (e.g. BigBench Srivastava et al.
    ([2022](https://arxiv.org/html/2401.16788v1#bib.bib22))), common sense QA (e.g. MMLU
    Hendrycks et al. ([2020](https://arxiv.org/html/2401.16788v1#bib.bib13))), or
    code generation (e.g. HumanEval Chen et al. ([2021b](https://arxiv.org/html/2401.16788v1#bib.bib6))).
    These are indicative of the capabilities of the models, but do not measure model
    abilities for open-ended tasks requiring generation of free-form text.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对 LLM 进行评估的最常见范式是通过标准基准评估其在推理（例如 BigBench Srivastava 等人（[2022](https://arxiv.org/html/2401.16788v1#bib.bib22)）），常识问答（例如
    MMLU Hendrycks 等人（[2020](https://arxiv.org/html/2401.16788v1#bib.bib13)）），或代码生成（例如
    HumanEval Chen 等人（[2021b](https://arxiv.org/html/2401.16788v1#bib.bib6)））等任务上的能力。这些基准能反映模型的能力，但并不衡量模型在生成自由文本的开放性任务中的能力。
- en: To adapt to the rapid growth in the capabilities of LLMs for open-ended tasks,
    LLM evaluation has started to shift towards evaluating generated text directly,
    often using LLMs themselves as evaluators Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10));
    Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)); Zheng et al.
    ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)).
    In addition, there are a few recent works that perform LLM-based multi-agent debate
    to improve the fidelity of evaluation Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4));
    Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18)). While these
    methods take advantage of the instruction-following capabilities and versatility
    of LLMs, directly using LLMs as evaluators or communicative agents out-of-the-box
    in diverse, unseen user-defined scenarios provides no guarantees with respect
    to the accuracy of these methods. We aim to address this issue by introducing
    scalable meta-evaluation to ensure the reliability of the evaluation protocol
    under diverse scenarios.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适应LLM在开放性任务中能力的快速增长，LLM评估已经开始转向直接评估生成的文本，通常使用LLM本身作为评估者（Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10))；Li
    et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19))；Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25))；Wang
    et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23))）。此外，还有一些近期的研究通过LLM驱动的多代理辩论来提高评估的准确性（Chan
    et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4))；Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18))）。尽管这些方法利用了LLM的指令跟随能力和多功能性，但在多样化的、未见过的用户定义场景中，直接将LLM作为评估者或交互代理来使用，无法保证这些方法的准确性。我们旨在通过引入可扩展的元评估来解决这个问题，以确保在不同场景下评估协议的可靠性。
- en: Another widely used evaluation platform, Chatbot Arena Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25))
    supports a crowd-sourcing method to collect diverse user prompts from various
    scenarios. However, the process of evaluating LLMs’ performance in Chatbot Arena
    relies heavily on human evaluations, which may not be readily accessible to everyone
    interested in assessing LLMs’ abilities for a specific tasks or scenario. In addition,
    the human evaluators involved are not subject to a uniform set of standards or
    explicit evaluation guidelines, which could lead to biased or imprecise evaluation
    assessments.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个广泛使用的评估平台，Chatbot Arena（Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25))）支持众包方法，从各种场景中收集多样的用户提示。然而，在Chatbot
    Arena中评估LLM表现的过程在很大程度上依赖于人工评估，这可能并非每个有兴趣评估LLM在特定任务或场景中的能力的人都能轻易获取。此外，参与的人工评估者并未遵循统一的标准或明确的评估指南，这可能导致评估结果的偏差或不准确。
- en: 2.2 Meta-Evaluation of LLMs as Evaluators
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLMs作为评估者的元评估
- en: 'Previous research proposing methods for LLMs as evaluators usually involves
    conducting meta-evaluation in 3 different ways: (i) leveraging existing NLP meta-evaluation
    benchmarks Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10));
    Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)), (ii) conducting
    small-scale meta-evaluations on expert-annotated datasets for specific tasks or
    scenarios Chiang and Lee ([2023](https://arxiv.org/html/2401.16788v1#bib.bib7));
    Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)); Zheng et al.
    ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)), or (iii) using crowd-sourcing
    platforms to collect human annotations Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)).
    However, due to the lack of coverage in existing datasets and annotation budgets,
    both (i) and (ii) are inherently limited in their comprehensiveness. (iii) can
    provide more comprehensive meta-evaluation via crowd-sourcing, but the amount
    of human annotation required in the meta-evaluation process limits the scalability
    of the approach, and crowd workers may not be particularly accurate at more complex
    tasks. To address these issues, we propose an agent-debate-assisted meta-evaluation
    approach to mitigate this effort.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究提出的将LLMs作为评估者的方法通常涉及通过三种不同的方式进行元评估：（i）利用现有的自然语言处理（NLP）元评估基准 Fu 等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib10)）；Chan
    等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib4)），（ii）在专家标注的数据集上进行小规模元评估，针对特定任务或场景
    Chiang 和 Lee（[2023](https://arxiv.org/html/2401.16788v1#bib.bib7)）；Wang 等人（[2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)）；Zheng
    等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)），或（iii）使用众包平台收集人工标注 Zheng
    等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)）。然而，由于现有数据集和标注预算的覆盖范围不足，（i）和（ii）在全面性上本质上受到限制。（iii）可以通过众包提供更全面的元评估，但元评估过程中所需的大量人工标注限制了该方法的可扩展性，并且众包工人在处理更复杂任务时可能不特别准确。为了解决这些问题，我们提出了一种代理辩论辅助的元评估方法，以减轻这一工作量。
- en: 3 Preliminaries
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步准备
- en: In this section, we provide an introduction to the concepts of automatic evaluation
    and meta-evaluation systems, particularly focused on evaluation of LLM-generated
    outputs in the era of generative AI.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍自动评估和元评估系统的概念，特别聚焦于生成式人工智能时代对LLM生成输出的评估。
- en: 3.1 Key Terms
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 关键术语
- en: We first define some key terms that will be used throughout our paper.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一些将在本文中使用的关键术语。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Criterion: A criterion defines a standard that measures the quality of the
    response generated by LLMs based on the user prompt. Some examples include: helpfulness,
    fluency, factuality, or creativity, among others.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准：标准定义了一个衡量LLM根据用户提示生成的响应质量的标准。一些例子包括：有用性、流畅性、事实性或创造性等。
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scenario: A scenario describes the real-world situations in which users are
    interacting with LLMs. For example, brainstorming, coding, and dialog, among others.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 场景：场景描述了用户与大型语言模型（LLMs）交互的现实世界情境。例如，头脑风暴、编码和对话等。
- en: 3.2 Automatic Evaluation
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自动评估
- en: 'Automatic evaluation using LLMs measures the quality of LLM-generated responses
    given prompts under different criteria. Usually, automatic evaluation is conducted
    with one of two different protocols: single-response evaluation and pairwise response
    comparison Ouyang et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib21));
    Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Li et al.
    ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib17)). In this paper, we focus
    on pairwise response comparison. Pairwise response comparison is intuitive for
    both humans and LLMs as evaluators when conducting assessments. It could be further
    extended to provide win-rates and Elo scores across models Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)),
    offering a straightforward leaderboard to understand the relative performance
    of different models under various scenarios. Formally, given an automatic evaluation
    metric $E$, a user-defined evaluation criterion $c$ (e.g. helpfulness, reasoning,
    creativity), a user prompt $p$, and responses generated by two systems $r_{1},r_{2}$,
    evaluation for pairwise response comparison is done in the following way:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大语言模型（LLMs）进行自动评估，衡量LLM生成的响应在不同标准下的质量。通常，自动评估采用两种不同的协议之一进行：单一响应评估和成对响应比较 Ouyang
    等人（[2022](https://arxiv.org/html/2401.16788v1#bib.bib21)）；郑等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)）；李等人（[2023a](https://arxiv.org/html/2401.16788v1#bib.bib17)）。本文聚焦于成对响应比较。成对响应比较对于人类和LLM作为评估者来说，都是直观的评估方法。它还可以进一步扩展，用于提供模型间的胜率和Elo评分，郑等人（[2023](https://arxiv.org/html/2401.16788v1#bib.bib25)），提供了一个简明的排行榜，以便了解不同模型在各种场景下的相对表现。形式上，给定一个自动评估指标
    $E$，一个用户定义的评估标准 $c$（例如：有用性、推理能力、创造性），一个用户提示 $p$，以及由两个系统生成的响应 $r_{1},r_{2}$，成对响应比较的评估方法如下：
- en: '|  | $o=E(c,p,r_{1},r_{2}).$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $o=E(c,p,r_{1},r_{2}).$ |  | (1) |'
- en: $o\in\{1,0,-1\}$ represents that $r_{1}$ is better, equal, or worse than $r_{2}$,
    respectively, given the user prompt $p$ under criterion $c$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $o\in\{1,0,-1\}$ 表示在评估标准 $c$ 下，给定用户提示 $p$，$r_{1}$ 比 $r_{2}$ 好、相等或差。
- en: 3.3 Meta-Evaluation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 元评估
- en: Meta-evaluation assesses the quality of an automatic evaluation metric. Formally,
    we define a gold-standard evaluation metric $G$ (e.g. human experts) that other
    automatic metrics should aspire to match. In pairwise response comparison, the
    meta-evaluation dataset $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$
    contains user prompts and corresponding responses from two systems, annotated
    with gold-standard evaluations. The meta-evaluation process assesses the performance
    $\textsc{meta}(E)$ of the automatic evaluation metric $E$ under a certain criterion
    $c$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估评估自动评估指标的质量。形式上，我们定义一个黄金标准评估指标 $G$（例如：人类专家），其他自动评估指标应当与之匹配。在成对响应比较中，元评估数据集
    $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$ 包含用户提示和来自两个系统的相应响应，这些响应已由黄金标准评估进行了标注。元评估过程评估自动评估指标
    $E$ 在某一评估标准 $c$ 下的表现 $\textsc{meta}(E)$。
- en: In pairwise response comparison, the meta-evaluation measures the example-level
    agreement rate or the system-level agreement rate between $E$ and $G$ across the
    meta-evaluation dataset. A high agreement rate between $E$ and $G$ represents
    that $E$ is a good automatic evaluation metric.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在成对响应比较中，元评估衡量的是在元评估数据集上，$E$ 和 $G$ 之间的示例级别一致性率或系统级别一致性率。$E$ 和 $G$ 之间的一致性率越高，表明
    $E$ 是一个优秀的自动评估指标。
- en: 'For the example-level agreement rate, we calculate:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于示例级别的一致性率，我们计算：
- en: '|  | $\textsc{meta}(E)=\frac{1}{n}\sum_{i=1}^{n}\delta_{E(c,p_{i},r_{1,i},r_{2,i}),G%
    (c,p_{i},r_{1,i},r_{2,i})},$ |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{meta}(E)=\frac{1}{n}\sum_{i=1}^{n}\delta_{E(c,p_{i},r_{1,i},r_{2,i}),G%
    (c,p_{i},r_{1,i},r_{2,i})},$ |  | (2) |'
- en: where $0\leq\textsc{meta}(E)\leq 1$, and $\delta_{\cdot,\cdot}$ refers to the
    Kronecker delta function.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $0\leq\textsc{meta}(E)\leq 1$，且 $\delta_{\cdot,\cdot}$ 代表克罗内克（Kronecker）delta函数。
- en: 'For the system-level agreement rate, given that $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$
    and $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$, we calculate:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于系统级一致性率，假设 $\mathcal{E}=\{E(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$ 和 $\mathcal{G}=\{G(c,p_{i},r_{1,i},r_{2,i})\}_{i=1}^{n}$，我们计算：
- en: '|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})},$
    |  | (3) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textsc{meta}(E)=\delta_{\mathrm{mode}(\mathcal{E}),\mathrm{mode}(\mathcal{G})},$
    |  | (3) |'
- en: where $\textsc{meta}(E)\in\{0,1\}$, $\delta_{\cdot,\cdot}$ refers to the Kronecker
    delta function, and $\mathrm{mode(\cdot)}$ refers to the value (either $1,0,-1$
    in this case) that appears most often in the set $\mathcal{\mathcal{E}}$ or $\mathcal{\mathcal{G}}$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\textsc{meta}(E)\in\{0,1\}$，$\delta_{\cdot,\cdot}$ 指的是克罗内克δ函数，$\mathrm{mode(\cdot)}$
    指的是在集合 $\mathcal{\mathcal{E}}$ 或 $\mathcal{\mathcal{G}}$ 中最常出现的值（在此情况下为 $1,0,-1$）。
- en: 4 Methodology
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: In this section, we detail the frameworks that ScaleEval employs for meta-evaluation,
    evaluation, and human expert meta-meta evaluation. For meta-evaluation, we generally
    follow the pairwise response comparison setting described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    Notably, instead of relying solely on human labor to construct the meta-evaluation
    benchmark $\mathcal{G}$, we use a scalable, agent-debate assisted framework to
    instantiate the golden metric $G$ and construct the benchmark $\mathcal{G}$. For
    evaluation, we follow the pairwise response comparison setting outlined in §[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    The meta-meta evaluation process also follows the rules for meta-evaluation, as
    described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation
    ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"). The process is included
    to ensure the reliability of using the agent-debate assisted meta-evaluation framework.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了ScaleEval用于元评估、评估和人类专家元-元评估的框架。对于元评估，我们通常遵循§[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")中描述的成对响应比较设置。特别地，我们使用一种可扩展的、代理辩论辅助的框架来实例化黄金标准
    $G$ 并构建评估基准 $\mathcal{G}$，而不是仅仅依赖人工来构建元评估基准 $\mathcal{G}$。对于评估，我们遵循§[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")中概述的成对响应比较设置。元-元评估过程也遵循元评估的规则，正如§[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")中所描述的那样。这个过程的包含是为了确保使用代理辩论辅助的元评估框架的可靠性。
- en: 4.1 Meta-Evaluation Framework via Multi-Agent Debate
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 通过多代理辩论进行元评估
- en: The meta-evaluation framework involves multiple communicative agents $\{A_{j}\}_{j=1}^{m}$
    that conduct rounds of discussion $d=0\sim D-1$ with each other. This is less
    time-consuming and costly compared to traditional methods for meta-evaluation
    that relies entirely on human effort. With this agent-debate-assisted meta-evaluation
    framework, we can leverage each LLM agent’s distinct understanding about each
    query prompt $p_{i}$, LLM responses $r_{1,i},r_{2,i}$, and defined criterion $c$
    to make a comprehensive assessment of LLMs under different scenarios and criteria.
    Each LLM agent is capable of providing an evaluation result regarding which response
    is better, along with its corresponding justifications. Note that each LLM agent
    can also review other agents’ evaluation results and justifications after the
    initial round of discussion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估框架涉及多个沟通代理 $\{A_{j}\}_{j=1}^{m}$，它们在不同的讨论回合 $d=0\sim D-1$ 中互相交流。这比传统的完全依赖人工的元评估方法更节省时间和成本。通过这种代理辩论辅助的元评估框架，我们可以利用每个LLM代理对每个查询提示
    $p_{i}$、LLM响应 $r_{1,i},r_{2,i}$ 和定义的标准 $c$ 的独特理解，对LLM在不同场景和标准下进行全面评估。每个LLM代理能够提供一个关于哪个响应更好的评估结果，并附上相应的理由。请注意，每个LLM代理在初始讨论回合后，还可以回顾其他代理的评估结果和理由。
- en: 'In the initial round of discussion $d=0$, each LLM agent independently provides
    an evaluation result and justification:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始讨论回合 $d=0$ 中，每个LLM代理独立提供一个评估结果和理由：
- en: '|  | $\mathcal{A}_{0}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\varnothing),\ldots,\\
    A_{m}(c,p_{i},r_{1,i},r_{2,i},\varnothing)],$ |  | (4) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{0}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\varnothing),\ldots,\\
    A_{m}(c,p_{i},r_{1,i},r_{2,i},\varnothing)],$ |  | (4) |'
- en: where
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 哪里
- en: '|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (5) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{0}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (5) |'
- en: 'indicates whether $r_{1,i}$ is better, equal, or worse than $r_{2,i}$, respectively,
    along with its justification. Note that the $\varnothing$ in the last argument
    of $A_{j}$ represents that in the initial round of discussion, each agent doesn’t
    have access to previous rounds of discussion. In subsequent discussion rounds
    $d=1\sim D-1$, agents are allowed to look at other agents’ previous assessments
    and conduct re-evaluations, in which each agent is prompted to stick with or change
    their original evaluation result. Specifically, given $\mathcal{A}_{d-1}(d\geq
    1)$, which represents the evaluation results and justifications of agents after
    $(d-1)^{th}$ rounds of discussions, we conduct the $d^{th}$ round of discussion:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表示$r_{1,i}$相对于$r_{2,i}$的好坏、相等或更差，以及其理由。请注意，$A_{j}$最后一个参数中的$\varnothing$表示在初始讨论回合中，每个代理人无法访问之前讨论回合的内容。在随后的讨论回合$d=1\sim
    D-1$中，代理人可以查看其他代理人的先前评估并进行重新评估，其中每个代理人会被提示坚持或改变原始评估结果。具体来说，给定$\mathcal{A}_{d-1}(d\geq
    1)$，表示代理人在$(d-1)^{th}$讨论回合之后的评估结果和理由，我们进行第$d$回合的讨论：
- en: '|  | $\mathcal{A}_{d}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1}),\ldots,\\
    A_{m}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1})]$ |  | (6) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{d}=[A_{1}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1}),\ldots,\\
    A_{m}(c,p_{i},r_{1,i},r_{2,i},\mathcal{A}_{d-1})]$ |  | (6) |'
- en: where similarly to $\mathcal{A}_{0}$,
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与$\mathcal{A}_{0}$类似，
- en: '|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (7) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}_{d}[j]_{j=1,\ldots,m}\in(\{1,0,-1\},\textsc{justification}),$
    |  | (7) |'
- en: The detailed prompt template for meta-evaluation can be found in Table [6](https://arxiv.org/html/2401.16788v1#A1.T6
    "Table 6 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")
    under Appendix.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 元评估的详细提示模板可在附录中的表格[6](https://arxiv.org/html/2401.16788v1#A1.T6 "表格 6 ‣ 附录A
    元评估提示 ‣ 大型语言模型能否用于评估？通过代理人辩论对LLM作为评估者进行可扩展的元评估")中找到。
- en: In cases where agents fail to reach a consensus after $d=D-1$ rounds of discussions,
    a human evaluator intervenes. The human evaluator reviews the assessment reports
    provided by the agents and makes a final decision. Through this process, we incorporate
    an element of human oversight, thereby increasing the reliability of the final
    decision. This approach strikes a balance between efficiency and the need for
    human judgment, ensuring that evaluations are done in a timely and accurate manner.
    An example of the multi-agent debate process during meta-evaluation is demonstrated
    in Fig. [2](https://arxiv.org/html/2401.16788v1#S4.F2 "Figure 2 ‣ 4.1 Meta-Evaluation
    Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large Language Models be
    Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent
    Debate").
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理人在经过$d=D-1$回合讨论后仍未达成共识的情况下，人工评估员将介入。人工评估员会审查代理人提供的评估报告，并做出最终决定。通过这一过程，我们加入了人工监督的元素，从而提高了最终决策的可靠性。这种方法在效率与人工判断需求之间取得了平衡，确保评估能及时且准确地完成。图[2](https://arxiv.org/html/2401.16788v1#S4.F2
    "图 2 ‣ 4.1 通过多代理人辩论的元评估框架 ‣ 4 方法论 ‣ 大型语言模型能否用于评估？通过代理人辩论对LLM作为评估者进行可扩展的元评估")展示了多代理人辩论过程的示例（用于元评估）。
- en: '![Refer to caption](img/b0599204bb08edc76b3a6e97c3fe7f0c.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/b0599204bb08edc76b3a6e97c3fe7f0c.png)'
- en: 'Figure 2: An example of the multi-agent debate process during meta-evaluation.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：多代理人辩论过程示例（用于元评估）。
- en: 4.2 Evaluation Framework
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估框架
- en: We follow the pairwise response comparison setting outlined in §[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2
    "3.2 Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    Note that in the LLM era, the automatic evaluation metric $E$ is often instantiated
    through single LLMs Fu et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10));
    Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)); Zheng et al.
    ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Wang et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23)),
    or multi-agent debate Chan et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4));
    Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18)). In ScaleEval,
    we focus on instantiating $E$ through single LLMs (e.g., gpt-3.5-turbo). However,
    it is important to note that our framework can be further generalized to other
    instantiations of $E$.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循第§[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2 "3.2 Automatic Evaluation
    ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中概述的成对响应比较设置。需要注意的是，在LLM时代，自动评估指标$E$通常通过单一LLMs来实现（Fu
    et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib10)); Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19));
    Zheng et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib25)); Wang et
    al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib23))）；或通过多代理辩论来实现（Chan
    et al. ([2023](https://arxiv.org/html/2401.16788v1#bib.bib4)); Li et al. ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib18))）。在ScaleEval中，我们专注于通过单一LLMs（例如gpt-3.5-turbo）来实现$E$。然而，值得注意的是，我们的框架可以进一步推广到$E$的其他实现方式。
- en: 4.3 Human Expert Meta-Meta Evaluation
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 人工专家元-元评估
- en: To test the reliability of our proposed meta-evaluation framework, we apply
    meta-meta evaluation. The meta-meta evaluation process also follows the meta-evaluation
    process described in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3 "3.3 Meta-Evaluation
    ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted for Evaluation? Scalable
    Meta-Evaluation of LLMs as Evaluators via Agent Debate"), where $E$ is instantiated
    as the agent-debated assisted protocol as described in §[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1
    "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate"), and $G$ is instantiated as the human expert annotation
    protocol.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们提出的元评估框架的可靠性，我们应用了元-元评估过程。元-元评估过程也遵循第§[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")节中描述的元评估过程，其中$E$被实例化为代理辩论辅助协议，如第§[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1
    "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")节中所述，$G$则被实例化为人工专家注释协议。
- en: 5 Examined Scenarios
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 个考察场景
- en: Establishing real-life scenarios that reflect individuals’ daily usage is key
    to assess the performance and limitations of LLMs in a comprehensive manner. In
    the current instantiation of ScaleEval, we include 8 different scenarios that
    are closely related to everyday situations and tasks Liang et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib20));
    Li et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib17)). Some example
    prompts for each defined scenario is shown in Table [2](https://arxiv.org/html/2401.16788v1#S5.T2
    "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate"). We describe
    more about exactly how we collect data for each of these scenarios below. Individuals
    interested in evaluating LLMs with our framework can supplement their assessment
    with additional scenarios.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 建立反映个体日常使用情况的现实场景是全面评估大语言模型（LLMs）性能和局限性的关键。在当前版本的ScaleEval中，我们包括了8个与日常情境和任务密切相关的不同场景（Liang
    et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib20)); Li et al. ([2023a](https://arxiv.org/html/2401.16788v1#bib.bib17))）。每个定义场景的示例提示如表[2](https://arxiv.org/html/2401.16788v1#S5.T2
    "Table 2 ‣ 5 Examined Scenarios ‣ Can Large Language Models be Trusted for Evaluation?
    Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")所示。我们将详细描述如何为每个场景收集数据。对评估LLMs感兴趣的个人可以在我们的框架基础上补充额外的场景进行评估。
- en: '| Scenario | Examples |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 示例 |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Brainstorming | - Can you tell me how to make chocolate chip cookies? |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 头脑风暴 | - 你能告诉我如何做巧克力曲奇饼干吗？ |'
- en: '| - Make a list of snacks and foods to serve as party snacks on a game day!
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| - 列出一些适合在比赛日作为派对小吃的零食和食品！ |'
- en: '| Coding | - What is the difference between HTML and JavaScript? |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | - HTML 和 JavaScript 有什么区别？ |'
- en: '| - Implement a binary search algorithm to find a specific element in a sorted
    array. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| - 实现一个二分查找算法，用于在排序数组中找到特定元素。 |'
- en: '| Dialog | - Act as the Norse Goddess Freyja. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 对话 | - 扮演北欧女神弗蕾雅。 |'
- en: '| - Can you think and feel like a human? |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| - 你能像人类一样思考和感受吗？ |'
- en: '| Judgement | - What if the Aztecs had successfully repelled the Spanish conquistadors?
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 判断 | - 如果阿兹特克人成功击退西班牙征服者会怎样？ |'
- en: '| - How can you determine if a person is genuinely interested in a conversation
    or simply being polite? |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| - 如何判断一个人是真正对对话感兴趣，还是只是出于礼貌？ |'
- en: '| Math | - Given that f(x) = 5$x^{3}$ - 2$x$ + 3, find the value of f(2). |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | - 给定 f(x) = 5$x^{3}$ - 2$x$ + 3，求 f(2) 的值。 |'
- en: '| - If the endpoints of a line segment are (2, -2) and (10, 4), what is the
    length of the segment? |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| - 如果线段的端点是(2, -2) 和 (10, 4)，那么该线段的长度是多少？ |'
- en: '| ODG | - Is there a meaning for Christmas wreaths? |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| ODG | - 圣诞花环有什么意义吗？ |'
- en: '| - What are some of the best universities for studying robotics? |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| - 学习机器人学的最佳大学有哪些？ |'
- en: '| ODS | - What causes the northern lights? |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ODS | - 北极光是由什么引起的？ |'
- en: '| - What do the different octane values of gasoline mean? |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| - 汽油的不同辛烷值意味着什么？ |'
- en: '| Writing | - Can you help me write a formal email to a potential business
    partner proposing a joint venture? |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 写作 | - 你能帮我写一封正式的电子邮件给潜在的商业合作伙伴，提议进行合资吗？ |'
- en: '| - Take MLK speech "I had a dream" but turn it into a top 100 rap song. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| - 把马丁·路德·金的演讲《我有一个梦想》改编成一首前100的嘻哈歌曲。 |'
- en: 'Table 2: Examined scenarios and corresponding selected examples.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：已检查的场景及对应的选定示例。
- en: Brainstorming
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 头脑风暴
- en: The brainstorming scenario is designed to test the LLMs’ ability to engage in
    problem-solving, creative ideation, and generation of insightful responses, especially
    in situations that require critical thinking and detailed, step-by-step reasoning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 头脑风暴场景旨在测试大语言模型（LLMs）在解决问题、创造性构思和生成有见地的回应方面的能力，尤其是在需要批判性思维和详细逐步推理的情境中。
- en: Coding
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 编程
- en: The code scenario evaluates LLMs’ ability to comprehend, produce, and debug
    code, as well as answering coding-related questions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 编程场景评估大语言模型（LLMs）理解、生成和调试代码的能力，以及回答与编程相关的问题。
- en: Dialog
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对话
- en: The dialog scenario measures LLMs’ ability to engage with users in a manner
    that is intuitive, human-like, and dynamic, testing their proficiency through
    context-sensitive conversations and role-playing that require maintaining a consistent
    persona throughout a series of interactions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对话场景衡量大语言模型（LLMs）与用户进行直观、类人且动态互动的能力，通过需要保持一致人格的上下文敏感对话和角色扮演测试其熟练度。
- en: Judgement
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 判断
- en: The judgement scenario assesses LLMs‘ ability to make inferences and formulate
    opinions, including soliciting insights on diverse situations or emotions, and
    posing questions that require logical thinking or reasoning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 判断场景评估大语言模型（LLMs）做出推理和形成意见的能力，包括就不同的情境或情感寻求见解，并提出需要逻辑思考或推理的问题。
- en: Math
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数学
- en: The math scenario evaluates the LLMs’ proficiency in understanding and solving
    mathematical problems, emphasizing their accuracy in tasks ranging from simple
    calculations to complex reasoning.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数学场景评估大语言模型（LLMs）在理解和解决数学问题方面的能力，强调其在从简单计算到复杂推理任务中的准确性。
- en: Open-Domain General (ODG)
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放领域通用（ODG）
- en: The ODG scenario measures LLMs’ proficiency in applying diverse knowledge and
    exercising reasoning across a wide array of topics, such as answering questions
    with definitive answers.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ODG场景衡量大语言模型（LLMs）应用广泛知识并进行推理的能力，涵盖诸如回答有明确答案的问题等任务。
- en: Open-Domain Science (ODS)
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 开放领域科学（ODS）
- en: The ODS scenario tests the LLMs’ application of scientific knowledge, and gauges
    their ability to accurately interpret and respond to queries related to scientific
    disciplines like biology, chemistry, physics, astronomy, and more.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ODS场景测试大语言模型（LLMs）应用科学知识的能力，评估其准确解读和回应涉及生物学、化学、物理学、天文学等学科的科学问题的能力。
- en: Writing
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 写作
- en: The writing scenario evaluates LLMs’ ability to summarize, translate, and generate
    various texts, testing their core language processing and production skills.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 写作场景评估大语言模型（LLMs）总结、翻译和生成各种文本的能力，测试其核心语言处理和生成技能。
- en: '6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate'
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '6 Exp-I: 多智能体辩论的元-元评估'
- en: In this section, we first perform meta-meta-evaluation, examining whether the
    meta-evaluation results of using ScaleEval match closely to those resulting from
    meta-evaluation using human evaluators.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先进行元-元评估，检查使用 ScaleEval 进行的元评估结果是否与使用人工评估者进行的元评估结果高度一致。
- en: Setup
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'For our ScaleEval meta-evaluation framework (as described in §[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1
    "4.1 Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate")), we deploy three LLM agents to perform multi-agent
    debate: gpt-4-turbo, claude-2, and gpt-3.5-turbo.¹¹1Results collected in December
    2023\. Specific models used are: gpt-4-1106-preview, claude-2, and gpt-3.5-turbo-1106.
    In our meta-evaluation experiment, we analyze a total of 160 prompts. This set
    is comprised 137 prompts from AlpacaEval Li et al. ([2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)),
    10 coding problem prompts from HumanEval Chen et al. ([2021a](https://arxiv.org/html/2401.16788v1#bib.bib5)),
    and 13 math problem prompts from GSM-Hard Gao et al. ([2022](https://arxiv.org/html/2401.16788v1#bib.bib11)).
    We categorize these prompts into four distinct scenarios: brainstorming, coding,
    math, and writing, where each scenario contains 40 prompts.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的 ScaleEval 元评估框架（如 §[4.1](https://arxiv.org/html/2401.16788v1#S4.SS1 "4.1
    Meta-Evaluation Framework via Multi-Agent Debate ‣ 4 Methodology ‣ Can Large Language
    Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators
    via Agent Debate") 所述），我们部署了三个 LLM 代理来进行多代理辩论：gpt-4-turbo、claude-2 和 gpt-3.5-turbo。¹¹12023年12月收集的结果。使用的具体模型包括：gpt-4-1106-preview、claude-2
    和 gpt-3.5-turbo-1106。在我们的元评估实验中，我们分析了共计160个提示。这些提示包括137个来自 AlpacaEval Li 等人（[2023c](https://arxiv.org/html/2401.16788v1#bib.bib19)），10个来自
    HumanEval Chen 等人（[2021a](https://arxiv.org/html/2401.16788v1#bib.bib5)）的编程问题提示，和13个来自
    GSM-Hard Gao 等人（[2022](https://arxiv.org/html/2401.16788v1#bib.bib11)）的数学问题提示。我们将这些提示分为四个不同的场景：头脑风暴、编程、数学和写作，每个场景包含40个提示。
- en: 'Each scenario is evaluated based on the following criteria, respectively: helpfulness,
    interpretability, reasoning, and creativity. We evaluate the generated responses
    from the following three LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro.
    We select the above LLMs to evaluate due to their rather similar performances
    according to past research and public user feedback, which can help us establish
    a more nuanced understanding of their performance in various real-world scenarios,
    and to identify specific contexts where one may outperform the others.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个场景的评估基于以下标准，分别是：有用性、可解释性、推理能力和创造力。我们评估来自以下三个大型语言模型（LLM）的生成回答：gpt-3.5-turbo、claude-instant
    和 gemini-pro。我们选择这三个 LLM 进行评估，是因为根据过去的研究和公开用户反馈，它们的表现非常相似，这有助于我们更细致地了解它们在各种实际场景中的表现，并识别出在特定上下文中某个模型可能优于其他模型的情况。
- en: Our meta-meta evaluation involves having human experts annotate which LLM submission
    they think is better based on a defined criterion during pairwise comparisons.
    A total of seven human experts were selected from a pool of Carnegie Mellon University
    students who have the relevant expertise in answering the queries in each scenario.
    Different groups of three human experts are responsible for answering the prompts
    in each scenario, where they are assigned to the scenario that relates to their
    expertise. Each expert received identical instructions for the task – they were
    asked to decide which submission is better based on our defined criteria, and
    for each comparison, label either 0 (neither submission is better), 1 (submission
    1 is better), or 2 (submission 2 is better). The label 2 corresponds to the label
    -1 as denoted in section [3.2](https://arxiv.org/html/2401.16788v1#S3.SS2 "3.2
    Automatic Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    The experts were tasked to conduct 30 comparisons for each of the four different
    scenarios (brainstorming, coding, math, and writing), based on their corresponding
    defined criteria (helpfulness, interpretability, reasoning, and creativity). This
    results in a total of 120 final judgements. The question prompts, LLM responses,
    and criteria utilized for human expert annotations were consistent with those
    used during our meta-evaluation experiment. All the details were presented in
    a google sheet that allowed experts to record their answers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的元-元评估包括让人类专家根据定义的标准在成对比较中标注他们认为更好的LLM提交。我们从卡内基梅隆大学的学生中选出了七位具有相关专业知识的专家，他们能回答每个场景中的查询。不同的三位人类专家组负责回答每个场景中的提示，他们被分配到与自己专长相关的场景。每位专家都收到了相同的任务说明——他们需要根据我们定义的标准判断哪个提交更好，对于每次比较，他们会标记为0（没有提交更好）、1（提交1更好）或2（提交2更好）。标签2对应于第[3.2](https://arxiv.org/html/2401.16788v1#S3.SS2
    "3.2 自动评估 ‣ 3 前提 ‣ 大型语言模型能否被信任进行评估？通过代理辩论对LLM作为评估者的可扩展元评估")节中标记的-1。专家们被要求在四个不同场景（头脑风暴、编程、数学和写作）中进行30次比较，根据相应的定义标准（有用性、可解释性、推理和创造力）。这将产生120个最终判断。问题提示、LLM响应和用于人类专家标注的标准与我们在元评估实验中使用的完全一致。所有细节都以Google表格的形式呈现，允许专家记录他们的答案。
- en: 'Q1: Can LLM agents with multi-agent debate be used as meta-evaluators in new
    user-defined scenarios?'
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Q1：多代理辩论的LLM代理能否在新的用户定义场景中作为元评估者使用？
- en: 'To validate the reliability of ScaleEval’s meta-evaluation framework, we perform
    comparisons between the results from human experts and ScaleEval’s multi-agent
    debate by two key metrics: the example-level agreement rate and the system-level
    agreement rate, as mentioned in §[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 Meta-Evaluation ‣ 3 Preliminaries ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate").
    The example-level agreement rate measures the proportion of instances where the
    multi-agent debate results correspond with the human experts judgements. On the
    other hand, the system-level agreement rate assesses whether the human experts
    and multi-agents concur in their overall evaluation of which LLMs produce the
    best responses for each scenario. A high agreement rate in both metrics would
    suggest a strong reliability and validity of our meta-evaluation framework, indicating
    that both human and LLM agents consistently recognize and agree on the quality
    of responses generated by LLMs.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证ScaleEval元评估框架的可靠性，我们通过两个关键指标——示例级一致性率和系统级一致性率——对比了人类专家的结果与ScaleEval多代理辩论的结果，如§[3.3](https://arxiv.org/html/2401.16788v1#S3.SS3
    "3.3 元评估 ‣ 3 前提 ‣ 大型语言模型能否被信任进行评估？通过代理辩论对LLM作为评估者的可扩展元评估")中所述。示例级一致性率衡量多代理辩论结果与人类专家判断一致的实例比例。另一方面，系统级一致性率评估人类专家与多代理在整体评估中是否一致，即哪种LLM在每个场景中生成最佳响应。如果这两个指标的高一致性率表明我们的元评估框架具有较强的可靠性和有效性，表明人类专家与LLM代理在识别和认同LLM生成的响应质量方面达成一致。
- en: Results
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'From Table [3](https://arxiv.org/html/2401.16788v1#S6.T3 "Table 3 ‣ Results
    ‣ 6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models
    be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via
    Agent Debate"), we generally observe a higher example-level agreement rate between
    human experts and ScaleEval, compared to the agreement rate between human experts
    and individual LLM evaluations. The consistently high agreement rates observed
    suggest that our meta-evaluation framework aligns well with human expert judgments
    in these areas, indicating a reliable performance of the collective use of LLMs
    in meta-evaluating complex scenarios. Across all LLM submission comparisons in
    our experiment, we observe higher agreement rates in decisions between ScaleEval
    outcomes and those of human experts, particularly in coding and math scenarios.
    This observed trend could be attributed to the inherently objective nature of
    these subjects, which have relatively clear, definitive answers unlike more subjective
    areas like creative writing.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '从表 [3](https://arxiv.org/html/2401.16788v1#S6.T3 "表 3 ‣ 结果 ‣ 6 Exp-I: 多代理辩论的元元评估
    ‣ 大型语言模型是否可以信任作为评估者？通过代理辩论的可扩展元评估")中，我们通常观察到在人类专家与ScaleEval之间的例子级一致性率较高，相较于人类专家与单个LLM评估之间的协议率。观察到的始终较高的一致性率表明，我们的元评估框架在这些领域与人类专家的判断高度一致，表明LLM集体使用在元评估复杂场景中的表现是可靠的。在我们的实验中，所有LLM提交的比较中，我们观察到ScaleEval的结果与人类专家的判断在决策上的一致性率较高，特别是在编程和数学场景中。这一观察到的趋势可能归因于这些学科本身的客观性质，它们有相对清晰、明确的答案，不像创造性写作等更具主观性的领域。'
- en: 'Based on Fig. [3](https://arxiv.org/html/2401.16788v1#S6.F3 "Figure 3 ‣ Results
    ‣ 6 Exp-I: Meta-Meta-Evaluation of Multi-Agent Debate ‣ Can Large Language Models
    be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via
    Agent Debate"), we notice a consistent "preference in the same direction" between
    human experts and multi-agent debates across all LLM pairwise comparisons and
    scenarios. Notably, gpt-3.5-turbo is favored (higher win rates) in brainstorming,
    math, and writing scenarios when compared with claude-instant. Similarly, gemini-pro
    is also preferred over claude-instant in all scenarios. When comparing gpt-3.5-turbo
    with gemini-pro, a varied pattern in decision outcomes is observed: both human
    experts and multi-agent systems agree that gpt-3.5-turbo outperforms gemini-pro
    in scenarios involving math and writing. Conversely, gemini-pro is deemed superior
    in brainstorming and coding scenarios. The high agreement of multi-agent preferences
    with human expert judgement results verifies the reliability of using multiple
    LLMs agents as meta-evaluators in various user-defined scenarios.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '基于图 [3](https://arxiv.org/html/2401.16788v1#S6.F3 "图 3 ‣ 结果 ‣ 6 Exp-I: 多代理辩论的元元评估
    ‣ 大型语言模型是否可以信任作为评估者？通过代理辩论的可扩展元评估")，我们注意到在人类专家和多代理辩论之间，在所有LLM的配对比较和场景中，始终存在一致的“同向偏好”。特别是，gpt-3.5-turbo在头脑风暴、数学和写作场景中，比claude-instant更受偏爱（更高的胜率）。同样，gemini-pro在所有场景中也优于claude-instant。将gpt-3.5-turbo与gemini-pro进行比较时，观察到决策结果的多样化模式：在人类专家和多代理系统的判断中，都认为gpt-3.5-turbo在涉及数学和写作的场景中优于gemini-pro。相反，gemini-pro在头脑风暴和编程场景中被认为更为出色。多代理偏好与人类专家判断结果的一致性验证了在各种用户定义的场景中，使用多个LLM代理作为元评估者的可靠性。'
- en: '| LLM Pairwise Comparisons | Criterion | Scenario | Meta-Evaluation | GPT-4-Turbo
    | Claude-2 | GPT-3.5-Turbo |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM配对比较 | 标准 | 场景 | 元评估 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| GPT-3.5-Turbo vs. Claude-Instant | Helpfulness | Brainstorming | 0.600 |
    0.633 | 0.433 | 0.267 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo 与 Claude-Instant | 有用性 | 头脑风暴 | 0.600 | 0.633 | 0.433 | 0.267
    |'
- en: '|  | Interpretability | Coding | 0.733 | 0.700 | 0.533 | 0.567 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.733 | 0.700 | 0.533 | 0.567 |'
- en: '|  | Reasoning | Math | 0.867 | 0.600 | 0.400 | 0.367 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.867 | 0.600 | 0.400 | 0.367 |'
- en: '|  | Creativity | Writing | 0.700 | 0.667 | 0.400 | 0.333 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.700 | 0.667 | 0.400 | 0.333 |'
- en: '| Claude-Instant vs. Gemini-Pro | Helpfulness | Brainstorming | 0.667 | 0.533
    | 0.467 | 0.500 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Claude-Instant 与 Gemini-Pro | 有用性 | 头脑风暴 | 0.667 | 0.533 | 0.467 | 0.500
    |'
- en: '|  | Interpretability | Coding | 0.833 | 0.600 | 0.500 | 0.567 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.833 | 0.600 | 0.500 | 0.567 |'
- en: '|  | Reasoning | Math | 0.767 | 0.667 | 0.330 | 0.367 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.767 | 0.667 | 0.330 | 0.367 |'
- en: '|  | Creativity | Writing | 0.733 | 0.633 | 0.400 | 0.500 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.733 | 0.633 | 0.400 | 0.500 |'
- en: '| GPT-3.5-Turbo vs. Gemini-Pro | Helpfulness | Brainstorming | 0.733 | 0.600
    | 0.467 | 0.467 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo 与 Gemini-Pro | 有用性 | 头脑风暴 | 0.733 | 0.600 | 0.467 | 0.467 |'
- en: '|  | Interpretability | Coding | 0.833 | 0.733 | 0.567 | 0.667 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编程 | 0.833 | 0.733 | 0.567 | 0.667 |'
- en: '|  | Reasoning | Math | 0.867 | 0.767 | 0.500 | 0.433 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.867 | 0.767 | 0.500 | 0.433 |'
- en: '|  | Creativity | Writing | 0.767 | 0.667 | 0.500 | 0.433 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.767 | 0.667 | 0.500 | 0.433 |'
- en: 'Table 3: Example-level agreement rate comparison between human expert and ScaleEval’s
    meta-evaluation vs. human expert and single LLM evaluation across four scenarios
    and criteria.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：人类专家与 ScaleEval 元评估之间的示例级别一致性比较，及人类专家与单一 LLM 评估在四个场景和标准下的比较。
- en: '![Refer to caption](img/d917865a84db39b38b00a0e85da6c462.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d917865a84db39b38b00a0e85da6c462.png)'
- en: (a) GPT-3.5-Turbo vs. Claude-Instant
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPT-3.5-Turbo 与 Claude-Instant
- en: '![Refer to caption](img/25d9b406729d7c32ca2dfd0ea7b15afc.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25d9b406729d7c32ca2dfd0ea7b15afc.png)'
- en: (b) Claude-Instant vs. Gemini-Pro
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Claude-Instant 与 Gemini-Pro
- en: '![Refer to caption](img/0b68ddd2906ef37fb2203868a1bc4548.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0b68ddd2906ef37fb2203868a1bc4548.png)'
- en: (c) GPT-3.5-Turbo vs. Gemini-Pro
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GPT-3.5-Turbo 与 Gemini-Pro
- en: 'Figure 3: System-level agreement – win rates for each LLM pairwise comparison.
    Left bars in each scenario represent human expert results; right bars represent
    ScaleEval’s meta-evaluation results.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：系统级别一致性 – 各 LLM 配对比较的胜率。每个场景中的左侧条形图代表人类专家的结果；右侧条形图代表 ScaleEval 的元评估结果。
- en: '![Refer to caption](img/491218262822d0859c53ca32eae59b62.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/491218262822d0859c53ca32eae59b62.png)'
- en: 'Figure 4: Human Fleiss Kappa for each LLM pairwise comparison under four scenarios.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：四个场景下每个 LLM 配对比较的人类 Fleiss Kappa 值。
- en: '7 Exp-II: Meta-Evaluation vs. LLM Evaluators'
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 Exp-II：元评估与 LLM 评估者
- en: Next, we use the fact that ScaleEval allows for reliable and scalable meta-evaluation
    to examine the traits of LLMs as evaluators.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们利用 ScaleEval 允许可靠且可扩展的元评估的事实，来考察 LLM 作为评估者的特征。
- en: 'Q2: What are the capabilities and limitations of each LLM evaluator?'
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题 2：每个 LLM 评估者的能力和局限性是什么？
- en: To effectively evaluate the performance of each LLM in its role as an evaluator,
    we adopt an approach that involves comparing the outcomes from our meta-evaluation
    process with the evaluations made independently by each LLM evaluator, which uncovers
    any disagreements or alignments between them. In the process, we aim to shed light
    on the performance characteristics of each LLM evaluator, which helps us identify
    which of them demonstrate superior evaluative abilities, thereby contributing
    to our understanding of their reliability in evaluating responses under each scenario.
    In addition, we provide a comprehensive cost-performance analysis to decide which
    LLM evaluator is the most suitable choice in each scenario.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效评估每个 LLM 作为评估者的表现，我们采用一种方法，通过将元评估过程的结果与每个 LLM 评估者独立进行的评估结果进行比较，从而揭示它们之间的分歧或一致性。在此过程中，我们旨在揭示每个
    LLM 评估者的表现特征，帮助我们识别哪些评估者展现出更强的评估能力，从而有助于我们了解它们在每个场景下评估回应的可靠性。此外，我们还提供了全面的性价比分析，以决定在每个场景下哪个
    LLM 评估者最为合适。
- en: Setup
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'For meta-evaluation, we employed three LLMs (gpt-4-turbo, claude-2, and gpt-3.5-turbo)
    as evaluators to perform pairwise comparisons of responses from three distinct
    LLMs: gpt-3.5-turbo, claude-instant, and gemini-pro. Previous studies have highlighted
    the presence of positional biases when LLMs are used as evaluators Wang et al.
    ([2023b](https://arxiv.org/html/2401.16788v1#bib.bib24)). In response to these
    findings, we have implemented a strategy of randomization to mitigate such biases.
    Specifically, the sequence in which submissions from LLMs are presented to the
    agent evaluators is randomized. Additionally, we also randomize the order of discussions
    for each agent evaluator in every case. These approaches ensure that the process
    is fair and unbiased as much as possible, allowing for a more accurate assessment
    of the LLM evaluators’ performance. The meta-evaluations were done under the following
    8 scenarios: brainstorming, coding, dialog, judgement, open-domain general, open-domain
    science, and writing, with the same set of 4 criteria used during human expert
    annotation.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在元评估中，我们使用了三种LLM（gpt-4-turbo、claude-2和gpt-3.5-turbo）作为评估者，执行了三种不同LLM（gpt-3.5-turbo、claude-instant和gemini-pro）响应的成对比较。之前的研究突出了当LLM作为评估者时存在的定位偏差（Wang等，([2023b](https://arxiv.org/html/2401.16788v1#bib.bib24))）。为了应对这些发现，我们实施了一种随机化策略，以减少这种偏差。具体来说，LLM的提交顺序在呈现给评估者时是随机化的。此外，我们还随机化了每个评估者在每个案例中的讨论顺序。这些方法确保了过程尽可能公平且无偏，从而使LLM评估者的表现能够得到更准确的评估。元评估是在以下8个场景下进行的：头脑风暴、编程、对话、判断、开放领域常识、开放领域科学和写作，使用与人类专家标注时相同的4个标准。
- en: Results
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Table [4](https://arxiv.org/html/2401.16788v1#S7.T4 "Table 4 ‣ Results ‣ 7
    Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")
    compares the agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator
    across criteria and scenarios. We observe that gpt-4-turbo, when serving as an
    evaluator, has the highest agreement rates with our meta-evaluation, particularly
    in the scenarios of brainstorming, dialog, and ODG with the helpfulness criterion.
    It stands out with the highest overall average score of 0.780\. However, our selected
    open-source model evaluator, auto-j, outperforms gpt-4-turbo in evaluating coding
    questions based on the helpfulness criterion. In addition, it exhibits the highest
    agreement rate with our meta-evaluation in the judgement scenario, according to
    the helpfulness criterion, indicating it as the most capable evaluator in this
    setting. It also achieves comparable results with other closed-source models like
    claude-2 and gpt-3.5-turbo in most of the other scenarios.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '表格[4](https://arxiv.org/html/2401.16788v1#S7.T4 "Table 4 ‣ Results ‣ 7 Exp-II:
    Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be Trusted for
    Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")比较了ScaleEval的元评估与每个LLM评估者在各个标准和场景中的一致性率。我们观察到，当gpt-4-turbo作为评估者时，它与我们的元评估具有最高的一致性率，特别是在头脑风暴、对话和ODG场景中，有用性标准下表现尤为突出。它以0.780的最高整体平均分脱颖而出。然而，我们选择的开源模型评估者auto-j在根据有用性标准评估编程问题时超越了gpt-4-turbo。此外，根据有用性标准，它在判断场景中的一致性率最高，表明它在这种设置下是最有能力的评估者。它在大多数其他场景中也与claude-2和gpt-3.5-turbo等其他闭源模型取得了相当的结果。'
- en: While gpt-4-turbo performs the best as an evaluator in a majority of scenarios,
    it is not necessarily the best choice when we take into consideration its relatively
    high API costs. In fact, both the more affordable version (gpt-3.5-turbo) and
    our selected free, open-source model (auto-j) show comparable performance in scenarios
    like judgement and writing. For coding-related evaluations, the slightly less
    expensive claude-2 could be a more cost-effective alternative to gpt-4-turbo.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然gpt-4-turbo在大多数场景中作为评估者表现最好，但当考虑到其相对较高的API费用时，它不一定是最佳选择。事实上，价格更实惠的版本（gpt-3.5-turbo）和我们选择的免费开源模型（auto-j）在判断和写作等场景中表现相当。对于与编程相关的评估，稍微便宜的claude-2可能是gpt-4-turbo更具成本效益的替代选择。
- en: '| Criterion | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 场景 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo | Auto-J |'
- en: '| Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 | 0.575 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 有用性 | 头脑风暴 | 0.800 | 0.500 | 0.650 | 0.575 |'
- en: '|  | Coding | 0.600 | 0.725 | 0.675 | 0.675 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 编程 | 0.600 | 0.725 | 0.675 | 0.675 |'
- en: '|  | Dialog | 0.800 | 0.700 | 0.700 | 0.625 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话 | 0.800 | 0.700 | 0.700 | 0.625 |'
- en: '|  | Judgement | 0.725 | 0.625 | 0.725 | 0.750 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 判断 | 0.725 | 0.625 | 0.725 | 0.750 |'
- en: '|  | Math | 0.825 | 0.650 | 0.600 | 0.350 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 数学 | 0.825 | 0.650 | 0.600 | 0.350 |'
- en: '|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | ODG | 0.850 | 0.525 | 0.575 | 0.700 |'
- en: '|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | ODS | 0.875 | 0.525 | 0.575 | 0.675 |'
- en: '|  | Writing | 0.750 | 0.600 | 0.750 | 0.600 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 写作 | 0.750 | 0.600 | 0.750 | 0.600 |'
- en: '| Interpretability | Coding | 0.825 | 0.600 | 0.550 | 0.525 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: 可解释性 | 编码 | 0.825 | 0.600 | 0.550 | 0.525 |
- en: '| Reasoning | Math | 0.650 | 0.525 | 0.475 | 0.450 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 数学 | 0.650 | 0.525 | 0.475 | 0.450 |'
- en: '|  | Judgement | 0.750 | 0.650 | 0.700 | 0.675 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 判断 | 0.750 | 0.650 | 0.700 | 0.675 |'
- en: '| Creativity | Writing | 0.775 | 0.600 | 0.575 | 0.650 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 创造力 | 写作 | 0.775 | 0.600 | 0.575 | 0.650 |'
- en: '|  | Brainstorming | 0.800 | 0.525 | 0.550 | 0.625 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 头脑风暴 | 0.800 | 0.525 | 0.550 | 0.625 |'
- en: '|  | Dialog | 0.875 | 0.750 | 0.700 | 0.800 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 对话 | 0.875 | 0.750 | 0.700 | 0.800 |'
- en: '| Average | Overall | 0.780 | 0.607 | 0.629 | 0.619 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 总体 | 0.780 | 0.607 | 0.629 | 0.619 |'
- en: 'Table 4: Agreement rate between ScaleEval’s meta-evaluation and each LLM evaluator
    for comparing GPT3.5-Turbo vs. Claude-Instant.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：ScaleEval的元评估与各LLM评估者的协议率，比较GPT3.5-Turbo与Claude-Instant。
- en: '| Criteria Format | Criteria | Scenario | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 标准格式 | 标准 | 场景 | GPT-4-Turbo | Claude-2 | GPT-3.5-Turbo |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| General | Helpfulness | Brainstorming | 0.800 | 0.500 | 0.650 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 一般 | 有用性 | 头脑风暴 | 0.800 | 0.500 | 0.650 |'
- en: '|  | Interpretability | Coding | 0.825 | 0.600 | 0.550 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.825 | 0.600 | 0.550 |'
- en: '|  | Reasoning | Math | 0.650 | 0.525 | 0.475 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.650 | 0.525 | 0.475 |'
- en: '|  | Creativity | Writing | 0.800 | 0.600 | 0.575 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.800 | 0.600 | 0.575 |'
- en: '| Shortened | Helpfulness | Brainstorming | 0.675 | 0.500 | 0.575 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 缩短 | 有用性 | 头脑风暴 | 0.675 | 0.500 | 0.575 |'
- en: '|  | Interpretability | Coding | 0.675 | 0.325 | 0.425 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.675 | 0.325 | 0.425 |'
- en: '|  | Reasoning | Math | 0.625 | 0.425 | 0.400 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.625 | 0.425 | 0.400 |'
- en: '|  | Creativity | Writing | 0.675 | 0.250 | 0.525 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.675 | 0.250 | 0.525 |'
- en: '| Gibberish | Helpfulness | Brainstorming | 0.575 | 0.450 | 0.575 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 胡言乱语 | 有用性 | 头脑风暴 | 0.575 | 0.450 | 0.575 |'
- en: '|  | Interpretability | Coding | 0.700 | 0.275 | 0.525 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.700 | 0.275 | 0.525 |'
- en: '|  | Reasoning | Math | 0.650 | 0.200 | 0.400 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.650 | 0.200 | 0.400 |'
- en: '|  | Creativity | Writing | 0.550 | 0.150 | 0.450 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.550 | 0.150 | 0.450 |'
- en: '| Shuffled | Helpfulness | Brainstorming | 0.625 | 0.550 | 0.500 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 洗牌 | 有用性 | 头脑风暴 | 0.625 | 0.550 | 0.500 |'
- en: '|  | Interpretability | Coding | 0.600 | 0.400 | 0.525 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.600 | 0.400 | 0.525 |'
- en: '|  | Reasoning | Math | 0.625 | 0.225 | 0.600 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.625 | 0.225 | 0.600 |'
- en: '|  | Creativity | Writing | 0.625 | 0.275 | 0.500 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.625 | 0.275 | 0.500 |'
- en: '| Flipped | Helpfulness | Brainstorming | 0.725 | 0.325 | 0.550 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 翻转 | 有用性 | 头脑风暴 | 0.725 | 0.325 | 0.550 |'
- en: '|  | Interpretability | Coding | 0.725 | 0.425 | 0.300 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.725 | 0.425 | 0.300 |'
- en: '|  | Reasoning | Math | 0.575 | 0.250 | 0.500 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.575 | 0.250 | 0.500 |'
- en: '|  | Creativity | Writing | 0.750 | 0.075 | 0.550 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.750 | 0.075 | 0.550 |'
- en: '| Masked | Helpfulness | Brainstorming | 0.725 | 0.300 | 0.500 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 掩码 | 有用性 | 头脑风暴 | 0.725 | 0.300 | 0.500 |'
- en: '|  | Interpretability | Coding | 0.650 | 0.225 | 0.475 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解释性 | 编码 | 0.650 | 0.225 | 0.475 |'
- en: '|  | Reasoning | Math | 0.575 | 0.150 | 0.375 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | 推理 | 数学 | 0.575 | 0.150 | 0.375 |'
- en: '|  | Creativity | Writing | 0.575 | 0.200 | 0.400 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | 创造力 | 写作 | 0.575 | 0.200 | 0.400 |'
- en: 'Table 5: Agreement rate between ScaleEval’s meta-evaluation results and each
    LLM evaluator under various criteria prompt formats and scenarios comparing GPT3.5-Turbo
    vs. Claude-Instant.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ScaleEval的元评估结果与各LLM评估者在不同标准提示格式和场景下的协议率，比较GPT3.5-Turbo与Claude-Instant。
- en: '8 Exp-III: Meta-Evaluation with Criteria Prompt Format Variations'
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '8 Exp-III: 带有标准提示格式变化的元评估'
- en: 'Q3: How do the qualities of criteria prompts influence the robustness of LLMs
    as evaluators in different scenarios?'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 'Q3: 标准提示的特质如何影响LLM作为评估者在不同场景下的鲁棒性？'
- en: Prior studies have revealed that variations in prompts can substantially affect
    the behavior of LLMs, particularly with the text they generate. With this in mind,
    we define various formatted criteria for evaluating LLM responses under each scenario.
    This approach aims to examine the extent to which different formats of criteria
    prompts influence both the performance and robustness of LLMs as evaluators.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，提示词的变化会显著影响大型语言模型（LLMs）的行为，特别是在它们生成的文本方面。鉴于此，我们定义了不同的格式化标准，用于在各种场景下评估LLM的响应。该方法旨在检验不同格式的标准提示词在多大程度上影响LLM作为评估者的表现和鲁棒性。
- en: Setup
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'We define five variations of the same criteria prompts: shortened, gibberish,
    shuffled, flipped, and masked (see Table [7](https://arxiv.org/html/2401.16788v1#A1.T7
    "Table 7 ‣ Appendix A Meta-Evaluation Prompt ‣ Can Large Language Models be Trusted
    for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate")
    under Appendix [A](https://arxiv.org/html/2401.16788v1#A1 "Appendix A Meta-Evaluation
    Prompt ‣ Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation
    of LLMs as Evaluators via Agent Debate") for detailed format). With these criteria
    format variations, we intend to observe how the LLMs as evaluators would respond
    differently when conducting evaluation. We compare the example-level agreement
    rate between ScaleEval’s meta-evaluation results and each LLM evaluator.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了同一标准提示的五种变体：缩短、胡言乱语、打乱、翻转和掩蔽（详细格式见附录[A](https://arxiv.org/html/2401.16788v1#A1
    "附录A 元评估提示 ‣ 大型语言模型能否信任用于评估？通过代理辩论进行LLM作为评估者的可扩展元评估")中的表[7](https://arxiv.org/html/2401.16788v1#A1.T7
    "表7 ‣ 附录A 元评估提示 ‣ 大型语言模型能否信任用于评估？通过代理辩论进行LLM作为评估者的可扩展元评估")）。通过这些标准格式变体，我们旨在观察LLM作为评估者在进行评估时如何作出不同的反应。我们比较了ScaleEval的元评估结果与每个LLM评估者之间的示例级别一致性率。
- en: Results
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果
- en: 'Based on Table [5](https://arxiv.org/html/2401.16788v1#S7.T5 "Table 5 ‣ Results
    ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large Language Models be
    Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent
    Debate"), we observe that the performance of LLMs as evaluators generally deteriorates
    when certain letters in the criteria prompts are masked. Furthermore, the removal
    of guiding phrases at the beginning, such as "Not Helpful" or "Highly Helpful",
    can also diminish their effectiveness as evaluators. Both gpt-4-turbo and gpt-3.5-turbo
    demonstrate some resilience to these adversarially formatted criteria prompts,
    maintaining a relatively consistent agreement rates across various criteria formats.
    In contrast, Claude-2 often showcases confusion and refuses to evaluate, particularly
    in cases with gibberish and masked criteria prompts, where it rejects answering
    about half of the questions. It typically responds with statements like, "Unfortunately
    I do not have enough information here to provide a fair evaluation… The criteria
    describe different quality levels, but there is no detail on what specific aspects
    of the responses should be assessed… any judgement risks being arbitrary or biased…".
    None of the LLMs as evaluators we tested maintained very similar evaluation capabilities
    when faced with these adversarially formatted criteria prompts, indicating a limitation
    in these LLMs as evaluators’ current design and application. Despite their advanced
    capabilities in fulfilling a variety of tasks, they may still struggle with understanding
    and responding accurately to substituted criteria information, highlighting an
    area for potential improvement in future iterations of LLM technology. Among all
    the different formatted criteria, we highlight the cases where the LLMs perform
    the best as evaluators in Table [5](https://arxiv.org/html/2401.16788v1#S7.T5
    "Table 5 ‣ Results ‣ 7 Exp-II: Meta-Evaluation vs. LLM Evaluators ‣ Can Large
    Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as
    Evaluators via Agent Debate").'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 根据表[5](https://arxiv.org/html/2401.16788v1#S7.T5 "表5 ‣ 结果 ‣ 7 Exp-II：元评估与LLM评估者
    ‣ 大型语言模型能否信任用于评估？通过代理辩论进行LLM作为评估者的可扩展元评估")，我们观察到，当标准提示中的某些字母被掩蔽时，LLM作为评估者的表现通常会下降。此外，移除开头的引导短语，如“无帮助”或“高度有帮助”，也可能降低其作为评估者的有效性。gpt-4-turbo和gpt-3.5-turbo在这些对抗性格式的标准提示中表现出一定的抗压能力，在各种标准格式下维持了相对一致的一致性率。相比之下，Claude-2经常表现出困惑并拒绝评估，特别是在遇到胡言乱语和掩蔽标准提示的情况下，它拒绝回答大约一半的问题。它通常回应类似这样的陈述：“不幸的是，我在这里没有足够的信息来提供公正的评估……标准描述了不同的质量级别，但没有具体说明应评估哪些响应的方面……任何判断都有可能是任意或有偏见的……”我们测试的所有LLM作为评估者在面对这些对抗性格式的标准提示时都没有保持非常相似的评估能力，这表明这些LLM作为评估者的当前设计和应用存在局限性。尽管它们在完成各种任务方面具有先进的能力，但它们在理解和准确响应替代标准信息时仍然可能存在困难，这突显了LLM技术未来版本改进的潜力。在所有不同格式的标准中，我们在表[5](https://arxiv.org/html/2401.16788v1#S7.T5
    "表5 ‣ 结果 ‣ 7 Exp-II：元评估与LLM评估者 ‣ 大型语言模型能否信任用于评估？通过代理辩论进行LLM作为评估者的可扩展元评估")中重点展示了LLM作为评估者表现最好的情况。
- en: 9 Conclusion
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 结论
- en: In this work, we propose ScaleEval, a scalable, agent-debate assisted meta-evaluation
    framework for assessing the reliability and robustness of LLMs as evaluators.
    This approach addresses the expensive and time-intensive challenges inherent in
    traditional meta-evaluation methods, particularly pertinent as the usage of LLMs
    expands, necessitating a more scalable solution. Through our research, we have
    not only demonstrated the reliability of our proposed meta-evaluation framework,
    but also shed light on the capabilities and limitations of LLMs as evaluators
    in various scenarios. We observe how the results from these LLMs as evaluators
    vary based on modifications to the same criteria prompts. By open-sourcing our
    framework, we aim to foster further research in this field and encourage the development
    of more advanced and reliable LLMs as evaluators in the future.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了 ScaleEval，一个可扩展的、由智能体辩论辅助的元评估框架，用于评估 LLM 作为评估器的可靠性和稳健性。该方法解决了传统元评估方法中固有的高成本和时间密集型挑战，特别是在
    LLM 使用日益增加的背景下，迫切需要一种更具可扩展性的解决方案。通过我们的研究，我们不仅展示了我们提出的元评估框架的可靠性，还揭示了 LLM 作为评估器在不同场景下的能力与局限性。我们观察到，基于相同的标准提示，LLM
    作为评估器的结果会有所不同。通过开源我们的框架，我们旨在促进该领域的进一步研究，并鼓励未来开发更先进、可靠的 LLM 评估器。
- en: Acknowledgements
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank Chunting Zhou, Weizhe Yuan, Chunpu Xu, Yan Ma, and Binjie Wang for
    the helpful discussions and feedback.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢周春亭、袁伟哲、徐春浦、马燕和王宾杰的有益讨论和反馈。
- en: References
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bang et al. (2023) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai,
    Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V.
    Do, Yan Xu, and Pascale. Fung. 2023. A multitask, multilingual, multimodal evaluation
    of chatgpt on reasoning, hallucination, and interactivity. *arXiv:2302.04023v3*.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bang 等人（2023）Yejin Bang、Samuel Cahyawijaya、Nayeon Lee、Wenliang Dai、Dan Su、Bryan
    Wilie、Holy Lovenia、Ziwei Ji、Tiezheng Yu、Willy Chung、Quyet V. Do、Yan Xu 和 Pascale
    Fung。2023年。针对推理、幻觉和互动性的多任务、多语言、多模态 ChatGPT 评估。*arXiv:2302.04023v3*。
- en: Bhandari et al. (2020) Manik Bhandari, Pranav Gour, Atabak Ashfaq, Pengfei Liu,
    and Graham Neubig. 2020. Re-evaluating evaluation in text summarization. *arXiv
    preprint arXiv:2010.07100*.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhandari 等人（2020）Manik Bhandari、Pranav Gour、Atabak Ashfaq、Pengfei Liu 和 Graham
    Neubig。2020年。重新评估文本摘要中的评估方法。*arXiv 预印本 arXiv:2010.07100*。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人（2023）Sébastien Bubeck、Varun Chandrasekaran、Ronen Eldan、Johannes Gehrke、Eric
    Horvitz、Ece Kamar、Peter Lee、Yin Tat Lee、Yuanzhi Li、Scott Lundberg 等人。2023年。人工通用智能的火花：与
    GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*。
- en: 'Chan et al. (2023) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. Chateval: Towards better llm-based
    evaluators through multi-agent debate. *arXiv preprint arXiv:2308.07201*.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等人（2023）Chi-Min Chan、Weize Chen、Yusheng Su、Jianxuan Yu、Wei Xue、Shanghang
    Zhang、Jie Fu 和 Zhiyuan Liu。2023年。Chateval：通过多智能体辩论推动更好的基于 LLM 的评估器。*arXiv 预印本
    arXiv:2308.07201*。
- en: Chen et al. (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021a. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2021a）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, 和Wojciech
    Zaremba. 2021a. [评估基于代码训练的大型语言模型](http://arxiv.org/abs/2107.03374)。
- en: Chen et al. (2021b) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021b. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2021b）Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman等. 2021b. 评估基于代码训练的大型语言模型。*arXiv预印本 arXiv:2107.03374*。
- en: Chiang and Lee (2023) Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language
    models be an alternative to human evaluations? *arXiv preprint arXiv:2305.01937*.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang和Lee（2023）Cheng-Han Chiang和Hung-yi Lee. 2023. 大型语言模型能否成为人类评估的替代方案？*arXiv预印本
    arXiv:2305.01937*。
- en: Dang et al. (2008) Hoa Trang Dang, Karolina Owczarzak, et al. 2008. Overview
    of the tac 2008 update summarization task. In *TAC*.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dang等人（2008）Hoa Trang Dang, Karolina Owczarzak等. 2008. TAC 2008更新摘要任务概述。在*TAC*中。
- en: 'Freitag et al. (2022) Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo,
    Craig Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, and
    André FT Martins. 2022. Results of wmt22 metrics shared task: Stop using bleu–neural
    metrics are better and more robust. In *Proceedings of the Seventh Conference
    on Machine Translation (WMT)*, pages 46–68.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Freitag等人（2022）Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig
    Stewart, Eleftherios Avramidis, Tom Kocmi, George Foster, Alon Lavie, 和André FT
    Martins. 2022. wmt22指标共享任务结果：停止使用bleu–神经网络指标更好且更具鲁棒性。在*第七届机器翻译会议（WMT）论文集*中，页码46–68。
- en: 'Fu et al. (2023) Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu.
    2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu等人（2023）Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, 和Pengfei Liu. 2023. Gptscore：根据需求进行评估。*arXiv预印本
    arXiv:2302.04166*。
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2022. Pal: Program-aided language
    models. *arXiv preprint arXiv:2211.10435*.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人（2022）Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming
    Yang, Jamie Callan, 和Graham Neubig. 2022. Pal：程序辅助语言模型。*arXiv预印本 arXiv:2211.10435*。
- en: 'Gemini Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui
    Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M
    Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemini团队等人（2023）Gemini团队, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth等.
    2023. Gemini：一系列高性能多模态模型。*arXiv预印本 arXiv:2312.11805*。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2020）Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和Jacob Steinhardt. 2020. 测量大规模多任务语言理解。*arXiv预印本 arXiv:2009.03300*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2021）丹·亨德里克斯、科林·伯恩斯、萨乌拉夫·卡达瓦斯、阿库尔·阿罗拉、史蒂文·巴萨特、埃里克·唐、道恩·宋、雅各布·斯坦哈特。2021.
    使用数学数据集衡量数学问题解决能力。*arXiv预印本 arXiv:2103.03874*。
- en: 'Hu et al. (2020) Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig,
    Orhan Firat, and Melvin. Johnson. 2020. Xtreme: A massively multilingual multi-task
    benchmark for evaluating cross-lingual generalization. *arXiv:2003.11080v5*.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人（2020）胡俊杰、塞巴斯蒂安·鲁德尔、阿迪蒂亚·西丹特、格雷厄姆·纽比格、奥尔罕·菲拉特、梅尔文·约翰逊。2020. Xtreme：用于评估跨语言泛化的多语言多任务基准。*arXiv:2003.11080v5*。
- en: Huang et al. (2023) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, and Denny Zhou. 2023. Large language models cannot
    self-correct reasoning yet. *arXiv preprint arXiv:2310.01798*.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人（2023）黄杰、陈欣云、斯瓦罗普·米什拉、郑华秀·史蒂文、余伟、宋心颖、周登妮。2023. 大型语言模型尚不能自我纠正推理。*arXiv预印本
    arXiv:2310.01798*。
- en: Li et al. (2023a) Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao,
    and Pengfei Liu. 2023a. Generative judge for evaluating alignment. *arXiv preprint
    arXiv:2310.05470*.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2023a）李俊龙、孙世超、袁伟哲、范润泽、赵海、刘鹏飞。2023a. 生成式裁判用于评估对齐度。*arXiv预印本 arXiv:2310.05470*。
- en: 'Li et al. (2023b) Ruosen Li, Teerth Patel, and Xinya Du. 2023b. Prd: Peer rank
    and discussion improve large language model based evaluations. *arXiv preprint
    arXiv:2307.02762*.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2023b）李若森、特尔斯·帕特尔、杜新雅。2023b. Prd：同行排名和讨论改善基于大型语言模型的评估。*arXiv预印本 arXiv:2307.02762*。
- en: 'Li et al. (2023c) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023c. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人（2023c）李雪晨、张天逸、扬·杜布瓦、罗汉·塔奥里、伊沙恩·古尔贾尼、卡洛斯·格斯特林、梁鹏、桥本辰纪。2023c. Alpacaeval：一种自动评估指令跟随模型的工具。
    [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)。
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梁等人（2022）梁鹏、瑞希·博马萨尼、托尼·李、迪米特里斯·齐普拉斯、迪拉拉·索伊卢、安田道宏、张奕安、迪帕克·纳拉扬、吴雨淮、安纳亚·库马尔等。2022.
    语言模型的整体评估。*arXiv预印本 arXiv:2211.09110*。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人（2022）欧阳龙、杰弗里·吴、江旭、迪奥戈·阿尔梅达、卡罗尔·韦恩赖特、帕梅拉·米什金、张冲、桑迪尼·阿贾瓦尔、卡塔里娜·斯拉马、亚历克斯·雷等。2022.
    训练语言模型遵循指令并通过人类反馈改进。*神经信息处理系统进展*，35:27730–27744。
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯里瓦斯塔瓦等人（2022）阿罗希·斯里瓦斯塔瓦、阿比纳夫·拉斯托吉、阿比谢克·拉奥、阿布·阿瓦尔·穆罕默德·肖比、阿布巴卡尔·阿比德、亚当·费什、亚当·R·布朗、亚当·圣托罗、阿迪提亚·古普塔、阿德里亚·加里加-阿隆索等。2022.
    超越模仿游戏：量化和推断语言模型的能力。*arXiv预印本 arXiv:2206.04615*。
- en: Wang et al. (2023a) Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang Shi, Zhixu
    Li, Jinan Xu, Jianfeng Qu, and Jie Zhou. 2023a. Is chatgpt a good nlg evaluator?
    a preliminary study. *arXiv preprint arXiv:2303.04048*.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023a）王佳安、梁云龙、孟凡东、史浩翔、李志旭、徐济南、屈建锋、周杰。2023a. ChatGPT是一个好的NLG评估者吗？一项初步研究。*arXiv预印本
    arXiv:2303.04048*。
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin,
    Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. Large language models are
    not fair evaluators. *ArXiv*, abs/2305.17926.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023b）王培怡、李磊、陈亮、朱大伟、林冰怀、曹云波、刘琪、刘天宇、隋志方。2023b. 大型语言模型不是公平的评估者。*ArXiv*，abs/2305.17926。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint
    arXiv:2306.05685*.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人（2023）郑连敏、蒋维麟、盛颖、庄思远、吴张浩、庄永豪、林子、李卓涵、李大成、邢艾瑞等。2023. 使用mt-bench和聊天机器人竞技场评判大型语言模型作为裁判。*arXiv预印本
    arXiv:2306.05685*。
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan. Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv:2304.06364v2*.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan. Duan. 2023. Agieval: 一个人本位的基准，用于评估基础模型。*arXiv:2304.06364v2*。'
- en: Appendix A Meta-Evaluation Prompt
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 元评估提示
- en: '| <Initial Evaluation> |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| <初始评估> |'
- en: '| Compare the two submissions based on the criteria above. Which one is better?
    First, provide a step-by-step explanation of your evaluation reasoning according
    to the criteria. Avoid any potential bias. Ensure that the order in which the
    submissions were presented does not affect your judgement. Keep your explanation
    strictly under 150 words. Afterwards, choose one of the following options: |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 根据上面的标准比较两个提交内容，哪个更好？首先，按步骤解释你的评估推理，避免任何潜在偏见。确保提交内容的呈现顺序不会影响你的判断。保持解释严格不超过150字。然后，选择以下选项之一：
    |'
- en: '| Submission 1 is better: "1" |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 提交 1 更好: "1" |'
- en: '| Submission 2 is better: "2" |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 提交 2 更好: "2" |'
- en: '| Neither is better: "0" |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 都不更好: "0" |'
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 直接输入 "1" 或 "2" 或 "0"（不带引号或标点符号），对应你的推理。最后，再次单独输入该数字在新的一行。 |'
- en: '| [Question]: {question} |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| [问题]: {question} |'
- en: '| [Submission 1]: {submission_1} |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| [提交 1]: {submission_1} |'
- en: '| [Submission 2]: {submission_2} |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| [提交 2]: {submission_2} |'
- en: '| [Criteria]: {criteria} |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| [标准]: {criteria} |'
- en: '| [User]: {user_prompt} |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| [用户]: {user_prompt} |'
- en: '| You are evaluating two submissions for a particular question, using a specific
    set of criteria. Above is the data. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 你正在评估两个提交内容，针对一个特定的问题，使用一套具体的标准。上面是相关数据。 |'
- en: '| <Discussion Rounds> |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| <讨论回合> |'
- en: '| Always remember you are Speaker 1/2/3. Review again your own previous evaluations/discussions
    first, then answer user’s request from Speaker 1/2/3’s perspective. |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 请始终记住你是发言者 1/2/3。首先回顾你自己之前的评估/讨论，然后从发言者 1/2/3 的角度回答用户的请求。 |'
- en: '| [Question]: {question} |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| [问题]: {question} |'
- en: '| [Submission 1]: {submission_1} |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| [提交 1]: {submission_1} |'
- en: '| [Submission 2]: {submission_2} |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| [提交 2]: {submission_2} |'
- en: '| [Criteria]: {criteria} |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| [标准]: {criteria} |'
- en: '| [Speaker 1’s Initial Evaluation]: {evaluation_1} |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| [发言者 1 的初始评估]: {evaluation_1} |'
- en: '| [Speaker 2’s Initial Evaluation]: {evaluation_2} |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| [发言者 2 的初始评估]: {evaluation_2} |'
- en: '| [Speaker 3’s Initial Evaluation]: {evaluation_3} |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| [发言者 3 的初始评估]: {evaluation_3} |'
- en: '| [Speaker {speaker_number}’s Discussion -- Round {round_number}]: {discussion_reasoning}
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| [发言者 {speaker_number} 的讨论 -- 回合 {round_number}]: {discussion_reasoning} |'
- en: '| ... |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| ... |'
- en: '| Read the question, submissions, criteria, and evaluations above. First, explain
    your thoughts step-by-step about other speakers’ evaluations. Second, explain
    your reasoning step-by-step regarding whether or not to change your original answer
    about which submission you think is better after considering other speakers’ perspectives.
    Keep your reasoning strictly under 150 words. Afterwards, choose one of the following
    options: |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 阅读上面的题目、提交内容、标准和评估。首先，逐步解释你对其他发言者评估的想法。其次，逐步解释你在考虑了其他发言者的观点后，是否会改变你原先对哪个提交更好的看法。保持推理严格不超过150字。然后，选择以下选项之一：
    |'
- en: '| Submission 1 is better: "1" |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 提交 1 更好: "1" |'
- en: '| Submission 2 is better: "2" |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 提交 2 更好: "2" |'
- en: '| Neither is better: "0" |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 都不更好: "0" |'
- en: '| Directly type in "1" or "2" or "0" (without quotes or punctuation) that corresponds
    to your reasoning. At the end, repeat just the number again by itself on a new
    line. |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 直接输入 "1" 或 "2" 或 "0"（不带引号或标点符号），对应你的推理。最后，再次单独输入该数字在新的一行。 |'
- en: 'Table 6: Prompt template for meta-evaluation via multi-agent debate'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 6：通过多方辩论进行元评估的提示模板
- en: '| <Type 1: General Format Version> |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| <类型 1: 一般格式版本> |'
- en: '| "1": "Not Helpful - The response is completely unrelated, lacks coherence,
    and fails to provide any meaningful information." |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| "1": "无帮助 - 该回应完全不相关，缺乏连贯性，未能提供任何有意义的信息。" |'
- en: '| "2": "Somewhat Helpful - The response bears some relevance but remains largely
    superficial and unclear, addressing only the peripheral aspects of the user’s
    needs." |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| "2": "有些帮助 - 回应在某种程度上相关，但仍然相当表面和模糊，仅涉及用户需求的边缘部分。" |'
- en: '| "3": "Moderately Helpful - The response is mostly relevant and clear, covering
    the basic aspects of the query, but lacks depth and comprehensive elucidation."
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| "3": "大致有帮助 - 回应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐释。" |'
- en: '| "4": "Helpful - The response is on-point, detailed, and well-articulated,
    offering valuable information and clarifications that meet the user’s primary
    needs and enhance understanding." |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| "4": "有帮助 - 回应直击要点，详细且表达清晰，提供有价值的信息和澄清，满足用户的主要需求并增强理解。" |'
- en: '| "5": "Highly Helpful - The response is exceptionally thorough and precise,
    providing additional insights and valuable supplementary information." |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| "5": "非常有帮助 - 回应极为全面且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '| <Type 2: Shortened Format Version> |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| <类型 2: 缩短格式版本> |'
- en: '| "1": "The response is completely unrelated, lacks coherence, and fails to
    provide any meaningful information." |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| "1": "回应完全无关，缺乏连贯性，无法提供任何有意义的信息。" |'
- en: '| "2": "The response bears some relevance but remains largely superficial and
    unclear, addressing only the peripheral aspects of the user’s needs." |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| "2": "该回应具有一定相关性，但依然表面且不清晰，仅涉及用户需求的外围方面。" |'
- en: '| "3": "The response is mostly relevant and clear, covering the basic aspects
    of the query, but lacks depth and comprehensive elucidation." |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| "3": "回应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐释。" |'
- en: '| "4": "The response is on-point, detailed, and well-articulated, offering
    valuable information and clarifications that meet the user’s primary needs and
    enhance understanding." |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| "4": "回应直击要点，详细且表达清晰，提供有价值的信息和澄清，满足用户的主要需求并增强理解。" |'
- en: '| "5": "The response is exceptionally thorough and precise, providing additional
    insights and valuable supplementary information." |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| "5": "回应极为全面且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '| <Type 3: Gibberish Format Version> |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| <类型 3: 无意义格式版本> |'
- en: '| "1": "N*t H$l%ful - Th$ r$sp0n$e is c mplt$l? unr€la7$d, la$ks c()h$r$n(€,
    and f#i/s t# p$o&id$ any m€an*&gful !format$on." |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| "1": "无帮助 - 回应完全无关，缺乏连贯性，无法提供任何有意义的信息。" |'
- en: '| "2": "S#m$*ha+ H$%*fu/ - Th$ r#s0!n$ b%ars $o/e re$ev*nc$ b$t r$ma$n$ l#rg$l4
    $u/7$r7cial an* !ncl=4r, a6r$ss@n4 o7ly th$ p$r4ph@r$l a5p$cts #f th$ $s*r’s n**ds."
    |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| "2": "部分有帮助 - 回应具有一定相关性，但仍然显得较为表面且不清晰，仅涉及用户需求的外围方面。" |'
- en: '| "3": "M$!7r$t#ly H$lpfu& - Th$ r@s0*n$@ !s m$%stl€ r$’$van7 an cl$ar, c$%$r$n4
    th$ ba$!c a$%cts of th$ qu€ry, b$t l#cks d$pth an cmpr$h$ns$v$ lu$7$dat!on." |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| "3": "大致有帮助 - 回应大部分相关且清晰，涵盖了查询的基本方面，但缺乏深度和全面的阐释。" |'
- en: '| "4": "H$lpfu& - Th$ r!s0*n$e !s o/7-p$!nt, d$ta$!l$d, an w$l/-a&!u/at$d,
    #ff$r!n4 v#l$%bl$ #nformat$on and cl*r$!cat!ons th#t m=t th$ u/7$rś pr!/ary n$$ds
    an* @n7anc$ un#rstand!n4." |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| "4": "有帮助 - 回应直击要点，详细且表达清晰，提供有价值的信息和澄清，满足用户的主要需求并增强理解。" |'
- en: '| "5": "H4#h7y H$!p%u& - Th$ r$s&*n!e !s $xc$pt$#nally th#r#7gh an* pr$c$%$,
    pr#v$d$n# a4*!t$#nal !ns$4hts an* v#lu%bl$ @*pp%$%ntary #n%ormat$on." |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| "5": "非常有帮助 - 回应极为全面且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '| <Type 4: Shuffled Format Version> |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| <类型 4: 打乱格式版本> |'
- en: '| "1": "coherence fails provide unrelated, completely response - and the meaningful
    any to lacks Not Helpful is The information." |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| "1": "无帮助 - 回应完全无关，缺乏连贯性，无法提供任何有意义的信息。" |'
- en: '| "2": "superficial response largely addressing unclear, remains only needs.
    - relevance user’s and the Helpful the peripheral some bears but aspects Somewhat
    The of" |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| "2": "表面化回应 - 主要涉及用户需求的外围方面，但相关性不足，未能清晰阐明。" |'
- en: '| "3": "basic aspects query, lacks Moderately covering clear, - Helpful is
    depth response and comprehensive elucidation. relevant mostly the The and the
    of but" |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| "3": "基本有帮助 - 回应与查询的基本方面相关且清晰，但缺乏深度和全面的阐释。" |'
- en: '| "4": "clarifications the is response information needs enhance and Helpful
    - on-point, valuable well-articulated, offering understanding. The and detailed,
    primary that user’s meet" |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| "4": "澄清信息的回应 - 有帮助，直击要点，详细且表达清晰，提供有价值的信息和澄清，满足用户的主要需求并增强理解。" |'
- en: '| "5": "valuable Highly response is providing - the exceptionally Helpful information.
    insights thorough and additional precise, supplementary and The" |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| "5": "非常有帮助 - 回应极为全面且精确，提供了额外的见解和有价值的补充信息。" |'
- en: '| <Type 5: Flipped Format Version> |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| <类型 5: 反转格式版本> |'
- en: '| "1": "toN lufpleH - ehT esnopser si yletelpmoc detalernu, skcal ecnerehoc,
    dna sliaf ot edivorp yna lufgninaem noitamrofni." |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| "1": "非有用 - 响应是完全无关的，缺乏一致性，并且未能提供任何有意义的信息。" |'
- en: '| "2": "tamewoS lufpleH - ehT esnopser sraeb emos ecnaveler tub sniamer ylegral
    laicifrepus dna raelcnu, gnisserdda ylno eht larehpirep stcepsa fo eht s’resu
    sdeen." |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| "2": "部分**有用** - 响应包含一些相关内容，但仍然主要是表面化的，并且不清晰，仅涉及用户需求的部分方面。" |'
- en: '| "3": "yletaredoM lufpleH - ehT esnopser si yltsom tnaveler dna raelc, gnirevoc
    eht cisab stcepsa fo eht yreuq, tub skcal htped dna evisneherpmoc noitadicule."
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| "3": "中等**有用** - 响应大致上是相关和清晰的，覆盖了问题的基本方面，但缺乏深度和全面的解释。" |'
- en: '| "4": "lufpleH - ehT esnopser si tniop-no, deliated, dna detalucitra-llew,
    gnireffo elbaulav noitamrofni dna snoitacifralc taht teem eht s’resu yramirp sdeen
    dna ecnahne gnidnatsrednu." |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| "4": "**有用** - 响应是要点清晰、详细且结构良好的，提供了有价值的信息和分类，满足了用户的主要需求并增强了理解。" |'
- en: '| "5": "ylhgiH lufpleH - ehT esnopser si yllanoitpecxe hguoroht dna esicerp,
    gnidivorp lanoitidda sthgisni dna elbaulav yratnemelppus noitamrofni." |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| "5": "高度**有用** - 响应是异常彻底且精准的，提供了额外的见解和有价值的补充信息。" |'
- en: '| <Type 6: Masked Format Version> |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| <类型 6：隐藏格式版本> |'
- en: '| "1": "N__ H_l_ful - The r__pnse is c_m__et__y unr_l_te_, lacks _ohe_en_e,
    _nd _ai_s to p_ov_de _ny m_a__ngfu_ _nfo_ma_ion." |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| "1": "不**有用** - 响应完全无关，缺乏连贯性，并且未能提供任何有意义的信息。" |'
- en: '| "2": "_om_w_at He_p_ul - T_e re_ponse be_rs _ome rel__a_ce but r__ains la__ely
    s__erfi__al and u_cle__, ad_res__ng onl_ _he __ri__er_l a_pe_ts of t__ u_e_’s
    ne_ds." |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| "2": "部分**有用** - 响应包含一些相关内容，但仍然主要是表面化的，并且不清晰，仅涉及用户需求的部分方面。" |'
- en: '| "3": "Mod___tely _elp__l - Th_ _esp__se is mos__y re__va_t an_ _le_r, c_v__ing
    the ba_ic _spe_ts of the q_e_y, but __cks _e_th and co_preh_ns_ve el_c_d_t_on."
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| "3": "中等**有用** - 响应大致上是相关和清晰的，覆盖了问题的基本方面，但缺乏深度和全面的解释。" |'
- en: '| "4": "__lpful - _he respo_se is on-p_in_, d___iled, and we_l-ar_icu_ated,
    of_er_ng val_ab_e __for_ation and cl_r_fi__t_ons t_at mee_ the _se_’s p_im_r_
    _eeds and en__nce u_de__tan_ing." |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| "4": "**有用** - 响应是要点清晰、详细且结构良好的，提供了有价值的信息和分类，满足了用户的主要需求并增强了理解。" |'
- en: '| "5": "Hi_h_y H__p_ul - The r_spon_e is e_c_p_io_al__ th_r_ugh and p_ec_se,
    pr_vi_ing a_di__on_l ins_g_ts and va_u_b_e _upp_e_en_a_y inf_rma_io_." |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| "5": "高度**有用** - 响应是异常彻底且精准的，提供了额外的见解和有价值的补充信息。" |'
- en: 'Table 7: Criteria prompt format variations for helpfulness'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：有用性标准提示格式变化
