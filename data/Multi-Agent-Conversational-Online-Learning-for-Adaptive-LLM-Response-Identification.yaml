- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 11:41:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2025-01-11 11:41:02'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多智能体对话在线学习用于自适应LLM响应识别
- en: 来源：[https://arxiv.org/html/2501.01849/](https://arxiv.org/html/2501.01849/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2501.01849/](https://arxiv.org/html/2501.01849/)
- en: \usephysicsmodule
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \usephysicsmodule
- en: ab
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ab
- en: Xiangxiang Dai^†, Yuejin Xie^‡, Maoli Liu^†, Xuchuang Wang^§,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 代向向^†, 谢月进^‡, 刘茂立^†, 王旭创^§,
- en: Zhuohua Li^†, Huanyu Wang^♭, John C.S. Lui^†
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 朱华李^†, 黄宇王^♭, 约翰·C.S. 刘^†
- en: ^†The Chinese University of Hong Kong
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^†香港中文大学
- en: ^‡Huazhong University of Science and Technology
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^‡华中科技大学
- en: ^§University of Massachusetts Amherst
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ^§马萨诸塞大学阿姆赫斯特分校
- en: ^♭Huawei Technologies Co., Ltd.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^♭华为技术有限公司
- en: Email:{xxdai23, mlliu, zhli, cslui}@cse.cuhk.edu.hk, yuejinxie@hust.edu.cn,
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Email:{xxdai23, mlliu, zhli, cslui}@cse.cuhk.edu.hk, yuejinxie@hust.edu.cn,
- en: xuchuangwang@umass.edu, huanyuhello@zju.edu.cn Zhuohua Li is the corresponding
    author.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: xuchuangwang@umass.edu, huanyuhello@zju.edu.cn 朱华李是通讯作者。
- en: Abstract
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'The remarkable generative capability of large language models (LLMs) has sparked
    a growing interest in automatically generating responses for different applications.
    Given the dynamic nature of user preferences and the uncertainty of LLM response
    performance, it is crucial to design efficient online learning algorithms to identify
    optimal LLM responses (i.e., high-quality responses that also meet user preferences).
    Most existing online algorithms adopt a centralized approach and fail to leverage
    explicit user preferences for more efficient and personalized LLM response identification.
    In contrast, this paper introduces MACO (Multi-Agent Conversational Online Learning
    for Adaptive LLM Response Identification): 1) The online LLM response identification
    process is accelerated by multiple local agents (such as smartphones), while enhancing
    data privacy; 2) A novel conversational mechanism is proposed to adaptively conduct
    conversations for soliciting user preferences (e.g., a preference for a humorous
    tone over a serious one in generated responses), so to minimize uncertainty in
    preference estimation. Our theoretical analysis demonstrates that MACO is near-optimal
    regarding cumulative regret. Additionally, MACO offers reduced communication costs
    and computational complexity by eliminating the traditional, computing-intensive
    “G-optimal design” found in previous works. Extensive experiments with the open
    LLM Llama, coupled with two different embedding models from Google and OpenAI
    for text vector representation, demonstrate that MACO significantly outperforms
    the current state-of-the-art in online LLM response identification.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）出色的生成能力激发了人们对自动生成不同应用响应的浓厚兴趣。鉴于用户偏好的动态性和LLM响应表现的不确定性，设计高效的在线学习算法以识别最优LLM响应（即高质量且符合用户偏好的响应）至关重要。现有的大多数在线算法采用集中式方法，未能充分利用显式的用户偏好，从而未能实现更高效和个性化的LLM响应识别。与之相反，本文介绍了MACO（多智能体对话在线学习用于自适应LLM响应识别）：1）通过多个本地代理（如智能手机）加速在线LLM响应识别过程，同时增强数据隐私性；2）提出了一种新型对话机制，旨在自适应地进行对话以征求用户偏好（例如，偏好幽默语气而非严肃语气的生成响应），以减少偏好估计的不确定性。我们的理论分析表明，MACO在累积遗憾方面接近最优。此外，MACO通过消除传统计算密集型的“G-optimal设计”，降低了通信成本和计算复杂度。与开源LLM
    Llama的广泛实验，以及来自Google和OpenAI的两种不同嵌入模型用于文本向量表示，表明MACO在在线LLM响应识别中显著优于当前的最先进技术。
- en: I Introduction
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Large language models (LLMs) have swiftly transformed the technological landscape
    of our society [[1](https://arxiv.org/html/2501.01849v1#bib.bib1), [2](https://arxiv.org/html/2501.01849v1#bib.bib2)].
    A significant line of research is the exploration of prompts to identify optimal
    responses from LLMs [[3](https://arxiv.org/html/2501.01849v1#bib.bib3)]. This
    approach is compelling since it does not need to alter the internal parameters
    of an LLM, and can align well with human conversational patterns. Consequently,
    there is a growing interest in automatically identifying LLM responses, e.g.,
    through prompt engineering methods [[4](https://arxiv.org/html/2501.01849v1#bib.bib4),
    [5](https://arxiv.org/html/2501.01849v1#bib.bib5), [6](https://arxiv.org/html/2501.01849v1#bib.bib6)].
    These efforts aim to enhance LLMs’ capability to produce more accurate and relevant
    responses, collectively referred to as “LLM response identification”. Note that
    these prompt engineering methods are done offline, and only provide a “initiatory
    set of relatively good responses” by pre-specified prompt instructions. However,
    considering the diversity of responses generated by LLMs and the uncertainty in
    LLM performance, identifying the most suitable LLM response is inherently challenging
    [[7](https://arxiv.org/html/2501.01849v1#bib.bib7), [8](https://arxiv.org/html/2501.01849v1#bib.bib8)],
    as suitable responses are usually unknown in advance and context-dependent. Therefore,
    continuous online response adaptation is necessary [[9](https://arxiv.org/html/2501.01849v1#bib.bib9)],
    especially in scenarios such as medical diagnosis where highly accurate answers
    are required. Note that the online response identification approach can enhance
    the initiatory set of offline-generated responses so to match the specific context.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）迅速改变了我们社会的技术格局[[1](https://arxiv.org/html/2501.01849v1#bib.bib1),
    [2](https://arxiv.org/html/2501.01849v1#bib.bib2)]。一个重要的研究方向是探索如何通过提示词（prompts）识别LLM的最优回答[[3](https://arxiv.org/html/2501.01849v1#bib.bib3)]。这种方法具有吸引力，因为它不需要修改LLM的内部参数，且能够很好地与人类的对话模式对接。因此，自动识别LLM回答的兴趣日益增长，例如通过提示工程方法[[4](https://arxiv.org/html/2501.01849v1#bib.bib4),
    [5](https://arxiv.org/html/2501.01849v1#bib.bib5), [6](https://arxiv.org/html/2501.01849v1#bib.bib6)]。这些努力旨在提升LLM生成更准确、更相关的回答的能力，统称为“LLM回答识别”。需要注意的是，这些提示工程方法是在离线进行的，仅通过预先指定的提示指令提供一组“相对较好的初始回答”。然而，考虑到LLM生成的回答具有多样性且LLM性能具有不确定性，识别最合适的LLM回答本质上是具有挑战性的[[7](https://arxiv.org/html/2501.01849v1#bib.bib7),
    [8](https://arxiv.org/html/2501.01849v1#bib.bib8)]，因为适合的回答通常无法预先知道，并且具有依赖上下文的特点。因此，持续的在线回答适应是必要的[[9](https://arxiv.org/html/2501.01849v1#bib.bib9)]，特别是在医学诊断等需要高精度答案的场景中。需要注意的是，在线回答识别方法能够增强离线生成回答的初始集合，以更好地匹配特定的上下文。
- en: Furthermore, previous research has often overlooked the need to address diverse
    user preferences. It is crucial to not only ensure the quality of responses generated
    by LLMs, but also to tailor them to meet the specific preferences and expectations
    of different users. For instance, some users may prefer LLM-generated responses
    to be humorous, while others might prefer a more formal tone. Although [[10](https://arxiv.org/html/2501.01849v1#bib.bib10)]
    considers the optimization of preferences for LLMs, it only addresses the binary
    case of users’ likes and dislikes. LLM response identification must address the
    growing demand to cater to diverse user preferences. To address such needs, one
    can utilize cloud servers to continuously learn and refine LLM response identification
    by collecting feedback on the assessment of LLM responses. This feedback can be
    derived from users’ direct input or measurement of score functions [[11](https://arxiv.org/html/2501.01849v1#bib.bib11),
    [12](https://arxiv.org/html/2501.01849v1#bib.bib12)]. A response that not only
    meets quality standards but also aligns with user preferences is termed an “optimal
    LLM response.”
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，之前的研究常常忽视了应对不同用户偏好的需求。至关重要的是，不仅要确保大语言模型（LLM）生成的回答质量，还要根据不同用户的具体偏好和期望进行调整。例如，一些用户可能偏好LLM生成的回答具有幽默感，而其他用户则可能更倾向于正式的语气。虽然[[10](https://arxiv.org/html/2501.01849v1#bib.bib10)]考虑了LLM偏好的优化，但它仅解决了用户喜好和不喜好的二元问题。LLM回答识别必须应对日益增长的需求，即迎合多样化的用户偏好。为了解决这些需求，可以利用云服务器通过收集LLM回答评估的反馈，持续学习和完善LLM回答识别。这些反馈可以来自用户的直接输入或对评分函数的测量[[11](https://arxiv.org/html/2501.01849v1#bib.bib11),
    [12](https://arxiv.org/html/2501.01849v1#bib.bib12)]。一个既符合质量标准又与用户偏好相符的回答被称为“最优LLM回答”。
- en: I-A Multi-Agent Conversational Properties
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 多智能体对话特性
- en: In the context of LLM response identification, we observe two significant properties
    in typical LLM application scenarios. These properties inform and motivate our
    proposed formulation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM响应识别的背景下，我们观察到典型LLM应用场景中有两个显著的特性。这些特性为我们的提议公式提供了启示和动机。
- en: First, in the utilization of LLMs, users commonly access LLM services across
    multiple devices, such as smartphones, tablets, and desktops, collectively referred
    to as “local agents.” For example, the Poe AI chatting platform [[13](https://arxiv.org/html/2501.01849v1#bib.bib13)]
    handles user queries originating from various devices. Leveraging this multi-agent
    framework, LLM response identification tailored to specific user preferences can
    be performed concurrently on each local agent, facilitating data aggregation and
    enhancing learning efficiency on a user preference. Moreover, this approach offers
    an added layer of privacy protection, as sensitive information remains localized
    and is neither transmitted nor stored on central servers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在使用LLM时，用户通常会跨多个设备访问LLM服务，如智能手机、平板电脑和桌面电脑，统称为“本地智能体”。例如，Poe AI聊天平台 [[13](https://arxiv.org/html/2501.01849v1#bib.bib13)]
    处理来自不同设备的用户查询。利用这种多智能体框架，可以在每个本地智能体上并行执行针对特定用户偏好的LLM响应识别，从而促进数据聚合并提高用户偏好学习的效率。此外，这种方法还提供了额外的隐私保护层，因为敏感信息保留在本地，不会传输或存储在中央服务器上。
- en: 'Second, a key challenge for online LLM methods lies in addressing the “cold
    start” problem, where response identification may be inaccurate for new users
    with limited historical data. To address this, *conversational recommendation* [[14](https://arxiv.org/html/2501.01849v1#bib.bib14),
    [15](https://arxiv.org/html/2501.01849v1#bib.bib15), [16](https://arxiv.org/html/2501.01849v1#bib.bib16)]
    has been applied in LLM applications. In this approach, the cloud server can proactively
    query users with questions and obtain feedback, thereby quickly eliciting user
    preferences. For example, in OpenAI’s design, when ChatGPT is tasked with computing
    factorials in Python, it may provide two “correct” implementations with different
    styles: one recursive, the other iterative. During the interaction, the user provides
    feedback on their preferred coding style. This “conversation” process allows ChatGPT
    to learn from the user’s code preferences, enabling it to tailor its future responses
    more effectively to individual users.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在线LLM方法的一个关键挑战在于解决“冷启动”问题，在这个问题中，对于历史数据有限的新用户，响应识别可能不准确。为了解决这个问题，*对话推荐* [[14](https://arxiv.org/html/2501.01849v1#bib.bib14),
    [15](https://arxiv.org/html/2501.01849v1#bib.bib15), [16](https://arxiv.org/html/2501.01849v1#bib.bib16)]
    已在LLM应用中得到应用。在这种方法中，云服务器可以主动向用户提问并获取反馈，从而迅速引导出用户偏好。例如，在OpenAI的设计中，当ChatGPT被要求用Python计算阶乘时，它可能会提供两种“正确”的实现方式，分别是递归和迭代。在互动过程中，用户对自己偏好的编码风格提供反馈。这种“对话”过程使ChatGPT能够根据用户的编码偏好进行学习，从而在未来的响应中更有效地调整，以满足个体用户的需求。
- en: I-B Challenges and Our Contributions
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 挑战与我们的贡献
- en: 'To adaptively identify the appropriate LLM responses, which were generated
    from an initiatory set of responses generated through offline prompt engineering
    techniques, we propose to utilize online *contextual bandit* approaches, where
    a sequential decision-making cloud server selects LLM responses (i.e., an arms
    corresponds to a response) for users and receives feedback. Besides the arm-level
    feedback, the cloud server can occasionally prompt users with questions about
    *key terms* [[17](https://arxiv.org/html/2501.01849v1#bib.bib17), [18](https://arxiv.org/html/2501.01849v1#bib.bib18)].
    For example, asking about the user’s preference on a category: “Are you interested
    in news about basketball?”, or asking about the user’s preference on an entity:
    “Do you like to read news related to LeBron James?”. The feedback from key terms
    like “basketball” and “LeBron James” can reflect user preferences, allowing the
    cloud server to accelerate the learning process. The objective is to develop an
    online adaptive strategy that maximizes user satisfaction over the long term.
    However, the current works of conversational contextual bandit algorithms fall
    short of addressing the unique challenges of online adaptive LLM response identification:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了自适应地识别适当的LLM响应，这些响应是通过离线提示工程技术生成的初始响应集合生成的，我们建议利用在线*上下文赌博*方法，其中一个顺序决策云服务器为用户选择LLM响应（即，一个臂对应一个响应）并接收反馈。除了臂级反馈外，云服务器还可以偶尔通过向用户提出关于*关键术语*的问题来获取反馈[[17](https://arxiv.org/html/2501.01849v1#bib.bib17),
    [18](https://arxiv.org/html/2501.01849v1#bib.bib18)]。例如，询问用户对某一类别的偏好：“您对篮球新闻感兴趣吗？”，或询问用户对某一实体的偏好：“您喜欢阅读关于勒布朗·詹姆斯的新闻吗？”。像“篮球”和“勒布朗·詹姆斯”这样的关键术语的反馈可以反映用户偏好，从而使云服务器能够加速学习过程。目标是开发一种在线自适应策略，在长期内最大化用户满意度。然而，当前关于对话式上下文赌博算法的研究未能解决在线自适应LLM响应识别中的独特挑战：
- en: ❶ Firstly, existing bandit models that account for user preferences are predominantly
    employed in recommendation systems [[18](https://arxiv.org/html/2501.01849v1#bib.bib18),
    [19](https://arxiv.org/html/2501.01849v1#bib.bib19), [20](https://arxiv.org/html/2501.01849v1#bib.bib20)].
    These models typically utilize Singular Value Decomposition (SVD) to extract feature
    vectors of comparatively lower dimensions. However, quantifying features from
    LLM text responses, which contain complex semantic information and lead to much
    higher dimensional feature spaces, presents significant computational challenges.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 首先，现有的考虑用户偏好的赌博模型主要应用于推荐系统[[18](https://arxiv.org/html/2501.01849v1#bib.bib18),
    [19](https://arxiv.org/html/2501.01849v1#bib.bib19), [20](https://arxiv.org/html/2501.01849v1#bib.bib20)]。这些模型通常利用奇异值分解（SVD）提取相对较低维度的特征向量。然而，从包含复杂语义信息的LLM文本响应中量化特征，并且这些响应导致更高维度的特征空间，这带来了巨大的计算挑战。
- en: ❷ Secondly, previous conversational bandit works primarily follow the framework
    by [[21](https://arxiv.org/html/2501.01849v1#bib.bib21)], which addresses the
    *infinitely* arms. However, the number of LLM responses that need online identification
    from an initiatory set of responses generated via prompt engineering is typically
    finite. While elimination-based contextual bandit algorithms can handle this setting,
    they rely on the computationally intensive *G-optimal design* procedure [[22](https://arxiv.org/html/2501.01849v1#bib.bib22),
    [23](https://arxiv.org/html/2501.01849v1#bib.bib23), [24](https://arxiv.org/html/2501.01849v1#bib.bib24)]
    to calculate a distribution for arm selection, thus slowing down the online LLM
    response identification.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 其次，以前的对话式赌博工作主要遵循[[21](https://arxiv.org/html/2501.01849v1#bib.bib21)]的框架，该框架解决了*无限*臂的问题。然而，需要从通过提示工程生成的初始响应集合中在线识别的LLM响应数量通常是有限的。尽管基于消除的上下文赌博算法可以处理这种设置，但它们依赖于计算密集型的*G-最优设计*过程[[22](https://arxiv.org/html/2501.01849v1#bib.bib22),
    [23](https://arxiv.org/html/2501.01849v1#bib.bib23), [24](https://arxiv.org/html/2501.01849v1#bib.bib24)]来计算臂选择的分布，从而减缓了在线LLM响应识别的速度。
- en: ❸ Thirdly, existing studies on conversational bandits [[19](https://arxiv.org/html/2501.01849v1#bib.bib19),
    [17](https://arxiv.org/html/2501.01849v1#bib.bib17)] rely on predetermined functions
    to control conversation frequency, which typically follow a fixed sequence of
    engagements to initiate a specific number of conversations. This approach is not
    suitable for the dynamic nature of LLM response identification, as it imposes
    unnecessary restrictions and could degrade user experience.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第三，现有的对话型赌博机研究[[19](https://arxiv.org/html/2501.01849v1#bib.bib19), [17](https://arxiv.org/html/2501.01849v1#bib.bib17)]依赖于预定的函数来控制对话频率，这些函数通常遵循固定的交互顺序，以启动特定数量的对话。此方法不适用于LLM响应识别的动态特性，因为它强加了不必要的限制，可能会降低用户体验。
- en: ❹ Finally, existing literature on conversational bandits solely considers centralized
    scenarios, neglecting the inherent multi-agent property of data source of LLM
    platforms. While there are works on distributed bandits with finite arms [[22](https://arxiv.org/html/2501.01849v1#bib.bib22),
    [25](https://arxiv.org/html/2501.01849v1#bib.bib25), [26](https://arxiv.org/html/2501.01849v1#bib.bib26)],
    they either require all local agents to upload user feedback to the cloud server
    or share the exactly same arm set. These restrictive settings can leak sensitive
    information, reduce the flexibility of local agents, and increase communication
    costs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 最后，现有的对话型赌博机文献仅考虑了集中式场景，忽略了LLM平台数据源的固有多代理特性。虽然已有一些关于具有有限臂的分布式赌博机的研究[[22](https://arxiv.org/html/2501.01849v1#bib.bib22),
    [25](https://arxiv.org/html/2501.01849v1#bib.bib25), [26](https://arxiv.org/html/2501.01849v1#bib.bib26)]，但它们要么要求所有本地代理将用户反馈上传到云服务器，要么共享完全相同的臂集。这些限制性的设置可能泄露敏感信息，降低本地代理的灵活性，并增加通信成本。
- en: 'This paper makes the following contributions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文做出了以下贡献：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Model Formulation: We propose a distributed conversational bandit model for
    online LLM response identification. Complementing existing methods that rely on
    offline selection from a pre-generated pool of LLM responses. Our model emphasizes
    “online identification” of the optimal LLM response from the pre-generated arm
    set with uncertain performance. This involves ensuring the quality of the generated
    response while considering user preferences.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型构建：我们提出了一种分布式对话型赌博机模型，用于在线LLM响应识别。补充了现有依赖于从预生成的LLM响应池中离线选择的方法。我们的模型强调从具有不确定性能的预生成臂集在线识别最优LLM响应。这涉及在考虑用户偏好的同时，确保生成响应的质量。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Algorithm Design: We propose the Conversational Adaptive Distributed Identifier
    (MACO), comprising MACO-A, which is executed by local agents, and MACO-S, which
    is executed by the cloud server. Unlike previous works with predetermined conversation
    frequencies, MACO adaptively decides when to engage in conversations based on
    the current context. Additionally, it enhances collaboration among local agents
    to improve the efficiency of LLM response identification.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 算法设计：我们提出了对话自适应分布式标识符（MACO），由本地代理执行的MACO-A和由云服务器执行的MACO-S组成。与以前预定对话频率的工作不同，MACO根据当前上下文自适应地决定何时进行对话。此外，它增强了本地代理之间的协作，以提高LLM响应识别的效率。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Theoretical Analysis: We establish the regret upper bound for MACO at $\mathcal{\widetilde{O}}(\sqrt{dMT})$,
    with a lower bound analysis of $\Omega(\sqrt{dMT})$, indicating that MACO is near-optimal.
    Additionally, we leverage the conversational setting to enhance efficiency in
    both computation and communication, compared to existing work on distributed linear
    contextual bandits with finite arm sets. Specifically, we provide the upper bound
    of communication cost as $\mathcal{O}(d^{2}M\log T)$. The development of distributed
    conversational bandits in MACO  successfully avoids the computationally intensive
    G-optimal design, which is required in previous elimination-based linear bandits.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论分析：我们为MACO建立了后悔上界$\mathcal{\widetilde{O}}(\sqrt{dMT})$，并进行了下界分析$\Omega(\sqrt{dMT})$，表明MACO接近最优。此外，我们利用对话设置提升了计算和通信的效率，相较于现有的关于具有有限臂集的分布式线性上下文赌博机的工作。具体来说，我们提供了通信成本的上界为$\mathcal{O}(d^{2}M\log
    T)$。MACO中分布式对话型赌博机的发展成功避免了先前基于消除的线性赌博机中需要的计算密集型G最优设计。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Experimental Evaluation: We conduct extensive experiments using the open LLM
    Llama to generate responses, coupled with two different embedding models from
    Google and OpenAI for text vector representation. Testing under various conditions,
    including different arm pool sizes and numbers of local agents, our algorithm
    consistently outperforms state-of-the-art methods. Additionally, by eliminating
    the time-intensive G-optimal design procedure, our approach significantly reduces
    execution time. This reduction does not compromise performance, thanks to our
    conversational mechanisms design, which enhances the speed of online LLM response
    identification and estimation of user preference.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验评估：我们使用开放的LLM Llama进行广泛实验，生成响应，并结合Google和OpenAI的两种不同嵌入模型用于文本向量表示。在各种条件下进行测试，包括不同的臂池大小和本地智能体数量，我们的算法始终优于现有的最先进方法。此外，通过消除耗时的G-最优设计过程，我们的方法显著减少了执行时间。得益于我们设计的对话机制，它提升了在线LLM响应识别和用户偏好估计的速度，这一时间缩短不会影响性能。
- en: II System Model
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 系统模型
- en: This section formulates the multi-agent conversational bandit for online LLM
    response identification.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 本节制定了用于在线LLM响应识别的多智能体对话强盗算法。
- en: '![Refer to caption](img/41ecbf14758c7910e0326c542e5c1e62.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/41ecbf14758c7910e0326c542e5c1e62.png)'
- en: 'Figure 1: An adaptive multi-agent conversational bandit framework for identifying
    online LLM responses. Local agents handle response selection (arms), while a central
    server manages conversation flow through key term selection. The server aggregates
    interaction data across multiple agents to accelerate user preference learning.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：一种用于识别在线LLM响应的自适应多智能体对话强盗算法框架。本地智能体处理响应选择（臂），而中央服务器通过关键术语选择管理对话流程。服务器聚合多个智能体的交互数据，以加速用户偏好的学习。
- en: II-A Online LLM Response Identification
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 在线LLM响应识别
- en: We define the set of local agents as ${\mathcal{M}}$ with $|{\mathcal{M}}|=M$,
    which represent devices such as smartphones, laptops, and tablets. For any local
    agent $m\in{\mathcal{M}}$, the finite arm set of LLM responses is denoted as ${\mathcal{A}}_{m}$,
    which represents possible responses generated from various prompts. Given the
    heterogeneity of agents, different local agents may have different arm sets, which
    is different from the assumption in [[26](https://arxiv.org/html/2501.01849v1#bib.bib26)]
    that all local agents share the same arm set. As mentioned in Section [I](https://arxiv.org/html/2501.01849v1#S1
    "I Introduction ‣ Multi-Agent Conversational Online Learning for Adaptive LLM
    Response Identification"), traditional offline techniques (e.g., prompt engineering)
    can help to construct a set of initial responses, but due to the diversity of
    LLM outputs and user preferences, it is essential to adaptively fine-tune the
    optimal response online, despite having an offline initiatory set of LLM responses.
    Our model adopts a time-slotted approach, denoted by discrete-time rounds $\mathcal{T}=\{1,2,3,\ldots,T\}$,
    where each local agent selects one arm, i.e., LLM response, at each round $t\in\mathcal{T}$.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义本地智能体集为${\mathcal{M}}$，其中$|{\mathcal{M}}|=M$，代表诸如智能手机、笔记本电脑和平板电脑等设备。对于任意本地智能体$m\in{\mathcal{M}}$，LLM响应的有限臂集表示为${\mathcal{A}}_{m}$，它代表从各种提示生成的可能响应。考虑到智能体的异质性，不同的本地智能体可能有不同的臂集，这与[[26](https://arxiv.org/html/2501.01849v1#bib.bib26)]中假设所有本地智能体共享相同的臂集有所不同。如在[第一部分](https://arxiv.org/html/2501.01849v1#S1
    "I Introduction ‣ Multi-Agent Conversational Online Learning for Adaptive LLM
    Response Identification")中提到，传统的离线技术（例如提示工程）可以帮助构建初始响应集，但由于LLM输出和用户偏好的多样性，即使有离线的初始LLM响应集，仍然需要自适应地在线微调最优响应。我们的模型采用时间分段的方法，表示为离散时间回合$\mathcal{T}=\{1,2,3,\ldots,T\}$，其中每个本地智能体在每个回合$t\in\mathcal{T}$选择一个臂，即LLM响应。
- en: II-B Multi-Agent User-Personalized Bandits
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 多智能体用户个性化强盗算法
- en: 'We consider a multi-agent conversational bandit setting involving $M$ agents
    and a cloud server. At each round $t\in\mathcal{T}$, a local agent $m\in{\mathcal{M}}$
    selects an arm $a_{m,t}\in\mathcal{A}_{m}$, which denotes one possible LLM response,
    and receives reward feedback $r_{m,t}$ that reflects the corresponding performance.
    Eliciting user feedback is beyond the scope of this work. Here, the term “feedback”
    broadly encompasses direct user input, data inferred from techniques that measure
    user behavior, and preference simulators [[12](https://arxiv.org/html/2501.01849v1#bib.bib12)].
    The user’s preference for LLM responses is represented by an “unknown” preference
    feature vector $\bm{\theta}^{*}\in\mathbb{R}^{d}$, which all local agents aim
    to learn. For a local agent $m\in{\mathcal{M}}$, considering both the impact of
    the LLM response (i.e., arm $a_{m,t}\in\mathcal{A}_{m}$) and the unknown user
    preference $\bm{\theta}^{*}$, the reward can be expressed as a linear combination
    with a noise term $\eta_{m,t}$: $r_{a_{m},t}=\langle\bm{x}_{a_{m},t},\bm{\theta}^{*}\rangle+\eta_{m,t},$
    where $\bm{x}_{a_{m},t}\in{\mathbb{R}}^{d}$ is the embedding feature vector the
    corresponding arm $a_{m}$, to capture the textual information [[1](https://arxiv.org/html/2501.01849v1#bib.bib1),
    [3](https://arxiv.org/html/2501.01849v1#bib.bib3)]. We will demonstrate the generalization
    of our model using two different open embedding approaches in Section [V](https://arxiv.org/html/2501.01849v1#S5
    "V Performance Evaluation ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"). Our objective is to design a policy that selects
    arms (i.e., LLM responses) each round to minimize cumulative regret, defined as
    the difference between the cumulative rewards of our policy and the best unknown
    policy across all local agents, tailored to personalized user preferences, which
    is defined as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个涉及$M$个智能体和一个云服务器的多智能体对话性赌博问题。在每一轮$t\in\mathcal{T}$中，一个本地智能体$m\in{\mathcal{M}}$选择一个臂$a_{m,t}\in\mathcal{A}_{m}$，该臂表示一个可能的LLM响应，并接收奖励反馈$r_{m,t}$，该反馈反映了相应的表现。获取用户反馈超出了本文的范围。在此，“反馈”一词广义上包括直接的用户输入、通过测量用户行为的技术推断的数据以及偏好模拟器[[12](https://arxiv.org/html/2501.01849v1#bib.bib12)]。用户对LLM响应的偏好由一个“未知的”偏好特征向量$\bm{\theta}^{*}\in\mathbb{R}^{d}$表示，所有本地智能体的目标是学习它。对于本地智能体$m\in{\mathcal{M}}$，考虑到LLM响应的影响（即臂$a_{m,t}\in\mathcal{A}_{m}$）和未知的用户偏好$\bm{\theta}^{*}$，奖励可以表示为带有噪声项$\eta_{m,t}$的线性组合：$r_{a_{m},t}=\langle\bm{x}_{a_{m},t},\bm{\theta}^{*}\rangle+\eta_{m,t}$，其中$\bm{x}_{a_{m},t}\in{\mathbb{R}}^{d}$是相应臂$a_{m}$的嵌入特征向量，用于捕捉文本信息[[1](https://arxiv.org/html/2501.01849v1#bib.bib1),
    [3](https://arxiv.org/html/2501.01849v1#bib.bib3)]。我们将在第[V](https://arxiv.org/html/2501.01849v1#S5
    "V 性能评估 ‣ 针对自适应LLM响应识别的多智能体对话性在线学习")节中使用两种不同的开放嵌入方法展示我们模型的泛化能力。我们的目标是设计一种策略，在每轮选择臂（即LLM响应）以最小化累积遗憾，遗憾定义为我们策略的累积奖励与所有本地智能体中最佳未知策略的累积奖励之间的差异，旨在个性化用户偏好，定义如下：
- en: '|  | $\vspace{-0.01in}R_{M}(T)=\sum_{m=1}^{M}\sum_{t=1}^{T}\left(\bm{x}_{a_{m}^{*}}^%
    {\mathsf{T}}\bm{\theta}^{*}-\bm{x}_{a_{m},t}^{\mathsf{T}}\bm{\theta}^{*}\right%
    ).\vspace{-0.01in}$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.01in}R_{M}(T)=\sum_{m=1}^{M}\sum_{t=1}^{T}\left(\bm{x}_{a_{m}^{*}}^%
    {\mathsf{T}}\bm{\theta}^{*}-\bm{x}_{a_{m},t}^{\mathsf{T}}\bm{\theta}^{*}\right%
    ).\vspace{-0.01in}$ |  | (1) |'
- en: where $a_{m}^{*}\in\arg\max_{a\in\mathcal{A}_{m}}\bm{x}_{a}^{\mathsf{T}}\bm{\theta}^{*}$
    denotes the locally optimal arm with the highest expected reward at local agent
    $m\in{\mathcal{M}}$. This regret definition follows prior works [[21](https://arxiv.org/html/2501.01849v1#bib.bib21),
    [17](https://arxiv.org/html/2501.01849v1#bib.bib17), [18](https://arxiv.org/html/2501.01849v1#bib.bib18)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$a_{m}^{*}\in\arg\max_{a\in\mathcal{A}_{m}}\bm{x}_{a}^{\mathsf{T}}\bm{\theta}^{*}$表示在本地智能体$m\in{\mathcal{M}}$处具有最高期望奖励的本地最优臂。这个遗憾定义继承自先前的研究[[21](https://arxiv.org/html/2501.01849v1#bib.bib21),
    [17](https://arxiv.org/html/2501.01849v1#bib.bib17), [18](https://arxiv.org/html/2501.01849v1#bib.bib18)]。
- en: II-C Conversational Contextual Mechanism
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 对话上下文机制
- en: In addition to obtaining feedback by selecting arms on suitable LLM responses,
    the cloud server can occasionally query users from each local agent for feedback
    to better estimate user preferences. However, relying solely on directly considering
    all answers can lead to inefficiencies due to the issue of information dispersion.
    Specifically, the contextual vectors of different answers may vary significantly,
    even if they share similarities at an abstract level. For instance, responses
    about “syntax rules,” “best practices,” or “compiler optimizations” may all relate
    to “C/C++,” but their contextual representations can differ greatly. Similarly,
    responses with a “humorous tone” could vary between “lighthearted,” “sarcastic,”
    or “playful” expressions. To address this issue, we introduce “key terms” to represent
    core topics or features of user interests from [[17](https://arxiv.org/html/2501.01849v1#bib.bib17),
    [18](https://arxiv.org/html/2501.01849v1#bib.bib18)]. A key term groups multiple
    related arms under a single concept. For example, the key term “C/C++” can encompass
    responses about “syntax rules,” “best practices,” and “compiler optimizations,”
    while the key term “humorous tone” might include responses that are “lighthearted,”
    “sarcastic,” or “playful.” Feedback on a key term propagates to its related arms,
    enabling the system to infer preferences across multiple responses with minimal
    interaction.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过选择适当的LLM响应来获得反馈之外，云服务器还可以偶尔从每个本地代理查询用户反馈，以更好地估计用户的偏好。然而，仅仅依赖直接考虑所有答案可能会导致效率低下，因为信息分散的问题。具体来说，即使不同的答案在抽象层面上具有相似性，它们的上下文向量也可能存在显著差异。例如，关于“语法规则”、“最佳实践”或“编译器优化”的回答可能都与“C/C++”相关，但它们的上下文表示可能大不相同。同样，带有“幽默语气”的回答可能在“轻松愉快”、“讽刺”或“俏皮”表达之间有所不同。为了解决这个问题，我们引入了“关键词”来表示用户兴趣的核心话题或特征，参见[[17](https://arxiv.org/html/2501.01849v1#bib.bib17),
    [18](https://arxiv.org/html/2501.01849v1#bib.bib18)]。一个关键词将多个相关的选择臂聚集在一个概念下。例如，关键词“C/C++”可以涵盖关于“语法规则”、“最佳实践”和“编译器优化”的回答，而关键词“幽默语气”可能包括“轻松愉快”、“讽刺”或“俏皮”的回答。对一个关键词的反馈会传播到其相关的选择臂，从而使系统能够通过最小的互动推断出多个回答的偏好。
- en: 'Formally, let $\mathcal{K}$ denote the finite set of key terms, with each element
    $\tilde{\bm{x}}_{k}\in{\mathbb{R}}^{d}$ representing the feature vector for key
    term $k\in\mathcal{K}$. Let $\mathcal{K}$ denote the finite set of key terms,
    with each element $\tilde{\bm{x}}_{k}\in{\mathbb{R}}^{d}$ being a feature vector
    for the corresponding key term $k\in\mathcal{K}$. Applying the conversational
    bandits to our multi-agent framework, a user served by local agent $m$ can be
    queried with a key term $k_{m}\in\mathcal{K}_{m}$, where $\mathcal{K}_{m}\subseteq\mathcal{K}$
    is the subset of key terms at local agent $m$. Considering user preference $\bm{\theta}^{*}$
    with a noise term $\widetilde{\eta}_{m,t}$, the conversational feedback is modeled
    as: $\widetilde{r}_{k_{m},t}=\langle\tilde{\bm{x}}_{k_{m},t},\bm{\theta}^{*}\rangle%
    +\widetilde{\eta}_{m,t}.$ Note that our model diverges from previous conversational
    bandits [[17](https://arxiv.org/html/2501.01849v1#bib.bib17), [18](https://arxiv.org/html/2501.01849v1#bib.bib18),
    [27](https://arxiv.org/html/2501.01849v1#bib.bib27), [28](https://arxiv.org/html/2501.01849v1#bib.bib28)],
    which employ a fixed conversation function, typically linear or logarithmic of
    round $t$, to regulate the frequency of conversations. These methods initiate
    conversations periodically, regardless of whether user preferences have been sufficiently
    estimated, which can negatively impact the user experience. (A more detailed comparison
    is provided in Section [IV](https://arxiv.org/html/2501.01849v1#S4 "IV Performance
    Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response
    Identification")). Conversely, as we will elaborate in Section [III](https://arxiv.org/html/2501.01849v1#S3
    "III Algorithm Design ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"), our algorithm conducts conversations “adaptively”,
    engaging users only when necessary to refine the user preference estimation.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，设$\mathcal{K}$为关键术语的有限集合，其中每个元素$\tilde{\bm{x}}_{k}\in{\mathbb{R}}^{d}$表示关键术语$k\in\mathcal{K}$的特征向量。设$\mathcal{K}$为关键术语的有限集合，其中每个元素$\tilde{\bm{x}}_{k}\in{\mathbb{R}}^{d}$是对应关键术语$k\in\mathcal{K}$的特征向量。将对话型老虎机应用于我们的多智能体框架中，用户由本地代理$m$提供服务，可以用关键术语$k_{m}\in\mathcal{K}_{m}$进行查询，其中$\mathcal{K}_{m}\subseteq\mathcal{K}$是本地代理$m$的关键术语子集。考虑到用户偏好$\bm{\theta}^{*}$和噪声项$\widetilde{\eta}_{m,t}$，对话反馈被建模为：$\widetilde{r}_{k_{m},t}=\langle\tilde{\bm{x}}_{k_{m},t},\bm{\theta}^{*}\rangle%
    +\widetilde{\eta}_{m,t}$。请注意，我们的模型与先前的对话型老虎机模型[[17](https://arxiv.org/html/2501.01849v1#bib.bib17),
    [18](https://arxiv.org/html/2501.01849v1#bib.bib18), [27](https://arxiv.org/html/2501.01849v1#bib.bib27),
    [28](https://arxiv.org/html/2501.01849v1#bib.bib28)]不同，这些模型采用固定的对话函数，通常是关于回合$t$的线性或对数函数，用于调节对话频率。这些方法定期发起对话，无论用户偏好是否已经充分估计，这可能会对用户体验产生负面影响。（更详细的比较见第[IV](https://arxiv.org/html/2501.01849v1#S4
    "IV 性能分析 ‣ 多智能体对话型在线学习与自适应LLM响应识别")节）。相反，正如我们将在第[III](https://arxiv.org/html/2501.01849v1#S3
    "III 算法设计 ‣ 多智能体对话型在线学习与自适应LLM响应识别")节中详细阐述的那样，我们的算法进行“自适应”对话，仅在必要时与用户互动，以改进用户偏好的估计。
- en: II-D Distributed Communication Model
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 分布式通信模型
- en: We consider a distributed model with $M$ local agents and a cloud server, adopting
    a synchronous communication paradigm. In this setup, as shown in Fig. [1](https://arxiv.org/html/2501.01849v1#S2.F1
    "Figure 1 ‣ II System Model ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"), each local agent communicates with the cloud server
    by uploading and downloading data with negligible latency. Moreover, the local
    agents do not directly communicate with each other. For simplicity, we focus on
    discrete-slot rounds solely for recording the selected arm. Querying key terms
    is interspersed with identifying LLM responses, allowing a key term to be queried
    and an arm to be pulled simultaneously. This aligns with the practical operations
    of conversational LLM systems. Consistent with  [[22](https://arxiv.org/html/2501.01849v1#bib.bib22)],
    we define communication cost as the cumulative count of scalar units transmitted
    between the cloud server and local agents, which include both integers and real
    numbers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个分布式模型，其中有$M$个本地代理和一个云服务器，采用同步通信范式。在此设置中，如图[1](https://arxiv.org/html/2501.01849v1#S2.F1
    "图 1 ‣ II 系统模型 ‣ 用于自适应LLM响应识别的多代理对话在线学习")所示，每个本地代理通过上传和下载数据与云服务器进行通信，且延迟可忽略不计。此外，本地代理之间不直接通信。为了简化起见，我们仅关注离散时隙轮次，用于记录选择的臂。查询关键词与识别LLM响应交替进行，从而允许同时查询关键词和拉动臂。这与对话型LLM系统的实际操作相符。与[[22](https://arxiv.org/html/2501.01849v1#bib.bib22)]一致，我们将通信成本定义为在云服务器和本地代理之间传输的标量单位的累积计数，其中包括整数和实数。
- en: III Algorithm Design
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 算法设计
- en: We present the design of multi-agent conversational online learning (MACO) algorithms,
    implemented by local agents and a cloud server for adaptive identifying LLM response.
    Then, we compare our design to the traditional phase elimination-based online
    learning algorithm [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了多代理对话在线学习（MACO）算法的设计，由本地代理和云服务器实现，用于自适应识别LLM响应。随后，我们将我们的设计与传统的基于阶段消除的在线学习算法[[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]进行比较。
- en: For any real vector $\bm{x}$ and a positive semi-definite matrix $\bm{M}$, let
    $\|\bm{x}\|_{\bm{M}}\coloneqq\sqrt{\bm{x}^{\mathsf{T}}\bm{M}\bm{x}}$. Denote the
    cardinality of a set $\mathcal{A}$ as $|\mathcal{A}|$. We introduce the notation
    $[z]\coloneqq\{1,\dots,z\}$ for $\forall z\in{\mathbb{N}}^{+}$. Define $\mathcal{T}_{m,a}^{p}$
    as the set of rounds where local agent $m$ selects arm $a$ in phase $p$, $\mathcal{\widetilde{T}}_{m,k}^{p}$
    as the set of rounds when agent $m$ conducts interaction on key term $k$ in the
    same phase, and $A$ (where $A\leq|\mathcal{A}|)$ as the size of actually pulled
    arms from the LLM response set at each round.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何实向量$\bm{x}$和一个正半定矩阵$\bm{M}$，令$\|\bm{x}\|_{\bm{M}}\coloneqq\sqrt{\bm{x}^{\mathsf{T}}\bm{M}\bm{x}}$。设集合$\mathcal{A}$的基数为$|\mathcal{A}|$。我们引入符号$[z]\coloneqq\{1,\dots,z\}$，其中$\forall
    z\in{\mathbb{N}}^{+}$。定义$\mathcal{T}_{m,a}^{p}$为本地代理$m$在阶段$p$中选择臂$a$的轮次集合，$\mathcal{\widetilde{T}}_{m,k}^{p}$为本地代理$m$在同一阶段对关键词$k$进行交互的轮次集合，$A$（其中$A\leq|\mathcal{A}|$）为在每一轮中从LLM响应集合中实际拉动的臂的大小。
- en: III-A MACO Algorithm on Local Agent
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A MACO算法在本地代理上的应用
- en: 'Input: Round horizon $T$, number of local agent $M$, input dimension $d$, arm
    set $\mathcal{A}_{m}$, arm pool size $A$, confidence parameter $\delta\in(0,1]$Initialization:
    Let $p=1,\mathcal{A}_{m}^{p}=\mathcal{A}_{m}$12while *$T$ has not been reached* do3      
    Calculate $\bm{M}_{m}^{p}=\sum_{a\in\mathcal{A}_{m}^{p}}\frac{1}{|\mathcal{A}_{m}^{p}|}%
    \bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}$4       Diagonalize $\bm{M}_{m}^{p}=\sum_{j=1}^{d}\lambda_{\bm{v}_{j}}\bm{v}_{j}\bm{v}_{j}^{\mathsf%
    {T}}$5       Upload eigenvector $\bm{v}_{j}$, if its corresponding eigenvalue
    satisfies $\lambda_{\bm{v}_{j}}<h_{p}\coloneqq\frac{3}{4(1-2^{-2p})d}$6      
    Download $\mathcal{K}_{m}^{p}$ and $\set{n_{m,k}^{p}}_{k\in\mathcal{K}_{m}^{p}}$
    from the cloud server7      8      foreach *$k\in\mathcal{K}_{m}^{p}$*  do  $\triangleright$ Conduct
    conversations9             Querying key term $k$ for $n_{m,k}^{p}$ times10            
    Receive rewards $\set{\widetilde{r}_{k,t}}_{t\in\mathcal{\widetilde{T}}_{m,k}^{p}}$
    from direct conversational feedback11            12      13      foreach *$a\in\mathcal{A}_{m}^{p}$* do  $\triangleright$ Pull
    arms14             Set $n_{m,a}^{p}=\left\lceil\frac{d}{2^{(-2p-1)}|\mathcal{A}_{m}^{p}|}\log\frac{2AM%
    \log T}{\delta}\right\rceil$15             Pull $a$ for $n_{m,a}^{p}$ times on
    the targeted LLM16             Receive rewards $\set{r_{a,t}}_{t\in\mathcal{T}_{m,a}^{p}}$
    on the LLM response17            18      19      $\begin{aligned} &\text{Upload
    }\bm{G}_{m}^{p}=\sum_{k\in\mathcal{K}_{m}^{p}}n_% {m,k}^{p}\tilde{\bm{x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}}+\sum_{a\in\mathcal{%
    A}_{m}^{p}}n_{m,a}^{p}\bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}\text{, and}\\ &\bm{W}_{m}^{p}=\sum_{t\in\bigcup_{k\in\mathcal{K}_{m}^{p}}\mathcal{\widetilde%
    {T}}_{m,k}^{p}}\widetilde{r}_{k,t}\tilde{\bm{x}}_{k,t}+\sum_{t\in\bigcup_{a\in%
    \mathcal{A}_{m}^{p}}\mathcal{T}_{m,a}^{p}}r_{a,t}\bm{x}_{a,t}\end{aligned}$20      
    Download $\widehat{\bm{\theta}}_{p}$ from the cloud server21       Update the
    active LLM response set $\mathcal{A}_{m}^{p+1}$ by eliminating sub-optimal LLM
    responses: $\displaystyle\mathcal{A}_{m}^{p+1}=\set{a\in\mathcal{A}_{m}^{p}:\max_{a^{%
    \prime}\in\mathcal{A}_{m}^{p}}\left\langle\widehat{\bm{\theta}}_{p},\bm{x}_{a^%
    {\prime}}-\bm{x}_{a}\right\rangle\leq\frac{2^{-p+1}}{\sqrt{M}}}$22        $p=p+1$23'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '输入：回合时间 $T$，本地智能体数量 $M$，输入维度 $d$，臂集 $\mathcal{A}_{m}$，臂池大小 $A$，置信度参数 $\delta
    \in (0, 1]$  '
- en: Algorithm 1 MACO on Local Agent (MACO-A)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 本地智能体上的 MACO（MACO-A）
- en: As outlined in Algorithm [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), which
    is executed by the local agents and referred to as MACO Agent (MACO-A), the online
    process of handling and updating information for LLM response identification within
    the multi-agent system operates as follows. Initially, the local agent $m\in{\mathcal{M}}$
    computes the information matrix $\bm{M}_{m}^{p}$ from its active arm set $\mathcal{A}_{m}^{p}$
    (which is later updated in Line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")) during
    each phase $p$. Specifically, $\bm{M}_{m}^{p}$ is calculated as $\bm{M}_{m}^{p}\coloneqq\sum_{a\in\mathcal{A}_{m}^{p}}\frac{1}{|\mathcal{A}_{m}%
    ^{p}|}\bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}$, which refines the model’s ability to
    adapt to LLM responses by analyzing the principal directions in the feature space
    (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm 1 ‣ III-A
    MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")). The eigenvalue $\lambda_{\bm{v}}$
    of its eigenvector $\bm{v}$ represents the variance captured along its direction,
    with higher values indicating richer information, which is essential for the precise
    estimation of $\bm{\theta}^{*}$. Following this, the local agent $m$ diagonalizes
    its information matrix $\bm{M}_{m}^{p}=\sum_{j=1}^{d}\lambda_{\bm{v}_{j}}\bm{v}_{j}\bm{v}_{j}^{\mathsf%
    {T}}$, examining all principal directions in the feature space (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). If
    an eigenvalue $\lambda_{\bm{v}_{j}}$ falls below the threshold $h_{p}\coloneqq\frac{3}{4(1-2^{-2p})d}$,
    whose value is determined by Lemma [1](https://arxiv.org/html/2501.01849v1#Thmlemma1
    "Lemma 1 (Stability of the Information Matrix). ‣ IV-B Technical Analysis ‣ IV
    Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification") in Section [IV](https://arxiv.org/html/2501.01849v1#S4
    "IV Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"), the local agent $m$ uploads the corresponding eigenvector
    to the cloud server (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm
    1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")). This mechanism helps
    to address under-explored areas of the feature space, enhancing the accuracy in
    selecting LLM responses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如算法[1](https://arxiv.org/html/2501.01849v1#algorithm1 "算法 1 ‣ III-A 本地代理上的 MACO
    算法 ‣ III 算法设计 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")中所述，由本地代理执行并称为 MACO 代理（MACO-A），处理和更新信息以进行
    LLM 响应识别的在线过程在多智能体系统中如下进行。最初，本地代理 $m\in{\mathcal{M}}$ 根据其活跃臂集 $\mathcal{A}_{m}^{p}$（该臂集在第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A 本地代理上的 MACO 算法 ‣ III 算法设计 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")行中更新）计算信息矩阵
    $\bm{M}_{m}^{p}$。具体地，$\bm{M}_{m}^{p}$ 计算为 $\bm{M}_{m}^{p}\coloneqq\sum_{a\in\mathcal{A}_{m}^{p}}\frac{1}{|\mathcal{A}_{m}^{p}|}\bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}$，通过分析特征空间中的主方向，细化模型对
    LLM 响应的适应能力（第[1](https://arxiv.org/html/2501.01849v1#algorithm1 "算法 1 ‣ III-A
    本地代理上的 MACO 算法 ‣ III 算法设计 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")行）。其特征向量 $\bm{v}$ 的特征值 $\lambda_{\bm{v}}$
    表示沿其方向捕获的方差，较高的值表示包含更多信息，这对于准确估计 $\bm{\theta}^{*}$ 至关重要。随后，本地代理 $m$ 对其信息矩阵 $\bm{M}_{m}^{p}=\sum_{j=1}^{d}\lambda_{\bm{v}_{j}}\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}$
    进行对角化，检查特征空间中的所有主方向（第[1](https://arxiv.org/html/2501.01849v1#algorithm1 "算法 1
    ‣ III-A 本地代理上的 MACO 算法 ‣ III 算法设计 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")行）。如果某个特征值 $\lambda_{\bm{v}_{j}}$
    低于阈值 $h_{p}\coloneqq\frac{3}{4(1-2^{-2p})d}$，其值由第[1](https://arxiv.org/html/2501.01849v1#Thmlemma1
    "引理 1（信息矩阵的稳定性） ‣ IV-B 技术分析 ‣ IV 性能分析 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")节中的引理确定，在[IV](https://arxiv.org/html/2501.01849v1#S4
    "IV 性能分析 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")节中给出，则本地代理 $m$ 将相应的特征向量上传至云服务器（第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A 本地代理上的 MACO 算法 ‣ III 算法设计 ‣ 自适应 LLM 响应识别的多智能体对话在线学习")行）。此机制有助于解决特征空间中未充分探索的区域，从而提高选择
    LLM 响应的准确性。
- en: The cloud server processes the uploaded information and returns a set of key
    terms $\mathcal{K}_{m}^{p}$ along with the required repetition times $\{n_{m,k}^{p}\}_{k\in\mathcal{K}_{m}^{p}}$
    (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm 1 ‣ III-A
    MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")). The local agent $m$
    then engages in conversations with these key terms while pulling arms the requisite
    number of times, to ensure robust exploration of LLM responses. During this process,
    the local agent has the flexibility to intersperse the querying of key terms with
    arm pulls (Lines [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm
    1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")-[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). Note
    that The procedures of conducting conversations and Pulling arms are presented
    sequentially for clarity, but can be executed in parallel or interleaved without
    strict ordering. The local agent then uploads the corresponding information of
    pulled arms, key terms, and observed rewards, which are stored in the matrices
    $\bm{G}_{m}^{p}$ and $\bm{W}_{m}^{p}$ (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). Finally,
    the local agent downloads the updated preference parameter $\widehat{\bm{\theta}}_{p}$
    from the cloud server, and revises its active arm set, eliminating less effective
    arms based on the updated user preference estimations (Line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). This
    adaptive adjustment process allows each local agent to maintain high responsiveness
    and accuracy in LLM response identification, which caters to user-specific needs
    and preferences while preserving data privacy by sharing only aggregated data
    ($\bm{G}_{m}^{p}$ and $\bm{W}_{m}^{p}$) with the cloud server.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务器处理上传的信息，并返回一组关键术语$\mathcal{K}_{m}^{p}$，以及所需的重复次数$\{n_{m,k}^{p}\}_{k\in\mathcal{K}_{m}^{p}}$（第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 自适应LLM响应识别的多代理对话在线学习")行）。本地代理$m$随后与这些关键术语进行对话，并进行相应次数的臂拉取，以确保对LLM响应的充分探索。在此过程中，本地代理可以灵活地在查询关键术语和进行臂拉取之间插入操作（第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 自适应LLM响应识别的多代理对话在线学习")-第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 自适应LLM响应识别的多代理对话在线学习")行）。请注意，进行对话和拉臂的过程是按顺序呈现的，以便清晰表达，但可以并行执行或交替进行，而不要求严格的顺序。本地代理随后上传所拉取臂的相关信息、关键术语和观察到的奖励，这些数据存储在矩阵$\bm{G}_{m}^{p}$和$\bm{W}_{m}^{p}$中（第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 自适应LLM响应识别的多代理对话在线学习")行）。最后，本地代理从云服务器下载更新后的偏好参数$\widehat{\bm{\theta}}_{p}$，并根据更新后的用户偏好估计修订其活动臂集，淘汰效果较差的臂（第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 自适应LLM响应识别的多代理对话在线学习")行）。这一自适应调整过程使得每个本地代理能够保持高响应性和准确性，识别LLM响应，满足用户特定的需求和偏好，同时通过仅与云服务器共享聚合数据（$\bm{G}_{m}^{p}$和$\bm{W}_{m}^{p}$），保护数据隐私。
- en: III-B MACO Algorithm on Cloud Server
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 云服务器上的MACO算法
- en: 'Input: Key term set $\mathcal{K}$, coverage parameter $\beta$ in Condition [1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "Condition 1 (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification").Initialization:
    Let $p=1,\bm{G}=\bm{0},\bm{W}=\bm{0}$12while *$T$ has not been reached* do3        foreach *$m\in{\mathcal{M}}$* do4            
    Receive all eigenvectors uploaded by local agent $m$, and denote this set as $\mathcal{S}_{m}$5            
    Initialize the set of key terms at phase $p$ as $\mathcal{K}_{m}^{p}=\emptyset$6            7            foreach *$\bm{v}_{j}\in\mathcal{S}_{m}$* do8                    $k=\argmax_{i\in\mathcal{K}}\tilde{\bm{x}}_{i}^{\mathsf{T}}\bm{v}_{j}$,
    $\mathcal{K}_{m}^{p}=\mathcal{K}_{m}^{p}\cup\set{k}$9                    $n_{m,k}^{p}=\left\lceil\frac{\frac{3}{2(1-2^{-2p})}-2d\lambda_{\bm{v}_{j}}}{%
    \beta^{2}2^{-2p}}\log\frac{2AM\log T}{\delta}\right\rceil$10                  11            Send
    $\mathcal{K}_{m}^{p}$ and $\set{n_{m,k}^{p}}_{\bm{k}\in\mathcal{K}_{m}^{p}}$ to
    local agent $m$12             Receive $\bm{G}_{m}^{p}$ and $\bm{W}_{m}^{p}$ from
    local agent $m$13            14      $\bm{G}=\sum_{p\in[p]}\sum_{m\in{\mathcal{M}}}\bm{G}_{m}^{p},\
    \ \bm{W}=\sum_{p% \in[p]}\sum_{m\in{\mathcal{M}}}\bm{W}_{m}^{p}$15       Broadcast
    $\widehat{\bm{\theta}}_{p}=\bm{G}^{-1}\bm{W}$ to all local agents16        $p=p+1$17'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：关键术语集合$\mathcal{K}$，覆盖参数$\beta$，见条件[1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "条件 1（特征空间覆盖）。 ‣ IV-A 主要结果 ‣ IV 性能分析 ‣ 多智能体对话在线学习用于自适应LLM响应识别")。初始化：令$p=1,\bm{G}=\bm{0},\bm{W}=\bm{0}$。12当*达到$T$时*停止3      
    对于*每个$m\in{\mathcal{M}}$*：4             接收本地代理$m$上传的所有特征向量，并将该集合记作$\mathcal{S}_{m}$。5            
    将第$p$阶段的关键术语集合初始化为$\mathcal{K}_{m}^{p}=\emptyset$。6            7            对于*$\bm{v}_{j}\in\mathcal{S}_{m}$*：8                    $k=\argmax_{i\in\mathcal{K}}\tilde{\bm{x}}_{i}^{\mathsf{T}}\bm{v}_{j}$，$\mathcal{K}_{m}^{p}=\mathcal{K}_{m}^{p}\cup\set{k}$。9                    $n_{m,k}^{p}=\left\lceil\frac{\frac{3}{2(1-2^{-2p})}-2d\lambda_{\bm{v}_{j}}}{\beta^{2}2^{-2p}}\log\frac{2AM\log
    T}{\delta}\right\rceil$。10                  11            将$\mathcal{K}_{m}^{p}$和$\set{n_{m,k}^{p}}_{\bm{k}\in\mathcal{K}_{m}^{p}}$发送给本地代理$m$。12            
    接收本地代理$m$的$\bm{G}_{m}^{p}$和$\bm{W}_{m}^{p}$。13            14      $\bm{G}=\sum_{p\in[p]}\sum_{m\in{\mathcal{M}}}\bm{G}_{m}^{p},\
    \ \bm{W}=\sum_{p\in[p]}\sum_{m\in{\mathcal{M}}}\bm{W}_{m}^{p}$。15       将$\widehat{\bm{\theta}}_{p}=\bm{G}^{-1}\bm{W}$广播给所有本地代理。16        $p=p+1$。17
- en: Algorithm 2 MACO on Cloud Server (MACO-S)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 云服务器上的MACO（MACO-S）
- en: Next, we present the part of the MACO algorithm, which is executed on the cloud
    server, called MACO Server (MACO-S). As mentioned in Section [I](https://arxiv.org/html/2501.01849v1#S1
    "I Introduction ‣ Multi-Agent Conversational Online Learning for Adaptive LLM
    Response Identification"), a significant challenge arises from the heterogeneity
    of local agents in the multi-agent conversational bandits model. This diversity
    can hinder effective data aggregation, potentially leading to suboptimal estimation
    of the user preference vector $\bm{\theta}^{*}$. To address this issue, the cloud
    server employs a strategic approach using key terms to probe and enrich the information
    in underrepresented directions of the feature space, thereby enhancing the overall
    accuracy of the estimation process.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍MACO算法中在云服务器上执行的部分，称为MACO服务器（MACO-S）。正如在第[I](https://arxiv.org/html/2501.01849v1#S1
    "I 引言 ‣ 多智能体对话在线学习用于自适应LLM响应识别")节中提到的，来自多智能体对话老虎机模型的本地代理异质性是一个重大挑战。这种多样性可能会阻碍有效的数据聚合，从而导致用户偏好向量$\bm{\theta}^{*}$的估计不准确。为了解决这一问题，云服务器采用了一种战略性方法，通过使用关键术语来探测并丰富特征空间中表现不足的方向，从而提高整个估计过程的准确性。
- en: As detailed in Algorithm [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), the
    cloud server first receives eigenvectors representing directions with insufficient
    information about the LLM response space from each local agent (Line [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). Utilizing
    these insights, the cloud server identifies and selects key terms by calculating
    the closest match in terms of the inner product with the underexplored directions.
    The chosen key term $k\in\mathcal{K}$, along with the designated repetition times
    $n_{m,k}^{p}$, is then communicated back to the respective local agents (Line [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). This
    targeted intervention allows for focused exploration and refinement of LLM responses
    related to these key terms. Finally, the cloud server aggregates the enriched
    data from all local agents. This aggregated data is used to estimate the unknown
    preference parameter $\bm{\theta}^{*}$ via linear regression, effectively minimizing
    uncertainty and enhancing the model’s ability to predict and adapt LLM responses
    tailored to user preferences (Lines [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")-[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")). Moreover,
    $\bm{G}$ can also be initialized as an identity matrix to ensure invertibility,
    especially when the dimension $d$ is large.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如算法[2](https://arxiv.org/html/2501.01849v1#algorithm2 "Algorithm 2 ‣ III-B MACO
    Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")中详细描述的，云服务器首先接收每个本地代理提供的特征向量，这些向量表示具有不足信息的LLM响应空间方向（第[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")行）。利用这些洞察力，云服务器通过计算与未充分探索方向的内积最接近的匹配项，识别并选择关键术语。所选择的关键术语$k\in\mathcal{K}$及指定的重复次数$n_{m,k}^{p}$随后会传回各个本地代理（第[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")行）。这种针对性的干预允许集中探索和优化与这些关键术语相关的LLM响应。最后，云服务器将所有本地代理收集到的丰富数据进行汇总。汇总数据用于通过线性回归估算未知的偏好参数$\bm{\theta}^{*}$，有效地减少不确定性，并增强模型预测和适应基于用户偏好的LLM响应的能力（第[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")- [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")行）。此外，$\bm{G}$也可以初始化为单位矩阵，以确保其可逆性，特别是在维度$d$较大时。
- en: III-C Comparative Analysis
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 比较分析
- en: 'Generally, as mentioned in Section [I](https://arxiv.org/html/2501.01849v1#S1
    "I Introduction ‣ Multi-Agent Conversational Online Learning for Adaptive LLM
    Response Identification"), the number of LLM responses needing online identification
    from an initial set generated by prompt engineering is typically finite. Therefore,
    we employ phase elimination-based algorithms for linear bandits, referred to as
    PE-Lin, instead of the classical conversational bandit framework proposed by [[17](https://arxiv.org/html/2501.01849v1#bib.bib17)].
    This choice is motivated by the better performance guarantees of PE-Lin under
    finite arm sets. Our work builds upon and improves the classical PE-Lin [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)].
    In PE-Lin, a learning agent always estimates the unknown preference vector $\bm{\theta}^{*}$
    using optimal least squares design. Specifically, the algorithm minimizes prediction
    variance by implementing the computing-intensive *G-optimal design*, a probability
    distribution over the arm feature vector set $\mathcal{X}\subset{\mathbb{R}}^{d}$
    (represented by distribution policy $\pi:\mathcal{X}\to[0,1]$), to ensure minimal
    variance $g(\pi)$. The conditions are defined as [[29](https://arxiv.org/html/2501.01849v1#bib.bib29)]:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[I节](https://arxiv.org/html/2501.01849v1#S1 "I Introduction ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")中提到的，LLM响应的数量通常是有限的，这些响应需要从通过提示工程生成的初始集合中进行在线识别。因此，我们采用基于阶段消除的线性老虎机算法，称为PE-Lin，而不是由[[17](https://arxiv.org/html/2501.01849v1#bib.bib17)]提出的经典对话老虎机框架。这个选择的动机在于，PE-Lin在有限臂集下提供了更好的性能保证。我们的工作建立并改进了经典的PE-Lin[[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]。在PE-Lin中，学习代理始终使用最优最小二乘设计来估计未知的偏好向量$\bm{\theta}^{*}$。具体而言，该算法通过实施计算密集型的*G-optimal设计*，最小化预测方差，*G-optimal设计*是一个在臂特征向量集$\mathcal{X}\subset{\mathbb{R}}^{d}$上的概率分布（由分布策略$\pi:\mathcal{X}\to[0,1]$表示），以确保最小方差$g(\pi)$。条件定义如下[[29](https://arxiv.org/html/2501.01849v1#bib.bib29)]：
- en: '|  | $\displaystyle\sum_{\bm{x}\in\mathcal{X}}\pi(\bm{x})$ | $\displaystyle=1,\quad\bm{M}_{m}^{p}(\pi)=\sum_{\bm{x}\in\mathcal{X}}\pi(\bm{x}%
    )\bm{x}\bm{x}^{\mathsf{T}},$ |  | (2) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sum_{\bm{x}\in\mathcal{X}}\pi(\bm{x})$ | $\displaystyle=1,\quad\bm{M}_{m}^{p}(\pi)=\sum_{\bm{x}\in\mathcal{X}}\pi(\bm{x}%
    )\bm{x}\bm{x}^{\mathsf{T}},$ |  | (2) |'
- en: '|  | $\displaystyle g(\pi)$ | $\displaystyle=\max_{\bm{x}\in\mathcal{X}}\&#124;\bm{x}\&#124;_{\bm{M}(\pi)^{-1}}^{2}=d.$
    |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle g(\pi)$ | $\displaystyle=\max_{\bm{x}\in\mathcal{X}}\&#124;\bm{x}\&#124;_{\bm{M}(\pi)^{-1}}^{2}=d.$
    |  |'
- en: Then the learning agent plays arms according to the policy $\pi$ for local agent
    $m$ at phase $p$, estimates the unknown parameter $\bm{\theta}^{*}$, and eliminates
    inferior arms accordingly. As noted in [[22](https://arxiv.org/html/2501.01849v1#bib.bib22)],
    there is currently no efficient algorithm for computing the *G-optimal design*
    in the multi-agent scenario.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，学习代理根据策略$\pi$在阶段$p$为本地代理$m$执行臂操作，估计未知参数$\bm{\theta}^{*}$，并相应地消除不良臂。如[[22](https://arxiv.org/html/2501.01849v1#bib.bib22)]中所述，目前尚无高效算法可以在多代理场景中计算*G-optimal设计*。
- en: We avoid using G-optimal design by leveraging the inherent multi-agent heterogeneity
    in LLM application, combined with an adaptive conversational mechanism to address
    this issue. MACO eliminates the need for the resource-intensive G-optimal design,
    thereby significantly reducing computation time and resources. Additionally, merely
    executing PE-Lin independently on each local agent with subsequent data aggregation
    by the server cloud may fail to minimize regret efficiently. This is because different
    agents may have distinct LLM response sets, resulting in a trivial regret bound
    of $\mathcal{\widetilde{O}}(M\sqrt{dT})$, which is equivalent to running PE-Lin
    on each agent without any direct communication. In contrast, our algorithm improves
    the regret upper bound to $\mathcal{\widetilde{O}}(\sqrt{dMT})$ via efficiently
    utilizing the conversation to aggregate the information from different local agents,
    which will be detailed in Section [IV](https://arxiv.org/html/2501.01849v1#S4
    "IV Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification").
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用LLM应用中的固有多代理异质性，结合自适应对话机制来避免使用G-optimal设计，从而解决这一问题。MACO消除了对资源密集型G-optimal设计的需求，显著减少了计算时间和资源。此外，仅在每个本地代理上独立执行PE-Lin，并通过服务器云进行数据聚合，可能无法有效地最小化遗憾。这是因为不同的代理可能有不同的LLM响应集，导致一个平凡的遗憾上界$\mathcal{\widetilde{O}}(M\sqrt{dT})$，这相当于在每个代理上运行PE-Lin而没有任何直接通信。相比之下，我们的算法通过有效利用对话来聚合来自不同本地代理的信息，将遗憾上界提升至$\mathcal{\widetilde{O}}(\sqrt{dMT})$，这一点将在第[IV](https://arxiv.org/html/2501.01849v1#S4
    "IV Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification")节中详细说明。
- en: IV Performance Analysis
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 性能分析
- en: This section presents the theoretical results of MACO, including its cumulative
    regret, communication costs, and conversation frequency. In line with common practices
    in [[21](https://arxiv.org/html/2501.01849v1#bib.bib21), [20](https://arxiv.org/html/2501.01849v1#bib.bib20)],
    we assume for any arm $a$ and key term $k$, $\|\bm{x}_{a}\|=\|\tilde{\bm{x}}_{k}\|=1$.
    The length of preference vector $\bm{\theta}^{*}$ is bounded by 1, and the noise
    terms $\eta_{m,t}$ and $\widetilde{\eta}_{m,t}$ are modeled as 1-subgaussian.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了MACO的理论结果，包括其累积遗憾、通信成本和对话频率。根据[[21](https://arxiv.org/html/2501.01849v1#bib.bib21),
    [20](https://arxiv.org/html/2501.01849v1#bib.bib20)]的常见做法，我们假设对于任何臂$ a $和关键术语$
    k $，$\|\bm{x}_{a}\|=\|\tilde{\bm{x}}_{k}\|=1$。偏好向量$\bm{\theta}^{*}$的长度被限制为1，噪声项$\eta_{m,t}$和$\widetilde{\eta}_{m,t}$被建模为1-子高斯分布。
- en: IV-A Main Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 主要结果
- en: We first present a “new technical condition” that addresses general issues related
    to feature space coverage.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提出一个“新技术条件”，解决与特征空间覆盖相关的普遍问题。
- en: Condition 1  (Feature Space Coverage).
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 条件1（特征空间覆盖）。
- en: We say a key term set $\mathcal{K}$ as *sufficiently rich* for covering the
    feature space if, for any unit vector $\bm{v}\in{\mathbb{R}}^{d}$, there exists
    a key term $k\in\mathcal{K}$ such that its feature vector $\tilde{\bm{x}}_{k}$
    satisfies $\tilde{\bm{x}}_{k}^{\mathsf{T}}\bm{v}\geq\beta$, where $\beta\in(0,1]$
    is a positive coverage parameter close to 1.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说一个关键术语集$\mathcal{K}$是*充分丰富的*，如果对于任何单位向量$\bm{v}\in{\mathbb{R}}^{d}$，都存在一个关键术语$k\in\mathcal{K}$，使得其特征向量$\tilde{\bm{x}}_{k}$满足$\tilde{\bm{x}}_{k}^{\mathsf{T}}\bm{v}\geq\beta$，其中$\beta\in(0,1]$是一个接近1的正覆盖参数。
- en: Remark 1.
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1。
- en: Condition [1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "Condition 1
    (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") is crucial
    for ensuring the comprehensive distribution of key terms across the feature space,
    which can facilitate effective uncertainty minimization for each local agent.
    This condition is easily met if the key term set $\mathcal{K}$ includes an orthonormal
    basis of ${\mathbb{R}}^{d}$. Condition [1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "Condition 1 (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")
    enables us to sidestep the *G-optimal design* procedure, typically employed in
    traditional elimination-based algorithms to minimize maximum prediction variance,
    as described in [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 [1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "条件 1（特征空间覆盖）。 ‣ IV-A
    主要结果 ‣ IV 性能分析 ‣ 自适应LLM响应识别的多智能体在线学习") 对于确保关键术语在特征空间中的全面分布至关重要，这有助于有效地最小化每个本地智能体的不确定性。如果关键术语集
    $\mathcal{K}$ 包含 ${\mathbb{R}}^{d}$ 的一个正交基，则该条件容易满足。条件 [1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "条件 1（特征空间覆盖）。 ‣ IV-A 主要结果 ‣ IV 性能分析 ‣ 自适应LLM响应识别的多智能体在线学习") 使我们能够绕过 *G-最优设计*
    程序，该程序通常在传统的基于淘汰的算法中使用，以最小化最大预测方差，如 [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]
    所描述。
- en: For sufficiently rich key term sets, based on Condition [1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "Condition 1 (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification"),
    we provide the following theorems.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 对于充分丰富的关键术语集，基于条件 [1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "条件
    1（特征空间覆盖）。 ‣ IV-A 主要结果 ‣ IV 性能分析 ‣ 自适应LLM响应识别的多智能体在线学习")，我们给出了以下定理。
- en: Theorem 1  (Regret Bounds).
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1（遗憾界限）。
- en: 'For the cumulative regret defined in Eq. [1](https://arxiv.org/html/2501.01849v1#S2.E1
    "Equation 1 ‣ II-B Multi-Agent User-Personalized Bandits ‣ II System Model ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), we
    have the following upper bound and lower bound:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在公式 [1](https://arxiv.org/html/2501.01849v1#S2.E1 "方程 1 ‣ II-B 多智能体用户个性化赌博机
    ‣ II 系统模型 ‣ 自适应LLM响应识别的多智能体在线学习") 中定义的累计遗憾，我们得出以下上界和下界：
- en: '1.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Upper Bound: With probability at least $1-\delta$, the regret is bounded above
    by $\mathcal{O}(\sqrt{dMT\log\frac{AM\log T}{\delta}})$.'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上界：以至少 $1-\delta$ 的概率，遗憾的上界为 $\mathcal{O}(\sqrt{dMT\log\frac{AM\log T}{\delta}})$。
- en: '2.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Lower Bound: For any policy that selects at most one key term per round, there
    exists an instance where the policy incurs an expected regret of at least $\Omega(\sqrt{dMT})$.'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下界：对于每回合最多选择一个关键术语的任何策略，存在一种情况，该策略的期望遗憾至少为 $\Omega(\sqrt{dMT})$。
- en: Remark 2.
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 2。
- en: 'The regret bounds established in Theorem [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "Theorem 1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") reveal
    important insights into the performance of our approach:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1 "定理 1（遗憾界限）。 ‣ IV-A 主要结果
    ‣ IV 性能分析 ‣ 自适应LLM响应识别的多智能体在线学习") 中建立的遗憾界限揭示了我们方法性能的重要见解：
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When $M=1$, the problem simplifies to single-agent conversational bandits, reducing
    the regret to $\mathcal{\widetilde{O}}(\sqrt{dT})$. This reduction outperforms
    previous regret upper bound results of $\mathcal{\widetilde{O}}(d\sqrt{T})$ from
    studies such as [[19](https://arxiv.org/html/2501.01849v1#bib.bib19), [17](https://arxiv.org/html/2501.01849v1#bib.bib17)],
    by leveraging phase elimination on finite arm sets. This improvement is particularly
    significant in high-dimensional LLM response feature vectors.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当 $M=1$ 时，问题简化为单一智能体对话型赌博机问题，将遗憾降至 $\mathcal{\widetilde{O}}(\sqrt{dT})$。这种简化优于以往的遗憾上界结果
    $\mathcal{\widetilde{O}}(d\sqrt{T})$，这一结果来自于 [[19](https://arxiv.org/html/2501.01849v1#bib.bib19),
    [17](https://arxiv.org/html/2501.01849v1#bib.bib17)] 等研究，利用了有限臂集上的阶段淘汰方法。这个改进在高维LLM响应特征向量中尤其重要。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For multi-agent systems, our upper bound result aligns with the nearly optimal
    results described in [[22](https://arxiv.org/html/2501.01849v1#bib.bib22), [24](https://arxiv.org/html/2501.01849v1#bib.bib24)],
    while eliminating the reliance on computationally intensive *G-optimal design*,
    thereby speeding up the online process.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于多代理系统，我们的上界结果与[[22](https://arxiv.org/html/2501.01849v1#bib.bib22), [24](https://arxiv.org/html/2501.01849v1#bib.bib24)]中描述的几乎最优结果一致，同时消除了对计算密集型*G-最优设计*的依赖，从而加快了在线过程。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Collectively, the regret upper and lower bound indicate that MACO is minimax
    optimal up to a logarithmic factor [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)],
    aligning closely with the theoretical regret bounds in multi-agent conversational
    bandits scenarios.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 综合来看，后悔的上界和下界表明，MACO在对数因子上是最小最大最优的[[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]，并与多代理对话赌博情境中的理论后悔边界紧密对接。
- en: Theorem 2  (Communication Cost).
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 2 （通信成本）。
- en: The total communication cost scales in $\mathcal{O}(d^{2}M\log T)$ for MACO
    algorithm.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MACO算法，总通信成本在$\mathcal{O}(d^{2}M\log T)$的量级。
- en: Remark 3.
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 3。
- en: The communication cost of our algorithm MACO is notably independent of the arm
    pool size $A$, which can range into thousands based on the diversity of candidate
    LLM responses. This contrasts with the approach described in [[22](https://arxiv.org/html/2501.01849v1#bib.bib22)],
    where the communication cost scales as $O(d^{2}AM\log T)$, reflecting a substantial
    increase with the number of arms. Our approach significantly reduces communication
    costs by eliminating the need for each local agent to upload its entire active
    arm set, whose cardinality is $\mathcal{O}(A)$. Instead, local agents independently
    process their data and transmit only aggregated results to the cloud server, which
    also enhances privacy by limiting external data sharing in LLM response adaptations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的MACO算法的通信成本显著独立于臂池大小$A$，该大小根据候选LLM响应的多样性可达到数千。这与[[22](https://arxiv.org/html/2501.01849v1#bib.bib22)
    ]中描述的方法形成对比，该方法的通信成本随着臂数增加按$O(d^{2}AM\log T)$的比例增长。我们的方法通过消除每个局部代理上传其整个活动臂集（其基数为$\mathcal{O}(A)$）的需求，显著减少了通信成本。相反，局部代理独立处理其数据并仅将汇总结果传输到云服务器，这也通过限制LLM响应适配中的外部数据共享增强了隐私。
- en: Theorem 3  (Bound on Conversation Frequency).
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 3 （对话频率的界限）。
- en: 'For any local agent $m\in{\mathcal{M}}$ during phase $p$, let $\gamma=\lambda_{\text{min}}(\bm{M}_{m}^{p})$,
    where $\lambda_{\text{min}}$ denotes the smallest eigenvalue, we have:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意局部代理$m\in{\mathcal{M}}$在阶段$p$期间，令$\gamma=\lambda_{\text{min}}(\bm{M}_{m}^{p})$，其中$\lambda_{\text{min}}$表示最小特征值，我们有：
- en: '1.'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: If $\gamma\geq h_{p}$, no conversations will be initiated.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果$\gamma\geq h_{p}$，则不会发起任何对话。
- en: '2.'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: If $\gamma<h_{p}$, the fraction of conversations relative to the total phase
    length is capped at $\beta^{-2}(\frac{3}{4(1-2^{-2p})}-d\gamma)$.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果$\gamma<h_{p}$，则相对于总阶段长度的对话比例上限为$\beta^{-2}(\frac{3}{4(1-2^{-2p})}-d\gamma)$。
- en: Remark 4.
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 4。
- en: Our approach introduces an “adaptive” method that differs significantly from
    the common deterministic functions $b(t)$, such as linear or logarithmic dependencies
    on round $t$, as widely employed in existing studies on conversational bandits
    [[17](https://arxiv.org/html/2501.01849v1#bib.bib17), [19](https://arxiv.org/html/2501.01849v1#bib.bib19)].
    These traditional methods initiate conversations at fixed intervals, which can
    lead to inefficiencies, especially when user preferences are already well-understood.
    In contrast, our model dynamically adjusts the conversation frequency based on
    the current gaps in user preference information, offering a more realistic and
    responsive interaction paradigm.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法引入了一种“自适应”方法，这与常见的确定性函数$b(t)$有显著区别，例如在线性或对回合$t$的对数依赖性等，在现有对话赌博研究中广泛使用[[17](https://arxiv.org/html/2501.01849v1#bib.bib17),
    [19](https://arxiv.org/html/2501.01849v1#bib.bib19)]。这些传统方法在固定间隔启动对话，这可能导致低效，特别是当用户偏好已经被充分理解时。相反，我们的模型基于当前用户偏好信息的空白动态调整对话频率，提供了更为真实和响应的交互范式。
- en: IV-B Technical Analysis
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 技术分析
- en: We now provide an analysis of the upper bound in Theorem [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "Theorem 1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"). Proofs
    for other theorems can be found in [Sections A-C](https://arxiv.org/html/2501.01849v1#A1.SS3
    "A-C Proof of Regret Lower Bound in Theorem 1 ‣ Appendix A Appendix ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), [A-D](https://arxiv.org/html/2501.01849v1#A1.SS4
    "A-D Proof of Theorem 2 ‣ Appendix A Appendix ‣ Multi-Agent Conversational Online
    Learning for Adaptive LLM Response Identification") and [A-E](https://arxiv.org/html/2501.01849v1#A1.SS5
    "A-E Proof of Theorem 3 ‣ Appendix A Appendix ‣ Multi-Agent Conversational Online
    Learning for Adaptive LLM Response Identification"). Below, we present two critical
    lemmas related to the design of our multi-agent conversational bandit algorithm.
    [Lemma 1](https://arxiv.org/html/2501.01849v1#Thmlemma1 "Lemma 1 (Stability of
    the Information Matrix). ‣ IV-B Technical Analysis ‣ IV Performance Analysis ‣
    Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")
    guarantees that for any local agent $m$, the smallest eigenvalue of the information
    matrix, adjusted for conversational feedback, remains above $h_{p}$. This supports
    the design of line [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm
    1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification") in Algorithm [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"). [Lemma 2](https://arxiv.org/html/2501.01849v1#Thmlemma2
    "Lemma 2 (Reliability of Estimation Error Bounds). ‣ IV-B Technical Analysis ‣
    IV Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification") ensures that the algorithm operates within established
    error limits, which is essential for reliable LLM response identification.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们提供定理[1](https://arxiv.org/html/2501.01849v1#Thmtheorem1 "定理 1（后悔界限）。 ‣
    IV-A 主要结果 ‣ IV 性能分析 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")中的上界分析。其他定理的证明可以在[附录 A-C](https://arxiv.org/html/2501.01849v1#A1.SS3
    "A-C 后悔下界的证明 ‣ 附录 A 附录 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")，[A-D](https://arxiv.org/html/2501.01849v1#A1.SS4
    "A-D 定理 2 的证明 ‣ 附录 A 附录 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别") 和[A-E](https://arxiv.org/html/2501.01849v1#A1.SS5
    "A-E 定理 3 的证明 ‣ 附录 A 附录 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")中找到。下面，我们展示了与我们多代理对话式算法设计相关的两个关键引理。[引理
    1](https://arxiv.org/html/2501.01849v1#Thmlemma1 "引理 1（信息矩阵的稳定性）。 ‣ IV-B 技术分析
    ‣ IV 性能分析 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")保证，对于任何本地代理$m$，经过对话反馈调整后的信息矩阵的最小特征值始终大于$h_{p}$。这支持算法[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A 本地代理上的 MACO 算法 ‣ III 算法设计 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")中的设计。[引理
    2](https://arxiv.org/html/2501.01849v1#Thmlemma2 "引理 2（估计误差界限的可靠性）。 ‣ IV-B 技术分析
    ‣ IV 性能分析 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")确保算法在既定的误差界限内运行，这对于可靠的大规模语言模型响应识别至关重要。
- en: Lemma 1  (Stability of the Information Matrix).
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 1（信息矩阵的稳定性）。
- en: For any local agent $m\in{\mathcal{M}}$ during phase $p$, we have $\lambda_{\text{min}}(\bm{M}_{m}^{p^{\prime}})\geq
    h_{p},$ where $\bm{M}_{m}^{p^{\prime}}\coloneqq\bm{M}_{m}^{p}+\sum_{k\in\mathcal{K}_{m}^{p}}%
    \frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}}$.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何本地代理$m\in{\mathcal{M}}$在阶段$p$期间，我们有$\lambda_{\text{min}}(\bm{M}_{m}^{p^{\prime}})\geq
    h_{p},$ 其中$\bm{M}_{m}^{p^{\prime}}\coloneqq\bm{M}_{m}^{p}+\sum_{k\in\mathcal{K}_{m}^{p}}%
    \frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}}$。
- en: Proof.
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Please refer to Appendix [A-A](https://arxiv.org/html/2501.01849v1#A1.SS1 "A-A
    Proof of Lemma 1 ‣ Appendix A Appendix ‣ Multi-Agent Conversational Online Learning
    for Adaptive LLM Response Identification") for the proof. ∎
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅附录[A-A](https://arxiv.org/html/2501.01849v1#A1.SS1 "A-A 证明 － 引理 1 ‣ 附录 A
    附录 ‣ 多代理对话式在线学习用于自适应大规模语言模型响应识别")中的证明。∎
- en: Lemma 2  (Reliability of Estimation Error Bounds).
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2（估计误差界限的可靠性）。
- en: 'Define the “bad” event $\mathcal{E}$ where any local agent $m$ at phase $p$
    has:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 定义“坏”事件$\mathcal{E}$，即在阶段$p$时，任何本地代理$m$具有：
- en: '|  | $\mathcal{E}=\{\exists m\in{\mathcal{M}},a\in\mathcal{A}_{m}^{p},\left&#124;\langle%
    \widehat{\bm{\theta}}_{p}-\bm{\theta}^{*},\bm{x}_{a}\rangle\right&#124;>\frac{2^{-p%
    }}{\sqrt{M}}\}.$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{E}=\{\exists m\in{\mathcal{M}},a\in\mathcal{A}_{m}^{p},\left&#124;\langle%
    \widehat{\bm{\theta}}_{p}-\bm{\theta}^{*},\bm{x}_{a}\rangle\right&#124;>\frac{2^{-p%
    }}{\sqrt{M}}\}.$ |  |'
- en: The probability of $\mathcal{E}$ is bounded by $\delta$, i.e., $\Pr[\mathcal{E}]\leq\delta$.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 事件$\mathcal{E}$的概率被$\delta$所限制，即$\Pr[\mathcal{E}]\leq\delta$。
- en: Proof.
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: See Appendix [A-B](https://arxiv.org/html/2501.01849v1#A1.SS2 "A-B Proof of
    Lemma 2 ‣ Appendix A Appendix ‣ Multi-Agent Conversational Online Learning for
    Adaptive LLM Response Identification") for details. ∎
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请见附录[A-B](https://arxiv.org/html/2501.01849v1#A1.SS2 "A-B 引理 2 的证明 ‣ 附录 A
    ‣ 多代理会话在线学习用于自适应LLM响应识别")。∎
- en: 'Now, consider the “good” event $\mathcal{E}^{c}$ for agent $m$ at phase $p$.
    Lemma [2](https://arxiv.org/html/2501.01849v1#Thmlemma2 "Lemma 2 (Reliability
    of Estimation Error Bounds). ‣ IV-B Technical Analysis ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")
    confirms that the discrepancy for any arm $a$ in $\mathcal{A}_{m}^{p}$: $\langle\bm{x}_{a}-\bm{x}_{a_{m}^{*}},\widehat{\bm{\theta}}_{p}\rangle\leq\frac%
    {2^{-p+1}}{\sqrt{M}}.$ This, combined with line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") in Algorithm
    [1](https://arxiv.org/html/2501.01849v1#algorithm1 "Algorithm 1 ‣ III-A MACO Algorithm
    on Local Agent ‣ III Algorithm Design ‣ Multi-Agent Conversational Online Learning
    for Adaptive LLM Response Identification"), supports the following lemma on the
    arm preservation and performance bound under good event $\mathcal{E}^{c}$.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑阶段$p$下代理$m$的“良好”事件$\mathcal{E}^{c}$。引理[2](https://arxiv.org/html/2501.01849v1#Thmlemma2
    "引理 2（估计误差界限的可靠性）。 ‣ IV-B 技术分析 ‣ IV 性能分析 ‣ 多代理会话在线学习用于自适应LLM响应识别")确认，对于$\mathcal{A}_{m}^{p}$中的任意臂$a$，其差异$\langle\bm{x}_{a}-\bm{x}_{a_{m}^{*}},\widehat{\bm{\theta}}_{p}\rangle\leq\frac%
    {2^{-p+1}}{\sqrt{M}}$。结合算法[1](https://arxiv.org/html/2501.01849v1#algorithm1 "算法
    1 ‣ III-A MACO 算法在局部代理上 ‣ III 算法设计 ‣ 多代理会话在线学习用于自适应LLM响应识别")中第[1]行的内容，支持以下引理，说明在良好事件$\mathcal{E}^{c}$下，臂的保持和性能界限。
- en: Lemma 3  (Properties Under Good Event).
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 3（良好事件下的性质）。
- en: 'Under event $\mathcal{E}^{c}$, for any local agent $m$ at phase $p$, two key
    properties are ensured:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在事件$\mathcal{E}^{c}$下，对于阶段$p$的任何局部代理$m$，保证以下两个关键性质：
- en: '1.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The locally optimal arm $\bm{a}_{m}^{*}$ remains within the active arm set $\mathcal{A}_{m}^{p}$,
    ensuring it is never eliminated.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局部最优臂$\bm{a}_{m}^{*}$始终位于活跃臂集$\mathcal{A}_{m}^{p}$中，确保其永不被淘汰。
- en: '2.'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The performance gap for any arm $a\in\mathcal{A}_{m}^{p}$, defined as $\Delta_{m,a}\triangleq\left\langle\bm{\theta}^{*},\bm{x}_{a_{m}^{*}}-\bm{x}_{a%
    }\right\rangle$, is bounded by $\frac{2^{-p+3}}{\sqrt{M}}$.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任意臂$a\in\mathcal{A}_{m}^{p}$的性能差距定义为$\Delta_{m,a}\triangleq\left\langle\bm{\theta}^{*},\bm{x}_{a_{m}^{*}}-\bm{x}_{a%
    }\right\rangle$，其上界为$\frac{2^{-p+3}}{\sqrt{M}}$。
- en: Finally, with probability $1-\delta$, the cumulative regret $R_{M}(T)$ $=\sum_{m=1}^{M}\sum_{t=1}^{T}\left\langle\bm{\theta}^{*},\bm{x}_{a_{m}^{*}}-%
    \bm{x}_{a_{m},t}\right\rangle$ is bounded by $\sum_{m=1}^{M}\sum_{p=1}^{P}\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\frac{2^{%
    -p+3}}{\sqrt{M}}$, where $P$ denotes the total number of phases. Given that $\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\leq$$2^{-2p+1}d\log\frac{2AM\log
    T}{\delta}+|\mathcal{A}_{m}^{p}|$, we derive that $R_{M}(T)\leq\mathcal{O}\left(d\sqrt{M}\log\frac{AM\log
    T}{\delta}2^{P}\right).$ Furthermore, $T\geq\sum_{p=1}^{P}\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\geq\sum_{p=1}^{P}%
    2^{-2p+1}d\log\frac{2AM\log T}{\delta}$, which simplifies to $T\geq 2d2^{2P}\log\frac{AM\log
    T}{\delta}$. Thus, $R_{M}(T)\leq\mathcal{O}\left(\sqrt{dMT\log\frac{KM\log T}{\delta}}\right)$.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以概率$1-\delta$，累计遗憾$R_{M}(T)$ $=\sum_{m=1}^{M}\sum_{t=1}^{T}\left\langle\bm{\theta}^{*},\bm{x}_{a_{m}^{*}}-%
    \bm{x}_{a_{m},t}\right\rangle$的上界为$\sum_{m=1}^{M}\sum_{p=1}^{P}\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\frac{2^{%
    -p+3}}{\sqrt{M}}$，其中$P$表示阶段的总数。由于$\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\leq$$2^{-2p+1}d\log\frac{2AM\log
    T}{\delta}+|\mathcal{A}_{m}^{p}|$，我们得到$R_{M}(T)\leq\mathcal{O}\left(d\sqrt{M}\log\frac{AM\log
    T}{\delta}2^{P}\right)$。进一步地，$T\geq\sum_{p=1}^{P}\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}\geq\sum_{p=1}^{P}%
    2^{-2p+1}d\log\frac{2AM\log T}{\delta}$，简化为$T\geq 2d2^{2P}\log\frac{AM\log T}{\delta}$。因此，$R_{M}(T)\leq\mathcal{O}\left(\sqrt{dMT\log\frac{KM\log
    T}{\delta}}\right)$。
- en: V Performance Evaluation
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 性能评估
- en: '![Refer to caption](img/883bbeb60a4e762b4f2dfcc093191f66.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/883bbeb60a4e762b4f2dfcc093191f66.png)'
- en: 'Figure 2: Cumulative regret of Response Setting 1 on two embedding models from
    Google and OpenAI across different arm pool sizes $A$.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在不同臂池大小$A$下，谷歌和OpenAI的两个嵌入模型在响应设置1上的累积遗憾。
- en: 'In this section, we conduct extensive experiments to demonstrate the effectiveness
    of our algorithm.¹¹1Our experimental setup does not assume any prior knowledge
    of user preferences or reward distributions, thus requiring more trial rounds.
    Although practical scenarios often have pre-existing information that could reduce
    initial exploration, our study focuses on the performance of online learning algorithms
    without this offline information. The code is accessible at the following link:
    [Code Repository](https://github.com/TarferSoul/MACO).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行广泛的实验以证明我们算法的有效性。¹¹1我们的实验设置不假设用户偏好或奖励分布的任何先验知识，因此需要更多的试验轮次。尽管实际场景通常具有可以减少初始探索的预先信息，但我们的研究重点在于没有这些离线信息时在线学习算法的表现。代码可以通过以下链接访问：[代码仓库](https://github.com/TarferSoul/MACO)。
- en: V-A Experimental Settings
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 实验设置
- en: 'Embedding Models. We demonstrate our framework’s generalization capabilities
    using two open embedding models: Google’s text-embedding-preview-0409 and OpenAI’s
    Text-embedding-3-large, which generate the embedding feature vector $\bm{x}_{a}\in{\mathbb{R}}^{d}$
    for the corresponding arm $a$ (i.e., response) to capture text information.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型。我们使用两个开放的嵌入模型演示了我们框架的泛化能力：谷歌的text-embedding-preview-0409和OpenAI的Text-embedding-3-large，这些模型为对应的臂$a$（即响应）生成嵌入特征向量$\bm{x}_{a}\in{\mathbb{R}}^{d}$，以捕捉文本信息。
- en: '1.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Text-embedding-preview-0409: Google’s advanced embedding model, which streamlines
    synthetic training data creation by generating queries and task descriptions [[30](https://arxiv.org/html/2501.01849v1#bib.bib30)].'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Text-embedding-preview-0409：谷歌的高级嵌入模型，通过生成查询和任务描述来简化合成训练数据的创建[[30](https://arxiv.org/html/2501.01849v1#bib.bib30)]。
- en: '2.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Text-embedding-3-large: OpenAI’s new generation embedding model, which surpasses
    its predecessor, though its technical details remain undisclosed [[31](https://arxiv.org/html/2501.01849v1#bib.bib31)].'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Text-embedding-3-large：OpenAI的新一代嵌入模型，超越了其前身，尽管其技术细节尚未公开[[31](https://arxiv.org/html/2501.01849v1#bib.bib31)]。
- en: Response Settings. We explore the implementation of two response settings using
    the aforementioned embedding models, based on a real-world dataset and an open-source
    LLM.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 响应设置。我们基于一个真实世界的数据集和一个开源LLM，探索了使用上述嵌入模型的两种响应设置的实现。
- en: '1.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Following the style classification by [[32](https://arxiv.org/html/2501.01849v1#bib.bib32)],
    we gather a comprehensive set of 13 keywords representing diverse styles such
    as “humorous” and “helpful”, each representing a key term. These keyword styles
    generate 510 unique combinations, each forming an “arm”, where each arm represents
    a potential style of LLM response. Users have varying priorities for different
    keyword combinations, and their preference vector $\bm{\theta}$ has the highest
    cosine similarity with the feature vector $\bm{x}$ of their most favored keyword
    style (which is unknown to the algorithms in advance). To generate these feature
    vectors $\bm{x}$ for LLM responses and user preference vectors $\bm{\theta}$ on
    keywords, we utilize two previously mentioned embedding models. We select the
    top $d=256$ dimensions as the feature representation and normalized them into
    a more concise and efficient dimensional space. The reward is obtained from the
    cosine similarity between a specific user’s preference vector and the feature
    vector of the selected arm, and the optimal LLM response is defined as the one
    with the largest reward according to [[33](https://arxiv.org/html/2501.01849v1#bib.bib33)].
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据[[32](https://arxiv.org/html/2501.01849v1#bib.bib32)]的风格分类，我们收集了一套包含13个关键词的全面集合，代表了“幽默”和“有帮助”等不同风格，每个关键词代表一个关键术语。这些关键词风格生成了510个独特的组合，每个组合形成一个“臂”，其中每个臂代表LLM响应的潜在风格。用户对不同关键词组合有不同的优先级，他们的偏好向量$\bm{\theta}$与其最喜欢的关键词风格的特征向量$\bm{x}$的余弦相似度最高（该信息在算法中是未知的）。为了生成这些LLM响应的特征向量$\bm{x}$和用户对关键词的偏好向量$\bm{\theta}$，我们利用了前面提到的两个嵌入模型。我们选择了前$d=256$个维度作为特征表示，并将其归一化为更简洁、高效的维度空间。奖励是通过特定用户的偏好向量与选定臂的特征向量之间的余弦相似度获得的，最优的LLM响应被定义为根据[[33](https://arxiv.org/html/2501.01849v1#bib.bib33)]，具有最大奖励的响应。
- en: '2.'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Prompt engineering is utilized to construct the initiatory set of responses
    offline. Following [[34](https://arxiv.org/html/2501.01849v1#bib.bib34)], we select
    a set of keyword styles (i.e., key term) rich in personal identifiers to establish
    a diverse style collection, including terms like helpful, and creative use of
    emojis. Two keyword styles are jointly selected for each query, which forms a
    style-specific question to the LLM, ensuring focused and relevant responses. We
    utilize Llama-3-8B-Instruct [[35](https://arxiv.org/html/2501.01849v1#bib.bib35)]
    to generate corresponding responses. Each prompt triggers a specific response
    from the LLM, with each user preference dictating a response styled according
    to their selected input. For example, User: ”Tell me a joke.” The response Arm:
    A variety of jokes under different styles. Key-term: Different styles. By formulating
    responses to five different questions, each with two keyword styles, we construct
    a total arm set of $|\mathcal{A}|=455$ responses. This extensive collection allows
    for a comprehensive mapping of responses to specific user preferences, effectively
    forming a set of $455$ user-preference pairs. Regarding the reward definition,
    the feature vector extraction, and subsequent steps, we apply the same procedures
    described above.'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程用于离线构建初始响应集。根据[[34](https://arxiv.org/html/2501.01849v1#bib.bib34)]，我们选择一组富含个人标识符的关键词风格（即关键词），建立一个多样化的风格集合，包括诸如“有帮助”和富有创意的表情符号使用等术语。每个查询选择两种关键词风格，共同形成一个针对LLM的风格特定问题，确保响应集中且相关。我们利用Llama-3-8B-Instruct
    [[35](https://arxiv.org/html/2501.01849v1#bib.bib35)]生成相应的响应。每个提示触发LLM的特定响应，每个用户偏好根据其选择的输入样式生成响应。例如，用户：“讲个笑话。”
    响应臂：不同风格的笑话。关键词：不同风格。通过设计五个不同问题的响应，每个问题使用两种关键词风格，我们构建了一个总共有$|\mathcal{A}|=455$个响应的臂集。这个庞大的集合使得可以全面地映射响应到特定用户偏好，从而有效地形成$455$个用户偏好对。关于奖励定义、特征向量提取及后续步骤，我们应用上述相同的程序。
- en: Comparison Algorithms. The following online learning algorithms from existing
    studies are used as baselines, each executed individually on different local agents.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 比较算法。以下来自现有研究的在线学习算法作为基准，每个算法在不同的本地代理上单独执行。
- en: •
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TRIPLE-SH [[8](https://arxiv.org/html/2501.01849v1#bib.bib8)]: Select optimal
    prompts for LLMs by adaptively eliminating arms with poor performance, where we
    directly set each arm as the corresponding LLM response.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TRIPLE-SH [[8](https://arxiv.org/html/2501.01849v1#bib.bib8)]：通过自适应消除表现较差的臂，选择LLM的最佳提示词，其中我们直接将每个臂设置为对应的LLM响应。
- en: •
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'LinUCB [[21](https://arxiv.org/html/2501.01849v1#bib.bib21)]: Online select
    arms and estimate user preference for *infinite* arm sets, excluding the conversational
    setting.'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LinUCB [[21](https://arxiv.org/html/2501.01849v1#bib.bib21)]：在线选择臂并估计用户偏好，适用于*无限*臂集，排除对话场景。
- en: •
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Arm-Con [[36](https://arxiv.org/html/2501.01849v1#bib.bib36)]: Initiate conversations
    on user preference about arms, and use LinUCB for arm selection.'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Arm-Con [[36](https://arxiv.org/html/2501.01849v1#bib.bib36)]：启动关于臂的用户偏好的对话，并使用LinUCB进行臂选择。
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ConUCB [[17](https://arxiv.org/html/2501.01849v1#bib.bib17)]: Query key terms
    if conversations are allowed and utilize conversational feedback to accelerate
    learning.'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ConUCB [[17](https://arxiv.org/html/2501.01849v1#bib.bib17)]：如果允许对话，则查询关键词，并利用对话反馈加速学习。
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ConLinUCB [[19](https://arxiv.org/html/2501.01849v1#bib.bib19)]: The series
    includes three algorithms: ConLinUCB-BS calculates the barycentric spanner for
    conducing conversations; ConLinUCB-MCR selects key terms with the largest confidence
    radius; ConLinUCB-UCB adopts a LinUCB-like method to choose key terms.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ConLinUCB [[19](https://arxiv.org/html/2501.01849v1#bib.bib19)]：该系列包括三种算法：ConLinUCB-BS计算对话的重心跨度；ConLinUCB-MCR选择具有最大置信半径的关键词；ConLinUCB-UCB采用类似LinUCB的方法来选择关键词。
- en: All results are averaged from five trials, conducted on a Linux Ubuntu machine
    (kernel 6.5.0) with a 5.40 GHz 13th Gen Intel(R) Core(TM) i7-13700KF CPU and 32GB
    RAM. We set coverage parameter $\beta=1$ and confidence parameter $\delta=0.1$,
    and conduct an ablation study to ensure robustness.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所有结果来自五次试验的平均值，试验在一台运行Linux Ubuntu（内核6.5.0）的机器上进行，配置为5.40 GHz的13代Intel(R) Core(TM)
    i7-13700KF处理器和32GB内存。我们设置了覆盖参数$\beta=1$和置信参数$\delta=0.1$，并进行了一项消融研究以确保结果的鲁棒性。
- en: V-B Evaluation Results
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 评估结果
- en: '![Refer to caption](img/294353afc8cf892e6b9d126b5e4ef3d4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/294353afc8cf892e6b9d126b5e4ef3d4.png)'
- en: 'Figure 3: Cumulative regret of Response Setting 2 on two embedding models from
    Google and OpenAI across different numbers of agents $M$.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在不同数量的代理 $M$ 下，Google 和 OpenAI 的两个嵌入模型在响应设置 2 中的累积后悔值。
- en: Regret Across Different Arm Pool Sizes. We initially compare the cumulative
    regret of MACO against seven baseline algorithms under Scenario Setting 1 with
    $M=4$ local agents, employing the above two embedding models. We further explore
    the influence of varying arm pool sizes $A$, setting $A=40$ and $A=50$ under each
    embedding model respectively, and selecting $A$ arms at random from $\mathcal{A}$
    for the local agent. Fig. [2](https://arxiv.org/html/2501.01849v1#S5.F2 "Figure
    2 ‣ V Performance Evaluation ‣ Multi-Agent Conversational Online Learning for
    Adaptive LLM Response Identification") demonstrates that algorithms lacking a
    conversational mechanism (LinUCB and Arm-Con), exhibit the poorest performance.
    In contrast, our algorithm, MACO, significantly outperforms all competitors, achieving
    a minimum improvement of 8.29% compared to ConLinUCB-MCR, the best-performing
    baseline. This superior performance originates from the multi-agent framework
    employed by MACO, wherein the cloud server aggregates data from each local agent
    to more accurately estimate the unknown user preference. Notably, the increase
    in arm pool size $A$ does not significantly increase the cumulative regret for
    MACO, confirming Theorem [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1 "Theorem
    1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") which
    states that our algorithm’s regret growth increases at a square-root logarithmic
    rate with respect to arm pool size $A$.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 不同臂池大小下的后悔值。我们首先在场景设置 1 中比较了 MACO 与七个基线算法的累积后悔值，其中 $M=4$ 本地代理，并使用上述两种嵌入模型。我们进一步探索了不同臂池大小
    $A$ 的影响，分别设置 $A=40$ 和 $A=50$，在每个嵌入模型下随机从 $\mathcal{A}$ 中选择 $A$ 个臂给本地代理。图 [2](https://arxiv.org/html/2501.01849v1#S5.F2
    "Figure 2 ‣ V Performance Evaluation ‣ Multi-Agent Conversational Online Learning
    for Adaptive LLM Response Identification") 显示，缺乏对话机制的算法（LinUCB 和 Arm-Con）表现最差。相比之下，我们的算法
    MACO 显著优于所有竞争者，相较于表现最好的基线 ConLinUCB-MCR，最小提升为 8.29%。这种卓越的表现来源于 MACO 所采用的多代理框架，在该框架下，云服务器汇总每个本地代理的数据，更准确地估计未知的用户偏好。值得注意的是，臂池大小
    $A$ 的增加并未显著增加 MACO 的累积后悔值，验证了定理 [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "Theorem 1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")，该定理指出我们算法的后悔增长随着臂池大小
    $A$ 的增大而以平方根对数的速率增长。
- en: '![Refer to caption](img/abab26228ec7faa1da069e6b64212fdb.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/abab26228ec7faa1da069e6b64212fdb.png)'
- en: 'Figure 4: Cumulative regret under various number of local agents.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同本地代理数量下的累积后悔值。
- en: 'Regret Across Different Number of Local Agents. We next examine the regret
    under Scenario Setting 2 with arm pool size $A=40$, while using the embedding
    models above. Additionally, we assess the impact of varying the number of local
    agents, setting $M=8$ and $M=12$. We consider more agents here because, in practice,
    platforms often group users with similar labels to share learning, making $M$
    naturally larger. Therefore, we aim to explore our algorithm’s performance with
    larger $M$ for a comprehensive demonstration. Fig. [3](https://arxiv.org/html/2501.01849v1#S5.F3
    "Figure 3 ‣ V-B Evaluation Results ‣ V Performance Evaluation ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification") presents four subfigures
    that illustrate consistent trends: in the absence of a multi-agent framework,
    the cumulative regrets of all baseline algorithms increase linearly with the number
    of local agents, following a $\mathcal{\widetilde{O}}(dM\sqrt{T})$ pattern. Conversely,
    MACO capitalizes on the aggregated data from all local agents, managing to scale
    its regret according to $\mathcal{\widetilde{O}}(\sqrt{dMT})$. This scaling significantly
    dampens the increase in regret, demonstrating the effectiveness of our algorithm’s
    multi-agent approach for online LLM response identification. A clearer depiction
    of this regret trend is shown in Fig. [4](https://arxiv.org/html/2501.01849v1#S5.F4
    "Figure 4 ‣ V-B Evaluation Results ‣ V Performance Evaluation ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification"), where TRIPLE-SH is
    excluded due to its inferior performance, under Scenario Setting 1 with the Google’s
    model and $T=100000$.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 不同本地代理数量的遗憾。接下来，我们在场景设置 2 下，使用上面的嵌入模型，考察了在手臂池大小 $A=40$ 下的遗憾。此外，我们评估了不同本地代理数量对结果的影响，设置
    $M=8$ 和 $M=12$。我们在这里考虑更多的代理，因为在实际应用中，平台通常会将具有相似标签的用户分组进行共享学习，这使得 $M$ 自然较大。因此，我们旨在通过较大的
    $M$ 来探索我们算法的性能，以进行全面的展示。图 [3](https://arxiv.org/html/2501.01849v1#S5.F3 "图 3 ‣
    V-B 评估结果 ‣ V 性能评估 ‣ 多代理对话在线学习用于自适应 LLM 响应识别") 展示了四个子图，说明了一致的趋势：在没有多代理框架的情况下，所有基准算法的累积遗憾随着本地代理数量的增加而线性增加，遵循
    $\mathcal{\widetilde{O}}(dM\sqrt{T})$ 模式。相反，MACO 利用来自所有本地代理的聚合数据，能够根据 $\mathcal{\widetilde{O}}(\sqrt{dMT})$
    进行遗憾的缩放。这种缩放显著抑制了遗憾的增加，展示了我们算法在在线 LLM 响应识别中的多代理方法的有效性。图 [4](https://arxiv.org/html/2501.01849v1#S5.F4
    "图 4 ‣ V-B 评估结果 ‣ V 性能评估 ‣ 多代理对话在线学习用于自适应 LLM 响应识别") 更清晰地展示了这一遗憾趋势，在此图中，TRIPLE-SH
    被排除，因为其性能较差，且在场景设置 1 下，使用 Google 的模型和 $T=100000$。
- en: 'TABLE I: Execution time (s) ($\pm$ standard deviation) on four settings.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：在四种设置下的执行时间（秒）（$\pm$ 标准差）。
- en: '| <svg height="12.22" overflow="visible" version="1.1" width="61.92"><g transform="translate(0,12.22)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">Setting</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(30.96,6.07)"><g transform="translate(0,6.15)
    scale(1, -1)"><foreignobject height="6.15" overflow="visible" width="30.96">Algorithm</foreignobject></g></g></g></svg>
    | MACO (w/o G) | MACO (w/G) | ConLinUCB-BS |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| <svg height="12.22" overflow="visible" version="1.1" width="61.92"><g transform="translate(0,12.22)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">设置</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(30.96,6.07)"><g transform="translate(0,6.15)
    scale(1, -1)"><foreignobject height="6.15" overflow="visible" width="30.96">算法</foreignobject></g></g></g></svg>
    | MACO (无 G) | MACO (有 G) | ConLinUCB-BS |'
- en: '| --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Setting (a) | $2.576\pm 0.047$ | $9.766\pm 2.709$ | $18.124\pm 0.111$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (a) | $2.576\pm 0.047$ | $9.766\pm 2.709$ | $18.124\pm 0.111$ |'
- en: '| Setting (b) | $2.546\pm 0.039$ | $14.272\pm 7.107$ | $18.056\pm 0.065$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (b) | $2.546\pm 0.039$ | $14.272\pm 7.107$ | $18.056\pm 0.065$ |'
- en: '| Setting (c) | $2.576\pm 0.085$ | $6.369\pm 2.832$ | $17.926\pm 0.095$ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (c) | $2.576\pm 0.085$ | $6.369\pm 2.832$ | $17.926\pm 0.095$ |'
- en: '| Setting (d) | $2.661\pm 0.056$ | $6.270\pm 2.013$ | $17.919\pm 0.072$ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (d) | $2.661\pm 0.056$ | $6.270\pm 2.013$ | $17.919\pm 0.072$ |'
- en: Comparison of Execution Time. We assess the execution time of our algorithm,
    termed MACO w/o G for emphasis, against ConLinUCB-BS (previously identified as
    the fastest in [[19](https://arxiv.org/html/2501.01849v1#bib.bib19)]) under conditions
    of $T=5000$ across 6 phases ($A=40,M=4$), and compare it with MACO w/G, which
    continues to employ the traditional G-optimal design. For clarity, the results
    on text-embedding-preview-0409 and text-embedding-3-large under Response Settings
    1, 2 are abbreviated as Settings (a), (b), (c), and (d). The results, detailed
    in [Table I](https://arxiv.org/html/2501.01849v1#S5.T1 "In V-B Evaluation Results
    ‣ V Performance Evaluation ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"), show that our algorithm significantly reduces execution
    time by avoiding the G-optimal design and leveraging data aggregation from multiple
    local agents to accelerate the learning process. [Table I](https://arxiv.org/html/2501.01849v1#S5.T1
    "In V-B Evaluation Results ‣ V Performance Evaluation ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification") further illustrates
    that MACO w/o G exhibits the lowest deviation since the information matrix $\bm{M}_{m}^{p}$
    is no longer dependent on a continuously adjusted distribution policy (see Eq.
    ([2](https://arxiv.org/html/2501.01849v1#S3.E2 "Equation 2 ‣ III-C Comparative
    Analysis ‣ III Algorithm Design ‣ Multi-Agent Conversational Online Learning for
    Adaptive LLM Response Identification"))). Additionally, the results in [Table II](https://arxiv.org/html/2501.01849v1#S5.T2
    "In V-B Evaluation Results ‣ V Performance Evaluation ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification") show that the average
    reward for MACO w/o G matches that of MACO w/G, demonstrating that our conversational
    approach maintains performance while replacing the traditional G-optimal design
    with a more practical, conversation-based design. This not only sustains robust
    performance, as supported by Theorem [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "Theorem 1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), but
    also enhances efficiency, representing an interesting finding.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 执行时间比较。我们评估了算法的执行时间，将其命名为MACO不含G以示强调，并与ConLinUCB-BS（在[[19](https://arxiv.org/html/2501.01849v1#bib.bib19)]中被认为是最快的算法）进行了比较。实验条件为$T=5000$，在6个阶段（$A=40,M=4$）下进行，并与继续采用传统G-optimal设计的MACO含G进行比较。为了清晰起见，Response
    Settings 1和2下的text-embedding-preview-0409和text-embedding-3-large的结果简称为Settings
    (a)、(b)、(c)和(d)。结果详细见[表I](https://arxiv.org/html/2501.01849v1#S5.T1 "V-B评估结果 ‣
    V 性能评估 ‣ 多代理对话在线学习用于自适应LLM响应识别")，表明我们的算法通过避免使用G-optimal设计，并利用多个本地代理的数据聚合来加速学习过程，从而显著减少了执行时间。[表I](https://arxiv.org/html/2501.01849v1#S5.T1
    "V-B评估结果 ‣ V 性能评估 ‣ 多代理对话在线学习用于自适应LLM响应识别")进一步说明，MACO不含G的偏差最小，因为信息矩阵$\bm{M}_{m}^{p}$不再依赖于持续调整的分布策略（见公式([2](https://arxiv.org/html/2501.01849v1#S3.E2
    "公式2 ‣ III-C 比较分析 ‣ III 算法设计 ‣ 多代理对话在线学习用于自适应LLM响应识别")))。此外，[表II](https://arxiv.org/html/2501.01849v1#S5.T2
    "V-B评估结果 ‣ V 性能评估 ‣ 多代理对话在线学习用于自适应LLM响应识别")的结果表明，MACO不含G的平均奖励与MACO含G相匹配，证明我们的对话方法在替代传统的G-optimal设计后，仍能保持性能，同时采用更实用的基于对话的设计。这不仅维持了强健的性能，正如定理[1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "定理1（遗憾界限）。 ‣ IV-A 主要结果 ‣ IV 性能分析 ‣ 多代理对话在线学习用于自适应LLM响应识别")所支持的那样，而且提高了效率，代表了一个有趣的发现。
- en: 'TABLE II: Average reward ($\pm$ standard deviation) on four settings.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：四种设置下的平均奖励（$\pm$ 标准差）。
- en: '| <svg height="12.22" overflow="visible" version="1.1" width="61.92"><g transform="translate(0,12.22)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">Setting</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(30.96,6.07)"><g transform="translate(0,6.15)
    scale(1, -1)"><foreignobject height="6.15" overflow="visible" width="30.96">Algorithm</foreignobject></g></g></g></svg>
    | MACO (w/o G) | MACO (w/G) | ConLinUCB-BS |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| <svg height="12.22" overflow="visible" version="1.1" width="61.92"><g transform="translate(0,12.22)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">设置</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(30.96,6.07)"><g transform="translate(0,6.15)
    scale(1, -1)"><foreignobject height="6.15" overflow="visible" width="30.96">算法</foreignobject></g></g></g></svg>
    | MACO (不含G) | MACO (含G) | ConLinUCB-BS |'
- en: '| --- | --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Setting (a) | $61.849\pm 0.558$ | $61.847\pm 0.565$ | $59.811\pm 0.610$ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (a) | $61.849\pm 0.558$ | $61.847\pm 0.565$ | $59.811\pm 0.610$ |'
- en: '| Setting (b) | $61.605\pm 0.642$ | $61.591\pm 0.649$ | $59.663\pm 0.671$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (b) | $61.605\pm 0.642$ | $61.591\pm 0.649$ | $59.663\pm 0.671$ |'
- en: '| Setting (c) | $47.405\pm 0.977$ | $47.381\pm 1.002$ | $46.104\pm 0.962$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (c) | $47.405\pm 0.977$ | $47.381\pm 1.002$ | $46.104\pm 0.962$ |'
- en: '| Setting (d) | $41.770\pm 0.349$ | $41.858\pm 0.412$ | $40.720\pm 0.349$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 设置 (d) | $41.770\pm 0.349$ | $41.858\pm 0.412$ | $40.720\pm 0.349$ |'
- en: Ablation Study. [Table III](https://arxiv.org/html/2501.01849v1#S5.T3 "In V-B
    Evaluation Results ‣ V Performance Evaluation ‣ Multi-Agent Conversational Online
    Learning for Adaptive LLM Response Identification") reveals that the introduction
    of the coverage parameter $\beta$ in our design has a minimal impact on the outcomes,
    contrasting with the significant influence exerted by the statistical confidence
    parameter $\delta$, which is established by convention [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)].
    This observation underscores that our framework does not introduce new dependencies
    on parameters beyond those traditionally used in bandit algorithms.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 消融研究。[表 III](https://arxiv.org/html/2501.01849v1#S5.T3 "在 V-B 评估结果 ‣ V 性能评估
    ‣ 多智能体对话在线学习用于自适应 LLM 响应识别")揭示了我们设计中引入的覆盖参数$\beta$对结果的影响很小，这与统计置信度参数$\delta$的显著影响形成对比，后者是按照惯例设定的[[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]。这一观察结果强调，我们的框架没有引入超出传统用于多臂赌博机算法的参数依赖。
- en: 'TABLE III: Cumulative regret under $T=100000,A=40,M=4$.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：在$T=100000,A=40,M=4$下的累积遗憾值。
- en: '| <svg height="10.8" overflow="visible" version="1.1" width="62.54"><g transform="translate(0,10.8)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,4.73)
    scale(1, -1)"><foreignobject height="4.73" overflow="visible" width="31.27">Parameter</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(41.02,4.73)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">Setting</foreignobject></g></g></g></svg>
    | Setting (a) | Setting (b) | Setting (c) | Setting (d) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| <svg height="10.8" overflow="visible" version="1.1" width="62.54"><g transform="translate(0,10.8)
    scale(1,-1)"><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,4.73)
    scale(1, -1)"><foreignobject height="4.73" overflow="visible" width="31.27">参数</foreignobject></g></g>
    <g class="ltx_svg_fog" transform="translate(41.02,4.73)"><g transform="translate(0,6.07)
    scale(1, -1)"><foreignobject height="6.07" overflow="visible" width="21.52">设置</foreignobject></g></g></g></svg>
    | 设置 (a) | 设置 (b) | 设置 (c) | 设置 (d) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $\beta=1.0,\delta=0.1$ | $20213.773$ | $16277.413$ | $15033.483$ | $8261.335$
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=1.0,\delta=0.1$ | $20213.773$ | $16277.413$ | $15033.483$ | $8261.335$
    |'
- en: '| $\beta=0.9,\delta=0.05$ | $21439.795$ | $17205.540$ | $16039.654$ | $8772.119$
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.9,\delta=0.05$ | $21439.795$ | $17205.540$ | $16039.654$ | $8772.119$
    |'
- en: '| $\beta=0.8,\delta=0.05$ | $21430.625$ | $17215.402$ | $16033.950$ | $8770.108$
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.8,\delta=0.05$ | $21430.625$ | $17215.402$ | $16033.950$ | $8770.108$
    |'
- en: '| $\beta=0.9,\delta=0.15$ | $19495.106$ | $15734.833$ | $15092.586$ | $7962.415$
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.9,\delta=0.15$ | $19495.106$ | $15734.833$ | $15092.586$ | $7962.415$
    |'
- en: '| $\beta=0.8,\delta=0.15$ | $19492.169$ | $15738.395$ | $15094.809$ | $7961.321$
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| $\beta=0.8,\delta=0.15$ | $19492.169$ | $15738.395$ | $15094.809$ | $7961.321$
    |'
- en: VI Related Work
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 相关工作
- en: Bandits tackle the exploitation-exploration tradeoff of online decision-making
    problems [[21](https://arxiv.org/html/2501.01849v1#bib.bib21)]. Based on this,
    conversational contextual linear bandits, introduced by [[17](https://arxiv.org/html/2501.01849v1#bib.bib17)],
    allow the cloud server to obtain user feedback on key terms to elicit preferences,
    in addition to arm selection. Later studies introduce clustering to avoid labeling
    efforts [[18](https://arxiv.org/html/2501.01849v1#bib.bib18)], integrate knowledge
    graphs for term selection [[27](https://arxiv.org/html/2501.01849v1#bib.bib27)],
    and compute the barycentric spanner as an efficient exploration basis [[19](https://arxiv.org/html/2501.01849v1#bib.bib19)].
    Regarding the multi-agent bandit setting under finite arm sets, [[26](https://arxiv.org/html/2501.01849v1#bib.bib26)]
    assumes homogeneous arm sets, and [[22](https://arxiv.org/html/2501.01849v1#bib.bib22)]
    requires the local agents to upload arm sets, increasing costs and privacy concerns,
    and [[24](https://arxiv.org/html/2501.01849v1#bib.bib24)] utilizes the computationally
    intensive G-optimal design. Unlike existing works, we are the first to extend
    conversational bandits to multi-agent settings for online LLM response adaptation,
    with reduced computation resources, where the theoretical analysis can be an independent
    component.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 强盗（Bandits）解决了在线决策问题中的**利用-探索**权衡[[21](https://arxiv.org/html/2501.01849v1#bib.bib21)]。基于此，**会话上下文线性强盗**（conversational
    contextual linear bandits），由[[17](https://arxiv.org/html/2501.01849v1#bib.bib17)]提出，使得云服务器不仅可以通过选择手臂，还能通过关键术语的用户反馈来引导偏好。后续研究引入了聚类方法以避免标注工作[[18](https://arxiv.org/html/2501.01849v1#bib.bib18)]，将知识图谱整合到术语选择中[[27](https://arxiv.org/html/2501.01849v1#bib.bib27)]，并计算**重心张量**作为有效的探索基础[[19](https://arxiv.org/html/2501.01849v1#bib.bib19)]。在有限手臂集的多智能体强盗设置下，[[26](https://arxiv.org/html/2501.01849v1#bib.bib26)]假设手臂集是同质的，而[[22](https://arxiv.org/html/2501.01849v1#bib.bib22)]要求本地智能体上传手臂集，从而增加了成本和隐私问题，[[24](https://arxiv.org/html/2501.01849v1#bib.bib24)]则利用了计算密集型的**G-最优设计**。与现有的研究不同，我们首次将会话强盗扩展到多智能体设置，用于在线大语言模型（LLM）响应的适配，同时减少计算资源，且其理论分析可以作为独立组件进行。
- en: Research on prompt learning for automatically generating suitable LLM responses
    has made significant progress [[4](https://arxiv.org/html/2501.01849v1#bib.bib4),
    [37](https://arxiv.org/html/2501.01849v1#bib.bib37)]. However, offline generating
    methods face challenges like “data drift,” emphasizing the need for online approaches
    to optimize LLM responses [[38](https://arxiv.org/html/2501.01849v1#bib.bib38),
    [7](https://arxiv.org/html/2501.01849v1#bib.bib7)]. [[39](https://arxiv.org/html/2501.01849v1#bib.bib39)]
    introduces an online non-stationary bandit method across different LLMs. [[8](https://arxiv.org/html/2501.01849v1#bib.bib8)]
    proposes an online budget-limited LLM response optimization using various prompts.
    And [[11](https://arxiv.org/html/2501.01849v1#bib.bib11)] focuses on response
    identification over multiple LLM coordination. Nevertheless, these studies ignore
    the impact of user preferences and the natural multi-agent setting in LLM response
    identification.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自动生成合适LLM响应的提示学习研究取得了显著进展[[4](https://arxiv.org/html/2501.01849v1#bib.bib4),
    [37](https://arxiv.org/html/2501.01849v1#bib.bib37)]。然而，离线生成方法面临着“数据漂移”等挑战，强调了需要在线方法来优化LLM响应[[38](https://arxiv.org/html/2501.01849v1#bib.bib38),
    [7](https://arxiv.org/html/2501.01849v1#bib.bib7)]。[[39](https://arxiv.org/html/2501.01849v1#bib.bib39)]提出了一种跨不同LLM的在线非平稳强盗方法。[[8](https://arxiv.org/html/2501.01849v1#bib.bib8)]提出了使用各种提示进行在线预算限制的LLM响应优化。而[[11](https://arxiv.org/html/2501.01849v1#bib.bib11)]则聚焦于多个LLM协调下的响应识别。然而，这些研究忽视了用户偏好和LLM响应识别中的自然多智能体设置的影响。
- en: VII Conclusion
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 结论
- en: This paper presents MACO, a multi-agent conversational online framework designed
    to identify optimal responses from LLMs while minimizing cumulative regret and
    aligning with user preferences. The framework consists of local agents (MACO-A)
    that adaptively manage conversations and response selection, and a cloud server
    (MACO-S) that aggregates data to learn user preferences efficiently. We have proved
    that MACO achieves optimal regret bounds, reduces conversations, and enhances
    computational efficiency. Our extensive evaluations, utilizing open LLMs like
    Llama and embedding models from Google and OpenAI, confirm that our approach significantly
    improves performance over traditional methods. Future work could explore clustering
    similar user preferences and extending beyond the linear reward model to further
    enhance the adaptability and effectiveness of the MACO framework.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了MACO，一个多智能体对话在线框架，旨在从大型语言模型（LLMs）中识别最优响应，同时最小化累积遗憾并与用户偏好对齐。该框架由本地智能体（MACO-A）组成，能够自适应地管理对话和响应选择，以及一个云服务器（MACO-S），它有效地聚合数据以学习用户偏好。我们已经证明，MACO实现了最优的遗憾界限，减少了对话次数，并提高了计算效率。通过广泛的评估，利用像Llama这样的开放LLM和来自Google和OpenAI的嵌入模型，我们证实了我们的方法在性能上明显优于传统方法。未来的工作可以探索聚类相似的用户偏好，并超越线性奖励模型，以进一步增强MACO框架的适应性和有效性。
- en: Appendix A Appendix
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A-A Proof of Lemma [1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "Condition
    1 (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A-A 引理[1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "条件 1（特征空间覆盖）。 ‣
    IV-A 主要结果 ‣ IV 性能分析 ‣ 多智能体对话在线学习用于自适应LLM响应识别")的证明
- en: Proof.
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Using the eigenvectors as an orthonormal basis, for any $j\in[d]$, any key term’s
    $k$ feature vector can be expressed as $\tilde{\bm{x}}_{k}=\sum_{i=1}^{d}c_{i}\bm{v}_{i}=\sum_{i=1,i\neq
    j}^{d}c_{i}% \bm{v}_{i}+c_{j}\bm{v}_{j}$, where $\bm{x}\coloneqq\sum_{i=1,i\neq
    j}^{d}c_{i}\bm{v}_{i}$ is orthogonal to $\bm{v}_{j}$. According to Line [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") of Algorithm [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") and
    Condition [1](https://arxiv.org/html/2501.01849v1#Thmcondition1 "Condition 1 (Feature
    Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification"), we have $\tilde{\bm{x}}_{k}^{\mathsf{T}}\bm{v}_{j}\geq\beta$
    for the selected key term $k$. Therefore, we have $(\sum_{i=1}^{d}c_{i}\bm{v}_{i})^{\mathsf{T}}\bm{v}_{j}=c_{j}\geq\beta$,
    and $\tilde{\bm{x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}}=(c_{j}\bm{v}_{j}+\bm{x})(c_{%
    j}\bm{v}_{j}+\bm{x})^{\mathsf{T}}=c_{j}^{2}\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}+%
    \bm{x}\bm{x}^{\mathsf{T}}$. By spectral decomposition and line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") in Algorithm [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), we
    have $\bm{M}_{m}^{p^{\prime}}=$ $\sum_{i=1}^{d}\lambda_{i}\bm{v}_{i}\bm{v}_{i}^{\mathsf{T}}+\sum_{j:\lambda_{j}%
    <h_{p}}\frac{h_{p}-\lambda_{j}}{C^{2}}\ab(c_{j}^{2}\bm{v}_{j}\bm{v}_{j}^{% \mathsf{T}}+\bm{x}\bm{x}^{\mathsf{T}})$.
    Then, $\bm{M}_{m}^{p^{\prime}}\succeq$$\sum_{i=1}^{d}\lambda_{m}\bm{v}_{m}\bm{v}_{m}^{\mathsf{T}}+\sum_{j:\lambda_{j}%
    <h_{p}}\ab(h_{p}-\lambda_{j})\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}$ $\succeq\sum_{i=1}^{d}\frac{3}{4(1-2^{-2p})d}\bm{v}_{m}\bm{v}_{m}^{\mathsf{T}}.$
    The proof concludes by the Loewner order property, stating if $\bm{A}\succeq\bm{B}$,
    then $\lambda_{j}(\bm{A})\geq\lambda_{j}(\bm{B})$. ∎
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征向量作为正交标准基，对于任意$j \in [d]$，任何关键术语的$k$特征向量可以表示为$\tilde{\bm{x}}_{k}=\sum_{i=1}^{d}c_{i}\bm{v}_{i}=\sum_{i=1,i\neq
    j}^{d}c_{i}\bm{v}_{i}+c_{j}\bm{v}_{j}$，其中$\bm{x} \coloneqq \sum_{i=1,i\neq j}^{d}c_{i}\bm{v}_{i}$与$\bm{v}_{j}$正交。根据算法[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")的第[2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")行以及条件[1](https://arxiv.org/html/2501.01849v1#Thmcondition1
    "Condition 1 (Feature Space Coverage). ‣ IV-A Main Results ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")，我们有$\tilde{\bm{x}}_{k}^{\mathsf{T}}\bm{v}_{j}\geq\beta$，对于所选择的关键术语$k$。因此，我们有$(\sum_{i=1}^{d}c_{i}\bm{v}_{i})^{\mathsf{T}}\bm{v}_{j}=c_{j}\geq\beta$，并且$\tilde{\bm{x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}}=(c_{j}\bm{v}_{j}+\bm{x})(c_{j}\bm{v}_{j}+\bm{x})^{\mathsf{T}}=c_{j}^{2}\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}+\bm{x}\bm{x}^{\mathsf{T}}$。通过谱分解和算法[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")中第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")行，我们得到$\bm{M}_{m}^{p^{\prime}}=$
    $\sum_{i=1}^{d}\lambda_{i}\bm{v}_{i}\bm{v}_{i}^{\mathsf{T}}+\sum_{j:\lambda_{j}<h_{p}}\frac{h_{p}-\lambda_{j}}{C^{2}}\ab(c_{j}^{2}\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}+\bm{x}\bm{x}^{\mathsf{T}})$。然后，$\bm{M}_{m}^{p^{\prime}}\succeq
    \sum_{i=1}^{d}\lambda_{m}\bm{v}_{m}\bm{v}_{m}^{\mathsf{T}} + \sum_{j:\lambda_{j}<h_{p}}\ab(h_{p}-\lambda_{j})\bm{v}_{j}\bm{v}_{j}^{\mathsf{T}}$
    $\succeq \sum_{i=1}^{d}\frac{3}{4(1-2^{-2p})d}\bm{v}_{m}\bm{v}_{m}^{\mathsf{T}}$。证明通过洛伊纳顺序性质得以完成，即如果$\bm{A}
    \succeq \bm{B}$，则$\lambda_{j}(\bm{A}) \geq \lambda_{j}(\bm{B})$。∎
- en: A-B Proof of Lemma [2](https://arxiv.org/html/2501.01849v1#Thmlemma2 "Lemma
    2 (Reliability of Estimation Error Bounds). ‣ IV-B Technical Analysis ‣ IV Performance
    Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response
    Identification")
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A-B 证明引理[2](https://arxiv.org/html/2501.01849v1#Thmlemma2 "Lemma 2 (Reliability
    of Estimation Error Bounds). ‣ IV-B Technical Analysis ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")
- en: Proof.
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'For any phase $p$, given $\bm{G}$’s definition in Algorithm [2](https://arxiv.org/html/2501.01849v1#algorithm2
    "Algorithm 2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"), it
    follows that $\bm{G}=\sum_{s=1}^{p}\sum_{m=1}^{M}\bm{G}_{m}^{p}\succeq 2d\log\ab(\frac{2AM%
    \log T}{\delta})$ $\sum_{m=1}^{M}\underbrace{\ab[\sum_{s=1}^{p}\frac{1}{2^{-2p}}(\sum_{a\in%
    \mathcal{A}_{m}^{p}}\frac{\bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}}{|\mathcal{A}_{m}^%
    {p}|}+\sum_{k\in\mathcal{K}_{m}^{p}}\frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{%
    x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}})]}_{\triangleq\bm{Q}_{m}^{p}}.$ By the
    Weyl’s inequality, we have the lower bound of the smallest eigenvalue of $\bm{Q}_{m}^{p}$:
    $\lambda_{\text{min}}\ab(\bm{Q}_{m}^{p})\geq$$\sum_{s=1}^{p}\frac{1}{2^{-2p}}\lambda_{\text{min}}\ab(\bm{M}_{m}^{p}+\sum_{k%
    \in\mathcal{K}_{m}^{p}}\frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{x}}_{k}\tilde%
    {\bm{x}}_{k}^{\mathsf{T}})$. By Lemma [1](https://arxiv.org/html/2501.01849v1#Thmlemma1
    "Lemma 1 (Stability of the Information Matrix). ‣ IV-B Technical Analysis ‣ IV
    Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification"), $\lambda_{\text{min}}\ab(\bm{Q}_{m}^{p})\geq$ $\sum_{s=1}^{p}\frac{1}{2^{-2p}}\frac{3}{4(1-2^{-2p})d}\geq\frac{3}{4(1-2^{-2p}%
    )d}\sum_{s=1}^{p}\frac{1}{2^{-2p}}=\frac{1}{d\cdot 2^{-2p}}$. Based on this, we
    have $\lambda_{\text{min}}\left(\bm{G}\right)\geq 2^{2p+1}M\log\frac{2AM\log T}{%
    \delta}.$ According to the concentration of linear regression in Chapter 20.1
    of [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)] (with the gram matrix
    refined as $\bm{G}$ for incorporating information from key terms), for any $\delta>0$,
    $s\in[p]$, $\bm{x}\in{\mathbb{R}}^{d}$, with probability at least $1-2\delta$,
    we have $\left|\left\langle\widehat{\bm{\theta}}_{s}-\bm{\theta}^{*},\bm{x}\right%
    \rangle\right|\leq\sqrt{2\|\bm{x}\|_{\bm{G}^{-1}}^{2}\log\frac{1}{\delta}}.$ Then,
    by the Courant-Fischer theorem, with probability at least $1-\frac{\delta}{AM\log
    T}$, for any $m\in{\mathcal{M}}$ and all arm $a\in\mathcal{A}_{m}^{p}$, we have
    $\left|\left\langle\widehat{\bm{\theta}}_{p}-\bm{\theta}^{*},\bm{x}_{a}\right%
    \rangle\right|\leq\sqrt{2\|\bm{x}_{a}\|_{\bm{G}^{-1}}^{2}\log\frac{2AM\log T}{%
    \delta}}$$\leq\sqrt{\frac{2}{\lambda_{\text{min}}(\bm{G})}\log\frac{2AM\log T}{\delta}}%
    \leq\frac{2^{-p}}{\sqrt{M}}.$ Finally, by the union bound, $\Pr\left[\mathcal{E}\right]\leq
    MPK\frac{\delta}{AM\log T}\leq\delta$ is obtained with $P\leq\log T$ (deduced
    from Section [IV-B](https://arxiv.org/html/2501.01849v1#S4.SS2 "IV-B Technical
    Analysis ‣ IV Performance Analysis ‣ Multi-Agent Conversational Online Learning
    for Adaptive LLM Response Identification"): $T\geq 2d2^{2P}\log\frac{AM\log T}{\delta}\geq
    2^{P}$). ∎'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任意阶段$p$，给定算法[2](https://arxiv.org/html/2501.01849v1#algorithm2 "Algorithm
    2 ‣ III-B MACO Algorithm on Cloud Server ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")中$\bm{G}$的定义，可以得到$\bm{G}=\sum_{s=1}^{p}\sum_{m=1}^{M}\bm{G}_{m}^{p}\succeq
    2d\log\ab(\frac{2AM% \log T}{\delta})$ $\sum_{m=1}^{M}\underbrace{\ab[\sum_{s=1}^{p}\frac{1}{2^{-2p}}(\sum_{a\in%
    \mathcal{A}_{m}^{p}}\frac{\bm{x}_{a}\bm{x}_{a}^{\mathsf{T}}}{|\mathcal{A}_{m}^%
    {p}|}+\sum_{k\in\mathcal{K}_{m}^{p}}\frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{%
    x}}_{k}\tilde{\bm{x}}_{k}^{\mathsf{T}})]}_{\triangleq\bm{Q}_{m}^{p}}$。根据魏尔不等式，我们可以得到$\bm{Q}_{m}^{p}$最小特征值的下界：$\lambda_{\text{min}}\ab(\bm{Q}_{m}^{p})\geq$$\sum_{s=1}^{p}\frac{1}{2^{-2p}}\lambda_{\text{min}}\ab(\bm{M}_{m}^{p}+\sum_{k%
    \in\mathcal{K}_{m}^{p}}\frac{h_{p}-\lambda}{\beta^{2}}\tilde{\bm{x}}_{k}\tilde%
    {\bm{x}}_{k}^{\mathsf{T}})$。根据引理[1](https://arxiv.org/html/2501.01849v1#Thmlemma1
    "Lemma 1 (Stability of the Information Matrix). ‣ IV-B Technical Analysis ‣ IV
    Performance Analysis ‣ Multi-Agent Conversational Online Learning for Adaptive
    LLM Response Identification")，$\lambda_{\text{min}}\ab(\bm{Q}_{m}^{p})\geq$ $\sum_{s=1}^{p}\frac{1}{2^{-2p}}\frac{3}{4(1-2^{-2p})d}\geq\frac{3}{4(1-2^{-2p}%
    )d}\sum_{s=1}^{p}\frac{1}{2^{-2p}}=\frac{1}{d\cdot 2^{-2p}}$。基于此，我们得到$\lambda_{\text{min}}\left(\bm{G}\right)\geq
    2^{2p+1}M\log\frac{2AM\log T}{% \delta}$。根据[[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]第20.1章中关于线性回归的集中性（通过将格拉姆矩阵细化为$\bm{G}$以纳入关键信息），对于任意$\delta>0$，$s\in[p]$，$\bm{x}\in{\mathbb{R}}^{d}$，以至少$1-2\delta$的概率，我们有$\left|\left\langle\widehat{\bm{\theta}}_{s}-\bm{\theta}^{*},\bm{x}\right%
    \rangle\right|\leq\sqrt{2\|\bm{x}\|_{\bm{G}^{-1}}^{2}\log\frac{1}{\delta}}$。然后，应用柯朗-费舍尔定理，以至少$1-\frac{\delta}{AM\log
    T}$的概率，对于任意$m\in{\mathcal{M}}$和所有臂$a\in\mathcal{A}_{m}^{p}$，我们有$\left|\left\langle\widehat{\bm{\theta}}_{p}-\bm{\theta}^{*},\bm{x}_{a}\right%
    \rangle\right|\leq\sqrt{2\|\bm{x}_{a}\|_{\bm{G}^{-1}}^{2}\log\frac{2AM\log T}{%
    \delta}}$$\leq\sqrt{\frac{2}{\lambda_{\text{min}}(\bm{G})}\log\frac{2AM\log T}{\delta}}%
    \leq\frac{2^{-p}}{\sqrt{M}}$。最后，通过并联合概率界，我们得到$\Pr\left[\mathcal{E}\right]\leq
    MPK\frac{\delta}{AM\log T}\leq\delta$，其中$P\leq\log T$（根据[IV-B](https://arxiv.org/html/2501.01849v1#S4.SS2
    "IV-B Technical Analysis ‣ IV Performance Analysis ‣ Multi-Agent Conversational
    Online Learning for Adaptive LLM Response Identification")部分推导：$T\geq 2d2^{2P}\log\frac{AM\log
    T}{\delta}\geq 2^{P}$）。∎
- en: A-C Proof of Regret Lower Bound in Theorem [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1
    "Theorem 1 (Regret Bounds). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定理 [1](https://arxiv.org/html/2501.01849v1#Thmtheorem1 "Theorem 1 (Regret Bounds).
    ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent Conversational Online
    Learning for Adaptive LLM Response Identification") 中的 A-C 后悔下界证明
- en: Proof.
  id: totrans-200
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Define $R_{M,\bm{\theta}}^{\pi}(T)$ as the expected cumulative regret of policy
    $\pi$ with user preference $\bm{\theta}$ over $M$ local agents and time horizon
    $T$. Assume that for all local agents $m$, the arms vectors can span ${\mathbb{R}}^{d}$,
    and $\{\bm{x}_{a}\}_{a\in\mathcal{A}_{m}}=$ $\{\bm{x}_{k}\}_{k\in\mathcal{K}}=$
    $\set{\bm{e}_{1},\bm{e}_{2},\dots,\bm{e}_{d}}\cup\set{(A-d)\text{ arbitrary %
    unit vectors}}$, where $\bm{e}_{i}$ is the $i$-th standard basis vector in ${\mathbb{R}}^{d}$.
    Choose $\bm{\theta}=(\Delta,0,\dots,0)^{\mathsf{T}}$ (with $\Delta\in[0,\frac{1}{2}]$
    to be determined later). Let random variables $N_{i}(t)$, $\widetilde{N}_{j}(t)$
    be the number of times the $i$-th arm and the $j$-th key term are selected, by
    the end of round $t$. Define another user preference $\bm{\theta}^{\prime}=(\Delta,0,\dots,2\Delta,\dots,0)^{\mathsf{T}}$,
    where $\theta_{\ell}=2\Delta$ and $\ell=\argmin_{j>1}\max\ab\{\operatorname*{\mathbb{E}}_{\bm{\theta}}[N_{j}(MT)]%
    ,\operatorname*{\mathbb{E}}_{\bm{\theta}}[\widetilde{N}_{j}(MT)]\}$. Denote $N_{m,a}(t)$
    as the number of times the $a$-th arm is chosen by local agent $m\in{\mathcal{M}}$
    after the end of round $t$. Given that the optimal arm for $\bm{\theta}$ is arm
    1, pulling other arms increases the expected regret by $\Delta$. Thus, by Lemma
    4.5 in [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)], $R_{M,\bm{\theta}}^{\pi}(T)=$
    $\sum_{m=1}^{M}\Delta\sum_{a=2}^{A}\operatorname*{\mathbb{E}}\nolimits_{\bm{%
    \theta}}[N_{m,a}(T)]]$. Using the inequality $\operatorname*{\mathbb{E}}_{\bm{\theta}}[N_{j}(MT)]\leq\frac{MT}{K-1}$
    and $\operatorname*{\mathbb{E}}_{\bm{\theta}}[\widetilde{N}_{j}(MT)]\leq\frac{MT}{K%
    -1}$ and Markov inequality, we get: $R_{M,\bm{\theta}}^{\pi}(T)$ $\geq\Delta\Pr\nolimits_{\bm{\theta}}\ab[MT-\sum_{m=1}^{M}N_{i,1}(T)\geq\frac{%
    MT}{2}]\frac{MT}{2}.$'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 $R_{M,\bm{\theta}}^{\pi}(T)$ 为在 $M$ 个本地代理和时间范围 $T$ 内，策略 $\pi$ 和用户偏好 $\bm{\theta}$
    的期望累计后悔。假设对于所有本地代理 $m$，臂向量可以遍历 ${\mathbb{R}}^{d}$，且 $\{\bm{x}_{a}\}_{a\in\mathcal{A}_{m}}=$
    $\{\bm{x}_{k}\}_{k\in\mathcal{K}}=$ $\set{\bm{e}_{1},\bm{e}_{2},\dots,\bm{e}_{d}}\cup\set{(A-d)\text{
    任意 % 单位向量}}$，其中 $\bm{e}_{i}$ 是 ${\mathbb{R}}^{d}$ 中的第 $i$ 个标准基向量。选择 $\bm{\theta}=(\Delta,0,\dots,0)^{\mathsf{T}}$（其中
    $\Delta\in[0,\frac{1}{2}]$，稍后确定）。令随机变量 $N_{i}(t)$ 和 $\widetilde{N}_{j}(t)$ 分别表示第
    $i$ 个臂和第 $j$ 个关键术语在第 $t$ 回合结束时被选择的次数。定义另一个用户偏好 $\bm{\theta}^{\prime}=(\Delta,0,\dots,2\Delta,\dots,0)^{\mathsf{T}}$，其中
    $\theta_{\ell}=2\Delta$ 且 $\ell=\argmin_{j>1}\max\ab\{\operatorname*{\mathbb{E}}_{\bm{\theta}}[N_{j}(MT)]%
    ,\operatorname*{\mathbb{E}}_{\bm{\theta}}[\widetilde{N}_{j}(MT)]\}$。记 $N_{m,a}(t)$
    为本地代理 $m\in{\mathcal{M}}$ 在第 $t$ 回合结束时选择第 $a$ 个臂的次数。由于对于 $\bm{\theta}$，最优臂是臂 1，拉取其他臂会使期望后悔增加
    $\Delta$。因此，利用 [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)] 中的引理 4.5，得到
    $R_{M,\bm{\theta}}^{\pi}(T)=$ $\sum_{m=1}^{M}\Delta\sum_{a=2}^{A}\operatorname*{\mathbb{E}}\nolimits_{\bm{%
    \theta}}[N_{m,a}(T)]]$。使用不等式 $\operatorname*{\mathbb{E}}_{\bm{\theta}}[N_{j}(MT)]\leq\frac{MT}{K-1}$
    和 $\operatorname*{\mathbb{E}}_{\bm{\theta}}[\widetilde{N}_{j}(MT)]\leq\frac{MT}{K%
    -1}$ 以及马尔可夫不等式，我们得到：$R_{M,\bm{\theta}}^{\pi}(T)$ $\geq\Delta\Pr\nolimits_{\bm{\theta}}\ab[MT-\sum_{m=1}^{M}N_{i,1}(T)\geq\frac{%
    MT}{2}]\frac{MT}{2}.$
- en: For $\bm{\theta}^{\prime}$, similarly, we have $R_{M,\bm{\theta}^{\prime}}^{\pi}(T)\geq$
    $\Delta\Pr\nolimits_{\bm{\theta}^{\prime}}\ab[\sum_{m=1}^{M}N_{i,1}(T)>\frac{MT%
    }{2}]\frac{MT}{2}.$ Therefore, applying the Bretagnolle-Huber theorem (Theorem
    14.2 in [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]), $R_{M,\bm{\theta}}^{\pi}(T)+R_{M,\bm{\theta}^{\prime}}^{\pi}(T)$
    $\geq\frac{\Delta MT}{4}\exp\ab(-D(\PP_{\theta}\parallel\PP_{\theta^{\prime}})).$
    According to the properties of Kullback–Leibler (KL) divergence, with $P\sim\mathcal{N}(\mu_{1},\sigma^{2})$
    and $Q\sim\mathcal{N}(\mu_{2},\sigma^{2})$, we have $D(\PP_{\bm{\theta}}\parallel\PP_{\bm{\theta}^{\prime}})=$
    $\operatorname*{\mathbb{E}}\nolimits_{\bm{\theta}}[N_{\ell}(MT)+\widetilde{N}_{%
    \ell}(MT)]D(\mathcal{N}(0,1)\parallel\mathcal{N}(2\Delta,1))$ $=\frac{(\mu_{1}-\mu_{2})^{2}}{2\sigma^{2}}$.
    Let $\Delta=\sqrt{\frac{d-1}{MT}}$, $\max\ab\{R_{M,\bm{\theta}}^{\pi}(T),R_{M,\bm{\theta}^{\prime}}^{\pi}(T)\}$
    $\geq\frac{R_{M,\bm{\theta}}^{\pi}(T)+R_{M,\bm{\theta}^{\prime}}^{\pi}(T)}{2}$
    $\geq\frac{e^{-4}}{8}\sqrt{(d-1)MT}=\Omega\ab(\sqrt{dMT}).$ ∎
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 $\bm{\theta}^{\prime}$，类似地，我们有 $R_{M,\bm{\theta}^{\prime}}^{\pi}(T)\geq$
    $\Delta\Pr\nolimits_{\bm{\theta}^{\prime}}\ab[\sum_{m=1}^{M}N_{i,1}(T)>\frac{MT\%
    }{2}]\frac{MT}{2}.$ 因此，应用 Bretagnolle-Huber 定理（定理 14.2 在 [[23](https://arxiv.org/html/2501.01849v1#bib.bib23)]），$R_{M,\bm{\theta}}^{\pi}(T)+R_{M,\bm{\theta}^{\prime}}^{\pi}(T)$
    $\geq\frac{\Delta MT}{4}\exp\ab(-D(\PP_{\theta}\parallel\PP_{\theta^{\prime}}))$。根据
    Kullback–Leibler（KL）散度的性质，设 $P\sim\mathcal{N}(\mu_{1},\sigma^{2})$ 且 $Q\sim\mathcal{N}(\mu_{2},\sigma^{2})$，我们有
    $D(\PP_{\bm{\theta}}\parallel\PP_{\bm{\theta}^{\prime}})=$ $\operatorname*{\mathbb{E}}\nolimits_{\bm{\theta}}[N_{\ell}(MT)+\widetilde{N}_{\ell}(MT)]D(\mathcal{N}(0,1)\parallel\mathcal{N}(2\Delta,1))$
    $=\frac{(\mu_{1}-\mu_{2})^{2}}{2\sigma^{2}}$。设 $\Delta=\sqrt{\frac{d-1}{MT}}$，则
    $\max\ab\{R_{M,\bm{\theta}}^{\pi}(T),R_{M,\bm{\theta}^{\prime}}^{\pi}(T)\}$ $\geq\frac{R_{M,\bm{\theta}}^{\pi}(T)+R_{M,\bm{\theta}^{\prime}}^{\pi}(T)}{2}$
    $\geq\frac{e^{-4}}{8}\sqrt{(d-1)MT}=\Omega\ab(\sqrt{dMT})$。∎
- en: A-D Proof of Theorem [2](https://arxiv.org/html/2501.01849v1#Thmtheorem2 "Theorem
    2 (Communication Cost). ‣ IV-A Main Results ‣ IV Performance Analysis ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification")
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A-D 定理 [2](https://arxiv.org/html/2501.01849v1#Thmtheorem2 "定理 2（通信成本）。 ‣ IV-A
    主要结果 ‣ IV 性能分析 ‣ 多智能体对话式在线学习用于自适应 LLM 响应识别")
- en: Proof.
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'At each phase $p$, each local agent $m$ downloads the following: (a) The key
    term vector set, containing at most $d$ feature vectors of dimension $d$; (b)
    The repetition counts for each key term $n_{m,k}^{p},\forall k\in\mathcal{K}_{m}^{p}$,
    totaling at most $d$ integers; And (3) the estimated preference vector $\widehat{\bm{\theta}}_{p}$,
    a $d$-dimensional vector. On the other hand, the local agent uploads the following:
    (a) At most $d$ eigenvalues and their corresponding eigenvectors; (2) The matrix
    $\bm{G}_{m}^{p}$ and $\bm{W}_{m}^{p}$, each size of $d^{2}$. Considering that
    the number of phases is at most $\log T$, the upload and download costs are both
    $\mathcal{O}(d^{2}M\log T)$. ∎'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个阶段 $p$，每个本地智能体 $m$ 下载以下内容：（a）包含最多 $d$ 个特征向量的关键术语向量集，每个特征向量的维度为 $d$；（b）每个关键术语的重复次数
    $n_{m,k}^{p},\forall k\in\mathcal{K}_{m}^{p}$，总共最多为 $d$ 个整数；（3）估计的偏好向量 $\widehat{\bm{\theta}}_{p}$，一个
    $d$ 维向量。另一方面，本地智能体上传以下内容：（a）最多 $d$ 个特征值及其对应的特征向量；（2）矩阵 $\bm{G}_{m}^{p}$ 和 $\bm{W}_{m}^{p}$，每个大小为
    $d^{2}$。考虑到阶段的数量最多为 $\log T$，上传和下载的成本均为 $\mathcal{O}(d^{2}M\log T)$。∎
- en: A-E Proof of Theorem [3](https://arxiv.org/html/2501.01849v1#Thmtheorem3 "Theorem
    3 (Bound on Conversation Frequency). ‣ IV-A Main Results ‣ IV Performance Analysis
    ‣ Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification")
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A-E 定理 [3](https://arxiv.org/html/2501.01849v1#Thmtheorem3 "定理 3（对话频率的界限）。 ‣
    IV-A 主要结果 ‣ IV 性能分析 ‣ 多智能体对话式在线学习用于自适应 LLM 响应识别")
- en: Proof.
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: '1) follows directly from line [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification") of Algorithm [1](https://arxiv.org/html/2501.01849v1#algorithm1
    "Algorithm 1 ‣ III-A MACO Algorithm on Local Agent ‣ III Algorithm Design ‣ Multi-Agent
    Conversational Online Learning for Adaptive LLM Response Identification"). For
    2), in phase $p$, the number of arms $n_{m}^{p}$ pulled by each local agent $m$
    is $\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}=\sum_{a\in\mathcal{A}_{m}^{p}}\left%
    \lceil\frac{2^{2p+1}d}{{\mathcal{A}}_{m}^{p}}\log\frac{2AM\log T}{\delta}% \right\rceil\geq
    2^{2p+1}d\log\frac{2AM\log T}{\delta}.$ And the number of key terms pulled $\widetilde{n}_{m}^{p}$
    by local agent $m$ is given by: $\sum_{k\in\mathcal{K}_{m}^{p}}n_{m,k}^{p}$ $=\sum_{j:\lambda_{j}<h_{p}}\frac{2d\ab(h_{p}-\lambda_{j})}{\beta^{2}2^{-2p}}%
    \log\ab(\frac{2AM\log T}{\delta})$ $\leq\sum_{j=1}^{d}\frac{d\ab(h_{p}-\gamma)}{\beta^{2}2^{-2p-1}}\log\ab(\frac{2%
    AM\log T}{\delta})$. Thus, the ratio between the number of key terms and arms
    for any $m\in{\mathcal{M}}$ is upper bounded by $\frac{\widetilde{n}_{m}^{p}}{n_{m}^{p}}\leq\frac{h_{p}-d\gamma}{\beta^{2}}=%
    \frac{\frac{3}{4(1-2^{-2p})}-d\gamma}{\beta^{2}}\leq\frac{1-d\gamma}{\beta^{2}}.$
    ∎'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 直接来源于算法[1](https://arxiv.org/html/2501.01849v1#algorithm1 "算法 1 ‣ III-A MACO
    算法在本地代理上的应用 ‣ III 算法设计 ‣ 多代理对话在线学习以适应 LLM 响应识别")的第[1](https://arxiv.org/html/2501.01849v1#algorithm1
    "算法 1 ‣ III-A MACO 算法在本地代理上的应用 ‣ III 算法设计 ‣ 多代理对话在线学习以适应 LLM 响应识别")行。对于2)，在阶段$p$，每个本地代理$m$所拉取的臂数$n_{m}^{p}$为$\sum_{a\in\mathcal{A}_{m}^{p}}n_{m,a}^{p}=\sum_{a\in\mathcal{A}_{m}^{p}}\left%
    \lceil\frac{2^{2p+1}d}{{\mathcal{A}}_{m}^{p}}\log\frac{2AM\log T}{\delta}% \right\rceil\geq
    2^{2p+1}d\log\frac{2AM\log T}{\delta}.$ 而本地代理$m$拉取的关键术语数$\widetilde{n}_{m}^{p}$为：$\sum_{k\in\mathcal{K}_{m}^{p}}n_{m,k}^{p}$
    $=\sum_{j:\lambda_{j}<h_{p}}\frac{2d\ab(h_{p}-\lambda_{j})}{\beta^{2}2^{-2p}}%
    \log\ab(\frac{2AM\log T}{\delta})$ $\leq\sum_{j=1}^{d}\frac{d\ab(h_{p}-\gamma)}{\beta^{2}2^{-2p-1}}\log\ab(\frac{2%
    AM\log T}{\delta})$。因此，对于任何$m\in{\mathcal{M}}$，关键术语与臂数的比率上界为$\frac{\widetilde{n}_{m}^{p}}{n_{m}^{p}}\leq\frac{h_{p}-d\gamma}{\beta^{2}}=%
    \frac{\frac{3}{4(1-2^{-2p})}-d\gamma}{\beta^{2}}\leq\frac{1-d\gamma}{\beta^{2}}.$
    ∎
- en: References
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. Lundberg *et al.*, “Sparks of artificial general
    intelligence: Early experiments with gpt-4,” *arXiv preprint arXiv:2303.12712*,
    2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar,
    P. Lee, Y. T. Lee, Y. Li, S. Lundberg *等*，“人工通用智能的火花：与 GPT-4 的早期实验，” *arXiv 预印本
    arXiv:2303.12712*，2023年。'
- en: '[2] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale *等*，“Llama 2: 开放基础模型和微调的聊天模型，” *arXiv
    预印本 arXiv:2307.09288*，2023年。'
- en: '[3] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train,
    prompt, and predict: A systematic survey of prompting methods in natural language
    processing,” *ACM Computing Surveys*, vol. 55, no. 9, pp. 1–35, 2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, 和 G. Neubig，“预训练、提示和预测：自然语言处理中的提示方法系统综述，”
    *ACM 计算机调查*，第55卷，第9期，第1-35页，2023年。'
- en: '[4] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, and Y. Yang,
    “Connecting large language models with evolutionary algorithms yields powerful
    prompt optimizers,” *arXiv preprint arXiv:2309.08532*, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Q. Guo, R. Wang, J. Guo, B. Li, K. Song, X. Tan, G. Liu, J. Bian, 和 Y.
    Yang，“将大型语言模型与进化算法相结合，产生强大的提示优化器，” *arXiv 预印本 arXiv:2309.08532*，2023年。'
- en: '[5] R. Pan, S. Xing, S. Diao, X. Liu, K. Shum, J. Zhang, and T. Zhang, “Plum:
    Prompt learning using metaheuristic,” *arXiv preprint arXiv:2311.08364*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] R. Pan, S. Xing, S. Diao, X. Liu, K. Shum, J. Zhang, 和 T. Zhang，“Plum:
    使用元启发式进行提示学习，” *arXiv 预印本 arXiv:2311.08364*，2023年。'
- en: '[6] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, “Automatic
    prompt optimization with “gradient descent” and beam search,” *arXiv preprint
    arXiv:2305.03495*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, 和 M. Zeng，“使用“梯度下降”和束搜索的自动提示优化，”
    *arXiv 预印本 arXiv:2305.03495*，2023年。'
- en: '[7] L. Chen, M. Zaharia, and J. Zou, “Frugalgpt: How to use large language
    models while reducing cost and improving performance,” *arXiv preprint arXiv:2305.05176*,
    2023.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] L. Chen, M. Zaharia, 和 J. Zou，“Frugalgpt: 如何在降低成本并提高性能的同时使用大型语言模型，” *arXiv
    预印本 arXiv:2305.05176*，2023年。'
- en: '[8] C. Shi, K. Yang, J. Yang, and C. Shen, “Best arm identification for prompt
    learning under a limited budget,” *arXiv preprint arXiv:2402.09723*, 2024.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. Shi, K. Yang, J. Yang, 和 C. Shen，“在有限预算下进行提示学习的最佳臂识别，” *arXiv 预印本 arXiv:2402.09723*，2024年。'
- en: '[9] K. Shuster, J. Xu, M. Komeili, D. Ju, E. M. Smith, S. Roller, M. Ung, M. Chen,
    K. Arora, J. Lane *et al.*, “Blenderbot 3: a deployed conversational agent that
    continually learns to responsibly engage,” *arXiv preprint arXiv:2208.03188*,
    2022.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] K. Shuster, J. Xu, M. Komeili, D. Ju, E. M. Smith, S. Roller, M. Ung, M.
    Chen, K. Arora, J. Lane *等*， “Blenderbot 3：一个持续学习并负责任地参与的部署式对话代理，” *arXiv预印本 arXiv:2208.03188*，2022年。'
- en: '[10] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn,
    “Direct preference optimization: Your language model is secretly a reward model,”
    *Advances in Neural Information Processing Systems*, vol. 36, 2024.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, 和 C. Finn,
    “直接偏好优化：你的语言模型实际上是一个奖励模型，” *神经信息处理系统进展*，第36卷，2024年。'
- en: '[11] X. Dai, J. Li, X. Liu, A. Yu, and J. Lui, “Cost-effective online multi-llm
    selection with versatile reward models,” *arXiv preprint arXiv:2405.16587*, 2024.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] X. Dai, J. Li, X. Liu, A. Yu, 和 J. Lui, “具有多样奖励模型的成本效益在线多LLM选择，” *arXiv预印本
    arXiv:2405.16587*，2024年。'
- en: '[12] V. Dwaracherla, S. M. Asghari, B. Hao, and B. Van Roy, “Efficient exploration
    for llms,” *arXiv preprint arXiv:2402.00396*, 2024.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] V. Dwaracherla, S. M. Asghari, B. Hao, 和 B. Van Roy, “高效的LLM探索，” *arXiv预印本
    arXiv:2402.00396*，2024年。'
- en: '[13] Poe, [https://poe.com/ChatGPT](https://poe.com/ChatGPT), 2024.03.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Poe, [https://poe.com/ChatGPT](https://poe.com/ChatGPT)，2024年3月。'
- en: '[14] C. Gao, W. Lei, X. He, M. de Rijke, and T.-S. Chua, “Advances and challenges
    in conversational recommender systems: A survey,” *AI Open*, vol. 2, pp. 100–126,
    2021.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. Gao, W. Lei, X. He, M. de Rijke, 和 T.-S. Chua, “对话推荐系统的进展与挑战：一项调查，”
    *AI Open*，第2卷，第100–126页，2021年。'
- en: '[15] Y. Sun and Y. Zhang, “Conversational recommender system,” in *ACM SIGIR
    Conference*, 2018, p. 235–244.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Sun 和 Y. Zhang, “对话推荐系统，”发表于 *ACM SIGIR 会议*，2018年，第235–244页。'
- en: '[16] X. Dai, Z. Wang, J. Xie, X. Liu, and J. C. Lui, “Conversational recommendation
    with online learning and clustering on misspecified users,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 36, no. 12, pp. 7825–7838, 2024.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] X. Dai, Z. Wang, J. Xie, X. Liu, 和 J. C. Lui, “基于在线学习和聚类的对话推荐：针对错误指定用户的算法，”
    *IEEE 知识与数据工程学报*，第36卷，第12期，第7825–7838页，2024年。'
- en: '[17] X. Zhang, H. Xie, H. Li, and J. C.S. Lui, “Conversational contextual bandit:
    Algorithm and application,” in *Proceedings of The Web Conference*, 2020, p. 662–672.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] X. Zhang, H. Xie, H. Li, 和 J. C.S. Lui, “对话上下文带宽：算法与应用，”发表于 *Web 会议论文集*，2020年，第662–672页。'
- en: '[18] J. Wu, C. Zhao, T. Yu, J. Li, and S. Li, “Clustering of conversational
    bandits for user preference learning and elicitation,” in *Proceedings of the
    ACM CIKM*, 2021, p. 2129–2139.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Wu, C. Zhao, T. Yu, J. Li, 和 S. Li, “对话带宽的聚类用于用户偏好学习与引导，”发表于 *ACM CIKM会议论文集*，2021年，第2129–2139页。'
- en: '[19] Z. Wang, X. Liu, S. Li, and J. C. S. Lui, “Efficient explorative key-term
    selection strategies for conversational contextual bandits,” *Proceedings of the
    AAAI*, pp. 10 288–10 295, 2023.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Wang, X. Liu, S. Li, 和 J. C. S. Lui, “高效的对话上下文带宽关键术语选择策略，”发表于 *AAAI会议论文集*，第10,288–10,295页，2023年。'
- en: '[20] X. Liu, H. Zhao, T. Yu, S. Li, and J. C. Lui, “Federated online clustering
    of bandits,” in *Proceedings of the UAI*, 2022, pp. 1221–1231.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] X. Liu, H. Zhao, T. Yu, S. Li, 和 J. C. Lui, “联邦在线带宽聚类，”发表于 *UAI会议论文集*，2022年，第1221–1231页。'
- en: '[21] Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári, “Improved algorithms for
    linear stochastic bandits,” in *Proceedings of the NeurIPS*, 2011.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Y. Abbasi-Yadkori, D. Pál, 和 C. Szepesvári, “改进的线性随机带宽算法，”发表于 *NeurIPS
    会议论文集*，2011年。'
- en: '[22] R. Huang, W. Wu, J. Yang, and C. Shen, “Federated linear contextual bandits,”
    *Advances in neural information processing systems*, vol. 34, pp. 27 057–27 068,
    2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] R. Huang, W. Wu, J. Yang, 和 C. Shen, “联邦线性上下文带宽，”发表于 *神经信息处理系统进展*，第34卷，第27,057–27,068页，2021年。'
- en: '[23] T. Lattimore and C. Szepesvári, *Bandit Algorithms*.   Cambridge University
    Press, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] T. Lattimore 和 C. Szepesvári, *带宽算法*。剑桥大学出版社，2020年。'
- en: '[24] Z. Li, M. Liu, and J. C. S. Lui, “Fedconpe: Efficient federated conversational
    bandits with heterogeneous clients,” in *Proceedings of the Thirty-Third International
    Joint Conference on Artificial Intelligence, IJCAI-24*.   International Joint
    Conferences on Artificial Intelligence Organization, 8 2024, pp. 4533–4541.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Z. Li, M. Liu, 和 J. C. S. Lui, “Fedconpe：具有异质客户端的高效联邦对话带宽，”发表于 *第33届国际人工智能联合会议论文集，IJCAI-24*。国际人工智能联合会议组织，2024年8月，第4533–4541页。'
- en: '[25] J. Lin and S. Moothedath, “Federated stochastic bandit learning with unobserved
    context,” *arXiv preprint arXiv:2303.17043*, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Lin 和 S. Moothedath, “具有未观测上下文的联邦随机带宽学习，” *arXiv预印本 arXiv:2303.17043*，2023年。'
- en: '[26] Y. Wang, J. Hu, X. Chen, and L. Wang, “Distributed bandit learning: Near-optimal
    regret with efficient communication,” in *Proceedings of the ICLR*, 2020.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Y. Wang, J. Hu, X. Chen, 和 L. Wang, “分布式赌博学习：高效通信下的近优悔过率，” 见于 *ICLR 会议论文集*，2020年。'
- en: '[27] C. Zhao, T. Yu, Z. Xie, and S. Li, “Knowledge-aware conversational preference
    elicitation with bandit feedback,” in *Proceedings of the ACM Web Conference 2022*,
    2022, p. 483–492.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] C. Zhao, T. Yu, Z. Xie, 和 S. Li, “知识感知的对话偏好引导与赌博反馈，” 见于 *ACM Web 会议论文集
    2022*，2022年，第483-492页。'
- en: '[28] X. Dai, Z. Wang, J. Xie, T. Yu, and J. C. Lui, “Online learning and detecting
    corrupted users for conversational recommendation systems,” *IEEE Transactions
    on Knowledge and Data Engineering*, vol. 36, no. 12, pp. 8939–8953, 2024.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] X. Dai, Z. Wang, J. Xie, T. Yu, 和 J. C. Lui, “在线学习和检测受损用户的对话推荐系统，” *IEEE
    知识与数据工程学报*，第36卷，第12期，第8939-8953页，2024年。'
- en: '[29] J. Kiefer and J. Wolfowitz, “The equivalence of two extremum problems,”
    *Canadian Journal of Mathematics*, vol. 12, p. 363–366, 1960.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] J. Kiefer 和 J. Wolfowitz, “两个极值问题的等价性，” *加拿大数学杂志*，第12卷，第363-366页，1960年。'
- en: '[30] J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko,
    R. Kapadia, W. Ding, Y. Luan, S. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta,
    A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W. Chang, and I. Naim, “Gecko: Versatile
    text embeddings distilled from large language models,” *arXiv preprint arXiv:2403.20327*,
    2024.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] J. Lee, Z. Dai, X. Ren, B. Chen, D. Cer, J. R. Cole, K. Hui, M. Boratko,
    R. Kapadia, W. Ding, Y. Luan, S. M. K. Duddu, G. H. Abrego, W. Shi, N. Gupta,
    A. Kusupati, P. Jain, S. R. Jonnalagadda, M.-W. Chang, 和 I. Naim, “Gecko：从大型语言模型提炼的多用途文本嵌入，”
    *arXiv 预印本 arXiv:2403.20327*，2024年。'
- en: '[31] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers, “Mteb: Massive text
    embedding benchmark,” *arXiv preprint arXiv:2210.07316*, 2023.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] N. Muennighoff, N. Tazi, L. Magne, 和 N. Reimers, “Mteb：大规模文本嵌入基准，” *arXiv
    预印本 arXiv:2210.07316*，2023年。'
- en: '[32] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens,
    A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri,
    A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick, “Openassistant conversations
    – democratizing large language model alignment,” *arXiv preprint arXiv:2304.07327*,
    2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] A. Köpf, Y. Kilcher, D. von Rütte, S. Anagnostidis, Z.-R. Tam, K. Stevens,
    A. Barhoum, N. M. Duc, O. Stanley, R. Nagyfi, S. ES, S. Suri, D. Glushkov, A.
    Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, 和 A. Mattick, “Openassistant对话——民主化大型语言模型的对齐，”
    *arXiv 预印本 arXiv:2304.07327*，2023年。'
- en: '[33] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings using
    Siamese BERT-networks,” in *Proceedings of the EMNLP-IJCNLP*, 2019, pp. 3982–3992.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] N. Reimers 和 I. Gurevych, “Sentence-BERT：使用孪生BERT网络的句子嵌入，” 见于 *EMNLP-IJCNLP
    会议论文集*，2019年，第3982-3992页。'
- en: '[34] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, and A. Chadha, “A
    systematic survey of prompt engineering in large language models: Techniques and
    applications,” *arXiv preprint arXiv:2402.07927*, 2024.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] P. Sahoo, A. K. Singh, S. Saha, V. Jain, S. Mondal, 和 A. Chadha, “大型语言模型中提示工程的系统性调查：技术与应用，”
    *arXiv 预印本 arXiv:2402.07927*，2024年。'
- en: '[35] Ollama, [https://github.com/jmorganca/ollama/](https://github.com/jmorganca/ollama/),
    2024.06.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Ollama, [https://github.com/jmorganca/ollama/](https://github.com/jmorganca/ollama/)，2024年06月。'
- en: '[36] K. Christakopoulou, F. Radlinski, and K. Hofmann, “Towards conversational
    recommender systems,” in *Proceedings of ACM SIGKDD International Conference*,
    2016, p. 815–824.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] K. Christakopoulou, F. Radlinski, 和 K. Hofmann, “走向对话推荐系统，” 见于 *ACM SIGKDD
    国际会议论文集*，2016年，第815-824页。'
- en: '[37] Z. Zhang, S. Wang, W. Yu, Y. Xu, D. Iter, Q. Zeng, Y. Liu, C. Zhu, and
    M. Jiang, “Auto-instruct: Automatic instruction generation and ranking for black-box
    language models,” *arXiv preprint arXiv:2310.13127*, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Z. Zhang, S. Wang, W. Yu, Y. Xu, D. Iter, Q. Zeng, Y. Liu, C. Zhu, 和 M.
    Jiang, “Auto-instruct：用于黑箱语言模型的自动指令生成与排名，” *arXiv 预印本 arXiv:2310.13127*，2023年。'
- en: '[38] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Karianakis,
    K. Hsieh, P. Bahl, and I. Stoica, “Ekya: Continuous learning of video analytics
    models on edge compute servers,” in *Proceedings of the NSDI*, 2022, pp. 119–135.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] R. Bhardwaj, Z. Xia, G. Ananthanarayanan, J. Jiang, Y. Shu, N. Karianakis,
    K. Hsieh, P. Bahl, 和 I. Stoica, “Ekya：在边缘计算服务器上持续学习视频分析模型，” 见于 *NSDI 会议论文集*，2022年，第119-135页。'
- en: '[39] Y. Xia, F. Kong, T. Yu, L. Guo, R. A. Rossi, S. Kim, and S. Li, “Which
    llm to play? convergence-aware online model selection with time-increasing bandits,”
    *arXiv preprint arXiv:2403.07213*, 2024.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Xia, F. Kong, T. Yu, L. Guo, R. A. Rossi, S. Kim, 和 S. Li, “选择哪个LLM？具有时间递增赌博的收敛感知在线模型选择，”
    *arXiv 预印本 arXiv:2403.07213*，2024年。'
