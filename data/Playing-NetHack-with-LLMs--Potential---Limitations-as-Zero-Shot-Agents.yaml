- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:49:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:49:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用LLM玩《NetHack》：作为零-shot代理的潜力与局限性
- en: 来源：[https://arxiv.org/html/2403.00690/](https://arxiv.org/html/2403.00690/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.00690/](https://arxiv.org/html/2403.00690/)
- en: Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow Queen Mary University of
    London
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow 伦敦玛丽女王大学
- en: '{d.jeurissen, diego.perez, jeremy.gow}@qmul.ac.uk    Duygu Çakmak, James Kwan
    Creative Assembly'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '{d.jeurissen, diego.perez, jeremy.gow}@qmul.ac.uk    Duygu Çakmak, James Kwan
    Creative Assembly'
- en: '{duygu.cakmak, james.kwan}@creative-assembly.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{duygu.cakmak, james.kwan}@creative-assembly.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have shown great success as high-level planners
    for zero-shot game-playing agents. However, these agents are primarily evaluated
    on Minecraft, where long-term planning is relatively straightforward. In contrast,
    agents tested in dynamic robot environments face limitations due to simplistic
    environments with only a few objects and interactions. To fill this gap in the
    literature, we present NetPlay, the first LLM-powered zero-shot agent for the
    challenging roguelike NetHack. NetHack is a particularly challenging environment
    due to its diverse set of items and monsters, complex interactions, and many ways
    to die.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）作为零-shot游戏代理的高级规划者，已经取得了巨大的成功。然而，这些代理主要在《Minecraft》上进行评估，而在该游戏中，长期规划相对简单。相比之下，在动态机器人环境中测试的代理，由于环境过于简单，只有少数物体和交互，面临一定的限制。为填补这一文献空白，我们提出了NetPlay——首个用于挑战性roguelike游戏《NetHack》的LLM驱动零-shot代理。《NetHack》是一个特别具有挑战性的环境，因为它拥有多样的物品和怪物、复杂的交互以及多种死亡方式。
- en: NetPlay uses an architecture designed for dynamic robot environments, modified
    for NetHack. Like previous approaches, it prompts the LLM to choose from predefined
    skills and tracks past interactions to enhance decision-making. Given NetHack’s
    unpredictable nature, NetPlay detects important game events to interrupt running
    skills, enabling it to react to unforeseen circumstances. While NetPlay demonstrates
    considerable flexibility and proficiency in interacting with NetHack’s mechanics,
    it struggles with ambiguous task descriptions and a lack of explicit feedback.
    Our findings demonstrate that NetPlay performs best with detailed context information,
    indicating the necessity for dynamic methods in supplying context information
    for complex games such as NetHack.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay采用了为动态机器人环境设计的架构，并对其进行了修改以适应《NetHack》。像以前的方法一样，它通过提示LLM从预定义的技能中选择，并追踪过去的交互以增强决策过程。考虑到《NetHack》具有不可预测的特点，NetPlay会检测到重要的游戏事件，并中断正在进行的技能，从而能够应对突发情况。尽管NetPlay在与《NetHack》的机制交互方面表现出了相当的灵活性和熟练度，但它在处理模糊的任务描述和缺乏明确反馈时遇到了困难。我们的研究结果表明，NetPlay在拥有详细上下文信息时表现最佳，这表明在为《NetHack》等复杂游戏提供上下文信息时，动态方法的必要性。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: NetHack, Large Language Models, Zero-Shot Agent.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 《NetHack》, 大型语言模型, 零-shot代理。
- en: I Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Recently, agents based on Large Language Models (LLMs) [[1](https://arxiv.org/html/2403.00690v1#bib.bib1)]
    have been successfully applied to robot environments [[2](https://arxiv.org/html/2403.00690v1#bib.bib2),
    [3](https://arxiv.org/html/2403.00690v1#bib.bib3)] and Minecraft [[4](https://arxiv.org/html/2403.00690v1#bib.bib4),
    [5](https://arxiv.org/html/2403.00690v1#bib.bib5), [6](https://arxiv.org/html/2403.00690v1#bib.bib6)],
    among others. These agents do not require pre-training and typically involve prompting
    an LLM to solve tasks by choosing from predefined skills. They have proven effective
    for tasks demanding extensive knowledge, like crafting a diamond pickaxe in Minecraft.
    Additionally, they can understand a wide range of task descriptions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLMs）的代理已经成功应用于机器人环境[[1](https://arxiv.org/html/2403.00690v1#bib.bib1)]
    [[2](https://arxiv.org/html/2403.00690v1#bib.bib2), [3](https://arxiv.org/html/2403.00690v1#bib.bib3)]
    和《Minecraft》[[4](https://arxiv.org/html/2403.00690v1#bib.bib4), [5](https://arxiv.org/html/2403.00690v1#bib.bib5),
    [6](https://arxiv.org/html/2403.00690v1#bib.bib6)]等。这些代理无需预训练，通常通过提示LLM从预定义的技能中选择来解决任务。它们已被证明在需要广泛知识的任务中非常有效，比如在《Minecraft》中制作钻石镐。此外，它们能够理解各种任务描述。
- en: LLM agents utilizing predefined skills are particularly promising for game development
    as developing a set of simple skills is often more feasible than designing an
    entire agent. However, existing studies predominantly focus on the capabilities
    of LLMs for game-playing, neglecting to address their limitations. Evaluations
    typically focus on predictable tasks, for example, finding a diamond in Minecraft,
    which can consistently be achieved through strip mining. Many games require more
    dynamic decision-making, where long-term planning is challenging, and the correct
    course of action is more ambiguous. While evaluations have been done on more dynamic
    robot environments, these environments often contain only a handful of objects
    and lack complex interactions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预定义技能的LLM智能体在游戏开发中尤其具有前景，因为开发一组简单的技能通常比设计一个完整的智能体更可行。然而，现有研究主要集中在LLM在游戏中的能力，忽视了它们的局限性。评估通常集中在可预测的任务上，例如在《Minecraft》中找到钻石，这通常可以通过条带采矿稳定地实现。许多游戏需要更动态的决策制定，其中长期规划具有挑战性，正确的行动方向更加模糊。虽然在更动态的机器人环境中进行过评估，但这些环境通常仅包含少数几个物体，缺乏复杂的互动。
- en: We build upon existing literature by evaluating an LLM agent in the context
    of the complex and unpredictable roguelike NetHack [[7](https://arxiv.org/html/2403.00690v1#bib.bib7)].
    NetHack is a challenging game with many monsters, items, interactions, partial
    observability, and an intricate goal condition. The sheer size of NetHack, paired
    with the many sub-systems the player has to understand, make it an excellent candidate
    for evaluating the limitations of LLM agents. NetHack’s description files also
    allow us to define levels, enabling us to evaluate the agent’s abilities in isolation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在复杂且不可预测的类 Rogue 游戏《NetHack》 [[7](https://arxiv.org/html/2403.00690v1#bib.bib7)]
    中评估一个LLM智能体，构建了在现有文献基础上的工作。《NetHack》是一个充满许多怪物、物品、互动、部分可观察性以及复杂目标条件的挑战性游戏。加之《NetHack》的庞大规模和玩家需要理解的众多子系统，使其成为评估LLM智能体局限性的绝佳候选。《NetHack》的描述文件还允许我们定义关卡，从而使我们能够单独评估智能体的能力。
- en: In the following, we present (NetPlay), a GPT-4 powered agent designed to tackle
    a wide range of tasks in NetHack. NetPlay is inspired by autoascend [[8](https://arxiv.org/html/2403.00690v1#bib.bib8)]
    a handcrafted agent that won the NetHack Challenge 2021 [[9](https://arxiv.org/html/2403.00690v1#bib.bib9)].
    While autoascend relied on a large network of handcrafted rules to handle the
    complexity of NetHack, NetPlay only requires a set of isolated skills. Our experiments
    show that NetPlay can interact with most of NetHack’s game mechanics and that
    it excels in following detailed instructions. Additionally, the agent exhibits
    creative behavior when focusing its attention on a specific problem. However,
    when tasked to play autonomously, NetPlay is far outperformed by autoascend. Consequently,
    this paper delves into reasons for this, such as the agent’s struggles to handle
    ambiguous instructions, confusing observations, and a lack of explicit feedback.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍（NetPlay），一个由GPT-4驱动的智能体，旨在应对《NetHack》中的广泛任务。NetPlay的灵感来源于autoascend
    [[8](https://arxiv.org/html/2403.00690v1#bib.bib8)]，一个手工设计的智能体，曾赢得2021年《NetHack挑战赛》
    [[9](https://arxiv.org/html/2403.00690v1#bib.bib9)]。虽然autoascend依赖于一个庞大的手工规则网络来处理《NetHack》的复杂性，NetPlay只需一组独立的技能。我们的实验表明，NetPlay能够与《NetHack》的大多数游戏机制互动，并且在执行详细指令方面表现出色。此外，当集中注意力解决特定问题时，该智能体还展现了创造性行为。然而，当被要求自主游戏时，NetPlay远远不及autoascend。因此，本文深入探讨了其中的原因，例如智能体在处理模糊指令、混淆观察以及缺乏明确反馈方面的困难。
- en: 'We begin in [section II](https://arxiv.org/html/2403.00690v1#S2 "II Background
    ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents") with
    an overview of NetHack and a review of existing work on LLM-powered agents. [Section III](https://arxiv.org/html/2403.00690v1#S3
    "III NetPlay ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents") discusses the architecture of NetPlay, including many of the design decisions
    we had to make due to limitations caused by the LLM. In [section IV](https://arxiv.org/html/2403.00690v1#S4
    "IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents"), we first evaluate NetPlay’s ability to autonomously play the game and
    compare its performance with a simple handcrafted agent and autoascend. We follow
    this up with an in-depth analysis of the agent’s behavior across various isolated
    scenarios. Subsequently, we analyze the experiment results in [section V](https://arxiv.org/html/2403.00690v1#S5
    "V Potential and Limitations ‣ Playing NetHack with LLMs: Potential & Limitations
    as Zero-Shot Agents") and conclude this study in [section VI](https://arxiv.org/html/2403.00690v1#S6
    "VI Conclusion ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents"). The source code can be found on GitHub¹¹1[https://github.com/CommanderCero/NetPlay](https://github.com/CommanderCero/NetPlay).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在[第二节](https://arxiv.org/html/2403.00690v1#S2 "II Background ‣ Playing NetHack
    with LLMs: Potential & Limitations as Zero-Shot Agents")中首先介绍了《NetHack》并回顾了现有的基于大语言模型（LLM）的代理相关研究。[第三节](https://arxiv.org/html/2403.00690v1#S3
    "III NetPlay ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents")讨论了NetPlay的架构，包括我们因LLM的局限性所做的设计决策。在[第四节](https://arxiv.org/html/2403.00690v1#S4
    "IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents")中，我们首先评估了NetPlay在自动玩游戏方面的能力，并将其表现与一个简单的手工代理和自动升天（autoascend）进行比较。随后，我们对代理在不同隔离场景中的行为进行了深入分析。接下来，我们在[第五节](https://arxiv.org/html/2403.00690v1#S5
    "V Potential and Limitations ‣ Playing NetHack with LLMs: Potential & Limitations
    as Zero-Shot Agents")中分析了实验结果，并在[第六节](https://arxiv.org/html/2403.00690v1#S6 "VI
    Conclusion ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents")中总结了本研究。源代码可以在GitHub上找到¹¹1[https://github.com/CommanderCero/NetPlay](https://github.com/CommanderCero/NetPlay)。'
- en: II Background
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 背景
- en: II-A NetHack
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 《NetHack》
- en: '![Refer to caption](img/a9a8cac3cf9a8f020e85241abef7ba3b.png)![Refer to caption](img/046fab5598462d8eaaead0623d36ee00.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/a9a8cac3cf9a8f020e85241abef7ba3b.png)![请参见说明文字](img/046fab5598462d8eaaead0623d36ee00.png)'
- en: 'Figure 1: The terminal view of the game NetHack. The left image presents an
    annotated view of the in-game screen, featuring the game’s map, an example of
    a game message, and the agent’s stats. The right image showcases a menu for picking
    up items from a tile containing multiple objects. Image source: [alt.org/nethack](https://alt.org/nethack/)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：游戏《NetHack》的终端视图。左侧图像展示了游戏屏幕的注释视图，包含了游戏地图、一个游戏消息示例和代理的统计信息。右侧图像展示了一个菜单，用于从包含多个物品的格子中拾取物品。图片来源：[alt.org/nethack](https://alt.org/nethack/)
- en: 'NetHack [[7](https://arxiv.org/html/2403.00690v1#bib.bib7)], released in 1987,
    is an extremely challenging turn-based roguelike that continues to receive updates
    to this date. The objective is to traverse 50 procedurally generated levels, retrieving
    the Amulet of Yendor and successfully returning to the surface. Doing so unlocks
    the final challenge of the game: the four elemental planes, followed by the astral
    plane, where players must present the Amulet to their deity. See [fig. 1](https://arxiv.org/html/2403.00690v1#S2.F1
    "Figure 1 ‣ II-A NetHack ‣ II Background ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents") for a screenshot of the game.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '《NetHack》[[7](https://arxiv.org/html/2403.00690v1#bib.bib7)]，于1987年发布，是一款极具挑战性的回合制
    Roguelike 游戏，至今仍在不断更新。游戏目标是穿越50个程序生成的关卡，取回“尤恩多的护符”并成功返回地面。完成这一目标后，将解锁游戏的最终挑战：四大元素平面，接着是星界平面，玩家必须在这里向他们的神明献上护符。见[图1](https://arxiv.org/html/2403.00690v1#S2.F1
    "Figure 1 ‣ II-A NetHack ‣ II Background ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents")中的游戏截图。'
- en: Most aspects of the game are generated, such as level layouts, the player’s
    starting class, and the inventory. The levels follow a somewhat linear structure
    with many branches and sub-dungeons in between. For instance, the entrance to
    the gnomish mines always spawns somewhere between depth 2 and 4, giving the player
    the option to explore them immediately or to postpone exploration until they are
    stronger.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏的大部分内容是生成的，例如关卡布局、玩家的起始职业和物品栏。关卡遵循一种略带线性的结构，并在其中包含许多分支和子地下城。例如，侏儒矿洞的入口总是出现在第2到第4深度之间，玩家可以选择立即探索，或者推迟探索，直到自己变得更强。
- en: NetHack encompasses a diverse array of monsters, items, and interactions. Players
    must skillfully utilize their resources while avoiding many of the game’s lethal
    threats. Even for seasoned players possessing extensive knowledge of the game,
    victory is far from guaranteed. The game’s inherent complexity requires players
    to continuously re-assess their situation to adapt to the unpredictability of
    the elements at play.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack 包含了多种多样的怪物、物品和互动元素。玩家必须巧妙地利用自己的资源，同时避免许多致命的威胁。即使是拥有丰富游戏知识的老玩家，胜利也并非板上钉钉。游戏固有的复杂性要求玩家不断重新评估自己的处境，以适应游戏中各种元素的不可预测性。
- en: Nethack uses description files (des-files) to describe special levels like the
    oracle level that always contains a room with an oracle monster, centaur statues,
    and four fountains. Des-files offer extensive control over the level-generation
    process, allowing entirely handcrafted levels or a slightly constrained level-generation
    process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack 使用描述文件（des-files）来描述特殊关卡，例如始终包含一个神谕怪物、半人马雕像和四个喷泉的神谕关卡。描述文件提供了对关卡生成过程的广泛控制，可以创建完全手工制作的关卡，或采用稍微受限的关卡生成过程。
- en: II-B NetHack Learning Environment
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B NetHack 学习环境
- en: The NetHack Learning Environment NLE [[10](https://arxiv.org/html/2403.00690v1#bib.bib10)]
    serves as a reinforcement learning environment for playing NetHack 3.6.6\. NLE
    offers easy access to most aspects of the game, such as the map, the agent’s inventory,
    game messages, and the player’s stats. While NLE provides simplified environments
    for learning purposes, it also allows users to play the entire game without any
    restrictions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: NetHack 学习环境 NLE [[10](https://arxiv.org/html/2403.00690v1#bib.bib10)] 是一个用于玩
    NetHack 3.6.6 的强化学习环境。NLE 提供了对游戏大多数方面的轻松访问，例如地图、代理的背包、游戏消息和玩家的状态数据。尽管 NLE 提供了简化的学习环境，但它也允许用户在没有任何限制的情况下玩完整的游戏。
- en: MiniHack [[11](https://arxiv.org/html/2403.00690v1#bib.bib11)] utilizes NLE
    alongside des-files to construct small-scale environments that isolate specific
    challenges that agents will encounter in NetHack. Although MiniHack provides a
    list of challenges, its primary purpose is to streamline the process of designing
    new challenges.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: MiniHack [[11](https://arxiv.org/html/2403.00690v1#bib.bib11)] 利用 NLE 和 des-files
    构建小规模的环境，孤立出在 NetHack 中，代理将面临的特定挑战。尽管 MiniHack 提供了挑战列表，但其主要目的是简化新挑战设计的过程。
- en: II-C autoascend
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C autoascend
- en: In the 2021 NeurIPS NetHack Challenge [[9](https://arxiv.org/html/2403.00690v1#bib.bib9)],
    participants tackled the symbolic and neural tracks, where solutions were either
    handcrafted or designed using machine learning. Notably, the top-performing agents
    were exclusively symbolic, with the autoascend agent emerging as the frontrunner
    [[12](https://arxiv.org/html/2403.00690v1#bib.bib12)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2021 年 NeurIPS NetHack 挑战赛 [[9](https://arxiv.org/html/2403.00690v1#bib.bib9)]
    中，参赛者处理了符号跟踪和神经跟踪，其中解决方案可以是手工制作的，也可以是通过机器学习设计的。值得注意的是，表现最好的代理完全是符号代理，其中 autoascend
    代理脱颖而出，成为领先者 [[12](https://arxiv.org/html/2403.00690v1#bib.bib12)]。
- en: The autoascend agent [[13](https://arxiv.org/html/2403.00690v1#bib.bib13)] succeeded
    by meticulously parsing observations and creating an internal state representation
    to track essential information. The agent utilized the enriched data to implement
    a behavior tree by hierarchically combining strategies representing specific behaviors,
    like fighting, picking up objects, or exploring levels. Overall, autoascend’s
    strategy consists of staying on the first dungeon level until reaching experience
    level 8, after which it will rapidly progress deeper into the dungeon. While following
    this general strategy, autoascend uses many sub-strategies to improve its chance
    of success, such as a solver for solving the Sokoban levels, using altars for
    farming or identifying items, or dipping a long sword into a fountain to gain
    Excalibur.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: autoascend 代理 [[13](https://arxiv.org/html/2403.00690v1#bib.bib13)] 通过细致解析观察结果并创建内部状态表示来追踪关键信息，从而取得成功。该代理利用丰富的数据，通过分层组合表示特定行为的策略，如战斗、拾取物品或探索关卡，来实现行为树。总体而言，autoascend
    的策略是保持在第一层地牢，直到达到经验等级 8，此后它会迅速深入地牢。在遵循这一总体策略的同时，autoascend 还使用许多子策略来提高成功的机会，例如解决
    Sokoban 关卡的解算器、使用祭坛进行物品识别或打怪，或者将一把长剑蘸入喷泉中以获得 Excalibur。
- en: Despite its victory, autoascend’s performance depended heavily on its starting
    class, demonstrating optimal results with the Valkyrie class. The agent occasionally
    descended to depth 10 and reached experience level 10\. However, it is crucial
    to highlight that reaching around depth 50 is only one of the objectives to beat
    NetHack, emphasizing how challenging the environment still is.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了胜利，autoascend的表现仍然在很大程度上依赖于其起始类别，且在女武神（Valkyrie）类别下表现最佳。该智能体偶尔会下降到深度10，并达到经验等级10。然而，必须强调的是，达到大约深度50只是击败NetHack的目标之一，这凸显了环境依然充满挑战。
- en: II-D LLM Agents
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D LLM智能体
- en: Recently, a plethora of LLM-based agents have emerged, aiming to leverage the
    planning capabilities of these models. A prominent testbed for these agents is
    Minecraft [[4](https://arxiv.org/html/2403.00690v1#bib.bib4), [5](https://arxiv.org/html/2403.00690v1#bib.bib5),
    [6](https://arxiv.org/html/2403.00690v1#bib.bib6)], primarily focusing on the
    agent’s ability to obtain the various items in the game. While the details vary,
    most approaches implement a closed-loop planning system in which the LLM generates
    a plan consisting of a sequence of predefined skills. The plan is then executed
    and, in case of failure, the agent will re-plan using only feedback from the previous
    plan. A noteworthy aspect of these agents is the storage and reuse of successful
    plans, significantly enhancing overall performance due to the hierarchical nature
    of obtaining items like a diamond pickaxe. The agents primarily utilize an LLM
    for their knowledge of how to acquire items. However, one agent has demonstrated
    the ability to construct structures with human feedback [[6](https://arxiv.org/html/2403.00690v1#bib.bib6)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了大量基于大语言模型（LLM）的智能体，旨在利用这些模型的规划能力。这些智能体的一个显著测试平台是Minecraft [[4](https://arxiv.org/html/2403.00690v1#bib.bib4),
    [5](https://arxiv.org/html/2403.00690v1#bib.bib5), [6](https://arxiv.org/html/2403.00690v1#bib.bib6)]，主要聚焦于智能体在游戏中获取各种物品的能力。尽管细节有所不同，大多数方法实现了一个闭环规划系统，在该系统中，LLM生成一个由一系列预定义技能组成的计划。然后执行该计划，若失败，智能体将仅根据前一个计划的反馈重新规划。这些智能体的一个显著特点是存储和重用成功的计划，由于获取如钻石镐等物品的层级性，本质上显著提升了整体表现。智能体主要利用LLM来获取关于如何获得物品的知识。然而，有一个智能体已经展示了通过人类反馈构建结构的能力
    [[6](https://arxiv.org/html/2403.00690v1#bib.bib6)]。
- en: Other popular applications are robot environments, where tasks include rearranging
    objects on a tabletop, interacting within a kitchen, or engaging in simulated
    household activities [[14](https://arxiv.org/html/2403.00690v1#bib.bib14), [15](https://arxiv.org/html/2403.00690v1#bib.bib15),
    [2](https://arxiv.org/html/2403.00690v1#bib.bib2), [3](https://arxiv.org/html/2403.00690v1#bib.bib3)].
    Because these environments require more dynamic decision-making compared to acquiring
    items in Minecraft, agents like DEPS [[14](https://arxiv.org/html/2403.00690v1#bib.bib14)]
    and Inner Monologue [[2](https://arxiv.org/html/2403.00690v1#bib.bib2)] adopt
    a distinctive approach. Instead of relying solely on feedback from the last failed
    plan, they re-plan by considering a substantial portion of their recent interaction
    history. Similar to our approach, Inner Monologue models the interaction history
    as a chat containing the LLM’s actions and thoughts, human feedback, and feedback
    from the environment, such as scene descriptions and if an action was successful.
    While the robot environments require more dynamic decision-making, the complexity
    of the observations is limited, usually consisting of a list of visible objects
    with spatial information being omitted as the low-level skills are handling it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个流行的应用是机器人环境，其中任务包括在桌面上重新排列物品、在厨房内互动或进行模拟家务活动 [[14](https://arxiv.org/html/2403.00690v1#bib.bib14),
    [15](https://arxiv.org/html/2403.00690v1#bib.bib15), [2](https://arxiv.org/html/2403.00690v1#bib.bib2),
    [3](https://arxiv.org/html/2403.00690v1#bib.bib3)]。由于这些环境需要比在Minecraft中获取物品更动态的决策过程，像DEPS
    [[14](https://arxiv.org/html/2403.00690v1#bib.bib14)] 和Inner Monologue [[2](https://arxiv.org/html/2403.00690v1#bib.bib2)]这样的智能体采取了不同的方法。它们不仅依赖于来自上一个失败计划的反馈，而是通过考虑大量的近期交互历史来重新规划。与我们的方法相似，Inner
    Monologue将交互历史建模为一个聊天，其中包含LLM的行动和思考、人类反馈，以及来自环境的反馈，例如场景描述和某个行动是否成功。尽管机器人环境需要更动态的决策，但观察的复杂性是有限的，通常由一列可见物体构成，空间信息则被省略，因为低级技能已经处理了这些信息。
- en: An alternative use of LLMs involves employing them to design reward functions,
    which are then used to train reinforcement learning agents [[16](https://arxiv.org/html/2403.00690v1#bib.bib16),
    [17](https://arxiv.org/html/2403.00690v1#bib.bib17), [18](https://arxiv.org/html/2403.00690v1#bib.bib18)].
    Most relevant to our work, Motif employs an LLM to learn various playstyles in
    NetHack. It achieves this by tasking the LLM to decide which of NetHack’s game
    messages it prefers. Motif can leverage these preferences to learn reward functions
    for different playstyles by conditioning the LLM to prefer game messages associated
    with a specific playstyle, such as fighting monsters.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的另一种应用是用于设计奖励函数，这些函数随后被用来训练强化学习代理[[16](https://arxiv.org/html/2403.00690v1#bib.bib16)、[17](https://arxiv.org/html/2403.00690v1#bib.bib17)、[18](https://arxiv.org/html/2403.00690v1#bib.bib18)]。与我们的工作最相关的是，Motif使用LLM在NetHack中学习各种游戏风格。它通过要求LLM决定NetHack游戏消息中它偏好的内容来实现这一目标。Motif可以利用这些偏好，通过使LLM偏好与特定游戏风格相关的游戏消息（例如战斗怪物），来学习不同游戏风格的奖励函数。
- en: III NetPlay
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III NetPlay
- en: '![Refer to caption](img/4815a09752dfb94eb427bd692d4e3964.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/4815a09752dfb94eb427bd692d4e3964.png)'
- en: 'Figure 2: Illustration of NetPlay playing NetHack. The process involves constructing
    a prompt using messages representing past events, the current observation, and
    a task description containing available skills and the desired output format.
    The response is parsed to retrieve the next skill. While executing the selected
    skill, a tracker enriches the given observations and detects important events,
    such as when a new monster appears. When the skill is done, or events interrupt
    the skill execution, the agent will restart the prompting process.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：NetPlay玩NetHack的示意图。该过程涉及使用表示过去事件的消息、当前观察结果以及包含可用技能和期望输出格式的任务描述来构建提示。响应被解析以提取下一个技能。在执行所选技能时，跟踪器会丰富给定的观察结果并检测重要事件，例如当新的怪物出现时。当技能执行完成或事件中断技能执行时，代理将重新开始提示过程。
- en: 'This section discusses our LLM-powered Nethack agent NetPlay. See [fig. 2](https://arxiv.org/html/2403.00690v1#S3.F2
    "Figure 2 ‣ III NetPlay ‣ Playing NetHack with LLMs: Potential & Limitations as
    Zero-Shot Agents") for an overview of the architecture. Long-term planning in
    NetHack proves challenging due to its unpredictability, as we cannot know when,
    where, or what will appear as we explore. Consequently, our agent shares many
    similarities with Inner Monologue, which is designed for dynamic environments.
    It implements a closed-loop system where the LLM selects skills sequentially while
    accumulating feedback in the form of game messages, errors, or manually detected
    events. Although we avoid constructing entire plans, the LLM’s thoughts are included
    for future prompts, allowing for strategic planning if deemed necessary by the
    LLM.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论我们基于LLM的NetHack代理NetPlay。有关架构的概述，请参见[图 2](https://arxiv.org/html/2403.00690v1#S3.F2
    "图 2 ‣ III NetPlay ‣ 使用LLM玩NetHack：作为零-shot代理的潜力与局限性")。NetHack中的长期规划具有挑战性，因为它具有不可预测性，我们无法知道在探索过程中何时、何地或会发生什么。因此，我们的代理与Inner
    Monologue非常相似，后者是为动态环境设计的。它实现了一个闭环系统，在这个系统中，LLM按顺序选择技能，同时通过游戏消息、错误或手动检测的事件积累反馈。尽管我们避免构建完整的计划，但LLM的思维会被包含在未来的提示中，如果LLM认为有必要，它会进行战略规划。
- en: III-A Prompting
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 提示
- en: 'We prompt the LLM to choose a skill from a predefined list. The prompt comprises
    three components: $(a)$ the agent’s short-term memory, $(b)$ a description of
    the observation, and $(c)$ a task description alongside the output format.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提示大语言模型（LLM）从预定义的列表中选择一项技能。该提示包括三个部分：$(a)$ 代理的短期记忆，$(b)$ 观察的描述，以及 $(c)$ 任务描述和输出格式。
- en: $(a)$ The observation description primarily focuses on the current level alongside
    additional data like context, inventory, and the agent’s stats. Because we do
    not use a multi-modal LLM, we attempt to convey spatial information by dividing
    the level into structures like rooms and corridors. Each structure is described
    using a unique identifier, the number of steps to reach it, the objects it contains
    with their respective positions, and the number of steps to reach each object.
    Monsters are described separately from the structures by categorizing them as
    close or distant, indicating their potential threat level. Each monster is described
    using its name, position, and number of steps to reach it. For close monsters,
    we also include compass coordinates. The LLM is also informed about which structures
    can be further explored alongside the positions of boulders and doors that block
    exploration progress. Note that despite our emphasis on providing spatial information,
    navigating the environment proved challenging for the LLM. Consequently, we automated
    a large portion of the exploration process using a single skill, potentially rendering
    certain aspects of this observation description obsolete.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $(a)$ 观察描述主要集中在当前层级，并附加上下文、库存和代理的统计数据。由于我们不使用多模态 LLM，因此我们尝试通过将层级划分为房间和走廊等结构来传递空间信息。每个结构都使用唯一标识符描述，包括到达它的步数、它包含的物品及其位置，以及到达每个物品的步数。怪物则与结构分开描述，通过将其分类为近距离或远距离来指示其潜在威胁等级。每个怪物都使用其名称、位置和到达它的步数来描述。对于近距离怪物，我们还包括了指北针坐标。LLM
    还会被告知哪些结构可以进一步探索，以及阻碍探索进度的岩石和门的位置。请注意，尽管我们强调提供空间信息，但导航环境对 LLM 来说仍然具有挑战性。因此，我们使用单一技能自动化了大部分探索过程，可能会使得某些观察描述的部分失效。
- en: $(b)$ The short-term memory is implemented using a list of messages representing
    the timeline of events. Each message is either categorized as system, AI, or human.
    System messages convey feedback from the environment like game messages or errors,
    AI messages capture the LLM’s responses, and new tasks are indicated by human
    messages. Note that while it is possible for a human to provide continual feedback,
    we only study the case where the agent is given a task at the start of the game.
    The memory size is capped at 500 tokens, with older messages being deleted first.
    Observation descriptions are not stored in the memory due to their size.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: $(b)$ 短期记忆通过一系列消息来实现，表示事件的时间线。每条消息可以是系统消息、AI 消息或人类消息。系统消息传达来自环境的反馈，如游戏消息或错误，AI
    消息记录 LLM 的响应，人类消息则指示新的任务。请注意，虽然人类可以持续提供反馈，但我们只研究代理在游戏开始时接收到任务的情况。记忆大小限制为 500 个令牌，较旧的消息会被首先删除。观察描述不会存储在记忆中，因为其大小较大。
- en: $(c)$ The task description includes details about the current task, available
    skills, and a JSON output format. We employ chain-of-thought prompting [[19](https://arxiv.org/html/2403.00690v1#bib.bib19)]
    to guide the LLM to a skill choice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: $(c)$ 任务描述包括当前任务的细节、可用的技能和 JSON 输出格式。我们使用思维链提示[[19](https://arxiv.org/html/2403.00690v1#bib.bib19)]来引导
    LLM 选择技能。
- en: III-B Skills
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 技能
- en: 'TABLE I: Skill Examples: Skills represent parametrizable behaviors that the
    LLM uses to play the game. The name, parameters, and descriptions help to understand
    what each skill does. For some skills, the LLM can omit optional parameters marked
    in [square brackets]. Note that the skill type is only used internally and does
    not matter for the final agent.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 技能示例：技能代表了 LLM 用来玩游戏的可参数化行为。名称、参数和描述有助于理解每个技能的功能。对于某些技能，LLM 可以省略在[方括号]中标记为可选的参数。请注意，技能类型仅在内部使用，对最终的代理没有影响。'
- en: '| Type | Name | Parameters | Description |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 名称 | 参数 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Special | explore_level |  | Explores the level to find new rooms, as well
    as hidden doors and corridors. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | explore_level |  | 探索当前层级，寻找新的房间，以及隐藏的门和走廊。 |'
- en: '| Special | set_avoid_monster_flag | value: bool | If set to true skills will
    try to avoid monsters. |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | set_avoid_monster_flag | value: bool | 如果设置为 true，技能将尝试避开怪物。 |'
- en: '| Special | press_key | key: string | Presses the given letter. For special
    keys only ESC, SPACE, and ENTER are supported. |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 特殊 | press_key | key: string | 按下给定的字母。只有 ESC、SPACE 和 ENTER 被支持作为特殊键。 |'
- en: '| Position | pickup | [x: int, y: int] | Pickup things at your location or
    specify where you want to pickup an item. |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | pickup | [x: int, y: int] | 在当前位置拾取物品，或指定要拾取物品的地点。 |'
- en: '| Position | up | [x: int, y: int] | Go up a staircase at your location or
    specify the position of the staircase you want to use. |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | up | [x: int, y: int] | 在当前位置向上走楼梯，或指定你想使用的楼梯位置。 |'
- en: '| Inventory | drop | item_letter: string | Drop an item. |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 库存 | drop | item_letter: string | 丢弃物品。 |'
- en: '| Inventory | wield | item_letter: string | Wield a weapon. |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 库存 | wield | item_letter: string | 持有武器。 |'
- en: '| Direction | kick | x: int, y: int | Kick something. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 方向 | kick | x: int, y: int | 踢某物。 |'
- en: '| Basic | cast |  | Opens your spellbook to cast a spell. |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | cast |  | 打开你的法术书以施放法术。 |'
- en: '| Basic | pay |  | Pay your shopping bill. |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | pay |  | 支付你的购物账单。 |'
- en: 'Skills, similar to strategies in autoascend, implement specific behaviors by
    returning a sequence of actions. They accept both mandatory and optional parameters
    as input. Skills can generate messages as feedback, which are stored in the agent’s
    memory. Messages are often used, for example, to report why a skill failed. An
    excerpt of skills can be found in [table I](https://arxiv.org/html/2403.00690v1#S3.T1
    "TABLE I ‣ III-B Skills ‣ III NetPlay ‣ Playing NetHack with LLMs: Potential &
    Limitations as Zero-Shot Agents").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '技能，类似于 autoascend 中的策略，通过返回一系列动作来实现特定行为。它们接受必需和可选的参数作为输入。技能可以生成消息作为反馈，这些消息会存储在代理人的记忆中。消息常用于报告技能失败的原因。例如，可以在[表格
    I](https://arxiv.org/html/2403.00690v1#S3.T1 "TABLE I ‣ III-B Skills ‣ III NetPlay
    ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot Agents")中找到技能的摘录。'
- en: Navigation is automated through skills like “move_to x y” or “go_to room_id”.
    However, exploring levels with only these skills proved challenging for the LLM.
    To address this, we introduced the “explore_level” skill, which uses the exploration
    strategy from autoascend. This skill explores the current level by uncovering
    tiles, opening doors, and searching for hidden corridors. We removed the ability
    to kick open doors to avoid potential issues such as aggravating shopkeepers.
    Note that the agent can still decide to kick open doors using a separate “kick”
    skill. All movement-related skills will attack monsters that are in the way. The
    LLM can turn off this behavior using the “set_avoid_monster_flag” skill.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 导航通过像“move_to x y”或“go_to room_id”这样的技能进行自动化。然而，仅凭这些技能进行关卡探索对 LLM 来说具有挑战性。为了解决这个问题，我们引入了“explore_level”技能，该技能使用
    autoascend 的探索策略。这个技能通过揭示地砖、打开门和寻找隐藏的走廊来探索当前关卡。我们去除了踢开门的能力，以避免可能的问题，比如激怒店主。请注意，代理人仍然可以决定使用独立的“kick”技能踢开门。所有与移动相关的技能都会攻击挡路的怪物。LLM
    可以使用“set_avoid_monster_flag”技能关闭这种行为。
- en: To indicate when the agent is done with a given task, it has access to the “finish_task”
    skill. Additionally, the LLM is equipped with the “press_key” and “type_text”
    skills for navigating NetHack’s various game menus. While a menu is open, only
    the “finish_task” and text input skills remain available.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指示代理人在完成特定任务时，它可以使用“finish_task”技能。此外，LLM 配备了“press_key”和“type_text”技能，用于导航
    NetHack 的各种游戏菜单。在菜单打开时，只有“finish_task”和文本输入技能仍然可用。
- en: The remaining skills are thin wrappers around NetHack commands, such as drink
    or pickup. However, these commands often involve multiple steps, such as confirming
    which item to drink or first positioning the agent correctly to then pick up an
    item. Consequently, the LLM often assumed that the “drink” command accepts an
    item parameter or that “pickup” works seamlessly regardless of the agent’s current
    position. To mitigate these issues, we implemented four types of command skills.
    Base commands only invoke the command. Position commands offer the option to first
    move to the desired location. Inventory commands accept an item parameter to resolve
    the following popup menu. Finally, direction commands like “kick” move the agent
    close to a desired position before executing the command in the correct direction.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的技能是对 NetHack 命令的简单封装，例如饮用或拾取。然而，这些命令通常涉及多个步骤，比如确认饮用哪个物品，或者首先将代理人正确定位，然后再拾取物品。因此，LLM
    经常假设“drink”命令接受一个物品参数，或者认为“pickup”无论代理人当前的位置如何都能无缝执行。为了缓解这些问题，我们实现了四种类型的命令技能。基础命令只调用命令。位置命令提供先移动到目标位置的选项。库存命令接受物品参数以解决随后的弹出菜单。最后，像“kick”这样的方向命令会将代理人移动到接近目标位置的地方，然后朝正确的方向执行命令。
- en: III-C Agent Loop
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 代理循环
- en: Upon receiving a new task, the agent prompts the LLM to select the first skill
    to execute. The LLM’s thoughts and the selected skill are stored in the agent’s
    memory as feedback. While executing the chosen skill, a data tracker observes
    and records details such as found structures, features hidden by monsters or items,
    which tiles the agent has already seen or searched, and events. The information
    collected by the data tracker is used by skills to make decisions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到新任务时，代理会提示LLM选择要执行的第一个技能。LLM的思考过程和选定的技能会作为反馈存储在代理的记忆中。在执行选择的技能时，数据追踪器观察并记录如发现的结构、被怪物或物品隐藏的特征、代理已查看或搜索过的瓦片以及事件等细节。数据追踪器收集的信息会被技能用来做出决策。
- en: The data tracker also looks for specific events in the game to provide additional
    feedback to the LLM. Events include new in-game messages, newly discovered structures,
    level changes or teleports, stat changes, low health, and the discovery of new
    monsters, items, and some map features such as fountains or altars.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 数据追踪器还会在游戏中寻找特定事件，为LLM提供额外的反馈。事件包括新的游戏内消息、新发现的结构、关卡变化或传送、属性变化、低生命值、以及新怪物、物品的发现，还有一些地图特征，如喷泉或祭坛。
- en: A skill continues to run until completion or interruption. Skills are interrupted
    when specific events occur, such as changing the level, teleporting, discovering
    new objects, and reaching low health. In addition to events, many skills are interrupted
    when a menu shows up due to their inability to handle them. Regardless of why
    a skill stopped, the agent then prompts the LLM to select the next skill. The
    sole exception is when the “finish_task” skill is selected, or the game has ended,
    at which point the agent will stop until it receives a new task.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一个技能会持续运行直到完成或被中断。当发生特定事件时，技能会被中断，比如更换关卡、传送、发现新物品和生命值过低。除了事件，许多技能也会因菜单出现而被中断，因为它们无法处理菜单。无论技能为何停止，代理都会提示LLM选择下一个技能。唯一的例外是当选择了“finish_task”技能，或者游戏已经结束，这时代理将停止，直到收到新任务。
- en: III-D Handcrafted Agent
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 手工制作代理
- en: To assess the impact of the LLM in contrast to the predefined skills, we implemented
    a handcrafted agent that aims to replicate the behavior of NetPlay with the task
    set to “Win the Game”. The following list shows a breakdown of the agent’s decision-making
    process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM相对于预定义技能的影响，我们实现了一个手工制作的代理，旨在复制NetPlay的行为，任务设置为“赢得游戏”。以下列表展示了代理的决策过程细分。
- en: '1.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Abort any open menu, as we did not implement a way to navigate them.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中止任何打开的菜单，因为我们没有实现导航它们的方式。
- en: '2.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: If there are hostile monsters nearby, fight them.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果附近有敌对怪物，攻击它们。
- en: '3.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: If health is below 60%, try healing with potions or by praying.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果生命值低于60%，尝试通过药水或祈祷进行治疗。
- en: '4.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Eat food from the inventory when hungry.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当饥饿时，从背包中吃食物。
- en: '5.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Pick up items, which in this case are potions and food.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 捡起物品，在此情况下是药水和食物。
- en: '6.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: If nothing to explore, move to the next level if possible.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有其他可探索的内容，尽可能移动到下一个关卡。
- en: '7.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: If nothing else to do, explore the level and try kicking open doors.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果没有其他事情可做，探索关卡并尝试踢开门。
- en: All the conditions are evaluated in sequence. Once a condition is met, a corresponding
    skill is executed. The selected skill will be interrupted in the same way as NetPlay.
    Once a skill is interrupted, the agent will choose the next skill by again checking
    all conditions in order starting from the first. Note that although we aimed to
    imitate NetPlay’s behavior, the provided rules are too simplistic to capture all
    the nuances.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所有条件会按顺序进行评估。一旦满足某个条件，相应的技能会被执行。选定的技能将以与NetPlay相同的方式被中断。一旦技能被中断，代理将通过重新检查所有条件，从第一个开始，选择下一个技能。请注意，虽然我们旨在模仿NetPlay的行为，但提供的规则过于简化，无法捕捉所有细微差别。
- en: IV Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: Our goals for the experiments were two-fold. First, to evaluate the ability
    of NetPlay to play NetHack. Second, to provide an analysis of the agent’s strengths
    and weaknesses, focusing on identifying which aspects are influenced by the LLM.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的目标有两个。首先，评估NetPlay玩NetHack的能力。其次，提供对代理优缺点的分析，重点识别哪些方面受到LLM的影响。
- en: IV-A Setup
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 设置
- en: All of our experiments used OpenAI’s GPT-4-Turbo (gpt-4-1106-preview) API as
    LLM with the temperature set to 0 and the response format set to JSON. Other models
    were not considered as initial tests revealed that models like GPT-3.5 and a 70B
    parameter instruct version of LLAMA 2 [[20](https://arxiv.org/html/2403.00690v1#bib.bib20)]
    could not correctly utilize our skills. The agent’s memory size was set to 500
    tokens.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有的实验都使用了 OpenAI 的 GPT-4-Turbo（gpt-4-1106-preview）API 作为大型语言模型（LLM），温度设置为
    0，响应格式设置为 JSON。其他模型未被考虑，因为初步测试显示，像 GPT-3.5 和 70B 参数版本的 LLAMA 2 [[20](https://arxiv.org/html/2403.00690v1#bib.bib20)]
    等模型无法正确利用我们的技巧。代理人的内存大小设置为 500 个标记。
- en: The agent had access to most commands that interact with the game directly,
    except for some rarely relevant commands, like turning undead or using a monster’s
    special ability. All control and system commands, like opening the help menu or
    hiding icons on the map, were excluded. We also implemented a time limit of 10
    LLM calls, at which point the experiment would terminate if the in-game time did
    not advance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人可以访问大多数与游戏直接交互的命令，除了某些较少相关的命令，如转变为不死生物或使用怪物的特殊能力。所有控制和系统命令，如打开帮助菜单或隐藏地图上的图标，都被排除在外。我们还实施了
    10 次 LLM 调用的时间限制，如果游戏中的时间未能推进，则实验将在此时终止。
- en: IV-B Full Runs
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 完整运行
- en: 'TABLE II: Results summary of the mean and standard error for the agents achieved
    score, depth, experience level, and game time.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：代理人所达到的得分、深度、经验等级和游戏时间的平均值及标准误差总结。
- en: '| Metric | NetPlay (Unguided) | NetPlay (Guided) | autoascend | handcrafted
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | NetPlay（无指导） | NetPlay（有指导） | autoascend | 手工制作 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Score | 284.85 $\pm$ 222.10 | 405.00 $\pm$ 216.38 | 11341.94 $\pm$ 11625.39
    | 250.24 $\pm$ 159.17 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 得分 | 284.85 $\pm$ 222.10 | 405.00 $\pm$ 216.38 | 11341.94 $\pm$ 11625.39
    | 250.24 $\pm$ 159.17 |'
- en: '| Depth | 2.60 $\pm$ 1.39 | 2.00 $\pm$ 1.05 | 4.01 $\pm$ 3.04 | 2.35 $\pm$
    0.93 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 深度 | 2.60 $\pm$ 1.39 | 2.00 $\pm$ 1.05 | 4.01 $\pm$ 3.04 | 2.35 $\pm$ 0.93
    |'
- en: '| Level | 2.40 $\pm$ 1.23 | 3.30 $\pm$ 0.95 | 3.34 $\pm$ 7.69 | 2.39 $\pm$
    1.05 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 等级 | 2.40 $\pm$ 1.23 | 3.30 $\pm$ 0.95 | 3.34 $\pm$ 7.69 | 2.39 $\pm$ 1.05
    |'
- en: '| Time | 1292.10 $\pm$ 942.74 | 2627.40 $\pm$ 1545.12 | 21169.81 $\pm$ 9155.59
    | 1306 $\pm$ 924.17 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 时间 | 1292.10 $\pm$ 942.74 | 2627.40 $\pm$ 1545.12 | 21169.81 $\pm$ 9155.59
    | 1306 $\pm$ 924.17 |'
- en: We started evaluating NetPlay by letting it play NetHack without any constraints,
    tasking it to win the game. We will refer to this agent as the “unguided agent.”
    Although the task was to play the entire game, the agent occasionally confused
    its own objectives with the assigned task, resulting in the agent marking the
    task as done too early. To address this issue, we disabled the “finish_task” skill
    for this experiment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始通过让 NetPlay 在没有任何约束的情况下玩 NetHack 来进行评估，任务是让它赢得游戏。我们将此代理人称为“无指导代理”。尽管任务是玩完整个游戏，但代理人偶尔将自己的目标与分配的任务混淆，导致代理人过早标记任务完成。为了解决这个问题，我们在本次实验中禁用了“finish_task”技能。
- en: 'Due to budget limitations, we evaluated all agents using only the Valkyrie
    role, as most agents performed best with this class during the NetHack 2021 challenge.
    We conducted 20 runs with the unguided agent. Additionally, we performed 100 runs
    each with autoascend and the handcrafted agent for comparison. After evaluating
    the unguided agent, we carried out an additional 10 runs employing a “guided agent”
    who was informed on how to play better. A detailed description of the guided agent
    will be provided below. For now, a summary of the results can be found in [table II](https://arxiv.org/html/2403.00690v1#S4.T2
    "TABLE II ‣ IV-B Full Runs ‣ IV Experiments ‣ Playing NetHack with LLMs: Potential
    & Limitations as Zero-Shot Agents").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '由于预算限制，我们仅使用 Valkyrie 角色评估了所有代理人，因为大多数代理人在 NetHack 2021 挑战中使用此角色表现最好。我们进行了
    20 次无指导代理的运行。此外，我们还进行了各 100 次 autoascend 和手工制作代理的运行进行比较。在评估完无指导代理后，我们又进行了 10 次额外的运行，使用了一名“有指导代理”，该代理被告知如何更好地玩游戏。关于有指导代理的详细描述将在下文提供。现在，可以在[表
    II](https://arxiv.org/html/2403.00690v1#S4.T2 "TABLE II ‣ IV-B Full Runs ‣ IV
    Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents")中找到结果的总结。'
- en: '[Table II](https://arxiv.org/html/2403.00690v1#S4.T2 "TABLE II ‣ IV-B Full
    Runs ‣ IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as
    Zero-Shot Agents") shows that autoascend far outperforms both NetPlay and the
    handcrafted agent. While NetPlay managed to beat the handcrafted agent by a small
    margin, it is likely that with a few tweaks, the handcrafted agent can also outperform
    NetPlay.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 II](https://arxiv.org/html/2403.00690v1#S4.T2 "TABLE II ‣ IV-B Full Runs
    ‣ IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents") 显示 autoascend 明显优于 NetPlay 和手工制作的代理。虽然 NetPlay 以微弱优势战胜了手工制作的代理，但很可能通过一些调整，手工制作的代理也能超越
    NetPlay。'
- en: The unguided agent primarily failed due to timeouts, followed by deaths caused
    by eating rotten corpses, fighting with low health, or being overwhelmed by enemies.
    Many timeouts were caused by the agent attempting to move past friendly monsters,
    such as a shopkeeper. By default, bumping into monsters attacks them, but for
    passive monsters, the game prompts the player before initiating an attack. The
    agent’s refusal to attack these monsters often leads to a loop of canceling the
    prompt and moving, resulting in eventual timeouts. A similar loop took place when
    the agent attempted to pick up an item with a generic name on the map but a detailed
    name in the game’s menu. This confusion led the agent to repeatedly close and
    reopen the menu, unable to locate the desired item.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 未引导代理主要是由于超时失败，其次是因食用腐烂的尸体、以低生命值战斗或被敌人压倒而死亡。许多超时是由于代理试图经过友好的怪物，如商人。默认情况下，碰到怪物会攻击它们，但对于被动怪物，游戏会在发起攻击前提示玩家。代理拒绝攻击这些怪物，通常会导致取消提示并继续移动的循环，最终造成超时。当代理试图拾取地图上有通用名称但在游戏菜单中有详细名称的物品时，也发生了类似的循环。这种混淆导致代理反复关闭和重新打开菜单，无法定位所需物品。
- en: Based on the results of the unguided agent, we constructed a guide that included
    strategies from autoascend, such as staying on the first two dungeon levels until
    reaching experience level 8, consuming only freshly slain corpses to avoid eating
    rotten ones, and leveraging altars to acquire items. Furthermore, we provided
    tips for common mistakes by the unguided agent, such as avoiding getting stuck
    behind passive monsters and informing the agent about the time limit to avoid
    timeouts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据未引导代理的结果，我们构建了一份指南，其中包括来自autoascend的策略，例如在达到经验等级8之前停留在前两个地牢层，食用新鲜的尸体以避免腐烂尸体，以及利用祭坛来获取物品。此外，我们提供了针对未引导代理常见错误的提示，例如避免被被动怪物卡住，并告知代理时间限制以避免超时。
- en: The guided agent often managed to stay alive longer by consuming freshly killed
    corpses and praying when hungry or at low health. Its causes of death have been
    a mixture of timeouts, starvation, and dying in combat. Most of the timeouts stemmed
    from a bug with our tracker, which fails to detect when an object disappears while
    being obscured by a monster. For example, the agent repeatedly attempted to pick
    up a dagger already taken by its pet due to the tracker’s misleading observation.
    Despite receiving game messages indicating the absence of the item, the agent
    failed to recognize the situation accurately.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 引导代理通常通过食用新鲜的尸体和在饥饿或低生命值时祈祷，成功地活得更久。它的死亡原因通常是超时、饥饿或战斗中死亡。大多数超时是由于我们的跟踪器存在一个bug，无法在物体被怪物遮挡时检测到物体消失。例如，由于跟踪器的错误观察，代理重复尝试拾起已经被其宠物拿走的匕首。尽管收到了游戏信息，提示物品已不存在，但代理未能准确识别这种情况。
- en: Because we tasked the guided agent to stay on the first two dungeon levels,
    its average depth is lower than that of the unguided agent. However, because monsters
    keep spawning over time, staying on the first levels is an excellent way to grind
    experience. This results in the guided agent gaining more experience than the
    unguided agent. Nevertheless, the agent’s tendency to stay on the first dungeon
    levels frequently caused it to die of starvation due to not finding enough monster
    corpses to eat. Note that autoascend had a similar starvation issue.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们让引导代理停留在前两个地牢层，它的平均深度低于未引导代理。然而，由于怪物会随时间不断生成，停留在前几个层是磨练经验的绝佳方式。这使得引导代理获得的经验比未引导代理更多。然而，代理频繁停留在前两个地牢层，导致它因找不到足够的怪物尸体而饿死。需要注意的是，autoascend也有类似的饥饿问题。
- en: IV-C Scenarios
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 场景
- en: After conducting the full runs, we hypothesized that although NetPlay can be
    creative and interact with most mechanics in the game, it tends to fixate on the
    most straightforward approach for a given task. To confirm this hypothesis, we
    constructed various small-scale scenarios using des-files and a corresponding
    task description. Note that we excluded the handcrafted agent and autoascend for
    this experiment as they cannot easily alter their behavior.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 完成全部运行后，我们假设尽管NetPlay可以富有创意并与游戏中的大多数机制互动，但它倾向于固守给定任务的最简单方法。为了验证这一假设，我们构建了各种小规模的场景，使用des文件和相应的任务描述。请注意，出于实验的需要，我们排除了手工制作的代理和自动上升，因为它们无法轻松改变行为。
- en: 'The tested scenarios evaluated NetPlay’s ability to interact with game mechanics,
    follow instructions, and its creativity. We conducted five runs for each scenario,
    with all roles and the “finish_task” skill enabled. We also repeated some scenarios
    where the agent performed poorly with additional guidelines. We censored the word
    NetHack for the scenarios to evaluate the agent’s ability independently of its
    knowledge about the game. To avoid the agent never using the “finish_task” skill,
    we set a time limit of 500 timesteps for creative scenarios and 200 for the others.
    See [table III](https://arxiv.org/html/2403.00690v1#S4.T3 "TABLE III ‣ IV-C Scenarios
    ‣ IV Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents") for a summary of the tested scenarios and their results.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '测试的场景评估了NetPlay与游戏机制的互动能力、遵循指令的能力以及其创造力。我们为每个场景进行了五次运行，启用了所有角色和“finish_task”技能。我们还针对代理表现不佳的某些场景进行了重复测试，并提供了额外的指导。为了评估代理的能力，而不受其对游戏的了解影响，我们对“NetHack”一词进行了审查。为了防止代理从未使用“finish_task”技能，我们为创意场景设置了500步的时间限制，其他场景为200步。有关测试场景及其结果的摘要，请参见[表
    III](https://arxiv.org/html/2403.00690v1#S4.T3 "TABLE III ‣ IV-C Scenarios ‣ IV
    Experiments ‣ Playing NetHack with LLMs: Potential & Limitations as Zero-Shot
    Agents")。'
- en: The tested scenarios show that NetPlay performs best when provided with concrete
    instructions. The focused boulder task and both escape tasks, in particular, highlight
    how the agent can act creative if we focus its attention on a specific problem.
    However, without very detailed instructions, the agent often fails to do what
    it wants due to incorrect actions and a lack of explicit feedback.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 测试的场景表明，当提供具体指令时，NetPlay表现最佳。特别是在专注的巨石任务和两个逃脱任务中，突显了如果我们将代理的注意力集中在一个具体问题上，它可以表现得很有创意。然而，若没有非常详细的指令，代理往往因错误的动作和缺乏显式反馈而未能完成预期的任务。
- en: The agent’s struggle with explicit feedback is particularly evident in the bag
    and multipickup scenarios, where the agent often failed to navigate the menus
    correctly. While it understood the menus and often chose the correct course of
    action, it often failed by forgetting a crucial step, such as closing the menu.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 代理在显式反馈方面的困难在袋子和多次拾取场景中尤为明显，在这些场景中，代理经常未能正确导航菜单。尽管它理解菜单，并且通常选择正确的行动步骤，但它经常因忘记某个关键步骤（如关闭菜单）而失败。
- en: 'TABLE III: Scenarios: A detailed description of all the tested scenarios, their
    results, and the agent’s success rate. Note that in some scenarios, the agent
    did not use the “finish_task” skill, even after completing it. We still count
    these as success.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：场景：所有测试场景的详细描述、它们的结果以及代理的成功率。请注意，在某些场景中，代理没有使用“finish_task”技能，即使它已经完成了任务。我们仍然将这些视为成功。
- en: '| Scenario | Success | Description | Results |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 成功率 | 描述 | 结果 |'
- en: '| Game Mechanics |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 游戏机制 |'
- en: '| bag | 1/5 | A room with four random objects and a bag of holding with the
    task of stuffing all objects into the bag. | The bag of holding menu is quite
    complex. The agent was only successful when using the option that automatically
    stuffs all items into the bag. In the other cases, the agent forgot to mark an
    item or to confirm its selection. |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 袋子 | 1/5 | 一个房间里有四个随机物品和一个持物袋，任务是将所有物品塞进袋子里。 | 持物袋的菜单相当复杂。代理只有在使用自动将所有物品塞入袋子的选项时才成功。在其他情况下，代理忘记标记某个物品或确认选择。
    |'
- en: '| guided bag | 3/5 | Same as bag, but we told the agent the quickest way to
    pick up items and to navigate the bag’s menu. | The agent used the automatic option
    three times. In the other cases, the agent marked the task done too early, stating
    that it would pick up the remaining items next. |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 引导袋子 | 3/5 | 与袋子场景相同，但我们告诉代理捡取物品的最快方式以及如何导航袋子的菜单。 | 代理使用了自动选项三次。在其他情况下，代理过早地标记任务已完成，表示接下来会捡起剩余的物品。
    |'
- en: '| multipickup | 3/5 | A room with 2-5 objects on the same spot, challenging
    the agent to navigate the multipickup menu. | The agent often picked up items
    inefficiently by opening the pickup menu multiple times. It failed twice by forgetting
    to confirm its item selection. |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 多物品拾取 | 3/5 | 一个房间内有2到5个物品位于同一位置，挑战智能体导航多物品拾取菜单。 | 智能体常常低效地拾取物品，重复打开拾取菜单多次。它因为忘记确认选择的物品而失败了两次。
    |'
- en: '| wand | 1/5 | A room with a statue and a wand with the task of hitting the
    statue with the wand. | The agent often failed by standing atop the statue and
    casting the wand onto itself. Only once did the wand spawn next to the statue,
    causing the agent to cast the wand towards the statue. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 魔杖任务 | 1/5 | 一个房间内有一座雕像和一根魔杖，任务是用魔杖击打雕像。 | 智能体常常失败，因为它站在雕像上并将魔杖施放到自己身上。只有一次，魔杖生成在雕像旁，导致智能体将魔杖施放到雕像方向。
    |'
- en: '| guided wand | 5/5 | Same task as wand, but we asked the agent to stand next
    to the statue instead of on top of it and fire in the statue’s direction. | Most
    of the time, the agent succeeded on the first try, except once when he got it
    on the second try after repositioning himself. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 引导魔杖 | 5/5 | 和魔杖任务相同，但我们要求智能体站在雕像旁而不是站在上面，并朝雕像的方向施放魔杖。 | 大多数时候，智能体第一次就成功了，只有一次因为重新定位后，它在第二次尝试时成功。
    |'
- en: '| Instructions |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 指令 |'
- en: '| ordered | 5/5 | A room with the task to pick up two wands, then a scroll
    of identification, and finally to identify one wand. | The agent executed the
    tasks accurately in the given order. |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 有序任务 | 5/5 | 一个房间，任务是先拾起两根魔杖，然后拿起一卷鉴定卷轴，最后鉴定其中一根魔杖。 | 智能体按给定顺序准确地执行了任务。 |'
- en: '| unordered | 3/5 | A room with the task to drink from a fountain, open a locked
    and a closed door, and kill a monster in any order. | The agent completed the
    tasks in no particular order. One fail stemmed from high-level mobs spawning from
    the fountain, and one from incorrectly using the lockpick. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 无序任务 | 3/5 | 一个房间，任务是从喷泉喝水、打开一扇上锁的门、一扇关着的门，以及击杀一个怪物，顺序不限。 | 智能体按任意顺序完成了这些任务。有一次失败是因为喷泉旁生成了高级怪物，另一次失败是因为错误地使用了开锁工具。
    |'
- en: '| alternative | 5/5 | Three rooms with a fountain and a potion somewhere. The
    task was to drink from a fountain or a potion. | The agent always drank from the
    fountain, which in all cases was found first or was closest to the agent. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 替代任务 | 5/5 | 三个房间，每个房间中都有一个喷泉和某种药水。任务是从喷泉或药水中喝水。 | 智能体总是从喷泉中喝水，而喷泉在所有情况下都是最先被找到或距离智能体最近的。
    |'
- en: '| conditional | 4/5 | Three rooms, with only a single potion hidden in one
    of the rooms. The task was to drink from a fountain, or if unavailable a potion.
    | The agent always drinks the first potion it finds without exploring further.
    In one case, it deemed the task impossible due to spawning with no fountain or
    potion in sight. |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 条件任务 | 4/5 | 三个房间，其中只有一个房间里隐藏着一瓶药水。任务是喝喷泉水，若无喷泉则喝药水。 | 智能体总是喝到它找到的第一个药水，而不会继续探索。一次，它因没有喷泉或药水而认为任务无法完成。
    |'
- en: '| Creativity |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 创造力 |'
- en: '| carry | 1/5 | The agent has to carry two very heavy objects through a monster-filled
    room. We also provided tools such as a bag of holding, a teleportation wand, and
    an invisibility cloak. | The agent often refused to play because it could not
    see the required items or it dropped them in the wrong room. |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 携带任务 | 1/5 | 智能体需要携带两个非常重的物品穿过充满怪物的房间。我们还提供了诸如无底袋、传送魔杖和隐形斗篷等工具。 | 智能体常常因为看不见所需的物品或将物品丢进错误的房间而拒绝执行任务。
    |'
- en: '| guided carry | 4/5 | Same task as carry, but we told the agent to prioritize
    killing monsters first, to carry only one of the heavy items at a time, and to
    use the teleportation wand for easier travel. | Most of the time, the agent carried
    only one item, and it often used the wand to teleport. It failed once by dropping
    one item in the incorrect room. |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 引导携带 | 4/5 | 和携带任务相同，但我们要求智能体优先击杀怪物，每次只携带一个重物，并使用传送魔杖以便更轻松地移动。 | 大多数时候，智能体只携带一个物品，而且经常使用魔杖进行传送。它曾因在错误的房间丢下物品而失败一次。
    |'
- en: '| boulder | 1/5 | Two rooms connected by a corridor with a boulder. The agent
    starts either with pickaxes or wands to remove the boulder. | When given only
    wands, the agent only used explore_level and ignored the boulder. Only once did
    it start with a pickaxe that it used to mine the boulder. |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 巨石 | 1/5 | 两个房间通过走廊相连，中间有一块巨石。智能体要么从起始房间拿到鹤嘴锄，要么拿到魔杖来移除巨石。 | 只有在提供魔杖时，智能体才使用“探索”功能而忽略巨石。只有一次，它从起始房间拿到鹤嘴锄并使用它来开采巨石。
    |'
- en: '| focused boulder | 3/5 | Same task as boulder, but the agent was told to remove
    any boulders blocking its path. | The agent often tried kicking the boulder, which
    failed, after which it then used a pickaxe or a wand. It failed twice due to not
    correctly utilizing the available tools. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 集中石块 | 3/5 | 与石块任务相同，但告知代理人移除任何阻挡其路径的石块。 | 代理人经常尝试踢石块，但未能成功，随后使用了鹤嘴锄或魔杖。由于未能正确使用可用工具，它失败了两次。
    |'
- en: '| guided boulder | 5/5 | Same task as focused boulder, but the agent was told
    explicitly to remove the boulder with the wands or pickaxes. We also provided
    directions on how to utilize the tools. | In all cases, the agent quickly used
    the pickaxe or a wand to remove the boulder. |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 指导石块 | 5/5 | 与集中石块任务相同，但明确告知代理人使用魔杖或鹤嘴锄移除石块。我们还提供了如何使用工具的指引。 | 在所有情况下，代理人迅速使用鹤嘴锄或魔杖移除石块。
    |'
- en: '| escape | 3/5 | The agent must escape from a stone-walled room. Escape methods:
    Digging with a wand through a wall, teleporting with a wand, or morphing into
    a wall-phasing monster using a polymorph control ring with a polymorph wand. |
    The agent escaped twice by teleporting, despite initial teleport failure. It also
    experimented with the wand of digging, casting it in all directions to find an
    exit. It failed twice due to incorrectly using the wands. |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 逃脱 | 3/5 | 代理人必须从一个石墙房间中逃脱。逃脱方法：用魔杖挖掘墙壁、用魔杖传送，或使用变形控制戒指和变形魔杖变成墙壁穿越怪物。 | 代理人两次通过传送成功逃脱，尽管最初的传送失败了。它还尝试了挖掘魔杖，向四面八方施法寻找出口。由于错误使用魔杖，它失败了两次。
    |'
- en: '| hint escape | 5/5 | Same as escape, with a hint engraved on the floor. The
    hint either reveals which wall is brittle and leads to an escape or hints at the
    name of the wall-phasing monster. | After finding the hint, the agent often used
    the suggested escape method, except for one occasion when it teleported instead.
    In one instance, the initial attempts to dig through the wall failed, so it resorted
    to exploring other methods. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 提示逃脱 | 5/5 | 与逃脱任务相同，但地板上刻有提示。提示要么揭示哪个墙壁易碎并可以逃脱，要么暗示墙壁穿越怪物的名字。 | 发现提示后，代理人通常使用建议的逃脱方法，只有一次它选择了传送而非逃脱。在一次情况下，最初尝试挖穿墙壁失败，之后它转向探索其他方法。
    |'
- en: V Potential and Limitations
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 潜力与局限性 |
- en: NetPlay uses a similar architecture to Inner Monologue and DEPS, which have
    shown promising results for simple dynamic environments. Our experiments show
    that despite the immense complexity of NetHack, the agent can fulfill a wide range
    of tasks given enough context information. To our knowledge, this is the first
    NetHack agent to exhibit such flexible behavior. However, the benefits of the
    presented approach seem to diminish the more ambiguous a given task is, making
    tasks such as “Win the Game” impossible.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay 使用了与 Inner Monologue 和 DEPS 类似的架构，这些架构在简单的动态环境中已显示出有前景的结果。我们的实验表明，尽管
    NetHack 的复杂性极高，但只要提供足够的上下文信息，代理人可以完成各种任务。据我们所知，这是第一个展示出如此灵活行为的 NetHack 代理人。然而，所提出方法的优点似乎随着任务的模糊性增加而减弱，使得诸如“赢得游戏”之类的任务变得不可能。
    |
- en: A promising use case of the presented architecture is regression testing during
    game development. Game developers could test specific aspects of their game by
    providing NetPlay with detailed instructions on what to test. This approach could
    not only streamline the testing process, but it would also benefit from NetPlay’s
    flexibility, enabling the tests to adapt dynamically as the game evolves.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 所展示架构的一个有前景的使用案例是游戏开发中的回归测试。游戏开发者可以通过向 NetPlay 提供详细的测试指令来测试游戏的特定方面。这种方法不仅可以简化测试过程，还能受益于
    NetPlay 的灵活性，使得测试能够随着游戏的演变动态适应。 |
- en: Given NetPlay’s proficiency when given detailed context information, an obvious
    extension to our approach would be granting the agent access to the NetHack Wikipedia.
    This could be done using a skill that accepts a query and adds the resulting information
    to the agent’s short-term memory. While we think this can improve the results
    at the cost of more LLM calls, finding the most relevant information for a given
    situation would be tricky. Instead, we recommend investing future research into
    automated methods for finding relevant context information, with a particular
    focus on finding the most successful past interactions as guidelines on how to
    play.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 NetPlay 在提供详细上下文信息时的高效性，我们方法的一个显著扩展将是允许代理访问 NetHack 维基百科。这可以通过一个接受查询并将结果信息添加到代理短期记忆的技能来实现。尽管我们认为这样可以提高结果，但需要更多
    LLM 调用，而为给定情况找到最相关的信息将是一个挑战。相反，我们建议将未来的研究投资于自动化方法，以便查找相关的上下文信息，特别是通过找到最成功的历史互动作为如何玩游戏的指南。
- en: A significant limitation of our approach lies in the predefined skills and the
    observation descriptions, which struggle to encompass the vast complexity of NetHack.
    Designing the agent to handle all potential edge cases proved challenging, as
    it is difficult to anticipate every scenario. While the premise of this approach
    is that the LLM can handle these edge cases, this is only true as long as we have
    a comprehensive description of the environment and flexible skills. In practice,
    achieving such a well-designed agent requires an ever-growing repertoire of skills
    and an observation description that grows infinitely. As such, another promising
    research direction is to use machine learning to replace the handcrafted components
    of the agent.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的一个重要局限性在于预定义的技能和观察描述，它们难以涵盖 NetHack 的复杂性。设计能够处理所有潜在边界情况的代理 proved 挑战，因为很难预测每一种情况。尽管这种方法的前提是
    LLM 可以处理这些边界情况，但这一点仅在我们有全面的环境描述和灵活技能的情况下成立。实际上，设计出这样一个良好的代理需要一个不断扩展的技能库和一个无限扩展的观察描述。因此，另一个有前景的研究方向是使用机器学习来替代代理的手工构件。
- en: VI Conclusion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: In this work, we introduce NetPlay, the first LLM-powered zero-shot agent for
    the challenging roguelike NetHack. Building upon an existing approach tailored
    for simpler dynamic environments, we extended its capabilities to address the
    complexities of NetHack. We evaluated the agent’s performance on the whole game
    and analyzed its behavior using various isolated scenarios.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中，我们介绍了 NetPlay，这是第一个用于挑战性 Roguelike 游戏 NetHack 的 LLM 驱动的零-shot 代理。在现有方法的基础上，我们将其能力扩展到应对
    NetHack 的复杂性。我们对代理的整体游戏表现进行了评估，并使用多种隔离场景分析了其行为。
- en: NetPlay demonstrates proficiency in executing detailed instructions but struggles
    with more ambiguous tasks, such as winning the game. Notably, a simple rule-based
    agent can achieve comparable performance in playing the game. NetPlay’s strength
    lies in its flexibility and creativity. Our experiments show that, given enough
    context information, NetPlay can perform a wide range of tasks. Moreover, by focusing
    its attention on a particular problem, NetPlay is adept at exploring a wide range
    of potential solutions but often with limited success due to a lack of explicit
    feedback guiding it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NetPlay 展现了执行详细指令的能力，但在处理更模糊的任务时（例如赢得游戏）表现不佳。值得注意的是，一个简单的基于规则的代理在玩游戏时能达到类似的表现。NetPlay
    的优势在于其灵活性和创造力。我们的实验表明，给定足够的上下文信息，NetPlay 可以执行广泛的任务。此外，通过将注意力集中在特定问题上，NetPlay 擅长探索各种潜在的解决方案，但由于缺乏明确的反馈指导，其成功往往有限。
- en: VII Acknowledgements
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 致谢
- en: This work was supported by the EPSRC Centre for Doctoral Training in Intelligent
    Games & Games Intelligence (IGGI) (EP/S022325/1).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了 EPSRC Intelligent Games & Games Intelligence (IGGI) 博士培训中心（EP/S022325/1）的支持。
- en: This work was supported by Creative Assembly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了 Creative Assembly 的支持。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar,
    N. Barnes, and A. Mian, “A comprehensive overview of large language models,” 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman, N. Akhtar,
    N. Barnes, 和 A. Mian，“大型语言模型的全面概述”，2023年。'
- en: '[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson,
    I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu, S. Levine,
    K. Hausman, and B. Ichter, “Inner monologue: Embodied reasoning through planning
    with language models,” 2022.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J.
    Tompson, I. Mordatch, Y. Chebotar, P. Sermanet, N. Brown, T. Jackson, L. Luu,
    S. Levine, K. Hausman, 和 B. Ichter，“内心独白：通过语言模型进行具身推理和规划”，2022年。'
- en: '[3] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,
    “Llm-planner: Few-shot grounded planning for embodied agents with large language
    models,” 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, 和 Y. Su，“LLM-planner：用于具身智能体的少量示例基础规划，基于大型语言模型”，2023年。'
- en: '[4] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng,
    Y. Yang, X. Ma, and Y. Liang, “Jarvis-1: Open-world multi-task agents with memory-augmented
    multimodal language models,” 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Z. Wang, S. Cai, A. Liu, Y. Jin, J. Hou, B. Zhang, H. Lin, Z. He, Z. Zheng,
    Y. Yang, X. Ma, 和 Y. Liang，“Jarvis-1：具有记忆增强的多模态语言模型的开放世界多任务智能体”，2023年。'
- en: '[5] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, and J. Dai, “Ghost in the minecraft: Generally capable
    agents for open-world environments via large language models with text-based knowledge
    and memory,” 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] X. Zhu, Y. Chen, H. Tian, C. Tao, W. Su, C. Yang, G. Huang, B. Li, L. Lu,
    X. Wang, Y. Qiao, Z. Zhang, 和 J. Dai，“Minecraft中的幽灵：通过大型语言模型和基于文本的知识与记忆实现的开放世界环境中的通用智能体”，2023年。'
- en: '[6] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar,
    “Voyager: An open-ended embodied agent with large language models,” 2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, 和 A.
    Anandkumar，“Voyager: 一个开放式的具身智能体，基于大型语言模型”，2023年。'
- en: '[7] K. Lorber, “Nethack home page.” [Online]. Available: [https://nethack.org/](https://nethack.org/)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] K. Lorber，“Nethack主页。” [在线]。可用：[https://nethack.org/](https://nethack.org/)'
- en: '[8] “autoascend,” GitHub, 10 2023\. [Online]. Available: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] “autoascend，”GitHub，2023年10月。[在线]。可用：[https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
- en: '[9] “Neurips 2021 - nethack challenge,” 10 2023\. [Online]. Available: [https://nethackchallenge.com/report.html](https://nethackchallenge.com/report.html)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] “Neurips 2021 - Nethack挑战赛，”2023年10月。[在线]。可用：[https://nethackchallenge.com/report.html](https://nethackchallenge.com/report.html)'
- en: '[10] H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette,
    and T. Rocktäschel, “The nethack learning environment,” *CoRR*, vol. abs/2006.13760,
    2020\. [Online]. Available: [https://arxiv.org/abs/2006.13760](https://arxiv.org/abs/2006.13760)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] H. Küttler, N. Nardelli, A. H. Miller, R. Raileanu, M. Selvatici, E. Grefenstette,
    和 T. Rocktäschel，“Nethack学习环境”，*CoRR*，卷 abs/2006.13760，2020年。[在线]。可用：[https://arxiv.org/abs/2006.13760](https://arxiv.org/abs/2006.13760)'
- en: '[11] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro,
    F. Petroni, H. Küttler, E. Grefenstette, and T. Rocktäschel, “Minihack the planet:
    A sandbox for open-ended reinforcement learning research,” *CoRR*, vol. abs/2109.13202,
    2021\. [Online]. Available: [https://arxiv.org/abs/2109.13202](https://arxiv.org/abs/2109.13202)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Samvelyan, R. Kirk, V. Kurin, J. Parker-Holder, M. Jiang, E. Hambro,
    F. Petroni, H. Küttler, E. Grefenstette, 和 T. Rocktäschel，“Minihack the planet:
    一个用于开放式强化学习研究的沙盒”，*CoRR*，卷 abs/2109.13202，2021年。[在线]。可用：[https://arxiv.org/abs/2109.13202](https://arxiv.org/abs/2109.13202)'
- en: '[12] E. Hambro, S. Mohanty, D. Babaev, M. Byeon, D. Chakraborty, E. Grefenstette,
    M. Jiang, D. Jo, A. Kanervisto, J. Kim, S. Kim, R. Kirk, V. Kurin, H. Küttler,
    T. Kwon, D. Lee, V. Mella, N. Nardelli, I. Nazarov, N. Ovsov, J. Parker-Holder,
    R. Raileanu, K. Ramanauskas, T. Rocktäschel, D. Rothermel, M. Samvelyan, D. Sorokin,
    M. Sypetkowski, and M. Sypetkowski, “Insights from the neurips 2021 nethack challenge,”
    2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] E. Hambro, S. Mohanty, D. Babaev, M. Byeon, D. Chakraborty, E. Grefenstette,
    M. Jiang, D. Jo, A. Kanervisto, J. Kim, S. Kim, R. Kirk, V. Kurin, H. Küttler,
    T. Kwon, D. Lee, V. Mella, N. Nardelli, I. Nazarov, N. Ovsov, J. Parker-Holder,
    R. Raileanu, K. Ramanauskas, T. Rocktäschel, D. Rothermel, M. Samvelyan, D. Sorokin,
    M. Sypetkowski, 和 M. Sypetkowski，“来自NeurIPS 2021 Nethack挑战赛的见解”，2022年。'
- en: '[13] maciej sypetkowski, “Autoascend – 1st place nethack agent for the nethack
    challenge at neurips 2021,” GitHub, 01 2024\. [Online]. Available: [https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] maciej sypetkowski，“Autoascend – NeurIPS 2021 Nethack挑战赛中的第1名Nethack智能体”，GitHub，2024年1月。[在线]。可用：[https://github.com/maciej-sypetkowski/autoascend](https://github.com/maciej-sypetkowski/autoascend)'
- en: '[14] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, “Describe, explain, plan
    and select: Interactive planning with large language models enables open-world
    multi-task agents,” 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Z. Wang, S. Cai, A. Liu, X. Ma, 和 Y. Liang，“描述、解释、规划和选择：通过大型语言模型进行互动规划，使开放世界多任务智能体成为可能”，2023年。'
- en: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    和 A. Zeng，“代码作为策略：用于体现控制的语言模型程序”，2023年。'
- en: '[16] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y. Zhu,
    L. Fan, and A. Anandkumar, “Eureka: Human-level reward design via coding large
    language models,” 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. J. Ma, W. Liang, G. Wang, D.-A. Huang, O. Bastani, D. Jayaraman, Y.
    Zhu, L. Fan, 和 A. Anandkumar，“Eureka: 通过编程大型语言模型设计人类级奖励”，2023年。'
- en: '[17] M. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent,
    A. Zhang, and M. Henaff, “Motif: Intrinsic motivation from artificial intelligence
    feedback,” 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. Klissarov, P. D’Oro, S. Sodhani, R. Raileanu, P.-L. Bacon, P. Vincent,
    A. Zhang, 和 M. Henaff，“Motif: 来自人工智能反馈的内在动机”，2023年。'
- en: '[18] M. Kwon, S. M. Xie, K. Bullard, and D. Sadigh, “Reward design with language
    models,” 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. Kwon, S. M. Xie, K. Bullard, 和 D. Sadigh，“使用语言模型进行奖励设计”，2023年。'
- en: '[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou,
    “Chain of thought prompting elicits reasoning in large language models,” *CoRR*,
    vol. abs/2201.11903, 2022\. [Online]. Available: [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, 和 D. Zhou，“思维链提示激发大型语言模型的推理能力”，*CoRR*,
    vol. abs/2201.11903, 2022年。[在线]。可访问：[https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)'
- en: '[20] H. Touvron and et al., “Llama 2: Open foundation and fine-tuned chat models,”
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] H. Touvron 等人，“Llama 2: 开放基础模型和微调聊天模型”，2023年。'
