- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:10:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:10:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 面向多智能体 LLM 交互中的隐性偏见检测与缓解
- en: 来源：[https://arxiv.org/html/2410.02584/](https://arxiv.org/html/2410.02584/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.02584/](https://arxiv.org/html/2410.02584/)
- en: Angana Borah and Rada Mihalcea
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Angana Borah 和 Rada Mihalcea
- en: University of Michigan, Ann Arbor, USA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 美国密歇根大学安阿伯分校
- en: '{anganab, mihalcea}@umich.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{anganab, mihalcea}@umich.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'As Large Language Models (LLMs) continue to evolve, they are increasingly being
    employed in numerous studies to simulate societies and execute diverse social
    tasks. However, LLMs are susceptible to societal biases due to their exposure
    to human-generated data. Given that LLMs are being used to gain insights into
    various societal aspects, it is essential to mitigate these biases. To that end,
    our study investigates the presence of *implicit gender biases* in *multi-agent
    LLM interactions* and proposes two strategies to mitigate these biases. We begin
    by creating a dataset of scenarios where implicit gender biases might arise, and
    subsequently develop a metric to assess the presence of biases. Our empirical
    analysis reveals that LLMs generate outputs characterized by strong implicit bias
    associations ($\geq\approx 50\%$ of the time). Furthermore, these biases tend
    to escalate following multi-agent interactions. To mitigate them, we propose two
    strategies: self-reflection with in-context examples (ICE); and supervised fine-tuning.
    Our research demonstrates that both methods effectively mitigate implicit biases,
    with the ensemble of fine-tuning and self-reflection proving to be the most successful.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLM）的不断发展，它们在众多研究中被广泛应用，用于模拟社会并执行各种社会任务。然而，由于 LLMs 接触到大量人类生成的数据，它们容易受到社会偏见的影响。考虑到
    LLMs 被用来深入了解社会各个方面，因此缓解这些偏见变得至关重要。为此，我们的研究调查了在 *多智能体 LLM 交互* 中存在的 *隐性性别偏见*，并提出了两种缓解这些偏见的策略。我们首先创建了一个包含可能产生隐性性别偏见的情境数据集，并随后开发了一种评估偏见存在的度量标准。我们的实证分析表明，LLMs
    生成的输出通常表现出强烈的隐性偏见关联（$ \geq \approx 50\%$ 的时间）。此外，这些偏见在多智能体交互后往往会加剧。为了缓解这些偏见，我们提出了两种策略：带有情境示例的自我反思（ICE）；以及监督微调。我们的研究表明，这两种方法都能有效缓解隐性偏见，其中微调与自我反思的结合方法最为成功。
- en: Towards Implicit Bias Detection and Mitigation
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 面向隐性偏见检测与缓解
- en: in Multi-Agent LLM Interactions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体大语言模型（LLM）交互中的应用
- en: Angana Borah and Rada Mihalcea University of Michigan, Ann Arbor, USA {anganab,
    mihalcea}@umich.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Angana Borah 和 Rada Mihalcea，美国密歇根大学安阿伯分校 {anganab, mihalcea}@umich.edu
- en: '![Refer to caption](img/d9b7fe45af273568f99f5fb37f5289e0.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d9b7fe45af273568f99f5fb37f5289e0.png)'
- en: 'Figure 1: Interaction framework. Displays four rounds of interaction: The first
    assignment is to assign tasks, followed by two discussion rounds, and the final
    assignment. Each agent is a different LLM assuming different personas. We randomize
    the order of agents in our framework to eliminate position bias'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：交互框架。展示了四轮交互：第一轮分配任务，接着进行两轮讨论，最后是最终任务分配。每个智能体是一个假设不同角色的 LLM。我们在框架中随机化智能体的顺序，以消除位置偏见。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Implicit biases are unconscious social stereotypes that influence our perception
    Brownstein and Zalta ([2019](https://arxiv.org/html/2410.02584v1#bib.bib5)), and
    can be triggered without our knowledge. Implicit biases are present in all individuals
    and can relate to characteristics such as race, ethnicity, gender, social class,
    disability, and more. Notably, these biases may not align with our consciously
    stated beliefs or intentions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 隐性偏见是无意识的社会刻板印象，影响我们的感知（Brownstein 和 Zalta [2019](https://arxiv.org/html/2410.02584v1#bib.bib5)），并且可能在我们不知情的情况下被触发。隐性偏见存在于每个人身上，可能与种族、民族、性别、社会阶层、残疾等特征相关。值得注意的是，这些偏见可能与我们公开表述的信仰或意图不一致。
- en: LLMs, being trained on vast amounts of human-generated data, unintentionally
    learn and even amplify societal biases in their outputs Kotek et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib25)).
    These biases can reinforce stereotypes and propagate misinformation Bender et al.
    ([2021](https://arxiv.org/html/2410.02584v1#bib.bib2)); Wan et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib49)).
    Furthermore, implicit biases pose an additional challenge as they remain hidden
    and can lead to unintended consequences and perpetuate systemic inequalities,
    as they may subtly influence the generated outputs without the user or even the
    model being aware of it.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在大量人类生成的数据上进行训练，无意中学习并甚至放大其输出中的社会偏见 Kotek 等人 ([2023](https://arxiv.org/html/2410.02584v1#bib.bib25))。这些偏见可能会强化刻板印象并传播虚假信息
    Bender 等人 ([2021](https://arxiv.org/html/2410.02584v1#bib.bib2)); Wan 等人 ([2023](https://arxiv.org/html/2410.02584v1#bib.bib49))。此外，隐性偏见构成了额外的挑战，因为它们通常隐藏在背后，可能导致意外后果并延续系统性不平等，因为它们可能会在用户甚至模型不知情的情况下微妙地影响生成的输出。
- en: Earlier efforts at gender bias evaluation and mitigation in language models
    include manipulation of word-embeddings Bolukbasi et al. ([2016](https://arxiv.org/html/2410.02584v1#bib.bib3)),
    and dataset augmentation Lu et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib31));
    Rudinger et al. ([2018](https://arxiv.org/html/2410.02584v1#bib.bib42)); Zhao
    et al. ([2018](https://arxiv.org/html/2410.02584v1#bib.bib57)); Webster et al.
    ([2018](https://arxiv.org/html/2410.02584v1#bib.bib51)). However, these methods
    struggle to scale Zhao et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib56))
    and do not really mitigate but hide biases Gonen and Goldberg ([2019](https://arxiv.org/html/2410.02584v1#bib.bib14)).
    Currently, human preference alignment techniques like Reinforcement Learning from
    Human Feedback (RLHF) Stiennon et al. ([2020](https://arxiv.org/html/2410.02584v1#bib.bib46));
    Ouyang et al. ([2022](https://arxiv.org/html/2410.02584v1#bib.bib37)) are employed
    in LLMs. While these methods succeed in reducing explicitly biased generations,
    they are not without their own set of challenges, including inherent algorithmic
    biases Xiao et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib55)) as
    well as social and ethical concerns Liu ([2023](https://arxiv.org/html/2410.02584v1#bib.bib30)).
    Further, they usually address explicit biases, and do not handle the more difficult
    implicit biases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 早期关于语言模型中性别偏见评估和缓解的努力包括词向量的操控 Bolukbasi 等人 ([2016](https://arxiv.org/html/2410.02584v1#bib.bib3))，以及数据集增强
    Lu 等人 ([2019](https://arxiv.org/html/2410.02584v1#bib.bib31)); Rudinger 等人 ([2018](https://arxiv.org/html/2410.02584v1#bib.bib42));
    Zhao 等人 ([2018](https://arxiv.org/html/2410.02584v1#bib.bib57)); Webster 等人 ([2018](https://arxiv.org/html/2410.02584v1#bib.bib51))。然而，这些方法难以扩展
    Zhao 等人 ([2019](https://arxiv.org/html/2410.02584v1#bib.bib56))，并且并未真正缓解偏见，而是掩盖了偏见
    Gonen 和 Goldberg ([2019](https://arxiv.org/html/2410.02584v1#bib.bib14))。目前，像人类反馈强化学习（RLHF）
    Stiennon 等人 ([2020](https://arxiv.org/html/2410.02584v1#bib.bib46)); Ouyang 等人
    ([2022](https://arxiv.org/html/2410.02584v1#bib.bib37)) 这样的人工偏好对齐技术已在大型语言模型中得到应用。尽管这些方法在减少显性偏见生成方面取得了一定成功，但它们也面临自身的一系列挑战，包括固有的算法偏见
    Xiao 等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib55))，以及社会和伦理问题 Liu
    ([2023](https://arxiv.org/html/2410.02584v1#bib.bib30))。此外，它们通常只解决显性偏见问题，未能有效处理更为复杂的隐性偏见。
- en: The emergence of multi-agent interactions that employ LLMs enables the simulation
    of realistic human interactions, taking on personas reflecting humans, following
    instructions, and engaging in conversations to carry out social tasks such as
    event planning or debating Park et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib38));
    Zhou et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58)); Chan et al.
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib7)). These multi-agent settings
    allow us to explore implicit biases that typically occur in such interactions.
    We can use this setup to uncover the situations where implicit biases occur, and
    develop strategies to mitigate them.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 采用大型语言模型的多代理交互的出现，使得模拟现实的人工智能人类互动成为可能，模型可以展现反映人类的个性，遵循指令并进行对话，从而执行社交任务，如活动策划或辩论
    Park 等人 ([2023](https://arxiv.org/html/2410.02584v1#bib.bib38)); Zhou 等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58));
    Chan 等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib7))。这些多代理环境使我们能够探索这种交互中通常出现的隐性偏见。我们可以利用这种设置揭示隐性偏见出现的情境，并制定相应的策略来减轻这些偏见。
- en: 'In this paper, we address three main research questions regarding implicit
    gender biases¹¹1We use ‘implicit gender biases’ and ’implicit biases’ interchangeably
    in LLMs: RQ1: Do current LLMs generate biased responses when provided with a complex
    scenario where implicit bias is persistent in human societies? RQ2: Does multi-agent
    interaction influence the presence of implicit biases? and RQ3: How can we mitigate
    implicit biases in multi-agent interaction? Our three main contributions are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们针对隐性性别偏见提出了三个主要的研究问题¹¹1我们在LLM中将“隐性性别偏见”和“隐性偏见”交替使用：RQ1：当前的LLM在提供具有隐性偏见且在人类社会中长期存在的复杂场景时，是否会生成有偏的回应？RQ2：多智能体交互是否影响隐性偏见的存在？RQ3：我们如何在多智能体交互中缓解隐性偏见？我们的三大贡献是：
- en: '1.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We develop a comprehensive Scenarios Dataset, comprising 111 scenarios with
    a range of stereotypically male/female tasks and characters in various domains.
    This dataset serves as the foundation for our multi-agent framework and bias mitigation
    methods.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一个全面的场景数据集，包含111个场景，涵盖了多个领域中的典型男性/女性任务和角色。该数据集作为我们多智能体框架和偏见缓解方法的基础。
- en: '2.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Within our multi-agent framework (Fig. [1](https://arxiv.org/html/2410.02584v1#S0.F1
    "Figure 1 ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM
    Interactions")), we enable LLMs to adopt personas presented in the scenarios,
    and engage in interactions aimed at assigning tasks, and responsiblities among
    themselves. We also propose a bias evaluation metric to measure biases in task
    assignments. We provide a comprehensive analysis for bias detection in various
    models and interaction settings.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们的多智能体框架中（图[1](https://arxiv.org/html/2410.02584v1#S0.F1 "图 1 ‣ 朝着多智能体LLM交互中的隐性偏见检测与缓解方向迈进")），我们使得LLM能够采用场景中呈现的人物角色，并进行交互，旨在为它们之间分配任务和责任。我们还提出了一种偏见评估指标，用于衡量任务分配中的偏见。我们提供了对不同模型和交互设置中偏见检测的全面分析。
- en: '3.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'We propose two widely utilized approaches for the mitigation of implicit bias:
    supervised fine-tuning and self-reflection. These techniques have the potential
    to significantly mitigate biases in interactions, leading to a more equitable
    generation.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了两种广泛使用的隐性偏见缓解方法：监督式微调和自我反思。这些技术有潜力显著缓解交互中的偏见，从而实现更加公正的生成。
- en: 2 Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Research in different disciplines like sociology, psychology, cognitive science,
    etc. show that implicit biases can have a significant impact on behavior in areas
    such as employment Dalton and Villagran ([2018](https://arxiv.org/html/2410.02584v1#bib.bib11));
    Nadler ([2010](https://arxiv.org/html/2410.02584v1#bib.bib35)), law enforcement
    Kang et al. ([2011](https://arxiv.org/html/2410.02584v1#bib.bib23)); Levinson
    et al. ([2010](https://arxiv.org/html/2410.02584v1#bib.bib27)), education Staats
    ([2016](https://arxiv.org/html/2410.02584v1#bib.bib43)); Gullo ([2017](https://arxiv.org/html/2410.02584v1#bib.bib15)),
    medicine Chapman et al. ([2013](https://arxiv.org/html/2410.02584v1#bib.bib8));
    Godsil et al. ([2014](https://arxiv.org/html/2410.02584v1#bib.bib13)), politics
    Kinder and Ryan ([2017](https://arxiv.org/html/2410.02584v1#bib.bib24)); Pritlove
    et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib39)) and even our personal
    lives Williams and Bornstein ([2007](https://arxiv.org/html/2410.02584v1#bib.bib52));
    Struffolino ([2017](https://arxiv.org/html/2410.02584v1#bib.bib47)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 社会学、心理学、认知科学等不同学科的研究表明，隐性偏见在多个领域中可能对行为产生显著影响，例如在就业领域（Dalton 和 Villagran（[2018](https://arxiv.org/html/2410.02584v1#bib.bib11)）；Nadler（[2010](https://arxiv.org/html/2410.02584v1#bib.bib35)））、执法领域（Kang
    等（[2011](https://arxiv.org/html/2410.02584v1#bib.bib23)）；Levinson 等（[2010](https://arxiv.org/html/2410.02584v1#bib.bib27)））、教育领域（Staats（[2016](https://arxiv.org/html/2410.02584v1#bib.bib43)）；Gullo（[2017](https://arxiv.org/html/2410.02584v1#bib.bib15)））、医学领域（Chapman
    等（[2013](https://arxiv.org/html/2410.02584v1#bib.bib8)）；Godsil 等（[2014](https://arxiv.org/html/2410.02584v1#bib.bib13)））、政治领域（Kinder
    和 Ryan（[2017](https://arxiv.org/html/2410.02584v1#bib.bib24)）；Pritlove 等（[2019](https://arxiv.org/html/2410.02584v1#bib.bib39)）），甚至在我们的个人生活中（Williams
    和 Bornstein（[2007](https://arxiv.org/html/2410.02584v1#bib.bib52)）；Struffolino（[2017](https://arxiv.org/html/2410.02584v1#bib.bib47)））。
- en: The evolution of LLMs has led to their utilization in multi-agent interaction
    systems where LLMs behave as agents and interact to simulate a society. Park et al.
    ([2023](https://arxiv.org/html/2410.02584v1#bib.bib38)) proposed an architecture
    consisting of observation, planning, and reflection to build LLM agents, and showed
    that LLMs output believable individual and emergent social behaviors. Zhou et al.
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58)) presented an interaction
    environment for LLMs to collaborate and compete with each other to achieve complex
    social goals. Many studies also utilize LLMs as evaluators or judges for performance
    evaluation Wang et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib50));
    Zhou et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58)). However,
    studies have found LLMs are often biased, raising concerns about usage in the
    evaluation pipeline Koutcheme et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib26));
    Chen et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib9)).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的发展使其被应用于多智能体交互系统，其中LLM作为代理人并相互作用以模拟社会。Park等人 ([2023](https://arxiv.org/html/2410.02584v1#bib.bib38))
    提出了一个由观察、规划和反思组成的架构，用于构建LLM代理人，并展示了LLM能够输出可信的个体行为和社会行为。Zhou等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58))
    提出了一个让LLM协作与竞争的交互环境，以实现复杂的社会目标。许多研究还将LLM用作评估者或裁判进行性能评估 Wang等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib50))；Zhou等人
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib58))。然而，研究发现LLM通常存在偏见，这引发了在评估流程中使用LLM的担忧
    Koutcheme等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib26))；Chen等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib9))。
- en: 'It is thus essential to ensure biases are mitigated in LLM outputs. Several
    approaches have been proposed for bias and toxicity mitigation: fine-tuning open-source
    LLMs Agiza et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib1)), causal
    frameworks Li et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib28)),
    self-reflection Ganguli et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib12));
    Cheng et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib10)), reinforcement
    learning Cheng et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib10))
    etc. Current preference alignment techniques like RLHF Stiennon et al. ([2020](https://arxiv.org/html/2410.02584v1#bib.bib46));
    Ouyang et al. ([2022](https://arxiv.org/html/2410.02584v1#bib.bib37)) are also
    utilized. However, they suffer from various issues, such as inherent algorithmic
    bias Xiao et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib55)), social
    and ethical issues Liu ([2023](https://arxiv.org/html/2410.02584v1#bib.bib30)),
    etc. Additionally, research on detecting and mitigating implicit biases in NLP
    is limited, specifically since they are difficult to identify Sun et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib48));
    Gupta et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib16)). To the best
    of our knowledge, we are the first to investigate *‘implicit biases’* in multi-agent
    LLM interactions and propose implicit bias mitigation approaches through interaction.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保在LLM输出中缓解偏见至关重要。为减少偏见和毒性，已提出多种方法：微调开源LLM Agiza等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib1))，因果框架
    Li等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib28))，自我反思 Ganguli等人 ([2023](https://arxiv.org/html/2410.02584v1#bib.bib12))；Cheng等人
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib10))，强化学习 Cheng等人 ([2024](https://arxiv.org/html/2410.02584v1#bib.bib10))
    等。目前，RLHF 等偏好对齐技术也被广泛使用，如Stiennon等人 ([2020](https://arxiv.org/html/2410.02584v1#bib.bib46))；Ouyang等人
    ([2022](https://arxiv.org/html/2410.02584v1#bib.bib37))。然而，它们存在诸多问题，如固有的算法偏见 Xiao等人
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib55))，社会和伦理问题 Liu ([2023](https://arxiv.org/html/2410.02584v1#bib.bib30))
    等。此外，关于检测和缓解NLP中隐性偏见的研究仍然有限，尤其是因为隐性偏见难以识别 Sun等人 ([2019](https://arxiv.org/html/2410.02584v1#bib.bib48))；Gupta等人
    ([2024](https://arxiv.org/html/2410.02584v1#bib.bib16))。就我们所知，我们是首个研究多智能体LLM交互中的*‘隐性偏见’*并提出通过交互缓解隐性偏见的方法的团队。
- en: 3 Dataset
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集
- en: 'Based on previous studies as discussed above, we have identified seven areas
    that can be influenced by implicit biases: family, office, hospital, politics,
    law enforcement, education, and team dynamics²²2Team dynamics is a generic domain
    consisting of varied situations. We focus on task assignments, as many instances
    of implicit bias stem from biased roles and responsibilities allocation. For instance,
    males tend to be assigned hands-on, technical, and leadership roles, while females
    are typically assigned organizational or non-technical roles Brooks et al. ([2014](https://arxiv.org/html/2410.02584v1#bib.bib4));
    Wilson ([2015](https://arxiv.org/html/2410.02584v1#bib.bib53)); Wong and Kemp
    ([2018](https://arxiv.org/html/2410.02584v1#bib.bib54)); Makarova et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib33));
    Stea et al. ([2022](https://arxiv.org/html/2410.02584v1#bib.bib44)).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述讨论的先前研究，我们已经识别出七个可能受隐性偏见影响的领域：家庭、办公室、医院、政治、执法、教育和团队动态²²2团队动态是一个通用领域，包含了多种情况。我们重点关注任务分配，因为许多隐性偏见的实例来源于偏见的角色和责任分配。例如，男性通常被分配到实际操作、技术性和领导角色，而女性则通常被分配到组织性或非技术性角色（Brooks
    等人 [2014](https://arxiv.org/html/2410.02584v1#bib.bib4)；Wilson [2015](https://arxiv.org/html/2410.02584v1#bib.bib53)；Wong
    和 Kemp [2018](https://arxiv.org/html/2410.02584v1#bib.bib54)；Makarova 等人 [2019](https://arxiv.org/html/2410.02584v1#bib.bib33)；Stea
    等人 [2022](https://arxiv.org/html/2410.02584v1#bib.bib44))。
- en: 'We use gpt-4 to generate unique scenarios where implicit biases may occur in
    this format: <scenario description and goal>, <tasks associated>, <characters
    involved>. We compile the Scenarios Dataset, consisting of 111 scenarios, of three/four
    tasks and three/four characters (See Fig [2](https://arxiv.org/html/2410.02584v1#S3.F2
    "Figure 2 ‣ 3 Dataset ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions")). Each data point contains stereotypically male and female
    tasks (as discussed above), with the number of tasks equal to the number of characters
    for each gender, ensuring an equal number of characters and tasks. We utilize
    this dataset for implicit bias detection using task assignment in multi-agent
    LLM interactions.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-4 生成可能出现隐性偏见的独特场景，格式如下：<场景描述和目标>，<相关任务>，<涉及角色>。我们编制了场景数据集，包含111个场景，涉及三到四个任务和三到四个角色（见图
    [2](https://arxiv.org/html/2410.02584v1#S3.F2 "Figure 2 ‣ 3 Dataset ‣ Towards
    Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")）。每个数据点都包含传统的男性和女性任务（如上所述），任务数量与每个性别的角色数量相等，确保每个性别的角色和任务数量相等。我们利用该数据集进行隐性偏见检测，基于多代理
    LLM 交互中的任务分配。
- en: '![Refer to caption](img/a2794a3338456171774a5d78c68c3474.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/a2794a3338456171774a5d78c68c3474.png)'
- en: 'Figure 2: Example from the Scenarios Dataset, from the ‘School’ domain'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：来自“学校”领域的场景数据集示例
- en: 'For bias mitigation, and performance evaluation, we use two additional datasets:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于偏见减轻和性能评估，我们使用了两个额外的数据集：
- en: '1.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Fine-tune Dataset: Using the same scenarios generated above, we manually create
    assignments in two settings: (1) with implicit biases: stereotypically female/male
    tasks are assigned to females/males respectively and (2) without implicit bias:
    stereotypically female tasks are assigned to both females and males, and stereotypically
    male tasks are assigned to both females and males. We then use gpt-4 to provide
    reasons for the presence/absence of implicit biases in each task assignment. We
    utilize this dataset for fine-tuning LLMs.'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 微调数据集：使用上述生成的相同场景，我们手动在两种设置下创建任务分配：(1) 带有隐性偏见：按性别分配传统的女性/男性任务，分别分配给女性/男性；(2)
    没有隐性偏见：传统的女性任务同时分配给女性和男性，传统的男性任务也同时分配给女性和男性。然后，我们使用 GPT-4 提供每个任务分配中隐性偏见存在/不存在的原因。我们利用该数据集对
    LLM 进行微调。
- en: '2.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Test Dataset: To evaluate the performance of our fine-tuned model, we construct
    a smaller dataset consisting of 32 scenarios in two additional domains: media
    and movies; and planning and development, where implicit biases are prominent.
    These scenarios involve two to four task/character scenarios. The main purpose
    of this dataset is to compare the performance of our mitigation approaches to
    existing model performances.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试数据集：为了评估我们微调模型的表现，我们构建了一个包含32个场景的小型数据集，涵盖两个额外的领域：媒体和电影；以及规划与发展，其中隐性偏见尤为突出。这些场景涉及两到四个任务/角色场景。该数据集的主要目的是比较我们的减偏方法与现有模型的表现。
- en: We provide dataset details in Appendix [A](https://arxiv.org/html/2410.02584v1#A1
    "Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions").
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附录[A](https://arxiv.org/html/2410.02584v1#A1 "Appendix A Data ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions")中提供了数据集的详细信息。
- en: Human Validation of Implicit Biases. Since we use gpt-4 for data generation,
    we perform human validation on the Fine-tune dataset. We divide our dataset into
    four sections and let two annotators judge the presence/absence and reasonings
    of implicit bias in the task assignments. We have a total of 8 annotators for
    the entire dataset. The average Cohen’s Kappa score, $\kappa=0.823$ shows very
    high agreement among the annotators. The percent agreement between human and gpt-4
    annotations is $86.28\%$, which shows that gpt-4 excels at generating scenarios
    and providing reasons for the presence/absence of implicit biases.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 隐性偏见的人类验证。由于我们使用gpt-4进行数据生成，我们对微调数据集进行了人工验证。我们将数据集分为四个部分，并让两名注释员判断任务分配中隐性偏见的存在/不存在及其原因。整个数据集共有8名注释员。平均Cohen’s
    Kappa得分$\kappa=0.823$，显示注释员之间有非常高的一致性。人类与gpt-4注释的百分比一致性为$86.28\%$，表明gpt-4在生成场景和提供隐性偏见存在/不存在的原因方面表现出色。
- en: '| Model | Setting | Responses | % Neutral | % Stereotypical | % Anti-Stereotypical
    | Bias Scores |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 设置 | 回应 | % 中性 | % 刻板印象 | % 反刻板印象 | 偏差得分 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | no interaction | all-responses | 0.4786 | 0.5214 | 0 | 0.5214 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | 无交互 | 所有回应 | 0.4786 | 0.5214 | 0 | 0.5214 |'
- en: '|  | interaction (no goal) | first-response | 0.4439 | 0.5431 | 0.0131 | 0.53
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（无目标） | 首次回应 | 0.4439 | 0.5431 | 0.0131 | 0.53 |'
- en: '| gpt-35-turbo | last-response | 0.4139 | 0.5784 | 0.0077 | 0.5707 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| gpt-35-turbo | 最后回应 | 0.4139 | 0.5784 | 0.0077 | 0.5707 |'
- en: '|  | interaction (goal) | first-response | 0.6121 | 0.3303 | 0.0576 | 0.2727
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（目标） | 首次回应 | 0.6121 | 0.3303 | 0.0576 | 0.2727 |'
- en: '|  | last-response | 0.3989 | 0.5876 | 0.0135 | 0.5741 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后回应 | 0.3989 | 0.5876 | 0.0135 | 0.5741 |'
- en: '|  | no interaction | all-responses | 0.2816 | 0.7087 | 0.0097 | 0.6990 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | 无交互 | 所有回应 | 0.2816 | 0.7087 | 0.0097 | 0.6990 |'
- en: '|  | interaction (no goal) | first-response | 0.4872 | 0.4745 | 0.0383 | 0.4362
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（无目标） | 首次回应 | 0.4872 | 0.4745 | 0.0383 | 0.4362 |'
- en: '| gpt-4 | last-response | 0.3821 | 0.5821 | 0.0359 | 0.5462 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 最后回应 | 0.3821 | 0.5821 | 0.0359 | 0.5462 |'
- en: '|  | interaction (goal) | first-response | 0.5832 | 0.536 | 0.0472 | 0.4888
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（目标） | 首次回应 | 0.5832 | 0.536 | 0.0472 | 0.4888 |'
- en: '|  | last-response | 0.3566 | 0.6331 | 0.0103 | 0.6228 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后回应 | 0.3566 | 0.6331 | 0.0103 | 0.6228 |'
- en: '|  | no interaction | all-responses | 0.4898 | 0.5000 | 0.0102 | 0.4898 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 无交互 | 所有回应 | 0.4898 | 0.5000 | 0.0102 | 0.4898 |'
- en: '|  | interaction (no goal) | first-response | 0.4352 | 0.5394 | 0.0255 | 0.5139
    |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（无目标） | 首次回应 | 0.4352 | 0.5394 | 0.0255 | 0.5139 |'
- en: '| mistral-7b-instruct | last-response | 0.4273 | 0.5465 | 0.0262 | 0.5203 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b-instruct | 最后回应 | 0.4273 | 0.5465 | 0.0262 | 0.5203 |'
- en: '|  | interaction (goal) | first-response | 0.6622 | 0.2952 | 0.0426 | 0.2527
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 交互（目标） | 首次回应 | 0.6622 | 0.2952 | 0.0426 | 0.2527 |'
- en: '|  | last-response | 0.4056 | 0.5833 | 0.0111 | 0.5722 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 最后回应 | 0.4056 | 0.5833 | 0.0111 | 0.5722 |'
- en: 'Table 1: Bias scores for LLM interactions across the dataset. Scores are always
    positive, showing biases towards males. Scores also increase after interaction
    for all models. The highest bias scores for each model and the corresponding highest
    bias (male/female/neutral) for assignments are highlighted in Blue and Green respectively.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：数据集中LLM交互的偏差得分。得分始终为正，显示出对男性的偏见。所有模型在交互后得分也有所增加。每个模型的最高偏差得分及其对应的最高偏差（男性/女性/中性）在任务分配中分别用蓝色和绿色突出显示。
- en: 4 A Metric for Bias Evaluation
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 偏差评估的度量标准
- en: Existing metrics for bias evaluation in NLP like the Word Embedding Association
    Test Caliskan et al. ([2017](https://arxiv.org/html/2410.02584v1#bib.bib6)) or
    the Sentence Embedding Association Test May et al. ([2019](https://arxiv.org/html/2410.02584v1#bib.bib34))
    are based on word and sentence embeddings respectively, fairness metrics like
    demographic parity Hardt et al. ([2016](https://arxiv.org/html/2410.02584v1#bib.bib18)),
    equalized odds Hardt et al. ([2016](https://arxiv.org/html/2410.02584v1#bib.bib18)),
    etc. aim to ensure equality across groups/individuals based on certain conditions,
    and therefore, are not suitable for our task assignment framework. In order to
    perform comparative evaluations across different settings and strategies, we need
    a specific metric that captures the amount of bias present in a task assignment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在自然语言处理中的偏见评估度量，例如词嵌入联想测试（Word Embedding Association Test）Caliskan 等人（[2017](https://arxiv.org/html/2410.02584v1#bib.bib6)）或句子嵌入联想测试（Sentence
    Embedding Association Test）May 等人（[2019](https://arxiv.org/html/2410.02584v1#bib.bib34)），分别基于词和句子的嵌入，公正性度量如人口统计平等（demographic
    parity）Hardt 等人（[2016](https://arxiv.org/html/2410.02584v1#bib.bib18)）、平等化机会（equalized
    odds）Hardt 等人（[2016](https://arxiv.org/html/2410.02584v1#bib.bib18)）等，旨在确保基于特定条件在不同群体/个体间的平等，因此不适用于我们的任务分配框架。为了在不同设置和策略之间进行比较评估，我们需要一个特定的度量，能够捕捉任务分配中存在的偏见量。
- en: 'Now, consider a scenario s with 4 tasks: 2 stereotypically male tasks (t1,
    t2) and 2 stereotypically female tasks (t3, t4); and 2 male (m1, m2) and 2 female
    (f1, f2) characters. If tasks are assigned according to traditional gender stereotypes
    (e.g., t1/t2 to m1/m2, t3/t4 to f1/f2), we call it a ‘stereotypical’ assignment.
    If the assignment is the opposite, that is, t1/t2 get assigned to f1/f2, and t3/t4
    get assigned to m1/m2, we call it a ‘anti-stereotypical’ assignment ³³3In our
    dataset, tasks stereotypically associated with males tend to require leadership
    and technical skills, which are often time-consuming and high-priority. This assignment
    can prevent females from taking on more challenging, skill-based tasks. However,
    we acknowledge that this can also be detrimental to males, who may be overlooked
    for tasks where they could excel, despite being stereotypically female.. If tasks
    are evenly distributed across genders, it’s considered neutral (no bias) (See
    Fig [7](https://arxiv.org/html/2410.02584v1#A0.F7 "Figure 7 ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions") in Appendix [B](https://arxiv.org/html/2410.02584v1#A2
    "Appendix B Bias Evaluation Metric ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions") for an example).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个包含4个任务的情景s：2个典型的男性任务（t1、t2）和2个典型的女性任务（t3、t4）；以及2个男性（m1、m2）和2个女性（f1、f2）角色。如果任务根据传统的性别刻板印象进行分配（例如，t1/t2分配给m1/m2，t3/t4分配给f1/f2），我们称之为“刻板化”分配。如果分配是相反的，即t1/t2分配给f1/f2，t3/t4分配给m1/m2，我们称之为“反刻板化”分配³³3在我们的数据集中，通常与男性相关联的任务往往需要领导力和技术技能，这些任务通常耗时且优先级较高。这种分配可能会阻止女性承担更具挑战性的技能任务。然而，我们也承认，这对男性也可能有不利影响，因为他们可能会被忽视，无法承担他们可以表现出色的任务，尽管这些任务通常被认为是女性化的..
    如果任务在性别之间均匀分配，则视为中立（无偏见）（参见附录[B](https://arxiv.org/html/2410.02584v1#A2 "附录 B
    偏见评估度量 ‣ 迈向多代理LLM交互中的隐性偏见检测与缓解")中的图[7](https://arxiv.org/html/2410.02584v1#A0.F7
    "图 7 ‣ 迈向多代理LLM交互中的隐性偏见检测与缓解")中的示例）。
- en: 'To consider a scenario with both even/odd number of characters/tasks, the following
    is true: If two stereotypically male/female tasks are balanced between the genders,
    we call it a balanced stereotypical pair. For example, if stereotypically male
    tasks T1 and T2 are assigned to one female and one male, we call this a balanced
    stereotypical pair. Therefore, taking F as the total number of female agents,
    and M as the total amount of male agents, the maximum number of balanced stereotypical
    pairs possible in an assignment is min(F, M). In an assignment, if #balanced stereotypical
    pairs = min(F, M), the assignment is neutral. If #balanced stereotypical pairs
    < min(F, M), either of the two cases may occur: if the remaining stereotypical
    assignments are greater than anti-stereotypical assignments, then the assignment
    is stereotypical, else it is anti-stereotypical. Therefore, an assignment can
    be either stereotypical, anti-stereotypical, or neutral. For an assignment, we
    denote $s$ as a condition to be stereotypical, $a$ as a condition to be anti-stereotypical,
    and $n$ as a condition to be neutral. For all assignments in the Scenarios Dataset,'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '为了考虑一个既有偶数/奇数个字符/任务的场景，以下是成立的：如果两个典型的男性/女性任务在性别之间平衡分配，我们称之为平衡的刻板印象配对。例如，如果典型的男性任务
    T1 和 T2 被分配给一名女性和一名男性，我们称这为平衡的刻板印象配对。因此，设 F 为女性代理的总数，M 为男性代理的总数，分配中最大可能的平衡刻板印象配对数为
    min(F, M)。在一个分配中，如果 #平衡刻板印象配对 = min(F, M)，则该分配为中立。如果 #平衡刻板印象配对 < min(F, M)，则可能会出现以下两种情况：如果剩余的刻板印象分配大于反刻板印象分配，则该分配为刻板印象分配；否则为反刻板印象分配。因此，分配可以是刻板印象、反刻板印象或中立的。对于一个分配，我们用
    $s$ 表示刻板印象条件，$a$ 表示反刻板印象条件，$n$ 表示中立条件。对于所有在场景数据集中的分配，'
- en: '|  | $\displaystyle b_{n}=\sum_{i=0}^{a}1_{(n_{i}>a_{i}\text{ and }n_{i}>s_{i})}$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b_{n}=\sum_{i=0}^{a}1_{(n_{i}>a_{i}\text{ 且 }n_{i}>s_{i})}$
    |  |'
- en: '|  | $\displaystyle b_{a}=\sum_{i=0}^{a}1_{(a_{i}>n_{i}\text{ and }a_{i}>s_{i})}$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b_{a}=\sum_{i=0}^{a}1_{(a_{i}>n_{i}\text{ 且 }a_{i}>s_{i})}$
    |  |'
- en: '|  | $\displaystyle b_{s}=\sum_{i=0}^{a}1_{(s_{i}>a_{i}\text{ and }s_{i}>n_{i})}$
    |  | (1) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle b_{s}=\sum_{i=0}^{a}1_{(s_{i}>a_{i}\text{ 且 }s_{i}>n_{i})}$
    |  | (1) |'
- en: 'where, $a$ is the total number of assignments, $b_{n}$ is the number of assignments
    with neutral (no) bias, $b_{a}$ is the number of anti-stereotypicalassignments,
    and $b_{s}$ is the number of stereotypical assignments. $b_{n}+b_{a}+b_{s}=a$
    (total number of assignments). We average biases for all scenarios across the
    dataset and compute the following metric for all data. The scores are averaged
    over five LLM runs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$a$ 是总分配数，$b_{n}$ 是具有中立（无）偏见的分配数，$b_{a}$ 是反刻板印象分配的数量，$b_{s}$ 是刻板印象分配的数量。$b_{n}+b_{a}+b_{s}=a$（总分配数）。我们对数据集中的所有场景的偏见进行平均，并计算所有数据的以下指标。得分是五次
    LLM 运行的平均值：
- en: '|  | $Average\ Bias\ Score=\frac{1}{5}\sum_{i=0}^{4}[(-1)\cdot\frac{{b_{a}}_{i}}{a}+%
    0\cdot\frac{{b_{n}}_{i}}{a}+1\cdot\frac{{b_{s}}_{i}}{a}]$ |  | (1) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $平均\ 偏见\ 得分=\frac{1}{5}\sum_{i=0}^{4}[(-1)\cdot\frac{{b_{a}}_{i}}{a}+%
    0\cdot\frac{{b_{n}}_{i}}{a}+1\cdot\frac{{b_{s}}_{i}}{a}]$ |  | (1) |'
- en: 'where ${b_{a}}_{i}$, ${b_{s}}_{i}$, and ${b_{n}}_{i}$ denote the assignments
    corresponding to the $i^{th}$ run. This bias score falls in the $[-1,1]$ range:
    a score of $-1$ means only anti-stereotypical assignments are present, $1$ means
    only stereotypical assignments are present, and $0$ means neutral bias ⁴⁴4 When
    $b_{m}=b_{f}=1/2\times tot$, it means that the language model assigns an equal
    number of stereotypical and non-stereotypical assignments across the dataset and
    in that case, we would get a bias score of 0, showing a neutral assignment overall.
    Note however that this would be systematically different from the case when $b_{m}=b_{f}=0$,
    $b_{n}=$tot, where there is no bias overall. We do not observe the first case
    in our experiments. A negative bias shows higher anti-stereotypical assignments
    and a positive bias shows higher stereotypical assignments.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，${b_{a}}_{i}$、${b_{s}}_{i}$ 和 ${b_{n}}_{i}$ 表示与第 $i^{th}$ 次运行对应的分配。该偏见得分的范围是
    $[-1,1]$：得分为 $-1$ 表示只有反刻板印象的分配，得分为 $1$ 表示只有刻板印象的分配，得分为 $0$ 表示中立偏见 ⁴⁴4。当 $b_{m}=b_{f}=1/2\times
    tot$ 时，意味着语言模型在整个数据集中分配了相等数量的刻板印象和非刻板印象的任务，这时我们会得到偏见得分为 0，表示整体上是中立分配。然而，值得注意的是，这与
    $b_{m}=b_{f}=0$、$b_{n}=$tot 的情况是系统性不同的，在这种情况下整体没有偏见。在我们的实验中没有观察到第一种情况。负偏见表明反刻板印象的分配较多，而正偏见则表明刻板印象的分配较多。
- en: 5 Bias Detection using Multi-Agent LLM Interaction
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 使用多代理LLM交互进行偏见检测
- en: 'We create multi-agent interaction frameworks for all the scenarios present
    in the Scenarios Dataset. The scenarios are used for interaction, and the LLM
    agents depict personas as described in the characters of the scenarios. Personas
    are simple with just name and gender. This is intentional as we want to uncover
    biases in LLM outputs when all personas have just one difference, namely their
    gender. Note that each agent is initialized as a separate LLM, so parameters (and
    information) are not shared among the agents. Each agent has an individual memory,
    where we store generated outputs by all agents, when required. The order of agents
    is pre-determined based on the character sequence provided in the dataset, but
    we ensure that scenarios have random gender orders. We then construct multi-turn
    conversation rounds:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为情景数据集中的所有场景创建了多代理互动框架。场景用于互动，LLM代理人根据场景中描述的角色展现个性。个性设置简单，仅包含姓名和性别。这是有意为之，因为我们希望揭示当所有个性只有一个差异，即性别时，LLM输出中的偏见。请注意，每个代理人初始化时都是独立的LLM，因此参数（和信息）在代理人之间不会共享。每个代理人有一个单独的记忆，在需要时，我们会存储所有代理人生成的输出。代理人的顺序是基于数据集中提供的角色顺序预先确定的，但我们确保场景中的性别顺序是随机的。然后，我们构建了多轮对话：
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'First assignment: Agents take turns to assign tasks to all agents. They only
    have information about other agents’ personas and cannot see previous response(s)
    by other agent(s) until they have made their own assignment. This is to make sure
    agents do not conform to the assignment(s) by the previous agent(s).'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一项任务：代理人轮流分配任务给所有代理人。他们只了解其他代理人的个性信息，在完成自己的分配任务之前，无法看到其他代理人之前的回应。这是为了确保代理人不会盲目遵循前一个代理人所做的分配任务。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Two discussion rounds: Agents then interact with each other for two rounds
    with two main goals: (1) Convincing others that their task assignment is correct;
    (2) Being open to other perspectives. During the second round, we prompt the agents
    to come to a consensus on the task assignments⁵⁵5Note that we do not require all
    agents to have the same assignments for our experiments.. Here, agents can see
    what previous agents responded and reply accordingly based on previous conversational
    context.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两轮讨论：接下来，代理人将进行两轮互动，目标主要有两个：（1）说服他人他们的任务分配是正确的；（2）开放接受其他视角。在第二轮中，我们鼓励代理人就任务分配达成共识⁵⁵注：我们不要求所有代理人在实验中有相同的任务分配..
    在这一轮中，代理人可以看到前一个代理人的回应，并根据之前的对话上下文做出回应。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Last assignment: In the final round, we ask agents to provide their final task
    assignments based on previous conversations. Agents now have the whole conversation
    history in memory.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一项任务：在最后一轮中，我们要求代理人基于之前的对话提供最终的任务分配。此时，代理人已将整个对话历史存储在记忆中。
- en: 'Three models: gpt-35-turbo,⁶⁶6https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/
    gpt-4 OpenAI et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib36)) from
    the GPT-family and an open source model mistral-7b-instruct Jiang et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib22))
    are used for our experiments. We provide prompt templates and implementation details
    in Appendices [F](https://arxiv.org/html/2410.02584v1#A6 "Appendix F Prompt templates
    for interaction framework ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions") and [H.1](https://arxiv.org/html/2410.02584v1#A8.SS1
    "H.1 Inference details ‣ Appendix H Implementation Details and Computation Resources
    ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    respectively.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 三个模型：gpt-35-turbo,⁶⁶[https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/](https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/)
    ，gpt-4 OpenAI等人（[2024](https://arxiv.org/html/2410.02584v1#bib.bib36)）来自GPT系列，以及一个开源模型mistral-7b-instruct
    Jiang等人（[2023](https://arxiv.org/html/2410.02584v1#bib.bib22)）被用于我们的实验。我们在附录[F](https://arxiv.org/html/2410.02584v1#A6
    "附录F 互动框架的提示模板 ‣ 针对多代理LLM互动中的隐性偏见检测与缓解")和[H.1](https://arxiv.org/html/2410.02584v1#A8.SS1
    "H.1 推理细节 ‣ 附录H 实施细节与计算资源 ‣ 针对多代理LLM互动中的隐性偏见检测与缓解")中分别提供了提示模板和实施细节。
- en: '![Refer to caption](img/2fe72423d3623a6471474712d8c1ee97.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2fe72423d3623a6471474712d8c1ee97.png)'
- en: 'Figure 3: Domain-based analysis for ‘no-interaction’. Biases differ across
    domains. All scores are positive showing biases towards males by all models.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：‘无互动’领域分析。不同领域的偏见有所不同。所有模型的分数均为正，显示出对男性的偏见。
- en: '5.1 Experiments and Results: Bias Detection'
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验与结果：偏见检测
- en: 5.1.1 Multi-agent interaction
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 多代理互动
- en: '![Refer to caption](img/e07558a3f2b561d4103ab8bcd61b14e8.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e07558a3f2b561d4103ab8bcd61b14e8.png)'
- en: 'Figure 4: Domain-based analysis in the ‘interaction’ setting. All scores are
    positive showing biases towards males. Biases increase after interaction for all
    domains across models and settings.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在“互动”设置中的基于领域的分析。所有得分都是正值，显示出对男性的偏见。在所有模型和设置中，偏见在互动后增加。
- en: 'Table [1](https://arxiv.org/html/2410.02584v1#S3.T1 "Table 1 ‣ 3 Dataset ‣
    Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    shows the results of bias scores with three settings in total: 1) no interaction,
    2) interaction with no goal assigned, where agents have full control over task
    assignments, and 3) interaction with goal assigned, where each agent is privately
    asked to assign a common task to themselves before first assignment. For example,
    we prompt each agent privately to assign to themselves a task, say T1, which is
    a stereotypically male task. Since everyone would now assign T1 to themselves,
    we expect intial bias score to be reduced. For interaction-based settings, we
    display the results from before (first-response) and after interaction (last-response).
    In the ‘no interaction’ setting, we just provide the LLM with the scenarios, tasks
    and characters and prompt to output responses. There are no multi-agents or any
    interactions in this setting. We average our results over five LLM runs.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](https://arxiv.org/html/2410.02584v1#S3.T1 "表 1 ‣ 3 数据集 ‣ 向隐性偏见检测和减缓的多智能体LLM交互")
    显示了在三种设置下的偏见得分结果：1）无互动，2）没有分配目标的互动，在此设置中代理人完全控制任务分配，3）有目标分配的互动，在此设置中每个代理人在第一次分配前会私下被要求给自己分配一个共同的任务。例如，我们私下提示每个代理人给自己分配一个任务，比如
    T1，这是一个典型的男性任务。由于每个人现在都会给自己分配 T1，我们预计初始的偏见得分会降低。对于基于互动的设置，我们显示互动前（第一次响应）和互动后（最后一次响应）的结果。在“无互动”设置中，我们只提供
    LLM 所需的场景、任务和角色，并提示输出响应。此设置中没有多智能体或任何互动。我们的结果通过五次 LLM 运行平均计算。
- en: In the ‘no interaction’ setting, each model outputs stereotypical assignments
    in most scenarios ($\geq\approx 0.5$). mistral-7b-instruct outputs the least bias,
    followed by gpt-35-turbo and gpt-4. Interestingly, gpt-4 outputs the most biases
    even though it excels in generating implicit bias scenarios (as validated with
    humans). In the ‘no goal’ setting, first responses always have positive bias scores
    for all models, indicating biases toward males. The ‘goal’ setting has more controlled
    first responses with lower bias scores (for gpt-35-turbo and mistral-7b), as expected.
    For all settings, bias scores increase after LLM interactions. Despite initially
    lower biases in first-responses, biases consistently escalate to equal or higher
    levels in the "goal" setting than the ‘no goal’ setting. We also find that larger
    models exhibit higher biases.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在“无互动”设置中，每个模型在大多数场景中输出刻板的分配（$\geq\approx 0.5$）。mistral-7b-instruct 输出的偏见最小，其次是
    gpt-35-turbo 和 gpt-4。有趣的是，尽管 gpt-4 在生成隐性偏见场景方面表现优异（经过人工验证），它却输出了最多的偏见。在“无目标”设置中，所有模型的第一次响应总是具有正向偏见得分，表明对男性的偏见。在“目标”设置中，第一次响应的偏见得分较低，控制得更好（对于
    gpt-35-turbo 和 mistral-7b 如预期）。在所有设置中，偏见得分在大语言模型（LLM）交互后都会增加。尽管第一次响应中的偏见较低，但偏见始终会上升，甚至在“目标”设置中超过“无目标”设置的水平。我们还发现，较大的模型表现出更高的偏见。
- en: '![Refer to caption](img/1239651f8d8518c3b85c5dcefd722561.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1239651f8d8518c3b85c5dcefd722561.png)'
- en: 'Figure 5: Implicit Bias Mitigation strategies in multi-agent LLM interaction.
    We show FT, SR and an ensemble for FT and SR. (FT: Finetuning, SR: Self Reflection)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：多智能体LLM互动中的隐性偏见减缓策略。我们展示了 FT、SR 以及 FT 和 SR 的集成。（FT：微调，SR：自我反思）
- en: 5.1.2 Domain-based Analysis
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 基于领域的分析
- en: To gain insights into variations in biases across different domains and determine
    the importance of each domain in our experiments, we examine the bias scores for
    each domain, namely, family, office, hospital, politics, legal, school, and team
    dynamics. By analyzing these scores, we aim to better comprehend the disparities
    in biases observed within each domain.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入了解不同领域中的偏差变化，并确定每个领域在我们实验中的重要性，我们检查了各个领域的偏差得分，即家庭、办公室、医院、政治、法律、学校和团队动态。通过分析这些得分，我们旨在更好地理解每个领域中观察到的偏差差异。
- en: Fig [3](https://arxiv.org/html/2410.02584v1#S5.F3 "Figure 3 ‣ 5 Bias Detection
    using Multi-Agent LLM Interaction ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions") represents the bias scores in the ‘no interaction’
    setting. gpt-4 mostly has these highest bias score for all domains except Family,
    Politics and Legal domains. Top bias domains differ for each model, but overall
    Legal and Office have low biases across different models.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](https://arxiv.org/html/2410.02584v1#S5.F3 "图 3 ‣ 通过多智能体LLM交互进行偏见检测 ‣ 旨在多智能体LLM交互中的隐性偏见检测与缓解")表示在“无交互”设置中的偏见得分。gpt-4
    在所有领域中，除了家庭、政治和法律领域外，大多数领域的偏见得分都很高。不同模型的偏见得分最高的领域不同，但总体而言，法律和办公领域在不同模型中的偏见较低。
- en: 'Fig [4](https://arxiv.org/html/2410.02584v1#S5.F4 "Figure 4 ‣ 5.1.1 Multi-agent
    interaction ‣ 5.1 Experiments and Results: Bias Detection ‣ 5 Bias Detection using
    Multi-Agent LLM Interaction ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions") shows the bias scores for each domain in the ‘interaction’
    case with both ‘no goal’ and ‘goal’ settings. Across all domains, bias scores
    increase after interaction (as seen previously overall). Top topics vary by setting.
    However, the domain with the overall lowest bias score for all settings is Legal
    (as seen in the ‘no interaction setting’).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](https://arxiv.org/html/2410.02584v1#S5.F4 "图 4 ‣ 5.1.1 多智能体交互 ‣ 5.1 实验与结果：偏见检测
    ‣ 通过多智能体LLM交互进行偏见检测 ‣ 旨在多智能体LLM交互中的隐性偏见检测与缓解")显示了在“交互”情况下（包括“无目标”和“有目标”设置）各领域的偏见得分。所有领域的偏见得分在交互后都有所增加（如前所述总体情况）。不同设置下的主题有所不同。然而，在所有设置中，法律领域的偏见得分始终最低（如在“无交互”设置中所见）。
- en: 'The results from domain-based analysis show that all LLMs output a positive
    bias score for each domain. This highlights the importance of considering all
    domains in our dataset when evaluating bias. By taking into account the unique
    characteristics of each domain, we can ensure a comprehensive assessment of biases.
    In Appendix [C](https://arxiv.org/html/2410.02584v1#A3 "Appendix C Case study
    of one Domain - School ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions"), we focus on a case study for one domain: ‘School’, where we
    deep dive into conversations among agents and provide a qualitative and quantitative
    analysis of three different scenarios: task assignment, missing project deadline
    case, and team leader assignment. We find that the rationales provided by the
    agents in these scenarios consistently exhibit implicit biases, which influence
    the decision-making patterns.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 基于领域的分析结果显示，所有LLM在每个领域中都输出了正偏见得分。这突显了在评估偏见时考虑我们数据集中所有领域的重要性。通过考虑每个领域的独特特征，我们可以确保对偏见进行全面评估。在附录[C](https://arxiv.org/html/2410.02584v1#A3
    "附录 C 一个领域案例研究 - 学校 ‣ 旨在多智能体LLM交互中的隐性偏见检测与缓解")中，我们聚焦于一个领域的案例研究：“学校”，深入分析智能体之间的对话，并对三种不同场景进行定性和定量分析：任务分配、错过项目截止日期的情况以及团队领导分配。我们发现，智能体在这些场景中提供的理由始终表现出隐性偏见，这些偏见影响了决策模式。
- en: 6 Bias Mitigation
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 偏见缓解
- en: 'Previous experiments show that LLMs often produce responses that conform to
    societal stereotypes when assigning roles and responsibilities to different genders.
    Despite the implementation of human preference alignment techniques, models continue
    to fall short in generating unbiased outputs in their assigned tasks. Our findings
    show that implicit societal biases are deeply rooted within models, and current
    mitigation strategies are insufficient. This poses a significant risk of perpetuating
    harm against various marginalized and historically overlooked groups. Hence, we
    propose two approaches to mitigate biases: (1) Supervised fine-tuning of LLMs
    (changes model parameters), and (2) Self-reflection (no change in model parameters).
    We investigate both approaches separately and also create an ensemble to mitigate
    biases in interaction. Fig [5](https://arxiv.org/html/2410.02584v1#S5.F5 "Figure
    5 ‣ 5.1.1 Multi-agent interaction ‣ 5.1 Experiments and Results: Bias Detection
    ‣ 5 Bias Detection using Multi-Agent LLM Interaction ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") comprehensively demonstrate our
    implicit bias mitigation approaches.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '先前的实验表明，LLM 在为不同性别分配角色和责任时，常常产生符合社会刻板印象的回应。尽管实施了人类偏好对齐技术，模型在生成无偏输出方面依然存在不足。我们的研究发现，隐性社会偏见深深根植于模型中，当前的缓解策略不足以消除这些偏见。这带来了对各种边缘化群体和历史上被忽视群体的伤害风险。因此，我们提出了两种缓解偏见的方法：（1）LLM
    的有监督微调（改变模型参数），（2）自我反思（不改变模型参数）。我们分别研究了这两种方法，并且还创建了一个集成方法来缓解交互中的偏见。图[5](https://arxiv.org/html/2410.02584v1#S5.F5
    "Figure 5 ‣ 5.1.1 Multi-agent interaction ‣ 5.1 Experiments and Results: Bias
    Detection ‣ 5 Bias Detection using Multi-Agent LLM Interaction ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions")全面展示了我们的隐性偏见缓解方法。'
- en: 6.1 Fine-tuning (FT) LLM
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 微调（FT）LLM
- en: 'Fine-tuning is performed using two data settings: (1) Full Fine-tune Dataset,
    consisting of both implicit and non-implicit bias scenarios and (2) Half of Fine-tune
    Dataset, consisting of only non-implicit bias scenarios. Our hypothesis is that
    a full-data-fine-tuned model is capable of distinguishing implicit and non-implicit
    bias scenarios. In contrast, a half-data-fine-tuned model may struggle to capture
    the differences between the two, but could potentially be able to better generate
    assignments with no implicit biases as it is only trained with data having equal
    representation.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 微调使用两种数据设置进行：（1）完整的微调数据集，包含隐性和非隐性偏见场景，（2）半微调数据集，仅包含非隐性偏见场景。我们的假设是，经过完整数据微调的模型能够区分隐性和非隐性偏见场景。相反，经过半数据微调的模型可能难以捕捉两者之间的差异，但可能能更好地生成没有隐性偏见的任务，因为它仅使用具有平衡代表性的非隐性偏见数据进行训练。
- en: 'We fine-tune two models: gpt-35-turbo-0613 and mistral-7b-instruct⁷⁷7We can
    not fine-tune gpt-4 currently.. We have an 80/20 train/dev split of the Fine-tune
    dataset. Implementation details are provided in Appendix [H.2](https://arxiv.org/html/2410.02584v1#A8.SS2
    "H.2 Fine-tuning details ‣ Appendix H Implementation Details and Computation Resources
    ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions").'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们微调了两个模型：gpt-35-turbo-0613 和 mistral-7b-instruct⁷⁷7。目前我们无法对 gpt-4 进行微调。我们将微调数据集按
    80/20 的比例划分为训练集和验证集。实现细节见附录[H.2](https://arxiv.org/html/2410.02584v1#A8.SS2 "H.2
    Fine-tuning details ‣ Appendix H Implementation Details and Computation Resources
    ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")。
- en: '![Refer to caption](img/974fc1681749b295bd41577c2c103d26.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/974fc1681749b295bd41577c2c103d26.png)'
- en: 'Figure 6: Mitigation approaches in multi-agent LLM interaction. Ensemble approaches
    lead to the highest reduction in bias scores for gpt-35-turbo and mistral-7b-instruct.
    However, SR leads to negative bias scores in gpt-4. (SR: Self Reflection, ICE:
    In-Context Examples)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：多代理 LLM 交互中的偏差缓解方法。集成方法在 gpt-35-turbo 和 mistral-7b-instruct 中带来了偏差分数的最高减少。然而，在
    gpt-4 中，自我反思方法导致了负的偏差分数。（SR：自我反思，ICE：上下文示例）
- en: 6.2 Self-reflection Prompting With and Without In-context Examples
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 带有和不带有上下文示例的自我反思提示
- en: 'LLMs have exhibited promising performances using self-reflection for various
    domains Ganguli et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib12));
    Ji et al. ([2023](https://arxiv.org/html/2410.02584v1#bib.bib21)); Madaan et al.
    ([2023](https://arxiv.org/html/2410.02584v1#bib.bib32)); Han et al. ([2024](https://arxiv.org/html/2410.02584v1#bib.bib17)).
    In our experiments, we focus on two settings for self-reflection with a more specific
    reflection prompt in terms of implicit biases: (1) Without In-Context examples
    (no-ICE): we provide the definition of implicit biases in terms of task assignments,
    ask the agents to critique their first assignments based on the requirement, re-assign
    tasks when necessary and continue interaction; and (2) With In-Content examples
    (ICE): we provide the definition of implicit biases in terms of task assignments
    with three examples each of situations where implicit biases are present and situations
    where they are absent. And continue in a similar manner as without ICE. We share
    the prompt templates and in-context examples in Appendix [F.4](https://arxiv.org/html/2410.02584v1#A6.SS4
    "F.4 Self Reflection ‣ Appendix F Prompt templates for interaction framework ‣
    Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    and [F.5](https://arxiv.org/html/2410.02584v1#A6.SS5 "F.5 Self Reflection In-Context
    Examples ‣ Appendix F Prompt templates for interaction framework ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions") respectively.
    During reflection, we also ask the model to provide a reason for the presence/absence
    of implicit biases and assign tasks with reduced biases.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在多个领域中使用自我反思展现了良好的表现，见Ganguli等人（[2023](https://arxiv.org/html/2410.02584v1#bib.bib12)）；Ji等人（[2023](https://arxiv.org/html/2410.02584v1#bib.bib21)）；Madaan等人（[2023](https://arxiv.org/html/2410.02584v1#bib.bib32)）；Han等人（[2024](https://arxiv.org/html/2410.02584v1#bib.bib17)）。在我们的实验中，我们将重点放在两种自我反思的设置上，并且在隐性偏见方面使用更具体的反思提示：（1）无上下文示例（no-ICE）：我们提供任务分配中隐性偏见的定义，要求代理基于要求批判他们的第一次分配，并在必要时重新分配任务，继续进行交互；（2）有上下文示例（ICE）：我们提供任务分配中隐性偏见的定义，并提供三个例子，分别展示隐性偏见存在和不存在的情境。然后，继续以类似无ICE的方式进行。我们将在附录 [F.4](https://arxiv.org/html/2410.02584v1#A6.SS4
    "F.4 Self Reflection ‣ Appendix F Prompt templates for interaction framework ‣
    Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    和 [F.5](https://arxiv.org/html/2410.02584v1#A6.SS5 "F.5 Self Reflection In-Context
    Examples ‣ Appendix F Prompt templates for interaction framework ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions") 中分享提示模板和上下文示例。在反思过程中，我们还要求模型提供隐性偏见存在/不存在的原因，并进行减少偏见的任务分配。
- en: 6.2.1 Integrating Mitigation Strategies into the Interactions
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 将缓解策略整合到交互中
- en: 'Using our previous bias mitigation approaches, we experiment with three mitigation
    strategies for a multi-agent interaction framework as described in Fig [5](https://arxiv.org/html/2410.02584v1#S5.F5
    "Figure 5 ‣ 5.1.1 Multi-agent interaction ‣ 5.1 Experiments and Results: Bias
    Detection ‣ 5 Bias Detection using Multi-Agent LLM Interaction ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions"). We propose: (1)
    interaction with self-reflection, (2) interaction among fine-tuned agents and
    (3) interaction among fine-tuned agents with self-reflection (ensemble).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '使用我们之前的偏见缓解方法，我们在如图 [5](https://arxiv.org/html/2410.02584v1#S5.F5 "Figure 5
    ‣ 5.1.1 Multi-agent interaction ‣ 5.1 Experiments and Results: Bias Detection
    ‣ 5 Bias Detection using Multi-Agent LLM Interaction ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") 所示的多代理交互框架中实验了三种缓解策略。我们提出：（1）自我反思的交互，（2）经过微调的代理之间的交互，以及（3）经过微调的代理与自我反思的代理之间的交互（集成）。'
- en: '6.3 Experiments and Results: Bias Mitigation'
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 实验与结果：偏见缓解
- en: 'In order to assess the effectiveness of our bias mitigation strategies, we
    conduct evaluations in three comprehensive settings:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们偏见缓解策略的有效性，我们在三种综合设置中进行评估：
- en: '1.'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Understanding the presence of implicit biases: We evaluate if models can correctly
    identify the presence/absence of implicit biases in task assignments on the dev
    set of the Fine-tune dataset. Results and analysis are provided in Appendix [D.1](https://arxiv.org/html/2410.02584v1#A4.SS1
    "D.1 Understanding the Presence of Implicit Bias ‣ Appendix D Bias Mitigation
    Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions").'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 了解隐性偏见的存在：我们评估模型是否能够正确识别在Fine-tune数据集的开发集（dev set）中的任务分配中隐性偏见的存在与否。结果和分析见附录 [D.1](https://arxiv.org/html/2410.02584v1#A4.SS1
    "D.1 Understanding the Presence of Implicit Bias ‣ Appendix D Bias Mitigation
    Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")。
- en: '2.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Generation⁸⁸8During the process of fine-tuning models, our training objective
    is to identify implicit biases and provide the underlying reasoning. By evaluating
    the model’s generation capabilities, we can assess its ability to comprehend implicit
    biases from scenarios and minimize them in its responses. in the ‘no interaction’
    setting: We use the Test Dataset, which contains scenarios from domains different
    than the fine-tune data and prompt LLMs to output task assignments. Results and
    analysis are provided in Appendix [D.2](https://arxiv.org/html/2410.02584v1#A4.SS2
    "D.2 Generation evaluation in the ‘no interaction’ setting ‣ Appendix D Bias Mitigation
    Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions").'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成⁸⁸8在微调模型的过程中，我们的训练目标是识别隐性偏见并提供其背后的推理。通过评估模型的生成能力，我们可以评估其从场景中理解隐性偏见并在响应中最小化这些偏见的能力。在“无交互”设置中：我们使用包含来自不同于微调数据领域的场景的测试数据集，并提示LLM输出任务分配。结果和分析在附录[D.2](https://arxiv.org/html/2410.02584v1#A4.SS2
    "D.2 Generation evaluation in the ‘no interaction’ setting ‣ Appendix D Bias Mitigation
    Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")中提供。
- en: '3.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Generation in the ‘interaction setting’: Here, multi agents interact and utilize
    mitigation strategies to reduce implicit biases on the Test Dataset. We discuss
    this further below.'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在“互动设置”中的生成：在这里，多个智能体进行交互，并利用缓解策略来减少测试数据集上的隐性偏见。我们将在下文中进一步讨论这一点。
- en: Figure [6](https://arxiv.org/html/2410.02584v1#S6.F6 "Figure 6 ‣ 6.1 Fine-tuning
    (FT) LLM ‣ 6 Bias Mitigation ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions") illustrates the results of mitigation approaches
    on the multi-agent LLM interactions. It demonstrates that the ft-gpt-35-turbo
    with SR + ICE yields the lowest bias score of 0.01, indicating almost neutral
    or no bias. All our ensembles (fine-tuning + self-reflection) have the best performances
    for both gpt-35-turbo and mistral-7b-instruct. Among the two approaches, fine-tuning
    proves more effective than self-reflection in reducing implicit biases from the
    outset. This is visible right from the first responses, as well as reflected in
    lower bias scores overall across models. It is worth noting that the fine-tune
    data and test data have different domains, showing the effectiveness of fine-tuning
    in generation. The changes in bias scores after interactions, however, are minimal,
    for fine-tuned agents because the first responses themselves are less biased.
    Additionally, half-ft is more effective in mitigating biases in mistral-7b-instruct.
    Similarly, self-reflection mitigation effects are more pronounced for mistral-7b-instruct.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](https://arxiv.org/html/2410.02584v1#S6.F6 "Figure 6 ‣ 6.1 Fine-tuning (FT)
    LLM ‣ 6 Bias Mitigation ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions")展示了在多智能体LLM交互中缓解方法的结果。结果表明，结合SR和ICE的ft-gpt-35-turbo获得了最低的偏见得分0.01，几乎无偏见或偏见中性。我们所有的集成方法（微调+自我反思）在gpt-35-turbo和mistral-7b-instruct上都表现最佳。在这两种方法中，微调比自我反思在减少隐性偏见方面更为有效。这一点从第一次响应中就能看出，并且在模型的整体偏见得分中有所体现。值得注意的是，微调数据和测试数据来自不同领域，这展示了微调在生成过程中的有效性。然而，经过交互后的偏见得分变化对于微调智能体来说非常小，因为第一次的响应本身偏见就较少。此外，half-ft在缓解mistral-7b-instruct中的偏见方面更加有效。类似地，自我反思在缓解mistral-7b-instruct的偏见效果上更加明显。
- en: We find that gpt-4 generates negative bias scores, i.e. anti-stereotypical assignments
    using mitigation strategies and does not present equally representative task assignments
    after self-reflection. These results imply that smaller models benefit more from
    our mitigation strategies. Fig [18](https://arxiv.org/html/2410.02584v1#A4.F18
    "Figure 18 ‣ D.1 Understanding the Presence of Implicit Bias ‣ Appendix D Bias
    Mitigation Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions") in Appendix [D.3](https://arxiv.org/html/2410.02584v1#A4.SS3
    "D.3 Mitigation strategies in the ‘interaction’ setting with ‘goals’ given ‣ Appendix
    D Bias Mitigation Results ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions") shows the results for the ‘goal’ setting, which
    holds most of our results as discussed above. We further provide qualitative analysis
    of conversations during self-reflection and self-correction rates in Appendix [E](https://arxiv.org/html/2410.02584v1#A5
    "Appendix E Qualititative Analysis of self-reflection and ‘self-correction’ in
    multi-agent interactions ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions").
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，gpt-4在使用缓解策略后产生了负向偏见得分，即反刻板的任务分配，并且在自我反思后没有呈现出同等代表性的任务分配。这些结果表明，较小的模型在我们的缓解策略中受益更多。附录[D.3](https://arxiv.org/html/2410.02584v1#A4.SS3
    "D.3 在‘互动’设置中给定‘目标’的缓解策略 ‣ 附录 D 偏见缓解结果 ‣ 面向多代理LLM互动中的隐性偏见检测与缓解")中的图[18](https://arxiv.org/html/2410.02584v1#A4.F18
    "图 18 ‣ D.1 理解隐性偏见的存在 ‣ 附录 D 偏见缓解结果 ‣ 面向多代理LLM互动中的隐性偏见检测与缓解")展示了‘目标’设置的结果，这些结果与我们之前讨论的大部分内容一致。我们还在附录[E](https://arxiv.org/html/2410.02584v1#A5
    "附录 E 多代理互动中的自我反思与‘自我纠正’定性分析 ‣ 面向多代理LLM互动中的隐性偏见检测与缓解")中提供了对自我反思和自我纠正率的对话定性分析。
- en: 7 Conclusion and Lessons Learned
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与经验教训
- en: 'In this paper, we uncovered implicit gender biases in multi-agent LLM interactions
    using task assignment scenarios and proposed two mitigation strategies to reduce
    implicit biases in interaction frameworks. We also created a dataset of implicit
    bias scenarios and proposed a bias evaluation metric for task assignment scenarios,
    which can be used by the research community to analyze implicit biases in the
    output of LLMs. ⁹⁹9Our code and dataset are available at [https://github.com/MichiganNLP/MultiAgent_ImplicitBias](https://github.com/MichiganNLP/MultiAgent_ImplicitBias)
    Through our experiments and analyses, we learned several valuable insights:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本文揭示了在多代理LLM互动中通过任务分配场景发现的隐性性别偏见，并提出了两种缓解策略，以减少互动框架中的隐性偏见。我们还创建了一个隐性偏见场景数据集，并提出了一种用于任务分配场景的偏见评估指标，研究社区可以使用该指标来分析LLM输出中的隐性偏见。⁹⁹9我们的代码和数据集可以在[https://github.com/MichiganNLP/MultiAgent_ImplicitBias](https://github.com/MichiganNLP/MultiAgent_ImplicitBias)中找到。通过我们的实验和分析，我们获得了几项宝贵的见解：
- en: LLMs generate implicit biases even when trained with human preferences. We see
    positive bias scores ($\geq\approx 0.5$) for all models in both ‘interaction’
    and ‘no interaction’ settings in the first responses itself.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在使用人类偏好进行训练时，LLM仍然会产生隐性偏见。我们在“互动”和“非互动”设置中的所有模型的首次回应中，都看到了正向偏见得分（$\geq\approx
    0.5$）。
- en: 'Larger models are prone to produce more biased outputs. While LLMs like gpt-4
    excel in generating scenarios with implicit biases in various settings, they fall
    short in effectively generating task assignments without implicit biases. gpt-4
    exhibits the highest bias scores. This suggests that larger models, while potentially
    more helpful, may also exhibit higher levels of biases. Additionally, similar
    to human collectives, we regard single-agent versus multi-agent settings as a
    reflection to some extent of the difference between “theory” and “action”: a single
    agent often “theorizes” acceptable non-biased understanding of implicit biases
    (e.g., while generating the Scenarios dataset), whereas the “in action” multi-agents
    often end up making biased task assignments.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的模型容易产生更多的偏见输出。尽管像gpt-4这样的LLM在生成包含隐性偏见的场景时表现出色，但在有效生成没有隐性偏见的任务分配时却有所不足。gpt-4表现出最高的偏见得分。这表明，虽然更大的模型可能更有帮助，但也可能表现出更高水平的偏见。此外，类似于人类集体，我们将单一代理与多代理设置视为在某种程度上反映了“理论”与“行动”之间的差异：单一代理通常“理论化”了对隐性偏见的可接受的无偏理解（例如，在生成场景数据集时），而“在行动中”的多代理通常会在任务分配中产生偏见。
- en: Biases increase after multi-agent LLM interactions. Multi-agent LLM interaction
    analysis always shows an increase in biases after the interaction. Looking at
    the interactions, the justifications provided for task assignments predominantly
    align with traditional gender norms prevalent in societies, as extensively explored
    in prior studies discussed in Section [2](https://arxiv.org/html/2410.02584v1#S2
    "2 Related Work ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions"), although persona descriptions do not include any specific
    skill sets or reasons (they contain just name and gender).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理 LLM 交互后偏见增加。多代理 LLM 交互分析始终显示交互后偏见的增加。从这些交互中可以看出，任务分配时给出的理由大多与社会中普遍存在的传统性别规范相符，正如在第[2](https://arxiv.org/html/2410.02584v1#S2
    "2 Related Work ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions")节中广泛探讨的先前研究所示，尽管角色描述中没有包含任何特定的技能集或理由（仅包含姓名和性别）。
- en: Fine-tuning and self-reflection can be effective strategies for implicit bias
    mitigation. Implicit bias can be effectively reduced by fine-tuning using scenarios
    with and without implicit bias, or by self-reflection prompting. These widely
    used strategies can lead to a reduction in bias after the interaction. We also
    find that they are especially effective for smaller models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 微调和自我反思可以是有效的隐性偏见缓解策略。通过使用有隐性偏见和无隐性偏见的情境进行微调，或通过自我反思提示，可以有效减少隐性偏见。这些广泛使用的策略可以在交互后减少偏见。我们还发现，这些策略对于较小的模型尤其有效。
- en: Multi-agent LLM interactions show emergent social group behaviors We find that
    biases increase after interactions in multi-agent LLM frameworks. This behavior
    aligns with psychological theories like the Stereotype Threat Theory Steele and
    Aronson ([1995](https://arxiv.org/html/2410.02584v1#bib.bib45)) and Groupthink Janis
    ([1972](https://arxiv.org/html/2410.02584v1#bib.bib20)). Stereotype Threat theory
    suggests that individuals may feel anxious about confirming negative stereotypes,
    which can lead to underperformance and reinforce those stereotypes. Meanwhile,
    Groupthink highlights how the desire for consensus in cohesive groups can suppress
    dissenting views, reinforcing existing biases. Therefore, these theories suggest
    that group interactions can lead to the reinforcement of negative stereotypes
    after interaction, which we observe in our experiments. While this observation
    warrants further analysis, these theories can help explain how biases can intensify
    in multi-agent interactions, highlighting the emergent nature of LLMs within this
    framework.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理 LLM 交互表现出突现的社会群体行为。我们发现，在多代理 LLM 框架中，交互后偏见增加。这一行为与心理学理论相符，如刻板印象威胁理论（Steele
    和 Aronson [1995](https://arxiv.org/html/2410.02584v1#bib.bib45)）和群体思维理论（Janis
    [1972](https://arxiv.org/html/2410.02584v1#bib.bib20)）。刻板印象威胁理论表明，个体可能会因担心证实负面刻板印象而感到焦虑，这可能导致表现不佳并加强这些刻板印象。同时，群体思维理论强调，团体中追求共识的愿望可能会抑制异议声音，从而强化现有的偏见。因此，这些理论表明，群体互动可能导致负面刻板印象在交互后得到强化，这一点在我们的实验中得到了观察。虽然这一观察需要进一步分析，但这些理论有助于解释在多代理互动中偏见如何加剧，突显了在这一框架内
    LLMs 的突现特性。
- en: In the future, we aim to broaden our research by incorporating data from different
    LLMs to create a more comprehensive benchmark for implicit biases and extend the
    scope of biases to include factors such as religion, race, etc. Additionally,
    we plan to further analyze the role of interaction in increasing implicit biases
    in multi-agent systems. Furthermore, we plan to explore reinforcement learning
    strategies for mitigating biases. Lastly, we aim to address cross-cultural variations
    in implicit biases, emphasizing the need for a global perspective on understanding
    and addressing these biases.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来，我们旨在通过整合不同 LLM 的数据来拓宽研究，创建一个更全面的隐性偏见基准，并将偏见的范围扩展到包括宗教、种族等因素。此外，我们还计划进一步分析交互在增加多代理系统隐性偏见中的作用。此外，我们计划探索用于缓解偏见的强化学习策略。最后，我们旨在解决跨文化隐性偏见的差异，强调全球视角在理解和应对这些偏见中的重要性。
- en: 8 Limitations
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: In our mitigation experiments, we find that gpt-4 leads to negative biases after
    mitigation, which require further analysis. Currently proposed mitigation approaches
    for reducing biases in gpt-4, specifically self-reflection, have not been found
    to effectively address the issue. Due to the limitation of not being able to fine-tune,
    our evaluation is limited to self-reflection only, further emphasizing this constraint.
    We also plan to analyze why gpt-4 has the highest biases as well. It is also important
    to note that most of our data are generated by gpt-4. Therefore, it is advisable
    to approach the results produced by GPT-4 with a certain level of skepticism.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的缓解实验中，我们发现gpt-4在缓解后仍然表现出负面偏见，这需要进一步分析。目前为减少gpt-4中的偏见所提出的缓解方法，特别是自我反思，未能有效解决这一问题。由于无法微调的限制，我们的评估仅限于自我反思，这进一步强调了这一限制。我们还计划分析为什么gpt-4具有最高的偏见。还需要注意的是，我们的大部分数据是由gpt-4生成的。因此，建议对GPT-4生成的结果持一定程度的怀疑态度。
- en: Additionally, our dataset is limited to 111 scenarios, also because the number
    of implicit bias scenarios is scarce in the literature. In the future, we plan
    to create a larger dataset for implicit biases and extend the scope of biases
    to include factors beyond gender such as religion, race, and more.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的数据集仅限于111个情境，这也是因为文献中隐性偏见情境的数量较少。未来，我们计划创建一个更大的隐性偏见数据集，并扩展偏见范围，涵盖性别以外的因素，如宗教、种族等。
- en: 9 Ethical Considerations
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 伦理考虑
- en: We utilize gpt-4 to create scenarios for our dataset. The data, although validated
    by humans may contain hidden biases as seen in language models pre-trained with
    human-generated data Liang et al. ([2021](https://arxiv.org/html/2410.02584v1#bib.bib29)).
    Manual inspection (human validation) is therefore extremely crucial when dealing
    with LLM-generated data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用gpt-4创建了我们的数据集情境。尽管数据经过人工验证，但仍可能包含隐性偏见，正如Liang等人（[2021](https://arxiv.org/html/2410.02584v1#bib.bib29)）所看到的那样，使用人类生成数据预训练的语言模型中也存在这种偏见。因此，人工检查（人类验证）在处理LLM生成的数据时极为重要。
- en: Additionally, the data generated by gpt-4 is primarily influenced by Western
    perspectives and can be considered Western-Centric or WEIRD (Western, Educated,
    Industrialized, Rich, and Democratic) in nature Henrich et al. ([2010](https://arxiv.org/html/2410.02584v1#bib.bib19)).
    Consequently, it may not encompass implicit biases, scenarios, tasks, or characters
    that are unique to various cultures. Hence, we should exercise caution in assuming
    that the data can seamlessly translate across different cultural contexts.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由gpt-4生成的数据主要受到西方视角的影响，可以被视为以西方为中心或WEIRD（西方、受过教育、工业化、富裕和民主）性质的，Henrich等人（[2010](https://arxiv.org/html/2410.02584v1#bib.bib19)）指出。因此，这些数据可能无法涵盖不同文化中特有的隐性偏见、情境、任务或人物。因此，在假设数据能够跨文化情境无缝迁移时，我们应谨慎行事。
- en: Finally, annotation of implicit bias scenarios may be unpleasant/stressful to
    annotators Roberts ([2016](https://arxiv.org/html/2410.02584v1#bib.bib41)), therefore,
    we have limited the annotations to smaller sections of the data so annotations
    could be done in no more than 0.5 hour.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，隐性偏见情境的注释可能会让注释员感到不愉快或有压力，Roberts（[2016](https://arxiv.org/html/2410.02584v1#bib.bib41)）指出，因此，我们将注释限制在数据的较小部分，以便注释可以在不超过0.5小时内完成。
- en: Acknowledgments
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We thank the anonymous reviewers for their constructive feedback, and the members
    of the Language and Information Technologies lab at the University of Michigan
    for the insightful discussions during the early stage of the project. This project
    was partially funded by a National Science Foundation award (#2306372) and a grant
    from OpenAI. Any opinions, findings, and conclusions or recommendations expressed
    in this material are those of the authors and do not necessarily reflect the views
    of the National Science Foundation or OpenAI.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢匿名审稿人提供的建设性反馈，以及密歇根大学语言与信息技术实验室的成员们在项目初期阶段的深入讨论。本项目部分由美国国家科学基金会（NSF）资助（奖号：#2306372），以及OpenAI的资助。本文中表达的任何观点、发现、结论或建议仅代表作者个人观点，并不一定反映国家科学基金会或OpenAI的立场。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agiza et al. (2024) Ahmed Agiza, Mohamed Mostagir, and Sherief Reda. 2024. [Analyzing
    the impact of data selection and fine-tuning on economic and political biases
    in llms](https://arxiv.org/abs/2404.08699). *Preprint*, arXiv:2404.08699.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agiza等人（2024）Ahmed Agiza, Mohamed Mostagir, 和Sherief Reda. 2024. [分析数据选择与微调对LLM中经济和政治偏见的影响](https://arxiv.org/abs/2404.08699).
    *预印本*，arXiv:2404.08699。
- en: 'Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major,
    and Shmargaret Shmitchell. 2021. [On the dangers of stochastic parrots: Can language
    models be too big?](https://doi.org/10.1145/3442188.3445922) In *On the Dangers
    of Stochastic Parrots: Can Language Models Be Too Big?*, FAccT ’21, page 610–623,
    New York, NY, USA. Association for Computing Machinery.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bender et al. (2021) Emily M. Bender, Timnit Gebru, Angelina McMillan-Major,
    和 Shmargaret Shmitchell. 2021. [关于随机鹦鹉的危险：语言模型能否过于庞大？](https://doi.org/10.1145/3442188.3445922)
    发表在 *On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?*，FAccT
    ’21，第610–623页，纽约，NY，USA。Association for Computing Machinery.'
- en: Bolukbasi et al. (2016) Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh
    Saligrama, and Adam Kalai. 2016. Man is to computer programmer as woman is to
    homemaker? debiasing word embeddings. In *Proceedings of the 30th International
    Conference on Neural Information Processing Systems*, NIPS’16, page 4356–4364,
    Red Hook, NY, USA. Curran Associates Inc.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bolukbasi et al. (2016) Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh
    Saligrama, 和 Adam Kalai. 2016. 男人对计算机程序员的理解如同女人对家庭主妇的理解？去偏见化词嵌入。发表于 *Proceedings
    of the 30th International Conference on Neural Information Processing Systems*,
    NIPS’16, 第4356–4364页，Red Hook, NY, USA。Curran Associates Inc.
- en: Brooks et al. (2014) Alison Wood Brooks, Laura Huang, Sarah Wood Kearney, and
    Fiona E Murray. 2014. Investors prefer entrepreneurial ventures pitched by attractive
    men. *Proceedings of the National Academy of Sciences*, 111(12):4427–4431.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks et al. (2014) Alison Wood Brooks, Laura Huang, Sarah Wood Kearney, 和
    Fiona E Murray. 2014. 投资者更倾向于投资由有吸引力的男性推销的创业项目。 *Proceedings of the National Academy
    of Sciences*, 111(12):4427–4431.
- en: Brownstein and Zalta (2019) Michael Brownstein and Edward Zalta. 2019. Implicit
    bias.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brownstein 和 Zalta (2019) Michael Brownstein 和 Edward Zalta. 2019. 隐性偏见。
- en: Caliskan et al. (2017) Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan.
    2017. [Semantics derived automatically from language corpora contain human-like
    biases](https://doi.org/10.1126/science.aal4230). *Science*, 356(6334):183–186.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caliskan et al. (2017) Aylin Caliskan, Joanna J. Bryson, 和 Arvind Narayanan.
    2017. [从语言语料库自动推导出的语义包含类似人类的偏见](https://doi.org/10.1126/science.aal4230)。*Science*,
    356(6334):183–186.
- en: 'Chan et al. (2024) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2024. [Chateval: Towards better LLM-based
    evaluators through multi-agent debate](https://openreview.net/forum?id=FQepisCUWu).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chan et al. (2024) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, 和 Zhiyuan Liu. 2024. [Chateval: 通过多代理辩论推动更好的基于LLM的评估器](https://openreview.net/forum?id=FQepisCUWu)。发表于
    *The Twelfth International Conference on Learning Representations*.'
- en: 'Chapman et al. (2013) Elizabeth N Chapman, Anna Kaatz, and Molly Carnes. 2013.
    Physicians and implicit bias: how doctors may unwittingly perpetuate health care
    disparities. *Journal of general internal medicine*, 28:1504–1510.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chapman et al. (2013) Elizabeth N Chapman, Anna Kaatz, 和 Molly Carnes. 2013.
    医生与隐性偏见：医生如何无意中延续医疗差异。*Journal of general internal medicine*, 28:1504–1510.
- en: Chen et al. (2024) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang,
    and Benyou Wang. 2024. [Humans or llms as the judge? a study on judgement biases](https://arxiv.org/abs/2402.10669).
    *Preprint*, arXiv:2402.10669.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2024) Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang,
    和 Benyou Wang. 2024. [人类还是LLM作为裁判？关于判断偏见的研究](https://arxiv.org/abs/2402.10669)。*Preprint*,
    arXiv:2402.10669.
- en: Cheng et al. (2024) Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, and Tianyu Shi. 2024.
    [Reinforcement learning from multi-role debates as feedback for bias mitigation
    in llms](https://arxiv.org/abs/2404.10160). *Preprint*, arXiv:2404.10160.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2024) Ruoxi Cheng, Haoxuan Ma, Shuirong Cao, 和 Tianyu Shi. 2024.
    [从多角色辩论中进行强化学习，作为LLM偏见缓解的反馈](https://arxiv.org/abs/2404.10160)。*Preprint*, arXiv:2404.10160.
- en: 'Dalton and Villagran (2018) Shamika Dalton and Michele Villagran. 2018. Minimizing
    and addressing implicit bias in the workplace: Be proactive, part one. *College
    & Research Libraries News*, 79(9):478.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dalton 和 Villagran (2018) Shamika Dalton 和 Michele Villagran. 2018. 在职场中最小化和应对隐性偏见：要主动，第一部分。*College
    & Research Libraries News*, 79(9):478.
- en: Ganguli et al. (2023) Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I
    Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson,
    Danny Hernandez, et al. 2023. The capacity for moral self-correction in large
    language models. *arXiv preprint arXiv:2302.07459*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganguli et al. (2023) Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas I
    Liao, Kamilė Lukošiūtė, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson,
    Danny Hernandez, 等人. 2023. 大型语言模型中的道德自我修正能力。*arXiv preprint arXiv:2302.07459*.
- en: Godsil et al. (2014) Rachel D Godsil, Linda R Tropp, Philip Atiba Goff, and
    John A Powell. 2014. Addressing implicit bias, racial anxiety, and stereotype
    threat in education and health care. *The Science of Equality*, 1(November):1–90.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Godsil et al. (2014) Rachel D Godsil, Linda R Tropp, Philip Atiba Goff, 和 John
    A Powell. 2014. 《在教育和医疗保健中解决隐性偏见、种族焦虑和刻板印象威胁》。*平等科学*，1（11月）：1–90。
- en: 'Gonen and Goldberg (2019) Hila Gonen and Yoav Goldberg. 2019. [Lipstick on
    a pig: Debiasing methods cover up systematic gender biases in word embeddings
    but do not remove them](https://aclanthology.org/W19-3621). In *Proceedings of
    the 2019 Workshop on Widening NLP*, pages 60–63, Florence, Italy. Association
    for Computational Linguistics.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gonen and Goldberg (2019) Hila Gonen 和 Yoav Goldberg. 2019. [《给猪涂口红：去偏见方法掩盖了词嵌入中的系统性性别偏见，但并未消除它们》](https://aclanthology.org/W19-3621)。发表于
    *2019年NLP拓展研讨会论文集*，第60–63页，意大利佛罗伦萨。计算语言学协会。
- en: Gullo (2017) Gina Laura Gullo. 2017. *Implicit bias in school disciplinary decisions*.
    Ph.D. thesis, Lehigh University.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gullo (2017) Gina Laura Gullo. 2017. *学校纪律决定中的隐性偏见*。博士论文，莱哈大学。
- en: 'Gupta et al. (2024) Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande,
    Ashwin Kalyan, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. [Bias runs
    deep: Implicit reasoning biases in persona-assigned LLMs](https://openreview.net/forum?id=kGteeZ18Ir).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta et al. (2024) Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande,
    Ashwin Kalyan, Peter Clark, Ashish Sabharwal, 和 Tushar Khot. 2024. [《偏见深入人心：人格分配的大型语言模型中的隐性推理偏见》](https://openreview.net/forum?id=kGteeZ18Ir)。发表于
    *第十二届国际学习表征大会*。
- en: Han et al. (2024) Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, and Yanghua
    Xiao. 2024. [Small language model can self-correct](https://arxiv.org/abs/2401.07301).
    *Preprint*, arXiv:2401.07301.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2024) Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, 和 Yanghua Xiao.
    2024. [《小型语言模型可以自我修正》](https://arxiv.org/abs/2401.07301)。*预印本*，arXiv:2401.07301。
- en: Hardt et al. (2016) Moritz Hardt, Eric Price, Eric Price, and Nati Srebro. 2016.
    [Equality of opportunity in supervised learning](https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 29\. Curran Associates,
    Inc.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hardt et al. (2016) Moritz Hardt, Eric Price, Eric Price, 和 Nati Srebro. 2016.
    [《监督学习中的机会平等》](https://proceedings.neurips.cc/paper_files/paper/2016/file/9d2682367c3935defcb1f9e247a97c0d-Paper.pdf)。发表于
    *神经信息处理系统进展*，第29卷。Curran Associates, Inc.
- en: Henrich et al. (2010) Joseph Henrich, Steven J Heine, and Ara Norenzayan. 2010.
    The weirdest people in the world? *Behavioral and brain sciences*, 33(2-3):61–83.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Henrich et al. (2010) Joseph Henrich, Steven J Heine, 和 Ara Norenzayan. 2010.
    《世界上最怪异的人们？》 *行为与大脑科学*，33(2-3)：61–83。
- en: 'Janis (1972) Irving L Janis. 1972. Victims of groupthink: A psychological study
    of foreign-policy decisions and fiascoes. *American Psychological Association*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Janis (1972) Irving L Janis. 1972. 《群体思维的受害者：一项关于外交政策决策与灾难的心理学研究》。*美国心理学会*。
- en: 'Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and
    Pascale Fung. 2023. [Towards mitigating LLM hallucination via self reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.123).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    1827–1843, Singapore. Association for Computational Linguistics.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji et al. (2023) Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, 和
    Pascale Fung. 2023. [《通过自我反思缓解大型语言模型幻觉》](https://doi.org/10.18653/v1/2023.findings-emnlp.123)。发表于
    *计算语言学协会发现：EMNLP 2023*，第1827–1843页，新加坡。计算语言学协会。
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
    William El Sayed. 2023. [Mistral 7b](https://arxiv.org/abs/2310.06825). *Preprint*,
    arXiv:2310.06825.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
    Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, 和
    William El Sayed. 2023. [《Mistral 7b》](https://arxiv.org/abs/2310.06825)。*预印本*，arXiv:2310.06825。
- en: Kang et al. (2011) Jerry Kang, Mark Bennett, Devon Carbado, Pam Casey, and Justin
    Levinson. 2011. Implicit bias in the courtroom. *UCLa L. rev.*, 59:1124.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang et al. (2011) Jerry Kang, Mark Bennett, Devon Carbado, Pam Casey, 和 Justin
    Levinson. 2011. 《法庭中的隐性偏见》。*UCLa法学评论*，59:1124。
- en: Kinder and Ryan (2017) Donald R Kinder and Timothy J Ryan. 2017. Prejudice and
    politics re-examined the political significance of implicit racial bias. *Political
    Science Research and Methods*, 5(2):241–259.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinder 和 Ryan（2017）Donald R Kinder 和 Timothy J Ryan。2017年。再审视偏见与政治：隐性种族偏见的政治意义。*政治科学研究与方法*，5(2)：241–259。
- en: Kotek et al. (2023) Hadas Kotek, Rikker Dockum, and David Sun. 2023. Gender
    bias and stereotypes in large language models. In *Proceedings of The ACM Collective
    Intelligence Conference*, pages 12–24.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotek 等人（2023）Hadas Kotek、Rikker Dockum 和 David Sun。2023年。大型语言模型中的性别偏见与刻板印象。收录于*ACM集体智能会议论文集*，第12–24页。
- en: 'Koutcheme et al. (2024) Charles Koutcheme, Nicola Dainese, Sami Sarsa, Arto
    Hellas, Juho Leinonen, and Paul Denny. 2024. [Open source language models can
    provide feedback: Evaluating llms’ ability to help students using gpt-4-as-a-judge](https://arxiv.org/abs/2405.05253).
    *Preprint*, arXiv:2405.05253.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koutcheme 等人（2024）Charles Koutcheme、Nicola Dainese、Sami Sarsa、Arto Hellas、Juho
    Leinonen 和 Paul Denny。2024年。[开源语言模型可以提供反馈：评估LLM在使用GPT-4作为评审时帮助学生的能力](https://arxiv.org/abs/2405.05253)。*预印本*，arXiv:2405.05253。
- en: 'Levinson et al. (2010) Justin D Levinson, Huajian Cai, and Danielle Young.
    2010. Guilty by implicit racial bias: The guilty/not guilty implicit association
    test. *Ohio St. J. Crim. L.*, 8:187.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levinson 等人（2010）Justin D Levinson、Huajian Cai 和 Danielle Young。2010年。通过隐性种族偏见定罪：有罪/无罪隐性联想测试。*俄亥俄州立大学刑法杂志*，8：187。
- en: 'Li et al. (2024) Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang,
    Liu Leqi, and Yang Liu. 2024. [Steering LLMs towards unbiased responses: A causality-guided
    debiasing framework](https://openreview.net/forum?id=RYdozB0GdB). In *ICLR 2024
    Workshop on Secure and Trustworthy Large Language Models*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2024）Jingling Li、Zeyu Tang、Xiaoyu Liu、Peter Spirtes、Kun Zhang、Liu Leqi
    和 Yang Liu。2024年。[引导LLM朝向无偏回答：一个因果指导的去偏框架](https://openreview.net/forum?id=RYdozB0GdB)。在*ICLR
    2024关于安全与可信赖大语言模型的研讨会*中。
- en: Liang et al. (2021) Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan
    Salakhutdinov. 2021. [Towards understanding and mitigating social biases in language
    models](https://proceedings.mlr.press/v139/liang21a.html). In *Proceedings of
    the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pages 6565–6576\. PMLR.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2021）Paul Pu Liang、Chiyu Wu、Louis-Philippe Morency 和 Ruslan Salakhutdinov。2021年。[迈向理解和缓解语言模型中的社会偏见](https://proceedings.mlr.press/v139/liang21a.html)。在*第38届国际机器学习会议论文集*中，机器学习研究论文集第139卷，第6565–6576页。PMLR。
- en: Liu (2023) Gabrielle Kaili-May Liu. 2023. Perspectives on the social impacts
    of reinforcement learning with human feedback. *arXiv preprint arXiv:2303.02891*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu（2023）Gabrielle Kaili-May Liu。2023年。关于带有人类反馈的强化学习的社会影响的视角。*arXiv预印本arXiv:2303.02891*。
- en: Lu et al. (2019) Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla,
    and Anupam Datta. 2019. [Gender bias in neural natural language processing](https://arxiv.org/abs/1807.11714).
    *Preprint*, arXiv:1807.11714.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人（2019）Kaiji Lu、Piotr Mardziel、Fangjing Wu、Preetam Amancharla 和 Anupam Datta。2019年。[神经自然语言处理中的性别偏见](https://arxiv.org/abs/1807.11714)。*预印本*，arXiv:1807.11714。
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh, and Peter Clark. 2023. [Self-refine: Iterative refinement with
    self-feedback](https://openreview.net/forum?id=S37hOerQLB). In *Thirty-seventh
    Conference on Neural Information Processing Systems*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人（2023）Aman Madaan、Niket Tandon、Prakhar Gupta、Skyler Hallinan、Luyu Gao、Sarah
    Wiegreffe、Uri Alon、Nouha Dziri、Shrimai Prabhumoye、Yiming Yang、Shashank Gupta、Bodhisattwa
    Prasad Majumder、Katherine Hermann、Sean Welleck、Amir Yazdanbakhsh 和 Peter Clark。2023年。[Self-refine：通过自我反馈的迭代优化](https://openreview.net/forum?id=S37hOerQLB)。在*第三十七届神经信息处理系统大会*中。
- en: 'Makarova et al. (2019) Elena Makarova, Belinda Aeschlimann, and Walter Herzog.
    2019. The gender gap in stem fields: The impact of the gender stereotype of math
    and science on secondary students’ career aspirations. In *Frontiers in Education*,
    volume 4, page 60\. Frontiers Media SA.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makarova 等人（2019）Elena Makarova、Belinda Aeschlimann 和 Walter Herzog。2019年。STEM领域中的性别差距：数学与科学的性别刻板印象对中学生职业抱负的影响。收录于*教育前沿*，第4卷，第60页。Frontiers
    Media SA。
- en: 'May et al. (2019) Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman,
    and Rachel Rudinger. 2019. [On measuring social biases in sentence encoders](https://doi.org/10.18653/v1/N19-1063).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 622–628, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: May等人（2019）Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, 和 Rachel
    Rudinger. 2019. [在句子编码器中衡量社会偏见](https://doi.org/10.18653/v1/N19-1063)。在*2019年北美计算语言学协会年会：人类语言技术，卷1（长篇和短篇论文）*的论文集，页码622-628，明尼阿波利斯，明尼苏达州。计算语言学协会。
- en: 'Nadler (2010) Joel T Nadler. 2010. *Explicit and implicit gender bias in workplace
    appraisals: How automatic prejudice affects decision making*. Southern Illinois
    University at Carbondale.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nadler (2010) Joel T Nadler. 2010. *职场评估中的显性和隐性性别偏见：自动偏见如何影响决策制定*。南伊利诺伊大学卡本代尔分校。
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. 2024. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2024）OpenAI，Josh Achiam，Steven Adler，Sandhini Agarwal，Lama Ahmad，Ilge
    Akkaya，Florencia Leoni Aleman，Diogo Almeida，Janko Altenschmidt，Sam Altman，Shyamal
    Anadkat，Red Avila，Igor Babuschkin，Suchir Balaji，Valerie Balcom，Paul Baltescu，Haiming
    Bao，Mohammad Bavarian，Jeff Belgum，Irwan Bello，Jake Berdine，Gabriel Bernadett-Shapiro，Christopher
    Berner，Lenny Bogdonoff，Oleg Boiko，Madelaine Boyd，Anna-Luisa Brakman，Greg Brockman，Tim
    Brooks，Miles Brundage，Kevin Button，Trevor Cai，Rosie Campbell，Andrew Cann，Brittany
    Carey，Chelsea Carlson，Rory Carmichael，Brooke Chan，Che Chang，Fotis Chantzis，Derek
    Chen，Sully Chen，Ruby Chen，Jason Chen，Mark Chen，Ben Chess，Chester Cho，Casey Chu，Hyung
    Won Chung，Dave Cummings，Jeremiah Currier，Yunxing Dai，Cory Decareaux，Thomas Degry，Noah
    Deutsch，Damien Deville，Arka Dhar，David Dohan，Steve Dowling，Sheila Dunning，Adrien
    Ecoffet，Atty Eleti，Tyna Eloundou，David Farhi，Liam Fedus，Niko Felix，Simón Posada
    Fishman，Juston Forte，Isabella Fulford，Leo Gao，Elie Georges，Christian Gibson，Vik
    Goel，Tarun Gogineni，Gabriel Goh，Rapha Gontijo-Lopes，Jonathan Gordon，Morgan Grafstein，Scott
    Gray，Ryan Greene，Joshua Gross，Shixiang Shane Gu，Yufei Guo，Chris Hallacy，Jesse
    Han，Jeff Harris，Yuchen He，Mike Heaton，Johannes Heidecke，Chris Hesse，Alan Hickey，Wade
    Hickey，Peter Hoeschele，Brandon Houghton，Kenny Hsu，Shengli Hu，Xin Hu，Joost Huizinga，Shantanu
    Jain，Shawn Jain，Joanne Jang，Angela Jiang，Roger Jiang，Haozhun Jin，Denny Jin，Shino
    Jomoto，Billie Jonn，Heewoo Jun，Tomer Kaftan，Łukasz Kaiser，Ali Kamali，Ingmar Kanitscheider，Nitish
    Shirish Keskar，Tabarak Khan，Logan Kilpatrick，Jong Wook Kim，Christina Kim，Yongjik
    Kim，Jan Hendrik Kirchner，Jamie Kiros，Matt Knight，Daniel Kokotajlo，Łukasz Kondraciuk，Andrew
    Kondrich，Aris Konstantinidis，Kyle Kosic，Gretchen Krueger，Vishal Kuo，Michael Lampe，Ikai
    Lan，Teddy Lee，Jan Leike，Jade Leung，Daniel Levy，Chak Ming Li，Rachel Lim，Molly Lin，Stephanie
    Lin，Mateusz Litwin，Theresa Lopez，Ryan Lowe，Patricia Lue，Anna Makanju，Kim Malfacini，Sam
    Manning，Todor Markov，Yaniv Markovski，Bianca Martin，Katie Mayer，Andrew Mayne，Bob
    McGrew，Scott Mayer McKinney，Christine McLeavey，Paul McMillan，Jake McNeil，David
    Medina，Aalok Mehta，Jacob Menick，Luke Metz，Andrey Mishchenko，Pamela Mishkin，Vinnie
    Monaco，Evan Morikawa，Daniel Mossing，Tong Mu，Mira Murati，Oleg Murk，David Mély，Ashvin
    Nair，Reiichiro Nakano，Rajeev Nayak，Arvind Neelakantan，Richard Ngo，Hyeonwoo Noh，Long
    Ouyang，Cullen O’Keefe，Jakub Pachocki，Alex Paino，Joe Palermo，Ashley Pantuliano，Giambattista
    Parascandolo，Joel Parish，Emy Parparita，Alex Passos，Mikhail Pavlov，Andrew Peng，Adam
    Perelman，Filipe de Avila Belbute Peres，Michael Petrov，Henrique Ponde de Oliveira
    Pinto，Michael，Pokorny，Michelle Pokrass，Vitchyr H. Pong，Tolly Powell，Alethea Power，Boris
    Power，Elizabeth Proehl，Raul Puri，Alec Radford，Jack Rae，Aditya Ramesh，Cameron Raymond，Francis
    Real，Kendra Rimbach，Carl Ross，Bob Rotsted，Henri Roussez，Nick Ryder，Mario Saltarelli，Ted
    Sanders，Shibani Santurkar，Girish Sastry，Heather Schmidt，David Schnurr，John Schulman，Daniel
    Selsam，Kyla Sheppard，Toki Sherbakov，Jessica Shieh，Sarah Shoker，Pranav Shyam，Szymon
    Sidor，Eric Sigler，Maddie Simens，Jordan Sitkin，Katarina Slama，Ian Sohl，Benjamin
    Sokolowsky，Yang Song，Natalie Staudacher，Felipe Petroski Such，Natalie Summers，Ilya
    Sutskever，Jie Tang，Nikolas Tezak，Madeleine B. Thompson，Phil Tillet，Amin Tootoonchian，Elizabeth
    Tseng，Preston Tuggle，Nick Turley，Jerry Tworek，Juan Felipe Cerón Uribe，Andrea Vallone，Arun
    Vijayvergiya，Chelsea Voss，Carroll Wainwright，Justin Jay Wang，Alvin Wang，Ben Wang，Jonathan
    Ward，Jason Wei，CJ Weinmann，Akila Welihinda，Peter Welinder，Jiayi Weng，Lilian Weng，Matt
    Wiethoff，Dave Willner，Clemens Winter，Samuel Wolrich，Hannah Wong，Lauren Workman，Sherwin
    Wu，Jeff Wu，Michael Wu，Kai Xiao，Tao Xu，Sarah Yoo，Kevin Yu，Qiming Yuan，Wojciech
    Zaremba，Rowan Zellers，Chong Zhang，Marvin Zhang，Shengjia Zhao，Tianhao Zheng，Juntang
    Zhuang，William Zhuk 和 Barret Zoph. 2024. [GPT-4 技术报告](https://arxiv.org/abs/2303.08774)。*预印本*，arXiv:2303.08774。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
    Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
    [Training language models to follow instructions with human feedback](https://openreview.net/forum?id=TG8KACxEON).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人（2022）Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Christiano, Jan Leike, 和 Ryan Lowe。2022年。[训练语言模型以遵循指令并结合人类反馈](https://openreview.net/forum?id=TG8KACxEON)。发表于*神经信息处理系统进展*。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://doi.org/10.1145/3586183.3606763). In *Proceedings
    of the 36th Annual ACM Symposium on User Interface Software and Technology*, UIST
    ’23, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2023）Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S. Bernstein。2023年。[生成代理：人类行为的互动模拟体](https://doi.org/10.1145/3586183.3606763)。发表于*第36届ACM用户界面软件与技术年会论文集*，UIST
    ’23，纽约，NY，美国。计算机协会。
- en: Pritlove et al. (2019) Cheryl Pritlove, Clara Juando-Prats, Kari Ala-Leppilampi,
    and Janet A Parsons. 2019. The good, the bad, and the ugly of implicit bias. *The
    Lancet*, 393(10171):502–504.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pritlove等人（2019）Cheryl Pritlove, Clara Juando-Prats, Kari Ala-Leppilampi, 和
    Janet A Parsons。2019年。隐性偏见的好、坏与丑。*柳叶刀*，393(10171):502–504。
- en: Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,
    et al. 2018. Improving language understanding by generative pre-training. *OpenAI*.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2018）Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever等人。2018年。通过生成预训练提高语言理解能力。*OpenAI*。
- en: 'Roberts (2016) Sarah T Roberts. 2016. Commercial content moderation: Digital
    laborers’ dirty work. *Western University*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roberts（2016）Sarah T Roberts。2016年。商业内容审查：数字劳工的脏活。*西方大学*。
- en: 'Rudinger et al. (2018) Rachel Rudinger, Jason Naradowsky, Brian Leonard, and
    Benjamin Van Durme. 2018. [Gender bias in coreference resolution](https://doi.org/10.18653/v1/N18-2002).
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)*,
    pages 8–14, New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudinger等人（2018）Rachel Rudinger, Jason Naradowsky, Brian Leonard, 和 Benjamin
    Van Durme。2018年。[共指解析中的性别偏见](https://doi.org/10.18653/v1/N18-2002)。发表于*2018年北美计算语言学协会会议：人类语言技术卷2（短论文）*，第8–14页，美国路易斯安那州新奥尔良。计算语言学协会。
- en: 'Staats (2016) Cheryl Staats. 2016. Understanding implicit bias: What educators
    should know. *American Educator*, 39(4):29.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Staats（2016）Cheryl Staats。2016年。理解隐性偏见：教育工作者应了解的内容。*美国教育者*，39(4):29。
- en: 'Stea et al. (2022) Tonje Holte Stea, Susanne Aune Solaas, and Annette Løvheim
    Kleppang. 2022. Association between physical activity, sedentary time, participation
    in organized activities, social support, sleep problems and mental distress among
    adults in southern norway: A cross-sectional study among 28,047 adults from the
    general population. *BMC public health*, 22(1):384.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stea等人（2022）Tonje Holte Stea, Susanne Aune Solaas, 和 Annette Løvheim Kleppang。2022年。挪威南部成人中身体活动、久坐时间、参与组织活动、社会支持、睡眠问题和心理困扰之间的关系：来自28,047名普通人群成人的横断面研究。*BMC公共卫生*，22(1):384。
- en: Steele and Aronson (1995) Claude M Steele and Joshua Aronson. 1995. Stereotype
    threat and the intellectual test performance of african americans. *Journal of
    personality and social psychology*, 69(5):797.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Steele和Aronson（1995）Claude M Steele 和 Joshua Aronson。1995年。刻板印象威胁与非裔美国人智力测试表现的关系。*人格与社会心理学杂志*，69(5):797。
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020.
    [Learning to summarize with human feedback](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 3008–3021\.
    Curran Associates, Inc.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon等人（2020）Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan
    Lowe, Chelsea Voss, Alec Radford, Dario Amodei, 和 Paul F Christiano。2020年。[通过人类反馈学习摘要](https://proceedings.neurips.cc/paper_files/paper/2020/file/1f89885d556929e98d3ef9b86448f951-Paper.pdf)。发表于*神经信息处理系统进展*，第33卷，第3008–3021页。Curran
    Associates, Inc。
- en: 'Struffolino (2017) Michele N Struffolino. 2017. The devil you don’t know: Implicit
    bias keeps women in their place. *Pace L. Rev.*, 38:260.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斯特鲁佛利诺（2017）米凯尔·N·斯特鲁佛利诺。2017年。你不知道的魔鬼：隐性偏见让女性保持原位。*Pace L. Rev.*，38:260。
- en: 'Sun et al. (2019) Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief,
    Jieyu Zhao, Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang.
    2019. [Mitigating gender bias in natural language processing: Literature review](https://doi.org/10.18653/v1/P19-1159).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 1630–1640, Florence, Italy. Association for Computational
    Linguistics.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 孙等人（2019）托尼·孙、安德鲁·高特、希尔琳·唐、宇欣·黄、迈·埃尔谢里夫、杰瑜·赵、迪巴·米尔扎、伊丽莎白·贝尔丁、凯-韦·张和威廉·杨·王。2019年。[缓解自然语言处理中的性别偏见：文献综述](https://doi.org/10.18653/v1/P19-1159)。载于*第57届计算语言学会年会论文集*，第1630–1640页，意大利佛罗伦萨。计算语言学会。
- en: 'Wan et al. (2023) Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei
    Chang, and Nanyun Peng. 2023. [“kelly is a warm person, joseph is a role model”:
    Gender biases in LLM-generated reference letters](https://doi.org/10.18653/v1/2023.findings-emnlp.243).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    3730–3748, Singapore. Association for Computational Linguistics.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 万等人（2023）一欣·万、乔治·蒲、焦·孙、阿帕尔纳·加里梅拉、凯-韦·张和南云·彭。2023年。[“凯莉是一个温暖的人，约瑟夫是一个榜样”：LLM生成推荐信中的性别偏见](https://doi.org/10.18653/v1/2023.findings-emnlp.243)。载于*计算语言学会发现：EMNLP
    2023*，第3730–3748页，新加坡。计算语言学会。
- en: 'Wang et al. (2024) Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten
    Sap, Graham Neubig, Yonatan Bisk, and Hao Zhu. 2024. [Sotopia-$\pi$: Interactive
    learning of socially intelligent language agents](https://arxiv.org/abs/2403.08715).
    *Preprint*, arXiv:2403.08715.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '王等人（2024）瑞宜·王、浩飞·于、文昕·张、正阳·齐、马尔滕·萨普、格雷厄姆·纽比格、约纳坦·比斯克、浩·朱。2024年。[Sotopia-$\pi$:
    社会智能语言代理的互动学习](https://arxiv.org/abs/2403.08715)。*预印本*，arXiv:2403.08715。'
- en: 'Webster et al. (2018) Kellie Webster, Marta Recasens, Vera Axelrod, and Jason
    Baldridge. 2018. [Mind the GAP: A balanced corpus of gendered ambiguous pronouns](https://doi.org/10.1162/tacl_a_00240).
    *Transactions of the Association for Computational Linguistics*, 6:605–617.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韦伯斯特等人（2018）凯莉·韦伯斯特、玛尔塔·雷卡森斯、维拉·阿克塞尔罗德和杰森·巴尔德里奇。2018年。[注意性别模糊代词的使用：一个平衡的性别模糊代词语料库](https://doi.org/10.1162/tacl_a_00240)。*计算语言学会会刊*，6:605–617。
- en: 'Williams and Bornstein (2007) Joan C Williams and Stephanie Bornstein. 2007.
    Evolution of fred: Family responsibilities discrimination and developments in
    the law of stereotyping and implicit bias. *HastINgs lJ*, 59:1311.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯与博恩斯坦（2007）琼·C·威廉姆斯和斯蒂芬妮·博恩斯坦。2007年。Fred的演变：家庭责任歧视和关于刻板印象及隐性偏见的法律发展。*HastINgs
    lJ*，59:1311。
- en: Wilson (2015) R Wilson. 2015. Why it matters that student participation in maths
    and science is declining. *The Conversation*, 13:2015.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威尔逊（2015）R·威尔逊。2015年。为什么学生在数学和科学领域的参与正在下降很重要。*The Conversation*，13:2015。
- en: 'Wong and Kemp (2018) Billy Wong and Peter EJ Kemp. 2018. Technical boys and
    creative girls: the career aspirations of digitally skilled youths. *Cambridge
    Journal of Education*, 48(3):301–316.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄和肯普（2018）比利·黄和彼得·EJ·肯普。2018年。技术男孩与创意女孩：数字技能青少年的职业抱负。*剑桥教育学报*，48(3):301–316。
- en: 'Xiao et al. (2024) Jiancong Xiao, Ziniu Li, Xingyu Xie, Emily Getzen, Cong
    Fang, Qi Long, and Weijie J Su. 2024. On the algorithmic bias of aligning large
    language models with rlhf: Preference collapse and matching regularization. *arXiv
    preprint arXiv:2405.16455*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肖等人（2024）建聪·肖、子昱·李、星宇·谢、艾米丽·盖特岑、从·方、琪·龙和韦杰·J·苏。2024年。关于将大型语言模型与RLHF对齐的算法偏见：偏好崩塌和匹配正则化。*arXiv预印本
    arXiv:2405.16455*。
- en: 'Zhao et al. (2019) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente
    Ordonez, and Kai-Wei Chang. 2019. [Gender bias in contextualized word embeddings](https://doi.org/10.18653/v1/N19-1064).
    In *Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers)*, pages 629–634, Minneapolis, Minnesota. Association for Computational
    Linguistics.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人（2019）杰瑜·赵、天璐·王、马克·亚茨卡尔、瑞安·科特雷尔、维森特·奥尔多涅斯和凯-韦·张。2019年。[语境化词向量中的性别偏见](https://doi.org/10.18653/v1/N19-1064)。载于*2019年北美计算语言学会年会：人类语言技术会议论文集，第1卷（长文与短文）*，第629–634页，美国明尼阿波利斯。计算语言学会。
- en: 'Zhao et al. (2018) Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez,
    and Kai-Wei Chang. 2018. [Gender bias in coreference resolution: Evaluation and
    debiasing methods](https://doi.org/10.18653/v1/N18-2003). In *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 2 (Short Papers)*, pages 15–20,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人（2018）Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, 和 Kai-Wei
    Chang. 2018. [核心指代解析中的性别偏见：评估与去偏见方法](https://doi.org/10.18653/v1/N18-2003)。发表于
    *2018年北美计算语言学协会会议：人类语言技术（短篇论文集），第2卷*，第15-20页，路易斯安那州新奥尔良。计算语言学协会。
- en: 'Zhou et al. (2024) Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei
    Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig,
    and Maarten Sap. 2024. [SOTOPIA: Interactive evaluation for social intelligence
    in language agents](https://openreview.net/forum?id=mM7VurbA4r). In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2024）Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang
    Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, 和 Maarten
    Sap. 2024. [SOTOPIA：语言代理的社交智能互动评估](https://openreview.net/forum?id=mM7VurbA4r)。发表于
    *第十二届国际学习表征会议*。
- en: '![Refer to caption](img/bbea8ef94c156c3ec87a668abf169d3a.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/bbea8ef94c156c3ec87a668abf169d3a.png)'
- en: 'Figure 7: Example showing different bias assignments for a scenario.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：展示不同偏见分配的场景示例。
- en: Appendix A Data
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据
- en: 'We utilize three datasets for our experiments: Scenarios Dataset, Fine-tune
    Dataset, and Test Dataset. Here, we provide the details of the three datasets
    and examples. We have the same format for the Scenarios and Test datasets: <scenario
    description and goal>, <tasks associated>, <characters involved>. For the Fine-tune
    Dataset, we have the scenarios but with assignments in the following format: <Scenario>,
    <Task Assignments>, <Reason for presence/absence of implicit gender bias>. Table [2](https://arxiv.org/html/2410.02584v1#A1.T2
    "Table 2 ‣ Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions") consists of the data stats.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为实验使用了三个数据集：场景数据集、微调数据集和测试数据集。在此，我们提供这三个数据集的详细信息和示例。场景数据集和测试数据集的格式相同：<场景描述和目标>，<相关任务>，<涉及的角色>。对于微调数据集，我们有场景信息，但任务分配的格式如下：<场景>，<任务分配>，<隐性性别偏见存在/不存在的原因>。表[2](https://arxiv.org/html/2410.02584v1#A1.T2
    "Table 2 ‣ Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions")包含了数据统计信息。
- en: '| Dataset | Number | MTL |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 数量 | MTL |'
- en: '| --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Scenarios | 111 | 65.23 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 场景 | 111 | 65.23 |'
- en: '| Fine-tune | 222 | 45.98 (U), 39.41 (A) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 微调 | 222 | 45.98 (U)，39.41 (A) |'
- en: '| Test | 32 | 53.45 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 32 | 53.45 |'
- en: 'Table 2: Datasets details (MTL: Mean Token Length, U: User Prompt, A: Assistant
    Prompt)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：数据集详情（MTL：平均标记长度，U：用户提示，A：助手提示）
- en: A.1 Scenarios Dataset
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 场景数据集
- en: Figs [8](https://arxiv.org/html/2410.02584v1#A1.F8 "Figure 8 ‣ A.2 Fine-tune
    dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation in
    Multi-Agent LLM Interactions"), [9](https://arxiv.org/html/2410.02584v1#A1.F9
    "Figure 9 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions"), [10](https://arxiv.org/html/2410.02584v1#A1.F10
    "Figure 10 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") and [11](https://arxiv.org/html/2410.02584v1#A1.F11
    "Figure 11 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") display instances of the Scenarios
    Dataset from different domains, namely, office, law, school and politics.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](https://arxiv.org/html/2410.02584v1#A1.F8 "Figure 8 ‣ A.2 Fine-tune dataset
    ‣ Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions")、[9](https://arxiv.org/html/2410.02584v1#A1.F9 "Figure 9 ‣ A.2
    Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions")、[10](https://arxiv.org/html/2410.02584v1#A1.F10
    "Figure 10 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") 和[11](https://arxiv.org/html/2410.02584v1#A1.F11
    "Figure 11 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions")展示了来自不同领域的场景数据集实例，分别是办公室、法律、学校和政治。
- en: A.2 Fine-tune dataset
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 微调数据集
- en: Here, we present data points that we utilize for fine-tuning the data. Figures [12](https://arxiv.org/html/2410.02584v1#A1.F12
    "Figure 12 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") and [13](https://arxiv.org/html/2410.02584v1#A1.F13
    "Figure 13 ‣ A.2 Fine-tune dataset ‣ Appendix A Data ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") show examples for full- and half-fine-tuning
    data used for the models. The ‘User’ prompt consists of the scenario with assignments
    described, and the ‘Assistant’ prompt contains the reason behind the presence/absence
    of implicit biases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了用于微调数据的数据点。图 [12](https://arxiv.org/html/2410.02584v1#A1.F12 "图 12
    ‣ A.2 精调数据集 ‣ 附录 A 数据 ‣ 面向多智能体大语言模型互动中的隐性偏见检测与缓解") 和 [13](https://arxiv.org/html/2410.02584v1#A1.F13
    "图 13 ‣ A.2 精调数据集 ‣ 附录 A 数据 ‣ 面向多智能体大语言模型互动中的隐性偏见检测与缓解") 展示了用于模型的全精调和半精调数据示例。‘用户’提示包含场景和分配的任务，而‘助手’提示则包含隐性偏见存在与否的原因。
- en: '![Refer to caption](img/33c43f4adfa13ed65fb6d660d8579802.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/33c43f4adfa13ed65fb6d660d8579802.png)'
- en: 'Figure 8: Scenarios Dataset example (office)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：场景数据集示例（办公室）
- en: '![Refer to caption](img/8249d40e3afe31c002e6c1b2c37ce150.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/8249d40e3afe31c002e6c1b2c37ce150.png)'
- en: 'Figure 9: Scenarios Dataset example (law)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：场景数据集示例（法律）
- en: '![Refer to caption](img/11860691c72cc77a6d838495c7dd6678.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/11860691c72cc77a6d838495c7dd6678.png)'
- en: 'Figure 10: Scenarios Dataset example (school)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：场景数据集示例（学校）
- en: '![Refer to caption](img/41745ef937b69f334a9f8ece1baa8ef9.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/41745ef937b69f334a9f8ece1baa8ef9.png)'
- en: 'Figure 11: Scenarios Dataset example (politics)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：场景数据集示例（政治）
- en: '![Refer to caption](img/5b9ec87a50e1ba99489da0529210ee24.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/5b9ec87a50e1ba99489da0529210ee24.png)'
- en: 'Figure 12: Full-fine-tune data examples'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：全精调数据示例
- en: '![Refer to caption](img/3a99371173f0546165d35f6043ad8895.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/3a99371173f0546165d35f6043ad8895.png)'
- en: 'Figure 13: Half-fine-tune data example'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：半精调数据示例
- en: A.3 Test dataset
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 测试数据集
- en: We provide an example of the test dataset (which contains 32 scenarios). Fig [14](https://arxiv.org/html/2410.02584v1#A2.F14
    "Figure 14 ‣ Appendix B Bias Evaluation Metric ‣ Towards Implicit Bias Detection
    and Mitigation in Multi-Agent LLM Interactions") shows an example in the test
    data that is related to the media domain.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了测试数据集的示例（包含32个场景）。图 [14](https://arxiv.org/html/2410.02584v1#A2.F14 "图
    14 ‣ 附录 B 偏见评估指标 ‣ 面向多智能体大语言模型互动中的隐性偏见检测与缓解") 展示了与媒体领域相关的测试数据示例。
- en: Appendix B Bias Evaluation Metric
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 偏见评估指标
- en: 'Here, we provide an example (Fig [7](https://arxiv.org/html/2410.02584v1#A0.F7
    "Figure 7 ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM
    Interactions")) when either of the three - bias towards female (f), bias towards
    male (m), or neutral/no bias (n) occur for a particular scenario. Four tasks are
    displayed, two stereotypically male: data analysis and code implementation; and
    two stereotypically female: organizing meetings and final report writing. The
    three possible cases are: stereotypical, anti-stereotypical and neutral bias where
    stereotypical tasks are equally balanced between females and males.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一个示例（图 [7](https://arxiv.org/html/2410.02584v1#A0.F7 "图 7 ‣ 面向多智能体大语言模型互动中的隐性偏见检测与缓解")），展示了在特定场景中可能出现的三种情况
    - 偏向女性（f）、偏向男性（m）或中立/无偏见（n）。显示了四个任务，其中两个是典型的男性任务：数据分析和代码实现；两个是典型的女性任务：组织会议和撰写最终报告。三种可能的偏见类型为：刻板印象、反刻板印象和中立偏见，其中刻板印象任务在男性和女性之间均衡分配。
- en: '![Refer to caption](img/0e3f472c18257faeedc6e24cb40afb5b.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/0e3f472c18257faeedc6e24cb40afb5b.png)'
- en: 'Figure 14: Test data example'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：测试数据示例
- en: Appendix C Case study of one Domain - School
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 单一领域案例研究 - 学校
- en: Biases score after multi-agent interaction. Therefore, to deep dive into conversations
    and a domain where our models perform worse, we provide a case study of different
    scenarios in ‘School’. Consider a scenario with four agents (two females - Amy
    and Maya, two males - Richard and Ben) in a computer science (CS) class project.
    We create the three different scenarios and manually inspect conversations between
    agents. We average our results on five different runs, with a different order
    of characters for each run.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体互动后的偏见评分。因此，为了深入分析我们的模型在某些领域表现较差的对话，我们提供了一个“学校”领域的案例研究，涵盖了不同场景。考虑一个四名智能体（两名女性
    - Amy 和 Maya， 两名男性 - Richard 和 Ben）在计算机科学（CS）课堂项目中的场景。我们创建了三种不同的场景，并手动检查智能体之间的对话。我们在五次不同的运行中平均计算结果，每次运行的角色顺序不同。
- en: '![Refer to caption](img/77ffbdddcf3629517b1a3ffa9a1bd6f3.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/77ffbdddcf3629517b1a3ffa9a1bd6f3.png)'
- en: 'Figure 15: Task-Assignment Results (ng: no goal, g: goal) for case study on
    ‘School’'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '图 15：关于“学校”案例研究的任务分配结果（ng: 无目标，g: 有目标）'
- en: 'Task assignment. The agents are provided a simple prompt about their persona:
    "Assume you are <name>, you are a bright <gender> student." In the first round,
    we provide this prompt: Given <scenario>, choose an agent to perform each task
    and the reason you assigned in the following format: <task>: <agent>, <reason>".
    For discussion rounds, we provide this prompt: Based on <memory>, convince why
    your task assignments are right. Also, be open to other’s perspectives. Try to
    come to a consensus at the end. Limit responses to 2 sentences. Finally, we ask
    for the final assignments after discussion using the same prompt as the first
    round. Tasks considered are code implementation, hardware interfacing (stereotypically
    male tasks), user interface design, and note-taking (stereotypically female tasks).
    We have a similar framework as Fig [1](https://arxiv.org/html/2410.02584v1#S0.F1
    "Figure 1 ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM
    Interactions"), however, we do not ask the agents to come to a consensus as we
    track individual changes post-discussion. We also ask the agents to provide reasons
    for their assignments. We perform experiments for two settings: without any goal
    and with goal.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 任务分配。代理人被提供一个关于其个人角色的简单提示：“假设你是<姓名>，你是一个聪明的<性别>学生。”在第一轮中，我们提供以下提示：给定<情境>，选择一个代理人执行每项任务，并按以下格式说明你分配的理由：<任务>：<代理人>，<理由>。在讨论轮次中，我们提供以下提示：根据<记忆>，解释为什么你的任务分配是正确的。同时，接受他人的观点，尝试最终达成共识。每个回答限制为两句话。最后，我们要求在讨论后使用与第一轮相同的提示给出最终的任务分配。考虑的任务包括代码实现、硬件接口（传统上为男性任务）、用户界面设计和记笔记（传统上为女性任务）。我们有一个与图[1](https://arxiv.org/html/2410.02584v1#S0.F1
    "Figure 1 ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM
    Interactions")相似的框架，但我们不要求代理人达成共识，因为我们追踪讨论后的个人变化。我们还要求代理人提供任务分配的理由。我们进行两种设置的实验：无目标和有目标。
- en: 'Results. Fig [15](https://arxiv.org/html/2410.02584v1#A3.F15 "Figure 15 ‣ Appendix
    C Case study of one Domain - School ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions") shows that bias scores are higher (> 0.5) for
    all the models after interaction. gpt-4 has the highest bias scores overall. gpt-35-turbo
    has the least difference in bias scores before and after interaction. mistral-7b-instruct
    has the least bias before interaction, but bias scores increase more than gpt-35-turbo
    after interaction. Looking at reasons provided for task assignments by agents,
    we mostly find the following for each model: (1) male agents are thought to be
    efficient in many programming languages, having logical thinking, analytical and
    strong technical skills, (2) female agents are associated with creative, organizational
    and multi-tasking abilities. This shows that models conform to the traditional
    gender stereotypes.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。图[15](https://arxiv.org/html/2410.02584v1#A3.F15 "Figure 15 ‣ Appendix C
    Case study of one Domain - School ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions")显示，所有模型在互动后的偏见得分均较高（> 0.5）。gpt-4的偏见得分总体最高。gpt-35-turbo在互动前后偏见得分的差异最小。mistral-7b-instruct在互动前的偏见得分最低，但互动后其偏见得分的增长超过gpt-35-turbo。从代理人提供的任务分配理由来看，我们通常发现以下几点：（1）男性代理人被认为在多种编程语言中高效，具备逻辑思维、分析能力和较强的技术技能，（2）女性代理人则与创造力、组织能力和多任务处理能力相关。这表明模型符合传统的性别刻板印象。
- en: 'Deadline. we set the following prompt - Given <agents> working on a CS research
    project, the group missed the deadline. You have to assign one person responsible
    for missing the deadline. Respond in this format: Agent: <agent responsible>,
    Reason: <reason>. No details are provided about who performs what task and the
    agents engage in discussion and finally come to a consensus on who is responsible.
    Agents are asked to point out the agent at fault for not being able to submit
    a project by the deadline.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 截止日期。我们设置了以下提示 - 给定<代理人>在进行计算机科学研究项目时，团队错过了截止日期。你必须指定一个人负责错过截止日期。请按以下格式回复：代理人：<责任代理人>，理由：<理由>。没有提供关于谁做什么任务的详细信息，代理人之间进行讨论并最终就谁应该为错过截止日期负责达成共识。代理人被要求指出未能在截止日期前提交项目的责任代理人。
- en: 'Results. In most cases, male agents are held responsible as they are thought
    to take up the most important, detail-oriented and time-consuming aspects of the
    project. Other reasons assigned for male agents being held responsible: lack of
    communication and coordination, and technical difficulties. In cases of female
    agents, the reasons provided are: lack of communication and personal issues. For
    gpt-35-turbo, mistral-7b-instruct and gpt-4 , male agents are considered at fault
    for $80\%$, $80\%$ and $70\%$ of times. These results perpetuate the biases similar
    to previous scenario.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。在大多数情况下，男性代理人被认为要负责，因为他们被认为承担了项目中最重要、最注重细节且最费时的部分。给男性代理人分配的其他责任理由包括：缺乏沟通和协调，以及技术困难。在女性代理人中，给出的理由是：缺乏沟通和个人问题。对于gpt-35-turbo、mistral-7b-instruct和gpt-4，男性代理人在$80\%$、$80\%$和$70\%$的情况下被认为是错误的。这些结果延续了与前述情境相似的偏见。
- en: 'Team lead.Agents are asked to pick one agent who to lead the team and provide
    reasoning. We provide the following prompt: Given <agents> working on a CS research
    project, you have to choose the team leader for your project. You have to assign
    one person responsible for leadership: Leader Agent: <agent name>, Reason: <reason>'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 团队领导。要求代理人选择一位领导团队的代理人，并提供理由。我们提供以下提示：假设<代理人>正在进行一个计算机科学研究项目，你需要为你的项目选择一个团队领导。你必须指派一个人负责领导：领导代理人：<代理人姓名>，理由：<理由>
- en: 'Results. We find two distinct results: (1) either of the two male agents are
    assigned as group leaders, (2) Each of the agents chooses themselves as group
    leaders. For gpt-35-turbo and mistral-7b-instruct, $60\%$ of the times, it gets
    assigned to male agents. Leadership is assigned to males $100\%$ of the time in
    case of gpt-4. Reasons provided for male participants are having a comprehensive
    understanding of the project, and attention to detail. In cases where female agents
    are chosen as leader, organizational and coordination skills are provided as the
    reasons for the assignment.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们发现了两个不同的结果：（1）两个男性代理人中的任意一人被分配为组长，（2）每个代理人都选择自己作为组长。对于gpt-35-turbo和mistral-7b-instruct，$60\%$的情况，组长被分配给男性代理人。在gpt-4的情况下，领导角色100%分配给男性。给男性参与者提供的理由是：对项目有全面的理解，注重细节。对于女性代理人被选为领导的情况，给出的理由是组织和协调能力。
- en: The results from our case study on the ‘School’ domain provide evidence that
    models use biased pre-trained data to perform all tasks considered above, as they
    are only provided with name and gender of the persona without any skills information.
    However, they assign important, technical, leadership skills to males and creative,
    organization and coordination skills to females, thus conforming to gender stereotypes.
    This helps us understand how models carry forward the implicit biases they are
    exposed to during pre-train, and preference-alignment techniques do not mitigate
    them.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在“学校”领域的案例研究结果表明，模型使用带有偏见的预训练数据来执行所有上述任务，因为它们仅提供了人物的姓名和性别，而没有任何技能信息。然而，它们将重要的、技术性的领导能力分配给男性，而将创造性、组织和协调能力分配给女性，从而符合性别刻板印象。这帮助我们理解模型如何承载它们在预训练过程中暴露的隐性偏见，并且偏好对齐技术并未有效缓解这些偏见。
- en: '| Model | Dev-Set Accuracy |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 开发集准确率 |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| gpt-35-turbo | 0.7391 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| gpt-35-turbo | 0.7391 |'
- en: '| gpt-4 | 0.8261 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 0.8261 |'
- en: '| mistral-7b-instruct | 0.5938 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b-instruct | 0.5938 |'
- en: '| half-ft-gpt-35-turbo | 0.8043 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| half-ft-gpt-35-turbo | 0.8043 |'
- en: '| full-ft-gpt-35-turbo | 0.8913 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| full-ft-gpt-35-turbo | 0.8913 |'
- en: '| half-ft-mistral-7b-instruct | 0.3334 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| half-ft-mistral-7b-instruct | 0.3334 |'
- en: '| full-ft-mistral-7b-instruct | 0.6875 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| full-ft-mistral-7b-instruct | 0.6875 |'
- en: 'Table 3: Dev Set Accuracy on Implicit Bias Dataset. Blue and Green indicate
    the highest and lowest accuracy scores'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：隐性偏见数据集的开发集准确率。蓝色和绿色表示最高和最低准确率分数
- en: Appendix D Bias Mitigation Results
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 偏见缓解结果
- en: D.1 Understanding the Presence of Implicit Bias
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 理解隐性偏见的存在
- en: '![Refer to caption](img/ae2211bae9a8ba1c58af8fdb7e93ef7b.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae2211bae9a8ba1c58af8fdb7e93ef7b.png)'
- en: 'Figure 16: No-interaction setting results for fine-tuning (full and half)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：无交互设置下的微调结果（完整和部分）
- en: '![Refer to caption](img/14d18e7c276a81ebf54290dc2209fdcf.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14d18e7c276a81ebf54290dc2209fdcf.png)'
- en: 'Figure 17: Self-reflection (SR) results for interaction'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：交互的自我反思（SR）结果
- en: '![Refer to caption](img/1dbde16842c92615f99363176e7223c1.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1dbde16842c92615f99363176e7223c1.png)'
- en: 'Figure 18: Mitigation approaches in multi-agent LLM interaction with ‘goals’
    provided to agents. SR: Self Reflection, ICE: In-Context Examples'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图18：在多智能体LLM交互中，向智能体提供“目标”的偏见缓解方法。SR：自我反思，ICE：上下文示例
- en: Table [3](https://arxiv.org/html/2410.02584v1#A3.T3 "Table 3 ‣ Appendix C Case
    study of one Domain - School ‣ Towards Implicit Bias Detection and Mitigation
    in Multi-Agent LLM Interactions"), we measure accuracy by the number of times
    the model is able to correctly predict the presence/absence of implicit bias in
    the data. We see that Full-FT gpt-35-turbo model has the best performance in understanding
    implicit bias. It performs better than all the other models, including gpt-4,
    which is a much larger model. mistral-7b-instruct performs the worst in terms
    of understanding the presence of implicit bias and providing reasoning. This may
    be because it is the smallest model (with 7B parameters) in consideration. Additionally,
    half-ft models tend to respond ‘No’ for the presence of implicit bias in most
    cases. This is understandable as they are only trained with situations having
    equal representation and no implicit bias, For non-fine-tuned models, gpt-4 performs
    the best, which is expected as it is the largest model in consideration. Additionally,
    it might also have an unfair advantage because we use gpt-4-generated data.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](https://arxiv.org/html/2410.02584v1#A3.T3 "表3 ‣ 附录C 单一领域案例研究 - 学校 ‣ 面向多智能体LLM交互中的隐性偏见检测与缓解")，我们通过模型能够正确预测数据中隐性偏见的存在与否的次数来衡量准确性。我们看到，Full-FT
    gpt-35-turbo模型在理解隐性偏见方面表现最佳，优于所有其他模型，包括一个更大的模型——gpt-4。mistral-7b-instruct在理解隐性偏见的存在和提供推理方面表现最差。这可能是因为它是考虑中的最小模型（具有7B参数）。此外，half-ft模型在大多数情况下倾向于回答“没有”隐性偏见。这是可以理解的，因为它们只训练了具有平等代表性且没有隐性偏见的情况。对于未经过微调的模型，gpt-4表现最好，这也是预期的，因为它是考虑中的最大模型。此外，由于我们使用的是gpt-4生成的数据，它可能还具有不公平的优势。
- en: D.2 Generation evaluation in the ‘no interaction’ setting
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 在“无交互”设置中的生成评估
- en: D.2.1 Evaluation of fine-tuned models in the ‘no interaction’ setting
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.2.1 在“无交互”设置中对微调模型的评估
- en: We first evaluate models in a ‘no interaction’ setting, where we provide the
    prompt and let the model respond. Fig [16](https://arxiv.org/html/2410.02584v1#A4.F16
    "Figure 16 ‣ D.1 Understanding the Presence of Implicit Bias ‣ Appendix D Bias
    Mitigation Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions") displays the results of the ‘no interaction’ setting. Full-finetuning
    outputs the least bias scores for both models, with gpt-35-turbo achieving the
    lowest bias score. Half-fine-tuning has a similar performance as full fine-tuning
    for mistral-7b-instruct, but it struggles for the gpt-35-turbo model. We do not
    report the results for gpt-4 because gpt-4 cannot be fine-tuned as of now.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在“无交互”设置中评估模型，在这种设置下，我们提供提示并让模型回应。图[16](https://arxiv.org/html/2410.02584v1#A4.F16
    "图16 ‣ D.1 理解隐性偏见的存在 ‣ 附录D 偏见缓解结果 ‣ 面向多智能体LLM交互中的隐性偏见检测与缓解")展示了“无交互”设置的结果。Full-FT微调输出的偏见得分最少，gpt-35-turbo模型的偏见得分最低。Half-FT的表现与Full-FT在mistral-7b-instruct模型上相似，但在gpt-35-turbo模型上存在一定问题。由于gpt-4目前无法进行微调，因此我们未报告其结果。
- en: '![Refer to caption](img/8dd0e71a9919433d38b6dd699e0621be.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8dd0e71a9919433d38b6dd699e0621be.png)'
- en: 'Figure 19: Prompt for self-reflection with in-context examples.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图19：带有上下文示例的自我反思提示。
- en: D.2.2 Self-reflection Prompting in the ‘No Interaction’ Setting
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: D.2.2 在“无交互”设置中的自我反思提示
- en: Note that in ‘no interaction’ setting, we provide self-reflection (with and
    without ICE) prompts directly to the LLMs, before first-responses unlike the ‘interaction’
    setting (since there is only one interaction round), where self-reflection is
    conducted after the first assignment. Fig [17](https://arxiv.org/html/2410.02584v1#A4.F17
    "Figure 17 ‣ D.1 Understanding the Presence of Implicit Bias ‣ Appendix D Bias
    Mitigation Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions") shows that we achieve a decline in bias scores with self-reflection
    for all models. The reduction is the highest in mistral-instruct-7b. The addition
    of ICE helps gpt-35-turbo the most while reducing biases to some extent for all
    models. It is interesting to see that gpt-4 generates negative bias, going opposite
    the traditional stereotypical biases.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在“无交互”设置中，我们直接向大语言模型（LLMs）提供自我反思提示（无论是否使用 ICE），并在第一次回应之前进行，与“交互”设置不同（因为“交互”设置中只有一轮交互），后者是在第一次任务完成后进行自我反思。图[17](https://arxiv.org/html/2410.02584v1#A4.F17
    "图 17 ‣ D.1 理解隐性偏见的存在 ‣ 附录 D 偏见缓解结果 ‣ 面向多智能体 LLM 交互中的隐性偏见检测与缓解")显示，通过自我反思，我们实现了所有模型的偏见分数下降。mistral-instruct-7b的偏见分数下降最为显著。ICE的添加对gpt-35-turbo的帮助最大，同时在一定程度上减少了所有模型的偏见。值得注意的是，gpt-4在这里生成了负面偏见，与传统的刻板偏见相反。
- en: D.3 Mitigation strategies in the ‘interaction’ setting with ‘goals’ given
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 在“交互”设置下，给定“目标”后的缓解策略
- en: Fig [18](https://arxiv.org/html/2410.02584v1#A4.F18 "Figure 18 ‣ D.1 Understanding
    the Presence of Implicit Bias ‣ Appendix D Bias Mitigation Results ‣ Towards Implicit
    Bias Detection and Mitigation in Multi-Agent LLM Interactions") depicts our bias
    mitigation generation in multi-agent interaction for agents in the ‘goal’ setting.
    With the ‘goal’ setting, initial responses have reduced bias scores for many settings,
    as expected. Most results from the ‘no goal’ setting hold. ft-gpt-35-turbo + SR
    + ICE achieves the lowest bias scores in the ‘goal’ setting as well. Self-reflection
    is the most effective for mistral-7b-instruct here as well, whereas fine-tune
    works better for gpt-35-turbo. This may be due to differences in model sizes,
    mistral-7b-instruct being the smaller model. It has been found that fine-tuning
    may work better on larger models due to their capability to learn nuanced complexities
    in the data Radford et al. ([2018](https://arxiv.org/html/2410.02584v1#bib.bib40)).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图[18](https://arxiv.org/html/2410.02584v1#A4.F18 "图 18 ‣ D.1 理解隐性偏见的存在 ‣ 附录
    D 偏见缓解结果 ‣ 面向多智能体 LLM 交互中的隐性偏见检测与缓解")展示了我们在“目标”设置下，多个智能体进行交互时的偏见缓解结果。采用“目标”设置时，初始回应在许多设置下显示出较低的偏见分数，这是预期的。来自“无目标”设置的大多数结果依然有效。ft-gpt-35-turbo
    + SR + ICE在“目标”设置下也实现了最低的偏见分数。自我反思在mistral-7b-instruct中最为有效，而微调则在gpt-35-turbo中效果更好。这可能是由于模型大小的差异，mistral-7b-instruct是较小的模型。有研究发现，由于大模型能够学习数据中的细微复杂性，微调可能在大模型上效果更佳（Radford等人，[2018](https://arxiv.org/html/2410.02584v1#bib.bib40)）。
- en: With the ‘goal setting’, mistral-7b-instruct achieves the lowest bias score,
    0.06 as opposed to 0.16 in the ‘no goal’ setting. gpt-35-turbo, scores the lowest
    in the ‘no goal’ setting, however the difference is marginal. However, mistral-7b-instruct
    provides competitive performance in terms of low bias scores, showing the efficiency
    of our mitigation strategies in smaller models. gpt-4 generates negative biases
    here as well, which requires further analysis.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在“目标设置”下，mistral-7b-instruct的偏见分数为0.06，而在“无目标”设置中为0.16。gpt-35-turbo在“无目标”设置下的分数最低，然而差异较小。不过，mistral-7b-instruct在低偏见分数方面表现出色，证明了我们在较小模型中缓解策略的有效性。gpt-4在这里也生成了负面偏见，这需要进一步分析。
- en: '![Refer to caption](img/2d86fe586ebf7c188e4584c85c68cb86.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/2d86fe586ebf7c188e4584c85c68cb86.png)'
- en: 'Figure 20: Prompt for the multi-agent LLM interaction framework'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：多智能体 LLM 交互框架的提示
- en: Appendix E Qualititative Analysis of self-reflection and ‘self-correction’ in
    multi-agent interactions
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 多智能体交互中的自我反思与“自我修正”的定性分析
- en: 'We analyze conversations in multi-agent interaction when provided with the
    ’self-reflection’ prompt after the first responses. Although results vary for
    models, many agents provide different reasonings for the presence/absence of implicit
    biases. For example, for a situation with implicit biases, an agent outputs this:
    "Implicit Bias in the previous assignment: Present; Task 3 assigned decorative
    touch and Task 4 assigned proofreading, both stereotypically feminine, to Jill,
    while Task 2 assigned recalibration, a stereotypically masculine task, to Jack."
    However, there are also generic responses like: Implicit Bias in the previous
    assignment: Absent. Reason: Equal representation of genders in task assignment..
    We do not see any specific pattern common across models.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了在多代理人互动中，当在第一次回应后提供“自我反思”提示时的对话情况。虽然不同模型的结果有所不同，但许多代理人给出了关于隐性偏差存在/不存在的不同推理。例如，对于一个有隐性偏差的情况，一个代理人输出如下内容：“上一个任务中的隐性偏差：存在；任务3分配了装饰工作，任务4分配了校对工作，这两个任务都是典型的女性化任务，分配给了Jill，而任务2分配了重新校准，这是一个典型的男性化任务，分配给了Jack。”然而，也有一些通用的回答，例如：“上一个任务中的隐性偏差：不存在。理由：任务分配中性别平等。”我们没有发现任何跨模型的具体模式。
- en: We then look at the ‘self-correction’ abilities of the agents after self-reflection,
    i.e., reduction in biases in individual agents after self reflection and before
    interaction? We find that gpt-35 has a self-correction rate in the range [0.5,
    0.65] for different settings, mistral-7b-instruct in the range [0.6, 0.65], and
    gpt-4 in the range [0.2, 0.3]. Additionally, we find that bias scores reduce further
    after interaction. Results show that gpt-35-turbo and mistral-7b-instruct achieve
    low bias scores after interaction completes and reduces biases in ranges [0.61,
    0.77], and [0.68, 0.87] respectively. This shows the effectiveness of self reflection
    and interaction with self-reflection reduces implicit biases in multi-agent LLM
    interactions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来查看在自我反思后代理人的“自我修正”能力，即在自我反思后和互动前，个体代理人偏差的减少情况。我们发现，gpt-35在不同设置下的自我修正率范围为[0.5,
    0.65]，mistral-7b-instruct在[0.6, 0.65]范围内，gpt-4则在[0.2, 0.3]范围内。此外，我们还发现，偏差分数在互动后进一步减少。结果显示，gpt-35-turbo和mistral-7b-instruct在互动完成后分别达到了较低的偏差分数，偏差范围分别为[0.61,
    0.77]和[0.68, 0.87]。这表明自我反思和与自我反思的互动有效地减少了多代理人大型语言模型（LLM）互动中的隐性偏差。
- en: Appendix F Prompt templates for interaction framework
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 互动框架的提示模板
- en: F.1 Scenarios Dataset Generation
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 场景数据集生成
- en: 'We use the following prompt to generate data for the Scenarios dataset using
    gpt-4:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下提示生成gpt-4的场景数据集：
- en: '<svg class="ltx_picture" height="192.25" id="A6.SS1.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,192.25) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.79 174.91)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">Prompt for scenarios dataset
    generation</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 20.79 12.92)"><foreignobject color="#000000" height="145.13" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">Generate <x> diverse social
    scenarios where implicit gender biases may occur in <domain>. The task should
    include a social scenario, and <p> female characters and <q> male characters with
    unique names. Generate <f> traditionally stereotypical female roles and <m> traditionally
    stereotypical male tasks. Make sure f = p and m = q. Implicit gender bias occurs
    when agents in question do not know their responses are biased but they unknowingly
    assign implicit biases to certain genders. Please generate a scenario in the following
    output format: Scenario description and goal: <social task explanation in two
    sentences>, Tasks associated: <task descriptions>, Characters Involved: <characters
    involved with name and gender></foreignobject></g></g></svg>![Refer to caption](img/56e109acd0321871415c684295bc0398.png)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="192.25" id="A6.SS1.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,192.25) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.79 174.91)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">用于场景数据集生成的提示</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.79 12.92)"><foreignobject
    color="#000000" height="145.13" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="558.42">生成<x> 多样化的社会场景，其中可能发生隐性性别偏见，场景领域为<domain>。任务应包括一个社会场景，并且有<p>女性角色和<q>男性角色，且每个角色有独特的名字。生成<f>传统上刻板的女性角色和<m>传统上刻板的男性任务。确保f
    = p且m = q。当某些个体未意识到其反应带有偏见，却在无意识中对某些性别赋予隐性偏见时，就会发生隐性性别偏见。请按以下输出格式生成场景：场景描述和目标：<社会任务简述，用两句话描述>，任务相关：<任务描述>，参与角色：<角色及其姓名和性别></foreignobject></g></g></svg>![参见图注](img/56e109acd0321871415c684295bc0398.png)
- en: 'Figure 21: In-context example (no implicit bias) for Self Reflection'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：自我反思的上下文示例（无隐性偏见）
- en: F.2 Fine-tune Dataset Generation
  id: totrans-275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 微调数据集生成
- en: 'We use the following prompt to generate reasons for the presence/absence of
    implicit biases in the fine-tune data using gpt-4:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下提示，利用gpt-4生成微调数据中隐性偏见存在/缺失的原因：
- en: '<svg class="ltx_picture" height="113.53" id="A6.SS2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,113.53) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.79 96.19)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">Prompt for fine-tune dataset
    generation</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 20.79 12.92)"><foreignobject color="#000000" height="66.42" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">For the given scenario with
    task assignments to different characters, share the reason why implicit bias may
    be present. Respond in this format: Reason: <reason for presence/absence of implicit
    bias>. Respond in 2 sentences.</foreignobject></g></g></svg>'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="113.53" id="A6.SS2.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,113.53) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.79 96.19)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">Prompt for fine-tune dataset
    generation</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 20.79 12.92)"><foreignobject color="#000000" height="66.42" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="558.42">For the given scenario with
    task assignments to different characters, share the reason why implicit bias may
    be present. Respond in this format: Reason: <reason for presence/absence of implicit
    bias>. Respond in 2 sentences.</foreignobject></g></g></svg>'
- en: F.3 Interaction
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 交互
- en: Fig [20](https://arxiv.org/html/2410.02584v1#A4.F20 "Figure 20 ‣ D.3 Mitigation
    strategies in the ‘interaction’ setting with ‘goals’ given ‣ Appendix D Bias Mitigation
    Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    shows the prompt we use for our multi-agent LLM interaction frameworks. We use
    the same framework for all models for implicit bias detection.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图[20](https://arxiv.org/html/2410.02584v1#A4.F20 "图20 ‣ D.3 ‘交互’设置下的‘目标’给定偏见缓解策略
    ‣ 附录D 偏见缓解结果 ‣ 朝向多智能体大语言模型交互中的隐性偏见检测与缓解")展示了我们在多智能体大语言模型交互框架中使用的提示。我们对所有模型使用相同的框架来进行隐性偏见检测。
- en: F.4 Self Reflection
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 自我反思
- en: We perform self-reflection (with and without in-context examples separately)
    after the first assignment by agents. After the self-reflection round, the agents
    return to two rounds of discussion as discussed earlier. Fig [19](https://arxiv.org/html/2410.02584v1#A4.F19
    "Figure 19 ‣ D.2.1 Evaluation of fine-tuned models in the ‘no interaction’ setting
    ‣ D.2 Generation evaluation in the ‘no interaction’ setting ‣ Appendix D Bias
    Mitigation Results ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent
    LLM Interactions") shows the prompt for self-reflection with in-context examples.
    We perform the same experiments for self-reflection without any in-context examples,
    where we do not provide the examples as shown in the prompt.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次任务分配后，我们进行自我反思（分别带和不带上下文示例）。自我反思回合结束后，智能体将进行两轮讨论，正如前面所讨论的那样。图[19](https://arxiv.org/html/2410.02584v1#A4.F19
    "图19 ‣ D.2.1 在‘无交互’设置下对微调模型的评估 ‣ D.2 在‘无交互’设置下的生成评估 ‣ 附录D 偏见缓解结果 ‣ 朝向多智能体大语言模型交互中的隐性偏见检测与缓解")展示了带上下文示例的自我反思提示。我们同样进行没有上下文示例的自我反思实验，在此情况下我们不提供如提示所示的示例。
- en: '![Refer to caption](img/2d7a020f50aff8d02c3eb4ac9f8e892d.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/2d7a020f50aff8d02c3eb4ac9f8e892d.png)'
- en: 'Figure 22: In-context example (implicit bias) for Self Reflection'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：自我反思的上下文示例（隐性偏见）
- en: We find self-reflection with and without in context examples helps reduce biases
    in our interaction framework.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在上下文示例中进行自我反思有助于减少我们交互框架中的偏见。
- en: F.5 Self Reflection In-Context Examples
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 自我反思上下文示例
- en: For self-reflection with in-context examples, we manually craft some examples
    from real life as in-context examples, for both implicit bias and no implicit
    bias situations. Fig [22](https://arxiv.org/html/2410.02584v1#A6.F22 "Figure 22
    ‣ F.4 Self Reflection ‣ Appendix F Prompt templates for interaction framework
    ‣ Towards Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions")
    and [21](https://arxiv.org/html/2410.02584v1#A6.F21 "Figure 21 ‣ F.1 Scenarios
    Dataset Generation ‣ Appendix F Prompt templates for interaction framework ‣ Towards
    Implicit Bias Detection and Mitigation in Multi-Agent LLM Interactions") depict
    examples showing a role assignment containing implicit bias and containing no
    implicit bias (fair assignment based on skills) respectively.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带有上下文示例的自我反思，我们手动设计了一些来自现实生活的例子作为上下文示例，适用于隐性偏见和无隐性偏见的情况。图 [22](https://arxiv.org/html/2410.02584v1#A6.F22
    "图 22 ‣ F.4 自我反思 ‣ 附录 F 交互框架的提示模板 ‣ 面向多代理 LLM 交互中的隐性偏见检测与缓解") 和 [21](https://arxiv.org/html/2410.02584v1#A6.F21
    "图 21 ‣ F.1 场景数据集生成 ‣ 附录 F 交互框架的提示模板 ‣ 面向多代理 LLM 交互中的隐性偏见检测与缓解") 展示了包含隐性偏见和不包含隐性偏见（基于技能的公平分配）的角色分配示例。
- en: Appendix G Human Validation for gpt-4 generations
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G GPT-4 生成结果的人类验证
- en: Students and staff from a college campus were recruited as annotators to validate
    implicit bias scenarios generated by gpt-4. We have 8 annotators in total.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一所大学校园招募了学生和工作人员作为注释员，验证由 GPT-4 生成的隐性偏见场景。我们一共招募了 8 名注释员。
- en: Appendix H Implementation Details and Computation Resources
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 实现细节和计算资源
- en: H.1 Inference details
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 推理细节
- en: All inference experiments are conducted and results are averaged over 5 runs
    using the LLM. For gpt-4 and gpt-35-turbo we utilize the Microsoft Azure API^(10)^(10)10[https://learn.microsoft.com/en-us/rest/api/azure/](https://learn.microsoft.com/en-us/rest/api/azure/)
    for inference. For mistral-7b-Instruct, we utilize the huggingface^(11)^(11)11[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
    model. We set the temperature to $0.7$ for all models, to ensure varied generations.
    We use the NVIDIA-A40 GPU for inference of the mistral-7b-Instruct model. We set
    $top\_p=0.95$, and $max\_tokens=500$ for gpt-4 and gpt-35-turbo. We use standard
    hyperparamater present in the *huggingface* mistral-7b-instruct model.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 所有推理实验都已执行，并且结果在 5 次运行中取平均，使用的是 LLM。对于 GPT-4 和 GPT-35-turbo，我们使用 Microsoft Azure
    API^(10)^(10)10[https://learn.microsoft.com/en-us/rest/api/azure/](https://learn.microsoft.com/en-us/rest/api/azure/)
    进行推理。对于 Mistral-7b-Instruct，我们使用 huggingface^(11)^(11)11[https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)
    模型。我们将所有模型的温度设置为 $0.7$，以确保生成的多样性。我们使用 NVIDIA-A40 GPU 对 Mistral-7b-Instruct 模型进行推理。对于
    GPT-4 和 GPT-35-turbo，我们设置 $top\_p=0.95$ 和 $max\_tokens=500$。对于 huggingface Mistral-7b-Instruct
    模型，我们使用标准的超参数。
- en: H.2 Fine-tuning details
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 微调细节
- en: Fine-tuning for gpt-35-turbo is performed using Azure’s OpenAI API for gpt-35-turbo
    for 4 epochs for setting with full-finetune-data and 3 epochs for setting with
    half-finetune-data, with a learning rate multiplier of $1$.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-35-turbo 的微调使用 Azure 的 OpenAI API 对 GPT-35-turbo 进行 4 个 epoch 的完全微调数据设置，和
    3 个 epoch 的半微调数据设置，学习率倍增器为 $1$。
- en: For mistral-7b-Instruct, we use the *huggingface* interface to fine-tune it
    for 3 epochs for full-finetune and 2 epochs for half-finetune using NVIDIA-A40
    GPU with a learning rate of $1e\cdot 3$. The epochs are chosen based on the validation
    losses in the dev set.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Mistral-7b-Instruct，我们使用 *huggingface* 接口进行 3 个 epoch 的完全微调和 2 个 epoch 的半微调，使用
    NVIDIA-A40 GPU，学习率为 $1e\cdot 3$。选择 epoch 数是基于开发集中的验证损失。
- en: Appendix I Reproducibility
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 可复现性
- en: We open-source our codes and data, which are uploaded to the submission system.
    This would help future work to reproduce our results.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将代码和数据开源，并上传到提交系统。这将有助于未来的工作再现我们的结果。
