- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:55:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:55:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过协作LLM-代理实现自动驾驶的可编辑场景仿真
- en: 来源：[https://arxiv.org/html/2402.05746/](https://arxiv.org/html/2402.05746/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.05746/](https://arxiv.org/html/2402.05746/)
- en: Yuxi Wei¹¹¹1Equal contribution.  Zi Wang³¹¹1Equal contribution.  Yifan Lu¹¹¹1Equal
    contribution.  Chenxin Xu¹¹¹1Equal contribution.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yuxi Wei¹¹¹1平等贡献。  Zi Wang³¹¹1平等贡献。  Yifan Lu¹¹¹1平等贡献。  Chenxin Xu¹¹¹1平等贡献。
- en: Changxing Liu¹  Hao Zhao⁴  Siheng Chen^(1,2)  Yanfeng Wang^(1,2)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Changxing Liu¹  Hao Zhao⁴  Siheng Chen^(1,2)  Yanfeng Wang^(1,2)
- en: ¹ Shanghai Jiao Tong University  ² Shanghai AI Laboratory
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 上海交通大学  ² 上海人工智能实验室
- en: ³ Carnegie Mellon University  ⁴ Tsinghua University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 卡内基梅隆大学  ⁴ 清华大学
- en: '{wyx3590236732, yifan_lu, xcxwakaka, cx-liu}@sjtu.edu.cn'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{wyx3590236732, yifan_lu, xcxwakaka, cx-liu}@sjtu.edu.cn'
- en: '{sihengc, wangyanfeng}@sjtu.edu.cn'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{sihengc, wangyanfeng}@sjtu.edu.cn'
- en: ziwang2@andrew.cmu.edu zhaohao@air.tsinghua.edu.cn
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ziwang2@andrew.cmu.edu zhaohao@air.tsinghua.edu.cn
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Scene simulation in autonomous driving has gained significant attention because
    of its huge potential for generating customized data. However, existing editable
    scene simulation approaches face limitations in terms of user interaction efficiency,
    multi-camera photo-realistic rendering and external digital assets integration.
    To address these challenges, this paper introduces ChatSim, the first system that
    enables editable photo-realistic 3D driving scene simulations via natural language
    commands with external digital assets. To enable editing with high command flexibility, ChatSim
    leverages a large language model (LLM) agent collaboration framework. To generate
    photo-realistic outcomes, ChatSim employs a novel multi-camera neural radiance
    field method. Furthermore, to unleash the potential of extensive high-quality
    digital assets, ChatSim employs a novel multi-camera lighting estimation method
    to achieve scene-consistent assets’ rendering. Our experiments on Waymo Open Dataset
    demonstrate that ChatSim can handle complex language commands and generate corresponding
    photo-realistic scene videos. Code can be accessed at: [https://github.com/yifanlu0227/ChatSim](https://github.com/yifanlu0227/ChatSim).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的场景仿真因其在生成定制化数据方面的巨大潜力而受到广泛关注。然而，现有的可编辑场景仿真方法在用户交互效率、多摄像头照片级渲染以及外部数字资产集成方面存在局限性。为了解决这些挑战，本文介绍了ChatSim，这是第一个通过自然语言命令与外部数字资产实现可编辑照片级3D驾驶场景仿真的系统。为了实现高命令灵活性的编辑，ChatSim利用了大型语言模型（LLM）代理协作框架。为了生成照片级结果，ChatSim采用了一种新颖的多摄像头神经辐射场方法。此外，为了释放大量高质量数字资产的潜力，ChatSim还采用了一种新颖的多摄像头光照估计方法，以实现场景一致性的资产渲染。我们在Waymo开放数据集上的实验表明，ChatSim能够处理复杂的语言命令并生成相应的照片级场景视频。代码可以通过以下链接访问：[https://github.com/yifanlu0227/ChatSim](https://github.com/yifanlu0227/ChatSim)。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Perception [[15](https://arxiv.org/html/2402.05746v3#bib.bib15), [17](https://arxiv.org/html/2402.05746v3#bib.bib17),
    [70](https://arxiv.org/html/2402.05746v3#bib.bib70), [16](https://arxiv.org/html/2402.05746v3#bib.bib16),
    [12](https://arxiv.org/html/2402.05746v3#bib.bib12), [40](https://arxiv.org/html/2402.05746v3#bib.bib40)]
    is the window of an autonomous vehicle into the external environment. To ensure
    the robustness of the vehicle’s perceptual capabilities during both training and
    testing phases, it necessitates the collection of high-quality perception data
    in substantial volumes [[13](https://arxiv.org/html/2402.05746v3#bib.bib13), [61](https://arxiv.org/html/2402.05746v3#bib.bib61)].
    However, the operation of a fleet for the acquisition of real-world data often
    incurs prohibitive expenses, particularly for specialized or customized requirements.
    For instance, in the aftermath of an accident or intervention involving an autonomous
    vehicle, it is imperative to test the vehicle’s perception system across a spectrum
    of similar scenarios. While replicating such scenario data from real-world instances
    is nearly impossible due to the uncontrollability of actual scenes [[56](https://arxiv.org/html/2402.05746v3#bib.bib56),
    [9](https://arxiv.org/html/2402.05746v3#bib.bib9)], customized scene simulation
    emerges as a vital and feasible alternative. It enables the precise modeling of
    specific conditions without high costs and logistical complexities of real-world
    data collection [[4](https://arxiv.org/html/2402.05746v3#bib.bib4), [76](https://arxiv.org/html/2402.05746v3#bib.bib76)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 感知[[15](https://arxiv.org/html/2402.05746v3#bib.bib15), [17](https://arxiv.org/html/2402.05746v3#bib.bib17),
    [70](https://arxiv.org/html/2402.05746v3#bib.bib70), [16](https://arxiv.org/html/2402.05746v3#bib.bib16),
    [12](https://arxiv.org/html/2402.05746v3#bib.bib12), [40](https://arxiv.org/html/2402.05746v3#bib.bib40)]
    是自动驾驶车辆与外部环境之间的窗口。为了确保车辆在训练和测试阶段的感知能力的鲁棒性，必须收集大量高质量的感知数据[[13](https://arxiv.org/html/2402.05746v3#bib.bib13),
    [61](https://arxiv.org/html/2402.05746v3#bib.bib61)]。然而，运营一个车队以获取现实世界数据通常需要巨额开支，特别是针对专业化或定制化的需求。例如，在自动驾驶车辆发生事故或干预后，必须在多个类似场景下测试车辆的感知系统。由于实际场景无法控制，几乎不可能从现实世界实例中复制这些场景数据[[56](https://arxiv.org/html/2402.05746v3#bib.bib56),
    [9](https://arxiv.org/html/2402.05746v3#bib.bib9)]，因此定制化场景模拟成为一个至关重要且可行的替代方案。它可以精确建模特定条件，而不需要高昂的成本和现实数据收集的后勤复杂性[[4](https://arxiv.org/html/2402.05746v3#bib.bib4),
    [76](https://arxiv.org/html/2402.05746v3#bib.bib76)]。
- en: '![Refer to caption](img/1929ffacec6feb1bb8581d8de620b8c0.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1929ffacec6feb1bb8581d8de620b8c0.png)'
- en: 'Figure 1: ChatSim enables the editing of photo-realistic 3D driving scene simulations
    via language commands.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：ChatSim 通过语言指令使得照片级真实感的 3D 驾驶场景模拟编辑成为可能。
- en: To effectively simulate customized driving scenes, we identify three key properties
    as fundamental. First, the simulation should be capable of following sophisticated
    or abstract demands, thereby facilitating the production. Second, the simulation
    should generate photo-realistic, view-consistent outcomes, which allow for the
    closest approximation to vehicle observations in real-world scenarios. Third,
    it should allow for the integration of external digital assets [[48](https://arxiv.org/html/2402.05746v3#bib.bib48),
    [6](https://arxiv.org/html/2402.05746v3#bib.bib6)] with their photo-realistic
    textures and materials while fitting the lighting conditions. This capability
    would unlock the potential for data expansion by incorporating a wide array of
    external digital assets, satisfying customized needs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地模拟定制化的驾驶场景，我们识别了三种关键属性作为基础。首先，模拟应该能够满足复杂或抽象的需求，从而促进生产。其次，模拟应生成照片级真实感且视角一致的结果，使得其与现实世界场景中的车辆观察最为接近。第三，它应允许整合外部数字资产[[48](https://arxiv.org/html/2402.05746v3#bib.bib48),
    [6](https://arxiv.org/html/2402.05746v3#bib.bib6)]，这些资产具有照片级的纹理和材料，并能与现有的光照条件相匹配。此功能将通过整合大量外部数字资产，满足定制化需求，从而解锁数据扩展的潜力。
- en: A vast array of significant works have been proposed for scene simulation, yet
    they fail to meet all three of these requirements. Traditional graphics engines,
    such as CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)] and UE [[25](https://arxiv.org/html/2402.05746v3#bib.bib25)],
    offer editable virtual environments with external digital assets, but the data
    realism is restricted by asset modeling and rendering qualities. Image generation
    based methods, such as BEVControl [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)],
    DriveDreamer [[67](https://arxiv.org/html/2402.05746v3#bib.bib67)], MagicDrive
    [[26](https://arxiv.org/html/2402.05746v3#bib.bib26)], can generate realistic
    scene images based on various control signals, including BEV maps, bounding boxes
    and camera poses. However, they struggle to maintain view consistency and face
    challenges in importing external digital assets due to the absence of 3D spatial
    modeling. Rendering-based methods have been proposed to obtain photo-realistic
    and view-consistent scene simulation. Notable examples like UniSim [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)]
    and MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)] come equipped with
    a suite of scene-editing tools. However, these systems require extensive user
    involvement in every trivial editing step via code implementation, which is ineffective
    when performing the editing. Furthermore, while they handle vehicles in observed
    scenarios effectively, their inability to support external digital assets restricts
    opportunities for data expansion and customization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经提出了大量重要的作品用于场景模拟，但它们未能满足这三项要求。传统的图形引擎，如CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)]
    和UE [[25](https://arxiv.org/html/2402.05746v3#bib.bib25)]，提供了可编辑的虚拟环境和外部数字资产，但数据的真实性受到资产建模和渲染质量的限制。基于图像生成的方法，如BEVControl
    [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)]、DriveDreamer [[67](https://arxiv.org/html/2402.05746v3#bib.bib67)]、MagicDrive
    [[26](https://arxiv.org/html/2402.05746v3#bib.bib26)]，可以基于各种控制信号（包括BEV地图、边界框和相机姿势）生成逼真的场景图像。然而，它们在保持视图一致性方面存在困难，并且由于缺乏3D空间建模，无法导入外部数字资产。基于渲染的方法被提出用于获取照片真实感和视图一致性的场景模拟。值得注意的例子，如UniSim
    [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)] 和MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)]，配备了一套场景编辑工具。然而，这些系统需要用户在每个琐碎的编辑步骤中大量参与代码实现，这在执行编辑时是低效的。此外，尽管它们在观察场景中的车辆表现良好，但由于无法支持外部数字资产，它们限制了数据扩展和定制的机会。
- en: To fulfill the identified requirements, we introduce ChatSim, the first system
    that enables editable photo-realistic 3D driving scene simulations via natural
    language commands with external digital assets. To use ChatSim, users simply engage
    in a conversation with the system, issuing commands through natural language without
    any involvement in intermediate simulation steps; see Figure [1](https://arxiv.org/html/2402.05746v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Editable Scene Simulation for Autonomous Driving
    via Collaborative LLM-Agents") for illustration.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些要求，我们介绍了ChatSim，这是第一个通过自然语言命令与外部数字资产实现可编辑照片真实感3D驾驶场景模拟的系统。使用ChatSim时，用户只需与系统进行对话，通过自然语言发出命令，无需参与中间的模拟步骤；有关说明，请参见图[1](https://arxiv.org/html/2402.05746v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Editable Scene Simulation for Autonomous Driving
    via Collaborative LLM-Agents")。
- en: To address complex or abstract user commands effectively, ChatSim adopts a large
    language model (LLM)-based multi-agent collaboration framework. The key idea is
    to exploit multiple LLM agents, each with a specialized role, to decouple an overall
    simulation demand into specific editing tasks, thereby mirroring the task division
    and execution typically founded in the workflow of a human-operated company. This
    workflow offers two key advantages for scene simulation. First, LLM agents’ ability
    to process human language commands allows for intuitive and dynamic editing of
    complex driving scenes, enabling precise adjustments and feedback. Second, the
    collaboration framework enhances simulation efficiency and accuracy by distributing
    specific editing tasks among specialized agents, ensuring detailed and realistic
    simulations with improved task completion rates.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效应对复杂或抽象的用户指令，ChatSim采用了基于大型语言模型（LLM）的多智能体协作框架。其关键思想是利用多个LLM智能体，每个智能体承担一个特定角色，将整体仿真需求拆解为具体的编辑任务，从而模拟人类运营公司工作流中通常的任务分配与执行。这个工作流为场景仿真提供了两个关键优势。首先，LLM智能体处理人类语言指令的能力，使得复杂驾驶场景的编辑变得直观且动态，从而实现精确的调整和反馈。其次，协作框架通过将特定的编辑任务分配给专业的智能体，提升了仿真效率和精度，确保了详细且逼真的仿真效果，并提高了任务完成率。
- en: 'To generate photo-realistic outcomes, we propose McNeRF in  ChatSim, a novel
    neural radiance field method that incorporates multi-camera inputs, offering a
    broader scene rendering. This integration fully exploits camera setups on vehicles
    but raises two significant challenges: camera pose misalignment due to asynchronized
    trigger times and brightness inconsistency due to different camera exposure times.
    To address camera pose misalignment, McNeRF uses a multi-camera alignment to reduce
    extrinsic parameter noises, ensuring rendering quality. To address brightness
    inconsistency, McNeRF integrates the critical exposure times to recover scene
    radiance in HDR, markedly mitigating the issue of color discrepancies at the intersections
    of two camera images with different exposure times.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成照片级真实感的结果，我们在ChatSim中提出了McNeRF，这是一种新型的神经辐射场方法，结合了多摄像头输入，提供了更广阔的场景渲染。该集成充分利用了车辆上的摄像头配置，但也带来了两个重要的挑战：由于触发时间不同步而导致的相机姿态不对齐问题，以及由于不同摄像头曝光时间不同而导致的亮度不一致问题。为了应对相机姿态不对齐，McNeRF使用了多摄像头对齐来减少外部参数噪声，从而确保渲染质量。为了应对亮度不一致，McNeRF集成了关键的曝光时间，以恢复场景的HDR辐射，从而显著缓解了两幅具有不同曝光时间的相机图像交界处的颜色差异问题。
- en: To import external digital assets with their realistic textures and materials,
    we propose McLight, a novel multi-camera lighting estimation that blends skydome
    and surrounding lighting. Our skydome estimation restores accurate sun behavior
    with peak intensity residual connection, enabling the rendering of prominent shadows.
    For surrounding lighting, McLight queries McNeRF to achieve complex location-specific
    illumination effects, like those in the tree shade with sunlight being blocked.
    This significantly improves the rendering realism of the integrated 3D assets.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了导入具有真实纹理和材质的外部数字资产，我们提出了McLight，一种新型的多摄像头光照估计方法，它融合了天幕和周围的光照。我们的天幕估计通过峰值强度残差连接恢复了准确的太阳行为，从而能够渲染显著的阴影效果。对于周围光照，McLight查询McNeRF，以实现复杂的特定位置光照效果，例如在树荫下阳光被遮挡的情况。这大大提高了集成3D资产的渲染真实感。
- en: We conduct extensive experiments on the Waymo autonomous driving dataset and
    show that  ChatSim generates photo-realistic customized perception data including
    dangerous corner cases according to various human language commands. Our method
    is compatible with mixed, highly-abstract and multi-round commands. Our method
    achieves SoTA performance with an improvement of 4.5% in photo-realism with a
    wide-angle rendering. Moreover, we demonstrate our lighting estimation outperforms
    the SoTA methods both qualitatively and quantitatively, reducing the intensity
    error and angular error by 57.0% and 9.9%.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Waymo自动驾驶数据集上进行了广泛的实验，结果表明ChatSim能够根据不同的人类语言指令生成照片级真实感的定制感知数据，包括危险的极限情况。我们的方法兼容混合、抽象性较强的多轮指令。我们的方法在广角渲染下实现了SoTA（最先进）表现，照片真实感提高了4.5%。此外，我们还展示了我们的光照估计在定性和定量上均优于现有的最先进方法，将强度误差和角度误差分别降低了57.0%和9.9%。
- en: 2 Related Work
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Scene simulation for autonomous driving. Current scene simulation methods can
    be generally divided into three categories: graphics engines, image generation,
    and scene rendering. Graphics engines, such as CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)],
    AirSim [[58](https://arxiv.org/html/2402.05746v3#bib.bib58)], OpenScenario Editor
    [[24](https://arxiv.org/html/2402.05746v3#bib.bib24)], 51Sim-One [[1](https://arxiv.org/html/2402.05746v3#bib.bib1)]
    and RoadRunner [[21](https://arxiv.org/html/2402.05746v3#bib.bib21)], create a
    virtual world for simulating a wide range of driving scenarios. However, there
    exists a significant domain gap between the virtual world and reality. Image generation
    methods can generate realistic scene images based on different control signals,
    such as HD maps [[63](https://arxiv.org/html/2402.05746v3#bib.bib63), [26](https://arxiv.org/html/2402.05746v3#bib.bib26),
    [41](https://arxiv.org/html/2402.05746v3#bib.bib41)], sketch layout [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)],
    bounding boxes [[41](https://arxiv.org/html/2402.05746v3#bib.bib41), [67](https://arxiv.org/html/2402.05746v3#bib.bib67),
    [26](https://arxiv.org/html/2402.05746v3#bib.bib26)], text [[41](https://arxiv.org/html/2402.05746v3#bib.bib41),
    [67](https://arxiv.org/html/2402.05746v3#bib.bib67), [26](https://arxiv.org/html/2402.05746v3#bib.bib26),
    [34](https://arxiv.org/html/2402.05746v3#bib.bib34)] and driving actions [[67](https://arxiv.org/html/2402.05746v3#bib.bib67),
    [34](https://arxiv.org/html/2402.05746v3#bib.bib34)]. However, these approaches
    can hardly maintain scene consistency. To obtain a coherent driving scene, methods
    based on scene rendering target to reconstruct the 3D scene. READ [[42](https://arxiv.org/html/2402.05746v3#bib.bib42)]
    employs point clouds and uses a U-Net to render images. With the rapid development
    of Neural Radiance Field (NeRF) [[47](https://arxiv.org/html/2402.05746v3#bib.bib47),
    [7](https://arxiv.org/html/2402.05746v3#bib.bib7), [8](https://arxiv.org/html/2402.05746v3#bib.bib8),
    [49](https://arxiv.org/html/2402.05746v3#bib.bib49), [60](https://arxiv.org/html/2402.05746v3#bib.bib60),
    [66](https://arxiv.org/html/2402.05746v3#bib.bib66)], several works  [[73](https://arxiv.org/html/2402.05746v3#bib.bib73),
    [28](https://arxiv.org/html/2402.05746v3#bib.bib28), [77](https://arxiv.org/html/2402.05746v3#bib.bib77),
    [72](https://arxiv.org/html/2402.05746v3#bib.bib72), [52](https://arxiv.org/html/2402.05746v3#bib.bib52),
    [65](https://arxiv.org/html/2402.05746v3#bib.bib65), [36](https://arxiv.org/html/2402.05746v3#bib.bib36),
    [51](https://arxiv.org/html/2402.05746v3#bib.bib51)] also exploit NeRFs to model
    cars and static street backgrounds in outdoor environments. Moreover, notable
    examples like UniSim [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)] and
    MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)] come equipped with
    a suite of scene-editing tools. However, these methods require extensive user
    involvement in intermediate editing steps and they fail to support external digital
    assets for data expansion. In this work, we propose ChatSim that achieves automatic
    simulation editing via language commands and integrates external digital assets
    to enhance realism and flexibility. In ChatSim, we integrate McNeRF, a novel neural
    radiance field designed to leverage multi-camera inputs for high-fidelity rendering.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶的场景模拟。目前的场景模拟方法大致可以分为三类：图形引擎、图像生成和场景渲染。图形引擎，如CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)]、AirSim
    [[58](https://arxiv.org/html/2402.05746v3#bib.bib58)]、OpenScenario Editor [[24](https://arxiv.org/html/2402.05746v3#bib.bib24)]、51Sim-One
    [[1](https://arxiv.org/html/2402.05746v3#bib.bib1)] 和RoadRunner [[21](https://arxiv.org/html/2402.05746v3#bib.bib21)]，创建了一个虚拟世界，用于模拟各种驾驶场景。然而，虚拟世界与现实之间存在着显著的领域差距。图像生成方法可以根据不同的控制信号生成逼真的场景图像，例如高清地图
    [[63](https://arxiv.org/html/2402.05746v3#bib.bib63)、[26](https://arxiv.org/html/2402.05746v3#bib.bib26)、[41](https://arxiv.org/html/2402.05746v3#bib.bib41)]、草图布局
    [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)]、边界框 [[41](https://arxiv.org/html/2402.05746v3#bib.bib41)、[67](https://arxiv.org/html/2402.05746v3#bib.bib67)、[26](https://arxiv.org/html/2402.05746v3#bib.bib26)]、文本
    [[41](https://arxiv.org/html/2402.05746v3#bib.bib41)、[67](https://arxiv.org/html/2402.05746v3#bib.bib67)、[26](https://arxiv.org/html/2402.05746v3#bib.bib26)、[34](https://arxiv.org/html/2402.05746v3#bib.bib34)]
    和驾驶行为 [[67](https://arxiv.org/html/2402.05746v3#bib.bib67)、[34](https://arxiv.org/html/2402.05746v3#bib.bib34)]。然而，这些方法很难保持场景一致性。为了获得一致的驾驶场景，基于场景渲染的方法旨在重建三维场景。READ
    [[42](https://arxiv.org/html/2402.05746v3#bib.bib42)] 使用点云并利用U-Net进行图像渲染。随着神经辐射场（NeRF）[[47](https://arxiv.org/html/2402.05746v3#bib.bib47)、[7](https://arxiv.org/html/2402.05746v3#bib.bib7)、[8](https://arxiv.org/html/2402.05746v3#bib.bib8)、[49](https://arxiv.org/html/2402.05746v3#bib.bib49)、[60](https://arxiv.org/html/2402.05746v3#bib.bib60)、[66](https://arxiv.org/html/2402.05746v3#bib.bib66)]
    的快速发展，一些研究 [[73](https://arxiv.org/html/2402.05746v3#bib.bib73)、[28](https://arxiv.org/html/2402.05746v3#bib.bib28)、[77](https://arxiv.org/html/2402.05746v3#bib.bib77)、[72](https://arxiv.org/html/2402.05746v3#bib.bib72)、[52](https://arxiv.org/html/2402.05746v3#bib.bib52)、[65](https://arxiv.org/html/2402.05746v3#bib.bib65)、[36](https://arxiv.org/html/2402.05746v3#bib.bib36)、[51](https://arxiv.org/html/2402.05746v3#bib.bib51)]
    也利用NeRF对户外环境中的汽车和静态街道背景进行建模。此外，一些显著的例子，如UniSim [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)]
    和MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)]，配备了一套场景编辑工具。然而，这些方法需要用户在中间编辑步骤中进行大量参与，并且无法支持外部数字资产进行数据扩展。在本研究中，我们提出了ChatSim，通过语言命令实现自动化的模拟编辑，并整合外部数字资产，以增强逼真度和灵活性。在ChatSim中，我们集成了McNeRF，这是一种新型的神经辐射场，旨在利用多摄像头输入进行高保真渲染。
- en: Lighting estimation. Lighting estimation focuses on assessing the illumination
    conditions of a real-world environment to seamlessly integrate digital objects.
    Early methods [[37](https://arxiv.org/html/2402.05746v3#bib.bib37), [38](https://arxiv.org/html/2402.05746v3#bib.bib38)]
    for outdoor environments use explicit cues like detected shadows on the ground.
    Recent works usually adopt learning-based approaches [[27](https://arxiv.org/html/2402.05746v3#bib.bib27),
    [31](https://arxiv.org/html/2402.05746v3#bib.bib31), [39](https://arxiv.org/html/2402.05746v3#bib.bib39),
    [32](https://arxiv.org/html/2402.05746v3#bib.bib32), [43](https://arxiv.org/html/2402.05746v3#bib.bib43),
    [78](https://arxiv.org/html/2402.05746v3#bib.bib78)] by predicting different lighting
    representations like spherical lobes [[10](https://arxiv.org/html/2402.05746v3#bib.bib10),
    [43](https://arxiv.org/html/2402.05746v3#bib.bib43)], light probes [[39](https://arxiv.org/html/2402.05746v3#bib.bib39)],
    environment map [[57](https://arxiv.org/html/2402.05746v3#bib.bib57), [59](https://arxiv.org/html/2402.05746v3#bib.bib59)],
    HDR sky model [[31](https://arxiv.org/html/2402.05746v3#bib.bib31), [78](https://arxiv.org/html/2402.05746v3#bib.bib78),
    [68](https://arxiv.org/html/2402.05746v3#bib.bib68)] and lighting volume [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)].
    However, few of them consider multi-camera input, which is common for driving
    scenarios. In this paper, we propose a novel multi-camera lighting estimation
    method, McLight, combining with our McNeRF, to estimate a wider range of lighting
    and obtain the spatially-varying lighting effects of assets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 照明估计。照明估计旨在评估真实世界环境中的光照条件，以便无缝地将数字对象融入其中。早期方法[[37](https://arxiv.org/html/2402.05746v3#bib.bib37),
    [38](https://arxiv.org/html/2402.05746v3#bib.bib38)]针对户外环境，使用像地面阴影这样的显式线索。最近的研究通常采用基于学习的方法[[27](https://arxiv.org/html/2402.05746v3#bib.bib27),
    [31](https://arxiv.org/html/2402.05746v3#bib.bib31), [39](https://arxiv.org/html/2402.05746v3#bib.bib39),
    [32](https://arxiv.org/html/2402.05746v3#bib.bib32), [43](https://arxiv.org/html/2402.05746v3#bib.bib43),
    [78](https://arxiv.org/html/2402.05746v3#bib.bib78)]，通过预测不同的光照表示，如球形叶片[[10](https://arxiv.org/html/2402.05746v3#bib.bib10),
    [43](https://arxiv.org/html/2402.05746v3#bib.bib43)]、光探针[[39](https://arxiv.org/html/2402.05746v3#bib.bib39)]、环境图[[57](https://arxiv.org/html/2402.05746v3#bib.bib57),
    [59](https://arxiv.org/html/2402.05746v3#bib.bib59)]、HDR天空模型[[31](https://arxiv.org/html/2402.05746v3#bib.bib31),
    [78](https://arxiv.org/html/2402.05746v3#bib.bib78), [68](https://arxiv.org/html/2402.05746v3#bib.bib68)]和光照体积[[68](https://arxiv.org/html/2402.05746v3#bib.bib68)]。然而，少数研究考虑了多摄像头输入，而多摄像头输入在驾驶场景中是很常见的。本文提出了一种新型的多摄像头照明估计方法McLight，结合我们的McNeRF，用于估计更广泛的光照范围，并获取资产的空间变化光照效果。
- en: '| Method | Photo- realistic | Dim. | Multi- camera | Editable | External assets
    | Language | Open- source |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 写实 | 维度 | 多摄像头 | 可编辑 | 外部资产 | 语言 | 开源 |'
- en: '| CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)] | \usym'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '| CARLA [[22](https://arxiv.org/html/2402.05746v3#bib.bib22)] | \usym'
- en: 2613  | 3D | ✓ | ✓ | ✓ | \usym
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | 3D | ✓ | ✓ | ✓ | \usym
- en: 2613  | ✓ |
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| AirSim [[58](https://arxiv.org/html/2402.05746v3#bib.bib58)] | \usym'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '| AirSim [[58](https://arxiv.org/html/2402.05746v3#bib.bib58)] | \usym'
- en: 2613  | 3D | ✓ | ✓ | ✓ | \usym
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | 3D | ✓ | ✓ | ✓ | \usym
- en: 2613  | ✓ |
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| OpenScenario [[24](https://arxiv.org/html/2402.05746v3#bib.bib24)] | \usym'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '| OpenScenario [[24](https://arxiv.org/html/2402.05746v3#bib.bib24)] | \usym'
- en: 2613  | 3D | ✓ | ✓ | ✓ | \usym
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | 3D | ✓ | ✓ | ✓ | \usym
- en: 2613  | ✓ |
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| 51Sim-One [[1](https://arxiv.org/html/2402.05746v3#bib.bib1)] | \usym'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '| 51Sim-One [[1](https://arxiv.org/html/2402.05746v3#bib.bib1)] | \usym'
- en: 2613  | 3D | ✓ | ✓ | ✓ | \usym
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | 3D | ✓ | ✓ | ✓ | \usym
- en: 2613  | \usym
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  |
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| RoadRunner [[21](https://arxiv.org/html/2402.05746v3#bib.bib21)] | \usym'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '| RoadRunner [[21](https://arxiv.org/html/2402.05746v3#bib.bib21)] | \usym'
- en: 2613  | 3D | ✓ | ✓ | ✓ | \usym
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | 3D | ✓ | ✓ | ✓ | \usym
- en: 2613  | ✓ |
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| BEVGen [[63](https://arxiv.org/html/2402.05746v3#bib.bib63)] | ✓ | 2D | ✓
    | ✓ | \usym'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '| BEVGen [[63](https://arxiv.org/html/2402.05746v3#bib.bib63)] | ✓ | 2D | ✓
    | ✓ | \usym'
- en: 2613  | \usym
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| BEVControl [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)] | ✓ | 2D
    | ✓ | ✓ | \usym'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '| BEVControl [[75](https://arxiv.org/html/2402.05746v3#bib.bib75)] | ✓ | 2D
    | ✓ | ✓ | \usym'
- en: 2613  | \usym
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  |
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| DriveDreamer [[67](https://arxiv.org/html/2402.05746v3#bib.bib67)] | ✓ |
    2D | ✓ | ✓ | \usym'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '| DriveDreamer [[67](https://arxiv.org/html/2402.05746v3#bib.bib67)] | ✓ |
    2D | ✓ | ✓ | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  |
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| DrivingDiffusion [[41](https://arxiv.org/html/2402.05746v3#bib.bib41)] |
    ✓ | 2D | ✓ | ✓ | \usym'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '| DrivingDiffusion [[41](https://arxiv.org/html/2402.05746v3#bib.bib41)] |
    ✓ | 2D | ✓ | ✓ | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  |
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| GAIA-1 [[34](https://arxiv.org/html/2402.05746v3#bib.bib34)] | ✓ | 2D | \usym'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '| GAIA-1 [[34](https://arxiv.org/html/2402.05746v3#bib.bib34)] | ✓ | 2D | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  | ✓ | \usym
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  |
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| MagicDrive [[26](https://arxiv.org/html/2402.05746v3#bib.bib26)] | ✓ | 2D
    | ✓ | ✓ | \usym'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '| MagicDrive [[26](https://arxiv.org/html/2402.05746v3#bib.bib26)] | ✓ | 2D
    | ✓ | ✓ | \usym'
- en: 2613  | \usym
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  |
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| READ [[42](https://arxiv.org/html/2402.05746v3#bib.bib42)] | ✓ | 3D | \usym'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '| READ [[42](https://arxiv.org/html/2402.05746v3#bib.bib42)] | ✓ | 3D | \usym'
- en: 2613  | \usym
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| Neural SG [[52](https://arxiv.org/html/2402.05746v3#bib.bib52)] | ✓ | 3D
    | \usym'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '| Neural SG [[52](https://arxiv.org/html/2402.05746v3#bib.bib52)] | ✓ | 3D
    | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  | \usym
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| Neural PLF [[51](https://arxiv.org/html/2402.05746v3#bib.bib51)] | ✓ | 3D
    | \usym'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '| Neural PLF [[51](https://arxiv.org/html/2402.05746v3#bib.bib51)] | ✓ | 3D
    | \usym'
- en: 2613  | \usym
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| S-NeRF [[73](https://arxiv.org/html/2402.05746v3#bib.bib73)] | ✓ | 3D | ✓
    | \usym'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '| S-NeRF [[73](https://arxiv.org/html/2402.05746v3#bib.bib73)] | ✓ | 3D | ✓
    | \usym'
- en: 2613  | \usym
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| UniSim [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)] | ✓ | 3D | \usym'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '| UniSim [[77](https://arxiv.org/html/2402.05746v3#bib.bib77)] | ✓ | 3D | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  | \usym
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | \usym
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  |
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  |
- en: '| MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)] | ✓ | 3D | \usym'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '| MARS [[72](https://arxiv.org/html/2402.05746v3#bib.bib72)] | ✓ | 3D | \usym'
- en: 2613  | ✓ | \usym
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ | \usym
- en: 2613  | \usym
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | \usym
- en: 2613  | ✓ |
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 2613  | ✓ |
- en: '| ChatSim (Ours) | ✓ | 3D | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| ChatSim (我们的) | ✓ | 3D | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 'Table 1: Comparison of existing and proposed methods for autonomous driving
    simulation.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有方法与提出的自动驾驶仿真方法比较。
- en: Large language model and collaborative framework. Large Language Models (LLMs)
    are AI systems trained on extensive data to understand, generate, and respond
    to human language. GPT [[11](https://arxiv.org/html/2402.05746v3#bib.bib11)] is
    a pioneering work to generate human-like content. The following updated versions
    GPT-3.5 [[14](https://arxiv.org/html/2402.05746v3#bib.bib14)] and GPT-4 [[50](https://arxiv.org/html/2402.05746v3#bib.bib50)],
    provide more intelligent capabilities like chatting, browsing and coding. Notable
    other large language models include InstructGPT [[53](https://arxiv.org/html/2402.05746v3#bib.bib53)],
    LLaMA [[64](https://arxiv.org/html/2402.05746v3#bib.bib64)] and PaLM [[18](https://arxiv.org/html/2402.05746v3#bib.bib18),
    [5](https://arxiv.org/html/2402.05746v3#bib.bib5)]. Based on LLM, many works [[69](https://arxiv.org/html/2402.05746v3#bib.bib69),
    [23](https://arxiv.org/html/2402.05746v3#bib.bib23), [80](https://arxiv.org/html/2402.05746v3#bib.bib80),
    [30](https://arxiv.org/html/2402.05746v3#bib.bib30), [3](https://arxiv.org/html/2402.05746v3#bib.bib3)]
    improve the problem-solving abilities by integrating communication among multiple
    agents. [[33](https://arxiv.org/html/2402.05746v3#bib.bib33)] and [[71](https://arxiv.org/html/2402.05746v3#bib.bib71)]
    define a group of well-organized agents to form operating procedures with conversation
    and code programming. In this paper, we exploit the power of collaborative LLM
    agents in simulation for autonomous driving, enabling the various editing of 3D
    scenes via language commands.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型和协作框架。大型语言模型（LLMs）是通过大量数据训练的人工智能系统，能够理解、生成和回应人类语言。GPT [[11](https://arxiv.org/html/2402.05746v3#bib.bib11)]
    是生成类人内容的开创性作品。随后的更新版本 GPT-3.5 [[14](https://arxiv.org/html/2402.05746v3#bib.bib14)]
    和 GPT-4 [[50](https://arxiv.org/html/2402.05746v3#bib.bib50)] 提供了更智能的功能，如聊天、浏览和编程。其他著名的大型语言模型包括
    InstructGPT [[53](https://arxiv.org/html/2402.05746v3#bib.bib53)]、LLaMA [[64](https://arxiv.org/html/2402.05746v3#bib.bib64)]
    和 PaLM [[18](https://arxiv.org/html/2402.05746v3#bib.bib18), [5](https://arxiv.org/html/2402.05746v3#bib.bib5)]。基于LLM的许多工作
    [[69](https://arxiv.org/html/2402.05746v3#bib.bib69), [23](https://arxiv.org/html/2402.05746v3#bib.bib23),
    [80](https://arxiv.org/html/2402.05746v3#bib.bib80), [30](https://arxiv.org/html/2402.05746v3#bib.bib30),
    [3](https://arxiv.org/html/2402.05746v3#bib.bib3)] 通过多智能体之间的通信来提高问题解决能力。[[33](https://arxiv.org/html/2402.05746v3#bib.bib33)]
    和 [[71](https://arxiv.org/html/2402.05746v3#bib.bib71)] 定义了一组组织良好的智能体，通过对话和代码编程来形成操作流程。在本文中，我们利用协作LLM智能体在自动驾驶仿真中的力量，通过语言命令进行3D场景的多种编辑。
- en: 3 Collaborative LLM-Agents for Editing
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 协作LLM智能体编辑
- en: The ChatSim system analyzes specific user commands and returns a video that
    meets customized needs; see Figure [2](https://arxiv.org/html/2402.05746v3#S3.F2
    "Figure 2 ‣ 3 Collaborative LLM-Agents for Editing ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents"). Since user commands could
    be abstract and sophisticated, it requires the system to have flexible task-handling
    ability. Directly applying a single LLM agent struggles with multi-step reasoning
    and cross-referencing. To address this, we design a series of collaborative LLM
    agents, where each agent is responsible for a unique aspect of the editing task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ChatSim 系统分析特定的用户指令并返回满足定制需求的视频；见图 [2](https://arxiv.org/html/2402.05746v3#S3.F2
    "Figure 2 ‣ 3 Collaborative LLM-Agents for Editing ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")。由于用户指令可能非常抽象且复杂，因此要求系统具备灵活的任务处理能力。直接应用单一的
    LLM 代理会在多步推理和交叉引用方面遇到困难。为了解决这个问题，我们设计了一系列协作 LLM 代理，每个代理负责编辑任务中的一个独特方面。
- en: '![Refer to caption](img/5ab76accda8270dc38d8f04b1e6a2ea2.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅图注](img/5ab76accda8270dc38d8f04b1e6a2ea2.png)'
- en: 'Figure 2: ChatSim system overview. The system exploit multiple collaborative
    LLM agents with specialized roles to decouple an overall demand into specific
    editing tasks. Each agent equips an LLM and corresponding role functions to interpret
    and execute its specific tasks.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：ChatSim 系统概述。该系统利用多个协作的 LLM 代理，具有专门的角色，将整体需求分解为具体的编辑任务。每个代理配备了 LLM 和相应的角色功能，以解释并执行其特定任务。
- en: '![Refer to caption](img/37c3766e71edca548999f29603651016.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅图注](img/37c3766e71edca548999f29603651016.png)'
- en: 'Figure 3: Prompt example of view adjustment agent.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：视图调整代理的提示示例。
- en: 3.1 Specific Agent’s Functionality
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 特定代理的功能
- en: 'Agents in ChatSim comprise two key components: a Large Language Model (LLM)
    and the corresponding role functions. The LLM is responsible for understanding
    the received commands while the role functions process the received data. Each
    agent is equipped with unique LLM prompts and role functions tailored to their
    specific duties within the system. To accomplish their tasks, agents first convert
    the received commands to a structured configuration using LLM with the assistance
    of prompts. Then the role functions utilize the structured configuration as parameters
    to process the received data and produce the desired outcomes; see an agent example
    on the right side of Figure [2](https://arxiv.org/html/2402.05746v3#S3.F2 "Figure
    2 ‣ 3 Collaborative LLM-Agents for Editing ‣ Editable Scene Simulation for Autonomous
    Driving via Collaborative LLM-Agents"). This workflow endows agents with both
    language interpretation capabilities and precise execution capabilities.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ChatSim 中的代理包括两个关键组件：大型语言模型（LLM）和相应的角色功能。LLM 负责理解接收到的指令，而角色功能处理接收到的数据。每个代理都配备了针对其系统内特定职责定制的
    LLM 提示和角色功能。为了完成任务，代理首先使用 LLM 在提示的帮助下将接收到的指令转换为结构化配置。然后，角色功能利用结构化配置作为参数来处理接收到的数据，并产生所需的结果；见图
    [2](https://arxiv.org/html/2402.05746v3#S3.F2 "Figure 2 ‣ 3 Collaborative LLM-Agents
    for Editing ‣ Editable Scene Simulation for Autonomous Driving via Collaborative
    LLM-Agents") 右侧的代理示例。该工作流程赋予代理语言解释能力和精确执行能力。
- en: Project Manager Agent. The project manager agent decomposes direct commands
    into clear natural language instructions dispatched to other editing agents. To
    equip the project manager agent with the capability of command decomposition,
    we design a series of prompts for its LLM. The core idea of the prompts is to
    describe the action set, give the overall goal, and define the output form with
    examples; The role functions send the decomposed instructions to other agents
    for editing. The presence of the project manager agent enhances the system’s robustness
    in interpreting various inputs and streamlines operations for clarity and fine
    granularity.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 项目经理代理。项目经理代理将直接指令分解为清晰的自然语言指令，并将其分发给其他编辑代理。为了使项目经理代理具备指令分解能力，我们为其 LLM 设计了一系列提示。提示的核心思想是描述动作集合，给出整体目标，并通过示例定义输出形式；角色功能将分解后的指令发送给其他代理进行编辑。项目经理代理的存在增强了系统在解释各种输入方面的鲁棒性，并简化了操作，确保清晰和精细化。
- en: Tech agent for view adjustment. The view adjustment agent generates suitable
    extrinsic camera parameters. The LLM in the agent translates the natural language
    instructions for viewpoint adjustment into movement parameters to the target viewpoint’s
    position and angle. In role functions, these movement parameters are turned into
    transformation matrices required by the extrinsic, which are then multiplied by
    the original parameters to yield a new viewpoint.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 用于视角调整的技术代理。视角调整代理生成合适的外部相机参数。代理中的LLM将自然语言的视角调整指令转化为运动参数，指向目标视角的位置和角度。在角色功能中，这些运动参数被转化为外部所需的变换矩阵，然后与原始参数相乘，以得到新的视角。
- en: Tech agent for background rendering. The background rendering agent renders
    the scene background based on multi-camera images. The LLM receives the rendering
    command and then operates the role functions for rendering. Notably, in role functions,
    we specifically integrate a novel neural radiance field method (McNeRF) taking
    multi-camera inputs and considering exposure time, solving the problem of blurring
    and brightness inconsistency in multi-camera rendering, see more details in Section
    [4.1](https://arxiv.org/html/2402.05746v3#S4.SS1 "4.1 McNeRF for Background Rendering
    ‣ 4 Novel Rendering Methods ‣ Editable Scene Simulation for Autonomous Driving
    via Collaborative LLM-Agents").
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 用于背景渲染的技术代理。背景渲染代理根据多相机图像渲染场景背景。LLM接收渲染指令，然后操作角色功能进行渲染。特别地，在角色功能中，我们特别整合了一种新型的神经辐射场方法（McNeRF），它接受多相机输入并考虑曝光时间，解决了多相机渲染中的模糊和亮度不一致问题，更多细节请参见第[4.1节](https://arxiv.org/html/2402.05746v3#S4.SS1
    "4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene
    Simulation for Autonomous Driving via Collaborative LLM-Agents")。
- en: Tech agent for vehicle deleting. The vehicle deleting agent removes specified
    vehicles from the background. It first identifies current vehicle attributes like
    3D bounding boxes and colors from given scene information or results from a scene
    perception model like [[46](https://arxiv.org/html/2402.05746v3#bib.bib46)]. The
    LLM gathers attributes of the vehicles and performs matching with user requests.
    Upon confirming the targeted vehicles, it employs a per-frame inpainting model
    as the role functions, such as latent diffusion methods [[55](https://arxiv.org/html/2402.05746v3#bib.bib55)],
    to effectively delete them from the scene.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用于车辆删除的技术代理。车辆删除代理从背景中删除指定的车辆。它首先从给定的场景信息或来自场景感知模型的结果中识别当前车辆的属性，如3D边界框和颜色[[46](https://arxiv.org/html/2402.05746v3#bib.bib46)]。LLM收集车辆属性并与用户请求进行匹配。确认目标车辆后，它使用逐帧修复模型作为角色功能，如潜在扩散方法[[55](https://arxiv.org/html/2402.05746v3#bib.bib55)]，有效地将其从场景中删除。
- en: Tech agent for 3D asset management. The 3D asset management agent selects and
    modifies 3D digital assets according to user specifications. It constructs and
    maintains a 3D digital asset bank; see our bank details in the Appendix. To facilitate
    the addition of various objects, the agent first uses LLM to select the most suitable
    asset by key attributes matching with the requirements, such as color and type.
    If the matching is not perfect, the agent could modify the asset through its role
    functions like changing the color.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 用于3D资产管理的技术代理。3D资产管理代理根据用户规格选择和修改3D数字资产。它构建并维护一个3D数字资产库；有关我们库的详细信息，请参见附录。为了便于添加各种物体，代理首先通过LLM根据关键属性（如颜色和类型）匹配用户需求，选择最合适的资产。如果匹配不完美，代理可以通过其角色功能修改资产，例如更改颜色。
- en: Tech agent for vehicle motion. The vehicle motion agent creates the initial
    places and subsequent motions of vehicles following the requests. Existing vehicle
    motion generation methods cannot directly generate motion purely from text and
    the scene map. To solve the problem, here we propose a novel text-to-motion method.
    The key idea is linking a placement and planning module as role functions with
    LLMs to extract and turn motion attributes into coordinates. Motion attributes
    include position attributes (e.g., distance, direction) and movement attributes
    (e.g., speed, action). For the placement module, we endow each lane node in the
    lane map with its attributes to match with the position attributes. The planning
    module plans the vehicle’s approximate destination lane node and then plans the
    intermediate trajectory by fitting the Bezier curves. We also add trajectory tracking [[74](https://arxiv.org/html/2402.05746v3#bib.bib74)]
    to fit vehicle dynamics; see more details in the Appendix.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆运动技术代理。车辆运动代理根据请求生成车辆的初始位置和后续运动。现有的车辆运动生成方法无法仅通过文本和场景地图直接生成运动。为了解决这个问题，我们提出了一种新型的文本到运动方法。关键思想是将位置与规划模块作为角色功能与大语言模型（LLMs）结合，从中提取并将运动属性转换为坐标。运动属性包括位置属性（例如，距离、方向）和运动属性（例如，速度、动作）。对于位置模块，我们赋予车道图中的每个车道节点属性，以匹配位置属性。规划模块规划车辆的大致目的地车道节点，然后通过拟合贝塞尔曲线规划中间轨迹。我们还增加了轨迹追踪[[74](https://arxiv.org/html/2402.05746v3#bib.bib74)]以适应车辆动力学；更多细节请见附录。
- en: Tech agent for foreground rendering. The foreground rendering agent integrates
    camera extrinsic infomation, 3D assets, and motion information to render foreground
    objects in the scene. Notably, to seamlessly integrate the external assets with
    the current scene, we design a multi-camera lighting estimation method (McLight)
    into the role functions, coupling with McNeRF. The estimated illumination is then
    utilized by Blender API to generate foreground images. The detailed technical
    aspects will be elaborated in Section [4.2](https://arxiv.org/html/2402.05746v3#S4.SS2
    "4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene
    Simulation for Autonomous Driving via Collaborative LLM-Agents").
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 前景渲染技术代理。前景渲染代理将相机外部信息、3D资产和运动信息集成，以渲染场景中的前景物体。特别是，为了无缝地将外部资产与当前场景结合，我们在角色功能中设计了多相机光照估计方法（McLight），并与McNeRF结合。然后，通过Blender
    API利用估算的光照生成前景图像。详细的技术细节将在第[4.2](https://arxiv.org/html/2402.05746v3#S4.SS2 "4.2
    McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene
    Simulation for Autonomous Driving via Collaborative LLM-Agents")节中阐述。
- en: 3.2 Agent Collaboration Workflow
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 代理协作工作流
- en: 'Agents with tailored functions collaboratively work together to edit based
    on user commands. The project manager orchestrates and dispatches instructions
    to editing agents. The editing agents form two teams: background generation and
    foreground generation. For background generation, the background rendering agent
    generates rendered images using the extrinsic parameters from the view adjustment
    agent, followed by inpainting by the vehicle deleting agent. For foreground generation,
    the foreground rendering agent renders the images using the extrinsic parameters
    from the view adjustment agent, selected 3D assets from 3D asset management agent,
    and generated motions from vehicle motion agent. Finally, the foreground and background
    images are composed to create and deliver a video to the user. The editing information
    in each agent’s configuration is recorded by the project manager agent for possible
    multi-round editings.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有定制功能的代理根据用户指令协同工作进行编辑。项目经理协调并派发指令给编辑代理。编辑代理分为两个团队：背景生成和前景生成。对于背景生成，背景渲染代理使用视角调整代理提供的外部参数生成渲染图像，随后由车辆删除代理进行修补。在前景生成方面，前景渲染代理利用视角调整代理提供的外部参数、3D资产管理代理选择的3D资产以及车辆运动代理生成的运动来渲染图像。最终，前景图像和背景图像合成，生成并交付视频给用户。每个代理的编辑信息都会由项目经理代理记录，便于进行多轮编辑。
- en: 4 Novel Rendering Methods
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 种新型渲染方法
- en: Based on the collaborative LLM agents framework introduced in Section [3](https://arxiv.org/html/2402.05746v3#S3
    "3 Collaborative LLM-Agents for Editing ‣ Editable Scene Simulation for Autonomous
    Driving via Collaborative LLM-Agents"), this section presents two novel rendering
    techniques to enhance photo-realism in simulations. To tackle the rendering challenges
    caused by multiple cameras, we propose multi-camera neural radiance field (McNeRF),
    a novel NeRF model considering the varied camera exposure times for visual consistency.
    To render realistic external digital assets with location-specific lighting and
    accurate shadows, we propose McLight, a novel hybrid lighting estimation method
    combined with our McNeRF. Note that McNeRF and McLight are leveraged by the background
    rendering agent and the foreground rendering agent, respectively.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 基于第[3](https://arxiv.org/html/2402.05746v3#S3 "3 Collaborative LLM-Agents for
    Editing ‣ Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")节介绍的协作
    LLM 代理框架，本节提出了两种新的渲染技术，以增强仿真中的照片级真实感。为了应对由多相机引起的渲染挑战，我们提出了多相机神经辐射场（McNeRF），这是一种新型的
    NeRF 模型，考虑了不同相机曝光时间对视觉一致性的影响。为了渲染具有特定位置光照和准确阴影的真实外部数字资产，我们提出了 McLight，这是一种与 McNeRF
    结合的新型混合光照估计方法。需要注意的是，McNeRF 和 McLight 分别被背景渲染代理和前景渲染代理所利用。
- en: 4.1 McNeRF for Background Rendering
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 McNeRF 背景渲染
- en: 'An autonomous vehicle typically equips multiple cameras to achieve a comprehensive
    perception view. However, this poses challenges for NeRF training due to the misaligned
    multi-camera poses from asynchronized camera trigger times and the brightness
    inconsistency originating from different exposure times. To address these challenges,
    the proposed McNeRF uses two techniques: multi-camera alignment and brightness-consistent
    rendering.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆通常配备多个相机以实现全面的感知视角。然而，由于相机触发时间的异步性导致的多相机位姿错位，以及不同曝光时间带来的亮度不一致，这为 NeRF
    训练带来了挑战。为了解决这些问题，提出的 McNeRF 使用了两项技术：多相机对齐和亮度一致性渲染。
- en: Multi-camera alignment. Autonomous vehicles, despite having a localization module
    for accurate camera poses, face challenges with asynchronous trigger times across
    multiple cameras. To align camera extrinsics for NeRF training, our core idea
    is to leverage a consistent spatial coordinate system provided by Agisoft Metashape [[2](https://arxiv.org/html/2402.05746v3#bib.bib2)]
    to align the images captured by multiple cameras at different timestamps.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 多相机对齐。尽管自动驾驶车辆拥有用于准确相机位姿的定位模块，但由于多个相机触发时间的异步性，仍然面临挑战。为了对齐用于 NeRF 训练的相机外参，我们的核心思路是利用
    Agisoft Metashape 提供的一致空间坐标系统[[2](https://arxiv.org/html/2402.05746v3#bib.bib2)]，将不同时间戳下由多个相机拍摄的图像对齐。
- en: 'Specifically, let $\mathcal{I}^{(i,k)}$ and $\mathcal{\xi}^{(i,k)}$ be the
    image captured by the $i$th camera at the $k$th trigger and the corresponding
    camera pose in the vehicle’s global coordinate space, respectively. We first input
    all images into Metashape for recalibration. The aligned camera pose is then obtained
    as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，设 $\mathcal{I}^{(i,k)}$ 和 $\mathcal{\xi}^{(i,k)}$ 分别表示第 $i$ 个相机在第 $k$ 次触发时拍摄的图像和对应的相机位姿，该位姿在车辆全球坐标空间中。我们首先将所有图像输入到
    Metashape 进行重新校准。然后获得对齐后的相机位姿，公式如下：
- en: '|  | $\widehat{\xi}^{(i,k)}=T_{M\rightarrow G}\cdot\xi_{M}^{(i,k)},$ |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{\xi}^{(i,k)}=T_{M\rightarrow G}\cdot\xi_{M}^{(i,k)},$ |  |'
- en: where $\xi_{M}^{(i,k)}$ denotes the recalibrated camera pose in the Metashape’s
    unified spatial coordinate space, and $T_{M\rightarrow G}$ is the transformation
    from the Metashape’s coordinate space to the vehicle’s global coordinate space.
    After alignment, the pose noise can be significantly reduced. Then, the aligned
    camera pose $\widehat{\xi}^{(i,t)}$ can be used to generate the origins and directions
    of rays for McNeRF, enabling high-fidelity rendering. The aligned pose can also
    facilitate the foreground rendering agent’s operations.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\xi_{M}^{(i,k)}$ 表示在 Metashape 统一空间坐标系中的重新校准相机位姿，$T_{M\rightarrow G}$ 是从
    Metashape 坐标空间到车辆全球坐标空间的转换矩阵。经过对齐后，相机位姿噪声可以显著减少。然后，对齐后的相机位姿 $\widehat{\xi}^{(i,t)}$
    可以用于生成 McNeRF 的光线起源和方向，从而实现高保真渲染。对齐后的位姿还可以促进前景渲染代理的操作。
- en: '![Refer to caption](img/e5354bea6115213449497c3964d9d576.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/e5354bea6115213449497c3964d9d576.png)'
- en: 'Figure 4: Rendering framework. The main components include McNeRF and McLight.
    Background rendering uses McNeRF to predict HDR pixel value and convert it to
    LDR with sRGB OETF. McLight includes a skydome lighting estimation network and
    adopts McNeRF to generate surrounding lighting.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：渲染框架。主要组件包括 McNeRF 和 McLight。背景渲染使用 McNeRF 预测 HDR 像素值，并通过 sRGB OETF 转换为
    LDR。McLight 包括一个天幕光照估计网络，并采用 McNeRF 生成周围的光照。
- en: Brightness-consistent rendering. The exposure times of cameras can differ substantially,
    causing significant brightness differences across images, hindering the NeRF training.
    As shown in Figure [4](https://arxiv.org/html/2402.05746v3#S4.F4 "Figure 4 ‣ 4.1
    McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents"), McNeRF, addresses this
    by incorporating exposure times into HDR radiance fields, prompting brightness
    consistency.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 亮度一致性渲染。相机的曝光时间可能差异很大，这会导致图像之间的亮度差异显著，从而阻碍 NeRF 的训练。如图[4](https://arxiv.org/html/2402.05746v3#S4.F4
    "Figure 4 ‣ 4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")所示，McNeRF
    通过将曝光时间融入 HDR 辐射场，解决了这一问题，从而促进了亮度的一致性。
- en: 'We adopt F2-NeRF [[66](https://arxiv.org/html/2402.05746v3#bib.bib66)] as our
    backbone model to handle the unbounded scene, sampling $K$ points along the ray
    $\mathbf{r}$ and estimating each point’s HDR radiance $\mathbf{e}_{k}$ and density
    $\sigma_{k}$. The HDR light intensity is then calculated as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用 F2-NeRF [[66](https://arxiv.org/html/2402.05746v3#bib.bib66)] 作为我们的主干模型来处理无界场景，在光线
    $\mathbf{r}$ 上采样 $K$ 个点，并估计每个点的 HDR 辐射度 $\mathbf{e}_{k}$ 和密度 $\sigma_{k}$。然后，HDR
    光强计算如下：
- en: '|  | $\widehat{\mathcal{I}}_{\rm HDR}(\mathbf{r})=f(\Delta t)\cdot\sum^{K}_{k=1}T_{k%
    }\alpha_{k}\mathbf{e}_{k},$ |  | (1) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\widehat{\mathcal{I}}_{\rm HDR}(\mathbf{r})=f(\Delta t)\cdot\sum^{K}_{k=1}T_{k%
    }\alpha_{k}\mathbf{e}_{k},$ |  | (1) |'
- en: where $\alpha_{k}=1-\text{exp}(-\sigma_{k}\delta_{i})$ is the opacity, $\delta_{i}$
    is the point sampling interval, $T_{k}=\prod^{k-1}_{i=0}(1-\alpha_{i})$ is the
    accumulated transmittance and $\Delta t$ is the exposure time. The normalization
    function $f(\Delta t)=1+\epsilon(\Delta t-\mu)/{\sigma}$ is designed to stabilize
    training, where $\epsilon$ is a hyperparameter for scaling, $\mu$ and $\sigma$
    are the mean and standard deviation of the exposure times of all images, respectively.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha_{k}=1-\text{exp}(-\sigma_{k}\delta_{i})$ 是不透明度，$\delta_{i}$ 是采样点间隔，$T_{k}=\prod^{k-1}_{i=0}(1-\alpha_{i})$
    是累计透射率，$\Delta t$ 是曝光时间。归一化函数 $f(\Delta t)=1+\epsilon(\Delta t-\mu)/{\sigma}$
    用于稳定训练，其中 $\epsilon$ 是用于缩放的超参数，$\mu$ 和 $\sigma$ 分别是所有图像曝光时间的均值和标准差。
- en: By predicting scene radiance in HDR and multiplying it by the exposure time,
    we recover the light intensity received by the sensor and tackle the inconsistent
    color supervision at the intersections of two camera images with distinct exposure
    times. Moreover, the HDR light intensity outputted by McNeRF can provide scene-level
    illumination for foreground object rendering, a topic further discussed in Section [4.2](https://arxiv.org/html/2402.05746v3#S4.SS2
    "4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene
    Simulation for Autonomous Driving via Collaborative LLM-Agents").
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过预测场景辐射度（HDR）并将其与曝光时间相乘，我们恢复了传感器接收到的光强，并解决了曝光时间不同的两幅相机图像交叉处的不一致颜色监督问题。此外，McNeRF
    输出的 HDR 光强可以为前景物体渲染提供场景级的照明，这一话题将在第[4.2](https://arxiv.org/html/2402.05746v3#S4.SS2
    "4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣ Editable Scene
    Simulation for Autonomous Driving via Collaborative LLM-Agents")节进一步讨论。
- en: 'To train the rendering network, we enforce the consistency of radiance between
    the rendered image (prediction) and the captured image (ground-truth). Given the
    ground-truth image $\mathcal{I}$, the loss function is then:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练渲染网络，我们强制渲染图像（预测）和捕获图像（地面真相）之间的辐射度一致性。给定地面真相图像 $\mathcal{I}$，损失函数如下：
- en: '|  | $\mathcal{L}=\frac{1}{&#124;R&#124;}\sum_{\mathbf{r}\in R}\left(\mathrm{OETF}\left(%
    \widehat{\mathcal{I}}_{\rm HDR}(\mathbf{r})\right)-\mathcal{I}(\mathbf{r})% \right)^{2},$
    |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}=\frac{1}{\mid R \mid}\sum_{\mathbf{r}\in R}\left(\mathrm{OETF}\left(%
    \widehat{\mathcal{I}}_{\rm HDR}(\mathbf{r})\right)-\mathcal{I}(\mathbf{r})% \right)^{2},$
    |  |'
- en: where $R$ represents the ray set and $\mathrm{OETF}(\cdot)$ is the sRGB opto-electronic
    transfer function (gamma correction) [[19](https://arxiv.org/html/2402.05746v3#bib.bib19)]
    that converts HDR light intensity to LDR colors.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$R$ 表示光线集，$\mathrm{OETF}(\cdot)$ 是 sRGB 光电传输函数（伽马校正）[[19](https://arxiv.org/html/2402.05746v3#bib.bib19)]，用于将
    HDR 光强转换为 LDR 颜色。
- en: 4.2 McLight for Foreground Rendering
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 McLight 用于前景渲染
- en: To enrich the scene’s content with substantial digital 3D assets, we employ
    Blender [[20](https://arxiv.org/html/2402.05746v3#bib.bib20)] foreground virtual
    objects’ rendering. A seamless insertion critically depends on accurately estimating
    the scene’s illumination conditions. Thus, as shown in Figure [4](https://arxiv.org/html/2402.05746v3#S4.F4
    "Figure 4 ‣ 4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents"),
    we propose McLight, a novel hybrid lighting estimation consisting of skydome lighting
    and surrounding lighting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了通过大量数字化 3D 资产丰富场景内容，我们采用 Blender [[20](https://arxiv.org/html/2402.05746v3#bib.bib20)]
    前景虚拟物体的渲染。无缝插入的关键在于准确估计场景的照明条件。因此，如图 [4](https://arxiv.org/html/2402.05746v3#S4.F4
    "Figure 4 ‣ 4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")
    所示，我们提出了 McLight，一种新型混合照明估计方法，包括天穹光照和周围光照。
- en: Skydome lighting estimation. Estimating skydome lighting from images is challenging
    for restoring accurate sun behavior. To achieve this, we propose a novel residual
    connection from the estimated peak intensity to the HDR reconstruction to address
    over-smoothing output. Further, we adopt a self-attention mechanism to fuse multi-camera
    inputs, capturing complementary visual cues.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 天穹光照估计。从图像中估计天穹光照对于恢复准确的太阳行为是具有挑战性的。为此，我们提出了一种新型的残差连接，将估计的峰值强度与 HDR 重建结果相结合，以解决过度平滑的输出问题。此外，我们采用了一种自注意力机制来融合多摄像头输入，捕捉互补的视觉线索。
- en: Here we employ a two-stage process. In the first stage, we train an autoencoder
    to reconstruct the corresponding HDR panorama from an LDR panorama. Following
     [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)], the encoder transforms
    the LDR skydome panorama into three intermediate vectors, including the peak direction
    vector $\mathbf{f}_{\rm dir}\in\mathbb{R}^{3}$, the intensity vector $\mathbf{f}_{\rm
    int}\in\mathbb{R}^{3}_{+}$, and the sky content vector $\mathbf{f}_{\rm content}\in\mathbb{R}^{64}$.
    However, as HDR intensity behaves like an impulse response at its peak position,
    with pixel values thousands of times higher than its neighbors, it is difficult
    for the decoder to recover such patterns. To tackle this, we design a residual
    connection that injects $\mathbf{f}_{\rm int}$ into the decoded HDR panorama with
    a spherical Gaussian lobe attenuation. This explicitly restores the peak intensity
    of the sun in the reconstructed HDR panorama, allowing us to render strong shadows
    for virtual objects.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们采用了一个两阶段的过程。在第一阶段，我们训练一个自编码器，从 LDR 全景图重建对应的 HDR 全景图。根据 [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)]，编码器将
    LDR 天穹全景图转换为三个中间向量，包括峰值方向向量 $\mathbf{f}_{\rm dir}\in\mathbb{R}^{3}$、强度向量 $\mathbf{f}_{\rm
    int}\in\mathbb{R}^{3}_{+}$ 和天空内容向量 $\mathbf{f}_{\rm content}\in\mathbb{R}^{64}$。然而，由于
    HDR 强度在其峰值位置表现得像一个冲击响应，其像素值比邻近像素高出数千倍，解码器很难恢复这种模式。为了解决这个问题，我们设计了一种残差连接，通过球形高斯叶片衰减将
    $\mathbf{f}_{\rm int}$ 注入解码后的 HDR 全景图。这显式地恢复了重建的 HDR 全景图中太阳的峰值强度，使我们能够为虚拟物体渲染强烈的阴影。
- en: 'In the second stage, we train an image encoder and a multi-camera fusion module
    built upon the pretrained decoder from the first stage. Specifically, for images
    from each camera, a shared image encoder predicts the peak direction vector $\mathbf{f}_{\rm
    dir}^{(i)}$, the intensity vector $\mathbf{f}_{\rm int}^{(i)}$, and the sky content
    vector $\mathbf{f}_{\rm content}^{(i)}$ for each image $\mathcal{I}^{(i)}$, where
    $i$ is the camera index. We design the latent vector fusion across the multiple
    camera views as follows: all $\mathbf{f}_{\rm dir}^{(i)}$ are aligned to the front-facing
    view using their extrinsic parameters and averaged to form $\bar{\mathbf{f}}_{\rm
    dir}$; all $\mathbf{f}_{\rm int}^{(i)}$ are averaged to yield $\bar{\mathbf{f}}_{\rm
    int}$; all $\mathbf{f}_{\rm content}^{(i)}$ are integrated into $\bar{\mathbf{f}}_{\rm
    content}$ through a self-attention module. Finally, the pretrained decoder reconstructs
    the HDR skydome image $\mathcal{I}_{\rm skydome}$ from $\bar{\mathbf{f}}_{\rm
    dir}$, $\bar{\mathbf{f}}_{\rm int}$ and $\bar{\mathbf{f}}_{\rm content}$.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们训练了一个图像编码器和一个多摄像头融合模块，该模块基于第一阶段的预训练解码器。具体来说，对于来自每个摄像头的图像，使用共享的图像编码器预测每个图像$\mathcal{I}^{(i)}$的峰值方向向量$\mathbf{f}_{\rm
    dir}^{(i)}$、强度向量$\mathbf{f}_{\rm int}^{(i)}$和天空内容向量$\mathbf{f}_{\rm content}^{(i)}$，其中$i$是摄像头的索引。我们设计了跨多个摄像头视角的潜在向量融合，具体过程如下：所有$\mathbf{f}_{\rm
    dir}^{(i)}$都通过它们的外部参数对齐到正面视角，并取平均得到$\bar{\mathbf{f}}_{\rm dir}$；所有$\mathbf{f}_{\rm
    int}^{(i)}$取平均得到$\bar{\mathbf{f}}_{\rm int}$；所有$\mathbf{f}_{\rm content}^{(i)}$通过自注意力模块融合成$\bar{\mathbf{f}}_{\rm
    content}$。最后，预训练的解码器从$\bar{\mathbf{f}}_{\rm dir}$、$\bar{\mathbf{f}}_{\rm int}$和$\bar{\mathbf{f}}_{\rm
    content}$重建HDR天空穹顶图像$\mathcal{I}_{\rm skydome}$。
- en: Compared to alternative approaches [[68](https://arxiv.org/html/2402.05746v3#bib.bib68),
    [31](https://arxiv.org/html/2402.05746v3#bib.bib31)], our multi-camera sky dome
    estimation technique accurately reproduces the sun’s intensity response behavior
    at its peak with our residual designs, significantly improving the accuracy and
    fidelity of the skydome reconstruction.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法相比[[68](https://arxiv.org/html/2402.05746v3#bib.bib68), [31](https://arxiv.org/html/2402.05746v3#bib.bib31)]，我们的多摄像头天空穹顶估计技术通过残差设计准确地重现了太阳在其峰值时的强度响应行为，显著提高了天空穹顶重建的准确性和保真度。
- en: '![Refer to caption](img/bc96bad070aff9cfbed5100e1eea8853.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/bc96bad070aff9cfbed5100e1eea8853.png)'
- en: 'Figure 5: Editing result under a complex and mixed command.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在复杂混合命令下的编辑结果。
- en: Surrounding lighting estimation. Merely modeling the skydome cannot replicate
    the complex location-specific lighting effects, like those in the shade with sunlight
    blocked by trees or buildings. Our McNeRF is capable of storing precise 3D scene
    information, enabling us to capture the surrounding scene’s impact on lighting.
    This approach facilitates the achievement of spatially-varying lighting estimation.
    Specifically, we sample the hemisphere rays at the virtual object’s position o.
    The rays’ directions, $\textbf{d}_{i},i=0,1,\cdots,h\times w$, are aligned with
    pixel coordinates on a unit sphere using equirectangular projection from an environment
    map, where $h$ and $w$ are map’s height and width. With the ray $\mathbf{r}=\mathbf{o}+t\mathbf{d}_{i}$,
    we query our McNeRF as Equation [1](https://arxiv.org/html/2402.05746v3#S4.E1
    "In 4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣ Editable
    Scene Simulation for Autonomous Driving via Collaborative LLM-Agents") to obtain
    HDR surrounding lighting $\mathcal{I}_{\rm surround}(\textbf{o},\textbf{d}_{i})$.
    The surrounding lighting estimation reconstructs complex environmental lighting,
    achieving a spatially varying effect and high consistency with the background.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 周围光照估计。仅仅建模天空穹顶无法复制复杂的地点特定光照效果，例如那些树木或建筑物挡住阳光的阴影区域。我们的McNeRF能够存储精确的3D场景信息，使我们能够捕捉到周围场景对光照的影响。这种方法有助于实现空间变化的光照估计。具体来说，我们在虚拟物体的位置o处采样半球射线。射线的方向$\textbf{d}_{i},i=0,1,\cdots,h\times
    w$，通过等距矩形投影对准单位球面上的像素坐标，该投影来自环境图，其中$h$和$w$分别是地图的高度和宽度。对于射线$\mathbf{r}=\mathbf{o}+t\mathbf{d}_{i}$，我们根据公式[1](https://arxiv.org/html/2402.05746v3#S4.E1
    "In 4.1 McNeRF for Background Rendering ‣ 4 Novel Rendering Methods ‣ Editable
    Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")查询我们的McNeRF，得到HDR周围光照$\mathcal{I}_{\rm
    surround}(\textbf{o},\textbf{d}_{i})$。周围光照估计重建了复杂的环境光照，实现了空间变化的效果，并与背景高度一致。
- en: 'Blending. We blend the HDR intensity value from the skydome and surrounding
    lighting by transmittance of the final sampling point from McNeRF. The idea is
    that the rays emitted outside the radiance fields will definitely hit the skydome.
    Given the direction $\textbf{d}_{i}$, we retrieve the skydome’s intensity $\mathcal{I}_{\rm
    skydome}(\textbf{d}_{i})$ with equirectangular projection. The final HDR light
    intensity $\mathcal{I}_{\rm env}(\textbf{o},\textbf{d}_{i})$ is a combination
    of scene and skydome:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 混合。我们通过McNeRF最终采样点的透射率将天穹和周围光照的HDR强度值进行混合。其思路是，射线从辐射场外发射后，必定会击中天穹。给定方向$\textbf{d}_{i}$，我们使用等经纬度投影检索天穹的强度$\mathcal{I}_{\rm
    skydome}(\textbf{d}_{i})$。最终的HDR光强度$\mathcal{I}_{\rm env}(\textbf{o},\textbf{d}_{i})$是场景与天穹的结合：
- en: '|  | $\mathcal{I}_{\rm env}(\textbf{o},\textbf{d}_{i})=\mathcal{I}_{\rm surround}(%
    \textbf{o},\textbf{d}_{i})+T_{K}\mathcal{I}_{\rm skydome}(\textbf{d}_{i}),$ |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{I}_{\rm env}(\textbf{o},\textbf{d}_{i})=\mathcal{I}_{\rm surround}(\textbf{o},\textbf{d}_{i})+T_{K}\mathcal{I}_{\rm
    skydome}(\textbf{d}_{i}),$ |  |'
- en: where $T_{K}$ is the last sampling point’s transmittance.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$T_{K}$是最后一个采样点的透射率。
- en: 'McLight offers two main advantages: i) it explicitly recovers the illuminance
    behavior at the peak and use complementary information from multiple cameras to
    restore accurate skydome; and ii) it enables location-specific lighting with consideration
    of complex scene structures.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: McLight提供了两个主要优势：i）它显式地恢复了峰值处的照明行为，并利用来自多个相机的互补信息来恢复准确的天穹；ii）它能够考虑复杂场景结构，实现特定位置的照明。
- en: '![Refer to caption](img/70ca2562c2a4b2cb342ca04d5bc5d077.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/70ca2562c2a4b2cb342ca04d5bc5d077.png)'
- en: 'Figure 6: Editing result under a highly abstract command.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在高度抽象命令下的编辑结果。
- en: 5 Experimental Results
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: 5.1 Datasets and implementation details
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集与实现细节
- en: We demonstrate a variety of results mainly on the Waymo Open Dataset [[62](https://arxiv.org/html/2402.05746v3#bib.bib62)],
    which contains high-quality multi-camera images and the corresponding calibrations.
    For McLight skydome estimation, we collect 449 HDRIs from online HDRI databases
    for the autoencoder training and use HoliCity [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)],
    a street view panorama dataset for the second stage; see more dataset details
    in the Appendix.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要在Waymo Open Dataset上展示了多种结果[[62](https://arxiv.org/html/2402.05746v3#bib.bib62)]，该数据集包含高质量的多相机图像及其对应的标定信息。对于McLight天穹估计，我们从在线HDRI数据库收集了449个HDRI用于自编码器训练，并使用HoliCity
    [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)]，一个街景全景数据集作为第二阶段的数据；更多数据集细节请参见附录。
- en: In our experiment, we use front, left front, and right front cameras in each
    frame. During the rendering process, we choose 40 frames per scene at a 10Hz sampling
    rate, totaling 120 images. We evenly select $1/8$ of these as the test set, with
    the remainder used for training. The input images are used at the dataset’s initial
    resolution of $1920\times 1280$; we employ GPT-4 as the LLMs in all of our experiments;
    see more implementation details in the Appendix.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们在每帧中使用前置、左前和右前相机。在渲染过程中，我们以10Hz的采样率选择每个场景的40帧，总共120张图像。我们均匀地选择其中$1/8$作为测试集，其余部分用于训练。输入图像使用数据集的初始分辨率$1920\times
    1280$；我们在所有实验中使用GPT-4作为大语言模型（LLMs）；更多实现细节请参见附录。
- en: 5.2 System results
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 系统结果
- en: Editing via language commands. We select three representative commands to demonstrate
    the editing results. All of the results demonstrate we achieve photo-realistic
    wide angle results, thanks to McNeRF and McLight.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过语言命令进行编辑。我们选择了三个具有代表性的命令来展示编辑结果。所有结果表明，得益于McNeRF和McLight，我们实现了照片级真实感的广角效果。
- en: '*Mixed and complex command.* We send the system with a mixed and complex command,
    implying that a police car is chasing a wrong-way racer. The target scene, command
    and the result are shown in Figure [5](https://arxiv.org/html/2402.05746v3#S4.F5
    "Figure 5 ‣ 4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents").
    We see that i) every requirement in the complex command is accurately executed
    thanks to our multi-agent collaboration design; ii) this command successfully
    simulates one rare but dangerous driving condition, which is significant in accident
    testing.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '*混合与复杂指令。* 我们向系统发送了一条混合复杂的指令，意味着一辆警车正在追逐一名逆行赛车手。目标场景、指令和结果如图[5](https://arxiv.org/html/2402.05746v3#S4.F5
    "Figure 5 ‣ 4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")所示。我们看到：i)
    复杂指令中的每个要求都得到了准确执行，这要归功于我们多代理协作设计；ii) 这条指令成功地模拟了一种罕见但危险的驾驶情况，在事故测试中具有重要意义。'
- en: '*Highly abstract command.* The second type is a highly abstract command. The
    inputs and results are presented in Figure [6](https://arxiv.org/html/2402.05746v3#S4.F6
    "Figure 6 ‣ 4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents").
    We see that i) this highly abstract command is hard to decompose by sentence division
    but still can be correctly executed by our method, and ii) our 3D asset bank offers
    a large variety of objects for addition.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*高度抽象的指令。* 第二类是高度抽象的指令。输入和结果如图[6](https://arxiv.org/html/2402.05746v3#S4.F6
    "Figure 6 ‣ 4.2 McLight for Foreground Rendering ‣ 4 Novel Rendering Methods ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")所示。我们看到：i)
    这种高度抽象的指令很难通过句子划分来分解，但我们的方式仍然能正确执行；ii) 我们的3D资产库提供了丰富的对象供添加。'
- en: '![Refer to caption](img/273bcf91aa325ba1e3a38027da31b945.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/273bcf91aa325ba1e3a38027da31b945.png)'
- en: 'Figure 7: Editing result under multi-round commands.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：多轮指令下的编辑结果。
- en: '*Multi-round command.* We also perform a multi-round chat with our system,
    and the commands in different rounds exist context dependencies. The final results
    are shown in Figure [7](https://arxiv.org/html/2402.05746v3#S5.F7 "Figure 7 ‣
    5.2 System results ‣ 5 Experimental Results ‣ Editable Scene Simulation for Autonomous
    Driving via Collaborative LLM-Agents"). We see that i) our system is well-equipped
    to handle multi-round commands and execute the commands in each round precisely;
    ii) our system can handle the context dependencies across different rounds thanks
    to the recording ability of the project manager agent.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*多轮指令。* 我们还与系统进行了多轮对话，不同轮次的指令之间存在上下文依赖关系。最终结果如图[7](https://arxiv.org/html/2402.05746v3#S5.F7
    "Figure 7 ‣ 5.2 System results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")所示。我们看到：i) 我们的系统能够很好地处理多轮指令，并准确执行每一轮的指令；ii)
    得益于项目经理代理的记录能力，我们的系统能够处理跨轮次的上下文依赖关系。'
- en: '![Refer to caption](img/a5ccfdbe6466e22e137745869dc25839.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a5ccfdbe6466e22e137745869dc25839.png)'
- en: 'Figure 8: Qualitative comparison with Visual Programming[[29](https://arxiv.org/html/2402.05746v3#bib.bib29)].'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：与视觉编程的定性比较[[29](https://arxiv.org/html/2402.05746v3#bib.bib29)]。
- en: Comparison with Visual Programming [[29](https://arxiv.org/html/2402.05746v3#bib.bib29)].
    Visual Programming (VP) is the latest SoTA language-driven 2D image neuro-symbolic
    system, which can also be used for editing with language commands. Given that
    Visual programming only supports editing of single frames in 2D images, we limit
    our comparison to the deletion and replacement operations within the original
    view that it can support. The comparison in Figure [8](https://arxiv.org/html/2402.05746v3#S5.F8
    "Figure 8 ‣ 5.2 System results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") shows that ChatSim significantly
    outperforms VP in the two samples. Actually, in our extensive experiments, VP
    fails in most cases. The reason is that VP only has a single agent, making it
    difficult to handle mixed tasks. In comparison, ChatSim has multiple collaborative
    agents with specific roles, ensuring accurate task execution.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与视觉编程的比较[[29](https://arxiv.org/html/2402.05746v3#bib.bib29)]。视觉编程（VP）是最新的基于语言的SoTA二维图像神经符号系统，也可以通过语言命令进行编辑。由于视觉编程仅支持在二维图像中编辑单帧，因此我们将比较限制为其支持的删除和替换操作。图[8](https://arxiv.org/html/2402.05746v3#S5.F8
    "Figure 8 ‣ 5.2 System results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")中的比较表明，ChatSim在这两个样本中明显优于VP。实际上，在我们的广泛实验中，VP在大多数情况下都失败了。原因是VP只有一个单一的代理，难以处理混合任务。相比之下，ChatSim有多个协作代理，每个代理有特定的角色，确保任务的准确执行。
- en: '![Refer to caption](img/c244ce73e8f90c683c583ffb3ec1a896.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c244ce73e8f90c683c583ffb3ec1a896.png)'
- en: 'Figure 9: Comparison of detection performance w/o and with our simulated data
    under different amounts of real data during training.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在不同真实数据量下，使用和不使用我们模拟数据的检测性能比较。
- en: 3D detection with simulation data. We validate the benefits of our simulation
    as data augmentation for a downstream 3D object detection task on Waymo Open Dataset [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)].
    We simulate 1960 frames, derived from scenes in the training dataset. In the simulation,
    cars with various types, locations, and orientations are incorporated. The detection
    model adopts Lift-Splat [[54](https://arxiv.org/html/2402.05746v3#bib.bib54)].
    Figure [9](https://arxiv.org/html/2402.05746v3#S5.F9 "Figure 9 ‣ 5.2 System results
    ‣ 5 Experimental Results ‣ Editable Scene Simulation for Autonomous Driving via
    Collaborative LLM-Agents") shows detection performances with and without fixed
    augmentation under various amounts of real data. We see that i) a significant
    and consistent improvement across different data sizes is achieved; ii) when real
    data is limited, our simulation notably aids in rough detection (AP30); iii) when
    the amount of real data increases, our simulation further significantly improves
    fine-grained detection (AP70), reflecting the high-quality of our simulation.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟数据的3D检测。我们验证了将模拟数据作为数据增强在Waymo开放数据集上的下游3D物体检测任务中的优势[[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]。我们模拟了1960帧，源自训练数据集中的场景。在模拟中，加入了各种类型、位置和方向的汽车。检测模型采用Lift-Splat
    [[54](https://arxiv.org/html/2402.05746v3#bib.bib54)]。图[9](https://arxiv.org/html/2402.05746v3#S5.F9
    "Figure 9 ‣ 5.2 System results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")展示了在不同真实数据量下，使用和不使用固定增强的数据的检测性能。我们看到：i)
    在不同数据规模下，性能有显著且一致的提升；ii) 当真实数据有限时，我们的模拟数据显著帮助了粗略检测（AP30）；iii) 当真实数据量增加时，我们的模拟数据进一步显著提高了精细检测（AP70），反映出我们模拟数据的高质量。
- en: 5.3 Component results
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 组件结果
- en: Multi-agent collaboration. We evaluated the effectiveness of the multi-agent
    collaboration by checking whether the command is successfully executed in Table
    [2](https://arxiv.org/html/2402.05746v3#S5.T2 "Table 2 ‣ 5.3 Component results
    ‣ 5 Experimental Results ‣ Editable Scene Simulation for Autonomous Driving via
    Collaborative LLM-Agents"). In scenarios without multi-agent collaboration, all
    operations are executed by a single LLM agent. We see that a single LLM agent
    leads to notably lower execution accuracy across all categories due to process
    limitations. In contrast, the collaborative multi-agent approach can manage most
    commands, attributed to its task division and agent role specificity.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 多智能体协作。我们通过检查命令是否成功执行，评估了多智能体协作的有效性，详情见表格[2](https://arxiv.org/html/2402.05746v3#S5.T2
    "Table 2 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")。在没有多智能体协作的场景中，所有操作都由单一LLM代理执行。我们发现，由于过程限制，单一LLM代理在所有类别中导致执行精度明显降低。相反，协作的多智能体方法能够处理大多数命令，归功于任务分工和代理角色的特定性。
- en: '| Multi-agent collaboration | Language command category |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 多智能体协作 | 语言命令类别 |'
- en: '| Deletion | Addition | View change | Revision | Abstract |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | 添加 | 视图变化 | 修订 | 摘要 |'
- en: '|  | 0.617 | 0.383 | 0.717 | 0.367 | 0.216 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 0.617 | 0.383 | 0.717 | 0.367 | 0.216 |'
- en: '| ✓ | 0.983 | 0.867 | 0.967 | 0.917 | 0.883 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | 0.983 | 0.867 | 0.967 | 0.917 | 0.883 |'
- en: 'Table 2: The accuracy (%) of task completion by LLM without and with multi-agent
    collaboration.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表格2：LLM在没有和有多智能体协作的情况下的任务完成准确度（%）。
- en: '| Methods | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | Inf. time
    (s)$\downarrow$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | 推理时间 (秒)$\downarrow$
    |'
- en: '| DVGO [[60](https://arxiv.org/html/2402.05746v3#bib.bib60)] | 23.57 | 0.770
    | 0.508 | 7.7 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| DVGO [[60](https://arxiv.org/html/2402.05746v3#bib.bib60)] | 23.57 | 0.770
    | 0.508 | 7.7 |'
- en: '| Mip-NeRF360 [[8](https://arxiv.org/html/2402.05746v3#bib.bib8)] | 24.40 |
    0.754 | 0.528 | 101.8 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Mip-NeRF360 [[8](https://arxiv.org/html/2402.05746v3#bib.bib8)] | 24.40 |
    0.754 | 0.528 | 101.8 |'
- en: '| S-NeRF [[73](https://arxiv.org/html/2402.05746v3#bib.bib73)] | 24.71 | 0.759
    | 0.519 | 114.5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| S-NeRF [[73](https://arxiv.org/html/2402.05746v3#bib.bib73)] | 24.71 | 0.759
    | 0.519 | 114.5 |'
- en: '| F2NeRF [[66](https://arxiv.org/html/2402.05746v3#bib.bib66)] | 23.26 | 0.773
    | 0.439 | 2.4 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| F2NeRF [[66](https://arxiv.org/html/2402.05746v3#bib.bib66)] | 23.26 | 0.773
    | 0.439 | 2.4 |'
- en: '| Ours w/o alignment | 23.32 | 0.776 | 0.437 | 2.5 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 无对齐的我们的 | 23.32 | 0.776 | 0.437 | 2.5 |'
- en: '| Ours w/o exposure | 25.18 | 0.819 | 0.381 | 2.4 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 无曝光的我们的 | 25.18 | 0.819 | 0.381 | 2.4 |'
- en: '| McNeRF (Ours) | 25.82 | 0.822 | 0.378 | 2.5 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| McNeRF (我们的) | 25.82 | 0.822 | 0.378 | 2.5 |'
- en: 'Table 3: Background novel view rendering performance evaluation.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表格3：背景新视图渲染性能评估。
- en: 'Background rendering. We compare our McNeRF with several other state-of-the-art
    methods on the background novel view synthesis task. We perform reconstruction
    and rendering on 32 selected scenes. Table [3](https://arxiv.org/html/2402.05746v3#S5.T3
    "Table 3 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") shows the quantitative results
    comparison on three metrics: PSNR, SSIM, and LPIPS. We see that i) McNeRF achieves
    SoTA performance on all three metrics, significantly outperforming other baselines;
    ii) McNeRF has a fast inference speed, enabling quick responses to user requests
    for image rendering.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 背景渲染。我们将我们的McNeRF与其他几种最先进的方法在背景新视图合成任务上进行了比较。我们在32个选定场景上执行了重建和渲染。表格[3](https://arxiv.org/html/2402.05746v3#S5.T3
    "Table 3 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents")显示了三个指标（PSNR、SSIM和LPIPS）的定量结果比较。我们看到：i)
    McNeRF在所有三个指标上达到了SoTA（最先进技术）水平，显著优于其他基线方法；ii) McNeRF具有快速的推理速度，能够迅速响应用户对图像渲染的请求。
- en: '![Refer to caption](img/f9ab497a961f533d682520f5c6e45916.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/f9ab497a961f533d682520f5c6e45916.png)'
- en: 'Figure 10: Comparisons of wide-angle images generation. (a) S-NeRF.(b) F2NeRF.
    (c) McNeRF (Ours). Last row: target images.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：广角图像生成比较。（a）S-NeRF。（b）F2NeRF。（c）McNeRF（我们的）。最后一行：目标图像。
- en: Figure [10](https://arxiv.org/html/2402.05746v3#S5.F10 "Figure 10 ‣ 5.3 Component
    results ‣ 5 Experimental Results ‣ Editable Scene Simulation for Autonomous Driving
    via Collaborative LLM-Agents") demonstrates qualitative comparisons between other
    methods and ours. We see that existing NeRF methods do not consider the exposure
    time, leading to noticeable changes in brightness at the junctions of different
    cameras in the image, as well as an overall inconsistency in exposure across the
    wide-angle view. Our method can make the brightness of the entire image more consistent.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](https://arxiv.org/html/2402.05746v3#S5.F10 "Figure 10 ‣ 5.3 组件结果 ‣ 5 实验结果
    ‣ 用协作LLM-Agents进行自动驾驶的可编辑场景模拟")展示了与其他方法的定性比较。我们看到，现有的NeRF方法没有考虑曝光时间，导致图像中不同摄像机接缝处亮度的明显变化，以及广角视图中曝光的一致性问题。我们的方法可以使整个图像的亮度更加一致。
- en: '| Method | Peak Intensity(log10) Error | Peak Angular Error (deg) | User study(%)
    $\uparrow$ |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 峰值强度（log10）误差 | 峰值角度误差（度） | 用户研究（%） $\uparrow$ |'
- en: '| Mean $\downarrow$ | Median $\downarrow$ | Mean $\downarrow$ | Median $\downarrow$
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 均值 $\downarrow$ | 中位数 $\downarrow$ | 均值 $\downarrow$ | 中位数 $\downarrow$ |'
- en: '| Hold-Geoffroy et al. [[31](https://arxiv.org/html/2402.05746v3#bib.bib31)]
    | 0.899 | 0.975 | 48.4 | 51.6 | 19.5 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Hold-Geoffroy 等人 [[31](https://arxiv.org/html/2402.05746v3#bib.bib31)] |
    0.899 | 0.975 | 48.4 | 51.6 | 19.5 |'
- en: '| Wang et al. [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)] | 0.590
    | 0.628 | 33.5 | 29.4 | 37.3 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等人 [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)] | 0.590 | 0.628
    | 33.5 | 29.4 | 37.3 |'
- en: '| McLight (Ours) | 0.449 | 0.270 | 32.3 | 26.5 | 43.1 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| McLight（我们的方法） | 0.449 | 0.270 | 32.3 | 26.5 | 43.1 |'
- en: 'Table 4: Comparison with previous methods on lighting estimation.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：与先前方法在照明估计上的比较。
- en: '![Refer to caption](img/f1c8c711c149b29f2dfe95778bc98c09.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/f1c8c711c149b29f2dfe95778bc98c09.png)'
- en: 'Figure 11: Comparison with different lighting estimation methods.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：与不同照明估计方法的比较。
- en: Foreground rendering. We compare our McLight with the other two SoTA methods [[68](https://arxiv.org/html/2402.05746v3#bib.bib68),
    [31](https://arxiv.org/html/2402.05746v3#bib.bib31)]. Table [4](https://arxiv.org/html/2402.05746v3#S5.T4
    "Table 4 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") shows the comparison of
    relative intensity(log 10) error on our HDRI dataset, angular error on HoliCity [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)],
    and user study. We see that McLight achieves more accurate peak behavior and receives
    noticeably higher user preferences. Figure [11](https://arxiv.org/html/2402.05746v3#S5.F11
    "Figure 11 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") shows the visualizations
    of vehicle insertion. The vehicles added through McLight feature significantly
    more realistic reflections and strong shadows consistent with the scene.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 前景渲染。我们将我们的McLight与其他两个最先进的（SoTA）方法进行比较[[68](https://arxiv.org/html/2402.05746v3#bib.bib68),
    [31](https://arxiv.org/html/2402.05746v3#bib.bib31)]。表[4](https://arxiv.org/html/2402.05746v3#S5.T4
    "Table 4 ‣ 5.3 组件结果 ‣ 5 实验结果 ‣ 用协作LLM-Agents进行自动驾驶的可编辑场景模拟")展示了我们在HDRI数据集上的相对强度（log
    10）误差、HoliCity上的角度误差[[79](https://arxiv.org/html/2402.05746v3#bib.bib79)]，以及用户研究的对比。我们看到，McLight实现了更精确的峰值行为，并获得了明显更高的用户偏好。图[11](https://arxiv.org/html/2402.05746v3#S5.F11
    "Figure 11 ‣ 5.3 组件结果 ‣ 5 实验结果 ‣ 用协作LLM-Agents进行自动驾驶的可编辑场景模拟")展示了车辆插入的可视化效果。通过McLight加入的车辆表现出显著更逼真的反射和与场景一致的强烈阴影。
- en: '| Methods | Straight | Left Turn | Right Turn | Speed | Within-road |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 直行 | 左转 | 右转 | 速度 | 道路内 |'
- en: '| GPT2Code | 0.738 | 0.559 | 0.536 | 0.893 | 0.214 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| GPT2Code | 0.738 | 0.559 | 0.536 | 0.893 | 0.214 |'
- en: '| GPT2Motion | 0.595 | 0.119 | 0.167 | 0.345 | 0.277 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| GPT2Motion | 0.595 | 0.119 | 0.167 | 0.345 | 0.277 |'
- en: '| Ours | 0.988 | 0.940 | 0.976 | 0.952 | 1.000 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | 0.988 | 0.940 | 0.976 | 0.952 | 1.000 |'
- en: 'Table 5: Comparison with motion generation from text methods.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：与文本生成运动方法的比较。
- en: 'Vehicle motion. As shown in Table [5](https://arxiv.org/html/2402.05746v3#S5.T5
    "Table 5 ‣ 5.3 Component results ‣ 5 Experimental Results ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents"), we compare the motion generation
    method from user commands with two of our designed baselines: 1\. GPT2Motion,
    which directly uses LLM to return the motion coordinates; 2\. GPT2Code, which
    first generates code using LLM and executes it to obtain the vehicle motion. We
    validate multiple actions in multiple scenarios and report the user study result.
    The user study is to determine if the generated motions matched the command intents
    and fitted with the lane map. We see that our method demonstrated a significant
    advantage in generating motions from language commands. Additionally, it maintained
    a high rate of keeping the trajectories within the lane boundaries.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆运动。如表[5](https://arxiv.org/html/2402.05746v3#S5.T5 "Table 5 ‣ 5.3 Component
    results ‣ 5 Experimental Results ‣ Editable Scene Simulation for Autonomous Driving
    via Collaborative LLM-Agents")所示，我们将用户命令生成的运动方法与我们设计的两种基线进行比较：1. GPT2Motion，直接使用LLM返回运动坐标；2.
    GPT2Code，首先使用LLM生成代码并执行，获取车辆运动。我们验证了多个场景中的多种动作，并报告了用户研究结果。用户研究的目的是确定生成的运动是否符合命令意图，并是否适应车道图。我们发现我们的方法在从语言命令生成运动方面表现出了显著的优势。此外，它保持了高比例的轨迹符合车道边界。
- en: 6 Conclusions and Limitations
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与局限性
- en: This paper introduces ChatSim, the first system for editing 3D driving scene
    simulations via language commands and realistic rendering with import of external
    digital assets. To effectively execute user commands, ChatSim adopts an LLM-agent
    collaboration workflow. To promote photo-realistic simulation, we propose McNeRF
    and McLight for background and foreground rendering, respectively, accommodating
    multi-camera inputs. Experiments show that ChatSim successfully simulates customized
    data via language commands, achieving high-quality, photo-realistic outcomes.
    In future, we plan to integrate more background editing functionalities to ChatSim,
    such as weather changes.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了ChatSim，这是首个通过语言命令编辑3D驾驶场景仿真系统，并支持导入外部数字资产的真实渲染。为了有效执行用户命令，ChatSim采用了LLM-代理协作工作流。为了促进照片级真实仿真，我们提出了McNeRF和McLight分别用于背景和前景渲染，并支持多摄像头输入。实验表明，ChatSim成功地通过语言命令模拟了定制数据，实现了高质量、照片级真实的结果。未来，我们计划将更多的背景编辑功能集成到ChatSim中，例如天气变化。
- en: References
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] 51Sim-One. https://wdp.51aes.com/news/27.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 51Sim-One. https://wdp.51aes.com/news/27.'
- en: '[2] LLC Agisoft. Metashape–photogrammetric processing of digital images and
    3d spatial data generation, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] LLC Agisoft. Metashape——数字图像的摄影测量处理与3D空间数据生成, 2019.'
- en: '[3] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    and Eric Schulz. Playing repeated games with large language models. arXiv preprint
    arXiv:2305.16867, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    和 Eric Schulz. 使用大语言模型进行重复博弈. arXiv预印本 arXiv:2305.16867, 2023.'
- en: '[4] Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia Moseyko, Rohan
    Banerjee, Sertac Karaman, and Daniela Rus. Learning robust control policies for
    end-to-end autonomous driving from data-driven simulation. IEEE Robotics and Automation
    Letters, 5(2):1143–1150, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Alexander Amini, Igor Gilitschenski, Jacob Phillips, Julia Moseyko, Rohan
    Banerjee, Sertac Karaman, 和 Daniela Rus. 从数据驱动的仿真中学习端到端的鲁棒控制策略用于自动驾驶. IEEE Robotics
    and Automation Letters, 5(2):1143–1150, 2020.'
- en: '[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    等. Palm 2技术报告. arXiv预印本 arXiv:2305.10403, 2023.'
- en: '[6] Natalie M Banta. Property interests in digital assets: The rise of digital
    feudalism. Cardozo L. Rev., 38:1099, 2016.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Natalie M Banta. 数字资产中的财产权：数字封建主义的崛起. Cardozo L. Rev., 38:1099, 2016.'
- en: '[7] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo
    Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation
    for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 5855–5864, 2021.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo
    Martin-Brualla, 和 Pratul P Srinivasan. Mip-nerf: 一种用于抗锯齿神经辐射场的多尺度表示. 见于IEEE/CVF国际计算机视觉会议论文集，页面5855–5864,
    2021.'
- en: '[8] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and
    Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 5470–5479, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, 和 Peter
    Hedman. Mip-nerf 360: 无限反走样神经辐射场。载于IEEE/CVF计算机视觉与模式识别大会论文集，页码5470–5479，2022年。'
- en: '[9] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero,
    Błażej Osiński, Hugo Grimmett, and Peter Ondruska. Simnet: Learning reactive self-driving
    simulations from real-world observations. In 2021 IEEE International Conference
    on Robotics and Automation (ICRA), pages 5119–5125\. IEEE, 2021.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Luca Bergamini, Yawei Ye, Oliver Scheel, Long Chen, Chih Hu, Luca Del Pero,
    Błażej Osiński, Hugo Grimmett, 和 Peter Ondruska. Simnet: 从真实世界观测中学习反应式自动驾驶仿真。载于2021
    IEEE国际机器人与自动化大会（ICRA）论文集，页码5119–5125，IEEE，2021年。'
- en: '[10] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik Lensch, and Jan Kautz. Two-shot
    spatially-varying brdf and shape estimation. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 3982–3991, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Mark Boss, Varun Jampani, Kihwan Kim, Hendrik Lensch, 和 Jan Kautz. 双镜头空间变化BRDF与形状估计。载于IEEE/CVF计算机视觉与模式识别大会论文集，页码3982–3991，2020年。'
- en: '[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等人. 语言模型是少样本学习者。神经信息处理系统进展，33:1877–1901，2020年。'
- en: '[12] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
    Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes:
    A multimodal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 11621–11631, 2020.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong,
    Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, 和 Oscar Beijbom. nuscenes:
    一个用于自动驾驶的多模态数据集。载于IEEE/CVF计算机视觉与模式识别大会论文集，页码11621–11631，2020年。'
- en: '[13] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir
    Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse:
    3d tracking and forecasting with rich maps. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 8748–8757, 2019.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir
    Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan 等人. Argoverse:
    使用丰富地图的3D跟踪与预测。载于IEEE/CVF计算机视觉与模式识别大会论文集，页码8748–8757，2019年。'
- en: '[14] ChatGPT. https://openai.com/blog/chatgpt.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] ChatGPT. https://openai.com/blog/chatgpt。'
- en: '[15] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving:
    Learning affordance for direct perception in autonomous driving. In Proceedings
    of the IEEE international conference on computer vision, pages 2722–2730, 2015.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Chenyi Chen, Ari Seff, Alain Kornhauser, 和 Jianxiong Xiao. Deepdriving:
    为自动驾驶中的直接感知学习可感知性。载于IEEE国际计算机视觉大会论文集，页码2722–2730，2015年。'
- en: '[16] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and
    Raquel Urtasun. Monocular 3d object detection for autonomous driving. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2147–2156,
    2016.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, 和 Raquel
    Urtasun. 单目3D物体检测用于自动驾驶。载于IEEE计算机视觉与模式识别大会论文集，页码2147–2156，2016年。'
- en: '[17] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object
    detection network for autonomous driving. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 1907–1915, 2017.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, 和 Tian Xia. 基于多视角的3D物体检测网络用于自动驾驶。载于IEEE计算机视觉与模式识别大会论文集，页码1907–1915，2017年。'
- en: '[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann 等人. Palm: 利用路径扩展语言建模。arXiv预印本arXiv:2204.02311，2022年。'
- en: '[19] International Electrotechnical Commission et al. Iec 61966-2-1: 1999 multimedia
    systems and equipment- colour measurement and management- part 2-1: Colour management-
    default rgb colour space- srgb, 1999.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 国际电工委员会等人. IEC 61966-2-1: 1999 多媒体系统与设备-颜色测量与管理-第2-1部分: 颜色管理-默认RGB颜色空间-sRGB，1999年。'
- en: '[20] Blender Online Community. Blender - a 3D modelling and rendering package.
    Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Blender 在线社区. Blender - 一款 3D 建模和渲染软件包. Blender 基金会，Stichting Blender
    基金会，阿姆斯特丹，2018.'
- en: '[21] Valter Crescenzi, Giansalvatore Mecca, Paolo Merialdo, et al. Roadrunner:
    Towards automatic data extraction from large web sites. In VLDB, volume 1, pages
    109–118, 2001.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Valter Crescenzi, Giansalvatore Mecca, Paolo Merialdo 等. Roadrunner：迈向从大型网站自动提取数据.
    载于 VLDB, 第 1 卷，第 109–118 页，2001.'
- en: '[22] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen
    Koltun. Carla: An open urban driving simulator. In Conference on robot learning,
    pages 1–16\. PMLR, 2017.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez 和 Vladlen
    Koltun. Carla：一个开放的城市驾驶模拟器. 载于机器人学习会议论文集，第 1–16 页. PMLR, 2017.'
- en: '[23] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch.
    Improving factuality and reasoning in language models through multiagent debate.
    arXiv preprint arXiv:2305.14325, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum 和 Igor Mordatch.
    通过多智能体辩论改善语言模型的事实性和推理能力. arXiv 预印本 arXiv:2305.14325, 2023.'
- en: '[24] Openscanerio Editor. https://github.com/ebadi/openscenarioeditor.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Openscanerio 编辑器. https://github.com/ebadi/openscenarioeditor.'
- en: '[25] Unreal Engine. https://www.unrealengine.com/.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Unreal Engine. https://www.unrealengine.com/.'
- en: '[26] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung,
    and Qiang Xu. Magicdrive: Street view generation with diverse 3d geometry control.
    arXiv preprint arXiv:2310.02601, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan Yeung,
    和 Qiang Xu. Magicdrive：利用多样化的 3D 几何控制生成街景. arXiv 预印本 arXiv:2310.02601, 2023.'
- en: '[27] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, and Jean-François
    Lalonde. Fast spatially-varying indoor lighting estimation. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6908–6917,
    2019.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Mathieu Garon, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr 和 Jean-François
    Lalonde. 快速估计空间变化的室内光照. 载于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 6908–6917 页，2019.'
- en: '[28] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang,
    Chenjing Ding, Dongliang Wang, and Yikang Li. Streetsurf: Extending multi-view
    implicit surface reconstruction to street views. arXiv preprint arXiv:2306.04988,
    2023.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang,
    Chenjing Ding, Dongliang Wang 和 Yikang Li. Streetsurf：将多视角隐式表面重建扩展到街景. arXiv 预印本
    arXiv:2306.04988, 2023.'
- en: '[29] Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional
    visual reasoning without training. In Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 14953–14962, 2023.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Tanmay Gupta 和 Aniruddha Kembhavi. 视觉编程：无需训练的组合视觉推理. 载于 IEEE/CVF 计算机视觉与模式识别会议论文集，第
    14953–14962 页，2023.'
- en: '[30] Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie.
    Chatllm network: More brains, more intelligence. arXiv preprint arXiv:2304.12998,
    2023.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Rui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, 和 Liqiang Nie.
    Chatllm 网络：更多的大脑，更多的智能。arXiv 预印本 arXiv:2304.12998, 2023.'
- en: '[31] Yannick Hold-Geoffroy, Akshaya Athawale, and Jean-François Lalonde. Deep
    sky modeling for single image outdoor lighting estimation. In Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pages 6927–6935,
    2019.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Yannick Hold-Geoffroy, Akshaya Athawale 和 Jean-François Lalonde. 单图像户外光照估计的深度天空建模.
    载于 IEEE/CVF 计算机视觉与模式识别会议论文集，第 6927–6935 页，2019.'
- en: '[32] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto,
    and Jean-François Lalonde. Deep outdoor illumination estimation. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 7312–7321,
    2017.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano Gambaretto
    和 Jean-François Lalonde. 深度户外光照估计. 载于 IEEE 计算机视觉与模式识别会议论文集，第 7312–7321 页，2017.'
- en: '[33] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili
    Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, et al. Metagpt:
    Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,
    2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Ceyao Zhang, Zili
    Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran 等. Metagpt：面向多智能体协作框架的元编程.
    arXiv 预印本 arXiv:2308.00352, 2023.'
- en: '[34] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex
    Kendall, Jamie Shotton, and Gianluca Corrado. Gaia-1: A generative world model
    for autonomous driving. arXiv preprint arXiv:2309.17080, 2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George Fedoseev, Alex
    Kendall, Jamie Shotton 和 Gianluca Corrado. Gaia-1：一种用于自动驾驶的生成世界模型. arXiv 预印本 arXiv:2309.17080,
    2023.'
- en: '[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland,
    Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
    Segment anything. arXiv preprint arXiv:2304.02643, 2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 亚历山大·基里洛夫、埃里克·明顿、尼基拉·拉维、汉子·毛、克洛伊·罗兰、劳拉·古斯塔夫森、谢天特·肖、斯宾塞·怀特黑德、亚历山大·C·伯格、罗万·颜·罗等人。Segment
    anything。arXiv预印本arXiv:2304.02643，2023年。'
- en: '[36] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caroline Pantofaru,
    Leonidas J Guibas, Andrea Tagliasacchi, Frank Dellaert, and Thomas Funkhouser.
    Panoptic neural fields: A semantic object-aware neural scene representation. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 12871–12881, 2022.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 阿比吉特·昆杜、凯尔·吉诺瓦、夏乔奇·殷、阿里雷扎·法希、卡罗琳·潘托法鲁、利奥尼达斯·J·吉巴斯、安德烈亚·塔利亚萨奇、弗兰克·德拉尔特和托马斯·芬克豪泽。全景神经场：一种语义物体感知的神经场景表示。在IEEE/CVF计算机视觉与模式识别会议论文集，页面12871–12881，2022年。'
- en: '[37] Jean-François Lalonde and Iain Matthews. Lighting estimation in outdoor
    image collections. In 2014 2nd international conference on 3D vision, volume 1,
    pages 131–138\. IEEE, 2014.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 让-弗朗索瓦·拉隆德和伊恩·马修斯。户外图像集中的光照估计。2014年第二届国际三维视觉会议，卷1，第131–138页。IEEE，2014年。'
- en: '[38] Jean-François Lalonde, Srinivasa G Narasimhan, and Alexei A Efros. What
    do the sun and the sky tell us about the camera? International Journal of Computer
    Vision, 88:24–51, 2010.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 让-弗朗索瓦·拉隆德、斯里尼瓦萨·G·纳拉西曼和亚历克谢·A·埃夫罗斯。太阳和天空告诉我们关于相机什么信息？《计算机视觉国际期刊》，88:24–51，2010年。'
- en: '[39] Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent Charbonnel,
    Jay Busch, and Paul Debevec. Deeplight: Learning illumination for unconstrained
    mobile mixed reality. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 5918–5928, 2019.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 克洛伊·勒金德尔、马万春、格雷厄姆·费夫、约翰·弗林、劳伦特·夏博内尔、杰伊·布什和保罗·德贝维克。Deeplight：为无约束的移动混合现实学习光照。在IEEE/CVF计算机视觉与模式识别会议论文集，页面5918–5928，2019年。'
- en: '[40] Peixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. Rtm3d: Real-time
    monocular 3d detection from object keypoints for autonomous driving. In European
    Conference on Computer Vision, pages 644–660\. Springer, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 李佩轩、赵怀慈、刘鹏飞和曹飞道。Rtm3d：基于物体关键点的实时单目三维检测用于自动驾驶。欧洲计算机视觉会议，页面644–660。Springer，2020年。'
- en: '[41] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: Layout-guided
    multi-view driving scene video generation with latent diffusion model. arXiv preprint
    arXiv:2310.07771, 2023.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] 李小凡、张一夫和叶小青。Drivingdiffusion：基于潜在扩散模型的布局引导多视角驾驶场景视频生成。arXiv预印本arXiv:2310.07771，2023年。'
- en: '[42] Zhuopeng Li, Lu Li, and Jianke Zhu. Read: Large-scale neural scene rendering
    for autonomous driving. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 37, pages 1522–1529, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] 李卓鹏、李璐和朱建科。Read：面向自动驾驶的大规模神经场景渲染。在2023年AAAI人工智能会议论文集，卷37，第1522–1529页，2023年。'
- en: '[43] Zhengqin Li, Zexiang Xu, Ravi Ramamoorthi, Kalyan Sunkavalli, and Manmohan
    Chandraker. Learning to reconstruct shape and spatially-varying reflectance from
    a single image. ACM Transactions on Graphics (TOG), 37(6):1–11, 2018.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 李政钦、许泽翔、拉维·拉马穆尔蒂、卡良·孙卡瓦利和曼莫汉·钱德拉克。通过单张图像学习重建形状和空间变化反射率。《ACM图形学报》（TOG），37(6):1–11，2018年。'
- en: '[44] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang,
    Wenyu Liu, and Chang Huang. Maptr: Structured modeling and learning for online
    vectorized hd map construction. arXiv preprint arXiv:2208.14437, 2022.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] 廖本成、陈绍宇、王兴刚、程天恒、张倩、刘文宇和黄昌。Maptr：用于在线向量化高清地图构建的结构化建模与学习。arXiv预印本arXiv:2208.14437，2022年。'
- en: '[45] Bencheng Liao, Shaoyu Chen, Yunchi Zhang, Bo Jiang, Qian Zhang, Wenyu
    Liu, Chang Huang, and Xinggang Wang. Maptrv2: An end-to-end framework for online
    vectorized hd map construction. arXiv preprint arXiv:2308.05736, 2023.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] 廖本成、陈绍宇、张云池、姜博、张倩、刘文宇、黄昌和王兴刚。Maptrv2：一种端到端的在线向量化高清地图构建框架。arXiv预印本arXiv:2308.05736，2023年。'
- en: '[46] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L
    Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird’s-eye
    view representation. In 2023 IEEE International Conference on Robotics and Automation
    (ICRA), pages 2774–2781\. IEEE, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] 刘志健、唐昊天、亚历山大·阿米尼、杨欣宇、毛慧紫、丹妮拉·L·鲁斯和宋汉。Bevfusion：统一的鸟瞰图表示的多任务多传感器融合。2023年IEEE国际机器人与自动化大会（ICRA），页面2774–2781。IEEE，2023年。'
- en: '[47] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron,
    Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields
    for view synthesis. Communications of the ACM, 65(1):99–106, 2021.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron,
    Ravi Ramamoorthi 和 Ren Ng。NeRF：将场景表示为神经辐射场进行视图合成。《计算机协会通讯》，65(1):99–106，2021年。'
- en: '[48] Milan Miric, Kevin J Boudreau, and Lars Bo Jeppesen. Protecting their
    digital assets: The use of formal & informal appropriability strategies by app
    developers. Research Policy, 48(8):103738, 2019.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Milan Miric, Kevin J Boudreau 和 Lars Bo Jeppesen。保护他们的数字资产：应用开发者使用的正式与非正式可得性策略。《研究政策》，48(8):103738，2019年。'
- en: '[49] Thomas Müller, Alex Evans, Christoph Schied, and Alexander Keller. Instant
    neural graphics primitives with a multiresolution hash encoding. ACM Transactions
    on Graphics (ToG), 41(4):1–15, 2022.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Thomas Müller, Alex Evans, Christoph Schied 和 Alexander Keller。使用多分辨率哈希编码的即时神经图形原语。《ACM
    图形学学报》(ToG)，41(4):1–15，2022年。'
- en: '[50] OpenAI. Gpt-4 technical report. 2023.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] OpenAI。GPT-4技术报告，2023年。'
- en: '[51] Julian Ost, Issam Laradji, Alejandro Newell, Yuval Bahat, and Felix Heide.
    Neural point light fields. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 18419–18429, 2022.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Julian Ost, Issam Laradji, Alejandro Newell, Yuval Bahat 和 Felix Heide。神经点光场。收录于
    IEEE/CVF 计算机视觉与模式识别会议论文集，页码18419–18429, 2022年。'
- en: '[52] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide.
    Neural scene graphs for dynamic scenes. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 2856–2865, 2021.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt 和 Felix Heide。用于动态场景的神经场景图。收录于
    IEEE/CVF 计算机视觉与模式识别会议论文集，页码2856–2865, 2021年。'
- en: '[53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    Training language models to follow instructions with human feedback. Advances
    in Neural Information Processing Systems, 35:27730–27744, 2022.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray 等人。通过人类反馈训练语言模型执行指令。《神经信息处理系统进展》，35:27730–27744,
    2022年。'
- en: '[54] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from
    arbitrary camera rigs by implicitly unprojecting to 3d. In Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part XIV 16, pages 194–210\. Springer, 2020.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Jonah Philion 和 Sanja Fidler。Lift, splat, shoot：通过隐式反投影到3D来编码来自任意相机设备的图像。收录于《计算机视觉–ECCV
    2020：第16届欧洲会议》，英国格拉斯哥，2020年8月23–28日，会议录，第XIV部分，第16卷，页码194–210，Springer，2020年。'
- en: '[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models, 2021.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser 和 Björn
    Ommer。使用潜在扩散模型的高分辨率图像合成，2021年。'
- en: '[56] Guodong Rong, Byung Hyun Shin, Hadi Tabatabaee, Qiang Lu, Steve Lemke,
    Mārtiņš Možeiko, Eric Boise, Geehoon Uhm, Mark Gerow, Shalin Mehta, et al. Lgsvl
    simulator: A high fidelity simulator for autonomous driving. In 2020 IEEE 23rd
    International conference on intelligent transportation systems (ITSC), pages 1–6\.
    IEEE, 2020.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Guodong Rong, Byung Hyun Shin, Hadi Tabatabaee, Qiang Lu, Steve Lemke,
    Mārtiņš Možeiko, Eric Boise, Geehoon Uhm, Mark Gerow, Shalin Mehta 等人。LGSVL模拟器：用于自动驾驶的高保真模拟器。收录于2020年IEEE第23届国际智能交通系统会议（ITSC）论文集，页码1–6，IEEE，2020年。'
- en: '[57] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W Jacobs,
    and Jan Kautz. Neural inverse rendering of an indoor scene from a single image.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    8598–8607, 2019.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Soumyadip Sengupta, Jinwei Gu, Kihwan Kim, Guilin Liu, David W Jacobs
    和 Jan Kautz。基于单张图像的室内场景神经逆渲染。收录于 IEEE/CVF 国际计算机视觉会议论文集，页码8598–8607, 2019年。'
- en: '[58] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. Airsim:
    High-fidelity visual and physical simulation for autonomous vehicles. In Field
    and Service Robotics: Results of the 11th International Conference, pages 621–635\.
    Springer, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Shital Shah, Debadeepta Dey, Chris Lovett 和 Ashish Kapoor。Airsim：用于自动驾驶车辆的高保真视觉与物理模拟。收录于《领域与服务机器人：第11届国际会议成果》，页码621–635，Springer，2018年。'
- en: '[59] Gowri Somanath and Daniel Kurz. Hdr environment map estimation for real-time
    augmented reality. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 11298–11306, 2021.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Gowri Somanath 和 Daniel Kurz。用于实时增强现实的 HDR 环境图估计。收录于 IEEE/CVF 计算机视觉与模式识别会议论文集，页码11298–11306,
    2021年。'
- en: '[60] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization:
    Super-fast convergence for radiance fields reconstruction. In Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5459–5469,
    2022.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Cheng Sun, Min Sun 和 Hwann-Tzong Chen。直接体素网格优化：辐射场重建的超快收敛。发表于 IEEE/CVF
    计算机视觉与模式识别会议论文集，页面 5459–5469, 2022。'
- en: '[61] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability
    in perception for autonomous driving: Waymo open dataset. In Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pages 2446–2454,
    2020.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine 等人。自动驾驶感知的可扩展性：Waymo
    开放数据集。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，页面 2446–2454, 2020。'
- en: '[62] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan,
    Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon,
    Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov.
    Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
    June 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai
    Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan,
    Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon,
    Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen 和 Dragomir Anguelov。自动驾驶感知的可扩展性：Waymo
    开放数据集。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集（CVPR），2020年6月。'
- en: '[63] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-view image generation
    from a bird’s-eye view layout. arXiv preprint arXiv:2301.04634, 2023.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Alexander Swerdlow, Runsheng Xu 和 Bolei Zhou。基于鸟瞰视角布局的街景图像生成。arXiv 预印本
    arXiv:2301.04634, 2023。'
- en: '[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等人。Llama 2: 开放基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288, 2023。'
- en: '[65] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan. Suds:
    Scalable urban dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 12375–12385, 2023.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Haithem Turki, Jason Y Zhang, Francesco Ferroni 和 Deva Ramanan。Suds: 可扩展的城市动态场景。发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，页面 12375–12385, 2023。'
- en: '[66] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura,
    Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training
    with free camera trajectories. In Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pages 4150–4159, 2023.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Peng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura,
    Christian Theobalt 和 Wenping Wang。F2-nerf: 通过自由相机轨迹快速训练神经辐射场。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，页面
    4150–4159, 2023。'
- en: '[67] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Drivedreamer:
    Towards real-world-driven world models for autonomous driving. arXiv preprint
    arXiv:2309.09777, 2023.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen 和 Jiwen Lu。Drivedreamer:
    面向自动驾驶的现实驱动世界模型。arXiv 预印本 arXiv:2309.09777, 2023。'
- en: '[68] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz, and Sanja Fidler. Neural
    light field estimation for street scenes with differentiable virtual object insertion.
    In European Conference on Computer Vision, pages 380–397\. Springer, 2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Zian Wang, Wenzheng Chen, David Acuna, Jan Kautz 和 Sanja Fidler。街景场景的神经光场估计与可微分虚拟物体插入。发表于欧洲计算机视觉会议，页面
    380–397。Springer, 2022。'
- en: '[69] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng
    Ji. Unleashing cognitive synergy in large language models: A task-solving agent
    through multi-persona self-collaboration. arXiv preprint arXiv:2307.05300, 2023.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei 和 Heng Ji。释放大语言模型中的认知协同：通过多角色自我协作解决任务的智能体。arXiv
    预印本 arXiv:2307.05300, 2023。'
- en: '[70] Bichen Wu, Forrest Iandola, Peter H Jin, and Kurt Keutzer. Squeezedet:
    Unified, small, low power fully convolutional neural networks for real-time object
    detection for autonomous driving. In Proceedings of the IEEE conference on computer
    vision and pattern recognition workshops, pages 129–137, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Bichen Wu, Forrest Iandola, Peter H Jin 和 Kurt Keutzer。Squeezedet: 统一的小型低功耗全卷积神经网络，用于自动驾驶的实时物体检测。发表于
    IEEE 计算机视觉与模式识别会议论文集，页面 129–137, 2017。'
- en: '[71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. Autogen: 通过多代理对话框架启用下一代LLM应用.
    arXiv预印本arXiv:2308.08155，2023年。'
- en: '[72] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao,
    Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan,
    Yongliang Shi, Yiyi Liao, and Hao Zhao. Mars: An instance-aware, modular and realistic
    simulator for autonomous driving. CICAI, 2023.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao,
    Chao Hou, Haozhe Lou, Yuantao Chen, Runyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan,
    Yongliang Shi, Yiyi Liao, 和 Hao Zhao. Mars: 一种实例感知、模块化和真实的自动驾驶模拟器. CICAI，2023年。'
- en: '[73] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li Zhang. S-nerf:
    Neural radiance fields for street views. arXiv preprint arXiv:2303.00749, 2023.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, 和 Li Zhang. S-nerf: 用于街景的神经辐射场.
    arXiv预印本arXiv:2303.00749，2023年。'
- en: '[74] Yinda Xu and Lidong Yu. Drl-based trajectory tracking for motion-related
    modules in autonomous driving. arXiv preprint arXiv:2308.15991, 2023.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yinda Xu 和 Lidong Yu. 基于DRL的自动驾驶运动相关模块轨迹跟踪. arXiv预印本arXiv:2308.15991，2023年。'
- en: '[75] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and Kaicheng Yu.
    Bevcontrol: Accurately controlling street-view elements with multi-perspective
    consistency via bev sketch layout. arXiv preprint arXiv:2308.01661, 2023.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, 和 Kaicheng Yu. Bevcontrol:
    通过BEV草图布局精确控制街景元素的多视角一致性. arXiv预印本arXiv:2308.01661，2023年。'
- en: '[76] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru
    Erhan, Sean Rafferty, and Henrik Kretzschmar. Surfelgan: Synthesizing realistic
    sensor data for autonomous driving. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 11118–11127, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru
    Erhan, Sean Rafferty, 和 Henrik Kretzschmar. Surfelgan: 为自动驾驶合成逼真的传感器数据. 见于IEEE/CVF计算机视觉与模式识别会议论文集，页面11118–11127，2020年。'
- en: '[77] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma,
    Anqi Joyce Yang, and Raquel Urtasun. Unisim: A neural closed-loop sensor simulator.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 1389–1399, 2023.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma,
    Anqi Joyce Yang, 和 Raquel Urtasun. Unisim: 一个神经闭环传感器模拟器. 见于IEEE/CVF计算机视觉与模式识别会议论文集，页面1389–1399，2023年。'
- en: '[78] Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Sunil Hadap,
    Jonathan Eisenman, and Jean-François Lalonde. All-weather deep outdoor lighting
    estimation. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern
    Recognition, pages 10158–10166, 2019.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Jinsong Zhang, Kalyan Sunkavalli, Yannick Hold-Geoffroy, Sunil Hadap,
    Jonathan Eisenman, 和 Jean-François Lalonde. 全天候深度户外光照估计. 见于IEEE/CVF计算机视觉与模式识别会议论文集，页面10158–10166，2019年。'
- en: '[79] Yichao Zhou, Jingwei Huang, Xili Dai, Linjie Luo, Zhili Chen, and Yi Ma.
    HoliCity: A city-scale data platform for learning holistic 3D structures. 2020.
    arXiv:2008.03286 [cs.CV].'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Yichao Zhou, Jingwei Huang, Xili Dai, Linjie Luo, Zhili Chen, 和 Yi Ma.
    HoliCity: 一个用于学习整体3D结构的城市级数据平台. 2020年。arXiv:2008.03286 [cs.CV]。'
- en: '[80] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás,
    Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann,
    Kazuki Irie, et al. Mindstorms in natural language-based societies of mind. arXiv
    preprint arXiv:2305.17066, 2023.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Mingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, Róbert Csordás,
    Anand Gopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann,
    Kazuki Irie 等人. 基于自然语言的心智社会中的心智风暴. arXiv预印本arXiv:2305.17066，2023年。'
- en: Appendix A Supplementary Explanation of Table 2
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 表2的补充说明
- en: Table 2 evaluates the execution success rate of commands of 5 instruction categories
    across 4 different driving sequences. For each category, the accuracy is measured
    as the average success rate across 3 trials of 15 commands that are specifically
    designed for this category. Each trial is deemed successfully executed if the
    LLM-agent(s) accurately perform the required operations, including setting correct
    configurations and parameter values.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 表2评估了5种指令类别在4种不同驾驶序列中的执行成功率。对于每个类别，准确率通过3次试验中15个专门为该类别设计的指令的平均成功率来衡量。每次试验如果LLM代理能够准确执行所需操作，包括设置正确的配置和参数值，则视为成功执行。
- en: Appendix B Supplementary Experiments
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 补充实验
- en: B.1 Supplementary Experiments of Lighting Estimation.
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 光照估计的补充实验。
- en: We merge multi-view inputs into a wide-angle image for lighting estimation baselines
    with results in right table. McLight still significantly outperforms. Intensity
    evaluation, not involving multi-view inputs, remains the same as paper Tab.4.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将多视角输入合并为一个广角图像，以进行光照估计基线，结果见右表。McLight仍显著优于其他方法。强度评估不涉及多视角输入，与论文中的表4一致。
- en: '| Method, Multi-view (MV) version | MV Hold-Geoffroy | MV Wang | McLight (Ours)
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 方法，多视角（MV）版本 | MV Hold-Geoffroy | MV Wang | McLight（我们的） |'
- en: '| Peak Angular Error(Mean/Median) | 36.7/37.1 | 33.7/29.3 | 32.3/26.5 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 峰值角度误差（均值/中位数） | 36.7/37.1 | 33.7/29.3 | 32.3/26.5 |'
- en: B.2 Supplementary Experiments of Background Rendering.
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 背景渲染的补充实验。
- en: Table below shows ablations to train baselines with the multi-camera alignment
    used in McNeRF. Multi-camera alignment is a general and practical trick which
    improves the rendering performance consistently, and our McNeRF(with exposure)
    still outperforms other baselines.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 下表展示了使用McNeRF中的多视角对齐进行训练的基线消融实验。多视角对齐是一种通用且实用的技巧，它能够持续改善渲染性能，而我们的McNeRF（带曝光）依然优于其他基线。
- en: '| Methods | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | Inf. time
    (s)$\downarrow$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | PSNR$\uparrow$ | SSIM$\uparrow$ | LPIPS$\downarrow$ | 推理时间（秒）$\downarrow$
    |'
- en: '| DVGO + Alignment | 24.65 | 0.787 | 0.487 | 7.7 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| DVGO + 对齐 | 24.65 | 0.787 | 0.487 | 7.7 |'
- en: '| Mip-NeRF360 + Alignment | 25.50 | 0.759 | 0.514 | 101.8 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Mip-NeRF360 + 对齐 | 25.50 | 0.759 | 0.514 | 101.8 |'
- en: '| S-NeRF + Alignment | 25.53 | 0.760 | 0.513 | 114.5 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| S-NeRF + 对齐 | 25.53 | 0.760 | 0.513 | 114.5 |'
- en: '| F2NeRF + Alignment | 25.18 | 0.819 | 0.381 | 2.4 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| F2NeRF + 对齐 | 25.18 | 0.819 | 0.381 | 2.4 |'
- en: '| F2NeRF + Alignment + Exposure (Ours) | 25.82 | 0.822 | 0.378 | 2.5 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| F2NeRF + 对齐 + 曝光（我们的） | 25.82 | 0.822 | 0.378 | 2.5 |'
- en: Appendix C LLM-Agents Details
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C LLM-Agents 详情
- en: C.1 Agent Implement Details
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 Agent 实现详情
- en: LLM-Agents consist of their LLM (Large Language Model) component and corresponding
    functionalities. All experiments utilize the GPT-4 API[[50](https://arxiv.org/html/2402.05746v3#bib.bib50)]
    to implement the LLM part. In each agent’s prompt, there are elements involving
    the agent’s function, the definition of actions that the agent needs to perform,
    the definition of information inputted to the agent, and the definition of outputs
    required from the agent. To facilitate the integration of Python code and ensure
    stable calls, the LLM part is required to return information in the format of
    a JSON dictionary. Additionally, each LLM part’s prompt includes some examples,
    which contain inputs for certain scenarios and the corresponding expected outputs.
    If the input command does not contain the information of the keys of the output
    JSON dictionary, a default one will be filled in the dictionary. The parameters
    related to the GPT-4 API are all set to the official default values.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-Agents由其LLM（大型语言模型）组件及相应功能组成。所有实验均使用GPT-4 API[[50](https://arxiv.org/html/2402.05746v3#bib.bib50)]来实现LLM部分。在每个代理的提示中，包含了涉及代理功能的元素、代理需要执行的动作定义、输入到代理的定义信息，以及代理需要输出的定义结果。为了便于Python代码的集成并确保稳定的调用，LLM部分要求以JSON字典格式返回信息。此外，每个LLM部分的提示还包括一些示例，包含特定场景的输入和相应的预期输出。如果输入命令中没有包含输出JSON字典的键信息，则会在字典中填充默认值。与GPT-4
    API相关的所有参数都设置为官方默认值。
- en: Note that, for supporting modification operations during multi-round commands,
    the 3D asset management agent, the vehicle motion agent, and the vehicle deleting
    agent have the ability to modify the information of already added or deleted cars.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了支持在多轮命令中的修改操作，3D资产管理代理、车辆运动代理和车辆删除代理具备修改已添加或删除车辆信息的能力。
- en: C.2 Reasoning Processes
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 推理过程
- en: This section describes the natural language reasoning processes for the three
    cases presented in Section 5.2 of the main text.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了第5.2节中提出的三个案例的自然语言推理过程。
- en: 'Mixed and complex command. The initial input command is: "Remove all cars in
    the scene and add a Porsche driving the wrong way toward me fast. Additionally,
    add a police car also driving the wrong way and chasing behind the Porsche. The
    view should be moved 5 meters ahead and 0.5 meters above." The command is decoupled
    by the project manager agent as following commands: 1."Remove all cars."; 2\.
    "Add a Porsche driving the wrong way toward me fast."; 3\. "Add a police car also
    driving the wrong way and chasing behind the Porsche."; 4\. "The view should be
    moved 5 meters ahead and 0.5 meters above.". The "Remove all cars." command is
    distributed to the vehicle deleting agent, and then the agent finds the 3D boxes
    of all cars and applies the inpainting function for the removal operation. "Add
    a Porsche driving the wrong way toward me fast." command is distributed to the
    3D asset management agent for selecting the proper 3D asset. This command will
    also be distributed to the vehicle motion agent, which utilizes the key information
    in the command including "wrong way", "toward me" and "fast" to choose the appropriate
    start and end points and generate the motion with the motion generation function.
    "Add a police car also driving the wrong way and chasing behind the Porsche."
    command will also be executed in the same way as the former operation. This command
    mentions the information of the added car, and the added car’s information has
    been memorized by the project manager. This information is offered to the vehicle
    motion agent for determining the added police car’s location. "The view should
    be moved 5 meters ahead and 0.5 meters above." command is distributed to the view
    adjustment agent. The view adjustment agent returns the adjustment information
    of extrinsic as configuration, and calls the function to change the extrinsics
    to achieve view adjustment. Finally, background rendering and foreground rendering
    agents are required to generate the background and foreground results according
    to the information returned by the other agents, and the results are composed
    as the final outputs.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 混合和复杂的命令。初始输入命令是：“移除场景中的所有汽车，并快速添加一辆逆行的保时捷朝我驶来。此外，添加一辆警车也在逆行并追赶保时捷。视角应该向前移动5米，并上移0.5米。”该命令被项目经理代理解耦为以下命令：1.
    “移除所有汽车。”；2. “添加一辆逆行的保时捷朝我快速驶来。”；3. “添加一辆警车也在逆行并追赶保时捷。”；4. “视角应该向前移动5米，并上移0.5米。”。“移除所有汽车。”命令被分配给车辆删除代理，然后该代理查找所有汽车的3D框并应用修复功能执行删除操作。“添加一辆逆行的保时捷朝我快速驶来。”命令被分配给3D资源管理代理，以选择合适的3D资源。该命令还将分配给车辆运动代理，后者利用命令中的关键信息，包括“逆行”，“朝我”和“快速”来选择合适的起始和结束点，并使用运动生成函数生成运动。“添加一辆警车也在逆行并追赶保时捷。”命令将以与前述操作相同的方式执行。该命令提到新增车辆的信息，新增车辆的信息已由项目经理记忆并提供给车辆运动代理，用于确定新增警车的位置。“视角应该向前移动5米，并上移0.5米。”命令被分配给视角调整代理。视角调整代理返回外部调整的配置信息，并调用函数来改变外部参数以实现视角调整。最后，背景渲染和前景渲染代理需要根据其他代理返回的信息生成背景和前景结果，最终结果将组成最终输出。
- en: 'Highly abstract command. The initial input command is: "Create a traffic jam."
    The project manager agent analyzes the command and decouples it as multiple repeats
    of car addition. These addition commands are processed by the 3D asset management
    agent and vehicle motion agent successively and are rendered by the foreground
    rendering agent. Combined with the rendered results from the background rendering
    agent, we can get the final outputs.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 高度抽象的命令。初始输入命令是：“创建一个交通拥堵。”项目经理代理分析该命令并将其解耦为多个重复的汽车添加命令。这些添加命令依次由3D资源管理代理和车辆运动代理处理，并由前景渲染代理渲染。结合背景渲染代理渲染的结果，我们可以得到最终输出。
- en: 'Multi-round command. The first initial command is: "Ego vehicle drives ahead
    slowly. Add a car to the close front that is moving ahead.” The command is decoupled
    by the project manager agent as 1: "Ego vehicle drives ahead slowly."; 2: "Add
    a car to the close front that is moving ahead.". The first sub-command is distributed
    to view the adjustment agent, and the agent generates the extrinsics that represent
    moving ahead slowly. The second sub-command is executed as the process introduced
    above.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 多轮指令。第一条初始指令是：“自驾车缓慢前进。向近前添加一辆正在前进的车。”该指令由项目经理代理人解耦为1：“自驾车缓慢前进。”；2：“向近前添加一辆正在前进的车。”第一条子指令分发给视图调整代理人，代理人生成表示缓慢前进的外参。第二条子指令按上述过程执行。
- en: 'The second initial command is: "Modify the added car to turn left. Add a Chevrolet
    to the front of the added car. Add another vehicle to the left of the added Mini
    driving toward me." The command is decoupled by the project manager agent as 1:
    "Modify the added car to turn left."; 2: "Add a Chevrolet to the front of the
    added car."; 3: "Add another vehicle to the left of the added Mini driving toward
    me." The first sub-command is distributed to the vehicle motion agent, which generates
    new motion based on the command for the determined added car. The following two
    sub-commands are executed in the same way as mentioned in the paragraphs above.
    Compositing the outputs of background rendering and foreground rendering agents
    can get the final outputs.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 第二条初始指令是：“修改已添加的车使其向左转。向已添加的车前方添加一辆雪佛兰。向已添加的 Mini 左侧添加另一辆朝我行驶的车。”该指令由项目经理代理人解耦为1：“修改已添加的车使其向左转。”；2：“向已添加的车前方添加一辆雪佛兰。”；3：“向已添加的
    Mini 左侧添加另一辆朝我行驶的车。”第一条子指令分发给车辆运动代理人，该代理人基于命令为已确定的添加车辆生成新的运动。接下来的两条子指令按上述方式执行。将背景渲染和前景渲染代理人的输出合成，可以得到最终的输出。
- en: Appendix D Skydome Lighting Estimation Details
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 天穹照明估算细节
- en: D.1 HDRI dataset
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 HDRI 数据集
- en: We collect 449 high-quality outdoor panorama HDRIs from [Poly Heaven Website](https://polyhaven.com/hdris).
    These HDRIs are all licensed as CC0\. We randomly selected 357 HDRIs for the training
    set and the remaining for the test set. A script for downloading these HDRIs will
    be available.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[Poly Heaven 网站](https://polyhaven.com/hdris)收集了449张高质量的户外全景 HDRI 图像。这些 HDRI
    图像全部采用 CC0 许可。我们随机选择了357张 HDRI 用于训练集，其余用于测试集。用于下载这些 HDRI 的脚本将会提供。
- en: '![Refer to caption](img/6746445f1a48a6913acb28261e915981.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6746445f1a48a6913acb28261e915981.png)'
- en: 'Figure 12: LDR to HDR reconstruction network. We add an explicit spherical
    Gaussian lobe encoded attenuation to overcome the over-smoothness in the decoded
    HDR panorama. It effectively ensures that the sun’s intensity significantly exceeds
    that of surrounding pixels, rendering strong shadow effects for inserted objects.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：从 LDR 到 HDR 的重建网络。我们添加了一个显式的球面高斯瓣编码衰减，以克服解码后的 HDR 全景图中的过度平滑现象。它有效地确保了太阳的强度显著超过周围像素，从而为插入的物体渲染出强烈的阴影效果。
- en: '![Refer to caption](img/2740238a5406b98db9eae7ba38f5b3aa.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2740238a5406b98db9eae7ba38f5b3aa.png)'
- en: 'Figure 13: Reconstructing HDR skydome from multi-camera images. Training on
    HoliCity [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)] dataset.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：从多摄像头图像重建 HDR 天穹。基于 HoliCity 数据集[[79](https://arxiv.org/html/2402.05746v3#bib.bib79)]进行训练。
- en: D.2 LDR to HDR Skydome Reconstruction
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 从 LDR 到 HDR 的天穹重建
- en: In this step, we utilize our HDRI dataset to train an LDR to HDR autoencoder
    with the aim of converting the skydome into a compact feature representation.
    We use the sRGB opto-electronic transfer function (also known as gamma correction)
    to get the LDR sky panorama, and follow  [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)]
    to transform the LDR sky panorama to 3 intermediate vectors, including the sky
    content vector $\mathbf{f}_{\rm content}\in\mathbb{R}^{64}$, the peak direction
    vector $\mathbf{f}_{\rm dir}\in\mathbb{R}^{3}$ and the intensity vector $\mathbf{f}_{\rm
    int}\in\mathbb{R}^{3}_{+}$. In the process of converting intermediate vectors
    into a reconstructed HDR sky panorama, we construct the peak direction map $\mathbf{M}_{\rm
    dir}$, the peak intensity map $\mathbf{M}_{\rm int}$ and the positional encoding
    map $\mathbf{M}_{\rm pe}$.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们利用我们的HDRI数据集训练一个LDR到HDR的自编码器，目的是将天穹转换为紧凑的特征表示。我们使用sRGB光电传递函数（也称为伽马校正）来获取LDR天空全景图，并参考[[68](https://arxiv.org/html/2402.05746v3#bib.bib68)]将LDR天空全景图转换为3个中间向量，包括天空内容向量$\mathbf{f}_{\rm
    content}\in\mathbb{R}^{64}$、峰值方向向量$\mathbf{f}_{\rm dir}\in\mathbb{R}^{3}$和强度向量$\mathbf{f}_{\rm
    int}\in\mathbb{R}^{3}_{+}$。在将中间向量转换为重建的HDR天空全景图的过程中，我们构造了峰值方向图$\mathbf{M}_{\rm
    dir}$、峰值强度图$\mathbf{M}_{\rm int}$和位置信息编码图$\mathbf{M}_{\rm pe}$。
- en: 'Peak direction map ($\mathbf{M}_{\rm dir}$): For each pixel in $\mathbf{M}_{\rm
    dir}$, we calculate the peak direction embedding. This calculation utilizes a
    spherical Gaussian lobe, formulated as $\mathbf{M}_{\rm dir}(\mathbf{u})=e^{100*(\mathbf{u}\cdot\mathbf{f}_{dir}-1)}$,
    where $\mathbf{f}_{\rm dir}$ denotes the peak direction vector. This map is represented
    in $\mathbb{R}^{H\times W\times 1}$.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值方向图（$\mathbf{M}_{\rm dir}$）：对于$\mathbf{M}_{\rm dir}$中的每个像素，我们计算峰值方向嵌入。此计算利用球面高斯叶片，公式为$\mathbf{M}_{\rm
    dir}(\mathbf{u})=e^{100*(\mathbf{u}\cdot\mathbf{f}_{dir}-1)}$，其中$\mathbf{f}_{\rm
    dir}$表示峰值方向向量。该图表示在$\mathbb{R}^{H\times W\times 1}$中。
- en: 'Peak intensity map ($\mathbf{M}_{\rm int}$): Each pixel in this map is determined
    based on its corresponding value in the peak direction map. Specifically, for
    a given direction $\mathbf{u}$, if $\mathbf{M}_{\rm dir}(\mathbf{u})>0.9$, then
    $\mathbf{M}_{\rm int}(\mathbf{u})$ is assigned the value of $\mathbf{f}_{\rm int}$.
    If not, $\mathbf{M}_{\rm int}(\mathbf{u})$ is set to zero. This map is represented
    in $\mathbb{R}_{+}^{H\times W\times 3}$.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 峰值强度图（$\mathbf{M}_{\rm int}$）：该图中的每个像素值是基于其在峰值方向图中的对应值来确定的。具体来说，对于给定方向$\mathbf{u}$，如果$\mathbf{M}_{\rm
    dir}(\mathbf{u})>0.9$，则$\mathbf{M}_{\rm int}(\mathbf{u})$将被赋值为$\mathbf{f}_{\rm
    int}$。否则，$\mathbf{M}_{\rm int}(\mathbf{u})$被设置为零。该图表示在$\mathbb{R}_{+}^{H\times
    W\times 3}$中。
- en: 'Positional encoding map ($\mathbf{M}_{\rm pe}$): This map encodes the direction
    vector of each pixel, determined through equirectangular projection, thus contributing
    to the accurate reconstruction of the HDR sky panorama. It is defined in $\mathbb{R}^{H\times
    W\times 3}$.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 位置信息编码图（$\mathbf{M}_{\rm pe}$）：该图编码了每个像素的方向向量，通过等矩形投影确定，从而有助于准确重建HDR天空全景图。它的定义在$\mathbb{R}^{H\times
    W\times 3}$中。
- en: The input of the decoder $\mathbf{M}_{\rm input}$ is a concatenation of $\mathbf{M}_{\rm
    pe},\mathbf{M}_{\rm dir}$ and $\mathbf{M}_{\rm int}$. We use a 2D UNet to decode
    the concatenated input map to the HDR sky panorama. For sky content vector $\mathbf{f}_{\rm
    content}$, we use an MLP to increase its feature dimension, reshape it to a 2D
    feature map, and concatenate it with the intermediate features at the bottleneck
    of the UNet. This concatenated feature will be further decoded to the HDR sky
    panorama.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的输入$\mathbf{M}_{\rm input}$是$\mathbf{M}_{\rm pe}$、$\mathbf{M}_{\rm dir}$和$\mathbf{M}_{\rm
    int}$的拼接。我们使用2D UNet解码拼接后的输入图像，得到HDR天空全景图。对于天空内容向量$\mathbf{f}_{\rm content}$，我们使用多层感知机（MLP）来增加其特征维度，将其重塑为2D特征图，并将其与UNet瓶颈处的中间特征进行拼接。这个拼接后的特征将进一步解码为HDR天空全景图。
- en: In the context of HDR imaging, the intensity of the peak often exhibits characteristics
    akin to an impulse response, displaying pixel values that are significantly elevated
    by orders of magnitude in comparison to adjacent pixels. This presents a substantial
    challenge for the decoder in accurately recovering these patterns. Thus, we design
    a residual connection to explicitly inject the peak intensity information into
    the final HDR sky panorama. Let $\mathbf{M}_{\rm peak}$ be the product of $\mathbf{M}_{\rm
    dir}$ and $\mathbf{M}_{\rm int}$, representing an attenuation encoded by a spherical
    Gaussian lobe. In our design, we specifically substitute the decoded HDR sky panorama
    at the peak position with $\mathbf{M}_{\rm peak}$. This substitution is applied
    where the value of $\mathbf{M}_{\rm int}(\mathbf{u})$ is non-zero, ensuring that
    the peak position in the HDR sky panorama is accurately represented by $\mathbf{M}_{\rm
    peak}$. This makes a significant difference between us and  [[68](https://arxiv.org/html/2402.05746v3#bib.bib68)].
    Accurate and strong peak intensity can generate very strong shadow effects, resulting
    in better rendering realism. See Figure  [12](https://arxiv.org/html/2402.05746v3#A4.F12
    "Figure 12 ‣ D.1 HDRI dataset ‣ Appendix D Skydome Lighting Estimation Details
    ‣ Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents").
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在HDR成像的背景下，峰值强度通常呈现出类似脉冲响应的特征，显示出与相邻像素相比，像素值被大幅度地提升。对于解码器来说，准确恢复这些模式是一个巨大的挑战。因此，我们设计了一种残差连接，明确地将峰值强度信息注入到最终的HDR天空全景图中。设$\mathbf{M}_{\rm
    peak}$为$\mathbf{M}_{\rm dir}$和$\mathbf{M}_{\rm int}$的乘积，表示由球形高斯叶片编码的衰减。在我们的设计中，我们特别将峰值位置的解码HDR天空全景图替换为$\mathbf{M}_{\rm
    peak}$。这种替换仅在$\mathbf{M}_{\rm int}(\mathbf{u})$的值非零时应用，确保HDR天空全景图中的峰值位置被$\mathbf{M}_{\rm
    peak}$准确表示。这与[[68](https://arxiv.org/html/2402.05746v3#bib.bib68)]有所不同。准确且强烈的峰值强度能够产生非常强的阴影效果，从而实现更好的渲染真实感。请参见图 [12](https://arxiv.org/html/2402.05746v3#A4.F12
    "图12 ‣ D.1 HDRI数据集 ‣ 附录D 天穹光照估计详情 ‣ 通过协同LLM代理的自动驾驶可编辑场景仿真")。
- en: 'To train the LDR to HDR skydome reconstruction, we computer the ground truth
    peak direction $\mathbf{f}_{\rm dir}^{\rm gt}$ and peak intensity $\mathbf{f}_{\rm
    int}^{\rm gt}$ from the HDR ground-truth. During the network training process,
    we employ four losses for supervision. These losses are as follows: peak direction
    loss $L_{\rm dir}$, which measures the L1 angular error of the peak direction
    vectors; peak intensity loss $L_{\rm int}$, which quantifies the log-encoded L2
    error of the peak intensity vectors; HDR reconstruction loss $L_{\rm hdr-recon}$,
    which evaluates the log-encoded L2 error between the reconstructed HDR output
    and the ground truth HDR data; LDR reconstruction loss $L_{\rm ldr-recon}$, which
    is calculated as the L1 error between the input LDR sky panorama and the gamma-corrected
    HDR reconstruction.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练LDR到HDR的天穹重建，我们从HDR的地面真值计算出峰值方向$\mathbf{f}_{\rm dir}^{\rm gt}$和峰值强度$\mathbf{f}_{\rm
    int}^{\rm gt}$。在网络训练过程中，我们使用四种损失函数进行监督。这些损失函数如下：峰值方向损失$L_{\rm dir}$，用于衡量峰值方向向量的L1角度误差；峰值强度损失$L_{\rm
    int}$，用于量化峰值强度向量的对数编码L2误差；HDR重建损失$L_{\rm hdr-recon}$，用于评估重建的HDR输出与地面真值HDR数据之间的对数编码L2误差；LDR重建损失$L_{\rm
    ldr-recon}$，通过计算输入的LDR天空全景图与伽马校正后的HDR重建之间的L1误差来得到。
- en: The total loss is $L_{\rm total}=\lambda_{1}L_{\rm dir}+\lambda_{2}L_{\rm int}+\lambda_{3}L_{\rm
    hdr% -recon}+\lambda_{4}L_{\rm ldr-recon}$, where $\lambda_{1}=1,\lambda_{2}=0.1,\lambda_{3}=2$
    and $\lambda_{4}=0.2$.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 总损失为$L_{\rm total}=\lambda_{1}L_{\rm dir}+\lambda_{2}L_{\rm int}+\lambda_{3}L_{\rm
    hdr-recon}+\lambda_{4}L_{\rm ldr-recon}$，其中$\lambda_{1}=1,\lambda_{2}=0.1,\lambda_{3}=2$和$\lambda_{4}=0.2$。
- en: Data augmentation methods, including rotation, flipping, exposure adjustment
    and white balance adjustment, are implemented to enrich the training data. Noticing
    a strong white balance inaccuracy (the color temperature is too high) in the image
    data from Waymo Open Dataset [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)],
    we augment the HDRI with corresponding white balance adjustment. The blue channel
    is randomly enlarged by 1.2-1.3 times, and the red channel is randomly reduced
    by 1.2-1.3 times.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强方法，包括旋转、翻转、曝光调整和白平衡调整，被用于丰富训练数据。注意到Waymo开放数据集[[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]中的图像数据存在明显的白平衡不准确（色温过高），我们对HDRI进行了相应的白平衡调整增强。蓝色通道被随机放大1.2-1.3倍，红色通道被随机缩小1.2-1.3倍。
- en: D.3 Predict HDR Skydome from Multi-Camera Images
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 从多摄像头图像预测HDR天穹
- en: This step involves estimating skydome lighting from multi-camera images collected
    by the vehicle. The core idea is to estimate intermediate features from multiple
    views and restore the skydome lighting using the well-trained HDR reconstruction
    decoding module. We emphasize the fusion of intermediate features from multiple
    cameras to get a complementary and comprehensive prediction for the skydome lighting.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤涉及从车辆收集的多摄像头图像中估计天穹照明。核心思想是从多个视角估计中间特征，并使用经过良好训练的HDR重建解码模块恢复天穹照明。我们强调融合来自多个摄像头的中间特征，以便为天穹照明提供互补且全面的预测。
- en: 'Multi-camera image data will first go through a shared image encoder to predict
    the peak direction vector $\mathbf{f}_{\rm dir}^{(i)}$, the intensity vector $\mathbf{f}_{\rm
    int}^{(i)}$, and the sky content vector $\mathbf{f}_{\rm content}^{(i)}$ for each
    image $\mathcal{I}^{(i)}$, where $i$ is the camera index. For those vectors from
    $N$ cameras, we fuse all the features in the following strategy:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 多摄像头图像数据首先通过共享的图像编码器，预测每个图像$\mathcal{I}^{(i)}$的峰值方向向量$\mathbf{f}_{\rm dir}^{(i)}$、强度向量$\mathbf{f}_{\rm
    int}^{(i)}$和天空内容向量$\mathbf{f}_{\rm content}^{(i)}$，其中$i$是摄像头索引。对于来自$N$个摄像头的这些向量，我们通过以下策略融合所有特征：
- en: We transform $\mathbf{f}_{\rm dir}^{(i)},i=1,2,...,N$ to the front-facing view
    using their extrinsic parameters and averaged the rotated direction vector to
    $\bar{\mathbf{f}}_{\rm dir}$; we average $\mathbf{f}_{\rm int}^{(i)},i=1,2,...,N$
    to $\bar{\mathbf{f}}_{\rm int}$; we utilize the attention mechanism to fuse sky
    content vectors as $\bar{\mathbf{f}}_{\rm content}=\texttt{Attn(q, k, v)}$, where
    $\texttt{q}=\mathbf{f}^{(0)}_{\rm content}$, $\texttt{k}=\texttt{v}=\texttt{stack}(\{\mathbf{f}^{i}_{\rm
    content}\}_{i=0,1,.% ..,N-1})$. Here index 0 refers to the first (front-facing)
    view image and $\texttt{Attn}(\cdot,\cdot,\cdot)$ the standard attention operator.
    Given $\bar{\mathbf{f}}_{\rm dir}$, $\bar{\mathbf{f}}_{\rm int}$, $\bar{\mathbf{f}}_{\rm
    content}$, we use the pre-trained decoding module from the previous stage to recover
    the fused intermediate vectors to HDR panorama. See Figure  [13](https://arxiv.org/html/2402.05746v3#A4.F13
    "Figure 13 ‣ D.1 HDRI dataset ‣ Appendix D Skydome Lighting Estimation Details
    ‣ Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents").
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用它们的外部参数将$\mathbf{f}_{\rm dir}^{(i)},i=1,2,...,N$转换到前向视图，并将旋转后的方向向量平均为$\bar{\mathbf{f}}_{\rm
    dir}$；我们将$\mathbf{f}_{\rm int}^{(i)},i=1,2,...,N$平均为$\bar{\mathbf{f}}_{\rm int}$；我们利用注意力机制将天空内容向量融合为$\bar{\mathbf{f}}_{\rm
    content}=\texttt{Attn(q, k, v)}$，其中$\texttt{q}=\mathbf{f}^{(0)}_{\rm content}$，$\texttt{k}=\texttt{v}=\texttt{stack}(\{\mathbf{f}^{i}_{\rm
    content}\}_{i=0,1,.% ..,N-1})$。这里索引0表示第一（前向）视图图像，$\texttt{Attn}(\cdot,\cdot,\cdot)$是标准的注意力操作符。给定$\bar{\mathbf{f}}_{\rm
    dir}$、$\bar{\mathbf{f}}_{\rm int}$、$\bar{\mathbf{f}}_{\rm content}$，我们使用前一阶段的预训练解码模块将融合的中间向量恢复为HDR全景图像。见图[13](https://arxiv.org/html/2402.05746v3#A4.F13
    "Figure 13 ‣ D.1 HDRI dataset ‣ Appendix D Skydome Lighting Estimation Details
    ‣ Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")。
- en: Since there is no relevant panoramic data in the autonomous driving dataset
    for supervision, We use HoliCity [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)]
    to simulate multi-camera images. Based on the arrangement and FOV of the three
    forward-facing cameras on the Waymo vehicle [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)],
    we cropped the corresponding image from the HoliCity panorama as the model inputs.
    To supervise the learning of the image encoder, we use the LDR to HDR reconstruction
    network from the previous stage to generate pseudo peak intensity vector GT, peak
    direction vector GT, sky content vector GT, and HDR skydome GT.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在自动驾驶数据集中没有相关的全景数据用于监督学习，我们使用HoliCity [[79](https://arxiv.org/html/2402.05746v3#bib.bib79)]来模拟多摄像头图像。基于Waymo车辆上三台前向摄像头的排列和视场（FOV）[[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]，我们从HoliCity全景图像中裁剪出相应的图像作为模型输入。为了监督图像编码器的学习，我们使用前一阶段的LDR到HDR重建网络生成伪峰值强度向量GT、峰值方向向量GT、天空内容向量GT和HDR天穹GT。
- en: 'We apply five losses to supervise the network during training. These losses
    are as follows: the peak direction loss $L_{\rm dir}$, which measures the L1 angular
    error of the fused peak direction vector; the peak intensity loss $L_{\rm int}$,
    which calculates the log-encoded L2 error of the fused peak intensity vectors;
    the sky content loss $L_{\rm content}$, which evaluates the L1 error of the fused
    sky content vectors; the HDR reconstruction loss $L_{\rm hdr-recon}$ with log-encoded
    L2 error; the LDR reconstruction $L_{\rm ldr-recon}$ with L1 error.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练过程中应用五个损失来监督网络。这些损失如下：峰值方向损失 $L_{\rm dir}$，衡量融合峰值方向向量的 L1 角度误差；峰值强度损失 $L_{\rm
    int}$，计算融合峰值强度向量的对数编码 L2 误差；天空内容损失 $L_{\rm content}$，评估融合天空内容向量的 L1 误差；HDR 重建损失
    $L_{\rm hdr-recon}$，具有对数编码的 L2 误差；LDR 重建损失 $L_{\rm ldr-recon}$，具有 L1 误差。
- en: The total loss is $L_{\rm total}=\lambda_{1}L_{\rm dir}+\lambda_{2}L_{\rm int}+\lambda_{3}L_{\rm
    content% }+\lambda_{4}L_{\rm hdr-recon}+\lambda_{5}L_{\rm ldr-recon}$, where $\lambda_{1}=0.5,\lambda_{2}=0.25,\lambda_{3}=0.005,\lambda_{4}=0.1$
    and $\lambda_{5}=0.2$.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 总损失为 $L_{\rm total}=\lambda_{1}L_{\rm dir}+\lambda_{2}L_{\rm int}+\lambda_{3}L_{\rm
    content% }+\lambda_{4}L_{\rm hdr-recon}+\lambda_{5}L_{\rm ldr-recon}$，其中 $\lambda_{1}=0.5,\lambda_{2}=0.25,\lambda_{3}=0.005,\lambda_{4}=0.1$
    和 $\lambda_{5}=0.2$。
- en: Appendix E 3D Asset Bank
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 3D 资产库
- en: 'To ensure ease of access and modification of 3D assets, we normalize our Blender
    models within their Blender files using the following procedure:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 3D 资产的便捷访问和修改，我们在其 Blender 文件中按照以下程序对 Blender 模型进行归一化处理：
- en: '1.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We ensure that the model has accurate physical dimensions in the unit of meter.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们确保模型具有准确的物理尺寸，单位为米。
- en: '2.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The origin of the car model is set at the middle of the bottom of the car. We
    position the model at the center of the world coordinate system, ensuring that
    the car model’s origin aligns with the origin of the world coordinate system.
    The car is oriented to face the positive direction of the x-axis.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 汽车模型的原点设置在汽车底部的中央。我们将模型定位于世界坐标系的中心，确保汽车模型的原点与世界坐标系的原点对齐。汽车朝向 x 轴的正方向。
- en: '3.'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We uniformly apply the Principled BSDF material to the car body, and name the
    material "car_paint". Prompt that changes the asset’s color will affect the "Base
    Color" attribute of the Principled BSDF node.
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们统一将 Principled BSDF 材质应用于汽车车身，并将材质命名为“car_paint”。改变资产颜色的提示将影响 Principled BSDF
    节点的“Base Color”属性。
- en: '4.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We use the Join operator to merge all meshes into one object.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 Join 操作符将所有网格合并为一个对象。
- en: Following the aforementioned approach, we normalize the Blender models collected
    from the Internet to continuously expand our 3D Asset Bank.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述方法，我们将从互联网收集的 Blender 模型进行归一化处理，以不断扩展我们的 3D 资产库。
- en: Appendix F Blender Rendering Details
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F Blender 渲染细节
- en: We fully implement the Blender rendering workflow using Python scripting, incorporating
    features such as alpha channel, depth channel, and shadow effect, all achieved
    within a single rendering pass.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 Python 脚本完全实现了 Blender 渲染工作流，包含了 alpha 通道、深度通道和阴影效果等功能，所有这些都在一次渲染过程中完成。
- en: '1.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To get a transparent background, we first enable the Render Properties - Film
    - Transparent option.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了获得透明背景，我们首先启用了渲染属性 - 电影 - 透明选项。
- en: '2.'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: To get multiple rendering output, we enable the Combined pass, Z pass and Shadow
    Catcher pass in View Layer Properties panel.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了获得多个渲染输出，我们在视图层属性面板中启用了合成通道、Z 通道和阴影捕捉通道。
- en: '3.'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: To render the shadow, we add a very large plane under the car and enable the
    plane’s Object Properties - Visibility - Mask - Shadow Catcher option.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了渲染阴影，我们在汽车下方添加了一个非常大的平面，并启用了平面的对象属性 - 可见性 - 遮罩 - 阴影捕捉选项。
- en: '4.'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: To obtain scene-related colored shadows, we construct the compositing node graph
    as Figure  [14](https://arxiv.org/html/2402.05746v3#A6.F14 "Figure 14 ‣ item 4
    ‣ Appendix F Blender Rendering Details ‣ Editable Scene Simulation for Autonomous
    Driving via Collaborative LLM-Agents"). This configuration generates the rendered
    image overlaid on the scene image, along with the accompanying depth information
    and mask of the vehicle and its corresponding shadow.
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了获得与场景相关的彩色阴影，我们构建了如图 [14](https://arxiv.org/html/2402.05746v3#A6.F14 "Figure
    14 ‣ item 4 ‣ Appendix F Blender Rendering Details ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") 所示的合成节点图。这一配置生成了叠加在场景图像上的渲染图像，以及附带的深度信息和车辆的遮罩及其对应的阴影。
- en: '![Refer to caption](img/a80ab2d77be5f2b2193cacbe9edbec22.png)'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/a80ab2d77be5f2b2193cacbe9edbec22.png)'
- en: 'Figure 14: Compositing node graph design in Blender [[20](https://arxiv.org/html/2402.05746v3#bib.bib20)]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14：Blender 中的合成节点图设计 [[20](https://arxiv.org/html/2402.05746v3#bib.bib20)]
- en: '5.'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Using depth information and mask, we can handle the occlusion relationship with
    the original objects in the scene. We also added a moderate amount of motion blur
    to the rendered car to match the background.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过使用深度信息和遮罩，我们可以处理与场景中原始物体的遮挡关系。我们还为渲染的汽车添加了适量的运动模糊，以与背景相匹配。
- en: Appendix G Motion Generation Details
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 运动生成详情
- en: 'The vehicle motion agent creates the initial places and subsequent motions
    of vehicles following the requests commands. Existing vehicle motion generation
    methods cannot directly generate motion purely from text and the scene map. Here
    we elaborate on the details of our text-to-motion methods. Our method consists
    of two parts: vehicle placement to generate the starting points and vehicle motion
    planning to generate the subsequent motions.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆运动代理根据请求的命令创建车辆的初始位置和随后的运动。现有的车辆运动生成方法不能仅通过文本和场景地图直接生成运动。在这里，我们详细阐述了我们的文本到运动的方法。我们的方法由两部分组成：车辆放置生成起始点和车辆运动规划生成随后的运动。
- en: '![Refer to caption](img/61948a7517566ed85f4ce354820e4a4d.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/61948a7517566ed85f4ce354820e4a4d.png)'
- en: 'Figure 15: The neighboring area division for vehicle placement.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：车辆放置的邻近区域划分。
- en: G.1 Vehicle Placement
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 车辆放置
- en: We use the language command and the scene map to generate the initial position.
    The scene map $\mathcal{M}$ follows the lane map form $\mathcal{M}=\{\mathbf{n}_{i},i=1,2,\cdots,m\}$,
    where $m$ is the number of lane nodes and the $i$th lane node $\mathbf{n}_{i}=(x_{\rm
    s},y_{\rm s},x_{\rm e},y_{\rm e},c_{\rm type})$ consists of lane starting position
    $(x_{\rm s},y_{\rm s})$, ending position $(x_{\rm e},y_{\rm e})$ and the lane
    type $c_{\rm type}$. The map range is cropped with the range of front 80m, left
    20m and right 20m. Generally, we use the lane map from the ground-truth data.
    If the lane map does not exist, it is applicable to use a lane map estimation
    method like [[44](https://arxiv.org/html/2402.05746v3#bib.bib44), [45](https://arxiv.org/html/2402.05746v3#bib.bib45)]
    to obtain the lane map.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用语言命令和场景地图生成初始位置。场景地图 $\mathcal{M}$ 采用车道地图形式 $\mathcal{M}=\{\mathbf{n}_{i},i=1,2,\cdots,m\}$，其中
    $m$ 是车道节点的数量，第 $i$ 个车道节点 $\mathbf{n}_{i}=(x_{\rm s},y_{\rm s},x_{\rm e},y_{\rm
    e},c_{\rm type})$ 由车道起始位置 $(x_{\rm s},y_{\rm s})$、结束位置 $(x_{\rm e},y_{\rm e})$
    和车道类型 $c_{\rm type}$ 组成。地图范围是从前方 80 米、左侧 20 米和右侧 20 米范围裁剪而来。通常，我们使用来自真实数据的车道地图。如果车道地图不存在，可以使用车道地图估计方法，如
    [[44](https://arxiv.org/html/2402.05746v3#bib.bib44), [45](https://arxiv.org/html/2402.05746v3#bib.bib45)]
    来获取车道地图。
- en: 'Given the language command, the LLM first extracts key placement attributes,
    including vehicle number, distance range, relative direction with the observer
    and direction of driving, and crazy mode. With these attributes, the role function
    of placement begins to find suitable lane nodes from the scene map. Here we assume
    all the placed vehicles are on the centerline of the road. If the distance range
    $(d_{\rm min},d_{\rm max})$ is identified, the role function selects the lane
    centerline nodes according to their distance with the ego location. For the relative
    direction, we divide the ego neighboring area into 6 categories: front, left front,
    right front, left, right, and back, see Figure [15](https://arxiv.org/html/2402.05746v3#A7.F15
    "Figure 15 ‣ Appendix G Motion Generation Details ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") for illustration. For the
    direction of driving, we consider two types: driving close to the ego and driving
    away from the ego, which determines the left/right side of the vehicle on the
    road. The crazy mode, which is designed for non-compliant inverse driving behavior,
    is a bool variable. When it is true, we will inverse the direction of the map
    (swap the starting and ending point of each lane) for that vehicle to represent
    inverse driving. We select the matched lane node set and randomly select one lane
    node from the set. We also consider the conflict of placing vehicles by an iterative
    approach that incoming vehicles should not overlap with the existing vehicles.
    After obtaining lane nodes for every vehicle, we set the midpoint of the lane
    node to be the initial position of a vehicle and the direction of the lane to
    be the initial heading of the vehicle.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 给定语言命令，LLM首先提取关键放置属性，包括车辆编号、距离范围、与观察者的相对方向、行驶方向和疯狂模式。有了这些属性，放置功能开始从场景地图中找到合适的车道节点。这里我们假设所有放置的车辆都位于道路的中心线。如果距离范围$(d_{\rm
    min}, d_{\rm max})$已确定，放置功能根据与自车位置的距离选择车道中心线节点。对于相对方向，我们将自车邻近区域分为6类：前方、左前方、右前方、左侧、右侧和后方，参见图[15](https://arxiv.org/html/2402.05746v3#A7.F15
    "图 15 ‣ 附录 G 运动生成细节 ‣ 协作LLM代理的自动驾驶可编辑场景模拟")进行说明。对于行驶方向，我们考虑两种类型：靠近自车行驶和远离自车行驶，这决定了车辆在道路上的左右侧。疯狂模式是为不符合规范的逆向行驶行为设计的布尔变量。当该值为真时，我们会反转地图的方向（交换每条车道的起点和终点），以表示逆向行驶。我们选择匹配的车道节点集，并从中随机选择一个车道节点。我们还通过迭代方法考虑车辆放置的冲突，即来车不应与已存在的车辆重叠。获取每辆车的车道节点后，我们将车道节点的中点设置为车辆的初始位置，车道的方向设置为车辆的初始航向。
- en: G.2 Vehicle Motion Planning
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 车辆运动规划
- en: 'After obtaining the initial positions, we generate motions in two steps: plan
    the destination and plan the middle trajectory. We first extract movement attributes
    including speed, action, interval and time length. Notably, we divide actions
    into 5 categories: straightforward, turn left, turn right, park, and backward.
    To obtain the destination, if the action category is straightforward or park,
    and backward, we directly calculate a raw destination by assuming the car driving
    following a line with the target speed. Then we find the closest lane node with
    the raw destination to be the final destination. If the action category is turning
    left or turning right, we select a set of nodes whose vertical distance with the
    initial line of heading is in a range (5m-30m) and fit the driving directions
    (the direction of the line should be away from the starting point). We randomly
    pick a lane node to be the destination.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取初始位置后，我们通过两步生成动作：规划目标地点和规划中间轨迹。我们首先提取运动属性，包括速度、动作、间隔和时间长度。值得注意的是，我们将动作分为五类：直行、左转、右转、停车和倒车。为了获取目标地点，如果动作类别是直行、停车或倒车，我们通过假设车辆沿着一条直线行驶并保持目标速度，直接计算出一个初步的目标地点。然后，我们找到与该初步目标地点最近的车道节点作为最终目标地点。如果动作类别是左转或右转，我们选择一组与初始行驶方向垂直距离在（5米-30米）范围内的节点，并拟合行驶方向（该方向应远离起点）。我们随机选择一个车道节点作为目标地点。
- en: To plan the middle trajectory, we use an iterative adjustment approach to make
    the trajectory match with the map and avoid off-road driving. We first use one
    cubic Bezier curve to fit the overall trajectory with the condition of starting
    point, starting direction, ending point and ending direction. The cubic Bezier
    curve is formulated by
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为了规划中间轨迹，我们采用迭代调整方法，使轨迹与地图匹配，并避免越野驾驶。我们首先使用一条三次贝塞尔曲线，在起点、起始方向、终点和终止方向的条件下拟合整体轨迹。三次贝塞尔曲线的公式为
- en: '|  | $\displaystyle B(t)=$ | $\displaystyle(1-t)^{3}P_{0}+3t(1-t)^{2}P_{1}$
    |  | (2) |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle B(t)=$ | $\displaystyle(1-t)^{3}P_{0}+3t(1-t)^{2}P_{1}$
    |  | (2) |'
- en: '|  |  | $\displaystyle+3t^{2}(1-t)P_{2}+t^{3}P_{3},t\in[0,1],$ |  |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+3t^{2}(1-t)P_{2}+t^{3}P_{3},t\in[0,1],$ |  |'
- en: where $P_{0},P_{1},P_{2},P_{3}\in\mathbb{R}^{2}$ is the control points that
    can be solved by given starting point, starting direction, ending point and ending
    direction. Then to avoid off-road driving of the intermediate trajectory, we adjust
    the middle coordinate by replacing it with the closest lane node. We split the
    whole trajectory into two parts with the boundary of the middle coordinate and
    use one cubic Bezier curve to fit each split trajectory. We iteratively repeat
    the process to represent the planned trajectory by multiple cubic Bezier curves.
    Finally, to make the planned trajectory fit with vehicle dynamics, we use a trajectory
    tracking method in [[74](https://arxiv.org/html/2402.05746v3#bib.bib74)] as post-processing
    to revise the planned trajectory^*^**[https://drl-based-trajectory-tracking.readthedocs.io/en/latest/](https://drl-based-trajectory-tracking.readthedocs.io/en/latest/).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $P_{0},P_{1},P_{2},P_{3}\in\mathbb{R}^{2}$ 是控制点，可以通过给定的起点、起始方向、终点和终止方向来求解。为了避免中间轨迹出现越野驾驶，我们通过将中间坐标替换为最近的车道节点来调整中间坐标。我们将整个轨迹分为两部分，以中间坐标为边界，并使用一条三次贝塞尔曲线拟合每一段轨迹。我们通过迭代重复这个过程，使用多条三次贝塞尔曲线表示规划轨迹。最后，为了使规划轨迹与车辆动态匹配，我们在后处理中使用[[74](https://arxiv.org/html/2402.05746v3#bib.bib74)]中提出的轨迹跟踪方法对规划轨迹进行修正^*^**[https://drl-based-trajectory-tracking.readthedocs.io/en/latest/](https://drl-based-trajectory-tracking.readthedocs.io/en/latest/)。
- en: Appendix H Background rendering details
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 背景渲染细节
- en: H.1 Dataset Selection
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 数据集选择
- en: For all Waymo Open Dataset [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]
    experiments, we use images captured from three frontal cameras. The details of
    selection are shown in Table [6](https://arxiv.org/html/2402.05746v3#A8.T6 "Table
    6 ‣ H.1 Dataset Selection ‣ Appendix H Background rendering details ‣ Editable
    Scene Simulation for Autonomous Driving via Collaborative LLM-Agents"). There
    are 120 images in total for each scenerio.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有Waymo Open Dataset [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]
    实验，我们使用从三个前置摄像头捕获的图像。选择的详细信息见表[6](https://arxiv.org/html/2402.05746v3#A8.T6 "Table
    6 ‣ H.1 Dataset Selection ‣ Appendix H Background rendering details ‣ Editable
    Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")。每个场景总共有120张图像。
- en: '| Sequence | Start Frame |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: 序列 | 起始帧 |
- en: '| segment-10247954040621004675_2180_000_2200_000 | 0 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| segment-10247954040621004675_2180_000_2200_000 | 0 |'
- en: '| segment-13469905891836363794_4429_660_4449_660 | 40 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| segment-13469905891836363794_4429_660_4449_660 | 40 |'
- en: '| segment-14333744981238305769_5658_260_5678_260 | 40 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| segment-14333744981238305769_5658_260_5678_260 | 40 |'
- en: '| segment-1172406780360799916_1660_000_1680_000 | 50 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| segment-1172406780360799916_1660_000_1680_000 | 50 |'
- en: '| segment-4058410353286511411_3980_000_4000_000 | 90 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| segment-4058410353286511411_3980_000_4000_000 | 90 |'
- en: '| segment-10061305430875486848_1080_000_1100_000 | 30 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| segment-10061305430875486848_1080_000_1100_000 | 30 |'
- en: '| segment-14869732972903148657_2420_000_2440_000 | 0 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| segment-14869732972903148657_2420_000_2440_000 | 0 |'
- en: '| segment-16646360389507147817_3320_000_3340_000 | 0 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| segment-16646360389507147817_3320_000_3340_000 | 0 |'
- en: '| segment-13238419657658219864_4630_850_4650_850 | 0 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| segment-13238419657658219864_4630_850_4650_850 | 0 |'
- en: '| segment-14424804287031718399_1281_030_1301_030 | 60 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| segment-14424804287031718399_1281_030_1301_030 | 60 |'
- en: '| segment-15270638100874320175_2720_000_2740_000 | 60 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| segment-15270638100874320175_2720_000_2740_000 | 60 |'
- en: '| segment-15349503153813328111_2160_000_2180_000 | 100 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| segment-15349503153813328111_2160_000_2180_000 | 100 |'
- en: '| segment-15868625208244306149_4340_000_4360_000 | 110 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| segment-15868625208244306149_4340_000_4360_000 | 110 |'
- en: '| segment-16608525782988721413_100_000_120_000 | 10 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| segment-16608525782988721413_100_000_120_000 | 10 |'
- en: '| segment-17761959194352517553_5448_420_5468_420 | 0 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| segment-17761959194352517553_5448_420_5468_420 | 0 |'
- en: '| segment-3425716115468765803_977_756_997_756 | 0 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| segment-3425716115468765803_977_756_997_756 | 0 |'
- en: '| segment-3988957004231180266_5566_500_5586_500 | 0 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| segment-3988957004231180266_5566_500_5586_500 | 0 |'
- en: '| segment-9385013624094020582_2547_650_2567_650 | 130 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| segment-9385013624094020582_2547_650_2567_650 | 130 |'
- en: '| segment-8811210064692949185_3066_770_3086_770 | 30 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| segment-8811210064692949185_3066_770_3086_770 | 30 |'
- en: '| segment-10275144660749673822_5755_561_5775_561 | 0 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| segment-10275144660749673822_5755_561_5775_561 | 0 |'
- en: '| segment-10676267326664322837_311_180_331_180 | 100 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| segment-10676267326664322837_311_180_331_180 | 100 |'
- en: '| segment-12879640240483815315_5852_605_5872_605 | 20 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| segment-12879640240483815315_5852_605_5872_605 | 20 |'
- en: '| segment-13142190313715360621_3888_090_3908_090 | 0 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| segment-13142190313715360621_3888_090_3908_090 | 0 |'
- en: '| segment-13196796799137805454_3036_940_3056_940 | 70 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| segment-13196796799137805454_3036_940_3056_940 | 70 |'
- en: '| segment-14348136031422182645_3360_000_3380_000 | 140 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| segment-14348136031422182645_3360_000_3380_000 | 140 |'
- en: '| segment-15365821471737026848_1160_000_1180_000 | 0 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| segment-15365821471737026848_1160_000_1180_000 | 0 |'
- en: '| segment-16470190748368943792_4369_490_4389_490 | 0 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| segment-16470190748368943792_4369_490_4389_490 | 0 |'
- en: '| segment-11379226583756500423_6230_810_6250_810 | 0 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| segment-11379226583756500423_6230_810_6250_810 | 0 |'
- en: '| segment-13085453465864374565_2040_000_2060_000 | 110 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| segment-13085453465864374565_2040_000_2060_000 | 110 |'
- en: '| segment-14004546003548947884_2331_861_2351_861 | 0 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| segment-14004546003548947884_2331_861_2351_861 | 0 |'
- en: '| segment-15221704733958986648_1400_000_1420_000 | 70 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| segment-15221704733958986648_1400_000_1420_000 | 70 |'
- en: '| segment-16345319168590318167_1420_000_1440_000 | 0 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| segment-16345319168590318167_1420_000_1440_000 | 0 |'
- en: 'Table 6: Information on the selected and trimmed Waymo Open Dataset [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)].
    For each sequence, we select 40 frames starting from the Start Frame.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 所选和裁剪的 Waymo Open Dataset 信息 [[61](https://arxiv.org/html/2402.05746v3#bib.bib61)]。对于每个序列，我们选择从起始帧开始的40帧。'
- en: '![Refer to caption](img/1ada43fdb37e010225c3a06e490d3c9b.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ada43fdb37e010225c3a06e490d3c9b.png)'
- en: 'Figure 16: Qualitative ablation of background rendering. (a) McNeRF w/o pose
    alignment.(b) McNeRF w/o exposure. (c) Full McNeRF. Last row: target images.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：背景渲染的定性消融研究。(a) McNeRF 无姿态对齐。(b) McNeRF 无曝光调整。(c) 完整的 McNeRF。最后一行：目标图像。
- en: H.2 Multi-Camera Alignment
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 多摄像头对齐
- en: 'This section will introduce the details of our multi-camera alignment algorithm.
    Let $R_{C_{i},t}$ and $T_{C_{i},t}$ represents the camera $C_{i}$’s extrinsic
    matrix that aligned to vehicle’s coordinates at timestamp $t$. $C_{0}$ is the
    front camera. The superscript $(V)$ and $(M)$ represents the original vehicle’s
    coordinates in autonomous driving dataset and the coordinates under Metashape’s
    unified space. Then the rotation $R_{C_{i},t}$ and translation $T_{C_{i},t}$ can
    be calculated as:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍我们多摄像头对齐算法的详细信息。设$R_{C_{i},t}$和$T_{C_{i},t}$表示摄像头$C_{i}$在时间戳$t$对齐到车辆坐标系的外参矩阵。$C_{0}$为前置摄像头。上标$(V)$和$(M)$分别表示自动驾驶数据集中的原始车辆坐标系和Metashape统一空间下的坐标系。然后，旋转矩阵$R_{C_{i},t}$和平移向量$T_{C_{i},t}$可以通过以下公式计算：
- en: '|  | $\displaystyle R_{C_{i},t}$ | $\displaystyle=R^{(V)}_{C_{0},0}(R^{(M)}_{C_{0},0})^{-1}R^{(M)}_{C_{i},t}$
    |  |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle R_{C_{i},t}$ | $\displaystyle=R^{(V)}_{C_{0},0}(R^{(M)}_{C_{0},0})^{-1}R^{(M)}_{C_{i},t}$
    |  |'
- en: '|  | $\displaystyle T_{C_{i},t}$ | $\displaystyle=\frac{R^{(V)}_{C_{0},0}(R^{(M)}_{C_{0},0})^{-1}(T^{(M)}_{C_{i},t%
    }-T^{(M)}_{C_{0},0})}{S}+T^{(V)}_{C_{0},0},$ |  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle T_{C_{i},t}$ | $\displaystyle=\frac{R^{(V)}_{C_{0},0}(R^{(M)}_{C_{0},0})^{-1}(T^{(M)}_{C_{i},t%
    }-T^{(M)}_{C_{0},0})}{S}+T^{(V)}_{C_{0},0},$ |  |'
- en: where $S=\frac{T^{(M)}_{C_{0},1}-T^{(M)}_{C_{0},0}}{T^{(V)}_{C_{0},1}-T^{(V)}_{C_{0},%
    0}}$ is a scaling factor that ensures the aligned space has the same unit length
    as the real world.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$S=\frac{T^{(M)}_{C_{0},1}-T^{(M)}_{C_{0},0}}{T^{(V)}_{C_{0},1}-T^{(V)}_{C_{0},%
    0}}$是一个缩放因子，确保对齐后的空间与现实世界具有相同的单位长度。
- en: Appendix I Supplymentary Experiments
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 补充实验
- en: I.1 Qualitative Ablation Study of Background Rendering
  id: totrans-414
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.1 背景渲染的定性消融研究
- en: Figure [16](https://arxiv.org/html/2402.05746v3#A8.F16 "Figure 16 ‣ H.1 Dataset
    Selection ‣ Appendix H Background rendering details ‣ Editable Scene Simulation
    for Autonomous Driving via Collaborative LLM-Agents") illustrates the effects
    of the ablation study on background rendering. It is evident that in the absence
    of pose adjustment, the rendered results exhibit significant blur and anomalies.
    Without the intervention of exposure adjustments, there are noticeable changes
    in brightness at the junctions of different cameras, particularly in the sky.
    McNeRF, however, successfully avoids these two issues and achieves the optimal
    rendering outcomes.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图[16](https://arxiv.org/html/2402.05746v3#A8.F16 "Figure 16 ‣ H.1 Dataset Selection
    ‣ Appendix H Background rendering details ‣ Editable Scene Simulation for Autonomous
    Driving via Collaborative LLM-Agents")展示了背景渲染的消融研究效果。显然，在没有姿态调整的情况下，渲染结果表现出显著的模糊和异常。如果没有曝光调整的介入，不同相机的接缝处会出现亮度变化，特别是在天空区域。然而，McNeRF成功避免了这两个问题，达到了最佳渲染效果。
- en: '![Refer to caption](img/1ae83424f119a80e343c5b3f86bc845a.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1ae83424f119a80e343c5b3f86bc845a.png)'
- en: 'Figure 17: Qualitative result of occlusion postprocess and the color control
    for added car.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：遮挡后处理结果和新增车辆的颜色控制定性结果。
- en: I.2 Occlusion with Depth Test
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.2 深度测试中的遮挡
- en: During the process of adding vehicles, there may be instances of occlusion.
    For occlusions among multiple vehicles to be added, Blender considers this issue
    during the rendering process. Therefore, we only need to focus on the occlusion
    between the foreground vehicles and the background objects. The most straightforward
    method to handle occlusion is determined by the depth map of the foreground and
    background, respectively. The depth maps of both the foreground and background
    could be used to choose for each pixel with the lesser depth to be displayed in
    the front, while the one with greater depth is occluded. However, accurately estimating
    the background’s depth map directly is challenging. The point cloud data in autonomous
    driving datasets is too sparse, and the depth maps obtained through depth completion
    are also sparse and excessively noisy, making them unsuitable for pixel-level
    accuracy in practical use. Here, we combine the sparse depth data from point clouds
    with the object segmentation method SAM[[35](https://arxiv.org/html/2402.05746v3#bib.bib35)].
    SAM can achieve pixel-level accuracy in segmentation results at the image level,
    without extra finetuning. We first use SAM to obtain different patches in the
    background image, then identify patches that overlap with the foreground objects.
    Using the sparse depth map derived from the point clouds, we calculate the average
    sparse depth within these patches as the depth of each patch. Since the segmentation
    results of patches often represent a complete instance, and occlusion occurs between
    instances, it is reasonable to calculate the depth for the entire instance represented
    by a patch. Subsequently, we create the background’s depth map from the depths
    of these patches and perform occlusion calculations with the depth map rendered
    for the foreground, presenting each pixel with the lesser depth to finalize the
    occlusion computation. The results of the occlusion calculation, as shown in Fig.
    [17](https://arxiv.org/html/2402.05746v3#A9.F17 "Figure 17 ‣ I.1 Qualitative Ablation
    Study of Background Rendering ‣ Appendix I Supplymentary Experiments ‣ Editable
    Scene Simulation for Autonomous Driving via Collaborative LLM-Agents"), illustrate
    that the added vehicles are occluded by those with shallower depths. This figure
    also displays the adjustment of the added vehicles’ colors.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在添加车辆的过程中，可能会出现遮挡现象。对于多辆车辆之间的遮挡问题，Blender 在渲染过程中会考虑这一问题。因此，我们只需关注前景车辆与背景物体之间的遮挡。处理遮挡的最直接方法是根据前景和背景的深度图来确定。前景和背景的深度图可以用来选择每个像素较小深度的显示在前面，较大深度的则被遮挡。然而，直接准确估计背景的深度图是具有挑战性的。自动驾驶数据集中的点云数据过于稀疏，通过深度补全获得的深度图也很稀疏且噪声过多，无法满足实际应用中的像素级精度。在这里，我们将点云中的稀疏深度数据与物体分割方法SAM[[35](https://arxiv.org/html/2402.05746v3#bib.bib35)]结合。SAM可以在图像级别上实现像素级精度的分割结果，无需额外的微调。我们首先使用SAM获取背景图像中的不同补丁，然后识别与前景物体重叠的补丁。利用从点云推导出的稀疏深度图，我们计算这些补丁内的平均稀疏深度作为每个补丁的深度。由于补丁的分割结果通常表示一个完整的实例，而遮挡发生在实例之间，因此计算代表补丁的整个实例的深度是合理的。随后，我们根据这些补丁的深度创建背景的深度图，并与前景渲染的深度图进行遮挡计算，最终通过展示较小深度的像素来完成遮挡计算。遮挡计算的结果，如图[17](https://arxiv.org/html/2402.05746v3#A9.F17
    "图 17 ‣ I.1 背景渲染的定性消融研究 ‣ 附录 I 补充实验 ‣ 通过协作LLM-代理进行自动驾驶场景仿真")所示，说明新增的车辆被深度较浅的车辆遮挡。该图还展示了新增车辆颜色的调整。
- en: '![Refer to caption](img/04308b2a5e5065b20ed547848395e2a6.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/04308b2a5e5065b20ed547848395e2a6.png)'
- en: 'Figure 18: Qualitative results of rare cases simulation.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：稀有情况模拟的定性结果。
- en: I.3 Rare Cases Simulation
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.3 稀有情况模拟
- en: Leveraging diverse external digital assets, ChatSim can simulate rare and challenging-to-collect
    real-world scenarios within reconstructed existing scenes. Figure [18](https://arxiv.org/html/2402.05746v3#A9.F18
    "Figure 18 ‣ I.2 Occlusion with Depth Test ‣ Appendix I Supplymentary Experiments
    ‣ Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents")
    demonstrates ChatSim’s ability to emulate rare cases by placing uncommon elements
    like bulldozers, isolation piers, fences, excavators, and other infrequently encountered
    situations in reconstructed scenes. This capability enables ChatSim to create
    rare digital twins for existing collected data, thus fulfilling the need for these
    specific scenarios.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 借助多样的外部数字资源，ChatSim 可以在重建的现有场景中模拟稀有且难以收集的真实世界场景。图 [18](https://arxiv.org/html/2402.05746v3#A9.F18
    "图 18 ‣ I.2 深度测试遮挡 ‣ 附录 I 补充实验 ‣ 通过协作的 LLM 代理进行自动驾驶的可编辑场景仿真") 展示了 ChatSim 通过将推土机、隔离码头、围栏、挖掘机等不常见的元素放置在重建的场景中，来模拟稀有情况的能力。这种能力使
    ChatSim 能够为现有的收集数据创建稀有的数字双胞胎，从而满足这些特定场景的需求。
- en: '| Simulation data | AP30 | AP50 | AP70 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 仿真数据 | AP30 | AP50 | AP70 |'
- en: '| 0 | 0.1263 | 0.0366 | 0.0034 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.1263 | 0.0366 | 0.0034 |'
- en: '| 600 | 0.1910 | 0.0878 | 0.0153 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 600 | 0.1910 | 0.0878 | 0.0153 |'
- en: '| 1000 | 0.2074 | 0.0930 | 0.0189 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 0.2074 | 0.0930 | 0.0189 |'
- en: '| 2200 | 0.2064 | 0.0900 | 0.0182 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 2200 | 0.2064 | 0.0900 | 0.0182 |'
- en: 'Table 7: Comparison of detection model’s performance with different number
    of data simulated by ChatSim'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：不同数量数据下，ChatSim 模拟的检测模型性能对比
- en: I.4 Supplementary 3D Detection Augmentation Experiment
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.4 补充 3D 检测增强实验
- en: 'We conducted 3D detection augmentation experiments under a new setting: we
    fixed the real data amount from the original dataset at 4200 frames and augmented
    it with varying quantities of simulation data generated by ChatSim. We continued
    to use Lift-Splat [[54](https://arxiv.org/html/2402.05746v3#bib.bib54)] as the
    detection model, with results shown in Table [7](https://arxiv.org/html/2402.05746v3#A9.T7
    "Table 7 ‣ I.3 Rare Cases Simulation ‣ Appendix I Supplymentary Experiments ‣
    Editable Scene Simulation for Autonomous Driving via Collaborative LLM-Agents").
    It is observed that the use of simulation data significantly enhances the performance
    of the 3D detection task. As the amount of simulation data increases, the final
    performance tends to stabilize after a certain point of improvement.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在一个新的设置下进行了 3D 检测增强实验：我们将原始数据集中的实际数据量固定为 4200 帧，并通过 ChatSim 生成不同数量的仿真数据进行增强。我们继续使用
    Lift-Splat [[54](https://arxiv.org/html/2402.05746v3#bib.bib54)] 作为检测模型，结果如表 [7](https://arxiv.org/html/2402.05746v3#A9.T7
    "表 7 ‣ I.3 稀有案例模拟 ‣ 附录 I 补充实验 ‣ 通过协作的 LLM 代理进行自动驾驶的可编辑场景仿真") 所示。观察到，使用仿真数据显著提升了
    3D 检测任务的性能。随着仿真数据量的增加，最终的性能在一定程度的提升后趋于稳定。
- en: '(FiXme) FiXme Summary: Number of notes: 0,Number of warnings: 0,Number of errors:
    0,Number of fatal errors: 0,Total: 0'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: (FiXme) FiXme 总结：注释数量：0，警告数量：0，错误数量：0，致命错误数量：0，总计：0
