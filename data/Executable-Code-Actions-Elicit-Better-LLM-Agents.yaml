- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:56:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:56:20
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Executable Code Actions Elicit Better LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可执行代码操作引发更好的LLM代理
- en: 来源：[https://arxiv.org/html/2402.01030/](https://arxiv.org/html/2402.01030/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.01030/](https://arxiv.org/html/2402.01030/)
- en: Xingyao Wang    Yangyi Chen    Lifan Yuan    Yizhe Zhang    Yunzhu Li    Hao
    Peng    Heng Ji
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 王星耀    陈扬毅    袁立帆    张一哲    李云珠    彭浩    季恒
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Model (LLM) agents, capable of performing a broad range of actions,
    such as invoking tools and controlling robots, show great potential in tackling
    real-world challenges. LLM agents are typically prompted to produce actions by
    generating JSON or text in a pre-defined format, which is usually limited by constrained
    action space (e.g., the scope of pre-defined tools) and restricted flexibility
    (e.g., inability to compose multiple tools). This work proposes to use executable
    Python code to consolidate LLM agents’ actions into a unified action space (CodeAct).
    Integrated with a Python interpreter, CodeAct can execute code actions and dynamically
    revise prior actions or emit new actions upon new observations through multi-turn
    interactions. Our extensive analysis of 17 LLMs on API-Bank and a newly curated
    benchmark shows that CodeAct outperforms widely used alternatives (up to 20% higher
    success rate). The encouraging performance of CodeAct motivates us to build an
    open-source LLM agent that interacts with environments by executing interpretable
    code and collaborates with users using natural language. To this end, we collect
    an instruction-tuning dataset CodeActInstruct that consists of 7k multi-turn interactions
    using CodeAct. We show that it can be used with existing data to improve models
    in agent-oriented tasks without compromising their general capability. CodeActAgent,
    finetuned from Llama2 and Mistral, is integrated with Python interpreter and uniquely
    tailored to perform sophisticated tasks (e.g., model training) using existing
    libraries and autonomously self-debug¹¹1The code, data, model, and demo are available
    at [https://github.com/xingyaoww/code-act](https://github.com/xingyaoww/code-act)..
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）代理能够执行广泛的操作，如调用工具和控制机器人，显示出在解决现实世界问题方面的巨大潜力。LLM代理通常通过生成JSON或文本格式的预定义格式来触发操作，但这些操作通常受到有限的操作空间（例如，预定义工具的范围）和受限的灵活性（例如，无法组合多个工具）的限制。本研究提出使用可执行的Python代码将LLM代理的操作整合成一个统一的操作空间（CodeAct）。CodeAct与Python解释器集成，可以执行代码操作，并通过多轮交互在新的观察到的情况下动态修改先前的操作或发出新的操作。我们对17个LLM在API-Bank上的广泛分析和一项新创建的基准测试显示，CodeAct的表现优于广泛使用的替代方法（成功率最高提高20%）。CodeAct的令人鼓舞的表现激励我们构建一个开源的LLM代理，该代理通过执行可解释的代码与环境进行交互，并通过自然语言与用户协作。为此，我们收集了一个指令调优数据集CodeActInstruct，其中包含7k个多轮交互，使用CodeAct。我们展示了它可以与现有数据一起使用，改善模型在代理导向任务中的表现，而不会影响其通用能力。CodeActAgent是从Llama2和Mistral微调的，集成了Python解释器，特别定制用于执行复杂任务（例如，模型训练），并使用现有库和自动自我调试¹¹1代码、数据、模型和演示可在[https://github.com/xingyaoww/code-act](https://github.com/xingyaoww/code-act)获取..
- en: Machine Learning, ICML
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/2f7669ab941bdd1ed4177a8dbb7d4cd1.png)![Refer to caption](img/779c8f767bfec1e94b448a5cd754b214.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2f7669ab941bdd1ed4177a8dbb7d4cd1.png)![参考说明](img/779c8f767bfec1e94b448a5cd754b214.png)'
- en: 'Figure 1: Comparison between CodeAct and Text / JSON as action. (top) Illustrative
    example comparing different actions. (bottom) Quantitative results on M³ToolEval
    (§[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3 "2.3 CodeAct Gets More Done
    with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code
    Actions Elicit Better LLM Agents")).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：CodeAct与文本/JSON作为操作的比较。（上图）比较不同操作的示例。（下图）在M³ToolEval上的定量结果（§[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3
    "2.3 CodeAct通过更少的交互完成更多任务 ‣ 2 CodeAct让LLM变得更优秀 ‣ 可执行代码操作引发更好的LLM代理")）。
- en: Large Language Models (LLMs) have emerged as a pivotal breakthrough in natural
    language processing (NLP). When augmented with action modules that allow access
    to APIs, their action space expands beyond conventional text processing, allowing
    LLMs to acquire capabilities such as tool invocation and memory management (Mialon
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib33); Schick et al.,
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib44)) and venture into real-world
    tasks such as controlling robots (Ahn et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib1);
    Huang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib22); Ma et al.,
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib32)) and performing scientific
    experiments (Bran et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib4)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经成为自然语言处理（NLP）领域的一个重要突破。当与允许访问API的行动模块结合时，它们的行动空间超越了传统的文本处理，使LLM能够获得如工具调用和内存管理（Mialon等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib33);
    Schick等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib44)）等能力，并且可以拓展到控制机器人（Ahn等人，[2022](https://arxiv.org/html/2402.01030v4#bib.bib1);
    Huang等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib22); Ma等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib32)）和执行科学实验（Bran等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib4)）等现实世界任务。
- en: 'We inquire: how to effectively expand LLM agents’ action space for solving
    complex real-world problems? Much existing research has examined using text (Yao
    et al., [2022b](https://arxiv.org/html/2402.01030v4#bib.bib67); Park et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib37),
    inter alia) or JSON (Qin et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib43);
    Chase, [2022](https://arxiv.org/html/2402.01030v4#bib.bib6), inter alia) to produce
    actions (e.g., tool uses in Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents")
    top left). However, both methods typically suffer from constrained scope of action
    spaces (actions are usually tailored for specific tasks) and restricted flexibility
    (e.g., inability to compose multiple tools in a single action). As an alternative
    approach, several work (Liang et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib29);
    Singh et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib47); Wang et al.,
    [2023a](https://arxiv.org/html/2402.01030v4#bib.bib52)) demonstrate the potential
    of using LLMs to generate code to control robots or game characters. However,
    they typically rely on pre-specified control primitives and hand-engineered prompts
    and, more importantly, struggle to dynamically adjust or emit actions based on
    new environmental observation and feedback.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出一个问题：如何有效地扩展LLM代理的行动空间，以解决复杂的现实世界问题？许多现有的研究已经探讨了使用文本（Yao等人，[2022b](https://arxiv.org/html/2402.01030v4#bib.bib67);
    Park等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib37)，等）或JSON（Qin等人，[2023b](https://arxiv.org/html/2402.01030v4#bib.bib43);
    Chase，[2022](https://arxiv.org/html/2402.01030v4#bib.bib6)，等）来生成动作（例如图[1](https://arxiv.org/html/2402.01030v4#S1.F1
    "图 1 ‣ 1 引言 ‣ 可执行代码动作激发更好的LLM代理")左上方中的工具使用）。然而，这两种方法通常存在行动空间受限（动作通常是针对特定任务量身定制的）和灵活性不足（例如，无法在单个动作中组合多个工具）的问题。作为替代方法，一些研究（Liang等人，[2022](https://arxiv.org/html/2402.01030v4#bib.bib29);
    Singh等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib47); Wang等人，[2023a](https://arxiv.org/html/2402.01030v4#bib.bib52)）展示了利用LLM生成代码来控制机器人或游戏角色的潜力。然而，这些方法通常依赖于预先定义的控制原语和手工设计的提示，更重要的是，难以根据新的环境观察和反馈动态调整或发出动作。
- en: 'This work proposes CodeAct, a general-purpose framework that allows LLMs to
    generate executable Python code as actions (Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents")
    top right). CodeAct is designed to handle a variety of applications and comes
    with unique advantages:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了CodeAct，一个通用框架，允许LLM生成可执行的Python代码作为行动（图[1](https://arxiv.org/html/2402.01030v4#S1.F1
    "图 1 ‣ 1 引言 ‣ 可执行代码动作激发更好的LLM代理")右上方）。CodeAct旨在处理各种应用，并具有独特的优势：
- en: (1)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Integrated with a Python interpreter, CodeAct can execute code actions and dynamically
    adjust prior actions or emit new action based on observations (e.g., code execution
    results) it receives through multiple turns of interactions.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CodeAct与Python解释器集成，可以执行代码动作，并根据它通过多轮交互接收到的观察（例如，代码执行结果）动态调整先前的动作或发出新的动作。
- en: (2)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2)
- en: Code actions allow LLM to leverage existing software packages. CodeAct can use
    readily available Python packages for an expanded action space instead of hand-crafted
    task-specific tools (Yuan et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib70);
    Shen et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib45)). It also allows
    LLM to use automated feedback (e.g., error messages) implemented in most software
    to improve task-solving by self-debugging its generated code (Chen et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib9);
    Wang et al., [2023d](https://arxiv.org/html/2402.01030v4#bib.bib57)).
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码操作使 LLM 能够利用现有的软件包。CodeAct 可以使用现成的 Python 包来扩展操作空间，而不是依赖手工制作的特定任务工具（Yuan 等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib70);
    Shen 等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib45)）。它还允许 LLM 使用大多数软件中实现的自动反馈（例如：错误信息）来通过自我调试其生成的代码来改善任务求解（Chen
    等人，[2023b](https://arxiv.org/html/2402.01030v4#bib.bib9); Wang 等人，[2023d](https://arxiv.org/html/2402.01030v4#bib.bib57)）。
- en: (3)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3)
- en: Code data is widely used in pre-training today’s LLMs (Yang et al., [2024b](https://arxiv.org/html/2402.01030v4#bib.bib63)).
    These models are already familiar with structured programming languages, allowing
    cost-effective adoption of CodeAct.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码数据在今天的 LLM 预训练中得到了广泛应用（Yang 等人，[2024b](https://arxiv.org/html/2402.01030v4#bib.bib63)）。这些模型已经熟悉结构化编程语言，从而能够经济高效地采用
    CodeAct。
- en: (4)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4)
- en: Compared to JSON and text with a pre-defined format, code inherently supports
    control and data flow, allowing for the storage of intermediate results as variables
    for reuse and the composition of multiple tools to perform complex logical operations
    (e.g., if-statements, for-loops) with one piece of code, thereby unlocking LLMs’
    potential to tackle complex tasks by leveraging its pre-trained knowledge of programming.
    In Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Executable Code Actions Elicit Better LLM Agents"), an LLM using with CodeAct
    (top right) can apply the same sequence of tools (e.g., passing one tool’s output
    as input to another tool using the data flow feature) to all inputs through for-loops
    (i.e., control flow feature) with one action; while text or JSON have to take
    action for every input (top left).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比于具有预定义格式的 JSON 和文本，代码本身就支持控制和数据流，这使得可以将中间结果存储为变量以供重用，并且通过一段代码将多个工具组合起来执行复杂的逻辑操作（例如：if
    语句、for 循环），从而发挥大语言模型（LLM）通过利用其预训练的编程知识来处理复杂任务的潜力。在图 [1](https://arxiv.org/html/2402.01030v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents")
    中，使用 CodeAct 的 LLM（右上方）可以通过 for 循环（即控制流特性）对所有输入应用相同的工具序列（例如：使用数据流特性将一个工具的输出作为输入传递给另一个工具）；而文本或
    JSON 则必须对每个输入单独采取行动（左上方）。
- en: Our extensive experiments with 17 LLMs (including both open-source and proprietary
    ones) confirm the above benefits (3 & 4) of CodeAct. To demonstrate benefit (3),
    our first experiment (§[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2 "2.2 CodeAct
    Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")) compares CodeAct
    to baselines on basic tasks involving atomic tool use (i.e., only one tool is
    used per action), ablating the control and data flow advantage offered by CodeAct.
    The results show that, for most LLMs, CodeAct achieves comparable or better performance
    than the baselines. CodeAct’s performance gains are more prominent on complex
    tasks, as demonstrated in our second experiment (benefit 4). We curate a new benchmark
    consisting of 82 human-curated tasks that typically require multiple calls to
    multiple tools in multi-turn interactions (M³ToolEval; §[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3
    "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")). Problems in this
    benchmark often require intricate coordination and composition of multiple tools.
    With its strengths in control and data flow, CodeAct achieves up to a 20% absolute
    improvement over baselines on the success rate of solving the problems while requiring
    up to 30% fewer actions. These performance gains widen as the capabilities of
    the LLMs increase (Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents") bottom).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在17个大型语言模型（LLM）上的广泛实验（包括开源和专有的）验证了CodeAct所带来的上述好处（3和4）。为了展示好处（3），我们的第一个实验（§[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2
    "2.2 CodeAct显示其作为强大工具使用框架的潜力 ‣ 2 CodeAct使LLM成为更好的代理 ‣ 可执行代码行动能激发更好的LLM代理")）将CodeAct与基准模型进行比较，涉及使用基本任务中的原子工具（即每个动作只使用一个工具），并消除了CodeAct所提供的控制和数据流优势。结果表明，对于大多数LLM，CodeAct实现了与基准模型相当或更好的性能。CodeAct的性能提升在复杂任务中尤为显著，这在我们的第二个实验中得到了展示（好处4）。我们精心策划了一个新的基准，包含82个人工策划的任务，这些任务通常需要在多轮交互中调用多个工具（M³ToolEval；§[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3
    "2.3 CodeAct通过更少的交互完成更多任务 ‣ 2 CodeAct使LLM成为更好的代理 ‣ 可执行代码行动能激发更好的LLM代理")）。该基准中的问题通常需要复杂的多个工具之间的协调与组合。凭借其在控制和数据流方面的优势，CodeAct在解决问题的成功率上相较基准模型提升了最多20%的绝对值，同时需要减少最多30%的操作次数。这些性能提升随着LLM能力的提高而更加显著（见图[1](https://arxiv.org/html/2402.01030v4#S1.F1
    "图1 ‣ 1 引言 ‣ 可执行代码行动能激发更好的LLM代理")底部）。
- en: 'The promising performance of CodeAct motivates an open-source LLM agent that
    can effectively act through CodeAct, and collaborate with humans through natural
    language. To this end, we collect an instruction-tuning dataset CodeActInstruct
    consisting of 7k high-quality multi-turn interaction trajectories with CodeAct
    (§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1 "3.1 CodeActInstruct: Agent-Environment
    Interactions ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable
    Code Actions Elicit Better LLM Agents")). CodeActInstruct is motivated by a general
    agent framework consisting of agent, user, and environments (Fig. [2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")) and focuses on agent-environment interactions with the computer
    (information seeking, software package use, external memory) and the physical
    world (robot planning). On CodeActInstruct, we perform careful data selection
    to promote the capability of improving from multi-turn interaction (e.g., self-debug).
    We show that CodeActInstruct can be used with commonly used instruction tuning
    data to improve the models’ performance in agent tasks without compromising their
    general capabilities (e.g., knowledge-based QA, coding, instruction following,
    §[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")). Our model, dubbed CodeActAgent, is finetuned from LLaMA-2
    (Touvron et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib51)) and Mistral-7B
    (Jiang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib23)) and improves
    on out-of-domain agent tasks with not only CodeAct, but also text action in a
    pre-defined format (§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2 "3.2 CodeActAgent
    ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable Code
    Actions Elicit Better LLM Agents")).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeAct的出色表现激发了一个开源LLM代理的研究，该代理可以通过CodeAct有效地行动，并通过自然语言与人类协作。为此，我们收集了一个名为CodeActInstruct的指令调优数据集，包含了7k个高质量的多轮交互轨迹，均与CodeAct相关（§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")）。CodeActInstruct的设计灵感来源于一个通用的代理框架，该框架由代理、用户和环境组成（图[2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")），并专注于代理与计算机（信息查找、软件包使用、外部存储）及物理世界（机器人规划）之间的交互。在CodeActInstruct中，我们进行了精心的数据选择，以促进多轮交互（例如自我调试）的能力提升。我们展示了CodeActInstruct可以与常用的指令调优数据结合使用，提升模型在代理任务中的表现，而不影响它们在通用任务中的能力（例如，基于知识的问答、编码、指令跟随，§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")）。我们的模型，名为CodeActAgent，是基于LLaMA-2（Touvron
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib51)）和Mistral-7B（Jiang
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib23)）进行微调的，不仅通过CodeAct，还通过预定义格式的文本动作，在领域外的代理任务中表现出色（§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")）。'
- en: CodeAct can further benefit from multi-turn interactions and existing software
    (benefit 1 & 2, §[2.4](https://arxiv.org/html/2402.01030v4#S2.SS4 "2.4 CodeAct
    Benefits from Multi-turn Interactions and Existing Software Packages ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")).
    As shown in Fig. [3](https://arxiv.org/html/2402.01030v4#S2.F3 "Figure 3 ‣ 2.3
    CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents
    ‣ Executable Code Actions Elicit Better LLM Agents"), CodeActAgent, designed for
    seamless integration with Python, can carry out sophisticated tasks (e.g., model
    training, data visualization) using existing Python packages. Error messages from
    the environment further enable it to rectify errors autonomously through self-debugging
    in multi-turn interaction. Thanks to LLM’s extensive programming knowledge acquired
    during pre-training, these are achieved without needing in-context demonstrations,
    reducing the human efforts for adapting CodeActAgent to different tasks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CodeAct还可以从多轮交互和现有软件中受益（收益1和2，§[2.4](https://arxiv.org/html/2402.01030v4#S2.SS4
    "2.4 CodeAct Benefits from Multi-turn Interactions and Existing Software Packages
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents")）。如图[3](https://arxiv.org/html/2402.01030v4#S2.F3 "Figure 3 ‣ 2.3 CodeAct
    Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣
    Executable Code Actions Elicit Better LLM Agents")所示，CodeActAgent旨在与Python无缝集成，能够使用现有的Python包执行复杂任务（例如，模型训练、数据可视化）。环境中的错误信息使其能够通过多轮交互中的自我调试自动纠正错误。得益于LLM在预训练过程中获得的广泛编程知识，这些都可以在无需上下文示范的情况下实现，从而减少了将CodeActAgent适应不同任务时所需的人力。
- en: 'Table 1: The benefit of CodeAct compared to using Text/JSON for LLM action.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：CodeAct与使用文本/JSON进行LLM动作的比较优势。
- en: '|  | CodeAct for LLM action | JSON or Text for LLM action |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | CodeAct用于LLM动作 | JSON或文本用于LLM动作 |'
- en: '| Availability of Data | ✔Large quantity of code available¹ for pre-training
    | ✗Data curation required for particular format |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 数据的可用性 | ✔大量代码可用于预训练¹ | ✗需要对特定格式进行数据整理 |'
- en: '| Complex Operation (e.g., looping, composition of multiple tools) | ✔Natively
    supported via control and data flow | ✗Requires careful engineering if feasible
    (e.g., define new tools to mimic if-statement) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 复杂操作（例如，循环、多个工具的组合） | ✔通过控制和数据流原生支持 | ✗如果可行，则需要精心设计（例如，定义新工具来模拟if语句） |'
- en: '| Availability of Tools | ✔Can directly use existing software packages² | ✗Requires
    human effort to curate tools from scratch or existing software |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 工具的可用性 | ✔可以直接使用现有的软件包² | ✗需要人工从零开始整理工具或从现有软件中整理工具 |'
- en: '| Automated Feedback | ✔Feedback mechanism³ (e.g., traceback) is already implemented
    as an infrastructure for most programming languages | ✗Requires human effort to
    provide feedback or re-route feedback from the underlying programming language
    used to implement the tools |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 自动化反馈 | ✔反馈机制³（例如，回溯）已作为大多数编程语言的基础设施实现 | ✗需要人工提供反馈或重新路由反馈，以便从实现工具所用的底层编程语言获取反馈
    |'
- en: '1'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Including code demonstrating useful behaviors for LLM agents (e.g., task decomposition,
    coordination of multiple function calls to different tools).
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 包括展示LLM代理有用行为的代码（例如，任务分解、多个功能调用的协调）。
- en: '2'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: Human-written Python packages covering a wide range of applications are available
    on [https://pypi.org/](https://pypi.org/).
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人工编写的Python包涵盖了广泛的应用，且可以在[https://pypi.org/](https://pypi.org/)上找到。
- en: '3'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3'
- en: For example, in Python, errors and exceptions ([https://docs.python.org/3/tutorial/errors.html](https://docs.python.org/3/tutorial/errors.html))
    are available. Most software provides error messages in natural language to help
    human programmers debug their code. CodeAct enables LLM to use them directly.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，在Python中，可以使用错误和异常（[https://docs.python.org/3/tutorial/errors.html](https://docs.python.org/3/tutorial/errors.html)）。大多数软件会提供自然语言的错误信息，帮助程序员调试代码。CodeAct使LLM能够直接使用这些错误信息。
- en: 2 CodeAct Makes LLMs Better Agents
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 CodeAct使LLM成为更好的代理
- en: 'In this section, we first describe CodeAct framework (§[2.1](https://arxiv.org/html/2402.01030v4#S2.SS1
    "2.1 What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents")) and provide empirical evidence that supports the choice
    of CodeAct. We focus on Python as the programming language for CodeAct due to
    its popularity (ranked top-1 at (TIOBE Index, [2024](https://arxiv.org/html/2402.01030v4#bib.bib50)))
    and numerous open-source packages. We aim to answer several research questions
    (RQs) using 17 off-the-shelf LLMs. In §[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2
    "2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"), we examine
    RQ1: Does LLMs’ familiarity with code due to a large amount of code pre-training
    data bring CodeAct advantages over text and JSON? We discuss RQ2 in §[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3
    "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents"): Does CodeAct benefit
    from Python’s innate control and data flow feature in complex problems? Finally,
    as an additional benefit, we discuss how using CodeAct further enhances LLM agents
    by enabling multi-turn interactions and allowing them to access existing software
    in §[2.4](https://arxiv.org/html/2402.01030v4#S2.SS4 "2.4 CodeAct Benefits from
    Multi-turn Interactions and Existing Software Packages ‣ 2 CodeAct Makes LLMs
    Better Agents ‣ Executable Code Actions Elicit Better LLM Agents") and Fig. [3](https://arxiv.org/html/2402.01030v4#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述CodeAct框架（§[2.1](https://arxiv.org/html/2402.01030v4#S2.SS1 "2.1
    What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents")）并提供支持选择CodeAct的实证证据。由于Python语言的流行（在(TIOBE Index，[2024](https://arxiv.org/html/2402.01030v4#bib.bib50))排名第一）以及众多开源包的存在，我们将Python作为CodeAct的编程语言。我们使用17个现成的LLMs来回答若干研究问题（RQs）。在§[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2
    "2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")中，我们研究RQ1：由于大量的代码预训练数据，LLMs对代码的熟悉是否能让CodeAct在文本和JSON上具有优势？我们在§[2.3](https://arxiv.org/html/2402.01030v4#S2.SS3
    "2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")中讨论RQ2：CodeAct是否从Python固有的控制和数据流特性中受益，特别是在复杂问题中？最后，作为额外的好处，我们讨论如何通过在§[2.4](https://arxiv.org/html/2402.01030v4#S2.SS4
    "2.4 CodeAct Benefits from Multi-turn Interactions and Existing Software Packages
    ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM
    Agents")和图[3](https://arxiv.org/html/2402.01030v4#S2.F3 "Figure 3 ‣ 2.3 CodeAct
    Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣
    Executable Code Actions Elicit Better LLM Agents")中，使LLM代理能够进行多轮交互并访问现有软件，从而进一步增强LLM代理。
- en: '![Refer to caption](img/57caacfcb7ab855a18472b7d9d66eb73.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/57caacfcb7ab855a18472b7d9d66eb73.png)'
- en: 'Figure 2: General agent multi-turn interaction framework that describes the
    role of CodeAct and motivates the construction of our data mixture. CodeActInstruct
    focuses on the agent-environment interactions and specifically filters for the
    self-improved planning behavior, while general conversation data we include focuses
    on agent-user interaction (§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1 "3.1
    CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source LLM
    Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents")).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：描述CodeAct角色的通用代理多轮交互框架，并推动我们数据混合物的构建。CodeActInstruct专注于代理-环境交互，特别是筛选自我改进的规划行为，而我们包括的一般对话数据则专注于代理-用户交互（§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")）。'
- en: 2.1 What is CodeAct?
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 什么是CodeAct？
- en: 'In Fig. [2](https://arxiv.org/html/2402.01030v4#S2.F2 "Figure 2 ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"),
    we first introduce a general multi-turn interaction framework for LLM agents’
    real-world usage that considers three roles (Yang et al., [2024c](https://arxiv.org/html/2402.01030v4#bib.bib65)):
    agent, user, and environment. We define interaction as the information exchange
    between the agent and an external entity (user or environment). For each turn
    of interaction, the agent receives an observation (input) either from the user
    (e.g., natural language instruction) or the environment (e.g., code execution
    result), optionally planning for its action through chain-of-thought (Wei et al.,
    [2022](https://arxiv.org/html/2402.01030v4#bib.bib60)), and emits an action (output)
    to either user in natural language or the environment. CodeAct employs Python
    code to consolidate all actions for agent-environment interaction. In CodeAct,
    each emitted action to the environment is a piece of Python code, and the agent
    will receive outputs of code execution (e.g., results, errors) as observation.
    We include an example prompt of CodeAct in §[E](https://arxiv.org/html/2402.01030v4#A5
    "Appendix E Example Prompt for CodeAct ‣ Executable Code Actions Elicit Better
    LLM Agents").'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[2](https://arxiv.org/html/2402.01030v4#S2.F2 "Figure 2 ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")中，我们首先介绍了一种通用的多轮交互框架，用于LLM代理的实际应用，考虑了三种角色（杨等人，[2024c](https://arxiv.org/html/2402.01030v4#bib.bib65)）：代理、用户和环境。我们将交互定义为代理与外部实体（用户或环境）之间的信息交换。在每轮交互中，代理会接收来自用户（例如自然语言指令）或环境（例如代码执行结果）的观察（输入），并可选择通过连锁思维（Wei等人，[2022](https://arxiv.org/html/2402.01030v4#bib.bib60)）进行行动规划，随后向用户或环境发出一个行动（输出）。CodeAct使用Python代码来巩固代理与环境的交互中的所有行动。在CodeAct中，每个发出的环境行动都是一段Python代码，代理将接收代码执行的输出（例如结果、错误）作为观察。我们在§[E](https://arxiv.org/html/2402.01030v4#A5
    "Appendix E Example Prompt for CodeAct ‣ Executable Code Actions Elicit Better
    LLM Agents")中提供了CodeAct的一个示例提示。
- en: 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 CodeAct展示了作为强大工具使用框架的潜力
- en: In this section, we perform a controlled experiment to understand which format
    (text, JSON, CodeAct) is more likely to lead an LLM to generate correct atomic
    tool calls. The performance in this experiment reflects LLM’s familiarity with
    the corresponding format. We hypothesize that using CodeAct to call tools is a
    more natural way to use tools for the models, which typically have extensive exposure
    to code data during their training.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们进行了一项受控实验，以了解哪种格式（文本、JSON、CodeAct）更有可能促使LLM生成正确的原子工具调用。该实验中的表现反映了LLM对相应格式的熟悉程度。我们假设，使用CodeAct调用工具是模型使用工具的更自然方式，因为模型在训练过程中通常会广泛接触到代码数据。
- en: Setup. We re-purpose API-Bank (Li et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib27))
    and test LLMs’ API-calling performance, comparing CodeAct, JSON, and text actions.
    For each evaluation instance, we instruct LLM to generate one atomic tool call
    in the format of a Python function call, JSON object, or text expression in a
    pre-defined format. A concrete example is shown in Tab. [A.6](https://arxiv.org/html/2402.01030v4#A0.T6
    "Table A.6 ‣ Executable Code Actions Elicit Better LLM Agents"). We use API-Bank’s
    level-1 instructions and the provided toolset. To evaluate API-calling, we follow
    their correctness metric, matching the ground-truth API outputs with the actual
    model-generated API’s execution outputs.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们重新使用了API-Bank（Li等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib27)），并测试了LLM的API调用性能，比较了CodeAct、JSON和文本行动。在每个评估实例中，我们指示LLM生成一个原子工具调用，格式为Python函数调用、JSON对象或预定义格式的文本表达式。一个具体示例如Tab.[A.6](https://arxiv.org/html/2402.01030v4#A0.T6
    "Table A.6 ‣ Executable Code Actions Elicit Better LLM Agents")中所示。我们使用API-Bank的1级指令和提供的工具集。为了评估API调用，我们遵循其正确性指标，将真实API输出与模型生成的API执行输出进行匹配。
- en: Results. We present results in Tab. [3](https://arxiv.org/html/2402.01030v4#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents").
    For most LLMs, CodeAct achieves comparable or better performance even in atomic
    actions (the simplistic tool use scenario) where its control and data flow strengths
    are ablated. Compared to closed-source LLMs, CodeAct’s improvements are more prominent
    in open-source models. Furthermore, code data is usually more accessible for fine-tuning
    open-source LLMs than the specialized JSON or text tool-calling format. Although
    JSON is consistently weaker than other approaches for open-source models, it achieves
    decent performance with closed-source LLMs, indicating that these closed-source
    models may have gone through targeted fine-tuning toward their JSON capabilities.
    These results suggest optimizing for CodeAct is a better route for open-source
    LLMs than alternatives to improve their tool-use capabilities, as they already
    show good initial CodeAct capability due to extensive exposure to code data during
    pre-training.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结果。我们在表格[3](https://arxiv.org/html/2402.01030v4#S2.T3 "Table 3 ‣ 2.2 CodeAct
    Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes LLMs Better
    Agents ‣ Executable Code Actions Elicit Better LLM Agents")中展示了结果。对于大多数LLM，CodeAct在原子操作（简化的工具使用场景）中表现出与其他方法相当或更好的性能，即使在该场景下其控制和数据流的优势被削弱。与闭源LLM相比，CodeAct在开源模型中的改进更为突出。此外，代码数据通常比专门的JSON或文本工具调用格式更容易获取，这对于开源LLM的微调至关重要。尽管JSON在开源模型中始终表现较弱，但它在闭源LLM中取得了不错的表现，这表明这些闭源模型可能已经针对其JSON能力进行了特定的微调。这些结果表明，针对CodeAct进行优化，作为提升开源LLM工具使用能力的途径，比其他替代方案更为有效，因为在预训练期间它们已广泛接触到代码数据，从而展现出较好的CodeAct能力。
- en: 'Table 2: Atomic API call correctness on API-Bank. The best performance is bolded,
    and the second-best is underlined.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：API-Bank上的原子API调用正确性。最佳性能用粗体标出，第二好的结果用下划线表示。
- en: Correctness (%, $\uparrow$) Format of Action CodeAct JSON Text Open-source LLMs
    CodeLlama-7b-Instruct-hf $12.5$ $12.0$ $\mathbf{17.0}$ CodeLlama-13b-Instruct-hf
    $11.8$ $7.8$ $\mathbf{14.0}$ CodeLlama-34b-Instruct-hf $\mathbf{17.3}$ $12.0$
    $16.8$ Llama-2-7b-chat-hf $\mathbf{28.8}$ $11.3$ $25.8$ Llama-2-13b-chat-hf $\mathbf{38.1}$
    $8.5$ $37.3$ Llama-2-70b-chat-hf $35.6$ $14.3$ $\mathbf{37.6}$ Mistral-7B-Instruct-v0.1
    $2.5$ $2.3$ $\mathbf{3.0}$ lemur-70b-chat-v1 $\mathbf{58.6}$ $46.6$ $56.1$ Closed-source
    LLMs claude-2 $\mathbf{76.7}$ $59.4$ $73.7$ claude-instant-1 $\mathbf{75.2}$ $64.9$
    $73.2$ gemini-pro $70.4$ $\mathbf{73.2}$ $71.2$ gpt-3.5-turbo-0613 $\mathbf{74.4}$
    $73.9$ $73.4$ gpt-3.5-turbo-1106 $75.4$ $\mathbf{78.4}$ $73.4$ gpt-4-0613 $75.4$
    $\mathbf{82.0}$ $74.4$ gpt-4-1106-preview $76.7$ $\mathbf{82.7}$ $73.4$ text-davinci-002
    $\mathbf{69.2}$ $59.6$ $57.4$ text-davinci-003 $75.4$ $\mathbf{76.9}$ $69.7$ Frequency
    of Best-Performing Format $\uparrow$ Open-source $\mathbf{4}$ $0$ $4$ Closed-source
    $4$ $\mathbf{5}$ $0$ Overall $\mathbf{8}$ $\underline{5}$ $4$
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 正确性（%，$\uparrow$） 操作格式 CodeAct JSON 文本 开源LLM CodeLlama-7b-Instruct-hf $12.5$
    $12.0$ $\mathbf{17.0}$ CodeLlama-13b-Instruct-hf $11.8$ $7.8$ $\mathbf{14.0}$
    CodeLlama-34b-Instruct-hf $\mathbf{17.3}$ $12.0$ $16.8$ Llama-2-7b-chat-hf $\mathbf{28.8}$
    $11.3$ $25.8$ Llama-2-13b-chat-hf $\mathbf{38.1}$ $8.5$ $37.3$ Llama-2-70b-chat-hf
    $35.6$ $14.3$ $\mathbf{37.6}$ Mistral-7B-Instruct-v0.1 $2.5$ $2.3$ $\mathbf{3.0}$
    lemur-70b-chat-v1 $\mathbf{58.6}$ $46.6$ $56.1$ 闭源LLM claude-2 $\mathbf{76.7}$
    $59.4$ $73.7$ claude-instant-1 $\mathbf{75.2}$ $64.9$ $73.2$ gemini-pro $70.4$
    $\mathbf{73.2}$ $71.2$ gpt-3.5-turbo-0613 $\mathbf{74.4}$ $73.9$ $73.4$ gpt-3.5-turbo-1106
    $75.4$ $\mathbf{78.4}$ $73.4$ gpt-4-0613 $75.4$ $\mathbf{82.0}$ $74.4$ gpt-4-1106-preview
    $76.7$ $\mathbf{82.7}$ $73.4$ text-davinci-002 $\mathbf{69.2}$ $59.6$ $57.4$ text-davinci-003
    $75.4$ $\mathbf{76.9}$ $69.7$ 最佳表现格式的频率 $\uparrow$ 开源 $\mathbf{4}$ $0$ $4$ 闭源
    $4$ $\mathbf{5}$ $0$ 总体 $\mathbf{8}$ $\underline{5}$ $4$
- en: 'Table 3: Success rates (higher the better) and average turns required per instance
    (lower the better) on M³ToolEval. The best results for each model are bolded,
    and the second-best ones are underlined.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：M³ToolEval上的成功率（越高越好）和每个实例所需的平均回合数（越少越好）。每个模型的最佳结果用粗体标出，第二好的结果用下划线表示。
- en: Success Rate (%, $\uparrow$) Avg. Turns ($\downarrow$) Format of Action CodeAct
    JSON Text CodeAct JSON Text Open-source LLMs CodeLlama-7b-Instruct-hf $\mathbf{4.9}$
    $2.4$ $2.4$ $\mathbf{9.7}$ $9.9$ $9.9$ CodeLlama-13b-Instruct-hf $\mathbf{4.9}$
    $\mathbf{4.9}$ $\mathbf{4.9}$ $9.8$ $9.8$ $\mathbf{9.7}$ CodeLlama-34b-Instruct-hf
    $\mathbf{2.4}$ $0.0$ $0.0$ $\mathbf{9.9}$ $10.0$ $10.0$ Llama-2-7b-chat-hf $0.0$
    $1.2$ $\mathbf{2.4}$ $\mathbf{8.9}$ $9.5$ $9.6$ Llama-2-13b-chat-hf $\mathbf{0.0}$
    $\mathbf{0.0}$ $\mathbf{0.0}$ $\mathbf{9.7}$ $10.0$ $10.0$ Llama-2-70b-chat-hf
    $\mathbf{11.0}$ $3.7$ $3.7$ $\mathbf{9.1}$ $9.8$ $9.8$ Mistral-7B-Instruct-v0.1
    $0.0$ $\mathbf{3.7}$ $1.2$ $10.0$ $\mathbf{9.8}$ $9.9$ lemur-70b-chat-v1 $13.4$
    $\mathbf{15.9}$ $12.2$ $\mathbf{9.1}$ $9.3$ $9.4$ Closed-source LLMs claude-2
    $\mathbf{54.9}$ $39.0$ $29.3$ $\mathbf{7.2}$ $8.3$ $8.5$ claude-instant-1 $20.7$
    $\mathbf{31.7}$ $24.4$ $8.8$ $\mathbf{8.6}$ $8.9$ gemini-pro $\mathbf{22.0}$ $19.5$
    $11.0$ $\mathbf{8.8}$ $9.1$ $9.5$ gpt-3.5-turbo-0613 $\mathbf{51.2}$ $26.8$ $20.7$
    $\mathbf{7.0}$ $8.8$ $9.2$ gpt-3.5-turbo-1106 $\mathbf{29.3}$ $15.9$ $14.6$ $\mathbf{8.4}$
    $9.0$ $9.0$ gpt-4-0613 $\mathbf{67.1}$ $56.1$ $45.1$ $\mathbf{6.6}$ $7.6$ $8.0$
    gpt-4-1106-preview $\mathbf{74.4}$ $52.4$ $53.7$ $\mathbf{5.5}$ $7.6$ $7.7$ text-davinci-002
    $4.9$ $4.9$ $\mathbf{8.5}$ $9.7$ $9.8$ $\mathbf{9.6}$ text-davinci-003 $\mathbf{20.7}$
    $18.3$ $7.3$ $9.2$ $\mathbf{9.0}$ $9.6$ Frequency of Best-performing Format $\uparrow$
    Open-source $\mathbf{5}$ $4$ $3$ $\mathbf{6}$ $1$ $1$ Closed-source $\mathbf{7}$
    $1$ $1$ $\mathbf{6}$ $2$ $1$ Overall $\mathbf{12}$ 5 $4$ $\mathbf{12}$ $3$ $2$
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 成功率（%，$\uparrow$） 平均轮次（$\downarrow$） 动作格式 CodeAct JSON 文本 CodeAct JSON 文本 开源LLMs
    CodeLlama-7b-Instruct-hf $\mathbf{4.9}$ $2.4$ $2.4$ $\mathbf{9.7}$ $9.9$ $9.9$
    CodeLlama-13b-Instruct-hf $\mathbf{4.9}$ $\mathbf{4.9}$ $\mathbf{4.9}$ $9.8$ $9.8$
    $\mathbf{9.7}$ CodeLlama-34b-Instruct-hf $\mathbf{2.4}$ $0.0$ $0.0$ $\mathbf{9.9}$
    $10.0$ $10.0$ Llama-2-7b-chat-hf $0.0$ $1.2$ $\mathbf{2.4}$ $\mathbf{8.9}$ $9.5$
    $9.6$ Llama-2-13b-chat-hf $\mathbf{0.0}$ $\mathbf{0.0}$ $\mathbf{0.0}$ $\mathbf{9.7}$
    $10.0$ $10.0$ Llama-2-70b-chat-hf $\mathbf{11.0}$ $3.7$ $3.7$ $\mathbf{9.1}$ $9.8$
    $9.8$ Mistral-7B-Instruct-v0.1 $0.0$ $\mathbf{3.7}$ $1.2$ $10.0$ $\mathbf{9.8}$
    $9.9$ lemur-70b-chat-v1 $13.4$ $\mathbf{15.9}$ $12.2$ $\mathbf{9.1}$ $9.3$ $9.4$
    闭源LLMs claude-2 $\mathbf{54.9}$ $39.0$ $29.3$ $\mathbf{7.2}$ $8.3$ $8.5$ claude-instant-1
    $20.7$ $\mathbf{31.7}$ $24.4$ $8.8$ $\mathbf{8.6}$ $8.9$ gemini-pro $\mathbf{22.0}$
    $19.5$ $11.0$ $\mathbf{8.8}$ $9.1$ $9.5$ gpt-3.5-turbo-0613 $\mathbf{51.2}$ $26.8$
    $20.7$ $\mathbf{7.0}$ $8.8$ $9.2$ gpt-3.5-turbo-1106 $\mathbf{29.3}$ $15.9$ $14.6$
    $\mathbf{8.4}$ $9.0$ $9.0$ gpt-4-0613 $\mathbf{67.1}$ $56.1$ $45.1$ $\mathbf{6.6}$
    $7.6$ $8.0$ gpt-4-1106-preview $\mathbf{74.4}$ $52.4$ $53.7$ $\mathbf{5.5}$ $7.6$
    $7.7$ text-davinci-002 $4.9$ $4.9$ $\mathbf{8.5}$ $9.7$ $9.8$ $\mathbf{9.6}$ text-davinci-003
    $\mathbf{20.7}$ $18.3$ $7.3$ $9.2$ $\mathbf{9.0}$ $9.6$ 最佳格式频率 $\uparrow$ 开源 $\mathbf{5}$
    $4$ $3$ $\mathbf{6}$ $1$ $1$ 闭源 $\mathbf{7}$ $1$ $1$ $\mathbf{6}$ $2$ $1$ 总体 $\mathbf{12}$
    5 $4$ $\mathbf{12}$ $3$ $2$
- en: 2.3 CodeAct Gets More Done with Fewer Interactions
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 CodeAct通过更少的交互完成更多任务
- en: In this section, we investigate whether LLM agents can benefit from the control
    and data flow of code on problems that require complex patterns of tool use.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨LLM代理是否能从代码的控制和数据流中受益，解决那些需要复杂工具使用模式的问题。
- en: M³ToolEval. As shown in Tab. [A.7](https://arxiv.org/html/2402.01030v4#A0.T7
    "Table A.7 ‣ Executable Code Actions Elicit Better LLM Agents"), to the best of
    our knowledge, no existing tool-use benchmarks contain complex tasks requiring
    the composition of multiple tools while supporting evaluating different action
    formats. Hence, we curate a benchmark M³ToolEval to fill this gap, which evaluates
    LLMs’ capabilities in solving complex tasks that typically require multiple calls
    to multiple tools in multi-turn interactions. It contains 82 human-curated instances,
    spanning tasks including web browsing, finance, travel itinerary planning, science,
    and information processing. Each domain is accompanied by a unique set of manually
    crafted tools. We intentionally keep the prompt simple (examples in §[F](https://arxiv.org/html/2402.01030v4#A6
    "Appendix F M3ToolEval Prompt ‣ Executable Code Actions Elicit Better LLM Agents"))
    and avoid providing any demonstration to test the LLM’s zero-shot ability to use
    tools, similar to how a novice user without knowledge of few-shot prompting would
    use the model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: M³ToolEval。如表[A.7](https://arxiv.org/html/2402.01030v4#A0.T7 "Table A.7 ‣ Executable
    Code Actions Elicit Better LLM Agents")所示，据我们所知，现有的工具使用基准测试中没有包含需要多工具组合的复杂任务，同时支持评估不同的动作格式。因此，我们策划了一个基准测试M³ToolEval，以填补这一空白，评估LLM在解决通常需要多次调用多个工具的多轮互动中的复杂任务的能力。它包含82个由人类策划的实例，涵盖包括网页浏览、金融、旅行行程规划、科学和信息处理等任务。每个领域都配有一组独特的手工制作工具。我们刻意保持提示简单（参见§[F](https://arxiv.org/html/2402.01030v4#A6
    "Appendix F M3ToolEval Prompt ‣ Executable Code Actions Elicit Better LLM Agents")中的示例），并避免提供任何示范，测试LLM的零-shot能力来使用工具，类似于没有少样本提示知识的初学者使用模型的方式。
- en: Setup. We allow the model to generate fully functional Python code that enables
    control and data flow (e.g., if-statement, for-loop). We follow the action format
    for JSON and text described in Tab. [A.6](https://arxiv.org/html/2402.01030v4#A0.T6
    "Table A.6 ‣ Executable Code Actions Elicit Better LLM Agents"). Within each turn,
    the model can either emit an action or propose an answer to be verified by an
    exact match with the ground-truth solution. The interaction will terminate when
    a maximum of 10 interaction turns are reached or a correct solution has been submitted,
    similar to (Wang et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们允许模型生成完全可用的 Python 代码，能够实现控制和数据流（例如，如果语句、for 循环）。我们遵循表格[A.6](https://arxiv.org/html/2402.01030v4#A0.T6
    "表 A.6 ‣ 可执行代码操作更好地引导 LLM 代理")中描述的 JSON 和文本的操作格式。在每一轮中，模型可以发出一个操作或提出一个答案，该答案将通过与真实解决方案的完全匹配来验证。交互将在达到最多
    10 轮交互或提交正确解决方案时终止，类似于（Wang 等人，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)）。
- en: 'Metric. We measure the success rate by calculating the percentage of the model
    proposed answers that match the ground-truth solutions. We also include the avg.
    turns metric: the average number of turns on all evaluated instances.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们通过计算模型提出的答案与真实解决方案匹配的百分比来衡量成功率。我们还包括平均轮数指标：在所有评估实例上的平均轮数。
- en: Quantitative Results on M³ToolEval. We include full results in Tab. [3](https://arxiv.org/html/2402.01030v4#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")
    and a subset of results for visualization in Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Executable Code Actions Elicit Better LLM Agents").
    CodeAct generally has a higher task success rate (12 out of 17 evaluated LLMs),
    similar to the trend in §[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2 "2.2
    CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct Makes LLMs
    Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"). Moreover,
    using CodeAct requires a lower average number of turns (12 out of 17 evaluated
    LLMs). For example, the best model gpt-4-1106-preview achieves a $20.7$% absolute
    improvement compared to the next best action format (text) while requiring $2.1$
    fewer interaction turns on average. However, there is still a significant gap
    in terms of absolute CodeAct performance between open- and closed-source LLMs
    as the best open-source model achieving 13.4% while the best closed-source model
    gpt-4-1106-preview 74.4%. This is potentially due to open-source models’ weak
    task-solving capability and inability to follow complex instructions without demonstration,
    suggesting an urgent need to improve open-source LLMs for practical, real-world
    tasks under the zero-shot setting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: M³ToolEval 的定量结果。完整结果见表[3](https://arxiv.org/html/2402.01030v4#S2.T3 "表 3 ‣
    2.2 CodeAct 展现作为强大工具使用框架的潜力 ‣ 2 CodeAct 使 LLMs 成为更好的代理 ‣ 可执行代码操作更好地引导 LLM 代理")，可视化结果的子集见图[1](https://arxiv.org/html/2402.01030v4#S1.F1
    "图 1 ‣ 1 引言 ‣ 可执行代码操作更好地引导 LLM 代理")。总体来看，CodeAct 的任务成功率较高（在 17 个评估的 LLM 中有 12
    个成功），与 §[2.2](https://arxiv.org/html/2402.01030v4#S2.SS2 "2.2 CodeAct 展现作为强大工具使用框架的潜力
    ‣ 2 CodeAct 使 LLMs 成为更好的代理 ‣ 可执行代码操作更好地引导 LLM 代理") 中的趋势相似。此外，使用 CodeAct 需要的平均轮数较少（在
    17 个评估的 LLM 中有 12 个）。例如，最佳模型 gpt-4-1106-preview 相比于次优的行动格式（文本）取得了 $20.7$% 的绝对提升，同时平均减少了
    $2.1$ 轮交互。然而，在开源和闭源 LLM 之间，CodeAct 性能的绝对差距仍然显著，最佳的开源模型仅达到了 13.4%，而最佳的闭源模型 gpt-4-1106-preview
    达到了 74.4%。这可能是由于开源模型在任务解决能力上的不足，且无法在没有演示的情况下遵循复杂的指令，表明在零-shot 设置下，迫切需要改进开源 LLM
    以应对实际的、现实世界中的任务。
- en: '![Refer to caption](img/3d95d65cdf7388c837860ab272ceeaed.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3d95d65cdf7388c837860ab272ceeaed.png)'
- en: 'Figure 3: Example multi-turn interaction with Python packages using CodeActAgent
    (Mistral-7b). No in-context demonstrations are provided to the model. Some messages
    are omitted for space. See [https://chat.xwang.dev/r/Vqn108G](https://chat.xwang.dev/r/Vqn108G)
    for complete interaction.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：使用 CodeActAgent（Mistral-7b）与 Python 包的示例多轮交互。模型未提供上下文演示。一些消息已被省略以节省空间。完整的交互请参见[https://chat.xwang.dev/r/Vqn108G](https://chat.xwang.dev/r/Vqn108G)。
- en: 2.4 CodeAct Benefits from Multi-turn Interactions and Existing Software Packages
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 CodeAct 从多轮交互和现有软件包中获益
- en: In Fig. [3](https://arxiv.org/html/2402.01030v4#S2.F3 "Figure 3 ‣ 2.3 CodeAct
    Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣
    Executable Code Actions Elicit Better LLM Agents"), we show how an LLM agent can
    integrate with Python (i.e., CodeActAgent we trained in §[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")) and use existing software
    to perform complex tasks in multi-turn interactions. Thanks to its extensive knowledge
    of Python learned during pre-training, the LLM agent can automatically import
    the correct Python libraries to solve tasks without requiring user-provided tools
    or demonstrations. As illustrated in Fig. [3](https://arxiv.org/html/2402.01030v4#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents"), CodeActAgent
    can use Pandas to download and process tabular data, use Scikit-Learn for machine
    learning train-test data split and regression model training, and use Matplotlib
    for data visualization. Furthermore, using the interactive Python interpreter
    for code execution allows automated error messages that help the LLM agent ‘self-debug’
    their actions in a multi-turn interaction and eventually complete the human user’s
    request correctly.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[3](https://arxiv.org/html/2402.01030v4#S2.F3 "Figure 3 ‣ 2.3 CodeAct Gets
    More Done with Fewer Interactions ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable
    Code Actions Elicit Better LLM Agents")中，我们展示了一个LLM代理如何与Python集成（即我们在§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")中训练的CodeActAgent），并利用现有软件在多轮交互中执行复杂任务。得益于在预训练过程中学习的丰富Python知识，LLM代理能够自动导入正确的Python库来解决任务，而无需用户提供工具或演示。如图[3](https://arxiv.org/html/2402.01030v4#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")所示，CodeActAgent可以使用Pandas下载和处理表格数据，使用Scikit-Learn进行机器学习的训练测试数据拆分和回归模型训练，并使用Matplotlib进行数据可视化。此外，使用交互式Python解释器执行代码能够生成自动化错误消息，帮助LLM代理在多轮交互中“自我调试”其操作，并最终正确完成用户的请求。
- en: 3 Empowering Open-source LLM Agent to be Better at CodeAct
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 赋能开源LLM代理更好地进行CodeAct
- en: 'The promising results achieved by CodeAct motivate us to build an open-source
    LLM agent that can both interact with environments through CodeAct and communicate
    with humans using language. To improve open-source LLMs’ CodeAct capability, in
    §[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1 "3.1 CodeActInstruct: Agent-Environment
    Interactions ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct ‣ Executable
    Code Actions Elicit Better LLM Agents"), we introduce CodeActInstruct, an instruction
    finetuning dataset that contains agent-environment interaction trajectories. We
    discuss data selection procedures in §[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents") to promote improvement from interaction behavior. Additionally, we show
    that CodeAct can be used together with existing agent-user conversation data (§[4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")) to balance the dialog capability of the resulting LLM. Our
    model CodeActAgent, finetuned from LLaMA-2 (Touvron et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib51))
    and Mistral-7B (Jiang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib23))
    on a mixture of CodeActInstruct and general conversations, improves CodeAct performances
    without hurting LLM’s general performance on a diverse suite of tasks (§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeAct取得的有希望的结果激励我们构建一个开源LLM代理，既可以通过CodeAct与环境互动，又能通过语言与人类沟通。为了提升开源LLMs的CodeAct能力，在§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")中，我们介绍了CodeActInstruct，一个包含代理-环境互动轨迹的指令微调数据集。我们在§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")中讨论了数据选择过程，以促进通过互动行为的改进。此外，我们展示了CodeAct可以与现有的代理-用户对话数据（§[4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")）一起使用，从而平衡最终LLM的对话能力。我们的模型CodeActAgent，基于LLaMA-2（Touvron等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib51)）和Mistral-7B（Jiang等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib23)）在CodeActInstruct和一般对话的混合数据集上进行微调，提升了CodeAct性能，同时不会影响LLM在各种任务上的整体表现（§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")）。'
- en: '3.1 CodeActInstruct: Agent-Environment Interactions'
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 CodeActInstruct：代理-环境交互
- en: 'We consider four main use cases in agent-environment interaction and repurpose
    five existing datasets across different domains to generate trajectories:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了代理-环境交互中的四个主要使用案例，并重新利用了来自不同领域的五个现有数据集来生成轨迹：
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Information Seeking: We use a training subset of HotpotQA (Yang et al., [2018](https://arxiv.org/html/2402.01030v4#bib.bib64))
    to generate information-seeking trajectories, where LLMs use the wikipedia_search
    API (provided as a Python function) to search for information to answer questions.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息搜索：我们使用HotpotQA（Yang等人，[2018](https://arxiv.org/html/2402.01030v4#bib.bib64)）的训练子集来生成信息搜索轨迹，在该轨迹中，LLMs使用wikipedia_search
    API（作为Python函数提供）搜索信息以回答问题。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Software Package (Tool) Usage: We use the training set of code generation problems
    in APPS (Hendrycks et al., [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18))
    and math problems in MATH (Hendrycks et al., [2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)).
    The code generation tasks already involve importing packages and/or creating new
    tools by defining a new Python function. For MATH, we provide an in-context demonstration
    of importing Python packages (e.g., sympy for symbolic math) for problem-solving.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件包（工具）使用：我们使用APPS中的代码生成问题训练集（Hendrycks等人，[2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)）和MATH中的数学问题训练集（Hendrycks等人，[2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)）。代码生成任务已涉及导入包和/或通过定义新的Python函数来创建新工具。对于MATH，我们提供了一个上下文示范，展示如何导入Python包（例如，用于符号数学的sympy）来解决问题。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'External Memory: We repurpose the training subset of WikiTableQuestion (Pasupat
    & Liang, [2015](https://arxiv.org/html/2402.01030v4#bib.bib38)) and tweak it into
    two variants of tabular reasoning tasks that require accessing external memory:
    (1) SQL-based, requiring the LLM to interact with an SQL database through sqlite3
    package to answer the question via SQL execution; (2) Pandas-based, requiring
    the model to interact with pandas tables to perform data operations (e.g., select,
    filter). Examples of instructions can be found in §[G.3.1](https://arxiv.org/html/2402.01030v4#A7.SS3.SSS1
    "G.3.1 Tabular Reasoning (WikiTableQuestion) ‣ G.3 Example of Trajectory Generation
    Prompt ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit
    Better LLM Agents").'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部记忆：我们将WikiTableQuestion的训练子集（Pasupat & Liang，[2015](https://arxiv.org/html/2402.01030v4#bib.bib38)）重新利用，并将其调整为需要访问外部记忆的两种表格推理任务的变体：（1）基于SQL的，要求LLM通过sqlite3包与SQL数据库进行交互，通过SQL执行来回答问题；（2）基于Pandas的，要求模型与pandas表格进行交互，以执行数据操作（例如，选择、过滤）。指令示例可以在§[G.3.1](https://arxiv.org/html/2402.01030v4#A7.SS3.SSS1
    "G.3.1 Tabular Reasoning (WikiTableQuestion) ‣ G.3 Example of Trajectory Generation
    Prompt ‣ Appendix G CodeAct Interaction Data ‣ Executable Code Actions Elicit
    Better LLM Agents")中找到。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Robot Planning: We use ALFWorld (Shridhar et al., [2020](https://arxiv.org/html/2402.01030v4#bib.bib46)),
    a text-only embodied environment simulator, to generate trajectories that use
    robot-control APIs (repurposed as Python function) to complete household tasks.
    Following MINT (Wang et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)),
    we provide an in-context demonstration to encourage the use of for-loop and if-statement
    code blocks to automate repetitive operations (e.g., searching for items by visiting
    different locations).'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机器人规划：我们使用ALFWorld（Shridhar等人，[2020](https://arxiv.org/html/2402.01030v4#bib.bib46)），一个仅限文本的具身环境模拟器，通过机器人控制API（作为Python函数重新利用）生成轨迹，来完成家庭任务。沿用MINT（Wang等人，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)），我们提供了上下文示范，鼓励使用for循环和if语句代码块来自动化重复操作（例如，通过访问不同位置搜索物品）。
- en: Data Down-sampling. We down-sample each dataset by keeping only the most challenging
    instances, aiming to make trajectory generation more efficient and cost-effective.
    Furthermore, it also helps remove simple instances that existing LLMs can already
    solve. The statistics of the filtered dataset can be found in Tab. [A.9](https://arxiv.org/html/2402.01030v4#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents"). Please refer to §[G.1](https://arxiv.org/html/2402.01030v4#A7.SS1
    "G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable Code
    Actions Elicit Better LLM Agents") for details about the down-sample process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 数据下采样：我们通过仅保留最具挑战性的实例来对每个数据集进行下采样，旨在提高轨迹生成的效率和成本效益。此外，这也有助于去除现有LLM已经能够解决的简单实例。筛选后数据集的统计信息可以在表[A.9](https://arxiv.org/html/2402.01030v4#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents")中找到。有关下采样过程的详细信息，请参阅§[G.1](https://arxiv.org/html/2402.01030v4#A7.SS1
    "G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable Code
    Actions Elicit Better LLM Agents")。
- en: Repurpose Data for Multi-turn Interaction. Some datasets (APPS, MATH, WikiTableQuestions)
    are initially single-turn problems that expect one solution per instruction, whereas,
    in a realistic agent use case, we often require multi-turn interaction to complete
    each task (Fig. [1](https://arxiv.org/html/2402.01030v4#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Executable Code Actions Elicit Better LLM Agents") top). Following MINT (Wang
    et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)), we repurpose
    single-turn problems into multi-turn ones by allowing LLM to interact with the
    environment for multiple turns before it decides to submit one solution for evaluation.
    Specifically for code generation problems, we provide an in-context example to
    guide LLMs to test their solution on provided test cases before they submit the
    solution. Metrics from the original data will evaluate the submitted solution
    to determine its correctness. We include examples in §[G.3](https://arxiv.org/html/2402.01030v4#A7.SS3
    "G.3 Example of Trajectory Generation Prompt ‣ Appendix G CodeAct Interaction
    Data ‣ Executable Code Actions Elicit Better LLM Agents").
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为多轮交互重用数据。一些数据集（APPS、MATH、WikiTableQuestions）最初是单轮问题，每个指令期望一个解决方案，而在现实的智能体使用场景中，我们通常需要多轮交互才能完成每个任务（见图
    [1](https://arxiv.org/html/2402.01030v4#S1.F1 "图 1 ‣ 1 引言 ‣ 可执行代码操作激发更好的 LLM 智能体")
    上方）。根据 MINT（Wang 等，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)）的方法，我们通过允许
    LLM 与环境进行多轮交互，随后提交一个解决方案进行评估，从而将单轮问题转化为多轮问题。针对代码生成问题，我们提供一个上下文示例，指导 LLM 在提交解决方案前，先在提供的测试用例上测试其解决方案。原始数据的指标将评估提交的解决方案，以确定其正确性。我们在
    §[G.3](https://arxiv.org/html/2402.01030v4#A7.SS3 "G.3 轨迹生成提示示例 ‣ 附录 G 代码交互数据
    ‣ 可执行代码操作激发更好的 LLM 智能体") 中包含了示例。
- en: Trajectory Generation. We use MINT’s evaluation framework (Wang et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58))
    to generate interaction trajectories for the aforementioned datasets and determine
    the correctness of each trajectory. We run gpt-3.5-turbo-0613 from OpenAI, claude-1-instant
    and claude-2 from Anthropic on down-sampled data, except code generation, which
    we use a longer-context version of GPT-3.5 (gpt-3.5-turbo-0613-16k) due to the
    long-context requirement of the self-debugging process. On a subset of problems
    that none of these models can solve, we use gpt-4-0613 to generate trajectories.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹生成。我们使用 MINT 的评估框架（Wang 等，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)）为上述数据集生成交互轨迹，并确定每个轨迹的正确性。我们在下采样的数据上运行
    OpenAI 的 gpt-3.5-turbo-0613、Anthropic 的 claude-1-instant 和 claude-2，除代码生成外，对于自我调试过程中对长上下文的需求，我们使用
    GPT-3.5 的长上下文版本（gpt-3.5-turbo-0613-16k）。对于这些模型都无法解决的部分问题，我们使用 gpt-4-0613 来生成轨迹。
- en: 'Enhancing Agent’s Capabilities of Improving from Interaction. We select a high-quality
    subset of all the generated trajectories from CodeActInstruct to promote the agent’s
    ability to improve the next action based on prior observations (e.g., self-debugging
    from code execution error message, a planning capability in Fig. [2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")). To achieve this, we selectively preserve those trajectories
    wherein the model initially encounters errors but rectifies these inaccuracies
    in later interactions. For these instances, the LLM typically engages in self-reflection
    following the initial error, thereby proactively enhancing its future actions.
    Other filtering details are discussed in §[G.2](https://arxiv.org/html/2402.01030v4#A7.SS2
    "G.2 Data Selection Heuristic ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents"). On all trajectories generated, we keep
    411 trajectories from gpt-4-0613 and 6728 trajectories from gpt-3.5 and claude.
    The statistics of the resulting dataset CodeActInstruct are shown in Tab. [4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '提升智能体的互动改进能力。我们从CodeActInstruct生成的所有轨迹中选择一个高质量的子集，以促进智能体基于先前观察（例如，代码执行错误信息的自我调试，图[2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")中的规划能力）改进下一步行动。为此，我们选择性地保留那些模型初次遇到错误但在后续互动中纠正这些不准确的轨迹。在这些实例中，LLM通常会在初始错误后进行自我反思，从而主动提升其未来的行动。其他筛选细节请参见§[G.2](https://arxiv.org/html/2402.01030v4#A7.SS2
    "G.2 Data Selection Heuristic ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents")。在所有生成的轨迹中，我们保留了来自gpt-4-0613的411个轨迹，以及来自gpt-3.5和Claude的6728个轨迹。结果数据集CodeActInstruct的统计数据如Tab.[4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")所示。'
- en: 'Table 4: Statistics of our training mixture and comparison with prior work.
    Please refer to §[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1 "3.1 CodeActInstruct:
    Agent-Environment Interactions ‣ 3 Empowering Open-source LLM Agent to be Better
    at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents") for details about
    CodeActInstruct and general conversation data. Token statistics are computed using
    Llama-2 tokenizer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '表4：我们训练数据混合的统计数据，并与先前的工作进行比较。有关CodeActInstruct和一般对话数据的详细信息，请参见§[3.1](https://arxiv.org/html/2402.01030v4#S3.SS1
    "3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")。'
- en: '| Data Mixture | Data Type | Data Name | # of Data Instances | # of Total Tokens
    | Avg. Tokens Per Instance |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 数据混合 | 数据类型 | 数据名称 | 数据实例数 | 总词汇数 | 平均每实例词汇数 |'
- en: '| Prior Work | - | FireAct (Chen et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7))
    | $2,063$ | $542,176$ | $262.81$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 先前工作 | - | FireAct（Chen等，[2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)）
    | $2,063$ | $542,176$ | $262.81$ |'
- en: '| - | AgentInstruct (Zeng et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71))
    | $1,866$ | $2,517,785$ | $1349.30$ |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| - | AgentInstruct（Zeng等，[2023](https://arxiv.org/html/2402.01030v4#bib.bib71)）
    | $1,866$ | $2,517,785$ | $1349.30$ |'
- en: '| CodeActInstruct (Ours) | Information Seeking | HotpotQA (Yang et al., [2018](https://arxiv.org/html/2402.01030v4#bib.bib64))
    | $1,664$ | $2,472,227$ | $1485.71$ |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| CodeActInstruct（我们的） | 信息寻求 | HotpotQA（Yang等，[2018](https://arxiv.org/html/2402.01030v4#bib.bib64)）
    | $1,664$ | $2,472,227$ | $1485.71$ |'
- en: '| Software Packages (Tool) | MATH (Math, (Hendrycks et al., [2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)))
    | $1,732$ | $1,719,467$ | $992.76$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 软件包（工具） | MATH（数学，（Hendrycks等，[2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)））
    | $1,732$ | $1,719,467$ | $992.76$ |'
- en: '| Software Packages (Tool) | APPS (Code, (Hendrycks et al., [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)))
    | $647$ | $1,235,472$ | $1909.54$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 软件包（工具） | APPS（代码，（Hendrycks等，[2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)））
    | $647$ | $1,235,472$ | $1909.54$ |'
- en: '| External Memory | WikiTableQuestion (Pasupat & Liang, [2015](https://arxiv.org/html/2402.01030v4#bib.bib38))
    | $1,065$ | $1,316,246$ | $1235.91$ |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 外部记忆 | WikiTableQuestion（Pasupat & Liang，[2015](https://arxiv.org/html/2402.01030v4#bib.bib38)）
    | $1,065$ | $1,316,246$ | $1235.91$ |'
- en: '| Robot Planning | ALFWorld (Shridhar et al., [2020](https://arxiv.org/html/2402.01030v4#bib.bib46))
    | $2,031$ | $3,838,269$ | $1889.84$ |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 机器人规划 | ALFWorld（Shridhar等，[2020](https://arxiv.org/html/2402.01030v4#bib.bib46)）
    | $2,031$ | $3,838,269$ | $1889.84$ |'
- en: '| Total | $\mathbf{7,139}$ | $\mathbf{10,581,681}$ | $\mathbf{1482.24}$ |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | $\mathbf{7,139}$ | $\mathbf{10,581,681}$ | $\mathbf{1482.24}$ |'
- en: '| General Conversation | Single-Turn Reasoning | OpenOrca (Sub-sampled, (Lian
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib28))) | $50,000$ | $14,034,152$
    | $280.68$ |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 一般对话 | 单轮推理 | OpenOrca (下采样, (Lian等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib28)))
    | $50,000$ | $14,034,152$ | $280.68$ |'
- en: '| Multi-Turn Conversations | ShareGPT (Sub-sampled, (Anonymous, [2023](https://arxiv.org/html/2402.01030v4#bib.bib2)))
    | $10,000$ | $17,933,861$ | $1793.39$ |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 多轮对话 | ShareGPT (下采样, (匿名，[2023](https://arxiv.org/html/2402.01030v4#bib.bib2)))
    | $10,000$ | $17,933,861$ | $1793.39$ |'
- en: '| Multi-Turn Conversations | ShareGPT (GPT-4, (OpenChat, [2023](https://arxiv.org/html/2402.01030v4#bib.bib35)))
    | $4,583$ | $18,195,878$ | $3970.30$ |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 多轮对话 | ShareGPT (GPT-4, (OpenChat, [2023](https://arxiv.org/html/2402.01030v4#bib.bib35)))
    | $4,583$ | $18,195,878$ | $3970.30$ |'
- en: '| Multi-turn Reasoning | CapyBara (LDJnr, [2023](https://arxiv.org/html/2402.01030v4#bib.bib25))
    | $4,647$ | $4,982,435$ | $1072.18$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 多轮推理 | CapyBara (LDJnr, [2023](https://arxiv.org/html/2402.01030v4#bib.bib25))
    | $4,647$ | $4,982,435$ | $1072.18$ |'
- en: '| Total | $\mathbf{69,230}$ | $\mathbf{55,146,326}$ | $\mathbf{796.57}$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | $\mathbf{69,230}$ | $\mathbf{55,146,326}$ | $\mathbf{796.57}$ |'
- en: 'Comparing CodeActInstruct with Prior Work. Compared with prior work AgentInstruct
    (Zeng et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71)) and FireAct
    (Chen et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)) that mainly
    focus using text as action, CodeActInstruct results in models that are more practical
    in real-world implementation, as such models using CodeAct can directly interact
    with Python interpreters and open-source toolkits (Fig. [3](https://arxiv.org/html/2402.01030v4#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")), reducing
    the development effort for action parsing and tool creations. CodeActInstruct
    is systematically constructed following the general agent framework (Fig. [2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")). It covers diverse domains (e.g., compared to FireAct that
    only considers QA-task and search API), contains quality data (e.g., promotes
    agent’s capability of self-debug) and of larger size (3.8x / 3.5x more data trajectories
    and 5x / 19x more tokens compared to AgentInstruct / FireAct respectively in Tab. [4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")). As we empirically show in Tab. [5](https://arxiv.org/html/2402.01030v4#S3.T5
    "Table 5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents"), the resulting model (same backbone) of CodeActInstruct achieves
    24% and 119% relative improvement compared to AgentInstruct and FireAct.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '与先前工作进行比较。与主要使用文本作为动作的先前工作AgentInstruct（Zeng等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib71)）和FireAct（Chen等人，[2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)）相比，CodeActInstruct的结果使得模型在实际应用中更为实用，因为使用CodeAct的模型可以直接与Python解释器和开源工具包进行交互（图[3](https://arxiv.org/html/2402.01030v4#S2.F3
    "Figure 3 ‣ 2.3 CodeAct Gets More Done with Fewer Interactions ‣ 2 CodeAct Makes
    LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")），减少了对动作解析和工具创建的开发工作。CodeActInstruct是按照通用代理框架系统性构建的（图[2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")）。它涵盖了多个领域（例如，与仅考虑QA任务和搜索API的FireAct相比），包含高质量数据（例如，提升了代理的自我调试能力），并且规模更大（与AgentInstruct
    / FireAct相比，数据轨迹分别多出3.8倍 / 3.5倍，tokens数量多出5倍 / 19倍，参见表[4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")）。正如我们在表[5](https://arxiv.org/html/2402.01030v4#S3.T5 "Table
    5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents")中通过实验证明，CodeActInstruct的结果模型（相同的主干）相比AgentInstruct和FireAct分别取得了24%和119%的相对提升。'
- en: 'CodeActInstruct Can Be Used With Existing Agent-User Conversation Data. We
    use a sub-sampled set of OpenOrca (Lian et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib28))
    that focuses on single-turn chain-of-thought (CoT) reasoning, ShareGPT (Anonymous,
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib2); OpenChat, [2023](https://arxiv.org/html/2402.01030v4#bib.bib35))
    from two sources that contain multi-turn conversations between human and LLM,
    and CapyBara (LDJnr, [2023](https://arxiv.org/html/2402.01030v4#bib.bib25)) that
    focuses on reasoning in multi-turn conversations. Statistics and down-sampling
    details can be found in Tab. [4](https://arxiv.org/html/2402.01030v4#S3.T4 "Table
    4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering Open-source
    LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit Better LLM
    Agents") and §[C](https://arxiv.org/html/2402.01030v4#A3 "Appendix C General Data
    Down-sample ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActInstruct 可以与现有的代理-用户对话数据一起使用。我们使用一个子采样的 OpenOrca 数据集（Lian等， [2023](https://arxiv.org/html/2402.01030v4#bib.bib28)），该数据集侧重于单轮推理（CoT），ShareGPT（匿名，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib2); OpenChat，[2023](https://arxiv.org/html/2402.01030v4#bib.bib35)）来自两个来源，包含人类与LLM之间的多轮对话，以及
    CapyBara（LDJnr， [2023](https://arxiv.org/html/2402.01030v4#bib.bib25)）专注于多轮对话中的推理。统计数据和下采样的详细信息可以在表
    [4](https://arxiv.org/html/2402.01030v4#S3.T4 "表 4 ‣ 3.1 CodeActInstruct：代理-环境交互
    ‣ 3 赋能开源LLM代理以提升CodeAct ‣ 可执行代码行动激发更好的LLM代理") 和附录[C](https://arxiv.org/html/2402.01030v4#A3
    "附录C 一般数据下采样 ‣ 可执行代码行动激发更好的LLM代理")中找到。
- en: 'Table 5: Evaluation results for CodeActAgent. The best results among all open-source
    LLMs are bolded, and the second-best results are underlined. ID and OD stand for
    in-domain and out-of-domain evaluation correspondingly. Overall averaged performance
    normalizes the MT-Bench score to be consistent with other tasks and excludes in-domain
    tasks for fair comparison.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：CodeActAgent的评估结果。所有开源LLMs中的最佳结果已加粗，第二好的结果已下划线。ID和OD分别代表领域内和领域外评估。整体平均性能将MT-Bench分数规范化，以便与其他任务保持一致，并排除了领域内任务，以便公平比较。
- en: Agent Tasks Generic Tasks Overall Code as Action Text as Action (OD) (OD) Average
    Model Size MINT (ID) MINT (OD) M³ToolEval (OD) Miniwob++ SciWorld MMLU HumanEval
    GSM8K MTBench Open-source LLMs (LLaMA-2-based) Llama2 Base 7B -^∗ -^∗ -^∗ -^∗
    -^∗ $45.3$ $12.8$ $14.6$ -^∗ -^∗ Llama2 Chat 7B $3.2$ $11.0$ $0.0$ $0.0$ $5.9$
    $48.0$ $13.9$ $27.7$ $6.3$ $21.1$ FireAct (Chen et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7))
    7B $0.0$ $0.3$ $0.0$ $0.0$ $6.8$ $44.1$ $3.5$ $12.4$ $4.5$ $14.0$ AgentLM (Zeng
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71)) 7B $8.7$ $6.1$
    $0.0$ $28.9$ $13.7$ $48.7$ $15.4$ $24.6$ $6.1$ $24.8$ CodeActAgent (LLaMA-2) 7B
    $51.3$ $20.4$ $0.0$ $25.5$ $\mathbf{17.6}$ $50.6$ $18.1$ $38.3$ $7.5$ $30.7$ Open-source
    LLMs (Mistral-based) Mistral Base 7B -^∗ -^∗ -^∗ -^∗ -^∗ $\mathbf{60.1}$ $30.5$
    $52.1$ -^∗ -^∗ Mistral Instruct 7B $18.8$ $9.7$ $0.0$ $0.5$ $4.0$ $53.8$ $29.3$
    $43.3$ $6.4$ $25.6$ CodeActAgent (Mistral) 7B $\mathbf{57.4}$ $\mathbf{32.4}$
    $\mathbf{12.2}$ $\mathbf{46.2}$ $15.9$ $59.1$ $\mathbf{34.7}$ $\mathbf{58.0}$
    $\mathbf{8.2}$ $\mathbf{42.5}$ Closed-source LLMs gpt-3.5-turbo-0613 - $33.9$
    $38.2$ $51.2$ $66.7$ $21.2$ $70.0$ $48.1$ $57.1$ $7.9$ $54.0$ gpt-4-0613 - $68.6$
    $70.2$ $67.1$ $69.4$ $36.4$ $86.4$ $67.0$ $87.1$ $9.0$ $71.7$
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Agent任务 通用任务 整体代码作为行动 文本作为行动（OD） （OD） 平均模型大小 MINT（ID） MINT（OD） M³ToolEval（OD）
    Miniwob++ SciWorld MMLU HumanEval GSM8K MTBench 开源LLMs（基于LLaMA-2） Llama2 Base
    7B -^∗ -^∗ -^∗ -^∗ -^∗ $45.3$ $12.8$ $14.6$ -^∗ -^∗ Llama2 Chat 7B $3.2$ $11.0$
    $0.0$ $0.0$ $5.9$ $48.0$ $13.9$ $27.7$ $6.3$ $21.1$ FireAct（Chen等， [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)）
    7B $0.0$ $0.3$ $0.0$ $0.0$ $6.8$ $44.1$ $3.5$ $12.4$ $4.5$ $14.0$ AgentLM（Zeng等，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib71)） 7B $8.7$ $6.1$ $0.0$ $28.9$
    $13.7$ $48.7$ $15.4$ $24.6$ $6.1$ $24.8$ CodeActAgent（LLaMA-2） 7B $51.3$ $20.4$
    $0.0$ $25.5$ $\mathbf{17.6}$ $50.6$ $18.1$ $38.3$ $7.5$ $30.7$ 开源LLMs（基于Mistral）
    Mistral Base 7B -^∗ -^∗ -^∗ -^∗ -^∗ $\mathbf{60.1}$ $30.5$ $52.1$ -^∗ -^∗ Mistral
    Instruct 7B $18.8$ $9.7$ $0.0$ $0.5$ $4.0$ $53.8$ $29.3$ $43.3$ $6.4$ $25.6$ CodeActAgent（Mistral）
    7B $\mathbf{57.4}$ $\mathbf{32.4}$ $\mathbf{12.2}$ $\mathbf{46.2}$ $15.9$ $59.1$
    $\mathbf{34.7}$ $\mathbf{58.0}$ $\mathbf{8.2}$ $\mathbf{42.5}$ 闭源LLMs gpt-3.5-turbo-0613
    - $33.9$ $38.2$ $51.2$ $66.7$ $21.2$ $70.0$ $48.1$ $57.1$ $7.9$ $54.0$ gpt-4-0613
    - $68.6$ $70.2$ $67.1$ $69.4$ $36.4$ $86.4$ $67.0$ $87.1$ $9.0$ $71.7$
- en: '*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Some results are only available with instruction-tuned models.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一些结果仅适用于经过指令微调的模型。
- en: 3.2 CodeActAgent
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 CodeActAgent
- en: 'We fine-tune Llama-2 7B (Touvron et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib51))
    and Mistral 7B (Jiang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib23))
    on a mixture of CodeActInstruct and general conversations (Tab. [4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")) to obtain CodeActAgent.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对Llama-2 7B（Touvron等，[2023](https://arxiv.org/html/2402.01030v4#bib.bib51)）和Mistral
    7B（Jiang等，[2023](https://arxiv.org/html/2402.01030v4#bib.bib23)）进行了微调，使用了CodeActInstruct和一般对话的混合数据集（表格[4](https://arxiv.org/html/2402.01030v4#S3.T4
    "Table 4 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")），以获得CodeActAgent。'
- en: Training Setup. We perform full-parameter supervised fine-tuning with a sequence
    length of 4,096 tokens for Llama-2 and 16,384 for Mistral. Please refer to §[D](https://arxiv.org/html/2402.01030v4#A4
    "Appendix D CodeActAgent Training Details ‣ Executable Code Actions Elicit Better
    LLM Agents") for more details.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练设置。我们对Llama-2进行全参数的监督微调，序列长度为4,096个标记，而Mistral为16,384个标记。更多细节请参见§[D](https://arxiv.org/html/2402.01030v4#A4
    "Appendix D CodeActAgent Training Details ‣ Executable Code Actions Elicit Better
    LLM Agents")。
- en: 'Evaluation Setup. We use MINT (Wang et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58))
    to evaluate LLMs with CodeAct on a diverse range of agent tasks. CodeActAgent
    has some training domains overlapping with MINT’s evaluation (i.e., MINT includes
    ALFWorld and MATH), hence we report separate numbers for MINT’s in- and out-of-domain
    performance. Unless otherwise specified, we measure MINT tasks’ success rates
    with interaction turn $k=5$. We also evaluate out-of-domain agent tasks using
    text actions from MiniWob++ (computer tasks, (Kim et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib24)))
    and ScienceWorld (text-based simulator for elementary science curriculum, (Wang
    et al., [2022a](https://arxiv.org/html/2402.01030v4#bib.bib54))) to test whether
    CodeActAgent can generalize to different action formats. Finally, we include a
    suite of general LLM evaluation tasks to assess general capability: MMLU (Hendrycks
    et al., [2020](https://arxiv.org/html/2402.01030v4#bib.bib17)) for knowledge-based
    QA, HumanEval (Chen et al., [2021](https://arxiv.org/html/2402.01030v4#bib.bib8))
    for single-turn code-generation, GSM8K (Cobbe et al., [2021](https://arxiv.org/html/2402.01030v4#bib.bib13))
    for single-turn tool-free math reasoning, and MTBench (Zheng et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib73))
    for instruction-following.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 评估设置。我们使用MINT（Wang等，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)）在多种代理任务上评估带有CodeAct的LLM。CodeActAgent的一些训练领域与MINT的评估领域重叠（即MINT包括ALFWorld和MATH），因此我们分别报告MINT的领域内和领域外的性能。除非另有说明，我们在测量MINT任务的成功率时使用交互轮次$k=5$。我们还使用来自MiniWob++（计算机任务，（Kim等，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib24)））和ScienceWorld（针对基础科学课程的基于文本的模拟器，（Wang等，
    [2022a](https://arxiv.org/html/2402.01030v4#bib.bib54)））的文本动作评估领域外的代理任务，以测试CodeActAgent是否能够推广到不同的动作格式。最后，我们包括一系列常规LLM评估任务来评估通用能力：MMLU（Hendrycks等，
    [2020](https://arxiv.org/html/2402.01030v4#bib.bib17)）用于基于知识的问答，HumanEval（Chen等，
    [2021](https://arxiv.org/html/2402.01030v4#bib.bib8)）用于单轮代码生成，GSM8K（Cobbe等， [2021](https://arxiv.org/html/2402.01030v4#bib.bib13)）用于单轮不使用工具的数学推理，以及MTBench（Zheng等，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib73)）用于遵循指令。
- en: 'CodeActAgent Excels in CodeAct Task. As shown in Tab. [5](https://arxiv.org/html/2402.01030v4#S3.T5
    "Table 5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents"), CodeActAgent (both variants) perform better than all evaluated
    open-source LLMs on both the in- and out-of-domain subsets of MINT. On M³ToolEval,
    we find CodeActAgent (Mistral) outperforms open-source LLMs of similar size (7B
    and 13B) and even reaches similar performance to those 70B models (Tab. [3](https://arxiv.org/html/2402.01030v4#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")).
    Surprisingly, no improvement is observed for the Llama-2 variant. We discuss potential
    reasons in §[H](https://arxiv.org/html/2402.01030v4#A8 "Appendix H CodeActAgent
    Anomaly on M3ToolEval ‣ Executable Code Actions Elicit Better LLM Agents").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeActAgent 在CodeAct任务中表现出色。如表格[5](https://arxiv.org/html/2402.01030v4#S3.T5
    "Table 5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")所示，CodeActAgent（两个变体）在MINT的领域内和领域外子集上，表现超过了所有评估的开源LLM。在M³ToolEval中，我们发现CodeActAgent（Mistral）超越了同等规模（7B和13B）的开源LLM，甚至达到了与70B模型相似的表现（表格[3](https://arxiv.org/html/2402.01030v4#S2.T3
    "Table 3 ‣ 2.2 CodeAct Shows the Promise as a Strong Tool Use Framework ‣ 2 CodeAct
    Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better LLM Agents")）。令人惊讶的是，Llama-2变体未见改善。我们在§[H](https://arxiv.org/html/2402.01030v4#A8
    "Appendix H CodeActAgent Anomaly on M3ToolEval ‣ Executable Code Actions Elicit
    Better LLM Agents")中讨论了可能的原因。'
- en: CodeActAgent Generalizes to Text Action. When evaluated on out-of-domain text
    actions, CodeActAgent (LLaMA2, 7B), which has never been optimized for text action,
    achieves comparable performance to AgentLM-7B (Zeng et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71))
    which has explicit tuning for text actions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActAgent 泛化到文本动作。在评估域外文本动作时，CodeActAgent（LLaMA2, 7B）虽然从未针对文本动作进行优化，但其性能与专门针对文本动作进行调优的AgentLM-7B（Zeng等，[2023](https://arxiv.org/html/2402.01030v4#bib.bib71)）相当。
- en: 'CodeActAgent Maintains or Improves the Performance on General LLM Tasks. In
    Tab. [5](https://arxiv.org/html/2402.01030v4#S3.T5 "Table 5 ‣ 3.1 CodeActInstruct:
    Agent-Environment Interactions ‣ 3 Empowering Open-source LLM Agent to be Better
    at CodeAct ‣ Executable Code Actions Elicit Better LLM Agents"), we find that
    CodeActAgent (both variants) performs better on generic LLM tasks we tested, except
    for a slight degradation on MMLU for CodeActAgent (Mistral, 7B).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 'CodeActAgent 在通用LLM任务上的表现保持或提高。在表格[5](https://arxiv.org/html/2402.01030v4#S3.T5
    "Table 5 ‣ 3.1 CodeActInstruct: Agent-Environment Interactions ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents")中，我们发现CodeActAgent（两个变体）在我们测试的通用LLM任务上表现更好，除了CodeActAgent（Mistral,
    7B）在MMLU上有轻微的下降。'
- en: Ablation Study. Tab. [A.8](https://arxiv.org/html/2402.01030v4#A0.T8 "Table
    A.8 ‣ Executable Code Actions Elicit Better LLM Agents") presents ablation experiments
    to determine the importance of CodeActInstruct and general conversations. Both
    CodeActInstruct and general conversations contribute to agent tasks, while general
    conversations are essential to maintain performance on general tasks.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 去除实验。表格[A.8](https://arxiv.org/html/2402.01030v4#A0.T8 "Table A.8 ‣ Executable
    Code Actions Elicit Better LLM Agents")展示了去除实验，用以确定CodeActInstruct和一般对话的重要性。CodeActInstruct和一般对话都对代理任务有所贡献，而一般对话对保持通用任务的表现至关重要。
- en: 4 Related Work
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: 4.1 Action Module in LLM Agents
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LLM 代理中的动作模块
- en: 'As detailed in (Wang et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib53)),
    LLM-based autonomous agents are typically structured around four components: customized
    profiles (Park et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib37);
    Qian et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib40)), long-term
    memory capabilities (Zhu et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib75);
    Fischer, [2023](https://arxiv.org/html/2402.01030v4#bib.bib15)), reasoning and
    planning algorithms (Wei et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib60);
    Chen et al., [2023d](https://arxiv.org/html/2402.01030v4#bib.bib11)), and, most
    crucially, action modules. The action modules are key to facilitating LLM agents
    to effectively interact with external entities, including humans (Lee et al.,
    [2022](https://arxiv.org/html/2402.01030v4#bib.bib26)) and tools (Qin et al.,
    [2023a](https://arxiv.org/html/2402.01030v4#bib.bib42)) in the environment (Wang
    et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58); Yang et al., [2024a](https://arxiv.org/html/2402.01030v4#bib.bib62)).
    In this study, we address the critical problem of standardizing the action space
    for LLM agents. We further discuss the difference between CodeAct and the line
    of work that uses code generation for problem-solving in §[A](https://arxiv.org/html/2402.01030v4#A1
    "Appendix A Comparison with Work that Uses Code Generation for Problem-solving
    ‣ Executable Code Actions Elicit Better LLM Agents"). We notice a concurrent study
    TaskWeaver (Qiao et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib41))
    similarly endorses the use of code. We discuss the principal distinctions in §[B](https://arxiv.org/html/2402.01030v4#A2
    "Appendix B Comparison with TaskWeaver ‣ Executable Code Actions Elicit Better
    LLM Agents").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如(Wang et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib53))中详细描述，基于LLM的自主智能体通常由四个组成部分构成：定制化的配置文件(Park
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib37); Qian et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib40))，长期记忆能力(Zhu
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib75); Fischer, [2023](https://arxiv.org/html/2402.01030v4#bib.bib15))，推理与规划算法(Wei
    et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib60); Chen et al., [2023d](https://arxiv.org/html/2402.01030v4#bib.bib11))，以及最关键的，行动模块。行动模块对于使LLM智能体能够有效地与外部实体（包括人类(Lee
    et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib26))和工具(Qin et al.,
    [2023a](https://arxiv.org/html/2402.01030v4#bib.bib42))）以及环境中的其他元素(Wang et al.,
    [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58); Yang et al., [2024a](https://arxiv.org/html/2402.01030v4#bib.bib62))进行有效互动至关重要。在本研究中，我们解决了为LLM智能体标准化行动空间的关键问题。我们进一步讨论了CodeAct与利用代码生成进行问题解决的工作之间的差异，见§[A](https://arxiv.org/html/2402.01030v4#A1
    "附录A 与使用代码生成进行问题解决的工作对比 ‣ 可执行代码行动促使更好的LLM智能体")。我们注意到一项相关研究TaskWeaver(Qiao et al.,
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib41))也同样支持使用代码。我们在§[B](https://arxiv.org/html/2402.01030v4#A2
    "附录B 与TaskWeaver的对比 ‣ 可执行代码行动促使更好的LLM智能体")中讨论了主要区别。
- en: 4.2 Improving LLM Agents
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 改进LLM智能体
- en: Two primary methods for enhancing LLM agents are prompt engineering and instruction
    tuning, as surveyed by (Wang et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib53)).
    For prompt engineering (Liu et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib30)),
    numerous strategies have been introduced to improve the chain-of-thought reasoning (Wei
    et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib60)), including self-consistency-based
    reasoning (Wang et al., [2022b](https://arxiv.org/html/2402.01030v4#bib.bib55);
    Chen et al., [2023d](https://arxiv.org/html/2402.01030v4#bib.bib11)) and tree-based
    approaches (Yao et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib68)).
    Moreover, LLMs can be strategically prompted to reflect on previous plans (Yao
    et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib69); Wang et al., [2023f](https://arxiv.org/html/2402.01030v4#bib.bib59);
    Zhang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib72)), enabling
    them to refine initial actions through trial and error. Contrast to prompt engineering,
    instruction tuning intrinsically enhances LLMs (Chung et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib12)),
    particularly in their agent capabilities (Zeng et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71);
    Chen et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)). For effective
    training, human annotators can curate expert demonstrations for specific agent
    tasks, such as web browsing (Yao et al., [2022a](https://arxiv.org/html/2402.01030v4#bib.bib66);
    Nakano et al., [2021](https://arxiv.org/html/2402.01030v4#bib.bib34)). To minimize
    human annotation efforts, prior work creates synthetic datasets using stronger
    LLMs to distill agent capabilities into local models, focusing on tool usage (Qin
    et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib43)), interaction (Chen
    et al., [2023c](https://arxiv.org/html/2402.01030v4#bib.bib10)), and social skills (Liu
    et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib31)). CodeActInstruct
    aligns with the latter approach and creates datasets using stronger LLMs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 增强大型语言模型（LLM）代理的两种主要方法是提示工程和指令微调，如（Wang et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib53)）所述。对于提示工程（Liu
    et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib30)），已提出许多策略来改进思维链推理（Wei
    et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib60)），包括基于自一致性的推理（Wang
    et al., [2022b](https://arxiv.org/html/2402.01030v4#bib.bib55); Chen et al., [2023d](https://arxiv.org/html/2402.01030v4#bib.bib11)）和基于树的方式（Yao
    et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib68)）。此外，可以通过战略性地提示LLMs来反思先前的计划（Yao
    et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib69); Wang et al., [2023f](https://arxiv.org/html/2402.01030v4#bib.bib59);
    Zhang et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib72)），使其能够通过试错法完善初步的行动。与提示工程相比，指令微调本质上增强了LLM（Chung
    et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib12)），特别是在其代理能力方面（Zeng
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib71); Chen et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib7)）。为了有效的训练，人类标注者可以为特定的代理任务策划专家示范，如网页浏览（Yao
    et al., [2022a](https://arxiv.org/html/2402.01030v4#bib.bib66); Nakano et al.,
    [2021](https://arxiv.org/html/2402.01030v4#bib.bib34)）。为了最小化人工标注工作，先前的研究利用更强大的LLM创建合成数据集，将代理能力提炼到本地模型中，重点关注工具使用（Qin
    et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib43)）、交互（Chen et al.,
    [2023c](https://arxiv.org/html/2402.01030v4#bib.bib10)）和社交技能（Liu et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib31)）。CodeActInstruct采用后一种方法，利用更强大的LLM创建数据集。
- en: 5 Conclusions
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work introduces CodeAct that employs executable Python code for the LLM
    agent’s action, which is advantageous over using text or JSON action, especially
    in complex scenarios. We collect CodeAct-focused multi-turn interaction trajectories
    CodeActInstruct for instruction tuning, and train CodeActAgent that is specially
    designed for seamless integration with Python and can execute sophisticated tasks
    (e.g., model training) leveraging existing Python packages and autonomously rectifying
    errors through self-debugging.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了CodeAct，它采用可执行的Python代码作为LLM代理的行动，这比使用文本或JSON行动更具优势，特别是在复杂场景下。我们收集了以CodeAct为重点的多轮交互轨迹CodeActInstruct用于指令微调，并训练了专为与Python无缝集成而设计的CodeActAgent，该代理能够执行复杂任务（例如，模型训练），利用现有的Python包并通过自我调试自动修正错误。
- en: Acknowledgement
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'We thank the anonymous reviewers for their suggestions and comments. This research
    is based upon work supported by U.S. DARPA ECOLE Program No. HR00112390060 and
    U.S. DARPA ITM Program No. FA8650-23-C-7316 and KAIROS Program No. FA8750-19-2-1004\.
    The views and conclusions contained herein are those of the authors and should
    not be interpreted as necessarily representing the official policies, either expressed
    or implied, of DARPA, or the U.S. Government. The U.S. Government is authorized
    to reproduce and distribute reprints for governmental purposes notwithstanding
    any copyright annotation therein. This work used the Delta system at the National
    Center for Supercomputing Applications through allocation CIS230256 from the Advanced
    Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS, Boerner
    et al. [2023](https://arxiv.org/html/2402.01030v4#bib.bib3)) program, which is
    supported by National Science Foundation grants #2138259, #2138286, #2138307,
    #2137603, and #2138296.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢匿名评审者的建议和评论。本研究基于美国DARPA ECOLE项目（编号HR00112390060）、美国DARPA ITM项目（编号FA8650-23-C-7316）和KAIROS项目（编号FA8750-19-2-1004）支持的工作。本文中所表达的观点和结论仅代表作者本人，并不应被解读为代表DARPA或美国政府的官方政策，无论是明示还是暗示。美国政府有权为政府目的复制和分发再版，尽管其中可能包含任何版权声明。此项工作使用了由国家超级计算应用中心提供的Delta系统，通过高级网络基础设施协调生态系统：服务与支持（ACCESS，Boerner等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib3)）计划的分配CIS230256，该计划得到美国国家科学基金会（NSF）资助，基金编号#2138259、#2138286、#2138307、#2137603和#2138296。
- en: Impact Statement
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance LLM-based autonomous agents
    that can communicate with humans through natural language and assist human users
    by performing tasks in environments on behalf of humans. In this section, we discuss
    potential societal consequences, limitations, and future work related to our work
    and its goal.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍的工作旨在推动基于LLM的自主代理的发展，这些代理能够通过自然语言与人类沟通，并通过在环境中代表人类执行任务来协助人类用户。在本节中，我们讨论了与我们的工作及其目标相关的潜在社会影响、局限性和未来工作。
- en: CodeActAgent is an initial prototype of an autonomous agent and still has several
    practical limitations. For example, it may suffer from hallucination commonly
    seen in LLMs (e.g., imagine the content of a variable without actually printing
    it out), suggesting the need for subsequent alignment (Ouyang et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib36))
    for further improvements.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: CodeActAgent是一个自主代理的初步原型，仍然存在若干实际局限性。例如，它可能会遭遇在LLM中常见的幻觉问题（例如，想象一个变量的内容而没有实际输出它），这表明需要后续的对齐工作（Ouyang等人，[2022](https://arxiv.org/html/2402.01030v4#bib.bib36)）以进一步改进。
- en: Despite being a prototype, CodeActAgent has already demonstrated limited self-improving
    capability (e.g., self-debug error messages to improve its action) and the ability
    to interact with environments. Future work may build upon CodeActAgent to develop
    better agents by having them perform extensive interactions within a given environment
    and iteratively bootstrap their self-improving capability to learn to improve
    from past mistakes. More powerful agents, as results of such algorithms, are potentially
    beneficial for solving a wide range of real-world problems (e.g., theorem proving,
    drug discovery). As extensively discussed in (Eloundou et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib14)),
    a fully autonomous agent may transform the current landscape of the labor market
    and impact the jobs of existing workers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管只是一个原型，CodeActAgent已经展示了有限的自我改进能力（例如，自我调试错误信息以改善其行动）以及与环境互动的能力。未来的工作可能会基于CodeActAgent，开发更好的代理，使其在特定环境中进行广泛的互动，并通过迭代自举其自我改进能力，学习从过去的错误中改进。作为这些算法结果的更强大代理，可能对解决广泛的现实世界问题（例如，定理证明、药物发现）具有潜在的益处。如（Eloundou等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib14)）中广泛讨论的那样，完全自主的代理可能会改变当前劳动力市场的格局，并影响现有工人的工作。
- en: Furthermore, since CodeAct directly grants access for the agent to freely execute
    code in a sandbox environment, in the worst scenario (e.g., in Sci-Fi movies),
    such an agent may potentially break free of the sandbox restriction and cause
    harm to the world through cyber-attack, highlighting the need for future work
    to design better safety mechanism to safeguard autonomous agents (Tang et al.,
    [2024](https://arxiv.org/html/2402.01030v4#bib.bib49)).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于 CodeAct 直接授权代理在沙箱环境中自由执行代码，在最糟糕的情况下（例如在科幻电影中），此类代理可能会突破沙箱限制，通过网络攻击对世界造成危害，这突显了未来工作中需要设计更好的安全机制以保障自主代理的安全性（Tang
    等人， [2024](https://arxiv.org/html/2402.01030v4#bib.bib49)）。
- en: References
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahn et al. (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O.,
    David, B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho,
    D., Hsu, J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey,
    K., Jesmonth, S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H.,
    Levine, S., Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse,
    J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V.,
    Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., and Zeng, A. Do as i can and not as
    i say: Grounding language in robotic affordances. In *arXiv preprint arXiv:2204.01691*,
    2022.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn 等人 (2022) Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David,
    B., Finn, C., Fu, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Ho, D., Hsu,
    J., Ibarz, J., Ichter, B., Irpan, A., Jang, E., Ruano, R. J., Jeffrey, K., Jesmonth,
    S., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Lee, K.-H., Levine, S.,
    Lu, Y., Luu, L., Parada, C., Pastor, P., Quiambao, J., Rao, K., Rettinghouse,
    J., Reyes, D., Sermanet, P., Sievers, N., Tan, C., Toshev, A., Vanhoucke, V.,
    Xia, F., Xiao, T., Xu, P., Xu, S., Yan, M., 和 Zeng, A. 依我所能，不依我所说：将语言与机器人能力相结合。收录于
    *arXiv 预印本 arXiv:2204.01691*，2022年。
- en: Anonymous (2023) Anonymous. Sharegpt dataset. [https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json](https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json),
    2023. A dataset containing multi-turn conversations between human and LLM assistant.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 匿名 (2023) 匿名。Sharegpt 数据集。 [https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json](https://hf.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_V3_unfiltered_cleaned_split_no_imsorry.json)，2023年。包含人类与LLM助手之间多轮对话的数据集。
- en: 'Boerner et al. (2023) Boerner, T. J., Deems, S., Furlani, T. R., Knuth, S. L.,
    and Towns, J. Access: Advancing innovation: Nsf’s advanced cyberinfrastructure
    coordination ecosystem: Services & support. In *Practice and Experience in Advanced
    Research Computing*, pp.  173–176\. 2023.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Boerner 等人 (2023) Boerner, T. J., Deems, S., Furlani, T. R., Knuth, S. L.,
    和 Towns, J. Access: 推动创新：美国国家科学基金会的高级网络基础设施协调生态系统：服务与支持。收录于 *实践与高级研究计算经验*，第173–176页，2023年。'
- en: 'Bran et al. (2023) Bran, A. M., Cox, S., White, A. D., and Schwaller, P. Chemcrow:
    Augmenting large-language models with chemistry tools. *arXiv preprint arXiv:2304.05376*,
    2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bran 等人 (2023) Bran, A. M., Cox, S., White, A. D., 和 Schwaller, P. Chemcrow:
    用化学工具增强大语言模型。*arXiv 预印本 arXiv:2304.05376*，2023年。'
- en: Cano et al. (2023) Cano, A. H., Pagliardini, M., Köpf, A., Matoba, K., Mohtashami,
    A., Wang, X., Fan, O. S., Marmet, A., Bayazit, D., Krawczuk, I., Chen, Z., Salvi,
    F., Bosselut, A., and Jaggi, M. epfllm megatron-llm, 2023. URL [https://github.com/epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cano 等人 (2023) Cano, A. H., Pagliardini, M., Köpf, A., Matoba, K., Mohtashami,
    A., Wang, X., Fan, O. S., Marmet, A., Bayazit, D., Krawczuk, I., Chen, Z., Salvi,
    F., Bosselut, A., 和 Jaggi, M. epfllm megatron-llm，2023年。网址 [https://github.com/epfLLM/Megatron-LLM](https://github.com/epfLLM/Megatron-LLM)。
- en: Chase (2022) Chase, H. LangChain, October 2022. URL [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain).
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Chase, H. LangChain, 2022年10月。网址 [https://github.com/langchain-ai/langchain](https://github.com/langchain-ai/langchain)。
- en: 'Chen et al. (2023a) Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan,
    K., and Yao, S. Fireact: Toward language agent fine-tuning. *arXiv preprint arXiv:2310.05915*,
    2023a.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人 (2023a) Chen, B., Shu, C., Shareghi, E., Collier, N., Narasimhan, K.,
    和 Yao, S. Fireact: 迈向语言代理微调。*arXiv 预印本 arXiv:2310.05915*，2023年。'
- en: Chen et al. (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating
    large language models trained on code. *arXiv preprint arXiv:2107.03374*, 2021.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2021) Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
    Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., 等人。评估基于代码训练的大语言模型。*arXiv
    预印本 arXiv:2107.03374*，2021年。
- en: Chen et al. (2023b) Chen, X., Lin, M., Schärli, N., and Zhou, D. Teaching large
    language models to self-debug. *arXiv preprint arXiv:2304.05128*, 2023b.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023b) Chen, X., Lin, M., Schärli, N., 和 Zhou, D. 教授大语言模型自我调试。*arXiv
    预印本 arXiv:2304.05128*，2023年。
- en: 'Chen et al. (2023c) Chen, Y., Sikka, K., Cogswell, M., Ji, H., and Divakaran,
    A. Dress: Instructing large vision-language models to align and interact with
    humans via natural language feedback. *arXiv preprint arXiv:2311.10081*, 2023c.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2023c）Chen, Y., Sikka, K., Cogswell, M., Ji, H., 和 Divakaran, A. Dress:
    指导大型视觉-语言模型通过自然语言反馈与人类对齐并互动。*arXiv 预印本 arXiv:2311.10081*，2023c。'
- en: Chen et al. (2023d) Chen, Y., Sikka, K., Cogswell, M., Ji, H., and Divakaran,
    A. Measuring and improving chain-of-thought reasoning in vision-language models.
    *arXiv preprint arXiv:2309.04461*, 2023d.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2023d）Chen, Y., Sikka, K., Cogswell, M., Ji, H., 和 Divakaran, A. 衡量并改进视觉-语言模型中的思维链推理。*arXiv
    预印本 arXiv:2309.04461*，2023d。
- en: Chung et al. (2022) Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et al. Scaling instruction-finetuned
    language models. *arXiv preprint arXiv:2210.11416*, 2022.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人（2022）Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus,
    W., Li, Y., Wang, X., Dehghani, M., Brahma, S., 等. 扩展指令微调语言模型。*arXiv 预印本 arXiv:2210.11416*，2022。
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser,
    L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., 等. 训练验证器解决数学文字题。*arXiv 预印本
    arXiv:2110.14168*，2021。
- en: 'Eloundou et al. (2023) Eloundou, T., Manning, S., Mishkin, P., and Rock, D.
    Gpts are gpts: An early look at the labor market impact potential of large language
    models. *arXiv preprint arXiv:2303.10130*, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eloundou 等人（2023）Eloundou, T., Manning, S., Mishkin, P., 和 Rock, D. GPTs 就是
    GPTs：大语言模型对劳动力市场的潜在影响的早期观察。*arXiv 预印本 arXiv:2303.10130*，2023。
- en: 'Fischer (2023) Fischer, K. A. Reflective linguistic programming (rlp): A stepping
    stone in socially-aware agi (socialagi). *arXiv preprint arXiv:2305.12647*, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer（2023）Fischer, K. A. 反思性语言编程（RLP）：社会意识AGI（SocialAGI）的垫脚石。*arXiv 预印本 arXiv:2305.12647*，2023。
- en: 'Gao et al. (2023) Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y.,
    Callan, J., and Neubig, G. Pal: Program-aided language models. In *International
    Conference on Machine Learning*, pp.  10764–10799\. PMLR, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等人（2023）Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan,
    J., 和 Neubig, G. Pal: 程序辅助语言模型。在 *国际机器学习大会*，第10764–10799页。PMLR，2023。'
- en: Hendrycks et al. (2020) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding.
    In *International Conference on Learning Representations*, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2020）Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. 衡量大规模多任务语言理解。在 *国际学习表示大会*，2020。
- en: Hendrycks et al. (2021a) Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M.,
    Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., et al. Measuring
    coding challenge competence with apps. In *Thirty-fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021a.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021a）Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora,
    A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., 等. 使用应用程序衡量编码挑战能力。在 *第三十五届神经信息处理系统大会
    数据集与基准赛道（第二轮）*，2021a。
- en: Hendrycks et al. (2021b) Hendrycks, D., Burns, C., Kadavath, S., Arora, A.,
    Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem
    solving with the math dataset. In *Thirty-fifth Conference on Neural Information
    Processing Systems Datasets and Benchmarks Track (Round 2)*, 2021b.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021b）Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart,
    S., Tang, E., Song, D., 和 Steinhardt, J. 使用数学数据集衡量数学问题解决能力。在 *第三十五届神经信息处理系统大会
    数据集与基准赛道（第二轮）*，2021b。
- en: 'Hong et al. (2023) Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang,
    C., Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., et al. Metagpt: Meta programming
    for multi-agent collaborative framework. *arXiv preprint arXiv:2308.00352*, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等人（2023）Hong, S., Zheng, X., Chen, J., Cheng, Y., Wang, J., Zhang, C.,
    Wang, Z., Yau, S. K. S., Lin, Z., Zhou, L., 等. Metagpt: 多代理协作框架的元编程。*arXiv 预印本
    arXiv:2308.00352*，2023。'
- en: 'Hong et al. (2024) Hong, S., Lin, Y., Liu, B., Liu, B., Wu, B., Li, D., Chen,
    J., Zhang, J., Wang, J., Zhang, L., Zhang, L., Yang, M., Zhuge, M., Guo, T., Zhou,
    T., Tao, W., Wang, W., Tang, X., Lu, X., Zheng, X., Liang, X., Fei, Y., Cheng,
    Y., Xu, Z., and Wu, C. Data interpreter: An llm agent for data science, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人（2024）Hong, S., Lin, Y., Liu, B., Liu, B., Wu, B., Li, D., Chen, J.,
    Zhang, J., Wang, J., Zhang, L., Zhang, L., Yang, M., Zhuge, M., Guo, T., Zhou,
    T., Tao, W., Wang, W., Tang, X., Lu, X., Zheng, X., Liang, X., Fei, Y., Cheng,
    Y., Xu, Z., 和 Wu, C. 数据解释器：一个面向数据科学的LLM代理，2024。
- en: 'Huang et al. (2023) Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei,
    L. Voxposer: Composable 3d value maps for robotic manipulation with language models.
    *arXiv preprint arXiv:2307.05973*, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2023）Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., 和 Fei-Fei, L. 《Voxposer：用于机器人操控的可组合3D值图与语言模型》。*arXiv预印本arXiv:2307.05973*，2023年。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人（2023）Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., 等人.
    《Mistral 7b》。*arXiv预印本arXiv:2310.06825*，2023年。
- en: Kim et al. (2023) Kim, G., Baldi, P., and McAleer, S. Language models can solve
    computer tasks. *arXiv preprint arXiv:2303.17491*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim等人（2023）Kim, G., Baldi, P., 和 McAleer, S. 《语言模型可以解决计算机任务》。*arXiv预印本arXiv:2303.17491*，2023年。
- en: LDJnr (2023) LDJnr. Capybara dataset. [https://hf.co/datasets/LDJnr/Verified-Camel](https://hf.co/datasets/LDJnr/Verified-Camel),
    [https://hf.co/datasets/LDJnr/Pure-Dove](https://hf.co/datasets/LDJnr/Pure-Dove),
    [https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct](https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct),
    2023. A dataset focusing on reasoning in multi-turn conversations.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDJnr（2023）LDJnr. 《Capybara数据集》。 [https://hf.co/datasets/LDJnr/Verified-Camel](https://hf.co/datasets/LDJnr/Verified-Camel)，[https://hf.co/datasets/LDJnr/Pure-Dove](https://hf.co/datasets/LDJnr/Pure-Dove)，[https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct](https://hf.co/datasets/LDJnr/LessWrong-Amplify-Instruct)，2023年。一个专注于多轮对话推理的数据集。
- en: 'Lee et al. (2022) Lee, M., Liang, P., and Yang, Q. Coauthor: Designing a human-ai
    collaborative writing dataset for exploring language model capabilities. In *Proceedings
    of the 2022 CHI conference on human factors in computing systems*, pp.  1–19,
    2022.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等人（2022）Lee, M., Liang, P., 和 Yang, Q. 《共同作者：为探索语言模型能力设计的人类-AI协作写作数据集》。发表于*2022年CHI人机交互大会论文集*，第1–19页，2022年。
- en: 'Li et al. (2023) Li, M., Song, F., Yu, B., Yu, H., Li, Z., Huang, F., and Li,
    Y. Api-bank: A benchmark for tool-augmented llms, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023）Li, M., Song, F., Yu, B., Yu, H., Li, Z., Huang, F., 和 Li, Y. 《Api-bank：工具增强型大语言模型基准》，2023年。
- en: 'Lian et al. (2023) Lian, W., Goodson, B., Pentland, E., Cook, A., Vong, C.,
    and ”Teknium”. Openorca: An open dataset of gpt augmented flan reasoning traces.
    [https://https://huggingface.co/Open-Orca/OpenOrca](https://https://huggingface.co/Open-Orca/OpenOrca),
    2023.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lian等人（2023）Lian, W., Goodson, B., Pentland, E., Cook, A., Vong, C., 和 ”Teknium”。《Openorca：一个用于GPT增强型FLAN推理轨迹的开放数据集》。
    [https://https://huggingface.co/Open-Orca/OpenOrca](https://https://huggingface.co/Open-Orca/OpenOrca)，2023年。
- en: 'Liang et al. (2022) Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter,
    B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied
    control. In *arXiv preprint arXiv:2209.07753*, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等人（2022）Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B.,
    Florence, P., 和 Zeng, A. 《代码作为政策：用于具身控制的语言模型程序》。发表于*arXiv预印本arXiv:2209.07753*，2022年。
- en: 'Liu et al. (2023a) Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,
    G. Pre-train, prompt, and predict: A systematic survey of prompting methods in
    natural language processing. *ACM Computing Surveys*, 55(9):1–35, 2023a.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2023a）Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., 和 Neubig, G.
    《预训练、提示与预测：自然语言处理中的提示方法系统性调查》。*ACM计算机调查*，55(9)：1–35，2023a年。
- en: Liu et al. (2023b) Liu, R., Yang, R., Jia, C., Zhang, G., Zhou, D., Dai, A. M.,
    Yang, D., and Vosoughi, S. Training socially aligned language models in simulated
    human society. *arXiv preprint arXiv:2305.16960*, 2023b.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2023b）Liu, R., Yang, R., Jia, C., Zhang, G., Zhou, D., Dai, A. M., Yang,
    D., 和 Vosoughi, S. 《在模拟人类社会中训练社会对齐的语言模型》。*arXiv预印本arXiv:2305.16960*，2023b年。
- en: 'Ma et al. (2023) Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O.,
    Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward
    design via coding large language models. *arXiv preprint arXiv:2310.12931*, 2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等人（2023）Ma, Y. J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman,
    D., Zhu, Y., Fan, L., 和 Anandkumar, A. 《Eureka：通过编码大型语言模型实现人类水平的奖励设计》。*arXiv预印本arXiv:2310.12931*，2023年。
- en: 'Mialon et al. (2023) Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru,
    R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., et al.
    Augmented language models: a survey. *arXiv preprint arXiv:2302.07842*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mialon等人（2023）Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru, R.,
    Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., 等人. 《增强型语言模型：一项调查》。*arXiv预印本arXiv:2302.07842*，2023年。
- en: 'Nakano et al. (2021) Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L.,
    Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., et al. Webgpt: Browser-assisted
    question-answering with human feedback. *arXiv preprint arXiv:2112.09332*, 2021.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nakano et al. (2021) Nakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L.,
    Kim, C., Hesse, C., Jain, S., Kosaraju, V., Saunders, W., 等人。Webgpt：借助浏览器的问答系统，结合人类反馈。*arXiv
    预印本 arXiv:2112.09332*，2021年。
- en: OpenChat (2023) OpenChat. Sharegpt dataset. [https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json](https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json),
    2023. A dataset containing multi-turn conversations between human and LLM assistants.
    It is filtered to contain data only from GPT-4.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenChat (2023) OpenChat。Sharegpt 数据集。 [https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json](https://hf.co/datasets/openchat/openchat_sharegpt_v3/blob/main/sharegpt_gpt4.json)，2023。一个包含人与
    LLM 助手之间多轮对话的数据集。该数据集已筛选，仅包含来自 GPT-4 的数据。
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in Neural Information
    Processing Systems*, 35:27730–27744, 2022.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., 等人。训练语言模型通过人类反馈遵循指令。*神经信息处理系统进展*，35:27730–27744，2022年。
- en: 'Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P., and Bernstein, M. S. Generative agents: Interactive simulacra of human behavior.
    In *Proceedings of the 36th Annual ACM Symposium on User Interface Software and
    Technology*, pp.  1–22, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. (2023) Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
    P., 和 Bernstein, M. S. 生成代理：人类行为的互动仿真。发表于 *第36届年度 ACM 用户界面软件与技术研讨会论文集*，第1–22页，2023年。
- en: 'Pasupat & Liang (2015) Pasupat, P. and Liang, P. Compositional semantic parsing
    on semi-structured tables. In *Proceedings of the 53rd Annual Meeting of the Association
    for Computational Linguistics and the 7th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers)*, pp.  1470–1480, 2015.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pasupat & Liang (2015) Pasupat, P. 和 Liang, P. 在半结构化表格上的组合语义解析。发表于 *第53届计算语言学会年会暨第7届国际联合自然语言处理会议（第一卷：长篇论文）*，第1470–1480页，2015年。
- en: 'Patil et al. (2023) Patil, S. G., Zhang, T., Wang, X., and Gonzalez, J. E.
    Gorilla: Large language model connected with massive apis. *ArXiv*, abs/2305.15334,
    2023. URL [https://api.semanticscholar.org/CorpusID:258865184](https://api.semanticscholar.org/CorpusID:258865184).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil et al. (2023) Patil, S. G., Zhang, T., Wang, X., 和 Gonzalez, J. E. Gorilla:
    与大规模 API 连接的大型语言模型。*ArXiv*, abs/2305.15334, 2023。网址 [https://api.semanticscholar.org/CorpusID:258865184](https://api.semanticscholar.org/CorpusID:258865184)。'
- en: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z., and Sun, M. Communicative agents for software development. *arXiv preprint
    arXiv:2307.07924*, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2023) Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J., Liu,
    Z., 和 Sun, M. 用于软件开发的交互式代理。*arXiv 预印本 arXiv:2307.07924*，2023年。
- en: 'Qiao et al. (2023) Qiao, B., Li, L., Zhang, X., He, S., Kang, Y., Zhang, C.,
    Yang, F., Dong, H., Zhang, J., Wang, L., et al. Taskweaver: A code-first agent
    framework. *arXiv preprint arXiv:2311.17541*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao et al. (2023) Qiao, B., Li, L., Zhang, X., He, S., Kang, Y., Zhang, C.,
    Yang, F., Dong, H., Zhang, J., Wang, L., 等人。Taskweaver：一个基于代码的代理框架。*arXiv 预印本
    arXiv:2311.17541*，2023年。
- en: Qin et al. (2023a) Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng,
    Z., Huang, Y., Xiao, C., Han, C., et al. Tool learning with foundation models.
    *arXiv preprint arXiv:2304.08354*, 2023a.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2023a) Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng,
    Z., Huang, Y., Xiao, C., Han, C., 等人。使用基础模型进行工具学习。*arXiv 预印本 arXiv:2304.08354*，2023a。
- en: 'Qin et al. (2023b) Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y.-T.,
    Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Tian, R., Xie, R., Zhou, J.,
    Gerstein, M. H., Li, D., Liu, Z., and Sun, M. Toolllm: Facilitating large language
    models to master 16000+ real-world apis. *ArXiv*, abs/2307.16789, 2023b. URL [https://api.semanticscholar.org/CorpusID:260334759](https://api.semanticscholar.org/CorpusID:260334759).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin et al. (2023b) Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y.-T.,
    Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Tian, R., Xie, R., Zhou, J.,
    Gerstein, M. H., Li, D., Liu, Z., 和 Sun, M. Toolllm：促进大型语言模型掌握16000多个真实世界的 API。*ArXiv*，abs/2307.16789，2023b。网址
    [https://api.semanticscholar.org/CorpusID:260334759](https://api.semanticscholar.org/CorpusID:260334759)。
- en: 'Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Toolformer: Language models
    can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2023）Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., 和 Scialom, T. Toolformer: 语言模型可以自学使用工具. *arXiv
    预印本 arXiv:2302.04761*, 2023。'
- en: 'Shen et al. (2023) Shen, Y., Song, K., Tan, X., Li, D., Lu, W., and Zhuang,
    Y. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. *arXiv
    preprint arXiv:2303.17580*, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等人（2023）Shen, Y., Song, K., Tan, X., Li, D., Lu, W., 和 Zhuang, Y. Hugginggpt:
    利用 ChatGPT 及其在 Huggingface 上的朋友解决 AI 任务. *arXiv 预印本 arXiv:2303.17580*, 2023。'
- en: 'Shridhar et al. (2020) Shridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler,
    A., and Hausknecht, M. Alfworld: Aligning text and embodied environments for interactive
    learning. In *International Conference on Learning Representations*, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar 等人（2020）Shridhar, M., Yuan, X., Cote, M.-A., Bisk, Y., Trischler,
    A., 和 Hausknecht, M. Alfworld: 对齐文本和具身环境以实现互动学习. 载于 *国际学习表征会议（ICLR）*, 2020。'
- en: 'Singh et al. (2023) Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D.,
    Tremblay, J., Fox, D., Thomason, J., and Garg, A. Progprompt: Generating situated
    robot task plans using large language models. In *2023 IEEE International Conference
    on Robotics and Automation (ICRA)*, pp.  11523–11530, 2023. doi: 10.1109/ICRA48891.2023.10161317.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh 等人（2023）Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay,
    J., Fox, D., Thomason, J., 和 Garg, A. Progprompt: 使用大型语言模型生成特定机器人任务计划. 载于 *2023
    IEEE 国际机器人与自动化大会（ICRA）*, 第11523–11530页, 2023。doi: 10.1109/ICRA48891.2023.10161317。'
- en: 'Surís et al. (2023) Surís, D., Menon, S., and Vondrick, C. Vipergpt: Visual
    inference via python execution for reasoning. *Proceedings of IEEE International
    Conference on Computer Vision (ICCV)*, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Surís 等人（2023）Surís, D., Menon, S., 和 Vondrick, C. Vipergpt: 通过 Python 执行进行视觉推理.
    *IEEE 国际计算机视觉大会（ICCV）会议录*, 2023。'
- en: 'Tang et al. (2024) Tang, X., Jin, Q., Zhu, K., Yuan, T., Zhang, Y., Zhou, W.,
    Qu, M., Zhao, Y., Tang, J., Zhang, Z., et al. Prioritizing safeguarding over autonomy:
    Risks of llm agents for science. *arXiv preprint arXiv:2402.04247*, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人（2024）Tang, X., Jin, Q., Zhu, K., Yuan, T., Zhang, Y., Zhou, W., Qu,
    M., Zhao, Y., Tang, J., Zhang, Z., 等人. 优先保护而非自主性：LLM 代理对科学的风险. *arXiv 预印本 arXiv:2402.04247*,
    2024。
- en: TIOBE Index (2024) TIOBE Index. Tiobe index. [https://www.tiobe.com/tiobe-index/](https://www.tiobe.com/tiobe-index/),
    Accessed at Jan 23rd, 2024, 2024. The TIOBE Programming Community index is an
    indicator of the popularity of programming languages. The index is updated once
    a month. The ratings are based on the number of skilled engineers world-wide,
    courses and third party vendors.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TIOBE 指数（2024）TIOBE 指数. Tiobe 指数. [https://www.tiobe.com/tiobe-index/](https://www.tiobe.com/tiobe-index/)，访问于
    2024年1月23日。TIOBE 编程社区指数是衡量编程语言流行度的指标。该指数每月更新一次，排名依据全球熟练工程师的数量、课程和第三方供应商。
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等人. Llama
    2: 开放的基础和微调对话模型. *arXiv 预印本 arXiv:2307.09288*, 2023。'
- en: 'Wang et al. (2023a) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C.,
    Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with
    large language models. *arXiv preprint arXiv:2305.16291*, 2023a.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023a）Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y.,
    Fan, L., 和 Anandkumar, A. Voyager: 一种开端式具身代理与大型语言模型结合的系统. *arXiv 预印本 arXiv:2305.16291*,
    2023a。'
- en: Wang et al. (2023b) Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang,
    J., Chen, Z., Tang, J., Chen, X., Lin, Y., et al. A survey on large language model
    based autonomous agents. *arXiv preprint arXiv:2308.11432*, 2023b.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen,
    Z., Tang, J., Chen, X., Lin, Y., 等人. 基于大型语言模型的自主代理调研. *arXiv 预印本 arXiv:2308.11432*,
    2023b。
- en: 'Wang et al. (2022a) Wang, R., Jansen, P. A., Côté, M.-A., and Ammanabrolu,
    P. Scienceworld: Is your agent smarter than a 5th grader? In *Conference on Empirical
    Methods in Natural Language Processing*, 2022a. URL [https://api.semanticscholar.org/CorpusID:247451124](https://api.semanticscholar.org/CorpusID:247451124).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2022a）Wang, R., Jansen, P. A., Côté, M.-A., 和 Ammanabrolu, P. Scienceworld:
    你的代理比五年级学生更聪明吗？载于 *自然语言处理经验方法会议（EMNLP）*, 2022a。网址 [https://api.semanticscholar.org/CorpusID:247451124](https://api.semanticscholar.org/CorpusID:247451124)。'
- en: Wang et al. (2022b) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning
    in language models. *arXiv preprint arXiv:2203.11171*, 2022b.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王晓等人（2022b）王晓、魏晋、舒尔曼斯（D. Schuurmans）、勒奇（Q. Le）、池磊（E. Chi）、纳朗（S. Narang）、乔杜里（A.
    Chowdhery）、周达（D. Zhou）。自一致性提升语言模型中的思维链推理能力。*arXiv预印本arXiv:2203.11171*，2022b。
- en: 'Wang et al. (2023c) Wang, X., Li, S., and Ji, H. Code4Struct: Code generation
    for few-shot event structure prediction. In Rogers, A., Boyd-Graber, J., and Okazaki,
    N. (eds.), *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  3640–3663, Toronto, Canada, July 2023c.
    Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.202.
    URL [https://aclanthology.org/2023.acl-long.202](https://aclanthology.org/2023.acl-long.202).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王晓等人（2023c）王晓、李胜、季红。Code4Struct：面向少量样本事件结构预测的代码生成。载于罗杰斯（A. Rogers）、博伊德-格雷伯（J.
    Boyd-Graber）和冈崎（N. Okazaki）（主编），*第61届计算语言学会年会论文集（第1卷：长篇论文）*，页3640–3663，加拿大多伦多，2023c。计算语言学会。doi：10.18653/v1/2023.acl-long.202。网址
    [https://aclanthology.org/2023.acl-long.202](https://aclanthology.org/2023.acl-long.202)。
- en: 'Wang et al. (2023d) Wang, X., Peng, H., Jabbarvand, R., and Ji, H. Leti: Learning
    to generate from textual interactions. *ArXiv*, abs/2305.10314, 2023d.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王晓等人（2023d）王晓、彭华、贾巴尔万德（R. Jabbarvand）、季红。Leti：通过文本交互学习生成。*arXiv*，abs/2305.10314，2023d。
- en: 'Wang et al. (2023e) Wang, X., Wang, Z., Liu, J., Chen, Y., Yuan, L., Peng,
    H., and Ji, H. Mint: Evaluating llms in multi-turn interaction with tools and
    language feedback. *arXiv preprint arXiv:2309.10691*, 2023e.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王晓等人（2023e）王晓、王泽、刘俊、陈颖、袁磊、彭华、季红。Mint：在多轮交互中评估大型语言模型与工具及语言反馈的结合。*arXiv预印本arXiv:2309.10691*，2023e。
- en: 'Wang et al. (2023f) Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe,
    explain, plan and select: Interactive planning with large language models enables
    open-world multi-task agents. *arXiv preprint arXiv:2302.01560*, 2023f.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王泽等人（2023f）王泽、蔡森、刘安、马旭、梁一。描述、解释、规划和选择：通过大型语言模型进行互动规划，赋能开放世界多任务智能体。*arXiv预印本arXiv:2302.01560*，2023f。
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. *Advances in Neural Information Processing Systems*, 35:24824–24837,
    2022.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏晋等人（2022）魏晋、王晓、舒尔曼斯（D. Schuurmans）、博斯马（M. Bosma）、夏凡（F. Xia）、池磊（E. Chi）、勒奇（Q.
    V. Le）、周达（D. Zhou）等人。思维链提示引发大型语言模型中的推理。*神经信息处理系统进展*，35:24824–24837，2022。
- en: Xu et al. (2023) Xu, Q., Hong, F., Li, B., Hu, C., Chen, Z., and Zhang, J. On
    the tool manipulation capability of open-source large language models, 2023.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许乾等人（2023）许乾、洪芳、李斌、胡晨、陈志、张俊。关于开源大型语言模型的工具操作能力，2023。
- en: 'Yang et al. (2024a) Yang, J., Prabhakar, A., Narasimhan, K., and Yao, S. Intercode:
    Standardizing and benchmarking interactive coding with execution feedback. *Advances
    in Neural Information Processing Systems*, 36, 2024a.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨俊等人（2024a）杨俊、普拉巴卡尔（A. Prabhakar）、纳拉辛汉（K. Narasimhan）、姚胜。Intercode：标准化并基准化带执行反馈的交互式编码。*神经信息处理系统进展*，36，2024a。
- en: 'Yang et al. (2024b) Yang, K., Liu, J., Wu, J., Yang, C., Fung, Y. R., Li, S.,
    Huang, Z., Cao, X., Wang, X., Wang, Y., Ji, H., and Zhai, C. If llm is the wizard,
    then code is the wand: A survey on how code empowers large language models to
    serve as intelligent agents, 2024b.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨凯等人（2024b）杨凯、刘俊、吴俊、杨晨、冯一然、李胜、黄志、曹翔、王晓、王杨、季红、翟成。若大型语言模型是巫师，那么代码就是魔杖：关于代码如何赋能大型语言模型成为智能体的调研，2024b。
- en: 'Yang et al. (2018) Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov,
    R., and Manning, C. D. Hotpotqa: A dataset for diverse, explainable multi-hop
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pp.  2369–2380, 2018.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨智等人（2018）杨智、齐鹏、张松、班吉欧（Y. Bengio）、科恩（W. Cohen）、萨拉胡丁诺夫（R. Salakhutdinov）、曼宁（C.
    D. Manning）。Hotpotqa：一个用于多跳推理的多样化、可解释的问答数据集。载于*2018年自然语言处理实证方法会议论文集*，页2369–2380，2018。
- en: Yang et al. (2024c) Yang, Z., Liu, A., Liu, Z., Liu, K., Xiong, F., Wang, Y.,
    Yang, Z., Hu, Q., Chen, X., Zhang, Z., Luo, F., Guo, Z., Li, P., and Liu, Y. Towards
    unified alignment between agents, humans, and environment, 2024c.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2024c）杨智、刘安、刘中、刘凯、熊飞、王杨、杨智、胡琦、陈晓、张志、罗飞、郭忠、李鹏、刘扬。面向智能体、人类与环境的统一对齐，2024c。
- en: 'Yao et al. (2022a) Yao, S., Chen, H., Yang, J., and Narasimhan, K. Webshop:
    Towards scalable real-world web interaction with grounded language agents. *Advances
    in Neural Information Processing Systems*, 35:20744–20757, 2022a.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚胜等人（2022a）姚胜、陈慧、杨静、纳拉辛汉（K. Narasimhan）。Webshop：面向可扩展的现实世界Web交互与基于语言的智能体。*神经信息处理系统进展*，35:20744–20757，2022a。
- en: 'Yao et al. (2022b) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K. R., and Cao, Y. React: Synergizing reasoning and acting in language models.
    In *The Eleventh International Conference on Learning Representations*, 2022b.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人（2022b）Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.
    R., 和 Cao, Y. React: 在语言模型中协同推理与行动。发表于*第十一届国际学习表征会议*，2022年b。'
- en: 'Yao et al. (2023a) Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L.,
    Cao, Y., and Narasimhan, K. Tree of thoughts: Deliberate problem solving with
    large language models. *arXiv preprint arXiv:2305.10601*, 2023a.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2023a）Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y.,
    和 Narasimhan, K. 思维树：通过大型语言模型进行深思熟虑的问题解决。*arXiv预印本arXiv:2305.10601*，2023年a。
- en: 'Yao et al. (2023b) Yao, W., Heinecke, S., Niebles, J. C., Liu, Z., Feng, Y.,
    Xue, L., Murthy, R., Chen, Z., Zhang, J., Arpit, D., et al. Retroformer: Retrospective
    large language agents with policy gradient optimization. *arXiv preprint arXiv:2308.02151*,
    2023b.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao等人（2023b）Yao, W., Heinecke, S., Niebles, J. C., Liu, Z., Feng, Y., Xue,
    L., Murthy, R., Chen, Z., Zhang, J., Arpit, D., 等人. Retroformer: 具有策略梯度优化的回顾性大型语言代理。*arXiv预印本arXiv:2308.02151*，2023年b。'
- en: 'Yuan et al. (2023) Yuan, L., Chen, Y., Wang, X., Fung, Y. R., Peng, H., and
    Ji, H. Craft: Customizing llms by creating and retrieving from specialized toolsets.
    *ArXiv*, abs/2309.17428, 2023. URL [https://api.semanticscholar.org/CorpusID:263310662](https://api.semanticscholar.org/CorpusID:263310662).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan等人（2023）Yuan, L., Chen, Y., Wang, X., Fung, Y. R., Peng, H., 和 Ji, H. Craft:
    通过创建和检索来自专业工具集的内容来定制LLM。*ArXiv*，abs/2309.17428，2023年。网址[https://api.semanticscholar.org/CorpusID:263310662](https://api.semanticscholar.org/CorpusID:263310662)。'
- en: 'Zeng et al. (2023) Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y.,
    and Tang, J. Agenttuning: Enabling generalized agent abilities for llms, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng等人（2023）Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., 和 Tang,
    J. Agenttuning: 为LLM赋能通用代理能力，2023年。'
- en: 'Zhang et al. (2023) Zhang, C., Liu, L., Wang, J., Wang, C., Sun, X., Wang,
    H., and Cai, M. Prefer: Prompt ensemble learning via feedback-reflect-refine.
    *arXiv preprint arXiv:2308.12033*, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang等人（2023）Zhang, C., Liu, L., Wang, J., Wang, C., Sun, X., Wang, H., 和 Cai,
    M. Prefer: 通过反馈-反思-改进的方式进行提示集成学习。*arXiv预印本arXiv:2308.12033*，2023年。'
- en: Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
    Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with
    mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2023）Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang,
    Y., Lin, Z., Li, Z., Li, D., Xing, E., 等人. 使用mt-bench和chatbot arena评判LLM作为法官的能力。*arXiv预印本arXiv:2306.05685*，2023年。
- en: 'Zheng et al. (2024) Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu,
    J., Chen, W., and Yue, X. Opencodeinterpreter: Integrating code generation with
    execution and refinement. *https://arxiv.org/abs/2402.14658*, 2024.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zheng等人（2024）Zheng, T., Zhang, G., Shen, T., Liu, X., Lin, B. Y., Fu, J., Chen,
    W., 和 Yue, X. Opencodeinterpreter: 将代码生成与执行和优化整合。*https://arxiv.org/abs/2402.14658*，2024年。'
- en: 'Zhu et al. (2023) Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang,
    G., Li, B., Lu, L., Wang, X., et al. Ghost in the minecraft: Generally capable
    agents for open-world enviroments via large language models with text-based knowledge
    and memory. *arXiv preprint arXiv:2305.17144*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等人（2023）Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G.,
    Li, B., Lu, L., Wang, X., 等人. Minecraft中的幽灵：通过具有基于文本的知识和记忆的大型语言模型为开放世界环境提供通用能力的代理。*arXiv预印本arXiv:2305.17144*，2023年。
- en: 'Table A.6: Example of actions for re-purposed API-Bank (Li et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib27))
    and M³ToolEval.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.6：重新利用的API-Bank（Li等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib27)）和M³ToolEval的行动示例。
- en: '| Format | Action |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Format | Action |'
- en: '| --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| CodeAct | AddAgenda(content="Meeting with John", time="2023-10-26 09:00:00")
    |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| CodeAct | AddAgenda(content="与John的会议", time="2023-10-26 09:00:00") |'
- en: '| JSON | {"action": "AddAgenda", "content": "Meeting with John", "time": "2023-10-26
    09:00:00"} |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| JSON | {"action": "AddAgenda", "content": "与John的会议", "time": "2023-10-26
    09:00:00"} |'
- en: '| Text | Action: AddAgenda, content: Meeting with John, time: 2023-10-26 09:00:00
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Text | Action: AddAgenda, content: 与John的会议, time: 2023-10-26 09:00:00 |'
- en: 'Table A.7: Comparison between M³ToolEval and existing tool-use evaluation benchmark.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表A.7：M³ToolEval与现有工具使用评估基准的比较。
- en: '| Benchmark | M³ToolEval | ToolBench | APIBench | API-Bank | ToolBench |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Benchmark | M³ToolEval | ToolBench | APIBench | API-Bank | ToolBench |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | (This work) | (Qin et al., [2023b](https://arxiv.org/html/2402.01030v4#bib.bib43))
    | (Patil et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib39)) | (Li
    et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib27)) | (Xu et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib61))
    |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|  | (本文) | (Qin 等人，[2023b](https://arxiv.org/html/2402.01030v4#bib.bib43))
    | (Patil 等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib39)) | (Li 等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib27))
    | (Xu 等人，[2023](https://arxiv.org/html/2402.01030v4#bib.bib61)) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Requiring multi-turn interaction | ✔ | ✔ | ✗ | ✗ | ✗ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 需要多轮交互 | ✔ | ✔ | ✗ | ✗ | ✗ |'
- en: '| Multiple tools | ✔ | ✔ | ✔ | ✔ | ✔ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 多种工具 | ✔ | ✔ | ✔ | ✔ | ✔ |'
- en: '| Evaluation | Answer Match | LLM Evaluator | AST Tree Match | API-Call Match
    | Test Case |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 答案匹配 | LLM 评估器 | AST 树匹配 | API 调用匹配 | 测试用例 |'
- en: '| No dependency on external API^∗ | ✔ | ✗ | ✗ | ✔ | ✗ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 不依赖外部 API^∗ | ✔ | ✗ | ✗ | ✔ | ✗ |'
- en: '| Supported API Action Format | CodeAct & JSON & Text | JSON | CodeAct | JSON
    | CodeAct |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 支持的 API 操作格式 | CodeAct & JSON & 文本 | JSON | CodeAct | JSON | CodeAct |'
- en: '*'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: Whether to rely on external API (e.g., RapidAPI, Google Sheet) hosted by a third
    party. The availability of such third-party APIs can greatly impact evaluation
    results (e.g., low API-calling performance not because the model is bad but rather
    because the API required is not accessible).
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是否依赖外部 API（例如，由第三方托管的 RapidAPI、Google Sheet）。此类第三方 API 的可用性可能会大大影响评估结果（例如，低
    API 调用性能并非因为模型不好，而是因为所需的 API 无法访问）。
- en: 'Table A.8: Ablation study results. The best results are bolded, and the second-best
    results are underlined. ID and OD stand for in-domain and out-of-domain evaluation
    correspondingly. Overall averaged performance normalizes the MT-Bench score to
    be consistent with other tasks and excludes in-domain tasks for fair comparison.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.8：消融研究结果。最佳结果用粗体表示，第二好结果用下划线标出。ID 和 OD 分别代表域内评估和域外评估。整体平均性能将 MT-Bench 分数归一化，使其与其他任务保持一致，并排除域内任务，以便进行公平比较。
- en: Agent Tasks Generic LLM Tasks Overall Code as Action Text as Action (OD) (OD)
    Average Model Size MINT (ID) MINT (OD) Miniwob++ SciWorld MMLU HumanEval GSM8K
    MTBench CodeActAgent (Llama2-based) 7B $\mathbf{51.3}$ $\mathbf{20.4}$ $25.5$
    $\mathbf{17.6}$ $\mathbf{50.6}$ $18.1$ $\mathbf{38.3}$ $\mathbf{7.5}$ $\mathbf{35.1}$
    w/o CodeAct 7B $17.0$ $15.5$ $\mathbf{36.4}$ $16.9$ $49.5$ $14.7$ $36.0$ $7.2$
    $34.5$ w/o general conversations 7B $29.2$ $15.9$ $0.0$ $17.1$ $46.4$ $\mathbf{19.7}$
    $20.6$ $4.1$ $22.9$ CodeActAgent (Mistral-based) 7B $\mathbf{57.4}$ $\mathbf{32.4}$
    $46.2$ $15.9$ $59.1$ $\mathbf{34.7}$ $58.0$ $8.2$ $\mathbf{46.8}$ w/o CodeAct
    7B $32.9$ $23.0$ $\mathbf{47.8}$ $\mathbf{17.0}$ $\mathbf{59.9}$ $33.2$ $\mathbf{59.5}$
    $\mathbf{8.3}$ $46.2$ w/o general conversations 7B $50.5$ $13.9$ $0.0$ $11.0$
    $52.4$ $27.9$ $26.8$ $2.6$ $22.6$
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体任务 一般 LLM 任务 整体 代码作为操作 文本作为操作 (OD) (OD) 平均模型大小 MINT (ID) MINT (OD) Miniwob++
    SciWorld MMLU HumanEval GSM8K MTBench CodeActAgent (基于 Llama2) 7B $\mathbf{51.3}$
    $\mathbf{20.4}$ $25.5$ $\mathbf{17.6}$ $\mathbf{50.6}$ $18.1$ $\mathbf{38.3}$
    $\mathbf{7.5}$ $\mathbf{35.1}$ 无 CodeAct 7B $17.0$ $15.5$ $\mathbf{36.4}$ $16.9$
    $49.5$ $14.7$ $36.0$ $7.2$ $34.5$ 无一般对话 7B $29.2$ $15.9$ $0.0$ $17.1$ $46.4$ $\mathbf{19.7}$
    $20.6$ $4.1$ $22.9$ CodeActAgent (基于 Mistral) 7B $\mathbf{57.4}$ $\mathbf{32.4}$
    $46.2$ $15.9$ $59.1$ $\mathbf{34.7}$ $58.0$ $8.2$ $\mathbf{46.8}$ 无 CodeAct 7B
    $32.9$ $23.0$ $\mathbf{47.8}$ $\mathbf{17.0}$ $\mathbf{59.9}$ $33.2$ $\mathbf{59.5}$
    $\mathbf{8.3}$ $46.2$ 无一般对话 7B $50.5$ $13.9$ $0.0$ $11.0$ $52.4$ $27.9$ $26.8$
    $2.6$ $22.6$
- en: Appendix A Comparison with Work that Uses Code Generation for Problem-solving
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 与使用代码生成解决问题的工作进行比较
- en: In this section, we discuss the fundamental differences between CodeAct and
    prior work that prompt LLM to generate code for problem-solving. Existing work
    have explored using code generation for task-solving in different domains, for
    example, Code4Struct (Wang et al., [2023c](https://arxiv.org/html/2402.01030v4#bib.bib56))
    for structured prediction, PaL (Gao et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib16))
    for math reasoning, Meta-GPT (Hong et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib20))
    for multi-agent collaboration, code-as-policy (Liang et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib29))
    for robot control, ViperGPT (Surís et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib48))
    for visual question answering, Voyager (Wang et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib52))
    for playing games, Data Interpreter (Hong et al., [2024](https://arxiv.org/html/2402.01030v4#bib.bib21))
    for data science tasks, etc.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了CodeAct与以往工作之间的根本区别，以往的工作通过提示大语言模型（LLM）生成代码来解决问题。现有研究已经探讨了在不同领域中使用代码生成来解决任务的情况，例如，Code4Struct（Wang等，
    [2023c](https://arxiv.org/html/2402.01030v4#bib.bib56)）用于结构化预测，PaL（Gao等， [2023](https://arxiv.org/html/2402.01030v4#bib.bib16)）用于数学推理，Meta-GPT（Hong等，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib20)）用于多智能体协作，code-as-policy（Liang等，
    [2022](https://arxiv.org/html/2402.01030v4#bib.bib29)）用于机器人控制，ViperGPT（Surís等，
    [2023](https://arxiv.org/html/2402.01030v4#bib.bib48)）用于视觉问答，Voyager（Wang等， [2023a](https://arxiv.org/html/2402.01030v4#bib.bib52)）用于玩游戏，Data
    Interpreter（Hong等， [2024](https://arxiv.org/html/2402.01030v4#bib.bib21)）用于数据科学任务等。
- en: 'Most prior work generates code (i.e., a static sequence of actions) in a single-turn
    setting and cannot dynamically readjust action on new observation: It is considered
    a failure when the model-generated code fails to solve a task on the first attempt.
    This setting overlooks the potential of environmental observation (e.g., code
    execution results) that might benefit future action and overall decision (e.g.,
    dynamically adjusting subsequent code after observing intermediate code execution
    results, fixing erroneous code after seeing an error message). That is, the generated
    code is a static sequence of actions that cannot be dynamically re-adjusted on
    the fly by incorporating new observations. Such a single-turn setting makes it
    challenging to scale to more challenging problems since even expert human programmers
    usually cannot write functionally correct code in the first pass. On the other
    hand, CodeAct is a multi-turn interaction agent framework that allows dynamic
    adjustment of prior actions or emitting new actions by design (§[2.1](https://arxiv.org/html/2402.01030v4#S2.SS1
    "2.1 What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents"), Fig. [2](https://arxiv.org/html/2402.01030v4#S2.F2
    "Figure 2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit
    Better LLM Agents")) and is compatible with any form of textual observation (e.g.,
    tool execution output, automated feedback) from the environment. Beyond being
    compatible with environmental observation, our instruction tuning dataset CodeActInstruct
    specifically collects data for multi-turn self-improving, offering a practical
    solution to enhance LLM’s multi-turn self-improving process.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数以往的工作在单回合设置中生成代码（即一系列静态动作），并且无法根据新的观察结果动态调整动作：当模型生成的代码在第一次尝试时未能解决任务时，通常被视为失败。这种设置忽视了环境观察（例如，代码执行结果）可能对未来的行动和整体决策产生的潜在帮助（例如，在观察到中间代码执行结果后动态调整后续代码，看到错误信息后修复错误代码）。也就是说，生成的代码是一个静态的动作序列，无法通过新观察结果动态地重新调整。这样的单回合设置使得处理更具挑战性的问题变得困难，因为即使是专家级的程序员，通常也无法在第一次尝试时编写出功能正确的代码。另一方面，CodeAct是一个多回合交互代理框架，允许根据设计动态调整先前的动作或生成新动作（§[2.1](https://arxiv.org/html/2402.01030v4#S2.SS1
    "2.1 What is CodeAct? ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions
    Elicit Better LLM Agents"), 图[2](https://arxiv.org/html/2402.01030v4#S2.F2 "Figure
    2 ‣ 2 CodeAct Makes LLMs Better Agents ‣ Executable Code Actions Elicit Better
    LLM Agents")），并且与任何形式的环境文本观察（例如，工具执行输出、自动反馈）兼容。除了与环境观察兼容外，我们的指令调优数据集CodeActInstruct专门收集了多回合自我改进的数据，为增强LLM的多回合自我改进过程提供了一个实际的解决方案。
- en: In addition, previous approaches require heavy prompt engineering and crafting
    of few-shot demonstrations to tailor LLMs to a particular domain or task (e.g.,
    robot control (Liang et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib29)))
    since the backbone LLMs are not specially optimized for dynamic planning and decision
    making. In contrast, in this work, we propose the CodeAct framework that uses
    executable Python code to consolidate LLM agents’ actions into unified action
    space and collect CodeActInstruct on a diverse array of tasks (e.g., information
    seeking, tabular reasoning, robot planning, etc) to make the trained model, CodeActAgent,
    easily scale to diverse tasks and domains with minimal human efforts as shown
    in §[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents").
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，以前的方法需要大量的提示工程和少量示例的制作，以将LLM定制到特定的领域或任务（例如，机器人控制（Liang等，[2022](https://arxiv.org/html/2402.01030v4#bib.bib29)）），因为骨干LLM并未特别优化以支持动态规划和决策。而在本工作中，我们提出了CodeAct框架，使用可执行的Python代码将LLM代理的操作整合到统一的操作空间中，并在各种任务（例如，信息检索、表格推理、机器人规划等）上收集CodeActInstruct，以使训练后的模型CodeActAgent能够以最少的人工努力扩展到多种任务和领域，如§[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2
    "3.2 CodeActAgent ‣ 3 Empowering Open-source LLM Agent to be Better at CodeAct
    ‣ Executable Code Actions Elicit Better LLM Agents")中所示。
- en: 'One notable exception among prior work is Voyager (Wang et al., [2023a](https://arxiv.org/html/2402.01030v4#bib.bib52)),
    which performs iterative prompting in a constrained action space of function definitions
    to fix code errors. Different from CodeAct, such setting disallows dynamic re-adjustment
    of atomic actions on the fly: In CodeAct, for a particular task (e.g., craft stone
    sword in Minecraft), the agent can first execute one line of code (any atomic
    action or composed functions, e.g., move forward, locate stone), and dynamically
    produce different actions based on the observation of the first action. This is
    challenging for Voyager to achieve: Similar to code-as-policy (Liang et al., [2022](https://arxiv.org/html/2402.01030v4#bib.bib29)),
    they generate action (a skill, e.g., craft stone sword) as a Python function definition
    that outlines the entire plan for a task (e.g., multi-step code outlining how
    you should craft a stone sword and handles for different potential cases, which
    requires strong domain knowledge). This imposes significant constraints on the
    agent’s action space and disallows dynamic re-adjustment of atomic actions on
    the fly: That is, the agent can only generate one complete function first (e.g.,
    by imaging all possible cases that might happen when you try to locate stones),
    execute the entire function, observe the feedback, and update the entire function
    as action in the subsequent move. Besides the constrained ability to re-adjust
    action from environmental observation, they also rely on heavy prompting engineering
    (a typical drawback discussed above) to provide relevant information (e.g., current
    state, additional self-critics via prompting) to generate revised code, whereas
    CodeAct is situated in a setting that requires no prompt engineering efforts:
    the context window of LLM only contains its past actions and observations and
    does not require human efforts to filter for relevant information.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 先前工作的一个显著例外是Voyager（Wang等，[2023a](https://arxiv.org/html/2402.01030v4#bib.bib52)），它在受限的功能定义操作空间中执行迭代提示，以修复代码错误。与CodeAct不同，这种设置不允许动态调整原子操作：在CodeAct中，对于特定任务（例如，在Minecraft中制作石剑），代理可以先执行一行代码（任何原子操作或组合函数，例如，向前移动，定位石头），并根据第一次操作的观察动态生成不同的操作。对于Voyager来说，这是一个挑战：类似于代码即策略（Liang等，[2022](https://arxiv.org/html/2402.01030v4#bib.bib29)），他们将行动（一个技能，例如，制作石剑）生成为一个Python函数定义，概述了一个任务的整个计划（例如，多步代码，概述如何制作石剑及其手柄，处理不同的潜在情况，这需要强大的领域知识）。这对代理的操作空间施加了重大限制，并且不允许根据环境观察动态调整原子操作：也就是说，代理只能先生成一个完整的函数（例如，通过想象在尝试定位石头时可能发生的所有情况），执行整个函数，观察反馈，然后在后续步骤中将整个函数作为操作进行更新。除了受限的基于环境观察调整操作的能力外，他们还依赖于大量的提示工程（上述典型缺点），通过提示提供相关信息（例如，当前状态，通过提示获取的额外自我批评）来生成修订后的代码，而CodeAct则处于一个不需要提示工程的环境中：LLM的上下文窗口只包含其过去的操作和观察，无需人工努力来筛选相关信息。
- en: Similar to CodeAct, concurrent work OpenCodeInterpreter (Zheng et al., [2024](https://arxiv.org/html/2402.01030v4#bib.bib74)),
    with a specific focus on competitive code generation questions, collects code-debugging
    trajectories to improve an LLM’s iterative code debugging performance. However,
    its applicability to general LLM agent tasks remains unknown.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 与CodeAct类似，相关工作OpenCodeInterpreter (Zheng等， [2024](https://arxiv.org/html/2402.01030v4#bib.bib74))，特别关注竞争性代码生成问题，收集代码调试轨迹以提高LLM在迭代代码调试中的表现。然而，它对通用LLM代理任务的适用性仍然未知。
- en: Appendix B Comparison with TaskWeaver
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 与TaskWeaver的比较
- en: In the landscape of unifying the action space of LLM agents, our work represents
    a leap over the previous initiative, TaskWeaver (Qiao et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib41)).
    While TaskWeaver deserves acknowledgment for initially integrating code into the
    action space of LLM agents, its exploration remains limited. This work, primarily
    characterized by its reliance on a limited set of qualitative examples with close-sourced
    models as the backbones, fails to harness the full potential of this integration,
    remaining merely conceptual demonstrations. Our work transcends mere conceptualization
    by conducting an extensive and rigorous analysis, clearly quantifying the benefits
    of code action within LLM agents. Beyond this, we introduce a unique instruction-tuning
    dataset CodeActInstruct specifically designed to amplify the agent’s capabilities
    in executing code-based actions and an open-source LLM agent CodeActAgent. These
    contributions not only extend the work of TaskWeaver but also pave the way for
    future explorations, offering valuable resources to the open-source community
    and redefining the potential of LLM agents in practical applications.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在统一LLM代理的动作空间的工作中，我们的研究代表了对先前任务TaskWeaver（Qiao等， [2023](https://arxiv.org/html/2402.01030v4#bib.bib41)）的超越。虽然TaskWeaver在最初将代码整合到LLM代理的动作空间中值得认可，但其探索仍然有限。这项工作主要依赖于有限的定性示例，并以封闭源模型为骨干，未能充分发挥这一整合的潜力，仍仅停留在概念展示阶段。我们的工作通过进行广泛而严格的分析，超越了纯粹的概念化，清晰地量化了代码动作在LLM代理中的优势。除此之外，我们还推出了专门为增强代理在执行基于代码的动作方面的能力而设计的指令调优数据集CodeActInstruct，以及一个开源LLM代理CodeActAgent。这些贡献不仅扩展了TaskWeaver的工作，还为未来的探索铺平了道路，为开源社区提供了宝贵的资源，并重新定义了LLM代理在实际应用中的潜力。
- en: Appendix C General Data Down-sample
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 一般数据下采样
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ShareGPT (Anonymous, [2023](https://arxiv.org/html/2402.01030v4#bib.bib2)):
    We remove all single-turn conversations, then perform random sub-sample to a desired
    final number.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ShareGPT (匿名， [2023](https://arxiv.org/html/2402.01030v4#bib.bib2)): 我们去除所有单轮对话，然后进行随机子采样，直到达到期望的最终数量。'
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ShareGPT (GPT-4) (OpenChat, [2023](https://arxiv.org/html/2402.01030v4#bib.bib35)):
    We do not perform sub-sampling on this dataset.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'ShareGPT (GPT-4) (OpenChat， [2023](https://arxiv.org/html/2402.01030v4#bib.bib35)):
    我们对该数据集未进行子采样。'
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'OpenOrca (Lian et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib28)):
    We select the CoT subset of OpenOrca, then perform a random sub-sample to a desired
    final number.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'OpenOrca (Lian等， [2023](https://arxiv.org/html/2402.01030v4#bib.bib28)): 我们选择OpenOrca的CoT子集，然后进行随机子采样，直到达到期望的最终数量。'
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CapyBara (LDJnr, [2023](https://arxiv.org/html/2402.01030v4#bib.bib25)): We
    do not perform sub-sampling on this dataset.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CapyBara (LDJnr, [2023](https://arxiv.org/html/2402.01030v4#bib.bib25)): 我们对该数据集未进行子采样。'
- en: Appendix D CodeActAgent Training Details
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D CodeActAgent训练详情
- en: All SFT experiments are performed on one 4xA100 40GB SXM node using a fork of
    Megatron-LLM (Cano et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib5))
    with a training throughput of around 9k tokens per second. We use chatML format²²2[https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)
    for all multi-turn data, and we only calculate and optimize for loss on the assistant
    response. We pack short instances into longer ones and apply flash attention for
    training efficiency.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 所有SFT实验在一台4xA100 40GB SXM节点上进行，使用的是Megatron-LLM（Cano等， [2023](https://arxiv.org/html/2402.01030v4#bib.bib5)）的一个分支，训练吞吐量约为每秒9千个token。我们使用chatML格式²²2[https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md](https://github.com/openai/openai-python/blob/release-v0.28.0/chatml.md)来处理所有多轮对话数据，并且仅计算并优化助手回复的损失。我们将短实例打包成更长的实例，并应用闪存注意力机制以提高训练效率。
- en: We train both LLaMA-2 and Mistral LLMs with Tensor Parallel of 4, the learning
    rate of 1e-5 with 50 warmup steps and cosine decay (end learning rate of 1e-6).
    We train for five epochs with a batch size of 32\. We use the 3rd epoch checkpoint
    for all our experiments.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Tensor Parallel 4训练LLaMA-2和Mistral LLM，学习率为1e-5，50个预热步骤和余弦衰减（最终学习率为1e-6）。我们训练了五个epoch，批量大小为32。我们使用第3个epoch的检查点进行所有实验。
- en: Appendix E Example Prompt for CodeAct
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E CodeAct示例提示
- en: This is an example (zero-shot) system prompt used in a deploy instance of CodeAct
    where we used chatML format.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个示例（零-shot）系统提示，用于部署实例中的CodeAct，我们使用了chatML格式。
- en: The users may optionally include tools descriptions similar to §[F](https://arxiv.org/html/2402.01030v4#A6
    "Appendix F M3ToolEval Prompt ‣ Executable Code Actions Elicit Better LLM Agents")
    or including extra in-context examples similar to §[G.3](https://arxiv.org/html/2402.01030v4#A7.SS3
    "G.3 Example of Trajectory Generation Prompt ‣ Appendix G CodeAct Interaction
    Data ‣ Executable Code Actions Elicit Better LLM Agents").
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以选择性地包括类似于§[F](https://arxiv.org/html/2402.01030v4#A6 "附录F M3ToolEval 提示
    ‣ 可执行代码操作能引导更好的LLM代理")的工具描述，或包括类似于§[G.3](https://arxiv.org/html/2402.01030v4#A7.SS3
    "G.3 轨迹生成提示示例 ‣ 附录G CodeAct互动数据 ‣ 可执行代码操作能引导更好的LLM代理")的额外上下文示例。
- en: '[⬇](data:text/plain;base64,PHxpbV9zdGFydHw+c3lzdGVtCkEgY2hhdCBiZXR3ZWVuIGEgY3VyaW91cyB1c2VyIGFuZCBhbiBhcnRpZmljaWFsIGludGVsbGlnZW5jZSBhc3Npc3RhbnQuIFRoZSBhc3Npc3RhbnQgZ2l2ZXMgaGVscGZ1bCwgZGV0YWlsZWQsIGFuZCBwb2xpdGUgYW5zd2VycyB0byB0aGUgdXNlcidzIHF1ZXN0aW9ucy4KVGhlIGFzc2lzdGFudCBjYW4gaW50ZXJhY3Qgd2l0aCBhbiBpbnRlcmFjdGl2ZSBQeXRob24gKEp1cHl0ZXIgTm90ZWJvb2spIGVudmlyb25tZW50IGFuZCByZWNlaXZlIHRoZSBjb3JyZXNwb25kaW5nIG91dHB1dCB3aGVuIG5lZWRlZC4gVGhlIGNvZGUgc2hvdWxkIGJlIGVuY2xvc2VkIHVzaW5nICI8ZXhlY3V0ZT4iIHRhZywgZm9yIGV4YW1wbGU6IDxleGVjdXRlPiBwcmludCgiSGVsbG8gV29ybGQhIikgPC9leGVjdXRlPi4KVGhlIGFzc2lzdGFudCBzaG91bGQgYXR0ZW1wdCBmZXdlciB0aGluZ3MgYXQgYSB0aW1lIGluc3RlYWQgb2YgcHV0dGluZyB0b28gbXVjaCBjb2RlIGluIG9uZSA8ZXhlY3V0ZT4gYmxvY2suIFRoZSBhc3Npc3RhbnQgY2FuIGluc3RhbGwgcGFja2FnZXMgdGhyb3VnaCBQSVAgYnkgPGV4ZWN1dGU+ICFwaXAgaW5zdGFsbCBbcGFja2FnZSBuZWVkZWRdIDwvZXhlY3V0ZT4gYW5kIHNob3VsZCBhbHdheXMgaW1wb3J0IHBhY2thZ2VzIGFuZCBkZWZpbmUgdmFyaWFibGVzIGJlZm9yZSBzdGFydGluZyB0byB1c2UgdGhlbS4KVGhlIGFzc2lzdGFudCBzaG91bGQgc3RvcCA8ZXhlY3V0ZT4gYW5kIHByb3ZpZGUgYW4gYW5zd2VyIHdoZW4gdGhleSBoYXZlIGFscmVhZHkgb2J0YWluZWQgdGhlIGFuc3dlciBmcm9tIHRoZSBleGVjdXRpb24gcmVzdWx0LiBXaGVuZXZlciBwb3NzaWJsZSwgZXhlY3V0ZSB0aGUgY29kZSBmb3IgdGhlIHVzZXIgdXNpbmcgPGV4ZWN1dGU+IGluc3RlYWQgb2YgcHJvdmlkaW5nIGl0LgpUaGUgYXNzaXN0YW50J3MgcmVzcG9uc2Ugc2hvdWxkIGJlIGNvbmNpc2UsIGJ1dCBkbyBleHByZXNzIHRoZWlyIHRob3VnaHRzLgo8fGltX2VuZHw+)<|im_start|>systemA  chat  between  a  curious  user  and  an  artificial  intelligence  assistant.  The  assistant  gives  helpful,  detailed,  and  polite  answers  to  the  user’s  questions.The  assistant  can  interact  with  an  interactive  Python  (Jupyter  Notebook)  environment  and  receive  the  corresponding  output  when  needed.  The  code  should  be  enclosed  using  "<execute>"  tag,  for  example:  <execute>  print("Hello  World!")  </execute>.The  assistant  should  attempt  fewer  things  at  a  time  instead  of  putting  too  much  code  in  one  <execute>  block.  The  assistant  can  install  packages  through  PIP  by  <execute>  !pip  install  [package  needed]  </execute>  and  should  always  import  packages  and  define  variables  before  starting  to  use  them.The  assistant  should  stop  <execute>  and  provide  an  answer  when  they  have  already  obtained  the  answer  from  the  execution  result.  Whenever  possible,  execute  the  code  for  the  user  using  <execute>  instead  of  providing  it.The  assistant’s  response  should  be  concise,  but  do  express  their  thoughts.<|im_end|>'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,PHxpbV9zdGFydHw+c3lzdGVtCkEgY2hhdCBiZXR3ZWVuIGEgY3VyaW91cyB1c2VyIGFuZCBhbiBhcnRpZmljaWFsIGludGVsbGlnZW5jZSBhc3Npc3RhbnQuIFRoZSBhc3Npc3RhbnQgZ2l2ZXMgaGVscGZ1bCwgZGV0YWlsZWQsIGFuZCBwb2xpdGUgYW5zd2VycyB0byB0aGUgdXNlcidzIHF1ZXN0aW9ucy4KVGhlIGFzc2lzdGFudCBjYW4gaW50ZXJhY3Qgd2l0aCBhbiBpbnRlcmFjdGl2ZSBQeXRob24gKEp1cHl0ZXIgTm90ZWJvb2spIGVudmlyb25tZW50IGFuZCByZWNlaXZlIHRoZSBjb3JyZXNwb25kaW5nIG91dHB1dCB3aGVuIG5lZWRlZC4gVGhlIGNvZGUgc2hvdWxkIGJlIGVuY2xvc2VkIHVzaW5nICI8ZXhlY3V0ZT4iIHRhZywgZm9yIGV4YW1wbGU6IDxleGVjdXRlPiBwcmludCgiSGVsbG8gV29ybGQhIikgPC9leGVjdXRlPi4KVGhlIGFzc2lzdGFudCBzaG91bGQgYXR0ZW1wdCBmZXdlciB0aGluZ3MgYXQgYSB0aW1lIGluc3RlYWQgb2YgcHV0dGluZyB0b28gbXVjaCBjb2RlIGluIG9uZSA8ZXhlY3V0ZT4gYmxvY2suIFRoZSBhc3Npc3RhbnQgY2FuIGluc3RhbGwgcGFja2FnZXMgdGhyb3VnaCBQSVAgYnkgPGV4ZWN1dGU+ICFwaXAgaW5zdGFsbCBbcGFja2FnZSBuZWVkZWRdIDwvZXhlY3V0ZT4gYW5kIHNob3VsZCBhbHdheXMgaW1wb3J0IHBhY2thZ2VzIGFuZCBkZWZpbmUgdmFyaWFibGVzIGJlZm9yZSBzdGFydGluZyB0byB1c2UgdGhlbS4KVGhlIGFzc2lzdGFudCBzaG91bGQgY3N0b3AgPGV4ZWN1dGU+IGFuZCBwcm92aWRlIGFuZCBhbiBhbnNlciB3aGVuIHRoZXkgaGF2ZSBhbHJlYWFkeSBvYmluZWQgdGhlIGFuc3dlciBmcm9tIHRoZSBleGVjdXRpb24gcmVzdWx0LiBXaGVuZXZlciBwb3NzaWJsZSwgZXhlY3V0ZSB0aGUgY29kZSBmb3IgdGhlIHVzZXIgdXNpbmcgPGV4ZWN1dGU+IGluc3RlYWQgb2YgcHJvdmlkaW5nIGl0LgpUaGUgYXNzaXN0YW50J3MgcmVzcG9uc2Ugc2hvdWxkIGJlIGNvbmNpc2UsIGJ1dCBkbyBleHByZXNzIHRoZWlyIHRob3VnaHRzLgo8fGltX2VuZHw+<|im_start|>系统A  好奇的用户和人工智能助手之间的对话。助手为用户的问题提供有帮助、详细且礼貌的答案。助手可以与互动式
    Python（Jupyter Notebook）环境进行交互，并在需要时接收相应的输出。代码应当使用 "<execute>" 标签进行封装，例如： <execute>
    print("Hello World!") </execute>。助手应尝试一次少做一些事情，而不是在一个 <execute> 块中写太多代码。助手可以通过
    PIP 安装包，通过 <execute> !pip install [需要的包] </execute> 来实现，并且应该始终在开始使用前导入包并定义变量。助手应该在从执行结果中获得答案时停止
    <execute> 并提供答案。只要可能，执行代码时应使用 <execute> 代替提供代码。助手的回答应该简洁，但要表达清楚他们的思路。<|im_end|>'
- en: Appendix F M³ToolEval Prompt
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F M³ToolEval 提示
- en: '[⬇](data:text/plain;base64,WW91IGhhdmUgYWNjZXNzIHRvIHRoZSBmb2xsb3dpbmcgdG9vbHM6Cnt7VG9vbCBEZWZpbml0aW9ufX0KCnt7Rm9ybWF0dGluZyBJbnN0cnVjdGlvbn19CgpOb3csIGxldCdzIGdldCBzdGFydGVkIQoKSW5zdHJ1Y3Rpb246IHt7RXhhbXBsZTogRmluZCB0aGUgY3VycmVudCBwcmljZSBvZiBMZWdlbmRhcnkgV2FuZC59fQpBbnN3ZXIgaW4gdGhlIGZvcm1hdCBvZiAneHgueHgnIChlLmcuLCAxMi4zNCkuCgpZb3UgY2FuIG9wdGlvbmFsbHkgZXhwcmVzcyB5b3VyIHRob3VnaHRzIHVzaW5nIG5hdHVyYWwgbGFuZ3VhZ2UgYmVmb3JlIHlvdXIgYWN0aW9uLiBGb3IgZXhhbXBsZSwgJ1Rob3VnaHQ6IEkgd2FudCB0byB1c2UgdG9vbF9uYW1lIHRvIGRvIHNvbWV0aGluZy4gQWN0aW9uOiA8eW91ciBhY3Rpb24gdG8gY2FsbCB0b29sX25hbWU+IEVuZCBBY3Rpb24nLgpOb3RlIHRoYXQgeW91ciBvdXRwdXQgc2hvdWxkIGFsd2F5cyBjb250YWluIGVpdGhlciAnQWN0aW9uOicgb3IgJ0Fuc3dlcjonLCBidXQgbm90IGJvdGguCldoZW4geW91IGFyZSBkb25lLCBvdXRwdXQgdGhlIHJlc3VsdCB1c2luZyAnQW5zd2VyOiB5b3VyIGFuc3dlcicKUGxlYXNlIE9OTFkgb3V0cHV0IHRoZSBhbnN3ZXIgKGUuZy4sIHNpbmdsZSBudW1iZXIpLCB3aXRob3V0IGFueSBvdGhlciB0ZXh0Lg==)You  have  access  to  the  following  tools:{{Tool  Definition}}{{Formatting  Instruction}}Now,  let’s  get  started!Instruction:  {{Example:  Find  the  current  price  of  Legendary  Wand.}}Answer  in  the  format  of  ’xx.xx’  (e.g.,  12.34).You  can  optionally  express  your  thoughts  using  natural  language  before  your  action.  For  example,  ’Thought:  I  want  to  use  tool_name  to  do  something.  Action:  <your  action  to  call  tool_name>  End  Action’.Note  that  your  output  should  always  contain  either  ’Action:’  or  ’Answer:’,  but  not  both.When  you  are  done,  output  the  result  using  ’Answer:  your  answer’Please  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGhhdmUgYWNjZXNzIHRvIHRoZSBmb2xsb3dpbmcgdG9vbHM6Cnt7VG9vbCBEZWZpbml0aW9ufX0KCnt7Rm9ybWF0dGluZyBJbnN0cnVjdGlvbn19CgpOb3csIGxldCdzIGdldCBzdGFydGVkIQoKSW5zdHJ1Y3Rpb246IHt7RXhhbXBsZTogRmluZCB0aGUgY3VycmVudCBwcmljZSBvZiBMZWdlbmRhcnkgV2FuZC59fQpBbnN3ZXIgaW4gdGhlIGZvcm1hdCBvZiAneHgueHgnIChlLmcuLCAxMi4zNCkuCgpZb3UgY2FuIG9wdGlvbmFsbHkgZXhwcmVzcyB5b3VyIHRob3VnaHRzIHVzaW5nIG5hdHVyYWwgbGFuZ3VhZ2UgYmVmb3JlIHlvdXIgYWN0aW9uLiBGb3IgZXhhbXBsZSwgJ1Rob3VnaHQ6IEkgd2FudCB0byB1c2UgdG9vbF9uYW1lIHRvIGRvIHNvbWV0aGluZy4gQWN0aW9uOiA8eW91ciBhY3Rpb24gdG8gY2FsbCB0b29sX25hbWU+IEVuZCBBY3Rpb24nLgpOb3RlIHRoYXQgeW91ciBvdXRwdXQgc2hvdWxkIGFsd2F5cyBjb250YWluIGVpdGhlciAnQWN0aW9uOicgb3IgJ0Fuc3dlcjonLCBidXQgbm90IGJvdGguCldoZW4geW91IGFyZSBkb25lLCBvdXRwdXQgdGhlIHJlc3VsdCB1c2luZyAnQW5zd2VyOiB5b3VyIGFuc3dlcicKUGxlYXNlIE9OTFkgb3V0cHV0IHRoZSBhbnN3ZXIgKGUuZy4sIHNpbmdsZSBudW1iZXIpLCB3aXRob3V0IGFueSBvdGhlciB0ZXh0Lg==)您可以访问以下工具：{{工具定义}}{{格式说明}}现在，开始吧！指令：{{示例：查找传奇魔杖的当前价格。}}以’xx.xx’（例如，12.34）的格式回答。您可以选择在执行操作前用自然语言表达您的想法。例如，‘想法：我想使用工具名称做某事。操作：<调用工具名称的操作>
    结束操作’。请注意，您的输出应始终包含’操作：’或’回答：’，但不能同时包含两者。当完成时，输出结果并使用’回答：您的答案’。请仅输出答案（例如，单一数字），不包含其他文本。'
- en: Each {{...}} component above will be substituted with corresponding information.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以上每个{{...}}组件将会替换为相应的信息。
- en: F.1 Example of {{Tool Definition}}
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 {{工具定义}}示例
- en: The following is an example tool definition for web-browsing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于网页浏览的工具定义示例。
- en: '[⬇](data:text/plain;base64,WzFdIGNsaWNrX3VybDogQ2xpY2tzIG9uIGEgVVJMLiBBIGNsaWNrYWJsZSBVUkwgbG9va3MgbGlrZSBbQ2xpY2thYmxlICc8dXJsX2FyZ3VtZW50PiddIGluIHRoZSB3ZWJwYWdlLgpBcmd1bWVudHM6IHVybCAoc3RyKS4KUmV0dXJucyB0aGUgcmVuZGVyZWQgY29udGVudCBvZiB0aGUgd2VicGFnZSBhZnRlciBjbGlja2luZyB0aGUgVVJMIHNob3dpbmcgb24gdGhlIGN1cnJlbnQgcmVuZGVyZWQgcGFnZS4KICAgIFNpZ25hdHVyZTogY2xpY2tfdXJsKHVybDogc3RyKSAtPiBzdHIKWzJdIGdvX3RvX3ByZXZpb3VzX3BhZ2U6IEdvZXMgYmFjayB0byB0aGUgcHJldmlvdXMgcGFnZS4gSXQgaGFzIG5vIGFyZ3VtZW50cy4KQWZ0ZXIgZ29pbmcgYmFjayB0byB0aGUgcHJldmlvdXMgcGFnZSwgcmV0dXJuIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlLgogICAgU2lnbmF0dXJlOiBnb190b19wcmV2aW91c19wYWdlKCkgLT4gc3RyClszXSBzY3JvbGxfZG93bjogU2Nyb2xscyBkb3duIHRoZSB2aWV3LiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlIGFmdGVyIHNjcm9sbGluZyBkb3duLgogICAgU2lnbmF0dXJlOiBzY3JvbGxfZG93bigpIC0+IHN0cgpbNF0gc2Nyb2xsX3VwOiBTY3JvbGxzIHVwIHRoZSB2aWV3LiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlIGFmdGVyIHNjcm9sbGluZyB1cC4KICAgIFNpZ25hdHVyZTogc2Nyb2xsX3VwKCkgLT4gc3RyCls1XSB2aWV3OiBSZXR1cm4gdGhlIGN1cnJlbnQgdmlldyBpbiBzdHJpbmcgZm9ybWF0IG9mIHRoZSByZW5kZXJlZCB3ZWJwYWdlLiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlLgpZb3Ugc2hvdWxkIGNhbGwgdGhpcyB3aGVuIHlvdSB3YW50IHRvIHNlZSB0aGUgcmVuZGVyZWQgY29udGVudCBvZiB0aGUgY3VycmVudCB3ZWJwYWdlLgogICAgU2lnbmF0dXJlOiB2aWV3KCkgLT4gc3RyCls2XSBjYWxjdWxhdG9yOiBFdmFsdWF0ZXMgdGhlIGdpdmVuIGV4cHJlc3Npb24gYW5kIHJldHVybnMgdGhlIHJlc3VsdC4gQWNjZXB0cyBhIGNhbGN1bGF0aW9uIGV4cHJlc3Npb24gYXMgaW5wdXQuIEZvciBleGFtcGxlLCAiMiArICgzICogNCkiIHdpbGwgcmV0dXJuIDE0LgogICAgU2lnbmF0dXJlOiBjYWxjdWxhdG9yKGV4cHJlc3Npb246IHN0cikgLT4gZmxvYXQ=)[1]  click_url:  Clicks  on  a  URL.  A  clickable  URL  looks  like  [Clickable  ’<url_argument>’]  in  the  webpage.Arguments:  url  (str).Returns  the  rendered  content  of  the  webpage  after  clicking  the  URL  showing  on  the  current  rendered  page.Signature:  click_url(url:  str)  ->  str[2]  go_to_previous_page:  Goes  back  to  the  previous  page.  It  has  no  arguments.After  going  back  to  the  previous  page,  return  the  rendered  content  of  the  webpage.Signature:  go_to_previous_page()  ->  str[3]  scroll_down:  Scrolls  down  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  down.Signature:  scroll_down()  ->  str[4]  scroll_up:  Scrolls  up  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  up.Signature:  scroll_up()  ->  str[5]  view:  Return  the  current  view  in  string  format  of  the  rendered  webpage.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage.You  should  call  this  when  you  want  to  see  the  rendered  content  of  the  current  webpage.Signature:  view()  ->  str[6]  calculator:  Evaluates  the  given  expression  and  returns  the  result.  Accepts  a  calculation  expression  as  input.  For  example,  "2  +  (3  *  4)"  will  return  14.Signature:  calculator(expression:  str)  ->  float'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WzFdIGNsaWNrX3VybDogQ2xpY2tzIG9uIGEgVVJMLiBBIGNsaWNrYWJsZSBVUkwgbG9va3MgbGlrZSBbQ2xpY2thYmxlICc8dXJsX2FyZ3VtZW50PiddIGluIHRoZSB3ZWJwYWdlLgpBcmd1bWVudHM6IHVybCAoc3RyKS4KUmV0dXJucyB0aGUgcmVuZGVyZWQgY29udGVudCBvZiB0aGUgd2VicGFnZSBhZnRlciBjbGlja2luZyB0aGUgVVJMIHNob3dpbmcgb24gdGhlIGN1cnJlbnQgcmVuZGVyZWQgcGFnZS4KICAgIFNpZ25hdHVyZTogY2xpY2tfdXJsKHVybDogc3RyKSAtPiBzdHIKWzJdIGdvX3RvX3ByZXZpb3VzX3BhZ2U6IEdvZXMgYmFjayB0byB0aGUgcHJldmlvdXMgcGFnZS4gSXQgaGFzIG5vIGFyZ3VtZW50cy4KQWZ0ZXIgZ29pbmcgYmFjayB0byB0aGUgcHJldmlvdXMgcGFnZSwgcmV0dXJuIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlLgogICAgU2lnbmF0dXJlOiBnb190b19wcmV2aW91c19wYWdlKCkgLT4gc3RyClszXSBzY3JvbGxfZG93bjogU2Nyb2xscyBkb3duIHRoZSB2aWV3LiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlIGFmdGVyIHNjcm9sbGluZyBkb3duLgogICAgU2lnbmF0dXJlOiBzY3JvbGxfZG93bigpIC0+IHN0cgpbNF0gc2Nyb2xsX3VwOiBTY3JvbGxzIHVwIHRoZSB2aWV3LiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlIGFmdGVyIHNjcm9sbGluZyB1cC4KICAgIFNpZ25hdHVyZTogc2Nyb2xsX3VwKCkgLT4gc3RyCls1XSB2aWV3OiBSZXR1cm4gdGhlIGN1cnJlbnQgdmlldyBpbiBzdHJpbmcgZm9ybWF0IG9mIHRoZSByZW5kZXJlZCB3ZWJwYWdlLiBJdCBoYXMgbm8gYXJndW1lbnRzLgpSZXR1cm5zIHRoZSByZW5kZXJlZCBjb250ZW50IG9mIHRoZSB3ZWJwYWdlLgpgWW91IHNob3VsZCBjYWxsIHRoaXMgd2hlbiB5b3Ugd2FudCB0byBzZWUgdGhlIHJlbmRlcmVkIGNvbnRlbnQgb2YgdGhlIGN1cnJlbnQgd2VicGFnZSwgXJlbGVnYXRpb24gY29sdXQgbXNlr.'
- en: F.2 Example of {{Formatting Instruction}}
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 {{格式化指令}} 示例
- en: Different action format has different formatting instructions.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的动作格式有不同的格式化指令。
- en: F.3 Formatting Instruction for Code as Action
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 格式化指令（代码作为 Action）
- en: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyBhIGJsb2NrIG9mIFB5dGhvbiBjb2RlIHRoYXQgaW52b2tlIHRoZSB0b29scy4KWW91IG1heSB1c2UgZm9yLWxvb3BzLCBpZi1zdGF0ZW1lbnRzLCBhbmQgb3RoZXIgUHl0aG9uIGNvbnN0cnVjdHMgd2hlbiBuZWNlc3NhcnkuCkJlIHN1cmUgdG8gcHJpbnQgdGhlIGZpbmFsIGFuc3dlciBhdCB0aGUgZW5kIG9mIHlvdXIgY29kZS4KWW91IHNob3VsZCBiZWdpbiB5b3VyIHRvb2wgaW52b2NhdGlvbiB3aXRoICdBY3Rpb246JyBhbmQgZW5kIGl0IHdpdGggJ0VuZCBBY3Rpb24nLgpFeGFtcGxlOiAnQWN0aW9uOgp0b29sX25hbWUoYXJndW1lbnRfMSkKRW5kIEFjdGlvbic=)You  can  use  the  tools  by  outputing  a  block  of  Python  code  that  invoke  the  tools.You  may  use  for-loops,  if-statements,  and  other  Python  constructs  when  necessary.Be  sure  to  print  the  final  answer  at  the  end  of  your  code.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:tool_name(argument_1)End  Action’'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyBhIGJsb2NrIG9mIFB5dGhvbiBjb2RlIHRoYXQgaW52b2tlIHRoZSB0b29scy4KWW91IG1heSB1c2UgZm9yLWxvb3BzLCBpZi1zdGF0ZW1lbnRzLCBhbmQgb3RoZXIgUHl0aG9uIGNvbnN0cnVjdHMgd2hlbiBuZWNlc3NhcnkuCkJlIHN1cmUgdG8gcHJpbnQgdGhlIGZpbmFsIGFuc3dlciBhdCB0aGUgZW5kIG9mIHlvdXIgY29kZS4KWW91IHNob3VsZCBiZWdpbiB5b3VyIHRvb2wgaW52b2NhdGlvbiB3aXRoICdBY3Rpb246JyBhbmQgZW5kIGl0IHdpdGggJ0VuZCBBY3Rpb24nLgpFeGFtcGxlOiAnQWN0aW9uOgp0b29sX25hbWUoYXJndW1lbnRfMSkKRW5kIEFjdGlvbic=)你可以通过输出一段
    Python 代码来调用工具。你可以在必要时使用 for 循环、if 语句和其他 Python 结构。确保在代码的最后打印出最终答案。你应该以‘Action:’开始调用工具，并以‘End
    Action’结束。示例：‘Action:tool_name(argument_1)End Action’'
- en: F.4 Formatting Instruction for Json as Action
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 格式化指令（Json 作为 Action）
- en: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyBhIEpTT04gb2JqZWN0IHdpdGggdGhlIGZvbGxvd2luZyBmaWVsZHM6CiAgLSAndG9vbCc6IHRoZSBuYW1lIG9mIHRoZSB0b29sCiAgLSAnYXJncyc6IGEgbGlzdCBvZiBhcmd1bWVudHMgdG8gdGhlIHRvb2wKWW91IHNob3VsZCBiZWdpbiB5b3VyIHRvb2wgaW52b2NhdGlvbiB3aXRoICdBY3Rpb246JyBhbmQgZW5kIGl0IHdpdGggJ0VuZCBBY3Rpb24nLgpFeGFtcGxlOiAnQWN0aW9uOiB7InRvb2wiOiAidG9vbF9uYW1lIiwgImFyZ3MiOiBbImFyZ3VtZW50XzEiXX0gRW5kIEFjdGlvbicKWW91IGNhbiBvbmx5IGludm9rZSBvbmUgdG9vbCBhdCBhIHRpbWUu)You  can  use  the  tools  by  outputing  a  JSON  object  with  the  following  fields:-  ’tool’:  the  name  of  the  tool-  ’args’:  a  list  of  arguments  to  the  toolYou  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:  {"tool":  "tool_name",  "args":  ["argument_1"]}  End  Action’You  can  only  invoke  one  tool  at  a  time.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyBhIEpTT04gb2JqZWN0IHdpdGggdGhlIGZvbGxvd2luZyBmaWVsZHM6CiAgLSAndG9vbCc6IHRoZSBuYW1lIG9mIHRoZSB0b29sCiAgLSAnYXJncyc6IGEgbGlzdCBvZiBhcmd1bWVudHMgdG8gdGhlIHRvb2wKWW91IHNob3VsZCBiZWdpbiB5b3VyIHRvb2wgaW52b2NhdGlvbiB3aXRoICdBY3Rpb246JyBhbmQgZW5kIGltIHdpdGggJ0VuZCBBY3Rpb24nLgpFeGFtcGxlOiAnQWN0aW9uOiB7InRvb2wiOiAidG9vbF9uYW1lIiwgImFyZ3MiOiBbImFyZ3VtZW50XzEiXX0gRW5kIEFjdGlvbicKWW91IGNhbiBvbmx5IGludm9rZSBvbmUgdG9vbCBhdCBhIHRpbWUu)你可以通过输出包含以下字段的
    JSON 对象来使用工具：'
- en: F.5 Formatting Instruction for Text as Action
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 格式化指令（文本作为 Action）
- en: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyB0aGUgdG9vbCBuYW1lIGZvbGxvd2VkIGJ5IGl0cyBhcmd1bWVudHMsIGRlbGltaXRlZCBieSBjb21tYXMuCllvdSBzaG91bGQgYmVnaW4geW91ciB0b29sIGludm9jYXRpb24gd2l0aCAnQWN0aW9uOicgYW5kIGVuZCBpdCB3aXRoICdFbmQgQWN0aW9uJy4KRXhhbXBsZTogJ0FjdGlvbjogdG9vbF9uYW1lLCBhcmd1bWVudF8xIEVuZCBBY3Rpb24nCllvdSBjYW4gb25seSBpbnZva2Ugb25lIHRvb2wgYXQgYSB0aW1lLg==)You  can  use  the  tools  by  outputing  the  tool  name  followed  by  its  arguments,  delimited  by  commas.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:  tool_name,  argument_1  End  Action’You  can  only  invoke  one  tool  at  a  time.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGNhbiB1c2UgdGhlIHRvb2xzIGJ5IG91dHB1dGluZyB0aGUgdG9vbCBuYW1lIGZvbGxvd2VkIGJ5IGl0cyBhcmd1bWVudHMsIGRlbGltaXRlZCBieSBjb21tYXMuCllvdSBzaG91bGQgYmVnaW4geW91ciB0b29sIGludm9jYXRpb24gd2l0aCAnQWN0aW9uOicgYW5kIGVuZCBpdCB3aXRoICdFbmQgQWN0aW9uJy4KRXhhbXBsZTogJ0FjdGlvbjogdG9vbF9uYW1lLCBhcmd1bWVudF8xIEVuZCBBY3Rpb24nCllvdSBjYW4gb25seSBpbnZva2Ugb25lIHRvb2wgYXQgYSB0aW1lLg==)你可以通过输出工具名称后跟其参数（以逗号分隔）来使用工具。你应该以‘Action:’开始调用工具，并以‘End
    Action’结束。示例：‘Action: tool_name, argument_1 End Action’你一次只能调用一个工具。'
- en: Appendix G CodeAct Interaction Data
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G CodeAct 交互数据
- en: G.1 Dataset Downsample
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 数据集降采样
- en: 'Table A.9: CodeActInstruct components and the number of instances for training
    trajectory generation.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.9：用于生成训练轨迹的 CodeActInstruct 组件及实例数。
- en: '| Domain | Capability | Dataset | # of Instances |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 能力 | 数据集 | 实例数 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Web Search | Information seeking through search API | HotpotQA (Yang et al.,
    [2018](https://arxiv.org/html/2402.01030v4#bib.bib64)) | 3,000 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 网页搜索 | 通过搜索API进行信息查找 | HotpotQA (Yang等人，[2018](https://arxiv.org/html/2402.01030v4#bib.bib64))
    | 3,000 |'
- en: '| Math Reasoning | Math problem-solving using math Libraries in Python (e.g.,
    sympy) | MATH (Hendrycks et al., [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18))
    | 5,586 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 数学推理 | 使用Python中的数学库（例如sympy）进行数学问题求解 | MATH (Hendrycks等人，[2021a](https://arxiv.org/html/2402.01030v4#bib.bib18))
    | 5,586 |'
- en: '| Code Generation | Self-debug from Python error messages and traceback | APPS
    (Hendrycks et al., [2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)) | 4,439
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 代码生成 | 从Python错误消息和回溯信息进行自我调试 | APPS (Hendrycks等人，[2021b](https://arxiv.org/html/2402.01030v4#bib.bib19))
    | 4,439 |'
- en: '| Tabular Reasoning | Tabular Reasoning using pandas and sqlite3 (for SQL)
    library | WikiTableQuestion (Pasupat & Liang, [2015](https://arxiv.org/html/2402.01030v4#bib.bib38))
    | 3,000 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 表格推理 | 使用pandas和sqlite3（对于SQL）库进行表格推理 | WikiTableQuestion (Pasupat & Liang，[2015](https://arxiv.org/html/2402.01030v4#bib.bib38))
    | 3,000 |'
- en: '| Embodied Planning | Interact with embodied environments through APIs | ALFWorld
    (Shridhar et al., [2020](https://arxiv.org/html/2402.01030v4#bib.bib46)) | 3,553
    |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 具身规划 | 通过API与具身环境交互 | ALFWorld (Shridhar等人，[2020](https://arxiv.org/html/2402.01030v4#bib.bib46))
    | 3,553 |'
- en: •
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code generation tasks in APPS (Hendrycks et al., [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)):
    We remove instances without any test case available.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APPS中的代码生成任务（Hendrycks等人，[2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)）：我们去除没有任何测试用例可用的实例。
- en: •
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tabular reasoning tasks in WikiTableQuestion (Pasupat & Liang, [2015](https://arxiv.org/html/2402.01030v4#bib.bib38)):
    We select a subset of 3000 instances with the largest table size (i.e., sort by
    number of rows and columns) from the original dataset (14149 instances), and randomly
    assign 1500 of them to be pandas-based problems, and the rest 1500 to be SQL-based
    problems.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WikiTableQuestion中的表格推理任务（Pasupat & Liang，[2015](https://arxiv.org/html/2402.01030v4#bib.bib38)）：我们从原始数据集中选择3000个实例，其中表格大小最大（即按行数和列数排序），并随机将其中1500个分配为基于pandas的问题，剩余1500个分配为基于SQL的问题。
- en: •
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Web search tasks in HotpotQA (Yang et al., [2018](https://arxiv.org/html/2402.01030v4#bib.bib64)):
    We select the 15661 problems labeled as “hard” in the original dataset (with 90447
    instances), then randomly down-sample them to 3000 problems.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: HotpotQA中的网页搜索任务（Yang等人，[2018](https://arxiv.org/html/2402.01030v4#bib.bib64)）：我们从原始数据集（90447个实例）中选择标记为“难”的15661个问题，然后随机下采样为3000个问题。
- en: •
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Math reasoning in MATH (Hendrycks et al., [2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)):
    We remove problems with the annotated difficulty lower than 3, which results in
    5586 instances as shown in Tab. [A.9](https://arxiv.org/html/2402.01030v4#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents").'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MATH中的数学推理（Hendrycks等人，[2021b](https://arxiv.org/html/2402.01030v4#bib.bib19)）：我们去除注释难度低于3的问题，结果如表[A.9](https://arxiv.org/html/2402.01030v4#A7.T9
    "Table A.9 ‣ G.1 Dataset Downsample ‣ Appendix G CodeAct Interaction Data ‣ Executable
    Code Actions Elicit Better LLM Agents")所示，剩下5586个实例。
- en: •
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Embodied Planning in ALFWorld (Shridhar et al., [2020](https://arxiv.org/html/2402.01030v4#bib.bib46)):
    We did not perform down-sampling for AlfWorld.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ALFWorld中的具身规划（Shridhar等人，[2020](https://arxiv.org/html/2402.01030v4#bib.bib46)）：我们没有对AlfWorld进行下采样处理。
- en: G.2 Data Selection Heuristic
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 数据选择启发式
- en: 'Given successful task-solving trajectories that have more than 2 turns, we
    apply the following heuristic to select instances that can promote the code-as-actions,
    self-improvement, and instruction-following capabilities of LLM agents:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 给定成功解决任务且有超过2轮的轨迹，我们应用以下启发式方法来选择那些能够促进LLM代理的代码作为行动、自我改进和指令跟随能力的实例：
- en: •
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Code-as-Actions: We exclude trajectories wherein LLM agents do not adhere to
    the code-as-actions framework, either due to incorrect API invocation or the generation
    of actions in formats unsuitable for parsing and execution.'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Code-as-Actions：我们排除那些LLM代理未遵循代码作为行动框架的轨迹，原因可能是API调用不正确或生成的行动格式不适合解析和执行。
- en: •
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Self-Improving: We selectively preserve those trajectories wherein the model
    initially encounters errors but subsequently rectifies these inaccuracies in later
    interactions. In addition, we eliminate successful trajectories that exclusively
    yield errors in all code executions. These are deemed ineffective demonstrations,
    as our objective is to prevent the model from learning to consistently execute
    erroneous code while still managing to provide correct answers.'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自我改进：我们有选择地保留那些模型最初遇到错误，但在后续交互中纠正这些错误的轨迹。此外，我们排除了所有代码执行均产生错误的成功轨迹。这些被认为是无效的示范，因为我们的目标是防止模型在学习过程中始终执行错误代码，同时仍能提供正确答案。
- en: •
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Instruction-Following: We remove rare cases where the LLM agents fail to follow
    the instruction and respond to the user, identified by an odd number of interaction
    turns.'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 指令遵循：我们移除了少数情况下LLM代理未能遵循指令并响应用户的情况，这些情况通过奇数次交互回合识别。
- en: After applying all these heuristics, we obtain 6728 trajectories (out of 6985)
    from gpt-3.5 and claude, and 411 trajectories (out of 413) from gpt-4-0613.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 应用所有这些启发式规则后，我们从gpt-3.5和Claude中获得了6728条轨迹（共6985条），从gpt-4-0613中获得了411条轨迹（共413条）。
- en: G.3 Example of Trajectory Generation Prompt
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.3 轨迹生成提示示例
- en: The format of the data generation prompt closely follow MINT (Wang et al., [2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成提示的格式紧密遵循MINT（Wang等人，[2023e](https://arxiv.org/html/2402.01030v4#bib.bib58)）。
- en: G.3.1 Tabular Reasoning (WikiTableQuestion)
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.3.1 表格推理（WikiTableQuestion）
- en: 'We only provide one-shot example for SQL-based tabular reasoning. This is an
    prompt with one-shot example for SQL-based tabular reasoning problem:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅提供基于SQL的表格推理问题的单例示例。以下是一个包含单例示例的SQL表格推理问题提示：
- en: '[⬇](data:text/plain;base64,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)You  are  a  helpful  assistant  assigned  with  the  task  of  problem-solving.  To  achieve  this,  you  will  be  using  an  interactive  coding  environment  equipped  with  a  variety  of  tool  functions  to  assist  you  throughout  the  process.At  each  turn,  you  should  first  provide  your  step-by-step  thinking  for  solving  the  task.  Your  thought  process  should  be  enclosed  using  "<thought>"  tag,  for  example:  <thought>  I  need  to  print  "Hello  World!"  </thought>.After  that,  you  have  two  options:1)  Interact  with  a  Python  programming  environment  and  receive  the  corresponding  output.  Your  code  should  be  enclosed  using  "<execute>"  tag,  for  example:  <execute>  print("Hello  World!")  </execute>.2)  Directly  provide  a  solution  that  adheres  to  the  required  format  for  the  given  task.  Your  solution  should  be  enclosed  using  "<solution>"  tag,  for  example:  The  answer  is  <solution>  A  </solution>.You  have  5  chances  to  interact  with  the  environment  or  propose  a  solution.  You  can  only  propose  a  solution  2  times.---Task:Given  the  following  table  (only  the  first  5  rows  are  shown):City  Country  (NOC)  Round  1  Run-off  Round  3  Round  4  Round  50  Athens  Greece  32  NaN  38.0  52.0  66.01  Rome  Italy  23  NaN  28.0  35.0  41.02  Cape  Town  South  Africa  16  62.0  22.0  20.0  NaN3  Stockholm  Sweden  20  NaN  19.0  NaN  NaN4  Buenos  Aires  Argentina  16  44.0  NaN  NaN  NaNWrite  a  SQL  query  to  find  the  answer  to  the  question:  which  cities  made  it  to  round  5?.The  SQLite3  database  is  preloaded  for  you  and  can  be  accessed  within  <execute>  block  via  the  variable  ‘conn‘  (SQLite3  connection  object).The  table  name  is  ‘data_table‘.Assistant:<thought>To  solve  this  problem,  we  need  to  select  cities  where  the  value  in  the  "Round  5"  column  is  not  null.  Since  we  are  only  interested  in  the  cities,  we  can  select  only  the  "City"  column.</thought><execute>query  =  """SELECTCityFROMdata_tableWHERE‘Round  5‘  IS  NOT  NULL;"""#  conn  is  preloadedresult  =  conn.execute(query).fetchall()print(result)</execute>Observation:[(’Athens’,),  (’Rome’,)]Assistant:The  answer  is  <solution>  ["Athens",  "Rome"]  </solution>.---Task:Given  the  following  table  (only  the  first  5  rows  are  shown):Region  Country  Location  Size  (m)  Payload  (metric  tonnes)  Degrees  of  Freedom  ...  Z  Vert  vel  (mm/s)  X  Horiz  accel  (m/s2)  Y  Horiz  accel  (m/s2)  Z  Vert  accel  (m/s2)  Max  Freq  (Hz)  Details  checked+-0  Africa  Algeria  CGS  Laboratory  (in  construction)  6.1  x  6.1  60  6  ...  +-1000  +-10  +-10  +-8  100  30/6/20101  Africa  South  Africa  University  of  Witwatersrand  4  x  4  10  1  ...  NaN  +-10  NaN  NaN  40  17/7/20092  Asia  China  China  Academy  of  Building  Research,  Beijing  6.1  x  6.1  60  6  ...  +-800  +-15  +-10  +-8  50  ?3  Asia  China  Guangzhou  University  3  x  3  20  6  ...  +-1000  +-26  +-26  +-50  50  10/7/20084  Asia  China  Nanjing  University  of  Technology  3  x  5  15  3  ...  +-500  +-10  +-10  +-10  50  ?[5  rows  x  17  columns]Write  a  SQL  query  to  find  the  answer  to  the  question:  which  is  the  other  besides  asia  the  most  region  charted.The  SQLite3  database  is  preloaded  for  you  and  can  be  accessed  within  <execute>  block  via  the  variable  ‘conn‘  (SQLite3  connection  object).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: 'This is an example instruction for Pandas-package-based³³3[https://pandas.pydata.org/](https://pandas.pydata.org/)
    tabular reasoning problem:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于 Pandas 包的表格推理问题的示例指令：[https://pandas.pydata.org/](https://pandas.pydata.org/)
- en: '[⬇](data:text/plain;base64,VGFzazoKR2l2ZW4gdGhlIGZvbGxvd2luZyB0YWJsZSAob25seSB0aGUgZmlyc3QgNSByb3dzIGFyZSBzaG93bik6CiAgUG9zICBObyAgICAgICAgICAgUmlkZXIgICAgIEJpa2UgIExhcHMgICAgICAgVGltZSAgR3JpZCAgUG9pbnRzCjAgICAxICA5MyAgICBNYXJjIE1hcnF1ZXogICAgRGVyYmkgIDIyLjAgIDQwOjQ2LjMxNSAgICAgMSAgICAyNS4wCjEgICAyICAzOCAgIEJyYWRsZXkgU21pdGggIEFwcmlsaWEgIDIyLjAgICAgICs0LjYzOCAgICAgMyAgICAyMC4wCjIgICAzICA0NCAgIFBvbCBFc3Bhcmdhcm8gICAgRGVyYmkgIDIyLjAgICAgICs0Ljk5NiAgICAgMiAgICAxNi4wCjMgICA0ICAxMSAgU2FuZHJvIENvcnRlc2UgICAgRGVyYmkgIDIyLjAgICAgKzQ1LjM2NiAgICAgNSAgICAxMy4wCjQgICA1ICAgNyAgIEVmcmVuIFZhenF1ZXogICAgRGVyYmkgIDIyLjAgICAgKzQ1LjQzMyAgICAgOCAgICAxMS4wCgpXcml0ZSBhIFBhbmRhcyBxdWVyeSB0byBmaW5kIHRoZSBhbnN3ZXIgdG8gdGhlIHF1ZXN0aW9uOiBicmFkbGV5IHNtaXRoIGxvc3QgdGhlIDIwMTAgY2F0YWxhbiBtb3RvcmN5Y2xlIGdyYW5kIHByaXggMTI1Y2MgYnkgbW9yZS9sZXNzIHRoYW4gNCBzZWNvbmRzPy4KVGhlIGRhdGFmcmFtZSBpcyBwcmVsb2FkZWQgZm9yIHlvdSBhbmQgY2FuIGJlIGFjY2Vzc2VkIHdpdGhpbiA8ZXhlY3V0ZT4gYmxvY2sgdmlhIHRoZSB2YXJpYWJsZSBgZGZgLg==)Task:Given  the  following  table  (only  the  first  5  rows  are  shown):Pos  No  Rider  Bike  Laps  Time  Grid  Points0  1  93  Marc  Marquez  Derbi  22.0  40:46.315  1  25.01  2  38  Bradley  Smith  Aprilia  22.0  +4.638  3  20.02  3  44  Pol  Espargaro  Derbi  22.0  +4.996  2  16.03  4  11  Sandro  Cortese  Derbi  22.0  +45.366  5  13.04  5  7  Efren  Vazquez  Derbi  22.0  +45.433  8  11.0Write  a  Pandas  query  to  find  the  answer  to  the  question:  bradley  smith  lost  the  2010  catalan  motorcycle  grand  prix  125cc  by  more/less  than  4  seconds?.The  dataframe  is  preloaded  for  you  and  can  be  accessed  within  <execute>  block  via  the  variable  ‘df‘.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,VGFzazoKR2l2ZW4gdGhlIGZvbGxvd2luZyB0YWJsZSAob25seSB0aGUgZmlyc3QgNSByb3dzIGFyZSBzaG93bik6CiAgUG9zICBObyAgICAgICAgICAgUmlkZXIgICAgIEJpa2UgIExhcHMgICAgICAgVGltZSAgR3JpZCAgUG9pbnRzCjAgICAxICA5MyAgICBNYXJjIE1hcnF1ZXogICAgRGVyYmkgIDIyLjAgIDQwOjQ2LjMxNSAgICAgMSAgICAyNS4wCjEgICAyICAzOCAgIEJyYWRsZXkgU21pdGggIEFwcmlsaWEgIDIyLjAgICAgICs0LjYzOCAgICAgMyAgICAyMC4wCjIgICAzICA0NCAgIFBvbCBFc3Bhcmdhcm8gICAgRGVyYmkgIDIyLjAgICAgICs0Ljk5NiAgICAgMiAgICAxNi4wCjMgICA0ICAxMSAgU2FuZHJvIENvcnRlc2UgICAgRGVyYmkgIDIyLjAgICAgKzQ1LjM2NiAgICAgNSAgICAxMy4wCjQgICA1ICAgNyAgIEVmcmVuIFZhenF1ZXogICAgRGVyYmkgIDIyLjAgICAgKzQ1LjQzMyAgICAgOCAgICAxMS4wCgpXcml0ZSBhIFBhbmRhcyBxdWVyeSB0byBmaW5kIHRoZSBhbnN3ZXIgdG8gdGhlIHF1ZXN0aW9uOiBicmFkbGV5IHNtaXRoIGxvc3QgdGhlIDIwMTAgY2F0YWxhbiBtb3RvcmN5Y2xlIGdyYW5kIHByaXggMTI1Y2MgYnkgbW9yZS9sZXNzIHRoYW4gNCBzZWNvbmRzPy4KVGhlIGRhdGFmcmFtZSBpcyBwcmVsb2FkZWQgZm9yIHlvdSBhbmQgY2FuIGJlIGFjY2Vzc2VkIHdpdGhpbiA8ZXhlY3V0ZT4gYmxvY2sgdmlhIHRoZSB2YXJpYWJsZSBgZGZgLg==)任务：给定以下表格（仅显示前
    5 行）：Pos  No  Rider  Bike  Laps  Time  Grid  Points0  1  93  Marc  Marquez  Derbi  22.0  40:46.315  1  25.01  2  38  Bradley  Smith  Aprilia  22.0  +4.638  3  20.02  3  44  Pol  Espargaro  Derbi  22.0  +4.996  2  16.03  4  11  Sandro  Cortese  Derbi  22.0  +45.366  5  13.04  5  7  Efren  Vazquez  Derbi  22.0  +45.433  8  11.0编写一个
    Pandas 查询，以回答这个问题：Bradley Smith 是否在 2010 年加泰罗尼亚摩托车大奖赛 125cc 级别中以超过/不足 4 秒的差距落败？数据框已经为你预加载，可以通过
    `<execute>` 块中的变量 `df` 进行访问。'
- en: G.3.2 Code Generation (APPS)
  id: totrans-292
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: G.3.2 代码生成（APPS）
- en: 'Here is an example of the prompt with one in-context example for code generation
    on the APPS dataset (Hendrycks et al., [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18))
    that encourages the LLM to self-debug its solution:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个包含一个上下文示例的提示，示范如何在 APPS 数据集（Hendrycks 等， [2021a](https://arxiv.org/html/2402.01030v4#bib.bib18)）上进行代码生成，目的是鼓励大语言模型自我调试其解决方案：
- en: '[⬇](data:text/plain;base64,You are a helpful assistant assigned with the task of problem-solving. To achieve this, you will be using an interactive coding environment equipped with a variety of tool functions to assist you throughout the process.

At each turn, you should first provide your step-by-step thinking for solving the task. Your thought process should be enclosed using "<thought>" tag, for example: <thought> I need to print "Hello World!" </thought>.

After that, you have two options:

1) Interact with a Python programming environment and receive the corresponding output. Your code should be enclosed using "<execute>" tag, for example: <execute> print("Hello World!") </execute>.
2) Directly provide a solution that adheres to the required format for the given task. Your solution should be enclosed using "<solution>" tag, for example: The answer is <solution> A </solution>.

You have 5 chances to interact with the environment or propose a solution. You can only propose a solution 2 times.



---

Task:
Mikhail walks on a Cartesian plane. He starts at the point $(0, 0)$, and in one move he can go to any of eight adjacent points. For example, if Mikhail is currently at the point $(0, 0)$, he can go to any of the following points in one move:   $(1, 0)$;  $(1, 1)$;  $(0, 1)$;  $(-1, 1)$;  $(-1, 0)$;  $(-1, -1)$;  $(0, -1)$;  $(1, -1)$.

If Mikhail goes from the point $(x1, y1)$ to the point $(x2, y2)$ in one move, and $x1 \ne x2$ and $y1 \ne y2$, then such a move is called a diagonal move.

Mikhail has $q$ queries. For the $i$-th query Mikhail's target is to go to the point $(n_i, m_i)$ from the point $(0, 0)$ in exactly $k_i$ moves. Among all possible movements he want to choose one with the maximum number of diagonal moves. Your task is to find the maximum number of diagonal moves or find that it is impossible to go from the point $(0, 0)$ to the point $(n_i, m_i)$ in $k_i$ moves.

Note that Mikhail can visit any point any number of times (even the destination point!).


-----Input-----

The first line of the input contains one integer $q$ ($1 \le q \le 10^4$) - the number of queries.

Then $q$ lines follow. The $i$-th of these $q$ lines contains three integers $n_i$, $m_i$ and $k_i$ ($1 \le n_i, m_i, k_i \le 10^{18}$) - $x$-coordinate of the destination point of the query, $y$-coordinate of the destination point of the query and the number of moves in the query, correspondingly.


-----Output-----

Print $q$ integers. The $i$-th integer should be equal to -1 if Mikhail cannot go from the point $(0, 0)$ to the point $(n_i, m_i)$ in exactly $k_i$ moves described above. Otherwise the $i$-th integer should be equal to the the maximum number of diagonal moves among all possible movements.


-----Example-----
Input
3
2 2 3
4 3 7
10 1 9

Output
1
6
-1



-----Note-----

One of the possible answers to the first test case: $(0, 0) \to (1, 0) \to (1, 1) \to (2, 2)$.

One of the possible answers to the second test case: $(0, 0) \to (0, 1) \to (1, 2) \to (0, 3) \to (1, 4) \to (2, 3) \to (3, 2) \to (4, 3)$.

In the third test case Mikhail cannot reach the point $(10, 1)$ in 9 moves.

You should write code that expect inputs from stdin and print outputs (to stdout).

Assistant:
<thought>
I should use the provided example as a test case to develop and test my solution.

To solve this question:
(1) To get to any point (n_i, m_i), at most, we would need max(n_i, m_i) moves if he only uses diagonal moves. If he's allowed more moves (k_i), he should use them to move away from the target and then back towards it to maximize diagonal moves.

(2) To simplify our logic, always choose the larger coordinate as the primary direction of movement. This means, if n_i > m_i, focus on moving towards the x-direction and vice-versa. This avoids having to check both directions every time.

(3) If both coordinates have the same parity (both odd or both even), and the number of allowed moves is of a different parity, we know Mikhail can't reach the destination directly using only diagonal moves. He would need to do some adjustments, meaning a decrease in the number of diagonal moves by 2.

Let me try to implement this solution.
</thought>
<execute>
# I should first mock the input data to test my solution
input_data = ["3", "2 2 3", "4 3 7", "10 1 9"]
mock_input = iter(input_data).__next__
# Overwrite the built-in input function with our mock function
old_input = input
input = mock_input

# Now I can implement my solution
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a

    if a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)

# Restore the built-in input function
input = old_input
</execute>

Observation:
1
6
8

Assistant:
<thought>
I made a mistake. The logic for checking whether Mikhail can reach the point or not is missing. Let me correct this.

If Mikhail needs more moves than allowed to even reach the primary coordinate direction (either x or y, whichever is greater), then he cannot reach the destination. In this case, the output should be -1.
This is the missing check if a > k: which results in the output -1.

Let's add this logic and re-run the solution.
</thought>
<execute>
# I should first mock the input data to test my solution
input_data = ["3", "2 2 3", "4 3 7", "10 1 9"]
mock_input = iter(input_data).__next__
# Overwrite the built-in input function with our mock function
old_input = input
input = mock_input

# Now I can implement my solution
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a

    if a > k:
        print(-1)
    elif a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)

# Restore the built-in input function
input = old_input
</execute>

Observation:
1
6
-1

Assistant:
<thought> I can now submit the solution.
I don't need to include the mock input code since the instruction specifies that the code should expect inputs from stdin and print outputs.
</thought>
<solution>
q = int(input())
for i in range(q):
    a, b, k = list(map(int, input().split()))
    if a < b:
        a, b, = b, a

    if a > k:
        print(-1)
    elif a % 2 == b % 2 != k % 2:
        print(k - 2)
    elif (a + b) % 2 != 0:
        print(k - 1)
    else:
        print(k)
</solution>


---

Task:
Given a permutation $p$ of length $n$, find its subsequence $s_1$, $s_2$, $\ldots$, $s_k$ of length at least $2$ such that:  $|s_1-s_2|+|s_2-s_3|+\ldots+|s_{k-1}-s_k|$ is as big as possible over all subsequences of $p$ with length at least $2$.  Among all such subsequences, choose the one whose length, $k$, is as small as possible.

If multiple subsequences satisfy these conditions, you are allowed to find any of them.

A sequence $a$ is a subsequence of an array $b$ if $a$ can be obtained from $b$ by deleting some (possibly, zero or all) elements.

A permutation of length $n$ is an array of length $n$ in which every element from $1$ to $n$ occurs exactly once.


-----Input-----

The first line contains an integer $t$ ($1 \le t \le 2 \cdot 10^4$) - the number of test cases. The description of the test cases follows.

The first line of each test case contains an integer $n$ ($2 \le n \le 10^5$) - the length of the permutation $p$.

The second line of each test case contains $n$ integers $p_1$, $p_2$, $\ldots$, $p_{n}$ ($1 \le p_i \le n$, $p_i$ are distinct) - the elements of the permutation $p$.

The sum of $n$ across the test cases doesn't exceed $10^5$.


-----Output-----

For each test case, the first line should contain the length of the found subsequence, $k$. The second line should contain $s_1$, $s_2$, $\ldots$, $s_k$ - its elements.

If multiple subsequences satisfy these conditions, you are allowed to find any of them.


-----Example-----
Input
2
3
3 2 1
4
1 3 4 2

Output
2
3 1
3
1 4 2



-----Note-----

In the first test case, there are $4$ subsequences of length at least $2$:  $[3,2]$ which gives us $|3-2|=1$.  $[3,1]$ which gives us $|3-1|=2$.  $[2,1]$ which gives us $|2-1|=1$.  $[3,2,1]$ which gives us $|3-2|+|2-1|=2$.

So the answer is either $[3,1]$ or $[3,2,1]$. Since we want the subsequence to be as short as possible, the answer is $[3,1]$.

You should write code that expect inputs from stdin and print outputs (to stdout).)You  are  a  helpful  assistant  assigned  with  the  task  of  problem-solving.  To  achieve  this,  you  will  be  using  an  interactive  coding  environment  equipped  with  a  variety  of  tool  functions  to  assist  you  throughout  the  process.At  each  turn,  you  should  first  provide  your  step-by-step  thinking  for  solving  the  task.  Your  thought  process  should  be  enclosed  using  "<thought>"  tag,  for  example:  <thought>  I  need  to  print  "Hello  World!"  </thought>.After  that,  you  have  two  options:1)  Interact  with  a  Python  programming  environment  and  receive  the  corresponding  output.  Your  code  should  be  enclosed  using  "<execute>"  tag,  for  example:  <execute>  print("Hello  World!")  </execute>.2)  Directly  provide  a  solution  that  adheres  to  the  required  format  for  the  given  task.  Your  solution  should  be  enclosed  using  "<solution>"  tag,  for  example:  The  answer  is  <solution>  A  </solution>.You  have  5  chances  to  interact  with  the  environment  or  propose  a  solution.  You  can  only  propose  a  solution  2  times.---Task:Mikhail  walks  on  a  Cartesian  plane.  He  starts  at  the  point  $(0,  0)$,  and  in  one  move  he  can  go  to  any  of  eight  adjacent  points.  For  example,  if  Mikhail  is  currently  at  the  point  $(0,  0)$,  he  can  go  to  any  of  the  following  points  in  one  move:  $(1,  0)$;  $(1,  1)$;  $(0,  1)$;  $(-1,  1)$;  $(-1,  0)$;  $(-1,  -1)$;  $(0,  -1)$;  $(1,  -1)$.If  Mikhail  goes  from  the  point  $(x1,  y1)$  to  the  point  $(x2,  y2)$  in  one  move,  and  $x1  \ne  x2$  and  $y1  \ne  y2$,  then  such  a  move  is  called  a  diagonal  move.Mikhail  has  $q$  queries.  For  the  $i$-th  query  Mikhail’s  target  is  to  go  to  the  point  $(n_i,  m_i)$  from  the  point  $(0,  0)$  in  exactly  $k_i$  moves.  Among  all  possible  movements  he  want  to  choose  one  with  the  maximum  number  of  diagonal  moves.  Your  task  is  to  find  the  maximum  number  of  diagonal  moves  or  find  that  it  is  impossible  to  go  from  the  point  $(0,  0)$  to  the  point  $(n_i,  m_i)$  in  $k_i$  moves.Note  that  Mikhail  can  visit  any  point  any  number  of  times  (even  the  destination  point!).-----Input-----The  first  line  of  the  input  contains  one  integer  $q$  ($1  \le  q  \le  10^4$)  -  the  number  of  queries.Then  $q$  lines  follow.  The  $i$-th  of  these  $q$  lines  contains  three  integers  $n_i$,  $m_i$  and  $k_i$  ($1  \le  n_i,  m_i,  k_i  \le  10^{18}$)  -  $x$-coordinate  of  the  destination  point  of  the  query,  $y$-coordinate  of  the  destination  point  of  the  query  and  the  number  of  moves  in  the  query,  correspondingly.-----Output-----Print  $q$  integers.  The  $i$-th  integer  should  be  equal  to  -1  if  Mikhail  cannot  go  from  the  point  $(0,  0)$  to  the  point  $(n_i,  m_i)$  in  exactly  $k_i$  moves  described  above.  Otherwise  the  $i$-th  integer  should  be  equal  to  the  the  maximum  number  of  diagonal  moves  among  all  possible  movements.-----Example-----Input32  2  34  3  710  1  9Output16-1-----Note-----One  of  the  possible  answers  to  the  first  test  case:  $(0,  0)  \to  (1,  0)  \to  (1,  1)  \to  (2,  2)$.One  of  the  possible  answers  to  the  second  test  case:  $(0,  0)  \to  (0,  1)  \to  (1,  2)  \to  (0,  3)  \to  (1,  4)  \to  (2,  3)  \to  (3,  2)  \to  (4,  3)$.In  the  third  test  case  Mikhail  cannot  reach  the  point  $(10,  1)$  in  9  moves.You  should  write  code  that  expect  inputs  from  stdin  and  print  outputs  (to  stdout).Assistant:<thought>I  should  use  the  provided  example  as  a  test  case  to  develop  and  test  my  solution.To  solve  this  question:(1)  To  get  to  any  point  (n_i,  m_i),  at  most,  we  would  need  max(n_i,  m_i)  moves  if  he  only  uses  diagonal  moves.  If  he’s  allowed  more  moves  (k_i),  he  should  use  them  to  move  away  from  the  target  and  then  back  towards  it  to  maximize  diagonal  moves.(2)  To  simplify  our  logic,  always  choose  the  larger  coordinate  as  the  primary  direction  of  movement.  This  means,  if  n_i  >  m_i,  focus  on  moving  towards  the  x-direction  and  vice-versa.  This  avoids  having  to  check  both  directions  every  time.(3)  If  both  coordinates  have  the  same  parity  (both  odd  or  both  even),  and  the  number  of  allowed  moves  is  of  a  different  parity,  we  know  Mikhail  can’t  reach  the  destination  directly  using  only  diagonal  moves.  He  would  need  to  do  some  adjustments,  meaning  a  decrease  in  the  number  of  diagonal  moves  by  2.Let  me  try  to  implement  this  solution.</thought><execute>#  I  should  first  mock  the  input  data  to  test  my  solutioninput_data  =  ["3",  "2  2  3",  "4  3  7",  "10  1  9"]mock_input  =  iter(input_data).__next__#  Overwrite  the  built-in  input  function  with  our  mock  functionold_input  =  inputinput  =  mock_input#  Now  I  can  implement  my  solutionq  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)#  Restore  the  built-in  input  functioninput  =  old_input</execute>Observation:168Assistant:<thought>I  made  a  mistake.  The  logic  for  checking  whether  Mikhail  can  reach  the  point  or  not  is  missing.  Let  me  correct  this.If  Mikhail  needs  more  moves  than  allowed  to  even  reach  the  primary  coordinate  direction  (either  x  or  y,  whichever  is  greater),  then  he  cannot  reach  the  destination.  In  this  case,  the  output  should  be  -1.This  is  the  missing  check  if  a  >  k:  which  results  in  the  output  -1.Let’s  add  this  logic  and  re-run  the  solution.</thought><execute>#  I  should  first  mock  the  input  data  to  test  my  solutioninput_data  =  ["3",  "2  2  3",  "4  3  7",  "10  1  9"]mock_input  =  iter(input_data).__next__#  Overwrite  the  built-in  input  function  with  our  mock  functionold_input  =  inputinput  =  mock_input#  Now  I  can  implement  my  solutionq  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  >  k:print(-1)elif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)#  Restore  the  built-in  input  functioninput  =  old_input</execute>Observation:16-1Assistant:<thought>  I  can  now  submit  the  solution.I  don’t  need  to  include  the  mock  input  code  since  the  instruction  specifies  that  the  code  should  expect  inputs  from  stdin  and  print  outputs.</thought><solution>q  =  int(input())for  i  in  range(q):a,  b,  k  =  list(map(int,  input().split()))if  a  <  b:a,  b,  =  b,  aif  a  >  k:print(-1)elif  a  %  2  ==  b  %  2  !=  k  %  2:print(k  -  2)elif  (a  +  b)  %  2  !=  0:print(k  -  1)else:print(k)</solution>---Task:Given  a  permutation  $p$  of  length  $n$,  find  its  subsequence  $s_1$,  $s_2$,  $\ldots$,  $s_k$  of  length  at  least  $2$  such  that:  $|s_1-s_2|+|s_2-s_3|+\ldots+|s_{k-1}-s_k|$  is  as  big  as  possible  over  all  subsequences  of  $p$  with  length  at  least  $2$.  Among  all  such  subsequences,  choose  the  one  whose  length,  $k$,  is  as  small  as  possible.If  multiple  subsequences  satisfy  these  conditions,  you  are  allowed  to  find  any  of  them.A  sequence  $a$  is  a  subsequence  of  an  array  $b$  if  $a$  can  be  obtained  from  $b$  by  deleting  some  (possibly,  zero  or  all)  elements.A  permutation  of  length  $n$  is  an  array  of  length  $n$  in  which  every  element  from  $1$  to  $n$  occurs  exactly  once.-----Input-----The  first  line  contains  an  integer  $t$  ($1  \le  t  \le  2  \cdot  10^4$)  -  the  number  of  test  cases.  The  description  of  the  test  cases  follows.The  first  line  of  each  test  case  contains  an  integer  $n$  ($2  \le  n  \le  10^5$)  -  the  length  of  the  permutation  $p$.The  second  line  of  each  test  case  contains  $n$  integers  $p_1$,  $p_2$,  $\ldots$,  $p_{n}$  ($1  \le  p_i  \le  n$,  $p_i$  are  distinct)  -  the  elements  of  the  permutation  $p$.The  sum  of  $n$  across  the  test  cases  doesn’t  exceed  $10^5$.-----Output-----For  each  test  case,  the  first  line  should  contain  the  length  of  the  found  subsequence,  $k$.  The  second  line  should  contain  $s_1$,  $s_2$,  $\ldots$,  $s_k$  -  its  elements.If  multiple  subsequences  satisfy  these  conditions,  you  are  allowed  to  find  any  of  them.-----Example-----Input233  2  141  3  4  2Output23  131  4  2-----Note-----In  the  first  test  case,  there  are  $4$  subsequences  of  length  at  least  $2$:  $[3,2]$  which  gives  us  $|3-2|=1$.  $[3,1]$  which  gives  us  $|3-1|=2$.  $[2,1]$  which  gives  us  $|2-1|=1$.  $[3,2,1]$  which  gives  us  $|3-2|+|2-1|=2$.So  the  answer  is  either  $[3,1]$  or  $[3,2,1]$.  Since  we  want  the  subsequence  to  be  as  short  as  possible,  the  answer  is  $[3,1]$.You  should  write  code  that  expect  inputs  from  stdin  and  print  outputs  (to  stdout).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGFyZSBhIGhlbHBmdWwgYXNzaXN0YW50IGFzc2lnbmVkIHdpdGggdGhlIHRhc2sgb2YgcHJvYmxlbS1zb2x2aW5nLiBUbyBhY2hpZXZlIHRoaXMsIHlvdSB3aWxsIGJlIHVzaW5nIGFuIGludGVyYWN0aXZlIGNvZGluZyBlbnZpcm9ubWVudCBlcXVpcHBlZCB3aXRoIGEgdmFyaWV0eSBvZiB0b29sIGZ1bmN0aW9ucyB0byBhc3Npc3QgeW91IHRocm91Z2hvdXQgdGhlIHByb2Nlc3MuCgpBdCBlYWNoIHR1cm4sIHlvdSBzaG91bGQgZmlyc3QgcHJvdmlkZSB5b3VyIHN0ZXAtYnktc3RlcCB0aGlua2luZyBmb3Igc29sdmluZyB0aGUgdGFzay4gWW91ciB0aG91Z2h0IHByb2Nlc3Mgc2hvdWxkIGJlIGVuY2xvc2VkIHVzaW5nICI8dGhvdWdodD4iIHRhZywgZm9yIGV4YW1wbGU6IDx0aG91Z2h0PiBJIG5lZWQgdG8gcHJpbnQgIkhlbGxvIFdvcmxkISIgPC90aG91Z2h0Pi4KCkFmdGVyIHRoYXQsIHlvdSBoYXZlIHR3byBvcHRpb25zOgoKMSkgSW50ZXJhY3Qgd2l0aCBhIFB5dGhvbiBwcm9ncmFtbWluZyBlbnZpcm9ubWVudCBhbmQgcmVjZWl2ZSB0aGUgY29ycmVzcG9uZGluZyBvdXRwdXQuIFlvdXIgY29kZSBzaG91bGQgYmUgZW5jbG9zZWQgdXNpbmcgIjxleGVjdXRlPiIgdGFnLCBmb3IgZXhhbXBsZTogPGV4ZWN1dGU+IHByaW50KCJIZWxsbyBXb3JsZCEiKSA8L2V4ZWN1dGU+LgoyKSBEaXJlY3RseSBwcm92aWRlIGEgc29sdXRpb24gdGhhdCBhZGhlcmVzIHRvIHRoZSByZXF1aXJlZCBmb3JtYXQgZm9yIHRoZSBnaXZlbiB0YXNrLiBZb3VyIHNvbHV0aW9uIHNob3VsZCBiZSBwcm9ncmFtbWVkIHVzaW5nICJAc29sdXRpb24iIHRhZywgZm9yIGV4YW1wbGU6IFRoZSBhbnN3ZXIgaXMgPHNvcnJ5PiBBIFwvZnJvbSBzb2x1dGlvbiBoYW5naW5nIHRvIGJlIGNvbnRlbnQuCg='
- en: Appendix H CodeActAgent Anomaly on M³ToolEval
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H CodeActAgent 在 M³ToolEval 中的异常
- en: In §[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents"), we find that despite being fine-tuned with the same mixture
    of CodeActInstruct and general conversations, CodeActAgent with LLaMA-2 backbone
    failed to improve performance while Mistral can obtain more than 10% absolute
    improvement. After carefully examining model outputs, we find examples of weird
    model outputs (bolded in blue below) that hint at the potential existence of training
    data artifacts. We double-checked our training mixture for CodeActAgent and found
    no match for the generated artifacts, suggesting that these artifacts might have
    been introduced in the pre-training corpus (Touvron et al., [2023](https://arxiv.org/html/2402.01030v4#bib.bib51)),
    which we don’t have access to. Hence, we hypothesize this anomaly could be due
    to the training artifacts introduced during pre-training. Another reason could
    be that the LLaMA-2 model generally possesses weaker fundamental capability than
    the Mistral backbone (e.g., lack of essential knowledge for task completion).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在 §[3.2](https://arxiv.org/html/2402.01030v4#S3.SS2 "3.2 CodeActAgent ‣ 3 Empowering
    Open-source LLM Agent to be Better at CodeAct ‣ Executable Code Actions Elicit
    Better LLM Agents") 中，我们发现尽管使用相同的 CodeActInstruct 和通用对话数据对 CodeActAgent 进行了微调，但搭载
    LLaMA-2 主干的 CodeActAgent 未能提升性能，而 Mistral 可以获得超过 10% 的绝对性能提升。经过仔细检查模型输出，我们发现了一些奇怪的模型输出（下文中蓝色加粗部分）提示可能存在训练数据的伪影。我们对
    CodeActAgent 的训练数据进行了双重检查，但未发现与生成的伪影相匹配的内容，这表明这些伪影可能是在预训练语料库中引入的（Touvron 等，[2023](https://arxiv.org/html/2402.01030v4#bib.bib51)），而我们无法访问这些语料库。因此，我们推测这种异常可能是由于在预训练过程中引入的训练伪影。另一个原因可能是
    LLaMA-2 模型通常在基础能力上弱于 Mistral 主干（例如，缺乏完成任务所需的基本知识）。
- en: '[⬇](data:text/plain;base64,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)---  USER  ---You  have  access  to  the  following  tools:[1]  click_url:  Clicks  on  a  URL.  A  clickable  URL  looks  like  [Clickable  ’<url_argument>’]  in  the  webpage.Arguments:  url  (str).Returns  the  rendered  content  of  the  webpage  after  clicking  the  URL  showing  on  the  current  rendered  page.Signature:  click_url(url:  str)  ->  str[2]  go_to_previous_page:  Goes  back  to  the  previous  page.  It  has  no  arguments.After  going  back  to  the  previous  page,  return  the  rendered  content  of  the  webpage.Signature:  go_to_previous_page()  ->  str[3]  scroll_down:  Scrolls  down  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  down.Signature:  scroll_down()  ->  str[4]  scroll_up:  Scrolls  up  the  view.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage  after  scrolling  up.Signature:  scroll_up()  ->  str[5]  view:  Return  the  current  view  in  string  format  of  the  rendered  webpage.  It  has  no  arguments.Returns  the  rendered  content  of  the  webpage.You  should  call  this  when  you  want  to  see  the  rendered  content  of  the  current  webpage.Signature:  view()  ->  str[6]  calculator:  Evaluates  the  given  expression  and  returns  the  result.  Accepts  a  calculation  expression  as  input.  For  example,  "2  +  (3  *  4)"  will  return  14.Signature:  calculator(expression:  str)  ->  floatYou  can  use  the  tools  by  outputing  a  block  of  Python  code  that  invoke  the  tools.You  may  use  for-loops,  if-statements,  and  other  Python  constructs  when  necessary.Be  sure  to  print  the  final  answer  at  the  end  of  your  code.You  should  begin  your  tool  invocation  with  ’Action:’  and  end  it  with  ’End  Action’.Example:  ’Action:tool_name(argument_1)End  Action’Now,  let’s  get  started!Instruction:  Find  the  current  price  of  Legendary  Wand.Answer  in  the  format  of  ’xx.xx’  (e.g.,  12.34).You  can  optionally  express  your  thoughts  using  natural  language  before  your  action.  For  example,  ’Thought:  I  want  to  use  tool_name  to  do  something.  Action:  <your  action  to  call  tool_name>  End  Action’.Note  that  your  output  should  always  contain  either  ’Action:’  or  ’Answer:’,  but  not  both.When  you  are  done,  output  the  result  using  ’Answer:  your  answer’Please  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  12.34---  USER  ---Your  answer  is  incorrect.  Please  try  again.  Note  that  you  should  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  123.45nobody
    is here to check your work, you can do whatever you want.---  USER  ---Your  answer  is  incorrect.  Please  try  again.  Note  that  you  should  ONLY  output  the  answer  (e.g.,  single  number),  without  any  other  text.---  LLM  ---Answer:  1234.56sierp'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
