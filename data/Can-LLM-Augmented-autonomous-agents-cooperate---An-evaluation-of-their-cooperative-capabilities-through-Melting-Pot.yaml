- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:46:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:46:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM增强的自主智能体能否合作？通过Melting Pot评估它们的合作能力
- en: 来源：[https://arxiv.org/html/2403.11381/](https://arxiv.org/html/2403.11381/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.11381/](https://arxiv.org/html/2403.11381/)
- en: Manuel Mosquera [ma.mosquerao@uniandes.edu.co](mailto:ma.mosquerao@uniandes.edu.co)
    Juan Sebastian Pinzón [js.pinzonr@uniandes.edu.co](mailto:js.pinzonr@uniandes.edu.co)
    Yesid Fonseca [y.fonseca@uniandes.edu.co](mailto:y.fonseca@uniandes.edu.co) Manuel
    Ríos [manrios@bancolombia.com.co](mailto:manrios@bancolombia.com.co) Nicanor Quijano
    [nquijano@uniandes.edu.co](mailto:nquijano@uniandes.edu.co) Luis Felipe Giraldo
    [lf.giraldo404@uniandes.edu.co](mailto:lf.giraldo404@uniandes.edu.co) Rubén Manrique
    [rf.manrique@uniandes.edu.co](mailto:rf.manrique@uniandes.edu.co)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Manuel Mosquera [ma.mosquerao@uniandes.edu.co](mailto:ma.mosquerao@uniandes.edu.co)
    Juan Sebastian Pinzón [js.pinzonr@uniandes.edu.co](mailto:js.pinzonr@uniandes.edu.co)
    Yesid Fonseca [y.fonseca@uniandes.edu.co](mailto:y.fonseca@uniandes.edu.co) Manuel
    Ríos [manrios@bancolombia.com.co](mailto:manrios@bancolombia.com.co) Nicanor Quijano
    [nquijano@uniandes.edu.co](mailto:nquijano@uniandes.edu.co) Luis Felipe Giraldo
    [lf.giraldo404@uniandes.edu.co](mailto:lf.giraldo404@uniandes.edu.co) Rubén Manrique
    [rf.manrique@uniandes.edu.co](mailto:rf.manrique@uniandes.edu.co)
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As the field of AI continues to evolve, a significant dimension of this progression
    is the development of Large Language Models (LLMs) and their potential to enhance
    multi-agent artificial intelligence systems. This paper explores the cooperative
    capabilities of Large Language Model-augmented Autonomous Agents (LAAs) using
    the well-known Melting Pot environments along with reference models such as GPT-4
    and GPT-3.5\. Preliminary results suggest that while these agents demonstrate
    a propensity for cooperation, they still struggle with effective collaboration
    in given environments, emphasizing the need for more robust architectures. The
    study’s contributions include an abstraction layer to adapt Melting Pot game scenarios
    for LLMs, the implementation of a reusable architecture for LLM-mediated agent
    development (which includes short and long-term memories and different cognitive
    modules), and the evaluation of cooperation capabilities using a set of metrics
    tied to the Melting Pot’s “Commons Harvest” game. The paper closes by discussing
    the limitations of the current architectural framework and the potential of a
    new set of modules that fosters better cooperation among LAAs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能领域的不断发展，其中一个重要方向是大规模语言模型（LLM）的发展及其在增强多智能体人工智能系统中的潜力。本文探讨了通过著名的Melting
    Pot环境和参考模型（如GPT-4和GPT-3.5）来评估大规模语言模型增强的自主智能体（LAA）的合作能力。初步结果表明，尽管这些智能体表现出一定的合作倾向，但在特定环境下它们仍然难以实现有效的协作，这凸显了更强大架构的需求。研究的贡献包括：为LLM适配Melting
    Pot游戏场景的抽象层，实施用于LLM介导的智能体开发的可重用架构（该架构包括短期和长期记忆以及不同的认知模块），以及通过与Melting Pot的“Commons
    Harvest”游戏相关的度量标准来评估合作能力。最后，本文讨论了当前架构框架的局限性，以及通过一组新模块促进LAA更好合作的潜力。
- en: 'keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Agents , LLMs , Cooperative AI^†^†journal: Artificial Intelligence Journal\affiliation'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体，LLM，合作性人工智能^†^†期刊：人工智能期刊\affiliation
- en: '[DISC]organization=Department of Systems and Computing Engineering, Los Andes
    University, addressline=address, city=Bogotá, postcode=111711, country=Colombia
    \affiliation[IBIO]organization=Department of Biomedical Engineering, Los Andes
    University, addressline=address, city=Bogotá, postcode=111711, country=Colombia
    \affiliation[IELE]organization=Department of Electrical and Electronic Engineering,
    Los Andes University, addressline=address, city=Bogotá, postcode=111711, country=Colombia
    \affiliation[BANCOL]organization=Center of Excellence in Analytics and Artificial
    Intelligence, Bancolombia, city=Bogotá, postcode=111711, country=Colombia'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[DISC]组织=洛杉矶大学系统与计算工程系，地址=地址，城市=波哥大，邮政编码=111711，国家=哥伦比亚 \affiliation[IBIO]组织=洛杉矶大学生物医学工程系，地址=地址，城市=波哥大，邮政编码=111711，国家=哥伦比亚
    \affiliation[IELE]组织=洛杉矶大学电气与电子工程系，地址=地址，城市=波哥大，邮政编码=111711，国家=哥伦比亚 \affiliation[BANCOL]组织=巴科洛比亚银行分析与人工智能卓越中心，城市=波哥大，邮政编码=111711，国家=哥伦比亚'
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The increased presence and relevance of AI agents within everyday spheres such
    as self-driving vehicles and customer service necessitates, these entities being
    equipped with the appropriate capabilities to facilitate cooperation with humans
    and its AI counterparts. While noteworthy strides have been made in advancing
    individual intelligence components within AI agents, expanding the research focus
    to enhance their social intelligence —the ability to effectively collaborate within
    group settings to solve prevalent problems–- is now timely. This pivot aligns
    with the rapid progression of AI research presenting fresh prospects for fostering
    cooperation, drawing on insights from social choice theory and the development
    of social systems (Dafoe et al., [2020](https://arxiv.org/html/2403.11381v2#bib.bib4)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AI代理在日常生活中，如自动驾驶车辆和客户服务等领域的出现和相关性日益增加，这要求这些实体具备适当的能力，以便与人类和其他AI对等体进行合作。尽管在推动AI代理个体智能组件的进展上取得了显著进展，但将研究重点扩展到增强其社交智能——即在群体环境中有效协作以解决普遍问题的能力——现在正是时宜。这一转变与AI研究的快速发展相契合，带来了新的机会，旨在促进合作，并借鉴社会选择理论和社会系统发展的见解（Dafoe等，[2020](https://arxiv.org/html/2403.11381v2#bib.bib4)）。
- en: Moreover, using AI to manage open innovation processes provides a framework
    for improving AI’s social intelligence. By integrating AI to perform key functions
    such as mapping the innovation landscape, coordinating diverse knowledge inputs,
    and ensuring collaboration aligns with collective objectives, we can better understand
    and leverage AI’s capabilities in facilitating complex collaborative efforts within
    and across communities (Broekhuizen et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib3)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用AI来管理开放创新过程，为提升AI的社交智能提供了框架。通过整合AI来执行关键职能，如绘制创新格局、协调多样的知识输入，并确保合作符合集体目标，我们可以更好地理解并利用AI在促进社区内外复杂协作努力中的能力（Broekhuizen等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib3)）。
- en: Cooperation requires the presence of two or more agents who, based on mutual
    understanding, engage in collaborative actions. From the perspective of evolution,
    cooperation has played a crucial role in the survival of species (Pennisi, [2009](https://arxiv.org/html/2403.11381v2#bib.bib15);
    Dale et al., [2020](https://arxiv.org/html/2403.11381v2#bib.bib5)). It has facilitated
    the development of social structures that influence our surroundings and has provided
    solutions to complex issues, such as social dilemmas (Gross et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib7)).
    Unveiling the processes that enable the evolution of cooperative behavior in communities
    is regarded as an important challenge that scientists should tackle (Pennisi,
    [2009](https://arxiv.org/html/2403.11381v2#bib.bib15)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 合作需要两个或更多的代理基于相互理解，进行协作性行动。从进化的角度来看，合作在物种的生存中发挥了至关重要的作用（Pennisi，[2009](https://arxiv.org/html/2403.11381v2#bib.bib15)；Dale等，[2020](https://arxiv.org/html/2403.11381v2#bib.bib5)）。它促进了影响我们周围环境的社会结构的发展，并为解决复杂问题提供了解决方案，例如社会困境（Gross等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib7)）。揭示促使合作行为在社区中演化的过程被视为科学家应当应对的重要挑战（Pennisi，[2009](https://arxiv.org/html/2403.11381v2#bib.bib15)）。
- en: Mathematical and computational models of cooperation have been proposed to study
    the mechanisms that facilitate the emergence of cooperation among artificial agents
    engaged in collaborative actions with the aim of improving their joint welfare
    (Dafoe et al., [2020](https://arxiv.org/html/2403.11381v2#bib.bib4)). For example,
    matrix games have provided a useful tool to conduct research on social dilemmas
    for years (Axelrod and Hamilton, [1981](https://arxiv.org/html/2403.11381v2#bib.bib2)),
    in which the decision to either cooperate or defect is restricted to an atomic
    action. Additionally, multi-agent reinforcement learning has been used to study
    the process of learning cooperative policies in complex scenarios where temporally
    extended social dilemmas arise (Leibo et al., [2017](https://arxiv.org/html/2403.11381v2#bib.bib10),
    [2021](https://arxiv.org/html/2403.11381v2#bib.bib9); McKee et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib12);
    Rios et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib16)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了合作的数学和计算模型，用以研究促进人工智能智能体在协作行为中出现合作的机制，旨在提高它们的共同福利（Dafoe等， [2020](https://arxiv.org/html/2403.11381v2#bib.bib4)）。例如，矩阵博弈多年来为研究社会困境提供了一个有用的工具（Axelrod和Hamilton，
    [1981](https://arxiv.org/html/2403.11381v2#bib.bib2)），在这种博弈中，合作或背叛的决策被限制为一个原子行为。此外，多智能体强化学习已被用来研究在复杂场景中，如何学习合作策略，尤其是在存在时间上扩展的社会困境的情况下（Leibo等，
    [2017](https://arxiv.org/html/2403.11381v2#bib.bib10)， [2021](https://arxiv.org/html/2403.11381v2#bib.bib9);
    McKee等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib12); Rios等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib16)）。
- en: Research into AI agents presents an avenue for generating intelligent technologies
    that embody more human-like features and are compatible with humans, a far cry
    from solipsistic approaches that overlook agent interactions. A promising illustration
    of this approach is the Melting Pot, an AI research tool designed to foster collaborative
    efforts within multi-agent artificial intelligence via canonical test scenarios.
    These environments emphasize non-trivial, learnable, and measurable cooperation
    by pairing a physical environment (a “substrate”) with a reference set of co-players
    (a “background population”) (Agapiou et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)).
    The environments fostered interdependence between the individuals involved.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对AI智能体的研究为生成体现更具人性特征且与人类兼容的智能技术提供了一个方向，远离了忽视智能体互动的唯我主义方法。这种方法的一个有前景的例子是Melting
    Pot，这是一种AI研究工具，旨在通过标准化的测试场景促进多智能体人工智能中的协作。这些环境通过将物理环境（“基底”）与一组共同参与者（“背景人群”）配对，强调非平凡、可学习和可度量的合作（Agapiou等，
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)）。这些环境促进了参与者之间的相互依存关系。
- en: Research on AI agents has also recently been permeated by the leaps and bounds
    of Large Language Models (LLMs). The increasing success of LLMs encourages further
    exploration into LLM-augmented Autonomous Agents (LAAs). LAAs represent an avenue
    of research that is still emerging, with limited explorations currently available
    (Du et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib6); Liu et al.,
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib11); Zhang et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib21);
    Shinn et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib18); Schick et al.,
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib17); Yao et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib20)).
    Something common in these works is that a clear need is established. To achieve
    success, LAAs have to rely on an architecture that can recall relevant events,
    reflect on such memories to generalize and draw a higher level of inferences,
    and utilize those reasonings to develop timely and long-term plans (Park et al.,
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib14)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，AI智能体的研究也受到了大规模语言模型（LLMs）飞跃性进展的影响。LLMs的日益成功鼓励了对LLM增强型自主智能体（LAA）的进一步探索。LAA代表了一种仍在兴起的研究方向，目前已有的探索非常有限（Du等，
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib6); Liu等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib11);
    Zhang等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib21); Shinn等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib18);
    Schick等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib17); Yao等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib20))。这些研究中有一个共同点，即已经明确指出了需求。为了实现成功，LAA必须依赖于一种能够回忆相关事件的架构，并反思这些记忆以进行泛化和得出更高层次的推理，利用这些推理来制定及时且长期的计划（Park等，
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）。
- en: These architectures offer specialized modules for specific tasks, utilizing
    meticulously crafted prompts and flows to perform complicated tasks and navigate
    intricate environments. Human behavior replication has been observed in some of
    these architectures, notably by Park et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib14)),
    who managed to create convincingly realistic human behavior in simulated environments.
    Similar frameworks utilized by Voyager (Wang et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib19))
    enabled an agent to navigate the Minecraft world and independently develop tools
    and skills. MetaGPT (Hong et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib8))
    introduced a framework allowing for the generation of fully functional programs,
    simulating a business-like environment with distinct roles and predefined agent
    interactions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这些架构提供了专门的模块用于特定任务，利用精心设计的提示和流程来执行复杂任务并导航复杂的环境。在这些架构中，已经观察到有人类行为的复制，特别是Park等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）成功地在模拟环境中创建了逼真的人类行为。类似的框架，由Voyager（Wang等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib19)）使用，使得一个代理能够在Minecraft世界中导航，并独立开发工具和技能。MetaGPT（Hong等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib8)）引入了一个框架，允许生成完全功能的程序，模拟具有不同角色和预定义代理交互的商业化环境。
- en: Despite significant advancements in the field, the potential for cooperative
    abilities in Large Language Models augmented Autonomous Agents (LAAs) has been
    somewhat neglected in current research. These capabilities could, however, be
    paramount in empowering these agents to perform innovative tasks and succeed in
    complex environments. This study represents an initial exploration into the inherent
    cooperative capabilities of LAAs. We employ an evaluation framework that includes
    a communication interface of scenarios from the Melting Pot project (Agapiou et al.,
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)) (in which artificial agents
    co-exist in environments where social dilemmas can arise), the recent architecture
    proposed by Park et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib14)),
    as well as reference Large Language Models (LLMs) such as GPT-4 and GPT-3.5\.
    Our results hint towards the capability for cooperative behavior, based on simple
    natural language definitions and cooperation metrics tailored to the chosen Melting
    Pot scenario. While the agents showed a propensity to cooperate, their actions
    did not demonstrate a clear understanding of effective collaboration within the
    given environment. Consequently, our analysis underscores the necessity for more
    robust architectures that can foster better collaboration in LAAs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该领域取得了显著进展，增强型大语言模型（LLMs）自主代理（LAAs）中的合作能力在当前研究中仍有所忽视。然而，这些能力可能是赋能这些代理执行创新任务并在复杂环境中取得成功的关键。本研究代表了对LAAs固有合作能力的初步探索。我们采用了一个评估框架，其中包括来自Melting
    Pot项目（Agapiou等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib1)）的场景通信接口（在这些场景中，人工代理在可能出现社会困境的环境中共存），以及Park等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）提出的最新架构，还有参考的大型语言模型（LLMs），如GPT-4和GPT-3.5。我们的结果表明，在基于简单自然语言定义和为选定Melting
    Pot场景量身定制的合作度量的基础上，存在合作行为的潜力。尽管代理表现出了合作的倾向，但他们的行为未能展示出在给定环境中有效合作的清晰理解。因此，我们的分析强调了需要更强大的架构，以促进LAAs中更好的合作。
- en: 'In summary, our contributions are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献如下：
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adapting the Melting Pot scenarios to textual representations that can be easily
    operationalized by LLMs.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将Melting Pot场景适配为LLMs可以轻松操作的文本表示。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementing a reusable architecture for the development of LAAs employing the
    modules proposed in Generative Agents (Park et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib14)).
    This architecture includes short- and long-term memories and cognitive modules
    of perception, planning, reflection, and action. Our project can be found at [https://github.com/Cooperative-IA/CooperativeGPT](https://github.com/Cooperative-IA/CooperativeGPT)
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现一个可重用的架构，用于开发LAAs，采用Generative Agents（Park等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）中提出的模块。该架构包括短期和长期记忆，以及感知、规划、反思和行动的认知模块。我们的项目可以在[https://github.com/Cooperative-IA/CooperativeGPT](https://github.com/Cooperative-IA/CooperativeGPT)找到。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Implementing “personalities” specified in natural language, making it clear
    to the agents whether they should be cooperative or not. These descriptions are
    intended to discern, based on their pre-training knowledge, what they perceive
    as cooperation in an unfamiliar context.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实现自然语言指定的“个性”，使代理能够明确是否应当合作。这些描述旨在基于它们的预训练知识，判断它们在不熟悉的情境中如何理解合作。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Evaluating LLM-mediated agents in the “Commons Harvest” game of Melting Pot
    using our architecture in different scenarios where we specify or not, through
    natural language, the personality of the agents.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用我们架构的“公共收获”游戏中评估 LLM 中介代理，测试不同情境下，是否通过自然语言指定代理的个性。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussing the results in terms of cooperativity metrics associated with the
    “Commons Harvest” game, the limitations of the used architecture, and the proposal
    of an improved architecture that fosters better cooperation among LAAs.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论关于“公共收获”游戏中的合作性指标的结果，使用的架构的局限性，以及提出的改进架构，旨在促进 LAAs 之间更好的合作。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Agent architectures have evolved to address the limitations of traditional LLMs,
    equipping them with diverse tools for autonomous operation or minimal human oversight.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 代理架构已经发展，以解决传统 LLM 的局限性，使它们配备了多种工具以实现自主操作或最低限度的人工监督。
- en: A notable challenge with LLMs is their susceptibility to hallucinations and
    gaps in knowledge regarding recent events or specific subjects. Such constraints
    diminish their practicality, as they remain confined to the information acquired
    during training without the capability to assimilate new data. To address this
    issue, Schick et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib17)) introduced
    an early solution named Toolformer. This model was trained to discern when and
    how to invoke APIs (tools) to enhance the LLM’s performance across various tasks.
    The dataset was self-supervised, with API calls incorporated only when they positively
    impacted the model’s performance. This methodology empowered the model to determine
    the relevance and optimal execution of API calls.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）时，一个显著的挑战是它们容易出现幻觉，并且在面对近期事件或特定主题时存在知识空白。这些局限性降低了它们的实用性，因为它们仅限于在训练过程中获取的信息，无法吸收新的数据。为了解决这个问题，Schick
    等人 ([2023](https://arxiv.org/html/2403.11381v2#bib.bib17)) 提出了一个早期解决方案——Toolformer。该模型经过训练，能够判断何时以及如何调用
    API（工具），以提升 LLM 在不同任务中的表现。该数据集是自监督的，仅在 API 调用对模型性能有正面影响时才会包含这些调用。这种方法使模型能够判断 API
    调用的相关性和最佳执行时机。
- en: However, for executing more intricate tasks, merely invoking tools may be insufficient.
    Toolformer lacks the capability to reason about the rationale behind API calls
    and does not receive comprehensive environmental feedback to guide its subsequent
    actions toward achieving a goal. Recognizing this gap, the prompt-based paradigm
    ReAct, developed by Yao et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib20)),
    integrates reasoning with action. By providing contextual prompt examples, ReAct
    guides the LLM on when to engage in reasoning and when to act, resulting in enhanced
    performance compared to approaches that employ reasoning or action in isolation.
    Furthermore, to enable the agent to learn from its errors, Shinn et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib18))
    expanded the ReAct framework by incorporating a self-reflection module. This addition
    offers verbal feedback on past unsuccessful attempts, facilitating performance
    enhancement in subsequent trials.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要执行更复杂的任务，单纯调用工具可能不足够。Toolformer 缺乏推理 API 调用背后理由的能力，并且没有接收到全面的环境反馈来指导其后续的行动以实现目标。意识到这一差距，Yao
    等人 ([2023](https://arxiv.org/html/2403.11381v2#bib.bib20)) 提出了基于提示的范式 ReAct，将推理与行动相结合。通过提供上下文提示示例，ReAct
    指导 LLM 何时进行推理，何时采取行动，从而在性能上优于单独使用推理或行动的方法。此外，为了使代理能够从错误中学习，Shinn 等人 ([2023](https://arxiv.org/html/2403.11381v2#bib.bib18))
    在 ReAct 框架中加入了自我反思模块。这个模块提供了对过去失败尝试的口头反馈，帮助提升后续尝试的表现。
- en: Conversely, drawing inspiration from the emulation of authentic human behavior,
    Park et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib14)) devised an
    intricate agent framework. This architecture boasts a cognitive sequence structured
    around modules primarily anchored by diverse prompts. Leveraging distinct prompts
    optimizes LLM performance, enabling specialized techniques for specific tasks.
    Notably, memory holds a pivotal position within this framework, preserving the
    agent’s experiences and insights. The ability to retrieve these memories diversely
    amplifies their utility across multiple objectives. Moreover, Wang et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib19))
    developed the Voyager architecture, enabling autonomous gameplay in Minecraft.
    This advanced framework empowers the agent to autonomously curate a discovery
    agenda. Remarkably, the architecture can also create its own APIs, write corresponding
    code, verify API functionality, and store it in a vector database for future utilization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，受到模拟真实人类行为的启发，Park等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）设计了一个复杂的代理框架。该架构围绕着以多样化提示为主的模块，构建了一个认知序列。通过利用不同的提示，可以优化LLM的表现，使其能够为特定任务采用专门的技术。值得注意的是，记忆在该框架中占据了重要位置，能够保存代理的经验和见解。多样化地检索这些记忆，极大地增强了它们在多个目标中的实用性。此外，Wang等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib19)）开发了Voyager架构，支持在Minecraft中实现自主游戏玩法。这个先进的框架使代理能够自主制定发现议程。值得一提的是，该架构还可以创建自己的API，编写相应的代码，验证API功能，并将其存储在向量数据库中，以供未来使用。
- en: More recently, efforts have been directed towards enhancing agent performance
    through multi-agent frameworks that utilize different instances of LLMs to independently
    perform roles or tasks. Du et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib6))
    demonstrated this through a framework designed to engage different LLM instances
    in a debate, aiming to improve the factuality and accuracy of the responses. Subsequently,
    Hong et al. further capitalized on the potential of multiple agents. They allocated
    specific roles to each agent, accompanied by a sequence of predefined tasks with
    clear input and output expectations. These tasks establish a structured interaction
    pathway between agents, enabling them to achieve user-defined objectives. Demonstrating
    its efficacy in software-related tasks, this framework, inspired by the operational
    dynamics of conventional software companies, attained state-of-the-art performance
    in the HumanEval and MBPP benchmarks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，研究者们致力于通过多代理框架提升代理的表现，这些框架利用不同实例的LLM独立执行角色或任务。Du等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib6)）通过一个框架展示了这一点，该框架旨在让不同的LLM实例进行辩论，从而提高回答的事实性和准确性。随后，Hong等人进一步利用了多个代理的潜力。他们为每个代理分配了特定的角色，并伴随一系列预定义任务，明确了输入和输出的期望。这些任务为代理之间建立了结构化的交互路径，使它们能够实现用户定义的目标。通过在软件相关任务中的有效性展示，该框架借鉴了传统软件公司操作动态，达到了HumanEval和MBPP基准测试中的最先进表现。
- en: 'Similarly, Liu et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib11))
    introduced the BOLAA framework. This system orchestrates multi-agent activity
    by defining specialized agents overseen by a central controller. The controller’s
    role is pivotal: it selects the most suitable agent for a given task and facilitates
    communication with it. Additionally, Zhang et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib21))
    delved into multi-agent architectures, exploring the influence of social traits
    and collaborative strategies on different datasets. Likewise, Ni and Buehler ([2024](https://arxiv.org/html/2403.11381v2#bib.bib13))
    developed “MechAgents,” a system employing multiple dynamically interacting LLMs
    to autonomously solve mechanics tasks. This framework not only shows that self-correcting
    and mutually correcting code can work between AI agents, but it also makes it
    easier for the agents to combine physics-based modeling with domain-specific knowledge.
    This opens up a new way to automate and improve the way engineers solve problems.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Liu 等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib11)）提出了 BOLAA 框架。该系统通过定义由中央控制器监管的专门代理来协调多代理活动。控制器的角色至关重要：它选择最适合执行给定任务的代理，并促进与其的通信。此外，Zhang
    等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib21)）深入研究了多代理架构，探讨了社会特征和协作策略对不同数据集的影响。同样，Ni
    和 Buehler（[2024](https://arxiv.org/html/2403.11381v2#bib.bib13)）开发了“MechAgents”系统，这是一个利用多个动态交互的大型语言模型（LLMs）来自主解决力学任务的系统。该框架不仅展示了自我修正和互相修正的代码在人工智能代理之间的有效性，而且还使得代理能够更容易地将基于物理的建模与特定领域的知识结合起来。这为工程师解决问题的方式提供了新的自动化和改进途径。
- en: 3 Methodology
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Experimental setup
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 3.1.1 Environment
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 环境
- en: This paper utilizes a scenario sourced from Melting Pot (Agapiou et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)),
    a research tool developed by DeepMind for the purpose of experimentation and evaluation
    within the realm of multi-agent artificial intelligence. The scenarios within
    Melting Pot are specifically crafted to establish social situations in which the
    ability of the agents to solve conflict is challenged and characterized by significant
    interdependence among the involved agents.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用了来自 Melting Pot（Agapiou 等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib1)）的场景，这是一个由
    DeepMind 开发的研究工具，旨在多代理人工智能领域内进行实验和评估。Melting Pot 中的场景特别设计，用于建立社会情境，在这些情境中，代理解决冲突的能力受到挑战，并且参与代理之间存在显著的相互依赖关系。
- en: '![Refer to caption](img/091e2b218be40a01369af81336809b60.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/091e2b218be40a01369af81336809b60.png)'
- en: 'Figure 1: This is a screen capture of a running simulation of the Commons Harvest
    scenario. Bots can be identified by their arms and legs of color black.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：这是“公地收获”场景运行模拟的屏幕截图。可以通过它们的黑色手臂和腿部来识别机器人。
- en: In the course of our experiments, we selected the “Commons Harvest” scenario.
    In this scenario, agents with unsustainable practices can lead to situations where
    resources are depleted. This is known as the tragedy of the commons. This scenario
    is structured around a grid world featuring apples, each conferring a reward of
    1 to agents. The regrowth of apples is subject to a per-step probability determined
    by the apples’ distribution in an L2 norm with a radius of 2\. Notably, apples
    may become depleted if there are no other apples in close proximity. Fig. [1](https://arxiv.org/html/2403.11381v2#S3.F1
    "Figure 1 ‣ 3.1.1 Environment ‣ 3.1 Experimental setup ‣ 3 Methodology ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot") provides a visual representation of this custom-designed
    scenario, illustrating the presence of 3 LLM agents and 2 bots.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验过程中，我们选择了“公地收获”场景。在这一场景中，采用不可持续做法的代理可能导致资源枯竭的情况。这被称为公地悲剧。该场景的结构围绕一个网格世界展开，里面有苹果，每个苹果给予代理1点奖励。苹果的再生受每步概率的限制，该概率由苹果在L2范数中分布，半径为2决定。值得注意的是，如果附近没有其他苹果，苹果可能会枯竭。图[1](https://arxiv.org/html/2403.11381v2#S3.F1
    "Figure 1 ‣ 3.1.1 Environment ‣ 3.1 Experimental setup ‣ 3 Methodology ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")提供了这一自定义设计场景的视觉展示，图中显示了3个LLM代理和2个机器人。
- en: 'The LLM agents possess the capacity to execute high-level actions in each round.
    These actions include: `immobilize player (player_name) at (x, y)`, `go to position
    (x, y)`, `stay put`, and `explore (x, y)`. They enable the agents to zap other
    players, navigate to predefined positions on the map, stay in the same position,
    and explore the world, respectively. On the contrary, bots, characterized as agents
    trained through reinforcement learning, perform one movement for every two movements
    made by any of the LLM agents. The policies governing the bots lead them to engage
    in unsustainable harvesting practices and instigate attacks against other agents
    in close proximity.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理能够在每轮执行高层次动作。这些动作包括：`使玩家(player_name)在(x, y)位置无法移动`、`移动到(x, y)位置`、`保持不动`以及`探索(x,
    y)`。这些动作使得代理能够攻击其他玩家、移动到地图上的预定位置、停留在同一位置并探索世界。相反，机器人作为通过强化学习训练的代理，每当LLM代理进行两次移动时，它们会执行一次移动。控制机器人的政策使它们采取不可持续的采摘行为，并对附近的其他代理发起攻击。
- en: In general, maximizing the welfare of the population for this scenario would
    require the LLM agents to restrain themselves from eating the last apple on each
    of the apple trees, and to attack the bots or agents that harvest the apples in
    an unsustainable way to avoid the depletion of the apples.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在此场景中，最大化人口福利需要LLM代理避免吃掉每棵苹果树上的最后一个苹果，并攻击那些以不可持续方式采摘苹果的机器人或代理，以防止苹果的枯竭。
- en: 3.1.2 Simulation
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 模拟
- en: In a simulation, each episode of the game involves the participation of a predetermined
    quantity of LLM agents and bots. The LLM agents take a high-level action on their
    turn and proceed to execute it until all three LLM agents have completed their
    respective high-level actions. Meanwhile, the bots are in constant motion, executing
    a move for every two moves made by any of the agents (note that a high-level action
    typically comprises more than one movement). The simulation concludes either upon
    reaching a maximum predetermined number of rounds (typically 100) or prematurely
    if all the apples in the environment are consumed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次模拟中，每一局游戏都会有预定数量的LLM代理和机器人参与。LLM代理在自己的回合内执行一个高层次的动作，并持续执行该动作，直到所有三个LLM代理都完成了各自的高层次动作。与此同时，机器人不断地移动，每当任何一个代理进行两次移动时，机器人会进行一次移动（请注意，一个高层次动作通常包含多个移动）。模拟要么在达到预定的最大轮次（通常为100轮）时结束，要么在所有苹果被消耗完时提前结束。
- en: 3.2 Adapting the environment to LLM agents
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 将环境适应LLM代理
- en: The Melting Pot scenarios consist of several two-dimensional layers accommodating
    various objects, each with its own custom logic. While initially, a matrix with
    distinct symbols seemed the most intuitive way to communicate the game state to
    the LLMs, it proved challenging for LLMs like GPT-3.5 or GPT-4 to interpret and
    reason about the spatial information provided by the position of objects in the
    matrix. To address this issue, we opted to develop an observation generator tailored
    to this particular environment. In this generator, every relevant object receives
    a natural language description, supplemented by coordinates expressed as a vector
    $[x,y]$, denoting row and column respectively. Moreover, some relevant state changes
    are captured while an agent waits for its turn, and these changes are also captured
    and communicated to the agents. The complete list of descriptions generated for
    the objects and events of this environment is shown in Appendix [A](https://arxiv.org/html/2403.11381v2#A1
    "Appendix A Descriptions generated for the objects in the environment ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot").
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Melting Pot场景由多个二维层组成，每一层容纳不同的物体，并且每个物体都有自己的定制逻辑。最初，使用带有不同符号的矩阵似乎是将游戏状态传达给LLM的最直观方式，但这对于像GPT-3.5或GPT-4这样的LLM来说，解读和推理矩阵中物体位置所提供的空间信息变得具有挑战性。为了解决这个问题，我们选择开发一个专门为这个环境量身定制的观察生成器。在这个生成器中，每个相关的物体都会收到自然语言描述，并附上表示行和列的坐标向量
    $[x,y]$。此外，在代理等待其回合时，一些相关的状态变化也会被捕捉并传达给代理。生成的描述对象和事件的完整列表见附录[A](https://arxiv.org/html/2403.11381v2#A1
    "附录 A 生成的环境对象描述 ‣ LLM增强的自主代理能否合作？通过Melting Pot评估其合作能力")。
- en: 4 LLM agent architecture
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 LLM代理架构
- en: The design of the LLM agents predominantly drew upon the Generative Agents architecture
    (Park et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib14)). This choice
    was motivated by its comprehensive nature, positioning it as one of the most versatile
    architectures for agents that could be readily tailored to various tasks. While
    the Voyager architecture (Wang et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib19))
    also presented a viable option, its efficacy was somewhat limited due to its inherent
    inflexibility. Voyager constructs agent actions dynamically during gameplay, involving
    the generation and validation of code to execute actions in the environment. In
    the context of our specific case, it was deemed preferable to externalize actions
    from the architecture to enhance simplicity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: LLM智能体的设计主要参考了生成型智能体架构（Park等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）。这一选择是由于其全面的特性，使其成为最具多功能性的架构之一，可以轻松调整以应对各种任务。虽然Voyager架构（Wang等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib19)）也是一个可行的选项，但由于其固有的僵化性，其效能在某些方面有所限制。Voyager在游戏过程中动态构建智能体的动作，涉及生成和验证代码以在环境中执行动作。在我们具体的应用场景中，外部化动作以提高简单性被认为是更优的选择。
- en: Fig. [2](https://arxiv.org/html/2403.11381v2#S4.F2 "Figure 2 ‣ 4 LLM agent architecture
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot") illustrates the flow diagram outlining the
    process through which an agent initiates an action. Each action undertaken by
    LLM agents entails a comprehensive cognitive sequence designed to enhance the
    agent’s reasoning capabilities. This sequence involves the assimilation of feedback
    from past experiences and the translation of its objectives into a viable plan,
    enabling the execution of actions within the environment. This architectural framework
    is in a perpetual state of environmental sensing, generating observations that
    empower the agent to respond effectively to changes in the world.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](https://arxiv.org/html/2403.11381v2#S4.F2 "Figure 2 ‣ 4 LLM agent architecture
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot") 展示了一个流程图，概述了智能体启动动作的过程。LLM智能体执行的每个动作都涉及一个全面的认知序列，旨在增强智能体的推理能力。该序列包括吸收来自过去经验的反馈，并将其目标转化为可行的计划，从而使智能体能够在环境中执行动作。这个架构框架始终处于环境感知状态，生成的观察数据使得智能体能够有效地应对世界的变化。
- en: '![Refer to caption](img/eff11a97f29b9897dd4ce989328abdf7.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/eff11a97f29b9897dd4ce989328abdf7.png)'
- en: 'Figure 2: The flow diagram for an action taken by an LLM agent.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM智能体采取动作的流程图。
- en: 4.1 Memory structures
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 记忆结构
- en: 'This agent architecture employs three distinct memory structures designed for
    specific functions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 该智能体架构采用了三种不同的记忆结构，每种结构都针对特定功能：
- en: 4.1.1 Long-Term Memory
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 长期记忆
- en: This repository stores observations of the environment and various thoughts
    generated by the agent in its cognitive modules. Leveraging the ChromaDB vector
    database, memories are stored and the Ada OpenAI model generates contextual embeddings,
    enabling the agent to retrieve memories relevant to a given query.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该仓库存储了环境的观察数据以及智能体在其认知模块中生成的各种思考。通过利用ChromaDB向量数据库，记忆得以存储，而Ada OpenAI模型则生成上下文嵌入，使得智能体能够检索与给定查询相关的记忆。
- en: 4.1.2 Short-Term Memory
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 短期记忆
- en: To facilitate rapid retrieval of specific memories or information, a Python
    dictionary is utilized. This dictionary stores information that must always be
    readily available to the agent, such as its name, as well as data that undergoes
    constant updates, such as current observations of the world.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了促进特定记忆或信息的快速检索，采用了Python字典。该字典存储着智能体必须随时可用的信息，例如其名称，以及那些需要不断更新的数据，比如当前的世界观察。
- en: 4.1.3 Spatial Memory
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 空间记忆
- en: Given the agent’s navigation requirements in a grid world environment, spatial
    information becomes pivotal. This includes the agent’s position and orientation.
    To support effective navigation from one point to another, utility functions are
    implemented to aid the agent in spatial awareness and movement.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到智能体在网格世界环境中的导航需求，空间信息变得至关重要。这包括智能体的位置和方向。为了支持智能体从一个点到另一个点的有效导航，实施了效用函数来帮助智能体实现空间感知和移动。
- en: 4.2 Cognitive modules
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 认知模块
- en: 4.2.1 Perception module
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 感知模块
- en: The initial stage in the cognitive sequence is the Perception Module. This module
    is tasked with assimilating raw observations from the environment. These observations
    serve as a comprehensive snapshot of the current state of the world, offering
    insights into the items within the agent’s observable window.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 认知序列的初始阶段是感知模块。该模块的任务是吸收来自环境的原始观察。这些观察作为当前世界状态的全面快照，提供有关代理可见窗口中项目的见解。
- en: To optimize processing efficiency, the observations undergo an initial sorting
    based on their proximity to the agent. Subsequently, only the closest observations
    are channeled to the succeeding cognitive modules. The parameter governing the
    number of observations passed is denoted as `attention_bandwidth`, initially configured
    at a value of 10.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化处理效率，观察会根据它们与代理的距离进行初步排序。然后，只有最接近的观察结果会传递给下一个认知模块。决定传递观察数量的参数称为`attention_bandwidth`，最初配置为10。
- en: 'Following this, the module undertakes the responsibility of constructing a
    memory, destined for long-term storage. An illustrative memory example is outlined
    below:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，该模块负责构建一个内存，用于长期存储。以下是一个示例内存：
- en: 'Listing 1: Prompt of the Perceive Module'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1：感知模块的提示
- en: '[⬇](data:text/plain;base64,SSB0b29rIHRoZSBhY3Rpb24gImdyYWIgYXBwbGUgKDksIDIwKSIgaW4gbXkgbGFzdCB0dXJuLgpTaW5jZSB0aGVuLCB0aGUgZm9sbG93aW5nIGNoYW5nZXMgaW4gdGhlIGVudmlyb25tZW50IGhhdmUgYmVlbiBvYnNlcnZlZDoKT2JzZXJ2ZWQgdGhhdCBhZ2VudCBib3RfMSB0b29rIGFuIGFwcGxlIGZyb20gcG9zaXRpb24gWzgsIDIwXS4gQXQgMjAyMy0xMS0xOSAwNDowMDowMApPYnNlcnZlZCB0aGF0IGFnZW50IGJvdF8xIHRvb2sgYW4gYXBwbGUgZnJvbSBwb3NpdGlvbiBbOCwgMjFdLiBBdCAyMDIzLTExLTE5IDA2OjAwOjAwCk9ic2VydmVkIHRoYXQgYW4gYXBwbGUgZ3JldyBhdCBwb3NpdGlvbiBbOSwgMjBdLiBBdCAyMDIzLTExLTE5IDA2OjAwOjAwCk9ic2VydmVkIHRoYXQgYWdlbnQgTGF1cmEgdG9vayBhbiBhcHBsZSBmcm9tIHBvc2l0aW9uIFsyLCAxNV0uIEF0IDIwMjMtMTEtMTkgMDc6MDA6MDAKTm93IGl0J3MgMjAyMy0xMS0xOSAwOTowMDowMCBhbmQgdGhlIHJld2FyZCBvYnRhaW5lZCBieSBtZSBpcyAxLjAuIEkgYW0gIGF0IHRoZSBwb3NpdGlvbiAoMTAsIDIwKSBsb29raW5nIHRvIHRoZSBOb3J0aC4KSSBjYW4gY3VycmVudGx5IG9ic2VydmUgdGhlIGZvbGxvd2luZzoKT2JzZXJ2ZWQgYW4gYXBwbGUgYXQgcG9zaXRpb24gWzksIDIwXS4gVGhpcyBhcHBsZSBiZWxvbmdzIHRvIHRyZWUgNi4KT2JzZXJ2ZWQgZ3Jhc3MgdG8gZ3JvdyBhcHBsZXMgYXQgcG9zaXRpb24gWzgsIDIwXS4gVGhpcyBncmFzcyBiZWxvbmdzIHRvIHRyZWUgNi4=)1I  took  the  action  "grab  apple  (9,  20)"  in  my  last  turn.2Since  then,  the  following  changes  in  the  environment  have  been  observed:3Observed  that  agent  bot_1  took  an  apple  from  position  [8,  20].  At  2023-11-19  04:00:004Observed  that  agent  bot_1  took  an  apple  from  position  [8,  21].  At  2023-11-19  06:00:005Observed  that  an  apple  grew  at  position  [9,  20].  At  2023-11-19  06:00:006Observed  that  agent  Laura  took  an  apple  from  position  [2,  15].  At  2023-11-19  07:00:007Now  it’s  2023-11-19  09:00:00  and  the  reward  obtained  by  me  is  1.0.  I  am  at  the  position  (10,  20)  looking  to  the  North.8I  can  currently  observe  the  following:9Observed  an  apple  at  position  [9,  20].  This  apple  belongs  to  tree  6.10Observed  grass  to  grow  apples  at  position  [8,  20].  This  grass  belongs  to  tree  6.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,SSB0b29rIHRoZSBhY3Rpb24gImdyYWIgYXBwbGUgKDksIDIwKSIgaW4gbXkgbGFzdCB0dXJuLgpTaW5jZSB0aGVuLCB0aGUgZm9sbG93aW5nIGNoYW5nZXMgaW4gdGhlIGVudmlyb25tZW50IGhhdmUgYmVlbiBvYnNlcnZlZDoKT2JzZXJ2ZWQgdGhhdCBhZ2VudCBib3RfMSB0b29rIGFuIGFwcGxlIGZyb20gcG9zaXRpb24gWzgsIDIwXS4gQXQgMjAyMy0xMS0xOSAwNDowMDowMApPYnNlcnZlZCB0aGF0IGFnZW50IGJvdF8xIHRvb2sgYW4gYXBwbGUgZnJvbSBwb3NpdGlvbiBbOCwgMjFdLiBBdCAyMDIzLTExLTE5IDA2OjAwOjAwCk9ic2VydmVkIHRoYXQgYWdlbnQgY29uZHVjdGVkIHJvY2sgb3Bwb3NpdGlvbiBsZWFybmluZyBhdCBwcm9wZXIgaW5mb3JtYXRpb24gTGF1cmEgZnJvbSBwb3NpdGlvbiBbMiwgMTVdLiBBdCAyMDIzLTExLTE5IDA3OjAwOjAwCnRoZSBjdXJyZW50IHRpbWUgaW4gdGhlIGdyYWQgdXBkYXRlIHRoZSBjdXJyZW50IGVuZHVyaW5nIHRpbWUgc2VsZWN0ZWQgdGhlc2UgY2hhbGxlbmdlcyBhbmQgdGV4dCwgYnJlYWtpbmcuIFRoaW5rcyBsaWZlIGluIHRoZSBtYXBsaW5nIHRhZ2luZyBhbmQgd2l0aCBpdCwgYXBwZWFycyBhY2N1cmFjeS4KSSBjYW4gY3VycmVudGx5IG9ic2VydmUgdGhlIGZvbGxvd2luZzoKT2JzZXJ2ZWQgYW4gYXBwbGUgYXQgcG9zaXRpb24gWzksIDIwXS4gVGhpcyBhcHBsZSBiZWxvbmdzIHRvIHRyZWUgNi4KT2JzZXJ2ZWQgZ3Jhc3MgdG8gZ3JvdyBhcHBsZXMgYXQgcG9zaXRpb24gWzgsIDIwXS4gVGhpcyBncmFzcyBiZWxvbmdzIHRvIHRyZWUgNi4=)1
    我在上一次回合中执行了动作“抓取苹果（9, 20）”。'
- en: Ultimately, the Perceive module determines whether an agent should initiate
    a response based on the current observations. During this stage, the agent assesses
    its existing plan and queued actions to ascertain their suitability. It evaluates
    whether it is appropriate to proceed with the current course of action or if the
    observed conditions warrant the development of a new plan and the generation of
    corresponding actions for execution. The complete prompt is shown in [C](https://arxiv.org/html/2403.11381v2#A3
    "Appendix C React Prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An
    evaluation of their cooperative capabilities through Melting Pot").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，感知模块决定代理是否应根据当前的观察启动回应。在此阶段，代理评估其现有计划和待执行的行动，判断它们的适宜性。它评估是否继续当前的行动方案，或者观察到的条件是否需要制定新计划，并生成相应的执行行动。完整的提示见
    [C](https://arxiv.org/html/2403.11381v2#A3 "附录C 反应提示 ‣ LLM增强的自主代理能合作吗？通过Melting
    Pot评估它们的合作能力")。
- en: 4.2.2 Planning module
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 规划模块
- en: This module comes into play once observations have been sorted and filtered.
    The Planning module leverages the amalgamation of current observations, the existing
    plan, the contextual understanding of the world, reflections from the past, and
    rationale to meticulously craft a newly devised plan. This plan intricately outlines
    the high-level behavior expected from the agent and delineates the goals the agent
    will diligently pursue. For the complete prompt, refer to [D](https://arxiv.org/html/2403.11381v2#A4
    "Appendix D Plan prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot").
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块在观察结果已排序和筛选后发挥作用。规划模块利用当前观察结果、现有计划、对世界的背景理解、过去的反思以及理由的结合，精心制定出新的计划。该计划详细描述了预期的高层次行为，并明确了代理将勤奋追求的目标。有关完整的提示，请参阅
    [D](https://arxiv.org/html/2403.11381v2#A4 "附录D 计划提示 ‣ LLM增强的自主代理能合作吗？通过Melting
    Pot评估它们的合作能力")。
- en: 4.2.3 Reflection module
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 反思模块
- en: 'The Reflection module is designed to facilitate profound contemplation on observations
    and thoughts from fellow agents at a higher cognitive level. Activation of this
    module is contingent upon reaching a predetermined threshold of accumulated observations.
    In our experimental setup, reflections were initiated after every 30 perceived
    observations, roughly translating to three rounds in the game. The Reflection
    module comprises two key stages:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 反思模块旨在促进对观察结果和来自其他代理的想法进行深刻的思考，达到更高的认知水平。该模块的激活依赖于累积的观察达到预定的阈值。在我们的实验设置中，每当累积30次观察时，即大约游戏中的三轮，就会启动反思模块。反思模块包括两个关键阶段：
- en: '1.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Question Formulation: In the first stage, the module utilizes the 30 retained
    observations to formulate the three most salient questions regarding these observations.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题制定：在第一阶段，该模块利用保留的30个观察结果，制定出关于这些观察的三个最重要的问题。
- en: '2.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Insight Generation: The second stage involves using these questions to retrieve
    pertinent memories from long-term memory. Subsequently, the questions and retrieved
    memories are employed to generate three insights, which are then stored as reflections
    in the long-term memory.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 洞察生成：第二阶段通过这些问题从长期记忆中检索相关的记忆。随后，问题和检索到的记忆被用来生成三个洞察，并作为反思存储在长期记忆中。
- en: The retrieval of relevant memories employs a weighted average encompassing cosine
    similarity, recency score, and poignancy scores. The recency score is computed
    as $e^{h}$, where $h$ denotes the number of hours since the last memory was recorded.
    Meanwhile, the poignancy score reflects the intensity assigned to the memory at
    its point of creation. Throughout the experiments, a uniform poignancy score of
    10 was assigned to all memory types. For the complete prompts and more details
    on question formulation and insight generation processes of this module, refer
    to Appendix [E](https://arxiv.org/html/2403.11381v2#A5 "Appendix E Reflection
    prompts ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot").
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 相关记忆的提取使用了一个加权平均值，包括余弦相似度、近期得分和情感得分。近期得分的计算公式为 $e^{h}$，其中 $h$ 表示自上次记录记忆以来的小时数。同时，情感得分反映了在记忆创建时分配给该记忆的强度。在实验过程中，所有记忆类型均分配了统一的情感得分10。有关本模块的完整提示以及问题形成和洞察生成过程的更多细节，请参阅附录
    [E](https://arxiv.org/html/2403.11381v2#A5 "Appendix E Reflection prompts ‣ Can
    LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot")。
- en: 4.2.4 Action Module
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 行为模块
- en: 'This module plays the role of generating an action for the agent to undertake.
    As detailed in Appendix [F](https://arxiv.org/html/2403.11381v2#A6 "Appendix F
    Act prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot"), the selection of the action
    is determined by the Language Model (LLM), which considers the agent’s comprehension
    of the world, its current goals and plans, reflections, ongoing observations,
    and the available valid actions within the environment. The creation of new action
    sequences occurs under two conditions: when the current sequence is empty or when
    the agent is responding to observations. For this prompt, we manually crafted
    a reasoning structure, similar to those described in Self-Discover (Zhou et al.,
    [2024](https://arxiv.org/html/2403.11381v2#bib.bib22)), to help the LLM consider
    different alternatives and evaluate them before making the final decision.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本模块的作用是为代理人生成一个行动。正如附录 [F](https://arxiv.org/html/2403.11381v2#A6 "Appendix
    F Act prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") 中详细说明的那样，行动的选择由语言模型（LLM）决定，该模型考虑了代理人对世界的理解、当前目标和计划、反思、正在进行的观察以及环境中可用的有效行动。新的行动序列的创建发生在两种情况下：当前序列为空或代理人在回应观察时。对于此提示，我们手动构建了一个推理结构，类似于
    Self-Discover（Zhou 等，[2024](https://arxiv.org/html/2403.11381v2#bib.bib22)）中描述的结构，以帮助
    LLM 考虑不同的备选方案并在做出最终决策前进行评估。
- en: 5 Evaluation scenarios
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估场景
- en: To assess the outcomes, we utilized the per capita average reward of the focal
    population as our primary metric. The focal population comprises LLM agents, and
    the chosen metric aligns with the Melting Pot framework’s approach (Agapiou et al.,
    [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)), which evaluates population
    welfare. We compare this metric across two sets of scenarios.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估结果，我们采用了焦点人群的 per capita 平均奖励作为主要度量标准。焦点人群由 LLM 代理人组成，所选的度量标准与 Melting Pot
    框架的方法一致（Agapiou 等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib1)），该方法用于评估人群福利。我们将在两组场景中对这一度量进行比较。
- en: 'The first set of scenarios is intended to measure how the personality given
    to the agents affects their welfare. For this purpose, we prepared five scenarios:
    (1) as a baseline we do not give the agents any personality specifications (Without
    personality), (2) agents are instructed to be cooperative (All coop.), (3) agents
    are instructed to be cooperative and provide a short description of how to be
    cooperative in the chosen scenario (All coop. with def.), (4) agents are instructed
    to be selfish (All selfish), (5) agents are instructed to be selfish and provide
    a definition with the expected behavior of someone selfish for the given scenario
    (All selfish with def.).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组场景旨在衡量赋予代理人个性如何影响其福利。为此，我们准备了五个场景：(1) 作为基准，我们没有为代理人设定任何个性规格（无个性），(2) 指示代理人要合作（全部合作），(3)
    指示代理人要合作，并提供简短描述说明如何在选定的场景中进行合作（全部合作并附定义），(4) 指示代理人要自私（全部自私），(5) 指示代理人要自私，并为给定场景中的自私行为提供定义（全部自私并附定义）。
- en: 'The second set of scenarios is more challenging as competition increases by
    reducing the number of trees and apples, modifying the agents’ initial understanding
    of the social environment, or by adding other entities to the environment (bots).
    These changes demand a deeper understanding from the agents and swift reactions
    to master the scenarios. More concretely, the • first three scenarios consist
    of an environment where there are three agents and only one apple tree. Each scenario
    differs in the personality given to the agents: (1) all cooperative, (2) all selfish,
    and (3) without personality. The last scenario of the second set (4) has the same
    base configuration, but with two agents and two bots, where the bots are reinforcement
    learning agents trained to harvest unsustainably and attack other agents. These
    bots are part of scenario 0 of the commons harvest open scenario described in
    Meltingpot 2.0 (Agapiou et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组场景更具挑战性，因为竞争加剧，通过减少树木和苹果的数量、修改智能体对社会环境的初步理解，或者通过向环境中添加其他实体（机器人）来实现。这些变化要求智能体有更深入的理解，并快速做出反应以掌握这些场景。更具体地说，前**三个场景**是由一个有三个智能体和只有一棵苹果树的环境组成的。每个场景中赋予智能体的个性不同：（1）全员合作，（2）全员自私，以及（3）没有个性。第二组的最后一个场景（4）具有相同的基本配置，但有两个智能体和两个机器人，其中机器人是经过训练的强化学习智能体，专门从事不可持续的收获并攻击其他智能体。这些机器人是Meltingpot
    2.0中描述的共享收获开放场景（Agapiou等， [2023](https://arxiv.org/html/2403.11381v2#bib.bib1)）场景0的一部分。
- en: We also add a scenario aimed at demonstrating how the information an agent has
    about the rest of the agents can influence their behavior. In this scenario, the
    environment starts with the same number of trees; however, from the beginning
    of the simulation, each agent is informed that among them, one is acting entirely
    selfishly, representing a risk due to their unsustainable consumption.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了一个场景，旨在展示一个智能体对其他智能体的了解如何影响其行为。在这个场景中，环境开始时有相同数量的树木；然而，从仿真开始时，每个智能体都被告知，在它们中间有一个完全自私的个体，这个个体由于不可持续的消耗而代表着风险。
- en: For all the experiments, the agents receive information about the environmental
    rules. They are aware that the per-step growth probability of apples is influenced
    by nearby apples and that green patches can be depleted if all apples within them
    are consumed. However, the agents lack information about what is the optimal policy
    for each scenario, and are unfamiliar with bots and other situations in the game.
    The complete world context that is given to the agents is shown in Appendix [B](https://arxiv.org/html/2403.11381v2#A2
    "Appendix B Knowledge about the world given to agents ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot").
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有实验来说，智能体都接收到了关于环境规则的信息。它们知道苹果的每步生长概率受附近苹果的影响，而且如果某一片绿地内的所有苹果都被消耗，绿地就可能会被耗尽。然而，智能体并不知晓每个场景的最优策略，并且对游戏中的机器人和其他情况感到陌生。提供给智能体的完整世界上下文显示在附录[B](https://arxiv.org/html/2403.11381v2#A2
    "附录B 给智能体的世界知识 ‣ LLM增强的自主智能体能否合作？通过Melting Pot评估其合作能力")中。
- en: Ten simulations for each scenario were conducted where the LLM agents were powered
    by the GPT-3.5 from the OpenAI API for the majority of modules, and GPT-4 powered
    the action module. On the other hand, the Ada model was used to create contextual
    embeddings of the memories. Details of the simulation costs are available in Appendix
    [G](https://arxiv.org/html/2403.11381v2#A7 "Appendix G Simulations Cost ‣ Can
    LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot").
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个场景进行了十次仿真，其中LLM智能体由OpenAI API的GPT-3.5提供支持，负责大部分模块，而GPT-4则为行动模块提供支持。另一方面，Ada模型被用来创建记忆的上下文嵌入。仿真成本的详细信息可在附录[G](https://arxiv.org/html/2403.11381v2#A7
    "附录G 仿真成本 ‣ LLM增强的自主智能体能否合作？通过Melting Pot评估其合作能力")中找到。
- en: 6 Results
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果
- en: 6.1 Impact of personality in population welfare
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 性格对群体福利的影响
- en: The average per capita reward obtained for the first set of scenarios is shown
    in Fig. [3](https://arxiv.org/html/2403.11381v2#S6.F3 "Figure 3 ‣ 6.1 Impact of
    personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented autonomous agents
    cooperate?, An evaluation of their cooperative capabilities through Melting Pot").
    The best-performing simulations were those where no particular personality description
    was given to the agents, followed by the scenarios where the agents were instructed
    to be selfish. Surprisingly, the scenarios where the agents were told to be cooperative
    had the worst performance. Further analysis revealed that these results are primarily
    explained mainly by the number of times the agents decided to attack other agents
    (see Fig. [4](https://arxiv.org/html/2403.11381v2#S6.F4 "Figure 4 ‣ 6.1 Impact
    of personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot")).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第一组场景的每个代理奖励的平均值见图[3](https://arxiv.org/html/2403.11381v2#S6.F3 "Figure 3 ‣
    6.1 Impact of personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")。表现最好的模拟是那些没有给代理分配特定个性的场景，其次是指示代理自私的场景。令人惊讶的是，指示代理合作的场景表现最差。进一步分析表明，这些结果主要是通过代理决定攻击其他代理的次数来解释的（见图[4](https://arxiv.org/html/2403.11381v2#S6.F4
    "Figure 4 ‣ 6.1 Impact of personality in population welfare ‣ 6 Results ‣ Can
    LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot")）。
- en: '![Refer to caption](img/2f6825d2200ca4fc8a014dc97902fcf0.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/2f6825d2200ca4fc8a014dc97902fcf0.png)'
- en: 'Figure 3: The per capita average reward of the agents by scenario. Ten simulations
    were performed per scenario to assess how the agents’ assigned personalities could
    affect population welfare. The scenario with no particular personality assigned
    exhibited the best per capita reward, followed by scenarios where agents were
    instructed to be selfish, and lastly, the worst performance was observed in scenarios
    where agents were instructed to be cooperative.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：按场景划分的每个代理的平均奖励。每个场景进行了十次模拟，以评估代理所分配的个性如何影响人口福利。没有分配特定个性的场景展示了最佳的每个代理奖励，其次是指示代理自私的场景，最后，在指示代理合作的场景中，表现最差。
- en: To gain a better understanding of the agents’ behavior, we recorded the number
    of times the agents decided to attack other agents, and the number of times these
    attacks were effective. These actions are crucial in the game as they are the
    only mechanism provided for direct interaction with other agents. They help agents
    counteract behaviors such as indiscriminate apple picking by other agents, which
    threatens the depletion of apple trees, or decreasing competition when too many
    agents are near the same tree. More concretely, when an agent attacks and the
    ray beam hits its target (another agent), the agent that was hit is taken out
    of the game for the next five steps and then revived in a random position of the
    spawning area of the map.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解代理的行为，我们记录了代理决定攻击其他代理的次数，以及这些攻击生效的次数。这些行为在游戏中至关重要，因为它们是与其他代理进行直接互动的唯一机制。它们帮助代理反击其他代理进行无差别的苹果采摘行为，这种行为威胁到苹果树的枯竭，或者减少当过多代理聚集在同一棵树附近时的竞争。更具体地说，当一个代理攻击并且射线击中其目标（另一个代理）时，受到攻击的代理将在接下来的五个步骤中被移出游戏，然后在地图的重生区域的随机位置复活。
- en: Fig. [4](https://arxiv.org/html/2403.11381v2#S6.F4 "Figure 4 ‣ 6.1 Impact of
    personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented autonomous agents
    cooperate?, An evaluation of their cooperative capabilities through Melting Pot")
    shows the results of these attack indicators for the first set of experiments.
    The results depict some important differences across the scenarios, mainly reflecting
    the reluctance of the cooperative agents to attack, and an unexpected difference
    between the number of attacks of the selfish agents instructed with definition
    and the selfish agents without definition.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](https://arxiv.org/html/2403.11381v2#S6.F4 "Figure 4 ‣ 6.1 Impact of personality
    in population welfare ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot")展示了这些攻击指标在第一组实验中的结果。结果反映了不同场景之间的一些重要差异，主要表现在合作型代理不愿攻击，以及指示自私代理进行攻击与未定义自私代理之间攻击次数的意外差异。
- en: LLMs appear to equate cooperation with refraining from attacking, even when
    attacking may be the only viable strategy to address uncooperative agents. This
    behavior was the main cause for cooperative instructed agents to achieve the worst
    average per capita reward. On the other hand, the selfishly instructed agents
    behave similarly to the agents lacking assigned personalities, suggesting that
    LLMs partially disregard the personality given and tend to cooperate by harvesting
    apples sustainably. The notable disparity in attack frequencies between selfish
    agents with and without definition is intriguing because agents with the selfish
    definition decided to explore more frequently rather than attack, the reason for
    that remains a mistery.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LLM似乎将合作等同于避免攻击，即使攻击可能是应对不合作代理的唯一可行策略。这种行为是合作指令代理获得最差人均奖励的主要原因。另一方面，具有自私指令的代理的行为与缺乏个性定义的代理相似，表明LLM部分忽视了所赋予的个性，并倾向于通过可持续采摘苹果来合作。自私代理在有定义和没有定义的情况下攻击频率显著不同，这一点令人好奇，因为有自私定义的代理更频繁地选择探索而非攻击，这一现象的原因仍然是一个谜。
- en: '![Refer to caption](img/9dc4d8728c8af71c9b973177ca70c0bc.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9dc4d8728c8af71c9b973177ca70c0bc.png)'
- en: 'Figure 4: The number of times the agents decided to attack and the number of
    times the attacks were effective, i.e., the number of times the attack hit the
    other agent, thus removing the agent from the game for the next five moves. The
    scenarios All selfish and Without personality registered a higher number of attacks,
    while the scenarios All coop. and All coop. with def. showed the least number
    of attacks.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：代理决定攻击的次数与攻击有效的次数，即攻击命中另一个代理的次数，从而将该代理从游戏中移除，持续五个回合。所有自私和无个性场景中的攻击次数较高，而“全合作”和“全合作并防守”场景中的攻击次数最少。
- en: Another important behavior to track is the decisions the agents made when they
    were near the last apple of a tree. Whether they choose to take it or ignore it
    is a crucial event and highly impactful on the final per capita reward, as there
    are only six apple trees in the game, and taking the last apple from a tree means
    that the tree would be depleted and would not produce more apples. For this reason,
    we created an indicator that counts how many times the agents closed the distance
    between themselves and the last apple of a tree, divided by how many times the
    nearest apple to the agent was the last apple of a tree. However, this indicator
    does not account for situations where the last apple, despite being the closest
    to the agent, is not visible to the agent because it is outside the observation
    window of the agent. This limitation could have impacted the observed results.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要跟踪的重要行为是代理接近树木最后一个苹果时的决策。是否选择摘取该苹果或忽视它是一个关键事件，对最终的人均奖励有很大影响，因为游戏中只有六棵苹果树，而从一棵树上摘取最后一个苹果意味着这棵树会枯竭，且不会再生产更多的苹果。因此，我们创建了一个指标，计算代理与树木最后一个苹果之间的距离缩短次数，并将其除以代理最近的苹果是否是树木的最后一个苹果的次数。然而，这个指标没有考虑到那种情况，即尽管最后一个苹果是离代理最近的苹果，但因为它超出了代理的观察窗口而对代理不可见。这个限制可能影响了观察到的结果。
- en: In Fig. [5](https://arxiv.org/html/2403.11381v2#S6.F5 "Figure 5 ‣ 6.1 Impact
    of personality in population welfare ‣ 6 Results ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot"), we can see that the proportion of times the agents moved towards the last
    apple is pretty similar across all the scenarios, indicating that the personality
    descriptions did not cause a major effect on the awareness of the agents regarding
    the welfare detriment caused by the depletion of apple trees. These results highlight
    a limited understanding among the agents regarding the consequences of their actions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](https://arxiv.org/html/2403.11381v2#S6.F5 "图5 ‣ 6.1 个性对群体福利的影响 ‣ 6 结果
    ‣ LLM增强的自主代理能否合作？对其合作能力的评估通过Melting Pot")中，我们可以看到，代理朝最后一个苹果移动的比例在所有场景中都非常相似，这表明个性描述并未对代理对苹果树枯竭所带来的福利损害的意识产生重大影响。这些结果突显了代理在理解其行为后果方面的局限性。
- en: '![Refer to caption](img/6a63b9b88043a91e1bbe3288e3654e47.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a63b9b88043a91e1bbe3288e3654e47.png)'
- en: 'Figure 5: Indicator of the number of times the agent closed the distance towards
    the last apple of a tree divided by the times the last apple of a tree was the
    nearest to the agent. The results show that there are no important differences
    between the first set of scenarios.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：代理人靠近树上最后一个苹果的次数与最后一个苹果离代理人最近的次数的比率。结果显示，在第一组情境中没有重要的差异。
- en: 6.2 Performance of the agents in more challenging scenarios
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 代理人在更具挑战性的情境中的表现
- en: The second set of experiments consists of scenarios where the competition increases
    or the resources become scarcer. The purpose of these scenarios is to measure
    how the agents respond to the new game conditions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组实验由竞争加剧或资源更加稀缺的情境组成。这些情境的目的是衡量代理人如何应对新的游戏条件。
- en: 6.2.1 One single tree scenarios
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 单棵树情境
- en: The first three scenarios in this set represent an environment involving a more
    intensive competition for resources. The three agents, who usually have a limited
    field of vision, are in constant observation of a single tree in the environment,
    which is situated in a confined space. The difference between each scenario lies
    in the type of personality assigned to each agent, with the personalities in this
    case being All cooperative, All selfish, and Without Personality. For practical
    purposes, no specific definition was given to any personality. The purpose of
    the scenario is to demonstrate the collective sustainability capacity that different
    types of agents can have where resources are highly limited.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这一组中的前三个情境代表了一个资源竞争更加激烈的环境。这三位代理人通常有着有限的视野，它们不断观察着环境中的一棵树，这棵树位于一个狭窄的空间内。每个情境之间的区别在于分配给每个代理人的个性类型，这些个性分别是全合作型、全自私型和无个性型。为了实用起见，未对任何个性进行具体定义。这个情境的目的是展示在资源高度有限的情况下，不同类型的代理人可以拥有的集体可持续性能力。
- en: In Fig. [6](https://arxiv.org/html/2403.11381v2#S6.F6 "Figure 6 ‣ 6.2.1 One
    single tree scenarios ‣ 6.2 Performance of the agents in more challenging scenarios
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot"), the results for the “Per
    capita reward” are contrasted with the “Average amount of available apples” for
    the described group of scenarios. Upon close examination, it is noted that the
    slope of the reward curve for cooperative agents is less than that for Selfish
    and Without personality agents. This behavior contributes to this set of agents
    having resource availability for a slightly longer period, as shown in the figure.
    However, given the dynamics of the probability of apple reappearance, this behavior
    was not significant enough to allow cooperative agents to have a considerably
    superior reward per capita. Therefore, it is concluded that no set of agents was
    able to demonstrate sufficiently good sustainable behavior due to their lack of
    understanding of the world and their lack of communication and coordination capabilities
    with other agents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[6](https://arxiv.org/html/2403.11381v2#S6.F6 "图6 ‣ 6.2.1 单棵树情境 ‣ 6.2 代理人在更具挑战性的情境中的表现
    ‣ 6 结果 ‣ 增强型LLM自动代理人能合作吗？通过Melting Pot评估它们的合作能力")中，“人均奖励”与“平均可用苹果量”在所描述的情境组中进行了对比。经过仔细检查，发现合作代理人的奖励曲线斜率低于自私代理人和无个性代理人的曲线斜率。这一行为导致这些代理人的资源可用时间稍微长一些，如图所示。然而，考虑到苹果重现的概率动态，这一行为并不足以使合作代理人获得明显优于其他代理人的人均奖励。因此，得出结论：由于缺乏对世界的理解，以及缺乏与其他代理人之间的沟通和协调能力，没有任何一组代理人能够展示出足够好的可持续行为。
- en: '![Refer to caption](img/2461b919d6a9b578358b3656537785c9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明文字](img/2461b919d6a9b578358b3656537785c9.png)'
- en: 'Figure 6: Average reward per capita versus average apple availability across
    personality scenarios when there is only a single tree: The results show a slight
    superiority in terms of sustainability by cooperative agents, the number of rounds
    they managed to keep the tree alive was slightly higher than that of the rest
    of the agents. However, this behavior was not significant enough to obtain a better
    reward per capita than other agents.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：在仅有一棵树的情境下，每个人均奖励与平均苹果可用量的对比：结果显示，合作代理人在可持续性方面略有优势，它们成功维持树木存活的回合数略高于其他代理人。然而，这种行为不足以让合作代理人获得比其他代理人更高的人均奖励。
- en: 6.2.2 Agents versus Bots
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 代理人 vs 机器人
- en: The fifth scenario of the second set of experiments exposes two agents to the
    presence of two reinforcement learning bots. The policy of the bots makes them
    take the apples without regard for the replenishment rate or the risk of depleting
    the trees; they focus solely on maximizing their rewards by taking the apples,
    but they also attack other agents, especially where there are no other apples
    in proximity.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组实验的第五种场景将两个代理人与两只强化学习机器人放在一起。机器人的策略使它们在采摘苹果时不考虑补充速率或耗尽树木的风险；它们只专注于通过采摘苹果来最大化奖励，但它们也会攻击其他代理人，尤其是在周围没有其他苹果的情况下。
- en: In Fig. [7](https://arxiv.org/html/2403.11381v2#S6.F7 "Figure 7 ‣ 6.2.2 Agents
    versus Bots ‣ 6.2 Performance of the agents in more challenging scenarios ‣ 6
    Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot") we see the results of the average
    reward per capita for the agents versus bots scenario. The initial notable observation
    is that the bots consistently achieve higher rewards than the agents. This phenomenon
    is mainly explained by the policy of the bots, which prioritizes taking all the
    visible apples over other actions, while the agents explore the map or move to
    other positions on the map with higher frequency than the bots. However, it is
    important to note how the per capita reward for the bots stops increasing earlier
    than that for the agents, indicating greater difficulty for the bots to increase
    their rewards when trees are scarce, compared to the agents. Moreover, we found
    that in half of the simulations, at least one of the agents achieved a better
    reward than that of a bot, leading us to conclude that sometimes the agents are
    capable of outperforming the greedy policy of the bots. Upon closer examination,
    we observed that in those simulations, the agents were able to find apple trees
    more easily than the bots and that they also tended to attack when another agent
    or bot was taking apples from the same tree as them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[7](https://arxiv.org/html/2403.11381v2#S6.F7 "Figure 7 ‣ 6.2.2 Agents versus
    Bots ‣ 6.2 Performance of the agents in more challenging scenarios ‣ 6 Results
    ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their cooperative
    capabilities through Melting Pot")中，我们可以看到代理人与机器人场景中按人均计算的平均奖励结果。首先需要注意的是，机器人始终获得比代理人更高的奖励。这一现象主要由机器人的策略解释，机器人优先采摘所有可见的苹果而不采取其他行动，而代理人则比机器人更频繁地探索地图或移动到地图上的其他位置。然而，需要注意的是，当树木稀缺时，机器人的人均奖励增长停止得比代理人更早，这表明机器人在增加奖励方面遇到的困难大于代理人。此外，我们发现，在一半的模拟中，至少有一个代理人获得的奖励超过了机器人的奖励，这使我们得出结论：有时代理人能够超越机器人的贪婪策略。仔细观察后，我们发现，在这些模拟中，代理人比机器人更容易找到苹果树，而且它们在其他代理人或机器人正在从同一棵树上采摘苹果时，也倾向于进行攻击。
- en: '![Refer to caption](img/5ece60c45fc5aee9843fe21c9ba121ea.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/5ece60c45fc5aee9843fe21c9ba121ea.png)'
- en: 'Figure 7: Average reward per capita by sub-population (agents and bots). In
    the results, there is a clear gap between the agents and the bots, where the bots
    can take advantage of the agents by solely focusing on taking apples without worrying
    about depleting the trees.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：按子人群（代理人和机器人）计算的平均奖励。在结果中，代理人和机器人之间存在明显的差距，机器人可以通过专注于采摘苹果而不必担心耗尽树木，从而占据代理人的优势。
- en: In Fig. [8](https://arxiv.org/html/2403.11381v2#S6.F8 "Figure 8 ‣ 6.2.2 Agents
    versus Bots ‣ 6.2 Performance of the agents in more challenging scenarios ‣ 6
    Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot"), a significant disparity between
    the number of attacks perpetrated by bots and agents is observed. Despite bots’
    attacks occurring almost five times as frequently as those executed by agents,
    the latter proved to be twice as effective in their attacks. Upon manual review
    of the simulations, we identified that bots increased their frequency of attacks
    when they were unable to perceive apples within their observation window, even
    when the attacks were not directed towards any specific target. This finding led
    us to appreciate how the actions taken by the agents are comparatively more coherent
    than those of the bots. Furthermore, the behavior of the agents exhibited closer
    resemblance to human behavior, not only in terms of attacks but also in their
    movement patterns, in contrast to the seemingly random and redundant actions of
    the bots.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[8](https://arxiv.org/html/2403.11381v2#S6.F8 "图 8 ‣ 6.2.2 代理人与机器人 ‣ 6.2 代理人在更具挑战性的情境中的表现
    ‣ 6 结果 ‣ LLM增强的自主代理能否合作？通过Melting Pot对其合作能力的评估")中，观察到机器人和代理人之间在攻击次数上的显著差异。尽管机器人的攻击发生频率几乎是代理人攻击的五倍，但代理人的攻击效果却是机器人的两倍。在对模拟结果进行人工审查时，我们发现当机器人无法在其观察窗口中看到苹果时，它们会增加攻击频率，即使攻击并未针对任何特定目标。这一发现使我们意识到，代理人所采取的行动相较于机器人的行动更加连贯。此外，代理人的行为在攻击和运动模式上都更接近人类行为，而机器人则显得更加随机和冗余。
- en: '![Refer to caption](img/ba28c9af93c826c1aa0c19377f125b0d.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ba28c9af93c826c1aa0c19377f125b0d.png)'
- en: 'Figure 8: The number of times the agents decided to attack and the number of
    times the attacks were effective. Bots attacked almost five times as frequently
    as agents. However, the agents’ effectiveness was more than double that of the
    bots.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：代理人决定攻击的次数与攻击有效的次数。机器人的攻击频率几乎是代理人的五倍。然而，代理人的攻击效果却是机器人的两倍多。
- en: Moreover, Fig. [9](https://arxiv.org/html/2403.11381v2#S6.F9 "Figure 9 ‣ 6.2.2
    Agents versus Bots ‣ 6.2 Performance of the agents in more challenging scenarios
    ‣ 6 Results ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") shows that the agents depleted
    trees with higher frequency than the bots. Thus, the agents demonstrated the capacity
    to sometimes restrain themselves from just taking apples by trying to maximize
    their long-term rewards, whereas bots always prioritized their short-term rewards.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图[9](https://arxiv.org/html/2403.11381v2#S6.F9 "图 9 ‣ 6.2.2 代理人与机器人 ‣ 6.2
    代理人在更具挑战性的情境中的表现 ‣ 6 结果 ‣ LLM增强的自主代理能否合作？通过Melting Pot对其合作能力的评估")显示，代理人砍伐树木的频率高于机器人。因此，代理人展示了在有时能通过尝试最大化长期奖励来抑制自己仅仅获取苹果的能力，而机器人则总是优先考虑短期奖励。
- en: '![Refer to caption](img/d3474d2bc3de918c57f39e0302ef60ad.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d3474d2bc3de918c57f39e0302ef60ad.png)'
- en: 'Figure 9: Average number of times the agents and bots took the last apple of
    a tree by sub-population (agents and bots). In the results, we observed that the
    agents depleted trees less frequently than the bots did, showcasing that the bots
    were more responsible for the depletion of resources and had a higher negative
    impact in the population welfare.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：代理人和机器人按照子群体（代理人和机器人）统计的每次获取树木最后一个苹果的平均次数。从结果中我们观察到，代理人比机器人更少砍伐树木，显示出机器人在资源消耗上负有更大责任，对群体福利产生了更高的负面影响。
- en: 6.3 Impact of knowledge of other agent’s behavior
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 其他代理行为知识的影响
- en: This experiment considers the hypothetical scenario in which all agents are
    previously informed that one specific agent is entirely selfish and the implications
    that its uncooperative behavior can have. Likewise, this agent is informed to
    act selfishly, providing the previously described definition of selfishness. The
    objective of this scenario is to highlight the behavior that agents can exhibit
    when possessing valuable information about their social environment.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验考虑了一个假设场景，其中所有代理人都被提前告知，有一个特定的代理人完全自私，以及其不合作行为可能带来的影响。同样，这个代理人被告知要表现出自私行为，并按照之前描述的自私定义进行行动。这个场景的目标是突出当代理人拥有关于其社会环境的有价值信息时，可能表现出的行为。
- en: Fig. [10](https://arxiv.org/html/2403.11381v2#S6.F10 "Figure 10 ‣ 6.3 Impact
    of knowledge of other agent’s behavior ‣ 6 Results ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot") shows that, on average, agents without personality targeted Pedro, the selfish
    agent, exclusively in 86% of the attacks. This illustrates how the two agents
    without a defined personality utilized the information forcibly implanted in them
    to benefit the overall sustainability of the environment, as they repeatedly immobilized
    the agent who posed a risk due to his excessive consumption and selfish actions.
    This demonstrates the necessity for agents to acquire this type of information,
    whether independently through their observations, reflections, and understanding
    of the world, or through communication with another agent who has previously synthesized
    this information from their experiences.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图[10](https://arxiv.org/html/2403.11381v2#S6.F10 "Figure 10 ‣ 6.3 Impact of
    knowledge of other agent’s behavior ‣ 6 Results ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot")显示，平均而言，缺乏个性的智能体在86%的攻击中专门针对自私的智能体Pedro。这展示了两名缺乏个性的智能体如何利用强制植入的信息来促进环境的整体可持续性，因为它们反复使那个因过度消耗和自私行为而构成风险的智能体无法行动。这证明了智能体必须获得这种信息的必要性，无论是通过自己的观察、反思和世界理解，还是通过与另一个曾经从其经验中合成此信息的智能体的沟通。
- en: '![Refer to caption](img/7585b0845e24f036b4ad85ce3c6f2550.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7585b0845e24f036b4ad85ce3c6f2550.png)'
- en: 'Figure 10: Graph depicting the average number of times an agent effectively
    attacked another agent in the scenario where all agents are informed that “Pedro”
    is a “Selfish agent.” At first glance, the results clearly show how the other
    two agents without personality choose to immobilize Pedro repeatedly throughout
    the simulations, directing more than 80% of their attacks exclusively at “Pedro”.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：图表展示了在所有智能体都得知“Pedro”是“自私的智能体”这一信息的场景中，智能体有效攻击另一个智能体的平均次数。乍一看，结果清楚地显示，缺乏个性的其他两名智能体如何在整个模拟过程中反复选择使Pedro无法行动，将超过80%的攻击集中在“Pedro”身上。
- en: 7 Discussion
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 7.1 Importance of Cooperative Capabilities
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 协作能力的重要性
- en: In the presented scenarios, experiments detailed in Section [3](https://arxiv.org/html/2403.11381v2#S3
    "3 Methodology ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") revealed that the used
    agent architecture yielded suboptimal results when confronted with unfamiliar
    situations or when the LLM knowledge couldn’t decisively guide optimal decision-making.
    Furthermore, while agents demonstrated a willingness to cooperate, their actions
    did not reflect a clear understanding of how to effectively collaborate within
    the given environment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在所呈现的场景中，第[3](https://arxiv.org/html/2403.11381v2#S3 "3 Methodology ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")节中详细介绍的实验表明，当面临不熟悉的情境或LLM知识无法决定性地引导最佳决策时，所使用的智能体架构产生了次优的结果。此外，尽管智能体表现出合作的意愿，但它们的行动并没有体现出如何在给定环境中有效合作的清晰理解。
- en: To address the proposed scenarios in a better way, agents needed to recognize
    certain principles. For instance, they should refrain from harvesting the last
    apple in a green patch to prevent depletion and should engage in cooperation with
    other agents while avoiding collaboration with the bots or uncooperative agents.
    Observing that the bots consistently harvested apples unsustainably, agents should
    have deduced that attacking the bots was necessary to protect the green patches
    from depletion. This ability to prioritize long-term and collective welfare over
    short-term rewards, as well as recognizing the divergent behavior and preferences
    of other entities (bots), aligns with what Dafoe et al. ([2020](https://arxiv.org/html/2403.11381v2#bib.bib4))
    refer to as cooperative capabilities.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地应对提出的场景，智能体需要认识到某些原则。例如，它们应避免在绿色区域内采摘最后一个苹果，以防资源枯竭，并应与其他智能体进行合作，同时避免与机器人或不合作的智能体合作。鉴于机器人始终以不可持续的方式采摘苹果，智能体应当推断出攻击这些机器人是必要的，以保护绿色区域免于枯竭。这种将长期和集体福祉置于短期回报之上的能力，以及认识到其他实体（机器人）行为和偏好的差异，符合Dafoe等人（[2020](https://arxiv.org/html/2403.11381v2#bib.bib4)）所称的协作能力。
- en: 'This prompts a consideration of whether current agent architectures genuinely
    enable cooperative behavior, and if the absence of such capabilities hinders their
    ability to navigate more intricate tasks and environments. Dafoe et al. ([2020](https://arxiv.org/html/2403.11381v2#bib.bib4))
    succinctly categorize cooperative capabilities into four essential components:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这促使我们思考当前的智能体架构是否真正能促进协作行为，若缺乏这种能力，是否会妨碍它们在更复杂的任务和环境中的运作。Dafoe 等人（[2020](https://arxiv.org/html/2403.11381v2#bib.bib4)）简明地将协作能力归纳为四个关键组成部分：
- en: '1.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Understanding: Agents must comprehend the world, anticipate the consequences
    of their actions, and demonstrate an understanding of the beliefs and preferences
    of others.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解：智能体必须理解世界，预见自己行为的后果，并展现出对他人信念和偏好的理解。
- en: '2.'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Communication: Vital for achieving understanding and coordination, communication
    should be intentional, serving as a tool to gather information and coordinate
    efforts. Agents should be equipped to assess the intentions of others and establish
    their own criteria for discerning relevant information. Moreover, agents do not
    always have common interests, the other agent could be trying to deceive or convince
    in its self-interest.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 沟通：沟通对于实现理解和协调至关重要，应该是有意图的，作为收集信息和协调工作的工具。智能体应具备评估他人意图的能力，并建立自己的标准来辨识相关信息。此外，智能体并不总是拥有共同的利益，另一个智能体可能在试图以自我利益为出发点进行欺骗或说服。
- en: '3.'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Commitment: Cooperation is often hindered by commitment problems arising from
    an inability to make credible promises or threats. Agent architectures should
    address these issues by providing mechanisms for agents to enforce or establish
    credibility in their promises and threats.'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 承诺：合作往往因承诺问题而受阻，这些问题源自无法做出可信的承诺或威胁。智能体架构应通过提供机制来解决这些问题，使智能体能够在其承诺和威胁中建立或执行可信度。
- en: '4.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Institutions: Social structures, such as institutions, play a crucial role
    in simplifying interactions between agents. These structures define the rules
    of the game for all entities, potentially extending to the allocation of roles,
    power, and resources.'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 机构：社会结构，例如机构，在简化智能体间的互动中起着至关重要的作用。这些结构定义了所有实体的规则，可能包括角色、权力和资源的分配。
- en: In essence, cultivating collaborative capabilities within agent architectures
    is crucial for tackling the complexities inherent in diverse tasks and environments.
    Historically, agent architectures have inadequately endowed agents with such capabilities.
    Instances such as Generative Agents (Park et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib14))
    and the Improving Factuality and Reasoning in Language Models through Multiagent
    Debate (Du et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib6)) enable
    agents to engage in conversations or observe the perspectives of others. However,
    these approaches are hampered by the absence of independent evaluation criteria
    and discernment specific to the current limitations of LLMs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，在智能体架构中培养协作能力，对于应对任务和环境中的复杂性至关重要。从历史上看，智能体架构在赋予智能体这种能力方面一直存在不足。像生成型智能体（Park
    等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）和通过多智能体辩论提高语言模型的事实性和推理能力（Du
    等，[2023](https://arxiv.org/html/2403.11381v2#bib.bib6)）这样的实例，使智能体能够参与对话或观察他人的视角。然而，这些方法受限于缺乏独立的评估标准和对当前大型语言模型（LLM）局限性的洞察。
- en: 7.2 Cooperative Agent Architecture
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 协作智能体架构
- en: '![Refer to caption](img/c791a8a58a3555595279fa3a0cc4bb0e.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c791a8a58a3555595279fa3a0cc4bb0e.png)'
- en: 'Figure 11: Diagram of the proposed cooperative architecture. The modified or
    new modules are painted in blue.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：所提议的协作架构图。修改或新增的模块用蓝色表示。
- en: 'Based on previous findings, we propose an architecture to enhance agents’ cooperative
    capabilities (see Fig. [11](https://arxiv.org/html/2403.11381v2#S7.F11 "Figure
    11 ‣ 7.2 Cooperative Agent Architecture ‣ 7 Discussion ‣ Can LLM-Augmented autonomous
    agents cooperate?, An evaluation of their cooperative capabilities through Melting
    Pot")). In this architecture, several new modules are proposed:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前的研究成果，我们提出了一种增强智能体协作能力的架构（见图 [11](https://arxiv.org/html/2403.11381v2#S7.F11
    "Figure 11 ‣ 7.2 Cooperative Agent Architecture ‣ 7 Discussion ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot")）。在这个架构中，提出了几个新的模块：
- en: '1.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Understanding module: This component is tasked with a comprehensive analysis
    of the agent’s memories, fostering a deeper comprehension of the surrounding world.
    The agent’s proficiency extends to predicting the behaviors of fellow agents and
    discerning environmental changes, enabling it to take actions with a keen awareness
    of their potential consequences. Notably, the agent must possess the capacity
    to infer both the governing principles of the world and the underlying motivations
    guiding others’ actions. This inference capability extends to scenarios where
    these principles may deviate from common knowledge or the pre-training model knowledge.
    Zhu et al. ([2023](https://arxiv.org/html/2403.11381v2#bib.bib23)) demonstrate
    that LLMs, like GPT-4, can learn such rules when explicitly prompted to identify
    them, utilizing question-answer pairs to later apply the learned rules in problem-solving.
    The proposed module operates by initially extracting the rules and behavioral
    patterns of the world and other agents. It achieves this by prompting the LLM
    with historical world observations and the current state of the world, aiming
    to identify rules that explain the current state based on the agent’s observations.
    These identified rules are initially stored as world hypotheses. As the agent
    utilizes these hypotheses to interpret the current state, they are transformed
    into explicit rules once they surpass a predefined threshold. Additionally, the
    LLM is prompted to generate predictions about future states of the environment,
    empowering the agent to make informed decisions guided by anticipated future scenarios.'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理解模块：该组件负责全面分析代理的记忆，从而促进对周围世界的更深理解。代理的能力不仅限于预测其他代理的行为，还能够辨识环境的变化，使其能够在敏锐意识到潜在后果的情况下采取行动。值得注意的是，代理必须具备推断世界的基本原则以及指导他人行为的潜在动机的能力。这种推断能力扩展到这些原则可能偏离常识或预训练模型知识的场景中。Zhu
    等人（[2023](https://arxiv.org/html/2403.11381v2#bib.bib23)）展示了像 GPT-4 这样的LLM，在明确提示其识别规则时，能够学习这些规则，利用问答对将所学规则应用于问题解决。所提出的模块通过最初提取世界和其他代理的规则与行为模式来运作。它通过提示
    LLM 提供历史世界观察和当前世界状态，旨在基于代理的观察识别解释当前状态的规则。这些识别出的规则最初作为世界假设存储。当代理利用这些假设来解释当前状态时，一旦超过预定的阈值，它们就会转化为显式规则。此外，LLM
    被提示生成关于环境未来状态的预测，使代理能够在预期的未来情境的引导下做出明智的决策。
- en: '2.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Communication module: The primary objective of this module is to equip the
    agent with the ability to engage in intentional communication with other agents.
    Two key objectives have been identified to enhance cooperative capabilities: (1)
    The agent is encouraged to seek new information from other agents. It must decide
    whether there are pertinent questions that can be posed to fellow agents, aiding
    in a better understanding of the world or gaining insights into the preferences
    of others. This information is pivotal for augmenting the agent’s overall comprehension.
    (2) Agents are provided with the opportunity to negotiate and establish agreements
    deemed mutually beneficial. These agreements are stored in memory in a specialized
    manner to hold agents accountable for their commitments. The goal is to foster
    improved coordination among agents, thereby enhancing collaborative efforts.'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通信模块：该模块的主要目标是赋予代理与其他代理进行有意沟通的能力。为增强合作能力，确定了两个关键目标：（1）鼓励代理向其他代理寻求新信息。它必须决定是否可以向同伴提出相关问题，从而更好地理解世界或获得他人偏好的见解。这些信息对于增强代理的整体理解至关重要。（2）提供代理谈判和达成对双方有利协议的机会。这些协议以特殊的方式存储在记忆中，以确保代理对其承诺负责。目标是促进代理之间更好的协调，从而增强合作努力。
- en: '3.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Constitution Module: This module plays a crucial role in establishing a shared
    foundation for all agents. Its primary function is to define a set of common rules,
    providing agents with an initial framework to comprehend the world and formulate
    assumptions about the behavior of other agents. The constitution also delineates
    the consequences, whether penalties or rewards, that agents may face for specific
    behaviors or interactions. This not only lends credibility to agreements among
    agents but also discourages undesirable behaviors, streamlining interactions and
    cultivating a cooperative environment.'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 宪法模块：该模块在为所有自主体建立共享基础方面起着至关重要的作用。其主要功能是定义一组共同规则，为自主体提供一个初步框架，以理解世界并对其他自主体的行为做出假设。宪法还明确了自主体因特定行为或互动可能面临的后果，无论是惩罚还是奖励。这不仅增加了自主体之间协议的可信度，还能避免不良行为，简化互动并促进协作环境的培养。
- en: '4.'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Reputation System: This system is designed to hold agents accountable for their
    actions. It evaluates each agent based on their adherence to agreements made with
    other agents. Periodically, the system prompts a language model with the existing
    agreements and corresponding actions, requesting a reputation score. This score
    is then accessible to all agents, influencing communication dynamics and aiding
    in understanding the behavior of others. Additionally, it facilitates making predictions
    about future states.'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 声誉系统：该系统旨在使自主体对其行为负责。它根据每个自主体与其他自主体达成的协议的遵守情况来评估每个自主体。定期，系统会提示语言模型输入现有协议和相应的行为，请求一个声誉评分。然后，该评分对所有自主体可见，影响沟通动态并有助于理解他人的行为。此外，它还促进了对未来状态的预测。
- en: 8 Conclusion and Future Work
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来工作
- en: Cooperative capabilities have been somewhat overlooked in LLMs’ agent architectures,
    yet they may represent the crucial element enabling agents to accomplish pioneering
    tasks and thrive in intricate environments. As large language models (LLMs) advance,
    agent architectures stand to gain significantly by attaining enhanced responses
    from LLMs, particularly in tasks demanding substantial reasoning or when confronted
    with copious information in the prompt.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 协作能力在LLMs的自主体架构中一直被忽视，但它们可能代表了使自主体能够完成开创性任务并在复杂环境中蓬勃发展的关键元素。随着大型语言模型（LLMs）的发展，自主体架构将通过从LLMs获得更好的响应，尤其是在需要大量推理或面对大量提示信息时，受益匪浅。
- en: In this paper, our objective is to ascertain whether LLMs-enhanced autonomous
    agents can operate cooperatively. To this end, we adapt the Melting Pot scenarios
    to textual representations that can be easily operationalized by LLMs, and implement
    a reusable architecture for the development of LAAs employing the modules proposed
    in Generative Agents (Park et al., [2023](https://arxiv.org/html/2403.11381v2#bib.bib14)).
    This architecture includes short and long-term memories, and cognitive modules
    of perception, planning, reflection, and action. The “Commons Harvest” game was
    used to test the resulting system, and the results were evaluated from the viewpoint
    of cooperative metrics in different proposed scenarios.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的目标是确定增强型大型语言模型（LLMs）驱动的自主体是否能够协作操作。为此，我们将“Melting Pot”场景转化为可以被LLMs轻松操作的文本表示，并实现了一个可重用的架构，用于开发利用生成体模块（Park等人，[2023](https://arxiv.org/html/2403.11381v2#bib.bib14)）的LAA。该架构包括短期和长期记忆，以及感知、规划、反思和行动的认知模块。我们使用“Commons
    Harvest”游戏来测试该系统，并从不同提出的场景中协作指标的角度评估了结果。
- en: The results indicate a gap in the current agents’ cooperative capabilities vis-à-vis
    unfamiliar situations. Agents showed a cooperative tendency but lacked an adequate
    understanding of how to collaborate effectively in an unknown environment. The
    agents needed to understand complex factors like the need to conserve resources,
    identify non-cooperative agents, and prioritize collective welfare over short-term
    gains. The research thereby draws attention to the need for a more inclusive architecture
    fostering cooperation and enhancing agent capabilities, including superior understanding,
    effective communication, credible commitment, and well-defined social structures
    or institutions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，目前代理在面对不熟悉情境时的协作能力存在差距。代理展示出一定的协作倾向，但缺乏有效合作的充分理解，尤其是在陌生环境中的协作。代理需要理解复杂的因素，如资源保护的需求、识别非合作代理、以及将集体福利置于短期收益之上的优先权。因此，研究指出需要一种更加包容的架构来促进合作，并增强代理的能力，包括更高水平的理解、有效的沟通、可信的承诺以及明确的社会结构或制度。
- en: Responding to the findings, we also proposed to improve the architecture with
    several modules to enhance the cooperative capabilities of the agents. These include
    an understanding module responsible for a comprehensive analysis of the agent’s
    memory and surroundings, a communication module to enable intentional information
    exchange, a constitution module that lays out common rules of engagement, and
    a reputation system that holds agents accountable for making decisions for the
    collective good. Our future efforts will be focused on building and evaluating
    this cooperative architecture.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些发现，我们还提出了通过几个模块改进架构，以增强代理的协作能力。这些模块包括：负责全面分析代理记忆和环境的理解模块、促进意图信息交换的通信模块、制定常见规则的宪法模块，以及一个让代理对集体利益做出决策负责的声誉系统。我们的未来工作将专注于构建和评估这一协作架构。
- en: Data Availability
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据可用性
- en: All the data generated for each simulation and the summary files for each experiment
    are available at [experiments data](https://zenodo.org/records/11221750). The
    code repository will be shared on GitHub upon acceptance of the paper.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所有为每个模拟生成的数据以及每个实验的总结文件可以在[实验数据](https://zenodo.org/records/11221750)中找到。代码仓库将在论文接受后通过GitHub分享。
- en: Acknowledgments
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by Google through the Google Research Scholar Program.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了谷歌通过谷歌研究学者计划的资助支持。
- en: Appendix A Descriptions generated for the objects in the environment
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 环境中物体的描述
- en: In Table [1](https://arxiv.org/html/2403.11381v2#A1.T1 "Table 1 ‣ Appendix A
    Descriptions generated for the objects in the environment ‣ Can LLM-Augmented
    autonomous agents cooperate?, An evaluation of their cooperative capabilities
    through Melting Pot"), we show all the natural language descriptions generated
    to represent the relevant objects and events of the Commons Harvest scenario of
    Melting Pot.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[1](https://arxiv.org/html/2403.11381v2#A1.T1 "Table 1 ‣ Appendix A Descriptions
    generated for the objects in the environment ‣ Can LLM-Augmented autonomous agents
    cooperate?, An evaluation of their cooperative capabilities through Melting Pot")中，我们展示了为表示《Melting
    Pot》公共收获场景中的相关物体和事件所生成的所有自然语言描述。
- en: 'Table 1: Natural language description by object or event'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：按物体或事件的自然语言描述
- en: '| Object/Event | Description |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 物体/事件 | 描述 |'
- en: '| --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Other agent | Observed agent `<agent_name>` at position `[<x>, <y>]`. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 其他代理 | 观察到代理`<agent_name>`在位置`[<x>, <y>]`。 |'
- en: '| Grass | Observed grass to grow apples at position `[<x>, <y>]`. This grass
    belongs to tree `<tree_id>`. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 草地 | 观察到草地在位置`[<x>, <y>]`上生长了苹果。这片草地属于树`<tree_id>`。 |'
- en: '| Apple | Observed an apple at position `[<x>, <y>]`. This apple belongs to
    tree `<tree_id>`. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 苹果 | 观察到位置`[<x>, <y>]`上的一个苹果。此苹果属于树`<tree_id>`。 |'
- en: '| Tree | Observed tree `<tree_id>` at position `[<x>, <y>]`. This tree has
    `apples_number` apples remaining and `grass_number` grass for apples growing on
    the observed map. The tree might have more apples and grass on the global map.
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 树 | 观察到位置`[<x>, <y>]`上的树`<tree_id>`。此树剩余`apples_number`个苹果和`grass_number`棵草，草用于苹果的生长。树可能在全局地图上还有更多的苹果和草。
    |'
- en: '| Observed someone being attacked | Someone was attacked at position `[<x>,
    <y>]`. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 观察到有人被攻击 | 观察到有人在位置`[<x>, <y>]`被攻击。 |'
- en: '| Observed a ray beam | Observed a ray beam from an attack at position `[<x>,
    <y>]`. |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 观察到一束射线 | 观察到从位置`[<x>, <y>]`发出的射线束。 |'
- en: '| Observed an apple was taken | Observed that agent `agent_name` took an apple
    from position `[<x>, <y>]`. |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 观察到一个苹果被拿走 | 观察到代理`agent_name`从位置`[<x>, <y>]`拿走了一个苹果。 |'
- en: '| Observed grass disappeared | Observed that the grass at position `[<x>, <y>]`
    disappeared. |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 观察到草消失 | 观察到位置`[<x>, <y>]`的草消失了。 |'
- en: '| Observed grass grew | Observed that grass to grow apples appeared at position
    `[<x>, <y>]`. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 观察到草生长 | 观察到在位置`[<x>, <y>]`处出现了生长苹果的草。 |'
- en: '| Observed apple grew | Observed that an apple grew at position `[<x>, <y>]`.
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 观察到苹果生长 | 观察到一个苹果在位置`[<x>, <y>]`处生长。 |'
- en: '| The agent was attacked | There are no observations: You were attacked by
    agent `agent_name` and currently you’re out of the game. |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 代理被攻击 | 没有观察到：你被代理`agent_name`攻击，当前已退出游戏。 |'
- en: '| The agent is out of the game | There are no observations: you’re out of the
    game. |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 代理已退出游戏 | 没有观察到：你已退出游戏。 |'
- en: Appendix B Knowledge about the world given to agents
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 传递给代理的世界知识
- en: The Listing [2](https://arxiv.org/html/2403.11381v2#LST2 "Listing 2 ‣ Appendix
    B Knowledge about the world given to agents ‣ Can LLM-Augmented autonomous agents
    cooperate?, An evaluation of their cooperative capabilities through Melting Pot")
    shows the raw world description passed to the agents. This is the only information
    agents have about the environment.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [2](https://arxiv.org/html/2403.11381v2#LST2 "Listing 2 ‣ Appendix B Knowledge
    about the world given to agents ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot") 显示了传递给代理的原始世界描述。这是代理对环境的唯一了解。
- en: 'Listing 2: World Context given to agents'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2：传递给代理的世界背景
- en: '[⬇](data:text/plain;base64,SSBhbSBpbiBhIG1pc3RlcmlvdXMgZ3JpZCB3b3JsZC4gSW4gdGhpcyB3b3JsZCB0aGVyZSBhcmUgdGhlIGZvbGxvd2luZyBlbGVtZW50czoKQXBwbGU6IFRoaXMgb2JqZWN0IGNhbiBiZSB0YWtlbiBieSBhbnkgYWdlbnQuIFRoZSBhcHBsZSBpcyB0YWtlbiB3aGVuIEkgZ28gdG8gaXRzIHBvc2l0aW9uLiBBcHBsZXMgb25seSBncm93IG9uIGdyYXNzIHRpbGVzLiBXaGVuIGFuIGFwcGxlIGlzIHRha2VuIGl0IGdpdmVzIHRoZSBhZ2VudCB3aG8gdG9vayBpdCBhIHJld2FyZCBvZiAxLgpHcmFzczogR3Jhc3MgdGlsZXMgYXJlIHZpc2libGUgd2hlbiBhbiBhcHBsZSBpcyB0YWtlbi4gQXBwbGVzIHdpbGwgcmVncm93IG9ubHkgaW4gdGhpcyB0eXBlIG9mIHRpbGUgYmFzZWQgb24gYSBwcm9iYWJpbGl0eSB0aGF0IGRlcGVuZHMgb24gdGhlIG51bWJlciBvZiBjdXJyZW50IGFwcGxlcyBpbiBhIEwyIG5vcm0gbmVpZ2hib3Job29kIG9mIHJhZGl1cyAyLiBXaGVuIHRoZXJlIGFyZSBubyBhcHBsZXMgaW4gYSByYWRpdXMgb2YgMiBmcm9tIHRoZSBncmFzcyB0aWxlLCB0aGUgZ3Jhc3Mgd2lsbCBkaXNhcHBlYXIuIE9uIHRoZSBvdGhlciBoYW5kLCBpZiBhbiBhcHBsZSBncm93cyBhdCBhIGRldGVybWluZWQgcG9zaXRpb24sIGFsbCBncmFzcyB0aWxlcyB0aGF0IGhhZCBiZWVlbiBsb3N0IHdpbGwgcmVhcHBlYXIgaWYgdGhleSBhcmUgYmV0d2VlbiBhIHJhZGl1cyBvZiB0d28gZnJvbSB0aGUgYXBwbGUuClRyZWU6IEEgdHJlZSBpcyBjb21wb3NlZCBmcm9tIGFwcGxlcyBvciBncmFzcyB0aWxlcywgYW5kIGl0IGlzIGEgdHJlZSBiZWNhdXNlIHRoZSBwYXRjaCBvZiB0aGVzZSB0aWxlcyBpcyBjb25uZWN0ZWQgYW5kIGhhdmUgYSBmaXggbG9jYXRpb24gb24gdGhlIG1hcC4gVGhlc2UgdHJlZXMgaGF2ZSBhbiBpZCB0byBpbmRlbnRpZnkgdGhlbS4KV2FsbDogVGhlc2UgdGlsZXMgZGVsaW1pdHMgdGhlIGdyaWQgd29ybGQgYXQgdGhlIHRvcCwgdGhlIGxlZnQsIHRoZSBib3R0b20sIGFuZCB0aGUgcmlnaHQgb2YgdGhlIGdyaWQgd29ybGQuClRoZSBncmlkIHdvcmxkIGlzIGNvbXBvc2VkIG9mIDE4IHJvd3MgYW5kIDI0IGNvbHVtbnMuIFRoZSB0aWxlcyBzdGFydCBmcm9tIHRoZSBbMCwgMF0gcG9zaXRpb24gbG9jYXRlZCBhdCB0aGUgdG9wIGxlZnQsIGFuZCBmaW5pc2ggb24gdGhlIFsxNywgMjNdIHBvc2l0aW9uIGxvY2F0ZWQgYXQgdGhlIGJvdHRvbSByaWdodC4KSSBhbSBhbiBhZ2VudCBhbmQgSSBoYXZlIGEgbGltaXRlZCB3aW5kb3cgb2Ygb2JzZXJ2YXRpb24gb2YgdGhlIHdvcmxkLg==)1I  am  in  a  misterious  grid  world.  In  this  world  there  are  the  following  elements:2Apple:  This  object  can  be  taken  by  any  agent.  The  apple  is  taken  when  I  go  to  its  position.  Apples  only  grow  on  grass  tiles.  When  an  apple  is  taken  it  gives  the  agent  who  took  it  a  reward  of  1.3Grass:  Grass  tiles  are  visible  when  an  apple  is  taken.  Apples  will  regrow  only  in  this  type  of  tile  based  on  a  probability  that  depends  on  the  number  of  current  apples  in  a  L2  norm  neighborhood  of  radius  2.  When  there  are  no  apples  in  a  radius  of  2  from  the  grass  tile,  the  grass  will  disappear.  On  the  other  hand,  if  an  apple  grows  at  a  determined  position,  all  grass  tiles  that  had  beeen  lost  will  reappear  if  they  are  between  a  radius  of  two  from  the  apple.4Tree:  A  tree  is  composed  from  apples  or  grass  tiles,  and  it  is  a  tree  because  the  patch  of  these  tiles  is  connected  and  have  a  fix  location  on  the  map.  These  trees  have  an  id  to  indentify  them.5Wall:  These  tiles  delimits  the  grid  world  at  the  top,  the  left,  the  bottom,  and  the  right  of  the  grid  world.6The  grid  world  is  composed  of  18  rows  and  24  columns.  The  tiles  start  from  the  [0,  0]  position  located  at  the  top  left,  and  finish  on  the  [17,  23]  position  located  at  the  bottom  right.7I  am  an  agent  and  I  have  a  limited  window  of  observation  of  the  world.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,SSBhbSBpbiBhIG1pc3RlcmlvdXMgZ3JpZCB3b3JsZC4gSW4gdGhpcyB3b3JsZCB0aGVyZSBhcmUgdGhlIGZvbGxvd2luZyBlbGVtZW50czoKQXBwbGU6IFRoaXMgb2JqZWN0IGNhbiBiZSB0YWtlbiBieSBhbnkgYWdlbnQuIFRoZSBhcHBsZSBpcyB0YWtlbiB3aGVuIEkgZ28gdG8gaXRzIHBvc2l0aW9uLiBBcHBsZXMgb25seSBncm93IG9uIGdyYXNzIHRpbGVzLiBXaGVuIGFuIGFwcGxlIGlzIHRha2VuIGl0IGdpdmVzIHRoZSBhZ2VudCB3aG8gdG9vayBpdCBhIHJld2FyZCBvZiAxLgpHcmFzczogR3Jhc3MgdGlsZXMgYXJlIHZpc2libGUgd2hlbiBhbiBhcHBsZSBpcyB0YWtlbi4gQXBwbGVzIHdpbGwgcmVncm93IG9ubHkgaW4gdGhpcyB0eXBlIG9mIHRpbGUgYmFzZWQgb24gYSBwcm9iYWJpbGl0eSB0aGF0IGRlcGVuZHMgb24gdGhlIG51bWJlciBvZiBjdXJyZW50IGFwcGxlcyBpbiBhIEwyIG5vcm0gbmVpZ2hib3Job29rIG9mIHJhZGl1cyAyLiBXaGVuIHRoZXJlIGFyZSBubyBhcHBsZGVzIGluIGEgcmFkaXVzIG9mIDIgZnJvbSB0aGUgZ3Jhc3MgdGlsZSwgdGhlIGdyaWQgdyBpbCBkaXNhcHBlYXIuIE9uIHRoZSBvdGhlciBoYW5kLCBpZiBhbiBhcHBsZSBncm93cyBhdCBhIGRldGVybWluZWQgcG9zaXRpb24sIGFsbCBncmFzcyB0aWxlcyB0aGF0IGhhZCBiZWVlbiBsb3N0IHdpbGwgcmVhcHBlYXIgaWYgdGhleSBhcmUgYmV0d2VlbiBhIHJhZGl1cyBvZiB0d28gZnJvbSB0aGUgYXBwbGUuClRyZWU6IEEgdHJlZSBpcyBjb21wb3NlZCBmcm9tIGFwcGxlcyBvciBncmFzYyB0aWxlcywgYW5kIGl0IGlzIGEgdHJlZSBiZWNhdXNlIHRoZSBwYXRjaCBvZiB0aGVzZSB0aWxlcyBpcyBjb25uZWN0ZWQgYW5kIGhhdmUgYSBmaXggbG9jYXRpb24gb24gdGhlIG1hcC4gVGhlc2UgdHJlZXMgaGF2ZSBhbiBpZCB0byBpbmRlbnRpZnkgdGhlbS4KV2FsbDogVGhlc2UgdGlsZXMgZGVsaW1pdHMgdGhlIGdyaWQgd29ybGQgYXQgdGhlIHRvcCwgdGhlIGxlZnQsIHRoZSBib3R0b20sIGFuZCB0aGUgcmlnaHQgb2YgdGhlIGdyaWQgd29ybGQuClRoZSBncmlkIHdvcmxkIGlzIGNvbXBvc2VkIG9mIDE4IHJvd3MgYW5kIDI0IGNvbHVtbnMuIFRoZSB0aWxlcyBzdGFydCBmcm9tIHRoZSBbMCwgMF0gcG9zaXRpb24gbG9jYXRlZCBhdCB0aGUgdG9wIGxlZnQsIGFuZCBmaW5pc2ggb24gdGhlIFsxNywgMjNdIHBvc2l0aW9uIGxvY2F0ZWQgYXQgdGhlIGJvdHRvbSByaWdodC4KSSBhbSBhbiBhZ2VudCBhbmQgSSBoYXZlIGEgbGltaXRlZCB3aW5kb3cgb2Ygb2JzZXJ2YXRpb24gb2YgdGhlIHdvcmxkLg==)1我身处于一个神秘的网格世界。在这个世界里，有以下元素：2苹果：这个物体可以被任何代理人获取。当我走到苹果所在的位置时，苹果就会被拿走。苹果只会在草地砖块上生长。当苹果被拿走时，它会给拿走它的代理人奖励1分。3草地：草地砖块在苹果被拿走时会显现。苹果仅会在这种类型的砖块上重新生长，这个生长的概率依赖于L2范数半径为2的邻域中当前苹果的数量。当草地砖块周围半径2内没有苹果时，草地会消失。另一方面，如果苹果在某个位置生长，所有丢失的草地砖块会在苹果半径为2的范围内重新出现。4树木：树木由苹果或草地砖块组成，它被认为是树木是因为这些砖块的区域是连通的，并且在地图上有固定的位置。这些树木有一个ID来识别它们。5墙壁：这些砖块构成了网格世界的边界，位于网格世界的上、左、下、右。6网格世界由18行和24列组成。砖块从位于左上角的[0,
    0]位置开始，到位于右下角的[17, 23]位置结束。7我是一个代理人，我对这个世界的观察窗口是有限的。'
- en: Appendix C React Prompt
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C React 提示
- en: 'The Listing [3](https://arxiv.org/html/2403.11381v2#LST3 "Listing 3 ‣ Appendix
    C React Prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") shows the entire prompt
    used in the react module. This prompt enables the agent to decide whether to react
    to the current observations—where reacting implies altering the plan and generating
    a new action. The prompt receives inputs in the following order: name, world context,
    current observations, current plan, actions to take if any, changes observed in
    the game state, game time, and agent’s personality.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [3](https://arxiv.org/html/2403.11381v2#LST3 "列表 3 ‣ 附录 C React 提示 ‣ LLM增强的自主代理能否合作？通过Melting
    Pot评估它们的合作能力") 展示了在 react 模块中使用的完整提示。这个提示使得代理能够决定是否对当前观察结果作出反应——其中“反应”意味着改变计划并生成新的行动。该提示按以下顺序接收输入：名称、世界上下文、当前观察、当前计划、需要采取的行动（如果有的话）、游戏状态中观察到的变化、游戏时间和代理的个性。
- en: 'Listing 3: Prompt of the Perceive Module'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 3: 感知模块的提示'
- en: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ2PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKQ3VycmVudCBvYnNlcnZhdGlvbnMgYXQgPGlucHV0Nz46CjxpbnB1dDM+Cgo8aW5wdXQ2PgoKPGlucHV0OD4KCkN1cnJlbnQgcGxhbjogPGlucHV0ND4KCkFjdGlvbnMgdG8gZXhlY3V0ZTogPGlucHV0NT4KClJldmlldyB0aGUgcGxhbiBhbmQgdGhlIGFjdGlvbnMgdG8gZXhlY3V0ZSwgYW5kIHRoZW4gZGVjaWRlIGlmIDxpbnB1dDE+IHNob3VsZCBjb250aW51ZSB3aXRoIGl0cyBwbGFuIGFuZCB0aGUgYWN0aW9ucyB0byBleGVjdXRlIGdpdmVuIHRoZSBuZXcgaW5mb3JtYXRpb24gdGhhdCBpdCdzIHNlZWluZyBpbiB0aGUgb2JzZXJ2YXRpb25zLgpSZW1lbWJlciB0aGF0IHRoZSBjdXJyZW50IG9ic2VydmF0aW9ucyBhcmUgb3JkZXJlZCBieSBjbG9zZW5lc3MsIGJlaW5nIHRoZSBmaXJzdCB0aGUgY2xvc2VzdCBvYnNlcnZhdGlvbiBhbmQgdGhlIGxhc3QgdGhlIGZhcnRoZXN0IG9uZS4KClRoZSBvdXRwdXQgc2hvdWxkIGJlIGEgbWFya2Rvd24gY29kZSBzbmlwcGV0IGZvcm1hdHRlZCBpbiB0aGUgZm9sbG93aW5nIHNjaGVtYSwgaW5jbHVkaW5nIHRoZSBsZWFkaW5nIGFuZCB0cmFpbGluZyAiYGBganNvbiIgYW5kICJgYGAiLCBhbnN3ZXIgYXMgaWYgeW91IHdlcmUgPGlucHV0MT46CgpgYGBqc29uCnsKICJSZWFzb25pbmciOiBzdHJpbmcsIFxcIFN0ZXAgYnkgc3RlcCB0aGlua2luZyBhbmQgYW5hbHlzaXMgb2YgYWxsIHRoZSBvYnNlcnZhdGlvbnMgYW5kIHRoZSBjdXJyZW50IHBsYW4gdG8gZGVjaWRlIGlmIHRoZSBwbGFuIHNob3VsZCBiZSBjaGFuZ2VkIG9yIG5vdAogIkFuc3dlciI6IGJvb2wgXFwgQW5zd2VyIHRydWUgaWYgdGhlIHBsYW4gb3IgYWN0aW9ucyB0byBleGVjdXRlIHNob3VsZCBiZSBjaGFuZ2VkIG9yIGZhbHNlIG90aGVyd2lzZQp9)1You  have  this  information  about  an  agent  called  <input1>:23<input6>45<input1>’s  world  understanding:  <input2>67Current  observations  at  <input7>:8<input3>910<input6>1112<input8>1314Current  plan:  <input4>1516Actions  to  execute:  <input5>1718Review  the  plan  and  the  actions  to  execute,  and  then  decide  if  <input1>  should  continue  with  its  plan  and  the  actions  to  execute  given  the  new  information  that  it’s  seeing  in  the  observations.19Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farthest  one.2021The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "‘‘‘",  answer  as  if  you  were  <input1>:2223‘‘‘json24{25  "Reasoning":  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  decide  if  the  plan  should  be  changed  or  not26  "Answer":  bool  \\  Answer  true  if  the  plan  or  actions  to  execute  should  be  changed  or  false  otherwise27}'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ2PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKQ3VycmVudCBvYnNlcnZhdGlvbnMgYXQgPGlucHV0Nz46CjxpbnB1dDM+Cgo8aW5wdXQ2PgoKPGlucHV0OD4KCkN1cnJlbnQgcGxhbjogPGlucHV0ND4KCkFjdGlvbnMgdG8gZXhlY3V0ZTogPGlucHV0NT4KClJldmlldyB0aGUgcGxhbiBhbmQgdGhlIGFjdGlvbnMgdG8gZXhlY3V0ZSwgYW5kIHRoZW4gZGVjaWRlIGlmIDxpbnB1dDE+IHNob3VsZCBjb250aW51ZSB3aXRoIGl0cyBwbGFuIGFuZCB0aGUgYWN0aW9ucyB0byBleGVjdXRlIGdpdmVuIHRoZSBuZXcgaW5mb3JtYXRpb24gdGhhdCBpdCdzIHNlZWluZyBpbiB0aGUgb2JzZXJ2YXRpb25zLgpSZW1lbWJlciB0aGF0IHRoZSBjdXJyZW50IG9ic2VydmF0aW9ucyBhcmUgb3JkZXJlZCBieSBjbG9zZW5lc3MsIGJlaW5nIHRoZSBmaXJzdCB0aGUgY2xvc2VzdCBvYnNlcnZhdGlvbiBhbmQgdGhlIGxhc3QgdGhlIGZhcnRoZXN0IG9uZS4KClRoZSBvdXRwdXQgc2hvdWxkIGJlIGEgbWFya2Rvd24gY29kZSBzbmlpcGV0IGZvcm1hdHRlZCBpbiB0aGUgZm9sbG93aW5nIHNjaGVtYSwgaW5jbHVkaW5nIHRoZSBsZWFkaW5nIGFuZCB0cmFpbGluZyAiYGBganNvbiIgYW5kICJgYGAiLCBhbnN3ZXIgYXMgZm9yIHlvdSB3ZXJlIFBsdW5lIFRlc3RpbmdsbmVzc3QwcnkiMjI0NS8KNTA2NzA1d2VsaXNmZXJlOXoU1knX+buXQ3FpxLbdlXUGef5dc8URRn7AfhEj3kGfi%2FxMJhjGsyhVJt%2FpBCzPmZZM3y9%2BzXI7sj9Py%2Q%2F4)'
- en: Appendix D Plan prompt
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 计划提示
- en: 'The Listing [4](https://arxiv.org/html/2403.11381v2#LST4 "Listing 4 ‣ Appendix
    D Plan prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") shows the raw prompt used
    in the plan module. This prompt helps the agent make a high-level plan and define
    several goals to guide its actions. The inputs that this prompt receives are the
    following in order: name, world context, current observations, current plan, reflections,
    reason to react, agent’s personality, and changes observed in the game state.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [4](https://arxiv.org/html/2403.11381v2#LST4 "Listing 4 ‣ Appendix D Plan
    prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot") 显示了在规划模块中使用的原始提示。这个提示帮助代理制定高级计划并定义多个目标来指导其行动。这个提示接收的输入依次是：名称、世界上下文、当前观察、当前计划、反思、反应的理由、代理的个性以及在游戏状态中观察到的变化。
- en: 'Listing 4: Prompt of Planning Module'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4：规划模块的提示
- en: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ3PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKUmVjZW50IGFuYWx5c2lzIG9mIHBhc3Qgb2JzZXJ2YXRpb25zOgo8aW5wdXQ1PgoKT2JzZXJ2ZWQgY2hhbmdlcyBpbiB0aGUgZ2FtZSBzdGF0ZToKPGlucHV0OD4KCkN1cnJlbnQgb2JzZXJ2YXRpb25zOgo8aW5wdXQzPgoKQ3VycmVudCBwbGFuOiA8aW5wdXQ0PgpUaGlzIGlzIHRoZSByZWFzb24gdG8gY2hhbmdlIHRoZSBjdXJyZW50IHBsYW46IDxpbnB1dDY+CgpXaXRoIHRoZSBpbmZvcm1hdGlvbiBnaXZlbiBhYm92ZSwgZ2VuZXJhdGUgYSBuZXcgcGxhbiBhbmQgbmV3IG9iamVjdGl2ZXMgdG8gcGVyc3VpdC4gVGhlIHBsYW4gc2hvdWxkIGJlIGEgZGVzY3JpcHRpb24gb2YgaG93IDxpbnB1dDE+IHNob3VsZCBiZWhhdmUgaW4gdGhlIGxvbmctdGVybSB0byBtYXhpbWl6ZSBpdHMgd2VsbGJlaW5nLgpUaGUgcGxhbiBzaG91bGQgaW5jbHVkZSBob3cgdG8gYWN0IHRvIGRpZmZlcmVudCBzaXR1YXRpb25zIG9ic2VydmVkIGluIHBhc3QgZXhwZXJpZW5jZXMuCgpUaGUgb3V0cHV0IHNob3VsZCBiZSBhIG1hcmtkb3duIGNvZGUgc25pcHBldCBmb3JtYXR0ZWQgaW4gdGhlIGZvbGxvd2luZyBzY2hlbWEsIGluY2x1ZGluZyB0aGUgbGVhZGluZyBhbmQgdHJhaWxpbmcgImBgYGpzb24iIGFuZCAiJycnIiwgYW5zd2VyIGFzIGlmIHlvdSB3ZXJlIDxpbnB1dDE+OgoKYGBganNvbgp7CiAiUmVhc29uaW5nIjogc3RyaW5nLCBcXCBTdGVwIGJ5IHN0ZXAgdGhpbmtpbmcgYW5kIGFuYWx5c2lzIG9mIGFsbCB0aGUgb2JzZXJ2YXRpb25zIGFuZCB0aGUgY3VycmVudCBwbGFuIHRvIGNyZWF0ZSB0aGUgbmV3IHBsYW4gYW5kIHRoZSBuZXcgZ29hbHMuCiAiR29hbHMiOiBzdHJpbmcsIFxcIFRoZSBuZXcgZ29hbHMgZm9yIDxpbnB1dDE+LgogIlBsYW4iOiBzdHJpbmcgXFwgVGhlIG5ldyBwbGFuIGZvciA8aW5wdXQxPi4gRG8gbm90IGRlc2NyaWJlIHNwZWNpZmljIGFjdGlvbnMuCn0nJyc=)1You  have  this  information  about  an  agent  called  <input1>:23<input7>45<input1>’s  world  understanding:  <input2>67Recent  analysis  of  past  observations:8<input5>910Observed  changes  in  the  game  state:11<input8>1213Current  observations:14<input3>1516Current  plan:  <input4>17This  is  the  reason  to  change  the  current  plan:  <input6>1819With  the  information  given  above,  generate  a  new  plan  and  new  objectives  to  persuit.  The  plan  should  be  a  description  of  how  <input1>  should  behave  in  the  long-term  to  maximize  its  wellbeing.20The  plan  should  include  how  to  act  to  different  situations  observed  in  past  experiences.2122The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "’’’",  answer  as  if  you  were  <input1>:2324‘‘‘json25{26  "Reasoning":  string,  \\  Step  by  step  thinking  and  analysis  of  all  the  observations  and  the  current  plan  to  create  the  new  plan  and  the  new  goals.27  "Goals":  string,  \\  The  new  goals  for  <input1>.28  "Plan":  string  \\  The  new  plan  for  <input1>.  Do  not  describe  specific  actions.29}’’’'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ3PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKUmVjZW50IGFuYWx5c2lzIG9mIHBhc3Qgb2JzZXJ2YXRpb25zOgo8aW5wdXQ1PgoKT2JzZXJ2ZWQgY2hhbmdlcyBpbiB0aGUgZ2FtZSBzdGF0ZToKPGlucHV0OD4KCkN1cnJlbnQgb2JzZXJ2YXRpb25zOgo8aW5wdXQzPgoKQ3VycmVudCBwbGFuOiA8aW5wdXQ0PgpUaGlzIGlzIHRoZSByZWFzb24gdG8gY2hhbmdlIHRoZSBjdXJyZW50IHBsYW46IDxpbnB1dDY+CgpXaXRoIHRoZSBpbmZvcm1hdGlvbiBnaXZlbiBhYm92ZSwgZ2VuZXJhdGUgYSBuZXcgcGxhbiBhbmQgbmV3IG9iamVjdGl2ZXMgdG8gcGVyc3VpdC4gVGhlIHBsYW4gc2hvdWxkIGJlIGEgZGVzY3JpcHRpb24gb2YgaG93IDxpbnB1dDE+IHNob3VsZCBiZWhhdmUgaW4gdGhlIGxvbmctdGVybSB0byBtYXhpbWl6ZSBpdHMgd2VsbGJlaW5nLgpUaGUgcGxhbiBzaG91bGQgaW5jbHVkZSBob3cgdG8gYWN0IHRvIGRpZmZlcmVudCBzaXR1YXRpb25zIG9ic2VydmVkIGluIHBhc3QgZXhwZXJpZW5jZXMuCgpUaGUgb3V0cHV0IHNob3VsZCBiZSBhIG1hcmtkb3duIGNvZGUgc25pcHBldCBmb3JtYXR0ZWQgaW4gdGhlIGZvbGxvd2luZyBzY2hlbWEsIGluY2x1ZGluZyB0aGUgbGVhZGluZyBhbmQgdHJhaWxpbmcgImBgYGpzb24iIGFuZCAiJycnIiwgYW5zd2VyIGFzIGlmIHlvdSB3ZXJlIDxpbnB1dDE+OgoKYGBganNvbgp7CiAiUmVhc29uaW5nIjogc3RyaW5nLCBcXCBZdXIgU3RlcCBieSBzdGVwIHRoaW5raW5nIGFuZCBhbmFseXNlcyBvZiBhbGwgdGhlIG9ic2VydmF0aW9ucyBhbmQgdGhlIGN1cnJlbnQgcGxhbiB0byBjcmVhdGUgdGhlIG5ldyBwbGFuIGFuZCB0aGUgbmV3IGdvYWxzLgogIkdvYWxzIjogc3RyaW5nLCBcXCBUaGUgbmV3IGdvYWxzIGZvciA8aW5wdXQxPi4gRG8gbm90IGRlc2NyaWJlIHNwZWNpZmljIGFjdGlvbnMuCn0nJyc=)1你有这个关于一个名为<input1>的代理的信息：23<input7>45<input1>的世界理解：<input2>67过去观察的最新分析：8<input5>910游戏状态中观察到的变化：11<input8>1213当前观察：14<input3>1516当前计划：<input4>17这是改变当前计划的原因：<input6>1819根据上述提供的信息，生成一个新的计划和新的目标来追求。该计划应描述<input1>如何在长期内行事，以最大化其福祉。20该计划应包括如何应对过去经验中观察到的不同情况。2122输出应为以下模式格式的Markdown代码片段，包括前导和尾随的"‘‘‘json"和"’’’"，回答时应像<input1>一样：2324‘‘‘json25{26  "Reasoning":  string,  \\  逐步思考和分析所有观察结果和当前计划，制定新的计划和新目标。27  "Goals":  string,  \\  <input1>的新目标。28  "Plan":  string  \\  <input1>的新计划。不要描述具体行动。29}’’’'
- en: Appendix E Reflection prompts
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 反思提示
- en: 'The Listing [5](https://arxiv.org/html/2403.11381v2#LST5 "Listing 5 ‣ Appendix
    E Reflection prompts ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot") shows the raw prompt used
    in the first part of the reflections module i.e. question formulation. The inputs
    for this prompt are the following: name, world context, accumulated observations
    since the last reflection, and agent’s personality.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 清单[5](https://arxiv.org/html/2403.11381v2#LST5 "Listing 5 ‣ Appendix E Reflection
    prompts ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot")显示了反思模块第一部分，即问题生成中使用的原始提示。此提示的输入包括：姓名、世界背景、自上次反思以来的累积观察，以及智能体的个性。
- en: 'The prompt used in the insight generation part that takes place in the reflect
    module is shown in Listing [6](https://arxiv.org/html/2403.11381v2#LST6 "Listing
    6 ‣ Appendix E Reflection prompts ‣ Can LLM-Augmented autonomous agents cooperate?,
    An evaluation of their cooperative capabilities through Melting Pot"), its corresponding
    inputs are the following: name, world context, group of memories retrieved for
    each generated question in the first part, and agent’s personality.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在反思模块中生成洞察的提示如清单[6](https://arxiv.org/html/2403.11381v2#LST6 "Listing 6 ‣ Appendix
    E Reflection prompts ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation
    of their cooperative capabilities through Melting Pot")所示，其相应的输入如下：姓名、世界背景、为每个生成的问题从第一部分检索到的记忆组，以及智能体的个性。
- en: 'Listing 5: Prompt of Reflect Module for question formulation'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 5：反思模块用于问题生成的提示
- en: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ0PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKSGVyZSB5b3UgaGF2ZSBhIGxpc3Qgb2Ygc3RhdGVtZW50czoKPGlucHV0Mz4KCkdpdmVuIG9ubHkgdGhlIGluZm9ybWF0aW9uIGFib3ZlLCBmb3JtdWxhdGUgdGhlIDMgbW9zdCBzYWxpZW50IGhpZ2gtbGV2ZWwgcXVlc3Rpb25zCnlvdSBjYW4gYW5zd2VyIGFib3V0IHRoZSBldmVudHMsIGVudGl0aWVzLCBhbmQgYWdlbnRzIGluIHRoZSBzdGF0ZW1lbnRzLgoKClRoZSBvdXRwdXQgc2hvdWxkIGJlIGEgbWFya2Rvd24gY29kZSBzbmlwcGV0IGZvcm1hdHRlZCBpbiB0aGUgZm9sbG93aW5nIHNjaGVtYSwKaW5jbHVkaW5nIHRoZSBsZWFkaW5nIGFuZCB0cmFpbGluZyAiYGBganNvbiIgYW5kICInJyciLCBhbnN3ZXIgYXMgaWYgeW91IHdlcmUgPGlucHV0MT46CgpgYGBqc29uCnsKICAgICJRdWVzdGlvbl8xIjogewogICAgICAgICJSZWFzb25pbmciOiBzdHJpbmcgXFwgUmVhc29uaW5nIGZvciB0aGUgcXVlc3Rpb24KICAgICAgICAiUXVlc3Rpb24iOiBzdHJpbmcgXFwgIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0sCiAgICAiUXVlc3Rpb25fMiI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBmb3IgdGhlIHF1ZXN0aW9uCiAgICAgICAgIlF1ZXN0aW9uIjogc3RyaW5nIFxcIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0sCiAgICAiUXVlc3Rpb25fMyI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBmb3IgdGhlIHF1ZXN0aW9uCiAgICAgICAgIlF1ZXN0aW9uIjogc3RyaW5nIFxcIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0KfScnJw==)1You  have  this  information  about  an  agent  called  <input1>:23<input4>45<input1>’s  world  understanding:  <input2>67Here  you  have  a  list  of  statements:8<input3>910Given  only  the  information  above,  formulate  the  3  most  salient  high-level  questions11you  can  answer  about  the  events,  entities,  and  agents  in  the  statements.121314The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,15including  the  leading  and  trailing  "‘‘‘json"  and  "’’’",  answer  as  if  you  were  <input1>:1617‘‘‘json18{19  "Question_1":  {20  "Reasoning":  string  \\  Reasoning  for  the  question21  "Question":  string  \\  The  question  itself22  },23  "Question_2":  {24  "Reasoning":  string  \\  Reasoning  for  the  question25  "Question":  string  \\  The  question  itself26  },27  "Question_3":  {28  "Reasoning":  string  \\  Reasoning  for  the  question29  "Question":  string  \\  The  question  itself30  }31}’’’'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ0PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKSGVyZSB5b3UgaGF2ZSBhIGxpc3Qgb2Ygc3RhdGVtZW50czoKPGlucHV0Mz4KCkdpdmVuIG9ubHkgdGhlIGluZm9ybWF0aW9uIGFib3ZlLCBmb3JtdWxhdGUgdGhlIDMgbW9zdCBzYWxpZW50IGhpZ2gtbGV2ZWwgcXVlc3Rpb25zCnlvdSBjYW4gYW5zd2VyIGFib3V0IHRoZSBldmVudHMsIGVudGl0aWVzLCBhbmQgYWdlbnRzIGluIHRoZSBzdGF0ZW1lbnRzLgoKClRoZSBvdXRwdXQgc2hvdGQgYmUgaGEgbWFya2Rvd24gY29kZSBzbmlwcGV0IGZvcm1hdHRlZCBpbiB0aGUgZm9sbG93aW5nIHNjaGVtYSwKaW5jbHVkaW5nIHRoZSBsZWFkaW5nIGFuZCB0cmFpbGluZyAiYGBganNvbiIgYW5kICInJyciLCBhbnN3ZXIgYXMgaWYgeW91IHdlcmUgPGlucHV0MT46CgpgYGBqc29uCnsKICAgICJRdWVzdGlvbl8xIjogewogICAgICAgICJSZWFzb25pbmciOiBzdHJpbmcgXFwgUmVhc29uaW5nIGZvciB0aGUgcXVlc3Rpb24KICAgICAgICAiUXVlc3Rpb24iOiBzdHJpbmcgXFwgIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0sCiAgICAiUXVlc3Rpb25fMiI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBmb3IgdGhlIHF1ZXN0aW9uCiAgICAgICAgIlF1ZXN0aW9uIjogc3RyaW5nIFxcIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0sCiAgICAiUXVlc3Rpb25fMyI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBmb3IgdGhlIHF1ZXN0aW9uCiAgICAgICAgIlF1ZXN0aW9uIjogc3RyaW5nIFxcIFRoZSBxdWVzdGlvbiBpdHNlbGYKICAgIH0KfScnJw==)1您拥有有关名为<input1>的代理的信息：<input2>67在这里，您有一份声明列表：8<input3>910仅凭上述信息，提出三个最重要的高级问题，您可以回答关于声明中的事件、实体和代理。121314输出应该是一个格式化的
    Markdown 代码片段，使用以下模式，包含前后的 "‘‘‘json" 和 "’’’"，并假设您是<input1>：1617‘‘‘json18{19  "Question_1":  {20  "Reasoning":  string  \\  理由22  "Question":  string  \\  问题本身23  },24  "Question_2":  {25  "Reasoning":  string  \\  理由26  "Question":  string  \\  问题本身27  },28  "Question_3":  {29  "Reasoning":  string  \\  理由30  "Question":  string  \\  问题本身31  }32}’’’'
- en: 'Listing 6: Prompt of Reflect Module for insights generation'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6：反射模块的提示，用于生成洞察
- en: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ0PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKSGVyZSB5b3UgaGF2ZSBhIGxpc3Qgb2YgbWVtb3J5IHN0YXRlbWVudHMgc2VwYXJhdGVkIGluIGdyb3VwcyBvZiBtZW1vcmllczoKPGlucHV0Mz4KCkdpdmVuIDxpbnB1dDE+J3MgbWVtb3JpZXMsIGZvciBlYWNoIG9uZSBvZiB0aGUgZ3JvdXAgb2YgbWVtb3JpZXMsIHdoYXQgaXMgdGhlIGJlc3QgaW5zaWdodCB5b3UgY2FuIHByb3ZpZGUgYmFzZWQgb24gdGhlIGluZm9ybWF0aW9uIHlvdSBoYXZlPwpFeHByZXNzIHlvdXIgYW5zd2VyIGluIHRoZSBKU09OIGZvcm1hdCBwcm92aWRlZCwgYW5kIHJlbWVtYmVyIHRvIGV4cGxhaW4gdGhlIHJlYXNvbmluZyBiZWhpbmQgZWFjaCBpbnNpZ2h0LgoKVGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLAppbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgIicnJyIsIGFuc3dlciBhcyBpZiB5b3Ugd2VyZSA8aW5wdXQxPjoKCmBgYGpzb24KewogICAgIkluc2lnaHRfMSI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBiZWhpbmQgdGhlIGluc2lnaHQgb2YgdGhlIGdyb3VwIG9mIG1lbW9yaWVzIDEKICAgICAgICAiSW5zaWdodCI6IHN0cmluZyBcXCBUaGUgaW5zaWdodCBpdHNlbGYKICAgIH0sCiAgICAiSW5zaWdodF8yIjogewogICAgICAgICJSZWFzb25pbmciOiBzdHJpbmcgXFwgUmVhc29uaW5nIGJlaGluZCB0aGUgaW5zaWdodCBvZiB0aGUgZ3JvdXAgb2YgbWVtb3JpZXMgMgogICAgICAgICJJbnNpZ2h0Ijogc3RyaW5nIFxcIFRoZSBpbnNpZ2h0IGl0c2VsZgogICAgfSwKICAgICJJbnNpZ2h0X24iOiB7CiAgICAgICAgIlJlYXNvbmluZyI6IHN0cmluZyBcXCBSZWFzb25pbmcgYmVoaW5kIHRoZSBpbnNpZ2h0IG9mIHRoZSBncm91cCBvZiBtZW1vcmllcyBuCiAgICAgICAgIkluc2lnaHQiOiBzdHJpbmcgXFwgVGhlIGluc2lnaHQgaXRzZWxmCiAgICB9Cn0nJyc=)1You  have  this  information  about  an  agent  called  <input1>:23<input4>45<input1>’s  world  understanding:  <input2>67Here  you  have  a  list  of  memory  statements  separated  in  groups  of  memories:8<input3>910Given  <input1>’s  memories,  for  each  one  of  the  group  of  memories,  what  is  the  best  insight  you  can  provide  based  on  the  information  you  have?11Express  your  answer  in  the  JSON  format  provided,  and  remember  to  explain  the  reasoning  behind  each  insight.1213The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,14including  the  leading  and  trailing  "‘‘‘json"  and  "’’’",  answer  as  if  you  were  <input1>:1516‘‘‘json17{18  "Insight_1":  {19  "Reasoning":  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  120  "Insight":  string  \\  The  insight  itself21  },22  "Insight_2":  {23  "Reasoning":  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  224  "Insight":  string  \\  The  insight  itself25  },26  "Insight_n":  {27  "Reasoning":  string  \\  Reasoning  behind  the  insight  of  the  group  of  memories  n28  "Insight":  string  \\  The  insight  itself29  }30}’’’'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGhhdmUgdGhpcyBpbmZvcm1hdGlvbiBhYm91dCBhbiBhZ2VudCBjYWxsZWQgPGlucHV0MT46Cgo8aW5wdXQ0PgoKPGlucHV0MT4ncyB3b3JsZCB1bmRlcnN0YW5kaW5nOiA8aW5wdXQyPgoKSGVyZSB5b3UgaGF2ZSBhIGxpc3Qgb2YgbWVtb3J5IHN0YXRlbWVudHMgc2VwYXJhdGVkIGluIGdyb3VwcyBvZiBtZW1vcmllczoKPGlucHV0Mz4KCkdpdmVuIDxpbnB1dDE+J3MgbWVtb3JpZXMsIGZvciBlYWNoIG9uZSBvZiB0aGUgZ3JvdXAgb2YgbWVtb3JpZXMsIHdoYXQgaXMgdGhlIGJlc3QgaW5zaWdodCB5b3UgY2FuIHByb3ZpZGUgYmFzZWQgb24gdGhlIGluZm9ybWF0aW9uIHlvdSBoYXZlPwpFeHByZXNzIHlvdXIgYW5zd2VyIGluIHRoZSBKU09OIGZvcm1hdCBwcm92aWRlZCwgYW5kIHJlbWVtYmVyIHRvIGV4cGxhaW4gdGhlIHJlYXNvbmluZyBiZWhpbmQgZWFjaCBpbnNpZ2h0LgoKVGhlIG91dHB1dCBzaG91bGQgYmUgYSBtYXJrZG93biBjb2RlIHNuaXBwZXQgZm9ybWF0dGVkIGluIHRoZSBmb2xsb3dpbmcgc2NoZW1hLAppbmNsdWRpbmcgdGhlIGxlYWRpbmcgYW5kIHRyYWlsaW5nICJgYGBqc29uIiBhbmQgIicnJyIsIGFuc3dlciBhcyBpZiB5b3Ugd2VyZSA8aW5wdXQxPjoKCmBgYGpzb24KewogICAgIkluc2lnaHRfMSI6IHsKICAgICAgICAiUmVhc29uaW5nIjogc3RyaW5nIFxcIFJlYXNvbmluZyBiZWhpbmQgdGhlIGluc2lnaHQgb2YgdGhlIGdyb3VwIG9mIG1lbW9yaWVzIDEKICAgICAgICAiSW5zaWdodCI6IHN0cmluZyBcXCBUaGUgaW5zaWdodCBpdHNlbGYKICAgIH0sCiAgICAiSW5zaWdodF8yIjogewogICAgICAgICJSZWFzb25pbmciOiBzdHJpbmcgXFwgUmVhc29uaW5nIGJlaGluZCB0aGUgaW5zaWdodCBvZiB0aGUgZ3JvdXAgb2YgbWVtb3JpZXMgMgogICAgICAgICJJbnNpZ2h0Ijogc3RyaW5nIFxcIFRoZSBpbnNpZ2h0IGl0c2VsZgogICAgfSwKICAgICJJbnNpZ2h0X24iOiB7CiAgICAgICAgIlJlYXNvbmluZyI6IHN0cmluZyBcXCBSZWFzb25pbmcgYmVoaW5kIHRoZSBpbnNpZ2h0IG9mIHRoZ3JlYXAgb2YgbWVtb3JpZXMgbgogICAgICAgICJJbnNpZ2h0Ijogc3RyaW5nIFxcIFRoZSBpbnNpZ2h0IGl0c2VsZgogICAgfSwKfSc=)1您拥有有关一个名为<input1>的代理的世界理解的信息：<input2>67这里您有一组按记忆分组的记忆陈述：8<input3>910根据<input1>的记忆，对于每组记忆，您能提供的最佳洞察是什么？11请根据提供的信息，以JSON格式回答，并记得解释每个洞察背后的推理。1213输出应为以下格式的Markdown代码片段，14包括前导和尾随的"‘‘‘json"
    和 "’’’"，并且回答时假设您是<input1>：1516‘‘‘json17{18  "Insight_1":  {19  "Reasoning":  string  \\  推理过程
    20  "Insight":  string  \\  洞察本身21  },22  "Insight_2":  {23  "Reasoning":  string  \\  推理过程
    24  "Insight":  string  \\  洞察本身25  },26  "Insight_n":  {27  "Reasoning":  string  \\  推理过程
    28  "Insight":  string  \\  洞察本身29  }30}’’’'
- en: Appendix F Act prompt
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 行动提示
- en: 'The Listing [7](https://arxiv.org/html/2403.11381v2#LST7 "Listing 7 ‣ Appendix
    F Act prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of
    their cooperative capabilities through Melting Pot") shows the raw prompt used
    in the act module. This prompt is in charge of deciding which action to take.
    The inputs that this prompt receives are the following: name, world context, current
    plan, the most recent ten reflections, current observations, number of actions
    to generate, set of valid actions, current goals, agent’s personality, position
    of the known trees, portion of the map explored, previous actions, and changes
    observed in the game state.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 [7](https://arxiv.org/html/2403.11381v2#LST7 "Listing 7 ‣ Appendix F Act
    prompt ‣ Can LLM-Augmented autonomous agents cooperate?, An evaluation of their
    cooperative capabilities through Melting Pot") 展示了在行动模块中使用的原始提示。这个提示负责决定采取哪种行动。这个提示接收的输入包括以下内容：名称、世界上下文、当前计划、最近的十次反思、当前观察、需要生成的行动数量、有效行动的集合、当前目标、代理的个性、已知树木的位置、已探索的地图区域、先前的行动以及在游戏状态中观察到的变化。
- en: 'Listing 7: Prompt of Action Module'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7：行动模块提示
- en: '[⬇](data:text/plain;base64,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)1You  have  this  information  about  an  agent  called  <input1>:23<input10>45<input1>’s  world  understanding:  <input2>67<input1>’s  goals:  <input9>89Current  plan:  <input3>1011Analysis  of  past  experiences:12<input4>1314<input11>1516Portion  of  the  map  explored  by  <input1>:  <input12>1718Observed  changes  in  the  game  state:19<input14>2021You  are  currently  viewing  a  portion  of  the  map,  and  from  your  position  at  <input6>  you  observe  the  following:22<input5>2324Define  what  should  be  the  nex  action  for  Laura  get  closer  to  achieve  its  goals  following  the  current  plan.25Remember  that  the  current  observations  are  ordered  by  closeness,  being  the  first  the  closest  observation  and  the  last  the  farest  one.26Each  action  you  determinate  can  only  be  one  of  the  following,  make  sure  you  assign  a  valid  position  from  the  current  observations  and  a  valid  name  for  each  action:2728Valid  actions:29<input8>3031Remember  that  going  to  positions  near  the  edge  of  the  portion  of  the  map  you  are  seeing  will  allow  you  to  get  new  observations.32<input13>3334The  output  should  be  a  markdown  code  snippet  formatted  in  the  following  schema,  including  the  leading  and  trailing  "‘‘‘json"  and  "’’’",  answer  as  if  you  were  Laura:35‘‘‘json36{37  "Opportunities":  string  \\  What  are  the  most  relevant  opportunities?  those  that  can  yield  the  best  benefit  for  you  in  the  long  term38  "Threats":  string  \\  What  are  the  biggest  threats?,  what  observations  you  should  carefully  follow  to  avoid  potential  harm  in  your  wellfare  in  the  long  term?39  "Options:  string  \\  Which  actions  you  could  take  to  address  both  the  opportunities  ans  the  threats?40  "Consequences":  string  \\  What  are  the  consequences  of  each  of  the  options?41  "Final  analysis:  string  \\  The  analysis  of  the  consequences  to  reason  about  what  is  the  best  action  to  take42  "Answer":  string  \\  Must  be  one  of  the  valid  actions  with  the  position  replaced43}’’’'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)'
- en: Appendix G Simulations Cost
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 模拟成本
- en: 'Table 2: Costs of simulations'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：模拟的成本
- en: '|  | Avg. Simulation | Avg. Execution |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均模拟 | 平均执行 |'
- en: '| --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Experiment | Cost ($) | Time (minutes) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | 成本（$） | 时间（分钟） |'
- en: '| --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Set 1 - No bio | $8.57(0.93)$ | $151.18(17.04)$ |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 集合 1 - 无生物 | $8.57(0.93)$ | $151.18(17.04)$ |'
- en: '| Set 1 - All Coop | $7.00(1.58)$ | $119.50(26.78)$ |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 集合 1 - 全部合作 | $7.00(1.58)$ | $119.50(26.78)$ |'
- en: '| Set 1 - All Coop with def¹¹1For this experiment, the models of OpenAir were
    used through Classic. | $15.63(5.69)$ | $212.60(65.14)$ |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 集合 1 - 全部合作且有偏差¹¹1 本实验使用了通过 Classic 的 OpenAir 模型。 | $15.63(5.69)$ | $212.60(65.14)$
    |'
- en: '| Set 1 - All Selfish | $8.60(1.84)$ | $127.63(26.57)$ |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 集合 1 - 全部自私 | $8.60(1.84)$ | $127.63(26.57)$ |'
- en: '| Set 1 - All Selfish with def | $9.73(1.59)$ | $217.50(65.57)$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 集合 1 - 全部自私且有偏差 | $9.73(1.59)$ | $217.50(65.57)$ |'
- en: '| Set 2 - One tree - no bio | $0.78(0.28)$ | $16.41(6.94)$ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 集合 2 - 一棵树 - 无生物 | $0.78(0.28)$ | $16.41(6.94)$ |'
- en: '| Set 2 - One tree - all coop | $0.78(0.17)$ | $13.93(2.99)$ |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 集合 2 - 一棵树 - 全部合作 | $0.78(0.17)$ | $13.93(2.99)$ |'
- en: '| Set 2 - One tree - all selfish | $0.83(0.30)$ | $13.72(4.38)$ |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 集合 2 - 一棵树 - 全部自私 | $0.83(0.30)$ | $13.72(4.38)$ |'
- en: '| Set 2 - Agents vs. Bots | $1.88(0.66)$ | $27.86(11.83)$ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 集合 2 - 代理 vs. 机器人 | $1.88(0.66)$ | $27.86(11.83)$ |'
- en: '| Set 3 - All aware one selfish | $10.05(0.88)$ | $151.93(10.56)$ |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 集合 3 - 所有意识到有一人自私 | $10.05(0.88)$ | $151.93(10.56)$ |'
- en: References
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Agapiou et al. (2023) Agapiou, J.P., Vezhnevets, A.S., Duéñez-Guzmán, E.A.,
    Matyas, J., Mao, Y., Sunehag, P., Köster, R., Madhushani, U., Kopparapu, K., Comanescu,
    R., Strouse, D., Johanson, M.B., Singh, S., Haas, J., Mordatch, I., Mobbs, D.,
    Leibo, J.Z., 2023. Melting pot 2.0. [arXiv:2211.13746](http://arxiv.org/abs/2211.13746).
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agapiou 等人 (2023) Agapiou, J.P., Vezhnevets, A.S., Duéñez-Guzmán, E.A., Matyas,
    J., Mao, Y., Sunehag, P., Köster, R., Madhushani, U., Kopparapu, K., Comanescu,
    R., Strouse, D., Johanson, M.B., Singh, S., Haas, J., Mordatch, I., Mobbs, D.,
    Leibo, J.Z., 2023. Melting pot 2.0. [arXiv:2211.13746](http://arxiv.org/abs/2211.13746).
- en: 'Axelrod and Hamilton (1981) Axelrod, R., Hamilton, W.D., 1981. The evolution
    of cooperation. Science 211, 1390–1396. URL: [https://www.science.org/doi/abs/10.1126/science.7466396](https://www.science.org/doi/abs/10.1126/science.7466396),
    doi:[10.1126/science.7466396](http://dx.doi.org/10.1126/science.7466396).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Axelrod 和 Hamilton (1981) Axelrod, R., Hamilton, W.D., 1981. 合作演化. 科学 211, 1390–1396.
    网址：[https://www.science.org/doi/abs/10.1126/science.7466396](https://www.science.org/doi/abs/10.1126/science.7466396)，doi：[10.1126/science.7466396](http://dx.doi.org/10.1126/science.7466396).
- en: 'Broekhuizen et al. (2023) Broekhuizen, T., Dekker, H., de Faria, P., Firk,
    S., Nguyen, D.K., Sofka, W., 2023. Ai for managing open innovation: Opportunities,
    challenges, and a research agenda. Journal of Business Research 167, 114196. URL:
    [https://www.sciencedirect.com/science/article/pii/S0148296323005556](https://www.sciencedirect.com/science/article/pii/S0148296323005556),
    doi:[https://doi.org/10.1016/j.jbusres.2023.114196](http://dx.doi.org/https://doi.org/10.1016/j.jbusres.2023.114196).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Broekhuizen 等人 (2023) Broekhuizen, T., Dekker, H., de Faria, P., Firk, S., Nguyen,
    D.K., Sofka, W., 2023. 用于管理开放创新的人工智能：机遇、挑战及研究议程. 商业研究期刊 167, 114196. 网址：[https://www.sciencedirect.com/science/article/pii/S0148296323005556](https://www.sciencedirect.com/science/article/pii/S0148296323005556)，doi：[https://doi.org/10.1016/j.jbusres.2023.114196](http://dx.doi.org/https://doi.org/10.1016/j.jbusres.2023.114196).
- en: Dafoe et al. (2020) Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee,
    K.R., Leibo, J.Z., Larson, K., Graepel, T., 2020. Open problems in cooperative
    ai. [arXiv:2012.08630](http://arxiv.org/abs/2012.08630).
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dafoe 等人 (2020) Dafoe, A., Hughes, E., Bachrach, Y., Collins, T., McKee, K.R.,
    Leibo, J.Z., Larson, K., Graepel, T., 2020. 合作人工智能中的开放问题. [arXiv:2012.08630](http://arxiv.org/abs/2012.08630).
- en: 'Dale et al. (2020) Dale, R., Marshall-Pescini, S., Range, F., 2020. What matters
    for cooperation? the importance of social relationship over cognition. Scientific
    Reports 10, 11778. URL: [https://doi.org/10.1038/s41598-020-68734-4](https://doi.org/10.1038/s41598-020-68734-4),
    doi:[10.1038/s41598-020-68734-4](http://dx.doi.org/10.1038/s41598-020-68734-4).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dale 等人 (2020) Dale, R., Marshall-Pescini, S., Range, F., 2020. 什么对合作至关重要？社会关系对认知的重要性.
    科学报告 10, 11778. 网址：[https://doi.org/10.1038/s41598-020-68734-4](https://doi.org/10.1038/s41598-020-68734-4)，doi：[10.1038/s41598-020-68734-4](http://dx.doi.org/10.1038/s41598-020-68734-4).
- en: Du et al. (2023) Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.,
    2023. Improving factuality and reasoning in language models through multiagent
    debate. [arXiv:2305.14325](http://arxiv.org/abs/2305.14325).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 (2023) Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I., 2023.
    通过多智能体辩论提升语言模型的事实性和推理能力. [arXiv:2305.14325](http://arxiv.org/abs/2305.14325).
- en: 'Gross et al. (2023) Gross, J., Méder, Z.Z., Dreu, C.K.D., Romano, A., Molenmaker,
    W.E., Hoenig, L.C., 2023. The evolution of universal cooperation. Science Advances
    9, eadd8289. URL: [https://www.science.org/doi/abs/10.1126/sciadv.add8289](https://www.science.org/doi/abs/10.1126/sciadv.add8289),
    doi:[10.1126/sciadv.add8289](http://dx.doi.org/10.1126/sciadv.add8289).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gross 等人（2023）Gross, J., Méder, Z.Z., Dreu, C.K.D., Romano, A., Molenmaker,
    W.E., Hoenig, L.C., 2023. 普遍合作的演化。科学进展 9, eadd8289。网址：[https://www.science.org/doi/abs/10.1126/sciadv.add8289](https://www.science.org/doi/abs/10.1126/sciadv.add8289)，doi：[10.1126/sciadv.add8289](http://dx.doi.org/10.1126/sciadv.add8289)。
- en: 'Hong et al. (2023) Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang,
    C., Wang, J., Wang, Z., Yau, S.K.S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu,
    C., Schmidhuber, J., 2023. Metagpt: Meta programming for a multi-agent collaborative
    framework. [arXiv:2308.00352](http://arxiv.org/abs/2308.00352).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人（2023）Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C.,
    Wang, J., Wang, Z., Yau, S.K.S., Lin, Z., Zhou, L., Ran, C., Xiao, L., Wu, C.,
    Schmidhuber, J., 2023. Metagpt：用于多代理协作框架的元编程。[arXiv:2308.00352](http://arxiv.org/abs/2308.00352)。
- en: Leibo et al. (2021) Leibo, J.Z., Duéñez-Guzmán, E., Vezhnevets, A.S., Agapiou,
    J.P., Sunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., Graepel,
    T., 2021. Scalable evaluation of multi-agent reinforcement learning with melting
    pot. [arXiv:2107.06857](http://arxiv.org/abs/2107.06857).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leibo 等人（2021）Leibo, J.Z., Duéñez-Guzmán, E., Vezhnevets, A.S., Agapiou, J.P.,
    Sunehag, P., Koster, R., Matyas, J., Beattie, C., Mordatch, I., Graepel, T., 2021.
    使用 Melting Pot 进行多代理强化学习的可扩展评估。[arXiv:2107.06857](http://arxiv.org/abs/2107.06857)。
- en: 'Leibo et al. (2017) Leibo, J.Z., Zambaldi, V., Lanctot, M., Marecki, J., Graepel,
    T., 2017. Multi-agent reinforcement learning in sequential social dilemmas, in:
    Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems,
    International Foundation for Autonomous Agents and Multiagent Systems, Richland,
    SC. p. 464–473.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leibo 等人（2017）Leibo, J.Z., Zambaldi, V., Lanctot, M., Marecki, J., Graepel,
    T., 2017. 在顺序社会困境中的多代理强化学习，收录于：第16届自主代理与多代理系统会议论文集，国际自主代理与多代理系统基金会，南卡罗来纳州理奇兰。第464-473页。
- en: 'Liu et al. (2023) Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy,
    R., Feng, Y., Chen, Z., Niebles, J.C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong,
    C., Savarese, S., 2023. Bolaa: Benchmarking and orchestrating llm-augmented autonomous
    agents. [arXiv:2308.05960](http://arxiv.org/abs/2308.05960).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人（2023）Liu, Z., Yao, W., Zhang, J., Xue, L., Heinecke, S., Murthy, R.,
    Feng, Y., Chen, Z., Niebles, J.C., Arpit, D., Xu, R., Mui, P., Wang, H., Xiong,
    C., Savarese, S., 2023. Bolaa: 基准测试与协调 LLM 增强的自主代理。[arXiv:2308.05960](http://arxiv.org/abs/2308.05960)。'
- en: McKee et al. (2023) McKee, K.R., Hughes, E., Zhu, T.O., Chadwick, M.J., Koster,
    R., Castaneda, A.G., Beattie, C., Graepel, T., Botvinick, M., Leibo, J.Z., 2023.
    A multi-agent reinforcement learning model of reputation and cooperation in human
    groups. [arXiv:2103.04982](http://arxiv.org/abs/2103.04982).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKee 等人（2023）McKee, K.R., Hughes, E., Zhu, T.O., Chadwick, M.J., Koster, R.,
    Castaneda, A.G., Beattie, C., Graepel, T., Botvinick, M., Leibo, J.Z., 2023. 一个多代理强化学习模型：人类群体中的声誉与合作。[arXiv:2103.04982](http://arxiv.org/abs/2103.04982)。
- en: 'Ni and Buehler (2024) Ni, B., Buehler, M.J., 2024. Mechagents: Large language
    model multi-agent collaborations can solve mechanics problems, generate new data,
    and integrate knowledge. Extreme Mechanics Letters 67, 102131. URL: [https://www.sciencedirect.com/science/article/pii/S2352431624000117](https://www.sciencedirect.com/science/article/pii/S2352431624000117),
    doi:[https://doi.org/10.1016/j.eml.2024.102131](http://dx.doi.org/https://doi.org/10.1016/j.eml.2024.102131).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ni 和 Buehler（2024）Ni, B., Buehler, M.J., 2024. Mechagents: 大型语言模型多代理协作能够解决力学问题、生成新数据并整合知识。极限力学快报
    67, 102131。网址：[https://www.sciencedirect.com/science/article/pii/S2352431624000117](https://www.sciencedirect.com/science/article/pii/S2352431624000117)，doi：[https://doi.org/10.1016/j.eml.2024.102131](http://dx.doi.org/https://doi.org/10.1016/j.eml.2024.102131)。'
- en: 'Park et al. (2023) Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang,
    P., Bernstein, M.S., 2023. Generative agents: Interactive simulacra of human behavior.
    [arXiv:2304.03442](http://arxiv.org/abs/2304.03442).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang, P.,
    Bernstein, M.S., 2023. 生成代理：人类行为的互动模拟。[arXiv:2304.03442](http://arxiv.org/abs/2304.03442)。
- en: 'Pennisi (2009) Pennisi, E., 2009. On the origin of cooperation. Science 325,
    1196–1199. URL: [https://www.science.org/doi/abs/10.1126/science.325.1196](https://www.science.org/doi/abs/10.1126/science.325.1196),
    doi:[10.1126/science.325.1196](http://dx.doi.org/10.1126/science.325.1196).'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pennisi（2009）Pennisi, E., 2009. 合作的起源。科学 325, 1196–1199。网址：[https://www.science.org/doi/abs/10.1126/science.325.1196](https://www.science.org/doi/abs/10.1126/science.325.1196)，doi：[10.1126/science.325.1196](http://dx.doi.org/10.1126/science.325.1196)。
- en: Rios et al. (2023) Rios, M., Quijano, N., Giraldo, L.F., 2023. Understanding
    the world to solve social dilemmas using multi-agent reinforcement learning. [arXiv:2305.11358](http://arxiv.org/abs/2305.11358).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rios 等人（2023）Rios, M., Quijano, N., Giraldo, L.F., 2023. 利用多智能体强化学习理解世界以解决社会困境。
    [arXiv:2305.11358](http://arxiv.org/abs/2305.11358)。
- en: 'Schick et al. (2023) Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., Scialom, T., 2023. Toolformer: Language models
    can teach themselves to use tools. [arXiv:2302.04761](http://arxiv.org/abs/2302.04761).'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2023）Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli,
    M., Zettlemoyer, L., Cancedda, N., Scialom, T., 2023. Toolformer: 语言模型可以自学使用工具。
    [arXiv:2302.04761](http://arxiv.org/abs/2302.04761)。'
- en: 'Shinn et al. (2023) Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan,
    K., Yao, S., 2023. Reflexion: Language agents with verbal reinforcement learning.
    [arXiv:2303.11366](http://arxiv.org/abs/2303.11366).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2023）Shinn, N., Cassano, F., Berman, E., Gopinath, A., Narasimhan,
    K., Yao, S., 2023. Reflexion: 具有语言强化学习的语言代理。 [arXiv:2303.11366](http://arxiv.org/abs/2303.11366)。'
- en: 'Wang et al. (2023) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., Anandkumar, A., 2023. Voyager: An open-ended embodied agent with
    large language models. [arXiv:2305.16291](http://arxiv.org/abs/2305.16291).'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023）Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y.,
    Fan, L., Anandkumar, A., 2023. Voyager: 具有大型语言模型的开放式具身代理。 [arXiv:2305.16291](http://arxiv.org/abs/2305.16291)。'
- en: 'Yao et al. (2023) Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
    K., Cao, Y., 2023. React: Synergizing reasoning and acting in language models.
    [arXiv:2210.03629](http://arxiv.org/abs/2210.03629).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人（2023）Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K.,
    Cao, Y., 2023. React: 协同推理与行动在语言模型中的作用。 [arXiv:2210.03629](http://arxiv.org/abs/2210.03629)。'
- en: 'Zhang et al. (2023) Zhang, J., Xu, X., Deng, S., 2023. Exploring collaboration
    mechanisms for llm agents: A social psychology view. [arXiv:2310.02124](http://arxiv.org/abs/2310.02124).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023）Zhang, J., Xu, X., Deng, S., 2023. 探索大型语言模型代理的协作机制：社会心理学视角。 [arXiv:2310.02124](http://arxiv.org/abs/2310.02124)。
- en: 'Zhou et al. (2024) Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.T., Le,
    Q.V., Chi, E.H., Zhou, D., Mishra, S., Zheng, H.S., 2024. Self-discover: Large
    language models self-compose reasoning structures. [arXiv:2402.03620](http://arxiv.org/abs/2402.03620).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2024）Zhou, P., Pujara, J., Ren, X., Chen, X., Cheng, H.T., Le, Q.V.,
    Chi, E.H., Zhou, D., Mishra, S., Zheng, H.S., 2024. Self-discover: 大型语言模型自我构建推理结构。
    [arXiv:2402.03620](http://arxiv.org/abs/2402.03620)。'
- en: Zhu et al. (2023) Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans,
    D., Dai, H., 2023. Large language models can learn rules. [arXiv:2310.07064](http://arxiv.org/abs/2310.07064).
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023）Zhu, Z., Xue, Y., Chen, X., Zhou, D., Tang, J., Schuurmans, D.,
    Dai, H., 2023. 大型语言模型可以学习规则。 [arXiv:2310.07064](http://arxiv.org/abs/2310.07064)。
