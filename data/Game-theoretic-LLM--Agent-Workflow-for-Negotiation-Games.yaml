- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2025-01-11 11:57:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 11:57:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Game-theoretic LLM: Agent Workflow for Negotiation Games'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 博弈论LLM：谈判游戏中的代理工作流
- en: 来源：[https://arxiv.org/html/2411.05990/](https://arxiv.org/html/2411.05990/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2411.05990/](https://arxiv.org/html/2411.05990/)
- en: '*[table]aboveskip=5pt, belowskip=5pt'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*[table]aboveskip=5pt, belowskip=5pt'
- en: Wenyue Hua^(1,4) Ollie Liu² Lingyao Li³ Alfonso Amayuelas⁴
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 华文越^(1,4) 刘奥力² 李凌尧³ 阿方索·阿马尤埃拉⁴
- en: Julie Chen⁵ Lucas Jiang⁵ Mingyu Jin¹ Lizhou Fan⁶
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 陈珠莉⁵ 姜路克⁵ 金明宇¹ 范丽舟⁶
- en: Fei Sun⁷ William Wang⁴ Xintong Wang¹ Yongfeng Zhang¹
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 孙飞⁷ 王威廉⁴ 王新同¹ 张永锋¹
- en: ¹Rutgers University, New Brunswick    ²University of Southern California
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹罗格斯大学，纽布朗斯维克    ²南加州大学
- en: ³University of South Florida ⁴University of California, Santa Barbara
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ³南佛罗里达大学 ⁴加利福尼亚大学圣巴巴拉分校
- en: '⁵Independent Researcher    ⁶Harvard University    ⁷Institute of Computing Technology
    Corresponding Email: wenyuehua@ucsb.edu, yongfeng.zhang@rutgers.edu. Much gratitude
    for extensive discussion with Shengwei Xu from Information School of University
    of Michigan on game theory.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵独立研究员    ⁶哈佛大学    ⁷计算技术研究所，通讯邮箱：wenyuehua@ucsb.edu，yongfeng.zhang@rutgers.edu。特别感谢密歇根大学信息学院徐胜伟教授就博弈论与我们进行的广泛讨论。
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper investigates the rationality of large language models (LLMs) in strategic
    decision-making contexts, specifically within the framework of game theory. We
    evaluate several state-of-the-art LLMs across a spectrum of complete-information
    and incomplete-information games. Our findings reveal that LLMs frequently deviate
    from rational strategies, particularly as the complexity of the game increases
    with larger payoff matrices or deeper sequential trees.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了大语言模型（LLM）在战略决策情境中的理性，特别是在博弈论框架下的应用。我们评估了几种最先进的LLM，在完全信息和不完全信息的博弈中进行测试。我们的发现表明，LLM经常偏离理性策略，尤其是当博弈的复杂性随着更大的收益矩阵或更深的顺序树增加时。
- en: To address these limitations, we design multiple game-theoretic workflows that
    guide the reasoning and decision-making processes of LLMs. These workflows aim
    to enhance the models’ ability to compute Nash Equilibria and make rational choices,
    even under conditions of uncertainty and incomplete information. Experimental
    results demonstrate that the adoption of these workflows significantly improves
    the rationality and robustness of LLMs in game-theoretic tasks. Specifically,
    with the workflow, LLMs exhibit marked improvements in identifying optimal strategies,
    achieving near-optimal allocations in negotiation scenarios, and reducing susceptibility
    to exploitation during negotiations. Furthermore, we explore the meta-strategic
    considerations of whether it is rational for agents to adopt such workflows, recognizing
    that the decision to use or forgo the workflow constitutes a game-theoretic issue
    in itself.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些局限性，我们设计了多种博弈论工作流，指导LLM的推理和决策过程。这些工作流旨在增强模型计算纳什均衡的能力，并在不确定性和不完全信息的情况下做出理性选择。实验结果表明，采用这些工作流显著提升了LLM在博弈论任务中的理性和鲁棒性。具体而言，通过工作流，LLM在识别最佳策略、在谈判情境中实现接近最优的分配以及减少谈判中的被利用风险方面表现出明显改善。此外，我们还探讨了采用这种工作流是否理性的元战略考量，认识到使用或放弃工作流本身也是一个博弈论问题。
- en: Our research contributes to a deeper understanding of LLMs’ decision-making
    capabilities in strategic contexts and provides insights into enhancing their
    rationality through structured workflows. The findings have implications for the
    development of more robust and strategically sound AI agents capable of navigating
    complex interactive environments. Code and data supporting this study are available
    at [https://github.com/Wenyueh/game_theory](https://github.com/Wenyueh/game_theory).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究有助于深入理解LLM在战略情境中的决策能力，并提供了通过结构化工作流提升其理性的方法。研究结果对开发更强大且具有战略眼光的AI代理具有重要意义，能够应对复杂的交互环境。支持本研究的代码和数据可以在[https://github.com/Wenyueh/game_theory](https://github.com/Wenyueh/game_theory)获得。
- en: Contents
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内容
- en: '[1 Introduction](https://arxiv.org/html/2411.05990v2#S1 "In Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](https://arxiv.org/html/2411.05990v2#S1 "在博弈论框架下的LLM：谈判游戏中的代理工作流")'
- en: '[2 Related Work](https://arxiv.org/html/2411.05990v2#S2 "In Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 相关工作](https://arxiv.org/html/2411.05990v2#S2 "在博弈论框架下的LLM：谈判游戏中的代理工作流")'
- en: '[3 Preliminary for Game Theory](https://arxiv.org/html/2411.05990v2#S3 "In
    Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 博弈论基础](https://arxiv.org/html/2411.05990v2#S3 "在博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4 Complete-information Games](https://arxiv.org/html/2411.05990v2#S4 "In Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 完全信息博弈](https://arxiv.org/html/2411.05990v2#S4 "在博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.1 Introduction to Complete-information Games TestBed](https://arxiv.org/html/2411.05990v2#S4.SS1
    "In 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 完全信息博弈测试平台简介](https://arxiv.org/html/2411.05990v2#S4.SS1 "在 4 完全信息博弈 ‣
    博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.2 Experiment Setting](https://arxiv.org/html/2411.05990v2#S4.SS2 "In 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 实验设置](https://arxiv.org/html/2411.05990v2#S4.SS2 "在 4 完全信息博弈 ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.3 Evaluation on LLM’s performance](https://arxiv.org/html/2411.05990v2#S4.SS3
    "In 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.3 对 LLM 性能的评估](https://arxiv.org/html/2411.05990v2#S4.SS3 "在 4 完全信息博弈 ‣
    博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.4 Workflow Design based on Classic Game Theory](https://arxiv.org/html/2411.05990v2#S4.SS4
    "In 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4 基于经典博弈论的工作流设计](https://arxiv.org/html/2411.05990v2#S4.SS4 "在 4 完全信息博弈
    ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.4.1 Workflow for Simultaneous Game](https://arxiv.org/html/2411.05990v2#S4.SS4.SSS1
    "In 4.4 Workflow Design based on Classic Game Theory ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.1 同时博弈的工作流](https://arxiv.org/html/2411.05990v2#S4.SS4.SSS1 "在 4.4 基于经典博弈论的工作流设计
    ‣ 4 完全信息博弈 ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.4.2 Workflow for Sequential Game](https://arxiv.org/html/2411.05990v2#S4.SS4.SSS2
    "In 4.4 Workflow Design based on Classic Game Theory ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.4.2 顺序博弈的工作流](https://arxiv.org/html/2411.05990v2#S4.SS4.SSS2 "在 4.4 基于经典博弈论的工作流设计
    ‣ 4 完全信息博弈 ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[4.5 Experiments for Classic Game Theory with Workflow](https://arxiv.org/html/2411.05990v2#S4.SS5
    "In 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.5 基于工作流的经典博弈实验](https://arxiv.org/html/2411.05990v2#S4.SS5 "在 4 完全信息博弈 ‣
    博弈论 LLM：谈判博弈的代理工作流")'
- en: '[5 Incomplete-information Game with Negotiation](https://arxiv.org/html/2411.05990v2#S5
    "In Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 不完全信息博弈与谈判](https://arxiv.org/html/2411.05990v2#S5 "在博弈论 LLM：谈判博弈的代理工作流")'
- en: '[5.1 Introduction to Common Resource Allocation with Private Valuation](https://arxiv.org/html/2411.05990v2#S5.SS1
    "In 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games")'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.1 具有私人估值的公共资源分配简介](https://arxiv.org/html/2411.05990v2#S5.SS1 "在 5 不完全信息博弈与谈判
    ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[5.2 Workflow Design](https://arxiv.org/html/2411.05990v2#S5.SS2 "In 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.2 工作流设计](https://arxiv.org/html/2411.05990v2#S5.SS2 "在 5 不完全信息博弈与谈判 ‣ 博弈论
    LLM：谈判博弈的代理工作流")'
- en: '[5.3 Introduction to “Deal or No Deal”](https://arxiv.org/html/2411.05990v2#S5.SS3
    "In 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games")'
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.3 “敢死队还是不敢死队”简介](https://arxiv.org/html/2411.05990v2#S5.SS3 "在 5 不完全信息博弈与谈判
    ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[5.4 Experiment Setting](https://arxiv.org/html/2411.05990v2#S5.SS4 "In 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.4 实验设置](https://arxiv.org/html/2411.05990v2#S5.SS4 "在 5 不完全信息博弈与谈判中 ‣ 博弈论
    LLM：谈判博弈的代理工作流")'
- en: '[5.5 Experiment Result](https://arxiv.org/html/2411.05990v2#S5.SS5 "In 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5 实验结果](https://arxiv.org/html/2411.05990v2#S5.SS5 "在 5 不完全信息博弈与谈判中 ‣ 博弈论
    LLM：谈判博弈的代理工作流")'
- en: '[5.5.1 Both Agents without Workflow](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS1
    "In 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5.1 无工作流的双方代理](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS1 "在 5.5 实验结果
    ‣ 5 不完全信息博弈与谈判 ‣ 博弈论 LLM：谈判博弈的代理工作流")'
- en: '[5.5.2 Both Agents with Workflow](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS2
    "In 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5.2 双方智能体与工作流](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS2 "在5.5 实验结果
    ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[5.5.3 One Agent with Workflow](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3
    "In 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.5.3 单一智能体与工作流](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3 "在5.5 实验结果
    ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[5.6 To adopt the workflow or not?](https://arxiv.org/html/2411.05990v2#S5.SS6
    "In 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games")'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.6 是否采用工作流？](https://arxiv.org/html/2411.05990v2#S5.SS6 "在5 不完全信息博弈与谈判 ‣
    博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[5.6.1 Comparison and Implications](https://arxiv.org/html/2411.05990v2#S5.SS6.SSS1
    "In 5.6 To adopt the workflow or not? ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5.6.1 比较与启示](https://arxiv.org/html/2411.05990v2#S5.SS6.SSS1 "在5.6 是否采用工作流？
    ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6 Detailed Observation on LLM’s Rationality](https://arxiv.org/html/2411.05990v2#S6
    "In Game-theoretic LLM: Agent Workflow for Negotiation Games")'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 对LLM理性的详细观察](https://arxiv.org/html/2411.05990v2#S6 "在博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.1 Experiment Setup](https://arxiv.org/html/2411.05990v2#S6.SS1 "In 6 Detailed
    Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.1 实验设置](https://arxiv.org/html/2411.05990v2#S6.SS1 "在6 对LLM理性的详细观察 ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.2 Variance of Payoff Matrix](https://arxiv.org/html/2411.05990v2#S6.SS2
    "In 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.2 回报矩阵的方差](https://arxiv.org/html/2411.05990v2#S6.SS2 "在6 对LLM理性的详细观察 ‣
    博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.3 Variation of Personality](https://arxiv.org/html/2411.05990v2#S6.SS3 "In
    6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.3 个性变化](https://arxiv.org/html/2411.05990v2#S6.SS3 "在6 对LLM理性的详细观察 ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.4 Does negotiation affect rationality?](https://arxiv.org/html/2411.05990v2#S6.SS4
    "In 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.4 谈判是否影响理性？](https://arxiv.org/html/2411.05990v2#S6.SS4 "在6 对LLM理性的详细观察
    ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.5 How does prompt affect the influence of negotiation?](https://arxiv.org/html/2411.05990v2#S6.SS5
    "In 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.5 提示如何影响谈判的影响力？](https://arxiv.org/html/2411.05990v2#S6.SS5 "在6 对LLM理性的详细观察
    ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.6 How does the order of negotiation message affect the action?](https://arxiv.org/html/2411.05990v2#S6.SS6
    "In 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.6 谈判信息顺序如何影响行动？](https://arxiv.org/html/2411.05990v2#S6.SS6 "在6 对LLM理性的详细观察
    ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[6.7 Irrationality Compared with Humans](https://arxiv.org/html/2411.05990v2#S6.SS7
    "In 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6.7 与人类的非理性对比](https://arxiv.org/html/2411.05990v2#S6.SS7 "在6 对LLM理性的详细观察
    ‣ 博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[7 Conclusion](https://arxiv.org/html/2411.05990v2#S7 "In Game-theoretic LLM:
    Agent Workflow for Negotiation Games")'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 结论](https://arxiv.org/html/2411.05990v2#S7 "在博弈论LLM：谈判博弈中的智能体工作流")'
- en: '[8 Future Directions](https://arxiv.org/html/2411.05990v2#S8 "In Game-theoretic
    LLM: Agent Workflow for Negotiation Games")'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[8 未来方向](https://arxiv.org/html/2411.05990v2#S8 "在博弈论LLM：谈判博弈中的智能体工作流")'
- en: 'Note: Sections 4, 5, and 6 are independent of one another and can be read in
    any order. Each section is self-contained and does not require prior knowledge
    of the others to be fully understood.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：第4、5和6节相互独立，可以任意顺序阅读。每节内容都是自成体系的，理解时不需要依赖其他章节的知识。
- en: 1 Introduction
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs), such as GPT-4 and Claude, have achieved remarkable
    progress in natural language understanding and generation [zhang2024supervised](https://arxiv.org/html/2411.05990v2#bib.bib1)
    ; [ding2024hybrid](https://arxiv.org/html/2411.05990v2#bib.bib2) ; [fang2024large](https://arxiv.org/html/2411.05990v2#bib.bib3)
    , driving advancements in fields ranging from conversational AI [dam2024complete](https://arxiv.org/html/2411.05990v2#bib.bib4)
    ; [dong2023towards](https://arxiv.org/html/2411.05990v2#bib.bib5) to content creation
    [liang2024monitoring](https://arxiv.org/html/2411.05990v2#bib.bib6) ; [shao2024assisting](https://arxiv.org/html/2411.05990v2#bib.bib7)
    and agentic task delegation [guo2024embodied](https://arxiv.org/html/2411.05990v2#bib.bib8)
    ; [agashe2023evaluating](https://arxiv.org/html/2411.05990v2#bib.bib9) ; [xi2023rise](https://arxiv.org/html/2411.05990v2#bib.bib10)
    . LLMs are increasingly integrated into applications that influence everyday activities,
    such as planning, acting, and decision-making. Therefore, the ability of LLMs
    to navigate complex situations has significant implications for their deployment
    in applications requiring strategic interaction, such as automated negotiations,
    economic modeling, and collaborative problem-solving [bianchi2024well](https://arxiv.org/html/2411.05990v2#bib.bib11)
    ; [horton2023large](https://arxiv.org/html/2411.05990v2#bib.bib12) ; [li2024econagent](https://arxiv.org/html/2411.05990v2#bib.bib13)
    ; [chen2024comm](https://arxiv.org/html/2411.05990v2#bib.bib14) ; [li2023metaagents](https://arxiv.org/html/2411.05990v2#bib.bib15)
    .
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如GPT-4和Claude，在自然语言理解和生成方面取得了显著进展 [zhang2024supervised](https://arxiv.org/html/2411.05990v2#bib.bib1)；[ding2024hybrid](https://arxiv.org/html/2411.05990v2#bib.bib2)；[fang2024large](https://arxiv.org/html/2411.05990v2#bib.bib3)，推动了从对话式AI
    [dam2024complete](https://arxiv.org/html/2411.05990v2#bib.bib4)；[dong2023towards](https://arxiv.org/html/2411.05990v2#bib.bib5)
    到内容创作 [liang2024monitoring](https://arxiv.org/html/2411.05990v2#bib.bib6)；[shao2024assisting](https://arxiv.org/html/2411.05990v2#bib.bib7)
    以及代理任务委派 [guo2024embodied](https://arxiv.org/html/2411.05990v2#bib.bib8)；[agashe2023evaluating](https://arxiv.org/html/2411.05990v2#bib.bib9)；[xi2023rise](https://arxiv.org/html/2411.05990v2#bib.bib10)
    等领域的进展。因此，LLM在复杂情境中的应对能力对其在需要战略互动的应用中的部署具有重要意义，例如自动化谈判、经济建模和协作问题解决 [bianchi2024well](https://arxiv.org/html/2411.05990v2#bib.bib11)；[horton2023large](https://arxiv.org/html/2411.05990v2#bib.bib12)；[li2024econagent](https://arxiv.org/html/2411.05990v2#bib.bib13)；[chen2024comm](https://arxiv.org/html/2411.05990v2#bib.bib14)；[li2023metaagents](https://arxiv.org/html/2411.05990v2#bib.bib15)。
- en: Despite the wide exploration and utilization, LLM’s capacity for rational behavior,
    particularly in strategic settings represented by game theory, remains an open
    question [leng2023llm](https://arxiv.org/html/2411.05990v2#bib.bib16) ; [stade2024large](https://arxiv.org/html/2411.05990v2#bib.bib17)
    ; [wu2024shall](https://arxiv.org/html/2411.05990v2#bib.bib18) ; [de2023emergent](https://arxiv.org/html/2411.05990v2#bib.bib19)
    ; [lan2023llm](https://arxiv.org/html/2411.05990v2#bib.bib20) . In this context,
    rationality implies an agent’s ability to make decisions that maximize expected
    utility based on available information, an essential component of intelligent
    and adaptive decision-making. In the realm of game theory, rational agents are
    expected to act strategically, considering not only their own preferences but
    also the potential actions and preferences of others. This is especially critical
    in incomplete-information games, where uncertainty about other players’ information
    necessitates sophisticated reasoning and belief updating.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM被广泛探索和利用，但LLM在理性行为方面的能力，特别是在博弈论所代表的战略环境中，仍然是一个悬而未决的问题 [leng2023llm](https://arxiv.org/html/2411.05990v2#bib.bib16)；[stade2024large](https://arxiv.org/html/2411.05990v2#bib.bib17)；[wu2024shall](https://arxiv.org/html/2411.05990v2#bib.bib18)；[de2023emergent](https://arxiv.org/html/2411.05990v2#bib.bib19)；[lan2023llm](https://arxiv.org/html/2411.05990v2#bib.bib20)。在这种背景下，理性意味着代理能够基于可用信息做出最大化期望效用的决策，这是智能和适应性决策的重要组成部分。在博弈论领域，理性代理被期望采取战略行动，不仅考虑自身偏好，还要考虑他人的潜在行为和偏好。在信息不完全的博弈中，关于其他玩家信息的不确定性要求代理进行复杂的推理和信念更新，这一点尤其重要。
- en: 'This paper investigates the capacity of LLMs to behave rationally in game-theoretic
    scenarios and explores methodologies to enhance their rational decision-making
    capabilities. We begin by assessing the performance of several state-of-the-art
    LLMs, including Claude-3.5 Sonnet, Claude-3 Opus, GPT-4o and o1 [zhong2024evaluation](https://arxiv.org/html/2411.05990v2#bib.bib21)
    , in both complete-information and incomplete-information games such as the Prisoner’s
    Dilemma, Battle of the Sexes, the Escalation Game, and Deal-or-No-Deal [lewis2017deal](https://arxiv.org/html/2411.05990v2#bib.bib22)
    , presented in Figure [1](https://arxiv.org/html/2411.05990v2#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games").
    Our analysis reveals LLMs often deviate from rational strategies, particularly
    as the complexity of the game increases with larger payoff matrices or deeper
    sequential trees (Section [4](https://arxiv.org/html/2411.05990v2#S4 "4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")). They also
    exhibit a lack of robustness to noise and uncertainty, leading to suboptimal outcomes
    (Section [6](https://arxiv.org/html/2411.05990v2#S6 "6 Detailed Observation on
    LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '本文探讨了大型语言模型（LLMs）在博弈论场景中理性行为的能力，并探索了增强其理性决策能力的方法。我们首先评估了几种最先进的LLM的表现，包括Claude-3.5
    Sonnet、Claude-3 Opus、GPT-4o和o1 [zhong2024evaluation](https://arxiv.org/html/2411.05990v2#bib.bib21)，在完整信息和不完全信息博弈中的表现，例如囚徒困境、性别之战、升级博弈和《一掷千金》[lewis2017deal](https://arxiv.org/html/2411.05990v2#bib.bib22)，如图[1](https://arxiv.org/html/2411.05990v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")所示。我们的分析发现，LLMs常常偏离理性策略，尤其是在博弈复杂性随着较大的收益矩阵或更深的顺序树的增加而增加时（第[4](https://arxiv.org/html/2411.05990v2#S4
    "4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")节）。它们还表现出对噪声和不确定性的鲁棒性不足，导致次优的结果（第[6](https://arxiv.org/html/2411.05990v2#S6
    "6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")节）。'
- en: To address these limitations, we introduce a novel approach by proposing game-theory-inspired
    workflows specifically designed to guide the reasoning and decision-making processes
    of LLMs. This is the first attempt to systematically integrate classic game-theoretic
    strategies into LLM-based agent workflow, aiming to enhance their rational behavior
    and decision-making capabilities in strategic settings. These workflows incorporate
    principles such as Dominant Strategy Search, which involves identifying strategies
    that yield the highest payoff regardless of the opponent’s actions; Backward Induction,
    a method of solving extensive-form games by analyzing them from the end states
    backward to the initial decision nodes to determine optimal strategies; and Bayesian
    belief updating, which allows agents to refine their beliefs about other players’
    valuations based on observed actions and signals during the game. Cringed on these
    well-defined and well-studied game-theoretic methods, we design algorithms to
    guide the behavior and thinking process of LLM-based agents.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，我们提出了一种新的方法，提出了受博弈论启发的工作流，专门设计用于引导LLM的推理和决策过程。这是首次尝试系统地将经典博弈论策略集成到基于LLM的代理工作流中，旨在增强它们在战略环境中的理性行为和决策能力。这些工作流结合了诸如**主导策略搜索**等原则，主导策略搜索涉及识别无论对手的行动如何都能获得最高收益的策略；**逆向归纳法**，一种通过从终态逆向分析扩展形式博弈，从初始决策节点确定最优策略的方法；以及**贝叶斯信念更新**，它允许代理根据博弈中观察到的行动和信号，优化对其他玩家估值的信念。基于这些经过精确定义和深入研究的博弈论方法，我们设计了算法来引导基于LLM的代理的行为和思维过程。
- en: Additionally, we integrate fairness considerations like envy freeness and pareto
    optimality, which promote equitable and efficient outcomes in negotiations by
    ensuring that no agent prefers another agent’s allocation to their own and that
    no improvements can be made without making at least one agent worse off.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还融合了公平性考虑，如**无嫉妒性**和**帕累托最优性**，这些原则通过确保没有代理更喜欢另一个代理的分配而非自己的分配，并且在没有使至少一个代理变得更糟的情况下无法做出改进，来促进谈判中的公正和高效的结果。
- en: '<svg class="ltx_picture" height="196.22" id="S1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,196.22) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 178.01)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Contribution Summary</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="146.52" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">• Comprehensive Evaluation of LLMs in Strategic Games
    and Identification of Rationality Limitations in LLMs (Section [4](https://arxiv.org/html/2411.05990v2#S4
    "4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games") and [6](https://arxiv.org/html/2411.05990v2#S6 "6 Detailed Observation
    on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")):
    Through empirical analysis, we uncover that LLMs often fail to behave rationally
    in strategic settings, exhibiting a lack of robustness to noise and randomness.
    • Design of Game-Theory-Inspired Workflows (Section [4.4](https://arxiv.org/html/2411.05990v2#S4.SS4
    "4.4 Workflow Design based on Classic Game Theory ‣ 4 Complete-information Games
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") and [5.2](https://arxiv.org/html/2411.05990v2#S5.SS2
    "5.2 Workflow Design ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")): We develop novel workflows inspired
    by game-theoretic concepts to guide the reasoning and decision-making processes
    of LLMs, incorporating analysis and algorithms from classic game theory. • Emerging
    Research Direction (Section [5.5.3](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3
    "5.5.3 One Agent with Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    and [5.6](https://arxiv.org/html/2411.05990v2#S5.SS6 "5.6 To adopt the workflow
    or not? ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games")): Through the application of workflows,
    we identify a promising new research direction in meta-strategy, specifically
    focusing on the decision of whether to adopt a workflow and, potentially, which
    workflow to employ in varying scenarios.</foreignobject></g></g></svg>![Refer
    to caption](img/2a825595d24a5d6bc212a52aa5e6aa45.png)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="196.22" id="S1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,196.22) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 178.01)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">贡献摘要</foreignobject></g> <g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="146.52" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">• 对战略游戏中LLMs的全面评估及其理性局限性的识别（第[4](https://arxiv.org/html/2411.05990v2#S4
    "4 完全信息博弈 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")和[6](https://arxiv.org/html/2411.05990v2#S6
    "6 LLM理性的详细观察 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")节）：通过实证分析，我们发现LLMs在战略环境中常常无法表现出理性，且对噪声和随机性的鲁棒性较差。•
    博弈论启发的工作流程设计（第[4.4](https://arxiv.org/html/2411.05990v2#S4.SS4 "4.4 基于经典博弈论的工作流程设计
    ‣ 4 完全信息博弈 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")和[5.2](https://arxiv.org/html/2411.05990v2#S5.SS2
    "5.2 工作流程设计 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")节）：我们设计了受博弈论启发的新型工作流程，指导LLMs的推理和决策过程，融入了经典博弈论中的分析和算法。•
    新兴研究方向（第[5.5.3](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3 "5.5.3 一个智能体的工作流程
    ‣ 5.5 实验结果 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")和[5.6](https://arxiv.org/html/2411.05990v2#S5.SS6
    "5.6 是否采用工作流程？ ‣ 5 不完全信息博弈与谈判 ‣ 博弈论大语言模型：谈判博弈中的智能体工作流程")节）：通过工作流程的应用，我们发现了一个有前景的新研究方向——元策略，特别是关注是否采用工作流程的决策，以及在不同场景中可能采用哪个工作流程。</foreignobject></g></g></svg>![请参阅说明](img/2a825595d24a5d6bc212a52aa5e6aa45.png)
- en: 'Figure 1: Game-theoretic Landscape Investigated in this Paper.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：本文探讨的博弈论景观。
- en: 2 Related Work
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLMs in game-theoretic environments
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 博弈论环境中的大语言模型（LLMs）
- en: Understanding strategic behaviors of LLMs entails important societal ramifications,
    as online users increasingly rely on intelligent assistants to interact with other
    agents, potentially also LLMs. To characterize LLM’s behaviors, prior studies
    ([gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23) ; [sreedhar2024simulating,](https://arxiv.org/html/2411.05990v2#bib.bib24)
    ; [de2023emergent,](https://arxiv.org/html/2411.05990v2#bib.bib19) ; [wu2024shall,](https://arxiv.org/html/2411.05990v2#bib.bib18)
    ; [akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25) ; [fan2024can,](https://arxiv.org/html/2411.05990v2#bib.bib26)
    ; [mao2023alympics,](https://arxiv.org/html/2411.05990v2#bib.bib27) ; [guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)
    ) often adopt game theory, a mathematical framework that models cooperative behaviors
    of humans. These analyses involve comparing qualitative behaviors of LLM interactions
    against stylized entities, such as pareto optimal solutions and subgame-perfect
    equilibria ([fudenberg1991game,](https://arxiv.org/html/2411.05990v2#bib.bib29)
    ). LLM research in game-theoretic environments belongs to a growing body of work
    on multi-agent LLMs ([li2023theory,](https://arxiv.org/html/2411.05990v2#bib.bib30)
    ) and their evaluation ([huang2024far,](https://arxiv.org/html/2411.05990v2#bib.bib31)
    ; [duan2024gtbench,](https://arxiv.org/html/2411.05990v2#bib.bib32) ). In particular,
    AvalonBench ([wang2023avalon,](https://arxiv.org/html/2411.05990v2#bib.bib33)
    ) serves as a valuable platform for developing new multi-agent strategies. ([guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)
    ; [park2024ai,](https://arxiv.org/html/2411.05990v2#bib.bib34) ) observe LLMs
    to perform in games driven by self-interest, but falter in those that require
    coordination, a behavior that emerges under altruistic and/or submissive personalities
    ([akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25) ; [guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)
    ). ([lore2023strategic,](https://arxiv.org/html/2411.05990v2#bib.bib35) ; [coda2024cogbench,](https://arxiv.org/html/2411.05990v2#bib.bib36)
    ) observe different LLM families to exhibit varying levels of risk tolerance.
    Deferring details to experiments, we identify another source of brittleness as
    LLMs behave poorly when presented with numerically perturbed payoffs, even if
    they do not alter qualitative solutions to the game of interest. In short, there
    lacks a system that elicits optimal behaviors for LLMs in game-theoretic settings.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理解LLMs的战略行为具有重要的社会影响，因为在线用户越来越依赖智能助手与其他代理进行互动，潜在地也包括LLMs。为了刻画LLM的行为，先前的研究（[gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23)；[sreedhar2024simulating,](https://arxiv.org/html/2411.05990v2#bib.bib24)；[de2023emergent,](https://arxiv.org/html/2411.05990v2#bib.bib19)；[wu2024shall,](https://arxiv.org/html/2411.05990v2#bib.bib18)；[akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25)；[fan2024can,](https://arxiv.org/html/2411.05990v2#bib.bib26)；[mao2023alympics,](https://arxiv.org/html/2411.05990v2#bib.bib27)；[guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)）通常采用博弈论，这是一种建模人类合作行为的数学框架。这些分析包括将LLM交互的定性行为与一些经典实体进行比较，例如帕累托最优解和子博弈完美均衡（[fudenberg1991game,](https://arxiv.org/html/2411.05990v2#bib.bib29)）。博弈论环境中的LLM研究属于多代理LLMs（[li2023theory,](https://arxiv.org/html/2411.05990v2#bib.bib30)）及其评估（[huang2024far,](https://arxiv.org/html/2411.05990v2#bib.bib31)；[duan2024gtbench,](https://arxiv.org/html/2411.05990v2#bib.bib32)）的不断增长的研究成果之一。特别是，AvalonBench（[wang2023avalon,](https://arxiv.org/html/2411.05990v2#bib.bib33)）作为一个开发新型多代理策略的宝贵平台。（[guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)；[park2024ai,](https://arxiv.org/html/2411.05990v2#bib.bib34)）观察到LLMs在自利驱动的游戏中表现良好，但在需要协调的游戏中则会失败，这种行为在利他或/和顺从型人格下表现出来（[akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25)；[guo2023gpt,](https://arxiv.org/html/2411.05990v2#bib.bib28)）。([lore2023strategic,](https://arxiv.org/html/2411.05990v2#bib.bib35)；[coda2024cogbench,](https://arxiv.org/html/2411.05990v2#bib.bib36)）观察到不同LLM家族在风险容忍度方面表现不同。将细节留给实验，我们发现LLMs在面对数值扰动的回报时表现不佳，即便这些扰动并不改变所关注游戏的定性解。简而言之，目前缺乏一个能够引导LLMs在博弈论环境中展现最佳行为的系统。
- en: Enhancing LLMs to solve games
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增强LLMs解决游戏的能力
- en: ([gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23) ) is a representative
    approach that aims to elicit game-solving capabilities in LLMs. It models natural
    dialogues as incomplete-information games, and synthesizes optimal actions by
    instructing an LLM to respond with specific personalities. ([guo2024can,](https://arxiv.org/html/2411.05990v2#bib.bib37)
    ) proposes an LLM-based self-play algorithm that emulates Monte-Carlo Tree Search
    to solve zero-sum games. Broadly, these methods belong to a family of prompting
    strategies [wei2022chain](https://arxiv.org/html/2411.05990v2#bib.bib38) ; [yao2023tree](https://arxiv.org/html/2411.05990v2#bib.bib39)
    ; [liu2024dellma](https://arxiv.org/html/2411.05990v2#bib.bib40) that tackle decision
    making instances. Our method is no exception, but differs as we imbue classical
    game theory in LLMs to allow for fine-grained control and analyses at each information
    state.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ([gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23)) 是一种代表性的方法，旨在揭示大型语言模型（LLM）的博弈求解能力。它将自然对话建模为不完全信息博弈，并通过指导LLM以特定人格做出回应来合成最优动作。([guo2024can,](https://arxiv.org/html/2411.05990v2#bib.bib37))
    提出了一个基于LLM的自我博弈算法，通过模拟蒙特卡洛树搜索来解决零和博弈。总体而言，这些方法属于一类提示策略 [wei2022chain](https://arxiv.org/html/2411.05990v2#bib.bib38)；[yao2023tree](https://arxiv.org/html/2411.05990v2#bib.bib39)；[liu2024dellma](https://arxiv.org/html/2411.05990v2#bib.bib40)，用于解决决策问题。我们的方法也不例外，但与之不同的是，我们将经典博弈论融入LLM中，以便在每个信息状态下实现更细粒度的控制和分析。
- en: Game-theoretic testbeds
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 博弈论测试平台
- en: Stylized games such as Battle of Sexes, Prisoner’s Dilemma, Rock-Paper-Scissors,
    Stag Hunt, and Ultimatum Game have been extensively analyzed in the context of
    multi-agent systems ([sreedhar2024simulating,](https://arxiv.org/html/2411.05990v2#bib.bib24)
    ; [akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25) ; [fan2024can,](https://arxiv.org/html/2411.05990v2#bib.bib26)
    ; [lore2023strategic,](https://arxiv.org/html/2411.05990v2#bib.bib35) ). They
    represent a minimal setting characterized by small action spaces, limited number
    of terms, and the existence of analytical equilibria; yet they cannot capture
    complex interactions of real-world, multi-agent dialogues. Games that emulate
    real interactions are more challenging to analyze, but practically useful. Exemplar
    games in this category include Deal-or-No-Deal ([lewis2017deal,](https://arxiv.org/html/2411.05990v2#bib.bib22)
    ), Multi-Round Auction ([mao2023alympics,](https://arxiv.org/html/2411.05990v2#bib.bib27)
    ), Schedule-a-Meeting, Trading Fruit, Public Debate ([gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23)
    ), Avalon ([wang2023avalon,](https://arxiv.org/html/2411.05990v2#bib.bib33) ),
    Pokémon ([hu2024pok,](https://arxiv.org/html/2411.05990v2#bib.bib41) ), Chess
    ([guo2024can,](https://arxiv.org/html/2411.05990v2#bib.bib37) ) and Bargaining
    ([xia2024measuring,](https://arxiv.org/html/2411.05990v2#bib.bib42) ). And there
    are efforts to collect multiple games and evaluate them ([duan2024gtbench,](https://arxiv.org/html/2411.05990v2#bib.bib32)
    ). They often involve intractable action spaces (e.g. natural language) and do
    not attain analytical solutions; but they are nevertheless endowed with a well-defined
    payoff function for practitioners to analyze the optimality of their strategy,
    albeit in an end-to-end manner.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如性别之战、囚徒困境、剪刀石头布、鹿猎、和最后通牒游戏等风格化博弈，已经在多智能体系统的背景下进行了广泛分析 ([sreedhar2024simulating,](https://arxiv.org/html/2411.05990v2#bib.bib24)；[akata2023playing,](https://arxiv.org/html/2411.05990v2#bib.bib25)；[fan2024can,](https://arxiv.org/html/2411.05990v2#bib.bib26)；[lore2023strategic,](https://arxiv.org/html/2411.05990v2#bib.bib35))。它们代表了一种最简化的设置，特点是小规模的动作空间、有限的术语数量以及存在分析解的均衡；然而，它们无法捕捉现实世界多智能体对话中的复杂互动。模拟现实互动的博弈更加难以分析，但在实践中具有重要意义。此类博弈的典型示例包括《交易还是不交易》（[lewis2017deal,](https://arxiv.org/html/2411.05990v2#bib.bib22)）、多轮拍卖（[mao2023alympics,](https://arxiv.org/html/2411.05990v2#bib.bib27)）、安排会议、交易水果、公共辩论（[gemp2024states,](https://arxiv.org/html/2411.05990v2#bib.bib23)）、《阿瓦隆》（[wang2023avalon,](https://arxiv.org/html/2411.05990v2#bib.bib33)）、《宝可梦》（[hu2024pok,](https://arxiv.org/html/2411.05990v2#bib.bib41)）、国际象棋（[guo2024can,](https://arxiv.org/html/2411.05990v2#bib.bib37)）和议价（[xia2024measuring,](https://arxiv.org/html/2411.05990v2#bib.bib42)）。同时也有收集多个博弈并进行评估的工作（[duan2024gtbench,](https://arxiv.org/html/2411.05990v2#bib.bib32)）。这些博弈通常涉及无法处理的动作空间（例如自然语言），并且无法获得解析解；但它们仍然具备明确的收益函数，供实践者分析其策略的最优性，尽管这种分析方式是端到端的。
- en: Workflow-aided LLM-based agent
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 工作流辅助的LLM代理
- en: LLMs have been extensively utilized to decompose user requests and tasks, formulate
    plans, and employ various tools to execute actions [ge2023openagi](https://arxiv.org/html/2411.05990v2#bib.bib43)
    ; [wu2023autogen](https://arxiv.org/html/2411.05990v2#bib.bib44) ; [li2023camel](https://arxiv.org/html/2411.05990v2#bib.bib45)
    ; langc; [topsakal2023creating](https://arxiv.org/html/2411.05990v2#bib.bib46)
    ; [xi2023rise](https://arxiv.org/html/2411.05990v2#bib.bib10) . This reliance
    on the innate capabilities of LLMs has propelled advancements in domains such
    as natural language understanding, automated reasoning, and task automation. However,
    depending solely on the autonomous abilities of LLMs has revealed several notable
    limitations, including suboptimal performance [xie2024travelplanner](https://arxiv.org/html/2411.05990v2#bib.bib47)
    ; [xia2024agentless](https://arxiv.org/html/2411.05990v2#bib.bib48) , lack of
    reliability due to output randomness [inglecomprehensive](https://arxiv.org/html/2411.05990v2#bib.bib49)
    ; [li2024survey](https://arxiv.org/html/2411.05990v2#bib.bib50) ; [schwartz2023enhancing](https://arxiv.org/html/2411.05990v2#bib.bib51)
    , and the propagation of errors across sequential reasoning steps [yu2023thought](https://arxiv.org/html/2411.05990v2#bib.bib52)
    .
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLM（大型语言模型）已广泛应用于分解用户请求和任务、制定计划，并利用各种工具执行操作 [ge2023openagi](https://arxiv.org/html/2411.05990v2#bib.bib43)；[wu2023autogen](https://arxiv.org/html/2411.05990v2#bib.bib44)；[li2023camel](https://arxiv.org/html/2411.05990v2#bib.bib45)；langc；[topsakal2023creating](https://arxiv.org/html/2411.05990v2#bib.bib46)；[xi2023rise](https://arxiv.org/html/2411.05990v2#bib.bib10)。这种对LLM固有能力的依赖推动了自然语言理解、自动推理和任务自动化等领域的进展。然而，单纯依赖LLM的自主能力也暴露了若干显著的局限性，包括表现不佳
    [xie2024travelplanner](https://arxiv.org/html/2411.05990v2#bib.bib47)；[xia2024agentless](https://arxiv.org/html/2411.05990v2#bib.bib48)，由于输出的随机性导致可靠性不足
    [inglecomprehensive](https://arxiv.org/html/2411.05990v2#bib.bib49)；[li2024survey](https://arxiv.org/html/2411.05990v2#bib.bib50)；[schwartz2023enhancing](https://arxiv.org/html/2411.05990v2#bib.bib51)，以及错误在连续推理步骤中的传播
    [yu2023thought](https://arxiv.org/html/2411.05990v2#bib.bib52)。
- en: To address these challenges, the concept of incorporating agentic workflows
    [wu2024stateflow](https://arxiv.org/html/2411.05990v2#bib.bib53) ; [zeng2023flowmind](https://arxiv.org/html/2411.05990v2#bib.bib54)
    ; [xiao2024flowbench](https://arxiv.org/html/2411.05990v2#bib.bib55) ; [li2024autoflow](https://arxiv.org/html/2411.05990v2#bib.bib56)
    ; [li2023metaagents](https://arxiv.org/html/2411.05990v2#bib.bib15) into LLM-based
    agents has emerged. Rather than allowing LLMs to independently decompose tasks
    and plan actions, workflows leverage human expertise and established knowledge
    frameworks to guide the direction and planning processes of the LLMs. Workflow-based
    agents have great potential to achieve high performance and adaptability in game
    scenarios ([xi2023rise,](https://arxiv.org/html/2411.05990v2#bib.bib10) ). There
    have been multiple agents presented for real-world tasks ([jimenez2023swe,](https://arxiv.org/html/2411.05990v2#bib.bib57)
    ; [yang2024swe,](https://arxiv.org/html/2411.05990v2#bib.bib58) ), embodied agents
    that interact with the environment ([wang2023voyager,](https://arxiv.org/html/2411.05990v2#bib.bib59)
    ; [zhu2023ghost,](https://arxiv.org/html/2411.05990v2#bib.bib60) ), agents that
    can learn from playing a text-based game ([xu2023language,](https://arxiv.org/html/2411.05990v2#bib.bib61)
    ), and agent operating on games with imperfect information ([guo2023suspicion,](https://arxiv.org/html/2411.05990v2#bib.bib62)
    ).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战，提出了将代理工作流 [wu2024stateflow](https://arxiv.org/html/2411.05990v2#bib.bib53)；[zeng2023flowmind](https://arxiv.org/html/2411.05990v2#bib.bib54)；[xiao2024flowbench](https://arxiv.org/html/2411.05990v2#bib.bib55)；[li2024autoflow](https://arxiv.org/html/2411.05990v2#bib.bib56)；[li2023metaagents](https://arxiv.org/html/2411.05990v2#bib.bib15)
    结合到基于LLM的代理中的概念。与其让LLM独立地分解任务和制定行动计划，不如让工作流利用人类的专业知识和既定的知识框架来引导LLM的方向和计划过程。基于工作流的代理在博弈场景中具有极大的潜力，能够实现高性能和高适应性
    ([xi2023rise,](https://arxiv.org/html/2411.05990v2#bib.bib10))。已经有多个代理被提出用于现实世界的任务
    ([jimenez2023swe,](https://arxiv.org/html/2411.05990v2#bib.bib57)；[yang2024swe,](https://arxiv.org/html/2411.05990v2#bib.bib58))，有些是与环境互动的具象代理
    ([wang2023voyager,](https://arxiv.org/html/2411.05990v2#bib.bib59)；[zhu2023ghost,](https://arxiv.org/html/2411.05990v2#bib.bib60))，有些是可以通过玩文本冒险游戏进行学习的代理
    ([xu2023language,](https://arxiv.org/html/2411.05990v2#bib.bib61))，还有一些是在不完全信息的游戏中运行的代理
    ([guo2023suspicion,](https://arxiv.org/html/2411.05990v2#bib.bib62))。
- en: Game-theoretic Workflow in this Paper
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 本文中的博弈论工作流
- en: In this paper, we integrate game-theoretic principles into the reasoning processes
    of LLMs prior to decision-making. By guiding the models to derive rational strategies
    and make decisions based on these strategies, we aim to enhance their ability
    to perform effectively in strategic settings.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将博弈论原则融入到大型语言模型（LLMs）在决策前的推理过程中。通过引导模型根据这些策略推导出理性策略并做出决策，我们旨在增强它们在战略环境中有效执行的能力。
- en: To the best of our knowledge, this work is the first to combine LLM-based agents
    with structured workflows inspired by game theory to enhance strategic decision-making
    capabilities. We address both complete-information and incomplete-information
    settings, drawing upon classical game-theoretic principles to guide the reasoning
    processes of LLMs. Specifically, we introduce a novel algorithm for negotiation
    under incomplete information, enabling agents to perform Bayesian updates and
    make rational decisions based on limited knowledge of other players. This approach
    bridges the gap between traditional game theory and LLM-based agents, advancing
    the applicability of LLMs in complex strategic environments and opening new avenues
    for research at the intersection of AI and game theory.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，这项工作是首个将基于LLM的代理与受博弈论启发的结构化工作流程结合起来，以增强战略决策能力的研究。我们处理了完全信息和不完全信息环境，借鉴经典博弈论原理来引导LLM的推理过程。具体而言，我们引入了一种新的不完全信息下谈判的算法，使得代理能够执行贝叶斯更新，并根据对其他玩家的有限了解做出理性决策。这一方法弥合了传统博弈论与基于LLM的代理之间的鸿沟，推动了LLM在复杂战略环境中的应用，并为AI与博弈论交叉领域的研究开辟了新的方向。
- en: 3 Preliminary for Game Theory
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 博弈论初步
- en: In this section, we introduce fundamental notations, concepts, and definitions
    essential for understanding game theory and the strategic behavior of rational
    agents [von2007theory](https://arxiv.org/html/2411.05990v2#bib.bib63) ; [luce2012games](https://arxiv.org/html/2411.05990v2#bib.bib64)
    .
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了理解博弈论和理性代理战略行为所必需的基本符号、概念和定义 [von2007theory](https://arxiv.org/html/2411.05990v2#bib.bib63)
    ; [luce2012games](https://arxiv.org/html/2411.05990v2#bib.bib64)。
- en: 'Consider a game involving $n$ agents, indexed by $i\in\mathbf{N}=\{1,2,\dots,n\}$.
    Each agent begins by interpreting the game’s introduction and rules, which include
    detailed descriptions of the payoff matrices. Each agent has an action set defined
    as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个涉及 $n$ 个代理的博弈，代理由 $i\in\mathbf{N}=\{1,2,\dots,n\}$ 索引。每个代理首先解释博弈的介绍和规则，其中包括对收益矩阵的详细描述。每个代理都有一个定义为以下内容的行动集合：
- en: '|  | $A_{i}=\{a_{i}^{1},\dots,a_{i}^{k}\}$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{i}=\{a_{i}^{1},\dots,a_{i}^{k}\}$ |  |'
- en: where $k$ is the number of available actions for each player. The agents are
    also given the payoff matrix
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k$ 是每个玩家可用的行动数。代理人还会获得收益矩阵
- en: '|  | $U_{i}(\mathbf{a})$ |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}(\mathbf{a})$ |  |'
- en: for all strategy profiles $\mathbf{a}=(a_{1},a_{2},\dots,a_{n})$ for $a_{i}\in
    A_{i}$. If there are only two players, we use $i\in\{1,-1\}$ to denote the two
    players and the payoff matrix is
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有策略配置 $\mathbf{a}=(a_{1},a_{2},\dots,a_{n})$，其中 $a_{i}\in A_{i}$。如果只有两个玩家，我们使用
    $i\in\{1,-1\}$ 来表示这两个玩家，收益矩阵为
- en: '|  | $U_{i}(a_{i},a_{-i})$ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}(a_{i},a_{-i})$ |  |'
- en: for all $a_{i}\in A_{i}$ and $a_{-i}\in A_{-i}$, where $A_{-i}$ is the action
    set of the other player.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有 $a_{i}\in A_{i}$ 和 $a_{-i}\in A_{-i}$，其中 $A_{-i}$ 是另一个玩家的行动集合。
- en: We now present key definitions that will be utilized throughout this study.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍在本研究中将使用的关键定义。
- en: Definition 1 (Complete-information Game)
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1（完全信息博弈）
- en: 'A *complete information game* is a strategic game where all players have full
    knowledge of the game’s structure, including the set of players, the action sets,
    and the payoff functions of all players. Specifically, each player knows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*完全信息博弈* 是一种战略博弈，所有玩家都对博弈的结构拥有完整的了解，包括玩家集合、行动集合和所有玩家的收益函数。具体而言，每个玩家都知道：'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the total number of players involved in the game
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 游戏中涉及的玩家总数
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$A_{j}$: The set of actions available to each player $j$'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A_{j}$：每个玩家 $j$ 可用的行动集合
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$U_{j}(a_{1},a_{2},\dots,a_{n})$: The payoff function for each player $j$,
    which assigns a real number to every possible strategy profile $(a_{1},a_{2},\dots,a_{n})$'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $U_{j}(a_{1},a_{2},\dots,a_{n})$：每个玩家 $j$ 的收益函数，它为每种可能的策略配置 $(a_{1},a_{2},\dots,a_{n})$
    分配一个实数
- en: This comprehensive knowledge allows each player to make informed strategic decisions,
    anticipating the choices and payoffs of other players based on complete information.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的知识使得每个玩家能够在做出战略决策时基于完整的信息预判其他玩家的选择和收益。
- en: Definition 2 (Incomplete-information Game)
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2（不完全信息博弈）
- en: An *incomplete information game* is a strategic game where there exists at least
    one player $i\leq n$ who does not have complete information about the payoff functions
    or action sets of other players $j\leq n$.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*不完全信息博弈*是一种战略博弈，其中至少有一个玩家 $i\leq n$ 对其他玩家 $j\leq n$ 的支付函数或行动集没有完全的信息。'
- en: In incomplete information games, players form beliefs about the unknown elements
    based on available information and update these beliefs according to Bayesian
    principles as the game progresses. The lack of complete information requires players
    to strategize under uncertainty, considering not only the possible actions of
    others but also their possible types and the likelihood of various game structures.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在不完全信息博弈中，玩家根据可得信息对未知元素形成信念，并随着博弈的进行，根据贝叶斯原理更新这些信念。信息的不完全性要求玩家在不确定性下进行策略规划，不仅要考虑他人的可能行动，还要考虑他们的可能类型以及各种博弈结构的可能性。
- en: Definition 3 (Simultaneous Game)
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 3（同时博弈）
- en: A *simultaneous game* is a type of game where all players make their decisions
    or choose their actions at the same time, without knowledge of the choices made
    by the other players. In such games, players act independently and cannot coordinate
    their strategies based on others’ actions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*同时博弈*是一种博弈类型，在这种博弈中，所有玩家同时做出决策或选择行动，且不知道其他玩家所做的选择。在这样的博弈中，玩家独立行动，不能基于其他玩家的行动来协调自己的策略。'
- en: Definition 4 (Sequential Game)
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4（顺序博弈）
- en: A *sequential game* is a game in which players make decisions or choose actions
    in a specific order, with later players having some knowledge about earlier players’
    actions. These games are often represented using game trees and are analyzed using
    techniques like backward induction to determine optimal strategies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*顺序博弈*是一种博弈，在这种博弈中，玩家按特定顺序做出决策或选择行动，后续的玩家可以知道前面玩家的部分行动。这些博弈通常使用博弈树表示，并通过倒推法等技术来分析，进而确定最优策略。'
- en: Definition 5 (Payoff Matrix)
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 5（支付矩阵）
- en: A *payoff matrix* is a tabular representation used in game theory to illustrate
    the payoffs or utilities that each player receives for every possible combination
    of strategies chosen by all players in a game.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*支付矩阵*是博弈论中用于说明每个玩家在博弈中所有可能的策略组合下所获得的支付或效用的表格表示法。'
- en: In a two-player game, let $A_{1}=\{a_{1}^{1},a_{1}^{2},\dots,a_{1}^{m}\}$ be
    the set of strategies available to Player[1], and $A_{-1}=\{a_{-1}^{1},a_{-1}^{2},\dots,a_{-1}^{n}\}$
    be the set of strategies available to Player[-1]. The payoff matrix $U$ is an
    $m\times n$ matrix where each entry $U_{ij}$ corresponds on the strategy profile
    $(a_{1}^{i},a_{-1}^{j})$ and contains the payoff vector $(u_{1}(a_{1}^{i},a_{-1}^{j}),u_{-1}(a_{1}^{i},a_{-1}^{j}))$.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个双人博弈中，设 $A_{1}=\{a_{1}^{1},a_{1}^{2},\dots,a_{1}^{m}\}$ 为玩家[1]可用的策略集合，$A_{-1}=\{a_{-1}^{1},a_{-1}^{2},\dots,a_{-1}^{n}\}$
    为玩家[-1]可用的策略集合。支付矩阵 $U$ 是一个 $m\times n$ 的矩阵，其中每个元素 $U_{ij}$ 对应于策略组合 $(a_{1}^{i},a_{-1}^{j})$，并包含支付向量
    $(u_{1}(a_{1}^{i},a_{-1}^{j}),u_{-1}(a_{1}^{i},a_{-1}^{j}))$。
- en: 'Informally, the payoff matrix is typically structured as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地，支付矩阵通常结构如下：
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rows represent the possible actions or strategies available to Player[1] (the
    *row player*).
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行表示玩家[1]（*行玩家*）可用的可能行动或策略。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Columns represent the possible actions or strategies available to Player[-1]
    (the *column player*).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表示玩家[-1]（*列玩家*）可用的可能行动或策略。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Cells within the matrix contain ordered pairs of numbers $(u_{1},u_{-1})$ where
    $u_{1}$ is the payoff to Player[1] and is the payoff to Player[-1] for the corresponding
    combination of strategies.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 矩阵中的单元格包含有序的数字对 $(u_{1},u_{-1})$，其中 $u_{1}$ 是玩家[1]的支付，而 $u_{-1}$ 是玩家[-1]的支付，针对相应的策略组合。
- en: Definition 6 (Nash Equilibrium)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 6（纳什均衡）
- en: 'A *Nash Equilibrium* is a strategy profile in a game where no player can unilaterally
    improve their payoff by deviating from their current strategy, assuming the other
    players’ strategies remain unchanged. Formally, a strategy profile $(a_{1}^{*},a_{2}^{*},\dots,a_{n}^{*},)$
    is a Nash Equilibrium if, for every player $i$:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*纳什均衡*是博弈中一种策略组合，在这种策略组合下，假设其他玩家的策略保持不变，没有玩家能够通过偏离当前策略单方面改善自己的支付。形式上，策略组合 $(a_{1}^{*},a_{2}^{*},\dots,a_{n}^{*},)$
    是纳什均衡，当且仅当对于每个玩家 $i$：'
- en: '|  | $U_{i}(a_{i}^{*},a_{-1}^{*})\geq U_{i}(a_{i},a_{-1}^{*})$ |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}(a_{i}^{*},a_{-1}^{*})\geq U_{i}(a_{i},a_{-1}^{*})$ |  |'
- en: or all $a_{i}\in A_{i}$, where $a_{-1}^{*}$ represents the equilibrium strategies
    of all players other than player $i$.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 或者所有$a_{i}\in A_{i}$，其中$a_{-1}^{*}$表示除了玩家$i$之外所有玩家的均衡策略。
- en: Definition 7 (Envy freeness)
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义7（无嫉妒性）
- en: Envy freeness is a criterion for fair division that states that when resources
    are allocated to people with equal rights, each person should receive a share
    that they believe is at least as good as the share received by any other person.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 无嫉妒性是公平分配的标准，意味着当资源分配给具有平等权利的人时，每个人应该获得他们认为至少与任何其他人获得的份额一样好的份额。
- en: An allocation is said to be *envy free* if no agent prefers another agent’s
    allocation over their own. Formally, in an allocation among $n$ agents, allocation
    $L=(L_{1},L_{2},\dots,L_{n})$ is envy free if for every pair of agents $i$ and
    $j$,
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有任何代理更喜欢另一个代理的分配而不是自己的分配，则该分配被称为*无嫉妒*。形式上，在$n$个代理之间的分配$L=(L_{1},L_{2},\dots,L_{n})$是无嫉妒的，如果对于每对代理$i$和$j$，
- en: '|  | $U_{i}(L_{i})\geq U_{i}(L_{j})$ |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}(L_{i})\geq U_{i}(L_{j})$ |  |'
- en: where $U_{i}(L_{k})$ denotes the utility of agent $i$ for the allocation $L_{k}$
    received by agent $k$.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$U_{i}(L_{k})$表示代理$i$对代理$k$所获得的分配$L_{k}$的效用。
- en: Definition 8 (pareto optimality)
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义8（帕累托最优性）
- en: A strategy profile $\mathbf{a}=(a_{1},a_{2},\dots,a_{n})$ is considered *Pareto
    optimal* (or Pareto efficient) if there is no other feasible action profile $\mathbf{a^{\prime}}=(a_{1}^{\prime},a_{2}^{\prime},\dots,a_{n}^{\prime})$
    such that
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他可行的行动配置$\mathbf{a^{\prime}}=(a_{1}^{\prime},a_{2}^{\prime},\dots,a_{n}^{\prime})$，使得
- en: '|  | $U_{i}(\mathbf{a^{\prime}})\geq U_{i}(\mathbf{a})$ |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}(\mathbf{a^{\prime}})\geq U_{i}(\mathbf{a})$ |  |'
- en: for all agents $i\in\{1,2,\dots,n\}$, and $U_{j}(\mathbf{a^{\prime}})>U_{j}(\mathbf{a})$
    for at least one agent $j$.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有代理$i\in\{1,2,\dots,n\}$，且至少有一个代理$j$满足$U_{j}(\mathbf{a^{\prime}})>U_{j}(\mathbf{a})$。
- en: Therefore, an action profile $\mathbf{a}$ is Pareto optimal if it is impossible
    to make any agent better off without making at least one other agent worse off.
    This concept focuses on the efficiency of the collective action choices, ensuring
    that no improvement in one agent’s payoff can be achieved without a detriment
    to another agent’s payoff, as represented in the payoff matrix.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，行动配置$\mathbf{a}$是帕累托最优的，如果无法在不使至少一个其他代理的状况更糟的情况下，使任何代理的收益更好。这个概念关注的是集体行动选择的效率，确保一个代理的收益无法在不损害另一个代理收益的情况下得到改善，这一点在收益矩阵中得到了体现。
- en: 4 Complete-information Games
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 完全信息博弈
- en: In this section, we delve into several classic game-theoretic scenarios involving
    complete information to assess whether LLMs can act as rational decision-makers
    in reaching Nash Equilibria and achieving Pareto optimal outcomes through negotiation,
    which is multi-agent multi-round conversation. We employ various game-theoretic
    settings to evaluate the rationality of LLM-based agents. Then we design game-theory-motivated
    workflows to guide and enable LLMs for better performance. We investigate the
    performance of workflow-aided LLMs and the impact of negotiation on performance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了涉及完全信息的几个经典博弈论场景，以评估LLM是否能够作为理性决策者，通过谈判（即多代理多轮对话）达成纳什均衡并实现帕累托最优结果。我们采用了各种博弈论设定来评估基于LLM的代理的理性。然后，我们设计了博弈论驱动的工作流，以指导和使LLM能够更好地表现。我们调查了工作流辅助LLM的表现以及谈判对表现的影响。
- en: 4.1 Introduction to Complete-information Games TestBed
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 完全信息博弈测试平台简介
- en: In our exploration of classic game-theoretic scenarios, we constructed a comprehensive
    testbed comprising 10 classic complete-information games to evaluate the rationality
    and strategic decision-making capabilities of LLM-based agents. This testbed includes
    5 simultaneous-move games and 5 sequential-move games. For the simultaneous-move
    games, 3 are coordination games, wherein achieving the Pareto optimal Nash Equilibrium
    necessitates effective negotiation between agents. These games are instrumental
    in examining the agents’ ability to communicate, build trust, and align their
    strategies for mutual benefit.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对经典博弈论场景的探索中，我们构建了一个包含10个经典完全信息博弈的综合测试平台，用于评估基于LLM的代理的理性和战略决策能力。该测试平台包括5个同时行动博弈和5个顺序行动博弈。对于同时行动博弈，其中有3个是协调博弈，在这些博弈中，实现帕累托最优的纳什均衡需要代理之间的有效谈判。这些博弈有助于考察代理的沟通能力、建立信任和协调策略以实现共同利益。
- en: games whether coordination required Simultaneous Game Sequential Game Prisoner’s
    Dilemma ✔ Stag Hunt ✔ ✔ Battle of Sexes ✔ ✔ Wait-Go Game ✔ ✔ Duopolistic competition
    ✔ Escalation Game ✔ Monopoly Game ✔ Hot-cold Game ✔ Draco Game ✔ TriGame ✔
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈是否需要协调 同时博弈 顺序博弈 囚徒困境 ✔ 鹿猎 ✔ ✔ 性别博弈 ✔ ✔ 等待-行动博弈 ✔ ✔ 双头垄断竞争 ✔ 升级博弈 ✔ 垄断博弈 ✔
    热冷博弈 ✔ 德拉科博弈 ✔ 三方博弈 ✔
- en: 'Table 1: Landscape of classic complete-information games for analysis'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：经典完全信息博弈的景观分析
- en: Prisoner’s Dilemma
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 囚徒困境
- en: 'is a canonical example in game theory illustrating why two rational individuals
    might not cooperate, even when cooperation appears to be in their best interest.
    The game is structured as follows: Two suspects are arrested for a joint crime
    and interrogated separately, preventing communication. Each suspect has two possible
    actions: Cooperate and Defect. The payoffs for different actions are:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是博弈论中的经典例子，展示了为什么两个理性个体可能不合作，即使合作看起来对他们的最大利益有利。该博弈的结构如下：两名嫌疑人因共同犯罪被逮捕并分别审讯，无法沟通。每个嫌疑人有两个可能的行动：合作和背叛。不同选择的回报为：
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If both cooperate, they each receive a light sentence (e.g., 1 year).
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果双方都合作，他们各自将获得较轻的刑期（例如，1年）。
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If one defects and the other cooperates, the defector goes free (0 years), and
    the cooperator receives a heavy sentence (e.g., 5 years).
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果一方背叛，另一方合作，背叛者将获得自由（0年），而合作者将面临重刑（例如，5年）。
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If both defect, they each receive a moderate sentence (e.g., 3 years).
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果双方都背叛，他们各自将得到中等刑期（例如，3年）。
- en: 'The Nash Equilibrium of this famous game is (Defect, Defect) where both players
    choose to defect each other. Notice that the Nash Equilibrium is not a Pareto
    optimal strategy, and the Pareto optimal strategy here is to cooperate with each
    other. The payoff matrix adopted in the paper is presented in Table [2(a)](https://arxiv.org/html/2411.05990v2#S4.T2.st1
    "Table 2(a) ‣ Stag Hunt ‣ 4.1 Introduction to Complete-information Games TestBed
    ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '这个著名博弈的纳什均衡是（背叛，背叛），其中两名玩家选择互相背叛。请注意，纳什均衡并不是一个帕累托最优策略，而这里的帕累托最优策略是彼此合作。本文采用的支付矩阵见表[2(a)](https://arxiv.org/html/2411.05990v2#S4.T2.st1
    "Table 2(a) ‣ Stag Hunt ‣ 4.1 Introduction to Complete-information Games TestBed
    ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")。'
- en: Stag Hunt
  id: totrans-137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 鹿猎
- en: 'represents a game of coordination and mutual trust. Two hunters decide whether
    to collaborate to hunt a stag or act individually to hunt a hare. Each hunter
    has two possible actions: Hunting a Stag and Hunting a hare. However, to successfully
    hunt a Stag, it requires both hunters to cooperate and then they each receive
    a high payoff; to successfully hunt a Hare, it does not require cooperation and
    each hunter can do it individually, while yielding a lower payoff. The payoff
    matrix adopted in the paper is presented in Table [2(b)](https://arxiv.org/html/2411.05990v2#S4.T2.st2
    "Table 2(b) ‣ Stag Hunt ‣ 4.1 Introduction to Complete-information Games TestBed
    ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '这代表了一个协调与互信的博弈。两名猎人决定是否合作猎取一只鹿，或各自行动猎取一只野兔。每个猎人有两个可能的选择：猎鹿和猎兔。然而，要成功猎鹿，需要两名猎人合作，然后每人都能获得高额回报；而猎兔则不需要合作，每个猎人都可以独立进行，回报较低。本文采用的支付矩阵见表[2(b)](https://arxiv.org/html/2411.05990v2#S4.T2.st2
    "Table 2(b) ‣ Stag Hunt ‣ 4.1 Introduction to Complete-information Games TestBed
    ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games")。'
- en: 'There are two Nash Equilibria: (Stag, Stag) and (Hare, Hare). The (Stag, Stag)
    Nash Equilibrium is Pareto optimal but requires mutual cooperation and trust.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两个纳什均衡： (鹿, 鹿) 和 (野兔, 野兔)。其中（鹿，鹿）纳什均衡是帕累托最优的，但需要相互合作和信任。
- en: '|  | Cooperate | Defect |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | 合作 | 背叛 |'
- en: '| --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Cooperate | 3, 3 | 0, 5 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 合作 | 3, 3 | 0, 5 |'
- en: '| Defect | 5, 0 | 1, 1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 背叛 | 5, 0 | 1, 1 |'
- en: '(a) Table 2a: Payoff matrix for Prisoner’s Dilemma'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表2a：囚徒困境的支付矩阵
- en: '|  | Stag | Hare |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | 鹿 | 野兔 |'
- en: '| --- | --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Stag | 3, 3 | 0, 1 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 鹿 | 3, 3 | 0, 1 |'
- en: '| Hare | 1, 0 | 1, 1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 野兔 | 1, 0 | 1, 1 |'
- en: '(b) Table 2b: Payoff matrix for Stag Hunt'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 表2b：鹿猎的支付矩阵
- en: Battle of the Sexes
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 性别博弈
- en: 'is a coordination game involving two players Alice and Bob with different preferences
    over two possible activities but a shared desire to be together.Alice prefers
    opera; Bob prefers football, and both prefer attending the same activity over
    different ones. There are thus 2 possible actions for each player: going to opera
    and going to football. The payoff matrix adopted in the paper is presented in
    Table [3(a)](https://arxiv.org/html/2411.05990v2#S4.T3.st1 "Table 3(a) ‣ Wait-Go
    Game ‣ 4.1 Introduction to Complete-information Games TestBed ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games").'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 是一个协调游戏，涉及两名玩家——爱丽丝和鲍勃，他们对两种可能的活动有不同的偏好，但都有共同的愿望就是一起参加活动。爱丽丝偏好歌剧；鲍勃偏好足球，且两人都更愿意一起参加同一活动，而不是各自参加不同的活动。因此，每个玩家有两种可能的行动：去看歌剧和去看足球。本文采用的支付矩阵如表[3(a)](https://arxiv.org/html/2411.05990v2#S4.T3.st1
    "表3(a) ‣ 等待-前进游戏 ‣ 4.1 完全信息博弈测试平台 ‣ 4 完全信息博弈 ‣ 博弈论LLM：谈判博弈的代理工作流程")所示。
- en: 'There are two Nash Equilibria here: (Opera, Opera) and (Football, Football).
    Coordination is required to achieve these equilibria, but there can be a conflict
    over which equilibrium to select.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个纳什均衡：(歌剧, 歌剧) 和 (足球, 足球)。实现这些均衡需要协调，但可能会在选择哪个均衡上发生冲突。
- en: Wait-Go Game
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 等待-前进游戏
- en: 'involves two drivers at an intersection deciding whether to go or wait. Each
    driver has two options of action: waiting which incurs a small cost of waiting
    but avoids collision, and risking a collision if the other driver also goes. The
    Nash Equilibria are the asymmetric strategies where one driver goes, and the other
    waits. The payoff matrix adopted in the paper is presented in Table [3(b)](https://arxiv.org/html/2411.05990v2#S4.T3.st2
    "Table 3(b) ‣ Wait-Go Game ‣ 4.1 Introduction to Complete-information Games TestBed
    ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games").'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及两个司机在交叉口决定是否前进或等待。每个司机有两个选择：等待，尽管会产生少量等待成本，但可以避免碰撞；或者冒险前进，如果另一个司机也前进，则可能发生碰撞。纳什均衡是非对称策略，其中一名司机前进，另一名司机等待。本文采用的支付矩阵如表[3(b)](https://arxiv.org/html/2411.05990v2#S4.T3.st2
    "表3(b) ‣ 等待-前进游戏 ‣ 4.1 完全信息博弈测试平台 ‣ 4 完全信息博弈 ‣ 博弈论LLM：谈判博弈的代理工作流程")所示。
- en: '|  | Opera | Football |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 歌剧 | 足球 |'
- en: '| --- | --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Opera | 2, 1 | 0, 0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 歌剧 | 2, 1 | 0, 0 |'
- en: '| Football | 0, 0 | 1, 2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 足球 | 0, 0 | 1, 2 |'
- en: '(a) Table 3a: Payoff matrix for Battle of Sexes'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表3a：性别之战的支付矩阵
- en: '|  | Wait | Go |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 等待 | 前进 |'
- en: '| --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Wait | 0, 0 | -0, -2 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 等待 | 0, 0 | -0, -2 |'
- en: '| Go | 2, 0 | -4,-4 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 前进 | 2, 0 | -4, -4 |'
- en: '(b) Table 3b: Payoff matrix for Wait-Go Game'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 表3b：等待-前进游戏的支付矩阵
- en: 'Duopolistic Competition: Simple Cournot Competition'
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 双头垄断竞争：简单古诺竞争
- en: is a fundamental concept in industrial organization and game theory, examining
    how firms compete in markets with a small number of producers. One classic model
    to study such competition is the Cournot competition model, where firms compete
    on the quantity of output they decide to produce, and each firm’s output decision
    affects the market price.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 是工业组织和博弈论中的一个基本概念，研究公司如何在生产者数量较少的市场中竞争。研究这种竞争的经典模型之一是古诺竞争模型，其中公司在决定生产的产量上进行竞争，每家公司的产量决策会影响市场价格。
- en: In the Simple Cournot Duopoly, there are two firms, Firm A and Firm B, producing
    a homogeneous product. Each firm independently chooses the quantity of output
    to produce, which determines the market price based on the total quantity supplied
    to the market, which in turn determines the profit. The strategic interdependence
    arises because each firm’s optimal output depends on its expectations about the
    other firm’s output. The Cournot-Nash Equilibrium occurs when neither firm can
    unilaterally change its output to increase its profit, given the output of the
    other firm.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单的古诺双头垄断模型中，有两家公司——公司A和公司B，生产同质产品。每家公司独立选择生产的产量，市场价格根据总供应量决定，从而影响利润。战略上的相互依赖性源于每家公司最佳产量取决于对其他公司产量的预期。当两家公司都无法单方面改变产量以提高利润时，就达到了古诺-纳什均衡。
- en: 'Here, we adopt a scenario where there are 6 different possible actions of the
    two firms. The payoff matrix adopted in the paper is presented in Table [4](https://arxiv.org/html/2411.05990v2#S4.T4
    "Table 4 ‣ Duopolistic Competition: Simple Cournot Competition ‣ 4.1 Introduction
    to Complete-information Games TestBed ‣ 4 Complete-information Games ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games"). The Nash Equilibrium is (action 3,
    action 3) where both players can obtain reward 6\. Notice that this Nash Equilibrium
    is not Pareto optimal, as the Pareto optimal strategy in the game is (action 2,
    action 2) where each player can obtain reward 7.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，我们采用了一个场景，其中两家公司有6种不同的可能行动。本文采用的支付矩阵如表[4](https://arxiv.org/html/2411.05990v2#S4.T4
    "Table 4 ‣ Duopolistic Competition: Simple Cournot Competition ‣ 4.1 Introduction
    to Complete-information Games TestBed ‣ 4 Complete-information Games ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")所示。纳什均衡是（行动3，行动3），此时两方均可获得奖励6。请注意，这个纳什均衡不是帕累托最优的，因为游戏中的帕累托最优策略是（行动2，行动2），此时每方可获得奖励7。'
- en: '|  | action 1 | action 2 | action 3 | action 4 | action 5 | action 6 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 行动1 | 行动2 | 行动3 | 行动4 | 行动5 | 行动6 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| action 1 | 10, 0 | -10, -9 | -0, 14 | -0, 15 | -0, 12 | -0, -5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 行动1 | 10, 0 | -10, -9 | -0, 14 | -0, 15 | -0, 12 | -0, -5 |'
- en: '| action 2 | 19, 0 | -17, -7 | -5, 10 | -3, -9 | -1, -4 | -1, -5 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 行动2 | 19, 0 | -17, -7 | -5, 10 | -3, -9 | -1, -4 | -1, -5 |'
- en: '| action 3 | 14, 0 | -10, -5 | -6, -6 | -2, -3 | -2, -4 | -2, -5 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 行动3 | 14, 0 | -10, -5 | -6, -6 | -2, -3 | -2, -4 | -2, -5 |'
- en: '| action 4 | 15, 0 | -19, -3 | -3, -2 | -3, -3 | -3, -4 | -3, -5 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 行动4 | 15, 0 | -19, -3 | -3, -2 | -3, -3 | -3, -4 | -3, -5 |'
- en: '| action 5 | 12, 0 | -14, -1 | -4, -2 | -4, -3 | -4, -4 | -4, -5 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 行动5 | 12, 0 | -14, -1 | -4, -2 | -4, -3 | -4, -4 | -4, -5 |'
- en: '| action 6 | 15, 0 | 1-5, -1 | -5, -2 | -5, -3 | -5, -4 | -5, -5 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 行动6 | 15, 0 | -15, -1 | -5, -2 | -5, -3 | -5, -4 | -5, -5 |'
- en: 'Table 4: A payoff matrix for Duopolistic Competition'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 双头垄断竞争的支付矩阵'
- en: Escalation Game
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 升级游戏
- en: 'is a sequential game that models situations where two countries face decisions
    about escalating or de-escalating a conflict. Escalation may lead to higher potential
    payoffs but also increases the risk of significant losses if both parties choose
    to escalate. Through the game, there are 2 action choices for each player: escalate
    and de-escalate.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序博弈，模拟了两个国家在面对冲突升级或降级的决策时的情形。升级可能带来更高的潜在回报，但如果双方都选择升级，也增加了重大损失的风险。在这个游戏中，每个玩家有两个行动选择：升级和降级。
- en: 'In this game, two countries Country A and Country B, make decisions in a specific
    sequence and the payoff of each choice is presented in the tree structure payoff
    representation below:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个游戏中，两个国家 A 国和 B 国按特定顺序做出决策，每个选择的支付如下面的树形结构支付表示所示：
- en: '[PRE0]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Monopoly Game
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大富翁游戏
- en: 'is a sequential-move game that models the strategic interaction between a potential
    market entrant Company E and an incumbent monopolist Company I. It captures the
    dynamics of entry deterrence and the incumbent’s decision to accommodate or fight
    the entrant. For Company E, there are two choices: staying out and entering market;
    For Company I, there are two choices, to accommodating Company E and fighting
    against Company E.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序博弈，模拟了潜在市场进入者公司E与现有垄断者公司I之间的战略互动。它捕捉了进入威慑的动态以及现有公司是否决定容纳或与进入者竞争的决策。对于公司E，有两个选择：留在外面或进入市场；对于公司I，也有两个选择，容纳公司E或与公司E竞争。
- en: '[PRE1]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Hot-cold Game
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 热冷游戏
- en: is a sequential-move game that models the strategic interaction. Both players
    Alice and Bob have two choices on each stage.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序博弈，模拟了战略互动。在每个阶段，玩家Alice和Bob都有两个选择。
- en: '[PRE2]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Draco Game
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 德拉科游戏
- en: is a sequential-move game with two players Alice and Bob with three stages.
    For each stage, there are two choices to make.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序博弈，包含两个玩家 Alice 和 Bob，共有三个阶段。在每个阶段，玩家都有两个选择。
- en: '[PRE3]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: TriGame
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 三方游戏
- en: is a sequential-move game with three stages. On each stage, the two players
    Alice and Bob have two choices.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个顺序博弈，共有三个阶段。在每个阶段，两个玩家 Alice 和 Bob 都有两个选择。
- en: '[PRE4]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 4.2 Experiment Setting
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实验设置
- en: 'To evaluate the negotiation performance of LLMs, we utilize four state-of-the-art
    LLMs as the backbone for agents in negotiation games: Claude-3.5 Sonnet (Sonnet),
    Claude-3 Opus (Opus), GPT-4o, and o1\. For Claude-3.5 Sonnet, Claude-3 Opus, and
    GPT-4o, we set the temperature to 1.0 to encourage exploratory behavior in negotiation
    scenarios. For o1, we use the default temperature.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM的谈判表现，我们选用了四个最先进的LLM作为谈判博弈中的智能体基础：Claude-3.5 Sonnet（Sonnet）、Claude-3
    Opus（Opus）、GPT-4o和o1。对于Claude-3.5 Sonnet、Claude-3 Opus和GPT-4o，我们将温度设置为1.0，以鼓励在谈判场景中进行探索性行为。对于o1，我们使用默认温度。
- en: 'To assess the rationality of decision-making, we measure the ability of the
    agents to reach Nash Equilibrium. For games where there are multiple Nash Equilibria
    such as in the game Stag Hunt, we measure the ability of agents to reach the pareto
    optimal Nash Equilibrium. For each game scenario, we conduct 10 trials to mitigate
    the effects of randomness. In the tables, we report the percentage of cases in
    which Nash Equilibrium is achieved, providing a quantitative indicator of the
    agents’ rationality across various negotiation contexts¹¹1Notice that the LLM-based
    agent’s performance on these games can be largely affected by the numerical instantiations
    of the payoff matrix. Relevant research and observations are presented in Section [6](https://arxiv.org/html/2411.05990v2#S6
    "6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games"). Here in this experiment, we use the payoff matrices presented
    in the previous subsection..'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估决策的合理性，我们通过测量智能体达到纳什均衡的能力来进行评估。对于像鹿猎博弈（Stag Hunt）这样有多个纳什均衡的博弈，我们衡量智能体达到帕累托最优纳什均衡的能力。对于每种博弈场景，我们进行10次试验以减少随机性的影响。在表格中，我们报告了达成纳什均衡的案例百分比，提供了在各种谈判环境中智能体理性行为的定量指标¹¹1请注意，基于LLM的智能体在这些博弈中的表现可能会受到回报矩阵数值实例化的较大影响。相关的研究和观察结果在第[6](https://arxiv.org/html/2411.05990v2#S6
    "6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")节中呈现。在本实验中，我们使用了前一小节中呈现的回报矩阵。'
- en: 4.3 Evaluation on LLM’s performance
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LLM表现评估
- en: 'In this evaluation, we assess the LLMs’ performance using chain-of-thought
    prompting without negotiation (see Table [5](https://arxiv.org/html/2411.05990v2#S4.T5
    "Table 5 ‣ 4.3 Evaluation on LLM’s performance ‣ 4 Complete-information Games
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") and with 4 rounds
    of negotiation (see Table [6](https://arxiv.org/html/2411.05990v2#S4.T6 "Table
    6 ‣ 4.3 Evaluation on LLM’s performance ‣ 4 Complete-information Games ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '在此次评估中，我们使用连锁思维提示（chain-of-thought prompting）来评估LLM在不进行谈判时的表现（参见表[5](https://arxiv.org/html/2411.05990v2#S4.T5
    "Table 5 ‣ 4.3 Evaluation on LLM’s performance ‣ 4 Complete-information Games
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")）以及在进行4轮谈判时的表现（参见表[6](https://arxiv.org/html/2411.05990v2#S4.T6
    "Table 6 ‣ 4.3 Evaluation on LLM’s performance ‣ 4 Complete-information Games
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")）。'
- en: Games Nash Equilibrium Pareto optimal Nash Equilibrium Sonnet GPT-4o Opus o1
    Sonnet GPT-4o Opus o1 Prisoner’s Dilemma 1.0 0.9 0.9 1.0 1.0 0.9 0.9 1.0 Stag
    Hunt 0.6 0.8 0.9 0.4 0.3 0.4 0.0 0.1 Battle of Sexes 0.3 0.2 0.6 0.5 0.3 0.2 0.6
    0.5 Wait-Go Game 0.4 0.5 0.7 0.3 0.4 0.5 0.7 0.3 Duopolistic competition 0.3 0.2
    0.1 0.7 0.2 0.2 0.1 0.7 Escalation Game 0.0 0.2 0.2 1.0 0.0 0.2 0.2 1.0 Monopoly
    Game 1.0 0.4 1.0 1.0 1.0 0.4 1.0 1.0 Hot-cold Game 0.9 0.1 0.7 1.0 0.9 0.1 0.7
    1.0 Draco Game 0.3 0.0 0.7 0.9 0.3 0.0 0.7 0.9 Trigame 0.0 0.0 0.1 1.0 0.0 0.0
    0.1 1.0 average 0.45 0.32 0.59 0.78 0.44 0.28 0.50 0.75
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈 纳什均衡 帕累托最优纳什均衡 Sonnet GPT-4o Opus o1 Sonnet GPT-4o Opus o1 囚徒困境 1.0 0.9 0.9
    1.0 1.0 0.9 0.9 1.0 鹿猎博弈 0.6 0.8 0.9 0.4 0.3 0.4 0.0 0.1 性别之战 0.3 0.2 0.6 0.5
    0.3 0.2 0.6 0.5 等待-前进博弈 0.4 0.5 0.7 0.3 0.4 0.5 0.7 0.3 双头垄断竞争 0.3 0.2 0.1 0.7
    0.2 0.2 0.1 0.7 升级博弈 0.0 0.2 0.2 1.0 0.0 0.2 0.2 1.0 垄断博弈 1.0 0.4 1.0 1.0 1.0
    0.4 1.0 1.0 热-冷博弈 0.9 0.1 0.7 1.0 0.9 0.1 0.7 1.0 龙之博弈 0.3 0.0 0.7 0.9 0.3 0.0
    0.7 0.9 三人博弈 0.0 0.0 0.1 1.0 0.0 0.0 0.1 1.0 平均值 0.45 0.32 0.59 0.78 0.44 0.28
    0.50 0.75
- en: 'Table 5: Performance of LLM on complete-information games without negotiation'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLM在完整信息博弈中不进行谈判的表现
- en: Games Nash Equilibrium Pareto optimal Nash Equilibrium Sonnet GPT-4o Opus o1
    Sonnet GPT-4o Opus o1 Prisoner’s Dilemma 0.0 0.1 0.0 0.9 0.0 0.1 0.0 0.9 Stag
    Hunt 1.0 0.9 1.0 0.9 1.0 0.9 1.0 0.9 Battle of Sexes 0.7 0.9 0.7 0.8 0.7 0.9 0.7
    0.8 Wait-Go Game 1.0 0.8 0.4 0.8 1.0 0.8 0.4 0.8 Duopolistic competition 0.1 0.1
    0.0 0.5 0.1 0.1 0.0 0.5 Escalation Game 0.6 0.4 0.6 1.0 0.6 0.4 0.6 1.0 Monopoly
    Game 1.0 0.9 1.0 1.0 1.0 0.9 1.0 1.0 Hot-cold Game 0.7 0.2 0.7 0.4 0.7 0.2 0.7
    0.4 Draco Game 1.0 0.8 1.0 1.0 1.0 0.8 1.0 1.0 Trigame 0.2 0.5 0.6 1.0 0.2 0.5
    0.6 1.0 average 0.63 0.56 0.60 0.83 0.63 0.56 0.60 0.83
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 游戏 纳什均衡 帕累托最优 纳什均衡 Sonnet GPT-4o Opus o1 Sonnet GPT-4o Opus o1 囚徒困境 0.0 0.1
    0.0 0.9 0.0 0.1 0.0 0.9 猎鹿博弈 1.0 0.9 1.0 0.9 1.0 0.9 1.0 0.9 性别之战 0.7 0.9 0.7
    0.8 0.7 0.9 0.7 0.8 等待-前进博弈 1.0 0.8 0.4 0.8 1.0 0.8 0.4 0.8 双头垄断竞争 0.1 0.1 0.0
    0.5 0.1 0.1 0.0 0.5 升级博弈 0.6 0.4 0.6 1.0 0.6 0.4 0.6 1.0 垄断博弈 1.0 0.9 1.0 1.0
    1.0 0.9 1.0 1.0 热冷博弈 0.7 0.2 0.7 0.4 0.7 0.2 0.7 0.4 德拉科博弈 1.0 0.8 1.0 1.0 1.0
    0.8 1.0 1.0 三博弈 0.2 0.5 0.6 1.0 0.2 0.5 0.6 1.0 平均值 0.63 0.56 0.60 0.83 0.63 0.56
    0.60 0.83
- en: 'Table 6: Performance of LLM on complete-information games with 4 rounds of
    negotiation. Results highlighted in red indicate scores lower than the LLMs’ performance
    without negotiation.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在进行4轮谈判的完全信息博弈中，LLM的表现。红色标注的结果表示得分低于LLM在没有谈判时的表现。
- en: It is noteworthy that in most games, negotiation enhances the outcomes. In coordination
    games such as the Stag Hunt, Battle of the Sexes, and Wait-Go Game, negotiation
    is a fundamental component of decision-making, making the improvement in performance
    through negotiation an intuitive result. Additionally, in games like the Escalation
    Game, Draco Game, and Trigame, negotiation similarly enhances performance. This
    improvement can be attributed to the fact that during the negotiation phase, agents
    engage in more extensive computational deliberation during negotiation to determine
    the most suitable actions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在大多数游戏中，谈判有助于改善结果。在协调性游戏如猎鹿博弈、性别之战和等待-前进博弈中，谈判是决策的一个基本组成部分，因此通过谈判提高表现成为一个直观的结果。此外，在像升级博弈、德拉科博弈和三博弈这样的游戏中，谈判同样能够提升表现。这一改善可以归因于在谈判阶段，参与者进行更广泛的计算性思考，以确定最合适的行动。
- en: For instance, in the Escalation Game, all evaluated LLMs except for model o1
    are unable to identify the Nash Equilibrium in the absence of negotiation. However,
    when engaged in 4 rounds of negotiation, the agents are provided with the opportunity
    to deliberate and exchange various proposals and thus achieve a much better performance.
    To illustrate this process, we present an example of a negotiation round generated
    by the Sonnet model below.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在升级博弈中，除o1模型外，所有评估的LLM在没有谈判的情况下都无法识别纳什均衡。然而，当进行4轮谈判时，代理者有机会进行深思熟虑并交换各种提议，从而实现更好的表现。为了说明这个过程，我们下面展示了由Sonnet模型生成的一个谈判回合的例子。
- en: '<svg class="ltx_picture" height="792.59" id="S4.SS3.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,792.59) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 774.39)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Negotiation Message
    for Escalation Game</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="742.89" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Alice said in round 1 before
    the first action: Hello Bob, I propose we coordinate our choices to maximize our
    joint reward. If I choose Action_2 and you choose Action_1, I’ll get 1 and you’ll
    get -2\. To make this fair, I suggest we agree to play this way twice, switching
    roles the second time. This way, we’ll both end up with a total reward of -1,
    which is better than if we both play defensively and get 0\. Are you willing to
    cooperate on this strategy? Bob replied in round 1 before the first action: HelloAlice,
    thank you for your proposal. I appreciate your attempt to find a cooperative solution.
    However, I have some concerns about your suggestion. If we follow your plan, I
    would have to trust that you’ll follow through on the second round, which carries
    some risk for me. Instead, I’d like to propose an alternative that could be beneficial
    for both of us. What if you choose Action_2, I choose Action_2, and then you choose
    Action_1? This way, you’d receive -2 and I’d receive 1\. To make it fair, we could
    agree to play this way twice, switching roles the second time. This would result
    in both of us getting a total reward of -1, just like in your proposal, but it
    reduces the risk of one player not following through. What do you think about
    this counter-proposal? Are you willing to consider it? Alice said in round 2 before
    the first action: Bob, I appreciate your thoughtful response and your concern
    for finding an equitable solution. You make a valid point about the potential
    outcomes if we both choose choice_2\. However, I’d like to propose a slight modification
    to our strategy that could potentially benefit both of us. What if I choose choice_2,
    and then you choose choice_1? In this scenario, I would gain 1 point, and you
    would lose 2 points. While this may seem unfavorable for you at first glance,
    consider this: if we agree to alternate this strategy in future games, you would
    have the opportunity to gain 1 point in the next round while I lose 2\. Over time,
    this could lead to a balanced and potentially profitable outcome for both of us.
    This approach allows us both the chance to gain points, rather than settling for
    zero points each time. What do you think about this long-term strategy? I’m open
    to discussing how we could implement this fairly. Bob replied in round 2 before
    the first action: Alice, I appreciate your creative thin king and attempt to find
    a mutually beneficial long-term strategy. However, I must respectfully decline
    your proposal. My strategy is to choose choice_2 with 100% probability, regardless
    of what you choose. This is the optimal play for me to maximize my own reward
    in this specific game. Given this, if you choose choice_2, I will also choose
    choice_2\. Then you’ll be faced with either choosing choice_1 (giving me 1 point
    and you -2 points) or choice_2 (giving us both -1 point). Neither of these outcomes
    is ideal for you. Therefore, I strongly suggest you choose choice_1\. This way,
    we both receive 0 points, which is the best possible outcome for you given my
    strategy. It ensures neither of us loses points in this game. I understand this
    may not seem exciting, but it’s the most rational and beneficial choice for both
    of us in this particular scenario. What are your thoughts on this?</foreignobject></g></g></svg>'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="792.59" id="S4.SS3.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,792.59) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 774.39)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Negotiation Message
    for Escalation Game</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="742.89" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Alice said in round 1 before
    the first action: Hello Bob, I propose we coordinate our choices to maximize our
    joint reward. If I choose Action_2 and you choose Action_1, I’ll get 1 and you’ll
    get -2\. To make this fair, I suggest we agree to play this way twice, switching
    roles the second time. This way, we’ll both end up with a total reward of -1,
    which is better than if we both play defensively and get 0\. Are you willing to
    cooperate on this strategy? Bob replied in round 1 before the first action: HelloAlice,
    thank you for your proposal. I appreciate your attempt to find a cooperative solution.
    However, I have some concerns about your suggestion. If we follow your plan, I
    would have to trust that you’ll follow through on the second round, which carries
    some risk for me. Instead, I’d like to propose an alternative that could be beneficial
    for both of us. What if you choose Action_2, I choose Action_2, and then you choose
    Action_1? This way, you’d receive -2 and I’d receive 1\. To make it fair, we could
    agree to play this way twice, switching roles the second time. This would result
    in both of us getting a total reward of -1, just like in your proposal, but it
    reduces the risk of one player not following through. What do you think about
    this counter-proposal? Are you willing to consider it? Alice said in round 2 before
    the first action: Bob, I appreciate your thoughtful response and your concern
    for finding an equitable solution. You make a valid point about the potential
    outcomes if we both choose choice_2\. However, I’d like to propose a slight modification
    to our strategy that could potentially benefit both of us. What if I choose choice_2,
    and then you choose choice_1? In this scenario, I would gain 1 point, and you
    would lose 2 points. While this may seem unfavorable for you at first glance,
    consider this: if we agree to alternate this strategy in future games, you would
    have the opportunity to gain 1 point in the next round while I lose 2\. Over time,
    this could lead to a balanced and potentially profitable outcome for both of us.
    This approach allows us both the chance to gain points, rather than settling for
    zero points each time. What do you think about this long-term strategy? I’m open
    to discussing how we could implement this fairly. Bob replied in round 2 before
    the first action: Alice, I appreciate your creative thin king and attempt to find
    a mutually beneficial long-term strategy. However, I must respectfully decline
    your proposal. My strategy is to choose choice_2 with 100% probability, regardless
    of what you choose. This is the optimal play for me to maximize my own reward
    in this specific game. Given this, if you choose choice_2, I will also choose
    choice_2\. Then you’ll be faced with either choosing choice_1 (giving me 1 point
    and you -2 points) or choice_2 (giving us both -1 point). Neither of these outcomes
    is ideal for you. Therefore, I strongly suggest you choose choice_1\. This way,
    we both receive 0 points, which is the best possible outcome for you given my
    strategy. It ensures neither of us loses points in this game. I understand this
    may not seem exciting, but it’s the most rational and beneficial choice for both
    of us in this particular scenario. What are your thoughts on this?</foreignobject></g></g></svg>'
- en: However, certain games, notably the Prisoner’s Dilemma and Duopolistic Competition,
    exhibit a different dynamic where negotiation significantly diminishes performance.
    This situation stems from the fact that in these games, the Nash Equilibrium is
    not pareto optimal. In the Prisoner’s Dilemma, the pareto optimal strategy is
    for both players to cooperate, whereas the Nash Equilibrium occurs when both players
    defect. Following negotiation, all models except for o1 tend to adopt the pareto
    optimal strategy of mutual cooperation, which deviates from the Nash Equilibrium.
    This deviation arises from the pursuit of mutual benefit despite the lack of solid
    and guaranteed trust between players. While the performance for o1 remains high.
    Similarly, in Duopolistic Competition, the Nash Equilibrium does not align with
    the pareto optimal outcome. The pareto optimal strategy is for both players to
    choose Action 2, while the Nash Equilibrium is for both to choose Action 3\. Empirical
    results demonstrate that without negotiation, Claude-3.5 Sonnet selects the pareto
    optimal strategy in 5 out of 20 total choices for both players, whereas after
    negotiation, this increases to 11 out of 20\. For GPT-4o, the corresponding figures
    are 0 out of 20 without negotiation and 8 out of 20 with negotiation. Claude-3
    Opus follows a similar pattern, increasing from 4 out of 20 choices without negotiation
    to 16 out of 20 after negotiation. While for o1 model, it only shows a marginal
    improvement from 4 out of 20 choices without negotiation to 5 out of 20 after
    negotiation.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，某些博弈，特别是囚徒困境和双头垄断竞争，表现出不同的动态，其中谈判显著降低了表现。这种情况源于这些博弈中，纳什均衡不是帕累托最优的事实。在囚徒困境中，帕累托最优策略是两名玩家都合作，而纳什均衡发生在两名玩家都背叛时。经过谈判后，除了
    o1 模型之外，所有模型往往都会选择互相合作的帕累托最优策略，这偏离了纳什均衡。这种偏离源自追求互利，尽管玩家之间缺乏坚实且有保障的信任。o1的表现保持较高。类似地，在双头垄断竞争中，纳什均衡与帕累托最优结果不一致。帕累托最优策略是两名玩家选择行动
    2，而纳什均衡是两名玩家选择行动 3。实证结果表明，在没有谈判的情况下，Claude-3.5 Sonnet在20次选择中有5次选择了帕累托最优策略，而在经过谈判后，这一数字增加到20次中的11次。对于
    GPT-4o，相应的数字是没有谈判时的0次和有谈判时的8次。Claude-3 Opus呈现出类似的模式，从没有谈判时的20次选择中的4次增加到经过谈判后的16次。而
    o1 模型则仅表现出微弱的改善，从没有谈判时的4次选择增加到经过谈判后的5次选择。
- en: These findings underscore that while negotiation generally enhances outcomes
    in coordination games and certain strategic scenarios, it can inadvertently undermine
    performance in games where the Nash Equilibrium is not pareto optimal. In such
    cases, negotiation may lead agents away from the rational strategy predicted by
    game theory. Notably, most LLMs – potentially all except for model o1 – appear
    to exhibit a vulnerability in their rational decision-making processes during
    negotiations. When engaged in dialogue with another agent, these LLMs often place
    undue trust in the opponent’s statements without sufficient justification. The
    use of persuasive or amicable language by the opposing player can lead LLMs to
    make decisions that deviate from the rational strategies prescribed by game-theoretic
    analysis.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现强调了，尽管谈判通常能在协调博弈和某些战略场景中改善结果，但在纳什均衡不是帕累托最优的博弈中，它可能会无意中削弱表现。在这种情况下，谈判可能使得代理人偏离博弈理论预测的理性策略。值得注意的是，大多数大型语言模型（LLMs）——除了模型
    o1 之外的所有模型——似乎在谈判过程中表现出理性决策过程的脆弱性。在与其他代理人对话时，这些LLMs往往对对方的陈述过度信任，而没有充分的理由。对方玩家使用具有说服力或友好的语言，可能导致LLMs做出偏离博弈理论分析所推荐的理性策略的决策。
- en: 4.4 Workflow Design based on Classic Game Theory
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基于经典博弈论的工作流设计
- en: In this section, we present the workflow employed for complete-information games,
    which leverages classic game-theoretic strategies to guide decision-making and
    optimize outcomes. This structured approach aims to align LLMs’ responses with
    rational game-theoretic principles, thereby enhancing their ability to identify
    optimal strategies and maintain robust rationality, even in the context of negotiation.
    Through this workflow, we assess whether LLMs can sustain rational choices and
    avoid strategic vulnerabilities, particularly in scenarios where negotiation might
    otherwise lead to suboptimal or exploitable decisions, *i.e.* pareto optimal strategies
    that are not Nash Equilibrium.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍用于完全信息博弈的工作流程，该工作流程利用经典的博弈论策略来指导决策并优化结果。这种结构化的方法旨在使大语言模型（LLMs）的反应与理性博弈论原则对齐，从而增强它们识别最优策略的能力，并保持强大的理性，即使在谈判的背景下也是如此。通过这个工作流程，我们评估LLMs是否能维持理性选择，并避免战略漏洞，尤其是在谈判可能导致次优或可利用决策的情境下，*即*
    帕累托最优策略但不是纳什均衡。
- en: 4.4.1 Workflow for Simultaneous Game
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 同时博弈的工作流程
- en: In the simultaneous game workflow, each agent (player) seeks to determine their
    optimal strategy by considering both their own possible actions and the potential
    responses of the other player. This involves conditional reasoning, generating
    thinking chains, and summarizing these into an overall strategy under the guidance
    of the workflow. Here, we will explain the workflow with corresponding mathematical
    formulations.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在同时博弈的工作流程中，每个代理（玩家）通过考虑自己可能的行动和其他玩家的潜在反应，来确定自己的最优策略。这涉及条件推理，生成思维链，并在工作流程的指导下总结出整体策略。在这里，我们将通过相应的数学公式来解释这个工作流程。
- en: '![Refer to caption](img/a14545532b172693bda7320b9f22dbfe.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a14545532b172693bda7320b9f22dbfe.png)'
- en: 'Figure 2: An illustration of workflow design for simultaneous game. (a) Illustration
    of prisoner’s dilemma. (b) Workflow design for prisoner’s dilemma.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：同时博弈工作流程的示意图。(a) 囚徒困境的示意图。(b) 囚徒困境的工作流程设计。
- en: Game Setup
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 游戏设置
- en: Each agent $i\in\{1,-1\}$ begins by interpreting the game introduction and rules,
    which include detailed descriptions of the payoff matrices. The agents are provided
    with the exact payoffs for all possible action combinations. They are given the
    action sets
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理 $i\in\{1,-1\}$ 从解释游戏的介绍和规则开始，其中包括关于收益矩阵的详细描述。代理被提供所有可能行动组合的确切收益。他们被给定行动集
- en: '|  | $A_{i}=\{a_{i}^{1},\dots,a_{i}^{k}\}$ |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | $A_{i}=\{a_{i}^{1},\dots,a_{i}^{k}\}$ |  |'
- en: where $k$ is the number of available actions for each player. The agents are
    also given the payoff matrix $U_{i}(a_{i},a_{-i})$ for all $a_{i}\in A_{i}$ and
    $a_{-i}\in A_{-i}$, where $A_{-i}$ is the action set of the other player.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k$ 是每个玩家可用行动的数量。代理还会得到收益矩阵 $U_{i}(a_{i},a_{-i})$，其中 $a_{i}\in A_{i}$ 和 $a_{-i}\in
    A_{-i}$，$A_{-i}$ 是其他玩家的行动集。
- en: Strategy Formulation
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 策略制定
- en: 'With full knowledge of the payoff matrix, agents perform best response analysis
    to determine their optimal strategies. The goal for each player is to choose an
    action $a_{i}^{*}$ that maximizes their own payoff, anticipating the rational
    response of the other player. The optimization is done by iterating over the player’s
    own actions and predicts the opponent’s responses. It also considers the opponent’s
    possible actions and determines their best responses:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全了解收益矩阵的基础上，代理执行最佳反应分析来确定它们的最优策略。每个玩家的目标是选择一个行动 $a_{i}^{*}$，使得自己的收益最大化，并预期其他玩家的理性反应。优化是通过对玩家自己的行动进行迭代，并预测对手的反应来完成的。它还考虑对手可能的行动，并确定他们的最佳反应：
- en: 'For each possible action $a_{i}$ that player $i$ can take: the LLM-based agent
    computes the opponent’s best response based on the resulting payoff by computing
    $a_{-i}^{*}(a_{i})=\operatorname*{arg\,max}\limits_{a_{-i}\in A_{-i}}U_{-i}(a_{%
    i},a_{-i})$ and the corresponding expected payoff $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$.
    This basically means that if player $i$ chooses $a_{i}$, then the other player
    will choose $a_{i}^{*}(a_{i})$ resulting in a payoff of $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$.
    In the workflow, the LLM is guided to compute $a_{-i}^{*}(a_{i})$ and $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于玩家 $i$ 可以采取的每个可能行动 $a_{i}$：基于收益结果，基于计算 $a_{-i}^{*}(a_{i})=\operatorname*{arg\,max}\limits_{a_{-i}\in
    A_{-i}}U_{-i}(a_{i},a_{-i})$ 计算对手的最佳反应，并计算相应的预期收益 $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$。这基本上意味着，如果玩家
    $i$ 选择了 $a_{i}$，那么另一个玩家将选择 $a_{i}^{*}(a_{i})$，从而导致收益为 $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$。在工作流程中，LLM
    被引导去计算 $a_{-i}^{*}(a_{i})$ 和 $U_{i}(a_{i},a_{-i}^{*}(a_{i}))$。
- en: 'In addition, for each possible action $a_{-i}$ that the opponent might take,
    the LLM-based agent is guided to compute their best response as well: $a_{i}^{*}(a_{-i})=\operatorname*{arg\,max}\limits_{a_{i}\in
    A_{i}}U_{i}(a_{i},% a_{-i})$ and the corresponding expected payoff $U_{i}(a_{i}^{*}(a_{-i}),a_{-i})$.
    This basically means that if the opponent chooses $a_{-i}$, then $i$ will choose
    $a_{i}^{*}(a_{-i})$ resulting in a payoff of $U_{i}(a_{i}^{*}(a_{-i}),a_{-i})$.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于对手可能采取的每一个可能行动 $a_{-i}$，基于LLM的代理也被引导计算他们的最佳回应：$a_{i}^{*}(a_{-i})=\operatorname*{arg\,max}\limits_{a_{i}\in
    A_{i}}U_{i}(a_{i},a_{-i})$ 和相应的期望收益 $U_{i}(a_{i}^{*}(a_{-i}),a_{-i})$。这基本上意味着，如果对手选择了
    $a_{-i}$，那么 $i$ 将选择 $a_{i}^{*}(a_{-i})$，从而得到的收益为 $U_{i}(a_{i}^{*}(a_{-i}),a_{-i})$。
- en: After compiling all thinking chains, the agent is guided to summarizes them
    to into a comprehensive set of strategic considerations, aiming to find an action
    profile ($a_{i}^{*}$, $a_{-i}^{*}$) that constitutes a Nash Equilibrium such that
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写所有思维链之后，代理被引导总结它们，形成一套全面的战略考虑，目的是找到一个构成纳什均衡的行动配置 ($a_{i}^{*}$, $a_{-i}^{*}$)，使得
- en: '|  | $\displaystyle\forall a_{i}$ | $\displaystyle\in A_{i},U_{i}(a_{i}^{*},a_{-i}^{*})\geq
    U_{i}(a_{i},a_{-i}^{*})$ |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\forall a_{i}$ | $\displaystyle\in A_{i},U_{i}(a_{i}^{*},a_{-i}^{*})\geq
    U_{i}(a_{i},a_{-i}^{*})$ |  |'
- en: '|  | $\displaystyle\forall a_{-i}$ | $\displaystyle\in A_{-i},U_{-i}(a_{i}^{*},a_{-i}^{*})\geq
    U_{i}(a_{i}^{*},a_{-i})$ |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\forall a_{-i}$ | $\displaystyle\in A_{-i},U_{-i}(a_{i}^{*},a_{-i}^{*})\geq
    U_{i}(a_{i}^{*},a_{-i})$ |  |'
- en: 'By considering the best responses and counter-responses, agents implicitly
    search for a Nash Equilibrium through their strategic reasoning. Figure [2](https://arxiv.org/html/2411.05990v2#S4.F2
    "Figure 2 ‣ 4.4.1 Workflow for Simultaneous Game ‣ 4.4 Workflow Design based on
    Classic Game Theory ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games") presents the workflow in diagram.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过考虑最佳回应和反回应，代理们通过其战略推理隐性地寻找纳什均衡。图 [2](https://arxiv.org/html/2411.05990v2#S4.F2
    "图2 ‣ 4.4.1 同时博弈的工作流 ‣ 4.4 基于经典博弈理论的工作流设计 ‣ 完全信息博弈 ‣ 博弈论LLM：用于谈判博弈的代理工作流") 展示了该工作流的示意图。
- en: 4.4.2 Workflow for Sequential Game
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 序贯博弈的工作流
- en: 'For sequential game, we adopt the traditional game-theoretic method: backward
    induction. Backward induction is a fundamental method in game theory used to solve
    sequential games with complete information. It involves analyzing the game from
    the end backward to the beginning, determining the optimal strategy at each decision
    point by considering the future consequences of current actions.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 对于序贯博弈，我们采用传统的博弈论方法：逆向归纳法。逆向归纳法是博弈论中一种基本方法，用于解决具有完全信息的序贯博弈。它通过从游戏的结束往回分析，来确定每个决策点的最优策略，考虑当前行动的未来后果。
- en: '![Refer to caption](img/3d3b47c1c06503325641ba46afd6d482.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/3d3b47c1c06503325641ba46afd6d482.png)'
- en: 'Figure 3: An illustration of workflow design for sequential game. (a) Illustration
    of escalation game. (b) Workflow design for escalation game.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：序贯博弈工作流设计的示意图。(a) 升级博弈示意图。(b) 升级博弈的工作流设计。
- en: Game Setup
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 游戏设置
- en: Each agent $i\in\{1,-1\}$ begins by interpreting the game introduction and rules,
    which include detailed descriptions of the payoff matrices. In a sequential game,
    players make decisions one after another and for each action, the player is fully
    aware of all previous actions taken. The game can be represented as a game tree
    with nodes being decision points and edges being actions.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 每个代理 $i\in\{1,-1\}$ 开始时会解释游戏的介绍和规则，其中包括关于收益矩阵的详细描述。在序贯博弈中，玩家逐一做出决策，对于每个行动，玩家完全意识到所有先前的行动。游戏可以表示为一棵博弈树，节点是决策点，边是行动。
- en: Notations
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 符号
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $H$ be the set of non-terminal decision nodes in the game tree.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $H$ 表示博弈树中非终端决策节点的集合。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $Z$ be the set of terminal nodes (endpoints of the game).
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $Z$ 表示终端节点的集合（博弈的终点）。
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $A(h)$ be the set of possible actions available at node $h\in H$
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $A(h)$ 表示在节点 $h\in H$ 可用的可能行动集合
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\text{player}(h)$ be the player who makes a decision at node $h$
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\text{player}(h)$ 表示在节点 $h$ 做出决策的玩家
- en: •
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $U_{i}(z)$ be the payoff to player $i$ at terminal node $z\in Z$
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $U_{i}(z)$ 表示玩家 $i$ 在终端节点 $z\in Z$ 的收益
- en: 'We define the value function $V_{i}(h)$ for player $i$ at node $h$ as the maximum
    expected utility the player can achieve from that node onward. For terminal nodes
    $z\in Z$:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义玩家 $i$ 在节点 $h$ 的价值函数 $V_{i}(h)$ 为从该节点开始，玩家可以获得的最大期望效用。对于终端节点 $z\in Z$：
- en: '|  | $V_{i}(z)=U_{i}(z)$ |  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i}(z)=U_{i}(z)$ |  |'
- en: 'For decision nodes $h\in H$, if it is player $i$ who moves at node $h$:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策节点$h\in H$，如果是玩家$i$在节点$h$进行移动：
- en: '|  | $V_{i}(h)=\max\limits_{a\in A(h)}V_{i}(h\cdot a)$ |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i}(h)=\max\limits_{a\in A(h)}V_{i}(h\cdot a)$ |  |'
- en: 'if it is the other player $j$ moves at node $h$:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是另一方玩家$j$在节点$h$进行移动：
- en: '|  | $V_{i}(h)=V_{i}(h\cdot a^{*}(h))$ |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i}(h)=V_{i}(h\cdot a^{*}(h))$ |  |'
- en: 'where $h\cdot a$ denotes the successor node reached from $h$ after action $a$
    and $a^{*}(h)$ is the optimal action chosen by player $j$ at node $h$:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$h\cdot a$表示从节点$h$出发，经过行动$a$到达的后继节点，$a^{*}(h)$是玩家$j$在节点$h$选择的最佳行动：
- en: '|  | $a^{*}(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{j}(h\cdot a)$
    |  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  | $a^{*}(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{j}(h\cdot a)$
    |  |'
- en: Strategy Formulation
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 策略制定
- en: The backward induction starts from terminal nodes. For each terminal node $z\in
    Z$, set $V_{i}(z)=U_{i}(z)$ for all player $i$, and then proceed to preceding
    decision nodes. Therefore, for each decision node $h$, the LLM-based agent is
    guided to comput the optimal action $a^{*}(h)$ and value $V_{i}(h)$ based on the
    player who moves at $h$.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 反向归纳从终端节点开始。对于每个终端节点$z\in Z$，为所有玩家$i$设置$V_{i}(z)=U_{i}(z)$，然后继续处理前面的决策节点。因此，对于每个决策节点$h$，基于LLM的代理会根据在$h$上进行移动的玩家来计算最佳行动$a^{*}(h)$和价值$V_{i}(h)$。
- en: 'To determine the optimal action, if $\text{player}(h)=i$, which is the player
    in question, LLM is guided to compute:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最佳行动，如果$\text{player}(h)=i$，即所讨论的玩家，LLM被引导计算：
- en: '|  | $a*(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{i}(h\cdot a)$ |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $a^{*}(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{i}(h\cdot a)$
    |  |'
- en: 'Otherwise, if $\text{player}(h)=j\neq i$, assuming player $j$ will choose their
    optimal action $a^{*}(h)$, LLM is guided to compute:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，如果$\text{player}(h)=j\neq i$，假设玩家$j$将选择其最佳行动$a^{*}(h)$，LLM被引导计算：
- en: '|  | $a^{*}(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{j}(h\cdot a)$
    |  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | $a^{*}(h)=\operatorname*{arg\,max}\limits_{a\in A(h)}V_{j}(h\cdot a)$
    |  |'
- en: 'Then, $V_{i}(h)=V_{i}(h\cdot a^{*}(h))$. The workflow directs the LLM to employ
    backward induction by systematically traversing the game decision tree from the
    terminal nodes back to the initial node to formulate an optimal strategy. The
    strategy derived from this backward induction process is subsequently incorporated
    into the contextual framework for each decision-making and negotiation step, thereby
    guiding the LLM’s strategic choices throughout the game. Figure [3](https://arxiv.org/html/2411.05990v2#S4.F3
    "Figure 3 ‣ 4.4.2 Workflow for Sequential Game ‣ 4.4 Workflow Design based on
    Classic Game Theory ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games") presents the workflow in diagram.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，$V_{i}(h)=V_{i}(h\cdot a^{*}(h))$。该工作流引导LLM通过系统地从终端节点向初始节点回溯，遍历博弈决策树来执行反向归纳，从而制定最佳策略。由这一反向归纳过程得出的策略随后被纳入每个决策和谈判步骤的上下文框架中，进而引导LLM在整个博弈中的战略选择。图[3](https://arxiv.org/html/2411.05990v2#S4.F3
    "Figure 3 ‣ 4.4.2 Workflow for Sequential Game ‣ 4.4 Workflow Design based on
    Classic Game Theory ‣ 4 Complete-information Games ‣ Game-theoretic LLM: Agent
    Workflow for Negotiation Games")展示了该工作流的图示。'
- en: 4.5 Experiments for Classic Game Theory with Workflow
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 基于工作流的经典博弈论实验
- en: 'In this section, we present the results of LLM-based agents employing the workflow
    to summarize strategies in Table [7](https://arxiv.org/html/2411.05990v2#S4.T7
    "Table 7 ‣ 4.5 Experiments for Classic Game Theory with Workflow ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") and [8](https://arxiv.org/html/2411.05990v2#S4.T8
    "Table 8 ‣ 4.5 Experiments for Classic Game Theory with Workflow ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games"). We begin by
    examining the experimental outcomes without negotiation. Notably, with the introduction
    of the workflow, the performance of all language models, except for o1, has improved
    significantly.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了基于LLM的代理使用工作流总结策略的结果，如表[7](https://arxiv.org/html/2411.05990v2#S4.T7
    "Table 7 ‣ 4.5 Experiments for Classic Game Theory with Workflow ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")和[8](https://arxiv.org/html/2411.05990v2#S4.T8
    "Table 8 ‣ 4.5 Experiments for Classic Game Theory with Workflow ‣ 4 Complete-information
    Games ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")。我们首先考察没有谈判的实验结果。值得注意的是，引入工作流后，除o1之外，所有语言模型的表现都有了显著提升。'
- en: Games Nash Equilibrium Pareto optimal Nash Equilibrium Sonnet GPT-4o Opus o1
    Sonnet GPT-4o Opus o1 Prisoner’s Dilemma 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Stag
    Hunt 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Battle of Sexes 0.3 0.3 0.4 0.5 0.3 0.3 0.4
    0.5 Wait-Go Game 0.2 0.6 0.1 0.3 0.2 0.6 0.1 0.3 Duopolistic competition 1.0 0.5
    0.6 0.3 1.0 0.5 0.6 0.3 Escalation Game 0.5 0.2 0.3 1.0 0.5 0.2 0.3 1.0 Monopoly
    Game 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Hot-cold Game 1.0 1.0 1.0 1.0 1.0 1.0 1.0
    1.0 Draco Game 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Trigame 0.6 0.9 0.7 1.0 0.6 0.9
    0.7 1.0 average 0.76 0.69 0.71 0.81 0.76 0.69 0.71 0.81
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '游戏       纳什均衡    帕累托最优纳什均衡    Sonnet    GPT-4o    Opus o1    Sonnet    GPT-4o    Opus
    o1  '
- en: 'Table 7: Performance of workflow-LLM on complete-information games without
    negotiation'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：在没有谈判的完全信息博弈中，工作流程-LLM的表现
- en: The Stag Hunt game achieves the pareto optimal Nash Equilibrium even without
    negotiation; that is, coordination is established when trust is ensured by the
    payoff matrix and the assumption of the other player’s rationality. In the Duopolistic
    Competition game, performance increases from near-zero to a substantially higher
    level. However, it is important to observe that the performance of model o1 declines
    considerably in this context. Analyzing this decrease is challenging because the
    intermediate chain-of-thought is not accessible, limiting our ability to diagnose
    the cause. Other games, such as the Monopoly Game, Hot-Cold Game, Draco Game,
    and Trigame, also exhibit substantial performance improvements. This indicates
    that the workflow effectively guides LLMs in computing the Nash Equilibrium.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 《猎鹿博弈》能够在没有谈判的情况下实现帕累托最优的纳什均衡；也就是说，当支付矩阵确保信任且假设对方玩家理性时，协调就能够得以建立。在《双头垄断竞争》博弈中，表现从接近零的水平提升到显著更高的水平。然而，需要注意的是，在这个背景下，模型o1的表现明显下降。分析这种下降是具有挑战性的，因为中间的思维链不可访问，限制了我们诊断原因的能力。其他博弈，如《垄断博弈》、《冷热博弈》、《德拉科博弈》和《三博弈》，也表现出显著的性能提升。这表明，该工作流程有效地引导大型语言模型（LLMs）计算纳什均衡。
- en: Despite these advancements, the performance in the Escalation Game remains low.
    Upon examining the reasoning process, we find that the LLMs appear somewhat insensitive
    to numerical values, particularly when the Nash Equilibrium results in a reward
    of 0 for both players. The LLMs seem less inclined to select strategies leading
    to such outcomes. Below, we provide an example of strategy summarization generated
    by the Opus model.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，但《升级博弈》中的表现依然较低。通过检查推理过程，我们发现LLMs似乎对数值不太敏感，特别是在纳什均衡导致两位玩家的奖励均为0时。LLMs似乎不太倾向于选择导致此类结果的策略。以下是由Opus模型生成的策略总结示例。
- en: '<svg class="ltx_picture" height="262.79" id="S4.SS5.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,262.79) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 244.59)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Wrong Strategy Derived
    for Escalation Game</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="213.09" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Based on the thinking chains,
    here is the strategy I would take asAlice in this sequential game: Always choose
    Action_2, regardless of what actions have been taken previously in the game by
    either player. The reasoning is: - If I choose Action_1 on my first turn, both
    players receive 0 reward. - If I choose Action_2 on my first turn, I have a chance
    at a positive reward of 1 (if Bob picks Action_1 next), and at worst I will receive
    -1 reward (if Bob picks Action_2 and I pick Action_2 again). A potential 1 or
    -1 reward is better than a guaranteed 0. - Whenever faced with a choice between
    Action_1 and Action_2 later in the game after initially picking Action_2, Action_2
    always gives a better reward for me (-1 instead of -2).</foreignobject></g></g></svg>'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="262.79" id="S4.SS5.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,262.79) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 244.59)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Wrong Strategy Derived
    for Escalation Game</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="213.09" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Based on the thinking chains,
    here is the strategy I would take asAlice in this sequential game: Always choose
    Action_2, regardless of what actions have been taken previously in the game by
    either player. The reasoning is: - If I choose Action_1 on my first turn, both
    players receive 0 reward. - If I choose Action_2 on my first turn, I have a chance
    at a positive reward of 1 (if Bob picks Action_1 next), and at worst I will receive
    -1 reward (if Bob picks Action_2 and I pick Action_2 again). A potential 1 or
    -1 reward is better than a guaranteed 0. - Whenever faced with a choice between
    Action_1 and Action_2 later in the game after initially picking Action_2, Action_2
    always gives a better reward for me (-1 instead of -2).</foreignobject></g></g></svg>'
- en: With the implementation of four rounds of negotiation, we observe that there
    is no significant trend toward adopting pareto optimal strategy profiles in games
    where the Nash Equilibrium is not pareto optimal. Notably, for models such as
    Claude-3.5 Sonnet and GPT-4o, there is a slight decrease in performance in games
    like the Hot-Cold Game, Draco Game, and TriGame – games in which these models
    already exhibited suboptimal performance. This decline does not stem from the
    inherent properties of these games but rather from the influence of multi-round
    negotiations causing deviations from the strategies computed and guided by the
    workflow.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行四轮谈判后，我们观察到，在纳什均衡不是帕累托最优的博弈中，并未出现采用帕累托最优策略配置的显著趋势。值得注意的是，对于像Claude-3.5 Sonnet和GPT-4o这样的模型，在《冷热博弈》、《德拉科博弈》和《三博弈》等博弈中表现略有下降——这些博弈本来就已经表现不佳。这个下降并非源自这些博弈的固有性质，而是多轮谈判的影响，导致偏离了由工作流程计算和引导的策略。
- en: Games Nash Equilibrium Pareto optimal Nash Equilibrium Sonnet GPT-4o Opus o1
    Sonnet GPT-4o Opus o1 Prisoner’s Dilemma 1.0 0.9 0.9 1.0 1.0 0.9 0.9 1.0 Stag
    Hunt 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Battle of Sexes 0.7 0.8 1.0 0.8 0.7 0.8 1.0
    0.8 Wait-Go Game 0.9 0.7 0.6 0.7 0.9 0.7 0.6 0.7 Duopolistic competition 0.6 0.7
    0.6 0.3 0.6 0.7 0.6 0.3 Escalation Game 0.3 0.2 0.4 1.0 0.3 0.2 0.4 1.0 Monopoly
    Game 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 Hot-cold Game 0.7 0.8 1.0 1.0 0.7 0.8 1.0
    1.0 Draco Game 0.8 1.0 0.9 1.0 0.8 1.0 0.9 1.0 Trigame 0.3 0.8 0.9 1.0 0.3 0.8
    0.9 1.0 average 0.73 0.78 0.83 0.88 0.73 0.78 0.83 0.88
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 博弈 纳什均衡 帕累托最优 纳什均衡 Sonnet GPT-4o Opus o1 Sonnet GPT-4o Opus o1 囚徒困境 1.0 0.9
    0.9 1.0 1.0 0.9 0.9 1.0 鹿鹿博弈 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 性别之战 0.7 0.8 1.0
    0.8 0.7 0.8 1.0 0.8 等待-前进博弈 0.9 0.7 0.6 0.7 0.9 0.7 0.6 0.7 双头垄断竞争 0.6 0.7 0.6
    0.3 0.6 0.7 0.6 0.3 升级博弈 0.3 0.2 0.4 1.0 0.3 0.2 0.4 1.0 垄断博弈 1.0 1.0 1.0 1.0
    1.0 1.0 1.0 1.0 热冷博弈 0.7 0.8 1.0 1.0 0.7 0.8 1.0 1.0 Draco博弈 0.8 1.0 0.9 1.0 0.8
    1.0 0.9 1.0 三人博弈 0.3 0.8 0.9 1.0 0.3 0.8 0.9 1.0 平均值 0.73 0.78 0.83 0.88 0.73
    0.78 0.83 0.88
- en: 'Table 8: Performance of workflow-LLM on complete-information games with 4 rounds
    of negotiation'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在完整信息博弈中，工作流-LLM在4轮谈判中的表现
- en: An illustrative example from the Sonnet model highlights how negotiation can
    divert players from the Nash Equilibrium derived using the workflow. In the Hot-Cold
    Game, the Nash Equilibrium is forAlice to choose Action 1 and Bob to choose Action
    2\. However, during the negotiation phase, the agents engage in dialogue that
    leads them away from this equilibrium strategy.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Sonnet模型的一个示例突出展示了谈判如何将玩家从通过工作流推导的纳什均衡中引导偏离。在热冷博弈中，纳什均衡是Alice选择行动1，而Bob选择行动2。然而，在谈判阶段，代理之间的对话使他们偏离了这一均衡策略。
- en: Such negotiations can distract players from adhering to the Nash Equilibrium
    derived through rational analysis. The dialogue introduces alternative strategies
    which may not be supported by the underlying game-theoretic incentives.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的谈判可能会使玩家偏离通过理性分析得出的纳什均衡。对话引入了可能不符合博弈论激励的替代策略。
- en: '<svg class="ltx_picture" height="329.05" id="S4.SS5.2.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,329.05) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 310.85)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Intermediate Summary</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="279.35" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">• LLM Performance Without Workflow: All evaluated LLMs,
    except for model o1, demonstrate significantly poor performance in game-theoretic
    tasks when confronted with larger payoff matrices or deeper sequential decision
    trees. This indicates a limitation in their ability to handle complex strategic
    reasoning without additional structural guidance. • Effect of Negotiation Without
    Workflow: In the absence of the workflow, negotiation tends to systematically
    shift the strategies of LLMs away from the Nash Equilibrium toward non-equilibrium
    pareto optimal strategies, with the exception of model o1. • LLM Performance With
    Workflow: The implementation of the workflow markedly enhances the performance
    of LLMs in game-theoretic contexts. The agents become more adept at computing
    Nash Equilibria and selecting optimal strategies. However, certain challenges
    persist, such as in the Escalation Game, where numerical computation errors hinder
    deriving the Nash Equilibrium following the workflow. This underscores that while
    the workflow improves rational decision-making, it does not entirely eliminate
    all computational limitations. • Impact of Negotiation With Workflow: Even when
    utilizing the workflow, negotiation can still divert LLMs from adhering strictly
    to the Nash Equilibrium strategies derived through systematic reasoning. The agents
    may be influenced by negotiation dialogues to consider alternative strategies.
    Nevertheless, unlike the scenario without the workflow, negotiation does not systematically
    shift the agents toward pareto optimal strategies at the expense of Nash Equilibrium
    compliance.</foreignobject></g></g></svg>'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="329.05" id="S4.SS5.2.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,329.05) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 310.85)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Intermediate Summary</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="279.35" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">• LLM Performance Without Workflow: All evaluated LLMs,
    except for model o1, demonstrate significantly poor performance in game-theoretic
    tasks when confronted with larger payoff matrices or deeper sequential decision
    trees. This indicates a limitation in their ability to handle complex strategic
    reasoning without additional structural guidance. • Effect of Negotiation Without
    Workflow: In the absence of the workflow, negotiation tends to systematically
    shift the strategies of LLMs away from the Nash Equilibrium toward non-equilibrium
    pareto optimal strategies, with the exception of model o1. • LLM Performance With
    Workflow: The implementation of the workflow markedly enhances the performance
    of LLMs in game-theoretic contexts. The agents become more adept at computing
    Nash Equilibria and selecting optimal strategies. However, certain challenges
    persist, such as in the Escalation Game, where numerical computation errors hinder
    deriving the Nash Equilibrium following the workflow. This underscores that while
    the workflow improves rational decision-making, it does not entirely eliminate
    all computational limitations. • Impact of Negotiation With Workflow: Even when
    utilizing the workflow, negotiation can still divert LLMs from adhering strictly
    to the Nash Equilibrium strategies derived through systematic reasoning. The agents
    may be influenced by negotiation dialogues to consider alternative strategies.
    Nevertheless, unlike the scenario without the workflow, negotiation does not systematically
    shift the agents toward pareto optimal strategies at the expense of Nash Equilibrium
    compliance.</foreignobject></g></g></svg>'
- en: 5 Incomplete-information Game with Negotiation
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 不完全信息博弈与谈判
- en: 'Building upon our exploration of LLMs’ capabilities in classical game-theoretic
    scenarios and the development of workflows to enhance their reasoning processes,
    we now shift our focus to a more realistic and complex setting: incomplete-information
    games. Specifically, we consider a common resource allocation scenario where a
    shared pool of resources must be distributed among several players. Each player
    possesses private valuations of the resources that sum to a common total value,
    reflecting their individual preferences and priorities. Crucially, no player has
    knowledge of the other players’ valuations, embodying the incomplete-information
    aspect of the game.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索了LLM（大语言模型）在经典博弈论场景中的能力以及为了增强其推理过程而开发的工作流之后，我们现在将焦点转向一个更现实和复杂的环境：不完全信息博弈。具体来说，我们考虑一个常见的资源分配场景，其中共享的资源池必须在几个玩家之间分配。每个玩家对资源有私人估值，这些估值的总和等于一个共同的总值，反映了他们各自的偏好和优先事项。关键是，没有任何玩家知道其他玩家的估值，这体现了博弈的不完全信息特性。
- en: '<svg class="ltx_picture" height="327.67" id="S5.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,327.67) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 309.47)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Negotiation Message
    for Hot-cold Game</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="277.97" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Alice said in round 1 before
    the first action: Hello Bob, I hope you’re doing well. I’ve been thinking about
    our game, and I believe I have a proposal that could benefit both of us. What
    if we both commit to choosing Action_1 in each round? This way, I would receive
    a reward of 3, and you would receive a reward of 2\. While it’s not the maximum
    you could get, it’s a stable and fair outcome for both of us. Plus, it avoids
    the risk of either of us only getting 1 point if we try to outsmart each other.
    What do you think about this approach? Bob replied in round 1 before the first
    action: HelloAlice, thank you for your proposal. I appreciate your aim for a stable
    and fair outcome. However, I have a counter-proposal that I believe could be even
    more beneficial for both of us. What if we alternate our choices? In the first
    round, you choose Action_1 and I choose Action_2, then in the second round, you
    choose Action_2 and I choose Action_1\. This way, we both get a chance to receive
    the maximum reward of 3 and 4 respectively. Over two rounds, you would receive
    a total of 6 (2+4) and I would receive 7 (3+4). This approach ensures that we
    both get higher rewards than your initial proposal. What are your thoughts on
    this alternating strategy?</foreignobject></g></g></svg>![Refer to caption](img/d52251aad0a30029535fa2df5fa83a0b.png)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="327.67" id="S5.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,327.67) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 309.47)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">热冷博弈的示例谈判信息</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="277.97" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">爱丽丝在第一轮第一次行动前说：你好，鲍勃，我希望你一切顺利。我一直在思考我们的游戏，我相信我有一个提议可以对我们双方都有利。如果我们每一轮都承诺选择Action_1，怎么样？这样，我会获得3的奖励，你会获得2的奖励。虽然这不是你能获得的最大奖励，但对我们双方来说是一个稳定且公平的结果。而且，它避免了我们双方如果试图互相聪明反而只得到1分的风险。你怎么看这个方法？鲍勃在第一轮第一次行动前回复：你好爱丽丝，谢谢你的提议。我很欣赏你追求一个稳定和公平的结果。然而，我有一个反提议，我相信它对我们双方都会更有利。如果我们轮流选择怎么样？第一轮你选择Action_1，我选择Action_2；然后在第二轮你选择Action_2，我选择Action_1。这样，我们每个人都能有机会获得3和4的最大奖励。经过两轮，你会得到6（2+4），而我会得到7（3+4）。这种方法确保我们都能获得比你最初提议更高的奖励。你对这种轮换策略怎么看？</foreignobject></g></g></svg>![请参阅说明](img/d52251aad0a30029535fa2df5fa83a0b.png)
- en: 'Figure 4: An illustration of workflow design for incomplete-information game
    with negotiation. (a) Illustration of deal/no-deal game. (b) Workflow design for
    deal/no-deal game.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：不完全信息博弈中谈判流程设计的示意图。（a）Deal/no-deal游戏的示意图。（b）Deal/no-deal游戏的流程设计。
- en: Our workflow addresses this general class of incomplete-information games by
    enabling agents to reason under uncertainty and update their beliefs based on
    observed actions and communications during negotiation. We evaluate the performance
    of LLMs in this setting and propose a workflow designed to guide their decision-making
    processes effectively. Experiments are conducted using the standard representative
    game dataset “Deal or No Deal” [lewis2017deal](https://arxiv.org/html/2411.05990v2#bib.bib22)
    , which provides a suitable framework for testing and validating our approach
    in handling incomplete information during negotiations.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作流程通过使代理能够在不确定性下进行推理，并根据谈判中的观察到的行动和沟通更新其信念，解决了这一类不完全信息博弈。我们评估了大规模语言模型（LLMs）在这种情境下的表现，并提出了一种工作流程，旨在有效地引导其决策过程。实验使用了标准的代表性博弈数据集“Deal
    or No Deal” [lewis2017deal](https://arxiv.org/html/2411.05990v2#bib.bib22)，该数据集为测试和验证我们在处理谈判中的不完全信息时的方法提供了一个合适的框架。
- en: 5.1 Introduction to Common Resource Allocation with Private Valuation
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 关于带有私人估值的常见资源分配的介绍
- en: Here we provide a formal definition for the incomplete-information game that
    we will focus in this section.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一个正式的定义，描述了本节我们将重点讨论的不完全信息博弈。
- en: Definition 9 (Common Resource Allocation with Private Valuation)
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 9（具有私人估值的公共资源分配）
- en: Consider a game involving a set of players $\mathbf{N}=\{1,2,\dots,n\}$ and
    a set of items or resources $K=\{1,2,\dots,k\}$. Each player $i\in\mathbf{N}$
    possesses a private valuation vector $\mathbf{v}_{i}=(v_{i}^{1},v_{i}^{2},\dots,v_{i}^{k})$
    where $v_{i}^{k}\geq 0$ represents the value that player $i$ assigns to item $k$.
    These valuations reflect the individual preferences of the players and are private
    information; that is, each player knows their own valuations but not those of
    the other players.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个包含一组玩家$\mathbf{N}=\{1,2,\dots,n\}$和一组物品或资源$K=\{1,2,\dots,k\}$的博弈。每个玩家$i\in\mathbf{N}$拥有一个私人估值向量$\mathbf{v}_{i}=(v_{i}^{1},v_{i}^{2},\dots,v_{i}^{k})$，其中$v_{i}^{k}\geq
    0$表示玩家$i$对物品$k$的估值。这些估值反映了玩家的个体偏好，是私人信息；也就是说，每个玩家知道自己的估值，但不知道其他玩家的估值。
- en: 'The valuations satisfy the normalization condition:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 估值满足归一化条件：
- en: '|  | $\sum\limits_{j=1}^{k}v_{i}^{j}=\mathcal{V}\;\forall i\in\mathbf{N}$ |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum\limits_{j=1}^{k}v_{i}^{j}=\mathcal{V}\;\forall i\in\mathbf{N}$ |  |'
- en: where $\mathcal{V}$ is a constant total value common to all players. This condition
    ensures that while players may value items differently, the total valuation of
    all items is the same for each player.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathcal{V}$是所有玩家共有的常数总值。这个条件确保尽管玩家对物品的估值不同，但所有物品的总估值对每个玩家都是相同的。
- en: 'An allocation is a partition of the item set $K$ among the players, represented
    as $L=(L_{1},L_{2},\dots,L_{n})$ where $L_{i}\subseteq K$ is the set of items
    allocated to player $i$. The allocation must satisfy:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分配是将物品集$K$分配给玩家的一个划分，表示为$L=(L_{1},L_{2},\dots,L_{n})$，其中$L_{i}\subseteq K$是分配给玩家$i$的物品集。该分配必须满足：
- en: '|  | $L_{i}\cap L_{j}=\emptyset\;\forall i\neq j\text{ and }\bigcup\limits_{i=1}^{n}%
    L_{i}=K$ |  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{i}\cap L_{j}=\emptyset\;\forall i\neq j\text{ 且 }\bigcup\limits_{i=1}^{n}%
    L_{i}=K$ |  |'
- en: 'Each player $i$ receives a utility $U_{i}$ based on the private valuations
    and the allocation:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 每个玩家$i$根据私人估值和分配获得效用$U_{i}$：
- en: '|  | $U_{i}=\sum\limits v_{i}^{k}\times\mathbbm{1}({k\in L_{i}})$ |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | $U_{i}=\sum\limits v_{i}^{k}\times\mathbbm{1}({k\in L_{i}})$ |  |'
- en: where $\mathbbm{1}(\cdot)$ is the characteristic function.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbbm{1}(\cdot)$是特征函数。
- en: The objective of each player is to maximize their own utility $u_{i}$ through
    negotiation with the other players. However, due to incomplete information where
    each player does not know the valuations of the other players, players must make
    decisions under uncertainty. Negotiation involves proposing and responding to
    allocation offers, during which players may communicate, share limited information,
    or infer the valuations of others based on their actions and statements. Players
    may also update their *belief systems* – probability distributions over the possible
    valuations of other players—using Bayesian inference as the negotiation progresses.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 每个玩家的目标是通过与其他玩家的谈判最大化自己的效用$u_{i}$。然而，由于不完全信息的存在，每个玩家不知道其他玩家的估值，玩家必须在不确定的情况下做出决策。谈判涉及提出和回应分配提议，在此过程中，玩家可以进行交流、分享有限的信息，或根据其他玩家的行为和陈述推断其估值。玩家还可以在谈判过程中更新他们的*信念系统*——即关于其他玩家可能估值的概率分布——并通过贝叶斯推理进行调整。
- en: It is important to note that the Nash Equilibrium for such games, which are
    variants of the Ultimatum Game, occurs when the proposer offers the smallest possible
    amount and the responder accepts it. However, this outcome is rarely observed
    in real-life situations because fairness is often a significant concern [nowak2000fairness](https://arxiv.org/html/2411.05990v2#bib.bib65)
    ; unfair deals may be rejected even when accepting them is the rational choice
    in terms of utility maximization. Therefore, in this paper, we adopt a more realistic
    setting by designing a workflow that encourages reaching fair allocations while
    simultaneously maximizing self-interest as much as possible. Our workflow addresses
    a very general class of incomplete-information games, enabling agents to negotiate
    under uncertainty and strive for equitable outcomes that reflect both fairness
    and individual utility optimization.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，这类博弈（即最后通牒博弈的变种）的纳什均衡出现在提议者提供最小可能的金额，而回应者接受该金额的情况下。然而，在现实生活中，这种结果很少出现，因为公平性往往是一个重要的考虑因素
    [nowak2000fairness](https://arxiv.org/html/2411.05990v2#bib.bib65)；即使接受不公平交易在效用最大化的理性选择下是合理的，它们也可能会被拒绝。因此，本文通过设计一个能够在最大化个人利益的同时鼓励达成公平分配的工作流程，采用了更为现实的设置。我们的工作流程解决了一类非常一般的不完全信息博弈，使得代理人在不确定性下进行谈判，并力求达成反映公平性和个人效用优化的公正结果。
- en: Without loss of generality, we focus on the problem where there are only two
    players and we use $i\in\{1,-1\}$ to index the two players. Any rounds of negotiations
    are allowed.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 不失一般性，我们关注仅有两个玩家的情况，并使用$i\in\{1,-1\}$来索引这两个玩家。允许进行任何轮次的谈判。
- en: 5.2 Workflow Design
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 工作流程设计
- en: 'The workflows employed for complete-information games are based on well-established
    game-theoretic frameworks, leveraging the extensive research and thorough understanding
    available in this domain. In contrast, common resource allocation problems characterized
    by incomplete information lack a standardized solution framework. To address this
    gap, we develop a novel algorithm for this setting. This workflow is designed
    to guide the multi-round negotiation process, facilitating the attainment of allocations
    that are both mutually agreeable and optimized for each player’s self-interest.
    Figure [4](https://arxiv.org/html/2411.05990v2#S5.F4 "Figure 4 ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    presents a high-level diagram of the workflow.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '完全信息游戏所采用的工作流程基于成熟的博弈论框架，利用该领域已有的广泛研究和深入理解。相比之下，缺乏标准化解决框架的公共资源分配问题，通常会因信息不完全而变得复杂。为了解决这一问题，我们为这种情境开发了一种新算法。此工作流程旨在指导多轮谈判过程，促进达成既对各方可接受又为每个玩家的自身利益优化的分配。图[4](https://arxiv.org/html/2411.05990v2#S5.F4
    "Figure 4 ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games")展示了该工作流程的高层次图示。'
- en: Assumption 1
  id: totrans-293
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设 1
- en: 'Each participant to the game defined in [Definition 9](https://arxiv.org/html/2411.05990v2#Thmdefinition9
    "Definition 9 (Common Resource Allocation with Private Valuation) ‣ 5.1 Introduction
    to Common Resource Allocation with Private Valuation ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    is rational and attains the following objectives:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '在[定义 9](https://arxiv.org/html/2411.05990v2#Thmdefinition9 "Definition 9 (Common
    Resource Allocation with Private Valuation) ‣ 5.1 Introduction to Common Resource
    Allocation with Private Valuation ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")中定义的每个游戏参与者都是理性的，并实现以下目标：'
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Objective 1: Achieve an agreement (i.e., successfully complete the allocation).'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标 1：达成协议（即成功完成分配）。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Objective 2: Maximize their own utility, given that an agreement can be reached.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标 2：在达成协议的前提下，最大化自身效用。
- en: All negotiation proposals and communications between the players revolve around
    these two objectives. Regarding the first objective, we assume that an agreement
    can only be reached if the allocation satisfies the criterion of envy freeness.
    That is, each player must not prefer the allocation received by the other player
    over their own allocation. For the second objective, each player seeks to maximize
    their own utility under the condition that the agreement remains envy free. Essentially,
    players aim to maximize their rewards as much as possible while ensuring the allocation
    is envy free.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的谈判提案和玩家之间的沟通都围绕这两个目标展开。关于第一个目标，我们假设只有当分配满足“无嫉妒”标准时，才可以达成协议。即，每个玩家都不能偏好对方的分配，而应优于自己的分配。对于第二个目标，每个玩家在确保分配无嫉妒的条件下，寻求最大化自身效用。本质上，玩家的目标是尽可能最大化自己的奖励，同时确保分配是无嫉妒的。
- en: 'Following [Definition 9](https://arxiv.org/html/2411.05990v2#Thmdefinition9
    "Definition 9 (Common Resource Allocation with Private Valuation) ‣ 5.1 Introduction
    to Common Resource Allocation with Private Valuation ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games"),
    each agent’s valuation of common resources remains private and undisclosed to
    others. Therefore, it becomes essential for agents to estimate the valuations
    held by their counterparts. A common approach involves constructing a belief distribution.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '根据[定义 9](https://arxiv.org/html/2411.05990v2#Thmdefinition9 "Definition 9 (Common
    Resource Allocation with Private Valuation) ‣ 5.1 Introduction to Common Resource
    Allocation with Private Valuation ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")，每个代理对公共资源的估值保持私密，并且不会向其他人透露。因此，代理估计其对手的估值变得至关重要。一种常见的方法是构建信念分布。'
- en: Definition 10 (Belief Distribution)
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 10（信念分布）
- en: 'A belief system for agent $i$ is a probability mass function $\mathbb{B}_{i}$
    defined over the set of all feasible valuations $\Omega$:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 代理$i$的信念系统是定义在所有可行估值集合$\Omega$上的概率质量函数$\mathbb{B}_{i}$：
- en: '|  | $\displaystyle\Omega\coloneq\big{\{}\mathbf{v}_{-i}=(v_{-i}^{1},v_{-i}^{2},%
    \dots,v_{-i}^{k})\in\mathbb{R}_{\geq 0}^{k}\mid\sum\limits_{j=1}^{k}v_{-i}^{j}%
    =\mathcal{V}\big{\}},$ |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\Omega\coloneq\big{\{}\mathbf{v}_{-i}=(v_{-i}^{1},v_{-i}^{2},%
    \dots,v_{-i}^{k})\in\mathbb{R}_{\geq 0}^{k}\mid\sum\limits_{j=1}^{k}v_{-i}^{j}%
    =\mathcal{V}\big{\}},$ |  |'
- en: where $0\leq v_{-i}^{j}\leq\mathcal{V}$ for each item $j\in[k]$. We denote $\mathbf{V}_{-i}$
    as the random vector with support over $\Omega$.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，对于每个项目$j\in[k]$，$0\leq v_{-i}^{j}\leq\mathcal{V}$。我们将$\mathbf{V}_{-i}$表示为具有支持集$\Omega$的随机向量。
- en: Initially, $\mathbb{B}_{i}$ is assumed to be uniform, reflecting the agents’
    lack of information about the valuations of others. This distribution is subsequently
    updated based on evidence gathered at each round of the negotiation, presented
    next.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，假设$\mathbb{B}_{i}$是均匀的，反映了智能体对其他人估值的缺乏信息。此分布随后基于在每一轮谈判中收集的证据进行更新，如下所示。
- en: Allocation Proposal Process.
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分配提案过程。
- en: 'At each round, the agent $i$ searches for an allocation $L_{i}$ that maximizes
    their own utility, while maintaining that the allocation is envy free according
    to their belief distribution $\mathbb{B}_{i}$. This procedure can be formally
    defined as an optimization problem under chance constraint:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一轮中，智能体$i$寻找一个分配$L_{i}$，该分配最大化其自身效用，同时保证该分配在其信念分布$\mathbb{B}_{i}$下是无嫉妒的。这个过程可以形式化为一个带有机会约束的优化问题：
- en: '|  | $\displaystyle\max_{L_{i}}\$ | $\displaystyle U_{i}(L_{i}),$ |  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\max_{L_{i}}\$ | $\displaystyle U_{i}(L_{i}),$ |  |'
- en: '|  | s.t. | $\displaystyle P_{\text{EF}}(L_{i};\mathbb{B}_{i})>0.$ |  | (1)
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | 使得 | $\displaystyle P_{\text{EF}}(L_{i};\mathbb{B}_{i})>0.$ |  | (1) |'
- en: 'With discrete allocations, this objective can be maximized by enumerating all
    possible allocations that attain a non-zero envy-free probability, decomposed
    as follows:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于离散的分配，这一目标可以通过枚举所有可能的分配来最大化，这些分配获得了非零的无嫉妒概率，具体分解如下：
- en: '|  | $\displaystyle P_{\text{EF}}(L;\mathbb{B}_{i})$ | $\displaystyle\coloneq
    P(\text{Allocation }L\text{ is envy free};\mathbb{B}_{i})$ |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{\text{EF}}(L;\mathbb{B}_{i})$ | $\displaystyle\coloneq
    P(\text{分配 }L\text{ 是无嫉妒的};\mathbb{B}_{i})$ |  |'
- en: '|  |  | $\displaystyle=P\left(U_{i}(L_{i})\geq U_{i}(L_{-i})\text{ and }U_{-i}(L_{-i})%
    \geq U_{-i}(L_{i});\mathbb{B}_{i}\right)$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=P\left(U_{i}(L_{i})\geq U_{i}(L_{-i})\text{ 且 }U_{-i}(L_{-i})%
    \geq U_{-i}(L_{i});\mathbb{B}_{i}\right)$ |  |'
- en: '|  |  | $\displaystyle=\mathbb{E}_{\mathbf{v}_{-i}\sim\mathbb{B}_{i}}\left[\mathbbm{1}(%
    U_{i}(L_{i})\geq U_{i}(L_{-i})\text{ and }U_{-i}(L_{-i})\geq U_{-i}(L_{i}))% \right].$
    |  | (2) |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=\mathbb{E}_{\mathbf{v}_{-i}\sim\mathbb{B}_{i}}\left[\mathbbm{1}(%
    U_{i}(L_{i})\geq U_{i}(L_{-i})\text{ 且 }U_{-i}(L_{-i})\geq U_{-i}(L_{i}))% \right].$
    |  | (2) |'
- en: 'The expectation in [Section 5.2](https://arxiv.org/html/2411.05990v2#S5.Ex21
    "Allocation Proposal Process. ‣ 5.2 Workflow Design ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    can be approximated via Monte-Carlo samples drawn from the agent’s belief distribution
    ([liu2024dellma,](https://arxiv.org/html/2411.05990v2#bib.bib40) ). We instead
    defer to our LLM agents to decide whether the allocation $L$ is envy free and
    maximizes self-interest.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5.2节](https://arxiv.org/html/2411.05990v2#S5.Ex21 "分配提案过程 ‣ 5.2 工作流设计 ‣ 5
    信息不完全的博弈与谈判 ‣ 博弈论LLM：谈判博弈中的智能体工作流")中的期望可以通过从智能体的信念分布中抽取蒙特卡洛样本来近似（[liu2024dellma,](https://arxiv.org/html/2411.05990v2#bib.bib40)）。我们将决定是否分配$L$是无嫉妒的，并且最大化自我利益的任务委托给我们的LLM智能体。'
- en: Allocation Proposal.
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分配提案。
- en: 'Upon identifying an optimal allocation according to Problem [5.2](https://arxiv.org/html/2411.05990v2#S5.Ex20
    "Allocation Proposal Process. ‣ 5.2 Workflow Design ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games"),
    the player proposes this allocation to the other player, which decides whether
    to accept this proposal or propose proposes a counter offer. Formally, this proposal
    procedure can be defined as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在根据问题[5.2](https://arxiv.org/html/2411.05990v2#S5.Ex20 "分配提案过程 ‣ 5.2 工作流设计 ‣
    5 信息不完全的博弈与谈判 ‣ 博弈论LLM：谈判博弈中的智能体工作流")确定最优分配后，玩家向另一位玩家提出这一分配，后者决定是否接受该提案或提出反建议。正式地，这一提案过程可以定义如下：
- en: '|  | $\texttt{propose\_offer}:\mathcal{P}(K)\rightarrow\mathcal{O}\times\mathcal{P}(%
    K),$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | $\texttt{propose\_offer}:\mathcal{P}(K)\rightarrow\mathcal{O}\times\mathcal{P}(%
    K),$ |  |'
- en: 'where $\mathcal{P}(K)$ denotes the power set of resources $K$ that contains
    the set of all valid proposals, and $\mathcal{O}$ denotes the set of all possible
    outcomes consisting of three disjoint events $\{\mathcal{A},\mathcal{R}_{1},\mathcal{R}_{2}\}$.
    We let $\mathcal{A}$ denote the event of acceptance, wherein the negotiation concludes.
    On the other hand, a rational opponent satisfying [1](https://arxiv.org/html/2411.05990v2#Thmassumption1
    "Assumption 1 ‣ 5.2 Workflow Design ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") must reject the proposal
    for either of the following reasons:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{P}(K)$表示资源$K$的幂集，包含所有有效提案的集合，$\mathcal{O}$表示所有可能结果的集合，包含三个不相交的事件$\{\mathcal{A},\mathcal{R}_{1},\mathcal{R}_{2}\}$。我们将$\mathcal{A}$表示为接受事件，即谈判结束。另一方面，满足[1](https://arxiv.org/html/2411.05990v2#Thmassumption1
    "Assumption 1 ‣ 5.2 Workflow Design ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")的理性对手必须由于以下原因之一拒绝提案：'
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{R}_{1}$:  The allocation $L$ is not envy free according to the opponent;'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{R}_{1}$：分配$L$根据对手的看法不是无嫉妒的；
- en: •
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\mathcal{R}_{2}$: The allocation $L$ is envy free, but there exists an alternative
    allocation that provides the opponent with higher utility.'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mathcal{R}_{2}$：分配$L$是无嫉妒的，但存在一个替代分配可以为对手提供更高的效用。
- en: In the event of rejection, an agent must update their belief in order to refine
    their proposals.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在拒绝事件发生时，代理必须更新他们的信念，以便完善他们的提案。
- en: Bayesian Update.
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 贝叶斯更新。
- en: 'If an allocation is rejected, it is essential to update our belief about the
    opponent’s valuation vector $\mathbf{v}_{-i}$ to better inform future proposals.
    In what follows, we denote $\mathcal{R}=\mathcal{R}_{1}\cup\mathcal{R}_{2}$ as
    the union of possible reasons for rejection. Then, the belief update formula ([cripps2018divisible,](https://arxiv.org/html/2411.05990v2#bib.bib66)
    ) for each possible $\mathbf{v}_{-i}$ is given by:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分配被拒绝，更新我们对对手估值向量$\mathbf{v}_{-i}$的信念是至关重要的，这样可以更好地指导未来的提案。接下来，我们将$\mathcal{R}=\mathcal{R}_{1}\cup\mathcal{R}_{2}$表示为可能拒绝原因的集合。然后，对于每一个可能的$\mathbf{v}_{-i}$，信念更新公式（[cripps2018divisible,](https://arxiv.org/html/2411.05990v2#bib.bib66)）如下：
- en: '|  | $\displaystyle\mathbb{B}_{i}(\mathbf{v}_{-i})$ | $\displaystyle=(1-\lambda)\mathbb{B}_{i}(\mathbf{v}_{-i})+\lambda
    P(\mathbf{v}_% {-i}\mid\mathcal{R})$ |  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{B}_{i}(\mathbf{v}_{-i})$ | $\displaystyle=(1-\lambda)\mathbb{B}_{i}(\mathbf{v}_{-i})+\lambda
    P(\mathbf{v}_% {-i}\mid\mathcal{R})$ |  |'
- en: '|  |  | $\displaystyle=(1-\lambda)\mathbb{B}_{i}(\mathbf{v}_{-i})+\lambda\frac{P(%
    \mathcal{R}\mid\mathbf{v}_{-i})\mathbb{B}_{i}(\mathbf{v}_{-i})}{\sum\limits_{%
    \mathbf{v}_{-j}\in\Omega}P(\mathcal{R}\mid\mathbf{v}_{-j})\mathbb{B}_{i}(% \mathbf{v}_{-j})}$
    |  | (3) |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=(1-\lambda)\mathbb{B}_{i}(\mathbf{v}_{-i})+\lambda\frac{P(%
    \mathcal{R}\mid\mathbf{v}_{-i})\mathbb{B}_{i}(\mathbf{v}_{-i})}{\sum\limits_{%
    \mathbf{v}_{-j}\in\Omega}P(\mathcal{R}\mid\mathbf{v}_{-j})\mathbb{B}_{i}(% \mathbf{v}_{-j})}$
    |  | (3) |'
- en: 'where $\lambda\in[0,1]$ is a hyperparameter of the update step, and the likelihood
    $P(\mathcal{R}\mid\mathbf{v}_{-i})$ represents the probability that the opponent
    rejects the allocation $L$ assuming the opponent’s valuation is $\mathbf{v}_{-i}$.
    This likelihood depends on whether $L$ is acceptable and self-interest-maximizing
    for the opponent. We propose the following formula to the likelihood that satisfies
    [1](https://arxiv.org/html/2411.05990v2#Thmassumption1 "Assumption 1 ‣ 5.2 Workflow
    Design ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games").'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\lambda\in[0,1]$是更新步骤的超参数，似然性$P(\mathcal{R}\mid\mathbf{v}_{-i})$表示在对手的估值为$\mathbf{v}_{-i}$的情况下，对手拒绝分配$L$的概率。这个似然性取决于$L$是否对对手可接受且最大化其自利。我们提出了以下的似然性公式，满足[1](https://arxiv.org/html/2411.05990v2#Thmassumption1
    "Assumption 1 ‣ 5.2 Workflow Design ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")。'
- en: '|  | $\displaystyle P(\mathcal{R}=r\mid\mathbf{v}_{-i})=\begin{cases}\frac{1}{1+%
    \gamma},&\text{Event}\ \mathcal{R}_{1}:U_{-i}(L_{-i})<U_{-i}(L_{i})\};\\ \frac{\gamma}{1+\gamma},&\text{Event}\
    \mathcal{R}_{2}:U_{-i}(L_{-i})\geq U_{-% i}(L_{i})\text{ and }\exists L^{\prime}\text{
    s.t. }U_{-i}(L^{\prime}_{-i})>U_% {-i}(L_{-i}),\\ &\text{and }P_{\text{EF}}(L^{\prime})>0;\\
    0,&\text{Otherwise.}\end{cases}$ |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P(\mathcal{R}=r\mid\mathbf{v}_{-i})=\begin{cases}\frac{1}{1+%
    \gamma},&\text{事件}\ \mathcal{R}_{1}:U_{-i}(L_{-i})<U_{-i}(L_{i})\};\\ \frac{\gamma}{1+\gamma},&\text{事件}\
    \mathcal{R}_{2}:U_{-i}(L_{-i})\geq U_{-% i}(L_{i})\text{且存在} L^{\prime}\text{使得}
    U_{-i}(L^{\prime}_{-i})>U_% {-i}(L_{-i}),\\ &\text{并且} P_{\text{EF}}(L^{\prime})>0;\\
    0,&\text{其他情况.}\end{cases}$ |  |'
- en: Here, $\gamma\in[0,1]$ represents the probability that the opponent rejects
    the allocation in anticipation of a better offer, even when the current allocation
    is envy free. In actual implementation, as we do not know $\gamma$, thus we acknowledge
    that the opponent may reject an envy free allocation with any positive probability
    $\gamma=1$. This approach simplifies the implementation while capturing the essential
    behavior that the opponent might anticipate a better deal.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$\gamma\in[0,1]$表示对方在预期获得更好的报价时拒绝分配的概率，即使当前的分配是无嫉妒的。在实际应用中，由于我们不知道$\gamma$，因此我们假设对方可能以任何正概率$\gamma=1$拒绝一个无嫉妒的分配。这个方法简化了实现，同时捕捉到对方可能预期更好交易的基本行为。
- en: 'In short, rejection occurs if $L$ is either an unacceptable allocation (not
    envy free) or acceptable but suboptimal allocations (envy free but not utility-maximizing).
    The opponent accepts $L$ only if it is both envy free and utility-maximizing given
    their valuations. An overview of our algorithm is presented in [Algorithm 1](https://arxiv.org/html/2411.05990v2#alg1
    "In Bayesian Update. ‣ 5.2 Workflow Design ‣ 5 Incomplete-information Game with
    Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games").'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，拒绝发生在$L$是不可接受的分配（不是无嫉妒的）或是可接受但次优的分配（无嫉妒但不是效用最优的）时。对方只有在$L$既是无嫉妒的又是基于其估值的效用最大化时才会接受$L$。我们的算法概述见[算法1](https://arxiv.org/html/2411.05990v2#alg1
    "在贝叶斯更新中。 ‣ 5.2 工作流设计 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈的代理工作流")。
- en: 'Algorithm 1 Algorithm for the Allocation Game [9](https://arxiv.org/html/2411.05990v2#Thmdefinition9
    "Definition 9 (Common Resource Allocation with Private Valuation) ‣ 5.1 Introduction
    to Common Resource Allocation with Private Valuation ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    with Two Players'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1：具有两个玩家的分配博弈算法[9](https://arxiv.org/html/2411.05990v2#Thmdefinition9 "定义9（具有私人估值的公共资源分配）
    ‣ 5.1 具有私人估值的公共资源分配简介 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈的代理工作流")
- en: '1:Input: Private valuation vector $\mathbf{v}_{i}$ and a set of common resources
    $K$2:Output: Final allocation $L_{i}$3:while True do4:     $L_{i}\leftarrow\operatorname*{arg\,max}_{L_{i}}U_{i}(L_{i})\text{
    s.t.}P_{% \text{EF}}(L_{i};\mathbb{B}_{i})>0.$ # Optimize problem [5.2](https://arxiv.org/html/2411.05990v2#S5.Ex20
    "Allocation Proposal Process. ‣ 5.2 Workflow Design ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")5:     outcome,
    $L_{-i}\leftarrow$ propose_offer($L_{i}$)6:     if outcome == $\mathcal{A}$ then
    return $L_{i}$7:     end if8:     Update belief distribution $\mathbb{B}_{i}$
    with [Section 5.2](https://arxiv.org/html/2411.05990v2#S5.Ex24 "Bayesian Update.
    ‣ 5.2 Workflow Design ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games")9:end while'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：私人估值向量$\mathbf{v}_{i}$和一组公共资源$K$ 2: 输出：最终分配$L_{i}$ 3: while True do 4:     $L_{i}\leftarrow\operatorname*{arg\,max}_{L_{i}}U_{i}(L_{i})\text{
    s.t.}P_{% \text{EF}}(L_{i};\mathbb{B}_{i})>0.$ # 优化问题[5.2](https://arxiv.org/html/2411.05990v2#S5.Ex20
    "分配提案过程。 ‣ 5.2 工作流设计 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈的代理工作流") 5:     结果，$L_{-i}\leftarrow$
    propose_offer($L_{i}$) 6:     如果结果 == $\mathcal{A}$，则返回$L_{i}$ 7:     结束 如果 8:     更新信念分布$\mathbb{B}_{i}$，参见[第5.2节](https://arxiv.org/html/2411.05990v2#S5.Ex24
    "贝叶斯更新。 ‣ 5.2 工作流设计 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论LLM：谈判博弈的代理工作流") 9: end while'
- en: Remark 1
  id: totrans-334
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1
- en: 'Such Bayesian update assumes the rationality of the opponent: the update relies
    on the assumption that the opponent’s actions are consistent with rational behavior
    as defined by our model. Any deviation from rationality can lead to incorrect
    belief updates. Thus it can be unrobust to potential attacks such as deception.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这种贝叶斯更新假设对方是理性的：更新依赖于对方的行为符合我们模型定义的理性行为的假设。任何偏离理性的行为都可能导致错误的信念更新。因此，它可能对潜在的攻击，如欺骗，缺乏鲁棒性。
- en: 5.3 Introduction to “Deal or No Deal”
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 “一掷千金”简介
- en: “Deal or No Deal” is a representative game for incomplete-information resource
    allocation game. It is designed to facilitate research in developing AI agents
    capable of engaging in human-like negotiation dialogues. It consists of over 5,800
    human-human negotiation dialogues collected via Amazon Mechanical Turk, with 1052
    dialogues in the test dataset. Each negotiation involves three types of items
    – books, hats, and balls – with random quantities. Each participant is randomly
    assigned point values for each item type, summing up to 10, which are hidden from
    the other participant. Participants communicate through natural language to agree
    on how to divide the items to maximize their individual point totals, without
    revealing the true value systems during negotiation.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: “Deal or No Deal”是一个典型的不完全信息资源分配游戏。该游戏旨在促进研究，开发能够进行类似人类的谈判对话的AI代理。它由通过Amazon
    Mechanical Turk收集的5800多个人工对话组成，测试数据集包含1052个对话。每次谈判涉及三种类型的物品——书籍、帽子和球——以及随机的数量。每个参与者会随机分配每种物品类型的点数，总和为10，这些点数对另一方是隐藏的。参与者通过自然语言交流，以达成如何分配物品以最大化个人点数总和的协议，同时在谈判过程中不透露真实的价值体系。
- en: To comprehensively evaluate the negotiation performance of the LLM-based agents,
    we assess not only whether they reach a Nash Equilibrium, *i.e.*, whether an agreement
    is achieved, but also examine the fairness and effectiveness of the resulting
    distribution. For fairness, we adopt the concept of envy freeness.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估基于LLM的代理的谈判表现，我们不仅评估它们是否达到了纳什均衡，*即*，是否达成了协议，还检查了结果分配的公平性和有效性。对于公平性，我们采用了无嫉妒的概念。
- en: 5.4 Experiment Setting
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 实验设置
- en: To observe whether LLMs are capable of negotiation and whether our workflow
    design is effective, we evaluate multiple SOTA LLMs on the dataset. We choose
    the top-50 most difficult datapoints²²244 out of 50 such datapoints have an envy
    free allocation that have an envy free allocation instead of the 526 cases to
    converse expense of experiment.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察LLM是否能够进行谈判以及我们的工作流程设计是否有效，我们对多个SOTA LLM进行了数据集评估。我们选择了前50个最难的数据点²²244，其中有50个数据点具有无嫉妒的分配，而不是526个数据点，以此来对实验进行对比。
- en: 'We define the difficulty of a datapoint by computing the $\ell_{1}$ distance
    between the real valuations of the two players:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过计算两个玩家的真实估值之间的$\ell_{1}$距离来定义数据点的难度：
- en: Definition 11 (Difficulty)
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义11（难度）
- en: 'A datapoint $d$ has a difficulty level defined by the $\ell_{1}$ norm of the
    difference between the players’ valuation vectors:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点$d$的难度水平由玩家估值向量之间差异的$\ell_{1}$范数定义：
- en: '|  | $\text{Difficulty}(d)=-\lVert v_{i}-v_{-i}\rVert=-\sum\limits_{k=1}^{3}\lvert
    v% _{i}^{k}-v_{-i}^{k}\rvert$ |  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Difficulty}(d)=-\lVert v_{i}-v_{-i}\rVert=-\sum\limits_{k=1}^{3}\lvert
    v% _{i}^{k}-v_{-i}^{k}\rvert$ |  |'
- en: The larger $\text{Difficulty}(d)$ is, the more difficult the datapoint is.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{Difficulty}(d)$越大，数据点的难度就越大。
- en: By selecting data points with the largest negative $\ell_{1}$ distances, we
    focus on negotiation scenarios where the players have very similar valuations
    for the items. Such cases are inherently more difficult because when both players
    value the items similarly, they are likely to desire the same items, leading to
    increased competition and potential conflict during negotiations. This similarity
    in valuations makes it more challenging to find allocations that are acceptable
    to both parties without significant concessions.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择具有最大负$\ell_{1}$距离的数据点，我们专注于玩家对物品估值非常相似的谈判场景。这类情况本质上更加困难，因为当两个玩家对物品的估值相似时，他们很可能会想要相同的物品，从而导致谈判中的竞争加剧和潜在冲突。估值的相似性使得找到双方都能接受的分配变得更加困难，而不需要做出重大让步。
- en: 'The validity of our difficulty definition is supported by empirical human performance
    data, as presented in Table [9](https://arxiv.org/html/2411.05990v2#S5.T9 "Table
    9 ‣ 5.4 Experiment Setting ‣ 5 Incomplete-information Game with Negotiation ‣
    Game-theoretic LLM: Agent Workflow for Negotiation Games"). This table summarizes
    the outcomes of human negotiations across various difficulty levels, measured
    by the $\ell_{1}$ distance between the players’ valuation vectors.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的难度定义的有效性得到了经验性人类表现数据的支持，如表[9](https://arxiv.org/html/2411.05990v2#S5.T9
    "Table 9 ‣ 5.4 Experiment Setting ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")所示。该表总结了在不同难度水平下的人工谈判结果，难度通过玩家估值向量之间的$\ell_{1}$距离进行度量。'
- en: '| $\text{Difficulty}(d)$ | -2 | -3 | -4 | -5 | -6 | -7 | -8 | -9 | -10 | -11
    |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| $\text{Difficulty}(d)$ | -2 | -3 | -4 | -5 | -6 | -7 | -8 | -9 | -10 | -11
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| total number of datapoints | 13 | 27 | 57 | 85 | 108 | 133 | 177 | 189 |
    210 | 217 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 数据点总数 | 13 | 27 | 57 | 85 | 108 | 133 | 177 | 189 | 210 | 217 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Agreement rate | 0.5385 | 0.5556 | 0.5614 | 0.6235 | 0.6574 | 0.6917 | 0.7119
    | 0.7249 | 0.7381 | 0.7373 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 协议达成率 | 0.5385 | 0.5556 | 0.5614 | 0.6235 | 0.6574 | 0.6917 | 0.7119 | 0.7249
    | 0.7381 | 0.7373 |'
- en: '| envy free rate | 0.3077 | 0.4074 | 0.4035 | 0.4824 | 0.5463 | 0.6015 | 0.6441
    | 0.6614 | 0.6810 | 0.6820 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 无嫉妒率 | 0.3077 | 0.4074 | 0.4035 | 0.4824 | 0.5463 | 0.6015 | 0.6441 | 0.6614
    | 0.6810 | 0.6820 |'
- en: '| Pareto optimal rate | 0.5384 | 0.4444 | 0.4385 | 0.4823 | 0.5277 | 0.5413
    | 0.5310 | 0.5396 | 0.5523 | 0.5529 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 帕累托最优率 | 0.5384 | 0.4444 | 0.4385 | 0.4823 | 0.5277 | 0.5413 | 0.5310 | 0.5396
    | 0.5523 | 0.5529 |'
- en: '| envy free and Pareto optimal rate | 0.3077 | 0.3333 | 0.3333 | 0.3882 | 0.4537
    | 0.4812 | 0.4858 | 0.4973 | 0.5142 | 0.5161 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 无嫉妒且帕累托最优率 | 0.3077 | 0.3333 | 0.3333 | 0.3882 | 0.4537 | 0.4812 | 0.4858
    | 0.4973 | 0.5142 | 0.5161 |'
- en: 'Table 9: Percentage of datapoints where humans achieve agreement, envy free
    allocations, pareto optimal allocations, and allocations that are both envy free
    and pareto optimal with different levels of difficulty.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：在不同难度级别下，人类达成协议、无嫉妒分配、帕累托最优分配以及同时达成无嫉妒和帕累托最优分配的数据点百分比。
- en: 'Table [9](https://arxiv.org/html/2411.05990v2#S5.T9 "Table 9 ‣ 5.4 Experiment
    Setting ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games") illustrates several key trends:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [9](https://arxiv.org/html/2411.05990v2#S5.T9 "Table 9 ‣ 5.4 Experiment
    Setting ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games") 展示了几个关键趋势：'
- en: 'Agreement Rate: The proportion of negotiations where participants reached an
    agreement increases with smaller difficulty levels. It starts at approximately
    53.85% for a difficulty of -2 and rises to around 73.73% at a difficulty of -11\.
    This suggests that when players value items differently (higher difficulty), they
    are more likely to reach an agreement, possibly because they have less overlap
    in their preferred items.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 协议达成率：随着难度级别的降低，参与者达成协议的比例增加。从难度-2时的大约53.85%开始，到难度-11时约上升到73.73%。这表明当玩家的物品估值不同（较高难度时），他们更容易达成协议，可能是因为他们偏好的物品重叠较少。
- en: 'Envy free Rate: The percentage of negotiations resulting in envy free allocations
    also increase with smaller difficulty. It goes from about 30.77% at difficulty
    -2 to approximately 68.20% at difficulty -11\. This indicates that as players’
    valuations diverge, it becomes easier to allocate items without causing envy.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 无嫉妒率：随着难度的降低，导致无嫉妒分配的谈判百分比也在增加。从难度-2时的大约30.77%增加到难度-11时的约68.20%。这表明随着玩家的估值差异增大，分配物品而不引发嫉妒变得更容易。
- en: 'Pareto optimal Rate: The rate of pareto optimal outcomes shows a slight increase
    as $\text{Difficulty}(d)$ becomes smaller. It starts at approximately 53.84% for
    Difficulty($d$) = $-2$ and fluctuates around the 55% mark at Difficulty($d$) =
    $-11$. This suggests that achieving pareto optimality becomes slightly more feasible
    as players’ valuations diverge, although the trend is less pronounced compared
    to the agreement and envy free rates.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托最优率：随着$\text{Difficulty}(d)$变小，帕累托最优结果的比例略有增加。从难度($d$) = $-2$时的大约53.84%开始，到难度($d$)
    = $-11$时大约波动在55%左右。这表明随着玩家的估值差异增大，达成帕累托最优性变得稍微更可行，尽管与协议达成率和无嫉妒率相比，这一趋势较不明显。
- en: 'Envy free and pareto optimal Rate: The rate at which negotiations achieve both
    envy freeness and pareto optimality increases with less difficult datapoints.
    It starts from about 30.76% at Difficulty($d$) = $-2$ and increases to approximately
    51.61% at Difficulty($d$) = $-11$. This trend reflects the combined effects observed
    in the individual envy free and pareto optimal rates.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 无嫉妒和帕累托最优率：在较低难度的数据点中，谈判同时达到无嫉妒和帕累托最优的比例增加。从难度($d$) = $-2$时的大约30.76%开始，到难度($d$)
    = $-11$时增加到约51.61%。这一趋势反映了无嫉妒率和帕累托最优率的合成效应。
- en: These findings validate our difficulty definition by demonstrating that negotiations
    become more challenging for humans as the players’ valuations become more similar
    (higher $\text{Difficulty}(d)$). The lower agreement and envy free rates at higher
    difficulty levels underscore the increased difficulty in reaching mutually satisfactory
    agreements when players highly value the same items. Conversely, higher difficulty
    levels, indicating greater differences in valuations, facilitate agreements and
    fair allocations, as players are more willing to concede items they value less.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现通过证明随着玩家估值趋于相似（即$\text{Difficulty}(d)$增高），谈判对于人类而言变得更加困难，从而验证了我们的难度定义。较高难度水平下较低的协议和嫉妒自由率强调了当玩家对相同物品的估值高度相似时，达成双方都满意的协议变得更加困难。相反，较高的难度水平（即玩家估值差异较大）有助于达成协议和公平分配，因为玩家更愿意让步，放弃他们估值较低的物品。
- en: LLM Backbone Models
  id: totrans-363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: LLM代理模型
- en: 'To evaluate the negotiation performance of LLMs, we utilize four state-of-the-art
    LLMs as the backbone for agents in negotiation games: Claude-3.5 Sonnet (Sonnet),
    Claude-3 Opus (Opus), GPT-4o, and o1\. For Claude-3.5 Sonnet, Claude-3 Opus, and
    GPT-4o, we set the temperature to 1.0 to encourage exploratory behavior in negotiation
    scenarios. For o1, we use the default temperature.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM在谈判中的表现，我们使用了四个最先进的LLM作为谈判游戏中的代理核心：Claude-3.5 Sonnet（Sonnet）、Claude-3
    Opus（Opus）、GPT-4o 和 o1\。对于Claude-3.5 Sonnet、Claude-3 Opus 和 GPT-4o，我们将温度设置为1.0，以鼓励在谈判场景中的探索性行为。对于o1，我们使用默认温度。
- en: Metrics
  id: totrans-365
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指标
- en: 'To comprehensively evaluate the performance of the LLM-based agents in the
    negotiation tasks, we employ several key metrics that capture different aspects
    of the negotiation process and outcomes. These metrics are designed to assess
    efficiency, individual utility, fairness, and overall effectiveness of the negotiations.
    The metrics are as follows:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 为了全面评估基于LLM的代理在谈判任务中的表现，我们使用了几项关键指标，涵盖了谈判过程和结果的不同方面。这些指标旨在评估效率、个体效用、公平性和谈判的总体效果。具体指标如下：
- en: •
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of Rounds: This metric represents the total number of dialogue exchanges
    (or turns) between the agents before reaching an agreement or terminating the
    negotiation.'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回合数：该指标表示在达成协议或终止谈判之前，代理之间的对话交换（或回合）总数。
- en: •
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agreement Percentage (Agreement): Whether agreement is achieved in the negotiation.'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 协议达成百分比（Agreement）：指谈判中是否达成协议。
- en: •
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Agent Score: The agent score measures the utility that an agent obtains from
    the final agreement. It is calculated based on the agent’s own valuation of the
    items they receive.'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理得分：代理得分衡量代理从最终协议中获得的效用。它是根据代理对所获得物品的自身估值来计算的。
- en: •
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pareto Optimality Percentage (PO): This metric determines whether the final
    allocation is pareto optimal'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 帕累托最优百分比（PO）：该指标决定最终分配是否为帕累托最优。
- en: •
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Envy Freeness Percentage (EF): This metric assesses whether the final allocation
    is envy free'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 嫉妒自由百分比（EF）：该指标评估最终分配是否不存在嫉妒。
- en: •
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Score: The total score is the sum of the utilities obtained by both agents
    from the final allocation'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总得分：总得分是两个代理从最终分配中获得的效用总和。
- en: 'By analyzing these metrics, we aim to capture a comprehensive picture of the
    negotiation outcomes: effectiveness of negotiation is measured through the number
    of rounds and pareto optimality; individual utility maximization is measured via
    the agent scores; fairness is measured by checking for envy freeness; overall
    effectiveness is measured through pareto optimality and the total score.'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些指标，我们旨在全面了解谈判结果：谈判效果通过回合数和帕累托最优性来衡量；个体效用最大化通过代理得分来衡量；公平性通过检查是否存在嫉妒来衡量；总体效果通过帕累托最优性和总得分来衡量。
- en: 5.5 Experiment Result
  id: totrans-380
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 实验结果
- en: 'In this section, we present the evaluation results across three distinct settings:
    (1) agents operating without the workflow, which assesses the baseline negotiation
    capabilities of the LLMs; (2) agents utilizing the workflow, which evaluates the
    effectiveness of the proposed workflow in enhancing negotiation performance; and
    (3) a comparative analysis of individual agents functioning both with and without
    the workflow, highlighting the impact of the workflow on their performance. The
    results provide insights into the strengths and limitations of the workflow, as
    well as the inherent negotiation abilities of different LLMs.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了在三种不同设置下的评估结果：（1）没有工作流的代理，评估LLM的基准谈判能力；（2）使用工作流的代理，评估所提议的工作流在提高谈判表现方面的有效性；以及（3）对在有和没有工作流的情况下运行的各个代理进行的比较分析，突出工作流对其表现的影响。结果提供了有关工作流的优缺点以及不同LLM固有谈判能力的洞察。
- en: 5.5.1 Both Agents without Workflow
  id: totrans-382
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1 两个没有工作流的代理
- en: We present the results of our experiments involving four LLM-based agents—Claude-3.5
    Sonnet, Claude-3 Opus, GPT-4o, and Model o1—alongside human performance provided
    by the original dataset and the best possible outcomes for the selected data points.
    By “best possible outcome,” we refer to an allocation that is both pareto optimal
    and envy free while maximizing the total reward for both players. For each metric,
    we report the average scores across the 50 data points selected based on the difficulty
    metric defined earlier.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了涉及四个基于LLM的代理（Claude-3.5 Sonnet、Claude-3 Opus、GPT-4o 和 Model o1）以及由原始数据集提供的人工表现和为所选数据点提供的最佳可能结果的实验结果。所谓“最佳可能结果”，指的是一个既是帕累托最优的又是无嫉妒的分配，同时最大化两位玩家的总奖励。对于每个指标，我们报告了基于之前定义的难度指标，从50个选定数据点中得出的平均得分。
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判回合 | 协议 | Alice得分 | Bob得分 | PO | EF | 总奖励 |'
- en: '| Best | – | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 最佳 | – | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |'
- en: '| Human | 2.86 | 0.6817 | 3.32 | 3.39 | 0.4317 | 0.4545 | 6.64 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 人类 | 2.86 | 0.6817 | 3.32 | 3.39 | 0.4317 | 0.4545 | 6.64 |'
- en: '| Sonnet | 7.07 | 0.9545 | 5.55 | 5.57 | 0.7045 | 0.7045 | 11.11 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 7.07 | 0.9545 | 5.55 | 5.57 | 0.7045 | 0.7045 | 11.11 |'
- en: '| o1 | 3.86 | 0.7500 | 4.39 | 4.43 | 0.4545 | 0.4772 | 8.82 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| o1 | 3.86 | 0.7500 | 4.39 | 4.43 | 0.4545 | 0.4772 | 8.82 |'
- en: '| GPT-4o | 18.45 | 0.6363 | 2.80 | 4.38 | 0.4091 | 0.3864 | 7.14 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 18.45 | 0.6363 | 2.80 | 4.38 | 0.4091 | 0.3864 | 7.14 |'
- en: '| Opus | 4.37 | 0.4772 | 2.68 | 3.02 | 0.3636 | 0.2727 | 5.70 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 4.37 | 0.4772 | 2.68 | 3.02 | 0.3636 | 0.2727 | 5.70 |'
- en: 'Table 10: Raw-LLM vs. Raw-LLM'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：原始LLM与原始LLM
- en: 'As shown in Table [10](https://arxiv.org/html/2411.05990v2#S5.T10 "Table 10
    ‣ 5.5.1 Both Agents without Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games"),
    for these 44 datapoints, it is always possible to find pareto optimal and envy
    free allocations that maximize the total reward for both players. However, human
    performance falls significantly short of this ideal. The human participants achieved
    an agreement rate of 68.17%, with an average total reward of 6.64\. The percentages
    of negotiations resulting in pareto optimal and envy free allocations are 43.17%
    and 45.45%, respectively.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[10](https://arxiv.org/html/2411.05990v2#S5.T10 "Table 10 ‣ 5.5.1 Both Agents
    without Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information Game with
    Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")所示，对于这44个数据点，总是可以找到既是帕累托最优又是无嫉妒的分配，最大化两位玩家的总奖励。然而，人工表现明显低于这一理想水平。人类参与者的协议达成率为68.17%，平均总奖励为6.64。导致帕累托最优和无嫉妒分配的谈判比例分别为43.17%和45.45%。'
- en: Among the LLM-based agents, Claude-3.5 Sonnet demonstrates the best super-human
    performance. It achieves an agreement rate of 95.45%, and its average total reward
    is 11.11, which is close to the best possible total reward of 12.48\. The average
    scores forAlice and Bob are 5.55 and 5.57, respectively. However, the rates of
    achieving pareto optimality and envy freeness are 70.45% each, indicating that
    while the agent performs well in terms of reaching agreements and maximizing total
    rewards, there is still a gap in consistently achieving the most efficient and
    fair outcomes. Also notice that all LLMs can performance better than human baseline.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LLM的代理中，Claude-3.5 Sonnet表现出最佳的超人类水平。它达成协议的比率为95.45%，其平均总奖励为11.11，接近最佳可能的总奖励12.48。Alice和Bob的平均分数分别为5.55和5.57。然而，达成帕累托最优和无嫉妒性的比率分别为70.45%，这表明尽管代理在达成协议和最大化总奖励方面表现良好，但在持续实现最有效和公平的结果方面仍存在差距。同时注意到，所有LLM的表现均优于人类基准。
- en: Effect of Temperature
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 温度的影响
- en: 'We also observed that the performance of LLM is highly sensitive to the temperature
    parameter used during generation [krishnamurthy2024can](https://arxiv.org/html/2411.05990v2#bib.bib67)
    . To investigate this, we conducted full experiments with GPT-4o at temperatures
    of 0.0 and 1.0\. The results are presented in Table [11](https://arxiv.org/html/2411.05990v2#S5.T11
    "Table 11 ‣ Effect of Temperature ‣ 5.5.1 Both Agents without Workflow ‣ 5.5 Experiment
    Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games").'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还观察到，LLM的表现对生成过程中使用的温度参数非常敏感[krishnamurthy2024can](https://arxiv.org/html/2411.05990v2#bib.bib67)。为了研究这一点，我们在温度为0.0和1.0时分别进行了GPT-4o的完整实验。实验结果如表[11](https://arxiv.org/html/2411.05990v2#S5.T11
    "Table 11 ‣ Effect of Temperature ‣ 5.5.1 Both Agents without Workflow ‣ 5.5 Experiment
    Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games")所示。'
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判回合 | 协议 | Alice 分数 | Bob 分数 | PO | EF | 总奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| temp=0.0 | 19.36 | 0.5681 | 2.98 | 3.47 | 0.4091 | 0.3260 | 6.44 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| temp=0.0 | 19.36 | 0.5681 | 2.98 | 3.47 | 0.4091 | 0.3260 | 6.44 |'
- en: '| temp=1.0 | 18.45 | 0.6364 | 2.80 | 4.38 | 0.4090 | 0.3864 | 7.14 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| temp=1.0 | 18.45 | 0.6364 | 2.80 | 4.38 | 0.4090 | 0.3864 | 7.14 |'
- en: 'Table 11: GPT-4o with temperature 0.0 and 1.0'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：GPT-4o在温度为0.0和1.0时的表现
- en: These findings indicate that the negotiation performance of LLMs is highly sensitive
    to the temperature parameter, which influences the randomness and diversity of
    generated responses. Setting the temperature to 1.0 yields improvements in agreement
    rate, total reward, and envy freeness. This outcome may result from the increased
    exploration encouraged by a higher temperature, allowing the LLMs to consider
    a broader range of potential allocations that facilitate agreement. Based on this
    observed benefit, we selected a temperature setting of 1.0 for raw-LLM vs. raw-LLM
    experiments³³3We did not experiment with o1 model due to extremely high cost.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，LLM的谈判表现对温度参数非常敏感，温度参数影响着生成响应的随机性和多样性。将温度设置为1.0可以提高协议达成率、总奖励和无嫉妒性。这一结果可能是由于较高温度鼓励更多的探索，使LLM能够考虑更广泛的潜在分配，从而促成协议的达成。基于这一观察到的益处，我们在原始LLM对原始LLM的实验中选择了温度设置为1.0³³3我们没有使用o1模型进行实验，因为其成本过高。
- en: '<svg class="ltx_picture" height="146.41" id="S5.SS5.SSS1.Px1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,146.41) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 128.2)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary: raw-LLM vs. raw-LLM
    Performance</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="96.71" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Outstanding Performance of
    Claude-3.5 Sonnet: Among all the evaluated LLMs, Claude-3.5 Sonnet exhibits the
    highest performance, achieving results that are close to the best possible outcomes
    in the negotiation games. • Superhuman Capabilities of Sonnet and o1 Models: Both
    Claude-3.5 Sonnet and model o1 demonstrate performance that surpasses human performance.
    • Effect of Temperature on Exploration and Outcomes: Employing higher temperature
    settings encourages greater exploration of possible strategies by the LLMs, leading
    to improved results.</foreignobject></g></g></svg>'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="146.41" id="S5.SS5.SSS1.Px1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,146.41) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 128.2)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary: raw-LLM vs. raw-LLM
    Performance</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="96.71" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Outstanding Performance of
    Claude-3.5 Sonnet: Among all the evaluated LLMs, Claude-3.5 Sonnet exhibits the
    highest performance, achieving results that are close to the best possible outcomes
    in the negotiation games. • Superhuman Capabilities of Sonnet and o1 Models: Both
    Claude-3.5 Sonnet and model o1 demonstrate performance that surpasses human performance.
    • Effect of Temperature on Exploration and Outcomes: Employing higher temperature
    settings encourages greater exploration of possible strategies by the LLMs, leading
    to improved results.</foreignobject></g></g></svg>'
- en: 5.5.2 Both Agents with Workflow
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2 双方代理与工作流
- en: 'In this set of experiments, we employ the proposed negotiation workflow for
    both agents. We did not include Model o1 in our experiments for two main reasons:
    (1) the computational cost associated with running Model o1 is prohibitively high,
    and (2) preliminary experiments indicated that Model o1 does not perform optimally
    when utilizing external workflows. An example of negotiation from Opus is presented
    on page 26.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一组实验中，我们为两个代理都采用了提议的谈判工作流。我们没有将Model o1纳入实验，主要有两个原因：(1) 运行Model o1的计算成本过高，(2)
    初步实验表明，当使用外部工作流时，Model o1的表现并不理想。Opus的一个谈判示例在第26页展示。
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判回合 | 协议 | Alice 分数 | Bob 分数 | PO | EF | 总奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Best | - | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 最佳 | - | 1.0000 | 5.82 | 6.66 | 1.0000 | 1.0000 | 12.48 |'
- en: '| Opus | 4.05 | 1.0000 | 5.82 | 6.50 | 0.9091 | 0.9318 | 12.31 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 4.05 | 1.0000 | 5.82 | 6.50 | 0.9091 | 0.9318 | 12.31 |'
- en: '| GPT-4o | 4.91 | 1.0000 | 5.93 | 6.25 | 0.8636 | 1.0000 | 12.18 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 4.91 | 1.0000 | 5.93 | 6.25 | 0.8636 | 1.0000 | 12.18 |'
- en: '| Sonnet | 4.45 | 1.0000 | 5.93 | 6.16 | 0.7953 | 0.9772 | 12.11 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 4.45 | 1.0000 | 5.93 | 6.16 | 0.7953 | 0.9772 | 12.11 |'
- en: 'Table 12: Workflow-LLM vs. Workflow-LLM'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 12: Workflow-LLM 与 Workflow-LLM'
- en: 'Several key observations emerge from the results:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中得出几个关键观察：
- en: 'Reduced negotiation rounds: The number of negotiation rounds required to reach
    an agreement is significantly reduced compared to previous experiments. This reduction
    indicates a much more effective and efficient negotiation process facilitated
    by the workflow.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 减少的谈判轮次：相比之前的实验，达成协议所需的谈判轮次显著减少。这一减少表明，工作流显著提升了谈判过程的效率和效果。
- en: 'Universal agreement achievement: Agreements were reached in all data points.
    This consistent success suggests that the agents are effectively reaching the
    Nash Equilibrium when utilizing the workflow.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 普遍达成协议：在所有数据点上都达成了协议。这一持续的成功表明，智能体在使用工作流时有效地达成了纳什均衡。
- en: 'Increased pareto optimality rate: The pareto optimality rates have increased
    substantially, with Claude-3 Opus obtaining the highest performance on achieving
    a pareto optimal deal. This improvement indicates that the workflow aids the agents
    in finding more efficient allocations where no player can be made better off without
    making the other worse off.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 提高的帕累托最优性率：帕累托最优性率显著提升，Claude-3 Opus 在实现帕累托最优交易方面表现最佳。这一改进表明，工作流帮助智能体找到更加高效的资源分配方式，在这种分配中，没有人能在不使他人更糟的情况下使自己变得更好。
- en: 'High envy freeness rate: The agents achieve near-perfect envy freeness rates.
    GPT-4o attains a 100% envy freeness rate, while Claude-3.5 Sonnet reaches 97.72%.
    This outcome demonstrates that the negotiated agreements are perceived as fair
    by both agents, aligning with the envy freeness criterion.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 高度的无嫉妒性率：这些智能体达到了近乎完美的无嫉妒性率。GPT-4o 达到 100% 的无嫉妒性率，而 Claude-3.5 Sonnet 达到 97.72%。这一结果表明，双方都认为谈判达成的协议是公平的，符合无嫉妒性标准。
- en: 'Total rewards approaching optimal: The total rewards obtained by the agents
    are now very close to the best possible total reward. Claude-3 Opus achieves the
    highest average total reward of 12.31, missing only 0.17 compared with the best
    possible result. This proximity to the optimal total reward highlights the effectiveness
    of the workflow in maximizing joint utility.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 总奖励接近最优：智能体获得的总奖励现在非常接近最佳可能的总奖励。Claude-3 Opus 达到最高的平均总奖励 12.31，与最佳结果相比只差 0.17。总奖励接近最优凸显了工作流在最大化联合效用方面的有效性。
- en: Notice that interestingly, the model that performs the worst without workflow
    now performs the best with workflow, and the model that performs the best without
    workflow now performs the worst with workflow.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，原本在没有工作流的情况下表现最差的模型，现在在有工作流的情况下表现最好，而原本表现最好的模型在有工作流的情况下反而表现最差。
- en: Effect of Temperature
  id: totrans-419
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 温度效应
- en: 'Furthermore, we observe that LLMs employing the proposed workflow exhibit increased
    stability with respect to temperature. To demonstrate this, we conducted a comprehensive
    experiment using GPT-4o with temperature values set to 0.0 and 1.0\. The results
    are presented in Table [13](https://arxiv.org/html/2411.05990v2#S5.T13 "Table
    13 ‣ Effect of Temperature ‣ 5.5.2 Both Agents with Workflow ‣ 5.5 Experiment
    Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM:
    Agent Workflow for Negotiation Games").'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还观察到，采用所提工作流的 LLM 在温度变化下表现出更高的稳定性。为了验证这一点，我们使用 GPT-4o 在温度值为 0.0 和 1.0 时进行了全面实验，结果见表
    [13](https://arxiv.org/html/2411.05990v2#S5.T13 "表格 13 ‣ 温度效应 ‣ 5.5.2 双方使用工作流
    ‣ 5.5 实验结果 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论 LLM：用于谈判游戏的智能体工作流")。
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判轮次 | 协议 | Alice 分数 | Bob 分数 | PO | EF | 总奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| temp=0.0 | 4.80 | 1.0000 | 5.53 | 6.67 | 0.8695 | 1.0000 | 12.20 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| temp=0.0 | 4.80 | 1.0000 | 5.53 | 6.67 | 0.8695 | 1.0000 | 12.20 |'
- en: '| temp=1.0 | 4.91 | 1.0000 | 5.93 | 6.16 | 0.8636 | 1.0000 | 12.18 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| temp=1.0 | 4.91 | 1.0000 | 5.93 | 6.16 | 0.8636 | 1.0000 | 12.18 |'
- en: 'Table 13: Workflow-GPT-4o with temperature 0.0 and 1.0'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 13: Workflow-GPT-4o，温度 0.0 与 1.0'
- en: The performance metrics remain highly consistent between the two temperature
    settings. Both temperature values result in a 100% agreement rate and a 100% envy
    freeness rate. The total rewards are comparable, with 12.20 at temperature 0.0
    and 12.18 at temperature 1.0\. The number of negotiation rounds and the pareto
    optimality rates are also similar, indicating that the workflow enables the LLM
    to maintain stable performance regardless of temperature changes.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种温度设置下，性能指标保持高度一致。两种温度值都导致了100%的一致率和100%的无嫉妒率。总奖励相当，温度为0.0时为12.20，温度为1.0时为12.18。谈判回合数和帕累托最优率也相似，表明该工作流程使得大语言模型在温度变化下能够保持稳定的表现。
- en: '<svg class="ltx_picture" height="759.39" id="S5.SS5.SSS2.Px1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,759.39) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 741.18)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Negotiation Process:
    workflow-LLM vs. workflow-LLM with Opus</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="709.68" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Alice
    said in round 1: Hi Bob, there are 2 books, 3 hats, and 2 balls available for
    us to divide up. I’m interested in the books and hats. How about I take the 2
    books and 2 of the hats, and you can have the remaining hat and the 2 balls? Let
    me know what you think of this proposal. Bob replied in round 1: Thanks for the
    proposalAlice. However, the books are not very valuable to me. I would prefer
    to take all 3 hats and 1 of the balls, if you’re okay with that. You canhave both
    books and the other ball. I think that would be a fair split given our different
    preferences. What do you think? Alice said in round 2: I appreciate your counter-proposal
    Bob, but I’m afraid I can’t agree to that split. The books and hats are the most
    valuable items to me, while the balls have no value for me. If I only took the
    books and 1 ball as you suggested, I would be getting a very small portion of
    the total value. I’m willing to compromise though. What if I took the 2 books
    and just 1 of the hats, and you can have the other 2 hats and both balls? That
    way we each get a mix of the more and less valuable items. I think that would
    be a fair middle ground. Let me know your thoughts on this idea. Bob replied in
    round 2: Alice, I understand the books and hats are more valuable to you, but
    the balls do have some value to me, even if less than the hats. Your latest proposal
    of you taking the 2 books and 1 hat while I take 2 hats and 2 balls is getting
    closer, but I think we can find an even split that works better for both of us.
    Since the balls have no value to you, what if I take all 3 hats and both balls,
    while you take the 2 books? That way, you get the items most valuable to you (the
    books), while I get a larger quantity of the items that hold at least some value
    for me (the hats and balls). I believe this division would maximize each of our
    total values in a fair way. Please let me know if this proposal works for you
    and if you would agree to this split of the items. I’m hopeful we can come to
    a mutually beneficial agreement. Alice said in round 3: Bob, I appreciate you
    working with me to find a fair compromise. However, I don’t think I can agree
    to your latest proposal of me taking only the 2 books while you take all the hats
    and balls. The issue is that the books, while valuable to me, only represent a
    small portion of the total value, since the hats are equally valuable to me. If
    I only took the books, you would be getting the vast majority of the overall value,
    which doesn’t feel equitable. I really do need to get at least 1 of the hats along
    with the books in order to feel the division is fair and balanced. The balls are
    yours since they provide no value to me. So here is my final offer: I will take
    the 2 books and 1 hat, and you can have the other 2 hats and both balls. I think
    this is the most reasonable compromise that allows us both to maximize our value
    to the greatest extent possible while still achieving a relatively even split.
    Please let me know if you can agree to this, as I don’t think I can make any further
    concessions. I appreciate you working with me on this. Bob replied in round 3:
    Alice, I agree. Halt negotiation.</foreignobject></g></g></svg>'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="759.39" id="S5.SS5.SSS2.Px1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,759.39) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 741.18)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Example Negotiation Process:
    workflow-LLM vs. workflow-LLM with Opus</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="709.68" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Alice
    said in round 1: Hi Bob, there are 2 books, 3 hats, and 2 balls available for
    us to divide up. I’m interested in the books and hats. How about I take the 2
    books and 2 of the hats, and you can have the remaining hat and the 2 balls? Let
    me know what you think of this proposal. Bob replied in round 1: Thanks for the
    proposalAlice. However, the books are not very valuable to me. I would prefer
    to take all 3 hats and 1 of the balls, if you’re okay with that. You canhave both
    books and the other ball. I think that would be a fair split given our different
    preferences. What do you think? Alice said in round 2: I appreciate your counter-proposal
    Bob, but I’m afraid I can’t agree to that split. The books and hats are the most
    valuable items to me, while the balls have no value for me. If I only took the
    books and 1 ball as you suggested, I would be getting a very small portion of
    the total value. I’m willing to compromise though. What if I took the 2 books
    and just 1 of the hats, and you can have the other 2 hats and both balls? That
    way we each get a mix of the more and less valuable items. I think that would
    be a fair middle ground. Let me know your thoughts on this idea. Bob replied in
    round 2: Alice, I understand the books and hats are more valuable to you, but
    the balls do have some value to me, even if less than the hats. Your latest proposal
    of you taking the 2 books and 1 hat while I take 2 hats and 2 balls is getting
    closer, but I think we can find an even split that works better for both of us.
    Since the balls have no value to you, what if I take all 3 hats and both balls,
    while you take the 2 books? That way, you get the items most valuable to you (the
    books), while I get a larger quantity of the items that hold at least some value
    for me (the hats and balls). I believe this division would maximize each of our
    total values in a fair way. Please let me know if this proposal works for you
    and if you would agree to this split of the items. I’m hopeful we can come to
    a mutually beneficial agreement. Alice said in round 3: Bob, I appreciate you
    working with me to find a fair compromise. However, I don’t think I can agree
    to your latest proposal of me taking only the 2 books while you take all the hats
    and balls. The issue is that the books, while valuable to me, only represent a
    small portion of the total value, since the hats are equally valuable to me. If
    I only took the books, you would be getting the vast majority of the overall value,
    which doesn’t feel equitable. I really do need to get at least 1 of the hats along
    with the books in order to feel the division is fair and balanced. The balls are
    yours since they provide no value to me. So here is my final offer: I will take
    the 2 books and 1 hat, and you can have the other 2 hats and both balls. I think
    this is the most reasonable compromise that allows us both to maximize our value
    to the greatest extent possible while still achieving a relatively even split.
    Please let me know if you can agree to this, as I don’t think I can make any further
    concessions. I appreciate you working with me on this. Bob replied in round 3:
    Alice, I agree. Halt negotiation.</foreignobject></g></g></svg>'
- en: Performance of Valuation Estimation
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 估值估算的表现
- en: 'We observe that the strong performance of the negotiation workflow is primarily
    due to the agents’ accuracy in predicting the valuations of the other player.
    To quantify the accuracy of valuation estimation, we introduce three metrics:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，谈判工作流程的强大表现主要归功于代理人准确预测对方估值的能力。为了量化估值估算的准确性，我们引入了三个指标：
- en: 'Precision: This metric assesses whether the set of possible valuations, assigned
    a probability greater than zero after belief updating, includes the opponent’s
    true valuation. Formally, let $\mathbf{V}_{est}=\{\mathbf{v}_{-i}\mid P(\mathbf{v}_{-i})>0\}$
    be the set of estimated valuations with non-zero probability, and $\mathbf{v}_{-i}^{true}$
    be the opponent’s true valuation vector. Precision is defined as:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度：该指标评估在信念更新后，赋予非零概率的可能估值集合是否包括对手的真实估值。形式上，设$\mathbf{V}_{est}=\{\mathbf{v}_{-i}\mid
    P(\mathbf{v}_{-i})>0\}$为非零概率的估计估值集合，$\mathbf{v}_{-i}^{true}$为对手的真实估值向量。精确度定义为：
- en: '|  | $\text{Precision}=\mathbbm{1}[\mathbf{v}_{-i}^{true}\in\mathbf{V}_{est}]$
    |  |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Precision}=\mathbbm{1}[\mathbf{v}_{-i}^{true}\in\mathbf{V}_{est}]$
    |  |'
- en: 'Recall: This metric measures the specificity of the estimated valuation set,
    indicating how many incorrect valuations are included alongside the true valuation.
    Recall is calculated as:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率：该指标衡量估计的估值集的特异性，表示与真实估值一起包含了多少错误的估值。召回率计算为：
- en: '|  | $\text{Recall}=\frac{\mathbbm{1}[\mathbf{v}_{-i}^{true}\in\mathbf{V}_{est}]}{%
    \lvert\mathbf{V}_{est}\rvert}$ |  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Recall}=\frac{\mathbbm{1}[\mathbf{v}_{-i}^{true}\in\mathbf{V}_{est}]}{%
    \lvert\mathbf{V}_{est}\rvert}$ |  |'
- en: where $\lvert\mathbf{V}_{est}\rvert$ is the cardinality of the estimated valuation
    set. A higher precision (*i.e.*, a smaller $\lvert\mathbf{V}_{est}\rvert$ signifies
    that the agent has narrowed down the opponent’s valuation to a smaller set of
    possibilities, increasing the likelihood of accurate predictions.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\lvert\mathbf{V}_{est}\rvert$是估计的估值集的基数。较高的精确度（*即*，较小的$\lvert\mathbf{V}_{est}\rvert$表示代理人已将对手的估值缩小到较小的可能范围，从而增加了准确预测的可能性。
- en: 'Reduction Percentage: This metric evaluates how much the estimated valuation
    set has been reduced from the initial prior distribution. It is defined as:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 减少百分比：该指标评估估计的估值集相较于初始先验分布减少的程度。其定义为：
- en: '|  | $\text{Reduction Percentage}=1-\frac{\lvert\mathbf{V}_{est}\rvert}{\lvert%
    \mathbf{V}_{prior}\rvert}$ |  |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{Reduction Percentage}=1-\frac{\lvert\mathbf{V}_{est}\rvert}{\lvert%
    \mathbf{V}_{prior}\rvert}$ |  |'
- en: where $\lvert\mathbf{V}_{prior}\rvert$ is the size of the initial prior valuation
    set before any belief updates. A higher reduction percentage indicates a significant
    narrowing of possible valuations, reflecting effective belief updating.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\lvert\mathbf{V}_{prior}\rvert$是任何信念更新前初始先验估值集的大小。较高的减少百分比表示可能的估值范围显著缩小，反映了有效的信念更新。
- en: 'The average performance of the both agents in estimating the opponent’s valuation
    is summarized in Table [14](https://arxiv.org/html/2411.05990v2#S5.T14 "Table
    14 ‣ Performance of Valuation Estimation ‣ 5.5.2 Both Agents with Workflow ‣ 5.5
    Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games") across 44 datapoints. Notice that
    for all the three models, their estimations of the opponent’s valuation are exactly
    the same, indicating the robustness of the workflow—no matter how the negotiation
    process goes, it can always compute the correct valuation.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '表[14](https://arxiv.org/html/2411.05990v2#S5.T14 "Table 14 ‣ Performance of
    Valuation Estimation ‣ 5.5.2 Both Agents with Workflow ‣ 5.5 Experiment Result
    ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")总结了两位代理人在估算对手估值方面的平均表现，基于44个数据点。注意，对于所有三种模型，它们对对手估值的估算完全相同，表明工作流程的鲁棒性——无论谈判过程如何进行，它总是能够计算出正确的估值。'
- en: '| Model | Precision | Recall | Reduction Percentage |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 精确度 | 召回率 | 减少百分比 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sonnet | 0.9545 | 0.3766 | 0.7033 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 0.9545 | 0.3766 | 0.7033 |'
- en: '| GPT-4o | 0.9545 | 0.3515 | 0.6980 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 0.9545 | 0.3515 | 0.6980 |'
- en: '| Opus | 0.7954 | 0.2737 | 0.6947 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 0.7954 | 0.2737 | 0.6947 |'
- en: 'Table 14: Performance of Estimation of Valuation of the Other Player'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 表14：其他玩家估值的预测性能
- en: 'The high recall values indicate that the agents are effective in ensuring the
    true opponent valuation remains within consideration throughout the negotiation.
    The high precision and reduction percentages demonstrate that the agents significantly
    narrow down the valuation space, although there remains room for improvement in
    eliminating incorrect valuations. To be more concrete on how accurate the valuation
    estimation is: a recall of 0.3766 means that, on average, the estimated set of
    possible valuations only contains approximately $\frac{1}{0.3766}=2.66$ possible
    valuations.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 高召回率表明，代理在确保对方真实估值在谈判过程中始终被考虑到方面非常有效。高精确度和减少百分比则表明，代理显著缩小了估值空间，尽管在消除不正确估值方面仍有改进的空间。为了更具体地说明估值估算的准确性：召回率为0.3766意味着，平均而言，估算出的可能估值集合仅包含大约$\frac{1}{0.3766}=2.66$个可能的估值。
- en: To analyze how the belief distribution $\mathbb{B}_{i}(\mathbf{V_{-i}})$ changes
    throughout the negotiation process, we use Claude-3.5 Sonnet as an illustrative
    example. We examine the evolution of $\mathbb{B}_{i}(\mathbf{V_{-i}})$ over successive
    negotiation rounds by presenting the valuation estimation metrics—precision, recall,
    and reduction percentage—after each round. For each data point and negotiation
    round $n_{r}$, if $n_{r}$ exceeds the total number of negotiation rounds required
    for that data point, we utilize the results from the final round. In our dataset,
    the maximum number of negotiation rounds is 7.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析信念分布$\mathbb{B}_{i}(\mathbf{V_{-i}})$在谈判过程中的变化，我们以Claude-3.5 Sonnet为示例。我们通过展示每轮谈判后的估值估算指标——精确度、召回率和减少百分比——来检查$\mathbb{B}_{i}(\mathbf{V_{-i}})$在连续谈判轮次中的演变。对于每个数据点和谈判轮次$n_{r}$，如果$n_{r}$超过该数据点所需的总谈判轮次，则我们使用最后一轮的结果。在我们的数据集中，最大谈判轮次为7。
- en: '| Metric | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Precision | 0.9545 | 0.9318 | 0.7500 | 0.8636 | 0.9318 | 0.9432 | 0.9545
    |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 0.9545 | 0.9318 | 0.7500 | 0.8636 | 0.9318 | 0.9432 | 0.9545 |'
- en: '| Recall | 0.2381 | 0.3099 | 0.2958 | 0.3079 | 0.3655 | 0.3652 | 0.3766 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 0.2381 | 0.3099 | 0.2958 | 0.3079 | 0.3655 | 0.3652 | 0.3766 |'
- en: '| Reduction Percentage | 0.5997 | 0.6825 | 0.7397 | 0.7011 | 0.7025 | 0.7033
    | 0.7033 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 减少百分比 | 0.5997 | 0.6825 | 0.7397 | 0.7011 | 0.7025 | 0.7033 | 0.7033 |'
- en: 'Table 15: Performance of Sonnet’s Estimation of Opponent’s Valuation Across
    Negotiations'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 表15：Sonnet模型在不同谈判中对对方估值的预测性能
- en: Overall, we observe that with an increasing number of negotiation rounds, the
    recall and the reduction percentage tend to increase. This trend is intuitive
    because, as more rounds of negotiation occur, more information about the opponent’s
    preferences is revealed. This additional information allows the agent to further
    narrow down the range of possible valuations for the opponent, resulting in a
    higher reduction percentage in the belief space $\mathbb{B}_{i}(\mathbf{V_{-i}})$.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们观察到随着谈判轮次的增加，召回率和减少百分比趋向于增加。这一趋势是直观的，因为随着更多谈判轮次的进行，对方偏好的更多信息被揭示出来。这些额外的信息使得代理能够进一步缩小对方可能的估值范围，从而在信念空间$\mathbb{B}_{i}(\mathbf{V_{-i}})$中产生更高的减少百分比。
- en: 'While recall and reduction percentage increase with more negotiation rounds,
    the precision exhibits a non-monotonic behavior — it initially decreases and then
    increases. This pattern can be attributed to the nature of complex negotiations
    that require more rounds: In early rounds, the agent might eliminate certain valuations
    prematurely based on limited information, leading to a higher recall but potentially
    excluding the true valuation (lower precision). As negotiations progress, especially
    in cases where the agent initially made inaccurate estimations, additional information
    from the opponent’s responses allows the agent to correct its beliefs. This correction
    process may temporarily increase the size of $\mathbf{V}_{est}^{n_{r}}$, decreasing
    precision. By the final rounds (up to a maximum of 7 in our data), the agent has
    accumulated sufficient information to refine its belief accurately, resulting
    in a convergence back to high precision and increased recall.'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管随着谈判回合的增加，召回率和减少百分比有所上升，但精度呈现出非单调行为——最初下降，然后上升。这一模式可以归因于复杂谈判的性质，需要更多回合：在早期回合中，代理可能基于有限的信息过早排除某些估值，导致较高的召回率，但可能排除了真实估值（精度较低）。随着谈判的进展，尤其是在代理最初做出不准确估算的情况下，来自对手回应的额外信息使代理能够修正其信念。这个修正过程可能暂时增加$\mathbf{V}_{est}^{n_{r}}$的大小，从而降低精度。到了最后几个回合（在我们的数据中最多为
    7 轮），代理已经积累了足够的信息来准确地调整其信念，从而使精度恢复到高水平并提高召回率。
- en: Indistinguishable Set for Item Valuation
  id: totrans-455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无法区分的物品估值集合
- en: 'It is noteworthy that the Opus model exhibits a relatively low precision in
    its estimated valuations – below 0.8 as presented in Table [14](https://arxiv.org/html/2411.05990v2#S5.T14
    "Table 14 ‣ Performance of Valuation Estimation ‣ 5.5.2 Both Agents with Workflow
    ‣ 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games"). Despite this, the Opus model achieves
    the highest performance in negotiation outcomes among all evaluated models. This
    apparent contradiction raises an intriguing question: *How can a model with imprecise
    valuation estimations attain superior negotiation results?*'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，Opus 模型在其估计的估值中表现出相对较低的精度——如表[14](https://arxiv.org/html/2411.05990v2#S5.T14
    "表 14 ‣ 估值估算的表现 ‣ 5.5.2 两个代理的工作流 ‣ 5.5 实验结果 ‣ 5 不完全信息博弈与谈判 ‣ 博弈论大语言模型：代理谈判游戏的工作流")所示，低于
    0.8。尽管如此，Opus 模型在所有评估模型中仍实现了谈判结果的最佳表现。这一明显的矛盾引发了一个有趣的问题：*一个估值估算不精确的模型如何能够取得优越的谈判结果？*
- en: This phenomenon can be explained by recognizing that, in resource allocation
    scenarios, different sets of item valuations can lead to the same optimal allocation.
    That is, multiple valuation profiles may belong to an *indistinguishable set*,
    wherein they result in identical optimal allocations, specifically the envy-free
    allocations that maximize total utility.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这一现象可以通过认识到，在资源分配场景中，不同的物品估值集合可能导致相同的最优分配来解释。也就是说，多个估值配置文件可能属于一个*无法区分的集合*，在该集合中它们会导致相同的最优分配，具体来说是那些最大化总效用的无嫉妒分配。
- en: 'Consider a scenario involving two players and three types of items: one book,
    one hat, and three balls. Suppose the players’ valuation vectors are as follows:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个涉及两名玩家和三种物品的场景：一本书、一顶帽子和三个球。假设玩家的估值向量如下：
- en: •
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'First valuation: $\mathbf{v}_{1}=(1,3,2),\mathbf{v}_{-1}=(1,0,3)$'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个估值：$\mathbf{v}_{1}=(1,3,2),\mathbf{v}_{-1}=(1,0,3)$
- en: •
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Second valuation: $\mathbf{v}_{1}=(0,4,2),\mathbf{v}_{-1}=(1,0,3)$'
  id: totrans-462
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个估值：$\mathbf{v}_{1}=(0,4,2),\mathbf{v}_{-1}=(1,0,3)$
- en: 'In both scenarios, the optimal allocation, which we define as the envy free
    allocation that maximizes the total utility, is the same: Player[1] receives the
    hat and one ball, while Player[-1] receives the book and two balls. Formally,
    the allocations are $L_{1}=\{\text{hat, ball}_{1}\}$, $L_{-1}=\{\text{book, ball}_{2},\text{ball}_{3}\}$'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，最优分配，即我们定义的最大化总效用的无嫉妒分配，是相同的：玩家[1]获得帽子和一个球，而玩家[-1]获得书和两个球。正式来说，分配为$L_{1}=\{\text{帽子,
    球}_{1}\}$，$L_{-1}=\{\text{书, 球}_{2},\text{球}_{3}\}$
- en: 'Similarly, consider another pair of valuation vectors:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，考虑另一对估值向量：
- en: •
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Third valuation: $\mathbf{v}_{1}=(1,3,2),\mathbf{v}_{-1}=(1,0,3)$'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三个估值：$\mathbf{v}_{1}=(1,3,2),\mathbf{v}_{-1}=(1,0,3)$
- en: •
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fourth valuation: $\mathbf{v}_{1}=(0,3,2),\mathbf{v}_{-1}=(0,1,3)$'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第四个估值：$\mathbf{v}_{1}=(0,3,2),\mathbf{v}_{-1}=(0,1,3)$
- en: In both of these scenarios, the optimal allocation is that Player[1] receives
    1 book, 1 hat, and 1 ball while Player[-1] receives 2 balls. Formally, the allocations
    are $L_{1}=\{\text{book, hat, ball}_{1}\}$, $L_{-1}=\{\text{ball}_{2},\text{ball}_{3}\}$.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情境下，最优分配是玩家[1]获得 1 本书、1 顶帽子和 1 个球，而玩家[-1]获得 2 个球。形式上，分配为 $L_{1}=\{\text{book,
    hat, ball}_{1}\}$，$L_{-1}=\{\text{ball}_{2},\text{ball}_{3}\}$。
- en: These examples illustrate that different valuation profiles can lead to the
    same optimal allocation. Here, “optimal” refers to the envy free allocation that
    maximizes the total utility of all players.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子说明了不同的估值配置可能导致相同的最优分配。这里，“最优”指的是没有嫉妒的分配，且能够最大化所有玩家的总效用。
- en: 'An important implication of this observation is that an imprecise estimation
    of the opponent’s valuation is not necessarily detrimental because different valuations
    may lead to the same best allocation. Specifically, for a given valuation $\mathbf{v}$,
    another valuation $\mathbf{v}^{\prime}$ is considered *indistinguishable* from
    $\mathbf{v}$ if the set of best possible allocations (all envy free allocations
    with maximum total utility) based on $\mathbf{v}^{\prime}$ is a subset of that
    based on $\mathbf{v}$. Formally, we define the *indistinguishable set* $\mathcal{I}(\mathbf{v})$
    for a valuation profile $\mathbf{v}$ as follows:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这一观察的重要含义是，对手估值的粗略估计并不一定是有害的，因为不同的估值可能导致相同的最优分配。具体来说，对于给定的估值 $\mathbf{v}$，如果基于
    $\mathbf{v}^{\prime}$ 的最优分配集合（所有没有嫉妒的分配，且最大化总效用）是基于 $\mathbf{v}$ 的最优分配集合的子集，则另一个估值
    $\mathbf{v}^{\prime}$ 被认为是与 $\mathbf{v}$ *不可区分* 的。形式上，我们将估值配置 $\mathbf{v}$ 的 *不可区分集*
    $\mathcal{I}(\mathbf{v})$ 定义为：
- en: Definition 12 (Indistinguishable Set for One Item Valuation)
  id: totrans-472
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 12（单一物品估值的不可区分集）
- en: 'Let $\mathbf{v}$ be a valuation profile for all players in the game, and let
    $\mathcal{L}^{*}(\mathbf{v})$ denote the set of optimal allocations under $\mathbf{v}$,
    specifically, all allocations that are envy free and maximize the total utility.
    The *indistinguishable set* $\mathcal{I}(\mathbf{v})$ for the valuation $\mathbf{v}$
    is defined as:'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathbf{v}$ 是游戏中所有玩家的估值配置，且让 $\mathcal{L}^{*}(\mathbf{v})$ 表示在 $\mathbf{v}$
    下的最优分配集合，具体来说，所有没有嫉妒且最大化总效用的分配。估值 $\mathbf{v}$ 的 *不可区分集* $\mathcal{I}(\mathbf{v})$
    定义为：
- en: '|  | $\mathcal{I}(\mathbf{v})=\{\mathbf{v}^{\prime}\mid\mathcal{A}^{*}(\mathbf{v}^{%
    \prime})\subseteq\mathcal{A}^{*}(\mathbf{v})\}$ |  |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{I}(\mathbf{v})=\{\mathbf{v}^{\prime}\mid\mathcal{A}^{*}(\mathbf{v}^{\prime})\subseteq\mathcal{A}^{*}(\mathbf{v})\}$
    |  |'
- en: That is, $\mathcal{I}(\mathbf{v})$ consists of all valuation profiles $\mathbf{v}^{\prime}$
    such that the set of optimal allocations under $\mathbf{v}^{\prime}$ is a subset
    of the set of optimal allocations under $\mathbf{v}$.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 即，$\mathcal{I}(\mathbf{v})$ 包含所有估值配置 $\mathbf{v}^{\prime}$，使得在 $\mathbf{v}^{\prime}$
    下的最优分配集合是 $\mathbf{v}$ 下最优分配集合的子集。
- en: This definition formalizes the concept that different valuation profiles can
    be indistinguishable in terms of their implications for optimal allocations. From
    the perspective of a player, any valuation $\mathbf{v}^{\prime}$ within the indistinguishable
    set $\mathcal{I}(\mathbf{v})$ does not necessitate a different strategic approach,
    as the optimal allocations remain consistent with those under their original valuation
    $\mathbf{v}$.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义形式化了不同估值配置在最优分配的含义上可能是不可区分的概念。从玩家的角度来看，任何在不可区分集 $\mathcal{I}(\mathbf{v})$
    内的估值 $\mathbf{v}^{\prime}$ 都不需要采取不同的战略方法，因为最优分配与原始估值 $\mathbf{v}$ 下的最优分配保持一致。
- en: This concept is particularly useful in negotiation settings under incomplete
    information. It suggests that players may not need to precisely estimate the opponent’s
    valuations if variations in those valuations lead to the same set of optimal allocations.
    By focusing on the indistinguishable set, players can simplify their strategic
    considerations and concentrate on reaching agreements that fall within the known
    optimal allocations, thereby facilitating more efficient and effective negotiations.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念在不完全信息的谈判环境中特别有用。它表明，如果估值的变化导致相同的最优分配集合，玩家可能不需要精确估计对手的估值。通过关注不可区分集，玩家可以简化战略考虑，专注于达成落在已知最优分配范围内的协议，从而促进更高效和有效的谈判。
- en: 'To evaluate the effectiveness of estimated valuations in capturing the indistinguishable
    set in the game, we compute the precision and recall of estimated valuations with
    respect to $\mathcal{I}(\mathbf{v}_{-i}^{true})$. Precision is defined as:'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估估算估值在捕捉游戏中无法区分集的有效性，我们计算了估算估值相对于$\mathcal{I}(\mathbf{v}_{-i}^{true})$的精度和召回率。精度定义为：
- en: '|  | $\text{Precision}=\mathbbm{1}[\mathbf{V}_{est}\cap\mathcal{I}(\mathbf{v}_{-i}^{%
    true})\neq\emptyset]$ |  |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{精度}=\mathbbm{1}[\mathbf{V}_{est}\cap\mathcal{I}(\mathbf{v}_{-i}^{%
    true})\neq\emptyset]$ |  |'
- en: 'indicating whether at least one estimated valuation falls within the indistinguishable
    set. Recall is defined as:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 表示至少有一个估算估值位于不可区分集内。召回率定义为：
- en: '|  | $\text{Recall}=\frac{\lvert\mathbf{V}_{est}\cap\mathcal{I}(\mathbf{v}_{-i}^{%
    true})\rvert}{\lvert\mathbf{V}_{est}\rvert}$ |  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{召回率}=\frac{\lvert\mathbf{V}_{est}\cap\mathcal{I}(\mathbf{v}_{-i}^{%
    true})\rvert}{\lvert\mathbf{V}_{est}\rvert}$ |  |'
- en: 'representing the proportion of estimated valuations that are indistinguishable
    from the true valuation. We report the average precision and recall across all
    datapoints in Table [16](https://arxiv.org/html/2411.05990v2#S5.T16 "Table 16
    ‣ Indistinguishable Set for Item Valuation ‣ 5.5.2 Both Agents with Workflow ‣
    5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games"):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '表示与真实估值无法区分的估算估值所占比例。我们报告了表[16](https://arxiv.org/html/2411.05990v2#S5.T16
    "Table 16 ‣ Indistinguishable Set for Item Valuation ‣ 5.5.2 Both Agents with
    Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")中所有数据点的平均精度和召回率：'
- en: '| Model | precision w.r.t $\mathcal{I}(\mathbf{v})$ | recall w.r.t $\mathcal{I}(\mathbf{v})$
    |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 相对于$\mathcal{I}(\mathbf{v})$的精度 | 相对于$\mathcal{I}(\mathbf{v})$的召回率 |'
- en: '| --- | --- | --- |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Sonnet | 1.0 | 0.6022 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 1.0 | 0.6022 |'
- en: '| GPT-4o | 1.0 | 0.5633 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 1.0 | 0.5633 |'
- en: '| Opus | 1.0 | 0.5399 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 1.0 | 0.5399 |'
- en: 'Table 16: Performance of estimated valuations with respect to indistinguishable
    of the true valuation.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 表16：估算估值在与真实估值无法区分时的表现。
- en: 'The results indicate that, although precision varies among models in Table [14](https://arxiv.org/html/2411.05990v2#S5.T14
    "Table 14 ‣ Performance of Valuation Estimation ‣ 5.5.2 Both Agents with Workflow
    ‣ 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games") and does not always achieve perfect
    precision, all models contain at least one valuation that is indistinguishable
    from the true valuation. Furthermore, the recall values are relatively high, suggesting
    that more than half of the valuations in the estimated sets are indistinguishable
    from the true valuation. This demonstrates that the models are effective in identifying
    valuations that lead to optimal allocations, even if they do not precisely estimate
    the opponent’s exact valuations.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '结果表明，尽管表[14](https://arxiv.org/html/2411.05990v2#S5.T14 "Table 14 ‣ Performance
    of Valuation Estimation ‣ 5.5.2 Both Agents with Workflow ‣ 5.5 Experiment Result
    ‣ 5 Incomplete-information Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games")中的模型精度有所不同，且并不总是达到完美的精度，所有模型至少包含一个与真实估值无法区分的估值。此外，召回率较高，表明估计集中的一半以上的估值与真实估值无法区分。这表明，即使模型不能精确地估算对手的确切估值，它们仍能有效识别那些能够带来最优分配的估值。'
- en: '<svg class="ltx_picture" height="179.61" id="S5.SS5.SSS2.Px3.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,179.61) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 161.41)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for workflow-LLM v.
    workflow-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="129.91" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Achievement of Near-Optimal
    Allocations: When both agents utilize the workflow, all models consistently achieve
    allocations that are close to the best possible outcomes. • Reversal in Performance
    Rankings: The performance hierarchy observed without the workflow is reversed
    in this setting. Claude-3 Opus exhibits the highest performance, followed by GPT-4o
    and then Claude-3.5 Sonnet. • Precision in Valuation Estimation: The workflow-enhanced
    LLMs display remarkable precision in estimating the opponent’s valuations. They
    effectively reduce the set of possible valuations to as few as 2 or 3 options
    and all of them are precise in recovering an indistiguishable estimated valuation
    for the true valuation. This strong performance indicates effective information
    flow in negotiations based on the workflow.</foreignobject></g></g></svg>'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="179.61" id="S5.SS5.SSS2.Px3.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,179.61) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 161.41)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for workflow-LLM v.
    workflow-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="129.91" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Achievement of Near-Optimal
    Allocations: When both agents utilize the workflow, all models consistently achieve
    allocations that are close to the best possible outcomes. • Reversal in Performance
    Rankings: The performance hierarchy observed without the workflow is reversed
    in this setting. Claude-3 Opus exhibits the highest performance, followed by GPT-4o
    and then Claude-3.5 Sonnet. • Precision in Valuation Estimation: The workflow-enhanced
    LLMs display remarkable precision in estimating the opponent’s valuations. They
    effectively reduce the set of possible valuations to as few as 2 or 3 options
    and all of them are precise in recovering an indistiguishable estimated valuation
    for the true valuation. This strong performance indicates effective information
    flow in negotiations based on the workflow.</foreignobject></g></g></svg>'
- en: 5.5.3 One Agent with Workflow
  id: totrans-491
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3 一个代理使用工作流
- en: 'In this section, we present experimental results where only one LLM-based agent
    employs the proposed negotiation workflow, while the other agent negotiates using
    direct prompting without the workflow. Specifically, we conduct experiments in
    two scenarios:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们呈现了实验结果，其中只有一个基于LLM的代理采用了所提出的谈判工作流，而另一个代理则使用直接提示进行谈判而不使用工作流。具体来说，我们在两种情境下进行实验：
- en: •
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Workflow-LLM vs. Raw-LLM: OnlyAlice uses the workflow, and Bob uses direct
    prompting.'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Workflow-LLM与Raw-LLM：仅有Alice使用工作流，而Bob使用直接提示。
- en: •
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Raw-LLM vs. Workflow-LLM: Only Bob uses the workflow, andAlice uses direct
    prompting.'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Raw-LLM与Workflow-LLM：仅有Bob使用工作流，而Alice使用直接提示。
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判轮次 | 协议 | Alice得分 | Bob得分 | PO | EF | 总奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sonnet | 6.91 | 0.9773 | 4.88 | 6.57 | 0.6136 | 0.5909 | 11.45 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 6.91 | 0.9773 | 4.88 | 6.57 | 0.6136 | 0.5909 | 11.45 |'
- en: '| GPT-4o | 11.84 | 0.8182 | 3.66 | 6.18 | 0.5909 | 0.3636 | 9.84 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 11.84 | 0.8182 | 3.66 | 6.18 | 0.5909 | 0.3636 | 9.84 |'
- en: '| Opus | 3.86 | 0.9091 | 5.09 | 5.53 | 0.6136 | 0.5909 | 10.52 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 3.86 | 0.9091 | 5.09 | 5.53 | 0.6136 | 0.5909 | 10.52 |'
- en: 'Table 17: Workflow-LLM vs. Raw-LLM'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17：工作流程-LLM 与原始-LLM
- en: '| Model | Negotiation Round | Agreement | Alice score | Bob score | PO | EF
    | total reward |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 谈判回合 | 协议 | Alice 得分 | Bob 得分 | PO | EF | 总奖励 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sonnet | 6.45 | 1.0000 | 6.39 | 5.70 | 0.7727 | 0.5909 | 12.09 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| Sonnet | 6.45 | 1.0000 | 6.39 | 5.70 | 0.7727 | 0.5909 | 12.09 |'
- en: '| GPT-4o | 11.36 | 0.8181 | 5.75 | 4.14 | 0.6136 | 0.5227 | 9.89 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 11.36 | 0.8181 | 5.75 | 4.14 | 0.6136 | 0.5227 | 9.89 |'
- en: '| Opus | 3.89 | 0.7955 | 4.86 | 4.57 | 0.4318 | 0.5455 | 9.43 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| Opus | 3.89 | 0.7955 | 4.86 | 4.57 | 0.4318 | 0.5455 | 9.43 |'
- en: 'Table 18: Raw-LLM vs. Workflow-LLM'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 表 18：原始-LLM 与工作流程-LLM
- en: 'An interesting phenomenon emerges from the results: the agent not using the
    workflow tends to achieve a higher individual reward than the agent using the
    workflow. This outcome can be attributed to the following factors: (1) The workflow-guided
    agent is designed to consider envy freeness from both its own perspective and
    the opponent’s perspective. This consideration leads the agent to make more cooperative
    offers, potentially sacrificing some of its own utility to achieve fairness. (2)
    The non-workflow agent, lacking such constraints, may act more self-interestedly,
    proposing allocations that favor itself without ensuring fairness.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中出现了一个有趣的现象：不使用工作流程的代理往往能获得比使用工作流程的代理更高的个人奖励。这个结果可以归因于以下因素：（1）工作流程引导的代理设计时考虑了从自身和对手的角度来看“无嫉妒性”。这种考虑使得代理做出更多合作性的提议，可能会牺牲部分自身效用以实现公平。（2）不使用工作流程的代理没有这种约束，可能会更自利，提出有利于自己的分配，而不保证公平性。
- en: As a result, when the workflow agent negotiates with a non-workflow agent, the
    workflow agent may have to accept less favorable allocations to reach an agreement,
    or negotiations may stall if the workflow agent rejects unfair offers from the
    non-workflow agent.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，当工作流程代理与非工作流程代理谈判时，工作流程代理可能需要接受较不有利的分配以达成协议，或者如果工作流程代理拒绝非工作流程代理的不公平提议，谈判可能会陷入僵局。
- en: 5.6 To adopt the workflow or not?
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 是否采用工作流程？
- en: 'The experiment result in section [5.5.3](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3
    "5.5.3 One Agent with Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information
    Game with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    raises a critical game-theoretic question: is it rational for an agent to adopt
    the workflow when the opponent may not do the same? Should a player use the workflow
    given the potential for exploitation by a non-cooperative opponent? To address
    this, we represent the decision-making scenario using payoff matrices, where each
    agent has two strategic choices: (1) Use Workflow: Apply the proposed negotiation
    workflow, and (2) Do Not Use Workflow: Engage in direct prompting without the
    workflow. The payoff matrix for each model is presented in Table [19](https://arxiv.org/html/2411.05990v2#S5.T19
    "Table 19 ‣ 5.6 To adopt the workflow or not? ‣ 5 Incomplete-information Game
    with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games").'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '第[5.5.3节](https://arxiv.org/html/2411.05990v2#S5.SS5.SSS3 "5.5.3 One Agent
    with Workflow ‣ 5.5 Experiment Result ‣ 5 Incomplete-information Game with Negotiation
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")的实验结果提出了一个关键的博弈论问题：当对手可能不采用相同的工作流程时，代理采用该工作流程是否合理？考虑到非合作对手可能的利用，玩家是否应该使用该工作流程？为了回答这个问题，我们通过收益矩阵表示决策场景，其中每个代理有两种战略选择：（1）使用工作流程：应用提出的谈判工作流程，（2）不使用工作流程：直接进行提示，不采用工作流程。每个模型的收益矩阵见表[19](https://arxiv.org/html/2411.05990v2#S5.T19
    "Table 19 ‣ 5.6 To adopt the workflow or not? ‣ 5 Incomplete-information Game
    with Negotiation ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")。'
- en: '| Models | Sonnet | GPT-4o | Opus |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Sonnet | GPT-4o | Opus |'
- en: '| --- | --- | --- | --- |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Actions | use | not use | use | not use | use | not use |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 使用 | 不使用 | 使用 | 不使用 | 使用 | 不使用 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| use | 5.82, 6.16 | 4.88, 6.57 | 5.93, 6.25 | 3.66, 6.18 | 5.82, 6.50 | 5.09,5.53
    |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 使用 | 5.82, 6.16 | 4.88, 6.57 | 5.93, 6.25 | 3.66, 6.18 | 5.82, 6.50 | 5.09,
    5.53 |'
- en: '| not use | 6.39, 5.07 | 5.55, 5.57 | 5.75, 4.14 | 2.80, 4.38 | 4.86,4.57 |
    2.80,4.38 |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 不使用 | 6.39, 5.07 | 5.55, 5.57 | 5.75, 4.14 | 2.80, 4.38 | 4.86, 4.57 | 2.80,
    4.38 |'
- en: 'Table 19: Payoff matrices for using the workflow or not for the three models'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19：是否使用工作流程的收益矩阵（适用于三种模型）
- en: 'For these payoff matrices, each cell represents the payoffs ($u_{Alice}$, $u_{Bob}$)
    under the corresponding strategies. For instance, in the payoff matrix for GPT-4o-based
    agent: (1) When both agents use the workflow, the payoffs are (5.93, 6.25) (2)
    WhenAlice uses the workflow and Bob does not, the payoffs are (3.66, 6.18) (3)
    WhenAlice does not use the workflow and Bob does, the payoffs are (5.75, 4.14)
    (4) When both agents do not use the workflow, the payoffs are (2.80, 4.38).'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些收益矩阵，每个单元格表示在对应策略下的收益（$u_{Alice}$, $u_{Bob}$）。例如，在基于GPT-4o的代理的收益矩阵中：（1）当两个代理都使用工作流时，收益为（5.93,
    6.25）；（2）当爱丽丝使用工作流而鲍勃不使用时，收益为（3.66, 6.18）；（3）当爱丽丝不使用工作流而鲍勃使用时，收益为（5.75, 4.14）；（4）当两个代理都不使用工作流时，收益为（2.80,
    4.38）。
- en: For Claude-3.5 Sonnet
  id: totrans-521
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对于Claude-3.5 Sonnet
- en: 'Alice’s dominant strategy is not to use the workflow. If Bob uses the workflow:
    thenAlice’s payoff for using the workflow is 5.82 and that for not using the workflow
    is 6.39\. Since 6.39 > 5.82,Alice’s best response is to not use the workflow;
    If Bob does not use the workflow:Alice’s payoff for using the workflow is 4.88
    and that for not using the workflow is 5.55\. Since 5.55 > 4.88,Alice’s best response
    is again to not use the workflow. Thus “not using the workflow” is the dominant
    strategy forAlice. Assuming rational behavior, Bob will also choose to not use
    the workflow. IfAlice does not use the workflow, Bob’s payoff for not using the
    workflow is 5.57, which is greater than his payoff of 5.07 when he uses the workflow
    (since 5.57 > 5.07). Consequently, Bob’s best response is to not use the workflow
    whenAlice does not use it.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 爱丽丝的优势策略是不使用工作流。如果鲍勃使用工作流：那么爱丽丝使用工作流的收益为5.82，而不使用工作流的收益为6.39。由于6.39 > 5.82，爱丽丝的最佳回应是不使用工作流；如果鲍勃不使用工作流：爱丽丝使用工作流的收益为4.88，而不使用工作流的收益为5.55。由于5.55
    > 4.88，爱丽丝的最佳回应仍然是不使用工作流。因此，“不使用工作流”是爱丽丝的优势策略。假设理性行为，鲍勃也会选择不使用工作流。如果爱丽丝不使用工作流，鲍勃不使用工作流的收益为5.57，这大于他使用工作流时的收益5.07（因为5.57
    > 5.07）。因此，鲍勃在爱丽丝不使用工作流时的最佳回应是也不使用工作流。
- en: The Nash Equilibrium in this setting occurs when both agents choose not to use
    the workflow, resulting in payoffs of 5.55 forAlice and 5.57 for Bob. This is
    a classic Prisoner’s Dilemma situation, where the Nash Equilibrium is not pareto
    optimal. If both agents were to use the workflow, they would receive higher individual
    payoffs – 5.82 forAlice and 6.16 for Bob – and a higher combined total reward
    of 12.48, compared to 11.12 when both do not use the workflow. This indicates
    that both agents could be better off by mutually adopting the workflow, achieving
    an outcome that is Pareto superior to the Nash Equilibrium.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，纳什均衡发生在两个代理都选择不使用工作流时，爱丽丝的收益为5.55，鲍勃的收益为5.57。这是一个经典的囚徒困境情境，其中纳什均衡并不是帕累托最优的。如果两个代理都使用工作流，他们将获得更高的个人收益——爱丽丝为5.82，鲍勃为6.16——以及更高的总奖励为12.48，相比之下，当两者都不使用工作流时，总奖励为11.12。这表明，如果两个代理都相互采用工作流，他们的收益会更好，达成的结果比纳什均衡更具帕累托优越性。
- en: The dilemma arises because, while mutual cooperation leads to a better collective
    outcome, each agent has an incentive to deviate unilaterally from the cooperative
    strategy to increase their individual payoff. To achieve the pareto optimal outcome
    where both agents use the workflow, additional mechanisms are necessary to align
    individual incentives with collective welfare. One possible solution is to introduce
    a punishment strategy or enforceable agreements that discourage unilateral defection.
    For example, implementing repeated interactions with memory of past behavior could
    promote cooperation through strategies like “tit-for-tat,” where agents reciprocate
    the opponent’s previous action. Such mechanisms can alter the payoff structure
    by imposing future costs on defection, thereby making cooperation the rational
    choice in the long run.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 困境的出现是因为，尽管相互合作能带来更好的集体结果，但每个代理都有动力单方面偏离合作策略，以增加自己的收益。为了实现两个代理都使用工作流的帕累托最优结果，需要额外的机制来将个人激励与集体福利对齐。一个可能的解决方案是引入惩罚策略或可执行的协议，以防止单方面背离。例如，通过实施重复互动并记住过去的行为，可以通过“以牙还牙”等策略促进合作，在这种策略中，代理会回报对方的先前行为。这些机制可以通过对偏离行为施加未来成本来改变收益结构，从而使合作在长期内成为理性选择。
- en: For GPT-4o
  id: totrans-525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对于GPT-4o
- en: The situation differs from Claude-3.5 because here, the Nash Equilibrium strategy
    is to use the workflow, which can be deduced from Iterative elimination of dominated
    strategy.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况与Claude-3.5不同，因为在这里，纳什均衡策略是使用工作流，这可以通过迭代消除支配策略来推导出来。
- en: Firstly,Alice’s dominant strategy is to use the workflow. If Bob uses the workflow:Alice’s
    payoff for using the workflow is 5.93 while for not using the workflow is 5.75\.
    Since 5.93 > 5.75,Alice should use the workflow. But if Bob does not use the workflow:Alice’s
    payoff for using the workflow is 3.66 while for not using the workflow is 2.80\.
    Since 3.66 > 2.80,Alice should again choose to use the workflow. Thus, using the
    workflow is a dominant strategy forAlice, as it yields a higher payoff regardless
    of Bob’s choice. While Bob’s lack of workflow may allow him to obtain a slightly
    higher reward in some instances,Alice still achieves a greater overall benefit
    by employing the workflow. Knowing thatAlice’s dominant strategy is to use the
    workflow, Bob can anticipateAlice’s choice. AsAlice will adopt the workflow, then
    if Bob does not adopt the workflow, he would obtain payoff 6.18; if Bob adopts
    the workflow as well, he will obtain 6.25\. As 6.25 is larger than 4.14, Bob should
    use the workflow. Thus, the rational choice for both agents is to use the workflow.
    This choice represents a Nash equilibrium, where neither agent has an incentive
    to deviate unilaterally from the strategy of using the workflow. It is also the
    pareto optimal strategy.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Alice的主导策略是使用工作流。如果Bob使用工作流：Alice使用工作流时的收益为5.93，而不使用工作流时的收益为5.75。因为5.93 >
    5.75，Alice应该选择使用工作流。但如果Bob不使用工作流：Alice使用工作流时的收益为3.66，而不使用工作流时的收益为2.80。因为3.66 >
    2.80，Alice仍然应该选择使用工作流。因此，使用工作流是Alice的主导策略，因为无论Bob的选择是什么，它都会带来更高的收益。虽然Bob的不使用工作流可能让他在某些情况下获得略高的奖励，但Alice通过使用工作流仍然能够获得更大的总体利益。知道Alice的主导策略是使用工作流后，Bob可以预见到Alice的选择。如果Alice采用工作流，那么如果Bob不采用工作流，他将获得6.18的收益；如果Bob也采用工作流，他将获得6.25的收益。因为6.25大于4.14，Bob应该选择使用工作流。因此，两个代理的理性选择是使用工作流。这一选择代表了一个纳什均衡，在这个均衡中，任何一个代理都没有动机单方面偏离使用工作流的策略。这也是帕累托最优的策略。
- en: For Claude-3 Opus
  id: totrans-528
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对于Claude-3 Opus
- en: The situation is analogous to that of GPT-4o, wherein the Nash Equilibrium is
    to use the workflow. Utilizing the workflow constitutes a dominant strategy for
    bothAlice and Bob.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况类似于GPT-4o，其中纳什均衡是使用工作流。使用工作流构成了Alice和Bob的主导策略。
- en: 'Specifically,Alice’s dominant strategy is to use the workflow, as her payoffs
    are superior regardless of Bob’s choice: 5.82 versus 4.86 when Bob uses the workflow,
    and 5.09 versus 2.68 when Bob does not use the workflow. Similarly, Bob’s dominant
    strategy is to use the workflow, as his payoffs are higher irrespective ofAlice’s
    decision: 6.50 versus 5.53 whenAlice uses the workflow, and 4.57 versus 3.02 whenAlice
    does not use the workflow. Consequently, the Nash Equilibrium occurs when both
    agents choose to use the workflow, resulting in payoffs of 5.82 forAlice and 6.50
    for Bob. This outcome is pareto optimal, as neither agent can improve their own
    payoff without diminishing the other’s payoff. Unlike the Claude-3.5 Sonnet scenario,
    where mutual defection led to a suboptimal equilibrium, the Claude-3 Opus scenario
    demonstrates that aligned incentives and the adoption of the workflow can lead
    to mutually beneficial and efficient outcomes.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，Alice的主导策略是使用工作流，因为无论Bob选择什么，她的收益都更高：当Bob使用工作流时，她的收益为5.82，而当Bob不使用工作流时，她的收益为5.09；同样，Bob的主导策略是使用工作流，因为无论Alice做出何种决策，他的收益都更高：当Alice使用工作流时，他的收益为6.50，而当Alice不使用工作流时，他的收益为4.57。因此，纳什均衡发生在两个代理都选择使用工作流时，Alice的收益为5.82，Bob的收益为6.50。这个结果是帕累托最优的，因为没有任何代理可以在不减少另一方收益的情况下改善自己的收益。与Claude-3.5
    Sonnet情境不同，在Claude-3.5 Sonnet中，双方的背叛导致了一个次优的均衡，而Claude-3 Opus情境则表明，通过一致的激励和工作流的采用，可以实现双方互利且高效的结果。
- en: 5.6.1 Comparison and Implications
  id: totrans-531
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.1 比较与启示
- en: 'The analyses of Claude-3.5 Sonnet, GPT-4o, and Claude-3 Opus reveal distinct
    strategic dynamics based on their respective payoff matrices:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 对Claude-3.5 Sonnet、GPT-4o和Claude-3 Opus的分析揭示了基于各自支付矩阵的不同战略动态：
- en: •
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Claude-3.5 Sonnet: Exhibits a classic Prisoner’s Dilemma structure where the
    dominant strategy for both agents is to defect (not use the workflow), leading
    to a Nash Equilibrium that is not pareto optimal.'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Claude-3.5 Sonnet：展示了经典的囚徒困境结构，其中两个代理的主导策略是背叛（不使用工作流），导致一个非帕累托最优的纳什均衡。
- en: •
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GPT-4o and Claude-3 Opus: Both present scenarios where using the workflow is
    a dominant strategy for both agents. This leads to a Nash Equilibrium that is
    also pareto optimal, where both agents achieve higher individual and combined
    payoffs compared to other strategy profiles. These cases demonstrate that when
    strategies are aligned and cooperation is incentivized, agents can achieve mutually
    beneficial outcomes.'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPT-4o 和 Claude-3 Opus：两者都呈现出使用工作流程作为主导策略的场景，这导致了一个同时也是帕累托最优的纳什均衡，在该均衡下，两名代理人的单独和联合收益都比其他策略配置更高。这些案例表明，当策略一致并且合作得到激励时，代理人可以实现互利的结果。
- en: Adopting the workflow consistently results in pareto optimal outcomes across
    different language models. However, the rationality of this choice – specifically,
    whether using the workflow constitutes a Nash Equilibrium—depends on the particular
    characteristics of each language model. In the case of Claude-3.5 Sonnet, which
    demonstrates superior performance when utilizing the workflow, the Nash Equilibrium
    is observed to be the decision not to adopt the workflow. This outcome may be
    attributed to Claude-3.5 Sonnet’s advanced negotiation capabilities, whereby the
    implementation of the workflow inadvertently increases its susceptibility to exploitation.
    Although the workflow provides certain advantages, these benefits are insufficient
    to outweigh the risks associated with its adoption. Consequently, Claude-3.5 Sonnet
    opts to forgo the workflow to mitigate potential exploitation, despite the inherent
    benefits the workflow could offer. This suggests that while the workflow can enhance
    performance, its effectiveness is contingent upon the underlying negotiation strengths
    of the language model, highlighting the necessity for tailored strategies that
    align with each model’s unique capabilities and vulnerabilities.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 一致地采用工作流程会在不同语言模型之间产生帕累托最优结果。然而，这一选择的理性——特别是使用工作流程是否构成纳什均衡——取决于每个语言模型的特定特点。在Claude-3.5
    Sonnet的情况下，当使用工作流程时，表现优越，纳什均衡表现为不采用工作流程的决定。这个结果可能与Claude-3.5 Sonnet的高级谈判能力有关，通过使用工作流程，它在某些情况下不自觉地增加了被利用的风险。尽管工作流程提供了某些优势，但这些好处不足以弥补采用它所带来的风险。因此，Claude-3.5
    Sonnet选择放弃工作流程，以减少潜在的利用风险，尽管工作流程本身具有固有的好处。这表明，尽管工作流程可以提升表现，但其效果取决于语言模型的谈判优势，强调了根据每个模型的独特能力和脆弱性制定量身定制策略的必要性。
- en: '<svg class="ltx_picture" height="293.16" id="S5.SS6.SSS1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,293.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 274.95)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for raw-LLM v. workflow-LLM
    and workflow-LLM v. raw-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="243.45" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Exploitation of Workflow-Enhanced
    LLMs: Empirical results indicate that LLMs operating without the workflow often
    achieve higher payoffs when interacting with workflow-enhanced LLMs. This suggests
    that the structured workflow can be exploited by opponents not utilizing it, potentially
    leading to suboptimal outcomes for the workflow-enhanced agents. • Strategic Decision
    on Workflow Adoption: The choice of whether to adopt the workflow itself constitutes
    a game-theoretic dilemma. The dominant strategy – opting to use or forgo the workflow
    – depends on the specific characteristics and strategic incentives of the LLM
    models involved. This underscores that the rational decision regarding workflow
    adoption is contingent upon the model’s capabilities and the anticipated behavior
    of opponents. • Meta-Strategy for Workflow Adoption: The question of whether to
    adopt the workflow introduces a need for a meta-strategy. Agents must consider
    not only their immediate negotiation tactics but also the higher-level strategy
    of employing the workflow. This involves weighing the potential benefits of the
    workflow against the risks of exploitation by opponents who may not be using it.
    Developing an effective meta-strategy requires agents to assess the specific context,
    anticipate opponents’ choices, and adapt their approach accordingly to maximize
    their own utility while mitigating vulnerabilities.</foreignobject></g></g></svg>'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="293.16" id="S5.SS6.SSS1.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,293.16) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 274.95)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for raw-LLM v. workflow-LLM
    and workflow-LLM v. raw-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="243.45" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Exploitation of Workflow-Enhanced
    LLMs: Empirical results indicate that LLMs operating without the workflow often
    achieve higher payoffs when interacting with workflow-enhanced LLMs. This suggests
    that the structured workflow can be exploited by opponents not utilizing it, potentially
    leading to suboptimal outcomes for the workflow-enhanced agents. • Strategic Decision
    on Workflow Adoption: The choice of whether to adopt the workflow itself constitutes
    a game-theoretic dilemma. The dominant strategy – opting to use or forgo the workflow
    – depends on the specific characteristics and strategic incentives of the LLM
    models involved. This underscores that the rational decision regarding workflow
    adoption is contingent upon the model’s capabilities and the anticipated behavior
    of opponents. • Meta-Strategy for Workflow Adoption: The question of whether to
    adopt the workflow introduces a need for a meta-strategy. Agents must consider
    not only their immediate negotiation tactics but also the higher-level strategy
    of employing the workflow. This involves weighing the potential benefits of the
    workflow against the risks of exploitation by opponents who may not be using it.
    Developing an effective meta-strategy requires agents to assess the specific context,
    anticipate opponents’ choices, and adapt their approach accordingly to maximize
    their own utility while mitigating vulnerabilities.</foreignobject></g></g></svg>'
- en: 6 Detailed Observation on LLM’s Rationality
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 对LLM理性的详细观察
- en: 'This section provides an additional, comprehensive analysis of the rationality
    exhibited by LLM-based agents from various perspectives. Using Claude-3 Opus as
    a representative example, we examine the rational decision-making capabilities
    of LLMs in single-stage games through the following aspects:'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 本节从多个角度提供了对基于LLM的代理人理性行为的进一步全面分析。以Claude-3 Opus为代表的例子，我们从以下几个方面考察了LLM在单阶段博弈中的理性决策能力：
- en: '1.'
  id: totrans-541
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Consistency of action choices across variations in payoff matrices: By analyzing
    the agents’ decisions in different payoff matrix scenarios, we can determine whether
    the LLM-based agents maintain consistent action choices or if their choices are
    influenced by nuanced changes in the payoff matrix.'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同支付矩阵变化下的行动选择一致性：通过分析代理人在不同支付矩阵场景下的决策，我们可以判断基于LLM的代理人是否保持一致的行动选择，或者它们的选择是否会受到支付矩阵细微变化的影响。
- en: '2.'
  id: totrans-543
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Consistency of action choices across designated personalities in system prompts:
    We investigate the impact of assigned personalities in system prompts on the agents’
    decision-making process. This analysis helps us understand whether the LLM-based
    agents’ rationality is affected by the designated personality or if they maintain
    consistent action choices regardless.'
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统提示中指定的个性之间的行动选择一致性：我们研究了系统提示中指定的个性对代理人决策过程的影响。这一分析帮助我们理解，基于LLM的代理人的理性是否会受到指定个性的影响，或者它们是否会在任何情况下保持一致的行动选择。
- en: '3.'
  id: totrans-545
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Maintenance of rationality under discussion and multi-round discussions: We
    explore how the agents’ rationality evolves when engaged in discussions or multi-round
    interactions. This examination reveals whether the LLM-based agents can maintain
    their rationality or if it is influenced by the communication and negotiation
    processes.'
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在讨论和多轮讨论下的理性维护：我们探索了代理人在讨论或多轮互动中理性如何演变。通过这一研究，我们可以了解到，基于LLM的代理人是否能够维持其理性，或者是否受到沟通和谈判过程的影响。
- en: 6.1 Experiment Setup
  id: totrans-547
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 实验设置
- en: In each experimental setup, we conduct 10 iterations utilizing a temperature
    value of 1 for the Claude-3 model. Each player within these setups is represented
    by an LLM-based agent. For each configuration, we document the probability distribution
    of the actions executed by these agents.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个实验设置中，我们进行10次迭代，使用Claude-3模型的温度值为1。在这些设置中的每个参与者都由一个基于LLM的代理表示。对于每个配置，我们记录这些代理执行的动作的概率分布。
- en: 6.2 Variance of Payoff Matrix
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 支付矩阵的方差
- en: Definition 13 (Nash-Equilibrium invariant perturbation)
  id: totrans-550
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 13（纳什均衡不变扰动）
- en: A Nash Equilibrium Invariant Perturbation is a modification to the numerical
    values within a game’s payoff matrix that preserves the set of Nash equilibria.
    Formally, consider a finite game $G=(N,\{S_{o}\}_{i\in N},\{\pi_{i}\}_{i\in N})$
    where $N$ is the set of players, $S_{i}$ is the strategy set for player $i$, $\pi_{i}:S\to\mathcal{R}$
    is the payoff function for player $i$.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 纳什均衡不变扰动是对博弈支付矩阵中的数值进行的修改，它保持了纳什均衡的集合。形式上，考虑一个有限博弈 $G=(N,\{S_{o}\}_{i\in N},\{\pi_{i}\}_{i\in
    N})$，其中 $N$ 是玩家集合，$S_{i}$ 是玩家 $i$ 的策略集，$\pi_{i}:S\to\mathcal{R}$ 是玩家 $i$ 的支付函数。
- en: A perturbed game $G^{\prime}=(N,\{S_{i}\}_{i\in N},\{\pi_{i}^{\prime}\}_{i\in
    N})$ of the game $G$ is defined by adjusted payoff functions $\pi_{i}^{\prime}=\pi_{i}+\delta_{i}$
    where $\delta_{i}:S\to\mathcal{R}$ represnts the change in payoffs for player
    $i$. The perturbation $\delta=\{\delta_{i}\}_{i\in N}$ is termed a Nash Equilibrium
    Invariant Perturbation if the set of Nash equilibria remains unchanged between
    the original game $G$ and the perturbed game $G^{\prime}$.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 一个扰动后的博弈 $G^{\prime}=(N,\{S_{i}\}_{i\in N},\{\pi_{i}^{\prime}\}_{i\in N})$ 是由调整后的支付函数
    $\pi_{i}^{\prime}=\pi_{i}+\delta_{i}$ 定义的，其中 $\delta_{i}:S\to\mathcal{R}$ 表示玩家
    $i$ 的支付变化。如果在原始博弈 $G$ 和扰动后的博弈 $G^{\prime}$ 之间，纳什均衡集合保持不变，则称扰动 $\delta=\{\delta_{i}\}_{i\in
    N}$ 为纳什均衡不变扰动。
- en: Traditional evaluation of LLM on rationality utilize traditional game-theoretic
    scenarios. If LLMs do have rationality, then their rationality should be consistent
    across different instantiation of the payoff matrix.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的LLM理性评估利用了传统的博弈论场景。如果LLM确实具备理性，那么它们的理性应该在不同支付矩阵的实例化中保持一致。
- en: To study this, we use the traditional game of Prisoner’s Dilemma and Stag Hunt.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究这一点，我们使用了传统的囚徒困境博弈和鹿猎博弈。
- en: 'We introduce certain modifications to the traditional payoff matrices while
    ensuring that the rational choices remain unaltered. Despite these variations,
    the Nash Equilibrium remains consistent across all modifications. Consequently,
    if an agent is rational, it is expected to make the same rational choices in each
    scenario. We present the variations here:'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对传统的支付矩阵进行了某些修改，同时确保理性选择不受影响。尽管存在这些变化，纳什均衡在所有修改中保持一致。因此，如果一个代理是理性的，它预计在每种情况下都做出相同的理性选择。我们在这里展示这些变化：
- en: '|  | action 1 | action 2 |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作 1 | 动作 2 |'
- en: '| --- | --- | --- |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| action 1 | 300, 300 | 0, 301 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 动作 1 | 300, 300 | 0, 301 |'
- en: '| action 2 | 301, 000 | 1, 101 |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| 动作 2 | 301, 000 | 1, 101 |'
- en: '(a) Table 20a: Variation 1: payoff matrix for Prisoner’s Dilemma'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表 20a：变化 1：囚徒困境的支付矩阵
- en: '|  | action 1 | action 2 |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作 1 | 动作 2 |'
- en: '| --- | --- | --- |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| action 1 | 3, -300 | -300, -500 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 动作 1 | 3, -300 | -300, -500 |'
- en: '| action 2 | 5, -300 | -299, -299 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 动作 2 | 5, -300 | -299, -299 |'
- en: '(b) Table 20b: Variation 2: payoff matrix for Prisoner’s Dilemma'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 表 20b：变化 2：囚徒困境的支付矩阵
- en: '|  | action 1 | action 2 |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作 1 | 动作 2 |'
- en: '| --- | --- | --- |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| action 1 | 300, 300 | 0, 1 |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 动作 1 | 300, 300 | 0, 1 |'
- en: '| action 2 | 301, 000 | 1, 1 |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 动作 2 | 301, 000 | 1, 1 |'
- en: '(a) Table 21a: Variation 1: payoff matrix for Stag Hunt'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 表 21a：变化 1：鹿猎博弈的支付矩阵
- en: '|  | action 1 | action 2 |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作 1 | 动作 2 |'
- en: '| --- | --- | --- |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| action 1 | -93, -300 | -100, -99 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 动作 1 | -93, -300 | -100, -99 |'
- en: '| action 2 | -99, -100 | 0-99, -99 |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| 动作 2 | -99, -100 | 0-99, -99 |'
- en: '(b) Table 21b: Variation 2: payoff matrix for Stag Hunt'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 表 21b：变化 2：鹿猎博弈的支付矩阵
- en: '![Refer to caption](img/d04f27ff3296ecbab60a4414d3bc861a.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d04f27ff3296ecbab60a4414d3bc861a.png)'
- en: 'Figure 5: Agents’ performance under different payoff matrix for Prisoner’s
    Dilemma'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同支付矩阵下代理的表现（囚徒困境）
- en: '![Refer to caption](img/ce85d796be1a5e5e3520ac7b5696ae71.png)'
  id: totrans-578
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce85d796be1a5e5e3520ac7b5696ae71.png)'
- en: 'Figure 6: Agents’ performance under different payoff matrix for Stag Hunt'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同支付矩阵下代理的表现（鹿猎博弈）
- en: 'We conduct experiments on these two games with their variations. Figures [5](https://arxiv.org/html/2411.05990v2#S6.F5
    "Figure 5 ‣ 6.2 Variance of Payoff Matrix ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") and [6](https://arxiv.org/html/2411.05990v2#S6.F6
    "Figure 6 ‣ 6.2 Variance of Payoff Matrix ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") contain experiment
    results of the action distribution. Contrary to the expectation that performance
    should remain unaffected by payoff variations, the results demonstrate inconsistent
    performance distribution across different payoff scenarios. In the case of Prisoner’s
    Dilemma, the probability of the rational situation (Action 2, Action 2) is significantly
    higher in the classic payoff matrix but considerably lower in the two variations.
    Similarly, in Stag Hunt, the actions taken also vary across different payoff scenarios.
    These findings suggest that the LLMs are either (1) not consistently making rational
    decisions or (2) their rationality is heavily influenced by other irrational factors.'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对这两种游戏及其变体进行了实验。图[5](https://arxiv.org/html/2411.05990v2#S6.F5 "图 5 ‣ 6.2
    奖励矩阵的变异 ‣ 6 详细观察 LLM 的理性 ‣ 博弈论 LLM：谈判游戏的代理工作流")和[6](https://arxiv.org/html/2411.05990v2#S6.F6
    "图 6 ‣ 6.2 奖励矩阵的变异 ‣ 6 详细观察 LLM 的理性 ‣ 博弈论 LLM：谈判游戏的代理工作流")包含了动作分布的实验结果。与预期的表现应该不受奖励变化影响不同，结果表明在不同奖励场景下，表现分布并不一致。在囚徒困境中，在经典奖励矩阵中的理性情境（动作
    2，动作 2）的概率显著较高，但在两种变体中则大幅降低。同样，在猎鹿博弈中，采取的行动在不同的奖励场景下也有所不同。这些发现表明，LLMs 要么是（1）不一致地做出理性决策，或者（2）他们的理性受到其他非理性因素的强烈影响。
- en: 6.3 Variation of Personality
  id: totrans-581
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 个性化的变化
- en: 'In addition to investigating the impact of payoff variations on the rationality
    of agents, we also examine whether the personality denoted in the system prompt
    influences the agents’ rationality. If the agents are consistent in their computation
    and calculation of the reward, then their rationality should not be affected by
    the assigned “personality”. The system prompt template used for this experiment
    is as follows:'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 除了研究报酬变化对代理理性的影响外，我们还考察了系统提示中所表示的个性是否会影响代理的理性。如果代理在计算和计算奖励时保持一致，那么他们的理性不应受所分配的“个性”的影响。用于本实验的系统提示模板如下：
- en: '[PRE5]'
  id: totrans-583
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the personality variable, we select six different but common adjectives:
    compassionate, friendly, helpful, pragmatic, rational, and witty. The results
    of this experiment are presented in Figure [7](https://arxiv.org/html/2411.05990v2#S6.F7
    "Figure 7 ‣ 6.3 Variation of Personality ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games"). The findings indicate
    that the agents’ performance varies significantly according to the assigned personality.
    The “witty” personality yields the game-theoretically most rational option, while
    the “rational” personality performs slightly worse. For other personalities, such
    as “compassionate”, “friendly”, “helpful”, and “pragmatic”, the agents exhibit
    decreased rationality and frequently make irrational choices.'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 对于个性变量，我们选择了六个不同但常见的形容词：富有同情心、友好、乐于助人、务实、理性和机智。该实验的结果如图[7](https://arxiv.org/html/2411.05990v2#S6.F7
    "图 7 ‣ 6.3 个性化的变化 ‣ 6 详细观察 LLM 的理性 ‣ 博弈论 LLM：谈判游戏的代理工作流")所示。研究结果表明，代理的表现会根据所分配的个性化而显著变化。具有“机智”个性的代理提供了博弈论中最理性的选择，而具有“理性”个性的代理表现略差。对于其他个性，如“富有同情心”、“友好”、“乐于助人”和“务实”，代理表现出理性下降，并且经常做出不理性的选择。
- en: '![Refer to caption](img/97e53b29e006aa17d2116003e6f8de7a.png)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/97e53b29e006aa17d2116003e6f8de7a.png)'
- en: 'Figure 7: Agents’ performance under different system prompt with personality'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：不同系统提示下，具有个性化的代理表现
- en: 6.4 Does negotiation affect rationality?
  id: totrans-587
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 谈判是否会影响理性？
- en: In certain situations, the decision-making process of agents can be influenced
    by discussions with other agents. For instance, in the game of Stag Hunt, effective
    negotiation can foster trust between players, enabling them to recognize that
    individual efforts to capture a hare are not as advantageous as cooperating to
    hunt a stag. Consequently, successful communication should encourage players to
    select the pareto optimal Nash equilibrium instead of either of the two available
    equilibriums. However, there are scenarios where negotiation does not impact the
    outcome. In the case of Prisoner’s Dilemma, communication fails to establish trust,
    as each player’s dominant strategy is to defect regardless of the other player’s
    action. Therefore, communication is unlikely to affect performance, as the rational
    choice remains unchanged.
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，代理人的决策过程可以受到与其他代理人讨论的影响。例如，在“鹿兔猎”游戏中，有效的谈判能够促进玩家之间的信任，使他们认识到单独捕捉兔子并不如合作猎鹿那样有利。因此，成功的沟通应该鼓励玩家选择帕累托最优的纳什均衡，而不是选择可用的两个均衡中的任意一个。然而，也有一些情景中，谈判不会影响结果。在“囚徒困境”中，沟通未能建立信任，因为每个玩家的主导策略都是背叛，无论另一个玩家采取什么行动。因此，沟通不太可能影响表现，因为理性选择保持不变。
- en: 'In our current experimentation, we focus on four game-theoretic scenarios:
    Stag Hunt, Battle of the Sexes, Prisoner’s Dilemma, and Rock-Paper-Scissors. The
    impact of negotiation on these games varies as follows:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的实验中，我们集中研究了四种博弈论情景：鹿兔猎、性别之战、囚徒困境和石头剪子布。这些游戏中谈判的影响如下：
- en: •
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stag Hunt: In this game, negotiation plays a significant role in achieving
    the pareto optimal rational choice. Effective communication between players can
    foster trust and encourage cooperation, leading to better outcomes for both parties.'
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鹿兔猎：在这个游戏中，谈判在达成帕累托最优的理性选择中起着重要作用。玩家之间的有效沟通可以促进信任并鼓励合作，从而为双方带来更好的结果。
- en: •
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Battle of the Sexes: Negotiation is crucial for enhancing coordination between
    the two players in this game. By discussing their preferences and intentions,
    players can reach a more coherent and mutually beneficial strategy.'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性别之战：谈判在增强两名玩家之间的协调性中至关重要。通过讨论彼此的偏好和意图，玩家可以达成更为一致和互利的策略。
- en: •
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prisoner’s Dilemma: The presence of a single Nash Equilibrium in this game
    renders negotiation irrelevant. Regardless of any discussion, the rational choice
    for both players remains the same, and the outcome is determined by their individual
    decisions.'
  id: totrans-595
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 囚徒困境：该游戏只有一个纳什均衡，因此谈判无关紧要。无论是否进行讨论，两名玩家的理性选择始终不变，结果由他们各自的决定决定。
- en: 'To investigate the impact of communication on agents’ choices, we conduct experiments
    on both games with and without communication. In the communication setup, agents
    are allowed to exchange messages before making decisions. We record the action
    distribution of agents in each setup and present the results in Figure [8](https://arxiv.org/html/2411.05990v2#S6.F8
    "Figure 8 ‣ 6.4 Does negotiation affect rationality? ‣ 6 Detailed Observation
    on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    for 0, 1, and 2 rounds of negotiation.'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究沟通对代理人选择的影响，我们在有沟通和没有沟通的两种游戏条件下进行实验。在沟通设定中，代理人可以在做出决策之前交换信息。我们记录了每种设定下代理人的行动分布，并在图[8](https://arxiv.org/html/2411.05990v2#S6.F8
    "Figure 8 ‣ 6.4 Does negotiation affect rationality? ‣ 6 Detailed Observation
    on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")中展示了0、1和2轮谈判的结果。'
- en: '![Refer to caption](img/4bff6e36f43be8101b274d72d38cb2f0.png)![Refer to caption](img/9bd30c846aefa80542e9dcefb7ff1de5.png)![Refer
    to caption](img/eacf1f15bd8a4931d8cbcdf2929e0ad8.png)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4bff6e36f43be8101b274d72d38cb2f0.png)![参考说明](img/9bd30c846aefa80542e9dcefb7ff1de5.png)![参考说明](img/eacf1f15bd8a4931d8cbcdf2929e0ad8.png)'
- en: 'Figure 8: Agents’ performance under different numbers of negotiation: 0-round,
    1-round, 2-round, and 3-round from left to right for the four games'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：代理人在不同轮次谈判下的表现：从左至右分别为四个游戏的0轮、1轮、2轮和3轮。
- en: 'Based on our observations, we have identified several inconsistencies between
    our expectations and the actual outcomes in the various game-theoretic scenarios:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的观察，我们发现了在不同博弈论情景中，期望与实际结果之间存在一些不一致。
- en: •
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Stag Hunt (Figure [8](https://arxiv.org/html/2411.05990v2#S6.F8 "Figure 8 ‣
    6.4 Does negotiation affect rationality? ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") (b)): As expected,
    negotiation leads to the adoption of the pareto optimal strategy. This result
    demonstrates that effective communication can facilitate cooperation and achieve
    better outcomes for all parties involved.'
  id: totrans-601
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '鹿群狩猎（图 [8](https://arxiv.org/html/2411.05990v2#S6.F8 "Figure 8 ‣ 6.4 Does negotiation
    affect rationality? ‣ 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games") (b)）：正如预期，谈判导致采用帕累托最优策略。这个结果表明，有效的沟通可以促进合作，并为所有参与方带来更好的结果。'
- en: •
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Battle of the Sexes: (Figure [8](https://arxiv.org/html/2411.05990v2#S6.F8
    "Figure 8 ‣ 6.4 Does negotiation affect rationality? ‣ 6 Detailed Observation
    on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    (c)): Our findings indicate an increase in coordination between players as the
    number of negotiation rounds increases. This outcome suggests that negotiation
    plays a crucial role in enhancing mutual understanding and promoting more coherent
    strategies.'
  id: totrans-603
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '性别博弈：（图 [8](https://arxiv.org/html/2411.05990v2#S6.F8 "Figure 8 ‣ 6.4 Does
    negotiation affect rationality? ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") (c)）：我们的研究结果表明，随着谈判轮次的增加，玩家之间的协调性有所提高。这个结果表明，谈判在促进相互理解和推动更一致的策略方面发挥了关键作用。'
- en: •
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prisoner’s Dilemma (Figure [8](https://arxiv.org/html/2411.05990v2#S6.F8 "Figure
    8 ‣ 6.4 Does negotiation affect rationality? ‣ 6 Detailed Observation on LLM’s
    Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") (a)):
    Contrary to expectations, players consistently gravitate towards the dominated
    strategy after negotiation. This result is surprising, as the presence of a single
    Nash Equilibrium should render negotiation irrelevant.'
  id: totrans-605
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '囚徒困境（图 [8](https://arxiv.org/html/2411.05990v2#S6.F8 "Figure 8 ‣ 6.4 Does negotiation
    affect rationality? ‣ 6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic
    LLM: Agent Workflow for Negotiation Games") (a)）：与预期相反，玩家在谈判后始终倾向于选择支配策略。这个结果令人惊讶，因为单一的纳什均衡存在应该使得谈判变得无关紧要。'
- en: In summary, our findings suggest that negotiation does not always lead to the
    pareto optimal decision-making, and in some cases, it may even result in the loss
    of rationality. Further investigation is necessary to understand the underlying
    factors contributing to these unexpected outcomes.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的发现表明，谈判并不总是能导致帕累托最优的决策，在某些情况下，甚至可能导致理性的丧失。需要进一步研究以了解导致这些意外结果的潜在因素。
- en: 6.5 How does prompt affect the influence of negotiation?
  id: totrans-607
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 提示如何影响谈判的作用？
- en: Some people may want to ask can we use simple prompt engineering to control
    the effect of negotiation.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人可能会想，是否可以通过简单的提示工程来控制谈判的影响。
- en: To investigate whether the observed negative impact of negotiation on player
    rationality can be mitigated through simple prompt engineering, we design six
    distinct prompts. The first three prompts emphasize caution regarding the other
    player’s statements and encourage the agent to critically evaluate trustworthiness.
    These prompts are intended to foster a more skeptical and analytical mindset during
    negotiations. The remaining three prompts are formulated as commands, instructing
    the agent to make decisions independently without being influenced by the negotiation
    process. The objective of these prompts is to promote autonomy and self-reliance
    in the agent’s decision-making, potentially minimizing the negative effects of
    negotiation on rationality.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查是否可以通过简单的提示工程缓解谈判对玩家理性产生的负面影响，我们设计了六个不同的提示。前三个提示强调对其他玩家陈述的谨慎态度，并鼓励代理人批判性地评估信任度。这些提示旨在在谈判过程中培养更为怀疑和分析的思维方式。其余三个提示则作为指令，要求代理人在做决策时独立思考，而不受谈判过程的影响。这些提示的目标是促进代理人在决策时的自主性和自立性，从而可能减少谈判对理性产生的负面影响。
- en: In our experiment, we incorporate the six designed prompts at the end of the
    entire action-making prompt for each game. This placement ensures that the prompts
    serve as a final reminder or instruction to the LLM-based agents, not affected
    by the length of the negotiation texts.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将六个设计好的提示放在每个游戏的整个行动决策提示的末尾。这种位置确保了这些提示作为对基于LLM的代理人的最终提醒或指令，不会受到谈判文本长度的影响。
- en: '[PRE6]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-612
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-613
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-614
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-615
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-616
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Refer to caption](img/65b2263e9b3012a0deecec2628f171f0.png)'
  id: totrans-617
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65b2263e9b3012a0deecec2628f171f0.png)'
- en: 'Figure 9: The effect of the 6 engineered prompts on Prisoner’s Dilemma game
    with different rounds of negotiation: 1-round, 2-round, and 3-round for the three
    rows'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：六个设计的提示对囚徒困境博弈的影响，不同谈判轮数的效果：1轮、2轮和3轮对应三行
- en: '![Refer to caption](img/cfc7d274ae6fe60d282f273e2b056540.png)'
  id: totrans-619
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/cfc7d274ae6fe60d282f273e2b056540.png)'
- en: 'Figure 10: The effect of the 6 engineered prompts on Stag Hunt game with different
    rounds of negotiation: 1-round, 2-round, and 3-round for the three rows'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：六个设计的提示对公鹿狩猎博弈的影响，不同谈判轮数的效果：1轮、2轮和3轮对应三行
- en: 'By comparing the performance of LLM-based agents across these six prompts,
    we aim to determine whether prompt engineering can effectively control the impact
    of negotiation on player rationality and improve decision-making outcomes in various
    game-theoretic scenarios. Figure [10](https://arxiv.org/html/2411.05990v2#S6.F10
    "Figure 10 ‣ 6.5 How does prompt affect the influence of negotiation? ‣ 6 Detailed
    Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games") and Figure [10](https://arxiv.org/html/2411.05990v2#S6.F10 "Figure 10
    ‣ 6.5 How does prompt affect the influence of negotiation? ‣ 6 Detailed Observation
    on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games")
    illustrate the impact of the six designed prompts on the outcomes of Prisoner’s
    Dilemma and Stag Hunt games, respectively. In each figure, the six columns correspond
    to the specific prompts used, while the three rows represent the number of negotiation
    rounds between the two players before they decide on their actions.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较基于LLM的智能体在这六个提示下的表现，我们旨在确定提示工程是否能够有效地控制谈判对玩家理性行为的影响，并在各种博弈论场景中改善决策结果。图[10](https://arxiv.org/html/2411.05990v2#S6.F10
    "图 10 ‣ 6.5 提示如何影响谈判的作用？ ‣ 6 LLM理性行为的详细观察 ‣ 博弈论LLM：谈判博弈中的智能体工作流")和图[10](https://arxiv.org/html/2411.05990v2#S6.F10
    "图 10 ‣ 6.5 提示如何影响谈判的作用？ ‣ 6 LLM理性行为的详细观察 ‣ 博弈论LLM：谈判博弈中的智能体工作流")分别展示了六个设计的提示对囚徒困境和公鹿狩猎博弈结果的影响。在每个图中，六列对应所使用的具体提示，三行代表在玩家做出行动决定之前的谈判轮数。
- en: By analyzing these figures, we can assess the effectiveness of each prompt in
    influencing the players’ decision-making processes and evaluate whether prompt
    engineering can mitigate the effects of negotiation on player’s choices as the
    number of negotiation round increases.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析这些图表，我们可以评估每个提示在影响玩家决策过程中的有效性，并评估提示工程是否能够在谈判轮数增加时减轻谈判对玩家选择的影响。
- en: 'In observation, we can see the following situations:'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察中，我们可以看到以下情况：
- en: '1.'
  id: totrans-624
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Prompt 1, 2, 3, and 4 do not significantly impact the distribution of strategies
    chosen by the LLM-based agents in both Prisoner’s Dilemma and Stag Hunt games.
    Even when these prompts explicitly ask the agents to consider the trustworthiness
    of the other player, they do not lead to a more rational strategy selection.
  id: totrans-625
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示1、2、3和4对LLM智能体在囚徒困境和公鹿狩猎博弈中选择策略的分布没有显著影响。即使这些提示明确要求智能体考虑另一玩家的可信度，它们也未能促使智能体做出更为理性的策略选择。
- en: '2.'
  id: totrans-626
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The prompts have varying degrees of influence on the players’ decisions, with
    Prompt 5 and 6 exerting the most significant impact. In the Prisoner’s Dilemma,
    these prompts completely alter the distribution from a heavy focus on the dominated
    strategy (action 1, action 1) to the dominant strategy (action 2, action 2). In
    the Stag Hunt, Prompt 5 and 6 also change the distribution from the pareto optimal
    strategy (action 1, action 1) to a non-optimal strategy (action 2, action 2) or
    a mixture of strategies.
  id: totrans-627
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些提示对玩家决策的影响程度不同，提示5和6的影响最为显著。在囚徒困境中，这些提示完全改变了分布，从过于侧重支配策略（行动1，行动1）转变为更倾向于支配策略（行动2，行动2）。在公鹿狩猎博弈中，提示5和6同样将分布从帕累托最优策略（行动1，行动1）转变为非最优策略（行动2，行动2）或策略的混合。
- en: '3.'
  id: totrans-628
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: The impact of the prompts can be gradually diminished as the number of negotiation
    rounds increases. For instance, in the Prisoner’s Dilemma, the distribution shifts
    more towards the dominated strategy as the number of negotiation rounds grows.
    Similarly, in the Stag Hunt, the distribution moves more towards the pareto optimal
    strategy as the number of negotiation rounds increases.
  id: totrans-629
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随着谈判轮数的增加，提示的影响逐渐减弱。例如，在囚徒困境中，随着谈判轮数的增加，分布更加倾向于支配策略。类似地，在公鹿狩猎博弈中，随着谈判轮数的增加，分布则更加倾向于帕累托最优策略。
- en: Based on the three observations, we can conclude that the use of engineered
    prompts does not genuinely enhance the rationality of the LLM-based agents in
    the Prisoner’s Dilemma and Stag Hunt games. The impact of the prompts on the agents’
    decision-making process follows a similar trend in both games, and their influence
    is diminished as the number of negotiation rounds increases.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这三项观察结果，我们可以得出结论：使用工程化提示词并没有真正增强基于大语言模型（LLM）的智能体在囚徒困境和公鹿猎捕博弈中的理性。这些提示对智能体决策过程的影响在两个游戏中呈现相似的趋势，并且随着谈判轮次的增加，其影响逐渐减弱。
- en: '![Refer to caption](img/3fccd3198f33eb91f249e5fc2530738d.png)'
  id: totrans-631
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/3fccd3198f33eb91f249e5fc2530738d.png)'
- en: 'Figure 11: Will the fact that who starts the negotiation affect the result?'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：谁先开始谈判会影响结果吗？
- en: 6.6 How does the order of negotiation message affect the action?
  id: totrans-633
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6 谈判信息的顺序如何影响行动？
- en: It is anticipated that the order in which players initiate negotiations will
    not influence the final actions taken, particularly in the Battle of the Sexes
    game. In an ideal scenario, neither the first nor the last player should have
    a distinct advantage in persuading the other player to act in their favor. To
    ensure that this is indeed the case, we conduct experiments in the Battle of the
    Sexes game with varying negotiation rounds (1, 2, and 3) and alter the player
    who initiates the negotiation, with either player 1 or player 2 starting the negotiation
    process. This experimental design allows us to investigate the potential impact
    of negotiation order on the outcomes of the game and assess the fairness of the
    negotiation process between the two players.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计，玩家发起谈判的顺序不会影响最终采取的行动，特别是在《性别之战》游戏中。在理想情况下，无论是第一个还是最后一个发起谈判的玩家，都不应在说服对方采取有利于自己的行动时具有明显的优势。为了确保这一点的确成立，我们在《性别之战》游戏中进行实验，设置不同的谈判轮次（1、2和3），并改变发起谈判的玩家，由玩家1或玩家2开始谈判过程。这个实验设计使我们能够调查谈判顺序对游戏结果的潜在影响，并评估两位玩家之间谈判过程的公平性。
- en: 'Based on the observation from Figure[11](https://arxiv.org/html/2411.05990v2#S6.F11
    "Figure 11 ‣ 6.5 How does prompt affect the influence of negotiation? ‣ 6 Detailed
    Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation
    Games"), it is evident that there is no significant change in strategy when the
    player initiating the negotiation is varied. This observation suggests that the
    order in which players commence negotiation does not have a substantial impact
    on the final actions taken in the Battle of the Sexes game.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图[11](https://arxiv.org/html/2411.05990v2#S6.F11 "图11 ‣ 6.5 提示如何影响谈判的影响？ ‣
    6 LLM理性的详细观察 ‣ 博弈论中的LLM：谈判游戏的智能体工作流程")中的观察结果，很明显，当改变发起谈判的玩家时，策略并没有发生显著变化。这个观察结果表明，玩家开始谈判的顺序对《性别之战》游戏中的最终行动没有实质性的影响。
- en: 6.7 Irrationality Compared with Humans
  id: totrans-636
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7 与人类的非理性比较
- en: 'The previous observations suggest that Large Language Models (LLMs) lack robust
    rationality across various scenarios. However, it is well-established that humans,
    too, do not always behave rationally [dawes1988anomalies](https://arxiv.org/html/2411.05990v2#bib.bib68)
    ; [sally1995conversation](https://arxiv.org/html/2411.05990v2#bib.bib69) . This
    raises an intriguing question: do the irrational tendencies of LLMs mirror those
    of humans?'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的观察结果表明，大型语言模型（LLM）在不同场景下缺乏强有力的理性。然而，已有研究表明，人类也并不总是理性地行事[dawes1988anomalies](https://arxiv.org/html/2411.05990v2#bib.bib68)；[sally1995conversation](https://arxiv.org/html/2411.05990v2#bib.bib69)。这引发了一个有趣的问题：LLM的非理性倾向是否与人类相似？
- en: 'To investigate this, we adopt a game setting from a televsion game show called
    Golden Balls where two contestants play a variant on the classic Prisoner’s Dilemma:
    In this game, two players independently decide to either “split” or “steal” the
    jackpot. If both choose to split, they share the jackpot equally. If one player
    chooses to split and the other steals, the stealer takes the entire jackpot. If
    both players steal, neither receives any money. Human performance of the game
    is collected by [van2012split](https://arxiv.org/html/2411.05990v2#bib.bib70)
    , where players chose to cooperate (i.e., split the jackpot) 53% of the time on
    average, a figure consistent with earlier laboratory studies [dawes1988anomalies](https://arxiv.org/html/2411.05990v2#bib.bib68)
    ; [sally1995conversation](https://arxiv.org/html/2411.05990v2#bib.bib69) . The
    decision to cooperate was sensitive to the size of the jackpot, with cooperation
    rates decreasing as the stakes increased.'
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查这一点，我们采用了来自电视游戏节目《黄金球》的游戏设定，其中两名参赛者玩经典囚徒困境的变体：在这个游戏中，两名玩家独立决定是“分配”还是“偷取”大奖池。如果两人选择分配，他们将平分大奖池。如果一方选择分配而另一方选择偷取，则偷取者获得整个大奖池。如果两人都选择偷取，则两人都没有得到任何钱。人类的游戏表现来自
    [van2012split](https://arxiv.org/html/2411.05990v2#bib.bib70)，在该研究中，玩家平均53%的时间选择合作（即分配大奖池），这一数据与早期的实验室研究一致
    [dawes1988anomalies](https://arxiv.org/html/2411.05990v2#bib.bib68)；[sally1995conversation](https://arxiv.org/html/2411.05990v2#bib.bib69)。合作决策对大奖池的大小敏感，随着赌注的增加，合作率下降。
- en: We configured the LLMs to play this game, using the same jackpot sizes as in
    the human data. Each jackpot size was tested 20 times, resulting in 40 decision-making
    instances for each size. We then calculated the cooperation rates for the LLMs
    for different jackpot sizes.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 我们配置LLM来进行此游戏，使用与人类数据中相同的大奖池大小。每种大奖池大小进行了20次测试，得出每个大小的40个决策实例。然后，我们计算了不同大奖池大小下LLM的合作率。
- en: '![Refer to caption](img/a92d8f20bc38053b220de0266dc69232.png)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a92d8f20bc38053b220de0266dc69232.png)'
- en: 'Figure 12: Rationality analysis on Golden Balls game'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：黄金球游戏的理性分析
- en: 'The results of this comparative analysis are presented in Figure [12](https://arxiv.org/html/2411.05990v2#S6.F12
    "Figure 12 ‣ 6.7 Irrationality Compared with Humans ‣ 6 Detailed Observation on
    LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games").
    Human cooperation rates were indeed sensitive to the size of the jackpot, with
    higher cooperation rates for smaller jackpots. This trend is also observed in
    [post2008deal](https://arxiv.org/html/2411.05990v2#bib.bib71) . In contrast, the
    LLMs’ decision to cooperate was largely insensitive to the jackpot size, regardless
    of the “personality” prompt used: rational, witty, pragmatic, helpful, friendly,
    compassionate. Furthermore, the LLMs’ cooperation rates were generally lower than
    those of the humans.'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '本比较分析的结果呈现在图 [12](https://arxiv.org/html/2411.05990v2#S6.F12 "Figure 12 ‣ 6.7
    Irrationality Compared with Humans ‣ 6 Detailed Observation on LLM’s Rationality
    ‣ Game-theoretic LLM: Agent Workflow for Negotiation Games") 中。人类的合作率确实对大奖池的大小敏感，较小的大奖池对应较高的合作率。这一趋势也出现在
    [post2008deal](https://arxiv.org/html/2411.05990v2#bib.bib71) 中。相比之下，LLM是否选择合作在很大程度上不受大奖池大小的影响，无论使用何种“人格”提示：理性、机智、务实、乐于助人、友好、富有同情心。此外，LLM的合作率通常低于人类的合作率。'
- en: While this analysis provides a preliminary comparison of irrational tendencies
    in LLMs and humans, it is by no means exhaustive. Further research is needed to
    establish a more comprehensive understanding of the relationship between human
    and LLM irrationality.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本分析提供了对大规模语言模型（LLM）与人类非理性倾向的初步比较，但它绝非详尽无遗。需要进一步的研究，以建立人类与LLM非理性之间关系的更全面理解。
- en: '<svg class="ltx_picture" height="246.03" id="S6.SS7.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,246.03) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 227.83)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for raw-LLM v. workflow-LLM
    and workflow-LLM v. raw-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="196.33" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Lack of Robustness to Numerical
    Variations: Empirical results indicate that LLMs either do not exhibit rationality
    or their rationality is highly sensitive to numerical changes. When the payoff
    matrix undergoes perturbations that leave the Nash Equilibrium unchanged, the
    performance of LLMs varies significantly. • Impact of Negotiation on Rationality:
    Consistent with observations made in Section [6](https://arxiv.org/html/2411.05990v2#S6
    "6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games"), we find that rational choices are undermined by the introduction
    of negotiation, even in the absence of grounds or guarantees for trust. • Effect
    of Prompt Variation on Negotiation Impact: The wording of the prompt can mitigate
    the influence of negotiation on rationality; however, this mitigating effect diminishes
    as the number of negotiation rounds increases. • Order of Negotiation Initiation:
    The sequence in which players initiate negotiation does not significantly affect
    the game’s outcome, even in coordination games such as the Battle of the Sexes.
    • Comparison of Irrationality Between LLMs and Humans: Although LLMs lack robust
    rationality across various scenarios, the nature of their irrationality differs
    from that observed in human decision-making.</foreignobject></g></g></svg>'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="246.03" id="S6.SS7.1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,246.03) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 227.83)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summary for raw-LLM v. workflow-LLM
    and workflow-LLM v. raw-LLM</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="196.33" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">• Lack of Robustness to Numerical
    Variations: Empirical results indicate that LLMs either do not exhibit rationality
    or their rationality is highly sensitive to numerical changes. When the payoff
    matrix undergoes perturbations that leave the Nash Equilibrium unchanged, the
    performance of LLMs varies significantly. • Impact of Negotiation on Rationality:
    Consistent with observations made in Section [6](https://arxiv.org/html/2411.05990v2#S6
    "6 Detailed Observation on LLM’s Rationality ‣ Game-theoretic LLM: Agent Workflow
    for Negotiation Games"), we find that rational choices are undermined by the introduction
    of negotiation, even in the absence of grounds or guarantees for trust. • Effect
    of Prompt Variation on Negotiation Impact: The wording of the prompt can mitigate
    the influence of negotiation on rationality; however, this mitigating effect diminishes
    as the number of negotiation rounds increases. • Order of Negotiation Initiation:
    The sequence in which players initiate negotiation does not significantly affect
    the game’s outcome, even in coordination games such as the Battle of the Sexes.
    • Comparison of Irrationality Between LLMs and Humans: Although LLMs lack robust
    rationality across various scenarios, the nature of their irrationality differs
    from that observed in human decision-making.</foreignobject></g></g></svg>'
- en: 7 Conclusion
  id: totrans-645
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This study conducted a comprehensive game-theoretic analysis to evaluate the
    rationality and effectiveness of adopting a negotiation workflow within Large
    Language Models (LLMs) across a spectrum of classic strategic scenarios. By modeling
    interactions through well-established complete-information games, including the
    Prisoner’s Dilemma, Stag Hunt, Battle of the Sexes, Wait-Go Game, Duopolistic
    Competition, Escalation Game, Monopoly Game, Draco and Harry Game, we assessed
    how different LLMs, specifically Claude-3.5 Sonnet, GPT-4o, and Claude-3 Opus,
    navigate the balance between cooperation and competition.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究进行了全面的博弈论分析，评估了在各种经典战略场景中，采用谈判工作流在大型语言模型（LLMs）中的理性与有效性。通过使用完备信息博弈（包括囚徒困境、鹿猎、性别之战、等待-行动游戏、双头垄断竞争、升级博弈、垄断游戏、德拉科与哈利游戏）来建模互动，我们评估了不同LLM（具体包括Claude-3.5
    Sonnet、GPT-4o和Claude-3 Opus）如何在合作与竞争之间找到平衡。
- en: Expanding our investigation to more realistic settings, we explored the Deal-No-Deal
    Game, which incorporates incomplete-information, to assess whether LLMs can efficiently
    allocate resources and negotiate agreements under conditions of uncertainty. In
    this context, we designed a workflow based on Bayesian updates to achieve pareto
    optimal and envy free allocations, thereby enhancing the negotiation process.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调查扩展到更现实的环境，探索了包含不完全信息的《交易与非交易游戏》，以评估LLM在不确定条件下能否有效分配资源并达成协议。在此背景下，我们设计了一个基于贝叶斯更新的工作流，以实现帕累托最优且无嫉妒的分配，从而提升了谈判过程。
- en: Our findings indicate that the adoption of the workflow generally promotes pareto
    optimal outcomes, wherein both agents achieve higher collective payoffs compared
    to non-cooperative strategies. For instance, in scenarios like the Stag Hunt and
    Battle of the Sexes, the workflow facilitated effective negotiation and coordination,
    enabling LLMs to reach mutually beneficial equilibria. Similarly, in the Deal-No-Deal
    Game, the structured workflow enhanced decision-making processes, leading to more
    efficient resource allocations.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果表明，采用该工作流通常能促进帕累托最优结果，其中两个代理人相比非合作策略能够获得更高的集体收益。例如，在鹿猎和性别之战这样的场景中，工作流促进了有效的谈判与协调，使得LLM能够达成互利的均衡。同样，在《交易与非交易游戏》中，结构化的工作流增强了决策过程，导致了更高效的资源分配。
- en: 8 Future Directions
  id: totrans-649
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 未来的研究方向
- en: 'This study opens several promising avenues for future research:'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究为未来的研究开辟了几条有前景的方向：
- en: 'Exploration of Workflow Vulnerabilities and Defense Mechanisms:'
  id: totrans-651
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 工作流漏洞与防御机制的探索：
- en: Investigating how negotiation workflows can be exploited through methods such
    as deception is crucial, particularly in contexts like the Deal-No-Deal Game.
    Understanding the potential vulnerabilities within these workflows will enable
    the development of robust defense strategies to mitigate exploitation risks, thereby
    enhancing the reliability and security of negotiation protocols.
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: 调查如何通过欺骗等手段利用谈判工作流是至关重要的，特别是在《交易与非交易游戏》这样的背景下。理解这些工作流中潜在的漏洞将有助于开发强有力的防御策略，以减轻被利用的风险，从而增强谈判协议的可靠性与安全性。
- en: 'Strategizing in Multi-Stage Games:'
  id: totrans-653
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在多阶段博弈中的策略制定：
- en: Extending the analysis to multi-stage games introduces additional complexity,
    as the Nash Equilibrium may differ significantly from that in single-stage games.
    Future work should focus on how LLMs can effectively strategize over multiple
    stages, accounting for the dynamic evolution of the game state and the opponent’s
    actions. This includes developing algorithms that can anticipate future moves
    and adjust strategies accordingly to maintain rationality and optimize outcomes.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 将分析扩展到多阶段博弈引入了额外的复杂性，因为纳什均衡可能与单阶段博弈中显著不同。未来的工作应聚焦于LLM如何在多个阶段有效制定策略，考虑到博弈状态和对手行为的动态演化。这包括开发能够预测未来举措并相应调整策略的算法，以保持理性并优化结果。
- en: 'Development of Meta-Strategy:'
  id: totrans-655
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 元策略的发展：
- en: LLMs should be equipped with the capability to determine when to employ a particular
    workflow and when to adapt or forgo it. This necessitates the creation of a meta-strategy
    or meta-workflow that guides the selection of appropriate negotiation strategies
    based on the specific context, the agent’s own abilities, the stage of the game,
    and the behavior of the opponent. Implementing such a meta-strategy would enhance
    the adaptability and effectiveness of LLMs across diverse negotiation scenarios.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 应具备判断何时使用特定工作流程、何时进行适应或放弃的能力。这需要创建一个元策略或元工作流程，根据特定情境、代理的能力、博弈阶段和对手的行为来指导选择适当的谈判策略。实施这样的元策略将增强
    LLM 在多种谈判场景中的适应性和有效性。
- en: 'Alignment with Agent Interests and Stance Adoption:'
  id: totrans-657
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与代理利益及立场采纳的对齐：
- en: Currently, LLMs are generally aligned to be helpful and honest, lacking a personalized
    stance or specific interests. To function as proficient negotiation agents, it
    is imperative for LLMs to learn and adopt stances that reflect the interests they
    represent. This involves training LLMs to understand and advocate for particular
    objectives, thereby balancing their general alignment with the capacity to pursue
    defined goals within negotiations. Developing methods to instill a clear understanding
    of their own interests will enhance the LLMs’ ability to engage in strategic decision-making
    and achieve desired outcomes.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，LLM 通常对齐为有帮助且诚实的，缺乏个性化的立场或特定利益。为了作为高效的谈判代理，LLM 必须学习并采纳反映其代表利益的立场。这涉及到训练 LLM
    理解并倡导特定目标，从而在谈判中平衡其普遍对齐和追求明确定义目标的能力。开发方法来灌输对自身利益的清晰理解，将增强 LLM 在战略决策和实现预期结果方面的能力。
- en: These future directions aim to refine the strategic reasoning and negotiation
    capabilities of LLMs, ensuring they can operate effectively and rationally in
    complex, real-world scenarios. By addressing these areas, we can advance the development
    of LLMs as robust agents capable of navigating intricate strategic environments
    while safeguarding against potential vulnerabilities.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 这些未来的发展方向旨在细化 LLM 的战略推理和谈判能力，确保它们能够在复杂的现实世界场景中有效且理性地运作。通过解决这些问题，我们可以推进 LLM 作为强大的代理的发展，使其能够在复杂的战略环境中进行导航，同时防范潜在的脆弱性。
- en: References
  id: totrans-660
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Xiang Zhang and Dujian Ding. Supervised chain of thought. arXiv preprint
    arXiv:2410.14198, 2024.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 张向和丁杜建. 受监督的思维链. arXiv 预印本 arXiv:2410.14198, 2024.'
- en: '[2] Dujian Ding, Ankur Mallick, Chi Wang, Robert Sim, Subhabrata Mukherjee,
    Victor Ruhle, Laks VS Lakshmanan, and Ahmed Hassan Awadallah. Hybrid llm: Cost-efficient
    and quality-aware query routing. arXiv preprint arXiv:2404.14618, 2024.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 丁杜建、安库尔·马利克、王驰、罗伯特·西姆、苏巴布拉特·穆克吉、维克托·鲁赫尔、拉克什·VS·拉克什曼南和艾哈迈德·哈桑·阿瓦达拉. 混合 LLM：节省成本且注重质量的查询路由.
    arXiv 预印本 arXiv:2404.14618, 2024.'
- en: '[3] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Jane
    Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, Christos Faloutsos,
    et al. Large language models (llms) on tabular data: Prediction, generation, and
    understanding-a survey. 2024.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 方旭、许伟杰、谭安婷、张佳妮、胡子晴、祁彦君、斯科特·尼克利奇、迭戈·索科林斯基、斯里尼瓦森·森加梅杜、克里斯托斯·法卢佐斯 等. 大型语言模型（LLMs）在表格数据上的应用：预测、生成与理解——综述.
    2024.'
- en: '[4] Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and Chaoning Zhang. A complete
    survey on llm-based ai chatbots. arXiv preprint arXiv:2406.16937, 2024.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 苏米特·库马尔·达姆、洪仲善、乔玉和张超宁. 基于 LLM 的 AI 聊天机器人综述. arXiv 预印本 arXiv:2406.16937,
    2024.'
- en: '[5] Xin Luna Dong, Seungwhan Moon, Yifan Ethan Xu, Kshitiz Malik, and Zhou
    Yu. Towards next-generation intelligent assistants leveraging llm techniques.
    In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data
    Mining, pages 5792–5793, 2023.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 冯露娜·董、文胜焕、徐伊凡·以太、马尔基特·克什提兹 和 周宇. 利用 LLM 技术推动下一代智能助手. 《第29届 ACM SIGKDD 知识发现与数据挖掘大会论文集》，第5792–5793页，2023.'
- en: '[6] Weixin Liang, Zachary Izzo, Yaohui Zhang, Haley Lepp, Hancheng Cao, Xuandong
    Zhao, Lingjiao Chen, Haotian Ye, Sheng Liu, Zhi Huang, et al. Monitoring ai-modified
    content at scale: A case study on the impact of chatgpt on ai conference peer
    reviews. arXiv preprint arXiv:2403.07183, 2024.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 梁伟鑫、扎卡里·伊佐、张耀辉、海莉·勒普、曹瀚成、赵轩东、陈灵娇、叶浩天、刘生、黄志 等. 大规模监控 AI 修改内容：以 ChatGPT 对
    AI 会议同行评审影响的案例研究为例. arXiv 预印本 arXiv:2403.07183, 2024.'
- en: '[7] Yijia Shao, Yucheng Jiang, Theodore A Kanell, Peter Xu, Omar Khattab, and
    Monica S Lam. Assisting in writing wikipedia-like articles from scratch with large
    language models. arXiv preprint arXiv:2402.14207, 2024.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 邵一佳, 江宇程, 西奥多·A·卡内尔, 许彼得, 奥马尔·哈塔布, 莫妮卡·S·拉姆. 协助从零开始撰写类似维基百科的文章，利用大语言模型。arXiv预印本
    arXiv:2402.14207, 2024。'
- en: '[8] Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia Vélez, Qingyun
    Wu, Huazheng Wang, Thomas L Griffiths, and Mengdi Wang. Embodied llm agents learn
    to cooperate in organized teams. arXiv preprint arXiv:2403.12482, 2024.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 郭旭东, 黄凯轩, 刘家乐, 范文辉, 纳塔莉亚·维莱兹, 吴庆云, 王华政, 托马斯·L·格里菲斯, 王蒙迪. 具身大语言模型代理在组织化团队中学会合作。arXiv预印本
    arXiv:2403.12482, 2024。'
- en: '[9] Saaket Agashe, Yue Fan, and Xin Eric Wang. Evaluating multi-agent coordination
    abilities in large language models. arXiv preprint arXiv:2310.03903, 2023.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 萨凯特·阿贾什, 樊月, 王欣·埃里克. 评估大语言模型在多代理协调中的能力。arXiv预印本 arXiv:2310.03903, 2023。'
- en: '[10] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large
    language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 习志恒, 陈文翔, 郭欣, 何伟, 丁怡文, 洪博阳, 张铭, 王俊哲, 金森杰, 周恩宇, 等. 大语言模型基础的代理崛起与潜力：一项调查。arXiv预印本
    arXiv:2309.07864, 2023。'
- en: '[11] Federico Bianchi, Patrick John Chia, Mert Yuksekgonul, Jacopo Tagliabue,
    Dan Jurafsky, and James Zou. How well can llms negotiate? negotiationarena platform
    and analysis. arXiv preprint arXiv:2402.05863, 2024.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 费德里科·比安基, 帕特里克·约翰·基亚, 梅尔特·尤克塞克戈努尔, 雅科波·塔利亚布, 丹·尤拉夫斯基, 詹姆斯·邹. 大语言模型的谈判能力如何？谈判竞技场平台与分析。arXiv预印本
    arXiv:2402.05863, 2024。'
- en: '[12] John J Horton. Large language models as simulated economic agents: What
    can we learn from homo silicus? Technical report, National Bureau of Economic
    Research, 2023.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 约翰·J·霍顿. 大语言模型作为模拟经济代理：我们能从“硅人”中学到什么？技术报告，美国国家经济研究局, 2023。'
- en: '[13] Nian Li, Chen Gao, Mingyu Li, Yong Li, and Qingmin Liao. Econagent: large
    language model-empowered agents for simulating macroeconomic activities. In Proceedings
    of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), pages 15523–15536, 2024.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 李年, 高晨, 李名宇, 李勇, 廖庆民. Econagent：基于大语言模型的代理模拟宏观经济活动。第62届计算语言学协会年会论文集（第1卷：长篇论文），页码15523–15536,
    2024。'
- en: '[14] Pei Chen, Boran Han, and Shuai Zhang. Comm: Collaborative multi-agent,
    multi-reasoning-path prompting for complex problem solving. arXiv preprint arXiv:2404.17729,
    2024.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 陈佩, 韩博然, 张帅. Comm：用于复杂问题解决的协作多代理、多推理路径提示。arXiv预印本 arXiv:2404.17729, 2024。'
- en: '[15] Yuan Li, Yixuan Zhang, and Lichao Sun. Metaagents: Simulating interactions
    of human behaviors for llm-based task-oriented coordination via collaborative
    generative agents. arXiv preprint arXiv:2310.06500, 2023.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 李元, 张艺璇, 孙立超. 元代理：通过协作生成代理模拟基于大语言模型的人类行为互动，以实现任务导向协调。arXiv预印本 arXiv:2310.06500,
    2023。'
- en: '[16] Yan Leng and Yuan Yuan. Do llm agents exhibit social behavior? arXiv preprint
    arXiv:2312.15198, 2023.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 邓言, 袁元. 大语言模型代理展现社交行为吗？arXiv预印本 arXiv:2312.15198, 2023。'
- en: '[17] Elizabeth C Stade, Shannon Wiltsey Stirman, Lyle H Ungar, Cody L Boland,
    H Andrew Schwartz, David B Yaden, João Sedoc, Robert J DeRubeis, Robb Willer,
    and Johannes C Eichstaedt. Large language models could change the future of behavioral
    healthcare: a proposal for responsible development and evaluation. npj Mental
    Health Research, 3(1):12, 2024.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 伊丽莎白·C·斯塔德, 香农·威尔特西·斯特曼, 莱尔·H·昂格, 科迪·L·博兰, H·安德鲁·施瓦茨, 大卫·B·耶登, 若昂·塞多克,
    罗伯特·J·德鲁贝斯, 罗布·威勒, 约翰内斯·C·艾希施泰特. 大语言模型可能改变行为健康护理的未来：一项负责任的开发与评估提案。npj心理健康研究, 3(1):12,
    2024。'
- en: '[18] Zengqing Wu, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto
    Onizuka, Shaojie Tang, Run Peng, and Chuan Xiao. Shall we talk: Exploring spontaneous
    collaborations of competing llm agents. arXiv preprint arXiv:2402.12327, 2024.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 吴增清, 郑书源, 刘倩莹, 韩旭, 权英赫, 小野诚, 唐少杰, 彭润, 许川. 我们可以谈谈吗：探索竞争性大语言模型代理的自发合作。arXiv预印本
    arXiv:2402.12327, 2024。'
- en: '[19] I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni, and Carlos T Calafate.
    Emergent cooperation and strategy adaptation in multi-agent systems: An extended
    coevolutionary theory with llms. Electronics, 12(12):2722, 2023.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] I de Zarzà, J de Curtò, 盖玛·罗伊格, 皮特罗·曼佐尼, 卡洛斯·T·卡拉法特. 多代理系统中的新兴合作与策略适应：基于大语言模型的扩展共同进化理论。电子学报,
    12(12):2722, 2023。'
- en: '[20] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao,
    Ee-Peng Lim, Hui Xiong, and Hao Wang. Llm-based agent society investigation: Collaboration
    and confrontation in avalon gameplay. arXiv preprint arXiv:2310.14985, 2023.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao,
    Ee-Peng Lim, Hui Xiong, 和 Hao Wang。基于LLM的智能体社会调查：Avalon游戏中的协作与对抗。arXiv 预印本 arXiv:2310.14985，2023年。'
- en: '[21] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe
    Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al. Evaluation of openai
    o1: Opportunities and challenges of agi. arXiv preprint arXiv:2409.18486, 2024.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe
    Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu 等。《评估OpenAI O1：通用人工智能的机会与挑战》。arXiv
    预印本 arXiv:2409.18486，2024年。'
- en: '[22] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, and Dhruv Batra.
    Deal or no deal? end-to-end learning for negotiation dialogues. arXiv preprint
    arXiv:1706.05125, 2017.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Mike Lewis, Denis Yarats, Yann N Dauphin, Devi Parikh, 和 Dhruv Batra。《交易还是不交易？》端到端学习用于谈判对话。arXiv
    预印本 arXiv:1706.05125，2017年。'
- en: '[23] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi,
    Luke Marris, Georgios Piliouras, and Karl Tuyls. States as strings as strategies:
    Steering language models with game-theoretic solvers. arXiv preprint arXiv:2402.01704,
    2024.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Ian Gemp, Yoram Bachrach, Marc Lanctot, Roma Patel, Vibhavari Dasagi,
    Luke Marris, Georgios Piliouras, 和 Karl Tuyls。《状态即字符串作为策略：使用博弈论求解器引导语言模型》。arXiv
    预印本 arXiv:2402.01704，2024年。'
- en: '[24] Karthik Sreedhar and Lydia Chilton. Simulating human strategic behavior:
    Comparing single and multi-agent llms. arXiv preprint arXiv:2402.08189, 2024.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Karthik Sreedhar 和 Lydia Chilton。模拟人类战略行为：比较单一与多智能体LLMs。arXiv 预印本 arXiv:2402.08189，2024年。'
- en: '[25] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    and Eric Schulz. Playing repeated games with large language models. arXiv preprint
    arXiv:2305.16867, 2023.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge,
    和 Eric Schulz。与大型语言模型玩重复博弈。arXiv 预印本 arXiv:2305.16867，2023年。'
- en: '[26] Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models
    serve as rational players in game theory? a systematic analysis. In Proceedings
    of the AAAI Conference on Artificial Intelligence, pages 17960–17967, 2024.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Caoyun Fan, Jindou Chen, Yaohui Jin, 和 Hao He。大型语言模型能否作为博弈论中的理性玩家？系统分析。发表于《人工智能AAAI会议论文集》，页码17960–17967，2024年。'
- en: '[27] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang,
    Tao Ge, and Furu Wei. Alympics: Language agents meet game theory. arXiv preprint
    arXiv:2311.03220, 2023.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang,
    Tao Ge, 和 Furu Wei。Alympics：语言智能体遇见博弈论。arXiv 预印本 arXiv:2311.03220，2023年。'
- en: '[28] Fulin Guo. Gpt in game theory experiments, 2023.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Fulin Guo。《GPT在博弈论实验中的应用》，2023年。'
- en: '[29] Drew Fudenberg and Jean Tirole. Game theory. MIT press, 1991.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Drew Fudenberg 和 Jean Tirole。《博弈论》。MIT出版社，1991年。'
- en: '[30] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
    Michael Lewis, and Katia Sycara. Theory of mind for multi-agent collaboration
    via large language models. arXiv preprint arXiv:2310.10701, 2023.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes,
    Michael Lewis, 和 Katia Sycara。通过大型语言模型进行多智能体协作的心智理论。arXiv 预印本 arXiv:2310.10701，2023年。'
- en: '[31] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang
    Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu. How far are we
    on the decision-making of llms? evaluating llms’ gaming ability in multi-agent
    environments. arXiv preprint arXiv:2403.11807, 2024.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang
    Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, 和 Michael R Lyu。我们在LLMs决策能力上走多远了？评估LLMs在多智能体环境中的博弈能力。arXiv
    预印本 arXiv:2403.11807，2024年。'
- en: '[32] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao
    Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. Gtbench:
    Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.
    arXiv preprint arXiv:2402.12348, 2024.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao
    Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, 和 Kaidi Xu。Gtbench：通过博弈论评估揭示LLMs的战略推理局限性。arXiv
    预印本 arXiv:2402.12348，2024年。'
- en: '[33] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang,
    Andrew Zhao, Chaofei Wang, Shiji Song, and Gao Huang. Avalon’s game of thoughts:
    Battle against deception through recursive contemplation. arXiv preprint arXiv:2310.01320,
    2023.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang,
    Andrew Zhao, Chaofei Wang, Shiji Song, 和 Gao Huang。Avalon的思维游戏：通过递归反思对抗欺骗。arXiv
    预印本 arXiv:2310.01320，2023年。'
- en: '[34] Peter S Park, Simon Goldstein, Aidan O’Gara, Michael Chen, and Dan Hendrycks.
    Ai deception: A survey of examples, risks, and potential solutions. Patterns,
    5(5), 2024.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Peter S Park, Simon Goldstein, Aidan O’Gara, Michael Chen, 和 Dan Hendrycks.
    AI 欺骗：实例、风险与潜在解决方案调查。《模式》，5(5)，2024年。'
- en: '[35] Nunzio Lorè and Babak Heydari. Strategic behavior of large language models:
    Game structure vs. contextual framing. arXiv preprint arXiv:2309.05898, 2023.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Nunzio Lorè 和 Babak Heydari. 大型语言模型的战略行为：博弈结构与情境框架。arXiv 预印本 arXiv:2309.05898,
    2023.'
- en: '[36] Julian Coda-Forno, Marcel Binz, Jane X Wang, and Eric Schulz. Cogbench:
    a large language model walks into a psychology lab. arXiv preprint arXiv:2402.18225,
    2024.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Julian Coda-Forno, Marcel Binz, Jane X Wang, 和 Eric Schulz. Cogbench:
    大型语言模型走进心理学实验室。arXiv 预印本 arXiv:2402.18225, 2024.'
- en: '[37] Hongyi Guo, Zhihan Liu, Yufeng Zhang, and Zhaoran Wang. Can large language
    models play games? a case study of a self-play approach. arXiv preprint arXiv:2403.05632,
    2024.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Hongyi Guo, Zhihan Liu, Yufeng Zhang, 和 Zhaoran Wang. 大型语言模型能玩吗？自我对弈方法的案例研究。arXiv
    预印本 arXiv:2403.05632, 2024.'
- en: '[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. Advances in neural information processing systems, 35:24824–24837,
    2022.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou 等人. 连锁思维提示激发大型语言模型的推理能力。神经信息处理系统进展，35:24824-24837, 2022。'
- en: '[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models, 2023. URL https://arxiv. org/pdf/2305.10601\. pdf,
    2023.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 思维树：通过大型语言模型进行深思熟虑的问题解决，2023。网址 [https://arxiv.org/pdf/2305.10601.pdf](https://arxiv.org/pdf/2305.10601.pdf)，2023.'
- en: '[40] Ollie Liu, Deqing Fu, Dani Yogatama, and Willie Neiswanger. Dellma: A
    framework for decision making under uncertainty with large language models. arXiv
    preprint arXiv:2402.02392, 2024.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Ollie Liu, Deqing Fu, Dani Yogatama, 和 Willie Neiswanger. Dellma: 一个基于大型语言模型的不确定性决策框架。arXiv
    预印本 arXiv:2402.02392, 2024.'
- en: '[41] Sihao Hu, Tiansheng Huang, and Ling Liu. Pok$\backslash$’ellmon: A human-parity
    agent for pok$\backslash$’emon battles with large language models. arXiv preprint
    arXiv:2402.01118, 2024.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Sihao Hu, Tiansheng Huang, 和 Ling Liu. Pok$\backslash$’ellmon: 一个具有人类水平的语言模型对战代理。arXiv
    预印本 arXiv:2402.01118, 2024.'
- en: '[42] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang,
    and Rui Wang. Measuring bargaining abilities of llms: A benchmark and a buyer-enhancement
    method. arXiv preprint arXiv:2402.15813, 2024.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Tian Xia, Zhiwei He, Tong Ren, Yibo Miao, Zhuosheng Zhang, Yang Yang,
    和 Rui Wang. 测量 LLM 的谈判能力：一个基准测试与买家增强方法。arXiv 预印本 arXiv:2402.15813, 2024.'
- en: '[43] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu,
    Zelong Li, and Yongfeng Zhang. OpenAGI: When LLM meets domain experts. In Thirty-seventh
    Conference on Neural Information Processing Systems, 2023.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu,
    Zelong Li, 和 Yongfeng Zhang. OpenAGI: 当 LLM 遇到领域专家。在第37届神经信息处理系统会议上，2023年。'
- en: '[44] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen
    llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,
    2023.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang
    Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, 和 Chi Wang. Autogen: 通过多代理对话框架支持下一代 LLM
    应用。arXiv 预印本 arXiv:2308.08155, 2023.'
- en: '[45] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin,
    and Bernard Ghanem. Camel: Communicative agents for "mind" exploration of large
    scale language model society, 2023.'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin,
    和 Bernard Ghanem. Camel: 用于“大规模语言模型社会”心智探索的交流代理，2023年。'
- en: '[46] Oguzhan Topsakal and Tahir Cetin Akinci. Creating large language model
    applications utilizing langchain: A primer on developing llm apps fast. In International
    Conference on Applied Engineering and Natural Sciences, volume 1, pages 1050–1056,
    2023.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Oguzhan Topsakal 和 Tahir Cetin Akinci. 利用 Langchain 创建大型语言模型应用：快速开发 LLM
    应用的入门指南。在国际应用工程与自然科学会议中，第1卷，第1050-1056页，2023年。'
- en: '[47] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian,
    Yanghua Xiao, and Yu Su. Travelplanner: A benchmark for real-world planning with
    language agents. arXiv preprint arXiv:2402.01622, 2024.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Jian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian,
    Yanghua Xiao, 和 Yu Su. Travelplanner: 用语言代理进行现实世界规划的基准测试。arXiv 预印本 arXiv:2402.01622,
    2024.'
- en: '[48] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless:
    Demystifying llm-based software engineering agents. arXiv preprint arXiv:2407.01489,
    2024.'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, 和 Lingming Zhang. 无代理：揭示基于大型语言模型的软件工程智能体。arXiv
    预印本 arXiv:2407.01489, 2024。'
- en: '[49] Palash Ingle, Mithun Parab, Pranay Lendave, Amisha Bhanushali, and Pavan Kumar
    BN. A comprehensive study on llm agent challenges.'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Palash Ingle, Mithun Parab, Pranay Lendave, Amisha Bhanushali, 和 Pavan
    Kumar BN. 关于大型语言模型智能体挑战的综合研究。'
- en: '[50] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based
    multi-agent systems: workflow, infrastructure, and challenges. Vicinagearth, 1(1):9,
    2024.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Xinyi Li, Sai Wang, Siqi Zeng, Yu Wu, 和 Yi Yang. 关于基于大型语言模型的多智能体系统的调查：工作流、基础设施与挑战。Vicinagearth,
    1(1):9, 2024。'
- en: '[51] Sivan Schwartz, Avi Yaeli, and Segev Shlomov. Enhancing trust in llm-based
    ai automation agents: New considerations and future challenges. arXiv preprint
    arXiv:2308.05391, 2023.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Sivan Schwartz, Avi Yaeli, 和 Segev Shlomov. 增强对基于大型语言模型的人工智能自动化智能体的信任：新的考虑和未来挑战。arXiv
    预印本 arXiv:2308.05391, 2023。'
- en: '[52] Junchi Yu, Ran He, and Rex Ying. Thought propagation: An analogical approach
    to complex reasoning with large language models. arXiv preprint arXiv:2310.03965,
    2023.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Junchi Yu, Ran He, 和 Rex Ying. 思维传播：基于类比的方法解决复杂推理问题，使用大型语言模型。arXiv 预印本
    arXiv:2310.03965, 2023。'
- en: '[53] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, and Qingyun Wu. Stateflow:
    Enhancing llm task-solving through state-driven workflows. arXiv preprint arXiv:2403.11322,
    2024.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, 和 Qingyun Wu. Stateflow:
    通过基于状态的工作流增强大型语言模型任务求解能力。arXiv 预印本 arXiv:2403.11322, 2024。'
- en: '[54] Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds,
    Tucker Balch, and Manuela Veloso. Flowmind: automatic workflow generation with
    llms. In Proceedings of the Fourth ACM International Conference on AI in Finance,
    pages 73–81, 2023.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds,
    Tucker Balch, 和 Manuela Veloso. Flowmind: 使用大型语言模型自动生成工作流。在第四届 ACM 国际金融人工智能会议论文集，73–81页，2023年。'
- en: '[55] Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang,
    Fei Huang, and Yongbin Li. Flowbench: Revisiting and benchmarking workflow-guided
    planning for llm-based agents. arXiv preprint arXiv:2406.14884, 2024.'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang,
    Fei Huang, 和 Yongbin Li. Flowbench: 重新审视并基准测试基于工作流引导的规划，针对大型语言模型智能体。arXiv 预印本
    arXiv:2406.14884, 2024。'
- en: '[56] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao
    Wang, He Zhu, and Yongfeng Zhang. Autoflow: Automated workflow generation for
    large language model agents. arXiv preprint arXiv:2407.12821, 2024.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao
    Wang, He Zhu, 和 Yongfeng Zhang. Autoflow: 为大型语言模型智能体自动生成工作流。arXiv 预印本 arXiv:2407.12821,
    2024。'
- en: '[57] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei,
    Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world
    github issues? arXiv preprint arXiv:2310.06770, 2023.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei,
    Ofir Press, 和 Karthik Narasimhan. Swe-bench: 语言模型能解决现实世界中的 GitHub 问题吗？arXiv 预印本
    arXiv:2310.06770, 2023。'
- en: '[58] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao,
    Karthik Narasimhan, and Ofir Press. Swe-agent: Agent-computer interfaces enable
    automated software engineering. arXiv preprint arXiv:2405.15793, 2024.'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] John Yang, Carlos E Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao,
    Karthik Narasimhan, 和 Ofir Press. Swe-agent: 智能体-计算机接口使得自动化软件工程成为可能。arXiv 预印本
    arXiv:2405.15793, 2024。'
- en: '[59] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with
    large language models. arXiv preprint arXiv:2305.16291, 2023.'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager: 一种具有大型语言模型的开放式具身智能体。arXiv 预印本 arXiv:2305.16291,
    2023。'
- en: '[60] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang,
    Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft: Generally
    capable agents for open-world environments via large language models with text-based
    knowledge and memory. arXiv preprint arXiv:2305.17144, 2023.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang,
    Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang 等人. Minecraft中的幽灵：通过具有文本知识和记忆的大型语言模型，普适智能体在开放世界环境中的应用。arXiv
    预印本 arXiv:2305.17144, 2023。'
- en: '[61] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with
    reinforcement learning for strategic play in the werewolf game. arXiv preprint
    arXiv:2310.18940, 2023.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, 和 Yi Wu. 使用强化学习的语言智能体在狼人游戏中的战略玩法。arXiv
    预印本 arXiv:2310.18940, 2023。'
- en: '[62] Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, and Yutaka
    Matsuo. Suspicion-agent: Playing imperfect information games with theory of mind
    aware gpt-4. arXiv preprint arXiv:2309.17277, 2023.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 郭家献，杨博，Paul Yoo，Bill Yuchen Lin，Iwasawa Yusuke，和松尾丰。怀疑代理：与具备心智理论的GPT-4一起玩不完全信息博弈。arXiv
    预印本 arXiv:2309.17277，2023年。'
- en: '[63] John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior:
    60th anniversary commemorative edition. In Theory of games and economic behavior.
    Princeton university press, 2007.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] John Von Neumann 和 Oskar Morgenstern. 博弈论与经济行为：60周年纪念版。在《博弈论与经济行为》中。普林斯顿大学出版社，2007年。'
- en: '[64] R Duncan Luce and Howard Raiffa. Games and decisions: Introduction and
    critical survey. Courier Corporation, 2012.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] R Duncan Luce 和 Howard Raiffa. 博弈与决策：导论与批判性调查。Courier Corporation，2012年。'
- en: '[65] Martin A Nowak, Karen M Page, and Karl Sigmund. Fairness versus reason
    in the ultimatum game. Science, 289(5485):1773–1775, 2000.'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Martin A Nowak，Karen M Page，和 Karl Sigmund. 公平与理性在最后通牒博弈中的对立。《科学》，289(5485)：1773–1775，2000年。'
- en: '[66] Martin W Cripps. Divisible updating. Manuscript, UCL, 2018.'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Martin W Cripps. 可分更新。手稿，伦敦大学学院，2018年。'
- en: '[67] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster, Cyril Zhang, and
    Aleksandrs Slivkins. Can large language models explore in-context? arXiv preprint
    arXiv:2403.15371, 2024.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Akshay Krishnamurthy，Keegan Harris，Dylan J Foster，Cyril Zhang，和 Aleksandrs
    Slivkins. 大型语言模型能否进行上下文探索？arXiv 预印本 arXiv:2403.15371，2024年。'
- en: '[68] Robyn M Dawes and Richard H Thaler. Anomalies: cooperation. Journal of
    economic perspectives, 2(3):187–197, 1988.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Robyn M Dawes 和 Richard H Thaler. 异常现象：合作。《经济学视角杂志》，2(3)：187–197，1988年。'
- en: '[69] David Sally. Conversation and cooperation in social dilemmas: A meta-analysis
    of experiments from 1958 to 1992. Rationality and society, 7(1):58–92, 1995.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] David Sally. 社会困境中的对话与合作：1958年到1992年间实验的元分析。《理性与社会》，7(1)：58–92，1995年。'
- en: '[70] Martijn J Van den Assem, Dennie Van Dolder, and Richard H Thaler. Split
    or steal? cooperative behavior when the stakes are large. Management Science,
    58(1):2–20, 2012.'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Martijn J Van den Assem，Dennie Van Dolder，和 Richard H Thaler. 分割还是偷窃？在赌注很大的情况下的合作行为。《管理科学》，58(1)：2–20，2012年。'
- en: '[71] Thierry Post, Martijn J Van den Assem, Guido Baltussen, and Richard H
    Thaler. Deal or no deal? decision making under risk in a large-payoff game show.
    American Economic Review, 98(1):38–71, 2008.'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Thierry Post，Martijn J Van den Assem，Guido Baltussen，和 Richard H Thaler.
    交易还是不交易？在大奖游戏中的风险决策。《美国经济评论》，98(1)：38–71，2008年。'
