- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:14:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:14:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升LLM推理能力：多代理思想树验证者代理
- en: 来源：[https://arxiv.org/html/2409.11527/](https://arxiv.org/html/2409.11527/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.11527/](https://arxiv.org/html/2409.11527/)
- en: Fatemeh Haji
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 法特梅赫·哈吉
- en: Secure AI and Autonomy Lab
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 安全人工智能与自治实验室
- en: University of Texas at San Antonio
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学圣安东尼奥分校
- en: fatemeh.haji@utsa.edu
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: fatemeh.haji@utsa.edu
- en: '&Mazal Bethany'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '&马扎尔·贝瑟妮'
- en: Secure AI and Autonomy Lab
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 安全人工智能与自治实验室
- en: University of Texas at San Antonio
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学圣安东尼奥分校
- en: mazal.bethany@utsa.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: mazal.bethany@utsa.edu
- en: '&Maryam Tabar'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '&玛丽亚姆·塔巴尔'
- en: University of Texas at San Antonio
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学圣安东尼奥分校
- en: maryam.tabar@utsa.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: maryam.tabar@utsa.edu
- en: '&Cho-Yu Jason Chiang'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '&周宇·杰森·蒋'
- en: Peraton Labs
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Peraton Labs
- en: jchiang@peratonlabs.com
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: jchiang@peratonlabs.com
- en: '&Anthony Rios'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '&安东尼·里奥斯'
- en: University of Texas at San Antonio
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学圣安东尼奥分校
- en: anthony.rios@utsa.edu
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: anthony.rios@utsa.edu
- en: '&Peyman Najafirad'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '&佩伊曼·纳贾菲拉德'
- en: Secure AI and Autonomy Lab
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 安全人工智能与自治实验室
- en: University of Texas at San Antonio
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学圣安东尼奥分校
- en: peyman.najafirad@utsa.edu
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: peyman.najafirad@utsa.edu
- en: Abstract
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Multi-agent strategies have emerged as a promising approach to enhance the
    reasoning abilities of Large Language Models (LLMs) by assigning specialized roles
    in the problem-solving process. Concurrently, Tree of Thoughts (ToT) methods have
    shown potential in improving reasoning for complex question-answering tasks by
    exploring diverse reasoning paths. A critical limitation in multi-agent reasoning
    is the ’Reasoner’ agent’s shallow exploration of reasoning paths. While ToT strategies
    could help mitigate this problem, they may generate flawed reasoning branches,
    which could harm the trustworthiness of the final answer. To leverage the strengths
    of both multi-agent reasoning and ToT strategies, we introduce a novel approach
    combining ToT-based Reasoner agents with a Thought Validator agent. Multiple Reasoner
    agents operate in parallel, employing ToT to explore diverse reasoning paths.
    The Thought Validator then scrutinizes these paths, considering a Reasoner’s conclusion
    only if its reasoning is valid. This method enables a more robust voting strategy
    by discarding faulty reasoning paths, enhancing the system’s ability to tackle
    tasks requiring systematic and trustworthy reasoning. Our method demonstrates
    superior performance compared to existing techniques when evaluated on the GSM8K
    dataset, outperforming the standard ToT strategy by an average 5.6% across four
    LLMs. The code and related content can be found in: [https://github.com/SecureAIAutonomyLab/MA-ToT](https://github.com/SecureAIAutonomyLab/MA-ToT)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多代理策略作为一种有前景的方法，通过在问题解决过程中分配专业角色，提升了大型语言模型（LLMs）的推理能力。同时，思想树（ToT）方法通过探索多样的推理路径，在复杂的问答任务中展现出改善推理的潜力。多代理推理的一个关键局限性是“推理者”（Reasoner）代理对推理路径的浅层探索。尽管ToT策略有助于缓解这一问题，但它们可能会生成错误的推理分支，从而损害最终答案的可信度。为了发挥多代理推理和ToT策略的优势，我们提出了一种新方法，结合了基于ToT的推理者代理和思想验证者代理（Thought
    Validator）。多个推理者代理并行工作，利用ToT探索多样的推理路径。然后，思想验证者对这些路径进行审查，只有在推理有效时，才考虑推理者的结论。这种方法通过丢弃错误的推理路径，实现了更为强大的投票策略，增强了系统处理需要系统性和可信推理任务的能力。在GSM8K数据集上的评估结果表明，我们的方法相比现有技术表现出更优的性能，在四种LLM中，平均提升了5.6%，优于标准的ToT策略。相关代码和内容可以在以下链接找到：[https://github.com/SecureAIAutonomyLab/MA-ToT](https://github.com/SecureAIAutonomyLab/MA-ToT)
- en: 1 Introduction
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'LLMs have demonstrated remarkable capabilities across various tasks, yet they
    often struggle with complex reasoning, particularly in situations where human-like
    reasoning capabilities are crucial [[18](https://arxiv.org/html/2409.11527v2#bib.bib18)].
    To address this, multi-agent strategies have emerged as a promising method to
    enhance LLM reasoning. Using multi-agent strategies, multiple specialized agents
    collaborate, with each agent assigned distinct roles in the problem-solving process.
    By allowing different agents to tackle various aspects of a task, we they are
    able to utilize specialized expertise to each phase of the task to improve performance [[5](https://arxiv.org/html/2409.11527v2#bib.bib5)].
    This has been shown to improve the quality of answers in reasoning-intensive domains.
    However, despite the promise of multi-agent reasoning, one critical limitation
    remains: Reasoner agents often explore reasoning paths shallowly, failing to fully
    consider the complexity of the problem space. Tree of Thoughts (ToT) methods offer
    a potential solution to this limitation by encouraging a more systematic exploration
    of multiple reasoning paths. ToT allows LLMs to simulate human-like thought processes
    by branching out and examining various possibilities before converging on a solution [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)].
    By enabling LLMs to consider diverse reasoning pathways, ToT can mitigate the
    shallow exploration issue seen in some other multi-agent systems. However, while
    ToT encourages exploration, it also introduces a new challenge: the risk of generating
    flawed reasoning branches. Without proper validation, these erroneous paths could
    lower the overall trustworthiness of the final answer.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务中展现了卓越的能力，但在复杂推理方面常常存在困难，尤其是在需要类似人类推理能力的情境中[[18](https://arxiv.org/html/2409.11527v2#bib.bib18)]。为了解决这一问题，多智能体策略作为一种有前景的方法出现，旨在增强LLM的推理能力。通过使用多智能体策略，多个专门化的智能体协作，每个智能体在问题解决过程中扮演不同的角色。通过允许不同的智能体处理任务的各个方面，我们可以利用每个阶段的专业知识来提升性能[[5](https://arxiv.org/html/2409.11527v2#bib.bib5)]。研究表明，这种方法能够提高在推理密集型领域中的答案质量。然而，尽管多智能体推理有其潜力，仍然存在一个关键限制：推理智能体往往对推理路径的探索较为浅显，未能充分考虑问题空间的复杂性。思维树（ToT）方法提供了一种可能的解决方案，通过鼓励对多条推理路径的更系统性探索，来克服这一限制。ToT通过模拟人类思维过程，允许LLM分支并检查多种可能性，然后再集中于一个解决方案[[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]。通过使LLM考虑多种推理路径，ToT可以缓解一些多智能体系统中出现的浅层探索问题。然而，尽管ToT鼓励探索，它也带来了一个新的挑战：生成错误推理分支的风险。如果没有适当的验证，这些错误路径可能会降低最终答案的可信度。
- en: To address these challenges, we propose a novel approach that combines the strengths
    of multi-agent reasoning with ToT while introducing a critical validation mechanism.
    In our framework, multiple Reasoner agents operate in parallel, each employing
    ToT to explore different reasoning paths. These Reasoner agents are supported
    by a Thought Validator agent, which evaluates the proposed reasoning branches
    produced by the Reasoners. The Validator discards faulty reasoning branches, ensuring
    that only logically sound paths contribute to the final decision. This approach
    allows for both exploration of the problem space and increased reliability of
    the answers by eliminating flawed reasoning paths before they can impact the outcome.
    Our proposed approach is evaluated on the GSM8K dataset [[1](https://arxiv.org/html/2409.11527v2#bib.bib1)],
    a benchmark known for its challenging arithmetic reasoning tasks. Results show
    that our method outperforms existing techniques, demonstrating improved accuracy
    and trustworthiness in solving complex reasoning problems.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们提出了一种新颖的方法，将多智能体推理的优势与ToT结合，同时引入了一个关键的验证机制。在我们的框架中，多个推理智能体并行操作，每个智能体使用ToT探索不同的推理路径。这些推理智能体由一个思维验证智能体支持，该智能体评估推理智能体提出的推理分支。验证智能体会丢弃错误的推理分支，确保只有逻辑上合理的路径才会对最终决策产生影响。这种方法既允许探索问题空间，又通过在错误的推理路径影响结果之前将其剔除，增加了答案的可靠性。我们在GSM8K数据集[[1](https://arxiv.org/html/2409.11527v2#bib.bib1)]上评估了我们提出的方法，该数据集以其具有挑战性的算术推理任务而闻名。结果表明，我们的方法优于现有技术，在解决复杂推理问题时表现出更高的准确性和可信度。
- en: 'Our key contributions are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献如下：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The integration of ToT into a multi-agent reasoning framework.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将ToT集成到多智能体推理框架中。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The introduction of a novel Thought Validator agent that evaluates and filters
    reasoning branches produced by Reasoner agents.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引入了一种新颖的思维验证代理，用于评估和过滤由推理者代理产生的推理分支。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results on the GSM8K dataset demonstrating improved accuracy and
    performance in complex arithmetic reasoning tasks compared to existing techniques.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在GSM8K数据集上的实验结果，展示了与现有技术相比，在复杂算术推理任务中提高的准确性和表现。
- en: 2 Background
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 Multi-agent Systems for Enhancing LLM Reasoning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 增强LLM推理的多智能体系统
- en: Recent work has demonstrated various approaches to enhance LLM outputs. Over-generation
    and reranking strategies have shown promising results, with RANKGEN [[6](https://arxiv.org/html/2409.11527v2#bib.bib6)]
    introducing a ranking model that scores generated outputs based on relevance and
    coherence, and Faithfulness-Aware Decoding [[14](https://arxiv.org/html/2409.11527v2#bib.bib14)]
    demonstrating how different decoding strategies affect output quality. While these
    methods are effective for general text generation, complex reasoning tasks often
    benefit from more structured approaches. Multi-agent systems have emerged as a
    particularly effective framework, improving performance on reasoning-based tasks
    by distributing tasks among specialized agents [[2](https://arxiv.org/html/2409.11527v2#bib.bib2),
    [12](https://arxiv.org/html/2409.11527v2#bib.bib12)]. For example, CausalGPT [[13](https://arxiv.org/html/2409.11527v2#bib.bib13)]
    introduces evaluative layers to verify the reasoning branches produced by LLMs,
    while the Counterfactual Multi-Agent Debate (CFMAD) framework [[4](https://arxiv.org/html/2409.11527v2#bib.bib4)]
    provides an innovative method to mitigate the potentially biased reasoning branches
    of LLMs by assigning agents to fixed roles to generate justifications from specific
    perspectives, and a third-party judge evaluates these arguments to decide the
    most rational outcome. Despite these advancements, current methods still suffer
    from shallow sampling of reasoning paths or majority vote schemes. These techniques
    can overlook critical inferential errors and are especially prone to early-stage
    errors, which can propagate through multiple rounds of reasoning. This limitation
    is especially problematic in complex scenarios where systematically evaluating
    and eliminating incorrect options is crucial. Recent research has demonstrated
    that LLMs can effectively identify both factual and inferential mistakes [[7](https://arxiv.org/html/2409.11527v2#bib.bib7)],
    making the integration of a dedicated verification component in multi-agent systems
    particularly beneficial for assessing the faithfulness and reliability of generated
    solutions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究展示了多种方法来增强大语言模型（LLM）的输出。过度生成和重新排序策略已显示出有希望的结果，其中RANKGEN [[6](https://arxiv.org/html/2409.11527v2#bib.bib6)]引入了一个排名模型，根据相关性和连贯性对生成的输出进行评分，而信实感知解码 [[14](https://arxiv.org/html/2409.11527v2#bib.bib14)]展示了不同解码策略如何影响输出质量。尽管这些方法对于一般文本生成有效，但复杂的推理任务通常需要更结构化的方法。多智能体系统作为一个特别有效的框架，已在推理任务中提高了表现，通过将任务分配给专门的代理 [[2](https://arxiv.org/html/2409.11527v2#bib.bib2),
    [12](https://arxiv.org/html/2409.11527v2#bib.bib12)]。例如，CausalGPT [[13](https://arxiv.org/html/2409.11527v2#bib.bib13)]引入了评估层来验证LLM生成的推理分支，而反事实多智能体辩论（CFMAD）框架 [[4](https://arxiv.org/html/2409.11527v2#bib.bib4)]则通过将代理分配到固定角色，生成来自特定视角的辩护，并由第三方评审员评估这些论点，从而提供了一种创新的方法来减轻LLM推理分支的潜在偏差，决定最合理的结果。尽管这些进展取得了一定成效，但目前的方法仍然存在推理路径抽样浅显或多数投票方案的缺陷。这些技术可能忽略关键的推理错误，尤其容易出现早期阶段的错误，这些错误可能会在多个推理回合中传播。在复杂情境中，这一局限尤其成问题，因为在这些场景中，系统性地评估并排除不正确的选项至关重要。最近的研究表明，LLM能够有效识别事实性和推理性错误 [[7](https://arxiv.org/html/2409.11527v2#bib.bib7)]，这使得在多智能体系统中集成专门的验证组件，尤其对于评估生成解决方案的信实性和可靠性，具有特别重要的意义。
- en: 2.2 The Role of the ’Reasoner’ Agent in Multi-Agent Frameworks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多智能体框架中“推理者”代理的角色
- en: Within multi-agent architectures, the Reasoner agent plays a pivotal role. It
    serves as the system’s core decision-maker, ensuring that valid conclusions are
    derived from the reasoning process. However, Reasoner agents in current frameworks
    often struggle to systematically evaluate and eliminate incorrect reasoning paths,
    particularly in more challenging problem spaces. This bottleneck highlights the
    need for more advanced reasoning strategies to be integrated into the Reasoner
    agent. CFMAD has also shown that checking all available options can enhance the
    overall ability of the multi-agent systems [[4](https://arxiv.org/html/2409.11527v2#bib.bib4)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在多智能体架构中，推理器（Reasoner）智能体起着关键作用。它作为系统的核心决策者，确保从推理过程中得出有效的结论。然而，当前框架中的推理器智能体通常难以系统性地评估并排除错误的推理路径，特别是在更具挑战性的问题空间中。这个瓶颈突显了将更先进的推理策略集成到推理器智能体中的必要性。CFMAD
    还表明，检查所有可用选项可以增强多智能体系统的整体能力 [[4](https://arxiv.org/html/2409.11527v2#bib.bib4)]。
- en: 3 Method
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: We propose a novel multi-agent reasoning framework that integrates the ToT strategy
    with a robust validation mechanism to enhance complex problem-solving. Our approach
    employs multiple concurrent Reasoner agents, each using ToT to explore diverse
    reasoning paths. At each tree level, a state evaluation agent scores the generated
    reasoning, with the highest-scored reasoning expanded in the subsequent level.
    Upon reaching the final tree level, each Reasoner agent produces a proposed reasoning
    chain composed of the chain of the highest-scored reasoning in the tree. These
    reasoning branches are then independently assessed by a Thought Validator agent
    to either validate or invalidate the proposed reasoning. We then use a consensus-based
    voting mechanism, where verified reasoning paths contribute to the vote, and invalidated
    ones are abstained. If consensus is not reached, we initiate a new reasoning round,
    incorporating feedback from the Thought Validator on the reasoning branch to refine
    the next reasoning round. Our proposed framework is illustrated in Figure [1](https://arxiv.org/html/2409.11527v2#S3.F1
    "Figure 1 ‣ 3 Method ‣ Improving LLM Reasoning with Multi-Agent Tree-of-Thought
    Validator Agent").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新型的多智能体推理框架，将 ToT 策略与强大的验证机制结合，以增强复杂问题的解决能力。我们的方法采用多个并发的推理器智能体，每个智能体都使用
    ToT 探索多种推理路径。在每个树层级上，状态评估智能体对生成的推理进行评分，得分最高的推理将在下一层级中得到扩展。当达到最终树层级时，每个推理器智能体都会生成一个由树中得分最高的推理链组成的提议推理链。这些推理分支随后由思维验证器智能体独立评估，以验证或使其无效。然后，我们使用基于共识的投票机制，验证的推理路径参与投票，而无效的路径则不参与。如果未达成共识，我们将启动一个新的推理回合，结合思维验证器对推理分支的反馈，精炼下一轮推理。我们提出的框架如图
    [1](https://arxiv.org/html/2409.11527v2#S3.F1 "图 1 ‣ 3 方法 ‣ 改进 LLM 推理与多智能体树状思维验证器智能体")
    所示。
- en: '![Refer to caption](img/b309ae92b09faa614ec82db26e6231ec.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b309ae92b09faa614ec82db26e6231ec.png)'
- en: 'Figure 1: The process starts with a query being processed by multiple Reasoner
    agents. Each Reasoner agent explores various reasoning paths using the ToT strategy,
    which includes decomposition of thought steps, generation of paths, state evaluation,
    and path selection. The Thought Validator agent then evaluates the proposed reasoning
    branches, followed by a consensus-based voting mechanism. If consensus is not
    reached, a new reasoning round is initiated with feedback incorporation.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：该过程从一个查询开始，多个推理器智能体对其进行处理。每个推理器智能体使用 ToT 策略探索不同的推理路径，ToT 策略包括思维步骤的分解、路径的生成、状态评估和路径选择。然后，思维验证器（Thought
    Validator）智能体评估提出的推理分支，接着进行基于共识的投票机制。如果未达成共识，则会启动一个新的推理回合，并结合反馈进行修正。
- en: 3.1 Reasoner Agent
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 推理器智能体
- en: The Reasoner agents in our multi-agent architecture employ the ToT strategy,
    which enables structured exploration of reasoning paths in parallel. ToT improves
    upon Chain of Thought (CoT) prompting [[15](https://arxiv.org/html/2409.11527v2#bib.bib15)]
    by enabling parallel exploration and dynamic path evaluation. While CoT follows
    a single, linear path, ToT actively explores and evaluates multiple reasoning
    paths, making it better suited for complex problems that benefit from diverse
    thought exploration [[16](https://arxiv.org/html/2409.11527v2#bib.bib16)]. We
    formalize the reasoning process as a search over a tree of states. Let $Q$ denote
    the input prompt or query, and each Reasoner agent $R_{i}$ constructs a Tree of
    Thoughts $T_{i}(Q)$, where each node represents a state $s_{t}$. A state $s_{t}=[Q,z_{1},z_{2},\dots,z_{t}]$
    consists of the problem $Q$ and a sequence of intermediate reasoning steps up
    to that point $z_{1},z_{2},\dots,z_{t}$, with each step $z_{j}$ being a coherent
    unit of reasoning generated by the language model.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们多代理架构中的推理代理采用了思维树（ToT）策略，该策略使推理路径能够并行结构化探索。思维树在链式思维（CoT）提示方法的基础上进行了改进[[15](https://arxiv.org/html/2409.11527v2#bib.bib15)]，通过启用并行探索和动态路径评估，提升了性能。虽然链式思维（CoT）沿着单一线性路径前进，思维树（ToT）则主动探索并评估多条推理路径，使其更适合解决那些能够从多样化思维探索中受益的复杂问题[[16](https://arxiv.org/html/2409.11527v2#bib.bib16)]。我们将推理过程形式化为在状态树上的搜索。设
    $Q$ 为输入提示或查询，每个推理代理 $R_{i}$ 构建一个思维树 $T_{i}(Q)$，其中每个节点表示一个状态 $s_{t}$。一个状态 $s_{t}=[Q,z_{1},z_{2},\dots,z_{t}]$
    由问题 $Q$ 和直到此时为止的中间推理步骤的序列 $z_{1},z_{2},\dots,z_{t}$ 组成，每个步骤 $z_{j}$ 都是由语言模型生成的连贯推理单元。
- en: 'Step 1: Decomposition and Generation of Thought Paths'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步：思维路径的分解与生成
- en: The process is decomposed into intermediate thought steps using LLM prompting.
    For each state $s_{t}$, the next potential thought $z_{t+1}$ is generated by the
    Thought Generator $G(p_{\theta},s_{t},k)$, where $p_{\theta}$ denotes the language
    model. The Reasoner agents explore multiple branches from any given state $s_{t}$,
    corresponding to different continuations of the reasoning process. This approach
    ensures that the exploration process covers a diverse range of possible solutions,
    avoiding the linearity of CoT and allowing reconsideration of earlier steps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程通过使用大型语言模型（LLM）提示将其分解为中间的思维步骤。对于每个状态 $s_{t}$，下一步潜在的思维 $z_{t+1}$ 由思维生成器 $G(p_{\theta},s_{t},k)$
    生成，其中 $p_{\theta}$ 表示语言模型。推理代理从任何给定的状态 $s_{t}$ 探索多个分支，对应于推理过程的不同延续。这种方法确保了探索过程覆盖多种可能的解决方案，避免了链式思维（CoT）的线性性，并允许重新考虑早期的步骤。
- en: 'Step 2: State Evaluation and Path Selection'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步：状态评估与路径选择
- en: 'To evaluate each state $s_{t}$, we introduce a state evaluation agent that
    assigns a score to the generated reasoning. This evaluation can be implemented
    through prompting, where the state evaluation agent assesses the quality and potential
    of each reasoning step. At each tree level, the highest-scored reasoning is selected
    for expansion in the subsequent level. This process continues until the final
    tree level is reached. The selection mechanism can be formalized as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估每个状态 $s_{t}$，我们引入了一个状态评估代理，用于为生成的推理分配分数。此评估可以通过提示实现，状态评估代理评估每个推理步骤的质量和潜力。在每一层树中，选择得分最高的推理步骤进行下一层的扩展。这个过程将持续到最终的树层为止。选择机制可以形式化为：
- en: '|  | $s_{t+1}^{*}=\arg\max_{s_{t+1}}V(p_{\theta},s_{t+1})$ |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{t+1}^{*}=\arg\max_{s_{t+1}}V(p_{\theta},s_{t+1})$ |  |'
- en: where $V(p_{\theta},s_{t+1})$ is the evaluation score assigned by the state
    evaluation agent.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V(p_{\theta},s_{t+1})$ 是由状态评估代理分配的评估分数。
- en: 'Step 3: Reasoning Branch Construction'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步：推理分支构建
- en: 'Upon reaching the final tree level, each Reasoner agent constructs a proposed
    reasoning chain. This chain is composed of the highest-scored reasoning steps
    from each level of the tree. Formally, the reasoning branch $C_{i}$ for Reasoner
    Agent $R_{i}$ can be represented as:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 达到最终树层时，每个推理代理构建了一个提出的推理链。这个链由树中每一层的最高得分推理步骤组成。形式上，推理分支 $C_{i}$ 对于推理代理 $R_{i}$
    可以表示为：
- en: '|  | $C_{i}=[z_{1}^{,}z_{2}^{,}\dots,z_{T}^{*}]$ |  |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{i}=[z_{1}^{,}z_{2}^{,}\dots,z_{T}^{*}]$ |  |'
- en: where $z_{t}^{*}$ is the highest-scored reasoning step at level $t$ of the tree.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $z_{t}^{*}$ 是树中第 $t$ 层的最高得分推理步骤。
- en: 3.2 Thought Validator Agent
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 思维验证代理
- en: 'The Thought Validator agent, inspired by the role of a teacher providing feedback
    to students, plays a crucial role in assessing the validity of the reasoning branches
    produced by the Reasoner agents. Much like a teacher helping students refine their
    answers, this agent independently evaluates each proposed reasoning branch to
    either validate or invalidate it. For each reasoning branch $C_{i}$, the Thought
    Validator agent performs several key steps. It begins with a logical consistency
    check to evaluate the internal logic and coherence of the reasoning chain, similar
    to how a teacher might assess a student’s argument. This is followed by a factual
    accuracy assessment to verify any factual claims made within the reasoning, akin
    to a teacher fact-checking a student’s work. Finally, the agent conducts a completeness
    evaluation to ensure that the reasoning branch adequately addresses all aspects
    of the original query, much as a teacher would ensure a student’s response fully
    answers the question. Through this comprehensive process, the Thought Validator
    agent ensures the robustness and reliability of the reasoning branches, ultimately
    helping to improve the quality of the final output. Based on these assessments,
    the Thought Validator assigns a binary validation status $V_{i}$ to each reasoning
    chain:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 思维验证者代理人受教师给学生提供反馈的角色启发，在评估推理代理人所产生的推理分支的有效性方面发挥着至关重要的作用。就像教师帮助学生完善他们的答案一样，这个代理人独立地评估每一个提出的推理分支，以验证或否定它。对于每个推理分支
    $C_{i}$，思维验证者代理人执行几个关键步骤。首先进行逻辑一致性检查，以评估推理链的内部逻辑和连贯性，类似于教师评估学生论证的方式。接下来是事实准确性评估，用于验证推理中提出的任何事实性主张，类似于教师对学生工作进行事实核查。最后，代理人会进行完整性评估，确保推理分支充分解决了原始问题的所有方面，类似于教师确保学生的回答完整无缺地回答了问题。通过这一综合过程，思维验证者代理人确保推理分支的稳健性和可靠性，最终有助于提高最终输出的质量。基于这些评估，思维验证者为每个推理链分配一个二进制验证状态
    $V_{i}$：
- en: '|  | $V_{i}=\begin{cases}1&\text{if }C_{i}\text{ is validated}\\ 0&\text{if
    }C_{i}\text{ is invalidated}\end{cases}$ |  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{i}=\begin{cases}1&\text{如果 }C_{i}\text{ 被验证} \\ 0&\text{如果 }C_{i}\text{
    被否定}\end{cases}$ |  |'
- en: Consensus-Based Voting Mechanism
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于共识的投票机制
- en: 'After the validation process, we employ a consensus-based voting mechanism
    to determine the final outcome. Only validated reasoning branches contribute to
    the vote, while invalidated ones are abstained. The consensus solution $S^{*}$
    can be represented as:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证过程之后，我们使用基于共识的投票机制来确定最终结果。只有验证过的推理分支才会参与投票，而无效的推理分支则会被弃权。共识解决方案 $S^{*}$ 可以表示为：
- en: '|  | $S^{*}=\arg\max_{S}\sum_{i=1}^{N}V_{i}\cdot\delta(S=S_{i})$ |  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $S^{*}=\arg\max_{S}\sum_{i=1}^{N}V_{i}\cdot\delta(S=S_{i})$ |  |'
- en: Where $S_{i}$ represents the solution derived from reasoning branch $C_{i}$,
    $V_{i}$ is the validation status of $C_{i}$, $\delta$ is an indicator function
    that returns 1 if the solutions match and 0 otherwise, and $N$ is the total number
    of Reasoner agents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{i}$ 表示从推理分支 $C_{i}$ 中得出的解决方案，$V_{i}$ 是 $C_{i}$ 的验证状态，$\delta$ 是一个指示函数，当解决方案匹配时返回1，否则返回0，$N$
    是推理代理人的总数。
- en: 3.3 Iterative Refinement
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 迭代改进
- en: If consensus is not reached (i.e., no solution receives a majority of validated
    votes), we initiate a new reasoning round. This refinement process incorporates
    feedback from the Thought Validator on the reasoning branches to guide the next
    iteration. This iterative process continues until consensus is reached or a predefined
    maximum number of iterations is exceeded.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果未达成共识（即没有任何解决方案获得大多数有效投票），我们将启动新的推理回合。这个改进过程结合了来自思维验证者的反馈，指导下一次迭代。这个迭代过程会持续进行，直到达成共识或超过预定义的最大迭代次数。
- en: 4 Experiments
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'Dataset: GSM8K [[1](https://arxiv.org/html/2409.11527v2#bib.bib1)] is a dataset
    of 8.5K high-quality linguistically diverse grade school math word problems created
    by human problem writers. GSM8K is widely recognized as a benchmark for testing
    arithmetic reasoning in LLMs. The dataset comprises complex, multi-step mathematical
    word problems that challenge both the reasoning and computation capabilities of
    LLMs. Our experiments utilized a random subset of 500 samples from the GSM8K dataset
    as the test set. Following other works on LLM reasoning on the GSM8K dataset,
    we evaluated the performance of reasoning approaches using accuracy as the primary
    metric [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集：GSM8K[[1](https://arxiv.org/html/2409.11527v2#bib.bib1)] 是一个包含 8.5K 高质量、语言多样的小学数学文字题的数据集，由人工问题编写者创建。GSM8K
    被广泛认为是测试大语言模型（LLM）算术推理的基准。该数据集包含复杂的、多步骤的数学文字题，挑战着 LLM 的推理和计算能力。我们的实验使用了来自 GSM8K
    数据集的 500 个样本的随机子集作为测试集。遵循其他关于 LLM 推理在 GSM8K 数据集上应用的工作，我们使用准确率作为主要评估指标来评估推理方法的表现[[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]。
- en: 'Implementation Details: Our experiments cover two versions of OpenAI’s GPT
    models and two versions of Meta’s Llama 3.1 models [[3](https://arxiv.org/html/2409.11527v2#bib.bib3)].
    Specifically, we use GPT-3.5-turbo-0125 [[11](https://arxiv.org/html/2409.11527v2#bib.bib11)]
    and GPT-4o-mini-2024-07-18 [[10](https://arxiv.org/html/2409.11527v2#bib.bib10)]
    from OpenAI, accessed through their API. For the Llama 3.1 models, we employ the
    8B [[9](https://arxiv.org/html/2409.11527v2#bib.bib9)] and 70B [[8](https://arxiv.org/html/2409.11527v2#bib.bib8)]
    parameter variants. These models offer a range of capabilities and sizes, allowing
    us to explore different trade-offs between model complexity and performance in
    our experiments. We conduct all of our experiments on four Nvidia DGX A100 80
    GB GPUs, and running all these experiments in parallel took about 18 hours. For
    our baseline comparisons, we employed several prompting strategies. We began with
    input-output (IO) prompting, a standard approach that transforms a problem input
    into an output by conditioning on task instructions. We then implemented more
    advanced techniques, including Chain of Thought (CoT) [[15](https://arxiv.org/html/2409.11527v2#bib.bib15)]
    and the original ToT strategy [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)].
    For the ToT implementation, we followed the parameters used by  Yao et al. [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]
    on the GSM8K dataset, setting a tree depth of 2 and a width of 5. To ensure consistency
    across our baseline models, we used a temperature of 1 and a top_p value of 1
    for IO, CoT, and ToT. However, for our novel Thought Validator Agent, we adjusted
    these parameters to a temperature of 0.5 and a top_p of 0.4\. This adjustment
    was made to increase the determinism of the Thought Validator’s outputs, as its
    role is to validate existing reasoning rather than generate creative solutions.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节：我们的实验涵盖了两种版本的 OpenAI GPT 模型和两种版本的 Meta Llama 3.1 模型[[3](https://arxiv.org/html/2409.11527v2#bib.bib3)]。具体来说，我们使用了来自
    OpenAI 的 GPT-3.5-turbo-0125[[11](https://arxiv.org/html/2409.11527v2#bib.bib11)]
    和 GPT-4o-mini-2024-07-18[[10](https://arxiv.org/html/2409.11527v2#bib.bib10)]，通过其
    API 进行访问。对于 Llama 3.1 模型，我们使用了 8B[[9](https://arxiv.org/html/2409.11527v2#bib.bib9)]
    和 70B[[8](https://arxiv.org/html/2409.11527v2#bib.bib8)] 参数版本。这些模型提供了不同的能力和规模，使我们能够在实验中探索模型复杂性与性能之间的不同权衡。我们在四台
    Nvidia DGX A100 80 GB GPU 上进行了所有实验，并且并行运行这些实验花费了大约 18 小时。为了进行基准比较，我们采用了几种提示策略。我们首先使用了输入输出（IO）提示，这是一种标准方法，通过对任务指令进行条件化，将问题输入转换为输出。然后，我们实施了更高级的技术，包括思维链（CoT）[[15](https://arxiv.org/html/2409.11527v2#bib.bib15)]
    和原始的 ToT 策略[[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]。对于 ToT 实现，我们遵循了
    Yao 等人[[17](https://arxiv.org/html/2409.11527v2#bib.bib17)] 在 GSM8K 数据集上使用的参数，设置了树的深度为
    2，宽度为 5。为了确保基准模型之间的一致性，我们对 IO、CoT 和 ToT 使用了 1 的温度和 1 的 top_p 值。然而，对于我们的新型思维验证代理（Thought
    Validator Agent），我们将这些参数调整为温度 0.5 和 top_p 0.4。这一调整是为了提高思维验证代理输出的确定性，因为它的角色是验证现有的推理，而不是生成创意解决方案。
- en: 'Experimental Results: Table [1](https://arxiv.org/html/2409.11527v2#S4.T1 "Table
    1 ‣ 4 Experiments ‣ Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator
    Agent") shows the performance comparison of these different methods. Our results
    show that our proposed multi-agent ToT reasoner with a Thought Validator agent
    outperforms the other reasoning methods, showing an improvement of 8.8 percentage
    points over ToT for GPT-3.5-turbo (from 75.4% to 84.2%). We also see that while
    ToT and other techniques showed significant improvements over standard IO prompting
    when the LLM struggled with a task (such as with GPT 3.5 Turbo and Llama 3.1 8B),
    the performance gap narrowed considerably for problems where the model with standard
    IO prompting already exhibited strong capabilities (such as with GPT 4o mini,
    and Llama 3.1 70B). This observation suggests that the efficacy of ToT may be
    dependent on the complexity of the task and capability of the model, with its
    benefits more pronounced in challenging reasoning tasks that push the boundaries
    of the model’s baseline abilities. The effectiveness of ToT in these scenarios
    can be likened to a teacher providing feedback to a struggling student, guiding
    them through complex problems, and reinforcing correct thought processes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果：表格 [1](https://arxiv.org/html/2409.11527v2#S4.T1 "Table 1 ‣ 4 Experiments
    ‣ Improving LLM Reasoning with Multi-Agent Tree-of-Thought Validator Agent") 显示了这些不同方法的性能比较。我们的结果表明，我们提出的带有思维验证器的多智能体ToT推理器优于其他推理方法，GPT-3.5-turbo的ToT性能提升了8.8个百分点（从75.4%提升到84.2%）。我们还发现，虽然ToT和其他技术在LLM在处理任务时遇到困难时（如GPT
    3.5 Turbo和Llama 3.1 8B）显著提高了性能，但对于那些标准IO提示已表现出强大能力的任务（如GPT 4o mini和Llama 3.1 70B），性能差距明显缩小。这个观察结果表明，ToT的有效性可能依赖于任务的复杂性和模型的能力，其在推动模型基本能力边界的挑战性推理任务中表现出更为明显的优势。在这些情境下，ToT的效果可以类比为教师对
    struggling 学生提供反馈，指导他们解决复杂问题，并强化正确的思维过程。
- en: '| Method | Gpt-3.5-turbo | Gpt-4o-mini | Llama3.1-8B | Llama3.1-70B |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Gpt-3.5-turbo | Gpt-4o-mini | Llama3.1-8B | Llama3.1-70B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Standard IO | 60.0 | 91.2 | 75.4 | 93.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 标准IO | 60.0 | 91.2 | 75.4 | 93.0 |'
- en: '| CoT | 68.0 | 89.2 | 76.0 | 89.4 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 68.0 | 89.2 | 76.0 | 89.4 |'
- en: '| ToT | 75.4 | 91.6 | 80.2 | 92.8 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ToT | 75.4 | 91.6 | 80.2 | 92.8 |'
- en: '| MA ToT with Thought Validator | 84.2 | 92.2 | 89.0 | 94.8 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 使用思维验证器的MA ToT | 84.2 | 92.2 | 89.0 | 94.8 |'
- en: 'Table 1: Performance comparison of our Multi-agent ToT Reasoner with a Thought
    Validator compared to other LLM reasoning methods on the GSM8K reasoning dataset,
    evaluated across different LLMs.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：我们的多智能体ToT推理器与思维验证器在GSM8K推理数据集上与其他LLM推理方法的性能比较，评估涉及不同的LLM。
- en: 5 Limitations and Conclusion
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 限制与结论
- en: 'While the ToT approach has shown promise in enhancing reasoning capabilities,
    our observations of the outputs and reasoning trees revealed several limitations
    that warrant further investigation. A key challenge we observed is the lack of
    dynamic exploration in the search space. The ToT method proposed by Yao et al.
    [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)] employs a fixed width and
    depth for the tree structure, which our analysis showed can lead to suboptimal
    performance in certain scenarios. For instance, when examining the reasoning trees
    for problems that could be solved efficiently without extensive reasoning, we
    found that the predetermined depth of exploration often introduced unnecessary
    complexity, potentially leading to errors or confusion in the reasoning process.
    Conversely, for problems requiring more in-depth analysis, we observed that the
    fixed depth proved insufficient, limiting the model’s ability to fully explore
    complex reasoning paths. Additionally, our proposed approach, while addressing
    some of these limitations, is computationally expensive due to the use of the
    ToT method, which requires significant resources for generating and evaluating
    multiple thought paths. Our analysis shows that our multi-agent ToT approach with
    the Thought Validator requires substantially more compute resources than standard
    methods. In our 500-sample evaluation, the average token usage per question increases
    significantly: for GPT-3.5-turbo, from 256 tokens with CoT to 4000 tokens with
    ToT, while for GPT-4o-mini, from 341 to 10,600 tokens. Each Reasoner agent requires
    approximately 20 API calls per problem, with our multi-agent approach multiplying
    this cost by the number of agents plus validation steps. While this increased
    computational investment yields meaningful improvements in reasoning accuracy—demonstrated
    by an 8.8 percentage point improvement for GPT-3.5-turbo—the trade-off may require
    careful consideration in resource-constrained environments. Furthermore, while
    our evaluation on GSM8K demonstrates the effectiveness of our approach for arithmetic
    reasoning, testing on additional reasoning-intensive benchmarks would help establish
    the method’s generalizability across different types of reasoning tasks.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 ToT 方法在增强推理能力方面展现出一定的前景，但我们对输出结果和推理树的观察揭示了若干限制，这些限制值得进一步研究。我们观察到的一个关键挑战是搜索空间中缺乏动态探索。Yao
    等人提出的 ToT 方法[[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]采用了固定的树结构宽度和深度，而我们的分析显示，这在某些场景下可能导致次优的表现。例如，在检查那些不需要大量推理就能高效解决的问题时，我们发现预定的探索深度往往引入了不必要的复杂性，可能导致推理过程中出现错误或混乱。相反，对于需要更深入分析的问题，我们观察到固定的深度证明不足，限制了模型充分探索复杂推理路径的能力。此外，我们提出的方法虽然解决了一些这些限制，但由于采用了
    ToT 方法，它在计算上非常昂贵，因为该方法需要大量资源来生成和评估多个推理路径。我们的分析表明，采用 Thought Validator 的多代理 ToT
    方法相比标准方法需要显著更多的计算资源。在我们对 500 个样本的评估中，每个问题的平均 token 使用量显著增加：对于 GPT-3.5-turbo，从
    CoT 的 256 个 tokens 增加到 ToT 的 4000 个 tokens，而对于 GPT-4o-mini，从 341 个 tokens 增加到
    10,600 个 tokens。每个 Reasoner 代理大约需要每个问题 20 次 API 调用，而我们的多代理方法将这个成本乘以代理的数量和验证步骤。尽管这种增加的计算投资带来了推理准确性的显著提高——在
    GPT-3.5-turbo 上表现为 8.8 个百分点的提升——但在资源有限的环境中，这一权衡可能需要谨慎考虑。此外，尽管我们在 GSM8K 上的评估展示了我们方法在算术推理方面的有效性，但在更多推理密集型基准测试中的测试将有助于验证该方法在不同类型推理任务中的泛化能力。
- en: The Thought Validator agent demonstrates strong capabilities in assessing reasoning
    paths, and its effectiveness could be further enhanced through advanced validation
    techniques such as ensemble validation strategies or meta-learning approaches
    to improve robustness across diverse reasoning scenarios. Furthermore, while our
    evaluation on GSM8K demonstrates the effectiveness of our approach for mathematical
    reasoning, testing on additional reasoning-intensive benchmarks would help establish
    the method’s generalizability across different types of reasoning tasks.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Thought Validator 代理展示了在评估推理路径方面的强大能力，其效果可以通过先进的验证技术进一步增强，例如集成验证策略或元学习方法，以提高在多样化推理场景中的稳健性。此外，虽然我们在
    GSM8K 上的评估展示了我们方法在数学推理方面的有效性，但在更多推理密集型基准测试中的测试将有助于验证该方法在不同类型推理任务中的泛化能力。
- en: In conclusion, we have presented a novel approach that combines the ToT strategy
    with a multi-agent reasoning framework enhanced by a Thought Validator agent.
    Our method addresses key limitations in existing reasoning strategies for LLMs
    by enabling a more systematic exploration of reasoning paths while simultaneously
    improving the reliability of generated solutions. Experimental results on the
    GSM8K dataset demonstrate that our approach outperforms state-of-the-art methods,
    particularly for complex arithmetic reasoning tasks. Future work could explore
    dynamic tree structuring based on problem complexity, potentially improving efficiency
    and performance across a wider range of problem types.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们提出了一种新颖的方法，将ToT策略与多代理推理框架相结合，并通过思维验证器代理增强。我们的方法通过启用更系统地探索推理路径，同时提高生成解决方案的可靠性，解决了现有推理策略在大型语言模型中的关键局限性。针对GSM8K数据集的实验结果表明，我们的方法在复杂的算术推理任务中优于现有的最先进方法。未来的工作可以探索基于问题复杂度的动态树结构，可能在更广泛的问题类型中提高效率和性能。
- en: 6 Social Impact Statement
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 社会影响声明
- en: By improving the depth of reasoning and enabling more systematic option elimination,
    our approach could lead to more trustworthy AI applications. However, these advancements
    also raise ethical considerations regarding the deployment of highly autonomous
    reasoning systems, particularly in high-stakes domains. It is essential to carefully
    manage the use of such systems to avoid over-reliance on AI, ensuring that human
    oversight and accountability remain integral to decision-making processes. Additionally,
    the broader societal implications must be monitored to prevent unintended consequences,
    such as biases being amplified through algorithmic decision-making or the replacement
    of human expertise in fields where nuanced judgment is required.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提高推理深度并实现更系统的选项消除，我们的方法可能会推动更值得信赖的人工智能应用。然而，这些进展也提出了有关高度自主推理系统部署的伦理考虑，特别是在高风险领域。必须谨慎管理这些系统的使用，避免过度依赖人工智能，确保人类监督和问责始终融入决策过程中。此外，必须监控其更广泛的社会影响，防止意外后果，例如算法决策中偏见的放大或在人类专业知识要求细致判断的领域中取代人类专家。
- en: References
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, and John Schulman. Training verifiers to solve math word problems.
    URL [http://arxiv.org/abs/2110.14168](http://arxiv.org/abs/2110.14168).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun,
    Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 训练验证器解决数学文字题。网址 [http://arxiv.org/abs/2110.14168](http://arxiv.org/abs/2110.14168)。'
- en: '[2] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch.
    Improving factuality and reasoning in language models through multiagent debate.
    URL [http://arxiv.org/abs/2305.14325](http://arxiv.org/abs/2305.14325).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, 和 Igor Mordatch.
    通过多代理辩论改善语言模型的事实性和推理能力。网址 [http://arxiv.org/abs/2305.14325](http://arxiv.org/abs/2305.14325)。'
- en: Dubey et al. [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
    Angela Fan, et al. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*,
    2024.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等人 [2024] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
    Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
    等人. Llama 3 模型群体。*arXiv 预印本 arXiv:2407.21783*，2024。
- en: '[4] Yi Fang, Moxin Li, Wenjie Wang, Hui Lin, and Fuli Feng. Counterfactual
    debating with preset stances for hallucination elimination of LLMs. URL [http://arxiv.org/abs/2406.11514](http://arxiv.org/abs/2406.11514).'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Yi Fang, Moxin Li, Wenjie Wang, Hui Lin, 和 Fuli Feng. 通过预设立场的反事实辩论来消除大型语言模型的幻觉。网址
    [http://arxiv.org/abs/2406.11514](http://arxiv.org/abs/2406.11514)。'
- en: 'Guo et al. [2024] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model
    based multi-agents: A survey of progress and challenges. In Kate Larson, editor,
    *Proceedings of the Thirty-Third International Joint Conference on Artificial
    Intelligence, IJCAI-24*, pages 8048–8057\. International Joint Conferences on
    Artificial Intelligence Organization, 8 2024. Survey Track.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei,
    Nitesh V. Chawla, Olaf Wiest, 和 Xiangliang Zhang. 基于大型语言模型的多代理：进展与挑战综述。在 Kate
    Larson 编辑的 *《第三十三届国际人工智能联合会议论文集，IJCAI-24》* 中，第8048–8057页。国际人工智能联合会议组织，2024年8月。综述篇章。
- en: 'Krishna et al. [2022] Kalpesh Krishna, Yapei Chang, John Wieting, and Mohit
    Iyyer. Rankgen: Improving text generation with large ranking models, 2022.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等人 [2022] Kalpesh Krishna, Yapei Chang, John Wieting, 和 Mohit Iyyer.
    Rankgen：通过大型排序模型改进文本生成，2022年。
- en: '[7] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen.
    HaluEval: A large-scale hallucination evaluation benchmark for large language
    models. URL [http://arxiv.org/abs/2305.11747](http://arxiv.org/abs/2305.11747).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, 和 Ji-Rong Wen. HaluEval：大规模幻觉评估基准，面向大语言模型。网址
    [http://arxiv.org/abs/2305.11747](http://arxiv.org/abs/2305.11747)。'
- en: NousResearch [2024a] NousResearch. Nousresearch/meta-llama-3.1-70b. [https://huggingface.co/NousResearch/Meta-Llama-3.1-70B](https://huggingface.co/NousResearch/Meta-Llama-3.1-70B),
    2024a.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NousResearch [2024a] NousResearch. Nousresearch/meta-llama-3.1-70b. [https://huggingface.co/NousResearch/Meta-Llama-3.1-70B](https://huggingface.co/NousResearch/Meta-Llama-3.1-70B)，2024a。
- en: NousResearch [2024b] NousResearch. Nousresearch/meta-llama-3.1-8b. [https://huggingface.co/NousResearch/Meta-Llama-3.1-8B](https://huggingface.co/NousResearch/Meta-Llama-3.1-8B),
    2024b.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NousResearch [2024b] NousResearch. Nousresearch/meta-llama-3.1-8b. [https://huggingface.co/NousResearch/Meta-Llama-3.1-8B](https://huggingface.co/NousResearch/Meta-Llama-3.1-8B)，2024b。
- en: OpenAI [2024a] OpenAI. GPT-4o-mini. [https://platform.openai.com/docs/models/gpt-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini),
    2024a. Accessed via API gpt-4o-mini-2024-07-18.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024a] OpenAI. GPT-4o-mini. [https://platform.openai.com/docs/models/gpt-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini)，2024a。通过API
    gpt-4o-mini-2024-07-18访问。
- en: OpenAI [2024b] OpenAI. GPT-3.5 Turbo. [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo),
    2024b. Accessed via API gpt-3.5-turbo-0125.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024b] OpenAI. GPT-3.5 Turbo. [https://platform.openai.com/docs/models/gpt-3-5-turbo](https://platform.openai.com/docs/models/gpt-3-5-turbo)，2024b。通过API
    gpt-3.5-turbo-0125访问。
- en: '[12] Yashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing
    the power of intelligent LLM agents. URL [http://arxiv.org/abs/2306.03314](http://arxiv.org/abs/2306.03314).'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Yashar Talebirad 和 Amirhossein Nadiri. 多智能体协作：利用智能 LLM 代理的力量。网址 [http://arxiv.org/abs/2306.03314](http://arxiv.org/abs/2306.03314)。'
- en: '[13] Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen,
    and Liang Lin. Towards CausalGPT: A multi-agent approach for faithful knowledge
    reasoning via promoting causal consistency in LLMs. URL [http://arxiv.org/abs/2308.11914](http://arxiv.org/abs/2308.11914).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Ziyi Tang, Ruilin Wang, Weixing Chen, Keze Wang, Yang Liu, Tianshui Chen,
    和 Liang Lin. 朝着 CausalGPT 迈进：通过促进大语言模型中的因果一致性实现忠实知识推理的多智能体方法。网址 [http://arxiv.org/abs/2308.11914](http://arxiv.org/abs/2308.11914)。'
- en: Wan et al. [2023] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, and
    Mohit Bansal. Faithfulness-aware decoding strategies for abstractive summarization,
    2023.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2023] David Wan, Mengwen Liu, Kathleen McKeown, Markus Dreyer, 和 Mohit
    Bansal. 面向忠实度的摘要生成解码策略，2023年。
- en: '[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning
    in large language models. URL [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 思维链提示在大语言模型中引发推理。网址 [http://arxiv.org/abs/2201.11903](http://arxiv.org/abs/2201.11903)。'
- en: '[16] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. URL [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601).'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 思维树：利用大语言模型进行深思熟虑的问题解决。网址 [http://arxiv.org/abs/2305.10601](http://arxiv.org/abs/2305.10601)。'
- en: 'Yao et al. [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36, 2023.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2023] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 思维树：利用大语言模型进行深思熟虑的问题解决。*神经信息处理系统进展*，36, 2023年。
- en: 'Zelikman et al. [2023] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman,
    and Nick Haber. Parsel: Algorithmic reasoning with language models by composing
    decompositions. *Advances in Neural Information Processing Systems*, 36:31466–31523,
    2023.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelikman 等人 [2023] Eric Zelikman, Qian Huang, Gabriel Poesia, Noah Goodman,
    和 Nick Haber. Parsel：通过组合分解进行语言模型的算法推理。*神经信息处理系统进展*，36:31466–31523，2023年。
- en: Appendix
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Experiment Prompts
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验提示
- en: 'In our experiments, we designed a number of carefully crafted prompts to guide
    language models during reasoning tasks. Here are the key prompts used and their
    purposes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们设计了若干精心制作的提示来引导语言模型进行推理任务。以下是使用的关键提示及其目的：
- en: Standard Input-Output (IO) Prompt
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标准输入输出（IO）提示
- en: 'The standard IO prompt is used as a baseline approach:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 标准 IO 提示作为基线方法使用：
- en: '[PRE0]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This prompt directly asks the model to solve the math problem and provide the
    answer in a specific format.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示直接要求模型解决数学问题，并以特定格式提供答案。
- en: Chain of Thought (CoT) Prompt
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维链（CoT）提示
- en: 'The CoT prompt encourages the model to show its reasoning:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: CoT 提示鼓励模型展示其推理过程：
- en: '[PRE1]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This prompt explicitly asks the model to formulate a strategy before providing
    an answer, leading to a more structured thought process.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示明确要求模型在提供答案之前制定一个策略，从而引导更结构化的思维过程。
- en: Tree of Thoughts (ToT) Prompt
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维树（ToT）提示
- en: 'Our implementation of ToT is inspired by the approach described by Yao et al.
    [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)] but with specific modifications
    tailored to our multi-agent framework. The ToT method uses the CoT prompt as a
    base and applies it iteratively, allowing for branching and exploration of multiple
    reasoning paths. Our implementation includes the following components:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 ToT 的实现受到 Yao 等人描述的方法的启发 [[17](https://arxiv.org/html/2409.11527v2#bib.bib17)]，但进行了针对我们多代理框架的特定修改。ToT
    方法以 CoT 提示为基础，进行迭代应用，允许分支并探索多个推理路径。我们的实现包括以下组件：
- en: '1.'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Thought Generation: We use the ’sample’ method for generating thoughts. This
    method uses the CoT prompt as a base but applies it iteratively, allowing for
    branching and exploration of multiple reasoning paths. The prompt flow for ToT
    includes:'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 思维生成：我们使用“样本”方法来生成思维。该方法以 CoT 提示为基础，但进行迭代应用，允许分支并探索多个推理路径。ToT 的提示流程包括：
- en: '[PRE2]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '2.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'State Evaluation: For evaluating the generated thoughts, we employ the ’vote’
    method. This involves using a prompt to assign votes to different reasoning paths:'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态评估：为了评估生成的思维，我们采用“投票”方法。这个方法通过一个提示来为不同的推理路径分配投票：
- en: '[PRE3]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '3.'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Path Selection: We use the ’greedy’ method for selecting the most promising
    paths to expand further. This doesn’t involve a specific prompt but rather selection
    of the highest-scored paths from the evaluation step.'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 路径选择：我们使用“贪婪”方法选择最有前景的路径进行进一步扩展。这不涉及具体的提示，而是选择评估步骤中得分最高的路径。
- en: Each step involves multiple API calls to the language model, with the generated
    thoughts and their evaluations guiding the exploration of the reasoning space.
    This approach allows for a dynamic and adaptive exploration of potential solution
    paths, enhancing the model’s ability to tackle complex reasoning tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步涉及多个 API 调用到语言模型，生成的思维和它们的评估引导推理空间的探索。这种方法允许对潜在解决路径进行动态和适应性的探索，增强模型处理复杂推理任务的能力。
- en: Verifier Prompt
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 验证器提示
- en: 'A crucial component of our approach is the Thought Validator agent, which uses
    the following prompt:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法中的一个关键组件是思维验证器代理，它使用以下提示：
- en: '[PRE4]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This comprehensive prompt guides the Verifier in thoroughly assessing the validity
    of the reasoning process, ensuring that the final answer is not only correct but
    also logically sound and relevant to the original question.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个全面的提示指导验证者彻底评估推理过程的有效性，确保最终答案不仅正确，而且在逻辑上是合理的，并与原始问题相关。
- en: Examples
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: To demonstrate the effectiveness of our approach, we show a challenging example
    from the GSM8K dataset using the gpt-3.5-turbo model. Using this example, we can
    see how the Thought Validator Agent prevents incorrect reasoning from the ToT
    Reasoner agents from leading to errors in the final answer.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们方法的有效性，我们使用了来自 GSM8K 数据集的一个具有挑战性的示例，使用 gpt-3.5-turbo 模型。通过这个示例，我们可以看到思维验证器代理如何防止
    ToT 推理代理的错误推理导致最终答案的错误。
- en: 'Problem 1: Last month, Tasha made $80 from selling lemonade and mowing lawns.
    The first week, she mowed Kamala’s lawn three times as many times as Joe’s. The
    following week, she mowed Alba’s lawn five times as Joe’s. If Joe paid Tasha $6
    for her work, how much did she make from lemonade sales? Answer: 26.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 1：上个月，塔莎通过卖柠檬水和修剪草坪赚了 80 美元。第一周，她修剪了卡马拉的草坪的次数是乔的三倍。接下来的一个星期，她修剪了阿尔巴的草坪的次数是乔的五倍。如果乔为塔莎的工作支付了
    6 美元，那么她从卖柠檬水中赚了多少钱？答案：26。
- en: We have three rounds, each involving three Reasoner agents (R1, R2, and R3).
    After each round, the Thought Validator Agent evaluates their reasoning.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三轮，每轮有三位推理者代理（R1、R2、R3）。每轮结束后，思维验证代理会评估他们的推理。
- en: Reasoner Reasoning Summary Final Answer Verified R1 Incorrect reasoning. Algebraic
    error leads to the incorrect conclusion that Tasha did not mow Joe’s lawn. $80
    False R2 Correct strategy, but incorrect total earnings from mowing lawns ($60x
    instead of $48x). $80 False R3 Accurate reasoning. Correctly calculates the total
    earnings from mowing lawns and finds lemonade income to be $26. $26 True
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 推理者 推理总结 最终答案 是否验证 R1 错误推理。代数错误导致错误结论，认为塔莎没有给乔割草。$80 错误 R2 正确的策略，但割草收入计算错误（$60x
    而不是 $48x）。$80 错误 R3 精确推理。正确计算了割草总收入，并发现柠檬水收入为 $26。$26 正确
- en: 'Table 2: (Round 1) Reasoner Outputs and Verification Status'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：（第一轮）推理者输出和验证状态
- en: Round 1 Analysis
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一轮分析
- en: In Round 1, Reasoner 1 (R1) incorrectly calculated that Tasha did not mow Joe’s
    lawn, leading to an invalid final answer of $80\. Reasoner 2 (R2) correctly identified
    the structure of the problem but made a calculation error, arriving at $80\. Reasoner
    3 (R3) provided the correct reasoning and final answer of $26, which was verified
    as valid. However we have not reached an agreement since the only valid answer
    is the R3 response.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮中，推理者 1（R1）错误地计算出塔莎没有给乔割草，导致无效的最终答案 $80。推理者 2（R2）正确识别了问题的结构，但计算错误，得出了 $80。推理者
    3（R3）提供了正确的推理和最终答案 $26，并已验证为有效。然而，我们尚未达成共识，因为唯一有效的答案是 R3 的回答。
- en: 'Round 1 Conclusion: No Consensus Reached.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮结论：未达成共识。
- en: Reasoner Reasoning Summary Final Answer Verified R1 Repeated the previous error,
    miscalculating Tasha’s income from lawn mowing. $80 False R2 Corrected earlier
    miscalculation but again used the wrong mowing total, leading to an incorrect
    conclusion. $80 False R3 Maintained the correct reasoning and final answer ($26)
    as in Round 1. $26 True
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 推理者 推理总结 最终答案 是否验证 R1 重复了之前的错误，错误计算塔莎的割草收入。$80 错误 R2 修正了之前的计算错误，但再次使用了错误的割草总数，导致错误结论。$80
    错误 R3 保持了与第一轮相同的正确推理和最终答案（$26）。$26 正确
- en: 'Table 3: (Round 2) Reasoner Outputs and Verification Status'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：（第二轮）推理者输出和验证状态
- en: Round 2 Analysis
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第二轮分析
- en: In Round 2, Reasoner 1 (R1) repeated its earlier algebraic mistake. Reasoner
    2 (R2) adjusted its calculations but still produced an incorrect final answer.
    Reasoner 3 (R3) again provided the correct reasoning and final answer of $26.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二轮中，推理者 1（R1）重复了之前的代数错误。推理者 2（R2）调整了计算，但仍然得出了错误的最终答案。推理者 3（R3）再次提供了正确的推理和最终答案
    $26。
- en: 'Round 2 Conclusion: No Consensus Reached.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 第二轮结论：未达成共识。
- en: Reasoner Reasoning Summary Final Answer Verified R1 Corrects earlier algebraic
    error, but still does not address the lemonade sales correctly. $80 False R2 Adjusts
    previous mistake but there is a slight issue in the final calculation but the
    Validator Agent was not able to detect it. $32 False R3 Maintained the correct
    reasoning and final answer ($26) as in Round 1. $26 True
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 推理者 推理总结 最终答案 是否验证 R1 修正了之前的代数错误，但仍然未正确处理柠檬水销售问题。$80 错误 R2 调整了之前的错误，但最终计算上仍有小问题，验证代理未能发现该问题。$32
    错误 R3 保持了与第一轮相同的正确推理和最终答案（$26）。$26 正确
- en: 'Table 4: (Round3) Reasoner Outputs and Verification Status'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：（第三轮）推理者输出和验证状态
- en: Round 3 Analysis
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第三轮分析
- en: In Round 3, Reasoner 1 corrected its earlier algebraic errors but still provided
    an invalid answer ($80). Reasoner 2 finally corrected its calculations, but still
    has a small issue in final answer and reach to $32\. Reasoner 3 remained consistent
    and accurate, providing the correct answer of $26.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三轮中，推理者 1 修正了之前的代数错误，但仍然提供了无效的答案（$80）。推理者 2 最终修正了计算错误，但最终答案依然有小问题，得出了 $32。推理者
    3 保持一致且准确，提供了正确答案 $26。
- en: 'Final Conclusion: Most frequent valid answer is $26\. Final answer: 26'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结论：最常见的有效答案是 $26。最终答案：26
- en: 'Problem 2: Bob is in charge of doing laundry for a large hotel. Each room has
    two sheets, one comforter, twice as many pillow cases as sheets and twice as many
    towels as pillow cases. How many pieces of laundry are there in 80 rooms? Answer:
    26.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 2：鲍勃负责为一家大酒店做洗衣工作。每个房间有两条床单、一条羽绒被、床单数量的两倍的枕套和枕套数量的两倍的毛巾。80个房间有多少件衣物需要洗？答案：26。
- en: Reasoner Reasoning Summary Final Answer Verified R1 Accurate breakdown of laundry
    items per room and multiplication across 80 rooms. Correct total of 1200 pieces
    of laundry. 1200 True R2 CCorrect approach, but overestimated the number of pillowcases,
    leading to an incorrect total of 1280 pieces of laundry. 1280 False R3 Correct
    breakdown of laundry per room, yielding the correct total of 1200 pieces of laundry.
    1200 True
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 推理员推理总结最终答案验证 R1 精确分解了每个房间的洗衣物品，并在80个房间之间进行了乘法运算。正确的总数是1200件洗衣物。1200 正确 R2 正确的方法，但高估了枕套的数量，导致洗衣物总数为1280件，答案错误。1280
    错误 R3 正确分解了每个房间的洗衣物，得出了正确的1200件洗衣物总数。1200 正确
- en: 'Table 5: (Round 1) Reasoner Outputs and Verification Status'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：（第一轮）推理员输出和验证状态
- en: Round 1 Analysis
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一轮分析
- en: In Round 1, the Thought Validator Agent evaluated the responses from all three
    Reasoners. Both Reasoner 1 (R1) and Reasoner 3 (R3) provided correct and consistent
    reasoning, each arriving at the total of 1200 pieces of laundry, which was validated
    as accurate. However, Reasoner 2 (R2) overestimated the number of pillowcases,
    leading to an incorrect answer of 1280 pieces, which was marked as invalid.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一轮中，思维验证代理评估了所有三位推理员的回答。推理员1（R1）和推理员3（R3）提供了正确且一致的推理，每人得出的总数都是1200件洗衣物，且得到了验证为准确。然而，推理员2（R2）高估了枕套的数量，导致了1280件洗衣物的错误答案，并被标记为无效。
- en: Since two verified Reasoners (R1 and R3) agreed on the correct answer, the final
    result of 1200 pieces of laundry was confidently returned.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于两位经过验证的推理员（R1 和 R3）就正确答案达成一致，最终结果1200件洗衣物得到了可靠确认。
- en: 'Round 1 Conclusion: At least two verified reasoners agree. Final Answer: 1200'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮结论：至少有两位经过验证的推理员一致同意。最终答案：1200
