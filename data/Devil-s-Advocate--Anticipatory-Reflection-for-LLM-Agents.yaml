- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2025-01-11 12:37:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:37:12'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Devil’s Advocate: Anticipatory Reflection for LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Devil’s Advocate: LLM代理的预见性反思'
- en: 来源：[https://arxiv.org/html/2405.16334/](https://arxiv.org/html/2405.16334/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.16334/](https://arxiv.org/html/2405.16334/)
- en: Haoyu Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Haoyu Wang
- en: UPenn
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: UPenn
- en: why16gzl@seas.upenn.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: why16gzl@seas.upenn.edu
- en: '&Tao Li'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Tao Li'
- en: Google DeepMind
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Google DeepMind
- en: tlinlp@google.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: tlinlp@google.com
- en: '&Zhiwei Deng'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Zhiwei Deng'
- en: Google DeepMind
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Google DeepMind
- en: zhiweideng@google.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: zhiweideng@google.com
- en: \ANDDan Roth
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: \ANDDan Roth
- en: UPenn
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: UPenn
- en: danroth@seas.upenn.edu
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: danroth@seas.upenn.edu
- en: '&Yang Li'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&Yang Li'
- en: Google DeepMind
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Google DeepMind
- en: liyang@google.com Work done during internship at Google DeepMind.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: liyang@google.com 工作完成于Google DeepMind实习期间。
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this work, we introduce a novel approach that equips LLM agents with introspection,
    enhancing consistency and adaptability in solving complex tasks. Our approach
    prompts LLM agents to decompose a given task into manageable subtasks (i.e., to
    make a plan), and to continuously introspect upon the suitability and results
    of their actions. We implement a three-fold introspective intervention: 1) anticipatory
    reflection on potential failures and alternative remedy before action execution,
    2) post-action alignment with subtask objectives and backtracking with remedy
    to ensure utmost effort in plan execution, and 3) comprehensive review upon plan
    completion for future strategy refinement. By deploying and experimenting with
    this methodology—a zero-shot approach—within WebArena for practical tasks in web
    environments, our agent demonstrates superior performance with a success rate
    of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest
    that our introspection-driven approach not only enhances the agent’s ability to
    navigate unanticipated challenges through a robust mechanism of plan execution,
    but also improves efficiency by reducing the number of trials and plan revisions
    by 45% needed to achieve a task.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种新颖的方法，通过内省赋能LLM代理，增强其在解决复杂任务中的一致性和适应性。我们的方法促使LLM代理将给定任务分解为可管理的子任务（即，制定计划），并持续进行自我反思，评估其行动的适用性和结果。我们实施了三重内省干预：1）在执行前进行预见性反思，预测潜在失败并提出备用解决方案，2）行动后对照子任务目标进行对齐，并通过回溯和补救确保尽最大努力执行计划，3）在计划完成后进行全面回顾，以便未来策略的优化。通过在WebArena中部署并实验这一零-shot方法——应用于Web环境中的实际任务——我们的代理展示了优于现有零-shot方法3.5%的23.5%成功率。实验结果表明，我们的内省驱动方法不仅通过强大的计划执行机制增强了代理应对意外挑战的能力，还通过减少45%的试验次数和计划修订，提高了效率，从而完成任务。
- en: 'Devil’s Advocate: Anticipatory Reflection for LLM Agents'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'Devil’s Advocate: LLM代理的预见性反思'
- en: 'Haoyu Wang^†^†thanks: Work done during internship at Google DeepMind. UPenn
    why16gzl@seas.upenn.edu                        Tao Li Google DeepMind tlinlp@google.com
                           Zhiwei Deng Google DeepMind zhiweideng@google.com'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'Haoyu Wang^†^†感谢: 工作完成于Google DeepMind实习期间。UPenn why16gzl@seas.upenn.edu                       
    Tao Li Google DeepMind tlinlp@google.com                        Zhiwei Deng Google
    DeepMind zhiweideng@google.com'
- en: Dan Roth UPenn danroth@seas.upenn.edu                        Yang Li Google
    DeepMind liyang@google.com
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Dan Roth UPenn danroth@seas.upenn.edu                        Yang Li Google
    DeepMind liyang@google.com
- en: 1 Introduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Two roads diverged in a yellow wood,
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在一片黄色的树林中，两条路分岔，
- en: And sorry I could not travel both
  id: totrans-28
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 并且抱歉我无法同时走两条路
- en: $\cdots$
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: $\cdots$
- en: Then took the other, as just as fair,
  id: totrans-30
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 然后选择了另一条，正如它同样公平，
- en: And having perhaps the better claim
  id: totrans-31
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 并且也许更有理由声称
- en: ''
  id: totrans-32
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Robert Frost
  id: totrans-33
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 罗伯特·弗罗斯特
- en: '![Refer to caption](img/31bc6fe35d8efc90c47861f6fbe386aa.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/31bc6fe35d8efc90c47861f6fbe386aa.png)'
- en: 'Figure 1: Conceptual difference between our anticipatory reflection and regular
    ones. Circles denote states and arrows actions. At the branching level, our method
    does not only yield the next action, but also anticipates a potential error associated
    with it and plans for backups. In contrast, regular reflection performs trials
    sequentially, correcting one error for each pass.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：我们的预见性反思与常规反思之间的概念差异。圆圈表示状态，箭头表示动作。在分支层次上，我们的方法不仅预测下一步动作，还预见到可能的错误，并规划备用方案。相比之下，常规反思是顺序进行的，每次试验修正一个错误。
- en: The enduring appeal of Frost’s emblematic poem, “The Road Not Taken,” resides
    not just in its poetic elegance, but also in the profound lesson it imparts about
    decision-making. As we stand at the crossroads of a choice, it is a daunting challenge
    to assess probable outcomes and choose a course that best aligns with our objectives.
    This task becomes even more formidable when Large Language Model (LLM) agents
    Huang et al. ([2022b](https://arxiv.org/html/2405.16334v4#bib.bib9)); Yao et al.
    ([2023b](https://arxiv.org/html/2405.16334v4#bib.bib29)); Song et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib18))
    have to navigate complex scenarios unfolding in real time, e.g., solving tasks
    in web environments Liu et al. ([2018](https://arxiv.org/html/2405.16334v4#bib.bib11));
    Yao et al. ([preprint](https://arxiv.org/html/2405.16334v4#bib.bib27)); Deng et al.
    ([2023](https://arxiv.org/html/2405.16334v4#bib.bib2)); Zhou et al. ([2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)),
    conducting simulated science experiments Wang et al. ([2022](https://arxiv.org/html/2405.16334v4#bib.bib23)),
    and solving embodied household tasks Shridhar et al. ([2021](https://arxiv.org/html/2405.16334v4#bib.bib17)).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Frost的代表性诗篇《未选择的路》的持久吸引力，不仅仅在于其诗意的优雅，更在于它在决策制定方面传递的深刻教训。当我们站在人生的十字路口时，评估可能的结果并选择最符合目标的道路是一个令人畏惧的挑战。当大型语言模型（LLM）代理，Huang等人（[2022b](https://arxiv.org/html/2405.16334v4#bib.bib9)）；Yao等人（[2023b](https://arxiv.org/html/2405.16334v4#bib.bib29)）；Song等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib18)）需要在实时展开的复杂情境中进行导航时，这一任务变得更加艰巨，例如，在网络环境中解决任务，Liu等人（[2018](https://arxiv.org/html/2405.16334v4#bib.bib11)）；Yao等人（[preprint](https://arxiv.org/html/2405.16334v4#bib.bib27)）；Deng等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib2)）；Zhou等人（[2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)），进行模拟科学实验，Wang等人（[2022](https://arxiv.org/html/2405.16334v4#bib.bib23)），以及解决家务任务，Shridhar等人（[2021](https://arxiv.org/html/2405.16334v4#bib.bib17)）。
- en: 'Indeed, LLM agent decision-making has witnessed enhancement by post-hoc reflection
    and correction Shinn et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib16));
    Song et al. ([2024](https://arxiv.org/html/2405.16334v4#bib.bib19)), coupled with
    adaptive planning Sun et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib20));
    Prasad et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib15)), where the
    agents learn from past successes and failures while concurrently mapping out flexible
    strategies. However, reflection usually works sequentially where only one hypothetical
    error can be corrected for each head-to-toe execution trajectory. Considering
    that such reflection is a test-time strategy, it poses a great efficiency issue.
    For instance, the agent could retry 10 times before concluding it still can not
    solve the task. Furthermore, self-reflection involves frequent shifts in plans
    which, albeit a mere inconvenience for humans, can lead to disorientation for
    AI agents. This may produce confusion, a standstill, or even an infinite loop
    of failure, which substantiates the importance of *thoroughly executing a set
    plan with utmost effort before resorting to a plan revision*. Therefore, this
    paper puts forward a methodology aimed at achieving an optimal balance between
    consistency and adaptability. This critical equilibrium mirrors the resilience
    and agility that is anticipated of a capable system that is prepared for curveballs
    but unwavering in the execution of its plan. Fig. [1](https://arxiv.org/html/2405.16334v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Devil’s Advocate: Anticipatory Reflection for LLM
    Agents") highlight our design in comparison to existing reflection strategy.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '确实，LLM代理的决策制定已经通过事后反思和修正得到了增强，Shinn等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib16)）；Song等人（[2024](https://arxiv.org/html/2405.16334v4#bib.bib19)），以及自适应规划，Sun等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib20)）；Prasad等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib15)），其中代理从过去的成功与失败中学习，同时规划出灵活的策略。然而，反思通常是顺序进行的，每次执行轨迹只能纠正一个假设性错误。考虑到这种反思是一种测试时策略，它带来了巨大的效率问题。例如，代理可能需要尝试10次，才得出结论仍然无法解决任务。此外，自我反思涉及频繁的计划调整，尽管对人类来说这只是一个小小的不便，但对于AI代理而言，可能会导致迷失方向。这可能会产生困惑、停滞，甚至无限循环的失败，从而证明了*在寻求修订计划之前，彻底执行一个设定计划并全力以赴的重要性*。因此，本文提出了一种方法论，旨在实现一致性和适应性之间的最佳平衡。这个关键的平衡体现了一个有能力的系统应对挑战时的韧性和敏捷性，它准备应对各种突发情况，但在执行计划时坚定不移。图[1](https://arxiv.org/html/2405.16334v4#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Devil’s Advocate: Anticipatory Reflection for LLM
    Agents")展示了我们设计与现有反思策略的对比。'
- en: 'In this paper, we introduce a novel approach that integrates introspection
    into the fabric of LLM agents. This approach enables agents to continuously reflect
    on their actions, thereby stimulating a learning process that dynamically optimizes
    exploration paths and enhances robust decision-making under uncertainty. Our introspective
    intervention focuses on three principal dimensions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种新颖的方法，将内省融入大语言模型代理的构建中。这种方法使得代理能够持续反思其行动，从而激发一种学习过程，动态优化探索路径并增强在不确定性下的稳健决策。我们的内省干预集中在三个主要维度：
- en: '1.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Anticipatory reflection before action execution (similar to a devil’s advocate);
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行行动前的预期反思（类似于辩论者角色）；
- en: '2.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Post-action evaluation and backtracking with remedy when necessary, to ensure
    the outcome aligns with subtask objectives;
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行动后评估并在必要时进行回溯修正，以确保结果与子任务目标一致；
- en: '3.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: An extensive review upon plan completion to generate finer plans for subsequent
    trials.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在完成计划后进行广泛的审查，以生成更精细的后续试验计划。
- en: 'We implement this introspective methodology within WebArena Zhou et al. ([2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)),
    a comprehensive web environment featuring 812 tasks in five scenarios: online
    shopping, e-commerce management, social discussion forums, maps, and software
    development platforms. Experimental results demonstrate that our approach, which
    is zero-shot, substantially outperforms state-of-the-art zero-shot methods while
    improving efficiency, paving the way for a new paradigm of intelligent systems
    that are more consistent, adaptable, and effective¹¹1Code to reproduce our results
    will be released..'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这种内省方法实施在WebArena Zhou等人（[2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)）提出的Web环境中，该环境包含五个场景下的812个任务：在线购物、电子商务管理、社交讨论论坛、地图和软件开发平台。实验结果表明，我们的方法在零-shot任务上显著优于当前最先进的零-shot方法，同时提高了效率，为智能系统的新范式铺平了道路，使其更加一致、适应性强且高效¹¹1代码用于重现我们的结果将发布。
- en: 2 Related Works
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In this paper, we develop and expand upon several key themes within the realm
    of natural language processing, with a specific focus on the integration of action
    generation, planning, and reflection in the construction of LLM agents.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们发展并扩展了自然语言处理领域中的几个关键主题，特别关注行动生成、规划和反思在大语言模型（LLM）代理构建中的整合。
- en: Action Generation
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 行动生成
- en: LLMs have been employed in tasks requiring decision-making or action generation
    and have proven useful as agent-controlling policies in embodied environments
    Huang et al. ([2022b](https://arxiv.org/html/2405.16334v4#bib.bib9), [a](https://arxiv.org/html/2405.16334v4#bib.bib8));
    Driess et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib3)); Wang et al.
    ([2023a](https://arxiv.org/html/2405.16334v4#bib.bib21)); Zhu et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib33)).
    They have also demonstrated effectiveness in text-based environments Liu et al.
    ([2018](https://arxiv.org/html/2405.16334v4#bib.bib11)); Shridhar et al. ([2021](https://arxiv.org/html/2405.16334v4#bib.bib17));
    Liu et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib12)), where techniques
    like ReAct Yao et al. ([2023b](https://arxiv.org/html/2405.16334v4#bib.bib29))
    have shown notable benefits. Despite its success, ReAct’s limitation lies in its
    inability to adjust to changes in the environment. Several improvements Madaan
    et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib13)); Shinn et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib16))
    have been proposed to counter these limitations, advocating for self-reflection
    to enhance decision-making and reasoning. However, these techniques primarily
    aim to improve single plans or trajectories without considering alternative actions,
    which could modify the plan in a wrong direction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经被应用于需要决策或行动生成的任务，并且在具身环境中作为代理控制策略已证明非常有效 Huang 等人（[2022b](https://arxiv.org/html/2405.16334v4#bib.bib9),
    [a](https://arxiv.org/html/2405.16334v4#bib.bib8)）；Driess 等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib3)）；Wang
    等人（[2023a](https://arxiv.org/html/2405.16334v4#bib.bib21)）；Zhu 等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib33)）。它们在基于文本的环境中也表现出了有效性
    Liu 等人（[2018](https://arxiv.org/html/2405.16334v4#bib.bib11)）；Shridhar 等人（[2021](https://arxiv.org/html/2405.16334v4#bib.bib17)）；Liu
    等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib12)），其中像 ReAct Yao 等人（[2023b](https://arxiv.org/html/2405.16334v4#bib.bib29)）这样的技术显示出了显著的好处。尽管取得了成功，ReAct
    的局限性在于它无法适应环境变化。为了应对这些局限性，提出了若干改进方案 Madaan 等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib13)）；Shinn
    等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib16)），提倡通过自我反思来增强决策和推理。然而，这些技术主要旨在改进单一计划或轨迹，而没有考虑到可能改变计划方向的替代行动。
- en: Position Bias Mitigation
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 位置偏见缓解
- en: While comparing answer choices is generally effective, large language models
    used for action generation are not without flaws. They can exhibit bias, especially
    towards the first (or sometimes second) answer they see, regardless of its quality.
    This is known as position bias Zheng et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib30));
    Wang et al. ([2023b](https://arxiv.org/html/2405.16334v4#bib.bib22)). Our method
    mitigates this bias by asking follow-up questions that challenge its own answer.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管比较答案选项通常是有效的，但用于生成行动的大型语言模型并非没有缺陷。它们可能会表现出偏见，尤其是对它们首先看到的（有时是第二个）答案，无论其质量如何。这种偏见被称为位置偏见
    Zheng 等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib30)）；Wang 等人（[2023b](https://arxiv.org/html/2405.16334v4#bib.bib22)）。我们的方法通过提出挑战自身答案的后续问题来缓解这种偏见。
- en: Planning
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 规划
- en: Extensive research has explored the potential of LLMs in task planning Dror
    et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib4)); Prasad et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib15));
    Sun et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib20)); Wu et al.
    ([2023](https://arxiv.org/html/2405.16334v4#bib.bib25)); Guan et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib5));
    Gur et al. ([2024](https://arxiv.org/html/2405.16334v4#bib.bib6)). The concept
    of decoupling planning and execution in formulating LLM agents has been validated
    through numerous paradigms such as ReWOO Xu et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib26)),
    ADaPT Prasad et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib15)), Structured
    Self-Reflection Li et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib10)),
    and DEFS Wang et al. ([2023c](https://arxiv.org/html/2405.16334v4#bib.bib24)).
    Nonetheless, these methods exhibit a deficiency in establishing a resilient mechanism
    for plan execution, with agents frequently revisiting and revising their plans
    following each instance of adverse environmental feedback, often due to inaccurately
    executed actions. Our approach, conversely, emphasizes executing a previously
    defined plan with unwavering effort before considering any modifications. This
    guarantees a more stable and consistent problem-solving process. To implement
    this, the factor of tree search becomes crucial for exploring the best solutions.
    Past approaches, including ToT Yao et al. ([2023a](https://arxiv.org/html/2405.16334v4#bib.bib28)),
    RAP Hao et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib7)), LATS Zhou
    et al. ([2024a](https://arxiv.org/html/2405.16334v4#bib.bib31)), AdaPlanner Sun
    et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib20)), and ToolChain*
    Zhuang et al. ([2024](https://arxiv.org/html/2405.16334v4#bib.bib34)), have incorporated
    tree search techniques in identifying the optimal route to the desired solution.
    However, our approach distinguishes itself by engaging the LLM in preparing alternate
    solutions in anticipation of impending failures, ensuring more comprehensive consideration
    in action generation.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大量研究探索了LLM（大语言模型）在任务规划中的潜力，Dror等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib4)）；Prasad等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib15)）；Sun等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib20)）；Wu等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib25)）；Guan等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib5)）；Gur等人（[2024](https://arxiv.org/html/2405.16334v4#bib.bib6)）。在制定LLM代理的规划与执行过程中，解耦规划与执行的概念已通过多个范式得到验证，如ReWOO
    Xu等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib26)）、ADaPT Prasad等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib15)）、结构化自我反思Li等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib10)）和DEFS
    Wang等人（[2023c](https://arxiv.org/html/2405.16334v4#bib.bib24)）。然而，这些方法在建立一个韧性机制以执行计划方面存在不足，代理经常会在每次遭遇不利环境反馈后重新审视并修订他们的计划，通常是因为执行的动作不准确。相较之下，我们的方法强调在考虑任何修改之前，首先坚定不移地执行预先定义的计划。这确保了一个更加稳定和一致的问题解决过程。为了实现这一点，树搜索因素变得至关重要，用于探索最佳解决方案。过去的方法，包括ToT
    Yao等人（[2023a](https://arxiv.org/html/2405.16334v4#bib.bib28)）、RAP Hao等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib7)）、LATS
    Zhou等人（[2024a](https://arxiv.org/html/2405.16334v4#bib.bib31)）、AdaPlanner Sun等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib20)）和ToolChain*
    Zhuang等人（[2024](https://arxiv.org/html/2405.16334v4#bib.bib34)），已在识别到达预期解决方案的最佳路径时融入了树搜索技术。然而，我们的方法通过让LLM在预见到即将失败时准备备用解决方案，从而确保在生成行动时进行更全面的考虑，从而使其与众不同。
- en: Reflection and Self-refinement
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 反思与自我完善
- en: Reflection and refinement techniques have advanced significantly through works
    such as Reflexion Shinn et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib16)),
    AdaPlanner Sun et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib20)),
    and AutoEval Pan et al. ([2024](https://arxiv.org/html/2405.16334v4#bib.bib14)).
    Our methodology further enhances this by incorporating an anticipatory reflection
    mechanism that operates before each action rather than performing post-hoc reflection
    after each complete trial. This approach simplifies exploration by expediting
    remedial action and reducing extensive backtracking and serial plan revisions,
    thereby improving the overall efficiency.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反思和自我完善技术通过一些工作取得了显著进展，如Reflexion Shinn等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib16)）、AdaPlanner
    Sun等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib20)）和AutoEval Pan等人（[2024](https://arxiv.org/html/2405.16334v4#bib.bib14)）。我们的研究方法进一步通过引入一种预期反思机制来增强这一技术，该机制在每次行动之前进行反思，而不是在每次完整试验后进行事后反思。这一方法通过加快补救措施的执行、减少大量的回溯和连续的计划修订，简化了探索过程，从而提高了整体效率。
- en: 3 Method
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Given a task $\mathcal{T}$ and an environment $\mathcal{E}$ with which the
    LLM agent $G$ interacts, our objective is to enable the agent to systematically
    and adaptively complete the task through introspective methods. We first present
    how we decompose the task and generate action regarding each state in the environment
    in [section 3.1](https://arxiv.org/html/2405.16334v4#S3.SS1 "3.1 Task Decomposition
    and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")
    and [section 3.2](https://arxiv.org/html/2405.16334v4#S3.SS2 "3.2 State and Action
    Representation ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM
    Agents"). Then we introduce the introspection mechanism in [section 3.3](https://arxiv.org/html/2405.16334v4#S3.SS3
    "3.3 Introspective Mechanisms ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '给定任务$\mathcal{T}$和LLM代理$G$与之交互的环境$\mathcal{E}$，我们的目标是通过内省方法使代理能够系统地和自适应地完成任务。我们首先展示如何在[第3.1节](https://arxiv.org/html/2405.16334v4#S3.SS1
    "3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory
    Reflection for LLM Agents")和[第3.2节](https://arxiv.org/html/2405.16334v4#S3.SS2
    "3.2 State and Action Representation ‣ 3 Method ‣ Devil’s Advocate: Anticipatory
    Reflection for LLM Agents")中分解任务并生成与环境中每个状态相关的行动。接着，我们在[第3.3节](https://arxiv.org/html/2405.16334v4#S3.SS3
    "3.3 Introspective Mechanisms ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")介绍内省机制。'
- en: 3.1 Task Decomposition and Planning
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务分解与规划
- en: 'The first step involves decomposing the task $\mathcal{T}$ into subtasks in
    a sequential manner, forming a plan. This decomposition is achieved through an
    LLM generation process. Let $G_{\text{plan}}$ denote the agent’s plan generation
    function, prompted by the task $\mathcal{T}$, description of the initial state
    $S_{0}$, and any experience from past trials, i.e., history $\mathcal{H}$:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步涉及将任务$\mathcal{T}$按顺序分解为子任务，形成一个计划。这一分解通过LLM生成过程实现。让$G_{\text{plan}}$表示代理的计划生成函数，该函数由任务$\mathcal{T}$、初始状态描述$S_{0}$以及过去试验中的任何经验（即历史$\mathcal{H}$）所触发：
- en: '|  | $\displaystyle\mathcal{P}\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}).$
    |  | (1) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}).$
    |  | (1) |'
- en: 'Here, the plan $\mathcal{P}$ is parsed into a sequence of ordered subtasks:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，计划$\mathcal{P}$被解析为一系列有序的子任务：
- en: '|  | $\displaystyle\mathcal{P}=(\tau_{1},\tau_{2},\ldots,\tau_{N}),$ |  | (2)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}=(\tau_{1},\tau_{2},\ldots,\tau_{N}),$ |  | (2)
    |'
- en: 'where $\tau_{i}$ represents the $i$-th subtask in the plan, and $N$ is the
    number of subtasks. For instance, Fig. [2](https://arxiv.org/html/2405.16334v4#S3.F2
    "Figure 2 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate:
    Anticipatory Reflection for LLM Agents") shows a plan with 5 subtasks for solving
    a task in WebArena. The distribution of WebArena tasks based on the number of
    subtasks within each task is illustrated in Fig. [3](https://arxiv.org/html/2405.16334v4#S3.F3
    "Figure 3 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate:
    Anticipatory Reflection for LLM Agents"). This also reflects the difficulty of
    the tasks in WebArena, where most tasks take 4-9 steps to complete.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$\tau_{i}$表示计划中的第$i$个子任务，$N$是子任务的数量。例如，图[2](https://arxiv.org/html/2405.16334v4#S3.F2
    "Figure 2 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate:
    Anticipatory Reflection for LLM Agents")展示了一个包含5个子任务的计划，用于解决WebArena中的一个任务。WebArena任务的子任务数量分布如图[3](https://arxiv.org/html/2405.16334v4#S3.F3
    "Figure 3 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate:
    Anticipatory Reflection for LLM Agents")所示。这也反映了WebArena中任务的难度，大多数任务需要4到9步才能完成。'
- en: 'Plan for task: What is the color configuration of the picture frame I bought
    in Nov 2022: 1\. Click on the ‘My Account’ link to access your account details.
    2\. Click on the ‘Order History’ link to view your past orders. 3\. Scroll down
    the page until you find the order from November 2022. 4\. Click on the order details
    link for the order from November 2022. 5\. Scroll down to the product details
    section to find the color configuration of the picture frame.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 任务计划：我在2022年11月购买的相框的颜色配置是什么：1\. 点击“我的账户”链接查看账户详情。2\. 点击“订单历史”链接查看过去的订单。3\.
    向下滚动页面，直到找到2022年11月的订单。4\. 点击2022年11月订单的订单详情链接。5\. 向下滚动到产品详情部分，找到相框的颜色配置。
- en: 'Figure 2: An example plan with 5 subtasks, generated by GPT-4. Subtasks are
    generated based on the first observation $\mathcal{S}_{0}$ and prior knowledge
    about web operation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：一个由GPT-4生成的包含5个子任务的示例计划。子任务是根据第一次观察到的状态$\mathcal{S}_{0}$和关于网页操作的先验知识生成的。
- en: '![Refer to caption](img/8f1e0e3c8db0154d34e44c4845195174.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/8f1e0e3c8db0154d34e44c4845195174.png)'
- en: 'Figure 3: Distribution of WebArena tasks based on the number of subtasks within
    each task. The number of subtasks has a majority within 4-9 with a long tail distribution.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：根据每个任务中子任务的数量分布的 WebArena 任务分布。子任务的数量大多数集中在 4-9 之间，呈现长尾分布。
- en: Algorithm 1 Introspective Agent
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 内省智能体
- en: 'Input: task $\mathcal{T};$ initial observation $S_{\text{initial}};$ environment
    $\mathcal{E};$'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：任务 $\mathcal{T};$ 初始观察 $S_{\text{initial}};$ 环境 $\mathcal{E};$
- en: 'Initialization: time $t=0;$ state $S_{t}=S_{\text{initial}};$ action $a_{t}=\emptyset;$
    plan $\mathcal{P}=\emptyset;$ subtask $\tau=\emptyset;$ history $\mathcal{H}=\emptyset;$'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：时间 $t=0;$ 状态 $S_{t}=S_{\text{initial}};$ 动作 $a_{t}=\emptyset;$ 计划 $\mathcal{P}=\emptyset;$
    子任务 $\tau=\emptyset;$ 历史 $\mathcal{H}=\emptyset;$
- en: 1:while $\neg G_{\text{completed}}(\mathcal{T},\cdot)$ do2:   $\mathcal{P}\sim
    G_{\text{plan}}(\mathcal{T},S_{t},\mathcal{H});$ $\triangleright$ Plan Revision3:   $\mathrm{Stack}=[(S_{t},a_{t},\tau)];$4:   while $\mathrm{Stack}$ do5:     $(S_{t}^{\prime},a_{t},\tau)=\mathrm{Stack}.\text{pop}()$6:     if $S_{t}\neq
    S_{t}^{\prime}$ then $\text{go\_back}(S_{t}^{\prime});S_{t}=S_{t}^{\prime};$ $\triangleright$
    Backtracking      7:     if $\tau$ is $\emptyset$ then $\mathcal{C}_{\tau}=1;\tau=\mathcal{P}.\text{next}();$8:     else
    $S_{t+1}=\mathcal{E}(a_{t});\mathcal{H}.\text{add}(G_{\text{describe}}(S_{t},a_%
    {t},S_{t+1}));$ $\triangleright$ Grounding9:        $\mathcal{C}_{\tau}\sim G_{\text{align}}(S_{t},a_{t},S_{t+1},\tau);$
    $\triangleright$ Alignment with Subtask Objective10:        if $\mathcal{C}_{\tau}$ then11:          if $G_{\text{completed}}(\mathcal{T},S_{t+1})$ then
    Finished; $\triangleright$ Early Stop           12:          if $G_{\text{completed}}(\tau,S_{t+1})$ then
    $\tau=\mathcal{P}.\text{next}()$; $\triangleright$ Next Subtask                        13:     $t\texttt{++};$14:     if $\mathcal{C}_{\tau}$ then
    $a_{t}\sim G_{\text{action}}(\tau,S_{t});$15:        for $r=1$ to $R$ do16:          $a_{t}^{(r)}\sim
    G_{\text{remedy}}(\tau,S_{t},a_{t});$ $\triangleright$ Anticipatory Reflection17:          $\mathrm{Stack}.\text{push}((S_{t},a_{t}^{(r)},\tau));$
            18:        $\mathrm{Stack}.\text{push}((S_{t},a_{t},\tau));$ $\triangleright$
    Placing $a_{t}$ at the top of $\mathrm{Stack}$
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '1: while $\neg G_{\text{completed}}(\mathcal{T},\cdot)$ do 2: $\mathcal{P}\sim
    G_{\text{plan}}(\mathcal{T},S_{t},\mathcal{H});$ $\triangleright$ 计划修正 3: $\mathrm{Stack}=[(S_{t},a_{t},\tau)];$
    4: while $\mathrm{Stack}$ do 5: $(S_{t}^{\prime},a_{t},\tau)=\mathrm{Stack}.\text{pop}()$
    6: if $S_{t}\neq S_{t}^{\prime}$ then $\text{go\_back}(S_{t}^{\prime});S_{t}=S_{t}^{\prime};$
    $\triangleright$ 回溯 7: if $\tau$ is $\emptyset$ then $\mathcal{C}_{\tau}=1;\tau=\mathcal{P}.\text{next}();$
    8: else $S_{t+1}=\mathcal{E}(a_{t});\mathcal{H}.\text{add}(G_{\text{describe}}(S_{t},a_{t},S_{t+1}));$
    $\triangleright$ 基础化 9: $\mathcal{C}_{\tau}\sim G_{\text{align}}(S_{t},a_{t},S_{t+1},\tau);$
    $\triangleright$ 与子任务目标对齐 10: if $\mathcal{C}_{\tau}$ then 11: if $G_{\text{completed}}(\mathcal{T},S_{t+1})$
    then Finished; $\triangleright$ 提前停止 12: if $G_{\text{completed}}(\tau,S_{t+1})$
    then $\tau=\mathcal{P}.\text{next}();$ $\triangleright$ 下一个子任务 13: $t\texttt{++};$
    14: if $\mathcal{C}_{\tau}$ then $a_{t}\sim G_{\text{action}}(\tau,S_{t});$ 15:
    for $r=1$ to $R$ do 16: $a_{t}^{(r)}\sim G_{\text{remedy}}(\tau,S_{t},a_{t});$
    $\triangleright$ 预期反思 17: $\mathrm{Stack}.\text{push}((S_{t},a_{t}^{(r)},\tau));$
    18: $\mathrm{Stack}.\text{push}((S_{t},a_{t},\tau));$ $\triangleright$ 将 $a_{t}$
    放到 $\mathrm{Stack}$ 的顶部'
- en: 3.2 State and Action Representation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 状态与动作表示
- en: 'Let $S_{t}\in\mathcal{S}$ denote the current state of the environment at time
    $t$, where $\mathcal{S}$ is the set of all possible states. From state $S_{t}$,
    let $a_{t}\in\mathcal{A}$ denote the next action taken by the agent, where $\mathcal{A}$
    is the set of all possible actions. The next action is generated based on the
    the specific subtask $\tau_{i}$ being addressed, current state $S_{t}$, and action
    history $\mathcal{H}_{t-1}$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $S_{t}\in\mathcal{S}$ 表示时间 $t$ 时环境的当前状态，其中 $\mathcal{S}$ 是所有可能状态的集合。从状态 $S_{t}$
    出发，令 $a_{t}\in\mathcal{A}$ 表示智能体采取的下一个动作，其中 $\mathcal{A}$ 是所有可能动作的集合。下一个动作是基于正在处理的特定子任务
    $\tau_{i}$、当前状态 $S_{t}$ 和动作历史 $\mathcal{H}_{t-1}$ 生成的：
- en: '|  | $\displaystyle a_{t}\sim G_{\text{action}}(\tau_{i},S_{t},\mathcal{H}_{t-1}),$
    |  | (3) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}\sim G_{\text{action}}(\tau_{i},S_{t},\mathcal{H}_{t-1}),$
    |  | (3) |'
- en: 'where $G_{\text{action}}$ denotes the agent’s action generation function. Let
    $\mathcal{H}_{t}$ denote the history of actions taken up to time $t$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{\text{action}}$ 表示智能体的动作生成函数。令 $\mathcal{H}_{t}$ 表示直到时间 $t$ 为止所采取的动作历史：
- en: '|  | $\displaystyle\mathcal{H}_{t}=\{\hat{a}_{1},\hat{a}_{2},\ldots,\hat{a}_{t}\},$
    |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{H}_{t}=\{\hat{a}_{1},\hat{a}_{2},\ldots,\hat{a}_{t}\},$
    |  | (4) |'
- en: 'where $\hat{a}_{t}$ is a textual description of action $a_{t}$, along with
    useful information learned from this action execution, generated with function
    $G_{\text{describe}}$. The history would later be used to answer questions in
    the task or to revise the agent’s plan. $G_{\text{describe}}$ accepts as input
    the state before the action, the action itself, the state after the action:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{a}_{t}$是动作$a_{t}$的文本描述，连同从该动作执行中获得的有用信息，由函数$G_{\text{describe}}$生成。历史信息随后将用于回答任务中的问题或修改智能体的计划。$G_{\text{describe}}$接受作为输入的动作前的状态、动作本身及动作后的状态：
- en: '|  | $\displaystyle\hat{a}_{t}\sim G_{\text{describe}}(S_{t},a_{t},S_{t+1}).$
    |  | (5) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\hat{a}_{t}\sim G_{\text{describe}}(S_{t},a_{t},S_{t+1}).$
    |  | (5) |'
- en: 'When the state observation is too long to fit in the context window of an LLM,
    the state is first summarized by the LLM into a shorter description before being
    fed to $G_{\text{describe}}$ (e.g., this operation is commonly needed for solving
    web navigation tasks on content management platforms). Note that a subtask can
    involve several actions, and thus $i$ does not necessarily equal to $t$. Given
    the possibility that the task can be finished at some time $t$ before the completion
    of all subtasks, whenever the agent arrives at a new state, we ask the agent to
    check two things: whether the subtask is finished $\mathcal{C}_{\tau_{i}}\in(0,1)$²²2When
    the agent determines that a subtask is non-essential to solving the task, we also
    set $\mathcal{C}_{\tau_{i}}=1$., and whether the task is finished $\mathcal{C}_{\mathcal{T}}\in(0,1)$:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当状态观察过长，无法放入LLM的上下文窗口时，状态会先由LLM总结为一个较短的描述，然后传递给$G_{\text{describe}}$（例如，这种操作通常用于解决内容管理平台上的网页导航任务）。请注意，子任务可能涉及多个动作，因此$i$不一定等于$t$。鉴于任务可能在某个时间$t$之前就完成，而所有子任务尚未完成，每当智能体到达一个新状态时，我们会要求智能体检查两件事：子任务是否完成$\mathcal{C}_{\tau_{i}}\in(0,1)$²²2当智能体判断某个子任务对完成任务并不重要时，我们也将$\mathcal{C}_{\tau_{i}}=1$。以及任务是否完成$\mathcal{C}_{\mathcal{T}}\in(0,1)$：
- en: '|  | $\displaystyle\mathcal{C}_{\tau_{i}}$ | $\displaystyle\sim G_{\text{completed}}(\tau_{i},S_{t+1},\mathcal{H}_{t}),$
    |  | (6) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{\tau_{i}}$ | $\displaystyle\sim G_{\text{completed}}(\tau_{i},S_{t+1},\mathcal{H}_{t}),$
    |  | (6) |'
- en: '|  | $\displaystyle\mathcal{C}_{\mathcal{T}}$ | $\displaystyle\sim G_{\text{completed}}(\mathcal{T},S_{t+1},\mathcal{H}_{t}),$
    |  | (7) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{C}_{\mathcal{T}}$ | $\displaystyle\sim G_{\text{completed}}(\mathcal{T},S_{t+1},\mathcal{H}_{t}),$
    |  | (7) |'
- en: where $G_{\text{completed}}$ denotes the function for checking whether an objective
    is fulfilled. If $\mathcal{C}_{\tau_{i}}=1$, the agent moves on to solve the next
    subtask $\tau_{i+1}$; whereas when the agent determines $\mathcal{C}_{\mathcal{T}}=1$,
    it finishes the current trial regardless of whether the plan $\mathcal{P}$ is
    finished.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$G_{\text{completed}}$表示检查目标是否已完成的函数。如果$\mathcal{C}_{\tau_{i}}=1$，智能体将继续解决下一个子任务$\tau_{i+1}$；而当智能体判断$\mathcal{C}_{\mathcal{T}}=1$时，不管计划$\mathcal{P}$是否完成，智能体都会结束当前尝试。
- en: 3.3 Introspective Mechanisms
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 反思机制
- en: The sequential action generation above can potentially execute the plan and
    solve the task already. Nevertheless, without proper introspection and adaptation,
    the agent might be stuck at a certain unsolvable subtask or go into a loop of
    failure when unexpected problems emerge. Thus, we introduce three introspective
    mechanisms to enhance our LLM agent’s problem-solving ability below.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 上述的顺序动作生成可能已经能够执行计划并解决任务。然而，若没有适当的反思与适应，智能体可能会卡在某个无法解决的子任务上，或者在意外问题出现时陷入失败循环。因此，我们引入了三种反思机制，以增强我们的LLM智能体的解决问题能力。
- en: 3.3.1 Anticipatory Reflection (Devil’s Advocate)
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 预期反思（魔鬼代言人）
- en: 'The first layer of introspection occurs before each action execution. The agent
    anticipates potential failures and comes up with $R$ alternative remedies $[a_{t}^{1},a_{t}^{2},\cdots,a_{t}^{R}]$.
    Each remedy action is generated by prompting the LLM with a follow-up question:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层反思发生在每次执行动作之前。智能体预见到潜在的失败，并提出$R$个替代补救方案$[a_{t}^{1},a_{t}^{2},\cdots,a_{t}^{R}]$。每个补救动作是通过向LLM提问跟进问题生成的：
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '"If your answer above is not correct, instead, the next action should be:"'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '"如果你上面的答案不正确，那么，接下来的动作应该是："'
- en: 'We use $G_{\text{remedy}}$ to denote the generation of remedy actions, which
    accepts as input the subtask $\tau_{i}$, the current state $S_{t}$, the action
    history $\mathcal{H}_{t-1}$, and the LLM predicted next action $a_{t}$ at first
    attempt:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用$G_{\text{remedy}}$表示生成补救动作的过程，它接受子任务$\tau_{i}$、当前状态$S_{t}$、动作历史$\mathcal{H}_{t-1}$以及LLM预测的下一步动作$a_{t}$作为输入，首次尝试时：
- en: '|  | $\displaystyle a_{t}^{r}$ | $\displaystyle\sim G_{\text{remedy}}(\tau_{i},S_{t},\mathcal{H}_{t-1},a_{t}).$
    |  | (8) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{t}^{r}$ | $\displaystyle\sim G_{\text{remedy}}(\tau_{i},S_{t},\mathcal{H}_{t-1},a_{t}).$
    |  | (8) |'
- en: 'If later found necessary, the agent can go back to state $S_{t}$ to modify
    the original action $a_{t}$ to try the remedy action $a_{t}^{r}$ to ensure a smooth
    plan execution. For example, in Fig. [4](https://arxiv.org/html/2405.16334v4#S3.F4
    "Figure 4 ‣ 3.3.2 Post-action Evaluation and Backtracking ‣ 3.3 Introspective
    Mechanisms ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    we show a state observation where all three clicking actions align with the objective
    of the current subtask. The execution of any of these actions would complete the
    subtask; yet the agent might need to return to this state if it later determines
    that the action predicted at first attempt was incorrect³³3The action generated
    at first attempt still gets the highest priority, i.e., $a_{t}$ is the last one
    to be pushed to the stack so it can be popped and executed first (see line 18
    in Alg. [1](https://arxiv.org/html/2405.16334v4#alg1 "Algorithm 1 ‣ 3.1 Task Decomposition
    and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"))..'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果后续发现有必要，智能体可以返回状态$S_{t}$，修改原始行动$a_{t}$，尝试补救行动$a_{t}^{r}$，以确保计划的顺利执行。例如，在图[4](https://arxiv.org/html/2405.16334v4#S3.F4
    "图4 ‣ 3.3.2 行动后评估与回溯 ‣ 3.3 内省机制 ‣ 3 方法 ‣ 魔鬼代言人：LLM智能体的预期反思")中，我们展示了一个状态观察，所有三个点击行动都与当前子任务的目标一致。执行任何一个行动都将完成子任务；然而，如果智能体后来确定最初预测的行动是错误的，它可能需要返回到这个状态³³3最初执行的行动仍然具有最高优先级，即$a_{t}$是最后一个被推入堆栈的行动，因此它可以首先被弹出并执行（参见算法[1](https://arxiv.org/html/2405.16334v4#alg1
    "算法1 ‣ 3.1 任务分解与规划 ‣ 3 方法 ‣ 魔鬼代言人：LLM智能体的预期反思")中的第18行）。
- en: 3.3.2 Post-action Evaluation and Backtracking
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 行动后评估与回溯
- en: 'The second introspective mechanism kicks in after the execution of each action.
    Here, the agent evaluates whether the action and the resulting state align with
    the subtask objective. This introspective function, denoted as $G_{\text{align}}$,
    is motivated by the state before the action $S_{t}$, the action $a_{t}$, the resulting
    state $S_{t+1}$, the current subtask $\tau_{i}$:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个内省机制在每次行动执行后启动。在这里，智能体评估该行动及其结果状态是否与子任务目标一致。这个内省功能记作$G_{\text{align}}$，它受到行动前的状态$S_{t}$、行动$a_{t}$、结果状态$S_{t+1}$以及当前子任务$\tau_{i}$的影响：
- en: '|  | $\displaystyle\theta_{t}$ | $\displaystyle\sim G_{\text{align}}(S_{t},a_{t},S_{t+1},\tau_{i}).$
    |  | (9) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta_{t}$ | $\displaystyle\sim G_{\text{align}}(S_{t},a_{t},S_{t+1},\tau_{i}).$
    |  | (9) |'
- en: Here $\theta_{t}\in(0,1)$ denotes the evaluation score reflecting how well the
    state $S_{t+1}$ aligns with the subtask objective $\tau_{i}$. It is a binary signal
    indicating whether the agent needs to stop and backtrack to some previous state
    and take an alternative action $a_{k}^{r},k\leq t$, if the execution of $a_{t}$
    does not meet the objective of the current subtask. In our experiments with web
    environments, the URL of the webpage is a useful information recorded as part
    of $S_{t}$. When backtracking, we can easily navigate back to the URL. However,
    the element information on the URL might differ from the state we first encountered
    upon arriving at that page. To address this, we prompt the LLM to map the recorded
    element in the action to the new element with which we want to interact, if necessary.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里$\theta_{t}\in(0,1)$表示反映状态$S_{t+1}$与子任务目标$\tau_{i}$一致程度的评估分数。它是一个二进制信号，指示智能体是否需要停止并回溯到某个先前的状态，采取替代行动$a_{k}^{r},k\leq
    t$，如果行动$a_{t}$的执行未能达到当前子任务的目标。在我们与网页环境的实验中，网页的URL是作为$S_{t}$一部分记录的重要信息。当回溯时，我们可以轻松地返回到该URL。然而，URL上的元素信息可能与我们首次到达该页面时遇到的状态不同。为了解决这个问题，我们提示LLM将行动中记录的元素映射到我们希望交互的新元素上（如果需要的话）。
- en: '![Refer to caption](img/093c12c2ea7231e8c8fb53baf22573c9.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/093c12c2ea7231e8c8fb53baf22573c9.png)'
- en: 'Figure 4: Screen observation at one step in solving the subtask: Click on the
    order details link for the order from November 2022. The agent might decide to
    click ($a_{t}$) on the “View Order” button of any one of the three Nov 2022 orders
    to see if a picture frame was purchased in that order, and it is highly probable
    that backtracking is needed to view the details of the other two orders (if the
    first chosen is not a picture frame). In our proposed approach, the other two
    alternative clicking actions $[a_{t}^{1},a_{t}^{2}]$ would be pushed to stack
    before the agent executes action $a_{t}$.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：解决子任务时的屏幕观察：点击2022年11月订单的订单详情链接。代理人可能决定点击（$a_{t}$）2022年11月任何一个订单的“查看订单”按钮，以查看该订单是否购买了相框，如果所选的第一个订单不是相框，则很可能需要回溯查看其他两个订单的详情。在我们提出的方法中，其他两个替代的点击动作
    $[a_{t}^{1},a_{t}^{2}]$ 会在代理人执行动作 $a_{t}$ 之前被压入栈中。
- en: 3.3.3 Plan Revision
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 计划修订
- en: 'The third introspective mechanism occurs upon plan failure, i.e., when the
    stack is empty and $\mathcal{C}_{\mathcal{T}}=0$. Now the agent performs a thorough
    review of the actions executed and the notes taken, and refines its future plan
    based on identified problems:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个反思机制发生在计划失败时，即当栈为空且$\mathcal{C}_{\mathcal{T}}=0$时。此时，代理人会对已执行的动作和所做的笔记进行彻底回顾，并根据发现的问题细化未来的计划：
- en: '|  | $\displaystyle\mathcal{P}_{\text{new}}$ | $\displaystyle\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}_{t}).$
    |  | (10) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{P}_{\text{new}}$ | $\displaystyle\sim G_{\text{plan}}(\mathcal{T},S_{0},\mathcal{H}_{t}).$
    |  | (10) |'
- en: Here, $\mathcal{P}_{\text{new}}$ is the new plan after reflecting on the past
    failed trials. The agent then re-enters the plan execution phase and starts a
    new episode.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathcal{P}_{\text{new}}$ 是反思过去失败尝试后的新计划。代理人随后重新进入计划执行阶段，开始一个新的任务。
- en: 'Through these three layers of introspection, our agent is more capable of navigating
    the complexities of unforeseen circumstances and addressing tasks, bringing us
    a significant stride closer to achieving truly autonomous, adaptable, and intelligent
    systems. By structuring the problem in this manner, we have established a clear
    framework for enabling LLM agents to perform tasks autonomously and adaptively
    through introspection. Alg. [1](https://arxiv.org/html/2405.16334v4#alg1 "Algorithm
    1 ‣ 3.1 Task Decomposition and Planning ‣ 3 Method ‣ Devil’s Advocate: Anticipatory
    Reflection for LLM Agents") shows a pseudo code of our approach.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这三层反思机制，我们的代理人能够更好地应对复杂的突发情况并处理任务，使我们更接近实现真正自主、适应性强且智能的系统。通过这种方式构建问题，我们为使LLM代理能够通过反思自主和适应性地执行任务建立了一个清晰的框架。算法
    [1](https://arxiv.org/html/2405.16334v4#alg1 "算法 1 ‣ 3.1 任务分解与规划 ‣ 3 方法 ‣ 魔鬼代言人：LLM代理的预期反思")
    展示了我们方法的伪代码。
- en: '![Refer to caption](img/6b5c7f1fc163634d5236110a262fa15e.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/6b5c7f1fc163634d5236110a262fa15e.png)'
- en: 'Figure 5: Decision making process of our agent in solving the task: What is
    the color configuration of the picture frame that I bought in Sep 2022? Before
    execution of the predicted action, the agent asks a follow-up question to itself
    regarding its decision: what if the picture frame is not in order #179? what should
    be the alternative remedy? And after finding out that order #179 contains no picture
    frame at all, the agent backtracks to the previous state to view order #175 and
    continue.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：我们代理人在解决任务中的决策过程：我在2022年9月购买的相框的颜色配置是什么？在执行预测的动作之前，代理人会就其决策向自己提出后续问题：如果相框不在订单#179中怎么办？应该采取什么替代措施？在发现订单#179根本没有相框后，代理人会回溯到之前的状态，查看订单#175并继续。
- en: 4 Experiments
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we demonstrate how introspection enhances consistency and
    adaptability of LLM agents in solving complex tasks in web environments. We first
    introduce the experimental setup for evaluation ([section 4.1](https://arxiv.org/html/2405.16334v4#S4.SS1
    "4.1 Experimental Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")), followed by evaluation results ([section 4.2](https://arxiv.org/html/2405.16334v4#S4.SS2
    "4.2 Results ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM
    Agents")). Detailed error analysis is provided in [section 5](https://arxiv.org/html/2405.16334v4#S5
    "5 Error Analyses ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"),
    which highlights the directions for future endeavor.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了如何通过内省增强LLM代理在解决网页环境中复杂任务时的一致性和适应性。我们首先介绍了评估的实验设置（[section 4.1](https://arxiv.org/html/2405.16334v4#S4.SS1
    "4.1 Experimental Setup ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents")），接着是评估结果（[section 4.2](https://arxiv.org/html/2405.16334v4#S4.SS2
    "4.2 Results ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM
    Agents")）。详细的错误分析见[section 5](https://arxiv.org/html/2405.16334v4#S5 "5 Error
    Analyses ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")，其中突出显示了未来努力的方向。'
- en: 4.1 Experimental Setup
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Live Environments
  id: totrans-108
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实时环境
- en: 'We evaluate our proposed method in the simulated web environments of WebArena
    Zhou et al. ([2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)), a dataset
    of human-annotated web browsing tasks designed to evaluate the ability of LLMs
    to perform complex, real-world actions on the internet⁴⁴4Webarena ([https://webarena.dev](https://webarena.dev))
    is licensed under a Creative Commons Attribution-ShareAlike 4.0 International
    License.. The 812 tasks in WebArena involve five websites: an online shopping
    website, a software development website, a social forum platform, a map, and an
    e-commerce management platform; and these tasks can be categorized into three
    classes: information seeking tasks, site navigation and content & config tasks,
    and unachievable tasks. Though WebArena provides visual observation (screenshots),
    in this work we use the text observation only. The observation at each step is
    the accessibility tree of the webpage, and the elements in the accessibility tree
    are all within the current viewport of a 1280$\times$720 screen. The action space
    of our LLM agent includes actions that interact with environment: click, type,
    scroll, goto, go_back, go_forward, and also a note_down action that takes down
    useful snippet/summary for answering information-seeking questions.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在WebArena Zhou等人（[2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)）模拟的网页环境中评估了我们提出的方法，这是一个旨在评估大语言模型（LLMs）在互联网上执行复杂、现实世界任务能力的数据集，其中包含人工标注的网页浏览任务。Webarena（[https://webarena.dev](https://webarena.dev)）遵循创作共用署名-相同方式共享4.0国际许可证。WebArena中的812个任务涉及五个网站：一个在线购物网站、一个软件开发网站、一个社交论坛平台、一个地图和一个电子商务管理平台；这些任务可分为三类：信息检索任务、站点导航和内容与配置任务，以及无法完成的任务。尽管WebArena提供了视觉观测（截图），在本工作中我们仅使用文本观测。每个步骤的观测是网页的可访问性树，且可访问性树中的元素都在当前1280$\times$720屏幕的视口内。我们LLM代理的动作空间包括与环境互动的动作：点击、输入、滚动、跳转、后退、前进，以及一个记录下来的动作，用于记录有用的片段/摘要，以回答信息检索类问题。
- en: Baselines
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准测试
- en: 'We employ gpt-4-0613⁵⁵5[https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    Achiam et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib1)) with a context
    window of 8k tokens to build the agents and compare our method with three other
    agent construction strategies: planning and sequential decision making (Plan +
    Act w/o reflexion), similar to ReWOO Xu et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib26));
    planning and sequential decision making with reflection (Plan + Act), similar
    to AdaPlanner Sun et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib20));
    and tree search based planning, similar to LATS Zhou et al. ([2024a](https://arxiv.org/html/2405.16334v4#bib.bib31)),
    but with reflection. In all methods, we set the upper limit on the number of actions
    to 30, i.e., after the agent executes 30 actions for a given task, it has to stop.
    In all three methods, we adopt the same prompts for action generation $G_{\text{action}}$,
    plan generation $G_{\text{plan}}$, and evaluator $G_{\text{align}}$ and $G_{\text{completed}}$
    to ensure a fair comparison⁶⁶6Detailed prompts are shown in the Appendix.. In
    our experiments, we set the LLM temperature to 1.0 and max_tokens to 512, and
    keep all other parameters as default.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用gpt-4-0613⁵⁵5[https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4](https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4)
    Achiam等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib1)）的模型，具有8k令牌的上下文窗口，用于构建代理并将我们的方法与三种其他代理构建策略进行比较：无反思的规划和顺序决策（Plan
    + Act w/o reflexion），类似于ReWOO Xu等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib26)）；带反思的规划和顺序决策（Plan
    + Act），类似于AdaPlanner Sun等人（[2023](https://arxiv.org/html/2405.16334v4#bib.bib20)）；以及基于树搜索的规划，类似于LATS
    Zhou等人（[2024a](https://arxiv.org/html/2405.16334v4#bib.bib31)），但带反思。在所有方法中，我们将每个任务的最大动作次数设置为30次，即在代理执行30次动作后，它必须停止。在这三种方法中，我们采用相同的提示来生成动作$G_{\text{action}}$、生成计划$G_{\text{plan}}$以及评估器$G_{\text{align}}$和$G_{\text{completed}}$，以确保公平比较⁶⁶6详细提示请参见附录。在我们的实验中，我们将LLM温度设置为1.0，max_tokens设置为512，并保持所有其他参数为默认值。
- en: Metrics
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 指标
- en: We follow the evaluation metric “Success Rate” in Zhou et al. ([2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)),
    and count the number of actions per trial and the number of plan revisions per
    task. To determine whether a task is successfully completed, the exact_match metric
    is used for some site navigation and information seeking tasks. However, this
    can sometimes be overly stringent. For instance, consider the URLs below that
    display the same content (under ‘electronics’, the category id of ‘headphones’
    is 60). In fact, both of them point to exactly the same webpage. However, when
    evaluating for task completion, only the one that exactly matches a predefined
    finish URL is considered correct⁷⁷7In WebArena, only the first URL link is used
    as the ground truth thus agent that reaches the second URL is judged as task incomplete..
    To address this issue, we manually review the evaluation process and correct such
    misjudgements in our results⁸⁸8Our manual correction will also be released together
    with our code..
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循Zhou等人（[2024b](https://arxiv.org/html/2405.16334v4#bib.bib32)）提出的评价指标“成功率”，并统计每次试验中的动作数量和每个任务中的计划修订次数。为了判断任务是否成功完成，对于一些站点导航和信息查找任务，使用exact_match指标。然而，这有时可能过于严格。例如，考虑以下显示相同内容的网址（在‘electronics’下，‘headphones’的类别ID为60）。事实上，它们都指向完全相同的网页。然而，在评估任务完成情况时，只有与预定义完成网址完全匹配的网址才被视为正确⁷⁷7在WebArena中，仅使用第一个URL链接作为真实值，因此到达第二个URL的代理被判定为任务未完成..
    为解决这个问题，我们手动审查评估过程，并在结果中更正此类误判⁸⁸8我们的手动修正将与代码一起发布..
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[http://localhost:7770/electronics/headphones.html](http://localhost:7770/electronics/headphones.html)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://localhost:7770/electronics/headphones.html](http://localhost:7770/electronics/headphones.html)'
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[http://localhost:7770/electronics.html?cat=60](http://localhost:7770/electronics.html?cat=60)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://localhost:7770/electronics.html?cat=60](http://localhost:7770/electronics.html?cat=60)'
- en: 4.2 Results
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: '![Refer to caption](img/b439186d3a92df047b46f49bc229d346.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b439186d3a92df047b46f49bc229d346.png)'
- en: 'Figure 6: Results of different agent construction strategies on WebArena. AR
    is short for our method, anticipatory reflection; LATS represents our in-house
    implementation of the approach proposed by Zhou et al. ([2024a](https://arxiv.org/html/2405.16334v4#bib.bib31));
    Plan + Act is a method of decomposition of task and execution of each subtask,
    similar to ReWOO Xu et al. ([2023](https://arxiv.org/html/2405.16334v4#bib.bib26)).
    All three methods are equipped with plan revision (post-failure reflection).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同智能体构建策略在WebArena上的结果。AR是我们的方法——预期反思的缩写；LATS表示我们公司内部实现的Zhou等人提出的方法（[2024a](https://arxiv.org/html/2405.16334v4#bib.bib31)）；Plan
    + Act是一种任务分解并执行每个子任务的方法，类似于ReWOO Xu等人提出的方法（[2023](https://arxiv.org/html/2405.16334v4#bib.bib26)）。这三种方法都配备了计划修订（失败后的反思）。
- en: 'The experimental results, depicted in Fig. [6](https://arxiv.org/html/2405.16334v4#S4.F6
    "Figure 6 ‣ 4.2 Results ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection
    for LLM Agents"), demonstrate the efficacy of our introspection-driven approach
    in enhancing the consistency and adaptability of LLM agents in web environments.
    We compare the success rates of various agent construction strategies across multiple
    episodes. Our method, anticipatory reflection (AR), consistently outperforms the
    others, achieving a success rate of 23.5% after seven episodes, closely followed
    by LATS with 22.7%. In contrast, the Plan + Act method shows gradual improvement,
    reaching 19.8%, but remains significantly lower than the tree-search-based AR
    and LATS methods. Taking a closer look at the performance curve of LATS, there
    is an inconsistent pattern as success rate even drops at round 5. This is likely
    due to the homogeneous generated actions through direct sampling. In comparison,
    AR benefits from the “devil’s advocate” approach, enabling more thorough planning
    and execution due to introspective follow-up questions. This trend underscores
    the importance of incorporating introspection mechanisms for both plan execution
    and revision, highlighting their critical role in enhancing consistency and efficiency.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果如图[6](https://arxiv.org/html/2405.16334v4#S4.F6 "Figure 6 ‣ 4.2 Results
    ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents")所示，展示了我们基于内省驱动的方法在提高LLM智能体在Web环境中的一致性和适应性方面的有效性。我们比较了不同智能体构建策略在多个回合中的成功率。我们的方案——预期反思（AR），始终优于其他方法，在七个回合后取得了23.5%的成功率，紧随其后的是LATS，成功率为22.7%。相比之下，Plan
    + Act方法表现出逐步改进，达到了19.8%的成功率，但仍显著低于基于树搜索的AR和LATS方法。仔细查看LATS的性能曲线，可以看到其成功率在第5轮时甚至出现了下降。这可能是由于通过直接采样生成的动作同质化所致。相比之下，AR受益于“魔鬼代言人”方法，通过内省式的后续提问，能够促使更彻底的规划和执行。这一趋势凸显了在计划执行和修订过程中纳入内省机制的重要性，突显了其在提升一致性和效率方面的关键作用。'
- en: '|  | # of Actions | # of Plan Revisions |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 动作数量 | 计划修订次数 |'
- en: '| --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | First Trial | Last Trial |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 第一次试验 | 最后一次试验 |  |'
- en: '| --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Plan+Act | 4.01 | 4.47 | 2.03 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Plan+Act | 4.01 | 4.47 | 2.03 |'
- en: '| LATS | 6.08 | 6.45 | 1.16 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| LATS | 6.08 | 6.45 | 1.16 |'
- en: '| AR | 6.39 | 7.07 | 0.64 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| AR | 6.39 | 7.07 | 0.64 |'
- en: 'Table 1: Statistics of the trajectory of different agents solving tasks on
    WebArena. We report the number of actions in the first and last trial, and also
    the number of plan revisions, i.e., trials.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同智能体在WebArena上解决任务的轨迹统计。我们报告了第一次和最后一次试验中的动作数量，以及计划修订的数量，即试验次数。
- en: 'Further insights can be gleaned from [Table 1](https://arxiv.org/html/2405.16334v4#S4.T1
    "In 4.2 Results ‣ 4 Experiments ‣ Devil’s Advocate: Anticipatory Reflection for
    LLM Agents"), which compares the average number of actions in the first and last
    trials across different methods. Our AR method shows an increase in the average
    number of actions from 6.39 in the first trial to 7.07 in the last trial, indicating
    a robust learning and adaptation process. In comparison, the average number of
    actions in the first trial of the Plan+Act method is only 4.01, suggesting that
    it stops at an early stage without completing full plan execution. Thus, our method
    effectively leverages a greater number of actions to achieve better outcomes,
    thereby reducing the number of plan revisions by 45% and improving overall efficiency.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的见解可以从[表格 1](https://arxiv.org/html/2405.16334v4#S4.T1 "在 4.2 结果 ‣ 4 实验 ‣
    魔鬼代言人：LLM代理的预期反思")中获取，该表格比较了不同方法在首轮和最后一轮试验中的平均行动次数。我们的AR方法显示，首轮的平均行动次数为6.39，而最后一轮为7.07，表明存在一个稳健的学习和适应过程。相比之下，Plan+Act方法在首轮的平均行动次数仅为4.01，表明该方法在执行过程中停留在早期阶段，未能完成完整的计划执行。因此，我们的方法有效地利用更多的行动来取得更好的结果，从而减少了45%的计划修改次数，并提高了整体效率。
- en: 5 Error Analyses
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 错误分析
- en: 'The subsequent sections shed light on an analysis of errors we observed from
    the agent’s behavior when executing tasks. Two key areas have been identified
    for detailed discussion: an agent’s occasional inability to fully learn from past
    failures, and inefficiencies in solving specific kinds of tasks due to a sequential
    planning scheme.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 随后的部分分析了在执行任务时我们观察到的代理行为中的错误。我们确定了两个关键领域进行详细讨论：代理偶尔未能从过去的失败中充分学习，以及由于顺序规划方案导致在解决特定任务时的低效。
- en: 5.1 Agent Only Takes Partial Lesson from Past Failures
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 代理仅从过去的失败中吸取部分教训
- en: '![Refer to caption](img/e14ed5741054269f36684c4a9581dc20.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e14ed5741054269f36684c4a9581dc20.png)'
- en: 'Figure 7: Screen observation at the last step to solve the task: Draft a refund
    message via their "contact us" form for the bluetooth speaker I bought Feb 2023\.
    It broke after three days of use. The shop requires the order id, the reason and
    the amount to refund in the message. Don’t submit yet.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：解决任务的最后一步的屏幕观察：通过他们的“联系我们”表单起草退款信息，针对我在2023年2月购买的蓝牙音响。使用三天后该设备坏了。商店要求在消息中提供订单ID、退款原因及退款金额。请勿提交。
- en: 'One category of common errors we notice is that the agent is not taking full
    lesson from past failure when generating a new plan. As illustrated in Fig. [7](https://arxiv.org/html/2405.16334v4#S5.F7
    "Figure 7 ‣ 5.1 Agent Only Takes Partial Lesson from Past Failures ‣ 5 Error Analyses
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), the agent is at
    the final step of drafting a refund message for a Bluetooth speaker, after a series
    of steps taken to seek information for the order. From the screen, we know that
    the agent should consolidate all the information gathered from previous steps
    and type one piece of text into the (only) box titled “What’s on your mind?”.
    However, as can be seen from the plans at the lower right corner in Fig. [7](https://arxiv.org/html/2405.16334v4#S5.F7
    "Figure 7 ‣ 5.1 Agent Only Takes Partial Lesson from Past Failures ‣ 5 Error Analyses
    ‣ Devil’s Advocate: Anticipatory Reflection for LLM Agents"), while some improvements
    were made by adding the date of purchase and a more detailed explanation in the
    revised plan, the agent still failed to optimize the input process, repeating
    the typing actions separately for fields that do not exist. This inefficiency
    in the agent’s behavior showcases the need for either an LLM with stronger reasoning
    ability or a better mechanism to solicit more comprehensive and accurate reflection.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到的一类常见错误是代理在生成新计划时没有充分从过去的失败中吸取教训。如图[7](https://arxiv.org/html/2405.16334v4#S5.F7
    "图 7 ‣ 5.1 代理仅从过去的失败中吸取部分教训 ‣ 5 错误分析 ‣ 魔鬼代言人：LLM代理的预期反思")所示，代理在起草蓝牙音响退款信息的最后一步，经过一系列步骤寻找订单信息后，应该将所有从之前步骤中获取的信息汇总，并将一段文字输入到标题为“你在想什么？”的唯一文本框中。然而，从图[7](https://arxiv.org/html/2405.16334v4#S5.F7
    "图 7 ‣ 5.1 代理仅从过去的失败中吸取部分教训 ‣ 5 错误分析 ‣ 魔鬼代言人：LLM代理的预期反思")右下角的计划中可以看到，尽管在修改的计划中通过添加购买日期和更详细的说明进行了某些改进，但代理仍未能优化输入过程，重复为不存在的字段单独键入信息。代理行为中的这一低效展示了需要一个具有更强推理能力的LLM，或一个更好的机制来引导更全面、准确的反思。
- en: 5.2 Sequential Planning is Not Enough
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 顺序规划不足以满足需求
- en: 'In our analysis, we observed a recurrent error pertaining to the design of
    the agent’s planning process. The proposed methodology structures a plan as a
    sequence of tasks that are executed in a specific order. Though it is effective
    in a decent amount of use cases, it seems to falter when faced with tasks necessitating
    more sophisticated logic. Specifically, tasks that mandate implementing a reusable
    function encapsulating several actions and employing a loop construct tend to
    challenge the model’s current configuration. For example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们观察到一个与智能体规划过程设计相关的反复出现的错误。所提议的方法将计划结构化为按特定顺序执行的任务序列。虽然在相当多的使用案例中效果不错，但当面对需要更复杂逻辑的任务时，它似乎会出现问题。特别是，当任务要求实现一个可重用的函数，该函数封装了多个操作并使用循环结构时，当前模型的配置往往会遇到挑战。例如：
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: List out reviewers, if exist, who mention about average print quality.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列出提到平均打印质量的评论者（如果有的话）。
- en: •
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Give me the SKU of the products that have 1-3 units left.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给我剩余库存为1-3个单位的产品的SKU。
- en: •
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Like all submissions created by CameronKelsey in subreddit earthporn.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就像所有由CameronKelsey在subreddit earthporn中创建的提交一样。
- en: Performing such tasks is analogous to executing SQL commands without a direct
    query API, but instead, in a realistic environment. The ability to process these
    tasks effectively would necessitate the incorporation of additional cognitive
    constructs into the planning model—e.g., memory, loops, repetitive actions, or
    encapsulation of a group of actions into callable functions. Though taking notes
    can help the agent eliminate wrong choices, these systemic extensions would add
    crucial capabilities to the web agent, significantly enhancing its navigation
    and problem-solving competence in realistic web environments. Moreover, while
    the current agent can succeed in the limited search space of simple tasks, it
    often struggles to review and introspect upon more descriptive tasks that require
    dynamic problem-solving. By addressing these limitations in future work, i.e.,
    effectively converting textual description of a plan into robust execution of
    callable functions and loops, we believe that the reasoning capability of our
    agent can be substantially improved, leading to better outcomes in understanding
    and solving tasks that involve dynamic cognition in web environments.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这些任务类似于在没有直接查询API的情况下执行SQL命令，而是在一个现实的环境中。有效处理这些任务的能力将需要将额外的认知结构纳入规划模型中——例如，记忆、循环、重复性操作，或将一组操作封装成可调用的函数。尽管做笔记可以帮助智能体排除错误选择，但这些系统扩展将为网络智能体增加关键功能，显著提升其在现实网络环境中的导航和问题解决能力。此外，虽然当前的智能体可以在简单任务的有限搜索空间内取得成功，但它常常在审查和自我反思那些需要动态问题解决的更具描述性的任务时遇到困难。通过在未来的工作中解决这些局限性，即有效地将计划的文本描述转换为可执行的函数和循环，我们相信我们智能体的推理能力可以大大提升，从而在理解和解决涉及动态认知的网络环境任务时取得更好的成果。
- en: 6 Conclusions
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work, we introduce a novel introspective methodology that significantly
    enhances the problem-solving capabilities of LLMs in complex environments, as
    demonstrated through comprehensive evaluations in the WebArena setting. Our approach
    strategically decomposes tasks into actionable subtasks and incorporates a three-tiered
    introspection process, which includes anticipatory reflection, robust post-action
    evaluation, and episode-level plan revision. This setup not only allows LLM agents
    to adapt their strategies in real time but also fosters long-term learning, reducing
    the need for frequent interventions as experience accumulates. The application
    of our introspective agent design in the WebArena benchmark demonstrates substantial
    performance gain (3.5%) over state-of-the-art zero-shot approach, along with stable
    performance curve with increasing number of rounds. Such benefits are accompanied
    by almost halving the number of plan revisions (45%) during error handling. In
    summary, by enabling LLM agents to proactively contemplate potential failures,
    evaluate actions post-execution, and continuously refine their strategy based
    on experiential insights, our approach equips AI systems with a human-like strategic
    thinking capability.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们提出了一种新颖的自省方法，该方法显著增强了大型语言模型在复杂环境中的问题解决能力，且通过在WebArena设置中的全面评估得到了验证。我们的方法通过战略性地将任务拆解为可操作的子任务，并结合三层自省过程，包括预期反思、稳健的后行动评估以及阶段性计划修订。这种设置不仅使LLM代理能够实时调整其策略，还促进了长期学习，随着经验的积累，减少了频繁干预的需求。我们在WebArena基准测试中应用自省代理设计，显示出比最先进的零-shot方法高出3.5%的性能提升，并且随着回合数增加，性能曲线保持稳定。这些好处还伴随着在错误处理过程中几乎将计划修订次数减少了45%。总之，通过使LLM代理主动思考潜在的失败、执行后评估行动并根据经验洞察持续优化策略，我们的方法赋予了人工智能系统类似人类战略思维的能力。
- en: Broader Impact
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更广泛的影响
- en: Looking forward, the integration of multi-modal data inputs could further enhance
    the contextual understanding and decision-making accuracy of these agents. The
    principles and findings from our approach provide a robust foundation for future
    research in AI, particularly in aspects of autonomous decision-making, learning
    efficiency, and adaptability. As AI continues to integrate into diverse aspects
    of decision-making, embedding introspective capabilities will be essential to
    ensure these systems operate not only with precision but with an understanding
    akin to strategic human cognition.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，融合多模态数据输入可能进一步提升这些代理的上下文理解和决策准确性。我们方法中的原则和发现为未来的人工智能研究奠定了坚实的基础，特别是在自主决策、学习效率和适应性方面。随着人工智能不断融入决策的各个方面，嵌入自省能力将成为确保这些系统不仅精确执行，而且具备类似战略性人类认知理解的关键。
- en: Ethics Statement
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: As the capabilities of LLM agents enhance and their deployment in real-world
    applications increases, it is crucial to address potential ethical concerns, particularly
    regarding data privacy, bias, and transparency. Our work focuses on improving
    agent introspection to enhance task performance and decision-making explanations,
    aiming to develop more transparent and trustworthy AI systems. We emphasize the
    importance of human oversight to monitor and mitigate unforeseen consequences
    and encourage the responsible use of this technology for societal benefit. By
    promoting continuous evaluation and fair practices, we seek to minimize biases
    and ensure that the deployment of these agents does not exacerbate social inequalities.
    Furthermore, we are committed to optimizing computational resources to reduce
    the environmental impact, advocating for sustainable AI practices.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）代理的能力不断增强，并且它们在现实世界应用中的部署逐渐增多，解决潜在的伦理问题变得尤为重要，特别是在数据隐私、偏见和透明度方面。我们的工作聚焦于改进代理的自省能力，以提升任务执行效果和决策解释，旨在开发更加透明和可信的人工智能系统。我们强调人类监督的重要性，以监控和减轻不可预见的后果，并鼓励负责任地使用这些技术以造福社会。通过推动持续评估和公平实践，我们力求减少偏见，确保这些代理的部署不会加剧社会不平等。此外，我们致力于优化计算资源，以减少环境影响，倡导可持续的人工智能实践。
- en: Limitations
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Despite substantial progress made with our current design, limitations persist
    that inhibit optimal performance. Notably, the agent lacks a full learning mechanism
    to capitalize on past failures when generating a new plan, resulting in inefficient
    execution and recurring mistakes. Furthermore, while the sequential planning approach
    is effective for simpler tasks, it falls short for more sophisticated operations,
    such as those requiring encapsulated actions or loop constructs. Additionally,
    the agent struggles with tasks that expand beyond a simple search space, suggesting
    obstacles in handling dynamic problem-solving. Last but not least, our agent needs
    significant amounts of LLM generation (i.e., API calling), consequently requiring
    substantial time and computational resources, which dents its efficiency. Therefore,
    future work needs to concentrate on improving the agent’s ability to fully learn
    from prior shortcomings, adapt to handle complex tasks, enhance dynamic problem-solving
    capabilities, and optimize time and resource utilization with more efficient LLM
    calling.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们当前设计已取得显著进展，但仍存在限制，阻碍了其最佳性能的发挥。特别是，智能体缺乏完整的学习机制，无法在生成新计划时充分利用过去的失败，导致执行效率低下和重复性错误。此外，虽然顺序规划方法对简单任务有效，但在更复杂的操作（如需要封装的动作或循环构造的任务）中效果不佳。此外，智能体在处理超出简单搜索空间的任务时遇到困难，这表明它在动态问题求解方面存在障碍。最后但同样重要的是，智能体需要大量的大型语言模型生成（即
    API 调用），因此需要大量时间和计算资源，影响了其效率。因此，未来的工作需要集中于提升智能体从过去不足中全面学习的能力，适应复杂任务，增强动态问题求解能力，并通过更高效的大型语言模型调用来优化时间和资源的利用。
- en: References
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, 等人. 2023. Gpt-4 技术报告。 *arXiv 预印本 arXiv:2303.08774*。
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2Web: Towards a Generalist Agent for
    the Web](http://arxiv.org/abs/2306.06070).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人（2023）Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi
    Wang, Huan Sun, 和 Yu Su. 2023. [Mind2Web：迈向一个通用型的Web智能体](http://arxiv.org/abs/2306.06070)。
- en: 'Driess et al. (2023) Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,
    Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong,
    Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth,
    Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
    Andy Zeng, Igor Mordatch, and Pete Florence. 2023. PaLM-E: An Embodied Multimodal
    Language Model. In *arXiv preprint arXiv:2303.03378*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Driess 等人（2023）Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha
    Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu,
    Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine,
    Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor
    Mordatch, 和 Pete Florence. 2023. PaLM-E: 一种具身多模态语言模型。载于 *arXiv 预印本 arXiv:2303.03378*。'
- en: 'Dror et al. (2023) Rotem Dror, Haoyu Wang, and Dan Roth. 2023. [Zero-Shot On-the-Fly
    Event Schema Induction](https://doi.org/10.18653/v1/2023.findings-eacl.53). In
    *Findings of the Association for Computational Linguistics: EACL 2023*, pages
    705–725, Dubrovnik, Croatia. Association for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dror 等人（2023）Rotem Dror, Haoyu Wang, 和 Dan Roth. 2023. [零-shot 动态事件模式引导](https://doi.org/10.18653/v1/2023.findings-eacl.53)。载于
    *《计算语言学协会会议成果：EACL 2023》*，第705–725页，克罗地亚杜布罗夫尼克，计算语言学协会。
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging Pre-trained Large Language Models to Construct
    and Utilize World Models for Model-based Task Planning](http://arxiv.org/abs/2305.14909).
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等人（2023）Lin Guan, Karthik Valmeekam, Sarath Sreedharan, 和 Subbarao Kambhampati.
    2023. [利用预训练的大型语言模型构建和利用世界模型进行基于模型的任务规划](http://arxiv.org/abs/2305.14909)。
- en: Gur et al. (2024) Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari,
    Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. 2024. [A Real-World WebAgent
    with Planning, Long Context Understanding, and Program Synthesis](https://openreview.net/forum?id=9JQtrumvg8).
    In *The Twelfth International Conference on Learning Representations*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gur 等人（2024）Izzeddin Gur, Hiroki Furuta, Austin V Huang, Mustafa Safdari, Yutaka
    Matsuo, Douglas Eck, 和 Aleksandra Faust. 2024. [一个具备规划、长时间上下文理解和程序合成的现实世界WebAgent](https://openreview.net/forum?id=9JQtrumvg8)。载于
    *《第十二届国际学习表征会议》*。
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with Language Model is Planning with World
    Model](https://doi.org/10.18653/v1/2023.emnlp-main.507). In *Proceedings of the
    2023 Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao等人（2023）Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang, 和
    Zhiting Hu. 2023. [与语言模型推理即是与世界模型规划](https://doi.org/10.18653/v1/2023.emnlp-main.507)。发表于*2023年自然语言处理经验方法会议论文集*，第8154–8173页，新加坡。计算语言学协会。
- en: 'Huang et al. (2022a) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
    Mordatch. 2022a. Language Models as Zero-Shot Planners: Extracting Actionable
    Knowledge for Embodied Agents. *arXiv preprint arXiv:2201.07207*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2022a）Wenlong Huang, Pieter Abbeel, Deepak Pathak, 和 Igor Mordatch.
    2022a. 语言模型作为零-shot规划者：为具身代理提取可操作的知识。*arXiv预印本 arXiv:2201.07207*。
- en: 'Huang et al. (2022b) Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre
    Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,
    and Brian Ichter. 2022b. Inner Monologue: Embodied Reasoning through Planning
    with Language Models. In *arXiv preprint arXiv:2207.05608*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2022b）Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete
    Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre
    Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman,
    和 Brian Ichter. 2022b. 内心独白：通过语言模型的规划进行具身推理。发表于*arXiv预印本 arXiv:2207.05608*。
- en: 'Li et al. (2023) Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, and Yang Li. 2023.
    [A Zero-Shot Language Agent for Computer Control with Structured Reflection](https://doi.org/10.18653/v1/2023.findings-emnlp.753).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    11261–11274, Singapore. Association for Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等人（2023）Tao Li, Gang Li, Zhiwei Deng, Bryan Wang, 和 Yang Li. 2023. [面向计算机控制的零-shot语言代理与结构化反思](https://doi.org/10.18653/v1/2023.findings-emnlp.753)。发表于*计算语言学协会：EMNLP
    2023年发现*，第11261–11274页，新加坡。计算语言学协会。
- en: Liu et al. (2018) Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi,
    and Percy Liang. 2018. Reinforcement learning on web interfaces using workflow-guided
    exploration. *arXiv preprint arXiv:1802.08802*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等人（2018）Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, 和 Percy
    Liang. 2018. 在网页接口上使用工作流引导的探索进行强化学习。*arXiv预印本 arXiv:1802.08802*。
- en: 'Liu et al. (2023) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023. AgentBench: Evaluating LLMs
    as Agents. *arXiv preprint arXiv: 2308.03688*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人（2023）Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai,
    Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan
    Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun,
    Minlie Huang, Yuxiao Dong, 和 Jie Tang. 2023. AgentBench: 评估大语言模型作为代理的能力。*arXiv预印本
    arXiv: 2308.03688*。'
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. 2023. [Self-Refine: Iterative Refinement with Self-Feedback](http://arxiv.org/abs/2303.17651).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan等人（2023）Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu
    Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    和 Peter Clark. 2023. [Self-Refine: 自反馈的迭代优化](http://arxiv.org/abs/2303.17651)。'
- en: Pan et al. (2024) Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey
    Levine, and Alane Suhr. 2024. [Autonomous Evaluation and Refinement of Digital
    Agents](http://arxiv.org/abs/2404.06474).
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等人（2024）Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine,
    和 Alane Suhr. 2024. [数字代理的自主评估与优化](http://arxiv.org/abs/2404.06474)。
- en: 'Prasad et al. (2023) Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter
    Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2023. ADaPT: As-Needed
    Decomposition and Planning with Language Models. *arXiv*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prasad等人（2023）Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark,
    Ashish Sabharwal, Mohit Bansal, 和 Tushar Khot. 2023. ADaPT: 按需分解与语言模型规划。*arXiv*。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language Agents with Verbal
    Reinforcement Learning](http://arxiv.org/abs/2303.11366).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn等人（2023）Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, 和 Shunyu Yao. 2023. [Reflexion: 具有语言强化学习的语言代理](http://arxiv.org/abs/2303.11366)。'
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768).
    In *Proceedings of the International Conference on Learning Representations (ICLR)*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar 等人（2021）Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan
    Bisk, Adam Trischler 和 Matthew Hausknecht. 2021. [ALFWorld: Aligning Text and
    Embodied Environments for Interactive Learning](https://arxiv.org/abs/2010.03768).
    收录于 *国际学习表征会议（ICLR）论文集*。'
- en: 'Song et al. (2023) Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler,
    Wei-Lun Chao, and Yu Su. 2023. LLM-Planner: Few-Shot Grounded Planning for Embodied
    Agents with Large Language Models. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人（2023）Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler,
    Wei-Lun Chao 和 Yu Su. 2023. LLM-Planner: 基于少样本的具身智能体规划方法，结合大型语言模型的实际应用. 收录于 *IEEE/CVF国际计算机视觉会议（ICCV）论文集*。'
- en: 'Song et al. (2024) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. 2024. Trial and Error: Exploration-Based Trajectory Optimization
    for LLM Agents. *arXiv preprint arXiv:2403.02502*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2024）Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li 和 Bill Yuchen
    Lin. 2024. 试错法：基于探索的轨迹优化方法用于LLM智能体. *arXiv 预印本 arXiv:2403.02502*。
- en: 'Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. 2023. [AdaPlanner: Adaptive Planning from Feedback with Language Models](http://arxiv.org/abs/2305.16653).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun 等人（2023）Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai 和 Chao Zhang.
    2023. [AdaPlanner: 基于反馈的语言模型自适应规划](http://arxiv.org/abs/2305.16653)。'
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An Open-Ended
    Embodied Agent with Large Language Models. *arXiv preprint arXiv: Arxiv-2305.16291*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023a）Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. 2023a. Voyager: 基于大型语言模型的开放式具身智能体.
    *arXiv 预印本 arXiv: Arxiv-2305.16291*。'
- en: Wang et al. (2023b) Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai
    Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b. [Large Language Models
    are not Fair Evaluators](http://arxiv.org/abs/2305.17926).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023b）Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai
    Lin, Yunbo Cao, Qi Liu, Tianyu Liu 和 Zhifang Sui. 2023b. [大型语言模型不是公平的评估者](http://arxiv.org/abs/2305.17926)。
- en: 'Wang et al. (2022) Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj
    Ammanabrolu. 2022. [ScienceWorld: Is your Agent Smarter than a 5th Grader?](https://doi.org/10.18653/v1/2022.emnlp-main.775)
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 11279–11298, Abu Dhabi, United Arab Emirates. Association for
    Computational Linguistics.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2022）Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté 和 Prithviraj Ammanabrolu.
    2022. [ScienceWorld: 你的智能体比五年级学生聪明吗？](https://doi.org/10.18653/v1/2022.emnlp-main.775)
    收录于 *2022年自然语言处理实证方法会议（EMNLP）论文集*，第11279–11298页，阿布扎比，阿联酋。计算语言学协会。'
- en: 'Wang et al. (2023c) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023c. [Describe, Explain, Plan and Select: Interactive Planning
    with LLMs Enables Open-World Multi-Task Agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023c）Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma
    和 Yitao Liang. 2023c. [描述、解释、规划与选择：与大型语言模型交互的开放世界多任务智能体](https://openreview.net/forum?id=KtvPdGb31Z).
    收录于 *第37届神经信息处理系统会议（NeurIPS）*。
- en: Wu et al. (2023) Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, and Haibin Yan.
    2023. Embodied Task Planning with Large Language Models. *arXiv preprint arXiv:2305.03716*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2023）Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu 和 Haibin Yan. 2023. 基于大型语言模型的具身任务规划.
    *arXiv 预印本 arXiv:2305.03716*。
- en: 'Xu et al. (2023) Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee,
    Yuchen Liu, and Dongkuan Xu. 2023. [ReWOO: Decoupling Reasoning from Observations
    for Efficient Augmented Language Models](http://arxiv.org/abs/2305.18323).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人（2023）Binfeng Xu, Zhiyuan Peng, Bowen Lei, Subhabrata Mukherjee, Yuchen
    Liu 和 Dongkuan Xu. 2023. [ReWOO: 从观察中解耦推理以提高增强型语言模型的效率](http://arxiv.org/abs/2305.18323)。'
- en: 'Yao et al. (preprint) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    preprint. WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents. In *ArXiv*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人（预印本）Shunyu Yao, Howard Chen, John Yang 和 Karthik Narasimhan. 预印本. WebShop:
    面向可扩展的真实世界网页交互的具身语言智能体. 收录于 *ArXiv*。'
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. [Tree of Thoughts: Deliberate
    Problem Solving with Large Language Models](https://openreview.net/forum?id=5Xc1ecxO1h).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等（2023a）顺宇姚、典宇、杰弗里·赵、伊扎克·沙弗兰、托马斯·L·格里菲斯、袁曹和卡尔蒂克·R·纳拉西曼。2023a年。[思维之树：使用大语言模型进行深思熟虑的问题解决](https://openreview.net/forum?id=5Xc1ecxO1h)。发表于*第37届神经信息处理系统大会*。
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. 2023b. ReAct: Synergizing Reasoning and Acting
    in Language Models. In *International Conference on Learning Representations (ICLR)*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等（2023b）顺宇姚、杰弗里·赵、典宇、南杜、伊扎克·沙弗兰、卡尔蒂克·纳拉西曼和袁曹。2023b年。ReAct：在语言模型中协同推理与行动。发表于*国际学习表征大会（ICLR）*。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena](https://openreview.net/forum?id=uccHPGDlao). In *Thirty-seventh
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2023）连敏郑、魏林蒋、英生、思源庄、张昊吴、永浩庄、子琳、卓瀚李、大成李、埃里克·邢、浩张、约瑟夫·E·冈萨雷斯和伊昂·斯托伊卡。2023年。[使用MT-Bench和Chatbot
    Arena评判大语言模型作为法官](https://openreview.net/forum?id=uccHPGDlao)。发表于*第37届神经信息处理系统大会数据集和基准测试分会*。
- en: Zhou et al. (2024a) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2024a. [Language Agent Tree Search Unifies Reasoning Acting
    and Planning in Language Models](https://openreview.net/forum?id=6LNTSrJjBe).
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2024a）安迪·周、凯·颜、米哈尔·斯拉彭托赫-罗斯曼、浩瀚·王和宇雄·王。2024a年。[语言代理树搜索统一了语言模型中的推理、行动与规划](https://openreview.net/forum?id=6LNTSrJjBe)。
- en: 'Zhou et al. (2024b) Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    and Graham Neubig. 2024b. [WebArena: A Realistic Web Environment for Building
    Autonomous Agents](https://openreview.net/forum?id=oKn9c6ytLx). In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等（2024b）舒言周、弗兰克·F·许、浩朱、徐辉周、罗伯特·洛、阿比谢克·斯里达尔、显义程、天岳欧、约纳坦·比斯克、丹尼尔·弗里德、尤里·阿隆和格雷厄姆·纽比格。2024b年。[WebArena：构建自主智能体的真实网络环境](https://openreview.net/forum?id=oKn9c6ytLx)。发表于*第十二届国际学习表征大会*。
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang,
    and Jifeng Dai. 2023. Ghost in the Minecraft: Generally Capable Agents for Open-World
    Environments via Large Language Models with Text-based Knowledge and Memory. *arXiv
    preprint arXiv:2305.17144*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等（2023）席周朱、云涛陈、浩田、晨欣陶、伟杰苏、晨宇杨、高黄、彬李、乐威卢、晓刚王、宇乔、赵翔张和季锋戴。2023年。在Minecraft中的幽灵：通过大语言模型与基于文本的知识和记忆为开放世界环境提供通用能力的智能体。*arXiv预印本arXiv:2305.17144*。
- en: 'Zhuang et al. (2024) Yuchen Zhuang, Xiang Chen, Tong Yu, Saayan Mitra, Victor
    Bursztyn, Ryan A. Rossi, Somdeb Sarkhel, and Chao Zhang. 2024. [ToolChain*: Efficient
    Action Space Navigation in Large Language Models with A* Search](https://openreview.net/forum?id=B6pQxqUcT8).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 庄等（2024）宇辰庄、翔陈、同余、萨亚恩·米特拉、维克多·伯尔斯滕、瑞安·A·罗西、索姆德布·萨尔凯尔和超张。2024年。[ToolChain*：通过A*搜索在大语言模型中高效导航动作空间](https://openreview.net/forum?id=B6pQxqUcT8)。发表于*第十二届国际学习表征大会*。
- en: Appendix
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Prompt for Plan Generation ($G_{\text{plan}}$)
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成计划的提示（$G_{\text{plan}}$）
- en: Imagine that you are imitating humans doing a task on a website step by step.
    You can click an element with the mouse, scroll up or down, go to a certain URL
    or go back to previous page, or type some text with the keyboard (e.g., click(),
    scroll(), goto(), go_back(), and type() functions in playwright). One step means
    one operation within any of the mentioned actions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你在模仿人类逐步完成网站上的任务。你可以点击页面元素、滚动页面、访问某个URL或返回上一页，或用键盘输入一些文本（例如，playwright中的click()、scroll()、goto()、go_back()和type()函数）。一步操作意味着进行上述任意一种操作。
- en: 'You are within a sandbox and only have access to the following websites to
    work with:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你当前处于一个沙盒中，仅能访问以下网站进行操作：
- en: •
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个在线购物网站（OneStopShop）：{webarena_root}:7770
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个电子商务管理网站（Magento）：{webarena_root}:7780/admin
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个Reddit网站（Postmill）：{webarena_root}:9999
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A GitLab website: {webarena_root}:8023'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 GitLab 网站：{webarena_root}:8023
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个地图网站（OpenStreetMap）：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 Wikipedia 网站：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)
- en: 'Notes:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记：
- en: '1.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: If you want to use the search function, you don’t need to click on the search
    bar. You can directly use “type [element_id] [things_to_type]”, and generally
    afterwards, you don’t need to click the search button (by default, the command
    contains an ENTER at the end).
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想使用搜索功能，你无需点击搜索框。你可以直接使用“type [element_id] [things_to_type]”，通常之后你无需点击搜索按钮（默认情况下，命令的末尾会包含一个回车）。
- en: '2.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: You can assume that you have signed in to your account (we have set up the cookies,
    so login is not needed).
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以假设你已经登录到你的账户（我们已经设置了 cookies，所以无需再次登录）。
- en: 'The website that you will be working with is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你将要使用的网站是：
- en: '{WEBSITE INTRO}'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '{WEBSITE INTRO}'
- en: 'Please follow these specific instructions to solve tasks:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照以下具体指示解决任务：
- en: '{INSTRUCTION}'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '{INSTRUCTION}'
- en: 'Here is a more detailed description of the starting screen:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是启动屏幕的更详细描述：
- en: '{STARTING SCREEN DESCRIPTION}'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '{STARTING SCREEN DESCRIPTION}'
- en: 'Now, based on the information above, what should be the steps to achieve the
    following goal (please give me a list of textual description of playwright actions,
    starting with ‘List’):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据以上信息，达成以下目标的步骤应是什么（请给我一份文本描述的剧本动作列表，开头使用“List”）：
- en: '{TASK}'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'For your reference, here are some experiences from previous failed trials (please
    consider the following information to generate a better plan):'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 供参考，以下是之前失败尝试中的一些经验（请考虑以下信息，以生成更好的计划）：
- en: '{FAILED PLAN}'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '{FAILED PLAN}'
- en: 'Past experience:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 过去的经验：
- en: '{HISTORY}'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: To be successful in generating a new plan, you need to provide a list (1, 2,
    3, …), in which each item is a natural language description of one playwright
    action that is necessary to complete the task (e.g., click on the ‘Account’ button;
    scroll down; use the search bar to search for iPhone 13). You should use the information
    from the past experiences to save unnecessary steps!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要成功生成新计划，你需要提供一个列表（1、2、3、……），其中每个项目都是完成任务所必需的一个戏剧性动作的自然语言描述（例如，点击“账户”按钮；向下滚动；使用搜索栏搜索
    iPhone 13）。你应该利用过去的经验来节省不必要的步骤！
- en: Prompt for Action Generation ($G_{\text{action}}$)
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作生成提示 ($G_{\text{action}}$)
- en: 'I am in a sandbox and only have access to the following websites (i.e., no
    access to external website like www.reddit.com):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我处于沙盒环境，只能访问以下网站（即，无法访问诸如 www.reddit.com 之类的外部网站）：
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An online shopping website (OneStopShop): {webarena_root}:7770'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个在线购物网站（OneStopShop）：{webarena_root}:7770
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An e-commerce management website (Magento): {webarena_root}:7780/admin'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个电子商务管理网站（Magento）：{webarena_root}:7780/admin
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Reddit website (Postmill): {webarena_root}:9999'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 Reddit 网站（Postmill）：{webarena_root}:9999
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A GitLab website: {webarena_root}:8023'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 GitLab 网站：{webarena_root}:8023
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A map website (OpenStreetMap): [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个地图网站（OpenStreetMap）：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:3000)
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A Wikipedia website: [http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 Wikipedia 网站：[http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing](http://ec2-3-131-244-37.us-east-2.compute.amazonaws.com:8888/wikipedia_en_all_maxi_2022-05/A/User:The_other_Kiwix_guy/Landing)
- en: Now I’m trying to complete a task on a website.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我正试图在一个网站上完成任务。
- en: 'The task is:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是：
- en: '{TASK}'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'The plan to complete this task is:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此任务的计划是：
- en: '{PLAN}'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}'
- en: 'I have executed the following actions:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我已执行以下操作：
- en: '{HISTORY}'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'And now I’m at this step: {STEP}'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我处于此步骤：{STEP}
- en: 'Here is the screen I am looking at:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我正在查看的屏幕：
- en: '{OBS}'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: 'I have taken down the following notes:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经记下了以下笔记：
- en: '{NOTES}'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: What should be the next action to complete this step in my plan (only give one
    action)?
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为完成我的计划中的这一步，下一步操作应该是什么（只给出一个操作）？
- en: 'Note:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to click, please indicate the element id in [] (format:
    click [element_id]).'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步是点击，请在 [] 中注明元素 ID（格式：click [element_id]）。
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to scroll, please indicate the direction in [] (format:
    scroll [up or down]).'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步是滚动，请在 [] 中注明方向（格式：scroll [up 或 down]）。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If you need to navigate to a URL, please indicate the URL in [] (format: goto
    [url]).'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你需要导航到一个网址，请在 [] 中注明网址（格式：goto [url]）。
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If you need to go back to the previous page, please use go_back.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你需要返回到上一页，请使用 go_back。
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If the next action is to type, please indicate both element id and the things
    to type in [] (format: type [element_id] [things to type]).'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果下一步是输入，请在 [] 中注明元素 ID 和要输入的内容（格式：type [element_id] [要输入的内容]）。
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'If you want to note down something, use this format: note_down [things to note
    down].'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你想记下某些内容，请使用以下格式：note_down [要记下的内容]。
- en: 'The next action is:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步操作是：
- en: Prompt for Objective Alignment ($G_{\text{align}}$)
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 目标对齐提示 ($G_{\text{align}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在一步步模仿人类在网站上执行任务。
- en: 'You are currently working on this step:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在正在处理这个步骤：
- en: '{STEP}.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '{STEP}。'
- en: 'The step above is one of the steps in the following plan:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的步骤是以下计划中的一部分：
- en: '{PLAN}.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}。'
- en: From Screen 1, you executed an action and then arrived at Screen 2.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 从屏幕 1 开始，你执行了一个操作然后到了屏幕 2。
- en: 'The action you executed was:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你执行的操作是：
- en: '{ACTION}.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '{ACTION}。'
- en: 'Screen 1:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕 1：
- en: '{OBS1}.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS1}。'
- en: 'Screen 2:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕 2：
- en: '{OBS2}.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS2}。'
- en: Now describe what this action is about in one sentence, starting with ‘The action
    is to’.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用一句话描述这个操作的内容，格式为“这个操作是...”
- en: Does this action align with the goal of the following step (i.e., are we moving
    towards the right direction; Answer YES or NO)?
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作是否与以下步骤的目标一致（即，我们是否朝着正确的方向前进；回答 YES 或 NO）？
- en: '{STEP}'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '{STEP}'
- en: Prompt for Task / Subtask Completion Evaluation ($G_{\text{completed}}$)
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任务/子任务完成评估提示 ($G_{\text{completed}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在一步步模仿人类在网站上执行任务。
- en: 'You are asked to solve the following task:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求解决以下任务：
- en: '{TASK}'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'You made the following plan to solve it:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你为解决该任务做出了以下计划：
- en: '{PLAN}'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '{PLAN}'
- en: 'To reach the current screen, you have previously executed the following actions:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了到达当前屏幕，你之前执行了以下操作：
- en: '{HISTORY}'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'You have taken down a few notes after each action as follows:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 你在每个操作后记录了以下笔记：
- en: '{NOTES}'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你正在查看的当前屏幕的可访问性树：
- en: '{OBS}'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: Look at the screen, the task, and the actions you executed, and think thoroughly,
    is the task completed now?
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下屏幕、任务和你执行的操作，仔细思考，任务现在完成了吗？
- en: If the task is completed, answer YES.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务已完成，请回答 YES。
- en: If the task is not yet completed (meaning further actions are yet to be executed),
    answer NO.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务尚未完成（意味着还有后续操作需要执行），请回答 NO。
- en: Prompt for Answer Delivery ($G_{\text{answer}}$)
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 答案交付提示 ($G_{\text{answer}}$)
- en: Imagine that you are imitating humans doing a task on a website step by step.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你正在一步步模仿人类在网站上执行任务。
- en: 'You are asked to solve the following task:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你被要求解决以下任务：
- en: '{TASK}'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '{TASK}'
- en: 'To reach the current screen, you have previously executed the following actions:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了到达当前屏幕，你之前执行了以下操作：
- en: '{HISTORY}'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '{HISTORY}'
- en: 'You have taken down the following notes (to help you answer the question eventually)
    after each action:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个操作后，你记录了以下笔记（帮助你最终回答问题）：
- en: '{NOTES}'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '{NOTES}'
- en: 'And here is the accessibility tree of the current screen you are looking at:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你正在查看的当前屏幕的可访问性树：
- en: '{OBS}'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS}'
- en: 'Based on the above information, answer the question in the task (starting with
    ###Answer).'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '根据上述信息，回答任务中的问题（以 ###Answer 开头）。'
- en: Prompt for Element Mapping ($G_{\text{map}}$)
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 元素映射提示 ($G_{\text{map}}$)
- en: 'I want to interact with an element with element id: {element_id} in the following
    screen:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我想在接下来的屏幕上与元素 ID 为 {element_id} 的元素互动：
- en: '{OBS1}'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS1}'
- en: Now if I want to click on the same element in the following screen, what should
    be the element id now?
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我想在接下来的屏幕上点击相同的元素，元素 ID 应该是什么？
- en: '{OBS2}'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '{OBS2}'
- en: 'New element id is:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 新的元素 ID 是：
