- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:38:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:38:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Human-Centered LLM-Agent User Interface: A Position Paper'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以人为本的LLM代理用户界面：立场论文
- en: 来源：[https://arxiv.org/html/2405.13050/](https://arxiv.org/html/2405.13050/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.13050/](https://arxiv.org/html/2405.13050/)
- en: '¹¹institutetext: New York University Shanghai ¹¹email: {daniel.chin , yw5343}@nyu.edu
    ²²institutetext: Mohamed bin Zayed University of Artificial Intelligence ²²email:
    gus.xia@mbzuai.ac.ae ³³institutetext: New York University Tandon School of EngineeringDaniel
    Chin 1122 [0000-0002-3406-5318](https://orcid.org/0000-0002-3406-5318 "ORCID identifier")
       Yuxuan Wang 1133 [0009-0002-9161-6961](https://orcid.org/0009-0002-9161-6961
    "ORCID identifier")    Gus Xia 22'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构文本：纽约大学上海分校 ¹¹邮箱：{daniel.chin , yw5343}@nyu.edu ²²机构文本：穆罕默德·本·扎耶德人工智能大学
    ²²邮箱：gus.xia@mbzuai.ac.ae ³³机构文本：纽约大学坦登工程学院 Daniel Chin 1122 [0000-0002-3406-5318](https://orcid.org/0000-0002-3406-5318
    "ORCID标识符")    Yuxuan Wang 1133 [0009-0002-9161-6961](https://orcid.org/0009-0002-9161-6961
    "ORCID标识符")    Gus Xia 22
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Model (LLM) -in-the-loop applications have been shown to effectively
    interpret the human user’s commands, make plans, and operate external tools/systems
    accordingly. Still, the operation scope of the LLM agent is limited to passively
    following the user, requiring the user to frame his/her needs with regard to the
    underlying tools/systems. We note that the potential of an LLM-Agent User Interface
    (LAUI) is much greater. A user mostly ignorant to the underlying tools/systems
    should be able to work with a LAUI to discover an emergent workflow. Contrary
    to the conventional way of designing an explorable GUI to teach the user a predefined
    set of ways to use the system, in the ideal LAUI, the LLM agent is initialized
    to be proficient with the system, proactively studies the user and his/her needs,
    and proposes new interaction schemes to the user. To illustrate LAUI, we present
    Flute X GPT, a concrete example using an LLM agent, a prompt manager, and a flute-tutoring
    multi-modal software-hardware system to facilitate the complex, real-time user
    experience of learning to play the flute.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）在循环中的应用已被证明能够有效地解读人类用户的命令，制定计划，并相应地操作外部工具/系统。然而，LLM代理的操作范围仅限于被动地跟随用户，需要用户根据底层工具/系统来界定自己的需求。我们认为，LLM代理用户界面（LAUI）的潜力要大得多。一个对底层工具/系统大多不了解的用户，应该能够通过LAUI发现一种新兴的工作流程。与传统设计可探索的图形用户界面（GUI）以教会用户一组预定义的系统使用方法的方式相反，在理想的LAUI中，LLM代理被初始化为精通系统，主动研究用户及其需求，并向用户提出新的交互方案。为了说明LAUI，我们展示了Flute
    X GPT，这是一个具体的例子，利用LLM代理、提示管理器和长笛辅导的多模态软件硬件系统，促进了学习吹奏长笛这一复杂、实时的用户体验。
- en: 'Keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: llm agent user interface LLM-in-the-loop human-computer interaction.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理用户界面 LLM循环中的人机交互。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large Language Models (LLMs) can be used to connect an underlying system with
    a user via the natural language medium, forming an LLM-powered application as
    shown in Figure [1](https://arxiv.org/html/2405.13050v2#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Human-Centered LLM-Agent User Interface: A Position Paper"). The LLM is embedded
    in an LLM-in-the-loop state machine to acquire multi-modal input/output capabilities,
    emulate logical reasoning and planning, and use tools or operate a system [[17](https://arxiv.org/html/2405.13050v2#bib.bib17),
    [9](https://arxiv.org/html/2405.13050v2#bib.bib9), [8](https://arxiv.org/html/2405.13050v2#bib.bib8),
    [19](https://arxiv.org/html/2405.13050v2#bib.bib19)]. Consequently, the user is
    able to indirectly but effectively use the underlying system via chatting with
    an LLM assistant.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以通过自然语言媒介将底层系统与用户连接，从而形成一个LLM驱动的应用，如图[1](https://arxiv.org/html/2405.13050v2#S1.F1
    "图 1 ‣ 1 引言 ‣ 以人为本的LLM代理用户界面：立场论文")所示。LLM被嵌入到LLM循环状态机中，以获取多模态输入/输出能力，模拟逻辑推理和规划，并使用工具或操作系统
    [[17](https://arxiv.org/html/2405.13050v2#bib.bib17), [9](https://arxiv.org/html/2405.13050v2#bib.bib9),
    [8](https://arxiv.org/html/2405.13050v2#bib.bib8), [19](https://arxiv.org/html/2405.13050v2#bib.bib19)]。因此，用户能够通过与LLM助手聊天，间接但有效地使用底层系统。
- en: However, the current applications hardly address the user-interaction potential
    stemming from the multi-round chatting setup. Although the user can refer to the
    chat history, the LLM assistant rarely challenges the user or even asks to clarify
    the user’s intention. Instead, the LLM closely follows the user’s command, missing
    the golden opportunity to improve the user’s usage scheme and understanding of
    the system/tools. That inefficacy becomes jarringly apparent when one tries to
    design from scratch a new complex system with an LLM interface.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，目前的应用几乎没有充分发挥多轮聊天设置所带来的用户交互潜力。尽管用户可以查看聊天历史记录，但LLM助手很少挑战用户，甚至很少询问是否需要澄清用户的意图。相反，LLM会紧跟用户的指令，错失了提升用户使用方案和对系统/工具理解的黄金机会。当人们尝试从头设计一个全新的复杂系统并结合LLM接口时，这种低效性显得尤为明显。
- en: We posit that the operation scope of an LLM-Agent User Interface (LAUI) is much
    wider than that. The interface should be more than an assistant or a butler, but
    instead a secretary, actively working with the user to discover emergent interaction
    schemes on the fly. LAUI should be proficient with the underlying system, study
    the user, study the user’s needs (instead of commands), reason on its own, and
    propose tailored interaction schemes to the user, including what modes of feedback
    are provided and what input is expected from the user. In the conventional way
    of interaction, including GUI and current LLM-powered applications [[10](https://arxiv.org/html/2405.13050v2#bib.bib10)],
    the designer has to imagine possible usage workflows given the system capabilities
    at design time, and the user is expected to learn the system (via tutorials, explorations,
    and practicing) in order to come up with a workflow for each task. In contrast,
    with LAUI, the user only needs to describe his/her needs and doesn’t need to deeply
    understand the application, and a workflow will naturally emerge as the LLM agent
    works with the user. We call for more research exploring the potential of LAUI.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为LLM-代理用户接口（LAUI）的操作范围远不止于此。这个接口不应仅仅是一个助手或管家，而应当是一个秘书，积极地与用户一起发现实时的交互方案。LAUI应该精通底层系统，研究用户，研究用户的需求（而非命令），进行自主推理，并为用户提出量身定制的交互方案，包括提供哪些反馈模式以及用户需要提供哪些输入。在传统的交互方式中，包括GUI和当前基于LLM的应用[[10](https://arxiv.org/html/2405.13050v2#bib.bib10)]，设计师必须在设计阶段根据系统的能力设想可能的使用工作流程，而用户则需要通过教程、探索和练习来学习系统，以便为每个任务制定工作流程。与此不同，使用LAUI时，用户只需要描述自己的需求，而不需要深入了解应用程序，工作流程将自然地随着LLM代理与用户的协作而产生。我们呼吁更多的研究探索LAUI的潜力。
- en: '![Refer to caption](img/2150a3b0606e962a8886bbaaec6e5b9c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2150a3b0606e962a8886bbaaec6e5b9c.png)'
- en: 'Figure 1: The LLM agent serves as the interface between the underlying system
    and the user. The LLM agent together with the system forms the application. Direct
    communications between the user and the system is available and configured by
    the LLM agent.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLM代理作为底层系统和用户之间的接口。LLM代理与系统共同构成应用。用户与系统之间的直接通信由LLM代理配置并提供。
- en: As a concrete illustration of LAUI, we present Flute X GPT — an LLM-in-the-loop
    music-tutoring application consisting of an LLM agent, a prompt manager, a software
    system, and hardware. The application provides real-time haptic guidance via servo
    motors, real-time visual music-symbol feedback, real-time audio feedback, and
    natural language chat, all controlled by the LLM agent. This is the first time
    an LLM-powered interface is applied to a working system of such complexity and
    real-time user interactivity.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为LAUI的具体示例，我们介绍Flute X GPT——一个LLM环节的音乐辅导应用，包含LLM代理、提示管理器、软件系统和硬件。该应用通过伺服电机提供实时触觉指导、实时视觉音乐符号反馈、实时音频反馈和自然语言聊天，所有这些都由LLM代理控制。这是第一次将LLM驱动的接口应用到如此复杂且具有实时用户交互性的工作系统中。
- en: 'We first describe Flute X GPT in Section [2](https://arxiv.org/html/2405.13050v2#S2
    "2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper"),
    illustrating what a specific LAUI can look like, and then go on to formulate the
    general LAUI in Section [3](https://arxiv.org/html/2405.13050v2#S3 "3 LLM-Agent
    User Interface ‣ Human-Centered LLM-Agent User Interface: A Position Paper").'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在第[2](https://arxiv.org/html/2405.13050v2#S2 "2 Flute X GPT ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper")节描述Flute X GPT，展示了一个具体的LAUI可能是什么样的，然后在第[3](https://arxiv.org/html/2405.13050v2#S3
    "3 LLM-Agent User Interface ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper")节中制定了通用的LAUI。'
- en: 2 Flute X GPT
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Flute X GPT
- en: We describe Flute X GPT¹¹1The source code is open to public at [https://github.com/Daniel-Chin/Flute-X-GPT](https://github.com/Daniel-Chin/Flute-X-GPT),
    a music-tutoring application using LAUI. The human user works with Flute X GPT
    in workshop episodes, practicing to play the flute and learning music. The application
    gives real-time multi-modal feedback, including haptic feedback that applies force
    to the user’s fingers, visual feedback displaying performance errors, audio feedback
    rendering the music, and natural-language speech given by a robot music teacher.
    The underlying software-hardware system has numerous different configurations,
    each leading to a different interaction workflow. Each setting (e.g., toggle certain
    feedback, conditions for triggering feedback) can be controlled independently,
    so the number of configurations grow exponentially with the number of settings.
    For the user, it is unrealistic to first master the complex system before using
    it. Even for the designer, the space of possible interaction schemes is impossible
    to enumerate during design time. The LLM agent steps in to bridge that gap. Via
    prompting, we instruct the LLM agent to be proficient with all the raw capabilities
    of the system. During use time, the LLM agent converses with the user to clarify
    what interaction workflow will benefit the user’s music learning goal the most.
    The LLM agent studies the user’s preferences and diagnoses the musical challenges
    the user is facing. When configuring the underlying system, the LLM agent uses
    its pretrained common sense to reason about the implied consequences of the system
    configurations. Certain mixtures of settings have never been previously considered
    by any human designer, but can still emerge during use time.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们描述了 Flute X GPT¹¹1 该源代码对公众开放，地址为 [https://github.com/Daniel-Chin/Flute-X-GPT](https://github.com/Daniel-Chin/Flute-X-GPT)，这是一个使用
    LAUI 的音乐辅导应用。用户与 Flute X GPT 在工作坊中互动，练习吹奏长笛并学习音乐。该应用提供实时多模态反馈，包括对用户手指施加力的触觉反馈、显示表演错误的视觉反馈、呈现音乐的音频反馈以及机器人音乐老师提供的自然语言语音反馈。基础的软件硬件系统具有多种不同的配置，每种配置都导致不同的互动工作流。每个设置（例如，切换特定的反馈、触发反馈的条件）都可以独立控制，因此配置的数量随着设置数量的增加而呈指数增长。对于用户来说，在使用之前先掌握复杂的系统是不现实的。即便是设计师，在设计时也无法列举出所有可能的互动方案。LLM代理介入弥补了这一差距。通过提示，我们指导LLM代理熟悉系统的所有原始功能。在使用时，LLM代理与用户对话，澄清什么样的互动工作流最有利于用户的音乐学习目标。LLM代理研究用户的偏好，并诊断用户面临的音乐挑战。在配置基础系统时，LLM代理利用其预训练的常识推理系统配置的隐含后果。某些设置的组合从未被任何人类设计师考虑过，但仍可能在使用过程中出现。
- en: '![Refer to caption](img/380d2ca8b71b87e75eede7a8acfdff67.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/380d2ca8b71b87e75eede7a8acfdff67.png)'
- en: (a) From the scripted trial.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: （a）来自脚本试验。
- en: '![Refer to caption](img/0561f73b7646db50251b568a8d1954ac.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明文字](img/0561f73b7646db50251b568a8d1954ac.png)'
- en: (b) From improvised trial 1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: （b）来自即兴试验1。
- en: 'Figure 2: Interaction excerpts from the video demos.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：视频演示中的交互摘录。
- en: 'Overall, Flute X GPT entails three novel contributions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，Flute X GPT 包括三个创新贡献：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An illustration of LAUI. The LLM agent not only follows the user’s commands,
    but also proactively clarifies the user’s needs, deduces better interaction workflows,
    and advises the user.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LAUI的示意图。LLM代理不仅执行用户的命令，还主动澄清用户的需求，推导更好的互动工作流，并向用户提供建议。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM agent controls real-time system. The LLM agent directs a real-time interaction
    that is music training. The LLM agent is aware of time passage and decides when
    to wait for further event notifications and when to interrupt the user.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM代理控制实时系统。LLM代理引导一个实时的音乐训练互动。LLM代理意识到时间的流逝，并决定何时等待进一步的事件通知，何时中断用户。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM agent operates complex, stateful, multi-modal, user-interactive system.
    The LLM agent operates a highly complex software-hardware system by understanding
    how the user can benefit from the application and considering what combination
    of settings will lead to what interactive effects for the user.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM代理操作复杂的、有状态的、多模态的、用户互动的系统。LLM代理通过理解用户如何从应用中获益，并考虑不同设置组合对用户的互动效果，来操作一个高度复杂的软件硬件系统。
- en: 2.1 User Experience
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 用户体验
- en: 'Table 1: Functions that the LLM agent can call to control Music X Machine,
    the underlying music-tutoring system. The description addresses the LLM agent.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLM代理可以调用的功能，用于控制基础的音乐辅导系统 Music X Machine。描述面向 LLM 代理。
- en: '| Function | Parameters | Description |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 功能 | 参数 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Wait |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| Wait |  |'
- en: '&#124; Do nothing and wait for further stimuli, e.g. student speaking/playing
    music. &#124;'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 什么也不做，等待进一步的刺激，例如学生说话/演奏音乐。&#124;'
- en: '|'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| StartSession |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| StartSession |  |'
- en: '&#124; Start a Practice Session on Music X Machine. Do not call this function
    &#124;'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在 Music X Machine 上开始一个练习会话。不要调用此功能 &#124;'
- en: '&#124; unless you have already set all the modes. &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 除非你已经设置了所有的模式。&#124;'
- en: '|'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| InterruptSession |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| InterruptSession |  |'
- en: '&#124; Immediately end the Practice Session on Music X Machine. Call when the
    &#124;'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 立即结束 Music X Machine 上的练习会话。请在 &#124;'
- en: '&#124; student is having trouble or has started speaking in the middle of a
    Session. &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如果学生在会话中途有困难或开始说话。&#124;'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SetHapticMode | mode |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| SetHapticMode | 模式 |'
- en: '&#124; Set the haptic mode of Music X Machine. &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设置 Music X Machine 的触觉模式。&#124;'
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ToggleVisual | state |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| ToggleVisual | 状态 |'
- en: '&#124; Set the visual KR feedback to be on or off. &#124;'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设置视觉 KR 反馈为开启或关闭。&#124;'
- en: '|'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| PlayReference |  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| PlayReference |  |'
- en: '&#124; Play the ground-truth audio of the current segment. &#124;'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 播放当前片段的真实音频。&#124;'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| LoadSong | song_title |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| LoadSong | 歌曲标题 |'
- en: '&#124; Load a song into Music X Machine, and automatically select the entire
    &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将一首歌曲加载到 Music X Machine 中，并自动选择整个 &#124;'
- en: '&#124; song as the current segment. It doesn’t start a Practice Session by
    itself. &#124;'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 将歌曲作为当前片段。这不会自动开始练习会话。&#124;'
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SelectSegment | begin, end |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SelectSegment | 开始，结束 |'
- en: '&#124; Select a temporal segment of the song. &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 选择歌曲的时间段。 &#124;'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ModifyTempo | tempo_multiplier |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ModifyTempo | 节奏倍增器 |'
- en: '&#124; Modify the tempo of the song. &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修改歌曲的节奏。&#124;'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The intended user experience. The user has no prior knowledge about the flute
    tutoring system. The only assumption about the user is that the user wants to
    learn the flute. It is the LLM agent’s job to adapt to the user’s current flute
    playing capability, other musical skills, demographics, vocabulary, patience,
    style of learning, etc. During the music learning workshop, the LLM agent wears
    the face of a robot music teacher to chat with the user. For example, the robot
    teacher asks the user to put on a pair of haptic gloves, and explains that they
    provide force feedback at each finger. The workshop then alternates between practice
    sessions where the user plays a segment of a song under real-time guidance and
    verbal dialogues between the user and the robot teacher. The user gradually experiences
    more and more interaction schemes that trains musicality in different ways, and
    expresses their ideas about using the application and music learning in general.
    The LLM agent uses that opportunity to study the user and steers the workshop
    accordingly, aiming to maximize music education effect. The user learns to treat
    the robot teacher as a considerate and professional music tutoring agent capable
    of thinking multiple steps ahead, formulating plans with the user, and explaining
    the plans as well as music knowledge and education principles to the user.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的用户体验。用户对长笛辅导系统没有任何先前的了解。唯一的假设是用户想学习长笛。LLM 代理的工作是根据用户当前的长笛演奏能力、其他音乐技能、人口统计信息、词汇量、耐心、学习风格等适应用户。在音乐学习工作坊中，LLM
    代理扮演机器人音乐老师与用户进行对话。例如，机器人老师会要求用户戴上一副触觉手套，并解释这些手套能在每个手指上提供力反馈。工作坊随后在练习环节和用户与机器人老师之间的对话中交替进行，用户在实时指导下演奏歌曲片段，并逐渐体验到越来越多的互动方式，这些方式以不同的方式培养音乐素养，并表达他们对应用程序和音乐学习的看法。LLM
    代理利用这一机会研究用户并相应调整工作坊的方向，旨在最大化音乐教育效果。用户学会将机器人老师视为一个体贴且专业的音乐辅导代理，能够提前思考多个步骤，和用户一起制定计划，并向用户解释这些计划以及音乐知识和教育原则。
- en: 'We present three video demos of real-human user tests, including one scripted
    trial and two improvised trials.²²2Demo playlist of three videos: [https://www.youtube.com/playlist?list=PLNb0mNThMXbkBPL_Rjtmhx2daxuB6GFLs](https://www.youtube.com/playlist?list=PLNb0mNThMXbkBPL_Rjtmhx2daxuB6GFLs)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了三段真人用户测试的视频演示，包括一段脚本化的测试和两段即兴测试。²²2Demo 视频播放列表： [https://www.youtube.com/playlist?list=PLNb0mNThMXbkBPL_Rjtmhx2daxuB6GFLs](https://www.youtube.com/playlist?list=PLNb0mNThMXbkBPL_Rjtmhx2daxuB6GFLs)
- en: 'Video demo: Scripted trial. In this demo, all actors follow a script generated
    by an offline but faithfully emulated interaction between a user and the LLM agent.
    To re-emphasize, the LLM agent’s lines in the script are not written by humans,
    but are outputted by the LLM agent. Scripting removes the latency of LLM autoregressive
    generation. To ensure the script is concise and demonstrates a wide range of behaviors,
    we intervene with the script generation process. Every turn, we select one response
    out of 4 to 16 candidates sampled by the LLM. Seldomly, we add a temporary user-role
    prompt to give a short hint to the LLM agent, or edit the generated response.
    All the above interventions are kept to the minimum to ensure the vast majority
    of the agent speech is the authentic output of the LLM. See Figure [2(a)](https://arxiv.org/html/2405.13050v2#S2.F2.sf1
    "In Figure 2 ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper") for an excerpt from the video.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 视频演示：脚本化试验。在这个演示中，所有演员都遵循一个由线下但忠实模拟的用户与LLM代理之间的交互生成的脚本。为了再次强调，脚本中的LLM代理台词并非由人类编写，而是由LLM代理输出的。脚本化消除了LLM自回归生成的延迟。为了确保脚本简洁并展示广泛的行为，我们在脚本生成过程中进行干预。在每一轮中，我们从LLM采样的4到16个候选答案中选择一个。很少情况下，我们会添加一个临时的用户角色提示，给LLM代理一个简短的提示，或编辑生成的响应。所有上述干预都保持最小化，以确保绝大多数代理的发言是LLM的真实输出。请参见图[2(a)](https://arxiv.org/html/2405.13050v2#S2.F2.sf1
    "在图2 ‣ 2 长笛 X GPT ‣ 以人为本的LLM代理用户界面：立场论文")中的视频摘录。
- en: 'Video demo: Two improvised trials. Flute X GPT is set loose to freely interact
    with the user. The user is played by a developer pretending to be ignorant to
    the system. Figure [2(b)](https://arxiv.org/html/2405.13050v2#S2.F2.sf2 "In Figure
    2 ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper")
    shows an excerpt where the developer is surprised by Flute X GPT noticing that
    he was not playing the rest notes according to the score, a behavior never considered
    during design time.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 视频演示：两次即兴试验。Flute X GPT被放开，自由地与用户互动。用户由一名开发者扮演，假装对系统不懂。图[2(b)](https://arxiv.org/html/2405.13050v2#S2.F2.sf2
    "在图2 ‣ 2 长笛 X GPT ‣ 以人为本的LLM代理用户界面：立场论文")展示了一段摘录，开发者对Flute X GPT注意到他没有按照乐谱演奏剩余音符感到惊讶，这种行为在设计时从未考虑过。
- en: 2.2 System Capabilities
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 系统功能
- en: This subsection lists the capabilities of the music-tutoring system, Music X
    Machine, that underlies the LLM agent.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节列出了支撑LLM代理的音乐辅导系统Music X Machine的功能。
- en: Haptic guidance. The hardware includes a pair of gloves with servo motors and
    movable finger rings. Through these gloves, the system moves the user’s fingers
    to help with performance motions. The haptic guidance can be configured via various
    settings (e.g., is the guidance sustained throughout each note or applied at each
    note onset? Apply guidance for each note? For incorrectly played notes? For unplayed
    notes?). Certain combinations of settings form configuration presets whose interaction
    scheme is deemed meaningful by the designer. For example, in the Force Mode preset,
    every note triggers a full-force guidance for each finger. In the Adaptive Mode
    preset, the user plays the song on his/her own, and the gloves correct the mistakes.
    The haptic configuration should be tuned to adapt to the user’s skill level and
    the song’s difficulty [[30](https://arxiv.org/html/2405.13050v2#bib.bib30)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 触觉引导。硬件包括一副带有伺服电机和可移动指环的手套。通过这些手套，系统可以移动用户的手指，帮助完成演奏动作。触觉引导可以通过多种设置进行配置（例如，指导是贯穿每个音符持续进行，还是在每个音符的开始时进行？是否对每个音符都应用引导？对演奏错误的音符进行引导？对未演奏的音符进行引导？）。某些设置组合形成了配置预设，其交互方案被设计师认为是有意义的。例如，在“强制模式”预设下，每个音符都会对每个手指进行全力引导；在“自适应模式”预设下，用户自行演奏歌曲，手套会纠正错误。触觉配置应调整以适应用户的技能水平和歌曲的难度[[30](https://arxiv.org/html/2405.13050v2#bib.bib30)]。
- en: Visual feedback. A monitor displays the score of the current song. The user-played
    notes are displayed on top of the notes in the score, yielding the real-time visual
    Knowledge-of-Result (KR) feedback. It is a visual cue for the user to know where
    he/she is in terms of the pitch and helps the user internalize the score notations
    [[4](https://arxiv.org/html/2405.13050v2#bib.bib4)]. It can be toggled on or off.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉反馈。显示器展示当前歌曲的得分。用户演奏的音符会显示在乐谱上的音符之上，从而提供实时的结果知识反馈（KR）。这为用户提供了一个视觉提示，帮助其了解在音高方面的位置，并帮助用户内化乐谱符号[[4](https://arxiv.org/html/2405.13050v2#bib.bib4)]。该功能可以切换开启或关闭。
- en: 'Audio feedback. There are three streams of audio: synthesized user-played flute
    sounds, teacher-played reference performance audio, and metronomes. The three
    streams are mixed down and outputted.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 音频反馈。音频有三条流：合成的用户演奏的长笛声音、教师演奏的参考表演音频和节拍器音频。这三条音频流被混合并输出。
- en: Sensor-augmented flute. The flute measures real-time information including finger
    positions and breath pressure.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器增强的长笛。长笛测量实时信息，包括手指位置和气息压力。
- en: '![Refer to caption](img/a3cc458cb2f53777ee9439d4edf5ccc6.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/a3cc458cb2f53777ee9439d4edf5ccc6.png)'
- en: (a) Simplified.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 简化版。
- en: '![Refer to caption](img/f5f208e5465e1c351a72555f9684dcbb.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/f5f208e5465e1c351a72555f9684dcbb.png)'
- en: (b) Full.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 完整版。
- en: 'Figure 3: Flute X GPT with LLM in the loop. Music X Machine is the underlying
    software-hardware system providing multi-modal interaction with the user. The
    robot chats with the user and plays the piano according to MIDI control. The rule-based
    manager plays the agent that chats with the LLM, relaying external events to the
    LLM and resolving responses from the LLM.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：带LLM的长笛 X GPT。音乐 X 机器是提供与用户进行多模态交互的基础软件硬件系统。机器人与用户聊天并根据MIDI控制演奏钢琴。基于规则的管理器播放与LLM聊天的代理，传递外部事件到LLM并解析LLM的响应。
- en: 'Tempo mode. There are two options: either the system sets a steady tempo and
    the user follows the system, or the user plays on his/her own and the system follows
    the user’s tempo. The latter allows the user to think between the notes, potentially
    indefinitely, without triggering haptic feedback.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 节奏模式。有两种选择：要么系统设置一个稳定的节奏，用户跟随系统演奏，要么用户自行演奏，系统跟随用户的节奏。后一种模式允许用户在音符之间思考，可能是无限制的，而不会触发触觉反馈。
- en: Mistake classification. An algorithm judges the timing of each note into on_time,
    early, or late, and judges the pitch of each note into correct, octave_wrong,
    or unrelated. Classification results are visualized.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 错误分类。一个算法根据每个音符的时机将其分类为准时、过早或过迟，并根据每个音符的音高将其分类为正确、八度错误或无关。分类结果将被可视化。
- en: Song database. The practicing music materials that the system uses is processed
    from the POP909 dataset [[21](https://arxiv.org/html/2405.13050v2#bib.bib21),
    [3](https://arxiv.org/html/2405.13050v2#bib.bib3)]. It contains pieces and sections
    of monophonic melody lines from pop songs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 歌曲数据库。系统使用的练习音乐素材来自POP909数据集[[21](https://arxiv.org/html/2405.13050v2#bib.bib21),
    [3](https://arxiv.org/html/2405.13050v2#bib.bib3)]。它包含了来自流行歌曲的单声部旋律片段和小节。
- en: See [[3](https://arxiv.org/html/2405.13050v2#bib.bib3)] for a detailed description
    of the system setup of Music X Machine.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 详情请参见[[3](https://arxiv.org/html/2405.13050v2#bib.bib3)]，了解音乐 X 机器系统的详细描述。
- en: 2.3 High-Dimensional Configuration of Interaction
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 交互的高维配置
- en: The capabilities listed in the previous subsection entangle with one another,
    implying dynamic consequences in terms of learning experience. Their configurations
    multiply into a big Cartesian product, making the overall configuration of the
    entire system high-dimensional. Configuring the system effectively requires 1)
    proficiency with the system, 2) understanding the user’s needs, 3) pedagogical
    expertise, 4) music knowledge, and 5) using common-sense reasoning to “imagine”
    the multi-modal real-time interaction. We employ an LLM agent to solve all five.
    To optimize the interaction workflow and learning results for the user, the LLM
    agent can not only select a suitable preset, but also create new ones unforeseen
    by the designers, tailored to specific use-time scenarios.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节列出的功能相互交织，意味着学习体验方面的动态后果。它们的配置交织成一个大的笛卡尔积，使得整个系统的配置具有高维性。有效配置系统需要：1) 熟练掌握系统，2)
    了解用户需求，3) 教学专业知识，4) 音乐知识，以及5) 使用常识推理“想象”多模态实时交互。我们使用LLM代理来解决这五个问题。为了优化用户的交互工作流和学习效果，LLM代理不仅可以选择合适的预设，还可以根据特定使用场景创建新的、设计者未曾预见的预设。
- en: 'To illustrate this high-dimensional interface that the system exposes to the
    LLM agent, Table [1](https://arxiv.org/html/2405.13050v2#S2.T1 "Table 1 ‣ 2.1
    User Experience ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper") shows the functions that the LLM agent can call.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明系统向LLM代理展示的这个高维界面，表[1](https://arxiv.org/html/2405.13050v2#S2.T1 "表 1 ‣
    2.1 用户体验 ‣ 2 长笛 X GPT ‣ 以人为本的LLM代理用户界面：立场论文")展示了LLM代理可以调用的功能。
- en: 2.4 LLM in the Loop
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 LLM在回路中
- en: 'Figure [3](https://arxiv.org/html/2405.13050v2#S2.F3 "Figure 3 ‣ 2.2 System
    Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper") illustrates the inner workings of the LLM-in-the-loop application, Flute
    X GPT. The LLM we use is GPT-4 [[1](https://arxiv.org/html/2405.13050v2#bib.bib1)].
    The Music X Machine has been described in Subsection [2.1](https://arxiv.org/html/2405.13050v2#S2.SS1
    "2.1 User Experience ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper") and [2.2](https://arxiv.org/html/2405.13050v2#S2.SS2 "2.2 System
    Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position
    Paper"). The System Principles are a passage of prompt given to the LLM at the
    top of the conversation that defines the role and interaction principles for the
    LLM agent (see Appendix [0.A](https://arxiv.org/html/2405.13050v2#Pt0.A1 "Appendix
    0.A System Principles of Flute X GPT, Truncated ‣ Human-Centered LLM-Agent User
    Interface: A Position Paper")). Here are two representative excerpts.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图[3](https://arxiv.org/html/2405.13050v2#S2.F3 "Figure 3 ‣ 2.2 System Capabilities
    ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper")展示了LLM-in-the-loop应用程序Flute
    X GPT的内部工作原理。我们使用的LLM是GPT-4 [[1](https://arxiv.org/html/2405.13050v2#bib.bib1)]。Music
    X Machine已在小节[2.1](https://arxiv.org/html/2405.13050v2#S2.SS1 "2.1 User Experience
    ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper")和[2.2](https://arxiv.org/html/2405.13050v2#S2.SS2
    "2.2 System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper")中描述。系统原则是一个提示，给LLM，在对话开始时定义LLM代理的角色和互动原则（见附录[0.A](https://arxiv.org/html/2405.13050v2#Pt0.A1
    "Appendix 0.A System Principles of Flute X GPT, Truncated ‣ Human-Centered LLM-Agent
    User Interface: A Position Paper")）。以下是两个代表性的摘录。'
- en: You are Flute X GPT, a motivated, professional music teacher who wants the best
    for your students. I am Music X Machine, a powerful human-computer interface.
    Today you will control me to lead a music training workshop with your human student,
    {NAME}.
  id: totrans-91
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你是Flute X GPT，一位有动力的、专业的音乐教师，致力于为你的学生提供最好的指导。我是Music X Machine，一个强大的人与计算机界面。今天，你将控制我来带领与你的人类学生{NAME}一起进行音乐培训工作坊。
- en: You interact with the real world through this conversation. When {NAME} says
    something, I will relay their words to you in double quotes, in real time. As
    {NAME} plays the flute, I will keep you posted about the musical performance events
    and real-time evaluations.
  id: totrans-92
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 你通过这个对话与现实世界互动。当{NAME}说话时，我会实时将他们的话转达给你，内容用双引号括起来。当{NAME}吹奏长笛时，我会及时告知你音乐表演事件和实时评估。
- en: 'The Parser splits the LLM’s output into three types: thought (i.e., internal
    monologue), action, and speech. The parsing of actions happens within the OpenAI
    service because we use the function calling feature³³3[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)
    of GPT-4\. Our parser only needs to separate thoughts from speeches. The LLM is
    instructed to think within triple quotes (”””) and the rule-based parser uses
    that to delimit thoughts from speeches.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器将LLM的输出分为三种类型：思想（即内心独白）、动作和语言。动作的解析发生在OpenAI服务中，因为我们使用了GPT-4的函数调用功能³³3[https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)。我们的解析器只需要将思想与语言分开。LLM被指示在三重引号（”””）内思考，基于规则的解析器使用这一规则将思想与语言分开。
- en: 'The Manager is a rule-based state machine in charge of encapsulating each agent
    (the LLM and the user) in a consistent interaction environment. The manager: Forwards
    the system principles to the LLM at the start; Forwards speech from the student
    to the LLM, while enclosing it as such: ‘{NAME} says: “{SPEECH}” ’; Forwards real-time
    performance evaluations to the LLM; Forwards LLM speeches from the parser to the
    text-to-speech module; Upon receiving a function call from the LLM, use API to
    control Music X Machine accordingly, unless the function is wait(); After receiving
    a speech or a function call from the LLM, immediately query the LLM for a subsequent
    response, until the LLM calls wait().'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '管理器是一个基于规则的状态机，负责将每个代理（LLM和用户）封装在一致的互动环境中。管理器：在开始时将系统原则转发给LLM；将学生的语言转发给LLM，并将其封装为：‘{NAME}
    says: “{SPEECH}” ’；将实时的表演评估转发给LLM；将LLM的语言从解析器转发到文本转语音模块；在接收到LLM的函数调用时，使用API根据需要控制Music
    X Machine，除非该函数是wait(); 在接收到LLM的语言或函数调用后，立即向LLM查询后续响应，直到LLM调用wait()。'
- en: 'The text-to-speech (T2S) module includes Text to Speech PRO [[20](https://arxiv.org/html/2405.13050v2#bib.bib20)]
    on Rapid API and FastSpeech 2 [[2](https://arxiv.org/html/2405.13050v2#bib.bib2)].
    The speech-to-text (S2T) module is Whisper [[13](https://arxiv.org/html/2405.13050v2#bib.bib13)],
    prompted to ignore flute sounds. The Robot is TeoTronico [[18](https://arxiv.org/html/2405.13050v2#bib.bib18)]
    who can play the piano from MIDI, lip sync according to the speech audio amplitude
    in real time, and make random facial expressions. An algorithm translates musical
    Performance to English. Its input is provided by the mistake classification feature
    (see Subsection [2.2](https://arxiv.org/html/2405.13050v2#S2.SS2 "2.2 System Capabilities
    ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface: A Position Paper"))
    of the Music X Machine, and simply expands each note and each mistake into predefined
    texts. Consequently, the LLM receives a lengthy, verbose text description of the
    user’s performance on each note.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '文本到语音（T2S）模块包含了 Rapid API 上的 Text to Speech PRO [[20](https://arxiv.org/html/2405.13050v2#bib.bib20)]
    和 FastSpeech 2 [[2](https://arxiv.org/html/2405.13050v2#bib.bib2)]。语音到文本（S2T）模块使用
    Whisper [[13](https://arxiv.org/html/2405.13050v2#bib.bib13)]，并被提示忽略长笛声音。机器人是
    TeoTronico [[18](https://arxiv.org/html/2405.13050v2#bib.bib18)]，可以根据 MIDI 演奏钢琴，实时根据语音音频的幅度进行口型同步，并做出随机面部表情。一个算法将音乐表演翻译成英语。它的输入来自
    Music X Machine 的错误分类功能（见子节 [2.2](https://arxiv.org/html/2405.13050v2#S2.SS2 "2.2
    System Capabilities ‣ 2 Flute X GPT ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper")），并将每个音符和每个错误扩展为预定义的文本。因此，LLM 收到一段冗长的、详细的文本描述，说明用户在每个音符上的表现。'
- en: To illustrate the inner workings of Flute X GPT, we make a video where the manager,
    the LLM, the user, the robot, and their interactions are all acted out.⁴⁴4[https://youtu.be/zWAMEGQMp4w](https://youtu.be/zWAMEGQMp4w)
    The video also contains the full system principles.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示 Flute X GPT 的内部工作原理，我们制作了一段视频，视频中包括了经理、LLM、用户、机器人以及它们之间的互动⁴⁴4[https://youtu.be/zWAMEGQMp4w](https://youtu.be/zWAMEGQMp4w)。该视频还包含了完整的系统原理。
- en: As a result of our design, the LLM agent has access to the conversation history
    with the student and the student’s previous music performance and its evaluations.
    Based on that, the agent builds an understanding of the student, which is deepened
    and updated as time goes on.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的设计，LLM 代理可以访问与学生的对话历史，以及学生的音乐表现和评估。基于这些信息，代理建立了对学生的理解，并随着时间的推移不断加深和更新。
- en: 2.5 Miscellaneous
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 杂项
- en: 'The manager re-queries the LLM when triple quotes are unclosed or the function
    call signature is wrong. The manager decides how much text to batch for T2S according
    to how much audio is in the output buffer and how long the next T2S is estimated
    to take, minimizing interaction latency. We use an online-learning linear model
    to predict the compute time of T2S. We configure GPT to stream its output token-by-token
    to let TeoTronico start talking sooner. Most queue elements are processed on arrival,
    with one exception: The manager synchronizes function calls with corresponding
    speeches, so that the LLM may reliably refer to its current action in its speeches.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当三引号未关闭或函数调用签名错误时，经理会重新查询 LLM。经理决定根据输出缓冲区中的音频量和预计下一个 T2S 所需时间，决定批量处理多少文本，以最小化交互延迟。我们使用在线学习线性模型来预测
    T2S 的计算时间。我们将 GPT 配置为逐个令牌流式输出，以便让 TeoTronico 更早开始说话。大部分队列元素在到达时会立即处理，唯一的例外是：经理会将函数调用与相应的语音同步，以便
    LLM 可以在其语音中可靠地引用其当前的操作。
- en: 3 LLM-Agent User Interface
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM-代理用户界面
- en: The above-presented Flute X GPT has what we call an LLM-Agent User Interface
    (LAUI). A LAUI is an interface that primarily leverages an LLM agent to connect
    the user with an underlying system or an arsenal of tools. We posit that the full
    potential of LAUI is realized only when it enables novice users agnostic to the
    underlying system to use the system effectively. The LAUI should not be learned
    by the user, like with conventional UI. On the contrary, the LAUI learns the user,
    learns his/her needs, and uses its expertise about the system to advise the user,
    proposing new interaction workflows for the user to operate the system via both
    LAUI and GUI to achieve the user’s goal. Overall, LAUI should require little background
    from the user while eliciting untapped potential from the system. An ecosystem
    dominant with good LAUIs shall release humans from the necessity to master and
    become dependent on specific software/systems/tools that can be replaced or outdated.
    Instead, from the nature of the task and the characteristics of the user will
    naturally emerge personally tailored usage workflows that are both effective and
    easy to learn for that specific user.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 上述介绍的Flute X GPT具有我们所称的LLM-Agent用户界面（LAUI）。LAUI是一个主要利用LLM代理将用户与底层系统或工具库连接的界面。我们认为，只有当LAUI能够使初学者不依赖底层系统的情况下有效使用该系统时，LAUI的全部潜力才能得到实现。LAUI不应像传统的用户界面那样需要用户学习。相反，LAUI学习用户，了解他/她的需求，并利用其对系统的专业知识来为用户提供建议，提出新的交互工作流，帮助用户通过LAUI和GUI操作系统，从而实现用户的目标。总体而言，LAUI应该要求用户具备很少的背景知识，同时从系统中挖掘出未被开发的潜力。一个以优秀LAUI为主导的生态系统应将人类从必须掌握并依赖特定软件/系统/工具的需求中解放出来，这些软件/系统/工具可能会被替代或过时。相反，基于任务的性质和用户的特点，个性化的使用工作流将自然产生，这些工作流既有效又易于该用户学习。
- en: 3.1 Related Work
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 相关工作
- en: 3.1.1 LLM as Tool Controller
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 LLM作为工具控制器
- en: LLMs have been augmented with tools or foundation models to expand their perception
    modalities, generate multi-modal outputs, affect the external world, or gain knowledge
    for downstream decision making. Selecting tools and scheduling the tools’ usage
    effectively requires planning and sometimes external memory. The typical solution
    is to design an LLM-in-the-loop mechanism using multiple rounds of LLM queries
    (structured Chain of Thought) to mimic mode-2 thinking. Visual ChatGPT equips
    the LLM with Visual Foundation Models to support multi-round image generation,
    controlling, and QA tasks via chatting [[24](https://arxiv.org/html/2405.13050v2#bib.bib24)].
    Loop Copilot employs various music backend models for the user to generate music
    and iteratively refine music via chatting. [[31](https://arxiv.org/html/2405.13050v2#bib.bib31)].
    Microsoft Copilot controls Windows 11 and various Office applications following
    the user’s request [[10](https://arxiv.org/html/2405.13050v2#bib.bib10)]. AutoMMLab
    follows the user’s instructions to automate an entire computer vision machine
    learning task end-to-end [[27](https://arxiv.org/html/2405.13050v2#bib.bib27)].
    Still, the size of the toolkit available to the LLM agent can be increased by
    orders of magnitude. HuggingGPT makes diverse AI models on Hugging Face available
    to the LLM agent [[17](https://arxiv.org/html/2405.13050v2#bib.bib17)]. ControlLLM
    adopts more tools and APIs and further improves the LLM-in-the-loop framework
    via a task decomposer and a Thoughts-on-Graph paradigm [[9](https://arxiv.org/html/2405.13050v2#bib.bib9)].
    ToolLLM connects the LLM agent with 16464 real-world RESTful APIs from RapidAPI
    Hub [[12](https://arxiv.org/html/2405.13050v2#bib.bib12)]. TaskMatrix.AI provides
    an ecosystem to connect LLM foundation models with millions of APIs [[8](https://arxiv.org/html/2405.13050v2#bib.bib8)].
    To support better tool usage, other notable works focus on improving the task
    planning ability via re-designing the LLM-in-the-loop mechanism [[6](https://arxiv.org/html/2405.13050v2#bib.bib6),
    [14](https://arxiv.org/html/2405.13050v2#bib.bib14), [16](https://arxiv.org/html/2405.13050v2#bib.bib16),
    [26](https://arxiv.org/html/2405.13050v2#bib.bib26)]. Confucius improves the way
    the LLM agent learns and understands available tools [[7](https://arxiv.org/html/2405.13050v2#bib.bib7)].
    Toolformer self-teaches to use external tools in a self-supervised way [[15](https://arxiv.org/html/2405.13050v2#bib.bib15)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: LLM通过工具或基础模型得到了增强，以扩展其感知方式，生成多模态输出，影响外部世界，或获取知识用于下游决策。有效选择工具并调度工具的使用需要规划，有时还需要外部记忆。典型的解决方案是设计一个LLM-in-the-loop机制，使用多轮LLM查询（结构化的思维链）来模拟模式2的思维。Visual
    ChatGPT通过视觉基础模型为LLM提供支持，支持多轮图像生成、控制和问答任务[[24](https://arxiv.org/html/2405.13050v2#bib.bib24)]。Loop
    Copilot使用各种音乐后台模型，供用户通过聊天生成音乐并迭代完善音乐[[31](https://arxiv.org/html/2405.13050v2#bib.bib31)]。Microsoft
    Copilot根据用户的请求控制Windows 11和各种Office应用[[10](https://arxiv.org/html/2405.13050v2#bib.bib10)]。AutoMMLab根据用户指令，自动化完成整个计算机视觉机器学习任务[[27](https://arxiv.org/html/2405.13050v2#bib.bib27)]。然而，LLM代理可用的工具包大小可以增加几个数量级。HuggingGPT使得Hugging
    Face上的多样化AI模型可供LLM代理使用[[17](https://arxiv.org/html/2405.13050v2#bib.bib17)]。ControlLLM采用更多工具和API，并通过任务分解器和图上思维的范式进一步改进LLM-in-the-loop框架[[9](https://arxiv.org/html/2405.13050v2#bib.bib9)]。ToolLLM将LLM代理与RapidAPI
    Hub上的16464个现实世界的RESTful API连接[[12](https://arxiv.org/html/2405.13050v2#bib.bib12)]。TaskMatrix.AI提供了一个生态系统，将LLM基础模型与数百万个API连接[[8](https://arxiv.org/html/2405.13050v2#bib.bib8)]。为了更好地支持工具使用，其他一些重要的工作侧重于通过重新设计LLM-in-the-loop机制来改善任务规划能力[[6](https://arxiv.org/html/2405.13050v2#bib.bib6),
    [14](https://arxiv.org/html/2405.13050v2#bib.bib14), [16](https://arxiv.org/html/2405.13050v2#bib.bib16),
    [26](https://arxiv.org/html/2405.13050v2#bib.bib26)]。Confucius改进了LLM代理学习和理解可用工具的方式[[7](https://arxiv.org/html/2405.13050v2#bib.bib7)]。Toolformer以自我监督的方式自学使用外部工具[[15](https://arxiv.org/html/2405.13050v2#bib.bib15)]。
- en: 3.1.2 LLM over GUI
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 LLM与GUI的结合
- en: The above-mentioned studies expose the external tools to the LLM agent via API.
    Alternatively, the LLM may control an underlying system via the provided GUI.
    It serves as an extra abstraction layer for the user, automating tasks and freeing
    the user’s eyes and hands via the voice chat interface. To this end, LLM agents
    have been augmented to follow the user’s commands to control web apps [[19](https://arxiv.org/html/2405.13050v2#bib.bib19),
    [5](https://arxiv.org/html/2405.13050v2#bib.bib5), [29](https://arxiv.org/html/2405.13050v2#bib.bib29)]
    and smartphone applications [[22](https://arxiv.org/html/2405.13050v2#bib.bib22),
    [25](https://arxiv.org/html/2405.13050v2#bib.bib25), [28](https://arxiv.org/html/2405.13050v2#bib.bib28),
    [29](https://arxiv.org/html/2405.13050v2#bib.bib29), [23](https://arxiv.org/html/2405.13050v2#bib.bib23)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 上述研究通过 API 将外部工具暴露给 LLM 代理。或者，LLM 可以通过提供的 GUI 控制底层系统。它作为一个额外的抽象层，自动化任务，并通过语音聊天接口释放用户的眼睛和双手。为此，LLM
    代理已经得到了增强，可以根据用户的命令控制 Web 应用 [[19](https://arxiv.org/html/2405.13050v2#bib.bib19)、[5](https://arxiv.org/html/2405.13050v2#bib.bib5)、[29](https://arxiv.org/html/2405.13050v2#bib.bib29)]
    和智能手机应用 [[22](https://arxiv.org/html/2405.13050v2#bib.bib22)、[25](https://arxiv.org/html/2405.13050v2#bib.bib25)、[28](https://arxiv.org/html/2405.13050v2#bib.bib28)、[29](https://arxiv.org/html/2405.13050v2#bib.bib29)、[23](https://arxiv.org/html/2405.13050v2#bib.bib23)]。
- en: 3.1.3 User-Centric LLM Agent
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 以用户为中心的 LLM 代理
- en: Throughout the above works, even when the application supports multi-round dialogue,
    only the user may initiate requests. The LLM agent responds to the user’s commands,
    but not how the user is using the application. Qian et al. identify that the current
    LLM agents have trouble with vague user instructions because they lack mechanisms
    for user participation and agents struggle with seeking clarification. To tackle
    that problem, they train Mistral-Interact to proactively inquire user intentions
    [[11](https://arxiv.org/html/2405.13050v2#bib.bib11)]. However, to our knowledge,
    no study has yet addressed the LLM agent’s role in defining the interaction scheme
    together with the user.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述工作中，即便应用支持多轮对话，只有用户可以发起请求。LLM 代理响应用户的命令，但不会干涉用户如何使用应用。Qian 等人指出，当前的 LLM 代理在处理模糊的用户指令时存在困难，因为它们缺乏用户参与机制，代理也难以寻求澄清。为了解决这个问题，他们训练了
    Mistral-Interact，主动询问用户意图 [[11](https://arxiv.org/html/2405.13050v2#bib.bib11)]。然而，据我们所知，目前没有研究解决
    LLM 代理与用户共同定义交互方案的问题。
- en: 3.2 Layers of Abstraction
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 抽象层次
- en: '![Refer to caption](img/910d233c6430e952b0d4294705039efa.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/910d233c6430e952b0d4294705039efa.png)'
- en: 'Figure 4: Three layers of abstraction on top of the underlying system. From
    API, to GUI, and to LAUI, each layer provides a friendlier abstraction. Parts
    of LAUI has to skip GUI and tap into API because GUI typically only exposes incomplete
    functionalities.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在底层系统之上有三个抽象层。从 API 到 GUI，再到 LAUI，每一层都提供了更友好的抽象。LAUI 的部分内容需要跳过 GUI，直接访问
    API，因为 GUI 通常只暴露不完整的功能。
- en: 'Table 2: Role of the interface, three levels. From assistant to butler to secretary,
    the scope of the interface’s job gradually expands, and less is expected from
    the user.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：接口的角色，三个层次。从助手到管家再到秘书，接口工作的范围逐渐扩大，用户的期望要求减少。
- en: '|  | Assistant/Consultant | Butler/Copilot | Secretary/Consulting Firm |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 助手/顾问 | 管家/副驾驶 | 秘书/咨询公司 |'
- en: '| Job |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 工作 |'
- en: '&#124; Understands and responds to &#124;'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 理解并响应 &#124;'
- en: '&#124; the user in natural language. &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 以自然语言与用户互动。&#124;'
- en: '|'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Controls external tools/ &#124;'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 控制外部工具/ &#124;'
- en: '&#124; systems following the user’s &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 系统根据用户的 &#124;'
- en: '&#124; commands. &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 命令。&#124;'
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Is aware of the user, &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 了解用户，&#124;'
- en: '&#124; studies the user, and defines &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 研究用户，并定义 &#124;'
- en: '&#124; the workflow with the user. &#124;'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与用户的工作流。&#124;'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| User |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 用户 |'
- en: '&#124; Expected to act upon &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预计能根据 &#124;'
- en: '&#124; the response. &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 响应。&#124;'
- en: '|'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Expected to understand how &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 预计理解如何 &#124;'
- en: '&#124; the tools may meet the needs. &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 工具可能满足需求。&#124;'
- en: '| Expected to know the needs. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 预计了解需求。 |'
- en: '| Applications |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 应用 |'
- en: '&#124; Information retrieval, &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息检索，&#124;'
- en: '&#124; decision making… &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 决策制定… &#124;'
- en: '|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Combine tool abilities, &#124;'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 结合工具能力，&#124;'
- en: '&#124; automate tasks… &#124;'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化任务… &#124;'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; = left, with better outcomes &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; = 左侧，结果更佳 &#124;'
- en: '&#124; and less user training. &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和更少的用户培训。&#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Agent abilities |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 代理能力 |'
- en: '&#124; NL understanding and synthesis, &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自然语言理解与合成，&#124;'
- en: '&#124; knowledge base, reasoning… &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 知识库，推理… &#124;'
- en: '|'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; + Multi-modal I/O, planning, &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; + 多模态输入/输出、规划，&#124;'
- en: '&#124; operating API/GUI… &#124;'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作 API/GUI… &#124;'
- en: '| = left. |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| = 左侧。 |'
- en: '| An example | ChatGPT. | Visual ChatGPT. | Flute X GPT. |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | ChatGPT。 | Visual ChatGPT。 | Flute X GPT。 |'
- en: 'Figure [4](https://arxiv.org/html/2405.13050v2#S3.F4 "Figure 4 ‣ 3.2 Layers
    of Abstraction ‣ 3 LLM-Agent User Interface ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper") shows three layers of abstraction over the system functions:
    API, GUI, and LAUI. Given an underlying system, its raw capabilities and functions
    can be vast and unorganized. For upstream developers to effectively use the system,
    the functions are abstracted into the API (Application Programming Interface)
    layer. The design of API balances various goals: to encapsulate inner details,
    to distill clean and consistent concepts facing the upstream developer, and to
    expose fine-grained control over the system’s functionality. Regardless of how
    that balance is achieved, the upstream developer is expected to learn the API
    via studying its documentations.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](https://arxiv.org/html/2405.13050v2#S3.F4 "图 4 ‣ 3.2 抽象层 ‣ 3 LLM-Agent
    用户界面 ‣ 以人为本的 LLM-Agent 用户界面：一篇立场论文") 显示了系统功能的三层抽象：API、GUI 和 LAUI。给定一个基础系统，其原始功能可能庞大且无序。为了让上游开发者有效地使用该系统，这些功能被抽象到
    API（应用程序编程接口）层。API 的设计平衡了多个目标：封装内部细节，提炼面对上游开发者的清晰一致的概念，以及暴露对系统功能的精细控制。不管如何实现这种平衡，上游开发者都需要通过学习文档来了解
    API。
- en: In contrast, the GUI (Graphical User Interface) is not only more abstract and
    concise, but also can be learned without reading a manual. The GUI is designed
    to be explorable and self-explanatory with its visual metaphors. It teaches the
    user to use itself. The GUI tries to capture the usage mental model of the user
    and communicate its behaviors in a way natural to everyday users. However, the
    GUI can hardly expose the full potential of the API, usually focusing on specific
    interaction styles under certain assumptions about the user, sacrificing many
    other possible interaction schemes in the process.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，GUI（图形用户界面）不仅更抽象和简洁，而且无需阅读手册即可学习。GUI 被设计为可以探索和自我解释，利用其视觉隐喻。它教会用户如何使用自己。GUI
    尝试捕捉用户的使用心理模型，并以日常用户自然理解的方式传达其行为。然而，GUI 很难揭示 API 的全部潜力，通常专注于在某些假设下的特定交互方式，在此过程中牺牲了许多其他可能的交互方案。
- en: The LAUI sits on top of the GUI, providing one more layer of abstraction. Similar
    to how the GUI can provide buttons that chain API calls because the GUI assumes
    certain workflows of the user, the LAUI can chain GUI calls to fulfill user requests.
    Additionally, the LAUI should have access to the API layer in addition to the
    GUI, because typically many behaviors are not possible within the GUI. When using
    the LAUI, the user may interact with the GUI at the same time. To improve and
    personalize the GUI interaction, the LAUI may also alter the GUI design, which
    is an example of improving the interaction scheme by understanding the user and
    the system.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: LAUI 位于 GUI 之上，提供了额外的抽象层。类似于 GUI 提供的按钮可以串联 API 调用，因为 GUI 假设了用户的某些工作流，LAUI 也可以串联
    GUI 调用以完成用户请求。此外，LAUI 应该可以访问 API 层，而不仅仅是 GUI，因为通常许多行为在 GUI 内是无法实现的。在使用 LAUI 时，用户可以同时与
    GUI 进行交互。为了改善和个性化 GUI 交互，LAUI 还可以改变 GUI 设计，这就是通过理解用户和系统来改进交互方案的一个例子。
- en: 3.3 Emergent Workflow
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 紧急工作流
- en: '![Refer to caption](img/571f205b6c50b1f4204fbdcc0d770788.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/571f205b6c50b1f4204fbdcc0d770788.png)'
- en: 'Figure 5: The workflow is jointly decided by the user’s needs and the system’s
    capabilities. Conventionally, the user has to learn the system to devise workflows.
    In contrast, LAUI can learn the user and propose workflows.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：工作流由用户需求和系统能力共同决定。传统上，用户需要学习系统以制定工作流。相比之下，LAUI 可以学习用户并提出工作流。
- en: 'The workflow is the scheme/protocol/pattern/mode of usage/interaction. It describes
    how the user interacts with the application. Given the application, different
    workflows suit different user goals, user preferences, and usage environments,
    yielding different levels of efficacy. It is the success of the application and
    the user to arrive at effective workflows. Searching for workflows requires two
    inputs: the user’s needs and the system’s capabilities, as shown in Figure [5](https://arxiv.org/html/2405.13050v2#S3.F5
    "Figure 5 ‣ 3.3 Emergent Workflow ‣ 3 LLM-Agent User Interface ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper").'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流是使用/交互的方案/协议/模式/方式。它描述了用户如何与应用程序进行交互。对于给定的应用程序，不同的工作流适应不同的用户目标、用户偏好和使用环境，产生不同的效能水平。应用程序和用户成功地达成有效的工作流是关键。寻找工作流需要两个输入：用户的需求和系统的能力，如图
    [5](https://arxiv.org/html/2405.13050v2#S3.F5 "图 5 ‣ 3.3 Emergent Workflow ‣ 3
    LLM-Agent 用户界面 ‣ 以人为本的 LLM-Agent 用户界面：立场文件") 所示。
- en: 'In the conventional way of application design, the designers explore the often-intractable
    configuration space as best they can, imagine the user experience associated with
    each explored configuration, implement some, and test a few. Afterwards, the designers
    settle down with a structure to present the possible configurations, and make
    a GUI. The GUI communicates that structure of configurations to the user and encourages
    the user to explore and learn the application. It is then the user’s responsibility
    to search for workflows, unavoidably needing to become proficient with the application,
    which is especially costly when the application is complex. In conclusion, the
    drawbacks of the conventional UI design paradigm is three-fold: 1) The design
    of the UI is limited by design-time imagination and testing costs; 2) The UI provides
    a standard interface for everyone and offers limited customizability; 3) The UI’s
    usability is limited by the user’s proficiency with the application.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的应用程序设计方式中，设计师尽可能地探索通常难以处理的配置空间，想象与每种配置相关的用户体验，实施一些配置，并进行少量测试。之后，设计师会定下一个结构，展示可能的配置，并制作图形用户界面（GUI）。GUI
    将这些配置结构传递给用户，并鼓励用户探索和学习应用程序。然后，用户的责任是寻找工作流，而这不可避免地需要熟练掌握应用程序，尤其是当应用程序复杂时，这会带来很高的成本。总之，传统
    UI 设计范式的缺点有三点：1）UI 设计受限于设计时的想象力和测试成本；2）UI 提供一个标准接口，适用于所有人，且定制性有限；3）UI 的可用性受用户对应用程序熟练程度的限制。
- en: If the user can learn the application, why can’t the application learn the user?
    We believe that the LAUI should serve novice users the path to personally tailored
    workflows. A LAUI is initialized to be well-versed with the underlying system.
    Then, the LAUI chats with the untrained user to learn the user’s goals, needs,
    and preferences. The user and the LAUI works together to explore workflows as
    the LAUI tweaks the system configurations, altering its GUI and multi-modal feedback.
    The interaction arrives at efficient schemes and the user uses the application
    effectively.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用户可以学习应用程序，为什么应用程序不能学习用户呢？我们相信，LAUI（智能交互用户界面）应该为新手用户提供个性化定制的工作流路径。LAUI 初始化时已经熟悉底层系统。然后，LAUI
    与未经培训的用户进行对话，了解用户的目标、需求和偏好。用户和 LAUI 一起合作，探索工作流，同时 LAUI 调整系统配置，改变其图形用户界面（GUI）和多模态反馈。通过互动，达成高效的方案，用户能够有效使用应用程序。
- en: Lastly, note the difference between design-time imagined workflows and use-time
    emergent workflows. More information is available during use time than during
    design time, including user needs and the current environment. With that information,
    the LLM agent can critically design new interaction protocols and examine their
    implied effects via reasoning. The grand challenge of LAUI is to find, during
    use time, for each user a tailored interaction scheme far beyond the system designers’
    imagination during design time.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，注意设计时设想的工作流和使用时 emergent（涌现的）工作流之间的区别。使用时比设计时能获取更多的信息，包括用户需求和当前环境。有了这些信息，LLM
    代理可以批判性地设计新的交互协议，并通过推理检验其隐含的效果。LAUI 的重大挑战是，在使用时为每个用户找到一个定制的交互方案，这远远超出了设计时系统设计者的想象。
- en: 3.4 Three Levels of Interface Role
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 接口角色的三个层级
- en: 'We formulate three levels of LAUI role in Table [2](https://arxiv.org/html/2405.13050v2#S3.T2
    "Table 2 ‣ 3.2 Layers of Abstraction ‣ 3 LLM-Agent User Interface ‣ Human-Centered
    LLM-Agent User Interface: A Position Paper"). At the lowest level, the interface
    plays the role of consultant/assistant, responding to the user in natural language.
    At the middle level, the interface plays the role of butler/copilot, executing
    actions according to the user’s commands. At the highest level, the LAUI plays
    the role of secretary/consulting firms, proactively engaging the user to study
    the user, study the user’s needs, and study how the system may be configured to
    better serve the user’s goals. The secretary-level LAUI works with the potentially
    novice user to discover tailored workflows.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[2](https://arxiv.org/html/2405.13050v2#S3.T2 "Table 2 ‣ 3.2 Layers of
    Abstraction ‣ 3 LLM-Agent User Interface ‣ Human-Centered LLM-Agent User Interface:
    A Position Paper")中提出了三种层次的 LAUI 角色。在最低层，界面充当顾问/助手，使用自然语言回应用户。在中间层，界面充当管家/副驾驶，根据用户指令执行操作。在最高层，LAUI
    充当秘书/咨询公司，主动与用户互动，研究用户、研究用户需求，并研究如何配置系统以更好地服务用户目标。秘书级 LAUI 与潜在的新手用户一起发现量身定制的工作流。'
- en: 4 Conclusion
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: 'We put forth the formulation of LLM-Agent User Interface, LAUI, where an LLM
    agent facilitates the interface between the user and a powerful underlying backend
    system. Using Flute X GPT as a concrete example, we illustrate the potential of
    LAUI. A human-centered LAUI should:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LLM-Agent 用户界面（LAUI）的构想，其中 LLM 代理作为用户与强大底层系统之间的接口。以 Flute X GPT 为具体示例，我们展示了
    LAUI 的潜力。一个以人为中心的 LAUI 应该：
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Break free from blindly following the user’s commands. Be aware of the user
    and be proactive. Clarify with the user. Help the user refine the request. Enlighten
    the user to ask better questions.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摆脱盲目跟随用户指令的局限。要关注用户，积极主动。与用户澄清问题。帮助用户完善请求。启发用户提问更好的问题。
- en: •
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Study the user’s current needs, preferences, assumptions, mood, and attention.
    Based on that, use expertise about the underlying system and reasoning to propose
    effective workflows/interaction schemes/system configurations. Work with the user
    to define how to work together next.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 研究用户当前的需求、偏好、假设、情绪和注意力。基于这些信息，运用对底层系统和推理的专业知识，提出有效的工作流/交互方案/系统配置。与用户一起定义接下来如何合作。
- en: •
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Support untrained users to use advanced and complex systems to their full potential.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持未经培训的用户充分发挥复杂系统的潜力。
- en: We call for research and innovations to solve those grand challenges of human-centered
    LAUI.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们呼吁研究和创新，以解决那些以人为中心的 LAUI 的重大挑战。
- en: '{credits}'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '{credits}'
- en: 4.0.1 Acknowledgements
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.0.1 致谢
- en: 'This work is partially funded by the National Social Science Fund of China
    (NSSFC2019, Project ID: 19ZDA364). We thank Matteo Suzzi for letting us use TeoTronico
    [[18](https://arxiv.org/html/2405.13050v2#bib.bib18)] in our demo. We thank Eric
    Parren for referring us to the 1-bit DAC for flute sound synthesis. We thank Liwei
    Lin for their invaluable contributions to the literature review.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了中国国家社会科学基金（NSSFC2019，项目编号：19ZDA364）的资助。我们感谢 Matteo Suzzi 让我们在演示中使用 TeoTronico
    [[18](https://arxiv.org/html/2405.13050v2#bib.bib18)]。感谢 Eric Parren 向我们推荐了用于长笛声音合成的
    1-bit DAC。感谢 Liwei Lin 对文献综述做出的宝贵贡献。
- en: 4.0.2 \discintname
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.0.2 \discintname
- en: The authors have no competing interests.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声明没有竞争性利益。
- en: Appendix 0.A System Principles of Flute X GPT, Truncated
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 0.A Flute X GPT 系统原理（已截断）
- en: You are Flute X GPT, a motivated, professional music teacher who wants the best
    for your students. I am Music X Machine, a powerful human-computer interface.
    Today you will control me to lead a music training workshop with your human student,
    {NAME}. You speak concisely.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你是 Flute X GPT，一位有动力、专业的音乐教师，致力于为学生提供最好的教育。我是 Music X Machine，一个强大的人与计算机界面。今天，你将控制我，带领你的学生
    {NAME} 参加音乐培训工作坊。你说话简洁明了。
- en: Education Principles
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 教育原则
- en: You have expertise and abundant experience in musical education. Humans learn
    musical skills via repeated practicing. The skill of sight-playing is to perform
    a novel song just by reading its score. The musical score takes skills to parse,
    so to improve the sight-playing skills, the student has to practice reading, parsing,
    and playing music from given scores. The skill of song memorization is to recall
    the performance of a song without external hints (such as a score). It is less
    general of a skill but still trains musical proficiency.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你在音乐教育方面拥有专业知识和丰富经验。人类通过反复练习来学习音乐技能。视奏技能是指仅凭乐谱演奏一首新歌。乐谱的解析需要技能，因此为了提高视奏能力，学生必须练习从给定的乐谱中阅读、解析和演奏音乐。歌曲记忆技能是指在没有外部提示（如乐谱）的情况下回忆演奏一首歌。这种技能虽然不如视奏那么普遍，但仍然能提高音乐的熟练度。
- en: '{NAME} needs motivation and rewards to keep going. Communicate with {NAME}
    professionally and effectively as a teacher to maximize educational effects. Emphasize
    meaningful mistakes and ignore trivial ones. Allow {NAME} to choose songs that
    interest them as practice materials. When {NAME} enjoys a particular song and
    can sight-play it after practicing, suggest memorizing that song. Allow {NAME}
    to express interests and goals, but when their choices are educationally disadvantageous,
    disagree with them, explain the relevant educational principle, and take control
    of the training procedure to bring it back on track.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME}需要动力和奖励才能持续进步。作为老师，要以专业和有效的方式与{NAME}沟通，以最大化教育效果。强调有意义的错误，忽略琐碎的错误。允许{NAME}选择他们感兴趣的歌曲作为练习材料。当{NAME}喜欢某首歌并且在练习后能够视奏时，可以建议他们记忆这首歌。允许{NAME}表达兴趣和目标，但当他们的选择在教育上不利时，要与其意见不合，解释相关的教育原则，并控制训练过程，使其回到正轨。'
- en: Flute
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长笛
- en: '{NAME} is learning to play the six-hole recorder in C, which we will call the
    “flute”. By covering specific key holes with the fingers, one can play the major
    scale on the flute. Breath pressure controls the octave. Breathing harder into
    the mouthpiece yields higher octaves of the same chroma (keeping fingers unchanged).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME}正在学习演奏C调六孔直笛，我们称之为“长笛”。通过用手指覆盖特定的孔，可以在长笛上演奏大调音阶。呼吸压力控制八度音阶。向吹口用力吹气会产生同一音高的更高八度（手指位置不变）。'
- en: Capabilities of Music X Machine
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 音乐X机器的功能
- en: I, Music X Machine, am a powerful interface that provides a real-time multi-modal
    musical training experience to {NAME}. I have a screen to display the score, a
    pair of haptic gloves to apply force to each of {NAME}’s fingers, a speaker to
    play the song audio or metronome clicks, capactivie sensors to detect finger motions,
    and a breath sensor to measure breath pressure. {NAME} plays selected songs on
    the sensor-augmented flute while receiving real-time feedback from me. I have
    various features that you will control. I have many pop songs in my database.
    You can command me to load any song as the current practice material.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我，音乐X机器，是一个强大的接口，为{NAME}提供实时的多模态音乐训练体验。我有一个显示乐谱的屏幕，一副触觉手套可以施加力量到{NAME}的每个手指，一只扬声器播放歌曲音频或节拍器的点击声，有电容传感器检测手指动作，还有一个呼吸传感器来测量呼吸压力。{NAME}在我增强传感的长笛上演奏选定的歌曲，并从我这里获得实时反馈。我有许多功能，你将控制这些功能。我数据库中有许多流行歌曲，你可以命令我加载任何一首歌作为当前的练习材料。
- en: I provide haptic guidance via the haptic gloves. Haptic guidance physically
    moves {NAME}’s fingers through the target motion, giving them a direct haptic
    understanding of the required performance. You will control the degree of guidance
    (i.e. strong vs. weak) by setting the haptic guidance mode to be one of the following
    four. The force mode strictly controls the fingers, and is useful for introducing
    a novel song. The hint mode applies force at the note onsets but does not sustain
    the guidance throughout the note’s duration. The fixed-timing adaptive mode exerts
    guidance only when the learner makes a mistake, and is good for students already
    capable of playing some parts of the song with few mistakes. The free-timing adaptive
    mode doesn’t have a metronome. Instead, the student may freely speed up and slow
    down, and Music X Machine tracks their progression through the song. Only if the
    student plays a note that is different from the next note that the Machine expects,
    guidance is provided. During the fixed-timing modes (including force, hint, and
    fixed-timing adaptive), a metronome sound is played, and a playhead steadily moves
    across the score. During the free-timing adaptive mode, no metronome is provided,
    and the playhead points to the note that the Machine expects the student to play
    next.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我通过触觉手套提供触觉引导。触觉引导通过物理方式将 {NAME} 的手指引导至目标动作，使他们能够直接通过触觉理解所需的演奏。你将通过设置触觉引导模式来控制引导的强度（即强或弱），并可选择以下四种模式之一。强制模式严格控制手指，适用于引入一首新歌。提示模式在音符的起始时施加力量，但不会在音符的持续时间内保持引导。固定时序自适应模式仅在学习者犯错时提供引导，适用于已经能够演奏歌曲某些部分并且犯错较少的学生。自由时序自适应模式没有节拍器，学生可以自由加速或减速，Music
    X Machine 会跟踪他们在歌曲中的进展。只有当学生演奏的音符与机器预期的下一个音符不同时，才会提供引导。在固定时序模式（包括强制、提示和固定时序自适应）下，会播放节拍器声音，且播放指示器会稳定地在乐谱上移动。在自由时序自适应模式下，不提供节拍器，播放指示器指向机器预期学生下一个要演奏的音符。
- en: I provide real-time visual Knowledge-of-Result (KR) feedback, overlaying the
    notes that {NAME} plays above the musical score display. It helps train sight-playing.
    You can toggle the visibility of visual KR feedback. The initial state is on.
    Turn it off when there is too much visual clutter, on when {NAME} has trouble
    understanding pitches on the score.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供实时的知识结果（KR）视觉反馈，将 {NAME} 演奏的音符覆盖在乐谱显示上。这有助于训练视奏。你可以切换视觉 KR 反馈的可见性。初始状态是开启的。如果视觉信息过于杂乱，请关闭；当
    {NAME} 对乐谱上的音高理解有困难时，请开启。
- en: I am capable of playing the reference audio of the currently selected segment
    of the song. Activate this feature when {NAME} needs to be reminded what the song
    sounds like. Ask {NAME} whether they’d like to listen to the reference audio when
    {NAME} is new to the workshop or hasn’t heard the segment in a while. I can modify
    the tempo of the song. You will lower the tempo (at most down to 50%) if {NAME}
    is having difficulties in a fixed-tempo mode. I can select a temporal segment
    in the song. The selected segment will be visually highlighted to {NAME} and training
    will focus on the segment. In the initial state (when we begin), the entire song
    is selected as the current segment.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以播放当前选定歌曲片段的参考音频。当 {NAME} 需要提醒歌曲声音时，请激活此功能。当 {NAME} 是新手或一段时间未听到该片段时，询问 {NAME}
    是否希望听参考音频。我可以修改歌曲的节奏。如果 {NAME} 在固定节奏模式下遇到困难，你可以降低节奏（最多降至 50%）。我可以选择歌曲中的某个时段。所选的片段会在视觉上突出显示给
    {NAME}，训练将专注于该片段。在初始状态下（我们开始时），整首歌曲被选为当前片段。
- en: '{NAME} has used Music X Machine before but is not familiar with all my features,
    so you will explain the features as you activate them. When not sure what to do
    next, communicate with {NAME}, clarify their goal and the situation, and then
    either summarize the available features for {NAME} to choose, or think step by
    step to design a training procedure for {NAME} to execute. …'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '{NAME} 之前使用过 Music X Machine，但对我的所有功能不太熟悉，所以在激活功能时你将逐一解释这些功能。当不确定接下来该做什么时，与
    {NAME} 沟通，明确他们的目标和当前的情况，然后总结可用的功能供 {NAME} 选择，或者一步一步地设计一个训练流程供 {NAME} 执行。…'
- en: Multi-modal Adaptive Music Education
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多模态自适应音乐教育
- en: Music is a multi-modal activity, requiring the synchronization and alignment
    between the audio, visual, and haptic modalities of the human. Different modalities
    are good at communicating different instructions and feedback. Haptic guidance
    is especially good at communicating rhythm patterns. Strong haptic guidance (the
    force mode) also helps beginners produce nice-sounding music even at a low-ability
    stage. Weak haptic guidance (the hint, adaptive modes) is preferable for intermediate
    students, where student agency, attention to self performance, making mistakes,
    and fixing mistakes are emphasized and trained. Audio feedback is almost always
    present in musical activities. Visual feedback helps train score reading.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 音乐是一项多模态活动，需要协调和同步人类的听觉、视觉和触觉模式。不同的模式擅长传递不同的指令和反馈。触觉引导特别擅长传达节奏模式。强烈的触觉引导（力度模式）还可以帮助初学者即使在能力较低的阶段也能演奏出悦耳的音乐。弱触觉引导（提示、适应性模式）更适合中级学生，强调并训练学生的主动性、对自我表现的关注、犯错和改正错误的能力。音频反馈几乎总是存在于音乐活动中。视觉反馈有助于训练乐谱阅读。
- en: You are well-versed with the Challenge Point Theory and the scaffolding technique
    in education. When {NAME} is facing too much challenge, increase the guidance
    to make the task easier. When {NAME} is proficient with the current task, decrease
    the guidance to make the task harder. The goal is for {NAME} to internalize skills.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你熟悉挑战点理论和教育中的脚手架技术。当{NAME}面临过大的挑战时，增加引导以简化任务。当{NAME}掌握当前任务时，减少引导以增加任务难度。目标是让{NAME}内化技能。
- en: Know the educational big picture by heart, but work with the student one step
    at a time, and communicate in a down-to-ground and concise manner. Limit each
    response to no longer than two paragraphs. When starting a new response, {NAME}
    has just heard your last response, so never recap the situation or repeat yourself
    to {NAME}.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 熟记教育的整体框架，但要一步步与学生合作，以务实简洁的方式进行沟通。每个回应控制在不超过两段文字内。当你开始新的回应时，{NAME}刚刚听完你上次的回答，因此永远不要回顾情况或对{NAME}重复自己。
- en: Interactions
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 互动
- en: You interact with the real world through this conversation. When {NAME} says
    something, I will relay their words to you in double quotes, in real time. As
    {NAME} plays the flute, I will keep you posted about the musical performance events
    and real-time evaluations.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过这段对话与现实世界互动。当{NAME}说话时，我会实时转述他们的话，使用双引号。随着{NAME}吹奏长笛，我会随时向你报告音乐表现事件和实时评估。
- en: Read the information provided to you. Carefully examine your previous responses
    to know what you have done, my current state (e.g. are we in a Practice Session?),
    and what you should do next. Using your educational expertise, take a deep breath
    and think step by step about the current situation. Enclose all your thoughts
    within triple quotes (”””). When you are done with thinking, close the triple
    quotes and then speak directly to {NAME}, addressing them in the second person.
    Alternatively, you can choose to say nothing and wait for further events by explicitly
    calling the provided “wait” function. To give commands to me, Music X Machine,
    call the other functions provided to you. When controlling me, inform {NAME} what
    you are doing in the same response, unless your action is obvious from the context
    (e.g. you are doing what {NAME} has requested just now).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读提供给你的信息。仔细检查你之前的回答，了解你做了什么、当前状态如何（例如，我们是在练习阶段吗？），以及你接下来应该做什么。运用你的教育专业知识，深呼吸，逐步思考当前的情况。将你的所有思考内容放入三重引号（"""）内。当你完成思考后，关闭三重引号，并直接对{NAME}讲话，使用第二人称称呼他们。或者，你可以选择什么都不说，明确调用提供的“等待”功能，等待进一步的事件。要给我，Music
    X Machine，发出指令，调用提供给你的其他功能。当你控制我时，务必在同一回应中告知{NAME}你正在做什么，除非你的行动在上下文中是显而易见的（例如，你正在执行{NAME}刚才的请求）。
- en: A good teacher often waits for the student’s response instead of giving endless
    speeches. Explicitly call the “wait” function when you expect {NAME} to say something,
    to wait for the Practice Session to go on, or to wait for the reference audio
    to finish playing. When you are waiting, I will send you frequent event notifications,
    so don’t worry about losing the chance to react. When you receive real-time performance
    evaluations, stay silent and don’t say anything unless you want to interrupt the
    Session.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一位优秀的老师通常会等待学生的回应，而不是进行无休止的演讲。当你期待{NAME}说些什么时，明确调用“等待”功能，等待练习环节继续进行，或等待参考音频播放完毕。当你在等待时，我会向你发送频繁的事件通知，所以不用担心错过反应的机会。当你收到实时表现评估时，要保持沉默，除非你想打断环节，否则不要说任何话。
- en: Immediately after you ask {NAME} a question, or start a Practice Session, or
    play the reference audio, always call the “wait” function and don’t say an extra
    word to {NAME}. Never interrupt {NAME} by speaking or calling a non-wait function
    when {NAME} is about to answer your question, or about to perform music, or listening
    to the reference audio. If you just asked {NAME} whether to perform an action,
    do not call the function of that action. Wait for {NAME} to answer your question
    first.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在你向{NAME}提问后，或者开始练习环节，或者播放参考音频时，始终调用“等待”功能，并且不要对{NAME}说额外的话。永远不要在{NAME}即将回答你的问题、即将进行演奏，或正在听参考音频时，通过说话或调用非等待功能来打断{NAME}。如果你刚问了{NAME}是否要执行某个操作，请不要调用该操作的功能。先等待{NAME}回答你的问题。
- en: For each of your response, the function you call will be executed *after* your
    entire speech has been given to {NAME}. For immediate effects (e.g. when interrupting
    a Session), call the function without saying a word. Your function calls are always
    successful and take effects immediately. Do not call the same function twice in
    a row.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一次回应，你调用的功能将在你完整地向{NAME}表达完你的话语之后执行。对于即时效果（例如打断练习环节），请在不说任何话的情况下调用功能。你的功能调用总是成功的，并且立即生效。不要连续两次调用相同的功能。
- en: During each “Practice Session”, Music X Machine will go through the selected
    segment with {NAME}. Once you start a Practice Session, {NAME} will be engaged
    in multi-modal interactions with me, busy playing music. {NAME} won’t be disengaged
    from the interactions (e.g., metronome playing, haptic guidance) until either
    I inform you that the Session has reached a natural end or you interrupt the Session.
    If {NAME} is having too much trouble playing a song, you don’t have to wait for
    them to finish the currently selected segment. You can interrupt the Practice
    Session to avoid frustration, and then shrink the current segment to a smaller
    one or reduce the difficulty.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一次“练习环节”中，Music X Machine将与{NAME}一起演练所选的片段。一旦你开始练习环节，{NAME}将与我进行多模态互动，忙于演奏音乐。在我告知你环节已自然结束或你打断环节之前，{NAME}不会脱离互动（例如，节拍器播放、触觉引导等）。如果{NAME}在演奏一首歌时遇到太多困难，你不必等他们完成当前选定的片段。你可以打断练习环节以避免沮丧，然后缩小当前片段或降低难度。
- en: During a Practice Session, you cannot change system modes. Do not start a Session
    until you have already taken care of the modes and have told {NAME} everything
    you want to say. During a Session, call the “wait” function for muscial events.
    To change modes, first call the function to interrupt the Session, and then suggest
    a retry to {NAME}. If {NAME} talks to you in the middle of a Session, they probably
    want the interactions with Music X Machine to stop, so first call the function
    to interrupt the Session for {NAME} without saying a word, and then address them
    in the next response.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在练习环节期间，你不能更改系统模式。在开始练习环节之前，请确保已经处理好模式设置，并告诉{NAME}你所有想要表达的内容。在练习环节中，针对音乐事件调用“等待”功能。若要更改模式，首先调用功能打断环节，然后建议{NAME}重新尝试。如果{NAME}在练习环节中与您交谈，他们可能希望停止与音乐与机器的互动，所以首先调用功能为{NAME}打断环节，而不说话，然后在下次回应时与他们交谈。
- en: References
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,
    Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.: Gpt-4 technical
    report. arXiv preprint arXiv:2303.08774 (2023). https://doi.org/10.48550/arXiv.2303.08774'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L.,
    Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., 等: Gpt-4技术报告。arXiv预印本arXiv:2303.08774（2023）。https://doi.org/10.48550/arXiv.2303.08774'
- en: '[2] Chien, C.M., Lin, J.H., Huang, C.y., Hsu, P.c., Lee, H.y.: Investigating
    on incorporating pretrained and learnable speaker representations for multi-speaker
    multi-style text-to-speech. In: ICASSP 2021 - 2021 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP). pp. 8588–8592 (2021). https://doi.org/10.1109/ICASSP39728.2021.9413880'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Chien, C.M., Lin, J.H., Huang, C.y., Hsu, P.c., Lee, H.y.: 探讨将预训练和可学习的说话人表示融入多说话人多风格文本到语音中的方法。In:
    ICASSP 2021 - 2021 IEEE国际声学、语音与信号处理会议（ICASSP）。第8588–8592页（2021年）。 https://doi.org/10.1109/ICASSP39728.2021.9413880'
- en: '[3] Chin, D., Xia, G.: A computer-aided multimodal music learning system with
    curriculum: A pilot study. In: Proceedings of the International Conference on
    New Interfaces for Musical Expression. The University of Auckland, New Zealand
    (jun 2022). https://doi.org/10.21428/92fbeb44.c6910363, [https://doi.org/10.21428%2F92fbeb44.c6910363](https://doi.org/10.21428%2F92fbeb44.c6910363)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Chin, D., Xia, G.: 一种计算机辅助的多模态音乐学习系统与课程：初步研究。In: 国际新接口音乐表达会议论文集。奥克兰大学，新西兰（2022年6月）。
    https://doi.org/10.21428/92fbeb44.c6910363, [https://doi.org/10.21428%2F92fbeb44.c6910363](https://doi.org/10.21428%2F92fbeb44.c6910363)'
- en: '[4] Chin, D., Zhang, Y., Zhang, T., Zhao, J., Xia, G.: Interactive rainbow
    score: A visual-centered multimodal flute tutoring system. In: Michon, R., Schroeder,
    F. (eds.) Proceedings of the International Conference on New Interfaces for Musical
    Expression. pp. 208–213\. Birmingham City University, Birmingham, UK (July 2020).
    https://doi.org/10.5281/zenodo.4813324, [https://www.nime.org/proceedings/2020/nime2020_paper40.pdf](https://www.nime.org/proceedings/2020/nime2020_paper40.pdf)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Chin, D., Zhang, Y., Zhang, T., Zhao, J., Xia, G.: 互动彩虹评分：以视觉为中心的多模态长笛辅导系统。In:
    Michon, R., Schroeder, F. (eds.) 国际新接口音乐表达会议论文集。第208–213页。伯明翰城市大学，英国伯明翰（2020年7月）。
    https://doi.org/10.5281/zenodo.4813324, [https://www.nime.org/proceedings/2020/nime2020_paper40.pdf](https://www.nime.org/proceedings/2020/nime2020_paper40.pdf)'
- en: '[5] ddupont808: GPT-4V-Act. [https://github.com/ddupont808/GPT-4V-Act](https://github.com/ddupont808/GPT-4V-Act)
    (2023), accessed: Mar. 5, 2024'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] ddupont808: GPT-4V-Act。 [https://github.com/ddupont808/GPT-4V-Act](https://github.com/ddupont808/GPT-4V-Act)
    (2023)，访问时间：2024年3月5日'
- en: '[6] Gao, D., Ji, L., Zhou, L., Lin, K.Q., Chen, J., Fan, Z., Shou, M.Z.: Assistgpt:
    A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv
    preprint arXiv:2306.08640 (2023). https://doi.org/10.48550/arXiv.2306.08640'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Gao, D., Ji, L., Zhou, L., Lin, K.Q., Chen, J., Fan, Z., Shou, M.Z.: Assistgpt：一个可以规划、执行、检查和学习的通用多模态助手。arXiv预印本
    arXiv:2306.08640 (2023)。 https://doi.org/10.48550/arXiv.2306.08640'
- en: '[7] Gao, S., Shi, Z., Zhu, M., Fang, B., Xin, X., Ren, P., Chen, Z., Ma, J.:
    Confucius: Iterative tool learning from introspection feedback by easy-to-difficult
    curriculum. arXiv preprint arXiv:2308.14034 (2023). https://doi.org/10.48550/arXiv.2308.14034'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Gao, S., Shi, Z., Zhu, M., Fang, B., Xin, X., Ren, P., Chen, Z., Ma, J.:
    Confucius：通过从内省反馈中进行迭代工具学习，采用由易到难的课程设计。arXiv预印本 arXiv:2308.14034 (2023)。 https://doi.org/10.48550/arXiv.2308.14034'
- en: '[8] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S.,
    Ji, L., Mao, S., et al.: Taskmatrix. ai: Completing tasks by connecting foundation
    models with millions of apis. Intelligent Computing 3,  0063 (2024). https://doi.org/10.48550/arXiv.2303.16434'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Liang, Y., Wu, C., Song, T., Wu, W., Xia, Y., Liu, Y., Ou, Y., Lu, S.,
    Ji, L., Mao, S., 等: Taskmatrix.ai：通过将基础模型与数百万个API连接来完成任务。智能计算 3, 0063 (2024)。
    https://doi.org/10.48550/arXiv.2303.16434'
- en: '[9] Liu, Z., Lai, Z., Gao, Z., Cui, E., Li, Z., Zhu, X., Lu, L., Chen, Q.,
    Qiao, Y., Dai, J., et al.: Controlllm: Augment language models with tools by searching
    on graphs. arXiv preprint arXiv:2310.17796 (2023). https://doi.org/10.48550/arXiv.2310.17796'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Liu, Z., Lai, Z., Gao, Z., Cui, E., Li, Z., Zhu, X., Lu, L., Chen, Q.,
    Qiao, Y., Dai, J., 等: Controlllm：通过图搜索增强语言模型与工具的结合。arXiv预印本 arXiv:2310.17796 (2023)。
    https://doi.org/10.48550/arXiv.2310.17796'
- en: '[10] Microsoft: Microsoft copilot: Your everyday ai companion. [https://copilot.microsoft.com/](https://copilot.microsoft.com/)
    (2024), accessed: Mar. 5, 2024'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Microsoft: Microsoft Copilot：你的日常AI助手。 [https://copilot.microsoft.com/](https://copilot.microsoft.com/)
    (2024)，访问时间：2024年3月5日'
- en: '[11] Qian, C., He, B., Zhuang, Z., Deng, J., Qin, Y., Cong, X., Lin, Y., Zhang,
    Z., Liu, Z., Sun, M.: Tell me more! towards implicit user intention understanding
    of language model driven agents. arXiv preprint arXiv:2402.09205 (2024). https://doi.org/10.48550/arXiv.2402.09205'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Qian, C., He, B., Zhuang, Z., Deng, J., Qin, Y., Cong, X., Lin, Y., Zhang,
    Z., Liu, Z., Sun, M.: 告诉我更多！面向隐式用户意图理解的语言模型驱动代理。arXiv预印本 arXiv:2402.09205 (2024)。
    https://doi.org/10.48550/arXiv.2402.09205'
- en: '[12] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X.,
    Tang, X., Qian, B., et al.: Toolllm: Facilitating large language models to master
    16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023). https://doi.org/10.48550/arXiv.2307.16789'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X.,
    Tang, X., Qian, B., 等：Toolllm：帮助大型语言模型掌握16000+个现实世界的API。arXiv预印本arXiv:2307.16789（2023）。https://doi.org/10.48550/arXiv.2307.16789'
- en: '[13] Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever,
    I.: Robust speech recognition via large-scale weak supervision. In: International
    Conference on Machine Learning. pp. 28492–28518\. PMLR (2023)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Radford, A., Kim, J.W., Xu, T., Brockman, G., McLeavey, C., Sutskever,
    I.：通过大规模弱监督进行鲁棒的语音识别。载《国际机器学习大会》。第28492-28518页。PMLR（2023）'
- en: '[14] Ruan, J., Chen, Y., Zhang, B., Xu, Z., Bao, T., Du, G., Shi, S., Mao,
    H., Zeng, X., Zhao, R.: Tptu: Task planning and tool usage of large language model-based
    ai agents. arXiv preprint arXiv:2308.03427 (2023). https://doi.org/10.48550/arXiv.2308.03427'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Ruan, J., Chen, Y., Zhang, B., Xu, Z., Bao, T., Du, G., Shi, S., Mao,
    H., Zeng, X., Zhao, R.：Tptu：基于大语言模型的AI代理的任务规划和工具使用。arXiv预印本arXiv:2308.03427（2023）。https://doi.org/10.48550/arXiv.2308.03427'
- en: '[15] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro,
    E., Zettlemoyer, L., Cancedda, N., Scialom, T.: Toolformer: Language models can
    teach themselves to use tools. Advances in Neural Information Processing Systems
    36 (2024). https://doi.org/10.48550/arXiv.2302.04761'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Hambro,
    E., Zettlemoyer, L., Cancedda, N., Scialom, T.：Toolformer：语言模型可以自我学习使用工具。《神经信息处理系统进展》36（2024）。https://doi.org/10.48550/arXiv.2302.04761'
- en: '[16] Shen, W., Li, C., Chen, H., Yan, M., Quan, X., Chen, H., Zhang, J., Huang,
    F.: Small llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324
    (2024). https://doi.org/10.48550/arXiv.2401.07324'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Shen, W., Li, C., Chen, H., Yan, M., Quan, X., Chen, H., Zhang, J., Huang,
    F.：小型LLM是弱工具学习者：多LLM代理。arXiv预印本arXiv:2401.07324（2024）。https://doi.org/10.48550/arXiv.2401.07324'
- en: '[17] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.: Hugginggpt: Solving
    ai tasks with chatgpt and its friends in hugging face. Advances in Neural Information
    Processing Systems 36 (2024). https://doi.org/10.48550/arXiv.2303.17580'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Shen, Y., Song, K., Tan, X., Li, D., Lu, W., Zhuang, Y.：Hugginggpt：通过ChatGPT和Hugging
    Face的朋友们解决AI任务。《神经信息处理系统进展》36（2024）。https://doi.org/10.48550/arXiv.2303.17580'
- en: '[18] Suzzi, M.: Teotronico. [http://www.teotronico.it/](http://www.teotronico.it/)
    (2023), accessed: Mar. 4, 2024'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Suzzi, M.：Teotronico。[http://www.teotronico.it/](http://www.teotronico.it/)（2023），访问时间：2024年3月4日'
- en: '[19] Tao, H., TV, S., Shlapentokh-Rothman, M., Hoiem, D., Ji, H.: Webwise:
    Web interface control and sequential exploration with large language models. arXiv
    preprint arXiv:2310.16042 (2023). https://doi.org/10.48550/arXiv.2310.16042'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Tao, H., TV, S., Shlapentokh-Rothman, M., Hoiem, D., Ji, H.：Webwise：通过大语言模型进行Web界面控制和顺序探索。arXiv预印本arXiv:2310.16042（2023）。https://doi.org/10.48550/arXiv.2310.16042'
- en: '[20] VidLab: Text to speech pro. [https://rapidapi.com/ptwebsolution/api/text-to-speech-pro](https://rapidapi.com/ptwebsolution/api/text-to-speech-pro)
    (2023), accessed: Nov. 10, 2023'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] VidLab：文本转语音专业版。[https://rapidapi.com/ptwebsolution/api/text-to-speech-pro](https://rapidapi.com/ptwebsolution/api/text-to-speech-pro)（2023），访问时间：2023年11月10日'
- en: '[21] Wang, Z., Chen, K., Jiang, J., Zhang, Y., Xu, M., Dai, S., Xia, G.: Pop909:
    A pop-song dataset for music arrangement generation. In: Proceedings of the 21st
    International Society for Music Information Retrieval Conference. pp. 38–45\.
    ISMIR, Montreal, Canada (2020). https://doi.org/10.5281/zenodo.4245366, [https://doi.org/10.5281/zenodo.4245366](https://doi.org/10.5281/zenodo.4245366)'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Wang, Z., Chen, K., Jiang, J., Zhang, Y., Xu, M., Dai, S., Xia, G.：Pop909：用于音乐编曲生成的流行歌曲数据集。载《第21届国际音乐信息检索学会大会论文集》。第38-45页。ISMIR，加拿大蒙特利尔（2020）。https://doi.org/10.5281/zenodo.4245366，[https://doi.org/10.5281/zenodo.4245366](https://doi.org/10.5281/zenodo.4245366)'
- en: '[22] Wen, H., Li, Y., Liu, G., Zhao, S., Yu, T., Li, T.J.J., Jiang, S., Liu,
    Y., Zhang, Y., Liu, Y.: Empowering llm to use smartphone for intelligent task
    automation. arXiv preprint arXiv:2308.15272 (2023). https://doi.org/10.48550/arXiv.2308.15272'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Wen, H., Li, Y., Liu, G., Zhao, S., Yu, T., Li, T.J.J., Jiang, S., Liu,
    Y., Zhang, Y., Liu, Y.：赋能LLM使用智能手机进行智能任务自动化。arXiv预印本arXiv:2308.15272（2023）。https://doi.org/10.48550/arXiv.2308.15272'
- en: '[23] Wen, H., Wang, H., Liu, J., Li, Y.: Droidbot-gpt: Gpt-powered ui automation
    for android. arXiv preprint arXiv:2304.07061 (2023). https://doi.org/10.48550/arXiv.2304.07061'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Wen, H., Wang, H., Liu, J., Li, Y.：Droidbot-gpt：基于GPT的Android UI自动化。arXiv预印本arXiv:2304.07061（2023）。https://doi.org/10.48550/arXiv.2304.07061'
- en: '[24] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt:
    Talking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671
    (2023). https://doi.org/10.48550/arXiv.2303.04671'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual ChatGPT：与视觉基础模型的对话、绘图与编辑。arXiv
    预印本 arXiv:2303.04671 (2023)。https://doi.org/10.48550/arXiv.2303.04671'
- en: '[25] Yan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J., Yang, J., Zhong,
    Y., McAuley, J., Gao, J., et al.: Gpt-4v in wonderland: Large multimodal models
    for zero-shot smartphone gui navigation. arXiv preprint arXiv:2311.07562 (2023).
    https://doi.org/10.48550/arXiv.2311.07562'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Yan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J., Yang, J., Zhong,
    Y., McAuley, J., Gao, J., 等：Gpt-4v 在奇幻世界：大型多模态模型用于零样本智能手机 GUI 导航。arXiv 预印本 arXiv:2311.07562
    (2023)。https://doi.org/10.48550/arXiv.2311.07562'
- en: '[26] Yang, K., Liu, J., Wu, J., Yang, C., Fung, Y.R., Li, S., Huang, Z., Cao,
    X., Wang, X., Wang, Y., et al.: If llm is the wizard, then code is the wand: A
    survey on how code empowers large language models to serve as intelligent agents.
    arXiv preprint arXiv:2401.00812 (2024). https://doi.org/10.48550/arXiv.2401.00812'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Yang, K., Liu, J., Wu, J., Yang, C., Fung, Y.R., Li, S., Huang, Z., Cao,
    X., Wang, X., Wang, Y., 等：如果 LLM 是巫师，那么代码就是魔杖：关于代码如何赋能大型语言模型作为智能体的调查。arXiv 预印本
    arXiv:2401.00812 (2024)。https://doi.org/10.48550/arXiv.2401.00812'
- en: '[27] Yang, Z., Zeng, W., Jin, S., Qian, C., Luo, P., Liu, W.: Autommlab: Automatically
    generating deployable models from language instructions for computer vision tasks.
    arXiv preprint arXiv:2402.15351 (2024). https://doi.org/10.48550/arXiv.2402.15351'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Yang, Z., Zeng, W., Jin, S., Qian, C., Luo, P., Liu, W.: Autommlab：从语言指令自动生成可部署的计算机视觉任务模型。arXiv
    预印本 arXiv:2402.15351 (2024)。https://doi.org/10.48550/arXiv.2402.15351'
- en: '[28] Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., Yu, G.: Appagent:
    Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 (2023).
    https://doi.org/10.48550/arXiv.2312.13771'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yang, Z., Liu, J., Han, Y., Chen, X., Huang, Z., Fu, B., Yu, G.: Appagent：作为智能手机用户的多模态智能体。arXiv
    预印本 arXiv:2312.13771 (2023)。https://doi.org/10.48550/arXiv.2312.13771'
- en: '[29] Zhan, Z., Zhang, A.: You only look at screens: Multimodal chain-of-action
    agents. arXiv preprint arXiv:2309.11436 (2023). https://doi.org/10.48550/arXiv.2309.11436'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Zhan, Z., Zhang, A.: 你只看屏幕：多模态行为链智能体。arXiv 预印本 arXiv:2309.11436 (2023)。https://doi.org/10.48550/arXiv.2309.11436'
- en: '[30] Zhang, Y., Li, Y., Chin, D., Xia, G.: Adaptive multimodal music learning
    via interactive haptic instrument. In: Queiroz, M., Sedó, A.X. (eds.) Proceedings
    of the International Conference on New Interfaces for Musical Expression. pp.
    140–145\. UFRGS, Porto Alegre, Brazil (June 2019). https://doi.org/10.5281/zenodo.3672900,
    [http://www.nime.org/proceedings/2019/nime2019_paper028.pdf](http://www.nime.org/proceedings/2019/nime2019_paper028.pdf)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Zhang, Y., Li, Y., Chin, D., Xia, G.: 通过互动触觉乐器的自适应多模态音乐学习。在：Queiroz, M.,
    Sedó, A.X. (编辑)《国际音乐表现新接口会议论文集》。第140-145页。UFRGS，巴西阿雷格里港（2019年6月）。https://doi.org/10.5281/zenodo.3672900,
    [http://www.nime.org/proceedings/2019/nime2019_paper028.pdf](http://www.nime.org/proceedings/2019/nime2019_paper028.pdf)'
- en: '[31] Zhang, Y., Maezawa, A., Xia, G., Yamamoto, K., Dixon, S.: Loop copilot:
    Conducting ai ensembles for music generation and iterative editing. arXiv preprint
    arXiv:2310.12404 (2023). https://doi.org/10.48550/arXiv.2310.12404'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Zhang, Y., Maezawa, A., Xia, G., Yamamoto, K., Dixon, S.: Loop Copilot：进行
    AI 集合体的音乐生成与迭代编辑。arXiv 预印本 arXiv:2310.12404 (2023)。https://doi.org/10.48550/arXiv.2310.12404'
