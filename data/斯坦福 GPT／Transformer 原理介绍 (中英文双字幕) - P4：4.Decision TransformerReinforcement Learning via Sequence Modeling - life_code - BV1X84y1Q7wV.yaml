- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P4ï¼š4.Decision TransformerReinforcement Learning
    via Sequence Modeling - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P4ï¼š4.å†³ç­–å˜æ¢å™¨ é€šè¿‡åºåˆ—å»ºæ¨¡è¿›è¡Œå¼ºåŒ–å­¦ä¹  - life_code - BV1X84y1Q7wV
- en: '![](img/041a2ae7aae080f075f80cd7c4c33a68_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/041a2ae7aae080f075f80cd7c4c33a68_0.png)'
- en: So I'm excited to talk today about our recent work on using Transers for reinforcement
    learning and this is joint work with a bunch of really exciting collaboratorsã€‚most
    of them at UC Berkeley and some of them at Facebook and Googleã€‚I should mention
    this work was led by way to talented undergradsï¼Œ Li Chen and Kevin Blueã€‚![](img/041a2ae7aae080f075f80cd7c4c33a68_2.png)
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆé«˜å…´ä»Šå¤©èƒ½è°ˆè®ºæˆ‘ä»¬æœ€è¿‘å…³äºä½¿ç”¨å˜æ¢å™¨è¿›è¡Œå¼ºåŒ–å­¦ä¹ çš„å·¥ä½œï¼Œè¿™é¡¹å·¥ä½œæ˜¯ä¸ä¸€ç¾¤éå¸¸ä¼˜ç§€çš„åˆä½œè€…å…±åŒå®Œæˆçš„ï¼Œä»–ä»¬å¤§å¤šåœ¨åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ï¼Œæœ‰ä¸€äº›åœ¨Facebookå’ŒGoogleã€‚æˆ‘åº”è¯¥æåˆ°è¿™é¡¹å·¥ä½œæ˜¯ç”±ä¸¤ä½æ‰åæ¨ªæº¢çš„æœ¬ç§‘ç”Ÿææ™¨å’Œå‡¯æ–‡Â·å¸ƒé²é¢†å¯¼çš„ã€‚![](img/041a2ae7aae080f075f80cd7c4c33a68_2.png)
- en: And I'm excited to present the results we hadï¼Œ so let's try to motivate why
    we even care about this problemã€‚So we have seen in the lastã€‚Three or four yearsï¼Œ
    that transformers since the introduction in 2017 have taken over lots and lots
    of different fields of artificial intelligenceã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆé«˜å…´èƒ½å±•ç¤ºæˆ‘ä»¬æ‰€å–å¾—çš„æˆæœï¼Œæ‰€ä»¥è®©æˆ‘ä»¬æ¥åŠ¨æœºä¸ºä»€ä¹ˆæˆ‘ä»¬ç”šè‡³å…³å¿ƒè¿™ä¸ªé—®é¢˜ã€‚åœ¨è¿‡å»çš„ä¸‰å››å¹´é‡Œï¼Œè‡ª2017å¹´å¼•å…¥å˜æ¢å™¨ä»¥æ¥ï¼Œå·²ç»åœ¨äººå·¥æ™ºèƒ½çš„è®¸å¤šä¸åŒé¢†åŸŸä¸­å æ®äº†é‡è¦åœ°ä½ã€‚
- en: so we saw them having a big impact for language processingï¼Œ we saw them being
    used for visionã€‚the vision transformer very recently they were in nature trying
    to solve protein folding and very soon they might just replace as computer scientists
    by having automatically generate codeã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çœ‹åˆ°å®ƒä»¬å¯¹è¯­è¨€å¤„ç†äº§ç”Ÿäº†é‡å¤§å½±å“ï¼Œçœ‹åˆ°å®ƒä»¬è¢«ç”¨äºè§†è§‰å¤„ç†ã€‚æœ€è¿‘ï¼Œè§†è§‰å˜æ¢å™¨åœ¨ã€Šè‡ªç„¶ã€‹æ‚å¿—ä¸Šå°è¯•è§£å†³è›‹ç™½è´¨æŠ˜å é—®é¢˜ï¼Œå¾ˆå¿«å®ƒä»¬å¯èƒ½ä¼šå–ä»£è®¡ç®—æœºç§‘å­¦å®¶ï¼Œè‡ªåŠ¨ç”Ÿæˆä»£ç ã€‚
- en: So with all of these advancesï¼Œ it seems like we are getting closer to having
    a unified model for decision making for artificial intelligenceã€‚But artificial
    intelligence is much more about not just having perceptionã€‚but also using the
    perception knowledge to make decisionsã€‚And this is what this talk is going to
    be aboutã€‚But before I go into actually thinking about how we will use these models
    decision makingã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: éšç€è¿™äº›è¿›å±•ï¼Œä¼¼ä¹æˆ‘ä»¬è¶Šæ¥è¶Šæ¥è¿‘äºä¸ºäººå·¥æ™ºèƒ½å»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„å†³ç­–æ¨¡å‹ã€‚ä½†äººå·¥æ™ºèƒ½ä¸ä»…ä»…æ¶‰åŠæ„ŸçŸ¥ï¼Œè¿˜æ¶‰åŠå¦‚ä½•åˆ©ç”¨æ„ŸçŸ¥çŸ¥è¯†åšå‡ºå†³ç­–ã€‚è¿™å°±æ˜¯æœ¬æ¬¡æ¼”è®²çš„ä¸»é¢˜ã€‚ä½†åœ¨æˆ‘æ·±å…¥æ¢è®¨å¦‚ä½•ä½¿ç”¨è¿™äº›æ¨¡å‹è¿›è¡Œå†³ç­–ä¹‹å‰ã€‚
- en: here is a motivation for why I think it is important to ask this questionã€‚So
    unlike models for RLã€‚when we look at transformers for perception modalities like
    I showed in the previous slideã€‚we find that these models are very scalable and
    have very stable training dynamics so you can keep as long as you have enough
    computation and you have more and more data that can be sourcedã€‚you can train
    bigger and bigger models and you'll see very smooth reductions in the lossã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æˆ‘è®¤ä¸ºè¯¢é—®è¿™ä¸ªé—®é¢˜é‡è¦æ€§çš„åŠ¨æœºã€‚å› æ­¤ï¼Œä¸å¼ºåŒ–å­¦ä¹ æ¨¡å‹ä¸åŒï¼Œå½“æˆ‘ä»¬æŸ¥çœ‹å˜æ¢å™¨åœ¨æ„ŸçŸ¥æ¨¡å¼ä¸Šçš„è¡¨ç°æ—¶ï¼Œå°±åƒæˆ‘åœ¨å‰ä¸€å¼ å¹»ç¯ç‰‡ä¸­å±•ç¤ºçš„é‚£æ ·ï¼Œæˆ‘ä»¬å‘ç°è¿™äº›æ¨¡å‹éå¸¸å¯æ‰©å±•ï¼Œå¹¶ä¸”å…·æœ‰éå¸¸ç¨³å®šçš„è®­ç»ƒåŠ¨æ€ã€‚å› æ­¤ï¼Œåªè¦ä½ æœ‰è¶³å¤Ÿçš„è®¡ç®—èƒ½åŠ›å’Œè¶Šæ¥è¶Šå¤šçš„å¯è·å–æ•°æ®ï¼Œä½ å°±å¯ä»¥è®­ç»ƒè¶Šæ¥è¶Šå¤§çš„æ¨¡å‹ï¼Œå¹¶ä¸”å¯ä»¥çœ‹åˆ°æŸå¤±å¹³æ»‘ä¸‹é™ã€‚
- en: And the overall training dynamics are very stableï¼Œ and this makes it very easy
    for practitioners and researchers to build these models and learn richer and richer
    distributionsã€‚So like I saidï¼Œ all of these advances have so far occurred in perception
    what we've been interested in this talk is to think about how we can go from perceptionã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ•´ä½“è®­ç»ƒåŠ¨æ€éå¸¸ç¨³å®šï¼Œè¿™ä½¿å¾—ä»ä¸šè€…å’Œç ”ç©¶äººå‘˜æ„å»ºè¿™äº›æ¨¡å‹å¹¶å­¦ä¹ æ›´ä¸°å¯Œçš„åˆ†å¸ƒå˜å¾—éå¸¸ç®€å•ã€‚å› æ­¤ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™äº›è¿›å±•è¿„ä»Šä¸ºæ­¢ä¸»è¦å‘ç”Ÿåœ¨æ„ŸçŸ¥æ–¹é¢ï¼Œè€Œæˆ‘ä»¬åœ¨æœ¬æ¬¡æ¼”è®²ä¸­å…³æ³¨çš„æ˜¯å¦‚ä½•ä»æ„ŸçŸ¥è½¬å‘ã€‚
- en: looking at imagesï¼Œ looking at text and all these kinds of sensory signals to
    then going into the field of actually taking actions and making our agents do
    interesting things in the worldã€‚ğŸ˜Šï¼Œå•Šã€‚And here throughout the talkï¼Œ we should be
    thinking about why this perspective is going to enable us to do scalable learningã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éœ€è¦è§‚å¯Ÿå›¾åƒã€æ–‡æœ¬å’Œå„ç§æ„Ÿå®˜ä¿¡å·ï¼Œç„¶åè¿›å…¥å®é™…é‡‡å–è¡ŒåŠ¨çš„é¢†åŸŸï¼Œè®©æˆ‘ä»¬çš„æ™ºèƒ½ä½“åœ¨ä¸–ç•Œä¸­åšå‡ºæœ‰è¶£çš„äº‹æƒ…ã€‚ğŸ˜Šï¼Œå•Šã€‚åœ¨æ•´ä¸ªæ¼”è®²ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘ä¸ºä»€ä¹ˆè¿™ç§è§†è§’èƒ½å¤Ÿè®©æˆ‘ä»¬å®ç°å¯æ‰©å±•å­¦ä¹ ã€‚
- en: like I showed on the previous slideï¼Œ as well as brings stability into the whole
    procedureã€‚ğŸ˜Šã€‚So sequential decision making is a very broad area and what I'm specifically
    going to be focusing on today is the one route to sequential decision making that's
    reinforcement learningã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘åœ¨å‰ä¸€å¼ å¹»ç¯ç‰‡ä¸Šå±•ç¤ºçš„é‚£æ ·ï¼Œä¹Ÿä¸ºæ•´ä¸ªè¿‡ç¨‹å¸¦æ¥äº†ç¨³å®šæ€§ã€‚ğŸ˜Šã€‚å› æ­¤ï¼Œé¡ºåºå†³ç­–æ˜¯ä¸€ä¸ªéå¸¸å¹¿æ³›çš„é¢†åŸŸï¼Œæˆ‘ä»Šå¤©è¦ç‰¹åˆ«å…³æ³¨çš„æ˜¯é€šå¾€é¡ºåºå†³ç­–çš„ä¸€ä¸ªè·¯å¾„ï¼Œå³å¼ºåŒ–å­¦ä¹ ã€‚
- en: So just as a brief backgroundï¼Œ what is reinforcement learningã€‚so we are given
    an agent who' is in a current state and the agent is going to interact with the
    environment by taking actionsã€‚And by taking these actionsï¼Œ the environment is
    going to return to it a rewardã€‚For how good that action was as well as next state
    into which the agent will transition and this whole feedback loop will continueã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç®€å•ä»‹ç»ä¸€ä¸‹ï¼Œä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ã€‚æˆ‘ä»¬ç»™å®šä¸€ä¸ªå¤„äºå½“å‰çŠ¶æ€çš„ä»£ç†ï¼Œè¯¥ä»£ç†å°†é€šè¿‡é‡‡å–è¡ŒåŠ¨ä¸ç¯å¢ƒè¿›è¡Œäº’åŠ¨ã€‚é€šè¿‡é‡‡å–è¿™äº›è¡ŒåŠ¨ï¼Œç¯å¢ƒå°†ç»™å®ƒè¿”å›ä¸€ä¸ªå¥–åŠ±ï¼Œä»¥è¯„ä¼°è¯¥è¡ŒåŠ¨çš„å¥½åï¼Œä»¥åŠä»£ç†å°†è½¬ç§»åˆ°çš„ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œè¿™æ•´ä¸ªåé¦ˆå¾ªç¯å°†ç»§ç»­è¿›è¡Œã€‚
- en: å•Šã€‚The goal here for an intelligent agent is to then using trial and errorã€‚so
    try out different actionsï¼Œ see what rewards will lead to learn a policy which
    maps your states to actions such that the policy maximizes the agent's cumulative
    rewards over time horizonã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: å•Šã€‚è¿™é‡Œæ™ºèƒ½ä»£ç†çš„ç›®æ ‡æ˜¯é€šè¿‡è¯•é”™æ¥å®ç°ã€‚æ‰€ä»¥å°è¯•ä¸åŒçš„åŠ¨ä½œï¼Œçœ‹çœ‹ä»€ä¹ˆå¥–åŠ±èƒ½å¤Ÿå¼•å¯¼ï¼Œå­¦ä¹ ä¸€ç§å°†çŠ¶æ€æ˜ å°„åˆ°åŠ¨ä½œçš„ç­–ç•¥ï¼Œä»¥ä¾¿æœ€å¤§åŒ–ä»£ç†åœ¨æ—¶é—´è·¨åº¦å†…çš„ç´¯ç§¯å¥–åŠ±ã€‚
- en: So you take a sequence of actions and then based on the reward you accumulate
    for that sequence of actionsã€‚will' judge how good your policy isã€‚This talk is
    also going to be specifically focused on a form of reinforcement learning that
    goes by the name of offline reinforcement learningã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ è¿›è¡Œä¸€ç³»åˆ—åŠ¨ä½œï¼Œç„¶åæ ¹æ®ä½ ä¸ºè¿™ä¸€ç³»åˆ—åŠ¨ä½œç§¯ç´¯çš„å¥–åŠ±æ¥åˆ¤æ–­ä½ çš„ç­–ç•¥æœ‰å¤šå¥½ã€‚è¿™æ¬¡è®²åº§è¿˜å°†ç‰¹åˆ«å…³æ³¨ä¸€ç§åä¸ºç¦»çº¿å¼ºåŒ–å­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ å½¢å¼ã€‚
- en: So the idea here is that what changes from the previous picture where I was
    talking about online learningã€‚online reinforcement learning is that here now instead
    of doing actively interacting with the environmentã€‚you have a collection of log
    data of interactions so think about some robot that's going out in the fields
    and it collects a bunch of sensory data and you have all logged it and using that
    log data you now want to train another agentã€‚it could be another robot to then
    learn something interesting about that environment just by looking at the log
    dataã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œå’Œæˆ‘ä¹‹å‰è°ˆè®ºåœ¨çº¿å­¦ä¹ çš„å›¾åƒç›¸æ¯”ï¼Œåœ¨çº¿å¼ºåŒ–å­¦ä¹ çš„å˜åŒ–åœ¨äºï¼Œç°åœ¨ä¸å†æ˜¯ä¸»åŠ¨ä¸ç¯å¢ƒäº’åŠ¨ï¼Œè€Œæ˜¯æ‹¥æœ‰ä¸€ç»„äº¤äº’çš„æ—¥å¿—æ•°æ®ã€‚æƒ³è±¡ä¸€ä¸‹æŸä¸ªæœºå™¨äººåœ¨ç”°é‡ä¸­æ”¶é›†äº†ä¸€å †ä¼ æ„Ÿå™¨æ•°æ®ï¼Œå¹¶ä¸”ä½ å·²ç»è®°å½•äº†æ‰€æœ‰è¿™äº›æ•°æ®ï¼Œåˆ©ç”¨è¿™äº›æ—¥å¿—æ•°æ®ï¼Œä½ ç°åœ¨æƒ³è®­ç»ƒå¦ä¸€ä¸ªä»£ç†ï¼Œä¹Ÿè®¸æ˜¯å¦ä¸€ä¸ªæœºå™¨äººï¼Œä»…ä»…é€šè¿‡æŸ¥çœ‹æ—¥å¿—æ•°æ®æ¥å­¦ä¹ å…³äºè¯¥ç¯å¢ƒçš„æœ‰è¶£ä¿¡æ¯ã€‚
- en: G so theres no trial and error componentã€‚Which is currently one of the extensions
    of this frameworkã€‚which would be very excitingï¼Œ so I'll talk about this towards
    the end of the talk why it's exciting to think about how we can extend this framework
    to include an exploration component and have trial and errorã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Gï¼Œå› æ­¤æ²¡æœ‰è¯•é”™æˆåˆ†ã€‚è¿™ç›®å‰æ˜¯è¯¥æ¡†æ¶çš„ä¸€ä¸ªæ‰©å±•ï¼Œå°†ä¼šéå¸¸æ¿€åŠ¨äººå¿ƒï¼Œæ‰€ä»¥æˆ‘å°†åœ¨è®²åº§ç»“æŸæ—¶è°ˆè°ˆä¸ºä»€ä¹ˆæ€è€ƒå¦‚ä½•æ‰©å±•è¿™ä¸ªæ¡†æ¶ä»¥åŒ…æ‹¬æ¢ç´¢æˆåˆ†å¹¶è¿›è¡Œè¯•é”™æ˜¯ä»¤äººå…´å¥‹çš„ã€‚
- en: Okayï¼Œ so not to go more concretely into what the motivating challenge of the
    stock was now that they have introduced ArLã€‚so let's look at some statisticsã€‚So
    large language modelsã€‚I have billions of parameters and they have today they have
    roughly about 100 layers in Transformerã€‚they are very stable to train using supervised
    learningï¼Œ style lossesã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä¸å†å…·ä½“è®¨è®ºè‚¡ç¥¨çš„æ¿€åŠ±æŒ‘æˆ˜ï¼Œæ—¢ç„¶ä»–ä»¬å·²ç»å¼•å…¥äº†ArLã€‚é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹ä¸€äº›ç»Ÿè®¡æ•°æ®ã€‚å¤§å‹è¯­è¨€æ¨¡å‹æ‹¥æœ‰æ•°åäº¿çš„å‚æ•°ï¼Œä»Šå¤©å®ƒä»¬å¤§çº¦æœ‰100å±‚çš„Transformerã€‚å®ƒä»¬åœ¨ä½¿ç”¨ç›‘ç£å­¦ä¹ å’Œé£æ ¼æŸå¤±æ—¶éå¸¸ç¨³å®šã€‚
- en: which are the building blocks of autoregressive generationï¼Œ for instanceã€‚or
    for mass language modeling as in Bã€‚And this is like a field that's growing every
    day and there's a course on it at Stanford that we're all taking just because
    it has had such a monumental impact on AIã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯è‡ªå›å½’ç”Ÿæˆçš„æ„å»ºå—ï¼Œä¾‹å¦‚ï¼Œæˆ–è€…ç”¨äºå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡çš„Bæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯å¤©éƒ½åœ¨å¢é•¿çš„é¢†åŸŸï¼Œæ–¯å¦ç¦å¤§å­¦æœ‰ä¸€é—¨è¯¾ç¨‹ï¼Œæˆ‘ä»¬éƒ½åœ¨å‚åŠ ï¼Œå› ä¸ºå®ƒå¯¹äººå·¥æ™ºèƒ½äº§ç”Ÿäº†å·¨å¤§çš„å½±å“ã€‚
- en: å—¯ã€‚RL policiesï¼Œ on the other handï¼Œ and I'm talking about deep RLã€‚the maximum
    they would extend to is maybe millions of parameters at 20 layersã€‚å—¯ã€‚And what's
    really unnerving is that they're very unstable to train so the current algorithms
    for reinforcement learning they build on mostly dynamic programming which involves
    solving an inter loop optimization problem that's very unstable and it's very
    common to see practitioners in RL looking at reward reward code that look like
    this so what I really want you to see here is the variance in the returns that
    we tend to get in RL it's really huge even after doing multiple rounds of experimentation
    and that's often that is really at the code got to done with the fact thatã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚å¦ä¸€æ–¹é¢ï¼Œå¼ºåŒ–å­¦ä¹ ç­–ç•¥ï¼Œå°¤å…¶æ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œå…¶æœ€å¤§å‚æ•°é‡å¤§æ¦‚åœ¨2000ä¸‡ä¸ªå‚æ•°å’Œ20å±‚ä¹‹é—´ã€‚å—¯ã€‚æ›´è®©äººä¸å®‰çš„æ˜¯ï¼Œå®ƒä»¬çš„è®­ç»ƒéå¸¸ä¸ç¨³å®šï¼Œå› æ­¤ç›®å‰çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸»è¦åŸºäºåŠ¨æ€ç¼–ç¨‹ï¼Œæ¶‰åŠè§£å†³ä¸€ä¸ªéå¸¸ä¸ç¨³å®šçš„å†…å¾ªç¯ä¼˜åŒ–é—®é¢˜ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼Œå®è·µè€…ç»å¸¸çœ‹åˆ°è¿™æ ·çš„å¥–åŠ±ä»£ç ï¼Œæ‰€ä»¥æˆ‘å¸Œæœ›ä½ èƒ½çœ‹åˆ°åœ¨RLä¸­æˆ‘ä»¬å¾—åˆ°çš„å›æŠ¥çš„æ–¹å·®éå¸¸å¤§ï¼Œå³ä½¿åœ¨è¿›è¡Œå¤šè½®å®éªŒåï¼Œè¿™å¾€å¾€ä¸ä»£ç æœ¬èº«çš„å¤æ‚æ€§æœ‰å…³ã€‚
- en: Our algorithms learning checks need better improvements so that the performance
    can be stably achieved by Asiansã€‚In complex environmentsã€‚So what this work is
    hoping to do is it's meant to introduce transformersã€‚And I'll first show in one
    slide what exactly that model looks like and we're going to go into deeper details
    of each of the componentsã€‚I have good questionã€‚Yeahï¼Œ I can ask a question real
    quickã€‚Yesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„ç®—æ³•å­¦ä¹ æ£€æŸ¥éœ€è¦æ›´å¥½çš„æ”¹è¿›ï¼Œä»¥ä¾¿åœ¨å¤æ‚ç¯å¢ƒä¸­å®ç°ç¨³å®šçš„æ€§èƒ½ã€‚å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œçš„ç›®çš„æ˜¯å¼•å…¥å˜æ¢å™¨ã€‚æˆ‘ä¼šå…ˆåœ¨ä¸€å¼ å¹»ç¯ç‰‡ä¸­å±•ç¤ºè¿™ä¸ªæ¨¡å‹çš„å…·ä½“æ ·å­ï¼Œç„¶åæ·±å…¥è®²è§£æ¯ä¸ªç»„ä»¶ã€‚æˆ‘æœ‰ä¸ªå¥½é—®é¢˜ã€‚æ˜¯çš„ï¼Œæˆ‘å¯ä»¥å¿«é€Ÿé—®ä¸€ä¸ªé—®é¢˜ã€‚å¥½çš„ã€‚
- en: and thank you what I'm curious to know is is the what is the cause for why RL
    typically has several orders of magnitude fewer parametersï¼Ÿ
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è°¢è°¢ï¼Œæˆ‘å¥½å¥‡çš„æ˜¯ï¼Œä¸ºä»€ä¹ˆå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰é€šå¸¸å‚æ•°é‡æ¯”å…¶ä»–æ–¹æ³•å°‘å‡ ä¸ªæ•°é‡çº§ï¼Ÿ
- en: That's a great questionï¼Œ so typically when you think about reinforcement learning
    algorithmsã€‚ğŸ˜Šã€‚In deep art in particularï¼Œ so the most common algorithmsï¼Œ for exampleã€‚have
    different networks playing different roles in the task so you'll have a network
    for instanceã€‚playing the role of an actor so trying to figure out a policy and
    then therell a different network that's playing the role of a criticã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œé€šå¸¸å½“ä½ è€ƒè™‘å¼ºåŒ–å­¦ä¹ ç®—æ³•æ—¶ã€‚ğŸ˜Šã€‚ç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œæœ€å¸¸è§çš„ç®—æ³•ä¾‹å¦‚ï¼Œæœ‰ä¸åŒçš„ç½‘ç»œåœ¨ä»»åŠ¡ä¸­æ‰®æ¼”ä¸åŒçš„è§’è‰²ï¼Œä½ ä¼šæœ‰ä¸€ä¸ªç½‘ç»œæ‰®æ¼”æ¼”å‘˜çš„è§’è‰²ï¼Œè¯•å›¾æ‰¾å‡ºç­–ç•¥ï¼Œè¿˜æœ‰ä¸€ä¸ªä¸åŒçš„ç½‘ç»œæ‰®æ¼”è¯„è®ºè€…çš„è§’è‰²ã€‚
- en: And these networks are trained on data that's adaptively gathered so unlike
    perception where you will have a huge data set of interactions on which you can
    train your models in this case the architectures and even the environments to
    some extentã€‚are very simplistic because of the fact that we are trying to train
    very small components the functions that we're training and then bringing them
    all together and these functions are often trained in not super complex environmentsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç½‘ç»œåœ¨è‡ªé€‚åº”æ”¶é›†çš„æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤ä¸æ„ŸçŸ¥ä»»åŠ¡ä¸åŒï¼Œåœ¨æ„ŸçŸ¥ä¸­ä½ ä¼šæœ‰ä¸€ä¸ªå·¨å¤§çš„äº¤äº’æ•°æ®é›†æ¥è®­ç»ƒæ¨¡å‹ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¶æ„ç”šè‡³åœ¨æŸç§ç¨‹åº¦ä¸Šç¯å¢ƒéƒ½éå¸¸ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬è¯•å›¾è®­ç»ƒéå¸¸å°çš„ç»„ä»¶ï¼Œæˆ‘ä»¬è®­ç»ƒçš„å‡½æ•°ï¼Œç„¶åå°†å®ƒä»¬ç»“åˆåœ¨ä¸€èµ·ï¼Œè¿™äº›å‡½æ•°é€šå¸¸æ˜¯åœ¨ä¸å¤ªå¤æ‚çš„ç¯å¢ƒä¸­è®­ç»ƒçš„ã€‚
- en: so it's a mix of different issues I wouldn't say it's purely just got to with
    the fact that the learning objectives are faultã€‚but it's a combination of the
    environments we use the combination of the targets that each of the neural networks
    are predicting which leads to networks which are much bigger thanã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ¶‰åŠåˆ°ä¸åŒçš„é—®é¢˜ï¼Œæˆ‘ä¸ä¼šè¯´è¿™çº¯ç²¹ä¸å­¦ä¹ ç›®æ ‡æœ‰å…³ï¼Œè€Œæ˜¯ä¸æˆ‘ä»¬ä½¿ç”¨çš„ç¯å¢ƒã€æ¯ä¸ªç¥ç»ç½‘ç»œé¢„æµ‹çš„ç›®æ ‡çš„ç»„åˆæœ‰å…³ï¼Œè¿™å¯¼è‡´ç½‘ç»œçš„è§„æ¨¡æ¯”é¢„æœŸè¦å¤§å¾—å¤šã€‚
- en: What we currently see tending to orï¼Œ and that's why it's very common to see
    neural networks with much fewer layers being used in RL as opposed to perceptionã€‚Thank
    youã€‚You want to ask a questionep yeahï¼Œ I was going to is there a reason why you
    chose offline RL versus online RLï¼Ÿ
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç›®å‰çœ‹åˆ°çš„å€¾å‘ï¼Œæ­£å› ä¸ºå¦‚æ­¤ï¼Œå¼ºåŒ–å­¦ä¹ ä¸­ä½¿ç”¨çš„ç¥ç»ç½‘ç»œé€šå¸¸å±‚æ•°è¦æ¯”æ„ŸçŸ¥ä»»åŠ¡å°‘å¾ˆå¤šã€‚è°¢è°¢ã€‚ä½ æƒ³é—®ä¸ªé—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘æƒ³çŸ¥é“ä¸ºä»€ä¹ˆä½ é€‰æ‹©ç¦»çº¿å¼ºåŒ–å­¦ä¹ è€Œä¸æ˜¯åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Ÿ
- en: That's another great question so the question is why offline R is opposed to
    online RL and the plain reason is because all this is a first work trying to look
    at reinforcement learning so offline RL avoids this problem of explorationã€‚You
    are given a log data set of interactions you're not allowed to further interact
    with the environment so just from this data set you're trying to unearth a policy
    of what the optimal agent would look like so it would right like if you do online
    RL wouldn't that like just give you this opportunity of exploration basically
    it would it would what would also do which is a technically challenging here is
    that the exploration would be harder to encode so offline R is the first step
    there is no reason why we should not study why online RL cannot be done it's just
    that it provides a more contained setup where ideas from transformers will directly
    extendã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œé—®é¢˜æ˜¯ä¸ºä»€ä¹ˆç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRï¼‰ä¸åœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å¯¹ç«‹ï¼Œç®€å•çš„åŸå› æ˜¯ï¼Œå› ä¸ºè¿™æ˜¯ç¬¬ä¸€æ¬¡å°è¯•ç ”ç©¶å¼ºåŒ–å­¦ä¹ ï¼Œæ‰€ä»¥ç¦»çº¿RLé¿å…äº†æ¢ç´¢çš„é—®é¢˜ã€‚ä½ å¾—åˆ°äº†ä¸€ä¸ªäº¤äº’çš„æ—¥å¿—æ•°æ®é›†ï¼Œä¸èƒ½è¿›ä¸€æ­¥ä¸ç¯å¢ƒäº¤äº’ï¼Œå› æ­¤ä»…ä»è¿™ä¸ªæ•°æ®é›†ä¸­ï¼Œä½ è¯•å›¾æŒ–æ˜å‡ºæœ€ä¼˜ä»£ç†çš„ç­–ç•¥ã€‚å¦‚æœä½ è¿›è¡Œåœ¨çº¿RLï¼Œéš¾é“ä¸ä¼šç»™ä½ è¿™ä¸ªæ¢ç´¢çš„æœºä¼šå—ï¼ŸåŸºæœ¬ä¸Šä¼šï¼Œä½†æŠ€æœ¯ä¸Šæ›´å…·æŒ‘æˆ˜çš„æ˜¯ï¼Œæ¢ç´¢ä¼šæ›´éš¾ç¼–ç ã€‚å› æ­¤ï¼Œç¦»çº¿Ræ˜¯ç¬¬ä¸€æ­¥ï¼Œæ²¡æœ‰ç†ç”±ä¸ç ”ç©¶ä¸ºä»€ä¹ˆåœ¨çº¿RLä¸èƒ½åšåˆ°ï¼Œåªæ˜¯å®ƒæä¾›äº†ä¸€ä¸ªæ›´å°é—­çš„è®¾ç½®ï¼Œå…¶ä¸­æ¥è‡ªå˜æ¢å™¨çš„æƒ³æ³•å°†ç›´æ¥æ‰©å±•ã€‚
- en: Okayï¼Œ sounds goodï¼Œ so let's look at the model and it's really simple on purposeã€‚So
    what we're going to do is we're going to look at our offline dataã€‚which is essentially
    in the form of tra treesï¼Œ so offline data would look like a sequence of statesã€‚actionsï¼Œ
    returns over multiple time stepsã€‚It's a sequence that's natural to think of us
    as directly feeding as input to a transformer in this case we use a causal transformer
    as it's common in GPTã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¬èµ·æ¥ä¸é”™ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹æ¨¡å‹ï¼Œå®ƒæ•…æ„åšå¾—éå¸¸ç®€å•ã€‚æˆ‘ä»¬è¦åšçš„æ˜¯æŸ¥çœ‹æˆ‘ä»¬çš„ç¦»çº¿æ•°æ®ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä»¥æ ‘å½¢ç»“æ„çš„å½¢å¼å‘ˆç°çš„ï¼Œç¦»çº¿æ•°æ®çœ‹èµ·æ¥åƒæ˜¯å¤šä¸ªæ—¶é—´æ­¥ä¸­çš„çŠ¶æ€åºåˆ—ã€è¡ŒåŠ¨ã€å›æŠ¥ã€‚è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„åºåˆ—ï¼Œå¯ä»¥ç›´æ¥ä½œä¸ºè¾“å…¥æä¾›ç»™å˜æ¢å™¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯å› æœå˜æ¢å™¨ï¼Œè¿™åœ¨GPTä¸­å¾ˆå¸¸è§ã€‚
- en: So we go from left to right and because the data set comes with the notion of
    time step causality here is much more well intended than the general meaning that's
    used for perception this is really causality how it should be in perspective of
    timeã€‚What we predict out of this transformer are the actions conditioned on everything
    that comes before that token in the sequenceã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä»å·¦åˆ°å³ï¼Œå› ä¸ºæ•°æ®é›†å¸¦æœ‰æ—¶é—´æ­¥å› æœæ€§çš„æ¦‚å¿µï¼Œè¿™é‡Œæ¯”ç”¨äºæ„ŸçŸ¥çš„ä¸€èˆ¬æ„ä¹‰æ›´å…·æ„å›¾ã€‚è¿™å®é™…ä¸Šæ˜¯å¦‚ä½•ä»æ—¶é—´çš„è§’åº¦çœ‹å¾…å› æœå…³ç³»ã€‚æˆ‘ä»¬ä»è¿™ä¸ªå˜æ¢å™¨é¢„æµ‹çš„ï¼Œæ˜¯åŸºäºåºåˆ—ä¸­è¯¥ä»¤ç‰Œä¹‹å‰æ‰€æœ‰å†…å®¹çš„æ¡ä»¶ä¸‹çš„è¡ŒåŠ¨ã€‚
- en: So if you want to predict the action at this t minus one stepï¼Œ we'll use everything
    that cameã€‚At time step D minus twoï¼Œ as well as the returns and states at time
    step D minus oneã€‚Okayã€‚So we will go into the details of how exactly each of these
    are encodedã€‚but essentially this is in a one linerï¼Œ it's taking the tragic pre
    datata from the offline dataã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æƒ³é¢„æµ‹åœ¨è¿™ä¸ªtå‡ä¸€çš„æ­¥éª¤ä¸­çš„è¡ŒåŠ¨ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ä¹‹å‰æ‰€æœ‰çš„ä¿¡æ¯ã€‚åœ¨Då‡äºŒçš„æ—¶é—´æ­¥ï¼Œä»¥åŠåœ¨Då‡ä¸€çš„æ—¶é—´æ­¥ä¸­çš„å›æŠ¥å’ŒçŠ¶æ€ã€‚å¥½çš„ã€‚æ‰€ä»¥æˆ‘ä»¬å°†æ·±å…¥äº†è§£è¿™äº›æ˜¯å¦‚ä½•è¢«ç¼–ç çš„ã€‚ä½†ä»æœ¬è´¨ä¸Šè®²ï¼Œè¿™å°±æ˜¯ä¸€æ¡çº¿ï¼Œå®ƒä»ç¦»çº¿æ•°æ®ä¸­æå–æ‚²æƒ¨çš„é¢„æ•°æ®ã€‚
- en: treating it as a sequence of tokensï¼Œ passing it through a causal transformer
    and getting a sequence of actions as the outputã€‚Okayï¼Œ so how exactly do we do
    the forward path through the networkï¼Ÿ
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶è§†ä¸ºä¸€ä¸ªä»¤ç‰Œåºåˆ—ï¼Œç»è¿‡å› æœå˜æ¢å™¨å¤„ç†ï¼Œå¾—åˆ°ä¸€ç³»åˆ—è¡ŒåŠ¨ä½œä¸ºè¾“å‡ºã€‚é‚£ä¹ˆï¼Œæˆ‘ä»¬åˆ°åº•å¦‚ä½•åœ¨ç½‘ç»œä¸­è¿›è¡Œå‰å‘ä¼ æ’­å‘¢ï¼Ÿ
- en: So one important aspect of this workï¼Œ which is the U states' actions and this
    quantity called returns to roomã€‚So these are not direct rewardsã€‚These are returns
    to go and let's see what they really meanã€‚So this is a trajectory that goes as
    inputã€‚and the returns to go are the sum of rewards starting from the current time
    step into until the end of the episodeã€‚So really what we want the transformer
    is to get better at using a target returnã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™é¡¹å·¥ä½œçš„ä¸€ä¸ªé‡è¦æ–¹é¢æ˜¯UçŠ¶æ€çš„è¡ŒåŠ¨å’Œè¿™ä¸ªç§°ä¸ºâ€œè¿”å›â€çš„é‡ã€‚å› æ­¤è¿™äº›ä¸æ˜¯ç›´æ¥çš„å¥–åŠ±ã€‚è¿™äº›æ˜¯æœªæ¥å›æŠ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®ƒä»¬çš„çœŸæ­£å«ä¹‰ã€‚è¿™æ˜¯ä¸€æ¡ä½œä¸ºè¾“å…¥çš„è½¨è¿¹ï¼Œæœªæ¥å›æŠ¥æ˜¯ä»å½“å‰æ—¶é—´æ­¥åˆ°è¯¥å‰§é›†ç»“æŸçš„å¥–åŠ±æ€»å’Œã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›å˜æ¢å™¨èƒ½å¤Ÿæ›´å¥½åœ°ä½¿ç”¨ç›®æ ‡è¿”å›ã€‚
- en: this is how you should think of returns to goã€‚As the input in deciding what
    action to takeã€‚This perspective is going to have multiple advantages if it willll
    allow us to actually do much more than offline RL and generalizing to different
    tasks by just changing the returns to goã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ä½ åº”è¯¥å¦‚ä½•çœ‹å¾…æœªæ¥å›æŠ¥ä½œä¸ºå†³å®šé‡‡å–ä»€ä¹ˆè¡ŒåŠ¨çš„è¾“å…¥ã€‚è¿™ç§è§†è§’å°†æœ‰å¤šä¸ªä¼˜åŠ¿ï¼Œå®ƒå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåšæ›´å¤šçš„äº‹æƒ…ï¼Œè¶…è¶Šç¦»çº¿RLï¼Œå¹¶é€šè¿‡ä»…æ”¹å˜æœªæ¥å›æŠ¥æ¥æ³›åŒ–åˆ°ä¸åŒçš„ä»»åŠ¡ã€‚
- en: And here it's very importantï¼Œ so at time step one we will just have the overall
    sum of rewards for the entire tragic dream at time step two we subtract the reward
    we get by taking the first action and then have the sum of rewards for the remainder
    of the tragic dreamã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šéå¸¸é‡è¦ï¼Œå› æ­¤åœ¨æ—¶é—´æ­¥é•¿ä¸€ï¼Œæˆ‘ä»¬å°†ä»…æ‹¥æœ‰æ•´ä¸ªæ‚²æƒ¨æ¢¦å¢ƒçš„æ€»ä½“å¥–åŠ±ä¹‹å’Œï¼›åœ¨æ—¶é—´æ­¥é•¿äºŒï¼Œæˆ‘ä»¬å‡å»é‡‡å–ç¬¬ä¸€è¡ŒåŠ¨æ‰€è·å¾—çš„å¥–åŠ±ï¼Œç„¶åè·å¾—æ‚²æƒ¨æ¢¦å¢ƒå…¶ä½™éƒ¨åˆ†çš„å¥–åŠ±ä¹‹å’Œã€‚
- en: Okayï¼Œ so that's how we quality turns to go like how many more rewards in accumulation
    you need to acquire to fulfill yourã€‚Re your return goal that is set the beginningã€‚What
    is the outputã€‚the output is the sequence of predicted actionsï¼Œ so as I showed
    in the previous slideã€‚we use a causal transformer so we predict in sequence the
    desired actionsã€‚The attentionï¼Œ which isã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•è´¨é‡è½¬å‘çš„æ–¹å¼ï¼Œåƒæ˜¯ä½ éœ€è¦ç§¯ç´¯å¤šå°‘æ›´å¤šçš„å¥–åŠ±æ¥å®ç°ä½ åœ¨å¼€å§‹æ—¶è®¾å®šçš„è¿”å›ç›®æ ‡ã€‚è¾“å‡ºæ˜¯ä»€ä¹ˆï¼Ÿè¾“å‡ºæ˜¯é¢„æµ‹è¡ŒåŠ¨çš„åºåˆ—ï¼Œæ­£å¦‚æˆ‘åœ¨ä¸Šä¸€å¼ å¹»ç¯ç‰‡ä¸­å±•ç¤ºçš„ã€‚æˆ‘ä»¬ä½¿ç”¨å› æœå˜æ¢å™¨ï¼Œå› æ­¤æˆ‘ä»¬æŒ‰é¡ºåºé¢„æµ‹æ‰€éœ€çš„è¡ŒåŠ¨ã€‚æ³¨æ„åŠ›ã€‚
- en: Going to be computed inside the transformer with taking an important hyperparameter
    keyã€‚which is the context lengthã€‚We see that in perception as well hereï¼Œ and for
    the rest of the talkã€‚I'm going to use the notation K to denote how many tokens
    in the past would we be attending over to predict the action and the current time
    stepã€‚Okayï¼Œ so againï¼Œ digging a little bit deeper into codeã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: å°†åœ¨å˜æ¢å™¨å†…éƒ¨è®¡ç®—ï¼Œå…³é”®çš„ä¸€ä¸ªé‡è¦è¶…å‚æ•°æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦ã€‚æˆ‘ä»¬åœ¨æ„ŸçŸ¥ä¸­ä¹Ÿçœ‹åˆ°äº†è¿™ä¸€ç‚¹ï¼Œæ¥ä¸‹æ¥åœ¨æ•´ä¸ªè®²åº§ä¸­ï¼Œæˆ‘å°†ä½¿ç”¨ç¬¦å· K æ¥è¡¨ç¤ºæˆ‘ä»¬åœ¨è¿‡å»éœ€è¦å…³æ³¨å¤šå°‘ä¸ªæ ‡è®°ä»¥é¢„æµ‹å½“å‰æ—¶é—´æ­¥çš„è¡ŒåŠ¨ã€‚å¥½çš„ï¼Œå†æ·±å…¥ä¸€ç‚¹ä»£ç ã€‚
- en: There are some subtle differences with how a decision transformer operates as
    opposed to a normal transformerã€‚the first is that here the time step notion is
    going to beã€‚å“¦ã€‚Have a much bigger semantics that extends across three tokensã€‚So
    in perceptionã€‚you just think about the time step per wordï¼Œ for instanceï¼Œ like
    in NLP or per patch for visionã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å†³ç­–å˜æ¢å™¨ä¸æ™®é€šå˜æ¢å™¨çš„æ“ä½œæœ‰ä¸€äº›å¾®å¦™çš„å·®å¼‚ã€‚é¦–å…ˆï¼Œè¿™é‡Œçš„æ—¶é—´æ­¥é•¿æ¦‚å¿µå°†å…·æœ‰æ›´å¤§çš„è¯­ä¹‰ï¼Œæ‰©å±•åˆ°ä¸‰ä¸ªæ ‡è®°ã€‚å› æ­¤åœ¨æ„ŸçŸ¥ä¸­ï¼Œä½ åªéœ€è€ƒè™‘æ¯ä¸ªå•è¯çš„æ—¶é—´æ­¥é•¿ï¼Œä¾‹å¦‚åœ¨
    NLP ä¸­ï¼Œæˆ–æ¯ä¸ªè¡¥ä¸åœ¨è§†è§‰ä¸­ã€‚
- en: and in this case we will have a time step encapsulating three tokensï¼Œ one for
    the statesã€‚one for the actions and one for the rewardsã€‚And then we embed each
    of these tokens and then add the pollution embedding as it common in the transformer
    and we feed those inputs to the transformer at the output we only care about one
    of these fee tokens in those default side I will say experiments where even the
    other tokens might be of interest as target predictions but for now let's keep
    it simpleã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†æœ‰ä¸€ä¸ªæ—¶é—´æ­¥é•¿åŒ…å«ä¸‰ä¸ªæ ‡è®°ï¼Œä¸€ä¸ªç”¨äºçŠ¶æ€ï¼Œä¸€ä¸ªç”¨äºè¡ŒåŠ¨ï¼Œä¸€ä¸ªç”¨äºå¥–åŠ±ã€‚ç„¶åæˆ‘ä»¬åµŒå…¥æ¯ä¸ªæ ‡è®°ï¼Œå¹¶æ·»åŠ æ±¡æŸ“åµŒå…¥ï¼Œæ­£å¦‚åœ¨å˜æ¢å™¨ä¸­å¸¸è§çš„é‚£æ ·ï¼Œç„¶åå°†è¿™äº›è¾“å…¥æä¾›ç»™å˜æ¢å™¨ï¼›åœ¨è¾“å‡ºæ—¶ï¼Œæˆ‘ä»¬åªå…³å¿ƒè¿™äº›è´¹ç”¨æ ‡è®°ä¸­çš„ä¸€ä¸ªï¼Œåœ¨é‚£äº›é»˜è®¤çš„å®éªŒä¸­ï¼Œæˆ‘ä¼šè¯´å³ä½¿å…¶ä»–æ ‡è®°å¯èƒ½å¯¹ç›®æ ‡é¢„æµ‹æ„Ÿå…´è¶£ï¼Œä½†ç°åœ¨è®©æˆ‘ä»¬ä¿æŒç®€å•ã€‚
- en: we want to learn a policy a policy which is trying to predict actionsã€‚So when
    we try to decodeã€‚we'll only be looking at the actions from the hidden representation
    in the prefin layerã€‚Okayã€‚so this is the forward pass now what do we do with this
    networkï¼Œ we train it how do we train itï¼ŸğŸ˜Šã€‚Sorryï¼Œ just a quick question on semantics
    there if you go back one slideã€‚The plus in this caseã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³è¦å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œè¿™ä¸ªç­–ç•¥è¯•å›¾é¢„æµ‹è¡ŒåŠ¨ã€‚æ‰€ä»¥å½“æˆ‘ä»¬å°è¯•è§£ç æ—¶ï¼Œæˆ‘ä»¬åªä¼šå…³æ³¨ä»é¢„å…ˆå®šä¹‰å±‚ä¸­çš„éšè—è¡¨ç¤ºå¾—åˆ°çš„è¡ŒåŠ¨ã€‚å¥½çš„ã€‚è¿™æ˜¯å‰å‘ä¼ é€’ï¼Œé‚£ä¹ˆæˆ‘ä»¬ç”¨è¿™ä¸ªç½‘ç»œåšä»€ä¹ˆï¼Ÿæˆ‘ä»¬è®­ç»ƒå®ƒï¼Œæ€ä¹ˆè®­ç»ƒå‘¢ï¼ŸğŸ˜Šã€‚æŠ±æ­‰ï¼Œå…³äºè¯­ä¹‰çš„ä¸€ä¸ªå¿«é€Ÿé—®é¢˜ï¼Œå¦‚æœä½ å›åˆ°ä¸Šä¸€å¼ å¹»ç¯ç‰‡ã€‚è¿™é‡Œçš„åŠ å·ã€‚
- en: the syntax means that you are actually adding the values element wise and not
    concatenating themã€‚is that rightï¼ŸThat is correctã€‚å…¬ã€‚Okayï¼Œ so what's the last function
    follow up on thatã€‚I thought it was concotï¼Œ why are we just adding itï¼ŸSorryï¼Œ can
    you go backï¼ŸYeahã€‚I think it's a design choiceï¼Œ you can pattern in itï¼Œ you can
    add itã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è¯­æ³•æ„å‘³ç€ä½ å®é™…ä¸Šæ˜¯åœ¨é€å…ƒç´ æ·»åŠ å€¼ï¼Œè€Œä¸æ˜¯å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ã€‚æ˜¯è¿™æ ·å—ï¼Ÿæ²¡é”™ã€‚å…¬ã€‚å¥½çš„ï¼Œé‚£æœ€åä¸€ä¸ªå‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿæˆ‘ä»¥ä¸ºæ˜¯è¿æ¥ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬åªæ˜¯åŠ å®ƒå‘¢ï¼ŸæŠ±æ­‰ï¼Œä½ èƒ½å›å»ä¸€ä¸‹å—ï¼Ÿå¥½çš„ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªè®¾è®¡é€‰æ‹©ï¼Œä½ å¯ä»¥åœ¨å…¶ä¸­è¿›è¡Œæ¨¡å¼æ“ä½œï¼Œå¯ä»¥æ·»åŠ å®ƒã€‚
- en: it leads to different functions being encoded in our case it was editionedã€‚Oã€‚Why
    did you try the other one and it just didn't work or why is that because I think
    intuitively concatenating would like make more senseã€‚So I think both of them have
    different use cases for the functional of encoding like one is really mixing in
    the embeddings for the state and basically shifting it so and you add something
    if you think of the embedding of the states as a vector and you add something
    you are actually shifting itã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¼è‡´äº†ä¸åŒåŠŸèƒ½çš„ç¼–ç ï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­æ˜¯ç‰ˆæœ¬åŒ–çš„Oã€‚ä½ ä¸ºä»€ä¹ˆå°è¯•å¦ä¸€ä¸ªï¼Œè€Œå®ƒä¸èµ·ä½œç”¨ï¼Ÿæˆ–è€…è¯´ï¼Œä¸ºä»€ä¹ˆè¿™æ ·ï¼Ÿå› ä¸ºæˆ‘è§‰å¾—ç›´è§‚ä¸Šä¸²è”ä¼¼ä¹æ›´æœ‰æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºå®ƒä»¬åœ¨ç¼–ç åŠŸèƒ½ä¸Šæœ‰ä¸åŒçš„ç”¨ä¾‹ï¼Œä¸€ä¸ªæ˜¯çœŸæ­£æ··åˆçŠ¶æ€çš„åµŒå…¥å¹¶åŸºæœ¬ä¸Šè¿›è¡Œä½ç§»ï¼Œå¦‚æœä½ æŠŠçŠ¶æ€çš„åµŒå…¥è§†ä¸ºä¸€ä¸ªå‘é‡ï¼Œæ·»åŠ æŸäº›ä¸œè¥¿å®é™…ä¸Šæ˜¯åœ¨ä½ç§»ã€‚
- en: Whereas in the concatetnation caseï¼Œ you are actually increasing the dimensionality
    of this spaceã€‚ğŸ˜Šã€‚Yeahå‘€ã€‚So those are different choices which are doing very different
    things we found this one to be worked better i'm not sure I remember if the results
    was very significantly different if you would concatetnate themã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œåœ¨ä¸²è”çš„æƒ…å†µä¸‹ï¼Œä½ å®é™…ä¸Šæ˜¯åœ¨å¢åŠ è¿™ä¸ªç©ºé—´çš„ç»´åº¦ã€‚ğŸ˜Šã€‚æ˜¯çš„ã€‚æ‰€ä»¥è¿™äº›æ˜¯åšç€éå¸¸ä¸åŒäº‹æƒ…çš„ä¸åŒé€‰æ‹©ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªæ•ˆæœæ›´å¥½ï¼Œæˆ‘ä¸ç¡®å®šæˆ‘æ˜¯å¦è®°å¾—å¦‚æœä½ å°†å®ƒä»¬ä¸²è”ï¼Œç»“æœæ˜¯å¦ä¼šæœ‰æ˜¾è‘—å·®å¼‚ã€‚
- en: but this is the one which we operate withã€‚But wouldn't there because like if
    you're shifting it right like if you have an em bedding for a state and let's
    say like you perform certain actions and you like end up at the same state againã€‚You
    would want these embeddings to be the same however now you're at a different time
    step so like you shifted it so wouldn't that be like harder to learnï¼Ÿ
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ˜¯æˆ‘ä»¬ä½¿ç”¨çš„è¿™ä¸ªã€‚å¦‚æœä½ æ˜¯å¯¹çš„ä½ç§»ï¼Œé‚£ä¹ˆå¦‚æœä½ å¯¹ä¸€ä¸ªçŠ¶æ€è¿›è¡ŒåµŒå…¥ï¼Œå¹¶ä¸”å‡è®¾ä½ æ‰§è¡ŒæŸäº›æ“ä½œï¼Œç„¶ååˆå›åˆ°åŒä¸€çŠ¶æ€ã€‚ä½ å¸Œæœ›è¿™äº›åµŒå…¥æ˜¯ç›¸åŒçš„ï¼Œç„¶è€Œç°åœ¨ä½ å¤„äºä¸åŒçš„æ—¶é—´æ­¥éª¤ï¼Œæ‰€ä»¥ä½ è¿›è¡Œäº†ä½ç§»ï¼Œé‚£ä¹ˆè¿™ä¸æ˜¯æ›´éš¾å­¦ä¹ å—ï¼Ÿ
- en: So there's a bigger and interesting question in that what you said is basicallyã€‚are
    we losing the Marco propertyï¼ŸBecause as you said that if we come back to the same
    state at a different time stepã€‚shouldn't we be doing similar operations and the
    answer here is yesï¼Œ we are actually being nonmarcoã€‚And this might seem very nonintuitive
    at first that why is nonmarkovness important hereã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ä½ æ‰€è¯´çš„åŸºæœ¬ä¸Šï¼Œæœ‰ä¸€ä¸ªæ›´å¤§ä¸”æœ‰è¶£çš„é—®é¢˜ã€‚æˆ‘ä»¬æ˜¯å¦åœ¨å¤±å»é©¬å°”å¯å¤«æ€§è´¨ï¼Ÿå› ä¸ºå¦‚ä½ æ‰€è¯´ï¼Œå¦‚æœæˆ‘ä»¬åœ¨ä¸åŒçš„æ—¶é—´æ­¥éª¤å›åˆ°ç›¸åŒçš„çŠ¶æ€ï¼Œéš¾é“æˆ‘ä»¬ä¸åº”è¯¥æ‰§è¡Œç±»ä¼¼çš„æ“ä½œå—ï¼Ÿç­”æ¡ˆæ˜¯è‚¯å®šçš„ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯éé©¬å°”å¯å¤«çš„ã€‚è¿™åœ¨æœ€å¼€å§‹å¯èƒ½çœ‹èµ·æ¥éå¸¸åç›´è§‰ï¼Œä¸ºä»€ä¹ˆéé©¬å°”å¯å¤«æ€§åœ¨è¿™é‡Œå¾ˆé‡è¦ã€‚
- en: and I want to refer to another paper which came very much in conjunction with
    this a transer that actually shows in more detail and its basically says that
    if you were trying to predict the transition dynamicsã€‚then you could have actually
    had a Markovvian system built in hereï¼Œ which would do just as goodã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³æåˆ°å¦ä¸€ç¯‡è®ºæ–‡ï¼Œå®ƒä¸è¿™ä¸ªç ”ç©¶éå¸¸ç›¸å…³ï¼Œå®é™…ä¸Šæ›´è¯¦ç»†åœ°å±•ç¤ºäº†è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”åŸºæœ¬ä¸Šè¯´ï¼Œå¦‚æœä½ è¯•å›¾é¢„æµ‹è¿‡æ¸¡åŠ¨æ€ï¼Œé‚£ä¹ˆä½ å®é™…ä¸Šå¯ä»¥åœ¨è¿™é‡Œæ„å»ºä¸€ä¸ªé©¬å°”å¯å¤«ç³»ç»Ÿï¼Œè¿™æ ·æ•ˆæœä¹Ÿä¼šå¾ˆå¥½ã€‚
- en: Howeverï¼Œ for the perspective of trying to actually predict actionsã€‚it does help
    to look at the previous time stepsï¼Œ even more so when you have missing observations
    so for instanceã€‚if you have the observations being a subset of the true state
    so looking at the previous states and actions helps you better fill in the missing
    pieces in some sense so this is commonly known as partial observability where
    you by looking at the previous tokens you can do a better job at predicting the
    actions that you should take at the current time stepã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä»å®é™…é¢„æµ‹è¡ŒåŠ¨çš„è§’åº¦æ¥çœ‹ï¼Œè€ƒè™‘ä¹‹å‰çš„æ—¶é—´æ­¥éª¤æ˜¯æœ‰å¸®åŠ©çš„ï¼Œå°¤å…¶æ˜¯åœ¨æœ‰ç¼ºå¤±è§‚å¯Ÿçš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ çš„è§‚å¯Ÿæ˜¯å®é™…çŠ¶æ€çš„ä¸€ä¸ªå­é›†ï¼Œé‚£ä¹ˆæŸ¥çœ‹ä¹‹å‰çš„çŠ¶æ€å’ŒåŠ¨ä½œå¯ä»¥å¸®åŠ©ä½ æ›´å¥½åœ°å¡«è¡¥æŸç§æ„ä¹‰ä¸Šçš„ç¼ºå¤±éƒ¨åˆ†ï¼Œè¿™é€šå¸¸è¢«ç§°ä¸ºéƒ¨åˆ†å¯è§‚å¯Ÿæ€§ï¼Œé€šè¿‡æŸ¥çœ‹ä¹‹å‰çš„æ ‡è®°ï¼Œä½ å¯ä»¥æ›´å¥½åœ°é¢„æµ‹åœ¨å½“å‰æ—¶é—´æ­¥éª¤åº”é‡‡å–çš„è¡ŒåŠ¨ã€‚
- en: So nonmarkness is on purposeï¼Œ and it's non intuitiveã€‚but I think it's one of
    the things that separates this framework from existing onesã€‚So it will basically
    help you because like RL usually works on like infinite like works better on like
    infinite horizon problems right so technically the way you formulated it it would
    work better on finite horizon problems I'm assuming because you want to take different
    actions based on like a history based on like given the fact that now you have
    to a different time stepã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥éé©¬å°”å¯å¤«æ€§æ˜¯æ•…æ„çš„ï¼Œè€Œä¸”æ˜¯åç›´è§‰çš„ã€‚ä½†æˆ‘è®¤ä¸ºè¿™æ˜¯å°†è¿™ä¸ªæ¡†æ¶ä¸ç°æœ‰æ¡†æ¶åŒºåˆ†å¼€æ¥çš„äº‹æƒ…ä¹‹ä¸€ã€‚å› æ­¤ï¼Œå®ƒå®é™…ä¸Šä¼šå¸®åŠ©ä½ ï¼Œå› ä¸ºå¼ºåŒ–å­¦ä¹ é€šå¸¸åœ¨æ— é™é—®é¢˜ä¸Šè¡¨ç°æ›´å¥½ï¼Œå¯¹å§ï¼Ÿä»æŠ€æœ¯ä¸Šè®²ï¼ŒæŒ‰ç…§ä½ æ‰€åˆ¶å®šçš„æ–¹å¼ï¼Œå®ƒåœ¨æœ‰é™é—®é¢˜ä¸Šè¡¨ç°ä¼šæ›´å¥½ï¼Œæˆ‘å‡è®¾æ˜¯å› ä¸ºä½ å¸Œæœ›æ ¹æ®å†å²é‡‡å–ä¸åŒçš„è¡ŒåŠ¨ï¼Œè€ƒè™‘åˆ°ç°åœ¨ä½ å¤„äºä¸åŒçš„æ—¶é—´æ­¥éª¤ã€‚
- en: Yeah yeah so if you wanted to work on In Verorizon maybe something like discounting
    would work just as well to get that effect in this case we were using a discount
    factor of one or basically like no discounting at allã€‚but you're right if I think
    we really want to extend it to In horizonizon you would need to change the discount
    factorã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå¦‚æœä½ æƒ³åœ¨ In Verorizon ä¸Šå·¥ä½œï¼Œæˆ–è®¸æŠ˜æ‰£æ”¿ç­–åœ¨è¿™ç§æƒ…å†µä¸‹ä¹Ÿèƒ½èµ·åˆ°ç±»ä¼¼çš„æ•ˆæœï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æŠ˜æ‰£å› å­æ˜¯1ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯æ²¡æœ‰æŠ˜æ‰£ã€‚ä½†ä½ è¯´å¾—å¯¹ï¼Œå¦‚æœæˆ‘ä»¬çœŸçš„æƒ³å°†å…¶æ‰©å±•åˆ°
    In horizonizonï¼Œå°±éœ€è¦æ›´æ”¹æŠ˜æ‰£å› å­ã€‚
- en: Thanksã€‚Okayï¼Œ so questionsã€‚Ohã€‚I think it was just answered in chatï¼Œ but I'll
    ask it anywaysã€‚I think I might have missed this or maybe you're about to talk
    about itã€‚the offline data that was collectedï¼Œ what policy was used to collect
    itã€‚So this is a very important question and it will be something I mentioned the
    experimentsã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è°¢è°¢ã€‚å¥½çš„ï¼Œé—®é¢˜ã€‚å“¦ã€‚æˆ‘æƒ³åœ¨èŠå¤©ä¸­åˆšåˆšå›ç­”äº†ï¼Œä½†æˆ‘è¿˜æ˜¯æƒ³é—®ã€‚æˆ‘å¯èƒ½é”™è¿‡äº†ï¼Œæˆ–è€…ä¹Ÿè®¸ä½ è¦è°ˆè®ºå®ƒã€‚æ”¶é›†çš„ç¦»çº¿æ•°æ®ï¼Œä½¿ç”¨äº†ä»€ä¹ˆæ”¿ç­–è¿›è¡Œæ”¶é›†ï¼Ÿè¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„é—®é¢˜ï¼Œæˆ‘ä¼šåœ¨å®éªŒä¸­æåˆ°ã€‚
- en: so we were using the benchmarks that exist for our scenario where essentially
    the way these benchmarks are constructed asã€‚You train an agent using online RL
    and then you look at its replay buffer at some time stepã€‚So while training so
    while it's like a medium sort of expertã€‚you collect it the transition its' experience
    so far and make that as the offline data' it's something which is like our framework
    is very agnostic to what offline data that you use so I've not discussed so far
    but make something that in our experiments is based on traditional benchmarks
    got it so the reason I ask isn't I'm sure that your framework can accommodate
    any offline dataã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯é’ˆå¯¹æˆ‘ä»¬åœºæ™¯å­˜åœ¨çš„åŸºå‡†ï¼ŒåŸºæœ¬ä¸Šè¿™äº›åŸºå‡†çš„æ„å»ºæ–¹å¼æ˜¯ã€‚ä½ ä½¿ç”¨åœ¨çº¿å¼ºåŒ–å­¦ä¹ è®­ç»ƒä¸€ä¸ªä»£ç†ï¼Œç„¶ååœ¨æŸä¸ªæ—¶é—´æ­¥æŸ¥çœ‹å®ƒçš„é‡æ”¾ç¼“å†²åŒºã€‚å› æ­¤ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œä½œä¸ºä¸€ç§ä¸­ç­‰æ°´å¹³çš„ä¸“å®¶ï¼Œä½ æ”¶é›†äº†å®ƒçš„è½¬ç§»ç»éªŒï¼Œä½œä¸ºç¦»çº¿æ•°æ®ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¯¹ä½ ä½¿ç”¨çš„ç¦»çº¿æ•°æ®æ˜¯éå¸¸ä¸æ•æ„Ÿçš„ï¼Œæ‰€ä»¥æˆ‘å°šæœªè®¨è®ºè¿‡ï¼Œä½†åœ¨æˆ‘ä»¬çš„å®éªŒä¸­æ˜¯åŸºäºä¼ ç»ŸåŸºå‡†çš„ã€‚æ˜ç™½äº†ï¼Œæˆ‘é—®çš„åŸå› å¹¶ä¸æ˜¯å› ä¸ºæˆ‘ä¸ç›¸ä¿¡ä½ çš„æ¡†æ¶èƒ½å¤Ÿå®¹çº³ä»»ä½•ç¦»çº¿æ•°æ®ã€‚
- en: but it seems to me like the results that you're about to present are going to
    be heavily contingent on what that that data collection policy isã€‚Indeedï¼Œ indeedï¼Œ
    and also so we willï¼Œ I think I have a slide where we show an experiment where
    the amount of data can make a difference in how we compare with baselines and
    essentially we will see how distant transformer especially shines when there is
    small amounts of offline dataã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘è§‰å¾—ä½ å³å°†å±•ç¤ºçš„ç»“æœå°†ä¸¥é‡ä¾èµ–äºæ•°æ®æ”¶é›†æ”¿ç­–ã€‚ç¡®å®ï¼Œç¡®å®å¦‚æ­¤ï¼Œæˆ‘æƒ³æˆ‘æœ‰ä¸€å¼ å¹»ç¯ç‰‡å±•ç¤ºäº†ä¸€ä¸ªå®éªŒï¼Œå…¶ä¸­æ•°æ®é‡åœ¨æˆ‘ä»¬ä¸åŸºå‡†æ¯”è¾ƒæ—¶å¯èƒ½äº§ç”Ÿå·®å¼‚ï¼Œå°¤å…¶æ˜¯å½“ç¦»çº¿æ•°æ®è¾ƒå°‘æ—¶ï¼Œå˜æ¢å™¨ç‰¹åˆ«é—ªè€€çš„åœ°æ–¹ã€‚
- en: çŸ¥é“ã€‚Okayï¼Œ coolã€‚thank youã€‚Okay great questions so let let's go ahead so we have
    defined our model which is going to look at these tra trees and now let's see
    how we train it so very simple we are trying to fill actionsã€‚we'll try to match
    them to the ones we have in our data set if they are continuous using the mean
    square error if they are discrete and we can use across entropyã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥é“ã€‚å¥½çš„ï¼Œé…·ã€‚è°¢è°¢ã€‚å¥½çš„ï¼Œä¼Ÿå¤§çš„é—®é¢˜ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ç»§ç»­ã€‚æˆ‘ä»¬å·²ç»å®šä¹‰äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå®ƒå°†å…³æ³¨è¿™äº›æ ‘ç»“æ„ï¼Œç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•è®­ç»ƒå®ƒï¼Œæ‰€ä»¥éå¸¸ç®€å•ï¼Œæˆ‘ä»¬è¯•å›¾å¡«å……åŠ¨ä½œã€‚æˆ‘ä»¬å°†å°è¯•å°†å®ƒä»¬ä¸æ•°æ®é›†ä¸­å·²æœ‰çš„åŠ¨ä½œåŒ¹é…ï¼Œå¦‚æœæ˜¯è¿ç»­çš„ï¼Œå°±ä½¿ç”¨å‡æ–¹è¯¯å·®ï¼›å¦‚æœæ˜¯ç¦»æ•£çš„ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨äº¤å‰ç†µã€‚
- en: ğŸ˜Šï¼ŒUmã€‚But there is something very deep in here for our researchã€‚which is that
    these objectives are very stable to train and easily regularize because they've
    been developed for supervised learningã€‚In contrastï¼Œ what R is more used to is
    dynamic programmingã€‚style objectives which are based on the Belman equationã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå—¯ã€‚ä½†æ˜¯è¿™é‡Œå¯¹æˆ‘ä»¬çš„ç ”ç©¶æœ‰å¾ˆæ·±çš„æ„ä¹‰ã€‚å› ä¸ºè¿™äº›ç›®æ ‡åœ¨è®­ç»ƒæ—¶éå¸¸ç¨³å®šä¸”å®¹æ˜“æ­£åˆ™åŒ–ï¼Œå› ä¸ºå®ƒä»¬æ˜¯ä¸ºç›‘ç£å­¦ä¹ å¼€å‘çš„ã€‚ç›¸è¾ƒä¹‹ä¸‹ï¼ŒR æ›´å¸¸ç”¨äºåŸºäºè´å°”æ›¼æ–¹ç¨‹çš„åŠ¨æ€è§„åˆ’é£æ ¼ç›®æ ‡ã€‚
- en: And those end up being much harder to optimize and scaleã€‚and that's why you
    see a lot of the variance in the results as wellã€‚Okay so this is how we train
    the model now how do we use the model and that's the point about trying to do
    rollouts for the model so here again this is going to be similar to doing an autoregressive
    generation there was an important token here which was the returns to goã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œè¿™äº›æœ€ç»ˆä¼šå˜å¾—æ›´éš¾ä»¥ä¼˜åŒ–å’Œæ‰©å±•ã€‚è¿™å°±æ˜¯ä½ çœ‹åˆ°ç»“æœä¸­å­˜åœ¨å¾ˆå¤šå·®å¼‚çš„åŸå› ã€‚å¥½çš„ï¼Œé‚£ä¹ˆè¿™æ˜¯æˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„æ–¹å¼ï¼Œç°åœ¨æˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æ¨¡å‹ï¼Œè¿™å°±æ¶‰åŠåˆ°ä¸ºæ¨¡å‹è¿›è¡Œå›æ”¾çš„é—®é¢˜ï¼Œè¿™é‡Œå°†ç±»ä¼¼äºè‡ªå›å½’ç”Ÿæˆï¼Œå…¶ä¸­ä¸€ä¸ªé‡è¦çš„æ ‡è®°æ˜¯â€œæœªæ¥å›æŠ¥â€ã€‚
- en: And what we need to set during evaluationï¼Œ presumably we want export level performance
    because that will have the highest returnsã€‚so we set the initial returns to goã€‚Not
    based on a tragic tree because now we don't have a tragic tree we're going to
    generate a tragic tree so this is at entrance timeã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨è¯„ä¼°æ—¶éœ€è¦è®¾ç½®ä»€ä¹ˆï¼Œå‡è®¾æˆ‘ä»¬æƒ³è¦ä¸“å®¶çº§è¡¨ç°ï¼Œå› ä¸ºé‚£å°†æœ‰æœ€é«˜çš„å›æŠ¥ã€‚æ‰€ä»¥æˆ‘ä»¬è®¾ç½®åˆå§‹å›æŠ¥ä¸ºè¿›è¡Œã€‚ä¸æ˜¯åŸºäºæ‚²æƒ¨æ ‘ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬æ²¡æœ‰æ‚²æƒ¨æ ‘ï¼Œæˆ‘ä»¬å°†ç”Ÿæˆä¸€ä¸ªæ‚²æƒ¨æ ‘ï¼Œæ‰€ä»¥è¿™æ˜¯åœ¨å…¥å£æ—¶ã€‚
- en: so we will set it to the expatory term for instanceã€‚So in code what this whole
    procedure would look like is basically you set this returns to go token to as
    some target return and you set your initial state to one from the environment
    distribution of initial statesã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä¼šå°†å…¶è®¾ç½®ä¸ºæš´éœ²é¡¹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»£ç ä¸­ï¼Œè¿™æ•´ä¸ªè¿‡ç¨‹çš„æ ·å­åŸºæœ¬ä¸Šæ˜¯å°†è¿™ä¸ªå›æŠ¥æ ‡è®°è®¾ç½®ä¸ºæŸä¸ªç›®æ ‡å›æŠ¥ï¼Œå¹¶å°†åˆå§‹çŠ¶æ€è®¾ç½®ä¸ºæ¥è‡ªåˆå§‹çŠ¶æ€åˆ†å¸ƒçš„ä¸€ä¸ªã€‚
- en: And then you just roll out your decision transformerã€‚So you get a new actionã€‚this
    action will also give you a state and reward from the environmentã€‚you append them
    to your sequence and you get a new returns to go and you take just the context
    and key because that's what's used by the transformer to making predictions and
    then field it back to the distant transformerã€‚So it's regular auto regressive
    generationï¼Œ but the only key point to notice is how you initialize the transformer
    for RLã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ åªæ˜¯æ¨å‡ºä½ çš„å†³ç­–å˜æ¢å™¨ã€‚è¿™æ ·ä½ å°±å¾—åˆ°äº†ä¸€ä¸ªæ–°çš„åŠ¨ä½œã€‚è¿™ä¸ªåŠ¨ä½œä¹Ÿä¼šç»™ä½ ç¯å¢ƒä¸­çš„çŠ¶æ€å’Œå¥–åŠ±ã€‚ä½ å°†å®ƒä»¬é™„åŠ åˆ°ä½ çš„åºåˆ—ä¸­ï¼Œä½ å¾—åˆ°æ–°çš„å›æŠ¥ï¼Œå¹¶ä¸”åªå–ä¸Šä¸‹æ–‡å’Œå…³é”®ï¼Œå› ä¸ºè¿™æ˜¯å˜æ¢å™¨ç”¨æ¥åšå‡ºé¢„æµ‹çš„ï¼Œç„¶åå°†å…¶åé¦ˆç»™è¿œç¨‹å˜æ¢å™¨ã€‚æ‰€ä»¥è¿™æ˜¯å¸¸è§„çš„è‡ªå›å½’ç”Ÿæˆï¼Œä½†è¦æ³¨æ„çš„å…³é”®ç‚¹æ˜¯ä½ å¦‚ä½•ä¸ºå¼ºåŒ–å­¦ä¹ åˆå§‹åŒ–å˜æ¢å™¨ã€‚
- en: So add one question hereã€‚So how much is it choice on the expert target written
    methodã€‚is it does it have to be like the mean expert reward or can it be like
    the maximum reward possible in demandï¼Ÿ
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™é‡Œæ·»åŠ ä¸€ä¸ªé—®é¢˜ã€‚é‚£ä¹ˆåœ¨ä¸“å®¶ç›®æ ‡ä¹¦å†™æ–¹æ³•ä¸Šï¼Œè¿™ä¸ªé€‰æ‹©æœ‰å¤šå¤§ï¼Ÿæ˜¯å¦å¿…é¡»åƒå¹³å‡ä¸“å®¶å¥–åŠ±é‚£æ ·ï¼Œè¿˜æ˜¯å¯ä»¥æ˜¯éœ€æ±‚ä¸­å¯èƒ½çš„æœ€å¤§å¥–åŠ±ï¼Ÿ
- en: ğŸ˜Šï¼ŒLikeï¼Œ does that try of the number really matterï¼ŸThat's a very good questionã€‚so
    we generally would set it to be slightly higher than the max return in the data
    so I think the fact that we use was 1ã€‚1 timesï¼Œ but I think we have done a lot
    of experimentation in the range and it's fairly robust to what choice you useã€‚So
    for exampleï¼Œ for hopper export returns about 3600 and we have found very stable
    performance from all the way from like 30ã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œé‚£ä¹ˆï¼Œè¿™ä¸ªæ•°å­—çš„å°è¯•çœŸçš„é‡è¦å—ï¼Ÿè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šå¸¸ä¼šå°†å…¶è®¾ç½®ä¸ºç¨é«˜äºæ•°æ®ä¸­çš„æœ€å¤§å›æŠ¥ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„äº‹å®æ˜¯1.1å€ï¼Œä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬åœ¨è¿™ä¸ªèŒƒå›´å†…è¿›è¡Œäº†å¾ˆå¤šå®éªŒï¼Œè€Œä¸”å®ƒå¯¹ä½ ä½¿ç”¨çš„é€‰æ‹©ç›¸å½“ç¨³å¥ã€‚ä¾‹å¦‚ï¼Œå¯¹äºhopperä¸“å®¶ï¼Œå›æŠ¥å¤§çº¦ä¸º3600ï¼Œæˆ‘ä»¬å‘ç°ä»30å¼€å§‹çš„è¡¨ç°éå¸¸ç¨³å®šã€‚
- en: 5400 to even going to very high numbers like 5000 it worksã€‚å—¯ã€‚Yeahï¼Œ so howeverã€‚I
    would want to point out that this is something which is not typically needed in
    regular RL like knowing the exposure return here we are actually going beyond
    regular Rl and that we can choose a return we want so we also actually need this
    information about what the exposure return isã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä»5400ç”šè‡³é«˜è¾¾5000çš„æ•°å­—éƒ½æœ‰æ•ˆã€‚å—¯ã€‚æ˜¯çš„ï¼Œä¸è¿‡æˆ‘æƒ³æŒ‡å‡ºçš„æ˜¯ï¼Œè¿™åœ¨å¸¸è§„å¼ºåŒ–å­¦ä¹ ä¸­é€šå¸¸å¹¶ä¸éœ€è¦ï¼Œæ¯”å¦‚çŸ¥é“æš´éœ²å›æŠ¥ï¼Œè€Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯è¶…è¶Šå¸¸è§„å¼ºåŒ–å­¦ä¹ ï¼Œå¯ä»¥é€‰æ‹©æˆ‘ä»¬æƒ³è¦çš„å›æŠ¥ï¼Œå› æ­¤æˆ‘ä»¬ç¡®å®éœ€è¦çŸ¥é“æš´éœ²å›æŠ¥æ˜¯ä»€ä¹ˆã€‚
- en: At that pointã€‚Thanksã€‚There's another questionã€‚Yesï¼Œ so yeahï¼Œ it' just youã€‚you
    cannot be on the regular oilï¼Œ but I'm curious aboutã€‚Do you also like restrict
    this framework to only offline oil costã€‚If you wanna run this kind of framework
    in online oilã€‚you'll have to determine the returns to go a prioraryã€‚ So this kind
    of frameworkã€‚I think is kind of restricted to only offline oilã€‚ Do you think soã€‚Yesã€‚and
    I think asking this question as well earlier that yesï¼Œ I think for now this is
    the first workã€‚so we were focusing on offline Rs where this information can be
    gathered from the offline data setã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°é‚£æ—¶ã€‚è°¢è°¢ã€‚è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥æ˜¯çš„ï¼Œè¿™åªæ˜¯ä½ ã€‚ä½ ä¸èƒ½åœ¨å¸¸è§„æ²¹ä¸Šï¼Œä½†æˆ‘å¾ˆå¥½å¥‡ã€‚ä½ æ˜¯å¦ä¹Ÿé™åˆ¶è¿™ä¸ªæ¡†æ¶ä»…é™äºç¦»çº¿æ²¹æˆæœ¬ã€‚å¦‚æœä½ æƒ³åœ¨åœ¨çº¿æ²¹ä¸­è¿è¡Œè¿™ç§æ¡†æ¶ï¼Œä½ å¿…é¡»äº‹å…ˆç¡®å®šå›æŠ¥ã€‚å› æ­¤ï¼Œè¿™ç§æ¡†æ¶æˆ‘è®¤ä¸ºä»…é™äºç¦»çº¿æ²¹ã€‚ä½ è§‰å¾—å‘¢ï¼Ÿæ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæ˜¯çš„ï¼Œæ—©äº›æ—¶å€™ä¹Ÿé—®è¿‡è¿™ä¸ªé—®é¢˜ï¼Œæ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºç°åœ¨è¿™æ˜¯ç¬¬ä¸€é¡¹å·¥ä½œã€‚æ‰€ä»¥æˆ‘ä»¬ä¸“æ³¨äºç¦»çº¿å¼ºåŒ–å­¦ä¹ ï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥ä»ç¦»çº¿æ•°æ®é›†ä¸­è·å–ã€‚
- en: å•Šã€‚It is possible to think about strategies on how you can even get this online
    what you'll need is a curriculum so early on during training as you're gathering
    dataã€‚You will set when you're doing rolloutsï¼Œ you will set your export return
    to whatever you see in the data setã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å•Šã€‚å¯ä»¥è€ƒè™‘ä¸€äº›ç­–ç•¥ï¼Œæƒ³æƒ³ä½ å¦‚ä½•åœ¨çº¿è·å¾—è¿™äº›ä¿¡æ¯ï¼Œä½ éœ€è¦ä¸€ä¸ªè¯¾ç¨‹ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒåˆæœŸï¼Œå½“ä½ æ”¶é›†æ•°æ®æ—¶ã€‚å½“ä½ è¿›è¡Œæ¨å‡ºæ—¶ï¼Œä½ ä¼šå°†ä½ çš„ä¸“å®¶å›æŠ¥è®¾ç½®ä¸ºæ•°æ®é›†ä¸­çœ‹åˆ°çš„ä»»ä½•å†…å®¹ã€‚
- en: And then incremented as when we start seeing that the transformer can actually
    exceed that performanceã€‚So you can think of specifying a curriculum from slow
    to high for what that export return could be for Biitzitchu rolloutã€‚Chs the the
    decision transformingã€‚I say coolã€‚ Thank youã€‚So yeahï¼Œ this was about the modelã€‚so
    we discussed how this model isï¼Œ what the input to these model areï¼Œ what the outputs
    areã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åéšç€æˆ‘ä»¬å¼€å§‹çœ‹åˆ°å˜æ¢å™¨å®é™…ä¸Šå¯ä»¥è¶…è¶Šè¯¥æ€§èƒ½è€Œé€’å¢ã€‚å› æ­¤ä½ å¯ä»¥è®¤ä¸ºï¼Œä»æ…¢åˆ°å¿«ä¸ºBiitzitchuå›æ»šæŒ‡å®šä¸€ä¸ªè¯¾ç¨‹ï¼Œå†³ç­–å˜æ¢ã€‚æˆ‘è¯´å¤ªå¥½äº†ã€‚è°¢è°¢ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œè¿™å°±æ˜¯å…³äºæ¨¡å‹çš„å†…å®¹ã€‚æˆ‘ä»¬è®¨è®ºäº†è¿™ä¸ªæ¨¡å‹æ˜¯ä»€ä¹ˆï¼Œè¾“å…¥æ˜¯ä»€ä¹ˆï¼Œè¾“å‡ºæ˜¯ä»€ä¹ˆã€‚
- en: what the loss function is used for training this modelï¼Œ and how do we use this
    model at test timeã€‚There is a connection to this framework as being one way to
    instantiate what is often known as all as probabilistic inferenceã€‚So we can formulate
    RL as a graphical model problem where you have these states and actions being
    used to determine what the next state isã€‚And to encode a notion of optimalityï¼Œ
    typically you would also have these additional a variablesï¼Œ 01ã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒè¯¥æ¨¡å‹ä½¿ç”¨çš„æŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Œä»¥åŠæˆ‘ä»¬åœ¨æµ‹è¯•æ—¶å¦‚ä½•ä½¿ç”¨è¯¥æ¨¡å‹ã€‚è¿™ä¸ªæ¡†æ¶ä¸é€šå¸¸ç§°ä¸ºæ¦‚ç‡æ¨æ–­çš„å®ä¾‹åŒ–æ–¹å¼æœ‰è”ç³»ã€‚å› æ­¤æˆ‘ä»¬å¯ä»¥å°†å¼ºåŒ–å­¦ä¹ å…¬å¼åŒ–ä¸ºä¸€ä¸ªå›¾æ¨¡å‹é—®é¢˜ï¼Œå…¶ä¸­ä½¿ç”¨è¿™äº›çŠ¶æ€å’Œè¡Œä¸ºæ¥ç¡®å®šä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚ä¸ºäº†ç¼–ç æœ€ä¼˜æ€§æ¦‚å¿µï¼Œé€šå¸¸è¿˜ä¼šæœ‰è¿™äº›é¢å¤–çš„å˜é‡ï¼Œ01ã€‚
- en: 02 and so on forthï¼Œ which are implicitly saying that encoding some notion of
    rewardã€‚and conditioned on this optimality being trueï¼Œ RL is the task of learning
    a policyã€‚which is the mapping from states to actions such that we get optimal
    behaviorã€‚And if you really squint your eyesï¼Œ you can see that these optimality
    variables and decision transformers are actually being encoded by the returns
    to goã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 02 ä»¥åŠå…¶ä»–éšå«è¡¨ç¤ºç¼–ç æŸç§å¥–åŠ±çš„å†…å®¹ã€‚åŸºäºè¿™ä¸€æœ€ä¼˜æ€§æˆç«‹ï¼Œå¼ºåŒ–å­¦ä¹ çš„ä»»åŠ¡æ˜¯å­¦ä¹ ä¸€ç§ç­–ç•¥ï¼Œå³ä»çŠ¶æ€åˆ°è¡Œä¸ºçš„æ˜ å°„ï¼Œä»¥å®ç°æœ€ä¼˜è¡Œä¸ºã€‚å¦‚æœä½ ä»”ç»†è§‚å¯Ÿï¼Œä½ ä¼šå‘ç°è¿™äº›æœ€ä¼˜æ€§å˜é‡å’Œå†³ç­–å˜æ¢å™¨å®é™…ä¸Šæ˜¯é€šè¿‡è¿”å›å€¼è¿›è¡Œç¼–ç çš„ã€‚
- en: ğŸ˜Šï¼ŒSo if then we giveã€‚A value that's high enough at this time during rollouts
    like the expert returnã€‚we are essentially saying that conditioned on this beingã€‚The
    mathematical form of quantification of optimalityã€‚Roll out your listen transformer
    to hopefully satisfy this conditionã€‚Soã€‚Yeahã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬åœ¨å›æ»šæœŸé—´ç»™äºˆä¸€ä¸ªè¶³å¤Ÿé«˜çš„å€¼ï¼Œæ¯”å¦‚ä¸“å®¶è¿”å›ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è¯´ï¼ŒåŸºäºè¿™ä¸€ç‚¹ã€‚æœ€ä¼˜æ€§çš„æ•°å­¦é‡åŒ–å½¢å¼ã€‚å¸Œæœ›ä½ èƒ½å®ç°è¿™ä¸ªæ¡ä»¶ã€‚
- en: so this was all I want to talk about the model' can you explain thatï¼Œ pleaseï¼Ÿ
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘æƒ³è°ˆè®ºæ¨¡å‹çš„å…¨éƒ¨å†…å®¹ã€‚ä½ èƒ½è§£é‡Šä¸€ä¸‹å—ï¼Ÿ
- en: What do you mean by optimality variables in the decision transformer and how
    do you mean like return to goï¼Ÿ
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ åœ¨å†³ç­–å˜æ¢å™¨ä¸­æ‰€è¯´çš„æœ€ä¼˜æ€§å˜é‡æ˜¯ä»€ä¹ˆæ„æ€ï¼Œè¿”å›å€¼åˆæ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ
- en: Rightï¼Œ so optimality variables we can think in the more simplest context as
    legislative or binaryã€‚so one is if youã€‚Solt the goal and zero as if you did not
    solve the goalã€‚And what basically in that caseï¼Œ you could also think of your decision
    transformer as at test time and we encode the returns to goã€‚We could set it to
    oneï¼Œ which will basically mean that conditioned on optimalities optimality here
    meansã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨æ›´ç®€å•çš„ä¸Šä¸‹æ–‡ä¸­å°†æœ€ä¼˜æ€§å˜é‡è§†ä¸ºç«‹æ³•æˆ–äºŒè¿›åˆ¶ã€‚ä¸€ä¸ªæ˜¯å¦‚æœä½ è¾¾æˆç›®æ ‡ï¼Œå¦ä¸€ä¸ªæ˜¯å¦‚æœä½ æ²¡æœ‰è¾¾æˆç›®æ ‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ ä¹Ÿå¯ä»¥è€ƒè™‘åœ¨æµ‹è¯•æ—¶ç¼–ç è¿”å›å€¼ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶è®¾ç½®ä¸ºä¸€ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€åŸºäºæœ€ä¼˜æ€§çš„æœ€ä¼˜æ€§ã€‚
- en: Solving the goal as oneã€‚Generate me the sequence of actions such that this would
    be trueã€‚Of courseã€‚our learning is not perfectï¼Œ so it's not guarantee we'll get
    thatã€‚but we have trained the transformer in a way to interpret the returns to
    go as some notion of optimalityã€‚So is it is it if I'm interpreting this correctlyï¼Œ
    it's roughly like sayingã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¾¾æˆç›®æ ‡è§†ä¸ºä¸€ã€‚ç”Ÿæˆä¸€ç³»åˆ—è¡ŒåŠ¨ï¼Œä½¿å…¶æˆç«‹ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬çš„å­¦ä¹ å¹¶ä¸å®Œç¾ï¼Œæ‰€ä»¥ä¸èƒ½ä¿è¯æˆ‘ä»¬èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚ä½†æˆ‘ä»¬å·²ç»è®­ç»ƒäº†å˜æ¢å™¨ï¼Œä»¥ä¸€ç§æ–¹å¼å°†è¿”å›å€¼è§£é‡Šä¸ºæŸç§æœ€ä¼˜æ€§æ¦‚å¿µã€‚å¦‚æœæˆ‘æ­£ç¡®ç†è§£çš„è¯ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥è¯´ã€‚
- en: show me what an optimal sequence of transitions look likeã€‚Because you've learned
    the model has learned sort of both successful and unsuccessful transitionsã€‚Exactly
    exactly and as we shall seen some experimentsï¼Œ we can for the binary caseã€‚it's
    either optimal or non optimalï¼Œ but rarely this can be a continuous variable which
    it is in our experiment so we can also see what happens in between experimentallyã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å±•ç¤ºä¸€ä¸ªæœ€ä¼˜è½¬æ¢åºåˆ—æ˜¯ä»€ä¹ˆæ ·å­çš„ã€‚å› ä¸ºä½ å·²ç»å­¦ä¹ åˆ°æ¨¡å‹å·²ç»å­¦ä¹ äº†æˆåŠŸå’Œä¸æˆåŠŸçš„è½¬æ¢ã€‚ç¡®åˆ‡åœ°è¯´ï¼Œæ­£å¦‚æˆ‘ä»¬å°†çœ‹åˆ°çš„ä¸€äº›å®éªŒï¼Œå¯¹äºäºŒå…ƒæƒ…å†µï¼Œå®ƒè¦ä¹ˆæ˜¯æœ€ä¼˜çš„ï¼Œè¦ä¹ˆä¸æ˜¯ï¼Œä½†å¾ˆå°‘æƒ…å†µä¸‹å®ƒå¯ä»¥æ˜¯è¿ç»­å˜é‡ï¼Œè€Œåœ¨æˆ‘ä»¬çš„å®éªŒä¸­ç¡®å®å¦‚æ­¤ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿå¯ä»¥å®éªŒæ€§åœ°è§‚å¯Ÿå…¶é—´çš„æƒ…å†µã€‚
- en: ğŸ˜Šï¼Œå“¦ã€‚Okayï¼Œ so let's jump into the experiments so there are a bunch of experiments
    and I've picked out a few which I think are interesting and give the key results
    in the paper but feel free to refer the paper for you and even more detailed analysis
    on some of the components of our moduleã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå“¦ã€‚å¥½çš„ï¼Œè®©æˆ‘ä»¬è¿›å…¥å®éªŒï¼Œæœ‰å¾ˆå¤šå®éªŒï¼Œæˆ‘æŒ‘é€‰äº†ä¸€äº›æˆ‘è®¤ä¸ºæœ‰è¶£çš„ï¼Œç»™å‡ºè®ºæ–‡ä¸­çš„å…³é”®ç»“æœï¼Œä½†éšæ—¶å¯ä»¥å‚è€ƒè®ºæ–‡ï¼Œè·å–æˆ‘ä»¬æ¨¡å—æŸäº›ç»„ä»¶çš„æ›´è¯¦ç»†åˆ†æã€‚
- en: So this first we can look at how well does it do on offline RLã€‚so there are
    benchmarks for the Atari suite of environments and the Open eye gym and we have
    another environment keyto door which is especially hard because it contains Pass
    Was and requires you to do credit assignment that I'll talk about laterã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹å®ƒåœ¨ç¦»çº¿RLä¸Šçš„è¡¨ç°ã€‚å¯¹äºAtariå¥—ä»¶ç¯å¢ƒå’ŒOpenAI Gymæœ‰åŸºå‡†æµ‹è¯•ï¼Œè¿˜æœ‰ä¸€ä¸ªç‰¹åˆ«éš¾çš„ç¯å¢ƒKeyto Doorï¼Œå› ä¸ºå®ƒåŒ…å«é€šé“ï¼Œå¹¶è¦æ±‚ä½ è¿›è¡Œä¿¡ç”¨åˆ†é…ï¼Œç¨åæˆ‘ä¼šè°ˆåˆ°è¿™ä¸€ç‚¹ã€‚
- en: But across the board we see that thisilent transformer is competitive with the
    state of the art model free offline RL methods in this caseã€‚this was a version
    of Q learning designed for offline RLã€‚And it can do excellent when especially
    when there is long term assignment where traditional methods based on T learning
    would failã€‚Yeahï¼Œ so the takeaway here should not be that we should we are at the
    stage where we can just substitute the existing algorithms for the decision transformerã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬çœ‹åˆ°ï¼Œè¿™ç§æ²‰é»˜çš„å˜æ¢å™¨åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ä¸æœ€å…ˆè¿›çš„æ— æ¨¡å‹ç¦»çº¿RLæ–¹æ³•å…·æœ‰ç«äº‰åŠ›ã€‚è¿™æ˜¯ä¸ºç¦»çº¿RLè®¾è®¡çš„Qå­¦ä¹ ç‰ˆæœ¬ã€‚å½“æ¶‰åŠé•¿æœŸåˆ†é…æ—¶ï¼Œå®ƒèƒ½è¡¨ç°å‡ºè‰²ï¼Œè€Œä¼ ç»Ÿçš„åŸºäºTå­¦ä¹ çš„æ–¹æ³•åˆ™ä¼šå¤±è´¥ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥è¿™é‡Œçš„è¦ç‚¹ä¸åº”è¯¥æ˜¯æˆ‘ä»¬å·²ç»å¯ä»¥ç”¨å†³ç­–å˜æ¢å™¨æ›¿ä»£ç°æœ‰ç®—æ³•ã€‚
- en: but this is a very strong evidence in favor that this paradigm which is building
    on transformers will permit us to better aerate and improve the models to hopefully
    surpass the existing algorithms uniformlyã€‚and there is some early evidence of
    that in harder environments which do require credit long term credit assignmentã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™å¼ºæœ‰åŠ›çš„è¯æ®æ”¯æŒè¿™ä¸ªåŸºäºå˜æ¢å™¨çš„èŒƒå¼å°†å…è®¸æˆ‘ä»¬æ›´å¥½åœ°é€šæ°”å’Œæ”¹è¿›æ¨¡å‹ï¼Œå¸Œæœ›èƒ½å¤Ÿå‡åŒ€è¶…è¶Šç°æœ‰ç®—æ³•ã€‚è€Œä¸”åœ¨éœ€è¦é•¿æœŸä¿¡ç”¨åˆ†é…çš„æ›´å›°éš¾ç¯å¢ƒä¸­ä¹Ÿæœ‰ä¸€äº›æ—©æœŸè¯æ®ã€‚
- en: Can I ask a question here about the baselineï¼Œ specifically TD learningï¼Ÿ
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥åœ¨è¿™é‡Œé—®ä¸€ä¸ªå…³äºåŸºçº¿çš„é—®é¢˜ï¼Œå…·ä½“æ˜¯TDå­¦ä¹ å—ï¼Ÿ
- en: I'm curious to know because I know that a lot of TD learning agents are feed
    forward networks are these baselines do they have recurrenceã€‚Yeahï¼Œ yeahï¼Œ so I
    think the conservative dual learning baselines here did have recurã€‚but I'm not
    very sure so I can check back on this offline and get back to you on thisã€‚O kã€‚Will
    be okayã€‚Thank youã€‚Also another quick questionã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆå¥½å¥‡ï¼Œå› ä¸ºæˆ‘çŸ¥é“å¾ˆå¤šTDå­¦ä¹ ä»£ç†éƒ½æ˜¯å‰é¦ˆç½‘ç»œï¼Œè¿™äº›åŸºçº¿æ˜¯å¦æœ‰é€’å½’ç»“æ„ã€‚æ˜¯çš„ï¼Œä¿å®ˆçš„åŒé‡å­¦ä¹ åŸºçº¿ç¡®å®æœ‰é€’å½’ã€‚ä½†æˆ‘ä¸å¤ªç¡®å®šï¼Œå¯ä»¥åœ¨ç¦»çº¿æ—¶å†æŸ¥ä¸€ä¸‹ï¼Œå›å¤´å‘Šè¯‰ä½ ã€‚å¥½çš„ï¼Œæ²¡é—®é¢˜ï¼Œè°¢è°¢ã€‚è¿˜æœ‰ä¸€ä¸ªå¿«é€Ÿçš„é—®é¢˜ã€‚
- en: so just how exactly do you evaluate the decision transformer here in the experiment
    so because you need to supply the returns to go so do you use the optimalã€‚Like
    policy to get what's optimal reward and that inã€‚so so here we basically look at
    the offline data that that is useful for trainingã€‚And we said whatever was the
    maximum return in the offline dataï¼Œ we set theã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œä½ æ˜¯å¦‚ä½•åœ¨å®éªŒä¸­è¯„ä¼°å†³ç­–å˜æ¢å™¨çš„ï¼Œå› ä¸ºä½ éœ€è¦æä¾›ç›®æ ‡å›æŠ¥ã€‚ä½ æ˜¯å¦ä½¿ç”¨äº†æœ€ä¼˜ç­–ç•¥æ¥è·å–æœ€ä¼˜å›æŠ¥ï¼Ÿåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åŸºæœ¬ä¸ŠæŸ¥çœ‹æœ‰ç”¨çš„ç¦»çº¿æ•°æ®è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬è®¾å®šäº†ç¦»çº¿æ•°æ®ä¸­çš„æœ€å¤§å›æŠ¥ã€‚
- en: Desire target are going to go as slightly higher than thatï¼Œ so 1ã€‚1 broke coefficient
    usedã€‚I see so and the performance sorry I'm not really what we're seeing or else
    but how is the performance defined here it just like is how much reward you get
    actually from the Yes yes so you can specify target return to goã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å›æŠ¥ç•¥é«˜äºæ­¤ï¼Œæ‰€ä»¥ä½¿ç”¨äº†1.1çš„ç³»æ•°ã€‚æˆ‘æ˜ç™½äº†ï¼Œå…³äºè¡¨ç°ï¼ŒæŠ±æ­‰æˆ‘ä¸å¤ªæ˜ç™½æˆ‘ä»¬çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Œä½†è¿™é‡Œçš„è¡¨ç°æ˜¯å¦‚ä½•å®šä¹‰çš„ï¼Œæ˜¯ä½ å®é™…è·å¾—çš„å¥–åŠ±å—ï¼Ÿæ˜¯çš„ï¼Œä½ å¯ä»¥æŒ‡å®šç›®æ ‡å›æŠ¥ã€‚
- en: but there's no guarantee that the actual actions that you take will achieve
    that return so yeah so you you measure the true environmental return based on
    yeah I see but then like just curious like so what so are these performance the
    percentage you get like for like how much I guess your you recover fromã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ²¡æœ‰ä¿è¯ä½ æ‰€é‡‡å–çš„å®é™…è¡ŒåŠ¨èƒ½å¤Ÿå®ç°é‚£ä¸ªå›æŠ¥ï¼Œæ‰€ä»¥ä½ éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒå›æŠ¥è¿›è¡Œè¡¡é‡ã€‚æˆ‘æ˜ç™½äº†ï¼Œä½†æˆ‘å¥½å¥‡è¿™äº›è¡¨ç°ç™¾åˆ†æ¯”æ˜¯å¦‚ä½•è®¡ç®—çš„ï¼Œæ¯”å¦‚ä½ ä»ä¸­æ¢å¤äº†å¤šå°‘ã€‚
- en: The next few months yeah so these are not percentagesã€‚these are some we have
    normalizing the returns so that everything costs between the 200ã€‚Yeahï¼Œ yeahã€‚I
    see then just wonder if you have a like a rough idea about how much like reward
    actually is recovered by the student transformers like does they say like if you
    specifyã€‚I want to get like 50 rewardsï¼Œ does it get 49 or is this even better sometimes
    orã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥çš„å‡ ä¸ªæœˆï¼Œæ˜¯çš„ï¼Œè¿™äº›ä¸æ˜¯ç™¾åˆ†æ¯”ã€‚è¿™æ˜¯æˆ‘ä»¬å¯¹å›æŠ¥è¿›è¡Œè§„èŒƒåŒ–ï¼Œä½¿å…¶åœ¨200ä¹‹é—´ã€‚æ˜¯çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚é‚£ä¹ˆæˆ‘åªæ˜¯æƒ³çŸ¥é“ï¼Œå­¦ç”Ÿå˜æ¢å™¨å®é™…å›æ”¶äº†å¤šå°‘å¥–åŠ±ï¼Œæ¯”å¦‚è¯´å¦‚æœä½ æŒ‡å®šã€‚æˆ‘æƒ³è¦50ä¸ªå¥–åŠ±ï¼Œå®ƒèƒ½å¾—åˆ°49ä¸ªï¼Œè¿˜æ˜¯æœ‰æ—¶èƒ½æ›´å¥½ä¸€äº›ï¼Ÿ
- en: That's an excellent question and my nextã€‚So here we're going to answer precisely
    this question that will asked is like if you feed in the target returnã€‚it could
    be expert or it could also non be expert how well does the mall actually do in
    attaining itï¼Ÿ
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é—®é¢˜ï¼Œæˆ‘çš„ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ç²¾ç¡®å›ç­”è¿™ä¸ªé—®é¢˜ï¼šå¦‚æœä½ è¾“å…¥ç›®æ ‡å›æŠ¥ï¼Œå®ƒå¯ä»¥æ˜¯ä¸“å®¶çš„ä¹Ÿå¯ä»¥æ˜¯éä¸“å®¶çš„ï¼Œé‚£ä¹ˆæ¨¡å‹å®é™…ä¸Šåœ¨å®ç°è¿™ä¸€ç‚¹æ–¹é¢è¡¨ç°å¦‚ä½•ï¼Ÿ
- en: ğŸ˜Šï¼ŒSo the x axis is what we specify as the target return we wantã€‚and the y axis
    is basically how muchï¼Œ how well do we actually getï¼ŸFor referenceã€‚we have this
    green lineï¼Œ which is the oraclesï¼Œ so which meansã€‚ğŸ˜Šã€‚Whatever you desire that this
    student transformer gives it to you so this would have been the ideal caseã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥xè½´æ˜¯æˆ‘ä»¬æŒ‡å®šçš„ç›®æ ‡å›æŠ¥ï¼Œyè½´åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬å®é™…è·å¾—çš„ç¨‹åº¦ã€‚ä½œä¸ºå‚è€ƒï¼Œæˆ‘ä»¬æœ‰è¿™æ¡ç»¿è‰²çº¿ï¼Œå³ç¥è°•çº¿ï¼Œè¿™æ„å‘³ç€ã€‚ğŸ˜Šã€‚æ— è®ºä½ å¸Œæœ›å¾—åˆ°ä»€ä¹ˆï¼Œè¿™ä¸ªå­¦ç”Ÿå˜æ¢å™¨éƒ½èƒ½ç»™ä½ ï¼Œæ‰€ä»¥è¿™å°†æ˜¯ç†æƒ³æƒ…å†µã€‚
- en: so it's a diagonalã€‚We also haveï¼Œ because this is offline RL we have in orangeã€‚what
    was the best project between data setsï¼Œ so the offline data is not perfectã€‚so
    we just plot what is the upper bound on the offline data performanceã€‚And here
    we find that for the majority of the environmentsã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå¯¹è§’çº¿ã€‚æˆ‘ä»¬è¿˜å› ä¸ºè¿™æ˜¯ç¦»çº¿RLï¼Œæ‰€ä»¥æœ‰æ©™è‰²çš„ã€‚å“ªä¸ªæ˜¯æ•°æ®é›†ä¹‹é—´çš„æœ€ä½³é¡¹ç›®ï¼Œæ‰€ä»¥ç¦»çº¿æ•°æ®å¹¶ä¸å®Œç¾ã€‚æˆ‘ä»¬åªæ˜¯ç»˜åˆ¶äº†ç¦»çº¿æ•°æ®æ€§èƒ½çš„ä¸Šé™ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬å‘ç°å¤§å¤šæ•°ç¯å¢ƒéƒ½æ˜¯å¦‚æ­¤ã€‚
- en: there is a good fit between the target return we feed in and the actual performance
    of the modelã€‚And there are some other observations which I wanted to take from
    this slide is thatã€‚Because they can vary this notion of rewardã€‚We can in some
    sense do multitask R by reward condition return conditioning this is not the only
    way to do multitask parallel you can specify a task via natural language you can
    via goal state and so on forthã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡å›æŠ¥ä¸æˆ‘ä»¬è¾“å…¥çš„ç›®æ ‡å›æŠ¥å’Œæ¨¡å‹å®é™…è¡¨ç°ä¹‹é—´çš„å¥‘åˆåº¦è‰¯å¥½ã€‚æˆ‘è¿˜æƒ³ä»è¿™ä¸€å¹»ç¯ç‰‡ä¸­æå–ä¸€äº›å…¶ä»–è§‚å¯Ÿç»“æœã€‚å› ä¸ºä»–ä»¬å¯ä»¥å˜åŒ–è¿™ç§å¥–åŠ±çš„æ¦‚å¿µã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å¥–åŠ±æ¡ä»¶è¿”å›è¿›è¡Œå¤šä»»åŠ¡Rï¼Œè¿™å¹¶ä¸æ˜¯è¿›è¡Œå¤šä»»åŠ¡å¹³è¡Œçš„å”¯ä¸€æ–¹æ³•ï¼Œä½ å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡å®šä»»åŠ¡ï¼Œæˆ–è€…é€šè¿‡ç›®æ ‡çŠ¶æ€ç­‰ç­‰ã€‚
- en: but this is one notion where the notion of a task could be how much reward you
    wantã€‚å—¯ã€‚And another thing to notice is occasionally these smallest extrapolate
    this is not a trend we have been seeing consistentlyã€‚but we do see some signs
    of itï¼Œ So if you look at for example Sequestã€‚ğŸ˜Šï¼ŒHereã€‚the highest return tri cleanerer
    data sets was pretty lowã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯è¿™æ˜¯ä¸€ä¸ªæ¦‚å¿µï¼Œå…¶ä¸­ä»»åŠ¡çš„æ¦‚å¿µå¯èƒ½æ˜¯ä½ æƒ³è¦å¤šå°‘å¥–åŠ±ã€‚å—¯ã€‚å¦ä¸€ä¸ªéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æœ€å°å€¼å¶å°”ä¼šå¤–æ¨ï¼Œè¿™ä¸æ˜¯æˆ‘ä»¬ä¸€ç›´çœ‹åˆ°çš„è¶‹åŠ¿ã€‚æˆ‘ä»¬ç¡®å®çœ‹åˆ°ä¸€äº›è¿¹è±¡ï¼Œæ‰€ä»¥å¦‚æœä½ çœ‹ä¸€ä¸‹ä¾‹å¦‚Sequestã€‚ğŸ˜Šï¼Œåœ¨è¿™é‡Œï¼Œæœ€é«˜çš„å›æŠ¥ä¸‰æ¸…æ•°æ®é›†ç›¸å½“ä½ã€‚
- en: And if we specify a return higher than thatã€‚For our decision transformerã€‚we
    do find that the model is able to achieveã€‚So it is able to generate tragic trees
    with returns higher than it ever saw in the dayã€‚å—¯ã€‚I do believe that future work
    in this space trying to improve this model should think about how can this trend
    be more consistent across environments because this would really achieve the goal
    of offine RLã€‚which is given suboptimal behaviorï¼Œ how do you get optimal behavior
    out of itï¼Ÿ
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æŒ‡å®šä¸€ä¸ªæ¯”è¿™æ›´é«˜çš„å›æŠ¥ã€‚å¯¹äºæˆ‘ä»¬çš„å†³ç­–å˜æ¢å™¨ã€‚æˆ‘ä»¬ç¡®å®å‘ç°æ¨¡å‹èƒ½å¤Ÿå®ç°ã€‚å› æ­¤ï¼Œå®ƒèƒ½å¤Ÿç”Ÿæˆæ¯”å®ƒä»¥å‰åœ¨æ•°æ®ä¸­çœ‹åˆ°çš„æ›´é«˜çš„å›æŠ¥æ ‘ã€‚å—¯ã€‚æˆ‘ç¡®å®ç›¸ä¿¡ï¼Œæœªæ¥åœ¨è¿™ä¸ªé¢†åŸŸçš„å·¥ä½œåº”è€ƒè™‘å¦‚ä½•ä½¿è¿™ä¸€è¶‹åŠ¿åœ¨ä¸åŒç¯å¢ƒä¸­æ›´åŠ ä¸€è‡´ï¼Œå› ä¸ºè¿™å°†çœŸæ­£å®ç°ç¦»çº¿RLçš„ç›®æ ‡ã€‚å³åœ¨æ¬¡ä¼˜è¡Œä¸ºä¸‹ï¼Œå¦‚ä½•ä»ä¸­è·å¾—æœ€ä½³è¡Œä¸ºï¼Ÿ
- en: But remains to be seen how well this trend can be made consistent across environmentsã€‚Can
    I jump in with a questionã€‚Yesï¼Œ so I think that last point is really interesting
    and it's cool that you guys occasionally see itã€‚I'm curious to know what happensã€‚So
    this is all condition you give as an inputã€‚what return you would like and it tries
    to select a sequence of actions that gives itã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»éœ€è§‚å¯Ÿè¿™ä¸ªè¶‹åŠ¿åœ¨ä¸åŒç¯å¢ƒä¸­èƒ½å¦ä¿æŒä¸€è‡´ã€‚æˆ‘å¯ä»¥æ’å…¥ä¸€ä¸ªé—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œæ‰€ä»¥æˆ‘è§‰å¾—æœ€åä¸€ç‚¹éå¸¸æœ‰è¶£ï¼Œä½ ä»¬å¶å°”çœ‹åˆ°è¿™ä¸ªçœŸä¸é”™ã€‚æˆ‘å¾ˆå¥½å¥‡ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ‰€ä»¥è¿™éƒ½æ˜¯ä½ ä½œä¸ºè¾“å…¥ç»™å‡ºçš„æ¡ä»¶ã€‚ä½ æƒ³è¦ä»€ä¹ˆå›æŠ¥ï¼Œå®ƒå°±ä¼šå°è¯•é€‰æ‹©ä¸€ç³»åˆ—èƒ½å¤Ÿæä¾›çš„è¡ŒåŠ¨ã€‚
- en: I'm curious to know what happens if you just give it ridiculous inputs like
    for exampleã€‚you know here here the order of magnitude for the return is like 50
    to 100ã€‚What happens if you put in 10000ã€‚Good question and this is something we
    tried early on I don't want to say if we went up to 10000 but we try like really
    high returns which not even an expert could get and generally we see this leveling
    performance so you can see hints of it in half Che arm and you know pong as well
    or Walker to some extent that if you look at the very end things start saturatingã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¾ˆå¥½å¥‡ï¼Œå¦‚æœä½ ç»™å®ƒä¸€äº›è’è°¬çš„è¾“å…¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œæ¯”å¦‚è¯´ï¼Œä½ çŸ¥é“è¿™é‡Œçš„å›æŠ¥æ•°é‡çº§å¤§æ¦‚æ˜¯50åˆ°100ã€‚å¦‚æœä½ è¾“å…¥10000ä¼šæ€æ ·ï¼Ÿå¥½é—®é¢˜ï¼Œè¿™æ˜¯æˆ‘ä»¬æ—©æœŸå°è¯•è¿‡çš„äº‹æƒ…ã€‚æˆ‘ä¸æƒ³è¯´æˆ‘ä»¬æ˜¯å¦å°è¯•è¿‡10000ï¼Œä½†æˆ‘ä»¬ç¡®å®å°è¯•è¿‡ä¸€äº›å¾ˆé«˜çš„å›æŠ¥ï¼Œè¿™è¿ä¸“å®¶éƒ½æ— æ³•è·å¾—ï¼Œé€šå¸¸æˆ‘ä»¬ä¼šçœ‹åˆ°è¿™ç§æ€§èƒ½æ°´å¹³çš„ç°è±¡ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨Half
    Cheetahå’ŒPongç­‰æ¸¸æˆä¸­çœ‹åˆ°ä¸€äº›è¿¹è±¡ï¼Œæˆ–è€…åœ¨Walkerä¸­ï¼Œåœ¨ä½ è§‚å¯Ÿæœ€åæ—¶åˆ»æ—¶ï¼Œäº‹æƒ…å¼€å§‹é¥±å’Œã€‚
- en: ğŸ˜Šï¼ŒSo if you exceed what is like certain threshold which often corresponds with
    the best tragic thresholdã€‚but not always beyond that everything is similar returns
    so it's not that it so at least one good thing is it does not degrade in performance
    so it would have been a little bit worrying if you specified a return of 10000
    and gives you return which isã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥å¦‚æœä½ è¶…è¿‡äº†æŸä¸ªé˜ˆå€¼ï¼Œè¿™ä¸ªé˜ˆå€¼é€šå¸¸å¯¹åº”äºæœ€ä½³çš„å›æŠ¥é˜ˆå€¼ï¼Œä½†å¹¶ä¸æ€»æ˜¯å¦‚æ­¤ï¼Œè¶…å‡ºè¿™ä¸€ç‚¹åï¼Œæ‰€æœ‰çš„å›æŠ¥éƒ½æ˜¯ç›¸ä¼¼çš„ã€‚å› æ­¤è‡³å°‘æœ‰ä¸€ç‚¹å¥½äº‹æ˜¯ï¼Œå®ƒçš„æ€§èƒ½ä¸ä¼šé€€åŒ–ï¼Œæ‰€ä»¥å¦‚æœä½ æŒ‡å®šä¸€ä¸ª10000çš„å›æŠ¥è€Œå¾—åˆ°çš„å›æŠ¥æ˜¯è¿™æ ·çš„ï¼Œé‚£å°±ä¼šæœ‰ç‚¹ä»¤äººæ‹…å¿§ã€‚
- en: Train0 or something really newã€‚So it's good that it stabilizesã€‚but it's not
    that it keeps increasing on and onï¼Œ so there would be a point where the performance
    would get saturatedã€‚Okayï¼Œ thank youã€‚I was also curious like so usually for transfer
    models you need a lot of dataã€‚so do you know how well like like how much data
    do you need like how where does it scale the data the performanceï¼Ÿ
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Train0æˆ–å…¶ä»–ä¸€äº›æ–°çš„ä¸œè¥¿ã€‚æ‰€ä»¥ç¨³å®šåŒ–æ˜¯å¥½çš„ï¼Œä½†è¿™å¹¶ä¸æ˜¯è¯´å®ƒä¼šæŒç»­ä¸æ–­åœ°å¢åŠ ï¼Œå› æ­¤ä¼šæœ‰ä¸€ä¸ªç‚¹ï¼Œæ€§èƒ½ä¼šè¾¾åˆ°é¥±å’Œã€‚å¥½çš„ï¼Œè°¢è°¢ã€‚æˆ‘ä¹Ÿå¾ˆå¥½å¥‡ï¼Œé€šå¸¸å¯¹äºè¿ç§»æ¨¡å‹ï¼Œä½ éœ€è¦å¾ˆå¤šæ•°æ®ã€‚é‚£ä¹ˆä½ çŸ¥é“éœ€è¦å¤šå°‘æ•°æ®å—ï¼Ÿæ•°æ®çš„è§„æ¨¡å’Œæ€§èƒ½ä¹‹é—´çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
- en: Yeahï¼Œ so here we useã€‚The standardã€‚UmData like the D4 RL benchmarks for Mexicoã€‚which
    I think have million u transitions in the order of millionsã€‚Forã€‚Atariã€‚we used
    one person of the replay bufferï¼Œ which is smaller than the one we used for the
    Mujoco benchmarksã€‚and I actually have a result in the very next slide which shows
    dis transformer especially being useful when you have a little dataã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨äº†æ ‡å‡†çš„æ•°æ®ï¼Œæ¯”å¦‚D4RLçš„å¢¨è¥¿å“¥åŸºå‡†ï¼Œæˆ‘è®¤ä¸ºæœ‰æ•°ç™¾ä¸‡çš„è½¬ç§»æ•°æ®ã€‚å¯¹äºAtariï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é‡æ”¾ç¼“å†²åŒºä¸­çš„ä¸€éƒ¨åˆ†ï¼Œè¿™ä¸ªè§„æ¨¡å°äºæˆ‘ä»¬ç”¨äºMujocoåŸºå‡†çš„æ•°æ®ã€‚æˆ‘åœ¨ä¸‹ä¸€å¼ å¹»ç¯ç‰‡ä¸­æœ‰ä¸€ä¸ªç»“æœï¼Œæ˜¾ç¤ºäº†åœ¨æ•°æ®è¾ƒå°‘æ—¶ï¼Œå˜æ¢å™¨ç‰¹åˆ«æœ‰ç”¨ã€‚
- en: So yeahï¼Œ so I guess one question to ask before you move onã€‚In the last slideã€‚what
    do you mean again by return conditioning for like the multitask partï¼ŸYeahã€‚so if
    you think about the returns to go at test timeã€‚the one you feed have to feed in
    as the starting tokenã€‚As one girlã€‚Specifying what policy you wantã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ˜¯çš„ï¼Œæˆ‘æƒ³åœ¨ä½ ç»§ç»­ä¹‹å‰é—®ä¸€ä¸ªé—®é¢˜ã€‚åœ¨ä¸Šä¸€å¼ å¹»ç¯ç‰‡ä¸­ï¼Œä½ å†æ¬¡æåˆ°çš„å¤šä»»åŠ¡éƒ¨åˆ†çš„å›æŠ¥æ¡ä»¶æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿæ˜¯çš„ï¼Œæ‰€ä»¥å¦‚æœä½ è€ƒè™‘æµ‹è¯•æ—¶çš„ç›®æ ‡å›æŠ¥ï¼Œä½ å¿…é¡»ä½œä¸ºèµ·å§‹æ ‡è®°è¾“å…¥çš„é‚£ä¸€ä¸ªã€‚ä½œä¸ºä¸€ä¸ªæ ‡è®°ï¼ŒæŒ‡å®šä½ æƒ³è¦çš„ç­–ç•¥ã€‚
- en: å–‚ã€‚How is that multitï¼ŸSo it's multitask in the sense that because you can get
    different policies by changing your target return to goã€‚you're essentially getting
    different behaviors encoded so think about for instance a hopper and you specify
    a return to go that's really lowã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å–‚ï¼Œè¿™æ ·çš„å¤šä»»åŠ¡æ€ä¹ˆç†è§£ï¼Ÿæ‰€ä»¥å®ƒæ˜¯å¤šä»»åŠ¡ï¼Œå› ä¸ºä½ å¯ä»¥é€šè¿‡æ”¹å˜ç›®æ ‡å›æŠ¥æ¥è·å¾—ä¸åŒçš„ç­–ç•¥ï¼Œå®è´¨ä¸Šæ˜¯åœ¨ç¼–ç ä¸åŒçš„è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œæƒ³è±¡ä¸€ä¸ªè·³è·ƒè€…ï¼Œä½ æŒ‡å®šä¸€ä¸ªéå¸¸ä½çš„ç›®æ ‡å›æŠ¥ã€‚
- en: So you're basically sayingï¼Œ get me an agent which will just stick around its
    initial stateã€‚And not going into unchared territoryã€‚And if you give it reallyï¼Œ
    really highã€‚then you're asking it to do the traditional taskï¼Œ which is to hop
    and go as far as possible without falling but can you qualify those multitask
    because that basically just means that your return conditioning is a cue for it
    to memorize right which is usually like one of the pitfalls of multitaskã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ åŸºæœ¬ä¸Šæ˜¯è¯´ï¼Œç»™æˆ‘ä¸€ä¸ªä»£ç†ï¼Œå®ƒåªä¼šåœç•™åœ¨åˆå§‹çŠ¶æ€ï¼Œè€Œä¸ä¼šè¿›å…¥æœªçŸ¥é¢†åŸŸã€‚å¦‚æœä½ ç»™å®ƒéå¸¸éå¸¸é«˜çš„ç›®æ ‡å›æŠ¥ï¼Œé‚£ä¹ˆä½ è¦æ±‚å®ƒæ‰§è¡Œçš„ä¼ ç»Ÿä»»åŠ¡å°±æ˜¯è·³è·ƒå¹¶å°½å¯èƒ½è¿œåœ°ç§»åŠ¨è€Œä¸æ‘”å€’ï¼Œä½†ä½ èƒ½å¦å¯¹é‚£äº›å¤šä»»åŠ¡è¿›è¡Œé‡åŒ–ï¼Œå› ä¸ºè¿™åŸºæœ¬ä¸Šæ„å‘³ç€ä½ çš„å›æŠ¥æ¡ä»¶æ˜¯å®ƒè®°å¿†çš„æç¤ºï¼Œå¯¹å§ï¼Ÿè¿™é€šå¸¸æ˜¯å¤šä»»åŠ¡çš„ä¸€ä¸ªé™·é˜±ã€‚
- en: So I'm not sure if it's a task identifierï¼Œ that's what I'm trying to sayã€‚So
    I'm not sure if it's memorization because like I think the purpose of thisã€‚I meanã€‚like
    having an offline data set that's fixed is basically saying that it's veryã€‚very
    specific to if you had the same start state and you took the same actions and
    you had the same target fair turnsã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¸ç¡®å®šè¿™æ˜¯å¦æ˜¯ä»»åŠ¡æ ‡è¯†ç¬¦ï¼Œè¿™æ­£æ˜¯æˆ‘æƒ³è¯´çš„ã€‚æˆ‘ä¸ç¡®å®šè¿™æ˜¯å¦æ˜¯è®°å¿†åŒ–ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºè¿™ä¸ªç›®çš„ã€‚å°±æ˜¯è¯´ï¼Œæ‹¥æœ‰ä¸€ä¸ªå›ºå®šçš„ç¦»çº¿æ•°æ®é›†åŸºæœ¬ä¸Šæ˜¯åœ¨è¯´ï¼Œå¦‚æœä½ æœ‰ç›¸åŒçš„èµ·å§‹çŠ¶æ€å¹¶é‡‡å–ç›¸åŒçš„åŠ¨ä½œï¼Œå¹¶ä¸”æœ‰ç›¸åŒçš„ç›®æ ‡å…¬å¹³æ”¶ç›Šï¼Œè¿™å°†æ˜¯éå¸¸ç‰¹å®šçš„ã€‚
- en: that would qualify as memorizationã€‚But here at test time we allow all of these
    things to change and in fact they do change so your initial state would be differentã€‚your
    target return it could be very could be a different scalar than one you ever saw
    during trainingã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†è¢«è§†ä¸ºè®°å¿†åŒ–ã€‚ä½†åœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å…è®¸æ‰€æœ‰è¿™äº›ä¸œè¥¿å‘ç”Ÿå˜åŒ–ï¼Œå®é™…ä¸Šå®ƒä»¬ç¡®å®ä¼šå˜åŒ–ï¼Œæ‰€ä»¥ä½ çš„åˆå§‹çŠ¶æ€ä¼šä¸åŒï¼Œç›®æ ‡æ”¶ç›Šå¯èƒ½ä¸è®­ç»ƒæœŸé—´çœ‹åˆ°çš„å®Œå…¨ä¸åŒã€‚
- en: It yeah and so so essentially the model has to learn to generate that behavior
    starting from a different initial state and maybe a different value of the target
    return than it saw during during during training if the dynamics are stochastic
    that also makes so that even if you memorize the actions you're not guaranteed
    to get the next the same next stateã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå› æ­¤æœ¬è´¨ä¸Šæ¨¡å‹å¿…é¡»å­¦ä¹ åœ¨ä¸åŒçš„åˆå§‹çŠ¶æ€å’Œç›®æ ‡æ”¶ç›Šå€¼ä¸‹ç”Ÿæˆè¯¥è¡Œä¸ºï¼Œè¿™ä¸è®­ç»ƒæœŸé—´çœ‹åˆ°çš„æƒ…å†µå¯èƒ½ä¸åŒã€‚å¦‚æœåŠ¨æ€æ˜¯éšæœºçš„ï¼Œå³ä½¿ä½ è®°ä½äº†åŠ¨ä½œï¼Œä¹Ÿä¸èƒ½ä¿è¯å¾—åˆ°ç›¸åŒçš„ä¸‹ä¸€ä¸ªçŠ¶æ€ã€‚
- en: so you would actually have a bad correlation with the performance of the dynamics
    are also stochasticã€‚Alsoï¼Œ I was very curious how much time does it take to train
    business transformerï¼ŸIn generalã€‚So it takes about a few hoursï¼Œ so I want to say
    like aboutã€‚45 hoursã€‚Depending on what quality GPU useï¼Œ but yeah thats that's a
    reasonable estimate y call it thanksã€‚Okayã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å®é™…ä¸Šä¼šä¸åŠ¨æ€æ€§èƒ½æœ‰ä¸è‰¯ç›¸å…³æ€§ï¼Œå› ä¸ºåŠ¨æ€ä¹Ÿæ˜¯éšæœºçš„ã€‚å¦å¤–ï¼Œæˆ‘å¾ˆå¥½å¥‡è®­ç»ƒå•†ä¸šå˜æ¢å™¨éœ€è¦å¤šå°‘æ—¶é—´ï¼Ÿä¸€èˆ¬æ¥è¯´ï¼Œå¤§çº¦éœ€è¦å‡ ä¸ªå°æ—¶ï¼Œæˆ‘æƒ³è¯´å¤§çº¦æ˜¯45å°æ—¶ã€‚è¿™å–å†³äºä½¿ç”¨çš„GPUè´¨é‡ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¼°è®¡ã€‚
- en: so actually while doing this experimentï¼Œ this projectã€‚we thought of a baseline
    which we were surprised as not there in previously traditional on offine RL but
    makes very much sense and we thought we should also think about whether decision
    transformers are actually doing something very similar that baseline and the baseline
    is what we call as person behavioral cloningã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®é™…ä¸Šåœ¨è¿›è¡Œè¿™ä¸ªå®éªŒå’Œé¡¹ç›®æ—¶ï¼Œæˆ‘ä»¬æƒ³åˆ°äº†ä¸€ä¸ªåŸºçº¿ï¼Œæˆ‘ä»¬å¾ˆæƒŠè®¶åœ¨ä¹‹å‰çš„ä¼ ç»Ÿç¦»çº¿RLä¸­å¹¶ä¸å­˜åœ¨ï¼Œä½†å®ƒéå¸¸æœ‰æ„ä¹‰ï¼Œæˆ‘ä»¬è¿˜æƒ³è€ƒè™‘å†³ç­–å˜æ¢å™¨æ˜¯å¦åœ¨åšä¸€äº›ç±»ä¼¼çš„äº‹æƒ…ï¼Œè€Œè¿™ä¸ªåŸºçº¿æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸ªäººè¡Œä¸ºå…‹éš†ã€‚
- en: So behavioral cloï¼Œ what it does is basically it ignores the returnsã€‚And simply
    imitates the agent looking by just trying to map the actions given the current
    statesã€‚This is not a good idea with an offline inter set which will have project
    trees of both low returns and high returnsã€‚So traditional vehicle cloningï¼Œ it's
    common to see that as a baseline in offline Ral methods and it is unless you have
    a very high quality data setã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: è¡Œä¸ºå…‹éš†çš„ä½œç”¨åŸºæœ¬ä¸Šæ˜¯å¿½ç•¥æ”¶ç›Šï¼Œç®€å•åœ°é€šè¿‡å°è¯•æ˜ å°„å½“å‰çŠ¶æ€ä¸‹çš„åŠ¨ä½œæ¥æ¨¡ä»¿ä»£ç†ã€‚è¿™åœ¨ç¦»çº¿æ•°æ®é›†ä¸Šä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ï¼Œå› ä¸ºå®ƒä¼šæœ‰ä½æ”¶ç›Šå’Œé«˜æ”¶ç›Šçš„é¡¹ç›®æ ‘ã€‚ä¼ ç»Ÿçš„è½¦è¾†å…‹éš†ï¼Œé€šå¸¸è¢«è§†ä¸ºç¦»çº¿RLæ–¹æ³•çš„åŸºçº¿ï¼Œé™¤éä½ æ‹¥æœ‰éå¸¸é«˜è´¨é‡çš„æ•°æ®é›†ã€‚
- en: does it is not a good baseline for offline RLã€‚Howeverï¼Œ there is a version that
    we call as Per BCã€‚which actually makes quite a lot of sense and in this versionã€‚We
    filter out the top tra trees from our offline setã€‚once stop the ones which have
    the highest rewardsã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹äºç¦»çº¿RLæ¥è¯´å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„åŸºçº¿ã€‚ç„¶è€Œï¼Œæœ‰ä¸€ä¸ªæˆ‘ä»¬ç§°ä¹‹ä¸ºPer BCçš„ç‰ˆæœ¬ï¼Œè¿™å®é™…ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ã€‚åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬ä»ç¦»çº¿é›†ä¸­è¿‡æ»¤å‡ºæ”¶ç›Šæœ€é«˜çš„é¡¹ç›®æ ‘ã€‚
- en: you know the rewards for each transition you calculate the returns of the tra
    trees and you take the tra trees with the highest returns and keep a certain percentage
    of themã€‚which is going to be hyperparameter hereã€‚And once you keep those top fraction
    of your tragic treesã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“æ¯ä¸ªè½¬ç§»çš„å¥–åŠ±ï¼Œä½ è®¡ç®—é¡¹ç›®æ ‘çš„æ”¶ç›Šï¼Œä¿ç•™æ”¶ç›Šæœ€é«˜çš„é¡¹ç›®æ ‘ï¼Œå¹¶ä¿æŒä¸€å®šçš„ç™¾åˆ†æ¯”ï¼Œè¿™å°†æ˜¯è¶…å‚æ•°ã€‚ä¸€æ—¦ä½ ä¿ç•™äº†è¿™äº›æœ€é«˜æ¯”ä¾‹çš„é¡¹ç›®æ ‘ã€‚
- en: you then just ask your module to imitate themã€‚Whi so imitation learning also
    uses especially when it's used in the form of behavioral planningã€‚it uses supervised
    learningï¼Œ essentially it's a supervised learning problem so you could actually
    also get supervised learning objective functions if you did this filtering stepã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ åªæ˜¯è¦æ±‚ä½ çš„æ¨¡å—æ¨¡ä»¿å®ƒä»¬ã€‚å› æ­¤ï¼Œæ¨¡ä»¿å­¦ä¹ ä¹Ÿç‰¹åˆ«ä½¿ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨è¡Œä¸ºè§„åˆ’çš„å½¢å¼ä¸­ã€‚å®ƒä½¿ç”¨ç›‘ç£å­¦ä¹ ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œå› æ­¤å¦‚æœä½ æ‰§è¡Œè¿™ä¸ªè¿‡æ»¤æ­¥éª¤ï¼Œå®é™…ä¸Šä¹Ÿå¯ä»¥å¾—åˆ°ç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ã€‚
- en: Ohã€‚Soã€‚And what we find actually that for the moderate and highal regimesã€‚the
    recent transform is actually very comparable to Pu and VC so it's a very strong
    baseline which I think all the future work in offline Rs should include there's
    actually an eyeC submission from last week which has a much more detailed analysis
    on justice this baseline that introduced in this paperã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚æˆ‘ä»¬å‘ç°ï¼Œå®é™…ä¸Šåœ¨ä¸­ç­‰å’Œé«˜æ•°æ®æƒ…å†µä¸‹ï¼Œæœ€è¿‘çš„å˜æ¢å™¨ä¸ Pu å’Œ VC éå¸¸å¯æ¯”ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºçš„åŸºçº¿ï¼Œæˆ‘è®¤ä¸ºæ‰€æœ‰æœªæ¥çš„ç¦»çº¿ Rs å·¥ä½œéƒ½åº”è¯¥åŒ…æ‹¬è¿™ä¸ªã€‚å®é™…ä¸Šï¼Œä¸Šå‘¨æœ‰ä¸€ç¯‡æäº¤çš„
    eyeC æ–‡ç« å¯¹æ­¤åŸºçº¿è¿›è¡Œäº†æ›´è¯¦ç»†çš„åˆ†æï¼Œè¿™åœ¨æœ¬è®ºæ–‡ä¸­æœ‰æ‰€ä»‹ç»ã€‚
- en: Andã€‚What we do find is that for low data regimes the dis transformer does much
    better than person behavioral cloã€‚so this is for the Atari benchmarks where like
    I previously mentioned we have a much smaller data setã€‚As compared to the Mojoko
    environmentsï¼Œ and here we find that for even after varying the different fraction
    of the percentage hyperparameter hereã€‚we are generally not able to get the strong
    performance status in transformer getsã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å‘ç°ï¼Œåœ¨ä½æ•°æ®æƒ…å†µä¸‹ï¼Œå˜æ¢å™¨çš„è¡¨ç°è¿œè¿œä¼˜äºè¡Œä¸ºå…‹éš†ã€‚è¿™æ˜¯é’ˆå¯¹ Atari åŸºå‡†æµ‹è¯•çš„ï¼Œæ­£å¦‚æˆ‘ä¹‹å‰æåˆ°çš„ï¼Œæˆ‘ä»¬çš„æ•°æ®é›†è¦å°å¾—å¤šã€‚ä¸ Mojoko ç¯å¢ƒç›¸æ¯”ï¼Œæˆ‘ä»¬å‘ç°å³ä½¿åœ¨ä¸åŒçš„ç™¾åˆ†æ¯”è¶…å‚æ•°å˜åŒ–ä¸‹ï¼Œé€šå¸¸ä¹Ÿæ— æ³•è¾¾åˆ°å˜æ¢å™¨çš„å¼ºæ€§èƒ½ã€‚
- en: So 10% DCc basically means that we filter out and keep the top 10% of the trajectories
    if you go even lower then the start data set becomes fairly small so the baseline
    become meaninglessã€‚But for even the reasonable rangesï¼Œ we'd never find the performance
    matching that of the transformers for these Atari benchmarksã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ 10% DCc åŸºæœ¬ä¸Šæ„å‘³ç€æˆ‘ä»¬è¿‡æ»¤æ‰å¹¶ä¿ç•™å‰ 10% çš„è½¨è¿¹ï¼Œå¦‚æœä½ å†ä½ä¸€äº›ï¼Œèµ·å§‹æ•°æ®é›†å°±ä¼šå˜å¾—ç›¸å½“å°ï¼Œå› æ­¤åŸºçº¿å˜å¾—æ¯«æ— æ„ä¹‰ã€‚ä½†æ˜¯å¯¹äºåˆç†çš„èŒƒå›´ï¼Œæˆ‘ä»¬ä»æœªå‘ç°æ€§èƒ½èƒ½åŒ¹é…å˜æ¢å™¨åœ¨è¿™äº›
    Atari åŸºå‡†æµ‹è¯•ä¸­çš„è¡¨ç°ã€‚
- en: å‘ƒçš„é“ I mayã€‚So I notice in table threeï¼Œ for exampleï¼Œ which is not this table with
    one just before in the paperã€‚there's a report on the CQL performanceï¼Œ which to
    me also feels you know intuitively pretty similar to the percent BC in the sense
    of like you pick trajectories you know performing well and you try and stay roughly
    within sort of the same kind of policy distributionã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œæˆ‘å¯èƒ½æ³¨æ„åˆ°äº†ã€‚åœ¨ç¬¬ä¸‰å¼ è¡¨ä¸­ï¼Œä¾‹å¦‚ï¼Œè¿™ä¸æ˜¯è®ºæ–‡å‰é¢çš„é‚£å¼ è¡¨ã€‚è¿™é‡Œæœ‰å…³äº CQL æ€§èƒ½çš„æŠ¥å‘Šï¼Œè¿™è®©æˆ‘è§‰å¾—åœ¨ç›´è§‰ä¸Šä¸ç™¾åˆ†æ¯” BC ç›¸ä¼¼ï¼Œæ„æ€æ˜¯ä½ é€‰æ‹©è¡¨ç°è‰¯å¥½çš„è½¨è¿¹ï¼Œå¹¶å°½é‡ä¿æŒåœ¨ç›¸ä¼¼çš„ç­–ç•¥åˆ†å¸ƒå†…ã€‚
- en: It state space distributionã€‚I was curious on this oneã€‚do you have a sense of
    what the CQL performance was relative to say the percent BC performance hereï¼Ÿ
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯çŠ¶æ€ç©ºé—´åˆ†å¸ƒã€‚æˆ‘å¯¹æ­¤æ„Ÿåˆ°å¥½å¥‡ã€‚ä½ æ˜¯å¦çŸ¥é“ CQL æ€§èƒ½ç›¸å¯¹äºè¿™é‡Œçš„ç™¾åˆ†æ¯” BC æ€§èƒ½å¦‚ä½•ï¼Ÿ
- en: So that's a great questionï¼Œ question is that even for CQL you rely on this notion
    of pessimism where you want to pick trajectories where you're more confident in
    and try to make sure policy remains in that regionã€‚so I don't have the numbers
    with CQL on this tableã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸ªå¥½é—®é¢˜ï¼Œé—®é¢˜æ˜¯å³ä½¿å¯¹äº CQLï¼Œä½ ä¹Ÿä¾èµ–äºè¿™ç§æ‚²è§‚çš„æ¦‚å¿µï¼Œåœ¨é€‰æ‹©æ›´æœ‰ä¿¡å¿ƒçš„è½¨è¿¹æ—¶ï¼Œå¹¶åŠªåŠ›ç¡®ä¿ç­–ç•¥ä¿æŒåœ¨è¯¥åŒºåŸŸã€‚æ‰€ä»¥æˆ‘æ²¡æœ‰è¿™ä¸ªè¡¨ä¸­ CQL çš„æ•°å­—ã€‚
- en: but if you look at the detailed results for Atringã€‚Then I think should they
    should have the SQL for sure because that's the numbers we are reporting here
    so I can tell you what the sQL performance is actually pretty good and it's very
    competitiveã€‚When the decision transformer for aaryï¼ŸSo this TD learning baseline
    here is SeQã€‚So financially by extensionï¼Œ I would imagine it doing better than
    person Bã€‚Yeahã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœä½ æŸ¥çœ‹ Atring çš„è¯¦ç»†ç»“æœã€‚æˆ‘è®¤ä¸ºä»–ä»¬è‚¯å®šåº”è¯¥æœ‰ SQLï¼Œå› ä¸ºè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡ŒæŠ¥å‘Šçš„æ•°å­—ï¼Œæ‰€ä»¥æˆ‘å¯ä»¥å‘Šè¯‰ä½ ï¼ŒSQL æ€§èƒ½å®é™…ä¸Šç›¸å½“ä¸é”™ï¼Œè€Œä¸”ç«äº‰åŠ›å¾ˆå¼ºã€‚å½“å†³ç­–å˜æ¢å™¨é’ˆå¯¹
    aary æ—¶ï¼Ÿæ‰€ä»¥è¿™é‡Œçš„ TD å­¦ä¹ åŸºçº¿æ˜¯ SeQã€‚å› æ­¤ï¼Œä»è´¢åŠ¡è§’åº¦æ¥çœ‹ï¼Œæˆ‘æƒ³å®ƒçš„è¡¨ç°ä¼šæ¯” B ä¸ªäººæ›´å¥½ã€‚æ˜¯çš„ã€‚
- en: And apologize apologize if this was mentionedï¼Œ I just missed itã€‚but if you have
    the sense that this is basically like a failure of CQL to be able to extrapolate
    well or sort of stitch together different parts of trajectoriesã€‚Whereas a decision
    transformer can sort of make that extrapolation between you have like the first
    half of one trajectory is really good the second half of one trajectory is really
    good so you can actually piece those together a decision transformer where you
    can't necessarily do that with CQL because the path connecting those may not necessarily
    be well covered by the behavior policyã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä¹‹å‰æåˆ°è¿‡ï¼Œæˆ‘è¡¨ç¤ºæ­‰æ„ï¼Œæˆ‘åªæ˜¯é”™è¿‡äº†ã€‚ä½†æ˜¯å¦‚æœä½ æœ‰æ„Ÿè§‰ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯CQLåœ¨å¤–æ¨æ–¹é¢çš„å¤±è´¥ï¼Œæˆ–è€…è¯´åœ¨æ‹¼æ¥è½¨è¿¹çš„ä¸åŒéƒ¨åˆ†æ—¶å­˜åœ¨é—®é¢˜ã€‚è€Œå†³ç­–å˜æ¢å™¨èƒ½å¤Ÿè¿›è¡Œè¿™ç§å¤–æ¨ï¼Œæ¯”å¦‚ç¬¬ä¸€æ®µè½¨è¿¹éå¸¸å¥½ï¼Œç¬¬äºŒæ®µè½¨è¿¹ä¹Ÿå¾ˆå¥½ï¼Œå› æ­¤å¯ä»¥å°†å®ƒä»¬æ‹¼æ¥èµ·æ¥ï¼Œè€ŒCQLå¯èƒ½åšä¸åˆ°ï¼Œå› ä¸ºè¿æ¥è¿™äº›çš„è·¯å¾„å¯èƒ½æœªè¢«è¡Œä¸ºç­–ç•¥è‰¯å¥½è¦†ç›–ã€‚
- en: Yeahï¼Œ yeahï¼Œ so and this actually goes to one of the intuitions which I did not
    emphasize too muchã€‚but we have a discussion on the paper where essentially why
    do we expect a transformer or any model for that matter to look at offline data
    that's suboptimal and get something a policy which generates optimal loadoutsï¼Ÿ
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ˜¯çš„ï¼Œè¿™å®é™…ä¸Šæ¶‰åŠåˆ°ä¸€ä¸ªæˆ‘æ²¡æœ‰å¼ºè°ƒå¤ªå¤šçš„ç›´è§‰ï¼Œä½†æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­è®¨è®ºäº†ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬ä¸ºä»€ä¹ˆæœŸæœ›å˜æ¢å™¨æˆ–ä»»ä½•æ¨¡å‹æŸ¥çœ‹æ¬¡ä¼˜çš„ç¦»çº¿æ•°æ®å¹¶å¾—åˆ°ç”Ÿæˆæœ€ä¼˜è´Ÿè½½çš„ç­–ç•¥ï¼Ÿ
- en: The intuition is that as Scott was mentioningï¼Œ you could perhapsã€‚Stitch togetherã€‚Good
    behaviors from suboptimal trajectories and that stitching could perhaps lead to
    a behavior that was better than anything you saw in individual tra trees in your
    data setã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ç›´è§‰æ˜¯ï¼Œå¦‚æ–¯ç§‘ç‰¹æåˆ°çš„ï¼Œä½ æˆ–è®¸å¯ä»¥å°†æ¬¡ä¼˜è½¨è¿¹ä¸­çš„è‰¯å¥½è¡Œä¸ºæ‹¼æ¥åœ¨ä¸€èµ·ï¼Œè€Œè¿™ç§æ‹¼æ¥å¯èƒ½ä¼šå¯¼è‡´æ¯”æ•°æ®é›†ä¸­å•ä¸ªè½¨è¿¹æ›´å¥½çš„è¡Œä¸ºã€‚
- en: It's something we find earlier evidence of small skill experiment for graphsã€‚and
    that's really our hope also that something that will transformers very good at
    because it can attend to very long sequences so it could identify those segments
    of behaviorã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬åœ¨å°è§„æ¨¡å›¾å®éªŒä¸­æ—©æœŸå‘ç°çš„è¯æ®ã€‚æˆ‘ä»¬ä¹Ÿå¸Œæœ›å˜æ¢å™¨åœ¨è¿™æ–¹é¢è¡¨ç°è‰¯å¥½ï¼Œå› ä¸ºå®ƒå¯ä»¥å…³æ³¨éå¸¸é•¿çš„åºåˆ—ï¼Œå› æ­¤èƒ½å¤Ÿè¯†åˆ«è¿™äº›è¡Œä¸ºç‰‡æ®µã€‚
- en: which when stitch together would give you optimal optimal behaviorã€‚Soã€‚And it's
    very much possible that is something unique to this in transformers and something
    like CQL would not be able to do person BCC because it's filtering out the data
    is automatically being limited and not being able to do that because those segments
    of good behavior could be in trajectories which overall do not have a high returnã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æ‹¼æ¥åœ¨ä¸€èµ·æ—¶ï¼Œä¼šç»™å‡ºæœ€ä¼˜çš„è¡Œä¸ºã€‚æ‰€ä»¥ï¼Œå¾ˆå¯èƒ½è¿™åœ¨å˜æ¢å™¨ä¸­æ˜¯ç‹¬ç‰¹çš„ï¼Œè€ŒåƒCQLè¿™æ ·çš„æ¨¡å‹å¯èƒ½åšä¸åˆ°ï¼Œå› ä¸ºå®ƒè¿‡æ»¤æ‰çš„æ•°æ®æ˜¯è‡ªåŠ¨é™åˆ¶çš„ï¼Œæ— æ³•è¿›è¡Œè¿™æ ·çš„æ“ä½œï¼Œå› ä¸ºè¿™äº›è‰¯å¥½è¡Œä¸ºçš„ç‰‡æ®µå¯èƒ½å‡ºç°åœ¨æ•´ä½“å›æŠ¥ä¸é«˜çš„è½¨è¿¹ä¸­ã€‚
- en: but so if you filter them outï¼Œ you are losing all of that informationã€‚Okayã€‚so
    I said there is hyperparameter the context in K and like with most of perceptionã€‚one
    of the big advantages of transformers opposed to other sequence models like LSTMs
    is that they can process really large sequencesã€‚And here at a first glanceï¼Œ it
    might seem that being Markcoan would have been helpful for RLã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¦‚æœä½ å°†å®ƒä»¬è¿‡æ»¤æ‰ï¼Œä½ å°±å¤±å»äº†æ‰€æœ‰è¿™äº›ä¿¡æ¯ã€‚å¥½çš„ï¼Œæˆ‘è¯´è¿‡è¿™é‡Œçš„è¶…å‚æ•°æ˜¯ä¸Šä¸‹æ–‡å’ŒKï¼Œå’Œå¤§å¤šæ•°æ„ŸçŸ¥æ¨¡å‹ä¸€æ ·ï¼Œå˜æ¢å™¨ç›¸å¯¹äºå…¶ä»–åºåˆ—æ¨¡å‹å¦‚LSTMçš„ä¸€ä¸ªå¤§ä¼˜åŠ¿æ˜¯å®ƒä»¬å¯ä»¥å¤„ç†éå¸¸å¤§çš„åºåˆ—ã€‚ä»è¡¨é¢ä¸Šçœ‹ï¼Œè¿™å¯èƒ½çœ‹èµ·æ¥åœ¨RLä¸­ä½¿ç”¨Markovianä¼šå¾ˆæœ‰å¸®åŠ©ã€‚
- en: which also was a question that was raised earlierã€‚so we did this experiment
    where we did compare performance with context and K equals1 and here we had context
    and between 30 for the environments in 50 for pong and we find that increasing
    the context length is veryã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¹Ÿæ˜¯ä¹‹å‰æåˆ°çš„ä¸€ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¿™ä¸ªå®éªŒï¼Œæ¯”è¾ƒäº†ä¸Šä¸‹æ–‡ä¸Kç­‰äº1çš„æ€§èƒ½ï¼Œè¿™é‡Œæˆ‘ä»¬æœ‰ä¸Šä¸‹æ–‡ï¼Œç¯å¢ƒä¸­ä¸º30ï¼Œä¹’ä¹“çƒä¸º50ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ ä¸Šä¸‹æ–‡é•¿åº¦æ˜¯éå¸¸é‡è¦çš„ã€‚
- en: very important to get good performanceã€‚Okayã€‚Nowå‘ƒã€‚So so far I've showed you how
    this in transformformerã€‚which is very simpleï¼Œ there was no slide I had which was
    going into the details of dynamic programmingã€‚which is the crux of most Rï¼Œ this
    was just pure supervised learning in an autoregressive framework that was getting
    us as good performanceã€‚å—¯ã€‚What about cases where this approach actually starts
    outperforming some of the traditional methods are so to probe a little bit further
    we start looking at sparse reward environments and basically we just took our
    existing mojoku environments and then instead of giving it the information for
    reward for every transition we fed it the cumulative reward at the end of the
    trajectory so every transition will have a zero reward except the very end where
    you get the entire reward at once it's a very sparse reward perform scenario for
    that reasonã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è·å–è‰¯å¥½æ€§èƒ½éå¸¸é‡è¦ã€‚å¥½çš„ã€‚é‚£ä¹ˆåˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ç»™ä½ å±•ç¤ºäº†å˜æ¢å™¨ä¸­çš„è¿™äº›å†…å®¹ï¼Œè¿™éå¸¸ç®€å•ï¼Œæˆ‘æ²¡æœ‰å“ªä¸€å¼ å¹»ç¯ç‰‡æ˜¯æ·±å…¥æ¢è®¨åŠ¨æ€ç¼–ç¨‹çš„ï¼Œè€ŒåŠ¨æ€ç¼–ç¨‹æ˜¯å¤§å¤šæ•°Rçš„å…³é”®ï¼Œè¿™åªæ˜¯çº¯ç²¹çš„ç›‘ç£å­¦ä¹ ï¼Œåœ¨è‡ªå›å½’æ¡†æ¶ä¸‹å–å¾—äº†è‰¯å¥½çš„æ€§èƒ½ã€‚å—¯ã€‚é‚£ä¹ˆåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿™ç§æ–¹æ³•å®é™…ä¸Šå¼€å§‹è¶…è¶Šä¸€äº›ä¼ ç»Ÿæ–¹æ³•å‘¢ï¼Ÿæ‰€ä»¥ä¸ºäº†è¿›ä¸€æ­¥æ¢è®¨ï¼Œæˆ‘ä»¬å¼€å§‹ç ”ç©¶ç¨€ç–å¥–åŠ±ç¯å¢ƒï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬åªæ˜¯æ‹¿ç°æœ‰çš„mojokuç¯å¢ƒï¼Œç„¶åä¸æ˜¯ä¸ºæ¯ä¸ªè¿‡æ¸¡æä¾›å¥–åŠ±ä¿¡æ¯ï¼Œè€Œæ˜¯åœ¨è½¨è¿¹ç»“æŸæ—¶ç»™äºˆç´¯è®¡å¥–åŠ±ï¼Œæ‰€ä»¥æ¯ä¸ªè¿‡æ¸¡çš„å¥–åŠ±ä¸ºé›¶ï¼Œåªæœ‰åœ¨æœ€åæ‰èƒ½ä¸€æ¬¡æ€§è·å¾—å…¨éƒ¨å¥–åŠ±ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªéå¸¸ç¨€ç–å¥–åŠ±çš„è¡¨ç°åœºæ™¯ã€‚
- en: And here we find thatã€‚å“¦ã€‚Compared to the original denses resultsï¼Œ the delayed
    results for Dtã€‚they will deteriorate a little bitï¼Œ which is expected because now
    you are withholding some of the more fine grain information at every time stepã€‚but
    the drop isã€‚Not too significant compared to the original E performance hereã€‚whereas
    for something like CQL there is a drastic drop in performance so CQL suffers quite
    a lot in sparse reward scenariosã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œæˆ‘ä»¬å‘ç°ï¼Œå“¦ã€‚ä¸åŸå§‹å¯†é›†ç»“æœç›¸æ¯”ï¼ŒDTçš„å»¶è¿Ÿç»“æœä¼šç¨å¾®æ¶åŒ–ï¼Œè¿™æ˜¯æ„æ–™ä¹‹ä¸­çš„ï¼Œå› ä¸ºç°åœ¨ä½ åœ¨æ¯ä¸ªæ—¶é—´æ­¥éª¤éƒ½ withholdingäº†ä¸€äº›æ›´ç»†ç²’åº¦çš„ä¿¡æ¯ï¼Œä½†ä¸åŸå§‹Eæ€§èƒ½ç›¸æ¯”ï¼Œä¸‹é™å¹¶ä¸å¤ªæ˜¾è‘—ã€‚è€Œå¯¹äºCQLè¿™æ ·çš„ç®—æ³•ï¼Œæ€§èƒ½ä¸‹é™éå¸¸æ˜æ˜¾ï¼Œæ‰€ä»¥CQLåœ¨ç¨€ç–å¥–åŠ±åœºæ™¯ä¸­å—åˆ°äº†å¾ˆå¤§çš„å½±å“ã€‚
- en: but the distant transformer does moreã€‚And just for completeness if you also
    have performance of behavioral training and person behavioral cloã€‚which because
    they don't look at reward informationã€‚except maybe person BC looks at only for
    preprocessing the data setã€‚these are agnostic to whether the environments as spa
    wordss or notã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿œç¨‹å˜æ¢å™¨åšå¾—æ›´å¤šã€‚ä¸ºäº†å®Œæ•´èµ·è§ï¼Œå¦‚æœä½ è¿˜çœ‹åˆ°äº†è¡Œä¸ºè®­ç»ƒå’Œä¸ªäººè¡Œä¸ºå…‹éš†çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒä»¬ä¸çœ‹å¥–åŠ±ä¿¡æ¯ï¼Œé™¤äº†ä¸ªäººBCå¯èƒ½åªæ˜¯åœ¨é¢„å¤„ç†æ•°æ®é›†æ—¶çœ‹ä¸€ä¸‹ã€‚è¿™äº›å¯¹ç¯å¢ƒæ˜¯å¦ç¨€ç–å¹¶ä¸æ•æ„Ÿã€‚
- en: Would you expect this to be different if you were doing online Rã€‚å—¯ã€‚What's the
    intuition for it being different April I would say noã€‚but maybe I'm missing out
    on a key piece of intuition behind that questionã€‚å—¯ã€‚I think that because you're
    training like offlineï¼Œ rightã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨åšåœ¨çº¿Rï¼Œä½ ä¼šæœŸæœ›è¿™æœ‰æ‰€ä¸åŒå—ï¼Ÿå—¯ã€‚ä¸ºä»€ä¹ˆä¼šæœ‰ä¸åŒçš„ç›´è§‰ï¼Ÿæˆ‘ä¼šè¯´æ²¡æœ‰ï¼Œä½†ä¹Ÿè®¸æˆ‘é”™è¿‡äº†è¿™ä¸ªé—®é¢˜èƒŒåå…³é”®çš„ç›´è§‰ã€‚å—¯ã€‚æˆ‘è®¤ä¸ºå› ä¸ºä½ æ˜¯åœ¨ç¦»çº¿è®­ç»ƒï¼Œå¯¹å§ã€‚
- en: like your're the next input will always be the correct action in that senseã€‚So
    you don't just like deviate and like go off the rails technically because you
    just don't knowã€‚So I could see like how online would have like a really hardã€‚Like
    cold start basically because it just doesn't know and it's just happeningping
    in the dark until it like maybe eventually hits the jackpotã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒä½ çš„ä¸‹ä¸€ä¸ªè¾“å…¥åœ¨æŸç§æ„ä¹‰ä¸Šæ€»æ˜¯æ­£ç¡®çš„è¡ŒåŠ¨ã€‚å› æ­¤ï¼Œä½ ä¸ä¼šåç¦»è½¨é“ï¼Œå› ä¸ºä½ åªæ˜¯ä¸çŸ¥é“ã€‚æ‰€ä»¥æˆ‘èƒ½ç†è§£åœ¨çº¿ç¯å¢ƒä¼šéå¸¸å›°éš¾ã€‚åŸºæœ¬ä¸Šå› ä¸ºå®ƒä»€ä¹ˆéƒ½ä¸çŸ¥é“ï¼Œéƒ½æ˜¯åœ¨é»‘æš—ä¸­æ‘¸ç´¢ï¼Œç›´åˆ°å¯èƒ½æœ€ç»ˆè·å¾—æˆåŠŸã€‚
- en: Rightï¼Œ right I think I agree that so that's a good piece of intuition out there
    that yeahã€‚I think here because offline R is reallyã€‚Getting rid of the trial and
    error aspect of it and firstpa reward environment that would be harder soã€‚The
    drop in DT performance should be more prominent thereã€‚I'm not sure how it would
    compare with the drop performance for other algorithms but it does seem like an
    interestingã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼Œå¯¹ï¼Œæˆ‘è®¤ä¸ºæˆ‘åŒæ„ï¼Œè¿™ç¡®å®æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„ç›´è§‰ã€‚æˆ‘è®¤ä¸ºè¿™é‡Œå› ä¸ºç¦»çº¿RçœŸçš„åœ¨å»é™¤è¯•é”™çš„æ–¹é¢ï¼Œè€Œåœ¨ç¨€ç–å¥–åŠ±ç¯å¢ƒä¸­ä¼šæ›´å›°éš¾ã€‚å› æ­¤ï¼ŒDTæ€§èƒ½çš„ä¸‹é™åœ¨è¿™é‡Œåº”è¯¥æ›´åŠ æ˜æ˜¾ã€‚æˆ‘ä¸ç¡®å®šå®ƒä¸å…¶ä»–ç®—æ³•æ€§èƒ½ä¸‹é™çš„æ¯”è¾ƒï¼Œä½†è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç‚¹ã€‚
- en: Set up to test DTNã€‚Well and I did to maybe i'm wrong hereã€‚but my understanding
    with the decision transformer as well is this is this critical piece that in the
    training you use the rewards to go right so is it is it not the sense that essentially
    likeã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: è®¾ç½®æµ‹è¯•DTNã€‚é‚£ä¹ˆæˆ‘ä¹Ÿè®¸é”™äº†ï¼Œä½†æˆ‘å¯¹å†³ç­–å˜æ¢å™¨çš„ç†è§£æ˜¯ï¼Œåœ¨è®­ç»ƒä¸­ä½ ä½¿ç”¨å¥–åŠ±è¿›è¡Œæ­£ç¡®çš„å†³ç­–ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸æ˜¯æ„å‘³ç€æœ¬è´¨ä¸Šã€‚
- en: ã¯ã€‚For each trajectory from the initial state based on the training regimeã€‚The
    model has access to whether or not the final result was a success or failureï¼Œ
    rightï¼Ÿ
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä»åˆå§‹çŠ¶æ€å‡ºå‘ï¼Œæ ¹æ®è®­ç»ƒæ–¹æ¡ˆï¼Œæ¨¡å‹å¯ä»¥çŸ¥é“æœ€ç»ˆç»“æœæ˜¯æˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œå¯¹å§ï¼Ÿ
- en: But that's sort of that's sort of the unique aspect of the training regime for
    decision transformers whereas in CQLã€‚My understanding is that it's based on sort
    of a per transitionã€‚Training regime and so each transition is decoupled somewhat
    to what what the final reward wasã€‚is that correctï¼ŸYesï¼Œ although like one difficulty
    which at a first glance you kind of imagined the discipline transform we're having
    is thatã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å°±æ˜¯å†³ç­–å˜æ¢å™¨è®­ç»ƒæ–¹æ¡ˆçš„ç‹¬ç‰¹ä¹‹å¤„ï¼Œè€ŒCQLåˆ™æœ‰æ‰€ä¸åŒã€‚æˆ‘çš„ç†è§£æ˜¯ï¼Œå®ƒåŸºäºæ¯ä¸ªè½¬æ¢çš„è®­ç»ƒæ–¹æ¡ˆï¼Œå› æ­¤æ¯ä¸ªè½¬æ¢åœ¨æŸç§ç¨‹åº¦ä¸Šä¸æœ€ç»ˆå¥–åŠ±è§£è€¦ã€‚è¿™æ˜¯æ­£ç¡®çš„å—ï¼Ÿæ˜¯çš„ï¼Œå°½ç®¡ä¹ä¸€çœ‹ï¼Œçºªå¾‹å˜æ¢å™¨æ‰€é¢ä¸´çš„ä¸€ä¸ªå›°éš¾æ˜¯ã€‚
- en: That initial token will not change throughout the trajectoryã€‚Yeah because it's
    this past reward scenarioï¼Œ so except the very last token where it will drop down
    to zero all of a sudden the token remains the same throughoutã€‚But maybe that that
    but I think you're right that maybe just even at this startã€‚Peding it in a manner
    which looks at the future rewards that you need to get to is perhaps one part
    of the reason why the drop in performance isã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åˆçš„æ ‡è®°åœ¨æ•´ä¸ªè½¨è¿¹ä¸­ä¸ä¼šæ”¹å˜ã€‚æ˜¯çš„ï¼Œå› ä¸ºè¿™æ˜¯è¿‡å»å¥–åŠ±çš„åœºæ™¯ï¼Œæ‰€ä»¥é™¤äº†æœ€åä¸€ä¸ªæ ‡è®°ä¼šçªç„¶é™åˆ°é›¶ä¹‹å¤–ï¼Œæ ‡è®°åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­ä¿æŒä¸å˜ã€‚ä½†ä¹Ÿè®¸æˆ‘è®¤ä¸ºä½ æ˜¯å¯¹çš„ï¼Œä¹Ÿè®¸åœ¨è¿™ä¸ªå¼€å§‹çš„æ—¶å€™å°±èƒ½çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚ä»¥ä¸€ç§å…³æ³¨æœªæ¥å¥–åŠ±çš„æ–¹å¼è¿›è¡Œè®­ç»ƒï¼Œæˆ–è®¸æ˜¯æ€§èƒ½ä¸‹é™çš„åŸå› ä¹‹ä¸€ã€‚
- en: Not noticeableã€‚Yeahï¼Œ I meanï¼Œ I guess one sort of abl experiment here would be
    if you change the training regime so that only the last trajectory had the awardã€‚but
    I'm trying to think about whether or not that would just be compensated for by
    sort of the attention mechanism anywayã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¯å¯Ÿè§‰ã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³è¿™é‡Œæœ‰ä¸€ä¸ªå®éªŒï¼Œå¦‚æœä½ æ”¹å˜è®­ç»ƒæ–¹æ¡ˆï¼Œä½¿å¾—åªæœ‰æœ€åä¸€ä¸ªè½¨è¿¹æœ‰å¥–åŠ±ï¼Œä½†æˆ‘åœ¨è€ƒè™‘è¿™æ˜¯å¦ä¼šè¢«æ³¨æ„æœºåˆ¶å¼¥è¡¥ã€‚
- en: And vice versa rightï¼Œ if you embedded that reward information into the CQL training
    procedure as wellã€‚I'd be curious to see what would happen thereã€‚Yeahï¼Œ a good experimentsã€‚Okayï¼Œ
    so related to thisã€‚theres another environment we tested I gave you a brief preview
    of the results in one of the earlier slidesã€‚so this is called the keycudo environmentã€‚ğŸ˜Šï¼ŒAnd
    it has three phasesã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: åä¹‹ä¹Ÿæ˜¯å¯¹çš„ï¼Œå¦‚æœä½ å°†å¥–åŠ±ä¿¡æ¯åµŒå…¥åˆ°CQLè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘å¾ˆæƒ³çŸ¥é“ä¼šå‘ç”Ÿä»€ä¹ˆã€‚æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„å®éªŒã€‚å¥½å§ï¼Œå…³äºè¿™ä¸ªï¼Œè¿˜æœ‰å¦ä¸€ä¸ªæˆ‘ä»¬æµ‹è¯•è¿‡çš„ç¯å¢ƒï¼Œæˆ‘åœ¨æ—©äº›å¹»ç¯ç‰‡ä¸­ç»™ä½ ç®€è¦é¢„è§ˆäº†ç»“æœã€‚è¿™ä¸ªç¯å¢ƒå«åškeycudoç¯å¢ƒã€‚ğŸ˜Šï¼Œå®ƒæœ‰ä¸‰ä¸ªé˜¶æ®µã€‚
- en: so in the first phase you the agent is placed in a room with the keyã€‚A good
    agent will pick up the key and then they do it replaced in an empty roomã€‚And in
    phase threeã€‚it will be placed in a room at the door where it will actually use
    the key that it collected in phase one if it did to open the doorã€‚ğŸ˜Šï¼ŒSo essentially
    the agent is going to receive a binding reward corresponding to whether it reached
    and opened the door in phase threeã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä»£ç†è¢«æ”¾ç½®åœ¨ä¸€ä¸ªæœ‰é’¥åŒ™çš„æˆ¿é—´é‡Œã€‚ä¸€ä¸ªå¥½çš„ä»£ç†ä¼šæ¡èµ·é’¥åŒ™ï¼Œç„¶åæŠŠå®ƒæ”¾åœ¨ä¸€ä¸ªç©ºæˆ¿é—´é‡Œã€‚åœ¨ç¬¬ä¸‰é˜¶æ®µï¼Œå®ƒä¼šè¢«æ”¾åœ¨ä¸€ä¸ªæˆ¿é—´çš„é—¨å£ï¼Œå®é™…ä½¿ç”¨åœ¨ç¬¬ä¸€é˜¶æ®µæ”¶é›†åˆ°çš„é’¥åŒ™æ¥æ‰“å¼€é—¨ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æœ¬è´¨ä¸Šï¼Œä»£ç†å°†è·å¾—ä¸€ä¸ªç»‘å®šå¥–åŠ±ï¼Œå–å†³äºå®ƒåœ¨ç¬¬ä¸‰é˜¶æ®µæ˜¯å¦åˆ°è¾¾å¹¶æ‰“å¼€äº†é—¨ã€‚
- en: Conditioned on the fact that itã€‚Did pick up the key in phase oneã€‚So there is
    this national notion of that you want to assign credit to something that happened
    to an event that happened really in the pastã€‚so it's very challengingã€‚And sensible
    scenario if you wanted to estimate models for how well they are at long term credit
    assignmentã€‚And here we find that so we tested it for different amounts of project
    treesã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åŸºäºå®ƒåœ¨ç¬¬ä¸€é˜¶æ®µæ˜¯å¦æ¡èµ·é’¥åŒ™çš„æ¡ä»¶ã€‚å› æ­¤ï¼Œä½ æƒ³è¦å°†åŠŸåŠ³åˆ†é…ç»™è¿‡å»å‘ç”Ÿçš„äº‹ä»¶ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹æŠ½è±¡çš„æ¦‚å¿µï¼Œæ‰€ä»¥è¿™éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚å¦‚æœä½ æƒ³ä¼°ç®—æ¨¡å‹åœ¨é•¿æœŸåŠŸåŠ³åˆ†é…ä¸Šçš„è¡¨ç°ï¼Œè¿™åœ¨ç°å®åœºæ™¯ä¸­æ˜¯éå¸¸åˆç†çš„ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œå‘ç°ï¼Œæˆ‘ä»¬æµ‹è¯•äº†ä¸åŒæ•°é‡çš„é¡¹ç›®æ ‘ã€‚
- en: so here the number of project trees basically see how often would you actually
    see this kindã€‚Of behavior and the listen transformer is and person behavioral
    to both of these actually baselineã€‚Do do much better than other models which struggle
    at thisã€‚doesã€‚å“¦ã€‚There's a related experiment there which is also of interestã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œé¡¹ç›®æ ‘çš„æ•°é‡åŸºæœ¬ä¸Šæ˜¯çœ‹ä½ å®é™…ä¸Šçœ‹åˆ°è¿™ç§è¡Œä¸ºçš„é¢‘ç‡ï¼Œè€Œå¬åŠ›å˜æ¢å™¨åœ¨è¿™ä¸¤ç§åŸºçº¿è¡Œä¸ºä¸Šè¡¨ç°å¾—æ›´å¥½ï¼Œå®é™…ä¸Šæ¯”å…¶ä»–æ¨¡å‹è¦å¥½å¾—å¤šï¼Œå…¶ä»–æ¨¡å‹åœ¨è¿™æ–¹é¢å¾ˆæŒ£æ‰ã€‚å“¦ï¼Œè¿˜æœ‰ä¸€ä¸ªç›¸å…³çš„å®éªŒä¹Ÿå¾ˆæœ‰è¶£ã€‚
- en: so generally a lot of oral algorithms have this notion of an actor and a criticã€‚actor
    is basically someone that takes actions condition on the states to think of a
    policyã€‚a critic is basically evaluating how good these actions are in terms of
    achieving a long termã€‚in terms of the cummulative sum of rewards in the long termã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸ï¼Œå¾ˆå¤šå£å¤´ç®—æ³•éƒ½æœ‰æ¼”å‘˜å’Œè¯„è®ºè€…çš„æ¦‚å¿µã€‚æ¼”å‘˜åŸºæœ¬ä¸Šæ˜¯æ ¹æ®çŠ¶æ€é‡‡å–è¡ŒåŠ¨çš„äººï¼Œæƒ³è±¡æˆç­–ç•¥ã€‚è¯„è®ºè€…åˆ™æ˜¯åœ¨å®ç°é•¿æœŸç›®æ ‡æ–¹é¢è¯„ä¼°è¿™äº›è¡ŒåŠ¨çš„å¥½åï¼Œå³é•¿æœŸå¥–åŠ±çš„ç´¯ç§¯å’Œã€‚
- en: This is a good environment because we can see how how well the distant transformer
    would do if it was trained as a critic so here what we did isã€‚Instead of having
    the actions as the output targetï¼Œ what if you substituted that with the rewardsï¼Ÿ
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè‰¯å¥½çš„ç¯å¢ƒï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿œç¨‹å˜å‹å™¨ä½œä¸ºè¯„è®ºè€…è®­ç»ƒæ—¶çš„è¡¨ç°ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œåšçš„å°±æ˜¯ï¼Œä¸æ˜¯å°†åŠ¨ä½œä½œä¸ºè¾“å‡ºç›®æ ‡ï¼Œè€Œæ˜¯å°†å…¶æ›¿æ¢ä¸ºå¥–åŠ±ã€‚
- en: So that's very much possibleï¼Œ we can again use the same causal transformer machinery
    to only look at transitions in the previous time step and try to play the rewardã€‚And
    here we see this interesting pattern where in the three phases that we had in
    that key to door environmentã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¾ˆæœ‰å¯èƒ½ï¼Œæˆ‘ä»¬å¯ä»¥å†æ¬¡ä½¿ç”¨ç›¸åŒçš„å› æœå˜å‹å™¨æœºåˆ¶ï¼Œä»…å…³æ³¨å‰ä¸€ä¸ªæ—¶é—´æ­¥éª¤çš„è½¬å˜ï¼Œå¹¶å°è¯•ç©å¼„å¥–åŠ±ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬çœ‹åˆ°åœ¨é’¥åŒ™åˆ°é—¨çš„ç¯å¢ƒä¸­å‡ºç°æœ‰è¶£çš„æ¨¡å¼ã€‚
- en: We do see the reward probability changing very much in how we expect so basically
    the three scenarios so the agentã€‚the first scenario let's look at blueã€‚In which
    the agent does not pick up the key in pays fundã€‚So the reward probabilityï¼Œ they
    all start around the sameã€‚but as it becomes apparent that the agent is not going
    to pick up the keyã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¡®å®çœ‹åˆ°å¥–åŠ±æ¦‚ç‡åœ¨é¢„æœŸä¸­å˜åŒ–å¾ˆå¤§ï¼Œå› æ­¤åŸºæœ¬ä¸Šæœ‰ä¸‰ç§åœºæ™¯ã€‚é¦–å…ˆï¼Œçœ‹çœ‹è“è‰²åœºæ™¯ï¼Œå…¶ä¸­ä»£ç†æ²¡æœ‰æ¡èµ·é’¥åŒ™å¹¶ä¸”ä»˜å‡ºäº†ä»£ä»·ã€‚å› æ­¤ï¼Œå¥–åŠ±æ¦‚ç‡éƒ½å¤§è‡´ç›¸åŒï¼Œä½†éšç€ä»£ç†ä¸å»æ¡é’¥åŒ™å˜å¾—æ˜æ˜¾ã€‚
- en: the reward starts going downã€‚And then it stays very much close to zero throughout
    the episode because there is no way you will have the key to open the door in
    the future phasesã€‚If you pick up the keyã€‚There are two possibilitiesã€‚å“¦ã€‚Which remain
    which are essentially the same in phase two where you are in an empty roomã€‚Which
    is just a distractor to make the episode really longã€‚But at the very endã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¥–åŠ±å¼€å§‹ä¸‹é™ã€‚ç„¶ååœ¨æ•´ä¸ªå‰§é›†ä¸­ä¿æŒæ¥è¿‘é›¶ï¼Œå› ä¸ºæœªæ¥é˜¶æ®µæ‚¨æ— æ³•æ‹¥æœ‰é’¥åŒ™æ¥æ‰“å¼€é—¨ã€‚å¦‚æœæ‚¨æ¡èµ·é’¥åŒ™ï¼Œæœ‰ä¸¤ç§å¯èƒ½æ€§ã€‚å“¦ã€‚åœ¨ç¬¬äºŒé˜¶æ®µï¼Œæ‚¨åœ¨ä¸€ä¸ªç©ºæˆ¿é—´ä¸­ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå¹²æ‰°å› ç´ ä½¿å‰§é›†å˜å¾—å¾ˆé•¿ã€‚ä½†åœ¨æœ€åã€‚
- en: the two possibilities are one that you take the key and you actually reach the
    doorã€‚which is the one we see in orange and brown here where you see that the reward
    probability goes upã€‚ğŸ˜Šï¼ŒAnd is is this other possibility that you actually pick
    up the key but do not reach the door in which caseã€‚againï¼Œ it starts seeing that
    the reward probability that's predicted starts going downï¼Ÿ
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ç§å¯èƒ½æ€§æ˜¯ï¼Œä¸€ç§æ˜¯æ‚¨æ‹¿åˆ°äº†é’¥åŒ™å¹¶ä¸”å®é™…ä¸Šåˆ°è¾¾äº†é—¨ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„æ©™è‰²å’Œæ£•è‰²éƒ¨åˆ†ï¼Œæ‚¨ä¼šçœ‹åˆ°å¥–åŠ±æ¦‚ç‡ä¸Šå‡ã€‚ğŸ˜Š å¦ä¸€ç§å¯èƒ½æ€§æ˜¯æ‚¨æ‹¿åˆ°äº†é’¥åŒ™ï¼Œä½†æ²¡æœ‰åˆ°è¾¾é—¨ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„æµ‹çš„å¥–åŠ±æ¦‚ç‡å¼€å§‹ä¸‹é™å—ï¼Ÿ
- en: So the takeaway from this experiment is that mission transformers are not just
    great actorsã€‚which is what we've been seeing so far in the results from the optimized
    policyã€‚but they are also very impressive critics in doing this long-tered and
    assignment where the reward is also very sparseã€‚So just to be correctï¼Œ are you
    predicting the rewards to go at each time stuff or is it is this like the the
    like the reward at each time stuffï¼Ÿ
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ¬¡å®éªŒçš„ä¸»è¦ç»“è®ºæ˜¯ï¼Œä»»åŠ¡å˜å‹å™¨ä¸ä»…ä»…æ˜¯ä¼˜ç§€çš„æ¼”å‘˜ï¼Œè¿™åœ¨æˆ‘ä»¬ä»ä¼˜åŒ–ç­–ç•¥ä¸­çœ‹åˆ°çš„ç»“æœä¸­å·²ç»å¾ˆæ˜æ˜¾ã€‚å®ƒä»¬åœ¨æ‰§è¡Œè¿™é¡¹é•¿æœŸä»»åŠ¡å’Œç¨€ç–å¥–åŠ±çš„ä»»åŠ¡æ—¶ï¼Œä¹Ÿå±•ç°å‡ºéå¸¸å‡ºè‰²çš„æ‰¹è¯„èƒ½åŠ›ã€‚é‚£ä¹ˆï¼Œä¸ºäº†ç¡®ä¿ï¼Œæ‚¨æ˜¯é¢„æµ‹æ¯ä¸ªæ—¶é—´ç‚¹çš„å¥–åŠ±å—ï¼Ÿè¿˜æ˜¯è¿™åƒæ˜¯æ¯ä¸ªæ—¶é—´ç‚¹çš„å¥–åŠ±ï¼Ÿ
- en: But you predictingã€‚So this was the rewards to go and I can also check my impression
    was in this particular the experiment it didn't really make a differenceã€‚whether
    we were predicting rewards to go or the actual rewardsã€‚But I think Lu returns
    to go further this oneã€‚Also most curious so how do you get the probability distribution
    on the rewardsã€‚is it just like you just evaluate a lot of different episodes and
    just part the rewards or are you explicitly predicting some sort of distributionï¼Ÿ
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æ‚¨åœ¨é¢„æµ‹ã€‚å› æ­¤ï¼Œè¿™é‡Œçš„å¥–åŠ±æ˜¯èµ°å‘ï¼Œè€Œæˆ‘ä¹Ÿå¯ä»¥æ£€æŸ¥æˆ‘çš„å°è±¡ï¼Œåœ¨è¿™ä¸ªç‰¹å®šçš„å®éªŒä¸­ï¼Œé¢„æµ‹èµ°å‘å¥–åŠ±æˆ–å®é™…å¥–åŠ±å¹¶æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚ä½†æˆ‘è®¤ä¸ºLuå¯¹æ­¤æ›´æ„Ÿå…´è¶£ã€‚é‚£ä¹ˆï¼Œæ‚¨æ˜¯å¦‚ä½•è·å¾—å¥–åŠ±çš„æ¦‚ç‡åˆ†å¸ƒçš„ï¼Ÿæ˜¯é€šè¿‡è¯„ä¼°å¾ˆå¤šä¸åŒçš„å‰§é›†å¹¶åªè®¡ç®—å¥–åŠ±ï¼Œè¿˜æ˜¯æ‚¨æ˜ç¡®é¢„æµ‹æŸç§åˆ†å¸ƒï¼Ÿ
- en: Ohï¼Œ so this is a binary reward so you can have a probabilistic outcomeã€‚å“ªçš„ã€‚have
    a questionã€‚yeahã€‚So generallyï¼Œ we will call like someï¼Œ something predicts the state
    value or state actual value as criticã€‚But in this caseï¼Œ youï¼Œ you ask decision
    transformer to only predict the rewardã€‚So why should you still call it a criticã€‚So
    I think the analogy here gets a bit clear with returns to goã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªäºŒå…ƒå¥–åŠ±ï¼Œè¿™æ ·ä½ å°±å¯ä»¥æœ‰ä¸€ä¸ªæ¦‚ç‡ç»“æœã€‚å“ªçš„ã€‚æˆ‘æœ‰ä¸ªé—®é¢˜ã€‚æ˜¯çš„ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šç§°æŸäº›ä¸œè¥¿é¢„æµ‹çŠ¶æ€å€¼æˆ–çŠ¶æ€å®é™…å€¼ä¸ºè¯„è®ºå‘˜ã€‚ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ è¦æ±‚å†³ç­–è½¬æ¢å™¨ä»…é¢„æµ‹å¥–åŠ±ã€‚é‚£ä¹ˆä½ ä¸ºä»€ä¹ˆè¿˜è¦ç§°å…¶ä¸ºè¯„è®ºå‘˜å‘¢ï¼Ÿæˆ‘è§‰å¾—è¿™é‡Œçš„ç±»æ¯”é€šè¿‡â€œæœªæ¥æ”¶ç›Šâ€å˜å¾—æ›´æ¸…æ™°ã€‚
- en: like if you think what returns to goï¼Œ it's really capturing that essence that
    you want to see the future rewards so I mean it's just going to predict the return
    to go instead of single step rewardã€‚rightï¼ŸYeahã‚„ï¼Œ yeahã€‚Okayï¼Œ soï¼Œ so if you're gonna
    predict the returns to go is kind of counterintuitive to me because in phase  oneã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒå¦‚æœä½ æƒ³æƒ³â€œæœªæ¥æ”¶ç›Šâ€ï¼Œå®ƒç¡®å®æŠ“ä½äº†ä½ æƒ³çœ‹åˆ°æœªæ¥å¥–åŠ±çš„æœ¬è´¨ï¼Œæ‰€ä»¥æˆ‘çš„æ„æ€æ˜¯å®ƒåªä¼šé¢„æµ‹æœªæ¥æ”¶ç›Šï¼Œè€Œä¸æ˜¯å•æ­¥å¥–åŠ±ã€‚å¯¹å§ï¼Ÿæ˜¯çš„ï¼Œå¥½çš„ï¼Œæ‰€ä»¥å¦‚æœä½ è¦é¢„æµ‹æœªæ¥æ”¶ç›Šï¼Œå¯¹æˆ‘æ¥è¯´æœ‰ç‚¹åç›´è§‰ï¼Œå› ä¸ºåœ¨ç¬¬ä¸€é˜¶æ®µã€‚
- en: when the agent is still in the key roomï¼Œ I think you should have a like high
    returns to go if it pick up the Kã€‚But in in the plot youï¼Œ you knowï¼Œ in the key
    roomï¼Œ the agents pick up K and the agent thatã€‚Didn't pick tea has the same kind
    level of returns to goã€‚ So that's quite countertuitive to meã€‚I think this is reflecting
    on a good propertyï¼Œ which is that your distributionã€‚Like if you interpretã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä»£ç†ä»åœ¨é’¥åŒ™æˆ¿é—´æ—¶ï¼Œæˆ‘è®¤ä¸ºå¦‚æœæ¡åˆ°Kï¼Œå®ƒåº”è¯¥æœ‰å¾ˆé«˜çš„æœªæ¥æ”¶ç›Šã€‚ä½†åœ¨å›¾ä¸­ï¼Œä½ çŸ¥é“ï¼Œåœ¨é’¥åŒ™æˆ¿é—´ï¼Œä»£ç†æ¡åˆ°Kï¼Œè€Œæ²¡æœ‰æ¡åˆ°Tçš„ä»£ç†å´æœ‰ç›¸åŒæ°´å¹³çš„æœªæ¥æ”¶ç›Šã€‚æ‰€ä»¥è¿™å¯¹æˆ‘æ¥è¯´ç›¸å½“åç›´è§‰ã€‚æˆ‘è®¤ä¸ºè¿™åæ˜ äº†ä¸€ä¸ªè‰¯å¥½çš„ç‰¹æ€§ï¼Œå°±æ˜¯ä½ çš„åˆ†å¸ƒã€‚å°±åƒå¦‚æœä½ è§£é‡Šã€‚
- en: it turns to going the right wayã€‚In phase oneï¼Œ you don't know which of these
    three outcomes are really possible and phase one also I'm talking about the very
    big meaning basicallyã€‚slowly you will learn about itï¼Œ but essentially in phase
    oneã€‚if you see the returns to go as one or zero for all three possibilities are
    equally likelyã€‚And all three possibilitiesï¼Œ so if we try to evaluate the predictedã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæœç€æ­£ç¡®çš„æ–¹å‘å‰è¿›ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œä½ ä¸çŸ¥é“è¿™ä¸‰ç§ç»“æœä¸­å“ªäº›æ˜¯å®é™…å¯èƒ½çš„ï¼Œè€Œç¬¬ä¸€é˜¶æ®µæˆ‘ä¹Ÿåœ¨è°ˆè®ºéå¸¸å¤§çš„æ„ä¹‰ã€‚ä½ ä¼šæ…¢æ…¢äº†è§£å®ƒï¼Œä½†åŸºæœ¬ä¸Šåœ¨ç¬¬ä¸€é˜¶æ®µï¼Œå¦‚æœä½ çœ‹åˆ°æ‰€æœ‰ä¸‰ç§å¯èƒ½çš„æœªæ¥æ”¶ç›Šéƒ½æ˜¯ä¸€æˆ–é›¶ï¼Œé‚£æ˜¯åŒæ ·å¯èƒ½çš„ã€‚è€Œæ‰€æœ‰ä¸‰ç§å¯èƒ½æ€§ï¼Œå¦‚æœæˆ‘ä»¬å°è¯•è¯„ä¼°é¢„æµ‹çš„ã€‚
- en: Reward for these possibilities it shouldn't be the sameã€‚Because we really haven't
    done how we don't know what's going to happen in phase three sorry it's my mistake
    because previous that salter green line is the agent which doesn't pick up theã€‚but
    it turns out a blue line is agent which doesn't pick upã€‚Okayï¼Œ so yeahï¼Œ it's my
    best take okayã€‚It makes sense to meã€‚ Thank youã€‚Alsomosts not like fully clear
    from the paperã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å¯èƒ½æ€§çš„å¥–åŠ±ä¸åº”è¯¥æ˜¯ç›¸åŒçš„ã€‚å› ä¸ºæˆ‘ä»¬çœŸçš„ä¸çŸ¥é“åœ¨ç¬¬ä¸‰é˜¶æ®µä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå¯¹ä¸èµ·ï¼Œè¿™æ˜¯æˆ‘çš„é”™è¯¯ï¼Œå› ä¸ºä¹‹å‰é‚£ä¸ªç›æ°´ç»¿çº¿æ˜¯ä»£ç†ï¼Œæ²¡é€‰åˆ°ã€‚ä½†ç»“æœæ˜¯è“çº¿æ˜¯ä»£ç†ï¼Œæ²¡é€‰åˆ°ã€‚å¥½çš„ï¼Œæ‰€ä»¥æ˜¯çš„ï¼Œè¿™æ˜¯æˆ‘æœ€å¥½çš„ç†è§£ï¼Œå¥½çš„ã€‚è¿™å¯¹æˆ‘æœ‰æ„ä¹‰ã€‚è°¢è°¢ã€‚è®ºæ–‡ä¸­ä¼¼ä¹å¹¶æ²¡æœ‰å®Œå…¨æ¸…æ™°ã€‚
- en: but did you do experiments where like predicting both the actions and both the
    rewards to go and like does it likeã€‚ğŸ˜Šï¼ŒK candidate liberal performance if we didn't
    work together so actually we did some preliminary experiments on that and it didn't
    help us much however I do want to again put in a plug for a paper that came concurruently
    tragically transform summer which tried to predict states actions and rewards
    actually all three of them they were in a modelal based set up where it made sense
    also toã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½ åšè¿‡å®éªŒï¼Œæ¯”å¦‚é¢„æµ‹åŠ¨ä½œå’Œæœªæ¥å¥–åŠ±ï¼Œç»“æœå¦‚ä½•ï¼ŸğŸ˜Šï¼ŒKå€™é€‰è€…è¡¨ç°æ˜¯å¦æœ‰æ‰€å¸®åŠ©ï¼Ÿå®é™…ä¸Šæˆ‘ä»¬åšäº†ä¸€äº›åˆæ­¥å®éªŒï¼Œä½†æ²¡å¤šå¤§å¸®åŠ©ã€‚ä¸è¿‡ï¼Œæˆ‘ç¡®å®æƒ³å†æ¬¡æåˆ°ä¸€ç¯‡åŒæ—¶å‘è¡¨çš„è®ºæ–‡ï¼Œâ€œæ‚²å‰§æ€§è½¬æ¢å¤å­£â€ï¼Œä»–ä»¬å°è¯•é¢„æµ‹çŠ¶æ€ã€åŠ¨ä½œå’Œå¥–åŠ±ï¼Œå®é™…ä¸Šè¿™ä¸‰è€…éƒ½æ˜¯åœ¨ä¸€ä¸ªåŸºäºæ¨¡å‹çš„è®¾ç½®ä¸­è¿›è¡Œçš„ï¼Œè¿™æ ·ä¹Ÿæœ‰æ„ä¹‰ã€‚
- en: Try to learn each of the components like the transition dynamicsï¼Œ the policyã€‚and
    maybe even the critic in their setup togetherã€‚We did not find any significant
    improvementsã€‚so in favor of simplicity and keeping it models freeï¼Œ we did not
    try to predict them togetherã€‚Go itã€‚Okayï¼Œ so the summaryã€‚We show dis transformersï¼Œ
    which isã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: å°è¯•å­¦ä¹ æ¯ä¸ªç»„ä»¶ï¼Œæ¯”å¦‚è½¬ç§»åŠ¨æ€ã€ç­–ç•¥ï¼Œç”šè‡³å¯èƒ½åœ¨å®ƒä»¬ä¸€èµ·çš„è®¾ç½®ä¸­å­¦ä¹ è¯„è®ºå‘˜ã€‚æˆ‘ä»¬æ²¡æœ‰å‘ç°ä»»ä½•æ˜¾è‘—çš„æ”¹è¿›ã€‚å› æ­¤ï¼Œä¸ºäº†ç®€å•èµ·è§å¹¶ä¿æŒæ¨¡å‹è‡ªç”±ï¼Œæˆ‘ä»¬æ²¡æœ‰å°è¯•å°†å®ƒä»¬ä¸€èµ·é¢„æµ‹ã€‚æ˜ç™½äº†ã€‚å¥½çš„ï¼Œæ€»ç»“ä¸€ä¸‹ã€‚æˆ‘ä»¬å±•ç¤ºäº†å†³ç­–è½¬æ¢å™¨ã€‚
- en: A first work in trying to approach R based on sequence modelingã€‚The main advantages
    of previous approaches is it's simple by designã€‚the hope is with further extensionsï¼Œ
    we will find it to scale much better than existing algorithmsã€‚It is stable to
    train because the loss functions we are using have been tested and are traded
    upon a lot byã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å°è¯•åŸºäºåºåˆ—å»ºæ¨¡æ¥æ¥è¿‘ R çš„ç¬¬ä¸€é¡¹å·¥ä½œã€‚ä¹‹å‰æ–¹æ³•çš„ä¸»è¦ä¼˜ç‚¹åœ¨äºå…¶è®¾è®¡ç®€å•ã€‚å¸Œæœ›é€šè¿‡è¿›ä¸€æ­¥æ‰©å±•ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒçš„æ‰©å±•èƒ½åŠ›è¿œè¿œä¼˜äºç°æœ‰ç®—æ³•ã€‚è®­ç»ƒæ˜¯ç¨³å®šçš„ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨çš„æŸå¤±å‡½æ•°ç»è¿‡äº†å¤§é‡æµ‹è¯•å’Œä¼˜åŒ–ã€‚
- en: Research and perceptionã€‚And in the futureï¼Œ we will also hope that because of
    these similarities with in the architecture and the training with how perception
    based tasks are conductedã€‚if it also be easy to integrate them within this loop
    so the states the actions or even the task of interestã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ç ”ç©¶å’Œæ„ŸçŸ¥ã€‚æœªæ¥ï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›ç”±äºæ¶æ„å’Œè®­ç»ƒä¹‹é—´çš„è¿™äº›ç›¸ä¼¼æ€§ï¼Œå¦‚ä½•è¿›è¡ŒåŸºäºæ„ŸçŸ¥çš„ä»»åŠ¡çš„å®æ–½ã€‚å¦‚æœå¯ä»¥è½»æ¾åœ°å°†å®ƒä»¬é›†æˆåˆ°è¿™ä¸ªå¾ªç¯ä¸­ï¼Œä»¥ä¾¿çŠ¶æ€ã€åŠ¨ä½œæˆ–ç”šè‡³æ„Ÿå…´è¶£çš„ä»»åŠ¡ã€‚
- en: they could be specified based onã€‚Perceptual based csã€‚or you could have a target
    task being specified by natural language instruction and because these models
    can very well play with these kinds of inputsã€‚the hope is that they would be easy
    to integrate within the decision making processã€‚And emlyã€‚we saw strong performance
    in the range of offline R settingsã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯ä»¥åŸºäºæ„ŸçŸ¥æ¥æŒ‡å®šã€‚æˆ–è€…ä½ å¯ä»¥é€šè¿‡è‡ªç„¶è¯­è¨€æŒ‡ä»¤æ¥æŒ‡å®šç›®æ ‡ä»»åŠ¡ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°å¤„ç†è¿™äº›ç±»å‹çš„è¾“å…¥ã€‚å¸Œæœ›å®ƒä»¬èƒ½å¤Ÿè½»æ¾é›†æˆåˆ°å†³ç­–è¿‡ç¨‹ä¸­ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬åœ¨ç¦»çº¿
    R è®¾ç½®ä¸­çœ‹åˆ°å¼ºåŠ²çš„è¡¨ç°ã€‚
- en: And especially with performance in scenarios which required us to do long term
    assignmentã€‚So there's a lot of future workï¼Œ this is definitely not the endã€‚this
    is a first work in rethinking how do we build our agents that can scale and generalizeã€‚å—¯ã€‚A
    few things that I picked outï¼Œ which I feel would be very exciting to extendã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å°¤å…¶æ˜¯åœ¨éœ€è¦æˆ‘ä»¬è¿›è¡Œé•¿æœŸä»»åŠ¡çš„åœºæ™¯ä¸­è¡¨ç°å‡ºè‰²ã€‚æ‰€ä»¥è¿˜æœ‰å¾ˆå¤šæœªæ¥çš„å·¥ä½œï¼Œè¿™ç»å¯¹ä¸æ˜¯ç»“æŸã€‚è¿™æ˜¯é‡æ–°æ€è€ƒå¦‚ä½•æ„å»ºèƒ½å¤Ÿæ‰©å±•å’Œæ¦‚æ‹¬çš„æ™ºèƒ½ä½“çš„ç¬¬ä¸€æ­¥ã€‚å—¯ã€‚æˆ‘æŒ‘é€‰å‡ºäº†ä¸€äº›æˆ‘è§‰å¾—éå¸¸ä»¤äººå…´å¥‹çš„æ‰©å±•ç‚¹ã€‚
- en: The first is multimodalityï¼Œ so really one of our big motivations with going
    after these kinds of models is that we can combine different kinds of inputs both
    online and offlineã€‚To really build decision making agents which work like humansã€‚we
    process so many inputs around us in different modalities and we act on them so
    we do take decisions and we want the same to happen in artificial agents and maybe
    decision transformers is one important step in that routeã€‚Multiitaask so I should
    or I described very limited form of multitasking hereã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯å¤šæ¨¡æ€æ€§ï¼Œæ‰€ä»¥è¿½æ±‚è¿™äº›æ¨¡å‹çš„ä¸€ä¸ªé‡è¦åŠ¨æœºæ˜¯æˆ‘ä»¬å¯ä»¥ç»“åˆä¸åŒç±»å‹çš„è¾“å…¥ï¼Œæ— è®ºæ˜¯åœ¨çº¿è¿˜æ˜¯ç¦»çº¿ã€‚çœŸæ­£æ„å»ºåƒäººç±»ä¸€æ ·çš„å†³ç­–æ™ºèƒ½ä½“ã€‚æˆ‘ä»¬å¤„ç†å‘¨å›´çš„å¤šç§è¾“å…¥ï¼Œå¹¶å¯¹æ­¤åšå‡ºååº”ï¼Œå› æ­¤æˆ‘ä»¬åšå‡ºå†³ç­–ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨äººå·¥æ™ºèƒ½ä½“ä¸­ä¹Ÿèƒ½å®ç°åŒæ ·çš„æ•ˆæœï¼Œä¹Ÿè®¸å†³ç­–å˜æ¢å™¨æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„é‡è¦ä¸€æ­¥ã€‚å¤šä»»åŠ¡ï¼Œå› æ­¤æˆ‘åœ¨è¿™é‡Œæè¿°çš„å¤šä»»åŠ¡å½¢å¼éå¸¸æœ‰é™ã€‚
- en: which was based on the desired returns to goã€‚But it could be more richer in
    terms ofã€‚Specifying a command to be a robot or a desired goal stateï¼Œ which could
    beï¼Œ for exampleï¼Œ even visualã€‚So trying to better explore the different multitask
    capabilities of this model would also be an interesting extensionã€‚Finallyï¼Œ multi
    agentã€‚As human beings we never act in isolationã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯åŸºäºæœŸæœ›çš„å›æŠ¥ã€‚ä½†åœ¨æŒ‡å®šå‘½ä»¤ç»™æœºå™¨äººæˆ–ç†æƒ³çš„ç›®æ ‡çŠ¶æ€æ–¹é¢ï¼Œå®ƒå¯ä»¥æ›´ä¸°å¯Œï¼Œä¾‹å¦‚ï¼Œç”šè‡³æ˜¯è§†è§‰ä¸Šçš„ã€‚å› æ­¤ï¼Œæ›´å¥½åœ°æ¢ç´¢è¿™ä¸ªæ¨¡å‹çš„ä¸åŒå¤šä»»åŠ¡èƒ½åŠ›ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ‰©å±•ã€‚æœ€åï¼Œå¤šæ™ºèƒ½ä½“ã€‚ä½œä¸ºäººç±»ï¼Œæˆ‘ä»¬ä»ä¸å­¤ç«‹åœ°è¡ŒåŠ¨ã€‚
- en: we are always acting within an environment that involves manyï¼Œ many more agentsã€‚things
    become partially observable in those scenariosã€‚which plays to the strengths of
    distant transformformers being non-marovviian by designã€‚so I think there is great
    possibilities of exploring even multi agentent scenarios where the fact the transformers
    can process very large sequences compared to existing algorithms could again help
    build better models of other agents in your environment and actã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»æ˜¯åœ¨ä¸€ä¸ªæ¶‰åŠè®¸å¤šå…¶ä»–æ™ºèƒ½ä½“çš„ç¯å¢ƒä¸­è¿›è¡Œæ“ä½œã€‚åœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œäº‹ç‰©å˜å¾—éƒ¨åˆ†å¯è§‚å¯Ÿï¼Œè¿™æ°å¥½å‘æŒ¥äº†å˜æ¢å™¨è®¾è®¡ä¸Šéé©¬å°”å¯å¤«æ€§è´¨çš„ä¼˜åŠ¿ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºæ¢ç´¢å¤šæ™ºèƒ½ä½“åœºæ™¯çš„å·¨å¤§å¯èƒ½æ€§ï¼Œå˜æ¢å™¨èƒ½å¤Ÿå¤„ç†æ¯”ç°æœ‰ç®—æ³•æ›´é•¿çš„åºåˆ—ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºæ„å»ºæ›´å¥½çš„ç¯å¢ƒä¸­å…¶ä»–æ™ºèƒ½ä½“çš„æ¨¡å‹å¹¶åšå‡ºååº”ã€‚
- en: So that yeahï¼Œ there's some just useful links in case you're interested the project
    websiteã€‚the paper and the code are all publicã€‚And I'm happy to take any more questionsã€‚UOkayï¼Œ
    so I saidã€‚thanks for the good talkã€‚really appreciate itã€‚Everyone had a good time
    hereã€‚è¯¶ã€‚So I think we are like near the class limit so usually I have like a round
    of record five questions for the speaker that just like the students usually knowã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œè¿™é‡Œæœ‰ä¸€äº›æœ‰ç”¨çš„é“¾æ¥ï¼Œä»¥é˜²ä½ å¯¹é¡¹ç›®ç½‘ç«™æ„Ÿå…´è¶£ï¼Œè®ºæ–‡å’Œä»£ç éƒ½æ˜¯å…¬å¼€çš„ã€‚æˆ‘å¾ˆä¹æ„å›ç­”æ›´å¤šé—®é¢˜ã€‚å¥½çš„ï¼Œæˆ‘è¯´è¿‡äº†ã€‚æ„Ÿè°¢ç²¾å½©çš„æ¼”è®²ï¼ŒçœŸçš„å¾ˆæ„Ÿæ¿€ã€‚å¤§å®¶åœ¨è¿™é‡Œéƒ½å¾ˆæ„‰å¿«ã€‚è¯¶ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬å¿«åˆ°è¯¾å ‚é™åˆ¶äº†ï¼Œæ‰€ä»¥é€šå¸¸æˆ‘ä¼šä¸ºå‘è¨€è€…å‡†å¤‡ä¸€è½®äº”ä¸ªé—®é¢˜ï¼Œè¿™äº›é—®é¢˜é€šå¸¸æ˜¯å­¦ç”Ÿä»¬çŸ¥é“çš„ã€‚
- en: but if someone is hurryï¼Œ you can you just like ask general questions folksã€‚ğŸ˜Šã€‚Before
    we stop the recordingï¼Œ so if anyone wants to like leave earlier at this timeã€‚just
    feel free to ask your questionsã€‚Otherwiseï¼Œ I would just like continue onã€‚So what
    do you think is like the future of like transformers in LL do you think they will
    take over like they've already taken over like language and visionã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœæœ‰äººå¾ˆç€æ€¥ï¼Œä½ å¯ä»¥é—®ä¸€äº›ä¸€èˆ¬æ€§çš„é—®é¢˜ï¼Œå¤§å®¶ã€‚ğŸ˜Šã€‚åœ¨æˆ‘ä»¬åœæ­¢å½•éŸ³ä¹‹å‰ï¼Œå¦‚æœæœ‰äººæƒ³æå‰ç¦»å¼€ï¼Œç°åœ¨è¯·éšæ„æé—®ã€‚å¦åˆ™ï¼Œæˆ‘å°†ç»§ç»­è¿›è¡Œã€‚é‚£ä¹ˆä½ è®¤ä¸ºå˜å‹å™¨åœ¨å¼ºåŒ–å­¦ä¹ ä¸­çš„æœªæ¥æ˜¯ä»€ä¹ˆï¼Ÿä½ è®¤ä¸ºå®ƒä»¬ä¼šåƒå·²ç»åœ¨è¯­è¨€å’Œè§†è§‰é¢†åŸŸé‚£æ ·å æ®ä¸»å¯¼åœ°ä½å—ï¼Ÿ
- en: do you think for like model based and like model field learningã€‚you think like
    youll see a lot more like transformers pop up in LL literatureã€‚Yesã€‚I think we'll
    see a flurry of workï¼Œ if not alreadyã€‚we have there's so many works using transformers
    at this year's eyeC conferenceã€‚Having said thatã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è®¤ä¸ºåœ¨åŸºäºæ¨¡å‹å’Œæ¨¡å‹é©±åŠ¨å­¦ä¹ ä¸­ï¼Œä½ ä¼šåœ¨å¼ºåŒ–å­¦ä¹ æ–‡çŒ®ä¸­çœ‹åˆ°æ›´å¤šçš„å˜å‹å™¨å‡ºç°å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬ä¼šçœ‹åˆ°å¤§é‡çš„ç ”ç©¶ï¼Œç”šè‡³å¯èƒ½å·²ç»å‡ºç°äº†ã€‚ä»Šå¹´çš„çœ¼åŠ¨å¤§ä¼šä¸Šæœ‰å¾ˆå¤šä½¿ç”¨å˜å‹å™¨çš„ç ”ç©¶ã€‚è¯è™½å¦‚æ­¤ã€‚
- en: I feel that an important piece of the puzzle that needs to be solved is explorationirationã€‚It's
    non trivialï¼Œ and it willã€‚my guess is that you will have to forego some of the
    advantages that I talked about for transformers in terms of loss functions to
    actually enable explorationirã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è§‰å¾—éœ€è¦è§£å†³çš„ä¸€ä¸ªé‡è¦ç¯èŠ‚æ˜¯æ¢ç´¢ã€‚è¿™ä¸ªé—®é¢˜å¹¶ä¸ç®€å•ï¼Œæˆ‘çŒœä½ å°†ä¸å¾—ä¸æ”¾å¼ƒä¸€äº›æˆ‘æåˆ°çš„å˜å‹å™¨åœ¨æŸå¤±å‡½æ•°æ–¹é¢çš„ä¼˜åŠ¿ï¼Œä»¥çœŸæ­£å®ç°æ¢ç´¢ã€‚
- en: So it remains to be seen whether those modified dose functionsã€‚For exploration
    actually hurt performance significantlyã€‚but as long as we cannot cross that bottleneckï¼Œ
    I think it isï¼Œ I'm notã€‚I do not want to commit that this is indeed the future
    of ourã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå°šä¸æ¸…æ¥šé‚£äº›ä¿®æ”¹è¿‡çš„æŸå¤±å‡½æ•°åœ¨æ¢ç´¢ä¸­æ˜¯å¦ä¼šæ˜¾è‘—å½±å“æ€§èƒ½ã€‚ä½†åªè¦æˆ‘ä»¬æ— æ³•çªç ´é‚£ä¸ªç“¶é¢ˆï¼Œæˆ‘è®¤ä¸ºï¼Œæˆ‘ä¸æƒ³æ‰¿è¯ºè¿™ç¡®å®æ˜¯æˆ‘ä»¬çš„æœªæ¥ã€‚
- en: Go it also your question Sure I'm not sure understood that pointã€‚so you're saying
    that in order to apply transformers in RL to do exploration there have to be particular
    loss functions and and they're tricky for some reasonã€‚Could you explain more like
    what are the modified loss functions and why do they seem trickyï¼Ÿ
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é—®é¢˜ï¼Œä½ çš„é—®é¢˜å½“ç„¶ã€‚æˆ‘ä¸å¤ªç¡®å®šæ˜¯å¦ç†è§£è¿™ä¸€ç‚¹ã€‚æ‰€ä»¥ä½ æ˜¯è¯´ï¼Œä¸ºäº†åœ¨å¼ºåŒ–å­¦ä¹ ä¸­åº”ç”¨å˜å‹å™¨è¿›è¡Œæ¢ç´¢ï¼Œå¿…é¡»æœ‰ç‰¹å®šçš„æŸå¤±å‡½æ•°ï¼Œè€Œå®ƒä»¬å‡ºäºæŸç§åŸå› å¾ˆæ£˜æ‰‹ã€‚ä½ èƒ½å¤šè§£é‡Šä¸€ä¸‹ä¿®æ”¹è¿‡çš„æŸå¤±å‡½æ•°æ˜¯ä»€ä¹ˆï¼Œä»¥åŠå®ƒä»¬ä¸ºä»€ä¹ˆçœ‹èµ·æ¥æ£˜æ‰‹å—ï¼Ÿ
- en: So a sense in explorationã€‚You have to do the opposite of exploitationï¼Œ which
    is not a with youã€‚And there is right now no nothing inbuilt in the transformer
    right nowã€‚which encourages that sort of random behaviorã€‚Where you seek outã€‚Unfaili
    unfamiliar parts of in state spaceã€‚That is something which is inbuil into traditional
    algorithms so usually has some sort of entropy bonus to encourage explorationir
    and those are the sort of modifications which one will also need to think about
    if one were to use this transforms for online RL So what happens if somebody I
    mean just naively suppose I have this exact same setup and the way that I sample
    the action is I sample epsilon greedily or I create a boltzman distribution and
    I sample from that I mean what happens it seemsã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨æ¢ç´¢ä¸­ï¼Œä½ å¿…é¡»åšä¸å¼€å‘ç›¸åçš„äº‹æƒ…ï¼Œè¿™ä¸ä½ æ— å…³ã€‚è€Œä¸”ç›®å‰åœ¨å˜å‹å™¨ä¸­æ²¡æœ‰ä»»ä½•å†…ç½®æœºåˆ¶æ¥é¼“åŠ±è¿™ç§éšæœºè¡Œä¸ºã€‚åœ¨çŠ¶æ€ç©ºé—´ä¸­å¯»æ±‚ä¸ç†Ÿæ‚‰çš„éƒ¨åˆ†ã€‚è¿™æ˜¯ä¼ ç»Ÿç®—æ³•ä¸­å†…ç½®çš„å†…å®¹ï¼Œé€šå¸¸ä¼šæœ‰æŸç§ç†µå¥–åŠ±æ¥é¼“åŠ±æ¢ç´¢ï¼Œå¦‚æœæƒ³å°†è¿™äº›å˜å‹å™¨ç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹ ï¼Œé‚£ä¹ˆè¿™äº›ä¿®æ”¹ä¹Ÿæ˜¯éœ€è¦è€ƒè™‘çš„ã€‚é‚£ä¹ˆï¼Œå¦‚æœæœ‰äººï¼Œæˆ‘æ˜¯è¯´ï¼Œç®€å•å‡è®¾æˆ‘æœ‰è¿™ä¸ªå®Œå…¨ç›¸åŒçš„è®¾ç½®ï¼Œè€Œæˆ‘é‡‡æ ·è¡ŒåŠ¨çš„æ–¹å¼æ˜¯ä»¥epsilonè´ªå©ªçš„æ–¹å¼é‡‡æ ·ï¼Œæˆ–è€…åˆ›å»ºä¸€ä¸ªBoltzmannåˆ†å¸ƒå¹¶ä»ä¸­é‡‡æ ·ï¼Œé‚£ä¹ˆä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿçœ‹èµ·æ¥ä¼¼ä¹ä¼šã€‚
- en: That's what OrL does so does what happensã€‚So ArL does a little bit more than
    thatã€‚it indeed does those kinds of things where it would change the distributionï¼Œ
    for exampleã€‚that your postal distribution sample from itã€‚But it's alsoï¼Œ there
    are theseã€‚as I said the devil lies in the detailï¼Œ it's also about how it controls
    that explorationir component with the exploitationã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ æ‰€åšçš„äº‹æƒ…ã€‚é‚£ä¹ˆä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè‡ªé€‚åº”å¼ºåŒ–å­¦ä¹ æ¯”è¿™å¤šåšäº†ä¸€ç‚¹ã€‚å®ƒç¡®å®åšäº†é‚£äº›äº‹æƒ…ï¼Œä¾‹å¦‚æ”¹å˜åˆ†å¸ƒï¼Œä½ çš„é‚®æ”¿åˆ†å¸ƒæ˜¯ä»ä¸­é‡‡æ ·çš„ã€‚ä½†åŒæ ·ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œé­”é¬¼è—åœ¨ç»†èŠ‚ä¸­ï¼Œè¿™ä¹Ÿä¸å¦‚ä½•æ§åˆ¶æ¢ç´¢ä¸åˆ©ç”¨çš„ç»„ä»¶æœ‰å…³ã€‚
- en: ğŸ˜Šï¼ŒAnd it remains to be seen whether that is compatible with distant transformersã€‚I
    don't want to jump the gunï¼Œ but I would say it it's I meanã€‚Preliminary evidence
    suggests that it's not directly transferable exact same setup to the online case
    that's what we have foundã€‚there has to be some adjustments to be made to make
    but we are still figuring it outã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿™æ˜¯å¦ä¸è¿œç¨‹å˜å‹å™¨å…¼å®¹è¿˜æœ‰å¾…è§‚å¯Ÿã€‚æˆ‘ä¸æƒ³è¿‡æ—©ä¸‹ç»“è®ºï¼Œä½†æˆ‘ä¼šè¯´ï¼Œåˆæ­¥è¯æ®è¡¨æ˜ï¼Œå®ƒå¹¶ä¸æ˜¯å®Œå…¨å¯ä»¥ç›´æ¥è½¬ç§»åˆ°åœ¨çº¿æ¡ˆä¾‹çš„ï¼Œè¿™æ˜¯æˆ‘ä»¬æ‰€å‘ç°çš„ã€‚å¿…é¡»è¿›è¡Œä¸€äº›è°ƒæ•´ä»¥é€‚åº”ï¼Œä½†æˆ‘ä»¬ä»åœ¨å¼„æ¸…æ¥šã€‚
- en: So the reason why I ask is as you said the devil's in the details and so someone
    naively like me might just come along and try doing what's an RLã€‚I want to hear
    more about thisï¼Œ so you're saying that what works in RL may not work for decision
    transformersã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é—®è¿™ä¸ªçš„åŸå› æ˜¯ï¼Œå¦‚ä½ æ‰€è¯´ï¼Œé­”é¬¼è—åœ¨ç»†èŠ‚ä¸­ï¼Œæ‰€ä»¥åƒæˆ‘è¿™æ ·å¤©çœŸçš„äººå¯èƒ½ä¼šç›´æ¥å°è¯•åšå¼ºåŒ–å­¦ä¹ ã€‚æˆ‘æƒ³å¬å¬æ›´å¤šå…³äºè¿™ä¸ªçš„å†…å®¹ï¼Œæ‰€ä»¥ä½ æ˜¯è¯´åœ¨å¼ºåŒ–å­¦ä¹ ä¸­æœ‰æ•ˆçš„æ–¹æ³•å¯èƒ½åœ¨å†³ç­–å˜å‹å™¨ä¸­å¹¶ä¸é€‚ç”¨ã€‚
- en: Can you tell us whyï¼Œ like what pathologies emergeï¼ŸWhat are those devilsï¼ŸHiding
    in the detailsã€‚also remind you like the sorry we are like also over time so definitely
    okay turn feel free to like follow up but like to me that's really exciting and
    I'm sure that it's trickyã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ èƒ½å‘Šè¯‰æˆ‘ä»¬ä¸ºä»€ä¹ˆå—ï¼Œåƒå“ªäº›ç—…ç†ä¼šå‡ºç°ï¼Ÿé‚£äº›â€œé­”é¬¼â€æ˜¯ä»€ä¹ˆï¼Ÿéšè—åœ¨ç»†èŠ‚ä¸­ã€‚åŒæ—¶æé†’ä½ ï¼Œæˆ‘ä»¬å·²ç»è¶…æ—¶äº†ï¼Œæ‰€ä»¥å½“ç„¶å¯ä»¥ï¼Œéšæ—¶è·Ÿè¿›ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œè¿™çœŸçš„å¾ˆä»¤äººå…´å¥‹ï¼Œæˆ‘ç›¸ä¿¡è¿™å¾ˆæ£˜æ‰‹ã€‚
- en: Yeah I will just ask like two more questions and like you finish sh those so
    one is like Eric did you think like something like this transformer is a way to
    solve a k center problem in oil instead of using some sort of like discon factororã€‚ğŸ˜Šï¼ŒThis
    isSo that can you repeat the question sorry Yesã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘å†é—®ä¸¤ä¸ªé—®é¢˜ï¼Œç„¶åå°±ç»“æŸäº†ã€‚ç¬¬ä¸€ä¸ªæ˜¯ï¼ŒåŸƒé‡Œå…‹ï¼Œä½ è®¤ä¸ºåƒè¿™ç§å˜å‹å™¨æ˜¯å¦å¯ä»¥ä½œä¸ºè§£å†³çŸ³æ²¹ä¸­çš„kä¸­å¿ƒé—®é¢˜çš„ä¸€ç§æ–¹æ³•ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æŸç§æŠ˜æ‰£å› å­ï¼ŸğŸ˜Šï¼Œè¿™èƒ½å¦è¯·ä½ é‡å¤ä¸€ä¸‹é—®é¢˜ï¼ŒæŠ±æ­‰ã€‚
- en: So I think usually in parallel we have to allow on some sort of disor to encode
    the rewards to go that but it in transformableer transformer is able to do this
    credit assignment without thatã€‚So do you think like something like this book is
    like the way you should do it like we shouldã€‚
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºé€šå¸¸æˆ‘ä»¬éœ€è¦åœ¨æŸç§å¤±è°ƒä¸­å…è®¸ç¼–ç å¥–åŠ±ï¼Œä½†å˜å‹å™¨èƒ½å¤Ÿåœ¨æ²¡æœ‰è¿™ä¸ªçš„æƒ…å†µä¸‹è¿›è¡Œä¿¡ç”¨åˆ†é…ã€‚æ‰€ä»¥ä½ è®¤ä¸ºåƒè¿™æ ·çš„ä¹¦æ˜¯ä½ åº”è¯¥è¿™æ ·åšçš„æ–¹å¼å—ï¼Ÿ
- en: Like instead of having some discountï¼Œ try to directly predict the rewardsã€‚So
    I would go on to say that I feel that discount factor is an important consideration
    in general and it's not incompatible with discipline transformersã€‚So basically
    what would change and I think the code actually gives that functionality whereã€‚The
    returns to Google would be computed as the discounted sum of rewardsã€‚
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œä¸æ˜¯ä½¿ç”¨æŸç§æŠ˜æ‰£ï¼Œè€Œæ˜¯ç›´æ¥é¢„æµ‹å¥–åŠ±ã€‚æ‰€ä»¥æˆ‘æƒ³è¯´ï¼ŒæŠ˜æ‰£å› å­åœ¨ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªé‡è¦çš„è€ƒè™‘å› ç´ ï¼Œå¹¶ä¸”å®ƒä¸çºªå¾‹å˜å‹å™¨å¹¶ä¸ç›¸äº’æ’æ–¥ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šä¼šæœ‰ä»€ä¹ˆå˜åŒ–ï¼Œæˆ‘è®¤ä¸ºä»£ç å®é™…ä¸Šæä¾›äº†è¿™ç§åŠŸèƒ½ï¼Œå…¶ä¸­è¿”å›å€¼å°†ä½œä¸ºæŠ˜æ‰£å¥–åŠ±çš„æ€»å’Œè¿›è¡Œè®¡ç®—ã€‚
- en: And so it is very much compatibleï¼Œ so there's scenarios where our context length
    is still is not enough to actually capture the long term behavior we really need
    for credit assignmentã€‚maybe traditional tricks that are used could be brought
    in back to solve those kinds of problemsã€‚
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯å…¼å®¹çš„ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¸Šä¸‹æ–‡é•¿åº¦ä»ç„¶ä¸è¶³ä»¥çœŸæ­£æ•æ‰åˆ°æˆ‘ä»¬è¿›è¡Œä¿¡ç”¨åˆ†é…æ‰€éœ€çš„é•¿æœŸè¡Œä¸ºã€‚ä¹Ÿè®¸å¯ä»¥å€Ÿç”¨ä¼ ç»ŸæŠ€å·§æ¥è§£å†³è¿™ç±»é—®é¢˜ã€‚
- en: Got itã€‚Yeahï¼Œ also thought like when I was reading the distance transform work
    that the interesting thing isã€‚Then you don't have a fixed gamma like a gamma is
    like usually a hypopenimeter likeã€‚You don't have a fixed gamma so do you think
    like can you also like learn this thing and could this also be likeã€‚can you have
    a different gamma for each gesture or somethingï¼Œ possiblyï¼Ÿ
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜ç™½äº†ã€‚æ˜¯çš„ï¼Œæˆ‘åœ¨é˜…è¯»è·ç¦»å˜æ¢å·¥ä½œæ—¶ä¹Ÿæƒ³åˆ°äº†æœ‰è¶£çš„äº‹æƒ…ã€‚ä½ æ²¡æœ‰å›ºå®šçš„ä¼½ç›ï¼Œä¼½ç›é€šå¸¸æ˜¯ä¸€ä¸ªè¶…å‚æ•°ã€‚ä½ æ²¡æœ‰å›ºå®šçš„ä¼½ç›ï¼Œæ‰€ä»¥ä½ è®¤ä¸ºå¯ä»¥å­¦ä¹ è¿™ä¸ªå—ï¼Ÿè¿™æ˜¯å¦å¯èƒ½ï¼Ÿä½ èƒ½å¦ä¸ºæ¯ä¸ªæ‰‹åŠ¿æœ‰ä¸åŒçš„ä¼½ç›ï¼Ÿ
- en: That would be interesting actuallyï¼Œ I had not thought of thatã€‚but maybe learning
    to predict the discount factor could be another extension of this workã€‚ğŸ˜Šï¼ŒAlsoã€‚do
    you think like this like a decision transformer work is like like is it compatible
    with Q learning so if you have something like EQL stuff like thatã€‚can you also
    implement those sort of loss functionsï¼ŸğŸ˜Šï¼ŒOn top of a decision transformerã€‚
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®é™…ä¸Šä¼šå¾ˆæœ‰è¶£ï¼Œæˆ‘ä¹‹å‰æ²¡æœ‰æƒ³åˆ°è¿‡ã€‚ä½†ä¹Ÿè®¸å­¦ä¹ é¢„æµ‹æŠ˜æ‰£å› å­å¯ä»¥æ˜¯è¿™é¡¹å·¥ä½œçš„å¦ä¸€ä¸ªæ‰©å±•ã€‚ğŸ˜Šï¼Œä½ è®¤ä¸ºè¿™ç§å†³ç­–å˜å‹å™¨çš„å·¥ä½œä¸ Q å­¦ä¹ å…¼å®¹å—ï¼Ÿå¦‚æœä½ æœ‰åƒ EQL
    è¿™æ ·çš„ä¸œè¥¿ï¼Œä½ ä¹Ÿå¯ä»¥å®ç°è¿™äº›æŸå¤±å‡½æ•°å—ï¼ŸğŸ˜Šï¼Œåœ¨å†³ç­–å˜å‹å™¨ä¹‹ä¸Šã€‚
- en: So I think maybe I could imagine ways in which you could encode pessimism in
    here as wellã€‚which is key to how CQL worksï¼Œ and actually most of oftenal algorithms
    workã€‚including the model based onesã€‚Our focus here deliberately was to go out
    is simplicity because we feel that part of the reason why oural literature has
    been so scattered as well if you think about different sub problems that everyone
    tries to solve has been because everyone's tried toã€‚
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³ï¼Œä¹Ÿè®¸æˆ‘å¯ä»¥æƒ³è±¡åœ¨è¿™é‡Œç¼–ç æ‚²è§‚ä¸»ä¹‰çš„æ–¹æ³•ï¼Œè¿™å¯¹ CQL çš„å·¥ä½œåŸç†è‡³å…³é‡è¦ï¼Œå®é™…ä¸Šå¤§å¤šæ•°ç®—æ³•ä¹Ÿæ˜¯å¦‚æ­¤ï¼ŒåŒ…æ‹¬åŸºäºæ¨¡å‹çš„é‚£äº›ã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ•…æ„å…³æ³¨ç®€å•æ€§ï¼Œå› ä¸ºæˆ‘ä»¬è®¤ä¸ºï¼Œæ•£ä¹±çš„å­¦æœ¯æ–‡çŒ®éƒ¨åˆ†åŸå› åœ¨äºï¼Œå¤§å®¶éƒ½è¯•å›¾è§£å†³ä¸åŒçš„å­é—®é¢˜ã€‚
- en: ğŸ˜Šï¼ŒPick up on IDOs which are very well suited for that narrow problem like for
    exampleã€‚you have whether you're doing offline or you're doing online or you're
    doing imitationã€‚you're doing multitals and all these different variantsã€‚And so
    by designï¼Œ we did not we were veryã€‚we did not want to incorporate exactly the
    components that exist in the current algorithms because then it just where it
    starts looking more likeã€‚
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå…³æ³¨ IDOsï¼Œå®ƒä»¬éå¸¸é€‚åˆè§£å†³åƒä½ åœ¨çº¿æˆ–ç¦»çº¿ã€æ¨¡ä»¿ã€è¿›è¡Œå¤šä»»åŠ¡ç­‰å„ç§ä¸åŒå˜ä½“çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„è®¾è®¡å¹¶æ²¡æœ‰æƒ³è¦ç›´æ¥åŒ…å«å½“å‰ç®—æ³•ä¸­çš„ç¡®åˆ‡ç»„ä»¶ï¼Œå› ä¸ºé‚£æ ·çœ‹èµ·æ¥å°±ä¼šæ›´åƒæ˜¯æ¶æ„çš„å˜åŒ–ï¼Œè€Œä¸æ˜¯æ›´æ¦‚å¿µæ€§çš„å˜åŒ–ï¼Œè½¬å‘æ€è€ƒ
    RS åºåˆ—å»ºæ¨¡ã€‚éå¸¸ä¸€èˆ¬åœ°è¯´ï¼Œæ˜¯çš„ï¼Œå°±è¿™æ ·ã€‚
- en: Architecture change that opposed to a more conceptual change engine to thinking
    about RS sequence modelingã€‚Very generallyã€‚cor itï¼Œ yeah justã€‚So do you think like
    we can use some sort of like like3 learning objectives instead of supervised learningï¼Ÿ
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è®¤ä¸ºæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æŸç§åƒ 3 å­¦ä¹ ç›®æ ‡è€Œä¸æ˜¯ç›‘ç£å­¦ä¹ å—ï¼Ÿ
- en: è¯¶ã€‚It's possible and maybe like I'm saying that for certain like for online RLï¼Œ
    it might be necessaryã€‚ğŸ˜Šï¼ŒOfï¼Œ we were happy to see it was not necessaryã€‚But it remains
    to be seen more generally for a transformer model or any other model for that
    matterã€‚encompassing R more broadly whether that becomes a necessityã€‚å“¦ä¹Ÿã€‚Wellï¼Œ thanks
    for your timeã€‚this was greatã€‚
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¶ï¼Œè¿™å¯èƒ½æ˜¯çš„ï¼Œä¹Ÿè®¸æˆ‘è¯´ï¼Œå¯¹äºæŸäº›æƒ…å†µï¼Œæ¯”å¦‚åœ¨çº¿ RLï¼Œè¿™å¯èƒ½æ˜¯å¿…è¦çš„ã€‚ğŸ˜Šï¼Œä¸è¿‡ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°å®ƒå¹¶ä¸æ˜¯å¿…éœ€çš„ã€‚ä½†å¯¹äºå˜å‹å™¨æ¨¡å‹æˆ–å…¶ä»–æ¨¡å‹æ˜¯å¦å˜å¾—å¿…è¦ï¼Œä»éœ€è¿›ä¸€æ­¥è§‚å¯Ÿã€‚å“¦ï¼Œä¹Ÿå¥½ã€‚æ„Ÿè°¢ä½ çš„æ—¶é—´ï¼Œè¿™çœŸæ˜¯å¤ªå¥½äº†ã€‚
- en: '![](img/041a2ae7aae080f075f80cd7c4c33a68_4.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/041a2ae7aae080f075f80cd7c4c33a68_4.png)'
