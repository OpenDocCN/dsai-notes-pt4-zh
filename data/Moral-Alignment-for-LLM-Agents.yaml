- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:12:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:12:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Moral Alignment for LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM代理的道德对齐
- en: 来源：[https://arxiv.org/html/2410.01639/](https://arxiv.org/html/2410.01639/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.01639/](https://arxiv.org/html/2410.01639/)
- en: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
- en: Department of Computer Science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: University College London
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 伦敦大学学院
- en: Gower St, London, United Kingdom
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 高尔街，伦敦，英国
- en: '{l.karmannaya.16, s.hailes, m.musolesi} @ucl.ac.uk'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{l.karmannaya.16, s.hailes, m.musolesi} @ucl.ac.uk'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Decision-making agents based on pre-trained Large Language Models (LLMs) are
    increasingly being deployed across various domains of human activity. While their
    applications are currently rather specialized, several research efforts are under
    way to develop more generalist agents. As LLM-based systems become more agentic,
    their influence on human activity will grow and the transparency of this will
    decrease. Consequently, developing effective methods for aligning them to human
    values is vital.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于预训练大语言模型（LLMs）的决策代理正在越来越多地应用于人类活动的各个领域。尽管它们目前的应用相对专门化，但多个研究正在进行，旨在开发更具通用性的代理。随着基于LLM的系统变得更加代理化，它们对人类活动的影响将增加，而这种影响的透明度将减少。因此，开发有效的方法来使它们与人类价值观对齐至关重要。
- en: The prevailing practice in alignment often relies on human preference data (e.g.,
    in RLHF or DPO), in which values are implicit and are essentially deduced from
    relative preferences over different model outputs. In this work, instead of relying
    on human feedback, we introduce the design of reward functions that explicitly
    encode core human values for Reinforcement Learning-based fine-tuning of foundation
    agent models. Specifically, we use intrinsic rewards for the moral alignment of
    LLM agents.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目前对齐的主流做法通常依赖于人类偏好数据（例如，RLHF 或 DPO），其中的价值观是隐含的，实际上是通过不同模型输出的相对偏好来推断的。在这项工作中，我们引入了一种不依赖于人类反馈的方法，即设计奖励函数，明确编码核心人类价值观，以用于基于强化学习的基础代理模型微调。具体来说，我们使用内在奖励来进行LLM代理的道德对齐。
- en: We evaluate our approach using the traditional philosophical frameworks of Deontological
    Ethics and Utilitarianism, quantifying moral rewards for agents in terms of actions
    and consequences on the Iterated Prisoner’s Dilemma (IPD) environment. We also
    show how moral fine-tuning can be deployed to enable an agent to unlearn a previously
    developed selfish strategy. Finally, we find that certain moral strategies learned
    on the IPD game generalize to several other matrix game environments. In summary,
    we demonstrate that fine-tuning with intrinsic rewards is a promising general
    solution for aligning LLM agents to human values, and it might represent a more
    transparent and cost-effective alternative to currently predominant alignment
    techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用传统的哲学框架——义务伦理学和功利主义，评估了我们的方法，通过对“重复囚徒困境”（IPD）环境中的行动及其后果进行量化，来衡量代理的道德奖励。我们还展示了如何通过道德微调，使代理能够放弃之前发展出的自私策略。最后，我们发现，某些在IPD游戏中学到的道德策略可以推广到其他几种矩阵博弈环境中。总之，我们证明了使用内在奖励进行微调是一种有前景的通用解决方案，能够将LLM代理与人类价值观对齐，它可能代表着一种比当前主流对齐技术更具透明度和成本效益的替代方案。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The alignment problem is an active field of research in Machine Learning (Christian,
    [2020](https://arxiv.org/html/2410.01639v2#bib.bib17); Weidinger et al., [2021](https://arxiv.org/html/2410.01639v2#bib.bib67);
    Anwar et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib6); Gabriel et al.,
    [2024](https://arxiv.org/html/2410.01639v2#bib.bib21); Ji et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib31);
    Ngo et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib38)). It is gaining
    even wider importance with the advances and rapid deployment of Large Language
    Models (LLMs, Anthropic [2024](https://arxiv.org/html/2410.01639v2#bib.bib5);
    Gemini Team [2024](https://arxiv.org/html/2410.01639v2#bib.bib23); OpenAI [2024](https://arxiv.org/html/2410.01639v2#bib.bib41)).
    The most common practices in the alignment of LLMs today involve Reinforcement
    Learning from Human Feedback (RLHF - Glaese et al. [2022](https://arxiv.org/html/2410.01639v2#bib.bib25);
    Ouyang et al. [2022](https://arxiv.org/html/2410.01639v2#bib.bib42); Bai et al.
    [2023](https://arxiv.org/html/2410.01639v2#bib.bib9)) or Direct Preference Optimization
    (DPO - Rafailov et al. [2024](https://arxiv.org/html/2410.01639v2#bib.bib45)).
    Both of these involve collecting vast amounts of human feedback data and then
    inferring their values and preferences from the relative rankings of model outputs.
    In doing so, human values are implicitly represented.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对齐问题是机器学习领域的一个活跃研究方向（Christian，[2020](https://arxiv.org/html/2410.01639v2#bib.bib17)；Weidinger
    等，[2021](https://arxiv.org/html/2410.01639v2#bib.bib67)；Anwar 等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib6)；Gabriel
    等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib21)；Ji 等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib31)；Ngo
    等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib38)）。随着大型语言模型（LLMs）的进展和快速部署，它的重要性日益增加（Anthropic
    [2024](https://arxiv.org/html/2410.01639v2#bib.bib5)；Gemini 团队 [2024](https://arxiv.org/html/2410.01639v2#bib.bib23)；OpenAI
    [2024](https://arxiv.org/html/2410.01639v2#bib.bib41)）。目前，LLM 对齐的最常见做法包括基于人类反馈的强化学习（RLHF
    - Glaese 等，[2022](https://arxiv.org/html/2410.01639v2#bib.bib25)；Ouyang 等，[2022](https://arxiv.org/html/2410.01639v2#bib.bib42)；Bai
    等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib9)）或直接偏好优化（DPO - Rafailov
    等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib45)）。这两种方法都涉及收集大量的人类反馈数据，然后通过模型输出的相对排名推断它们的价值和偏好。在此过程中，人类的价值观被隐式地表示出来。
- en: This approach poses certain challenges (Casper et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib16)).
    Specifically, collecting preference data is very costly and often relies on potentially
    unrepresentative samples of human raters. Indeed, the values derived through this
    process are strongly dependent on the selection criteria of the pool of individuals.
    Furthermore, human preferences are notoriously complex and inconsistent. In RLHF,
    the values that are ultimately incorporated into the fine-tuned models are learned
    by a reward model from data in a fully bottom-up fashion, and are never made explicit
    to any human oversight. One might argue that current LLMs fine-tuned with these
    methods are able to provide “honest, harmless and helpful” responses (Glaese et al.,
    [2022](https://arxiv.org/html/2410.01639v2#bib.bib25); Bai et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib9))
    and already display certain moral values (Schramowski et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib47);
    Abdulhai et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib1); Hartmann
    et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib26)). Methods to guide
    the social behavior of LLM agents via interaction in rule-guided simulated societies
    have also been proposed recently (Liu et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib34)).
    However, models’ apparent values can also be interpreted as “moral mimicry” of
    their users when responding to these prompts (Simmons, [2022](https://arxiv.org/html/2410.01639v2#bib.bib53);
    Shanahan et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib49)). As a
    consequence, given phenomena such as situationally-aware reward-hacking or misalignment
    in internally-represented goals (Ngo et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib38)),
    the true values learned by the models through these methods may give rise to dangerous
    behaviors, which will not be explicitly known until after deployment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法带来了一些挑战（Casper 等， [2023](https://arxiv.org/html/2410.01639v2#bib.bib16)）。具体来说，收集偏好数据成本非常高，并且通常依赖于可能不具代表性的人工评分样本。实际上，通过这一过程得出的价值观在很大程度上依赖于个体池的选择标准。此外，人的偏好是出了名的复杂和不一致。在
    RLHF 中，最终被纳入微调模型的价值观是通过奖励模型从数据中以完全自下而上的方式学习的，并且从未向任何人类监督明确揭示过。有人可能会认为，目前通过这些方法微调的
    LLMs 能够提供“诚实、无害且有帮助”的回答（Glaese 等， [2022](https://arxiv.org/html/2410.01639v2#bib.bib25)；Bai
    等， [2023](https://arxiv.org/html/2410.01639v2#bib.bib9)），并且已经展示出某些道德价值观（Schramowski
    等， [2022](https://arxiv.org/html/2410.01639v2#bib.bib47)；Abdulhai 等， [2023](https://arxiv.org/html/2410.01639v2#bib.bib1)；Hartmann
    等， [2023](https://arxiv.org/html/2410.01639v2#bib.bib26)）。最近也提出了通过在规则引导的模拟社会中与
    LLM 代理进行互动来指导其社会行为的方法（Liu 等， [2024](https://arxiv.org/html/2410.01639v2#bib.bib34)）。然而，模型的显性价值观也可以被解读为在回应这些提示时对用户的“道德模仿”（Simmons，[2022](https://arxiv.org/html/2410.01639v2#bib.bib53)；Shanahan
    等， [2023](https://arxiv.org/html/2410.01639v2#bib.bib49)）。因此，考虑到诸如情境感知奖励破解或内部表示目标不一致等现象（Ngo
    等， [2024](https://arxiv.org/html/2410.01639v2#bib.bib38)），通过这些方法学习到的模型的真实价值观可能会导致危险行为，这些行为在部署之前是不会明确知道的。
- en: 'Our work aims to address this type of goal misgeneralization in particular
    by providing clearer, explicit moral alignment goals as intrinsic rewards for
    fine-tuning RL-based algorithms¹¹1For a more comprehensive discussion of learning
    as a method for moral alignment with implicit (bottom-up) versus explicit (top-down)
    principles, we refer the interested reader to Tennant et al. ([2023b](https://arxiv.org/html/2410.01639v2#bib.bib60))..
    In this study, we approach alignment from an agent-based perspective. Since LLMs
    are increasingly adopted as a basis for strategic decision-making systems and
    agentic workflows (Wang et al., [2024b](https://arxiv.org/html/2410.01639v2#bib.bib65)),
    it is critical that we align the choices made by LLM agents with our values, including
    value judgments about what actions are morally good or bad (Amodei et al., [2016](https://arxiv.org/html/2410.01639v2#bib.bib3);
    Anwar et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib6)). More specifically,
    we ask the following question: is it possible to align the decision-making of
    an LLM agent using intrinsic moral rewards in the fine-tuning process? Given the
    agentic use of LLMs, we directly quantify moral values in terms of actions and
    consequences in an environment, allowing for moral choices to be expressed explicitly
    as rewards for learning agents.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作旨在特别解决这一类型的目标误泛化问题，通过提供更清晰、明确的道德对齐目标，作为强化学习算法微调的内在奖励¹¹1对于学习作为道德对齐方法的更全面讨论，涉及隐性（自下而上）与显性（自上而下）原则的区别，感兴趣的读者可以参考Tennant等人（[2023b](https://arxiv.org/html/2410.01639v2#bib.bib60)）的研究。在本研究中，我们从基于代理的视角来探讨对齐问题。由于LLM被越来越多地作为战略决策系统和代理工作流程（Wang等人，[2024b](https://arxiv.org/html/2410.01639v2#bib.bib65)）的基础，因此确保LLM代理所做出的选择与我们的价值观对齐至关重要，包括关于哪些行动是道德上良好或不良的价值判断（Amodei等人，[2016](https://arxiv.org/html/2410.01639v2#bib.bib3)；Anwar等人，[2024](https://arxiv.org/html/2410.01639v2#bib.bib6)）。更具体地说，我们提出以下问题：是否有可能通过在微调过程中使用内在道德奖励来对齐LLM代理的决策？考虑到LLM的代理化使用，我们通过环境中的行动和后果直接量化道德价值，从而使道德选择可以显式地作为学习代理的奖励来表达。
- en: We explore the proposed framework using an Iterated Prisoner’s Dilemma environment,
    in which we evaluate the effectiveness of fine-tuning based on intrinsic rewards
    as a mechanism for learning moral strategies as well as “unlearning”²²2We note
    that by “unlearning” we refer to re-prioritizing certain principles in an agent’s
    decision-making. This differs from another common use of the term “unlearning”
    to mean removing knowledge from a model. a selfish strategy. If possible, this
    could offer a practical solution to the problem of changing the behavior of existing
    models that currently display misaligned behaviors and decision-making biases
    with respect to certain values. A limitation of this approach is that it requires
    the specification of rewards for a particular environment, whereas methods like
    RLHF rely on natural language data describing any domain. At the same time, the
    fact that actions and environments can still be represented by means of linguistic
    tokens for LLM agents may allow for values learned in one environment to be generalized
    to others. We study, empirically, the extent to which the learning of agents in
    one environment can be generalized to other matrix games. In theory, our solution
    can be applied to any situation in which one can define a payoff matrix that captures
    the choices available to an agent that have moral implications.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个反复囚徒困境环境来探索所提出的框架，在这个环境中，我们评估基于内在奖励的微调作为学习道德策略的机制，并研究如何“去学习”²²2我们指出，所谓“去学习”是指在代理的决策中重新调整某些原则的优先级。这与“去学习”一词的另一种常见用法不同，后者是指从模型中移除知识。自私策略。如果可能，这可以为解决现有模型当前表现出与某些价值观不对齐的行为和决策偏差的问题提供一种实际的解决方案。该方法的一个局限性是，它需要为特定环境指定奖励，而像RLHF这样的技术依赖于描述任何领域的自然语言数据。同时，考虑到行动和环境仍然可以通过语言符号表示用于LLM代理，这可能使得在一个环境中学到的价值观能够推广到其他环境。我们通过实证研究了在一个环境中学习的代理，能够在多大程度上推广到其他矩阵博弈的情况。从理论上讲，我们的解决方案可以应用于任何可以定义支付矩阵的情况，该矩阵捕捉到代理的选择，并具有道德意义。
- en: 'To summarize, our study provides the following contributions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的研究提供了以下贡献：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a novel, general solution for aligning LLM agents to human moral
    values by means of fine-tuning via Reinforcement Learning with Intrinsic Rewards.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过强化学习与内在奖励的微调方式，提出了一种新颖的通用解决方案，用于将LLM代理与人类道德价值观对齐。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate the approach using a repeated social dilemma game environment (with
    fixed-strategy and learning opponents), and Deontological and Utilitarian moral
    values. We show that LLM agents fine-tuned with intrinsic rewards are able to
    successfully learn aligned moral strategies.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用重复的社会困境游戏环境（包括固定策略和学习型对手）以及义务论和功利主义道德价值观来评估该方法。我们展示了微调了内在奖励的LLM代理能够成功学习到与道德策略一致的行为。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We discuss how the proposed solution can be generalized and applied to different
    scenarios in which moral choices can be captured by means of payoff matrices.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们讨论了如何将所提出的解决方案推广并应用于不同的场景，其中道德选择可以通过收益矩阵来捕捉。
- en: 2 Background
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 2.1 LLM Agents
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 代理
- en: Agency refers to the ability of a system to decide to take actions in the world
    (Swanepoel & Corks, [2024](https://arxiv.org/html/2410.01639v2#bib.bib58)). In
    this paper, we equate agency with strategic decision-making - i.e., making a choice
    in an environment in which multiple actions are available and lead to different
    outcomes. In the case of LLMs, this view assumes that model outputs will be interpreted
    as actions in some environment. The simplest way of implementing this is through
    the use of specific tokens to represent the actions. Particular tokens can be
    reserved or fine-tuned from the model’s vocabulary to represent actions, and planning
    and reasoning ability can be improved via action-driven prompting strategies (Yao
    et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib69)). Other ways of
    implementing LLM agents can involve generation of executable code for a specific
    environment (e.g., a video game, Wang et al. [2024a](https://arxiv.org/html/2410.01639v2#bib.bib64))
    or connection to various tool APIs (e.g., Patil et al. [2023](https://arxiv.org/html/2410.01639v2#bib.bib44);
    Shen et al. [2023](https://arxiv.org/html/2410.01639v2#bib.bib50)), but these
    are more specialized and, therefore, not the focus of this work.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 代理指的是系统决定在世界中采取行动的能力（Swanepoel & Corks，[2024](https://arxiv.org/html/2410.01639v2#bib.bib58)）。在本文中，我们将代理等同于战略决策——即在一个有多个可选行动并导致不同结果的环境中做出选择。对于LLM（大型语言模型），这种观点假设模型输出将被解释为某个环境中的行动。实现这一点的最简单方法是通过使用特定的令牌来表示这些行动。可以保留或微调模型词汇中的特定令牌来代表行动，通过行动驱动的提示策略（Yao等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib69)）可以提高规划和推理能力。实现LLM代理的其他方法可以包括为特定环境生成可执行代码（例如视频游戏，Wang等，[2024a](https://arxiv.org/html/2410.01639v2#bib.bib64)）或连接到各种工具API（例如，Patil等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib44)；Shen等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib50)），但这些方法更为专业化，因此不属于本文的重点。
- en: Specific action tokens, as used in this study, can be defined in the prompt
    given to an LLM to represent an action choice for the agent. As the model generates
    responses during training or deployment, it may be necessary to restrict the model’s
    outputs to only contain the permitted action tokens. Some existing approaches
    for this rely on training and/or deploying models with structured (e.g., JSON)
    output formats or constrained generation (Beurer-Kellner et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib11)),
    which suppresses the probabilities of all tokens in the model’s output layer except
    for the legal action tokens. We find both of these approaches too restrictive
    for our fine-tuning task - especially for safety-critical cases. Fine-tuning based
    on a restricted output space or format poses risks of the model “hiding” undesirable
    behaviors (Anwar et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib6)).
    Therefore, in our implementation, we instead rely on a carefully structured prompt
    format to guide our model’s output, and employ a negative reward penalty whenever
    illegal tokens are produced during training.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的特定行动令牌可以在给定给LLM的提示中定义，表示代理的行动选择。当模型在训练或部署过程中生成响应时，可能需要限制模型的输出只包含允许的行动令牌。一些现有方法依赖于使用结构化（例如JSON）输出格式或受限生成（Beurer-Kellner等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib11)），通过这种方式抑制模型输出层中所有令牌的概率，除了合法的行动令牌。我们认为这两种方法对于我们的微调任务来说过于限制，尤其是在安全关键的情况下。基于受限输出空间或格式的微调存在模型“隐藏”不良行为的风险（Anwar等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib6)）。因此，在我们的实现中，我们依赖于精心构建的提示格式来引导模型的输出，并在训练过程中每当生成非法令牌时采用负奖励惩罚。
- en: Using the techniques outlined, agents based on pre-trained LLMs combined with
    other elements of various cognitive architectures (Sumers et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib57))
    such as a skill set (Wang et al., [2024a](https://arxiv.org/html/2410.01639v2#bib.bib64))
    or a memory store (Vezhnevets et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib62))
    have been used to reasonably simulate decision-making in open-ended environments
    (Wang et al., [2024b](https://arxiv.org/html/2410.01639v2#bib.bib65)), including
    those with only a single-agent (Wang et al., [2024a](https://arxiv.org/html/2410.01639v2#bib.bib64))
    or of a multi-agent nature (Park et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib43)).
    Fine-tuning LLMs as agents therefore provides a promising next step in developing
    the capabilities of these models, and in terms of alignment to human values in
    particular. LLMs fine-tuned with RLHF, and especially those fine-tuned to follow
    human instructions, have been shown to become more goal-directed than simple sequence-completion
    foundation models (Glaese et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib25);
    Ouyang et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib42); Bai et al.,
    [2023](https://arxiv.org/html/2410.01639v2#bib.bib9)). We rely on instruction-tuned
    LLMs in this study and use the Gemma2-2b-it model in particular (Gemma Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib24))
    as a decision-making agent in social dilemma games. Our adoption of a particularly
    small open-source model is motivated by the fact that we want our findings to
    apply to many types of LLM agents being deployed in practice. Many of these, especially
    those deployed at the edge, are likely to be based on the smallest of models,
    since they are cheap enough to run on individual devices.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 运用上述技术，基于预训练LLM并结合其他认知架构元素（Sumers等， [2024](https://arxiv.org/html/2410.01639v2#bib.bib57)）的代理，例如技能集（Wang等，[2024a](https://arxiv.org/html/2410.01639v2#bib.bib64)）或记忆存储（Vezhnevets等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib62)），已经被用于合理模拟开放环境中的决策制定（Wang等，[2024b](https://arxiv.org/html/2410.01639v2#bib.bib65)），包括那些只有单一代理（Wang等，[2024a](https://arxiv.org/html/2410.01639v2#bib.bib64)）或具有多代理特征的环境（Park等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib43)）。因此，对LLM进行微调作为代理，提供了一个有前景的下一步，来发展这些模型的能力，特别是在与人类价值观对齐方面。使用RLHF进行微调的LLM，尤其是那些微调以遵循人类指令的LLM，已经被证明比简单的序列完成基础模型更具目标导向性（Glaese等，[2022](https://arxiv.org/html/2410.01639v2#bib.bib25)；Ouyang等，[2022](https://arxiv.org/html/2410.01639v2#bib.bib42)；Bai等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib9)）。在本研究中，我们依赖于指令调优的LLM，特别使用Gemma2-2b-it模型（Gemma
    Team，[2024](https://arxiv.org/html/2410.01639v2#bib.bib24)）作为社交困境游戏中的决策代理。我们选择一个特别小的开源模型，主要是因为我们希望我们的发现能够应用于许多实际部署的LLM代理类型。尤其是那些部署在边缘设备上的LLM，往往会基于最小的模型，因为这些模型足够便宜，可以在单个设备上运行。
- en: 2.2 Fine-tuning LLM Agents with Reinforcement Learning
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 使用强化学习微调LLM代理
- en: Proximal Policy Optimization (PPO, Schulman et al. [2017](https://arxiv.org/html/2410.01639v2#bib.bib48))
    is the most commonly used technique for fine-tuning LLMs with RL (Stiennon et al.,
    [2022](https://arxiv.org/html/2410.01639v2#bib.bib56)). This on-policy method
    is often deployed in conjunction with a Kullback-Leibler (KL) penalty to prevent
    the new model from shifting too far away from the original underlying token distribution
    and thus losing other capabilities such as producing coherent linguistic output
    (Jaques et al., [2017](https://arxiv.org/html/2410.01639v2#bib.bib30); Ziegler
    et al., [2020](https://arxiv.org/html/2410.01639v2#bib.bib72); Stiennon et al.,
    [2022](https://arxiv.org/html/2410.01639v2#bib.bib56)). Offline fine-tuning methods
    have also been developed (Snell et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib54))
    and may provide a more sample-efficient alternative in the future. The reward
    signal for RL-based training in existing implementations tends to be derived from
    preference data provided by human raters (Glaese et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib25);
    Ouyang et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib42); Bai et al.,
    [2023](https://arxiv.org/html/2410.01639v2#bib.bib9)) or a constitution of other
    human and/or artificial agents (Bai et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib8);
    Huang et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib28)). In this
    study we propose a new methodology for RL-based fine-tuning with intrinsic moral
    rewards.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 近端策略优化（PPO，Schulman 等人，[2017](https://arxiv.org/html/2410.01639v2#bib.bib48)）是最常用的用于强化学习（RL）微调大规模语言模型（LLM）的方法（Stiennon
    等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib56)）。这种在线策略方法通常与 Kullback-Leibler（KL）惩罚一起使用，以防止新模型的分布偏离原始基础词元分布过远，从而丧失其他能力，如生成连贯的语言输出（Jaques
    等人，[2017](https://arxiv.org/html/2410.01639v2#bib.bib30)；Ziegler 等人，[2020](https://arxiv.org/html/2410.01639v2#bib.bib72)；Stiennon
    等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib56)）。离线微调方法也已被开发出来（Snell
    等人，[2023](https://arxiv.org/html/2410.01639v2#bib.bib54)），并且未来可能提供更高效的样本使用替代方案。现有实现中的基于强化学习的训练奖励信号通常来源于由人类评审员提供的偏好数据（Glaese
    等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib25)；Ouyang 等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib42)；Bai
    等人，[2023](https://arxiv.org/html/2410.01639v2#bib.bib9)）或由其他人类和/或人工智能代理组成的奖励信号（Bai
    等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib8)；Huang 等人，[2024](https://arxiv.org/html/2410.01639v2#bib.bib28)）。在本研究中，我们提出了一种基于强化学习的微调新方法，结合了内在的道德奖励。
- en: Compared to non-linguistic RL agent training, the pre-trained LLM in this case
    can be viewed as providing a common-sense model ³³3We note that the extent of
    true commonsense knowledge of LLMs is still debated (Mitchell, [2021](https://arxiv.org/html/2410.01639v2#bib.bib37)),
    especially for smaller models. Nevertheless, benchmark evaluations suggest the
    emergence of common sense and reasoning abilities even in models as small as 2b
    parameters - for example, Gemma2-2b-it scores over 85% (Gemma Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib24))
    on the commonsense benchmark introduced by Zellers et al. [2019](https://arxiv.org/html/2410.01639v2#bib.bib70).
    of the world (Wong et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib68)),
    equipping an LLM-based agent with some intuition about potential dynamics of various
    environments. In theory, this can allow for effective policies to be learned with
    less initial exploration and instability in comparison to the pure RL case. Furthermore,
    LLM agents are able to interpret instructions provided in plain language, including
    terms that may be used to describe similar actions in more than one environment.
    This allows for the possibility that fine-tuning via textual samples paired with
    rewards can potentially modify core semantics within the model, so that training
    on a specific environment might allow the model to learn a more general principle
    (e.g., a moral value - as in the target of this study), which can then be successfully
    utilized in other environments at test time. Early results from text-instructed
    video models suggest that this generalization of learned behaviors across environments
    is indeed possible (SIMA Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib52)).
    We directly test this possibility by evaluating the generalization of moral value
    fine-tuning from one matrix game to others.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 与非语言强化学习（RL）代理训练相比，这里的预训练大语言模型（LLM）可以被视为提供了一种常识模型³³3我们注意到，大语言模型的真实常识知识程度仍然存在争议（Mitchell,
    [2021](https://arxiv.org/html/2410.01639v2#bib.bib37)），尤其是对于较小的模型而言。尽管如此，基准评估表明，甚至是像2b参数这样的较小模型也开始展现出常识和推理能力——例如，Gemma2-2b-it在Zellers等人于[2019](https://arxiv.org/html/2410.01639v2#bib.bib70)提出的常识基准测试中得分超过85%（Gemma
    Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib24)）。这种常识可以为LLM代理提供一定的直觉，使其能够理解不同环境的潜在动态（Wong
    et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib68)），从而使基于LLM的代理能够在较少初期探索和不稳定性的情况下学会有效的策略，相较于纯RL方法。此外，LLM代理能够理解以普通语言提供的指令，包括描述多个环境中相似行为的术语。这使得通过文本样本和奖励的微调有可能修改模型中的核心语义，使得在特定环境下的训练可能使模型学会更一般的原则（例如道德价值——本研究的目标），并在测试时能成功应用到其他环境中。来自文本指令视频模型的早期结果表明，跨环境学习行为的泛化确实是可能的（SIMA
    Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib52)）。我们通过评估道德价值微调从一个矩阵博弈到其他博弈的泛化能力，直接测试了这一可能性。
- en: 2.3 Social Dilemma Games
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 社会困境博弈
- en: A prominent social dilemma game is the Iterated Prisoner’s Dilemma (IPD), in
    which a player can Cooperate (C) with their opponent for mutual benefit, or betray
    them - i.e., Defect (D) for individual reward (Rapoport, [1974](https://arxiv.org/html/2410.01639v2#bib.bib46);
    Axelrod & Hamilton, [1981](https://arxiv.org/html/2410.01639v2#bib.bib7)). The
    payoffs in any step of the game are determined by a payoff matrix, presented for
    the row player versus a column player in Figure [1](https://arxiv.org/html/2410.01639v2#S2.F1
    "Figure 1 ‣ 2.3 Social Dilemma Games ‣ 2 Background ‣ Moral Alignment for LLM
    Agents").
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个著名的社会困境博弈是反复囚徒困境（IPD），在这个博弈中，玩家可以与对手合作（C）以实现互惠，或者背叛对方——即背离（D）以获得个人奖励（Rapoport,
    [1974](https://arxiv.org/html/2410.01639v2#bib.bib46); Axelrod & Hamilton, [1981](https://arxiv.org/html/2410.01639v2#bib.bib7)）。博弈中的每一步的奖励由奖励矩阵决定，如图[1](https://arxiv.org/html/2410.01639v2#S2.F1
    "Figure 1 ‣ 2.3 Social Dilemma Games ‣ 2 Background ‣ Moral Alignment for LLM
    Agents")所示，表示行玩家与列玩家之间的支付情况。
- en: '|  | C | D |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | C | D |'
- en: '| C | 3,3 | 0,4 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| C | 3,3 | 0,4 |'
- en: '| D | 4,0 | 1,1 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| D | 4,0 | 1,1 |'
- en: 'Figure 1: Payoffs for the Iterated Prisoner’s Dilemma'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：反复囚徒困境的奖励
- en: In a single iteration of the game, the payoffs motivate each player to Defect
    due to the risk of facing an uncooperative opponent (i.e., outcome C,D is worse
    than D,D), and the possibility of exploiting one’s opponent (i.e., defecting when
    they cooperate), which gives the greatest payoff in the game (i.e., D,C is preferred
    over C,C). Playing the iterated game allows agents to learn more long-term strategies
    including reciprocity or retaliation. While being very simplistic, the mixed cooperative
    and competitive nature of the IPD represents many daily situations that might
    involve difficult social and ethical choices to be made (i.e., moral dilemmas).
    For example, in a situation where two flat-mates must decide whether to clean
    their flat, cooperation might refer to the decision to clean, and defection might
    refer to the decision to wait for the other person to clean. This is why it has
    been extensively used for studying social dilemmas in traditional RL-based agents
    (Bruns, [2015](https://arxiv.org/html/2410.01639v2#bib.bib13); Hughes et al.,
    [2018](https://arxiv.org/html/2410.01639v2#bib.bib29); Anastassacos et al., [2020](https://arxiv.org/html/2410.01639v2#bib.bib4);
    McKee et al., [2020](https://arxiv.org/html/2410.01639v2#bib.bib36); Leibo et al.,
    [2021](https://arxiv.org/html/2410.01639v2#bib.bib33)) and, more recently, utilized
    as a training environment for moral alignment of agents in particular (Tennant
    et al., [2023a](https://arxiv.org/html/2410.01639v2#bib.bib59); [2024](https://arxiv.org/html/2410.01639v2#bib.bib61)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在游戏的单次迭代中，由于面临不合作对手的风险（即，C,D 的结果比 D,D 更差），以及利用对手的可能性（即，在对方合作时背叛），这促使每个玩家做出背叛决策，而背叛能够在游戏中获得最大的收益（即，D,C
    比 C,C 更优）。进行迭代游戏使得代理能够学习更多的长期策略，包括互惠或报复。尽管非常简化，IPD 混合的合作与竞争性质代表了许多日常情境，这些情境可能涉及需要做出困难的社会和伦理选择（即，道德困境）。例如，在两位室友必须决定是否打扫公寓的情境中，合作可能指的是决定清洁，而背叛则可能指的是等待对方打扫。这就是为什么它在传统基于强化学习的代理中被广泛用于研究社会困境（Bruns，[2015](https://arxiv.org/html/2410.01639v2#bib.bib13)；Hughes
    等，[2018](https://arxiv.org/html/2410.01639v2#bib.bib29)；Anastassacos 等，[2020](https://arxiv.org/html/2410.01639v2#bib.bib4)；McKee
    等，[2020](https://arxiv.org/html/2410.01639v2#bib.bib36)；Leibo 等，[2021](https://arxiv.org/html/2410.01639v2#bib.bib33)）并且最近被作为一个训练环境，特别用于代理的道德对齐（Tennant
    等，[2023a](https://arxiv.org/html/2410.01639v2#bib.bib59)；[2024](https://arxiv.org/html/2410.01639v2#bib.bib61)）。
- en: The behavior of LLM agents on decision-making and game-theoretic scenarios has
    been extensively debated in recent literature (Gandhi et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib22);
    Fan et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib20); Zhang et al.,
    [2024](https://arxiv.org/html/2410.01639v2#bib.bib71)). LLM agents have been found
    to act differently to humans, and in ways that are still not fully “rational”
    in terms of forming goals from a prompt, refining beliefs, or taking optimal actions
    based on those goals and beliefs (Fan et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib20);
    Macmillan-Scott & Musolesi, [2024](https://arxiv.org/html/2410.01639v2#bib.bib35)).
    Large-scale state-of-the-art models playing the IPD have been observed to deploy
    sensible yet “unforgiving” strategies (Akata et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib2)),
    but some benchmark datasets suggest that these models lack true strategic reasoning
    in games including the IPD (Duan et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib18)).
    New developments in in-token reasoning capabilities of state-of-the-art LLM-based
    platforms (OpenAI, [2024](https://arxiv.org/html/2410.01639v2#bib.bib40)) as well
    as prompting strategies specifically centered around reasoning and acting (Wei
    et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib66); Shinn et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib51);
    Yao et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib69)) are likely
    to improve these capabilities, though existing results suggest that the benefits
    of these methods are more likely to arise for very large foundation models (Bubeck
    et al., [2023](https://arxiv.org/html/2410.01639v2#bib.bib14)). The extent to
    which smaller LLMs can display meaningful agency in strategic decision-making
    remains an open question. In this study, we address this question via fine-tuning
    a small model on the IPD as a fundamental and well-studied decision-making environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理在决策和博弈论场景中的行为已在近期文献中广泛讨论（Gandhi 等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib22)；Fan
    等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib20)；Zhang 等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib71)）。研究发现，LLM
    代理的行为与人类不同，并且在根据提示形成目标、完善信念或基于这些目标和信念采取最佳行动方面，仍然未完全“理性”（Fan 等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib20)；Macmillan-Scott
    & Musolesi，[2024](https://arxiv.org/html/2410.01639v2#bib.bib35)）。在IPD（囚徒困境）中，已观察到大规模的最先进模型采用合理但“无情”的策略（Akata
    等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib2)），但一些基准数据集表明，这些模型在包括 IPD
    在内的博弈中缺乏真正的战略推理能力（Duan 等，[2024](https://arxiv.org/html/2410.01639v2#bib.bib18)）。最先进的
    LLM 平台在代币内部推理能力的新发展（OpenAI，[2024](https://arxiv.org/html/2410.01639v2#bib.bib40)）以及专门围绕推理和行动的提示策略（Wei
    等，[2022](https://arxiv.org/html/2410.01639v2#bib.bib66)；Shinn 等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib51)；Yao
    等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib69)）可能会提高这些能力，尽管现有结果表明，这些方法的益处更可能出现在非常大的基础模型上（Bubeck
    等，[2023](https://arxiv.org/html/2410.01639v2#bib.bib14)）。较小的 LLM 是否能够在战略决策中展示有意义的自主性仍是一个未解的问题。在本研究中，我们通过对一个小型模型进行微调，以使其在
    IPD 这一基础且研究广泛的决策环境中进行学习，来解决这个问题。
- en: 2.4 Intrinsic Rewards for Moral Alignment
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 道德对齐的内在奖励
- en: 'In this work, we directly specify alignment goals for agents by defining intrinsic
    rewards in terms of actions in a social dilemma environment. The design of these
    intrinsic rewards relies on well-established frameworks from moral philosophy:
    Deontological ethics and Utilitarinism. Deontological ethics (Kant, [1785](https://arxiv.org/html/2410.01639v2#bib.bib32))
    considers an agent moral if their actions conform to certain norms. A prominent
    example of a norm is conditional cooperation (i.e., “it is unethical to defect
    against a cooperator"). This norm forms an essential component of direct and indirect
    reciprocity, a potentially essential mechanism for the evolution of cooperation
    in human and animal societies (Nowak, [2006](https://arxiv.org/html/2410.01639v2#bib.bib39)).
    Utilitarian morality (Bentham, [1780](https://arxiv.org/html/2410.01639v2#bib.bib10)),
    on the other hand, is a type of consequentialist reasoning, according to which
    an agent is deemed moral if their actions maximize collective “welfare” for all
    agents in their society (or, in this case, collective payoff for all players in
    the game), and less attention is paid to whether current actions adhere to norms.
    Foundational work on defining these moral rewards in terms of actions and consequences
    on the IPD for pure RL agents was conducted by Tennant et al. ([2023a](https://arxiv.org/html/2410.01639v2#bib.bib59))
    and Tennant et al. ([2024](https://arxiv.org/html/2410.01639v2#bib.bib61)). In
    this paper, we evaluate the extent to which this framework can be applied to align
    the behavior of LLM-based ones.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们通过在社会困境环境中定义与行动相关的内在奖励，直接为代理指定对齐目标。这些内在奖励的设计依赖于道德哲学中的成熟框架：义务伦理学和功利主义。义务伦理学（康德，[1785](https://arxiv.org/html/2410.01639v2#bib.bib32)）认为，如果一个代理的行为符合某些规范，则该代理是道德的。一个显著的规范例子是有条件合作（即“背叛合作伙伴是不道德的”）。这一规范是直接和间接互惠的基本组成部分，而互惠可能是人类和动物社会中合作进化的一个关键机制（诺瓦克，[2006](https://arxiv.org/html/2410.01639v2#bib.bib39)）。另一方面，功利主义道德（边沁，[1780](https://arxiv.org/html/2410.01639v2#bib.bib10)）是一种后果主义推理，根据这种推理，如果一个代理的行为最大化了他们社会中所有代理的集体“福利”（或在此情况下，最大化游戏中所有玩家的集体回报），那么该代理被认为是道德的，而较少关注当前的行为是否符合规范。Tennant等人（[2023a](https://arxiv.org/html/2410.01639v2#bib.bib59)）和Tennant等人（[2024](https://arxiv.org/html/2410.01639v2#bib.bib61)）在将这些道德奖励定义为基于IPD的纯RL代理的行动和后果方面做了开创性工作。本文中，我们评估了这一框架在多大程度上可以应用于对齐基于LLM的代理的行为。
- en: 3 Fine-tuning Methodology
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 微调方法
- en: 3.1 Agent and Environment
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 代理与环境
- en: The LLM agent and an opponent play a repeated Iterated Prisoner’s Dilemma game.
    At each time step, the model receives a prompt containing a description of the
    IPD game, including a state that contains the history of the opponent’s single
    previous move (see Figure [6](https://arxiv.org/html/2410.01639v2#S8.F6 "Figure
    6 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM
    Agents") in the Appendix). Within the MDP framework, each player’s current action
    affects the game’s state at the next time step.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理和对手进行重复的囚徒困境博弈。在每一个时间步，模型接收到一个提示，包含有关囚徒困境博弈的描述，包括一个状态，其中包含对手单次先前动作的历史（见附录中的图[6](https://arxiv.org/html/2410.01639v2#S8.F6
    "Figure 6 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")）。在MDP框架内，每个玩家的当前行动会影响下一时间步游戏的状态。
- en: 'We evaluate fine-tuning in two settings: an LLM agent learning by playing against
    a fixed-strategy Tit-for-Tat (TFT) opponent (LLM vs TFT), and an LLM agent learning
    by playing another learning LLM agent (LLM vs LLM). We chose TFT as a specific
    type of fixed-strategy opponent from the literature given its characteristics,
    i.e., being forgiving, defensive and, at the same time, interpretable (Axelrod
    & Hamilton, [1981](https://arxiv.org/html/2410.01639v2#bib.bib7); Binmore, [2005](https://arxiv.org/html/2410.01639v2#bib.bib12)).
    Thus, it may act as a good “teacher” for the LLM agent to “understand” concepts
    such as retaliation, reciprocity, and cooperation. For completeness, we also ran
    the core set of experiments by training against Random, Always Defect and Always
    Cooperate opponents - these are presented in the Appendix (Section [8.5](https://arxiv.org/html/2410.01639v2#S8.SS5
    "8.5 All fine-tuning results vs TFT, Random, AD, AC or LLM opponent ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents")). The LLM vs LLM case is a more complex scenario
    that may lead to non-stationarity due to two separate models being updated continuously,
    but which also presents great interest due to the difficulty in predicting the
    outcomes from multi-agent learning (Busoniu et al., [2008](https://arxiv.org/html/2410.01639v2#bib.bib15)).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个设置中评估了微调：一个是LLM代理与固定策略的以眼还眼（TFT）对手对战进行学习（LLM vs TFT），另一个是LLM代理与另一个学习中的LLM代理对战进行学习（LLM
    vs LLM）。我们选择TFT作为文献中提到的一种特定类型的固定策略对手，因为它的特点，即宽容、防御性，同时具有可解释性（Axelrod & Hamilton，[1981](https://arxiv.org/html/2410.01639v2#bib.bib7)；Binmore，[2005](https://arxiv.org/html/2410.01639v2#bib.bib12)）。因此，它可以作为LLM代理的良好“教师”，帮助其“理解”报复、互惠和合作等概念。为了完整性，我们还通过与随机、总是背叛和总是合作的对手进行训练，进行了核心实验集——这些实验结果见附录（[8.5节](https://arxiv.org/html/2410.01639v2#S8.SS5
    "8.5 All fine-tuning results vs TFT, Random, AD, AC or LLM opponent ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents")）。LLM vs LLM的情形则是一个更复杂的场景，可能导致非平稳性，因为两个独立的模型在不断更新，但由于多代理学习的结果难以预测，这一情形也极具研究价值（Busoniu等，[2008](https://arxiv.org/html/2410.01639v2#bib.bib15)）。
- en: The aim of this study is to enable moral decision-making capabilities in LLM
    agents. We perform fine-tuning based on a single environment - the IPD. However,
    we aim to mobilize the general decision-making elements of the model in playing
    the game, rather than allowing it to retrieve memorized responses for the Prisoner’s
    Dilemma that were present in its pre-training data. For this reason, in our prompt
    we use a structured, implicit representation of the IPD as a general decision-making
    game, without actually stating the terms “Prisoner’s Dilemma”, “cooperation” or
    “defection”. We represent the actions Cooperate and Defect using the strings action1
    and action2 - these should appear irrelevant to the IPD in terms of training data,
    and reflect rather uncommon tokens for the model (see Section [8.2](https://arxiv.org/html/2410.01639v2#S8.SS2
    "8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")
    in the Appendix for an illustration of the prompt). Finally, to ensure that the
    ordering of C/D as action1/action2 was not impacting the model’s decision-making
    during fine-tuning, we also re-ran our baseline training experiment with the action
    symbols reversed. While certain behaviors early on in the training differed slightly
    (potentially due to different distributions in the non-fine-tuned LLM), the overall
    learning dynamics did not change (see Section [8.4](https://arxiv.org/html/2410.01639v2#S8.SS4
    "8.4 Fine-tuning variation with C & D symbols reversed ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents") in the Appendix for the results).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究的目标是使大型语言模型（LLM）代理具备道德决策能力。我们在一个单一环境——囚徒困境（IPD）中进行微调。然而，我们的目标是调动模型在游戏中的一般决策元素，而不是让其检索预训练数据中关于囚徒困境的记忆响应。因此，在我们的提示中，我们使用了囚徒困境的结构化、隐性表示，将其视为一般的决策游戏，而没有明确使用“囚徒困境”、“合作”或“背叛”等术语。我们用字符串action1和action2表示合作和背叛的行为——这些字符串应该与训练数据中的囚徒困境无关，并且对于模型来说是较不常见的标记（参见附录中的[8.2节](https://arxiv.org/html/2410.01639v2#S8.SS2
    "8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")，其中有提示的示例）。最后，为了确保C/D作为action1/action2的顺序不会影响模型在微调过程中的决策，我们还通过反转行动符号重新进行了基线训练实验。尽管在训练初期某些行为略有不同（可能由于未微调的LLM中分布的差异），但整体学习动态没有发生变化（有关结果，请参见附录中的[8.4节](https://arxiv.org/html/2410.01639v2#S8.SS4
    "8.4 Fine-tuning variation with C & D symbols reversed ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")）。
- en: 3.2 Moral Fine-tuning Procedure
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 道德微调过程
- en: 'Table 1: Definitions of the types of moral rewards used in fine-tuning the
    LLM agent, from the point of view of the moral agent $M$ playing versus an opponent
    $O$ at time step $t$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：从道德代理$M$在时间步$t$与对手$O$对弈的角度，定义了用于微调LLM代理的道德奖赏类型。
- en: '| Moral Fine-tuning Type | Moral Reward Function |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 道德微调类型 | 道德奖赏函数 |'
- en: '| Game reward (selfish) | $R_{M}^{t}=\begin{cases}R^{t}_{M_{\text{game}}},&\text{if
    }a_{M}^{t}\in[C_{% \text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{otherwise}\end{cases}\$
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 游戏奖赏（自私） | $R_{M}^{t}=\begin{cases}R^{t}_{M_{\text{game}}},&\text{如果 }a_{M}^{t}\in[C_{%
    \text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{否则}\end{cases}\$ |'
- en: '| Deontological reward | $R_{M}^{t}=\begin{cases}$--$\xi,&\text{if }a_{M}^{t}=D,a_{O}^{t-1}=C\\
    0,&\text{otherwise if }a_{M}^{t}\in[C_{\text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{otherwise}\end{cases}\$
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 义务论奖赏 | $R_{M}^{t}=\begin{cases}$--$\xi,&\text{如果 }a_{M}^{t}=D,a_{O}^{t-1}=C\\
    0,&\text{如果 }a_{M}^{t}\in[C_{\text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{否则}\end{cases}\$
    |'
- en: '| Utilitarian reward | $R_{M}^{t}=\begin{cases}R_{M_{\text{game}}}^{t}+R_{O_{\text{game}}}^{t},&\text{%
    if }a_{M}^{t}\in[C_{\text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{otherwise}\end{cases}\$
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 功利奖赏 | $R_{M}^{t}=\begin{cases}R_{M_{\text{game}}}^{t}+R_{O_{\text{game}}}^{t},&\text{如果
    }a_{M}^{t}\in[C_{\text{legal}},D_{\text{legal}}]\\ R_{\text{illegal}},&\text{否则}\end{cases}\$
    |'
- en: '| Game+Deontological reward | $R_{M}^{t}=\begin{cases}R_{M_{\text{game}}}^{t}$--$\xi,&\text{if
    }a_{M}^{t}=D,a% _{O}^{t-1}=C\\ R_{M_{\text{game}}}^{t},&\text{otherwise if}a_{M}^{t}\in[C_{\text{legal}},D_{%
    \text{legal}}]\\ R_{\text{illegal}},&\text{otherwise}\end{cases}\$ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 游戏+义务论奖赏 | $R_{M}^{t}=\begin{cases}R_{M_{\text{game}}}^{t}$--$\xi,&\text{如果
    }a_{M}^{t}=D,a_{O}^{t-1}=C\\ R_{M_{\text{game}}}^{t},&\text{如果 }a_{M}^{t}\in[C_{\text{legal}},D_{%
    \text{legal}}]\\ R_{\text{illegal}},&\text{否则}\end{cases}\$ |'
- en: 'We run training in $T$ episodes: each episode begins with a random state being
    incorporated into the IPD prompt. The LLM-based agent $M$ then plays $N$ repetitions
    of the IPD game against an opponent $O$ (where $N$ is the batch size). On each
    repetition, the players’ actions from the previous time step are reflected in
    their opponent’s current state $s^{t}_{M}=(a^{t-1}_{O},a^{t-1}_{M})$. If an LLM
    player outputs an illegal move on a certain time step, this move is not used to
    update their opponent’s state, but the agent still learns from the experience.
    After $N$ games have been played, the LLM agent performs a PPO learning step update
    based on the gathered batch of experiences. This marks the end of an episode.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在$T$个回合中进行训练：每个回合开始时，随机状态会被引入到IPD提示中。基于LLM的代理$M$接着与对手$O$进行$N$次IPD游戏（其中$N$是批次大小）。在每一次重复中，玩家上一时刻的动作会反映到对手的当前状态$s^{t}_{M}=(a^{t-1}_{O},a^{t-1}_{M})$。如果LLM玩家在某个时间步输出非法动作，这个动作不会用来更新对手的状态，但代理仍会从中学习。当$N$局游戏结束后，LLM代理会基于收集的经验批次进行PPO学习步骤更新。这标志着一个回合的结束。
- en: 'In our core experiments, we test four different reward signals for moral fine-tuning
    of LLM agents (as outlined in Table [1](https://arxiv.org/html/2410.01639v2#S3.T1
    "Table 1 ‣ 3.2 Moral Fine-tuning Procedure ‣ 3 Fine-tuning Methodology ‣ Moral
    Alignment for LLM Agents")): 1) the Game reward $R^{t}_{M_{\text{game}}}$, representing
    the goals of a selfish or rational agent playing the IPD 2) a Deontological reward
    $-\xi$ for violating the moral norm “do not defect against an opponent who previously
    cooperated”, 3) Utilitarian reward, representing the collective payoff in the
    game, and 4) Game+Deontological reward which combine game payoff with a Deontological
    penalty in a multi-objective manner. In addition, we test whether a model fine-tuned
    on Game rewards is able to unlearn this selfish strategy via further fine-tuning
    with moral rewards. Therefore, we additionally fine-tune agents with: 5) Game,
    then Deontological reward (training with each reward type for half of the total
    number of episodes $T$), and 6) Game, then Utilitarian reward (again, each for
    half of the total duration of $T$ episodes). Finally, during each type of fine-tuning
    we also implement a penalty $R_{\text{illegal}}$ for generating “illegal” action
    tokens, to encourage the model to keep its answers within the permitted action
    space, as defined in the game prompt.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的核心实验中，我们测试了四种不同的奖励信号，用于对LLM代理进行道德微调（如表[1](https://arxiv.org/html/2410.01639v2#S3.T1
    "Table 1 ‣ 3.2 Moral Fine-tuning Procedure ‣ 3 Fine-tuning Methodology ‣ Moral
    Alignment for LLM Agents")所示）：1）博弈奖励$R^{t}_{M_{\text{game}}}$，表示在囚徒困境（IPD）中，利己或理性代理的目标；2）违反道德规范“不要对曾经合作过的对手背叛”时的义务论奖励$-\xi$；3）功利主义奖励，表示游戏中的集体回报；4）博弈+义务论奖励，将博弈回报与义务论惩罚以多目标方式结合。此外，我们还测试了一个模型是否能够通过在道德奖励下进一步微调，忘记其之前的利己策略。因此，我们还对代理进行了以下微调：5）博弈奖励，然后是义务论奖励（每种奖励类型训练总回合数$T$的一半）；6）博弈奖励，然后是功利主义奖励（同样，每种奖励类型训练$T$总回合数的一半）。最后，在每种类型的微调过程中，我们还实施了一个惩罚$R_{\text{illegal}}$，用于生成“非法”动作标记，鼓励模型将其回答保持在游戏提示所定义的允许动作空间内。
- en: 3.3 Implementation Details
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实施细节
- en: We use Gemma2-2b-it (Gemma Team, [2024](https://arxiv.org/html/2410.01639v2#bib.bib24))
    as our core agent model to be fine-tuned, being one of the most popular and performant
    small open-source models. The small model footprint allows us to run computationally
    feasible experiments through the use of LoRA (Hu et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib27))
    and 4-bit quantization. We use the TRL library (von Werra et al., [2020](https://arxiv.org/html/2410.01639v2#bib.bib63))
    to fine-tune the LLM with PPO. We run training for $T=1000$ episodes for each
    fine-tuning variation. In our PPO implementation, we use batch sizes of $N=3$
    and $N=5$ for LLM vs LLM and LLM vs TFT training, respectively, which strikes
    a nice balance between not running out of available CUDA memory, yet providing
    sufficient experience for stable and efficient training ⁴⁴4The code implementing
    the model fine-tuning and analysis will be made available upon acceptance. We
    use reward scaling and normalization (Engstrom et al., [2020](https://arxiv.org/html/2410.01639v2#bib.bib19)),
    as well as gradient accumulation with 4 steps, and employ LoRA (Hu et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib27))
    with rank 64, so that the total number of parameters we train is only around 5%
    of the original model. Otherwise, we keep all PPO parameters as their default
    values in the TRL package, including the optimizer’s learning rate and adaptive
    KL control (Jaques et al., [2017](https://arxiv.org/html/2410.01639v2#bib.bib30)).
    In terms of reward parameters, we set $\xi=3$ and $R_{\text{illegal}}=-6$. We
    select the tokens action1 and action2 as the only “legal” tokens in response to
    the IPD prompt ($C_{\text{legal}}=$ action1, $D_{\text{legal}}=$ action2). The
    action symbols are each encoded as two tokens in the model’s tokenizer, so during
    training we restrict the maximum output length for model generations to two tokens.
    For detail on parameter selection, please refer to Appendix [8.1](https://arxiv.org/html/2410.01639v2#S8.SS1
    "8.1 Implementation details for reproducibility ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Gemma2-2b-it（Gemma团队，[2024](https://arxiv.org/html/2410.01639v2#bib.bib24)）作为我们的核心代理模型进行微调，它是最受欢迎且性能优越的小型开源模型之一。该小型模型的占用空间使我们能够通过使用LoRA（Hu等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib27)）和4位量化来进行计算上可行的实验。我们使用TRL库（von
    Werra等人，[2020](https://arxiv.org/html/2410.01639v2#bib.bib63)）通过PPO对LLM进行微调。我们对每个微调变体进行$T=1000$轮训练。在我们的PPO实现中，对于LLM与LLM以及LLM与TFT的训练，我们分别使用批量大小$N=3$和$N=5$，这在不耗尽可用CUDA内存的同时，提供了足够的经验以实现稳定高效的训练⁴⁴4微调模型和分析的代码将在接受后公开。我们使用奖励缩放和归一化（Engstrom等人，[2020](https://arxiv.org/html/2410.01639v2#bib.bib19)），以及4步梯度累积，并采用LoRA（Hu等人，[2022](https://arxiv.org/html/2410.01639v2#bib.bib27)），秩为64，这样我们训练的参数总数仅为原始模型的约5%。除此之外，我们保持TRL包中所有PPO参数的默认值，包括优化器的学习率和自适应KL控制（Jaques等人，[2017](https://arxiv.org/html/2410.01639v2#bib.bib30)）。在奖励参数方面，我们设置$\xi=3$和$R_{\text{illegal}}=-6$。我们选择tokens
    action1和action2作为响应IPD提示的唯一“合法”tokens（$C_{\text{legal}}=$ action1，$D_{\text{legal}}=$
    action2）。这些动作符号在模型的分词器中每个都被编码为两个tokens，因此在训练过程中，我们将模型生成的最大输出长度限制为两个tokens。有关参数选择的详细信息，请参阅附录[8.1](https://arxiv.org/html/2410.01639v2#S8.SS1
    "8.1 实现细节以确保可重复性 ‣ 8 附录 ‣ LLM代理的道德对齐")。
- en: '4 Evaluating the effectiveness of fine-tuning: moral choices on the IPD'
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 评估微调效果：IPD上的道德选择
- en: 4.1 Evaluation Approach
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估方法
- en: 'First of all, we analyze the learning dynamics observed as models develop the
    ability to meet the moral goals set in their rewards (Section [4.2](https://arxiv.org/html/2410.01639v2#S4.SS2
    "4.2 Learning dynamics ‣ 4 Evaluating the effectiveness of fine-tuning: moral
    choices on the IPD ‣ Moral Alignment for LLM Agents")). We analyze learning against
    a static opponent and a learning opponent. We then assess the effectiveness of
    moral “unlearning” (Section [4.3](https://arxiv.org/html/2410.01639v2#S4.SS3 "4.3
    Learning and Unlearning the Selfish Strategy on the IPD ‣ 4 Evaluating the effectiveness
    of fine-tuning: moral choices on the IPD ‣ Moral Alignment for LLM Agents")).
    Beyond measuring success on IPD fine-tuning itself, we evaluate the generalization
    of the moral fine-tuning from one matrix game environment onto four other matrix
    games (Section [5.1](https://arxiv.org/html/2410.01639v2#S5.SS1 "5.1 Generalization
    to Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in
    Other Environments ‣ Moral Alignment for LLM Agents")): Iterated Stag Hunt, Iterated
    Chicken, Iterated Bach or Stravinsky and an Iterated Defective Coordination game.
    The payoffs and further details around these games are presented in the Appendix
    (Section [8.6](https://arxiv.org/html/2410.01639v2#S8.SS6 "8.6 Five matrix games
    used in the generalization analysis ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")).
    Finally, we evaluate the extent to which fine-tuning on the IPD alters the models’
    behavior on more general prompts, as well as the explicit IPD game (Section [5.2](https://arxiv.org/html/2410.01639v2#S5.SS2
    "5.2 Impact of Fine-tuning Beyond Matrix Games ‣ 5 Generalization to Moral Choices
    in Other Environments ‣ Moral Alignment for LLM Agents")). For each experiment,
    we report average results across five random seeds.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们分析了在模型发展出满足奖励中设定的道德目标的能力时所观察到的学习动态（见[4.2节](https://arxiv.org/html/2410.01639v2#S4.SS2
    "4.2 学习动态 ‣ 4 评估微调效果：IPD中的道德选择 ‣ LLM 代理的道德对齐")）。我们分析了对静态对手和学习型对手的学习情况。接着，我们评估了道德“去学习”（见[4.3节](https://arxiv.org/html/2410.01639v2#S4.SS3
    "4.3 在IPD上学习和去学习自私策略 ‣ 4 评估微调效果：IPD中的道德选择 ‣ LLM 代理的道德对齐")）的有效性。除了衡量IPD微调本身的成功，我们还评估了道德微调从一个矩阵游戏环境到其他四个矩阵游戏的泛化能力（见[5.1节](https://arxiv.org/html/2410.01639v2#S5.SS1
    "5.1 道德选择在其他矩阵游戏中的泛化 ‣ 5 道德选择在其他环境中的泛化 ‣ LLM 代理的道德对齐")）：迭代猎鹿游戏、迭代鸡鸡游戏、迭代巴赫或斯特拉文斯基游戏和迭代缺陷协调游戏。这些游戏的支付矩阵及更多详细信息见附录（见[8.6节](https://arxiv.org/html/2410.01639v2#S8.SS6
    "8.6 泛化分析中使用的五个矩阵游戏 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。最后，我们评估了IPD微调在更一般性提示以及显式IPD游戏中的行为变化（见[5.2节](https://arxiv.org/html/2410.01639v2#S5.SS2
    "5.2 微调对矩阵游戏以外的影响 ‣ 5 道德选择在其他环境中的泛化 ‣ LLM 代理的道德对齐")）。对于每个实验，我们报告了基于五个随机种子的平均结果。
- en: 4.2 Learning dynamics
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 学习动态
- en: a) ![Refer to caption](img/7c920cc67ee40ba844686e6a1adc5927.png) ![Refer to
    caption](img/3bd74d81fc6107c242f4bb7e9499ccf1.png) ![Refer to caption](img/805ffc635f1a98caa08239894b84d116.png)
    ![Refer to caption](img/e84b158c0ebdad440f0e8f1a313f5c27.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![参见说明](img/7c920cc67ee40ba844686e6a1adc5927.png) ![参见说明](img/3bd74d81fc6107c242f4bb7e9499ccf1.png)
    ![参见说明](img/805ffc635f1a98caa08239894b84d116.png) ![参见说明](img/e84b158c0ebdad440f0e8f1a313f5c27.png)
- en: b) ![Refer to caption](img/eddfc636b7765cf6f572ce4647603b06.png) ![Refer to
    caption](img/8b62aa2e86c547c240eb59b96763bdd2.png) ![Refer to caption](img/0f53c430be7c4db1c169b8740159bcc3.png)
    ![Refer to caption](img/8a9383064e4c7781dd2f83e58a693c39.png)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: b) ![参见说明](img/eddfc636b7765cf6f572ce4647603b06.png) ![参见说明](img/8b62aa2e86c547c240eb59b96763bdd2.png)
    ![参见说明](img/0f53c430be7c4db1c169b8740159bcc3.png) ![参见说明](img/8a9383064e4c7781dd2f83e58a693c39.png)
- en: 'Figure 2: Action types played by the LLM agent during different types of fine-tuning
    on the Iterated Prisoner’s Dilemma (IPD) game a) vs a TFT agent, and b) vs an
    LLM agent (i.e., two LLMs being fine-tuned at once). For each episode, we plot
    the actions of the LLM player $M$ given the last move of their opponent $O$.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLM 代理在不同类型的囚徒困境（IPD）游戏微调中所采取的行动类型 a) 对抗 TFT 代理，和 b) 对抗 LLM 代理（即同时微调两个 LLM）。对于每一轮，我们绘制了
    LLM 玩家 $M$ 在对手 $O$ 上一轮行动基础上采取的行动。
- en: 'In general, we find that it is possible to fine-tune the LLM agents to choose
    actions that are consistent with certain moral and/or game rewards in the IPD.
    We analyze learning dynamics over the four core types of fine-tuning in Figure
    [2](https://arxiv.org/html/2410.01639v2#S4.F2 "Figure 2 ‣ 4.2 Learning dynamics
    ‣ 4 Evaluating the effectiveness of fine-tuning: moral choices on the IPD ‣ Moral
    Alignment for LLM Agents"). Learning against a fixed-strategy opponent (panel
    a), fine-tuning on Game rewards (i.e., rewards assigned through the payoff matrix
    of the game), the agent learns a defective policy, which forms a Nash Equilibrium
    versus a TFT opponent (Axelrod & Hamilton, [1981](https://arxiv.org/html/2410.01639v2#bib.bib7)).
    In the case of Deontological fine-tuning, the agent quickly learns to avoid defecting
    against a cooperator nearly 100% of the time, thus never violating the moral norm
    encoded in the respective reward function. In practice, this agent also learns
    to prefer cooperation in general, though this was not directly encouraged by the
    Deontological norm (in terms of Deontological reward, defecting against a defector
    is just as valid as cooperating against a cooperator - see reward definition in
    Table [1](https://arxiv.org/html/2410.01639v2#S3.T1 "Table 1 ‣ 3.2 Moral Fine-tuning
    Procedure ‣ 3 Fine-tuning Methodology ‣ Moral Alignment for LLM Agents")). On
    Utilitarian fine-tuning, the agent learns to achieve mutual cooperation against
    a TFT opponent, which is expected given that this strategy offers the optimal
    way to obtain the highest collective reward on the IPD. Finally, in the case of
    fine-tuning with a multi-objective Game+Deontological reward, the agent learns
    a 50%-50% Cooperate-Defect strategy, but also learns to avoid defecting against
    a cooperator. Thus, this agent does not violate their moral norm even as they
    work to obtain high payoffs on the game itself. An analysis of moral reward obtained
    during learning is presented in the Appendix (Section [8.3](https://arxiv.org/html/2410.01639v2#S8.SS3
    "8.3 Moral reward during fine-tuning ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们发现可以对 LLM 代理进行微调，使其选择与某些道德和/或博弈奖励一致的行为，在 IPD 中取得预期效果。我们分析了图 [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "图 2 ‣ 4.2 学习动态 ‣ 4 评估微调效果：IPD 中的道德选择 ‣ LLM 代理的道德对齐")中的四种核心微调类型的学习动态。针对固定策略对手的学习（面板
    a），在博弈奖励（即通过博弈的支付矩阵分配的奖励）下进行微调，代理学习到一种背叛策略，这形成了与 TFT 对手的纳什均衡（Axelrod & Hamilton，[1981](https://arxiv.org/html/2410.01639v2#bib.bib7)）。在义务论微调的情况下，代理几乎
    100% 的时间学会避免对合作者背叛，从而始终遵循相应奖励函数中编码的道德规范。在实践中，这个代理也学会了一般性地偏好合作，尽管这一点并没有被义务论规范直接鼓励（就义务论奖励而言，对背叛者背叛和对合作者合作同样有效——请参见表
    [1](https://arxiv.org/html/2410.01639v2#S3.T1 "表 1 ‣ 3.2 道德微调程序 ‣ 3 微调方法 ‣ LLM
    代理的道德对齐") 中的奖励定义）。在功利主义微调中，代理学会在 TFT 对手面前实现相互合作，这是预期之中的，因为这种策略提供了在 IPD 中获得最高集体奖励的最佳方式。最后，在使用多目标博弈+义务论奖励进行微调时，代理学会了
    50%-50% 的合作-背叛策略，但也学会了避免对合作者背叛。因此，这个代理在努力在博弈中获得高收益的同时，也没有违反他们的道德规范。在附录中提供了学习过程中获得的道德奖励分析（见附录
    [8.3](https://arxiv.org/html/2410.01639v2#S8.SS3 "8.3 微调中的道德奖励 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。
- en: 'In addition to fine-tuning against a TFT opponent, we also implement the fine-tuning
    of two LLM agents at the same time (Figure [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "Figure 2 ‣ 4.2 Learning dynamics ‣ 4 Evaluating the effectiveness of fine-tuning:
    moral choices on the IPD ‣ Moral Alignment for LLM Agents"), panel b). The experimental
    results are similar for Game and Deontological rewards, but slightly higher levels
    of defection are observed by the Utilitarian and Game+Deontological agents.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了针对 TFT 对手进行微调外，我们还同时实现了两个 LLM 代理的微调（图 [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "图 2 ‣ 4.2 学习动态 ‣ 4 评估微调效果：IPD 中的道德选择 ‣ LLM 代理的道德对齐")，面板 b）。实验结果对于博弈和义务论奖励是相似的，但在功利主义和博弈+义务论代理中，观察到略微更高的背叛率。
- en: 4.3 Learning and Unlearning the Selfish Strategy on the IPD
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 在 IPD 上学习与遗忘自私策略
- en: a) ![Refer to caption](img/9b6f4966c03f60508a4a3670dfe86185.png) ![Refer to
    caption](img/6651c0805cd99684b173406f16fd1dab.png) b)  ![Refer to caption](img/49e104a1c6946285ba6249e235f2dbce.png)
    ![Refer to caption](img/32da7a8ece8dd7446dd26f1b86badc71.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![参见说明](img/9b6f4966c03f60508a4a3670dfe86185.png) ![参见说明](img/6651c0805cd99684b173406f16fd1dab.png)
    b) ![参见说明](img/49e104a1c6946285ba6249e235f2dbce.png) ![参见说明](img/32da7a8ece8dd7446dd26f1b86badc71.png)
- en: 'Figure 3: “Unlearning” experiments, where the reward function changes from
    the IPD Game payoffs to a moral intrinsic reward (Deontological or Utilitarian)
    at episode 500\. Action types played by the LLM agent during different types of
    fine-tuning on the Iterated Prisoner’s Dilemma (IPD) game a) vs a TFT agent, and
    b) vs an LLM agent (i.e., two LLMs being fine-tuned at once). For each episode,
    we plot the actions of the LLM player $M$ given the last move of their opponent
    $O$.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：“反学习”实验，在这些实验中，奖励函数从囚徒困境游戏（IPD）的收益转变为道德内在奖励（义务论或功利主义）在第 500 轮进行变化。LLM 代理在不同类型的细化调优过程中，所采取的行动类型：a）与
    TFT 代理对战，b）与 LLM 代理对战（即，两个 LLM 同时进行细化调优）。对于每一轮，我们绘制了 LLM 玩家 $M$ 在其对手 $O$ 做出最后一次行动后的行动。
- en: 'In addition to the moral fine-tuning on a single type of reward, we also evaluate
    the extent to which fine-tuning with intrinsic moral rewards can allow for an
    agent to unlearn a previously developed selfish strategy on the game. As shown
    in Figure [3](https://arxiv.org/html/2410.01639v2#S4.F3 "Figure 3 ‣ 4.3 Learning
    and Unlearning the Selfish Strategy on the IPD ‣ 4 Evaluating the effectiveness
    of fine-tuning: moral choices on the IPD ‣ Moral Alignment for LLM Agents"), we
    find that fine-tuning with purely prosocial (i.e., Deontological and Utilitarian)
    moral rewards on the second half of training allows the LLM agents to unlearn
    the selfish strategy to some extent (panel a), even in the case of two LLM agents
    being trained against one another (panel b). Given the shorter moral fine-tuning
    period on any one reward type (only 500 episodes vs 1000 in the core experiments),
    the training does not converge to levels of cooperation as high as in the purely
    prosocial fine-tuning (Figure [2](https://arxiv.org/html/2410.01639v2#S4.F2 "Figure
    2 ‣ 4.2 Learning dynamics ‣ 4 Evaluating the effectiveness of fine-tuning: moral
    choices on the IPD ‣ Moral Alignment for LLM Agents")). Nevertheless, as we discuss
    in Section [5](https://arxiv.org/html/2410.01639v2#S5 "5 Generalization to Moral
    Choices in Other Environments ‣ Moral Alignment for LLM Agents") below, at test
    time the agents based on “unlearned” models play similarly to those fine-tuned
    purely on the prosocial moral rewards (see Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4
    "Figure 4 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对单一奖励类型进行道德细化调优外，我们还评估了通过内在道德奖励的细化调优，是否能使代理“反学习”先前所开发的自私策略。正如图 [3](https://arxiv.org/html/2410.01639v2#S4.F3
    "图 3 ‣ 4.3 在 IPD 上学习与反学习自私策略 ‣ 4 评估细化调优效果：IPD 上的道德选择 ‣ LLM 代理的道德对齐") 所示，我们发现，在训练的后半段，使用纯粹的亲社会（即义务论和功利主义）道德奖励进行细化调优，可以在一定程度上使
    LLM 代理反学习自私策略（面板 a），即使是在两个 LLM 代理互相对战的情况下（面板 b）。由于任何一个奖励类型的道德细化调优周期较短（仅 500 轮，而核心实验为
    1000 轮），因此训练并未收敛到与纯亲社会细化调优相同的高合作水平（见图 [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "图 2 ‣ 4.2 学习动态 ‣ 4 评估细化调优效果：IPD 上的道德选择 ‣ LLM 代理的道德对齐")）。尽管如此，正如我们在下文的 [5](https://arxiv.org/html/2410.01639v2#S5
    "5 道德选择在其他环境中的泛化 ‣ LLM 代理的道德对齐") 章节中讨论的那样，在测试时，基于“反学习”模型的代理表现与那些仅通过亲社会道德奖励细化调优的代理类似（参见图
    [4](https://arxiv.org/html/2410.01639v2#S5.F4 "图 4 ‣ 5.1 道德选择在其他矩阵博弈中的泛化 ‣ 5 道德选择在其他环境中的泛化
    ‣ LLM 代理的道德对齐")）。
- en: 5 Generalization to Moral Choices in Other Environments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 道德选择在其他环境中的泛化
- en: 'After fine-tuning the models with moral reward, we evaluate each one through
    10 episodes: each episode begins with a randomly generated state and proceeds
    through 5 interaction steps. We average the results across the 5 runs of each
    fine-tuned model. In this section, we present evaluations of models which were
    fine-tuned versus a static (i.e., TFT) opponent. The results for models trained
    against another LLM show similar patterns - these are reported in the Appendix
    (Section [8.7](https://arxiv.org/html/2410.01639v2#S8.SS7 "8.7 Analysis of generalization
    for models fine-tuned against another LLM ‣ 8 Appendix ‣ Moral Alignment for LLM
    Agents")).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用道德奖励对模型进行细化调优后，我们通过 10 轮进行评估：每一轮开始时从随机生成的状态开始，并进行 5 步交互。我们对每个细化调优模型的 5 次实验结果取平均值。在这一部分，我们展示了与静态（即
    TFT）对手对战时细化调优模型的评估结果。与另一个 LLM 对战时训练出的模型结果呈现出相似的模式——这些结果在附录中有报告（见附录 [8.7](https://arxiv.org/html/2410.01639v2#S8.SS7
    "8.7 对抗另一个 LLM 进行细化调优的模型泛化分析 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。
- en: 5.1 Generalization to Moral Choices in Other Matrix Games
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 道德选择在其他矩阵博弈中的泛化
- en: 'We are interested in analyzing the generalization of moral strategies developed
    during fine-tuning from the IPD to other matrix game environments. To ensure that
    we evaluate the model’s response to the semantics of the tokens and payoffs in
    the prompt, rather than evaluating memorization of the particular training action
    tokens, we run this evaluation using a new pair of action tokens: action3=Cooperate,
    action4=Defect.⁵⁵5We note that evaluations using the same tokens as during training
    showed the same pattern (see Figure [20](https://arxiv.org/html/2410.01639v2#S8.F20
    "Figure 20 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents") in the Appendix). However, swapping
    the meaning of the training tokens during testing altered the model’s behavior
    (see Figure [21](https://arxiv.org/html/2410.01639v2#S8.F21 "Figure 21 ‣ 8.8 Analysis
    of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents") in the Appendix). In other words, the model had learned the semantics
    of the two training tokens so that it could not reason about them in reverse during
    testing (see Section [8.9](https://arxiv.org/html/2410.01639v2#S8.SS9 "8.9 Analysis
    of generalization across five games - using new and original action tokens in
    the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") in the Appendix
    for the full results).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有兴趣分析从IPD到其他矩阵游戏环境中的微调过程中发展出的道德策略的泛化性。为了确保我们评估的是模型对提示中令牌和支付的语义响应，而不是评估对特定训练行动令牌的记忆，我们使用一对新的行动令牌进行此评估：action3=合作，action4=背叛。⁵⁵5我们注意到，使用与训练过程中相同的令牌进行评估时，表现出了相同的模式（参见附录中的图[20](https://arxiv.org/html/2410.01639v2#S8.F20
    "Figure 20 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents")）。然而，在测试时交换训练令牌的意义改变了模型的行为（参见附录中的图[21](https://arxiv.org/html/2410.01639v2#S8.F21
    "Figure 21 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents")）。换句话说，模型已经学习了这两个训练令牌的语义，以至于它无法在测试时反向推理它们（有关完整结果，请参见附录中的第[8.9](https://arxiv.org/html/2410.01639v2#S8.SS9
    "8.9 Analysis of generalization across five games - using new and original action
    tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")节）。
- en: 'In Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization
    to Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in
    Other Environments ‣ Moral Alignment for LLM Agents"), we analyze the extent to
    which the moral strategies learned while fine-tuning on the IPD game generalize
    to other matrix games with a similar format but a different set of equilibria:
    the Iterated Stag Hunt, Iterated Chicken, Iterated Bach or Stravinsky and an Iterated
    Defective Coordination game. We are particularly interested in the extent to which
    actions taken according to the two core moral frameworks (i.e., Deontological
    and Utilitarian morality) can be consistently observed across the games by each
    agent type. For example, with regards to the Utilitarian goal (i.e., maximizing
    collective payoff), unconditional cooperation may not be the best strategy on
    the Iterated Bach or Stravinsky or the Iterated Defective Coordination game. (For
    a further discussion of the games, please refer to the Appendix, Section [8.6](https://arxiv.org/html/2410.01639v2#S8.SS6
    "8.6 Five matrix games used in the generalization analysis ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents").) We additionally seek to cross-compare how the actions
    of agents trained on one type of moral value align to those based on other values.
    Therefore, we conduct evaluations in terms of moral regret, defined as the difference
    between the maximum possible moral reward that could have been attained on a game
    and the moral reward that was actually received by the agent. During this test
    phase, we evaluate each fine-tuned model playing the matrix games against a Random
    opponent - this allows us to observe the agent responding to a variety of states.
    To aid interpretation, we also analyze the types of action-state combinations
    played by each agent in each case (see Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5
    "Figure 5 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization
    to Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in
    Other Environments ‣ Moral Alignment for LLM Agents") 中，我们分析了在 IPD 游戏上微调时学到的道德策略在其他具有相似格式但不同均衡集合的矩阵游戏中的泛化程度：如迭代公鹿狩猎游戏、迭代鸡鸡游戏、迭代巴赫或斯特拉文斯基游戏以及迭代缺陷协调游戏。我们特别关注通过两种核心道德框架（即义务论和功利主义道德）做出的行动，是否能在各个游戏中始终如一地观察到每种代理类型的表现。例如，就功利主义目标（即最大化集体回报）而言，在迭代巴赫或斯特拉文斯基游戏或迭代缺陷协调游戏中，无条件合作可能不是最佳策略。（有关游戏的进一步讨论，请参阅附录
    [8.6](https://arxiv.org/html/2410.01639v2#S8.SS6 "8.6 Five matrix games used in
    the generalization analysis ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")。）我们还寻求跨比较基于一种道德价值训练的代理行为与基于其他价值的代理行为之间的差异。因此，我们通过道德后悔来进行评估，道德后悔定义为在某个游戏中可以获得的最大道德奖励与代理实际获得的道德奖励之间的差异。在此测试阶段，我们评估每个微调模型与随机对手进行矩阵游戏——这使我们能够观察代理对各种状态的反应。为了帮助解释，我们还分析了每个代理在每种情况下执行的行动状态组合（见图
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents")）。
- en: a) ![Refer to caption](img/62e9fbf401361b4f4ab099f44d5c97a2.png) b) ![Refer
    to caption](img/3fbade00db153d802f6487e552b64e74.png)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![请参阅图例](img/62e9fbf401361b4f4ab099f44d5c97a2.png) b) ![请参阅图例](img/3fbade00db153d802f6487e552b64e74.png)
- en: 'Figure 4: Analysis of generalization of the fine-tuned agents’ learned morality
    to other matrix game environments, using new action tokens at test time. We visualize
    a) Deontological and b) Utilitarian regret (normalized across games) for all models,
    averaging values over 50 test games and five runs (+- 95%CI).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：分析经过微调的代理在其他矩阵游戏环境中的道德泛化能力，使用新的行动标记进行测试。我们可视化了所有模型在 50 次测试游戏和五次运行（+ - 95%
    置信区间）中，道德上的义务论和功利主义后悔（跨游戏归一化）的结果。
- en: '![Refer to caption](img/b403e54a858c454d4ac2da942e2cc3d4.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅图例](img/b403e54a858c454d4ac2da942e2cc3d4.png)'
- en: 'Figure 5: Analysis of action choices at test time on the five iterated matrix
    games, using new action tokens at test time. We visualize results by the type
    of fine-tuning reward used.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：分析在五个迭代矩阵游戏中，使用新的行动标记进行测试时的行动选择。我们根据使用的微调奖励类型可视化结果。
- en: In terms of moral regret with respect to Deontological norms (Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4
    "Figure 4 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents"), panel
    a), we find that all fine-tuned models are able to reasonably translate the moral
    strategy learned from the IPD to other matrix games. For any one model, performance
    in terms of reward (Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure
    4 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")) and
    action choices (Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5
    ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")) is
    generally similar across the five games. Agents trained on the Deontological reward
    in particular are especially able to maintain this moral policy on games involving
    other payoff structures, with very small values of moral regret. An analysis of
    their action choices (Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure
    5 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")) shows
    that while Deontological models mostly defect after observing a defective state,
    they are almost always meeting the norm of never defecting against a cooperator.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 关于道义悔恼与义务论规范的关系（图 [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣
    5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization to
    Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")，面板a），我们发现所有经过微调的模型都能够合理地将从IPD中学到的道德策略转移到其他矩阵博弈中。对于任何一个模型，在奖励（图
    [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents")）和行动选择（图 [5](https://arxiv.org/html/2410.01639v2#S5.F5
    "Figure 5 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")）方面的表现通常在五个博弈中是相似的。特别是那些在义务论奖励下训练的智能体，能够在涉及其他回报结构的博弈中保持这种道德策略，且道德悔恼值非常小。对其行动选择的分析（图
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents")）显示，尽管义务论模型在观察到对方违背合作后大多数情况下会选择背叛，但它们几乎总是遵循从不背叛合作方这一规范。
- en: 'In terms of moral regret with respect to the Utilitarian framework, (Figure
    [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents"), panel b - normalized to account
    for the different maximum values of collective payoff across the five games),
    we see that generalization differs across the four new games. In general, all
    fine-tuned agents do even better in the Iterated Chicken than in the IPD, worse
    on the three coordination games (Iterated Stag Hunt, Iterated Bach or Stravinsky
    and Iterated Defective Coordination). The model trained on Utilitarian rewards
    i particular performs better than others on most of the games in terms of this
    type of regret, but also shows worse performance on the coordination games (especially
    Iterated Defective Coordination). Analyzing the actions chosen (Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5
    "Figure 5 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")) provides
    an explanation: the Utilitarian model essentially always chooses to cooperate,
    regardless of its opponent’s last move or the game’s payoff structure, which is
    detrimental in terms of Utilitarian outcomes on the games where defection was
    required to achieve a Utilitarian goal (for detailed descriptions of the games,
    see Appendix, Section [8.6](https://arxiv.org/html/2410.01639v2#S8.SS6 "8.6 Five
    matrix games used in the generalization analysis ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")). The poorer generalization of the Utilitarian policy may be
    explained by the fact that this model was fine-tuned on the IPD, where mutual
    cooperation is the optimal behavior, hence it learned a policy biased towards
    cooperation irrespective of its intrinsic moral goal. Alternatively, this agent
    might simply be unable to consider the temporal dimension of the interaction,
    i.e., its opponent’s previous move, when making a decision. Further analyses interpreting
    models’ responses to states in non-matrix game environments are presented in Section
    [5.2](https://arxiv.org/html/2410.01639v2#S5.SS2 "5.2 Impact of Fine-tuning Beyond
    Matrix Games ‣ 5 Generalization to Moral Choices in Other Environments ‣ Moral
    Alignment for LLM Agents") and in the Appendix (Section [8.8](https://arxiv.org/html/2410.01639v2#S8.SS8
    "8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣
    Moral Alignment for LLM Agents")).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在效用主义框架下的道德遗憾方面，（图 [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣
    5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization to
    Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")，面板 b -
    已归一化以考虑五个游戏中集体收益的不同最大值），我们看到四个新游戏的泛化表现有所不同。通常，所有微调后的代理在反复“鸡游戏”（Iterated Chicken）中的表现比在囚徒困境（IPD）中更好，而在三个协调性游戏（反复的猎鹿游戏、反复的巴赫或斯特拉文斯基游戏和反复的缺陷协调游戏）中的表现较差。特别地，在效用主义奖励上训练的模型在大多数游戏中表现得比其他模型更好，但在协调性游戏（特别是反复的缺陷协调游戏）中表现较差。分析所选行动（图
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents")）提供了一个解释：效用主义模型本质上总是选择合作，而不管对手的上一步动作或游戏的收益结构如何，这在效用主义目标要求背叛以实现效用主义结果的游戏中是有害的（有关游戏的详细描述，请参见附录第
    [8.6](https://arxiv.org/html/2410.01639v2#S8.SS6 "8.6 Five matrix games used in
    the generalization analysis ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") 节）。效用主义策略的较差泛化可能是因为该模型是在囚徒困境（IPD）中进行微调的，而在IPD中，互利合作是最优行为，因此它学到了一种偏向合作的策略，而不管其内在的道德目标。或者，这个代理可能根本无法在做决策时考虑交互的时间维度，即无法考虑对手的上一步动作。进一步分析模型在非矩阵游戏环境中对状态的反应，请参见第
    [5.2](https://arxiv.org/html/2410.01639v2#S5.SS2 "5.2 Impact of Fine-tuning Beyond
    Matrix Games ‣ 5 Generalization to Moral Choices in Other Environments ‣ Moral
    Alignment for LLM Agents") 节以及附录中的第 [8.8](https://arxiv.org/html/2410.01639v2#S8.SS8
    "8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣
    Moral Alignment for LLM Agents") 节。
- en: In terms of cross-benefit from one value to another, we observe that the Utilitarian
    model appears to be just as good at minimizing regret with respect to Deontological
    ethics (Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization
    to Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in
    Other Environments ‣ Moral Alignment for LLM Agents")) as the Deontological model
    - this can be explained by the fact that Utilitarian models display fully cooperative
    behavior at test time (Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure
    5 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")), which
    is a safe strategy in terms of avoiding the Deontological punishment under our
    definition of that norm. Models fine-tuned on reward types other than purely Deontological
    or Utilitarian ethics display larger values of moral regret with regard to the
    two values of interest, as expected given that they develop less cooperative policies
    (Figure [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5 ‣ 5.1 Generalization
    to Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in
    Other Environments ‣ Moral Alignment for LLM Agents")).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就从一个价值到另一个价值的跨效益而言，我们观察到，功利主义模型在最小化相对于义务伦理学的遗憾方面似乎与义务伦理学模型一样有效（图 [4](https://arxiv.org/html/2410.01639v2#S5.F4
    "图 4 ‣ 5.1 其他矩阵博弈中的道德选择推广 ‣ 5 其他环境中的道德选择推广 ‣ LLM 代理的道德对齐")），这可以通过以下事实来解释：功利主义模型在测试时表现出完全合作的行为（图
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "图 5 ‣ 5.1 其他矩阵博弈中的道德选择推广 ‣ 5 其他环境中的道德选择推广
    ‣ LLM 代理的道德对齐")），这是避免在我们定义的义务伦理学惩罚下的安全策略。微调过的模型如果奖励类型既非纯粹的义务伦理学，也非功利主义伦理学，则会表现出较大的道德遗憾值，这与它们发展出较少合作的策略一致（图
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "图 5 ‣ 5.1 其他矩阵博弈中的道德选择推广 ‣ 5 其他环境中的道德选择推广
    ‣ LLM 代理的道德对齐")）。
- en: 5.2 Impact of Fine-tuning Beyond Matrix Games
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 微调对矩阵博弈以外的影响
- en: Given the fine-tuning process based on rewarding particular action tokens in
    certain states, it is important to understand the extent to which fine-tuning
    on a matrix game might have made the models learn a certain “meaning” of the action
    tokens more generally. To test this, we presented the models with three unrelated
    prompts involving a “call to action” of a similar format (and using the same action
    tokens), as well as an explicit IPD prompt, but all without a payoff matrix being
    provided. Our results show that, especially when responding to prompts mentioning
    a “game” or involving a previous action of another agent (i.e., a state), the
    LLM agents based on fine-tuned modes are likely to choose actions in a similar
    pattern to that seen on the IPD and in a way that is consistent with their learned
    moral values. For detailed results, see Section [8.8](https://arxiv.org/html/2410.01639v2#S8.SS8
    "8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣
    Moral Alignment for LLM Agents") of the Appendix.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于微调过程是基于在特定状态下奖励某些行为标记，因此重要的是要理解，基于矩阵博弈的微调过程是否使模型更普遍地学习了某些行为标记的“意义”。为了验证这一点，我们向模型展示了三个不相关的提示，这些提示涉及相似格式的“行动号召”（并使用相同的行为标记），以及一个明确的IPD提示，但都没有提供收益矩阵。我们的结果显示，尤其是在响应提到“游戏”或涉及另一个代理之前的行为（即一个状态）的提示时，基于微调模式的LLM代理更可能选择与IPD中看到的模式相似的行动，并且与它们所学习的道德价值观一致。详细结果请参见附录的
    [8.8](https://arxiv.org/html/2410.01639v2#S8.SS8 "8.8 微调对矩阵博弈以外的影响分析 ‣ 8 附录 ‣
    LLM 代理的道德对齐") 部分。
- en: 6 Discussion
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论
- en: In this work, we present a method for fine-tuning LLM agents to adhere to a
    specific moral strategy in matrix games by employing RL with intrinsic rewards.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种通过采用具有内在奖励的强化学习（RL）方法，来微调大型语言模型（LLM）代理，使其遵循特定道德策略的矩阵博弈方法。
- en: The two different moral payoff structures used in this study have different
    advantages and disadvantages in terms of implementation in real-world systems.
    Our definition of the consequentialist (Utilitarian) moral agents is a function
    of the payoffs given by the environment to both players. Thus, its implementation
    in practice requires that the LLM agent will has observability of the rewards
    received by both players from the environment on a given time step (or a reliable
    estimate). For Deontological morality, on the other hand, a norm may be easier
    to define in any environment without direct access to game payoffs or the opponent’s
    current actions. The definition of the Deontological norm used in this study (“do
    not defect against a cooperator”) only required a memory of one previous move
    of an opponent. This makes such a norm-base reward function easy to implement
    in cases in which the developer of an LLM agent only has access to their own agent’s
    observations of the environment and not anyone else’s. In future work, the intrinsic
    rewards approach can be applied to modeling a variety of other moral values.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中使用的两种不同的道德回报结构在现实世界系统中的实现有不同的优缺点。我们对结果主义（功利主义）道德代理的定义是由环境提供给两个玩家的回报的函数。因此，其在实践中的实现要求LLM代理能够观察到在给定时间步长内，两个玩家从环境中获得的回报（或者是一个可靠的估计）。另一方面，对于义务论道德，规范可能在任何环境中都更容易定义，而无需直接访问游戏回报或对手的当前行为。本研究中使用的义务论规范定义（“不对合作者背叛”）仅需要记住对手的一个前一步动作。这使得这种基于规范的奖励函数在LLM代理的开发者仅能访问自己代理的环境观察，而无法访问其他人数据的情况下容易实现。在未来的工作中，内在奖励方法可以应用于建模各种其他道德价值观。
- en: An extension of this method to other environments would be of great interest,
    including fine-tuning agents with other payoff structures, more complex games
    or longer history lengths (for example, to aid the development of continually-learning
    LLM agents in practice), as well as text-based scenarios that tap into a variety
    of moral values. Furthermore, the method of intrinsic rewards could also be applied
    in a multi-objective manner to address the issue of pluralistic alignment (Sorensen
    et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib55)) - in particular,
    a single agent could be trained with a combination of rewards representing different
    moral values. This may provide a promising direction for building agents that
    are able to satisfy the moral preferences of a wide range of individuals, which
    currently remains an open problem in alignment (Anwar et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib6);
    Ji et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib31)). Finally, agents
    trained via intrinsic rewards as proposed in this study could also form the basis
    for a Constitutional AI architecture composed of artificial agents characterized
    by different moral frameworks (Bai et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib8)).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 将这种方法扩展到其他环境将具有重要意义，包括使用其他回报结构来微调代理、更加复杂的游戏或更长的历史长度（例如，帮助实际中不断学习的大型语言模型代理的开发），以及基于文本的场景，涵盖多种道德价值观。此外，内在奖励的方法还可以以多目标的方式应用，以解决多元对齐的问题（Sorensen
    et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib55)）——特别是，一个单一的代理可以通过结合表示不同道德价值观的奖励进行训练。这可能为构建能够满足广泛个体道德偏好的代理提供有前景的方向，而这在对齐领域仍然是一个悬而未决的问题（Anwar
    et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib6); Ji et al., [2024](https://arxiv.org/html/2410.01639v2#bib.bib31)）。最后，按照本研究提出的内在奖励训练的代理也可以为一个由不同道德框架的人工代理组成的宪法性人工智能架构提供基础（Bai
    et al., [2022](https://arxiv.org/html/2410.01639v2#bib.bib8)）。
- en: 7 Conclusion
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper we have demonstrated that fine-tuning with intrinsic rewards is
    a promising general solution for aligning LLM agents to human moral values. We
    have evaluated the approach by quantifying moral rewards for agents in terms of
    actions and consequences on a matrix social dilemma game, and we have shown that
    both unlearning of undesirable behaviors and generalization to other environments
    are possible. We have identified promising future directions in using this methodology
    for advancing LLM agent alignment, and we hope that other researchers will be
    able to build upon the ideas presented in this work.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了使用内在奖励进行微调是一种有前景的通用解决方案，可以使大型语言模型（LLM）代理与人类道德价值观对齐。我们通过量化代理在矩阵社交困境游戏中的行为和后果来评估这种方法，并表明，既可以不学习不良行为，也可以将这种方法推广到其他环境。我们已经确定了在使用这种方法推动LLM代理对齐方面的有希望的未来方向，并希望其他研究人员能够在本工作中提出的思想基础上进行拓展。
- en: References
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abdulhai et al. (2023) Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy,
    Daria Valter, John Canny, and Natasha Jaques. Moral foundations of large language
    models. In *Proceedings of the AAAI 2023 Workshop on Representation Learning for
    Responsible Human-Centric AI (R2HCAI’23)*, 2023.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abdulhai et al. (2023) Marwa Abdulhai, Gregory Serapio-Garcia, Clément Crepy,
    Daria Valter, John Canny 和 Natasha Jaques。大型语言模型的道德基础。载于*2023年AAAI负责任以人为本的人工智能表征学习研讨会（R2HCAI'23）*，2023年。
- en: Akata et al. (2023) Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge, and Eric Schulz. Playing repeated games with large language models.
    arXiv Preprint. arXiv:2305.16867, 2023.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Akata et al. (2023) Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh,
    Matthias Bethge 和 Eric Schulz。与大型语言模型进行重复博弈。arXiv预印本。arXiv:2305.16867，2023年。
- en: Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman, and Dan Mané. Concrete problems in AI safety. arXiv preprint arXiv:1606.06565,
    2016.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amodei et al. (2016) Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman 和 Dan Mané。人工智能安全中的具体问题。arXiv预印本 arXiv:1606.06565，2016年。
- en: Anastassacos et al. (2020) Nicolas Anastassacos, Stephen Hailes, and Mirco Musolesi.
    Partner selection for the emergence of cooperation in multi-agent systems using
    reinforcement learning. In *Proceedings of the 34th AAAI Conference on Artificial
    Intelligence (AAAI’20)*, 2020.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anastassacos et al. (2020) Nicolas Anastassacos, Stephen Hailes 和 Mirco Musolesi。使用强化学习进行多智能体系统中合作出现的伙伴选择。载于*2020年第34届AAAI人工智能大会（AAAI'20）论文集*，2020年。
- en: 'Anthropic (2024) Anthropic. The Claude 3 model family: Opus, Sonnet, Haiku,
    2024. URL [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf).'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024) Anthropic. Claude 3模型家族：Opus，Sonnet，Haiku，2024年。网址 [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)。
- en: Anwar et al. (2024) Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka,
    Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver
    Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose
    Hernandez-Orallo, Lewis Hammond, Eric J Bigelow, Alexander Pan, Lauro Langosco,
    Tomasz Korbak, Heidi Chenyu Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia,
    Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov,
    Christian Schroeder de Witt, Sumeet Ramesh Motwani, Yoshua Bengio, Danqi Chen,
    Philip Torr, Samuel Albanie, Tegan Maharaj, Jakob Nicolaus Foerster, Florian Tramèr,
    He He, Atoosa Kasirzadeh, Yejin Choi, and David Krueger. Foundational challenges
    in assuring alignment and safety of large language models. *Transactions on Machine
    Learning Research*, 2024.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anwar et al. (2024) Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka,
    Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver
    Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario Günther, Anton Korinek, Jose
    Hernandez-Orallo, Lewis Hammond, Eric J Bigelow, Alexander Pan, Lauro Langosco,
    Tomasz Korbak, Heidi Chenyu Zhang, Ruiqi Zhong, Seán Ó hÉigeartaigh, Gabriel Recchia,
    Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov,
    Christian Schroeder de Witt, Sumeet Ramesh Motwani, Yoshua Bengio, Danqi Chen,
    Philip Torr, Samuel Albanie, Tegan Maharaj, Jakob Nicolaus Foerster, Florian Tramèr,
    He He, Atoosa Kasirzadeh, Yejin Choi 和 David Krueger。确保大型语言模型对齐和安全性的基础性挑战。*机器学习研究期刊*，2024年。
- en: Axelrod & Hamilton (1981) Robert Axelrod and William D. Hamilton. The evolution
    of cooperation. *Science*, 211(4489):1390–1396, 1981.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Axelrod & Hamilton (1981) Robert Axelrod 和 William D. Hamilton。合作的演化。*科学*，211(4489):1390–1396，1981年。
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional
    AI: Harmlessness from AI feedback. arXiv Preprint arXiv:2212.08073, 2022.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson
    Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
    Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep
    Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller,
    Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt,
    Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma,
    Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer
    El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown 和 Jared Kaplan. 《宪法人工智能：来自人工智能反馈的无害性》。arXiv
    预印本 arXiv:2212.08073，2022年。
- en: Bai et al. (2023) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback. *Transactions
    on Machine Learning Research*, 2023.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2023) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann 和 Jared Kaplan. 《通过人类反馈强化学习训练一个有用且无害的助手》。*Transactions
    on Machine Learning Research*，2023年。
- en: Bentham (1780) Jeremy Bentham. *An Introduction to the Principles of Morals
    and Legislation.* Clarendon Press, 1780.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bentham (1780) Jeremy Bentham. *道德与立法原理导论*。Clarendon Press，1780年。
- en: 'Beurer-Kellner et al. (2024) Luca Beurer-Kellner, Marc Fischer, and Martin
    Vechev. Guiding LLMs the right way: Fast, non-invasive constrained generation.
    arXiv Preprint. arXiv 2403.06988, 2024.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beurer-Kellner 等人 (2024) Luca Beurer-Kellner, Marc Fischer 和 Martin Vechev.
    《正确引导大语言模型：快速、非侵入性的约束生成》。arXiv 预印本 arXiv 2403.06988，2024年。
- en: Binmore (2005) Ken Binmore. *Natural Justice*. Oxford University Press, 2005.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binmore (2005) Ken Binmore. *自然正义*。Oxford University Press，2005年。
- en: 'Bruns (2015) Bryan Bruns. Names for Games: Locating 2 × 2 Games. *Games*, 6(4):495–520,
    2015.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bruns (2015) Bryan Bruns. 《游戏的名称：定位 2 × 2 游戏》。*Games*，6(4)：495–520，2015年。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of Artificial
    General Intelligence: Early experiments with GPT-4. arXiv Preprint. arXiv 2303.12712,
    2023.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人 (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro 和 Yi Zhang. 《人工通用智能的火花：与 GPT-4
    的早期实验》。arXiv 预印本 arXiv 2303.12712，2023年。
- en: Busoniu et al. (2008) Lucian Busoniu, Robert Babuska, and Bart De Schutter.
    A comprehensive survey of multiagent reinforcement learning. *IEEE Transactions
    on Systems, Man, and Cybernetics, Part C (Applications and Reviews)*, 38(2):156–172,
    2008.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Busoniu 等人 (2008) Lucian Busoniu, Robert Babuska 和 Bart De Schutter. 《多智能体强化学习的综合调查》。*IEEE
    Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)*，38(2)：156–172，2008年。
- en: Casper et al. (2023) Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl
    Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman, Tomasz Korbak, David
    Lindner, Pedro Freire, Tony Wang, Samuel Marks, Charbel-Raphaël Ségerie, Micah
    Carroll, Andi Peng, Phillip J.K. Christoffersen, Mehul Damani, Stewart Slocum,
    Usman Anwar, Anand Siththaranjan, Max Nadeau, Eric J. Michaud, Jacob Pfau, Dmitrii
    Krasheninnikov, Xin Chen, Lauro Langosco di Langosco, Peter Hase, Erdem Biyik,
    Anca D. Dragan, David Krueger, Dorsa Sadigh, and Dylan Hadfield-Menell. Open problems
    and fundamental limitations of Reinforcement Learning from Human Feedback. *Transactions
    on Machine Learning Research*, 2023.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡斯帕等人（2023）Stephen Casper、Xander Davies、Claudia Shi、Thomas Krendl Gilbert、Jérémy
    Scheurer、Javier Rando、Rachel Freedman、Tomasz Korbak、David Lindner、Pedro Freire、Tony
    Wang、Samuel Marks、Charbel-Raphaël Ségerie、Micah Carroll、Andi Peng、Phillip J.K.
    Christoffersen、Mehul Damani、Stewart Slocum、Usman Anwar、Anand Siththaranjan、Max
    Nadeau、Eric J. Michaud、Jacob Pfau、Dmitrii Krasheninnikov、Xin Chen、Lauro Langosco
    di Langosco、Peter Hase、Erdem Biyik、Anca D. Dragan、David Krueger、Dorsa Sadigh 和
    Dylan Hadfield-Menell. 强化学习中的开放问题与基本限制——来自人类反馈的强化学习. *机器学习研究杂志*, 2023.
- en: 'Christian (2020) Brian Christian. *The Alignment Problem: Machine Learning
    and Human Values*. WW Norton & Company, 2020.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克里斯蒂安（2020）Brian Christian. *对齐问题：机器学习与人类价值观*. WW Norton & Company, 2020.
- en: 'Duan et al. (2024) Jinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura,
    Lichao Sun, Elias Stengel-Eskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. GTBench:
    Uncovering the strategic reasoning limitations of llms via game-theoretic evaluations.
    arXiv Preprint. arXiv 2402.12348, 2024.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 段等人（2024）Jinhao Duan、Renming Zhang、James Diffenderfer、Bhavya Kailkhura、Lichao
    Sun、Elias Stengel-Eskin、Mohit Bansal、Tianlong Chen 和 Kaidi Xu. GTBench：通过博弈论评估揭示大规模语言模型的战略推理局限性.
    arXiv 预印本. arXiv 2402.12348, 2024.
- en: 'Engstrom et al. (2020) Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris
    Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. Implementation matters
    in Deep RL: A case study on PPO and TRPO. In *In Proceedings of the 8th International
    Conference on Learning Representations (ICLR’20)*, 2020.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恩格斯特罗姆等人（2020）Logan Engstrom、Andrew Ilyas、Shibani Santurkar、Dimitris Tsipras、Firdaus
    Janoos、Larry Rudolph 和 Aleksander Madry. 深度强化学习中的实现问题：关于 PPO 和 TRPO 的案例研究. 收录于
    *第8届国际学习表示会议（ICLR’20）论文集*, 2020.
- en: Fan et al. (2024) Caoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large
    language models serve as rational players in game theory? A systematic analysis.
    In *Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI’24)*,
    2024.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 范等人（2024）Caoyun Fan、Jindou Chen、Yaohui Jin 和 Hao He. 大型语言模型能否作为博弈论中的理性玩家？一个系统性的分析.
    收录于 *第38届人工智能AAA大会论文集（AAAI’24）*, 2024.
- en: Gabriel et al. (2024) Iason Gabriel, Arianna Manzini, Geoff Keeling, Lisa Anne
    Hendricks, Verena Rieser, Hasan Iqbal, Nenad Tomašev, Ira Ktena, Zachary Kenton,
    Mikel Rodriguez, Seliem El-Sayed, Sasha Brown, Canfer Akbulut, Andrew Trask, Edward
    Hughes, A. Stevie Bergman, Renee Shelby, Nahema Marchal, Conor Griffin, Juan Mateos-Garcia,
    Laura Weidinger, Winnie Street, Benjamin Lange, Alex Ingerman, Alison Lentz, Reed
    Enger, Andrew Barakat, Victoria Krakovna, John Oliver Siy, Zeb Kurth-Nelson, Amanda
    McCroskery, Vijay Bolina, Harry Law, Murray Shanahan, Lize Alberts, Borja Balle,
    Sarah de Haas, Yetunde Ibitoye, Allan Dafoe, Beth Goldberg, Sébastien Krier, Alexander
    Reese, Sims Witherspoon, Will Hawkins, Maribeth Rauh, Don Wallace, Matija Franklin,
    Josh A. Goldstein, Joel Lehman, Michael Klenk, Shannon Vallor, Courtney Biles,
    Meredith Ringel Morris, Helen King, Blaise Agüera y Arcas, William Isaac, and
    James Manyika. The ethics of advanced AI assistants. arXiv Preprint. arXiv 2404.16244,
    2024.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加布里埃尔等人（2024）Iason Gabriel、Arianna Manzini、Geoff Keeling、Lisa Anne Hendricks、Verena
    Rieser、Hasan Iqbal、Nenad Tomašev、Ira Ktena、Zachary Kenton、Mikel Rodriguez、Seliem
    El-Sayed、Sasha Brown、Canfer Akbulut、Andrew Trask、Edward Hughes、A. Stevie Bergman、Renee
    Shelby、Nahema Marchal、Conor Griffin、Juan Mateos-Garcia、Laura Weidinger、Winnie
    Street、Benjamin Lange、Alex Ingerman、Alison Lentz、Reed Enger、Andrew Barakat、Victoria
    Krakovna、John Oliver Siy、Zeb Kurth-Nelson、Amanda McCroskery、Vijay Bolina、Harry
    Law、Murray Shanahan、Lize Alberts、Borja Balle、Sarah de Haas、Yetunde Ibitoye、Allan
    Dafoe、Beth Goldberg、Sébastien Krier、Alexander Reese、Sims Witherspoon、Will Hawkins、Maribeth
    Rauh、Don Wallace、Matija Franklin、Josh A. Goldstein、Joel Lehman、Michael Klenk、Shannon
    Vallor、Courtney Biles、Meredith Ringel Morris、Helen King、Blaise Agüera y Arcas、William
    Isaac 和 James Manyika. 高级 AI 助手的伦理学. arXiv 预印本. arXiv 2404.16244, 2024.
- en: Gandhi et al. (2023) Kanishk Gandhi, Dorsa Sadigh, and Noah D. Goodman. Strategic
    reasoning with language models. arXiv Preprint. arXiv 2305.19165, 2023.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 甘地等人（2023）Kanishk Gandhi、Dorsa Sadigh 和 Noah D. Goodman. 使用语言模型进行战略推理. arXiv
    预印本. arXiv 2305.19165, 2023.
- en: 'Gemini Team (2024) Gemini Team. Gemini: A family of highly capable multimodal
    models. arXiv Preprint. arXiv 2312.11805, 2024.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双子团队（2024）双子团队. 双子：一类高度能力的多模态模型. arXiv 预印本. arXiv 2312.11805, 2024.
- en: Gemma Team (2024) Gemma Team. Gemma, 2024. URL [https://ai.google.dev/gemma](https://ai.google.dev/gemma).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gemma 团队（2024）Gemma 团队。Gemma，2024。网址 [https://ai.google.dev/gemma](https://ai.google.dev/gemma)。
- en: Glaese et al. (2022) Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides,
    Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe
    Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu,
    Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz,
    Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel
    Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis,
    Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment
    of dialogue agents via targeted human judgements. arXiv Preprint. arXiv:2209.14375,
    2022.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glaese 等人（2022）Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad
    Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker,
    Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan
    Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume
    Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley,
    Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray
    Kavukcuoglu, Lisa Anne Hendricks, 和 Geoffrey Irving. 通过有针对性的人类评判提高对话代理的对齐。arXiv
    预印本。arXiv:2209.14375, 2022。
- en: 'Hartmann et al. (2023) Jochen Hartmann, Jasper Schwenzow, and Maximilian Witte.
    The political ideology of conversational AI: Converging evidence on ChatGPT’s
    pro-environmental, left-libertarian orientation. arXiv Preprint arXiv:2301.01768,
    2023.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hartmann 等人（2023）Jochen Hartmann, Jasper Schwenzow, 和 Maximilian Witte。对话型 AI
    的政治意识形态：关于 ChatGPT 支持环境保护、左翼自由主义倾向的汇聚证据。arXiv 预印本 arXiv:2301.01768, 2023。
- en: 'Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *Proceedings of the 10th International Conference on
    Learning Representations (ICLR’22)*, 2022.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2022）Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen。LoRA：大规模语言模型的低秩适应。在*第10届国际学习表征会议（ICLR’22）论文集*，2022。
- en: 'Huang et al. (2024) Saffron Huang, Divya Siddarth, Liane Lovitt, Thomas I.
    Liao, Esin Durmus, Alex Tamkin, and Deep Ganguli. Collective Constitutional AI:
    Aligning a language model with public input. In *Proceedings of the 2024 ACM Conference
    on Fairness, Accountability, and Transparency (FAccT’24)*, 2024.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2024）Saffron Huang, Divya Siddarth, Liane Lovitt, Thomas I. Liao, Esin
    Durmus, Alex Tamkin, 和 Deep Ganguli。集体宪法 AI：通过公共输入对齐语言模型。在*2024 年 ACM 公平性、问责制与透明度会议（FAccT’24）论文集*，2024。
- en: Hughes et al. (2018) Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls,
    Edgar Dueñez-Guzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee,
    Raphael Koster, Tina Zhu, Heather Roff, and Thore Graepel. Inequity aversion improves
    cooperation in intertemporal social dilemmas. In *Proceedings of the 32nd International
    Conference on Neural Information Processing Systems (NeurIPS’18)*, 2018.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hughes 等人（2018）Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls, Edgar
    Dueñez-Guzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee,
    Raphael Koster, Tina Zhu, Heather Roff, 和 Thore Graepel。对不平等厌恶的回应在跨期社会困境中促进合作。在*第32届神经信息处理系统国际会议（NeurIPS’18）论文集*，2018。
- en: 'Jaques et al. (2017) Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel
    Hernández-Lobato, Richard E. Turner, and Douglas Eck. Sequence tutor: Conservative
    fine-tuning of sequence generation models with KL-control. In *Proceedings of
    the 34th International Conference on Machine Learning (ICML’17)*, pp.  1645–1654,
    2017.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaques 等人（2017）Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, José Miguel Hernández-Lobato,
    Richard E. Turner, 和 Douglas Eck。序列导师：使用 KL 控制进行序列生成模型的保守微调。在*第34届国际机器学习大会（ICML’17）论文集*，第1645-1654页，2017。
- en: 'Ji et al. (2024) Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao
    Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng,
    Kwan Yee Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara, Yingshan Lei, Hua Xu, Brian
    Tse, Jie Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo,
    and Wen Gao. AI Alignment: A comprehensive survey. arXiv Preprint. arXiv 2310.19852,
    2024.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等人（2024）Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile
    Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, Fanzhi Zeng, Kwan Yee
    Ng, Juntao Dai, Xuehai Pan, Aidan O’Gara, Yingshan Lei, Hua Xu, Brian Tse, Jie
    Fu, Stephen McAleer, Yaodong Yang, Yizhou Wang, Song-Chun Zhu, Yike Guo, 和 Wen
    Gao。AI 对齐：一项全面的调查。arXiv 预印本。arXiv 2310.19852, 2024。
- en: Kant (1785) Immanuel Kant. *Grounding for the Metaphysics of Morals*. Cambridge
    University Press, 1785.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 康德（1785）伊曼努尔·康德。*道德形而上学的基础*。剑桥大学出版社，1785。
- en: Leibo et al. (2021) Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets,
    John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie,
    Igor Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement
    learning with Melting Pot. In *Proceedings of the 38th International Conference
    on Machine Learning (ICML’21)*, 2021.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leibo 等人（2021）Joel Z. Leibo, Edgar Duéñez-Guzmán, Alexander Sasha Vezhnevets,
    John P. Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charles Beattie,
    Igor Mordatch, 和 Thore Graepel。使用 Melting Pot 进行可扩展的多智能体强化学习评估。发表于 *第38届国际机器学习大会（ICML’21）*
    论文集，2021年。
- en: Liu et al. (2024) Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou,
    Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. Training socially aligned language
    models on simulated social interactions. In *Proceedings of the 12th International
    Conference on Learning Representations (ICLR’24)*, 2024.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人（2024）Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew
    M. Dai, Diyi Yang, 和 Soroush Vosoughi。在模拟社会互动中训练社会对齐语言模型。发表于 *第12届国际学习表示大会（ICLR’24）*
    论文集，2024年。
- en: Macmillan-Scott & Musolesi (2024) Olivia Macmillan-Scott and Mirco Musolesi.
    (Ir)rationality and cognitive biases in large language models. *Royal Society
    Open Science*, 11(6):240255, 2024.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Macmillan-Scott & Musolesi（2024）Olivia Macmillan-Scott 和 Mirco Musolesi。大型语言模型中的（不）理性和认知偏差。*皇家学会开放科学*，11(6)：240255，2024年。
- en: McKee et al. (2020) Kevin R. McKee, Ian Gemp, Brian McWilliams, Edgar A. Duèñez
    Guzmán, Edward Hughes, and Joel Z. Leibo. Social diversity and social preferences
    in mixed-motive reinforcement learning. In *Proceedings of the 19th International
    Conference on Autonomous Agents and MultiAgent Systems (AAMAS’20)*, pp.  869–877,
    2020.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKee 等人（2020）Kevin R. McKee, Ian Gemp, Brian McWilliams, Edgar A. Duèñez Guzmán,
    Edward Hughes, 和 Joel Z. Leibo。在混合动机强化学习中的社会多样性和社会偏好。发表于 *第19届国际自主代理与多智能体系统会议（AAMAS’20）*
    论文集，第869–877页，2020年。
- en: Mitchell (2021) Melanie Mitchell. Why AI is harder than we think. *arXiv Preprint.
    arXiv:2104.12871*, 2021.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell（2021）Melanie Mitchell。为什么人工智能比我们想象的更难。*arXiv 预印本。arXiv:2104.12871*，2021年。
- en: Ngo et al. (2024) Richard Ngo, Lawrence Chan, and Sören Mindermann. The alignment
    problem from a deep learning perspective. In *Proceedings of the 12th International
    Conference on Learning Representations (ICLR’24)*, 2024.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ngo 等人（2024）Richard Ngo, Lawrence Chan, 和 Sören Mindermann。从深度学习的视角看对齐问题。发表于
    *第12届国际学习表示大会（ICLR’24）* 论文集，2024年。
- en: Nowak (2006) Martin A. Nowak. Five rules for the evolution of cooperation. *Science*,
    314(5805):1560–1563, 2006.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nowak（2006）Martin A. Nowak。合作演化的五条规则。*科学*，314(5805)：1560–1563，2006年。
- en: OpenAI (2024) OpenAI. OpenAI o1 System Card, 2024. URL [https://cdn.openai.com/o1-system-card-20240917.pdf](https://cdn.openai.com/o1-system-card-20240917.pdf).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI。OpenAI o1 系统卡，2024年。网址 [https://cdn.openai.com/o1-system-card-20240917.pdf](https://cdn.openai.com/o1-system-card-20240917.pdf)。
- en: OpenAI (2024) OpenAI. GPT-4 technical report. arXiv Preprint. arXiv 2303.08774,
    2024.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI。GPT-4 技术报告。arXiv 预印本。arXiv 2303.08774，2024年。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In *Proceedings of
    the 36th International Conference on Neural Information Processing Systems (NeurIPS’22)*,
    2022.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等人（2022）Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul F. Christiano, Jan Leike, 和 Ryan Lowe。训练语言模型以遵循人类反馈的指令。发表于
    *第36届神经信息处理系统大会（NeurIPS’22）* 论文集，2022年。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. Generative agents: Interactive simulacra
    of human behavior. arXiv Preprint. arXiv:2304.03442, 2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S Bernstein。生成代理：人类行为的互动模拟。arXiv 预印本。arXiv:2304.03442，2023年。
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. Gorilla: Large language model connected with massive APIs. arXiv Preprint.
    arXiv:2305.15334, 2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil 等人（2023）Shishir G. Patil, Tianjun Zhang, Xin Wang, 和 Joseph E. Gonzalez。Gorilla：连接大量
    API 的大型语言模型。arXiv 预印本。arXiv:2305.15334，2023年。
- en: 'Rafailov et al. (2024) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D. Manning, and Chelsea Finn. Direct Preference Optimization:
    Your language model is secretly a reward model, 2024.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov等人（2024）Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon,
    Christopher D. Manning, 和 Chelsea Finn. 直接偏好优化：你的语言模型实际上是一个奖励模型, 2024.
- en: Rapoport (1974) Anatol Rapoport. Prisoner’s dilemma — recollections and observations.
    In *Game Theory as a Theory of a Conflict Resolution*, pp.  17–34\. Springer,
    1974.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rapoport（1974）Anatol Rapoport. 囚徒困境——回忆与观察. 在 *博弈论作为冲突解决理论* 中, 第17–34页. Springer,
    1974.
- en: Schramowski et al. (2022) Patrick Schramowski, Cigdem Turan, Nico Andersen,
    Constantin A Rothkopf, and Kristian Kersting. Large pre-trained language models
    contain human-like biases of what is right and wrong to do. *Nature Machine Intelligence*,
    4(3):258–268, 2022.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schramowski等人（2022）Patrick Schramowski, Cigdem Turan, Nico Andersen, Constantin
    A Rothkopf, 和 Kristian Kersting. 大型预训练语言模型包含类似人类的关于何为正确与错误的偏见. *Nature Machine
    Intelligence*, 4(3):258–268, 2022.
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal Policy Optimization algorithms. arXiv Preprint.
    arXiv:1707.06347, 2017.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman等人（2017）John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,
    和 Oleg Klimov. 近端策略优化算法. arXiv预印本. arXiv:1707.06347, 2017.
- en: Shanahan et al. (2023) Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role-play
    with large language models. *Nature*, 623:493–498, 2023.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan等人（2023）Murray Shanahan, Kyle McDonell, 和 Laria Reynolds. 与大型语言模型的角色扮演.
    *Nature*, 623:493–498, 2023.
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. HuggingGPT: Solving AI tasks with ChatGPT and its friends
    in Hugging Face. In *Proceedings of the 37th Conference on Neural Information
    Processing Systems (NeurIPS’23)*, volume 36, pp.  38154–38180, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen等人（2023）Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu,
    和 Yueting Zhuang. HuggingGPT: 使用ChatGPT及其在Hugging Face中的伙伴解决AI任务. 在 *第37届神经信息处理系统会议（NeurIPS’23）论文集*
    中, 第36卷, 第38154–38180页, 2023.'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. arXiv Preprint. arXiv 2303.11366, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn等人（2023）Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, 和 Shunyu Yao. Reflexion: 带有语言强化学习的语言代理. arXiv预印本. arXiv 2303.11366,
    2023.'
- en: SIMA Team (2024) SIMA Team. Scaling instructable agents across many simulated
    worlds. arXiv Preprint. arXiv:2404.10179, 2024.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SIMA团队（2024）SIMA团队. 在多个模拟世界中扩展可指导代理. arXiv预印本. arXiv:2404.10179, 2024.
- en: 'Simmons (2022) Gabriel Simmons. Moral mimicry: Large language models produce
    moral rationalizations tailored to political identity. arXiv Preprint arXiv:2209.12106,
    2022.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simmons（2022）Gabriel Simmons. 道德模仿：大型语言模型根据政治身份生成道德合理化. arXiv预印本 arXiv:2209.12106,
    2022.
- en: Snell et al. (2023) Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and
    Sergey Levine. Offline RL for natural language generation with Implicit Language
    Q Learning. arXiv Preprint. arXiv:2206.11871, 2023.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Snell等人（2023）Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, 和 Sergey Levine.
    使用隐式语言Q学习的离线强化学习进行自然语言生成. arXiv预印本. arXiv:2206.11871, 2023.
- en: Sorensen et al. (2024) Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell
    Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang,
    Ximing Lu, Nouha Dziri, Tim Althoff, and Yejin Choi. A roadmap to Pluralistic
    Alignment. In *Proceedings of the 41st International Conference on Machine Learning
    (ICML’24)*, 2024.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorensen等人（2024）Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon,
    Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing
    Lu, Nouha Dziri, Tim Althoff, 和 Yejin Choi. 多元化对齐路线图. 在 *第41届国际机器学习大会（ICML’24）论文集*
    中, 2024.
- en: Stiennon et al. (2022) Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning
    to summarize from human feedback. arXiv Preprint. arXiv:2009.01325, 2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon等人（2022）Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan
    Lowe, Chelsea Voss, Alec Radford, Dario Amodei, 和 Paul Christiano. 从人类反馈中学习总结.
    arXiv预印本. arXiv:2009.01325, 2022.
- en: Sumers et al. (2024) Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and
    Thomas L. Griffiths. Cognitive architectures for language agents. *Transactions
    on Machine Learning Research*, 2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sumers等人（2024）Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, 和 Thomas L.
    Griffiths. 语言代理的认知架构. *机器学习研究期刊*, 2024.
- en: 'Swanepoel & Corks (2024) Danielle Swanepoel and Daniel Corks. Artificial intelligence
    and agency: Tie-breaking in AI decision-making. *Science and Engineering Ethics*,
    30(2):11, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Swanepoel & Corks（2024）Danielle Swanepoel 和 Daniel Corks. 人工智能与代理性：人工智能决策中的决策平局.
    *Science and Engineering Ethics*, 30(2):11, 2024.
- en: Tennant et al. (2023a) Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi.
    Modeling moral choices in social dilemmas with multi-agent reinforcement learning.
    In *Proceedings of the 32nd International Joint Conference on Artificial Intelligence
    (IJCAI’23)*, 2023a.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tennant 等人（2023a）Elizaveta Tennant、Stephen Hailes 和 Mirco Musolesi。使用多代理强化学习模拟社会困境中的道德选择。在
    *第32届国际人工智能联合会议（IJCAI’23）* 上，2023a。
- en: Tennant et al. (2023b) Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi.
    Learning machine morality through experience and interaction. arXiv Preprint.
    arXiv:2312.01818, 2023b.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tennant 等人（2023b）Elizaveta Tennant、Stephen Hailes 和 Mirco Musolesi。通过经验和互动学习机器道德。arXiv
    预印本。arXiv:2312.01818, 2023b。
- en: Tennant et al. (2024) Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi.
    Dynamics of moral behavior in heterogeneous populations of learning agents. In
    *Proceedings of the 7th AAAI/ACM Conference in AI, Ethics & Society (AIES’24)*,
    2024.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tennant 等人（2024）Elizaveta Tennant、Stephen Hailes 和 Mirco Musolesi。在异质学习代理的群体中，道德行为的动态变化。在
    *第7届 AAAI/ACM 人工智能、伦理与社会会议（AIES’24）* 上，2024。
- en: Vezhnevets et al. (2023) Alexander Sasha Vezhnevets, John P. Agapiou, Avia Aharon,
    Ron Ziv, Jayd Matyas, Edgar A. Duéñez-Guzmán, William A. Cunningham, Simon Osindero,
    Danny Karmon, and Joel Z. Leibo. Generative agent-based modeling with actions
    grounded in physical, social, or digital space using Concordia. arXiv Preprint
    arXiv:2312.03664, 2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vezhnevets 等人（2023）Alexander Sasha Vezhnevets、John P. Agapiou、Avia Aharon、Ron
    Ziv、Jayd Matyas、Edgar A. Duéñez-Guzmán、William A. Cunningham、Simon Osindero、Danny
    Karmon 和 Joel Z. Leibo。基于行为生成的代理建模，结合物理、社会或数字空间，使用 Concordia。arXiv 预印本 arXiv:2312.03664,
    2023。
- en: 'von Werra et al. (2020) Leandro von Werra, Younes Belkada, Lewis Tunstall,
    Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi Huang. TRL: Transformer
    reinforcement learning. [https://github.com/huggingface/trl](https://github.com/huggingface/trl),
    2020.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: von Werra 等人（2020）Leandro von Werra、Younes Belkada、Lewis Tunstall、Edward Beeching、Tristan
    Thrush、Nathan Lambert 和 Shengyi Huang。TRL：变换器强化学习。 [https://github.com/huggingface/trl](https://github.com/huggingface/trl)，2020。
- en: 'Wang et al. (2024a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *Transactions on Machine Learning Research*,
    2024a.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2024a）Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke
    Zhu、Linxi Fan 和 Anima Anandkumar。Voyager：一个开创性的具身代理，结合大型语言模型。*机器学习研究汇刊*，2024a。
- en: Wang et al. (2024b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Jirong Wen. A survey on large language model based autonomous agents.
    *Frontiers of Computer Science*, 18(6), 2024b.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2024b）Lei Wang、Chen Ma、Xueyang Feng、Zeyu Zhang、Hao Yang、Jingsen Zhang、Zhiyuan
    Chen、Jiakai Tang、Xu Chen、Yankai Lin、Wayne Xin Zhao、Zhewei Wei 和 Jirong Wen。基于大型语言模型的自主代理综述。*计算机科学前沿*，18(6)，2024b。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In *Proceedings of the 36th International
    Conference on Neural Information Processing Systems (NeurIPS’22)*, 2022.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei
    Xia、Ed H. Chi、Quoc V. Le 和 Denny Zhou。链式思维提示引发大型语言模型的推理能力。在 *第36届神经信息处理系统国际会议（NeurIPS’22）*
    上，2022。
- en: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane,
    Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick,
    Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language
    models. arXiv Preprint. arXiv:2112.04359, 2021.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger 等人（2021）Laura Weidinger、John Mellor、Maribeth Rauh、Conor Griffin、Jonathan
    Uesato、Po-Sen Huang、Myra Cheng、Mia Glaese、Borja Balle、Atoosa Kasirzadeh、Zac Kenton、Sasha
    Brown、Will Hawkins、Tom Stepleton、Courtney Biles、Abeba Birhane、Julia Haas、Laura
    Rimell、Lisa Anne Hendricks、William Isaac、Sean Legassick、Geoffrey Irving 和 Iason
    Gabriel。语言模型带来的伦理和社会风险。arXiv 预印本。arXiv:2112.04359, 2021。
- en: 'Wong et al. (2023) Lionel Wong, Gabriel Grand, Alexander K. Lew, Noah D. Goodman,
    Vikash K. Mansinghka, Jacob Andreas, and Joshua B. Tenenbaum. From word models
    to world models: Translating from natural language to the probabilistic language
    of thought. arXiv Preprint. arXiv 2306.12672, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 等人（2023）Lionel Wong、Gabriel Grand、Alexander K. Lew、Noah D. Goodman、Vikash
    K. Mansinghka、Jacob Andreas 和 Joshua B. Tenenbaum。从词汇模型到世界模型：从自然语言到思维的概率语言的翻译。arXiv
    预印本。arXiv 2306.12672, 2023。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing reasoning and acting in language
    models. *Proceedings of the 11th International Conference on Learning Representations
    (ICLR’23)*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等（2023）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan 和 Yuan Cao。ReAct：在语言模型中协同推理与行动。*第11届国际学习表示会议（ICLR'23）论文集*，2023。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. HellaSwag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics (ACL’19)*,
    2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等（2019）Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 和 Yejin
    Choi。HellaSwag：机器能否真的完成你的句子？发表于*第57届计算语言学协会年会（ACL’19）论文集*，2019。
- en: 'Zhang et al. (2024) Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter,
    Yan Xia, Wenshan Wu, Ting Song, Man Lan, and Furu Wei. LLM as a mastermind: A
    survey of strategic reasoning with large language models. arXiv Preprint. arXiv:2404.01230,
    2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024）Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter,
    Yan Xia, Wenshan Wu, Ting Song, Man Lan 和 Furu Wei。LLM作为策划者：大型语言模型战略推理的调研。arXiv预印本。arXiv:2404.01230，2024。
- en: Ziegler et al. (2020) Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown,
    Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning
    language models from human preferences. arXiv Preprint. arXiv::1909.08593, 2020.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 等（2020）Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec
    Radford, Dario Amodei, Paul Christiano 和 Geoffrey Irving。根据人类偏好微调语言模型。arXiv预印本。arXiv::1909.08593，2020。
- en: 8 Appendix
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 附录
- en: 8.1 Implementation details for reproducibility
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 可复现性的实现细节
- en: Over the course of the experiments, we tried various values for key parameters
    in the TRL library and in our reward definitions - these are are presented in
    Table [2](https://arxiv.org/html/2410.01639v2#S8.T2 "Table 2 ‣ 8.1 Implementation
    details for reproducibility ‣ 8 Appendix ‣ Moral Alignment for LLM Agents"). We
    chose the combination of values that resulted in the most stable fine-tuning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验过程中，我们尝试了TRL库中关键参数以及奖励定义的不同值，这些值在表[2](https://arxiv.org/html/2410.01639v2#S8.T2
    "Table 2 ‣ 8.1 Implementation details for reproducibility ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents")中展示。我们选择了导致最稳定微调的参数组合。
- en: '| Parameter | Values tested |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 测试的值 |'
- en: '| LoRA rank | 4; 64 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| LoRA秩 | 4; 64 |'
- en: '| LoRA target modules | “all-linear”; [“q_proj”, “k_proj”, “v_proj”, “o_proj”]
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| LoRA目标模块 | “all-linear”；[“q_proj”, “k_proj”, “v_proj”, “o_proj”] |'
- en: '| Use adaptive KL control | Yes; No |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 使用自适应KL控制 | 是; 否 |'
- en: '| Starting KL coefficient in adaptive KL control | 0.1; 0.2 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 自适应KL控制中的起始KL系数 | 0.1; 0.2 |'
- en: '| Gradient accumulation steps | 1 (no gradient accumulation); 4 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 梯度积累步骤 | 1（无梯度积累）；4 |'
- en: '| Reward normalization & scaling | Used; Not used |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 奖励归一化与缩放 | 使用; 未使用 |'
- en: '| $R_{\text{illegal}}$ | -6; -15; -100 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| $R_{\text{illegal}}$ | -6; -15; -100 |'
- en: '| IPD payoff range | 0-4; 0-100 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| IPD回报范围 | 0-4; 0-100 |'
- en: 'Table 2: Fine-tuning parameters tried.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：尝试的微调参数。
- en: 'We also tried fine-tuning with the following $C_{\text{legal}}$, $D_{\text{legal}}$
    action tokens: [action1, action2]; [action2, action1]; [A, B]; [B, A]; [X, Y];
    [0,1]; [1,0]; [XY, YX]; [randomly generated strings of ASCII characters of varying
    lengths (2,3,7 tokens)]. The action1 & action2 tokens resulted in the most stable
    training and the most consistent behavior across runs.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还尝试了使用以下 $C_{\text{legal}}$ 和 $D_{\text{legal}}$ 动作标记进行微调：[action1, action2]；[action2,
    action1]；[A, B]；[B, A]；[X, Y]；[0,1]；[1,0]；[XY, YX]；[随机生成的不同长度（2,3,7个标记）的ASCII字符字符串]。其中，action1
    和 action2 标记在训练过程中表现出最稳定的效果，并且在多次运行中行为最一致。
- en: We repeated each experiment with five random seeds and report average results
    in the paper. Occasionally (on one in six of the early runs), the training did
    not converge as the LLM model never produced a “legal” token in the game. These
    are not considered in our analysis.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用五个随机种子重复了每个实验，并在论文中报告了平均结果。偶尔（在六次早期实验中的一次），训练未能收敛，因为LLM模型在游戏中从未生成“合法”标记。这些结果没有被纳入我们的分析。
- en: 'We used the following versions of the key Python packages:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了以下版本的关键Python包：
- en: •
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: trl 0.9.4
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: trl 0.9.4
- en: •
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: peft 0.11.1
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: peft 0.11.1
- en: •
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: transformers 4.42.3
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: transformers 4.42.3
- en: 8.2 Training and Evaluation prompts
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 训练和评估提示
- en: During training, we used a prompt describing the IPD game with a history of
    one previous move as the state. This is presented in Figure [6](https://arxiv.org/html/2410.01639v2#S8.F6
    "Figure 6 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents"). At the evaluation stage, we used matrix games other than the
    IPD game. We presented these in the exact same format a the IPD training prompt,
    except with a different payoff matrix - see Figures [7](https://arxiv.org/html/2410.01639v2#S8.F7
    "Figure 7 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，我们使用了一个描述囚徒困境（IPD）游戏并包含一个先前动作历史的提示作为状态。这在图[6](https://arxiv.org/html/2410.01639v2#S8.F6
    "图 6 ‣ 8.2 训练和评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐")中展示。在评估阶段，我们使用了不同于囚徒困境游戏的矩阵博弈，并以与IPD训练提示完全相同的格式呈现这些博弈，唯一的不同是回报矩阵不同——请参见图[7](https://arxiv.org/html/2410.01639v2#S8.F7
    "图 7 ‣ 8.2 训练和评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐")。
- en: In addition to the structured matrix game prompts, we also tested four variations
    of the IPD, gradually relaxing the constraints of the original training prompt
    (see Figure [10](https://arxiv.org/html/2410.01639v2#S8.F10 "Figure 10 ‣ 8.2 Training
    and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")). As a
    final step in our evaluation, we also conducted analysis on three more general
    and unrelated prompts, which involved a “call to action” - these are presented
    in Figure [11](https://arxiv.org/html/2410.01639v2#S8.F11 "Figure 11 ‣ 8.2 Training
    and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents").
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了结构化的矩阵博弈提示外，我们还测试了囚徒困境的四种变体，逐渐放松原始训练提示中的约束（参见图[10](https://arxiv.org/html/2410.01639v2#S8.F10
    "图 10 ‣ 8.2 训练和评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。在评估的最后阶段，我们还对三种更为通用且无关的提示进行了分析，这些提示涉及“号召行动”——这些在图[11](https://arxiv.org/html/2410.01639v2#S8.F11
    "图 11 ‣ 8.2 训练和评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐")中展示。
- en: Throughout all prompts, we always randomized the order in which the action tokens
    are presented within the text (this is not reflected in the example prompts presented,
    where we show one example ordering only).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有提示中，我们始终随机化动作令牌在文本中出现的顺序（这在展示的示例提示中未反映出来，我们仅展示了一个示例顺序）。
- en: 'Core training prompt (action1=Cooperate, action2=Defect):'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 核心训练提示（action1=合作，action2=背叛）：
- en: Iterated Prisoner’s Dilemma game
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版囚徒困境游戏
- en: '![Refer to caption](img/6ea931b2fe1691985147b8397873017c.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6ea931b2fe1691985147b8397873017c.png)'
- en: 'Figure 6: Prompt with an implicit Iterated Prisoner’s Dilemma (IPD) game, which
    was used in training (i.e., fine-tuning) or at the evaluation step, with a history
    of one previous move. In our core training prompt, action1 means Cooperate, and
    action2 means Defect.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：包含隐式迭代版囚徒困境（IPD）游戏的提示，曾在训练（即微调）或评估步骤中使用，包含一个先前动作历史。在我们的核心训练提示中，action1表示合作，action2表示背叛。
- en: 'Iterated Prisoner’s Dilemma test-time prompt (evaluation only): ![Refer to
    caption](img/eaa19960f9e1cc41d9bb28d991f7a2a7.png)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版囚徒困境测试时提示（仅评估）：![参见标题](img/eaa19960f9e1cc41d9bb28d991f7a2a7.png)
- en: 'Iterated Stag Hunt test-time prompt: ![Refer to caption](img/7707ed7f49457896991e13af8bf51b77.png)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版猎鹿博弈测试时提示：![参见标题](img/7707ed7f49457896991e13af8bf51b77.png)
- en: 'Iterated Chicken test-time prompt: ![Refer to caption](img/bbcb6051d43b8c6830813dac975a1c01.png)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版“鸡游戏”测试时提示：![参见标题](img/bbcb6051d43b8c6830813dac975a1c01.png)
- en: 'Iterated Bach or Stravinsky test-time prompt: ![Refer to caption](img/11c9dc9f5ae534d9d42cf940a19b201b.png)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版巴赫或斯特拉文斯基测试时提示：![参见标题](img/11c9dc9f5ae534d9d42cf940a19b201b.png)
- en: 'Iterated Defective Coordination test-time prompt: ![Refer to caption](img/92ce9c7c19f05c9d662d82ddfc8b18f0.png)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代版缺陷协调测试时提示：![参见标题](img/92ce9c7c19f05c9d662d82ddfc8b18f0.png)
- en: 'Figure 7: Prompts for six different iterated matrix games used at evaluation,
    with a history of one previous move. Across these six prompts, only the payoff
    matrix differs - otherwise, the format is identical to the training IPD prompt.
    In the main results reported in the paper, we use action token action3 to mean
    Cooperate, and action4 to mean Defect. For an evaluation using the original training
    action tokens action1 and action2 instead, please refer to Section [8.9](https://arxiv.org/html/2410.01639v2#S8.SS9
    "8.9 Analysis of generalization across five games - using new and original action
    tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents").'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在评估中用于六种不同的迭代矩阵游戏的提示，每个游戏有一个之前的历史动作。在这六个提示中，只有支付矩阵不同——否则格式与训练时的 IPD 提示相同。在论文中报告的主要结果中，我们使用动作符号
    action3 表示合作，action4 表示背叛。有关使用原始训练动作符号 action1 和 action2 进行评估的详细信息，请参见第 [8.9](https://arxiv.org/html/2410.01639v2#S8.SS9
    "8.9 Analysis of generalization across five games - using new and original action
    tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")
    节。
- en: 'a) Core test-time prompt (action3=Cooperate, action4=Defect):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: a) 核心测试时提示（action3 = 合作，action4 = 背叛）：
- en: Iterated Prisoner’s Dilemma game
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代囚徒困境游戏
- en: '![Refer to caption](img/3ca7d651d3627274e48175e3019b2e41.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3ca7d651d3627274e48175e3019b2e41.png)'
- en: 'b) Version of test-time Iterated Prisoner’s Dilemma prompt with the meaning
    of the original action tokens reversed (action2 = Cooperate, action1 = Defect):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: b) 测试时“囚徒困境”提示版本，反转了原始动作符号的含义（action2 = 合作，action1 = 背叛）：
- en: '![Refer to caption](img/c25bd63448490994e02af7c66cc63df0.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c25bd63448490994e02af7c66cc63df0.png)'
- en: 'Figure 8: Versions of the IPD test-time prompt used in additional analyses.
    At test time, as reported in Section [5](https://arxiv.org/html/2410.01639v2#S5
    "5 Generalization to Moral Choices in Other Environments ‣ Moral Alignment for
    LLM Agents") in the paper, we use new symbols for the actions in each game: action3
    and action4 (prompt a in the Figure). We also run additional test-time evaluations
    with a prompt using the original action tokens but reversing the meaning of the
    original action tokens (b).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：用于额外分析的 IPD 测试时提示的版本。如论文第 [5](https://arxiv.org/html/2410.01639v2#S5 "5
    Generalization to Moral Choices in Other Environments ‣ Moral Alignment for LLM
    Agents") 节所述，在测试时，我们为每个游戏的动作使用新的符号：action3 和 action4（图中的提示 a）。我们还进行了额外的测试时评估，使用原始的动作符号，但反转了原始动作符号的含义（b）。
- en: 'Permutations of IPD test-time prompt:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: IPD 测试时提示的排列组合：
- en: 'Permutation 1:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 排列组合 1：
- en: '![Refer to caption](img/ab5d49823e253cac4545f9a7452d081d.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ab5d49823e253cac4545f9a7452d081d.png)'
- en: 'Permutation 2:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 排列组合 2：
- en: '![Refer to caption](img/6d6818a8173ce1a459d63e47ce7abc2e.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6d6818a8173ce1a459d63e47ce7abc2e.png)'
- en: 'Permutation 3:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 排列组合 3：
- en: '![Refer to caption](img/f53201e80aefdd14c2691a0211027505.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f53201e80aefdd14c2691a0211027505.png)'
- en: 'Permutation 4:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 排列组合 4：
- en: '![Refer to caption](img/d689ffade93e0baed853effa7c8a82d5.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d689ffade93e0baed853effa7c8a82d5.png)'
- en: 'Figure 9: Permuted versions of the game prompt (exemplified on the IPD) used
    to test the generality of results across payoff matrix orderings. In these evaluation
    prompts, we use the new action tokens, where action3=Cooperate, and action4=Defect.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：用于测试支付矩阵排序结果广泛性的游戏提示的排列组合（以 IPD 为例）。在这些评估提示中，我们使用新的动作符号，其中 action3 = 合作，action4
    = 背叛。
- en: Structured IPD test-time prompt
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化的 IPD 测试时提示
- en: '(with payoffs, as used in training): ![Refer to caption](img/03607b45fff816cc233bfa14f53a3011.png)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: （带有支付回报，如训练中使用的）： ![参见说明](img/03607b45fff816cc233bfa14f53a3011.png)
- en: Unstructured IPD test-time prompt
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化的 IPD 测试时提示
- en: '(with payoffs described in text): ![Refer to caption](img/b2c5cb13b2f51c849f76f63854762cf8.png)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: （文本中描述的支付回报）： ![参见说明](img/b2c5cb13b2f51c849f76f63854762cf8.png)
- en: IPD-like test-time prompt
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: IPD 类测试时提示
- en: '(no payoffs):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: （没有支付回报）：
- en: '![Refer to caption](img/0490da3f789a6c83cbe2d8c526095eba.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0490da3f789a6c83cbe2d8c526095eba.png)'
- en: Explicit IPD test-time prompt
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 明确的 IPD 测试时提示
- en: '(implicit payoffs assumed from model knowledge): ![Refer to caption](img/5f6f84293131ea2a170df4c99302b643.png)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: （假设模型知识中的隐式支付回报）： ![参见说明](img/5f6f84293131ea2a170df4c99302b643.png)
- en: 'Figure 10: Variations of IPD-like prompts used at evaluation. In these evaluation
    prompts, we use the new action tokens, where action3=Cooperate, and action4=Defect.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：在评估中使用的 IPD 类提示的变化。在这些评估提示中，我们使用新的动作符号，其中 action3 = 合作，action4 = 背叛。
- en: 'Unrelated “Action-only” test-time prompt : ![Refer to caption](img/fa28e3f855b160ea92b75757ed1face3.png)'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 无关的“仅动作”测试时提示： ![参见说明](img/fa28e3f855b160ea92b75757ed1face3.png)
- en: 'Unrelated “Action+Game” test-time prompt: ![Refer to caption](img/4567c87ecb8822fbf4a34fe256a1ea75.png)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 无关的“Action+Game”测试时提示：![参见说明](img/4567c87ecb8822fbf4a34fe256a1ea75.png)
- en: 'Unrelated “Action+Game+State” test-time prompt: ![Refer to caption](img/2c0cd118683fba2b1e153060126441a7.png)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 无关的“Action+Game+State”测试时提示：![参见说明](img/2c0cd118683fba2b1e153060126441a7.png)
- en: 'Figure 11: More general and unrelated prompts used at evaluation. In these
    evaluation prompts, we use the new action tokensaction3 and action4.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：评估中使用的更多一般性和无关提示。在这些评估提示中，我们使用了新的动作符号 action3 和 action4。
- en: 8.3 Moral reward during fine-tuning
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 微调过程中的道德奖励
- en: 'In Figure [12](https://arxiv.org/html/2410.01639v2#S8.F12 "Figure 12 ‣ 8.3
    Moral reward during fine-tuning ‣ 8 Appendix ‣ Moral Alignment for LLM Agents"),
    we visualize moral reward obtained by the LLM agent over the course of fine-tuning
    - to complement the action types observed during training, which were presented
    in Figures [2](https://arxiv.org/html/2410.01639v2#S4.F2 "Figure 2 ‣ 4.2 Learning
    dynamics ‣ 4 Evaluating the effectiveness of fine-tuning: moral choices on the
    IPD ‣ Moral Alignment for LLM Agents") and [3](https://arxiv.org/html/2410.01639v2#S4.F3
    "Figure 3 ‣ 4.3 Learning and Unlearning the Selfish Strategy on the IPD ‣ 4 Evaluating
    the effectiveness of fine-tuning: moral choices on the IPD ‣ Moral Alignment for
    LLM Agents") in the main paper. An interesting observation is the high variance
    in moral rewards of the Game, then Utilitarian agent - we hypothesize that this
    is caused by the slower convergence rate of the Utilitarian moral policy in general
    (c.f. the pure Utilitarian learner in Figure [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "Figure 2 ‣ 4.2 Learning dynamics ‣ 4 Evaluating the effectiveness of fine-tuning:
    moral choices on the IPD ‣ Moral Alignment for LLM Agents")), so converting from
    a selfish to a Utilitarian reward function leads to instability in the model’s
    behavior before convergence.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [12](https://arxiv.org/html/2410.01639v2#S8.F12 "图 12 ‣ 8.3 微调中的道德奖励 ‣ 8
    附录 ‣ LLM 智能体的道德对齐") 中，我们展示了 LLM 智能体在微调过程中获得的道德奖励，以补充训练过程中观察到的动作类型，这些类型在论文中的图 [2](https://arxiv.org/html/2410.01639v2#S4.F2
    "图 2 ‣ 4.2 学习动态 ‣ 4 微调效果评估：IPD 上的道德选择 ‣ LLM 智能体的道德对齐") 和 [3](https://arxiv.org/html/2410.01639v2#S4.F3
    "图 3 ‣ 4.3 学习与遗忘自私策略在 IPD 中的应用 ‣ 4 微调效果评估：IPD 上的道德选择 ‣ LLM 智能体的道德对齐") 中已展示。有一个有趣的观察是游戏和功利主义智能体的道德奖励的高方差——我们推测这可能是由于功利主义道德政策的收敛速度较慢（参见图
    [2](https://arxiv.org/html/2410.01639v2#S4.F2 "图 2 ‣ 4.2 学习动态 ‣ 4 微调效果评估：IPD 上的道德选择
    ‣ LLM 智能体的道德对齐") 中的纯功利主义学习者），因此，从自私奖励函数转换为功利主义奖励函数会导致模型行为在收敛之前的不稳定。
- en: '![Refer to caption](img/43cf279e38e88d3559c798c423173c8d.png)![Refer to caption](img/444a3aa7740a70381198b68364d47cbd.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43cf279e38e88d3559c798c423173c8d.png)![参见说明](img/444a3aa7740a70381198b68364d47cbd.png)'
- en: 'Figure 12: Moral reward obtained by the LLM agent during fine-tuning with each
    type of moral reward, normalized to the min & max possible values for each reward
    function. We average over 5 runs (+- 95%CI), and plot the moving average with
    window size 10.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：LLM 智能体在微调过程中获得的道德奖励，按每种奖励类型的最小值和最大值归一化。我们进行了 5 次实验并取平均值（+− 95% 置信区间），并绘制了窗口大小为
    10 的移动平均。
- en: 8.4 Fine-tuning variation with C & D symbols reversed
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 微调变化（C 和 D 符号交换）
- en: 'As a robustness check, we ran a core baseline experiment (fine-tuning on Game
    reward versus a TFT opponent) with the meaning of the action tokens reversed:
    here action2=Cooperate, action1=Defect. Compared to the original type of fine-tuning,
    we observe slightly more cooperation early on in the trailing process, but the
    end point is similar to the results presented in the main paper, with the LLM
    agent learning to Defect nearly 100% of the time (see comparison in Figure [13](https://arxiv.org/html/2410.01639v2#S8.F13
    "Figure 13 ‣ 8.4 Fine-tuning variation with C & D symbols reversed ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents")).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 作为稳健性检验，我们运行了一个核心基准实验（在游戏奖励上进行微调，且对手为 TFT），并交换了动作符号的含义：这里 action2=合作，action1=背叛。与原始的微调类型相比，我们观察到在追踪过程中初期略有更多的合作，但最终结果与主文中的结果相似，LLM
    智能体几乎在 100% 的时间里学习到背叛（参见图 [13](https://arxiv.org/html/2410.01639v2#S8.F13 "图 13
    ‣ 8.4 微调变化（C 和 D 符号交换） ‣ 8 附录 ‣ LLM 智能体的道德对齐") 中的对比）。
- en: '![Refer to caption](img/9255a7531d2d117d28b826bfe0ffd747.png)![Refer to caption](img/dd114cb357f3891f6a73805f0cebb8dd.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9255a7531d2d117d28b826bfe0ffd747.png)![参见说明](img/dd114cb357f3891f6a73805f0cebb8dd.png)'
- en: 'Figure 13: Comparing fine-tuning implementations with tokens Cooperate=action1,
    Defect=action2 (as in the main paper), versus the implementation in which these
    are swapped, on the baseline experiment (i.e., fine-tuning with the Game rewards
    vs a TFT opponent). We observe small differences early on during learning in the
    case in which symbols are reversed.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：比较在基础实验中（即，使用游戏奖励对TFT对手进行微调），符号合作=action1，背叛=action2（如论文中所述），与符号互换后的实现。在符号反转的情况下，我们观察到学习初期有小的差异。
- en: 8.5 All fine-tuning results vs TFT, Random, AD, AC or LLM opponent
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 所有微调结果与TFT、随机、AD、AC或LLM对手的对比
- en: '|  | Game Fine-tuning | Deontological Fine-tuning | Utilitarian Fine-tuning
    | Game + Deontological Fine-tuning |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | 游戏微调 | 道义微调 | 功利主义微调 | 游戏+道义微调 |'
- en: '| vs TFT |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 与TFT对比 |'
- en: '&#124; ![Refer to caption](img/d719ff96d1426e566d462a23c97fb405.png) &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/d719ff96d1426e566d462a23c97fb405.png) &#124;'
- en: '|'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/22691a2fee18346a6caf6ef2d2bc4640.png) &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/22691a2fee18346a6caf6ef2d2bc4640.png) &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/50725a345eaa3893b884fe6bf37d94e0.png) &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/50725a345eaa3893b884fe6bf37d94e0.png) &#124;'
- en: '|'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/018520a2627cc0f6d2f57066a34c5710.png) &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/018520a2627cc0f6d2f57066a34c5710.png) &#124;'
- en: '|'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| vs Random |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 与随机对比 |'
- en: '&#124; ![Refer to caption](img/ce54a9ee5ce7f1d873df793c02def704.png) &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/ce54a9ee5ce7f1d873df793c02def704.png) &#124;'
- en: '|'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/20a6a6967d37f50fede84bee8d30e81f.png) &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/20a6a6967d37f50fede84bee8d30e81f.png) &#124;'
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/0419364ba0bf9c5d50ac211a62a9a8bf.png) &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/0419364ba0bf9c5d50ac211a62a9a8bf.png) &#124;'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/076962b9ffbab2e1962ce401177babe8.png) &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/076962b9ffbab2e1962ce401177babe8.png) &#124;'
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| vs AD |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 与AD对比 |'
- en: '&#124; ![Refer to caption](img/a2262d7c4000ee186528867b551045fd.png) &#124;'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/a2262d7c4000ee186528867b551045fd.png) &#124;'
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/eb661b70fcb3167aac9e90ed95011ae9.png) &#124;'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/eb661b70fcb3167aac9e90ed95011ae9.png) &#124;'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/f5691b780a035f5b0978e4682a3b45e8.png) &#124;'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/f5691b780a035f5b0978e4682a3b45e8.png) &#124;'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/aae6bbea0216877a2f9d846b3664fce6.png) &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/aae6bbea0216877a2f9d846b3664fce6.png) &#124;'
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| vs AC |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 与AC对比 |'
- en: '&#124; ![Refer to caption](img/f92e0bd5dad2d3ae3f4292197da4b78d.png) &#124;'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/f92e0bd5dad2d3ae3f4292197da4b78d.png) &#124;'
- en: '|'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/ccad294f741cd81cd39d587bcc69b363.png) &#124;'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/ccad294f741cd81cd39d587bcc69b363.png) &#124;'
- en: '|'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/c786b78ecd867439654bf20f6623b7b5.png) &#124;'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/c786b78ecd867439654bf20f6623b7b5.png) &#124;'
- en: '|'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/abd1c31866f31052b0fa313f4d3c53b6.png) &#124;'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/abd1c31866f31052b0fa313f4d3c53b6.png) &#124;'
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| vs LLM |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 与LLM对比 |'
- en: '&#124; ![Refer to caption](img/eadac98ca0edeac5cee877d18e252572.png) &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/eadac98ca0edeac5cee877d18e252572.png) &#124;'
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/4348b570ad650aa9dcf385fdabc60513.png) &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/4348b570ad650aa9dcf385fdabc60513.png) &#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/b8d91b121e35aa66e141ff570b158075.png) &#124;'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/b8d91b121e35aa66e141ff570b158075.png) &#124;'
- en: '|'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ![Refer to caption](img/0cc4f6de835e33be076179d46cb31f2e.png) &#124;'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ![参见图例](img/0cc4f6de835e33be076179d46cb31f2e.png) &#124;'
- en: '|'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Figure 14: Action types displayed during fine-tuning on the Iterated Prisoner’s
    Dilemma (IPD) game against four fixed-strategy opponents and an LLM opponent.
    For each episode, we plot the actions of the LLM player $M$ given the last move
    of their opponent $O$.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在与四个固定策略对手和一个LLM对手的迭代囚徒困境（IPD）游戏中的微调过程中展示的动作类型。对于每一回合，我们绘制了LLM玩家$M$在对手$O$上一步动作之后的行为。
- en: 'To complement the results in the paper, where we fine-tune an LLM agent versus
    a TFT or another LLM opponent, in Figure [14](https://arxiv.org/html/2410.01639v2#S8.F14
    "Figure 14 ‣ 8.5 All fine-tuning results vs TFT, Random, AD, AC or LLM opponent
    ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") we add the results for fine-tuning
    versus three additional fixed-strategy opponents: Random, Always Defect (AD),
    Always Cooperate (AC). We present the results for fine-tuning versus a TFT and
    ann LLM opponent once again for comparability.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充论文中的结果，我们微调了一个LLM代理与TFT或其他LLM对手的对比，在图[14](https://arxiv.org/html/2410.01639v2#S8.F14
    "图14 ‣ 8.5 所有微调结果与TFT、随机、AD、AC或LLM对手的对比 ‣ 8 附录 ‣ LLM代理的道德对齐")中，我们添加了与三个额外固定策略对手的微调结果：随机、总是背叛（AD）、总是合作（AC）。我们再次展示了与TFT和LLM对手的微调结果，以便进行比较。
- en: 8.6 Five matrix games used in the generalization analysis
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 五种矩阵博弈用于泛化分析
- en: As discussed in the paper, when evaluating the generalization of the learned
    policies, in addition to the IPD, which was used in training, we relied on four
    other matrix games of a similar format, each of which presented a different set
    of strategies and theoretical equilibria. The payoff matrices for any one step
    of these iterated games are presented in Table [3](https://arxiv.org/html/2410.01639v2#S8.T3
    "Table 3 ‣ 8.6 Five matrix games used in the generalization analysis ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents"). The associated prompts are presented in Figure
    [7](https://arxiv.org/html/2410.01639v2#S8.F7 "Figure 7 ‣ 8.2 Training and Evaluation
    prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents").
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 正如论文中所讨论的，在评估所学策略的泛化时，除了训练中使用的 IPD 外，我们还依赖于其他四种具有相似格式的矩阵博弈，每种博弈都呈现了不同的策略集合和理论均衡。这些反复博弈的每一步的支付矩阵如表
    [3](https://arxiv.org/html/2410.01639v2#S8.T3 "Table 3 ‣ 8.6 Five matrix games
    used in the generalization analysis ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")
    中所示。相关提示如图 [7](https://arxiv.org/html/2410.01639v2#S8.F7 "Figure 7 ‣ 8.2 Training
    and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") 中所示。
- en: 'Table 3: Payoffs for each of the iterated games used to test generalization,
    compared with the Iterated Prisoner’s Dilemma environment used in training.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：用于测试泛化的每个反复博弈的支付，与训练中使用的反复囚徒困境环境进行比较。
- en: Iterated Prisoner’s Dilemma
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 反复囚徒困境游戏
- en: (as used in training)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: （如在训练中使用）
- en: '|  | C | D |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  | C | D |'
- en: '| C | 3, 3 | 0, 4 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| C | 3, 3 | 0, 4 |'
- en: '| D | 4, 0 | 1, 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| D | 4, 0 | 1, 1 |'
- en: Iterated Stag Hunt
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 反复博弈的梅花猎人游戏
- en: '|  | C | D |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | C | D |'
- en: '| C | 4, 4 | 0, 3 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| C | 4, 4 | 0, 3 |'
- en: '| D | 3, 0 | 1, 1 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| D | 3, 0 | 1, 1 |'
- en: Iterated Chicken
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 反复博弈的鸡兔游戏
- en: '|  | C | D |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | C | D |'
- en: '| C | 2, 2 | 1, 4 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| C | 2, 2 | 1, 4 |'
- en: '| D | 4, 1 | 0, 0 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| D | 4, 1 | 0, 0 |'
- en: Iterated Bach or Stravinsky
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 反复博弈的巴赫或斯特拉文斯基
- en: C D C 3, 2 0, 0 D 0, 0 2, 3
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: C D C 3, 2 0, 0 D 0, 0 2, 3
- en: Iterated Defective Coordination
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 反复博弈的缺陷协调
- en: '|  | C | D |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '|  | C | D |'
- en: '| C | 1, 1 | 0, 0 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| C | 1, 1 | 0, 0 |'
- en: '| D | 0, 0 | 4, 4 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| D | 0, 0 | 4, 4 |'
- en: For example, in terms of Utilitarian reward, these games differ in meaningful
    ways from the IPD. In the IPD, the highest collective payoff on any one step (which
    is equivalent to the Utilitarian moral reward in our definition) can be achieved
    via mutual cooperation. This is also the case on the Iterated Stag Hunt game.
    However, on the Iterated Chicken game greater collective payoff is obtained by
    unilateral defection (C,D or D,C), and on the Iterated Bach of Stravinsky game,
    equivalent collective rewards are received under mutual cooperation (C,C) or mutual
    defection (D,D). Finally, on the Iterated Defective Coordination game, the greatest
    collective payoff is obtained by mutual defection.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，就功利主义奖励而言，这些游戏在与 IPD 的比较中存在显著差异。在 IPD 中，任何一步的最高集体收益（相当于我们定义中的功利主义道德奖励）可以通过相互合作实现。在反复博弈的梅花猎人游戏中也是如此。然而，在反复博弈的鸡兔游戏中，通过单方面背叛（C,D
    或 D,C）可以获得更高的集体收益，而在反复博弈的斯特拉文斯基巴赫游戏中，双方合作（C,C）或双方背叛（D,D）时获得的集体奖励是等同的。最后，在反复博弈的缺陷协调游戏中，最大的集体收益是通过双方背叛实现的。
- en: Due to these differences, these games provide an interesting test-bed for the
    generalization of the moral policies learned by the LLM agents, which were fine-tuned
    in our experiments with Deontological and Utilitarian moral rewards.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些差异，这些游戏为 LLM 智能体学习的道德策略的泛化提供了一个有趣的测试平台，这些策略在我们的实验中通过义务论和功利主义道德奖励进行了微调。
- en: 8.7 Analysis of generalization for models fine-tuned against another LLM
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 针对另一个 LLM 微调模型的泛化分析
- en: The analyses in Figures [15](https://arxiv.org/html/2410.01639v2#S8.F15 "Figure
    15 ‣ 8.7 Analysis of generalization for models fine-tuned against another LLM
    ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") and [16](https://arxiv.org/html/2410.01639v2#S8.F16
    "Figure 16 ‣ 8.7 Analysis of generalization for models fine-tuned against another
    LLM ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") complement the results for
    models fine-tuned versus a TFT opponent presented in the main paper, presenting
    generalization analysis for models that were fine-tuned against another LLM opponent.
    The patterns of results are similar to those for fine-tuning against the static
    TFT opponent.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图表 [15](https://arxiv.org/html/2410.01639v2#S8.F15 "Figure 15 ‣ 8.7 Analysis
    of generalization for models fine-tuned against another LLM ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents") 和 [16](https://arxiv.org/html/2410.01639v2#S8.F16 "Figure
    16 ‣ 8.7 Analysis of generalization for models fine-tuned against another LLM
    ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") 中的分析补充了在主要论文中针对 TFT 对手微调的模型结果，展示了针对另一个
    LLM 对手微调模型的泛化分析。结果模式与针对静态 TFT 对手微调的结果相似。
- en: 'Core analyses (moral regret) for models fine-tuned versus an LLM opponent:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 针对LLM对手微调模型的核心分析（道德遗憾）：
- en: '![Refer to caption](img/c44eeacbb9cc160ed910d432b7e39915.png) ![Refer to caption](img/1c625b623c3a15d11df41d3d7d8127b8.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c44eeacbb9cc160ed910d432b7e39915.png) ![参见说明](img/1c625b623c3a15d11df41d3d7d8127b8.png)'
- en: 'Figure 15: Analysis of generalization of the fine-tuned agents’ learned morality
    to other matrix game environments. We present results for models fine-tuned against
    an LLM opponent, to complement the results for fine-tuning versus a TFT opponent
    presented in the main paper (Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4
    "Figure 4 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents")). This
    analysis is conducted with the new action tokens action3 and action4.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：分析微调后的代理在其他矩阵游戏环境中的道德泛化能力。我们展示了针对LLM对手微调模型的结果，以补充在正文中展示的针对TFT对手微调的结果（图
    [4](https://arxiv.org/html/2410.01639v2#S5.F4 "图 4 ‣ 5.1 其他矩阵游戏中的道德选择的泛化 ‣ 5 其他环境中的道德选择的泛化
    ‣ LLM 代理的道德对齐")）。此分析使用了新的动作标记 action3 和 action4。
- en: 'Core analyses (Action types) for models fine-tuned versus an LLM opponent:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 针对LLM对手微调模型的核心分析（动作类型）：
- en: '![Refer to caption](img/28a155795dfc74a776cca61ef6c9dc5c.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28a155795dfc74a776cca61ef6c9dc5c.png)'
- en: 'Figure 16: Analysis of action choices at test time on the five iterated matrix
    games. We present results for models trained against an LLM opponent, to complement
    the results for training versus a TFT opponent presented in the main paper (Figure
    [5](https://arxiv.org/html/2410.01639v2#S5.F5 "Figure 5 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents")). This analysis is conducted with
    the new action tokens action3 and action4.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：在五个迭代矩阵游戏上的测试时行动选择分析。我们展示了针对LLM对手训练模型的结果，以补充在正文中展示的针对TFT对手训练的结果（图 [5](https://arxiv.org/html/2410.01639v2#S5.F5
    "图 5 ‣ 5.1 其他矩阵游戏中的道德选择的泛化 ‣ 5 其他环境中的道德选择的泛化 ‣ LLM 代理的道德对齐")）。此分析使用了新的动作标记 action3
    和 action4。
- en: 8.8 Analysis of the impact of fine-tuning beyond Matrix Games.
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.8 微调对矩阵游戏以外环境的影响分析。
- en: 'As discussed in Section [5.2](https://arxiv.org/html/2410.01639v2#S5.SS2 "5.2
    Impact of Fine-tuning Beyond Matrix Games ‣ 5 Generalization to Moral Choices
    in Other Environments ‣ Moral Alignment for LLM Agents") of the paper, we conduct
    a further evaluation of the behavior of fine-tuned models on four unrelated prompts
    without a payoff matrix. Figure [11](https://arxiv.org/html/2410.01639v2#S8.F11
    "Figure 11 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents") presents the four extra prompts used in this analysis. In this
    evaluation, we used the original action tokens action1 and action2. In the prompts,
    we varied three elements in particular: an action choice (“You must choose either
    action action1 or action action2”), a “game” description (“You are playing a game
    with another agent A”), and a state representing an opponent’s previous action
    (e.g., “You have played with this opponent before. Last time, they played action1.
    What action would you take this time?”). Finally, we also tested responses to
    an explicit IPD prompt (“You are playing a repeated Prisoner’s Dilemma game with
    another agent A. You must choose either action action1 or action action2. Assume
    traditional payoffs from the Prisoner’s Dilemma. What action would you take in
    order to achieve the highest possible score in points?” - we note that the explicit
    IPD prompt we use does not involve a payoff matrix). Again, we randomize the order
    in which the action tokens are presented in the textual part of the prompt.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 正如论文[5.2](https://arxiv.org/html/2410.01639v2#S5.SS2 "5.2 微调对矩阵博弈之外的影响 ‣ 5 在其他环境中的道德选择的泛化
    ‣ LLM代理的道德对齐")部分讨论的那样，我们进一步评估了微调模型在四个与回报矩阵无关的提示下的行为。图[11](https://arxiv.org/html/2410.01639v2#S8.F11
    "图11 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")展示了本次分析中使用的四个额外提示。在这次评估中，我们使用了原始的动作标记action1和action2。在这些提示中，我们特别变化了三种元素：一个动作选择（“你必须选择动作action1或动作action2”），一个“游戏”描述（“你正在与另一个代理A玩游戏”），以及一个表示对手先前动作的状态（例如，“你以前与该对手玩过。上次，他们选择了action1。你这次会选择什么动作？”）。最后，我们还测试了对一个明确的IPD提示的响应（“你正在与另一个代理A玩重复的囚徒困境游戏。你必须选择动作action1或动作action2。假设传统的囚徒困境回报。为了获得最高的分数，你会选择什么动作？”
    - 我们注意到，我们使用的明确IPD提示并不涉及回报矩阵）。同样，我们随机化了动作标记在提示文本部分出现的顺序。
- en: We classify the models’ responses to these four prompts as either exactly matching
    one of the action tokens action1 and action2 used during fine-tuning, or as “other”
    (e.g., if the model responded with the likes of “please give me more information”,
    or if it produced an action token alongside other text). Results are presented
    in Figure [18](https://arxiv.org/html/2410.01639v2#S8.F18 "Figure 18 ‣ 8.8 Analysis
    of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型对这四种提示的响应分类为：与在微调过程中使用的动作标记action1和action2完全匹配，或分类为“其他”（例如，如果模型的回应类似于“请给我更多信息”，或者如果它在生成动作标记的同时还生成了其他文本）。结果见图[18](https://arxiv.org/html/2410.01639v2#S8.F18
    "图18 ‣ 8.8 微调对矩阵博弈之外的影响分析 ‣ 8 附录 ‣ LLM代理的道德对齐")。
- en: 'Extra analysis test-time performance on four types of IPD prompt:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 四种IPD提示的额外分析测试表现：
- en: '![Refer to caption](img/6482f1eebe3b35448120a21f1deb4b02.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6482f1eebe3b35448120a21f1deb4b02.png)'
- en: 'Figure 17: Analysis of action choices at test time on the four variations of
    the IPD prompt (see prompts in Figure [11](https://arxiv.org/html/2410.01639v2#S8.F11
    "Figure 11 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")). This analysis is conducted with the new action tokens action3
    and action4.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：在四种IPD提示变化下，测试时动作选择的分析（参见图[11](https://arxiv.org/html/2410.01639v2#S8.F11
    "图11 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")中的提示）。该分析使用了新的动作标记action3和action4。
- en: 'Extra analysis test-time performance on three prompts involving an Action,
    Game and/or a State:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 三个涉及动作、游戏和/或状态的提示的额外分析测试表现：
- en: '![Refer to caption](img/6afad1b9bb71a881310809d7822c8437.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6afad1b9bb71a881310809d7822c8437.png)'
- en: 'Figure 18: Analysis of action choices at test time on the three unrelated prompts
    that contain a “call to action” but no payoff matrix (see prompts in Figure [11](https://arxiv.org/html/2410.01639v2#S8.F11
    "Figure 11 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")). This analysis is conducted with the new action tokens action3
    and action4.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：在测试时分析三个无关提示中的行动选择，这些提示包含“行动呼吁”但没有收益矩阵（请参见图 [11](https://arxiv.org/html/2410.01639v2#S8.F11
    "图 11 ‣ 8.2 训练和评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。该分析使用了新的行动标记 action3 和 action4。
- en: We analyze the results for models trained against a TFT opponent, but the patterns
    are similar for models trained against another LLM. We find that fine-tuning on
    the implicit IPD game also modifies the behavior of the model in response to an
    explicit IPD prompt based on the same action tokens. The change in behavior is
    consistent with the moral value learned, assuming the agent maps the order of
    the two tokens onto the order seen during training. For example, the production
    of more action1 tokens by the Deontological agent would mean more cooperative
    behavior on the IPD. However, it is possible that the model simply learned to
    choose the first token of the two (in terms of digit order) in response to any
    similar prompt, rather than responding to the semantics of the IPD game in particular.
    To evaluate this, we assessed the models’ behavior on three other prompts, which
    do not mention the IPD or any payoffs, but request that an action token be output
    nonetheless. When simply asked to “choose an action” (“Action-only”), some of
    the models (specifically, those fine-tuned with Game, Deontological, Utilitarian
    or Game, then Deontological rewards) output unrelated tokens most of the time.
    On the other hand, the more consequentialist models - i.e., those fine-tuned with
    rewards which somehow depend on the payoffs of the game (namely, Game,, Game+Deontological
    or the Game, then Utilitarian) are biased towards outputting one of the action
    tokens more than any other symbol in response to this generic “Action-only” prompt.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了针对 TFT 对手训练的模型的结果，但对于其他 LLM 训练的模型，模式相似。我们发现，在隐式 IPD 游戏上的微调也会根据相同的行动标记改变模型对明确
    IPD 提示的响应行为。行为的变化与所学的道德价值是一致的，前提是代理将这两个标记的顺序映射到训练中看到的顺序。例如，道义论代理在 IPD 中产生更多 action1
    标记意味着更多的合作行为。然而，也有可能模型仅仅学会了对任何类似提示选择两个标记中的第一个（按数字顺序），而不是特别响应 IPD 游戏的语义。为了评估这一点，我们评估了模型在三个其他提示下的行为，这些提示没有提到
    IPD 或任何收益，但仍要求输出一个行动标记。当仅仅被要求“选择一个行动”（“Action-only”）时，一些模型（特别是那些使用 Game、Deontological、Utilitarian
    或 Game 然后 Deontological 奖励微调的模型）大多数时候输出无关标记。另一方面，那些更具结果主义的模型——即那些使用依赖于游戏收益的奖励进行微调的模型（即
    Game、Game+Deontological 或 Game 然后 Utilitarian）则倾向于在响应这一通用的“Action-only”提示时输出某个行动标记，而不是任何其他符号。
- en: When a prompt explicitly mentions a “game” (“Action+Game”), the probability
    of outputting one of the action tokens increased to over 80% for most models,
    and even slightly more so when the test prompt also mentioned a “state” (“Action+Game+State”).
    Once again, the specific action tokens chosen in response to these prompts appear
    to be influenced by the semantics of these tokens on the IPD fine-tuning (here,
    this would mean interpreting action1 as Cooperate, and action2 as Defect). For
    example, we observe that the Deontological model was very likely to choose the
    cooperation token action1 on these unrelated prompts as well as on the explicit
    IPD (see Figure [18](https://arxiv.org/html/2410.01639v2#S8.F18 "Figure 18 ‣ 8.8
    Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents")).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示明确提到“游戏”（“Action+Game”）时，对于大多数模型，输出其中一个行动标记的概率增加到超过 80%，当测试提示同时提到“状态”（“Action+Game+State”）时，概率甚至稍微更高。再次指出，响应这些提示时选择的具体行动标记似乎受到这些标记在
    IPD 微调中的语义影响（在这里，这意味着将 action1 解释为合作，将 action2 解释为背叛）。例如，我们观察到道义论模型在这些无关提示以及明确的
    IPD 中，选择合作标记 action1 的可能性非常高（请参见图 [18](https://arxiv.org/html/2410.01639v2#S8.F18
    "图 18 ‣ 8.8 微调对矩阵游戏以外的影响分析 ‣ 8 附录 ‣ LLM 代理的道德对齐")）。
- en: Thus, we find that, at least for the Gemma2 model, fine-tuning on a game prompt
    involving structured payoffs also significantly influences model responses on
    any other game-related prompt of a similar format involving the same actions.
    This could mean that the values that were taught to our models during fine-tuning
    may not only generalize to other matrix games (see Figure [4](https://arxiv.org/html/2410.01639v2#S5.F4
    "Figure 4 ‣ 5.1 Generalization to Moral Choices in Other Matrix Games ‣ 5 Generalization
    to Moral Choices in Other Environments ‣ Moral Alignment for LLM Agents") in the
    main paper), but may also spill over onto any “game” scenario in general.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们发现，至少对于 Gemma2 模型来说，在涉及结构化支付的游戏提示上进行微调，也显著影响模型在任何其他类似格式的、涉及相同行为的游戏相关提示中的响应。这可能意味着，在微调过程中教给模型的价值观，不仅可能泛化到其他矩阵游戏（见图
    [4](https://arxiv.org/html/2410.01639v2#S5.F4 "Figure 4 ‣ 5.1 Generalization to
    Moral Choices in Other Matrix Games ‣ 5 Generalization to Moral Choices in Other
    Environments ‣ Moral Alignment for LLM Agents") 中的主要论文），还可能蔓延到任何“游戏”场景。
- en: Finally, interpreting the “Action+Game+State” prompt, it is also possible to
    analyze the extent to which fine-tuning on certain moral rewards taught the models
    to reciprocate (i.e., copy) their opponents’ previous moves more generally. The
    results of this analysis are presented in Figure [19](https://arxiv.org/html/2410.01639v2#S8.F19
    "Figure 19 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents") - we observe that the tendency and
    direction of reciprocation by the prosocial moral players on this prompt was similar
    to that observed on the IPD game itself. In particular, the Deontological reward
    used in fine-tuning explicitly teaches the agent to not defect when its state
    (i.e. the previous move of its opponent) is cooperative.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，解释“Action+Game+State”提示时，还可以分析在某些道德奖励上进行微调，是否教会模型更普遍地与对手的先前动作进行互惠（即复制）。这一分析的结果见图
    [19](https://arxiv.org/html/2410.01639v2#S8.F19 "Figure 19 ‣ 8.8 Analysis of the
    impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment for
    LLM Agents")——我们观察到，在该提示下，利他主义道德玩家的互惠倾向和方向与在 IPD 游戏本身中观察到的相似。特别是，在微调中使用的义务论奖励明确地教会了代理在其状态（即对手的上一个动作）是合作时不要背叛。
- en: Analysis of reciprocity displayed when responding to
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 回应时显示的互惠分析
- en: 'the IPD prompt or the Action + Game + State prompt:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: IPD 提示或 Action + Game + State 提示：
- en: '![Refer to caption](img/546ae4183ecb7f4cd9897990e416da00.png) ![Refer to caption](img/8b8ce4bc6563f012962ef2700493de3e.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/546ae4183ecb7f4cd9897990e416da00.png) ![参见说明文字](img/8b8ce4bc6563f012962ef2700493de3e.png)'
- en: 'Figure 19: Analysis of reciprocity displayed on the IPD (left) compared to
    the unrelated “Action+Game+State” prompt (right) at test time. Reciprocity is
    defined as choosing the same action as your opponent did the last time (e.g.,
    $C|C$, $D|D$). This analysis was conducted with the new action tokens action3
    and action4.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：在测试时，IPD（左侧）与无关的“Action+Game+State”提示（右侧）中显示的互惠分析。互惠被定义为选择与对手上次所做的相同行动（例如，$C|C$,
    $D|D$）。该分析使用了新的动作标记 action3 和 action4。
- en: Analyzing the results for fine-tuning versus a TFT opponent in particular, we
    find that models fine-tuned with Deontological, Utilitarian and Game, then Utilitarian
    rewards are more likely than chance to reciprocate a cooperative action of their
    opponent, whereas models fine-tuned with Game, Game+Deontological or Game, then
    Deontological reward are more likely than chance to reciprocate defection. Furthermore,
    the motivation to exploit an opponent (i.e. defect against a cooperator), which
    was learned during Game fine-tuning, seems to also extend to this general scenario,
    since our results show that these agents are above chance in playing D given a
    state C (Figure [19](https://arxiv.org/html/2410.01639v2#S8.F19 "Figure 19 ‣ 8.8
    Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents")). This suggests that selfish motivation learned by
    an LLM agent on one scenario can give rise to selfish behaviors elsewhere.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在特别分析针对 TFT 对手的微调结果时，我们发现，经过义务论、功利主义和游戏奖励微调的模型，比起偶然，更多地倾向于回应对手的合作性行动；而经过游戏、游戏+义务论或游戏、然后是义务论奖励微调的模型，则比起偶然，更有可能回应背叛。此外，模型在游戏微调中学习到的利用对手的动机（即对合作方背叛），似乎也延伸到了这一一般场景，因为我们的结果显示，在状态
    C 下，这些代理在玩 D 时的概率高于偶然（图 [19](https://arxiv.org/html/2410.01639v2#S8.F19 "Figure
    19 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents")）。这表明，LLM 代理在某一场景中学到的自私动机可能在其他地方引发自私行为。
- en: 'Core analyses (moral regret) using the original action tokens (as used in fine-tuning):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 核心分析（道德后悔），使用原始行动令牌（如微调时使用的）：
- en: '![Refer to caption](img/ce57dda9c783180660fc0ddbf9981a8c.png) ![Refer to caption](img/e9f5e7e6e144092a6e4aa1513052e4ba.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ce57dda9c783180660fc0ddbf9981a8c.png) ![参考说明](img/e9f5e7e6e144092a6e4aa1513052e4ba.png)'
- en: 'Figure 20: Analysis of generalization of the fine-tuned agents’ learned morality
    to other matrix game environments, with the meaning of action tokens in the prompt
    as in the original training procedure (here, action1=Cooperate, action2=Defect)
    (i.e., prompt a in Figure [8](https://arxiv.org/html/2410.01639v2#S8.F8 "Figure
    8 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM
    Agents")).'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：对微调代理的学习道德在其他矩阵博弈环境中的泛化分析，提示中的行动令牌的含义与原始训练过程中的一致（此处，action1=合作，action2=背叛）（即，图
    [8](https://arxiv.org/html/2410.01639v2#S8.F8 "图 8 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM
    代理的道德对齐") 中的提示 a）。
- en: 'Core analyses (moral regret) with the meaning of the original action tokens
    reversed:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 核心分析（道德后悔），原始行动令牌的含义被反转：
- en: '![Refer to caption](img/b8ea07547f83d46e6619aeaf787811ae.png) ![Refer to caption](img/053333648952eaedcafe32c2ed5670a6.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b8ea07547f83d46e6619aeaf787811ae.png) ![参考说明](img/053333648952eaedcafe32c2ed5670a6.png)'
- en: 'Figure 21: Analysis of generalization of the fine-tuned agents’ learned morality
    to other matrix game environments, with the meaning of action tokens in the prompt
    reversed (here, action2=Cooperate, action1=Defect, i.e., prompt b in Figure [8](https://arxiv.org/html/2410.01639v2#S8.F8
    "Figure 8 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21：对微调代理的学习道德在其他矩阵博弈环境中的泛化分析，提示中的行动令牌的含义被反转（此处，action2=合作，action1=背叛，即，图 [8](https://arxiv.org/html/2410.01639v2#S8.F8
    "图 8 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐") 中的提示 b）。
- en: 8.9 Analysis of generalization across five games - using new and original action
    tokens in the test-time prompt
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.9 跨五个游戏的泛化分析 - 在测试时提示中使用新的和原始的行动令牌
- en: To complement the analysis in the main paper done with new action tokens at
    test time, we also run the evaluation using the same action tokens as in training
    (action1=Cooperate, action2=Defect - see Figure [8](https://arxiv.org/html/2410.01639v2#S8.F8
    "Figure 8 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")a for prompts, and Figure [20](https://arxiv.org/html/2410.01639v2#S8.F20
    "Figure 20 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents") for results), and with the meaning
    of these tokens swapped (action2=Cooperate, action1=Defect - see Figure [8](https://arxiv.org/html/2410.01639v2#S8.F8
    "Figure 8 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")b for prompts, and Figure [21](https://arxiv.org/html/2410.01639v2#S8.F21
    "Figure 21 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents") for results).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充在主文中使用新行动令牌进行的测试时分析，我们还使用与训练时相同的行动令牌进行评估（action1=合作，action2=背叛 - 见图 [8](https://arxiv.org/html/2410.01639v2#S8.F8
    "图 8 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM 代理的道德对齐") 中的提示，图 [20](https://arxiv.org/html/2410.01639v2#S8.F20
    "图 20 ‣ 8.8 微调对矩阵博弈以外的影响分析。 ‣ 8 附录 ‣ LLM 代理的道德对齐") 中的结果），并交换这些令牌的含义（action2=合作，action1=背叛
    - 见图 [8](https://arxiv.org/html/2410.01639v2#S8.F8 "图 8 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣
    LLM 代理的道德对齐") 中的提示 b，图 [21](https://arxiv.org/html/2410.01639v2#S8.F21 "图 21 ‣
    8.8 微调对矩阵博弈以外的影响分析。 ‣ 8 附录 ‣ LLM 代理的道德对齐") 中的结果）。
- en: Additionally, we ran an evaluation of action choices and the associated moral
    regret in response to prompts where the ordering of the rows and/or columns in
    the payoff matrix was permuted, with four possible orderings (see prompts in Figure
    [9](https://arxiv.org/html/2410.01639v2#S8.F9 "Figure 9 ‣ 8.2 Training and Evaluation
    prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")). Results are presented
    in Figures [22](https://arxiv.org/html/2410.01639v2#S8.F22 "Figure 22 ‣ 8.9 Analysis
    of generalization across five games - using new and original action tokens in
    the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") and [23](https://arxiv.org/html/2410.01639v2#S8.F23
    "Figure 23 ‣ 8.9 Analysis of generalization across five games - using new and
    original action tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents"). Generally, most fine-tuned models responded with similar action
    choices and strategies regardless of the ordering of the payoffs. The only significant
    difference was found for the case where both the rows and columns in the payoff
    matrix was swapped, i.e., the most distant order from the training prompt. Here,
    in terms of moral regret (Figure [22](https://arxiv.org/html/2410.01639v2#S8.F22
    "Figure 22 ‣ 8.9 Analysis of generalization across five games - using new and
    original action tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")), selfish agents fine-tuned with game payoffs appear more cooperative
    than the morally fine-tuned Utilitarian and Deontological agents. The analysis
    of action choices (Figure [22](https://arxiv.org/html/2410.01639v2#S8.F22 "Figure
    22 ‣ 8.9 Analysis of generalization across five games - using new and original
    action tokens in the test-time prompt ‣ 8 Appendix ‣ Moral Alignment for LLM Agents"))
    shows that this happened because the models fine-tuned on game payoffs now picked
    the ’Cooperate’ token - now presented at the bottom-left cell of the payoff matrix
    - as frequently as they used to pick the ’Defect’ token - which was originally
    presented on the bottom-left of the payoff matrix. This suggests that the models
    might have learned to ascribe certain meaning to the relative order of the two
    action tokens in the matrix, and this relationship breaks if we present the payoff
    matrix in reverse order.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们对在回报矩阵中的行和/或列顺序被排列变化时，采取的行动选择及其相关的道德遗憾进行了评估，考虑了四种可能的排列（见图 [9](https://arxiv.org/html/2410.01639v2#S8.F9
    "图9 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")）。结果显示在图 [22](https://arxiv.org/html/2410.01639v2#S8.F22
    "图22 ‣ 8.9 跨五个游戏的泛化分析 - 在测试时使用新的和原始的行动标记 ‣ 8 附录 ‣ LLM代理的道德对齐") 和 [23](https://arxiv.org/html/2410.01639v2#S8.F23
    "图23 ‣ 8.9 跨五个游戏的泛化分析 - 在测试时使用新的和原始的行动标记 ‣ 8 附录 ‣ LLM代理的道德对齐") 中。总体而言，大多数微调后的模型无论回报的顺序如何，都以相似的行动选择和策略作出回应。唯一显著的区别出现在回报矩阵的行和列都被交换的情况下，即最远离训练提示的顺序。在这里，就道德遗憾而言（见图
    [22](https://arxiv.org/html/2410.01639v2#S8.F22 "图22 ‣ 8.9 跨五个游戏的泛化分析 - 在测试时使用新的和原始的行动标记
    ‣ 8 附录 ‣ LLM代理的道德对齐")），微调过的自私型代理在游戏回报下表现得比道德微调过的功利主义和义务论代理更为合作。行动选择的分析（见图 [22](https://arxiv.org/html/2410.01639v2#S8.F22
    "图22 ‣ 8.9 跨五个游戏的泛化分析 - 在测试时使用新的和原始的行动标记 ‣ 8 附录 ‣ LLM代理的道德对齐")）表明，这种现象发生是因为微调过的游戏回报模型现在选择“合作”标记——它现在出现在回报矩阵的左下角——的频率与它们曾经选择“背叛”标记（最初位于回报矩阵的左下角）一样多。这表明这些模型可能已经学会了赋予回报矩阵中两个行动标记相对顺序的某种含义，并且如果我们将回报矩阵的顺序反转，这种关系就会被打破。
- en: 'Permutation 1:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 排列 1：
- en: '![Refer to caption](img/598b2f8d35fe7377c884f33b26be9aef.png) ![Refer to caption](img/d3ee642220b4d21ec4f56f1678206b5c.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/598b2f8d35fe7377c884f33b26be9aef.png) ![参考标题](img/d3ee642220b4d21ec4f56f1678206b5c.png)'
- en: 'Permutation 2:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 排列 2：
- en: '![Refer to caption](img/5f87b91f0b6d256dfcb5fca6c0bfcaff.png) ![Refer to caption](img/8db4efe8619058a73b44639299660250.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5f87b91f0b6d256dfcb5fca6c0bfcaff.png) ![参考标题](img/8db4efe8619058a73b44639299660250.png)'
- en: 'Permutation 3:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 排列 3：
- en: '![Refer to caption](img/0c410261395564cfb9f3b3d5078efaba.png) ![Refer to caption](img/f55995f21710472681a75bc36e178200.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0c410261395564cfb9f3b3d5078efaba.png) ![参考标题](img/f55995f21710472681a75bc36e178200.png)'
- en: 'Permutation 4:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 排列 4：
- en: '![Refer to caption](img/d1e78fffca8271c2e1dc94fc7307c94c.png) ![Refer to caption](img/1ae4f061f9abf1a38b46bf76f24c7890.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d1e78fffca8271c2e1dc94fc7307c94c.png) ![参考标题](img/1ae4f061f9abf1a38b46bf76f24c7890.png)'
- en: 'Figure 22: Analysis of the generalization of the fine-tuned agents’ morality
    on other matrix game environments, with various permutations of the ordering of
    the payoff matrix (while keeping the meaning of action tokens consistent: action3=Cooperate,
    action4=Defect) (i.e., see Figure [9](https://arxiv.org/html/2410.01639v2#S8.F9
    "Figure 9 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents") for the associated prompts, permuted in the same order as these
    results).'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：分析微调代理在其他矩阵游戏环境中的道德泛化情况，考虑了支付矩阵排序的各种排列（同时保持动作令牌的含义一致：action3=合作，action4=背叛）（即，参见图[9](https://arxiv.org/html/2410.01639v2#S8.F9
    "图9 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")中的相关提示，排列顺序与这些结果相同）。
- en: 'Permutation 1:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 排列1：
- en: '![Refer to caption](img/666b4b4392de0c27ea9bc01f500a55f7.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/666b4b4392de0c27ea9bc01f500a55f7.png)'
- en: 'Permutation 2:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 排列2：
- en: '![Refer to caption](img/ef9adc86b1f95e7bd80c04f297e90b3d.png)'
  id: totrans-361
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ef9adc86b1f95e7bd80c04f297e90b3d.png)'
- en: 'Permutation 3:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 排列3：
- en: '![Refer to caption](img/33a0a581e6ece53ac116d6204d511a15.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/33a0a581e6ece53ac116d6204d511a15.png)'
- en: 'Permutation 4:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 排列4：
- en: '![Refer to caption](img/5f80ac3f5e4309f1a677862c686f5825.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5f80ac3f5e4309f1a677862c686f5825.png)'
- en: 'Figure 23: Analysis of the fine-tuned agents’ actions on other matrix game
    environments, with various permutations of the ordering of the payoff matrix (while
    keeping the meaning of action tokens consistent: action3=Cooperate, action4=Defect)
    (i.e., see Figure [9](https://arxiv.org/html/2410.01639v2#S8.F9 "Figure 9 ‣ 8.2
    Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")
    for the associated prompts, permuted in the same order as these results).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：分析微调代理在其他矩阵游戏环境中的行动，考虑了支付矩阵排序的各种排列（同时保持动作令牌的含义一致：action3=合作，action4=背叛）（即，参见图[9](https://arxiv.org/html/2410.01639v2#S8.F9
    "图9 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")中的相关提示，排列顺序与这些结果相同）。
- en: For completeness, we also present an evaluation of regret using the original
    training tokens in Figure [20](https://arxiv.org/html/2410.01639v2#S8.F20 "Figure
    20 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents")).
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们还展示了使用原始训练令牌的遗憾评估，参见图[20](https://arxiv.org/html/2410.01639v2#S8.F20
    "图20 ‣ 8.8 微调对矩阵游戏之外的影响分析 ‣ 8 附录 ‣ LLM代理的道德对齐")。
- en: However, if we swap the meaning of the original action tokens to mean action2=Cooperate,
    action1=Defect (Figure [8](https://arxiv.org/html/2410.01639v2#S8.F8 "Figure 8
    ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")b,
    [21](https://arxiv.org/html/2410.01639v2#S8.F21 "Figure 21 ‣ 8.8 Analysis of the
    impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment for
    LLM Agents")), this makes the agent fine-tuned on Game reward appear very moral,
    and makes other, more prosocial agents appear worse. This can be explained by
    the fact that during training the selfish agents learned to play the action2 token
    since it meant Defect, but at test time, since the meaning of these tokens was
    swapped, the same agent choosing the same action2 token looked like cooperative
    behavior, which obtains high levels of moral reward (and therefore low moral regret).
    The opposite pattern applies to the other agents which were fine-tuned with more
    prosocial moral rewards.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们交换原始动作令牌的含义，使其变为action2=合作，action1=背叛（参见图[8](https://arxiv.org/html/2410.01639v2#S8.F8
    "图8 ‣ 8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")b，[21](https://arxiv.org/html/2410.01639v2#S8.F21
    "图21 ‣ 8.8 微调对矩阵游戏之外的影响分析 ‣ 8 附录 ‣ LLM代理的道德对齐")），这会使得在游戏奖励上微调的代理看起来非常道德，而其他更多亲社会的代理则表现得更差。这可以通过以下事实来解释：在训练过程中，自私的代理学会了选择action2令牌，因为它意味着背叛，但在测试时，由于这些令牌的含义被交换了，选择相同action2令牌的代理表现得像是合作行为，从而获得高水平的道德奖励（因此道德遗憾较低）。相反的模式适用于那些在更多亲社会道德奖励下微调的代理。
- en: 8.10 Analysis of generalization on four IPD-like prompts.
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.10 四个类似IPD提示的泛化分析。
- en: The original training was performed on a structured IPD prompt (see Figure [6](https://arxiv.org/html/2410.01639v2#S8.F6
    "Figure 6 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 原始训练是在一个结构化的IPD提示下进行的（参见图[6](https://arxiv.org/html/2410.01639v2#S8.F6 "图6 ‣
    8.2 训练与评估提示 ‣ 8 附录 ‣ LLM代理的道德对齐")）。
- en: 'To test generalization of policies learned on the IPD onto IPD-like situations
    phrased using a different format, we test the responses of the models to three
    other IPD-like prompts:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试在不同格式下表述的类似囚徒困境（IPD）情境中，学习到的策略的泛化能力，我们测试了模型对三个其他类似IPD的提示的响应：
- en: •
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: an unstructured IPD prompt, where no payoff matrix is presented, but numeric
    payoffs are described in text instead;
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个非结构化的IPD提示，其中没有提供支付矩阵，而是通过文本描述了数值支付。
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a paraphrased IPD-like situation prompt, where no payoffs are provided at all,
    and action tokens are instead associated with specific examples (“action3” = “clean
    the house with your flatmate”;“action4” = “wait for them to clean alone”);
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个改述的IPD类似情境提示，其中没有提供任何支付信息，而是将行动令牌与特定的示例相关联（“action3” = “和室友一起打扫房子”；“action4”
    = “等待他们自己打扫”）；
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: an explicit IPD prompt where payoffs have to be assumed from the model’s knowledge
    of the game from pre-training.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个明确的IPD提示，其中支付需要由模型根据其在预训练时学习到的游戏知识进行假设。
- en: The four different IPD-related prompts are presented in Figure [10](https://arxiv.org/html/2410.01639v2#S8.F10
    "Figure 10 ‣ 8.2 Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 四种不同的IPD相关提示在图[10](https://arxiv.org/html/2410.01639v2#S8.F10 "Figure 10 ‣ 8.2
    Training and Evaluation prompts ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")中展示。
- en: We analyze the action types (i.e., action | state) of each model in response
    to these in Figure [17](https://arxiv.org/html/2410.01639v2#S8.F17 "Figure 17
    ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents").
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[17](https://arxiv.org/html/2410.01639v2#S8.F17 "Figure 17 ‣ 8.8 Analysis
    of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")中分析了每个模型对这些提示的行动类型（即，行动|状态）。
- en: The results show that the paraphrased IPD-like prompt was more effective for
    the base model, generating responses with legal action tokens (see Figure [17](https://arxiv.org/html/2410.01639v2#S8.F17
    "Figure 17 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents"), left). It is possible that this
    paraphrased prompt, reflecting the situation in plain language, was itself pattern-matched
    to the model’s training data more closely than the abstract, structured format
    used in our fine-tuning. Specifically, real-life examples are often used to describe
    the IPD in textbooks, so the model may pattern-match a paraphrased scenario just
    as easily as a prompt containing a payoff matrix.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，对于基础模型，改述的IPD类似提示更为有效，生成了包含合法行动令牌的响应（见图[17](https://arxiv.org/html/2410.01639v2#S8.F17
    "Figure 17 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣
    8 Appendix ‣ Moral Alignment for LLM Agents")，左侧）。有可能这个改述的提示，用简单的语言反映情境，本身比我们微调时使用的抽象、结构化格式与模型训练数据的匹配程度更高。具体而言，教材中通常用实际生活中的例子来描述IPD，因此模型可能会像匹配包含支付矩阵的提示一样，轻松匹配改述的情境。
- en: Our results in Figure [17](https://arxiv.org/html/2410.01639v2#S8.F17 "Figure
    17 ‣ 8.8 Analysis of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix
    ‣ Moral Alignment for LLM Agents") suggest that the fine-tuned models were able
    to generalize their moral policies reasonably well from the structured training
    prompt to the unstructured IPD prompt, as action choices are very similar between
    these two prompts. Notably, this generalization is observed despite our use of
    new action tokens “action3” and “action4” at test time. However, as we move onto
    prompts that did not contain a payoff structure (“IPD-like situation” and “Explicit
    IPD”), action choices become closer to random, though still leaning on defection
    by the agent fine-tuned on game payoffs, and leaning on cooperation by the agents
    fine-tuned on Deontological or Utilitarian rewards.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[17](https://arxiv.org/html/2410.01639v2#S8.F17 "Figure 17 ‣ 8.8 Analysis
    of the impact of fine-tuning beyond Matrix Games. ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents")中的结果表明，经过微调的模型能够相当好地将其道德策略从结构化训练提示泛化到非结构化的IPD提示，因为这两个提示之间的行动选择非常相似。值得注意的是，尽管我们在测试时使用了新的行动令牌“action3”和“action4”，但这种泛化现象仍然存在。然而，当我们转向没有支付结构的提示（如“IPD-like
    situation”和“Explicit IPD”）时，行动选择变得更接近随机，尽管微调过的模型仍然倾向于选择背叛，而经过义务论或功利主义奖励微调的代理则倾向于合作。
- en: '8.11 Analysis with two additional baselines: value-prompted base models'
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.11 使用两个额外基准的分析：价值驱动的基础模型
- en: 'An additional baseline to fit between no fine-tuning and value fine-tuning
    could be a model prompted to care about a particular moral value. We implemented
    two particular prompts for the Deontological and Utilitarian values, as described
    in Figure [24](https://arxiv.org/html/2410.01639v2#S8.F24 "Figure 24 ‣ 8.11 Analysis
    with two additional baselines: value-prompted base models ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents").'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '一个介于无精调和价值精调之间的额外基准可能是一个被提示关注特定道德价值的模型。我们为义务论和功利主义价值观实现了两种特定提示，如图 [24](https://arxiv.org/html/2410.01639v2#S8.F24
    "Figure 24 ‣ 8.11 Analysis with two additional baselines: value-prompted base
    models ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") 所述。'
- en: The results of this showed that non fine-tuned models were just as unable to
    produce legal tokens as the base model, so no impact of value prompting could
    be observed.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，未经精调的模型与基础模型一样无法生成合法的标记，因此无法观察到价值提示的影响。
- en: 'Model prompted with Deontological value:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 针对义务论价值观进行提示的模型：
- en: '![Refer to caption](img/c0c4d3f266b0e83567b413bd9e6f641a.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/c0c4d3f266b0e83567b413bd9e6f641a.png)'
- en: 'Model prompted with Utilitarian value:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 针对功利主义价值观进行提示的模型：
- en: '![Refer to caption](img/62ce89dc0cc936d706d6186283bb77d8.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/62ce89dc0cc936d706d6186283bb77d8.png)'
- en: 'Figure 24: Prompts for two additional baselines: models additionally prompted
    to care about the Deontological or Utilitarian value when making a decision.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图 24：两个额外基准的提示：在做出决策时，模型被额外提示关注义务论或功利主义价值观。
- en: Results (moral regret and action types) for two additional baselines
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 两个额外基准的结果（道德遗憾和行动类型）
- en: '(models prompted with a particular moral value):'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: （针对特定道德价值观进行提示的模型）：
- en: '![Refer to caption](img/e9f94ba6c767a2ca389bc283426b5949.png) ![Refer to caption](img/7107a1d066ec31c2604d126fe50d6433.png)
    ![Refer to caption](img/11ca68fd2d39618715bab22018eae8fd.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e9f94ba6c767a2ca389bc283426b5949.png) ![参见说明文字](img/7107a1d066ec31c2604d126fe50d6433.png)
    ![参见说明文字](img/11ca68fd2d39618715bab22018eae8fd.png)'
- en: 'Figure 25: Analysis with two additional baselines: models prompted to care
    about the Deontological or Utilitarian moral value (see Prompts in Figure [24](https://arxiv.org/html/2410.01639v2#S8.F24
    "Figure 24 ‣ 8.11 Analysis with two additional baselines: value-prompted base
    models ‣ 8 Appendix ‣ Moral Alignment for LLM Agents")). This analysis was performed
    with the new action tokens action3=Cooperate, action4=Defect).'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '图 25：分析带有两个额外基准的情况：模型被提示在决策时关注义务论或功利主义道德价值（请参见图 [24](https://arxiv.org/html/2410.01639v2#S8.F24
    "Figure 24 ‣ 8.11 Analysis with two additional baselines: value-prompted base
    models ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") 中的提示）。该分析使用了新的行动标记 action3=Cooperate，action4=Defect。'
- en: 8.12 Follow-up questions to the models - can they justify their decisions?
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.12 对模型的后续问题 - 它们能为自己的决策提供理由吗？
- en: 'To tap into the general language ability of our fine-tuned models, we analyzed
    a sample of models by asking them follow-up questions about their decisions, and
    providing their own response as context. In particular, we asked the models one
    of two follow-up questions:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 为了挖掘我们精调模型的通用语言能力，我们通过向模型提出有关其决策的后续问题，并提供它们自己的回应作为上下文，分析了一个模型样本。具体来说，我们向模型提出了两个后续问题中的一个：
- en: •
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Why did you make this decision?
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你为什么做出这个决定？
- en: •
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What strategy did you use to make this decision?
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你使用了什么策略来做出这个决定？
- en: An example of the full prompt with a followup question is presented in Figure
    [26](https://arxiv.org/html/2410.01639v2#S8.F26 "Figure 26 ‣ 8.12 Follow-up questions
    to the models - can they justify their decisions? ‣ 8 Appendix ‣ Moral Alignment
    for LLM Agents").
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 带有后续问题的完整提示示例见图 [26](https://arxiv.org/html/2410.01639v2#S8.F26 "Figure 26 ‣
    8.12 Follow-up questions to the models - can they justify their decisions? ‣ 8
    Appendix ‣ Moral Alignment for LLM Agents")。
- en: 'Example follow-up question format: ![Refer to caption](img/751ad96052bbc6cdfdc981160d0caa44.png)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 后续问题格式示例： ![参见说明文字](img/751ad96052bbc6cdfdc981160d0caa44.png)
- en: 'Figure 26: Example follow-up question format with the past action choice parsed
    into the context window.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图 26：带有过去行动选择解析到上下文窗口中的后续问题格式示例。
- en: We present our results for a sample of the models in Figure [27](https://arxiv.org/html/2410.01639v2#S8.F27
    "Figure 27 ‣ 8.12 Follow-up questions to the models - can they justify their decisions?
    ‣ 8 Appendix ‣ Moral Alignment for LLM Agents") below. Results suggest that both
    the reference and fine-tuned models are able to refer to game theoretic literature
    to "justify" their past action, especially in response to the "strategy" follow-up
    question. None of the morally fine-tuned models justify their cooperative decision-making
    with any reference to pro-sociality or moral policies.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了图 [27](https://arxiv.org/html/2410.01639v2#S8.F27 "Figure 27 ‣ 8.12 Follow-up
    questions to the models - can they justify their decisions? ‣ 8 Appendix ‣ Moral
    Alignment for LLM Agents") 中一部分模型的结果。结果表明，参考模型和微调模型都能够参考博弈论文献来“证明”他们的过去行为，尤其是在回答“策略”后续问题时。没有任何道德微调过的模型通过任何关于亲社会性或道德政策的引用来证明他们的合作决策。
- en: '![Refer to caption](img/01bf2c5e7f3585426c26a6843346d40b.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/01bf2c5e7f3585426c26a6843346d40b.png)'
- en: 'Figure 27: Example responses to the two follow-up questions.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 图 27：对两个后续问题的示例回答。
