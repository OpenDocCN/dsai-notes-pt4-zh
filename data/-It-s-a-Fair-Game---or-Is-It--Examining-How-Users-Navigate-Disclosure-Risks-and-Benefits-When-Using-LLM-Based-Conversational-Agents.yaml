- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 13:06:32'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025年1月11日 13:06:32
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks
    and Benefits When Using LLM-Based Conversational Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: “这是一个公平的游戏”，还是它并非如此？探讨用户在使用基于大型语言模型（LLM）的对话代理时如何权衡披露风险与收益
- en: 来源：[https://arxiv.org/html/2309.11653/](https://arxiv.org/html/2309.11653/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2309.11653/](https://arxiv.org/html/2309.11653/)
- en: Zhiping Zhang [zhip.zhang@northeastern.edu](mailto:zhip.zhang@northeastern.edu)
    Northeastern UniversityBostonMAUSA [0000-0001-6794-0054](https://orcid.org/0000-0001-6794-0054
    "ORCID identifier") ,  Michelle Jia [michellj@andrew.cmu.edu](mailto:michellj@andrew.cmu.edu)
    Carnegie Mellon UniversityPittsburghPAUSA [0009-0008-5685-4618](https://orcid.org/0009-0008-5685-4618
    "ORCID identifier") ,  Hao-Ping (Hank) Lee [haopingl@cs.cmu.edu](mailto:haopingl@cs.cmu.edu)
    Carnegie Mellon UniversityPittsburghPAUSA [0000-0002-8063-1034](https://orcid.org/0000-0002-8063-1034
    "ORCID identifier") ,  Bingsheng Yao [arthuryao33@gmail.com](mailto:arthuryao33@gmail.com)
    Rensselaer Polytechnic InstituteTroyNYUSA [0009-0004-8329-4610](https://orcid.org/0009-0004-8329-4610
    "ORCID identifier") ,  Sauvik Das [sauvik@cmu.edu](mailto:sauvik@cmu.edu) Carnegie
    Mellon UniversityPittsburghPAUSA [0000-0002-9073-8054](https://orcid.org/0000-0002-9073-8054
    "ORCID identifier") ,  Ada Lerner [ada@ccs.neu.edu](mailto:ada@ccs.neu.edu) Northeastern
    UniversityBostonMAUSA [0000-0002-3238-2109](https://orcid.org/0000-0002-3238-2109
    "ORCID identifier") ,  Dakuo Wang [d.wang@neu.edu](mailto:d.wang@neu.edu) Northeastern
    UniversityBostonMAUSA [0000-0001-9371-9441](https://orcid.org/0000-0001-9371-9441
    "ORCID identifier")  and  Tianshi Li [tia.li@northeastern.edu](mailto:tia.li@northeastern.edu)
    [0000-0003-0877-5727](https://orcid.org/0000-0003-0877-5727 "ORCID identifier")
    Northeastern UniversityBostonMAUSA [0000-0003-0877-5727](https://orcid.org/0000-0003-0877-5727
    "ORCID identifier")(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhiping Zhang [zhip.zhang@northeastern.edu](mailto:zhip.zhang@northeastern.edu)
    诺思伊斯特大学 波士顿 马萨诸塞州 美国 [0000-0001-6794-0054](https://orcid.org/0000-0001-6794-0054
    "ORCID identifier")，Michelle Jia [michellj@andrew.cmu.edu](mailto:michellj@andrew.cmu.edu)
    卡内基梅隆大学 匹兹堡 宾夕法尼亚州 美国 [0009-0008-5685-4618](https://orcid.org/0009-0008-5685-4618
    "ORCID identifier")，Hao-Ping (Hank) Lee [haopingl@cs.cmu.edu](mailto:haopingl@cs.cmu.edu)
    卡内基梅隆大学 匹兹堡 宾夕法尼亚州 美国 [0000-0002-8063-1034](https://orcid.org/0000-0002-8063-1034
    "ORCID identifier")，Bingsheng Yao [arthuryao33@gmail.com](mailto:arthuryao33@gmail.com)
    伦斯勒理工学院 特洛伊 纽约州 美国 [0009-0004-8329-4610](https://orcid.org/0009-0004-8329-4610
    "ORCID identifier")，Sauvik Das [sauvik@cmu.edu](mailto:sauvik@cmu.edu) 卡内基梅隆大学
    匹兹堡 宾夕法尼亚州 美国 [0000-0002-9073-8054](https://orcid.org/0000-0002-9073-8054 "ORCID
    identifier")，Ada Lerner [ada@ccs.neu.edu](mailto:ada@ccs.neu.edu) 诺思伊斯特大学 波士顿
    马萨诸塞州 美国 [0000-0002-3238-2109](https://orcid.org/0000-0002-3238-2109 "ORCID identifier")，Dakuo
    Wang [d.wang@neu.edu](mailto:d.wang@neu.edu) 诺思伊斯特大学 波士顿 马萨诸塞州 美国 [0000-0001-9371-9441](https://orcid.org/0000-0001-9371-9441
    "ORCID identifier") 和 Tianshi Li [tia.li@northeastern.edu](mailto:tia.li@northeastern.edu)
    [0000-0003-0877-5727](https://orcid.org/0000-0003-0877-5727 "ORCID identifier")
    诺思伊斯特大学 波士顿 马萨诸塞州 美国 [0000-0003-0877-5727](https://orcid.org/0000-0003-0877-5727
    "ORCID identifier")（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The widespread use of Large Language Model (LLM)-based conversational agents
    (CAs), especially in high-stakes domains, raises many privacy concerns. Building
    ethical LLM-based CAs that respect user privacy requires an in-depth understanding
    of the privacy risks that concern users the most. However, existing research,
    primarily model-centered, does not provide insight into users’ perspectives. To
    bridge this gap, we analyzed sensitive disclosures in real-world ChatGPT conversations
    and conducted semi-structured interviews with 19 LLM-based CA users. We found
    that users are constantly faced with trade-offs between privacy, utility, and
    convenience when using LLM-based CAs. However, users’ erroneous mental models
    and the dark patterns in system design limited their awareness and comprehension
    of the privacy risks. Additionally, the human-like interactions encouraged more
    sensitive disclosures, which complicated users’ ability to navigate the trade-offs.
    We discuss practical design guidelines and the needs for paradigm shifts to protect
    the privacy of LLM-based CA users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型（LLM）的对话代理（CAs）在各个领域，特别是在高风险领域的广泛应用，带来了许多隐私问题。构建尊重用户隐私的伦理化LLM对话代理需要深入理解用户最关注的隐私风险。然而，现有的研究主要集中在模型层面，并未从用户的角度提供深入的见解。为了弥补这一空白，我们分析了真实世界中ChatGPT对话中的敏感披露，并对19位LLM对话代理用户进行了半结构化访谈。我们发现，在使用LLM对话代理时，用户不断面临隐私、效用和便利性之间的权衡。然而，用户错误的心理模型和系统设计中的暗黑模式限制了他们对隐私风险的意识和理解。此外，人类化的互动促使更多敏感信息的披露，这进一步复杂化了用户对这些权衡的处理能力。我们讨论了保护LLM对话代理用户隐私的实际设计指南，并提出了
    paradigm shifts 的需求。
- en: 'Large language models (LLM), Artificial general intelligence (AGI), Conversational
    agents, Chatbots, Privacy, Contextual integrity, Privacy risks, Privacy-enhancing
    technologies, Interviews, Empirical studies^†^†journalyear: 2024^†^†copyright:
    rightsretained^†^†conference: Proceedings of the CHI Conference on Human Factors
    in Computing Systems; May 11–16, 2024; Honolulu, HI, USA^†^†booktitle: Proceedings
    of the CHI Conference on Human Factors in Computing Systems (CHI ’24), May 11–16,
    2024, Honolulu, HI, USA^†^†doi: 10.1145/3613904.3642385^†^†isbn: 979-8-4007-0330-0/24/05^†^†ccs:
    Security and privacy Human and societal aspects of security and privacy^†^†ccs:
    Computing methodologies Discourse, dialogue and pragmatics^†^†ccs: Human-centered
    computing Human computer interaction (HCI)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLM）、人工通用智能（AGI）、对话代理、聊天机器人、隐私、情境完整性、隐私风险、隐私增强技术、访谈、实证研究^†^†journalyear:
    2024^†^†copyright: rightsretained^†^†conference: 计算机系统人因学会议论文集（CHI 会议），2024年5月11日至16日，美国夏威夷檀香山^†^†booktitle:
    计算机系统人因学会议论文集（CHI ’24），2024年5月11日至16日，美国夏威夷檀香山^†^†doi: 10.1145/3613904.3642385^†^†isbn:
    979-8-4007-0330-0/24/05^†^†ccs: 安全与隐私 人类与社会方面的安全与隐私^†^†ccs: 计算方法 话语、对话与语用学^†^†ccs:
    以人为本的计算 人机交互（HCI）'
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: LLM-based conversational agents (CAs), such as ChatGPT, are increasingly being
    incorporated into high-stakes application domains including healthcare (Leonard,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib41); Fox, [2023](https://arxiv.org/html/2309.11653v2#bib.bib19)),
    finance (Estrada, [2023](https://arxiv.org/html/2309.11653v2#bib.bib17); Ferreira,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib18); Taver, [2023](https://arxiv.org/html/2309.11653v2#bib.bib67)),
    and personal counseling (Germain, [2023](https://arxiv.org/html/2309.11653v2#bib.bib22);
    Kimmel, [2023](https://arxiv.org/html/2309.11653v2#bib.bib39)). To access this
    functionality, users must often disclose their private medical records, payslips,
    or personal trauma (e.g., [Figure 1](https://arxiv.org/html/2309.11653v2#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")),
    not only to the organizations that host the LLMs themselves but also to third-parties
    that build applications on top of the LLMs. These disclosures, in turn, can expose
    users to a whole suite of emerging privacy and security risks (Weidinger et al.,
    [2021](https://arxiv.org/html/2309.11653v2#bib.bib72); Peris et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib59);
    Carlini et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib10), [2022](https://arxiv.org/html/2309.11653v2#bib.bib9)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的对话代理（CAs），如ChatGPT，正越来越多地被纳入高风险应用领域，包括医疗保健（Leonard, [2023](https://arxiv.org/html/2309.11653v2#bib.bib41);
    Fox, [2023](https://arxiv.org/html/2309.11653v2#bib.bib19)）、金融（Estrada, [2023](https://arxiv.org/html/2309.11653v2#bib.bib17);
    Ferreira, [2023](https://arxiv.org/html/2309.11653v2#bib.bib18); Taver, [2023](https://arxiv.org/html/2309.11653v2#bib.bib67)）以及个人辅导（Germain,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib22); Kimmel, [2023](https://arxiv.org/html/2309.11653v2#bib.bib39)）。为了访问这些功能，用户通常需要披露他们的私人医疗记录、薪资单或个人创伤（例如，[图1](https://arxiv.org/html/2309.11653v2#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents)")），不仅要向托管LLM的组织披露，还需要向在LLM上构建应用程序的第三方披露。这些披露反过来可能会使用户面临一系列新兴的隐私和安全风险（Weidinger
    等, [2021](https://arxiv.org/html/2309.11653v2#bib.bib72); Peris 等, [2023](https://arxiv.org/html/2309.11653v2#bib.bib59);
    Carlini 等, [2021](https://arxiv.org/html/2309.11653v2#bib.bib10), [2022](https://arxiv.org/html/2309.11653v2#bib.bib9)）。
- en: '![Refer to caption](img/763bc486849c1ab01045397dd2c0a283.png)\Description'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见图例](img/763bc486849c1ab01045397dd2c0a283.png)\Description'
- en: '[A fictional ChatGPT user’s chat screenshot]This figure illustrates a sensitive
    disclosure conversation with ChatGPT 4, with highlighted notations beside the
    screenshot. The conversation content shown in the image: User: Are you capable
    of providing extended description of ICD-10-CM diagnostic codes? ChatGPT: Yes,
    I can provide descriptions for ICD-10-CM diagnostic codes. Please provide the
    code you want described. User: This is the email sent by my doctor. Any problems
    about the diagnosis results? Dear Johnathon Lara, I hope this email finds you
    well. I’m writing to inform you of the results from the ICD-10-CM tests. As you
    suggested, I highlight the results here for you: ICD-10-CM score : D51.8, G4789,
    G47.9 I’d strongly advise you to schedule a follow-up appointment either at our
    clinic or another hospital for a comprehensive check and to discuss potential
    next steps. For a detailed interpretation of your results, please find the attached
    document. Please don’t hesitate to reach out if you have any questions or concerns.
    Best regards, Dr. Eleanor Mitchell Wonderland Medical Center, 1234 Wonderland,
    Earthe-center, AA, 56789 Tel: (111)123-4567 “Dear Johnathon Lara” was highlighted
    with a notation “User’s name”. “ICD-10-CM score: D51.8, G4789, G47.9” was highlighted
    with “Diagnosis results”, “Dr. Eleanor Mitchell; Wonderland Medical Center, 1234
    Wonderland, Earthe-center, AA, 56789; Tel: (111)123-4567” was marked with “Doctor’s
    information.”'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[虚构的 ChatGPT 用户聊天截图] 该图展示了一次与 ChatGPT 4 进行的敏感信息披露对话，截图旁边有高亮标注。图中显示的对话内容：用户：你能提供
    ICD-10-CM 诊断代码的详细描述吗？ ChatGPT：是的，我可以提供 ICD-10-CM 诊断代码的描述。请提供你希望描述的代码。 用户：这是我医生发来的电子邮件。关于诊断结果有什么问题吗？
    亲爱的 Johnathon Lara， 希望这封邮件能让你安好。我写信是为了告知你 ICD-10-CM 检测结果。如你所建议，我在此为你高亮显示结果： ICD-10-CM
    结果：D51.8, G4789, G47.9 我强烈建议你尽快预约跟进检查，无论是在我们诊所还是其他医院，进行全面检查并讨论下一步可能的措施。有关你结果的详细解读，请查看附加文件。如有任何问题或疑虑，请随时与我联系。此致，敬礼，Dr.
    Eleanor Mitchell Wonderland 医疗中心，1234 Wonderland, Earthe-center, AA, 56789 电话：(111)123-4567
    “亲爱的 Johnathon Lara”被高亮标注为“用户的名字”。“ICD-10-CM 结果：D51.8, G4789, G47.9”被高亮为“诊断结果”，“Dr.
    Eleanor Mitchell；Wonderland 医疗中心，1234 Wonderland, Earthe-center, AA, 56789；电话：(111)123-4567”被标注为“医生信息”。'
- en: 'Figure 1. A fictional example of sensitive disclosure to ChatGPT inspired by
    real-world use cases: A user shared their doctor’s email and ICD-10-CM diagnosis
    results with ChatGPT upon its request. And then ChatGPT interpreted the codes,
    indicating the user had multiple diseases. Three issues are demonstrated in the
    example: 1\. disclosed PII (name) and non-identifiable but sensitive information
    (diagnosis results); 2\. disclosed other person’s information (doctor’s information);
    3\. ChatGPT actively requested for detailed information from the user which encouraged
    user’s disclosure behavior.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 一个虚构的 ChatGPT 敏感信息披露示例，灵感来源于真实世界的使用案例：用户根据 ChatGPT 的请求，分享了医生的电子邮件和 ICD-10-CM
    诊断结果。随后 ChatGPT 解释了这些代码，表明用户有多种疾病。此示例展示了三个问题：1. 披露了个人身份信息（姓名）和非可识别但敏感的信息（诊断结果）；2.
    披露了他人的信息（医生信息）；3. ChatGPT 主动请求用户提供详细信息，鼓励了用户的披露行为。
- en: There are two main types of privacy risks from LLM-based CAs. The first type
    includes traditional security and privacy risks, such as data breaches and the
    use or sale of personal data (Kshetri, [2023](https://arxiv.org/html/2309.11653v2#bib.bib40)).
    Most popular LLM-based CAs operate on the cloud due to computing power constraints
    and content moderation requirements. However, users lose control over their chat
    logs once they leave their devices, making them vulnerable to security and privacy
    risks. The second type is more unique and inherent to LLMs — i.e., memorization
    risks. Prior research has shown that LLMs memorize details in the training data
    and can leak this training data in response to specific prompting techniques (Carlini
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib10), [2022](https://arxiv.org/html/2309.11653v2#bib.bib9);
    Zhang et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib76)). As current
    LLM-based CAs (e.g., ChatGPT, Bard) use user data to train their models periodically,
    there is a risk that sensitive information in one user’s input may be memorized
    by the model and leaked in response to others’ prompts. Although language models
    have also been used in traditional CAs (e.g., Alexa, Siri), they have a more constrained
    use scenario (e.g., turning on lights, domain-specific Q&A) (Shalaby et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib63)).
    Conversely, the open-ended, human-like nature of LLM-based CAs provides more opportunities
    for users to disclose personal information, potentially increasing the scale and
    intensity of both types of risks compared to the older paradigms of CAs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLM）的虚拟助手存在两种主要的隐私风险。第一种类型包括传统的安全和隐私风险，例如数据泄露以及个人数据的使用或出售（Kshetri，[2023](https://arxiv.org/html/2309.11653v2#bib.bib40)）。由于计算能力的限制和内容审核的需求，大多数流行的基于LLM的虚拟助手都运行在云端。然而，一旦用户的聊天记录离开他们的设备，他们就失去了对这些记录的控制，从而使其面临安全和隐私风险。第二种类型则更加独特，且与LLM本身固有的特性相关——即记忆风险。先前的研究表明，LLM会记住训练数据中的细节，并可能通过特定的提示技巧泄露这些训练数据（Carlini
    et al.，[2021](https://arxiv.org/html/2309.11653v2#bib.bib10)，[2022](https://arxiv.org/html/2309.11653v2#bib.bib9)；Zhang
    et al.，[2021](https://arxiv.org/html/2309.11653v2#bib.bib76)）。由于当前基于LLM的虚拟助手（例如，ChatGPT，Bard）会定期使用用户数据来训练其模型，因此存在一种风险，即某个用户的输入中的敏感信息可能会被模型记住，并在响应其他用户的提示时泄露出来。尽管传统的虚拟助手（例如，Alexa，Siri）也使用过语言模型，但它们的使用场景较为有限（例如，开关灯，特定领域的问答）（Shalaby
    et al.，[2020](https://arxiv.org/html/2309.11653v2#bib.bib63)）。相反，基于LLM的虚拟助手具有人类般开放的特性，这使得用户有更多机会泄露个人信息，从而可能增加与传统虚拟助手相比，两种风险的规模和强度。
- en: Prior research has primarily studied LLM-related privacy issues from a model-centered
    perspective, focusing on measuring (Carlini et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib10),
    [2022](https://arxiv.org/html/2309.11653v2#bib.bib9); Li et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib42))
    and mitigating (Jang et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib31);
    Dupuy et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib14); Majmudar
    et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib48)) technical privacy
    risks during the model training and inference phases. While model-centered mitigations
    are important, building ethical and privacy-preserving LLM-based CAs also requires
    a more human-centered investigation of user disclosure behaviors and risk perceptions.
    LLM-based CAs are unique and powerful tools, and while the benefits of disclosing
    personal data to these CAs is often concrete to users — in that the CA can help
    them more directly with a task — the risks are more abstract and difficult to
    reason about. This asymmetry can lead to users divulging information that can
    increasingly expose them to both types of privacy risks over time, perhaps unwittingly.
    An understanding of how users navigate disclosure decisions with LLM-based CAs,
    how privacy concerns factor into those decisions, and how equipped people are
    to express their privacy preferences without regulatory intervention is essential
    if we are to build ethical and privacy-respecting LLM-based CAs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究主要从以模型为中心的角度研究LLM相关的隐私问题，重点在于衡量（Carlini等人，[2021](https://arxiv.org/html/2309.11653v2#bib.bib10)，[2022](https://arxiv.org/html/2309.11653v2#bib.bib9)；Li等人，[2023](https://arxiv.org/html/2309.11653v2#bib.bib42)）和减轻（Jang等人，[2022](https://arxiv.org/html/2309.11653v2#bib.bib31)；Dupuy等人，[2022](https://arxiv.org/html/2309.11653v2#bib.bib14)；Majmudar等人，[2022](https://arxiv.org/html/2309.11653v2#bib.bib48)）在模型训练和推理阶段的技术隐私风险。虽然以模型为中心的隐私风险缓解措施非常重要，但构建具有伦理和隐私保护功能的基于LLM的会话代理也需要更加以人为中心的研究，探讨用户的披露行为和风险认知。基于LLM的会话代理是独特且强大的工具，尽管向这些会话代理披露个人数据的好处对于用户来说通常是直接可感知的——因为会话代理可以更直接地帮助他们完成任务——但风险却更加抽象，难以理解。这种不对称性可能导致用户泄露越来越多的信息，从而随着时间推移增加暴露在两种隐私风险下的可能性，甚至是无意的。要想构建具备伦理和尊重隐私的基于LLM的会话代理，我们必须了解用户如何做出披露决策，隐私顾虑如何影响这些决策，以及用户是否具备表达隐私偏好的能力，而无需监管干预。
- en: 'To bridge this gap, this work aims to complement prior work by examining how
    users navigate the disclosure risks and benefits in the everyday use of LLM-based
    CAs. We are interested in examining the following research questions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一空白，本研究旨在通过考察用户在日常使用基于LLM的会话代理时如何平衡披露风险和利益，来补充先前的研究。我们有兴趣探讨以下研究问题：
- en: 'RQ1:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题1（RQ1）：
- en: What sensitive disclosure behaviors emerge in the real-world use of LLM-based
    CAs?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际使用基于LLM的会话代理时，出现了哪些敏感披露行为？
- en: 'RQ2:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题2（RQ2）：
- en: Why do users have sensitive disclosure behaviors, and when and why do users
    refrain from using LLM-based CAs because of privacy concerns?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么用户会有敏感信息披露行为，以及为什么在隐私顾虑下，用户会停止使用基于LLM的会话代理（CAs）？
- en: 'RQ3:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 研究问题3（RQ3）：
- en: To what extent are users aware of, motivated, and equipped to protect their
    privacy when using LLM-based CAs?
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在使用基于LLM的会话代理时，意识到、激励并具备多大程度的能力来保护他们的隐私？
- en: We conducted two complementary studies to answer these research questions. To
    answer RQ1, we qualitatively examined a sample of real-world ChatGPT chat histories
    from the ShareGPT52K dataset¹¹1[https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)
    (200 sessions containing 10380 messages). This provided us with a broad overview
    of end-user disclosure behaviors in their use of LLM-based CAs. We found that
    users disclosed various types of personally identifiable information (PII) in
    these conversations, including not only their own data but also that of other
    people, implicating interdependent privacy issues. We created a multidimensional
    typology of observed ChatGPT disclosure scenarios and identified emergent privacy
    concerns. For example, users’ conversations with ChatGPT sometimes follow a similar
    flow to conversations between real people, with some users gradually revealing
    more and more sensitive information at ChatGPT’s request, suggesting potential
    risks of LLMs actively influencing disclosure behaviors.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了两项互补性研究来回答这些研究问题。为了回答RQ1，我们定性分析了来自ShareGPT52K数据集¹¹1[https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)的真实世界ChatGPT聊天记录样本（200个会话，包含10380条消息）。这为我们提供了关于终端用户在使用基于LLM的CA时披露行为的广泛概览。我们发现用户在这些对话中披露了多种类型的个人身份信息（PII），不仅包括他们自己的数据，还有其他人的数据，涉及到相互依赖的隐私问题。我们创建了一个多维的ChatGPT披露场景类型学，并识别出了新兴的隐私问题。例如，用户与ChatGPT的对话有时与真实人之间的对话流程相似，一些用户在ChatGPT的要求下逐渐透露越来越多的敏感信息，表明LLM可能积极影响披露行为，带来潜在风险。
- en: To answer RQ2 and RQ3, we conducted semi-structured interviews with ChatGPT
    users (N=$19{}$), in which we directly asked users about their disclosure behaviors,
    how they navigate the disclosure risks and benefits, and their mental models of
    how ChatGPT handles their data. For RQ2, we found that participants’ disclosure
    intentions were primarily affected by the perceived capability of the AI. Many
    participants had a pessimistic attitude about both accomplishing their primary
    objective and protecting privacy, believing that
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答RQ2和RQ3，我们对ChatGPT用户（N=$19{}$）进行了半结构化访谈，直接询问用户关于他们的披露行为、如何在披露风险和收益之间做出权衡，以及他们关于ChatGPT如何处理他们数据的心理模型。对于RQ2，我们发现参与者的披露意图主要受到AI能力感知的影响。许多参与者对既能完成主要目标又能保护隐私持悲观态度，认为
- en: '*You can’t have it both ways* (P9, P19).'
  id: totrans-26
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*鱼与熊掌不可兼得*（P9, P19）。'
- en: However, we also found almost every participant took ad-hoc privacy-protective
    measures featuring different levels of convenience and utility cost. This suggests
    that users are conscious of privacy while interacting with LLMs-based CAs and
    engage in efforts to protect their privacy when possible while contending with
    perceived tradeoffs of both convenience and utility. For RQ3, we identified varied
    mental models of the response generation and service improvement processes, some
    of which were indicative of misunderstandings of how LLMs work that impacted participants’
    ability to reason about privacy risks. Additionally, most participants did not
    know they could opt out of having ChatGPT use their data for model training. We
    also show how ChatGPT’s opt out interface includes dark patterns which may discourage
    its use by unnecessarily linking privacy and utility loss.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也发现几乎每位参与者都采取了临时的隐私保护措施，这些措施具有不同的便利性和效用成本。这表明用户在与基于LLM的CA互动时对隐私有意识，并在可能的情况下采取措施保护自己的隐私，同时应对便利性和效用之间的权衡。对于RQ3，我们识别出了多种关于响应生成和服务改进过程的心理模型，其中一些模型表明参与者对LLM工作原理存在误解，这影响了他们推理隐私风险的能力。此外，大多数参与者并不知道他们可以选择退出让ChatGPT使用他们的数据进行模型训练。我们还展示了ChatGPT的退出界面包含了“黑暗模式”，这些模式可能会通过不必要地将隐私和效用损失联系起来，进而抑制用户的使用。
- en: Although many participants felt they had to pay the price of privacy to get
    benefits from ChatGPT and considered the tradeoff a
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多参与者觉得他们必须为从ChatGPT获取的利益付出隐私的代价，并认为这种权衡是一个
- en: '*fair game* (P15),'
  id: totrans-29
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*公平游戏*（P15），'
- en: the erroneous mental models we observed and the difficulty of exercising privacy
    controls due to dark patterns suggest the game is far from fair. Our work is the
    first to take a human-centered approach to LLM privacy, and our findings suggest
    that it is important to explore the design space of user-facing privacy-preserving
    techniques to improve users’ awareness, perceived control, and actual control
    over privacy when using LLM-based systems. We propose potential directions that
    could improve the privacy design of LLM-based systems as an initial step toward
    addressing this significant yet nascent research problem. We note, however, that
    there are many challenging problems that cannot be easily addressed by design
    interventions alone, such as fixing flawed mental models. Our findings on dark
    patterns and how many users harbor fundamental misunderstandings about LLM-based
    CAs could help regulators understand how to craft policies that would require
    these platforms to provide appropriate privacy controls and avoid dark patterns.
    There are also structural problems, such as the influence of human-like interactions
    on users’ disclosures and the interdependent privacy issues, that still lack clear
    solutions, requiring paradigmatic changes in technology, law, and society.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的错误心理模型以及由于黑暗模式导致的隐私控制困难表明，这场“游戏”远非公平。我们的工作首次采用以人为中心的方法研究LLM隐私问题，且我们的发现表明，在使用基于LLM的系统时，探索面向用户的隐私保护技术设计空间对于提升用户对隐私的意识、感知控制和实际控制非常重要。我们提出了可能改进LLM系统隐私设计的方向，作为解决这一重要但仍处于初步阶段的研究问题的第一步。然而，我们也注意到，许多具有挑战性的问题无法仅通过设计干预来轻易解决，例如修复有缺陷的心理模型。我们关于黑暗模式的发现以及许多用户对基于LLM的对话系统存在的根本误解，可以帮助监管机构理解如何制定政策，要求这些平台提供适当的隐私控制并避免黑暗模式。还有一些结构性问题，如类人交互对用户披露的影响和隐私问题的相互依赖性，仍然缺乏明确的解决方案，需要在技术、法律和社会方面进行范式的变革。
- en: 2\. Background and Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与相关工作
- en: 2.1\. Emerging Privacy Challenges in LLM-based CAs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 基于大语言模型的对话系统中的新兴隐私挑战
- en: 'In addition to traditional privacy risks such as data breaches and the use
    or sale of personal data (Kshetri, [2023](https://arxiv.org/html/2309.11653v2#bib.bib40)),
    here we detail two distinctive privacy challenges inherent to LLM-based CAs: (i)
    memorization risks and (ii) the human-like interactions that can nudge users to
    disclose more information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的隐私风险，如数据泄露以及个人数据的使用或出售（Kshetri，[2023](https://arxiv.org/html/2309.11653v2#bib.bib40)），我们在这里详细讨论了基于LLM的对话系统固有的两个隐私挑战：（i）记忆风险和（ii）类人交互可能促使用户披露更多信息的风险。
- en: 2.1.1\. Memorization and Extraction Risks in (Large) Language Models
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. （大）语言模型中的记忆与提取风险
- en: LLM-based CAs are conversational agents built primarily on top of large language
    models (LLMs). To optimize conversational performance, LLMs inherently require
    vast amounts of data for their training, often encompassing user interaction data (Pahune
    and Chandrasekharan, [2023](https://arxiv.org/html/2309.11653v2#bib.bib57)). However,
    a side effect of this data-centric nature of LLMs is the unintentional memorization
    of portions of the training data, which also contain user input data, including
    personally identifiable information (Peris et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib59);
    Brown et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib8)), which might
    also be included in the generated output. For example, ChatGPT, even with safety
    precautions, can inadvertently disclose Personal Identifiable Information (PII)
    through specifical crafted prompts (Li et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib42)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLM）的对话系统是主要建立在大语言模型基础上的对话代理。为了优化对话性能，LLM本质上需要大量的数据进行训练，这些数据通常包括用户互动数据（Pahune
    和 Chandrasekharan，[2023](https://arxiv.org/html/2309.11653v2#bib.bib57)）。然而，LLM数据中心化的特性也带来了一个副作用——无意中记住了部分训练数据，这些数据中也包含了用户输入数据，包括个人可识别信息（Peris
    等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib59); Brown 等，[2022](https://arxiv.org/html/2309.11653v2#bib.bib8)），这些信息可能会被包含在生成的输出中。例如，即使采取了安全措施，ChatGPT也可能通过特定的提示无意泄露个人可识别信息（PII）（Li
    等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib42)）。
- en: 2.1.2\. Overreliance and More Disclosure with Human-like CAs
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 对类人对话系统的过度依赖与更多的披露
- en: Users engage with LLM-based CAs through natural language, which is traditionally
    reserved for human-to-human communication. This can lead them to perceive these
    agents as human-like. Studies suggested that anthropomorphizing can increase users’
    trust in CAs and more user information disclosure (Kim and Sundar, [2012](https://arxiv.org/html/2309.11653v2#bib.bib38);
    Ischen et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib30)). Anthropomorphizing
    can inflate users’ perceptions of the CA’s competencies, fostering undue confidence,
    trust, or expectations in these agents (McKee et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib50);
    Kim and Sundar, [2012](https://arxiv.org/html/2309.11653v2#bib.bib38); Złotowski
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib81)). With more trust,
    users might be more inclined to share private information, even in contexts typically
    associated with sensitive personal information (McKee et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib50);
    Kim and Sundar, [2012](https://arxiv.org/html/2309.11653v2#bib.bib38); Złotowski
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib81); Waldman, [2018](https://arxiv.org/html/2309.11653v2#bib.bib69)).
    Anthropomorphization may amplify the risks of users yielding effective control
    by trusting CAs unquestioningly. Moreover, more private information may be revealed
    when CAs leverage psychological effects, such as nudging or framing (Weidinger
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib72)). In this work,
    we provide the first set of empirical evidence supporting the speculation that
    the human-like text-generation capability of LLMs induces more sensitive disclosure
    behaviors in certain contexts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 用户通过自然语言与基于LLMs的CA进行互动，这种方式传统上是为人类之间的交流所保留的。这可能导致他们将这些代理看作类人。研究表明，拟人化可以增加用户对CA的信任，以及更多的用户信息披露（Kim
    and Sundar, [2012](https://arxiv.org/html/2309.11653v2#bib.bib38); Ischen et al.,
    [2020](https://arxiv.org/html/2309.11653v2#bib.bib30)）。拟人化可能会夸大用户对CA能力的认知，导致过度的信任、自信或期望（McKee
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib50); Kim and Sundar,
    [2012](https://arxiv.org/html/2309.11653v2#bib.bib38); Złotowski et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib81)）。随着信任的增加，用户可能更倾向于分享私人信息，即使是在通常与敏感个人信息相关的情境中（McKee
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib50); Kim and Sundar,
    [2012](https://arxiv.org/html/2309.11653v2#bib.bib38); Złotowski et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib81);
    Waldman, [2018](https://arxiv.org/html/2309.11653v2#bib.bib69)）。拟人化可能会加剧用户因盲目信任CA而失去有效控制的风险。此外，当CA利用心理效应，如提示或框架效应时，可能会泄露更多的私人信息（Weidinger
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib72)）。在本研究中，我们提供了首批实证证据，支持这样一种推测：LLMs类人的文本生成能力会在某些情境中诱导出更敏感的披露行为。
- en: 2.2\. Existing Privacy-Preserving Methods Related to LLMs
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 与LLMs相关的现有隐私保护方法
- en: Existing work have studied privacy-preserving techniques for LLMs, particularly
    for the memorization issue, from a model-centered perspective. For the model training
    phase, both data sanitization techniques, removing private data from training
    data (Lison et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib46); Kandpal
    et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib34)), and differentially
    private training methods (Li et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib43);
    Yu et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib74)) are used to
    preserve privacy. After the model has been trained, post hoc methodologies such
    as knowledge unlearning (Jang et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib31))
    have been proposed to mitigate privacy risks in LLMs by discarding particular
    knowledge signified by token sequences. Methods have also been proposed to mitigate
    privacy risks at the inference phase, including PII detection and differentially-private
    decoding (Majmudar et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib48)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现有研究从以模型为中心的角度，研究了LLMs的隐私保护技术，特别是对于记忆化问题。在模型训练阶段，采用了数据清理技术，从训练数据中移除私人数据（Lison
    et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib46); Kandpal et al.,
    [2022](https://arxiv.org/html/2309.11653v2#bib.bib34)），以及差分隐私训练方法（Li et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib43);
    Yu et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib74)）来保护隐私。在模型训练完成后，提出了后验方法，如知识遗忘（Jang
    et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib31)），通过丢弃由标记序列表示的特定知识来缓解LLMs中的隐私风险。还提出了在推理阶段缓解隐私风险的方法，包括PII检测和差分隐私解码（Majmudar
    et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib48)）。
- en: There are fewer papers on user-facing privacy-preserving techniques for LLM-based
    applications. Kim et al. ([2023](https://arxiv.org/html/2309.11653v2#bib.bib37))
    designed ProPILE, a tool for probing privacy leakage in LLMs, to enhance user
    awareness of privacy issues associated with LLMs. Despite the significance of
    the privacy concerns and needs of users, there is a dearth of comprehensive insight
    into this subject. Even tools like ProPILE, designed to increase user awareness
    of privacy issues related to LLMs, did not include the user perspective, such
    as need-finding research or user evaluation. We seek to bridge this gap by proposing
    directions for designing privacy-preserving tools that benefit the end-users of
    LLM-based applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关于面向用户的隐私保护技术在基于LLM的应用中的研究较少。Kim等人（[2023](https://arxiv.org/html/2309.11653v2#bib.bib37)）设计了ProPILE，这是一个用于探测LLM隐私泄露的工具，旨在增强用户对与LLM相关的隐私问题的意识。尽管隐私问题和用户需求的重要性不言而喻，但对这一主题的全面了解仍然匮乏。即使像ProPILE这样的工具，尽管旨在提高用户对LLM相关隐私问题的意识，但也未纳入用户的视角，例如需求发现研究或用户评估。我们希望通过提出隐私保护工具的设计方向，填补这一空白，最终造福基于LLM应用的终端用户。
- en: 2.3\. Privacy Research on Online Disclosure
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 在线披露的隐私研究
- en: We also review prior research about online disclosures to contextualize our
    work focused on LLM-based CAs. This body of literature includes studies about
    social media platforms like Facebook (Wang et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib71);
    Dwyer et al., [2007](https://arxiv.org/html/2309.11653v2#bib.bib15)) and Twitter (Liang
    et al., [2017](https://arxiv.org/html/2309.11653v2#bib.bib44)), and search engines
    like Google (Gibbs et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib23)).
    Prior work found that factors such as perceived benefits, perceived costs, social
    influence, and trust in the platform affect how people navigate disclosure risks
    and benefits online (Gibbs et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib23);
    Wang et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib71); Dwyer et al.,
    [2007](https://arxiv.org/html/2309.11653v2#bib.bib15); Stutzman et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib66);
    Zlatolas et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib80)). Research
    has indicated that people tend to use Google search to gather information about
    their potential romantic partners before meeting them in person. This behavior
    is influenced by several factors, such as trust, privacy concerns, and the desire
    for self-disclosure (Gibbs et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib23)).
    Wang et al. ([2011](https://arxiv.org/html/2309.11653v2#bib.bib71))’s research
    on Facebook found that emotional states often drive users to share content which
    can later lead to regret due to issues like misunderstandings from public disputes
    or revealed secrets. Prior research has also utilized theoretical frameworks like
    contextual integrity (Nissenbaum, [2004](https://arxiv.org/html/2309.11653v2#bib.bib53),
    [2020](https://arxiv.org/html/2309.11653v2#bib.bib54)) and social exchange theory (Cropanzano
    and Mitchell, [2005](https://arxiv.org/html/2309.11653v2#bib.bib13)) to analyze
    online disclosure behaviors. For example, Grodzinsky and Tavani ([2010](https://arxiv.org/html/2309.11653v2#bib.bib25))
    used contextual integrity to investigate whether non-password-protected personal
    blogs align with “normatively private contexts”.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还回顾了关于在线披露的先前研究，以便为我们的研究提供背景，该研究集中在基于大语言模型（LLM）的聊天代理（CAs）上。这些文献包括关于社交媒体平台的研究，如Facebook（Wang等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib71)；Dwyer等，[2007](https://arxiv.org/html/2309.11653v2#bib.bib15)）和Twitter（Liang等，[2017](https://arxiv.org/html/2309.11653v2#bib.bib44)），以及搜索引擎如Google（Gibbs等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib23)）。先前的研究发现，感知的利益、感知的成本、社交影响和对平台的信任等因素会影响人们如何在网上权衡披露的风险和收益（Gibbs等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib23)；Wang等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib71)；Dwyer等，[2007](https://arxiv.org/html/2309.11653v2#bib.bib15)；Stutzman等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib66)；Zlatolas等，[2015](https://arxiv.org/html/2309.11653v2#bib.bib80)）。研究表明，人们往往在见面前使用Google搜索来获取关于潜在浪漫伴侣的信息。这一行为受到多个因素的影响，如信任、隐私关注和自我披露的愿望（Gibbs等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib23)）。Wang等（[2011](https://arxiv.org/html/2309.11653v2#bib.bib71)）对Facebook的研究发现，情绪状态往往驱使用户分享内容，这些内容随后可能由于公众争议或暴露的秘密等问题而引发后悔。先前的研究还利用了理论框架，如情境完整性（Nissenbaum，[2004](https://arxiv.org/html/2309.11653v2#bib.bib53)，[2020](https://arxiv.org/html/2309.11653v2#bib.bib54)）和社会交换理论（Cropanzano和Mitchell，[2005](https://arxiv.org/html/2309.11653v2#bib.bib13)）来分析在线披露行为。例如，Grodzinsky和Tavani（[2010](https://arxiv.org/html/2309.11653v2#bib.bib25)）使用情境完整性理论研究了非密码保护的个人博客是否符合“规范上的私密环境”。
- en: The human-like, interactive communication style differentiates LLM-based CAs
    from previous systems, potentially resulting in broader and deeper disclosure.
    Given the difference, our study focuses on understanding how users navigate disclosure
    risks and benefits in the context of LLM-based CAs to contribute to the online
    disclosure literature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 类人化的互动式沟通风格将基于LLM的聊天代理（CAs）与以往的系统区分开来，这可能导致更广泛、更深层次的披露。鉴于这种差异，我们的研究专注于理解用户在基于LLM的聊天代理（CAs）环境下如何权衡披露的风险与收益，从而为在线披露文献作出贡献。
- en: 2.4\. Users’ Mental Models on Machine Learning and Privacy
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 用户对机器学习和隐私的心理模型
- en: The inherent opacity of machine learning (ML) systems often leads users to form
    an oversimplified or inaccurate mental model (Kaur et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib36);
    Hitron et al., [2019](https://arxiv.org/html/2309.11653v2#bib.bib29)). Interacting
    with LLMs with flawed mental models can result in unsafe use, inappropriate trust
    levels, and other interaction-based harms (Weidinger et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib72);
    Norman, [2014](https://arxiv.org/html/2309.11653v2#bib.bib55)). To understand
    privacy issues regarding LLM-based CAs from user perspectives, we not only aim
    to study users’ disclosure behaviors but also their mental models.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）系统固有的不透明性常常导致用户形成过于简化或不准确的心智模型（Kaur 等，[2020](https://arxiv.org/html/2309.11653v2#bib.bib36);
    Hitron 等，[2019](https://arxiv.org/html/2309.11653v2#bib.bib29)）。与存在缺陷的心智模型的LLM互动可能导致不安全使用、不适当的信任水平以及其他基于互动的危害（Weidinger
    等，[2021](https://arxiv.org/html/2309.11653v2#bib.bib72); Norman，[2014](https://arxiv.org/html/2309.11653v2#bib.bib55)）。为了理解用户视角下与LLM相关的隐私问题，我们不仅旨在研究用户的披露行为，还要研究他们的心智模型。
- en: 2.4.1\. Mental Models in ML
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. 机器学习中的心智模型
- en: Much of the current mental model research in AI centers on optimizing human-AI
    teamwork (Bansal et al., [2019](https://arxiv.org/html/2309.11653v2#bib.bib5);
    Andrews et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib3)). Some papers
    explore security and explainable AI, but they typically provide a broad overview
    without specific insights into user behavior. For example, Rutjes et al. ([2019](https://arxiv.org/html/2309.11653v2#bib.bib61))
    and Liao and Vaughan ([2023](https://arxiv.org/html/2309.11653v2#bib.bib45)) argue
    it is important to learn user mental models for building explainable and responsible
    AI. Bieringer et al. ([2022](https://arxiv.org/html/2309.11653v2#bib.bib7)) uses
    drawing exercises to study industrial practitioners’ mental models on adversarial
    machine learning to learn about their perception on the potential security challenges.
    Anderson et al. ([2020](https://arxiv.org/html/2309.11653v2#bib.bib2)) found differences
    in users’ mental models of reinforcement learning when prompted with varying explanations,
    where the excessive explanations increased cognitive load. Our research contributes
    the first in-depth understanding of users’ mental models of LLM-based CAs from
    a privacy perspective.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当前AI中的大部分心智模型研究集中在优化人类与AI的协作（Bansal 等，[2019](https://arxiv.org/html/2309.11653v2#bib.bib5);
    Andrews 等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib3)）。一些论文探讨了安全性和可解释的AI，但它们通常提供的是一个广泛的概述，而没有深入探讨用户行为。例如，Rutjes
    等（[2019](https://arxiv.org/html/2309.11653v2#bib.bib61)）和Liao与Vaughan（[2023](https://arxiv.org/html/2309.11653v2#bib.bib45)）认为，学习用户心智模型对于构建可解释且负责任的AI至关重要。Bieringer
    等（[2022](https://arxiv.org/html/2309.11653v2#bib.bib7)）通过绘画练习研究工业从业者对对抗性机器学习的心智模型，以了解他们对潜在安全挑战的认知。Anderson
    等（[2020](https://arxiv.org/html/2309.11653v2#bib.bib2)）发现，当以不同的解释提示用户时，用户对强化学习的心智模型存在差异，过多的解释会增加认知负担。我们的研究首次从隐私角度深入理解了用户对基于LLM的CAs的心智模型。
- en: 2.4.2\. Mental Models in Privacy
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. 隐私中的心智模型
- en: Mental models have been widely examined in usable privacy research to understand
    users’ privacy-related perceptions. Examples include mental models of general
    privacy and security (Anell et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib4);
    Renaud et al., [2014](https://arxiv.org/html/2309.11653v2#bib.bib60)), the internet (Kang
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35)), the Tor anonymity
    network (Gallagher et al., [2017](https://arxiv.org/html/2309.11653v2#bib.bib20)),
    the smart home (Zeng et al., [2017](https://arxiv.org/html/2309.11653v2#bib.bib75)),
    and cryptocurrency systems (Mai et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib47)).
    Users create cognitive maps of system components, their relations, and potential
    privacy risks, which helps them to understand where threats could emerge and how
    they could take effect. Research shows that more technically advanced users have
    a different understanding of digital systems (Kang et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35);
    Gallagher et al., [2017](https://arxiv.org/html/2309.11653v2#bib.bib20)). These
    findings highlight the importance of reasonable technical knowledge for informed
    user decisions. In our work, we followed the method of studying mental models
    in prior literature (Kang et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35))
    to investigate users’ mental models of LLM-based CAs.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 心理模型在可用隐私研究中被广泛研究，以了解用户对隐私相关的感知。例如，关于一般隐私和安全的心理模型（Anell et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib4);
    Renaud et al., [2014](https://arxiv.org/html/2309.11653v2#bib.bib60)），互联网（Kang
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35)），Tor匿名网络（Gallagher
    et al., [2017](https://arxiv.org/html/2309.11653v2#bib.bib20)），智能家居（Zeng et al.,
    [2017](https://arxiv.org/html/2309.11653v2#bib.bib75)）以及加密货币系统（Mai et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib47)）。用户会创建系统组件的认知图，了解它们之间的关系和潜在的隐私风险，这帮助他们理解威胁可能出现的位置以及它们可能如何产生影响。研究表明，更技术先进的用户对数字系统有不同的理解（Kang
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35); Gallagher et al.,
    [2017](https://arxiv.org/html/2309.11653v2#bib.bib20)）。这些发现突出了合理技术知识在帮助用户做出知情决策中的重要性。在我们的工作中，我们遵循了前人文献中的研究方法（Kang
    et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35)）来调查用户对基于LLM的虚拟助手的心理模型。
- en: 3\. Dataset Analysis
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 数据集分析
- en: We first analyzed a dataset containing real-world ChatGPT conversations to answer
    RQ1 regarding real-world ChatGPT users’ disclosure behaviors. In this section,
    we present the methodology and the findings of the dataset analysis.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先分析了包含真实世界ChatGPT对话的数据集，以回答关于真实世界ChatGPT用户披露行为的研究问题RQ1。在本节中，我们将展示数据集分析的方法论和发现。
- en: 3.1\. Methodology
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 方法论
- en: 3.1.1\. The ShareGPT52K Dataset
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. ShareGPT52K 数据集
- en: We used the ShareGPT52K dataset in our first analysis to examine real-world
    examples of data sharing practices. This dataset contains 50,496 ChatGPT chat
    histories shared by users of the ShareGPT Chrome extension from December 2022
    to March 2023. The conversations from this dataset, each identified by a unique
    ID, includes both the users’ prompts and ChatGPT’s responses. The dataset contains
    conversations that disclose sensitive information (e.g., person’s names, personal
    experiences), as the extension users may not expect the data will be displayed
    on the website to the public. Notably, the ShareGPT dataset has become a popular
    dataset to fine-tune other models (Chiang et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib11);
    Zheng et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib79); Geng et al.,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib21); Gudibande et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib26);
    Ji et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib32); Mu et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib51);
    Zhang et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib77)) in academic
    research and open-source community, while the risks of handling potentially sensitive
    personal information shared by users are not well discussed in the literature.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一次分析中，我们使用了ShareGPT52K数据集来研究数据共享实践的真实案例。该数据集包含了2022年12月到2023年3月期间，ShareGPT
    Chrome扩展用户共享的50,496条ChatGPT聊天记录。每条记录都有一个唯一的ID，包含用户的提示语和ChatGPT的回复。该数据集包含了披露敏感信息的对话（例如，个人姓名、个人经历），因为扩展用户可能没有意识到这些数据会公开展示在网站上。值得注意的是，ShareGPT数据集已经成为学术研究和开源社区中微调其他模型的热门数据集（Chiang等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib11)；Zheng等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib79)；Geng等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib21)；Gudibande等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib26)；Ji等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib32)；Mu等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib51)；Zhang等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib77)），然而，文献中对处理用户共享的潜在敏感个人信息的风险讨论并不充分。
- en: Ethical considerations
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: Using this dataset may raise ethical concerns. We believe our use of this dataset
    is justifiable for two reasons. 1) The primary objective of our research is to
    comprehend the privacy risks associated with the current disclosure behaviors
    of ChatGPT users and to identify areas where users need more support to manage
    these risks. This research is crucial to prevent incidents like the ShareGPT data
    leak from recurring. 2) This dataset provides a unique opportunity to closely
    examine users’ sensitive disclosure behaviors. As users were unaware of being
    observed, they likely exhibited less self-censorship in their shared content.
    This is akin to the analysis of leaked password datasets in password security
    literature (Ur et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib68)).
    To minimize the potential harm to individuals included in this dataset, we avoided
    quoting any text verbatim from the dataset and removed all PII.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此数据集可能引发伦理问题。我们认为使用此数据集是可以辩护的，原因有两个。1) 我们研究的主要目标是理解当前ChatGPT用户披露行为所带来的隐私风险，并找出用户在管理这些风险时需要更多支持的领域。这项研究对于防止类似ShareGPT数据泄露的事件再次发生至关重要。2)
    此数据集为我们提供了一个独特的机会，能够深入分析用户的敏感披露行为。由于用户未意识到自己正在被观察，他们在共享内容时可能表现出较少的自我审查。这类似于密码安全文献中对泄露密码数据集的分析（Ur
    等，[2015](https://arxiv.org/html/2309.11653v2#bib.bib68)）。为了尽量减少对数据集中个人的潜在伤害，我们避免直接引用数据集中的任何文本，并且去除了所有个人身份信息（PII）。
- en: 3.1.2\. Sampling methods
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 抽样方法
- en: 'We used Microsoft Presidio²²2Microsoft Presidio: [https://microsoft.github.io/presidio/](https://microsoft.github.io/presidio/)
    to detect PII in the ShareGPT52K dataset. This narrowed down our dataset to 30K
    conversations containing PII. We further split these 30K conversations into two
    groups. One group consisted of 7K conversations that contained, on average, more
    than one detected PII per turn in the conversation. The second group consisted
    of 23K conversations that contained, on average, less than one detected PII per
    turn in the conversation. As our goal was to analyze private-sensitive disclosures,
    we oversampled responses from the first group. To balance the need for a diverse
    array of cases with the practicality of manual analysis, we randomly selected
    batches of conversation threads from each group until no new themes emerged and
    we reached saturation (Guest et al., [2006](https://arxiv.org/html/2309.11653v2#bib.bib27)).
    Hence, for our final dataset, we included a total of 200 conversation threads,
    each with multiple conversation turns, to be qualitatively coded. The sample covered
    conversations of various lengths, measured by turns of conversations ($min=2,max=572,mean=51.9,std=97.0$).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了Microsoft Presidio²²2Microsoft Presidio：[https://microsoft.github.io/presidio/](https://microsoft.github.io/presidio/)来检测ShareGPT52K数据集中的个人身份信息（PII）。通过这一过程，我们将数据集缩小到包含PII的30K个对话。接着，我们将这30K个对话分为两组。一组包含了7K个对话，每个对话平均每轮包含一个以上被检测到的PII。另一组包含了23K个对话，每个对话平均每轮包含少于一个被检测到的PII。由于我们的目标是分析私人敏感信息的披露，因此我们对第一组的回应进行了过度抽样。为了在多样化的案例和人工分析的可行性之间取得平衡，我们从每组中随机选择了一批对话线程，直到没有新的主题出现，并且我们达到了饱和（Guest等，[2006](https://arxiv.org/html/2309.11653v2#bib.bib27)）。因此，对于我们的最终数据集，我们包含了总共200个对话线程，每个对话线程包含多个对话轮次，进行定性编码。样本涵盖了不同长度的对话，以对话轮次为衡量标准（$min=2,max=572,mean=51.9,std=97.0$）。
- en: 3.1.3\. Coding process
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 编码过程
- en: 'The qualitative analysis was guided by the contextual integrity framework (Nissenbaum,
    [2004](https://arxiv.org/html/2309.11653v2#bib.bib53), [2020](https://arxiv.org/html/2309.11653v2#bib.bib54)).
    According to the framework, context-relative informational norms are characterized
    by four key parameters: contexts, actors, data types, and transmission principles.
    The transmission principles can be technically analyzed, while the first three
    parameters depend on the real-world usage of a system. The primary goal of this
    analysis is to gain insight into the three parameters that characterize disclosure
    behaviors in real-world ChatGPT conversations.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 定性分析是以情境完整性框架为指导进行的（Nissenbaum，[2004](https://arxiv.org/html/2309.11653v2#bib.bib53)，[2020](https://arxiv.org/html/2309.11653v2#bib.bib54)）。根据该框架，情境相关的信息规范由四个关键参数来表征：情境、参与者、数据类型和传输原则。传输原则可以通过技术分析，而前三个参数则依赖于系统的现实世界使用情况。本分析的主要目的是深入了解在现实世界ChatGPT对话中，能够表征披露行为的这三个参数。
- en: 'First, we analyzed who (actors) shared what sensitive information (data types)
    with ChatGPT by re-labeling the detected PII at the conversation level. To facilitate
    this task, we created a web-based tool to present a conversation and highlight
    all the detected occurrences of each PII type. The coder can use this website
    to directly label whether each detected PII type appears at least once in the
    conversation, and label which individuals the detected PIIs were about, with the
    following options: self (the user who conversed with ChatGPT), others, both (self
    and others), and unknown. Two researchers first coded 50 conversations independently
    and calculated the inter-rater reliability. They then discussed the discrepancies
    in their labeling results and created a set of coding criteria ([Appendix A](https://arxiv.org/html/2309.11653v2#A1
    "Appendix A PII coding criteria ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). We noticed that the automated detection tool resulted in many false
    positives and a few false negatives. For false positives, the most common reasons
    were that the PII data type was misclassified (e.g., a phone number labeled as
    a passport number), open information about public figures (e.g., Taylor Swift),
    and made-up examples (e.g., 123-456-7890 as a placeholder for phone number). The
    two researchers then labeled another 50 conversations and achieved high inter-rater
    reliability (Gwet’s AC1) for frequently detected PII types including Person Name
    (0.89), Email Address (0.87), URL (0.88), Date/Time (0.81), NRP (Nationality,
    Religious, or Political Group) (0.81), IP Address (1), Location (0.86), Phone
    Number (0.93), US Bank Number (1). Finally, one research coded another 100 conversations.
    This coding analysis surfaced 106 conversations out of the 200 conversations that
    actually contained PII.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过在对话层面重新标注检测到的PII，分析了谁（行为主体）与ChatGPT共享了哪些敏感信息（数据类型）。为方便此任务，我们创建了一个基于Web的工具来呈现对话，并突出显示每种PII类型的所有检测到的出现位置。编码员可以使用此网站直接标注每个检测到的PII类型是否至少出现在对话中一次，并标明该PII所涉及的个体，选项包括：自己（与ChatGPT对话的用户）、他人、双方（自己和他人）以及未知。两位研究人员首先独立编码了50个对话并计算了评分者间的可靠性。然后，他们讨论了标注结果中的差异，并制定了一套编码标准（[附录A](https://arxiv.org/html/2309.11653v2#A1
    "附录A PII编码标准 ‣ '公平的游戏'，还是不公平？探索用户在使用基于LLM的对话代理时如何平衡披露风险与利益")）。我们注意到，自动检测工具产生了许多假阳性和少数假阴性。对于假阳性，最常见的原因是PII数据类型被误分类（例如，将电话号码误标为护照号），公开人物的信息（例如，泰勒·斯威夫特），以及虚构的例子（例如，123-456-7890作为电话号码的占位符）。然后，两位研究人员又标注了另外50个对话，并且对常见的PII类型（如人名（0.89）、电子邮件地址（0.87）、URL（0.88）、日期/时间（0.81）、NRP（国籍、宗教或政治团体）（0.81）、IP地址（1）、位置（0.86）、电话号码（0.93）、美国银行号码（1））达到了较高的评分者间可靠性（Gwet的AC1）。最后，一位研究人员又编码了另外100个对话。此编码分析揭示了在200个对话中，实际包含PII的有106个。
- en: Next, we aimed to analyze the contexts parameter by creating a typology of sensitive
    disclosure scenarios. Two researchers read through the 106 conversations verified
    to contain PII and wrote extensive summaries of the scenarios. The affinity diagramming
    method was used to organize the different use scenarios based on similar themes (Beyer
    and Holtzblatt, [1999](https://arxiv.org/html/2309.11653v2#bib.bib6)). All conversations
    were put onto sticky notes. After grouping all sticky notes, the researchers iteratively
    placed labels on the created categories that described the general theme of each
    group ([Appendix B](https://arxiv.org/html/2309.11653v2#A2 "Appendix B Codebook
    for Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们旨在通过创建敏感披露场景的类型学来分析上下文参数。两位研究人员阅读了106个经验证包含个人身份信息（PII）的对话，并写下了详细的场景摘要。采用亲和图法（Affinity
    Diagramming Method）来根据相似主题组织不同的使用场景（Beyer 和 Holtzblatt，[1999](https://arxiv.org/html/2309.11653v2#bib.bib6)）。所有对话都被放在便签纸上。在将所有便签纸归类之后，研究人员通过反复为创建的类别贴上标签，描述每个小组的总体主题（[附录B](https://arxiv.org/html/2309.11653v2#A2
    "附录B 数据集分析代码书 ‣ '公平的游戏'，还是不公平？探索用户在使用基于LLM的对话代理时如何平衡披露风险与利益")）。
- en: 3.1.4\. Methodological limitations
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4. 方法论限制
- en: 'Our method has some limitations. Firstly, the context of the conversations
    in the dataset is sometimes unclear, which can lead to uncertainties. For example,
    it can be challenging to discern whether a name mentioned by a user is real or
    fictitious. Furthermore, users’ thought processes when sharing data are not directly
    observable, which may limit our understanding of their behaviors. Finally, our
    dataset has an inherent sample bias: the type of user who was willing to share
    their conversations with the ShareGPT Chrome extension may not be representative
    of all LLM users. Despite these limitations, we have strived to make conservative
    interpretations of the observed behaviors in the dataset. Additionally, our interview
    study will provide complementary insights, and the two studies will collectively
    provide a more comprehensive understanding of the phenomena under investigation.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法存在一些局限性。首先，数据集中的对话背景有时不明确，这可能导致不确定性。例如，判断用户提到的名字是真实的还是虚构的可能会很具挑战性。此外，用户在分享数据时的思维过程并不可直接观察，这可能限制了我们对其行为的理解。最后，我们的数据集存在固有的样本偏差：愿意与
    ShareGPT Chrome 扩展共享对话的用户类型，可能无法代表所有大型语言模型（LLM）用户。尽管存在这些局限性，我们仍努力对数据集中观察到的行为做出保守的解释。此外，我们的访谈研究将提供互补的见解，两个研究将共同提供对所调查现象的更全面理解。
- en: 3.2\. Findings
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2. 发现
- en: 3.2.1\. Data Types Disclosed in ChatGPT Conversations
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1. 在 ChatGPT 对话中披露的数据类型
- en: In our sample, we identified both highly identifiable information such as Person
    Name, Email Address, IP Address, Phone Number, and Passport Number, as well as
    less directly identifiable personal information including URL, Date/Time, NRP
    (Nationality, Religious, or Political Group), and Location. We also found many
    disclosures of sensitive personal experiences that do not directly include typical
    PII. Even the more abstract PII types listed above can be used to identify a person
    given a specific context. For example, a user appeared to be an elementary school
    teacher and asked ChatGPT to generate a teaching plan. The user disclosed information
    such as the grade they were teaching, as well as the name and the district of
    the school where they teach. These two pieces of information, along with the topic
    area of the teaching plan, might be sufficient to identify the specific person
    or at least significantly narrow down who it might be. Furthermore, we found that
    the PII users shared during these conversations may not always have been necessary
    for the primary task. For example, users asked ChatGPT to help fix issues in some
    code snippets. Two phone numbers were found in the code, possibly for testing,
    which were not needed for the task.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的样本中，我们识别出了高度可识别的信息，如个人姓名、电子邮件地址、IP 地址、电话号码和护照号码，以及较少直接可识别的个人信息，包括 URL、日期/时间、NRP（国籍、宗教或政治团体）和位置。我们还发现了许多涉及敏感个人经历的披露，这些信息并不直接包含典型的个人身份信息（PII）。即使是上述更抽象的个人身份信息类型，也可能在特定背景下用于识别一个人。例如，某用户似乎是一名小学教师，并要求
    ChatGPT 生成一份教学计划。该用户披露了他们所教年级的信息，以及他们任教学校的名称和学区。这两条信息，再加上教学计划的主题，可能足以识别出特定的人，或者至少大大缩小了可能的范围。此外，我们还发现，用户在这些对话中分享的个人身份信息可能并非完成主要任务所必需的。例如，用户请求
    ChatGPT 帮助修复一些代码片段中的问题。代码中发现了两个电话号码，可能是用于测试，但这些号码对于任务并非必需。
- en: '3.2.2\. Actors: Relationship Between the Data Subject and the User'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2. 参与者：数据主体与用户之间的关系
- en: Beyond sharing their own PII, we found examples of users disclosing other people’s
    PII in their conversations with ChatGPT. This finding suggests that in ChatGPT
    and other LLM-Based CAs, there are not only institutional privacy issues but also
    interdependent privacy issues. The disclosure of other individuals’ data was common
    when users used ChatGPT to handle tasks that involved other people (e.g., friends,
    colleagues, clients) in both personal and work-related scenarios. For example,
    a user shared email conversations regarding complaints about their living conditions
    and asked ChatGPT about further steps to resolve and go forward with the matter.
    The conversation included email communication between the user and a staff member
    responsible for handling the issue. It contained both individuals’ email addresses,
    names, and phone numbers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分享自己的个人身份信息（PII），我们还发现用户在与 ChatGPT 的对话中泄露了其他人的 PII。这一发现表明，在 ChatGPT 和其他基于大语言模型（LLM）的会话代理中，不仅存在机构隐私问题，还有相互依赖的隐私问题。当用户使用
    ChatGPT 处理涉及其他人的任务时（例如，朋友、同事、客户），无论是个人还是与工作相关的场景中，泄露他人数据的情况都很常见。例如，某用户分享了关于其居住条件投诉的电子邮件对话，并向
    ChatGPT 询问了下一步如何解决问题并继续推进此事。该对话包含了用户与负责处理该问题的工作人员之间的电子邮件沟通，邮件中包含了两人的电子邮件地址、姓名和电话号码。
- en: '3.2.3\. Contexts: A Typology of Disclosure Scenarios'
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 上下文：披露场景的类型学
- en: 'We developed a multidimensional typology to characterize the disclosure scenarios
    of ChatGPT. The typology consists of four dimensions: Context, Topic, Purpose,
    and Prompt strategy.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个多维类型学来表征 ChatGPT 的披露场景。该类型学由四个维度组成：上下文、主题、目的和提示策略。
- en: 'Context includes three categories: Work-Related, Academic-Related, and Life-Related.
    Context can affect the data-sharing norms. For example, a company may not allow
    its employees to share work-related data with ChatGPT.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文包括三个类别：与工作相关、与学术相关和与生活相关。上下文可能会影响数据共享的规范。例如，公司可能不允许员工将与工作相关的数据与 ChatGPT 分享。
- en: 'Topic includes eight categories: Business, Assignment, Programming, Financial,
    Legal, Medical, Life, and Entertainment (see [Table 1](https://arxiv.org/html/2309.11653v2#S3.T1
    "Table 1 ‣ 3.2.3\. Contexts: A Typology of Disclosure Scenarios ‣ 3.2\. Findings
    ‣ 3\. Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")). Some
    topics are inherently more sensitive, such as the Financial and Medical topics (Zhao
    et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib78); Pan et al., [2020](https://arxiv.org/html/2309.11653v2#bib.bib58);
    El Haddad et al., [2018](https://arxiv.org/html/2309.11653v2#bib.bib16)), which
    naturally lend themselves to users’ sharing information like their transaction
    histories and medical diagnoses, respectively.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 主题包括八个类别：商业、作业、编程、金融、法律、医疗、生活和娱乐（见[表1](https://arxiv.org/html/2309.11653v2#S3.T1
    "表1 ‣ 3.2.3\. 上下文：披露场景的类型学 ‣ 3.2\. 发现 ‣ 3\. 数据集分析 ‣ ”这是公平的游戏”，还是它？考察用户在使用基于大语言模型的会话代理时如何平衡披露风险与利益")）。一些主题天生更为敏感，如金融和医疗主题（赵等人，[2023](https://arxiv.org/html/2309.11653v2#bib.bib78);
    潘等人，[2020](https://arxiv.org/html/2309.11653v2#bib.bib58); El Haddad 等人，[2018](https://arxiv.org/html/2309.11653v2#bib.bib16)），这些话题自然会促使用户分享其交易历史和医疗诊断等信息。
- en: Table 1. A Typology of Disclosure Scenarios – Topic
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 披露场景的类型学 – 主题
- en: '| Topic | Examples | Potential Risks |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 示例 | 潜在风险 |'
- en: '| --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Business | ![[Uncaptioned image]](img/6a7e2d335c07c84073c0c7cbc6579999.png)  |
    Sharing business details like ideas, plans, or strategies can pose privacy risks.
    This information could be confidential, or potentially reveal sensitive data such
    as the user’s location or company name. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 商业 | ![[无标题图片]](img/6a7e2d335c07c84073c0c7cbc6579999.png) | 分享商业细节，如创意、计划或战略，可能会带来隐私风险。这些信息可能是机密的，或可能泄露敏感数据，例如用户的位置或公司名称。
    |'
- en: '| Assignment | ![[Uncaptioned image]](img/c207bfa3f9ff1e75031357980584fde6.png)  |
    Using ChatGPT for assignment could risk plagiarism accusations and inadvertently
    reveal user information, such as their academic focus. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 作业 | ![[无标题图片]](img/c207bfa3f9ff1e75031357980584fde6.png) | 使用 ChatGPT 进行作业可能会面临抄袭指控，并可能不小心泄露用户信息，例如其学术专注领域。
    |'
- en: '| Programming | ![[Uncaptioned image]](img/36008c34f6f2f96517a3369e05fac90d.png)  |
    These scenarios typically pose minimal privacy risks. However, users sometimes
    incorporate personal details like phone numbers or emails in the shared code.
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | ![[未标注的图片]](img/36008c34f6f2f96517a3369e05fac90d.png)  | 这些情境通常对隐私的风险较小。然而，用户有时会在共享代码中加入个人细节，如电话号码或电子邮件。
    |'
- en: '| Financial | ![[Uncaptioned image]](img/f37e2fe2f9da9434db025609eebc6350.png)  |
    Sharing financial details, including transaction histories, might increase the
    risk of fraudulent activity. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 财务 | ![[未标注的图片]](img/f37e2fe2f9da9434db025609eebc6350.png)  | 共享财务细节，包括交易历史，可能增加欺诈活动的风险。
    |'
- en: '| Legal | ![[Uncaptioned image]](img/09d14b7601dbddafe029f117f191cc64.png)  |
    Possible privacy risks could involve parties like attorneys, clients, or the legal
    case. Sharing confidential information with ChatGPT could have serious implications
    if leaked. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 法律 | ![[未标注的图片]](img/09d14b7601dbddafe029f117f191cc64.png)  | 可能的隐私风险涉及律师、客户或法律案件等各方。与ChatGPT共享机密信息，如果泄露，可能会产生严重影响。
    |'
- en: '| Medical | ![[Uncaptioned image]](img/749f5984d69e4fbd2d7e814d095b97ec.png)  |
    Possible privacy risks could involve parties like doctors, patients, or the medical
    case. Sharing confidential information with ChatGPT could have serious implications
    if leaked. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 医疗 | ![[未标注的图片]](img/749f5984d69e4fbd2d7e814d095b97ec.png)  | 可能的隐私风险涉及医生、患者或医疗案例等各方。与ChatGPT共享机密信息，如果泄露，可能会产生严重影响。
    |'
- en: '| Life | ![[Uncaptioned image]](img/a6f69f6f520fad3d4fd8c96524088d32.png)  |
    Sharing personal information with ChatGPT poses potential privacy risks. Any potential
    leak could damage reputations or relationships, especially when involving third
    parties. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 生活 | ![[未标注的图片]](img/a6f69f6f520fad3d4fd8c96524088d32.png)  | 与ChatGPT共享个人信息存在潜在的隐私风险。任何潜在的泄露都可能损害声誉或人际关系，特别是涉及第三方时。
    |'
- en: '| Entertainment | ![[Uncaptioned image]](img/a9a6f553ec3952f0c59a115f2cbcdc70.png)  |
    While much of the content in these scenarios is fictional and thus poses little
    privacy risk, incorporating personal details can increase this risk. Depending
    on the scenario type, reputational harm may also occur if users request inappropriate
    content from ChatGPT. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 娱乐 | ![[未标注的图片]](img/a9a6f553ec3952f0c59a115f2cbcdc70.png)  | 虽然这些情境中的大部分内容是虚构的，因此对隐私的风险较小，但如果涉及个人细节，则可能增加这一风险。根据情境类型，如果用户向ChatGPT请求不当内容，也可能造成名誉损害。
    |'
- en: 'Purpose is the reason users are using ChatGPT, and includes Generating Content,
    Generating Plans/Advice, Answering Questions, Data Analysis, and Casual Chat.
    Conversations in the Data Analysis category were particularly prone to sensitive
    disclosures: many users directly copied and pasted a data table and asked ChatGPT
    to derive insights from it.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 目的即用户使用ChatGPT的原因，包括生成内容、生成计划/建议、回答问题、数据分析和休闲聊天。在数据分析类别的对话中，尤其容易出现敏感信息的披露：许多用户直接复制并粘贴数据表格，并要求ChatGPT从中推导出见解。
- en: Prompt strategy captures tactical approaches users used to interact with ChatGPT
    to achieve their goals, including Direct Command, Interactively Defining the Task,
    Role-Playing, and Jail-Breaking. Interactively Defining the Task refers to scenarios
    where users engage in multi-round interactions with ChatGPT. In these scenarios,
    users gave ChatGPT instructions and adjusted their instructions based on ChatGPT’s
    response. This interactive process led users to gradually reveal more information
    as the conversation progressed; sometimes, this progressive disclosure was driven
    by ChatGPT. For example, a user set up ChatGPT to act as a therapist. The user
    conversed with ChatGPT about their experiences and sought advice through multiple
    rounds of conversation. As ChatGPT was giving advice, the user would respond with
    more specific details, similar to the flow of a conversation between real people.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 提示策略捕捉了用户与ChatGPT互动以实现目标的战术方法，包括直接命令、交互定义任务、角色扮演和越狱。交互定义任务是指用户与ChatGPT进行多轮互动的情境。在这些情境中，用户向ChatGPT发出指令，并根据ChatGPT的回应调整指令。这一互动过程导致用户随着对话的进展逐渐透露更多信息；有时，这种逐步披露是由ChatGPT推动的。例如，用户将ChatGPT设定为一个治疗师。用户与ChatGPT进行对话，讨论自己的经历并通过多轮对话寻求建议。在ChatGPT给出建议时，用户会回应更多具体细节，这类似于真实人们之间的对话流程。
- en: 3.2.4\. Users’ Concerns and Protective Behaviors in the Conversations
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 用户在对话中的关注点和保护行为
- en: We found that, in some conversations, users explicitly mentioned having privacy
    concerns and employed prompt strategies as protective behaviors. Several users
    expressed concern — in their prompts — that others would find out that they had
    used AI for the task at hand. For instance, one user who was writing a book provided
    ChatGPT with a list of content-generation tasks. The user explicitly wrote in
    the prompt like
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在一些对话中，用户明确提到有隐私顾虑，并采用了提示策略作为保护行为。有几位用户在他们的提示中表达了担心——其他人会发现他们在当前任务中使用了AI。例如，一位正在写书的用户向ChatGPT提供了一份内容生成任务清单，用户在提示中明确写道：
- en: '*do something for me so that no one will find out this book was written by
    AI.*.'
  id: totrans-89
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*为我做点什么，这样就没有人会发现这本书是由AI写的*。'
- en: We also observed users implementing privacy-protective behaviors in their original
    prompts. For example, a user asked ChatGPT to help analyze patient messages, and
    replaced all the names in the messages with “[PERSONALNAME]”.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还观察到用户在他们的原始提示中实施了隐私保护行为。例如，一位用户要求ChatGPT帮助分析患者消息，并将消息中的所有姓名替换为“[PERSONALNAME]”。
- en: 4\. Interview Methodology
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 访谈方法
- en: Analyzing the ShareGPT52k dataset allowed us to model in-situ disclosure behaviors
    (RQ1). Next, to understand when and why users have sensitive disclosure behaviors
    (RQ2) as well as how users perceive and handle privacy risks (RQ3), we conducted
    semi-structured interviews with 19 users of LLM-based CAs (18 ChatGPT users, 1
    Bing chat user). The study design was approved by our IRB, and we conducted the
    interviews remotely between July and August, 2023. To refine interview script
    and recruitment strategy, we conducted pilot studies with users from the authors’
    social networks, encompassing both technical and non-technical backgrounds. After
    each pilot interview, the researchers reviewed and reflected on the process, took
    notes, and made adjustments to the interview script.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 分析ShareGPT52k数据集使我们能够建模现场披露行为（RQ1）。接下来，为了理解用户何时以及为何会有敏感的披露行为（RQ2），以及用户如何看待和处理隐私风险（RQ3），我们对19位LLM基础对话系统用户（18位ChatGPT用户，1位Bing
    chat用户）进行了半结构化访谈。研究设计已通过我们的伦理委员会（IRB）审批，我们于2023年7月至8月期间远程进行访谈。为了完善访谈脚本和招募策略，我们与来自作者社交网络的用户进行了试点研究，涵盖了技术和非技术背景的用户。在每次试点访谈后，研究人员会回顾并反思过程，做笔记并对访谈脚本进行调整。
- en: 4.1\. Participants
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 参与者
- en: We recruited our participants from Prolific³³3[Prolific](https://www.prolific.co)
    is a website for recruiting research study participants. and the authors’ social
    network. To ensure that we had a diverse sample and that we only invited participants
    with relevant experience to participate in the interview, we used a pre-screening
    survey that asked about their ChatGPT use experiences, as well as their gender
    identity, age, and whether they have a technical background. We learned from pilot
    interviews that participants with limited experience using LLM-based CAs had little
    to say in response to our questions, especially with respect to privacy. Therefore,
    we mostly selected participants who used ChatGPT or a related LLM-based CA at
    least once weekly.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Prolific³³3[Prolific](https://www.prolific.co)网站和作者的社交网络中招募了参与者。为了确保我们获得多样化的样本，并且仅邀请具有相关经验的参与者参加访谈，我们使用了一个预筛选问卷，询问了他们使用ChatGPT的经验，以及他们的性别身份、年龄和是否具有技术背景。从试点访谈中我们了解到，使用LLM基础的对话系统经验较少的参与者对于我们的提问，特别是在隐私方面，几乎没有回应。因此，我们大多选择了至少每周使用一次ChatGPT或相关LLM基础对话系统的参与者。
- en: 'We intentionally did not mention “privacy” in our recruiting materials to avoid
    skewing our sample towards more privacy-conscious participants. Specifically,
    in the pre-screening survey, we did not directly ask them about what data they
    have shared with ChatGPT; instead, we designed multiple-choice questions based
    on the disclosure scenario typology developed from the dataset analysis ([Section 3.2.3](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3
    "3.2.3\. Contexts: A Typology of Disclosure Scenarios ‣ 3.2\. Findings ‣ 3\. Dataset
    Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure
    Risks and Benefits When Using LLM-Based Conversational Agents")) to collect information
    indicative of their data disclosure behaviors. We ended up recruiting 19 participants
    with a wide range of use cases, age groups, and technical backgrounds ([Table 2](https://arxiv.org/html/2309.11653v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Interview Methodology ‣ ”It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents")). The interviews were concluded when the research
    team stopped to learn new insights from new interviews, indicating that data saturation
    was reached (Guest et al., [2006](https://arxiv.org/html/2309.11653v2#bib.bib27)),
    thus ensuring a diverse and comprehensive range of insights. All 19 participants
    completed the main study (around 60 to 90 minutes) and were compensated $30 USD
    each.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '我们故意在招募材料中没有提到“隐私”一词，以避免将我们的样本偏向于更加关注隐私的参与者。具体来说，在筛选问卷中，我们没有直接询问他们与 ChatGPT
    共享了哪些数据；相反，我们根据从数据集分析中开发的披露场景类型学设计了多项选择题（[第 3.2.3 节](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3
    "3.2.3\. Contexts: A Typology of Disclosure Scenarios ‣ 3.2\. Findings ‣ 3\. Dataset
    Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure
    Risks and Benefits When Using LLM-Based Conversational Agents")），收集能够表明他们数据披露行为的信息。最终，我们招募了
    19 名参与者，涵盖了广泛的使用场景、年龄组和技术背景（[表 2](https://arxiv.org/html/2309.11653v2#S4.T2 "Table
    2 ‣ 4.1\. Participants ‣ 4\. Interview Methodology ‣ ”It’s a Fair Game”, or Is
    It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based
    Conversational Agents")））。当研究团队停止从新访谈中获得新的见解时，访谈结束，这表明数据饱和已达到（Guest et al., [2006](https://arxiv.org/html/2309.11653v2#bib.bib27)），从而确保了见解的多样性和全面性。所有
    19 名参与者完成了主要研究（约 60 到 90 分钟），并获得了每人 30 美元的补偿。'
- en: Table 2. Interview Participant Overview
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2. 受访者概况
- en: '| ID | Gender | Age | Tech | Version | Other services | Frequency | Use cases
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ID | 性别 | 年龄 | 技术 | 版本 | 其他服务 | 使用频率 | 使用场景 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| P1 | Woman | 18-24 | No | 3.5 | AI Chat | Weekly | Relocation Advice, Career
    Advice, Schoolwork |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| P1 | 女性 | 18-24 | 否 | 3.5 | AI 聊天 | 每周 | 迁移建议、职业建议、学校作业 |'
- en: '| P2 | Woman | 18-24 | No | 3.5 | - | Weekly | Career Advice, Schoolwork, Review
    Writing, Marketing Advice |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| P2 | 女性 | 18-24 | 否 | 3.5 | - | 每周 | 职业建议、学校作业、评论写作、市场营销建议 |'
- en: '| P3 | Woman | 45-54 | No | 3.5 | - | Weekly | Relocation Advice; Diet Advice,
    Exercise Advice, Career Advice, Medical Advice, Research Work |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| P3 | 女性 | 45-54 | 否 | 3.5 | - | 每周 | 迁移建议；饮食建议、运动建议、职业建议、医疗建议、研究工作 |'
- en: '| P4 | Woman | 25-34 | Yes | 3.5 | - | Not regular | Email Writing, Career
    Advice, Info search, Email Writing |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| P4 | 女性 | 25-34 | 是 | 3.5 | - | 非常规 | 电子邮件写作、职业建议、信息搜索、电子邮件写作 |'
- en: '| P5 | Woman | 25-34 | No | 3.5 | - | Weekly | Casual Chat, Math Learning,
    Email Writing, Copy-editing, Social Media Post Writing |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 女性 | 25-34 | 否 | 3.5 | - | 每周 | 休闲聊天、数学学习、电子邮件写作、文字编辑、社交媒体帖子写作 |'
- en: '| P6 | Woman | 45-54 | Yes | 3.5 | Bard; Pi.ai | Weekly | Finance Advice, Life
    Advice, Class Preparation, Info search, Test Capabilities |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| P6 | 女性 | 45-54 | 是 | 3.5 | Bard；Pi.ai | 每周 | 财务建议、生活建议、课程准备、信息搜索、能力测试 |'
- en: '| P7 | Woman | 25-34 | No | 3.5 | - | Weekly | Career Advice, Diet Advice,
    Test Capabilities |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| P7 | 女性 | 25-34 | 否 | 3.5 | - | 每周 | 职业建议、饮食建议、能力测试 |'
- en: '| P8 | Woman | 25-34 | No | 4; 3.5 | API Playground; Bing chat; Bard; Claude.ai
    | Daily | Casual Chat, Therapy, Data Analysis, Career Advice, Email Writing, Revise
    Writing, Programming, Language Learning, Schoolwork, Portfolio Making |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| P8 | 女性 | 25-34 | 否 | 4; 3.5 | API Playground；Bing chat；Bard；Claude.ai |
    每日 | 休闲聊天、治疗、数据分析、职业建议、电子邮件写作、修改写作、编程、语言学习、学校作业、作品集制作 |'
- en: '| P9 | Woman | 25-34 | No | 3.5 | Bard | Weekly | Relocation Advice, Ad Writing,
    Email Writing, Info Search, Test Capabilities |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| P9 | 女性 | 25-34 | 否 | 3.5 | Bard | 每周 | 迁移建议、广告写作、电子邮件写作、信息搜索、能力测试 |'
- en: '| P10 | Woman | 25-34 | Yes | 4; 3.5 | - | Daily | Casual Chat, Therapy, Data
    Analysis, Career Advice, Diet Advice, Email Writing, Language Learning |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| P10 | 女性 | 25-34 | 是 | 4; 3.5 | - | 每日 | 休闲聊天、治疗、数据分析、职业建议、饮食建议、电子邮件写作、语言学习
    |'
- en: '| P11 | Man | 25-34 | Yes | 4; 3.5 | API Playground | Daily | Legal Advice,
    Medical Advice, Programming, Data Analysis, Create Apps, Concepts Learning, Career
    Advice |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| P11 | 男性 | 25-34 | 是 | 4; 3.5 | API Playground | 每日 | 法律建议、医疗建议、编程、数据分析、创建应用、概念学习、职业建议
    |'
- en: '| P12 | Man | 25-34 | No | 4; 3.5 | Bard | Daily | Finance Advice, Legal Advice,
    Medical Advice, Book Chapter Writing, Email Writing, Joke Writing, |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| P12 | 男性 | 25-34 | 否 | 4; 3.5 | Bard | 每日 | 财务建议、法律建议、医疗建议、书籍章节写作、电子邮件写作、笑话创作
    |'
- en: '| P13 | Man | 18-24 | Yes | 3.5 | - | Weekly | Casual Chat, Life Advice, Schoolwork,
    Programming, Data Analysis, Revise Writing, Email/Work Message Writing, Career
    Advice |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| P13 | 男性 | 18-24 | 是 | 3.5 | - | 每周 | 休闲聊天、生活建议、学校作业、编程、数据分析、写作修订、电子邮件/工作信息写作、职业建议
    |'
- en: '| P14 | Man | 45-54 | Yes | 4; 3.5 | Bing chat; Pi.ai | Daily | Medical Advice,
    Finance Advice, |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| P14 | 男性 | 45-54 | 是 | 4; 3.5 | Bing chat; Pi.ai | 每日 | 医疗建议、财务建议 |'
- en: '| P15 | Man | 65-74 | Yes | 4; 3.5 | - | Daily | Medical Advice, Generate Survey
    Responses, Social Media Post Writing, |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| P15 | 男性 | 65-74 | 是 | 4; 3.5 | - | 每日 | 医疗建议、生成调查回复、社交媒体帖子写作 |'
- en: '| P16 | Man | 45-54 | No | - | Bing chat | Daily | Therapy, Casual Chat, Finance
    Advice |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| P16 | 男性 | 45-54 | 否 | - | Bing chat | 每日 | 治疗、休闲聊天、财务建议 |'
- en: '| P17 | Man | 25-34 | Yes | 4; 3.5 | - | Daily | Programming, Data Analysis,
    Immigration Advice, Literature Search, Revise Writing |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| P17 | 男性 | 25-34 | 是 | 4; 3.5 | - | 每日 | 编程、数据分析、移民建议、文献检索、写作修订 |'
- en: '| P18 | Man | 18-24 | Yes | 4; 3.5 | API Playground | Weekly | Programming,
    Data Analysis, Schoolwork, Revise Writing |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| P18 | 男性 | 18-24 | 是 | 4; 3.5 | API Playground | 每周 | 编程、数据分析、学校作业、写作修订 |'
- en: '| P19 | Man | 25-34 | No | 3.5 | - | Not regular | Therapy, Casual Chat, Finance
    Advice, Career Advice, Diet Advice, Exercise Advice |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| P19 | 男性 | 25-34 | 否 | 3.5 | - | 非常规 | 治疗、休闲聊天、财务建议、职业建议、饮食建议、运动建议 |'
- en: 4.2\. Study Design
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 研究设计
- en: At the beginning of each interview, the interviewer briefed the participants
    on study goals, procedures, and data protection measures. The interviewer then
    obtained the consent for recording (see interview script in [Appendix C](https://arxiv.org/html/2309.11653v2#A3
    "Appendix C Interview Script ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次访谈开始时，访谈者会向参与者简要介绍研究目标、程序和数据保护措施。然后，访谈者会征得参与者的录音同意（见访谈脚本 [附录C](https://arxiv.org/html/2309.11653v2#A3
    "Appendix C Interview Script ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")）。
- en: Our interview protocol consisted of four parts. First, we inquired into participants’
    lived disclosure behaviors and privacy considerations. Prior to the interview,
    we asked each participant to prepare at least three conversations with ChatGPT
    or other LLM-based CAs, redacting any information they did not want us to see.
    We encouraged them to select conversations that involved information they considered
    personal. During the interview, we first asked them to walk us through these conversations.
    For each, we asked them to explain their primary goals, as well as if and why
    they had any concerns sharing personal data. For any concerns they mentioned,
    we asked appropriate follow-up questions, such as if they had taken any measures
    to address the concerns, if they encountered any challenges, and if they refrained
    from using LLM-based CAs due to the expressed privacy concerns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的访谈协议包括四个部分。首先，我们询问参与者的披露行为和隐私考虑。在访谈之前，我们要求每个参与者准备至少三段与 ChatGPT 或其他基于 LLM
    的对话代理的对话，隐去他们不希望我们看到的信息。我们鼓励他们选择涉及个人信息的对话。访谈过程中，我们首先要求他们带我们回顾这些对话。对于每一段对话，我们要求他们解释主要目标，以及是否以及为何在分享个人数据时有任何担忧。对于他们提到的任何担忧，我们会提出适当的后续问题，例如他们是否采取了措施来解决这些担忧，是否遇到任何挑战，以及是否因为隐私问题而避免使用基于
    LLM 的对话代理。
- en: Second, we aimed to understand our participants’ mental models of how the LLM-based
    CAs use their input to generate responses and improve services. We designed a
    mental model drawing activity, as has been used in prior work (Kang et al., [2015](https://arxiv.org/html/2309.11653v2#bib.bib35)).
    Participants used the whiteboard feature of Zoom or Google Slides to draw diagrams
    and also verbally explained their understanding. We then debriefed them on how
    the system actually works, using a reference diagram we created. This diagram
    was based on the typical LLM inference and training process, as well as specific
    information from privacy policy of the company that produced the LLM-based CA
    (e.g., OpenAI, Google). Then, we asked about users’ understanding and feelings
    about certain topics, including data storage, training, and memorization risks.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们旨在了解参与者对基于LLM的CA如何利用其输入生成响应并改进服务的心理模型。我们设计了一个心理模型绘制活动，这种活动在先前的研究中已有使用（Kang等人，[2015](https://arxiv.org/html/2309.11653v2#bib.bib35)）。参与者使用Zoom或Google
    Slides的白板功能绘制图表，并口头解释他们的理解。随后，我们向他们讲解了系统的实际工作原理，使用了我们创建的参考图。这张图基于典型的LLM推理和训练过程，以及来自LLM基础的CA公司（如OpenAI、Google）隐私政策的具体信息。然后，我们询问了用户对某些话题的理解和感受，包括数据存储、训练和记忆风险。
- en: Third, we examined users’ awareness of and experiences with using existing privacy
    and data controls in ChatGPT⁴⁴4For the Bing chat user, we only asked about the
    history deletion feature in Bing chat., including chat history, delete chat, share
    chat, opt out of having user input used for training models. We also asked about
    other sensitive practices such as sharing their ChatGPT accounts with other people,
    and using ChatGPT plugins.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们检查了用户对使用现有隐私和数据控制措施的意识和经验，特别是在ChatGPT⁴⁴4对于Bing聊天用户，我们仅询问了Bing聊天中的历史删除功能。，包括聊天历史、删除聊天、分享聊天、选择不使用用户输入进行模型训练等功能。我们还询问了其他敏感的做法，如与他人共享他们的ChatGPT帐户，使用ChatGPT插件。
- en: Fourth, we asked whether participants learned anything new or surprising from
    the interview, whether they wanted to share additional privacy concerns, and whether
    they had specific requests for improving the existing system design. Finally,
    we asked participants to envision and share a wish-list for privacy and data controls
    for LLM-based CAs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第四，我们询问了参与者是否从访谈中学到了新知识或感到惊讶，是否想分享其他隐私担忧，以及是否有改进现有系统设计的具体要求。最后，我们要求参与者设想并分享关于基于LLM的CA的隐私和数据控制的愿望清单。
- en: 4.3\. Qualitative Analysis
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 定性分析
- en: We qualitatively analyzed the interview transcripts using a bottom-up open coding
    method and concurrently reviewed video recordings of participants reflecting on
    their chat histories and drawing their mental models. This approach ensured a
    more comprehensive understanding of the chatting contexts described by the participant
    and the mental model diagrams they drew. Our analysis involved two rounds of coding
    as recommended by Saldaña ([2015](https://arxiv.org/html/2309.11653v2#bib.bib62)).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自下而上的开放编码方法对访谈记录进行定性分析，并同时回顾了参与者反思他们的聊天历史并绘制心理模型的视频记录。这种方法确保了对参与者所描述的聊天情境以及他们绘制的心理模型图的更全面理解。我们的分析涉及两轮编码，按照Saldaña的建议进行（[2015](https://arxiv.org/html/2309.11653v2#bib.bib62)）。
- en: In the first round of coding, two researchers coded the same six interviews
    independently to develop a codebook. They held daily meetings to discuss the codes,
    reconcile coding discrepancies, and iteratively merge their codebooks. By the
    end of this round, we derived an initial codebook with 195 codes. Then, the two
    researchers collectively conducted axial coding to merge similar codes and assign
    high-level themes for answering the research questions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次编码过程中，两个研究人员独立对相同的六个访谈进行编码，以制定编码本。他们每天开会讨论编码，解决编码差异，并迭代地合并编码本。在这一轮结束时，我们得出了一个初步的编码本，其中包含195个编码。随后，两位研究人员共同进行了轴心编码，合并了相似的编码，并为回答研究问题分配了高层次的主题。
- en: In the second round of coding, the remaining 13 interviews were each independently
    coded by one of the two researchers using the new codebook. Changes were made
    to the codes and themes as needed, and all changes were discussed and agreed upon
    by both researchers in daily meetings. Following the guidance of McDonald et al.
    ([2019](https://arxiv.org/html/2309.11653v2#bib.bib49)), we did not calculate
    inter-rater reliability as our goal was to identify emergent themes rather than
    achieve consensus. The final codebook contains 62 codes grouped into 6 themes
    (see the codebook in [Appendix D](https://arxiv.org/html/2309.11653v2#A4 "Appendix
    D Codebook for the interview results ‣ ”It’s a Fair Game”, or Is It? Examining
    How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二轮编码中，剩余的13次访谈由两位研究人员中的一位使用新的编码手册进行独立编码。根据需要，对编码和主题进行了调整，所有更改都在每日会议中由两位研究人员讨论并达成一致。根据McDonald等人（[2019](https://arxiv.org/html/2309.11653v2#bib.bib49)）的指导，我们没有计算评分者间可靠性，因为我们的目标是识别新兴主题，而不是达成共识。最终的编码手册包含62个编码，分为6个主题（见[附录D](https://arxiv.org/html/2309.11653v2#A4
    "附录D 访谈结果的编码手册 ‣ ”这是公平游戏“，还是它？探讨用户在使用基于大型语言模型的对话代理时如何平衡披露风险和利益")）。
- en: 4.4\. Methodological Limitations
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 方法论局限性
- en: Our interview study has some limitations. Firstly, participants may have avoided
    discussing conversations with particularly sensitive information; moreover, individuals
    who share more sensitive information with ChatGPT may not have wanted to participate
    in the study. Accordingly, our sample could be skewed towards less sensitive conversations.
    In fact, one person who passed the prescreening requirements decided not to participate
    in the interview due to their concerns with sharing sensitive data. Secondly,
    there may be some ambiguity when participants reported experiences outside of
    the specific conversations they prepared for the interview, and this could potentially
    introduce some recall biases. There is also the possibility of social desirability
    bias (Grimm, [2010](https://arxiv.org/html/2309.11653v2#bib.bib24)), where participants
    may tailor their responses to what they perceive the interviewers want to hear.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的访谈研究存在一些局限性。首先，参与者可能避免讨论包含特别敏感信息的对话；此外，愿意与ChatGPT分享更敏感信息的个体可能不愿意参与研究。因此，我们的样本可能偏向于较少敏感的对话。事实上，有一位符合预筛选要求的人由于担心分享敏感数据而决定不参与访谈。其次，当参与者报告自己在访谈前准备的具体对话以外的经历时，可能会存在一些模糊性，这可能会引入某些回忆偏差。也存在社会期望偏差的可能性（Grimm，[2010](https://arxiv.org/html/2309.11653v2#bib.bib24)），即参与者可能会根据他们认为访谈者想听到的内容调整自己的回答。
- en: 5\. Interview Results
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 访谈结果
- en: In this section, we present our findings from the 19 interviews we conducted
    to answer RQ2 and RQ3. Sections 5.2 and 5.3 primarily explore factors influencing
    users’ sensitive disclosure intentions and their trade-off between disclosure
    benefits and risks, relevant to RQ2. Sections 5.4, 5.5 and 5.6 examine users’
    knowledge, awareness, motivations, and the barriers they encounter in adopting
    threat-protective behaviors, pertaining to RQ3. However, these aspects often intersect,
    with some sections relating to multiple research questions.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了从19次访谈中获得的结果，以回答研究问题2（RQ2）和研究问题3（RQ3）。第5.2节和第5.3节主要探讨影响用户敏感信息披露意图的因素，以及他们在披露利益与风险之间的权衡，相关于RQ2。第5.4、5.5和5.6节则考察了用户的知识、意识、动机以及他们在采用防护行为时遇到的障碍，涉及RQ3。然而，这些方面通常是交叉的，有些部分与多个研究问题相关。
- en: 5.1\. Overview of Use Cases and Disclosure Behaviors from the Interviews
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 访谈中的使用案例和披露行为概述
- en: Our participants’ conversations covered a wide range of scenarios ([Table 2](https://arxiv.org/html/2309.11653v2#S4.T2
    "Table 2 ‣ 4.1\. Participants ‣ 4\. Interview Methodology ‣ ”It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents")). Within these conversations, the types of personal
    data they shared included PII, personal experiences or conditions (e.g., health,
    financial, legal conditions), personal thoughts and emotions, and data from other
    people. For example, zip codes were shared for relocation advice (P1&P9). Physiological
    data like age, weight, and height were provided for generating tailored diet and
    exercise plans (P3&P8). Demographic information and medical conditions were shared
    for medical advice (P3, P11, P12, P14&P15). Educational and work experiences were
    disclosed for career advice or resume revision (P1, P3, P4, P7, P8, P10, P11,
    P13). Writing materials such as emails sent by others (P10) and unpublished papers
    or books (P2, P12, P13, P17, P18) were shared for content generation, reviewing,
    or revising purposes. Some users appeared to develop more emotional relationships
    with ChatGPT, treating the AI as a “pen pal” or a therapist (P8, P10, P13, P15,
    P16, P19).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的参与者的对话涵盖了广泛的场景（[表2](https://arxiv.org/html/2309.11653v2#S4.T2 "Table 2 ‣
    4.1\. Participants ‣ 4\. Interview Methodology ‣ ”It’s a Fair Game”, or Is It?
    Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based
    Conversational Agents")）。在这些对话中，他们分享的个人数据类型包括个人身份信息（PII）、个人经历或状况（例如健康、财务、法律状况）、个人思想和情感，以及来自他人的数据。例如，为了获取搬迁建议，分享了邮政编码（P1&P9）。为了制定量身定制的饮食和运动计划，提供了年龄、体重和身高等生理数据（P3&P8）。为了获得医疗建议，分享了人口统计信息和医疗状况（P3、P11、P12、P14&P15）。为了获取职业建议或修改简历，透露了教育和工作经验（P1、P3、P4、P7、P8、P10、P11、P13）。像他人发送的电子邮件（P10）和未发表的论文或书籍（P2、P12、P13、P17、P18）等写作材料也被用于内容生成、审阅或修改。有些用户似乎与ChatGPT发展了更为情感化的关系，把它当作“笔友”或“治疗师”（P8、P10、P13、P15、P16、P19）。
- en: 5.2\. Factors that Affect Users’ Disclosure Intentions with LLM-Based CAs
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 影响用户披露意图的因素——基于LLM的对话代理
- en: We found people generally thought about whether the primary goals of their tasks
    could be met and whether the operation was convenient enough when deciding what
    LLM-based CAs to use or what information to share. However, certain privacy concerns
    emerged when talking about specific use scenarios.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，人们通常会考虑他们的任务目标是否能够实现，以及操作是否足够便捷，这些因素会影响他们选择使用哪个基于LLM的对话代理或选择分享哪些信息。然而，在谈论具体的使用场景时，出现了一些隐私方面的担忧。
- en: 5.2.1\. Perceived Capability of the CAs (All participants)
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 对话代理的感知能力（所有参与者）
- en: People tend to disclose information if they perceive the CA’s capability as
    beneficial to their task goals. Conversely, they withhold information if they
    question the agent’s competence. For instance, P15 provided detailed medical diagnosis
    data after confirming with ChatGPT that it could provide an extensive explanation
    of the results. Apart from functionality support, LLM-based CAs provided emotional
    support that encouraged users to disclose information. Many participants mentioned
    they treated ChatGPT as a “friend” or a “therapist”, and sometimes had casual
    chats with it about their lives (P8, P10, P13, P15, P16, P19). P10 and P19 mentioned
    that they loved sharing personal life details with ChatGPT due to the positive
    feedback it gave. P16 demonstrated a conversation in which he told ChatGPT that
    he missed his brother, who had passed away, and disclosed a lot of his memories
    about his brother per ChatGPT’s request.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 人们倾向于披露信息，如果他们认为对话代理的能力对他们的任务目标有帮助。相反，如果他们质疑代理的能力，他们会选择保留信息。例如，P15在确认ChatGPT能够提供对结果的详细解释后，提供了详细的医学诊断数据。除了功能支持外，基于LLM的对话代理还提供了情感支持，鼓励用户披露信息。许多参与者提到，他们将ChatGPT视为“朋友”或“治疗师”，有时会和它聊聊自己的生活（P8、P10、P13、P15、P16、P19）。P10和P19提到，他们喜欢和ChatGPT分享个人生活细节，因为它给出了积极的反馈。P16展示了一段对话，他告诉ChatGPT他想念已故的兄弟，并在ChatGPT的要求下，透露了许多关于兄弟的回忆。
- en: '*He asked me to talk to him about my brother. It’s like a full conversation.
    He wanted to know everything.* (P16)'
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*他让我和他谈谈我的兄弟。这就像是一场完整的对话。他想知道一切。*（P16）'
- en: 5.2.2\. Convenience of Operation (P1, P2, P4, P8, P10)
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 操作便利性（P1、P2、P4、P8、P10）
- en: Participants were more inclined to share data when doing so was easily afforded
    by the interface. For example, P8 shared PDF documents including original research
    data with Claude AI⁵⁵5[https://claude.ai/login](https://claude.ai/login) due to
    the convenience of document input in Claude AI. Conversely, operational barriers
    deterred some users from sharing. P10 chose to copy and paste parts of her resume
    into ChatGPT rather than the entire document due to the inconvenience of inputting
    long blocks of text.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者更倾向于在界面便于操作时分享数据。例如，P8因Claude AI中便捷的文档输入功能，将包含原始研究数据的PDF文档分享给Claude AI⁵⁵5[https://claude.ai/login](https://claude.ai/login)。相反，操作障碍使得一些用户不愿分享。P10选择将她简历中的部分内容复制粘贴到ChatGPT，而不是分享整个文档，因为输入长文本块不方便。
- en: '*Because there were too many words in my resume. I’m kind of lazy. I don’t
    want to drag from the top to the bottom.* (P10)'
  id: totrans-141
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*因为我简历里的字太多了。我有点懒，不想从头拖到尾。*（P10）'
- en: 5.2.3\. Perceived Personal Data Sensitivity (All participants)
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 个人数据敏感性感知（所有参与者）
- en: 'Participants’ perceptions of the specificity and identifiability of personal
    data influenced their disclosure intention with LLM-based CAs. Participants sometimes
    refrained from sharing data that they considered uniquely identifying. For example,
    P3 never shared phone numbers and physical addresses with ChatGPT to avoid being
    tracked by human reviewers. P1 considered her birth date too sensitive to share
    due to its relation to account security and password resetting. They were more
    open to sharing data that could be considered PII, but was not deemed to be uniquely
    identifying. For example, P10 was fine with sharing the city in which they resided:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者对个人数据的特异性和可识别性的感知影响了他们与基于大型语言模型（LLM）的对话代理（CAs）之间的数据披露意图。参与者有时会避免分享他们认为具有唯一识别性的个人数据。例如，P3从未与ChatGPT分享电话号码和住址，以避免被人工审阅者追踪。P1认为她的出生日期与账户安全和密码重置相关，太过敏感，不愿分享。他们更愿意分享那些被视为个人身份信息（PII），但不被认为具有唯一识别性的内容。例如，P10愿意分享他们所在的城市：
- en: '*Telling ChatGPT I live in [city name redacted], it’s kind of like, saying
    I live on the earth.* (P10)'
  id: totrans-144
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*告诉ChatGPT我住在[城市名已删除]，这就像是在说我住在地球上一样。*（P10）'
- en: Participants also expressed concerns with sharing other types of personal information,
    even if it was not uniquely identifying. For instance, P8 preferred to be cautious
    when sharing personal opinions. P3 felt uncomfortable sharing her actual weight.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者还对分享其他类型的个人信息表示担忧，即使这些信息并不具有唯一的识别性。例如，P8在分享个人意见时倾向于保持谨慎。P3觉得分享她的实际体重让她感到不舒服。
- en: Notably, the perceived sensitivity of different types of data varies from person
    to person. For example, P3, P8, P11, and P14 were comfortable sharing their names,
    while others like P1 and P7 were more guarded. Even for the same task, seeking
    advice for exercise and diet plans, P8 felt ease to share weight data, while P3
    chose to provide false information. In extreme cases, some people expressed concern
    about sharing any personal information with ChatGPT (P2).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，不同类型数据的感知敏感性因人而异。例如，P3、P8、P11和P14愿意分享他们的名字，而像P1和P7则更加谨慎。即使是相同的任务，例如寻求运动和饮食计划的建议，P8也愿意分享体重数据，而P3则选择提供虚假信息。在极端情况下，一些人表达了对与ChatGPT分享任何个人信息的担忧（P2）。
- en: 5.2.4\. Resignation (P1, P3, P4, P6, P8, P10, P11, P13, P14)
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 辞职（P1, P3, P4, P6, P8, P10, P11, P13, P14）
- en: People also justified their sharing of personal information by saying their
    data was already accessible on various platforms such as social media, government
    databases, and educational systems. In other words, participants believed that
    the marginal risk of sharing personal data with LLM-based CAs was low and were
    resigned to the idea that their individual disclosure decisions had little impact
    on the accessibility of their personal data. For instance, P1 equated her data-sharing
    behavior on ChatGPT with those on social media.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 人们还通过表示他们的个人数据已经可以在各种平台上获取来为分享个人信息辩护，比如社交媒体、政府数据库和教育系统。换句话说，参与者认为与基于LLM的CAs分享个人数据的边际风险很低，并且他们已经接受了这样的想法：他们个人的披露决策对其个人数据的可访问性几乎没有影响。例如，P1将她在ChatGPT上的数据共享行为与在社交媒体上的行为等同起来。
- en: '*I’m doing the same risk by using the app like Instagram or Facebook.* (P1)'
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我使用Instagram或Facebook应用的风险也差不多。*（P1）'
- en: 5.2.5\. Perceived Risks and Harms
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.5\. 感知的风险和危害
- en: Concerns over data misuse by institutions (P2, P3, P5, P6, P8, P9, P11, P12,
    P13, P14, P17, P18)
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对机构滥用数据的担忧（P2, P3, P5, P6, P8, P9, P11, P12, P13, P14, P17, P18）
- en: Many participants mentioned they were not sure about how their data could be
    used by OpenAI, and expressed a range of concerns related to potential misuse,
    encompassing issues such as incomplete data deletion (P6, P9), the possibility
    of selling user data or using it for marketing (P9, P12), sharing data with third
    parties (P9, P17), human reviews by OpenAI staff (P3, P6, P14), and public disclosure
    of data (P6, P9). On the other hand, some participants (P8, P11, P13, P14) said
    they trust OpenAI more because they did not think it would sell user data and
    use that data for marketing purposes. P14 recounted experiences with Bing chat,
    where he faced targeted marketing after specific conversations, an issue he had
    not encountered with ChatGPT. This difference influenced his preference for ChatGPT
    over Bing chat.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者提到他们不确定自己的数据将如何被OpenAI使用，并表示对潜在误用的各种担忧，包括数据删除不完全（P6，P9）、出售用户数据或将其用于营销（P9，P12）、与第三方共享数据（P9，P17）、OpenAI员工进行人工审核（P3，P6，P14）以及公开披露数据（P6，P9）。另一方面，一些参与者（P8，P11，P13，P14）表示他们更信任OpenAI，因为他们认为OpenAI不会出售用户数据或将这些数据用于营销目的。P14回忆起与Bing聊天的经历，他在特定对话后遭遇了定向营销，这是他在使用ChatGPT时未曾遇到过的问题。这一差异影响了他对ChatGPT而非Bing聊天的偏好。
- en: Concerns about others finding out (P8, P17, P18)
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于别人发现的担忧（P8，P17，P18）
- en: Given the uncertainty surrounding the social acceptance of using AI for certain
    tasks, some people expressed concerns about others finding out that they used
    ChatGPT. For example, P18 did not want his friend to know he used ChatGPT for
    homework. P8 was worried that others might
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用AI执行某些任务的社会接受度存在不确定性，一些人表示担心别人发现他们使用了ChatGPT。例如，P18不希望他的朋友知道他用ChatGPT做作业。P8则担心其他人可能
- en: '*change their attitude to me*'
  id: totrans-155
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*改变他们对我的态度*'
- en: if they discovered her reliance on AI for tasks like schoolwork and email writing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果他们发现她在处理学校作业和写邮件时依赖AI的话。
- en: '*I hope they (my professors) will never know I used AI to do that (write emails).*
    (P8)'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我希望他们（我的教授）永远不会知道我用AI做了那件事（写邮件）。*（P8）'
- en: Concerns about idea theft (P2, P14, P17)
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 关于创意盗窃的担忧（P2，P14，P17）
- en: Users’ concerns about idea theft manifested in various ways. These included
    concerns about the system redistributing their work without acknowledging the
    author (P2), OpenAI employees seeing and stealing the user’s business idea (P14),
    and allowing other people to read parts of a paper that is under review (P17).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 用户对创意盗窃的担忧表现形式多种多样。这些担忧包括系统在没有承认作者的情况下重新分发他们的作品（P2），OpenAI员工看到并盗用用户的商业创意（P14），以及允许其他人阅读正在审稿中的论文部分（P17）。
- en: '*I don’t know if ChatGPT uses it (the fiction that I wrote) as inspiration
    for other people, or spits it out as it wrote it itself instead of me.* (P2)'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我不知道ChatGPT是否将我写的小说作为灵感用于其他人，或者将它当作是它自己写的，而不是我写的。*（P2）'
- en: Note that concerns about sharing original content also vary from person to person.
    For example, P17 was worried about model memorizing and spreading his unpublished
    work, while P18 willingly shared his unpublished work for paper revision.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，关于分享原创内容的担忧也因人而异。例如，P17担心模型记住并传播他的未发表作品，而P18则愿意分享他的未发表作品以供论文修订。
- en: 5.2.6\. Perceived Norms of Disclosure
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.6\. 披露的感知规范
- en: Attitude towards disclosing others’ data and having one’s own data shared by
    others (P3, P8, P10, P11, P13, P14, P16)
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对披露他人数据以及自己数据被他人分享的态度（P3，P8，P10，P11，P13，P14，P16）
- en: Apart from disclosing their own information, participants also discussed their
    thoughts about disclosing data about others. Some people expressed heightened
    caution when sharing data related to others, even more so than their own data
    (P3, P11, P13, P14). P3 and P14 voiced concerns regarding data ownership and the
    ethics of sharing information without the original owner’s consent, noting,
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 除了披露他们自己的信息外，参与者还讨论了他们对披露他人数据的看法。一些人在分享与他人相关的数据时表现出更高的谨慎，甚至比分享自己的数据还要小心（P3，P11，P13，P14）。P3和P14对数据所有权以及在没有原始所有者同意的情况下分享信息的伦理问题表示担忧，并指出，
- en: '*That’s not my decision to make.*'
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*那不是我可以决定的。*'
- en: Conversely, others expressed less concern about sharing information about others
    (P8, P10, P16). P10 also felt minimal concern about her own information being
    shared by others, viewing it as a fair exchange. Furthermore, P8 deemed it acceptable
    to share extensive research data containing details of various individuals, including
    full names, demographic specifics, personal experiences, and feedback, justifying
    her actions as being non-commercial in intention. However, P8 expressed discomfort
    with the idea of her data being shared by others, worrying about
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，其他人则表达了对分享他人信息的关注较少（P8, P10, P16）。P10 也对别人分享她自己的信息几乎不感到担忧，认为这是公平的交换。此外，P8
    认为分享包含各类个人详细信息的广泛研究数据是可以接受的，包括全名、人口统计特征、个人经历和反馈，并辩解称她的行为没有商业意图。然而，P8 对她的数据被他人分享表示不适，担心
- en: '*how AI will summarize or do what kind of judgment for me.*.'
  id: totrans-167
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*AI 将如何为我总结或做出什么样的判断。*'
- en: Caution with sharing work-related data due to company policies or NDAs (P5,
    P10, P11, P14, P15)
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 因公司政策或保密协议（NDA）而对分享与工作相关的数据保持谨慎（P5, P10, P11, P14, P15）
- en: Several users discussed using ChatGPT for work-related purposes, and expressed
    caution against sharing company or confidential data, citing company policies
    or non-disclosure agreements (NDAs) as the reason. For example, P10 recognized
    ChatGPT’s capability in data analysis but refrained from sharing company datasets
    due to her company’s policies.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用户讨论了将 ChatGPT 用于工作相关的目的，并表示在分享公司或机密数据时保持谨慎，引用公司政策或保密协议作为原因。例如，P10 认识到 ChatGPT
    在数据分析方面的能力，但由于公司政策，她避免分享公司数据集。
- en: 5.3\. How Users Navigate the Trade-off Between Disclosure Risks and Benefits
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 用户如何在披露风险与收益之间进行权衡
- en: Users’ use of ChatGPT for sensitive tasks surfaces inevitable tensions between
    privacy, utility, and convenience. We identified three broad strategies participants
    used when navigating this trade-off.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 用户使用 ChatGPT 处理敏感任务时，隐私、效用和便利性之间的张力不可避免。我们识别出了参与者在处理这一权衡时使用的三种主要策略。
- en: 5.3.1\. Accept Privacy Risks to Reap Benefits (P4, P8, P9, P10, P14, P15, P18,
    P19)
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 接受隐私风险以获得好处（P4, P8, P9, P10, P14, P15, P18, P19）
- en: Participants often found accomplishing their objective to be more important
    than avoiding privacy risks. As P15 said,
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者通常认为达成目标比避免隐私风险更为重要。正如P15所说，
- en: '*There is a price for getting the benefits of using this application*,'
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*使用这个应用程序获得好处是有代价的*，'
- en: 'further adding:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步补充：
- en: '*It’s a fair game*.'
  id: totrans-176
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*这是一个公平的游戏*。'
- en: Others appeared to be pessimistic about accessing the benefits of LLM-based
    CAs while also preserving privacy, feeling that
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其他人似乎对在保持隐私的同时获取基于大型语言模型（LLM）的对话助手（CAs）的好处持悲观态度，认为
- en: '*you can’t have it both ways* (P9, P19).'
  id: totrans-178
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*你不能两全其美*（P9, P19）。'
- en: For example, P10 used ChatGPT to revise her resume and shared detailed work
    experiences. She considered it a necessary trade-off, stating,
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，P10 使用 ChatGPT 修改她的简历，并分享了详细的工作经历。她认为这是必要的权衡，表示，
- en: '*Let’s say I need some advice about resume. If I don’t provide those contents
    that contain a lot of my private things, ChatGPT won’t work.*'
  id: totrans-180
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*假设我需要一些关于简历的建议。如果我不提供包含很多私人信息的内容，ChatGPT 就不能起作用。*'
- en: To some participants, ChatGPT has provided irreplaceable value and has become
    an indispensable part of their life. For example, P15 said
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对一些参与者来说，ChatGPT 提供了不可替代的价值，已成为他们生活中不可或缺的一部分。例如，P15说
- en: '*I cannot imagine myself doing my work, my daily activities, without ChatGPT.*'
  id: totrans-182
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我无法想象自己在没有 ChatGPT 的情况下做工作、进行日常活动。*'
- en: 5.3.2\. Avoid Tasks Requiring Personal Data Due to Privacy Concerns (P2, P3,
    P9, P14, P19)
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 由于隐私问题避免涉及个人数据的任务（P2, P3, P9, P14, P19）
- en: Some participants mentioned that their privacy concerns for certain tasks were
    so significant that they avoided using ChatGPT for those tasks. For example, P19
    once tried to ask ChatGPT for financial advice, and felt it would be really nice
    if he could provide detailed information like
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参与者提到，他们对某些任务的隐私担忧如此重大，以至于他们避免使用 ChatGPT 进行这些任务。例如，P19 曾尝试向 ChatGPT 请求财务建议，并觉得如果他能提供像这样详细的信息，那就太好了：
- en: '*the amount of money that we bring in, the kids that we have.*'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我们带进来的钱，和我们拥有的孩子。*'
- en: However, he also stated,
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，他也表示，
- en: '*I just never felt comfortable doing that.*'
  id: totrans-187
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我只是从来不觉得做这件事让我感到舒服。*'
- en: As a result, he only provided generic information, which was less helpful, leading
    him to terminate the conversation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，他只提供了通用信息，这些信息帮助不大，导致他中断了对话。
- en: 5.3.3\. Manually Sanitize Inputs (All but P5, P8, P11, P15)
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.3\. 手动清理输入（除了 P5, P8, P11, P15）
- en: While participants generally employed one of the two strategies above, we found
    nearly everyone had employed one or more of three ad-hoc privacy-protective measures
    to try to find a middle ground between privacy and utility when possible.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管参与者通常采用上述两种策略之一，但我们发现几乎每个人都采取了一个或多个临时的隐私保护措施，尽可能在隐私和实用性之间找到平衡。
- en: Censor and/or Falsify Sensitive Information (P1, P3, P6, P7, P9, P10, P12, P13,
    P16, P19)
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 审查和/或伪造敏感信息（P1, P3, P6, P7, P9, P10, P12, P13, P16, P19）
- en: Many participants mentioned that they avoided disclosing sensitive or identifiable
    information such as their name, social security number, and location. Participants
    sometimes chose to only provide coarse-grained or even fake information. For example,
    P3 provided a different college name from her alma mater that is in the same university
    system and asked for career advice, and P13 mentioned he had given ChatGPT fake
    names and fake information.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者提到，他们避免透露敏感或可识别的信息，如姓名、社会安全号码和位置。参与者有时选择只提供粗略的甚至是假的信息。例如，P3提供了一个与她母校同属一个大学系统的不同学院名称，并询问职业建议；P13提到他曾给ChatGPT提供虚假的姓名和信息。
- en: Sanitize Inputs Copied from Other Contexts (P4, P17, P18)
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清理从其他情境中复制的输入（P4, P17, P18）
- en: Tasks like copy-editing and programming require users to provide content copied
    from other contexts. Some participants mentioned that they post-processed these
    inputs to sanitize the data. For example, P4 and P18 removed personal information
    in emails that they asked ChatGPT to revise and manually added that information
    back later. P17 replaced document names included in a PowerShell script with placeholders
    before sharing it with ChatGPT. In addition to redacting sensitive information,
    P17 also mentioned limiting the amount of information he shared with ChatGPT in
    any one prompt. He used ChatGPT to proofread his paper and only copied one or
    a small number of sentences each time due to concerns that ChatGPT might remember
    the entire paper.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 诸如编辑文稿和编程等任务需要用户提供从其他情境中复制的内容。有些参与者提到他们会对这些输入进行后处理，以清除数据中的敏感信息。例如，P4和P18删除了他们要求ChatGPT修订的邮件中的个人信息，之后手动将这些信息重新添加回去。P17在与ChatGPT分享PowerShell脚本之前，用占位符替换了文档名。除了编辑敏感信息，P17还提到限制他在任何一个提示中分享的信息量。他使用ChatGPT来校对他的论文，每次只复制一小段或少数几句，因为他担心ChatGPT可能会记住整篇论文。
- en: Only Seek General Advice (P2, P10, P14, P17)
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 只寻求一般性建议（P2, P10, P14, P17）
- en: Some participants only sought general advice, trading the specificity of the
    response they received for improved privacy. For example, P14 mentioned he only
    felt comfortable having general conversations with ChatGPT about business. This
    strategy can require users to spend more time summarizing the task, making their
    conversations not only less specific but also less convenient. For example, P10
    mentioned that she used ChatGPT to help with data analysis tasks at work. Since
    she was not permitted to share the raw data, she needed to summarize the data
    schema to share with ChatGPT, rather than directly asking ChatGPT to generate
    the code based on the data.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 有些参与者仅寻求一般性的建议，通过牺牲他们所收到回应的具体性来换取更好的隐私保护。例如，P14提到他只愿意与ChatGPT就商业话题进行一般性的对话。这种策略可能需要用户花更多时间来总结任务内容，从而使他们的对话不仅不那么具体，而且也不那么方便。例如，P10提到她在工作中使用ChatGPT帮助进行数据分析任务。由于她不被允许共享原始数据，她需要总结数据结构以便与ChatGPT共享，而不是直接要求ChatGPT根据数据生成代码。
- en: 5.4\. Mental Models of How LLM-Based CAs Handle User Input
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4. 心理模型：LLM-based CAs如何处理用户输入
- en: 'We summarize users’ mental models for two processes: response generation (Model
    A: “ChatGPT is magic”, B: “ChatGPT is a super searcher”, C: “ChatGPT is a stochastic
    parrot”) and model improvement and training (Model D: “User input is a quality
    indicator”, E: “User input is training data”). We found that users’ mental models
    demonstrated overly simplified or flawed understandings of how their data was
    used and how LLM-based CAs worked.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总结了用户的心理模型，涉及两个过程：回应生成（模型A：“ChatGPT是魔法”，B：“ChatGPT是超级搜索者”，C：“ChatGPT是随机鹦鹉”）以及模型改进与训练（模型D：“用户输入是质量指标”，E：“用户输入是训练数据”）。我们发现，用户的心理模型展示了他们对数据如何使用以及基于LLM的CA如何工作的理解过于简化或存在缺陷。
- en: 5.4.1\. Mental Models of Response Generation
  id: totrans-199
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1. 回应生成的心理模型
- en: 'Model A: ChatGPT is magic (P8, P10, P12, P15)'
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型A：ChatGPT是魔法（P8, P10, P12, P15）
- en: '![Refer to caption](img/0aa484e7e7bb8efd9b595b5a63ff52f2.png)\Description'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![请参阅说明文字](img/0aa484e7e7bb8efd9b595b5a63ff52f2.png)\说明'
- en: '[Mental model drawing]This figure illustrates a drawing created by a participant
    with mental model A who thought the ChatGPT is an AI Blackbox. On the left of
    the image is a small figure, symbolizing the user. Arrows indicate data flow:
    starting from the small figure (user) with an arrow pointing right labeled ”interact”
    and ”input data”, leading to multiple small circles labeled ”blockchain”. From
    there, another arrow points to a larger oval labeled ”LLM 4”, which then points
    to a computer labeled ”output”. A large circle encompasses the user figure and
    computer, denoting local processes, while another encircles the blockchain circles
    and oval, indicating remote processes.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[心智模型图示] 这张图展示了一位持有心智模型 A 的参与者所画的图，他认为 ChatGPT 是一个 AI 黑箱。图示左侧是一个小人物，象征用户。箭头表示数据流：从小人物（用户）开始，箭头指向右侧，标注为“互动”和“输入数据”，然后指向多个小圆圈，标注为“区块链”。从这些圆圈出发，另有一箭头指向一个更大的椭圆形，标注为“LLM
    4”，接着指向一个标有“输出”的计算机。一个大圆圈包含了用户图示和计算机，表示本地处理，另一个圆圈包围了区块链圆圈和椭圆，表示远程处理。'
- en: 'Figure 2. Screenshot of P8’s drawing representing mental model A: ChatGPT is
    magic.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. P8 绘制的图示，代表心智模型 A：ChatGPT 是魔法。
- en: 'Mental model A represents a shallow technical understanding of how ChatGPT
    generates responses. Participants who harbored this mental model thought of the
    generation process as an abstract transaction: messages are sent to an LLM or
    a database, and an output is received. P8 illustrated a typical example of this
    model, shown in  [Figure 2](https://arxiv.org/html/2309.11653v2#S5.F2 "Figure
    2 ‣ Model A: ChatGPT is magic (P8, P10, P12, P15) ‣ 5.4.1\. Mental Models of Response
    Generation ‣ 5.4\. Mental Models of How LLM-Based CAs Handle User Input ‣ 5\.
    Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"). In
    her words:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 心智模型 A 代表了对 ChatGPT 如何生成回应的浅层技术理解。持有这一心智模型的参与者认为生成过程是一个抽象的交易：消息被发送到 LLM 或数据库，然后接收输出。P8
    通过 [图 2](https://arxiv.org/html/2309.11653v2#S5.F2 "图 2 ‣ 模型 A：ChatGPT 是魔法 (P8,
    P10, P12, P15) ‣ 5.4.1. 回应生成的心智模型 ‣ 5.4. 基于 LLM 的对话代理如何处理用户输入的心智模型 ‣ 5. 面试结果 ‣
    '这是公平的游戏'，还是它？审视用户在使用基于 LLM 的对话代理时如何平衡披露风险和利益") 举例说明了这一模型。在她的话中：
- en: '*ChatGPT uses the computing power to generate something to send to the LLM,
    the model of ChatGPT. And then you get your output data…Actually it likes a blackbox
    for me. I just use it. I mean, I never thought about that before.*'
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*ChatGPT 利用计算能力生成某些内容并发送到 LLM，即 ChatGPT 的模型。然后你得到输出数据……实际上对我来说，它就像一个黑箱。我只是使用它。我是说，我以前从未思考过这个。*'
- en: Similarly, P10 described the response generation process as
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，P10 描述了回应生成过程为：
- en: '*some kinds of magic I don’t know*.'
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*有些魔法我不知道*。'
- en: 'Model B: ChatGPT is a super searcher (P1, P2, P3, P4, P5, P7, P16, P19)'
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型 B：ChatGPT 是一个超级搜索引擎（P1, P2, P3, P4, P5, P7, P16, P19）
- en: '![Refer to caption](img/f6db3f5b8697b8530056cb7e0283916c.png)\Description'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见图注](img/f6db3f5b8697b8530056cb7e0283916c.png)\描述'
- en: '[Mental model drawing]This figure illustrates a drawing created by a participant
    with mental model B who thought the ChatGPT searches for relevant information
    and synthesizes the results into text response. On the left, a small figure symbolizes
    the user. Data flow is shown by arrows: from the user, one arrow extends right
    with labels ”interact” and ”input data”. It bifurcates into two: the top arrow
    pointing right-top reads ”send to the database for research” and the bottom one
    progresses rightward with labels showing the sequence: ”input gets broken down”
    -¿ ”look for keywords” -¿ ”search database for answers” -¿ ”combine answer using
    others’ data” -¿ ”return answer”.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[心智模型图示] 这张图展示了一位持有心智模型 B 的参与者所画的图，他认为 ChatGPT 会搜索相关信息并将结果合成文本回应。图示左侧是一个小人物，象征用户。数据流通过箭头表示：从用户开始，箭头向右延伸，标注为“互动”和“输入数据”。箭头分成两条：上方的箭头指向右上方，标注为“发送到数据库进行研究”，下方的箭头向右延伸，显示了序列：”输入被拆解“
    -¿ ”寻找关键词“ -¿ ”搜索数据库获取答案“ -¿ ”结合其他数据生成答案“ -¿ ”返回答案“。'
- en: 'Figure 3. Screenshot of P4’s drawing representing mental model B: ChatGPT is
    a super searcher.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. P4 绘制的图示，代表心智模型 B：ChatGPT 是一个超级搜索引擎。
- en: 'Participants with this mental model often envisioned the response generation
    process as a form of live keyword search on the internet or in a database sourced
    from the internet, followed by a synthesis of the gathered information. As shown
    in [Figure 3](https://arxiv.org/html/2309.11653v2#S5.F3 "Figure 3 ‣ Model B: ChatGPT
    is a super searcher (P1, P2, P3, P4, P5, P7, P16, P19) ‣ 5.4.1\. Mental Models
    of Response Generation ‣ 5.4\. Mental Models of How LLM-Based CAs Handle User
    Input ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents"),
    P4 generally described the generation process as'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种思维模型的参与者通常将回应生成过程视为一种实时的关键词搜索，搜索范围可能是在互联网或从互联网获取的数据库中，接着将收集到的信息进行合成。如[图3](https://arxiv.org/html/2309.11653v2#S5.F3
    "图3 ‣ 模型B：ChatGPT是超级搜索器（P1, P2, P3, P4, P5, P7, P16, P19） ‣ 5.4.1.回应生成的思维模型 ‣
    5.4.大模型在如何处理用户输入的思维模型 ‣ 5.采访结果 ‣ “这是公平的游戏”，还是不公平？检查用户在使用基于LLM的对话代理时如何应对披露风险与收益")中所示，P4通常描述回应生成过程为
- en: '*My input will get broken down. And then look for keywords…After the keywords,
    will start searching in their database, trying to find an answer. At this point
    they might try to combine the answer.*'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我的输入将被拆解。然后查找关键词……在关键词之后，将开始在他们的数据库中搜索，尝试找到答案。此时他们可能会尝试将答案合成起来。*'
- en: Participants with this mental model often expected rule-based methods or human
    interventions to play a role in generating the responses. For example, P19 envisioned
    rules that match keywords to databases pulled from the internet. P16 assumed that
    there were humans-in-the-loop in generating responses. Apart from the internet-at-large,
    a few participants noted that the system might include other data resources. For
    example, P7 believed users’ conversations would be included in ChatGPT’s knowledge
    base.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种思维模型的参与者通常期望基于规则的方法或人工干预在生成回答时发挥作用。例如，P19设想了将关键词与从互联网获取的数据库匹配的规则。P16则假设在生成回答时有人工干预的过程。除了广泛的互联网，一些参与者指出，系统可能还包括其他数据资源。例如，P7认为用户的对话会被纳入ChatGPT的知识库。
- en: Model C. ChatGPT is a stochastic parrot (P6, P11, P13, P14, P17, P18)
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型C。ChatGPT是一个随机鹦鹉（P6, P11, P13, P14, P17, P18）
- en: '![Refer to caption](img/17c7dea5b0e9f97d51540cbe3587b55e.png)\Description'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见标题](img/17c7dea5b0e9f97d51540cbe3587b55e.png)\描述'
- en: '[Mental model drawing]This figure illustrates a drawing created by a participant
    with mental model C who thought the ChatGPT uses an end-to-end machine learning
    model for response generation. On the left, a small figure represents the user.
    Arrows show data flow: starting with the user, an arrow labeled ”interact” and
    ”input data” points to a rectangle symbolizing the internet. This internet rectangle
    connects to two other rectangles: ”processed” and ”OpenAI servers,” forming a
    loop among the three. Finally, an arrow from the internet rectangle loops back
    to the user, denoting the returned responses.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维模型图示]此图展示了一位拥有思维模型C的参与者所创作的图示，该参与者认为ChatGPT使用端到端机器学习模型生成回应。在图的左侧，一个小人代表用户。箭头表示数据流：从用户开始，一条标注为“交互”和“输入数据”的箭头指向一个矩形，代表互联网。这个互联网矩形连接到另外两个矩形：“处理过的”和“OpenAI服务器”，形成三者之间的循环。最后，一条来自互联网矩形的箭头回到用户，表示返回的回应。'
- en: 'Figure 4. Screenshot of P14’s drawing representing mental model C: ChatGPT
    is a stochastic parrot. P14 verbally explained how the end-to-end machine learning
    model generates a response in technical detail.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。P14所画的图示，代表思维模型C：ChatGPT是一个随机鹦鹉。P14详细地口头解释了端到端机器学习模型是如何生成响应的。
- en: Participants with Model C articulated a more sophisticated understanding of
    the response generation process. They believed that the response generation was
    handled by an end-to-end machine learning model that stochastically generated
    each word, in sequence, based on user input and previously generated words. Unlike
    Model B, users with Model C did not expect the system to have separate components
    for understanding the query, gathering related information, and generating the
    reply; instead, they understood that the response was generated by a single, trained
    model. For example, P14 drew [Figure 4](https://arxiv.org/html/2309.11653v2#S5.F4
    "Figure 4 ‣ Model C. ChatGPT is a stochastic parrot (P6, P11, P13, P14, P17, P18)
    ‣ 5.4.1\. Mental Models of Response Generation ‣ 5.4\. Mental Models of How LLM-Based
    CAs Handle User Input ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It?
    Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based
    Conversational Agents") and verbalized,
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有模型 C 的参与者对回复生成过程有更为复杂的理解。他们认为回复生成是由一个端到端的机器学习模型处理的，该模型基于用户输入和先前生成的单词以随机的方式依次生成每个单词。与模型
    B 不同，拥有模型 C 的用户并不期望系统有独立的组件来理解查询、收集相关信息和生成回复；相反，他们理解回复是由一个单一的、经过训练的模型生成的。例如，P14
    绘制了[图 4](https://arxiv.org/html/2309.11653v2#S5.F4 "图 4 ‣ 模型 C. ChatGPT 是一个随机鹦鹉
    (P6, P11, P13, P14, P17, P18) ‣ 5.4.1\. 回复生成的心理模型 ‣ 5.4\. 基于大语言模型的对话代理如何处理用户输入的心理模型
    ‣ 5\. 面试结果 ‣ ”这是一场公平的游戏“，还是不是？探讨用户在使用基于大语言模型的对话代理时如何权衡披露风险和利益")并口头表述，
- en: '*It (my input data) would go through the Internet, then goes to their servers…They’re
    just getting a big block of vectors, and makes its predictive actions, does all
    of its what you know processing, and then it produces a response.*'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*它（我的输入数据）会通过互联网，然后到达他们的服务器……他们只是获得了一大块向量，然后做出预测行动，进行所有你知道的处理，然后生成一个回复。*'
- en: All participants with this mental model had technical backgrounds.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 所有拥有这种心理模型的参与者都具有技术背景。
- en: 5.4.2\. Mental Models on Improvement and Training
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 关于改进和训练的心理模型
- en: 'Model D: User input is a quality indicator (P1, P3, P4, P6, P9, P10, P15, P19)'
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型 D：用户输入是质量指标（P1, P3, P4, P6, P9, P10, P15, P19）
- en: Users with Mental Model D believed that their inputs were used to assess their
    satisfaction with responses, and that the system would learn over time to produce
    responses more similar to those that were rated highly. Others felt their inputs
    were used as a semantic key to help index similar questions. Thus, if someone
    asks a similar question, the LLM can select and respond with answers to similar
    questions that were rated more positively. Because they felt that user input was
    isolated from system output, participants with this model were less able to expect
    and understand memorization risks. For example, P1 could not see how the personal
    data mentioned in her prompt, such as zip code, can be used in generating future
    responses. P3 believed that human reviewers might label previous responses as
    good or bad, and determine if those responses can be reused in the future.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有模型 D 的用户认为他们的输入用于评估他们对回复的满意度，并且系统会随着时间的推移学习生成更类似于高评分回复的回应。其他人则认为他们的输入被用作语义关键，用来帮助索引类似的问题。因此，如果有人问类似的问题，LLM
    可以选择并用之前评价更高的类似问题的答案来回应。由于他们认为用户输入与系统输出是隔离的，拥有这种模型的参与者不太能够预见和理解记忆风险。例如，P1 无法理解她在提示中提到的个人数据，如邮政编码，如何用于生成未来的回复。P3
    认为人工审阅者可能会将之前的回复标记为好或坏，并决定这些回复是否可以在未来被重新使用。
- en: This mental model resembles the reinforcement learning from human feedback (RLHF)
    method that OpenAI used to train the InstructGPT model (Ouyang et al., [2022](https://arxiv.org/html/2309.11653v2#bib.bib56)).
    However, with RLHF, the context presented in the prompt may also be memorized
    and regenerated by future models, contradicting the expectations of people who
    hold this mental model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这种心理模型类似于 OpenAI 用于训练 InstructGPT 模型的强化学习来自人类反馈（RLHF）方法（Ouyang 等，[2022](https://arxiv.org/html/2309.11653v2#bib.bib56)）。然而，在
    RLHF 中，提示中呈现的上下文也可能被记住并由未来的模型重新生成，这与持有这种心理模型的人们的预期相悖。
- en: 'Model E: User input is training data (P2, P5, P7, P11, P12, P13, P14, P16,
    P17, P18).'
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型 E：用户输入是训练数据（P2, P5, P7, P11, P12, P13, P14, P16, P17, P18）。
- en: Users with this type of mental model understood their their input prompts could
    be used to influence future responses. For example, P11 said if his input data
    had been used for training,
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这种心理模型的用户理解他们的输入提示可以用来影响未来的回复。例如，P11 说如果他的输入数据用于训练，
- en: '*there’s a possibility that’s going to use the data as an output for somebody
    else.*'
  id: totrans-228
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*有可能它会将数据作为输出提供给其他人。*'
- en: Some people with this type of mental model had a technical background (P11,
    P13, P14, P17, P18). Others without a technical background did not know the specific
    training process that could be used, but they expected that their information
    could be reused in future responses to other users.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一些持有这种心态的参与者有技术背景（P11、P13、P14、P17、P18）。而没有技术背景的人则不清楚具体的训练过程，但他们预期他们的信息可能会在未来的回答中被重新利用，提供给其他用户。
- en: We found participants had different expectations as to whether their inputs
    are used to improve a global model accessible to all users (P7, P11, P12, P13,
    P14, P16, P17, P18) or a personalized model used only by themselves (P2, P5).
    P5 believed that the model is personalized to each user based on their inputs.
    This was because she experienced significantly improved responses from ChatGPT
    within a conversation thread. Similarly, P2 was hesitant to share her ChatGPT
    account with others because she believed the model was personalized and that inputs
    from others could adversely affect its training.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现参与者对于他们的输入是否会被用来改进一个全球模型（所有用户可访问）或是只供自己使用的个性化模型有不同的期望（P7、P11、P12、P13、P14、P16、P17、P18）。P5认为模型是根据每个用户的输入个性化的。这是因为她在一次对话中体验到了ChatGPT响应的显著改进。类似地，P2不愿意与他人共享她的ChatGPT账户，因为她认为模型是个性化的，其他人的输入可能会对其训练产生不利影响。
- en: 5.5\. Users’ Awareness and Reactions to Memorization Risks in LLM-based CAs
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 用户对LLM-based CAs中记忆风险的认知和反应
- en: 'Prior work has shown that LLMs can memorize training data and leak this training
    data as a response to the right prompt (Carlini et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib10),
    [2022](https://arxiv.org/html/2309.11653v2#bib.bib9)). This memorization effect
    entails unique privacy risks for using LLM-based CAs that utilize user data to
    improve their models. However, most participants lacked awareness of this issue,
    and only P2, P17 and P18 brought up this issue before we explicitly asked about
    it. P18 thought memorization risks allow for new privacy attacks, and P2 and P17
    brought it up as a concern that had limited their data sharing with ChatGPT for
    real-world use cases. After we explained memorization risks to the other 16 participants,
    most participants remained unconcerned. Four participants (P5, P12, P13, P19)
    expressed surprise and heightened concern about disclosing personal data to ChatGPT:
    especially data about emotional states, work-related information, and sensitive
    PII like social security numbers. Three of them stated that they might alter their
    data-sharing behaviors in the future.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，LLM（大规模语言模型）可以记住训练数据，并在收到正确的提示时泄露这些训练数据作为响应（Carlini et al.，[2021](https://arxiv.org/html/2309.11653v2#bib.bib10)，[2022](https://arxiv.org/html/2309.11653v2#bib.bib9)）。这种记忆效应带来了使用基于LLM的聊天助手（CAs）时的独特隐私风险，这些聊天助手利用用户数据来改善其模型。然而，大多数参与者并未意识到这一问题，只有P2、P17和P18在我们明确询问之前提到过这一问题。P18认为记忆风险可能引发新的隐私攻击，而P2和P17则表示这是他们在真实世界使用ChatGPT时限制数据共享的一个原因。在我们向其他16位参与者解释了记忆风险后，大多数人依然不以为意。四位参与者（P5、P12、P13、P19）对向ChatGPT泄露个人数据表示惊讶，并表现出更高的关注，特别是涉及情感状态、工作相关信息以及敏感的个人身份信息（PII），例如社会保障号码。他们中的三人表示，未来可能会改变数据共享行为。
- en: '5.5.1\. Concerned with Memorization: Prior Exposure to Leaks (P17)'
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1. 对记忆的担忧：先前泄露的经历（P17）
- en: P17 personally encountered a memorization leak when using Copilot, which made
    him concerned about the possibility of his own personal information being memorized
    by ChatGPT.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: P17在使用Copilot时亲自遇到了记忆泄露事件，这使得他对自己个人信息可能被ChatGPT记住的可能性感到担忧。
- en: '*When the Copilot plugin for VS Code was published, I installed it. I typed
    the name of my classmate and it auto completed my classmate’s school ID. It’s
    very terrible.* (P17)'
  id: totrans-235
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*当VS Code的Copilot插件发布时，我安装了它。我输入了我同学的名字，它自动填充了我同学的学校ID。太可怕了。*（P17）'
- en: '5.5.2\. Concerned with Memorization: Intellectual Property Leaks (P2, P17)'
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2. 对记忆的担忧：知识产权泄露（P2, P17）
- en: P2 and P17 expressed concerns ChatGPT memorizing and distributing their original
    writing without credit, notice, or consent. P2 hesitated to use ChatGPT to review
    her short stories, worrying that ChatGPT might generate new content based on her
    original work or inspire other users with her work. Similarly, P17 was cautious
    when sharing unpublished paper content and limited the input in each session to
    prevent the system from disclosing the whole paper or key ideas to other users
    before publication.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: P2 和 P17 表达了对 ChatGPT 记住并分发他们原创作品而没有信用、通知或同意的担忧。P2 犹豫是否使用 ChatGPT 来审阅她的短篇小说，担心
    ChatGPT 可能会基于她的原创作品生成新内容或激发其他用户的创作。类似地，P17 在分享未公开的论文内容时十分谨慎，并且限制了每次会话的输入量，以防系统在发布之前将整篇论文或关键思想透露给其他用户。
- en: '5.5.3\. Unconcerned with Memorization: Does Not Share Sensitive Data (P1, P6,
    P7, P8, P9, P10, P11, P16)'
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. 不担心记忆：没有分享敏感数据（P1, P6, P7, P8, P9, P10, P11, P16）
- en: After being introduced to memorization risks when using ChatGPT, most participants
    expressed minimal concerns, primarily because they had not shared data they deemed
    sensitive, or because they believed even the sensitive data being memorized was
    not linked to their identities. For instance, P9 was not particularly worried
    because she has been cautious and has not shared personal information beyond her
    age and the city where she lives. P6 was unconcerned because she believed any
    sensitive data memorized by the AI would not be linked to her identity.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解使用 ChatGPT 时的记忆风险后，大多数参与者表示担忧较少，主要因为他们没有分享被认为敏感的数据，或者他们认为即使敏感数据被记住，也与他们的身份无关。例如，P9
    并不特别担心，因为她一直小心谨慎，除了自己的年龄和居住的城市外，没有分享过个人信息。P6 也不担心，因为她认为 AI 记住的任何敏感数据都不会与她的身份相关联。
- en: '*It may remember some things, but I assume that it would disassociate my specific
    identity with what it memorized.* (P6)'
  id: totrans-240
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*它可能记住一些东西，但我认为它会把我的具体身份与它记住的内容分开。*（P6）'
- en: '5.5.4\. Unconcerned with Memorization: The Risk is Too Abstract (P1, P8, P10)'
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.4\. 不担心记忆：风险太过抽象（P1, P8, P10）
- en: 'Some participants expressed limited concern about memorization risks because
    the risks were difficult to comprehend. The difficulty was tied to their mental
    model of how ChatGPT improves AI performance based on their input. For example,
    both P1 and P10 held Mental Model D⁶⁶6We were not able to obtain a clear answer
    from P8 about her mental model about improvement and training., and believed that
    ChatGPT treated their input data strictly as a quality indicator rather than as
    training data. This mental model, in turn, made it hard to imagine memorization
    leaks (see [Section 5.4.2](https://arxiv.org/html/2309.11653v2#S5.SS4.SSS2.Px1
    "Model D: User input is a quality indicator (P1, P3, P4, P6, P9, P10, P15, P19)
    ‣ 5.4.2\. Mental Models on Improvement and Training ‣ 5.4\. Mental Models of How
    LLM-Based CAs Handle User Input ‣ 5\. Interview Results ‣ ”It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents")). The absence of prior exposure to memorization
    leaks also added to their difficulty in imagining memorization leaks.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '一些参与者对记忆风险的担忧较少，因为这些风险很难理解。困难的原因与他们对 ChatGPT 如何根据输入提高 AI 性能的思维模型有关。例如，P1 和
    P10 都持有思维模型 D⁶⁶6我们未能从 P8 获得关于她改进和训练的思维模型的明确回答。并且认为 ChatGPT 将他们的输入数据严格视为质量指标，而不是训练数据。这一思维模型使得他们难以想象记忆泄漏（见
    [第5.4.2节](https://arxiv.org/html/2309.11653v2#S5.SS4.SSS2.Px1 "Model D: User input
    is a quality indicator (P1, P3, P4, P6, P9, P10, P15, P19) ‣ 5.4.2\. Mental Models
    on Improvement and Training ‣ 5.4\. Mental Models of How LLM-Based CAs Handle
    User Input ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")）。缺乏对记忆泄漏的先前接触也使得他们更难以想象记忆泄漏的情形。'
- en: '*I don’t have this kind of experiences like my information was shown to others
    as output. And when I imagine it, I’m not so confident about it…I haven’t seen
    this kind of news before.* (P8)'
  id: totrans-243
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我没有过这种经验，像我的信息被当作输出展示给别人。当我想象时，我也不太自信……我之前没有见过这种新闻。*（P8）'
- en: '5.5.5\. Unconcerned with Memorization: Plausible Deniability of AI-Generated
    Output (P3, P8)'
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.5\. 不担心记忆：AI 生成输出的合理否认（P3, P8）
- en: Some participants felt unconcerned because the model would not produce accurate
    information about them (P3), and believed that others may not perceive the output
    as accurate information (P8). For example, P3 was indifferent about the potential
    disclosure of her weight — which she considered to be sensitive data — since she
    had provided ChatGPT with incorrect information. P8 was not concerned due to a
    similar reason.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参与者感到不以为然，因为该模型不会生成关于他们的准确信息（P3），并且认为其他人可能不会将输出视为准确信息（P8）。例如，P3 对可能泄露她的体重—她认为这是敏感数据—并不在意，因为她向
    ChatGPT 提供了错误的信息。P8 也因为类似的原因不感到担忧。
- en: '*Although I use ChatGPT for many tasks, I’m not concerned about my input being
    released to other people, because they can’t judge if it’s fake information.*
    (P8)'
  id: totrans-246
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*尽管我将 ChatGPT 用于许多任务，但我并不担心我的输入会泄露给其他人，因为他们无法判断这些信息是否是假的。*（P8）'
- en: 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing Privacy
    Controls
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6. 用户对现有隐私控制的认知、理解和态度
- en: Finally, we examined how users utilize existing support to protect their privacy
    and how they would like the system to be improved to make them feel safer.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们考察了用户如何利用现有的支持措施来保护他们的隐私，以及他们希望如何改进系统以让自己感觉更安全。
- en: 5.6.1\. Users Lack Awareness of Existing Privacy Controls (All but P11, P14,
    P16, P17, P18)
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.1. 用户缺乏对现有隐私控制的认知（除了 P11、P14、P16、P17 和 P18）
- en: ChatGPT provides a control that allows users to opt out of having their conversations
    used by OpenAI for training, negating memorization risks. However, most participants
    had not heard about this control. After learning about it, most participants felt
    it was a good feature. Many participants expressed a desire to use it even though
    they did not feel concerned about what they shared with ChatGPT. For example,
    P2 said
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 提供了一项控制功能，允许用户选择退出将其对话用于 OpenAI 的训练，从而避免了记忆风险。然而，大多数参与者并不知道这个控制功能。了解此功能后，大多数参与者认为它是一个不错的功能。许多参与者表示，尽管他们并不担心自己与
    ChatGPT 分享的内容，他们仍希望使用这一功能。例如，P2 说
- en: '*even though I know there’s nothing bad that’s gonna happen. But still, I would
    just want the peace of mind of being able to opt out.*'
  id: totrans-251
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*尽管我知道不会发生什么坏事，但我还是想要能够选择退出以获得心理上的安宁。*'
- en: 5.6.2\. Dark Patterns Impeded Adoption of the Opt-out Control (All participants)
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.2. 黑暗模式妨碍了退出控制的采纳（所有参与者）
- en: '![Refer to caption](img/e32728b159904fbae9b36b3c19441a3e.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e32728b159904fbae9b36b3c19441a3e.png)'
- en: (a) Chat history & training bundled together (easier to discover)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: （a）聊天历史与训练捆绑在一起（更容易发现）
- en: '![Refer to caption](img/2b51ed13e7c381089472b59012102b8a.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2b51ed13e7c381089472b59012102b8a.png)'
- en: (b) Turning off training and keeping history (harder to discover)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: （b）关闭训练并保留历史（更难发现）
- en: 'Figure 5. Dark patterns in ChatGPT: ChatGPT offers two ways for a user to opt
    out of having their data used for model training. The one in the user settings
    is easier to discover (all but P15 found it), while the training and chat history
    opt-out control are bundled together, so a user who wants to opt out of model
    training will have to turn off the chat history feature as well. The users could
    also submit a form to opt out of training and keep the history, while it is in
    an [FAQ article](https://help.openai.com/en/articles/7730893-data-controls-faq)
    that is harder to discover (none of our participants found it). The above issues
    were observed during our studies in August 2023\. As of November 2023, the form
    link is still in the FAQ article, while this form has been disabled and it further
    directs users to the OpenAI Privacy Request Portal to submit privacy requests.
    As of February 2023, the form link is replaced with the link to the privacy portal.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. ChatGPT 中的黑暗模式：ChatGPT 提供了两种方式供用户选择退出将其数据用于模型训练。在用户设置中的选项更容易发现（除了 P15 之外的所有人都找到了它），而训练和聊天历史的退出控制是捆绑在一起的，因此想要退出模型训练的用户必须同时关闭聊天历史功能。用户还可以提交表格选择退出训练并保留历史记录，而该表格位于一个[FAQ
    文章](https://help.openai.com/en/articles/7730893-data-controls-faq)中，但该文章较难被发现（我们的参与者中没有人发现它）。上述问题是在我们
    2023 年 8 月的研究中观察到的。截至 2023 年 11 月，表格链接仍在 FAQ 文章中，但该表格已被禁用，并且进一步引导用户访问 OpenAI 隐私请求门户提交隐私请求。到
    2023 年 2 月，表格链接已被隐私门户的链接取代。
- en: \Description
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: '[Two screenshots of dark patterns in ChatGPT]The two figures illustrate two
    methods for opting out, with image (a) highlighting an easily accessible method
    via ChatGPT’s interface and image (b) a more obscure method found in OpenAI’s
    Data Control FAQ. (a) On the left, the ChatGPT settings pop-up is displayed with
    ”Data controls” selected in the navigation. At the top is the ”Chat history &
    training” header accompanied by an activated switch for Opt-in. The subsequent
    text details that saved chats are retained for history and may be used to enhance
    models, while chats not saved are deleted after 30 days. A clickable ”Learn more”
    link is also present. (b) On the right, a section of OpenAI’s Data Control FAQ
    is displayed. Titled ”What if I want to keep my history on but disable model training?”
    it mentions the upcoming ChatGPT Business offering which defaults to opting users
    out of model training. Meanwhile, users can opt out by completing a linked form,
    ensuring that new conversations won’t be used for training.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[ChatGPT中的暗黑模式截图]这两张图展示了两种退出数据使用的方法，图（a）突出显示了通过ChatGPT界面可以轻松访问的方法，而图（b）则展示了在OpenAI的《数据控制FAQ》中找到的更为隐蔽的方法。(a)
    在左侧，展示了ChatGPT设置弹窗，其中导航栏中选中了“数据控制”。顶部是“聊天记录与训练”标题，旁边有一个启用的切换按钮用于选择加入。接下来的文字说明保存的聊天记录将用于历史保存，并可能被用来增强模型，而未保存的聊天记录会在30天后删除。还有一个可点击的“了解更多”链接。(b)
    在右侧，展示了OpenAI《数据控制FAQ》中的一部分，标题为“如果我想保持历史记录但禁用模型训练怎么办？”，其中提到了即将推出的ChatGPT商业版，它默认将用户排除在模型训练之外。同时，用户可以通过填写链接表单来选择退出，确保新的对话不会用于训练。'
- en: We identified multiple dark patterns in ChatGPT’s opt-out design that hindered
    users’ adoption of this feature. First, ChatGPT uses user data for training by
    default. This surprised many participants, especially for P17 who was a paid ChatGPT
    Plus user.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在ChatGPT的退出设计中发现了多个暗黑模式，阻碍了用户采纳此功能。首先，ChatGPT默认使用用户数据进行训练。这让许多参与者感到惊讶，尤其是P17，他是付费的ChatGPT
    Plus用户。
- en: '*I think OpenAI should not use my data (for model training), because I paid
    for my ChatGPT.* (P17)'
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我认为OpenAI不应该使用我的数据（用于模型训练），因为我为我的ChatGPT付费了。*（P17）'
- en: Second, there are two ways for users to opt out of having their data used for
    model training ([5(b)](https://arxiv.org/html/2309.11653v2#S5.F5.sf2 "5(b) ‣ Figure
    5 ‣ 5.6.2\. Dark Patterns Impeded Adoption of the Opt-out Control (All participants)
    ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing Privacy
    Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). One was easier to discover (all but P15 found it), but it forces users
    to turn off storing chat history at the same time ([5(a)](https://arxiv.org/html/2309.11653v2#S5.F5.sf1
    "Figure 5 ‣ 5.6.2\. Dark Patterns Impeded Adoption of the Opt-out Control (All
    participants) ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing
    Privacy Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining
    How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). Several participants said they were hesitant (P11) or did not want
    (P8, P7, P17, P18) to opt out of training because they wanted to keep the history
    feature.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，用户有两种方式可以选择退出数据用于模型训练的使用（[5(b)](https://arxiv.org/html/2309.11653v2#S5.F5.sf2
    "5(b) ‣ 图5 ‣ 5.6.2.暗黑模式阻碍了退出控制的采纳（所有参与者） ‣ 5.6.用户对现有隐私控制的意识、理解和态度 ‣ 5.面试结果 ‣ “这公平吗？”，还是它？考察用户如何在使用基于LLM的对话代理时应对披露风险与收益")).
    其中一种方法更容易发现（除了P15之外的所有人都找到了），但它要求用户同时关闭聊天历史记录保存功能（[5(a)](https://arxiv.org/html/2309.11653v2#S5.F5.sf1
    "图5 ‣ 5.6.2.暗黑模式阻碍了退出控制的采纳（所有参与者） ‣ 5.6.用户对现有隐私控制的意识、理解和态度 ‣ 5.面试结果 ‣ “这公平吗？”，还是它？考察用户如何在使用基于LLM的对话代理时应对披露风险与收益")）。几位参与者表示他们犹豫不决（P11）或不想（P8、P7、P17、P18）选择退出训练，因为他们希望保留历史记录功能。
- en: '*I’m a little annoyed about that…Because I still want to keep my history, I
    need to provide my information for them to train. It’s like something mandatory.*
    (P8)'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我有点生气……因为我还是想保留我的历史记录，但我必须提供我的信息给他们训练。就像是强制性的要求。*（P8）'
- en: 'Alternatively, users can also submit a form to opt out of training and keep
    the history feature ([5(b)](https://arxiv.org/html/2309.11653v2#S5.F5.sf2 "5(b)
    ‣ Figure 5 ‣ 5.6.2\. Dark Patterns Impeded Adoption of the Opt-out Control (All
    participants) ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing
    Privacy Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining
    How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")), but the link to access this form is hidden in a FAQ article⁷⁷7Data
    Controls FAQ: [https://help.openai.com/en/articles/7730893-data-controls-faq](https://help.openai.com/en/articles/7730893-data-controls-faq).
    None of our participants found this option independently. The inconvenience seemed
    to further discourage users from enabling the opt-out feature, as put by P2,'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，用户也可以提交表格来选择退出训练并保留历史功能（[5(b)](https://arxiv.org/html/2309.11653v2#S5.F5.sf2
    "5(b) ‣ 图5 ‣ 5.6.2\. 黑暗模式妨碍了退出控制的采用（所有参与者） ‣ 5.6\. 用户对现有隐私控制的认知、理解和态度 ‣ 5\. 访谈结果
    ‣ ”这就是公平游戏”，还是说不是？探讨用户在使用基于LLM的对话代理时如何应对披露风险和收益")），但访问该表格的链接被隐藏在一个常见问题文章中⁷⁷7数据控制常见问题：[https://help.openai.com/en/articles/7730893-data-controls-faq](https://help.openai.com/en/articles/7730893-data-controls-faq)。我们的参与者中没有人能独立找到这个选项。这种不便似乎进一步让用户不愿启用退出功能，正如P2所说，
- en: '*To be honest, if it’s not easy to find, I feel like the form will be complicated.
    So yeah, I would probably just not (fill it out).*'
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*说实话，如果找不到的话，我觉得表格会很复杂。所以，是的，我可能根本不会填写。*'
- en: 5.6.3\. Users Anticipated More Granular Opt-out Controls (P1, P6, P8, P13, P14,
    P18)
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.6.3\. 用户预期更细粒度的退出控制（P1、P6、P8、P13、P14、P18）
- en: 'Several participants shared that the opt-out control worked at the conversation-level,
    and not the account level whereby some conversations could be specially designated
    as “opt out”: i.e., they expected behavior similar to a browser’s incognito mode.
    This understanding was shared by both people with (P6, P13, P14, P18) and without
    (P1, P8) a technical background. P14 hoped to have this more granular control
    not for privacy but for improving the quality of training data. He often tested
    the capability of ChatGPT with'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 一些参与者分享了退出控制功能在会话层面生效，而非在账户层面，因此某些对话可以特别指定为“退出”：即他们期望的行为类似于浏览器的隐身模式。无论是有技术背景的参与者（P6、P13、P14、P18）还是没有技术背景的参与者（P1、P8）都持相同看法。P14希望能够有更细粒度的控制，不是为了隐私，而是为了提高训练数据的质量。他经常测试ChatGPT的能力。
- en: '*all kinds of wild things*,'
  id: totrans-268
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*各种疯狂的事情*，'
- en: and he said
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 他还说
- en: '*I would think that the appropriate thing to do is that anything communicated
    during an opt-out is not added to the training data…because that would just make
    the AI go insane.*'
  id: totrans-270
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*我认为适当的做法是，在选择退出时，任何交流内容都不应添加到训练数据中……因为那样做会让AI变得疯狂。*'
- en: 6\. Discussion
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 讨论
- en: In this section, we present the user-driven privacy threats synthesized from
    our results, and then establish guidelines for LLM privacy research in both the
    short and long term.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了基于用户驱动的隐私威胁，这些威胁是从我们的研究结果中提炼出来的，然后我们为LLM隐私研究制定了短期和长期的指导方针。
- en: 6.1\. User-Driven Privacy Threat Modeling of LLM-Based Conversational Agents
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 基于用户驱动的LLM对话代理隐私威胁建模
- en: Our results grounded the technical privacy risks of LLM such as the memorization
    risks (Carlini et al., [2021](https://arxiv.org/html/2309.11653v2#bib.bib10),
    [2022](https://arxiv.org/html/2309.11653v2#bib.bib9)) in the contexts of users’
    actual interactions with ChatGPT. We also provided empirical evidence supporting
    potential risks caused by LLMs, as speculated in prior literature (Weidinger et al.,
    [2021](https://arxiv.org/html/2309.11653v2#bib.bib72)). In the following, we summarize
    privacy threats around this new technology that users are concerned about and
    may cause harm to users under specific use cases.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果揭示了大型语言模型（LLM）在用户实际与ChatGPT互动中的技术性隐私风险，如记忆风险（Carlini等，[2021](https://arxiv.org/html/2309.11653v2#bib.bib10)，[2022](https://arxiv.org/html/2309.11653v2#bib.bib9)）。我们还提供了实证证据，支持先前文献中对LLM可能带来风险的推测（Weidinger等，[2021](https://arxiv.org/html/2309.11653v2#bib.bib72)）。接下来，我们总结了用户关注的与这项新技术相关的隐私威胁，以及在特定使用场景下可能对用户造成的危害。
- en: 6.1.1\. User Concerns About Institutional Privacy
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 用户对机构隐私的担忧
- en: 'Much as expected, institutional privacy issues were repeatedly brought up by
    our participants. Most participants are concerned about traditional privacy risks
    that do not solely exist in LLM-based CAs, such as incomplete data deletion and
    having user data reviewed by humans ([Section 5.2.5](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS5.Px1
    "Concerns over data misuse by institutions (P2, P3, P5, P6, P8, P9, P11, P12,
    P13, P14, P17, P18) ‣ 5.2.5\. Perceived Risks and Harms ‣ 5.2\. Factors that Affect
    Users’ Disclosure Intentions with LLM-Based CAs ‣ 5\. Interview Results ‣ ”It’s
    a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks and Benefits
    When Using LLM-Based Conversational Agents")). Fewer participants raised LLM-specific
    concerns such as memorization risks ([Section 5.5.2](https://arxiv.org/html/2309.11653v2#S5.SS5.SSS2
    "5.5.2\. Concerned with Memorization: Intellectual Property Leaks (P2, P17) ‣
    5.5\. Users’ Awareness and Reactions to Memorization Risks in LLM-based CAs ‣
    5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")). It
    is essential to note that users’ trust in ChatGPT has largely been influenced
    by the fact that it currently does not profit from user profiling, advertising,
    and selling data. However, since this technology is still in its nascent phase,
    we are likely to see it adopted in many more fields, accommodated by various business
    models. The use of rich conversational data for targeted advertising or marketing
    isn’t unimaginable, given the lack of clear regulations and the tempting potential,
    and might cause bigger concerns.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们的参与者反复提到了机构隐私问题。大多数参与者关心的是那些不仅仅存在于基于大语言模型（LLM）的对话助手中的传统隐私风险，例如数据删除不完全和用户数据被人类查看的情况（[第5.2.5节](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS5.Px1
    "机构数据滥用的担忧 (P2, P3, P5, P6, P8, P9, P11, P12, P13, P14, P17, P18) ‣ 5.2.5. 感知风险和危害
    ‣ 5.2. 影响用户在LLM对话助手中披露意图的因素 ‣ 5. 访谈结果 ‣ “这是一场公平的游戏”，还是这样吗？探讨用户如何在使用LLM对话助手时平衡披露风险与利益")）。较少有参与者提出特定于LLM的担忧，如记忆风险（[第5.5.2节](https://arxiv.org/html/2309.11653v2#S5.SS5.SSS2
    "5.5.2. 关注记忆：知识产权泄露 (P2, P17) ‣ 5.5. 用户对LLM对话助手中的记忆风险的认知与反应 ‣ 5. 访谈结果 ‣ “这是一场公平的游戏”，还是这样吗？探讨用户如何在使用LLM对话助手时平衡披露风险与利益")）。需要特别注意的是，用户对ChatGPT的信任在很大程度上受到了其目前不从用户画像、广告和销售数据中获利的影响。然而，由于这一技术仍处于初期阶段，我们可能会看到它在更多领域得到应用，并配合各种商业模式。鉴于缺乏明确的监管以及其诱人的潜力，利用丰富的对话数据进行定向广告或市场营销并非不可想象，这可能会引发更大的担忧。
- en: 6.1.2\. User Concerns About Interdependent Privacy
  id: totrans-277
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2. 用户对相互依赖的隐私的担忧
- en: 'We also observed that users use LLM-based CAs to handle tasks that involve
    other people’s data ([Section 3.2.2](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS2
    "3.2.2\. Actors: Relationship Between the Data Subject and the User ‣ 3.2\. Findings
    ‣ 3\. Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")) and
    they have concerns about it ([Section 5.2.6](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS6.Px1
    "Attitude towards disclosing others’ data and having one’s own data shared by
    others (P3, P8, P10, P11, P13, P14, P16) ‣ 5.2.6\. Perceived Norms of Disclosure
    ‣ 5.2\. Factors that Affect Users’ Disclosure Intentions with LLM-Based CAs ‣
    5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")). Specifically,
    the novel use cases enabled by LLM-based CAs (e.g., asking ChatGPT to draft a
    response to colleague’s emails) allow people to share unprecedented types and
    amounts of information about other people compared to similar systems (e.g., search
    engine, traditional CAs). Our results show that there are severe interdependent
    privacy issues in LLM-based CAs, which are even harder to address since most current
    models for privacy management are heavily individualized.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还观察到，用户使用基于LLM的对话代理（CAs）处理涉及他人数据的任务（[第3.2.2节](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS2
    "3.2.2\. Actors: Relationship Between the Data Subject and the User ‣ 3.2\. Findings
    ‣ 3\. Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")），并且他们对此存在担忧（[第5.2.6节](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS6.Px1
    "Attitude towards disclosing others’ data and having one’s own data shared by
    others (P3, P8, P10, P11, P13, P14, P16) ‣ 5.2.6\. Perceived Norms of Disclosure
    ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")）。具体来说，基于LLM的对话代理所启用的新型应用场景（例如，要求ChatGPT草拟对同事电子邮件的回复）使得人们能够分享比类似系统（如搜索引擎、传统对话代理）更多、前所未有的关于他人的信息。我们的研究结果表明，基于LLM的对话代理存在严重的相互依赖的隐私问题，而解决这些问题更加困难，因为当前大多数隐私管理模型过于个性化。'
- en: 6.1.3\. The Model is One-Size-Fits-All, but User Concerns Are Not
  id: totrans-279
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3\. 模型是“一刀切”的，但用户的担忧却不是
- en: A recurring theme of the interview results is that users’ privacy concerns are
    contextualized and subjective ([Section 5.2](https://arxiv.org/html/2309.11653v2#S5.SS2
    "5.2\. Factors that Affect Users’ Disclosure Intentions with LLM-Based CAs ‣ 5\.
    Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")). Users’
    reasons about why or why not sharing certain data with ChatGPT involved all the
    key parameters of the contextual integrity framework (Nissenbaum, [2004](https://arxiv.org/html/2309.11653v2#bib.bib53),
    [2020](https://arxiv.org/html/2309.11653v2#bib.bib54)). Despite varied use cases
    and expectations, only one end-to-end model is serving all the requests in these
    LLM-based systems. This means that the system works in a way that is agnostic
    to the sensitivity of use cases and the varied norms about data collection, sharing,
    and retention under each use case. As a result, the burden of navigating the varying
    privacy risks essentially falls back to users. Our results suggest that people
    take various ad-hoc approaches to desensitize their input ([Section 5.3.3](https://arxiv.org/html/2309.11653v2#S5.SS3.SSS3
    "5.3.3\. Manually Sanitize Inputs (All but P5, P8, P11, P15) ‣ 5.3\. How Users
    Navigate the Trade-off Between Disclosure Risks and Benefits ‣ 5\. Interview Results
    ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks
    and Benefits When Using LLM-Based Conversational Agents")). However, these methods
    can be tedious (e.g., redacting all the PII in an email), and users may not always
    remember to apply them (e.g., one participant forgot he was not permitted to share
    his work-related data with ChatGPT).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 面试结果中的一个反复出现的主题是，用户的隐私担忧是有情境背景的并且具有主观性（[第5.2节](https://arxiv.org/html/2309.11653v2#S5.SS2
    "5.2\. 影响用户披露意图的因素 ‣ 5\. 面试结果 ‣ ”这真的是公平的游戏吗？”，还是不是？检验用户在使用基于大语言模型的对话代理时如何应对披露风险和利益")）。用户关于是否与ChatGPT共享某些数据的理由，涉及到情境完整性框架中的所有关键参数（Nissenbaum，[2004](https://arxiv.org/html/2309.11653v2#bib.bib53)，[2020](https://arxiv.org/html/2309.11653v2#bib.bib54)）。尽管使用案例和期望有所不同，但只有一个端到端的模型在这些基于大语言模型的系统中处理所有请求。这意味着该系统的工作方式对使用案例的敏感性以及关于数据收集、共享和保留的多种规范是无关的。因此，理解和应对不同隐私风险的负担基本上落回到用户身上。我们的结果表明，人们采取了各种临时性的方法来去敏感化他们的输入（[第5.3.3节](https://arxiv.org/html/2309.11653v2#S5.SS3.SSS3
    "5.3.3\. 手动清理输入（除了P5、P8、P11、P15） ‣ 5.3\. 用户如何在披露风险与利益之间权衡 ‣ 5\. 面试结果 ‣ ”这真的是公平的游戏吗？”，还是不是？检验用户在使用基于大语言模型的对话代理时如何应对披露风险和利益")）。然而，这些方法可能很繁琐（例如，编辑电子邮件中的所有个人身份信息），而且用户也不总是记得应用这些方法（例如，一位参与者忘记了他不能与ChatGPT共享与工作相关的数据）。
- en: 6.1.4\. The Impact of Human-Like Interactions on Privacy
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4\. 类人交互对隐私的影响
- en: 'Work in robot-human interaction has shown that people relate to non-human agents
    in social ways, including extending moral judgments to them (Kahn Jr et al., [2012](https://arxiv.org/html/2309.11653v2#bib.bib33)),
    attributing them sociocultural awareness (Simmons et al., [2011](https://arxiv.org/html/2309.11653v2#bib.bib64)),
    and trusting them based on their behavior and anthropomorphism (Natarajan and
    Gombolay, [2020](https://arxiv.org/html/2309.11653v2#bib.bib52)) as well as, most
    relevantly for our work, their use of language (Ye et al., [2023](https://arxiv.org/html/2309.11653v2#bib.bib73)).
    While past work has emphasized the benefits of this trust for collaboration, our
    results suggest that it can also lead to the gradually increasing disclosure of
    sensitive information. Such human-like interactions may also act as a nudge that
    affects what types of information the user shares ([Section 3.2.3](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3
    "3.2.3\. Contexts: A Typology of Disclosure Scenarios ‣ 3.2\. Findings ‣ 3\. Dataset
    Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure
    Risks and Benefits When Using LLM-Based Conversational Agents") and [Section 5.2.1](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS1
    "5.2.1\. Perceived Capability of the CAs (All participants) ‣ 5.2\. Factors that
    Affect Users’ Disclosure Intentions with LLM-Based CAs ‣ 5\. Interview Results
    ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks
    and Benefits When Using LLM-Based Conversational Agents")). Future work needs
    to distinguish between increased trust as a benefit for collaboration and the
    dangers of unwarranted trust from insufficient transparency and the leveraging
    of human social cognition leading to greater privacy harms.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器人与人类互动的研究中，已经显示出人们以社交方式与非人类代理进行互动，包括对它们进行道德判断（Kahn Jr 等，[2012](https://arxiv.org/html/2309.11653v2#bib.bib33)），赋予它们社会文化意识（Simmons
    等，[2011](https://arxiv.org/html/2309.11653v2#bib.bib64)），并根据它们的行为和拟人化特征信任它们（Natarajan
    和 Gombolay，[2020](https://arxiv.org/html/2309.11653v2#bib.bib52)），最相关的，对于我们的研究而言，信任也依赖于它们使用语言的能力（Ye
    等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib73)）。虽然过去的研究强调了这种信任对合作的好处，但我们的结果表明，这也可能导致逐渐披露敏感信息。这种类人互动也可能作为一种提醒，影响用户分享何种类型的信息（[第3.2.3节](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3
    "3.2.3\. 情境：披露情境的类型学 ‣ 3.2\. 发现 ‣ 3\. 数据集分析 ‣ ”这是公平的游戏“，还是不是？审视用户在使用基于LLM的对话代理时如何处理披露风险与收益")
    和 [第5.2.1节](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS1 "5.2.1\. 感知到的对话代理能力（所有参与者）
    ‣ 5.2\. 影响用户披露意图的因素 ‣ 5\. 访谈结果 ‣ ”这是公平的游戏“，还是不是？审视用户在使用基于LLM的对话代理时如何处理披露风险与收益")）。未来的研究需要区分信任的增加作为合作的好处与由于透明度不足和利用人类社会认知所带来的不必要信任之间的危险，这可能会导致更大的隐私危害。
- en: 6.1.5\. The Emerging Fear of Being Found Out Using AI
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.5\. 使用AI时出现的被发现的恐惧
- en: The lack of established norms regarding when it is appropriate or not to use
    AI has raised concerns among users of ChatGPT. Some use cases, such as using AI
    to generate book chapters, are indeed inappropriate ([Section 3.2.4](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS4
    "3.2.4\. Users’ Concerns and Protective Behaviors in the Conversations ‣ 3.2\.
    Findings ‣ 3\. Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). Others are more benign but may lead to people questioning the user’s
    ability because they used AI to assist with their work. (e.g., a non-native English
    speaker using AI for polishing writing, [Section 5.2.5](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS5.Px2
    "Concerns about others finding out (P8, P17, P18) ‣ 5.2.5\. Perceived Risks and
    Harms ‣ 5.2\. Factors that Affect Users’ Disclosure Intentions with LLM-Based
    CAs ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")).
    It remains to be discussed how to balance individual privacy protection and the
    societal impact in these situations.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 关于何时使用 AI 合适，何时不合适，缺乏已建立的规范，这引发了 ChatGPT 用户的担忧。一些使用场景，比如使用 AI 生成书籍章节，的确不合适（[第
    3.2.4 节](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS4 "3.2.4\. 用户在对话中的担忧和保护行为
    ‣ 3.2\. 发现 ‣ 3\. 数据集分析 ‣ ”这是一场公平的游戏”，还是不是？检视用户如何应对使用基于 LLM 的对话代理时的披露风险和利益")）。另一些则比较温和，但可能会让人质疑用户的能力，因为他们在工作中使用了
    AI 辅助（例如，一位非母语英语使用者使用 AI 来润色写作，[第 5.2.5 节](https://arxiv.org/html/2309.11653v2#S5.SS2.SSS5.Px2
    "担心别人发现（P8、P17、P18） ‣ 5.2.5\. 感知的风险和危害 ‣ 5\. 影响用户披露意图的因素，基于 LLM 的对话代理 ‣ 5\. 访谈结果
    ‣ ”这是一场公平的游戏”，还是不是？检视用户如何应对使用基于 LLM 的对话代理时的披露风险和利益"))。在这些情况下，如何平衡个人隐私保护与社会影响仍需进一步讨论。
- en: 6.2\. Design Privacy-Friendly LLM-Based Conversational Agents and Other LLM-Based
    Applications
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 设计隐私友好的基于 LLM 的对话代理和其他基于 LLM 的应用
- en: 'Our studies suggest that the developers of LLM-based conversational agents
    or other applications should make more efforts to ensure the system is designed
    in the best interest of the users, and provide sufficient support for users to
    navigate the risks and benefits under different contexts (see our typology of
    disclosure behaviors [Section 3.2.3](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3
    "3.2.3\. Contexts: A Typology of Disclosure Scenarios ‣ 3.2\. Findings ‣ 3\. Dataset
    Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure
    Risks and Benefits When Using LLM-Based Conversational Agents")). We propose potential
    improvement directions below.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，基于 LLM 的对话代理或其他应用的开发者应更加努力，确保系统设计符合用户的最佳利益，并为用户在不同情境下应对风险和利益提供足够支持（请参见我们的披露行为类型学
    [第 3.2.3 节](https://arxiv.org/html/2309.11653v2#S3.SS2.SSS3 "3.2.3\. 情境：披露情境的类型学
    ‣ 3.2\. 发现 ‣ 3\. 数据集分析 ‣ ”这是一场公平的游戏”，还是不是？检视用户如何应对使用基于 LLM 的对话代理时的披露风险和利益")）。我们在下文提出了潜在的改进方向。
- en: 6.2.1\. Consider User Mental Models for Designing Privacy Support
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 考虑用户心理模型来设计隐私支持
- en: The types of mental models identified in our studies suggest that systems based
    on LLM often do not function as users anticipate. This suggests that system design
    should consider this mismatch, adopting an intuitive design that matches users’
    expectations or proactively communicating the privacy risks that users may not
    foresee. Users’ mental models might also be affected by the frontend and interaction
    design. For example, P4 felt GitHub Copilot was safer than ChatGPT because
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究中识别出的心理模型类型表明，基于 LLM 的系统往往不能按用户预期的方式工作。这表明，系统设计应该考虑到这种不匹配，采用符合用户期望的直观设计，或者主动传达用户可能未预见的隐私风险。用户的心理模型也可能受到前端和交互设计的影响。例如，P4
    认为 GitHub Copilot 比 ChatGPT 更安全，因为
- en: '*that’s offline*.'
  id: totrans-289
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*那是离线的*。'
- en: However, Copilot is supported by the OpenAI Codex model, which is also hosted
    on the cloud. It suggests that when LLMs are more deeply integrated into a system
    that feels private (e.g., the IDE), it may be difficult for users to understand
    that their data will be sent off the device. Furthermore, it might become harder
    for users to understand which part of their data remains on the device and which
    part gets sent out if the system employs a hybrid design combining local and remote
    models.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Copilot由OpenAI Codex模型支持，Codex模型同样托管在云端。这表明，当大型语言模型（LLMs）更深层次地集成到一个看似私密的系统中（例如IDE）时，用户可能难以理解其数据会被发送到设备外部。此外，如果该系统采用结合本地模型和远程模型的混合设计，用户可能更难理解哪些数据会保留在设备上，哪些会被发送出去。
- en: 6.2.2\. Assist Users in Taking Privacy-Protective Measures
  id: totrans-291
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 帮助用户采取隐私保护措施
- en: Many users are taking certain privacy-protective measures, such as omitting
    or obfuscating sensitive information, telling privacy lies, and segmenting input,
    while it could become tiring to manually desensitize all the input, and users
    may forget to do it. Therefore, there is an opportunity for designing privacy-preserving
    techniques that assist users in applying these measures in an automated or semi-automated
    manner. For example, a smaller model dedicated to detecting PII and other more
    complex disclosures may be distilled from a large model fine-tuned for the task,
    and can be run locally to either remind users of the sensitive information included
    in their input or automatically rewrite it before sending it out. Overall, there
    needs to be more research in designing and evaluating techniques that help users
    achieve utility while maintaining their privacy.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 许多用户正在采取某些隐私保护措施，如省略或模糊敏感信息、撒谎隐瞒隐私信息以及分段输入，然而手动去除所有输入中的敏感信息可能会变得繁琐，用户也可能会忘记进行这些操作。因此，有必要设计隐私保护技术，帮助用户以自动化或半自动化的方式应用这些措施。例如，可以从针对该任务进行微调的大型模型中提取出一个专门用于检测个人身份信息（PII）和其他更复杂泄露信息的小型模型，并将其本地运行，提醒用户其输入中包含的敏感信息，或在发送之前自动重写这些信息。总体而言，仍需更多的研究来设计和评估那些帮助用户在保持隐私的同时获得效用的技术。
- en: 6.2.3\. Be Cautious About Using User Data for Training Models
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 小心使用用户数据进行模型训练
- en: The lack of understanding of the model training process and awareness of memorization
    risks suggest that LLM-based systems should exercise caution when using user data
    for model training. If such data is used, it is crucial to communicate the associated
    risks to users effectively. It is also important to provide users with convenient
    and granular control, enabling them to easily opt-in or out according to their
    best interests ([Section 5.6.3](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS3
    "5.6.3\. Users Anticipated More Granular Opt-out Controls (P1, P6, P8, P13, P14,
    P18) ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing Privacy
    Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). We want to highlight that many participants had positive attitudes
    towards contributing their data for improving the system, and that increased transparency
    could also lead to improved data quality.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对模型训练过程缺乏理解以及对记忆风险的意识，表明基于大型语言模型（LLM）的系统在使用用户数据进行模型训练时应谨慎。如果确实使用了这些数据，至关重要的是有效地向用户传达相关风险。同时，还需要为用户提供便捷且细化的控制方式，使其能够根据自身利益轻松选择是否参与（[第5.6.3节](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS3
    "5.6.3\. 用户期望更细化的退出控制（P1、P6、P8、P13、P14、P18） ‣ 5.6\. 用户对现有隐私控制的认知、理解和态度 ‣ 5\. 访谈结果
    ‣ ”公平竞争”，还是这样？研究用户在使用基于LLM的对话代理时如何处理披露风险与收益")）。我们希望强调的是，许多参与者对提供数据以改善系统持积极态度，而提高透明度也可能导致数据质量的改善。
- en: 6.2.4\. Using Local LLMs for Building Apps in Specific Domains
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4\. 在特定领域使用本地LLMs构建应用程序
- en: Lastly, we want to note that most participants with a technical background believed
    that only an offline model could completely eliminate their concerns. Although
    there is still a significant gap in the performance between smaller, open-source
    models that can run on devices and the gigantic, proprietary models that run on
    the cloud, there may be certain use cases where a smaller model may suffice. The
    developers of LLM-based systems should keep in mind the option of using a local
    model whenever it is possible.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想指出，大多数具有技术背景的参与者认为，只有离线模型才能完全消除他们的顾虑。尽管能够在设备上运行的小型开源模型和在云端运行的大型专有模型之间仍存在显著的性能差距，但在某些使用场景中，小型模型可能就足够了。基于LLM的系统开发者应始终记住，在可能的情况下使用本地模型这一选项。
- en: 6.3\. “It’s a Fair Game”, or Is It? Considering the Impact of Erroneous Mental
    Models, Transparency, Trust, and the Nature of Current LLMs on Prospects for Privacy
    in LLMs
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. “这是公平的游戏”，还是不是？考虑错误的心理模型、透明度、信任以及当前LLM对隐私前景的影响
- en: Our results reveal challenging problems that may not have a clear direction
    for resolution, given the privacy models in current technology, law, and social
    literature.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果揭示了当前技术、法律和社会文献中的隐私模型所面临的挑战性问题，这些问题可能没有明确的解决方向。
- en: 6.3.1\. Current LLMs are Inherently Surveillant
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1. 当前的LLM本质上是监视性系统
- en: Today’s LLMs are only as capable as they are due to the sheer scale of data
    used in training them. This, we argue, marries them structurally to the modern
    internet’s surveillance-as-default mode of operation, with its imperative to always
    collect more data to train models with the goal of monetizing behavioral surplus (Zuboff,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib82)). Therefore, we caution
    against the expectation that guidelines encouraging privacy-preserving design
    (such as those we provide above) will be meaningfully adopted by the makers of
    LLMs, given the demonstrated tendency of companies operating under these imperatives
    to reduce privacy to mere compliance and public relations (Waldman, [2021](https://arxiv.org/html/2309.11653v2#bib.bib70)).
    After all, if everyone were to opt out of data collection, and if LLMs were to
    be meaningfully cautious in how they used public data for training, models of
    modern levels of sophistication could not exist. If models are to continue to
    be trained, some paradigmatic shifts will be needed to understand how such scales
    of data can be collected and used ethically.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当今的LLM（大规模语言模型）之所以如此强大，是因为它们在训练过程中使用了庞大的数据规模。我们认为，这使得它们在结构上与现代互联网的“监视默认模式”密切相关，后者的核心要求是不断收集更多数据，以训练模型，并通过盈利行为剩余（Zuboff，[2023](https://arxiv.org/html/2309.11653v2#bib.bib82)）来实现商业化。因此，我们提醒不要指望鼓励隐私保护设计的指南（如我们上述提供的）会被LLM的开发者切实采纳，鉴于在这些企业在此类要求下的表现倾向是将隐私仅仅视为合规和公关的工具（Waldman，[2021](https://arxiv.org/html/2309.11653v2#bib.bib70)）。毕竟，如果每个人都选择退出数据收集，且如果LLM在如何使用公开数据进行训练方面更加谨慎，现代复杂度水平的模型将无法存在。如果模型继续训练，则需要进行一些范式转变，以理解如何伦理地收集和使用如此规模的数据。
- en: 6.3.2\. Transparency and Mental Models
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2. 透明度与心理模型
- en: In laying out a human-centered research roadmap for transparency in LLMs, Liao
    and Vaughan ([2023](https://arxiv.org/html/2309.11653v2#bib.bib45)) define transparency
    as “enabling relevant stakeholders to form an appropriate understanding of a model
    or system’s capabilities, limitations, how it works, and how to use or control
    its outputs”. Our results showed that participants exhibited serious, privacy-relevant
    misconceptions about core concepts including how LLMs generate responses and how
    user data is used for training models, suggesting the goal is still far away from
    being achieved. Our findings around participants’ mental models of LLMs suggest
    that transparency isn’t in good shape for end users. The task of educating the
    mass about how LLM-based systems function remains a significant challenge. However,
    it is a prerequisite for the ethical deployment of such systems in society.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在为LLM透明度制定以人为本的研究路线图时，Liao和Vaughan（[2023](https://arxiv.org/html/2309.11653v2#bib.bib45)）将透明度定义为“使相关利益相关者能够形成对模型或系统的能力、局限性、运作方式以及如何使用或控制其输出的适当理解”。我们的结果显示，参与者在核心概念上存在严重的隐私相关误解，包括LLM如何生成回应，以及用户数据如何用于训练模型，这表明这一目标仍然遥不可及。我们关于参与者对LLM心理模型的发现表明，透明度对于最终用户来说状况堪忧。教育大众了解基于LLM的系统如何运作依然是一项巨大的挑战。然而，这是在社会中伦理地部署此类系统的前提条件。
- en: 6.3.3\. Trust and Control
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3. 信任与控制
- en: Extensive research on privacy law and the privacy paradox has demonstrated that
    not only cognitive biases (i.e., trust and dark patterns) but also the sheer scale,
    lack of transparency, and binary nature of privacy choices online leads people
    to quite rationally give up on attempting to engage in privacy self-management (Solove,
    [2020](https://arxiv.org/html/2309.11653v2#bib.bib65); Hargittai and Marwick,
    [2016](https://arxiv.org/html/2309.11653v2#bib.bib28)). The uniquely social way
    in which people relate to CAs, our results suggest, may only make an individually
    choice-driven approach to privacy even more intractable. Hence, we argue for caution
    in believing that incremental improvements to privacy controls will have broad
    effectiveness for LLM privacy.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 关于隐私法和隐私悖论的广泛研究表明，不仅是认知偏差（即信任和暗黑设计），还有隐私选择的规模庞大、缺乏透明度和二元性质，导致人们理性地放弃尝试进行隐私自我管理（Solove，[2020](https://arxiv.org/html/2309.11653v2#bib.bib65)；Hargittai和Marwick，[2016](https://arxiv.org/html/2309.11653v2#bib.bib28)）。我们结果表明，人们与对话代理的独特社交关系可能使得个人驱动的隐私选择方式变得更加棘手。因此，我们认为应对隐私控制的渐进式改进是否能广泛有效地保护LLM隐私保持谨慎态度。
- en: The other side of this coin, of course, is that neither participants’ weak enforcement
    of privacy boundaries nor their initial assertions about a lack of privacy concerns
    should be interpreted to suggest that they do not value privacy generally or with
    LLM-based CAs in particular. Our interview studies have revealed patterns in users’
    behaviors that may be explained as a “Reverse Privacy Paradox” (Colnago et al.,
    [2023](https://arxiv.org/html/2309.11653v2#bib.bib12)), in which individuals who
    seem to disregard privacy concerns still engage in privacy-seeking behaviors ([Section 5.3.3](https://arxiv.org/html/2309.11653v2#S5.SS3.SSS3
    "5.3.3\. Manually Sanitize Inputs (All but P5, P8, P11, P15) ‣ 5.3\. How Users
    Navigate the Trade-off Between Disclosure Risks and Benefits ‣ 5\. Interview Results
    ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate Disclosure Risks
    and Benefits When Using LLM-Based Conversational Agents") and [Section 5.6.1](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS1
    "5.6.1\. Users Lack Awareness of Existing Privacy Controls (All but P11, P14,
    P16, P17, P18) ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing
    Privacy Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining
    How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")). However, these privacy-protective behaviors are not well supported
    by the system, or even obstructed by it due to the dark patterns ([Section 5.6.2](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS2
    "5.6.2\. Dark Patterns Impeded Adoption of the Opt-out Control (All participants)
    ‣ 5.6\. Users’ Awareness, Understanding, and Attitudes About Existing Privacy
    Controls ‣ 5\. Interview Results ‣ ”It’s a Fair Game”, or Is It? Examining How
    Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")), likely due to the imperative to collect data for training models discussed
    above.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这枚硬币的另一面是，参与者对隐私边界的弱执行以及他们最初对隐私问题的否定陈述不应被解读为他们通常不重视隐私，或特别是对基于LLM的对话代理不重视隐私。我们的访谈研究揭示了用户行为中的一些模式，这些模式可以解释为一种“反向隐私悖论”（Colnago等，[2023](https://arxiv.org/html/2309.11653v2#bib.bib12)），在这种悖论中，似乎忽视隐私问题的个人仍然会参与寻求隐私的行为（[第5.3.3节](https://arxiv.org/html/2309.11653v2#S5.SS3.SSS3
    "5.3.3\. 手动清理输入（除P5、P8、P11、P15外） ‣ 5.3\. 用户如何平衡披露风险与收益 ‣ 5\. 访谈结果 ‣ ”这是公平的游戏”，还是不是？检视用户如何在使用基于LLM的对话代理时平衡披露风险与收益")
    和 [第5.6.1节](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS1 "5.6.1\. 用户对现有隐私控制缺乏意识（除P11、P14、P16、P17、P18外）
    ‣ 5.6\. 用户对现有隐私控制的意识、理解与态度 ‣ 5\. 访谈结果 ‣ ”这是公平的游戏”，还是不是？检视用户如何在使用基于LLM的对话代理时平衡披露风险与收益")）。然而，这些隐私保护行为未得到系统的充分支持，甚至由于暗黑设计（[第5.6.2节](https://arxiv.org/html/2309.11653v2#S5.SS6.SSS2
    "5.6.2\. 暗黑设计阻碍了退出控制的采用（所有参与者） ‣ 5.6\. 用户对现有隐私控制的意识、理解与态度 ‣ 5\. 访谈结果 ‣ ”这是公平的游戏”，还是不是？检视用户如何在使用基于LLM的对话代理时平衡披露风险与收益")）而受到阻碍，这可能是由于前述为训练模型收集数据的紧迫需求所致。
- en: 6.4\. Limitations and Future Work
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 局限性与未来工作
- en: In this work, we take the first step towards modeling end-user disclosure behaviors,
    mental models, and the privacy concerns thereof with LLM-based conversational
    agents. There are still some important questions that cannot be fully answered
    in this work, and we would like to leave them for future research. First, the
    study primarily focused on ChatGPT due to its broad user base and data availability.
    This focus may limit our understanding of privacy concerns on other LLM-based
    platforms with different interaction styles or user demographics. Second, the
    interview targeted the general population. However, we speculate that the opportunity
    cost of not using ChatGPT might be higher for certain populations, such as older
    adults, non-native speakers, and therapy seekers. And more research is needed
    to understand the privacy issues for specific vulnerable populations. Third, in
    this study, our qualitative findings suggest multiple factors that may affect
    users’ risk perceptions and privacy-seeking behaviors related to LLM-based CAs,
    such as contexts, folk models, and user awareness of privacy controls. However,
    more research is needed to quantitatively model the effect of these factors to
    guide future system design. Lastly, as LLM is a new technology and users’ mental
    models are still evolving, it is important to conduct longitudinal studies that
    measure how public attitudes towards privacy issues in LLM-based systems change
    over time.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究迈出了建模最终用户披露行为、心理模型及其隐私问题的第一步。仍有一些重要问题在本研究中无法得到充分解答，我们希望将其留待未来的研究中。首先，本研究主要集中在ChatGPT上，因为它拥有广泛的用户基础和数据可用性。这一聚焦可能会限制我们对其他具有不同交互方式或用户群体的LLM平台上隐私问题的理解。其次，访谈对象为普通人群。然而，我们推测，对于某些群体（如老年人、非母语使用者和寻求治疗的人群），不使用ChatGPT的机会成本可能更高。因此，需要更多的研究来了解特定弱势群体的隐私问题。第三，在本研究中，我们的定性发现揭示了多个可能影响用户对LLM基础CA的风险感知和隐私寻求行为的因素，如上下文、民间模型和用户对隐私控制的认知。然而，需要更多的研究来定量建模这些因素的影响，以指导未来的系统设计。最后，鉴于LLM是新兴技术，用户的心理模型仍在发展，因此，进行纵向研究以衡量公众对LLM系统隐私问题的态度随时间变化非常重要。
- en: 7\. Conclusion
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 结论
- en: In this work, we studied and distilled LLM privacy concerns from the perspective
    of end-users. This knowledge is crucial to advance ongoing debates around AI regulation,
    as well for HCI researchers seeking to design privacy controls that help end-users
    address these concerns. We first qualitatively analyzed the ShareGPT52K dataset,
    uncovering various sensitive disclosure behaviors of ChatGPT users in their use
    of the CA. Then we conducted semi-structured interviews with 19 users of LLM-based
    CAs (e.g., ChatGPT) to inquire users about their disclosure behaviors contextualized
    in their real-world chat logs with LLM-based CAs. Our results suggest that ChatGPT
    users want to protect their privacy when possible, while contending with the perceived
    trade-offs of both convenience and utility. We found that the one-size-fits-all
    nature of the model underlying LLM-based CAs largely places the responsibility
    of protecting privacy on the users. This is challenging for users due to flawed
    mental models and and the dark patterns in ChatGPT’s opt-out interface. Our user-centered
    investigation has revealed a host of novel problems that require attention from
    the HCI and LLM research communities. We also highlight complex issues and structural
    problems that require paradigmatic shifts in technology, law, and society.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究从最终用户的角度研究并提炼了大型语言模型（LLM）的隐私问题。这些知识对于推动围绕人工智能监管的持续讨论至关重要，同时对于HCI研究人员在设计隐私控制措施以帮助最终用户应对这些问题时也具有重要意义。我们首先对ShareGPT52K数据集进行了定性分析，揭示了ChatGPT用户在使用CA时的一些敏感信息披露行为。随后，我们对19名基于LLM的CA（如ChatGPT）用户进行了半结构化访谈，询问他们在与LLM基础的CA进行实际对话时的披露行为。我们的结果表明，ChatGPT用户在可能的情况下希望保护自己的隐私，但也面临着便利性和实用性的权衡。我们发现，LLM基础CA背后的“一刀切”模型在很大程度上将保护隐私的责任放在了用户身上。由于用户的心理模型存在缺陷，以及ChatGPT退出界面的“黑暗模式”，这对用户来说是一个挑战。我们的以用户为中心的调查揭示了一系列新问题，这些问题需要HCI和LLM研究界的关注。我们还强调了需要在技术、法律和社会方面进行范式转变的复杂问题和结构性问题。
- en: Acknowledgements.
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This project is in part supported by a CMU CyLab Seed Funding.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目部分得到了CMU CyLab种子资金的支持。
- en: References
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Anderson et al. (2020) Andrew Anderson, Jonathan Dodge, Amrita Sadarangani,
    Zoe Juozapaitis, Evan Newman, Jed Irvine, Souti Chattopadhyay, Matthew Olson,
    Alan Fern, and Margaret Burnett. 2020. Mental models of mere mortals with explanations
    of reinforcement learning. *ACM Transactions on Interactive Intelligent Systems
    (TiiS)* 10, 2 (2020), 1–37.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson等人（2020）Andrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis,
    Evan Newman, Jed Irvine, Souti Chattopadhyay, Matthew Olson, Alan Fern, 和 Margaret
    Burnett. 2020. 对强化学习的解释下普通人的心理模型. *ACM互动智能系统期刊（TiiS）* 10, 2 (2020), 1–37.
- en: 'Andrews et al. (2023) Robert W Andrews, J Mason Lilly, Divya Srivastava, and
    Karen M Feigh. 2023. The role of shared mental models in human-AI teams: a theoretical
    review. *Theoretical Issues in Ergonomics Science* 24, 2 (2023), 129–175.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrews等人（2023）Robert W Andrews, J Mason Lilly, Divya Srivastava, 和 Karen M
    Feigh. 2023. 共享心理模型在人类与AI团队中的作用：一项理论性回顾。*人体工程学科学理论问题期刊* 24, 2 (2023), 129–175.
- en: Anell et al. (2020) Simon Anell, Lea Gröber, and Katharina Krombholz. 2020.
    End user and expert perceptions of threats and potential countermeasures. In *2020
    IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)*. IEEE, 230–239.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anell等人（2020）Simon Anell, Lea Gröber, 和 Katharina Krombholz. 2020. 最终用户与专家对威胁及潜在对策的看法.
    见于*2020 IEEE欧洲安全与隐私研讨会（EuroS&PW）*。IEEE, 230–239.
- en: 'Bansal et al. (2019) Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki,
    Daniel S Weld, and Eric Horvitz. 2019. Beyond accuracy: The role of mental models
    in human-AI team performance. In *Proceedings of the AAAI conference on human
    computation and crowdsourcing*, Vol. 7\. 2–11.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bansal等人（2019）Gagan Bansal, Besmira Nushi, Ece Kamar, Walter S Lasecki, Daniel
    S Weld, 和 Eric Horvitz. 2019. 超越准确性：心理模型在人类与AI团队表现中的作用. 见于*AAAI人类计算与众包会议论文集*，第7卷。2–11.
- en: Beyer and Holtzblatt (1999) Hugh Beyer and Karen Holtzblatt. 1999. Contextual
    design. *interactions* 6, 1 (1999), 32–42.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beyer 和 Holtzblatt（1999）Hugh Beyer 和 Karen Holtzblatt. 1999. 语境设计. *interactions*
    6, 1 (1999), 32–42.
- en: Bieringer et al. (2022) Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista
    Biggio, and Katharina Krombholz. 2022. Industrial practitioners’ mental models
    of adversarial machine learning. In *Eighteenth Symposium on Usable Privacy and
    Security (SOUPS 2022)*. 97–116.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bieringer等人（2022）Lukas Bieringer, Kathrin Grosse, Michael Backes, Battista Biggio,
    和 Katharina Krombholz. 2022. 工业从业者对对抗性机器学习的心理模型. 见于*第十八届可用隐私与安全研讨会（SOUPS 2022）*。97–116.
- en: Brown et al. (2022) Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah,
    Reza Shokri, and Florian Tramèr. 2022. What does it mean for a language model
    to preserve privacy?. In *Proceedings of the 2022 ACM Conference on Fairness,
    Accountability, and Transparency*. 2280–2292.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2022）Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri,
    和 Florian Tramèr. 2022. 语言模型保持隐私意味着什么？见于*2022年ACM公平性、问责制和透明度会议论文集*。2280–2292.
- en: Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization
    across neural language models. *arXiv preprint arXiv:2202.07646* (2022).
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini等人（2022）Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine
    Lee, Florian Tramer, 和 Chiyuan Zhang. 2022. 量化神经语言模型的记忆化。*arXiv预印本arXiv:2202.07646*（2022年）。
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn
    Song, Ulfar Erlingsson, et al. 2021. Extracting Training Data from Large Language
    Models.. In *USENIX Security Symposium*, Vol. 6.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini等人（2021）Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski,
    Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B Brown, Dawn Song, Ulfar
    Erlingsson, 等人. 2021. 从大型语言模型中提取训练数据.. 见于*USENIX安全研讨会*，第6卷。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
    quality. *See https://vicuna. lmsys. org (accessed 14 April 2023)* (2023).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang等人（2023）Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, 等人.
    2023. Vicuna: 一个开源聊天机器人，凭借90%*chatgpt质量让gpt-4印象深刻。*参见 https://vicuna.lmsys.org（访问时间：2023年4月14日）*（2023年）。'
- en: Colnago et al. (2023) Jessica Colnago, Lorrie Faith Cranor, and Alessandro Acquisti.
    2023. Is there a reverse privacy paradox? an exploratory analysis of gaps between
    privacy perspectives and privacy-seeking behaviors. *Proceedings on Privacy Enhancing
    Technologies* 1 (2023), 455–476.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colnago等人（2023）Jessica Colnago, Lorrie Faith Cranor, 和 Alessandro Acquisti.
    2023. 是否存在反向隐私悖论？对隐私视角与隐私寻求行为之间差距的探索性分析。*隐私增强技术论文集* 1 (2023), 455–476.
- en: 'Cropanzano and Mitchell (2005) Russell Cropanzano and Marie S Mitchell. 2005.
    Social exchange theory: An interdisciplinary review. *Journal of management* 31,
    6 (2005), 874–900.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cropanzano and Mitchell (2005) Russell Cropanzano 和 Marie S Mitchell. 2005.
    社会交换理论：一项跨学科的综述。*管理学杂志* 31, 6（2005），874–900。
- en: Dupuy et al. (2022) Christophe Dupuy, Radhika Arava, Rahul Gupta, and Anna Rumshisky.
    2022. An efficient dp-sgd mechanism for large scale nlu models. In *ICASSP 2022-2022
    IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.
    IEEE, 4118–4122.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dupuy et al. (2022) Christophe Dupuy, Radhika Arava, Rahul Gupta, 和 Anna Rumshisky.
    2022. 一种用于大规模NLU模型的高效dp-sgd机制。在*ICASSP 2022-2022 IEEE国际声学、语音与信号处理会议（ICASSP）*。IEEE，4118–4122。
- en: 'Dwyer et al. (2007) Catherine Dwyer, Starr Hiltz, and Katia Passerini. 2007.
    Trust and privacy concern within social networking sites: A comparison of Facebook
    and MySpace. *AMCIS 2007 proceedings* (2007), 339.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dwyer et al. (2007) Catherine Dwyer, Starr Hiltz, 和 Katia Passerini. 2007. 社交网络网站中的信任与隐私关注：Facebook与MySpace的比较。*AMCIS
    2007会议记录*（2007），339。
- en: El Haddad et al. (2018) Ghada El Haddad, Esma Aimeur, and Hicham Hage. 2018.
    Understanding trust, privacy and financial fears in online payment. In *2018 17th
    IEEE International Conference On Trust, Security And Privacy In Computing And
    Communications/12th IEEE International Conference On Big Data Science And Engineering
    (TrustCom/BigDataSE)*. IEEE, 28–36.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: El Haddad et al. (2018) Ghada El Haddad, Esma Aimeur, 和 Hicham Hage. 2018. 理解在线支付中的信任、隐私与财务恐惧。在*2018年第17届IEEE国际计算与通信中的信任、安全与隐私会议/第12届IEEE大数据科学与工程国际会议（TrustCom/BigDataSE）*。IEEE，28–36。
- en: 'Estrada (2023) Sheryl Estrada. 2023. A startup CFO used ChatGPT to build an
    FP&A tool—here’s how it went. [https://fortune.com/2023/03/01/startup-cfo-chatgpt-finance-tool/](https://fortune.com/2023/03/01/startup-cfo-chatgpt-finance-tool/)
    Accessed: 09/11/2023.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Estrada (2023) Sheryl Estrada. 2023. 一位初创公司CFO使用ChatGPT构建FP&A工具——过程是这样的。[https://fortune.com/2023/03/01/startup-cfo-chatgpt-finance-tool/](https://fortune.com/2023/03/01/startup-cfo-chatgpt-finance-tool/)
    访问时间：2023年9月11日。
- en: 'Ferreira (2023) Pedro Ferreira. 2023. Can ChatGPT Improve Technical Analysis
    and Trading Techniques? [https://www.financemagnates.com/trending/can-chatgpt-improve-technical-analysis-and-trading-techniques/](https://www.financemagnates.com/trending/can-chatgpt-improve-technical-analysis-and-trading-techniques/)
    Accessed: 09/11/2023.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferreira (2023) Pedro Ferreira. 2023. ChatGPT能否改善技术分析与交易技巧？[https://www.financemagnates.com/trending/can-chatgpt-improve-technical-analysis-and-trading-techniques/](https://www.financemagnates.com/trending/can-chatgpt-improve-technical-analysis-and-trading-techniques/)
    访问时间：2023年9月11日。
- en: 'Fox (2023) Andrea Fox. 2023. ChatGPT scored 72study shows. [https://www.healthcareitnews.com/news/chatgpt-scored-72-clinical-decision-accuracy-mgb-study-shows](https://www.healthcareitnews.com/news/chatgpt-scored-72-clinical-decision-accuracy-mgb-study-shows)
    Accessed: 09/11/2023.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fox (2023) Andrea Fox. 2023. ChatGPT获得72分的临床决策准确度——研究显示。[https://www.healthcareitnews.com/news/chatgpt-scored-72-clinical-decision-accuracy-mgb-study-shows](https://www.healthcareitnews.com/news/chatgpt-scored-72-clinical-decision-accuracy-mgb-study-shows)
    访问时间：2023年9月11日。
- en: 'Gallagher et al. (2017) Kevin Gallagher, Sameer Patil, and Nasir Memon. 2017.
    New Me: Understanding Expert and $\{$Non-Expert$\}$ Perceptions and Usage of the
    Tor Anonymity Network. In *Thirteenth Symposium on Usable Privacy and Security
    (SOUPS 2017)*. 385–398.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallagher et al. (2017) Kevin Gallagher, Sameer Patil, 和 Nasir Memon. 2017.
    新的我：理解专家与$\{$非专家$\}$对Tor匿名网络的看法与使用。在*第十三届可用隐私与安全研讨会（SOUPS 2017）*。385–398。
- en: 'Geng et al. (2023) Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter
    Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A dialogue model for academic
    research. *Blog post, April* 1 (2023).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng et al. (2023) Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter
    Abbeel, Sergey Levine, 和 Dawn Song. 2023. Koala：一个用于学术研究的对话模型。*博客文章，2023年4月1日*。
- en: 'Germain (2023) Thomas Germain. 2023. A Mental Health App Tested ChatGPT on
    Its Users. The Founder Said Backlash Was Just a Misunderstanding. [https://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534/](https://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534/)
    Accessed: 09/11/2023.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Germain (2023) Thomas Germain. 2023. 一款心理健康应用测试了ChatGPT对用户的影响。创始人表示反响只是一个误解。[https://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534/](https://gizmodo.com/mental-health-therapy-app-ai-koko-chatgpt-rob-morris-1849965534/)
    访问时间：2023年9月11日。
- en: 'Gibbs et al. (2011) Jennifer L Gibbs, Nicole B Ellison, and Chih-Hui Lai. 2011.
    First comes love, then comes Google: An investigation of uncertainty reduction
    strategies and self-disclosure in online dating. *Communication Research* 38,
    1 (2011), 70–100.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gibbs et al. (2011) Jennifer L Gibbs, Nicole B Ellison, 和 Chih-Hui Lai. 2011.
    爱情先来，谷歌再来：在线约会中不确定性减少策略与自我揭示的研究。*传播研究* 38, 1（2011），70–100。
- en: Grimm (2010) Pamela Grimm. 2010. Social desirability bias. *Wiley international
    encyclopedia of marketing* (2010).
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grimm（2010）Pamela Grimm。2010年。社会期望偏差。*Wiley国际市场营销百科全书*（2010年）。
- en: Grodzinsky and Tavani (2010) Frances Grodzinsky and Herman T Tavani. 2010. Applying
    the “contextual integrity” model of privacy to personal blogs in the blogoshere.
    (2010).
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grodzinsky 和 Tavani（2010）Frances Grodzinsky 和 Herman T Tavani。2010年。将“情境完整性”隐私模型应用于博客圈中的个人博客。（2010年）。
- en: Gudibande et al. (2023) Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang
    Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. The False Promise
    of Imitating Proprietary LLMs. *arXiv preprint arXiv:2305.15717* (2023).
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gudibande 等人（2023）Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng,
    Hao Liu, Pieter Abbeel, Sergey Levine, 和 Dawn Song。2023年。模仿专有LLM的虚假承诺。*arXiv预印本arXiv:2305.15717*（2023年）。
- en: Guest et al. (2006) Greg Guest, Arwen Bunce, and Laura Johnson. 2006. How many
    interviews are enough? An experiment with data saturation and variability. *Field
    methods* 18, 1 (2006), 59–82.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guest 等人（2006）Greg Guest, Arwen Bunce, 和 Laura Johnson。2006年。多少次访谈才足够？关于数据饱和度和变异性的实验。*田野方法*
    18, 1（2006年），59–82。
- en: Hargittai and Marwick (2016) Eszter Hargittai and Alice Marwick. 2016. “What
    can I really do?” Explaining the privacy paradox with online apathy. *International
    journal of communication* 10 (2016), 21.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hargittai 和 Marwick（2016）Eszter Hargittai 和 Alice Marwick。2016年。“我真的能做什么？”用在线冷漠解释隐私悖论。*国际传播杂志*
    10（2016年），21。
- en: Hitron et al. (2019) Tom Hitron, Yoav Orlev, Iddo Wald, Ariel Shamir, Hadas
    Erel, and Oren Zuckerman. 2019. Can children understand machine learning concepts?
    The effect of uncovering black boxes. In *Proceedings of the 2019 CHI conference
    on human factors in computing systems*. 1–11.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hitron 等人（2019）Tom Hitron, Yoav Orlev, Iddo Wald, Ariel Shamir, Hadas Erel,
    和 Oren Zuckerman。2019年。孩子们能理解机器学习概念吗？揭开黑箱的影响。载于*2019年CHI人机交互大会论文集*，1–11。
- en: Ischen et al. (2020) Carolin Ischen, Theo Araujo, Hilde Voorveld, Guda van Noort,
    and Edith Smit. 2020. *Privacy Concerns in Chatbot Interactions*. Springer International
    Publishing, 34â€“48. [https://doi.org/10.1007/978-3-030-39540-7_3](https://doi.org/10.1007/978-3-030-39540-7_3)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ischen 等人（2020）Carolin Ischen, Theo Araujo, Hilde Voorveld, Guda van Noort,
    和 Edith Smit。2020年。*聊天机器人互动中的隐私问题*。Springer国际出版公司，34–48。 [https://doi.org/10.1007/978-3-030-39540-7_3](https://doi.org/10.1007/978-3-030-39540-7_3)
- en: Jang et al. (2022) Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae
    Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022. Knowledge unlearning for mitigating
    privacy risks in language models. *arXiv preprint arXiv:2210.01504* (2022).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等人（2022）Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee,
    Lajanugen Logeswaran, 和 Minjoon Seo。2022年。知识遗忘：缓解语言模型隐私风险的方法。*arXiv预印本arXiv:2210.01504*（2022年）。
- en: 'Ji et al. (2023) Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang
    Ma, and Xiangang Li. 2023. Towards Better Instruction Following Language Models
    for Chinese: Investigating the Impact of Training Data and Evaluation. *arXiv
    preprint arXiv:2304.07854* (2023).'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等人（2023）Yunjie Ji, Yan Gong, Yong Deng, Yiping Peng, Qiang Niu, Baochang
    Ma, 和 Xiangang Li。2023年。面向更好的中文指令跟随语言模型：探讨训练数据和评估的影响。*arXiv预印本arXiv:2304.07854*（2023年）。
- en: Kahn Jr et al. (2012) Peter H Kahn Jr, Takayuki Kanda, Hiroshi Ishiguro, Brian T
    Gill, Jolina H Ruckert, Solace Shen, Heather E Gary, Aimee L Reichert, Nathan G
    Freier, and Rachel L Severson. 2012. Do people hold a humanoid robot morally accountable
    for the harm it causes?. In *Proceedings of the seventh annual ACM/IEEE international
    conference on Human-Robot Interaction*. 33–40.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kahn Jr 等人（2012）Peter H Kahn Jr, Takayuki Kanda, Hiroshi Ishiguro, Brian T Gill,
    Jolina H Ruckert, Solace Shen, Heather E Gary, Aimee L Reichert, Nathan G Freier,
    和 Rachel L Severson。2012年。人们是否会对类人机器人造成的伤害承担道德责任？载于*第七届年度ACM/IEEE人机交互国际会议论文集*，33–40。
- en: Kandpal et al. (2022) Nikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.
    Deduplicating training data mitigates privacy risks in language models. In *International
    Conference on Machine Learning*. PMLR, 10697–10707.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kandpal 等人（2022）Nikhil Kandpal, Eric Wallace, 和 Colin Raffel。2022年。去重训练数据减少语言模型中的隐私风险。载于*国际机器学习大会*。PMLR，10697–10707。
- en: Kang et al. (2015) Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, and Sara Kiesler.
    2015. $\{$“My$\}$ Data Just Goes $\{$Everywhere:”$\}$ User Mental Models of the
    Internet and Implications for Privacy and Security. In *Eleventh symposium on
    usable privacy and security (SOUPS 2015)*. 39–52.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等人（2015）Ruogu Kang, Laura Dabbish, Nathaniel Fruchter, 和 Sara Kiesler。2015年。$\{$“我的$\}$数据就到处$\{$去了:”$\}$用户对互联网的心理模型及其对隐私和安全的影响。载于*第十一届可用隐私与安全研讨会（SOUPS
    2015）*，39–52。
- en: 'Kaur et al. (2020) Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana,
    Hanna Wallach, and Jennifer Wortman Vaughan. 2020. Interpreting interpretability:
    understanding data scientists’ use of interpretability tools for machine learning.
    In *Proceedings of the 2020 CHI conference on human factors in computing systems*.
    1–14.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaur et al. (2020) Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana,
    Hanna Wallach, 和 Jennifer Wortman Vaughan. 2020. 解释可解释性：理解数据科学家在机器学习中的可解释性工具使用。发表于
    *Proceedings of the 2020 CHI conference on human factors in computing systems*。1–14.
- en: 'Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, and Seong Joon Oh. 2023. ProPILE: Probing Privacy Leakage in Large Language
    Models. *arXiv preprint arXiv:2307.01881* (2023).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. (2023) Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh
    Yoon, 和 Seong Joon Oh. 2023. ProPILE：探测大型语言模型中的隐私泄露。*arXiv preprint arXiv:2307.01881*
    (2023).
- en: 'Kim and Sundar (2012) Youjeong Kim and S Shyam Sundar. 2012. Anthropomorphism
    of computers: Is it mindful or mindless? *Computers in Human Behavior* 28, 1 (2012),
    241–250.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim and Sundar (2012) Youjeong Kim 和 S Shyam Sundar. 2012. 计算机的拟人化：是有意识的还是无意识的？*Computers
    in Human Behavior* 28, 1 (2012), 241–250.
- en: 'Kimmel (2023) Daniel Kimmel. 2023. ChatGPT Therapy Is Good, But It Misses What
    Makes Us Human. [https://www.columbiapsychiatry.org/news/chatgpt-therapy-is-good-but-it-misses-what-makes-us-human](https://www.columbiapsychiatry.org/news/chatgpt-therapy-is-good-but-it-misses-what-makes-us-human).
    Accessed: 09/11/2023.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kimmel (2023) Daniel Kimmel. 2023. ChatGPT治疗很好，但它错过了让我们成为人类的东西。[https://www.columbiapsychiatry.org/news/chatgpt-therapy-is-good-but-it-misses-what-makes-us-human](https://www.columbiapsychiatry.org/news/chatgpt-therapy-is-good-but-it-misses-what-makes-us-human)。访问日期：09/11/2023。
- en: Kshetri (2023) Nir Kshetri. 2023. Cybercrime and privacy threats of large language
    models. *IT Professional* 25, 3 (2023), 9–13.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kshetri (2023) Nir Kshetri. 2023. 大型语言模型的网络犯罪与隐私威胁。*IT Professional* 25, 3 (2023),
    9–13.
- en: 'Leonard (2023) Andrew Leonard. 2023. ‘Dr. Google’ meets its match: Dr. ChatGPT.
    [https://www.latimes.com/science/story/2023-09-08/dr-google-meets-its-match-dr-chatgpt](https://www.latimes.com/science/story/2023-09-08/dr-google-meets-its-match-dr-chatgpt)
    Accessed: 09/11/2023.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leonard (2023) Andrew Leonard. 2023. ‘谷歌医生’遇到劲敌：ChatGPT医生。[https://www.latimes.com/science/story/2023-09-08/dr-google-meets-its-match-dr-chatgpt](https://www.latimes.com/science/story/2023-09-08/dr-google-meets-its-match-dr-chatgpt)
    访问日期：09/11/2023。
- en: 'Li et al. (2023) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu
    Meng, and Yangqiu Song. 2023. Multi-step Jailbreaking Privacy Attacks on ChatGPT.
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*. Association
    for Computational Linguistics. [https://doi.org/10.18653/v1/2023.findings-emnlp.272](https://doi.org/10.18653/v1/2023.findings-emnlp.272)'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu
    Meng, 和 Yangqiu Song. 2023. 对ChatGPT的多步骤越狱隐私攻击。发表于 *Findings of the Association
    for Computational Linguistics: EMNLP 2023*。计算语言学协会。[https://doi.org/10.18653/v1/2023.findings-emnlp.272](https://doi.org/10.18653/v1/2023.findings-emnlp.272)'
- en: Li et al. (2021) Xuechen Li, Florian Tramer, Percy Liang, and Tatsunori Hashimoto.
    2021. Large language models can be strong differentially private learners. *arXiv
    preprint arXiv:2110.05679* (2021).
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2021) Xuechen Li, Florian Tramer, Percy Liang, 和 Tatsunori Hashimoto.
    2021. 大型语言模型可以成为强大的差分隐私学习者。*arXiv preprint arXiv:2110.05679* (2021).
- en: 'Liang et al. (2017) Hai Liang, Fei Shen, and King-wa Fu. 2017. Privacy protection
    and self-disclosure across societies: A study of global Twitter users. *new media
    & society* 19, 9 (2017), 1476–1497.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2017) Hai Liang, Fei Shen, 和 King-wa Fu. 2017. 隐私保护与自我披露的社会差异：一项全球Twitter用户的研究。*new
    media & society* 19, 9 (2017), 1476–1497.
- en: 'Liao and Vaughan (2023) Q Vera Liao and Jennifer Wortman Vaughan. 2023. AI
    Transparency in the Age of LLMs: A Human-Centered Research Roadmap. *arXiv preprint
    arXiv:2306.01941* (2023).'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao and Vaughan (2023) Q Vera Liao 和 Jennifer Wortman Vaughan. 2023. LLM时代的AI透明度：以人为本的研究路线图。*arXiv
    preprint arXiv:2306.01941* (2023).
- en: 'Lison et al. (2021) Pierre Lison, Ildikó Pilán, David Sanchez, Montserrat Batet,
    and Lilja Øvrelid. 2021. Anonymisation Models for Text Data: State of the art,
    Challenges and Future Directions. In *Proceedings of the 59th Annual Meeting of
    the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*. Association
    for Computational Linguistics. [https://doi.org/10.18653/v1/2021.acl-long.323](https://doi.org/10.18653/v1/2021.acl-long.323)'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lison et al. (2021) Pierre Lison, Ildikó Pilán, David Sanchez, Montserrat Batet,
    和 Lilja Øvrelid. 2021. 文本数据的匿名化模型：现状、挑战与未来方向。发表于 *Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers)*。计算语言学协会。[https://doi.org/10.18653/v1/2021.acl-long.323](https://doi.org/10.18653/v1/2021.acl-long.323)'
- en: Mai et al. (2020) Alexandra Mai, Katharina Pfeffer, Matthias Gusenbauer, Edgar
    Weippl, and Katharina Krombholz. 2020. User mental models of cryptocurrency systems-a
    grounded theory approach. In *Sixteenth Symposium on Usable Privacy and Security
    (SOUPS 2020)*. 341–358.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mai 等 (2020) Alexandra Mai, Katharina Pfeffer, Matthias Gusenbauer, Edgar Weippl,
    和 Katharina Krombholz. 2020. 《加密货币系统的用户心理模型——一种扎根理论方法》。载于 *第十六届可用隐私与安全研讨会（SOUPS
    2020）*，341–358页。
- en: Majmudar et al. (2022) Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami
    Smaili, Rahul Gupta, and Richard Zemel. 2022. Differentially private decoding
    in large language models. *arXiv preprint arXiv:2205.13621* (2022).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Majmudar 等 (2022) Jimit Majmudar, Christophe Dupuy, Charith Peris, Sami Smaili,
    Rahul Gupta, 和 Richard Zemel. 2022. 《大语言模型中的差分隐私解码》。*arXiv预印本 arXiv:2205.13621*
    (2022)。
- en: 'McDonald et al. (2019) Nora McDonald, Sarita Schoenebeck, and Andrea Forte.
    2019. Reliability and inter-rater reliability in qualitative research: Norms and
    guidelines for CSCW and HCI practice. *Proceedings of the ACM on human-computer
    interaction* 3, CSCW (2019), 1–23.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDonald 等 (2019) Nora McDonald, Sarita Schoenebeck, 和 Andrea Forte. 2019. 《定性研究中的可靠性与评估者间可靠性：CSCW和HCI实践的规范与指南》。*ACM人机交互学报*
    3，CSCW (2019)，1–23页。
- en: McKee et al. (2021) KR McKee, X Bai, and S Fiske. 2021. Understanding human
    impressions of artificial intelligence. *Preprint]. PsyArXiv. https://doi. org/10.31234/osf.
    io/5ursp* (2021).
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKee 等 (2021) KR McKee, X Bai, 和 S Fiske. 2021. 《理解人类对人工智能的印象》。*预印本]。PsyArXiv.
    https://doi. org/10.31234/osf. io/5ursp* (2021)。
- en: 'Mu et al. (2023) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding,
    Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. 2023. EmbodiedGPT: Vision-Language
    Pre-Training via Embodied Chain of Thought. *arXiv preprint arXiv:2305.15021*
    (2023).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mu 等 (2023) Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun
    Jin, Bin Wang, Jifeng Dai, Yu Qiao, 和 Ping Luo. 2023. 《EmbodiedGPT：通过体现链式思维进行视觉-语言预训练》。*arXiv预印本
    arXiv:2305.15021* (2023)。
- en: Natarajan and Gombolay (2020) Manisha Natarajan and Matthew Gombolay. 2020.
    Effects of anthropomorphism and accountability on trust in human robot interaction.
    In *Proceedings of the 2020 ACM/IEEE international conference on human-robot interaction*.
    33–42.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Natarajan 和 Gombolay (2020) Manisha Natarajan 和 Matthew Gombolay. 2020. 《拟人化和责任感对人类与机器人互动中信任的影响》。载于
    *2020年ACM/IEEE国际人类与机器人互动会议论文集*，33–42页。
- en: Nissenbaum (2004) Helen Nissenbaum. 2004. Privacy as contextual integrity. *Wash.
    L. Rev.* 79 (2004), 119.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nissenbaum (2004) Helen Nissenbaum. 2004. 《隐私作为情境完整性》。*华盛顿法学评论* 79 (2004)，119页。
- en: 'Nissenbaum (2020) Helen Nissenbaum. 2020. *Privacy in context: Technology,
    policy, and the integrity of social life*. Stanford University Press.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nissenbaum (2020) Helen Nissenbaum. 2020. *情境中的隐私：技术、政策与社会生活的完整性*。斯坦福大学出版社。
- en: Norman (2014) Donald A Norman. 2014. Some observations on mental models. In
    *Mental models*. Psychology Press, 15–22.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Norman (2014) Donald A Norman. 2014. 《关于心理模型的一些观察》。载于 *心理模型*，心理学出版社，15–22页。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems* 35 (2022), 27730–27744.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, 等. 2022.
    《训练语言模型以通过人类反馈遵循指令》。*神经信息处理系统进展* 35 (2022)，27730–27744页。
- en: 'Pahune and Chandrasekharan (2023) Saurabh Pahune and Manoj Chandrasekharan.
    2023. Several categories of Large Language Models (LLMs): A Short Survey. *arXiv
    preprint arXiv:2307.10188* (2023).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pahune 和 Chandrasekharan (2023) Saurabh Pahune 和 Manoj Chandrasekharan. 2023.
    《大型语言模型（LLM）的几种类别：简短调查》。*arXiv预印本 arXiv:2307.10188* (2023)。
- en: Pan et al. (2020) Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy
    risks of general-purpose language models. In *2020 IEEE Symposium on Security
    and Privacy (SP)*. IEEE, 1314–1331.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等 (2020) Xudong Pan, Mi Zhang, Shouling Ji, 和 Min Yang. 2020. 《通用语言模型的隐私风险》。载于
    *2020年IEEE安全与隐私研讨会（SP）*，IEEE，1314–1331页。
- en: Peris et al. (2023) Charith Peris, Christophe Dupuy, Jimit Majmudar, Rahil Parikh,
    Sami Smaili, Richard Zemel, and Rahul Gupta. 2023. Privacy in the Time of Language
    Models. [https://doi.org/10.1145/3539597.3575792](https://doi.org/10.1145/3539597.3575792)
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peris 等 (2023) Charith Peris, Christophe Dupuy, Jimit Majmudar, Rahil Parikh,
    Sami Smaili, Richard Zemel, 和 Rahul Gupta. 2023. 《语言模型时代的隐私》。[https://doi.org/10.1145/3539597.3575792](https://doi.org/10.1145/3539597.3575792)
- en: Renaud et al. (2014) Karen Renaud, Melanie Volkamer, and Arne Renkema-Padmos.
    2014. *Why Doesn’t Jane Protect Her Privacy?* Springer International Publishing,
    244–262. [https://doi.org/10.1007/978-3-319-08506-7_13](https://doi.org/10.1007/978-3-319-08506-7_13)
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renaud et al. (2014) Karen Renaud, Melanie Volkamer, 和 Arne Renkema-Padmos.
    2014. *为什么简没有保护她的隐私？* Springer国际出版公司，244–262。[https://doi.org/10.1007/978-3-319-08506-7_13](https://doi.org/10.1007/978-3-319-08506-7_13)
- en: 'Rutjes et al. (2019) Heleen Rutjes, Martijn Willemsen, and Wijnand IJsselsteijn.
    2019. Considerations on explainable AI and users’ mental models. In *CHI 2019
    Workshop: Where is the Human? Bridging the Gap Between AI and HCI*. Association
    for Computing Machinery, Inc.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rutjes et al. (2019) Heleen Rutjes, Martijn Willemsen, 和 Wijnand IJsselsteijn.
    2019. 关于可解释AI和用户心理模型的思考。发表于 *CHI 2019研讨会：人类在哪里？架起AI与HCI之间的桥梁*。计算机协会（ACM）。
- en: Saldaña (2015) Johnny Saldaña. 2015. *The coding manual for qualitative researchers*.
    Sage.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saldaña (2015) Johnny Saldaña. 2015. *定性研究者编码手册*。Sage。
- en: 'Shalaby et al. (2020) Walid Shalaby, Adriano Arantes, Teresa GonzalezDiaz,
    and Chetan Gupta. 2020. Building chatbots from large scale domain-specific knowledge
    bases: Challenges and opportunities. In *2020 IEEE International Conference on
    Prognostics and Health Management (ICPHM)*. IEEE, 1–8.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shalaby et al. (2020) Walid Shalaby, Adriano Arantes, Teresa GonzalezDiaz, 和
    Chetan Gupta. 2020. 从大规模领域特定知识库构建聊天机器人：挑战与机遇。发表于 *2020年IEEE国际预后与健康管理大会（ICPHM）*。IEEE，1–8。
- en: Simmons et al. (2011) Reid Simmons, Maxim Makatchev, Rachel Kirby, Min Kyung
    Lee, Imran Fanaswala, Brett Browning, Jodi Forlizzi, and Majd Sakr. 2011. Believable
    robot characters. *AI Magazine* 32, 4 (2011), 39–52.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simmons et al. (2011) Reid Simmons, Maxim Makatchev, Rachel Kirby, Min Kyung
    Lee, Imran Fanaswala, Brett Browning, Jodi Forlizzi, 和 Majd Sakr. 2011. 可信的机器人角色。*人工智能杂志*
    32, 4（2011），39–52。
- en: Solove (2020) Daniel J. Solove. 2020. The Myth of the Privacy Paradox. *SSRN
    Electronic Journal* (2020). [https://doi.org/10.2139/ssrn.3536265](https://doi.org/10.2139/ssrn.3536265)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Solove (2020) Daniel J. Solove. 2020. 隐私悖论的神话。*SSRN电子期刊*（2020）。[https://doi.org/10.2139/ssrn.3536265](https://doi.org/10.2139/ssrn.3536265)
- en: Stutzman et al. (2011) Fred Stutzman, Robert Capra, and Jamila Thompson. 2011.
    Factors mediating disclosure in social network sites. *Computers in Human Behavior*
    27, 1 (2011), 590–598.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stutzman et al. (2011) Fred Stutzman, Robert Capra, 和 Jamila Thompson. 2011.
    社交网络网站中影响信息披露的因素。*人类行为中的计算机* 27, 1（2011），590–598。
- en: 'Taver (2023) Mikhail Taver. 2023. ChatGPT is Coming to Finance, So Let’s Talk
    About the Risks and Rewards. [https://www.unite.ai/chatgpt-is-coming-to-finance-so-lets-talk-about-the-risks-and-rewards/](https://www.unite.ai/chatgpt-is-coming-to-finance-so-lets-talk-about-the-risks-and-rewards/).
    Accessed: 09/11/2023.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taver (2023) Mikhail Taver. 2023. ChatGPT即将进入金融领域，让我们来谈谈其中的风险与回报。[https://www.unite.ai/chatgpt-is-coming-to-finance-so-lets-talk-about-the-risks-and-rewards/](https://www.unite.ai/chatgpt-is-coming-to-finance-so-lets-talk-about-the-risks-and-rewards/)。访问时间：09/11/2023。
- en: Ur et al. (2015) Blase Ur, Sean M Segreti, Lujo Bauer, Nicolas Christin, Lorrie Faith
    Cranor, Saranga Komanduri, Darya Kurilova, Michelle L Mazurek, William Melicher,
    and Richard Shay. 2015. Measuring $\{$Real-World$\}$ Accuracies and Biases in
    Modeling Password Guessability. In *24th USENIX Security Symposium (USENIX Security
    15)*. 463–481.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ur et al. (2015) Blase Ur, Sean M Segreti, Lujo Bauer, Nicolas Christin, Lorrie Faith
    Cranor, Saranga Komanduri, Darya Kurilova, Michelle L Mazurek, William Melicher,
    和 Richard Shay. 2015. 测量$\{$现实世界$\}$密码猜测性建模的准确性与偏差。发表于 *第24届USENIX安全研讨会（USENIX
    Security 15）*。463–481。
- en: 'Waldman (2018) Ari Ezra Waldman. 2018. *Privacy as trust: Information privacy
    for an information age*. Cambridge University Press.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waldman (2018) Ari Ezra Waldman. 2018. *作为信任的隐私：信息时代的信息隐私*。剑桥大学出版社。
- en: 'Waldman (2021) Ari Ezra Waldman. 2021. *Industry unbound: The inside story
    of privacy, data, and corporate power*. Cambridge University Press.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Waldman (2021) Ari Ezra Waldman. 2021. *行业无界：隐私、数据与企业权力的内幕故事*。剑桥大学出版社。
- en: Wang et al. (2011) Yang Wang, Gregory Norcie, Saranga Komanduri, Alessandro
    Acquisti, Pedro Giovanni Leon, and Lorrie Faith Cranor. 2011. ” I regretted the
    minute I pressed share” a qualitative study of regrets on Facebook. In *Proceedings
    of the seventh symposium on usable privacy and security*. 1–16.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2011) Yang Wang, Gregory Norcie, Saranga Komanduri, Alessandro
    Acquisti, Pedro Giovanni Leon, 和 Lorrie Faith Cranor. 2011. “我按下分享的那一刻就后悔了”——Facebook上后悔的定性研究。发表于
    *第七届可用隐私与安全研讨会论文集*。1–16。
- en: Weidinger et al. (2021) Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    et al. 2021. Ethical and social risks of harm from language models. *arXiv preprint
    arXiv:2112.04359* (2021).
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger等人（2021）Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin,
    Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh等人。2021年。《语言模型的伦理与社会风险》。*arXiv预印本arXiv:2112.04359*（2021）。
- en: Ye et al. (2023) Yang Ye, Hengxu You, and Jing Du. 2023. Improved Trust in Human-Robot
    Collaboration With ChatGPT. *IEEE Access* 11 (2023). [https://doi.org/10.1109/access.2023.3282111](https://doi.org/10.1109/access.2023.3282111)
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人（2023）Yang Ye, Hengxu You, 和Jing Du。2023年。《通过ChatGPT提高人机协作中的信任》。*IEEE Access*
    11（2023）。[https://doi.org/10.1109/access.2023.3282111](https://doi.org/10.1109/access.2023.3282111)
- en: Yu et al. (2021) Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A
    Inan, Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,
    et al. 2021. Differentially private fine-tuning of language models. *arXiv preprint
    arXiv:2110.06500* (2021).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2021）Da Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan,
    Gautam Kamath, Janardhan Kulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz等人。2021年。《差分隐私微调语言模型》。*arXiv预印本arXiv:2110.06500*（2021）。
- en: Zeng et al. (2017) Eric Zeng, Shrirang Mare, and Franziska Roesner. 2017. End
    user security and privacy concerns with smart homes. In *thirteenth symposium
    on usable privacy and security (SOUPS 2017)*. 65–80.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng等人（2017）Eric Zeng, Shrirang Mare, 和Franziska Roesner。2017年。《智能家居中终端用户的安全与隐私担忧》。收录于*第十三届可用隐私与安全研讨会（SOUPS
    2017）*。65–80。
- en: Zhang et al. (2021) Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski,
    Florian Tramèr, and Nicholas Carlini. 2021. Counterfactual memorization in neural
    language models. *arXiv preprint arXiv:2112.12938* (2021).
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2021）Chiyuan Zhang, Daphne Ippolito, Katherine Lee, Matthew Jagielski,
    Florian Tramèr, 和Nicholas Carlini。2021年。《神经语言模型中的反事实记忆》。*arXiv预印本arXiv:2112.12938*（2021）。
- en: 'Zhang et al. (2023) Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong
    Chen, Guiming Chen, Jianquan Li, Xiangbo Wu, Zhang Zhiyi, Qingying Xiao, Xiang
    Wan, Benyou Wang, and Haizhou Li. 2023. HuatuoGPT, Towards Taming Language Model
    to Be a Doctor. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*. Association for Computational Linguistics. [https://doi.org/10.18653/v1/2023.findings-emnlp.725](https://doi.org/10.18653/v1/2023.findings-emnlp.725)'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023）Hongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Guiming
    Chen, Jianquan Li, Xiangbo Wu, Zhang Zhiyi, Qingying Xiao, Xiang Wan, Benyou Wang,
    和Haizhou Li。2023年。《华佗GPT，朝着将语言模型驯服为医生的方向前进》。收录于*计算语言学会年会成果：EMNLP 2023*。计算语言学会。[https://doi.org/10.18653/v1/2023.findings-emnlp.725](https://doi.org/10.18653/v1/2023.findings-emnlp.725)
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223* (2023).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2023）Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong等人。2023年。《大规模语言模型的调查》
    *arXiv预印本arXiv:2303.18223*（2023）。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    2023. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. *arXiv preprint
    arXiv:2306.05685* (2023).
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng等人（2023）Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao
    Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing等人。2023年。《通过MT-Bench和Chatbot
    Arena评估LLM作为法官的能力》 *arXiv预印本arXiv:2306.05685*（2023）。
- en: 'Zlatolas et al. (2015) Lili Nemec Zlatolas, Tatjana Welzer, Marjan Heričko,
    and Marko Hölbl. 2015. Privacy antecedents for SNS self-disclosure: The case of
    Facebook. *Computers in Human Behavior* 45 (2015), 158–167.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zlatolas等人（2015）Lili Nemec Zlatolas, Tatjana Welzer, Marjan Heričko, 和Marko
    Hölbl。2015年。《社交网络服务自我披露的隐私前因：以Facebook为例》。*人类行为中的计算机* 45（2015），158–167。
- en: 'Złotowski et al. (2015) Jakub Złotowski, Diane Proudfoot, Kumar Yogeeswaran,
    and Christoph Bartneck. 2015. Anthropomorphism: opportunities and challenges in
    human–robot interaction. *International journal of social robotics* 7 (2015),
    347–360.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Złotowski等人（2015）Jakub Złotowski, Diane Proudfoot, Kumar Yogeeswaran, 和Christoph
    Bartneck。2015年。《人类拟人化：人类与机器人互动中的机遇与挑战》。*国际社会机器人学杂志* 7（2015），347–360。
- en: Zuboff (2023) Shoshana Zuboff. 2023. The age of surveillance capitalism. In
    *Social Theory Re-Wired*. Routledge, 203–213.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuboff（2023）Shoshana Zuboff。2023年。《监控资本主义时代》。收录于*社会理论再连接*。Routledge出版社，203–213。
- en: Appendix A PII coding criteria
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A PII编码标准
- en: A.1\. Rules for Determining Non PII vs. PII
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1\. 确定非PII与PII的规则
- en: If the data type is incorrect (e.g., it is not even a person’s name at all),
    or it’s public information (e.g., public figures, or public information online),
    select NA, meaning it is a non PII type.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据类型不正确（例如，它根本不是一个人的名字），或者是公开信息（例如，公众人物，或者在线的公开信息），请选择NA，表示它是非PII类型。
- en: If it’s not clear from the text whether it is a random name or a real person’s
    name, we can generalize the case to similar scenarios that we may encounter in
    real life, and think about whether it makes sense to include a real person’s name
    in that situation, and if so, label it as a PII type. A random name being used
    might be if the name appears to be used as a placeholder for a real person’s name.
    For example, instances of the name, “John Doe”, being used are likely using it
    as a placeholder.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文本中无法明确判断它是一个随机名字还是一个真实的人的名字，我们可以将这种情况概括为我们在现实生活中可能遇到的类似情形，并考虑在这种情况下是否合理包含一个真实的人的名字，如果合理，则将其标记为PII类型。使用随机名字的情况可能是名字看起来是作为真实人物名字的占位符。例如，“John
    Doe”这个名字通常被用作占位符。
- en: If unsure whether certain information is public or private, attempt to search
    it online. If the information is easily found online and is not linked to a specific
    individual, then it is considered more public information (non PII). If the information
    is only available to individuals who directly interact with the owner, or the
    information is linked directly to a specific person that can be identified, then
    it is considered more private information. If the information may be used to infer
    some personal information of the user or some other people, but it’s not clear
    whether it is directly associated with these people, don’t label them as PII.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不确定某些信息是公开的还是私密的，可以尝试在线搜索。如果该信息很容易在网上找到且没有与特定个人相关联，则视为更多的公开信息（非PII）。如果该信息仅对直接与所有者互动的人可见，或者信息直接与一个可识别的特定人相关联，则视为更多的私密信息。如果该信息可能用于推断用户或其他人的某些个人信息，但不清楚是否直接与这些人相关联，则不应将其标记为PII。
- en: A.2\. Rules for Determining Actor Type (Self/Others/Both/Unknown)
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 确定行为者类型（自己/他人/两者/未知）的规则
- en: If the text doesn’t provide sufficient evidence for us to determine the relationship
    between the user and the persons mentioned in the text, label as Unknown; otherwise,
    label as Self, Others, or Both according to the context.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文本没有提供足够的证据来确定用户与文本中提到的人物之间的关系，则标记为“未知”（Unknown）；否则，根据上下文标记为“自己”（Self）、“他人”（Others）或“两者”（Both）。
- en: A label of Self means that the PII data appears to be about the human who conversed
    with ChatGPT. If the PII data appears to be about other people, then label as
    Others. If the PII data appears to be of both cases, where it is related to the
    ChatGPT user and other people, label as Both. Otherwise, if there is limited information,
    or it is uncertain who the PII data belongs to, then label as Unknown.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: “自己”（Self）标签意味着PII数据似乎是关于与ChatGPT对话的人的。如果PII数据似乎是关于其他人的，则标记为“他人”（Others）。如果PII数据既与ChatGPT用户有关，也与其他人有关，则标记为“两者”（Both）。否则，如果信息有限，或者不确定PII数据属于谁，则标记为“未知”（Unknown）。
- en: If this text explicitly mentions the PII in the context of belonging to the
    user, such as saying “my information” or “I am xxx”, label as Self. Sometimes
    if the tone or other information imply that certain topics are associated with
    a specific person, then label it as PII. For example, if a user asked ChatGPT
    to perform a very specific task involving a URL, it may be safe to assume that
    the URL is associated with the user themself.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 如果文本明确提到PII属于用户本人，例如提到“我的信息”或“我就是xxx”，则标记为“自己”（Self）。有时，如果语气或其他信息暗示某些话题与特定的人相关联，也可以将其标记为PII。例如，如果用户要求ChatGPT执行一项非常具体的任务，涉及某个URL，那么可以推测该URL与用户本人相关。
- en: If there are multiple categories that may apply to the PII we will select Unknown
    for now, but make a note of it. This may occur in the case of the user presenting
    multiple pieces of PII, in which case, it may be true that the PII belongs to
    third party individuals, or it belongs to both the user themself and third party
    individuals.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能适用于PII的类别有多个，我们暂时选择“未知”（Unknown），但要做备注。这种情况可能出现在用户提供了多个PII的情况下，在这种情况下，PII可能属于第三方个人，或者同时属于用户本人和第三方个人。
- en: A.3\. Rules About PIIs not Detected by the Algorithm
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3\. 关于算法未检测到的PII的规则
- en: There may be cases of non-detected PIIs by the algorithm. In this case, if we
    notice PII that wasn’t detected, we should still label them, and make a note about
    them.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在算法未检测到的PII。在这种情况下，如果我们注意到未被检测到的PII，我们仍应标记它们，并作出相应注释。
- en: A.4\. Rules about Specific Categories of PII
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4\. 关于特定类别PII的规则
- en: When labeling the category of PERSON, first names, last names, full names, and
    aliases all count as being this PII type.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记PERSON类别时，名字、姓氏、全名和别名都算作这种PII类型。
- en: For DATE_TIME, there may be instances of the current date being included in
    web search results, in which case we label as Self. Timestamps in messages between
    individuals should be labeled as Both. Cases of transaction histories, or other
    examples of seemingly personal documents, should be labeled as Self.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DATE_TIME，可能会出现当前日期出现在网页搜索结果中的情况，在这种情况下我们标记为Self。在个人之间的消息中的时间戳应该标记为Both。交易历史记录或其他看似个人文件的情况，应标记为Self。
- en: For the IP_ADDRESS category, private IP addresses are not considered PII.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 对于IP_ADDRESS类别，私人IP地址不被视为PII。
- en: In the case of URL, if the URL helps identify a specific person, e.g., the URL
    is the website of the person’s company, label it as PII. If the URL contains tracking
    parameters such as _ga, _gac, label as Unknown.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在URL的情况下，如果URL有助于识别特定的人，例如该URL是某人公司的官方网站，则应将其标记为PII。如果URL包含跟踪参数，如_ga，_gac，则应标记为Unknown。
- en: For NRP and LOCATION, it may be more difficult to determine if the data is considered
    PII. Thus, it is important to consider the context, especially the prompt written
    by the user. If, given the context, we infer that the detected data describes
    the NRP or location of a particular person in a relatively private context, label
    it as PII.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 对于NRP和LOCATION，可能更难判断数据是否属于PII。因此，考虑上下文非常重要，尤其是用户编写的提示。如果根据上下文推断出检测到的数据描述的是某个人在相对私密的环境中的NRP或位置，则应将其标记为PII。
- en: Appendix B Codebook for Dataset Analysis
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 数据集分析的编码本
- en: Our codebook of the ShareGPT dataset analysis results is shown in [Table 3](https://arxiv.org/html/2309.11653v2#A2.T3
    "Table 3 ‣ Appendix B Codebook for Dataset Analysis ‣ ”It’s a Fair Game”, or Is
    It? Examining How Users Navigate Disclosure Risks and Benefits When Using LLM-Based
    Conversational Agents").
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的ShareGPT数据集分析结果的编码本见于[表3](https://arxiv.org/html/2309.11653v2#A2.T3 "Table
    3 ‣ Appendix B Codebook for Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining
    How Users Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational
    Agents")。
- en: Table 3. Codebook for Dataset Analysis
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 表3. 数据集分析的编码本
- en: '| Theme | Label | Definition |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 标签 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Context | Work-Related | Scenarios related to the workplace/industry context.
    Potential Privacy Risks: There could be potential privacy risks with sharing confidential
    business information, marketing ideas and strategies, workplace communications,
    and many other industry related information. It could create privacy risks for
    the user sharing, since it might be considered a violation of workplace etiquette
    or rules by sharing industry related information. |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 上下文 | 与工作相关 | 与工作场所/行业环境相关的场景。潜在隐私风险：分享机密的商业信息、营销创意和策略、工作场所通讯以及其他行业相关信息可能存在潜在隐私风险。这可能会对分享信息的用户构成隐私风险，因为这可能被视为违反职场礼仪或规则，尤其是分享行业相关信息时。
    |'
- en: '|  | Academic-Related | Scenarios related to an academic context. Potential
    Privacy Risks: There could be potential privacy risks with asking ChatGPT to generate
    academic related texts, finish assignments, or complete tasks related to academic
    scenarios. There might be risks of plagiarism, thus leading to reputational harm
    if it was discovered that these users used AI to generate texts that they claim
    to be their own. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|  | 与学术相关 | 与学术环境相关的场景。潜在隐私风险：要求ChatGPT生成与学术相关的文本、完成作业或执行与学术场景相关的任务可能存在潜在隐私风险。如果发现这些用户使用AI生成了他们声称是自己写的文本，可能会导致抄袭风险，进而造成声誉损害。
    |'
- en: '|  | Life-Related | Scenarios related to an individual’s personal life, emotions,
    problems, or more. Potential Privacy Risks: There could be potential privacy risks
    with sharing personal information and experiences with ChatGPT. It could potentially
    be leaked, thus leading to reputational harm or relationship harm if the information
    involves third party individuals. Users might not want their sensitive information
    to be shared to the public. The information being leaked or used by other organizations
    could also lead to the data being used for targeting by advertisers. |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '|  | 生活相关 | 与个人生活、情感、问题或其他相关的场景。潜在隐私风险：与ChatGPT分享个人信息和经历可能存在潜在隐私风险。如果这些信息涉及第三方，泄露后可能会对声誉或人际关系造成伤害。用户可能不希望自己的敏感信息被公开。信息被泄露或被其他组织使用，还可能导致数据被广告商用来进行精准广告投放。
    |'
- en: '| Topic | Business | Scenarios related to business, spanning from generating
    and ideating business ideas, to asking for business advice. Potential Privacy
    Risks: There are potential privacy risks with sharing business related details,
    such as ideas, plans, strategies, etc… Some of that information might be confidential,
    or it could imply more information about the user, such as their location or company
    name. |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 商业 | 与商业相关的场景，包括从生成和构思商业创意，到寻求商业建议。潜在隐私风险：分享与商业相关的细节（如创意、计划、战略等）可能存在隐私风险。部分信息可能是机密的，或者可能泄露更多关于用户的信息，例如其位置或公司名称。
    |'
- en: '|  | Assignment | Scenarios where the user is asking ChatGPT assignment-related
    questions, or they are asking for assistance in finishing tasks for assignments.
    Potential Privacy Risks: There could be potential privacy risks with asking ChatGPT
    to help on assignment, such as risk of plagiarism accusations, and more. Also,
    the assignment details might imply details about the user, such as their academic
    direction. |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|  | 作业 | 用户向ChatGPT提问作业相关问题，或寻求帮助完成作业的场景。潜在隐私风险：请求ChatGPT帮助完成作业可能带来隐私风险，例如抄袭指控等。此外，作业的细节可能揭示用户的某些信息，例如学术方向。
    |'
- en: '|  | Programming | Scenarios related to programming and coding. The scenarios
    span anywhere from code inquiries to code generation to debugging help. Potential
    Privacy Risks: Usually in these scenarios, there are limited privacy risks that
    exist. However, in some cases, the user will include personal information, such
    as phone numbers or email addresses, in the shared code. |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|  | 编程 | 与编程和编码相关的场景，涵盖从代码询问到代码生成再到调试帮助的各类情况。潜在隐私风险：通常情况下，这些场景的隐私风险较低。然而，在某些情况下，用户可能会在共享代码中包含个人信息，如电话号码或电子邮件地址。
    |'
- en: '|  | Financial | Scenarios involving financial cases and inquiries. Potential
    Privacy Risks: There might be risks with sharing financial information, such as
    transaction histories, or other financial details, such as an increased chance
    of fraudulent activity with the financial accounts. |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '|  | 财务 | 涉及财务案例和询问的场景。潜在隐私风险：分享财务信息（如交易历史或其他财务细节）可能存在风险，例如财务账户遭遇欺诈的风险。 |'
- en: '|  | Legal | Scenarios related to law of legal cases. This can include the
    user sharing legal cases or seeking legal advice. Potential Privacy Risks: There
    might be potential privacy risks involving multiple parties, such as the attorney,
    clients, or the legal case in general. There could be consequences with sharing
    confidential information about a case with ChatGPT, in the case it got leaked.
    |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | 法律 | 与法律案件相关的场景，包括用户分享法律案件或寻求法律建议。潜在隐私风险：涉及多个当事方（如律师、客户或法律案件本身）时，可能存在隐私风险。如果案件的机密信息泄露给ChatGPT，可能会产生后果。
    |'
- en: '|  | Medical | Scenarios that include medical related inquiries, tasks, or
    other similar conversations with ChatGPT. Potential Privacy Risks: There could
    be potential privacy risks leading to autonomy harms if the people involved do
    not know their medical information is being shared to ChatGPT. |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|  | 医疗 | 包含医疗相关询问、任务或其他类似对话的场景。潜在隐私风险：如果涉及的人员不知道他们的医疗信息被分享给ChatGPT，可能会导致自主权受损的隐私风险。
    |'
- en: '|  | Life | Scenarios related to daily life, personal inquiries about life/relationships,
    and more. Potential Privacy Risks: There could be potential privacy risks with
    sharing personal information about the user’s life, as well as experiences with
    ChatGPT. If it were potentially leaked, it could lead to reputational harm or
    relationship harm if the information involves third party individuals. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '|  | 生活 | 与日常生活、个人关于生活/人际关系等的查询相关的场景。潜在的隐私风险：分享关于用户生活的个人信息以及与 ChatGPT 的互动，可能带来隐私风险。如果这些信息被泄露，可能会导致声誉损害或关系破裂，尤其是当信息涉及第三方时。
    |'
- en: '|  | Entertainment | Scenarios involving entertainment-related topics. Potential
    Privacy Risks: Most scenarios for this topic have limited privacy risks since
    the content shared or generated is mostly fictional. However, when the user begins
    incorporating personal details into the conversation, there could be privacy risks
    involved. Also, depending on the type of scenario, there could be reputational
    harm for the user if they ask ChatGPT to generate some more inappropriate content.
    |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '|  | 娱乐 | 涉及娱乐相关话题的场景。潜在的隐私风险：这个话题的大多数场景隐私风险较小，因为分享或生成的内容大多是虚构的。然而，当用户开始在对话中加入个人细节时，可能会涉及隐私风险。而且，根据场景的不同，如果用户要求
    ChatGPT 生成不当内容，可能会导致声誉受损。 |'
- en: '| Purpose | Generate content/ writing | Generating different forms of written
    or text-based content. Potential Privacy Risks: There might be privacy risks leading
    to reputational harm if it was made known that the content/writing was generated
    by AI. The user could potentially be accused of plagiarism if they tried using
    the generated content publicly. |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 目的 | 生成内容/写作 | 生成不同形式的书面或文本内容。潜在的隐私风险：如果公开了内容/写作是由 AI 生成的，可能会导致声誉受损。用户如果公开使用生成的内容，可能会被指控抄袭。
    |'
- en: '[Table 3](https://arxiv.org/html/2309.11653v2#A2.T3 "Table 3 ‣ Appendix B Codebook
    for Dataset Analysis ‣ ”It’s a Fair Game”, or Is It? Examining How Users Navigate
    Disclosure Risks and Benefits When Using LLM-Based Conversational Agents") (continued)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 3](https://arxiv.org/html/2309.11653v2#A2.T3 "表格 3 ‣ 附录 B 数据集分析代码手册 ‣ “这真的是公平的游戏吗？”——探讨用户在使用基于大语言模型的对话代理时如何应对披露风险与收益")（续）'
- en: '| Theme | Label | Definition |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 标签 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Generating plans/advice | Generating solutions or advice in response to
    a problem or query. Potential Privacy Risks: There might be privacy risks with
    the user sharing personal details in order for ChatGPT to give advice or generate
    plans. The user might share more information in order to provide enough context
    for ChatGPT to generate a useful plan or give good advice. |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  | 生成计划/建议 | 针对问题或查询生成解决方案或建议。潜在的隐私风险：用户为了让 ChatGPT 提供建议或生成计划，可能会分享个人信息。为了让
    ChatGPT 生成有用的计划或提供好的建议，用户可能会提供更多的背景信息。 |'
- en: '|  | Answering questions | Using ChatGPT to answer direct questions that might
    typically be answered via a web search, or interpreting/responding to user input.
    Potential Privacy Risks: Privacy risks might occur less in this circumstance,
    since the user typically shares details that would be included in a Google search
    as well. However, the information could still be used to imply details about the
    user, such as personal interests or similar things. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '|  | 回答问题 | 使用 ChatGPT 回答通常通过网络搜索可以获得的直接问题，或对用户输入进行解读/响应。潜在的隐私风险：在这种情况下隐私风险可能较小，因为用户通常会共享可以包含在
    Google 搜索中的信息。然而，这些信息仍然可能被用来推测关于用户的细节，例如个人兴趣等。 |'
- en: '|  | Data analysis | Using ChatGPT to analyze content input by user. Potential
    Privacy Risks: There could be risks of data leakage or privacy violations by sharing
    the data needed for analysis. A lot of times, this could also lead to discovery
    of more information about the user’s work if the data shared is about work. |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据分析 | 使用 ChatGPT 分析用户输入的内容。潜在的隐私风险：可能存在数据泄露或隐私侵犯的风险，尤其是分享用于分析的数据时。如果分享的数据涉及工作内容，还可能会导致发现更多关于用户工作的细节。
    |'
- en: '|  | Casual conversations | Users engage in casual, objective-less chatting
    with ChatGPT. The aim is the act of conversing itself, not to reach a specific
    goal. Potential Privacy Risks: There could be a lot of privacy risks since the
    user might feel more comfortable with talking to ChatGPT like another human. They
    might start revealing more personal details about their life, rather than the
    typical identifying information like an address or phone number. There might also
    be instances of the shared details from the conversation being used to identity
    the user. |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '|  | 休闲对话 | 用户与 ChatGPT 进行轻松、没有明确目标的聊天。目标是交流本身，而非达成某个特定目标。潜在隐私风险：由于用户可能更习惯像与人类交流一样与
    ChatGPT 交谈，因此可能会暴露更多个人生活细节，而不仅仅是像地址或电话号码这样的识别信息。也有可能在对话中共享的细节被用于识别用户。 |'
- en: '| Way to prompt | Direct command | Users ask straightforward questions without
    providing much context or background information. It typically involves limited
    data sharing because users input minimum information required to formulate their
    questions. Potential Privacy Risks: While there might be limited data sharing
    since user’s likely only include the most necessary details in their questions/commands,
    there is still risk of them sharing information that could imply personal interests.
    |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 提示方式 | 直接命令 | 用户提出直接的问题，通常不提供太多上下文或背景信息。这种方式通常涉及的数据分享较少，因为用户仅输入形成问题所需的最少信息。潜在隐私风险：尽管数据分享可能有限，因为用户通常只在问题/命令中包括最必要的细节，但仍有泄露个人兴趣等信息的风险。
    |'
- en: '|  | Interactively define the tasks | Users engage in multi-round interactions
    or chats with ChatGPT. The nature of these ongoing conversations can sometimes
    lead to users sharing more data with ChatGPT, especially as the conversation deepens
    or becomes more complex. Potential Privacy Risks: There are potential risks of
    users sharing more and more information with ChatGPT. For example, if the user
    does not completely fulfill their tasks with the original information provided
    to ChatGPT, they might feel inclined to provide more specific information, which
    may lead to sharing more sensitive data about themselves. |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|  | 互动定义任务 | 用户与 ChatGPT 进行多轮互动或对话。这些持续的对话有时会导致用户与 ChatGPT 分享更多数据，特别是在对话加深或变得更复杂时。潜在隐私风险：用户与
    ChatGPT 分享更多信息存在潜在风险。例如，如果用户未能完全履行他们与 ChatGPT 的初步对话任务，他们可能会倾向于提供更具体的信息，从而可能导致分享更多个人敏感数据。
    |'
- en: '|  | Handle the tasks based on given text | Users provide more extensive background
    information before asking for a response or action from ChatGPT. This approach
    can lead to more significant data sharing because users are disclosing more context
    for ChatGPT to process. Potential Privacy Risks: There could be potential privacy
    risks both with the shared text and with the tasks involved. The shared text might
    present information that could be used to imply new pieces of information about
    the user, such as their interests, occupation, or location. Also, depending on
    the tasks given to ChatGPT, there could also be risks involving plagiarism or
    reputational harm if the user asked ChatGPT to write something for them. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于给定文本处理任务 | 用户在请求 ChatGPT 响应或采取行动之前提供更多的背景信息。这种方法可能导致更多的数据分享，因为用户披露了更多背景信息以供
    ChatGPT 处理。潜在隐私风险：共享的文本和所涉及的任务可能都存在潜在隐私风险。共享的文本可能包含可以揭示用户新信息的内容，如兴趣、职业或位置等。此外，取决于用户给
    ChatGPT 的任务，如果用户要求 ChatGPT 为他们写作，可能还存在抄袭或名誉损害的风险。 |'
- en: '|  | Role-playing | Users assign a specific role to ChatGPT and provide related
    content to request ChatGPT to complete tasks or generate responses. Although this
    method may involve sharing a lot of information, the data’s authenticity and ownership
    might be ambiguous because the information is framed within a role-play scenario.
    Potential Privacy Risks: In these circumstances, sometimes, the user-assigned
    role to ChatGPT might imply information about the user themself, such as their
    own occupation or feelings toward a topic. This information might be used to trace
    back to the user and discover more information about their identity. |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '|  | 角色扮演 | 用户为ChatGPT指定一个特定的角色，并提供相关内容以请求ChatGPT完成任务或生成响应。虽然这种方法可能涉及分享大量信息，但由于信息是在角色扮演情境中框定的，因此数据的真实性和所有权可能不明确。潜在的隐私风险：在这种情况下，有时用户为ChatGPT分配的角色可能会暗示有关用户自身的信息，如他们的职业或对某个话题的看法。这些信息可能会被用来追溯到用户，并揭示更多关于其身份的信息。|'
- en: '|  | Jailbreaking | This is a special category where users attempt to push
    beyond the designed limits of ChatGPT. This can lead to unexpected outputs, sometimes
    even generating potentially harmful or violent content. While this does not necessarily
    imply higher levels of data sharing, the behavior to generate unexpected output
    might present a potential risk. Potential Privacy Risks: There could be potential
    privacy risks leading to reputational harm if the conversations got leaked. Since
    in these circumstances, the user is usually trying to violate some community guidelines,
    or are inquiring about potentially harmful or violent actions, it could harm the
    user’s reputation. |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|  | 越狱 | 这是一个特殊类别，用户试图突破ChatGPT设计的限制。这可能导致意外的输出，有时甚至生成潜在的有害或暴力内容。虽然这不一定意味着更高水平的数据共享，但生成意外输出的行为可能会带来潜在风险。潜在的隐私风险：如果这些对话被泄露，可能会带来声誉损害的隐私风险。因为在这种情况下，用户通常是在尝试违反一些社区准则，或者是在询问可能有害或暴力的行为，这可能会损害用户的声誉。|'
- en: Appendix C Interview Script
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 访谈脚本
- en: C.1\. Introduction
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1\. 引言
- en: Hello! Nice to meet you. So happy you could join us today. I’m a researcher
    at Northeastern University and looking forward to chatting with you.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 你好！很高兴见到你。非常高兴你能今天加入我们。我是东北大学的研究员，期待与你的交流。
- en: Before we start, I will introduce a bit about the interview, so you can know
    more about this. For the interview, we’d like to know the challenges that GPT
    users have encountered when they are dealing with tasks that require them to share
    personal data with GPT, and their questions and confusions about how ChatGPT uses
    their data. The goal of our research is to gain a better understanding of users’
    perspectives and challenges so that we can design systems like ChatGPT that are
    safer and more respectful.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我会先介绍一下采访的背景，这样你可以更多地了解这次采访。关于这次采访，我们希望了解GPT用户在处理需要与GPT分享个人数据的任务时遇到的挑战，以及他们对ChatGPT如何使用他们数据的疑问和困惑。我们研究的目标是更好地理解用户的观点和挑战，从而设计出像ChatGPT这样的更安全、更具尊重性的系统。
- en: Feel free to answer only the questions you’re comfortable with. If there’s a
    question that you don’t want to discuss, feel free to let me know. It won’t affect
    your compensation.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 请随意回答你觉得舒适的问题。如果有不想讨论的问题，随时告诉我。不会影响你的补偿。
- en: We will record the chat but rest assured, all your responses are only accessible
    to researchers in our team and will be kept confidential. Does that work for you?
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将记录这次对话，但请放心，你的所有回答只有我们团队的研究人员可以访问，并且会保密。这样可以吗？
- en: Can I start recording the session?
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以开始记录这次会话吗？
- en: (After getting their affirmative answer) Great, I will start to record now.
    Let’s get started!
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: （在得到肯定的答复后）太好了，现在我开始记录了。让我们开始吧！
- en: C.2\. Basic Use Experience
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2\. 基本使用体验
- en: First, let’s talk about your experience with ChatGPT.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们谈谈你使用ChatGPT的经历。
- en: •
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When did you start using it? Why did you start using it?
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你什么时候开始使用它的？为什么开始使用它？
- en: •
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What do you usually use it for?
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你通常用它做什么？
- en: Interesting, now I’d like you to show me 3 examples of conversation history
    including personal data you have prepared. Before that, just to confirm, have
    you blurred or covered any information that you do not want others to see in these
    3 examples?
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 很有意思，现在我想让你展示一下你准备的包含个人数据的3个对话示例。在此之前，请确认，你是否已经模糊处理或遮掩了不希望他人看到的信息？
- en: •
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If not:*'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果不是的话：*'
- en: –
  id: totrans-459
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Don’t worry, you can take some time to do it now. [Provide URL with instructions
    and examples of ChatGPT conversations with blurred information]
  id: totrans-460
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 别担心，你现在可以花点时间去做这件事。[提供带有模糊信息的ChatGPT对话示例和说明的链接]
- en: That’s great! Feel free to share your screen and show it to us whenever you’re
    ready. [Participant shares his/her screen]
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！随时可以分享你的屏幕，展示给我们看。[参与者分享屏幕]
- en: •
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If some use cases are mentioned in the pre-screening survey, but not covered
    in the examples:*'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果预筛选调查中提到了一些使用案例，但这些案例未在示例中涵盖：*'
- en: –
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: I have noticed your responses in the questionnaire said you have used ChatGPT
    for [specific scenarios mentioend in the pre-screening surveys].
  id: totrans-465
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我注意到你在问卷中的回答提到，你曾经在[预筛选调查中提到的具体场景]使用过ChatGPT。
- en: –
  id: totrans-466
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Could you explain how you do it in this context?
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能解释一下在这个场景下你是如何操作的吗？
- en: –
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Could you pick an example to describe what information you have input to ChatGPT
    to get responses in this context? You don’t need to show the conversation to us.
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能挑选一个例子来描述你在这个情境下输入了哪些信息给ChatGPT以获得回应吗？你不需要向我们展示对话内容。
- en: 'C.3\. Reflections: Privacy Concerns and Challenges in Preserving Privacy in
    Selected Conversation Examples'
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3\. 反思：隐私问题及在选定对话示例中保护隐私的挑战
- en: •
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Could you tell me the story about this chat? What’s your primary goal for this
    chat?
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能告诉我这个对话的故事吗？这个对话的主要目标是什么？
- en: •
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In this chat, What is the information you covered?
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个对话中，你涵盖了哪些信息？
- en: •
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Why do you think this is the information you’re not comfortable sharing with
    others?
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为什么你认为这些是你不愿与他人分享的信息？
- en: •
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you had any concerns about sharing such data with ChatGPT?
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你在与ChatGPT分享这些数据时有任何顾虑吗？
- en: •
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you ever considered addressing these concerns?
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否曾考虑过解决这些顾虑？
- en: •
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What methods have you tried or thought about? Give more details about that process?
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你尝试过或考虑过哪些方法？请提供更多关于这个过程的细节。
- en: •
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you encounter any challenges or difficulties when trying to do this?
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在尝试这样做时，你是否遇到过任何挑战或困难？
- en: •
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If he/she has shared other people’s information:*'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他/她曾分享过其他人的信息：*'
- en: –
  id: totrans-487
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Have you thought that other people might disclose your personal information
    as well?
  id: totrans-488
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否考虑过其他人可能也会泄露你的个人信息？
- en: •
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*After discussion on all the examples:*'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*在讨论所有示例之后：*'
- en: –
  id: totrans-491
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Have you wanted to use ChatGPT to do something but refrained from doing so because
    of concerns about your privacy? Could you tell us more about it?
  id: totrans-492
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否曾想过使用ChatGPT做某些事情，但因为隐私顾虑而犹豫不决？你能告诉我们更多关于这方面的情况吗？
- en: –
  id: totrans-493
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Could you explain more about your thought process?
  id: totrans-494
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能更多地解释一下你的思维过程吗？
- en: –
  id: totrans-495
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '*If we noticed a conflict between what they mentioned using ChatGPT for and
    their concerns:*'
  id: totrans-496
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果我们注意到他们提到使用ChatGPT的情况与他们的顾虑之间存在冲突：*'
- en: '*'
  id: totrans-497
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: I’d like to give you a minute to review your chat histories that use xxx data.
    In what circumstance, you would still do so even if you thought of that? And under
    what circumstance, you would stop doing so based on the concern you mentioned?
  id: totrans-498
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我想给你一分钟时间回顾一下你使用xxx数据的对话历史。在什么情况下，即使你考虑到这个问题，你仍然会继续这样做？在什么情况下，基于你提到的顾虑，你会停止这么做？
- en: –
  id: totrans-499
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Interesting. Let’s think about the privacy issue from the other side. Have you
    used ChatGPT for any tasks to protect your privacy? Such as, you feel more comfortable
    asking ChatGPT to do certain tasks or sharing certain data with ChatGPT than other
    choices?
  id: totrans-500
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 很有意思。我们换个角度思考隐私问题。你是否曾使用ChatGPT执行某些任务来保护你的隐私？例如，觉得向ChatGPT请求执行某些任务或分享某些数据比其他选择更让你感到放心？
- en: C.4\. Mental Model on GPT-based Conversational Agent
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4\. 基于GPT的对话代理的心理模型
- en: Now, let’s go to the next part. [Open up writeboard on Zoom]
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入下一部分。[在Zoom上打开白板]
- en: C.4.1\. General System
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.1\. 通用系统
- en: '[For ChatGPT users]'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '[针对ChatGPT用户]'
- en: In this session, I’d like to learn more about your understanding on how the
    system works and how the system will use your data. Please feel free to share
    your best understanding about how the system handles your data. Your input will
    be extremely useful for us to understand the limitations of current system design.
    And we won’t judge your answer and your compensation won’t be affected. Does it
    sound good?
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次会议中，我希望了解更多关于你如何理解系统如何运作以及系统如何使用你的数据。请随时分享你对系统如何处理数据的最佳理解。你的反馈对我们理解当前系统设计的局限性非常有帮助。我们不会对你的回答做出评判，并且你的补偿不会受到影响。这样听起来怎么样？
- en: Great, now we will do a drawing exercise. I’m going to ask you to explain your
    perceptions and ideas about how ChatGPT works — keeping in mind how things work
    “behind the scenes” — when you are chatting with ChatGPT. Imagine you ask ChatGPT
    a question, how does the data you input go through the system?
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在我们来进行一个绘图练习。我将请你解释你对ChatGPT如何工作的看法和想法——记住要考虑到“幕后”如何运作——当你和ChatGPT聊天时。假设你问ChatGPT一个问题，输入的数据是如何在系统中流动的？
- en: You can use the drawing and texting tools on the left of the screen [show instructions],
    to draw how you think the ChatGPT works, what will happen to your input data after
    it is submitted through the interface [show the starting point]. Please talk aloud
    and explain your thought processes while you are drawing.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用屏幕左侧的绘图和文本工具 [展示说明]，画出你认为ChatGPT是如何工作的，在你通过界面提交输入数据后，这些数据会发生什么 [展示起始点]。请在你绘图时大声讲述并解释你的思维过程。
- en: Assume that one year later, other users asked ChatGPT similar questions. Do
    you think that the information you provided a year ago could potentially influence
    the responses produced by ChatGPT? Why or why not?
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一年后，其他用户向ChatGPT提出了类似的问题。你认为你一年前提供的信息会影响ChatGPT生成的回应吗？为什么或者为什么不？
- en: '*If the participants showed misunderstandings, use the following part to debrief
    them on the process:*'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果参与者表现出误解，请使用以下部分来对他们进行回顾：*'
- en: '[Ask to follow cursor in the Whiteboard]'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '[要求跟随白板上的光标]'
- en: •
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Great, you have mentioned most of the parts. I’d like to give you more information
    about how it works and how your data flows within ChatGPT.
  id: totrans-512
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 很好，你已经提到了大部分部分。我想给你更多关于它如何工作以及你的数据如何在ChatGPT中流动的信息。
- en: •
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: After the data gets input through the interface, your input will be preprocessed
    first and then be sent to the AI model (we have GPT here) which is trained by
    large public internet data. The GPT will generate the responses based on your
    input and then return it to you.
  id: totrans-514
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在数据通过界面输入后，首先会进行预处理，然后将其发送到由大量公共互联网数据训练的AI模型（这里我们使用的是GPT）。GPT会根据你的输入生成回应，然后将其返回给你。
- en: •
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: During the process of sending user input from the interface to the processing
    unit, your data will be uploaded to OpenAI’s remote server. So the later process
    will all run on the remote server until the responses are returned.
  id: totrans-516
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在将用户输入从界面发送到处理单元的过程中，你的数据会被上传到OpenAI的远程服务器。所以后续的处理过程都将在远程服务器上运行，直到响应返回。
- en: •
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: This is generally what happens behind when you’re using this.
  id: totrans-518
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这通常是在你使用时发生的幕后情况。
- en: '*[Data retention/ storage/storing in a database]:*'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '*[数据保留/存储/存储在数据库中]:*'
- en: It is worth mentioning that your input data will be stored in the data storage
    which is remote as well.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，你的输入数据也将被存储在远程的数据存储中。
- en: •
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are you aware that your data will be stored in the system before?
  id: totrans-522
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道你的数据会在之前被存储在系统中吗？
- en: •
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If not:*'
  id: totrans-524
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果不是这样的话：*'
- en: –
  id: totrans-525
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Will this have any impact on your preferences of sharing data with ChatGPT?
  id: totrans-526
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这会影响你与ChatGPT分享数据的偏好吗？
- en: •
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If so:*'
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果是这样的话：*'
- en: –
  id: totrans-529
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Did it have any impact on your preferences of sharing data with ChatGPT?
  id: totrans-530
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这对你与ChatGPT分享数据的偏好有影响吗？
- en: '*[Model training & Memorization]:*'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '*[模型训练与记忆化]:*'
- en: Another important process is model training. Parts of the data stored will be
    used for ChatGPT training. So the GPT AI model can improve overtime.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的过程是模型训练。存储的部分数据将用于ChatGPT的训练。这样，GPT AI模型就能随着时间的推移不断改进。
- en: •
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are you aware of that before? How did you know that?
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你之前知道这一点吗？你是怎么知道的？
- en: •
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What do you think of their use of your conversations for training models?
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你如何看待他们将你的对话用于训练模型？
- en: •
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do you think there are any risks that may be caused by using your or other user
    conversations for model training?
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为使用你或其他用户的对话进行模型训练可能会带来哪些风险？
- en: There has been some research showing a memorization risk in the large language
    models. These models may memorize parts of the data used for training. That means,
    OpenAI’s future models may memorize some data that you provided, and because the
    same model is used by all the users, your information may be leaked to other people
    when they ask certain questions to the model.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 有研究表明，大型语言模型存在记忆化风险。这些模型可能会记住用于训练的数据的部分内容。这意味着，OpenAI未来的模型可能会记住你提供的一些数据，因为相同的模型被所有用户使用，所以当其他用户向模型提问时，你的信息可能会泄露给其他人。
- en: '*[There was research that showed that GPT-3 offered detailed private information
    about the Editor-in-Chief of MIT Technology Review including his family members,
    work address, and phone number.]*'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '*[有研究表明，GPT-3提供了有关MIT科技评论主编的详细私人信息，包括他的家人、工作地址和电话号码。]*'
- en: •
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are you aware of training or memorization?
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道训练或记忆的相关情况吗？
- en: •
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: After learning about that, what do you think about your preferences for data
    retention?
  id: totrans-544
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 了解了这些信息后，你对数据保存偏好有什么看法？
- en: •
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Are there any data types you are concerned about being memorized?
  id: totrans-546
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有没有你担心会被记住的数据类型？
- en: C.4.2\. Detailed Function
  id: totrans-547
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.4.2. 详细功能
- en: Great, let’s dive into the detailed functions. Similar to the previous session,
    we’re evaluating the system design, not your capacity. Feel free to share your
    best understanding about the questions that we are going to discuss. The correctness
    of your answers won’t affect your compensation. We will have discussion towards
    the interface of ChatGPT. So, please open the ChatGPT interface and share your
    screen with me. You can choose to only share certain parts of the interface, specifically,
    you may hide any conversation titles for your privacy. You can follow the instructions
    here if needed. [Guidance to share portion of the screen]
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，让我们深入了解这些详细功能。和之前的环节类似，我们在评估系统设计，而不是你的能力。请随意分享你对我们将要讨论的问题的最佳理解。你的答案的正确性不会影响你的报酬。我们将讨论ChatGPT界面。所以，请打开ChatGPT界面并分享你的屏幕。你可以选择只分享界面的某些部分，具体来说，你可以隐藏任何对话标题以保护你的隐私。如果需要，你可以按照这里的指示进行操作。[如何分享屏幕部分]
- en: '*[Chat history]:*'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '*[聊天记录]:*'
- en: 'We have talked about the Data Storage:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论过数据存储问题：
- en: •
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you done anything with the Chat history before? For what? (e.g. delete,
    export, sharing link)
  id: totrans-552
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你之前有处理过聊天记录吗？是为了什么？（例如：删除、导出、分享链接）
- en: •
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What’s your expectation after deleting the Chat history? (Do you think it will
    be completely deleted from the storage?)
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 删除聊天记录后，你有什么预期？（你认为它会完全从存储中删除吗？）
- en: According to OpenAI’s policy, the history will be retained for a maximum of
    30 days after you delete it through the interface.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 根据OpenAI的政策，在你通过界面删除聊天记录后，记录将最多保留30天。
- en: •
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you know that before?
  id: totrans-557
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你之前知道这个吗？
- en: '*[Training data opt-out]:*'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '*[数据训练退出]:*'
- en: '[For ChatGPT users]'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '[针对ChatGPT用户]'
- en: As we discussed before, OpenAI will use the user’s conversations with ChatGPT
    to train their models.
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，OpenAI会使用用户与ChatGPT的对话来训练他们的模型。
- en: •
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do you know any method to opt-out and stop your data being used in model training?
    Have you thought of that before?
  id: totrans-562
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道有什么方法可以选择退出并停止使用你的数据进行模型训练吗？你之前有考虑过吗？
- en: Yes, there is an opt-out option there, the default setting is opt-in.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，那里有一个退出选项，默认设置是选择加入。
- en: •
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Did you know that before? Do you know how to opt out of data training?
  id: totrans-565
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你之前知道吗？你知道如何选择退出数据训练吗？
- en: •
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they say yes:*'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们回答是：*'
- en: –
  id: totrans-568
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: How did you know that?
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是怎么知道这个的？
- en: –
  id: totrans-570
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Could you show me how to do it?
  id: totrans-571
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你能展示一下如何操作吗？
- en: •
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they say no:*'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们回答否：*'
- en: –
  id: totrans-574
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Maybe you can have a try to find the opt-out option.
  id: totrans-575
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也许你可以尝试找到退出选项。
- en: •
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they fail:*'
  id: totrans-577
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们操作失败：*'
- en: –
  id: totrans-578
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: You can first find out the “settings” button in the bottom left corner. Then
    click the “Data controls”, you will see it in the first line.
  id: totrans-579
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以先在左下角找到“设置”按钮。然后点击“数据控制”，你会在第一行看到它。
- en: –
  id: totrans-580
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Now, you know this option. Would you consider opting out of data training? Why?
  id: totrans-581
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在你知道了这个选项。你会考虑选择退出数据训练吗？为什么？
- en: '*[Account sharing]:*'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '*[账户分享]:*'
- en: •
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you shared your ChatGPT account or OpenAI account with anyone else? Why
    or why not?
  id: totrans-584
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否与他人分享过你的ChatGPT账户或OpenAI账户？为什么或为什么不？
- en: •
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you shared any other account (besides ChatGPT) with others? How similar
    or different do you perceive about these account sharing practices compared with
    ChatGPT account sharing?
  id: totrans-586
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否与他人分享过其他账户（除了ChatGPT）？你认为这些账户分享的做法与ChatGPT账户分享有什么相似或不同之处？
- en: '*[Sharing conversations with other users]:*'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '*[与其他用户分享对话]:*'
- en: •
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you considered sharing conversations with others? Could you tell us more
    about it? (When, why, how, shared content, with whom)
  id: totrans-589
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你有考虑过与其他人分享你的对话内容吗？可以告诉我们更多吗？（何时、为什么、如何、分享了什么内容、与谁分享）
- en: •
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you used any browser extension for sharing your conversations or the native
    sharing feature? Why or why not?
  id: totrans-591
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是否使用过任何浏览器扩展来分享你的对话内容或使用过原生分享功能？为什么或为什么不？
- en: •
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they have used them:*'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们使用过这些功能：*'
- en: –
  id: totrans-594
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '*Do you remember, under what circumstances did you use the sharing tools?*'
  id: totrans-595
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*你记得在什么情况下使用过分享工具吗？*'
- en: •
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they have used the ChatGPT native “shared links” feature:*'
  id: totrans-597
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们使用了 ChatGPT 的原生“共享链接”功能：*'
- en: –
  id: totrans-598
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Who do you think can see the content in this link?
  id: totrans-599
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为谁可以看到这个链接中的内容？
- en: –
  id: totrans-600
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Do you know that anyone with the link can view and continue the linked conversation?
  id: totrans-601
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道任何拥有链接的人都可以查看并继续链接的对话吗？
- en: –
  id: totrans-602
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Do you know how to delete or invalidate the link? Could you try to do so?
  id: totrans-603
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道如何删除或使链接失效吗？你能尝试做一下吗？
- en: –
  id: totrans-604
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '*If they fail:*'
  id: totrans-605
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们失败：*'
- en: '*'
  id: totrans-606
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: You can first go to the “settings” in the bottom left corner. Then click the
    “Data controls”. In the 2nd line you can manage your shared links.
  id: totrans-607
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以首先去左下角的“设置”。然后点击“数据控制”。在第二行，你可以管理你分享的链接。
- en: •
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*If they have used (a) chrome extension(s):*'
  id: totrans-609
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果他们使用了 (a) Chrome 扩展程序：*'
- en: –
  id: totrans-610
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: What is it/are they?
  id: totrans-611
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它是什么/它们是什么？
- en: –
  id: totrans-612
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Who do you think can see the conversations you shared?
  id: totrans-613
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为谁可以看到你分享的对话？
- en: –
  id: totrans-614
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Do you know about [Public dataset curated from leaked conversations like ShareGPT]?
  id: totrans-615
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你知道[从泄露对话中整理的公开数据集如 ShareGPT]吗？
- en: –
  id: totrans-616
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: '*If not, describe.*'
  id: totrans-617
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*如果不是，请描述。*'
- en: –
  id: totrans-618
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: What do you think about the impact of this kind of data leak on your data?
  id: totrans-619
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为这种数据泄露对你的数据有什么影响？
- en: –
  id: totrans-620
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: What type of your own data pops up in your mind that would be sensitive to be
    leaked?
  id: totrans-621
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你想到的有哪些你自己的数据是敏感的，可能会被泄露？
- en: '*[Use ChatGPT plugin]:*'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '*[使用 ChatGPT 插件]：*'
- en: •
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Have you used any ChatGPT plugins? What plugins did you use? What did you use
    them for?
  id: totrans-624
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你使用过任何 ChatGPT 插件吗？你使用了哪些插件？你是用它们做什么的？
- en: •
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What data do you think the third-party plug-in developers can access? How do
    you think the data is used by the third party developers?
  id: totrans-626
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为第三方插件开发者可以访问哪些数据？你认为这些数据是如何被第三方开发者使用的？
- en: 'C.5\. Back to Reflections: Privacy Concerns and Challenges in Preserving Privacy'
  id: totrans-627
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5. 回到反思：隐私顾虑与隐私保护中的挑战
- en: Cool, we have discussed a lot about how ChatGPT (and other agents they use)
    works.
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒，我们讨论了很多关于 ChatGPT（以及他们使用的其他代理）是如何工作的。
- en: •
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Anything new or surprising for you today?
  id: totrans-630
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 今天有什么新鲜的或令人惊讶的事情吗？
- en: •
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Given what you learned today, are there any additional concerns about sharing
    data with ChatGPT that you want to discuss with us?
  id: totrans-632
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据你今天学到的内容，是否有关于与 ChatGPT 分享数据的其他顾虑，想和我们讨论的吗？
- en: •
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What do you think you can do to address these concerns?
  id: totrans-634
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为你能做些什么来应对这些顾虑？
- en: •
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do you wish for any improvement of existing features or tool support?
  id: totrans-636
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你希望现有的功能或工具支持有所改进吗？
- en: •
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Do you think there should be any additional support that is not yet available?
  id: totrans-638
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你认为应该增加哪些当前还没有的支持功能吗？
- en: •
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Let’s imagine together. If you have a magic wand to change anything, what would
    the ideal support or scenario look like for you, that can make you feel totally
    secure about your data when using ChatGPT?
  id: totrans-640
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一起想象一下。如果你有一根魔法棒，可以改变任何事情，理想的支持或场景对你来说应该是什么样的，让你在使用 ChatGPT 时能完全放心自己的数据？
- en: That’s a wrap for our interview today! Thank you so much for taking the time
    to share your insights and experiences with us - it’s been incredibly valuable.
    We hope you find it useful. Please don’t hesitate to reach out if you have any
    further questions or thoughts you’d like to share. Thanks again for your contribution
    to our study!
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 今天的访谈到此为止！非常感谢你抽出时间与我们分享你的见解和经验——这对我们非常有价值。希望你觉得它有用。如果你有任何其他问题或想法，欢迎随时联系我们。再次感谢你对我们研究的贡献！
- en: Appendix D Codebook for the interview results
  id: totrans-642
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 面试结果编码本
- en: Codebook for the interview results is shown in [Table 4](https://arxiv.org/html/2309.11653v2#A4.T4
    "Table 4 ‣ Appendix D Codebook for the interview results ‣ ”It’s a Fair Game”,
    or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using
    LLM-Based Conversational Agents").
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 面试结果的编码本显示在[表格 4](https://arxiv.org/html/2309.11653v2#A4.T4 "表格 4 ‣ 附录 D 面试结果编码本
    ‣ “这是公平的游戏”，还是它？检视用户在使用基于 LLM 的对话代理时如何权衡披露风险与收益")中。
- en: Table 4. Codebook for the interview results
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4. 面试结果编码本
- en: '| Theme | Code | Memo |'
  id: totrans-645
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 编码 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Use Cases and Disclosure Behaviors | Ad Writing | Users shared details of
    items they were selling and their expectations to generate an ad. |'
  id: totrans-647
  prefs: []
  type: TYPE_TB
  zh: '| 使用案例和披露行为 | 广告写作 | 用户分享了他们正在出售的物品的详细信息以及他们的广告预期。 |'
- en: '|  | Book Chapter Writing | Users provided topics or related contexts for drafting
    book chapters. |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '|  | 书籍章节写作 | 用户提供了写作书籍章节的主题或相关背景。 |'
- en: '|  | Career Advice | Users asked ChatGPT for career advice including generating
    templates or drafting resume or cover letter, or consulting, based on their educational
    and work experience and expectation. |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '|  | 职业建议 | 用户向ChatGPT咨询职业建议，包括生成模板或起草简历或求职信，或根据他们的教育和工作经验以及期望进行咨询。 |'
- en: '|  | Casual Chat | Users had casual chat with ChatGPT normally with less intention
    and just for fun or exploration. Many personal life information may be shared
    in casual chat. |'
  id: totrans-650
  prefs: []
  type: TYPE_TB
  zh: '|  | 轻松聊天 | 用户与ChatGPT进行轻松聊天，通常没有明确意图，仅仅是为了娱乐或探索。在轻松聊天中，可能会分享许多个人生活信息。 |'
- en: '|  | Class Preparation | Users used ChatGPT for class preparation including
    brainstorming class sections or methods to teach certain things, normally shared
    the information related to class. |'
  id: totrans-651
  prefs: []
  type: TYPE_TB
  zh: '|  | 课程准备 | 用户使用ChatGPT进行课程准备，包括头脑风暴课程内容或教学某些内容的方法，通常会分享与课程相关的信息。 |'
- en: '|  | Concepts Learning | User used ChatGPT to help learning new concepts. |'
  id: totrans-652
  prefs: []
  type: TYPE_TB
  zh: '|  | 概念学习 | 用户使用ChatGPT帮助学习新概念。 |'
- en: '|  | Copy-editing | Used ChatGPT for copy-writing, normally for work. |'
  id: totrans-653
  prefs: []
  type: TYPE_TB
  zh: '|  | 文案编辑 | 用户通常使用ChatGPT进行文案写作，通常用于工作。 |'
- en: '|  | Create Apps | Users used ChatGPT to guide APP creation, including generating
    codes, debugging and problem solving. |'
  id: totrans-654
  prefs: []
  type: TYPE_TB
  zh: '|  | 创建应用程序 | 用户使用ChatGPT指导APP创建，包括生成代码、调试和解决问题。 |'
- en: '|  | Data Analysis | Users used LLM-based CAs for data analysis in many ways.
    From directly sharing raw data asking for results, explaining the tasks asking
    for solutions and just asked certain questions. |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '|  | 数据分析 | 用户使用基于大语言模型的CA进行多种方式的数据分析。从直接共享原始数据请求结果，到解释任务请求解决方案，或仅仅提出特定问题。
    |'
- en: '|  | Diet Advice | User shared personal health related information or expectations
    to generate diet plan or ask for diet advice. |'
  id: totrans-656
  prefs: []
  type: TYPE_TB
  zh: '|  | 饮食建议 | 用户分享个人健康信息或期望，以生成饮食计划或寻求饮食建议。 |'
- en: '|  | Email Writing | Users used ChatGPT for email writing in many ways. From
    directly sharing emails sent by others and asking for responses, to elaborating
    the context, drafting emails by users themselves and ask for revising or just
    provide general information to generate emails. |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '|  | 邮件写作 | 用户以多种方式使用ChatGPT进行邮件写作。从直接分享他人发送的邮件并要求回应，到阐述上下文，用户自己起草邮件并要求修改，或仅提供一般信息以生成邮件。
    |'
- en: '|  | Email/Work Message Writing | Users used ChatGPT for work-related message
    or email writing or revising. |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '|  | 邮件/工作信息写作 | 用户使用ChatGPT进行与工作相关的消息或邮件写作或修改。 |'
- en: '|  | Exercise Advice | Users normally shared personal health related information
    and exercise goals for exercise advice. |'
  id: totrans-659
  prefs: []
  type: TYPE_TB
  zh: '|  | 健身建议 | 用户通常分享个人健康信息和健身目标，以获得健身建议。 |'
- en: '|  | Finance Advice | Users used ChatGPT for financial consultation such as
    debt problems, budget plan. |'
  id: totrans-660
  prefs: []
  type: TYPE_TB
  zh: '|  | 财务咨询 | 用户使用ChatGPT进行财务咨询，如债务问题、预算计划。 |'
- en: '|  | Generate Survey Responses | Users used ChatGPT to generate responses to
    the survey they have to do. |'
  id: totrans-661
  prefs: []
  type: TYPE_TB
  zh: '|  | 生成调查回答 | 用户使用ChatGPT生成他们需要完成的调查问卷的回答。 |'
- en: '|  | Immigration Advice | Users used ChatGPT to consult immigration problems
    such as visa application. |'
  id: totrans-662
  prefs: []
  type: TYPE_TB
  zh: '|  | 移民咨询 | 用户使用ChatGPT咨询移民问题，如签证申请。 |'
- en: '|  | Info search | Users used ChatGPT for normal information search like Google
    search. |'
  id: totrans-663
  prefs: []
  type: TYPE_TB
  zh: '|  | 信息搜索 | 用户使用ChatGPT进行常规的信息搜索，如同Google搜索。 |'
- en: '|  | Joke Writing | Users used ChatGPT to generate jokes. Sometime provided
    their friends’ names to generate “personalized” jokes. |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '|  | 写笑话 | 用户使用ChatGPT生成笑话。有时会提供朋友的名字，以生成“个性化”的笑话。 |'
- en: '|  | Language Learning | Users used ChatGPT for language learning such as learning
    the expression through chatting. |'
  id: totrans-665
  prefs: []
  type: TYPE_TB
  zh: '|  | 语言学习 | 用户使用ChatGPT进行语言学习，如通过聊天学习表达方式。 |'
- en: '|  | Legal Advice | Users used agents for consulting legal related problems
    such as drafting contract, consulting related law based on given context. |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '|  | 法律咨询 | 用户使用代理人咨询与法律相关的问题，如起草合同、根据给定背景咨询相关法律。 |'
- en: '|  | Life Advice | Asking for life advice based on given personal conditions
    such as advice on work-life-balance. |'
  id: totrans-667
  prefs: []
  type: TYPE_TB
  zh: '|  | 生活建议 | 根据个人情况寻求生活建议，如工作与生活平衡的建议。 |'
- en: '|  | Literature Search | Users used ChatGPT to search related literature based
    on certain topics or given content. |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '|  | 文献搜索 | 用户使用ChatGPT根据特定话题或给定内容搜索相关文献。 |'
- en: '|  | Marketing Advice | Asking for business marketing advice based on business
    conditions or general questions. |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '|  | 营销建议 | 根据商业状况或一般性问题寻求商业营销建议。 |'
- en: '|  | Math Learning | Used ChatGPT to learn math and prepare math exam. |'
  id: totrans-670
  prefs: []
  type: TYPE_TB
  zh: '|  | 数学学习 | 使用ChatGPT学习数学并准备数学考试。 |'
- en: '|  | Medical Advice | Asked for medical advice normally based on detailed personal
    medical or health information such as diagnosis results. |'
  id: totrans-671
  prefs: []
  type: TYPE_TB
  zh: '|  | 医疗建议 | 用户通常基于个人详细的医疗或健康信息（如诊断结果）寻求医疗建议。 |'
- en: '|  | Portfolio Making | Used ChatGPT for brainstorming portfolio idea and for
    suggestions. |'
  id: totrans-672
  prefs: []
  type: TYPE_TB
  zh: '|  | 作品集制作 | 用户使用ChatGPT进行头脑风暴，提出作品集创意并获取建议。 |'
- en: '|  | Programming | Used ChatGPT for programming for such as prototyping or
    debugging. |'
  id: totrans-673
  prefs: []
  type: TYPE_TB
  zh: '|  | 编程 | 用户使用ChatGPT进行编程，例如用于原型设计或调试。 |'
- en: '|  | Relocation Advice | Used ChatGPT for suggestions about relocation. Users
    normally provided the location information to ask for suggestions. |'
  id: totrans-674
  prefs: []
  type: TYPE_TB
  zh: '|  | 搬迁建议 | 用户使用ChatGPT获取搬迁建议，通常会提供位置相关信息以寻求建议。 |'
- en: '|  | Research Work | Users used ChatGPT to do research such as on clients (companies
    or persons) or certain topics. |'
  id: totrans-675
  prefs: []
  type: TYPE_TB
  zh: '|  | 研究工作 | 用户使用ChatGPT进行研究，例如关于客户（公司或个人）或特定话题的研究。 |'
- en: '[Table 4](https://arxiv.org/html/2309.11653v2#A4.T4 "Table 4 ‣ Appendix D Codebook
    for the interview results ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")
    (continued)'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 4](https://arxiv.org/html/2309.11653v2#A4.T4 "表格 4 ‣ 附录 D 访谈结果代码书 ‣ ''这是公平游戏''，还是这样？审视用户在使用基于LLM的对话代理时如何权衡披露风险与收益")（续）
    |'
- en: '| Theme | Code | Memo |'
  id: totrans-677
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 代码 | 备忘录 |'
- en: '| --- | --- | --- |'
  id: totrans-678
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Revise Writing | Users used ChatGPT to revise their own writing in various
    purpose such as paper revision. |'
  id: totrans-679
  prefs: []
  type: TYPE_TB
  zh: '|  | 修改写作 | 用户使用ChatGPT修改自己的写作，目的多样，如论文修改。 |'
- en: '|  | Schoolwork | Used ChatGPT to help with or finish the schoolwork. |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '|  | 学业 | 用户使用ChatGPT帮助完成或协助完成学业任务。 |'
- en: '|  | Social Media Post Writing | Used ChatGPT to generate social media post
    writing for personal, business or work purposes. |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '|  | 社交媒体帖子写作 | 用户使用ChatGPT为个人、商业或工作目的生成社交媒体帖子写作。 |'
- en: '|  | Test Capabilities | Users tries to test ChatGPT’s capacities and explored
    what the agent can do. |'
  id: totrans-682
  prefs: []
  type: TYPE_TB
  zh: '|  | 测试能力 | 用户尝试测试ChatGPT的能力，并探索代理能够做什么。 |'
- en: '|  | Therapy | Users used ChatGPT as therapy to share personal life conditions,
    thoughts, feelings and emotion to more ask for emotional support. |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '|  | 治疗 | 用户将ChatGPT作为治疗工具，分享个人生活状况、想法、感受和情绪，寻求情感支持。 |'
- en: '| Factors that Affect Users’ Disclosure Intentions with LLM-Based CAs | Perceived
    Capability of the CAs | Users decide which agents to share or what information
    to share primarily considering the capability of the CAs they perceived. They
    tend to share more information if they believe the agents can help with their
    tasks while withdraw if they don’t believe or unsatisfied with agents’ capability.
    |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| 影响用户披露意图的因素与基于LLM的对话代理 | 对话代理的感知能力 | 用户决定与哪些代理共享信息或共享哪些信息，主要考虑他们对代理能力的感知。如果用户认为代理能够帮助他们完成任务，他们倾向于分享更多信息；如果他们不相信代理或对其能力不满意，则会选择撤回信息。
    |'
- en: '|  | Convenience of Operation | Users’ disclosure intention or behavior can
    be influenced the convenience of operation. They may choose to share more or less
    depending on the operation convenience. |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '|  | 操作便利性 | 用户的披露意图或行为可能会受到操作便利性的影响。他们可能会根据操作的便利性选择分享更多或更少的信息。 |'
- en: '|  | Perceived Personal Data Sensitivity | Users’ disclosure would be influenced
    by the perceived sensitivity of the data. While the perceived sensitivity is vary
    from individuals. |'
  id: totrans-686
  prefs: []
  type: TYPE_TB
  zh: '|  | 感知的个人数据敏感性 | 用户的披露行为会受到数据敏感性感知的影响。虽然这种敏感性因人而异。 |'
- en: '|  | Resignation | Users feel less concern to share information that can accessed
    from other places such as other online platforms or other databases. |'
  id: totrans-687
  prefs: []
  type: TYPE_TB
  zh: '|  | 放弃 | 用户对分享那些可以从其他地方（如其他在线平台或数据库）获取的信息较少关注。 |'
- en: '|  | Perceived Risks and Harms-Concerns over data misuse by institutions |
    Have concerns to share information because users unsure how their data will be
    used or whether will be misused by the companies behind the systems. |'
  id: totrans-688
  prefs: []
  type: TYPE_TB
  zh: '|  | 感知的风险与危害——对机构数据滥用的担忧 | 用户对分享信息感到担忧，因为他们不确定自己的数据将如何被使用，或是否会被背后公司的滥用。 |'
- en: '|  | Perceived Risks and Harms-Concerns about others finding out | Users concerned
    on their usage of AI being discovered by others because unsure others’ acceptance
    on using AI for certain tasks. |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '|  | 感知的风险与危害——对他人发现的担忧 | 用户担心自己的AI使用被他人发现，因为他们不确定他人对在某些任务中使用AI的接受度。 |'
- en: '|  | Perceived Risks and Harms-Concerns about idea theft | Users concerned
    on their ides such as business ideas, story ideas, or any unpublished works being
    theft by the people or companies behind the agents. |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '|  | 感知风险与危害——对创意被盗的担忧 | 用户担心他们的创意，如商业创意、故事创意或任何未公开的作品，可能会被代理背后的人或公司窃取。 |'
- en: '|  | Perceived Norms of Disclosure-Attitude towards disclosing others’ data
    and have one’s own data shared by others | Users hold different attitudes on sharing
    others’ data and have their own data shared by others have different intentions
    to disclosure (others’) personal information. |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '|  | 信息披露规范认知——对披露他人数据以及将自己的数据分享给他人持何种态度 | 用户对分享他人数据以及将自己的数据分享给他人持有不同的态度，且在披露（他人）个人信息时有不同的意图。
    |'
- en: '|  | Perceived Norms of Disclosure-Caution on sharing work-related data due
    to company policies or NDAs | Users normally are more cautious about sharing data
    from work because of the companies’ policy or NDAs. |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '|  | 信息披露规范认知——由于公司政策或保密协议对分享与工作相关的数据持谨慎态度 | 用户通常会更加谨慎地分享工作相关的数据，因为公司政策或保密协议的存在。
    |'
- en: '| How Users Navigate the Trade-off Between Disclosure Risks and Benefits |
    Accept Privacy Risks to Reap Benefits | For the benefits, some users choose to
    accept the privacy risks. |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| 用户如何在披露风险与利益之间做权衡 | 接受隐私风险以获取利益 | 为了获得利益，一些用户选择接受隐私风险。 |'
- en: '|  | Avoid Certain Tasks Completely Due to Privacy Concerns | Some users just
    avoid using LLM-based CAs for certain tasks because they don’t want to take the
    privacy risks on sharing certain information. |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '|  | 由于隐私问题完全避免某些任务 | 一些用户仅因不愿意承担分享某些信息的隐私风险，而选择避免使用基于大语言模型的对话代理进行某些任务。 |'
- en: '|  | Manually Sanitize Inputs-Censor and/or Falsify Sensitive Information |
    Some users choose to censor or falsify sensitive information when sharing with
    the agents for protecting their privacy as well as reaping benefits. |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '|  | 手动清理输入——审查和/或篡改敏感信息 | 一些用户在与代理共享信息时，会选择审查或篡改敏感信息，以保护他们的隐私并获取利益。 |'
- en: '|  | Manually Sanitize Inputs-Desensitize Input Copied from Other Contexts
    | Some users desensitize the input copied from other context for protecting their
    privacy as well as reaping benefits. |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '|  | 手动清理输入——对从其他上下文复制的输入进行脱敏处理 | 一些用户会对从其他上下文复制的输入进行脱敏处理，以保护自己的隐私并获取利益。 |'
- en: '|  | Manually Sanitize Inputs-Only Seek General Advice | Some people choose
    to only ask for general advice instead of personalized ones for avoid sharing
    personal or detailed information. |'
  id: totrans-697
  prefs: []
  type: TYPE_TB
  zh: '|  | 手动清理输入——仅寻求一般性建议 | 一些人选择仅寻求一般性建议，而非个性化建议，以避免分享个人或详细信息。 |'
- en: '[Table 4](https://arxiv.org/html/2309.11653v2#A4.T4 "Table 4 ‣ Appendix D Codebook
    for the interview results ‣ ”It’s a Fair Game”, or Is It? Examining How Users
    Navigate Disclosure Risks and Benefits When Using LLM-Based Conversational Agents")
    (continued)'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](https://arxiv.org/html/2309.11653v2#A4.T4 "表4 ‣ 附录D 访谈结果编码本 ‣ ''这是公平的游戏''，还是它？探讨用户在使用基于大语言模型的对话代理时如何在披露风险与利益之间做权衡")（续）'
- en: '| Theme | Code | Memo |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| 主题 | 代码 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-700
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Mental Models of How LLM-Based CAs Handle User Input | Response Generation-A:
    ChatGPT is magic | Users with Mental Model A have limited understanding on how
    the system generates responses and they regard the system as an AI “blackbox”
    or “magic” thing. |'
  id: totrans-701
  prefs: []
  type: TYPE_TB
  zh: '| 基于大语言模型的对话代理如何处理用户输入的心智模型 | 回应生成-A：ChatGPT 是魔法 | 拥有心智模型A的用户对系统如何生成回应的理解有限，他们将系统视为一个人工智能“黑箱”或“魔法”般的存在。
    |'
- en: '|  | Response Generation-B: ChatGPT is a super searcher | Users with Mental
    Model B think the responses generation process is seperated into different stages
    and components. AI is trained for certain stages such as search information from
    internet or databases, or for synthesizing results. |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '|  | 回应生成-B：ChatGPT 是一个超级搜索者 | 拥有心智模型B的用户认为回应生成过程是分为不同阶段和组成部分的。人工智能被训练用于某些阶段，如从互联网上或数据库中搜索信息，或合成结果。
    |'
- en: '|  | Response Generation-C: ChatGPT is a stochastic parrot | Users with Mental
    Model C have more comprehensive understanding on how the system generates responses.
    They know the system is based on an end-to-end ML model. |'
  id: totrans-703
  prefs: []
  type: TYPE_TB
  zh: '|  | 回应生成-C：ChatGPT 是一个随机鹦鹉 | 拥有心智模型C的用户更全面地理解系统是如何生成回应的。他们知道该系统基于端到端的机器学习模型。
    |'
- en: '|  | Improvement and Training-D: User input as a quality indicator | Users
    with Mental Model D think their input works as the quality indicator of the generated
    responses. They thought the users input used to train is to rate or give feedback
    so that decide what types of responses generated by the agents can be reused in
    the future. |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
  zh: '|  | 改进与训练-D：用户输入作为质量指标 | 拥有心理模型D的用户认为他们的输入作为生成响应的质量指标。他们认为，用于训练的用户输入是为了评定或反馈，以决定哪些类型的响应可以在未来被重复使用。
    |'
- en: '|  | Improvement and Training-E: User input is training data | Users with Mental
    Model E understand their input would be included into the training dataset. As
    the result, their input might be integrated into future responses. (While several
    people with this model thought they have their own personalized models) |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '|  | 改进与训练-E：用户输入作为训练数据 | 拥有心理模型E的用户理解他们的输入将被纳入训练数据集中。因此，他们的输入可能会被整合到未来的回答中。（虽然一些拥有此模型的用户认为他们有自己的个性化模型）
    |'
- en: '| Users’ Awareness and Reactions to Memorization Risks in LLM-based CAs | Having
    Encountered LLM Memorization Risks | Users said they have encountered memorization
    risks in their previous experience on other LLM-based application. |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
  zh: '| 用户对基于LLM的CA中记忆化风险的认知与反应 | 遇到LLM记忆化风险 | 用户表示在使用其他基于LLM的应用时曾遇到过记忆化风险。 |'
- en: '|  | Having Concerns About Leaking Original Writing | Some users spontaneously
    shown concerns on their original writing shared being memorized and leaked to
    others, which may harm to novelty or authorship of their writing. |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
  zh: '|  | 担心原创作品泄露 | 一些用户自发地对自己的原创作品被记忆化并泄露给他人表示担忧，认为这可能会损害其作品的新颖性或版权。 |'
- en: '|  | Unconcerned About Memorization Because Not Sharing Sensitive Data | Some
    users showed no concern on the memorization risks because they don’t think their
    sharing included any sensitive data. |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '|  | 不关心记忆化，因为不分享敏感数据 | 一些用户对记忆化风险不关心，因为他们认为自己的分享并未包含任何敏感数据。 |'
- en: '|  | Unconcerned About Memorization Because They Could Not Imagine the Problem
    | Some users showed no more concern on the memorization risks because they cannot
    understand and see how the memorization happens. |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '|  | 不关心记忆化，因为他们无法想象问题 | 一些用户对记忆化风险不再关心，因为他们无法理解并看到记忆化是如何发生的。 |'
- en: '|  | Unconcerned About Memorization Due to the Perceived Deniability of the
    AI-Generated Output | Some users showed no more concern on data being memorized
    because they thought if the AI generate information about them that were inaccurate,
    then these data make no sense for them. |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '|  | 不关心记忆化，因为认为AI生成的输出具有可否认性 | 一些用户对数据被记忆化不再关心，因为他们认为如果AI生成关于他们的不准确信息，那么这些数据对他们没有意义。
    |'
- en: '| Users’ Awareness, Understanding, and Attitudes About Privacy-Protective Support
    | Users Lack Awareness of Existing Privacy-Protective Support | Users never thought
    about there would be some privacy-protective methods such as opt-out or using
    OpenAI API playground that they can use to protect their privacy meanwhile reap
    benefits from AI. |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| 用户对隐私保护支持的认知、理解与态度 | 用户缺乏对现有隐私保护支持的认知 | 用户从未想过会有一些隐私保护方法，如选择退出或使用OpenAI API
    Playground，能够在保护隐私的同时从AI中获得好处。 |'
- en: '|  | Dark Patterns Impeded Adoption of the Opt-out Control | There’s dark patterns
    related to Opt-out control which restrains users’ usage on the Opt-out to do data
    control. |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '|  | 黑暗模式阻碍了选择退出控制的采用 | 与选择退出控制相关的黑暗模式限制了用户对数据控制的选择退出使用。 |'
- en: '|  | Users Anticipated More Granular Opt-out Control | Users hoped they can
    have more granular on the opt-out control |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '|  | 用户期待更细粒度的选择退出控制 | 用户希望能拥有更细粒度的选择退出控制。 |'
