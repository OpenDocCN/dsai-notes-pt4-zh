- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P7ï¼š7.Self Attention and Non-parametric transformers
    (NPTs) - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPT/Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P7ï¼š7.è‡ªæ³¨æ„åŠ›ä¸éå‚æ•°å˜å‹å™¨ (NPTs) - life_code - BV1X84y1Q7wV
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_0.png)'
- en: Thanks so much great to be here and happy Halloween related Halloween everyone
    so I think the talk is going to be split into two sections so I'll start by spending
    like 10 minutes 15 minutes chatting about transformers in general but I'm assuming
    most of you are familiar with them and we can move on to MPTs which Ya and Neil
    will be presentingã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸æ„Ÿè°¢ï¼Œå¾ˆé«˜å…´æ¥åˆ°è¿™é‡Œï¼Œç¥å¤§å®¶ä¸‡åœ£èŠ‚å¿«ä¹ã€‚æˆ‘è®¤ä¸ºè¿™æ¬¡æ¼”è®²å°†åˆ†ä¸ºä¸¤ä¸ªéƒ¨åˆ†ï¼Œæ‰€ä»¥æˆ‘ä¼šå…ˆèŠ±å¤§çº¦ 10 åˆ° 15 åˆ†é’Ÿè®¨è®ºå˜å‹å™¨çš„ä¸€èˆ¬æƒ…å†µï¼Œä½†æˆ‘å‡è®¾ä½ ä»¬å¤§å¤šæ•°äººéƒ½ç†Ÿæ‚‰å®ƒä»¬ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¨è®º
    MPTï¼ŒYa å’Œ Neil ä¼šåšä»‹ç»ã€‚
- en: ğŸ˜Šã€‚![](img/1777b223f7bf8be27a8767d209c7f271_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/1777b223f7bf8be27a8767d209c7f271_2.png)
- en: So let's seeï¼Œ I'm going likeã€‚Try to fly through the transformer overview and
    maybe spendã€‚A little bit extra time on like the history of transformers and maybe
    just tell the story a little bitã€‚I think that might be more interestingã€‚å—¯ã€‚So just
    in terms of the transformer architectureã€‚the two kind of things that it introduced
    for the first time were multihead attention and selfat and then it combined those
    with fast utter aggressive decoding so before the Transer pretty much everyone
    was using LSTMs and LSTMs with attention I'll try to get into the difference of
    selfat multihead attentionã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œæˆ‘æ‰“ç®—å¿«é€Ÿä»‹ç»ä¸€ä¸‹å˜å‹å™¨çš„æ¦‚å†µï¼Œå¹¶å¯èƒ½åœ¨å˜å‹å™¨çš„å†å²ä¸ŠèŠ±ä¸€ç‚¹é¢å¤–çš„æ—¶é—´ï¼Œæˆ–è€…ç¨å¾®è®²è¿°ä¸€ä¸‹è¿™ä¸ªæ•…äº‹ã€‚æˆ‘è®¤ä¸ºè¿™å¯èƒ½ä¼šæ›´æœ‰è¶£ã€‚å—¯ã€‚å°±å˜å‹å™¨æ¶æ„è€Œè¨€ï¼Œå®ƒé¦–æ¬¡å¼•å…¥çš„ä¸¤ç§ä¸»è¦æ¦‚å¿µæ˜¯å¤šå¤´æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ï¼Œç„¶åå°†è¿™äº›ä¸å¿«é€Ÿè‡ªå›å½’è§£ç ç»“åˆåœ¨ä¸€èµ·ã€‚æ‰€ä»¥åœ¨å˜å‹å™¨ä¹‹å‰ï¼Œå‡ ä¹æ¯ä¸ªäººéƒ½åœ¨ä½¿ç”¨
    LSTM åŠå…¶å¸¦æ³¨æ„åŠ›çš„ LSTMï¼Œæˆ‘ä¼šå°½é‡è§£é‡Šè‡ªæ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ä¹‹é—´çš„åŒºåˆ«ã€‚
- en: So originally you would have two sequences and you would have a attention module
    which would attend from the source to the target and so each token or each word
    in the source sequence would get associated with you know a soft approximation
    of one element in the target sequenceã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æœ€åˆä½ ä¼šæœ‰ä¸¤ä¸ªåºåˆ—ï¼Œç„¶åä½ ä¼šæœ‰ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å—ï¼Œå®ƒä¼šä»æºåºåˆ—å…³æ³¨åˆ°ç›®æ ‡åºåˆ—ï¼Œå› æ­¤æºåºåˆ—ä¸­çš„æ¯ä¸ªä»¤ç‰Œæˆ–æ¯ä¸ªè¯éƒ½ä¼šä¸ç›®æ ‡åºåˆ—ä¸­çš„ä¸€ä¸ªå…ƒç´ çš„è½¯è¿‘ä¼¼ç›¸å…³è”ã€‚
- en: And so you'd end up with something like like thisï¼Œ but with self attentionã€‚we
    did away with the two separate sequencesï¼Œ we make them both the sameã€‚and so you're
    relating each element within the sequence to another element in the sequenceã€‚And
    so theã€‚The idea here is that you're learning a relationship of the words within
    a sentence to the other wordsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæœ€ç»ˆå¾—åˆ°çš„ç»“æœåƒè¿™æ ·ï¼Œä½†å…·æœ‰è‡ªæ³¨æ„åŠ›ã€‚æˆ‘ä»¬ä¸å†ä½¿ç”¨ä¸¤ä¸ªå•ç‹¬çš„åºåˆ—ï¼Œè€Œæ˜¯è®©å®ƒä»¬ç›¸åŒã€‚è¿™æ ·ï¼Œä½ å°±å¯ä»¥åœ¨åºåˆ—ä¸­å°†æ¯ä¸ªå…ƒç´ ä¸å¦ä¸€ä¸ªå…ƒç´ å…³è”èµ·æ¥ã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ä½ åœ¨å­¦ä¹ å¥å­ä¸­è¯è¯­ä¹‹é—´çš„å…³ç³»ã€‚
- en: so you can imagine something like an adjective which is being applied to a noun
    and so you want to relate that adjective like the blue ballã€‚you want to relate
    blue as referring to ball through learning patterns within the sequence interest
    sequence patternsã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥æƒ³è±¡æœ‰ä¸€ä¸ªå½¢å®¹è¯è¢«åº”ç”¨äºä¸€ä¸ªåè¯ï¼Œå› æ­¤ä½ æƒ³è¦å°†è¿™ä¸ªå½¢å®¹è¯ä¸åè¯å…³è”èµ·æ¥ï¼Œæ¯”å¦‚â€œè“è‰²çš„çƒâ€ã€‚ä½ æƒ³é€šè¿‡å­¦ä¹ åºåˆ—ä¸­çš„æ¨¡å¼å°†â€œè“è‰²â€ä¸â€œçƒâ€å…³è”èµ·æ¥ã€‚
- en: å—¯ã€‚So sorry I gave this talk in Kenyaï¼Œ so I' am using Kewa Heley hereï¼Œ but with
    multihead attentionã€‚the idea is you have like each word represented by an embedding
    which is in the depth dimension here and then you have your sentence of wordsã€‚you
    split that up into a bunch of different groupsã€‚so here I've droppedpped it depthwise
    into four groupsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æŠ±æ­‰ï¼Œæˆ‘åœ¨è‚¯å°¼äºšåšè¿‡è¿™ä¸ªæ¼”è®²ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œä½¿ç”¨ Kewa Heleyï¼Œä½†ä½¿ç”¨å¤šå¤´æ³¨æ„åŠ›ã€‚è¿™ä¸ªæ¦‚å¿µæ˜¯æ¯ä¸ªè¯éƒ½ç”¨ä¸€ä¸ªåµŒå…¥è¡¨ç¤ºï¼Œåœ¨è¿™é‡Œæ˜¯æ·±åº¦ç»´åº¦ï¼Œç„¶åä½ æœ‰ä¸€ä¸ªå•è¯çš„å¥å­ã€‚ä½ æŠŠå®ƒåˆ†æˆå‡ ç»„ã€‚æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘åœ¨æ·±åº¦ä¸Šåˆ†æˆäº†å››ç»„ã€‚
- en: You apply attention to each one of these groups independently and then when
    you get the result back you can catnate them together and you're back to your
    model dimension representationã€‚What this lets you do is if each attention like
    each attention head can now focus on learning one patternã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯¹æ¯ä¸€ç»„ç‹¬ç«‹åº”ç”¨æ³¨æ„åŠ›ï¼Œå½“ä½ å¾—åˆ°ç»“æœåï¼Œå¯ä»¥å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œå›åˆ°ä½ çš„æ¨¡å‹ç»´åº¦è¡¨ç¤ºã€‚è¿™è®©ä½ èƒ½å¤Ÿåšåˆ°çš„æ˜¯ï¼Œå¦‚æœæ¯ä¸ªæ³¨æ„åŠ›å¤´ç°åœ¨å¯ä»¥ä¸“æ³¨äºå­¦ä¹ ä¸€ç§æ¨¡å¼ã€‚
- en: so maybe attention head one is learning the relationship of adjectives to nouns
    and the second attention head can learn something different so this lets us learn
    like a hierarchy or a list of different relationshipsã€‚Okayï¼Œ so that was self attentionã€‚The
    other piece is fast auto oppressive decodingã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¹Ÿè®¸æ³¨æ„åŠ›å¤´ä¸€æ­£åœ¨å­¦ä¹ å½¢å®¹è¯ä¸åè¯ä¹‹é—´çš„å…³ç³»ï¼Œè€Œç¬¬äºŒä¸ªæ³¨æ„åŠ›å¤´å¯ä»¥å­¦ä¹ ä¸€äº›ä¸åŒçš„ä¸œè¥¿ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å­¦ä¹ ä¸€ä¸ªå±‚æ¬¡æˆ–ä¸åŒå…³ç³»çš„åˆ—è¡¨ã€‚å¥½çš„ï¼Œè¿™å°±æ˜¯è‡ªæ³¨æ„åŠ›ã€‚å¦ä¸€ä¸ªéƒ¨åˆ†æ˜¯å¿«é€Ÿè‡ªå›å½’è§£ç ã€‚
- en: And do I really want to go into thisï¼ŸOkayï¼Œ I willï¼Œ so the important thing about
    this is itã€‚If you're doing normal autoaggressive decodingï¼Œ what you do is you
    generate your first token and now conditioned on that first token you generate
    the second and condition on the first two you generate the third and so on and
    so forthã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çœŸçš„æƒ³æ·±å…¥æ¢è®¨è¿™ä¸ªå—ï¼Ÿå¥½å§ï¼Œæˆ‘ä¼šçš„ï¼Œé‡è¦çš„æ˜¯ï¼Œå¦‚æœä½ åœ¨è¿›è¡Œæ­£å¸¸çš„è‡ªå›å½’è§£ç ï¼Œä½ ä¼šç”Ÿæˆç¬¬ä¸€ä¸ªæ ‡è®°ï¼Œç„¶ååŸºäºç¬¬ä¸€ä¸ªæ ‡è®°ç”Ÿæˆç¬¬äºŒä¸ªæ ‡è®°ï¼ŒåŸºäºå‰ä¸¤ä¸ªç”Ÿæˆç¬¬ä¸‰ä¸ªï¼Œä¾æ­¤ç±»æ¨ã€‚
- en: but that's super slow right like it's a loop applying this thing again and again
    and so what we can do instead is we make an assumption in the code that our model
    always generates the right thing and we generate and then we generate a predictionã€‚Only
    one token ahead and so the way that this looks isã€‚You okay soã€‚hyã€‚Why hat hereï¼ŸSorry
    once againã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™è¶…çº§æ…¢ï¼Œå¯¹å§ï¼Œå°±åƒä¸€ä¸ªå¾ªç¯ä¸€æ¬¡åˆä¸€æ¬¡åœ°åº”ç”¨è¿™ä¸ªä¸œè¥¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥åšçš„æ˜¯åœ¨ä»£ç ä¸­å‡è®¾æˆ‘ä»¬çš„æ¨¡å‹å§‹ç»ˆç”Ÿæˆæ­£ç¡®çš„å†…å®¹ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªé¢„æµ‹ï¼Œä»…ä»…æå‰ä¸€ä¸ªæ ‡è®°ï¼Œæ‰€ä»¥è¿™ä¸ªè¿‡ç¨‹çœ‹èµ·æ¥æ˜¯è¿™æ ·çš„ã€‚ä½ è¿˜å¥½å—ï¼Ÿå—¯ï¼Œä¸ºä»€ä¹ˆè¿™é‡Œæœ‰ä¸ªå¸½å­ï¼ŸæŠ±æ­‰ï¼Œå†ä¸€æ¬¡ã€‚
- en: Input to outputï¼Œ so you have like your outputsï¼Œ which are yï¼Œ you have your targetsï¼Œ
    which are Y hatã€‚And what you do is you feed in those gold targets so that you
    don't need to actually do this loopã€‚so instead of assuming instead of having to
    generate the first token feed it back into your architectureã€‚generate a second
    tokenï¼Œ you feed in the entire target sequence and you just pretend that you generate
    all the right tokens up to position k and then you predict the K plus first and
    you compute your loss on thatã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è¾“å…¥åˆ°è¾“å‡ºï¼Œä½ æœ‰ä½ çš„è¾“å‡ºï¼Œå³ yï¼Œä½ æœ‰ä½ çš„ç›®æ ‡ï¼Œå³ Y hatã€‚ä½ æ‰€åšçš„æ˜¯è¾“å…¥é‚£äº›é»„é‡‘ç›®æ ‡ï¼Œè¿™æ ·ä½ å°±ä¸éœ€è¦å®é™…æ‰§è¡Œè¿™ä¸ªå¾ªç¯ã€‚å› æ­¤ï¼Œä¸å†å‡è®¾ç”Ÿæˆç¬¬ä¸€ä¸ªæ ‡è®°åå°†å…¶åé¦ˆåˆ°æ¶æ„ä¸­ï¼Œç”Ÿæˆç¬¬äºŒä¸ªæ ‡è®°ï¼Œè€Œæ˜¯è¾“å…¥æ•´ä¸ªç›®æ ‡åºåˆ—ï¼Œå¹¶å‡è£…ç”Ÿæˆæ‰€æœ‰æ­£ç¡®çš„æ ‡è®°ï¼Œç›´åˆ°ä½ç½®
    kï¼Œç„¶åé¢„æµ‹ K+1ï¼Œå¹¶è®¡ç®—è¯¥ä½ç½®çš„æŸå¤±ã€‚
- en: So in reality your model might have generated you know at the beginning of training
    junkã€‚but you're getting a loss as if your model had seen all the correct tokens
    and is now just predicting the next one this is a little bit subtle but it's hugely
    impactful for training speed because all of this can be done un in parallel and
    so it's actually what makes transformers so scalableã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œä½ çš„æ¨¡å‹å¯èƒ½åœ¨è®­ç»ƒå¼€å§‹æ—¶ç”Ÿæˆäº†ä¸€äº›æ— ç”¨çš„å†…å®¹ï¼Œä½†ä½ å¾—åˆ°çš„æŸå¤±å´ä»¿ä½›æ¨¡å‹å·²ç»çœ‹åˆ°äº†æ‰€æœ‰æ­£ç¡®çš„æ ‡è®°ï¼Œç°åœ¨åªæ˜¯åœ¨é¢„æµ‹ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œè¿™ä¸€ç‚¹æœ‰äº›å¾®å¦™ï¼Œä½†å¯¹è®­ç»ƒé€Ÿåº¦å½±å“å·¨å¤§ï¼Œå› ä¸ºæ‰€æœ‰è¿™äº›éƒ½å¯ä»¥å¹¶è¡Œè¿›è¡Œï¼Œè¿™æ­£æ˜¯è®©å˜å‹å™¨æ¨¡å‹å¦‚æ­¤å¯æ‰©å±•çš„åŸå› ã€‚
- en: Okayï¼Œ so in order to do this successfullyï¼Œ if you were just feeding in all of
    theã€‚All of the correct tokens naivelyï¼Œ what would happen is your model would just
    be able to look forward in time and cheatã€‚So you've put in all of your true targetsï¼Œ
    the things that you're trying to get your model to predict and so if that's where
    you're computing your loss onã€‚it could just look forward in time and sayï¼Œ okay
    I'm just going to grab that and it would get zero error trivially right because
    you've given it all the right answers so what we have to do inside the architecture
    is we need to actually prevent the attention mechanism from being able to look
    at tokens that it shouldn't have been able to see already so the way that this
    looks is you create a mask on your attentionã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œä¸ºäº†æˆåŠŸåšåˆ°è¿™ä¸€ç‚¹ï¼Œå¦‚æœä½ åªæ˜¯ç®€å•åœ°è¾“å…¥æ‰€æœ‰æ­£ç¡®çš„æ ‡è®°ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿä½ çš„æ¨¡å‹å°†èƒ½å¤Ÿå‘å‰çœ‹å¹¶â€œä½œå¼Šâ€ã€‚ä½ è¾“å…¥äº†æ‰€æœ‰çš„çœŸå®ç›®æ ‡ï¼Œä¹Ÿå°±æ˜¯ä½ å¸Œæœ›æ¨¡å‹é¢„æµ‹çš„å†…å®¹ã€‚å¦‚æœè¿™æ˜¯ä½ è®¡ç®—æŸå¤±çš„ä¾æ®ï¼Œå®ƒå°±èƒ½å‘å‰çœ‹å¹¶è¯´ï¼Œå¥½çš„ï¼Œæˆ‘åªéœ€æŠ“å–é‚£ä¸ªï¼Œå®ƒå°±ä¼šè½»æ¾è·å¾—é›¶é”™è¯¯ï¼Œå› ä¸ºä½ ç»™äº†å®ƒæ‰€æœ‰æ­£ç¡®çš„ç­”æ¡ˆã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ¶æ„å†…éƒ¨å®é™…é˜²æ­¢æ³¨æ„åŠ›æœºåˆ¶æŸ¥çœ‹å®ƒä¸åº”è¯¥å·²ç»çœ‹åˆ°çš„æ ‡è®°ã€‚è¿™ä¸ªè¿‡ç¨‹çš„æ–¹å¼æ˜¯åˆ›å»ºä¸€ä¸ªæ³¨æ„åŠ›æ©ç ã€‚
- en: å—¯ã€‚ğŸ˜Šï¼ŒAnd so sorry this is the example of like doing a trivial attention if you
    don't mask your attention properly what it's going to do is it's just going to
    look into the future just grab the token that you're telling it to predict and
    copy it over and so it learn something trivial something that doesn't actually
    generalize and so what we do is weã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æŠ±æ­‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ³¨æ„åŠ›ç¤ºä¾‹ï¼Œå¦‚æœä½ æ²¡æœ‰æ­£ç¡®åœ°æ©ç›–ä½ çš„æ³¨æ„åŠ›ï¼Œå®ƒå°†åªä¼šå‘æœªæ¥çœ‹ï¼Œç›´æ¥æŠ“å–ä½ å‘Šè¯‰å®ƒé¢„æµ‹çš„æ ‡è®°å¹¶å¤åˆ¶è¿‡æ¥ï¼Œå› æ­¤å®ƒå­¦åˆ°çš„ä¸œè¥¿æ˜¯ç®€å•çš„ï¼Œä¸ä¼šçœŸæ­£æ¨å¹¿ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ‰€åšçš„æ˜¯ã€‚
- en: Actually prevent it from attending to those tokensï¼Œ we prevent it from attending
    into the futureã€‚For each position in the source sequenceï¼Œ we block out everything
    that it shouldn't be able to seeã€‚everything into the futureï¼Œ and then as we move
    down we gradually unblock so it can start to see into the pastã€‚å—¯ã€‚So those are
    kind of like two the three major components of transformersï¼Œ the self attentionã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œä¸ºäº†é˜²æ­¢æ¨¡å‹å…³æ³¨é‚£äº›æ ‡è®°ï¼Œæˆ‘ä»¬é˜»æ­¢å®ƒå…³æ³¨æœªæ¥çš„å†…å®¹ã€‚å¯¹äºæºåºåˆ—ä¸­çš„æ¯ä¸ªä½ç½®ï¼Œæˆ‘ä»¬å±è”½äº†æ‰€æœ‰å®ƒä¸åº”è¯¥çœ‹åˆ°çš„å†…å®¹ã€‚æ‰€æœ‰æœªæ¥çš„å†…å®¹ï¼Œç„¶åéšç€æˆ‘ä»¬å‘ä¸‹ç§»åŠ¨ï¼Œé€æ¸è§£é™¤å±è”½ï¼Œè®©å®ƒå¯ä»¥å¼€å§‹çœ‹åˆ°è¿‡å»ã€‚å—¯ã€‚æ‰€ä»¥è¿™äº›å°±åƒæ˜¯å˜å‹å™¨çš„ä¸‰ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ä¹‹ä¸€ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: the multihead attentionï¼Œ and then deploying this gold targets decoding fast
    utter restive decodingã€‚å—¯ã€‚In terms of the storyï¼Œ which might be a little bit more
    interestingã€‚å—¯ã€‚So transformersã€‚I was an intern with Lukash Kaiser at Google back
    in 2017ã€‚and I was sitting next to Nome and Sheish was like a couple seats down
    from usã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå¤´æ³¨æ„åŠ›ï¼Œä»¥åŠå¿«é€Ÿè§£ç é»„é‡‘ç›®æ ‡çš„éƒ¨ç½²ã€‚å—¯ã€‚åœ¨æ•…äº‹ä¸Šï¼Œè¿™å¯èƒ½æ›´æœ‰è¶£ã€‚å—¯ã€‚å˜å‹å™¨ã€‚2017å¹´æˆ‘åœ¨è°·æ­Œæ—¶æ˜¯Lukash Kaiserçš„å®ä¹ ç”Ÿï¼Œæˆ‘ååœ¨Noamæ—è¾¹ï¼ŒSheishåœ¨æˆ‘ä»¬å‡ æ’ä¹‹å¤–ã€‚
- en: And what's really incredible is that essentially this entire project came together
    in like three monthsã€‚And it was done so I showed up at Google Noam had been working
    onã€‚Autoag modelsã€‚same thing with like Ashishish and Yaakov and Nikki and they'd
    just been kind of like exploring the space figuring it out and Luksh and I at
    the same time we had been working on this framework called Tensor to Tensorã€‚å—¯ã€‚Which
    was like explicitly made for multimodal learning autoreive learning and lukash
    is kind of like a master ofã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸”ä»¤äººéš¾ä»¥ç½®ä¿¡çš„æ˜¯ï¼Œè¿™ä¸ªé¡¹ç›®åŸºæœ¬ä¸Šåœ¨ä¸‰ä¸ªæœˆå†…å®Œæˆã€‚æˆ‘åœ¨è°·æ­Œæ—¶ï¼ŒNoamä¸€ç›´åœ¨ç ”ç©¶Autoagæ¨¡å‹ï¼ŒAshishã€Yaakovå’ŒNikkiä¹Ÿæ˜¯ï¼Œä»–ä»¬ä¸€ç›´åœ¨æ¢ç´¢è¿™ä¸ªé¢†åŸŸã€‚ä¸æ­¤åŒæ—¶ï¼ŒLukashå’Œæˆ‘åœ¨å·¥ä½œä¸€ä¸ªåä¸ºTensor
    to Tensorçš„æ¡†æ¶ã€‚å—¯ã€‚è¿™ä¸ªæ¡†æ¶æ˜¯ä¸“é—¨ä¸ºå¤šæ¨¡æ€å­¦ä¹ å’Œè‡ªå›å½’å­¦ä¹ è€Œåˆ›å»ºçš„ï¼ŒLukashåœ¨è¿™ä¸€é¢†åŸŸæ˜¯ä¸ªé«˜æ‰‹ã€‚
- en: Keeping track of everything that's happening in the field and adopting itã€‚and
    so within tensor to tensorï¼Œ there were like theseã€‚There were like these kind of
    emerging little things that maybe one paper had been written about and people
    were interested in like layerormã€‚but it hadn't actually taken off yet the warmup
    in the learning rate schedule all of these little pieces were just default like
    on by defaultã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è·Ÿè¸ªé¢†åŸŸä¸­å‘ç”Ÿçš„æ‰€æœ‰äº‹æƒ…å¹¶åŠ ä»¥é‡‡ç”¨ã€‚å› æ­¤ï¼Œåœ¨tensor to tensorä¸­ï¼Œæœ‰è¿™äº›ã€‚å¯èƒ½ä¸€ç¯‡è®ºæ–‡æåˆ°è¿‡çš„ä¸€äº›æ–°å…´å°ä¸œè¥¿ï¼Œåƒlayer normï¼Œä½†å…¶å®è¿˜æ²¡æœ‰æµè¡Œèµ·æ¥ï¼Œå­¦ä¹ ç‡é¢„çƒ­çš„æ‰€æœ‰è¿™äº›å°éƒ¨åˆ†éƒ½æ˜¯é»˜è®¤å¼€å¯çš„ã€‚
- en: and so whennome and aishish and the Nikki and Yaakï¼ŒCame over and adopted tensor
    to tensorã€‚all of these things were just on by defaultã€‚And so a lot of peopleã€‚when
    they look at the transformer paperï¼Œ it just seems like there's so many like arbitrary
    little things thrown in and when now like in present day these have become standard
    for like a lot of different training algorithms like the learning rate warm upã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å½“Noamã€Ashishã€Nikkiå’ŒYaakè¿‡æ¥å¹¶é‡‡ç”¨tensor to tensoræ—¶ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½æ˜¯é»˜è®¤å¼€å¯çš„ã€‚å› æ­¤å¾ˆå¤šäººï¼Œå½“ä»–ä»¬æŸ¥çœ‹å˜å‹å™¨è®ºæ–‡æ—¶ï¼Œä¼¼ä¹æœ‰å¾ˆå¤šä»»æ„çš„å°ä¸œè¥¿è¢«åŠ å…¥ï¼Œè€Œç°åœ¨è¿™äº›å·²ç»æˆä¸ºè®¸å¤šä¸åŒè®­ç»ƒç®—æ³•çš„æ ‡å‡†ï¼Œæ¯”å¦‚å­¦ä¹ ç‡é¢„çƒ­ã€‚
- en: The way that we did initializationï¼Œ all of these pieces have just become the
    normã€‚but back then they had like have just been introducedã€‚And soã€‚We we spent
    a lot of time running ablations trying to figure out like which were the necessary
    pieces and what made it work and if any of you have actually tried training transformers
    and tried like pulling out the learning rate warmup or changing any of these little
    piecesã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åˆå§‹åŒ–çš„æ–¹å¼ï¼Œæ‰€æœ‰è¿™äº›éƒ¨åˆ†éƒ½å·²æˆä¸ºè§„èŒƒã€‚ä½†é‚£æ—¶å®ƒä»¬åˆšåˆšè¢«å¼•å…¥ã€‚å› æ­¤ï¼Œæˆ‘ä»¬èŠ±äº†å¾ˆå¤šæ—¶é—´è¿›è¡Œæ¶ˆèå®éªŒï¼Œè¯•å›¾å¼„æ¸…æ¥šå“ªäº›æ˜¯å¿…è¦çš„éƒ¨åˆ†ï¼Œæ˜¯ä»€ä¹ˆè®©å®ƒæœ‰æ•ˆçš„ã€‚å¦‚æœä½ ä»¬ä¸­çš„ä»»ä½•äººå°è¯•è¿‡è®­ç»ƒå˜å‹å™¨ï¼Œå¹¶è¯•å›¾å»æ‰å­¦ä¹ ç‡é¢„çƒ­æˆ–æ›´æ”¹ä»»ä½•è¿™äº›å°éƒ¨åˆ†ã€‚
- en: you'll see that it really does break down optimization like it actually really
    does hurt performanceã€‚For instanceï¼Œ like removing the layer armsï¼Œ that type of
    thingã€‚å—¯ã€‚So I always thought it was kind of funny howã€‚All of these random editions
    that Luash had just like thrown in because he was playing around with them turned
    out to be crucial and they were just on by defaultã€‚å—¯ã€‚So anywayï¼Œ it was like three
    monthsï¼Œ I rememberã€‚It all really started coming together towards the endã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šå‘ç°ï¼Œå®ƒç¡®å®åœ¨ä¼˜åŒ–ä¸Šäº§ç”Ÿäº†å½±å“ï¼Œå®é™…ä¸Šç¡®å®ä¼šå½±å“æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œå»æ‰layer normï¼Œç±»ä¼¼çš„äº‹æƒ…ã€‚å—¯ã€‚æˆ‘æ€»è§‰å¾—å¾ˆæœ‰è¶£ï¼ŒLukashéšæ„æ·»åŠ çš„æ‰€æœ‰è¿™äº›éšæœºåŠŸèƒ½ï¼Œç»“æœå´æ˜¯è‡³å…³é‡è¦çš„ï¼Œéƒ½æ˜¯é»˜è®¤å¼€å¯çš„ã€‚å—¯ã€‚æ‰€ä»¥ï¼Œæ— è®ºå¦‚ä½•ï¼Œæˆ‘è®°å¾—æ˜¯åœ¨ä¸‰ä¸ªæœˆå†…ï¼Œä¸€åˆ‡çœŸçš„å¼€å§‹åœ¨æœ€åé˜¶æ®µæ±‡èšã€‚
- en: like just before the Nup deadlineã€‚And I can still remember sitting in the micro
    kitchen and a sheish telling me like as just like I was a little intern telling
    me like this is going to be such a big deal and I was like yeah sure okay like
    I have no idea what's happening I just showed upã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åœ¨Nupæˆªæ­¢æ—¥æœŸä¹‹å‰ã€‚æˆ‘ä»ç„¶è®°å¾—ååœ¨å¾®å‹å¨æˆ¿ï¼ŒæŸä¸ªåŒäº‹å‘Šè¯‰æˆ‘ï¼Œåƒæˆ‘è¿™æ ·çš„å°å®ä¹ ç”Ÿï¼Œè·Ÿæˆ‘è¯´è¿™å°†æ˜¯ä¸€ä¸ªå¤§äº‹ä»¶ï¼Œæˆ‘å½“æ—¶å¿ƒæƒ³ï¼Œå¥½çš„ï¼Œæˆ‘çœŸçš„ä¸çŸ¥é“å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæˆ‘åªæ˜¯å‡ºç°äº†ã€‚
- en: And he was likeï¼Œ no dudeï¼Œ like this this actually matters like you knowã€‚we bumped
    up blue three points and I was like sickï¼Œ great anywayã€‚å—¯ã€‚ğŸ˜Šã€‚And then I can remember
    on the night of the deadline for nervesã€‚It was like 2 am a shishã€‚Sheish was the
    only one left at the office and we were still like moving around figures and like
    adjusting things and then I went to bed but she stayed up and I slept in like
    this tiny little phone boothã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–å½“æ—¶è¯´ï¼Œä¸ï¼Œä¼™è®¡ï¼Œè¿™å®é™…ä¸Šå¾ˆé‡è¦ï¼Œä½ çŸ¥é“å—ã€‚æˆ‘ä»¬æå‡äº†è“è‰²ä¸‰åˆ†ï¼Œæˆ‘å¿ƒæƒ³å¤ªå¥½äº†ï¼Œéšä¾¿å§ã€‚å—¯ã€‚ğŸ˜Šã€‚ç„¶åæˆ‘è®°å¾—åœ¨æäº¤æˆªæ­¢æ—¥æœŸçš„å‰ä¸€æ™šã€‚é‚£æ—¶æ˜¯å‡Œæ™¨ä¸¤ç‚¹ï¼ŒæŸä¸ªåŒäº‹æ˜¯åŠå…¬å®¤é‡Œæœ€åä¸€ä¸ªç•™ä¸‹çš„äººï¼Œæˆ‘ä»¬ä»åœ¨ç§»åŠ¨æ•°æ®å’Œè°ƒæ•´ä¸œè¥¿ï¼Œç„¶åæˆ‘å»ç¡è§‰äº†ï¼Œä½†å¥¹ç»§ç»­ç†¬å¤œï¼Œè€Œæˆ‘åˆ™åœ¨ä¸€ä¸ªå°å°çš„ç”µè¯äº­é‡Œç¡è§‰ã€‚
- en: And then for the other paper that I was submittingï¼Œ I forgot to press submitï¼Œ
    but luckilyã€‚Like some lady opened the door to the phone booth and hit me in the
    head while I was sleeping in the morning and just before the deadline I got the
    paper in and so I owe it to that lady for submitting to N that year but yeah anyway
    the I think the crazy thing about transformers was that it all came together in
    like three months like most of the ideas happened in that span and it was just
    like this sprint towards the N deadlineã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æˆ‘æäº¤çš„å¦ä¸€ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘å¿˜è®°æŒ‰æäº¤ï¼Œä½†å¹¸è¿çš„æ˜¯ã€‚æœ‰ä½å¥³å£«åœ¨æ—©ä¸Šæ‰“å¼€ç”µè¯äº­çš„é—¨ï¼Œæ‰“äº†æˆ‘ä¸€ä¸‹å¤´ï¼Œå°±åœ¨æˆªæ­¢æ—¥æœŸä¹‹å‰ï¼Œæˆ‘æŠŠè®ºæ–‡æäº¤äº†ï¼Œæ‰€ä»¥æˆ‘å¾—æ„Ÿè°¢é‚£ä½å¥³å£«é‚£å¹´æäº¤åˆ°Nï¼Œä¸è¿‡æ²¡å…³ç³»ï¼Œæˆ‘è®¤ä¸ºå…³äºtransformersçš„ç–¯ç‹‚ä¹‹å¤„åœ¨äºï¼Œæ‰€æœ‰çš„äº‹æƒ…éƒ½åœ¨ä¸‰ä¸ªæœˆå†…èšé›†åœ¨ä¸€èµ·ï¼Œç»å¤§å¤šæ•°æƒ³æ³•éƒ½æ˜¯åœ¨é‚£æ®µæ—¶é—´å‘ç”Ÿçš„ï¼Œè¿™å°±åƒæ˜¯æœç€Næˆªæ­¢æ—¥æœŸçš„å†²åˆºã€‚
- en: Umï¼Œ and I think a lot of the other members on the teamã€‚Yaka Lukaash she they
    knew how important it wasï¼Œ but for me I was likeã€‚I don't knowã€‚I really did not
    appreciate the impactï¼Œ but in retrospect it's been amazing how the community has
    kind of like come together and adopted itã€‚ğŸ˜Šï¼ŒAnd I think most of that can be ad
    to the ease of optimizationã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œæˆ‘è®¤ä¸ºå›¢é˜Ÿä¸­çš„å…¶ä»–æˆå‘˜ã€‚Yaka Lukaashå¥¹ä»¬çŸ¥é“è¿™æœ‰å¤šé‡è¦ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œæˆ‘æ˜¯ã€‚ æˆ‘ä¸çŸ¥é“ã€‚æˆ‘çœŸçš„æ²¡æœ‰æ„è¯†åˆ°å®ƒçš„å½±å“ï¼Œä½†å›æƒ³èµ·æ¥ï¼Œç¤¾åŒºæ˜¯å¦‚ä½•å›¢ç»“åœ¨ä¸€èµ·å¹¶é‡‡çº³å®ƒçš„ï¼ŒçœŸæ˜¯å¤ªæƒŠäººäº†ã€‚ğŸ˜Šæˆ‘è®¤ä¸ºè¿™å¤§éƒ¨åˆ†å½’åŠŸäºä¼˜åŒ–çš„ä¾¿åˆ©æ€§ã€‚
- en: it seems like very robust to hyperparameter choices so you don't need to like
    tune the hell out of itã€‚spend a lot of time tweaking little thingsã€‚And the other
    side is that it's like super tailored to the accelerators that we run onã€‚So it's
    like very paralyzableï¼Œ hyper efficientï¼Œ and so it lends itself to that kind of
    scaling law effort that's really taken off in popularityã€‚Okayï¼Œ unless there are
    any questions thatã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼¼ä¹å¯¹è¶…å‚æ•°é€‰æ‹©éå¸¸ç¨³å¥ï¼Œæ‰€ä»¥ä½ ä¸éœ€è¦èŠ±è´¹å¾ˆå¤šæ—¶é—´å»è°ƒæ•´ç»†èŠ‚ã€‚è€Œå¦ä¸€æ–¹é¢ï¼Œå®ƒéå¸¸é€‚åˆæˆ‘ä»¬è¿è¡Œçš„åŠ é€Ÿå™¨ã€‚æ‰€ä»¥å®ƒéå¸¸æ˜“äºå¹¶è¡Œå¤„ç†ï¼Œè¶…çº§é«˜æ•ˆï¼Œå› æ­¤å®ƒé€‚åˆé‚£ç§è¿…é€Ÿæµè¡Œçš„è§„æ¨¡æ³•åŠªåŠ›ã€‚å¥½å§ï¼Œé™¤éæœ‰ä»»ä½•é—®é¢˜ã€‚
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_4.png)'
- en: We're both excitedï¼Œ so we just unmuteed at the same timeã€‚Yeahï¼Œ so coldã€‚Yeahã€‚sos
    that's my section if there's any questions happy to answer them otherwiseã€‚Let's
    get into NPs NPptTs are like I thinkã€‚There's such a nice next level abstraction
    of the architectureã€‚so you've probably seen the trend ofã€‚Transformers getting
    applied to new domainsã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬éƒ½å¾ˆå…´å¥‹ï¼Œæ‰€ä»¥æˆ‘ä»¬åŒæ—¶è§£é™¤é™éŸ³ã€‚æ˜¯çš„ï¼Œå¤ªå†·äº†ã€‚æ˜¯çš„ã€‚è¿™æ˜¯æˆ‘çš„éƒ¨åˆ†ï¼Œå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ‘å¾ˆä¹æ„å›ç­”ï¼Œå¦åˆ™ã€‚æˆ‘ä»¬å¼€å§‹è®¨è®ºNPã€‚NPptTså°±åƒæˆ‘è®¤ä¸ºçš„é‚£æ ·ã€‚æœ‰ä¸€ä¸ªå¾ˆå¥½çš„ä¸‹ä¸€ä¸ªå±‚æ¬¡çš„æ¶æ„æŠ½è±¡ã€‚æ‰€ä»¥ä½ å¯èƒ½çœ‹åˆ°äº†ï¼ŒTransformersè¢«åº”ç”¨åˆ°æ–°çš„é¢†åŸŸçš„è¶‹åŠ¿ã€‚
- en: first into vision and video and audioã€‚But this is kind of like cutting back
    to an even more abstract levelã€‚like I think tabular dataï¼Œ yeahï¼Œ I don't knowï¼Œ
    I'll Yna and Ne take over from hereã€‚but I think MPT is a pretty sickï¼Œ pretty sick
    projectã€‚Thanks A for the introduction thanks all for the invitation we're very
    happy to be here and Neil and I are now going to tell you about our selfattention
    between data points paper where we introduce introduce the nonprometric transformer
    architecture we'll start with a little bit of motivation we want to explaining
    the architecture detail Im show you the experiments this is more or less a step
    through of the paper but maybe you know with a little bit extra insight here and
    thereã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆæ˜¯è§†è§‰ã€è§†é¢‘å’ŒéŸ³é¢‘ã€‚ä½†è¿™å°±åƒå›åˆ°ä¸€ä¸ªæ›´æŠ½è±¡çš„å±‚é¢ã€‚æˆ‘è®¤ä¸ºè¡¨æ ¼æ•°æ®ï¼Œå—¯ï¼Œæˆ‘ä¸çŸ¥é“ï¼Œæˆ‘ä¼šè®© Yna å’Œ Ne ä»è¿™é‡Œæ¥æ‰‹ï¼Œä½†æˆ‘è®¤ä¸º MPT æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„é¡¹ç›®ã€‚æ„Ÿè°¢
    A çš„ä»‹ç»ï¼Œæ„Ÿè°¢æ‰€æœ‰äººçš„é‚€è¯·ï¼Œæˆ‘ä»¬éå¸¸é«˜å…´èƒ½åœ¨è¿™é‡Œï¼ŒNeil å’Œæˆ‘ç°åœ¨å°†å‘Šè¯‰ä½ å…³äºæˆ‘ä»¬æ•°æ®ç‚¹é—´è‡ªæ³¨æ„åŠ›è®ºæ–‡çš„å†…å®¹ï¼Œæˆ‘ä»¬å¼•å…¥éå‚æ•°å˜å‹å™¨æ¶æ„ï¼Œæˆ‘ä»¬å°†ä»ä¸€äº›åŠ¨æœºå¼€å§‹ï¼Œè§£é‡Šæ¶æ„ç»†èŠ‚ï¼Œå±•ç¤ºå®éªŒï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯å¯¹è®ºæ–‡çš„é€æ­¥è§£è¯»ï¼Œä½†å¯èƒ½ä¼šåœ¨è¿™é‡Œå’Œé‚£é‡Œæä¾›ä¸€äº›é¢å¤–çš„è§è§£ã€‚
- en: All rightï¼Œ as promisedï¼Œ the motivation and a brief summaryã€‚So we'll start by
    thinking about something that we don't often think aboutã€‚that is that from seal
    to transformersï¼Œ most of supervised deep learning relies on parametric predictionã€‚So
    what that means is that we have some self training data and we want to learn to
    predict the outcomes y from the inputs X and for this we set up some model with
    tunable parameters thetaã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ­£å¦‚æ‰¿è¯ºçš„é‚£æ ·ï¼ŒåŠ¨æœºå’Œç®€è¦æ€»ç»“ã€‚æ‰€ä»¥æˆ‘ä»¬å°†å¼€å§‹æ€è€ƒä¸€äº›æˆ‘ä»¬ä¸å¸¸è€ƒè™‘çš„äº‹æƒ…ï¼Œé‚£å°±æ˜¯ä»å° seal åˆ°å˜å‹å™¨ï¼Œå¤§å¤šæ•°ç›‘ç£æ·±åº¦å­¦ä¹ ä¾èµ–äºå‚æ•°é¢„æµ‹ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ä¸€äº›è‡ªæˆ‘è®­ç»ƒæ•°æ®ï¼Œæˆ‘ä»¬æƒ³è¦å­¦ä¹ ä»è¾“å…¥
    X é¢„æµ‹ç»“æœ yï¼Œä¸ºæ­¤æˆ‘ä»¬è®¾ç½®ä¸€äº›å¸¦æœ‰å¯è°ƒå‚æ•° theta çš„æ¨¡å‹ã€‚
- en: then we optimize these parameters to maximize predictive likelihoods on a training
    set or you know equivalently we minimize some lossã€‚And then after trainingï¼Œ we
    have this optimized set of parameters thetaã€‚and then at test time we just put
    these into the model and use these parameters to predict on novel test dataã€‚And
    so crucially hereï¼Œ our prediction at test time only depends on these parametersã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæˆ‘ä»¬ä¼˜åŒ–è¿™äº›å‚æ•°ä»¥æœ€å¤§åŒ–è®­ç»ƒé›†ä¸Šçš„é¢„æµ‹ä¼¼ç„¶ï¼Œæˆ–è€…è¯´æˆ‘ä»¬ç­‰ä»·äºæœ€å°åŒ–æŸäº›æŸå¤±ã€‚ç„¶ååœ¨è®­ç»ƒåï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¿™ç»„ä¼˜åŒ–çš„å‚æ•° thetaï¼Œç„¶ååœ¨æµ‹è¯•æ—¶æˆ‘ä»¬åªéœ€å°†è¿™äº›å‚æ•°æ”¾å…¥æ¨¡å‹ä¸­ï¼Œå¹¶ç”¨è¿™äº›å‚æ•°å¯¹æ–°æµ‹è¯•æ•°æ®è¿›è¡Œé¢„æµ‹ã€‚å› æ­¤ï¼Œå…³é”®æ˜¯ï¼Œæˆ‘ä»¬åœ¨æµ‹è¯•æ—¶çš„é¢„æµ‹ä»…ä¾èµ–äºè¿™äº›å‚æ•°ã€‚
- en: right it's parametricã€‚Also that means that given these parametersã€‚the prediction
    is entirely independent of the training data and so why would we want to do parametric
    predictionï¼Ÿ
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼Œå‚æ•°åŒ–ã€‚è¿™ä¹Ÿæ„å‘³ç€ï¼Œç»™å®šè¿™äº›å‚æ•°ï¼Œé¢„æµ‹å®Œå…¨ç‹¬ç«‹äºè®­ç»ƒæ•°æ®ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦è¿›è¡Œå‚æ•°é¢„æµ‹å‘¢ï¼Ÿ
- en: Wellï¼Œ it's really convenient because all that we've learned from the training
    data can be summarized in the parameters and so at prediction time we only need
    these final parameters and we do not need to store the training dataã€‚which might
    be reallyï¼Œ really largeã€‚On the other handã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸æ–¹ä¾¿ï¼Œå› ä¸ºæˆ‘ä»¬ä»è®­ç»ƒæ•°æ®ä¸­å­¦åˆ°çš„æ‰€æœ‰å†…å®¹éƒ½å¯ä»¥åœ¨å‚æ•°ä¸­æ€»ç»“ï¼Œå› æ­¤åœ¨é¢„æµ‹æ—¶æˆ‘ä»¬åªéœ€è¦è¿™äº›æœ€ç»ˆå‚æ•°ï¼Œè€Œä¸éœ€è¦å­˜å‚¨å¯èƒ½éå¸¸åºå¤§çš„è®­ç»ƒæ•°æ®ã€‚å¦ä¸€æ–¹é¢ã€‚
- en: we usually have models that already predict for a bunch of data in parallel
    right think of mini batching and modern architectures and actually things like
    batch on already make these data interactã€‚And so our thinking here was that if
    we've got all of this data in parallel anywaysã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šå¸¸æœ‰æ¨¡å‹å¯ä»¥å¹¶è¡Œé¢„æµ‹ä¸€å †æ•°æ®ï¼Œæƒ³æƒ³å°æ‰¹é‡å¤„ç†å’Œç°ä»£æ¶æ„ï¼Œå®é™…ä¸Šåƒæ‰¹é‡å¤„ç†è¿™æ ·çš„ä¸œè¥¿å·²ç»ä½¿è¿™äº›æ•°æ®ç›¸äº’ä½œç”¨ã€‚å› æ­¤æˆ‘ä»¬åœ¨è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬åæ­£æœ‰æ‰€æœ‰è¿™äº›å¹¶è¡Œæ•°æ®ã€‚
- en: there's no reason not to make use of it and so more a bit grounder we kind of
    challenge carmetric prediction as the dominant paradigm in deep learning and so
    we want to give models the additional flexibility of using the training data directly
    when making predictionsã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡æœ‰ç†ç”±ä¸åˆ©ç”¨å®ƒï¼Œå› æ­¤æˆ‘ä»¬æŒ‘æˆ˜å¡å°”æ¢…é‡Œå…‹é¢„æµ‹ä½œä¸ºæ·±åº¦å­¦ä¹ ä¸­çš„ä¸»å¯¼èŒƒå¼ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¿›è¡Œé¢„æµ‹æ—¶ç»™äºˆæ¨¡å‹ä½¿ç”¨è®­ç»ƒæ•°æ®çš„é¢å¤–çµæ´»æ€§ã€‚
- en: And so a bit more concretelyã€‚ğŸ˜Šï¼ŒWe introduced the nonparmetric transformer architectureã€‚and
    this is going to be a general deep learning architectureã€‚meaning we can apply
    to a variety of scenariosã€‚![](img/1777b223f7bf8be27a8767d209c7f271_6.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å…·ä½“ä¸€ç‚¹ã€‚ğŸ˜Š æˆ‘ä»¬å¼•å…¥äº†éå‚æ•°å˜å‹å™¨æ¶æ„ã€‚è¿™å°†æˆä¸ºä¸€ä¸ªé€šç”¨çš„æ·±åº¦å­¦ä¹ æ¶æ„ï¼Œæ„å‘³ç€æˆ‘ä»¬å¯ä»¥åº”ç”¨äºå¤šç§åœºæ™¯ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_6.png)
- en: NPTs will take the entire data set as input whenever possibleã€‚And NPTs then
    crucially learn to predict from interactions between data pointsã€‚And to achieve
    thisã€‚we use multi head self attentionã€‚That as Age has introduced us toã€‚has just
    really established itself as a general purpose layer for reasoningã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: NPTså°†å°½å¯èƒ½å°†æ•´ä¸ªæ•°æ®é›†ä½œä¸ºè¾“å…¥ã€‚ç„¶åï¼ŒNPTså…³é”®åœ°å­¦ä¹ ä»æ•°æ®ç‚¹ä¹‹é—´çš„äº¤äº’ä¸­è¿›è¡Œé¢„æµ‹ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›ã€‚è¿™æ­£å¦‚Ageå‘æˆ‘ä»¬ä»‹ç»çš„ï¼Œå·²ç»çœŸæ­£ç¡®ç«‹ä¸ºä¸€ç§é€šç”¨æ¨ç†å±‚ã€‚
- en: We also take another thing from the NLP community and we use a stochastic masking
    mechanism and we use that to tell entitiesmpes where to predict and also to regularise
    the learning task of itã€‚And last yearï¼Œ of courseï¼Œ we hope to convince that the
    ends up working reallyã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å€Ÿé‰´äº†è‡ªç„¶è¯­è¨€å¤„ç†ç¤¾åŒºçš„å¦ä¸€é¡¹å†…å®¹ï¼Œä½¿ç”¨éšæœºæ©ç æœºåˆ¶ï¼Œå‘Šè¯‰å®ä½“æ ·æœ¬åœ¨å“ªé‡Œè¿›è¡Œé¢„æµ‹ï¼Œå¹¶ä¸”ä¹Ÿå¯¹å…¶å­¦ä¹ ä»»åŠ¡è¿›è¡Œæ­£åˆ™åŒ–ã€‚å»å¹´ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿè¯æ˜æœ€ç»ˆæ•ˆæœå¾ˆå¥½ã€‚
- en: really well and that this kind of simple idea of learning to predict from the
    other data points of the inputã€‚from the training points of the input and up working
    as as wellã€‚Soã€‚and so very briefly summarizing what we've heard alreadyã€‚Aï¼Œ we input
    into NPptTCide datasetã€‚And then Bï¼Œ let's say for the purpose of this slide hereã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç®€å•çš„æƒ³æ³•ï¼Œå­¦ä¹ ä»è¾“å…¥çš„å…¶ä»–æ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹ï¼Œå®é™…ä¸Šå·¥ä½œå¾—å¾ˆå¥½ã€‚å› æ­¤ï¼Œéå¸¸ç®€è¦åœ°æ€»ç»“æˆ‘ä»¬å·²ç»å¬åˆ°çš„å†…å®¹ï¼ŒAï¼Œæˆ‘ä»¬è¾“å…¥NPptTCideæ•°æ®é›†ã€‚ç„¶åBï¼Œå‡è®¾ä¸ºäº†è¿™ä¸ªå¹»ç¯ç‰‡çš„ç›®çš„ã€‚
- en: we only care about predicting the orange question mark in that green rowã€‚And
    then we can compare entitiespts to parametric prediction right so a classical
    deep learning model would predict this target value only from the features of
    that single grid input to do that it would use the parameters thetaã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªå…³å¿ƒåœ¨é‚£ä¸€ç»¿è‰²è¡Œä¸­é¢„æµ‹æ©™è‰²é—®å·ã€‚ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ä½“ç‚¹ä¸å‚æ•°é¢„æµ‹è¿›è¡Œæ¯”è¾ƒï¼Œç»å…¸çš„æ·±åº¦å­¦ä¹ æ¨¡å‹å°†ä»…ä»è¯¥å•ä¸ªç½‘æ ¼è¾“å…¥çš„ç‰¹å¾ä¸­é¢„æµ‹è¿™ä¸ªç›®æ ‡å€¼ï¼Œä¸ºæ­¤å®ƒå°†ä½¿ç”¨å‚æ•°Î¸ã€‚
- en: those would depend on whatever training data we've seen and so onã€‚but at test
    time we only look at that single row for which we care about the predictionã€‚ğŸ˜Šã€‚In
    contrastï¼Œ NPpts predict an explicit dependence on all samples in the input they
    can look beyond that single green data of interest and look at all other samples
    that are there and consider their values for prediction so this presents an entirelyã€‚ğŸ˜Šï¼ŒDifferent
    way of thinking about how we learn predictive mechanisms somebody on Twitter called
    this Canan 2ã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å°†ä¾èµ–äºæˆ‘ä»¬æ‰€çœ‹åˆ°çš„ä»»ä½•è®­ç»ƒæ•°æ®ç­‰ï¼Œä½†åœ¨æµ‹è¯•æ—¶æˆ‘ä»¬åªå…³æ³¨é‚£ä¸€è¡Œï¼Œæˆ‘ä»¬å…³å¿ƒçš„é¢„æµ‹ã€‚ğŸ˜Šã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒNPptsæ˜ç¡®é¢„æµ‹è¾“å…¥ä¸­æ‰€æœ‰æ ·æœ¬çš„ä¾èµ–å…³ç³»ï¼Œå®ƒä»¬å¯ä»¥è¶…è¶Šé‚£å•ä¸ªç»¿è‰²æ„Ÿå…´è¶£çš„æ•°æ®ï¼ŒæŸ¥çœ‹æ‰€æœ‰å…¶ä»–æ ·æœ¬å¹¶è€ƒè™‘å®ƒä»¬çš„å€¼è¿›è¡Œé¢„æµ‹ï¼Œå› æ­¤è¿™å‘ˆç°å‡ºä¸€ç§å®Œå…¨ä¸åŒçš„æ–¹å¼æ¥æ€è€ƒæˆ‘ä»¬å¦‚ä½•å­¦ä¹ é¢„æµ‹æœºåˆ¶ï¼Œæœ‰äººåœ¨æ¨ç‰¹ä¸Šç§°ä¹‹ä¸ºCanan
    2ã€‚
- en: 0ï¼Œ which we would have not written in the paperï¼Œ but maybe is kind of a nice
    way of thinking about how NPTs can learn to predictã€‚ğŸ˜Šã€‚![](img/1777b223f7bf8be27a8767d209c7f271_8.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0ï¼Œè¿™åœ¨è®ºæ–‡ä¸­æ²¡æœ‰å†™ï¼Œä½†ä¹Ÿè®¸æ˜¯æ€è€ƒNPTså¦‚ä½•å­¦ä¹ é¢„æµ‹çš„ä¸€ä¸ªä¸é”™çš„æ–¹æ³•ã€‚ğŸ˜Šï¼[](img/1777b223f7bf8be27a8767d209c7f271_8.png)
- en: So of courseï¼Œ non parametric models are a thing alreadyï¼Œ we didn't invent them
    at all andã€‚![](img/1777b223f7bf8be27a8767d209c7f271_10.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå½“ç„¶ï¼Œæ— å‚æ•°æ¨¡å‹å·²ç»å­˜åœ¨ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å‘æ˜å®ƒä»¬ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_10.png)
- en: I defined them here as prediction in explicit dependence on the training dataã€‚which
    is certainly what MPptTs doã€‚Classical examples like Gaussian processesï¼Œ can neighborã€‚kernelnal
    methodsï¼Œ those might be familiar to youã€‚ğŸ˜Šã€‚And there exists also efforts to combine
    the benefits of nonprometrics and representation learning in a similar fashion
    to how we did it in entitiesã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™é‡Œå°†å®ƒä»¬å®šä¹‰ä¸ºå¯¹è®­ç»ƒæ•°æ®çš„æ˜¾å¼ä¾èµ–é¢„æµ‹ï¼Œè¿™æ— ç–‘æ˜¯MPptTsæ‰€åšçš„ã€‚ç»å…¸çš„ä¾‹å­å¦‚é«˜æ–¯è¿‡ç¨‹ã€é‚»åŸŸæ–¹æ³•ã€æ ¸æ–¹æ³•ï¼Œè¿™äº›å¯èƒ½å¯¹ä½ æ¥è¯´æ˜¯ç†Ÿæ‚‰çš„ã€‚ğŸ˜Šã€‚ä¹Ÿæœ‰åŠªåŠ›å°†æ— å‚æ•°å’Œè¡¨ç¤ºå­¦ä¹ çš„å¥½å¤„ç»“åˆèµ·æ¥ï¼Œç±»ä¼¼äºæˆ‘ä»¬åœ¨å®ä½“ä¸­æ‰€åšçš„ã€‚
- en: ğŸ˜Šï¼ŒHoweverï¼Œ these approaches are usually limited in some sense in comparison
    opportunities right they're often kind of motivated from the statistics community
    a bit more they often require more fiically approximate inference schemes are
    limited in the interactions they can learn or things like that and soã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨æŸç§æ„ä¹‰ä¸Šé€šå¸¸å—åˆ°é™åˆ¶ï¼Œä¸æœºä¼šç›¸æ¯”ï¼Œå®ƒä»¬å¾€å¾€æ›´å¤šåœ°å—åˆ°ç»Ÿè®¡å­¦ç•Œçš„é©±åŠ¨ï¼Œé€šå¸¸éœ€è¦æ›´ç²¾ç¡®çš„è¿‘ä¼¼æ¨æ–­æ–¹æ¡ˆï¼Œé™åˆ¶äº†å®ƒä»¬å¯ä»¥å­¦ä¹ çš„äº¤äº’æˆ–ç±»ä¼¼çš„äº‹æƒ…ã€‚
- en: ğŸ˜Šï¼ŒWe really think NPTs presentã€‚Maybe the most versatile and most widely applicable
    of these nonprometric prediction approaches but that's something we explicitly
    wanted to have we wanted to have something that's really easy to use plug play
    works in a ton of scenarios and works really wellã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘ä»¬çœŸçš„è®¤ä¸ºNPTså¾ˆå‡ºè‰²ã€‚ä¹Ÿè®¸æ˜¯è¿™äº›éå‚æ•°é¢„æµ‹æ–¹æ³•ä¸­æœ€é€šç”¨å’Œåº”ç”¨æœ€å¹¿æ³›çš„ï¼Œä½†è¿™æ­£æ˜¯æˆ‘ä»¬æ˜ç¡®æƒ³è¦çš„ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰ä¸€ç§çœŸæ­£æ˜“äºä½¿ç”¨çš„æ–¹å¼ï¼Œé€‚ç”¨äºè®¸å¤šåœºæ™¯ï¼Œå¹¶ä¸”æ•ˆæœå¾ˆå¥½ã€‚
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_12.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_12.png)'
- en: And so with thatï¼Œ I'd hand over to Neilï¼Œ whos going to tell you about the nonparmetric
    transformer architecture in all of its detailsã€‚you also have one question hi Jen
    could you please go to the previous slideï¼Ÿ
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘ä¼šæŠŠè¯è½¬ç»™Neilï¼Œä»–å°†å‘Šè¯‰ä½ æœ‰å…³éå‚æ•°å˜æ¢å™¨æ¶æ„çš„æ‰€æœ‰ç»†èŠ‚ã€‚ä½ è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå—¨ï¼ŒJenï¼Œæ‚¨èƒ½å¦å›åˆ°ä¸Šä¸€å¼ å¹»ç¯ç‰‡ï¼Ÿ
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_14.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_14.png)'
- en: The very previous slideã€‚Yeahï¼Œ yesã€‚This slideã€‚Yeahã€‚So in terms of the problem
    definitionã€‚I think is' quite similar to some meta learning problemã€‚which basically
    learns a mapping from a data point and the data sets to some predictionsã€‚So could
    you please suggest any differences between your problem setting and meta learning
    problem settingã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„å¹»ç¯ç‰‡ã€‚æ˜¯çš„ï¼Œæ²¡é”™ã€‚è¿™ä¸€å¹»ç¯ç‰‡ã€‚æ˜¯çš„ã€‚åœ¨é—®é¢˜å®šä¹‰æ–¹é¢ï¼Œæˆ‘è®¤ä¸ºè¿™ä¸æŸäº›å…ƒå­¦ä¹ é—®é¢˜ç›¸ä¼¼ï¼ŒåŸºæœ¬ä¸Šæ˜¯å­¦ä¹ ä»æ•°æ®ç‚¹å’Œæ•°æ®é›†åˆ°æŸäº›é¢„æµ‹çš„æ˜ å°„ã€‚é‚£ä¹ˆï¼Œè¯·é—®æ‚¨èƒ½æŒ‡å‡ºæ‚¨é—®é¢˜è®¾ç½®ä¸å…ƒå­¦ä¹ é—®é¢˜è®¾ç½®ä¹‹é—´çš„ä»»ä½•åŒºåˆ«å—ï¼Ÿ
- en: I can't reallyã€‚Fed out any differences between these two problemsã€‚Wellã€‚I think
    it really depends on the framing that you want to have right so I would say meta
    learning would be when I try to predict over multiple data setsã€‚so when I try
    to predict some when I try to learn some sort of prediction model or I can just
    plug in a different data set and it will automatically or almost automatically
    give me new predictions on this different data distributionã€‚But that's not what
    we do at allï¼Œ rightï¼Œ we're training a single model for a fixed data setã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å®åœ¨æ˜¯æ— æ³•æ‰¾å‡ºè¿™ä¸¤ä¸ªé—®é¢˜ä¹‹é—´çš„ä»»ä½•åŒºåˆ«ã€‚å—¯ï¼Œæˆ‘è®¤ä¸ºè¿™çœŸçš„å–å†³äºæ‚¨æƒ³è¦çš„æ¡†æ¶ï¼Œæ‰€ä»¥æˆ‘ä¼šè¯´å…ƒå­¦ä¹ æ˜¯å½“æˆ‘è¯•å›¾åœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œé¢„æµ‹æ—¶ã€‚å› æ­¤ï¼Œå½“æˆ‘è¯•å›¾å­¦ä¹ æŸç§é¢„æµ‹æ¨¡å‹ï¼Œæˆ–è€…æˆ‘å¯ä»¥æ’å…¥ä¸åŒçš„æ•°æ®é›†æ—¶ï¼Œå®ƒä¼šå‡ ä¹è‡ªåŠ¨åœ°ä¸ºæˆ‘æä¾›åœ¨è¿™ç§ä¸åŒæ•°æ®åˆ†å¸ƒä¸Šçš„æ–°é¢„æµ‹ã€‚ä½†è¿™æ ¹æœ¬ä¸æ˜¯æˆ‘ä»¬æ‰€åšçš„ï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬æ˜¯åœ¨ä¸ºå›ºå®šæ•°æ®é›†è®­ç»ƒå•ä¸ªæ¨¡å‹ã€‚
- en: And so this is why I wouldn't really call that meta learning because we're doingã€‚we're
    trying to predict on the same tasks that all the supervised deep learning or any
    supervised machine learning method is trying to predict well onã€‚Consuming you
    use kind of same test site to test your trend modelï¼Œ rightï¼ŸI meanï¼Œ I meanï¼Œ likeã€‚å‘ƒã€‚Soã€‚So
    basically in MI learning we're going to test on different kind of meta test setsã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä¸æƒ³ç§°å…¶ä¸ºå…ƒå­¦ä¹ ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åšçš„äº‹æƒ…æ˜¯ï¼Œæˆ‘ä»¬è¯•å›¾åœ¨æ‰€æœ‰ç›‘ç£æ·±åº¦å­¦ä¹ æˆ–ä»»ä½•ç›‘ç£æœºå™¨å­¦ä¹ æ–¹æ³•è¯•å›¾å‡†ç¡®é¢„æµ‹çš„ç›¸åŒä»»åŠ¡ä¸Šè¿›è¡Œé¢„æµ‹ã€‚å‰ææ˜¯ä½ ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•é›†æ¥æµ‹è¯•ä½ çš„è¶‹åŠ¿æ¨¡å‹ï¼Œå¯¹å§ï¼Ÿæˆ‘çš„æ„æ€æ˜¯ï¼Œå‘ƒã€‚æ‰€ä»¥ã€‚åŸºæœ¬ä¸Šåœ¨MIå­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ä¸åŒçš„å…ƒæµ‹è¯•é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚
- en: but in your case you just want to use a test set which is similar to the distribution
    Yeah of your training set rightï¼Ÿ
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯åœ¨æ‚¨çš„æƒ…å†µä¸‹ï¼Œæ‚¨åªæƒ³ä½¿ç”¨ä¸€ä¸ªä¸æ‚¨çš„è®­ç»ƒé›†åˆ†å¸ƒç›¸ä¼¼çš„æµ‹è¯•é›†ï¼Œå¯¹å§ï¼Ÿ
- en: Yeah absolutely so we explore data set distribution shift a bitã€‚I think it's
    a really interesting scenario I think meta learning different data sets is also
    an interesting scenario right when you have this model where you just pressure
    in different data sets but for the scope of this paper it's very much training
    set test set they come from the same distribution and we're just trying to do
    supervised learning in a standard setting thats so cool thank youã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œç»å¯¹å¦‚æ­¤ï¼Œæˆ‘ä»¬ç¨å¾®æ¢è®¨äº†ä¸€ä¸‹æ•°æ®é›†åˆ†å¸ƒçš„å˜åŒ–ã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„åœºæ™¯ï¼Œè€Œå…ƒå­¦ä¹ ä¸åŒçš„æ•°æ®é›†ä¹Ÿæ˜¯ä¸€ä¸ªæœ‰è¶£çš„åœºæ™¯ï¼Œå¯¹å§ï¼Ÿå½“ä½ æœ‰è¿™ä¸ªæ¨¡å‹æ—¶ï¼Œå¯ä»¥è¾“å…¥ä¸åŒçš„æ•°æ®é›†ï¼Œä½†å°±æœ¬æ–‡çš„èŒƒå›´è€Œè¨€ï¼Œè®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¥è‡ªåŒä¸€åˆ†å¸ƒï¼Œæˆ‘ä»¬åªæ˜¯è¯•å›¾åœ¨æ ‡å‡†è®¾ç½®ä¸­è¿›è¡Œç›‘ç£å­¦ä¹ ï¼Œè¿™çœŸæ˜¯å¤ªé…·äº†ï¼Œè°¢è°¢ã€‚
- en: ğŸ˜Šï¼ŒThank you for the questionã€‚![](img/1777b223f7bf8be27a8767d209c7f271_16.png)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ„Ÿè°¢æ‚¨çš„æé—®ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_16.png)
- en: Yeahï¼Œ and I would chime in a couple additional thingsï¼Œ I guessã€‚so at least from
    what I understand from the problem definition of meta learningã€‚I think the aim
    is moreã€‚Perhaps being able to perform well on a new data with a relatively small
    number of additional gradient steps on that data setã€‚so I think there's some interesting
    ways that you could actually consider applying NPTs in a meta learning type setting
    and so we'll get into this a little bit more but for example you know there might
    be ways to essentially add in a new data so let's suppose we've trained on a bunch
    of different data sets we now add in a new data set we can perhaps do some sorts
    of kind of zero zero shot meta learning basically where there's no need for additional
    gradient steps because we're basically predicting kind of similar to how you might
    do prompting nowadays in NLP literatureã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘æƒ³æˆ‘å¯ä»¥è¡¥å……å‡ ç‚¹ã€‚æˆ‘ç†è§£çš„å…ƒå­¦ä¹ é—®é¢˜å®šä¹‰çš„ç›®æ ‡æ›´å€¾å‘äºèƒ½å¤Ÿåœ¨æ–°æ•°æ®ä¸Šè¡¨ç°è‰¯å¥½ï¼Œå¹¶ä¸”åœ¨è¯¥æ•°æ®é›†ä¸Šåªéœ€å°‘é‡é¢å¤–çš„æ¢¯åº¦æ­¥éª¤ã€‚æ‰€ä»¥ï¼Œæˆ‘è®¤ä¸ºæœ‰ä¸€äº›æœ‰è¶£çš„æ–¹å¼å¯ä»¥åœ¨å…ƒå­¦ä¹ çš„è®¾ç½®ä¸­è€ƒè™‘åº”ç”¨NPTsï¼Œæˆ‘ä»¬ç¨åä¼šæ›´æ·±å…¥æ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å·²ç»åœ¨è®¸å¤šä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç°åœ¨æ·»åŠ ä¸€ä¸ªæ–°æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸€äº›ç±»ä¼¼äºé›¶æ ·æœ¬å…ƒå­¦ä¹ çš„æ“ä½œï¼ŒåŸºæœ¬ä¸Šä¸éœ€è¦é¢å¤–çš„æ¢¯åº¦æ­¥éª¤ï¼Œå› ä¸ºæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯ä»¥ç±»ä¼¼äºç°åœ¨NLPæ–‡çŒ®ä¸­çš„æç¤ºæ–¹å¼è¿›è¡Œé¢„æµ‹ã€‚
- en: Anywaysï¼Œ yeahï¼Œ I think we'll get into some more detailsã€‚Just to chime in on
    that I don't think that every meta learning algorithm I think the ones that you're
    described right now are like optimization basedã€‚but they're also black box ones
    like you don't need toã€‚Furtherã€‚I think the main difference seems to be that there
    is like one task versus multiple tasks for meta learningã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œæ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬ä¼šæ›´è¯¦ç»†åœ°è®¨è®ºè¿™ä¸ªé—®é¢˜ã€‚æˆ‘æƒ³è¡¥å……çš„æ˜¯ï¼Œæˆ‘å¹¶ä¸è®¤ä¸ºæ¯ç§å…ƒå­¦ä¹ ç®—æ³•éƒ½æ˜¯åŸºäºä¼˜åŒ–çš„ï¼Œåƒä½ ç°åœ¨æè¿°çš„é‚£äº›ç®—æ³•ä¹Ÿæ˜¯é»‘ç®±å‹çš„ï¼Œä¸éœ€è¦è¿›ä¸€æ­¥çš„ç»†åŒ–ã€‚æˆ‘è®¤ä¸ºä¸»è¦çš„åŒºåˆ«ä¼¼ä¹åœ¨äºå•ä»»åŠ¡ä¸å¤šä»»åŠ¡çš„å…ƒå­¦ä¹ ã€‚
- en: Yeahï¼Œ I I think so tooï¼Œ I think the the like mainï¼Œ yeahã€‚theing the main framing
    question is whether or not there's multiple data setsã€‚Coolã€‚Okayï¼Œ awesomeã€‚If there's
    no other questionsï¼Œ I'll dive a bit more into the architectureã€‚![](img/1777b223f7bf8be27a8767d209c7f271_18.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘ä¹Ÿè¿™æ ·è®¤ä¸ºï¼Œä¸»è¦çš„é—®é¢˜æ˜¯æ˜¯å¦æœ‰å¤šä¸ªæ•°æ®é›†ã€‚å¾ˆå¥½ã€‚å¥½çš„ï¼Œå¤ªå¥½äº†ã€‚å¦‚æœæ²¡æœ‰å…¶ä»–é—®é¢˜ï¼Œæˆ‘å°†æ›´æ·±å…¥åœ°æ¢è®¨æ¶æ„ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_18.png)
- en: Awesomeï¼Œ so there's three key components to NPptsã€‚I'm gonna to first state them
    at a high levelã€‚and then we'll go through each of them in more detailã€‚Soï¼Œ first
    of allã€‚we take the entire data setã€‚All data points is inputã€‚Soï¼Œ for exampleï¼Œ at
    test timeã€‚the model is going to take as inputï¼Œ both training and test dataã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¤ªæ£’äº†ï¼ŒNPptsæœ‰ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†ã€‚æˆ‘å°†é¦–å…ˆä»é«˜å±‚æ¬¡é™ˆè¿°å®ƒä»¬ï¼Œç„¶åæˆ‘ä»¬å°†æ›´è¯¦ç»†åœ°è®¨è®ºæ¯ä¸ªéƒ¨åˆ†ã€‚å› æ­¤ï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å–æ•´ä¸ªæ•°æ®é›†ã€‚æ‰€æœ‰æ•°æ®ç‚¹ä½œä¸ºè¾“å…¥ã€‚ä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œæ¨¡å‹å°†åŒæ—¶ä½¿ç”¨è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®ä½œä¸ºè¾“å…¥ã€‚
- en: And we approximate this with mini batches for large dataã€‚ğŸ˜Šã€‚We apply self attention
    between data pointsï¼Œ so for exampleï¼Œ at test timeã€‚we model relationships amongst
    training pointsï¼Œ amongst test points and between the two setsã€‚And then finally
    we have this masking based training objectiveã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡å°æ‰¹é‡æ¥è¿‘ä¼¼å¤„ç†å¤§æ•°æ®ã€‚ğŸ˜Šã€‚æˆ‘ä»¬åœ¨æ•°æ®ç‚¹ä¹‹é—´åº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œä¾‹å¦‚ï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å»ºæ¨¡è®­ç»ƒç‚¹ã€æµ‹è¯•ç‚¹ä¹‹é—´ä»¥åŠä¸¤ç»„ä¹‹é—´çš„å…³ç³»ã€‚æœ€åï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªåŸºäºæ©è”½çš„è®­ç»ƒç›®æ ‡ã€‚
- en: it's a burnt like stochastic masking and the key point is that we actually use
    it on both features as well as on training targets and we'll get into why that
    kind of leads to an interesting predictive mechanism laterã€‚Sureã€‚So to start with
    this idea of data sets as inputã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ç§ç±»ä¼¼éšæœºæ©è”½çš„çƒ§å½•ï¼Œå…³é”®ç‚¹åœ¨äºæˆ‘ä»¬å®é™…ä¸Šåœ¨ç‰¹å¾å’Œè®­ç»ƒç›®æ ‡ä¸Šéƒ½ä½¿ç”¨å®ƒï¼Œç¨åæˆ‘ä»¬ä¼šè®¨è®ºä¸ºä»€ä¹ˆè¿™å¯¼è‡´äº†ä¸€ç§æœ‰è¶£çš„é¢„æµ‹æœºåˆ¶ã€‚å¥½çš„ã€‚é‚£ä¹ˆé¦–å…ˆä»æ•°æ®é›†ä½œä¸ºè¾“å…¥çš„è¿™ä¸ªæƒ³æ³•å¼€å§‹ã€‚
- en: there's two things that compose the input to NPTï¼Œ it's a full data set in the
    form of a matrix X and a masking matrix Mã€‚And so Yick has described this data
    set matrix a little bitï¼Œ we basically have data points as rowsã€‚the columns are
    attributes and each attribute shares some kind of semantic meaning among all of
    its data pointsã€‚so say for example you're just doing single target classification
    or regression the last column would be the target and the rest of the matrix would
    be input features so for example the pixels of an imageã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç»„æˆNPTè¾“å…¥çš„æœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯ä»¥çŸ©é˜µXå½¢å¼çš„å®Œæ•´æ•°æ®é›†ï¼Œå¦ä¸€ä¸ªæ˜¯æ©è”½çŸ©é˜µMã€‚å› æ­¤ï¼ŒYickå¯¹è¿™ä¸ªæ•°æ®é›†çŸ©é˜µè¿›è¡Œäº†ç¨å¾®æè¿°ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæœ‰æ•°æ®ç‚¹ä½œä¸ºè¡Œï¼Œåˆ—æ˜¯å±æ€§ï¼Œæ¯ä¸ªå±æ€§åœ¨æ‰€æœ‰æ•°æ®ç‚¹ä¹‹é—´å…±äº«æŸç§è¯­ä¹‰æ„ä¹‰ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æ­£åœ¨è¿›è¡Œå•ç›®æ ‡åˆ†ç±»æˆ–å›å½’ï¼Œæœ€åä¸€åˆ—å°†æ˜¯ç›®æ ‡ï¼Œå…¶ä½™çš„çŸ©é˜µå°†æ˜¯è¾“å…¥ç‰¹å¾ï¼Œä¾‹å¦‚å›¾åƒçš„åƒç´ ã€‚
- en: We also have a masking matrixï¼Œ so let's say you know we're thinking about mass
    language modeling the mass tokens will just tell us where we're going to conceal
    words and where we're going to backproagate a loss we do a similar type of thing
    here where we use this binary mass matrix to specify which entries are maskedã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªæ©ç çŸ©é˜µï¼Œæ‰€ä»¥å‡è®¾ä½ çŸ¥é“æˆ‘ä»¬åœ¨è€ƒè™‘å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ï¼Œå¤§è§„æ¨¡çš„æ ‡è®°ä¼šå‘Šè¯‰æˆ‘ä»¬åœ¨å“ªé‡Œéšè—å•è¯ï¼Œä»¥åŠæˆ‘ä»¬åœ¨å“ªé‡Œåå‘ä¼ æ’­æŸå¤±ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œåšç±»ä¼¼çš„äº‹æƒ…ï¼Œæˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªäºŒè¿›åˆ¶æ©ç çŸ©é˜µæ¥æŒ‡å®šå“ªäº›æ¡ç›®æ˜¯è¢«æ©ç›–çš„ã€‚
- en: And the goal is to predict mass values from observed valuesã€‚I see that there
    was a question about handling inputs with different lengthsã€‚In the data sets we've
    considered we'll get into it in the results sectionã€‚but it's mostly been sort
    of tabular in image data where the lengths for each of the data points is the
    sameã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®æ ‡æ˜¯ä»è§‚å¯Ÿå€¼ä¸­é¢„æµ‹æ©ç å€¼ã€‚æˆ‘çœ‹åˆ°æœ‰ä¸€ä¸ªå…³äºå¤„ç†ä¸åŒé•¿åº¦è¾“å…¥çš„é—®é¢˜ã€‚åœ¨æˆ‘ä»¬è€ƒè™‘çš„æ•°æ®é›†ä¸­ï¼Œæˆ‘ä»¬å°†åœ¨ç»“æœéƒ¨åˆ†è®¨è®ºè¿™ä¸€ç‚¹ï¼Œä½†å¤§å¤šæ•°æ•°æ®é›†åŸºæœ¬ä¸Šéƒ½æ˜¯è¡¨æ ¼æ•°æ®å’Œå›¾åƒæ•°æ®ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®ç‚¹çš„é•¿åº¦æ˜¯ç›¸åŒçš„ã€‚
- en: but it would work just like padding that would be a reasonable way to go about
    that and there's also kind of an interesting yeah go for anã€‚This to add to that
    I'm not sure if length prefers refers to columns or two rows right rows we don't
    care about how many rows length padding or something would be an option Yeah my
    question was about column exactly so that that makes sense I thinkã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒçš„å·¥ä½œæ–¹å¼å°±åƒå¡«å……ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªåˆç†çš„æ–¹æ³•ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€äº›æœ‰è¶£çš„ï¼Œå—¯ï¼Œç»§ç»­ã€‚å¯¹æ­¤æˆ‘ä¸ç¡®å®šâ€œé•¿åº¦â€æ˜¯æŒ‡åˆ—è¿˜æ˜¯è¡Œï¼Œå®é™…ä¸Šæˆ‘ä»¬ä¸å…³å¿ƒè¡Œçš„æ•°é‡ï¼Œé•¿åº¦å¡«å……ä¹‹ç±»çš„ä¼šæ˜¯ä¸€ä¸ªé€‰æ‹©ã€‚æ˜¯çš„ï¼Œæˆ‘çš„é—®é¢˜æ­£æ˜¯å…³äºåˆ—çš„ï¼Œæ‰€ä»¥è¿™æœ‰é“ç†ï¼Œæˆ‘æƒ³ã€‚
- en: Yeahï¼Œ and I meanï¼Œ that goes along with the whole meta learning discussion is
    I think if we wanted to adapt to data sets that have a different number of data
    data points per data setã€‚you knowï¼Œ we can take advantage of the fact that self
    attention is kind of okay with thatã€‚Coolã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘æ˜¯è¯´ï¼Œè¿™ä¸æ•´ä¸ªå…ƒå­¦ä¹ è®¨è®ºæœ‰å…³ï¼Œæˆ‘è®¤ä¸ºå¦‚æœæˆ‘ä»¬æƒ³é€‚åº”æ¯ä¸ªæ•°æ®é›†å…·æœ‰ä¸åŒæ•°é‡æ•°æ®ç‚¹çš„æ•°æ®é›†ï¼Œä½ çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨è‡ªæ³¨æ„åŠ›åœ¨è¿™æ–¹é¢çš„ä¼˜åŠ¿ã€‚é…·ã€‚
- en: So continuing onã€‚Le to discuss here is basically how we do the embedding so
    to put this more explicitly we have this data matrix it has n data pointsã€‚it's
    called X and it all has D attributes and we have the binary mass matrix M we're
    going to stack them and then we're going to do a linear embedding so specifically
    we're doing the same linear embedding independently for each data point we're
    learning a different embedding for each attributeã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç»§ç»­ä¸‹å»ï¼Œæ¥ä¸‹æ¥è®¨è®ºçš„æ˜¯æˆ‘ä»¬å¦‚ä½•è¿›è¡ŒåµŒå…¥ï¼Œæ›´æ˜ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬æœ‰è¿™ä¸ªæ•°æ®çŸ©é˜µï¼Œå®ƒæœ‰ n ä¸ªæ•°æ®ç‚¹ï¼Œç§°ä¸º Xï¼Œæ‰€æœ‰æ•°æ®ç‚¹éƒ½æœ‰ D ä¸ªå±æ€§ï¼Œæˆ‘ä»¬æœ‰äºŒè¿›åˆ¶æ©ç çŸ©é˜µ
    Mï¼Œæˆ‘ä»¬å°†å®ƒä»¬å †å åœ¨ä¸€èµ·ï¼Œç„¶åè¿›è¡Œçº¿æ€§åµŒå…¥ï¼Œå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªæ•°æ®ç‚¹ç‹¬ç«‹åœ°è¿›è¡Œç›¸åŒçš„çº¿æ€§åµŒå…¥ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªå±æ€§å­¦ä¹ ä¸åŒçš„åµŒå…¥ã€‚
- en: We have a positional encoding on the index of the attributes because we don't
    really care about say being equi over the columnsã€‚if it's tabular data you of
    course want to treat all these kind of heterogeneous columns differently and then
    finally we have an encoding on the type of columns so whether or not it's continuous
    or categoricalã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯¹å±æ€§çš„ç´¢å¼•è¿›è¡Œäº†ä½ç½®ç¼–ç ï¼Œå› ä¸ºæˆ‘ä»¬å¹¶ä¸çœŸçš„å…³å¿ƒåœ¨åˆ—ä¸Šæ˜¯å‡åŒ€çš„ã€‚å¦‚æœæ˜¯è¡¨æ ¼æ•°æ®ï¼Œä½ å½“ç„¶æƒ³è¦ä»¥ä¸åŒçš„æ–¹å¼å¯¹å¾…æ‰€æœ‰è¿™äº›å¼‚æ„åˆ—ï¼Œæœ€åæˆ‘ä»¬å¯¹åˆ—çš„ç±»å‹è¿›è¡Œäº†ç¼–ç ï¼Œå› æ­¤æ— è®ºæ˜¯è¿ç»­çš„è¿˜æ˜¯åˆ†ç±»çš„ã€‚
- en: And that ends up giving us this input data set representation that is dimensions
    n by D by Eã€‚The second key component of NPpts is tension between data pointsã€‚So
    to do thatã€‚we first take this representation we have and flatten to an end by
    d times e representationã€‚so basically we're treating each of these d times e size
    rows as if it's a token representationã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ€ç»ˆç»™æˆ‘ä»¬å¸¦æ¥äº†è¿™ä¸ªè¾“å…¥æ•°æ®é›†çš„è¡¨ç¤ºï¼Œç»´åº¦ä¸º n x D x Eã€‚NPpts çš„ç¬¬äºŒä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯æ•°æ®ç‚¹ä¹‹é—´çš„å¼ åŠ›ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆå°†æˆ‘ä»¬æ‹¥æœ‰çš„è¡¨ç¤ºå±•å¹³ä¸º
    n x d x e çš„è¡¨ç¤ºã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼Œæˆ‘ä»¬å°†è¿™äº› d x e å¤§å°çš„è¡Œè§†ä¸ºä»¤ç‰Œè¡¨ç¤ºã€‚
- en: We're actually going to just accomplish this operation using multied selfat
    you know we've reviewed this a lotã€‚but the nice thing is that we know from language
    modeling we stack this multiple times we can model these higher order dependencies
    and here they between data points and that's really the key draw of this architecture
    there's been other kind of instances of people using attention for similar sorts
    of things so for example like attentive neural processes a lot of times they've
    sort of used just a single layer as kind of representational lookup and we believe
    that this actually ends up limiting expresssivity and that by sacking this many
    times you can learn more complex relationships between the data pointsã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®é™…ä¸Šè¦ç”¨å¤šé‡è‡ªæ³¨æ„åŠ›æ¥å®Œæˆè¿™ä¸ªæ“ä½œï¼Œä½ çŸ¥é“æˆ‘ä»¬å·²ç»å¤šæ¬¡å®¡æŸ¥äº†è¿™ä¸ªã€‚ä½†å¥½çš„ä¸€ç‚¹æ˜¯ï¼Œæˆ‘ä»¬çŸ¥é“åœ¨è¯­è¨€å»ºæ¨¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¤šæ¬¡å †å å®ƒï¼Œå»ºæ¨¡è¿™äº›æ•°æ®ç‚¹ä¹‹é—´çš„é«˜é˜¶ä¾èµ–å…³ç³»ï¼Œè¿™å®é™…ä¸Šæ˜¯è¯¥æ¶æ„çš„å…³é”®ä¼˜ç‚¹ã€‚è¿˜æœ‰å…¶ä»–å®ä¾‹ä½¿ç”¨æ³¨æ„åŠ›æ¥å¤„ç†ç±»ä¼¼çš„äº‹æƒ…ï¼Œä¾‹å¦‚æ³¨æ„ç¥ç»è¿‡ç¨‹ï¼Œå¾ˆå¤šæ—¶å€™ä»–ä»¬åªä½¿ç”¨å•å±‚ä½œä¸ºè¡¨ç¤ºæŸ¥æ‰¾ï¼Œæˆ‘ä»¬è®¤ä¸ºè¿™å®é™…ä¸Šé™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›ï¼Œé€šè¿‡å¤šæ¬¡å †å ï¼Œä½ å¯ä»¥å­¦ä¹ æ•°æ®ç‚¹ä¹‹é—´æ›´å¤æ‚çš„å…³ç³»ã€‚
- en: On any lots of questionsã€‚So you can go ahead firstã€‚OhCool thanks I have a question
    about like how you guys do the embedding is there always like arties like convolutional
    filters or like linear layers like what is the type of embedding that you guys
    use Yeah so i'm attempting to go back to the slide I think it's not not very happy
    with linear now but yeah so for for Tular data we did just linear embeddings actually
    so weã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šé—®é¢˜ã€‚é‚£ä¹ˆä½ å¯ä»¥å…ˆé—®ã€‚å“¦ï¼Œå¤ªå¥½äº†ï¼Œè°¢è°¢ï¼Œæˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œæ¯”å¦‚ä½ ä»¬å¦‚ä½•è¿›è¡ŒåµŒå…¥ï¼Œæ˜¯å¦æ€»æ˜¯åƒå·ç§¯æ»¤æ³¢å™¨æˆ–çº¿æ€§å±‚è¿™æ ·çš„å±æ€§ï¼Œæˆ–è€…ä½ ä»¬ä½¿ç”¨çš„åµŒå…¥ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯çš„ï¼Œæˆ‘å°è¯•å›åˆ°å¹»ç¯ç‰‡ï¼Œæˆ‘è®¤ä¸ºå¯¹çº¿æ€§ç°åœ¨ä¸æ˜¯å¾ˆæ»¡æ„ï¼Œä½†å¯¹äºTularæ•°æ®ï¼Œæˆ‘ä»¬å®é™…ä¸Šåªç”¨äº†çº¿æ€§åµŒå…¥ã€‚
- en: You knowï¼Œ we we could get into likeï¼Œ I guess details of feturization for categorical
    and continuousã€‚but it's literally likeï¼Œ say for categoricalï¼Œ you knowã€‚you do a
    one hot encoding and then you learn this embedding that is specific to that attribute
    and then for numericalã€‚I believe we were just standard onizing for the image data
    we did end up usingã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥è®¨è®ºç±»åˆ«å’Œè¿ç»­ç‰¹å¾åŒ–çš„ç»†èŠ‚ï¼Œä½†å®é™…ä¸Šï¼Œå¯¹äºç±»åˆ«ç‰¹å¾ï¼Œä½ ä¼šè¿›è¡Œç‹¬çƒ­ç¼–ç ï¼Œç„¶åå­¦ä¹ ä¸è¯¥å±æ€§ç‰¹å®šçš„åµŒå…¥ï¼›è€Œå¯¹äºæ•°å€¼ç‰¹å¾ï¼Œæˆ‘ç›¸ä¿¡æˆ‘ä»¬åªæ˜¯è¿›è¡Œäº†æ ‡å‡†åŒ–ï¼Œå¯¹äºå›¾åƒæ•°æ®æˆ‘ä»¬æœ€ç»ˆä½¿ç”¨äº†ã€‚
- en: A Resnet 18 encoder for C4 10ã€‚howeverï¼Œ I think that I meanã€‚we'll discuss that
    a bit later in resultsï¼Œ but that embedding is a bit arbitraryã€‚you can sort of
    do whatever the key part of the architecture is the attention between data pointsã€‚So
    it's in terms of how you actually want to embed each attributeã€‚ it's kind of up
    to youã€‚Thanksã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªç”¨äºC4 10çš„Resnet 18ç¼–ç å™¨ã€‚ç„¶è€Œï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬ä¼šåœ¨ç»“æœä¸­ç¨åè®¨è®ºè¿™ä¸€ç‚¹ï¼Œä½†è¿™ä¸ªåµŒå…¥æœ‰ç‚¹ä»»æ„ã€‚ä½ å¯ä»¥åšä»»ä½•äº‹æƒ…ï¼Œæ¶æ„çš„å…³é”®éƒ¨åˆ†æ˜¯æ•°æ®ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚æ‰€ä»¥åœ¨ä½ å®é™…æƒ³è¦åµŒå…¥æ¯ä¸ªå±æ€§æ—¶ï¼Œè¿™å–å†³äºä½ ã€‚è°¢è°¢ã€‚
- en: I think another questionï¼Œ same question as Victor Tosã€‚Awesomeã€‚Coolã€‚so here we
    have attention between data points doneã€‚so we can also do this attention between
    attributesã€‚So we reshape back to this N by D by E representation and then we can
    just apply self- attention independently to each row in other words to a single
    data point and the intuition for why we would kind of do this nested type idea
    where we switch between attention between data points and attention between attributes
    is just we're trying to learn better per data point representations for the between
    data point interactions this is literally just normal self-attenã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºå¦ä¸€ä¸ªé—®é¢˜ï¼Œå’ŒVictor Tosçš„é—®é¢˜ä¸€æ ·ã€‚å¤ªæ£’äº†ã€‚è¿™é‡Œæˆ‘ä»¬å®Œæˆäº†æ•°æ®ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚é‚£ä¹ˆæˆ‘ä»¬ä¹Ÿå¯ä»¥åœ¨å±æ€§ä¹‹é—´è¿›è¡Œè¿™ç§æ³¨æ„åŠ›ã€‚æˆ‘ä»¬å°†å…¶é‡å¡‘ä¸ºNä¹˜Dä¹˜Eçš„è¡¨ç¤ºï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ç‹¬ç«‹åœ°å¯¹æ¯ä¸€è¡Œåº”ç”¨è‡ªæ³¨æ„åŠ›ï¼Œæ¢å¥è¯è¯´ï¼Œå¯¹å•ä¸ªæ•°æ®ç‚¹è¿›è¡Œè‡ªæ³¨æ„åŠ›ã€‚æˆ‘ä»¬è¿™æ ·è¿›è¡ŒåµŒå¥—çš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬è¯•å›¾ä¸ºæ•°æ®ç‚¹ä¹‹é—´çš„äº¤äº’å­¦ä¹ æ›´å¥½çš„æ¯ä¸ªæ•°æ®ç‚¹è¡¨ç¤ºï¼Œè¿™å®é™…ä¸Šåªæ˜¯æ™®é€šçš„è‡ªæ³¨æ„åŠ›ã€‚
- en: as you'd see in language modeling or image classification the attributes are
    the tokens hereã€‚And finallyï¼Œ we just minins and repeatã€‚So what are we actually
    getting out of this to summarize we're learning higher order relationships between
    data pointsï¼Ÿ
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚ä½ åœ¨è¯­è¨€å»ºæ¨¡æˆ–å›¾åƒåˆ†ç±»ä¸­çœ‹åˆ°çš„ï¼Œå±æ€§åœ¨è¿™é‡Œæ˜¯ä»¤ç‰Œã€‚æœ€åï¼Œæˆ‘ä»¬åªéœ€æŒ–æ˜å¹¶é‡å¤ã€‚æ‰€ä»¥æˆ‘ä»¬ç©¶ç«Ÿä»ä¸­å¾—åˆ°ä»€ä¹ˆï¼Œæ€»ç»“ä¸€ä¸‹ï¼Œæˆ‘ä»¬åœ¨å­¦ä¹ æ•°æ®ç‚¹ä¹‹é—´çš„é«˜é˜¶å…³ç³»ï¼Ÿ
- en: We're learning transformations of individual data pointsã€‚And then importantlyã€‚NPT
    is equivariant to a permutation of the data pointsã€‚this basically just reflects
    the intuition that the learned relationships between the data points should not
    depend on the ordering in which you receive them or in which you observe your
    data setã€‚The third key component of NPptTs is a masking based string objectiveã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å­¦ä¹ å•ä¸ªæ•°æ®ç‚¹çš„å˜æ¢ã€‚é‡è¦çš„æ˜¯ï¼ŒNPTå¯¹æ•°æ®ç‚¹çš„æ’åˆ—æ˜¯ç­‰å˜çš„ã€‚è¿™åŸºæœ¬ä¸Šåæ˜ äº†ä¸€ä¸ªç›´è§‰ï¼šæ•°æ®ç‚¹ä¹‹é—´å­¦ä¹ åˆ°çš„å…³ç³»ä¸åº”è¯¥ä¾èµ–äºä½ æ¥æ”¶å®ƒä»¬çš„é¡ºåºæˆ–è§‚å¯Ÿæ•°æ®é›†çš„é¡ºåºã€‚NPptTsçš„ç¬¬ä¸‰ä¸ªå…³é”®ç»„æˆéƒ¨åˆ†æ˜¯åŸºäºæ©ç çš„å­—ç¬¦ä¸²ç›®æ ‡ã€‚
- en: So recall that what we're trying to do is we're trying to predict missing entries
    from observed entries and those mass values can be both features or targets so
    again the classic use say mass language modeling is to do self-supervised learning
    on a sequence of tokens which you could think of as just having features in our
    setting ours is a bit different in that we do stochastic feature masking to mass
    feature values with a probability piece sub future and then we also do this masking
    of training targets with this probability piece subtart so if we write out the
    training objectiveã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å›æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»è§‚å¯Ÿåˆ°çš„æ¡ç›®ä¸­é¢„æµ‹ç¼ºå¤±çš„æ¡ç›®ï¼Œè€Œè¿™äº›ç¼ºå¤±çš„å€¼å¯ä»¥æ˜¯ç‰¹å¾æˆ–ç›®æ ‡ã€‚å› æ­¤ï¼Œç»å…¸çš„åº”ç”¨ï¼Œæ¯”å¦‚æ©ç è¯­è¨€å»ºæ¨¡ï¼Œå°±æ˜¯å¯¹ä¸€ç³»åˆ—æ ‡è®°è¿›è¡Œè‡ªç›‘ç£å­¦ä¹ ï¼Œä½ å¯ä»¥è®¤ä¸ºåœ¨æˆ‘ä»¬çš„è®¾å®šä¸­ï¼Œå®ƒä¸ç‰¹å¾ç•¥æœ‰ä¸åŒï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨éšæœºç‰¹å¾æ©ç ä»¥æŸä¸ªæ¦‚ç‡æ©ç›–ç‰¹å¾å€¼ï¼Œç„¶åæˆ‘ä»¬è¿˜ä½¿ç”¨è¿™ä¸ªæ¦‚ç‡å¯¹è®­ç»ƒç›®æ ‡è¿›è¡Œæ©ç ã€‚å¦‚æœæˆ‘ä»¬å†™å‡ºè®­ç»ƒç›®æ ‡ã€‚
- en: We are just taking a weighted sum of the negative log likelihood loss from targets
    as well as from featuresã€‚and of course at test time we're only going to mask and
    compute a loss over the targets of test pointsã€‚So to break this down a bit further
    and point out some of the cool parts of it hereã€‚the thing that's highlighted right
    now on the far right is the term relating to the featuresã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»…ä»…æ˜¯å¯¹ç›®æ ‡å’Œç‰¹å¾çš„è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±è¿›è¡ŒåŠ æƒæ±‚å’Œã€‚å½“ç„¶ï¼Œåœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬åªä¼šå¯¹æµ‹è¯•ç‚¹çš„ç›®æ ‡è¿›è¡Œæ©ç å’Œè®¡ç®—æŸå¤±ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ†è§£è¿™ä¸€ç‚¹å¹¶æŒ‡å‡ºä¸€äº›æœ‰è¶£çš„éƒ¨åˆ†ï¼Œç°åœ¨å³ä¾§çªå‡ºæ˜¾ç¤ºçš„å°±æ˜¯ä¸ç‰¹å¾ç›¸å…³çš„æœ¯è¯­ã€‚
- en: it's the feature maskingï¼Œ basically we find that this has a nice regularizing
    effect more or less the model can now predict anywhere and makes the task a bit
    harder and introduce some more supervision and we found in an ablation for the
    tabular dataset sets that it helped for eight of 1 of thoseã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ç‰¹å¾æ©ç ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬å‘ç°è¿™æœ‰ä¸€ä¸ªè‰¯å¥½çš„æ­£åˆ™åŒ–æ•ˆæœï¼Œæ¨¡å‹ç°åœ¨å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹è¿›è¡Œé¢„æµ‹ï¼Œä½¿å¾—ä»»åŠ¡å˜å¾—ç¨å¾®å›°éš¾ä¸€äº›ï¼Œå¹¶å¼•å…¥æ›´å¤šçš„ç›‘ç£ã€‚åœ¨å¯¹è¡¨æ ¼æ•°æ®é›†è¿›è¡Œæ¶ˆèå®éªŒæ—¶ï¼Œæˆ‘ä»¬å‘ç°è¿™å¯¹å…«ä¸ªæ•°æ®é›†çš„æ•ˆæœéƒ½å¾ˆå¥½ã€‚
- en: And then there's this other termï¼Œ which is kind of interesting it's this stochastic
    target masking and the idea is thatã€‚You're actually going to have some training
    targets unmasked to the model at input at training timeã€‚which means that the NPptT
    can learn to predict the mask targets of certain training data points using the
    targets of other training data points as well as all of the training featuresã€‚And
    so that means you don't actually need to memorize a mapping between training inputs
    and outputs in your parametersã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿˜æœ‰å¦ä¸€ä¸ªæœ‰è¶£çš„æœ¯è¯­ï¼Œè¿™å°±æ˜¯éšæœºç›®æ ‡æ©ç ï¼Œæƒ³æ³•æ˜¯ï¼šåœ¨è®­ç»ƒæ—¶ï¼ŒæŸäº›è®­ç»ƒç›®æ ‡å®é™…ä¸Šä¸ä¼šè¢«æ¨¡å‹æ©ç›–ã€‚è¿™æ„å‘³ç€NPptTå¯ä»¥åˆ©ç”¨å…¶ä»–è®­ç»ƒæ•°æ®ç‚¹çš„ç›®æ ‡ä»¥åŠæ‰€æœ‰è®­ç»ƒç‰¹å¾æ¥é¢„æµ‹æŸäº›è®­ç»ƒæ•°æ®ç‚¹çš„æ©ç ç›®æ ‡ã€‚å› æ­¤ï¼Œè¿™æ„å‘³ç€ä½ å®é™…ä¸Šä¸éœ€è¦åœ¨å‚æ•°ä¸­è®°å¿†è®­ç»ƒè¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„æ˜ å°„ã€‚
- en: You can instead devote the representational capacity of the model to learn functions
    that use other training features and targets as inputã€‚So this is kind of getting
    into the idea of this sort of like learn K and N ideaã€‚You knowã€‚obviouslyï¼Œ we can
    be learn more complexï¼Œ relational lookups and those sorts of things from thisã€‚But
    you can imagine one such you know case beingï¼Œ we have a bunch of test data points
    coming inã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†æ¨¡å‹çš„è¡¨ç¤ºèƒ½åŠ›ç”¨äºå­¦ä¹ ä½¿ç”¨å…¶ä»–è®­ç»ƒç‰¹å¾å’Œç›®æ ‡ä½œä¸ºè¾“å…¥çš„å‡½æ•°ã€‚å› æ­¤ï¼Œè¿™æœ‰ç‚¹æ¶‰åŠåˆ°ç±»ä¼¼å­¦ä¹ Kå’ŒNçš„æƒ³æ³•ã€‚æ˜¾ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸­å­¦ä¹ æ›´å¤æ‚çš„å…³ç³»æŸ¥æ‰¾ç­‰å†…å®¹ã€‚ä½†ä½ å¯ä»¥æƒ³è±¡ä¸€ä¸ªè¿™æ ·çš„æ¡ˆä¾‹ï¼šæˆ‘ä»¬æœ‰ä¸€å †æµ‹è¯•æ•°æ®ç‚¹è¿›å…¥ã€‚
- en: We're going to look at their features and use that to assign them to clusters
    of training data points and then our prediction for those points is just going
    to be an interpolation of the training targets in that respective cluster that's
    like an example of something that this mechanism lets NPptTs learnã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æŸ¥çœ‹å®ƒä»¬çš„ç‰¹å¾ï¼Œå¹¶åˆ©ç”¨è¿™äº›ç‰¹å¾å°†å…¶åˆ†é…åˆ°è®­ç»ƒæ•°æ®ç‚¹çš„èšç±»ä¸­ï¼Œç„¶åæˆ‘ä»¬å¯¹è¿™äº›ç‚¹çš„é¢„æµ‹ä»…ä»…æ˜¯ç›¸åº”èšç±»ä¸­è®­ç»ƒç›®æ ‡çš„æ’å€¼ï¼Œè¿™å°±åƒæ˜¯è¿™ä¸ªæœºåˆ¶è®©NPptTså­¦ä¹ çš„ä¸€ä¸ªä¾‹å­ã€‚
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_20.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_20.png)'
- en: All rightï¼Œ so if there's any questionsï¼Œ we can take them nowã€‚otherwise I'm happy
    to take them in the discussion or somethingã€‚All rightã€‚So let's discuss go for
    it curious when you're using the entire data setã€‚Is that limit the type of data
    sets you can use because of the sizeï¼ŸYeahï¼Œ so in practiceã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥æé—®ã€‚å¦åˆ™ï¼Œæˆ‘å¾ˆä¹æ„åœ¨è®¨è®ºä¸­æˆ–è€…å…¶ä»–æ—¶å€™å›ç­”ã€‚å¥½çš„ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬è®¨è®ºä¸€ä¸‹ï¼Œå½“ä½ ä½¿ç”¨æ•´ä¸ªæ•°æ®é›†æ—¶ï¼Œè¿™æ˜¯å¦é™åˆ¶äº†ä½ èƒ½ä½¿ç”¨çš„æ•°æ®é›†ç±»å‹ï¼Œå› ä¸ºæ•°æ®é‡çš„å¤§å°ï¼Ÿæ˜¯çš„ï¼Œåœ¨å®è·µä¸­ã€‚
- en: we do random mini batchching as an approximationï¼Œ so the idea is just you know
    if you have a reasonably large mini batchã€‚you're going to benefit a bit from still
    having kind of this lookup ability because if youre reasonable number of classesã€‚probably
    you're going to be able to learn some you know interesting mappings based on features
    and targets amongst those classes we found in practice that you know and we'll
    get into this a little bitã€‚but we do actually indeed learn to use relationships
    between data points on prediction for data sets where we're doing mini batchching
    and we also didn't necessarily find that you need like a ludicrously large batch
    size for this to be a thing but I do think it's just this is in general and important
    point and it's one that points us towards looking into say you know sparse transformers
    literature for trying to expand to some larger data sets without having the mini
    batchching assumptionã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿›è¡Œéšæœºå°æ‰¹é‡å¤„ç†ä½œä¸ºä¸€ç§è¿‘ä¼¼ï¼Œæ‰€ä»¥è¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªç›¸å¯¹è¾ƒå¤§çš„å°æ‰¹é‡ï¼Œä½ ä¼šä»ä¸­å—ç›Šï¼Œå› ä¸ºå¦‚æœç±»åˆ«æ•°é‡åˆç†ï¼Œä½ å¯èƒ½ä¼šåŸºäºç‰¹å¾å’Œç›®æ ‡å­¦åˆ°ä¸€äº›æœ‰è¶£çš„æ˜ å°„ã€‚åœ¨å®è·µä¸­æˆ‘ä»¬å‘ç°ï¼Œç¡®å®èƒ½å¤Ÿåˆ©ç”¨æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»è¿›è¡Œé¢„æµ‹ï¼Œå¯¹äºæˆ‘ä»¬è¿›è¡Œå°æ‰¹é‡å¤„ç†çš„æ•°æ®é›†ï¼Œæˆ‘ä»¬ä¹Ÿå¹¶æ²¡æœ‰å‘ç°éœ€è¦éå¸¸å¤§çš„æ‰¹é‡å¤§å°æ‰èƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªæ™®éä¸”é‡è¦çš„è§‚ç‚¹ï¼Œè¿™ä¹ŸæŒ‡å¼•æˆ‘ä»¬å»ç ”ç©¶ç¨€ç–å˜æ¢å™¨æ–‡çŒ®ï¼Œä»¥å°è¯•åœ¨ä¸å‡è®¾å°æ‰¹é‡å¤„ç†çš„æƒ…å†µä¸‹æ‰©å±•åˆ°ä¸€äº›æ›´å¤§çš„æ•°æ®é›†ã€‚
- en: Greatï¼Œ thank youã€‚If I can add a number to thatï¼Œ we can without mini batchching
    accommodate data sets off around like 8000 points or soã€‚so that already accounts
    for a fair proportion I would say of the tabular data sets out there but we also
    do data sets with  11 million points where obviously we then resort two mini batchching
    so it's very good to have like an idea of the sizes that we're talking aboutã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œè°¢è°¢ã€‚å¦‚æœæˆ‘å¯ä»¥åŠ ä¸€å¥ï¼Œæˆ‘ä»¬åœ¨æ²¡æœ‰å°æ‰¹é‡å¤„ç†çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å®¹çº³å¤§çº¦8000ä¸ªç‚¹çš„æ•°æ®é›†ã€‚æ‰€ä»¥è¿™å·²ç»å æ®äº†ç›¸å½“ä¸€éƒ¨åˆ†æˆ‘è®¤ä¸ºçš„è¡¨æ ¼æ•°æ®é›†ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¤„ç†äº†1100ä¸‡ä¸ªç‚¹çš„æ•°æ®é›†ï¼Œæ˜¾ç„¶æˆ‘ä»¬é‚£æ—¶å°±éœ€è¦
    resort åˆ°å°æ‰¹é‡å¤„ç†ï¼Œæ‰€ä»¥äº†è§£æˆ‘ä»¬è®¨è®ºçš„æ•°æ®é›†å¤§å°æ˜¯éå¸¸é‡è¦çš„ã€‚
- en: I'm curious on thatï¼Œ I mean it's pretty excitingï¼Œ I feel like you don't normally
    hear aboutã€‚Transformers being applied to the sets sub size 8000ã€‚Uã€‚I'm curious
    and we could talk about this sort of later once we've covered the other material
    if you found that sample efficiency is one of the key gains here or just experience
    working on small data of transformers generally and yeahã€‚but I'll happy to put
    the answer to that until after as part of thisã€‚ğŸ˜Šï¼ŒYeahï¼Œ I I think that'd beã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯¹æ­¤å¾ˆæ„Ÿå…´è¶£ï¼Œæˆ‘è§‰å¾—è¿™å¾ˆä»¤äººå…´å¥‹ï¼Œæˆ‘æ„Ÿè§‰ä½ é€šå¸¸ä¸ä¼šå¬åˆ°å˜æ¢å™¨è¢«åº”ç”¨äºå°äº8000çš„å­é›†ã€‚Uã€‚æˆ‘å¾ˆå¥½å¥‡ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¦†ç›–å…¶ä»–ææ–™åå†è®¨è®ºä¸€ä¸‹ï¼Œå¦‚æœä½ å‘ç°æ ·æœ¬æ•ˆç‡æ˜¯è¿™é‡Œçš„å…³é”®æ”¶ç›Šä¹‹ä¸€ï¼Œæˆ–è€…åªæ˜¯ä¸€èˆ¬åœ¨å°æ•°æ®ä¸Šå·¥ä½œçš„å˜æ¢å™¨çš„ç»éªŒã€‚ä¸è¿‡ï¼Œæˆ‘å¾ˆä¹æ„å°†è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆç•™åˆ°åé¢ã€‚ğŸ˜Šï¼Œæ˜¯çš„ï¼Œæˆ‘æƒ³è¿™ä¼šå¾ˆä¸é”™ã€‚
- en: that'd be really nice to talk about a bitã€‚And it was something that in generalï¼Œ
    I guessï¼Œ I'd sayã€‚was surprising to us in terms of how robust NPTs were on small
    data sets and how we surprisingly didn't have to tune a terrible number of parametersã€‚But
    we can get into details in a bitã€‚ğŸ˜Šï¼Œawesomeã€‚So to get into the experimentsã€‚we focused
    a lot on tabular data because it's a very general settingã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœŸçš„å¾ˆä¸é”™ï¼Œå¯ä»¥èŠä¸€èŠã€‚è€Œä¸”ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘æƒ³æˆ‘ä¼šè¯´ï¼ŒNPTåœ¨å°æ•°æ®é›†ä¸Šæ˜¯å¤šä¹ˆç¨³å¥ï¼Œä»¤æˆ‘ä»¬æƒŠè®¶çš„æ˜¯æˆ‘ä»¬å¹¶æ²¡æœ‰éœ€è¦è°ƒæ•´å¾ˆå¤šå‚æ•°ã€‚ä½†æˆ‘ä»¬å¯ä»¥ç¨åæ·±å…¥ç»†èŠ‚ã€‚ğŸ˜Šï¼Œå¤ªæ£’äº†ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬å¼€å§‹å®éªŒã€‚æˆ‘ä»¬éå¸¸å…³æ³¨è¡¨æ ¼æ•°æ®ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸é€šç”¨çš„è®¾ç½®ã€‚
- en: and it's also notoriously challenging for deep learningã€‚So we knowï¼Œ you knowã€‚treebasedase
    boosting methodsï¼Œ stuff like X boost is very dominant and this is also a very
    relevant domain toã€‚I think people in industry and that sort of thingã€‚So we were
    excited about the idea of trying to do better on thisã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨æ·±åº¦å­¦ä¹ ä¸­ä¹Ÿæ˜¯ä¼—æ‰€å‘¨çŸ¥çš„æŒ‘æˆ˜ã€‚å› æ­¤æˆ‘ä»¬çŸ¥é“ï¼ŒåŸºäºæ ‘çš„æå‡æ–¹æ³•ï¼Œå¦‚XGBoostéå¸¸å ä¸»å¯¼åœ°ä½ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªä¸è¡Œä¸šç›¸å…³çš„é¢†åŸŸã€‚æ‰€ä»¥æˆ‘ä»¬å¯¹åœ¨è¿™ä¸€ç‚¹ä¸Šåšå¾—æ›´å¥½æ„Ÿåˆ°å…´å¥‹ã€‚
- en: So we chose a broad selection of data sets varying across a few different dimensionsã€‚you
    know as we mentionedï¼Œ you know on the order of hundreds to tens of millions of
    instances broad range in the number of features in the composition of features
    in terms of being categorical or continuous various types of tasksã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬é€‰æ‹©äº†å¹¿æ³›çš„æ•°æ®é›†ï¼Œæ¶µç›–å‡ ä¸ªä¸åŒçš„ç»´åº¦ã€‚ä½ çŸ¥é“çš„ï¼Œæ­£å¦‚æˆ‘ä»¬æåˆ°çš„ï¼Œä»æ•°ç™¾åˆ°æ•°åƒä¸‡ä¸ªå®ä¾‹ï¼Œç‰¹å¾æ•°é‡çš„å¹¿æ³›èŒƒå›´ï¼Œç‰¹å¾ç»„æˆåœ¨ç±»åˆ«æˆ–è¿ç»­æ€§æ–¹é¢ï¼Œæ¶‰åŠå„ç§ç±»å‹çš„ä»»åŠ¡ã€‚
- en: binary and multiclass classification as well as regression And like I saidã€‚the
    baselines were kind of the usual suspects for tabular dataï¼Œ X boost cat boostã€‚byGBM
    to MLPs and Tnetï¼Œ which is a transformer architecture for tabular dataã€‚![](img/1777b223f7bf8be27a8767d209c7f271_22.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ä»¥åŠå›å½’ã€‚æ­£å¦‚æˆ‘æ‰€è¯´ï¼ŒåŸºçº¿æ˜¯é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„å¸¸è§æ–¹æ³•ï¼ŒXGBoostã€CatBoostã€GBM åˆ° MLP å’Œ Tnetï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹è¡¨æ ¼æ•°æ®çš„å˜æ¢å™¨æ¶æ„ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_22.png)
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_23.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_23.png)'
- en: So to get into the results here I'm showing the average rank for the various
    subtasks we did well in terms of rankwise performance against methods like cat
    boostost and X boostt which are designed specifically for Tular data and in fact
    we find that NPT is the top performer on four of the 10 of these data sets on
    image data I mentioned that we used a CNN encoder and with that we were performing
    well on C410ã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¸ºäº†å¾—åˆ°è¿™é‡Œçš„ç»“æœï¼Œæˆ‘å±•ç¤ºäº†æˆ‘ä»¬åœ¨å„ç§å­ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½çš„å¹³å‡æ’åï¼Œé’ˆå¯¹åƒ CatBoost å’Œ XGBoost è¿™æ ·çš„ä¸“ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡çš„æ–¹æ³•ï¼Œäº‹å®ä¸Šæˆ‘ä»¬å‘ç°
    NPT åœ¨è¿™åä¸ªæ•°æ®é›†ä¸­å››ä¸ªæ˜¯è¡¨ç°æœ€å¥½çš„ï¼Œåœ¨æˆ‘æåˆ°çš„å›¾åƒæ•°æ®ä¸Šï¼Œæˆ‘ä»¬ä½¿ç”¨äº† CNN ç¼–ç å™¨ï¼Œå› æ­¤æˆ‘ä»¬åœ¨ C410 ä¸Šè¡¨ç°è‰¯å¥½ã€‚
- en: And we also think thatï¼Œ you knowï¼Œ in generalï¼Œ like withï¼Œ let's sayã€‚new work
    on image transformers on small dataï¼Œ this can probably just be done with linear
    patchingã€‚And so this kind of the manner in which you're embedding things is probably
    not the keyã€‚Neilã€‚if I can jump in with two questionsã€‚Can you go back two slides
    firstï¼Ÿ
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿè®¤ä¸ºï¼Œé€šå¸¸æ¥è¯´ï¼Œå‡è®¾è¯´ï¼Œåœ¨å°æ•°æ®ä¸Šçš„å›¾åƒå˜æ¢å™¨çš„æ–°å·¥ä½œï¼Œè¿™å¯èƒ½ä»…ä»…é€šè¿‡çº¿æ€§æ‹¼æ¥å®Œæˆã€‚æ‰€ä»¥ï¼Œè¿™ç§åµŒå…¥çš„æ–¹å¼å¯èƒ½ä¸æ˜¯å…³é”®ã€‚å°¼å°”ã€‚å¦‚æœæˆ‘å¯ä»¥æ’å…¥ä¸¤ä¸ªé—®é¢˜ã€‚ä½ èƒ½å…ˆè¿”å›ä¸¤å¼ å¹»ç¯ç‰‡å—ï¼Ÿ
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_25.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_25.png)'
- en: One is just a small minor pointï¼Œ back one more pleaseã€‚Thank youã€‚here for the
    featuresï¼Œ 50 plusã€‚what does plus mean hereï¼ŸI'll have to double check what the
    exact number is I'm pretty sure it's probably around 50ã€‚I would guess like so
    the 50 is really a order of it's not like 150 or 5000ã€‚Yesï¼Œ yeahã€‚ I meanã€‚I I'll
    double check for youï¼Œ or you can check with the the metadata statistics at the
    end of the paperã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ç‚¹å°é—®é¢˜ï¼Œè¯·å†é€€å›ä¸€å¼ ã€‚è°¢è°¢ã€‚åœ¨è¿™é‡Œï¼Œç‰¹å¾æ˜¯ 50 ä»¥ä¸Šã€‚è¿™é‡Œçš„ plus æ„å‘³ç€ä»€ä¹ˆï¼Ÿæˆ‘å¾—å†ç¡®è®¤ä¸€ä¸‹ç¡®åˆ‡çš„æ•°å­—ï¼Œæˆ‘å¾ˆç¡®å®šå®ƒå¤§çº¦åœ¨ 50 å·¦å³ã€‚æˆ‘çŒœæ˜¯è¿™æ ·çš„ï¼Œæ‰€ä»¥è¿™ä¸ª
    50 å…¶å®æ˜¯ä¸ªå¤§æ¦‚çš„æ•°é‡ï¼Œä¸æ˜¯è¯´ 150 æˆ– 5000ã€‚æ˜¯çš„ï¼Œæ²¡é”™ã€‚æˆ‘ä¼šä¸ºä½ å†ç¡®è®¤ä¸€ä¸‹ï¼Œæˆ–è€…ä½ å¯ä»¥åœ¨è®ºæ–‡æœ«å°¾çš„å…ƒæ•°æ®ç»Ÿè®¡ä¸­æŸ¥çœ‹ã€‚
- en: But noï¼Œ it wasn't like you knowï¼Œ arbitrarily largeï¼Œ I would sayï¼Œ thoughã€‚You
    knowã€‚we did these ablations on whether or not we actually need attention between
    attributesã€‚We did find that this ended ended up benefiting usï¼Œ but you could perhaps
    do kind of sayã€‚just an MLP embedding in that dimension and go to like a relatively
    small number of hidden dimensions and fit kind of an arbitrary number of featuresã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¸æ˜¯çš„ï¼Œå®ƒå¹¶ä¸æ˜¯è¯´ï¼Œéšä¾¿æœ‰å¤šå¤§ï¼Œæˆ‘ä¼šè¿™ä¹ˆè¯´ã€‚ä½ çŸ¥é“çš„ã€‚æˆ‘ä»¬è¿›è¡Œäº†ä¸€äº›å®éªŒï¼Œçœ‹çœ‹æ˜¯å¦çœŸçš„éœ€è¦å±æ€§ä¹‹é—´çš„æ³¨æ„åŠ›ã€‚æˆ‘ä»¬å‘ç°è¿™ç¡®å®å¯¹æˆ‘ä»¬æœ‰å¸®åŠ©ï¼Œä½†ä½ ä¹Ÿå¯ä»¥è¯´ï¼Œå•çº¯åœ¨é‚£ä¸ªç»´åº¦ä¸Šä½¿ç”¨ä¸€ç§
    MLP åµŒå…¥ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªç›¸å¯¹è¾ƒå°çš„éšè—ç»´åº¦ï¼Œé€‚åº”ä»»æ„æ•°é‡çš„ç‰¹å¾ã€‚
- en: So I think thatï¼Œ yeahï¼Œ if youï¼Œ if you kind of relax the necessity of attention
    between attributesã€‚you can probably scale out at least that dimension quite a
    lotã€‚Okayï¼Œ and then my second questionã€‚if you could go forward one slideã€‚![](img/1777b223f7bf8be27a8767d209c7f271_27.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è®¤ä¸ºï¼Œæ˜¯çš„ï¼Œå¦‚æœä½ æ”¾æ¾å±æ€§ä¹‹é—´çš„æ³¨æ„åŠ›éœ€æ±‚ï¼Œä½ å¯èƒ½ä¼šåœ¨è‡³å°‘é‚£ä¸ªç»´åº¦ä¸Šæ‰©å±•å¾ˆå¤šã€‚å¥½çš„ï¼Œç„¶åæˆ‘çš„ç¬¬äºŒä¸ªé—®é¢˜ã€‚å¦‚æœä½ èƒ½å¾€å‰æ¨è¿›ä¸€å¼ å¹»ç¯ç‰‡ã€‚![](img/1777b223f7bf8be27a8767d209c7f271_27.png)
- en: Thank you hereï¼Œ I'm not sure I quite caughtã€‚what does four of 10 data setsã€‚2
    of 10 data sets and four of 10 meanã€‚This is of theï¼Œ of all the tabular data sets
    that we hadã€‚So ohï¼Œ I you think binaryer classification is for I seeï¼Œ okayã€‚Yeahï¼Œ
    exactlyã€‚Awesomeã€‚Any other questionsï¼ŸThe standard errors here because I meanï¼Œ there's
    like there's just 10 data setsã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: è°¢è°¢ä½ ï¼Œæˆ‘ä¸å¤ªæ˜ç™½ã€‚å››ä¸ªæ•°æ®é›†ä¸­çš„åä¸ªï¼Œåä¸ªæ•°æ®é›†ä¸­çš„ä¸¤ä¸ªï¼Œå››ä¸ªæ•°æ®é›†æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿè¿™æ˜¯å…³äºæˆ‘ä»¬æ‰€æ‹¥æœ‰çš„æ‰€æœ‰è¡¨æ ¼æ•°æ®é›†ã€‚æ‰€ä»¥å“¦ï¼Œæˆ‘æƒ³äºŒåˆ†ç±»æ˜¯é’ˆå¯¹çš„ï¼Œæˆ‘æ˜ç™½äº†ï¼Œå¥½çš„ã€‚æ˜¯çš„ï¼Œæ­£æ˜¯å¦‚æ­¤ã€‚è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿè¿™é‡Œçš„æ ‡å‡†è¯¯å·®ï¼Œå› ä¸ºæˆ‘æƒ³ï¼Œåªæœ‰åä¸ªæ•°æ®é›†ã€‚
- en: rightã€‚Yeahï¼Œ correctã€‚1010 total tabular data setsã€‚Yeahï¼Œ but these are rank goodã€‚Yeahï¼Œ
    these areã€‚these are rankwise performanceï¼Œ correctï¼Œ Okayï¼Œ I'm justingã€‚How theã€‚Where
    the uncertainty comes from in this caseï¼ŸYeahã€‚average averaged over four of 10
    data sets the rank so for each particular data set we have a rank of all the different
    methods then we take the average and the very answer of the rankings within each
    of the types of task within binary classification within multiclassã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ã€‚æ˜¯çš„ï¼Œæ­£ç¡®ã€‚å…±æœ‰1010ä¸ªè¡¨æ ¼æ•°æ®é›†ã€‚æ˜¯çš„ï¼Œä½†è¿™äº›æ˜¯æ’åå¥½çš„ã€‚æ˜¯çš„ï¼Œè¿™äº›æ˜¯æ’åè¡¨ç°ï¼Œæ­£ç¡®ï¼Œå¥½çš„ï¼Œæˆ‘åªæ˜¯æƒ³çŸ¥é“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ç¡®å®šæ€§æ¥æºäºå“ªé‡Œï¼Ÿæ˜¯çš„ã€‚å¯¹å››ä¸ªæ•°æ®é›†çš„å¹³å‡æ’åï¼Œæ‰€ä»¥å¯¹äºæ¯ä¸ªç‰¹å®šæ•°æ®é›†ï¼Œæˆ‘ä»¬æœ‰æ‰€æœ‰ä¸åŒæ–¹æ³•çš„æ’åï¼Œç„¶åæˆ‘ä»¬å–å¹³å‡å€¼ï¼Œä»¥åŠæ¯ç§ä»»åŠ¡ç±»å‹åœ¨äºŒå…ƒåˆ†ç±»å’Œå¤šç±»ä¸­çš„æ’åå·®å¼‚ã€‚
- en: etcã€‚Goodã€‚We alsoï¼Œ if you're curiousï¼Œ you knowï¼Œ have the full results in the
    paperã€‚Yeahï¼Œ thank youã€‚We also have a couple of questions more some questionsã€‚Hey
    yeahï¼Œ thanksã€‚I guess I just found it a little surprising that the worst performer
    was KNN given that it's also nonparmetricã€‚I guess could you comment on that and
    yeahï¼Œ is it that there's something like intrinsic to the NPT that makes it just
    exceptional far beyond other nonparmetric methods or yeahã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ç­‰ç­‰ã€‚å¾ˆå¥½ã€‚å¦‚æœä½ æ„Ÿå…´è¶£çš„è¯ï¼Œæˆ‘ä»¬åœ¨è®ºæ–‡ä¸­ä¹Ÿæœ‰å®Œæ•´çš„ç»“æœã€‚æ˜¯çš„ï¼Œè°¢è°¢ã€‚æˆ‘ä»¬è¿˜æœ‰å‡ ä¸ªé—®é¢˜ã€‚å˜¿ï¼Œæ˜¯çš„ï¼Œè°¢è°¢ã€‚æˆ‘æƒ³æˆ‘åªæ˜¯å‘ç°KNNæ˜¯æœ€å·®è¡¨ç°è€…æœ‰ç‚¹æƒŠè®¶ï¼Œå› ä¸ºå®ƒä¹Ÿæ˜¯éå‚æ•°çš„ã€‚ä½ èƒ½å¯¹æ­¤åšä¸€ä¸‹è¯„è®ºå—ï¼Ÿæ˜¯çš„ï¼Œæ˜¯ä¸æ˜¯NPTæœ‰æŸç§å†…åœ¨ç‰¹æ€§ä½¿å…¶è¿œè¶…å…¶ä»–éå‚æ•°æ–¹æ³•ï¼Ÿ
- en: why is it that KN performs the worst hereï¼ŸWellï¼Œ I suppose ultimately can and
    is is still a relatively naive predictive method in that you know it might just
    be predicting based on kind of cluster means so for exampleã€‚you know I think this
    is probably universally true for all the data sets but there's probably some amount
    of kind of additional reasoning that needs to occur over the features at least
    to basic level so for exampleã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆKNNåœ¨è¿™é‡Œè¡¨ç°æœ€å·®ï¼Ÿå—¯ï¼Œæˆ‘æƒ³æœ€ç»ˆKNNä»ç„¶æ˜¯ä¸€ä¸ªç›¸å¯¹å¤©çœŸçš„é¢„æµ‹æ–¹æ³•ï¼Œå› ä¸ºä½ çŸ¥é“å®ƒå¯èƒ½åªæ˜¯åŸºäºæŸç§èšç±»å‡å€¼è¿›è¡Œé¢„æµ‹ã€‚ä¾‹å¦‚ï¼Œæˆ‘è®¤ä¸ºè¿™å¯¹æ‰€æœ‰æ•°æ®é›†æ¥è¯´å¯èƒ½éƒ½æ˜¯æ™®éçœŸå®çš„ï¼Œä½†åœ¨ç‰¹å¾ä¸Šå¯èƒ½éœ€è¦è¿›è¡ŒæŸç§é¢å¤–çš„æ¨ç†ï¼Œè‡³å°‘è¾¾åˆ°åŸºæœ¬æ°´å¹³ã€‚
- en: like one of the data sets is this poker hand data set where it's like a mapping
    between all of the different hands you have in poker and what like they're commonly
    known to people like full houses or whatever So this requires some amount of reasoning
    over the features to be able to group things togetherã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€ä¸ªæ•°æ®é›†æ˜¯æ‰‘å…‹æ‰‹æ•°æ®é›†ï¼Œå®ƒæ˜¯æ‰€æœ‰ä¸åŒæ‰‘å…‹æ‰‹ä¸äººä»¬å¸¸çŸ¥é“çš„ï¼Œä¾‹å¦‚è‘«èŠ¦ä¹‹é—´çš„æ˜ å°„ã€‚å› æ­¤ï¼Œè¿™éœ€è¦å¯¹ç‰¹å¾è¿›è¡Œä¸€äº›æ¨ç†ï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†äº‹ç‰©ç»„åˆåœ¨ä¸€èµ·ã€‚
- en: So just taking like the cluster means of the featurization of those different
    you know hands is likely not going to give you a great predictive functionã€‚whereas
    NPTs can kind of do the classic thing where say you have an MLP type of thing
    over the features or like a you know tree type of thing over the featuresã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä»…ä»…å–è¿™äº›ä¸åŒæ‰‹çš„ç‰¹å¾åŒ–èšç±»å‡å€¼å¯èƒ½ä¸ä¼šç»™ä½ ä¸€ä¸ªå¥½çš„é¢„æµ‹å‡½æ•°ã€‚è€ŒNPTå¯ä»¥åšåˆ°ç»å…¸çš„äº‹æƒ…ï¼Œæ¯”å¦‚è¯´åœ¨ç‰¹å¾ä¸Šæœ‰ä¸€ä¸ªMLPç±»å‹çš„ä¸œè¥¿ï¼Œæˆ–è€…è¯´åœ¨ç‰¹å¾ä¸Šæœ‰ä¸€ä¸ªæ ‘ç±»å‹çš„ä¸œè¥¿ã€‚
- en: you can learn some sort of complex embeddingï¼Œ but then you also can do some
    nonparmetric sort of prediction based on say like clusters of embeddingsã€‚I said
    yeah that makes senseï¼Œ I guess if what if you usedï¼Ÿ
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å­¦ä¹ æŸç§å¤æ‚çš„åµŒå…¥ï¼Œä½†ä½ ä¹Ÿå¯ä»¥åŸºäºåµŒå…¥çš„èšç±»è¿›è¡ŒæŸç§éå‚æ•°é¢„æµ‹ã€‚æˆ‘æ˜¯è¯´ï¼Œæ˜¯çš„ï¼Œè¿™æœ‰é“ç†ã€‚å¦‚æœä½ ä½¿ç”¨ä»€ä¹ˆå‘¢ï¼Ÿ
- en: Pre trained embeddings from a stack of encoders as your vector representation
    for the canonã€‚How do you think that would perform compared to the rest of the
    crowdï¼ŸYeahï¼Œ so this is likeï¼Œ I meanã€‚this idea is kind of like deep kernel learning
    or likeï¼Œ yeahã€‚I believe it is deep kernel learning is basically you use an MLP
    independentlyã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ä¸€å †ç¼–ç å™¨ä¸­é¢„è®­ç»ƒçš„åµŒå…¥ä½œä¸ºä½ çš„å‘é‡è¡¨ç¤ºç”¨äºç»å…¸ã€‚ä½ è§‰å¾—è¿™ä¸å…¶ä»–æ–¹æ³•ç›¸æ¯”è¡¨ç°å¦‚ä½•ï¼Ÿæ˜¯çš„ï¼Œè¿™å°±åƒï¼Œæˆ‘çš„æ„æ€æ˜¯ã€‚è¿™ç§æƒ³æ³•æœ‰ç‚¹åƒæ·±æ ¸å­¦ä¹ ï¼Œæˆ–è€…è¯´ï¼Œæ˜¯çš„ã€‚æˆ‘ç›¸ä¿¡æ·±æ ¸å­¦ä¹ åŸºæœ¬ä¸Šæ˜¯ä½ ç‹¬ç«‹ä½¿ç”¨ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰ã€‚
- en: So you learn an MP on each input data pointï¼Œ and then you apply a G over all
    the representations of thoseã€‚you get this sort of like complex embedding and then
    the lookupsã€‚The key difference between that type of idea and Ns is that we also
    learn the relationships between the data points themselvesã€‚because we use this
    parametric attention mechanism to learn the relationshipsã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ åœ¨æ¯ä¸ªè¾“å…¥æ•°æ®ç‚¹ä¸Šå­¦ä¹ ä¸€ä¸ªMLPï¼Œç„¶åä½ å¯¹æ‰€æœ‰è¿™äº›è¡¨ç¤ºåº”ç”¨ä¸€ä¸ªGã€‚ä½ å¾—åˆ°äº†è¿™ç§å¤æ‚çš„åµŒå…¥ï¼Œç„¶åè¿›è¡ŒæŸ¥æ‰¾ã€‚è¯¥æƒ³æ³•ä¸Nsä¹‹é—´çš„å…³é”®åŒºåˆ«åœ¨äºï¼Œæˆ‘ä»¬è¿˜å­¦ä¹ æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨è¿™ç§å‚æ•°åŒ–çš„æ³¨æ„æœºåˆ¶æ¥å­¦ä¹ è¿™äº›å…³ç³»ã€‚
- en: So we're not just learning like an embedding independentlyã€‚We're basically back
    propagating through the entire process learning the ways in which we would try
    to embed thisã€‚but also the the ways that say the lookup would occur and essentially
    the the relationships at that could potentially be kind of higher order as wellã€‚cool
    more follow upã€‚ğŸ˜Šï¼ŒOh yeah go for it cool yeah thanks so I guess then if if the
    advantage of NPT has to do with sort of the relationships between data pointsã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¹¶ä¸æ˜¯ä»…ä»…ç‹¬ç«‹åœ°å­¦ä¹ ä¸€ä¸ªåµŒå…¥ã€‚æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯é€šè¿‡æ•´ä¸ªè¿‡ç¨‹è¿›è¡Œåå‘ä¼ æ’­ï¼Œå­¦ä¹ æˆ‘ä»¬è¯•å›¾åµŒå…¥çš„æ–¹å¼ï¼Œä»¥åŠæŸ¥æ‰¾çš„æ–¹å¼ï¼Œæœ¬è´¨ä¸Šæ˜¯æ½œåœ¨çš„é«˜é˜¶å…³ç³»ã€‚å¾ˆé…·ï¼Œæ›´å¤šçš„åç»­é—®é¢˜ã€‚ğŸ˜Šï¼Œå“¦ï¼Œæ˜¯çš„ï¼Œç»§ç»­å§ï¼Œé…·ï¼Œå¥½çš„ï¼Œè°¢è°¢ã€‚æˆ‘æƒ³å¦‚æœNPTçš„ä¼˜åŠ¿ä¸æ•°æ®ç‚¹ä¹‹é—´çš„å…³ç³»æœ‰å…³ã€‚
- en: AndWhat if you you know took theï¼ŸTook the let's say you know encoder representations
    and then you passed that as input say for the you know 10 nearest neighbors along
    with like some other likeã€‚input representation and sort of had this like weighted
    average like attention style where you weighted the vectors of the nearest neighbors
    based on the attention weights between those input data points and then like the
    supplied input data point and then like past that as you know the vector to like
    the final prediction layerã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œå¦‚æœä½ çŸ¥é“è·å–äº†ç¼–ç å™¨çš„è¡¨ç¤ºï¼Œç„¶åå°†å…¶ä½œä¸ºè¾“å…¥ä¼ é€’ç»™10ä¸ªæœ€è¿‘é‚»ï¼Œå¹¶ä¸”è¿˜æœ‰ä¸€äº›å…¶ä»–çš„è¾“å…¥è¡¨ç¤ºï¼Œè¿›è¡ŒåŠ æƒå¹³å‡ï¼Œæ¯”å¦‚æ³¨æ„åŠ›æœºåˆ¶çš„é‚£ç§æ–¹å¼ï¼Œä¾æ®è¾“å…¥æ•°æ®ç‚¹ä¸æä¾›çš„è¾“å…¥æ•°æ®ç‚¹ä¹‹é—´çš„æ³¨æ„åŠ›æƒé‡å¯¹æœ€è¿‘é‚»çš„å‘é‡è¿›è¡ŒåŠ æƒï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æœ€ç»ˆçš„é¢„æµ‹å±‚ï¼Œä½ è§‰å¾—æ€ä¹ˆæ ·ï¼Ÿ
- en: like do you think that captures some amount of the relationship or is that off
    baseï¼Ÿ
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è®¤ä¸ºè¿™æ˜¯å¦æ•æ‰åˆ°äº†ä¸€å®šç¨‹åº¦çš„å…³ç³»ï¼Œè¿˜æ˜¯åç¦»äº†ï¼Ÿ
- en: So I think the nice partï¼Œ like and really what our ideas behind this whole thing
    is justã€‚These sorts of instances where certain fixed kernels would perform particularly
    well in tasks is like kind of an annoyanceã€‚And like ultimately like tuning a lot
    of these types of things or're trying to derive the predictive methods that might
    make a lot of sense for a given situationã€‚kind of stinks And ideallyï¼Œ you'd want
    to just back propagate on a data and kind of learn these relationships yourselfã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºä¸é”™çš„ä¸€ç‚¹ï¼Œä»¥åŠæˆ‘ä»¬å¯¹è¿™ä¸€åˆ‡çš„æƒ³æ³•æ˜¯ï¼ŒæŸäº›å›ºå®šæ ¸åœ¨ç‰¹å®šä»»åŠ¡ä¸­è¡¨ç°ç‰¹åˆ«å¥½çš„æƒ…å†µæ˜¯ä¸€ç§æ¼äººçš„äº‹æƒ…ã€‚æœ€ç»ˆï¼Œè°ƒä¼˜è¿™äº›ç±»å‹çš„ä¸œè¥¿æˆ–è¯•å›¾æ¨å¯¼å‡ºé€‚åˆç‰¹å®šæƒ…å†µçš„é¢„æµ‹æ–¹æ³•ï¼Œç¡®å®ä»¤äººæ²®ä¸§ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œä½ ä¼šå¸Œæœ›èƒ½å¤Ÿåœ¨æ•°æ®ä¸Šè¿›è¡Œåå‘ä¼ æ’­ï¼Œè‡ªå·±å­¦ä¹ è¿™äº›å…³ç³»ã€‚
- en: So I actually would be really interested to see if we can come up with some
    synthetic experiments that have these sort of like very particular K and Nã€‚like
    predictive mechanisms and just see if we can learn precisely those and get you
    knowã€‚zero error with NPptsã€‚ Andï¼Œ in factï¼Œ like we'll get into this a little bit
    with some of the interventional experiments we doã€‚We have like kind of precise
    lookup functions that NPpts end up being able to learnã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å…¶å®éå¸¸æƒ³çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿè®¾è®¡ä¸€äº›åˆæˆå®éªŒï¼Œå…·æœ‰è¿™äº›éå¸¸ç‰¹å®šçš„Kå’ŒNï¼Œåƒé¢„æµ‹æœºåˆ¶ï¼Œå¹¶çœ‹çœ‹æˆ‘ä»¬æ˜¯å¦èƒ½å¤Ÿç²¾ç¡®å­¦ä¹ è¿™äº›ï¼Œå¹¶è·å¾—é›¶è¯¯å·®ä¸NPptsã€‚äº‹å®ä¸Šï¼Œæˆ‘ä»¬å°†ç¨å¾®æ¶‰åŠä¸€äº›æˆ‘ä»¬æ‰€åšçš„å¹²é¢„å®éªŒã€‚æˆ‘ä»¬æœ‰ä¸€äº›ç²¾ç¡®çš„æŸ¥æ‰¾å‡½æ•°ï¼ŒNPptsæœ€ç»ˆèƒ½å¤Ÿå­¦ä¹ åˆ°è¿™äº›ã€‚
- en: So we can learn interesting relational functionsã€‚ğŸ˜Šï¼ŒCoolï¼Œ yeahï¼Œ thanks a lotï¼Œ
    appreciateã€‚å¥½ã€‚All rightã€‚we have one more question fromã€‚![](img/1777b223f7bf8be27a8767d209c7f271_29.png)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å­¦ä¹ æœ‰è¶£çš„å…³ç³»å‡½æ•°ã€‚ğŸ˜Šï¼Œé…·ï¼Œæ²¡é”™ï¼Œéå¸¸æ„Ÿè°¢ï¼Œå¤ªå¥½äº†ã€‚å¥½çš„ã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ¥è‡ªäºã€‚![](img/1777b223f7bf8be27a8767d209c7f271_29.png)
- en: I just wanted to clarify something about basically so at test time you just
    take the exact same data set and you just like add like your test examples right
    and then you like do the same type of like masking and is that how it worksï¼Ÿ
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯æƒ³æ¾„æ¸…ä¸€ä¸‹ï¼ŒåŸºæœ¬ä¸Šåœ¨æµ‹è¯•æ—¶ä½ åªæ˜¯å–ç›¸åŒçš„æ•°æ®é›†ï¼Œç„¶åæ·»åŠ ä½ çš„æµ‹è¯•ç¤ºä¾‹ï¼Œå¯¹å—ï¼Ÿç„¶åä½ ä¹Ÿåšç›¸åŒç±»å‹çš„æ©è”½ï¼Œæ˜¯è¿™æ ·å—ï¼Ÿ
- en: Yeahï¼Œ correctã€‚Okay got it and I do have one more question that is just because
    like I think I like misunderstood like how like the effects of your NPT objectiveã€‚do
    you mind going back to that slideï¼ŸSureã€‚Yeahï¼Œ can you repeat one more time like
    what makes this so specialï¼Ÿ
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ²¡é”™ã€‚å¥½çš„ï¼Œæˆ‘æ˜ç™½äº†ã€‚æˆ‘è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå°±æ˜¯å› ä¸ºæˆ‘è§‰å¾—æˆ‘è¯¯è§£äº†ä½ ä»¬çš„NPTç›®æ ‡çš„å½±å“ã€‚ä½ ä»‹æ„å›åˆ°é‚£å¼ å¹»ç¯ç‰‡å—ï¼Ÿå½“ç„¶å¯ä»¥ã€‚ä½ èƒ½å†é‡å¤ä¸€æ¬¡ï¼Œæ˜¯ä»€ä¹ˆè®©è¿™ä¸ªå¦‚æ­¤ç‰¹åˆ«çš„å—ï¼Ÿ
- en: Yeahï¼Œ so the regularizerizer on the right over the features I would think of
    very similarly to self-supervised learning with just a standard transformer like
    you're basically just introducing a lot more supervision and you're even if say
    you're just doing a supervised objective this is kind of like some amount of reconstruction
    over the features you learn a more interesting representation and like what a
    regularizing effectã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥åœ¨ç‰¹å¾å³ä¾§çš„æ­£åˆ™åŒ–å™¨ï¼Œæˆ‘ä¼šå°†å…¶è§†ä¸ºéå¸¸ç±»ä¼¼äºè‡ªç›‘ç£å­¦ä¹ ï¼Œä½¿ç”¨æ ‡å‡†çš„ transformerï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å¼•å…¥æ›´å¤šçš„ç›‘ç£ï¼Œå³ä½¿ä½ åªæ˜¯è¿›è¡Œä¸€ä¸ªç›‘ç£ç›®æ ‡ï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šå°±åƒæ˜¯å¯¹ä½ å­¦ä¹ åˆ°çš„ç‰¹å¾è¿›è¡Œé‡æ„ï¼Œä½ ä¼šå­¦ä¹ åˆ°æ›´æœ‰è¶£çš„è¡¨ç¤ºï¼Œä»¥åŠæ­£åˆ™åŒ–æ•ˆæœã€‚
- en: which we think is interesting but perhaps not as interesting as this stochastic
    target maskingã€‚this one is unique becauseã€‚In kind of standard parametric deep
    learningã€‚you're not going to have an instance in your training process where you're
    taking targets as inputã€‚And so basicallyï¼Œ what happens isã€‚If you have your training
    data set is inputã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è®¤ä¸ºè¿™æ˜¯æœ‰è¶£çš„ï¼Œä½†ä¹Ÿè®¸æ²¡æœ‰è¿™ç§éšæœºç›®æ ‡æ©è”½é‚£ä¹ˆæœ‰è¶£ã€‚è¿™ç§æ–¹æ³•æ˜¯ç‹¬ç‰¹çš„ï¼Œå› ä¸ºåœ¨æ ‡å‡†çš„å‚æ•°åŒ–æ·±åº¦å­¦ä¹ ä¸­ï¼Œä½ çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šæœ‰å°†ç›®æ ‡ä½œä¸ºè¾“å…¥çš„å®ä¾‹ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šå‘ç”Ÿçš„æ˜¯ï¼Œå¦‚æœä½ çš„è®­ç»ƒæ•°æ®é›†ä½œä¸ºè¾“å…¥ã€‚
- en: whatever you're gonna have some stochastic feature masking stuff happening on
    the features amongst the training targetsã€‚you're randomly going to have some of
    those unmasked and some of them will indeed be maskedã€‚You're going to be back
    propagating a loss on the ones that are maskedï¼Œ of courseã€‚because you don't want
    your model to have those available input if you're going to actually try to you
    know back propagate a loss on itã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ— è®ºå¦‚ä½•ï¼Œä½ ä¼šåœ¨è®­ç»ƒç›®æ ‡çš„ç‰¹å¾ä¸Šè¿›è¡Œä¸€äº›éšæœºç‰¹å¾æ©è”½æ“ä½œã€‚ä½ ä¼šéšæœºé€‰æ‹©ä¸€äº›ä¸è¢«æ©è”½ï¼Œè€Œæœ‰äº›åˆ™ä¼šè¢«æ©è”½ã€‚å½“ç„¶ï¼Œå¯¹äºè¢«æ©è”½çš„éƒ¨åˆ†ï¼Œä½ ä¼šåå‘ä¼ æ’­æŸå¤±ï¼Œå› ä¸ºä½ ä¸å¸Œæœ›ä½ çš„æ¨¡å‹åœ¨å®é™…å°è¯•åå‘ä¼ æ’­æŸå¤±æ—¶èƒ½è·å¾—è¿™äº›å¯ç”¨çš„è¾“å…¥ã€‚
- en: But you can use the other ones as inputï¼Œ And that means you can learn these
    kind of like interpoative functionsã€‚So that was like this whole idea of like being
    able to kind of like learn K and Nã€‚But doesn't that allowï¼ŸThe model to cheat againã€‚Yeahã€‚so
    this is like an interesting point and actually like subtle so I think it's it's
    really worthwhile to bring up so first of allã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¯ä»¥ä½¿ç”¨å…¶ä»–ä½œä¸ºè¾“å…¥ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥å­¦ä¹ è¿™äº›åƒæ’å€¼å‡½æ•°ä¸€æ ·çš„ä¸œè¥¿ã€‚è¿™å°±æ˜¯èƒ½å¤Ÿå­¦ä¹  K å’Œ N çš„æ•´ä¸ªæƒ³æ³•ã€‚ä½†è¿™éš¾é“ä¸å…è®¸æ¨¡å‹å†æ¬¡ä½œå¼Šå—ï¼Ÿæ˜¯çš„ã€‚è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è§‚ç‚¹ï¼Œå…¶å®å¾ˆå¾®å¦™ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºæå‡ºè¿™ä¸ªé—®é¢˜æ˜¯éå¸¸å€¼å¾—çš„ï¼Œé¦–å…ˆã€‚
- en: we never actually back propagate a loss on something that was visible to the
    model at input and so ifã€‚for example the model did actually end up basically overfitting
    on training labels we would not observe the model's ability to generalize to test
    data we don't observe this so obviously it seems like this kind of blocking of
    back propagation on labels that are visible at input to the NPT is helpingã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä»æœªåœ¨æ¨¡å‹è¾“å…¥å¯è§çš„å†…å®¹ä¸Šåå‘ä¼ æ’­æŸå¤±ï¼Œå› æ­¤å¦‚æœã€‚ä¾‹å¦‚ï¼Œæ¨¡å‹ç¡®å®æœ€ç»ˆåœ¨è®­ç»ƒæ ‡ç­¾ä¸Šè¿‡æ‹Ÿåˆï¼Œæˆ‘ä»¬å°†ä¸ä¼šè§‚å¯Ÿåˆ°æ¨¡å‹åœ¨æµ‹è¯•æ•°æ®ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œæˆ‘ä»¬å¹¶æœªè§‚å¯Ÿåˆ°è¿™ä¸€ç‚¹ï¼Œå› æ­¤æ˜¾ç„¶ï¼Œè¿™ç§å¯¹åœ¨
    NPT è¾“å…¥æ—¶å¯è§æ ‡ç­¾çš„åå‘ä¼ æ’­é˜»æ–­æ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: It could also be possible thatã€‚In bird style stochastic maskingã€‚you also randomly
    will flip some labels to be in a different categoryã€‚So this is like kind of just
    like a random fine print that was introduced in the bird masking textã€‚We also
    do thatã€‚So it's possible that that somehow contributes to the to thatã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿæœ‰å¯èƒ½åœ¨é¸Ÿå¼éšæœºæ©è”½ä¸­ï¼Œä½ ä¹Ÿä¼šéšæœºå°†ä¸€äº›æ ‡ç­¾ç¿»è½¬ä¸ºä¸åŒçš„ç±»åˆ«ã€‚è¿™å°±åƒæ˜¯åœ¨é¸Ÿæ©è”½æ–‡æœ¬ä¸­å¼•å…¥çš„ä¸€ç§éšæœºç»†åˆ™ã€‚æˆ‘ä»¬ä¹Ÿè¿™æ ·åšã€‚å› æ­¤ï¼Œè¿™å¯èƒ½åœ¨æŸç§ç¨‹åº¦ä¸Šæœ‰åŠ©äºè¿™ä¸ªã€‚
- en: but it's probably pretty likely to just be the fact that we're not back propagating
    a loss on something that's visibleã€‚Greatï¼Œ thanksï¼Œ make senseã€‚ğŸ˜Šï¼ŒI have two more
    questionsï¼Œ if I can jump inã€‚![](img/1777b223f7bf8be27a8767d209c7f271_31.png)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¾ˆå¯èƒ½åªæ˜¯å› ä¸ºæˆ‘ä»¬æ²¡æœ‰åœ¨å¯è§çš„å†…å®¹ä¸Šåå‘ä¼ æ’­æŸå¤±ã€‚å¤ªå¥½äº†ï¼Œè°¢è°¢ï¼Œæ˜ç™½äº†ã€‚ğŸ˜Šï¼Œå¦‚æœå¯ä»¥çš„è¯ï¼Œæˆ‘è¿˜æœ‰ä¸¤ä¸ªé—®é¢˜ï¼Œèƒ½æ’å˜´å—ï¼Ÿ![](img/1777b223f7bf8be27a8767d209c7f271_31.png)
- en: Yesï¼Œ sorryã€‚Can we go to the the metricsï¼Œ the performanceï¼Œ the results slideã€‚![](img/1777b223f7bf8be27a8767d209c7f271_33.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼ŒæŠ±æ­‰ã€‚æˆ‘ä»¬å¯ä»¥å»æŸ¥çœ‹åº¦é‡æŒ‡æ ‡ã€æ€§èƒ½ã€ç»“æœçš„å¹»ç¯ç‰‡å—ï¼Ÿ![](img/1777b223f7bf8be27a8767d209c7f271_33.png)
- en: Sureã€‚![](img/1777b223f7bf8be27a8767d209c7f271_35.png)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼![](img/1777b223f7bf8be27a8767d209c7f271_35.png)
- en: I feel like I missed something elseã€‚I'm sorry about thisã€‚So A U areã€‚so looking
    on the binary classificationï¼Œ A U R O Cã€‚Can you clarify what these numbers meanã€‚Are
    they the A U ROCã€‚So this is the so on each of the data setsã€‚So say for a particular
    binary classification data setã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æ„Ÿè§‰æˆ‘é”™è¿‡äº†ä¸€äº›ä¸œè¥¿ã€‚æˆ‘å¯¹æ­¤æ„Ÿåˆ°æŠ±æ­‰ã€‚æ‰€ä»¥ A U æ˜¯ã€‚å…³äºäºŒå…ƒåˆ†ç±»ï¼ŒA U R O Cã€‚ä½ èƒ½è§£é‡Šä¸€ä¸‹è¿™äº›æ•°å­—çš„å«ä¹‰å—ï¼Ÿå®ƒä»¬æ˜¯ A U ROC å—ï¼Ÿè¿™æ˜¯å…³äºæ¯ä¸ªæ•°æ®é›†çš„ã€‚é‚£ä¹ˆä»¥ä¸€ä¸ªç‰¹å®šçš„äºŒå…ƒåˆ†ç±»æ•°æ®é›†ä¸ºä¾‹ã€‚
- en: we're going to get a ranking of the methods we're going to repeat this yeahã€‚so
    these numbers here are the the relative ranking across in this particular caseã€‚the
    four data setsã€‚Correctï¼Œ yeahï¼Œ I seeã€‚So thisï¼Œ these values are not the A U R O
    Cs on averageã€‚across the data setsã€‚Noï¼Œ yeahã€‚ they're notã€‚ I meanï¼Œ like everythingã€‚Averaging
    our might make senseã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å¯¹è¿™äº›æ–¹æ³•è¿›è¡Œæ’åï¼Œæˆ‘ä»¬ä¼šé‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚æ‰€ä»¥è¿™äº›æ•°å­—åœ¨è¿™ä¸ªç‰¹å®šæƒ…å†µä¸‹æ˜¯å››ä¸ªæ•°æ®é›†çš„ç›¸å¯¹æ’åã€‚å¯¹ï¼Œæ²¡é”™ï¼Œæˆ‘æ˜ç™½äº†ã€‚æ‰€ä»¥è¿™äº›å€¼ä¸æ˜¯æ•°æ®é›†ä¸Šçš„ A U
    R O C å¹³å‡å€¼ã€‚ä¸æ˜¯ï¼Œæ²¡é”™ï¼Œå®ƒä»¬ä¸æ˜¯ã€‚æˆ‘æ˜¯è¯´ï¼Œåƒæ‰€æœ‰äº‹æƒ…ä¸€æ ·ï¼Œå¹³å‡å¯èƒ½æ˜¯æœ‰æ„ä¹‰çš„ã€‚
- en: but averaging things like accuracy and RMC seems like a bad idea right because
    you might have some data sets where everything has high accuracy or where RMC
    needs something drastically different I see so this these numbers here only tell
    us the relative ranking between the different methods not how well they actually
    performã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯å¹³å‡å‡†ç¡®ç‡å’Œ RMC ä¼¼ä¹æ˜¯ä¸ªåä¸»æ„ï¼Œå¯¹å§ï¼Œå› ä¸ºä½ å¯èƒ½æœ‰ä¸€äº›æ•°æ®é›†çš„å‡†ç¡®ç‡éƒ½å¾ˆé«˜ï¼Œæˆ–è€… RMC éœ€è¦ä¸€äº›å®Œå…¨ä¸åŒçš„ä¸œè¥¿ã€‚æˆ‘æ˜ç™½äº†ï¼Œæ‰€ä»¥è¿™äº›æ•°å­—åªå‘Šè¯‰æˆ‘ä»¬ä¸åŒæ–¹æ³•ä¹‹é—´çš„ç›¸å¯¹æ’åï¼Œè€Œä¸æ˜¯å®ƒä»¬å®é™…è¡¨ç°å¾—å¤šå¥½ã€‚
- en: I meanï¼Œ it tells us how they perform relative to one anotherï¼Œ but not how well
    they perform in I seeã€‚Okayï¼Œ but that's not in the appendixã€‚all we have that informationã€‚I
    seeï¼Œ Okayï¼Œ I wasã€‚I was sitting here confused going likeï¼Œ why is A U R O Cã€‚Why
    is the best one the smallest and accuracyï¼Œ What is an accuracy of 2ã€‚ anywaysã€‚
    Okayã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„æ€æ˜¯ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬å®ƒä»¬ç›¸å¯¹å½¼æ­¤çš„è¡¨ç°ï¼Œä½†å¹¶ä¸å‘Šè¯‰æˆ‘ä»¬å®ƒä»¬è¡¨ç°å¾—å¤šå¥½ã€‚æˆ‘æ˜ç™½äº†ã€‚ä¸è¿‡ï¼Œè¿™ä¸åœ¨é™„å½•ä¸­ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿™äº›ä¿¡æ¯ã€‚æˆ‘æ˜ç™½äº†ï¼Œå¥½çš„ï¼Œæˆ‘ä¸€ç›´åœ¨è¿™é‡Œå›°æƒ‘ï¼Œä¸ºä»€ä¹ˆ
    A U R O C æœ€å¥½çš„ä¸€ä¸ªæ˜¯æœ€å°çš„ï¼Œè€Œå‡†ç¡®ç‡ï¼Œå‡†ç¡®ç‡æ˜¯å¤šå°‘å‘¢ï¼Ÿæ€»ä¹‹ï¼Œå¥½çš„ã€‚
- en: that makes my more senseã€‚Thank you bothã€‚Awesomeã€‚ğŸ˜Šï¼ŒGreatã€‚so I'll try to speed
    through this just in the interest of timeã€‚But the basically thingã€‚the thing that
    you might be thinking after all these results is are we even learning any data
    point interactions on these real data setsã€‚And so basicallyï¼Œ we design an experiment
    to figure this outã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©æˆ‘æ›´èƒ½ç†è§£ã€‚è°¢è°¢ä½ ä»¬ä¿©ã€‚å¤ªæ£’äº†ã€‚ğŸ˜Š å¥½çš„ã€‚æ‰€ä»¥ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œæˆ‘ä¼šå°½é‡åŠ å¿«é€Ÿåº¦ã€‚ä½†åŸºæœ¬ä¸Šï¼Œä½ å¯èƒ½ä¼šåœ¨çœ‹å®Œæ‰€æœ‰è¿™äº›ç»“æœåæƒ³ï¼Œéš¾é“æˆ‘ä»¬çœŸçš„åœ¨è¿™äº›çœŸå®æ•°æ®é›†ä¸Šå­¦ä¹ åˆ°ä»»ä½•æ•°æ®ç‚¹çš„äº¤äº’å—ï¼Ÿå› æ­¤ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªå®éªŒæ¥å¼„æ¸…æ¥šè¿™ä¸€ç‚¹ã€‚
- en: And the idea is that we're going to disallow NPT from using other data points
    when predicting on one of themã€‚ğŸ˜Šã€‚![](img/1777b223f7bf8be27a8767d209c7f271_37.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæƒ³æ³•æ˜¯æˆ‘ä»¬è¦ç¦æ­¢ NPT åœ¨å¯¹æŸä¸€ä¸ªæ•°æ®ç‚¹è¿›è¡Œé¢„æµ‹æ—¶ä½¿ç”¨å…¶ä»–æ•°æ®ç‚¹ã€‚ğŸ˜Šã€‚![](img/1777b223f7bf8be27a8767d209c7f271_37.png)
- en: If we do thatï¼Œ and we observe that NPT actually predicts or performs significantly
    worseã€‚it is indeed using these interactions between data pointsã€‚A subtle challenge
    or kind of like an added added bonus we can get from this is that ideallyã€‚we wouldn't
    actually break batch statisticsã€‚ So let's say like the mean of each particular
    attributeã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¿™æ ·åšï¼Œå¹¶ä¸”è§‚å¯Ÿåˆ° NPT å®é™…ä¸Šé¢„æµ‹æˆ–è¡¨ç°æ˜¾è‘—æ›´å·®ï¼Œç¡®å®æ˜¯åœ¨åˆ©ç”¨æ•°æ®ç‚¹ä¹‹é—´çš„äº¤äº’ã€‚ä»ä¸­æˆ‘ä»¬å¯ä»¥è·å¾—ä¸€ä¸ªå¾®å¦™çš„æŒ‘æˆ˜æˆ–ä¸€ç§é™„åŠ çš„å¥½å¤„ï¼Œé‚£å°±æ˜¯ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸åº”è¯¥ç ´åæ‰¹é‡ç»Ÿè®¡æ•°æ®ã€‚æ¯”å¦‚è¯´æ¯ä¸ªç‰¹å®šå±æ€§çš„å‡å€¼ã€‚
- en: if we can find a way to do this experiment such that we don't break these thingsã€‚we
    can kind of rule out the possibility that we learn something that's a bit similar
    to bash normã€‚And so the way that we do this is we basically look at the predictions
    for each one of the data points in sequenceã€‚So let's say in this case we're looking
    at the prediction of the model for this particular green row and you know it's
    going be predicting in this last column that has this question mark which is mess
    what we're going do is we're going permute each of the attributes independently
    amongst all other data points except for that one so the information for that
    row if it was kind of just predicting like a classic parametric deep model is
    still intact but the information from all of the other rows is gone so that's
    why we call this sort of the corruption experimentã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬èƒ½æ‰¾åˆ°ä¸€ç§æ–¹å¼æ¥è¿›è¡Œè¿™ä¸ªå®éªŒè€Œä¸ç ´åè¿™äº›ä¸œè¥¿ï¼Œæˆ‘ä»¬å¯ä»¥æ’é™¤å­¦ä¹ åˆ°ä¸€äº›ä¸ bash norm æœ‰ç‚¹ç›¸ä¼¼çš„ä¸œè¥¿çš„å¯èƒ½æ€§ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„åšæ³•æ˜¯åŸºæœ¬ä¸Šä¾æ¬¡æŸ¥çœ‹æ¯ä¸ªæ•°æ®ç‚¹çš„é¢„æµ‹ã€‚æ‰€ä»¥å‡è®¾åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ­£åœ¨æŸ¥çœ‹æ¨¡å‹å¯¹è¿™ä¸€ç‰¹å®šç»¿è‰²è¡Œçš„é¢„æµ‹ï¼Œè€Œå®ƒä¼šåœ¨è¿™ä¸ªæœ€åä¸€åˆ—æœ‰ä¸€ä¸ªé—®å·è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬å°†ç‹¬ç«‹åœ°å¯¹é™¤äº†è¿™ä¸€è¡Œä»¥å¤–çš„æ‰€æœ‰å…¶ä»–æ•°æ®ç‚¹çš„æ¯ä¸ªå±æ€§è¿›è¡Œæ’åˆ—ï¼Œæ‰€ä»¥è¿™ä¸€è¡Œçš„ä¿¡æ¯å¦‚æœå®ƒåªæ˜¯åƒç»å…¸å‚æ•°æ·±åº¦æ¨¡å‹é‚£æ ·è¿›è¡Œé¢„æµ‹ä¾ç„¶ä¿æŒå®Œæ•´ï¼Œä½†å…¶ä»–è¡Œçš„ä¿¡æ¯åˆ™æ¶ˆå¤±äº†ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œå¹²æ‰°å®éªŒâ€çš„åŸå› ã€‚
- en: And so we find in general when we perform this experiment performance kind of
    falls off a cliff for the vast majority of these methods and I'll note that you
    the performances between the methods on a lot of these were fairly close and so
    this is actually indeed pretty significant so for example on protein we went from
    being the top performer amongst all the methods to the worst performer worse than
    even like KNN or something like that I'll also note that there's kind of this
    interesting behavior where on these data sets like For and kick and breast cancer
    we actually observe that there's basically no drop in performance and we basically
    see this as kind of an interesting feature and not necessarily a bug of the model
    which is that if we're back propagating on a given data the model can sort of
    just find that it's actually not that worthwhile to attempt to predict using some
    kind of you know relational predictive mechanism amongst data points and can instead
    just learn to predict parametrically and basically ignore other data points when
    it's predict on any given one of themã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å‘ç°ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œå½“æˆ‘ä»¬è¿›è¡Œè¿™ä¸ªå®éªŒæ—¶ï¼Œè¡¨ç°ä¼šåœ¨å¤§å¤šæ•°æ–¹æ³•ä¸­æ€¥å‰§ä¸‹é™ï¼Œæˆ‘è¦æŒ‡å‡ºçš„æ˜¯ï¼Œè¿™äº›æ–¹æ³•ä¹‹é—´çš„è¡¨ç°å·®è·ç›¸å¯¹è¾ƒå°ï¼Œå› æ­¤è¿™å®é™…ä¸Šæ˜¯ç›¸å½“é‡è¦çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨è›‹ç™½è´¨æ–¹é¢ï¼Œæˆ‘ä»¬ä»æ‰€æœ‰æ–¹æ³•ä¸­è¡¨ç°æœ€å¥½çš„è½¬å˜ä¸ºæœ€å·®ï¼Œç”šè‡³æ¯”
    KNN è¿˜å·®ã€‚æˆ‘è¿˜è¦æŒ‡å‡ºï¼Œåœ¨è¿™äº›æ•°æ®é›†ä¸Šï¼Œå¦‚Forã€Kickå’Œä¹³è…ºç™Œï¼Œæˆ‘ä»¬å®é™…ä¸Šè§‚å¯Ÿåˆ°æ€§èƒ½å‡ ä¹æ²¡æœ‰ä¸‹é™ï¼Œæˆ‘ä»¬å°†å…¶è§†ä¸ºä¸€ç§æœ‰è¶£çš„ç‰¹å¾ï¼Œè€Œä¸ä¸€å®šæ˜¯æ¨¡å‹çš„ç¼ºé™·ï¼Œæ¨¡å‹å¯ä»¥å‘ç°å°è¯•ä½¿ç”¨æŸç§å…³ç³»é¢„æµ‹æœºåˆ¶æ¥é¢„æµ‹æ•°æ®ç‚¹å¹¶ä¸å¤ªå€¼å¾—ï¼Œåè€Œå¯ä»¥é€‰æ‹©å­¦ä¹ å‚æ•°åŒ–é¢„æµ‹ï¼ŒåŸºæœ¬ä¸Šåœ¨é¢„æµ‹ä»»ä¸€æ•°æ®ç‚¹æ—¶å¿½ç•¥å…¶ä»–æ•°æ®ç‚¹ã€‚
- en: And so this probably leads to some kind of like interesting ideas where perhaps
    you could do like postdoc pruning or something like thatã€‚taking away the tension
    between data points and doing fine tuningï¼Œ let's sayã€‚All rightã€‚so now I'll hand
    over to Y to talk a bit about learning some interesting relationshipsã€‚Yeah will
    you though I see that we're at the end of what the time isã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¯èƒ½å¼•å‘ä¸€äº›æœ‰è¶£çš„æƒ³æ³•ï¼Œä¹Ÿè®¸ä½ å¯ä»¥è¿›è¡ŒåæœŸå‰ªæä¹‹ç±»çš„ï¼Œæ¶ˆé™¤æ•°æ®ç‚¹ä¹‹é—´çš„ç´§å¼ å…³ç³»ï¼Œè¿›è¡Œå¾®è°ƒã€‚å¥½å§ï¼Œç°åœ¨æˆ‘æŠŠæ—¶é—´äº¤ç»™Yï¼Œè®©ä»–è°ˆè°ˆå­¦ä¹ ä¸€äº›æœ‰è¶£çš„å…³ç³»ã€‚æ˜¯çš„ï¼Œæˆ‘çœ‹åˆ°æˆ‘ä»¬æ¥è¿‘æ—¶é—´çš„å°½å¤´ã€‚
- en: but like I know there's a buffer planned and or something can you I can go through
    this experiment we can have a more discussion what's what do you guys preferï¼Ÿ
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘çŸ¥é“æœ‰ä¸ªç¼“å†²è®¡åˆ’ï¼Œæˆ–è€…è¯´å¯ä»¥é€šè¿‡è¿™ä¸ªå®éªŒï¼Œæˆ‘ä»¬å¯ä»¥æ›´æ·±å…¥åœ°è®¨è®ºï¼Œå¤§å®¶æ›´å€¾å‘äºä»€ä¹ˆï¼Ÿ
- en: Yeahï¼Œ I thinkã€‚Normallyï¼Œ what we do is we would sort of stop the recording at
    this point and have an off the record discussionã€‚Andã€‚I guess the question to ask
    is does anyone have any questions at this pointï¼Ÿ
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºï¼Œé€šå¸¸æˆ‘ä»¬ä¼šåœ¨è¿™ä¸ªæ—¶å€™åœæ­¢å½•éŸ³ï¼Œè¿›è¡Œä¸€ä¸ªéæ­£å¼çš„è®¨è®ºã€‚ç„¶åï¼Œæˆ‘æƒ³é—®çš„æ˜¯ï¼Œå¤§å®¶ç›®å‰æœ‰ä»»ä½•é—®é¢˜å—ï¼Ÿ
- en: But I think we've basicallyã€‚Laantching questions as they comeï¼Œ so I personally
    feel fine just yeahã€‚considering this a sort of question throughoutã€‚Yeahã€‚I guess
    that sounds that sounds good ya you can go forward with it with like with your
    tongue as planned and yeah we can data we can see about the time thingã€‚I think
    this will only be like anotherã€‚4our or five minutes I shouldn't go for itï¼Œ yes
    for sureã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯åœ¨æ ¹æ®é—®é¢˜çš„å‡ºç°æ¥è¿›è¡Œæé—®ï¼Œæ‰€ä»¥æˆ‘ä¸ªäººè§‰å¾—è¿™æ ·ä¹Ÿä¸é”™ï¼Œæ˜¯çš„ï¼Œè€ƒè™‘åˆ°è¿™ä¸€ç‚¹å°±ç®—æ˜¯ä¸ªé—®é¢˜ã€‚æ˜¯çš„ï¼Œæˆ‘æƒ³è¿™å¬èµ·æ¥ä¸é”™ï¼Œä½ å¯ä»¥æŒ‰è®¡åˆ’ç»§ç»­è¿›è¡Œï¼Œå—¯ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹æ—¶é—´çš„äº‹æƒ…ã€‚æˆ‘è®¤ä¸ºè¿™åªä¼šå†èŠ±å¤§çº¦å››äº”åˆ†é’Ÿï¼Œæˆ‘åº”è¯¥å¯ä»¥ç»§ç»­ï¼Œæ²¡é—®é¢˜ã€‚
- en: All rightã€‚So Neil has now told us how well entities perform in real data and
    that they do make use of information from other samples of the inputã€‚But we're
    now going to take this a bit further and come up with some toy experiments that
    test a bitã€‚the extent to which entities can learn to look up information from
    other roleã€‚like the extent to which they can learn this nonprometric prediction
    mechanismã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æ‰€ä»¥å°¼å°”ç°åœ¨å‘Šè¯‰æˆ‘ä»¬å®ä½“åœ¨çœŸå®æ•°æ®ä¸­çš„è¡¨ç°ï¼Œä»¥åŠå®ƒä»¬ç¡®å®åˆ©ç”¨äº†è¾“å…¥å…¶ä»–æ ·æœ¬çš„ä¿¡æ¯ã€‚ä½†æˆ‘ä»¬ç°åœ¨è¦æ›´è¿›ä¸€æ­¥ï¼Œæå‡ºä¸€äº›ç©å…·å®éªŒæ¥æµ‹è¯•ä¸€ä¸‹å®ä½“ä»å…¶ä»–è§’è‰²ä¸­æŸ¥æ‰¾ä¿¡æ¯çš„èƒ½åŠ›ï¼Œä¹Ÿå°±æ˜¯å®ƒä»¬å­¦ä¹ è¿™ç§éå‚æ•°é¢„æµ‹æœºåˆ¶çš„ç¨‹åº¦ã€‚
- en: And so specifically what we'll do is we'll create the following semi syntheticthe
    data setã€‚So I want you to focus on A nowã€‚Yeah so we take one of the tabular data
    sets that we've used previously specifically the protein data setã€‚but it doesn't
    really matter what matters is there it's a regression data set and so now what
    we do is weã€‚The top half here is the original data setï¼Œ but the bottom half is
    a copy of the original data set where we have unveiled the true target value so
    now NPTs could learn to use attention between data points to achieve arbitrarily
    good performanceã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä»¥ä¸‹åŠåˆæˆæ•°æ®é›†ã€‚å› æ­¤ï¼Œæˆ‘å¸Œæœ›ä½ ç°åœ¨å…³æ³¨Aã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬å–ç”¨ä¹‹å‰ä½¿ç”¨çš„ä¸€ä¸ªè¡¨æ ¼æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯è›‹ç™½è´¨æ•°æ®é›†ã€‚ä½†å…¶å®å¹¶ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯è¿™æ˜¯ä¸€ä¸ªå›å½’æ•°æ®é›†ï¼Œæ‰€ä»¥ç°åœ¨æˆ‘ä»¬åšçš„æ˜¯ã€‚è¿™é‡Œçš„ä¸ŠåŠéƒ¨åˆ†æ˜¯åŸå§‹æ•°æ®é›†ï¼Œä½†ä¸‹åŠéƒ¨åˆ†æ˜¯åŸå§‹æ•°æ®é›†çš„å‰¯æœ¬ï¼Œæˆ‘ä»¬æ­ç¤ºäº†çœŸå®çš„ç›®æ ‡å€¼ï¼Œè¿™æ ·NPTå¯ä»¥å­¦ä¹ åœ¨æ•°æ®ç‚¹ä¹‹é—´ä½¿ç”¨æ³¨æ„åŠ›ï¼Œä»¥å®ç°ä»»æ„å¥½çš„æ€§èƒ½ã€‚
- en: they could learn to look up the target values in these matching duplicate row
    and then paste them back into that must out target value and then a test time
    of course we put in a novel test data input where this mechanism is also possible
    just to make sure that it hasn't learned to memorize anything but has actually
    learned this correct relational mechanismã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯ä»¥å­¦ä¹ åœ¨è¿™äº›åŒ¹é…çš„é‡å¤è¡Œä¸­æŸ¥æ‰¾ç›®æ ‡å€¼ï¼Œç„¶åå°†å®ƒä»¬ç²˜è´´å›å¿…é¡»è¾“å‡ºçš„ç›®æ ‡å€¼ä¸­ï¼Œç„¶ååœ¨æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬å½“ç„¶è¾“å…¥ä¸€ä¸ªæ–°çš„æµ‹è¯•æ•°æ®ï¼Œè¿™ä¸ªæœºåˆ¶ä¹Ÿæ˜¯å¯è¡Œçš„ï¼Œåªæ˜¯ä¸ºäº†ç¡®ä¿å®ƒæ²¡æœ‰å­¦ä¹ å»è®°å¿†ä»»ä½•ä¸œè¥¿ï¼Œè€Œæ˜¯å®é™…å­¦ä¼šäº†è¿™ä¸ªæ­£ç¡®çš„å…³ç³»æœºåˆ¶ã€‚
- en: And so what we see is that indeed MPs do successfully learn to perform this
    lookup so what I'm visualizing here is attention maps and they very clearly show
    that let's say when predicting for this green row hereã€‚this first green row what
    emmputees look at is exactly only that other green row here and soã€‚ğŸ˜Šã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬çœ‹åˆ°ç¡®å®MPæˆåŠŸå­¦ä¹ åˆ°äº†æ‰§è¡Œè¿™ä¸ªæŸ¥æ‰¾çš„èƒ½åŠ›ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå¯è§†åŒ–çš„æ˜¯æ³¨æ„åŠ›å›¾ï¼Œå®ƒä»¬éå¸¸æ¸…æ¥šåœ°æ˜¾ç¤ºå‡ºï¼Œå½“é¢„æµ‹è¿™ä¸ªç»¿è‰²è¡Œæ—¶ï¼Œè¿™ä¸ªç¬¬ä¸€ä¸ªç»¿è‰²è¡Œï¼ŒMPä»¬çœ‹åˆ°çš„æ­£æ˜¯è¿™ä¸ªå…¶ä»–ç»¿è‰²è¡Œã€‚ğŸ˜Šã€‚
- en: This is really nice we can further look at the kind of theã€‚ğŸ˜Šã€‚The pieson correlation
    between what MPs should predict and what they actually do predict and so this
    is 99ã€‚9% this is much better than anything you could achieve with parametric prediction
    and so it seems thatmp here can actually discover this mechanism and discover
    here I feel like it's the right word because MPmps could have as we've seen it's
    just also continued to predict in parametric fashion right from each row independently
    this is really kind of showing to us that there is this bias in the model to learn
    to predict from other rows and of course that is also very attractive in this
    setting because it allows you to achieve arbitrary load loss in this setting or
    as low as you can optimize for itã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœŸçš„å¾ˆå¥½ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥è§‚å¯Ÿåˆ°ã€‚ğŸ˜Šã€‚MPsåº”è¯¥é¢„æµ‹çš„å’Œå®ƒä»¬å®é™…é¢„æµ‹çš„ä¹‹é—´çš„çš®å°”é€Šç›¸å…³æ€§ï¼Œè¿™ä¸ªç›¸å…³æ€§æ˜¯99.9%ã€‚è¿™æ¯”ä½ é€šè¿‡å‚æ•°é¢„æµ‹æ‰€èƒ½è¾¾åˆ°çš„ä»»ä½•ç»“æœéƒ½è¦å¥½ï¼Œæ‰€ä»¥ä¼¼ä¹MPåœ¨è¿™é‡Œç¡®å®å¯ä»¥å‘ç°è¿™ä¸ªæœºåˆ¶ï¼Œæˆ‘è§‰å¾—â€œå‘ç°â€è¿™ä¸ªè¯æ˜¯æ°å½“çš„ï¼Œå› ä¸ºMPä»¬å¯ä»¥å¦‚æˆ‘ä»¬æ‰€è§ï¼Œç»§ç»­ä»¥å‚æ•°çš„æ–¹å¼ç‹¬ç«‹åœ°ä»æ¯ä¸€è¡Œè¿›è¡Œé¢„æµ‹ï¼Œè¿™å®é™…ä¸Šå‘æˆ‘ä»¬å±•ç¤ºäº†æ¨¡å‹æœ‰åå‘äºå­¦ä¹ ä»å…¶ä»–è¡Œé¢„æµ‹çš„å€¾å‘ï¼Œå½“ç„¶åœ¨è¿™ç§æƒ…å†µä¸‹ä¹Ÿéå¸¸æœ‰å¸å¼•åŠ›ï¼Œå› ä¸ºå®ƒå…è®¸ä½ åœ¨è¿™ç§æƒ…å†µä¸‹å®ç°ä»»æ„ä½çš„æŸå¤±ï¼Œæˆ–è€…å°½å¯èƒ½åœ°ä¼˜åŒ–å®ƒã€‚
- en: ğŸ˜Šï¼ŒAnd soã€‚We kind of take that to mean that our gradient based discoveryã€‚non
    parametrics philosophy seems to make some senseã€‚And so we can take this a bit
    further by performing somewhat of an interventional experiment that investigates
    the extent to which NPTs have actually learned a robust causal mechanism that's
    underlying this semi synthetic data setã€‚ğŸ˜Šï¼ŒJust depending you know thisã€‚This extra
    extra column of test data that' already kind of coolã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥ã€‚æˆ‘ä»¬è®¤ä¸ºè¿™æ„å‘³ç€æˆ‘ä»¬çš„åŸºäºæ¢¯åº¦çš„å‘ç°ã€‚éå‚æ•°å“²å­¦ä¼¼ä¹æ˜¯æœ‰æ„ä¹‰çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¿›è¡ŒæŸç§å¹²é¢„å®éªŒè¿›ä¸€æ­¥æ¢ç´¢ï¼Œè°ƒæŸ¥NPTä»¬æ˜¯å¦å®é™…ä¸Šå­¦ä¹ åˆ°è¿™ç§åŠåˆæˆæ•°æ®é›†èƒŒåæ‰€éšå«çš„ç¨³å¥å› æœæœºåˆ¶çš„ç¨‹åº¦ã€‚ğŸ˜Šï¼Œè¿™ä»…ä»…æ˜¯ä¾èµ–äºè¿™ä¸ªé¢å¤–çš„æµ‹è¯•æ•°æ®åˆ—ï¼Œå·²ç»å¾ˆé…·äº†ã€‚
- en: but I think we can take a bit further and actually study if this generalizes
    beyond the data that we see in the training set or beyond data coming from this
    specific distribution and so what we now do is we intervene on individual duplicate
    data points at test time by varying their target value so now we only care about
    the prediction in a specific row we do this across all rows but at each time we
    just care about a single row what we do is we change the target value here that
    what we're hoping to see is and then NPT just adjusts the prediction as well right
    there's a very simple intervention experiment for us to test if NPTs have actually
    learned this mechanism and to some extent it also test robustness because now
    we're associating target values with features that are not part of the train distribution
    hereã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼Œå®é™…ä¸Šç ”ç©¶ä¸€ä¸‹å®ƒæ˜¯å¦å¯ä»¥æ¨å¹¿åˆ°è®­ç»ƒé›†ä¸­çœ‹åˆ°çš„æ•°æ®ä¹‹å¤–ï¼Œæˆ–è€…æ¥è‡ªè¿™ä¸ªç‰¹å®šåˆ†å¸ƒä¹‹å¤–çš„æ•°æ®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨åœ¨æµ‹è¯•æ—¶å¯¹å•ä¸ªé‡å¤æ•°æ®ç‚¹è¿›è¡Œå¹²é¢„ï¼Œé€šè¿‡æ”¹å˜å®ƒä»¬çš„ç›®æ ‡å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ç°åœ¨åªå…³æ³¨ç‰¹å®šè¡Œçš„é¢„æµ‹ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰è¡Œä¸­éƒ½è¿™æ ·åšï¼Œä½†æ¯æ¬¡æˆ‘ä»¬åªå…³å¿ƒä¸€è¡Œã€‚æˆ‘ä»¬åœ¨è¿™é‡Œæ”¹å˜ç›®æ ‡å€¼ï¼Œæˆ‘ä»¬å¸Œæœ›çœ‹åˆ°çš„æ˜¯ï¼Œç„¶åNPTä¹Ÿä¼šè°ƒæ•´é¢„æµ‹ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„å¹²é¢„å®éªŒï¼Œæµ‹è¯•NPTæ˜¯å¦çœŸçš„å­¦ä¼šäº†è¿™ä¸€æœºåˆ¶ï¼Œåœ¨æŸç§ç¨‹åº¦ä¸Šä¹Ÿæµ‹è¯•äº†é²æ£’æ€§ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬å°†ç›®æ ‡å€¼ä¸ä¸å±äºè®­ç»ƒåˆ†å¸ƒçš„ç‰¹å¾ç›¸å…³è”ã€‚
- en: ğŸ˜Šï¼ŒAnd so what we see is that as we as we adjust these values hereã€‚this is the
    kind of the duplicate value and then we here see the target value as we adjust
    themã€‚we can see the correlation stay is really really goodï¼Œ it's not quite 99ã€‚9%
    like on averageã€‚we're now at 99ã€‚6ï¼Œ but it' still veryï¼Œ very goodã€‚ğŸ˜Šã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå› æ­¤æˆ‘ä»¬çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬è°ƒæ•´è¿™äº›å€¼æ—¶ï¼Œè¿™æ˜¯é‡å¤å€¼ï¼Œç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°ç›®æ ‡å€¼ã€‚éšç€è°ƒæ•´ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç›¸å…³æ€§ä¿æŒå¾—éå¸¸å¥½ï¼Œè™½ç„¶å¹³å‡å€¼å¹¶ä¸æ˜¯99.9%ã€‚ç°åœ¨æ˜¯99.6%ï¼Œä½†ä»ç„¶éå¸¸å¥½ã€‚ğŸ˜Šã€‚
- en: At this point you might be slightly annoyed with me because you know standard
    nonprometric models can also solve this task right this is a task that I could
    solve by nearest enables Sure maybe you know I would have to change the input
    format bit because this is kind of like in a batch setting and I can just use
    masks but most generally nearest neighbor can also you know it also looks up different
    input points based on their features nearest neighbor doesn't learn to do thisã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œä½ å¯èƒ½å¯¹æˆ‘ç¨æ„Ÿçƒ¦æ¼ï¼Œå› ä¸ºä½ çŸ¥é“æ ‡å‡†çš„éå‚æ•°æ¨¡å‹ä¹Ÿå¯ä»¥è§£å†³è¿™ä¸ªä»»åŠ¡ï¼Œå¯¹å§ï¼Ÿè¿™å®é™…ä¸Šæ˜¯ä¸€ä¸ªæˆ‘å¯ä»¥é€šè¿‡æœ€è¿‘é‚»æ¥è§£å†³çš„ä»»åŠ¡ã€‚æ²¡é”™ï¼Œä¹Ÿè®¸ä½ çŸ¥é“ï¼Œæˆ‘éœ€è¦ç¨å¾®æ”¹å˜è¾“å…¥æ ¼å¼ï¼Œå› ä¸ºè¿™æœ‰ç‚¹åƒæ‰¹å¤„ç†è®¾ç½®ï¼Œæˆ‘å¯ä»¥ä½¿ç”¨æ©ç ï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œæœ€è¿‘é‚»ä¹Ÿä¼šæ ¹æ®ç‰¹å¾æŸ¥æ‰¾ä¸åŒçš„è¾“å…¥ç‚¹ï¼Œæœ€è¿‘é‚»å¹¶ä¸æ˜¯é€šè¿‡å­¦ä¹ æ¥å®ç°è¿™ä¸€ç‚¹çš„ã€‚
- en: I still think it's cool that we need to learn this because it does require you
    know a decent amount of you know computational sequences that we have to learn
    like match on the features look up target you copy it back and so on butã€‚It is
    in fact very easy for us to complicate this task to a degree such that essentially
    no other model that we know of can can solve this very easily and so like a really
    simple thing to do is just to add a plus one to all of the duplicate valuesã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»ç„¶è§‰å¾—æˆ‘ä»¬éœ€è¦å­¦ä¹ è¿™ä¸€ç‚¹å¾ˆé…·ï¼Œå› ä¸ºè¿™ç¡®å®éœ€è¦æˆ‘ä»¬å­¦ä¹ ç›¸å½“å¤šçš„è®¡ç®—åºåˆ—ï¼Œæ¯”å¦‚æ ¹æ®ç‰¹å¾åŒ¹é…ï¼ŒæŸ¥æ‰¾ç›®æ ‡å¹¶å¤åˆ¶ç­‰ç­‰ã€‚ä½†å®é™…ä¸Šï¼Œæˆ‘ä»¬å¾ˆå®¹æ˜“å°†è¿™ä¸ªä»»åŠ¡å¤æ‚åŒ–åˆ°ä¸€ä¸ªç¨‹åº¦ï¼Œä»¥è‡³äºåŸºæœ¬ä¸Šæ²¡æœ‰å…¶ä»–æ¨¡å‹å¯ä»¥å¾ˆå®¹æ˜“åœ°è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œä¸€ä¸ªéå¸¸ç®€å•çš„æ–¹æ³•å°±æ˜¯ç»™æ‰€æœ‰é‡å¤å€¼åŠ ä¸€ã€‚
- en: ğŸ˜Šï¼ŒSo now nearest neighbor would look up the right collar the right rowï¼Œ of courseã€‚but
    it would always predict the wrong target with a plus one on it and in in factï¼Œ
    many of theã€‚The models that we're aware ofï¼Œ they're not modelingã€‚ğŸ˜¡ã€‚The joint distribution
    over features and targets what they're modeling is the traditional distribution
    of the targets given the input features and so they also cannot do this and so
    for us it's really not a problem at all MPs will just learn to subtract another
    one and no problems and sure this is also still a very synthetic setting but I
    do think I mean I challenge you to come up with something that MPmpes can't solve
    but the other models can solve I think in general this masking mechanism and the
    nonmetric of the approach really nice in general and leads to lots of nice behavior
    in a variety of settings and so with that I think we can go to the conclusions
    which Neil is going to give youã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥ç°åœ¨æœ€è¿‘é‚»ä¼šæŸ¥æ‰¾æ­£ç¡®çš„é¢œè‰²å’Œæ­£ç¡®çš„è¡Œï¼Œå½“ç„¶ã€‚ä½†å®ƒæ€»æ˜¯ä¼šé¢„æµ‹é”™è¯¯çš„ç›®æ ‡ï¼Œç»“æœåŠ ä¸€ï¼Œå®é™…ä¸Šï¼Œè®¸å¤šæˆ‘ä»¬çŸ¥é“çš„æ¨¡å‹å¹¶æ²¡æœ‰å»ºæ¨¡ã€‚ğŸ˜¡ã€‚ç‰¹å¾å’Œç›®æ ‡çš„è”åˆåˆ†å¸ƒï¼Œä»–ä»¬å»ºæ¨¡çš„æ˜¯ç»™å®šè¾“å…¥ç‰¹å¾çš„ä¼ ç»Ÿç›®æ ‡åˆ†å¸ƒï¼Œå› æ­¤ä»–ä»¬ä¹Ÿæ— æ³•åšåˆ°è¿™ä¸€ç‚¹ã€‚å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œè¿™æ ¹æœ¬ä¸æ˜¯é—®é¢˜ï¼ŒMPåªä¼šå­¦ä¹ å†å‡å»ä¸€ä¸ªï¼Œä¹Ÿæ²¡é—®é¢˜ã€‚å½“ç„¶ï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªéå¸¸åˆæˆçš„ç¯å¢ƒï¼Œä½†æˆ‘ç¡®å®è®¤ä¸ºï¼Œæˆ‘æŒ‘æˆ˜ä½ æƒ³å‡ºä¸€äº›MPæ— æ³•è§£å†³è€Œå…¶ä»–æ¨¡å‹èƒ½è§£å†³çš„é—®é¢˜ã€‚æ€»çš„æ¥è¯´ï¼Œè¿™ç§æ©ç æœºåˆ¶å’Œæ–¹æ³•çš„éåº¦é‡æ€§åœ¨ä¸€èˆ¬æƒ…å†µä¸‹éå¸¸ä¸é”™ï¼Œå¹¶å¯¼è‡´åœ¨å„ç§è®¾ç½®ä¸­è¡¨ç°å‡ºè®¸å¤šè‰¯å¥½è¡Œä¸ºã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬å¯ä»¥è¿›å…¥ç»“è®ºï¼Œå°¼å°”å°†ä¸ºæ‚¨æä¾›ã€‚
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_39.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_39.png)'
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_40.png)'
- en: Yeahï¼Œ I thinkï¼Œ I meanï¼Œ we're can cut out theã€‚Main part hereã€‚I'll just fast forwardã€‚look
    at them Yeah yeahï¼Œ I was gonna say I think you you all get the gist NPpts take
    the entire data set input and they use self- attentiontention to model complex
    relationships between data pointsã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºï¼Œæˆ‘ä»¬å¯ä»¥å‰ªæ‰è¿™é‡Œçš„ä¸»è¦éƒ¨åˆ†ã€‚æˆ‘åªæ˜¯å¿«è¿›ã€‚çœ‹çœ‹ä»–ä»¬ã€‚æ˜¯çš„ï¼Œæ˜¯çš„ï¼Œæˆ‘æƒ³è¯´ï¼Œæˆ‘è®¤ä¸ºä½ ä»¬éƒ½æ˜ç™½äº†ï¼ŒNPptså–æ•´ä¸ªæ•°æ®é›†ä½œä¸ºè¾“å…¥ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶å»ºæ¨¡æ•°æ®ç‚¹ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚
- en: knowï¼Œ they do well in experiments on type of their data as well as image data
    we present some of these interventional experiments to show that they can solve
    complex reasoning tasks there's some more experiments in the paperã€‚I'd say that
    you knowï¼Œ the interesting type of future work is scaling type thingsã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥é“ï¼Œä»–ä»¬åœ¨å…¶æ•°æ®ç±»å‹ä»¥åŠæˆ‘ä»¬å‘ˆç°çš„å›¾åƒæ•°æ®çš„å®éªŒä¸­è¡¨ç°è‰¯å¥½ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€äº›å¹²é¢„å®éªŒï¼Œè¡¨æ˜ä»–ä»¬èƒ½å¤Ÿè§£å†³å¤æ‚çš„æ¨ç†ä»»åŠ¡ï¼Œè®ºæ–‡ä¸­è¿˜æœ‰æ›´å¤šå®éªŒã€‚æˆ‘è®¤ä¸ºï¼Œæœ‰è¶£çš„æœªæ¥å·¥ä½œç±»å‹æ˜¯æ‰©å±•è¿™ç±»äº‹ç‰©ã€‚
- en: So we can you knowï¼Œ not having this mini batchching approximationã€‚and then also
    just trying to expand this to some more interesting application domain So we talked
    a little bit about metal learningã€‚but it could also be things like you know few
    shot generalization in general domain adaptationã€‚semi surprise learningï¼Œ etcã€‚ğŸ˜Šï¼ŒSo
    I think if there's some more questionsã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¯ä»¥ï¼Œä¸ä½¿ç”¨è¿™ä¸ªå°æ‰¹é‡è¿‘ä¼¼ã€‚è€Œä¸”è¿˜è¯•å›¾å°†å…¶æ‰©å±•åˆ°ä¸€äº›æ›´æœ‰è¶£çš„åº”ç”¨é¢†åŸŸã€‚æˆ‘ä»¬ç¨å¾®è®¨è®ºäº†ä¸€ä¸‹å…ƒå­¦ä¹ ï¼Œä½†ä¹Ÿå¯ä»¥æ˜¯ä¸€äº›åƒå°‘é‡æ ·æœ¬çš„æ³›åŒ–ã€ä¸€èˆ¬é¢†åŸŸçš„é€‚åº”ã€åŠç›‘ç£å­¦ä¹ ç­‰ã€‚ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºå¦‚æœè¿˜æœ‰æ›´å¤šé—®é¢˜ã€‚
- en: maybe we can do some more discussionã€‚Yeahï¼Œ things sounds goodã€‚Great thanks for
    the talkã€‚I think everyone had a fun time seeã€‚I will just ask some general questions
    and then we can have like a discussion session with everyone after that So I think
    one thing that I noticed is like like this like you said like this is similar
    to like canons and I thought like this seems similar to like graph neural networks
    where I can think like each data point is like a node and then you can think of
    everything as a fully connected graph and you're learning some sort of attention
    rate in this graphã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥è¿›è¡Œæ›´å¤šè®¨è®ºã€‚æ˜¯çš„ï¼Œå¬èµ·æ¥ä¸é”™ã€‚éå¸¸æ„Ÿè°¢æ¼”è®²ã€‚æˆ‘è®¤ä¸ºå¤§å®¶éƒ½åº¦è¿‡äº†æ„‰å¿«çš„æ—¶å…‰ã€‚æˆ‘ä¼šé—®ä¸€äº›ä¸€èˆ¬æ€§çš„é—®é¢˜ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åœ¨é‚£ä¹‹åä¸å¤§å®¶è¿›è¡Œè®¨è®ºã€‚æˆ‘æ³¨æ„åˆ°çš„ä¸€ä»¶äº‹æ˜¯ï¼Œä½ è¯´çš„è¿™å’Œ*å¡å†œ*ç›¸ä¼¼ï¼Œæˆ‘è§‰å¾—è¿™ä¼¼ä¹ä¸å›¾ç¥ç»ç½‘ç»œç›¸ä¼¼ï¼Œæˆ‘å¯ä»¥æŠŠæ¯ä¸ªæ•°æ®ç‚¹çœ‹ä½œä¸€ä¸ªèŠ‚ç‚¹ï¼Œç„¶åä½ å¯ä»¥æŠŠæ‰€æœ‰ä¸œè¥¿è§†ä¸ºä¸€ä¸ªå®Œå…¨è¿æ¥çš„å›¾ï¼Œå¹¶ä¸”ä½ åœ¨è¿™ä¸ªå›¾ä¸­å­¦ä¹ æŸç§æ³¨æ„åŠ›æƒé‡ã€‚
- en: So this is like a note prediction task you are kind of doing on this sort of
    like graph structureã€‚so any comments on that like is it similar to like graph
    neural networks or is it like other differencesï¼Ÿ
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±åƒæ˜¯ä¸€ä¸ªéŸ³ç¬¦é¢„æµ‹ä»»åŠ¡ï¼Œä½ åœ¨è¿™ç§å›¾ç»“æ„ä¸Šè¿›è¡Œçš„ã€‚å¯¹æ­¤æœ‰ä»€ä¹ˆè¯„è®ºå—ï¼Ÿè¿™æ˜¯å¦ç±»ä¼¼äºå›¾ç¥ç»ç½‘ç»œï¼Œè¿˜æ˜¯æœ‰å…¶ä»–ä¸åŒä¹‹å¤„ï¼Ÿ
- en: Yeah this is a very good observation yeah I think there are a lot of similarities
    to work on graphraph neural networks if we want to talk about differences the
    differences might be that we're kind of assuming a fully connected graph right
    and so you could maybe also phrase that as we're discovering the relational structure
    or as graphraph neural networks usually assume that it's given but that's also
    not always true and so there are a lot of similarities I don't know Neil if there
    is something specific you would like to mention go ahead but it's a very good
    observation and then we also do feel that that's the case and we've added an extra
    sectional related work to graphra neural networks in the updated version of the
    that will be online soonã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„è§‚å¯Ÿï¼Œæˆ‘è®¤ä¸ºåœ¨å›¾ç¥ç»ç½‘ç»œæ–¹é¢æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚å¦‚æœæˆ‘ä»¬è¦è°ˆè®ºå·®å¼‚ï¼Œå¯èƒ½çš„å·®å¼‚åœ¨äºæˆ‘ä»¬å‡è®¾ä¸€ä¸ªå®Œå…¨è¿æ¥çš„å›¾ï¼Œå› æ­¤ä½ ä¹Ÿå¯ä»¥å°†å…¶è¡¨è¿°ä¸ºæˆ‘ä»¬æ­£åœ¨å‘ç°å…³ç³»ç»“æ„ï¼Œè€Œå›¾ç¥ç»ç½‘ç»œé€šå¸¸å‡è®¾å®ƒæ˜¯ç»™å®šçš„ï¼Œä½†è¿™å¹¶ä¸æ€»æ˜¯æ­£ç¡®çš„ï¼Œå› æ­¤æœ‰å¾ˆå¤šç›¸ä¼¼ä¹‹å¤„ã€‚æˆ‘ä¸çŸ¥é“å°¼å°”ä½ æ˜¯å¦æƒ³æåˆ°ä¸€äº›å…·ä½“çš„å†…å®¹ï¼Œç»§ç»­è¯´å§ï¼Œè¿™çœŸæ˜¯ä¸ªå¾ˆå¥½çš„è§‚å¯Ÿï¼Œæˆ‘ä»¬ä¹Ÿç¡®å®æ„Ÿè§‰åˆ°è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬åœ¨æ›´æ–°ç‰ˆæœ¬ä¸­æ·»åŠ äº†ä¸€ä¸ªä¸å›¾ç¥ç»ç½‘ç»œç›¸å…³çš„é¢å¤–éƒ¨åˆ†ï¼Œè¿™ä¸ªç‰ˆæœ¬å¾ˆå¿«å°±ä¼šåœ¨çº¿ä¸Šã€‚
- en: Got itã€‚Yeahï¼Œ I agree with everything you've saidã€‚I think that the closest work
    from the GNN literature that we were looking at a little bit was this neural relational
    inference paperã€‚which uses mess passage message passing neural networks to try
    to kind of like learn edges that may or may not exist and help for like extrapolating
    I think positions of like particles in like a multipart system or somethingã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜ç™½äº†ï¼Œæ˜¯çš„ï¼Œæˆ‘åŒæ„ä½ æ‰€è¯´çš„ä¸€åˆ‡ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬æ‰€å…³æ³¨çš„ä¸å›¾ç¥ç»ç½‘ç»œæ–‡çŒ®ä¸­æœ€æ¥è¿‘çš„å·¥ä½œæ˜¯è¿™ç¯‡ç¥ç»å…³ç³»æ¨ç†è®ºæ–‡ï¼Œå®ƒä½¿ç”¨æ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œè¯•å›¾å­¦ä¹ å¯èƒ½å­˜åœ¨æˆ–ä¸å­˜åœ¨çš„è¾¹ï¼Œå¹¶å¸®åŠ©æ¨æ–­å¤šç²’å­ç³»ç»Ÿä¸­ç²’å­çš„ä½ç½®æˆ–å…¶ä»–ä»€ä¹ˆã€‚
- en: which is like kind of a similar idea to us like you knowã€‚if you don't have these
    edges as given the attention mechanism kind of approximate and interesting relationship
    amongst some interacting thingsã€‚I see got it yeah that's pretty cool Another thing
    is like so you mostly look on like tableular dataã€‚but can you also like have other
    modalities like if you want to do language or somethingã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯¹æˆ‘ä»¬æ¥è¯´æœ‰ç‚¹åƒç±»ä¼¼çš„æƒ³æ³•ï¼Œä½ çŸ¥é“çš„ã€‚å¦‚æœä½ æ²¡æœ‰è¿™äº›è¾¹ä½œä¸ºç»™å®šï¼Œæ³¨æ„æœºåˆ¶ä¼šå¤§è‡´è¿‘ä¼¼ä¸€äº›äº¤äº’äº‹ç‰©ä¹‹é—´çš„æœ‰è¶£å…³ç³»ã€‚æˆ‘æ˜ç™½äº†ï¼Œè¿™çœŸä¸é”™ã€‚å¦ä¸€ä»¶äº‹æ˜¯ï¼Œä½ ä¸»è¦å…³æ³¨çš„æ˜¯è¡¨æ ¼æ•°æ®ï¼Œä½†ä½ æ˜¯å¦ä¹Ÿå¯ä»¥ä½¿ç”¨å…¶ä»–æ¨¡æ€ï¼Œæ¯”å¦‚å¦‚æœä½ æƒ³å¤„ç†è¯­è¨€æˆ–å…¶ä»–ä¸œè¥¿ã€‚
- en: can you still use non para transformomersï¼ŸYeahï¼Œ so I think part of our motivation
    for doing tabular was because we felt like tabular data is in a senseã€‚a generalization
    of let's say the language dataï¼Œ for exampleï¼Œ I meanã€‚I guess there's these other
    notions like that people have brought up like padding but ultimately you can think
    of it as like a bunch of categorical attributes So it is it is definitely generalizable
    to things like sentences and we do you know images so yeah I think actually like
    like I always go back and forth on whether or not I think smaller or larger data
    is more interesting for us So I think small data is really interesting because
    we can't just fit the entire data set into it and all of this just works out of
    the books without any extra thought but large data is actually also really interesting
    becauseã€‚
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜èƒ½ä½¿ç”¨éå‚æ•°å˜æ¢å™¨å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬åšè¡¨æ ¼æ•°æ®çš„éƒ¨åˆ†åŠ¨æœºæ˜¯å› ä¸ºæˆ‘ä»¬è§‰å¾—è¡¨æ ¼æ•°æ®åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯è¯­è¨€æ•°æ®çš„ä¸€ç§æ¨å¹¿ã€‚ä¾‹å¦‚ï¼Œæˆ‘æƒ³ã€‚è¿™äº›å…¶ä»–æ¦‚å¿µåƒå¡«å……ï¼Œæœ€ç»ˆä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œä¸€å †ç±»åˆ«å±æ€§ï¼Œæ‰€ä»¥å®ƒç¡®å®å¯ä»¥æ¨å¹¿åˆ°åƒå¥å­è¿™æ ·çš„ä¸œè¥¿ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿå¤„ç†å›¾åƒã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºå…¶å®æˆ‘æ€»æ˜¯åœ¨æ€è€ƒå°æ•°æ®æˆ–å¤§æ•°æ®å¯¹æˆ‘ä»¬æ¥è¯´å“ªä¸ªæ›´æœ‰è¶£ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºå°æ•°æ®çœŸçš„å¾ˆæœ‰è¶£ï¼Œå› ä¸ºæˆ‘ä»¬ä¸èƒ½å°†æ•´ä¸ªæ•°æ®é›†æ”¾å…¥å…¶ä¸­ï¼Œè€Œè¿™ä¸€åˆ‡éƒ½å¯ä»¥æ¯«ä¸è´¹åŠ›åœ°è¿ä½œï¼Œä½†å¤§æ•°æ®å®é™…ä¸Šä¹Ÿå¾ˆæœ‰è¶£ï¼Œå› ä¸ºã€‚
- en: Sure you might have to introduce some app mechanism or some lookup mechanism
    because you can't always have the entire data set in but at the same time you
    are very explicitly kind of trading off the compute that you use to look up with
    the compute that you need to store like how much how many parameters in GPT are
    used for storing data right there's lots of memorization happening in these models
    and we know that and so maybe we can use the parameters more efficiently to learn
    lookup type behavior right that is more close to this you know neurocanin or whatever
    so I think these are very excitingã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ å¯èƒ½éœ€è¦å¼•å…¥ä¸€äº›åº”ç”¨æœºåˆ¶æˆ–æŸ¥æ‰¾æœºåˆ¶ï¼Œå› ä¸ºä½ ä¸èƒ½æ€»æ˜¯å°†æ•´ä¸ªæ•°æ®é›†åŠ è½½è¿›æ¥ï¼Œä½†ä¸æ­¤åŒæ—¶ï¼Œä½ éå¸¸æ˜ç¡®åœ°åœ¨æƒè¡¡ç”¨äºæŸ¥æ‰¾çš„è®¡ç®—ä¸ç”¨äºå­˜å‚¨çš„è®¡ç®—ï¼Œæ¯”å¦‚GPTä¸­æœ‰å¤šå°‘å‚æ•°ç”¨äºå­˜å‚¨æ•°æ®ï¼Œå¯¹å§ï¼Œè¿™äº›æ¨¡å‹ä¸­å‘ç”Ÿäº†å¤§é‡çš„è®°å¿†åŒ–ï¼Œæˆ‘ä»¬å¯¹æ­¤æ˜¯çŸ¥é“çš„ï¼Œå› æ­¤ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ›´æœ‰æ•ˆåœ°åˆ©ç”¨å‚æ•°æ¥å­¦ä¹ æŸ¥æ‰¾ç±»å‹çš„è¡Œä¸ºï¼Œè¿™æ›´æ¥è¿‘äºä½ çŸ¥é“çš„ç¥ç»åŠ¨åŠ›å­¦æˆ–å…¶ä»–ä»€ä¹ˆï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™äº›éƒ½æ˜¯éå¸¸ä»¤äººå…´å¥‹çš„ã€‚
- en: Questions yeah yeah I'll also be looking forward to the future works because
    this seems like a very good way to like do one shot learning kind of situations
    so yeah really very interesting to see thatã€‚Okayï¼Œ so I will stop the recording
    and we can have like any other questionsã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: é—®é¢˜æ˜¯çš„ï¼Œæˆ‘ä¹ŸæœŸå¾…æœªæ¥çš„ä½œå“ï¼Œå› ä¸ºè¿™ä¼¼ä¹æ˜¯å¤„ç†ä¸€-shotå­¦ä¹ æƒ…å¢ƒçš„å¾ˆå¥½æ–¹æ³•ï¼Œæ‰€ä»¥çœŸçš„å¾ˆæœ‰è¶£çœ‹åˆ°è¿™ä¸€ç‚¹ã€‚å¥½çš„ï¼Œæˆ‘å°†åœæ­¢å½•éŸ³ï¼Œæˆ‘ä»¬å¯ä»¥è¿›è¡Œå…¶ä»–é—®é¢˜ã€‚
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_42.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_42.png)'
