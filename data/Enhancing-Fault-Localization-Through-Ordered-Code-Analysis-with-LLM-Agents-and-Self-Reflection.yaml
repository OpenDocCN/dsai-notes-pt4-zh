- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2025-01-11 12:14:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:14:01'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents and
    Self-Reflection
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过有序代码分析与LLM代理和自我反思提升故障定位
- en: 来源：[https://arxiv.org/html/2409.13642/](https://arxiv.org/html/2409.13642/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2409.13642/](https://arxiv.org/html/2409.13642/)
- en: Md Nakhla Rafi [0009-0005-4707-8985](https://orcid.org/0009-0005-4707-8985 "ORCID
    identifier") Concordia UniversityMontrealCanada [mdnakhla.rafi@mail.concordia.ca](mailto:mdnakhla.rafi@mail.concordia.ca)
    ,  Dong Jae Kim [0000-0002-3181-0001](https://orcid.org/0000-0002-3181-0001 "ORCID
    identifier") DePaul UniversityChicagoUSA [dkim121@depaul.edu](mailto:dkim121@depaul.edu)
    ,  Tse-Hsun (Peter) Chen [0000-0003-4027-0905](https://orcid.org/0000-0003-4027-0905
    "ORCID identifier") Concordia UniversityMontrealCanada [peterc@encs.concordia.ca](mailto:peterc@encs.concordia.ca)
     and  Shaowei Wang [0000-0003-3823-1771](https://orcid.org/0000-0003-3823-1771
    "ORCID identifier") University of ManitobaWinnipegCanada [Shaowei.Wang@umanitoba.ca](mailto:Shaowei.Wang@umanitoba.ca)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Md Nakhla Rafi [0009-0005-4707-8985](https://orcid.org/0009-0005-4707-8985 "ORCID
    identifier") Concordia University蒙特利尔加拿大 [mdnakhla.rafi@mail.concordia.ca](mailto:mdnakhla.rafi@mail.concordia.ca)
    ,  Dong Jae Kim [0000-0002-3181-0001](https://orcid.org/0000-0002-3181-0001 "ORCID
    identifier") DePaul University芝加哥美国 [dkim121@depaul.edu](mailto:dkim121@depaul.edu)
    ,  Tse-Hsun (Peter) Chen [0000-0003-4027-0905](https://orcid.org/0000-0003-4027-0905
    "ORCID identifier") Concordia University蒙特利尔加拿大 [peterc@encs.concordia.ca](mailto:peterc@encs.concordia.ca)  和
    Shaowei Wang [0000-0003-3823-1771](https://orcid.org/0000-0003-3823-1771 "ORCID
    identifier") 曼尼托巴大学温尼伯加拿大 [Shaowei.Wang@umanitoba.ca](mailto:Shaowei.Wang@umanitoba.ca)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Locating and fixing software faults is a time-consuming and resource-intensive
    task in software development. Traditional fault localization methods, such as
    Spectrum-Based Fault Localization (SBFL), rely on statistical analysis of test
    coverage data but often suffer from lower accuracy. Learning-based techniques,
    while more effective, require extensive training data and can be computationally
    expensive. Recent advancements in Large Language Models (LLMs) offer promising
    improvements in fault localization by enhancing code comprehension and reasoning.
    However, these LLM-based techniques still face challenges, including token limitations,
    degraded performance with long inputs, and difficulties managing large-scale projects
    with complex systems involving multiple interacting components. To address these
    issues, we introduce LLM4FL, a novel LLM-agent-based fault localization approach
    that integrates SBFL rankings with a divide-and-conquer strategy. By dividing
    large coverage data into manageable groups and employing multiple LLM agents through
    prompt chaining, LLM4FL navigates the codebase and localizes faults more effectively.
    The approach also incorporates self-reflection and chain-of-thought reasoning,
    enabling agents to iteratively generate fixes and re-rank suspicious methods.
    We evaluated LLM4FL on the Defects4J (V2.0.0) benchmark, comprising 675 real-world
    faults from 14 open-source Java projects. Our results demonstrate that LLM4FL
    outperforms AutoFL by 19.27% in Top-1 accuracy and surpasses state-of-the-art
    supervised techniques such as DeepFL and Grace, all without task-specific training.
    Additionally, we highlight the impact of coverage splitting and prompt chaining
    on fault localization performance and show that different method ordering can
    improve Top-1 accuracy by up to 22%.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 定位和修复软件故障是软件开发中耗时且资源密集的任务。传统的故障定位方法，如基于谱的故障定位（SBFL），依赖于对测试覆盖数据的统计分析，但通常精度较低。基于学习的技术虽然更为有效，但需要大量训练数据，并且计算成本较高。近年来，大型语言模型（LLM）的进展为故障定位带来了有希望的改进，通过增强代码理解和推理能力。然而，这些基于LLM的技术仍面临一些挑战，包括令牌限制、长输入时性能下降，以及在处理涉及多个交互组件的复杂系统的大规模项目时的困难。为了解决这些问题，我们提出了LLM4FL，这是一种基于LLM代理的故障定位新方法，将SBFL排序与分而治之策略相结合。通过将大规模的覆盖数据分割为可管理的组，并通过提示链使用多个LLM代理，LLM4FL能够更有效地遍历代码库并定位故障。该方法还结合了自我反思和思维链推理，使代理能够迭代生成修复并重新排序可疑的方法。我们在Defects4J（V2.0.0）基准测试上评估了LLM4FL，该基准测试包括来自14个开源Java项目的675个真实故障。我们的结果表明，LLM4FL在Top-1准确度上比AutoFL高出19.27%，并且超越了深度学习和Grace等最先进的监督技术，且无需特定任务的训练。此外，我们还强调了覆盖拆分和提示链对故障定位性能的影响，并展示了不同方法排序可以将Top-1准确度提高多达22%。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: The process of locating and fixing software faults requires significant time
    and effort. Research shows that software development teams allocate more than
    half of their budgets to testing and debugging activities (Hait and Tassey, [2002](https://arxiv.org/html/2409.13642v1#bib.bib20);
    Alaboudi and LaToza, [2021](https://arxiv.org/html/2409.13642v1#bib.bib6)). As
    software systems become increasingly complex, the demand for more efficient and
    accurate fault localization techniques continues to grow. Hence, to assist developers
    and reduce debugging costs, researchers have developed various fault localization
    techniques (Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33);
    Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39); Qian et al.,
    [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47); Li et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib36);
    Abreu et al., [2009](https://arxiv.org/html/2409.13642v1#bib.bib4); Sohn and Yoo,
    [2017](https://arxiv.org/html/2409.13642v1#bib.bib55)). These techniques analyze
    code coverage and program execution to identify the most likely faulty components,
    assisting developers in finding the fault.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 定位和修复软件故障的过程需要大量的时间和精力。研究表明，软件开发团队将超过一半的预算用于测试和调试活动（Hait 和 Tassey，[2002](https://arxiv.org/html/2409.13642v1#bib.bib20)；Alaboudi
    和 LaToza，[2021](https://arxiv.org/html/2409.13642v1#bib.bib6)）。随着软件系统变得越来越复杂，对更高效、准确的故障定位技术的需求持续增长。因此，为了帮助开发人员并降低调试成本，研究人员开发了多种故障定位技术（Li
    等，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)；Lou 等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)；Qian
    等，[2023b](https://arxiv.org/html/2409.13642v1#bib.bib47)；Li 等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36)；Abreu
    等，[2009](https://arxiv.org/html/2409.13642v1#bib.bib4)；Sohn 和 Yoo，[2017](https://arxiv.org/html/2409.13642v1#bib.bib55)）。这些技术通过分析代码覆盖率和程序执行来识别最可能出现故障的组件，帮助开发人员找到故障所在。
- en: However, despite the advances in fault localization techniques, many existing
    approaches still struggle with scalability and precision. Traditional methods,
    such as Spectrum-Based Fault Localization (SBFL), use statistical analysis to
    analyze coverage data from passing and failing test cases to rank suspicious code
    elements (Abreu et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3)).
    While these techniques provide valuable insights, their accuracy is lower. Their
    reliance on statistical correlations between test failures and code coverage does
    not always capture the deeper semantic relationships needed for more accurate
    fault localization (Wong et al., [2016](https://arxiv.org/html/2409.13642v1#bib.bib63);
    Xie et al., [2013](https://arxiv.org/html/2409.13642v1#bib.bib67); Le et al.,
    [2013](https://arxiv.org/html/2409.13642v1#bib.bib30)). To address these issues,
    recent techniques applied machine learning and deep learning models to improve
    fault localization (Sohn and Yoo, [2017](https://arxiv.org/html/2409.13642v1#bib.bib55);
    Zhang et al., [2019b](https://arxiv.org/html/2409.13642v1#bib.bib75); Li et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib36), [2019](https://arxiv.org/html/2409.13642v1#bib.bib33)).
    These methods enhance the ranking of suspicious code elements by incorporating
    additional information like code complexity, text similarity, and historical fault
    data. Researchers also leveraged models like Graph Neural Networks (GNNs) to represent
    code structures and achieved state-of-the-art fault localization accuracy (Lou
    et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39); Qian et al., [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47);
    Rafi et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib50); Lou et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib39)). However, these techniques
    often require extensive training data and significant training time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管故障定位技术取得了一些进展，许多现有的方法仍然在可扩展性和精确度上存在困难。传统方法，如基于谱的故障定位（SBFL），通过统计分析来分析来自通过和失败测试用例的覆盖数据，以对可疑的代码元素进行排序（Abreu等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3)）。虽然这些技术提供了有价值的见解，但它们的准确性较低。它们依赖于测试失败与代码覆盖之间的统计相关性，但并不总是能够捕捉到更准确的故障定位所需的更深层语义关系（Wong等人，[2016](https://arxiv.org/html/2409.13642v1#bib.bib63);
    Xie等人，[2013](https://arxiv.org/html/2409.13642v1#bib.bib67); Le等人，[2013](https://arxiv.org/html/2409.13642v1#bib.bib30)）。为了解决这些问题，最近的技术应用了机器学习和深度学习模型来改进故障定位（Sohn和Yoo，[2017](https://arxiv.org/html/2409.13642v1#bib.bib55);
    Zhang等人，[2019b](https://arxiv.org/html/2409.13642v1#bib.bib75); Li等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36)，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)）。这些方法通过引入额外的信息，如代码复杂性、文本相似性和历史故障数据，增强了对可疑代码元素的排序。研究人员还利用了图神经网络（GNN）等模型来表示代码结构，并实现了最先进的故障定位精度（Lou等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Qian等人，[2023b](https://arxiv.org/html/2409.13642v1#bib.bib47); Rafi等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50);
    Lou等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)）。然而，这些技术通常需要大量的训练数据和较长的训练时间。
- en: Recent advances in Large Language Models (LLMs) have shown great potential for
    fault localization due to their strong language comprehension and generation capabilities (Abedu
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib2); Lin et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib37)).
    LLMs trained on extensive programming datasets can understand code structure,
    interpret error messages, and even suggest fixes for common software bugs (Kang
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26); Wu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib65);
    Pu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib45); Li et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib32)).
    These models, with their ability to analyze and process both natural language
    and code, present an opportunity to significantly improve traditional fault localization
    methods by incorporating deeper semantic analysis and context-aware reasoning.
    Wu et al. (Wu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib65)),
    directly present LLMs with faulty methods or classes with test failure information
    and ask to locate the issue, which provides valuable insights. Kang et al. (Kang
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)) which operates
    as an automated fault localization technique that leverages LLMs to localize fault
    given a single failing test. It focuses on method-level fault localization and
    provides bug location and also a natural language explanation of why a particular
    code location is likely to be faulty.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLM）在故障定位方面的最新进展展示了其巨大潜力，因为它们具备强大的语言理解和生成能力（Abedu 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib2);
    Lin 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib37)）。在大量编程数据集上训练的LLM能够理解代码结构、解释错误信息，甚至为常见软件缺陷提供修复建议（Kang
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26); Wu 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65);
    Pu 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib45); Li 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib32)）。这些模型凭借分析和处理自然语言及代码的能力，提供了显著改善传统故障定位方法的机会，通过结合更深层次的语义分析和上下文感知推理来提升定位效果。Wu
    等（Wu 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65)）直接将有缺陷的方法或类与测试失败信息呈现给LLM，要求定位问题，这为我们提供了宝贵的见解。Kang
    等（Kang 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）则提出了一种自动化故障定位技术，利用LLM在给定单一失败测试的情况下进行故障定位。该方法专注于方法级别的故障定位，并提供缺陷位置以及为何特定代码位置可能存在故障的自然语言解释。
- en: Despite their potential, existing LLM-based fault localization techniques face
    several challenges. The token limitations of LLMs restrict their ability to effectively
    process long code files or large sets of code coverage data, which is often required
    when dealing with large-scale software systems (Hadi et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib19);
    Hou et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib22); Wu et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib65)). Additionally, the performance
    of LLMs can degrade when applied to complex systems that require the model to
    reason over multiple interacting components, making it difficult to maintain accuracy
    and consistency across broader projects (Levy et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib31);
    Liu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib38)). Furthermore,
    current LLM-based techniques have yet to fully explore how these models can be
    effectively integrated with traditional fault localization techniques to maximize
    their strengths in a complementary and efficient manner (Wu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib65);
    Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26); Yang et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib73); Bin Murtaza et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib11)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的基于大型语言模型（LLM）的故障定位技术具有潜力，但仍面临若干挑战。LLM的令牌限制限制了它们有效处理长代码文件或大量代码覆盖数据的能力，而这些数据在处理大规模软件系统时通常是必需的（Hadi
    等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib19); Hou 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib22);
    Wu 等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65)）。此外，当应用于需要模型推理多个交互组件的复杂系统时，LLM的性能可能会下降，这使得在更广泛的项目中保持准确性和一致性变得困难（Levy
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib31); Liu 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib38)）。此外，当前基于LLM的技术尚未充分探索如何将这些模型与传统的故障定位技术有效集成，从而以互补和高效的方式最大化它们的优势（Wu
    等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65); Kang 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26);
    Yang 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib73); Bin Murtaza 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib11)）。
- en: In this paper, we propose LLM4FL, an LLM-based fault localization. To address
    the challenges of analyzing large-scale software projects, where code coverage
    and complexity often exceed LLM token limits (Wu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib65);
    Liu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib38)), LLM4FL implements
    a divide-and-conquer strategy. Before applying the divide-and-conquer strategy,
    we use an SBFL technique to sort the covered methods, building on findings from
    prior research (Chen et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib16))
    that demonstrate LLM performance improves when the order of instructions is carefully
    considered. Then, we divide the coverage data into manageable groups that the
    LLM can process within its token limits. Note that, regardless of the order, LLM4FL
    eventually analyzes every covered method.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了LLM4FL，一种基于LLM的故障定位方法。为了应对分析大规模软件项目的挑战，在这些项目中，代码覆盖率和复杂性通常超过LLM的令牌限制（Wu
    等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65); Liu 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib38)），LLM4FL实施了分治策略。在应用分治策略之前，我们使用一种SBFL技术对覆盖的方法进行排序，基于之前研究的发现（Chen
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib16)），该研究表明，当指令顺序得到仔细考虑时，LLM的性能会有所提升。然后，我们将覆盖数据分成可管理的组，这些组能够在LLM的令牌限制内处理。需要注意的是，无论顺序如何，LLM4FL最终会分析每一个被覆盖的方法。
- en: In addition to using a divide-and-conquer strategy, LLM4FL takes inspiration
    from how human developers debug software. Developers typically analyze multiple
    types of information, including error messages, stack traces, and code snippets,
    to incrementally narrow down potential faulty components (Alaboudi and LaToza,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib6); Böhme et al., [2017](https://arxiv.org/html/2409.13642v1#bib.bib12)).
    LLM4FL emulates this process by utilizing two LLM agents collaborating to iteratively
    and autonomously navigate the code to locate the faults. LLM4FL implements a Tester
    and a Debugger Agent, each tasked with specialized tools to assist in the fault
    localization process. The Tester Agent identifies and prioritizes suspicious methods
    by analyzing the failing test, and stack traces in different groups of covered
    methods to list a list of highly suspicious methods. It mimics the developer’s
    process of investigating suspicious areas in the code by understanding the context
    of error messages. Meanwhile, the Debugger Agent thoroughly evaluates the given
    list of candidate methods by navigating the code and ranks them based on its analysis.
    These two agents communicate through a prompt chaining mechanism that allows them
    to share insights and build upon each other’s findings.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了采用分治策略，LLM4FL还借鉴了人类开发人员调试软件的方式。开发人员通常会分析多种类型的信息，包括错误信息、堆栈跟踪和代码片段，以逐步缩小潜在故障组件的范围（Alaboudi
    和 LaToza，[2021](https://arxiv.org/html/2409.13642v1#bib.bib6); Böhme 等，[2017](https://arxiv.org/html/2409.13642v1#bib.bib12)）。LLM4FL通过利用两个LLM代理协作，迭代且自主地遍历代码以定位故障，模仿了这一过程。LLM4FL实现了一个测试者和一个调试者代理，每个代理都负责特定的工具，协助故障定位过程。测试者代理通过分析失败的测试和不同方法组中的堆栈跟踪，识别并优先考虑可疑的方法，并列出一份高度可疑的方法清单。它通过理解错误信息的上下文，模拟开发人员调查代码中可疑区域的过程。与此同时，调试者代理通过遍历代码，彻底评估给定的候选方法清单，并根据其分析对方法进行排名。这两个代理通过提示链机制进行沟通，从而共享见解并在彼此的发现基础上进行扩展。
- en: We evaluated LLM4FL using the Defects4J (V2.0.0) benchmark (Just et al., [2014](https://arxiv.org/html/2409.13642v1#bib.bib25)),
    which contains 675 real-world faults from 14 open-source Java projects. Our results
    demonstrate that LLM4FL surpasses LLM-based technique AutoFL (Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)),
    by achieving 19.27% higher Top-1 accuracy. Additionally, LLM4FL outperforms supervised
    techniques such as DeepFL (Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33))
    and Grace (Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39)),
    even without task-specific training. We also analyzed the impact of individual
    components within LLM4FL on fault localization accuracy. Our findings indicate
    that each component plays a significant role in its performance, with coverage
    splitting and prompt chaining contributing the most. When these components are
    removed, accuracy drops considerably, underscoring their importance in managing
    token limitations and facilitating efficient multi-agent collaboration. Moreover,
    we examined whether the initial ordering of methods provided to the LLM influences
    performance. The results reveal that method ordering is important, with a Top-1
    accuracy difference of up to 22% when comparing an execution-based ordering and
    the order provided by DepGraph (Rafi et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Defects4J（V2.0.0）基准（Just等人，[2014](https://arxiv.org/html/2409.13642v1#bib.bib25)）评估了LLM4FL，该基准包含来自14个开源Java项目的675个真实世界的缺陷。我们的结果表明，LLM4FL通过实现比基于LLM的技术AutoFL（Kang等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）高出19.27%的Top-1准确率，超越了AutoFL。此外，LLM4FL在没有任务特定训练的情况下，也优于深度学习监督技术，如DeepFL（Li等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)）和Grace（Lou等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)）。我们还分析了LLM4FL中各个组件对故障定位准确度的影响。我们的研究结果表明，每个组件在其性能中都发挥着重要作用，其中覆盖拆分和提示链条贡献最大。当这些组件被移除时，准确率大幅下降，强调了它们在管理令牌限制和促进高效的多代理协作中的重要性。此外，我们还考察了提供给LLM的方法初始顺序是否会影响性能。结果显示，方法顺序确实很重要，在比较基于执行的顺序和DepGraph（Rafi等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)）提供的顺序时，Top-1准确率差异高达22%。
- en: 'The paper makes the following contributions:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文做出了以下贡献：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce LLM4FL, a novel LLM-based fault localization technique that employs
    a divide-and-conquer strategy. This technique groups large coverage data and ranks
    the covered methods using an SBFL formula. By utilizing multiple agents and prompt
    chaining, LLM4FL navigates the code iteratively to effectively identify and localize
    faults.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了LLM4FL，这是一种新型的基于LLM的故障定位技术，采用了分而治之的策略。该技术通过SBFL公式对大范围的覆盖数据进行分组，并对已覆盖的方法进行排名。通过利用多个代理和提示链条，LLM4FL能够迭代地遍历代码，有效地识别并定位故障。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM4FL demonstrates superior performance, surpassing AutoFL (Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26))
    by 19.27% in Top-1 accuracy. It also outperforms state-of-the-art supervised techniques
    like DeepFL and Grace, achieving these results without requiring task-specific
    training.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM4FL表现出卓越的性能，Top-1准确率超越了AutoFL（Kang等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）19.27%。它还优于当前最先进的监督技术，如DeepFL和Grace，并且在没有任务特定训练的情况下也能达到这些结果。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our analysis of LLM4FL’s components shows that key features like coverage splitting
    and prompt chaining are essential to its fault localization accuracy. Removing
    these features leads to significant performance declines, emphasizing their importance
    in handling token limitations and enabling effective agent collaboration.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对LLM4FL组件的分析表明，覆盖拆分和提示链条等关键特性对于其故障定位准确度至关重要。去除这些特性会导致显著的性能下降，强调了它们在处理令牌限制和促进有效代理协作中的重要性。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We further investigate the effect of method ordering on LLM performance. The
    study reveals that different ordering can enhance fault localization accuracy
    by up to 22% in Top-1 scores. While LLM4FL[DepGraph] achieves the highest overall
    accuracy, LLM4FL[Ochiai] offers a more efficient solution, balancing accuracy
    gains with lower computational overhead, making it practical for broader use.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进一步研究了方法顺序对LLM性能的影响。研究结果表明，不同的顺序可以通过提高Top-1得分最多22%的方式增强故障定位准确性。虽然LLM4FL[DepGraph]在总体准确度上表现最佳，但LLM4FL[Ochiai]提供了一个更高效的解决方案，在提高准确度的同时，减少了计算开销，使其适用于更广泛的应用。
- en: In short, we provide a strategy to mitigate the token limitation issues in LLM-based
    fault localization and highlight the impact of initial method ordering. The findings
    may help inspire future research to improve LLM-based fault localization for large-scale
    software projects.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们提供了一种策略来缓解基于大语言模型（LLM）的故障定位中的令牌限制问题，并强调了初始方法顺序的影响。研究结果可能有助于启发未来的研究，改善大规模软件项目中基于LLM的故障定位。
- en: Paper Organization. Section [2](https://arxiv.org/html/2409.13642v1#S2 "2\.
    Background and Related Work ‣ Enhancing Fault Localization Through Ordered Code
    Analysis with LLM Agents and Self-Reflection") discusses related work. Section
    [3](https://arxiv.org/html/2409.13642v1#S3 "3\. Methodology ‣ Enhancing Fault
    Localization Through Ordered Code Analysis with LLM Agents and Self-Reflection")
    describes our technique, LLM4FL. Section [4](https://arxiv.org/html/2409.13642v1#S4
    "4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through Ordered Code
    Analysis with LLM Agents and Self-Reflection") presents the experimental results.
    Section [5](https://arxiv.org/html/2409.13642v1#S5 "5\. Threats To Validity ‣
    Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents and
    Self-Reflection") discusses the threats to validity. Section [6](https://arxiv.org/html/2409.13642v1#S6
    "6\. Conclusion ‣ Enhancing Fault Localization Through Ordered Code Analysis with
    LLM Agents and Self-Reflection") concludes the paper.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 论文组织结构。第[2](https://arxiv.org/html/2409.13642v1#S2 "2\. Background and Related
    Work ‣ Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents
    and Self-Reflection")节讨论了相关工作。第[3](https://arxiv.org/html/2409.13642v1#S3 "3\.
    Methodology ‣ Enhancing Fault Localization Through Ordered Code Analysis with
    LLM Agents and Self-Reflection")节描述了我们的技术——LLM4FL。第[4](https://arxiv.org/html/2409.13642v1#S4
    "4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through Ordered Code
    Analysis with LLM Agents and Self-Reflection")节展示了实验结果。第[5](https://arxiv.org/html/2409.13642v1#S5
    "5\. Threats To Validity ‣ Enhancing Fault Localization Through Ordered Code Analysis
    with LLM Agents and Self-Reflection")节讨论了有效性威胁。第[6](https://arxiv.org/html/2409.13642v1#S6
    "6\. Conclusion ‣ Enhancing Fault Localization Through Ordered Code Analysis with
    LLM Agents and Self-Reflection")节对论文进行了总结。
- en: 2\. Background and Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与相关工作
- en: 2.1\. Background
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 背景
- en: Large Language Models. Large Language Models (LLMs), primarily built on the
    transformer architecture (Meta AI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib40);
    Brown, [2020](https://arxiv.org/html/2409.13642v1#bib.bib13); Roziere et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib53)), have significantly advanced
    the field of natural language processing (NLP). These LLMs, such as the widely
    recognized GPT3 model with its 175 billion parameters (Brown, [2020](https://arxiv.org/html/2409.13642v1#bib.bib13)),
    are trained on diverse text data from various sources, including source code.
    The training involves self-supervised learning objectives that enable these models
    to develop a deep understanding of language and generate contextually relevant
    and semantically coherent text. LLMs have shown substantial capability in tasks
    that involve complex language comprehension and generation (Abedu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib2);
    Lin et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib37)), such as code
    recognition and generation. Recent research has leveraged LLMs in software engineering
    tasks, particularly in fault localization (Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26);
    Qin et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib49); Yang et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib73)), where they assist in identifying
    the faulty code groups responsible for software errors. One of the key advantages
    of using LLMs in fault localization is their ability to process both natural language
    and code without re-training, allowing them to analyze error messages, stack traces,
    and test case information to infer suspicious methods or code sections in an unsupervised
    zero-shot setting.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。大型语言模型（LLMs），主要基于变换器架构（Meta AI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib40)；Brown，[2020](https://arxiv.org/html/2409.13642v1#bib.bib13)；Roziere
    等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib53)），已显著推动了自然语言处理（NLP）领域的发展。这些LLMs，如广泛认可的GPT3模型，拥有1750亿个参数（Brown，[2020](https://arxiv.org/html/2409.13642v1#bib.bib13)），它们是在来自多种来源的多样化文本数据上进行训练的，包括源代码。训练涉及自监督学习目标，使这些模型能够深刻理解语言，并生成具有上下文相关性和语义一致性的文本。LLMs在涉及复杂语言理解和生成的任务中展现出了显著的能力（Abedu
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib2)；Lin 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib37)），例如代码识别和生成。近期的研究已经在软件工程任务中利用LLMs，特别是在故障定位（Kang
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)；Qin 等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib49)；Yang
    等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib73)），它们帮助识别导致软件错误的故障代码块。使用LLMs进行故障定位的一个关键优势是它们能够在不重新训练的情况下同时处理自然语言和代码，使它们能够分析错误信息、堆栈跟踪和测试用例信息，在无监督零-shot设置下推测出可疑的方法或代码部分。
- en: LLM Agents. LLM agents leverage LLMs to autonomously execute tasks described
    in natural language, making them versatile tools across various domains. LLM agents
    are artificial intelligence systems that utilize LLMs as their core computational
    engines to understand questions and generate human-like responses. They leverage
    functionalities like memory management (Zhou et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib77))
    and tool integration (Xia et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib66);
    Roy et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib52)) to handle multi-step
    and complex operations seamlessly. The agents can refine their responses based
    on feedback, learn from new information, and even interact with other AI agents
    to collaboratively solve complex tasks (Hong et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib21);
    Qian et al., [2023a](https://arxiv.org/html/2409.13642v1#bib.bib46); Xu et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib71); Lin et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib37)).
    Through prompting, agents can be assigned different roles (e.g., a developer or
    a tester), providing more domain-specific responses that help improve the answer (Hong
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib21); White et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib60);
    Shao et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib54)). As their
    potential expands, LLM agents play a crucial role in advancing automation and
    boosting productivity in software development. In this paper, we explore using
    LLM agents to improve fault localization by emulating developers’ debugging process.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 代理。LLM 代理利用 LLM（大语言模型）自主执行用自然语言描述的任务，使其在各个领域都成为多功能的工具。LLM 代理是利用 LLM 作为核心计算引擎的人工智能系统，能够理解问题并生成类人响应。它们利用诸如记忆管理（Zhou
    等人，[2023](https://arxiv.org/html/2409.13642v1#bib.bib77)）和工具集成（Xia 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib66);
    Roy 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib52)）等功能，能够无缝处理多步骤和复杂操作。这些代理可以根据反馈优化其响应，从新信息中学习，甚至与其他
    AI 代理互动，共同解决复杂任务（Hong 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib21);
    Qian 等人，[2023a](https://arxiv.org/html/2409.13642v1#bib.bib46); Xu 等人，[2023](https://arxiv.org/html/2409.13642v1#bib.bib71);
    Lin 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib37)）。通过提示，代理可以被分配不同的角色（例如，开发者或测试者），从而提供更具领域特定的响应，帮助改进答案（Hong
    等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib21); White 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib60);
    Shao 等人，[2023](https://arxiv.org/html/2409.13642v1#bib.bib54)）。随着其潜力的不断扩展，LLM
    代理在推动自动化和提高软件开发生产力方面发挥着至关重要的作用。在本文中，我们探讨了通过模拟开发者的调试过程来利用 LLM 代理改进故障定位。
- en: 2.2\. Related Work
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 相关工作
- en: Spectrum-based Fault Localization. Spectrum-based fault localization (SBFL) (Abreu
    et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3); Jones et al., [2002](https://arxiv.org/html/2409.13642v1#bib.bib24);
    Wong et al., [2013](https://arxiv.org/html/2409.13642v1#bib.bib61); Abreu et al.,
    [2009](https://arxiv.org/html/2409.13642v1#bib.bib4)) employs statistical techniques
    to evaluate the suspiciousness of individual code elements, such as methods, by
    analyzing test outcomes and execution traces. The core idea of SBFL is that code
    components that are executed more frequently in failing tests and less frequently
    in passing tests are more likely to contain faults. Despite its widespread study,
    SBFL’s practical effectiveness remains limited (Kochhar et al., [2016](https://arxiv.org/html/2409.13642v1#bib.bib27);
    Xie et al., [2016](https://arxiv.org/html/2409.13642v1#bib.bib68)). To enhance
    SBFL’s accuracy, recent research (Cui et al., [2020](https://arxiv.org/html/2409.13642v1#bib.bib17);
    Wen et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib59); Chen et al.,
    [2022](https://arxiv.org/html/2409.13642v1#bib.bib15); Xu et al., [2020b](https://arxiv.org/html/2409.13642v1#bib.bib70))
    has suggested incorporating additional data, such as code changes (Wen et al.,
    [2019](https://arxiv.org/html/2409.13642v1#bib.bib59); Chen et al., [2022](https://arxiv.org/html/2409.13642v1#bib.bib15))
    or mutation analysis (Cui et al., [2020](https://arxiv.org/html/2409.13642v1#bib.bib17);
    Xu et al., [2020b](https://arxiv.org/html/2409.13642v1#bib.bib70)). However, SBFL’s
    reliance on code coverage metrics still poses challenges, as its suspiciousness
    scores may not generalize effectively to different faults or systems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于谱的故障定位。基于谱的故障定位（SBFL）（Abreu 等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3);
    Jones 等人，[2002](https://arxiv.org/html/2409.13642v1#bib.bib24); Wong 等人，[2013](https://arxiv.org/html/2409.13642v1#bib.bib61);
    Abreu 等人，[2009](https://arxiv.org/html/2409.13642v1#bib.bib4)）采用统计技术，通过分析测试结果和执行跟踪来评估单个代码元素（如方法）的可疑性。SBFL
    的核心思想是，在失败的测试中执行频率较高，而在通过的测试中执行频率较低的代码组件更可能包含故障。尽管它被广泛研究，SBFL 的实际效果仍然有限（Kochhar
    等人，[2016](https://arxiv.org/html/2409.13642v1#bib.bib27); Xie 等人，[2016](https://arxiv.org/html/2409.13642v1#bib.bib68)）。为了提高
    SBFL 的准确性，最近的研究（Cui 等人，[2020](https://arxiv.org/html/2409.13642v1#bib.bib17);
    Wen 等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib59); Chen 等人，[2022](https://arxiv.org/html/2409.13642v1#bib.bib15);
    Xu 等人，[2020b](https://arxiv.org/html/2409.13642v1#bib.bib70)）建议加入额外的数据，如代码变更（Wen
    等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib59); Chen 等人，[2022](https://arxiv.org/html/2409.13642v1#bib.bib15)）或变异分析（Cui
    等人，[2020](https://arxiv.org/html/2409.13642v1#bib.bib17); Xu 等人，[2020b](https://arxiv.org/html/2409.13642v1#bib.bib70)）。然而，SBFL
    依赖于代码覆盖度指标仍然面临挑战，因为其可疑性得分可能无法有效地推广到不同的故障或系统。
- en: Learning-based fault localization. Recent efforts have focused on improving
    SBFL with learning-based methods (Sohn and Yoo, [2017](https://arxiv.org/html/2409.13642v1#bib.bib55);
    Zhang et al., [2019b](https://arxiv.org/html/2409.13642v1#bib.bib75); Li et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib36); Li and Zhang, [2017](https://arxiv.org/html/2409.13642v1#bib.bib34);
    Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33); Zhang et al.,
    [2019a](https://arxiv.org/html/2409.13642v1#bib.bib76)). These approaches use
    machine learning models like radial basis function networks (Wong et al., [2011](https://arxiv.org/html/2409.13642v1#bib.bib62)),
    back-propagation networks (Wong and Qi, [2009](https://arxiv.org/html/2409.13642v1#bib.bib64)),
    and convolutional neural networks (Zhang et al., [2019a](https://arxiv.org/html/2409.13642v1#bib.bib76);
    Li et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib36); Albawi et al.,
    [2017](https://arxiv.org/html/2409.13642v1#bib.bib7)) to estimate suspiciousness
    scores based on historical faults. Some techniques, such as FLUCCS (Sohn and Yoo,
    [2017](https://arxiv.org/html/2409.13642v1#bib.bib55)), combine SBFL scores with
    metrics like code complexity, while others, like DeepFL (Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33))
    and CombineFL (Zou et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib78)),
    merge multiple sources such as spectrum-based and mutation-based data (Moon et al.,
    [2014](https://arxiv.org/html/2409.13642v1#bib.bib41); Papadakis and Le Traon,
    [2015](https://arxiv.org/html/2409.13642v1#bib.bib43); Dutta and Godboley, [2021](https://arxiv.org/html/2409.13642v1#bib.bib18)).
    Graph neural networks (GNNs) have also been applied to fault localization (Qian
    et al., [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47); Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Qian et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib48); Xu et al.,
    [2020a](https://arxiv.org/html/2409.13642v1#bib.bib69)). Techniques like Grace (Lou
    et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39)) and GNET4FL (Qian
    et al., [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47)) utilize test coverage
    and source code structure for improved accuracy, while DepGraph (Rafi et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)) refines these approaches
    by graph pruning and incorporating code change information, resulting in higher
    performance with reduced computational demands. Although these learning-based
    techniques show improved results, they require training data that may not be available
    to every project, and the training process can be expensive due to model complexity.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 基于学习的故障定位。近期的研究集中在通过基于学习的方法来改进SBFL（Sohn和Yoo，[2017](https://arxiv.org/html/2409.13642v1#bib.bib55)；Zhang等，[2019b](https://arxiv.org/html/2409.13642v1#bib.bib75)；Li等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36)；Li和Zhang，[2017](https://arxiv.org/html/2409.13642v1#bib.bib34)；Li等，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)；Zhang等，[2019a](https://arxiv.org/html/2409.13642v1#bib.bib76)））。这些方法使用机器学习模型，如径向基函数网络（Wong等，[2011](https://arxiv.org/html/2409.13642v1#bib.bib62)），反向传播网络（Wong和Qi，[2009](https://arxiv.org/html/2409.13642v1#bib.bib64)），以及卷积神经网络（Zhang等，[2019a](https://arxiv.org/html/2409.13642v1#bib.bib76)；Li等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36)；Albawi等，[2017](https://arxiv.org/html/2409.13642v1#bib.bib7)），以根据历史故障估计可疑性分数。一些技术，如FLUCCS（Sohn和Yoo，[2017](https://arxiv.org/html/2409.13642v1#bib.bib55)），将SBFL分数与代码复杂度等度量相结合，而其他方法，如DeepFL（Li等，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)）和CombineFL（Zou等，[2019](https://arxiv.org/html/2409.13642v1#bib.bib78)），则结合了来自不同源的数据，如基于谱的和基于变异的测试数据（Moon等，[2014](https://arxiv.org/html/2409.13642v1#bib.bib41)；Papadakis和Le
    Traon，[2015](https://arxiv.org/html/2409.13642v1#bib.bib43)；Dutta和Godboley，[2021](https://arxiv.org/html/2409.13642v1#bib.bib18)）。图神经网络（GNNs）也已应用于故障定位（Qian等，[2023b](https://arxiv.org/html/2409.13642v1#bib.bib47)；Lou等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)；Qian等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib48)；Xu等，[2020a](https://arxiv.org/html/2409.13642v1#bib.bib69)）。像Grace（Lou等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)）和GNET4FL（Qian等，[2023b](https://arxiv.org/html/2409.13642v1#bib.bib47)）这样的技术利用测试覆盖率和源代码结构来提高准确性，而DepGraph（Rafi等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)）则通过图修剪和引入代码变更信息来优化这些方法，从而在降低计算需求的同时提高性能。尽管这些基于学习的技术显示出了改进的结果，但它们需要训练数据，而这些数据可能并非每个项目都能获得，而且由于模型复杂性，训练过程可能非常昂贵。
- en: LLM-Based Fault Localization. Large Language Models (LLMs), such as GPT-4o (OpenAI,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib42)), LLaMA (Meta AI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib40)),
    and ChatGPT (Achiam et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib5)),
    demonstrated remarkable abilities in processing both natural and programming languages.
    LLMs have shown potential in identifying and fixing errors using program code
    and error logs (Achiam et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib5)).
    However, one of the major challenges that LLMs face in fault localization is the
    token limitation issue. LLMs are restricted to a fixed number of tokens, ranging
    from 2,000 to 128,000 (OpenAI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib42);
    Meta AI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib40)), which poses difficulties
    in handling large-scale software projects with long stack traces and extensive
    codebases. This limitation can lead to incomplete analyses, as critical context
    might be truncated or lost, forcing models to work with fragmented information,
    ultimately affecting the overall fault localization performance (Hadi et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib19); Hou et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib22)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的故障定位。大型语言模型（LLMs），如GPT-4o（OpenAI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib42)）、LLaMA（Meta
    AI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib40)）和ChatGPT（Achiam等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib5)），在处理自然语言和编程语言方面展现了出色的能力。LLMs已被证明在使用程序代码和错误日志识别及修复错误方面具有潜力（Achiam等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib5)）。然而，LLMs在故障定位中面临的主要挑战之一是令牌限制问题。LLMs的令牌数量有限，通常范围从2,000到128,000（OpenAI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib42)；Meta
    AI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib40)），这使得它们在处理具有长堆栈跟踪和庞大代码库的大型软件项目时遇到困难。这个限制可能导致分析不完整，因为关键上下文可能会被截断或丢失，迫使模型只能处理零散的信息，最终影响整体的故障定位性能（Hadi等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib19)；Hou等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib22)）。
- en: Moreover, LLM-based fault localization techniques often focus on localizing
    faults within small code snippets due to these context limitations. For example,
    LLMAO (Yang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib73)) uses
    lightweight bidirectional adapters on LLMs to generate suspiciousness scores for
    code lines, but only within a limited context of 128 lines of code. Similarly,
    Wu et al. (Wu et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib65)) prompt
    ChatGPT with code and error logs to identify faulty lines, but these methods struggle
    to scale to larger software projects (Liu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib38)).
    AutoFL (Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)) is
    an LLM-based fault localization technique that detects faulty code locations and
    generates natural language explanations for bugs. However, it also faces difficulty
    handling large-scale projects due to LLM context length constraints and reduced
    performance on complex bugs requiring deeper repository exploration. To address
    these challenges, there is a growing need for approaches that enable LLMs to localize
    faults across entire software projects, ensuring they can handle larger inputs
    while maintaining accuracy and effectiveness. Thus, we propose LLM4FL, that leverages
    a divide-and-conquer approach to navigate through codebases and identify faults
    through multiple-agent collaboration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于这些上下文限制，基于LLM的故障定位技术通常侧重于在小的代码片段中定位故障。例如，LLMAO（Yang等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib73)）在LLM上使用轻量级双向适配器生成代码行的可疑评分，但仅限于128行代码的有限上下文内。类似地，Wu等（Wu等，[2023](https://arxiv.org/html/2409.13642v1#bib.bib65)）通过代码和错误日志提示ChatGPT来识别故障行，但这些方法在扩展到更大的软件项目时遇到困难（Liu等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib38)）。AutoFL（Kang等，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）是一种基于LLM的故障定位技术，能够检测故障代码位置并生成自然语言解释。然而，它也面临着由于LLM上下文长度限制以及在需要更深入仓库探索的复杂错误上的表现下降而难以处理大型项目的问题。为了应对这些挑战，越来越需要能够让LLMs跨越整个软件项目进行故障定位的方法，确保它们在处理更大输入的同时保持准确性和有效性。因此，我们提出了LLM4FL，它通过分而治之的方法，在多个代理协作下导航代码库并识别故障。
- en: 3\. Methodology
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法论
- en: '![Refer to caption](img/f375e9fd14770a35c195df7f4c4abe72.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f375e9fd14770a35c195df7f4c4abe72.png)'
- en: Figure 1\. An overview of LLM4FL.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图1\. LLM4FL概述。
- en: 'Applying Large Language Models (LLMs) for fault localization presents specific
    challenges, such as token size limitations and performance issues with longer
    input contexts (Levy et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib31);
    Liu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib38); Wu et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib65)). Processing large datasets
    like test coverage and source code in a single query is impractical due to these
    constraints. Moreover, there are limited studies on effective prompting techniques
    for fault localization. To address these issues, we propose LLM4FL, an LLM-agent-based
    fault localization system. Figure [1](https://arxiv.org/html/2409.13642v1#S3.F1
    "Figure 1 ‣ 3\. Methodology ‣ Enhancing Fault Localization Through Ordered Code
    Analysis with LLM Agents and Self-Reflection") illustrates an overview of our
    technique. LLM4FL operates in four stages: 1) Analyzing the Covered Methods Using
    a Divide-and-Conquer Strategy, 2) Defining LLM Agent Roles and Tools, 3) Code
    Navigation through Agent Collaboration and Prompt Chaining, and 4) Self-Reflection
    and Chain-of-Thought Reasoning. First, the coverage data is divided into smaller,
    manageable groups to fit within the LLM’s token limits. Second, LLM agents are
    assigned specific roles and equipped with tools to analyze these groups. Third,
    LLM agents collaborate through prompt chaining to navigate the code autonomously
    and identify potential faults. Finally, the agents refine their analysis using
    self-reflection and chain-of-thought reasoning. Below, we provide more details
    on each of these stages.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将大型语言模型（LLMs）应用于故障定位面临特定挑战，如令牌大小限制和处理更长输入上下文时的性能问题（Levy et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib31);
    Liu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib38); Wu et al.,
    [2023](https://arxiv.org/html/2409.13642v1#bib.bib65)）。由于这些限制，处理像测试覆盖和源代码这样的大型数据集在单个查询中是不现实的。此外，关于故障定位的有效提示技术的研究也有限。为了解决这些问题，我们提出了LLM4FL，一个基于LLM代理的故障定位系统。图[1](https://arxiv.org/html/2409.13642v1#S3.F1
    "Figure 1 ‣ 3\. Methodology ‣ Enhancing Fault Localization Through Ordered Code
    Analysis with LLM Agents and Self-Reflection")展示了我们技术的概述。LLM4FL在四个阶段中运行：1）使用分治策略分析已覆盖的方法，2）定义LLM代理角色和工具，3）通过代理协作和提示链进行代码导航，4）自我反思和连锁思维推理。首先，覆盖数据被划分为更小的、可管理的组，以适应LLM的令牌限制。其次，LLM代理被分配特定的角色，并配备工具来分析这些组。第三，LLM代理通过提示链协作，自主导航代码并识别潜在故障。最后，代理使用自我反思和连锁思维推理来完善其分析。接下来，我们将提供关于每个阶段的更多细节。
- en: 3.1\. Analyzing the Covered Methods Using Divide-and-Conquer
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 使用分治法分析已覆盖的方法
- en: Prior studies in fault localization utilized dynamic execution data, such as
    coverage information from both failing and passing test cases, to identify faulty
    code locations (Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33); Abreu et al.,
    [2009](https://arxiv.org/html/2409.13642v1#bib.bib4); Rafi et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib50);
    Qian et al., [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47)). However,
    Large Language Models (LLMs) face challenges when processing large-scale coverage
    data due to inherent token size limitations (Kang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)).
    These limitations lead to performance degradation, extended processing times,
    and reduced accuracy when input tokens are truncated. To address these constraints,
    we propose a divide-and-conquer strategy, dividing the coverage data (i.e., containing
    the covered methods and the corresponding code) into manageable groups that the
    LLM can process within its token limits.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的故障定位研究利用动态执行数据，如来自失败和通过测试用例的覆盖信息，来识别故障代码位置（Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33); Abreu et al.,
    [2009](https://arxiv.org/html/2409.13642v1#bib.bib4); Rafi et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib50);
    Qian et al., [2023b](https://arxiv.org/html/2409.13642v1#bib.bib47))。然而，大型语言模型（LLMs）在处理大规模覆盖数据时面临挑战，因为存在固有的令牌大小限制（Kang
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）。这些限制导致性能下降、处理时间延长，并且当输入令牌被截断时，准确性会降低。为了解决这些限制，我们提出了一种分治策略，将覆盖数据（即包含已覆盖的方法及相应代码）划分为LLM可以在其令牌限制内处理的可管理组。
- en: 'We generate a list of covered methods using tools like GZoltar (Campos et al.,
    [2012](https://arxiv.org/html/2409.13642v1#bib.bib14)), focusing only on methods
    executed by failing tests. To manage the token limitation issue, we include only
    the specific method statements that were executed by the failing tests, excluding
    any uncovered portions of the code. Before doing a divide-and-conquer, we use
    a Spectrum-Based Fault Localization (SBFL) technique to sort the covered methods
    so that those more likely to be faulty can be grouped together. This approach
    is inspired by a recent study (Tyen et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib56))
    that demonstrates LLMs perform better when the order of instruction is carefully
    considered. Note that, regardless of the order, LLM4FL eventually analyzes every
    covered method. Specifically, we use Ochiai (Abreu et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3))
    to sort the methods before the divide-and-conquer process. Ochiai is a widely
    used unsupervised spectrum-based fault localization technique because of its high
    efficiency and good fault localization accuracy (Abreu et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3);
    Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39); Li et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib36); Cui et al., [2020](https://arxiv.org/html/2409.13642v1#bib.bib17);
    Wen et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib59); Qian et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib48)). Intuitively, Ochiai assigns
    a higher suspiciousness score to a statement that is executed more frequently
    by failing cases and less frequently by passing test cases. Ochiai is defined
    as:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GZoltar 等工具（Campos 等人，[2012](https://arxiv.org/html/2409.13642v1#bib.bib14)）生成已覆盖方法的列表，仅关注由失败测试执行的方法。为了解决令牌限制问题，我们仅包括由失败测试执行的特定方法语句，排除任何未覆盖的代码部分。在进行分治之前，我们使用基于谱的故障定位（SBFL）技术对已覆盖的方法进行排序，以便那些更可能出现故障的方法能够被归为一类。这种方法受到最近一项研究的启发（Tyen
    等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib56)），该研究表明当指令顺序被仔细考虑时，LLMs
    的表现更好。需要注意的是，不论顺序如何，LLM4FL 最终会分析每个已覆盖的方法。具体来说，我们在分治过程之前使用 Ochiai（Abreu 等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3)）对方法进行排序。Ochiai
    是一种广泛使用的无监督谱基故障定位技术，因为它具有高效性和良好的故障定位准确性（Abreu 等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3);
    Lou 等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39); Li 等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36);
    Cui 等人，[2020](https://arxiv.org/html/2409.13642v1#bib.bib17); Wen 等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib59);
    Qian 等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib48))。直观地说，Ochiai 会对那些在失败测试中执行频率较高，而在通过测试中执行频率较低的语句赋予更高的可疑性得分。Ochiai
    的定义为：
- en: '|  | $Ochiai(a_{ef},a_{nf},a_{ep})=\frac{a_{ef}}{\sqrt{(a_{ef}+a_{nf})\times(a_{ef}+%
    a_{ep})}},$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $Ochiai(a_{ef},a_{nf},a_{ep})=\frac{a_{ef}}{\sqrt{(a_{ef}+a_{nf})\times(a_{ef}+a_{ep})}},$
    |  |'
- en: where $a_{ef}$ is the number of failed test cases that execute a statement,
    $a_{nf}$ is the number of failed test cases that do not execute the statement,
    and $a_{ep}$ is the number of passed test cases that execute the statement. The
    result of Ochiai is a value between 0 and 1, with higher values indicating a higher
    likelihood that a particular statement is faulty. We then aggregate the scores
    for every statement in a method to get the method-level score.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $a_{ef}$ 是执行某语句的失败测试用例数，$a_{nf}$ 是未执行该语句的失败测试用例数，$a_{ep}$ 是执行该语句的通过测试用例数。Ochiai
    结果的值介于 0 和 1 之间，值越高表示某个语句出现故障的可能性越大。然后，我们通过一种方法对每个语句的得分进行汇总，以获得方法级别的得分。
- en: Once the covered methods are sorted, we divide the methods into smaller groups,
    ensuring that each group fits within the token limitation of the LLM. More formally,
    let $C$ represent the sorted covered methods, containing a sequence of pairs ($m$,
    $s$), where $m$ denotes a covered source method and $s$ denotes the corresponding
    code statements. The group length is determined using the token limitation specified
    in the official documentation of GPT-4o-mini (the LLM that we use in this study),
    which has an input context window of 128,000 tokens (OpenAI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib42)).
    The length is calculated as $TotalTokenLength/TokenLimitation$, ensuring each
    group remains within the model’s token limitation. Specifically, we divide $C$
    into to sequence of $C_{1},C_{2},\ldots,C_{n}$ each $C_{i}$ contains the subset
    of pairs ($m_{i}$, $s_{i}$), and satisfies the constraint —$C_{i}$— $<=TokenLimitation$.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦覆盖方法被排序，我们将这些方法分成更小的组，确保每组适应LLM的令牌限制。更正式地说，设$C$为已排序的覆盖方法，包含一系列对（$m$，$s$），其中$m$表示一个已覆盖的源方法，$s$表示相应的代码语句。组长度是根据GPT-4o-mini（我们在本研究中使用的LLM）的官方文档中指定的令牌限制来确定的，它的输入上下文窗口为128,000个令牌（OpenAI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib42)）。长度的计算方式为$TotalTokenLength/TokenLimitation$，确保每个组的大小保持在模型的令牌限制内。具体来说，我们将$C$划分为一系列$C_{1},
    C_{2}, \ldots, C_{n}$，其中每个$C_{i}$包含对（$m_{i}$，$s_{i}$）的子集，并满足约束条件—$C_{i}$—$<=TokenLimitation$。
- en: 3.2\. LLM Agents and the Available Tools
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. LLM代理和可用工具
- en: 'LLM4FL emulates developers’ debugging process by implementing two LLM agents:
    1) Tester and 2) Debugger. Below we discuss the details of these agents:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL通过实现两个LLM代理来模拟开发人员的调试过程：1）测试代理和2）调试代理。以下我们将讨论这些代理的详细信息：
- en: Tester Agent. The tester agent’s main goal is to identify and prioritize suspicious
    methods. More formally, it navigates through each group $C_{i}$ of the covered
    methods. The Tester Agent’s input includes the sorted covered methods in each
    group $C_{i}$ and the corresponding test code, stack traces, and related test
    information from failing executions. To further reduce the input token size and
    allow LLM to focus on the relevant code, we only provide the portion of the test
    code leading to the failure as input. We analyze the stack trace to identify where
    the test failed and extract the code up to that point, omitting the rest.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 测试代理。测试代理的主要目标是识别和优先处理可疑的方法。更正式地说，它通过每个方法组$C_{i}$进行导航。测试代理的输入包括每个方法组$C_{i}$中已排序的覆盖方法以及来自失败执行的相应测试代码、堆栈跟踪和相关测试信息。为了进一步减少输入令牌的大小，并使LLM能够专注于相关代码，我们仅提供导致失败的测试代码部分作为输入。我们分析堆栈跟踪，识别测试失败的位置，并提取到该点的代码，忽略其余部分。
- en: Figure [2](https://arxiv.org/html/2409.13642v1#S3.F2 "Figure 2 ‣ 3.2\. LLM Agents
    and the Available Tools ‣ 3\. Methodology ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") gives an example of
    the Tester Agent’s prompt. The Tester Agent is responsible for identifying suspicious
    methods likely related to faults in the system. To achieve this, it needs to 1)
    analyze methods executed by failing tests, 2) review relevant test code, and 3)
    examine stack trace data to pinpoint where failures occurred. To facilitate this
    process, we provide the Tester Agent with three tools. The tool get_covered_methods
    allows the agent to retrieve the source code of the methods executed during failing
    tests, get_test_code fetches the relevant test code, and get_stacktrace gathers
    stack trace data. Each tool has a unique name and description, which the agent
    uses to determine when to apply the tool during its analysis. The agent decides
    autonomously, based on its task, which tool to invoke by interpreting the tool’s
    description and relevance to the current stage of fault localization. Additionally,
    each method within the grouped data is assigned a unique ID during the dividing
    process, allowing the agent to reference and retrieve specific methods. After
    analyzing all groups, the Tester Agent generates a list of suspicious methods,
    denoted as $S_{i}\subseteq M_{i}$, where $M_{i}$ represents the methods in group
    $C_{i}$. This list of suspicious methods is then consolidated into a final list
    of method IDs for further analysis.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](https://arxiv.org/html/2409.13642v1#S3.F2 "图 2 ‣ 3.2\. LLM 代理和可用工具 ‣ 3\.
    方法论 ‣ 通过有序代码分析与 LLM 代理和自我反思提升故障定位") 给出了测试代理的提示示例。测试代理负责识别可能与系统故障相关的可疑方法。为此，它需要
    1) 分析失败测试中执行的方法，2) 审查相关的测试代码，以及 3) 检查堆栈跟踪数据以确定故障发生的位置。为了便于这一过程，我们为测试代理提供了三种工具。工具
    get_covered_methods 允许代理检索在失败测试中执行的源代码，get_test_code 提取相关的测试代码，get_stacktrace
    收集堆栈跟踪数据。每个工具都有一个独特的名称和描述，代理利用这些信息来确定在分析过程中何时应用该工具。代理根据任务的不同，自动决定根据工具的描述及其与当前故障定位阶段的相关性来选择使用哪个工具。此外，在分组过程中，每个方法都被分配了一个唯一的
    ID，代理可以在分析时引用并检索特定的方法。分析完所有分组后，测试代理生成一个可疑方法列表，表示为 $S_{i}\subseteq M_{i}$，其中 $M_{i}$
    代表组 $C_{i}$ 中的方法。然后，将这些可疑方法的列表整合成一个最终的 ID 列表，以便进一步分析。
- en: '<svg class="ltx_picture ltx_centering" height="194.99" id="S3.F2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,194.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 176.78)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Prompt Template for Initial
    Candidate Selection with Advanced Tools</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="145.29" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">As
    an Intelligent Tester Agent, utilize the following tools to identify methods most
    likely related to the fault. Focus on analyzing the test code, stack trace, and
    covered methods to pinpoint suspicious or fault-inducing methods. Ensure the exact
    method_id is included in the final JSON output. Available Tools:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="194.99" id="S3.F2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,194.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 176.78)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">用于初步候选方法选择的提示模板，配有高级工具</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="145.29" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">作为智能测试代理，使用以下工具来识别最可能与故障相关的方法。重点分析测试代码、堆栈跟踪和已覆盖的方法，以确定可疑或引发故障的方法。确保最终的
    JSON 输出中包含准确的 method_id。可用工具：
- en: 'get_test_code() – Retrieves the complete test code. get_stacktrace() – Access
    the stack trace of the failed test. get_covered_methods() – Lists all methods
    executed during the test. Your Task: Use the insights from the tools and provide
    the potential suspicious methods in the following JSON format:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: get_test_code() – 检索完整的测试代码。get_stacktrace() – 访问失败测试的堆栈跟踪。get_covered_methods()
    – 列出测试过程中执行的所有方法。你的任务：利用这些工具提供的见解，并以以下 JSON 格式提供潜在的可疑方法：
- en: '[PRE0]</foreignobject></g></g></svg>'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE0]</foreignobject></g></g></svg>'
- en: Figure 2\. Enhanced Prompt for Initial Candidate Selection.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 初步候选方法选择的增强提示。
- en: Debugger Agent. Figure [3](https://arxiv.org/html/2409.13642v1#S3.F3 "Figure
    3 ‣ 3.2\. LLM Agents and the Available Tools ‣ 3\. Methodology ‣ Enhancing Fault
    Localization Through Ordered Code Analysis with LLM Agents and Self-Reflection")
    gives an example of the Debugger Agent’s prompt. The Debugger Agent is responsible
    for thoroughly evaluating and ranking a given list of candidate methods $S$. To
    perform its analysis, the Debugger Agent relies on three tools designed to extract
    relevant information. get_test_code and get_stacktrace provide LLM’s requested
    information by retrieving the relevant test code and stack trace data, helping
    the Debugger Agent understand the failure scenarios associated with each method.
    Additionally, the get_method_body tool allows the agent to fetch the full source
    code of a method using its unique method ID, enabling the Debugger Agent to analyze
    each method’s source code.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调试代理。图 [3](https://arxiv.org/html/2409.13642v1#S3.F3 "图 3 ‣ 3.2\. LLM 代理和可用工具
    ‣ 3\. 方法论 ‣ 通过有序代码分析和自我反思增强故障定位") 给出了调试代理的提示示例。调试代理负责全面评估并排序给定的候选方法列表 $S$。为了执行分析，调试代理依赖于三种工具，这些工具旨在提取相关信息。get_test_code
    和 get_stacktrace 通过检索相关的测试代码和堆栈跟踪数据，提供LLM所请求的信息，帮助调试代理理解与每个方法相关的故障场景。此外，get_method_body
    工具允许代理使用其唯一的方法ID获取方法的完整源代码，从而使调试代理能够分析每个方法的源代码。
- en: 'Once the Debugger Agent completes its analysis, it assigns a rank score $\sigma_{j}$
    to each method $m_{j}$, representing the likelihood that the method is contributing
    to the fault. The candidate methods in $S$ are then ranked based on these scores,
    producing a ranked list $R$ that orders the methods from most to least suspicious:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦调试代理完成分析，它会为每个方法 $m_{j}$ 分配一个排名分数 $\sigma_{j}$，表示该方法可能对故障的贡献程度。然后，根据这些分数对候选方法
    $S$ 进行排序，生成一个按可疑程度排序的列表 $R$：
- en: '|  | $R=\text{sort}(\{(m_{j},\sigma_{j}):m_{j}\in S\})$ |  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $R=\text{sort}(\{(m_{j},\sigma_{j}):m_{j}\in S\})$ |  |'
- en: This final ranked list $R$ is output in JSON format for further evaluation and
    parsing.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终排序的列表 $R$ 以 JSON 格式输出，供进一步评估和解析。
- en: '<svg class="ltx_picture ltx_centering" height="244.8" id="S3.F3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,244.8) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 226.6)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Prompt Template for Code Understanding
    and Fault Analysis</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="195.1" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">As an Advanced Debugger Agent,
    use the following tools to analyze the provided method bodies based on the Tester
    Agent’s insights from the test and stack traces. The fault may propagate across
    multiple methods or be indirectly related to the original issue. Available Tools:
    get_test_code()       – Retrieve the entire test code for detailed analysis.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture ltx_centering" height="244.8" id="S3.F3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,244.8) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 226.6)"><foreignobject color="#000000" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">代码理解与故障分析的提示模板</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="195.1" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">作为高级调试代理，请使用以下工具，根据测试代理从测试和堆栈跟踪中获得的洞察分析提供的方法体。故障可能会跨多个方法传播，或者间接与原始问题相关。可用工具：get_test_code()
          – 获取完整的测试代码以进行详细分析。
- en: get_stacktrace()         – Access the stack trace to trace method calls and
    identify potential faults.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: get_stacktrace()         – 访问堆栈跟踪以追踪方法调用并识别潜在的故障。
- en: 'get_method_body()      – Retrieve the body of specific methods for further
    inspection. Task: Using the provided tools, analyze and focus on the following
    Methods: {suspicious methods} Rank these methods from most to least suspicious,
    and provide brief reasoning for each based on its behavior and potential involvement
    in the fault. IMPORTANT: Provide your output in the following JSON format:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: get_method_body()      – 获取特定方法的代码体以供进一步检查。任务：使用提供的工具，分析并重点关注以下方法：{可疑方法} 按照从最可疑到最不疑的方法对这些方法进行排名，并根据每个方法的行为和可能涉及的故障提供简要的理由。重要提示：请按照以下
    JSON 格式提供您的输出：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Figure 3\. Enhanced Prompt for Code Understanding and Fault Analysis.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 增强的代码理解与故障分析提示。
- en: 3.3\. Code Navigation Through Prompt Chaining
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 通过提示链进行代码导航
- en: Code navigation is a critical part of how developers trace faults in real-world
    scenarios. When debugging, developers often start by analyzing a specific method
    and then explore its caller or callee methods to better understand the overall
    logic and pinpoint where the fault might lie. Our approach emulates this process
    by implementing code navigation through prompt chaining, allowing the Tester and
    Debugger Agents to collaborate and progressively focus their analysis on the most
    relevant parts of the codebase.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代码导航是开发者在真实场景中追踪故障的关键部分。在调试时，开发者通常从分析一个特定方法开始，然后探索其调用方法或被调用方法，以更好地理解整体逻辑并找出故障可能所在的地方。我们的方法通过实现基于提示链的代码导航来模拟这一过程，使测试员和调试员代理可以协作，逐步将分析重点集中在代码库中最相关的部分。
- en: 'The process begins with the Tester Agent, which works through each group of
    the coverage data $C=\{C_{1},C_{2},\ldots,C_{n}\}$, where each group $C_{i}$ contains
    a subset of the covered methods $M_{i}$. The Tester Agent identifies suspicious
    methods within each group, producing a list $S_{i}\subseteq M_{i}$ of candidate
    methods. This step mimics how a developer might flag areas of interest in the
    code that seem related to the fault. After analyzing all the groups, the Tester
    Agent compiles the suspicious methods into a consolidated list:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从测试员代理开始，测试员代理逐一分析覆盖数据$C=\{C_{1},C_{2},\ldots,C_{n}\}$中的每一组，每组$C_{i}$包含了一部分被覆盖的方法$M_{i}$。测试员代理在每个组中识别出可疑的方法，生成一个候选方法列表$S_{i}\subseteq
    M_{i}$。这个步骤模拟了开发者标记代码中可能与故障相关的感兴趣区域。分析完所有组后，测试员代理将可疑方法汇总成一个整合后的列表：
- en: '|  | $S=\bigcup_{i=1}^{n}S_{i}$ |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $S=\bigcup_{i=1}^{n}S_{i}$ |  |'
- en: This list $S$ is passed to the Debugger Agent, initiating the prompt chaining
    process.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表$S$被传递给调试员代理，启动提示链的过程。
- en: The Debugger Agent takes the output from the Tester Agent and dives deeper into
    the suspicious methods. In this stage, the Debugger Agent navigates through each
    method in $S$, inspecting not only the method itself but also the related methods,
    such as caller and callee methods, if the agent believes further analysis can
    help identify faulty code location. The Debugger Agent emulates the manual process
    of tracing how faults propagate through different parts of the code. By leveraging
    its tools, the Debugger Agent retrieves information on how each method interacts
    with others and how it may contribute to the fault. If the Debugger Agent identifies
    that a method $m_{j+1}$ is called within $m_{j}$ and might require further examination,
    it can request the code for $m_{j+1}$ if it is part of the candidate list, ensuring
    deeper analysis where necessary. LLM4FL implements an LLM-based fault navigation
    system that allows the Debugger Agent to dynamically traverse the codebase and
    retrieve additional relevant methods for further inspection. Through this process,
    the Debugger Agent traces potential fault propagation pathways and evaluates how
    faults spread across methods.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 调试员代理获取来自测试员代理的输出，深入分析可疑方法。在此阶段，调试员代理遍历$S$中的每个方法，不仅检查该方法本身，还检查相关的方法，例如调用方法和被调用方法，前提是代理认为进一步分析可以帮助识别故障代码的位置。调试员代理模拟了手动追踪故障在代码中不同部分传播的过程。通过利用其工具，调试员代理获取每个方法如何与其他方法交互的信息，以及它可能如何对故障产生影响。如果调试员代理发现方法$m_{j+1}$在$m_{j}$中被调用，并且可能需要进一步检查，它可以请求$m_{j+1}$的代码（如果它在候选列表中），确保在必要时进行更深入的分析。LLM4FL实现了一个基于LLM的故障导航系统，使调试员代理能够动态地遍历代码库，并检索额外相关的方法进行进一步检查。通过这一过程，调试员代理追踪潜在的故障传播路径，并评估故障如何在方法之间传播。
- en: 3.4\. Self-Reflection and Chain-of-Thought for Improved Fault Localization Results
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 自我反思和思维链提升故障定位结果
- en: LLM4FL uses two additional prompting techniques to further improve the fault
    localization results.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL使用两种额外的提示技术来进一步改善故障定位结果。
- en: 'Self-Reflection. In our approach, both the Tester and Debugger Agents engage
    in self-reflection to enhance the quality of their outputs. After completing their
    initial analysis, each agent operates in two phases: a generator phase and a reflector
    phase. During the generator phase, the agents produce their primary results, such
    as identifying suspicious methods or ranking them by suspiciousness. In the reflector
    phase, the agents critique their own work, offering feedback to refine their results
    and correct potential errors. While it has not yet been studied in the fault localization
    context, a recent study (Renze and Guven, [2024](https://arxiv.org/html/2409.13642v1#bib.bib51))
    found that self-reflection improves LLM’s performance in problem-solving tasks.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 自我反思。在我们的方法中，测试者（Tester）和调试者（Debugger）代理都参与自我反思，以提高输出结果的质量。在完成初步分析后，每个代理会进入两个阶段：生成阶段和反思阶段。在生成阶段，代理会生成其主要结果，如识别可疑方法或按可疑性对其进行排序。在反思阶段，代理会批评自己工作的不足，提供反馈以改进结果并纠正潜在的错误。尽管这一方法在故障定位背景下尚未被研究，但最近的研究（Renze
    和 Guven, [2024](https://arxiv.org/html/2409.13642v1#bib.bib51)）发现，自我反思能够提升LLM在问题解决任务中的表现。
- en: Chain-of-Thought for Refining Fault Localization. We incorporate a chain-of-thought
    reasoning process to enhance fault localization. After generating a ranked list
    of suspicious methods, the LLMs propose potential fixes for the top-ranked methods.
    The agents then re-rank the methods by considering the generated fixes. This process
    allows the agents to think more deeply about the faults and reassess the ranking
    based on the insights gained from considering these fixes (Bao et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib9);
    Wang et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib58)).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 用于完善故障定位的思维链。我们引入了思维链推理过程来增强故障定位能力。在生成了可疑方法的排序列表后，LLMs会为排名最高的方法提出潜在的修复建议。然后，代理会根据生成的修复建议重新排序这些方法。这个过程使得代理能更深入地思考故障，并基于考虑这些修复建议所获得的见解重新评估排名（Bao
    等, [2024](https://arxiv.org/html/2409.13642v1#bib.bib9); Wang 等, [2024](https://arxiv.org/html/2409.13642v1#bib.bib58)）。
- en: 4\. STUDY DESIGN AND RESULTS
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 研究设计与结果
- en: In this section, we first describe the study design and setup. Then, we present
    the motivation, approach, and results of the research questions.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先描述研究设计和设置。接着，我们展示研究问题的动机、方法和结果。
- en: 'Table 1\. An overview of our studied projects from Defects4J v2.0.0\. #Faults,
    LOC, and #Tests show the number of faults, lines of code, and tests in each system.
    Fault-triggering Tests shows the number of failing tests that trigger the fault.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '表1\. 我们研究的项目概览，数据来源于Defects4J v2.0.0\. #Faults、LOC和#Tests分别表示每个系统中的故障数、代码行数和测试用例数。Fault-triggering
    Tests表示触发故障的失败测试数量。'
- en: '|          Project |          #Faults |          LOC |          #Tests |          Fault-triggering
    Tests |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|          项目 |          #Faults |          LOC |          #Tests |          故障触发测试
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|          Cli |          39 |          4K |          94 |          66 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|          Cli |          39 |          4K |          94 |          66 |'
- en: '|          Closure |          174 |          90K |          7,911 |          545
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|          Closure |          174 |          90K |          7,911 |          545
    |'
- en: '|          Codec |          18 |          7K |          206 |          43 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|          Codec |          18 |          7K |          206 |          43 |'
- en: '|          Collections |          4 |          65K |          1,286 |          4
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|          Collections |          4 |          65K |          1,286 |          4
    |'
- en: '|          Compress |          47 |          9K |          73 |          72
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|          Compress |          47 |          9K |          73 |          72
    |'
- en: '|          Csv |          16 |          2K |          54 |          24 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|          Csv |          16 |          2K |          54 |          24 |'
- en: '|          Gson |          18 |          14K |          720 |          34 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|          Gson |          18 |          14K |          720 |          34 |'
- en: '|          JacksonCore |          26 |          22K |          206 |          53
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|          JacksonCore |          26 |          22K |          206 |          53
    |'
- en: '|          JacksonXml |          6 |          9K |          138 |          12
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|          JacksonXml |          6 |          9K |          138 |          12
    |'
- en: '|          Jsoup |          93 |          8K |          139 |          144
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|          Jsoup |          93 |          8K |          139 |          144
    |'
- en: '|          Lang |          64 |          22K |          2,291 |          121
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|          Lang |          64 |          22K |          2,291 |          121
    |'
- en: '|          Math |          106 |          85K |          4,378 |          176
    |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|          Math |          106 |          85K |          4,378 |          176
    |'
- en: '|          Mockito |          38 |          11K |          1,379 |          118
    |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|          Mockito |          38 |          11K |          1,379 |          118
    |'
- en: '|          Time |          26 |          28K |          4,041 |          74
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|          时间 |          26 |          28K |          4,041 |          74 |'
- en: '|          Total |          675 |          380K |          24,302 |          1,486
    |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|          总计 |          675 |          380K |          24,302 |          1,486
    |'
- en: Benchmark Dataset. To answer the RQs, we conducted the experiment on 675 faults
    across 14 projects from the Defects4J benchmark (V2.0.0) (Just et al., [2014](https://arxiv.org/html/2409.13642v1#bib.bib25)).
    Defects4J provides a controlled environment to reproduce faults collected from
    projects of various types and sizes. Defects4J is widely used in prior automated
    fault localization research (Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Sohn and Yoo, [2017](https://arxiv.org/html/2409.13642v1#bib.bib55); Chen et al.,
    [2022](https://arxiv.org/html/2409.13642v1#bib.bib15); Zhang et al., [2017](https://arxiv.org/html/2409.13642v1#bib.bib74)).
    We excluded three projects, JacksonDatabind, JxPath, and Chart, from Defects4J
    in our study because we encountered many execution errors and were not able to
    collect test coverage information for them. Table [1](https://arxiv.org/html/2409.13642v1#S4.T1
    "Table 1 ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") gives detailed information
    on the projects and faults we use in our study. In total, the faults have over
    1.4K fault-triggering tests (i.e., failing tests that cover the fault). The sizes
    of the studied projects range from 2K to 90K lines of code. Note that since a
    fault may have multiple fault-triggering tests, there are more fault-triggering
    tests than faults.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基准数据集。为了回答研究问题，我们在Defects4J基准（V2.0.0）（Just等，[2014](https://arxiv.org/html/2409.13642v1#bib.bib25)）中的14个项目、675个故障上进行了实验。Defects4J提供了一个受控环境，可以重现从各种类型和规模的项目中收集到的故障。Defects4J在以往的自动化故障定位研究中得到了广泛应用（Lou等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)；Sohn和Yoo，[2017](https://arxiv.org/html/2409.13642v1#bib.bib55)；Chen等，[2022](https://arxiv.org/html/2409.13642v1#bib.bib15)；Zhang等，[2017](https://arxiv.org/html/2409.13642v1#bib.bib74)）。在我们的研究中，我们排除了Defects4J中的三个项目：JacksonDatabind、JxPath和Chart，因为我们遇到了许多执行错误，无法为这些项目收集测试覆盖信息。表[1](https://arxiv.org/html/2409.13642v1#S4.T1
    "Table 1 ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection")提供了我们研究中使用的项目和故障的详细信息。总的来说，这些故障有超过1.4K个故障触发测试（即覆盖故障的失败测试）。研究项目的规模从2K到90K行代码不等。请注意，由于一个故障可能有多个故障触发测试，因此故障触发测试的数量超过故障的数量。
- en: 'Evaluation Metrics. According to prior findings, debugging faults at the class
    level lacks precision for effective location (Kochhar et al., [2016](https://arxiv.org/html/2409.13642v1#bib.bib27)).
    Alternatively, pinpointing them at the statement level might be overly detailed,
    omitting important context (Parnin and Orso, [2011](https://arxiv.org/html/2409.13642v1#bib.bib44)).
    Hence, in keeping with prior work (Benton et al., [2020](https://arxiv.org/html/2409.13642v1#bib.bib10);
    B. Le et al., [2016](https://arxiv.org/html/2409.13642v1#bib.bib8); Li et al.,
    [2019](https://arxiv.org/html/2409.13642v1#bib.bib33); Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Vancsics et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib57)), we perform
    our fault localization process at the method level. We apply the following commonly-used
    metrics for evaluation:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。根据先前的研究发现，在类级别调试故障缺乏有效定位的精度（Kochhar等，[2016](https://arxiv.org/html/2409.13642v1#bib.bib27)）。另一方面，在语句级别定位故障可能过于详细，忽略了重要的上下文（Parnin和Orso，[2011](https://arxiv.org/html/2409.13642v1#bib.bib44)）。因此，沿袭先前的研究工作（Benton等，[2020](https://arxiv.org/html/2409.13642v1#bib.bib10)；B.
    Le等，[2016](https://arxiv.org/html/2409.13642v1#bib.bib8)；Li等，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)；Lou等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)；Vancsics等，[2021](https://arxiv.org/html/2409.13642v1#bib.bib57)），我们在方法级别执行故障定位过程。我们使用以下常见的评估指标：
- en: Recall at Top-N. The Top-N metric measures the number of faults with at least
    one faulty program element (in this paper, methods) ranked in the top N. The result
    from LLM4FL is a ranked list based on the suspiciousness score. Prior research
    (Parnin and Orso, [2011](https://arxiv.org/html/2409.13642v1#bib.bib44)) indicates
    that developers typically only scrutinize a limited number of top-ranked faulty
    elements. Therefore, our study focuses on Top-N, where N is set to 1, 3, 5, and
    10.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Top-N召回率。Top-N指标衡量至少有一个故障程序元素（在本文中为方法）排在前N名的故障数量。LLM4FL的结果是基于可疑性得分的排序列表。先前的研究（Parnin和Orso，[2011](https://arxiv.org/html/2409.13642v1#bib.bib44)）表明，开发人员通常只会仔细检查少数排名靠前的故障元素。因此，我们的研究重点是Top-N，其中N设置为1、3、5和10。
- en: Implementation and Environment. To collect test coverage data and compute results
    for baseline techniques, we utilized Gzoltar (Campos et al., [2012](https://arxiv.org/html/2409.13642v1#bib.bib14)),
    an automated tool that executes tests and gathers coverage information. For the
    LLM-based components, we employed OpenAI’s GPT-4o mini, a more cost-effective
    and capable alternative to GPT-3.5 Turbo (OpenAI, [2024](https://arxiv.org/html/2409.13642v1#bib.bib42)).
    LangChain v0.2 was used to streamline the development of LLM4FL, facilitating
    the integration of language models with external tools and enhancing the system’s
    overall functionality (Langchain, [2024a](https://arxiv.org/html/2409.13642v1#bib.bib28)).
    To implement the self-reflection technique, we leveraged the LangGraph framework,
    which enabled graph-based reasoning and decision-making processes (Langchain,
    [2024b](https://arxiv.org/html/2409.13642v1#bib.bib29)). To minimize the variations
    in the output, we set the temperature parameter to 0 during model inference.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实施与环境。为了收集测试覆盖数据并计算基准技术的结果，我们使用了Gzoltar（Campos等人，[2012](https://arxiv.org/html/2409.13642v1#bib.bib14)），这是一种自动化工具，能够执行测试并收集覆盖信息。对于基于LLM的组件，我们采用了OpenAI的GPT-4o
    mini，这是一个比GPT-3.5 Turbo（OpenAI，[2024](https://arxiv.org/html/2409.13642v1#bib.bib42)）更具成本效益且功能更强大的替代品。LangChain
    v0.2用于简化LLM4FL的开发，促进了语言模型与外部工具的集成，并增强了系统的整体功能（Langchain，[2024a](https://arxiv.org/html/2409.13642v1#bib.bib28)）。为了实现自我反思技术，我们利用了LangGraph框架，它支持基于图的推理和决策过程（Langchain，[2024b](https://arxiv.org/html/2409.13642v1#bib.bib29)）。为了最小化输出的变化，我们在模型推理过程中将温度参数设置为0。
- en: '4.1\. RQ1: How does LLM4FL perform compared with other fault localization techniques?'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.1\. RQ1: LLM4FL与其他故障定位技术相比，表现如何？'
- en: Motivation. In this RQ we evaluate the fault localization accuracy of LLM4FL
    by comparing with various baseline techniques.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 动机。在这个研究问题中，我们通过与各种基准技术的比较，评估LLM4FL的故障定位准确性。
- en: 'Approach. We compare LLM4FL’s fault localization accuracy with five baselines:
    Ochiai (Abreu et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3)), DeepFL (Li
    et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33)), Grace (Lou et al.,
    [2021](https://arxiv.org/html/2409.13642v1#bib.bib39)), DepGraph (Rafi et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)), and AutoFL (Kang et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib26)).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。我们将LLM4FL的故障定位准确度与五个基准技术进行比较：Ochiai（Abreu等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3)），DeepFL（Li等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)），Grace（Lou等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)），DepGraph（Rafi等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)），以及AutoFL（Kang等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib26)）。
- en: Ochiai (Abreu et al., [2006](https://arxiv.org/html/2409.13642v1#bib.bib3))
    is widely recognized in fault localization research for its high efficiency and
    accuracy, making it a common baseline for comparison (Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39);
    Li et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib36); Cui et al.,
    [2020](https://arxiv.org/html/2409.13642v1#bib.bib17); Wen et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib59);
    Qian et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib48); Rafi et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)). As such, we use Ochiai
    to rank the methods during the segmentation process and include it as a baseline
    for accuracy comparison. DeepFL (Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33))
    is a deep-learning-based fault localization technique that integrates spectrum-based
    and other metrics such as code complexity, and textual similarity features to
    locate faults. It utilizes a Multi-layer Perceptron (MLP) model to analyze these
    varied feature dimensions. We follow the study (Li et al., [2019](https://arxiv.org/html/2409.13642v1#bib.bib33))
    to implement DeepFL and include the SBFL scores from 34 techniques, code complexity,
    and textual similarities as part of the features for the deep learning model.
    Grace (Lou et al., [2021](https://arxiv.org/html/2409.13642v1#bib.bib39)) is one
    of the first fault localization techniques based on graph neural networks (GNN)
    that represents code as a graph and uses a gated graph neural network to rank
    the faulty methods. Since then, GNN-based techniques have shown superior fault
    localization accuracy compared to traditional techniques. DepGraph (Rafi et al.,
    [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)) is a GNN-based technique
    that further improves Grace by enhancing code representation in a graph using
    interprocedural call graph analysis for graph pruning and integrating historical
    code change information in the graph.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Ochiai（Abreu 等人，[2006](https://arxiv.org/html/2409.13642v1#bib.bib3)）因其高效性和准确性，在故障定位研究中得到广泛认可，成为了常见的基准比较方法（Lou
    等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)；Li 等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib36)；Cui
    等人，[2020](https://arxiv.org/html/2409.13642v1#bib.bib17)；Wen 等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib59)；Qian
    等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib48)；Rafi 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)））。因此，我们在分割过程中使用
    Ochiai 来对方法进行排名，并将其作为准确度比较的基准。DeepFL（Li 等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)）是一种基于深度学习的故障定位技术，集成了基于谱的指标和其他如代码复杂性及文本相似度特征，用于定位故障。它利用多层感知机（MLP）模型来分析这些不同的特征维度。我们遵循该研究（Li
    等人，[2019](https://arxiv.org/html/2409.13642v1#bib.bib33)）来实现 DeepFL，并将来自 34 种技术的
    SBFL 分数、代码复杂性和文本相似度作为深度学习模型的特征之一。Grace（Lou 等人，[2021](https://arxiv.org/html/2409.13642v1#bib.bib39)）是基于图神经网络（GNN）的首批故障定位技术之一，它将代码表示为图，并使用门控图神经网络对故障方法进行排名。从那时起，基于
    GNN 的技术相比传统技术展示了更优越的故障定位准确性。DepGraph（Rafi 等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)）是一种基于
    GNN 的技术，通过增强图中代码表示，利用过程间调用图分析进行图修剪，并将历史代码变更信息集成到图中，进一步改进了 Grace。
- en: AutoFL is a LLM-based fault localization technique. It begins by providing the
    LLM with information about a failing test and descriptions of specific methods
    that can be used to navigate the codebase. The LLM then interacts with these methods
    to gather relevant information, such as covered classes, methods, and code snippets.
    Scoring and ranking candidate methods (depicted as black rectangles) based on
    five AutoFL prediction outcomes. The methods are ranked by assigning scores based
    on multiple runs of AutoFL. In each run, a method’s score is the inverse of the
    total number of predicted methods, and these scores are averaged across all runs.
    Methods are then ranked in descending order of their average scores, with earlier
    predictions used to break any ties. While the original paper used OpenAI’s GPT-3.5-turbo-0613
    model for their experiments, for our evaluation, we are using the latest lightweight
    GPT-4o mini model to perform AutoFL’s experiments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: AutoFL 是一种基于大语言模型（LLM）的故障定位技术。它首先通过向 LLM 提供故障测试信息以及描述可以用来浏览代码库的特定方法。然后，LLM 与这些方法进行交互，收集相关信息，如覆盖的类、方法和代码片段。根据五种
    AutoFL 预测结果，对候选方法（表示为黑色矩形）进行评分和排名。方法的排名是通过多次运行 AutoFL 来分配分数的。在每次运行中，方法的分数是预测方法总数的倒数，这些分数会在所有运行中取平均值。然后，方法根据其平均分数按降序排列，较早的预测用于打破任何平局。虽然原论文使用了
    OpenAI 的 GPT-3.5-turbo-0613 模型进行实验，但在我们的评估中，我们使用了最新的轻量级 GPT-4o mini 模型来执行 AutoFL
    的实验。
- en: 'Table 2\. Fault localization accuracy in terms of Top-1, 3, 5, and 10\. The
    numbers in the parenthesis show the number of faults in each project. The result
    continues in Table [3](https://arxiv.org/html/2409.13642v1#S4.T3 "Table 3 ‣ 4.1\.
    RQ1: How does LLM4FL perform compared with other fault localization techniques?
    ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through Ordered
    Code Analysis with LLM Agents and Self-Reflection").'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '表2. 故障定位准确率，按 Top-1、Top-3、Top-5 和 Top-10 计算。括号中的数字表示每个项目中的故障数量。结果将在表[3](https://arxiv.org/html/2409.13642v1#S4.T3
    "表 3 ‣ 4.1\. RQ1: LLM4FL 与其他故障定位技术的比较？ ‣ 4\. 研究设计与结果 ‣ 通过有序代码分析与 LLM 代理及自我反思提升故障定位")中继续。'
- en: '|         Project (# faults) |         Techniques |         Top-1 |         Top-3
    |         Top-5 |         Top-10 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|         项目（# 故障） |         技术 |         Top-1 |         Top-3 |         Top-5
    |         Top-10 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|         Cli (39) |         Ochiai |         3.0 |         5.0 |         10.0
    |         18.0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|         Cli (39) |         Ochiai |         3.0 |         5.0 |         10.0
    |         18.0 |'
- en: '|  |         DeepFL |         11.0 |         21.0 |         24.0 |         28.0
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         11.0 |         21.0 |         24.0 |         28.0
    |'
- en: '|  |         Grace |         14.0 |         24.0 |         26.0 |         30.0
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         14.0 |         24.0 |         26.0 |         30.0
    |'
- en: '|  |         DepGraph |         17.0 |         24.0 |         27.0 |         34.0
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         17.0 |         24.0 |         27.0 |         34.0
    |'
- en: '|  |         AutoFL |         12.0 |         19.0 |         19.0 |         20.0
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         12.0 |         19.0 |         19.0 |         20.0
    |'
- en: '|  |         LLM4FL |         16.0 |         21.0 |         23.0 |         24.0
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         16.0 |         21.0 |         23.0 |         24.0
    |'
- en: '|         Closure (174) |         Ochiai |         20.0 |         39.0 |         70.0
    |         72.0 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|         Closure (174) |         Ochiai |         20.0 |         39.0 |         70.0
    |         72.0 |'
- en: '|  |         DeepFL |         46.0 |         61.0 |         92.0 |         99.0
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         46.0 |         61.0 |         92.0 |         99.0
    |'
- en: '|  |         Grace |         51.0 |         78.0 |         102.0 |         121.0
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         51.0 |         78.0 |         102.0 |         121.0
    |'
- en: '|  |         DepGraph |         60.0 |         99.0 |         126.0 |         148.0
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         60.0 |         99.0 |         126.0 |         148.0
    |'
- en: '|  |         AutoFL |         45.0 |         58.0 |         65.0 |         82.0
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         45.0 |         58.0 |         65.0 |         82.0
    |'
- en: '|  |         LLM4FL |         52.0 |         77.0 |         102.0 |         118.0
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         52.0 |         77.0 |         102.0 |         118.0
    |'
- en: '|         Codec (18) |         Ochiai |         3.0 |         12.0 |         17.0
    |         17.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|         Codec (18) |         Ochiai |         3.0 |         12.0 |         17.0
    |         17.0 |'
- en: '|  |         DeepFL |         5.0 |         10.0 |         12.0 |         16.0
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         5.0 |         10.0 |         12.0 |         16.0
    |'
- en: '|  |         Grace |         6.0 |         11.0 |         13.0 |         17.0
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         6.0 |         11.0 |         13.0 |         17.0
    |'
- en: '|  |         DepGraph |         7.0 |         10.0 |         14.0 |         16.0
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         7.0 |         10.0 |         14.0 |         16.0
    |'
- en: '|  |         AutoFL |         12.0 |         14.0 |         14.0 |         16.0
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         12.0 |         14.0 |         14.0 |         16.0
    |'
- en: '|  |         LLM4FL |         9.0 |         13.0 |         13.0 |         13.0
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         9.0 |         13.0 |         13.0 |         13.0
    |'
- en: '|         Collections (4) |         Ochiai |         1.0 |         1.0 |         2.0
    |         2.0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|         Collections (4) |         Ochiai |         1.0 |         1.0 |         2.0
    |         2.0 |'
- en: '|  |         DeepFL |         1.0 |         1.0 |         2.0 |         2.0
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         1.0 |         1.0 |         2.0 |         2.0
    |'
- en: '|  |         Grace |         1.0 |         1.0 |         2.0 |         2.0
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         1.0 |         1.0 |         2.0 |         2.0
    |'
- en: '|  |         DepGraph |         1.0 |         2.0 |         2.0 |         2.0
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         1.0 |         2.0 |         2.0 |         2.0
    |'
- en: '|  |         AutoFL |         1.0 |         1.0 |         1.0 |         1.0
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         1.0 |         1.0 |         1.0 |         1.0
    |'
- en: '|  |         LLM4FL |         1.0 |         1.0 |         1.0 |         1.0
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         1.0 |         1.0 |         1.0 |         1.0
    |'
- en: '|         Compress (47) |         Ochiai |         5.0 |         12.0 |         17.0
    |         29.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|         Compress (47) |         Ochiai |         5.0 |         12.0 |         17.0
    |         29.0 |'
- en: '|  |         DeepFL |         22.0 |         27.0 |         31.0 |         38.0
    |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         22.0 |         27.0 |         31.0 |         38.0
    |'
- en: '|  |         Grace |         23.0 |         29.0 |         34.0 |         42.0
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         23.0 |         29.0 |         34.0 |         42.0
    |'
- en: '|  |         DepGraph |         25.0 |         33.0 |         36.0 |         45.0
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         25.0 |         33.0 |         36.0 |         45.0
    |'
- en: '|  |         AutoFL |         23.0 |         33.0 |         34.0 |         35.0
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         23.0 |         33.0 |         34.0 |         35.0
    |'
- en: '|  |         LLM4FL |         23.0 |         32.0 |         34.0 |         34.0
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         23.0 |         32.0 |         34.0 |         34.0
    |'
- en: '|         Csv (16) |         Ochiai |         3.0 |         8.0 |         10.0
    |         12.0 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|         Csv (16) |         Ochiai |         3.0 |         8.0 |         10.0
    |         12.0 |'
- en: '|  |         DeepFL |         7.0 |         8.0 |         9.0 |         11.0
    |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         7.0 |         8.0 |         9.0 |         11.0
    |'
- en: '|  |         Grace |         6.0 |         8.0 |         10.0 |         12.0
    |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         6.0 |         8.0 |         10.0 |         12.0
    |'
- en: '|  |         DepGraph |         8.0 |         9.0 |         12.0 |         13.0
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         8.0 |         9.0 |         12.0 |         13.0
    |'
- en: '|  |         AutoFL |         5.0 |         11.0 |         12.0 |         14.0
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         5.0 |         11.0 |         12.0 |         14.0
    |'
- en: '|  |         LLM4FL |         8.0 |         10.0 |         10.0 |         10.0
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         8.0 |         10.0 |         10.0 |         10.0
    |'
- en: '|         Gson (16) |         Ochiai |         4.0 |         9.0 |         9.0
    |         12.0 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|         Gson (16) |         Ochiai |         4.0 |         9.0 |         9.0
    |         12.0 |'
- en: '|  |         DeepFL |         8.0 |         11.0 |         12.0 |         12.0
    |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         8.0 |         11.0 |         12.0 |         12.0
    |'
- en: '|  |         Grace |         11.0 |         13.0 |         14.0 |         15.0
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         11.0 |         13.0 |         14.0 |         15.0
    |'
- en: '|  |         DepGraph |         14.0 |         15.0 |         16.0 |         16.0
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         14.0 |         15.0 |         16.0 |         16.0
    |'
- en: '|  |         AutoFL |         5.0 |         7.0 |         10.0 |         10.0
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         5.0 |         7.0 |         10.0 |         10.0
    |'
- en: '|  |         LLM4FL |         11.0 |         14.0 |         14.0 |         14.0
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         11.0 |         14.0 |         14.0 |         14.0
    |'
- en: 'Table 3\. Continued from Table [2](https://arxiv.org/html/2409.13642v1#S4.T2
    "Table 2 ‣ 4.1\. RQ1: How does LLM4FL perform compared with other fault localization
    techniques? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection"). Fault localization
    accuracy in terms of Top-1, 3, 5, and 10\. The numbers in the parenthesis show
    the number of faults in each project.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|   表 3．接续自表[2](https://arxiv.org/html/2409.13642v1#S4.T2 "表 2 ‣ 4.1. RQ1：LLM4FL
    与其他故障定位技术相比如何？ ‣ 4. 研究设计与结果 ‣ 通过有序代码分析与 LLM 代理和自我反思增强故障定位")。根据 Top-1、Top-3、Top-5
    和 Top-10 的准确度进行故障定位。括号中的数字表示每个项目中的故障数量。'
- en: '|         Project (# faults) |         Techniques |         Top-1 |         Top-3
    |         Top-5 |         Top-10 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|         Project (# faults) |         Techniques |         Top-1 |         Top-3
    |         Top-5 |         Top-10 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|         JacksonCore (26) |         Ochiai |         6.0 |         11.0 |
            13.0 |         14.0 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|         JacksonCore (26) |         Ochiai |         6.0 |         11.0 |
            13.0 |         14.0 |'
- en: '|  |         DeepFL |         5.0 |         5.0 |         9.0 |         10.0
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         5.0 |         5.0 |         9.0 |         10.0
    |'
- en: '|  |         Grace |         9.0 |         13.0 |         14.0 |         15.0
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         9.0 |         13.0 |         14.0 |         15.0
    |'
- en: '|  |         DepGraph |         12.0 |         15.0 |         15.0 |         16.0
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         12.0 |         15.0 |         15.0 |         16.0
    |'
- en: '|  |         AutoFL |         10.0 |         17.0 |         17.0 |         17.0
    |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         10.0 |         17.0 |         17.0 |         17.0
    |'
- en: '|  |         LLM4FL |         12.0 |         13.0 |         15.0 |         15.0
    |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         12.0 |         13.0 |         15.0 |         15.0
    |'
- en: '|         JacksonXml (6) |         Ochiai |         0.0 |         0.0 |         0.0
    |         0.0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|         JacksonXml (6) |         Ochiai |         0.0 |         0.0 |         0.0
    |         0.0 |'
- en: '|  |         DeepFL |         3.0 |         3.0 |         4.0 |         5.0
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         3.0 |         3.0 |         4.0 |         5.0
    |'
- en: '|  |         Grace |         3.0 |         3.0 |         4.0 |         5.0
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         3.0 |         3.0 |         4.0 |         5.0
    |'
- en: '|  |         DepGraph |         4.0 |         5.0 |         5.0 |         5.0
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         4.0 |         5.0 |         5.0 |         5.0
    |'
- en: '|  |         AutoFL |         2.0 |         2.0 |         2.0 |         3.0
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         2.0 |         2.0 |         2.0 |         3.0
    |'
- en: '|  |         LLM4FL |         4.0 |         4.0 |         4.0 |         4.0
    |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         4.0 |         4.0 |         4.0 |         4.0
    |'
- en: '|         Jsoup (93) |         Ochiai |         15.0 |         40.0 |         48.0
    |         57.0 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|         Jsoup (93) |         Ochiai |         15.0 |         40.0 |         48.0
    |         57.0 |'
- en: '|  |         DeepFL |         33.0 |         39.0 |         46.0 |         49.0
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         33.0 |         39.0 |         46.0 |         49.0
    |'
- en: '|  |         Grace |         40.0 |         64.0 |         72.0 |         77.0
    |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         40.0 |         64.0 |         72.0 |         77.0
    |'
- en: '|  |         DepGraph |         53.0 |         73.0 |         78.0 |         83.0
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         53.0 |         73.0 |         78.0 |         83.0
    |'
- en: '|  |         AutoFL |         36.0 |         52.0 |         52.0 |         54.0
    |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         36.0 |         52.0 |         52.0 |         54.0
    |'
- en: '|  |         LLM4FL |         41.0 |         56.0 |         60.0 |         60.0
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         41.0 |         56.0 |         60.0 |         60.0
    |'
- en: '|         Lang (64) |         Ochiai |         25.0 |         45.0 |         51.0
    |         59.0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|         语言 (64) |         Ochiai |         25.0 |         45.0 |         51.0
    |         59.0 |'
- en: '|  |         DeepFL |         42.0 |         53.0 |         55.0 |         57.0
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         42.0 |         53.0 |         55.0 |         57.0
    |'
- en: '|  |         Grace |         43.0 |         53.0 |         57.0 |         58.0
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         43.0 |         53.0 |         57.0 |         58.0
    |'
- en: '|  |         DepGraph |         48.0 |         55.0 |         60.0 |         61.0
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         48.0 |         55.0 |         60.0 |         61.0
    |'
- en: '|  |         AutoFL |         40.0 |         57.0 |         60.0 |         60.0
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         40.0 |         57.0 |         60.0 |         60.0
    |'
- en: '|  |         LLM4FL |         48.0 |         55.0 |         58.0 |         58.0
    |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         48.0 |         55.0 |         58.0 |         58.0
    |'
- en: '|         Math (106) |         Ochiai |         23.0 |         52.0 |         62.0
    |         82.0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|         数学 (106) |         Ochiai |         23.0 |         52.0 |         62.0
    |         82.0 |'
- en: '|  |         DeepFL |         52.0 |         81.0 |         90.0 |         95.0
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         52.0 |         81.0 |         90.0 |         95.0
    |'
- en: '|  |         Grace |         64.0 |         79.0 |         92.0 |         97.0
    |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         64.0 |         79.0 |         92.0 |         97.0
    |'
- en: '|  |         DepGraph |         72.0 |         92.0 |         97.0 |         102.0
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         72.0 |         92.0 |         97.0 |         102.0
    |'
- en: '|  |         AutoFL |         53.0 |         81.0 |         87.0 |         94.0
    |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         53.0 |         81.0 |         87.0 |         94.0
    |'
- en: '|  |         LLM4FL |         68.0 |         87.0 |         92.0 |         94.0
    |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         68.0 |         87.0 |         92.0 |         94.0
    |'
- en: '|         Time (26) |         Ochiai |         6.0 |         12.0 |         13.0
    |         16.0 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|         时间 (26) |         Ochiai |         6.0 |         12.0 |         13.0
    |         16.0 |'
- en: '|  |         DeepFL |         12.0 |         15.0 |         18.0 |         20.0
    |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         12.0 |         15.0 |         18.0 |         20.0
    |'
- en: '|  |         Grace |         11.0 |         16.0 |         20.0 |         21.0
    |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         11.0 |         16.0 |         20.0 |         21.0
    |'
- en: '|  |         DepGraph |         17.0 |         20.0 |         21.0 |         22.0
    |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         17.0 |         20.0 |         21.0 |         22.0
    |'
- en: '|  |         AutoFL |         13.0 |         18.0 |         21.0 |         22.0
    |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         13.0 |         18.0 |         21.0 |         22.0
    |'
- en: '|  |         LLM4FL |         14.0 |         21.0 |         22.0 |         23.0
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         14.0 |         21.0 |         22.0 |         23.0
    |'
- en: '|         Mockito (38) |         Ochiai |         7.0 |         14.0 |         18.0
    |         23.0 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '|         Mockito (38) |         Ochiai |         7.0 |         14.0 |         18.0
    |         23.0 |'
- en: '|  |         DeepFL |         10.0 |         18.0 |         23.0 |         26.0
    |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         10.0 |         18.0 |         23.0 |         26.0
    |'
- en: '|  |         Grace |         16.0 |         24.0 |         26.0 |         29.0
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         16.0 |         24.0 |         26.0 |         29.0
    |'
- en: '|  |         DepGraph |         21.0 |         29.0 |         32.0 |         34.0
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         21.0 |         29.0 |         32.0 |         34.0
    |'
- en: '|  |         AutoFL |         18.0 |         23.0 |         29.0 |         29.0
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         18.0 |         23.0 |         29.0 |         29.0
    |'
- en: '|  |         LLM4FL |         21.0 |         22.0 |         26.0 |         27.0
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         21.0 |         22.0 |         26.0 |         27.0
    |'
- en: '|         Total (675) |         Ochiai |         121.0 |         260.0 |         340.0
    |         413.0 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|         总计 (675) |         Ochiai |         121.0 |         260.0 |         340.0
    |         413.0 |'
- en: '|  |         DeepFL |         257.0 |         353.0 |         427.0 |         468.0
    |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  |         DeepFL |         257.0 |         353.0 |         427.0 |         468.0
    |'
- en: '|  |         Grace |         298.0 |         416.0 |         486.0 |         541.0
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  |         Grace |         298.0 |         416.0 |         486.0 |         541.0
    |'
- en: '|  |         DepGraph |         359.0 |         481.0 |         541.0 |         597.0
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |         DepGraph |         359.0 |         481.0 |         541.0 |         597.0
    |'
- en: '|  |         AutoFL |         275.0 |         393.0 |         423.0 |         457.0
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  |         AutoFL |         275.0 |         393.0 |         423.0 |         457.0
    |'
- en: '|  |         LLM4FL |         328.0 |         426.0 |         474.0 |         495.0
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  |         LLM4FL |         328.0 |         426.0 |         474.0 |         495.0
    |'
- en: 'Results. LLM4FL outperforms the LLM-based baseline, AutoFL, by achieving a
    much higher Pass@1 (328 v.s. 275). Table [2](https://arxiv.org/html/2409.13642v1#S4.T2
    "Table 2 ‣ 4.1\. RQ1: How does LLM4FL perform compared with other fault localization
    techniques? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") and [3](https://arxiv.org/html/2409.13642v1#S4.T3
    "Table 3 ‣ 4.1\. RQ1: How does LLM4FL perform compared with other fault localization
    techniques? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") show the fault localization
    results of LLM4FL and the baseline techniques. Between the two LLM-based techniques,
    LLM4FL achieves a better Top@N than AutoFL when N is 1, 3, 5, and 10. In the Top-1
    metric, LLM4FL locates the correct fault in 328 cases, compared to AutoFL’s 275,
    representing a 19.27% improvement. Similarly, in the Top-3 and Top-5 metrics,
    LLM4FL achieves scores of 426 and 474, respectively, outperforming AutoFL’s results
    of 393 and 423\. Even in the broader Top-10 metric, LLM4FL shows an 8.32% advantage,
    with a score of 495 compared to AutoFL’s 457\. These numbers highlight LLM4FL’s
    enhanced ability to pinpoint faulty methods more accurately and efficiently, reinforcing
    its superiority over AutoFL for fault localization tasks.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '结果。LLM4FL通过实现更高的Pass@1（328对275），超越了基于LLM的基准AutoFL。表[2](https://arxiv.org/html/2409.13642v1#S4.T2
    "Table 2 ‣ 4.1\. RQ1: How does LLM4FL perform compared with other fault localization
    techniques? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection")和[3](https://arxiv.org/html/2409.13642v1#S4.T3
    "Table 3 ‣ 4.1\. RQ1: How does LLM4FL perform compared with other fault localization
    techniques? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection")展示了LLM4FL和基准技术的故障定位结果。在这两种基于LLM的技术中，当N为1、3、5和10时，LLM4FL的Top@N表现优于AutoFL。在Top-1指标中，LLM4FL能够定位328个正确的故障，而AutoFL为275，提升了19.27%。类似地，在Top-3和Top-5指标中，LLM4FL的得分分别为426和474，超越了AutoFL的393和423。即使在更广泛的Top-10指标中，LLM4FL也展现出8.32%的优势，得分495，而AutoFL为457。这些数字凸显了LLM4FL在更准确、更高效地定位故障方法方面的增强能力，进一步证明了它在故障定位任务中的优势。'
- en: LLM4FL shows higher Top-1 and Top-3 compared to all other non-LLM-based techniques,
    except for DepGraph. For the Top-1 metric, LLM4FL scores 328, which is 171.07%
    higher than Ochai’s score of 121, 27.63% higher than DeepFL’s score of 257, and
    10.07% better than Grace’s result of 298. One exception is DepGraph, which achieves
    a Top-1 of 359, 8.64% higher than LLM4FL. As the range expands to Top-3 and beyond,
    LLM4FL continues to demonstrate its robustness, significantly outperforming DeepFL
    and maintaining strong performance alongside Grace. It is important to note that
    DeepFL, Grace, and DepGraph are supervised methods trained and evaluated using
    leave-one-out cross-validation, following the original study protocols. Despite
    LLM4FL being a zero-shot approach without task-specific training, its ability
    to perform competitively with these supervised techniques further highlights its
    effectiveness. The fact that LLM4FL achieves strong results without the need for
    extensive training data or cross-validation underscores its potential and sheds
    light on future research directions, especially when training models on specific
    data sets may not always be feasible.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL在Top-1和Top-3指标上相比所有其他非LLM技术表现更佳，除了DepGraph。对于Top-1指标，LLM4FL得分328，比Ochai的121高171.07%，比DeepFL的257高27.63%，比Grace的298高10.07%。唯一的例外是DepGraph，其Top-1得分为359，比LLM4FL高8.64%。随着范围扩展到Top-3及更高，LLM4FL继续展示其鲁棒性，显著优于DeepFL，并与Grace保持强劲的表现。需要注意的是，DeepFL、Grace和DepGraph是使用留一法交叉验证训练和评估的监督方法，遵循了原始研究协议。尽管LLM4FL是一种零样本方法，未进行任务特定训练，但它能够与这些监督方法竞争，进一步突显其有效性。LLM4FL能够在没有大量训练数据或交叉验证的情况下取得强劲的结果，这突出了其潜力，并为未来的研究方向提供了启示，尤其是在无法总是对特定数据集进行训练时。
- en: <svg class="ltx_picture" height="82.3" id="S4.SS1.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,82.3) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="59.42" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">LLM4FL outperforms the other
    LLM-based technique, AutoFL, by achieving a 19.27% higher Top-1 and delivering
    competitive results compared to supervised techniques like DeepFL and Grace, even
    without task-specific training. The findings highlight the potential of using
    LLM4FL for zero-shot fault localization.</foreignobject></g></g></svg>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="82.3" id="S4.SS1.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,82.3) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="59.42" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">LLM4FL outperforms the other
    LLM-based technique, AutoFL, by achieving a 19.27% higher Top-1 and delivering
    competitive results compared to supervised techniques like DeepFL and Grace, even
    without task-specific training. The findings highlight the potential of using
    LLM4FL for zero-shot fault localization.</foreignobject></g></g></svg>
- en: '4.2\. RQ2: How do different components affect LLM4FL’s fault localization accuracy?'
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.2\. RQ2: 不同组件如何影响LLM4FL的故障定位准确性？'
- en: Motivation. In this research question, we study the influence of each individual
    component within LLM4FL on its fault localization performance. LLM4FL employs
    a combination of advanced techniques, including coverage splitting to handle token
    limitations, prompt chaining for fault navigation, self-reflection for iterative
    refinement of the agents’ decisions, and chain-of-thought reasoning for re-ranking
    suspicious methods. Each of these components plays a distinct role in the overall
    process. To study their effects, we investigate how the removal of any one of
    these components affects the LLM4FL’s ability to accurately localize faults. By
    isolating the effects of each technique, we can better understand which components
    are critical to maintaining or improving the overall accuracy of the LLM4FL and
    provide insights to improving future LLM-based fault localization techniques.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 动机。在这个研究问题中，我们研究LLM4FL中每个单独组件对其故障定位性能的影响。LLM4FL采用了多种先进技术的组合，包括覆盖分割来处理令牌限制、提示链式以进行故障导航、自我反思来迭代完善智能体的决策，以及链式思维推理来重新排序可疑方法。每个组件在整体过程中扮演着不同的角色。为了研究它们的效果，我们探讨了移除这些组件中的任何一个，如何影响LLM4FL准确定位故障的能力。通过隔离每项技术的影响，我们可以更好地理解哪些组件对维持或提高LLM4FL整体准确性至关重要，并为未来基于LLM的故障定位技术的改进提供见解。
- en: 'Approach. To evaluate the impact of each component, we designed four experimental
    configurations: LLM4FL[w/o CodeNavigation], LLM4FL[w/o Divide&Conquer], LLM4FL[w/o Reflection],
    and LLM4FL[w/o CoT]. For each configuration, one component is systematically removed,
    and we compare the result with the full-featured baseline of LLM4FL. This allows
    us to directly measure how much each component contributes to the fault localization
    process.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。为了评估每个组件的影响，我们设计了四个实验配置：LLM4FL[w/o CodeNavigation]、LLM4FL[w/o Divide&Conquer]、LLM4FL[w/o
    Reflection] 和 LLM4FL[w/o CoT]。对于每个配置，都会系统地去除一个组件，并将结果与LLM4FL的完整特性基准进行比较。这使我们能够直接衡量每个组件在故障定位过程中的贡献。
- en: LLM4FL[w/o CodeNavigation] removes the prompt chaining mechanism, which essentially
    means the agents no longer collaborate do fault navigation through multiple rounds
    of reasoning. Instead, the LLM4FL uses a single prompt to perform fault localization
    without iterative agent communication and fault navigation. This configuration
    tests whether the multi-step agent collaboration improves the ranking and selection
    of faulty methods or if a single round of communication is sufficient.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[w/o CodeNavigation] 移除了提示链机制，这意味着智能体不再通过多轮推理协作进行故障导航。相反，LLM4FL使用单一提示进行故障定位，而没有智能体的迭代通信和故障导航。该配置测试了多步骤的智能体协作是否有助于改善故障方法的排名和选择，或者单轮通信是否足够。
- en: LLM4FL[w/o Divide&Conquer] removes the grouping of the covered methods, giving
    the agents the entire coverage data at once instead of dividing it into smaller,
    manageable groups. Coverage segmentation addresses token limitations in LLMs,
    so removing it explores the impact of feeding the full dataset to the agents in
    one step. We aim to see how handling large amounts of data in a single input influences
    the fault localization result, as it may overwhelm the model or reduce precision.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[w/o Divide&Conquer] 移除了方法分组，将覆盖数据一次性提供给智能体，而不是将其分成更小的、可管理的组。覆盖分段处理解决了LLM中的令牌限制问题，因此移除这一环节可以探索将完整数据集一次性输入智能体的影响。我们旨在观察一次性处理大量数据如何影响故障定位结果，因为这可能会使模型过载或降低精度。
- en: LLM4FL[w/o Reflection] removes the self-reflection technique, which is used
    to allow agents to review and refine their initial candidate selection and ranking.
    Without this self-reflection step, the agents rely solely on their initial assessments
    without iterative improvements.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[w/o Reflection] 移除了自我反思技巧，原本该技巧用于让智能体回顾并完善初步候选选择和排序。没有了这个自我反思步骤，智能体只能依赖初步评估，而没有经过迭代的改进。
- en: LLM4FL[w/o CoT] disables the chain-of-thought reasoning process, which asks
    the agents to think deeper by asking LLMs to generate potential fixes and re-rank
    the suspicious methods. The chain-of-thought approach is designed to enhance logical
    reasoning and ensure that intermediate results are critically evaluated and refined.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[w/o CoT] 禁用了链式思维推理过程，原本该过程要求智能体通过让LLM生成潜在的修复方案并重新排序可疑方法，从而进行更深层次的思考。链式思维方法旨在增强逻辑推理，并确保中间结果得到批判性评估和完善。
- en: Table 4\. Impacts of removing different components in LLM4FL on Top-1, 3, 5,
    and 10\. The numbers in the parenthesis show the percentage changes compared to
    LLM4FL with all the components.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 移除 LLM4FL 中不同组件对 Top-1、3、5 和 10 的影响。括号中的数字表示与包含所有组件的 LLM4FL 相比的百分比变化。
- en: '|        Project (# faults) |        Techniques |        Top-1 |        Top-3
    |        Top-5 |        Top-10 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|        Project (# faults) |        Techniques |        Top-1 |        Top-3
    |        Top-5 |        Top-10 |'
- en: '|        Cli (39) |        LLM4FL |        16 |        21 |        23 |        24
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '|        Cli (39) |        LLM4FL |        16 |        21 |        23 |        24
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        11 (-31.25%) |        19 (-9.52%)
    |        21 (-8.7%) |        21 (-12.5%) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含代码导航] |        11 (-31.25%) |        19 (-9.52%) |        21
    (-8.7%) |        21 (-12.5%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        11 (-31.25%) |        17 (-19.05%)
    |        17 (-26.09%) |        19 (-20.83%) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含分治法] |        11 (-31.25%) |        17 (-19.05%) |        17
    (-26.09%) |        19 (-20.83%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        11 (-31.25%) |        19 (-9.52%)
    |        21 (-8.7%) |        21 (-12.5%) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含反思] |        11 (-31.25%) |        19 (-9.52%) |        21
    (-8.7%) |        21 (-12.5%) |'
- en: '|  |        LLM4FL[w/o CoT] |        16 (0.0%) |        21 (0.0%) |        23
    (0.0%) |        24 (0.0%) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含CoT] |        16 (0.0%) |        21 (0.0%) |        23
    (0.0%) |        24 (0.0%) |'
- en: '|        Closure (174) |        LLM4FL |        52 |        77 |        102
    |        118 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|        Closure (174) |        LLM4FL |        52 |        77 |        102
    |        118 |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        32 (-38.46%) |        45 (-41.56%)
    |        53 (-48.04%) |        53 (-55.08%) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含代码导航] |        32 (-38.46%) |        45 (-41.56%) |        53
    (-48.04%) |        53 (-55.08%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        27 (-48.08%) |        40 (-48.05%)
    |        49 (-51.96%) |        50 (-57.63%) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含分治法] |        27 (-48.08%) |        40 (-48.05%) |        49
    (-51.96%) |        50 (-57.63%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        50 (-3.85%) |        74 (-3.9%)
    |        92 (-9.8%) |        109 (-7.63%) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含反思] |        50 (-3.85%) |        74 (-3.9%) |        92
    (-9.8%) |        109 (-7.63%) |'
- en: '|  |        LLM4FL[w/o CoT] |        52 (0.0%) |        77 (0.0%) |        102
    (0.0%) |        118 (0.0%) |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含CoT] |        52 (0.0%) |        77 (0.0%) |        102
    (0.0%) |        118 (0.0%) |'
- en: '|        Codec (18) |        LLM4FL |        9 |        13 |        13 |        13
    |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|        Codec (18) |        LLM4FL |        9 |        13 |        13 |        13
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        8 (-11.11%) |        12 (-7.69%)
    |        13 (0.0%) |        13 (0.0%) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含代码导航] |        8 (-11.11%) |        12 (-7.69%) |        13
    (0.0%) |        13 (0.0%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        8 (-11.11%) |        11 (-15.38%)
    |        12 (-7.69%) |        13 (0.0%) |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含分治法] |        8 (-11.11%) |        11 (-15.38%) |        12
    (-7.69%) |        13 (0.0%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        9 (0.0%) |        12 (-7.69%) |
           13 (0.0%) |        13 (0.0%) |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含反思] |        9 (0.0%) |        12 (-7.69%) |        13
    (0.0%) |        13 (0.0%) |'
- en: '|  |        LLM4FL[w/o CoT] |        9 (0.0%) |        13 (0.0%) |        13
    (0.0%) |        13 (0.0%) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含CoT] |        9 (0.0%) |        13 (0.0%) |        13
    (0.0%) |        13 (0.0%) |'
- en: '|        Compress (47) |        LLM4FL |        23 |        32 |        34
    |        34 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '|        Compress (47) |        LLM4FL |        23 |        32 |        34
    |        34 |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        23 (0.0%) |        32 (0.0%)
    |        34 (0.0%) |        34 (0.0%) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含代码导航] |        23 (0.0%) |        32 (0.0%) |        34
    (0.0%) |        34 (0.0%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        23 (0.0%) |        28 (-12.5%)
    |        30 (-11.76%) |        31 (-8.82%) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含分治法] |        23 (0.0%) |        28 (-12.5%) |        30
    (-11.76%) |        31 (-8.82%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        23 (0.0%) |        32 (0.0%) |
           35 (2.94%) |        35 (2.94%) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含反思] |        23 (0.0%) |        32 (0.0%) |        35
    (2.94%) |        35 (2.94%) |'
- en: '|  |        LLM4FL[w/o CoT] |        23 (0.0%) |        32 (0.0%) |        34
    (0.0%) |        34 (0.0%) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含CoT] |        23 (0.0%) |        32 (0.0%) |        34
    (0.0%) |        34 (0.0%) |'
- en: '|        Csv (16) |        LLM4FL |        8 |        10 |        10 |        10
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|        Csv (16) |        LLM4FL |        8 |        10 |        10 |        10
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        8 (0.0%) |        8 (-20.0%)
    |        9 (-10.0%) |        9 (-10.0%) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含代码导航] |        8 (0.0%) |        8 (-20.0%) |        9
    (-10.0%) |        9 (-10.0%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        8 (0.0%) |        8 (-20.0%)
    |        9 (-10.0%) |        9 (-10.0%) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含分治法] |        8 (0.0%) |        8 (-20.0%) |        9
    (-10.0%) |        9 (-10.0%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        7 (-12.5%) |        10 (0.0%) |
           10 (0.0%) |        10 (0.0%) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[不含反思] |        7 (-12.5%) |        10 (0.0%) |        10
    (0.0%) |        10 (0.0%) |'
- en: '|  |        LLM4FL[w/o CoT] |        8 (0.0%) |        9 (-10.0%) |        10
    (0.0%) |        10 (0.0%) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        8 (0.0%) |        9 (-10.0%) |        10
    (0.0%) |        10 (0.0%) |'
- en: '|        Gson (16) |        LLM4FL |        11 |        14 |        14 |        14
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|        Gson (16) |        LLM4FL |        11 |        14 |        14 |        14
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        9 (-18.18%) |        15 (7.14%)
    |        15 (7.14%) |        15 (7.14%) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CodeNavigation] |        9 (-18.18%) |        15 (7.14%)
    |        15 (7.14%) |        15 (7.14%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        10 (-9.09%) |        14 (0.0%)
    |        15 (7.14%) |        15 (7.14%) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Divide&Conquer] |        10 (-9.09%) |        14 (0.0%)
    |        15 (7.14%) |        15 (7.14%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        11 (0.0%) |        12 (-14.29%)
    |        13 (-7.14%) |        14 (0.0%) |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Reflection] |        11 (0.0%) |        12 (-14.29%)
    |        13 (-7.14%) |        14 (0.0%) |'
- en: '|  |        LLM4FL[w/o CoT] |        11 (0.0%) |        14 (0.0%) |        14
    (0.0%) |        14 (0.0%) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        11 (0.0%) |        14 (0.0%) |        14
    (0.0%) |        14 (0.0%) |'
- en: '|        JacksonCore (26) |        LLM4FL |        12 |        13 |        15
    |        15 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|        JacksonCore (26) |        LLM4FL |        12 |        13 |        15
    |        15 |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        9 (-25.0%) |        13 (0.0%)
    |        14 (-6.67%) |        14 (-6.67%) |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CodeNavigation] |        9 (-25.0%) |        13 (0.0%)
    |        14 (-6.67%) |        14 (-6.67%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        9 (-25.0%) |        13 (0.0%)
    |        13 (-13.33%) |        13 (-13.33%) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Divide&Conquer] |        9 (-25.0%) |        13 (0.0%)
    |        13 (-13.33%) |        13 (-13.33%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        10 (-16.67%) |        13 (0.0%)
    |        13 (-13.33%) |        13 (-13.33%) |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Reflection] |        10 (-16.67%) |        13 (0.0%)
    |        13 (-13.33%) |        13 (-13.33%) |'
- en: '|  |        LLM4FL[w/o CoT] |        12 (0.0%) |        13 (0.0%) |        15
    (0.0%) |        15 (0.0%) |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        12 (0.0%) |        13 (0.0%) |        15
    (0.0%) |        15 (0.0%) |'
- en: '|        JacksonXml (6) |        LLM4FL |        4 |        4 |        4 |
           4 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '|        JacksonXml (6) |        LLM4FL |        4 |        4 |        4 |
           4 |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        2 (-50.0%) |        3 (-25.0%)
    |        3 (-25.0%) |        3 (-25.0%) |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CodeNavigation] |        2 (-50.0%) |        3 (-25.0%)
    |        3 (-25.0%) |        3 (-25.0%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        2 (-50.0%) |        2 (-50.0%)
    |        2 (-50.0%) |        2 (-50.0%) |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Divide&Conquer] |        2 (-50.0%) |        2 (-50.0%)
    |        2 (-50.0%) |        2 (-50.0%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        2 (-50.0%) |        2 (-50.0%)
    |        2 (-50.0%) |        2 (-50.0%) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Reflection] |        2 (-50.0%) |        2 (-50.0%)
    |        2 (-50.0%) |        2 (-50.0%) |'
- en: '|  |        LLM4FL[w/o CoT] |        3 (-25.0%) |        4 (0.0%) |        4
    (0.0%) |        4 (0.0%) |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        3 (-25.0%) |        4 (0.0%) |        4
    (0.0%) |        4 (0.0%) |'
- en: '|        Jsoup (93) |        LLM4FL |        41 |        56 |        60 |        60
    |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|        Jsoup (93) |        LLM4FL |        41 |        56 |        60 |        60
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        40 (-2.44%) |        55 (-1.79%)
    |        58 (-3.33%) |        58 (-3.33%) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CodeNavigation] |        40 (-2.44%) |        55 (-1.79%)
    |        58 (-3.33%) |        58 (-3.33%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        36 (-12.2%) |        50 (-10.71%)
    |        51 (-15.0%) |        51 (-15.0%) |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Divide&Conquer] |        36 (-12.2%) |        50 (-10.71%)
    |        51 (-15.0%) |        51 (-15.0%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        38 (-7.32%) |        53 (-5.36%)
    |        54 (-10.0%) |        54 (-10.0%) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Reflection] |        38 (-7.32%) |        53 (-5.36%)
    |        54 (-10.0%) |        54 (-10.0%) |'
- en: '|  |        LLM4FL[w/o CoT] |        40 (-2.44%) |        55 (-1.79%) |        59
    (-1.67%) |        60 (0.0%) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        40 (-2.44%) |        55 (-1.79%) |        59
    (-1.67%) |        60 (0.0%) |'
- en: '|        Lang (64) |        LLM4FL |        48 |        55 |        58 |        58
    |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|        Lang (64) |        LLM4FL |        48 |        55 |        58 |        58
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        42 (-12.5%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CodeNavigation] |        42 (-12.5%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        43 (-10.42%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Divide&Conquer] |        43 (-10.42%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        42 (-12.5%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o Reflection] |        42 (-12.5%) |        54 (-1.82%)
    |        59 (1.72%) |        59 (1.72%) |'
- en: '|  |        LLM4FL[w/o CoT] |        48 (0.0%) |        55 (0.0%) |        59
    (1.72%) |        59 (1.72%) |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |        LLM4FL[w/o CoT] |        48 (0.0%) |        55 (0.0%) |        59
    (1.72%) |        59 (1.72%) |'
- en: '|        Math (106) |        LLM4FL |        68 |        87 |        92 |        94
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|        Math (106) |        LLM4FL |        68 |        87 |        92 |        94
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        59 (-13.24%) |        79 (-9.2%)
    |        85 (-7.61%) |        85 (-9.57%) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CodeNavigation] |       59 (-13.24%) |       79 (-9.2%)
    |       85 (-7.61%) |       85 (-9.57%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        49 (-27.94%) |        68 (-21.84%)
    |        70 (-23.91%) |        78 (-17.02%) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Divide&Conquer] |       49 (-27.94%) |       68 (-21.84%)
    |       70 (-23.91%) |       78 (-17.02%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        57 (-16.18%) |        80 (-8.05%)
    |        85 (-7.61%) |        85 (-9.57%) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Reflection] |       57 (-16.18%) |       80 (-8.05%) |       85
    (-7.61%) |       85 (-9.57%) |'
- en: '|  |        LLM4FL[w/o CoT] |        68 (0.0%) |        87 (0.0%) |        91
    (-1.09%) |        94 (0.0%) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CoT] |       68 (0.0%) |       87 (0.0%) |       91 (-1.09%)
    |       94 (0.0%) |'
- en: '|        Mockito (38) |        LLM4FL |        21 |        22 |        26 |
           27 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|       Mockito (38) |       LLM4FL |       21 |       22 |       26 |       27
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        18 (-14.29%) |        23 (4.55%)
    |        23 (-11.54%) |        23 (-14.81%) |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CodeNavigation] |       18 (-14.29%) |       23 (4.55%)
    |       23 (-11.54%) |       23 (-14.81%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        15 (-28.57%) |        20 (-9.09%)
    |        21 (-19.23%) |        21 (-22.22%) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Divide&Conquer] |       15 (-28.57%) |       20 (-9.09%)
    |       21 (-19.23%) |       21 (-22.22%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        18 (-14.29%) |        20 (-9.09%)
    |        20 (-23.08%) |        22 (-18.52%) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Reflection] |       18 (-14.29%) |       20 (-9.09%) |       20
    (-23.08%) |       22 (-18.52%) |'
- en: '|  |        LLM4FL[w/o CoT] |        21 (0.0%) |        22 (0.0%) |        26
    (0.0%) |        27 (0.0%) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CoT] |       21 (0.0%) |       22 (0.0%) |       26 (0.0%)
    |       27 (0.0%) |'
- en: '|        Time (26) |        LLM4FL |        14 |        21 |        22 |        23
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|       时间 (26) |       LLM4FL |       14 |       21 |       22 |       23
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        12 (-14.29%) |        20 (-4.76%)
    |        22 (0.0%) |        22 (-4.35%) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CodeNavigation] |       12 (-14.29%) |       20 (-4.76%)
    |       22 (0.0%) |       22 (-4.35%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        10 (-28.57%) |        16 (-23.81%)
    |        17 (-22.73%) |        20 (-13.04%) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Divide&Conquer] |       10 (-28.57%) |       16 (-23.81%)
    |       17 (-22.73%) |       20 (-13.04%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        12 (-14.29%) |        19 (-9.52%)
    |        19 (-13.64%) |        22 (-4.35%) |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Reflection] |       12 (-14.29%) |       19 (-9.52%) |       19
    (-13.64%) |       22 (-4.35%) |'
- en: '|  |        LLM4FL[w/o CoT] |        14 (0.0%) |        22 (4.76%) |        23
    (4.55%) |        23 (0.0%) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CoT] |       14 (0.0%) |       22 (4.76%) |       23 (4.55%)
    |       23 (0.0%) |'
- en: '|        Total (675) |        LLM4FL |        327 |        425 |        473
    |        494 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|       总计 (675) |       LLM4FL |       327 |       425 |       473 |       494
    |'
- en: '|  |        LLM4FL[w/o CodeNavigation] |        273 (-16.51%) |        378
    (-11.06%) |        409 (-13.53%) |        409 (-17.21%) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CodeNavigation] |       273 (-16.51%) |       378 (-11.06%)
    |       409 (-13.53%) |       409 (-17.21%) |'
- en: '|  |        LLM4FL[w/o Divide&Conquer] |        251 (-23.24%) |        341
    (-19.76%) |        365 (-22.83%) |        381 (-22.87%) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Divide&Conquer] |       251 (-23.24%) |       341 (-19.76%)
    |       365 (-22.83%) |       381 (-22.87%) |'
- en: '|  |        LLM4FL[w/o Reflection] |        290 (-11.31%) |        400 (-5.88%)
    |        436 (-7.82%) |        459 (-7.09%) |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无Reflection] |       290 (-11.31%) |       400 (-5.88%) |       436
    (-7.82%) |       459 (-7.09%) |'
- en: '|  |        LLM4FL[w/o CoT] |        325 (-0.61%) |        424 (-0.24%) |        473
    (0.0%) |        495 (0.2%) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |       LLM4FL[无CoT] |       325 (-0.61%) |       424 (-0.24%) |       473
    (0.0%) |       495 (0.2%) |'
- en: 'Results. While all components help improve the results, including coverage
    grouping and prompt chaining provide the largest improvement to fault localization
    results (23% and 17% in Top-1). Table [4](https://arxiv.org/html/2409.13642v1#S4.T4
    "Table 4 ‣ 4.2\. RQ2: How do different components affect LLM4FL’s fault localization
    accuracy? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") shows the Top-1, 3,
    5, and 10 scores when each component is removed. Removing coverage splitting has
    the largest overall impact across all scores, reducing Top-N by 19% to 23%. Removing
    prompt chaining has the second largest impact (11% to 17%). At the individual
    project level, these two components also have the largest impact in Top-1 in 9/13
    studied projects. Our finding shows that employing sorted coverage grouping following
    the divide and conquer technique and agent communication significantly improves
    fault localization results. Future research should consider these techniques when
    designing fault localization techniques.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '结果。虽然所有组件都有助于提高结果，但包括覆盖分组和提示链在内的组件对故障定位结果的改善最大（Top-1提高了23%和17%）。表[4](https://arxiv.org/html/2409.13642v1#S4.T4
    "表4 ‣ 4.2\. RQ2: 不同组件如何影响LLM4FL的故障定位准确性？ ‣ 4\. 研究设计与结果 ‣ 通过有序代码分析和LLM代理自我反思增强故障定位")展示了每个组件移除后Top-1、3、5和10的得分。移除覆盖分割对所有得分的整体影响最大，Top-N得分降低了19%到23%。移除提示链的影响次之（11%到17%）。在各个项目层面，这两个组件在9/13个研究项目中也对Top-1有最大影响。我们的研究发现，采用基于分治技术的有序覆盖分组和代理通信显著改善了故障定位结果。未来的研究在设计故障定位技术时应考虑这些技术。'
- en: Although there is no oracle during the fault localization process, asking LLMs
    to self-reflection still helps improve the overall Top-1 by 11%. LLMs often suffer
    from hallucinations, especially when there is a lack of feedback from external
    oracles (Xu et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib72); Huang
    et al., [2023](https://arxiv.org/html/2409.13642v1#bib.bib23)). Even though we
    did not provide any groundtruth or external feedback to LLM, we find that self-reflection
    is still effective in improving fault localization results. Self-reflect brings
    6% to 11% improvement across the Top-N metrics. Our finding suggests that future
    studies should consider self-reflection even if there is no external feedback.
    Interestingly, we did not see large differences after removing chain-of-thought
    reasoning via test generation. In some cases, removing it even slightly improves
    the fault localization results. One reason may be that LLM4FL already includes
    self-reflection to make LLM think deeply about the result, and adding a chain-of-thought
    did not further improve the result. Our finding highlights the effectiveness of
    self-reflection, which should be considered in future fault localization results.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在故障定位过程中没有oracle，但要求LLM进行自我反思仍有助于提高整体Top-1得分11%。LLM经常出现幻觉，尤其是在缺乏外部oracle反馈的情况下（Xu等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib72);
    Huang等人，[2023](https://arxiv.org/html/2409.13642v1#bib.bib23)）。尽管我们没有向LLM提供任何真实数据或外部反馈，但我们发现自我反思仍然有效地改善了故障定位结果。自我反思在Top-N指标上带来了6%到11%的改善。我们的发现表明，未来的研究应考虑自我反思，即使没有外部反馈。值得注意的是，在移除通过测试生成的思维链推理后，我们没有看到显著的差异。在某些情况下，移除它甚至略微改善了故障定位结果。一个原因可能是LLM4FL已经包含了自我反思功能，使LLM深入思考结果，添加思维链并没有进一步改善结果。我们的发现强调了自我反思的有效性，未来的故障定位结果中应考虑这一点。
- en: <svg class="ltx_picture" height="84.99" id="S4.SS2.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,84.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="62.11" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">The results show that each
    component of LLM4FL contributes to its overall fault localization performance,
    with coverage splitting and prompt chaining having the largest positive impact.
    Removing these components leads to significant declines in accuracy, confirming
    their crucial role in handling token limitations and enabling more effective multi-agent
    collaboration.</foreignobject></g></g></svg>
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="84.99" id="S4.SS2.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,84.99) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="62.11" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">The results show that each
    component of LLM4FL contributes to its overall fault localization performance,
    with coverage splitting and prompt chaining having the largest positive impact.
    Removing these components leads to significant declines in accuracy, confirming
    their crucial role in handling token limitations and enabling more effective multi-agent
    collaboration.</foreignobject></g></g></svg>
- en: '4.3\. RQ3: Does order matter in the initial list of methods provided to the
    LLM?'
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. RQ3：提供给LLM的初始方法列表中的顺序是否重要？
- en: Motivation. LLM4FL divides the coverage data into segments to address the token
    size limitation of LLMs. We sort the methods using the Ochiai scores before segmentation,
    though different sorting mechanisms may affect the final fault localization result.
    Although LLM4FL eventually visits and assesses every method, a recent study (Chen
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib16)) observes that the
    order of premises affects LLM’s results. However, whether this effect extends
    to software engineering tasks, particularly fault localization, remains unclear.
    Hence, in this research question, we investigate whether the order of methods
    within the segments affects the LLM’s fault localization performance.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 动机。LLM4FL将覆盖数据分为多个段，以解决LLM的令牌大小限制。在分段之前，我们使用Ochiai得分对方法进行排序，尽管不同的排序机制可能会影响最终的故障定位结果。尽管LLM4FL最终会访问并评估每个方法，但最近的一项研究（Chen等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib16)）观察到前提的顺序会影响LLM的结果。然而，这种影响是否扩展到软件工程任务，特别是故障定位，仍不清楚。因此，在这个研究问题中，我们探讨了在分段内方法的顺序是否会影响LLM的故障定位表现。
- en: 'Approach. To test the effect of method ordering, we experiment with three distinct
    sorting strategies: LLM4FL[Execution], LLM4FL[Ochiai] (the default sorting in
    LLM4FL), and LLM4FL[DepGraph]. Each strategy provides a different way to sort
    the methods before they are segmented for further analysis.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 方法。为了测试方法排序的效果，我们尝试了三种不同的排序策略：LLM4FL[执行]、LLM4FL[Ochiai]（LLM4FL中的默认排序）和LLM4FL[DepGraph]。每种策略提供了一种不同的方式来排序方法，以便在进一步分析之前进行分段。
- en: 'LLM4FL[Execution]: In this baseline approach, we use the unsorted list of methods
    executed during testing, as generated by Gzoltar (Campos et al., [2012](https://arxiv.org/html/2409.13642v1#bib.bib14)).
    This default list represents the natural execution order of the methods, with
    no explicit ranking or prioritization. By providing the LLM with methods based
    on the execution sequence, we establish a control case to measure its performance
    without any ranking influence. The result shows how the order of premises in reasoning
    tasks can lead to varied LLM performance in software engineering context (Chen
    et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib16)).'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM4FL[执行]: 在这个基线方法中，我们使用在测试过程中执行的未排序方法列表，该列表由Gzoltar生成（Campos等人，[2012](https://arxiv.org/html/2409.13642v1#bib.bib14)）。这个默认列表代表了方法的自然执行顺序，没有明确的排序或优先级。通过为LLM提供基于执行顺序的方法，我们建立了一个对照组，以衡量其性能，而不受任何排序的影响。结果表明，在推理任务中，前提的顺序会导致LLM在软件工程背景下的表现有所不同（Chen等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib16)）。'
- en: 'LLM4FL[Ochiai]: As discussed in Section [3.1](https://arxiv.org/html/2409.13642v1#S3.SS1
    "3.1\. Analyzing the Covered Methods Using Divide-and-Conquer ‣ 3\. Methodology
    ‣ Enhancing Fault Localization Through Ordered Code Analysis with LLM Agents and
    Self-Reflection"), we apply Ochiai to sort the methods during the segmentation
    process. Ochiai is unsupervised and is efficient to compute. We hypothesize that
    providing the LLM with methods sorted by their suspiciousness score will lead
    to more effective fault localization, as the model will focus on the most likely
    faulty candidates earlier in the process.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM4FL[Ochiai]: 如[3.1节](https://arxiv.org/html/2409.13642v1#S3.SS1 "3.1\. 使用分治法分析覆盖方法
    ‣ 3\. 方法论 ‣ 通过LLM代理和自我反思增强故障定位")中所讨论，我们在分段过程中应用了Ochiai对方法进行排序。Ochiai是无监督的，并且计算效率高。我们假设，向LLM提供按可疑度得分排序的方法将有助于更有效的故障定位，因为模型将在过程中更早地关注最可能出错的候选方法。'
- en: 'LLM4FL[DepGraph]: This approach uses the ranking produced by DepGraph, a state-of-the-art
    Graph Neural Network (GNN)-based fault localization technique (Li et al., [2015](https://arxiv.org/html/2409.13642v1#bib.bib35);
    Rafi et al., [2024](https://arxiv.org/html/2409.13642v1#bib.bib50)), to sort the
    methods during the segmentation process. DepGraph ranks methods based on structural
    code dependencies and code change history. As shown in RQ1, DepGraph shows the
    highest fault localization accuracy among all the techniques, surpassing LLM4FL[Ochiai].
    By examining the fault localization result after sorting the methods using DepGraph’s
    scores, we can better study if the initial order affects LLM’s results, even though
    LLM eventually visits all the methods.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[DepGraph]：该方法使用由DepGraph生成的排名，DepGraph是一种基于图神经网络（GNN）的先进故障定位技术（Li等人，[2015](https://arxiv.org/html/2409.13642v1#bib.bib35);
    Rafi等人，[2024](https://arxiv.org/html/2409.13642v1#bib.bib50)），在分段过程中对方法进行排序。DepGraph基于结构性代码依赖关系和代码变更历史对方法进行排名。如RQ1所示，DepGraph在所有技术中显示了最高的故障定位准确性，超越了LLM4FL[Ochiai]。通过检查使用DepGraph分数排序方法后的故障定位结果，我们可以更好地研究初始顺序是否影响LLM的结果，即使LLM最终会遍历所有方法。
- en: '![Refer to caption](img/53a4b11dfaaad4e1b687898b135b488e.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/53a4b11dfaaad4e1b687898b135b488e.png)'
- en: Figure 4\. Fault localization results when using different method sorting strategies
    during the segmentation process.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图4。使用不同方法排序策略进行分段过程时的故障定位结果。
- en: 'Results. Method ordering has a significant impact on LLM’s fault localization
    result, with up to 22% difference in Top-1 (from 299 to 366). Figure [4](https://arxiv.org/html/2409.13642v1#S4.F4
    "Figure 4 ‣ 4.3\. RQ3: Does order matter in the initial list of methods provided
    to the LLM? ‣ 4\. STUDY DESIGN AND RESULTS ‣ Enhancing Fault Localization Through
    Ordered Code Analysis with LLM Agents and Self-Reflection") shows the fault localization
    results when using different sorting strategies. When methods were presented in
    an execution-based order, LLM4FL[Execution] achieved a Top-1 score of 299, gradually
    increasing to 402 for Top-3, 431 for Top-5, and reaching 462 for Top-10\. This
    performance establishes a baseline, showing how the LLM behaves without strategic
    ordering. However, sorting methods with the lightweight Ochiai scores resulted
    in noticeable improvements. LLM4FL[Ochiai] improved the Top-1 score to 323, an
    8% increase over LLM4FL[Execution]. The improvements also extended to other metrics,
    with the Top-3 score increasing to 421, the Top-5 reaching 469, and the Top-10
    rising to 490.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '结果。方法排序对LLM的故障定位结果有显著影响，Top-1的差异可达到22%（从299到366）。图[4](https://arxiv.org/html/2409.13642v1#S4.F4
    "图 4 ‣ 4.3\. RQ3: 提供给LLM的初始方法列表中的顺序是否重要？ ‣ 4\. 研究设计与结果 ‣ 通过有序代码分析与LLM代理和自我反思增强故障定位")显示了使用不同排序策略时的故障定位结果。当方法按执行顺序呈现时，LLM4FL[Execution]的Top-1得分为299，Top-3逐渐提高到402，Top-5为431，Top-10达到462。这一表现为基准，展示了LLM在没有战略排序的情况下的表现。然而，使用轻量级Ochiai分数对方法进行排序则取得了显著改进。LLM4FL[Ochiai]将Top-1得分提高到了323，比LLM4FL[Execution]提高了8%。这一改进也扩展到了其他指标，Top-3得分提高到421，Top-5达到了469，Top-10升至490。'
- en: LLM4FL[DepGraph] provides further improvement to the already-promising result
    of DepGraph, indicating method ordering is critical to LLM4FL, or LLM-based fault
    localization in general. LLM4FL[DepGraph] achieved the highest Top-1 score of
    366, which is significantly outperforming both LLM4FL[Execution] and LLM4FL[Ochiai].
    The improvement was consistent across all the metrics. For Top-3, the score rose
    to 492, providing a substantial boost over LLM4FL[Ochiai]. Similarly, LLM4FL[DepGraph]
    excelled in the Top-5 and Top-10 categories, reaching 543 and 548, respectively.
    We also see that LLM4FL[DepGraph] has better Top-1, 3, and 5 scores compared to
    DepGraph. This consistent improvement underscores the importance of method ordering
    in enhancing the accuracy of LLM-based fault localization. This underscores the
    importance of method ordering for LLM-based fault localization, showing that lightweight
    techniques like Ochiai can achieve substantial performance gains without the heavy
    computational burden. Striking this balance between accuracy and efficiency makes
    these approaches particularly suitable for a wide range of fault localization
    tasks. However, it is important to recognize that LLM4FL[DepGraph] is computationally
    expensive due to its reliance on graph neural networks (GNNs). While LLM4FL[DepGraph]
    delivers top-tier results, LLM4FL[Ochiai] also offers a strong, efficient alternative,
    delivering good localization accuracy at a fraction of the computational cost
    due to the unsupervised nature of Ochiai. In other words, LLM4FL[Ochiai] may be
    more easily adapted in practice.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: LLM4FL[DepGraph]对已经具有前景的 DepGraph 结果进行了进一步改进，表明方法排序对 LLM4FL，或一般的基于 LLM 的故障定位至关重要。LLM4FL[DepGraph]达到了最高的
    Top-1 分数 366，显著优于 LLM4FL[Execution] 和 LLM4FL[Ochiai]。这一改进在所有度量标准中都是一致的。在 Top-3
    中，分数上升到 492，显著高于 LLM4FL[Ochiai]。类似地，LLM4FL[DepGraph]在 Top-5 和 Top-10 类别中表现优异，分别达到了
    543 和 548。我们还看到，LLM4FL[DepGraph]在 Top-1、3 和 5 的得分上优于 DepGraph。这一持续的改进强调了方法排序在提高基于
    LLM 的故障定位准确性中的重要性。它强调了方法排序对基于 LLM 的故障定位的重要性，表明像 Ochiai 这样的轻量级技术能够在不增加过大计算负担的情况下实现显著的性能提升。在准确性与效率之间取得这种平衡，使得这些方法特别适合广泛的故障定位任务。然而，值得注意的是，由于依赖于图神经网络（GNNs），LLM4FL[DepGraph]的计算成本较高。尽管
    LLM4FL[DepGraph]提供了顶级的结果，但 LLM4FL[Ochiai]也提供了一个强大的高效替代方案，由于 Ochiai 的无监督特性，它能够在较低的计算成本下提供良好的定位准确性。换句话说，LLM4FL[Ochiai]在实践中可能更容易适应。
- en: Nevertheless, our finding establishes a new research direction for LLM-based
    fault localization. It demonstrates that intelligent method ordering strategies
    significantly impact the result of LLM-based fault localization. This approach
    opens up further opportunities for optimizing LLM-based fault localization by
    exploring more advanced ordering techniques and how different premises of ordering
    affect other software engineering tasks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们的发现为基于 LLM 的故障定位开辟了新的研究方向。它表明，智能方法排序策略对基于 LLM 的故障定位结果有显著影响。这一方法为通过探索更先进的排序技术以及排序的不同前提如何影响其他软件工程任务，进一步优化基于
    LLM 的故障定位提供了机会。
- en: <svg class="ltx_picture" height="101.6" id="S4.SS3.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,101.6) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="78.72" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">The findings highlight that
    method ordering plays a crucial role in improving the performance of LLM-based
    fault localization, with a difference of up to 22% in Top-1 scores. While LLM4FL[DepGraph]
    delivers the best results, the lightweight LLM4FL[Ochiai] offers a highly efficient
    alternative, providing significant accuracy gains with far lower computational
    costs, making it more practical for real-world adoption.</foreignobject></g></g></svg>
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="101.6" id="S4.SS3.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,101.6) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 11.44 11.44)"><foreignobject color="#000000" height="78.72" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="577.12">The findings highlight that
    method ordering plays a crucial role in improving the performance of LLM-based
    fault localization, with a difference of up to 22% in Top-1 scores. While LLM4FL[DepGraph]
    delivers the best results, the lightweight LLM4FL[Ochiai] offers a highly efficient
    alternative, providing significant accuracy gains with far lower computational
    costs, making it more practical for real-world adoption.</foreignobject></g></g></svg>
- en: 5\. Threats To Validity
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 效度威胁
- en: Internal Validity. A potential threat to internal validity is the risk of data
    leakage in large language models (LLMs), where the model might have been exposed
    to data similar to the benchmark or specific fault localization cases during training.
    This could result in artificially inflated performance as the model may have prior
    knowledge of the evaluation data. Nevertheless, we mitigate this risk by preventing
    any content from being entered into ChatGPT that is related to the project name,
    the human-written bug report, or the bug ID.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 内部效度。内部效度的潜在威胁是大规模语言模型（LLMs）中数据泄漏的风险，即模型在训练过程中可能接触到与基准测试或特定故障定位案例相似的数据。这可能导致模型性能被人为地提高，因为模型可能对评估数据已有先验知识。尽管如此，我们通过防止任何与项目名称、人工编写的错误报告或错误
    ID 相关的内容输入到 ChatGPT 中来减轻这一风险。
- en: External Validity. The primary threat to external validity is the generalizability
    of our results. Our evaluation is based on Defects4J, a well-established dataset
    in the software engineering community. Although this dataset includes real-world
    bugs, the systems studied are primarily Java-based, and it remains uncertain whether
    our findings will generalize to other programming languages or domains.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 外部效度。外部效度的主要威胁是我们结果的泛化性。我们的评估基于 Defects4J，这是一个在软件工程界广为人知的数据集。尽管该数据集包含了真实世界的
    bug，但研究的系统主要是基于 Java 的，因此尚不确定我们的发现是否能够泛化到其他编程语言或领域。
- en: Construct Validity. Construct validity relates to whether the metrics we used
    accurately measure the performance of fault localization techniques. We used widely
    accepted Top-N metrics, which are commonly utilized in prior fault localization
    studies. However, our results are based on the assumption that developers primarily
    focus on the top-ranked faulty methods. Although this assumption aligns with previous
    research, different development practices could influence the effectiveness of
    our approach.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 构造效度。构造效度涉及我们使用的度量标准是否准确衡量了故障定位技术的性能。我们使用了广泛接受的 Top-N 度量标准，这些度量标准在以往的故障定位研究中得到了普遍应用。然而，我们的结果基于一个假设，即开发人员主要关注排名靠前的故障方法。虽然这一假设与以往的研究一致，但不同的开发实践可能会影响我们方法的有效性。
- en: 6\. Conclusion
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, we introduced LLM4FL, an LLM-agent-based fault localization technique
    that addresses the challenges of traditional fault localization methods and the
    limitations of current LLM-based approaches. By combining SBFL rankings with a
    divide-and-conquer strategy, LLM4FL groups large coverage data into manageable
    pieces, enabling effective analysis within the token limitations of LLMs. The
    use of multiple agents, prompt chaining, and self-reflection techniques allows
    for iterative refinement of fault localization, while chain-of-thought reasoning
    further enhances accuracy by generating potential fixes and re-rank the methods.
    Our evaluation on the Defects4J (V2.0.0) benchmark demonstrated that LLM4FL outperforms
    existing LLM-based techniques, achieving 19.27% higher Top-1 accuracy compared
    to AutoFL, and surpasses supervised approaches such as DeepFL and Grace without
    task-specific training. We also found that key components like coverage splitting
    and prompt chaining are essential to LLM4FL’s success, with method ordering playing
    a significant role in performance, improving Top-1 accuracy by up to 22%. Overall,
    LLM4FL presents a scalable and efficient solution for fault localization, providing
    improved accuracy and reducing computational costs, making it practical for real-world
    software projects. Future work will explore expanding LLM4FL’s capabilities for
    larger and more diverse codebases, further refining the agent collaboration and
    reasoning mechanisms.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 LLM4FL，这是一种基于 LLM 代理的故障定位技术，旨在解决传统故障定位方法的挑战以及当前基于 LLM 的方法的局限性。通过将
    SBFL 排名与分治策略相结合，LLM4FL 将大规模覆盖数据分成可管理的小块，使得在 LLM 的令牌限制内进行有效分析成为可能。使用多个代理、提示链和自我反思技术可以实现故障定位的迭代优化，而链式思维推理通过生成潜在的修复方案并重新排序方法进一步提高了准确性。我们在
    Defects4J（V2.0.0）基准测试上的评估表明，LLM4FL 超越了现有的基于LLM的方法，Top-1 精度比 AutoFL 高出 19.27%，并且超越了
    DeepFL 和 Grace 等无任务特定训练的监督方法。我们还发现，覆盖拆分和提示链等关键组件对 LLM4FL 的成功至关重要，方法排序在性能中起着重要作用，能够将
    Top-1 精度提高最多 22%。总的来说，LLM4FL 提供了一个可扩展且高效的故障定位解决方案，提供了更高的准确性并降低了计算成本，使其在实际软件项目中具有可行性。未来的工作将探索扩展
    LLM4FL 在更大和更多样化代码库中的能力，进一步优化代理协作和推理机制。
- en: References
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Abedu et al. (2024) Samuel Abedu, Ahmad Abdellatif, and Emad Shihab. 2024.
    LLM-Based Chatbots for Mining Software Repositories: Challenges and Opportunities.
    In *Proceedings of the 28th International Conference on Evaluation and Assessment
    in Software Engineering*. 201–210.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abedu 等人（2024） Samuel Abedu、Ahmad Abdellatif 和 Emad Shihab。2024年。基于LLM的聊天机器人用于挖掘软件库：挑战与机遇。发表于
    *第28届国际软件工程评估与评估会议论文集*。201–210。
- en: Abreu et al. (2006) Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2006.
    An evaluation of similarity coefficients for software fault localization. In *2006
    12th Pacific Rim International Symposium on Dependable Computing (PRDC’06)*. IEEE,
    39–46.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abreu 等人（2006） Rui Abreu、Peter Zoeteweij 和 Arjan JC Van Gemund。2006年。软件故障定位相似度系数的评估。发表于
    *2006年第12届太平洋地区国际可靠计算研讨会（PRDC’06）*。IEEE，39–46。
- en: Abreu et al. (2009) Rui Abreu, Peter Zoeteweij, and Arjan JC Van Gemund. 2009.
    Spectrum-based multiple fault localization. In *2009 IEEE/ACM International Conference
    on Automated Software Engineering*. IEEE, 88–99.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abreu 等人（2009） Rui Abreu、Peter Zoeteweij 和 Arjan JC Van Gemund。2009年。基于谱的多重故障定位。发表于
    *2009 IEEE/ACM 国际自动化软件工程会议*。IEEE，88–99。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*
    (2023).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人. 2023. Gpt-4 技术报告. *arXiv 预印本 arXiv:2303.08774*（2023）。
- en: Alaboudi and LaToza (2021) Abdulaziz Alaboudi and Thomas D LaToza. 2021. An
    exploratory study of debugging episodes. *arXiv preprint arXiv:2105.02162* (2021).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alaboudi 和 LaToza（2021）Abdulaziz Alaboudi 和 Thomas D LaToza. 2021. 一项关于调试过程的探索性研究.
    *arXiv 预印本 arXiv:2105.02162*（2021）。
- en: Albawi et al. (2017) Saad Albawi, Tareq Abed Mohammed, and Saad Al-Zawi. 2017.
    Understanding of a convolutional neural network. In *2017 international conference
    on engineering and technology (ICET)*. Ieee, 1–6.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Albawi 等人（2017）Saad Albawi, Tareq Abed Mohammed, 和 Saad Al-Zawi. 2017. 卷积神经网络的理解.
    在 *2017年国际工程与技术会议（ICET）* 中，IEEE，1–6。
- en: B. Le et al. (2016) Tien-Duy B. Le, David Lo, Claire Le Goues, and Lars Grunske.
    2016. A learning-to-rank based fault localization approach using likely invariants.
    In *Proceedings of the 25th international symposium on software testing and analysis*.
    177–188.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B. Le 等人（2016）Tien-Duy B. Le, David Lo, Claire Le Goues, 和 Lars Grunske. 2016.
    基于学习排序的故障定位方法，使用可能的不变式. 在 *第25届国际软件测试与分析研讨会论文集* 中，177–188。
- en: Bao et al. (2024) Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, and
    Yue Zhang. 2024. Llms with chain-of-thought are non-causal reasoners. *arXiv preprint
    arXiv:2402.16048* (2024).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等人（2024）Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, 和 Yue Zhang.
    2024. 具有思维链的大型语言模型是非因果推理者. *arXiv 预印本 arXiv:2402.16048*（2024）。
- en: 'Benton et al. (2020) Samuel Benton, Xia Li, Yiling Lou, and Lingming Zhang.
    2020. On the effectiveness of unified debugging: An extensive study on 16 program
    repair systems. In *Proceedings of the 35th IEEE/ACM International Conference
    on Automated Software Engineering*. 907–918.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Benton 等人（2020）Samuel Benton, Xia Li, Yiling Lou, 和 Lingming Zhang. 2020. 统一调试的有效性：对16个程序修复系统的广泛研究.
    在 *第35届 IEEE/ACM 国际自动化软件工程会议论文集* 中，907–918。
- en: Bin Murtaza et al. (2024) Sardar Bin Murtaza, Aidan Mccoy, Zhiyuan Ren, Aidan
    Murphy, and Wolfgang Banzhaf. 2024. LLM Fault Localisation within Evolutionary
    Computation Based Automated Program Repair. In *Proceedings of the Genetic and
    Evolutionary Computation Conference Companion*. 1824–1829.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bin Murtaza 等人（2024）Sardar Bin Murtaza, Aidan Mccoy, Zhiyuan Ren, Aidan Murphy,
    和 Wolfgang Banzhaf. 2024. 基于进化计算的自动程序修复中的 LLM 故障定位. 在 *遗传与进化计算会议附录* 中，1824–1829。
- en: Böhme et al. (2017) Marcel Böhme, Ezekiel O Soremekun, Sudipta Chattopadhyay,
    Emamurho Ugherughe, and Andreas Zeller. 2017. Where is the bug and how is it fixed?
    an experiment with practitioners. In *Proceedings of the 2017 11th joint meeting
    on foundations of software engineering*. 117–128.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Böhme 等人（2017）Marcel Böhme, Ezekiel O Soremekun, Sudipta Chattopadhyay, Emamurho
    Ugherughe, 和 Andreas Zeller. 2017. 错误在哪里，如何修复？与从业人员的实验. 在 *2017年第11届软件工程基础联合会议论文集*
    中，117–128。
- en: Brown (2020) Tom B Brown. 2020. Language models are few-shot learners. *arXiv
    preprint arXiv:2005.14165* (2020).
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown（2020）Tom B Brown. 2020. 语言模型是少量样本学习者. *arXiv 预印本 arXiv:2005.14165*（2020）。
- en: 'Campos et al. (2012) José Campos, André Riboira, Alexandre Perez, and Rui Abreu.
    2012. Gzoltar: an eclipse plug-in for testing and debugging. In *Proceedings of
    the 27th IEEE/ACM international conference on automated software engineering*.
    378–381.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Campos 等人（2012）José Campos, André Riboira, Alexandre Perez, 和 Rui Abreu. 2012.
    Gzoltar: 一个用于测试和调试的 Eclipse 插件. 在 *第27届 IEEE/ACM 国际自动化软件工程会议论文集* 中，378–381。'
- en: Chen et al. (2022) An Ran Chen, Tse-Hsun Chen, and Junjie Chen. 2022. How Useful
    is Code Change Information for Fault Localization in Continuous Integration?.
    In *Proceedings of the 37th IEEE/ACM International Conference on Automated Software
    Engineering*. 1–12.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2022）An Ran Chen, Tse-Hsun Chen, 和 Junjie Chen. 2022. 在持续集成中，代码变化信息对故障定位的帮助有多大？在
    *第37届 IEEE/ACM 国际自动化软件工程会议论文集* 中，1–12。
- en: Chen et al. (2024) Xinyun Chen, Ryan A Chi, Xuezhi Wang, and Denny Zhou. 2024.
    Premise Order Matters in Reasoning with Large Language Models. *arXiv preprint
    arXiv:2402.08939* (2024).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024）Xinyun Chen, Ryan A Chi, Xuezhi Wang, 和 Denny Zhou. 2024. 推理中前提顺序的重要性与大语言模型的关系.
    *arXiv 预印本 arXiv:2402.08939*（2024）。
- en: Cui et al. (2020) Zhanqi Cui, Minghua Jia, Xiang Chen, Liwei Zheng, and Xiulei
    Liu. 2020. Improving software fault localization by combining spectrum and mutation.
    *IEEE Access* 8 (2020), 172296–172307.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 等人（2020）Zhanqi Cui, Minghua Jia, Xiang Chen, Liwei Zheng, 和 Xiulei Liu.
    2020. 通过结合谱分析与变异改进软件故障定位. *IEEE Access* 8（2020），172296–172307。
- en: 'Dutta and Godboley (2021) Arpita Dutta and Sangharatna Godboley. 2021. Msfl:
    A model for fault localization using mutation-spectra technique. In *Lean and
    Agile Software Development: 5th International Conference, LASD 2021, Virtual Event,
    January 23, 2021, Proceedings 5*. Springer, 156–173.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dutta 和 Godboley (2021) Arpita Dutta 和 Sangharatna Godboley. 2021. Msfl: 一种使用变异谱技术进行故障定位的模型。在*精益与敏捷软件开发：第五届国际会议
    LASD 2021，虚拟会议，2021年1月23日，会议论文集 5*。Springer，156–173。'
- en: 'Hadi et al. (2023) Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad
    Irfan, Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili,
    et al. 2023. Large language models: a comprehensive survey of its applications,
    challenges, limitations, and future prospects. *Authorea Preprints* (2023).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadi 等人 (2023) Muhammad Usman Hadi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan,
    Anas Zafar, Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili,
    等人. 2023. 大型语言模型：其应用、挑战、局限性与未来前景的综合调查。*Authorea 预印本* (2023)。
- en: Hait and Tassey (2002) C. Hait and G. Tassey. 2002. *The Economic Impacts of
    Inadequate Infrastructure for Software Testing*. DIANE Publishing Company.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hait 和 Tassey (2002) C. Hait 和 G. Tassey. 2002. *软件测试基础设施不足的经济影响*。DIANE Publishing
    Company。
- en: 'Hong et al. (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    2024. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In
    *The Twelfth International Conference on Learning Representations*. [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o)'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hong 等人 (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng
    Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang
    Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, 和 Jürgen Schmidhuber. 2024. MetaGPT:
    面向多代理协作框架的元编程。在*第十二届国际学习表征会议*。 [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o)'
- en: 'Hou et al. (2023) Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang,
    Li Li, Xiapu Luo, David Lo, John Grundy, and Haoyu Wang. 2023. Large language
    models for software engineering: A systematic literature review. *arXiv preprint
    arXiv:2308.10620* (2023).'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou 等人 (2023) Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li,
    Xiapu Luo, David Lo, John Grundy, 和 Haoyu Wang. 2023. 大型语言模型在软件工程中的应用：一项系统文献综述。*arXiv
    预印本 arXiv:2308.10620* (2023)。
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.
    2023. A survey on hallucination in large language models: Principles, taxonomy,
    challenges, and open questions. *arXiv preprint arXiv:2311.05232* (2023).'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng,
    Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, 等人. 2023.
    大型语言模型中的幻觉调查：原理、分类、挑战与未解之谜。*arXiv 预印本 arXiv:2311.05232* (2023)。
- en: Jones et al. (2002) James A Jones, Mary Jean Harrold, and John Stasko. 2002.
    Visualization of test information to assist fault localization. In *Proceedings
    of the 24th international conference on Software engineering*. 467–477.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones 等人 (2002) James A Jones, Mary Jean Harrold, 和 John Stasko. 2002. 测试信息的可视化以辅助故障定位。在*第24届国际软件工程会议论文集*。467–477。
- en: 'Just et al. (2014) René Just, Darioush Jalali, and Michael D Ernst. 2014. Defects4J:
    A database of existing faults to enable controlled testing studies for Java programs.
    In *Proceedings of the 2014 international symposium on software testing and analysis*.
    437–440.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Just 等人 (2014) René Just, Darioush Jalali, 和 Michael D Ernst. 2014. Defects4J：一个现有故障的数据库，用于启用Java程序的控制测试研究。在*2014年国际软件测试与分析研讨会论文集*。437–440。
- en: Kang et al. (2024) Sungmin Kang, Gabin An, and Shin Yoo. 2024. A Quantitative
    and Qualitative Evaluation of LLM-Based Explainable Fault Localization. *Proc.
    ACM Softw. Eng.* 1, FSE, Article 64 (jul 2024), 23 pages. [https://doi.org/10.1145/3660771](https://doi.org/10.1145/3660771)
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等人 (2024) Sungmin Kang, Gabin An, 和 Shin Yoo. 2024. 基于大型语言模型的可解释故障定位的定量与定性评估。*Proc.
    ACM Softw. Eng.* 1, FSE, Article 64 (2024年7月)，23页。 [https://doi.org/10.1145/3660771](https://doi.org/10.1145/3660771)
- en: Kochhar et al. (2016) Pavneet Singh Kochhar, Xin Xia, David Lo, and Shanping
    Li. 2016. Practitioners’ expectations on automated fault localization. In *Proceedings
    of the 25th international symposium on software testing and analysis*. 165–176.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kochhar 等人 (2016) Pavneet Singh Kochhar, Xin Xia, David Lo, 和 Shanping Li. 2016.
    从业者对自动化故障定位的期望。在*第25届国际软件测试与分析研讨会论文集*。165–176。
- en: 'Langchain (2024a) Langchain. 2024a. Langchain Documentation: Overview. [https://python.langchain.com/v0.2/docs/versions/overview/](https://python.langchain.com/v0.2/docs/versions/overview/)
    Accessed: 2024-09-04.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain (2024a) Langchain. 2024a. Langchain 文档：概述。 [https://python.langchain.com/v0.2/docs/versions/overview/](https://python.langchain.com/v0.2/docs/versions/overview/)
    访问时间：2024-09-04。
- en: 'Langchain (2024b) Langchain. 2024b. LangGraph: Graph-Based Extensions for LangChain.
    [https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)
    Accessed: 2024-09-04.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langchain (2024b) Langchain. 2024b. LangGraph：LangChain 的图形扩展。 [https://langchain-ai.github.io/langgraph/](https://langchain-ai.github.io/langgraph/)
    访问时间：2024-09-04。
- en: Le et al. (2013) Tien-Duy B Le, Ferdian Thung, and David Lo. 2013. Theory and
    practice, do they match? a case with spectrum-based fault localization. In *2013
    IEEE International Conference on Software Maintenance*. IEEE, 380–383.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le 等人 (2013) Tien-Duy B Le, Ferdian Thung, 和 David Lo. 2013. 理论与实践，它们匹配吗？以基于谱的故障定位为例。*2013
    IEEE 国际软件维护会议*。IEEE，380–383。
- en: 'Levy et al. (2024) Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024. Same task,
    more tokens: the impact of input length on the reasoning performance of large
    language models. *arXiv preprint arXiv:2402.14848* (2024).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy 等人 (2024) Mosh Levy, Alon Jacoby, 和 Yoav Goldberg. 2024. 相同任务，更多的标记：输入长度对大型语言模型推理性能的影响。*arXiv
    预印本 arXiv:2402.14848* (2024)。
- en: Li et al. (2023) Jierui Li, Szymon Tworkowski, Yingying Wu, and Raymond Mooney.
    2023. Explaining competitive-level programming solutions using llms. *arXiv preprint
    arXiv:2307.05337* (2023).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Jierui Li, Szymon Tworkowski, Yingying Wu, 和 Raymond Mooney. 2023.
    使用 LLM 解释竞赛级编程解决方案。*arXiv 预印本 arXiv:2307.05337* (2023)。
- en: 'Li et al. (2019) Xia Li, Wei Li, Yuqun Zhang, and Lingming Zhang. 2019. Deepfl:
    Integrating multiple fault diagnosis dimensions for deep fault localization. In
    *Proceedings of the 28th ACM SIGSOFT international symposium on software testing
    and analysis*. 169–180.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2019) Xia Li, Wei Li, Yuqun Zhang, 和 Lingming Zhang. 2019. Deepfl: 整合多维故障诊断以实现深度故障定位。*第28届
    ACM SIGSOFT 国际软件测试与分析研讨会论文集*，169–180。'
- en: Li and Zhang (2017) Xia Li and Lingming Zhang. 2017. Transforming programs and
    tests in tandem for fault localization. *Proceedings of the ACM on Programming
    Languages* 1, OOPSLA (2017), 1–30.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Zhang (2017) Xia Li 和 Lingming Zhang. 2017. 同步转换程序和测试以进行故障定位。*ACM 编程语言会议论文集*
    1，OOPSLA (2017)，1–30。
- en: Li et al. (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
    2015. Gated graph sequence neural networks. *arXiv preprint arXiv:1511.05493*
    (2015).
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2015) Yujia Li, Daniel Tarlow, Marc Brockschmidt, 和 Richard Zemel. 2015.
    门控图序列神经网络。*arXiv 预印本 arXiv:1511.05493* (2015)。
- en: Li et al. (2021) Yi Li, Shaohua Wang, and Tien Nguyen. 2021. Fault localization
    with code coverage representation learning. In *2021 IEEE/ACM 43rd International
    Conference on Software Engineering (ICSE)*. IEEE, 661–673.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2021) Yi Li, Shaohua Wang, 和 Tien Nguyen. 2021. 基于代码覆盖率表示学习的故障定位。*2021
    IEEE/ACM 第43届国际软件工程会议 (ICSE)*。IEEE，661–673。
- en: Lin et al. (2024) Feng Lin, Dong Jae Kim, et al. 2024. When llm-based code generation
    meets the software development process. *arXiv preprint arXiv:2403.15852* (2024).
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等人 (2024) Feng Lin, Dong Jae Kim 等人. 2024. 当基于 LLM 的代码生成遇到软件开发过程时。*arXiv
    预印本 arXiv:2403.15852* (2024)。
- en: 'Liu et al. (2024) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, and Percy Liang. 2024. Lost in the middle: How language
    models use long contexts. *Transactions of the Association for Computational Linguistics*
    12 (2024), 157–173.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2024) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 2024. 迷失在中间：语言模型如何使用长上下文。*计算语言学会会刊*
    12 (2024)，157–173。
- en: Lou et al. (2021) Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan
    Hao, Lu Zhang, and Lingming Zhang. 2021. Boosting coverage-based fault localization
    via graph-based representation learning. In *Proceedings of the 29th ACM Joint
    Meeting on European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering*. 664–676.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lou 等人 (2021) Yiling Lou, Qihao Zhu, Jinhao Dong, Xia Li, Zeyu Sun, Dan Hao,
    Lu Zhang, 和 Lingming Zhang. 2021. 通过基于图的表示学习提升基于覆盖率的故障定位。*第29届 ACM 欧洲软件工程会议联合会议及软件工程基础研讨会论文集*，664–676。
- en: 'Meta AI (2024) Meta AI. 2024. Meta AI Introduces LLaMA 3: Advancing Open Foundation
    Models. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/).
    Accessed: 2024-09-06.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta AI (2024) Meta AI. 2024. Meta AI 推出 LLaMA 3：推进开放基础模型。 [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)。访问时间：2024-09-06。
- en: 'Moon et al. (2014) Seokhyeon Moon, Yunho Kim, Moonzoo Kim, and Shin Yoo. 2014.
    Ask the mutants: Mutating faulty programs for fault localization. In *2014 IEEE
    Seventh International Conference on Software Testing, Verification and Validation*.
    IEEE, 153–162.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moon et al. (2014) Seokhyeon Moon, Yunho Kim, Moonzoo Kim, 和 Shin Yoo. 2014.
    问问突变体：通过突变有缺陷的程序进行故障定位. 在*2014 IEEE 第七届国际软件测试、验证与验证会议*。IEEE, 153–162.
- en: 'OpenAI (2024) OpenAI. 2024. GPT-4O Mini: Advancing Cost-Efficient Intelligence.
    [https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
    Accessed: 2024-09-04.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2024) OpenAI. 2024. GPT-4O Mini: 推动成本效益智能的进步. [https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/](https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/)
    访问时间：2024-09-04.'
- en: 'Papadakis and Le Traon (2015) Mike Papadakis and Yves Le Traon. 2015. Metallaxis-FL:
    mutation-based fault localization. *Software Testing, Verification and Reliability*
    25, 5-7 (2015), 605–628.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papadakis 和 Le Traon (2015) Mike Papadakis 和 Yves Le Traon. 2015. Metallaxis-FL:
    基于变异的故障定位. *软件测试、验证与可靠性* 25, 5-7 (2015), 605–628.'
- en: Parnin and Orso (2011) Chris Parnin and Alessandro Orso. 2011. Are automated
    debugging techniques actually helping programmers?. In *Proceedings of the 2011
    international symposium on software testing and analysis*. 199–209.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parnin 和 Orso (2011) Chris Parnin 和 Alessandro Orso. 2011. 自动化调试技术真的帮助程序员了吗？.
    在*2011年国际软件测试与分析研讨会论文集*。199–209.
- en: Pu et al. (2023) Xiao Pu, Mingqi Gao, and Xiaojun Wan. 2023. Summarization is
    (almost) dead. *arXiv preprint arXiv:2309.09558* (2023).
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pu et al. (2023) Xiao Pu, Mingqi Gao, 和 Xiaojun Wan. 2023. 摘要几乎已经“死”了. *arXiv
    预印本 arXiv:2309.09558* (2023).
- en: Qian et al. (2023a) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, and Maosong Sun. 2023a. Communicative agents for software
    development. *arXiv preprint arXiv:2307.07924* 6 (2023).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2023a) Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su,
    Juyuan Xu, Zhiyuan Liu, 和 Maosong Sun. 2023a. 用于软件开发的沟通代理. *arXiv 预印本 arXiv:2307.07924*
    6 (2023).
- en: 'Qian et al. (2023b) Jie Qian, Xiaolin Ju, and Xiang Chen. 2023b. GNet4FL: effective
    fault localization via graph convolutional neural network. *Automated Software
    Engineering* 30, 2 (2023), 16.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian et al. (2023b) Jie Qian, Xiaolin Ju, 和 Xiang Chen. 2023b. GNet4FL: 通过图卷积神经网络实现有效的故障定位.
    *自动化软件工程* 30, 2 (2023), 16.'
- en: 'Qian et al. (2021) Jie Qian, Xiaolin Ju, Xiang Chen, Hao Shen, and Yiheng Shen.
    2021. AGFL: a graph convolutional neural network-based method for fault localization.
    In *2021 IEEE 21st International Conference on Software Quality, Reliability and
    Security (QRS)*. IEEE, 672–680.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian et al. (2021) Jie Qian, Xiaolin Ju, Xiang Chen, Hao Shen, 和 Yiheng Shen.
    2021. AGFL: 一种基于图卷积神经网络的故障定位方法. 在*2021 IEEE 第21届国际软件质量、可靠性与安全会议 (QRS)*. IEEE,
    672–680.'
- en: 'Qin et al. (2024) Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin
    Wang, Xiaoling Li, and Xiaoguang Mao. 2024. AgentFL: Scaling LLM-based Fault Localization
    to Project-Level Context. *arXiv preprint arXiv:2403.16362* (2024).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2024) Yihao Qin, Shangwen Wang, Yiling Lou, Jinhao Dong, Kaixin
    Wang, Xiaoling Li, 和 Xiaoguang Mao. 2024. AgentFL: 将基于大型语言模型的故障定位扩展到项目级上下文. *arXiv
    预印本 arXiv:2403.16362* (2024).'
- en: Rafi et al. (2024) Md Nakhla Rafi, Dong Jae Kim, An Ran Chen, Tse-Hsun (Peter)
    Chen, and Shaowei Wang. 2024. Towards Better Graph Neural Network-Based Fault
    Localization through Enhanced Code Representation. *Proc. ACM Softw. Eng.* 1,
    FSE, Article 86 (jul 2024), 23 pages. [https://doi.org/10.1145/3660793](https://doi.org/10.1145/3660793)
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafi et al. (2024) Md Nakhla Rafi, Dong Jae Kim, An Ran Chen, Tse-Hsun (Peter)
    Chen, 和 Shaowei Wang. 2024. 通过增强代码表示推动更好的图神经网络故障定位. *ACM 软件工程会议论文集* 1, FSE, 文章
    86 (2024年7月), 23 页. [https://doi.org/10.1145/3660793](https://doi.org/10.1145/3660793)
- en: 'Renze and Guven (2024) Matthew Renze and Erhan Guven. 2024. Self-Reflection
    in LLM Agents: Effects on Problem-Solving Performance. *arXiv preprint arXiv:2405.06682*
    (2024).'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Renze 和 Guven (2024) Matthew Renze 和 Erhan Guven. 2024. 大型语言模型代理中的自我反思：对问题解决能力的影响.
    *arXiv 预印本 arXiv:2405.06682* (2024).
- en: Roy et al. (2024) Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro
    Las-Casas, Rodrigo Fonseca, and Saravan Rajmohan. 2024. Exploring llm-based agents
    for root cause analysis. In *Companion Proceedings of the 32nd ACM International
    Conference on the Foundations of Software Engineering*. 208–219.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy et al. (2024) Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro
    Las-Casas, Rodrigo Fonseca, 和 Saravan Rajmohan. 2024. 探索基于大型语言模型的代理进行根本原因分析. 在*第32届
    ACM 国际软件工程基础会议论文集*。208–219.
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roziere 等人（2023）Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等.
    2023. Code llama：开放的代码基础模型。*arXiv 预印本 arXiv:2308.12950*（2023）。
- en: 'Shao et al. (2023) Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023.
    Character-llm: A trainable agent for role-playing. *arXiv preprint arXiv:2310.10158*
    (2023).'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人（2023）Yunfan Shao, Linyang Li, Junqi Dai, 和 Xipeng Qiu. 2023. Character-llm：一个可训练的角色扮演代理。*arXiv
    预印本 arXiv:2310.10158*（2023）。
- en: 'Sohn and Yoo (2017) Jeongju Sohn and Shin Yoo. 2017. Fluccs: Using code and
    change metrics to improve fault localization. In *Proceedings of the 26th ACM
    SIGSOFT International Symposium on Software Testing and Analysis*. 273–283.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn 和 Yoo（2017）Jeongju Sohn 和 Shin Yoo. 2017. Fluccs：使用代码和变化度量来改善故障定位。发表于*第26届
    ACM SIGSOFT 国际软件测试与分析研讨会论文集*，273–283。
- en: Tyen et al. (2024) Gladys Tyen, Hassan Mansoor, Victor Cărbune, Yuanzhu Peter
    Chen, and Tony Mak. 2024. LLMs cannot find reasoning errors, but can correct them
    given the error location. In *Findings of the Association for Computational Linguistics
    ACL 2024*. 13894–13908.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tyen 等人（2024）Gladys Tyen, Hassan Mansoor, Victor Cărbune, Yuanzhu Peter Chen,
    和 Tony Mak. 2024. LLM 无法发现推理错误，但能在给定错误位置的情况下纠正错误。发表于*计算语言学协会（ACL 2024）会议论文集*，13894–13908。
- en: Vancsics et al. (2021) Béla Vancsics, Ferenc Horváth, Attila Szatmári, and Arpád
    Beszédes. 2021. Call frequency-based fault localization. In *2021 IEEE International
    Conference on Software Analysis, Evolution and Reengineering (SANER)*. IEEE, 365–376.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vancsics 等人（2021）Béla Vancsics, Ferenc Horváth, Attila Szatmári, 和 Arpád Beszédes.
    2021. 基于调用频率的故障定位。发表于*2021 年 IEEE 国际软件分析、演化与重构会议（SANER）*，IEEE，365–376。
- en: 'Wang et al. (2024) Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan,
    Yubo Zhang, Zhixing Wang, Haijun Wang, and Ting Liu. 2024. Strategic Chain-of-Thought:
    Guiding Accurate Reasoning in LLMs through Strategy Elicitation. *arXiv preprint
    arXiv:2409.03271* (2024).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2024）Yu Wang, Shiwan Zhao, Zhihu Wang, Heyuan Huang, Ming Fan, Yubo
    Zhang, Zhixing Wang, Haijun Wang, 和 Ting Liu. 2024. 战略链式思维：通过战略引导提高 LLM 推理准确性。*arXiv
    预印本 arXiv:2409.03271*（2024）。
- en: Wen et al. (2019) Ming Wen, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao,
    Shi Han, and Shing-Chi Cheung. 2019. Historical spectrum based fault localization.
    *IEEE Transactions on Software Engineering* 47, 11 (2019), 2348–2368.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen 等人（2019）Ming Wen, Junjie Chen, Yongqiang Tian, Rongxin Wu, Dan Hao, Shi
    Han, 和 Shing-Chi Cheung. 2019. 基于历史谱系的故障定位。*IEEE 软件工程学报* 47, 11（2019），2348–2368。
- en: White et al. (2024) Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, and
    Douglas C Schmidt. 2024. Chatgpt prompt patterns for improving code quality, refactoring,
    requirements elicitation, and software design. In *Generative AI for Effective
    Software Development*. Springer, 71–108.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White 等人（2024）Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith, 和 Douglas
    C Schmidt. 2024. 改进代码质量、重构、需求提取和软件设计的 ChatGPT 提示模式。发表于*有效软件开发的生成性 AI*，Springer，71–108。
- en: Wong et al. (2013) W Eric Wong, Vidroha Debroy, Ruizhi Gao, and Yihao Li. 2013.
    The DStar method for effective software fault localization. *IEEE Transactions
    on Reliability* 63, 1 (2013), 290–308.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 等人（2013）W Eric Wong, Vidroha Debroy, Ruizhi Gao, 和 Yihao Li. 2013. DStar
    方法用于有效的软件故障定位。*IEEE 可靠性学报* 63, 1（2013），290–308。
- en: Wong et al. (2011) W Eric Wong, Vidroha Debroy, Richard Golden, Xiaofeng Xu,
    and Bhavani Thuraisingham. 2011. Effective software fault localization using an
    RBF neural network. *IEEE Transactions on Reliability* 61, 1 (2011), 149–169.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 等人（2011）W Eric Wong, Vidroha Debroy, Richard Golden, Xiaofeng Xu, 和 Bhavani
    Thuraisingham. 2011. 使用 RBF 神经网络的有效软件故障定位。*IEEE 可靠性学报* 61, 1（2011），149–169。
- en: Wong et al. (2016) W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, and Franz Wotawa.
    2016. A survey on software fault localization. *IEEE Transactions on Software
    Engineering* 42, 8 (2016), 707–740.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 等人（2016）W Eric Wong, Ruizhi Gao, Yihao Li, Rui Abreu, 和 Franz Wotawa. 2016.
    软件故障定位综述。*IEEE 软件工程学报* 42, 8（2016），707–740。
- en: Wong and Qi (2009) W Eric Wong and Yu Qi. 2009. BP neural network-based effective
    fault localization. *International Journal of Software Engineering and Knowledge
    Engineering* 19, 04 (2009), 573–597.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wong 和 Qi（2009）W Eric Wong 和 Yu Qi. 2009. 基于 BP 神经网络的有效故障定位。*国际软件工程与知识工程期刊*
    19, 04（2009），573–597。
- en: Wu et al. (2023) Yonghao Wu, Zheng Li, Jie M Zhang, Mike Papadakis, Mark Harman,
    and Yong Liu. 2023. Large language models in fault localisation. *arXiv preprint
    arXiv:2308.15276* (2023).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人（2023）Yonghao Wu, Zheng Li, Jie M Zhang, Mike Papadakis, Mark Harman, 和
    Yong Liu. 2023. 大型语言模型在故障定位中的应用。*arXiv 预印本 arXiv:2308.15276*（2023）。
- en: 'Xia et al. (2024) Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming
    Zhang. 2024. Agentless: Demystifying llm-based software engineering agents. *arXiv
    preprint arXiv:2407.01489* (2024).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等（2024）夏春秋 Steven、邓银琳、Soren Dunn 和 张凌铭。2024. Agentless: 揭开基于 LLM 的软件工程代理的神秘面纱。*arXiv
    预印本 arXiv:2407.01489*（2024）。'
- en: Xie et al. (2013) Xiaoyuan Xie, Tsong Yueh Chen, Fei-Ching Kuo, and Baowen Xu.
    2013. A theoretical analysis of the risk evaluation formulas for spectrum-based
    fault localization. *ACM Transactions on software engineering and methodology
    (TOSEM)* 22, 4 (2013), 1–40.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2013）谢晓远、Tsong Yueh Chen、郭飞庆 和 徐宝文。2013. 基于谱的故障定位风险评估公式的理论分析。*ACM 软件工程与方法学期刊（TOSEM）*
    22, 4（2013），1–40。
- en: Xie et al. (2016) Xiaoyuan Xie, Zicong Liu, Shuo Song, Zhenyu Chen, Jifeng Xuan,
    and Baowen Xu. 2016. Revisit of automatic debugging via human focus-tracking analysis.
    In *Proceedings of the 38th International Conference on Software Engineering*.
    808–819.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2016）谢晓远、刘子聪、宋硕、陈振宇、萧景锋 和 徐宝文。2016. 通过人类关注追踪分析重新审视自动调试。 在 *第38届国际软件工程会议论文集*。808–819。
- en: Xu et al. (2020a) Jiaxi Xu, Fei Wang, and Jun Ai. 2020a. Defect prediction with
    semantics and context features of codes based on graph representation learning.
    *IEEE Transactions on Reliability* 70, 2 (2020), 613–625.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020a）徐佳熙、王飞和艾俊。2020a。基于图表示学习的代码语义和上下文特征缺陷预测。*IEEE 可靠性学报* 70, 2（2020），613–625。
- en: 'Xu et al. (2020b) Xuezheng Xu, Changwei Zou, and Jingling Xue. 2020b. Every
    mutation should be rewarded: Boosting fault localization with mutated predicates.
    In *2020 IEEE International Conference on Software Maintenance and Evolution (ICSME)*.
    IEEE, 196–207.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2020b）徐雪政、邹长伟 和 薛景凌。2020b。每个变异都应该得到奖励：利用变异谓词提升故障定位。在 *2020 IEEE 国际软件维护与演化会议（ICSME）*。IEEE，196–207。
- en: 'Xu et al. (2023) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. 2023. Exploring large language models for communication
    games: An empirical study on werewolf. *arXiv preprint arXiv:2309.04658* (2023).'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2023）徐宇壮、王硕、李鹏、罗富文、王晓龙、刘卫东 和 刘杨。2023。探索大语言模型在交流游戏中的应用：狼人游戏的实证研究。*arXiv
    预印本 arXiv:2309.04658*（2023）。
- en: 'Xu et al. (2024) Ziwei Xu, Sanjay Jain, and Mohan Kankanhalli. 2024. Hallucination
    is inevitable: An innate limitation of large language models. *arXiv preprint
    arXiv:2401.11817* (2024).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等（2024）徐子伟、Sanjay Jain 和 Mohan Kankanhalli。2024。幻觉是不可避免的：大语言模型的固有局限性。*arXiv
    预印本 arXiv:2401.11817*（2024）。
- en: Yang et al. (2024) Aidan Z. H. Yang, Claire Le Goues, Ruben Martins, and Vincent
    Hellendoorn. 2024. Large Language Models for Test-Free Fault Localization. In
    *Proceedings of the IEEE/ACM 46th International Conference on Software Engineering*
    (Lisbon, Portugal) *(ICSE ’24)*. Association for Computing Machinery, New York,
    NY, USA, Article 17, 12 pages. [https://doi.org/10.1145/3597503.3623342](https://doi.org/10.1145/3597503.3623342)
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2024）Aidan Z. H. Yang、Claire Le Goues、Ruben Martins 和 Vincent Hellendoorn。2024。大语言模型在无测试故障定位中的应用。在
    *IEEE/ACM 第46届国际软件工程会议论文集*（葡萄牙里斯本）*(ICSE ’24)*。美国纽约计算机协会，文章17，12页。[https://doi.org/10.1145/3597503.3623342](https://doi.org/10.1145/3597503.3623342)
- en: Zhang et al. (2017) Mengshi Zhang, Xia Li, Lingming Zhang, and Sarfraz Khurshid.
    2017. Boosting spectrum-based fault localization using pagerank. In *Proceedings
    of the 26th ACM SIGSOFT international symposium on software testing and analysis*.
    261–272.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2017）张孟诗、李霞、张凌铭 和 Sarfraz Khurshid。2017。利用 Pagerank 提升基于谱的故障定位。在 *第26届
    ACM SIGSOFT 国际软件测试与分析研讨会论文集*。261–272。
- en: Zhang et al. (2019b) Mengshi Zhang, Yaoxian Li, Xia Li, Lingchao Chen, Yuqun
    Zhang, Lingming Zhang, and Sarfraz Khurshid. 2019b. An empirical study of boosting
    spectrum-based fault localization via pagerank. *IEEE Transactions on Software
    Engineering* 47, 6 (2019), 1089–1113.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2019b）张孟诗、李耀贤、李霞、陈凌超、张宇群、张凌铭 和 Sarfraz Khurshid。2019b. 通过 Pagerank 提升基于谱的故障定位的实证研究。*IEEE
    软件工程学报* 47, 6（2019），1089–1113。
- en: 'Zhang et al. (2019a) Zhuo Zhang, Yan Lei, Xiaoguang Mao, and Panpan Li. 2019a.
    CNN-FL: An effective approach for localizing faults using convolutional neural
    networks. In *2019 IEEE 26th International Conference on Software Analysis, Evolution
    and Reengineering (SANER)*. IEEE, 445–455.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2019a）张卓、雷岩、毛晓光 和 李盼盼。2019a. CNN-FL: 一种有效的利用卷积神经网络进行故障定位的方法。在 *2019
    IEEE 第26届软件分析、演化与重构国际会议（SANER）*。IEEE，445–455。'
- en: 'Zhou et al. (2023) Wangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan
    Wang, Zhenxin Xiao, Yifan Hou, Ryan Cotterell, and Mrinmaya Sachan. 2023. Recurrentgpt:
    Interactive generation of (arbitrarily) long text. *arXiv preprint arXiv:2305.13304*
    (2023).'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人（2023）Wangchunshu Zhou、Yuchen Eleanor Jiang、Peng Cui、Tiannan Wang、Zhenxin
    Xiao、Yifan Hou、Ryan Cotterell 和 Mrinmaya Sachan。2023年。Recurrentgpt：交互式生成（任意长度的）文本。*arXiv
    预印本 arXiv:2305.13304*（2023年）。
- en: Zou et al. (2019) Daming Zou, Jingjing Liang, Yingfei Xiong, Michael D Ernst,
    and Lu Zhang. 2019. An empirical study of fault localization families and their
    combinations. *IEEE Transactions on Software Engineering* 47, 2 (2019), 332–347.</foreignobject></g></g></svg>
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou 等人（2019）Daming Zou、Jingjing Liang、Yingfei Xiong、Michael D Ernst 和 Lu Zhang。2019年。关于故障定位家族及其组合的实证研究。*IEEE
    软件工程学报* 47卷，第2期（2019年），332-347。</foreignobject></g></g></svg>
