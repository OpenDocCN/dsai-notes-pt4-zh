- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: '<!--yml  '
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别：未分类  '
- en: 'date: 2025-01-11 12:09:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期：2025-01-11 12:09:24  '
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '-->  '
- en: Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '基于LLM的多代理可扩展与准确图推理  '
- en: 来源：[https://arxiv.org/html/2410.05130/](https://arxiv.org/html/2410.05130/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '来源：[https://arxiv.org/html/2410.05130/](https://arxiv.org/html/2410.05130/)  '
- en: Yuwei Hu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '胡宇伟  '
- en: Renmin University of China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '中国人民大学  '
- en: huyuweiyisui@ruc.edu.cn &Runlin Lei
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 'huyuweiyisui@ruc.edu.cn & 雷润林  '
- en: Renmin University of China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '中国人民大学  '
- en: runlin_lei@ruc.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'runlin_lei@ruc.edu.cn  '
- en: '&Xinyi Huang'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '& 黄欣怡  '
- en: Renmin University of China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '中国人民大学  '
- en: 2022201342@ruc.edu.cn
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '2022201342@ruc.edu.cn  '
- en: \ANDZhewei Wei
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '\AND魏哲伟  '
- en: Renmin University of China
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 中国人民大学
- en: zhewei@ruc.edu.cn
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'zhewei@ruc.edu.cn  '
- en: '&Yongchao Liu'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '& 刘永超  '
- en: Alibaba Group
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '阿里巴巴集团  '
- en: yongchao.ly@antgroup.com    Yuwei Hu¹, Runlin Lei¹, Xinyi Huang¹, Zhewei Wei¹,
    Yongchao Liu²
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 'yongchao.ly@antgroup.com    胡宇伟¹，雷润林¹，黄欣怡¹，魏哲伟¹，刘永超²  '
- en: ¹Renmin University of China, Beijing, China
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '¹中国人民大学，北京，中国  '
- en: ²Alibaba Group, Beijing, China
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '²阿里巴巴集团，北京，中国  '
- en: huyuweiyisui@ruc.edu.cn, runlin_lei@ruc.edu.cn, 2022201342@ruc.edu.cn
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 'huyuweiyisui@ruc.edu.cn，runlin_lei@ruc.edu.cn，2022201342@ruc.edu.cn  '
- en: zhewei@ruc.edu.cn, yongchao.ly@antgroup.com Corresponding Author.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 'zhewei@ruc.edu.cn，yongchao.ly@antgroup.com 通讯作者。  '
- en: Abstract
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '摘要  '
- en: Recent research has explored the use of Large Language Models (LLMs) for tackling
    complex graph reasoning tasks. However, due to the intricacies of graph structures
    and the inherent limitations of LLMs in handling long text, current approaches
    often fail to deliver satisfactory accuracy, even on small-scale graphs and simple
    tasks. To address these challenges, we introduce GraphAgent-Reasoner, a fine-tuning-free
    framework that utilizes a multi-agent collaboration strategy for explicit and
    precise graph reasoning. Inspired by distributed graph computation theory, our
    framework decomposes graph problems into smaller, node-centric tasks that are
    distributed among multiple agents. The agents collaborate to solve the overall
    problem, significantly reducing the amount of information and complexity handled
    by a single LLM, thus enhancing the accuracy of graph reasoning. By simply increasing
    the number of agents, GraphAgent-Reasoner can efficiently scale to accommodate
    larger graphs with over 1,000 nodes. Evaluated on the GraphInstruct dataset, our
    framework demonstrates near-perfect accuracy on polynomial-time graph reasoning
    tasks, significantly outperforming the best available models, both closed-source
    and fine-tuned open-source variants. Our framework also demonstrates the capability
    to handle real-world graph reasoning applications such as webpage importance analysis.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '近期的研究探讨了使用大型语言模型（LLM）来处理复杂的图推理任务。然而，由于图结构的复杂性以及LLM在处理长文本时的固有限制，当前的方法往往未能在小规模图和简单任务上取得令人满意的准确性。为了解决这些挑战，我们提出了GraphAgent-Reasoner，一个无需微调的框架，利用多代理协作策略进行明确且精确的图推理。我们的框架受到分布式图计算理论的启发，将图问题分解为更小的、以节点为中心的任务，并将这些任务分配给多个代理。各代理协作解决整体问题，显著减少了单一LLM需要处理的信息量和复杂度，从而提高了图推理的准确性。通过简单地增加代理的数量，GraphAgent-Reasoner能够高效地扩展，以适应超过1,000个节点的大型图。在GraphInstruct数据集上的评估表明，我们的框架在多项式时间图推理任务中表现出近乎完美的准确性，显著优于目前可用的最佳模型，无论是封闭源代码的模型还是微调后的开源变体。我们的框架还展示了处理真实世界图推理应用的能力，如网页重要性分析。  '
- en: 1 Introduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '1 引言  '
- en: Graphs, as a crucial data structure for modeling complex real-world relationships,
    are ubiquitous across various scenarios, e.g. citation networks, recommendation
    networks. Many important applications like drug discovery (Stokes et al., [2020](https://arxiv.org/html/2410.05130v1#bib.bib27)),
    traffic forecasting (Jiang & Luo, [2022](https://arxiv.org/html/2410.05130v1#bib.bib16)),
    and financial detection (Motie & Raahemi, [2024](https://arxiv.org/html/2410.05130v1#bib.bib23)),
    require reasoning over graphs to be realized. Noticing the powerful general knowledge
    and language processing capabilities of Large Language Models (LLMs) (Brown et al.,
    [2020](https://arxiv.org/html/2410.05130v1#bib.bib1)), a significant amount of
    works have focused on using LLMs to perform various reasoning tasks, such as mathematical
    formula derivation (Meadows et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib20)),
    commonsense reasoning (Madaan et al., [2022](https://arxiv.org/html/2410.05130v1#bib.bib18)),
    and multi-hop question answering (Creswell et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib7)).
    However, most of them primarily involve shallow or sequential reasoning. To bring
    the LLM reasoning closer to human thinking, it is necessary for LLMs to master
    deeper and more complex reasoning, such as graph reasoning.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图形作为建模复杂现实世界关系的关键数据结构，在各种场景中随处可见，例如引文网络、推荐网络。许多重要应用，如药物发现（Stokes等人，[2020](https://arxiv.org/html/2410.05130v1#bib.bib27)）、交通预测（Jiang
    & Luo，[2022](https://arxiv.org/html/2410.05130v1#bib.bib16)）和金融检测（Motie & Raahemi，[2024](https://arxiv.org/html/2410.05130v1#bib.bib23)），都需要在图形上进行推理才能实现。注意到大型语言模型（LLMs）具有强大的通用知识和语言处理能力（Brown等人，[2020](https://arxiv.org/html/2410.05130v1#bib.bib1)），大量研究开始聚焦于利用LLMs执行各种推理任务，如数学公式推导（Meadows等人，[2023](https://arxiv.org/html/2410.05130v1#bib.bib20)）、常识推理（Madaan等人，[2022](https://arxiv.org/html/2410.05130v1#bib.bib18)）和多跳问答（Creswell等人，[2023](https://arxiv.org/html/2410.05130v1#bib.bib7)）。然而，它们大多数主要涉及浅层或顺序推理。为了使LLM的推理更接近人类思维，LLM需要掌握更深层次和更复杂的推理，例如图形推理。
- en: 'Despite significant efforts by researchers to enable LLMs to memorize, comprehend,
    and perform basic reasoning on graph structures, several issues still persist:
    1) The scale of graphs that can be handled is limited. Describing graph structures
    in natural language inevitably leads to excessively long inputs. Due to context
    length limitations and the shortcomings of LLMs in handling lengthy text (Liu
    et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib17)), previous works (Chai
    et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib2); Fatemi et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib9);
    Perozzi et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib25)) could only
    handle graphs of very limited size (e.g. fewer than 20 nodes and 100 edges). 2)
    The performance on graph reasoning tasks is relatively poor. Unlike text, which
    can tolerate some degree of semantic deviation, reasoning and computation on graphs
    must be highly precise. However, current works demonstrate poor accuracy (average
    20$\sim$60%) in various graph reasoning tasks like connectivity and shortest path.
    3) Lacking explicit reasoning paths. Taking the shortest path as an example, the
    responses of existing models resemble a heuristic search approach to finding the
    shortest path on a graph, rather than strictly executing an algorithm. This makes
    it difficult to determine whether LLMs are genuinely deriving the answer through
    correct reasoning or merely making educated guesses. Although GraphWiz (Chen et al.,
    [2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)) attempts to generate explicit
    reasoning paths through fine-tuning, it often fails due to the presence of incomplete
    or wrong reasoning paths in its training data. Furthermore, GraphWiz exhibits
    overfitting, where it tends to treat new or unrelated questions as one of the
    fine-tuned problems, which will be detailed in Section [5.3](https://arxiv.org/html/2410.05130v1#S5.SS3
    "5.3 Experiment 3: Case Study ‣ 5 Experiments ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究人员为使LLM能够记忆、理解和进行基本的图形结构推理做出了巨大努力，但仍然存在一些问题：1）可处理的图形规模有限。用自然语言描述图形结构不可避免地会导致过长的输入。由于上下文长度的限制和LLM在处理冗长文本时的不足（Liu
    等人，[2023](https://arxiv.org/html/2410.05130v1#bib.bib17)），先前的研究（Chai 等人，[2023](https://arxiv.org/html/2410.05130v1#bib.bib2);
    Fatemi 等人，[2024](https://arxiv.org/html/2410.05130v1#bib.bib9); Perozzi 等人，[2024](https://arxiv.org/html/2410.05130v1#bib.bib25)）只能处理非常有限规模的图形（例如，少于20个节点和100条边）。2）在图形推理任务上的表现相对较差。与文本不同，文本可以容忍一定程度的语义偏差，而图形推理和计算必须非常精确。然而，现有工作在各种图形推理任务（如连通性和最短路径）中准确率较低（平均20$\sim$60%）。3）缺乏明确的推理路径。以最短路径为例，现有模型的响应更像是通过启发式搜索方法找到图形上的最短路径，而不是严格执行算法。这使得很难确定LLM是否真正通过正确的推理得出答案，还是仅仅在做出有根据的猜测。尽管GraphWiz（Chen
    等人，[2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)）试图通过微调生成明确的推理路径，但由于其训练数据中存在不完整或错误的推理路径，它往往会失败。此外，GraphWiz还表现出过拟合问题，它倾向于将新的或无关的问题视为其中一个微调过的问题，具体内容将在第[5.3](https://arxiv.org/html/2410.05130v1#S5.SS3
    "5.3 实验 3：案例研究 ‣ 5 实验 ‣ 基于LLM的多智能体可扩展且准确的图形推理")节中详细说明。
- en: Motivation. The ultimate goal of graph reasoning is to enable LLMs to leverage
    graph-related knowledge or algorithms to solve real-world graph problems. However,
    with the development of information science and hardware storage, the scale of
    graphs and information per node become too large for a single LLM to handle. To
    address this, a natural idea is to use distributed approaches, where a large graph
    is stored across multiple LLMs separately and compute collaboratively. Therefore,
    just as graph algorithms have generally evolved from non-distributed to distributed
    forms (Meng et al., [2024b](https://arxiv.org/html/2410.05130v1#bib.bib22))),
    we hope that LLMs can also learn the concept of distributed processing, thereby
    harnessing the power of swarm intelligence to solve graph problems in real-world
    scenarios.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 动机。图形推理的最终目标是使大型语言模型（LLMs）能够利用与图形相关的知识或算法来解决现实世界中的图形问题。然而，随着信息科学和硬件存储的发展，图形的规模和每个节点的信息变得过于庞大，单个LLM无法处理。为了解决这个问题，一个自然的想法是使用分布式方法，其中一个大型图形被分别存储在多个LLM中并协同计算。因此，正如图形算法通常从非分布式演变为分布式形式一样（Meng
    等人，[2024b](https://arxiv.org/html/2410.05130v1#bib.bib22)），我们希望LLM也能学习分布式处理的概念，从而利用群体智能的力量在现实场景中解决图形问题。
- en: '![Refer to caption](img/8b533b6211768f3072505ca9b27757e7.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明文字](img/8b533b6211768f3072505ca9b27757e7.png)'
- en: 'Figure 1: The current situation of LLMs in solving graph problems. Previous
    methods using a single LLM often failed due to the complex graph structures. In
    contrast, our approach leverages agents collaboration to effectively address graph
    problems.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：当前 LLM 在解决图问题中的应用现状。以往使用单一 LLM 的方法常常由于图的复杂结构而失败。相比之下，我们的方法利用代理协作有效地解决图问题。
- en: 'Our Contribution. To address the above limitations, in this paper, we propose
    the GraphAgent-Reasoner(GAR) framework, which leverages the power of swarm intelligence
    to solve graph reasoning problems, as shown in Figure [1](https://arxiv.org/html/2410.05130v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Scalable and Accurate Graph Reasoning with LLM-based
    Multi-Agents"). We follow a node-centric approach, assigning an agent to each
    node, allowing it to focus on processing its own information and communicate with
    neighbors. Thus, we can easily scale up the size of graphs that can be processed
    by simply increasing the number of agents. At the same time, under the direction
    of a Master LLM, graph problems are decomposed into smaller, node-centric tasks,
    which are assigned to agents for collaborative resolution. This approach significantly
    reduces the scale and complexity of information each agent needs to process, thereby
    greatly improving the overall accuracy. Furthermore, since agents must clearly
    transmit the processed information to neighboring agents, the reasoning process
    becomes transparent, demonstrating the framework solves graph reasoning problems
    through clear and correct reasoning, rather than lucky guessing. In summary, our
    contributions are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献。为了解决上述限制，本文提出了 GraphAgent-Reasoner（GAR）框架，利用群体智能的力量来解决图推理问题，如图[1](https://arxiv.org/html/2410.05130v1#S1.F1
    "图 1 ‣ 1 引言 ‣ 基于 LLM 的多代理的可扩展且精确的图推理")所示。我们采用节点中心的方法，为每个节点分配一个代理，使其专注于处理自己的信息，并与邻居进行通信。因此，我们可以通过简单地增加代理数量，轻松扩大可处理的图的规模。同时，在主
    LLM 的指导下，图问题被分解为更小的、以节点为中心的任务，并分配给代理进行协作解决。这种方法显著减少了每个代理需要处理的信息规模和复杂性，从而大大提高了整体准确性。此外，由于代理必须清晰地将处理过的信息传递给邻居代理，推理过程变得透明，证明了该框架是通过清晰且正确的推理来解决图推理问题，而非靠幸运猜测。总之，我们的贡献如下：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose GraphAgent-Reasoner, the first LLM-based multi-agents framework for
    graph reasoning, which requires no fine-tuning and can utilize any LLM as the
    underlying reasoning model. Our framework achieves near-perfect accuracy on various
    polynomial-time tasks, significantly surpassing the performance of existing methods.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 GraphAgent-Reasoner，这是首个基于 LLM 的多代理图推理框架，不需要微调，且能够利用任何 LLM 作为底层推理模型。我们的框架在各种多项式时间任务上实现了近乎完美的准确性，显著超越了现有方法的表现。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our framework expands the scale of graph reasoning tasks handled by LLMs from
    100 nodes to 1,000 nodes, demonstrating exceptional scalability. Furthermore,
    as the graph size increases, our framework does not exhibit the significant performance
    degradation seen in other methods and maintains robust accuracy.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的框架将 LLM 处理的图推理任务规模从 100 个节点扩展到 1,000 个节点，展示了卓越的可扩展性。此外，随着图大小的增加，我们的框架并未出现其他方法中常见的显著性能下降，并保持了稳健的准确性。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We explore the performance of our framework in real-world applications like
    webpage importance analysis, showcasing its potential for addressing complex graph
    reasoning problems in real-life situations.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在网页重要性分析等实际应用中探索了我们框架的表现，展示了其在解决现实中复杂图推理问题的潜力。
- en: 2 Preliminaries and Related Works
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前言与相关工作
- en: Preliminaries. In general scenarios, when discussing LLMs solving graph reasoning
    problems, the input is a ($\mathcal{G}$,$\mathcal{Q}$) pair. $\mathcal{G}$ is
    a graph represented as $\mathcal{G}=(\mathcal{V},\mathcal{E},\{s_{i}\},\{t_{i}\})$,
    where $\mathcal{V}$ is the node set and $\mathcal{E}$, the edge set. For each
    node $v_{i}\in\mathcal{V}$, a sequential text node feature $s_{i}$ is associated;
    similarly, for each edge $e_{i}\in\mathcal{E}$, a sequential text edge feature
    $t_{i}$ is assigned. The graph $\mathcal{G}$ is described in natural language,
    typically using edge or adjacency list representation. $\mathcal{Q}$ is a task-specific
    instruction or problem description. LLMs will process the ($\mathcal{G}$,$\mathcal{Q}$)
    pair and return an answer string $A$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 预备知识。一般情况下，在讨论大型语言模型（LLMs）解决图推理问题时，输入是一个（$\mathcal{G}$，$\mathcal{Q}$）对。$\mathcal{G}$
    是一个图，表示为 $\mathcal{G}=(\mathcal{V},\mathcal{E},\{s_{i}\},\{t_{i}\})$，其中 $\mathcal{V}$
    是节点集，$\mathcal{E}$ 是边集。对于每个节点 $v_{i}\in\mathcal{V}$，关联一个顺序文本节点特征 $s_{i}$；同样，对于每条边
    $e_{i}\in\mathcal{E}$，分配一个顺序文本边特征 $t_{i}$。图 $\mathcal{G}$ 用自然语言描述，通常使用边或邻接表表示法。$\mathcal{Q}$
    是任务特定的指令或问题描述。LLMs 将处理这个（$\mathcal{G}$，$\mathcal{Q}$）对并返回一个答案字符串 $A$。
- en: Large Language Models for Graph Reasoning. To further enhance the reasoning
    capabilities of LLMs, many works have attempted to improve the performance of
    LLMs in graph reasoning. Wang et al. ([2023](https://arxiv.org/html/2410.05130v1#bib.bib30))
    first introduces the NLGraph Benchmark to evaluate the performance of LLMs on
    various graph reasoning tasks. Fatemi et al. ([2024](https://arxiv.org/html/2410.05130v1#bib.bib9))
    explores the impact of different graph encoding methods and graph structure types
    on the performance of LLMs in graph reasoning tasks. Additionally, it introduces
    another benchmark called GraphQA. Considering the lengthy nature of describing
    graph structures in text, Chai et al. ([2023](https://arxiv.org/html/2410.05130v1#bib.bib2))
    and Perozzi et al. ([2024](https://arxiv.org/html/2410.05130v1#bib.bib25)) respectively
    use Transformers and GNNs to encode graph structures and attempt to align them
    with LLMs. Inspired by how humans understand structural information through the
    visual modality, Wei et al. ([2024](https://arxiv.org/html/2410.05130v1#bib.bib31))
    generates corresponding visual images based on graph structures and provides them
    to visual LLMs for graph reasoning. Chen et al. ([2024a](https://arxiv.org/html/2410.05130v1#bib.bib4))
    conducted Supervised Fine-Tuning and Directly Prefered Optimization on LLMs, enhancing
    the performance of LLMs and encouraging them to output explicit reasoning paths.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图推理的大型语言模型。为了进一步增强 LLMs 的推理能力，许多研究尝试提高 LLMs 在图推理中的表现。Wang 等人（[2023](https://arxiv.org/html/2410.05130v1#bib.bib30)）首次引入了
    NLGraph 基准测试，用以评估 LLMs 在各种图推理任务中的表现。Fatemi 等人（[2024](https://arxiv.org/html/2410.05130v1#bib.bib9)）探索了不同图编码方法和图结构类型对
    LLMs 在图推理任务中表现的影响。此外，他们还引入了另一个基准测试——GraphQA。考虑到图结构描述的文本通常较长，Chai 等人（[2023](https://arxiv.org/html/2410.05130v1#bib.bib2)）和
    Perozzi 等人（[2024](https://arxiv.org/html/2410.05130v1#bib.bib25)）分别使用 Transformers
    和 GNNs 来编码图结构，并尝试将其与 LLMs 对齐。受人类通过视觉模式理解结构信息的启发，Wei 等人（[2024](https://arxiv.org/html/2410.05130v1#bib.bib31)）基于图结构生成相应的视觉图像，并将其提供给视觉
    LLMs 进行图推理。Chen 等人（[2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)）对 LLMs
    进行了监督微调和直接优化，提升了 LLMs 的表现，并鼓励其输出明确的推理路径。
- en: Large Language Model based Multi-Agents. Recent advancements in LLMs have spurred
    interest in their application within multi-agent systems. LLM-based multi-agent
    frameworks leverage the natural language understanding and reasoning capabilities
    of LLMs to enable agents to collaborate, communicate, and solve complex tasks
    in a distributed manner. Existing multi-agents works for problem solving primarily
    focuses on applications such as Software Development (Dong et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib8);
    Hong et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib13); Qian et al.,
    [2024](https://arxiv.org/html/2410.05130v1#bib.bib26)), Embodied Agents (Zhang
    et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib35); Mandi et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib19);
    Chen et al., [2024b](https://arxiv.org/html/2410.05130v1#bib.bib5)) and Science
    Debate (Xiong et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib32); Chan
    et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib3)). However, using
    LLM-based multi-agents to handle graph data has been less explored, especially
    in the areas of graph reasoning and graph computation tasks. This may be due to
    the hallucination issue inherent in LLMs (Huang et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib15)),
    where their responses are factually incorrect. This problem becomes more complex
    in a multi-agent setting, as the hallucinations of a single agent may propagate
    to other nodes by communication (Guo et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib12)).
    This requires the performance of individual agents be sufficiently stable to ensure
    the correct operation of the entire multi-agent system.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型的多智能体系统。近期LLM的进展激发了人们对其在多智能体系统中应用的兴趣。基于LLM的多智能体框架利用LLM的自然语言理解和推理能力，使得智能体能够协作、沟通，并以分布式方式解决复杂任务。现有的多智能体问题解决研究主要集中在诸如软件开发（Dong等，[2023](https://arxiv.org/html/2410.05130v1#bib.bib8);
    Hong等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib13); Qian等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib26)）、具身智能体（Zhang等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib35);
    Mandi等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib19); Chen等，[2024b](https://arxiv.org/html/2410.05130v1#bib.bib5)）以及科学辩论（Xiong等，[2023](https://arxiv.org/html/2410.05130v1#bib.bib32);
    Chan等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib3)）等应用。然而，使用基于LLM的多智能体处理图数据的研究相对较少，特别是在图推理和图计算任务领域。这可能是由于LLM固有的幻觉问题（Huang等，[2023](https://arxiv.org/html/2410.05130v1#bib.bib15)），即它们的回答在事实上的错误。在多智能体环境中，这个问题变得更加复杂，因为单一智能体的幻觉可能通过通信传播到其他节点（Guo等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib12)）。这要求个体智能体的性能必须足够稳定，以确保整个多智能体系统的正确运行。
- en: 3 Limitations of Single LLM in Graph Reasoning
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 单一LLM在图推理中的限制
- en: 'Although LLMs exhibit strong language processing and logical reasoning capabilities,
    problems with the Transformer architecture and Attention mechanism (Vaswani et al.,
    [2017](https://arxiv.org/html/2410.05130v1#bib.bib29)) still limit the scale and
    accuracy when they process graph problems. There are two primary limitations:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型展现出强大的语言处理和逻辑推理能力，但Transformer架构和注意力机制（Vaswani等，[2017](https://arxiv.org/html/2410.05130v1#bib.bib29)）的问题仍然限制了它们处理图问题时的规模和准确性。主要有两个限制：
- en: '![Refer to caption](img/28dcc416b75ee00b1bba4c46f80082b9.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图注](img/28dcc416b75ee00b1bba4c46f80082b9.png)'
- en: 'Figure 2: The performance of a single LLM in memorizing first-order neighboring
    nodes. As the number of nodes increases, all models exhibit significant memory
    errors.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：单一大型语言模型（LLM）在记忆一阶邻接节点方面的表现。随着节点数量的增加，所有模型都表现出显著的记忆错误。
- en: The graph structure is too complex to memorize and understand for a single LLM.
    Using adjacency or edge lists to describe graph structures in natural language
    is the most intuitive and direct method, facilitating the processing of graph
    data by LLMs through text. However, this approach inevitably leads to a lengthy
    context, as the number of edges can grow quadratically with the number of nodes.
    As the graph scales up and becomes denser, the graph structure becomes highly
    complex, requiring a large amount of tokens to describe the edge relationships.
    When the text becomes too lengthy, it becomes difficult for LLMs to properly allocate
    attention, and they may even struggle with simple tasks such as key-value pair
    matching Liu et al. ([2023](https://arxiv.org/html/2410.05130v1#bib.bib17)). This
    presents significant challenges for LLMs in identifying key information for graph
    reasoning tasks from the lengthy context. Figure [2](https://arxiv.org/html/2410.05130v1#S3.F2
    "Figure 2 ‣ 3 Limitations of Single LLM in Graph Reasoning ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents") shows the performance of a single
    LLM in memorizing one-hop neighbor nodes. We observe that as the number of nodes
    in the graph increases, various LLMs exhibit a significant decline in accuracy.
    If a single LLM cannot even correctly recall basic graph structural information
    like node neighbors, it becomes difficult to proceed with more complex graph reasoning
    or computation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图结构对于单一的语言模型（LLM）来说过于复杂，难以记忆和理解。使用邻接或边列表在自然语言中描述图结构是最直观和直接的方法，能够通过文本促进LLM对图数据的处理。然而，这种方法不可避免地会导致上下文变得冗长，因为边的数量随着节点数量的增加呈二次增长。随着图的规模扩大和密度增加，图结构变得极为复杂，需要大量的标记来描述边关系。当文本变得过长时，LLM很难合理分配注意力，甚至可能在一些简单任务上遇到困难，例如键值对匹配Liu等人（[2023](https://arxiv.org/html/2410.05130v1#bib.bib17)）。这为LLM从冗长的上下文中识别图推理任务的关键信息带来了重大挑战。图[2](https://arxiv.org/html/2410.05130v1#S3.F2
    "Figure 2 ‣ 3 Limitations of Single LLM in Graph Reasoning ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents")展示了单一LLM在记忆一跳邻居节点时的表现。我们观察到，随着图中节点数量的增加，各种LLM的准确率显著下降。如果单一LLM甚至无法正确回忆起像节点邻居这样的基本图结构信息，那么进行更复杂的图推理或计算将变得非常困难。
- en: Furthermore, the graph structure is described in a sequential manner. LLMs have
    to identify implicit graph structures from sequential text. Since the processing
    of LLMs is a black-box operation, it is difficult to assert that they truly construct
    graph structures implicitly and thereby understand them. Huang et al. ([2024](https://arxiv.org/html/2410.05130v1#bib.bib14))
    conducted extensive experiments to explore whether LLMs treat the input prompts
    as graphs or merely as paragraphs with keywords on TAGs. The results show that
    the performance of LLMs in handling TAGs primarily stems from the context rather
    than the graph structure. LLMs tend to process the graph description as linearized
    paragraphs rather than graphs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图结构是以顺序方式描述的。LLM必须从顺序文本中识别隐式图结构。由于LLM的处理是一个黑箱操作，很难断言它们是否真的隐式构建了图结构并理解它们。Huang等人（[2024](https://arxiv.org/html/2410.05130v1#bib.bib14)）进行了大量实验，探索LLM是否将输入提示视为图，还是仅仅将其视为带有关键字的段落。结果表明，LLM在处理TAG时的表现主要来自上下文，而非图结构。LLM倾向于将图描述处理为线性化的段落，而非图结构。
- en: 'A single LLM struggles to solve reasoning problems in real-world scenarios.
    Researchers train LLMs on graph reasoning tasks to empower them to utilize learned
    graph-related knowledge or algorithms to tackle real-world graph problems. However,
    in practical scenarios, the amount of information associated with each node can
    be enormous. Take citation networks as an example: a single node represents a
    paper, and its node information includes the title, abstract, and references,
    which could amount to several thousand tokens. In addition to the complexity of
    graph structures, the need to handle a large amount of node information further
    exacerbates the burden on a single LLM and highlights its shortcomings in processing
    long contexts. Moreover, using a single LLM to handle the entire network is inefficient,
    as it cannot coherently process the entire network’s problems. Typically, it is
    necessary to manually compress or summarize the information for each node and
    then feed local subgraphs to the LLM for processing (Guo et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib11);
    Chen et al., [2023](https://arxiv.org/html/2410.05130v1#bib.bib6)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单一的大型语言模型（LLM）在解决现实世界场景中的推理问题时存在困难。研究人员在图推理任务上训练LLM，以使其能够利用学习到的图相关知识或算法来解决现实世界的图问题。然而，在实际场景中，每个节点所关联的信息量可能是巨大的。以引用网络为例：单个节点代表一篇论文，节点信息包括标题、摘要和参考文献，这些可能会占用几千个token。除了图结构的复杂性外，处理大量节点信息的需求进一步加重了单一LLM的负担，突显了其在处理长上下文时的不足。此外，使用单一LLM来处理整个网络效率低下，因为它无法连贯地处理整个网络的问题。通常需要手动压缩或总结每个节点的信息，然后将局部子图输入LLM进行处理（Guo等，
    [2023](https://arxiv.org/html/2410.05130v1#bib.bib11)；Chen等，[2023](https://arxiv.org/html/2410.05130v1#bib.bib6)）。
- en: 'Furthermore, many current works (Chen et al., [2024a](https://arxiv.org/html/2410.05130v1#bib.bib4);
    Perozzi et al., [2024](https://arxiv.org/html/2410.05130v1#bib.bib25)) require
    training GNNs or fine-tuning LLMs on individual or multiple graph reasoning tasks.
    However, when transferring to other graph tasks, a certain degree of performance
    degradation occurs, and retraining or fine-tuning for new graph tasks consumes
    a significant amount of time and resources. Whether LLMs can apply the graph knowledge
    and algorithms learned during the training process to actual graph reasoning also
    remains an open question. We explored this question in [5.3](https://arxiv.org/html/2410.05130v1#S5.SS3
    "5.3 Experiment 3: Case Study ‣ 5 Experiments ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents") and observed significant overfitting in LLMs fine-tuned
    on specific graph reasoning tasks. Therefore, the ideal solution would be to leverage
    the powerful general knowledge acquired during the pre-training phase of LLMs
    through an appropriate approach, enabling them to handle graph reasoning tasks
    as naturally as they do with natural language problems.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，许多当前的工作（Chen等， [2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)；Perozzi等，
    [2024](https://arxiv.org/html/2410.05130v1#bib.bib25)）需要在单个或多个图推理任务上训练图神经网络（GNN）或微调LLM。然而，当转移到其他图任务时，性能会出现一定程度的下降，重新训练或微调新的图任务需要消耗大量时间和资源。LLM是否能够将训练过程中学到的图知识和算法应用到实际的图推理中，仍然是一个悬而未决的问题。我们在[5.3](https://arxiv.org/html/2410.05130v1#S5.SS3
    "5.3 Experiment 3: Case Study ‣ 5 Experiments ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents")中探讨了这个问题，并观察到在特定图推理任务上微调的LLM出现了明显的过拟合。因此，理想的解决方案是通过一种适当的方法，利用LLM在预训练阶段获得的强大通用知识，使其能够像处理自然语言问题一样自然地处理图推理任务。'
- en: 4 GraphAgent-Reasoner
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 GraphAgent-Reasoner
- en: 'To solve the limitations above, we propose a novel framework based on multi-agent
    collaboration called GraphAgent-Reasoner as shown in Figure [3](https://arxiv.org/html/2410.05130v1#S4.F3
    "Figure 3 ‣ 4 GraphAgent-Reasoner ‣ Scalable and Accurate Graph Reasoning with
    LLM-based Multi-Agents"), aiming to solve graph reasoning problems explicitly
    and correctly. The interface of the framework is a Master LLM, which is responsible
    for processing the textual input of graph problems, constructing the agent network,
    directing them to collaboratively solve the problem, and finally aggregating the
    states of all agents to derive the solution. Its implementation is based on the
    React Agent proposed by Yao et al. ([2023](https://arxiv.org/html/2410.05130v1#bib.bib34)),
    which is capable of reasoning based on the environment and executing corresponding
    actions, as detailed later. The pipeline of GAR consists of four steps: Graph
    Construction, Algorithm Establishing, Distributed Execution and Master Summarization.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述局限性，我们提出了一种基于多代理协作的创新框架，称为GraphAgent-Reasoner，如图[3](https://arxiv.org/html/2410.05130v1#S4.F3
    "图3 ‣ 4 GraphAgent-Reasoner ‣ 基于LLM的多代理可扩展和准确的图推理")所示，旨在明确且正确地解决图推理问题。该框架的接口是Master
    LLM，负责处理图问题的文本输入，构建代理网络，指导它们协同解决问题，最后汇总所有代理的状态以得出解决方案。其实现基于Yao等人提出的React Agent（[2023](https://arxiv.org/html/2410.05130v1#bib.bib34)），该代理能够基于环境进行推理并执行相应的操作，具体细节将在后面详细说明。GAR的流程包括四个步骤：图构建、算法建立、分布式执行和Master总结。
- en: '![Refer to caption](img/2a43ee988f767586c518e0e7b69f85b9.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2a43ee988f767586c518e0e7b69f85b9.png)'
- en: 'Figure 3: The framework of GraphAgent-Reasoner. Given a graph problem, the
    Master LLM will first construct agents network according to graph strcutures.
    It then sequentially performs Algorithm Establishing, Distributed Execution and
    Master Summarization, as detailed in this section.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：GraphAgent-Reasoner框架。给定一个图问题，Master LLM将根据图结构首先构建代理网络。然后，它依次执行算法建立、分布式执行和Master总结，具体过程详见本节。
- en: Graph Construction. Given an input pair ($\mathcal{G}$, $\mathcal{Q}$), the
    Master LLM first extracts the node and edge information from the textual description
    of graph $\mathcal{G}$. It then constructs an agent for each node and initializes
    the node’s state and neighbor information, forming an interconnected network of
    agents. Each agent independently maintains its state and neighbor data, communicates
    with adjacent agents based on instructions from the Master LLM, and updates its
    state in each round.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图构建。给定输入对（$\mathcal{G}$, $\mathcal{Q}$），Master LLM首先从图$\mathcal{G}$的文本描述中提取节点和边的信息。然后，它为每个节点构建一个代理并初始化节点的状态和邻居信息，形成一个相互连接的代理网络。每个代理独立维护其状态和邻居数据，基于Master
    LLM的指令与相邻的代理进行通信，并在每一轮中更新其状态。
- en: 'Algorithm Establishing. To accommodate diverse graph tasks and fully exploit
    the knowledge embedded in LLMs during pre-training, we propose a unified solution
    approach framed within a distributed paradigm as shown in Algorithm [1](https://arxiv.org/html/2410.05130v1#alg1
    "In 4 GraphAgent-Reasoner ‣ Scalable and Accurate Graph Reasoning with LLM-based
    Multi-Agents"). This approach requires the Master LLM to specify six core components
    for each problem: State, Message, Initialization, Send, Update, and Termination.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 算法建立。为了适应多样化的图任务并充分利用LLM在预训练过程中嵌入的知识，我们提出了一种统一的解决方案方法，框架采用分布式范式，如算法[1](https://arxiv.org/html/2410.05130v1#alg1
    "在 4 GraphAgent-Reasoner ‣ 基于LLM的多代理可扩展和准确的图推理")所示。该方法要求Master LLM为每个问题指定六个核心组件：状态、消息、初始化、发送、更新和终止。
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'State: The local information maintained by each node, representing its current
    state. This can include attributes like node features, labels, or any other task-specific
    data. The states evolve as nodes receive messages and update their information.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 状态：每个节点维护的本地信息，表示其当前状态。这可以包括节点特征、标签或任何其他任务相关的数据。随着节点接收消息并更新其信息，状态会发生变化。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Message: The data transmitted between nodes during the communication phase.
    Messages typically contain information that neighboring nodes need to perform
    updates, such as feature values, distances, or other task-relevant information.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 消息：在通信阶段，节点之间传输的数据。消息通常包含邻近节点执行更新所需的信息，如特征值、距离或其他任务相关信息。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Initialization: At the start of the execution, each node initializes its state
    with predefined values, which may be based on node IDs, input features or task-specific
    requirements. This step ensures that the graph is ready to begin the communication
    process.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初始化：在执行开始时，每个节点使用预定义的值初始化其状态，这些值可能基于节点ID、输入特征或任务特定的要求。此步骤确保图形已准备好开始通信过程。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Send: After initialization, each node generates messages based on its current
    state and sends them to its neighboring nodes. This step is repeated in each iteration,
    allowing nodes to continuously exchange information with their neighbors.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 发送：初始化后，每个节点根据其当前状态生成消息并将其发送给邻近节点。此步骤在每次迭代中重复，允许节点持续与其邻居交换信息。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Update: Upon receiving messages from its neighbors, each node updates its state
    by aggregating the incoming messages and combining them with its current state.
    This iterative process enables nodes to refine their information over time.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更新：在接收到来自邻居的消息后，每个节点通过聚合收到的消息并将其与当前状态相结合来更新自己的状态。这个迭代过程使节点能够随着时间的推移不断完善其信息。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Termination: The algorithm halts when a predefined stopping condition is met,
    such as reaching a fixed number of iterations, achieving convergence, or satisfying
    a task-specific criterion. Once the termination condition is reached, each node
    will send its final state to the Master LLM, and the execution terminates.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 终止：当满足预定的停止条件时，算法会终止，例如达到固定的迭代次数、实现收敛，或满足任务特定的标准。一旦达到终止条件，每个节点将把其最终状态发送给主LLM，执行终止。
- en: '1 Input: Agent Nodes $\mathcal{A}$, each agent $a\in\mathcal{A}$ maintains
    a state $S_{a}$, the maximum iterations $I_{max}$ given by the Master LLM.2Output:
    Final state $S_{a}$ for each agent $a\in\mathcal{A}$/* Initialization */3 Each
    agent $a\in\mathcal{A}$ initializes its state $S_{a}$ based on Initialization
    rules.4Each agent $a$ sends an initial message $M_{a\rightarrow v}$ to each of
    its neighbors $v\in\text{Neighbors}(a)$ based on its current state $S_{a}$ and
    Send rules./* Communication */5 while *Iteration $i$ < $I_{max}$ and Termination
    not met* do      a. /* Receive */6      Each agent $a$ receives messages $M_{u\rightarrow
    a}$ from all neighboring agents $u$.      b. /* Update */7      Each agent $a$
    updates its state $S_{a}$ based on the received messages $M$ and its own current
    state $S_{a}$ according to Update rules.      c. /* Send */8      Each agent $a$
    sends updated messages $M_{a\rightarrow v}$ to each of its neighbors $v$ based
    on the updated state $S_{a}$ according to Send rules.Return: the final state $S_{a}$
    for all agents $a\in\mathcal{A}$'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 1 输入：代理节点$\mathcal{A}$，每个代理$a\in\mathcal{A}$保持一个状态$S_{a}$，最大迭代次数$I_{max}$由主LLM给定。2
    输出：每个代理$a\in\mathcal{A}$的最终状态$S_{a}$/* 初始化 */3 每个代理$a\in\mathcal{A}$根据初始化规则初始化其状态$S_{a}$。4
    每个代理$a$基于其当前状态$S_{a}$和发送规则向其每个邻居$v\in\text{Neighbors}(a)$发送初始消息$M_{a\rightarrow
    v}$。/* 通信 */5 当*迭代$i$ < $I_{max}$且终止条件未满足*时，执行以下操作：    a. /* 接收 */6 每个代理$a$从所有邻居代理$u$接收消息$M_{u\rightarrow
    a}$。    b. /* 更新 */7 每个代理$a$根据接收到的消息$M$和其自身的当前状态$S_{a}$，根据更新规则更新其状态$S_{a}$。  
     c. /* 发送 */8 每个代理$a$根据更新后的状态$S_{a}$，根据发送规则向其每个邻居$v$发送更新后的消息$M_{a\rightarrow v}$。返回：所有代理$a\in\mathcal{A}$的最终状态$S_{a}$。
- en: Algorithm 1 Distributed Paradigm
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 分布式范式
- en: Since LLMs lack prior knowledge of this distributed paradigm, to facilitate
    the Master LLM’s understanding and application of the framework, we develop a
    distributed algorithm library that adheres to this distributed paradigm, from
    which the Master LLM can query relevant algorithm templates to generate distributed
    solutions within this paradigm. Specifically, we selected classic distributed
    graph algorithms and documented their implementations under this distributed paradigm.
    Some examples are presented in Appendix [A.1](https://arxiv.org/html/2410.05130v1#A1.SS1
    "A.1 Example of distributed algorithms in distributed algorithm library ‣ Appendix
    A Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents"). Drawing on prior work (Zheng et al.,
    [2024](https://arxiv.org/html/2410.05130v1#bib.bib36); Meng et al., [2024a](https://arxiv.org/html/2410.05130v1#bib.bib21)),
    we endeavor to write detailed reasoning steps of each part in the algorithm to
    encourage the agent to think step by step as much as possible, which plays an
    important role in enhancing the success rate of individual agents.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM缺乏对这一分布式范式的先验知识，为了方便主LLM理解和应用该框架，我们开发了一个遵循这一分布式范式的分布式算法库，主LLM可以从中查询相关算法模板，生成该范式下的分布式解决方案。具体而言，我们选择了经典的分布式图算法，并记录了它们在这一分布式范式下的实现。一些示例见附录[A.1](https://arxiv.org/html/2410.05130v1#A1.SS1
    "A.1 分布式算法库中的分布式算法示例 ‣ 附录A 分布式范式下的分布式算法 ‣ 基于LLM的多智能体可扩展且准确的图推理")。借鉴了先前的工作（Zheng等，[2024](https://arxiv.org/html/2410.05130v1#bib.bib36);
    Meng等，[2024a](https://arxiv.org/html/2410.05130v1#bib.bib21)），我们努力详细编写算法中每一部分的推理步骤，尽可能鼓励代理逐步思考，这对于提高个体代理的成功率起到了重要作用。
- en: When receiving a problem input, the Master LLM first retrieves the $k$ algorithms
    most relevant to the problem description from the distributed algorithm library.
    If there are algorithms suitable for handling the problem, the Master LLM will
    adjust the algorithm according to the problem description, such as changing the
    initialization and termination conditions (e.g., the source node in the shortest
    path problem). If there are no appropriate algorithms, the Master LLM will design
    a distributed algorithm following the distributed paradigm based on the examples
    of the retrieved algorithms. For some generated examples, see Appendix [A.2](https://arxiv.org/html/2410.05130v1#A1.SS2
    "A.2 Example of distributed algorithms designed by the Master LLM ‣ Appendix A
    Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到问题输入时，主LLM首先从分布式算法库中检索出与问题描述最相关的$k$个算法。如果有适合处理该问题的算法，主LLM将根据问题描述调整算法，例如改变初始化和终止条件（例如，最短路径问题中的源节点）。如果没有合适的算法，主LLM将基于检索到的算法示例，按照分布式范式设计一个分布式算法。有关一些生成的示例，请参见附录[A.2](https://arxiv.org/html/2410.05130v1#A1.SS2
    "A.2 主LLM设计的分布式算法示例 ‣ 附录A 分布式范式下的分布式算法 ‣ 基于LLM的多智能体可扩展且准确的图推理")。
- en: Distributed Execution. After the distributed algorithm is designed, the Master
    LLM will relay the approach to each agent node for execution according to the
    process outlined in Algorithm [1](https://arxiv.org/html/2410.05130v1#alg1 "In
    4 GraphAgent-Reasoner ‣ Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents").
    Each agent will first initialize its state based on node information and algorithm
    rules and then send an initial message to neighboring agents. Subsequently, each
    agent will iteratively execute the operations of receiving messages, updating
    its state, and sending messages according to the algorithm rules, synchronizing
    progress after each communication round. Communication will continue until the
    maximum number of iterations is reached or the termination condition is met.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式执行。在设计好分布式算法后，主LLM将根据算法[1](https://arxiv.org/html/2410.05130v1#alg1 "在 4
    GraphAgent-Reasoner ‣ 基于LLM的多智能体可扩展且准确的图推理")中概述的过程，将该方法传递给每个代理节点进行执行。每个代理首先根据节点信息和算法规则初始化其状态，然后向邻近的代理发送初始消息。随后，每个代理将按照算法规则迭代地执行接收消息、更新状态和发送消息的操作，并在每轮通信后同步进度。通信将继续进行，直到达到最大迭代次数或满足终止条件为止。
- en: Master Summarization. Finally, the final state of all agent nodes will be aggregated
    to the Master LLM, which will summarize the results conclude based on the problem
    and return the final answer in natural language form.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 主总结。最后，所有代理节点的最终状态将汇总到主LLM，主LLM将基于问题总结结果并以自然语言形式返回最终答案。
- en: 5 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5个实验
- en: 'In this section, we summarize the key experiments conducted with GAR. We begin
    by highlighting some of the most exciting results from our analysis here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了使用GAR进行的一些关键实验。我们首先突出展示了以下分析中一些最令人兴奋的结果：
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R1: GAR achieves near-perfect accuracy on polynomial-time graph reasoning problems,
    significantly surpassing existing closed-source models and open-source models
    fine-tuned on extensive data.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R1：GAR在多项式时间的图推理问题上达到了接近完美的准确度，远超现有的封闭源模型和在大量数据上经过微调的开源模型。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R2: GAR maintains high accuracy on larger-scale graphs (up to 1000 nodes),
    demonstrating superior scalability. In contrast, as the number of nodes increases,
    other models exhibit a significant decline in performance or become incapable
    of handling the problem at all due to the context length limitation.'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R2：GAR在大规模图（最多1000个节点）上保持高准确性，展示了优越的可扩展性。相比之下，随着节点数量的增加，其他模型的表现显著下降，或由于上下文长度限制，完全无法处理该问题。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'R3: GAR showcases a robust understanding and application of graph algorithms
    in real-world graph reasoning scenarios, highlighting its potential for addressing
    complex graph problems encountered in daily life. In contrast, other open-source
    models that have undergone extensive fine-tuning on graph reasoning datasets fail
    to apply the learned graph reasoning knowledge when confronted with rephrased
    real-world graph problems.'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: R3：GAR展示了对图算法在实际图推理场景中的强大理解和应用，突显了它在解决日常生活中遇到的复杂图问题上的潜力。相比之下，其他在图推理数据集上经过大量微调的开源模型，在面对重新表述的实际图问题时，无法应用学到的图推理知识。
- en: Datasets. We conduct our experiments on the graph reasoning tasks proposed in
    GraphInstruct (Chen et al., [2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)).
    This dataset contains nine graph reasoning problems with different time complexity,
    ranging from linear and polynomial complexity to NP-complete.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们在GraphInstruct（Chen等人，[2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)）中提出的图推理任务上进行实验。该数据集包含九个图推理问题，具有不同的时间复杂度，从线性和多项式复杂度到NP完全。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Linear. Cycle Detection (Detect if a given graph $\mathcal{G}$ contains any
    cycles), Connectivity (Assess if two nodes $u$ and $v$ in a given graph $\mathcal{G}$
    are connected via a path), Bipartite Graph Check (Judge if a given graph $\mathcal{G}$
    is bipartite), and Topological Sort (Find a topological ordering of vertices in
    a directed acyclic graph $\mathcal{G}$).
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性。环检测（检测给定图$\mathcal{G}$是否包含环），连通性（评估给定图$\mathcal{G}$中两个节点$u$和$v$是否通过路径相连），二分图检查（判断给定图$\mathcal{G}$是否为二分图），拓扑排序（在有向无环图$\mathcal{G}$中找到一个顶点的拓扑排序）。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Polynomial. Shortest Path (Compute the shortest path between two specific nodes
    $u$ and $v$ in a given graph $\mathcal{G}$), Maximum Triangle Sum (Find the maximum
    sum of weights for any connected triplet of vertices in a given graph $\mathcal{G}$),
    and Maximum Flow (Calculate the maximum flow from a source node $s$ to a sink
    node $t$ in a directed graph $\mathcal{G}$).
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多项式。最短路径（计算给定图$\mathcal{G}$中两个特定节点$u$和$v$之间的最短路径），最大三角形和（找到给定图$\mathcal{G}$中任何连通三元组顶点的最大权重和），最大流（计算有向图$\mathcal{G}$中从源节点$s$到汇节点$t$的最大流）。
- en: Due to the complexity of NP-complete problems, there are currently no mature
    exact distributed algorithms available for their solution. Consequently, the Master
    LLM is unable to design correct and effective distributed algorithms based on
    the knowledge acquired during pre-training. Therefore, in our experiments, we
    only consider linear and polynomial-time problems. Detailed information of the
    dataset and partial test results for NP-complete problems will be presented in
    Appendix [B](https://arxiv.org/html/2410.05130v1#A2 "Appendix B The GraphInstruct
    Dataset ‣ Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents").
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于NP完全问题的复杂性，目前没有成熟的精确分布式算法可用于解决这些问题。因此，Master LLM无法基于在预训练期间获得的知识设计正确且有效的分布式算法。因此，在我们的实验中，我们仅考虑线性和多项式时间问题。关于NP完全问题的数据集详细信息和部分测试结果将呈现在附录[B](https://arxiv.org/html/2410.05130v1#A2
    "附录B GraphInstruct数据集 ‣ 基于LLM的多代理的可扩展和准确图形推理")中。
- en: Setting. The underlying reasoning LLM of Agent Node used in our framework is
    ChatGPT-4o-mini-2024-07-18, and the base model of Master LLM is ChatGPT-4-turbo(OpenAI,
    [2023](https://arxiv.org/html/2410.05130v1#bib.bib24)). The temperature is consistently
    set to 0\. Our framwork is built upon AgentScope (Gao et al. ([2024](https://arxiv.org/html/2410.05130v1#bib.bib10))),
    an innovative platform to easily build reliable, high-performance multi-agent
    applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 设置。我们框架中Agent Node所使用的基础推理LLM是ChatGPT-4o-mini-2024-07-18，Master LLM的基础模型是ChatGPT-4-turbo（OpenAI，[2023](https://arxiv.org/html/2410.05130v1#bib.bib24)）。温度始终设置为0。我们的框架基于AgentScope（Gao等人，[2024](https://arxiv.org/html/2410.05130v1#bib.bib10)），这是一个创新平台，可以轻松构建可靠的高性能多代理应用。
- en: '5.1 Experiment 1: Performance on GraphInstruct'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验 1：GraphInstruct上的性能
- en: 'In this experiment, we evaluate the performance of GAR on polynomial-time tasks
    of the GraphInstruct dataset. The results are shown in Table [1](https://arxiv.org/html/2410.05130v1#S5.T1
    "Table 1 ‣ 5.1 Experiment 1: Performance on GraphInstruct ‣ 5 Experiments ‣ Scalable
    and Accurate Graph Reasoning with LLM-based Multi-Agents"). We see GAR exhibits
    near-perfect results on these tasks, significantly outperforming other models.
    Especially on shortest and triangle tasks with high time complexity, GAR substantially
    improves the performance of LLMs. Problems that a single LLM struggles to solve
    have been effectively resolved through collaboration by agents after being decomposed
    into smaller, node-centric tasks.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本实验中，我们评估了GAR在GraphInstruct数据集的多项式时间任务上的表现。结果如表[1](https://arxiv.org/html/2410.05130v1#S5.T1
    "表1 ‣ 5.1 实验 1：GraphInstruct上的性能 ‣ 5 实验 ‣ 基于LLM的多代理的可扩展和准确图形推理")所示。我们看到，GAR在这些任务上表现出近乎完美的结果，显著超过了其他模型。特别是在时间复杂度较高的最短路径和三角形任务上，GAR大大提高了LLM的性能。单一LLM难以解决的问题，在代理协作后通过将问题分解为更小的、以节点为中心的任务，已得到了有效解决。
- en: 'As the number of nodes increases, the graph structures become more complex,
    making the solution of graph problems increasingly difficult. To investigate how
    the performance of models varies with increasing problem complexity, we conduct
    experiments on cycle detection and shortest path problems, gradually increasing
    the number of nodes from 5 to 100\. The results are presented in Figure [4](https://arxiv.org/html/2410.05130v1#S5.F4
    "Figure 4 ‣ 5.1 Experiment 1: Performance on GraphInstruct ‣ 5 Experiments ‣ Scalable
    and Accurate Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 随着节点数量的增加，图形结构变得更加复杂，使得图形问题的解决变得越来越困难。为了研究模型在问题复杂度增加时的性能变化，我们在循环检测和最短路径问题上进行了实验，逐渐将节点数量从5增加到100。结果如图[4](https://arxiv.org/html/2410.05130v1#S5.F4
    "图4 ‣ 5.1 实验 1：GraphInstruct上的性能 ‣ 5 实验 ‣ 基于LLM的多代理的可扩展和准确图形推理")所示。
- en: '![Refer to caption](img/8ecbe9b151774b21b498c89fc7abfcfd.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8ecbe9b151774b21b498c89fc7abfcfd.png)'
- en: (a) Cycle Detection
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 循环检测
- en: '![Refer to caption](img/a6cb80e743825c03675218e724994c02.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a6cb80e743825c03675218e724994c02.png)'
- en: (b) Shortest Path
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 最短路径
- en: 'Figure 4: Performance of GraphAgent-Reasoner, GPT4(2 shot) and GraphWiz(Mistral
    7B) on cycle detection and shortest path problems with different graph sizes.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：GraphAgent-Reasoner、GPT4（2次示例）和GraphWiz（Mistral 7B）在不同图形大小下的循环检测和最短路径问题上的性能
- en: We see with the number of nodes increasing, both ChatGPT-4 and Graphwiz exhibit
    a significant decline in performance. However, the accuracy of GAR remains stable,
    almost unaffected by the graph size, demonstrating robust scalability. Although
    the scale of the graph is increasing, the information processed by each agent
    has not significantly increased. Each agent still only handles its own information
    and communicates with neighboring agents. We observe that GAR occasionally makes
    errors in specific cases, likely due to the increasing communication rounds as
    the number of nodes and edges grows. Even when handling simple node-centric tasks,
    a single agent still has the potential to make mistakes. Therefore, as the number
    of agents and communication rounds increases, the overall likelihood of errors
    also rises. This can be improved by enhancing the capability of individual agents
    (such as using stronger LLMs as the underlying reasoning model) or by more finely
    designed prompts.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现随着节点数量的增加，ChatGPT-4和Graphwiz的性能均显著下降。然而，GAR的准确性保持稳定，几乎不受图形大小的影响，展示了其强大的可扩展性。尽管图形的规模在增加，但每个代理所处理的信息并没有显著增加。每个代理仍然只处理自己的信息并与邻近的代理进行通信。我们观察到，GAR偶尔会在特定情况下出现错误，这可能是由于随着节点和边的增加，通信轮次增多所导致的。即使是在处理简单的以节点为中心的任务时，单个代理仍然有可能出错。因此，随着代理数量和通信轮次的增加，整体错误发生的概率也会增加。通过提升单个代理的能力（例如使用更强大的LLM作为基础推理模型）或通过设计更精细的提示，可以改善这一问题。
- en: 'Table 1: Performance of GraphAgent-Reasoner and other models on polynomial-time
    tasks of GraphInstruct test set. Each task contains 400 test cases, with a maximum
    of 100 nodes. The first best result for each task is highlighted in bold, and
    the second best result is highlighted underlined.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：GraphAgent-Reasoner和其他模型在GraphInstruct测试集的多项式时间任务上的性能。每个任务包含400个测试案例，最多包含100个节点。每个任务的最佳结果以**粗体**显示，第二最佳结果以*下划线*标出。
- en: '| Models | Linear | Polynomial | Average |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 线性 | 多项式 | 平均 |'
- en: '| cycle | connect | bipartite | topology | shortest | triangle |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| cycle | connect | bipartite | topology | shortest | triangle |'
- en: '| Closed-source Models |  |  |  |  |  |  |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 封闭源模型 |  |  |  |  |  |  |  |'
- en: '| GPT-4 (zero-shot) | 38.75 | 17.00 | 65.25 | 5.00 | 9.25 | 5.75 | 23.50 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (zero-shot) | 38.75 | 17.00 | 65.25 | 5.00 | 9.25 | 5.75 | 23.50 |'
- en: '| GhatGPT (2-shot) | 51.25 | 43.75 | 70.75 | 4.50 | 3.50 | 17.25 | 31.83 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| GhatGPT (2-shot) | 51.25 | 43.75 | 70.75 | 4.50 | 3.50 | 17.25 | 31.83 |'
- en: '| GPT-4 (2-shot) | 52.50 | 62.75 | 74.25 | 25.25 | 18.25 | 31.00 | 44.00 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (2-shot) | 52.50 | 62.75 | 74.25 | 25.25 | 18.25 | 31.00 | 44.00 |'
- en: '| Fine-tuned Open-source Models |  |  |  |  |  |  |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 微调的开源模型 |  |  |  |  |  |  |  |'
- en: '| Naive SFT (LLaMA 2-7B) | 73.75 | 83.50 | 41.25 | 4.00 | 9.50 | 30.00 | 40.17
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Naive SFT (LLaMA 2-7B) | 73.75 | 83.50 | 41.25 | 4.00 | 9.50 | 30.00 | 40.17
    |'
- en: '| Naive SFT (Mistral-7B) | 73.75 | 83.50 | 78.50 | 1.00 | 23.00 | 47.00 | 51.13
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Naive SFT (Mistral-7B) | 73.75 | 83.50 | 78.50 | 1.00 | 23.00 | 47.00 | 51.13
    |'
- en: '| GraphWiz (LLaMA 2-7B) | 91.50 | 87.00 | 74.00 | 18.00 | 28.00 | 38.25 | 56.13
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GraphWiz (LLaMA 2-7B) | 91.50 | 87.00 | 74.00 | 18.00 | 28.00 | 38.25 | 56.13
    |'
- en: '| GraphWiz (Mistral-7B) | 92.00 | 89.50 | 72.00 | 19.00 | 31.25 | 38.75 | 57.08
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GraphWiz (Mistral-7B) | 92.00 | 89.50 | 72.00 | 19.00 | 31.25 | 38.75 | 57.08
    |'
- en: '| GraphWiz-DPO (LLaMA 2-7B) | 89.00 | 82.50 | 84.75 | 46.75 | 24.00 | 52.75
    | 63.29 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GraphWiz-DPO (LLaMA 2-7B) | 89.00 | 82.50 | 84.75 | 46.75 | 24.00 | 52.75
    | 63.29 |'
- en: '| GraphWiz-DPO (Mistral-7B) | 85.50 | 79.50 | 85.50 | 85.25 | 12.50 | 29.00
    | 62.88 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GraphWiz-DPO (Mistral-7B) | 85.50 | 79.50 | 85.50 | 85.25 | 12.50 | 29.00
    | 62.88 |'
- en: '| GraphAgent-Reasoner | 99.50 | 100.00 | 100.00 | 96.50 | 99.75 | 93.25 | 98.00
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GraphAgent-Reasoner | 99.50 | 100.00 | 100.00 | 96.50 | 99.75 | 93.25 | 98.00
    |'
- en: '5.2 Experiment 2: Performance on Large-Scale Graphs'
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实验 2：大规模图形上的性能
- en: 'In this experiment, we evaluate the performance of current LLMs on large-scale
    graphs. The largest graph size handled by existing graph reasoning work is 100
    nodes (Chen et al., [2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)), which
    is still far from sufficient for real-world graph reasoning scenarios. To evaluate
    the reasoning performance of existing models on larger graphs, we conduct shortest
    path experiments on graphs with 100, 200, 500, and 1000 nodes. Due to the excessively
    long input text (reaching 16,000 tokens for 1000 nodes) and the money cost, we
    only create 20 test samples for each graph size. The results are shown in Table [2](https://arxiv.org/html/2410.05130v1#S5.T2
    "Table 2 ‣ 5.2 Experiment 2: Performance on Large-Scale Graphs ‣ 5 Experiments
    ‣ Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在本实验中，我们评估了当前大语言模型（LLMs）在大规模图形上的表现。现有图形推理工作所处理的最大图形大小为100个节点（Chen等人，[2024a](https://arxiv.org/html/2410.05130v1#bib.bib4)），这仍然远远不足以应对现实世界中的图形推理场景。为了评估现有模型在更大图形上的推理性能，我们在包含100、200、500和1000个节点的图形上进行了最短路径实验。由于输入文本过长（1000个节点的文本达16,000个tokens）以及成本问题，我们仅为每个图形大小创建了20个测试样本。实验结果如表[2](https://arxiv.org/html/2410.05130v1#S5.T2
    "Table 2 ‣ 5.2 Experiment 2: Performance on Large-Scale Graphs ‣ 5 Experiments
    ‣ Scalable and Accurate Graph Reasoning with LLM-based Multi-Agents")所示。'
- en: 'Table 2: Performance on large-scale graphs dealing with shortest path problems.
    x/20 indicates that out of 20 test samples, x samples are correct. NA signifies
    that testing could not be conducted due to the fact that the context length limit
    is exceeded.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：处理最短路径问题的大规模图形表现。x/20表示在20个测试样本中，有x个样本正确。NA表示由于超出上下文长度限制，无法进行测试。
- en: '| Graph Size | 100 | 200 | 500 | 1000 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 图形大小 | 100 | 200 | 500 | 1000 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Graphwiz (LLaMA 2-7B) | 0/20 | 0/20 | NA | NA |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| Graphwiz (LLaMA 2-7B) | 0/20 | 0/20 | NA | NA |'
- en: '| Graphwiz (LLaMA 2-7B-DPO) | 0/20 | 0/20 | NA | NA |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Graphwiz (LLaMA 2-7B-DPO) | 0/20 | 0/20 | NA | NA |'
- en: '| Chatgpt-3.5-turbo-16k | 0/20 | 0/20 | 0/20 | 0/20 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Chatgpt-3.5-turbo-16k | 0/20 | 0/20 | 0/20 | 0/20 |'
- en: '| Chatgpt-4-32k | 0/20 | 1/20 | 0/20 | 0/20 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Chatgpt-4-32k | 0/20 | 1/20 | 0/20 | 0/20 |'
- en: '| GraphAgent-Reasoner | 20/20 | 20/20 | 20/20 | 18/20 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GraphAgent-Reasoner | 20/20 | 20/20 | 20/20 | 18/20 |'
- en: We see the two GraphWiz models fine-tuned on the LLaMA2-7B (Touvron et al.,
    [2023](https://arxiv.org/html/2410.05130v1#bib.bib28)) base model are unable to
    handle graphs with 500 or more nodes due to the context length limitation (the
    context length limit for Llama2 is 4096 tokens). Although ChatGPT-3.5-turbo-16k
    and ChatGPT-4-32k can manage longer contexts, they output wrong answers in almost
    all test samples, with only ChatGPT-4-32k being correct in one 200 nodes test
    sample. In contrast, GAR maintains a high accuracy in large-scale graph, only
    failed in two 1000-node test samples, further demonstrating its robust scalability.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，基于LLaMA2-7B（Touvron等人，[2023](https://arxiv.org/html/2410.05130v1#bib.bib28)）的两个GraphWiz模型经过微调后，无法处理500个或更多节点的图形，这是由于上下文长度的限制（Llama2的上下文长度限制为4096个tokens）。尽管ChatGPT-3.5-turbo-16k和ChatGPT-4-32k可以处理更长的上下文，但它们几乎在所有测试样本中都给出了错误的答案，仅有ChatGPT-4-32k在一个包含200个节点的测试样本中给出了正确答案。相比之下，GAR在大规模图形中保持了较高的准确性，仅在两个1000节点的测试样本中失败，进一步证明了其强大的可扩展性。
- en: '5.3 Experiment 3: Case Study'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 实验3：案例研究
- en: 'In this experiment, we explore the application of two graph reasoning models,
    Graphwiz and GAR, in real-world graph reasoning scenarios. We present a case study
    of webpage importance analysis in Figure [5](https://arxiv.org/html/2410.05130v1#S5.F5
    "Figure 5 ‣ 5.3 Experiment 3: Case Study ‣ 5 Experiments ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '在本实验中，我们探讨了两种图形推理模型Graphwiz和GAR在现实世界图形推理场景中的应用。我们展示了网页重要性分析的案例研究，见图[5](https://arxiv.org/html/2410.05130v1#S5.F5
    "Figure 5 ‣ 5.3 Experiment 3: Case Study ‣ 5 Experiments ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents")。'
- en: '![Refer to caption](img/fb875a7ec1321ab558548cc6ff832773.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/fb875a7ec1321ab558548cc6ff832773.png)'
- en: 'Figure 5: The importance analysis in webpage network. While the GraphWiz fails
    due to incorrect graph assessments, GAR correctly uses the PageRank algorithm
    to identify nodes 16, 14, and 5 as the most important.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：网页网络中的重要性分析。虽然GraphWiz由于图形评估错误而失败，但GAR正确使用PageRank算法识别了节点16、14和5为最重要的节点。
- en: 'Although GraphWiz performed well on fine-tuned tasks, it exhibits severe overfitting
    when faced with real-world graph problems, failing to apply the graph reasoning
    knowledge learned during the fine-tuning phase. Since GraphWiz uses a consistent
    graph node description, the sentence "The nodes are numbered from 0 to …" appears
    across all datasets during the mixed-task instruction tuning. When the actual
    problem has nodes numbered from 1 to 20, it still assumes the existence of node
    0\. As a result, both GraphWiz models first output that the graph has 21 nodes
    and an incorrect number of edges. Furthermore, neither of the two GraphWiz models
    recognizes that this is a problem associated with web page importance ranking.
    Instead, they approach it as the bipartite graph check or topological sort problems
    they had been fine-tuned on. Additionally, neither model generates an explicit
    and correct reasoning path. These observations indicate that there is still a
    significant gap between excelling in classic graph reasoning tasks and effectively
    solving real-world graph reasoning problems. In contrast, GAR correctly identifies
    that the problem should be solved using knowledge related to PageRank (Yang et al.,
    [2024](https://arxiv.org/html/2410.05130v1#bib.bib33)) and designs an algorithm
    that adhered to the distributed paradigm (Note: the distributed algorithm library
    does not contain a PageRank algorithm template). GAR then assigns the algorithm
    to agent nodes for execution, ultimately obtaining the PageRank value for each
    node and arriving at the correct conclusion. Through the distributed paradigm,
    GAR effectively bridges the powerful knowledge learned by LLMs with the solving
    of real-world graph reasoning problems, which enables it to flexibly handle practical
    issues in a distributed manner. This case study demonstrates the feasibility of
    using GAR to solve real-world graph reasoning problems, indicating its substantial
    practical applicability and offering researchers and practitioners a powerful
    framework to address such tasks.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管GraphWiz在精调任务中表现良好，但在面对真实世界的图形问题时，它表现出严重的过拟合，未能应用在精调阶段学习的图形推理知识。由于GraphWiz使用一致的图节点描述，在混合任务指令调优过程中，句子“节点从0到…编号”出现在所有数据集中。当实际问题中的节点编号为1到20时，它仍然假设存在节点0。因此，两个GraphWiz模型首先输出图有21个节点和不正确的边数。此外，两个GraphWiz模型都没有识别出这是与网页重要性排名相关的问题，而是将其视为他们在精调过程中接触过的二分图检查或拓扑排序问题。另外，两个模型都没有生成明确且正确的推理路径。这些观察结果表明，在经典的图推理任务中表现出色与有效解决现实世界的图推理问题之间仍存在显著差距。相比之下，GAR正确识别出该问题应使用与PageRank相关的知识进行解决（Yang等人，[2024](https://arxiv.org/html/2410.05130v1#bib.bib33)），并设计了一种遵循分布式范式的算法（注：分布式算法库中不包含PageRank算法模板）。GAR随后将该算法分配给代理节点进行执行，最终获得每个节点的PageRank值，并得出正确的结论。通过分布式范式，GAR有效地将LLM学习到的强大知识与解决现实世界的图推理问题相结合，从而使其能够以分布式方式灵活地处理实际问题。本案例研究展示了使用GAR解决现实世界图推理问题的可行性，表明其具有显著的实际应用性，并为研究人员和实践者提供了一个强大的框架来应对此类任务。
- en: 6 Conclusion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'We first summarize three key issues faced by existing LLMs in graph reasoning
    tasks: limited graph scale, poor performance, and the lack of explicit reasoning
    paths. We then reflect on the limitations of a single LLM in addressing graph
    reasoning problems, such as the graph structures being too complex to memorize
    and understand and the overwhelming information in real-world graph reasoning
    scenarios. To address these challenges, we propose GraphAgent-Reasoner, a framework
    based on multi-agent collaboration to solve graph reasoning problems. This framework
    demonstrates superior accuracy and scalability, significantly surpassing existing
    closed-source and fine-tuned open-source models. Our experiments show its robust
    scalability, maintaining high accuracy on large graphs (up to 1,000 nodes). Our
    case study on webpage importance analysis further illustrates its capability to
    handle real-world graph reasoning problems. Future work will focus on designing
    more accurate and scalable LLM-based multi-agent graph reasoning frameworks, aiming
    to apply them to larger and more complex real-world reasoning scenarios.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先总结了现有 LLM 在图推理任务中面临的三个关键问题：图的规模有限、性能较差以及缺乏明确的推理路径。接着，我们反思了单一 LLM 在解决图推理问题中的局限性，例如图结构过于复杂，难以记忆和理解，以及现实世界图推理场景中的信息过载问题。为了应对这些挑战，我们提出了
    GraphAgent-Reasoner，一个基于多智能体协作解决图推理问题的框架。该框架展示了优越的准确性和可扩展性，显著超越了现有的闭源和微调开源模型。我们的实验表明，GraphAgent-Reasoner
    展现了强大的可扩展性，在大规模图（最多 1,000 个节点）上仍能保持高准确性。我们在网页重要性分析中的案例研究进一步展示了它处理现实世界图推理问题的能力。未来的工作将集中于设计更精确和可扩展的基于
    LLM 的多智能体图推理框架，旨在将其应用于更大、更复杂的现实世界推理场景。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, et al. Language models are few-shot learners. In Hugo Larochelle,
    Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
    *Advances in Neural Information Processing Systems 33: Annual Conference on Neural
    Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual*,
    2020. URL [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）汤姆·布朗、本杰明·曼、尼克·赖德、梅拉妮·萨比亚、贾里德·D·卡普兰、普拉夫拉·达里瓦尔、阿尔文·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯凯尔等。语言模型是少样本学习者。在
    Hugo Larochelle、Marc’Aurelio Ranzato、Raia Hadsell、Maria-Florina Balcan 和 Hsuan-Tien
    Lin（编辑），*神经信息处理系统进展 33：2020年神经信息处理系统年会，NeurIPS 2020，2020年12月6日至12日，虚拟*，2020年。网址
    [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)。
- en: 'Chai et al. (2023) Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai
    Hu, Xuanwen Huang, and Yang Yang. Graphllm: Boosting graph reasoning ability of
    large language model. *CoRR*, abs/2310.05845, 2023. URL [https://doi.org/10.48550/arXiv.2310.05845](https://doi.org/10.48550/arXiv.2310.05845).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai 等人（2023）齐伟·柴、天杰·张、梁·吴、凯桥·韩、晓海·胡、轩文·黄和杨·杨。Graphllm：提升大语言模型的图推理能力。*CoRR*，abs/2310.05845，2023年。网址
    [https://doi.org/10.48550/arXiv.2310.05845](https://doi.org/10.48550/arXiv.2310.05845)。
- en: 'Chan et al. (2024) Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue,
    Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better llm-based evaluators
    through multi-agent debate. In *The Twelfth International Conference on Learning
    Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net,
    2024. URL [https://openreview.net/forum?id=FQepisCUWu](https://openreview.net/forum?id=FQepisCUWu).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan 等人（2024）志敏·陈、伟泽·陈、宇生·苏、建轩·余、伟·薛、尚杭·张、杰·傅和志远·刘。Chateval：通过多智能体辩论实现更好的基于
    LLM 的评估器。在 *第十二届国际学习表示会议，ICLR 2024，奥地利维也纳，2024年5月7日至11日*。OpenReview.net，2024年。网址
    [https://openreview.net/forum?id=FQepisCUWu](https://openreview.net/forum?id=FQepisCUWu)。
- en: 'Chen et al. (2024a) Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. Graphwiz:
    An instruction-following language model for graph computational problems. In Ricardo
    Baeza-Yates and Francesco Bonchi (eds.), *Proceedings of the 30th ACM SIGKDD Conference
    on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain, August 25-29,
    2024*, pp.  353–364\. ACM, 2024a. URL [https://doi.org/10.1145/3637528.3672010](https://doi.org/10.1145/3637528.3672010).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2024a）陈诺、李雨涵、唐建衡和李佳。Graphwiz：一种用于图计算问题的遵循指令的语言模型。发表于里卡多·贝萨-耶茨和弗朗切斯科·邦奇（编），*第30届ACM
    SIGKDD知识发现与数据挖掘大会论文集，KDD 2024，西班牙巴塞罗那，2024年8月25-29日*，第353–364页。ACM，2024a年。网址 [https://doi.org/10.1145/3637528.3672010](https://doi.org/10.1145/3637528.3672010)。
- en: 'Chen et al. (2024b) Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, and
    Chuchu Fan. Scalable multi-robot collaboration with large language models: Centralized
    or decentralized systems? In *IEEE International Conference on Robotics and Automation,
    ICRA 2024, Yokohama, Japan, May 13-17, 2024*, pp.  4311–4317\. IEEE, 2024b. URL
    [https://doi.org/10.1109/ICRA57147.2024.10610676](https://doi.org/10.1109/ICRA57147.2024.10610676).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2024b）陈永超、雅各布·阿金、杨章、尼古拉斯·罗伊和范楚楚。大语言模型下可扩展的多机器人协作：集中式还是分布式系统？发表于*IEEE国际机器人与自动化会议，ICRA
    2024，日本横滨，2024年5月13-17日*，第4311–4317页。IEEE，2024b年。网址 [https://doi.org/10.1109/ICRA57147.2024.10610676](https://doi.org/10.1109/ICRA57147.2024.10610676)。
- en: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, and Jiliang Tang. Exploring
    the potential of large language models (llms)in learning on graphs. *SIGKDD Explor.*,
    25(2):42–61, 2023. URL [https://doi.org/10.1145/3655103.3655110](https://doi.org/10.1145/3655103.3655110).
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人（2023）陈志凯、毛海涛、李航、金伟、温洪智、魏晓驰、王帅强、尹大伟、范文祺、刘辉和唐吉亮。探索大语言模型（LLMs）在图学习中的潜力。*SIGKDD
    Explor.*，25(2):42–61，2023年。网址 [https://doi.org/10.1145/3655103.3655110](https://doi.org/10.1145/3655103.3655110)。
- en: 'Creswell et al. (2023) Antonia Creswell, Murray Shanahan, and Irina Higgins.
    Selection-inference: Exploiting large language models for interpretable logical
    reasoning. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/forum?id=3Pf3Wg6o-A4](https://openreview.net/forum?id=3Pf3Wg6o-A4).'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克雷斯威尔等人（2023）安东尼娅·克雷斯威尔、穆雷·沙纳汉和伊琳娜·希金斯。选择推理：利用大语言模型进行可解释的逻辑推理。发表于*第十一届国际学习表征会议，ICLR
    2023，卢旺达基加利，2023年5月1-5日*。OpenReview.net，2023年。网址 [https://openreview.net/forum?id=3Pf3Wg6o-A4](https://openreview.net/forum?id=3Pf3Wg6o-A4)。
- en: Dong et al. (2023) Yihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration
    code generation via chatgpt. *CoRR*, abs/2304.07590, 2023. URL [https://doi.org/10.48550/arXiv.2304.07590](https://doi.org/10.48550/arXiv.2304.07590).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 董等人（2023）董一弘、蒋雪、金志和李格。通过ChatGPT进行自协作代码生成。*CoRR*，abs/2304.07590，2023年。网址 [https://doi.org/10.48550/arXiv.2304.07590](https://doi.org/10.48550/arXiv.2304.07590)。
- en: 'Fatemi et al. (2024) Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. Talk
    like a graph: Encoding graphs for large language models. In *The Twelfth International
    Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,
    2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=IuXR1CCrSi](https://openreview.net/forum?id=IuXR1CCrSi).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 法泰米等人（2024）巴哈雷·法泰米、乔纳森·哈尔克罗和布莱恩·佩罗兹。像图一样谈话：为大语言模型编码图。发表于*第十二届国际学习表征会议，ICLR 2024，奥地利维也纳，2024年5月7-11日*。OpenReview.net，2024年。网址
    [https://openreview.net/forum?id=IuXR1CCrSi](https://openreview.net/forum?id=IuXR1CCrSi)。
- en: 'Gao et al. (2024) Dawei Gao, Zitao Li, Weirui Kuang, Xuchen Pan, Daoyuan Chen,
    Zhijian Ma, Bingchen Qian, Liuyi Yao, Lin Zhu, Chen Cheng, Hongzhu Shi, Yaliang
    Li, Bolin Ding, and Jingren Zhou. Agentscope: A flexible yet robust multi-agent
    platform. *CoRR*, abs/2402.14034, 2024. URL [https://doi.org/10.48550/arXiv.2402.14034](https://doi.org/10.48550/arXiv.2402.14034).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人（2024）高大伟、李子涛、匡伟瑞、潘旭晨、陈道元、马志坚、钱炳晨、姚立宇、朱琳、程晨、施洪柱、李亚亮、丁博霖和周景仁。AgentScope：一个灵活而稳健的多智能体平台。*CoRR*，abs/2402.14034，2024年。网址
    [https://doi.org/10.48550/arXiv.2402.14034](https://doi.org/10.48550/arXiv.2402.14034)。
- en: 'Guo et al. (2023) Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large
    language models understand graph structured data ? an empirical evaluation and
    benchmarking. *CoRR*, abs/2305.15066, 2023. URL [https://doi.org/10.48550/arXiv.2305.15066](https://doi.org/10.48550/arXiv.2305.15066).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人（2023）郭嘉彦、杜伦和刘恒宇。Gpt4graph：大语言模型能否理解图结构数据？一项实证评估与基准测试。*CoRR*，abs/2305.15066，2023年。网址
    [https://doi.org/10.48550/arXiv.2305.15066](https://doi.org/10.48550/arXiv.2305.15066)。
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V. Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model
    based multi-agents: A survey of progress and challenges. *CoRR*, abs/2402.01680,
    2024. URL [https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei,
    Nitesh V. Chawla, Olaf Wiest, 和 Xiangliang Zhang. 基于大型语言模型的多智能体：进展与挑战的调查。*CoRR*，abs/2402.01680，2024年。网址
    [https://doi.org/10.48550/arXiv.2402.01680](https://doi.org/10.48550/arXiv.2402.01680)。
- en: 'Hong et al. (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: Meta programming for A multi-agent collaborative framework. In *The Twelfth
    International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
    May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng
    Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang
    Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, 和 Jürgen Schmidhuber. MetaGPT：面向多智能体协作框架的元编程。在*第十二届国际学习表征大会，ICLR
    2024，奥地利维也纳，2024年5月7-11日*。OpenReview.net，2024年。网址 [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o)。
- en: Huang et al. (2024) Jin Huang, Xingjian Zhang, Qiaozhu Mei, and Jiaqi Ma. Can
    llms effectively leverage graph structural information through prompts, and why?
    *Trans. Mach. Learn. Res.*, 2024, 2024. URL [https://openreview.net/forum?id=L2jRavXRxs](https://openreview.net/forum?id=L2jRavXRxs).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2024) Jin Huang, Xingjian Zhang, Qiaozhu Mei, 和 Jiaqi Ma. 大型语言模型能否通过提示有效利用图结构信息，为什么？*机器学习研究会刊*，2024年，2024年。网址
    [https://openreview.net/forum?id=L2jRavXRxs](https://openreview.net/forum?id=L2jRavXRxs)。
- en: 'Huang et al. (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin
    Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, and
    Ting Liu. A survey on hallucination in large language models: Principles, taxonomy,
    challenges, and open questions. *CoRR*, abs/2311.05232, 2023. URL [https://doi.org/10.48550/arXiv.2311.05232](https://doi.org/10.48550/arXiv.2311.05232).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2023) Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng,
    Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, 和 Ting Liu.
    关于大型语言模型中的幻觉：原理、分类法、挑战和开放问题的调查。*CoRR*，abs/2311.05232，2023年。网址 [https://doi.org/10.48550/arXiv.2311.05232](https://doi.org/10.48550/arXiv.2311.05232)。
- en: 'Jiang & Luo (2022) Weiwei Jiang and Jiayun Luo. Graph neural network for traffic
    forecasting: A survey. *Expert Syst. Appl.*, 207:117921, 2022. URL [https://doi.org/10.1016/j.eswa.2022.117921](https://doi.org/10.1016/j.eswa.2022.117921).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang & Luo (2022) Weiwei Jiang 和 Jiayun Luo. 用于交通预测的图神经网络：一项调查。*专家系统应用*，207:117921，2022年。网址
    [https://doi.org/10.1016/j.eswa.2022.117921](https://doi.org/10.1016/j.eswa.2022.117921)。
- en: 'Liu et al. (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language
    models use long contexts. *Transactions of the Association for Computational Linguistics*,
    12:157–173, 2023. URL [https://api.semanticscholar.org/CorpusID:259360665](https://api.semanticscholar.org/CorpusID:259360665).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2023) Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele
    Bevilacqua, Fabio Petroni, 和 Percy Liang. 迷失在中间：语言模型如何使用长上下文。*计算语言学会会刊*，12:157–173，2023年。网址
    [https://api.semanticscholar.org/CorpusID:259360665](https://api.semanticscholar.org/CorpusID:259360665)。
- en: Madaan et al. (2022) Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham
    Neubig. Language models of code are few-shot commonsense learners. In Yoav Goldberg,
    Zornitsa Kozareva, and Yue Zhang (eds.), *Proceedings of the 2022 Conference on
    Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United
    Arab Emirates, December 7-11, 2022*, pp.  1384–1403\. Association for Computational
    Linguistics, 2022. URL [https://doi.org/10.18653/v1/2022.emnlp-main.90](https://doi.org/10.18653/v1/2022.emnlp-main.90).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人 (2022) Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, 和 Graham Neubig.
    代码的语言模型是少量样本常识学习者。在 Yoav Goldberg、Zornitsa Kozareva 和 Yue Zhang (编)，*2022年自然语言处理实证方法会议论文集，EMNLP
    2022，阿布扎比，阿联酋，2022年12月7-11日*，第1384–1403页。计算语言学会，2022年。网址 [https://doi.org/10.18653/v1/2022.emnlp-main.90](https://doi.org/10.18653/v1/2022.emnlp-main.90)。
- en: 'Mandi et al. (2024) Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic
    multi-robot collaboration with large language models. In *IEEE International Conference
    on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024*, pp. 
    286–299\. IEEE, 2024. URL [https://doi.org/10.1109/ICRA57147.2024.10610855](https://doi.org/10.1109/ICRA57147.2024.10610855).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandi等人 (2024) Zhao Mandi, Shreeya Jain 和 Shuran Song. Roco：与大型语言模型的辩证多机器人协作。在
    *IEEE国际机器人与自动化大会（ICRA 2024），2024年5月13日至17日，横滨，日本*，第286-299页。IEEE，2024年。网址 [https://doi.org/10.1109/ICRA57147.2024.10610855](https://doi.org/10.1109/ICRA57147.2024.10610855)。
- en: Meadows et al. (2023) Jordan Meadows, Marco Valentino, and André Freitas. Generating
    mathematical derivations with large language models. *CoRR*, abs/2307.09998, 2023.
    URL [https://doi.org/10.48550/arXiv.2307.09998](https://doi.org/10.48550/arXiv.2307.09998).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meadows等人 (2023) Jordan Meadows, Marco Valentino 和 André Freitas. 使用大型语言模型生成数学推导。*CoRR*，abs/2307.09998，2023年。网址
    [https://doi.org/10.48550/arXiv.2307.09998](https://doi.org/10.48550/arXiv.2307.09998)。
- en: Meng et al. (2024a) Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng,
    Xue Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin, and Jingren Zhou. A survey of distributed
    graph algorithms on massive graphs. *CoRR*, abs/2404.06037, 2024a. URL [https://doi.org/10.48550/arXiv.2404.06037](https://doi.org/10.48550/arXiv.2404.06037).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng等人 (2024a) Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng, Xue
    Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin 和 Jingren Zhou. 大规模图上的分布式图算法综述。*CoRR*，abs/2404.06037，2024a年。网址
    [https://doi.org/10.48550/arXiv.2404.06037](https://doi.org/10.48550/arXiv.2404.06037)。
- en: Meng et al. (2024b) Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng,
    Xue Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin, and Jingren Zhou. A survey of distributed
    graph algorithms on massive graphs. *CoRR*, abs/2404.06037, 2024b. URL [https://doi.org/10.48550/arXiv.2404.06037](https://doi.org/10.48550/arXiv.2404.06037).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng等人 (2024b) Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng, Xue
    Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin 和 Jingren Zhou. 大规模图上的分布式图算法综述。*CoRR*，abs/2404.06037，2024b年。网址
    [https://doi.org/10.48550/arXiv.2404.06037](https://doi.org/10.48550/arXiv.2404.06037)。
- en: 'Motie & Raahemi (2024) Soroor Motie and Bijan Raahemi. Financial fraud detection
    using graph neural networks: A systematic review. *Expert Syst. Appl.*, 240:122156,
    2024. URL [https://doi.org/10.1016/j.eswa.2023.122156](https://doi.org/10.1016/j.eswa.2023.122156).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Motie 和 Raahemi (2024) Soroor Motie 和 Bijan Raahemi. 使用图神经网络的金融欺诈检测：一项系统评审。*Expert
    Syst. Appl.*，240：122156，2024年。网址 [https://doi.org/10.1016/j.eswa.2023.122156](https://doi.org/10.1016/j.eswa.2023.122156)。
- en: OpenAI (2023) OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
    URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. GPT-4技术报告。*CoRR*，abs/2303.08774，2023年。网址 [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)。
- en: 'Perozzi et al. (2024) Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin,
    Seyed Mehran Kazemi, Rami Al-Rfou, and Jonathan Halcrow. Let your graph do the
    talking: Encoding structured data for llms. *CoRR*, abs/2402.05862, 2024. URL
    [https://doi.org/10.48550/arXiv.2402.05862](https://doi.org/10.48550/arXiv.2402.05862).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perozzi等人 (2024) Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin,
    Seyed Mehran Kazemi, Rami Al-Rfou 和 Jonathan Halcrow. 让你的图表发声：为大型语言模型编码结构化数据。*CoRR*，abs/2402.05862，2024年。网址
    [https://doi.org/10.48550/arXiv.2402.05862](https://doi.org/10.48550/arXiv.2402.05862)。
- en: 'Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,
    Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li,
    Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development.
    In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), *Proceedings of the 62nd
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024*, pp.  15174–15186\.
    Association for Computational Linguistics, 2024. URL [https://doi.org/10.18653/v1/2024.acl-long.810](https://doi.org/10.18653/v1/2024.acl-long.810).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian等人 (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao
    Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan
    Liu 和 Maosong Sun. Chatdev：软件开发的通信代理。在 Lun-Wei Ku, Andre Martins 和 Vivek Srikumar
    (编)，《*第62届计算语言学协会年会论文集（第1卷：长篇论文），ACL 2024，2024年8月11日至16日，泰国曼谷*》，第15174-15186页。计算语言学协会，2024年。网址
    [https://doi.org/10.18653/v1/2024.acl-long.810](https://doi.org/10.18653/v1/2024.acl-long.810)。
- en: Stokes et al. (2020) Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin,
    Andres Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A
    Carfrae, Zohar Bloom-Ackermann, et al. A deep learning approach to antibiotic
    discovery. *Cell*, 180(4):688–702, 2020.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stokes等人 (2020) Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres
    Cubillos-Ruiz, Nina M Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae,
    Zohar Bloom-Ackermann 等人. 一种用于抗生素发现的深度学习方法。*Cell*，180(4)：688-702，2020年。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    et al. Llama 2: Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288,
    2023. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图弗龙等人（2023）雨果·图弗龙、路易·马丁、凯文·斯通、彼得·阿尔伯特等。Llama 2：开放基础和微调聊天模型。*CoRR*，abs/2307.09288，2023年。网址
    [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288)。
- en: 'Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach,
    Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), *Advances in Neural
    Information Processing Systems 30: Annual Conference on Neural Information Processing
    Systems 2017, December 4-9, 2017, Long Beach, CA, USA*, pp.  5998–6008, 2017.
    URL [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 瓦斯瓦尼等人（2017）阿希什·瓦斯瓦尼、诺姆·沙泽尔、尼基·帕尔马尔、雅各布·乌斯科雷特、利昂·琼斯、艾丹·N·戈麦斯、卢卡兹·凯泽和伊利亚·波洛苏欣。注意力机制是你所需要的一切。在伊莎贝尔·盖永、乌尔里克·冯·卢克斯堡、萨米·本吉奥、汉娜·M·沃拉赫、罗布·费格斯、S·V·N·维什瓦纳坦和罗曼·加内特（编），*神经信息处理系统进展
    30：2017年神经信息处理系统年会，2017年12月4日至9日，美国长滩*，第5998-6008页，2017年。网址 [https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)。
- en: 'Wang et al. (2023) Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang
    Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?
    In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey
    Levine (eds.), *Advances in Neural Information Processing Systems 36: Annual Conference
    on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
    USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/622afc4edf2824a1b6aaf5afe153fa93-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/622afc4edf2824a1b6aaf5afe153fa93-Abstract-Conference.html).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023）王恒、冯尚斌、贺天星、谭兆轩、韩晓创和尤莉娅·茨维特科夫。语言模型能否解决自然语言中的图问题？在爱丽丝·欧、特里斯坦·诺曼、阿米尔·格洛伯森、凯特·塞恩科、莫里茨·哈特和谢尔盖·莱文（编），*神经信息处理系统进展
    36：2023年神经信息处理系统年会，NeurIPS 2023，美国新奥尔良*，2023年12月10日至16日，2023年。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/622afc4edf2824a1b6aaf5afe153fa93-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/622afc4edf2824a1b6aaf5afe153fa93-Abstract-Conference.html)。
- en: 'Wei et al. (2024) Yanbin Wei, Shuai Fu, Weisen Jiang, James T. Kwok, and Yu Zhang.
    Gita: Graph to visual and textual integration for vision-language graph reasoning.
    2024. URL [https://api.semanticscholar.org/CorpusID:267413180](https://api.semanticscholar.org/CorpusID:267413180).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2024）魏彦斌、付帅、姜伟森、詹姆斯·T·郭和张宇。Gita：图像与文本整合的视觉语言图推理。2024年。网址 [https://api.semanticscholar.org/CorpusID:267413180](https://api.semanticscholar.org/CorpusID:267413180)。
- en: 'Xiong et al. (2023) Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin.
    Examining inter-consistency of large language models collaboration: An in-depth
    analysis via debate. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), *Findings
    of the Association for Computational Linguistics: EMNLP 2023, Singapore, December
    6-10, 2023*, pp.  7572–7590\. Association for Computational Linguistics, 2023.
    URL [https://doi.org/10.18653/v1/2023.findings-emnlp.508](https://doi.org/10.18653/v1/2023.findings-emnlp.508).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熊等人（2023）熊凯、丁晓、曹逸鑫、刘婷和秦兵。通过辩论对大型语言模型协作的内部一致性进行深入分析。在侯达·布阿莫尔、胡安·皮诺和卡莉卡·巴利（编），*计算语言学协会成果：EMNLP
    2023，新加坡，2023年12月6日至10日*，第7572-7590页。计算语言学协会，2023年。网址 [https://doi.org/10.18653/v1/2023.findings-emnlp.508](https://doi.org/10.18653/v1/2023.findings-emnlp.508)。
- en: 'Yang et al. (2024) Mingji Yang, Hanzhi Wang, Zhewei Wei, Sibo Wang, and Ji-Rong
    Wen. Efficient algorithms for personalized pagerank computation: A survey. *IEEE
    Trans. Knowl. Data Eng.*, 36(9):4582–4602, 2024. URL [https://doi.org/10.1109/TKDE.2024.3376000](https://doi.org/10.1109/TKDE.2024.3376000).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2024）杨名基、王汉志、魏哲伟、王思博、温继荣。个性化 PageRank 计算的高效算法：综述。*IEEE 知识与数据工程汇刊*，36(9)：4582–4602，2024年。网址
    [https://doi.org/10.1109/TKDE.2024.3376000](https://doi.org/10.1109/TKDE.2024.3376000)。
- en: 'Yao et al. (2023) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R. Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in
    language models. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/forum?id=WE_vluYUL-X](https://openreview.net/forum?id=WE_vluYUL-X).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2023）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    R. Narasimhan, 和 Yuan Cao。React：在语言模型中协同推理与行动。在 *第十一届国际学习表征会议，ICLR 2023，卢旺达基加利，2023年5月1日至5日*。OpenReview.net,
    2023。网址 [https://openreview.net/forum?id=WE_vluYUL-X](https://openreview.net/forum?id=WE_vluYUL-X)。
- en: Zhang et al. (2024) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models. In *The Twelfth International Conference
    on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net,
    2024. URL [https://openreview.net/forum?id=EnXJfQqy0K](https://openreview.net/forum?id=EnXJfQqy0K).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2024）Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du,
    Joshua B. Tenenbaum, Tianmin Shu, 和 Chuang Gan。用大型语言模型模块化构建合作体化代理。在 *第十二届国际学习表征会议，ICLR
    2024，维也纳，奥地利，2024年5月7日至11日*。OpenReview.net, 2024。网址 [https://openreview.net/forum?id=EnXJfQqy0K](https://openreview.net/forum?id=EnXJfQqy0K)。
- en: 'Zheng et al. (2024) Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han,
    and Le Sun. Executing natural language-described algorithms with large language
    models: An investigation. In Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,
    Alessandro Lenci, Sakriani Sakti, and Nianwen Xue (eds.), *Proceedings of the
    2024 Joint International Conference on Computational Linguistics, Language Resources
    and Evaluation, LREC/COLING 2024, 20-25 May, 2024, Torino, Italy*, pp.  6752–6837\.
    ELRA and ICCL, 2024. URL [https://aclanthology.org/2024.lrec-main.596](https://aclanthology.org/2024.lrec-main.596).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人（2024）Xin Zheng, Qiming Zhu, Hongyu Lin, Yaojie Lu, Xianpei Han, 和 Le
    Sun。使用大型语言模型执行自然语言描述的算法：一项研究。在 Nicoletta Calzolari, Min-Yen Kan, Véronique Hoste,
    Alessandro Lenci, Sakriani Sakti, 和 Nianwen Xue（编辑），*2024年联合国际计算语言学会议、语言资源与评估会议论文集，LREC/COLING
    2024，2024年5月20-25日，意大利都灵*，第6752–6837页。ELRA 和 ICCL, 2024。网址 [https://aclanthology.org/2024.lrec-main.596](https://aclanthology.org/2024.lrec-main.596)。
- en: Appendix A Distributed algorithms under the distributed paradigm
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 分布式范式下的分布式算法
- en: A.1 Example of distributed algorithms in distributed algorithm library
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 分布式算法库中的分布式算法示例
- en: 'Shortest Path: See Figure [6](https://arxiv.org/html/2410.05130v1#A1.F6 "Figure
    6 ‣ A.1 Example of distributed algorithms in distributed algorithm library ‣ Appendix
    A Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '最短路径: 参见图[6](https://arxiv.org/html/2410.05130v1#A1.F6 "Figure 6 ‣ A.1 Example
    of distributed algorithms in distributed algorithm library ‣ Appendix A Distributed
    algorithms under the distributed paradigm ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents")。'
- en: 'Connectivity: See Figure [7](https://arxiv.org/html/2410.05130v1#A1.F7 "Figure
    7 ‣ A.1 Example of distributed algorithms in distributed algorithm library ‣ Appendix
    A Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '连通性: 参见图[7](https://arxiv.org/html/2410.05130v1#A1.F7 "Figure 7 ‣ A.1 Example
    of distributed algorithms in distributed algorithm library ‣ Appendix A Distributed
    algorithms under the distributed paradigm ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents")。'
- en: '![Refer to caption](img/2ed14e1d84045c77d48d6b2c49b42c2a.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/2ed14e1d84045c77d48d6b2c49b42c2a.png)'
- en: 'Figure 6: Distributed algorithm for shortest path problem under the distributed
    paradigm.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 分布式范式下的最短路径问题的分布式算法。'
- en: '![Refer to caption](img/23431a7b84198f7878f1590c8279487b.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/23431a7b84198f7878f1590c8279487b.png)'
- en: 'Figure 7: Distributed algorithm for connectivity problem under the distributed
    paradigm.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 分布式范式下的连通性问题的分布式算法。'
- en: A.2 Example of distributed algorithms designed by the Master LLM
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 大型语言模型设计的分布式算法示例
- en: 'PageRank: See Figure [8](https://arxiv.org/html/2410.05130v1#A1.F8 "Figure
    8 ‣ A.2 Example of distributed algorithms designed by the Master LLM ‣ Appendix
    A Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 'PageRank: 参见图[8](https://arxiv.org/html/2410.05130v1#A1.F8 "Figure 8 ‣ A.2
    Example of distributed algorithms designed by the Master LLM ‣ Appendix A Distributed
    algorithms under the distributed paradigm ‣ Scalable and Accurate Graph Reasoning
    with LLM-based Multi-Agents")。'
- en: 'Hamilton Path: See Figure [9](https://arxiv.org/html/2410.05130v1#A1.F9 "Figure
    9 ‣ A.2 Example of distributed algorithms designed by the Master LLM ‣ Appendix
    A Distributed algorithms under the distributed paradigm ‣ Scalable and Accurate
    Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 哈密顿路径：见图[9](https://arxiv.org/html/2410.05130v1#A1.F9 "图9 ‣ A.2 分布式算法示例由Master
    LLM设计 ‣ 附录A 分布式范式下的分布式算法 ‣ 基于LLM的多智能体的可扩展和精确的图推理")。
- en: 'Subgraph Matching: See Figure [10](https://arxiv.org/html/2410.05130v1#A1.F10
    "Figure 10 ‣ A.2 Example of distributed algorithms designed by the Master LLM
    ‣ Appendix A Distributed algorithms under the distributed paradigm ‣ Scalable
    and Accurate Graph Reasoning with LLM-based Multi-Agents").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 子图匹配：见图[10](https://arxiv.org/html/2410.05130v1#A1.F10 "图10 ‣ A.2 分布式算法示例由Master
    LLM设计 ‣ 附录A 分布式范式下的分布式算法 ‣ 基于LLM的多智能体的可扩展和精确的图推理")。
- en: '![Refer to caption](img/780c3adb9c08499e900dffeafb93ff82.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/780c3adb9c08499e900dffeafb93ff82.png)'
- en: 'Figure 8: Distributed algorithm for pagerank calculation under the distributed
    paradigm.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：在分布式范式下的PageRank计算问题的分布式算法。
- en: '![Refer to caption](img/be982bc1d562074ee287bb30c472ef83.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/be982bc1d562074ee287bb30c472ef83.png)'
- en: 'Figure 9: Distributed algorithm for hamilton path problem under the distributed
    paradigm.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：在分布式范式下的哈密顿路径问题的分布式算法。
- en: '![Refer to caption](img/f7f43a6ea46e83ff5e5d169895890f65.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f7f43a6ea46e83ff5e5d169895890f65.png)'
- en: 'Figure 10: Distributed algorithm for subgraph matching problem under the distributed
    paradigm.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：在分布式范式下的子图匹配问题的分布式算法。
- en: Appendix B The GraphInstruct Dataset
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B GraphInstruct数据集
- en: The statistics and detailed information of GraphInstruct are shown in Table [3](https://arxiv.org/html/2410.05130v1#A2.T3
    "Table 3 ‣ Appendix B The GraphInstruct Dataset ‣ Scalable and Accurate Graph
    Reasoning with LLM-based Multi-Agents"). Hamilton Path and Subgraph Matching are
    NP-complete problems.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: GraphInstruct的统计信息和详细信息见表[3](https://arxiv.org/html/2410.05130v1#A2.T3 "表3 ‣
    附录B GraphInstruct数据集 ‣ 基于LLM的多智能体的可扩展和精确的图推理")。哈密顿路径和子图匹配是NP完全问题。
- en: 'Table 3: The detailed information of GraphInstruct dataset.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：GraphInstruct数据集的详细信息。
- en: '| Problem | Definition | Node Range | Test Size |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | 定义 | 节点范围 | 测试规模 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Cycle Detection | Detect if a given graph $\mathcal{G}$ contains any cycles.
    | [2, 100] | 400 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 环检测 | 检测给定图$\mathcal{G}$中是否包含任何环。 | [2, 100] | 400 |'
- en: '| Connectivity | Assess if two nodes $u$ and $v$ in a given graph $\mathcal{G}$
    are connected via a path. | [2, 100] | 400 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 连通性 | 判断在给定图$\mathcal{G}$中，两个节点$u$和$v$是否通过路径连接。 | [2, 100] | 400 |'
- en: '| Bipartite Graph Check | Judge if a given graph $\mathcal{G}$ is bipartite.
    | [2, 100] | 400 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 二分图检测 | 判断给定图$\mathcal{G}$是否是二分图。 | [2, 100] | 400 |'
- en: '| Topological Sort | Find a topological ordering of vertices in a directed
    acyclic graph $\mathcal{G}$. | [2, 50] | 400 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 拓扑排序 | 在有向无环图$\mathcal{G}$中，找到顶点的拓扑排序。 | [2, 50] | 400 |'
- en: '| Shortest Path | Compute the shortest path between two specific nodes $u$
    and $v$ in a given graph $\mathcal{G}$. | [2, 100] | 400 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 最短路径 | 计算给定图$\mathcal{G}$中两个特定节点$u$和$v$之间的最短路径。 | [2, 100] | 400 |'
- en: '| Maximum Triangle Sum | Find the maximum sum of weights for any connected
    triplet of vertices in a given graph $\mathcal{G}$. | [2, 25] | 400 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 最大三角形和 | 在给定图$\mathcal{G}$中，找出任意连接的三顶点三元组的权重最大和。 | [2, 25] | 400 |'
- en: '| Maximum Flow | Calculate the maximum flow from a source node $s$ to a sink
    node $t$ in a directed graph $\mathcal{G}$. | [2, 50] | 400 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 最大流 | 计算在有向图$\mathcal{G}$中，从源节点$s$到汇节点$t$的最大流量。 | [2, 50] | 400 |'
- en: '| Hamilton Path | Determine if a given graph $\mathcal{G}$ has a Hamiltonian
    path that visits each vertex exactly once. | [2, 50] | 400 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 哈密顿路径 | 确定给定图$\mathcal{G}$是否具有哈密顿路径，该路径恰好访问每个顶点一次。 | [2, 50] | 400 |'
- en: '| Subgraph Matching | Verify if there exists a subgraph in $\mathcal{G}$ that
    is isomorphic to a given graph $\mathcal{G}^{\prime}$. | [2, 30] | 400 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 子图匹配 | 验证在$\mathcal{G}$中是否存在一个与给定图$\mathcal{G}^{\prime}$同构的子图。 | [2, 30]
    | 400 |'
- en: Hamilton Path Execution Example.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 哈密顿路径执行示例。
- en: 'Problem Description:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 问题描述：
- en: 'Determine whether or not there is a Hamiltonian path in an undirected graph.
    In an undirected graph, (i,j) means that node i and node j are connected with
    an undirected edge. Given a graph, you need to output Yes or No, indicating whether
    there is a Hamiltonian path in the graph. Q: The nodes are numbered from 0 to
    5, and the edges are: (0, 3) (0, 1) (0, 2) (0, 4) (1, 5) (1, 4) (1, 2) (1, 3)
    (2, 4) (2, 5) (3, 5) (3, 4). Is there a Hamiltonian path in this graph?'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 判断一个无向图中是否存在哈密尔顿路径。在无向图中，(i,j)表示节点i和节点j之间有一条无向边。给定一个图，你需要输出“Yes”或“No”，表示图中是否存在哈密尔顿路径。问：节点编号从0到5，边为：(0,
    3) (0, 1) (0, 2) (0, 4) (1, 5) (1, 4) (1, 2) (1, 3) (2, 4) (2, 5) (3, 5) (3, 4)。该图中是否存在哈密尔顿路径？
- en: 'Execution Process:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 执行过程：
- en: '[PRE0]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Appendix C Execution examples of GraphAgent-Reasoner
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C GraphAgent-Reasoner的执行示例
- en: Shortest Path Execution Example.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径执行示例。
- en: 'Problem Description:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 问题描述：
- en: 'Find the shortest distance from a source node to other nodes in an undirected
    graph. In an undirected graph, (i,j,k) means that node i and node j are connected
    with an undirected edge with weight k. The graph has 8 nodes, and the edges are:
    (0,7,9) (0,1,7) (0,4,9) (1,7,1) (2,7,7) (2,6,5) (2,5,8) (3,5,9) (3,4,8) (3,6,1)
    (4,7,7) (4,5,6) (5,7,6). Give the weight of the shortest distance from node 1
    to other node.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 找到一个源节点到其他节点的最短距离。在无向图中，(i,j,k)表示节点i和节点j之间有一条带权重k的无向边。该图有8个节点，边为：(0,7,9) (0,1,7)
    (0,4,9) (1,7,1) (2,7,7) (2,6,5) (2,5,8) (3,5,9) (3,4,8) (3,6,1) (4,7,7) (4,5,6)
    (5,7,6)。给定节点1到其他节点的最短距离权重。
- en: 'Execution Process:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 执行过程：
- en: '[PRE1]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
