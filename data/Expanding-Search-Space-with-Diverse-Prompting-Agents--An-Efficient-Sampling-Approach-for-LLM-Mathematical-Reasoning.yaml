- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:06:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2025-01-11 12:06:57'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling
    Approach for LLM Mathematical Reasoning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展搜索空间与多样化提示代理：一种高效的LLM数学推理采样方法
- en: 来源：[https://arxiv.org/html/2410.09780/](https://arxiv.org/html/2410.09780/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.09780/](https://arxiv.org/html/2410.09780/)
- en: Gisang Lee^(1, 2), Sangwoo Park¹, Junyoung Park³, Andrew Chung³,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Gisang Lee^(1, 2), Sangwoo Park¹, Junyoung Park³, Andrew Chung³，
- en: Sieun Park⁴, Yoonah Park³, Byungju Kim²²²2, Min-gyu Cho²²²2
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Sieun Park⁴, Yoonah Park³, Byungju Kim²²²2, Min-gyu Cho²²²2
- en: ¹KAIST, ²Mathpresso Inc., ³Seoul National University,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹KAIST, ²Mathpresso公司, ³首尔国立大学，
- en: ⁴Goldsmiths, University of London
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴伦敦大学Goldsmiths学院
- en: '{bobopack, swgger}@kaist.ac.kr, {jyp0314, aschung01, wisdomsword21}@snu.ac.kr,'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{bobopack, swgger}@kaist.ac.kr, {jyp0314, aschung01, wisdomsword21}@snu.ac.kr,'
- en: sieunpark77@gmail.com, {peyton.kim, mike.cho}@mathpresso.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: sieunpark77@gmail.com, {peyton.kim, mike.cho}@mathpresso.com
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have exhibited remarkable capabilities in many
    complex tasks including mathematical reasoning. However, traditional approaches
    heavily rely on ensuring self-consistency within single prompting method, which
    limits the exploration of diverse problem-solving strategies. This study addresses
    these limitations by performing an experimental analysis of distinct prompting
    methods within the domain of mathematical reasoning. Our findings demonstrate
    that each method explores a distinct search space, and this differentiation becomes
    more evident with increasing problem complexity. To leverage this phenomenon,
    we applied efficient sampling process that uniformly combines samples from these
    diverse methods, which not only expands the maximum search space but achieves
    higher performance with fewer runs compared to single methods. Especially, within
    the subset of difficult questions of MATH dataset named MATH-hard, The maximum
    search space was achieved while utilizing approximately 43% fewer runs than single
    methods on average. These findings highlight the importance of integrating diverse
    problem-solving strategies to enhance the reasoning abilities of LLMs.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在许多复杂任务中展现了显著的能力，包括数学推理。然而，传统方法严重依赖于确保单一提示方法中的自一致性，这限制了多样化问题解决策略的探索。本研究通过在数学推理领域对不同提示方法进行实验分析，解决了这些局限性。我们的研究结果表明，每种方法探索了不同的搜索空间，且随着问题复杂度的增加，这种差异变得更加明显。为了利用这一现象，我们应用了高效的采样过程，均匀地结合来自这些多样化方法的样本，这不仅扩展了最大搜索空间，而且在与单一方法相比，使用更少的运行次数实现了更高的性能。特别是在名为MATH数据集的难题子集MATH-hard中，利用大约减少43%运行次数的情况下，达到了最大搜索空间。这些发现强调了整合多样化问题解决策略以提升LLM推理能力的重要性。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancements in large language models (LLMs) have significantly enhanced
    their reasoning abilities, particularly in mathematical reasoning and code generation.
    High-performing models such as GPT-4o (OpenAI, [2024](https://arxiv.org/html/2410.09780v1#bib.bib8)),
    Claude Opus (Claude, [2024](https://arxiv.org/html/2410.09780v1#bib.bib2)) have
    demonstrated their capabilities in these challenging domains, showcasing their
    advanced performance. These models are typically employed through step-by-step
    natural language reasoning methodologies named Chain-of-Thought (CoT) to ensure
    the validity and accuracy of their solutions (Wei et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib11)).
    Particularly in solving math problems, existing approaches either focus on validating
    the logical sequence during the solution process (Zhang et al., [2024](https://arxiv.org/html/2410.09780v1#bib.bib12);
    Zihao et al., [2024](https://arxiv.org/html/2410.09780v1#bib.bib16); Zhou et al.,
    [2024](https://arxiv.org/html/2410.09780v1#bib.bib15)), seek verification support
    for complex calculations (Chen et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib1);
    Zhou et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib14); Zhong et al.,
    [2024](https://arxiv.org/html/2410.09780v1#bib.bib13)), or aim to secure both
    logic validation and calculation accuracy (Gou et al., [2024](https://arxiv.org/html/2410.09780v1#bib.bib5)).
    A common feature of these methods is the use of sampling and voting processes
    to achieve self-consistency (CoT-SC) by generating multiple solutions (Wang et al.,
    [2023](https://arxiv.org/html/2410.09780v1#bib.bib10)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大型语言模型（LLMs）的进展显著增强了它们的推理能力，尤其是在数学推理和代码生成方面。像GPT-4o（OpenAI，[2024](https://arxiv.org/html/2410.09780v1#bib.bib8)）、Claude
    Opus（Claude，[2024](https://arxiv.org/html/2410.09780v1#bib.bib2)）这样的高性能模型在这些挑战性领域展示了其能力，展现了其先进的表现。这些模型通常通过逐步的自然语言推理方法——称为思维链（Chain-of-Thought，CoT）来确保解决方案的有效性和准确性（Wei
    et al.，[2023](https://arxiv.org/html/2410.09780v1#bib.bib11)）。特别是在解决数学问题时，现有的方法要么专注于验证解决过程中的逻辑顺序（Zhang
    et al.，[2024](https://arxiv.org/html/2410.09780v1#bib.bib12); Zihao et al.，[2024](https://arxiv.org/html/2410.09780v1#bib.bib16);
    Zhou et al.，[2024](https://arxiv.org/html/2410.09780v1#bib.bib15)），要么寻求对复杂计算的验证支持（Chen
    et al.，[2023](https://arxiv.org/html/2410.09780v1#bib.bib1); Zhou et al.，[2023](https://arxiv.org/html/2410.09780v1#bib.bib14);
    Zhong et al.，[2024](https://arxiv.org/html/2410.09780v1#bib.bib13)），要么旨在确保逻辑验证和计算准确性（Gou
    et al.，[2024](https://arxiv.org/html/2410.09780v1#bib.bib5)）。这些方法的一个共同特点是使用抽样和投票过程，通过生成多个解决方案来实现自我一致性（CoT-SC）（Wang
    et al.，[2023](https://arxiv.org/html/2410.09780v1#bib.bib10)）。
- en: '![Refer to caption](img/fb92d02932bc821e5ac562fd7e78c539.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/fb92d02932bc821e5ac562fd7e78c539.png)'
- en: 'Figure 1: Line graph of maximum search space’s accuracy achieved by sampling
    21 runs per methods. The three grey horizontal lines represent the upper bound
    values within a single method. The star markers indicate the points at which these
    upper bound values were achieved using our proposed Uniform Sampling method. It
    can be observed that for text, code, and CR, the same upper-bound was reached
    while utilizing approximately 48%, 45%, and 35% fewer runs, respectively.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：每种方法通过21次运行采样得到的最大搜索空间准确度的折线图。三条灰色水平线代表单一方法中的上限值。星标表示在使用我们提出的统一采样方法时，达到这些上限值的点。从图中可以看出，在处理文本、代码和CR时，分别使用大约48%、45%和35%更少的运行次数就达到了相同的上限。
- en: While these methods have been effective in verifying the solutions provided
    by LLMs and enhancing their reliability, they heavily rely on temperature adjustments
    to increase the diversity of problem-solving approaches. This reliance on self-consistency
    within their own generated solutions limits their ability to explore a wider range
    of problem-solving strategies. In contrast, human problem solvers invest significant
    effort not only in verifying the validity and accuracy of their calculations but
    also in exploring many potential solutions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些方法在验证LLMs提供的解决方案并增强其可靠性方面有效，但它们严重依赖温度调整来增加问题解决方法的多样性。这种对自身生成解决方案中的自我一致性的依赖限制了它们探索更广泛问题解决策略的能力。相比之下，人类问题解决者不仅投入大量精力验证计算的有效性和准确性，而且还会探索多种潜在解决方案。
- en: Recent efforts in the field of LLM’s high reasoning have focused on integrating
    diverse agentic problem-solving methods to address these limitations (Du et al.,
    [2023](https://arxiv.org/html/2410.09780v1#bib.bib4); Liu et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib7)).
    Although these studies have shown promising performance on benchmarks such as
    MATH (Hendrycks et al., [2021](https://arxiv.org/html/2410.09780v1#bib.bib6))
    and GSM8K (Cobbe et al., [2021](https://arxiv.org/html/2410.09780v1#bib.bib3)),
    they lack a comprehensive analysis of why different agents collectively achieve
    high performance. Furthermore, there is an absence of methodologies that explore
    how the unique characteristics of each approach can be effectively integrated,
    beyond merely improving performance metrics.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LLM高推理领域的研究集中在整合多样的智能体问题解决方法，以应对这些局限性（Du等， [2023](https://arxiv.org/html/2410.09780v1#bib.bib4);
    Liu等， [2023](https://arxiv.org/html/2410.09780v1#bib.bib7)）。尽管这些研究在MATH（Hendrycks等，
    [2021](https://arxiv.org/html/2410.09780v1#bib.bib6)）和GSM8K（Cobbe等， [2021](https://arxiv.org/html/2410.09780v1#bib.bib3)）等基准测试上取得了有希望的表现，但它们缺乏对不同智能体为何能共同实现高性能的全面分析。此外，尚缺乏探讨如何有效整合每种方法独特特征的研究，不仅仅是提高性能指标。
- en: Therefore, this study aims to address these gaps by performing an experimental
    analysis of the problem-solving strategies employed by various LLM agents within
    the domain of mathematical reasoning. Furthermore, we propose an efficient sampling
    process to effectively combine these diverse agents. Key observations obtained
    by experimental analysis and our contributions are as follows.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本研究旨在通过对不同LLM智能体在数学推理领域中采用的解决问题策略进行实验分析，来填补这些空白。此外，我们提出了一种高效的采样过程，能够有效地结合这些多样的智能体。通过实验分析获得的关键观察结果和我们的贡献如下。
- en: Observation
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 观察
- en: 'To specifically evaluate the high reasoning abilities of LLMs, we analyzed
    state-of-the-art methodologies on the MATH dataset, which requires higher capabilities
    than GSM-8K. We categorized the approaches into three main prompting methods:
    Text, Code, and CR (Cumulative Reasoning). We discovered that each method explores
    a distinct search space when generating correct answers, and this differentiation
    becomes more evident as the complexity of the problems increases.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体评估LLMs的高推理能力，我们分析了在MATH数据集上的最新方法，这要求的能力高于GSM-8K。我们将这些方法分为三种主要的提示方法：文本、代码和CR（累积推理）。我们发现，在生成正确答案时，每种方法探索了不同的搜索空间，而随着问题复杂性的增加，这种差异变得更加明显。
- en: Contribution
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献
- en: 'We observed that each prompting method explores a different search space, and
    this realization led us to an efficient sampling strategy. Instead of inefficiently
    generating multiple samples within a single method, we demonstrated that uniformly
    mixing samples from these distinct methods significantly increases the maximum
    search space. As shown in Figure [1](https://arxiv.org/html/2410.09780v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Expanding Search Space with Diverse Prompting Agents:
    An Efficient Sampling Approach for LLM Mathematical Reasoning"), within the MATH-hard
    subset, the maximum search space was achieved while utilizing approximately 43%
    fewer runs than single methods on average.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，每种提示方法探索了不同的搜索空间，这一认识促使我们提出了一种高效的采样策略。我们展示了，与其在单一方法中低效地生成多个样本，不如均匀混合来自这些不同方法的样本，这显著地增加了最大搜索空间。如图[1](https://arxiv.org/html/2410.09780v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Expanding Search Space with Diverse Prompting Agents:
    An Efficient Sampling Approach for LLM Mathematical Reasoning")所示，在MATH-hard子集内，利用比单一方法平均少约43%的运行次数就达到了最大搜索空间。'
- en: 2 Method
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: '![Refer to caption](img/ed0425f476268462b0259f643d1615e8.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ed0425f476268462b0259f643d1615e8.png)'
- en: 'Figure 2: Maximum search space for methods result on MATH- hard (* 280 test
    subset). From above, the Venn diagram’s $B\cup C\ -A$ represents the proportion
    of the search space that method A fails to explore.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：方法在MATH-hard（* 280个测试子集）上的最大搜索空间结果。从上图看，Venn图中的$B\cup C\ -A$表示方法A未能探索的搜索空间比例。
- en: 2.1 Expanding search space
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 扩展搜索空间
- en: 'Figure [2](https://arxiv.org/html/2410.09780v1#S2.F2 "Figure 2 ‣ 2 Method ‣
    Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach
    for LLM Mathematical Reasoning") shows a Venn diagram visualizing the maximum
    search space within the MATH-hard problems for the three prompting methods. We
    increased the sample sizes sequentially from (1,1,1) to (5,5,5) in intervals of
    4, and finally up to (21,21,21) to see if this phenomenon persisted. The results
    showed that as the sample sizes increased, the overlap in the center gray area,
    representing the shared search space, grew. Although the absolute size of each
    unique search space decreased, the proportion of the search space that any single
    method (Method A) could not explore $B\cup C\ -A$ remained within a certain bound.
    This demonstrated that even as the sample size k increased, the search spaces
    of each method remained robustly distinct.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2](https://arxiv.org/html/2410.09780v1#S2.F2 "Figure 2 ‣ 2 Method ‣ Expanding
    Search Space with Diverse Prompting Agents: An Efficient Sampling Approach for
    LLM Mathematical Reasoning")显示了一个维恩图，展示了三种提示方法在MATH-hard问题中的最大搜索空间。我们将样本大小按顺序从（1,1,1）增加到（5,5,5），间隔为4，最后增加到（21,21,21），以观察这一现象是否持续存在。结果显示，随着样本大小的增加，中心灰色区域的重叠部分（表示共享的搜索空间）增大。尽管每个独特搜索空间的绝对大小减小，但任何单一方法（方法A）无法探索的搜索空间比例$B\cup
    C\ -A$仍然保持在一定范围内。这表明，即使样本大小k增加，每种方法的搜索空间仍然保持稳健的独立性。'
- en: Prompting methods
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示方法
- en: 'We selected three prompting methods to analyze the differences in problem-solving
    approaches within the MATH dataset, building on the assumption that each method
    explores a distinct search space. We chose the following three prompting methods:
    (1) Text, (2) Code and (3) CR.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了三种提示方法来分析MATH数据集中问题解决方法的差异，基于每种方法探索不同搜索空间的假设。我们选择了以下三种提示方法：（1）文本，（2）代码和（3）CR。
- en: '1.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Text: As reported in Wei et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib11),
    this prompting method encourages natural language explanations using the Chain-of-Thought
    (CoT) approach. This serves as the base reasoning method of LLMs. The token cost
    for CoT-SC is used as the baseline.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文本：正如魏等人[2023](https://arxiv.org/html/2410.09780v1#bib.bib11)所报告的，这种提示方法鼓励使用链式推理（CoT）方法的自然语言解释。这是LLM的基础推理方法。CoT-SC的token成本作为基准。
- en: '2.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Code: This method directs the model to extract and execute code to derive the
    answer. Inspired by Chen et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib1),
    we specifically adopted the prompt presented in Gou et al., [2024](https://arxiv.org/html/2410.09780v1#bib.bib5),
    characterized by converting natural language problems into local code interpreter.
    According to the average of the logged values in our experiments, the token cost
    for Code is 3.0 times higher than the base text method.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码：这种方法引导模型提取并执行代码以推导答案。受陈等人[2023](https://arxiv.org/html/2410.09780v1#bib.bib1)的启发，我们特别采用了高等等人[2024](https://arxiv.org/html/2410.09780v1#bib.bib5)提出的提示，特点是将自然语言问题转换为本地代码解释器。根据我们实验中记录的平均值，Code方法的token成本是基础文本方法的3.0倍。
- en: '3.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Cumulative Reasoning (CR): The CR framework, proposed by Zhang et al., [2024](https://arxiv.org/html/2410.09780v1#bib.bib12),
    utilizes multiple LLMs cumulatively and iteratively for mathematical reasoning,
    mirroring human thought processes for problem-solving. We used CR with code to
    remove additional environmental variables besides the prompts aspects when comparing
    with Code (Method 2).'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 累积推理（CR）：由张等人[2024](https://arxiv.org/html/2410.09780v1#bib.bib12)提出的CR框架，利用多个LLM进行累积和迭代的数学推理，模拟人类的思维过程来解决问题。我们使用带代码的CR来去除除了提示部分外的其他环境变量，与代码（方法2）进行比较。
- en: Selecting (Sampling)
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择（采样）
- en: Although we secured a pool of runs by generating $n$ runs from each method,
    achieving an advantage in exploration over CoT-SC from a single method requires
    that the search space covered by these runs is extensive. Therefore, selecting
    a fixed number of runs should ensure high accuracy. To achieve this, an appropriate
    sampling algorithm that can effectively and efficiently combine solutions from
    various methods is necessary. To ensure that the final selected runs are as diverse
    as possible, we employed a method called uniform sampling.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通过从每种方法生成$n$次实验获得了一个运行池，但要通过单一方法在探索中超越CoT-SC，要求这些运行所覆盖的搜索空间是广泛的。因此，选择一个固定数量的运行应确保高准确性。为了实现这一点，需要一个能够有效并高效地结合各种方法解决方案的适当采样算法。为了确保最终选择的运行尽可能多样化，我们采用了一种叫做均匀采样的方法。
- en: 'Uniform Sampling: Uniform Sampling ensures an equal sampling ratio for each
    method. For example, if initial runs show the highest performance in the order
    of method A, B, and C, Sampling also follows the order of A, B, and C, then repeats
    (i.e., A, B, C, A, B, C, …).'
  id: totrans-40
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 均匀抽样：均匀抽样确保每种方法的抽样比例相等。例如，如果初始运行显示方法A、B和C的性能依次最好，则抽样也按照A、B和C的顺序进行，然后重复（即，A、B、C，A、B、C，…）。
- en: This sampling process provides a basis for efficient performance enhancements
    by leveraging a broader search space.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种抽样过程为通过利用更广泛的搜索空间来有效提升性能提供了基础。
- en: 2.2 Verify answer from sampled runs through LLM Re-ranking
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 通过LLM重排序验证从抽样运行中得到的答案
- en: Previous sampling and voting methods used for maintaining self-consistency (Zhou
    et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib14); Wang et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib10))
    have the drawback of not fully utilizing the high accuracy upper bound of multiple
    runs. For example, even if the selection process includes a run that correctly
    answers previously unsolved problems through improved exploration, sampling and
    voting tend to favor incorrect answers due to the prevalence of erroneous runs.
    Since our approach focuses on increasing the search space’s upper bound, it is
    crucial to identify correct answers even from the prevalence of wrong responses.
    Therefore, we employ LLM re-ranking to derive optimal performance from the selected
    runs. The re-ranking process follows the methodology proposed by RankGPT (Sun
    et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib9)), which introduces
    an effective approach for LLM re-ranking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之前用于保持自一致性的抽样和投票方法（Zhou et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib14);
    Wang et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib10)）有一个缺点，即未能充分利用多次运行的高准确性上限。例如，即使选择过程中包括了一次通过改进的探索正确回答先前未解答问题的运行，抽样和投票往往偏向于错误答案，因为错误的运行更为普遍。由于我们的方法着重于增加搜索空间的上限，因此即便在错误响应较多的情况下，也必须识别正确答案。因此，我们采用LLM重排序来从选定的运行中得出最佳性能。重排序过程遵循RankGPT（Sun
    et al., [2023](https://arxiv.org/html/2410.09780v1#bib.bib9)）提出的方法论，该方法为LLM重排序引入了一种有效的方法。
- en: 3 Experiments
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: Setup
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置
- en: 'Our experiments are conducted on the subset of MATH dataset (Hendrycks et al.,
    [2021](https://arxiv.org/html/2410.09780v1#bib.bib6)), which consists of 12,500
    challenging math problems from competitions like AMC and AIME, We sampled data
    from all mathematical domains within the MATH dataset, focusing on questions with
    difficulty levels 4 and 5\. This resulted in 280 challenging questions (comprising
    approximately 11% of the entire dataset), which we refer to as MATH-hard. We used
    GPT-4o as the base model for all experiments, and it was also utilized as a LLM
    re-ranker in Section [2.2](https://arxiv.org/html/2410.09780v1#S2.SS2 "2.2 Verify
    answer from sampled runs through LLM Re-ranking ‣ 2 Method ‣ Expanding Search
    Space with Diverse Prompting Agents: An Efficient Sampling Approach for LLM Mathematical
    Reasoning"). The temperature was set to 0.7 to obtain as diverse responses as
    possible from each prompting method.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验是在MATH数据集的子集上进行的，该数据集（Hendrycks et al., [2021](https://arxiv.org/html/2410.09780v1#bib.bib6)）包含了来自AMC和AIME等竞赛的12,500个具有挑战性的数学问题。我们从MATH数据集中的所有数学领域中抽取数据，重点关注难度等级为4和5的问题。最终选取了280个具有挑战性的问题（约占整个数据集的11%），我们将其称为MATH-hard。我们使用GPT-4o作为所有实验的基础模型，并且它也被用作第[2.2](https://arxiv.org/html/2410.09780v1#S2.SS2
    "2.2 通过LLM重排序验证从抽样运行中得到的答案 ‣ 2 方法 ‣ 通过多样化提示代理扩展搜索空间：一种高效的LLM数学推理抽样方法")节中的LLM重排序器。温度设置为0.7，以便从每种提示方法中获得尽可能多样的响应。
- en: 'Further details for ablation studies to assess the impact of different components
    (model size and difficulty level in MATH dataset) can be found in Appendix [A.1](https://arxiv.org/html/2410.09780v1#A1.SS1
    "A.1 Ablation Study ‣ Appendix A Appendix ‣ Expanding Search Space with Diverse
    Prompting Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 有关消融研究的更多细节，评估不同组件（模型大小和MATH数据集中的难度级别）影响的内容，请参见附录[A.1](https://arxiv.org/html/2410.09780v1#A1.SS1
    "A.1 消融研究 ‣ 附录A 附录 ‣ 通过多样化提示代理扩展搜索空间：一种高效的LLM数学推理抽样方法")。
- en: '|  | Sampling Methods |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 抽样方法 |'
- en: '| Sample k | Text | Code | CR | Uniform |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 样本 k | 文本 | 代码 | CR | 均匀 |'
- en: '| base (k=1) | 60.00 | 56.07 | 46.79 | (= Top1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 基础 (k=1) | 60.00 | 56.07 | 46.79 | (= Top1) |'
- en: '| 3 (1,1,1) | 70.0 | 71.07 | 67.5 | 70.71 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 3 (1,1,1) | 70.0 | 71.07 | 67.5 | 70.71 |'
- en: '| 6 (2,) | 75.71 | 77.5 | 76.07 | 77.14 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 6 (2,) | 75.71 | 77.5 | 76.07 | 77.14 |'
- en: '| 9 (3,) | 77.86 | 80.36 | 78.57 | 81.79 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 9 (3,) | 77.86 | 80.36 | 78.57 | 81.79 |'
- en: '| 12 (4,) | 78.93 | 82.14 | 81.79 | 84.29 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 12 (4,) | 78.93 | 82.14 | 81.79 | 84.29 |'
- en: '| 15 (5,) | 79.64 | 82.5 | 82.5 | 85.36 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 15 (5,) | 79.64 | 82.5 | 82.5 | 85.36 |'
- en: '| 18 (6,) | 81.79 | 83.21 | 83.21 | 85.36 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 18 (6,) | 81.79 | 83.21 | 83.21 | 85.36 |'
- en: '| 21 (7,) | 83.93 | 83.21 | 84.64 | 86.79 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 21 (7,) | 83.93 | 83.21 | 84.64 | 86.79 |'
- en: '| Average | 78.27 | 80.00 | 79.18 | 81.63 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 78.27 | 80.00 | 79.18 | 81.63 |'
- en: 'Table 1: Search space’s upper bound scores on each sampling methods. Result
    on MATH-hard (* 280 test subset): We increased the number of samples by adopting
    the default temperature value t=0.7 from CoT-SC. As mentioned in the Method section,
    each prompting method was based on or reproduced from the following: Text on CoT,
    Code on CSV (LLM with Local Code Interpreter), and CR from Cumulative Reasoning.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：各采样方法的搜索空间上限得分。MATH-hard（*280测试子集）上的结果：我们通过采用CoT-SC的默认温度值t=0.7增加了样本数。如方法部分所述，每种提示方法都是基于或从以下内容复制的：CoT上的文本，CSV（带本地代码解释器的LLM）上的代码，以及来自累积推理的CR。
- en: 3.1 Efficacy of aggregating distinct prompting methods
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 聚合不同提示方法的效果
- en: To quantitatively analyze how effective it is to incorporate various prompting
    methods, each prompting method was run 21 times, generating 21 different solutions
    for the entire 280 questions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定量分析各种提示方法的效果，每种提示方法都运行了21次，生成了280个问题的21个不同解答。
- en: This experiment analyzes how the accuracy upper bound changes by incrementally
    adding runs along the x-axis, comparing the upper bound accuracy of each prompting
    method against the upper bound obtained through uniform sampling from 21 * 3 runs
    generated by all prompting methods.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验分析了通过逐步增加运行次数，x轴上的准确度上限如何变化，并将每种提示方法的准确度上限与通过均匀采样从21 * 3次运行中获得的上限准确度进行比较。
- en: 'Results from Figure [1](https://arxiv.org/html/2410.09780v1#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Expanding Search Space with Diverse Prompting Agents: An Efficient
    Sampling Approach for LLM Mathematical Reasoning") demonstrates our method achieves
    the highest accuracy of individual prompting methods much earlier; from the 21st
    to the 11th run for text, from the 18th to the 10th run for Code, and from the
    20th to the 13th run for CR, respectively. These results support our hypothesis
    that employing diverse prompting techniques allows for a more extensive, faster
    exploration of problems that a single methodology fails to solve or cannot reach.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '来自图 [1](https://arxiv.org/html/2410.09780v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling
    Approach for LLM Mathematical Reasoning")的结果表明，我们的方法比单一提示方法更早地达到了最高准确度；对于文本从第21次运行到第11次运行，对于代码从第18次运行到第10次运行，对于CR从第20次运行到第13次运行。这些结果支持了我们的假设，即使用多样化的提示技术能够更广泛、更快速地探索那些单一方法无法解决或无法达到的问题。'
- en: '|  | Sampling Methods |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 采样方法 |'
- en: '| Sample k | Text | Code | CR | Uniform |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 样本 k | 文本 | 代码 | CR | 均匀 |'
- en: '| base (k=1) | 60.00 | 56.07 | 46.79 | (= Top1) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 基础（k=1） | 60.00 | 56.07 | 46.79 | (= Top1) |'
- en: '| SC (Sample & Voting) |  |  |  |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SC（样本与投票） |  |  |  |  |'
- en: '| 3 (1,1,1) | 60.0 | 60.0 | 45.36 | 57.14 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 3 (1,1,1) | 60.0 | 60.0 | 45.36 | 57.14 |'
- en: '| 6 (2,) | 60.0 | 60.0 | 48.21 | 57.5 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 6 (2,) | 60.0 | 60.0 | 48.21 | 57.5 |'
- en: '| 9 (3,) | 57.86 | 59.29 | 46.07 | 58.93 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 9 (3,) | 57.86 | 59.29 | 46.07 | 58.93 |'
- en: '| 12 (4,) | 58.57 | 61.43 | 48.57 | 58.21 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 12 (4,) | 58.57 | 61.43 | 48.57 | 58.21 |'
- en: '| 15 (5,) | 58.21 | 60.71 | 47.14 | 58.57 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 15 (5,) | 58.21 | 60.71 | 47.14 | 58.57 |'
- en: '| 18 (6,) | 59.29 | 60.71 | 47.14 | 59.29 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 18 (6,) | 59.29 | 60.71 | 47.14 | 59.29 |'
- en: '| 21 (7,) | 58.93 | 60.71 | 48.57 | 58.93 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 21 (7,) | 58.93 | 60.71 | 48.57 | 58.93 |'
- en: '| Average | 58.98 | 60.41 | 47.29 | 58.37 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 58.98 | 60.41 | 47.29 | 58.37 |'
- en: '| Rerank@1 (RankGPT, GPT-4o) |  |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序@1（RankGPT, GPT-4o） |  |  |  |  |'
- en: '| 3 (1,1,1) | 63.93 | 63.93 | 60.00 | 62.86 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3 (1,1,1) | 63.93 | 63.93 | 60.00 | 62.86 |'
- en: '| 6 (2,) | 64.29 | 66.43 | 65.36 | 65.71 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 6 (2,) | 64.29 | 66.43 | 65.36 | 65.71 |'
- en: '| 9 (3,) | 64.64 | 68.93 | 66.43 | 64.64 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 9 (3,) | 64.64 | 68.93 | 66.43 | 64.64 |'
- en: '| 12 (4,) | 65.71 | 69.29 | 67.14 | 66.43 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 12 (4,) | 65.71 | 69.29 | 67.14 | 66.43 |'
- en: '| 15 (5,) | 65.71 | 71.07 | 67.50 | 66.07 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 15 (5,) | 65.71 | 71.07 | 67.50 | 66.07 |'
- en: '| 18 (6,) | 65.71 | 71.07 | 67.50 | 66.07 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 18 (6,) | 65.71 | 71.07 | 67.50 | 66.07 |'
- en: '| 21 (7,) | 65.71 | 71.07 | 68.93 | 65.71 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 21 (7,) | 65.71 | 71.07 | 68.93 | 65.71 |'
- en: '| Average | 65.00 | 68.57 | 66.07 | 65.36 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 65.00 | 68.57 | 66.07 | 65.36 |'
- en: 'Table 2: Verifying candidate answers result on MATH-hard (* 280 test subset).
    The experimental settings from Table 1 were maintained, while in Table 2, the
    verification process for candidate answers found within the search space of each
    method was performed. The results compare the effectiveness of sample and voting
    versus LLM Reranking methods. Sampling and voting were performed using Self-Consistency,
    and LLM Reranking was implemented using RankGPT (GPT-4o, sliding_window=4, step_size=2).
    All accuracy metrics are based on Recall@1.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：验证候选答案在MATH-hard（* 280测试子集）上的结果。表1中的实验设置得以保持，而在表2中，针对每种方法在搜索空间内找到的候选答案进行了验证过程。结果对比了采样和投票与LLM重排序方法的效果。采样和投票使用自一致性方法，LLM重排序则通过RankGPT（GPT-4o，滑动窗口=4，步长=2）实现。所有的准确性度量都基于Recall@1。
- en: 3.2 Discussion
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 讨论
- en: The experiments validate our hypothesis that diverse prompting techniques enhance
    the exploration of the solution space, especially for challenging mathematical
    problems. By using multiple state-of-the-art prompting methods, we demonstrate
    that each method explores different parts of the problem space, leading to a more
    comprehensive and efficient exploration. Consequently, even a simple uniform sampling
    strategy, when combined with LLM Reranking, results in significant performance
    improvements and reduced sampling costs. These findings underscore the importance
    of incorporating multiple methods to achieve a broader and more effective exploration
    of problem-solving strategies in the MATH domain.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实验验证了我们的假设，即多样化的提示技术能够增强解空间的探索，尤其是对于具有挑战性的数学问题。通过使用多种最先进的提示方法，我们证明了每种方法探索了解问题空间的不同部分，从而实现了更全面和高效的探索。因此，即便是简单的均匀采样策略，在与LLM重排序结合时，也能带来显著的性能提升和降低采样成本。这些发现强调了在MATH领域中整合多种方法的重要性，以实现更广泛且更有效的问题解决策略探索。
- en: Unfortunately, expanding the search space with multiple agents and using verifiers
    with a self-consistency algorithm and LLM Reranking do not complement each other.
    It is due to the foundational philosophy of our algorithm. Our algorithm aims
    to *collect at least one correct response* by expanding our search space. It does
    not necessarily mean the correct response appears multiple times.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，通过多个代理扩展搜索空间并使用带有自一致性算法的验证器和LLM重排序并不能互相补充。这是由于我们算法的基础哲学所致。我们的算法旨在通过扩展搜索空间*至少收集到一个正确的响应*，但并不意味着正确的响应会出现多次。
- en: Therefore, to ensure the final performance improves with the expanded search
    space, we employed an LLM re-ranking method which is expected to consistently
    select correct answers, even from sparse values. However, contrary to our expectations,
    neither the traditional self-consistency (SC) approach nor the LLM re-ranking
    method consistently guaranteed this improvement.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了确保在扩展搜索空间后最终性能得到提升，我们采用了一种LLM重排序方法，预计它能够始终正确地选择答案，即使是在稀疏值的情况下。然而，出乎我们意料的是，无论是传统的自一致性（SC）方法还是LLM重排序方法，都未能始终保证这一提升。
- en: 4 Conclusion
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: 'In this work we highlight following observations regarding to mathematical
    reasoning:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们强调了关于数学推理的以下观察结果：
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Different prompting methods explore distinct solvable problem spaces, and the
    disparity between these search spaces is challenging to overcome, even by increasing
    the temperature within a single method.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的提示方法探索的是不同的可解问题空间，这些搜索空间之间的差异是很难克服的，即使在单一方法中增加温度也无法解决这一问题。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Therefore, aggregating multiple methods via the sampling approach can expand
    the solvable problem space, thereby raising the upper bound of accuracy. This
    approach surpasses the exploration and convergence speed of traditional single-method
    approaches.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 因此，通过采样方法聚合多种方法能够扩展可解问题空间，从而提高准确性的上限。这种方法超越了传统单一方法在探索和收敛速度上的优势。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The subsequent LLM re-ranking process yields promising results, demonstrating
    more efficient approach to produce correct solution compared to the traditional
    majority voting method used in self-consistency.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随后的LLM重排序过程产生了有前景的结果，展示了比传统的自一致性中使用的多数投票方法更高效的正确解生成方法。
- en: 5 Limitations
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 局限性
- en: Our study has yielded insightful findings in the mathematical domain, but it
    has the following limitations.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究在数学领域取得了有价值的发现，但也存在以下局限性。
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Due to the inherent cost issues associated with generating multiple solutions
    to a single problem, the number of runs produced by each method is not extensive.
    However, the Appendix [A](https://arxiv.org/html/2410.09780v1#A1 "Appendix A Appendix
    ‣ Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling
    Approach for LLM Mathematical Reasoning") describes further experimental results
    based on GPT-4, where the number of samples was increased to approximately 32%
    of the total dataset, compared to the 11% used in the MATH-hard dataset discussed
    in the main text. These results reaffirm that even with an increased number of
    runs, differences between output spaces persist when solving difficult problems.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于生成多个解决方案来解决单一问题固有的成本问题，每种方法生成的运行次数并不多。然而，附录[A](https://arxiv.org/html/2410.09780v1#A1
    "附录A 附录 ‣ 使用多样化提示代理扩展搜索空间：一种高效的LLM数学推理采样方法")中描述了基于GPT-4的进一步实验结果，其中样本数增加到了大约占总数据集32%，而与正文中讨论的MATH-hard数据集相比，样本占比为11%。这些结果再次证明，即使增加了运行次数，在解决困难问题时，输出空间之间的差异依然存在。
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The process of verifying the final answer from sampled runs through LLM re-ranking
    has shown inconsistent results. Various LLMs (e.g. Gemini 1.5) and methods were
    tested, but the data did not consistently demonstrate that an increase in the
    number of runs proportionally enhances both the upper bound of the search space
    and the final accuracy. It is anticipated that employing a formal math verifier
    specialized in verification, such as Isabelle, as proposed in the DTV paper, would
    ensure that the final accuracy consistently approaches the maximum value of the
    expanded search space.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过LLM重排序从采样运行中验证最终答案的过程已显示出不一致的结果。测试了多种LLM（例如Gemini 1.5）和方法，但数据并未一致地表明，增加运行次数会按比例增强搜索空间的上限和最终准确性。预计采用专门用于验证的正式数学验证器，如DTV论文中提出的Isabelle，将确保最终准确性始终接近扩展搜索空间的最大值。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We did not incorporate a broader range of problem-solving approaches. Recent
    studies have introduced promising methodologies for mathematical reasoning, such
    as agentic prompting methods (e.g. RAT). We leave the evaluation of these diverse
    methodologies as a future research.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们没有纳入更广泛的解题方法。最近的研究介绍了数学推理中一些有前景的方法，例如代理式提示方法（如RAT）。我们将这些多样化方法的评估留待未来研究。
- en: References
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Chen et al. (2023) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    2023. [Program of thoughts prompting: Disentangling computation from reasoning
    for numerical reasoning tasks](http://arxiv.org/abs/2211.12588).'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2023）陈文虎、马学广、王鑫怡、威廉·W·科恩。2023年。[思维提示程序：为数值推理任务解开计算与推理的关系](http://arxiv.org/abs/2211.12588)。
- en: Claude (2024) Claude. 2024. [Claude model card](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claude（2024）Claude。2024年。[Claude模型卡片](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo Jun、Lukasz
    Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano、Christopher
    Hesse、John Schulman。2021年。训练验证器解决数学文字问题。*arXiv预印本arXiv:2110.14168*。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. 2023. Improving factuality and reasoning in language models
    through multiagent debate. *arXiv preprint arXiv:2305.14325*.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等（2023）杜一伦、李爽、安东尼奥·托雷尔巴、乔舒亚·B·特南鲍姆、伊戈尔·莫达奇。2023年。通过多代理辩论提高语言模型的事实性和推理能力。*arXiv预印本arXiv:2305.14325*。
- en: 'Gou et al. (2024) Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu
    Yang, Minlie Huang, Nan Duan, and Weizhu Chen. 2024. [ToRA: A tool-integrated
    reasoning agent for mathematical problem solving](https://openreview.net/forum?id=Ep0TtjVoap).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邱等（2024）周志斌、邵志红、龚业云、沈业龙、杨宇九、黄敏磊、段楠、陈伟柱。2024年。[ToRA：一种工具集成推理代理，用于数学问题求解](https://openreview.net/forum?id=Ep0TtjVoap)。发表于*第十二届国际学习表征会议*。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical
    problem solving with the math dataset. *NeurIPS*.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021）Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven
    Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 2021. 使用数学数据集衡量数学问题解决能力。*NeurIPS*。
- en: 'Liu et al. (2023) Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang.
    2023. [Dynamic llm-agent network: An llm-agent collaboration framework with agent
    team optimization](http://arxiv.org/abs/2310.02170).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, 和 Diyi Yang. 2023. [动态
    LLM-Agent 网络：一个具有代理团队优化的 LLM-Agent 协作框架](http://arxiv.org/abs/2310.02170)。
- en: OpenAI (2024) OpenAI. 2024. [Gpt-4o blog post](https://openai.com/index/hello-gpt-4o/).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2024）OpenAI. 2024. [Gpt-4o 博客文章](https://openai.com/index/hello-gpt-4o/)。
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie
    Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. [Is chatgpt good at search?
    investigating large language models as re-ranking agents](http://arxiv.org/abs/2304.09542).
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren,
    Zhumin Chen, Dawei Yin, 和 Zhaochun Ren. 2023. [ChatGPT 在搜索中表现如何？调查大型语言模型作为重新排序代理](http://arxiv.org/abs/2304.09542)。
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. [Self-consistency improves
    chain of thought reasoning in language models](http://arxiv.org/abs/2203.11171).
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等（2023）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan
    Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023. [自一致性改善语言模型中的链式思维推理](http://arxiv.org/abs/2203.11171)。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](http://arxiv.org/abs/2201.11903).
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2023）Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter,
    Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 2023. [链式思维提示在大型语言模型中引发推理](http://arxiv.org/abs/2201.11903)。
- en: Zhang et al. (2024) Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih
    Yao. 2024. [Cumulative reasoning with large language models](http://arxiv.org/abs/2308.04371).
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2024）Yifan Zhang, Jingqin Yang, Yang Yuan, 和 Andrew Chi-Chih Yao. 2024.
    [使用大型语言模型进行累积推理](http://arxiv.org/abs/2308.04371)。
- en: 'Zhong et al. (2024) Li Zhong, Zilong Wang, and Jingbo Shang. 2024. [Debug like
    a human: A large language model debugger via verifying runtime execution step-by-step](http://arxiv.org/abs/2402.16906).'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等（2024）Li Zhong, Zilong Wang, 和 Jingbo Shang. 2024. [像人类一样调试：通过逐步验证运行时执行的大型语言模型调试器](http://arxiv.org/abs/2402.16906)。
- en: Zhou et al. (2023) Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng
    Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, and Hongsheng Li. 2023.
    [Solving challenging math word problems using gpt-4 code interpreter with code-based
    self-verification](http://arxiv.org/abs/2308.07921).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin,
    Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, 和 Hongsheng Li. 2023. [使用 GPT-4
    代码解释器与基于代码的自我验证解决具有挑战性的数学文字题](http://arxiv.org/abs/2308.07921)。
- en: 'Zhou et al. (2024) Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy,
    Kilian Q. Weinberger, and Yuhuai Wu. 2024. [Don’t trust: Verify – grounding llm
    quantitative reasoning with autoformalization](http://arxiv.org/abs/2403.18120).'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2024）Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian
    Q. Weinberger, 和 Yuhuai Wu. 2024. [不要相信：验证 – 通过自动形式化将 LLM 定量推理与基础结合](http://arxiv.org/abs/2403.18120)。
- en: 'Zihao et al. (2024) Wang Zihao, Liu Anji, Lin Haowei, Li Jiaqi, Ma Xiaojian,
    and Liang Yitao. 2024. Rat: Retrieval augmented thoughts elicit context-aware
    reasoning in long-horizon generation. *arXiv preprint arXiv: 2403.05313*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zihao 等（2024）Wang Zihao, Liu Anji, Lin Haowei, Li Jiaqi, Ma Xiaojian, 和 Liang
    Yitao. 2024. Rat: 检索增强的思维在长时程生成中的上下文感知推理激发。*arXiv 预印本 arXiv: 2403.05313*。'
- en: Appendix A Appendix
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Ablation Study
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 消融研究
- en: Data Sampling Details
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据抽样详情
- en: For all MATH data sampling, we fixed random_seed=42 and adjusted the level,
    domain, and number of samples to create various data samples.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有 MATH 数据抽样，我们固定 random_seed=42，并调整级别、领域和样本数量，以创建各种数据样本。
- en: 'a) MATH-hard: A subset experimented with GPT-4o in the main text. For hard
    levels (4 and 5), without domain restrictions (7 domains), 20 samples were drawn
    each, totaling 280 samples (11.03% of the test set). This subset, called MATH-hard,
    allows us to verify reasoning ability on particularly difficult problems.'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: a) MATH-hard：在主文中使用 GPT-4o 实验的一个子集。对于困难级别（4 和 5），在没有领域限制（7 个领域）的情况下，每个级别抽取 20
    个样本，总计 280 个样本（占测试集的 11.03%）。该子集被称为 MATH-hard，允许我们验证在特别困难的问题上的推理能力。
- en: '![Refer to caption](img/efee8864eb3356e3655dca9fa1cde8d8.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/efee8864eb3356e3655dca9fa1cde8d8.png)'
- en: 'Figure 3: Maximum search space for methods result on MATH-hard (* 280 test
    subset): Radar graph for showing the average accuracy per all 7 domains for each
    method (Text, Code, CR) based on their 21 runs.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：MATH-hard 方法结果的最大搜索空间（* 280 测试子集）：雷达图显示每个方法（文本、代码、CR）在所有 7 个领域的平均准确度，基于其
    21 次运行。
- en: 'b) MATH-hard-4doms: Our experimental results showed that even powerful models
    like GPT-4(o) performed poorly in four specific domains within MATH-hard: "counting_and_probability,"
    "geometry," "intermediate_algebra," and "precalculus" (see Figure [3](https://arxiv.org/html/2410.09780v1#A1.F3
    "Figure 3 ‣ Data Sampling Details ‣ A.1 Ablation Study ‣ Appendix A Appendix ‣
    Expanding Search Space with Diverse Prompting Agents: An Efficient Sampling Approach
    for LLM Mathematical Reasoning")). We increased the number of samples in these
    four domains from 20 to 50, totaling 400 samples (31.55% of the four domains),
    creating the MATH-hard-4doms subset.'
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: b) MATH-hard-4doms：我们的实验结果表明，即使是像 GPT-4(o) 这样的强大模型，在 MATH-hard 的四个特定领域表现也很差：“counting_and_probability”，“geometry”，“intermediate_algebra”和“precalculus”（参见图
    [3](https://arxiv.org/html/2410.09780v1#A1.F3 "图 3 ‣ 数据采样细节 ‣ A.1 消融研究 ‣ 附录 A
    附录 ‣ 使用多样提示代理扩展搜索空间：一种高效的采样方法")）。我们将这四个领域的样本数量从 20 增加到 50，总共 400 个样本（占这四个领域的 31.55%），创建了
    MATH-hard-4doms 子集。
- en: '![Refer to caption](img/efb103abc19344322de6b92bae75161a.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/efb103abc19344322de6b92bae75161a.png)'
- en: 'Figure 4: Maximum search space for methods result on MATH-hard-4doms (* 400
    test subset): Data sampling details are written in the section above.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MATH-hard-4doms 方法结果的最大搜索空间（* 400 测试子集）：数据采样细节在上述章节中已写明。
- en: 'c) MATH-all: To verify if the search space expands across the entire set of
    domains, not just the difficult problems, we sampled 10 samples per domain across
    all 7 domains and all 5 levels, totaling 350 samples (5% of the entire dataset).'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: c) MATH-all：为了验证搜索空间是否扩展到所有领域，而不仅仅是困难问题，我们在所有 7 个领域和 5 个层次上每个领域抽取了 10 个样本，总共
    350 个样本（占整个数据集的 5%）。
- en: Smaller Models on MATH-all
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小型模型在 MATH-all 上的表现
- en: Previous experiments confirmed that broader approaches are more effective on
    more difficult problems, leading to the MATH-hard subset for experiments based
    on GPT-4\. As an ablation study, we conducted experiments on MATH-all with general
    models GPT-3.5-Turbo and LLaMA-3-70B (which performs better than GPT-3.5-Turbo
    but is similar in cost). We examined whether the search space expands for all
    levels of problems across each prompting method as the number of method runs samples
    increases.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的实验已确认，更广泛的方法在更困难的问题上更有效，从而引出了基于 GPT-4 的 MATH-hard 子集实验。作为消融研究，我们对 MATH-all
    进行了实验，使用了通用模型 GPT-3.5-Turbo 和 LLaMA-3-70B（后者比 GPT-3.5-Turbo 表现更好，但成本相似）。我们考察了随着每种提示方法的运行次数样本增加，搜索空间是否会扩展到所有层次的问题。
- en: '|  | Sampling Methods |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 采样方法 |'
- en: '| Model: GPT-3.5-Turbo | Text | Code | CR | Uniform |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 模型：GPT-3.5-Turbo | 文本 | 代码 | CR | 均匀 |'
- en: '| base (k=1) | 48.86 46.29 42.57 | (= Top1) |  |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 基础 (k=1) | 48.86 46.29 42.57 | (= Top1) |  |  |'
- en: '| 5 (2,2,1) | 69.43 | 66.86 | 66.86 | 70.00 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 5 (2,2,1) | 69.43 | 66.86 | 66.86 | 70.00 |'
- en: '| 10 (4,3,3) | 76.86 | 78.86 | 73.14 | 76.57 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 10 (4,3,3) | 76.86 | 78.86 | 73.14 | 76.57 |'
- en: '| 15 (5,5,5) | 78.86 | 82.29 | 76.29 | 81.14 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 15 (5,5,5) | 78.86 | 82.29 | 76.29 | 81.14 |'
- en: '| 20 (7,7,6) | 80.86 | 84.29 | 80.00 | 84.00 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 20 (7,7,6) | 80.86 | 84.29 | 80.00 | 84.00 |'
- en: '| Average | 85.36 | 80.07 | 85.86 | 87.14 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 85.36 | 80.07 | 85.86 | 87.14 |'
- en: '|  | Sampling Methods |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | 采样方法 |'
- en: '| Model: LLaMA-3-70B | Text | Code | CR | Uniform |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 模型：LLaMA-3-70B | 文本 | 代码 | CR | 均匀 |'
- en: '| base (k=1) | 65.14 | 41.71 | 61.71 | (= Top1) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 基础 (k=1) | 65.14 | 41.71 | 61.71 | (= Top1) |'
- en: '| 5 (2,1,2) | 80.29 | 70.29 | 81.71 | 82.00 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 5 (2,1,2) | 80.29 | 70.29 | 81.71 | 82.00 |'
- en: '| 10 (4,3,3) | 85.14 | 80.29 | 85.71 | 86.86 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 10 (4,3,3) | 85.14 | 80.29 | 85.71 | 86.86 |'
- en: '| 15 (5,5,5) | 87.43 | 84.29 | 87.14 | 89.43 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 15 (5,5,5) | 87.43 | 84.29 | 87.14 | 89.43 |'
- en: '| 20 (7,6,7) | 88.57 | 85.43 | 88.86 | 90.29 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 20 (7,6,7) | 88.57 | 85.43 | 88.86 | 90.29 |'
- en: '| Average | 85.36 | 80.07 | 85.86 | 87.14 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 85.36 | 80.07 | 85.86 | 87.14 |'
- en: 'Table 3: Search space’s upper bound scores on each sampling methods. Result
    on MATH-all (* 350 test subset): Experimental Details are the same with Table [1](https://arxiv.org/html/2410.09780v1#S3.T1
    "Table 1 ‣ Setup ‣ 3 Experiments ‣ Expanding Search Space with Diverse Prompting
    Agents: An Efficient Sampling Approach for LLM Mathematical Reasoning") and data
    sampling details are written in the section above.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：每种采样方法的搜索空间上限得分。MATH-all 上的结果（* 350 测试子集）：实验细节与表 [1](https://arxiv.org/html/2410.09780v1#S3.T1
    "表 1 ‣ 设置 ‣ 3 实验 ‣ 使用多样提示代理扩展搜索空间：一种高效的采样方法") 相同，数据采样细节在上述章节中已写明。
