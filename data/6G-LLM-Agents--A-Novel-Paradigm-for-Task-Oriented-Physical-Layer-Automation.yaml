- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:10:02'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:10:02
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6G LLM 代理：一种面向任务的物理层自动化新范式
- en: 来源：[https://arxiv.org/html/2410.03688/](https://arxiv.org/html/2410.03688/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.03688/](https://arxiv.org/html/2410.03688/)
- en: Zhuoran Xiao, Chenhui Ye${2}$, Yunbo Hu, Honggang Yuan, Yihang Huang, Yijia Feng,
    Liyu Cai, and Jiang Chang Nokia Bell Labs, Shanghai, China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuoran Xiao, Chenhui Ye${2}$, Yunbo Hu, Honggang Yuan, Yihang Huang, Yijia
    Feng, Liyu Cai, 和 Jiang Chang，诺基亚贝尔实验室，中国上海
- en: 'E-mails: {zhuoran.xiao, chenhui.a.ye${2}$, yunbo.hu, honggang.yuan, yihang.huang,
    yijia.feng, liyu.cai, jiang.chang}'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{zhuoran.xiao, chenhui.a.ye${2}$, yunbo.hu, honggang.yuan, yihang.huang,
    yijia.feng, liyu.cai, jiang.chang}
- en: '@nokia-sbell.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '@nokia-sbell.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advancement in generative pre-training models is propelling a paradigm
    shift in technological progression from basic applications such as chatbots towards
    more sophisticated agent-based systems. It is with huge potential and necessity
    that the 6G system be combined with the copilot of large language model (LLM)
    agents to manage the highly complicated communication system with new emerging
    features such as native AI service and sensing. With the 6G-oriented agent, the
    base station could understand the transmission requirements of various dynamic
    upper-layer tasks, automatically orchestrate the optimal system workflow, and
    raise the performance accordingly. Differing from existing LLM agents designed
    for general application, the 6G-oriented agent aims to make highly rigorous and
    precise planning with a vast amount of extra expert knowledge, which inevitably
    requires a specific system design from model training to implementation. This
    paper proposes a novel comprehensive approach for building task-oriented 6G LLM
    agents. We first propose a two-stage continual pre-training and fine-tuning scheme
    to build the field basic model and diversities of specialized expert models for
    meeting the requirements of various tasks. Further, a novel retrieval-based agent
    inference framework for leveraging the existing communication-related functions
    is proposed. Experiment results of exemplary tasks, such as 3GPP protocol Q&A
    and physical-layer task decomposition, show the proposed paradigm’s feasibility
    and effectiveness.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式预训练模型的快速发展推动了技术进步的范式转变，从简单的应用如聊天机器人向更复杂的基于代理的系统发展。6G系统与大型语言模型（LLM）代理的副驾驶结合，管理具有新兴特性的复杂通信系统，如原生AI服务和感知，是具有巨大潜力和必要性的。通过6G定向代理，基站可以理解各种动态上层任务的传输需求，自动协调最优的系统工作流程，并相应提高性能。与现有为一般应用设计的LLM代理不同，6G定向代理旨在利用大量额外的专家知识进行高度严谨和精确的规划，这不可避免地需要从模型训练到实现的特定系统设计。本文提出了一种构建面向任务的6G
    LLM代理的新综合方法。我们首先提出了一个两阶段的持续预训练和微调方案，以构建领域基础模型以及满足各种任务要求的多样化专家模型。此外，本文还提出了一种新颖的基于检索的代理推理框架，用于利用现有的通信相关功能。实验结果表明，诸如3GPP协议问答和物理层任务分解等典型任务的实验表明，所提出的范式具有可行性和有效性。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: 6G networks, AI agents, large language models, physical layer, wireless communication.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 6G 网络，AI 代理，大型语言模型，物理层，无线通信。
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The wireless communication system is currently in a new era of evolving from
    the traditional one designed solely for symbol transmition to a more functional
    and complicated one. The introduction of AI (Artificial Intelligence) methods
    into the air interface empowers the system to achieve enhanced performance across
    a variety of scenarios [[1](https://arxiv.org/html/2410.03688v1#bib.bib1)], and
    the new features such as JCAS (Joint Communication and Sensing) greatly extend
    the functions of the system [[2](https://arxiv.org/html/2410.03688v1#bib.bib2)].
    Moreover, there emerges novel application scenarios, such as autonomous driving
    [[3](https://arxiv.org/html/2410.03688v1#bib.bib3)] and augmented reality [[4](https://arxiv.org/html/2410.03688v1#bib.bib4)],
    which introduce diverse requirements beyond traditional use cases. In addition,
    with the goal of optimizing the system specifically for a certain kind of downstream
    task, task-oriented communication is deemed a promising approach to maximize overall
    system efficiency [[5](https://arxiv.org/html/2410.03688v1#bib.bib5), [6](https://arxiv.org/html/2410.03688v1#bib.bib6)].
    Relevant research has demonstrated that through specialized system designs, the
    performance can be significantly boosted. It is evident that all the aforementioned
    necessitates the system’s capability to automatically adapt to diverse communication
    scenarios and respond to various UE tasks. In the present communication system,
    the base station is restricted to working with predefined workflows and lacks
    comprehensive task understanding, causing the system to work inefficiently. This
    inefficiency not only squanders spectrum resources and energy but also negatively
    impacts the user experiences.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 无线通信系统正处于一个新时代，正在从传统的仅为符号传输设计的系统，发展为一个更具功能性和复杂性的系统。将人工智能（AI）方法引入空中接口，使得该系统能够在各种场景下实现增强的性能[[1](https://arxiv.org/html/2410.03688v1#bib.bib1)]，而像JCAS（联合通信与感知）这样的新特性极大地扩展了系统的功能[[2](https://arxiv.org/html/2410.03688v1#bib.bib2)]。此外，新的应用场景也应运而生，如自动驾驶[[3](https://arxiv.org/html/2410.03688v1#bib.bib3)]和增强现实[[4](https://arxiv.org/html/2410.03688v1#bib.bib4)]，这些场景带来了超越传统使用案例的多样化需求。此外，随着针对特定下游任务优化系统的目标，面向任务的通信被认为是一种有前景的方法，可以最大化系统的整体效率[[5](https://arxiv.org/html/2410.03688v1#bib.bib5),
    [6](https://arxiv.org/html/2410.03688v1#bib.bib6)]。相关研究表明，通过专业化的系统设计，系统性能可以显著提升。显然，所有上述需求都要求系统具备自动适应多种通信场景并响应各种UE任务的能力。在现有的通信系统中，基站只能在预定义的工作流程下运行，缺乏全面的任务理解，导致系统效率低下。这种低效不仅浪费了频谱资源和能源，还对用户体验产生了负面影响。
- en: '![Refer to caption](img/39e225d7b75ceac52073eb2e436f16a4.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/39e225d7b75ceac52073eb2e436f16a4.png)'
- en: 'Figure 1: System framework of the proposed 6G LLM agent.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：所提议的 6G LLM 代理的系统框架。
- en: Through leveraging the powerful learning ability of large language models (LLMs),
    AI agents have demonstrated significant potential in orchestrating complex systems
    and formulating sequential plans and decisions [[7](https://arxiv.org/html/2410.03688v1#bib.bib7),
    [8](https://arxiv.org/html/2410.03688v1#bib.bib8)]. For a comprehensive AI agent,
    there are several essential functions, including observing the environment, formulating
    plans and making decisions, utilizing tools, and self-improvement through reinforcement
    learning. It is obvious that the vision of future wireless communication systems
    impeccably aligns with the prerequisites for developing LLM agents. First of all,
    the communication system possesses a solid ability to observe the scattering environments
    through multi-modal perception, including channel estimation, sensing, and imaging
    via electromagnetic waves, as well as visual capture through cameras. Secondly,
    there is a vast repository of accessible domain-specific data that includes protocol
    documents, software code repositories, and error logs, which can be utilized to
    train an LLM. This training would empower the system to perform planning and make
    decisions, such as devising system workflows and configuring system settings.
    Thirdly, a multitude of functional APIs (Application Programming Interface) already
    exist in the communication system, which can be regarded as the toolkit for agents,
    facilitating various operations. Finally, performance metrics, such as the QoS
    (quality of service) and QoT (quality of task), can serve as reinforcement rewards
    to refine the agent’s actions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用大型语言模型（LLM）的强大学习能力，AI代理在协调复杂系统、制定连续计划和决策方面展现了巨大的潜力[[7](https://arxiv.org/html/2410.03688v1#bib.bib7),
    [8](https://arxiv.org/html/2410.03688v1#bib.bib8)]。对于一个全面的AI代理，有几个必不可少的功能，包括观察环境、制定计划和做出决策、利用工具以及通过强化学习自我提升。显然，未来无线通信系统的愿景与开发LLM代理的前提条件完美契合。首先，通信系统具备通过多模态感知（包括信道估计、传感和电磁波成像，以及通过摄像头进行的视觉捕捉）观察散射环境的强大能力。其次，存在一个庞大的特定领域数据仓库，其中包括协议文档、软件代码库和错误日志，可以用于训练LLM。这样的训练将使系统能够进行规划和决策，例如设计系统工作流和配置系统设置。第三，通信系统中已经存在大量功能性的API（应用程序编程接口），这些可以视为代理的工具包，促进各种操作的执行。最后，性能指标，如服务质量（QoS）和任务质量（QoT），可以作为强化奖励来优化代理的行为。
- en: Owing to the distinct and unique characteristics, the 6G-oriented LLM agent
    exhibits marked differences from general-purpose LLMs. For instance, an advanced
    6G LLM agent necessitates extensive learning from a vast corpus of domain-specific
    knowledge. Besides, utilizing tools within most existing agent systems is trivial
    due to the limited variety of tools available. In contrast, for a sophisticated
    wireless communication system, many API tools with complicated invoking logic
    exist, thereby presenting substantial challenges for effective implementation
    and management.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于6G导向的大型语言模型（LLM）代理具有独特的特点，它与通用型LLM存在显著的差异。例如，一个先进的6G LLM代理需要从大量特定领域的知识库中进行广泛学习。此外，由于现有代理系统中可用的工具种类有限，使用这些工具变得相对简单。相比之下，对于复杂的无线通信系统，存在许多具有复杂调用逻辑的API工具，这为有效实施和管理带来了巨大挑战。
- en: In order to address the issue mentioned above, we propose an innovative system
    paradigm for the training and inferencing process of the task-oriented 6G LLM
    agents. Specifically, we propose a two-stage continual training methodology. In
    the first stage, we aim to inject domain-specific knowledge into a pre-training
    general LLM, enhancing its capacity to understand the communication tasks and
    orchestrate the system. The resulting model from this phase is termed the field
    basic model since it can be further fine-tuned to adapt to specified communication
    scenarios or tasks. In the second stage, we use a small amount of high-quality
    data to fine-tune the level-$1$ model. This refinement ensures that the model’s
    planning and decision-making capabilities are optimally tailored to the inherent
    functionalities of the host system. The adaptability of this fine-tuning stage
    allows for easy adoption across diverse scenarios and applications. Moreover,
    we devise a communication system-specified retrieval approach to broaden the agent’s
    proficiency in utilizing tools. This method significantly improves the accuracy
    of invoking professional functions compared to existing methodologies.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们提出了一种创新的系统范式，用于任务导向型6G LLM代理的训练和推理过程。具体来说，我们提出了一种两阶段的持续训练方法。在第一阶段，我们旨在将领域特定的知识注入到一个预训练的通用LLM中，增强其理解通信任务和协调系统的能力。通过这一阶段的训练得到的模型被称为领域基础模型，因为它可以进一步微调，以适应特定的通信场景或任务。在第二阶段，我们使用少量高质量数据对第$1$层模型进行微调。这一精炼过程确保了模型的规划和决策能力最优化地与宿主系统的固有功能相匹配。这一微调阶段的适应性使得该方法能够轻松适应各种场景和应用。此外，我们设计了一种通信系统特定的检索方法，拓宽了代理使用工具的能力。与现有方法相比，这种方法显著提高了调用专业功能的准确性。
- en: 'The remainder of this paper is organized as follows. The system framework is
    described in section [II](https://arxiv.org/html/2410.03688v1#S2 "II System Framework
    ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation").
    The two-stage LLM training approach is given in section [III](https://arxiv.org/html/2410.03688v1#S3
    "III Two-Stage Model training ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented
    Physical-Layer Automation"). Section [IV](https://arxiv.org/html/2410.03688v1#S4
    "IV Workflow Design for Task-Oriented Physical-Layer Automation ‣ 6G LLM Agents:
    A Novel Paradigm for Task-Oriented Physical-Layer Automation") introduces the
    proposed workflow of the LLM agents. Section [V](https://arxiv.org/html/2410.03688v1#S5
    "V Experiments ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer
    Automation") introduces our experiment setup and corresponding results, which
    evaluates the performance of our proposed approach from different perspectives.
    Section [VI](https://arxiv.org/html/2410.03688v1#S6 "VI Conclusions ‣ 6G LLM Agents:
    A Novel Paradigm for Task-Oriented Physical-Layer Automation") draws our main
    conclusions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织结构如下。系统框架在[II](https://arxiv.org/html/2410.03688v1#S2 "II System Framework
    ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation")节中进行了描述。两阶段LLM训练方法在[III](https://arxiv.org/html/2410.03688v1#S3
    "III Two-Stage Model training ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented
    Physical-Layer Automation")节中给出。[IV](https://arxiv.org/html/2410.03688v1#S4 "IV
    Workflow Design for Task-Oriented Physical-Layer Automation ‣ 6G LLM Agents: A
    Novel Paradigm for Task-Oriented Physical-Layer Automation")节介绍了所提议的LLM代理工作流。[V](https://arxiv.org/html/2410.03688v1#S5
    "V Experiments ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer
    Automation")节介绍了我们的实验设置及相应结果，从不同角度评估了我们提出的方法的性能。[VI](https://arxiv.org/html/2410.03688v1#S6
    "VI Conclusions ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer
    Automation")节总结了我们的主要结论。'
- en: '![Refer to caption](img/e7e7b50ae406bb1933b6593a0453987e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/e7e7b50ae406bb1933b6593a0453987e.png)'
- en: 'Figure 2: The proposed three model levels and the corresponding functions.
    The L1 model is trained from the pre-training L0 model and serves as the domain
    basic model. L2 models are fine-tuned based on the L1 model tailored for specific
    target scenarios or agent roles.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提出的三种模型层级及其对应功能。L1模型是从预训练的L0模型中训练得到的，并作为领域基础模型。L2模型则是基于L1模型进一步微调，针对特定目标场景或代理角色进行定制。
- en: II System Framework
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 系统框架
- en: 'We consider a typical scenario in which the LLM agent is deployed at the base
    station to orchestrate the entire radio access network (RAN) within a cell. As
    shown in Fig. [1](https://arxiv.org/html/2410.03688v1#S1.F1 "Figure 1 ‣ I Introduction
    ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation"),
    the 6G LLM agents system is mainly composed of three parts: the LLM, multi-source
    perception, and tools. The LLM serves as the brain of the system. It first takes
    the upper-layer task description and the environment perception as the input,
    thus understanding the transmission requirements. Then, it outputs the reasoning
    and planning via the chain of thoughts (CoT) or tree of thoughts (ToT), thereby
    dynamically reconfiguring the system’s setup and workflow accordingly.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑一个典型场景，其中LLM智能体被部署在基站，用于协调一个小区内的整个无线接入网络（RAN）。如图[1](https://arxiv.org/html/2410.03688v1#S1.F1
    "图 1 ‣ I 引言 ‣ 6G LLM智能体：任务导向物理层自动化的新范式")所示，6G LLM智能体系统主要由三个部分组成：LLM、多源感知和工具。LLM作为系统的大脑，首先将上层任务描述和环境感知作为输入，从而理解传输需求。然后，通过思维链（CoT）或思维树（ToT）输出推理和规划，从而动态调整系统的配置和工作流程。
- en: The distinguishing factors that set the 6G agent apart from other applications
    lie in its advanced capacity to perceive the scattering environment and adeptly
    utilize tools. This perception capability is significantly enhanced by integrating
    multi-modal abilities such as channel estimation, sensing, vision, and digital
    twins. With an increasingly precise understanding of the environment, the system
    is enabled to make effective adjustments. Moreover, future communication systems
    are poised to integrate more comprehensive tool sets, featuring cutting-edge functionalities
    like massive multiple-input multiple-output (MIMO) technology and AI-empowered
    physical layer functions, thereby enhancing their overall performance and versatility.
    In totality, these interconnected modules collectively constitute the architecture
    of the highly sophisticated 6G agent.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 6G智能体与其他应用的区别在于其先进的感知能力，能够有效感知散射环境并熟练运用工具。这种感知能力通过整合多模态功能，如信道估计、传感、视觉和数字双胞胎等，得到了显著增强。通过对环境的日益精确的理解，系统能够进行有效的调整。此外，未来的通信系统预计将集成更多的综合工具集，具有前沿的功能，如大规模多输入多输出（MIMO）技术和人工智能驱动的物理层功能，从而提升其整体性能和多功能性。总的来说，这些相互连接的模块共同构成了高度复杂的6G智能体架构。
- en: III Two-Stage Model training
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 两阶段模型训练
- en: 'Due to the complexity of the natural environment and the diverse roles of the
    devices being applied, it is not practical to deploy a universal model for any
    circumstances. Thus, we propose a three-level model architecture shown in Fig.
    [2](https://arxiv.org/html/2410.03688v1#S1.F2 "Figure 2 ‣ I Introduction ‣ 6G
    LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation"). This
    framework commences with a foundational pre-training model, referred to as the
    L0 model, which is designed for general-purpose targets such as understanding
    natural language and formatting the outputs. Building upon this foundation model,
    we develop an L1 model through knowledge injection. This L1, or field basic model,
    encapsulates a reservoir of common domain-specific knowledge and earns expert
    ability for solving domain-related problems. Subsequently, by harmonizing the
    generalized knowledge embodied in the L1 model with the unique circumstances encountered
    by a particular deployment device, we derive the L2 model, which helps reasoning
    and planning and can be directly deployed in the practical 6G system.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自然环境的复杂性以及应用设备的多样化角色，部署通用模型以适应所有环境是不现实的。因此，我们提出了一个三层模型架构，如图[2](https://arxiv.org/html/2410.03688v1#S1.F2
    "图 2 ‣ I 引言 ‣ 6G LLM智能体：任务导向物理层自动化的新范式")所示。该框架从一个基础的预训练模型开始，称为L0模型，旨在处理如理解自然语言和格式化输出等通用任务。在此基础模型之上，我们通过知识注入开发了L1模型。L1模型，即领域基础模型，包含了大量常见的领域特定知识，并具备解决领域相关问题的专家能力。随后，通过将L1模型中体现的通用知识与特定部署设备所遇到的独特情况相结合，我们得出了L2模型，该模型有助于推理和规划，并可以直接部署在实际的6G系统中。
- en: '![Refer to caption](img/63bc2a10ad9cca8638c516448d038e3c.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/63bc2a10ad9cca8638c516448d038e3c.png)'
- en: '(a) Stage 1: Continual Training'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 阶段 1：持续训练
- en: '![Refer to caption](img/65aaffe0fed333d616da5dfc272e9f17.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/65aaffe0fed333d616da5dfc272e9f17.png)'
- en: '(b) Stage 2: LoRA Fine-Tuning'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 阶段 2：LoRA微调
- en: 'Figure 3: Learning structure of two-stage training process.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：两阶段训练过程的学习结构。
- en: '![Refer to caption](img/0521799e6437c085d90230cd54375acd.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0521799e6437c085d90230cd54375acd.png)'
- en: 'Figure 4: Example of the workflow of a communication system orchestrated by
    LLM agents. LLM agents play different roles in different parts of the system.
    The agents could understand UE’s upper-layer task requirements and orchestrate
    the system step by step in an optimal way accordingly.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：LLM代理协调的通信系统工作流示例。LLM代理在系统的不同部分扮演不同角色。代理可以理解用户设备（UE）上层任务的要求，并根据需要逐步优化系统操作。
- en: 'III-A Stage One: Knowledge Injection Through Continual Learning'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 第一阶段：通过持续学习注入知识
- en: In the stage-1 training process, we propose a continual training scheme by leveraging
    a pre-training general-purpose LLM. The rationale behind utilizing a pre-training
    model lies in its capacity to leverage the inherent understanding of language
    directly. Further, we propose to train only the parameters within the fully connected
    layers of the pre-training model’s top $K$ transformer blocks rather than engaging
    in full parameter training. This idea is based on several key considerations.
    Firstly, despite the massive scale of available training datasets, it remains
    insufficient to effectively train a complete model with billions of parameters
    without causing significant overfitting issues. Secondly, catastrophic forgetting
    is a prevalent challenge when training large language models. This effect is pronounced
    when training with highly structured and simplified corpus, which is common in
    the telecommunication domain, such as the protocol documents and function instructions.
    The attention layer in the LLM works to maintain the implicit token correlations
    at the natural language level, which would be easily ruined by those data. Thus,
    by selectively training only the uppermost feed-forward layers while freezing
    the underlying attention layers, we significantly enhance the robustness and mitigate
    the risk of catastrophic forgetting in the LLM.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一阶段的训练过程中，我们提出了一种通过利用预训练通用大型语言模型（LLM）进行持续训练的方案。使用预训练模型的原因在于其能够直接利用语言的固有理解能力。此外，我们建议仅训练预训练模型顶部$K$个变换器块中的全连接层的参数，而不是进行全面的参数训练。这个想法基于几个关键考虑。首先，尽管可用的训练数据集规模庞大，但仍不足以有效训练一个包含数十亿参数的完整模型，而不引发显著的过拟合问题。其次，灾难性遗忘是训练大型语言模型时常见的挑战。当使用高度结构化且简化的语料库进行训练时，这种现象尤为突出，而这种语料库在电信领域尤为常见，例如协议文档和功能说明。LLM中的注意力层用于保持自然语言层面上隐含的词汇关联，这些关联容易被这些数据破坏。因此，通过选择性地仅训练最上层的前馈层，并冻结底层的注意力层，我们显著提高了模型的鲁棒性，并减少了灾难性遗忘的风险。
- en: The training task is set to a combination of unsupervised self-regression and
    supervised instruction following. The unsupervised self-regression approach is
    widely employed in the training regimen of LLMs and eliminates the need for labeled
    data. However, this learning methodology can lead to potential inefficiencies
    in the learning process and difficulty in directing the model’s attention toward
    crucial information. Thus, we propose to involve the utilization of a generic
    LLM to extract the critical information of the prompted document and generate
    high-quality labeled data in the format of question-answer pairs. This way, the
    model pays more attention to information of higher importance, such as technical
    terminology. This way, we greatly enhance the model’s expert knowledge while maintaining
    robustness.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练任务设定为无监督自回归与有监督指令跟随的结合。无监督自回归方法在LLM的训练过程中广泛应用，并且无需标注数据。然而，这种学习方法可能导致学习过程中的潜在低效，并且难以将模型的注意力集中在关键信息上。因此，我们提出利用通用LLM提取提示文档中的关键信息，并生成高质量的标注数据，采用问答对格式。这种方式可以使模型更加关注重要信息，例如技术术语。通过这种方式，我们大大增强了模型的专业知识，同时保持了鲁棒性。
- en: 'III-B Stage Two: Role-Specific Fine-Tuning'
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 第二阶段：角色特定的微调
- en: The objective of the stage-2 training regimen is to refine the model’s propensity
    for making decisions tailored to the specific role and function of the target
    agent. For example, an environment observing agent should typically exist that
    keeps monitoring the changing of the transmission environment and a system-configure
    agent that changes the system configuration accordingly. A distinguishing attribute
    among different agents lies in their respective API databases. Therefore, the
    initial step of data preprocessing of this step is the creation of comprehensive
    API database documentation, which encapsulates meticulous descriptions of the
    functionalities, parameter definitions, and output specifications. Subsequently,
    this enables the procurement of a query and task decomposition corpus. Queries
    within this corpus can manifest in diverse formats, from direct task descriptions
    to specific transmission requirements. Every composite step in the corpus corresponds
    to a potential API within the established database.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段训练计划的目标是优化模型在做出决策时能够根据目标代理的特定角色和功能量身定制。例如，通常应该存在一个环境观察代理，持续监控传输环境的变化，以及一个系统配置代理，根据变化调整系统配置。不同代理之间的一个重要特征是它们各自的API数据库。因此，本步骤数据预处理的第一步是创建全面的API数据库文档，其中包含功能、参数定义和输出规范的详细描述。接下来，这使得查询和任务分解语料库的获取成为可能。该语料库中的查询可以以多种格式呈现，从直接的任务描述到具体的传输需求。语料库中的每个复合步骤对应着已建立数据库中的一个潜在API。
- en: We propose to adopt the LoRA method to fine-tune the L1 model with a supervised
    fine-tuning scheme. This involves appending a low-rank adapter to each parameter
    matrix within the original transformer architecture, thereby enabling the model
    to swiftly adapt its planning and decision-making capabilities according to its
    unique tool package. Adopting this training configuration presents several salient
    benefits. Primarily, the compact size of the LoRA adapters allows them to be effortlessly
    updated in the cloud server and subsequently downloaded by the Base Station (BS)
    or User Equipment (UE), thus guaranteeing operational flexibility. Furthermore,
    the LoRA technique exhibits a lower propensity for causing catastrophic forgetting
    or overfitting when dealing with datasets of limited size, thereby ensuring the
    overall robustness of the entire system. It is worth mentioning that each query
    in the training corpus is concatenated with a fixed prompt that describes the
    agent’s role, which will help enhance the convergence of the training process.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议采用LoRA方法对L1模型进行监督微调。这涉及在原始变压器架构中的每个参数矩阵上附加一个低秩适配器，从而使得模型能够迅速根据其独特的工具包调整规划和决策能力。采用这种训练配置具有几个显著的好处。首先，LoRA适配器的小巧尺寸使得它们能够轻松地在云服务器上更新，并随后被基站（BS）或用户设备（UE）下载，从而保证了操作的灵活性。此外，LoRA技术在处理有限大小的数据集时，表现出较低的灾难性遗忘或过拟合的倾向，从而确保了整个系统的整体鲁棒性。值得一提的是，训练语料库中的每个查询都与一个固定的提示拼接，描述了代理的角色，这将有助于增强训练过程的收敛性。
- en: IV Workflow Design for Task-Oriented Physical-Layer Automation
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 面向任务的物理层自动化工作流设计
- en: IV-A Role-Playing and Collaboration of Agents
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 代理的角色扮演与协作
- en: There are two main concerns regarding clearly defining LLM agents’ different
    roles in the system. Firstly, from the point of practical usage of LLM, clear
    role definitions facilitate the optimization of prompt design, thereby enhancing
    the accuracy of model outputs and mitigating the likelihood of model hallucinations.
    Secondly, it will help facilitate streamlining system processes and enhance system
    efficiency since each role has straightforward triggering logic.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 关于明确界定LLM代理在系统中不同角色的两大主要关注点。首先，从LLM实际使用的角度来看，清晰的角色定义有助于优化提示设计，从而提高模型输出的准确性，并减少模型幻觉的可能性。其次，明确的角色定义将有助于简化系统流程并提高系统效率，因为每个角色都有明确的触发逻辑。
- en: 'In the proposed system workflow shown in Fig. [4](https://arxiv.org/html/2410.03688v1#S3.F4
    "Figure 4 ‣ III Two-Stage Model training ‣ 6G LLM Agents: A Novel Paradigm for
    Task-Oriented Physical-Layer Automation"), we define four LLM agent roles to finish
    the system workflow. For each coming UE task, the task awareness agent will first
    translate the upper-layer task description to the precise transmission requirements.
    The observer agent will constantly monitor the scattering environment and summarize
    the system state. Each time there is a new UE task or changing environment, the
    system configuration agent will give its reasoning and planning with the complete
    chain of thoughts. After API retrieval, the API invoking agent will help run those
    APIs correctly. After all the abovementioned processes, a reporting agent will
    summarize the executing results for potential error-catching and shooting.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[4](https://arxiv.org/html/2410.03688v1#S3.F4 "Figure 4 ‣ III Two-Stage Model
    training ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented Physical-Layer Automation")所示的提议系统工作流中，我们定义了四个LLM代理角色来完成系统工作流。对于每个即将到来的UE任务，任务感知代理首先会将上层任务描述翻译为精确的传输需求。观察者代理将持续监控散射环境并总结系统状态。每当有新的UE任务或环境变化时，系统配置代理会给出完整的推理和规划思路。在API检索之后，API调用代理将帮助正确执行这些API。在所有上述过程完成后，报告代理将总结执行结果以便捕获潜在错误并进行修正。'
- en: IV-B API Retrieve Through Vector-Based Semantic Similarity Search
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基于向量语义相似性搜索的API检索
- en: In existing works, tool utilization relies on putting all the API descriptions
    into the prompts. This approach proves practical for simple tasks when the number
    of APIs involved is limited. However, there are thousands of APIs for the complex
    communication system, and it is not practical to include all in the prompt due
    to the context length constraints. To address this issue, we propose a novel solution
    by semantic vectorizing API instructions in an extended vector database and utilizing
    similarity search for retrieval.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的工作中，工具的使用依赖于将所有的API描述放入提示中。这种方法在任务简单且涉及的API数量有限时是实用的。然而，复杂的通信系统中有成千上万的API，由于上下文长度的限制，将所有API包含在提示中并不现实。为了解决这个问题，我们提出了一种新颖的解决方案，通过在扩展的向量数据库中进行API指令的语义向量化，并利用相似性搜索进行检索。
- en: Firstly, we use a pre-trained semantic sentence-to-vector model to encode all
    the API instructions into fixed-length vectors. Then, for each output step of
    the LLM, we apply the same model to generate a semantic representation vector.
    Then, we calculate the GCS (Generalized Cosine Similarity) between the generated
    step vector and each API embedding stored in the database, which can be written
    as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用预训练的语义句子到向量模型将所有的API指令编码为固定长度的向量。然后，对于LLM的每个输出步骤，我们应用相同的模型生成语义表示向量。接着，我们计算生成的步骤向量与存储在数据库中的每个API嵌入之间的GCS（广义余弦相似度），可以表示为
- en: '|  | $GCS=\frac{\mathbf{D}\cdot\mathbf{S}}{\left\&#124;\mathbf{D}\right\&#124;\cdot\left\&#124;%
    \mathbf{S}\right\&#124;},$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $GCS=\frac{\mathbf{D}\cdot\mathbf{S}}{\left\&#124;\mathbf{D}\right\&#124;\cdot\left\&#124;%
    \mathbf{S}\right\&#124;},$ |  | (1) |'
- en: where $\mathbf{D}$ is the embedding vector of the items in the API database
    and $\mathbf{S}$ denotes the embedding vector of the planning steps by the LLM.
    The API with the highest similarity score is then selected as the most relevant
    match. This strategy allows for efficient and scalable API retrieval even when
    the API inventory is vast and diverse.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathbf{D}$是API数据库中项目的嵌入向量，$\mathbf{S}$表示LLM规划步骤的嵌入向量。然后，选择相似度得分最高的API作为最相关的匹配。该策略即使在API库庞大且多样时，也能高效、可扩展地进行API检索。
- en: IV-C Tool Execution Based on Self-Organization
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 基于自组织的工具执行
- en: How to accurately plan the invocation of tools and successfully execute them
    by finding the appropriate actual parameters for each API has always been a challenging
    issue. In existing works, the common approach is rigorously constraining the parameter
    format of the APIs and implementing argument filtering in the program. Thus, all
    the tools should be developed specifically at the beginning stage, or extra efforts
    should be made to refine them to meet the format requirement. However, there exists
    a huge library of API functions developed for different protocol standards, which
    are impractical to switch to a common format. Therefore, we propose employing
    LLMs as an intermediary to bridge these diverse APIs and effectively identify
    the requisite parameters.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如何准确规划工具的调用，并通过找到每个API的适当实际参数成功执行它们，一直是一个具有挑战性的问题。在现有的工作中，常见的方法是严格约束API的参数格式，并在程序中实现参数过滤。因此，所有工具应在初始阶段专门开发，或者需要额外的努力来完善它们以满足格式要求。然而，存在着大量为不同协议标准开发的API函数，转向统一格式是不可行的。因此，我们提出采用LLM作为中介，架起这些不同API之间的桥梁，并有效识别所需的参数。
- en: 'As shown in Fig. [4](https://arxiv.org/html/2410.03688v1#S3.F4 "Figure 4 ‣
    III Two-Stage Model training ‣ 6G LLM Agents: A Novel Paradigm for Task-Oriented
    Physical-Layer Automation"), it is illustrated that the API invoking agent will
    step in every time prior to executing an API and find the corresponding input
    parameters. The parameter inputs have three sources: values in the task descriptions,
    outcomes from previously executed APIs, and default values inherent to the current
    API under execution. Thus, we integrate all the sources in the prompt and require
    the LLM to find the value through physical meaning automatically. For example,
    if the former API instruction mentions that the estimated channel state information
    matrix will be stored in a temporary variable named ‘CSI_matrix’ and the under-executing
    API’s instruction mentions that it requires the estimated CSI as the value of
    a parameter named ‘estimated_channel’, the LLM will automatically realize that
    ‘estimated_channel = CSI_matrix’. It is worth mentioning that this method is not
    trivial since it requires the LLM to recognize those terminologies and connect
    them with the descriptions, which is highly reliant on expert knowledge injection.
    Thus, the API-invoking agent needs to be driven by our L1 model, which ensures
    the smoothness and continuity of system actions.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[4](https://arxiv.org/html/2410.03688v1#S3.F4 "图 4 ‣ III 两阶段模型训练 ‣ 6G LLM
    代理：任务导向物理层自动化的新范式")所示，API调用代理将在每次执行API之前介入，并查找相应的输入参数。参数输入有三个来源：任务描述中的值、之前执行的API的结果以及当前执行API固有的默认值。因此，我们将所有来源集成到提示中，并要求LLM通过物理含义自动找到值。例如，如果前一个API指令提到估计的信道状态信息矩阵将存储在一个名为‘CSI_matrix’的临时变量中，而正在执行的API指令提到它需要将估计的CSI作为名为‘estimated_channel’的参数值，则LLM将自动意识到‘estimated_channel
    = CSI_matrix’。值得一提的是，这种方法并非易事，因为它要求LLM识别这些术语并将它们与描述联系起来，这在很大程度上依赖于专家知识的注入。因此，API调用代理需要由我们的L1模型驱动，这确保了系统动作的流畅性和连续性。
- en: V Experiments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验
- en: V-A Parameters Setting and Datasets Generation
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 参数设置和数据集生成
- en: We employ two different sizes of open-source LLM models, specifically LLaMA2-13B
    and LLaMA2-70B, as our foundation pre-trained L0 models. During the stage-one
    continual training, we train the parameters within the feed-forward layers of
    the uppermost transformer blocks, and the total trainable parameters are approximately
    3.5 billion for both the 13B and 70B models. In stage-two training, we further
    adopt the LoRA method for all matrices within the attention blocks (Query, Key,
    and Value) across the entire transformer network. The adapter rank is set to $16$,
    thereby introducing approximately an additional $0.6\%$ of trainable parameters
    compared with the original model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用两种不同规模的开源LLM模型，分别是LLaMA2-13B和LLaMA2-70B，作为我们的基础预训练L0模型。在第一阶段的持续训练中，我们训练最高变换块中的前馈层参数，13B和70B模型的总可训练参数大约为35亿。在第二阶段训练中，我们进一步采用LoRA方法处理整个变换网络中注意力块（查询、键和值）中的所有矩阵。适配器的秩设置为$16$，因此与原始模型相比，引入了大约额外$0.6\%$的可训练参数。
- en: V-A1 Dataset Generation
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: V-A1 数据集生成
- en: For the stage one training, the datasets encompass diverse textual resources,
    including 3GPP documentation, ArXiv papers related to telecommunication, and open-source
    datasets such as Telecom Q&A [[9](https://arxiv.org/html/2410.03688v1#bib.bib9)].
    To ensure the quality of the datasets, all original texts undergo rigorous preprocessing
    procedures involving comprehensive data cleaning and structural formatting. Additionally,
    we employ Tongyi-Max to help clean and rephrase some corpus. Thus, the information-intensive
    parts will be highlighted.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一阶段的训练，数据集包含了多种文本资源，包括3GPP文档、与电信相关的ArXiv论文以及开源数据集如Telecom Q&A [[9](https://arxiv.org/html/2410.03688v1#bib.bib9)]。为了确保数据集的质量，所有原始文本都经过严格的预处理程序，涉及全面的数据清理和结构化格式化。此外，我们还使用了**Tongyi-Max**来帮助清理和重述部分语料。因此，信息密集的部分将被突出显示。
- en: For stage two fine-tuning, we initially constructed a tool library comprising
    approximately $200$ APIs. Subsequently, we manually create about $20000$ in corpus
    composed of UE tasks, transmission requirements, and corresponding decomposited
    action steps. Each action step corresponds to an existing API in the tool library.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二阶段的微调，我们最初构建了一个包含约$200$个API的工具库。随后，我们手动创建了约$20000$的语料，内容包括UE任务、传输要求及其相应的分解步骤。每个动作步骤对应工具库中的一个现有API。
- en: V-B Experimental Results
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 实验结果
- en: '![Refer to caption](img/e20171ebc379fa2efbcdded739c931d3.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e20171ebc379fa2efbcdded739c931d3.png)'
- en: 'Figure 5: The winning ratio comparison of answering communication related questions
    between three LLMs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：三种大型语言模型（LLM）在回答与通信相关问题时的胜率比较。
- en: '![Refer to caption](img/64e01306f3fc20df41432a230f2cbb3d.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/64e01306f3fc20df41432a230f2cbb3d.png)'
- en: 'Figure 6: The cosine similarity measurement is computed between the embedding
    vector representative of the $35$ exemplary step descriptions and the respective
    embedding vectors of the API instructions within the database.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：通过计算$35$个示例步骤描述的嵌入向量与数据库中API指令的嵌入向量之间的余弦相似度来进行度量。
- en: '![Refer to caption](img/bc77f307dfd84a9799b41aae18944c60.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bc77f307dfd84a9799b41aae18944c60.png)'
- en: 'Figure 7: The correction rate comparison of task decomposition between the
    comparison models.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：比较模型之间任务分解的修正率比较。
- en: '![Refer to caption](img/eb5018732fb88c87808b5d89f2b8ca65.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/eb5018732fb88c87808b5d89f2b8ca65.png)'
- en: 'Figure 8: A conceptual demo of adopting LLM agents to orchestrate a practical
    communication system.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：采用LLM代理来协调一个实际通信系统的概念演示。
- en: 'Firstly, we compare the performance of our L1 model against the LLaMA2 and
    Claude when answering telecommunication-related questions, thereby verifying the
    effect of domain-specific knowledge injection. We gather a testing dataset consisting
    of $4000$ 3GPP protocol-related questions, accompanied by their correct answers
    and corresponding explanations. These questions are presented to three comparative
    models with the same prompt. A third-party LLM, Tongyi-max, is prompted with all
    the questions alongside their respective answers and explanations. The task assigned
    to Tongyi-max is to evaluate which of the provided answers aligns more closely
    with the correct answer. Fig .[5](https://arxiv.org/html/2410.03688v1#S5.F5 "Figure
    5 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel Paradigm
    for Task-Oriented Physical-Layer Automation") shows the overall winning ratio
    for the three models. Evidently, with the implementation of knowledge injection,
    the L1 model consistently outperformed its counterparts across both the 13B and
    70B models, proving the effectiveness of training.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将L1模型与LLaMA2和Claude进行比较，验证领域特定知识注入的效果，特别是在回答电信相关问题时。我们收集了一个测试数据集，包含$4000$个与3GPP协议相关的问题，并附上正确答案及其相应的解释。这些问题通过相同的提示呈现给三种比较模型。一个第三方的LLM，**Tongyi-max**，接收到所有问题以及相应的答案和解释。分配给Tongyi-max的任务是评估哪些提供的答案与正确答案更为接近。图[5](https://arxiv.org/html/2410.03688v1#S5.F5
    "图5 ‣ V-B 实验结果 ‣ V 实验 ‣ 6G LLM代理：一种用于任务导向的物理层自动化的新范式")展示了三种模型的整体胜率。显然，通过知识注入的实施，L1模型在13B和70B模型上均表现优于其对手，证明了训练的有效性。
- en: 'Secondly, to illustrate the feasibility of the proposed semantic similarity
    search-based API retrieval method. We randomly selected $35$ exemplary step instances
    outputted by the L2 model, which should correspond to the first $50$ APIs in the
    library and obtain the semantic embedding vectors. Then, we calculate the GCS
    between the step description embedding vectors and the API instruction embedding
    vectors. Results are shown in Fig. [6](https://arxiv.org/html/2410.03688v1#S5.F6
    "Figure 6 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation"). It can be observed from
    the results that for each instance, the correct API always has the highest similarity,
    significantly surpassing other wrong APIs, which ensures that we can always find
    the correct API for each planning step.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，为了说明所提出的基于语义相似度搜索的API检索方法的可行性，我们随机选择了L2模型输出的$35$个示例步骤实例，这些实例应当对应于库中的前$50$个API，并获取它们的语义嵌入向量。然后，我们计算步骤描述嵌入向量与API指令嵌入向量之间的GCS。结果如图[6](https://arxiv.org/html/2410.03688v1#S5.F6
    "Figure 6 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation")所示。从结果中可以观察到，对于每个实例，正确的API总是具有最高的相似度，显著超过其他错误的API，这确保了我们始终能够为每个规划步骤找到正确的API。'
- en: 'Thirdly, to further validate the effectiveness of the stage-two training, we
    collect a validation dataset comprising $1000$ query instances across various
    styles, along with their corresponding optimal decomposition steps. Then, we compare
    the response of the L2 model with three comparison models shown in Fig. [7](https://arxiv.org/html/2410.03688v1#S5.F7
    "Figure 7 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation"), which include the original
    LLaMA2 model, the L1 model, and the directly fine-tuned model without knowledge
    injection continual training. To ensure a fair comparison, we employed the identical
    prompt and used the above-mentioned method to call the corresponding API of each
    step and compare the results with the grand truth. Thus, we can calculate the
    correcting rate of each model. As shown in Fig. [7](https://arxiv.org/html/2410.03688v1#S5.F7
    "Figure 7 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation"), the proposed L2 model
    exhibits the highest accuracy ratio and greatly surpasses other components, which
    proves the effectiveness of the stage-two training process. Besides, a noteworthy
    observation is that, following stage-one training, the L2 model significantly
    outperforms the model that was directly fine-tuned without any knowledge injection.
    This result further highlights the feasibility of our proposed training methodology.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，为了进一步验证二阶段训练的有效性，我们收集了一个验证数据集，其中包含$1000$个查询实例，涵盖了不同的风格，以及它们对应的最优分解步骤。然后，我们将L2模型的响应与图[7](https://arxiv.org/html/2410.03688v1#S5.F7
    "Figure 7 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation")中展示的三种对比模型进行了比较，包括原始的LLaMA2模型、L1模型以及没有知识注入持续训练的直接微调模型。为了确保公平比较，我们采用了相同的提示，并使用上述方法调用每个步骤的相应API，并将结果与实际结果进行比较。这样，我们可以计算每个模型的纠正率。如图[7](https://arxiv.org/html/2410.03688v1#S5.F7
    "Figure 7 ‣ V-B Experimental Results ‣ V Experiments ‣ 6G LLM Agents: A Novel
    Paradigm for Task-Oriented Physical-Layer Automation")所示，所提出的L2模型展示了最高的准确率，并大大超过了其他模型，这证明了二阶段训练过程的有效性。此外，一个值得注意的观察是，在第一阶段训练后，L2模型显著优于没有任何知识注入的直接微调模型。这一结果进一步凸显了我们所提训练方法的可行性。'
- en: Finally, in order to intuitively illustrate the advantages of applying LLMs
    in wireless communication systems, we have made a conceptual demo of adopting
    LLM agents for automatically orchestrating the invocation of DeepTx and DeepRx
    [[10](https://arxiv.org/html/2410.03688v1#bib.bib10)] to enhance system throughput
    and address PA nonlinearity issues caused by rising device temperatures. The detailed
    video demo can be obtained in ‘https://github.com/hongan-nokia/bell_labs_6G_llm’
    for further reference.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了直观地展示在无线通信系统中应用LLM的优势，我们制作了一个概念性演示，展示了采用LLM代理自动编排DeepTx和DeepRx的调用[[10](https://arxiv.org/html/2410.03688v1#bib.bib10)]，以增强系统吞吐量并解决由设备温度升高引起的PA非线性问题。详细的视频演示可以通过‘https://github.com/hongan-nokia/bell_labs_6G_llm’获取，供进一步参考。
- en: VI Conclusions
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 结论
- en: This paper proposed a novel paradigm for training and applying LLM agents for
    task-oriented 6G physic-layer automation. By employing a specifically designed
    two-stage continual training approach, the trained domain-adapted LLM effectively
    aids in understanding the upper-level requirements of users and accordingly recommends
    the workflow and the optimal system configurations. Further, with the proposed
    workflow of building the automation system, the responses of those LLM agents
    can be mapped to the actual invoking of corresponding APIs, which finally orchestrate
    the system for achieving higher performance. Experimental results validated the
    feasibility and effectiveness of the proposed methodology and its potential to
    tackle practical challenges in the future design of wireless communication systems.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种用于任务导向的6G物理层自动化的LLM代理训练和应用的新范式。通过采用专门设计的两阶段持续训练方法，训练后的领域适配LLM有效地帮助理解用户的高层次需求，并相应地推荐工作流和最优系统配置。此外，结合所提出的自动化系统构建工作流，这些LLM代理的响应可以映射到实际调用相应的API，最终协调系统以实现更高的性能。实验结果验证了所提方法的可行性和有效性，并展示了其在未来无线通信系统设计中应对实际挑战的潜力。
- en: References
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] N. A. Khan and S. Schmid, “Ai-ran in 6g networks: State-of-the-art and
    challenges,” *IEEE Open Journal of the Communications Society*, vol. 5, pp. 294–311,
    2024.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] N. A. Khan 和 S. Schmid, “6G网络中的Ai-ran：现状与挑战,” *IEEE开放通信学会期刊*, 第5卷，第294–311页,
    2024年。'
- en: '[2] Z. Wei, H. Qu, Y. Wang, X. Yuan, H. Wu, Y. Du, K. Han, N. Zhang, and Z. Feng,
    “Integrated sensing and communication signals toward 5g-a and 6g: A survey,” *IEEE
    Internet of Things Journal*, vol. 10, no. 13, pp. 11 068–11 092, 2023.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Z. Wei, H. Qu, Y. Wang, X. Yuan, H. Wu, Y. Du, K. Han, N. Zhang, 和 Z. Feng,
    “面向5G-A和6G的集成感知与通信信号：综述,” *IEEE物联网期刊*, 第10卷，第13期，第11 068–11 092页, 2023年。'
- en: '[3] Y. Tao, J. Wu, X. Lin, S. Mumtaz, and S. Cherkaoui, “Digital twin and drl-driven
    semantic dissemination for 6g autonomous driving service,” in *GLOBECOM 2023 -
    2023 IEEE Global Communications Conference*, 2023, pp. 2075–2080.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Tao, J. Wu, X. Lin, S. Mumtaz, 和 S. Cherkaoui, “数字双胞胎与基于DRL的语义传播用于6G自动驾驶服务,”
    在*GLOBECOM 2023 - 2023 IEEE全球通信大会*, 2023年，第2075–2080页。'
- en: '[4] S. I. Loutfi, U. Tureli, and I. Shayea, “Augmented reality with mobility
    awareness in mobile edge computing over 6g network: A survey,” in *2023 10th International
    Conference on Wireless Networks and Mobile Communications (WINCOM)*, 2023, pp.
    1–6.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. I. Loutfi, U. Tureli, 和 I. Shayea, “在6G网络上通过移动边缘计算增强的现实与移动性感知：综述,” 在*2023年第10届无线网络与移动通信国际会议
    (WINCOM)*, 2023年，第1–6页。'
- en: '[5] S. Wan, Q. Yang, Z. Shi, Z. Yang, and Z. Zhang, “Cooperative task-oriented
    communication for multi-modal data with transmission control,” in *2023 IEEE International
    Conference on Communications Workshops (ICC Workshops)*, 2023, pp. 1635–1640.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Wan, Q. Yang, Z. Shi, Z. Yang, 和 Z. Zhang, “面向多模态数据的合作任务导向通信与传输控制,”
    在*2023 IEEE国际通信研讨会 (ICC研讨会)*, 2023年，第1635–1640页。'
- en: '[6] J. Shao, Y. Mao, and J. Zhang, “Learning task-oriented communication for
    edge inference: An information bottleneck approach,” *IEEE Journal on Selected
    Areas in Communications*, vol. 40, no. 1, pp. 197–211, 2022.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Shao, Y. Mao, 和 J. Zhang, “面向边缘推理的任务导向通信学习：信息瓶颈方法,” *IEEE通信领域精选杂志*,
    第40卷，第1期，第197–211页, 2022年。'
- en: '[7] A. K, S. Sudarshan G S, and S. J U, “Llm’s for autonomous driving: A new
    way to teach machines to drive,” in *2023 3rd International Conference on Mobile
    Networks and Wireless Communications (ICMNWC)*, 2023, pp. 1–6.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. K, S. Sudarshan G S, 和 S. J U, “自动驾驶中的LLM：一种教机器驾驶的新方法,” 在*2023年第3届国际移动网络与无线通信大会
    (ICMNWC)*, 2023年，第1–6页。'
- en: '[8] C. H. Song, B. M. Sadler, J. Wu, W.-L. Chao, C. Washington, and Y. Su,
    “Llm-planner: Few-shot grounded planning for embodied agents with large language
    models,” in *2023 IEEE/CVF International Conference on Computer Vision (ICCV)*,
    2023, pp. 2986–2997.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] C. H. Song, B. M. Sadler, J. Wu, W.-L. Chao, C. Washington, 和 Y. Su, “LLM规划器：基于大型语言模型的具身智能体少量实例规划,”
    在*2023 IEEE/CVF计算机视觉国际会议 (ICCV)*, 2023年，第2986–2997页。'
- en: '[9] L. Bariah, H. Zou, Q. Zhao, B. Mouhouche, F. Bader, and M. Debbah, “Understanding
    telecom language through large language models,” in *GLOBECOM 2023 - 2023 IEEE
    Global Communications Conference*, 2023, pp. 6542–6547.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] L. Bariah, H. Zou, Q. Zhao, B. Mouhouche, F. Bader, 和 M. Debbah, “通过大型语言模型理解电信语言,”
    在*GLOBECOM 2023 - 2023 IEEE全球通信大会*, 2023年，第6542–6547页。'
- en: '[10] M. Honkala, D. Korpi, and J. M. J. Huttunen, “Deeprx: Fully convolutional
    deep learning receiver,” *IEEE Transactions on Wireless Communications*, vol. 20,
    no. 6, pp. 3925–3940, 2021.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Honkala, D. Korpi 和 J. M. J. Huttunen，“Deeprx：全卷积深度学习接收器，” *IEEE无线通信学报*，第20卷，第6期，页码3925–3940，2021年。'
