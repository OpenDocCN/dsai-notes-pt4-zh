- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:22:18'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:22:18
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过LLM代理实现端到端实时语音翻译达到人工水平
- en: 来源：[https://arxiv.org/html/2407.21646/](https://arxiv.org/html/2407.21646/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2407.21646/](https://arxiv.org/html/2407.21646/)
- en: Cross Language Agent Team
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 跨语言代理团队
- en: ByteDance Research
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 字节跳动研究
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In this paper, we present Cross Language Agent - Simultaneous Interpretation,
    CLASI, a high-quality and human-like Simultaneous Speech Translation (SiST)¹¹1In
    this paper, we use Simultaneous Interpretation and Simultaneous Speech Translation
    interchangeably. System. Inspired by professional human interpreters, we utilize
    a novel data-driven read-write strategy to balance the translation quality and
    latency. To address the challenge of translating in-domain terminologies, CLASI
    employs a multi-modal retrieving module to obtain relevant information to augment
    the translation. Supported by LLMs, our approach can generate error-tolerated
    translation by considering the input audio, historical context, and retrieved
    information. Experimental results show that our system outperforms other systems
    by significant margins. Aligned with professional human interpreters, we evaluate
    CLASI with a better human evaluation metric, valid information proportion (VIP),
    which measures the amount of information that can be successfully conveyed to
    the listeners. In the real-world scenarios, where the speeches are often disfluent,
    informal, and unclear, CLASI achieves VIP of 81.3% and 78.0% for Chinese-to-English
    and English-to-Chinese translation directions, respectively. In contrast, state-of-the-art
    commercial or open-source systems only achieve 35.4% and 41.6%. On the extremely
    hard dataset, where other systems achieve under 13% VIP, CLASI can still achieve
    70% VIP. Demonstrations and human-annotated test sets are available at [https://byteresearchcla.github.io/clasi](https://byteresearchcla.github.io/clasi).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了跨语言代理 - 同声传译（CLASI），一个高质量、类人化的同声语音翻译（SiST）系统¹¹1在本文中，我们将“同声传译”和“同声语音翻译”互换使用。受到专业人工口译员的启发，我们利用一种新颖的数据驱动的读写策略，在翻译质量和延迟之间找到平衡。为了应对领域内术语翻译的挑战，CLASI采用了一个多模态检索模块，获取相关信息以增强翻译效果。在大型语言模型（LLMs）的支持下，我们的方法能够通过考虑输入音频、历史上下文和检索信息，生成容错性强的翻译。实验结果表明，我们的系统在各方面均大幅优于其他系统。与专业人工口译员的表现相符，我们使用一种更好的人工评估指标——有效信息比例（VIP），来评估CLASI，该指标衡量成功传达给听众的信息量。在实际应用场景中，演讲常常存在不流畅、非正式和不清晰的情况，CLASI在中译英和英译中的VIP分别达到81.3%和78.0%。相比之下，最先进的商业或开源系统仅能达到35.4%和41.6%。在一个极为困难的数据集上，其他系统的VIP低于13%，而CLASI仍能达到70%的VIP。演示和人工标注的测试集可在[https://byteresearchcla.github.io/clasi](https://byteresearchcla.github.io/clasi)访问。
- en: '{CJK}'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{CJK}'
- en: UTF8gbsn
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: UTF8gbsn
- en: '![Refer to caption](img/2a1f666b8228dfb6b5e2afe93bccfd19.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/2a1f666b8228dfb6b5e2afe93bccfd19.png)'
- en: 'Figure 1: Performance evaluation. CLASI significantly outperforms the leading
    commercial and open-source systems using a more reliable VIP metric, achieving
    human interpreter parity.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：性能评估。CLASI显著优于领先的商业和开源系统，使用更可靠的VIP指标，达到了与人工口译员相当的水平。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Simultaneous speech translation (SiST) is recognized as one of the most challenging
    tasks in the translation domain [jones2014conference](https://arxiv.org/html/2407.21646v2#bib.bib33)
    . Machine-assisted automatic interpretation has been receiving much attention
    in the natural language processing (NLP) community [iwslt-2021-international](https://arxiv.org/html/2407.21646v2#bib.bib18)
    ; [iwslt-2020-international](https://arxiv.org/html/2407.21646v2#bib.bib19) ;
    [iwslt-2022-international](https://arxiv.org/html/2407.21646v2#bib.bib66) ; [iwslt-2023-international](https://arxiv.org/html/2407.21646v2#bib.bib65)
    . Traditional simultaneous translation approaches [cho2016can](https://arxiv.org/html/2407.21646v2#bib.bib11)
    ; [gu2017learning](https://arxiv.org/html/2407.21646v2#bib.bib24) ; [zhao2021volctrans](https://arxiv.org/html/2407.21646v2#bib.bib89)
    usually employs a cascaded system, involving a streaming Automatic Speech Recognition
    (ASR) model, a punctuation model and a Machine Translation (MT) model. However,
    such cascaded systems often suffer error propagation and latency from the ASR
    module. Despite these advancements in both academic SiST models [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    ; [fukuda-etal-2023-naist](https://arxiv.org/html/2407.21646v2#bib.bib22) ; [liu2024recentadvancesendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib41)
    ; [papi-etal-2023-direct](https://arxiv.org/html/2407.21646v2#bib.bib51) ; [ren2020simulspeech](https://arxiv.org/html/2407.21646v2#bib.bib63)
    ; [zeng-etal-2021-realtrans](https://arxiv.org/html/2407.21646v2#bib.bib84) ;
    [zhang2023end](https://arxiv.org/html/2407.21646v2#bib.bib87) and commercial SiST
    engines, the translation quality is still far from satisfactory. As shown in [Figure 1](https://arxiv.org/html/2407.21646v2#S0.F1
    "In Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent"), we conduct a human assessment of the current accessible SiST
    systems. From the user-centered perspective, these systems only deliver less than
    42% of the valid information to listeners, which heavily affects communication
    effectiveness. In contrast, professional human interpreters usually deliver more
    than 70% of the necessary information [chmiel2021effects](https://arxiv.org/html/2407.21646v2#bib.bib10)
    and 95% ideally. Thus in this paper, we use 80% to indicate high-level human interpreters.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the huge success of LLMs in machine translation [achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)
    ; [brown2020gpt3](https://arxiv.org/html/2407.21646v2#bib.bib9) and speech translation
    [chu2023qwen](https://arxiv.org/html/2407.21646v2#bib.bib12) ; [huang2023speechtranslationlargelanguage](https://arxiv.org/html/2407.21646v2#bib.bib30)
    ; [reid2024gemini](https://arxiv.org/html/2407.21646v2#bib.bib62) , we propose
    to employ LLMs to accomplish the SiST task. Specifically, we identify three primary
    challenges. First, a key challenge for incorporating LLM into the SiST is the
    read-write policy, where LLM needs to provide partial translation for input speech.
    Second, achieving human equivalent performance requires understanding and translation
    of terminologies and uncommon phrases that LLMs cannot learn from training data.
    Lastly, the scarcity of training data continues to hinder the performance on the
    SiST task.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 受LLM在机器翻译[achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)；[brown2020gpt3](https://arxiv.org/html/2407.21646v2#bib.bib9)
    和语音翻译[chu2023qwen](https://arxiv.org/html/2407.21646v2#bib.bib12)；[huang2023speechtranslationlargelanguage](https://arxiv.org/html/2407.21646v2#bib.bib30)；[reid2024gemini](https://arxiv.org/html/2407.21646v2#bib.bib62)中取得的巨大成功的启发，我们提出使用LLM来完成SiST任务。具体而言，我们确定了三个主要挑战。首先，将LLM引入SiST的一个关键挑战是读写策略，LLM需要为输入语音提供部分翻译。第二，实现与人类等效的表现需要理解和翻译术语以及LLM无法从训练数据中学习的不常见短语。最后，训练数据的稀缺性持续影响SiST任务的表现。
- en: To address these challenges, we introduce our end-to-end approach, CLASI, a
    Cross-Lingual Agent that accomplishes Simultaneous Interpretation by iteratively
    performing multiple actions, as illustrated in Figure [2](https://arxiv.org/html/2407.21646v2#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent"). Regarding the first challenge, we imitate
    professional human interpreters to learn their policy of segmenting a complete
    sentence into several semantic “chunks” through syntactic boundaries (pauses,
    commas, conjunctions, etc.) and contextual meaning. To enable CLASI to learn such
    a policy, we follow a data-driven policy learning process and invite human interpreters
    to annotate real-world speech, which includes the read-write timing for segmentation.
    From the data, CLASI learns the robust read-write policy for SiST from humans.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，我们介绍了我们的端到端方法CLASI，这是一种跨语言智能体，通过迭代执行多个动作来完成同声传译，如图[2](https://arxiv.org/html/2407.21646v2#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent")所示。针对第一个挑战，我们模仿专业的人类翻译员，学习他们将完整句子通过语法边界（停顿、逗号、连词等）和上下文意义分割成多个语义“块”的策略。为了使CLASI学习到这种策略，我们遵循数据驱动的策略学习过程，并邀请人类翻译员标注真实世界的语音数据，包括分段的读写时机。从数据中，CLASI向人类学习到适用于SiST的稳健读写策略。
- en: 'For the second challenge, we include two external modules to augment our CLASI
    agent: an external knowledge database that stores terminologies and paired translations,
    and a memory that stores the context of speech. However, the external knowledge
    database may contain tremendous terms that not only increase the inference time
    but may also lower the performance of our approach because of noisy intervention.
    Therefore, we propose a novel Multi-Modal Retrieval Augmented Generation (MM-RAG)
    process. A multi-modal retriever extracts knowledge from the external database
    based on the speech input. The retrieved information and the context from memory
    are then appended to the prompt of our LLM agent to augment the translation through
    in-context learning.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个挑战，我们引入了两个外部模块来增强我们的CLASI智能体：一个存储术语和配对翻译的外部知识数据库，以及一个存储语音上下文的记忆模块。然而，外部知识数据库可能包含大量术语，不仅会增加推理时间，还可能由于噪声干扰而降低我们方法的性能。因此，我们提出了一种新的多模态检索增强生成（MM-RAG）过程。一个多模态检索器根据语音输入从外部数据库中提取知识。然后，检索到的信息和来自记忆的上下文被附加到我们的LLM智能体的提示中，通过上下文学习来增强翻译。
- en: 'Addressing the data scarcity of the SiST task, we adopt a three-stage training
    methodology: pretraining, continual training, and fine-tuning. First, our LLM
    and audio encoder are independently pretrained on our large-size in-house datasets.
    Then, our model is continually trained with billions of tokens of mediocre-quality
    synthesized speech translation data, aiming to align the speech and text modalities.
    We also include multiple tasks to enhance the in-context learning ability of LLM
    to better utilize the contextual information from the retriever and prior translation.
    In the last stage, we fine-tune the model with a small amount of human-annotated
    data, further imitating professional human interpreters to improve the robustness
    and translation quality.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决SiST任务的数据稀缺问题，我们采用了三阶段训练方法：预训练、持续训练和微调。首先，我们的LLM和音频编码器在我们的大型内部数据集上独立进行预训练。然后，我们的模型通过数十亿个普通质量的合成语音翻译数据进行持续训练，旨在对齐语音和文本模态。我们还包括多个任务，以增强LLM的上下文学习能力，更好地利用从检索器和先前翻译中获得的上下文信息。在最后阶段，我们用少量人工标注的数据对模型进行微调，进一步模拟专业人类翻译员，提高模型的鲁棒性和翻译质量。
- en: '![Refer to caption](img/7db5a78a0b4af1adcf4285a6a72061f2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7db5a78a0b4af1adcf4285a6a72061f2.png)'
- en: 'Figure 2: Overall framework of CLASI. The process begins in Step 1, where CLASI
    processes the incoming audio data. Optionally, the retriever is activated to obtain
    the relevant information from the external knowledge database. For instance, translating
    “伊辛模型” to “Ising model” for accurate speech translation. Step 3 involves accessing
    transcription (optional) and translation in the last round memory. Steps 4 and
    5 entail using the Chain-of-Thought (CoT) method to generate both the transcription (optional)
    and translation, followed by a memory update. The cycle then repeats from Step
    1 for the subsequent speech segment.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：CLASI的整体框架。流程从步骤1开始，其中CLASI处理传入的音频数据。可选地，激活检索器从外部知识数据库中获取相关信息。例如，翻译“伊辛模型”为“Ising
    model”，以便准确进行语音翻译。步骤3涉及访问转录（可选）和最后一轮记忆中的翻译。步骤4和5使用链式思维（CoT）方法生成转录（可选）和翻译，随后更新记忆。然后，循环从步骤1开始，处理下一个语音段。
- en: 'In addition, we would like to highlight that the conventional automatic evaluation
    metrics [papi2022over](https://arxiv.org/html/2407.21646v2#bib.bib52) ; [papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)
    ; [rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61) ; [sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68)
    of simultaneous interpretation might not be good indicators for reflecting the
    performance of SiST, which often contains compaction, abstraction, and paraphrasing.
    Aligned with human interpreters [moores2024nerle](https://arxiv.org/html/2407.21646v2#bib.bib49)
    ; [wu2010assessing](https://arxiv.org/html/2407.21646v2#bib.bib81) , we propose
    a new evaluation metric named Valid Information Proportion (VIP)²²2Detailed guidelines
    of our proposed VIP metric can be found in Appendix [A](https://arxiv.org/html/2407.21646v2#A1
    "Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements ‣ Social
    Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related
    Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary
    Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards
    Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent")..
    VIP represents the percentage of information that can be precisely delivered,
    reflecting the central objective of SiST: communication in real-time. Through
    thorough human evaluation on diverse and challenging real-world long speech datasets,
    our approach outperforms other currently accessible systems by a large margin.
    As shown in Figure [1](https://arxiv.org/html/2407.21646v2#S0.F1 "Figure 1 ‣ Towards
    Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"),
    taking the Chinese-to-English direction as an example, CLASI achieves a VIP score
    of 81.3%, significantly narrowing the gap between machine-assisted systems and
    human interpreters.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还想强调，传统的自动评估指标[papi2022over](https://arxiv.org/html/2407.21646v2#bib.bib52)；[papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)；[rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61)；[sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68)可能不是反映SiST性能的良好指标，因为SiST通常包含压缩、抽象和释义。与人类翻译员一致，[moores2024nerle](https://arxiv.org/html/2407.21646v2#bib.bib49)；[wu2010assessing](https://arxiv.org/html/2407.21646v2#bib.bib81)，我们提出了一种新的评估指标，名为有效信息比例（VIP）²²2我们提出的VIP指标的详细指南可以在附录[A](https://arxiv.org/html/2407.21646v2#A1
    "附录A 人工评估指南 ‣ 作者与致谢 ‣ 社会影响 ‣ 限制与未来工作 ‣ 6 结论 ‣ 人工评估 ‣ 5 相关工作 ‣ 4.7 案例研究 ‣ 4.6.2
    ICL性能 ‣ 4.6 MM-RAG性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 迈向通过LLM代理实现与人类平等的端到端同声翻译")中找到。VIP表示能够精确传递的信息的百分比，反映了SiST的核心目标：实时沟通。通过对多样且具有挑战性的真实世界长语音数据集的详细人工评估，我们的方法在很大程度上超越了其他当前可用的系统。如图[1](https://arxiv.org/html/2407.21646v2#S0.F1
    "图1 ‣ 迈向通过LLM代理实现与人类平等的端到端同声翻译")所示，以中译英方向为例，CLASI达到了81.3%的VIP分数，显著缩小了机器辅助系统与人工翻译员之间的差距。
- en: 'Our contributions can be summarized as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献可以总结如下：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce our end-to-end approach, CLASI, an LLM agent that is designed to
    perform high-quality and human-like simultaneous translation. Through human evaluation,
    our approach demonstrates significantly better performance compared to existing
    accessible SiST systems.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了我们的端到端方法CLASI，一个旨在执行高质量、类人化同声翻译的LLM代理。通过人工评估，我们的方法展示了显著优于现有可用的SiST系统的表现。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a new data-driven read-write strategy by imitating professional human
    interpreters. Without the requirement of complicated human pre-design, the strategy
    could balance translation quality and latency effortlessly. Unlike most commercial
    systems where the outputs are frequently rewritten during the translation process
    for better quality, our strategy guarantees all the outputs are deterministic
    while maintaining high quality.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种通过模仿专业人工翻译员的数据驱动的读写策略。在无需复杂的人类预设计的情况下，该策略能够轻松平衡翻译质量和延迟。与大多数商业系统中在翻译过程中为了提高质量而频繁重写输出不同，我们的策略保证了所有输出是确定性的，同时保持高质量。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Motivated by the preparatory trajectory of human interpreters, we introduce
    a novel Multi-Modal Retrieval Augmented Generation (MM-RAG) process that empowers
    the LLM with domain-specific knowledge in real time. The proposed module further
    improves the translation quality with minimal computational overhead during inference.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 受人类口译员准备过程的启发，我们引入了一种新的多模态检索增强生成（MM-RAG）过程，使大语言模型（LLM）能够实时获取领域特定知识。所提出的模块在推理过程中进一步提高了翻译质量，同时计算开销最小。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We work closely with professional human interpreters to develop our evaluation
    strategy, Valid Information Proportion (VIP), and detailed guidelines are open-sourced.
    Meanwhile, we release a human-annotated test set focusing on diverse real-world
    scenarios and long speech translations.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们与专业的人工口译员密切合作，制定了我们的评估策略——有效信息比例（VIP），并开源了详细的指南。同时，我们发布了一个人工标注的测试集，聚焦于各种现实场景和长时间语音翻译。
- en: 2 Methods
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: '![Refer to caption](img/0e4cf09e732f277fb38e1d14838db4c0.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/0e4cf09e732f277fb38e1d14838db4c0.png)'
- en: 'Figure 3: Architecture of CLASI agent. At round $r$, our model processes the
    current input audio stream alongside the memory from the previous round ($r-1$),
    and any retrieved knowledge. CLASI generates a response based on specified instructions
    and concurrently updates its memory. Additionally, the model determines the cut-off
    timestamp of the last semantic chunk. For instance, in the provided example, the
    phrase preceding “就在” is identified as a complete semantic chunk, with the cut-off
    timestamp positioned right after this phrase.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：CLASI代理架构。在第$r$轮，我们的模型处理当前输入的音频流，同时结合上轮（$r-1$）的记忆和任何检索到的知识。CLASI根据指定的指令生成响应，并同时更新其记忆。此外，模型确定上一个语义块的截断时间戳。例如，在所提供的示例中，“就在”之前的短语被识别为一个完整的语义块，截断时间戳就位于这个短语之后。
- en: 2.1 Framework
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 框架
- en: '[Figure 2](https://arxiv.org/html/2407.21646v2#S1.F2 "In 1 Introduction ‣ Towards
    Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent")
    presents a flow of operation of CLASI. To perform the SiST task, we design 5 operations:
    <INPUT>, <OUTPUT>, <RETRIEVE>, <LOAD_MEM>, and <UPDATE_MEM>. The following sections
    describe the details of each operation. As further illustrated in [Figure 3](https://arxiv.org/html/2407.21646v2#S2.F3
    "In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech
    Translation via LLM Agent"), CLASI is an LLM agent that can take input speech,
    instruction, relevant information retrieved from external knowledge, and last
    round memory as context. The memory stores previous transcriptions (optional)
    and translations. At round $r$, it first reads speech ${\mathbf{x}}_{t^{r-1}:T^{r}}$,
    where $t^{r-1}$ is the predicted cut-off time of round $r-1$ and $T^{r}$ is the
    end time for audio stream at round $r$. Then the agent retrieves relevant information
    ${\mathbf{k}}_{r}$ from the external knowledge and loads context ${\mathbf{y}}_{1:r-1}$
    from the last round memory. Once CLASI “think” sufficient context is loaded, it
    generates the transcription (optional), translation, and cut-off timestamp $t^{r}$:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2](https://arxiv.org/html/2407.21646v2#S1.F2 "在1 引言 ‣ 实现基于LLM代理的端到端同声传译与人类平等")展示了CLASI的操作流程。为了执行SiST任务，我们设计了5个操作：<INPUT>、<OUTPUT>、<RETRIEVE>、<LOAD_MEM>和<UPDATE_MEM>。以下部分将详细描述每个操作的细节。如[图3](https://arxiv.org/html/2407.21646v2#S2.F3
    "在2 方法 ‣ 实现基于LLM代理的端到端同声传译与人类平等")进一步说明，CLASI是一个LLM代理，可以将输入的语音、指令、从外部知识中检索到的相关信息以及上轮记忆作为上下文。记忆存储先前的转录（可选）和翻译。在第$r$轮，它首先读取语音${\mathbf{x}}_{t^{r-1}:T^{r}}$，其中$t^{r-1}$是第$r-1$轮的预测截断时间，$T^{r}$是第$r$轮音频流的结束时间。然后，代理从外部知识中检索相关信息${\mathbf{k}}_{r}$，并从上轮记忆中加载上下文${\mathbf{y}}_{1:r-1}$。一旦CLASI认为足够的上下文已被加载，它就生成转录（可选）、翻译和截断时间戳$t^{r}$：'
- en: '|  | ${\mathbf{y}}_{r};t^{r}=\text{TextDecoder}({\mathbf{x}}_{t^{r-1}:T^{r}},{%
    \mathbf{k}}_{r},{\mathbf{y}}_{1:r-1})\;$ |  | (1) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\mathbf{y}}_{r};t^{r}=\text{TextDecoder}({\mathbf{x}}_{t^{r-1}:T^{r}},{\mathbf{k}}_{r},{\mathbf{y}}_{1:r-1})\;$
    |  | (1) |'
- en: where $t^{r}$ is the predicted cut-off timestamp indicating the end time for
    the current translation round $r$. ${\mathbf{y}}_{r}$ is then forwarded to update
    the memory. When instructed to output the transcription, the LLM optionally engages
    CoT to generate transcription first and then the speech translation. For the following
    round $r+1$, the audio stream begins with the predicted cut-off timestamp $t^{r}$.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$t^{r}$是预测的截止时间戳，表示当前翻译轮次$r$的结束时间。然后，${\mathbf{y}}_{r}$被转发以更新记忆。当指示输出转录时，LLM可选择启用CoT（链式思维）首先生成转录，然后进行语音翻译。在接下来的轮次$r+1$中，音频流将从预测的截止时间戳$t^{r}$开始。
- en: 2.2 Architecture
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 架构（Architecture）
- en: CLASI employs an Encoder-Conditioned LLM architecture. As shown in [Figure 3](https://arxiv.org/html/2407.21646v2#S2.F3
    "In 2 Methods ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech
    Translation via LLM Agent"), the audio encoder transforms input speech stream
    ${\mathbf{x}}$ to a series of continuous representations ${\mathbf{s}}$. Then,
    the LLM takes the speech representation ${\mathbf{s}}$, retrieved knowledge ${\mathbf{k}}$,
    historical translation ${\mathbf{y}}$ and instruction ${\mathbf{I}}$ as a sequence
    of prompt $({\mathbf{y}},{\mathbf{s}},{\mathbf{k}},{\mathbf{I}})$ to generate
    the translation result ${\mathbf{y}}$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CLASI采用了编码器条件LLM架构。如[图3](https://arxiv.org/html/2407.21646v2#S2.F3 "在2种方法中 ‣
    通过LLM代理实现端到端同步语音翻译的人工水平")所示，音频编码器将输入的语音流${\mathbf{x}}$转换为一系列连续的表示${\mathbf{s}}$。然后，LLM将语音表示${\mathbf{s}}$、检索到的知识${\mathbf{k}}$、历史翻译${\mathbf{y}}$和指令${\mathbf{I}}$作为一系列提示$({\mathbf{y}},{\mathbf{s}},{\mathbf{k}},{\mathbf{I}})$，生成翻译结果${\mathbf{y}}$。
- en: Audio Encoder. The audio encoder module contains a large-scale speech conformer
    ([gulati2020conformer,](https://arxiv.org/html/2407.21646v2#bib.bib25) ) pretrained
    on millions of hours of speech data to achieve human parity performance on ASR,
    and an audio adapter to connect the audio encoder and LLM. The adapter downsamples
    the speech representations and the resulting representations are linearly projected
    to match the dimension of the LLM embedding layer. The projected speech representations
    lower the computational latency for SiST.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 音频编码器（Audio Encoder）。音频编码器模块包含一个大规模的语音Conformer（[gulati2020conformer,](https://arxiv.org/html/2407.21646v2#bib.bib25)），该模块在数百万小时的语音数据上进行了预训练，旨在在自动语音识别（ASR）中实现与人类相当的表现，并配有音频适配器，用于连接音频编码器和LLM。适配器将语音表示下采样，生成的表示会通过线性投影匹配LLM嵌入层的维度。投影后的语音表示降低了SiST的计算延迟。
- en: Large Language Model. The language model³³3We use Doubao LLM as our foundation
    model. is a medium size decoder-only transformer [vaswani2017attention](https://arxiv.org/html/2407.21646v2#bib.bib73)
    to balance performance and computation efficiency. It is pretrained on a large
    amount of text data and fine-tuned with instructions. The LLM directly takes the
    continuous embedding from both the audio encoder and text embedder as input. It
    autoregressively generates the transcription and translation response of the provided
    speech stream.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（Large Language Model）。我们使用Doubao LLM作为基础模型，它是一个中等规模的仅解码器变换器[vaswani2017attention](https://arxiv.org/html/2407.21646v2#bib.bib73)，旨在平衡性能和计算效率。该模型在大量文本数据上进行了预训练，并通过指令进行了微调。LLM直接将音频编码器和文本嵌入器的连续嵌入作为输入，自动回归地生成提供的语音流的转录和翻译响应。
- en: Multi-Modal Retriever. The multi-modal retriever framework employs audio and
    text encoders to independently encode the audio stream and text key of the terminologies
    in the external knowledge database. To enhance the alignment between audio embeddings
    and text embeddings, we incorporate an embedding fusion layer, which includes
    a multi-head attention module followed by a pooling layer. The resulting pooled
    representation is subsequently fed into a linear projection layer to produce the
    final scores, indicating the probability of the text key’s presence in the audio
    stream. Terminologies with top scores are forwarded to the CLASI agent to enhance
    the translation quality.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态检索器（Multi-Modal Retriever）。多模态检索器框架使用音频和文本编码器分别对外部知识库中的音频流和术语的文本键进行编码。为了增强音频嵌入与文本嵌入之间的对齐，我们引入了一个嵌入融合层，其中包括一个多头注意力模块，后面跟着一个池化层。池化后的表示随后被输入到线性投影层中，以生成最终的得分，表示文本键在音频流中出现的概率。得分最高的术语被转发到CLASI代理，以提高翻译质量。
- en: '2.3 Data Driven Read-Write Policy: <INPUT> and <OUTPUT>'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据驱动的读写策略：<INPUT> 和 <OUTPUT>
- en: Unlike predetermined read-write probabilities and heuristic waiting policies
    detailed in prior research [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    ; [ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45) ; [li-etal-2023-hw-tsc](https://arxiv.org/html/2407.21646v2#bib.bib39)
    , interpreters engage in a dynamic process of listening (read) and translating
    (write). They attentively listen to the speaker’s speech and segment lengthy sentences
    into semantic chunks, representing the smallest linguistic units capable of conveying
    a complete thought independently [jones2014conference](https://arxiv.org/html/2407.21646v2#bib.bib33)
    . Upon identifying a chunk that encapsulates sufficient information, they proceed
    to translate this segment into the target language, thereby providing an accurate
    and contextually appropriate translation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与先前研究中详细描述的预定读写概率和启发式等待策略不同[barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)；[ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45)；[li-etal-2023-hw-tsc](https://arxiv.org/html/2407.21646v2#bib.bib39)，翻译员参与的是一个动态的听（读）与翻译（写）过程。他们专心听取说话者的语音，并将冗长的句子分割成语义块，语义块是能够独立传达完整思想的最小语言单位[jones2014conference](https://arxiv.org/html/2407.21646v2#bib.bib33)。当识别出一个包含足够信息的语义块时，他们会将该段翻译成目标语言，从而提供准确且语境适宜的翻译。
- en: Emulating the strategies of human interpreters, CLASI does not require to explicitly
    define the read-write policy. CLASI imitates their policies by waiting for complete
    semantic chunks. Specifically, given partial speech, CLASI only generates the
    translation for the complete chunks of the input speech. The model is trained
    with segmented speech data to learn such ability. Mathematically, given source
    audio ${\mathbf{x}}_{1:M}$, we segment its translation into a series of $n$ “chunks”
    ${\mathbf{y}}_{1:n}$ and obtain the corresponding pair $\{({\mathbf{x}}_{t^{j}:t^{j+1}},{\mathbf{y}}_{j}\}_{j=1}^{n}$,
    where ${\mathbf{x}}_{t^{j}:t^{j+1}}$ and ${\mathbf{y}}_{j}$ denote the $j$-th
    segment of the audio and the corresponding translation. For training, our objective
    is to output all complete segmented translations and the cut-off time given random
    partial input audio ${\mathbf{x}}_{1:t}$
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿人类翻译员的策略，CLASI不需要明确地定义读写策略。CLASI通过等待完整的语义块来模仿这些策略。具体来说，给定部分语音，CLASI仅对完整的语音块生成翻译。该模型通过使用分段语音数据进行训练，学习这种能力。从数学角度来看，给定源音频${\mathbf{x}}_{1:M}$，我们将其翻译分割成一系列$n$个“块”${\mathbf{y}}_{1:n}$，并获得相应的配对$\{({\mathbf{x}}_{t^{j}:t^{j+1}},{\mathbf{y}}_{j})\}_{j=1}^{n}$，其中${\mathbf{x}}_{t^{j}:t^{j+1}}$和${\mathbf{y}}_{j}$分别表示第$j$个音频段和相应的翻译。对于训练，我们的目标是输出所有完整的分段翻译以及给定随机部分输入音频${\mathbf{x}}_{1:t}$的截止时间。
- en: '|  | $\min\mathbb{E}_{t\sim\mathcal{U}[1,M]}-\log p_{\theta}({\mathbf{y}}_{1:j};t^{j%
    }&#124;{\mathbf{x}}_{1:t})\quad j=\max_{j}\{j&#124;t^{j}<t\}\;$ |  | (2) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min\mathbb{E}_{t\sim\mathcal{U}[1,M]}-\log p_{\theta}({\mathbf{y}}_{1:j};t^{j%
    }&#124;{\mathbf{x}}_{1:t})\quad j=\max_{j}\{j&#124;t^{j}<t\}\;$ |  | (2) |'
- en: 'where $\mathcal{U}$ indicates uniform distribution over time of speech. Trained
    with [Equation 2](https://arxiv.org/html/2407.21646v2#S2.E2 "In 2.3 Data Driven
    Read-Write Policy: <INPUT> and <OUTPUT> ‣ 2 Methods ‣ Towards Achieving Human
    Parity on End-to-end Simultaneous Speech Translation via LLM Agent"), CLASI learns
    to generate the cut-off time for the input speech. Additionally, the objective
    function makes the CLASI wait for appropriate time before starting translation
    as the LLM will output nothing when it “think” current speech stream does not
    contain a complete speech chunk.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{U}$表示语音在时间上的均匀分布。通过训练[方程 2](https://arxiv.org/html/2407.21646v2#S2.E2
    "在 2.3 数据驱动的读写策略：<INPUT> 和 <OUTPUT> ‣ 2 方法 ‣ 实现端到端同声传译人类平行性的途径")，CLASI学习生成输入语音的截止时间。此外，目标函数使得CLASI在开始翻译之前等待适当的时间，因为当LLM认为当前语音流不包含完整的语音块时，它将不会输出任何内容。
- en: '2.4 Context Information: <LOAD_MEM> and <UPDATE_MEM>'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 上下文信息：<LOAD_MEM> 和 <UPDATE_MEM>
- en: The memory stores translations and transcriptions in previous rounds ${\mathbf{y}}_{1:r-1}$.
    It has two functions. Firstly, it works with the input speech to determine which
    part of the speech has been translated and which part has not, helping CLASI make
    the read-write decisions and outputs the translation of the unfinished parts.
    Secondly, understanding human speech often requires context. For example, when
    a speaker talks about “barrel bridge”, it often refers to the bridges built upon
    rivers that are supported by barrels. However, in the context of “watch”, it refers
    to a mechanical structure in the watch. The phenomenon of polysemy in different
    contexts can lead to vastly different translation outcomes. Therefore, CLASI should
    be able to retrieve the context of the long speech for translating some keywords,
    and make appropriate translations under different contexts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆存储着前几轮的翻译和转录内容${\mathbf{y}}_{1:r-1}$。它有两个功能。首先，它与输入的语音一起工作，确定语音的哪些部分已经翻译，哪些部分尚未翻译，从而帮助CLASI做出读写决策并输出未翻译部分的翻译。其次，理解人类的语音往往需要上下文。例如，当说到“桶桥”时，通常指的是建在河上的由桶支撑的桥梁。然而，在“手表”这一上下文中，它指的是手表中的机械结构。在不同上下文中同一词汇的多义现象可能导致截然不同的翻译结果。因此，CLASI应能够检索长语音的上下文，以便翻译某些关键词，并在不同的上下文中作出适当的翻译。
- en: As shown in [Figure 3](https://arxiv.org/html/2407.21646v2#S2.F3 "In 2 Methods
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent"), at round $r$, <LOAD_MEM> forwards relevant translations ${\mathbf{y}}_{1:r-1}$
    to the LLM as a prompt. After CLASI agent generates the translation ${\mathbf{y}}_{r}$,
    <UPDATE_MEM> stores it to the memory and obtains ${\mathbf{y}}_{1:r}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图 3](https://arxiv.org/html/2407.21646v2#S2.F3 "在2种方法 ‣ 通过LLM代理实现端到端同步语音翻译的人工智能平行性")所示，在轮次$r$时，<LOAD_MEM>将相关的翻译${\mathbf{y}}_{1:r-1}$作为提示信息传递给LLM。CLASI代理生成翻译${\mathbf{y}}_{r}$后，<UPDATE_MEM>将其存储到记忆中，并获取${\mathbf{y}}_{1:r}$。
- en: '2.5 Multi-Modal Retrieval Augmented Generation: <RETRIEVE>'
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 多模态检索增强生成：<RETRIEVE>
- en: In real-world scenarios, the accurate speech transcription or translation of
    professional and domain-specific terminologies is challenging. Even human interpreters
    require prior domain knowledge to understand those terminologies, including names
    of people, locations, jargon, or special in-domain terms. For example, an interpreter
    unfamiliar with the machine learning theory may not recognize the word “Rademacher
    complexity” when hearing it. Therefore, in various scenarios, human interpreters
    often prepare in advance to get familiar with the corresponding domain knowledge.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，专业领域的术语翻译或语音转录是具有挑战性的。即使是人类翻译员，也需要具备相关领域的知识，才能理解这些术语，包括人名、地名、行话或领域内的特殊术语。例如，若翻译员不熟悉机器学习理论，可能听到“Rademacher复杂度”时无法理解其含义。因此，在各种场景中，人类翻译员通常会提前准备，熟悉相关领域的知识。
- en: Motivated by the preparatory trajectory of human interpreters, we propose to
    integrate an external database to empower LLM with necessary domain-specific knowledge.
    Each item in the database contains a key and the corresponding value in text modality.
    The key, which may appear in the speech, is used as the input for the retriever.
    The value of the item may be itself, a paired translation of the target language,
    or even an explanation of the key.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 受人类翻译员准备工作的启发，我们提出集成外部数据库，以赋予LLM必要的领域特定知识。数据库中的每一项包含一个键及其对应的文本值。键可能出现在语音中，用作检索器的输入。该项的值可以是该项本身、目标语言的配对翻译，甚至是该键的解释。
- en: Theoretically, all items in the external database might be added into the prompt
    to provide information for the translation. However, the external knowledge database
    often contains tremendous items. Simply prompting LLM with all the terms not only
    increases the inference time but may also hurt the performance of CLASI because
    of noisy intervention. Therefore, we design a novel Multi-Modal Retrieval Augmented
    Generation (MM-RAG) process. Our multi-modal retriever first retrieves the relevant
    terminologies from the database based on the input speech. A small number of filtered
    items are incorporated into the prompt of CLASI agent for in-context learning
    as shown in [Figure 3](https://arxiv.org/html/2407.21646v2#S2.F3 "In 2 Methods
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent").
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从理论上讲，外部数据库中的所有项都可以加入到提示中，以提供翻译所需的信息。然而，外部知识库通常包含大量条目。简单地将所有术语都传递给大语言模型（LLM）不仅增加了推理时间，还可能由于噪声干扰而影响CLASI的性能。因此，我们设计了一种新颖的多模态检索增强生成（MM-RAG）过程。我们的多模态检索器首先根据输入的语音从数据库中检索相关术语。少量经过筛选的条目被纳入到CLASI代理的提示中，以便进行上下文学习，如[图
    3](https://arxiv.org/html/2407.21646v2#S2.F3 "在方法2 ‣ 通过LLM代理实现端到端实时语音翻译的人工智能平行")所示。
- en: With the retrieved knowledge and previous context from the memory, our LLM has
    the in-context learning ability to better utilize the provided contextual information.
    To achieve this, we collect a series of in-context learning data to train the
    model. Compared with the previous approaches for intervention, such as shallow-fusion
    [Borgeaud2021ImprovingLM](https://arxiv.org/html/2407.21646v2#bib.bib8) ; [Kolehmainen2024MultiModalRF](https://arxiv.org/html/2407.21646v2#bib.bib34)
    and traditional substitution-based methods [li2019neural](https://arxiv.org/html/2407.21646v2#bib.bib38)
    ; [luong2015addressing](https://arxiv.org/html/2407.21646v2#bib.bib44) ; [wang2017sogou](https://arxiv.org/html/2407.21646v2#bib.bib78)
    , which generates fixed translation for given translation pair. Our method achieves
    better results and generates more coherent text. For example, in some internet
    companies, “大盘 == overall performance”, while in most cases, it should be “stock
    market”. Our method can choose the correct translation given different context.
    Besides, our method can use monolingual text from both source and target language
    to help the translation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索到的知识和来自记忆的先前上下文的帮助下，我们的LLM具备了更好地利用提供的上下文信息的能力。为了实现这一点，我们收集了一系列的上下文学习数据来训练模型。与之前的干预方法相比，如浅融合[Borgeaud2021ImprovingLM](https://arxiv.org/html/2407.21646v2#bib.bib8)；[Kolehmainen2024MultiModalRF](https://arxiv.org/html/2407.21646v2#bib.bib34)和传统的基于替代的方法[li2019neural](https://arxiv.org/html/2407.21646v2#bib.bib38)；[luong2015addressing](https://arxiv.org/html/2407.21646v2#bib.bib44)；[wang2017sogou](https://arxiv.org/html/2407.21646v2#bib.bib78)，这些方法为给定的翻译对生成固定翻译。我们的方法取得了更好的结果，并生成了更加连贯的文本。例如，在一些互联网公司中，“大盘
    == overall performance”，但在大多数情况下，它应该是“股市”。我们的方法可以根据不同的上下文选择正确的翻译。此外，我们的方法还可以使用源语言和目标语言的单语文本来辅助翻译。
- en: 3 Multi-Stage Training
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多阶段训练
- en: 'Our CLASI follows a multi-stage training process: pretraining, multi-task continual
    training, and multi-task supervised fine-tuning. In the first stage, the LLM and
    audio encoder are separately pretrained with massive amounts of in-house speech
    and text data. Next, a large amount of speech-text paired data is used to align
    audio and text modalities, building the fundamental capability for cross-modal
    multitasking. In the final stage, CLASI agent is fine-tuned with a small amount
    of human-annotated data to imitate the translation behavior of professional human
    interpreters. Our multi-stage training process enables high efficiency of learning
    with a small amount of human-labeled data.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的CLASI遵循一个多阶段的训练过程：预训练、多任务持续训练和多任务监督微调。在第一阶段，LLM和音频编码器分别通过大量的内部语音和文本数据进行预训练。接下来，使用大量的语音-文本配对数据来对齐音频和文本模态，构建跨模态多任务处理的基本能力。在最后阶段，CLASI代理通过少量人工标注的数据进行微调，以模仿专业人类译员的翻译行为。我们的多阶段训练过程使得在少量人工标注数据的情况下，高效地进行学习成为可能。
- en: 3.1 Pretraining
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 预训练
- en: The in-house LLM and audio encoder are independently pretrained on different
    modalities of data. The LLM follows a decoder-only transformer architecture and
    is first pretrained on a massive amount of monolingual and bilingual text data
    with cross-entropy loss and then fine-tuned on instruction-following data. The
    LLM performs excellently on various downstream tasks, especially translation tasks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 内部的LLM和音频编码器分别在不同的数据模态上进行独立预训练。LLM采用仅解码器的Transformer架构，首先在大量单语和双语文本数据上进行预训练，使用交叉熵损失，然后在指令跟随数据上进行微调。LLM在各种下游任务中表现出色，特别是在翻译任务上。
- en: The audio encoder also follows a classic pretrain-finetune paradigm (bai2024;
    [baevski2022data2vec,](https://arxiv.org/html/2407.21646v2#bib.bib5) ; [hsu2021hubert,](https://arxiv.org/html/2407.21646v2#bib.bib28)
    ; [zhang2023usm,](https://arxiv.org/html/2407.21646v2#bib.bib88) ) with a massive
    amount of speech-related data. The pretraining stage provides a proficiently trained
    LLM and audio encoder, setting a solid foundation for the following stages.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 音频编码器也遵循经典的预训练-微调范式（bai2024；[baevski2022data2vec,](https://arxiv.org/html/2407.21646v2#bib.bib5)；[hsu2021hubert,](https://arxiv.org/html/2407.21646v2#bib.bib28)；[zhang2023usm,](https://arxiv.org/html/2407.21646v2#bib.bib88)），并使用大量与语音相关的数据。预训练阶段提供了高效训练的LLM和音频编码器，为后续阶段奠定了坚实的基础。
- en: 3.2 Multi-task Continual Training
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多任务持续训练
- en: 'Training Method. We follow the work of ([huang2023speechtranslationlargelanguage,](https://arxiv.org/html/2407.21646v2#bib.bib30)
    ) for multi-task training. Specifically, for streaming and higher-quality translation,
    we mainly focus on three tasks for training CLASI: Automatic Speech Recognition
    (ASR), Speech Translation (ST), and Text Translation (MT). To align the modalities
    of the pretrained LLM and audio encoder, CLASI is continually trained on various
    tasks with a substantial volume of paired data. We further strengthen the in-context
    learning ability of our approach by incorporating translation in the memory and
    knowledge from external databases. As a result, we expand the ST tasks to different
    configurations as shown in [Table 1](https://arxiv.org/html/2407.21646v2#S3.T1
    "In 3.2 Multi-task Continual Training ‣ 3 Multi-Stage Training ‣ Towards Achieving
    Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"). An
    ST translation can either be streaming or offline, direct or COT, with or without
    context, which leads to 8 different tasks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 训练方法：我们参考了([huang2023speechtranslationlargelanguage,](https://arxiv.org/html/2407.21646v2#bib.bib30))的工作进行多任务训练。具体而言，为了实现流式处理和更高质量的翻译，我们主要专注于三个任务来训练CLASI：自动语音识别（ASR）、语音翻译（ST）和文本翻译（MT）。为了使预训练的LLM和音频编码器的模态对齐，CLASI在大量配对数据上进行持续训练，涵盖多个任务。我们进一步通过将翻译融入内存并结合外部数据库的知识，增强了方法的上下文学习能力。因此，我们将ST任务扩展到不同的配置，如[表
    1](https://arxiv.org/html/2407.21646v2#S3.T1 "在3.2 多任务持续训练 ‣ 3 多阶段训练 ‣ 通过LLM代理实现端到端同步语音翻译的人工平行").
    一个ST翻译任务可以是流式或离线的，直接或COT的，有或没有上下文，从而形成8种不同的任务。
- en: '| Configuration | Explanation |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 配置 | 说明 |'
- en: '| Direct | Speech is directly translated into the target language. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 直接 | 语音直接翻译为目标语言。 |'
- en: '| COT | Speech is first transcribed to the source language and then translated
    to the target language. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| COT | 语音首先转录为源语言，然后翻译为目标语言。 |'
- en: '| Streaming | Given partial speech, translate segments with complete semantics
    to the target language. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 流式 | 给定部分语音，翻译具有完整语义的片段为目标语言。 |'
- en: '| Offline | Given complete speech, translate the whole content to the target
    language. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 离线 | 给定完整的语音，将整个内容翻译为目标语言。 |'
- en: '| w/o Context | No historical translations and external knowledge are provided.
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 无上下文 | 不提供历史翻译和外部知识。 |'
- en: '| w/ Context | Historical translations or external knowledge are provided as
    context. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 带上下文 | 提供历史翻译或外部知识作为上下文。 |'
- en: 'Table 1: Illustration of different configurations of ST task in Multi-task
    Continual Training.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：多任务持续训练中ST任务的不同配置示意图。
- en: 'Training Data Construction. Both ASR and MT tasks have been last for a while,
    and there is a relatively large amount of ASR and MT data. The major challenge
    of developing an end-to-end SiST model is the data scarcity of simultaneous ST.
    To this end, we propose a synthetic data construction pipeline. With a strong
    LLM, we synthesize two types of speech translation data for continual training:
    offline ST data and context-aware segmented streaming ST data.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据构建。ASR和MT任务已经进行了一段时间，并且有相对大量的ASR和MT数据。开发端到端SiST模型的主要挑战是同步ST数据的稀缺性。为此，我们提出了一种合成数据构建流程。借助强大的LLM，我们合成了两种类型的语音翻译数据进行持续训练：离线ST数据和基于上下文的分段流式ST数据。
- en: '1\. Offline ST data: We mainly rely on ASR data to construct the offline ST
    data. Given the ground-truth transcription of speech, we use in-house LLM to translate
    the source language to target languages. To ensure the readability and conciseness
    of the target language, the LLM is prompted to conduct Inverse Text Normalization
    (ITN), filler word smoothing, etc.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 离线ST数据：我们主要依赖ASR数据构建离线ST数据。在提供语音的真实转录文本的基础上，我们使用内部LLM将源语言翻译为目标语言。为了确保目标语言的可读性和简洁性，LLM被提示执行逆文本规范化（ITN）、填充词平滑等操作。
- en: '2\. Context-aware segmented streaming ST data: The streaming ST data consists
    of fine-grained audio-text alignments and translation pairs for segmented semantic
    chunks. Compared to offline ST data, streaming ST data is even more challenging
    to collect. We find that human interpreter often segments long speech into a few
    semantic chunks, each of which can be translated independently to ensure an effective
    and smooth translation. Motivated by such findings, we leverage LLM to construct
    streaming ST data by imitating the chunking process. Long speech data are used
    to construct the streaming ST data, as the additional history can provide better
    contextual information. First, we prompt the LLM to break down the ASR transcription
    into multiple independent semantic chunks, which are then translated into the
    target language. Subsequently, we align the semantic chunks with the corresponding
    audio chunks, obtaining the streaming ST data. Such data enable our model to handle
    incomplete speech inputs and generate partial translation in coherent semantics.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 基于上下文的分段流式ST数据：流式ST数据由细粒度的音频-文本对齐和翻译对组成，针对语义块进行分段。与离线ST数据相比，流式ST数据的收集更具挑战性。我们发现，人工翻译员通常将长语音切分成若干语义块，每个语义块都可以独立翻译，以确保有效和平滑的翻译。受到这一发现的启发，我们利用LLM模拟分块过程来构建流式ST数据。我们使用长语音数据来构建流式ST数据，因为额外的上下文信息有助于提供更好的语境。首先，我们提示LLM将ASR转录文本分解为多个独立的语义块，然后将其翻译成目标语言。接着，我们将语义块与相应的音频块对齐，得到流式ST数据。这些数据使得我们的模型能够处理不完整的语音输入，并生成具有一致语义的部分翻译。
- en: To measure the quality of the synthetic data, we conduct human evaluations based
    on our proposed VIP metric. The synthetic data achieves a VIP score of 81%, satisfying
    the minimal requirement for further training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估合成数据的质量，我们基于我们提出的VIP指标进行了人工评估。合成数据的VIP得分为81%，满足了进一步训练的最小要求。
- en: 3.3 Multi-task Supervised Fine-tuning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 多任务监督微调
- en: Training Method. Even though CLASI possesses a good translation quality on the
    SiST tasks after the previous multi-task continual training stage, we further
    boost the performance by fine-tuning on human-annotated streaming ST data with
    diverse tasks listed in [Table 1](https://arxiv.org/html/2407.21646v2#S3.T1 "In
    3.2 Multi-task Continual Training ‣ 3 Multi-Stage Training ‣ Towards Achieving
    Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"). Such
    high-quality data enables our model to better align with the segmentation methodologies
    of professional human interpreters. Furthermore, this process enhances our model’s
    robustness to speech disfluencies such as stuttering, ensuring smoother communication
    in real-world scenarios.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练方法。尽管CLASI在经过之前的多任务持续训练阶段后，在SiST任务上表现出良好的翻译质量，我们通过在人工标注的流式ST数据上进行微调，进一步提升了性能，这些数据包含[表1](https://arxiv.org/html/2407.21646v2#S3.T1
    "在3.2多任务持续训练 ‣ 3多阶段训练 ‣ 通过LLM代理实现与人类水平的端到端同步语音翻译")中列出的多种任务。这些高质量数据使我们的模型能够更好地与专业人工翻译员的分段方法对齐。此外，这一过程增强了模型对语音不流利现象（如口吃）的鲁棒性，确保在真实场景中的沟通更加顺畅。
- en: Training Data Construction. The source of human-annotated streaming ST data
    originates from real-world scenarios that contain various speech characteristics,
    such as disfluencies, stuttering, code-mixing, and specialized terminologies.
    Such features ensure the robustness of our model in diverse conditions. We engage
    professional human interpreters to provide high-quality annotations for simultaneous
    segmentation and interpretation of the speech data. Additionally, terminologies
    are identified and translated within the context, further strengthening the context-aware
    capabilities of CLASI.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据构建。人工标注的流式ST数据来源于包含各种语音特征的真实场景，如不流利、结巴、混合代码和专业术语等。这些特征确保了我们的模型在多种条件下的鲁棒性。我们聘请了专业的人工口译员，为语音数据提供高质量的标注，以实现同步分割和口译。此外，术语在上下文中被识别并翻译，从而进一步增强了CLASI的上下文感知能力。
- en: 3.4 Multi-Modal Retriever Training
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 多模态检索器训练
- en: The multi-modal retriever is independently trained with a substantial dataset
    of speech recognition data. During training, words are randomly selected from
    speech transcription to serve as the positive sample, indicating their appearance
    in the speech. Negative words are selected from different sentences, indicating
    the speech does not mention these words. We assign a label of $1$ to positive
    samples and $0$ to negative samples, aiming to minimize the Binary Cross Entropy
    (BCE) loss. This approach helps refine the model’s ability to distinguish relevant
    from irrelevant information, enhancing its overall performance and accuracy. We
    label the positive sample as $1$ and the negative sample as 0, minimizing the
    Binary Cross Entropy (BCE) loss. To evaluate the effectiveness of our retriever,
    we build an in-house retrieve development set. Each sample in the development
    set includes a short audio chunk and the mentioned terms in the audio. Note that
    the term here is defined as special keywords, such as name, location, abbreviation,
    and domain-specific word.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态检索器通过大量的语音识别数据独立进行训练。在训练过程中，从语音转录中随机选择单词作为正样本，表示这些单词在语音中出现。负样本则从不同的句子中选取，表示语音中没有提到这些单词。我们将正样本标记为$1$，负样本标记为$0$，目的是最小化二元交叉熵（BCE）损失。该方法有助于提升模型区分相关信息和无关信息的能力，从而增强其整体性能和准确性。我们将正样本标记为$1$，负样本标记为$0$，以最小化二元交叉熵（BCE）损失。为了评估我们检索器的效果，我们构建了一个内部的检索开发集。开发集中的每个样本包括一个短的音频片段和音频中提到的术语。需要注意的是，这里的术语是指特定的关键词，如姓名、地点、缩写和领域特定的词汇。
- en: 4 Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Evaluation Benchmark
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 评估基准
- en: Quite a few number of evaluation benchmarks have been proposed for SiST over
    the past years, including MuST-C [di2019must](https://arxiv.org/html/2407.21646v2#bib.bib15)
    , FLUERS [conneau2023fleurs](https://arxiv.org/html/2407.21646v2#bib.bib14) ,
    CoVoST [wang-etal-2020-covost](https://arxiv.org/html/2407.21646v2#bib.bib74)
    ; [wang2020covost](https://arxiv.org/html/2407.21646v2#bib.bib75) , BSTC [zhang-etal-2021-bstc](https://arxiv.org/html/2407.21646v2#bib.bib85)
    , and GigaST [ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83)
    , etc. However, although much effort has been spent to build these benchmarks,
    they still suffer some shortcomings when facing real-world SiST applications.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年中，已经提出了相当多的评估基准用于SiST，包括MuST-C [di2019must](https://arxiv.org/html/2407.21646v2#bib.bib15)、FLUERS
    [conneau2023fleurs](https://arxiv.org/html/2407.21646v2#bib.bib14)、CoVoST [wang-etal-2020-covost](https://arxiv.org/html/2407.21646v2#bib.bib74)；[wang2020covost](https://arxiv.org/html/2407.21646v2#bib.bib75)、BSTC
    [zhang-etal-2021-bstc](https://arxiv.org/html/2407.21646v2#bib.bib85) 和GigaST
    [ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83) 等。然而，尽管在构建这些基准方面投入了大量的精力，但在面对现实世界的SiST应用时，它们仍然存在一些不足之处。
- en: First, these benchmarks often contain speeches that are either recorded by volunteers
    (e.g. CoVoST and FLUERS) or collected from formally, clearly, and fluently talk
    and podcasts by well-prepared speakers (e.g. MuST-C and GigaST). In real-world
    scenarios such as online meetings or social media videos, the characteristics
    of the speech might inevitably be informal, unclear, or disfluent. Second, these
    benchmarks provide a shortcut for evaluating the translation quality by giving
    the manually segmented sentences. Such a shortcut offers a gap between the current
    benchmark and real-world applications, where the models might need to take long
    speech and conduct segmentation [anastasopoulos2021findings](https://arxiv.org/html/2407.21646v2#bib.bib4)
    by themselves. Consequently, evaluations on manually segmented datasets are likely
    to overestimate the performances of a real-world SiST system. These discrepancies
    result in the evaluation on these benchmarks are not reliable for practical SiST
    systems.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这些基准通常包含由志愿者录制的演讲（例如CoVoST和FLUERS），或者由准备充分的演讲者正式、清晰且流利地讲述的演讲和播客（例如MuST-C和GigaST）。在现实世界的场景中，例如在线会议或社交媒体视频，演讲的特点可能不可避免地是非正式的、不清晰的或不流畅的。其次，这些基准通过提供手动分割的句子，为评估翻译质量提供了捷径。这样的捷径在当前的基准和实际应用之间存在差距，实际应用中，模型可能需要自行处理长时间的演讲并进行分割[anastasopoulos2021findings](https://arxiv.org/html/2407.21646v2#bib.bib4)。因此，在手动分割的数据集上的评估可能会高估实际SiST系统的表现。这些差异导致对这些基准的评估在实际SiST系统中并不可靠。
- en: 'As a preliminary attempt to address the shortcomings as mentioned earlier,
    we propose a new benchmark RealSI for Chinese-to-English (zh-en) and English-to-Chinese
    (en-zh). RealSI is collected from diverse sources, and most speakers talk naturally
    and casually without careful preparation. We choose 10 popular domains: technology,
    healthcare, education, finance, law, environment, entertainment, science, sports,
    and art. One video clip is selected for each domain from a well-known online video
    platform for both zh-en and en-zh settings.⁴⁴4RealSI is available at [https://github.com/byteresearchcla/RealSI](https://github.com/byteresearchcla/RealSI).
    We do not own the copyright of the videos and only release our annotations together
    with the publicly available website links of the corresponding videos. If anyone
    believes that the content constitutes infringement, please contact us. We will
    remove the relevant content as soon as it is confirmed. Each sample in RealSI
    is a nearly 5-minute speech to mock SiST without manual segmentation. For systems
    that cannot take long-form audio as input, we also provide sentence-level timestamps
    for segmentation. [Table 2](https://arxiv.org/html/2407.21646v2#S4.T2 "In 4.1
    Evaluation Benchmark ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end
    Simultaneous Speech Translation via LLM Agent") presents the detailed statistics
    of our RealSI.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种初步尝试，以解决前述的不足，我们提出了一个新的基准RealSI，适用于中文到英文（zh-en）和英文到中文（en-zh）。RealSI来源于多种渠道，大多数演讲者自然、随意地讲话，且没有经过精心准备。我们选择了10个热门领域：技术、医疗保健、教育、金融、法律、环境、娱乐、科学、体育和艺术。每个领域从一个知名的在线视频平台中选择一个视频片段，适用于zh-en和en-zh两种设置。⁴⁴RealSI可以在[https://github.com/byteresearchcla/RealSI](https://github.com/byteresearchcla/RealSI)上获取。我们不拥有视频的版权，仅发布我们对相应视频的注释以及视频的公开网址链接。如果有人认为内容构成侵权，请联系我们，我们会在确认后尽快删除相关内容。RealSI中的每个样本都是一个接近5分钟的演讲，用于模拟没有手动分割的SiST。对于无法处理长格式音频的系统，我们还提供了用于分割的句子级时间戳。[表2](https://arxiv.org/html/2407.21646v2#S4.T2
    "在4.1评估基准 ‣ 4实验 ‣ 通过LLM代理实现端到端同步语音翻译的人类平行性")展示了我们RealSI的详细统计数据。
- en: '| Domain | zh-en | en-zh |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | zh-en | en-zh |'
- en: '| --- | --- | --- |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Duration | #Segments | Duration | #Segments |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 时长 | #段落 | 时长 | #段落 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Technology | 5:23 | 51 | 3:25 | 31 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 技术 | 5:23 | 51 | 3:25 | 31 |'
- en: '| Healthcare | 3:16 | 30 | 3:34 | 22 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 医疗保健 | 3:16 | 30 | 3:34 | 22 |'
- en: '| Education | 4:56 | 48 | 5:00 | 41 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 教育 | 4:56 | 48 | 5:00 | 41 |'
- en: '| Finance | 5:22 | 29 | 5:01 | 40 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 金融 | 5:22 | 29 | 5:01 | 40 |'
- en: '| Law | 4:38 | 49 | 4:48 | 29 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 法律 | 4:38 | 49 | 4:48 | 29 |'
- en: '| Environment | 4:18 | 34 | 4:24 | 31 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 4:18 | 34 | 4:24 | 31 |'
- en: '| Entertainment | 5:16 | 53 | 5:12 | 39 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 娱乐 | 5:16 | 53 | 5:12 | 39 |'
- en: '| Science | 4:47 | 37 | 5:11 | 35 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 科学 | 4:47 | 37 | 5:11 | 35 |'
- en: '| Sports | 5:22 | 33 | 3:25 | 58 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 体育 | 5:22 | 33 | 3:25 | 58 |'
- en: '| Art | 7:54 | 67 | 4:17 | 21 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 艺术 | 7:54 | 67 | 4:17 | 21 |'
- en: '| Total | 51:12 | 431 | 44:17 | 347 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 51:12 | 431 | 44:17 | 347 |'
- en: 'Table 2: Statistics of our proposed RealSI benchmark.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：我们提出的RealSI基准的统计数据。
- en: 4.2 Baselines
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 基准测试
- en: We compare CLASI with the open-sourced SiST model, *SeamlessStreaming* [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    . In addition, because of the limited number of available SiST models, we choose
    to compare CLASI with several commercial systems. We denote the commercial systems
    as *Commercial* 1-4\. It is worth noting that unlike CLASI, most of the commercial
    SiST systems will first generate a temporary translation as soon as possible,
    then rewrite the temporary translation with a potentially better translation after
    getting more context. Notwithstanding, we evaluate the finalized translation of
    these systems in all our experiments. Although this re-writing strategy could
    improve translation quality, continually revising existing translations might
    affect the user experience, potentially leading to additional confusion. We would
    also like to highlight that human interpreters usually do not employ such a rewriting
    strategy during translation. During the entire evaluation, we employ a general
    external knowledge database that maintains the same for all the evaluation in
    this paper. It does not contain domain-specific external knowledge to form unfair
    comparisons. The improvement of external knowledge is independently reported in
    [Section 4.6](https://arxiv.org/html/2407.21646v2#S4.SS6 "4.6 MM-RAG Performance
    ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent").
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将CLASI与开源的SiST模型，*SeamlessStreaming* [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    进行了比较。此外，由于可用的SiST模型数量有限，我们选择将CLASI与几个商业系统进行比较。我们将这些商业系统表示为*Commercial* 1-4。值得注意的是，不同于CLASI，大多数商业SiST系统会尽快生成一个临时翻译，然后在获得更多上下文后，用潜在更好的翻译重写这个临时翻译。然而，我们在所有实验中评估了这些系统的最终翻译。尽管这种重写策略可能会提高翻译质量，但不断修改现有翻译可能会影响用户体验，导致额外的困惑。我们还想强调的是，人工翻译通常不会采用这种重写策略。在整个评估过程中，我们使用了一个通用的外部知识数据库，该数据库在本文中的所有评估中保持一致。它不包含特定领域的外部知识，以避免不公平的比较。外部知识的改进在[Section
    4.6](https://arxiv.org/html/2407.21646v2#S4.SS6 "4.6 MM-RAG Performance ‣ 4.5
    Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent")中独立报告。
- en: 4.3 Translation Quality
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 翻译质量
- en: '{NiceTabular}'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '{NiceTabular}'
- en: lccccccc[colortbl-like] \CodeBefore8 \Bodyzh-en BLEU BLEURT COMET VIP^† (%)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: lccccccc[colortbl-like] \CodeBefore8 \Bodyzh-en BLEU BLEURT COMET VIP^† (%)
- en: doc sent doc sent doc sent
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: doc sent doc sent doc sent
- en: '*SeamlessStreaming* 11.3 8.9 33.9 42.7 75.9 65.9 13.2'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*SeamlessStreaming* 11.3 8.9 33.9 42.7 75.9 65.9 13.2'
- en: '*Commercial 1* 15.0 10.8 30.6 43.9 72.4 67.1 10.4'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 1* 15.0 10.8 30.6 43.9 72.4 67.1 10.4'
- en: '*Commercial 2* 19.6 15.0 37.7 53.4 79.8 75.8 14.6'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 2* 19.6 15.0 37.7 53.4 79.8 75.8 14.6'
- en: '*Commercial 3* 24.5 19.9 40.2 56.4 81.8 78.9 25.0'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 3* 24.5 19.9 40.2 56.4 81.8 78.9 25.0'
- en: '*Commercial 4* 25.2 21.4 40.8 59.3 82.9 80.8 35.4'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 4* 25.2 21.4 40.8 59.3 82.9 80.8 35.4'
- en: '*CLASI* 32.6 28.1 44.4 65.9 84.6 84.7 81.3^∗ {NiceTabular}'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*CLASI* 32.6 28.1 44.4 65.9 84.6 84.7 81.3^∗ {NiceTabular}'
- en: lccccccc[colortbl-like] \CodeBefore8 \Bodyen-zh BLEU BLEURT COMET VIP^† (%)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: lccccccc[colortbl-like] \CodeBefore8 \Bodyen-zh BLEU BLEURT COMET VIP^† (%)
- en: doc sent doc sent doc sent
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: doc sent doc sent doc sent
- en: '*SeamlessStreaming* 14.8 10.4 22.1 27.8 64.6 60.6 2.0'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*SeamlessStreaming* 14.8 10.4 22.1 27.8 64.6 60.6 2.0'
- en: '*Commercial 1* 25.6 20.4 40.2 38.0 70.3 70.9 12.8'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 1* 25.6 20.4 40.2 38.0 70.3 70.9 12.8'
- en: '*Commercial 2* 29.6 26.0 50.5 46.8 78.2 75.8 16.8'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 2* 29.6 26.0 50.5 46.8 78.2 75.8 16.8'
- en: '*Commercial 3* 31.6 28.7 51.2 52.8 81.0 79.9 29.5'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 3* 31.6 28.7 51.2 52.8 81.0 79.9 29.5'
- en: '*Commercial 4* 29.8 26.4 47.0 48.0 77.5 76.7 41.6'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*Commercial 4* 29.8 26.4 47.0 48.0 77.5 76.7 41.6'
- en: '*CLASI* 37.4 32.8 54.2 61.3 87.4 85.6 78.0^∗'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*CLASI* 37.4 32.8 54.2 61.3 87.4 85.6 78.0^∗'
- en: 'Table 3: Experiment results of translation quality. Automatic evaluations were
    calculated on both document level and sentence level. In document level evaluations,
    each translation of a nearly 5-minute audio was considered one instance. For sentence
    level, automatic scores are calculated on human-segmented translations. VIP refers
    to the human-evaluated Valid Information Proportion that reflects the translation
    quality of these systems. ^† Due to the limitations in human evaluation capacity,
    the VIP scores are calculated on 4 randomly selected samples out of 10 in RealSI
    across all systems for fair comparison, while automatic metrics are evaluated
    on 10 samples. ^∗ Additionally, we evaluate the performance of CLASI on all 10
    samples, achieving VIP scores of 78.0% for zh-en and 74.9% for en-zh.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：翻译质量的实验结果。自动评估分别在文档级和句子级上进行了计算。在文档级评估中，每个近5分钟的音频翻译被视为一个实例。对于句子级，自动得分是基于人工分段的翻译计算的。VIP指的是人工评估的有效信息比例，反映了这些系统的翻译质量。^† 由于人工评估能力的限制，VIP得分是基于从RealSI的10个样本中随机选择的4个样本进行计算，以保证公平比较，而自动评估则在10个样本上进行。^∗ 此外，我们对CLASI在所有10个样本上的表现进行了评估，获得了78.0%的zh-en和74.9%的en-zh的VIP得分。
- en: Evaluation Metrics. Automatic evaluation metrics such as BLEU [papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)
    , BLEURT [sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68) , and
    COMET [rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61) are widely
    used for evaluating the translation quality [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    ; [iwslt-2023-international](https://arxiv.org/html/2407.21646v2#bib.bib65) ;
    [iwslt-2022-international](https://arxiv.org/html/2407.21646v2#bib.bib66) . However,
    they may not be able to fully reflect the quality of the translation, especially
    for paragraph-level translation of long speech. It is argued that the current
    evaluation metrics are not sufficient for ST and SiST tasks [gaido2024speech](https://arxiv.org/html/2407.21646v2#bib.bib23)
    . The work of [machavcek2023mt](https://arxiv.org/html/2407.21646v2#bib.bib47)
    ; [wein2024barriers](https://arxiv.org/html/2407.21646v2#bib.bib79) also highlighted
    that there might be a discrepancy between automatic evaluation metrics with human
    evaluation.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。自动评估指标如BLEU [papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)、BLEURT [sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68)和COMET [rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61)被广泛用于评估翻译质量 [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)；[iwslt-2023-international](https://arxiv.org/html/2407.21646v2#bib.bib65)；[iwslt-2022-international](https://arxiv.org/html/2407.21646v2#bib.bib66)
    。然而，它们可能无法完全反映翻译质量，特别是对于长篇语音的段落级翻译。有人认为，当前的评估指标对于语音翻译（ST）和语音翻译生成（SiST）任务来说并不充分 [gaido2024speech](https://arxiv.org/html/2407.21646v2#bib.bib23)。 [machavcek2023mt](https://arxiv.org/html/2407.21646v2#bib.bib47)；[wein2024barriers](https://arxiv.org/html/2407.21646v2#bib.bib79)的研究也强调，自动评估指标与人工评估之间可能存在差异。
- en: Therefore, besides the automatic evaluation, we collaborated with senior professional
    human simultaneous interpreters to standardize the guidelines for a more realistic
    human evaluation. Our proposed human evaluation metric focuses on whether the
    output of the translation model can accurately convey the speaker’s original intention
    for each semantic fragment. This is also the key objective of human interpreters
    in real-time translation. Note that a single semantic fragment indicates a complete
    piece of source speech. Typically, a single semantic fragment is a complete sentence.
    Detailed definition can be found in [Section A.2](https://arxiv.org/html/2407.21646v2#A1.SS2
    "A.2 Evaluation Process ‣ Appendix A Human Evaluation Guidelines ‣ Authorship
    and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion
    ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance
    ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation
    Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent"). The percentage of valid information fragments
    within a complete speech session is defined as VIP, which is consistent with real-world
    criteria for human simultaneous interpretation [wu2010assessing](https://arxiv.org/html/2407.21646v2#bib.bib81)
    .
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除了自动评估外，我们还与资深专业的同声传译员合作，制定了更为现实的人类评估标准。我们提出的人类评估指标着重于翻译模型的输出是否能准确传达讲话者每个语义片段的原意。这也是人类传译员在实时翻译中的关键目标。需要注意的是，单一的语义片段指的是一完整的源语言语音片段。通常，单一的语义片段是一个完整的句子。详细定义可以在[第A.2节](https://arxiv.org/html/2407.21646v2#A1.SS2
    "A.2 Evaluation Process ‣ Appendix A Human Evaluation Guidelines ‣ Authorship
    and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion
    ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance
    ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation
    Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent")中找到。一个完整语音会话中有效信息片段的百分比定义为VIP，这与现实世界中同声传译的标准一致[wu2010assessing](https://arxiv.org/html/2407.21646v2#bib.bib81)。
- en: 'Quantitative Analysis. As shown in [Section 4.3](https://arxiv.org/html/2407.21646v2#S4.SS3
    "4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end
    Simultaneous Speech Translation via LLM Agent"), we compare CLASI with the baseline
    methods on RealSI dataset. In terms of the reliable human evaluation metrics,
    VIP, CLASI achieves scores of 81.3% and 78.0% for zh-en and en-zh, respectively.
    While all other models’ VIP scores are lower than 42%. For more references, we
    use 3 widely-used automatic evaluation metrics: BLEU⁵⁵5We use SacreBLEU [post-2018-call](https://arxiv.org/html/2407.21646v2#bib.bib58)
    for all the BLEU calculations in this paper., BLEURT, COMET. Under the automatic
    evaluation metrics, CLASI also surpasses baselines by a large margin. The detailed
    human evaluation results of CLASI can be found in [Section B.2](https://arxiv.org/html/2407.21646v2#A2.SS2
    "B.2 Example of Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary
    Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future
    Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2
    ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4
    Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity
    on End-to-end Simultaneous Speech Translation via LLM Agent").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 定量分析。如[第4.3节](https://arxiv.org/html/2407.21646v2#S4.SS3 "4.3 Translation Quality
    ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech
    Translation via LLM Agent")所示，我们在RealSI数据集上将CLASI与基准方法进行了比较。在可靠的人类评估指标VIP方面，CLASI在zh-en和en-zh的得分分别为81.3%和78.0%。而其他所有模型的VIP得分均低于42%。为了进一步对比，我们使用了三种广泛使用的自动评估指标：BLEU⁵⁵5我们在本文中所有的BLEU计算均使用SacreBLEU [post-2018-call](https://arxiv.org/html/2407.21646v2#bib.bib58)。、BLEURT和COMET。在自动评估指标下，CLASI也大幅超越了基准方法。CLASI的详细人类评估结果可以在[第B.2节](https://arxiv.org/html/2407.21646v2#A2.SS2
    "B.2 Example of Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary
    Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future
    Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2
    ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4
    Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity
    on End-to-end Simultaneous Speech Translation via LLM Agent")中找到。
- en: High VIP marks CLASI a practical system that can help listeners understand real-time
    speech without professional human interpreters. Note that we only consider a system
    is better than others when the VIP is higher. For example, even though *Commercial
    1* achieves higher scores than *SeamlessStreaming* on BLEU and COMET, we still
    consider *SeamlessStreaming* is a better system for zh-en translation based on
    VIP.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 高 VIP 评分使 CLASI 成为一个实用的系统，能够帮助听众在没有专业人工翻译的情况下理解实时语音。请注意，只有当 VIP 更高时，我们才认为一个系统优于另一个系统。例如，尽管
    *商业系统 1* 在 BLEU 和 COMET 上得分高于 *SeamlessStreaming*，我们仍然认为基于 VIP 的 *SeamlessStreaming*
    是更好的 zh-en 翻译系统。
- en: 4.4 Latency
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 延迟
- en: Evaluation Metrics. Due to the differences of grammatical structures between
    languages, a delay in simultaneous interpretation is inevitable. In this paper,
    we adopt the widely-used Average Lagging (AL) [ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45)
    , Length Adaptive Average Lagging (LAAL) [papi2022over](https://arxiv.org/html/2407.21646v2#bib.bib52)
    for comparing the latency of different methods. To achieve a fair comparison with
    systems that rewrite the translation, we calculate the time of the definite translation
    of these systems. We also propose an additional metric, First Letter Appearance
    Lagging (FLAL), to reflect user experience on each system. FLAL represents the
    time that each system outputs the first determined translation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。由于不同语言之间的语法结构差异，同声传译中的延迟是不可避免的。本文采用了广泛使用的平均延迟 (AL) [ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45)、长度自适应平均延迟
    (LAAL) [papi2022over](https://arxiv.org/html/2407.21646v2#bib.bib52) 来比较不同方法的延迟。为了与重写翻译的系统进行公平比较，我们计算了这些系统的确定翻译时间。我们还提出了一个额外的指标，首字母出现延迟
    (FLAL)，用于反映每个系统的用户体验。FLAL 表示每个系统输出首个确定翻译的时间。
- en: Quantitative Results. [Table 4](https://arxiv.org/html/2407.21646v2#S4.T4 "In
    4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human
    Parity on End-to-end Simultaneous Speech Translation via LLM Agent") compares
    the latency of our model with various systems in terms of AL, LAAL, and our proposed
    FLAL on the RealSI and CoVoST. We find that the existing metrics AL and LAAL are
    not suitable latency measurements of paragraph-level SiST on RealSI. When the
    results are significantly shorter or longer than the reference translation, AL
    and LAAL may be largely exaggerated, leading to unreliable high latency. In these
    scenarios, FLAL is a more reliable and stable metric for all the systems.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 定量结果。 [表格 4](https://arxiv.org/html/2407.21646v2#S4.T4 "在 4.4 延迟 ‣ 4.3 翻译质量
    ‣ 4 实验 ‣ 通过 LLM 代理实现端到端实时语音翻译达到人类水平") 比较了我们的模型与多种系统在 RealSI 和 CoVoST 上的 AL、LAAL
    和我们提出的 FLAL 延迟。我们发现，现有的 AL 和 LAAL 指标并不适合用于衡量 RealSI 上段落级 SiST 的延迟。当结果显著短于或长于参考翻译时，AL
    和 LAAL 可能会被大幅夸大，从而导致不可靠的高延迟。在这些情况下，FLAL 是一个更可靠且稳定的指标，适用于所有系统。
- en: Besides the paragraph-level latency evaluation, we compare our approach with
    other systems on the sentence-level dataset CoVoST2 zh-en, where both AL and LAAL
    produce reasonable values and the results are shown on the right side of [Table 4](https://arxiv.org/html/2407.21646v2#S4.T4
    "In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving
    Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"). Since
    the commercial systems usually rewrite the translation, their latency is higher
    than the CLASI. Compared with the fastest approach *SeamlessStreaming*, CLASI
    achieves comparable latency but much better translation quality.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了段落级的延迟评估外，我们还在句子级数据集 CoVoST2 zh-en 上将我们的方法与其他系统进行了比较，其中 AL 和 LAAL 都生成了合理的数值，结果显示在
    [表格 4](https://arxiv.org/html/2407.21646v2#S4.T4 "在 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣
    通过 LLM 代理实现端到端实时语音翻译达到人类水平") 的右侧。由于商业系统通常会重写翻译，它们的延迟比 CLASI 高。与最快的方法 *SeamlessStreaming*
    相比，CLASI 实现了相似的延迟，但翻译质量要好得多。
- en: '| Model | RealSI (zh-en) | RealSI (en-zh) | CoVoST2 (zh-en) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | RealSI (zh-en) | RealSI (en-zh) | CoVoST2 (zh-en) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| AL | LAAL | FLAL | AL | LAAL | FLAL | AL | LAAL | FLAL |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| AL | LAAL | FLAL | AL | LAAL | FLAL | AL | LAAL | FLAL |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| *SeamlessStreaming* | 3.50 | 42.31 | 2.65 | 3.06 | 16.02 | 2.24 | 2.26 |
    2.46 | 4.03 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| *SeamlessStreaming* | 3.50 | 42.31 | 2.65 | 3.06 | 16.02 | 2.24 | 2.26 |
    2.46 | 4.03 |'
- en: '| *Commercial 1* | 2.10 | 13.22 | 3.27 | 4.53 | 20.71 | 1.88 | 3.05 | 3.26
    | 4.01 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| *商业系统 1* | 2.10 | 13.22 | 3.27 | 4.53 | 20.71 | 1.88 | 3.05 | 3.26 | 4.01
    |'
- en: '| *Commercial 2* | 2.92 | 4.30 | 5.90 | 1.05 | 8.02 | 12.42 | 2.65 | 2.88 |
    3.82 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| *商业系统 2* | 2.92 | 4.30 | 5.90 | 1.05 | 8.02 | 12.42 | 2.65 | 2.88 | 3.82
    |'
- en: '| *Commercial 3* | 12.31 | 12.65 | 15.70 | 8.45 | 15.81 | 9.68 | 3.67 | 3.86
    | 6.14 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| *商业系统 3* | 12.31 | 12.65 | 15.70 | 8.45 | 15.81 | 9.68 | 3.67 | 3.86 | 6.14
    |'
- en: '| *Commercial 4* | 26.59 | 27.17 | 6.62 | 16.94 | 24.47 | 5.73 | 3.53 | 3.71
    | 6.20 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| *商业系统 4* | 26.59 | 27.17 | 6.62 | 16.94 | 24.47 | 5.73 | 3.53 | 3.71 | 6.20
    |'
- en: '| *CLASI* | 2.17 | 6.34 | 4.20 | 0.34 | 3.17 | 6.00 | 2.63 | 2.83 | 5.02 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| *CLASI* | 2.17 | 6.34 | 4.20 | 0.34 | 3.17 | 6.00 | 2.63 | 2.83 | 5.02 |'
- en: 'Table 4: Comparison of latency between CLASI and baselines. AL and LAAL are
    standard metrics for measuring latency in sentence-level datasets. Even though
    AL and LAAL yield reliable results on the sentence-level CoVoST2 dataset, we argue
    that they are less effective for long speeches due to the complexity of long-speech
    translation. Therefore, we propose First Letter Appearance Lagging (FLAL), representing
    the time that each system outputs the first determined translation.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：CLASI与基准系统的延迟比较。AL和LAAL是用于衡量句子级数据集延迟的标准指标。尽管AL和LAAL在句子级CoVoST2数据集上提供了可靠的结果，但我们认为由于长语音翻译的复杂性，它们对于长语音的效果较差。因此，我们提出了首字母出现延迟（FLAL），代表每个系统输出第一个确定翻译的时间。
- en: Discussion. While existing works put a lot of emphasis on the latency-quality
    trade-off [koshkin2024transllamallmbasedsimultaneoustranslation](https://arxiv.org/html/2407.21646v2#bib.bib35)
    ; [papi2022attention](https://arxiv.org/html/2407.21646v2#bib.bib53) , human interpretation
    usually uses Ear-Voice-Span (EVS) to evaluate the lagging. EVS measures the average
    time from when the speaker finishes conveying a piece of information to when the
    audience hears the corresponding translation, which is similar to AL. The typical
    EVS of professional human interpreters usually ranges from 3 to 6 seconds [gumul2007time](https://arxiv.org/html/2407.21646v2#bib.bib26)
    to achieve high-quality translation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论。虽然现有的研究在延迟与质量的权衡上投入了大量的关注，[koshkin2024transllamallmbasedsimultaneoustranslation](https://arxiv.org/html/2407.21646v2#bib.bib35)；[papi2022attention](https://arxiv.org/html/2407.21646v2#bib.bib53)，但人工翻译通常使用耳语延迟（Ear-Voice-Span,
    EVS）来评估延迟。EVS衡量的是从说话者完成信息传达到观众听到相应翻译的平均时间，这与AL类似。专业口译员的典型EVS通常在3到6秒之间[gumul2007time](https://arxiv.org/html/2407.21646v2#bib.bib26)，以实现高质量的翻译。
- en: Consequently, we perform user studies and argue that the latency is less important
    than the translation quality for a practical SiST system. In the recent IWSLT
    2023 simultaneous track [iwslt-2023-international](https://arxiv.org/html/2407.21646v2#bib.bib65)
    , the ranking of models is also evaluated by the translation quality within certain
    latency constraints. We verify whether the latency of CLASI is acceptable to users
    through real-world user surveys. To the publication date of this paper, we collected
    14 user surveys on zh-en direction, each user using CLASI for at least 30 minutes.
    Under the current latency performance shown in [Table 4](https://arxiv.org/html/2407.21646v2#S4.T4
    "In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving
    Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"), only
    1/14 == 7% of them suggest that the latency significantly affects their user experiences
    while the rest think the improvement of translation quality outweighs the latency
    and overall output of CLASI largely helps them to understand the speech. Considering
    that the latency of CLASI is even lower than most of the commercial systems, We
    believe the latency of CLASI can be acceptable on most cases.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们进行用户研究，并认为在实际的SiST系统中，延迟比翻译质量更不重要。在最近的IWSLT 2023同步轨道中，[iwslt-2023-international](https://arxiv.org/html/2407.21646v2#bib.bib65)，模型的排名也是在一定的延迟约束下通过翻译质量来评估的。我们通过现实世界的用户调查验证了CLASI的延迟是否为用户所接受。到本文发布日期时，我们已经收集了14份关于zh-en方向的用户调查，每位用户至少使用CLASI
    30分钟。在[表4](https://arxiv.org/html/2407.21646v2#S4.T4 "在4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验
    ‣ 实现人类级别的端到端同步语音翻译")中显示的当前延迟表现下，只有1/14 == 7%的用户表示延迟显著影响了他们的用户体验，而其余用户认为翻译质量的提升比延迟更重要，并且CLASI的整体输出大大帮助他们理解语音。考虑到CLASI的延迟甚至低于大多数商业系统，我们认为CLASI的延迟在大多数情况下是可以接受的。
- en: Current latency metrics are proposed on sentence-level SiST. As shown in [Table 4](https://arxiv.org/html/2407.21646v2#S4.T4
    "In 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving
    Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent"), such
    metrics may not be suitable latency measurements for paragraph-level. As the importance
    of end-to-end evaluation for long speech keeps increasing, more refined metrics
    are required to measure the latency and provide a deeper insight into the systems.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当前延迟指标是基于句子级别的 SiST 提出的。如[表4](https://arxiv.org/html/2407.21646v2#S4.T4 "在 4.4
    延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 实现端到端同步语音翻译的人工智能代理的目标")所示，这些指标可能不适合用于段落级别的延迟度量。随着对长篇语音的端到端评估的重要性不断增加，需要更精细的指标来衡量延迟并深入了解系统表现。
- en: 4.5 Supplementary Experiments
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 补充实验
- en: To ensure a comprehensive evaluation of CLASI, our model is further evaluated
    on four additional datasets, including BSTC (zh-en) [zhang-etal-2021-bstc](https://arxiv.org/html/2407.21646v2#bib.bib85)
    , CoVoST2 (zh-en) [wang2020covost](https://arxiv.org/html/2407.21646v2#bib.bib75)
    , MuST-C (en-zh) [di2019must](https://arxiv.org/html/2407.21646v2#bib.bib15) ,
    and GigaST (en-zh) [ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83)
    ⁶⁶6We use the subset from in [GigaS2S](https://github.com/SpeechTranslation/GigaS2S)
    for evaluation. [Table 5](https://arxiv.org/html/2407.21646v2#S4.T5 "In 4.5 Supplementary
    Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards
    Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent")
    presents the results of the automatic evaluation metrics for both zh-en and en-zh.
    Due to the high cost of human evaluation, we are not able to provide VIP for these
    four datasets. We observe that our model achieves consistently better performance
    than the baseline models. Even though our system achieves the best automatic evaluation
    results among all the compared systems, we still would like to emphasize that
    such a sentence-level evaluation scheme might overestimate the performance of
    SiST systems.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保对 CLASI 的全面评估，我们的模型在四个附加数据集上进行了进一步评估，包括 BSTC (zh-en) [zhang-etal-2021-bstc](https://arxiv.org/html/2407.21646v2#bib.bib85)
    ，CoVoST2 (zh-en) [wang2020covost](https://arxiv.org/html/2407.21646v2#bib.bib75)
    ，MuST-C (en-zh) [di2019must](https://arxiv.org/html/2407.21646v2#bib.bib15) ，以及
    GigaST (en-zh) [ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83)
    ⁶⁶6我们使用[GigaS2S](https://github.com/SpeechTranslation/GigaS2S)中的子集进行评估。[表5](https://arxiv.org/html/2407.21646v2#S4.T5
    "在 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 实现端到端同步语音翻译的人工智能代理的目标")展示了对于 zh-en 和
    en-zh 的自动评估指标结果。由于人工评估成本较高，我们无法为这四个数据集提供 VIP。我们观察到我们的模型在性能上始终优于基线模型。尽管我们的系统在所有比较的系统中获得了最佳的自动评估结果，但我们仍然要强调，这种基于句子级别的评估方案可能会高估
    SiST 系统的性能。
- en: '| Model | BSTC zh-en | CoVoST2 zh-en |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BSTC zh-en | CoVoST2 zh-en |'
- en: '| BLEU | BLEURT | COMET | AL | LAAL | FLAL | BLEU | BLEURT | COMET | AL | LAAL
    | FLAL |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | BLEURT | COMET | AL | LAAL | FLAL | BLEU | BLEURT | COMET | AL | LAAL
    | FLAL |'
- en: '| *SeamlessStreaming* | 9.7 | 34.4 | 78.2 | 11.41 | 68.92 | 3.50 | 19.3 | 54.7
    | 77.1 | 2.27 | 2.46 | 4.03 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| *SeamlessStreaming* | 9.7 | 34.4 | 78.2 | 11.41 | 68.92 | 3.50 | 19.3 | 54.7
    | 77.1 | 2.27 | 2.46 | 4.03 |'
- en: '| *Commercial 1* | 14.1 | 32.0 | 73.0 | 9.01 | 16.73 | 13.95 | 17.6 | 47.6
    | 69.3 | 3.05 | 3.26 | 4.01 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| *商业 1* | 14.1 | 32.0 | 73.0 | 9.01 | 16.73 | 13.95 | 17.6 | 47.6 | 69.3 |
    3.05 | 3.26 | 4.01 |'
- en: '| *Commercial 2* | 17.6 | 39.2 | 81.2 | 6.35 | 7.92 | 13.04 | 24.7 | 56.7 |
    78.5 | 2.65 | 2.88 | 3.82 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| *商业 2* | 17.6 | 39.2 | 81.2 | 6.35 | 7.92 | 13.04 | 24.7 | 56.7 | 78.5 |
    2.65 | 2.88 | 3.82 |'
- en: '| *Commercial 3* | 21.5 | 41.6 | 83.7 | 12.88 | 13.63 | 22.55 | 24.2 | 54.1
    | 75.9 | 3.67 | 3.86 | 6.14 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| *商业 3* | 21.5 | 41.6 | 83.7 | 12.88 | 13.63 | 22.55 | 24.2 | 54.1 | 75.9
    | 3.67 | 3.86 | 6.14 |'
- en: '| *Commercial 4* | 21.2 | 41.9 | 82.3 | 30.50 | 31.84 | 9.61 | 22.1 | 56.1
    | 76.8 | 3.53 | 3.71 | 6.20 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| *商业 4* | 21.2 | 41.9 | 82.3 | 30.50 | 31.84 | 9.61 | 22.1 | 56.1 | 76.8 |
    3.53 | 3.71 | 6.20 |'
- en: '| *CLASI* | 25.6 | 44.8 | 85.6 | 4.68 | 9.03 | 13.13 | 24.2 | 56.8 | 81.0 |
    2.63 | 2.83 | 5.02 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| *CLASI* | 25.6 | 44.8 | 85.6 | 4.68 | 9.03 | 13.13 | 24.2 | 56.8 | 81.0 |
    2.63 | 2.83 | 5.02 |'
- en: '| Model | MuST-C en-zh | GigaST en-zh |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | MuST-C en-zh | GigaST en-zh |'
- en: '| BLEU | BLEURT | COMET | AL | LAAL | FLAL | BLEU | BLEURT | COMET | AL | LAAL
    | FLAL |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | BLEURT | COMET | AL | LAAL | FLAL | BLEU | BLEURT | COMET | AL | LAAL
    | FLAL |'
- en: '| *SeamlessStreaming* | 17.4 | 48.2 | 75.2 | 1.43 | 1.69 | 2.06 | 26.3 | 48.9
    | 75.4 | 1.41 | 1.57 | 2.16 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| *SeamlessStreaming* | 17.4 | 48.2 | 75.2 | 1.43 | 1.69 | 2.06 | 26.3 | 48.9
    | 75.4 | 1.41 | 1.57 | 2.16 |'
- en: '| *Commercial 1* | 24.0 | 55.2 | 81.2 | 2.62 | 2.91 | 2.07 | 43.1 | 59.6 |
    83.4 | 2.55 | 2.73 | 2.33 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| *商业 1* | 24.0 | 55.2 | 81.2 | 2.62 | 2.91 | 2.07 | 43.1 | 59.6 | 83.4 | 2.55
    | 2.73 | 2.33 |'
- en: '| *Commercial 2* | 28.2 | 59.5 | 83.1 | 3.25 | 3.51 | 4.84 | 45.7 | 63.2 |
    85.0 | 3.13 | 3.28 | 5.12 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| *商业 2* | 28.2 | 59.5 | 83.1 | 3.25 | 3.51 | 4.84 | 45.7 | 63.2 | 85.0 | 3.13
    | 3.28 | 5.12 |'
- en: '| *Commercial 3* | 26.9 | 59.9 | 83.7 | 3.59 | 3.90 | 4.86 | 48.3 | 66.2 |
    86.7 | 3.18 | 3.36 | 4.97 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| *商业 3* | 26.9 | 59.9 | 83.7 | 3.59 | 3.90 | 4.86 | 48.3 | 66.2 | 86.7 | 3.18
    | 3.36 | 4.97 |'
- en: '| *Commercial 4* | 27.3 | 60.0 | 83.4 | 3.25 | 3.54 | 4.86 | 43.3 | 59.9 |
    83.5 | 3.06 | 3.23 | 5.00 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| *商业 4* | 27.3 | 60.0 | 83.4 | 3.25 | 3.54 | 4.86 | 43.3 | 59.9 | 83.5 | 3.06
    | 3.23 | 5.00 |'
- en: '| *CLASI* | 26.6 | 61.8 | 85.2 | 3.76 | 3.90 | 4.97 | 50.4 | 69.0 | 88.8 |
    3.30 | 3.40 | 5.01 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| *CLASI* | 26.6 | 61.8 | 85.2 | 3.76 | 3.90 | 4.97 | 50.4 | 69.0 | 88.8 |
    3.30 | 3.40 | 5.01 |'
- en: 'Table 5: Comparisons of CLASI and baselines on paragraph-level (BSTC) and sentence-level
    (CoVoST2, MuST-C, and GigaST) zh-en and en-zh datasets in terms of automatic evaluation
    metrics. We would like to emphasize that sentence-level evaluation schemes by
    automatic metrics cannot truly reflect the models’ performance. VIP in [Section 4.3](https://arxiv.org/html/2407.21646v2#S4.SS3
    "4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end
    Simultaneous Speech Translation via LLM Agent") is a better metrics for comparing
    different systems.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：CLASI 与基线模型在段落级别（BSTC）和句子级别（CoVoST2、MuST-C 和 GigaST）中文-英文和英文-中文数据集上的自动评估指标比较。我们要强调的是，自动评估指标的句子级别评估方案无法真正反映模型的性能。[第
    4.3 节](https://arxiv.org/html/2407.21646v2#S4.SS3 "4.3 翻译质量 ‣ 4 实验 ‣ 通过 LLM 代理实现端到端同步语音翻译的人工对等性")中的
    VIP 是比较不同系统的更好指标。
- en: 4.6 MM-RAG Performance
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 MM-RAG 性能
- en: 4.6.1 Retriever
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.1 检索器
- en: 'Table [6](https://arxiv.org/html/2407.21646v2#S4.T6 "Table 6 ‣ 4.6.1 Retriever
    ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation
    Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent") presents the performance of various retrieve
    models on the development set of our proprietary dataset. Each sample in the test
    set includes a short audio chunk and the mentioned terms in the audio. Our MM-RAG
    retriever outperforms other open-source models by a large margin, achieving 91.3
    % vs. 26.0% for Top-10 retrieve accuracy. We compare two types of methodologies:
    audio-to-audio and audio-to-text. In the audio-to-audio approach, a Text-to-Speech
    (TTS) model is utilized to convert the text keys from the external knowledge database
    into audio format, forming a database with audio-based keys. The audio keys and
    the user-input audio are then encoded with the ASR model to produce the corresponding
    representations. The Top-$k$ retrieved items are subsequently determined using
    the Maximum Inner Product Search (MIPS) algorithm. For audio-to-text approach,
    we compare MM-RAG with CLAP [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    . As indicated in Table [6](https://arxiv.org/html/2407.21646v2#S4.T6 "Table 6
    ‣ 4.6.1 Retriever ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4
    Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity
    on End-to-end Simultaneous Speech Translation via LLM Agent"), the effectiveness
    of these models remains significantly below acceptable standards, and MM-RAG significantly
    outperforms them.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](https://arxiv.org/html/2407.21646v2#S4.T6 "表 6 ‣ 4.6.1 检索器 ‣ 4.6 MM-RAG
    性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 通过 LLM 代理实现端到端同步语音翻译的人工对等性") 展示了我们专有数据集开发集上各类检索模型的性能。测试集中的每个样本包括一段简短的音频和音频中的相关术语。我们的
    MM-RAG 检索器在 Top-10 检索准确率上大幅超越其他开源模型，达到 91.3% 对比 26.0%。我们比较了两种方法：音频到音频和音频到文本。在音频到音频方法中，使用文本到语音（TTS）模型将外部知识库中的文本键转换为音频格式，形成一个基于音频键的数据库。然后，使用
    ASR 模型对音频键和用户输入的音频进行编码，以生成相应的表示。随后，使用最大内积搜索（MIPS）算法确定 Top-$k$ 检索项。对于音频到文本方法，我们将
    MM-RAG 与 CLAP [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    进行了比较。如表 [6](https://arxiv.org/html/2407.21646v2#S4.T6 "表 6 ‣ 4.6.1 检索器 ‣ 4.6
    MM-RAG 性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 通过 LLM 代理实现端到端同步语音翻译的人工对等性")
    所示，这些模型的有效性远低于可接受的标准，而 MM-RAG 明显优于它们。
- en: It is worth noting that the same audio encoder employed in our CLASI is utilized
    for generating audio embedding in the MM-RAG retriever. Such a design ensures
    that the integration brings minimal computational latency to the overall framework.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们在 CLASI 中使用的相同音频编码器被用于生成 MM-RAG 检索器中的音频嵌入。这样的设计确保了集成对整体框架的计算延迟影响最小。
- en: '| Model | Method | Finetuned | Top-1 | Top-5 | Top-10 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 微调 | Top-1 | Top-5 | Top-10 |'
- en: '| *CLAP* [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    | Audio-to-Audio | No | 2.1 | 7.3 | 13.8 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *CLAP* [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    | 音频到音频 | 否 | 2.1 | 7.3 | 13.8 |'
- en: '| *Wav2Clip* [Wu2021Wav2CLIPLR](https://arxiv.org/html/2407.21646v2#bib.bib80)
    | Audio-to-Audio | No | 3.3 | 9.6 | 16.3 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| *Wav2Clip* [Wu2021Wav2CLIPLR](https://arxiv.org/html/2407.21646v2#bib.bib80)
    | 音频到音频 | 否 | 3.3 | 9.6 | 16.3 |'
- en: '| *Whisper* [Radford2022RobustSR](https://arxiv.org/html/2407.21646v2#bib.bib60)
    | Audio-to-Audio | No | 2.6 | 9.7 | 15.1 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| *Whisper* [Radford2022RobustSR](https://arxiv.org/html/2407.21646v2#bib.bib60)
    | 音频到音频 | 否 | 2.6 | 9.7 | 15.1 |'
- en: '| *In-house ASR* | Audio-to-Audio | No | 7.2 | 19.4 | 26.0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| *自研ASR* | 音频到音频 | 否 | 7.2 | 19.4 | 26.0 |'
- en: '| *CLAP* [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    | Audio-to-Text | No | 2.7 | 6.4 | 10.8 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| *CLAP* [Elizalde2023CLAPLA](https://arxiv.org/html/2407.21646v2#bib.bib17)
    | 音频到文本 | 否 | 2.7 | 6.4 | 10.8 |'
- en: '| *MM-RAG (Ours)* | Audio-to-Text | Yes | 63.2 | 88.4 | 91.3 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| *MM-RAG（我们的方法）* | 音频到文本 | 是 | 63.2 | 88.4 | 91.3 |'
- en: 'Table 6: Top-$k$ retrieve accuracy (%).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：Top-$k$检索准确率（%）。
- en: 4.6.2 ICL Performance
  id: totrans-176
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.6.2 ICL性能
- en: By incorporating the retrieved terms from the external knowledge database as
    contextual information, our model’s in-context learning ability significantly
    improves the performance of speech translation for in-domain terminologies. [Table 7](https://arxiv.org/html/2407.21646v2#S4.T7
    "In 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments
    ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human
    Parity on End-to-end Simultaneous Speech Translation via LLM Agent") compares
    our method with the widely-used shallow fusion for intervention in the generated
    conclusion. When calculating the Recall, we input 1 ground-truth keyword with
    9 similar negative words as context. When calculating the false positive rate
    for precision, we input 10 similar negative words as context. ICL is able to achieve
    a high recall rate with good precision, obtaining the highest F1 while shallow
    fusion only gets a recall rate that is only half of ICL.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将从外部知识数据库中检索到的术语作为上下文信息纳入，我们模型的上下文学习能力显著提高了领域内术语的语音翻译性能。[表7](https://arxiv.org/html/2407.21646v2#S4.T7
    "在4.6.2 ICL性能 ‣ 4.6 MM-RAG性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 致力于通过LLM代理实现端到端同步语音翻译与人类水平的对比")将我们的方法与广泛使用的浅层融合进行比较，用于干预生成的结论。在计算召回率时，我们输入1个真实的关键词与9个相似的负面词作为上下文。在计算精度的假阳性率时，我们输入10个相似的负面词作为上下文。ICL能够在保持较高的精度的同时，实现较高的召回率，获得最高的F1，而浅层融合的召回率仅为ICL的一半。
- en: Additionally, we conduct an ablation study on our MM-RAG module within terminology-intensive
    scenarios incorporating the whole <RETRIEVE> pipeline. The incorporation of the
    external knowledge database results in a significant increment in the VIP score
    by about 10%, highlighting the effectiveness of our proposed MM-RAG.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在包含整个<RETRIEVE>管道的术语密集场景中对我们的MM-RAG模块进行了消融研究。引入外部知识数据库后，VIP得分显著提高了约10%，突显了我们提出的MM-RAG的有效性。
- en: '|  | Recall (%) | Precision (%) | F1 (%) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | 召回率（%） | 精度（%） | F1（%） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Shallow-Fusion | 40.8 | 94.2 | 56.9 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 浅层融合 | 40.8 | 94.2 | 56.9 |'
- en: '| ICL + Shallow Fusion | 79.2 | 73.4 | 76.2 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ICL + 浅层融合 | 79.2 | 73.4 | 76.2 |'
- en: '| ICL | 79.2 | 86.3 | 82.6 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ICL | 79.2 | 86.3 | 82.6 |'
- en: 'Table 7: Recall and Precision of ICL and shallow-fusion for the intervention
    of the keywords. When calculating the Recall, we input 1 ground-truth keyword
    with 9 similar negative words as context. When calculating the false positive
    rate for precision, we input 10 similar negative words as context.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：ICL与浅层融合在干预关键词的召回率和精度上的对比。当计算召回率时，我们输入1个真实的关键词与9个相似的负面词作为上下文；当计算精度的假阳性率时，我们输入10个相似的负面词作为上下文。
- en: 4.7 Case Study
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 案例研究
- en: We present case studies to show the ability of CLASI in translating complicated
    speech for zh-en and en-zh in [Table 8](https://arxiv.org/html/2407.21646v2#S4.T8
    "In 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary
    Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards
    Achieving Human Parity on End-to-end Simultaneous Speech Translation via LLM Agent")
    and [Table 9](https://arxiv.org/html/2407.21646v2#S4.T9 "In 4.7 Case Study ‣ 4.6.2
    ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4
    Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity
    on End-to-end Simultaneous Speech Translation via LLM Agent"). We choose one of
    the most-performed cascaded systems Commerical 4 for comparison. The Commerical
    4 adopted a cascaded approach for SiST and it is shown in [Section 4.3](https://arxiv.org/html/2407.21646v2#S4.SS3
    "4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end
    Simultaneous Speech Translation via LLM Agent") to be one of the best previous
    SiST systems. Detailed explanations are described in the tables. For zh-en direction,
    we present cases regarding robustness to recognition errors, reasoning ability,
    and trending words translation. For the en-zh direction, we present cases regarding
    native, expressive, and accurate terminology translations. More cases are shown
    in [Table 11](https://arxiv.org/html/2407.21646v2#A2.T11 "In B.1 Supplementary
    Case Study ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements
    ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation.
    ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance
    ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent").
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了案例研究，展示了CLASI在中文到英文和英文到中文翻译复杂语音的能力，具体见[表8](https://arxiv.org/html/2407.21646v2#S4.T8
    "4.7案例研究 ‣ 4.6.2 ICL表现 ‣ 4.6 MM-RAG表现 ‣ 4.5补充实验 ‣ 4.4延迟 ‣ 4.3翻译质量 ‣ 4实验 ‣ 通过LLM代理实现端到端同步语音翻译的目标与人类水平接近")和[表9](https://arxiv.org/html/2407.21646v2#S4.T9
    "4.7案例研究 ‣ 4.6.2 ICL表现 ‣ 4.6 MM-RAG表现 ‣ 4.5补充实验 ‣ 4.4延迟 ‣ 4.3翻译质量 ‣ 4实验 ‣ 通过LLM代理实现端到端同步语音翻译的目标与人类水平接近")。我们选择了最常用的级联系统之一——商业4进行比较。商业4采用了级联方法进行同步语音翻译（SiST），并在[4.3节](https://arxiv.org/html/2407.21646v2#S4.SS3
    "4.3翻译质量 ‣ 4实验 ‣ 通过LLM代理实现端到端同步语音翻译的目标与人类水平接近")中被认为是最好的之前的SiST系统之一。详细的解释已在表格中描述。对于中文到英文的方向，我们展示了关于对识别错误的鲁棒性、推理能力和趋势词汇翻译的案例。对于英文到中文的方向，我们展示了关于本地化、表达性和准确术语翻译的案例。更多的案例展示在[表11](https://arxiv.org/html/2407.21646v2#A2.T11
    "B.1补充案例研究 ‣ 附录B补充材料 ‣ 作者和致谢 ‣ 社会影响 ‣ 限制与未来工作 ‣ 6结论 ‣ 人类评估 ‣ 5相关工作 ‣ 4.7案例研究 ‣
    4.6.2 ICL表现 ‣ 4.6 MM-RAG表现 ‣ 4.5补充实验 ‣ 4.4延迟 ‣ 4.3翻译质量 ‣ 4实验 ‣ 通过LLM代理实现端到端同步语音翻译的目标与人类水平接近")中。
- en: '| CASE 1: Robustness to recognition errors |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 案例1：对识别错误的鲁棒性 |'
- en: '| --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Golden Transcription | 欧文两罚命中，四分分差¹，不到最后² |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 欧文两罚命中，四分分差¹，不到最后² |'
- en: '| Commerical 4 ASR | 欧文两罚命中，四分分叉¹，不到最后² |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 ASR | 欧文两罚命中，四分分叉¹，不到最后² |'
- en: '| Commerical 4 Translation | Irving hit two free throws and split¹ the four-point
    spread to the end,² |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 欧文两罚命中，四分分差¹，直到最后²， |'
- en: '| CLASI ASR | 欧文两罚命中，四分分叉¹，不到最后² |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 欧文两罚命中，四分分叉¹，不到最后² |'
- en: '| CLASI Translation | Kyrie makes both free throws, a four-point gap¹, |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| CLASI翻译 | 凯里·欧文两罚命中，四分差距¹， |'
- en: '| Explanation | The word 分差¹ is mis-transcripted to 分叉¹, which actually means
    “branch” or “split” in English. CLASI still generates the correct translation.
    After only hearing 不到最后², CLASI decides not to translate immediately and leaves
    不到最后² to the next translation because of lacking context. While Commerical 4 translates
    it incorrectly. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 单词“分差”¹误转录为“分叉”¹，实际上“分叉”意味着“分支”或“分裂”在英语中的意思。CLASI仍然生成了正确的翻译。在仅听到“不到最后”²后，CLASI决定不立即翻译，并将“不到最后”²留给下一次翻译，因为缺乏上下文。而商业4则翻译错误。
    |'
- en: '| CASE 2: Reasoning Ability for Translation |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 案例2：翻译的推理能力 |'
- en: '| Golden Transcription | 绍兴二十年¹担任右正言，弹劾胡寅 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 绍兴二十年¹担任右正言，弹劾胡寅 |'
- en: '| Commerical 4 ASR | 绍兴二十年¹担任佑正言，弹劾胡莹。 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 ASR | 绍兴二十年¹担任佑正言，弹劾胡莹。 |'
- en: '| Commerical 4 Translation | Shaoxing twenty years¹ as YouZhengYan, impeach
    Hu Ying. |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 绍兴二十年¹担任右正言，弹劾胡莹。 |'
- en: '| CLASI ASR | 绍兴二十年¹担任右正言，弹劾胡寅 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 绍兴二十年¹担任右正言，弹劾胡寅 |'
- en: '| CLASI Translation | In 1150, during the 20th year of Emperor Gaozong’s Shaoxing
    era¹, he served as the Right Censor and impeached Hu Ying |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| CLASI翻译 | 在1150年，高宗皇帝绍兴二十年¹期间，他担任右侍郎并弹劾胡颖。 |'
- en: '| Explanation | Literally, 绍兴二十年¹ could be translated as “Shaoxing 20th Year”,
    while CLASI could understand the actual year of 绍兴二十年¹ is AD 1150, the 20th year
    under the reign of Emperor Gaozong. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 字面意思上，绍兴二十年¹可以翻译为“Shaoxing 20th Year”，而CLASI能够理解到绍兴二十年¹实际是公元1150年，即高宗皇帝治下的第20年。
    |'
- en: '| CASE 3：Trending words or slangs |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 案例 3：流行词汇或俚语 |'
- en: '| Golden Transcription | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
- en: '| Commerical 4 ASR | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 商业4语音识别 | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
- en: '| Commerical 4 Translation | We often say that you are too curly¹, don’t curl
    up, or lie down² comfortably. |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 我们常说，你太卷¹了，别卷了，还是躺下²舒服。 |'
- en: '| CLASI ASR | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| CLASI语音识别 | 我们常说，你们也太卷¹了吧，别卷了，还是躺平²舒服。 |'
- en: '| CLASI translation | We often say, “You are too competitive¹. Stop it. It’s
    more comfortable to lie flat².” |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| CLASI翻译 | 我们常说，“你太竞争了¹，停下来吧，躺平²更舒服。” |'
- en: '| Explanation | Although in Chinese 卷¹ could be translated to “curly” in some
    cases, it actually means “involution” in this context. CLASI translates it to
    “competitive”, which is acceptable. Lie flat² is comparable to lie down² for translating
    躺平², but the whole sentence is translated more naturally by CLASI. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 尽管在中文中，卷¹可以在某些情况下翻译为“curly”，但在这个语境下，它实际上指的是“内卷”。CLASI将其翻译为“competitive”，这是可以接受的。“躺平²”与“lie
    down²”在翻译时相似，但CLASI的整个句子翻译得更自然。 |'
- en: 'Table 8: Comparison between CLASI and Commerical 4 for zh-en direction.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '| 表8：CLASI与商业4在中英方向的比较 |'
- en: '| CASE 1: Native and Accurate Translation |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 案例 1：原汁原味和准确的翻译 |'
- en: '| --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Golden Transcription | You can’t think of it on a case-by-case¹ basis. Either
    we all have rights or not have rights. Right². |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 你不能根据具体情况¹来考虑。要么我们都有权利，要么没有权利。对²。 |'
- en: '| Commerical 4 ASR | You can’t think of it on a case-by-case¹ basis. Either
    we all have rights or nut have rights. Right² |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 商业4语音识别 | 你不能根据具体情况¹来考虑。要么我们都有权利，要么没有权利。对² |'
- en: '| Commerical 4 Translation | 你不能根据具体情况¹来考虑。要么我们都有权利，要么疯子都有权利²，对吧？ |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 你不能根据具体情况¹来考虑。要么我们都有权利，要么疯子都有权利²，对吧？ |'
- en: '| CLASI ASR | You can’t think of it on a case-by-case¹ basis. Either we all
    have rights or not have rights. Right². |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| CLASI语音识别 | 你不能根据具体情况¹来考虑。要么我们都有权利，要么没有权利。对²。 |'
- en: '| CLASI Translation | 你不能就事论事¹地考虑这个问题。 要么我们都有权利，要么我们都没有权利²。 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| CLASI翻译 | 你不能就事论事¹地考虑这个问题。 要么我们都有权利，要么我们都没有权利²。 |'
- en: '| Explanation | Although the Commerical 4 translation of case-by-case¹ is correct,
    CLASI uses 就事论事¹, a well-known Chinese idiom, which is more native. Besides, Commerical
    4 ASR mis-transcripted not have rights² as nut have rights², leading to a completely
    non-sense translation. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 尽管商业4的逐字翻译“case-by-case¹”是正确的，CLASI使用了“就事论事¹”，这是一个著名的中文成语，更加符合本土语言习惯。此外，商业4的自动语音识别错误地转录了“not
    have rights²”为“nut have rights²”，导致完全没有意义的翻译。 |'
- en: '| CASE 2: Expressive Translation |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 案例 2：表现力翻译 |'
- en: '| Golden Transcription | She was sobbing in fear that this test in a foreign
    language has been put in front of her. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 她因害怕这门外语考试摆在她面前而抽泣。 |'
- en: '| Commerical 4 ASR | She was sobbing in fear that this test in a foreign language
    has been put in front of her. |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 商业4语音识别 | 她因害怕这门外语考试摆在她面前而抽泣。 |'
- en: '| Commerical 4 Translation | 她哭了，害怕这个外语的测试摆在她面前， |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 她哭了，害怕这个外语的测试摆在她面前， |'
- en: '| CLASI ASR | She was sobbing in fear that this test in a foreign language
    has been put in front of her. |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| CLASI语音识别 | 她因害怕这门外语考试摆在她面前而抽泣。 |'
- en: '| CLASI Translation | 她因为害怕这门外语考试而哭泣， |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| CLASI翻译 | 她因为害怕这门外语考试而哭泣， |'
- en: '| Explanation | Theoretically, the Commerical 4 translation is correct literally.
    However, it’s not expressive for native Chinese speakers. CLASI translation is
    expressive, conveying the same meaning of the source English sentence, which means
    “She was sobbing because of fearing the foreign language test.” |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 理论上，商业4的翻译字面上是正确的。然而，这对母语为中文的讲者来说表达不够清晰。CLASI的翻译更具表现力，传达了原英文句子的相同含义，即“她因为害怕外语考试而哭泣”。
    |'
- en: '| CASE 3: Named Entity, Terminology Recognition and Translation |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 案例 3：命名实体、术语识别与翻译 |'
- en: '| Golden Transcription | So let’s let me put the COVID-19¹ for example, so
    now we we know that there are a lot of people are infected and they have um positive
    antibody tests |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 所以让我以COVID-19¹为例，现在我们知道有很多人感染了，他们有阳性抗体检测结果。 |'
- en: '| Commerical 4 ASR | So let’s let me put the CUBA 19¹ for example, so now we
    we know that there are a lot of people are infected and they have um positive,
    anybody? tests,² |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 ASR | 所以让我以CUBA 19¹为例，现在我们知道很多人感染了，他们的抗体检测呈阳性，任何人？测试，² |'
- en: '| Commerical 4 Translation | 所以让我以古巴19人¹为例。所以现在我们知道有很多人被感染，他们是阳性的。有人吗？测试，²
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 商业4翻译 | 所以让我以古巴19人¹为例。所以现在我们知道有很多人被感染，他们是阳性的。有人吗？测试，² |'
- en: '| CLASI ASR | So let’s let me put the COVID nineteen¹ for example, so now we
    we know that there are a lot of people are infected and they have um positive
    antibody tests.² |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 所以让我以COVID-19¹为例。我们现在知道有很多人被感染，他们的抗体检测呈阳性。² |'
- en: '| CLASI Translation | 以Covid-19¹为例。 我们现在知道 有很多人被感染了， 并且他们的抗体检测呈阳性。² |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| CLASI 翻译 | 以COVID-19¹为例。我们现在知道有很多人被感染了，并且他们的抗体检测呈阳性。² |'
- en: '| Explanation | Commerical 4 cannot correctly recognize COVID-19¹ and antibody
    tests², while CLASI successfully recognize and translate. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 说明 | 商业4无法正确识别COVID-19¹和抗体检测²，而CLASI能够成功识别并翻译。 |'
- en: 'Table 9: Comparison between CLASI and Commerical 4 for en-zh direction.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：CLASI与商业4在英文到中文方向的比较。
- en: 5 Related Work
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Large language model.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大型语言模型。
- en: The encoder-decoder architecture [polak-etal-2023-towards](https://arxiv.org/html/2407.21646v2#bib.bib56)
    ; [ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83) has been
    widely explored in early speech translation research, but with the advent of large
    language models [achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)
    , there has been a growing interest in employing decoder-only architectures [fu2023decoderonlyencoderdecoderinterpretinglanguage](https://arxiv.org/html/2407.21646v2#bib.bib21)
    ; [seide2024speechreallmrealtime](https://arxiv.org/html/2407.21646v2#bib.bib67)
    for sequence-to-sequence problems. While recent efforts have emerged in utilizing
    large language models for machine translation [li2024mtpatcherselectiveextendableknowledge](https://arxiv.org/html/2407.21646v2#bib.bib36)
    ; [li2024eliciting](https://arxiv.org/html/2407.21646v2#bib.bib37) ; [zheng2024finetuninglargelanguagemodels](https://arxiv.org/html/2407.21646v2#bib.bib90)
    ; [zhu2024multilingual](https://arxiv.org/html/2407.21646v2#bib.bib91) and speech
    translation [chu2023qwenaudioadvancinguniversalaudio](https://arxiv.org/html/2407.21646v2#bib.bib13)
    ; [huang2023speechtranslationlargelanguage](https://arxiv.org/html/2407.21646v2#bib.bib30)
    ; [tang2024salmonngenerichearingabilities](https://arxiv.org/html/2407.21646v2#bib.bib70)
    , the application of such models in simultaneous translation tasks remains limited.
    Although there has been early attempts to utilize LLM for simultaneous machine
    translation [agostinelli2023simul](https://arxiv.org/html/2407.21646v2#bib.bib3)
    ; [hu2024gentranslate](https://arxiv.org/html/2407.21646v2#bib.bib29) ; [koshkin2024transllamallmbasedsimultaneoustranslation](https://arxiv.org/html/2407.21646v2#bib.bib35)
    ; [zhang2024streamspeech](https://arxiv.org/html/2407.21646v2#bib.bib86) , to
    the best of our knowledge, no existing work has been found that explores the utilization
    of large language models for end-to-end simultaneous speech translation with such
    remarkable improvement.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器架构 [polak-etal-2023-towards](https://arxiv.org/html/2407.21646v2#bib.bib56)；[ye23b_interspeech](https://arxiv.org/html/2407.21646v2#bib.bib83)
    在早期的语音翻译研究中被广泛探讨，但随着大型语言模型的出现 [achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)，越来越多的研究开始关注仅解码器架构
    [fu2023decoderonlyencoderdecoderinterpretinglanguage](https://arxiv.org/html/2407.21646v2#bib.bib21)；[seide2024speechreallmrealtime](https://arxiv.org/html/2407.21646v2#bib.bib67)
    用于序列到序列问题。尽管最近有研究利用大型语言模型进行机器翻译 [li2024mtpatcherselectiveextendableknowledge](https://arxiv.org/html/2407.21646v2#bib.bib36)；[li2024eliciting](https://arxiv.org/html/2407.21646v2#bib.bib37)；[zheng2024finetuninglargelanguagemodels](https://arxiv.org/html/2407.21646v2#bib.bib90)；[zhu2024multilingual](https://arxiv.org/html/2407.21646v2#bib.bib91)
    和语音翻译 [chu2023qwenaudioadvancinguniversalaudio](https://arxiv.org/html/2407.21646v2#bib.bib13)；[huang2023speechtranslationlargelanguage](https://arxiv.org/html/2407.21646v2#bib.bib30)；[tang2024salmonngenerichearingabilities](https://arxiv.org/html/2407.21646v2#bib.bib70)，但是这些模型在同步翻译任务中的应用仍然有限。尽管早期有尝试将LLM用于同步机器翻译
    [agostinelli2023simul](https://arxiv.org/html/2407.21646v2#bib.bib3)；[hu2024gentranslate](https://arxiv.org/html/2407.21646v2#bib.bib29)；[koshkin2024transllamallmbasedsimultaneoustranslation](https://arxiv.org/html/2407.21646v2#bib.bib35)；[zhang2024streamspeech](https://arxiv.org/html/2407.21646v2#bib.bib86)，但据我们所知，目前没有发现相关工作探索如何利用大型语言模型进行端到端的同步语音翻译并实现显著改进。
- en: Furthermore, LLMs have demonstrated impressive capabilities in tasks such as
    instruction following [achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)
    ; [bai2023qwen](https://arxiv.org/html/2407.21646v2#bib.bib6) ; [feng2024agilenovelframeworkllm](https://arxiv.org/html/2407.21646v2#bib.bib20)
    ; [touvron2023llama](https://arxiv.org/html/2407.21646v2#bib.bib72) , reasoning
    [paul2024refinerreasoningfeedbackintermediate](https://arxiv.org/html/2407.21646v2#bib.bib55)
    ; [shinn2023reflexionlanguageagentsverbal](https://arxiv.org/html/2407.21646v2#bib.bib69)
    , and planning [qiao2024autoactautomaticagentlearning](https://arxiv.org/html/2407.21646v2#bib.bib59)
    ; [ruan2023tptulargelanguagemodelbased](https://arxiv.org/html/2407.21646v2#bib.bib64)
    . Recent research studies have leveraged prompt engineering to develop remarkable
    LLM agents that autonomously tackle complex tasks in diverse environments [wang2024survey](https://arxiv.org/html/2407.21646v2#bib.bib76)
    . In our work, we empower the LLM to perform sequential instructions to accomplish
    the simulation speech translation task.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，大型语言模型（LLMs）在指令跟随 [achiam2023gpt](https://arxiv.org/html/2407.21646v2#bib.bib2)
    ； [bai2023qwen](https://arxiv.org/html/2407.21646v2#bib.bib6) ； [feng2024agilenovelframeworkllm](https://arxiv.org/html/2407.21646v2#bib.bib20)
    ； [touvron2023llama](https://arxiv.org/html/2407.21646v2#bib.bib72) 、推理 [paul2024refinerreasoningfeedbackintermediate](https://arxiv.org/html/2407.21646v2#bib.bib55)
    ； [shinn2023reflexionlanguageagentsverbal](https://arxiv.org/html/2407.21646v2#bib.bib69)
    和规划 [qiao2024autoactautomaticagentlearning](https://arxiv.org/html/2407.21646v2#bib.bib59)
    ； [ruan2023tptulargelanguagemodelbased](https://arxiv.org/html/2407.21646v2#bib.bib64)
    等任务中展现了令人印象深刻的能力。近期的研究利用提示工程开发了出色的LLM代理，这些代理能够在各种环境中自主处理复杂任务 [wang2024survey](https://arxiv.org/html/2407.21646v2#bib.bib76)
    。在我们的工作中，我们赋予LLM执行顺序指令，以完成模拟语音翻译任务的能力。
- en: Simultaneous Speech Translation.
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 同步语音翻译。
- en: One of the important components of simultaneous speech translation is the segmentation
    strategy, which determines how the speech frames are fed to the models. Different
    strategies could affect the latency and performance of the translation. According
    to [liu2024recentadvancesendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib41)
    , segmentation strategies can be classified into fixed-length, word-based, and
    adaptive segmentation. Fixed-length strategies [nguyen2021empiricalstudyendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib50)
    divide the speech into equally-length segments, while word-based strategies [ma-etal-2020-simulmt](https://arxiv.org/html/2407.21646v2#bib.bib46)
    identify word boundaries within the speech. Adaptive segmentation [dong-etal-2022-learning](https://arxiv.org/html/2407.21646v2#bib.bib16)
    detects boundaries for speech units. Among these categories, our method utilizes
    a fixed-length strategy.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 同步语音翻译的一个重要组成部分是分段策略，它决定了如何将语音帧输入模型。不同的策略可能会影响翻译的延迟和性能。根据 [liu2024recentadvancesendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib41)
    的研究，分段策略可以分为固定长度、基于词的和自适应分段。固定长度策略 [nguyen2021empiricalstudyendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib50)
    将语音划分为等长的段，而基于词的策略 [ma-etal-2020-simulmt](https://arxiv.org/html/2407.21646v2#bib.bib46)
    在语音中识别词边界。自适应分段 [dong-etal-2022-learning](https://arxiv.org/html/2407.21646v2#bib.bib16)
    检测语音单元的边界。在这些分类中，我们的方法使用固定长度策略。
- en: Regarding the read/wait policy, the Wait-k method [ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45)
    and its variants [nguyen2021empiricalstudyendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib50)
    ; [zeng-etal-2021-realtrans](https://arxiv.org/html/2407.21646v2#bib.bib84) have
    been extensively studied in the context of text translation and speech translation.
    In comparison to these approaches, which explicitly learn the generation of read/write
    signals, another line of research focus on how to leverage offline translation
    models [yan-etal-2023-cmus](https://arxiv.org/html/2407.21646v2#bib.bib82) . When
    utilizing an offline translation model for simultaneous translation, it is important
    to address the stabilization of generated hypotheses to prevent excessive content
    refreshing experienced by the user. [liu2020lowlatencysequencetosequencespeechrecognition](https://arxiv.org/html/2407.21646v2#bib.bib40)
    first proposed a local agreement policy to stabilize the partial hypothesis, while
    [Polk2023](https://arxiv.org/html/2407.21646v2#bib.bib57) introduced an incremental
    blockwise beam-search algorithm. In contrast to these methods, we enforce our
    model to generate consistent hypotheses by constraining the prompt to the language
    model.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 关于读/等待策略，Wait-k 方法 [ma-etal-2019-stacl](https://arxiv.org/html/2407.21646v2#bib.bib45)
    及其变体 [nguyen2021empiricalstudyendtoendsimultaneous](https://arxiv.org/html/2407.21646v2#bib.bib50)；[zeng-etal-2021-realtrans](https://arxiv.org/html/2407.21646v2#bib.bib84)
    在文本翻译和语音翻译的背景下得到了广泛研究。与这些方法相比，它们明确学习读/写信号的生成，另一类研究则集中在如何利用离线翻译模型 [yan-etal-2023-cmus](https://arxiv.org/html/2407.21646v2#bib.bib82)。在使用离线翻译模型进行同步翻译时，重要的是解决生成假设的稳定性问题，以防止用户经历过度内容刷新的情况。[liu2020lowlatencysequencetosequencespeechrecognition](https://arxiv.org/html/2407.21646v2#bib.bib40)
    首次提出了本地一致性策略以稳定部分假设，而 [Polk2023](https://arxiv.org/html/2407.21646v2#bib.bib57)
    引入了一种增量块式束搜索算法。与这些方法相比，我们通过约束语言模型的提示来强制我们的模型生成一致的假设。
- en: 'For the model architecture, there are two primary methods for implementing
    speech translation systems: cascaded solutions [guo-etal-2023-hw](https://arxiv.org/html/2407.21646v2#bib.bib27)
    ; [iranzo2021streaming](https://arxiv.org/html/2407.21646v2#bib.bib31) and end-to-end
    solutions [fukuda-etal-2023-naist](https://arxiv.org/html/2407.21646v2#bib.bib22)
    ; [papi-etal-2023-direct](https://arxiv.org/html/2407.21646v2#bib.bib51) ; [polak-etal-2023-towards](https://arxiv.org/html/2407.21646v2#bib.bib56)
    . Cascaded solutions involve separated ASR and MT components, while end-to-end
    solutions directly map speech to translations. Cascaded systems benefit from established
    techniques but suffer from latency and error propagation. End-to-end models offer
    real-time translation and improved quality through deep learning but require large-size
    training data. In our work, we implement an end-to-end model which combines the
    capabilities of ASR, MT, and ST.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型架构，语音翻译系统的实现主要有两种方法：级联解决方案 [guo-etal-2023-hw](https://arxiv.org/html/2407.21646v2#bib.bib27)；[iranzo2021streaming](https://arxiv.org/html/2407.21646v2#bib.bib31)，以及端到端解决方案
    [fukuda-etal-2023-naist](https://arxiv.org/html/2407.21646v2#bib.bib22)；[papi-etal-2023-direct](https://arxiv.org/html/2407.21646v2#bib.bib51)；[polak-etal-2023-towards](https://arxiv.org/html/2407.21646v2#bib.bib56)。级联解决方案涉及分离的自动语音识别（ASR）和机器翻译（MT）组件，而端到端解决方案直接将语音映射到翻译。级联系统得益于成熟的技术，但存在延迟和错误传播的问题。端到端模型通过深度学习提供实时翻译和改进的质量，但需要大量的训练数据。在我们的工作中，我们实现了一个端到端模型，结合了ASR、MT和ST的能力。
- en: Human Evaluation.
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人类评估。
- en: In the realm of speech translation, the choice of evaluation metrics plays a
    crucial role in assessing the quality and effectiveness of translation systems.
    While automatic metrics, such as BLEU [papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)
    , BLEURT [sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68) , and
    COMET [rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61) , have traditionally
    been relied upon for evaluation, there is a growing recognition that they may
    not be the most suitable or comprehensive measure of performance [marie-etal-2021-scientific](https://arxiv.org/html/2407.21646v2#bib.bib48)
    . We observe that in more recent works [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)
    ; [liu2024chatqasurpassinggpt4conversational](https://arxiv.org/html/2407.21646v2#bib.bib42)
    ; [nllbteam2022languageleftbehindscaling](https://arxiv.org/html/2407.21646v2#bib.bib71)
    ; [wang-etal-2023-document-level](https://arxiv.org/html/2407.21646v2#bib.bib77)
    , there is an increasing trend of evaluating systems using human assessments,
    particularly when LLM is employed in the work. While human evaluation requires
    more resources and time compared to automatic metrics, its benefits outweigh the
    drawbacks. By incorporating human judgment, speech translation systems can be
    refined and optimized to align with user expectations, ensuring translations that
    are not only technically accurate but also linguistically and contextually appropriate.
    In contrast to the existing human evaluation metrics, e.g.“continuous rating”
    [javorsky-etal-2022-continuous](https://arxiv.org/html/2407.21646v2#bib.bib32)
    and MQM (Multidimensional Quality Metrics) [lommel2014multidimensional](https://arxiv.org/html/2407.21646v2#bib.bib43)
    , we have taken inspiration from professional human interpreters [moores2024nerle](https://arxiv.org/html/2407.21646v2#bib.bib49)
    and propose to use VIP (Valid Information Proportion) as a human evaluation metric
    which precisely reflects the goal of the simultaneous translation task.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音翻译领域，评估指标的选择在评估翻译系统的质量和效果中起着至关重要的作用。虽然自动化指标，如BLEU [papineni2002bleu](https://arxiv.org/html/2407.21646v2#bib.bib54)、BLEURT
    [sellam2020bleurt](https://arxiv.org/html/2407.21646v2#bib.bib68) 和COMET [rei2020comet](https://arxiv.org/html/2407.21646v2#bib.bib61)，长期以来一直被用作评估标准，但越来越多的认识到它们可能不是最适合或最全面的性能衡量标准
    [marie-etal-2021-scientific](https://arxiv.org/html/2407.21646v2#bib.bib48)。我们观察到，在近年来的研究中
    [barrault2023seamless](https://arxiv.org/html/2407.21646v2#bib.bib7)；[liu2024chatqasurpassinggpt4conversational](https://arxiv.org/html/2407.21646v2#bib.bib42)；[nllbteam2022languageleftbehindscaling](https://arxiv.org/html/2407.21646v2#bib.bib71)；[wang-etal-2023-document-level](https://arxiv.org/html/2407.21646v2#bib.bib77)，越来越多的趋势是使用人工评估来评价系统，特别是在使用大型语言模型（LLM）的工作中。尽管与自动化指标相比，人工评估需要更多的资源和时间，但其优势远大于缺点。通过引入人工判断，语音翻译系统可以被精细化和优化，以符合用户的期望，确保翻译不仅在技术上准确，而且在语言和语境上都恰当。与现有的人类评估指标，如“连续评分”
    [javorsky-etal-2022-continuous](https://arxiv.org/html/2407.21646v2#bib.bib32)
    和MQM（多维质量指标） [lommel2014multidimensional](https://arxiv.org/html/2407.21646v2#bib.bib43)
    相比，我们受到专业人工翻译员的启发 [moores2024nerle](https://arxiv.org/html/2407.21646v2#bib.bib49)，提出使用VIP（有效信息比例）作为人类评估指标，能够精确反映同声传译任务的目标。
- en: 6 Conclusion
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this work, we introduce Cross Language Agent - Simultaneous Interpretation,
    CLASI, an LLM agent to produce end-to-end simultaneous speech translation. Benefits
    from massive pretraining and imitation learning, CLASI achieves significantly
    better performances than state-of-the-art systems. Take the Chinese-to-English
    direction as an example, under strict and challenging human-evaluated metrics
    proposed by professional human interpreters, Valid Information Proportion (VIP),
    CLASI significantly outperforms baselines by a large margin. While all other systems
    obtain VIP by less than 40%, CLASI achieves a VIP of 81.3%, demonstrating human
    parity performance. More specifically, we propose the following crucial components
    for the supreme performance of CLASI: (1) An encoder-conditioned LLM agent architecture
    that performs high-quality or even human-parity SiST process through simple actions.
    (2) Imitation learning from human interpreters for a natural read-write policy
    balances translation quality and latency in a data-driven manner, without complex
    human pre-designing. Under such policy, CLASI achieves a stable output scheme,
    where each output is deterministic, thus potentially better user experience than
    most commercial systems. (3) Motivated by the preparatory trajectory of human
    interpreters, CLASI could perform in-context learning from historical translations
    and external knowledge to provide sufficient information for translation. With
    the powerful translation ability of CLASI, we believe it can further make cross-lingual
    communication seamless across different places all over the world.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们介绍了跨语言智能体——同声传译系统（Cross Language Agent - Simultaneous Interpretation，简称
    CLASI），这是一种生成端到端同声语音翻译的 LLM 智能体。得益于大规模的预训练和模仿学习，CLASI 的表现远超现有的最先进系统。以中文到英文的翻译方向为例，在由专业人工口译员提出的严格且具有挑战性的人工评估标准下——有效信息比例（Valid
    Information Proportion，VIP），CLASI 显著优于基准系统，且差距巨大。当其他所有系统的 VIP 都低于 40% 时，CLASI
    的 VIP 达到了 81.3%，展现了与人工口译员相当的表现。更具体地说，我们提出了以下几个关键组成部分，以实现 CLASI 的卓越表现：(1) 一种基于编码器条件的
    LLM 智能体架构，通过简单的操作执行高质量甚至接近人工水平的同声翻译处理。 (2) 通过向人工口译员学习模仿，形成一种自然的读写策略，以数据驱动的方式平衡翻译质量和延迟，无需复杂的人工预设。在这种策略下，CLASI
    实现了稳定的输出机制，其中每个输出都是确定性的，因此在用户体验上可能优于大多数商业系统。 (3) 受人工口译员准备过程的启发，CLASI 可以从历史翻译和外部知识中进行上下文学习，为翻译提供足够的信息。凭借
    CLASI 强大的翻译能力，我们相信它可以进一步实现跨语言、跨地区的无缝交流。
- en: Limitation and Future Work
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制与未来工作
- en: Although we achieved significant improvements over commercial systems in Chinese-to-English
    and English-to-Chinese tasks, more languages should be considered in the future.
    In our current implementation, CLASI performs a full action sequence for each
    translation round. Some of the actions, e.g., <RETRIEVE> is optional for easy
    translation scenarios since the model is capable of translating correctly without
    the help of external knowledge. Training the model to better determine whether
    to skip unnecessary actions is a future direction. For a product-level system,
    even though the latency of CLASI is acceptable in most cases, how to reduce the
    translation latency without lowering the translation quality is still interesting
    and potentially helpful for user experience. Furthermore, we argue that the current
    automatic metrics are not comprehensive for SiST evaluation. Most of the quality
    measurements do not consider key information, which is crucial in SiST scenarios.
    As such, we proposed VIP for better human evaluation. Consequently, more reliable
    automatic quality and latency metrics should be proposed in the future as well.
    Reinforcement learning from human feedback (RLHF) has been proven to be effective
    in enhancing LLM performance. Although CLASI achieves significantly superior results
    than previous state-of-the-art systems, further studies on how to build better
    multi-modal reward models and better RL methods for SiST is also an important
    direction. Incorporating more modalities, for example, end-to-end speech-to-speech
    generation, or even end-to-end video-to-video generation are also promising research
    topics.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在中英互译任务中相较于商业系统取得了显著的进展，但未来应考虑更多语言。在我们当前的实现中，CLASI 在每个翻译回合中执行完整的动作序列。其中一些动作，例如
    <RETRIEVE>，在简单翻译场景下是可选的，因为该模型能够在没有外部知识帮助的情况下正确翻译。训练模型更好地判断是否跳过不必要的动作是未来的一个方向。对于产品级系统而言，尽管在大多数情况下
    CLASI 的延迟是可以接受的，但如何在不降低翻译质量的前提下减少翻译延迟，仍然是一个值得研究且可能对用户体验有所帮助的问题。此外，我们认为当前的自动化评估指标对于
    SiST 评估来说并不全面。大多数质量衡量标准没有考虑到关键信息，而这些信息在 SiST 场景中至关重要。因此，我们提出了 VIP 以便进行更好的人工评估。因此，未来应该提出更可靠的自动质量和延迟评估指标。来自人工反馈的强化学习（RLHF）已被证明在提升大语言模型性能方面有效。尽管
    CLASI 相较于以往的最先进系统取得了显著的优越成绩，但如何构建更好的多模态奖励模型以及更好的强化学习方法，以提升 SiST 也是一个重要的研究方向。加入更多的模态，例如端到端的语音对语音生成，或甚至端到端的视频对视频生成，也是很有前景的研究课题。
- en: Social Impact
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 社会影响
- en: The powerful SiST system CLASI can be applied to various scenarios to facilitate
    cross-lingual communications. For example, it can be deployed to various conferences
    or daily meetings to help listeners understand speech in different languages.
    It can also be deployed as a system-level translation module to help users watch
    videos that are conveyed in different languages. For online gaming, it can also
    help to bridge the gap of cross-lingual communication and connect people speaking
    different languages. A powerful SiST system with human parity performance may
    significantly improve the efficiency of professional human interpreters.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的 SiST 系统 CLASI 可应用于各种场景，促进跨语言交流。例如，它可以部署在各种会议或日常会议中，帮助听众理解不同语言的演讲内容。它还可以作为系统级翻译模块，帮助用户观看以不同语言传达的视频。在在线游戏中，它也能帮助弥合跨语言交流的差距，连接讲不同语言的人们。一款具有类人水平表现的强大
    SiST 系统，可能显著提高专业人工翻译人员的工作效率。
- en: Despite the huge positive social impact that CLASI may bring, every coin has
    two sides. Neglecting some low-resource languages may also bring unfairness to
    some minorities. Resolving these problems needs further cooperation from the society.
    We leave more languages supporting as our future work.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CLASI 可能带来巨大的社会正面影响，但任何事物都有两面性。忽视一些低资源语言可能会对某些少数群体造成不公平。解决这些问题需要社会的进一步合作。我们将更多语言的支持作为未来的工作。
- en: Authorship and Acknowledgements
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作者及致谢
- en: All contributors are listed in alphabetical order by last name. Corresponding
    to this work can be sent to any core authors’ email.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 所有贡献者按姓氏的字母顺序排列。本研究的相关事宜可通过任何核心作者的邮箱联系。
- en: Core Authors. All core authors contributed equally to this work.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 核心作者。所有核心作者对本研究作出了平等的贡献。
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Shanbo Cheng
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Shanbo Cheng
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zhichao Huang
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zhichao Huang
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tom Ko
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Tom Ko
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Hang Li
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hang Li
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Ningxin Peng
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Ningxin Peng
- en: •
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Lu Xu
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Lu Xu
- en: •
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Qini Zhang
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Qini Zhang
- en: chengshanbo@bytedance.com
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: chengshanbo@bytedance.com
- en: zhichao.huang@bytedance.com
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: zhichao.huang@bytedance.com
- en: tom.ko@bytedance.com
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: tom.ko@bytedance.com
- en: lihang.lh@bytedance.com
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: lihang.lh@bytedance.com
- en: pengningxin@bytedance.com
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: pengningxin@bytedance.com
- en: xu.lu1@bytedance.com
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: xu.lu1@bytedance.com
- en: qini.z@bytedance.com
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: qini.z@bytedance.com
- en: Labeling, Evaluation and Interpretation Team. Our data labeling and human evaluation
    team led by Yifu Li, made diligent efforts in all kinds of help needed, which
    is irreplaceable in the success of this project. Special thanks to the human interpreter
    team led by Anna Liu for providing insightful and comprehensive analysis on data
    labeling, human evaluation, and other recommendations.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 数据标注、评估与解读团队。我们的数据标注与人工评估团队，由李一夫领导，在所有需要的帮助方面做出了不懈努力，这对项目的成功至关重要。特别感谢由刘安娜领导的人类翻译团队，提供了关于数据标注、人工评估及其他建议的深刻而全面的分析。
- en: •
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Jingwen Chen
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 陈晶文
- en: •
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Xiaoya Chen
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 陈晓雅
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Yifu Li
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 李一夫
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Huiying Lin
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 林慧颖
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Anna Liu
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 刘安娜
- en: Engineering Team. We collaborated with our engineering team led by Tingshuai
    Yan, their infrastructure support is crucial for this project.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 工程团队。我们与由颜廷帅领导的工程团队密切合作，他们的基础设施支持对本项目至关重要。
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Weicheng Fu
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 付伟成
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Tingshuai Yan
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 颜廷帅
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Liehao Zou
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 邹磊昊
- en: Acknowledgements. We appreciate the Speech Understanding team for all kinds
    of help, especially data sharing, thanks to their tremendous work for all the
    in-house data. We would like to express our deepest thanks to all the contributors
    to this project, their brilliant work guarantees the success of this project.
    We also want to thank Wenda Xu and Xi Xu for their suggestions on automatic evaluations.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢。我们感谢语音理解团队的各种帮助，特别是数据共享，感谢他们在所有内部数据方面的巨大努力。我们要向所有为本项目作出贡献的人表示最深的感谢，他们的卓越工作保证了本项目的成功。我们还要感谢徐文达和徐希对自动评估提出的建议。
- en: References
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Kendall Rank Correlation Coefficient, pages 278–281. Springer New York,
    New York, NY, 2008.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Kendall 排序相关系数，第278-281页。Springer New York，纽约，NY，2008年。'
- en: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等人。GPT-4
    技术报告。arXiv 预印本 arXiv:2303.08774，2023年。'
- en: '[3] Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif Fuad, and Lizhong
    Chen. Simul-llm: A framework for exploring high-quality simultaneous translation
    with large language models. arXiv preprint arXiv:2312.04691, 2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif Fuad 和 Lizhong
    Chen。Simul-LLM：探索基于大语言模型的高质量同步翻译框架。arXiv 预印本 arXiv:2312.04691，2023年。'
- en: '[4] Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni,
    Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan
    Niehues, et al. Findings of the iwslt 2021 evaluation campaign. In Proceedings
    of the 18th International Conference on Spoken Language Translation (IWSLT 2021),
    pages 1–29, 2021.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Antonios Anastasopoulos, Ondřej Bojar, Jacob Bremerman, Roldano Cattoni,
    Maha Elbayad, Marcello Federico, Xutai Ma, Satoshi Nakamura, Matteo Negri, Jan
    Niehues 等人。IWSLT 2021 评估活动的研究成果。载于《第18届国际口语语言翻译会议论文集》（IWSLT 2021），第1-29页，2021年。'
- en: '[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
    Auli. Data2vec: A general framework for self-supervised learning in speech, vision
    and language. In International Conference on Machine Learning, pages 1298–1312\.
    PMLR, 2022.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu 和 Michael
    Auli。Data2Vec：语音、视觉和语言中的自监督学习通用框架。载于《国际机器学习会议论文集》，第1298-1312页。PMLR，2022年。'
- en: '[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint
    arXiv:2309.16609, 2023.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang
    Fan, Wenbin Ge, Yu Han, Fei Huang 等人。Qwen 技术报告。arXiv 预印本 arXiv:2309.16609，2023年。'
- en: '[7] Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong,
    Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim,
    et al. Seamless: Multilingual expressive and streaming speech translation. arXiv
    preprint arXiv:2312.05187, 2023.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Loïc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong,
    Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim
    等人。Seamless：多语言表达式和流式语音翻译。arXiv 预印本 arXiv:2312.05187，2023年。'
- en: '[8] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford,
    Katie Millican, George van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc,
    Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, T. W.
    Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock,
    Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan,
    Jack W. Rae, Erich Elsen, and L. Sifre. Improving language models by retrieving
    from trillions of tokens. In International Conference on Machine Learning, 2021.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Sebastian Borgeaud、Arthur Mensch、Jordan Hoffmann、Trevor Cai、Eliza Rutherford、Katie
    Millican、George van den Driessche、Jean-Baptiste Lespiau、Bogdan Damoc、Aidan Clark、Diego
    de Las Casas、Aurelia Guy、Jacob Menick、Roman Ring、T. W. Hennigan、Saffron Huang、Lorenzo
    Maggiore、Chris Jones、Albin Cassirer、Andy Brock、Michela Paganini、Geoffrey Irving、Oriol
    Vinyals、Simon Osindero、Karen Simonyan、Jack W. Rae、Erich Elsen 和 L. Sifre. 通过从万亿个标记中检索信息来改进语言模型。在国际机器学习大会，2021年。'
- en: '[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,
    Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen,
    Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher
    Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
    models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan,
    and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
    pages 1877–1901\. Curran Associates, Inc., 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell、Sandhini
    Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon Child、Aditya Ramesh、Daniel
    Ziegler、Jeffrey Wu、Clemens Winter、Chris Hesse、Mark Chen、Eric Sigler、Mateusz Litwin、Scott
    Gray、Benjamin Chess、Jack Clark、Christopher Berner、Sam McCandlish、Alec Radford、Ilya
    Sutskever 和 Dario Amodei. 语言模型是少样本学习者。在 H. Larochelle、M. Ranzato、R. Hadsell、M.F.
    Balcan 和 H. Lin 主编的《神经信息处理系统进展》第33卷，第1877-1901页。Curran Associates, Inc.，2020年。'
- en: '[10] Agnieszka Chmiel. Effects of simultaneous interpreting experience and
    training on anticipation, as measured by word-translation latencies. Interpreting,
    23(1):18–44, 2021.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Agnieszka Chmiel. 同声传译经验和训练对预判的影响，通过词汇翻译延迟测量。Interpreting，23(1)：18-44，2021年。'
- en: '[11] Kyunghyun Cho and Masha Esipova. Can neural machine translation do simultaneous
    translation? arXiv preprint arXiv:1606.02012, 2016.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Kyunghyun Cho 和 Masha Esipova. 神经机器翻译能做同声传译吗？arXiv 预印本 arXiv:1606.02012，2016年。'
- en: '[12] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan,
    Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding
    via unified large-scale audio-language models. arXiv preprint arXiv:2311.07919,
    2023.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Yunfei Chu、Jin Xu、Xiaohuan Zhou、Qian Yang、Shiliang Zhang、Zhijie Yan、Chang
    Zhou 和 Jingren Zhou. Qwen-audio: 通过统一的大规模音频语言模型推动通用音频理解。arXiv 预印本 arXiv:2311.07919，2023年。'
- en: '[13] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan,
    Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal audio understanding
    via unified large-scale audio-language models, 2023.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Yunfei Chu、Jin Xu、Xiaohuan Zhou、Qian Yang、Shiliang Zhang、Zhijie Yan、Chang
    Zhou 和 Jingren Zhou. Qwen-audio: 通过统一的大规模音频语言模型推动通用音频理解，2023年。'
- en: '[14] Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth
    Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. Fleurs: Few-shot learning
    evaluation of universal representations of speech. In 2022 IEEE Spoken Language
    Technology Workshop (SLT), pages 798–805\. IEEE, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Alexis Conneau、Min Ma、Simran Khanuja、Yu Zhang、Vera Axelrod、Siddharth Dalmia、Jason
    Riesa、Clara Rivera 和 Ankur Bapna. Fleurs：语音通用表示的少样本学习评估。在2022年IEEE语音语言技术研讨会（SLT），第798-805页。IEEE，2023年。'
- en: '[15] Mattia A Di Gangi, Roldano Cattoni, Luisa Bentivogli, Matteo Negri, and
    Marco Turchi. Must-c: a multilingual speech translation corpus. In Proceedings
    of the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages
    2012–2017. Association for Computational Linguistics, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Mattia A Di Gangi、Roldano Cattoni、Luisa Bentivogli、Matteo Negri 和 Marco
    Turchi. Must-c：多语言语音翻译语料库。在2019年北美计算语言学会会议：人类语言技术，第1卷（长篇与短篇论文），第2012-2017页。计算语言学会，2019年。'
- en: '[16] Qian Dong, Yaoming Zhu, Mingxuan Wang, and Lei Li. Learning when to translate
    for streaming speech. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio,
    editors, Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 680–694, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16]  Qian Dong, Yaoming Zhu, Mingxuan Wang, 和 Lei Li。学习何时翻译流媒体语音。在Smaranda
    Muresan, Preslav Nakov, 和 Aline Villavicencio编辑的《计算语言学协会第60届年会论文集（第1卷：长篇论文）》中，页面680–694，爱尔兰都柏林，2022年5月。计算语言学协会出版。'
- en: '[17] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.
    Clap learning audio concepts from natural language supervision. ICASSP 2023 -
    2023 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP), 2023.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17]  Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, 和 Huaming Wang。Clap学习：通过自然语言监督学习音频概念。ICASSP
    2023 - 2023 IEEE国际声学、语音和信号处理会议，2023年。'
- en: '[18] Marcello Federico, Alex Waibel, Marta R. Costa-jussà, Jan Niehues, Sebastian
    Stuker, and Elizabeth Salesky, editors. Proceedings of the 18th International
    Conference on Spoken Language Translation (IWSLT 2021), Bangkok, Thailand (online),
    August 2021. Association for Computational Linguistics.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18]  Marcello Federico, Alex Waibel, Marta R. Costa-jussà, Jan Niehues, Sebastian
    Stuker, 和 Elizabeth Salesky，编辑。第18届国际口语语言翻译会议（IWSLT 2021）论文集，泰国曼谷（在线），2021年8月。计算语言学协会出版。'
- en: '[19] Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura, Hermann
    Ney, Jan Niehues, Sebastian Stüker, Dekai Wu, Joseph Mariani, and Francois Yvon,
    editors. Proceedings of the 17th International Conference on Spoken Language Translation,
    Online, July 2020\. Association for Computational Linguistics.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19]  Marcello Federico, Alex Waibel, Kevin Knight, Satoshi Nakamura, Hermann
    Ney, Jan Niehues, Sebastian Stüker, Dekai Wu, Joseph Mariani, 和 Francois Yvon，编辑。第17届国际口语语言翻译会议论文集，在线会议，2020年7月。计算语言学协会出版。'
- en: '[20] Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen
    Zhang, and Hang Li. Agile: A novel framework of llm agents, 2024.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20]  Peiyuan Feng, Yichen He, Guanhua Huang, Yuan Lin, Hanchong Zhang, Yuchen
    Zhang, 和 Hang Li。Agile：一种新型的LLM代理框架，2024年。'
- en: '[21] Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan
    Liu, and Nigel Collier. Decoder-only or encoder-decoder? interpreting language
    model as a regularized encoder-decoder, 2023.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21]  Zihao Fu, Wai Lam, Qian Yu, Anthony Man-Cho So, Shengding Hu, Zhiyuan
    Liu, 和 Nigel Collier。仅解码器还是编码-解码器？将语言模型解释为正则化的编码-解码器，2023年。'
- en: '[22] Ryo Fukuda, Yuta Nishikawa, Yasumasa Kano, Yuka Ko, Tomoya Yanagita, Kosuke
    Doi, Mana Makinae, Sakriani Sakti, Katsuhito Sudoh, and Satoshi Nakamura. NAIST
    simultaneous speech-to-speech translation system for IWSLT 2023. In Elizabeth
    Salesky, Marcello Federico, and Marine Carpuat, editors, Proceedings of the 20th
    International Conference on Spoken Language Translation (IWSLT 2023), pages 330–340,
    Toronto, Canada (in-person and online), July 2023\. Association for Computational
    Linguistics.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22]  Ryo Fukuda, Yuta Nishikawa, Yasumasa Kano, Yuka Ko, Tomoya Yanagita,
    Kosuke Doi, Mana Makinae, Sakriani Sakti, Katsuhito Sudoh, 和 Satoshi Nakamura。NAIST
    2023年IWSLT同时语音到语音翻译系统。在Elizabeth Salesky, Marcello Federico, 和 Marine Carpuat编辑的《第20届国际口语语言翻译会议（IWSLT
    2023）》论文集中，页面330–340，加拿大多伦多（线上与线下），2023年7月。计算语言学协会出版。'
- en: '[23] Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli. Speech translation
    with speech foundation models and large language models: What is there and what
    is missing? arXiv preprint arXiv:2402.12025, 2024.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23]  Marco Gaido, Sara Papi, Matteo Negri, 和 Luisa Bentivogli。使用语音基础模型和大型语言模型进行语音翻译：现有的和缺失的是什么？arXiv预印本arXiv:2402.12025，2024年。'
- en: '[24] Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li. Learning to
    translate in real-time with neural machine translation. In Proceedings of the
    15th Conference of the European Chapter of the Association for Computational Linguistics:
    Volume 1, Long Papers, pages 1053–1062, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24]  Jiatao Gu, Graham Neubig, Kyunghyun Cho, 和 Victor OK Li。使用神经机器翻译实时学习翻译。在《计算语言学协会欧洲分会第15届会议论文集：第一卷，长篇论文》中，页面1053–1062，2017年。'
- en: '[25] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui
    Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented
    transformer for speech recognition. arXiv preprint arXiv:2005.08100, 2020.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25]  Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui
    Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu等人。Conformer：用于语音识别的卷积增强型变换器。arXiv预印本arXiv:2005.08100，2020年。'
- en: '[26] Ewa Gumul and Andrzej Łyda. The time constraint in conference interpreting:
    Simultaneous vs. consecutive. Research in language, 5:165–183, 2007.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26]  Ewa Gumul 和 Andrzej Łyda。会议口译中的时间限制：同声传译与交替传译。语言研究，5:165–183，2007年。'
- en: '[27] Jiaxin Guo, Daimeng Wei, Zhanglin Wu, Zongyao Li, Zhiqiang Rao, Minghan
    Wang, Hengchao Shang, Xiaoyu Chen, Zhengzhe Yu, Shaojun Li, Yuhao Xie, Lizhi Lei,
    and Hao Yang. The HW-TSC’s simultaneous speech-to-text translation system for
    IWSLT 2023 evaluation. In Elizabeth Salesky, Marcello Federico, and Marine Carpuat,
    editors, Proceedings of the 20th International Conference on Spoken Language Translation
    (IWSLT 2023), pages 376–382, Toronto, Canada (in-person and online), July 2023\.
    Association for Computational Linguistics.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 郭家鑫, 魏岱梦, 吴张琳, 李宗耀, 饶志强, 王铭瀚, 尚恒超, 陈晓宇, 余正哲, 李绍军, 谢宇豪, 雷力智, 杨浩. HW-TSC的IWSLT
    2023评估同时语音转文本翻译系统. 见 Elizabeth Salesky, Marcello Federico, 和 Marine Carpuat 主编,
    《第20届国际口语语言翻译大会论文集 (IWSLT 2023)》，第376–382页, 加拿大多伦多（线上与线下）, 2023年7月\. 计算语言学协会.'
- en: '[28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan
    Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation
    learning by masked prediction of hidden units. IEEE/ACM transactions on audio,
    speech, and language processing, 29:3451–3460, 2021.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 徐伟宁, 本杰明·博尔特, 蔡耀宏, Kushal Lakhotia, Ruslan Salakhutdinov, 和 Abdelrahman
    Mohamed. Hubert: 通过隐藏单元的掩蔽预测进行自监督语音表示学习. 《IEEE/ACM音频、语音与语言处理学报》, 29:3451–3460,
    2021.'
- en: '[29] Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai
    Chen, and Eng Siong Chng. Gentranslate: Large language models are generative multilingual
    speech and machine translators. arXiv preprint arXiv:2402.06894, 2024.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 胡宇晨, 陈晨, 杨超汉, 李瑞哲, 张东, 陈哲怀, 张英松. Gentranslate: 大型语言模型是生成型多语言语音和机器翻译系统.
    arXiv预印本 arXiv:2402.06894, 2024.'
- en: '[30] Zhichao Huang, Rong Ye, Tom Ko, Qianqian Dong, Shanbo Cheng, Mingxuan
    Wang, and Hang Li. Speech translation with large language models: An industrial
    practice, 2023.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 黄志超, 叶荣, Tom Ko, 董倩倩, 程山波, 王铭轩, 李杭. 使用大型语言模型的语音翻译：一项工业实践, 2023.'
- en: '[31] Javier Iranzo-Sánchez, Javier Jorge, Pau Baquero-Arnal, Joan Albert Silvestre-Cerdà,
    Adrià Giménez, Jorge Civera, Albert Sanchis, and Alfons Juan. Streaming cascade-based
    speech translation leveraged by a direct segmentation model. Neural Networks,
    142:303–315, 2021.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Javier Iranzo-Sánchez, Javier Jorge, Pau Baquero-Arnal, Joan Albert Silvestre-Cerdà,
    Adrià Giménez, Jorge Civera, Albert Sanchis, 和 Alfons Juan. 通过直接分割模型优化的流式级联语音翻译.
    《神经网络》, 142:303–315, 2021.'
- en: '[32] Dávid Javorský, Dominik Macháček, and Ondřej Bojar. Continuous rating
    as reliable human evaluation of simultaneous speech translation. In Philipp Koehn,
    Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee, Marta R. Costa-jussà,
    Christian Federmann, Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham,
    Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,
    Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki Nagata, Toshiaki
    Nakazawa, Matteo Negri, Aurélie Névéol, Mariana Neves, Martin Popel, Marco Turchi,
    and Marcos Zampieri, editors, Proceedings of the Seventh Conference on Machine
    Translation (WMT), pages 154–164, Abu Dhabi, United Arab Emirates (Hybrid), December
    2022. Association for Computational Linguistics.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Dávid Javorský, Dominik Macháček, 和 Ondřej Bojar. 连续评分作为同声语音翻译的可靠人工评估.
    见 Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi Bougares, Rajen Chatterjee,
    Marta R. Costa-jussà, Christian Federmann, Mark Fishel, Alexander Fraser, Markus
    Freitag, Yvette Graham, Roman Grundkiewicz, Paco Guzman, Barry Haddow, Matthias
    Huck, Antonio Jimeno Yepes, Tom Kocmi, André Martins, Makoto Morishita, Christof
    Monz, Masaaki Nagata, Toshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana
    Neves, Martin Popel, Marco Turchi, 和 Marcos Zampieri 主编, 《第七届机器翻译大会论文集(WMT)》，第154–164页,
    阿布扎比, 阿联酋（混合形式）, 2022年12月. 计算语言学协会.'
- en: '[33] Roderick Jones. Conference interpreting explained. Routledge, 2014.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Roderick Jones. 《会议口译解读》. Routledge, 2014.'
- en: '[34] Jari Kolehmainen, Aditya Gourav, Prashanth Gurunath Shivakumar, Yile Gu,
    Ankur Gandhe, Ariya Rastrow, Grant P. Strimel, and Ivan Bulyko. Multi-modal retrieval
    for large language model based speech recognition. 2024.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Jari Kolehmainen, Aditya Gourav, Prashanth Gurunath Shivakumar, Yile Gu,
    Ankur Gandhe, Ariya Rastrow, Grant P. Strimel, 和 Ivan Bulyko. 基于大型语言模型的多模态检索与语音识别.
    2024.'
- en: '[35] Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura. Transllama: Llm-based
    simultaneous translation system, 2024.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 罗曼·科什金, 铃木胜人, 中村聪. Transllama: 基于大型语言模型的同声翻译系统, 2024.'
- en: '[36] Jiahuan Li, Shanbo Cheng, Shujian Huang, and Jiajun Chen. Mt-patcher:
    Selective and extendable knowledge distillation from large language models for
    machine translation, 2024.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 李家欢, 程山波, 黄树剑, 陈家俊. Mt-patcher: 从大型语言模型进行选择性与可扩展的知识蒸馏以支持机器翻译, 2024.'
- en: '[37] Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, and Jiajun Chen. Eliciting
    the translation ability of large language models via multilingual finetuning with
    translation instructions. Transactions of the Association for Computational Linguistics,
    12:576–592, 2024.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Jiahuan Li, Hao Zhou, Shujian Huang, Shanbo Cheng, 和 Jiajun Chen. 通过多语言微调和翻译指令引导大型语言模型的翻译能力。《计算语言学会会刊》，12:576–592，2024。'
- en: '[38] Xiaoqing Li, Jinghui Yan, Jiajun Zhang, and Chengqing Zong. Neural name
    translation improves neural machine translation. In Machine Translation: 14th
    China Workshop, CWMT 2018, Wuyishan, China, October 25-26, 2018, Proceedings 14,
    pages 93–100. Springer, 2019.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Xiaoqing Li, Jinghui Yan, Jiajun Zhang, 和 Chengqing Zong. 神经姓名翻译改进神经机器翻译。在《机器翻译：第14届中国研讨会CWMT
    2018》，中国武夷山，2018年10月25-26日，论文集14，第93–100页。施普林格，2019年。'
- en: '[39] Yuang Li, Chang Su, Ming Zhu, Mengyao Piao, Xinglin Lyu, Min Zhang, and
    Hao Yang. HW-TSC 2023 submission for the quality estimation shared task. In Philipp
    Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, Proceedings of the
    Eighth Conference on Machine Translation, pages 835–840, Singapore, December 2023\.
    Association for Computational Linguistics.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Yuang Li, Chang Su, Ming Zhu, Mengyao Piao, Xinglin Lyu, Min Zhang, 和
    Hao Yang. HW-TSC 2023提交的质量估计共享任务。在Philipp Koehn, Barry Haddow, Tom Kocmi, 和 Christof
    Monz编辑的《第八届机器翻译会议论文集》，第835–840页，新加坡，2023年12月。计算语言学协会。'
- en: '[40] Danni Liu, Gerasimos Spanakis, and Jan Niehues. Low-latency sequence-to-sequence
    speech recognition and translation by partial hypothesis selection, 2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Danni Liu, Gerasimos Spanakis, 和 Jan Niehues. 通过部分假设选择实现低延迟的序列到序列语音识别与翻译，2020。'
- en: '[41] Xiaoqian Liu, Guoqiang Hu, Yangfan Du, Erfeng He, YingFeng Luo, Chen Xu,
    Tong Xiao, and Jingbo Zhu. Recent advances in end-to-end simultaneous speech translation,
    2024.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Xiaoqian Liu, Guoqiang Hu, Yangfan Du, Erfeng He, YingFeng Luo, Chen Xu,
    Tong Xiao, 和 Jingbo Zhu. 端到端同步语音翻译的最新进展，2024。'
- en: '[42] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi,
    and Bryan Catanzaro. Chatqa: Surpassing gpt-4 on conversational qa and rag, 2024.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi,
    和 Bryan Catanzaro. Chatqa：在对话式QA和RAG任务中超越GPT-4，2024。'
- en: '[43] Arle Lommel, Hans Uszkoreit, and Aljoscha Burchardt. Multidimensional
    quality metrics (mqm): A framework for declaring and describing translation quality
    metrics. Tradumàtica, (12):0455–463, 2014.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Arle Lommel, Hans Uszkoreit, 和 Aljoscha Burchardt. 多维质量指标（mqm）：声明和描述翻译质量指标的框架。《Tradumàtica》，(12):0455–463，2014。'
- en: '[44] Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, and Wojciech
    Zaremba. Addressing the rare word problem in neural machine translation. In Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers), pages 11–19, 2015.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Minh-Thang Luong, Ilya Sutskever, Quoc Le, Oriol Vinyals, 和 Wojciech Zaremba.
    解决神经机器翻译中的稀有词问题。在《计算语言学协会第53届年会暨第7届国际联合自然语言处理会议（第一卷：长篇论文）》论文集，第11–19页，2015年。'
- en: '[45] Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng,
    Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and Haifeng Wang.
    STACL: Simultaneous translation with implicit anticipation and controllable latency
    using prefix-to-prefix framework. In Anna Korhonen, David Traum, and Lluís Màrquez,
    editors, Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 3025–3036, Florence, Italy, July 2019. Association for Computational
    Linguistics.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng,
    Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, 和 Haifeng Wang. STACL：使用前缀到前缀框架进行隐式预测和可控延迟的同步翻译。在Anna
    Korhonen, David Traum, 和 Lluís Màrquez编辑的《第57届计算语言学协会年会论文集》，第3025–3036页，意大利佛罗伦萨，2019年7月。计算语言学协会。'
- en: '[46] Xutai Ma, Juan Pino, and Philipp Koehn. SimulMT to SimulST: Adapting simultaneous
    text translation to end-to-end simultaneous speech translation. In Kam-Fai Wong,
    Kevin Knight, and Hua Wu, editors, Proceedings of the 1st Conference of the Asia-Pacific
    Chapter of the Association for Computational Linguistics and the 10th International
    Joint Conference on Natural Language Processing, pages 582–587, Suzhou, China,
    December 2020. Association for Computational Linguistics.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Xutai Ma, Juan Pino, 和 Philipp Koehn. SimulMT到SimulST：将同步文本翻译适应端到端同步语音翻译。在Kam-Fai
    Wong, Kevin Knight, 和 Hua Wu编辑的《亚太地区计算语言学协会第1届会议暨第10届国际联合自然语言处理会议论文集》，第582–587页，中国苏州，2020年12月。计算语言学协会。'
- en: '[47] Dominik Macháček, Ondřej Bojar, and Raj Dabre. Mt metrics correlate with
    human ratings of simultaneous speech translation. In Proceedings of the 20th International
    Conference on Spoken Language Translation (IWSLT 2023), pages 169–179, 2023.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Dominik Macháček, Ondřej Bojar 和 Raj Dabre。机器翻译指标与同声翻译的人工评分相关。在《第20届国际口语语言翻译会议论文集（IWSLT
    2023）》中，第169–179页，2023年。'
- en: '[48] Benjamin Marie, Atsushi Fujita, and Raphael Rubino. Scientific credibility
    of machine translation research: A meta-evaluation of 769 papers. In Chengqing
    Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th
    Annual Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages
    7297–7306, Online, August 2021\. Association for Computational Linguistics.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Benjamin Marie, Atsushi Fujita 和 Raphael Rubino。机器翻译研究的科学可信度：769篇论文的元评估。在Chengqing
    Zong, Fei Xia, Wenjie Li 和 Roberto Navigli 编辑的《计算语言学协会第59届年会论文集暨第11届国际自然语言处理联合会议论文集（第1卷：长篇论文）》中，第7297–7306页，在线，2021年8月。计算语言学协会出版。'
- en: '[49] Zoe Moores. The nerle model–a tool for assessing the quality of intralingual
    subtitles at live events. Universal Access in the Information Society, 23(2):589–607,
    2024.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Zoe Moores。nerle模型——一种评估现场活动中语言内字幕质量的工具。《信息社会中的普遍访问》23(2):589–607，2024年。'
- en: '[50] Ha Nguyen, Yannick Estève, and Laurent Besacier. An empirical study of
    end-to-end simultaneous speech translation decoding strategies, 2021.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Ha Nguyen, Yannick Estève 和 Laurent Besacier。端到端同声翻译解码策略的实证研究，2021年。'
- en: '[51] Sara Papi, Marco Gaido, and Matteo Negri. Direct models for simultaneous
    translation and automatic subtitling: FBK@IWSLT2023. In Elizabeth Salesky, Marcello
    Federico, and Marine Carpuat, editors, Proceedings of the 20th International Conference
    on Spoken Language Translation (IWSLT 2023), pages 159–168, Toronto, Canada (in-person
    and online), July 2023\. Association for Computational Linguistics.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Sara Papi, Marco Gaido 和 Matteo Negri。用于同声翻译和自动字幕的直接模型：FBK@IWSLT2023。在Elizabeth
    Salesky, Marcello Federico 和 Marine Carpuat 编辑的《第20届国际口语语言翻译会议论文集（IWSLT 2023）》中，第159–168页，加拿大多伦多（线上与线下），2023年7月。计算语言学协会出版。'
- en: '[52] Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi. Over-generation
    cannot be rewarded: Length-adaptive average lagging for simultaneous speech translation.
    arXiv preprint arXiv:2206.05807, 2022.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Sara Papi, Marco Gaido, Matteo Negri 和 Marco Turchi。过度生成不能被奖励：同声翻译的长度自适应平均滞后。在arXiv预印本arXiv:2206.05807，2022年。'
- en: '[53] Sara Papi, Matteo Negri, and Marco Turchi. Attention as a guide for simultaneous
    speech translation. arXiv preprint arXiv:2212.07850, 2022.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Sara Papi, Matteo Negri 和 Marco Turchi。注意力作为同声翻译的指导。arXiv 预印本 arXiv:2212.07850，2022。'
- en: '[54] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method
    for automatic evaluation of machine translation. In Proceedings of the 40th annual
    meeting of the Association for Computational Linguistics, pages 311–318, 2002.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Kishore Papineni, Salim Roukos, Todd Ward 和 Wei-Jing Zhu。Bleu：一种自动评估机器翻译的方法。在《计算语言学协会第40届年会论文集》中，第311–318页，2002年。'
- en: '[55] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
    Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate
    representations, 2024.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine
    Bosselut, Robert West 和 Boi Faltings。Refiner：中间表示的推理反馈，2024年。'
- en: '[56] Peter Polák, Danni Liu, Ngoc-Quan Pham, Jan Niehues, Alexander Waibel,
    and Ondřej Bojar. Towards efficient simultaneous speech translation: CUNI-KIT
    system for simultaneous track at IWSLT 2023. In Elizabeth Salesky, Marcello Federico,
    and Marine Carpuat, editors, Proceedings of the 20th International Conference
    on Spoken Language Translation (IWSLT 2023), pages 389–396, Toronto, Canada (in-person
    and online), July 2023\. Association for Computational Linguistics.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Peter Polák, Danni Liu, Ngoc-Quan Pham, Jan Niehues, Alexander Waibel
    和 Ondřej Bojar。朝着高效的同声翻译迈进：CUNI-KIT系统在IWSLT 2023同声翻译赛道的表现。在Elizabeth Salesky,
    Marcello Federico 和 Marine Carpuat 编辑的《第20届国际口语语言翻译会议论文集（IWSLT 2023）》中，第389–396页，加拿大多伦多（线上与线下），2023年7月。计算语言学协会出版。'
- en: '[57] Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, and Ondřej Bojar.
    Incremental blockwise beam search for simultaneous speech translation with controllable
    quality-latency tradeoff. In INTERSPEECH 2023, interspeech_2023\. ISCA, August
    2023.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel 和 Ondřej Bojar。增量式分块束搜索用于具有可控质量-延迟权衡的同声翻译。在INTERSPEECH
    2023中，interspeech_2023。ISCA，2023年8月。'
- en: '[58] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings
    of the Third Conference on Machine Translation: Research Papers, pages 186–191,
    Belgium, Brussels, October 2018. Association for Computational Linguistics.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Matt Post. 呼吁在报告BLEU分数时保持清晰. 载于《第三届机器翻译会议：研究论文集》，第186–191页，比利时布鲁塞尔，2018年10月。计算语言学协会。'
- en: '[59] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning
    from scratch for qa via self-planning, 2024.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
    Yuchen Eleanor Jiang, Chengfei Lv, 和 Huajun Chen. Autoact: 通过自我规划从头开始自动学习代理进行QA，2024年。'
- en: '[60] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey,
    and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    ArXiv, abs/2212.04356, 2022.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey,
    和 Ilya Sutskever. 通过大规模弱监督实现鲁棒的语音识别. ArXiv，abs/2212.04356，2022年。'
- en: '[61] Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. Comet: A neural
    framework for mt evaluation. In Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP), pages 2685–2702, 2020.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Ricardo Rei, Craig Stewart, Ana C Farinha, 和 Alon Lavie. Comet: 用于机器翻译评估的神经框架.
    载于《2020年自然语言处理经验方法会议论文集》(EMNLP)，第2685–2702页，2020年。'
- en: '[62] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy
    Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,
    Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across
    millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy
    Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat,
    Julian Schrittwieser 等. Gemini 1.5: 解锁跨越数百万标记上下文的多模态理解. ArXiv预印本 arXiv:2403.05530，2024年。'
- en: '[63] Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan
    Liu. Simulspeech: End-to-end simultaneous speech to text translation. In Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, pages
    3787–3796, 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Yi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, 和 Tie-Yan
    Liu. Simulspeech: 端到端同时语音转文本翻译. 载于《第58届计算语言学协会年会论文集》，第3787–3796页，2020年。'
- en: '[64] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. Tptu: Large language
    model-based ai agents for task planning and tool usage, 2023.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing
    Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, 和 Rui Zhao. Tptu: 基于大型语言模型的AI代理用于任务规划和工具使用，2023年。'
- en: '[65] Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors. Proceedings
    of the 20th International Conference on Spoken Language Translation (IWSLT 2023),
    Toronto, Canada (in-person and online), July 2023\. Association for Computational
    Linguistics.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Elizabeth Salesky, Marcello Federico, 和 Marine Carpuat, 编辑. 《第20届国际语音语言翻译大会论文集》(IWSLT
    2023)，加拿大多伦多（线上和线下），2023年7月。计算语言学协会。'
- en: '[66] Elizabeth Salesky, Marcello Federico, and Marta Costa-jussà, editors.
    Proceedings of the 19th International Conference on Spoken Language Translation
    (IWSLT 2022), Dublin, Ireland (in-person and online), May 2022\. Association for
    Computational Linguistics.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Elizabeth Salesky, Marcello Federico, 和 Marta Costa-jussà, 编辑. 《第19届国际语音语言翻译大会论文集》(IWSLT
    2022)，爱尔兰都柏林（线上和线下），2022年5月。计算语言学协会。'
- en: '[67] Frank Seide, Morrie Doulaty, Yangyang Shi, Yashesh Gaur, Junteng Jia,
    and Chunyang Wu. Speech reallm – real-time streaming speech recognition with multimodal
    llms by teaching the flow of time, 2024.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Frank Seide, Morrie Doulaty, Yangyang Shi, Yashesh Gaur, Junteng Jia,
    和 Chunyang Wu. Speech reallm – 通过教授时间流动进行多模态LLMs的实时流式语音识别，2024年。'
- en: '[68] Thibault Sellam, Dipanjan Das, and Ankur Parikh. Bleurt: Learning robust
    metrics for text generation. In Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, pages 7881–7892, 2020.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Thibault Sellam, Dipanjan Das, 和 Ankur Parikh. Bleurt: 为文本生成学习鲁棒的度量. 载于《第58届计算语言学协会年会论文集》，第7881–7892页，2020年。'
- en: '[69] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning, 2023.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik
    Narasimhan, 和 Shunyu Yao. Reflexion: 语言代理与语言强化学习，2023年。'
- en: '[70] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li,
    Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing abilities for
    large language models, 2024.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li,
    Lu Lu, Zejun Ma, 和 Chao Zhang. Salmonn: 面向大型语言模型的通用听力能力，2024年。'
- en: '[71] NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
    Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
    Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
    Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley
    Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
    Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj
    Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
    Safiyyah Saleem, Holger Schwenk, and Jeff Wang. No language left behind: Scaling
    human-centered machine translation, 2022.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] NLLB 团队，Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
    Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
    Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
    Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley
    Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
    Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj
    Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
    Safiyyah Saleem, Holger Schwenk 和 Jeff Wang。无语言落下：扩展以人为本的机器翻译，2022年。'
- en: '[72] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale
    等人。Llama 2：开放基础和微调的聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    Advances in neural information processing systems, 30, 2017.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin。注意力即一切。神经信息处理系统进展，30，2017年。'
- en: '[74] Changhan Wang, Juan Pino, Anne Wu, and Jiatao Gu. CoVoST: A diverse multilingual
    speech-to-text translation corpus. In Proceedings of The 12th Language Resources
    and Evaluation Conference, pages 4197–4203, Marseille, France, May 2020\. European
    Language Resources Association.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Changhan Wang, Juan Pino, Anne Wu 和 Jiatao Gu。CoVoST：一个多样化的多语言语音到文本翻译语料库。在《第12届语言资源与评估会议论文集》，4197–4203页，法国马赛，2020年5月。欧洲语言资源协会。'
- en: '[75] Changhan Wang, Anne Wu, and Juan Pino. Covost 2: A massively multilingual
    speech-to-text translation corpus, 2020.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Changhan Wang, Anne Wu 和 Juan Pino。Covost 2：一个大规模多语言语音到文本翻译语料库，2020年。'
- en: '[76] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large language
    model based autonomous agents. Frontiers of Computer Science, 18(6), 2024.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,
    Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等人。基于大规模语言模型的自主智能体综述。计算机科学前沿，18(6)，2024。'
- en: '[77] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming
    Shi, and Zhaopeng Tu. Document-level machine translation with large language models.
    In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing, pages 16646–16661,
    Singapore, December 2023\. Association for Computational Linguistics.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming
    Shi 和 Zhaopeng Tu。基于大规模语言模型的文档级机器翻译。在 Houda Bouamor、Juan Pino 和 Kalika Bali 编辑的《2023年自然语言处理经验方法大会论文集》，16646–16661页，新加坡，2023年12月。计算语言学协会。'
- en: '[78] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze
    Li, Lin Shi, Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation
    systems for wmt17. In Proceedings of the Second Conference on Machine Translation,
    pages 410–415, 2017.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze
    Li, Lin Shi, Yanfeng Wang 和 Hongtao Yang。搜狗神经机器翻译系统用于 WMT17。第二届机器翻译大会论文集，410–415页，2017。'
- en: '[79] Shira Wein, I Te, Colin Cherry, Juraj Juraska, Dirk Padfield, and Wolfgang
    Macherey. Barriers to effective evaluation of simultaneous interpretation. In
    Findings of the Association for Computational Linguistics: EACL 2024, pages 209–219,
    2024.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Shira Wein, I Te, Colin Cherry, Juraj Juraska, Dirk Padfield 和 Wolfgang
    Macherey。有效评估同声传译的障碍。在计算语言学协会发现：EACL 2024，209–219页，2024年。'
- en: '[80] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip:
    Learning robust audio representations from clip. ICASSP 2022 - 2022 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar 和 Juan Pablo Bello。Wav2clip：从
    Clip 学习鲁棒的音频表示。ICASSP 2022 - 2022 IEEE 国际声学、语音和信号处理会议（ICASSP），2021年。'
- en: '[81] Shao-Chuan Wu. Assessing simultaneous interpreting. A study on test reliability
    and Examiners’ assessment behaviour (Doctoral dissertation). Newcastle University,
    Newcastle, 2010.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] 吴绍川。评估同声传译。一项关于测试可靠性和考官评估行为的研究（博士学位论文）。纽卡斯尔大学，纽卡斯尔，2010年。'
- en: '[82] Brian Yan, Jiatong Shi, Soumi Maiti, William Chen, Xinjian Li, Yifan Peng,
    Siddhant Arora, and Shinji Watanabe. CMU’s IWSLT 2023 simultaneous speech translation
    system. In Elizabeth Salesky, Marcello Federico, and Marine Carpuat, editors,
    Proceedings of the 20th International Conference on Spoken Language Translation
    (IWSLT 2023), pages 235–240, Toronto, Canada (in-person and online), July 2023\.
    Association for Computational Linguistics.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] 闫百仁、石佳彤、迈提·苏米、陈威廉、李新建、彭一凡、阿罗拉·席丹特、渡边慎二。CMU IWSLT 2023 同声语音翻译系统。在伊丽莎白·塞尔斯基、马尔切洛·费德里科和玛琳·卡尔普阿特编辑的《第20届国际口语语言翻译会议（IWSLT
    2023）》论文集中，第235-240页，加拿大多伦多（线上与线下同步），2023年7月。计算语言学协会。'
- en: '[83] Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang,
    and Jun Cao. GigaST: A 10,000-hour Pseudo Speech Translation Corpus. In Proc.
    INTERSPEECH 2023, pages 2168–2172, 2023.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] 叶荣、赵成奇、汤姆·科、孟楚彤、王涛、王铭轩、曹俊。GigaST：一个包含10,000小时伪语音翻译语料库的研究。在INTERSPEECH
    2023会议论文集，第2168-2172页，2023年。'
- en: '[84] Xingshan Zeng, Liangyou Li, and Qun Liu. RealTranS: End-to-end simultaneous
    speech translation with convolutional weighted-shrinking transformer. In Chengqing
    Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, Findings of the Association
    for Computational Linguistics: ACL-IJCNLP 2021, pages 2461–2474, Online, August
    2021\. Association for Computational Linguistics.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] 曾兴山、李亮有、刘群。RealTranS：基于卷积加权收缩变压器的端到端同声语音翻译。郑成清、夏飞、李文杰和罗伯托·纳维吉编辑的《计算语言学协会成果：ACL-IJCNLP
    2021》论文集，第2461-2474页，在线，2021年8月。计算语言学协会。'
- en: '[85] Ruiqing Zhang, Xiyang Wang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Zhi
    Li, Haifeng Wang, Ying Chen, and Qinfei Li. BSTC: A large-scale Chinese-English
    speech translation dataset. In Hua Wu, Colin Cherry, Liang Huang, Zhongjun He,
    Qun Liu, Maha Elbayad, Mark Liberman, Haifeng Wang, Mingbo Ma, and Ruiqing Zhang,
    editors, Proceedings of the Second Workshop on Automatic Simultaneous Translation,
    pages 28–35, Online, June 2021\. Association for Computational Linguistics.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] 张瑞青、王熙阳、张传强、何忠军、吴华、李志、王海峰、陈颖、李秦飞。BSTC：大规模中英语语音翻译数据集。在吴华、科林·切里、黄亮、何忠军、刘群、马哈·艾尔巴亚德、马克·利伯曼、王海峰、马铭波和张瑞青编辑的《第二届自动化同声翻译研讨会论文集》上，第28-35页，在线，2021年6月。计算语言学协会。'
- en: '[86] Shaolei Zhang, Qingkai Fang, Shoutao Guo, Zhengrui Ma, Min Zhang, and
    Yang Feng. Streamspeech: Simultaneous speech-to-speech translation with multi-task
    learning. arXiv preprint arXiv:2406.03049, 2024.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] 张少磊、方清凯、郭守涛、马正睿、张敏、冯杨。Streamspeech：通过多任务学习进行同声语音到语音翻译。arXiv 预印本 arXiv:2406.03049，2024年。'
- en: '[87] Shaolei Zhang and Yang Feng. End-to-end simultaneous speech translation
    with differentiable segmentation. In Findings of the Association for Computational
    Linguistics: ACL 2023, pages 7659–7680, 2023.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] 张少磊、冯杨。基于可微分分段的端到端同声语音翻译。在《计算语言学协会成果：ACL 2023》上，第7659-7680页，2023年。'
- en: '[88] Yu Zhang, Wei Han, James Qin, Yongqiang Wang, Ankur Bapna, Zhehuai Chen,
    Nanxin Chen, Bo Li, Vera Axelrod, Gary Wang, et al. Google usm: Scaling automatic
    speech recognition beyond 100 languages. arXiv preprint arXiv:2303.01037, 2023.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] 张宇、韩伟、秦杰明、王永强、安库尔·巴普纳、陈哲怀、陈南新、李博、维拉·阿克塞尔罗德、王嘉瑞等人。谷歌 USM：将自动语音识别扩展到超过 100
    种语言。arXiv 预印本 arXiv:2303.01037，2023年。'
- en: '[89] Chengqi Zhao, Zhicheng Liu, Jian Tong, Tao Wang, Mingxuan Wang, Rong Ye,
    Qianqian Dong, Jun Cao, and Lei Li. The volctrans neural speech translation system
    for iwslt 2021. In Proceedings of the 18th International Conference on Spoken
    Language Translation (IWSLT 2021), pages 64–74, 2021.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] 赵成奇、刘志诚、佟建、王涛、王铭轩、叶荣、董倩倩、曹俊、李雷。Volctrans 神经语音翻译系统用于 IWSLT 2021。在第18届国际口语语言翻译会议（IWSLT
    2021）论文集中，第64-74页，2021年。'
- en: '[90] Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and
    Shikai Wu. Fine-tuning large language models for domain-specific machine translation,
    2024.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] 郑家伟、洪航海、王晓丽、苏景松、梁永贵、吴世凯。针对特定领域机器翻译的预训练大型语言模型微调，2024年。'
- en: '[91] Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng
    Kong, Jiajun Chen, and Lei Li. Multilingual machine translation with large language
    models: Empirical results and analysis. In Findings of the Association for Computational
    Linguistics: NAACL 2024, pages 2765–2781, 2024.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Zhu Wenhao, Liu Hongyi, Dong Qingxiu, Xu Jingjing, Huang Shujian, Kong
    Lingpeng, Chen Jiajun, and Li Lei. 使用大型语言模型进行多语种机器翻译：实证结果与分析。发表于《计算语言学会会议论文集：NAACL
    2024》，第2765–2781页，2024年。'
- en: Appendix A Human Evaluation Guidelines
  id: totrans-385
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 人工评估指南
- en: In this Appendix, we provide detailed human evaluation guidelines which are
    formulated by professional human interpreters.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们提供了由专业人工翻译人员制定的详细人工评估指南。
- en: A.1 Key Indicator
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 关键指标
- en: We define the key indicator as the “Valid Information Proportion,” denoted as
    VIP. This metric measures the proportion of valid semantic fragments within a
    complete speech session. A semantic fragment is deemed valid if it effectively
    conveys the core information, accurately representing the speaker’s original intent.
    Typically, one complete sentence is considered a single semantic fragment. VIP
    assesses the model’s ability to capture and communicate the essence of the spoken
    content.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关键指标定义为“有效信息比例”（Valid Information Proportion，简称VIP）。该指标衡量完整发言中有效语义片段的比例。如果一个语义片段能够有效传达核心信息，准确代表说话者的原意，则该片段被视为有效。通常，一个完整的句子被视为一个语义片段。VIP评估模型捕捉并传达口语内容精髓的能力。
- en: A.2 Evaluation Process
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 评估过程
- en: 'First, the human evaluators segment the long translation result into semantic
    fragments according to the formal rules as follows:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，人工评估者根据以下正式规则将长翻译结果分割成语义片段：
- en: •
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semantic Completeness: Each fragment should contain one complete concept or
    information point. For example, in a conference translation, an ideal semantic
    fragment often corresponds to a full sentence.'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语义完整性：每个片段应包含一个完整的概念或信息点。例如，在会议翻译中，一个理想的语义片段通常对应一个完整的句子。
- en: •
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Natural Language Pauses: Pauses that naturally occur in speech often indicate
    the boundaries of semantic fragments. During segmentation, natural pauses should
    be extensively considered and avoid irrational interruptions. Also, punctuation
    and conjunction in the text should be considered to maintain integrity and clarity
    of information.'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自然语言停顿：口语中自然发生的停顿通常指示语义片段的边界。在分段时，应充分考虑自然停顿，避免不合理的中断。同时，应考虑文本中的标点符号和连接词，以保持信息的完整性和清晰度。
- en: •
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Logical Coherence: Each segment should contain information that are logically
    coherent and continuous. Conditional sentences, causative sentences, or both parts
    of antithesis sentences should be kept within the same segment.'
  id: totrans-396
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逻辑连贯性：每个片段应包含逻辑上连贯且连续的信息。条件句、因果句或对比句的两个部分应保持在同一个片段内。
- en: •
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Grammatical Completeness: Each segment should include all necessary grammatical
    components (e.g., subject, verb, object) and have a complete grammatical structure.'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法完整性：每个片段应包含所有必要的语法成分（例如，主语、谓语、宾语），并具有完整的语法结构。
- en: •
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proper Information Density: Each segment should have a moderate amount of information,
    avoiding information overload. It is recommended that each segment not exceed
    50 words.'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合适的信息密度：每个片段应包含适量的信息，避免信息过载。建议每个片段不超过50个单词。
- en: 'After segmentation, the human evaluators follow the instructions below to evaluate
    the validity of the semantic fragments:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在分段后，人工评估者根据以下指导原则评估语义片段的有效性：
- en: •
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Key Information Recognition. Key information refers to the content that can
    constitute core information, including but not limited to proper nouns, keywords,
    terminologies, sentence structures, etc.
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 关键信息识别。关键信息是指能够构成核心信息的内容，包括但不限于专有名词、关键词、术语、句法结构等。
- en: •
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Correctness Assessment. Evaluators assess whether the translation of key information
    is accurate and successful in conveying the correct spoken intentions. Misinterpretations
    of the speaker’s words, inaccuracies in analyzing the context, or erroneous translations
    of specific terms can all contribute to the failure of the assessment.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正确性评估。评估者评估关键信息的翻译是否准确，并成功传达正确的口语意图。对说话者话语的误解、对上下文分析的错误或特定术语的翻译错误都可能导致评估失败。
- en: •
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Expressiveness Assessment. Evaluators assess whether the whole segment is translated
    accurately, comprehensibly, and expressively to humans. Assessing for any vague,
    ambiguous, or misleading statements. This indicator primarily evaluates the clarity,
    fluency, and intuitiveness of the translation, rather than its accuracy. Typically,
    verbosity, complex sentence structures, or challenging grammatical constructions
    that are unnecessary would reduce the expressiveness of the translation, thus
    leading to failure of the assessment.
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表现力评估。评估者评估整段翻译是否准确、易懂且具表现力。评估是否存在模糊、歧义或误导性表述。此指标主要评估翻译的清晰度、流畅性和直观性，而不是其准确性。通常，冗长、复杂的句子结构或不必要的难度语法会降低翻译的表现力，从而导致评估失败。
- en: If the translation fails any of the above assessments, the translation will
    be marked as invalid. After the evaluators assessed all semantic fragments, the
    VIP could be simply calculated as dividing the number of valid semantic fragments
    by the total number of fragments.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 如果翻译未通过上述任何评估，该翻译将被标记为无效。评估者评估所有语义片段后，VIP 可以通过将有效语义片段的数量除以总片段数来简单计算。
- en: We illustrate the evaluation criteria with two examples in Table [10](https://arxiv.org/html/2407.21646v2#A1.T10
    "Table 10 ‣ A.2 Evaluation Process ‣ Appendix A Human Evaluation Guidelines ‣
    Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣
    6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL
    Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency
    ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on
    End-to-end Simultaneous Speech Translation via LLM Agent"). Although these translations
    achieve “high accuracy” in automatic evaluations, we still categorize them as
    invalid. It’s important to note that our standard aims to emulate human interpreters,
    presenting a significant challenge to both human evaluators and translation systems.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过表[10](https://arxiv.org/html/2407.21646v2#A1.T10 "Table 10 ‣ A.2 Evaluation
    Process ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements
    ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation.
    ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance
    ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent")中的两个示例来说明评估标准。尽管这些翻译在自动评估中获得了“高准确率”，我们仍然将它们归类为无效。需要注意的是，我们的标准旨在模拟人工翻译员，这对人工评估者和翻译系统都是一个重要挑战。
- en: '{CJK*}'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '{CJK*}'
- en: UTF8gbsn
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: UTF8gbsn
- en: '| Example 1: Correctness Assessment |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 示例 1：正确性评估 |'
- en: '| --- |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Golden Chinese | 请确保服务器后端的API接口完全遵循RESTful¹架构原则。 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 黄金中文 | 请确保服务器后端的 API 接口完全遵循 RESTful¹ 架构原则。 |'
- en: '| Reference | Please ensure the server backend’s API interface fully complies
    with RESTful¹ architectural principles. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 请确保服务器后端的 API 接口完全遵循 RESTful¹ 架构原则。 |'
- en: '| Translation | Please ensure the server backend’s API interface fully complies
    with restless¹ architectural principles. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 | 请确保服务器后端的 API 接口完全遵循 restless¹ 架构原则。 |'
- en: '| Explanation | Although the sentence-level BLEU score is near 80, the translation
    is still considered invalid, because the keyword "RESTful" is mistranslated. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 说明 | 尽管句子级别的 BLEU 分数接近 80，但翻译仍然被视为无效，因为关键词“RESTful”被误翻译了。 |'
- en: '| Example 2: Expressiveness Assessment |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 示例 2：表现力评估 |'
- en: '| Golden Chinese | 这部分跟资源那边，前端资源这一块搞定了吗？ |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| 黄金中文 | 这部分跟资源那边，前端资源这一块搞定了吗？ |'
- en: '| Reference | Did you arrange the front-end resources well? |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 你安排好前端资源了吗？ |'
- en: '| Translation 1 | Is this part related to the front-end resources? Did you
    finish the front-end resources? |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 1 | 这一部分与前端资源有关吗？你完成前端资源了吗？ |'
- en: '| Translation 2 | Has the front-end resource been settled? |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 2 | 前端资源解决好了吗？ |'
- en: '| Explanation | Translation 1 is redundant, disfluent and contains minor errors,
    thus not easy to understand by human evaluators, while Translation 2 generated
    by CLASI is concise and fluent, and conveys the speaker’s intention appropriately.
    |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 说明 | 翻译 1 冗余、不流畅且包含小错误，因此人类评估者难以理解，而 CLASI 生成的翻译 2 简洁流畅，恰当地传达了说话者的意图。 |'
- en: 'Table 10: Human evaluation examples of Chinese-to-English Translation task.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：中译英翻译任务的人类评估示例。
- en: A.3 Correlation with Automatic Metrics
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 与自动指标的关联
- en: '[Figure 4](https://arxiv.org/html/2407.21646v2#A1.F4 "In A.3 Correlation with
    Automatic Metrics ‣ Appendix A Human Evaluation Guidelines ‣ Authorship and Acknowledgements
    ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation.
    ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance
    ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent") shows the distribution and regression curve for VIP with regard
    to BLEU, BLEURT, and COMET, respectively. From the scatter points in [Figure 4](https://arxiv.org/html/2407.21646v2#A1.F4
    "In A.3 Correlation with Automatic Metrics ‣ Appendix A Human Evaluation Guidelines
    ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work
    ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL
    Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency
    ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on
    End-to-end Simultaneous Speech Translation via LLM Agent") we may observe that
    as VIP score increases, the growth of the automatic metric curves slows down and
    becomes less significant, making it hard to reflect the real changes in translation
    quality. The correlation curves in [Figure 4](https://arxiv.org/html/2407.21646v2#A1.F4
    "In A.3 Correlation with Automatic Metrics ‣ Appendix A Human Evaluation Guidelines
    ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work
    ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL
    Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency
    ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on
    End-to-end Simultaneous Speech Translation via LLM Agent") also demonstrate the
    finding. Here, we calculate Kendall’s Tau correlation coefficient [[1](https://arxiv.org/html/2407.21646v2#bib.bib1)],
    which measures the monotonic correlation between two ordered variables. In low
    VIP ranges, the correlation between VIP and automatic metrics is observable; as
    the score increases, the correlation harshly drops, which indicates a significant
    distortion of the automatic metrics. A possible reason is that the translations
    may differ from the groundtruths by only a few words in the mediocre ranges. However,
    in a real simultaneous interpretation scenario, these words are likely to be keywords
    that play important roles in conveying precise information, which may significantly
    impact VIP scores.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](https://arxiv.org/html/2407.21646v2#A1.F4 "在 A.3 自动评分与人工评分的相关性 ‣ 附录 A
    人类评估指南 ‣ 作者与致谢 ‣ 社会影响 ‣ 限制与未来工作 ‣ 6 结论 ‣ 人类评估 ‣ 5 相关工作 ‣ 4.7 案例研究 ‣ 4.6.2 ICL
    性能 ‣ 4.6 MM-RAG 性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 迈向通过 LLM 代理实现端到端同步语音翻译的人工水平")
    显示了 VIP 与 BLEU、BLEURT 和 COMET 评分的分布和回归曲线。从 [图 4](https://arxiv.org/html/2407.21646v2#A1.F4
    "在 A.3 自动评分与人工评分的相关性 ‣ 附录 A 人类评估指南 ‣ 作者与致谢 ‣ 社会影响 ‣ 限制与未来工作 ‣ 6 结论 ‣ 人类评估 ‣ 5
    相关工作 ‣ 4.7 案例研究 ‣ 4.6.2 ICL 性能 ‣ 4.6 MM-RAG 性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量
    ‣ 4 实验 ‣ 迈向通过 LLM 代理实现端到端同步语音翻译的人工水平") 中的散点，我们可以观察到，随着 VIP 分数的提高，自动评分曲线的增长逐渐放缓，变得不那么显著，难以反映翻译质量的真实变化。
    [图 4](https://arxiv.org/html/2407.21646v2#A1.F4 "在 A.3 自动评分与人工评分的相关性 ‣ 附录 A 人类评估指南
    ‣ 作者与致谢 ‣ 社会影响 ‣ 限制与未来工作 ‣ 6 结论 ‣ 人类评估 ‣ 5 相关工作 ‣ 4.7 案例研究 ‣ 4.6.2 ICL 性能 ‣ 4.6
    MM-RAG 性能 ‣ 4.5 补充实验 ‣ 4.4 延迟 ‣ 4.3 翻译质量 ‣ 4 实验 ‣ 迈向通过 LLM 代理实现端到端同步语音翻译的人工水平")
    中的相关性曲线也证实了这一发现。在这里，我们计算了 Kendall''s Tau 相关系数 [[1](https://arxiv.org/html/2407.21646v2#bib.bib1)]，它衡量了两个有序变量之间的单调相关性。在低
    VIP 范围内，VIP 和自动评分之间的相关性是显而易见的；随着分数的提高，相关性急剧下降，这表明自动评分的失真显著。一个可能的原因是，在中等范围内，翻译与参考答案之间的差异可能只有几个词。然而，在实际的同声传译场景中，这些词可能是关键信息，起到了传递精确信息的重要作用，因此可能会显著影响
    VIP 分数。'
- en: '![Refer to caption](img/97d7624a8e53ca573083e4b281e5a119.png)![Refer to caption](img/41e3cfc204ab8447423da05ab2a01c7a.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/97d7624a8e53ca573083e4b281e5a119.png)![参见说明](img/41e3cfc204ab8447423da05ab2a01c7a.png)'
- en: 'Figure 4: Analysis of VIP vs different automatic metrics on the zh-en direction.
    The distribution and regression curve of the data points for each metric are shown
    in the above-left figure. Line charts for the calculated correlation between VIP
    and Automatic metric within multiple intervals are shown in the right figure.
    Due to the limitation of human labeling capacity, we collect 35 rounds of human
    evaluation results for zh-en direction on our in-house testset.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：VIP与不同自动评估指标在中英文方向上的分析。上图左侧展示了各个指标的数据点分布和回归曲线。右图展示了在多个区间内计算的VIP与自动评估指标的相关性曲线。由于人工标注能力的限制，我们在内部测试集上收集了35轮中英文方向的人类评估结果。
- en: Appendix B Supplementary Materials
  id: totrans-429
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 补充材料
- en: B.1 Supplementary Case Study
  id: totrans-430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 补充案例研究
- en: We provide more case studies in [Table 11](https://arxiv.org/html/2407.21646v2#A2.T11
    "In B.1 Supplementary Case Study ‣ Appendix B Supplementary Materials ‣ Authorship
    and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion
    ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance
    ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation
    Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent"). In terms of informal, disfluent, code-mixing,
    and named-entity translation, CLASI could achieve much better results than the
    commercial products. Benefits from the end-to-end approach, CLASI could also understand
    the original speech tone and generate better translations.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[表11](https://arxiv.org/html/2407.21646v2#A2.T11 "In B.1 Supplementary Case
    Study ‣ Appendix B Supplementary Materials ‣ Authorship and Acknowledgements ‣
    Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion ‣ Human Evaluation.
    ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance ‣ 4.6 MM-RAG Performance
    ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation Quality ‣ 4 Experiments
    ‣ Towards Achieving Human Parity on End-to-end Simultaneous Speech Translation
    via LLM Agent")中提供了更多的案例研究。在非正式、不流畅、代码混合和命名实体翻译方面，CLASI的表现明显优于商业产品。得益于端到端的方法，CLASI还能够理解原始语音的语气，并生成更好的翻译。
- en: '| CASE 1： Informal, disfluent speech translation |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 案例1：非正式、不流畅的语音翻译 |'
- en: '| --- |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Golden Transcription | 那基于这些观察，那我们是不是啊，我，我，我¹，我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？
    |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 那基于这些观察，那我们是不是啊，我，我，我¹，我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？ |'
- en: '| Commerical 4 ASR | 那基于这些观察，那我们是不是啊？我们是不是我？¹我们是不是可以去找一种，就是像gvt3.5²一样的，统一的建模方法？
    |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 ASR | 那基于这些观察，那我们是不是啊？我们是不是我？¹我们是不是可以去找一种，就是像GPT3.5²一样的，统一的建模方法？ |'
- en: '| Commerical 4 Translation | So based on these observations, are we? Are we
    me? Can we go¹, like gvt3.5², and do this unified modeling |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 翻译 | 所以基于这些观察，我们是不是？我们是不是我？我们能去做¹像GPT3.5²一样的统一建模方法吗？ |'
- en: '| CLASI ASR | 那基于这些观察，那我们是不是啊，我，我，我，¹我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？ |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 那基于这些观察，那我们是不是啊，我，我，我，¹我们是不是可以去找一种，就是像GPT3.5一样的，统一的建模方法？ |'
- en: '| CLASI Translation | Based on these observations, can we find¹ a unified modeling
    method like GPT3.5²? |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| CLASI 翻译 | 基于这些观察，我们能否找到¹一种像GPT3.5²一样的统一建模方法？ |'
- en: '| Explanation | The first labeled Chinese phrase (superscript 1) actually means
    "can we find"¹. CLASI can generate a much more fluent, concise translation than
    the Commerical 4\. Besides, for keyword GPT3.5², CLASI can generate correct ASR
    and translation. |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 第一个标注的中文短语（上标1）实际上意味着“我们能找到”¹。CLASI能够生成比商业4更流畅、简洁的翻译。而且对于关键词GPT3.5²，CLASI能够生成正确的ASR和翻译。'
- en: '| CASE 2：Disfluent and code-mixing speech |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| 案例2：不流畅和混合代码的语音 |'
- en: '| Golden Transcription | 我听过一句话叫，pri, pri, prioritization, prioritization¹
    is only real when it hurts。 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 我听过一句话叫，pri, pri, prioritization, prioritization¹ is only real when
    it hurts。 |'
- en: '| Commerical 4 ASR | 我听过一句话叫，Pro, 不管, prioritization, prioritization¹ is only
    real when it hurts. |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 ASR | 我听过一句话叫，Pro, 不管, prioritization, prioritization¹ is only real when
    it hurts。 |'
- en: '| Commerical 4 Translation | I heard a saying called Pro, Anyway, it is Prioritization¹
    is only real when it hurts. |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| 商业4 翻译 | 我听过一句话叫 Pro, 不管, prioritization, prioritization¹ is only real when
    it hurts。 |'
- en: '| CLASI ASR | 我听过一句话叫，priortizaiton, prioritization¹ is only real when it hurts。
    |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 我听过一句话叫，priortizaiton, prioritization¹ is only real when it hurts。
    |'
- en: '| CLASI Translation | I heard a saying that prioritization¹ is only real when
    it hurts. |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| CLASI 翻译 | 我听过一句话叫，prioritization¹ is only real when it hurts。 |'
- en: '| Explanation | The speaker stutters when saying the English sentence, which
    is very common in real-world scenarios. CLASI can fully understand and generate
    the correct English text without any repetition. |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 说话人在说英语句子时有口吃的现象，这是现实世界场景中很常见的。CLASI能够完全理解并生成正确的英文文本，且没有任何重复。 |'
- en: '| CASE 3: Named-entity recognition and translation |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 案例3：命名实体识别与翻译 |'
- en: '| Golden Transcription | 好球！迪亚斯¹的传中，C罗来争抢，这个就是C罗²最喜欢的 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 好球！迪亚斯¹的传中，C罗来争抢，这个就是C罗²最喜欢的 |'
- en: '| Commerical 4 ASR | 好球,比亚斯¹的传统，C罗来争抢这个就这是C罗²最喜欢的 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| 商业版 4 ASR | 好球，比亚斯¹的传统，C罗来争抢这个就这是C罗²最喜欢的 |'
- en: '| Commerical 4 Translation | Nice shot, Bias’¹ traditional C Ronaldo to compete
    for this is C Luo’s² favorite |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 商业版 4 翻译 | 好球，比亚斯¹的传统，C罗来争抢，这就是C罗²最喜欢的 |'
- en: '| CLASI ASR | 好球！迪亚斯¹的传中，C罗来争抢。 这个就这是C罗最喜欢的 |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 好球！迪亚斯¹的传中，C罗来争抢。 这个就这是C罗最喜欢的 |'
- en: '| CLASI Translation | Nice cross by Dias¹, Ronaldo goes for it. This is Ronaldo’s²
    favorite. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| CLASI 翻译 | 迪亚斯¹的漂亮传中，C罗来争抢。这是C罗²最喜欢的。 |'
- en: '| Explanation | Commerical 4 cannot correctly recognize the name of the famous
    football player, Ruben Dias. As for the name of Cristiano Ronaldo, although it
    translates correctly the first time, but fails the second time. CLASI can perform
    perfect recognition and translation. |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | 商业版 4 无法正确识别著名足球运动员鲁本·迪亚斯的名字。至于克里斯蒂亚诺·罗纳尔多的名字，虽然第一次翻译正确，但第二次失败。而 CLASI
    能够完美地识别并翻译。 |'
- en: '| CASE 4: Speech tone understanding |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| CASE 4: 语气理解 |'
- en: '| Golden Transcription | 门前斜传，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 黄金转录 | 门前斜传，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。 |'
- en: '| Commerical 4 ASR | 球没有斜转漂亮，这下球进了白骨龙，这白骨龙 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| Commerical 4 ASR | 球没有斜转漂亮，这下球进了白骨龙，这白骨龙 |'
- en: '| Commerical 4 Translation | The ball didn’t spin beautifully, and now the
    ball went into the bone dragon, the bone dragon |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 商业版 4 翻译 | 球没有斜转漂亮，这下球进了白骨龙，这白骨龙 |'
- en: '| CLASI ASR | 门前斜转，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| CLASI ASR | 门前斜转，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。 |'
- en: '| CLASI Translation | Diagonal shot in front of the goal. Beautiful! What a
    beauty! Goal! Own goal! Yes, own goal. |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| CLASI 翻译 | 门前斜传，漂亮！这下漂亮！球进了！摆乌龙！哎，自摆乌龙。 |'
- en: '| Explanation | CLASI could recognize the speaker with an exciting tone, thus
    generating the translation with exclamation marks. Besides, in this case, which
    is a complicate scenario of a football game, the ASR outputs of the Commerical
    4 are mostly incorrect, leads to nonsense translation. |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| 解释 | CLASI 能够识别出带有兴奋语气的发言者，因此生成了带有感叹号的翻译。此外，在这个复杂的足球比赛场景中，商业版 4 的语音识别结果大多不正确，导致翻译无意义。
    |'
- en: 'Table 11: Comparision between CLASI and Commerical 4 for zh-en direction.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：CLASI 与商业版 4 在中英方向上的对比。
- en: B.2 Example of Detailed Evaluation Result on RealSI
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 RealSI 上详细评估结果示例
- en: We provide a detailed human evaluation result for CLASI in Figure [5](https://arxiv.org/html/2407.21646v2#A2.F5
    "Figure 5 ‣ B.2 Example of Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary
    Materials ‣ Authorship and Acknowledgements ‣ Social Impact ‣ Limitation and Future
    Work ‣ 6 Conclusion ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2
    ICL Performance ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4
    Latency ‣ 4.3 Translation Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity
    on End-to-end Simultaneous Speech Translation via LLM Agent"), where we provide
    golden source transcription, CLASI output, human evaluation results, and reference
    translation. We randomly choose one of the test samples in RealSI. We share the
    full detailed evaluation results at online sheets⁷⁷7We provide the full evaluation
    results of CLASI at [https://bit.ly/clasi-eval](https://bit.ly/clasi-eval) for
    academic reference. Note that to ensure fair comparison, when evaluating multiple
    systems, we randomly shuffle the ordering between systems for each semantic fragment
    so that human evaluators cannot identify the specific system.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[5](https://arxiv.org/html/2407.21646v2#A2.F5 "Figure 5 ‣ B.2 Example of
    Detailed Evaluation Result on RealSI ‣ Appendix B Supplementary Materials ‣ Authorship
    and Acknowledgements ‣ Social Impact ‣ Limitation and Future Work ‣ 6 Conclusion
    ‣ Human Evaluation. ‣ 5 Related Work ‣ 4.7 Case Study ‣ 4.6.2 ICL Performance
    ‣ 4.6 MM-RAG Performance ‣ 4.5 Supplementary Experiments ‣ 4.4 Latency ‣ 4.3 Translation
    Quality ‣ 4 Experiments ‣ Towards Achieving Human Parity on End-to-end Simultaneous
    Speech Translation via LLM Agent）中提供了 CLASI 的详细人工评估结果，其中包括黄金源转录、CLASI 输出、人类评估结果和参考翻译。我们从
    RealSI 中随机选择了一个测试样本。我们将完整的详细评估结果分享到在线表格中⁷⁷7我们提供了 CLASI 的完整评估结果，学术参考可见 [https://bit.ly/clasi-eval](https://bit.ly/clasi-eval)。请注意，为确保公平比较，在评估多个系统时，我们会随机打乱各语义片段中系统的顺序，以防止人工评估人员识别出具体的系统。
- en: '![Refer to caption](img/834e15ae17bba815615c2270243ff6c5.png)'
  id: totrans-464
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/834e15ae17bba815615c2270243ff6c5.png)'
- en: 'Figure 5: The first column indicates the golden transcription of the source
    text. Each row indicates one semantic fragment split by human evaluators. The
    second column is the translation results of CLASI. The third and fourth columns
    indicate the validity of translation and reference translation, respectively.
    In this case, the VIP is 24/29 == 82.8%.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：第一列表示源文本的黄金转录。每一行表示由人工评估人员分割的一个语义片段。第二列是 CLASI 的翻译结果。第三列和第四列分别表示翻译的有效性和参考翻译的有效性。在此情况下，VIP
    为 24/29 == 82.8%。
