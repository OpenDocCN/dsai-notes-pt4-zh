- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:34:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:34:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG),
    Planning, and Feedback Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于基于LLM的代理的主要范式回顾：工具使用（包括RAG）、规划和反馈学习
- en: 来源：[https://arxiv.org/html/2406.05804/](https://arxiv.org/html/2406.05804/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2406.05804/](https://arxiv.org/html/2406.05804/)
- en: Xinzhe Li
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李鑫哲
- en: Independent Researcher
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 独立研究员
- en: sergioli212@outlook.com Part of work conducted while affiliated with Deakin
    University.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: sergioli212@outlook.com 部分工作是在迪肯大学期间进行的。
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Tool use, planning, and feedback learning are currently three prominent paradigms
    for developing Large Language Model (LLM)-based agents across various tasks. Although
    numerous frameworks have been devised for each paradigm, their intricate workflows
    and inconsistent taxonomy create challenges in understanding and reviewing the
    frameworks across different paradigms. This survey introduces a unified taxonomy
    to systematically review and discuss these frameworks. Specifically, 1) the taxonomy
    defines environments/tasks, common LLM-profiled roles or LMPRs (policy models,
    evaluators, and dynamic models), and universally applicable workflows found in
    prior work, and 2) it enables a comparison of key perspectives on the implementations
    of LMPRs and workflow designs across different agent paradigms and frameworks.
    3) Finally, we identify three limitations in existing workflow designs and systematically
    discuss the future work. Resources have been made publicly available at in our
    GitHub repository [https://github.com/xinzhel/LLM-Agent-Survey](https://github.com/xinzhel/LLM-Agent-Survey).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 工具使用、规划和反馈学习目前是开发基于大型语言模型（LLM）代理的三大主要范式，适用于各种任务。尽管为每种范式制定了众多框架，但它们复杂的工作流程和不一致的分类法给跨范式的框架理解和回顾带来了挑战。本调查介绍了一种统一的分类法，用于系统地回顾和讨论这些框架。具体来说，1）该分类法定义了环境/任务、常见的LLM角色（政策模型、评估者和动态模型）以及先前工作中发现的普遍适用的工作流程，2）它使得可以比较不同代理范式和框架中LMPR的实施和工作流程设计的关键视角，3）最后，我们识别出现有工作流程设计中的三个局限性，并系统地讨论了未来的工作。相关资源已在我们的GitHub仓库[https://github.com/xinzhel/LLM-Agent-Survey](https://github.com/xinzhel/LLM-Agent-Survey)公开提供。
- en: 'A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG),
    Planning, and Feedback Learning'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于基于LLM的代理的主要范式回顾：工具使用（包括RAG）、规划和反馈学习
- en: 'Xinzhe Li ^†^†thanks: Part of work conducted while affiliated with Deakin University.
    Independent Researcher sergioli212@outlook.com'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 李鑫哲 ^†^†致谢：部分工作是在迪肯大学期间进行的。独立研究员 sergioli212@outlook.com
- en: '![Refer to caption](img/0a16f2a84b059d677decafa960fc82ed.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![请参考图注](img/0a16f2a84b059d677decafa960fc82ed.png)'
- en: (a) Base Workflows.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基础工作流程。
- en: '![Refer to caption](img/6c8cddcf802b6bc745d2a550760a28d0.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![请参考图注](img/6c8cddcf802b6bc745d2a550760a28d0.png)'
- en: (b) Workflows for Feedback Learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 反馈学习工作流程。
- en: '![Refer to caption](img/ec844f7644d09cc5b821fa0d7d0664ec.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![请参考图注](img/ec844f7644d09cc5b821fa0d7d0664ec.png)'
- en: (c) Workflows for Tool Use, with validation types categorized under both Tool
    Use and Feedback Learning paradigms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 工具使用工作流程，验证类型在工具使用和反馈学习范式下进行分类。
- en: '![Refer to caption](img/505b569fa142c066cb7190cfe1221e5d.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![请参考图注](img/505b569fa142c066cb7190cfe1221e5d.png)'
- en: (d) Search Workflows for Planning.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 规划的搜索工作流程。
- en: 'Figure 1: Common workflows based on three LLM-Profiled Roles (LMPRs): Policy,
    Evaluator and Dynamic Model. Numbers in circles indicate the sequence of execution
    for each step. Unnumbered lines represent iterative steps. Tool-use steps autonomously
    determined by LMPRs are marked by dashed lines. Circles labeled “-1” in the circles
    indicate the final step. The ‘Tool Invocation‘ and ‘Search Tree/Graph‘ text boxes
    indicate that these processes/objects are maintained in agent programs.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于三种LLM角色（LMPRs）的常见工作流程：政策、评估者和动态模型。圆圈中的数字表示每个步骤的执行顺序。未编号的线表示迭代步骤。由LMPRs自主决定的工具使用步骤以虚线表示。标记为“-1”的圆圈表示最终步骤。‘工具调用’和‘搜索树/图’文本框表示这些过程/对象在代理程序中被维护。
- en: 1 Introduction
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) have acquired extensive general knowledge and human-like
    reasoning capabilities (Santurkar et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib26);
    Wang et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib40); Zhong et al.,
    [2022](https://arxiv.org/html/2406.05804v6#bib.bib55), [2023](https://arxiv.org/html/2406.05804v6#bib.bib54)),
    positioning them as pivotal in constructing AI agents known as LLM-based agents.
    In the context of this survey, LLM-based agents are defined by their ability to
    interact actively with external tools (tool use) or environments (Yao et al.,
    [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)) and are designed to function
    as integral components of agency for planning (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    and feedback learning (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）已经获得了广泛的通用知识和类人推理能力（Santurkar等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib26)；Wang等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib40)；Zhong等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib55)，[2023](https://arxiv.org/html/2406.05804v6#bib.bib54)），使它们在构建被称为LLM基础的代理的AI代理中占据了关键地位。在本次调查中，LLM基础的代理被定义为能够主动与外部工具（工具使用）或环境（Yao等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)）进行交互，并被设计为作为规划（Yao等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）和反馈学习（Shinn等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）的代理的重要组成部分。
- en: Comparisons with Existing Surveys
  id: totrans-24
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 现有调查的比较
- en: 'Current surveys lack a coherent and unified starting point for discussing the
    three paradigms due to two main limitations: 1) Focusing on a specific paradigm
    or domain: For example, Huang et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib14))
    explore frameworks in the planning paradigm. Hu et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib12));
    Gallotta et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib7)) examine
    LLM-based agents in the context of games. In contrast, these paradigms represent
    general principles and mental models that govern the development and behavior
    of these agents. Hence, their frameworks should be discussed in a task-agnostic
    way. To achieve this, we extract universal workflows. Unless otherwise specified,
    in this survey, the term “framework” refers to a complete workflow for specific
    low-level tasks, while “workflow” refers to the task-agnostic process derived
    from one or more frameworks. 2) Lack of a unified basis for comparison: Although
    Wang et al. ([2024a](https://arxiv.org/html/2406.05804v6#bib.bib38)) cover all
    three paradigms, they do not analyze the implementation of algorithmic frameworks
    in a unified basis. In contrast, we summarize task-agnostic LLM-profiled roles
    as the foundation for the development of algorithmic frameworks across different
    paradigms. Notably, Wang et al. ([2024a](https://arxiv.org/html/2406.05804v6#bib.bib38))
    also discuss LLM profiling, but their focus is on personas, which are not relevant
    to the general roles that underpin these frameworks. We select four surveys according
    to their citations on Google Scholar for demonstration purposes, as shown in Table [1](https://arxiv.org/html/2406.05804v6#S1.T1
    "Table 1 ‣ Comparisons with Existing Surveys ‣ 1 Introduction ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 目前的调查由于两个主要限制，缺乏一个连贯且统一的起点来讨论这三种范式：1）专注于特定范式或领域：例如，Huang等人（[2024](https://arxiv.org/html/2406.05804v6#bib.bib14)）探索了规划范式中的框架。Hu等人（[2024](https://arxiv.org/html/2406.05804v6#bib.bib12)）；Gallotta等人（[2024](https://arxiv.org/html/2406.05804v6#bib.bib7)）在游戏背景下考察了LLM基础的代理。相比之下，这些范式代表了支配这些代理发展和行为的通用原则和思维模型。因此，它们的框架应以任务无关的方式进行讨论。为此，我们提取了通用的工作流程。除非另有说明，在本次调查中，“框架”一词指的是特定低级任务的完整工作流程，而“工作流程”指的是从一个或多个框架中衍生出的任务无关的过程。2）缺乏统一的比较基础：尽管Wang等人（[2024a](https://arxiv.org/html/2406.05804v6#bib.bib38)）涵盖了所有三种范式，但他们并未在统一的基础上分析算法框架的实现。相比之下，我们总结了任务无关的LLM角色配置，作为跨不同范式开发算法框架的基础。值得注意的是，Wang等人（[2024a](https://arxiv.org/html/2406.05804v6#bib.bib38)）也讨论了LLM配置，但他们的重点是人物设定，而这与支撑这些框架的通用角色无关。我们根据它们在Google
    Scholar上的引用选择了四篇调查作为示范，如表[1](https://arxiv.org/html/2406.05804v6#S1.T1 "表 1 ‣
    与现有调查的比较 ‣ 1 介绍 ‣ LLM基础代理的突出范式回顾：工具使用（包括RAG）、规划和反馈学习")所示。
- en: '|  | Tool Use | Planning | Feedback Learning | Universal LMPRs |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | 工具使用 | 规划 | 反馈学习 | 通用LMPRs |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | Task-Specific Tools | Workflows | Direct Planning | Planning via Search
    | Workflows | Feedback Sources | Workflows |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | 任务特定工具 | 工作流 | 直接规划 | 通过搜索规划 | 工作流 | 反馈来源 | 工作流 |  |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Guo et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib10)) | ✗ |
    ✗ | ✗ | ✗ | ✗ | ✓(§3.4) | ✗ | ✗ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Guo等（[2024](https://arxiv.org/html/2406.05804v6#bib.bib10)） | ✗ | ✗ | ✗ |
    ✗ | ✗ | ✓（§3.4） | ✗ | ✗ |'
- en: '| Xi et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib44)) | ✓(§3.3)
    | ✗ | ✓(§3.1.4) | ✗ | ✗ | ✓(§3.1.4) | ✗ | ✗ |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| Xi等（[2023](https://arxiv.org/html/2406.05804v6#bib.bib44)） | ✓（§3.3） | ✗
    | ✓（§3.1.4） | ✗ | ✗ | ✓（§3.1.4） | ✗ | ✗ |'
- en: '| Huang et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib14)) | ✗
    | ✗ | ✓(§3) | ✓(§4) | ✗ | ✓(§6) | ✗ | ✗ |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| Huang等（[2024](https://arxiv.org/html/2406.05804v6#bib.bib14)） | ✗ | ✗ | ✓（§3）
    | ✓（§4） | ✗ | ✓（§6） | ✗ | ✗ |'
- en: '| Wang et al. ([2024a](https://arxiv.org/html/2406.05804v6#bib.bib38)) | ✓(§2.1.4)
    | ✗ | ✓(§2.1.3) | ✓(§2.1.3) | ✗ | ✓(§2.1.3) | ✗ | ✗ |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| Wang等（[2024a](https://arxiv.org/html/2406.05804v6#bib.bib38)） | ✓（§2.1.4）
    | ✗ | ✓（§2.1.3） | ✓（§2.1.3） | ✗ | ✓（§2.1.3） | ✗ | ✗ |'
- en: '| Ours | ✗ | ✓(§4.2) | ✓(§4.1) | ✓(§4.3) | ✓ | ✓(§4.4) | ✓(§4.4) | ✓(§3) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | ✗ | ✓（§4.2） | ✓（§4.1） | ✓（§4.3） | ✓ | ✓（§4.4） | ✓（§4.4） | ✓（§3） |'
- en: 'Table 1: Coverage of four highly cited surveys and ours. The section numbers
    of the original manuscripts for the covered perspectives are demonstrated for
    further reference. We acknowledge the coverage with at least one paragraph focus
    on that topic.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：四个高度引用的调查和我们方法的覆盖范围。为进一步参考，示出了涵盖观点的原文章节号。我们承认至少有一段聚焦于该主题的覆盖范围。
- en: Contributions
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 贡献
- en: This survey offers the following contributions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查提供了以下贡献。
- en: '1) Reviewing the breadth of environments the agents are applicable to (§[2](https://arxiv.org/html/2406.05804v6#S2
    "2 Task Universality ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool
    Use (Including RAG), Planning, and Feedback Learning")): Before investigating
    LLM-based agents, it is essential to define the scope of universality by summarizing
    the environments in which these agents operate. We categorize the evaluated environments
    and tasks from the original manuscripts of the reviewed frameworks into two main
    types: decision-making environments and Natural Language Interaction (NLI) environments,
    the latter framed from traditional Natural Language Processing (NLP) tasks for
    agent-based setups.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 回顾代理适用的环境范围（§[2](https://arxiv.org/html/2406.05804v6#S2 "2 任务通用性 ‣ 基于LLM的代理的突出范式回顾：工具使用（包括RAG）、规划和反馈学习")）：在研究基于LLM的代理之前，首先需要通过总结这些代理操作的环境来定义通用性的范围。我们将评审框架原文中评估的环境和任务分为两大类：决策环境和自然语言互动（NLI）环境，后者是从传统的自然语言处理（NLP）任务框架中衍生出来的，用于代理设置。
- en: '2) Summarizing universal LLM-profiled roles and workflows (§[3](https://arxiv.org/html/2406.05804v6#S3
    "3 LLM-Profiled Roles (LMPRs) ‣ A Review of Prominent Paradigms for LLM-Based
    Agents: Tool Use (Including RAG), Planning, and Feedback Learning") and §[4](https://arxiv.org/html/2406.05804v6#S4
    "4 LMPR-Based Workflows ‣ A Review of Prominent Paradigms for LLM-Based Agents:
    Tool Use (Including RAG), Planning, and Feedback Learning")): Workflow designs
    under planning and feedback-learning paradigms are often based on complex algorithms,
    such as depth- or breadth-first search (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    and Monte Carlo Tree Search (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11))
    for planning, as well as Reinforcement Learning (RL) for feedback learning (Shinn
    et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)). The complexity
    increases when these algorithms are adapted to different environments. We aim
    to transcend the boundaries of these complex workflows by exploring those that
    can be universal across all types of environments. Additionally, we also investigate
    the workflows for tool use. Such exploration is based on three universal LLM-profile
    roles.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '2) 总结普适的LLM-配置角色和工作流（§[3](https://arxiv.org/html/2406.05804v6#S3 "3 LLM-Profiled
    Roles (LMPRs) ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use
    (Including RAG), Planning, and Feedback Learning") 和 §[4](https://arxiv.org/html/2406.05804v6#S4
    "4 LMPR-Based Workflows ‣ A Review of Prominent Paradigms for LLM-Based Agents:
    Tool Use (Including RAG), Planning, and Feedback Learning")）：规划和反馈学习范式下的工作流设计通常基于复杂的算法，如深度优先或广度优先搜索（Yao
    等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）和蒙特卡洛树搜索（Hao 等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）用于规划，以及强化学习（RL）用于反馈学习（Shinn
    等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）。当这些算法被应用到不同的环境时，其复杂性会增加。我们旨在超越这些复杂工作流的边界，探索那些可以跨所有类型环境通用的工作流。此外，我们还研究了工具使用的工作流。这种探索基于三个普适的LLM配置角色。'
- en: '3) Highlighting essential perspectives on LMPRs, workflows, and frameworks
    (§[5](https://arxiv.org/html/2406.05804v6#S5 "5 Discussions ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning") and §[6](https://arxiv.org/html/2406.05804v6#S6 "6 Limitations and
    Future Works ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use
    (Including RAG), Planning, and Feedback Learning")): We compare the implementations
    of LMPRs across various workflow types, emphasize workflows across different paradigms,
    and present alternative viewpoints to our formalization to prevent confusion.
    Additionally, we critically discuss three limitations of current frameworks and
    systematically identify potential workflow designs in future research.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '3) 强调关于LMPR、工作流和框架的关键视角（§[5](https://arxiv.org/html/2406.05804v6#S5 "5 Discussions
    ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG),
    Planning, and Feedback Learning") 和 §[6](https://arxiv.org/html/2406.05804v6#S6
    "6 Limitations and Future Works ‣ A Review of Prominent Paradigms for LLM-Based
    Agents: Tool Use (Including RAG), Planning, and Feedback Learning")）：我们比较了不同工作流类型中LMPR的实现，强调了不同范式下的工作流，并提出了与我们形式化方法不同的视角，以防止混淆。此外，我们还批判性地讨论了当前框架的三个局限性，并系统地识别了未来研究中的潜在工作流设计。'
- en: '| Env Types | Entities Interacted With by Agent | Action Properties | Examples
    of Action Instances | Examples of Env |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 环境类型 | 代理交互的实体 | 行动属性 | 行动实例示例 | 环境示例 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Game Environments | Virtual game elements (objects, avatars, other characters),
    and possibly other players or game narratives | Discrete, Executable, Deterministic
    | Move(Right) | BlocksWorld (Valmeekam et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib36)),
    CrossWords (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 游戏环境 | 虚拟游戏元素（物体、虚拟角色、其他角色），以及可能的其他玩家或游戏叙事 | 离散的、可执行的、确定性的 | 移动（右） | BlocksWorld（Valmeekam
    等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib36)），CrossWords（Yao 等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）
    |'
- en: '| Embodied Environments | Physical world (through sensors and actuators) |
    Discrete, Executable, Deterministic | Pick_Up[Object] | AlfWorld (Shridhar et al.,
    [2021](https://arxiv.org/html/2406.05804v6#bib.bib30)), VirtualHome (Puig et al.,
    [2018](https://arxiv.org/html/2406.05804v6#bib.bib23)), Minecraft (Fan et al.,
    [2022](https://arxiv.org/html/2406.05804v6#bib.bib6)) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 具身环境 | 物理世界（通过传感器和执行器） | 离散的、可执行的、确定性的 | 拾取[对象] | AlfWorld（Shridhar 等人，[2021](https://arxiv.org/html/2406.05804v6#bib.bib30)），VirtualHome（Puig
    等人，[2018](https://arxiv.org/html/2406.05804v6#bib.bib23)），Minecraft（Fan 等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib6)）
    |'
- en: '| Web Environments | Virtual web elements | Discrete, Executable, Deterministic
    | search(3 ounce bright citrus), click(Buy Now) | Webshop (Yao et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib47)),
    WebArena (Zhou et al., [2024b](https://arxiv.org/html/2406.05804v6#bib.bib57)),
    AppWorld (Trivedi et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib35))
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 网络环境 | 虚拟网页元素 | 离散的，可执行的，确定性的 | 搜索（3盎司明亮柑橘），点击（立即购买） | Webshop（Yao等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib47)），WebArena（Zhou等人，[2024b](https://arxiv.org/html/2406.05804v6#bib.bib57)），AppWorld（Trivedi等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib35)）
    |'
- en: '| NLIEs | Humans (through conversation or text) | Free-form, Discrete, Stochastic
    | The answer is Answer, Finish[Answer] | GSM8K Cobbe et al. ([2021](https://arxiv.org/html/2406.05804v6#bib.bib4)),
    HotpotQA (Yang et al., [2018](https://arxiv.org/html/2406.05804v6#bib.bib46))
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| NLIEs | 人类（通过对话或文本） | 自由形式，离散的，随机的 | 答案是Answer，完成[Answer] | GSM8K Cobbe等人（[2021](https://arxiv.org/html/2406.05804v6#bib.bib4)），HotpotQA（Yang等人，[2018](https://arxiv.org/html/2406.05804v6#bib.bib46)）
    |'
- en: 'Table 2: Common Task Environments. An action instance is commonly formalized
    by action predicates and action arguments. NLIEs refer to Natural Language Interaction
    Environments.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：常见任务环境。一个行动实例通常通过行动谓词和行动参数来形式化。NLIEs指的是自然语言交互环境。
- en: 2 Task Universality
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 任务普适性
- en: This section explores task environments where various workflow types are applicable.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 本节探讨了适用于各种工作流类型的任务环境。
- en: 2.1 Feedback-Based, Decision-Making Environments
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于反馈的决策环境
- en: In this setting, actions yield clear environmental dynamics and rewards for
    agents to learn from. They are typical environments to evaluate planning and RL
    agents (Russell and Norvig, [2010](https://arxiv.org/html/2406.05804v6#bib.bib25);
    Sutton and Barto, [2018](https://arxiv.org/html/2406.05804v6#bib.bib34)).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种环境中，行动产生明确的环境动态和奖励供智能体学习。这些是评估规划和强化学习（RL）智能体的典型环境（Russell和Norvig，[2010](https://arxiv.org/html/2406.05804v6#bib.bib25)；Sutton和Barto，[2018](https://arxiv.org/html/2406.05804v6#bib.bib34)）。
- en: Rule-Based Game Environments
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于规则的游戏环境
- en: These environments, which are deterministic and fully observable, include a
    variety of abstract strategy games like Chess and Go, and logic puzzles such as
    the Game of 24 (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    and Blocksworld (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)).
    They demand deep logical reasoning and strategic planning to navigate and solve.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环境是确定性的且完全可观察的，包含了各种抽象策略游戏，如国际象棋和围棋，以及逻辑难题，如24点游戏（Yao等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）和Blocksworld（Hao等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）。它们要求深入的逻辑推理和战略规划来导航和解决问题。
- en: Embodied Environments
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 具身环境
- en: These settings simulate physical interactions and spatial relationships in the
    real world. They require agents to engage in navigation, object manipulation,
    and other complex physical tasks (Shridhar et al., [2021](https://arxiv.org/html/2406.05804v6#bib.bib30);
    Puig et al., [2018](https://arxiv.org/html/2406.05804v6#bib.bib23); Fan et al.,
    [2022](https://arxiv.org/html/2406.05804v6#bib.bib6)), reflecting changes in the
    physical environment.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些环境模拟了现实世界中的物理交互和空间关系。它们要求智能体进行导航、物体操作和其他复杂的物理任务（Shridhar等人，[2021](https://arxiv.org/html/2406.05804v6#bib.bib30)；Puig等人，[2018](https://arxiv.org/html/2406.05804v6#bib.bib23)；Fan等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib6)），反映了物理环境中的变化。
- en: 2.2 Information Processing Environments
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 信息处理环境
- en: Web Environments
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 网络环境
- en: The focus in web-based AI applications is more on processing information and
    user interaction. Webshop (Yao et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib47))
    necessitates a sequence of actions to interact with the environment, such as searching
    for a product, choosing a color, and clicking “buy.” AppWorld (Trivedi et al.,
    [2024](https://arxiv.org/html/2406.05804v6#bib.bib35)) demands a more complex
    control flow, based on rich programs.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于网络的人工智能应用中，重点更多地放在信息处理和用户交互上。Webshop（Yao等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib47)）需要一系列动作来与环境进行互动，例如搜索产品、选择颜色并点击“购买”。AppWorld（Trivedi等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib35)）要求基于丰富的程序，进行更复杂的控制流。
- en: Natural Language Interaction Environments
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自然语言交互环境
- en: 'Typical NLP tasks are often recontextualized as agentic environments in the
    study of LLM-based agents (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49);
    Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11); Yao et al.,
    [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)). In this survey, we refer
    to this setting as Natural Language Interaction Environments (NLIEs) for brevity.
    In NLIEs, the environment remains static until the agent acts. Unlike typical
    task environments where natural language serves as an intermediary, in NLIEs,
    both the states and actions are defined linguistically, making the states conceptual
    and the actions often ambiguous and broadly defined. Common setups include: 1)
    Single-Step NLIEs: Some works (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49);
    Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)) frame the
    traditional question-answering (QA) setup as a single-step decision-making process,
    where the agent generates an answer in response to a question. The process starts
    with the question as the initial state and concludes when the answer is provided
    as an action. Since each trial consists of just one step, there are no environmental
    dynamics or external feedback. Here, QA encompasses not only typical tasks (Cobbe
    et al., [2021](https://arxiv.org/html/2406.05804v6#bib.bib4)) but also those often
    modeled as QA tasks, such as code optimization (Shypula et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib31)).
    2) Deliberate Multi-step NLIEs: For tasks without naturally defined intermediate
    steps, several studies have transformed NLP tasks into a Markov Decision Process
    to facilitate agentic workflows, e.g., search and planning (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48);
    Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)). For example,
    Hao et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib11)) reformulate
    subquestions in QA tasks as actions, enabling responses to user queries through
    a multi-step process. This approach allows the initial question to serve as the
    beginning of a series of state transitions. Actions may vary from providing direct,
    free-form answers in single-step QA to strategically proposing subquestions that
    navigate the agent through sequential updates toward a comprehensive solution.
    Additionally, Wan et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib37))
    suggest that “splitting an output sequence into tokens might be a good choice”
    for defining multi-step NLIEs methodically. Furthermore, Yao et al. ([2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    formulate two-step NLIEs for creative writing by segmenting the problem-solving
    process into distinct planning and execution phases.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的自然语言处理（NLP）任务在基于大语言模型（LLM）的智能体研究中常常被重新框定为具有代理性的环境（Yao等，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)；Hao等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)；Yao等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）。在这项调查中，我们将这一设定简化为自然语言互动环境（NLIEs）。在NLIEs中，环境保持静态，直到智能体进行行动。与传统任务环境不同，在这些环境中，语言不仅定义了状态，还定义了行动，使得状态具备概念性，而行动则往往模糊且定义宽泛。常见的设置包括：1）单步NLIEs：一些研究（Yao等，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)；Shinn等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）将传统的问答（QA）设置框定为一个单步决策过程，智能体在该过程中生成回答作为对问题的响应。该过程从问题作为初始状态开始，直至提供答案作为行动为止。由于每次试验仅包含一步，因此不存在环境动态或外部反馈。在这里，QA不仅涵盖了典型任务（Cobbe等，[2021](https://arxiv.org/html/2406.05804v6#bib.bib4)），还包括那些通常被建模为QA任务的任务，例如代码优化（Shypula等，[2024](https://arxiv.org/html/2406.05804v6#bib.bib31)）。2）深思熟虑的多步NLIEs：对于没有自然定义的中间步骤的任务，几项研究已将NLP任务转化为马尔可夫决策过程，以促进智能体工作流程，例如搜索与规划（Yao等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)；Hao等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）。例如，Hao等（[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）将QA任务中的子问题重新表述为行动，通过多步过程对用户查询做出回应。这种方法使得初始问题成为一系列状态转变的起点。行动可能从在单步QA中提供直接的自由形式回答，到通过策略性地提出子问题引导智能体通过顺序更新达到全面解决方案。此外，Wan等（[2024](https://arxiv.org/html/2406.05804v6#bib.bib37)）建议，“将输出序列拆分为标记可能是定义多步NLIEs的一种良好选择”。此外，Yao等（[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）通过将问题解决过程分为规划和执行两个阶段，为创造性写作制定了两步NLIEs模型。
- en: Remark 1.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注 1.
- en: In NLP, tasks represent the highest level of abstraction for modeling, whereas
    in decision-making processes, environments serve this role. It is common to describe
    an agent as being developed for a specific environment, such as an embodied environment,
    or for a particular task, such as QA. We refer to it as an NLIE-QA.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）中，任务代表了建模的最高抽象层次，而在决策过程中，环境则充当这一角色。通常，我们会将代理描述为为特定环境（如具身环境）或特定任务（如问答任务）开发的。我们称其为
    NLIE-QA。
- en: 3 LLM-Profiled Roles (LMPRs)
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 种 LLM-Profiled 角色（LMPRs）
- en: 'In this section, we demonstrate three common types of LLM-profiled Roles (LMPRs):
    policy models, evaluators, and dynamic models. They are task-agnostic and commonly
    used across various workflows.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了三种常见的 LLM-Profiled 角色（LMPRs）：策略模型、评估器和动态模型。它们与任务无关，广泛应用于各种工作流中。
- en: LLM-Profiled Policy $\text{glm}_{\text{policy}}$
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM-Profiled 策略 $\text{glm}_{\text{policy}}$
- en: '$\text{glm}_{\text{policy}}$ is designed to generate decisions, which could
    be an action or a series of actions (plans) for execution in external environments
    or planning. In contrast to typical RL policy models, which learn to maximize
    cumulative rewards through trial and error, LLM-profiled policy models, denoted
    as $\text{glm}_{\text{policy}}$, utilize pre-trained knowledge and commonsense
    derived from extensive textual data. We distinguish between two types of $\text{glm}_{\text{policy}}$:
    an actor $\text{glm}_{\text{actor}}$ directly maps a state to an action, whereas
    a planner $\text{glm}_{\text{planner}}$ generates a sequence of actions from a
    given state.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{policy}}$ 旨在生成决策，这些决策可以是外部环境中执行的动作或一系列动作（计划）。与典型的强化学习策略模型不同，后者通过反复试验最大化累积奖励，LLM-Profiled
    策略模型（记作 $\text{glm}_{\text{policy}}$）则利用预训练的知识和从大量文本数据中获得的常识。我们区分两种类型的 $\text{glm}_{\text{policy}}$：演员
    $\text{glm}_{\text{actor}}$ 直接将状态映射到动作，而规划者 $\text{glm}_{\text{planner}}$ 则从给定状态生成一系列动作。
- en: LLM-Profiled Evaluator $\text{glm}_{\text{eval}}$
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM-Profiled 评估器 $\text{glm}_{\text{eval}}$
- en: $\text{glm}_{\text{eval}}$ provides feedback crucial for different workflows.
    During planning, it evaluates each step of actions or the resulting states (Hao
    et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11); Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)),
    and during feedback learning, they revise the entire decisions (Shinn et al.,
    [2023](https://arxiv.org/html/2406.05804v6#bib.bib29); Wang et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)).
    Further details are provided in the next subsection.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{eval}}$ 提供了对不同工作流至关重要的反馈。在规划过程中，它评估每一步的动作或结果状态（Hao 等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)；Yao
    等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)），而在反馈学习过程中，它们修正整个决策（Shinn
    等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)；Wang 等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)）。更多细节将在下一个小节中提供。
- en: LLM-Profiled Dynamic Models $\text{glm}_{\text{dynamic}}$
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM-Profiled 动态模型 $\text{glm}_{\text{dynamic}}$
- en: They predict or describe changes to the environment. Generally, dynamic models
    form part of a comprehensive world model by predicting the next state $s^{\prime}$
    from the current state $s$ and action $a$. While typical RL uses the probability
    distribution $p(s^{\prime}\mid s,a)$ to model potential next states, LLM-based
    dynamic models directly predict the next state $s^{\prime}=\text{glm}_{\text{dynamic}}(s,a)$.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 它们预测或描述环境的变化。通常，动态模型通过从当前状态 $s$ 和动作 $a$ 预测下一个状态 $s^{\prime}$，形成一个综合的世界模型。虽然典型的强化学习使用概率分布
    $p(s^{\prime}\mid s,a)$ 来建模潜在的下一个状态，基于 LLM 的动态模型直接预测下一个状态 $s^{\prime}=\text{glm}_{\text{dynamic}}(s,a)$。
- en: '| Types | Subtypes | Universal LMPRs | Used For | Related Frameworks |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 子类型 | 通用 LLM-Profiled 角色（LMPRs） | 用途 | 相关框架 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Base | $\text{glm}_{\text{actor}}$ | $\text{glm}_{\text{actor}}$ | / | ReAct
    (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)), CoT (Wei
    et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib43)) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | $\text{glm}_{\text{actor}}$ | $\text{glm}_{\text{actor}}$ | / | ReAct
    (Yao 等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49))，CoT (Wei 等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib43))
    |'
- en: '|  | $\text{glm}_{\text{planner}}$ | $\text{glm}_{\text{planner}}$ | Planning
    | Huang et al. ([2022](https://arxiv.org/html/2406.05804v6#bib.bib13)), DEPS (Wang
    et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)), Planner-Actor-Reporter
    (Dasgupta et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib5)), Plan-and-solve
    (Wang et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)), OPEx (Shi
    et al., [2024a](https://arxiv.org/html/2406.05804v6#bib.bib27)) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{glm}_{\text{planner}}$ | $\text{glm}_{\text{planner}}$ | 规划 | Huang
    等人 ([2022](https://arxiv.org/html/2406.05804v6#bib.bib13)), DEPS (Wang 等人, [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)),
    Planner-Actor-Reporter (Dasgupta 等人, [2022](https://arxiv.org/html/2406.05804v6#bib.bib5)),
    计划与解决（Plan-and-solve）（Wang 等人, [2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)),
    OPEx (Shi 等人, [2024a](https://arxiv.org/html/2406.05804v6#bib.bib27)) |'
- en: '| Tool-Use | RAG-Style (Passive) | $\text{glm}_{\text{policy}}$ | Tool Use
    | RAG (Lewis et al., [2020](https://arxiv.org/html/2406.05804v6#bib.bib19); Shi
    et al., [2024b](https://arxiv.org/html/2406.05804v6#bib.bib28); Wang et al., [2024b](https://arxiv.org/html/2406.05804v6#bib.bib41);
    Zhang et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib52)) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 工具使用 | RAG风格（被动） | $\text{glm}_{\text{policy}}$ | 工具使用 | RAG (Lewis 等人, [2020](https://arxiv.org/html/2406.05804v6#bib.bib19);
    Shi 等人, [2024b](https://arxiv.org/html/2406.05804v6#bib.bib28); Wang 等人, [2024b](https://arxiv.org/html/2406.05804v6#bib.bib41);
    Zhang 等人, [2024](https://arxiv.org/html/2406.05804v6#bib.bib52)) |'
- en: '|  | Passive Validation | Tool Use, Feedback Learning | $\text{glm}_{\text{policy}}$
    | Guan et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib9)) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 被动验证 | 工具使用，反馈学习 | $\text{glm}_{\text{policy}}$ | Guan 等人 ([2023](https://arxiv.org/html/2406.05804v6#bib.bib9))
    |'
- en: '|  | Autonomous | $\text{glm}_{\text{policy}}$ | Tool-Use | MultiTool-CoT (Inaba
    et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib15)), ReAct (Yao et al.,
    [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)), Active RAG Jiang et al.
    ([2023](https://arxiv.org/html/2406.05804v6#bib.bib16)) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 自主 | $\text{glm}_{\text{policy}}$ | 工具使用 | MultiTool-CoT (Inaba 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib15)),
    ReAct (Yao 等人, [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)), 主动RAG
    (Jiang 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib16)) |'
- en: '|  | Autonomous Validation | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$
    | Tool Use, Feedback Learning | CRITIC (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8))
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 自主验证 | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$ | 工具使用，反馈学习
    | CRITIC (Gou 等人, [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) |'
- en: '| Search | Traversal & Heuristic | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$
    | Planning | Tree-of-Thoughts (ToT) (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)),
    Tree-BeamSearch (Xie et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib45)),
    Boost-of-Thoughts (Chen et al., [2024a](https://arxiv.org/html/2406.05804v6#bib.bib2)),
    Graph-of-Thoughts (Besta et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib1)),
    Tree-of-Traversal (Markowitz et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib22))
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 搜索 | 遍历与启发式 | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$ |
    规划 | 思维树（ToT）（Yao 等人, [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)),
    Tree-BeamSearch (Xie 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib45)),
    思维提升（Boost-of-Thoughts）（Chen 等人, [2024a](https://arxiv.org/html/2406.05804v6#bib.bib2)),
    思维图（Graph-of-Thoughts）（Besta 等人, [2024](https://arxiv.org/html/2406.05804v6#bib.bib1)),
    遍历树（Tree-of-Traversal）（Markowitz 等人, [2024](https://arxiv.org/html/2406.05804v6#bib.bib22))
    |'
- en: '|  | Simulation-based (MCTS) | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$,
    $\text{glm}_{\text{dynamic}}$ | Planning | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)),
    LLM-MCTS (Zhao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib53)),
    Wan et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib37)), AgentQ (Putta
    et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib24)), Chen et al. ([2024b](https://arxiv.org/html/2406.05804v6#bib.bib3)),
    Yu et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib50)), Sprueill et al.
    ([2023](https://arxiv.org/html/2406.05804v6#bib.bib32)) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于仿真（MCTS） | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$,
    $\text{glm}_{\text{dynamic}}$ | 规划 | RAP (Hao 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)),
    LLM-MCTS (Zhao 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib53)), Wan
    等人 ([2024](https://arxiv.org/html/2406.05804v6#bib.bib37)), AgentQ (Putta 等人,
    [2024](https://arxiv.org/html/2406.05804v6#bib.bib24)), Chen 等人 ([2024b](https://arxiv.org/html/2406.05804v6#bib.bib3)),
    Yu 等人 ([2023](https://arxiv.org/html/2406.05804v6#bib.bib50)), Sprueill 等人 ([2023](https://arxiv.org/html/2406.05804v6#bib.bib32))
    |'
- en: '| Feedback Learning | from $\text{glm}_{\text{eval}}$ only | $\text{glm}_{\text{policy}}$,
    $\text{glm}_{\text{eval}}$ | Feedback Learning | Reflexion (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)),
    Self-refine (Madaan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib20)),
    TextGrad (Yuksekgonul et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib51))
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 反馈学习 | 仅来自$\text{glm}_{\text{eval}}$ | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$
    | 反馈学习 | 反思（Shinn等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)), 自我优化（Madaan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib20)),
    TextGrad（Yuksekgonul等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib51))
    |'
- en: '|  | from $\text{glm}_{\text{eval}}$ & Task Env | $\text{glm}_{\text{policy}}$,
    $\text{glm}_{\text{eval}}$ | Feedback Learning | Reflexion (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29))
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 来自$\text{glm}_{\text{eval}}$ & 任务环境 | $\text{glm}_{\text{policy}}$, $\text{glm}_{\text{eval}}$
    | 反馈学习 | 反思（Shinn等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)) |'
- en: '|  | from Humans | $\text{glm}_{\text{policy}}$ | Feedback Learning | CRITIC
    (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 来自人类 | $\text{glm}_{\text{policy}}$ | 反馈学习 | CRITIC（Gou等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8))
    |'
- en: 'Table 3: Universal Workflows of LLM-Based Agents.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：LLM基础代理的通用工作流。
- en: 4 LMPR-Based Workflows
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 基于LMPR的工作流
- en: 'We explore four types of workflows based on the three types of LMPRs, as illustrated
    in Figure [1](https://arxiv.org/html/2406.05804v6#S0.F1 "Figure 1 ‣ A Review of
    Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning"). Table [3](https://arxiv.org/html/2406.05804v6#S3.T3 "Table
    3 ‣ LLM-Profiled Dynamic Models "glm"_"dynamic" ‣ 3 LLM-Profiled Roles (LMPRs)
    ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG),
    Planning, and Feedback Learning") summarizes the use of these workflows for different
    paradigms in the prior works.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了基于三种类型LMPR的四种工作流，如图[1](https://arxiv.org/html/2406.05804v6#S0.F1 "图1 ‣
    LLM基础代理重要范式综述：工具使用（包括RAG）、规划和反馈学习")所示。表[3](https://arxiv.org/html/2406.05804v6#S3.T3
    "表3 ‣ LLM-配置动态模型"glm"_"动态" ‣ 3 LLM-配置角色（LMPR） ‣ LLM基础代理重要范式综述：工具使用（包括RAG）、规划和反馈学习")总结了这些工作流在前期研究中的不同范式下的使用情况。
- en: 4.1 Base Workflows
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基础工作流
- en: 'The base workflow is as simple as the interaction between $\text{glm}_{\text{policy}}$
    and the environment. These workflows can be categorized based on LLM profiling
    into two types: planners and actors. 1) Planners: Many existing frameworks, such
    as those designed for embodied environments (e.g., Huang et al. ([2022](https://arxiv.org/html/2406.05804v6#bib.bib13))),
    fall under this category. While some frameworks (Dasgupta et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib5);
    Wang et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)) involve complex
    interactions with task-specific components and low-level, non-LLM-based actors,
    their universal workflow remains as simple as the base workflow. For NLIEs, the
    complete workflow often follows this base model without additional interaction,
    as seen in Wang et al. ([2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)).
    2) Actors: Early prompting frameworks for language generation tasks (classified
    as single-step NLIEs), such as Chain-of-Thought (Wei et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib43);
    Kojima et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib18)), fit into
    this category. For embodied tasks, ReAct (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49))
    employs $\text{glm}_{\text{actor}}$.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基础工作流就像是$\text{glm}_{\text{policy}}$与环境之间的简单交互。这些工作流可以根据LLM的配置分为两类：规划者和执行者。1)
    规划者：许多现有框架，如为具身环境设计的框架（例如Huang等人（[2022](https://arxiv.org/html/2406.05804v6#bib.bib13)））都属于这一类。尽管有些框架（Dasgupta等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib5);
    Wang等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)）涉及与特定任务组件和低层次非LLM执行者的复杂交互，但它们的通用工作流依然像基础工作流一样简单。对于NLIE，完整的工作流通常遵循这种基础模型而没有额外的交互，正如Wang等人（[2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)）所见。2)
    执行者：早期的语言生成任务提示框架（归类为单步NLIE），例如Chain-of-Thought（Wei等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib43);
    Kojima等人，[2022](https://arxiv.org/html/2406.05804v6#bib.bib18)）符合这一类别。对于具身任务，ReAct（Yao等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)）使用$\text{glm}_{\text{actor}}$。
- en: 4.2 Tool-Use Workflows
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 工具使用工作流
- en: We categorize two types of passive workflows and identify two types of autonomous
    workflows described in previous studies.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将被动工作流分为两类，并识别出前期研究中描述的两种类型的自主工作流。
- en: RAG-Style Tool Use
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: RAG风格工具使用
- en: A common example of passive tool use is Retrieval-Augmented Generation (RAG)
    (Lewis et al., [2020](https://arxiv.org/html/2406.05804v6#bib.bib19)), commonly
    used in NLIE-QA tasks. In this setup, given a query, a retrieval mechanism collects
    relevant information to assist $\text{glm}_{\text{policy}}$ in generating a response.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 被动工具使用的一个常见示例是检索增强生成（RAG）(Lewis 等人, [2020](https://arxiv.org/html/2406.05804v6#bib.bib19))，通常用于
    NLIE-QA 任务。在这种设置下，给定一个查询，检索机制收集相关信息以帮助 $\text{glm}_{\text{policy}}$ 生成响应。
- en: Passive Validation
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 被动验证
- en: Guan et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib9)) adopt an
    inverse approach for plan generation. Here, $\text{glm}_{\text{policy}}$ first
    generates a plan, which is then validated by a separate tool. Depending on the
    validation outcome, the information may or may not be used to revise the initial
    plan generated by $\text{glm}_{\text{policy}}$.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Guan 等人 ([2023](https://arxiv.org/html/2406.05804v6#bib.bib9)) 采用了一种反向方法来生成计划。在这个方法中，$\text{glm}_{\text{policy}}$
    首先生成一个计划，然后由一个独立的工具进行验证。根据验证结果，信息可能会或不会被用于修订由 $\text{glm}_{\text{policy}}$ 生成的初始计划。
- en: Autonomous Tool Use
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自主工具使用
- en: 'In this paradigm, LLMs must be aware of the available tools, which requires
    including tool information during LLM profiling. The workflow must also handle
    signals from LMPR generation to invoke tools. Different methods can be applied
    to enable $\text{glm}_{\text{policy}}$ to autonomously trigger tool usage. 1)
    In-Generation Triggers: Tools could be invoked during the reasoning process (Inaba
    et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib15); Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)).
    The agent program monitors token generation and pauses when a tool trigger is
    detected. This pause allows the tool to be invoked, its output processed, and
    the results integrated into the reasoning process. Triggers are defined through
    tool descriptions, few-shot demonstrations ¹¹1See an example prompt in Table [13](https://arxiv.org/html/2406.05804v6#A1.T13
    "Table 13 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning"), or a combination of both ²²2See an example prompt in
    Table [7](https://arxiv.org/html/2406.05804v6#A0.T7 "Table 7 ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning"). 2) Reasoning-Acting Strategy: Introduced by Yao et al. ([2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)),
    each reasoning or acting step completes a full inference cycle, ending with the
    generation of a stop token. Hence, token-level monitoring is unnecessary. The
    workflow prompts explicitly for each acting step. 3) Confidence-Based Invocation:
    Firstly, $\text{glm}_{\text{policy}}$ generates an initial action, and the decision
    to invoke a tool is based on the confidence level of the generated tokens. Jiang
    et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib16)) use this method
    for retrieval invocation, although it is not suitable for general tool use since
    it cannot specify which tool to invoke.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '在这种范式中，LLM 必须意识到可用的工具，这需要在 LLM 配置过程中包括工具信息。工作流还必须处理来自 LMPR 生成的信号，以调用工具。可以应用不同的方法，使
    $\text{glm}_{\text{policy}}$ 自主触发工具的使用。1）生成中的触发：工具可以在推理过程中被调用（Inaba 等人, [2023](https://arxiv.org/html/2406.05804v6#bib.bib15);
    Gou 等人, [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)）。代理程序监控标记生成，当检测到工具触发时暂停。这一暂停使得工具能够被调用，处理其输出，并将结果整合到推理过程中。触发条件通过工具描述、少量示例¹¹1请参见表格[13](https://arxiv.org/html/2406.05804v6#A1.T13
    "Table 13 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning")或二者的结合²²2请参见表格[7](https://arxiv.org/html/2406.05804v6#A0.T7
    "Table 7 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning")来定义。2）推理-行动策略：由 Yao 等人 ([2023b](https://arxiv.org/html/2406.05804v6#bib.bib49))
    提出，每一个推理或行动步骤都完成一个完整的推理循环，最终生成停止标记。因此，不需要标记级别的监控。工作流明确提示每个行动步骤。3）基于信心的调用：首先，$\text{glm}_{\text{policy}}$
    生成一个初始行动，调用工具的决策基于生成标记的信心水平。Jiang 等人 ([2023](https://arxiv.org/html/2406.05804v6#bib.bib16))
    使用这种方法进行检索调用，尽管它不适用于一般的工具使用，因为它无法指定调用哪个工具。'
- en: Autonomous Validation
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自主验证
- en: Gou et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) utilize $\text{glm}_{\text{policy}}$
    to generate an initial response. The resulting action(s) and the state(s) (i.e.,
    a trajectory) are then passed to $\text{glm}_{\text{evaluator}}$, which autonomously
    determines whether tools should be invoked for validation.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Gou等（[2024](https://arxiv.org/html/2406.05804v6#bib.bib8)）利用$\text{glm}_{\text{policy}}$生成初始响应。然后，生成的动作和状态（即轨迹）被传递给$\text{glm}_{\text{evaluator}}$，该模块自主决定是否应调用工具进行验证。
- en: Remark 2.
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注2。
- en: Tool-use workflows for validation can be viewed as a form of feedback learning,
    where $\text{glm}_{\text{policy}}$ receives feedback from the tools.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 用于验证的工具使用工作流可以被视为一种反馈学习的形式，其中$\text{glm}_{\text{policy}}$从工具中获得反馈。
- en: 4.3 Search Workflows
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 搜索工作流
- en: Traversal and Heuristic-Based Search
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 遍历与启发式搜索
- en: Generations from $\text{glm}_{\text{policy}}$, instead of direct execution in
    environments, are used to expand nodes for exploration, stored in a tree or graph
    structure, such as Tree-Of-Thoughts (ToT) (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    and its variants (Chen et al., [2024a](https://arxiv.org/html/2406.05804v6#bib.bib2);
    Besta et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib1)). $\text{glm}_{\text{eval}}$
    provides a fixed value estimate to select a node for further expansion. To expand
    a tree, ToT applies depth-/breadth-first search (DFS and BFS), while Xie et al.
    ([2023](https://arxiv.org/html/2406.05804v6#bib.bib45)) apply beam search. Notably,
    the BFS here is functionally equivalent to beam search with $N$ beams as the utility
    model $\text{glm}_{\text{eval}}$ is used to maintain the $N$ most promising nodes.
    ³³3Typically, BFS does not rely on a utility model to decide which nodes to expand,
    since it systematically explores all possible nodes at each level until a terminal
    state.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 从$\text{glm}_{\text{policy}}$生成的结果，而不是直接在环境中执行，被用来扩展节点进行探索，这些节点存储在树或图结构中，例如思维树（ToT）（Yao等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）及其变种（Chen等，[2024a](https://arxiv.org/html/2406.05804v6#bib.bib2)；Besta等，[2024](https://arxiv.org/html/2406.05804v6#bib.bib1)）。$\text{glm}_{\text{eval}}$提供一个固定值估计，用于选择一个节点进行进一步扩展。为了扩展树，ToT应用深度优先搜索（DFS）/广度优先搜索（BFS），而Xie等（[2023](https://arxiv.org/html/2406.05804v6#bib.bib45)）则应用束搜索。值得注意的是，这里的BFS在功能上等同于以$N$束为单位的束搜索，因为效用模型$\text{glm}_{\text{eval}}$被用来保持$N$个最有前景的节点。³³3通常，BFS不依赖于效用模型来决定扩展哪些节点，因为它系统地探索每一层的所有可能节点，直到终止状态。
- en: Simulation-Based Search
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于仿真的搜索
- en: 'Simulation-based search for LLM-based planning agents is often carried out
    using the classic Monte Carlo Tree Search (MCTS) algorithm (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11);
    Zhao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib53); Putta et al.,
    [2024](https://arxiv.org/html/2406.05804v6#bib.bib24)). Similar to ToT (Yao et al.,
    [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)), a tree is built through
    search and is expanded with $\text{glm}_{\text{policy}}$ and $\text{glm}_{\text{eval}}$.
    However, there are two key differences: 1) Node Selection: The nodes chosen for
    expansion are determined not only by the static outputs from $\text{glm}_{\text{eval}}$
    or other heuristics (whether they indicate goal attainment), but also by the cumulative
    statistics accrued over multiple simulations. Specifically, nodes that lead to
    better average rewards for subsequent nodes across all simulations (or trajectories)
    are indeed more likely to be expanded further. 2) Simulation : Following the selection
    and expansion phases. A simulation phrase is required where $\text{glm}_{\text{policy}}$,
    $\text{glm}_{\text{dynamic}}$ and $\text{glm}_{\text{eval}}$ are intimately collaborated,
    functioning as the roll-out policy. Specifically, $\text{glm}_{\text{policy}}$
    samples an action $a_{t}$ given the current state $s_{t}$, which in turn, is assessed
    by $\text{glm}_{\text{eval}}$. The top-scoring action is selected, with $\text{glm}_{\text{dynamic}}$
    using it to derive $s_{t+1}$, iteratively simulating the trajectory.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于模拟的LLM（大语言模型）规划代理的搜索通常使用经典的蒙特卡罗树搜索（MCTS）算法进行（Hao等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)；Zhao等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib53)；Putta等，[2024](https://arxiv.org/html/2406.05804v6#bib.bib24)）。类似于ToT（Yao等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)），通过搜索构建一棵树，并通过$\text{glm}_{\text{policy}}$和$\text{glm}_{\text{eval}}$进行扩展。然而，有两个关键的区别：1）节点选择：用于扩展的节点不仅由$\text{glm}_{\text{eval}}$或其他启发式方法的静态输出（是否表示目标达成）决定，还由多次模拟中积累的统计数据决定。具体来说，导致后续节点在所有模拟（或轨迹）中获得更好平均奖励的节点，更可能被进一步扩展。2）模拟：在选择和扩展阶段之后，需要进行模拟阶段，其中$\text{glm}_{\text{policy}}$、$\text{glm}_{\text{dynamic}}$和$\text{glm}_{\text{eval}}$密切协作，充当展开策略。具体来说，$\text{glm}_{\text{policy}}$根据当前状态$s_{t}$采样动作$a_{t}$，然后由$\text{glm}_{\text{eval}}$进行评估。选择得分最高的动作，$\text{glm}_{\text{dynamic}}$使用该动作推导出$s_{t+1}$，迭代模拟轨迹。
- en: 4.4 Feedback-Learning Workflows
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 反馈学习工作流
- en: 'Within feedback-learning workflows, feedback is fed into $\text{glm}_{\text{policy}}$
    for learning. One common source of feedback is $\text{glm}_{\text{eval}}$, as
    in Self-Refine (Madaan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib20)).
    Other common feedback sources include task environments (e.g., Reflexion (Shinn
    et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29))), tools (Gou et al.,
    [2024](https://arxiv.org/html/2406.05804v6#bib.bib8); Guan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib9))
    and humans Guan et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib9)).
    Among these, $\text{glm}_{\text{eval}}$ can optionally be used to revise feedback
    with more contextualized information. In the original manuscript of Reflexion
    (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)), $\text{glm}_{\text{eval}}$
    corresponds to “self-reflection,” whereas the term “evaluator” refers to either
    heuristics or an LLM-profiled classifier that generates sparse feedback. However,
    this evaluator could be disregarded as a universal LMPR for two reasons: 1) Heuristics
    are mostly used and lead to better performance, and 2) the evaluator’s outputs
    eventually are fed to “self-reflection” for verbal feedback. When tools are employed
    to provide feedback (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8);
    Guan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib9)), the workflow
    is the same as the tool-use workflow. In this setup, the necessity of invoking
    tools for feedback is either autonomously determined by $\text{glm}_{\text{eval}}$
    (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) (See Table [13](https://arxiv.org/html/2406.05804v6#A1.T13
    "Table 13 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning") for an example) or hardcoded (Guan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib9)).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在反馈学习工作流中，反馈被输入到$\text{glm}_{\text{policy}}$中进行学习。一个常见的反馈来源是$\text{glm}_{\text{eval}}$，如Self-Refine（Madaan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib20)）中所示。其他常见的反馈来源包括任务环境（例如，Reflexion（Shinn等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）），工具（Gou等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8);
    Guan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib9)）和人类（Guan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib9)）。在这些中，$\text{glm}_{\text{eval}}$可以选择性地用于通过更多上下文化的信息来修正反馈。在Reflexion（Shinn等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）的原始手稿中，$\text{glm}_{\text{eval}}$对应于“自我反思”，而“评估者”一词指的是启发式或LLM配置的分类器，它生成稀疏的反馈。然而，这个评估者可以作为通用LMPR被忽略，原因有二：1）大多数情况下使用启发式方法，并且能够带来更好的性能；2）评估者的输出最终会被送往“自我反思”以提供口头反馈。当使用工具来提供反馈时（Gou等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8);
    Guan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib9)），工作流与工具使用工作流相同。在这种设置下，调用工具提供反馈的必要性由$\text{glm}_{\text{eval}}$自动决定（Gou等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8)）（参见表[13](https://arxiv.org/html/2406.05804v6#A1.T13
    "Table 13 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning")中的示例）或是硬编码的（Guan等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib9)）。'
- en: Remark 3.
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注3。
- en: 'Unrealistic Feedback Source: So far, external feedback from NLIE-QA cannot
    be realistically generated. Some frameworks use the feedback-learning loop where
    the feedback source is unrealistic Particularly, Shinn et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib29));
    Zhou et al. ([2024a](https://arxiv.org/html/2406.05804v6#bib.bib56)) use ground
    truths to generate feedback. However, we acknowledge that ground-truth feedback
    is appropriate when the framework serves as an evaluation baseline, as used by
    Yao et al. ([2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不现实的反馈来源：到目前为止，来自NLIE-QA的外部反馈无法现实地生成。一些框架使用反馈学习循环，其中反馈来源是不现实的。特别是，Shinn等人（[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）；Zhou等人（[2024a](https://arxiv.org/html/2406.05804v6#bib.bib56)）使用真实数据来生成反馈。然而，我们承认，当框架作为评估基准使用时，真实数据反馈是适当的，如Yao等人（[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）所用的那样。
- en: 5 Discussions
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: Prompting Methods for Profiling
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用于配置的提示方法
- en: 'In the previous subsection, we categorize the original works proposing Chain-of-Thought
    (CoT) prompting, including zero-shot CoT (Wei et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib43))
    and CoT with few-shot demonstrations (Kojima et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib18)),
    under the base workflow since the original work solves task directly via the base
    workflow. However, these methods themselves can be generalized to different types
    of LMPRs and workflows, as shown in Appendix [B](https://arxiv.org/html/2406.05804v6#A2
    "Appendix B Prompting Methods of LLM-Profiled Roles ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning").
    Some points of the specific use in agents should be highlighted: 1) For planner
    profiling, zero-shot CoT implementations often fail to produce long-horizon plans
    (Wang et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)). 2) To actor
    profiling under autonomous tool-use workflows, particularly with reasoning-acting
    strategies, the tool definitions are required to be included in the prompt; and
    few-shot demonstrations are important to give a clue of generation formats for
    LLMs including the indication of when to stop.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一小节中，我们将提出链式思维（Chain-of-Thought, CoT）提示的原始工作进行分类，包括零-shot CoT（Wei 等， [2022](https://arxiv.org/html/2406.05804v6#bib.bib43)）和带有少量示例的
    CoT（Kojima 等， [2022](https://arxiv.org/html/2406.05804v6#bib.bib18)），归类于基本工作流程，因为原始工作通过基本工作流程直接解决任务。然而，这些方法本身可以推广到不同类型的语言模型提示响应（LMPRs）和工作流程，详见附录
    [B](https://arxiv.org/html/2406.05804v6#A2 "附录 B LLM角色概况的提示方法 ‣ 基于LLM的智能体重要范式回顾：工具使用（包括RAG）、规划和反馈学习")。在智能体中使用时有几点需要特别注意：1)
    对于规划者概况，零-shot CoT 实现通常无法生成长期计划（Wang 等，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)）。2)
    在自主工具使用工作流程下，特别是采用推理-行动策略时，提示中必须包含工具定义；少量示例对指导生成格式至关重要，包括指示何时停止。
- en: Workflow Comparisons for Plan Generation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计划生成的工作流程比较
- en: 'Both the base workflow using $\text{glm}_{\text{planner}}$ and search workflows
    generate a sequence of actions (i.e., a plan). However, they differ fundamentally
    in how the plans are generated and used. 1) Greedy Generation vs. Exploration:
    The base workflow leverages $\text{glm}_{\text{planner}}$ to greedily generate
    a static plan in a single inference step. This approach often struggles with long-horizon
    plans for complex tasks (Sun et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib33)).
    In contrast, search workflows explore multiple potential solutions and support
    backtracking, allowing for more robust exploration of options. 2) Plan Execution:
    The plans generated by $\text{glm}_{\text{planner}}$, BFS, DFS, and Beam Search
    are intended for full execution. However, this can lead to unexecutable actions,
    especially in the stochastic decision-making environments, if prior actions $(a_{1},...,a_{t-1})$
    leads to a state where the next action $a_{t}$ is invalid (e.g., ‘standing in
    front of the fridge‘ but needing to ‘open the microwave‘). This may cause abrupt
    interruption in the agent performance. In contrast, in the simulated-absed search
    via MCTS, only the action at the root node is executed in the actual environment.
    Subsequent simulated states and actions are discarded, though some implementations
    may partially retain these states to avoid recomputation. This search process
    repeats after every action taken, continuously recalculating the best action for
    the updated state.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 $\text{glm}_{\text{planner}}$ 的基本工作流程和搜索工作流程都会生成一系列行动（即计划）。然而，它们在生成和使用计划的方式上有根本区别。1)
    贪婪生成与探索：基本工作流程利用 $\text{glm}_{\text{planner}}$ 贪婪地在单次推理步骤中生成静态计划。这种方法通常在面对复杂任务时难以生成长期计划（Sun
    等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib33)）。相比之下，搜索工作流程探索多个潜在的解决方案并支持回溯，允许对选项进行更强有力的探索。2)
    计划执行：由 $\text{glm}_{\text{planner}}$、广度优先搜索（BFS）、深度优先搜索（DFS）和束搜索（Beam Search）生成的计划旨在完全执行。然而，这可能导致无法执行的行动，特别是在随机决策环境中，如果先前的行动
    $(a_{1},...,a_{t-1})$ 导致下一个行动 $a_{t}$ 变得无效（例如，“站在冰箱前”但需要“打开微波炉”）。这可能会导致智能体的表现出现突然中断。相比之下，在基于模拟的
    MCTS 搜索中，只有根节点的行动会在实际环境中执行。随后的模拟状态和行动会被丢弃，尽管某些实现可能会部分保留这些状态以避免重新计算。每次行动后，搜索过程会重复进行，不断重新计算更新状态下的最佳行动。
- en: Workflow Comparisons for Using $\text{glm}_{\text{actor}}$
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 $\text{glm}_{\text{actor}}$ 的工作流程比较
- en: 'The $\text{glm}_{\text{actor}}$ performs one of three types of actions, depending
    on the workflow: 1) In base and feedback-learning workflows, $\text{glm}_{\text{actor}}$
    consistently generates task-relevant actions to be executed within the task environment
    (executable task actions). 2) In a search workflow, actions generated by $\text{glm}_{\text{actor}}$
    are used for planning algorithms rather than direct execution (planning task actions).
    3) In autonomous tool-use workflows, $\text{glm}_{\text{actor}}$ is responsible
    for generating tool-related actions (tool actions).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: $\text{glm}_{\text{actor}}$ 根据工作流程执行三种类型的动作之一：1) 在基础和反馈学习工作流程中，$\text{glm}_{\text{actor}}$
    一直生成与任务相关的动作，以便在任务环境中执行（可执行的任务动作）。 2) 在搜索工作流程中，$\text{glm}_{\text{actor}}$ 生成的动作用于规划算法，而不是直接执行（规划任务动作）。
    3) 在自主工具使用工作流程中，$\text{glm}_{\text{actor}}$ 负责生成与工具相关的动作（工具动作）。
- en: '| Task Formulation | Feedback Types | Applicable Workflows | Example Works
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 任务表述 | 反馈类型 | 适用的工作流程 | 示例工作 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Text Generation | Free-form reflection | Feedback-learning workflows | Self-Refine
    (Madaan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib20)), Reflexion
    (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)), CRITIC
    (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 文本生成 | 自由形式的反思 | 反馈学习工作流程 | Self-Refine (Madaan et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib20)),
    Reflexion (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)),
    CRITIC (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) |'
- en: '| Binary/Multi-class Classification | Discrete values | Search workflows |
    RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)), Tree-BeamSearch
    (Xie et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib45)), ToT (Yao
    et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)), Koh et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib17))
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 二元/多类分类 | 离散值 | 搜索工作流程 | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)),
    Tree-BeamSearch (Xie et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib45)),
    ToT (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)), Koh
    et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib17)) |'
- en: '| Binary Classification | Continuous values (logits) | Search workflow via
    MCTS | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11))
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 二元分类 | 连续值（logits） | 通过 MCTS 的搜索工作流程 | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11))
    |'
- en: '| Multi-choice QA | Choices of top-N actions | Search workflows via traversal
    and heuristic | ToT (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 多项选择 QA | 顶级 N 个动作的选择 | 通过遍历和启发式搜索的工作流程 | ToT (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48))
    |'
- en: 'Table 4: Types of LLM-Profiled Evaluators According to Task Formulation and
    Feedback Types'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：根据任务表述和反馈类型的 LLM 配置评估器类型
- en: Workflow Comparisons for Using $\text{glm}_{\text{eval}}$
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 $\text{glm}_{\text{eval}}$ 的工作流程比较
- en: 'In feedback-learning workflows, the generation is passed to $\text{glm}_{\text{policy}}$
    for learning, while in search workflows, it is used for planning. This fundamental
    difference (learning vs. planning) leads to the following distinctions: 1) Feedback
    Use: As shown in Figure [1](https://arxiv.org/html/2406.05804v6#S0.F1 "Figure
    1 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning"), for learning, the output is generated
    for $\text{glm}_{\text{policy}}$ to revise and regenerate the entire decision.
    In search workflows, the output is used to construct a search tree/graph for action
    selection and further expansion. 2) Feedback Forms: These components—$\text{glm}_{\text{policy}}$
    and serch trees/graphs—require different forms of feedback. $\text{glm}_{\text{policy}}$
    processes free-form text, while MCTS utilizes continuous values as rewards, and
    trees/graphs in BFS, DFS, or beam search rely on discrete values for node selection.
    To meet these varying needs, distinct task formulations are employed, as summarized
    in Table [4](https://arxiv.org/html/2406.05804v6#S5.T4 "Table 4 ‣ Workflow Comparisons
    for Using "glm"_"actor" ‣ 5 Discussions ‣ A Review of Prominent Paradigms for
    LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning").
    a) Generating free-form text: $\text{glm}_{\text{eval}}$ is prompted to reflect
    on previous states and actions, generating reflective text as part of the $\text{glm}_{\text{policy}}$
    prompt in feedback-learning workflows (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29);
    Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)). b) Binary/multiclass
    classification: $\text{glm}_{\text{eval}}$ is prompted with specific constraints
    to generate discrete output tokens, typically “no” or “yes.” The tokens are converted
    into scalar values (e.g., 0/1) for use as rewards in MCTS simulations (Hao et al.,
    [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)), or to guide decision-making
    during tree traversal (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)).
    c) Binary classification with scalar values: This approach differs from the previous
    one by employing token logits generated by LLMs as scalar feedback values. For
    example, the probability of a “yes” response is computed as:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '在反馈学习工作流中，生成的内容会传递给$\text{glm}_{\text{policy}}$进行学习，而在搜索工作流中，则用于规划。这个根本性的区别（学习与规划）导致了以下几个区分：1）反馈使用：如图[1](https://arxiv.org/html/2406.05804v6#S0.F1
    "Figure 1 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning")所示，在学习中，输出内容会生成供$\text{glm}_{\text{policy}}$修订并重新生成整个决策。而在搜索工作流中，输出则用于构建搜索树/图，用于选择动作和进一步扩展。2）反馈形式：这些组件——$\text{glm}_{\text{policy}}$和搜索树/图——需要不同形式的反馈。$\text{glm}_{\text{policy}}$处理自由格式的文本，而MCTS使用连续值作为奖励，BFS、DFS或束搜索中的树/图则依赖于离散值来进行节点选择。为了满足这些不同的需求，采用了不同的任务表述，具体总结见表[4](https://arxiv.org/html/2406.05804v6#S5.T4
    "Table 4 ‣ Workflow Comparisons for Using ''glm''_''actor'' ‣ 5 Discussions ‣
    A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG),
    Planning, and Feedback Learning")。a) 生成自由格式文本：$\text{glm}_{\text{eval}}$通过提示反思先前的状态和动作，生成反思性文本，作为反馈学习工作流中$\text{glm}_{\text{policy}}$提示的一部分（Shinn等，2023年；Gou等，2024年）。b)
    二元/多类分类：$\text{glm}_{\text{eval}}$通过特定约束进行提示，生成离散输出标记，通常为“no”或“yes”。这些标记被转换为标量值（例如0/1），用于MCTS模拟中的奖励（Hao等，2023年），或在树遍历过程中指导决策（Yao等，2023a）。c)
    带标量值的二元分类：这种方法与前一种方法不同，采用LLM生成的标记logits作为标量反馈值。例如，“yes”回应的概率计算如下：'
- en: '|  | $v=\text{probs}_{\text{``yes''''}}=\frac{e^{l_{\text{``yes''''}}}}{e^{l_{\text{``%
    yes''''}}}+e^{l_{\text{``no''''}}}},$ |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $v=\text{probs}_{\text{``yes''''}}=\frac{e^{l_{\text{``yes''''}}}}{e^{l_{\text{``%
    yes''''}}}+e^{l_{\text{``no''''}}}},$ |  |'
- en: 'where $l_{\text{``yes''''}}$ and $l_{\text{``no''''}}$ are the logits for “yes”
    and “no” tokens, respectively. ⁴⁴4Note that such implementations of $\text{glm}_{\text{eval}}$
    are less common due to their inaccessibility of state-of-the-art black-box LLMs.
    These scalar values can then be used as rewards in MCTS. d) Multi-choice QA: This
    formulation is used in scenarios that require selecting from multiple choices,
    such as choosing from top-N possible actions in traversal-based search workflows
    (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$l_{\text{``yes''}}$和$l_{\text{``no''}}$分别是“yes”和“no”标记的logits。⁴⁴4注意，由于最先进的黑箱LLM的不可获取性，这种$\text{glm}_{\text{eval}}$的实现较为少见。这些标量值随后可以作为奖励用于MCTS。d)
    多项选择问答：这种表述用于需要从多个选择中选择的场景，例如在基于遍历的搜索工作流中从前N个可能的动作中选择（Yao等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）。
- en: 6 Limitations and Future Works
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制与未来工作
- en: 'Limitation 1: A Unified Solution for Base Workflows and Autonomous Tool-Use
    Workflows'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制1：基础工作流和自主工具使用工作流的统一解决方案
- en: 'ReAct (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49))
    tries to unify the autonomous tool-use workflow and the base workflow via the
    reasoning-acting strategy. In this scenario, they implicitly unify tools as a
    part of task environments, and tool actions and task actions are unified to similar
    formats. However, the two workflows are still distinct: 1) specifying and comparing
    different tool-use workflows in a fine-grained manner, 2) distinguishing the two
    can avoid an illusion that it is universally implementable workflow. Specifically,
    for the latter, the sequence in which reasoning and action outputs alternate is
    task-dependent. For QA tasks, the generations of reasoning steps and tool actions
    are fixed, with alternating prompts for thinking and acting. ⁵⁵5Reasoning-Acting
    Strategy (QA tasks): See an example prompt in Table [8](https://arxiv.org/html/2406.05804v6#A0.T8
    "Table 8 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning"). In contrast, for embodied tasks, the
    decision whether to proceed with thinking or acting in the next step is autonomously
    determined by $\text{glm}_{\text{policy}}$ ⁶⁶6Reasoning-Acting Strategy (embodied
    tasks): See an example prompt in Table [6](https://arxiv.org/html/2406.05804v6#A0.T6
    "Table 6 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 'ReAct（Yao等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)）尝试通过推理-行动策略统一自主工具使用工作流和基础工作流。在这个场景中，他们隐性地将工具统一为任务环境的一部分，工具动作和任务动作被统一为类似的格式。然而，这两种工作流依然是不同的：1）以细粒度方式指定并比较不同的工具使用工作流，2）区分这两者可以避免产生工具使用工作流普遍可实施的错觉。具体来说，对于后者，推理和行动输出交替的顺序是任务依赖的。对于问答任务，推理步骤和工具动作的生成是固定的，思考和行动的提示交替出现。⁵⁵5推理-行动策略（问答任务）：参见表[8](https://arxiv.org/html/2406.05804v6#A0.T8
    "Table 8 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning")中的示例提示。相比之下，对于具身任务，是否在下一步继续思考或行动的决策是由$\text{glm}_{\text{policy}}$自主决定的。⁶⁶6推理-行动策略（具身任务）：参见表[6](https://arxiv.org/html/2406.05804v6#A0.T6
    "Table 6 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning")中的示例提示。'
- en: 'Limitation 2: Lack of Universal Tool-Use Workflow Design'
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制2：缺乏普适工具使用工作流设计
- en: Another future direction is the development of universal tool use. Although
    tool use is a universally applicable paradigm, current research tends to focus
    on using tools for specific tasks such as NLIE-QA or specialized purposes—either
    enabling $\text{glm}_{\text{policy}}$ to retrieve new information or enabling
    $\text{glm}_{\text{eval}}$ for validation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个未来方向是普适工具使用的开发。虽然工具使用是一个普遍适用的范式，但当前的研究倾向于聚焦于使用工具来完成特定任务，例如NLIE-QA或专门目的——要么使得$\text{glm}_{\text{policy}}$能够获取新信息，要么使得$\text{glm}_{\text{eval}}$用于验证。
- en: 'Limitation 3: Formulating Language Tasks as Agentic Tasks'
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 限制3：将语言任务表述为代理任务
- en: 'The formulation can help develop a more universally used workflow, e.g., Reflexion
    (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)). However,
    some deliberate design should be reconsidered for future work, such as the use
    of ground truths as external feedback in Remark [3](https://arxiv.org/html/2406.05804v6#Thmremark3
    "Remark 3\. ‣ 4.4 Feedback-Learning Workflows ‣ 4 LMPR-Based Workflows ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning"). Another questionable setting is deliberate multi-step
    NLIEs, e.g., what if a question is simple enough to be answered without the need
    to be decomposed into subquestions.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '这一公式有助于开发一种更为普遍使用的工作流程，例如，Reflexion（Shinn 等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)）。然而，某些刻意设计应当在未来的工作中重新考虑，例如，使用地面真实值作为外部反馈的Remark
    [3](https://arxiv.org/html/2406.05804v6#Thmremark3 "Remark 3\. ‣ 4.4 Feedback-Learning
    Workflows ‣ 4 LMPR-Based Workflows ‣ A Review of Prominent Paradigms for LLM-Based
    Agents: Tool Use (Including RAG), Planning, and Feedback Learning")。另一个值得怀疑的设置是故意的多步骤NLIEs，例如，如果一个问题足够简单，不需要被分解成子问题来回答会怎样。'
- en: Devising New Workflows
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计新工作流程
- en: Our systematic review reveals potential directions for devising new workflows
    by intertwining existing ones. These workflows can be combined within a single
    paradigm—for instance, integrating different feedback sources or blending validation-based
    tool use with autonomous tool use. There is even greater potential for combining
    workflows across paradigms, such as incorporating optional feedback sources into
    non-validation tool-use workflows. In fact, validation-style workflows (Gou et al.,
    [2024](https://arxiv.org/html/2406.05804v6#bib.bib8); Jiang et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib16))
    inherently merge elements of both the feedback-learning and tool-use paradigms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统性回顾揭示了通过将现有工作流程交织在一起来设计新工作流程的潜在方向。这些工作流程可以在单一范式内结合——例如，将不同的反馈源整合，或将基于验证的工具使用与自主工具使用相结合。跨范式结合工作流程的潜力更大，例如，将可选的反馈源纳入非验证工具使用工作流程中。事实上，验证式工作流程（Gou
    等人，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8); Jiang 等人，[2023](https://arxiv.org/html/2406.05804v6#bib.bib16)）本质上融合了反馈学习和工具使用范式的元素。
- en: 7 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: This survey formalizes three common types of LLM-Profiled Roles (LMPRs) and
    investigates universal workflows for tool use, planning, and feedback learning.
    Since they are not task-specific, we hope that this will enlighten future research
    on employing LLM-based workflow designs across different tasks. Also, under the
    shuttle of these LMPRs and workflows, we discuss some nuances of LLM profiling,
    workflow pros & cons, and framework implementations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调研正式定义了三种常见的LLM配置角色（LMPRs），并研究了工具使用、规划和反馈学习的普遍工作流程。由于这些并非任务特定的，我们希望这能够为未来研究在不同任务中应用LLM基础的工作流程设计提供启示。此外，在这些LMPRs和工作流程的推动下，我们讨论了LLM配置的一些细微差别、工作流程的优缺点以及框架实现。
- en: Limitations
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: Due to the page limit, we can only focus on representative and pioneering work
    (primarily in * ACL, ICML, ICLR, and NeurIPS) and the perspectives that align
    with our focus. That means some recent work may be missed, and task-specific components
    and complete workflow designs are mostly ignored, especially for those frameworks
    designed for embodied environments. For example, a visual model is normally required
    to translate pixel observations into textual inputs for $\text{glm}_{\text{policy}}$
    (Wang et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)). Again,
    this review aims to summarize the task-agnostic workflows to facilitate a coherent
    understanding and identify their limitations for future research.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于页面限制，我们只能专注于具有代表性和开创性的工作（主要是*ACL, ICML, ICLR, 和 NeurIPS*）以及与我们焦点一致的观点。这意味着一些近期的工作可能被遗漏，任务特定的组件和完整的工作流程设计大多被忽略，尤其是那些为具身环境设计的框架。例如，通常需要视觉模型将像素观察转换为文本输入，以便进行$\text{glm}_{\text{policy}}$（Wang
    等人，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)）。再次强调，本综述旨在总结任务无关的工作流程，以促进对其的连贯理解并识别其局限性，以便为未来研究提供参考。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Besta et al. (2024) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski,
    Piotr Nyczyk, et al. 2024. Graph of thoughts: Solving elaborate problems with
    large language models. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 38, pages 17682–17690.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Besta 等人（2024）Maciej Besta、Nils Blach、Ales Kubicek、Robert Gerstenberger、Michal
    Podstawski、Lukas Gianinazzi、Joanna Gajda、Tomasz Lehmann、Hubert Niewiadomski、Piotr
    Nyczyk 等人 2024年。《思维图：利用大语言模型解决复杂问题》。发表于*人工智能会议论文集*，第38卷，第17682–17690页。
- en: 'Chen et al. (2024a) Sijia Chen, Baochun Li, and Di Niu. 2024a. [Boosting of
    thoughts: Trial-and-error problem solving with large language models](https://openreview.net/forum?id=qBL04XXex6).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024a）陈思佳、李宝春和牛迪。2024a年。[增强思维：使用大型语言模型的试错问题求解](https://openreview.net/forum?id=qBL04XXex6)。在*第十二届国际学习表征会议*。
- en: 'Chen et al. (2024b) Ziru Chen, Michael White, Ray Mooney, Ali Payani, Yu Su,
    and Huan Sun. 2024b. [When is tree search useful for LLM planning? it depends
    on the discriminator](https://doi.org/10.18653/v1/2024.acl-long.738). In *Proceedings
    of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 13659–13678, Bangkok, Thailand. Association for Computational
    Linguistics.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024b）陈子如、迈克尔·怀特、雷·穆尼、阿里·帕亚尼、苏宇和孙欢。2024b年。[树搜索何时对LLM规划有用？这取决于判别器](https://doi.org/10.18653/v1/2024.acl-long.738)。在*第62届计算语言学协会年会论文集（第1卷：长篇论文）*，第13659–13678页，泰国曼谷。计算语言学协会。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）卡尔·科比、维尼特·科萨拉朱、穆罕默德·巴瓦里安、马克·陈、金熙宇、卢卡兹·凯泽、马蒂亚斯·普拉普特、杰里·特沃雷克、雅各布·希尔顿、中野丽一、克里斯托弗·赫塞和约翰·舒尔曼。2021年。训练验证器解决数学文字题。*arXiv
    预印本 arXiv:2110.14168*。
- en: Dasgupta et al. (2022) Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino,
    Arun Ahuja, Sheila Babayan, Felix Hill, and Rob Fergus. 2022. [Collaborating with
    language models for embodied reasoning](https://openreview.net/forum?id=YoS-abmWjJc).
    In *Second Workshop on Language and Reinforcement Learning*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dasgupta 等人（2022）伊什塔·达斯古普塔、克里斯汀·凯泽-陈、肯尼斯·马里诺、阿伦·阿胡贾、谢拉·巴巴扬、费利克斯·希尔和罗布·费格斯。2022年。[与语言模型合作进行具身推理](https://openreview.net/forum?id=YoS-abmWjJc)。在*第二届语言与强化学习工作坊*。
- en: 'Fan et al. (2022) Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong
    Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. 2022.
    [Minedojo: Building open-ended embodied agents with internet-scale knowledge](https://openreview.net/forum?id=rc8o_j8I8PX).
    In *Thirty-sixth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人（2022）范林熙、王冠志、蒋云凡、阿贾伊·曼德尔卡、杨云聪、朱昊义、唐安德鲁、黄德安、朱昱可和安尼玛·安南德库马尔。2022年。[Minedojo：构建具有互联网规模知识的开放式具身智能体](https://openreview.net/forum?id=rc8o_j8I8PX)。在*第36届神经信息处理系统会议数据集与基准跟踪*。
- en: 'Gallotta et al. (2024) Roberto Gallotta, Graham Todd, Marvin Zammit, Sam Earle,
    Antonios Liapis, Julian Togelius, and Georgios N Yannakakis. 2024. Large language
    models and games: A survey and roadmap. *arXiv preprint arXiv:2402.18659*.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallotta 等人（2024）罗伯托·加洛塔、格雷厄姆·托德、马尔文·扎米特、萨姆·厄尔、安东尼奥斯·利亚皮斯、朱利安·托格柳斯和乔治·N·扬纳卡基斯。2024年。大型语言模型与游戏：综述与路线图。*arXiv
    预印本 arXiv:2402.18659*。
- en: 'Gou et al. (2024) Zhibin Gou, Zhihong Shao, Yeyun Gong, yelong shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2024. [CRITIC: Large language models can self-correct
    with tool-interactive critiquing](https://openreview.net/forum?id=Sx038qxjek).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等人（2024）苟志斌、邵志洪、龚叶云、沈业龙、杨宇久、段楠和陈维柱。2024年。[CRITIC：大型语言模型能够通过工具交互批判进行自我纠正](https://openreview.net/forum?id=Sx038qxjek)。在*第十二届国际学习表征会议*。
- en: Guan et al. (2023) Lin Guan, Karthik Valmeekam, Sarath Sreedharan, and Subbarao
    Kambhampati. 2023. [Leveraging pre-trained large language models to construct
    and utilize world models for model-based task planning](https://openreview.net/forum?id=zDbsSscmuj).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guan 等人（2023）关琳、卡尔提克·瓦尔梅卡姆、萨拉斯·斯里达兰和苏巴拉奥·坎巴帕提。2023年。[利用预训练的大型语言模型构建和使用世界模型进行基于模型的任务规划](https://openreview.net/forum?id=zDbsSscmuj)。在*第37届神经信息处理系统会议*。
- en: 'Guo et al. (2024) Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao
    Pei, Nitesh V Chawla, Olaf Wiest, and Xiangliang Zhang. 2024. Large language model
    based multi-agents: A survey of progress and challenges. *arXiv preprint arXiv:2402.01680*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人（2024）郭太诚、陈秀英、王亚奇、常瑞迪、裴世超、尼特什·V·查乌拉、奥拉夫·维斯特和张向亮。2024年。基于大型语言模型的多智能体：进展与挑战的综述。*arXiv
    预印本 arXiv:2402.01680*。
- en: Hao et al. (2023) Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy
    Wang, and Zhiting Hu. 2023. [Reasoning with language model is planning with world
    model](https://aclanthology.org/2023.emnlp-main.507). In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing*, pages 8154–8173,
    Singapore. Association for Computational Linguistics.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao 等人（2023）Shibo Hao, Yi Gu, Haodi Ma, Joshua Hong, Zhen Wang, Daisy Wang 和
    Zhiting Hu。2023年。[与语言模型推理即为使用世界模型进行规划](https://aclanthology.org/2023.emnlp-main.507)。收录于
    *2023年自然语言处理实证方法会议论文集*，第8154–8173页，新加坡。计算语言学协会。
- en: Hu et al. (2024) Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen
    Liu, Ramana Kompella, and Ling Liu. 2024. A survey on large language model-based
    game agents. *arXiv preprint arXiv:2404.02039*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2024）Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu,
    Ramana Kompella 和 Ling Liu。2024年。一项关于基于大型语言模型的游戏代理的调查。*arXiv 预印本 arXiv:2404.02039*。
- en: 'Huang et al. (2022) Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch.
    2022. Language models as zero-shot planners: Extracting actionable knowledge for
    embodied agents. In *International Conference on Machine Learning*, pages 9118–9147\.
    PMLR.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2022）Wenlong Huang, Pieter Abbeel, Deepak Pathak 和 Igor Mordatch。2022年。语言模型作为零-shot规划器：为具身代理提取可操作的知识。收录于
    *国际机器学习会议*，第9118–9147页，PMLR。
- en: 'Huang et al. (2024) Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao
    Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding
    the planning of llm agents: A survey. *arXiv preprint arXiv:2402.02716*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2024）Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu
    Lian, Yasheng Wang, Ruiming Tang 和 Enhong Chen。2024年。理解大型语言模型代理的规划：一项调查。*arXiv
    预印本 arXiv:2402.02716*。
- en: 'Inaba et al. (2023) Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng, and Sadao
    Kurohashi. 2023. [MultiTool-CoT: GPT-3 can use multiple external tools with chain
    of thought prompting](https://doi.org/10.18653/v1/2023.acl-short.130). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    2: Short Papers)*, pages 1522–1532, Toronto, Canada. Association for Computational
    Linguistics.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Inaba 等人（2023）Tatsuro Inaba, Hirokazu Kiyomaru, Fei Cheng 和 Sadao Kurohashi。2023年。[MultiTool-CoT:
    GPT-3 可以通过链式思维提示使用多个外部工具](https://doi.org/10.18653/v1/2023.acl-short.130)。收录于
    *第61届计算语言学协会年会论文集（第2卷：短篇论文）*，第1522–1532页，加拿大多伦多。计算语言学协会。'
- en: Jiang et al. (2023) Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu,
    Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. [Active retrieval
    augmented generation](https://doi.org/10.18653/v1/2023.emnlp-main.495). In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    7969–7992, Singapore. Association for Computational Linguistics.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2023）Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane
    Dwivedi-Yu, Yiming Yang, Jamie Callan 和 Graham Neubig。2023年。[主动检索增强生成](https://doi.org/10.18653/v1/2023.emnlp-main.495)。收录于
    *2023年自然语言处理实证方法会议论文集*，第7969–7992页，新加坡。计算语言学协会。
- en: Koh et al. (2024) Jing Yu Koh, Stephen McAleer, Daniel Fried, and Ruslan Salakhutdinov.
    2024. Tree search for language model agents. *arXiv preprint arXiv:2407.01476*.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 等人（2024）Jing Yu Koh, Stephen McAleer, Daniel Fried 和 Ruslan Salakhutdinov。2024年。语言模型代理的树形搜索。*arXiv
    预印本 arXiv:2407.01476*。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. [Large language models are zero-shot reasoners](https://openreview.net/forum?id=e2TBb5y0yFf).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等人（2022）Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo
    和 Yusuke Iwasawa。2022年。[大型语言模型是零-shot推理器](https://openreview.net/forum?id=e2TBb5y0yFf)。收录于
    *神经信息处理系统进展*。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. [Retrieval-augmented generation
    for knowledge-intensive nlp tasks](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 9459–9474\.
    Curran Associates, Inc.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人（2020）Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, Sebastian Riedel 和 Douwe Kiela。2020年。[检索增强生成用于知识密集型自然语言处理任务](https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf)。收录于
    *神经信息处理系统进展*，第33卷，第9459–9474页，Curran Associates, Inc.
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    et al. 2023. Self-refine: Iterative refinement with self-feedback. *arXiv preprint
    arXiv:2303.17651*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Madaan 等 (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang
    等. 2023. Self-refine: 通过自我反馈进行迭代优化. *arXiv 预印本 arXiv:2303.17651*。'
- en: 'Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. 2023.
    Selfcheckgpt: Zero-resource black-box hallucination detection for generative large
    language models. *arXiv preprint arXiv:2303.08896*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Manakul 等 (2023) Potsawee Manakul, Adian Liusie, 和 Mark JF Gales. 2023. Selfcheckgpt:
    零资源黑箱幻觉检测用于生成大型语言模型. *arXiv 预印本 arXiv:2303.08896*。'
- en: 'Markowitz et al. (2024) Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh
    Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, and Aram Galstyan. 2024. [Tree-of-traversals:
    A zero-shot reasoning algorithm for augmenting black-box language models with
    knowledge graphs](https://doi.org/10.18653/v1/2024.acl-long.665). In *Proceedings
    of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 12302–12319, Bangkok, Thailand. Association for Computational
    Linguistics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Markowitz 等 (2024) Elan Markowitz, Anil Ramakrishna, Jwala Dhamala, Ninareh
    Mehrabi, Charith Peris, Rahul Gupta, Kai-Wei Chang, 和 Aram Galstyan. 2024. [Tree-of-traversals:
    一种零样本推理算法，用于通过知识图谱增强黑箱语言模型](https://doi.org/10.18653/v1/2024.acl-long.665). 收录于
    *第62届计算语言学协会年会论文集 (卷1: 长篇论文)*，页面 12302–12319，曼谷，泰国。计算语言学协会。'
- en: 'Puig et al. (2018) Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang,
    Sanja Fidler, and Antonio Torralba. 2018. Virtualhome: Simulating household activities
    via programs. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, pages 8494–8502.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Puig 等 (2018) Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja
    Fidler, 和 Antonio Torralba. 2018. Virtualhome: 通过程序模拟家庭活动. 收录于 *IEEE计算机视觉与模式识别大会论文集*，页面
    8494–8502。'
- en: 'Putta et al. (2024) Pranav Putta, Edmund Mills, Naman Garg, Sumeet Ramesh Motwani,
    Chelsea Finn, Divyansh Garg, and Rafael Rafailov. 2024. [Agent q: Advanced reasoning
    and learning for autonomous ai agents](https://api.semanticscholar.org/CorpusID:271865516).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Putta 等 (2024) Pranav Putta, Edmund Mills, Naman Garg, Sumeet Ramesh Motwani,
    Chelsea Finn, Divyansh Garg, 和 Rafael Rafailov. 2024. [Agent q: 高级推理与学习用于自主AI代理](https://api.semanticscholar.org/CorpusID:271865516)。'
- en: Russell and Norvig (2010) Stuart J Russell and Peter Norvig. 2010. *Artificial
    intelligence a modern approach*. London.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russell 和 Norvig (2010) Stuart J Russell 和 Peter Norvig. 2010. *人工智能：一种现代方法*.
    伦敦。
- en: Santurkar et al. (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo
    Lee, Percy Liang, and Tatsunori Hashimoto. 2023. Whose opinions do language models
    reflect? *arXiv preprint arXiv:2303.17548*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santurkar 等 (2023) Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee,
    Percy Liang, 和 Tatsunori Hashimoto. 2023. 语言模型反映了谁的观点？ *arXiv 预印本 arXiv:2303.17548*。
- en: 'Shi et al. (2024a) Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Côté,
    and Bang Liu. 2024a. [OPEx: A component-wise analysis of LLM-centric agents in
    embodied instruction following](https://doi.org/10.18653/v1/2024.acl-long.37).
    In *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 622–636, Bangkok, Thailand. Association
    for Computational Linguistics.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等 (2024a) Haochen Shi, Zhiyuan Sun, Xingdi Yuan, Marc-Alexandre Côté, 和
    Bang Liu. 2024a. [OPEx: LLM为中心的代理在体现指令跟随中的逐组件分析](https://doi.org/10.18653/v1/2024.acl-long.37).
    收录于 *第62届计算语言学协会年会论文集 (卷1: 长篇论文)*，页面 622–636，曼谷，泰国。计算语言学协会。'
- en: 'Shi et al. (2024b) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo,
    Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. 2024b. [REPLUG:
    Retrieval-augmented black-box language models](https://doi.org/10.18653/v1/2024.naacl-long.463).
    In *Proceedings of the 2024 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)*,
    pages 8371–8384, Mexico City, Mexico. Association for Computational Linguistics.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi 等 (2024b) Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard
    James, Mike Lewis, Luke Zettlemoyer, 和 Wen-tau Yih. 2024b. [REPLUG: 检索增强黑箱语言模型](https://doi.org/10.18653/v1/2024.naacl-long.463).
    收录于 *2024年北美计算语言学协会会议：人类语言技术（卷1：长篇论文）*，页面 8371–8384，墨西哥城，墨西哥。计算语言学协会。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan, and Shunyu Yao. 2023. [Reflexion: Language agents with verbal
    reinforcement learning](https://arxiv.org/abs/2303.11366). *Preprint*, arXiv:2303.11366.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人（2023）Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath,
    Karthik Narasimhan 和 Shunyu Yao. 2023. [Reflexion：具有言语强化学习的语言代理](https://arxiv.org/abs/2303.11366).
    *预印本*，arXiv:2303.11366。
- en: 'Shridhar et al. (2021) Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler, and Matthew Hausknecht. 2021. [{ALFW}orld: Aligning text
    and embodied environments for interactive learning](https://openreview.net/forum?id=0IOX0YcCdTn).
    In *International Conference on Learning Representations*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shridhar 等人（2021）Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan
    Bisk, Adam Trischler 和 Matthew Hausknecht. 2021. [{ALFW}orld: 将文本与具身环境对齐以进行互动学习](https://openreview.net/forum?id=0IOX0YcCdTn).
    发表在 *国际学习表征会议*。'
- en: Shypula et al. (2024) Alexander G Shypula, Aman Madaan, Yimeng Zeng, Uri Alon,
    Jacob R. Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan,
    Osbert Bastani, and Amir Yazdanbakhsh. 2024. [Learning performance-improving code
    edits](https://openreview.net/forum?id=ix7rLVHXyY). In *The Twelfth International
    Conference on Learning Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shypula 等人（2024）Alexander G Shypula, Aman Madaan, Yimeng Zeng, Uri Alon, Jacob
    R. Gardner, Yiming Yang, Milad Hashemi, Graham Neubig, Parthasarathy Ranganathan,
    Osbert Bastani 和 Amir Yazdanbakhsh. 2024. [学习性能提升代码编辑](https://openreview.net/forum?id=ix7rLVHXyY).
    发表在 *第十二届国际学习表征会议*。
- en: 'Sprueill et al. (2023) Henry Sprueill, Carl Edwards, Mariefel Olarte, Udishnu
    Sanyal, Heng Ji, and Sutanay Choudhury. 2023. [Monte Carlo thought search: Large
    language model querying for complex scientific reasoning in catalyst design](https://doi.org/10.18653/v1/2023.findings-emnlp.560).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    8348–8365, Singapore. Association for Computational Linguistics.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sprueill 等人（2023）Henry Sprueill, Carl Edwards, Mariefel Olarte, Udishnu Sanyal,
    Heng Ji 和 Sutanay Choudhury. 2023. [蒙特卡洛思维搜索：大语言模型在催化剂设计中的复杂科学推理查询](https://doi.org/10.18653/v1/2023.findings-emnlp.560).
    发表在 *计算语言学协会年会论文集：EMNLP 2023*，第8348-8365页，新加坡。计算语言学协会。
- en: 'Sun et al. (2023) Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao
    Zhang. 2023. [Adaplanner: Adaptive planning from feedback with language models](https://openreview.net/forum?id=rnKgbKmelt).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023）Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai 和 Chao Zhang. 2023.
    [Adaplanner：基于反馈的自适应规划与语言模型](https://openreview.net/forum?id=rnKgbKmelt). 发表在
    *第37届神经信息处理系统会议*。
- en: 'Sutton and Barto (2018) Richard S Sutton and Andrew G Barto. 2018. *Reinforcement
    learning: An introduction*. MIT press.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton 和 Barto（2018）Richard S Sutton 和 Andrew G Barto. 2018. *强化学习：入门*. MIT出版社。
- en: 'Trivedi et al. (2024) Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin
    Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian.
    2024. [AppWorld: A controllable world of apps and people for benchmarking interactive
    coding agents](https://aclanthology.org/2024.acl-long.850). In *Proceedings of
    the 62nd Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 16022–16076, Bangkok, Thailand. Association for Computational
    Linguistics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trivedi 等人（2024）Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku,
    Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal 和 Niranjan Balasubramanian.
    2024. [AppWorld：一个可控的应用与人物世界，用于基准测试互动编码代理](https://aclanthology.org/2024.acl-long.850).
    发表在 *计算语言学协会第62届年会论文集（第1卷：长篇论文）*，第16022-16076页，泰国曼谷。计算语言学协会。
- en: Valmeekam et al. (2022) Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan,
    and Subbarao Kambhampati. 2022. [Large language models still can’t plan (a benchmark
    for LLMs on planning and reasoning about change)](https://openreview.net/forum?id=wUU-7XTL5XO).
    In *NeurIPS 2022 Foundation Models for Decision Making Workshop*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valmeekam 等人（2022）Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan 和 Subbarao
    Kambhampati. 2022. [大型语言模型仍无法进行规划（LLM在规划和推理变更方面的基准）](https://openreview.net/forum?id=wUU-7XTL5XO).
    发表在 *NeurIPS 2022决策制定基础模型研讨会*。
- en: Wan et al. (2024) Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang,
    and Jun Wang. 2024. [Alphazero-like tree-search can guide large language model
    decoding and training](https://openreview.net/forum?id=fLO9VaAb3B).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人（2024）Ziyu Wan, Xidong Feng, Muning Wen, Ying Wen, Weinan Zhang 和 Jun
    Wang. 2024. [类似AlphaZero的树搜索可指导大语言模型解码和训练](https://openreview.net/forum?id=fLO9VaAb3B).
- en: Wang et al. (2024a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. A survey
    on large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6):1–26.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2024a）王磊、马辰、冯雪扬、张泽宇、杨浩、张敬森、陈志远、唐家凯、陈旭、林彦凯等人。2024a. 基于大型语言模型的自主代理综述。*计算机科学前沿*，18(6):1–26。
- en: 'Wang et al. (2023a) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023a. [Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models](https://doi.org/10.18653/v1/2023.acl-long.147).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 2609–2634, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023a）王磊、徐婉玉、蓝义怀、胡志强、兰云诗、李凯威、林奕鹏。2023a. [计划与解决提示：通过大型语言模型改进零-shot链式思维推理](https://doi.org/10.18653/v1/2023.acl-long.147)。在*第61届计算语言学会年会会议论文集（第一卷：长篇论文）*，第2609–2634页，加拿大多伦多。计算语言学会。
- en: 'Wang et al. (2022) Siyuan Wang, Zhongkun Liu, Wanjun Zhong, Ming Zhou, Zhongyu
    Wei, Zhumin Chen, and Nan Duan. 2022. [From lsat: The progress and challenges
    of complex reasoning](https://doi.org/10.1109/TASLP.2022.3164218). *IEEE/ACM Trans.
    Audio, Speech and Lang. Proc.*, 30:2201–2216.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2022）王思源、刘忠坤、钟万君、周鸣、魏中宇、陈竹敏、段楠。2022. [来自lsat：复杂推理的进展与挑战](https://doi.org/10.1109/TASLP.2022.3164218)。*IEEE/ACM音频、语音与语言处理学报*，30:2201–2216。
- en: Wang et al. (2024b) Xiaohua Wang, Zhenghua Wang, Xuan Gao, Feiran Zhang, Yixin
    Wu, Zhibo Xu, Tianyuan Shi, Zhengyuan Wang, Shizheng Li, Qi Qian, et al. 2024b.
    Searching for best practices in retrieval-augmented generation. *arXiv preprint
    arXiv:2407.01219*.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2024b）王晓华、王正华、高轩、张斐然、吴一鑫、徐志博、史天元、王正远、李世政、钱齐等人。2024b. 寻找最佳实践以增强检索生成的效果。*arXiv预印本
    arXiv:2407.01219*。
- en: 'Wang et al. (2023b) Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian
    Ma, and Yitao Liang. 2023b. [Describe, explain, plan and select: Interactive planning
    with LLMs enables open-world multi-task agents](https://openreview.net/forum?id=KtvPdGb31Z).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人（2023b）王子豪、蔡绍飞、陈冠洲、刘安吉、马小健、梁一涛。2023b. [描述、解释、规划和选择：与LLM互动的规划使开放世界多任务代理成为可能](https://openreview.net/forum?id=KtvPdGb31Z)。在*第37届神经信息处理系统会议*。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. [Chain of thought
    prompting elicits reasoning in large language models](https://openreview.net/forum?id=_VjQlMeSB_J).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 魏等人（2022）魏杰森、王雪智、达尔·舒尔曼、马尔滕·博斯马、布赖恩·伊切特、夏飞、艾德·H·池、阮文理和邓尼·周。2022. [链式思维提示激发大型语言模型中的推理能力](https://openreview.net/forum?id=_VjQlMeSB_J)。在*神经信息处理系统进展*。
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 习等人（2023）习志恒、陈文祥、郭鑫、何伟、丁易文、洪博阳、张铭、王俊哲、金森杰、周恩宇等人。2023. 基于大型语言模型的代理崛起与潜力：一项调查。*arXiv预印本
    arXiv:2309.07864*。
- en: Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan,
    Junxian He, and Qizhe Xie. 2023. [Self-evaluation guided beam search for reasoning](https://openreview.net/forum?id=Bw82hwg5Q3).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谢等人（2023）谢宇曦、川口健司、赵怡然、赵旭、简敏艳、何俊贤、谢启哲。2023. [自评引导的束搜索推理方法](https://openreview.net/forum?id=Bw82hwg5Q3)。在*第37届神经信息处理系统会议*。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. [HotpotQA: A dataset
    for diverse, explainable multi-hop question answering](https://doi.org/10.18653/v1/D18-1259).
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pages 2369–2380, Brussels, Belgium. Association for Computational
    Linguistics.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2018）杨智麟、齐鹏、张赛征、约书亚·本吉奥、威廉·科恩、鲁斯兰·萨拉赫赫丁诺夫、克里斯托弗·D·曼宁。2018. [HotpotQA：一个多样化、可解释的多跳问题解答数据集](https://doi.org/10.18653/v1/D18-1259)。在*2018年自然语言处理实证方法会议论文集*，第2369–2380页，比利时布鲁塞尔。计算语言学会。
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. Webshop: Towards scalable real-world web interaction with grounded language
    agents. *Advances in Neural Information Processing Systems*, 35:20744–20757.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2022）姚顺宇、陈浩宇、杨约翰、卡尔蒂克·纳拉辛汉。2022。[Webshop：面向可扩展的真实世界网页交互与有根语言代理](https://doi.org/10.18653/v1/2022.findings-naacl.177)。*神经信息处理系统进展*，35:20744–20757。
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. [Tree of thoughts: Deliberate
    problem solving with large language models](https://arxiv.org/abs/2305.10601).
    *Preprint*, arXiv:2305.10601.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2023a）姚顺宇、俞典、赵杰弗里、沙弗兰·伊扎克、托马斯·L·格里菲斯、曹远、卡尔蒂克·纳拉辛汉。2023a。[思想树：在大语言模型中进行深思熟虑的问题解决](https://arxiv.org/abs/2305.10601)。*预印本*，arXiv:2305.10601。
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. [React: Synergizing reasoning and acting
    in language models](https://openreview.net/forum?id=WE_vluYUL-X). In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 姚等人（2023b）姚顺宇、赵杰弗里、俞典、杜楠、沙弗兰·伊扎克、卡尔蒂克·R·纳拉辛汉、曹远。2023b。[React：在语言模型中协同推理与行动](https://openreview.net/forum?id=WE_vluYUL-X)。发表于*第十一届国际学习表示会议*。
- en: Yu et al. (2023) Xiao Yu, Maximillian Chen, and Zhou Yu. 2023. [Prompt-based
    Monte-Carlo tree search for goal-oriented dialogue policy planning](https://doi.org/10.18653/v1/2023.emnlp-main.439).
    In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language
    Processing*, pages 7101–7125, Singapore. Association for Computational Linguistics.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 于等人（2023）于晓、陈马克西米连、于周。2023。[基于提示的蒙特卡洛树搜索用于目标导向对话策略规划](https://doi.org/10.18653/v1/2023.emnlp-main.439)。发表于*2023年自然语言处理实证方法会议论文集*，第7101–7125页，新加坡。计算语言学会。
- en: 'Yuksekgonul et al. (2024) Mert Yuksekgonul, Federico Bianchi, Joseph Boen,
    Sheng Liu, Zhi Huang, Carlos Guestrin, and James Zou. 2024. Textgrad: Automatic"
    differentiation" via text. *arXiv preprint arXiv:2406.07496*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 俞塞克贡尔等人（2024）梅尔特·俞塞克贡尔、费德里科·比安基、约瑟夫·博恩、刘盛、黄志、卡洛斯·格斯特林、詹姆斯·邹。2024。Textgrad：通过文本实现自动"微分"。*arXiv预印本arXiv:2406.07496*。
- en: 'Zhang et al. (2024) Peitian Zhang, Zheng Liu, Shitao Xiao, Zhicheng Dou, and
    Jian-Yun Nie. 2024. [A multi-task embedder for retrieval augmented LLMs](https://doi.org/10.18653/v1/2024.acl-long.194).
    In *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 3537–3553, Bangkok, Thailand. Association
    for Computational Linguistics.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人（2024）张佩天、刘峥、肖仕涛、窦志成、聂建云。2024。[用于检索增强型大语言模型的多任务嵌入器](https://doi.org/10.18653/v1/2024.acl-long.194)。发表于*第62届计算语言学会年会（第一卷：长篇论文）*，第3537–3553页，泰国曼谷。计算语言学会。
- en: Zhao et al. (2023) Zirui Zhao, Wee Sun Lee, and David Hsu. 2023. [Large language
    models as commonsense knowledge for large-scale task planning](https://openreview.net/forum?id=Wjp1AYB8lH).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人（2023）赵子锐、李伟孙、许大卫。2023。[大语言模型作为常识知识用于大规模任务规划](https://openreview.net/forum?id=Wjp1AYB8lH)。发表于*第37届神经信息处理系统大会*。
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人（2023）钟万俊、崔瑞翔、郭奕多、梁耀博、吕帅、王彦林、赛义德·阿敏、陈伟珠、段楠。2023. Agieval：一种以人为本的基准评估框架，用于评估基础模型。*arXiv预印本arXiv:2304.06364*。
- en: 'Zhong et al. (2022) Wanjun Zhong, Siyuan Wang, Duyu Tang, Zenan Xu, Daya Guo,
    Yining Chen, Jiahai Wang, Jian Yin, Ming Zhou, and Nan Duan. 2022. [Analytical
    reasoning of text](https://doi.org/10.18653/v1/2022.findings-naacl.177). In *Findings
    of the Association for Computational Linguistics: NAACL 2022*, pages 2306–2319,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人（2022）钟万俊、王思远、唐独宇、许泽南、郭大亚、陈逸宁、王家海、尹建、周铭、段楠。2022。[文本的分析推理](https://doi.org/10.18653/v1/2022.findings-naacl.177)。发表于*计算语言学会发现：NAACL
    2022*，第2306–2319页，美国西雅图。计算语言学会。
- en: Zhou et al. (2024a) Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang,
    and Yu-Xiong Wang. 2024a. [Language agent tree search unifies reasoning acting
    and planning in language models](https://openreview.net/forum?id=6LNTSrJjBe).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人（2024a）周安迪、颜凯、米哈伊·什拉潘托赫-罗斯曼、王浩瀚、王宇雄。2024a。[语言代理树搜索统一了大语言模型中的推理、行动与规划](https://openreview.net/forum?id=6LNTSrJjBe)。
- en: 'Zhou et al. (2024b) Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    and Graham Neubig. 2024b. [Webarena: A realistic web environment for building
    autonomous agents](https://openreview.net/forum?id=oKn9c6ytLx). In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2024b）Shuyan Zhou、Frank F. Xu、Hao Zhu、Xuhui Zhou、Robert Lo、Abishek
    Sridhar、Xianyi Cheng、Tianyue Ou、Yonatan Bisk、Daniel Fried、Uri Alon 和 Graham Neubig。2024b。[Webarena:
    用于构建自主体的现实Web环境](https://openreview.net/forum?id=oKn9c6ytLx)。发表于 *第十二届国际学习表征会议*。'
- en: '| Q: In a dance class of 20 students, $20\%$ enrolled in contemporary dance,
    $25\%$ of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop
    dance. What percentage of the entire students enrolled in hip-hop dance? A: Let’s
    first understand the problem and devise a plan to solve the problem.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '| 问：在一个有20名学生的舞蹈班里，$20\%$ 的学生报名参加了现代舞，剩余学生的$25\%$ 报名参加了爵士舞，其余的报名参加了街舞。那么，所有学生中有多少百分比报名参加了街舞？答：首先，让我们理解问题并制定解决问题的计划。'
- en: Then, let’s carry out the plan and solve the problem step by step.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们按照计划一步步解决问题。
- en: 'Plan: <svg class="ltx_picture" height="192.22" id="A0.T5.3.3.3.3.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,192.22) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">Step
    1: Calculate the total number of students who enrolled in contemporary and jazz
    dance.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 计划：<svg class="ltx_picture" height="192.22" id="A0.T5.3.3.3.3.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,192.22) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="163.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">第1步：计算报名参加现代舞和爵士舞的学生总数。
- en: 'Step 2: Calculate the total number of students who enrolled in hiphop dance.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：计算报名参加街舞的学生总数。
- en: 'Step 3: Calculate the percentage of students who enrolled in hiphop dance.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：计算报名参加街舞的学生百分比。
- en: 'Solution:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案：
- en: 'Step 1: $20\%$ of 20 students is 4 students. $25\%$ of the remaining 16 students
    is 4 students. So, a total of 8 students enrolled in contemporary and jazz dance.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：$20\%$ 的20名学生是4名学生。剩下的16名学生中$25\%$ 是4名学生。所以，总共有8名学生报名参加了现代舞和爵士舞。
- en: 'Step 2: The remaining 12 students enrolled in hip-hop dance.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：剩下的12名学生报名参加了街舞。
- en: 'Step 3: The percentage of students who enrolled in hip-hop dance is $12/20=60\%$.</foreignobject></g></g></svg>
    Therefore, the answer (arabic numerals) is <svg class="ltx_picture" height="40.09"
    id="A0.T5.4.4.4.4.p2.pic1" overflow="visible" version="1.1" width="603.94"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.09)
    matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000" height="11.15" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="559.25">60%</foreignobject></g></g></svg>
    |'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：报名参加街舞的学生百分比是 $12/20=60\%$。</foreignobject></g></g></svg> 因此，答案（阿拉伯数字）是
    <svg class="ltx_picture" height="40.09" id="A0.T5.4.4.4.4.p2.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,40.09) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="11.15" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">60%</foreignobject></g></g></svg>
    |
- en: 'Table 5: An LLM planner under NLIE-QA, implemented by zero-shot CoT. The CoT
    trigger is highlighted in red font. The content in the shadow box is generated
    by LLM. Source from Wang et al. ([2023a](https://arxiv.org/html/2406.05804v6#bib.bib39))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在 NLIE-QA 下的 LLM 规划器，采用零-shot CoT 实现。CoT 触发器以红色字体突出显示。阴影框中的内容由 LLM 生成。来源于
    Wang 等人（[2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)）
- en: '| Interact with a household to solve a task. Here are two examples. {Demo 1}'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '| 与一个家庭互动以解决任务。以下是两个示例。{Demo 1}'
- en: '{Demo 2}'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '{Demo 2}'
- en: 'Here is the task: You are in the middle of a room. Looking quickly around you,
    you see a cabinet 6, a cabinet 5, a cabinet 4, a cabinet 3, a cabinet 2, a cabinet
    1, a coffeemachine 1, a countertop 3, a countertop 2, a countertop 1, a drawer
    3, a drawer 2, a drawer 1, a fridge 1, a garbagecan 1, a microwave 1, a shelf
    3, a shelf 2, a shelf 1, a sinkbasin 1, a stoveburner 4, a stoveburner 3, a stoveburner
    2, a stoveburner 1, and a toaster 1\.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是：你在房间的中央。快速环顾四周，你看到了一个橱柜 6、一个橱柜 5、一个橱柜 4、一个橱柜 3、一个橱柜 2、一个橱柜 1、一个咖啡机 1、一个台面
    3、一个台面 2、一个台面 1、一个抽屉 3、一个抽屉 2、一个抽屉 1、一个冰箱 1、一个垃圾桶 1、一个微波炉 1、一个架子 3、一个架子 2、一个架子
    1、一个水槽 1、一个炉灶 4、一个炉灶 3、一个炉灶 2、一个炉灶 1 和一个烤面包机 1\。
- en: 'Your task is to: put a cool tomato in microwave.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是：将冷却的番茄放入微波炉。
- en: '<svg class="ltx_picture" height="55.16" id="A0.T6.1.1.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,55.16) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">think:
    To solve the task, I need to find a tomato, then cool it with the fridge, and
    finally put it in the microwave.</foreignobject></g></g></svg> OK.'
  id: totrans-213
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="55.16" id="A0.T6.1.1.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,55.16) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">思考：为了解决这个任务，我需要找到一个番茄，然后用冰箱将其冷却，最后放入微波炉。</foreignobject></g></g></svg>
    好的。
- en: '<svg class="ltx_picture" height="73.76" id="A0.T6.2.2.2.2.p2.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,73.76) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="44.82" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">think:
    First, I need to find a tomato. A tomato is more likely to be found on the countertop
    (1-3), cabinet (1-6), drawer (1-3), shelf (1-3), or the sinkbasin. I can start
    by checking the countertop 1.</foreignobject></g></g></svg> OK. > <svg class="ltx_picture"
    height="40.55" id="A0.T6.3.3.3.3.p3.pic1" overflow="visible" version="1.1" width="603.94"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.55)
    matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000" height="11.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="559.25">go to countertop 1</foreignobject></g></g></svg>
    On the countertop 1, you see a dishsponge 1, and a mug 2. … |'
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="73.76" id="A0.T6.2.2.2.2.p2.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,73.76) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="44.82" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">思考：首先，我需要找到一个番茄。番茄更可能出现在台面（1-3）、橱柜（1-6）、抽屉（1-3）、架子（1-3）或水槽里。我可以从检查台面
    1 开始。</foreignobject></g></g></svg> 好的。 > <svg class="ltx_picture" height="40.55"
    id="A0.T6.3.3.3.3.p3.pic1" overflow="visible" version="1.1" width="603.94"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,40.55)
    matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000" height="11.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="559.25">前往台面 1</foreignobject></g></g></svg>
    在台面 1 上，你看到一个洗碗海绵 1 和一个杯子 2。…
- en: 'Table 6: An LLM actor within base workflow under the Alfred World benchmark,
    implemented by few-shot CoT. Source from (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：在阿尔弗雷德世界基准测试中的基本工作流下的LLM模型，使用少量样本 CoT 实现。来源于 (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49))。
- en: '| Use the following tools at your disposal and answer some questions: |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 使用以下工具来解答一些问题： |'
- en: '| - <<Calculator>> |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| - <<计算器>> |'
- en: '| - <<Chemical reaction predictor>> |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| - <<化学反应预测器>> |'
- en: '| - <<Molar mass list>> |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| - <<摩尔质量列表>> |'
- en: '| Q: Find the amount of $\mathrm{H}2\mathrm{CO}3$ formed on combining 2 moles
    of $\mathrm{CO}2$ and 2 moles of $\mathrm{H}2\mathrm{O}$. |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 问题：通过将 2 摩尔的 $\mathrm{CO}2$ 和 2 摩尔的 $\mathrm{H}2\mathrm{O}$ 结合，形成多少摩尔的 $\mathrm{H}2\mathrm{CO}3$？
    |'
- en: '| A: First, use the chemical reaction predictor to determine the chemical equation
    for the reaction. |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| A: 首先，使用化学反应预测器确定反应的化学方程式。 |'
- en: '| Reactants: $2\mathrm{CO}2,2\mathrm{H}2\mathrm{O}$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 反应物：$2\mathrm{CO}2,2\mathrm{H}2\mathrm{O}$ |'
- en: '| Products: ?$\mathrm{H}2\mathrm{CO}3$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 产物：?$\mathrm{H}2\mathrm{CO}3$ |'
- en: '| <<Chemical reaction predictor>> |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| <<化学反应预测器>> |'
- en: '| $2\mathrm{CO}2+2\mathrm{H}2\mathrm{O}\rightarrow 2\mathrm{H}2\mathrm{CO}3$
    |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| $2\mathrm{CO}2+2\mathrm{H}2\mathrm{O}\rightarrow 2\mathrm{H}2\mathrm{CO}3$
    |'
- en: '| Second, use the molar mass list to determine the molar mass of $\mathrm{H}2\mathrm{CO}3$.
    $\mathrm{H}2\mathrm{CO}3$ <<Molar mass list>> $62\mathrm{~{}g}/\mathrm{mol}$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 其次，使用摩尔质量列表来确定$\mathrm{H}2\mathrm{CO}3$的摩尔质量。$\mathrm{H}2\mathrm{CO}3$ <<摩尔质量列表>>
    $62\mathrm{~{}g}/\mathrm{mol}$ |'
- en: '| Third, use the calculator to calculate the amount of 2 moles of $\mathrm{H}2\mathrm{CO}3$.
    $2\times 62=$ <<Calculator>> $124$ |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 第三，使用计算器计算2摩尔$\mathrm{H}2\mathrm{CO}3$的量。$2\times 62=$ <<计算器>> $124$ |'
- en: '| Therefore, $124\mathrm{~{}g}$ of $\mathrm{H}2\mathrm{CO}3$ is formed. |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 因此，形成了$124\mathrm{~{}g}$的$\mathrm{H}2\mathrm{CO}3$。 |'
- en: '| Q: Find the amount of Calcium hydroxide that is required to react with 2
    moles of Carbon dioxide to form 2 moles of Calcium carbonate along with 2 moles
    of Water. |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 问题：找出与2摩尔二氧化碳反应生成2摩尔碳酸钙和2摩尔水所需的氢氧化钙的量。 |'
- en: '| A: |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 答案： |'
- en: 'Table 7: An LLM actor within tool-use workflow. In-generation triggers are
    highlighted in red font. Source from MultiTool-CoT (Inaba et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib15))'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：工具使用工作流中的LLM演员。生成触发器以红色字体突出显示。来源：MultiTool-CoT（Inaba等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib15)）
- en: '| Solve a question answering task by having a Thought, then Finish with your
    answer. Thought can reason about the current situation. Finish[answer] returns
    the answer and finishes the task. |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 通过先进行思考，再给出答案来解决问题回答任务。思考可以推理当前情况。完成[答案]返回答案并完成任务。 |'
- en: '| Here are some examples: |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 下面是一些示例： |'
- en: '| Question 1: What is the elevation range for the area that the eastern sector
    of the Colorado orogeny extends into? |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 问题1：科罗拉多造山带东部地区的海拔范围是多少？ |'
- en: '| Thought: Let’s think step by step. The eastern sector of Colorado orogeny
    extends into the High Plains. High Plains rise in elevation from around 1,800
    to 7,000 ft, so the answer is 1,800 to 7,000 ft. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 思考：让我们一步一步思考。科罗拉多造山带东部地区延伸到高原。高原的海拔范围从大约1,800英尺到7,000英尺，因此答案是1,800到7,000英尺。
    |'
- en: '| Action: Finish[1,800 to 7,000 ft] |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 动作：完成[1,800到7,000英尺] |'
- en: '| … |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 6: Were Pavel Urysohn and Leonid Levin known for the same type of
    work? |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 问题6：帕维尔·乌里松和列昂尼德·列文是否以相同类型的工作而闻名？ |'
- en: '| Thought: Let’s think step by step. Pavel Urysohn is a mathematician. Leonid
    Levin is a mathematician and computer scientist. So Pavel Urysohn and Leonid Levin
    have the same type of work. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 思考：让我们一步一步思考。帕维尔·乌里松（Pavel Urysohn）是数学家。列昂尼德·列文（Leonid Levin）是数学家和计算机科学家。因此，帕维尔·乌里松和列昂尼德·列文从事相同类型的工作。
    |'
- en: '| Action: Finish[Yes] |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 动作：完成[是] |'
- en: '| END OF EXAMPLES) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 示例结束) |'
- en: '| Question: VIVA Media AG changed itś name in 2004\. What does their new acronym
    stand for? |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 问题：VIVA Media AG在2004年更改了名称。它们的新缩写代表什么？ |'
- en: '| Thought: <svg class="ltx_picture" height="92.59" id="A0.T8.1.1.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,92.59) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="63.65" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="559.25">Let’s think step by step. VIVA Media AG changed its name
    in 2004\. The new acronym must stand for the new name they adopted. To find the
    answer, we need to research the new name of VIVA Media AG after 2004.Action: Finish[Further
    research is needed to determine the new acronym.]</foreignobject></g></g></svg>
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 思考：<svg class="ltx_picture" height="92.59" id="A0.T8.1.1.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,92.59) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="63.65" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">让我们一步一步思考。VIVA
    Media AG在2004年更改了名称。新的缩写必须代表他们采用的新名称。为了找到答案，我们需要研究2004年后VIVA Media AG的新名称。动作：完成[需要进一步研究以确定新的缩写。]</foreignobject></g></g></svg>
    |'
- en: '| Action: <svg class="ltx_picture" height="42.78" id="A0.T8.3.3.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,42.78) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="13.84" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="559.25">Finish[Further research is needed to determine the new acronym.]</foreignobject></g></g></svg>
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 动作: <svg class="ltx_picture" height="42.78" id="A0.T8.3.3.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,42.78) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="13.84" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">完成[需要进一步研究以确定新的缩写词。]</foreignobject></g></g></svg>
    |'
- en: 'Table 8: An LLM actor within tool-use workflow for the HotPot-QA benchmark,
    implemented by few-Shot CoT. Source from ReAct (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 用于HotPot-QA基准测试的LLM演员，结合工具使用工作流，通过少量样本的CoT实现。来源于ReAct（Yao 等人，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)）。'
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question:" |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题，请将其分解为子问题。对于每个子问题，请用完整的句子回答，并以“答案是”结束。当原问题可以回答时，请以“现在我们可以回答这个问题了”开头。
    |'
- en: '| Question 1: James writes a 3-page letter to 2 different friends twice a week.
    How many pages does he write a year? |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1: 詹姆斯每周给两个不同的朋友写三页的信，每周写几页？ |'
- en: '| Question 1.1: How many pages does he write every week? |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1: 他每周写多少页？ |'
- en: '| Answer 1.1: James writes a 3-page letter to 2 different friends twice a week,
    so he writes 3 * 2 * 2 = 12 pages every week. The answer is 12. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.1: 詹姆斯每周给两个不同的朋友写三页的信，所以他每周写3 * 2 * 2 = 12页。答案是12。 |'
- en: '| Question 1.2: How many weeks are there in a year? |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2: 一年有多少周？ |'
- en: '| Answer 1.2: There are 52 weeks in a year. The answer is 52. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.2: 一年有52周。答案是52。 |'
- en: '| Question 1.3: Now we can answer the question: How many pages does he write
    a year? |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.3: 现在我们可以回答这个问题了：他一年写多少页？ |'
- en: '| Answer 1.3: James writes 12 pages every week, so he writes 12 * 52 = 624
    pages a year. The answer is 624. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.3: 詹姆斯每周写12页，因此他一年写12 * 52 = 624页。答案是624。 |'
- en: '| … |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5: 珍妮特的鸭子每天产16个鸡蛋。她每天早上吃三个鸡蛋，并为她的朋友做四个鸡蛋松饼。她每天在农贸市场上以每个2美元的价格卖掉剩余的鸡蛋。她每天在农贸市场上能赚多少钱？
    |'
- en: '| Question 5.1: <svg class="ltx_picture" height="55.16" id="A0.T9.1.1.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,55.16) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="559.25">How many eggs does Janet have left after eating three for
    breakfast and baking muffins with four?</foreignobject></g></g></svg> |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5.1: <svg class="ltx_picture" height="55.16" id="A0.T9.1.1.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,55.16) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="26.21" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="559.25">珍妮特早餐吃了三个鸡蛋，做了四个鸡蛋松饼后，还剩下多少个鸡蛋？</foreignobject></g></g></svg>
    |'
- en: 'Table 9: An LLM actor for the GSM8K benchmark. Source from Hao et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib11)).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9: 用于GSM8K基准测试的LLM演员。来源于Hao 等人（[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）。'
- en: Appendix A Examples Prompts
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 示例提示
- en: A.1 LLM-Profiled Policy Models
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 LLM-配置的策略模型
- en: '1) Base Workflow: Prompts for a planner and an actor are demonstrated in Table
    [5](https://arxiv.org/html/2406.05804v6#A0.T5 "Table 5 ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning") and [6](https://arxiv.org/html/2406.05804v6#A0.T6 "Table 6 ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning"), respectively. The content in the shadow box is generated
    by LLM. 2) Tool-Use Workflow: Actors under NLIE-QA is demonstrated in Table [7](https://arxiv.org/html/2406.05804v6#A0.T7
    "Table 7 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning") and [8](https://arxiv.org/html/2406.05804v6#A0.T8
    "Table 8 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning"). The former shows the implementation of
    in-generation triggers, while the last one demonstrates the ReAct implementation.
    3) Search Workflow: An actor is required during the expansion stage of MCTS in
    the RAP workflow. The prompts and expected generations are shown in Table [9](https://arxiv.org/html/2406.05804v6#A0.T9
    "Table 9 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning").'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 基础工作流：规划者和执行者的提示分别在表格[5](https://arxiv.org/html/2406.05804v6#A0.T5 "表格 5
    ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")和[6](https://arxiv.org/html/2406.05804v6#A0.T6
    "表格 6 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")中展示。阴影框中的内容由LLM生成。 2) 工具使用工作流：NLIE-QA下的执行者展示在表格[7](https://arxiv.org/html/2406.05804v6#A0.T7
    "表格 7 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")和[8](https://arxiv.org/html/2406.05804v6#A0.T8
    "表格 8 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")中。前者展示了生成触发器的实现，后者则展示了ReAct实现。
    3) 搜索工作流：在RAP工作流的MCTS扩展阶段需要执行者。提示和预期生成内容展示在表格[9](https://arxiv.org/html/2406.05804v6#A0.T9
    "表格 9 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")中。
- en: A.2 Prompts for LLM Evaluators
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 LLM评估者提示
- en: 'Table [10](https://arxiv.org/html/2406.05804v6#A1.T10 "Table 10 ‣ A.2 Prompts
    for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning")
    and [11](https://arxiv.org/html/2406.05804v6#A1.T11 "Table 11 ‣ A.2 Prompts for
    LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning")
    show LLM evaluators that are profiled as classification tasks, while Table [13](https://arxiv.org/html/2406.05804v6#A1.T13
    "Table 13 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning") demonstrates another to generate free-form text.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[10](https://arxiv.org/html/2406.05804v6#A1.T10 "表格 10 ‣ A.2 LLM评估者提示 ‣ 附录A
    示例提示 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")和[11](https://arxiv.org/html/2406.05804v6#A1.T11
    "表格 11 ‣ A.2 LLM评估者提示 ‣ 附录A 示例提示 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")展示了被归类为分类任务的LLM评估者，而表格[13](https://arxiv.org/html/2406.05804v6#A1.T13
    "表格 13 ‣ A.2 LLM评估者提示 ‣ 附录A 示例提示 ‣ 一项关于LLM基础代理的显著范式回顾：工具使用（包括RAG）、规划与反馈学习")则展示了生成自由格式文本的另一个任务。
- en: '| Given a question and some sub-questions, determine whether the last sub-question
    is useful to answer the question. Output ’Yes’ or ’No’, and a reason. |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题和一些子问题，判断最后一个子问题是否对回答该问题有用。输出“是”或“否”，并给出理由。 |'
- en: '| Question 1: Four years ago, Kody was only half as old as Mohamed. If Mohamed
    is currently twice as 30 years old, how old is Kody? |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1：四年前，Kody的年龄只有Mohamed的一半。如果Mohamed现在30岁，那么Kody几岁？ |'
- en: '| Question 1.1: How old is Mohamed? |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1：Mohamed几岁？ |'
- en: '| Question 1.2: How old was Mohamed four years ago? |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2：四年前，Mohamed几岁？ |'
- en: '| New question 1.3: How old was Kody four years ago? |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 新问题 1.3：四年前，Kody几岁？ |'
- en: '| Is the new question useful? Yes. We need the answer to calculate how old
    is Kody now. |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 这个新问题有用吗？是的。我们需要这个答案来计算Kody现在几岁。 |'
- en: '| … |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5：Janet 的鸭子每天下 16 个蛋。她每天早晨吃三个作为早餐，并用其中四个为朋友们烤松饼。她每天在农贸市场以每个新鲜鸭蛋 2 美元的价格将剩余的卖出。她每天在农贸市场赚多少钱？|'
- en: '| New question 5.1: Now we can answer the question: How much in dollars does
    she make every day at the farmers’ market? |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 新问题 5.1：现在我们可以回答这个问题：她每天在农贸市场赚多少钱？|'
- en: '| Is the new question useful? |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 新问题是否有用？|'
- en: 'Table 10: An LLM evaluator within simulation-based search workflow for NLIE-QA,
    implemented by few-shot CoT. It assesses the usefulness of new sub-questions in
    solving the original question. Source from Hao et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib11))'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：一个基于仿真搜索工作流的 LLM 评估器，用于 NLIE-QA，采用少量样本 CoT 实现。它评估新子问题在解决原始问题中的有用性。来源：Hao
    等人（[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）。
- en: '| Evaluate if given numbers can reach 24 (sure/likely/impossible) 10 14'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '| 评估给定的数字是否可以达到 24（确定/可能/不可能） 10 14'
- en: 10 + 14 = 24
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 10 + 14 = 24
- en: sure
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 确定
- en: …
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: 1 3 3
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 1 3 3
- en: 1 * 3 * 3 = 9
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 1 * 3 * 3 = 9
- en: (1 + 3) * 3 = 12
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: (1 + 3) * 3 = 12
- en: 1 3 3 are all too small
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 1 3 3 都太小
- en: impossible
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 不可能
- en: 11, 12 |
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 11，12 |
- en: 'Table 11: An LLM evaluator within Tree-of-Thought Workflow under Game 24, implemented
    by few-Shot CoT prompting. The LLM is profiled for multi-class classification.
    Source from ToT (Yao et al., [2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)).'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：在游戏 24 中，基于思维树工作流的 LLM 评估器，采用少量样本 CoT 提示法实现。该 LLM 针对多类分类进行了配置。来源：ToT（Yao
    等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib48)）。
- en: '| Context: … Sentence: …'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '| 上下文：… 句子：…'
- en: Is the sentence supported by the context above?
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 句子是否得到上述上下文的支持？
- en: 'Answer Yes or No: |'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是“是”还是“否”：
- en: 'Table 12: An LLM evaluator. The LLM is profiled for multi-class classification.
    Source from Manakul et al. ([2023](https://arxiv.org/html/2406.05804v6#bib.bib21)).'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：一个 LLM 评估器。该 LLM 针对多类分类进行了配置。来源：Manakul 等人（[2023](https://arxiv.org/html/2406.05804v6#bib.bib21)）。
- en: '| {Few-shot Demonstrations Omitted for Brevity} |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| {为简洁起见省略了少量示范} |'
- en: '| Question: Serianna is a band of what genre that combines elements of heavy
    metal and hardcore punk? |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 问题：Serianna 是什么风格的乐队，结合了重金属和硬核朋克的元素？|'
- en: '| Proposed Answer: Let’s think step by step. Serianna is a band of metalcore
    genre. Metalcore is a subgenre of heavy metal and hardcore punk. So Serianna is
    a band of heavy metal and hardcore punk. So the answer is: heavy metal and hardcore
    punk. |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 提议的答案：让我们一步步思考。Serianna 是一支金属核风格的乐队。金属核是重金属和硬核朋克的子类型。所以，Serianna 是一支重金属和硬核朋克风格的乐队。因此，答案是：重金属和硬核朋克。|'
- en: '| 1\. Plausibility: <svg class="ltx_picture" height="138.72" id="A1.T13.1.1.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,138.72) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="109.78" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="559.25">The question asks for the genre that combines elements
    of heavy metal and hardcore punk, and the answer is "heavy metal and hardcore
    punk", simply repeat the question. So it’s not plausible. 2\. Truthfulness: Let’s
    search the question in google: > Search Query: Serianna is a band of what genre
    that combines elements of heavy metal and hardcore punk? > Evidence:</foreignobject></g></g></svg>
    [Metalcore - Wikipedia] Metalcore is a fusion music genre that combines elements
    of extreme metal and hardcore punk. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 1. 可信度：<svg class="ltx_picture" height="138.72" id="A1.T13.1.1.1.1.p1.pic1"
    overflow="visible" version="1.1" width="603.94"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,138.72) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g
    fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject
    color="#000000" height="109.78" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="559.25">问题询问的是结合了重金属和硬核朋克元素的风格，答案是“重金属和硬核朋克”，只是重复了问题本身。因此，这不太可信。
    2. 真实性：让我们在谷歌上搜索这个问题：> 搜索查询：Serianna 是什么风格的乐队，结合了重金属和硬核朋克的元素？ > 证据：</foreignobject></g></g></svg>
    [金属核 - 维基百科] 金属核是一种融合了极端金属和硬核朋克元素的音乐风格。|'
- en: '| <svg class="ltx_picture" height="105.51" id="A1.T13.4.4.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,105.51) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="76.57" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">The
    evidence suggests that metalcore is a genre that combines elements of extreme
    metal and hardcore punk, as stated in the proposed answer. Let’s search the proposed
    answer in google: > Search Query: Serianna is a band of metalcore genre. > Evidence:</foreignobject></g></g></svg>
    [Serianna - Wikipedia] Serianna was a metalcore band from Madison, Wisconsin.
    The band formed in 2006… |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| <svg class="ltx_picture" height="105.51" id="A1.T13.4.4.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,105.51) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="76.57" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">证据表明，金属核是一种结合了极限金属和硬核朋克元素的类型，正如提出的答案所述。让我们在
    Google 中搜索提出的答案：> 搜索查询：Serianna 是一支金属核乐队。> 证据：</foreignobject></g></g></svg> [Serianna
    - 维基百科] Serianna 是一支来自威斯康星州麦迪逊的金属核乐队。该乐队成立于 2006 年…… |'
- en: '| <svg class="ltx_picture" height="107.66" id="A1.T13.7.7.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,107.66) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">The
    evidence suggests Serianna is a metalcore band, the proposed answer is correct
    about this. Above all, the proposed answer correctly identifies that Serianna
    is a band of the metalcore genre, which combines elements of heavy metal and hardcore
    punk. However, the final answer is not plausible since it just lists the genres
    that metalcore combines.</foreignobject></g></g></svg> |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| <svg class="ltx_picture" height="107.66" id="A1.T13.7.7.1.1.p1.pic1" overflow="visible"
    version="1.1" width="603.94"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,107.66) matrix(1 0 0 -1 0 0) translate(0,3.94)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 20.38 12.5)"><foreignobject color="#000000"
    height="78.72" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="559.25">证据表明，Serianna
    是一支金属核乐队，提出的答案是正确的。最重要的是，提出的答案正确地识别了 Serianna 是一支结合了重金属和硬核朋克元素的金属核乐队。然而，最终答案并不可信，因为它仅列出了金属核所结合的类型。</foreignobject></g></g></svg>
    |'
- en: 'Table 13: An LLM evaluator within the Feedback-Learning workflow (feedback
    from tools). In-generation triggers are highlighted in red font, and tool-generated
    content is highlighted in green font. Source from Gou et al. ([2024](https://arxiv.org/html/2406.05804v6#bib.bib8)).'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：反馈学习工作流中的 LLM 评估器（来自工具的反馈）。生成中的触发器以红色字体突出显示，工具生成的内容以绿色字体突出显示。来源：Gou 等人（[2024](https://arxiv.org/html/2406.05804v6#bib.bib8)）。
    |
- en: A.3 Prompts as Dynamic Models
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 动态模型提示
- en: 'Table [14](https://arxiv.org/html/2406.05804v6#A1.T14 "Table 14 ‣ A.3 Prompts
    as Dynamic Models ‣ Appendix A Examples Prompts ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning")
    is profiled as a dynamic model.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [14](https://arxiv.org/html/2406.05804v6#A1.T14 "表 14 ‣ A.3 动态模型提示 ‣ 附录 A
    示例提示 ‣ 基于 LLM 的代理：工具使用（包括 RAG）、规划和反馈学习的著名范式回顾") 被描述为动态模型。 |
- en: '| Given a question, please decompose it into sub-questions. For each sub-question,
    please answer it in a complete sentence, ending with "The answer is". When the
    original question is answerable, please start the subquestion with "Now we can
    answer the question: ". |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 给定一个问题，请将其分解为子问题。对于每个子问题，请用完整的句子回答，并以“答案是”结尾。当原问题可以回答时，请以“现在我们可以回答这个问题：”开头子问题。
    |'
- en: '| Question 1: Weng earns $12 an hour for babysitting. Yesterday, she just did
    50 minutes of babysitting. How much did she earn? |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1: Weng 为照看孩子每小时赚 $12。昨天，她只做了 50 分钟的照看工作。她赚了多少钱？ |'
- en: '| Question 1.1: How much does Weng earn per minute? |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.1: Weng 每分钟赚多少钱？ |'
- en: '| Answer 1.1: Since Weng earns $12 an hour for babysitting, she earns $12 /
    60 = $0.2 per minute. The answer is 0.2. |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.1: 由于 Weng 为照看孩子每小时赚 $12，她每分钟赚 $12 / 60 = $0.2。答案是 0.2。 |'
- en: '| Question 1.2: Now we can answer the question: How much did she earn? |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 问题 1.2: 现在我们可以回答这个问题：她赚了多少钱？ |'
- en: '| Answer 1.2: Working 50 minutes, she earned $0.2 x 50 = $10\. The answer is
    10. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 答案 1.2: 工作了 50 分钟，她赚了 $0.2 x 50 = $10。答案是 10。 |'
- en: '| … |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Question 5: Janet’s ducks lay 16 eggs per day. She eats three for breakfast
    every morning and bakes muffins for her friends every day with four. She sells
    the remainder at the farmers’ market daily for $2 per fresh duck egg. How much
    in dollars does she make every day at the farmers’ market? |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5: Janet的鸭子每天产16个蛋。她每天早上早餐吃三个，且每天用四个做松饼给她的朋友们吃。她每天在农贸市场以每个$2的价格卖掉剩余的鸭蛋。她每天在农贸市场赚多少钱？
    |'
- en: '| Question 5.1: How many eggs does Janet have left after eating three for breakfast
    and using four for muffins? |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 问题 5.1: Janet在早餐吃了三个并用四个做了松饼后，剩下多少个蛋？ |'
- en: '| Answer 5.1: |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 答案 5.1: |'
- en: 'Table 14: An LLM-Profiled Dynamic Model.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 14: 一个LLM配置动态模型。'
- en: Appendix B Prompting Methods of LLM-Profiled Roles
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLM角色配置提示方法
- en: 'The use of different prompting methods are summarized in Table [15](https://arxiv.org/html/2406.05804v6#A2.T15
    "Table 15 ‣ Appendix B Prompting Methods of LLM-Profiled Roles ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning") .'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 不同提示方法的使用总结见表格 [15](https://arxiv.org/html/2406.05804v6#A2.T15 "表格 15 ‣ 附录 B
    LLM角色配置提示方法 ‣ LLM驱动代理的突出范式综述：工具使用（包括RAG）、规划与反馈学习")。
- en: '|  | Prompting | Example Works | Example Prompts (in Appendix) |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | 提示 | 示例作品 | 示例提示（见附录） |'
- en: '| $\text{glm}_{\text{actor}}$ | Few-shot | ReAct (Yao et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)),
    Reflexion (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)),
    RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)), MultiTool-CoT
    (Inaba et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib15)) | Table
    [7](https://arxiv.org/html/2406.05804v6#A0.T7 "Table 7 ‣ A Review of Prominent
    Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback
    Learning"), [8](https://arxiv.org/html/2406.05804v6#A0.T8 "Table 8 ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning") |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{actor}}$ | 少量示例 | ReAct（Yao等，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib49)），Reflexion（Shinn等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)），RAP（Hao等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)），MultiTool-CoT（Inaba等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib15)）
    | 表格 [7](https://arxiv.org/html/2406.05804v6#A0.T7 "表格 7 ‣ LLM驱动代理的突出范式综述：工具使用（包括RAG）、规划与反馈学习")，
    [8](https://arxiv.org/html/2406.05804v6#A0.T8 "表格 8 ‣ LLM驱动代理的突出范式综述：工具使用（包括RAG）、规划与反馈学习")
    |'
- en: '| $\text{glm}_{\text{planner}}$ | Zero-shot | Plan-and-Solve (Wang et al.,
    [2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)), LLM Planner (Huang et al.,
    [2022](https://arxiv.org/html/2406.05804v6#bib.bib13)) | Table [5](https://arxiv.org/html/2406.05804v6#A0.T5
    "Table 5 ‣ A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including
    RAG), Planning, and Feedback Learning") |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{planner}}$ | 零示例 | 计划与解决（Wang等，[2023a](https://arxiv.org/html/2406.05804v6#bib.bib39)），LLM规划器（Huang等，[2022](https://arxiv.org/html/2406.05804v6#bib.bib13)）
    | 表格 [5](https://arxiv.org/html/2406.05804v6#A0.T5 "表格 5 ‣ LLM驱动代理的突出范式综述：工具使用（包括RAG）、规划与反馈学习")
    |'
- en: '|  | Few-shot | DEPS (Wang et al., [2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)),
    Planner-Actor-Reporter (Dasgupta et al., [2022](https://arxiv.org/html/2406.05804v6#bib.bib5))
    |  |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '|  | 少量示例 | DEPS（Wang等，[2023b](https://arxiv.org/html/2406.05804v6#bib.bib42)），Planner-Actor-Reporter（Dasgupta等，[2022](https://arxiv.org/html/2406.05804v6#bib.bib5)）
    |  |'
- en: '| $\text{glm}_{\text{evaluator}}$ | Few-shot | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11)),
    Tree-BeamSearch (Xie et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib45)),
    Reflexion (Shinn et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib29)),
    CRITIC (Gou et al., [2024](https://arxiv.org/html/2406.05804v6#bib.bib8)) | Table [10](https://arxiv.org/html/2406.05804v6#A1.T10
    "Table 10 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning"), [11](https://arxiv.org/html/2406.05804v6#A1.T11 "Table
    11 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review of
    Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning") |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{evaluator}}$ | 少量示例 | RAP（Hao 等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)），Tree-BeamSearch（Xie
    等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib45)），Reflexion（Shinn 等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib29)），CRITIC（Gou
    等，[2024](https://arxiv.org/html/2406.05804v6#bib.bib8)） | 表 [10](https://arxiv.org/html/2406.05804v6#A1.T10
    "Table 10 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review
    of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning")，[11](https://arxiv.org/html/2406.05804v6#A1.T11 "Table
    11 ‣ A.2 Prompts for LLM Evaluators ‣ Appendix A Examples Prompts ‣ A Review of
    Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning,
    and Feedback Learning") |'
- en: '| $\text{glm}_{\text{dynamic}}$ | Few-shot | RAP (Hao et al., [2023](https://arxiv.org/html/2406.05804v6#bib.bib11))
    | Table [14](https://arxiv.org/html/2406.05804v6#A1.T14 "Table 14 ‣ A.3 Prompts
    as Dynamic Models ‣ Appendix A Examples Prompts ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning")
    |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| $\text{glm}_{\text{dynamic}}$ | 少量示例 | RAP（Hao 等，[2023](https://arxiv.org/html/2406.05804v6#bib.bib11)）
    | 表 [14](https://arxiv.org/html/2406.05804v6#A1.T14 "Table 14 ‣ A.3 Prompts as
    Dynamic Models ‣ Appendix A Examples Prompts ‣ A Review of Prominent Paradigms
    for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning")
    |'
- en: 'Table 15: Prompting Methods of LLM-Profiled Roles'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 表 15：LLM 定义角色的提示方法
