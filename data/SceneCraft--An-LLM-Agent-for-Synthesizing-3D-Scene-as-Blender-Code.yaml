- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 12:49:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:49:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SceneCraft：用于合成3D场景为Blender代码的LLM代理
- en: 来源：[https://arxiv.org/html/2403.01248/](https://arxiv.org/html/2403.01248/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2403.01248/](https://arxiv.org/html/2403.01248/)
- en: Ziniu Hu    Ahmet Iscen    Aashi Jain    Thomas Kipf    Yisong Yue    David
    A. Ross    Cordelia Schmid    Alireza Fathi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ziniu Hu    Ahmet Iscen    Aashi Jain    Thomas Kipf    Yisong Yue    David
    A. Ross    Cordelia Schmid    Alireza Fathi
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper introduces SceneCraft, a Large Language Model (LLM) Agent converting
    text descriptions into Blender-executable Python scripts which render complex
    scenes with up to a hundred 3D assets. This process requires complex spatial planning
    and arrangement. We tackle these challenges through a combination of advanced
    abstraction, strategic planning, and library learning. SceneCraft first models
    a scene graph as a blueprint, detailing the spatial relationships among assets
    in the scene. SceneCraft then writes Python scripts based on this graph, translating
    relationships into numerical constraints for asset layout. Next, SceneCraft leverages
    the perceptual strengths of vision-language foundation models like GPT-V to analyze
    rendered images and iteratively refine the scene. On top of this process, SceneCraft
    features a library learning mechanism that compiles common script functions into
    a reusable library, facilitating continuous self-improvement without expensive
    LLM parameter tuning. Our evaluation demonstrates that SceneCraft surpasses existing
    LLM-based agents in rendering complex scenes, as shown by its adherence to constraints
    and favorable human assessments. We also showcase the broader application potential
    of SceneCraft by reconstructing detailed 3D scenes from the Sintel movie and guiding
    a video generative model with generated scenes as intermediary control signal.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了SceneCraft，一种大型语言模型（LLM）代理，它将文本描述转换为可执行的Blender Python脚本，从而渲染包含多达一百个3D资源的复杂场景。这个过程需要复杂的空间规划和排列。我们通过先进的抽象、战略规划和库学习相结合来应对这些挑战。SceneCraft首先将场景图建模为蓝图，详细描述场景中资产之间的空间关系。然后，SceneCraft基于该图编写Python脚本，将关系转化为资产布局的数值约束。接着，SceneCraft利用类似GPT-V的视觉-语言基础模型的感知优势，分析渲染图像并迭代地优化场景。基于这一过程，SceneCraft还具备一个库学习机制，将常见的脚本函数汇编成可重用的库，促进持续自我改进，而无需昂贵的LLM参数调优。我们的评估表明，SceneCraft在渲染复杂场景方面超越了现有的基于LLM的代理，这从其对约束的遵循和良好的人工评估中得到了体现。我们还通过重建Sintel电影中的详细3D场景，并利用生成的场景作为中介控制信号，指导视频生成模型，展示了SceneCraft更广泛的应用潜力。
- en: 'Machine Learning, ICML¹¹1^*Correspondence to: Ziniu Hu $<$ziniu@google.com$>$,
    Yisong Yue $<$yyue@caltech.edu$>$ and Alireza Fathi $<$alirezafathi@google.com$>$.![Refer
    to caption](img/cc6db6fc463b3cd37a3ccdfe661b0083.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习，ICML¹¹1^*通讯作者：Ziniu Hu $<$ziniu@google.com$>$，Yisong Yue $<$yyue@caltech.edu$>$
    和 Alireza Fathi $<$alirezafathi@google.com$>$。![参考标题](img/cc6db6fc463b3cd37a3ccdfe661b0083.png)
- en: 'Figure 1: Examples comparing SceneCraft’s output against a BlenderGPT baseline
    for different queries.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：比较SceneCraft输出与BlenderGPT基线在不同查询上的示例。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Transforming natural language descriptions into 3D scenes is a key technology
    for industries like architectural design, game development, virtual reality, and
    cinematic production. Recent 3D generative models like DreamFusion (Poole et al.,
    [2022](https://arxiv.org/html/2403.01248v1#bib.bib19)) and Magic3D (Lin et al.,
    [2023](https://arxiv.org/html/2403.01248v1#bib.bib11)) have made great progress
    in transforming text to a 3D neural representation of an object. However, these
    works fall short of composing entire scenes with multiple assets due to dataset
    scale limitations and domain specificity. In this work, we are inspired by how
    human artists typically adopt a holistic process for designing 3D scenes, where
    they take an iterative, step-by-step approach that includes storyboarding, 3D
    modeling, texturing, rigging, layout, animation, and rendering, using professional
    software such as Blender²²2[https://www.blender.org/](https://www.blender.org/).
    This iterative process grants the artists in studios a nuanced control over each
    asset’s placement and movement — a level of control not yet achieved by existing
    models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 将自然语言描述转化为 3D 场景是建筑设计、游戏开发、虚拟现实和电影制作等行业的关键技术。近期的 3D 生成模型，如 DreamFusion（Poole
    et al., [2022](https://arxiv.org/html/2403.01248v1#bib.bib19)）和 Magic3D（Lin et
    al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib11)）在将文本转化为 3D 神经表示方面取得了巨大进展。然而，由于数据集规模的限制和领域特异性，这些工作未能在构建包含多个资产的完整场景方面取得突破。在这项工作中，我们受到人类艺术家通常采用整体设计过程的启发，在这一过程中，他们采取逐步迭代的方式，包括故事板、3D
    建模、纹理制作、装配、布局、动画和渲染，使用专业软件如 Blender²²2[https://www.blender.org/](https://www.blender.org/)。这种迭代过程为工作室中的艺术家提供了对每个资产的摆放和运动的细致控制——这种控制程度是现有模型尚未实现的。
- en: Our paper introduces SceneCraft, an LLM-powered agent that is designed to streamline
    this text-to-3D scene conversion process, closely emulating the workflow of studio
    artists. SceneCraft transforms textual descriptions into executable Blender code,
    rendering 3D scenes that are visually cohesive and contextually accurate. This
    task goes beyond mere data processing, demanding a nuanced understanding of spatial
    and semantic relationships, which remains a challenge even for today’s LLMs. While
    earlier systems like WordsEye (Coyne & Sproat, [2001](https://arxiv.org/html/2403.01248v1#bib.bib7))
    and SceneSere (Chang et al., [2017](https://arxiv.org/html/2403.01248v1#bib.bib5))
    have made progress towards using predefined templates and rules to extract spatial
    constraints from linguistic queries, they depend on extensive human input, especially
    in new domains with unique relationship patterns.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文介绍了 SceneCraft，这是一个由 LLM 驱动的智能体，旨在简化文本到 3D 场景的转换过程，紧密模仿工作室艺术家的工作流程。SceneCraft
    将文本描述转化为可执行的 Blender 代码，渲染出视觉上连贯且语境准确的 3D 场景。这个任务不仅仅是数据处理，还需要对空间和语义关系有细致的理解，即便是今天的
    LLM 也仍然面临这一挑战。尽管早期的系统如 WordsEye（Coyne & Sproat, [2001](https://arxiv.org/html/2403.01248v1#bib.bib7)）和
    SceneSere（Chang et al., [2017](https://arxiv.org/html/2403.01248v1#bib.bib5)）通过使用预定义模板和规则从语言查询中提取空间约束方面取得了一定进展，但它们仍依赖大量的人工输入，尤其是在具有独特关系模式的新领域中。
- en: '![Refer to caption](img/2b087aefa85791f61515370543537844.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2b087aefa85791f61515370543537844.png)'
- en: 'Figure 2: SceneCraft is composed of a dual-loop self-improving pipeline: in
    the inner-loop, per each scene, an LLM autonomously writes a script to interact
    with Blender, receives rendered image, and keeps improving the script until getting
    good scenes; in the outer-loop, SceneCraft summarizes common functions over a
    batch of written scripts to maintain a reusable design skill library.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：SceneCraft 由一个双循环自我改进的管道组成：在内循环中，对于每个场景，LLM 会自主编写脚本与 Blender 进行交互，接收渲染后的图像，并不断改进脚本，直到获得良好的场景；在外循环中，SceneCraft
    会总结一批已编写脚本的共性功能，以维持一个可重用的设计技能库。
- en: 'SceneCraft leverages LLMs to autonomously generate Python code, translating
    spatial relations within scenes into precise numerical constraints. To achieve
    this, the core of SceneCraft is a dual-loop optimization pipeline, illustrated
    in Figure [2](https://arxiv.org/html/2403.01248v1#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"). The inner-loop
    focuses on per-scene layout optimization. Here, an LLM-based planner constructs
    a scene graph outlining the spatial constraints for asset arrangement. SceneCraft
    then writes Python code to transform these relations into numerical constraints.
    These constraints are fed to a specialized solver that determines the layout parameters
    of each asset, including location, orientation and sizes. After rendering these
    scripts into images via Blender, we utilize a multimodal LLM (GPT-V (OpenAI, [2023](https://arxiv.org/html/2403.01248v1#bib.bib16)))
    to assess the alignment between the generated image and the textual description.
    If a misalignment is detected, the LLM identifies the problematic semantic relations
    and corresponding constraints, subsequently refining the scripts. This iterative
    process of refinement and feedback is crucial for enhancing the scene’s fidelity,
    ensuring each rendition progressively aligns more closely with the original vision,
    which also matches more the human artists’ designing process.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft利用大语言模型（LLMs）自动生成Python代码，将场景中的空间关系转化为精确的数值约束。为了实现这一点，SceneCraft的核心是一个双环优化管道，如图[2](https://arxiv.org/html/2403.01248v1#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene
    as Blender Code")所示。内环专注于每个场景的布局优化。在这里，基于LLM的规划器构建了一个场景图，概述了资产排列的空间约束。然后，SceneCraft编写Python代码，将这些关系转化为数值约束。这些约束被输入到一个专门的求解器中，求解器确定每个资产的布局参数，包括位置、方向和大小。在通过Blender将这些脚本渲染成图像后，我们利用多模态LLM（GPT-V (OpenAI,
    [2023](https://arxiv.org/html/2403.01248v1#bib.bib16)))来评估生成图像与文本描述之间的对齐情况。如果发现对齐不准确，LLM会识别出有问题的语义关系和相应的约束，并随后对脚本进行优化。这个迭代的精炼与反馈过程对于提高场景的真实性至关重要，确保每次渲染都能越来越接近原始设想，也更符合人类艺术家的设计过程。'
- en: Following the inner-loop refinement of scene scripts, SceneCraft starts its
    outer loop to dynamically expand its ’spatial skill’ library. Within this procedure,
    it reviews the incremental changes made to the constraint scripts across inner-loop
    iterations for each scene, identifying and integrating common code patterns, thereby
    streamlining the acquisition of new non-parametric skills for self-improvement.
    For instance, if the text query describes a lamp placed on a desk, but the initial
    rendering shows desk lamps floating mid-air, SceneCraft may learn to introduce
    a new ”grounded” constraint between lamp and desk surfaces. By continuously updating
    its library through such outer-loop learning over batches, SceneCraft acquires
    an expanding repertoire of spatial skills over time. SceneCraft is therefore able
    to handle increasingly complex scenes and descriptions without external human
    expertise or LLM parameter tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成内环的场景脚本精炼后，SceneCraft启动外环，动态扩展其“空间技能”库。在此过程中，它回顾每个场景在内环迭代中对约束脚本所做的增量变化，识别并整合常见的代码模式，从而简化了自我提升过程中获取新非参数化技能的过程。例如，如果文本查询描述了一盏放置在桌子上的台灯，但初始渲染显示桌灯漂浮在空中，SceneCraft可能会学会在台灯和桌面之间引入一个新的“固定”约束。通过在批次之间持续更新其库，SceneCraft通过外环学习不断积累空间技能。因此，SceneCraft能够处理越来越复杂的场景和描述，而无需外部人工专业知识或LLM参数调优。
- en: To evaluate SceneCraft, we conduct comprehensive experiments on both synthetic
    and real-world datasets. First, we create our own curated datasets with ground-truth
    spatial constraints to quantify SceneCraft’s fidelity in translating text to constraint
    scripts. Second, we apply SceneCraft to the Sintel movie dataset by finetuning
    a video generative model on the first half of the movie conditioned on ground-truth
    scene images. For the second half, we generate scenes using SceneCraft and other
    baselines as input to the video model. Across datasets, results demonstrate SceneCraft’s
    superior sample efficiency and accuracy in rendering intricate 3D scenes from
    textual descriptions, enabled by its dual-loop optimization. Quantitatively, SceneCraft
    achieves over 45.1% and 40.9% improvement on generated scenes’ CLIP score, compared
    with another popular LLM agent baseline BlenderGPT, over both unseen synthetic
    queries and real-world movies like Sintel. SceneCraft also achieves significantly
    better constraint passing score (88.9 against 5.6). Qualitatively, the scenes
    and videos generated using SceneCraft more accurately encapsulate the narrative
    and artistic nuances described in the text. It receives much higher human preference
    ratings on different perspective, and also benefit a video generative model with
    very light fine-tuning. Together, our comprehensive evaluation validates SceneCraft
    as an adaptable and efficient framework for translating imaginative text to 3D
    reality while continuously improving itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估SceneCraft，我们在合成数据集和真实世界数据集上进行了全面的实验。首先，我们创建了自己策划的数据集，并加入了真实空间约束，用于量化SceneCraft在将文本转化为约束脚本方面的可靠性。其次，我们将SceneCraft应用于Sintel电影数据集，通过对电影前半部分进行微调，使视频生成模型基于真实场景图像进行训练。对于后半部分，我们使用SceneCraft和其他基准方法作为输入，生成场景并供视频模型使用。在各数据集上的结果表明，得益于其双环优化，SceneCraft在从文本描述中渲染复杂的3D场景时，展现出了卓越的样本效率和准确性。从量化结果来看，SceneCraft在生成场景的CLIP得分上，比另一流行的LLM代理基准BlenderGPT提高了45.1%和40.9%，无论是在未见过的合成查询还是像Sintel这样的真实世界电影中。SceneCraft还显著提高了约束通过率（88.9对比5.6）。从定性结果来看，使用SceneCraft生成的场景和视频更准确地呈现了文本中描述的叙事和艺术细节。它在不同角度上的人类偏好评分远高于其他方法，同时也能通过极轻的微调，提升视频生成模型的效果。综上所述，我们的综合评估验证了SceneCraft作为一种适应性强、效率高的框架，在将富有创意的文本转化为3D现实的同时，不断自我优化。
- en: 'This paper’s contributions are:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献包括：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An LLM Agent that transforms an input text query into a 3D scene by generating
    a Blender script. The script is iteratively improved by a multimodal LLM that
    identifies unsatisfied constraints and fixes them in a feedback loop.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个LLM代理，将输入的文本查询转化为3D场景，通过生成Blender脚本来实现。该脚本通过多模态LLM不断改进，LLM识别未满足的约束，并在反馈循环中进行修复。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A spatial skill library learned given a set of synthetic input queries without
    requiring human involvement and LLM fine-tuning, resulting in improved scene generation
    results.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过一组合成输入查询学习到的空间技能库，无需人工干预或LLM微调，从而改善了场景生成的结果。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Experimental results show that comparing with BlenderGPT, another LLM-based
    agent baseline, SceneCraft achieves 45.1% and 40.9% improvement on generated scenes’
    CLIP score, over both unseen synthetic queries and real-world movies like Sintel.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验结果表明，与另一个LLM代理基准BlenderGPT相比，SceneCraft在生成场景的CLIP得分上提高了45.1%和40.9%，无论是在未见过的合成查询还是像Sintel这样的真实世界电影中。
- en: '![Refer to caption](img/25c479e222d1f11ea9d55dc2c3527840.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/25c479e222d1f11ea9d55dc2c3527840.png)'
- en: 'Figure 3: The workflow of SceneCraft’s inner-loop improvement of each scene.
    1) given query, a LLM writes a list of assets descriptions, then use CLIP retriever
    to fetch assets; 2) then LLM decomposes the full query into a sequence of sub-scene,
    each associated with a subset of assets and a text description; 3) a LLM-Planner
    generate a relational graph linking assets to spatial relationship; 4) Based on
    the graph, LLM-Coder writes python codes to get a list of numerical constraints,
    which can be executed to search optimal layout, and render into image using Blender;
    5) LLM-Reviewer with vision perception capability criticize the rendered image,
    and update the script accordingly. This critic-and-revise procedure can be done
    multiple times to iteratively improve the script and scene.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：SceneCraft内部循环改进每个场景的工作流程。1）给定查询，LLM编写一份资产描述列表，然后使用CLIP检索器来获取资产；2）然后LLM将完整查询分解为一系列子场景，每个子场景与一部分资产和一个文本描述相关联；3）LLM-规划器生成一个将资产与空间关系链接的关系图；4）基于该图，LLM-编码器编写Python代码，获取一组数值约束，可以执行以搜索最优布局，并使用Blender渲染成图像；5）具备视觉感知能力的LLM-审阅器批评渲染的图像，并相应地更新脚本。这个批评与修订过程可以多次进行，以迭代改进脚本和场景。
- en: 2 Approach
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法
- en: Our goal is to transform a text query $q$ into a 3D scene $s$ that is not only
    spatially coherent but also contextually rich and aesthetically pleasing. This
    requires (a) identifying the correct spatial and contextual relationships between
    assets, and (b) predicting a high fidelity and nice looking arrangement that aligns
    with these relationships.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是将一个文本查询$q$转化为一个3D场景$s$，该场景不仅在空间上是连贯的，而且在语境上丰富且具有美学吸引力。这需要（a）识别资产之间的正确空间和语境关系，以及（b）预测一个高保真且美观的布局，符合这些关系。
- en: SceneCraft performs this task by building on top of a state-of-the-art multimodal
    LLM (i.e., GPT-4V (OpenAI, [2023](https://arxiv.org/html/2403.01248v1#bib.bib16)))
    and a professional rendering software (Blender). We now describe the key components
    of our method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft通过构建在最先进的多模态LLM（即GPT-4V (OpenAI, [2023](https://arxiv.org/html/2403.01248v1#bib.bib16)))和专业渲染软件（Blender）之上来执行这一任务。接下来我们将描述我们方法的关键组件。
- en: 2.1 Asset Retrieval and Scene Decomposition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 资产检索与场景解构
- en: A scene consists of a set of assets, where each asset $a_{i}$ is a 3D model.
    Given the input text query $q$, the agent makes an LLM call to generate a list
    of asset names and description that shall be put in the scene. Based on them,
    a set of 3D assets ${\mathcal{A}}$ are retrieved from a large repository of 3D
    objects utilizing a CLIP-based retriever. The retrieval process first finds the
    top-10 assets based on the text description of each asset. Then each retrieved
    asset is rendered as an image and the one with the highest text-to-image score
    is selected.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个场景由一组资产组成，其中每个资产$a_{i}$是一个3D模型。给定输入的文本查询$q$，代理调用LLM生成一份资产名称和描述列表，这些资产将被放置在场景中。基于这些信息，使用基于CLIP的检索器从一个大型3D物体库中检索出一组3D资产${\mathcal{A}}$。检索过程首先根据每个资产的文本描述找到前10个最相关的资产。然后，每个检索到的资产都会被渲染成图像，选择文本与图像匹配得分最高的那个。
- en: Some scenes might contain up to a hundred assets, making the layout planning
    very difficult. Therefore, SceneCraft agent decomposes the scene into a set of
    sub-scenes, each representing a part of the entire scene. Breaking the problem
    into small pieces is a widely adopted strategy in natural language question answering (Perez
    et al., [2020](https://arxiv.org/html/2403.01248v1#bib.bib18)) and general reasoning (Zhou
    et al., [2023a](https://arxiv.org/html/2403.01248v1#bib.bib34)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一些场景可能包含多达一百个资产，使得布局规划非常困难。因此，SceneCraft代理将场景分解为一组子场景，每个子场景代表整个场景的一部分。将问题分解成小块是自然语言问答（Perez等人，[2020](https://arxiv.org/html/2403.01248v1#bib.bib18)）和一般推理（Zhou等人，[2023a](https://arxiv.org/html/2403.01248v1#bib.bib34)）中广泛采用的策略。
- en: The agent calls a LLM-empowered decomposer that breaks the input query into
    a sequence of sub-scenes $\hat{s}_{k}$, each containing a title, a list of asset
    names $\mathcal{A}_{k}$ and a sub-scene description $q_{k}$. The scene descriptions
    are used to guide the scene optimization in the later stages.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 该代理调用一个由LLM驱动的解构器，将输入的查询分解为一系列子场景$\hat{s}_{k}$，每个子场景包含一个标题、一个资产名称列表$\mathcal{A}_{k}$以及一个子场景描述$q_{k}$。这些场景描述将用于后续阶段的场景优化。
- en: '|  | $\displaystyle(q_{1},\mathcal{A}_{1}),\ldots,(q_{K},\mathcal{A}_{K})\leftarrow%
    \texttt{LLM-decomposer}(q).$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(q_{1},\mathcal{A}_{1}),\ldots,(q_{K},\mathcal{A}_{K})\leftarrow%
    \texttt{LLM-decomposer}(q).$ |  | (1) |'
- en: 'As an example shown in Figure [3](https://arxiv.org/html/2403.01248v1#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene
    as Blender Code"), given a query ”a girl hunter walking in a slum village with
    fantasy creatures”, SceneCraft decomposes it into three different steps, among
    which the first step includes the following information:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[3](https://arxiv.org/html/2403.01248v1#S1.F3 "图3 ‣ 1 引言 ‣ SceneCraft: 一种生成3D场景的LLM代理")所示，给定一个查询“一个女孩猎人在一个贫民窟村庄里走，周围有奇幻生物”，SceneCraft将其分解为三个不同的步骤，其中第一步包括以下信息：'
- en: '![[Uncaptioned image]](img/ae89fd2bcb4e168d2e55a56a06a5794d.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![[未加说明的图像]](img/ae89fd2bcb4e168d2e55a56a06a5794d.png)'
- en: 2.2 Scene Graph Construction
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 场景图构建
- en: In order to put the 3D assets together to create a scene $s$, each asset $a_{i}$
    requires its corresponding layout matrix $\mathcal{L}(a_{i})$, encapsulating the
    position, scale and orientation of $a_{i}$ in the scene’s coordinate frame. For
    instance, in a scene described as “a round table with a vase on it, placed near
    a window”, the assets $a_{i}$, $a_{j}$ and $a_{k}$ could represent the ‘table’,
    ‘vase’ and the ‘window’, respectively. Their layout matrices $\mathcal{L}(a_{i})$,
    $\mathcal{L}(a_{j})$ and $\mathcal{L}(a_{k})$ shall position the vase, table and
    window in the scene such that the vase is standing on the table and the table
    is located near the window.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将3D资产组合在一起创建场景$s$，每个资产$a_{i}$都需要其相应的布局矩阵$\mathcal{L}(a_{i})$，该矩阵封装了$a_{i}$在场景坐标系中的位置、缩放和方向。例如，在一个描述为“圆桌上放着一个花瓶，桌子靠近窗户”的场景中，资产$a_{i}$、$a_{j}$和$a_{k}$分别表示“桌子”、“花瓶”和“窗户”。它们的布局矩阵$\mathcal{L}(a_{i})$、$\mathcal{L}(a_{j})$和$\mathcal{L}(a_{k})$将花瓶、桌子和窗户在场景中定位，使得花瓶放在桌子上，桌子靠近窗户。
- en: The key challenge is to correctly put each asset in the right location and orientation
    by predicting the layout matrix $\mathcal{L}(a_{i})$ for each asset. The naive
    approach is to directly predict all the layout matrices directly given the scene
    description. However, this is a highly complex task even for the most advanced
    LLMs, due to the vast combinatorial space of the possible layouts and deep understanding
    the intricate spatial relations between assets. This is why in SceneCraft we use
    a relational scene graph as an intermediate layer of abstraction.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 关键挑战在于通过预测每个资产的布局矩阵$\mathcal{L}(a_{i})$，将每个资产正确地放置在正确的位置和方向。直接的做法是根据场景描述直接预测所有布局矩阵。然而，即使对于最先进的LLM，这也是一项高度复杂的任务，因为可能的布局组合空间非常庞大，而且需要深刻理解资产之间复杂的空间关系。这就是为什么在SceneCraft中我们使用关系场景图作为中间抽象层的原因。
- en: 'To model the spatial relations between assets, SceneCraft utilizes a set of
    spatial and contextual relations, such as proximity, alignment, parallelism, etc.
    Each relation $r$ applies to a specific set of assets within the scene. Full list
    of relations that we consider can be found in Sec [B](https://arxiv.org/html/2403.01248v1#A2
    "Appendix B List of relationships ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") in Appendix.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '为了模拟资产之间的空间关系，SceneCraft利用了一组空间和上下文关系，如接近性、对齐、平行性等。每个关系$r$适用于场景中的特定资产集合。我们考虑的关系的完整列表可以在附录[B](https://arxiv.org/html/2403.01248v1#A2
    "附录B 关系列表 ‣ SceneCraft: 一种生成3D场景的LLM代理")中找到。'
- en: 'Using these relations, the scene $s$ is abstracted into a relational bipartite
    graph $G(s)=(\mathcal{A},\mathcal{R},\mathcal{E})$, which contains two types of
    nodes: $\mathcal{A}$ represents the set of assets and $\mathcal{R}$ represents
    the set of relations as nodes. $\mathcal{E}$ represents the edges connecting a
    relation node to a subset of assets $\mathcal{E}(r)$ in the scene that satisfies
    this relation.³³3For each relation type, we can have multiple relation nodes linking
    to different subsets of assets, e.g., Align-1 relation node links table and vase,
    and Align-2 links vase and window.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些关系，场景$s$被抽象为一个关系二分图$G(s)=(\mathcal{A},\mathcal{R},\mathcal{E})$，其中包含两种类型的节点：$\mathcal{A}$表示资产集合，$\mathcal{R}$表示关系集合节点。$\mathcal{E}$表示连接关系节点与场景中满足该关系的资产子集$\mathcal{E}(r)$的边。³³3对于每种关系类型，我们可以有多个关系节点，分别连接到不同的资产子集，例如，Align-1关系节点连接桌子和花瓶，而Align-2关系节点连接花瓶和窗户。
- en: Based on this definition, SceneCraft then uses a LLM-Planner to construct a
    scene graph connecting assets to corresponding spatial relation nodes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个定义，SceneCraft接着使用LLM-规划器来构建一个场景图，将资产与相应的空间关系节点连接起来。
- en: '|  | $\displaystyle\mathcal{G}(s)=(\mathcal{A},\mathcal{R},\mathcal{E})\leftarrow%
    \texttt{LLM-Planner}(q_{k},\mathcal{A})$ |  | (2) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{G}(s)=(\mathcal{A},\mathcal{R},\mathcal{E})\leftarrow%
    \texttt{LLM-Planner}(q_{k},\mathcal{A})$ |  | (2) |'
- en: 'For example, when we create the outline of slum village, LLM-Planner predicts
    the following edges:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们创建贫民窟村庄的轮廓时，LLM-Planner预测了以下边：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$Alignment, list of houses$\rangle$: all housess are aligned side by
    side to form a side-street;'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$对齐，房屋列表$\rangle$: 所有房屋并排对齐，形成一条小街；'
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$Parallelism, two list of houses$\rangle$: Duplicate one side of street
    to form a pathway or road;'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$平行性，两排房屋$\rangle$: 复制街道一侧以形成一个通道或道路；'
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$\langle$Proximity, each lamp, each house$\rangle$: lamps are located in front
    of each house.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$\langle$接近性，每个灯，每栋房屋$\rangle$: 灯位于每栋房屋前面。'
- en: The relations between the assets provide soft spatial constraints for the layout
    matrices $\mathcal{L}$ of the assets. Thus, this intermediate graph serves as
    a high-level plan for subsequent code generation and self-improvement, which significantly
    reduces the complexity of arranging the assets in the scene.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 资产之间的关系为资产的布局矩阵$\mathcal{L}$提供了柔性空间约束。因此，这个中间图作为后续代码生成和自我改进的高层次规划，显著降低了在场景中排列资产的复杂度。
- en: 2.3 Scene Layout Optimization in a Feedback Loop
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 场景布局优化中的反馈循环
- en: 'After we obtain the spatial constraints between the assets, we use a set of
    scoring functions (one per relation) to optimize the scene layout. In Sec. [2.4](https://arxiv.org/html/2403.01248v1#S2.SS4
    "2.4 Library Learning ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") we describe how we learn the library of scoring functions
    automatically. The scoring function $F_{r}(\cdot)$ for relation $r$ captures whether
    the constraint is satisfied via $F_{r}\big{(}\{\mathcal{L}(a_{i})\mid a_{i}\in\mathcal{E}(r)\big{\}},\texttt{%
    arg}_{r})$. $F_{r}$ takes as input a list of layout matrices $\mathcal{L}(a_{i})$
    connected by $r$, as well as the function arguments $\texttt{arg}_{r}$ such as
    the distance, directions, etc. $F_{r}$ returns a real number between 0 and 1 describing
    how much this relational constraint is satisfied.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '在获得资产之间的空间约束后，我们使用一组评分函数（每个关系一个）来优化场景布局。在第[2.4节](https://arxiv.org/html/2403.01248v1#S2.SS4
    "2.4 Library Learning ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")中，我们描述了如何自动学习评分函数的库。关系$r$的评分函数$F_{r}(\cdot)$用于捕捉约束是否得到满足，表达式为$F_{r}\big{(}\{\mathcal{L}(a_{i})\mid
    a_{i}\in\mathcal{E}(r)\big{\}},\texttt{% arg}_{r})$。$F_{r}$的输入包括由$r$连接的布局矩阵$\mathcal{L}(a_{i})$列表，以及如距离、方向等函数参数$\texttt{arg}_{r}$。$F_{r}$返回一个介于0和1之间的实数，描述该关系约束满足的程度。'
- en: An LLM-Coder then reuses these existing functions stored in skill library to
    synthesize an overall Blender code script code, including loading the assets,
    doing grouping and generating all the numerical constraints, etc. The LLM-Coder
    will also predict all the arguments $\texttt{arg}_{r}$ for each function $F_{r}$,
    such as the exact distance for Proximity relation, etc.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: LLM-Coder随后会重用这些存储在技能库中的现有函数来合成一个整体的Blender代码脚本，包括加载资产、进行分组和生成所有数值约束等。LLM-Coder还会预测每个函数$F_{r}$的所有参数$\texttt{arg}_{r}$，例如接近关系的精确距离等。
- en: '|  | $\displaystyle\texttt{code},\texttt{arg}\leftarrow\texttt{LLM-Coder}(\mathcal{G%
    }(s),q_{k})$ |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\texttt{code},\texttt{arg}\leftarrow\texttt{LLM-Coder}(\mathcal{G%
    }(s),q_{k})$ |  | (3) |'
- en: 'For each scene $s$ abstracted by a relational scene graph $G(s)$ finding an
    optimal layout could be formalized as the following optimization problem:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通过关系场景图$G(s)$抽象出的每个场景$s$，寻找一个最优布局可以形式化为以下优化问题：
- en: '|  | $\hat{\mathcal{L}}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}\sum_{r\in%
    \mathcal{R}}F_{r}\Big{(}\big{\{}\mathcal{L}(a_{i})\mid a_{i}\in\mathcal{E}(r)%
    \big{\}},\texttt{arg}_{r}\Big{)}$ |  | (4) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{L}}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}\sum_{r\in%
    \mathcal{R}}F_{r}\Big{(}\big{\{}\mathcal{L}(a_{i})\mid a_{i}\in\mathcal{E}(r)%
    \big{\}},\texttt{arg}_{r}\Big{)}$ |  | (4) |'
- en: 'This enables SceneCraft to simultaneously balance multiple constraints, ensuring
    a comprehensive and contextually accurate scene layout planning. After getting
    the optimal layout $\hat{\mathcal{L}}$, we can render the scene with the Blender
    code script code to get image output. Examples of generated scripts and rendered
    images can be found at Sec. [A](https://arxiv.org/html/2403.01248v1#A1 "Appendix
    A Examples of SceneCraft’s Generated Scripts and Rendered Scenes ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") in Appendix.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '这使得 SceneCraft 能够同时平衡多个约束，确保全面且语境准确的场景布局规划。在获得最优布局 $\hat{\mathcal{L}}$ 后，我们可以通过
    Blender 代码脚本代码渲染场景以获得图像输出。生成的脚本和渲染图像的示例可以在附录的[A](https://arxiv.org/html/2403.01248v1#A1
    "附录 A SceneCraft 生成的脚本和渲染场景示例 ‣ SceneCraft: 用于合成 3D 场景的 LLM 智能体")中找到。'
- en: Self-Improvement of Scene Script
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 场景脚本的自我改进
- en: However, the agent often does not produce the correct layout outright. This
    is because either (a) the predicted constraints do not reflect the requirements
    in the input query or do not follow common-sense knowledge, which requires updating
    the scene graph edges $\mathcal{E}$ and the Blender code code); or (b) the generated
    constraint functions do not correctly reflect the semantic relationships and result
    in an incorrect layout (we need to update scoring functions $\mathcal{F}_{r}$
    and arguments $\texttt{arg}_{r}$).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，智能体通常无法直接生成正确的布局。这是因为（a）预测的约束条件没有反映输入查询中的要求，或者没有遵循常识知识，这需要更新场景图的边 $\mathcal{E}$
    和 Blender 代码代码）；或者（b）生成的约束函数没有正确反映语义关系，导致布局不正确（我们需要更新评分函数 $\mathcal{F}_{r}$ 和参数
    $\texttt{arg}_{r}$）。
- en: 'We iteratively improve the initially generated scene layout in a visual feedback
    loop by taking advantage of the perception capabilities of a multimodal LLM (GPT-V (OpenAI,
    [2023](https://arxiv.org/html/2403.01248v1#bib.bib16))). We render the generated
    scene into an image, then feed the rendered image and the scene description directly
    to the LLM+V-Reviewer, asking it which constraints are lacking or not correctly
    satisfied, asking it to revise the script to reflect all the mistakes it finds.
    If LLM+V-Reviewer finds out that the error is rooted in the constraint functions,
    it can either modify existing functions or add new sub-functions to improve the
    layout planning for the current scene. This procedure repeats at every iteration
    of the feedback loop. We denote that at the $t$-th iteration, the function for
    each relation is $\mathcal{F}^{t}$ and the graph edges are $\mathcal{E}^{t}$,
    and the predicted optimal layout is $\hat{\mathcal{L}^{t}}$. This shares a similar
    intuition with recent works that utilize foundational models to generate a reward
    signal (Baumli et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib1); Rocamonde
    et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib22); Ma et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib15);
    Shinn et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib25)). The feedback-loop
    optimization procedure can formally be written as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过利用多模态大语言模型（LLM）(GPT-V (OpenAI, [2023](https://arxiv.org/html/2403.01248v1#bib.bib16)))
    的感知能力，在视觉反馈回路中迭代地改进初始生成的场景布局。我们将生成的场景渲染成图像，然后将渲染图像和场景描述直接输入 LLM+V-Reviewer，询问其缺少哪些约束条件或哪些约束未正确满足，并要求其修订脚本以反映所有发现的错误。如果
    LLM+V-Reviewer 发现错误源自约束函数，它可以修改现有函数或添加新的子函数来改进当前场景的布局规划。该过程在每次反馈回路迭代中都会重复。我们定义在第
    $t$ 次迭代时，每个关系的函数为 $\mathcal{F}^{t}$，图的边为 $\mathcal{E}^{t}$，预测的最优布局为 $\hat{\mathcal{L}^{t}}$。这一方法与最近的研究工作相似，这些工作利用基础模型生成奖励信号（Baumli
    等人，[2023](https://arxiv.org/html/2403.01248v1#bib.bib1)；Rocamonde 等人，[2023](https://arxiv.org/html/2403.01248v1#bib.bib22)；Ma
    等人，[2023](https://arxiv.org/html/2403.01248v1#bib.bib15)；Shinn 等人，[2023](https://arxiv.org/html/2403.01248v1#bib.bib25)）。反馈回路优化过程可以正式写为：
- en: '|  | $\displaystyle\mathcal{E}^{(t+1)},\mathcal{F}^{(t+1)},\texttt{arg}^{(t+1)}%
    \leftarrow\texttt{LLM+V-Reviewer}(\texttt{img},q_{k})$ |  |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{E}^{(t+1)},\mathcal{F}^{(t+1)},\texttt{arg}^{(t+1)}%
    \leftarrow\texttt{LLM+V-Reviewer}(\texttt{img},q_{k})$ |  |'
- en: '|  | $\displaystyle\text{subject to}\ \ \texttt{img}\leftarrow\texttt{Blender-Render%
    }(\mathcal{A},\mathcal{L}^{t},\texttt{code}^{t})$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{受限于}\ \ \texttt{img}\leftarrow\texttt{Blender-Render%
    }(\mathcal{A},\mathcal{L}^{t},\texttt{code}^{t})$ |  |'
- en: '|  | $\displaystyle\mathcal{L}^{t}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}%
    \sum_{r\in\mathcal{R}}F^{t}_{r}\big{(}\big{\{}\mathcal{L}(a_{i})\mid a_{i}\in%
    \mathcal{E}^{t}(r)\big{\}},\texttt{arg}^{t}_{r}\big{)}$ |  | (5) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}^{t}\leftarrow\underset{\mathcal{L}}{\mathrm{argmax}}%
    \sum_{r\in\mathcal{R}}F^{t}_{r}\big{(}\big{\{}\mathcal{L}(a_{i})\mid a_{i}\in%
    \mathcal{E}^{t}(r)\big{\}},\texttt{arg}^{t}_{r}\big{)}$ |  | (5) |'
- en: '![Refer to caption](img/8df62218894becd5bd9adf79e5af38b5.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8df62218894becd5bd9adf79e5af38b5.png)'
- en: 'Figure 4: Example of function parallelism_score update in outer-loop library
    learning phase. The update adds constraint score forcing the orientation of the
    assets to be similar.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：外循环库学习阶段中函数并行性得分更新的示例。该更新添加了约束得分，强制资产的方向相似。
- en: Algorithm 1 Dual-Loop Improvement Workflow
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 双循环改进工作流程
- en: 'Data: $L=\{F_{r}\}$: Initialize SceneCraft’s library, $\mathcal{Q}=\{q\}$:
    a dataset of queries. $N_{\text{inner}}$ and $N_{\text{outer}}$: number of iterations
    for scene refinement and library learning.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据：$L=\{F_{r}\}$：初始化SceneCraft的库，$\mathcal{Q}=\{q\}$：查询数据集。$N_{\text{inner}}$和$N_{\text{outer}}$：场景优化和库学习的迭代次数。
- en: repeat // outer-loop
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: repeat // 外循环
- en: 2.4 Library Learning
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 库学习
- en: In the preceding sections, we described the methodology behind SceneCraft’s
    generation of scenes, which involves the formulation of relations and constraint
    scoring functions, followed by their iterative optimization through a feedback
    loop. In this section, we go over the process by which we learn a comprehensive
    spatial skill library of constraint functions, designed for re-application in
    the scene generation process for new input queries.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们描述了SceneCraft生成场景的背后方法论，涉及关系和约束评分函数的制定，随后通过反馈循环进行迭代优化。在本节中，我们将讨论如何学习一个全面的空间技能库，该库包含约束函数，旨在针对新的输入查询，在场景生成过程中重新应用。
- en: 'The core of SceneCraft’s library learning originates from the aforementioned
    self-refinement procedure. When a specific constraint function is not sufficient
    to cover all cases of a relation, the LLM+V-Reviewer is able to identify the pitfall
    of function implementation, and make corresponding modification. As an example
    shown in Figure [4](https://arxiv.org/html/2403.01248v1#S2.F4 "Figure 4 ‣ Self-Improvement
    of Scene Script ‣ 2.3 Scene Layout Optimization in a Feedback Loop ‣ 2 Approach
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"), the previous
    implementation of parallelism relation only consider the assets’ location. Through
    feedback-loop optimization for scene improvement, GPT-V identifies that it is
    necessary to consider the similarity over orientation. Therefore, the main goal
    of library learning procedure is to review these gradual changes of $F_{r}$, detect
    common patterns in the addition or modification, and merge these changes into
    the library.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft库学习的核心来源于前述的自我优化过程。当一个特定的约束函数不足以覆盖关系的所有情况时，LLM+V-Reviewer能够识别出函数实现中的缺陷，并进行相应的修改。如图[4](https://arxiv.org/html/2403.01248v1#S2.F4
    "图 4 ‣ 场景脚本自我改进 ‣ 2.3 场景布局优化中的反馈循环 ‣ 2 方法 ‣ SceneCraft：一种用于生成Blender代码的3D场景的LLM代理")所示，先前的并行关系实现仅考虑了资产的位置。通过反馈循环的场景优化，GPT-V识别到有必要考虑方向上的相似性。因此，库学习过程的主要目标是审查这些$F_{r}$的渐进变化，检测其中的共性模式，并将这些变化合并到库中。
- en: 'Specifically, we denote $\hat{F_{r}}(q)=F_{r}^{T}(q)$ as the updated functions
    for relation $r$ learned after $t=T$ step inner-loop self-improvement over query
    $q$. SceneCraft reviews all these updates, try to find one that represents the
    consensus of all, and merge it into the global skill library. This procedure shares
    similar intuition as universal self-consistency (Chen et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib6)).
    We thus learn the new function as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将$\hat{F_{r}}(q)=F_{r}^{T}(q)$表示为在查询$q$上经过$t=T$步内部自我改进后学习到的关系$r$的更新函数。SceneCraft会检查所有这些更新，尝试找到一个能够代表所有一致性的更新，并将其合并到全球技能库中。这个过程与通用自我一致性（Chen等，
    [2023](https://arxiv.org/html/2403.01248v1#bib.bib6)）有相似的直觉。因此，我们学习新的函数为：
- en: '|  | $\displaystyle F_{r}\leftarrow\texttt{Library-Learner}\Big{(}\big{\{}\hat{F_{r}%
    }(q)\mid q\in\mathcal{Q}\big{\}}\Big{)}$ |  | (6) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{r}\leftarrow\texttt{Library-Learner}\Big{(}\big{\{}\hat{F_{r}%
    }(q)\mid q\in\mathcal{Q}\big{\}}\Big{)}$ |  | (6) |'
- en: 'This process is conducted over a batch of queries $\mathcal{Q}=\{q_{i}\}$ to
    ensure the universality of the learned skills. Note that: 1) this procedure could
    be regarded as a meta-learning update of the function initialization to facilitate
    the feedback-loop optimization. 2) this procedure does not require any ground-truth
    scenes, explicit reward function, or any human intervention. All the internal
    learning signal is just the LLM+V-reviewer during the feedback-loop to maximize
    the alignment to textual query. 3) For both optimization stages, updates are made
    to non-parametric knowledge represented as Python code, avoiding the computational
    cost and inaccessibility issues associated with back-propagation in large language
    models.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程是在一批查询 $\mathcal{Q}=\{q_{i}\}$ 上进行的，以确保所学技能的普遍性。请注意：1) 该过程可以看作是函数初始化的元学习更新，以便促进反馈循环优化。2)
    该过程不需要任何真实数据场景、显式奖励函数或任何人工干预。所有内部学习信号仅由 LLM+V-reviewer 在反馈循环中提供，以最大化与文本查询的对齐。3)
    对于两个优化阶段，更新仅针对表示为 Python 代码的非参数知识，避免了大语言模型中反向传播的计算成本和不可访问性问题。
- en: 'SceneCraft’s library learning process is also highly sample-efficient. By manually
    creating 20 examples with ground-truth constraints and running dual-stage optimization
    on them, SceneCraft develops a robust skill library. This approach contrasts with
    traditional model fine-tuning, offering efficiency and adaptability in learning
    for complex tasks like 3D scene generation. The pseudo-code of the whole dual-loop
    learning is illustrated in Alg [1](https://arxiv.org/html/2403.01248v1#alg1 "Algorithm
    1 ‣ Self-Improvement of Scene Script ‣ 2.3 Scene Layout Optimization in a Feedback
    Loop ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender
    Code").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft 的库学习过程也具有高度的样本效率。通过手动创建 20 个带有真实数据约束的示例并对其进行双阶段优化，SceneCraft 开发出了一个强大的技能库。这种方法与传统的模型微调方法有所不同，提供了在像
    3D 场景生成这样复杂任务中学习的效率和适应性。整个双循环学习的伪代码在算法 [1](https://arxiv.org/html/2403.01248v1#alg1
    "Algorithm 1 ‣ Self-Improvement of Scene Script ‣ 2.3 Scene Layout Optimization
    in a Feedback Loop ‣ 2 Approach ‣ SceneCraft: An LLM Agent for Synthesizing 3D
    Scene as Blender Code") 中进行了说明。'
- en: 3 Experiments
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: We evaluate our proposed SceneCraft first on our curated synthetic queries where
    the ground-truth constraints are available. We then show how the generated 3D
    scenes can help video generation on the Sintel movie as a case study.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在我们精心策划的合成查询上评估提出的 SceneCraft，其中有真实数据约束。然后，我们展示如何利用生成的 3D 场景帮助 Sintel 电影中的视频生成，作为一个案例研究。
- en: 3.1 Evaluate Scene Synthesis with Given Constraints
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 在给定约束下评估场景合成
- en: 'SceneCraft is an agent for open-domain scene synthesis. Most of the existing
    3D scene datasets with ground-truth focus on a specific domain such as in-door
    scene (Song et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib26); Wei
    et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib30)) or road traffic (Savkin
    et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib23)). To systematically
    study and evaluate our agent in this task, we manually create 40 synthetic queries
    with ground-truth constraints. The way we generate these queries is by first sampling
    a subset of relation constraints from the full list (shown in Appendix [B](https://arxiv.org/html/2403.01248v1#A2
    "Appendix B List of relationships ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")). Based on this, the human annotators evaluate whether
    the scene satisfies this relational constraint. Assets are retrieved from Turbosquid⁴⁴4[https://www.turbosquid.com/](https://www.turbosquid.com/).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'SceneCraft 是一个用于开放领域场景合成的智能体。大多数现有的包含真实数据的 3D 场景数据集集中在特定领域，例如室内场景（Song et al.,
    [2023](https://arxiv.org/html/2403.01248v1#bib.bib26); Wei et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib30)）或道路交通（Savkin
    et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib23)）。为了系统地研究和评估我们在此任务中的智能体，我们手动创建了
    40 个带有真实数据约束的合成查询。生成这些查询的方式是先从完整的约束关系列表中采样一个子集（见附录 [B](https://arxiv.org/html/2403.01248v1#A2
    "Appendix B List of relationships ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")）。基于此，人工标注者评估场景是否满足这一关系约束。资产从 Turbosquid⁴⁴4[https://www.turbosquid.com/](https://www.turbosquid.com/)
    获取。'
- en: Evaluation Metric
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: 'To verify whether a generated scene fulfills the textual requirement, we ask
    human annotators to also write a scoring function to estimate how much the constraint
    is satisfied. Such function is different from the one SceneCraft learns in its
    skill library, because the scoring function only needs to work for this specific
    scene query. We show examples of the queries as well as the implemented scoring
    function in Sec [D](https://arxiv.org/html/2403.01248v1#A4 "Appendix D Examples
    of annotated queries ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender
    Code") in Appendix. The output of these scoring functions is less than or equal
    to 1, and only reaches equality when all constraints are strictly satisfied.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证生成的场景是否满足文本要求，我们要求人工标注者编写评分函数，估计约束条件的满足程度。此函数不同于 SceneCraft 在其技能库中学习的评分函数，因为该评分函数只需要适用于这个特定的场景查询。我们在附录
    [D](https://arxiv.org/html/2403.01248v1#A4 "附录 D 标注查询示例 ‣ SceneCraft: 一种用 Blender
    代码合成 3D 场景的 LLM 代理") 中展示了查询示例以及实现的评分函数。这些评分函数的输出值小于或等于 1，只有当所有约束严格满足时，才会达到 1。'
- en: For this synthetic dataset, as we don’t have the ground-truth scene layout,
    we adopt two metrics for evaluating scene synthesis model’s performance. The first
    is the standard text-to-image CLIP similarity score (Radford et al., [2021](https://arxiv.org/html/2403.01248v1#bib.bib20)),
    which measures how well the generated scene satisfies the textual description;
    we also use the functions human annotators wrote as a more fine-grained evaluation
    on how our generated scene satisfies all the semantic requirements hidden in the
    query. We use 20 of the 40 queries for building the spatial skill library through
    dual-loop optimization, during which the model only sees the query instead of
    ground-truth constraint score. Afterwards, we evaluate the model performance on
    the remaining 20 queries.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个合成数据集，由于我们没有真实的场景布局，我们采用了两种度量标准来评估场景合成模型的性能。第一个是标准的文本到图像 CLIP 相似度得分（Radford
    等人，[2021](https://arxiv.org/html/2403.01248v1#bib.bib20)），它衡量生成的场景与文本描述的匹配程度；我们还使用人工标注者编写的功能作为更细粒度的评估，评估我们生成的场景如何满足查询中所有隐藏的语义要求。我们使用
    40 个查询中的 20 个来通过双循环优化构建空间技能库，在此过程中，模型只看到查询，而不是真实的约束得分。之后，我们评估模型在剩余 20 个查询上的表现。
- en: Baselines
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准
- en: 'Most of the existing 3D scene synthesis works only focus on a specific domain,
    e.g., indoor scenes. The only prior system that serves similar purpose to SceneCraft
    might be BlenderGPT⁵⁵5[https://github.com/gd3kr/BlenderGPT](https://github.com/gd3kr/BlenderGPT),
    an LLM assistant that also takes text query as input and generates Blender code.
    The main difference of BlenderGPT against SceneCraft is that BlenderGPT is limited
    to only the basic Blender instructions such as moving an asset or changing texture.
    To allow it to solve the text-to-scene synthesis task, we modify their code to:
    1) enable BlenderGPT to use GPT-V to receive the screenshot of Blender as visual
    feedback; 2) asking itself to give the per-step instruction for generating the
    complex scene. We also report results of our own system’s ablation. There are
    three major design choices of SceneCraft: 1) Abstraction of scene as relational
    graph; 2) inner-loop optimization of the scene with visual feedback; 3) outer-loop
    learning of skill library. These three components have some dependencies: constraint
    function grounded by relational graph is the main interface to be updated by inner-loop
    (BlenderGPT can be regarded as a baseline only with inner-loop update but without
    graph grounding); while the inner-loop updates of function is the root for library
    learning. Therefore, we do ablation study by removing one component after the
    other.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大部分 3D 场景合成工作仅关注于特定领域，例如室内场景。唯一与 SceneCraft 具有相似目的的前置系统可能是 BlenderGPT⁵⁵5[https://github.com/gd3kr/BlenderGPT](https://github.com/gd3kr/BlenderGPT)，这是一个大型语言模型助手，也接受文本查询作为输入并生成
    Blender 代码。BlenderGPT 与 SceneCraft 的主要区别在于，BlenderGPT 仅限于基本的 Blender 指令，如移动资产或更改纹理。为了让它能够解决文本到场景的合成任务，我们修改了他们的代码：1）使
    BlenderGPT 能够使用 GPT-V 接收 Blender 截图作为视觉反馈；2）要求其自我生成每一步生成复杂场景的指令。我们还报告了我们自己系统的消融实验结果。SceneCraft
    设计的三个主要选择是：1）将场景抽象为关系图；2）使用视觉反馈进行场景的内循环优化；3）技能库的外循环学习。这三部分有一些依赖关系：由关系图驱动的约束函数是需要通过内循环更新的主要接口（BlenderGPT
    可以看作是一个只有内循环更新但没有图结构驱动的基线）；而内循环更新的函数是库学习的根本。因此，我们通过逐个移除组件进行消融研究。
- en: Experimental Results
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果
- en: 'Results are shown in Table [1](https://arxiv.org/html/2403.01248v1#S3.T1 "Table
    1 ‣ Experimental Results ‣ 3.1 Evaluate Scene Synthesis with Given Constraints
    ‣ 3 Experiments ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender
    Code"). We see that our method consistently improves over all baselines in terms
    of both CLIP similarity as well as the constraint score. Notably, on the constraint
    score, the BlenderGPT baseline only achieves 5.6 score. We show a few head-to-head
    comparisons in Figure [1](https://arxiv.org/html/2403.01248v1#S0.F1 "Figure 1
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"). As example,
    in the first query that asks three boxes stack one on top of each other, BlenderGPT
    simply lists the three boxes in a line and does not follow the instruction of
    stacking; on the second query that asks three trees in a row, BlenderGPT does
    organize the trees in a line, but perpendicular with the road edge. These examples
    show that BlenderGPT without the relational constraint is not able to conduct
    complex spatial planning.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '结果如表[1](https://arxiv.org/html/2403.01248v1#S3.T1 "Table 1 ‣ Experimental Results
    ‣ 3.1 Evaluate Scene Synthesis with Given Constraints ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code")所示。我们可以看到，我们的方法在CLIP相似度和约束得分方面均
    consistently 优于所有基准方法。值得注意的是，在约束得分上，BlenderGPT基准仅取得5.6分。我们在图[1](https://arxiv.org/html/2403.01248v1#S0.F1
    "Figure 1 ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")中展示了一些正面对比。举个例子，在第一个查询中，要求将三个盒子堆叠在一起，BlenderGPT只是将三个盒子排列成一条线，并没有遵循堆叠的指令；在第二个查询中，要求三个树木排成一行，BlenderGPT确实将树木排成一行，但与道路边缘垂直。这些例子表明，BlenderGPT在没有关系约束的情况下，无法进行复杂的空间规划。'
- en: In the meantime, the ablation studies by removing each component also shows
    that all components are very crucial for Scenecraft. Among these components, inner-loop
    optimization provides the most important leaps; removing it leads to 38.4 drop
    on constraint score, and it’s also the root for library learning that keep the
    system self-improving without human annotation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，通过去除每个组件的消融研究也表明，所有组件对Scenecraft都非常关键。在这些组件中，内部循环优化提供了最重要的飞跃；去除它会导致约束得分下降38.4分，并且它也是库学习的根本，使得系统在没有人工标注的情况下能够自我改进。
- en: '| Metric | CLIP SIM | Constraint Score |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 度量标准 | CLIP SIM | 约束得分 |'
- en: '| BlenderGPT | 24.7 | 5.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 24.7 | 5.6 |'
- en: '| SceneCraft | 69.8 | 88.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft | 69.8 | 88.9 |'
- en: '| (—-Ablation by removing one component after the other—-) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| （—-通过逐一去除组件进行消融实验—-） |'
- en: '|  – Learned Library | 48.3 | 64.5 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  – 学习库 | 48.3 | 64.5 |'
- en: '|    – Inner-Loop | 32.8 | 26.1 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|    – 内部循环 | 32.8 | 26.1 |'
- en: '|      – Relation Graph | 19.4 | 3.2 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|      – 关系图 | 19.4 | 3.2 |'
- en: 'Table 1: Comparison of SceneCraft against BlenderGPT and ablation baselines
    on synthetic queries with annotated constraints.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：SceneCraft与BlenderGPT以及消融基准在带注释约束的合成查询上的比较。
- en: 'We also conduct a qualitative evaluation of SceneCraft’s output versus BlenderGPT
    baseline. We randomly select 10 pairs generated by SceneCraft and BlenderGPT,
    and ask humans to judge which one is better, in terms of three major dimensions:
    1) text fidelity: how much the generated scene aligns with the textual query;
    2) composition & constraint agreement: we tell the raters the ground-truth relations,
    and ask whether the generated scene follows all these constraints; 3) Aesthetics:
    we ask which output has better overall visual quality. The order of our output
    against baseline is completely random. Detailed question and interface is shown
    in Figure [11](https://arxiv.org/html/2403.01248v1#A5.F11 "Figure 11 ‣ Appendix
    E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene
    as Blender Code") in Appendix. Altogether we collect 22 responses. Results in
    Table [2](https://arxiv.org/html/2403.01248v1#S3.T2 "Table 2 ‣ Experimental Results
    ‣ 3.1 Evaluate Scene Synthesis with Given Constraints ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") show that our method
    outperforms BlenderGPT in all the three dimensions significantly. Specifically,
    consistent with our results on constraint score, SceneCraft gains more improvement
    over the constraint agreement, making the scene logically correct.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进行了SceneCraft输出与BlenderGPT基线的定性评估。我们随机选择了10对由SceneCraft和BlenderGPT生成的场景，要求人类评估哪个更好，评判标准包括三个主要维度：1）文本忠实度：生成的场景与文本查询的契合度；2）组成与约束一致性：我们告诉评估者真实的关系，并询问生成的场景是否符合所有这些约束；3）美学：我们询问哪个输出的整体视觉质量更好。我们输出与基线的顺序完全随机。详细的问卷和界面如附录中图[11](https://arxiv.org/html/2403.01248v1#A5.F11
    "图11 ‣ 附录E 各阶段使用的提示 ‣ SceneCraft：一种用于合成3D场景的LLM代理作为Blender代码")所示。总共我们收集了22个回答。表[2](https://arxiv.org/html/2403.01248v1#S3.T2
    "表2 ‣ 实验结果 ‣ 3.1 使用给定约束评估场景合成 ‣ 3 实验 ‣ SceneCraft：一种用于合成3D场景的LLM代理作为Blender代码")中的结果显示，我们的方法在所有三个维度上明显优于BlenderGPT。具体来说，与我们在约束分数上的结果一致，SceneCraft在约束一致性上有更大的改进，使得场景在逻辑上更为正确。
- en: '| Win Rate | Text Fidelity | Composition | Aesthetics |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 胜率 | 文本忠实度 | 组成 | 美学 |'
- en: '| SceneCraft | 76.8% | 83.6% | 74.5% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft | 76.8% | 83.6% | 74.5% |'
- en: '| BlenderGPT | 12.7% | 11.4% | 14.5% |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 12.7% | 11.4% | 14.5% |'
- en: 'Table 2: Qualitative Human comparison of SceneCraft against BlenderGPT baseline.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：SceneCraft与BlenderGPT基线的定性人类比较。
- en: 3.2 Scene-Guided Video Generation over Sintel Movie
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 场景引导的视频生成（基于Sintel电影）
- en: In addition to synthetic queries, we also show that SceneCraft’s layout planning
    capability generalizes to real scenes , and has potential to control and benefit
    video generation. As open-domain videos do not always have ground-truth scenes,
    we take the Sintel Movie, which is an animated fantasy short film produced with
    Blender, where scripts and Blender scenes are open sourced⁶⁶6[https://studio.blender.org/films/sintel/](https://studio.blender.org/films/sintel/).
    We download all these scenes, using the first half as the training set and the
    remaining half for testing. For this task, we assume that the model is given the
    ground-truth assets for the scene, and only focuses on layout planning to satisfy
    the textual description. After we recover the scene, we study how it can benefit
    a video generation model to get higher-quality predictions. We thus fine-tune
    the VideoPoet model (Kondratyuk et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib10)),
    an autoregressive Transformer-based video generation framework, on the training
    set with one ground-truth scene image frame as a conditional input. The image
    will be converted into image tokens, and add as prefix after text prompt. We then
    take the fine-tuned VideoPoet model, taking our model and BlenderGPT’s predicted
    scene, to generate a 2 second video.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了合成查询，我们还展示了SceneCraft的布局规划能力可以推广到真实场景，并且有潜力控制和促进视频生成。由于开放领域的视频并不总是具有真实场景，我们选择了《Sintel电影》，这是一部由Blender制作的动画奇幻短片，脚本和Blender场景是开源的⁶⁶6[https://studio.blender.org/films/sintel/](https://studio.blender.org/films/sintel/)。我们下载了所有这些场景，使用前半部分作为训练集，剩余部分用于测试。对于这个任务，我们假设模型已经获得了场景的真实素材，并且只关注布局规划，以满足文本描述。在恢复场景之后，我们研究它如何帮助视频生成模型获得更高质量的预测。因此，我们对VideoPoet模型（Kondratyuk等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib10)）进行了微调，这是一种基于自回归Transformer的视频生成框架，使用一个真实场景图像帧作为条件输入进行训练。图像将被转换为图像令牌，并在文本提示后作为前缀添加。然后，我们将微调后的VideoPoet模型与我们模型和BlenderGPT预测的场景一起使用，生成一个2秒的视频。
- en: '![Refer to caption](img/3ce765b7d2c03c8d875ac249da661d9d.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/3ce765b7d2c03c8d875ac249da661d9d.png)'
- en: 'Figure 5: Predicted 3D Scenes as well as the generated videos by SceneCraft
    against other baselines.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：与其他基线对比，SceneCraft生成的预测3D场景以及视频。
- en: '| Quantitative Metric | Scene Comparison | Video Comparison |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 定量指标 | 场景比较 | 视频比较 |'
- en: '| Layout Matrix SIM | Scene CLIP SIM | CLIP-based RM | FVD $\downarrow$ |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 布局矩阵相似度 | 场景CLIP相似度 | 基于CLIP的RM | FVD $\downarrow$ |'
- en: '| Text-to-Video (w.o. / finetune) | / | / | 56.8 | 846 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 文本到视频（不带微调） | / | / | 56.8 | 846 |'
- en: '| Text-to-Video (w / finetune) | / | / | 64.2 | 531 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 文本到视频（带微调） | / | / | 64.2 | 531 |'
- en: '| Text-to-Scene-to-Video, finetune a videogen model on groundtruth scene, infer
    with scenes generated by: |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 文本到场景到视频，通过真实场景微调一个videogen模型，使用生成的场景进行推理： |'
- en: '| BlenderGPT | 27.5 | 41.8 | 69.1 | 574 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| BlenderGPT | 27.5 | 41.8 | 69.1 | 574 |'
- en: '| SceneCraft (Dual-Loop) | 69.3 | 82.7 | 46.2 | 317 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SceneCraft（双环路） | 69.3 | 82.7 | 46.2 | 317 |'
- en: 'Table 3: Comparison of SceneCraft with other ablated baselines on a Sintel
    movie. In this setting, we assume to be given fixed assets for each scene, try
    to recover the scene, and guide a video generative model which is fine-tuned on
    first half of the video. We compare with naiive text-to-video baselines without
    scene guidance.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：SceneCraft与其他去除部分模块的基线在Sintel电影中的比较。在此设置中，我们假设每个场景的资产已固定，尝试恢复该场景，并指导一个在视频前半部分微调过的视频生成模型。我们与没有场景指导的原始文本到视频基线进行了比较。
- en: 'We compare the output in terms of both the scene itself as well as how much
    it benefits the overall video generation. For the scene, we use two metrics: the
    layout matrix’s similarity (first calculate mutual similarity between assets,
    then calculate cosine similarity), and the rendered image’s CLIP score. For the
    video, as we use both the standard Frechet Video Distance (FVD) distribution score (Unterthiner
    et al., [2019](https://arxiv.org/html/2403.01248v1#bib.bib28)), as well as CLIP-based
    Relative Matching (RM) score (Wu et al., [2021](https://arxiv.org/html/2403.01248v1#bib.bib31)).
    The results shown in Table [3](https://arxiv.org/html/2403.01248v1#S3.T3 "Table
    3 ‣ 3.2 Scene-Guided Video Generation over Sintel Movie ‣ 3 Experiments ‣ SceneCraft:
    An LLM Agent for Synthesizing 3D Scene as Blender Code") illustrate that our method
    consistently improves the BlenderGPT output in terms of scene planning. In addition,
    the generated scene helps the video generation and outperform the vanilla text-to-video
    baseline. From the examples in Figure [5](https://arxiv.org/html/2403.01248v1#S3.F5
    "Figure 5 ‣ 3.2 Scene-Guided Video Generation over Sintel Movie ‣ 3 Experiments
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code"), we can
    see that the 3D scene grounding help the generated video follow more similar structure
    as ground-truth ones. This shows the potential of SceneCraft in controlling video
    generation in wider domain.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '我们从场景本身及其对整体视频生成的贡献来比较输出结果。对于场景，我们使用两个指标：布局矩阵的相似度（首先计算资产之间的互相相似度，然后计算余弦相似度），以及渲染图像的CLIP评分。对于视频，我们使用标准的Frechet视频距离（FVD）分布评分（Unterthiner等人，[2019](https://arxiv.org/html/2403.01248v1#bib.bib28)），以及基于CLIP的相对匹配（RM）评分（Wu等人，[2021](https://arxiv.org/html/2403.01248v1#bib.bib31)）。表[3](https://arxiv.org/html/2403.01248v1#S3.T3
    "Table 3 ‣ 3.2 Scene-Guided Video Generation over Sintel Movie ‣ 3 Experiments
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")中展示的结果表明，我们的方法在场景规划方面始终提升了BlenderGPT的输出。此外，生成的场景有助于视频生成，且优于传统的文本到视频基线。从图[5](https://arxiv.org/html/2403.01248v1#S3.F5
    "Figure 5 ‣ 3.2 Scene-Guided Video Generation over Sintel Movie ‣ 3 Experiments
    ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code")中的例子中可以看出，3D场景的基础帮助生成的视频在结构上更接近真实场景。这展示了SceneCraft在更广泛领域控制视频生成的潜力。'
- en: 4 Related Works
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: Text to 3D-Scene Synthesis
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本到3D场景合成
- en: One of the earliest forays into text-driven 3D scene synthesis is WordsEye (Coyne
    & Sproat, [2001](https://arxiv.org/html/2403.01248v1#bib.bib7)). This system,
    and its follow-up works (Seversky & Yin, [2006](https://arxiv.org/html/2403.01248v1#bib.bib24);
    Chang et al., [2014](https://arxiv.org/html/2403.01248v1#bib.bib3); Ma et al.,
    [2018](https://arxiv.org/html/2403.01248v1#bib.bib14)), can generate 3D scenes
    from natural language. However, these systems often require manual mapping between
    language and object placement, leading to somewhat unnatural commands for scene
    description. Zitnick et al. ([2013](https://arxiv.org/html/2403.01248v1#bib.bib36))
    learns to map visual features to semantic phrases extracted from sentences, focusing
    on binary spatial or semantic relationships. Chang et al. ([2014](https://arxiv.org/html/2403.01248v1#bib.bib3))
    build upon and improve these early systems. The key advancement is the use of
    spatial knowledge, derived from 3D scene data, to more accurately constrain scene
    generations. This approach allows for a more realistic interpretation of unstated
    facts or common sense in scene synthesis. In their subsequent work (Chang et al.,
    [2015](https://arxiv.org/html/2403.01248v1#bib.bib4)), they focused on lexical
    grounding of textual terms to 3D model references, combining rule-based models
    with user annotations to select appropriate objects. Their latest paper (Chang
    et al., [2017](https://arxiv.org/html/2403.01248v1#bib.bib5)) further refines
    this approach, introducing interactive text-based scene editing operations and
    an improved user interface. All these systems are most purely symbolic rule-based
    and require significant human efforts to maintain, and are, therefore, hard to
    generalize to new domains and types of constraints.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的文本驱动3D场景合成之一是WordsEye（Coyne & Sproat，[2001](https://arxiv.org/html/2403.01248v1#bib.bib7)）。该系统及其后续工作（Seversky
    & Yin，[2006](https://arxiv.org/html/2403.01248v1#bib.bib24); Chang 等，[2014](https://arxiv.org/html/2403.01248v1#bib.bib3);
    Ma 等，[2018](https://arxiv.org/html/2403.01248v1#bib.bib14)）能够从自然语言生成3D场景。然而，这些系统通常需要手动映射语言与物体位置之间的关系，从而导致场景描述中的指令显得不太自然。Zitnick
    等（[2013](https://arxiv.org/html/2403.01248v1#bib.bib36)）学习将视觉特征映射到从句子中提取的语义短语，重点关注二元空间或语义关系。Chang
    等（[2014](https://arxiv.org/html/2403.01248v1#bib.bib3)）在此基础上进行了改进，关键的进展是利用从3D场景数据中获得的空间知识，更准确地约束场景生成。这一方法能够更真实地解读场景合成中的未陈述事实或常识。在他们随后的工作（Chang
    等，[2015](https://arxiv.org/html/2403.01248v1#bib.bib4)）中，他们重点关注文本术语的词汇基础与3D模型引用的对接，将基于规则的模型与用户标注相结合，以选择合适的物体。他们的最新论文（Chang
    等，[2017](https://arxiv.org/html/2403.01248v1#bib.bib5)）进一步完善了这一方法，引入了交互式文本基础场景编辑操作和改进的用户界面。所有这些系统基本上都是纯符号规则基础的，需要大量人工工作来维护，因此，难以推广到新的领域和约束类型。
- en: There also exist a line of neural-based 3D scene generation that learns from
    data. Most works in this direction focus on a specific domain, such as in-door
    scenes (Patil et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib17)).
    For instance, RoomDreamer (Song et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib26))
    trains a diffusion model to simultaneously generate layout, geometry and texture
    for in-door scenes; LEGO-Net (Wei et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib30))
    focus on the layout planning, and trains a Transformer model to iteratively cleanup
    the messy room. Despite the impressive performance of these work, they are restricted
    by the available 3D scene data. For most open-domain image and videos, it is very
    hard to collect ground-truth 3D scenes, which is why most works in this domain
    focus on in-door scenes. On the contrary, this paper focus on exploring whether
    we can take advantage of the existing knowledge and reasoning capabilities of
    Large Language Models to directly do layout planning without tuning its parameters,
    and we try to learn general spatial planning skills that can be generalized from
    very small number of synthetic queries.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 也存在一类基于神经网络的3D场景生成方法，通过从数据中学习。这方面的大多数工作集中在特定领域，如室内场景（Patil 等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib17)）。例如，RoomDreamer（Song
    等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib26)）训练一个扩散模型，能够同时生成室内场景的布局、几何形状和纹理；LEGO-Net（Wei
    等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib30)）则专注于布局规划，训练一个Transformer模型来反复整理凌乱的房间。尽管这些工作的表现令人印象深刻，但它们受限于现有的3D场景数据。对于大多数开放领域的图像和视频，收集真实的3D场景非常困难，这也是为什么大多数该领域的工作集中在室内场景的原因。相反，本文的重点是探索是否可以利用现有的大型语言模型的知识和推理能力，直接进行布局规划而无需调整其参数，并尝试学习可以从少量合成查询中推广的一般空间规划技能。
- en: Multimodal LLM Agents
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多模态LLM代理
- en: 'Leverageing visual perception abilities of recent models like GPT-V, multimodal
    LLM Agents (Liu et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib12))
    are capable of interacting with external visual environments, such as web browsing (Deng
    et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib8); Zhou et al., [2023b](https://arxiv.org/html/2403.01248v1#bib.bib35);
    Hu et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib9); Zheng et al.,
    [2024](https://arxiv.org/html/2403.01248v1#bib.bib33)), gaming (Wang et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib29)),
    robotics (Brohan et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib2))
    and design (Lv et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib13);
    Yang et al., [2024](https://arxiv.org/html/2403.01248v1#bib.bib32)). The most
    related concurrent works is 3D-GPT (Sun et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib27)),
    which interacts with Infinigen (Raistrick et al., [2023](https://arxiv.org/html/2403.01248v1#bib.bib21)),
    a high-level wrapper on top of Blender, to create high-quality environmental scenes.
    The main difference of our work against 3D-GPT⁷⁷7The code of this work hasn’t
    released, and we plan to compare after they open-source the code. is: 1) Environment-wise,
    we directly interact with Blender and a large-scale asset pool, which provide
    richer assets to construct the scene, while Infinigen for now only supports limited
    number of assets and environment arguments; 2) methodology-wise, SceneCraft features
    a dual-loop self-improvement pipeline, which enables us to learn new design skills
    to handle unseen tasks, which differentiate us to many existing llm-agent works
    that heavily rely on manual prompt design.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 近期模型如GPT-V的视觉感知能力得到了充分利用，多模态LLM代理（Liu等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib12)）能够与外部视觉环境进行互动，例如网页浏览（Deng等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib8)；Zhou等，[2023b](https://arxiv.org/html/2403.01248v1#bib.bib35)；Hu等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib9)；Zheng等，[2024](https://arxiv.org/html/2403.01248v1#bib.bib33)），游戏（Wang等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib29)），机器人技术（Brohan等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib2)）和设计（Lv等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib13)；Yang等，[2024](https://arxiv.org/html/2403.01248v1#bib.bib32)）。与之最相关的并行工作是3D-GPT（Sun等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib27)），该工作通过与Blender上的高级封装工具Infinigen（Raistrick等，[2023](https://arxiv.org/html/2403.01248v1#bib.bib21)）进行交互，创建高质量的环境场景。我们工作的主要区别在于：1）环境方面，我们直接与Blender和一个大规模的资产库进行互动，提供了更丰富的资源来构建场景，而Infinigen目前仅支持有限数量的资产和环境参数；2）方法论方面，SceneCraft采用了双循环自我改进流程，使我们能够学习新的设计技能来处理未见过的任务，这也使我们区别于许多现有的LLM代理工作，因为它们过度依赖手动提示设计。
- en: 5 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we present SceneCraft, an LLM-powered autonomous agent for transforming
    input text query to a 3D Scene by generating a Blender-executable Python script.
    Scenecraft builds on top of multimodal LLMs for both planning and library learning
    in a dual-loop self-improving framework. In the inner-loop, SceneCraft generates
    Blender-executable Python scripts to render an image of the scene, and use a Self-critiquing
    loop to iteratively refine its output and learn from its performance. The outer-loop
    dynamically expands a ’spatial skill’ library, facilitating continuous self-improvement
    without the need for expensive LLM parameter tuning. In the future, we’d like
    to explore: 1) using our framework for reconstructing the 3D scene corresponding
    to a given open-domain image or video; 2) utilizing the generated dataset to fine-tune
    a video generation conditioned on a 3D scene as control signal.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了SceneCraft，这是一个基于LLM的自主代理，可以通过生成一个可在Blender中执行的Python脚本，将输入的文本查询转换为3D场景。SceneCraft建立在多模态LLM之上，采用双循环自我改进框架进行规划和库学习。在内循环中，SceneCraft生成可在Blender中执行的Python脚本以渲染场景图像，并使用自我批评循环来迭代地完善输出并从其表现中学习。外循环动态扩展一个“空间技能”库，促进持续的自我改进，无需昂贵的LLM参数调整。未来，我们希望探索：1）使用我们的框架重建与给定开放域图像或视频对应的3D场景；2）利用生成的数据集微调一个基于3D场景作为控制信号的视频生成模型。
- en: Impact Statements
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响声明
- en: This paper presents work whose goal is to advance the text-to-3d-scene synthesis.
    The work can have potential to benefit the gaming, cinematic and design industry,
    which are mostly positive impact. We only learn a non-parametric skill library
    from synthetic queries, and not using any private information. There are many
    potential societal consequences of our work, none which we feel must be specifically
    highlighted here.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍的工作旨在推进文本到 3D 场景合成的研究。这项工作有潜力造福于游戏、电影和设计行业，且影响大多是积极的。我们只从合成查询中学习非参数化的技能库，且不使用任何私人信息。我们工作的社会影响有很多潜在后果，但我们认为这里无需特别强调。
- en: References
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Baumli et al. (2023) Baumli, K., Baveja, S., Behbahani, F. M. P., Chan, H.,
    Comanici, G., Flennerhag, S., Gazeau, M., Holsheimer, K., Horgan, D., Laskin,
    M., Lyle, C., Masoom, H., McKinney, K., Mnih, V., Neitz, A., Pardo, F., Parker-Holder,
    J., Quan, J., Rocktäschel, T., Sahni, H., Schaul, T., Schroecker, Y., Spencer,
    S., Steigerwald, R., Wang, L., and Zhang, L. Vision-language models as a source
    of rewards. *CoRR*, abs/2312.09187, 2023. doi: [10.48550/ARXIV.2312.09187](10.48550/ARXIV.2312.09187).
    URL [https://doi.org/10.48550/arXiv.2312.09187](https://doi.org/10.48550/arXiv.2312.09187).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baumli 等人 (2023) Baumli, K., Baveja, S., Behbahani, F. M. P., Chan, H., Comanici,
    G., Flennerhag, S., Gazeau, M., Holsheimer, K., Horgan, D., Laskin, M., Lyle,
    C., Masoom, H., McKinney, K., Mnih, V., Neitz, A., Pardo, F., Parker-Holder, J.,
    Quan, J., Rocktäschel, T., Sahni, H., Schaul, T., Schroecker, Y., Spencer, S.,
    Steigerwald, R., Wang, L., 和 Zhang, L. 视觉-语言模型作为奖励源。*CoRR*, abs/2312.09187, 2023年。doi：[10.48550/ARXIV.2312.09187](10.48550/ARXIV.2312.09187)。网址
    [https://doi.org/10.48550/arXiv.2312.09187](https://doi.org/10.48550/arXiv.2312.09187)。
- en: 'Brohan et al. (2023) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen,
    X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P.,
    Fu, C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu,
    J., Ichter, B., Irpan, A., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y.,
    Leal, I., Lee, L., Lee, T. E., Levine, S., Lu, Y., Michalewski, H., Mordatch,
    I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M. S., Salazar, G., Sanketi, P.,
    Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H. T., Vanhoucke, V., Vuong,
    Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu,
    S., Yu, T., and Zitkovich, B. RT-2: vision-language-action models transfer web
    knowledge to robotic control. *CoRR*, abs/2307.15818, 2023. doi: [10.48550/ARXIV.2307.15818](10.48550/ARXIV.2307.15818).
    URL [https://doi.org/10.48550/arXiv.2307.15818](https://doi.org/10.48550/arXiv.2307.15818).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan 等人 (2023) Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X.,
    Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P., Fu,
    C., Arenas, M. G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu,
    J., Ichter, B., Irpan, A., Joshi, N. J., Julian, R., Kalashnikov, D., Kuang, Y.,
    Leal, I., Lee, L., Lee, T. E., Levine, S., Lu, Y., Michalewski, H., Mordatch,
    I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M. S., Salazar, G., Sanketi, P.,
    Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H. T., Vanhoucke, V., Vuong,
    Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu,
    S., Yu, T., 和 Zitkovich, B. RT-2：视觉-语言-动作模型将网络知识转移到机器人控制。*CoRR*, abs/2307.15818,
    2023年。doi：[10.48550/ARXIV.2307.15818](10.48550/ARXIV.2307.15818)。网址 [https://doi.org/10.48550/arXiv.2307.15818](https://doi.org/10.48550/arXiv.2307.15818)。
- en: 'Chang et al. (2014) Chang, A. X., Savva, M., and Manning, C. D. Learning spatial
    knowledge for text to 3d scene generation. In Moschitti, A., Pang, B., and Daelemans,
    W. (eds.), *Proceedings of the 2014 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of
    SIGDAT, a Special Interest Group of the ACL*, pp.  2028–2038\. ACL, 2014. doi:
    [10.3115/V1/D14-1217](10.3115/V1/D14-1217). URL [https://doi.org/10.3115/v1/d14-1217](https://doi.org/10.3115/v1/d14-1217).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 (2014) Chang, A. X., Savva, M., 和 Manning, C. D. 为文本到 3D 场景生成学习空间知识。在
    Moschitti, A., Pang, B., 和 Daelemans, W. (编辑)，*2014年自然语言处理实证方法会议论文集，EMNLP 2014，2014年10月25-29日，卡塔尔多哈，SIGDAT会议，ACL特别兴趣小组会议*，第2028–2038页，ACL，2014年。doi：[10.3115/V1/D14-1217](10.3115/V1/D14-1217)。网址
    [https://doi.org/10.3115/v1/d14-1217](https://doi.org/10.3115/v1/d14-1217)。
- en: 'Chang et al. (2015) Chang, A. X., Monroe, W., Savva, M., Potts, C., and Manning,
    C. D. Text to 3d scene generation with rich lexical grounding. In *Proceedings
    of the 53rd Annual Meeting of the Association for Computational Linguistics and
    the 7th International Joint Conference on Natural Language Processing of the Asian
    Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing,
    China, Volume 1: Long Papers*, pp.  53–62\. The Association for Computer Linguistics,
    2015. doi: [10.3115/V1/P15-1006](10.3115/V1/P15-1006). URL [https://doi.org/10.3115/v1/p15-1006](https://doi.org/10.3115/v1/p15-1006).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人（2015）Chang, A. X., Monroe, W., Savva, M., Potts, C., 和 Manning, C.
    D. 通过丰富的词汇基础进行文本到3D场景生成。In *第53届计算语言学协会年会暨第7届亚洲自然语言处理联合国际会议论文集，ACL 2015，北京，中国，2015年7月26日至31日，第1卷：长篇论文*，第53–62页，计算语言学协会，2015年。doi:
    [10.3115/V1/P15-1006](10.3115/V1/P15-1006). URL [https://doi.org/10.3115/v1/p15-1006](https://doi.org/10.3115/v1/p15-1006).'
- en: 'Chang et al. (2017) Chang, A. X., Eric, M., Savva, M., and Manning, C. D. Sceneseer:
    3d scene design with natural language. *CoRR*, abs/1703.00050, 2017. URL [http://arxiv.org/abs/1703.00050](http://arxiv.org/abs/1703.00050).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2017）Chang, A. X., Eric, M., Savva, M., 和 Manning, C. D. Sceneseer：基于自然语言的3D场景设计。*CoRR*,
    abs/1703.00050, 2017. URL [http://arxiv.org/abs/1703.00050](http://arxiv.org/abs/1703.00050).
- en: 'Chen et al. (2023) Chen, X., Aksitov, R., Alon, U., Ren, J., Xiao, K., Yin,
    P., Prakash, S., Sutton, C., Wang, X., and Zhou, D. Universal self-consistency
    for large language model generation. *CoRR*, abs/2311.17311, 2023. doi: [10.48550/ARXIV.2311.17311](10.48550/ARXIV.2311.17311).
    URL [https://doi.org/10.48550/arXiv.2311.17311](https://doi.org/10.48550/arXiv.2311.17311).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2023）Chen, X., Aksitov, R., Alon, U., Ren, J., Xiao, K., Yin, P., Prakash,
    S., Sutton, C., Wang, X., 和 Zhou, D. 大型语言模型生成的普适自一致性。*CoRR*, abs/2311.17311, 2023.
    doi: [10.48550/ARXIV.2311.17311](10.48550/ARXIV.2311.17311). URL [https://doi.org/10.48550/arXiv.2311.17311](https://doi.org/10.48550/arXiv.2311.17311).'
- en: 'Coyne & Sproat (2001) Coyne, R. and Sproat, R. Wordseye: an automatic text-to-scene
    conversion system. In Pocock, L. (ed.), *Proceedings of the 28th Annual Conference
    on Computer Graphics and Interactive Techniques, SIGGRAPH 2001, Los Angeles, California,
    USA, August 12-17, 2001*, pp.  487–496\. ACM, 2001. doi: [10.1145/383259.383316](10.1145/383259.383316).
    URL [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Coyne & Sproat（2001）Coyne, R. 和 Sproat, R. Wordseye：一个自动化的文本到场景转换系统。In Pocock,
    L.（编辑），*第28届计算机图形学与交互技术年会论文集，SIGGRAPH 2001，洛杉矶，加利福尼亚，美国，2001年8月12日至17日*，第487–496页，ACM，2001年。doi:
    [10.1145/383259.383316](10.1145/383259.383316). URL [https://doi.org/10.1145/383259.383316](https://doi.org/10.1145/383259.383316).'
- en: 'Deng et al. (2023) Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang,
    B., Sun, H., and Su, Y. Mind2web: Towards a generalist agent for the web. *CoRR*,
    abs/2306.06070, 2023. doi: [10.48550/ARXIV.2306.06070](10.48550/ARXIV.2306.06070).
    URL [https://doi.org/10.48550/arXiv.2306.06070](https://doi.org/10.48550/arXiv.2306.06070).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人（2023）Deng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang, B.,
    Sun, H., 和 Su, Y. Mind2web：面向网络的通用代理。*CoRR*, abs/2306.06070, 2023. doi: [10.48550/ARXIV.2306.06070](10.48550/ARXIV.2306.06070).
    URL [https://doi.org/10.48550/arXiv.2306.06070](https://doi.org/10.48550/arXiv.2306.06070).'
- en: 'Hu et al. (2023) Hu, Z., Iscen, A., Sun, C., Chang, K., Sun, Y., Ross, D. A.,
    Schmid, C., and Fathi, A. AVIS: autonomous visual information seeking with large
    language models. *CoRR*, abs/2306.08129, 2023. doi: [10.48550/ARXIV.2306.08129](10.48550/ARXIV.2306.08129).
    URL [https://doi.org/10.48550/arXiv.2306.08129](https://doi.org/10.48550/arXiv.2306.08129).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等人（2023）Hu, Z., Iscen, A., Sun, C., Chang, K., Sun, Y., Ross, D. A., Schmid,
    C., 和 Fathi, A. AVIS：使用大型语言模型进行自主视觉信息获取。*CoRR*, abs/2306.08129, 2023. doi: [10.48550/ARXIV.2306.08129](10.48550/ARXIV.2306.08129).
    URL [https://doi.org/10.48550/arXiv.2306.08129](https://doi.org/10.48550/arXiv.2306.08129).'
- en: 'Kondratyuk et al. (2023) Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang,
    J., Hornung, R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., Cheng, Y., Chiu,
    M., Dillon, J., Essa, I., Gupta, A., Hahn, M., Hauth, A., Hendon, D., Martinez,
    A., Minnen, D., Ross, D. A., Schindler, G., Sirotenko, M., Sohn, K., Somandepalli,
    K., Wang, H., Yan, J., Yang, M., Yang, X., Seybold, B., and Jiang, L. Videopoet:
    A large language model for zero-shot video generation. *CoRR*, abs/2312.14125,
    2023. doi: [10.48550/ARXIV.2312.14125](10.48550/ARXIV.2312.14125). URL [https://doi.org/10.48550/arXiv.2312.14125](https://doi.org/10.48550/arXiv.2312.14125).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kondratyuk 等人（2023）Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung,
    R., Adam, H., Akbari, H., Alon, Y., Birodkar, V., Cheng, Y., Chiu, M., Dillon,
    J., Essa, I., Gupta, A., Hahn, M., Hauth, A., Hendon, D., Martinez, A., Minnen,
    D., Ross, D. A., Schindler, G., Sirotenko, M., Sohn, K., Somandepalli, K., Wang,
    H., Yan, J., Yang, M., Yang, X., Seybold, B., 和 Jiang, L. Videopoet：一个用于零样本视频生成的大型语言模型。*CoRR*,
    abs/2312.14125, 2023. doi: [10.48550/ARXIV.2312.14125](10.48550/ARXIV.2312.14125).
    URL [https://doi.org/10.48550/arXiv.2312.14125](https://doi.org/10.48550/arXiv.2312.14125).'
- en: 'Lin et al. (2023) Lin, C., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang,
    X., Kreis, K., Fidler, S., Liu, M., and Lin, T. Magic3d: High-resolution text-to-3d
    content creation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023*, pp.  300–309\. IEEE, 2023.
    doi: [10.1109/CVPR52729.2023.00037](10.1109/CVPR52729.2023.00037). URL [https://doi.org/10.1109/CVPR52729.2023.00037](https://doi.org/10.1109/CVPR52729.2023.00037).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2023) Lin, C., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X.,
    Kreis, K., Fidler, S., Liu, M., 和 Lin, T. Magic3d: 高分辨率文本到三维内容创作。见 *IEEE/CVF计算机视觉与模式识别大会，CVPR
    2023，加拿大温哥华，2023年6月17日至24日*, 页码 300–309。IEEE, 2023. doi: [10.1109/CVPR52729.2023.00037](10.1109/CVPR52729.2023.00037).
    URL [https://doi.org/10.1109/CVPR52729.2023.00037](https://doi.org/10.1109/CVPR52729.2023.00037).'
- en: 'Liu et al. (2023) Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu,
    Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang,
    C., Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., and Tang, J. Agentbench:
    Evaluating llms as agents. *CoRR*, abs/2308.03688, 2023. doi: [10.48550/ARXIV.2308.03688](10.48550/ARXIV.2308.03688).
    URL [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2023) Liu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,
    Ding, H., Men, K., Yang, K., Zhang, S., Deng, X., Zeng, A., Du, Z., Zhang, C.,
    Shen, S., Zhang, T., Su, Y., Sun, H., Huang, M., Dong, Y., 和 Tang, J. Agentbench:
    评估大型语言模型作为智能体的表现。*CoRR*, abs/2308.03688, 2023. doi: [10.48550/ARXIV.2308.03688](10.48550/ARXIV.2308.03688).
    URL [https://doi.org/10.48550/arXiv.2308.03688](https://doi.org/10.48550/arXiv.2308.03688).'
- en: 'Lv et al. (2023) Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen,
    Y., Chen, X., and Chen, S. Gpt4motion: Scripting physical motions in text-to-video
    generation via blender-oriented gpt planning. *arXiv preprint arXiv:2311.12631*,
    2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lv 等人 (2023) Lv, J., Huang, Y., Yan, M., Huang, J., Liu, J., Liu, Y., Wen,
    Y., Chen, X., 和 Chen, S. Gpt4motion: 通过面向Blender的GPT规划脚本化物理动作在文本到视频生成中的应用。*arXiv预印本
    arXiv:2311.12631*, 2023.'
- en: 'Ma et al. (2018) Ma, R., Patil, A. G., Fisher, M., Li, M., Pirk, S., Hua, B.,
    Yeung, S., Tong, X., Guibas, L. J., and Zhang, H. Language-driven synthesis of
    3d scenes from scene databases. *ACM Trans. Graph.*, 37(6):212, 2018. doi: [10.1145/3272127.3275035](10.1145/3272127.3275035).
    URL [https://doi.org/10.1145/3272127.3275035](https://doi.org/10.1145/3272127.3275035).'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 (2018) Ma, R., Patil, A. G., Fisher, M., Li, M., Pirk, S., Hua, B., Yeung,
    S., Tong, X., Guibas, L. J., 和 Zhang, H. 基于语言的场景数据库三维场景合成。*ACM图形学会会刊*, 37(6):212,
    2018. doi: [10.1145/3272127.3275035](10.1145/3272127.3275035). URL [https://doi.org/10.1145/3272127.3275035](https://doi.org/10.1145/3272127.3275035).'
- en: 'Ma et al. (2023) Ma, Y. J., Liang, W., Wang, G., Huang, D., Bastani, O., Jayaraman,
    D., Zhu, Y., Fan, L., and Anandkumar, A. Eureka: Human-level reward design via
    coding large language models. *CoRR*, abs/2310.12931, 2023. doi: [10.48550/ARXIV.2310.12931](10.48550/ARXIV.2310.12931).
    URL [https://doi.org/10.48550/arXiv.2310.12931](https://doi.org/10.48550/arXiv.2310.12931).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 (2023) Ma, Y. J., Liang, W., Wang, G., Huang, D., Bastani, O., Jayaraman,
    D., Zhu, Y., Fan, L., 和 Anandkumar, A. Eureka: 通过编码大型语言模型设计人类级别的奖励。*CoRR*, abs/2310.12931,
    2023. doi: [10.48550/ARXIV.2310.12931](10.48550/ARXIV.2310.12931). URL [https://doi.org/10.48550/arXiv.2310.12931](https://doi.org/10.48550/arXiv.2310.12931).'
- en: OpenAI (2023) OpenAI. Gpt-4v(ision) system card. System Card, 2023. URL [URL_of_the_System_Card](URL_of_the_System_Card).
    Version 1.0.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. Gpt-4v(ision) 系统卡。系统卡, 2023. URL [URL_of_the_System_Card](URL_of_the_System_Card).
    版本 1.0.
- en: 'Patil et al. (2023) Patil, A. G., Patil, S. G., Li, M., Fisher, M., Savva,
    M., and Zhang, H. Advances in data-driven analysis and synthesis of 3d indoor
    scenes. *CoRR*, abs/2304.03188, 2023. doi: [10.48550/ARXIV.2304.03188](10.48550/ARXIV.2304.03188).
    URL [https://doi.org/10.48550/arXiv.2304.03188](https://doi.org/10.48550/arXiv.2304.03188).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Patil 等人 (2023) Patil, A. G., Patil, S. G., Li, M., Fisher, M., Savva, M.,
    和 Zhang, H. 基于数据驱动的三维室内场景分析与合成的进展。*CoRR*, abs/2304.03188, 2023. doi: [10.48550/ARXIV.2304.03188](10.48550/ARXIV.2304.03188).
    URL [https://doi.org/10.48550/arXiv.2304.03188](https://doi.org/10.48550/arXiv.2304.03188).'
- en: 'Perez et al. (2020) Perez, E., Lewis, P. S. H., Yih, W., Cho, K., and Kiela,
    D. Unsupervised question decomposition for question answering. In Webber, B.,
    Cohn, T., He, Y., and Liu, Y. (eds.), *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020*,
    pp.  8864–8880\. Association for Computational Linguistics, 2020. doi: [10.18653/V1/2020.EMNLP-MAIN.713](10.18653/V1/2020.EMNLP-MAIN.713).
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.713](https://doi.org/10.18653/v1/2020.emnlp-main.713).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Perez 等人 (2020) Perez, E., Lewis, P. S. H., Yih, W., Cho, K., 和 Kiela, D. 无监督问题分解用于问答。见
    Webber, B., Cohn, T., He, Y., 和 Liu, Y. (编辑), *2020年自然语言处理经验方法会议论文集，EMNLP 2020，在线会议，2020年11月16日至20日*,
    页码 8864–8880。计算语言学协会, 2020. doi: [10.18653/V1/2020.EMNLP-MAIN.713](10.18653/V1/2020.EMNLP-MAIN.713).
    URL [https://doi.org/10.18653/v1/2020.emnlp-main.713](https://doi.org/10.18653/v1/2020.emnlp-main.713).'
- en: 'Poole et al. (2022) Poole, B., Jain, A., Barron, J. T., and Mildenhall, B.
    Dreamfusion: Text-to-3d using 2d diffusion. *CoRR*, abs/2209.14988, 2022. doi:
    [10.48550/ARXIV.2209.14988](10.48550/ARXIV.2209.14988). URL [https://doi.org/10.48550/arXiv.2209.14988](https://doi.org/10.48550/arXiv.2209.14988).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Poole等人（2022）Poole, B., Jain, A., Barron, J. T., 和 Mildenhall, B. Dreamfusion：使用2D扩散进行文本到3D的转换。*CoRR*，abs/2209.14988，2022年。doi:
    [10.48550/ARXIV.2209.14988](10.48550/ARXIV.2209.14988)。URL [https://doi.org/10.48550/arXiv.2209.14988](https://doi.org/10.48550/arXiv.2209.14988)。'
- en: Radford et al. (2021) Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh,
    G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G.,
    and Sutskever, I. Learning transferable visual models from natural language supervision.
    In Meila, M. and Zhang, T. (eds.), *Proceedings of the 38th International Conference
    on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event*, volume 139 of
    *Proceedings of Machine Learning Research*, pp.  8748–8763\. PMLR, 2021. URL [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2021）Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,
    S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., 和 Sutskever,
    I. 从自然语言监督中学习可迁移的视觉模型。在Meila, M.和Zhang, T.（编辑），*第38届国际机器学习会议，ICML 2021，2021年7月18-24日，虚拟会议*，*机器学习研究论文集*第139卷，第8748-8763页。PMLR，2021年。URL
    [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html)。
- en: 'Raistrick et al. (2023) Raistrick, A., Lipson, L., Ma, Z., Mei, L., Wang, M.,
    Zuo, Y., Kayan, K., Wen, H., Han, B., Wang, Y., Newell, A., Law, H., Goyal, A.,
    Yang, K., and Deng, J. Infinite photorealistic worlds using procedural generation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023,
    Vancouver, BC, Canada, June 17-24, 2023*, pp.  12630–12641\. IEEE, 2023. doi:
    [10.1109/CVPR52729.2023.01215](10.1109/CVPR52729.2023.01215). URL [https://doi.org/10.1109/CVPR52729.2023.01215](https://doi.org/10.1109/CVPR52729.2023.01215).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Raistrick等人（2023）Raistrick, A., Lipson, L., Ma, Z., Mei, L., Wang, M., Zuo,
    Y., Kayan, K., Wen, H., Han, B., Wang, Y., Newell, A., Law, H., Goyal, A., Yang,
    K., 和 Deng, J. 使用程序生成的无限逼真世界。在*IEEE/CVF计算机视觉与模式识别会议，CVPR 2023，温哥华，BC，加拿大，2023年6月17-24日*，第12630-12641页。IEEE，2023年。doi:
    [10.1109/CVPR52729.2023.01215](10.1109/CVPR52729.2023.01215)。URL [https://doi.org/10.1109/CVPR52729.2023.01215](https://doi.org/10.1109/CVPR52729.2023.01215)。'
- en: 'Rocamonde et al. (2023) Rocamonde, J., Montesinos, V., Nava, E., Perez, E.,
    and Lindner, D. Vision-language models are zero-shot reward models for reinforcement
    learning. *CoRR*, abs/2310.12921, 2023. doi: [10.48550/ARXIV.2310.12921](10.48550/ARXIV.2310.12921).
    URL [https://doi.org/10.48550/arXiv.2310.12921](https://doi.org/10.48550/arXiv.2310.12921).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rocamonde等人（2023）Rocamonde, J., Montesinos, V., Nava, E., Perez, E., 和 Lindner,
    D. 视觉-语言模型是强化学习的零-shot奖励模型。*CoRR*，abs/2310.12921，2023年。doi: [10.48550/ARXIV.2310.12921](10.48550/ARXIV.2310.12921)。URL
    [https://doi.org/10.48550/arXiv.2310.12921](https://doi.org/10.48550/arXiv.2310.12921)。'
- en: 'Savkin et al. (2023) Savkin, A., Ellouze, R., Navab, N., and Tombari, F. Unsupervised
    traffic scene generation with synthetic 3d scene graphs. *CoRR*, abs/2303.08473,
    2023. doi: [10.48550/ARXIV.2303.08473](10.48550/ARXIV.2303.08473). URL [https://doi.org/10.48550/arXiv.2303.08473](https://doi.org/10.48550/arXiv.2303.08473).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Savkin等人（2023）Savkin, A., Ellouze, R., Navab, N., 和 Tombari, F. 无监督的交通场景生成与合成的3D场景图。*CoRR*，abs/2303.08473，2023年。doi:
    [10.48550/ARXIV.2303.08473](10.48550/ARXIV.2303.08473)。URL [https://doi.org/10.48550/arXiv.2303.08473](https://doi.org/10.48550/arXiv.2303.08473)。'
- en: 'Seversky & Yin (2006) Seversky, L. M. and Yin, L. Real-time automatic 3d scene
    generation from natural language voice and text descriptions. In Nahrstedt, K.,
    Turk, M. A., Rui, Y., Klas, W., and Mayer-Patel, K. (eds.), *Proceedings of the
    14th ACM International Conference on Multimedia, Santa Barbara, CA, USA, October
    23-27, 2006*, pp.  61–64\. ACM, 2006. doi: [10.1145/1180639.1180660](10.1145/1180639.1180660).
    URL [https://doi.org/10.1145/1180639.1180660](https://doi.org/10.1145/1180639.1180660).'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Seversky和Yin（2006）Seversky, L. M. 和 Yin, L. 基于自然语言语音和文本描述的实时自动3D场景生成。在Nahrstedt,
    K., Turk, M. A., Rui, Y., Klas, W., 和 Mayer-Patel, K.（编辑），*第14届ACM国际多媒体会议，圣巴巴拉，加利福尼亚，美国，2006年10月23-27日*，第61-64页。ACM，2006年。doi:
    [10.1145/1180639.1180660](10.1145/1180639.1180660)。URL [https://doi.org/10.1145/1180639.1180660](https://doi.org/10.1145/1180639.1180660)。'
- en: 'Shinn et al. (2023) Shinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous
    agent with dynamic memory and self-reflection. *arXiv preprint arXiv:2303.11366*,
    2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn等人（2023）Shinn, N., Labash, B., 和 Gopinath, A. Reflexion：一种具有动态记忆和自我反思的自主智能体。*arXiv预印本
    arXiv:2303.11366*，2023年。
- en: 'Song et al. (2023) Song, L., Cao, L., Xu, H., Kang, K., Tang, F., Yuan, J.,
    and Yang, Z. Roomdreamer: Text-driven 3d indoor scene synthesis with coherent
    geometry and texture. In El-Saddik, A., Mei, T., Cucchiara, R., Bertini, M., Vallejo,
    D. P. T., Atrey, P. K., and Hossain, M. S. (eds.), *Proceedings of the 31st ACM
    International Conference on Multimedia, MM 2023, Ottawa, ON, Canada, 29 October
    2023- 3 November 2023*, pp.  6898–6906\. ACM, 2023. doi: [10.1145/3581783.3611800](10.1145/3581783.3611800).
    URL [https://doi.org/10.1145/3581783.3611800](https://doi.org/10.1145/3581783.3611800).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. (2023) Song, L., Cao, L., Xu, H., Kang, K., Tang, F., Yuan, J.,
    and Yang, Z. Roomdreamer: 基于文本驱动的 3D 室内场景合成，具有一致的几何形状和纹理。在 El-Saddik, A., Mei,
    T., Cucchiara, R., Bertini, M., Vallejo, D. P. T., Atrey, P. K., 和 Hossain, M.
    S.（编），*第 31 届 ACM 国际多媒体会议论文集，MM 2023，渥太华，加拿大，2023 年 10 月 29 日 - 11 月 3 日*，第 6898–6906
    页。ACM，2023。doi：[10.1145/3581783.3611800](10.1145/3581783.3611800)。网址 [https://doi.org/10.1145/3581783.3611800](https://doi.org/10.1145/3581783.3611800)。'
- en: 'Sun et al. (2023) Sun, C., Han, J., Deng, W., Wang, X., Qin, Z., and Gould,
    S. 3d-gpt: Procedural 3d modeling with large language models. *CoRR*, abs/2310.12945,
    2023. doi: [10.48550/ARXIV.2310.12945](10.48550/ARXIV.2310.12945). URL [https://doi.org/10.48550/arXiv.2310.12945](https://doi.org/10.48550/arXiv.2310.12945).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2023) Sun, C., Han, J., Deng, W., Wang, X., Qin, Z., 和 Gould, S.
    3d-gpt: 基于大型语言模型的程序化 3D 建模。*CoRR*, abs/2310.12945，2023。doi：[10.48550/ARXIV.2310.12945](10.48550/ARXIV.2310.12945)。网址
    [https://doi.org/10.48550/arXiv.2310.12945](https://doi.org/10.48550/arXiv.2310.12945)。'
- en: 'Unterthiner et al. (2019) Unterthiner, T., van Steenkiste, S., Kurach, K.,
    Marinier, R., Michalski, M., and Gelly, S. FVD: A new metric for video generation.
    In *Deep Generative Models for Highly Structured Data, ICLR 2019 Workshop, New
    Orleans, Louisiana, United States, May 6, 2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=rylgEULtdN](https://openreview.net/forum?id=rylgEULtdN).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Unterthiner et al. (2019) Unterthiner, T., van Steenkiste, S., Kurach, K.,
    Marinier, R., Michalski, M., 和 Gelly, S. FVD: 一种用于视频生成的新指标。在*深度生成模型与高度结构化数据，ICLR
    2019 研讨会，新奥尔良，路易斯安那州，美国，2019 年 5 月 6 日*。OpenReview.net，2019。网址 [https://openreview.net/forum?id=rylgEULtdN](https://openreview.net/forum?id=rylgEULtdN)。'
- en: 'Wang et al. (2023) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large
    language models. *CoRR*, abs/2305.16291, 2023. doi: [10.48550/ARXIV.2305.16291](10.48550/ARXIV.2305.16291).
    URL [https://doi.org/10.48550/arXiv.2305.16291](https://doi.org/10.48550/arXiv.2305.16291).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu,
    Y., Fan, L., 和 Anandkumar, A. Voyager: 一个基于大型语言模型的开放式实体代理。*CoRR*, abs/2305.16291，2023。doi：[10.48550/ARXIV.2305.16291](10.48550/ARXIV.2305.16291)。网址
    [https://doi.org/10.48550/arXiv.2305.16291](https://doi.org/10.48550/arXiv.2305.16291)。'
- en: 'Wei et al. (2023) Wei, Q. A., Ding, S., Park, J. J., Sajnani, R., Poulenard,
    A., Sridhar, S., and Guibas, L. J. Lego-net: Learning regular rearrangements of
    objects in rooms. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023*, pp.  19037–19047\. IEEE,
    2023. doi: [10.1109/CVPR52729.2023.01825](10.1109/CVPR52729.2023.01825). URL [https://doi.org/10.1109/CVPR52729.2023.01825](https://doi.org/10.1109/CVPR52729.2023.01825).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei et al. (2023) Wei, Q. A., Ding, S., Park, J. J., Sajnani, R., Poulenard,
    A., Sridhar, S., 和 Guibas, L. J. Lego-net: 学习房间中物体的规则排列。 在*IEEE/CVF 计算机视觉与模式识别会议，CVPR
    2023，温哥华，不列颠哥伦比亚省，加拿大，2023 年 6 月 17 日 - 24 日*，第 19037–19047 页。IEEE，2023。doi：[10.1109/CVPR52729.2023.01825](10.1109/CVPR52729.2023.01825)。网址
    [https://doi.org/10.1109/CVPR52729.2023.01825](https://doi.org/10.1109/CVPR52729.2023.01825)。'
- en: 'Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro,
    G., and Duan, N. GODIVA: generating open-domain videos from natural descriptions.
    *CoRR*, abs/2104.14806, 2021. URL [https://arxiv.org/abs/2104.14806](https://arxiv.org/abs/2104.14806).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2021) Wu, C., Huang, L., Zhang, Q., Li, B., Ji, L., Yang, F., Sapiro,
    G., 和 Duan, N. GODIVA: 从自然描述生成开放域视频。*CoRR*, abs/2104.14806，2021。网址 [https://arxiv.org/abs/2104.14806](https://arxiv.org/abs/2104.14806)。'
- en: 'Yang et al. (2024) Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., and Cui,
    B. Mastering text-to-image diffusion: Recaptioning, planning, and generating with
    multimodal llms. *arXiv preprint arXiv:2401.11708*, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2024) Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., 和 Cui, B.
    精通文本到图像扩散：重新标注、规划与使用多模态大型语言模型生成。*arXiv 预印本 arXiv:2401.11708*，2024。
- en: Zheng et al. (2024) Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y. Gpt-4v(ision)
    is a generalist web agent, if grounded. *arXiv preprint arXiv:2401.01614*, 2024.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024) Zheng, B., Gou, B., Kil, J., Sun, H., 和 Su, Y. GPT-4v(ision)
    是一个通用的网页代理，如果得到支持的话。*arXiv 预印本 arXiv:2401.01614*，2024。
- en: Zhou et al. (2023a) Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang,
    X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q. V., and Chi, E. H. Least-to-most
    prompting enables complex reasoning in large language models. In *The Eleventh
    International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda,
    May 1-5, 2023*. OpenReview.net, 2023a. URL [https://openreview.net/pdf?id=WZH7099tgfM](https://openreview.net/pdf?id=WZH7099tgfM).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2023a）Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
    D., Cui, C., Bousquet, O., Le, Q. V., 和Chi, E. H. 最小到最大提示促进大型语言模型中的复杂推理。发表于*第十一届国际学习表征会议ICLR
    2023，卢旺达基加利，2023年5月1-5日*。OpenReview.net，2023a。URL [https://openreview.net/pdf?id=WZH7099tgfM](https://openreview.net/pdf?id=WZH7099tgfM)。
- en: 'Zhou et al. (2023b) Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar,
    A., Cheng, X., Bisk, Y., Fried, D., Alon, U., et al. Webarena: A realistic web
    environment for building autonomous agents. *arXiv preprint arXiv:2307.13854*,
    2023b. URL [https://webarena.dev](https://webarena.dev).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等（2023b）Zhou, S., Xu, F. F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng,
    X., Bisk, Y., Fried, D., Alon, U., 等。Webarena：一个用于构建自主代理的真实网页环境。*arXiv预印本arXiv:2307.13854*，2023b。URL
    [https://webarena.dev](https://webarena.dev)。
- en: 'Zitnick et al. (2013) Zitnick, C. L., Parikh, D., and Vanderwende, L. Learning
    the visual interpretation of sentences. In *IEEE International Conference on Computer
    Vision, ICCV 2013, Sydney, Australia, December 1-8, 2013*, pp.  1681–1688\. IEEE
    Computer Society, 2013. doi: [10.1109/ICCV.2013.211](10.1109/ICCV.2013.211). URL
    [https://doi.org/10.1109/ICCV.2013.211](https://doi.org/10.1109/ICCV.2013.211).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zitnick等（2013）Zitnick, C. L., Parikh, D., 和Vanderwende, L. 学习句子的视觉解释。发表于*IEEE国际计算机视觉会议ICCV
    2013，澳大利亚悉尼，2013年12月1-8日*，第1681-1688页。IEEE计算机学会，2013。doi：[10.1109/ICCV.2013.211](10.1109/ICCV.2013.211)。URL
    [https://doi.org/10.1109/ICCV.2013.211](https://doi.org/10.1109/ICCV.2013.211)。
- en: Supplementary Material for SceneCraft
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft的补充材料
- en: Appendix A Examples of SceneCraft’s Generated Scripts and Rendered Scenes
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A SceneCraft生成的脚本和渲染场景示例
- en: 'Example of SceneCraft’s generated scripts and rendered scene on the synthetic
    datasets are in Figure [6](https://arxiv.org/html/2403.01248v1#A1.F6 "Figure 6
    ‣ Appendix A Examples of SceneCraft’s Generated Scripts and Rendered Scenes ‣
    SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code") and Figure [7](https://arxiv.org/html/2403.01248v1#A1.F7
    "Figure 7 ‣ Appendix A Examples of SceneCraft’s Generated Scripts and Rendered
    Scenes ‣ SceneCraft: An LLM Agent for Synthesizing 3D Scene as Blender Code").'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft生成的脚本和渲染场景的示例见于图[6](https://arxiv.org/html/2403.01248v1#A1.F6 "图6
    ‣ 附录A SceneCraft生成的脚本和渲染场景示例 ‣ SceneCraft：用于合成3D场景为Blender代码的大型语言模型代理")和图[7](https://arxiv.org/html/2403.01248v1#A1.F7
    "图7 ‣ 附录A SceneCraft生成的脚本和渲染场景示例 ‣ SceneCraft：用于合成3D场景为Blender代码的大型语言模型代理")。
- en: '![Refer to caption](img/9058dc07c7af0617ac999b96b06dfdf0.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9058dc07c7af0617ac999b96b06dfdf0.png)'
- en: 'Figure 6: Examples of generated code and scenes'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：生成的代码和场景示例
- en: '![Refer to caption](img/b8337cfa5665079f81d599042a2ca81a.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b8337cfa5665079f81d599042a2ca81a.png)'
- en: 'Figure 7: Examples of generated code and scenes'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：生成的代码和场景示例
- en: Appendix B List of relationships
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 关系列表
- en: 'SceneCraft encapsulates several types of relationships and constraints, including:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft封装了几种类型的关系和约束，包括：
- en: •
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proximity: A constraint enforcing the closeness of two objects, e.g., a chair
    near a table.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 邻近性：一种约束，强制两个物体靠得很近，例如椅子靠近桌子。
- en: •
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direction: The angle of one object is targeting at the other.'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方向：一个物体的角度指向另一个物体。
- en: •
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Alignment: Ensuring objects align along a common axis, e.g., paintings aligned
    vertically on a wall.'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对齐：确保物体沿着共同的轴对齐，例如墙上垂直对齐的画作。
- en: •
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Symmetry: Mirroring objects along an axis, e.g., symmetrical placement of lamps
    on either side of a bed.'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对称性：沿轴对称地排列物体，例如床两侧对称放置灯具。
- en: •
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Overlap: One object partially covering another, creating depth, e.g., a rug
    under a coffee table.'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重叠：一个物体部分覆盖另一个物体，创造深度感，例如咖啡桌下的地毯。
- en: •
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Parallelism: Objects parallel to each other, suggesting direction, e.g., parallel
    rows of seats in a theater.'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 平行性：物体彼此平行，指示方向，例如剧院中的平行座椅排。
- en: •
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Perpendicularity: Objects intersecting at a right angle, e.g., a bookshelf
    perpendicular to a desk.'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 垂直性：物体在直角处相交，例如与桌子垂直的书架。
- en: •
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchy: Indicating a list of objects follow a certain order of size / volumns.'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层次：表示一组物体遵循某种大小/体积顺序。
- en: •
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rotation: a list of objects rotate a cirtain point, e.g., rotating chairs around
    a meeting table.'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转：一组物体围绕某一点旋转，例如围绕会议桌旋转的椅子。
- en: •
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Repetition: Repeating patterns for rhythm or emphasis, e.g., a sequence of
    street lights.'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重复：重复的模式用于节奏或强调，例如一排街灯。
- en: •
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scaling: Adjusting object sizes for depth or focus, e.g., smaller background
    trees to create depth perception.'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缩放：调整物体的大小以增强深度感或焦点，例如将背景中的树木缩小以创造深度感。
- en: These relationships are vital for creating scenes that are not only visually
    appealing but also contextually coherent. Traditionally the functions $F(\cdot)$
    for each constraint shall be written by human experts, and SceneCraft’s major
    contribution is to autonomously learn and evolve the library of constraint satisfaction
    functions $\mathcal{F}=\{{F_{c}(\cdot)}\}_{c\in\mathcal{C}}$, using a Large Language
    Model (LLM) Agent.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这些关系对于创建既视觉上吸引人又上下文上连贯的场景至关重要。传统上，每个约束的函数 $F(\cdot)$ 由人工专家编写，而 SceneCraft 的主要贡献在于通过大型语言模型（LLM）代理自主学习并演化约束满足函数库
    $\mathcal{F}=\{{F_{c}(\cdot)}\}_{c\in\mathcal{C}}$。
- en: Appendix C Spatial Skill Library
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 空间技能库
- en: 'Below listed all the functions our framework generate. There exist some basic
    fundamental editing functions like import object, add camera, scaling, repetition;
    some functions to get information from the scene, such as calculate shortest distance
    between objects, calculate volumn, etc; as well as functions that calculate constraint
    satisfying score for each relationship. All these functions are autonomously written
    and modified by LLM Agent itself, without ground-truth label or explicit human
    intervention:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出了我们的框架生成的所有函数。它们包括一些基本的基础编辑函数，如导入物体、添加相机、缩放、重复；一些用于从场景中获取信息的函数，例如计算物体之间的最短距离、计算体积等；以及用于计算每个关系的约束满足得分的函数。所有这些函数都由LLM代理自主编写和修改，无需真实标签或明确的人工干预：
- en: '[⬇](data:text/plain;base64,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)@dataclassclass Layout:    location: Tuple[float, float, float]    min: Tuple[float, float, float]    max: Tuple[float, float, float]    orientation: Tuple[float, float, float] # Euler angles (pitch, yaw, roll)def scale_group(objects: List[bpy.types.Object], scale_factor: float) -> None:    """    Scale a group of objects by a given factor.    Args:        objects (List[bpy.types.Object]): List of Blender objects to scale.        scale_factor (float): The scale factor to apply.    Example:        scale_group([object1, object2], 1.5)    """    for obj in objects:        obj.scale = (obj.scale.x * scale_factor,                     obj.scale.y * scale_factor,                     obj.scale.z * scale_factor)        obj.matrix_world = obj.matrix_world * scale_factordef find_highest_vertex_point(objs: List[bpy.types.Object]) -> Dict[str, float]:    """    Find the highest vertex point among a list of objects.    Args:        objs (List[bpy.types.Object]): List of Blender objects to evaluate.    Returns:        Dict[str, float]: The lowest x, y, and z coordinates.    Example:        lowest_point = find_lowest_vertex_point([object1, object2])    """    bpy.context.view_layer.update()    highest_points = {’x’: -float(’inf’), ’y’: -float(’inf’), ’z’: -float(’inf’)}    for obj in objs:        # Apply the object’s current transformation to its vertices        obj_matrix_world = obj.matrix_world        if obj.type == ’MESH’:            # Update mesh to the latest data            obj.data.update()            for vertex in obj.data.vertices:                world_vertex = obj_matrix_world @ vertex.co                highest_points[’x’] = max(highest_points[’x’], world_vertex.x)                highest_points[’y’] = max(highest_points[’y’], world_vertex.y)                highest_points[’z’] = max(highest_points[’z’], world_vertex.z)    return highest_pointsdef find_lowest_vertex_point(objs: List[bpy.types.Object]) -> Dict[str, float]:    """    Find the lowest vertex point among a list of objects.    Args:        objs (List[bpy.types.Object]): List of Blender objects to evaluate.    Returns:        Dict[str, float]: The lowest x, y, and z coordinates.    Example:        lowest_point = find_lowest_vertex_point([object1, object2])    """    bpy.context.view_layer.update()    lowest_points = {’x’: float(’inf’), ’y’: float(’inf’), ’z’: float(’inf’)}    for obj in objs:        # Apply the object’s current transformation to its vertices        obj_matrix_world = obj.matrix_world        if obj.type == ’MESH’:            # Update mesh to the latest data            obj.data.update()            for vertex in obj.data.vertices:                world_vertex = obj_matrix_world @ vertex.co                lowest_points[’x’] = min(lowest_points[’x’], world_vertex.x)                lowest_points[’y’] = min(lowest_points[’y’], world_vertex.y)                lowest_points[’z’] = min(lowest_points[’z’], world_vertex.z)    return lowest_points[⬇](data:text/plain;base64,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)def rotate_objects_z_axis(objects: List[bpy.types.Object], angle_degrees: float) -> None:    """    Rotate a group of objects around the Z-axis by a given angle.    Args:        objects (List[bpy.types.Object]): List of objects to rotate.        angle_degrees (float): The angle in degrees to rotate.    Example:        rotate_objects_z_axis([object1, object2], 45)    """    bpy.context.view_layer.update()    angle_radians = math.radians(angle_degrees)  # Convert angle to radians    rotation_matrix = mathutils.Matrix.Rotation(angle_radians, 4, ’Y’)    lowest_point = find_lowest_vertex_point(objects)    highest_points = find_highest_vertex_point(objects)    center_point = {’x’: (lowest_point[’x’] + highest_points[’x’]) / 2,                    ’y’: (lowest_point[’y’] + highest_points[’y’]) / 2,                    ’z’: 0}    for obj in objects:        if obj.type == ’MESH’:            obj.data.update()            obj.matrix_world = obj.matrix_world @ rotation_matrix    lowest_point = find_lowest_vertex_point(objects)    highest_points = find_highest_vertex_point(objects)    center_point[’x’] -= (lowest_point[’x’] + highest_points[’x’]) / 2    center_point[’y’] -= (lowest_point[’y’] + highest_points[’y’]) / 2    shift(objects, center_point)def shift(objects: List[bpy.types.Object], shift_loc: Dict[str, float]) -> None:    """    Shift a group of objects with shift_loc.    Args:        objects (List[bpy.types.Object]): List of objects to rotate.        shift_loc (float): The shift vector.    Example:        rotate_objects_z_axis([object1, object2], (5,3,1))    """    for obj in objects:        # Shift object so the lowest point is at (0,0,0)        obj.location.x += shift_loc[’x’]        obj.location.y += shift_loc[’y’]        obj.location.z += shift_loc[’z’]    bpy.context.view_layer.update()def calculate_shortest_distance(vertices1: Set[Tuple[float, float, float]], vertices2: Set[Tuple[float, float, float]]) -> float:    """    Calculate the shortest distance between two sets of vertices.    Args:        vertices1 (Set[Tuple[float, float, float]]): First set of vertices.        vertices2 (Set[Tuple[float, float, float]]): Second set of vertices.    Returns:        float: Shortest distance over the Z-axis.    """    min_distance = float(’inf’)    for v1_tuple in vertices1:        v1 = Vector(v1_tuple)        for v2_tuple in vertices2:            v2 = Vector(v2_tuple)            distance = (v1 - v2).length            min_distance = min(min_distance, distance)    return min_distance[⬇](data:text/plain;base64,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)def rotate_objects_z_axis(objects: List[bpy.types.Object], angle_degrees: float) -> None:    """    Rotate a group of objects around the Z-axis by a given angle.    Args:        objects (List[bpy.types.Object]): List of objects to rotate.        angle_degrees (float): The angle in degrees to rotate.    Example:        rotate_objects_z_axis([object1, object2], 45)    """    bpy.context.view_layer.update()    angle_radians = math.radians(angle_degrees)  # Convert angle to radians    rotation_matrix = mathutils.Matrix.Rotation(angle_radians, 4, ’Y’)    lowest_point = find_lowest_vertex_point(objects)    highest_points = find_highest_vertex_point(objects)    center_point = {’x’: (lowest_point[’x’] + highest_points[’x’]) / 2,                    ’y’: (lowest_point[’y’] + highest_points[’y’]) / 2,                    ’z’: 0}    for obj in objects:        if obj.type == ’MESH’:            obj.data.update()            obj.matrix_world = obj.matrix_world @ rotation_matrix    lowest_point = find_lowest_vertex_point(objects)    highest_points = find_highest_vertex_point(objects)    center_point[’x’] -= (lowest_point[’x’] + highest_points[’x’]) / 2    center_point[’y’] -= (lowest_point[’y’] + highest_points[’y’]) / 2    shift(objects, center_point)def shift(objects: List[bpy.types.Object], shift_loc: Dict[str, float]) -> None:    """    Shift a group of objects with shift_loc.    Args:        objects (List[bpy.types.Object]): List of objects to rotate.        shift_loc (float): The shift vector.    Example:        rotate_objects_z_axis([object1, object2], (5,3,1))    """    for obj in objects:        # Shift object so the lowest point is at (0,0,0)        obj.location.x += shift_loc[’x’]        obj.location.y += shift_loc[’y’]        obj.location.z += shift_loc[’z’]    bpy.context.view_layer.update()def calculate_shortest_distance(vertices1: Set[Tuple[float, float, float]], vertices2: Set[Tuple[float, float, float]]) -> float:    """    Calculate the shortest distance between two sets of vertices.    Args:        vertices1 (Set[Tuple[float, float, float]]): First set of vertices.        vertices2 (Set[Tuple[float, float, float]]): Second set of vertices.    Returns:        float: Shortest distance over the Z-axis.    """    min_distance = float(’inf’)    for v1_tuple in vertices1:        v1 = Vector(v1_tuple)        for v2_tuple in vertices2:            v2 = Vector(v2_tuple)            distance = (v1 - v2).length            min_distance = min(min_distance, distance)    return min_distance[⬇](data:text/plain;base64,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)def check_vertex_overlap(vertices1: Set[Vector], vertices2: Set[Vector], threshold: float = 0.01) -> float:    """    Check if there is any overlap between two sets of vertices within a threshold.    Args:        vertices1 (Set[Vector]): First set of vertices.        vertices2 (Set[Vector]): Second set of vertices.        threshold (float): Distance threshold to consider as an overlap.    Returns:        bool: True if there is an overlap, False otherwise.    """    for v1_tuple in vertices1:        v1 = Vector(v1_tuple)        for v2_tuple in vertices2:            v2 = Vector(v2_tuple)            if (v1 - v2).length <= threshold:                return 1.0    return 0.0def evaluate_constraints(assets, constraints):    """Evaluate all constraints and return the overall score."""    total_score = 0    for constraint_func, involved_assets in constraints:        # Assuming each constraint function takes involved assets and returns a score        scores = constraint_func([assets[name] for name in involved_assets])        total_score += sum(scores)  # Summing scores assuming each constraint can contribute multiple scores    return total_scoredef adjust_positions(assets, adjustment_step=0.1):    """Randomly adjust the positions of assets."""    for asset in assets.values():        # Randomly adjust position within a small range to explore the space        asset.location = (            asset.location[0] + random.uniform(-adjustment_step, adjustment_step),            asset.location[1] + random.uniform(-adjustment_step, adjustment_step),            asset.location[2]  # Z position kept constant for simplicity        )def constraint_solving(assets, constraints, max_iterations=100):    """Find an optimal layout of assets to maximize the score defined by constraints."""    best_score = evaluate_constraints(assets, constraints)    best_layout = {name: asset.copy() for name, asset in assets.items()}  # Assuming a copy method exists    for _ in range(max_iterations):        adjust_positions(assets)        current_score = evaluate_constraints(assets, constraints)        if current_score > best_score:            best_score = current_score            best_layout = {name: asset.copy() for name, asset in assets.items()}        else:            # Revert to best layout if no improvement            assets = {name: layout.copy() for name, layout in best_layout.items()}    return best_layout, best_score[⬇](data:text/plain;base64,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)def normalize_vector(v: np.ndarray) -> np.ndarray:    """Normalize a vector."""    norm = np.linalg.norm(v)    return v / norm if norm > 0 else np.zeros_like(v)def orientation_similarity(orientation1: Tuple[float, float, float], orientation2: Tuple[float, float, float]) -> float:    """Calculate the similarity between two orientations, represented as Euler angles."""    # Convert Euler angles to vectors for simplicity in comparison    vector1 = np.array(orientation1)    vector2 = np.array(orientation2)    # Calculate the cosine similarity between the two orientation vectors    cos_similarity = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))    return cos_similaritydef parallelism_score(assets: List[Layout]) -> float:    """    Evaluates and returns a score indicating the degree of parallelism in a list of assets’ layouts, considering both position and orientation.    Args:    assets (List[Layout]): A list of asset layouts.    Returns:    float: A score between 0 and 1 indicating the parallelism of the assets.    """    if len(assets) < 2:        return 1.0  # Single asset or no asset is arbitrarily considered perfectly parallel    # Positional parallelism    vectors = [calculate_vector(assets[i].location, assets[i+1].location) for i in range(len(assets)-1)]    normalized_vectors = [normalize_vector(v) for v in vectors]    dot_products_position = [np.dot(normalized_vectors[i], normalized_vectors[i+1]) for i in range(len(normalized_vectors)-1)]    # Rotational similarity    orientation_similarities = [orientation_similarity(assets[i].orientation, assets[i+1].orientation) for i in range(len(assets)-1)]    # Combine scores    position_score = np.mean([0.5 * (dot + 1) for dot in dot_products_position])    orientation_score = np.mean([(similarity + 1) / 2 for similarity in orientation_similarities])    # Average the position and orientation scores for the final score    final_score = (position_score + orientation_score) / 2    return final_scoredef calculate_distance(location1: Tuple[float, float, float], location2: Tuple[float, float, float]) -> float:    """Calculate the Euclidean distance between two points."""    return np.linalg.norm(np.array(location1) - np.array(location2))def proximity_score(object1: Layout, object2: Layout, min_distance: float = 1.0, max_distance: float = 5.0) -> float:    """    Calculates a proximity score indicating how close two objects are, with 1 being very close and 0 being far apart.    Args:    object1 (Layout): The first object’s layout.    object2 (Layout): The second object’s layout.    min_distance (float): The distance below which objects are considered to be at optimal closeness. Scores 1.    max_distance (float): The distance beyond which objects are considered too far apart. Scores 0.    Returns:    float: A score between 0 and 1 indicating the proximity of the two objects.    """    distance = calculate_distance(object1.location, object2.location)    if distance <= min_distance:        return 1.0    elif distance >= max_distance:        return 0.0    else:        # Linearly interpolate the score based on the distance        return 1 - (distance - min_distance) / (max_distance - min_distance)[⬇](data:text/plain;base64,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)def euler_to_forward_vector(orientation: Tuple[float, float, float]) -> np.ndarray:    """Convert Euler angles to a forward direction vector."""    pitch, yaw, _ = orientation    # Assuming the angles are in radians    x = np.cos(yaw) * np.cos(pitch)    y = np.sin(yaw) * np.cos(pitch)    z = np.sin(pitch)    return np.array([x, y, z])def calculate_vector(a: Tuple[float, float, float], b: Tuple[float, float, float]) -> np.ndarray:    """Calculate the directional vector from point a to b."""    return np.array(b) - np.array(a)def direction_score(object1: Layout, object2: Layout) -> float:    """    Calculates a score indicating how directly object1 is targeting object2.    Args:    object1 (Layout): The first object’s layout, assumed to be the one doing the targeting.    object2 (Layout): The second object’s layout, assumed to be the target.    Returns:    float: A score between 0 and 1 indicating the directionality of object1 towards object2.    """    forward_vector = euler_to_forward_vector(object1.orientation)    target_vector = calculate_vector(object1.location, object2.location)    # Normalize vectors to ensure the dot product calculation is based only on direction    forward_vector_normalized = normalize_vector(forward_vector)    target_vector_normalized = normalize_vector(target_vector)    # Calculate the cosine of the angle between the two vectors    cos_angle = np.dot(forward_vector_normalized, target_vector_normalized)    # Map the cosine range [-1, 1] to a score range [0, 1]    score = (cos_angle + 1) / 2    return scoredef alignment_score(assets: List[Layout], axis: str) -> float:    """    Calculates an alignment score for a list of assets along a specified axis.    Args:    assets (List[Layout]): A list of asset layouts to be evaluated for alignment.    axis (str): The axis along which to evaluate alignment (’x’, ’y’, or ’z’).    Returns:    float: A score between 0 and 1 indicating the degree of alignment along the specified axis.    """    if not assets or axis not in [’x’, ’y’, ’z’]:        return 0.0  # Return a score of 0 for invalid input    # Axis index mapping to the location tuple    axis_index = {’x’: 0, ’y’: 1, ’z’: 2}[axis]    # Extract the relevant coordinate for each asset based on the chosen axis    coordinates = [asset.location[axis_index] for asset in assets]    # Calculate the variance of these coordinates    variance = np.var(coordinates)    # Inverse the variance to calculate the score, assuming a lower variance indicates better alignment    # Normalize the score to be between 0 and 1, considering a reasonable threshold for "perfect" alignment    threshold_variance = 1.0  # Define a threshold variance for "perfect" alignment    score = 1 / (1 + variance / threshold_variance)    # Clamp the score between 0 and 1    score = max(0, min(score, 1))    return scoredef check_vertex_overlap(vertices1: Set[Vector], vertices2: Set[Vector], threshold: float = 0.01) -> float:    """    Check if there is any overlap between two sets of vertices within a threshold.    Args:        vertices1 (Set[Vector]): First set of vertices.        vertices2 (Set[Vector]): Second set of vertices.        threshold (float): Distance threshold to consider as an overlap.    Returns:        bool: True if there is an overlap, False otherwise.    """    for v1_tuple in vertices1:        v1 = Vector(v1_tuple)        for v2_tuple in vertices2:            v2 = Vector(v2_tuple)            if (v1 - v2).length <= threshold:                return 0.0    return 1.0[⬇](data:text/plain;base64,Cg==)[⬇](data:text/plain;base64,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)def symmetry_score(assets: List[Layout], axis: str) -> float:    """    Calculates a symmetry score for a list of assets along a specified axis.    Args:    assets (List[Layout]): A list of asset layouts to be evaluated for symmetry.    axis (str): The axis along which to evaluate symmetry (’x’, ’y’, or ’z’).    Returns:    float: A score between 0 and 1 indicating the degree of symmetry along the specified axis.    """    if not assets or axis not in [’x’, ’y’, ’z’]:        return 0.0  # Return a score of 0 for invalid input    # Axis index mapping to the location tuple    axis_index = {’x’: 0, ’y’: 1, ’z’: 2}[axis]    # Find the median coordinate along the specified axis to define the symmetry axis    coordinates = [asset.location[axis_index] for asset in assets]    symmetry_axis = np.median(coordinates)    # Calculate the deviation from symmetry for each asset    deviations = []    for asset in assets:        # Find the mirrored coordinate across the symmetry axis        mirrored_coordinate = 2 * symmetry_axis - asset.location[axis_index]        # Find the closest asset to this mirrored coordinate        closest_distance = min(abs(mirrored_coordinate - other.location[axis_index]) for other in assets)        deviations.append(closest_distance)    # Calculate the average deviation from perfect symmetry    avg_deviation = np.mean(deviations)    # Convert the average deviation to a score, assuming smaller deviations indicate better symmetry    # The scoring formula can be adjusted based on the specific requirements for symmetry in the application    max_deviation = 10.0  # Define a maximum deviation for which the score would be 0    score = max(0, 1 - avg_deviation / max_deviation)    return scoredef perpendicularity_score(object1: Layout, object2: Layout) -> float:    """    Calculates a score indicating how perpendicular two objects are, based on their forward direction vectors.    Args:    object1 (Layout): The first object’s layout, including its orientation as Euler angles.    object2 (Layout): The second object’s layout, including its orientation as Euler angles.    Returns:    float: A score between 0 and 1 indicating the degree of perpendicularity.    """    vector1 = euler_to_forward_vector(object1.orientation)    vector2 = euler_to_forward_vector(object2.orientation)    cos_angle = np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))    score = 1 - np.abs(cos_angle)    return scoredef calculate_volume(layout: Layout) -> float:    """Calculate the volume of an object based on its layout dimensions."""    length = abs(layout.max[0] - layout.min[0])    width = abs(layout.max[1] - layout.min[1])    height = abs(layout.max[2] - layout.min[2])    return length * width * height[⬇](data:text/plain;base64,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)def evaluate_hierarchy(assets: List[Layout], expected_order: List[str]) -> float:    """    Evaluates how well a list of objects follows a specified hierarchical order based on size.    Args:    assets (List[Layout]): A list of asset layouts to be evaluated.    expected_order (List[str]): A list of identifiers (names) for the assets, specifying the expected order of sizes.    Returns:    float: A metric indicating how well the actual sizes of the objects match the expected hierarchical order.    """    # Map identifiers to volumes    id_to_volume = {asset_id: calculate_volume(asset) for asset_id, asset in zip(expected_order, assets)}    # Calculate the actual order based on sizes    actual_order = sorted(id_to_volume.keys(), key=lambda x: id_to_volume[x], reverse=True)    # Evaluate the match between the expected and actual orders    correct_positions = sum(1 for actual, expected in zip(actual_order, expected_order) if actual == expected)    total_positions = len(expected_order)    # Calculate the match percentage as a measure of hierarchy adherence    match_percentage = correct_positions / total_positions    return match_percentagedef calculate_angle_from_center(center: Tuple[float, float, float], object_location: Tuple[float, float, float]) -> float:    """Calculate the angle of an object relative to a central point."""    vector = np.array(object_location) - np.array(center)    angle = np.arctan2(vector[1], vector[0])    return angledef rotation_uniformity_score(objects: List[Layout], center: Tuple[float, float, float]) -> float:    """    Calculates how uniformly objects are distributed around a central point in terms of rotation.    Args:    objects (List[Layout]): A list of object layouts to be evaluated.    center (Tuple[float, float, float]): The central point around which objects are rotating.    Returns:    float: A score between 0 and 1 indicating the uniformity of object distribution around the center.    """    angles = [calculate_angle_from_center(center, obj.location) for obj in objects]    angles = np.sort(np.mod(angles, 2*np.pi))  # Normalize angles to [0, 2\pi] and sort    # Calculate differences between consecutive angles, including wrap-around difference    angle_diffs = np.diff(np.append(angles, angles[0] + 2*np.pi))    # Evaluate uniformity as the variance of these differences    variance = np.var(angle_diffs)    uniformity_score = 1 / (1 + variance)  # Inverse variance, higher score for lower variance    return uniformity_scoredef put_ontop(obj_dict, moving_set_name, target_set_name, threshold, step):    """    Adjust objects in moving_set_name until the shortest distance to target_set_name is below the threshold.    Args:        obj_dict (dict): Dictionary of object sets.        moving_set_name (str): The key for the set of objects to move.        target_set_name (str): The key for the set of objects to calculate distance to.        threshold (float): The distance threshold.        step (float): The step by which to move objects in the Z direction.    """    while True:        vertices_set1 = get_all_vertices(obj_dict[moving_set_name])        vertices_set2 = get_all_vertices(obj_dict[target_set_name])        shortest_distance = calculate_shortest_distance(vertices_set1, vertices_set2)        print(shortest_distance)        if shortest_distance < threshold:            break        for obj in obj_dict[moving_set_name]:            obj.location.z -= max(step, shortest_distance)        bpy.context.view_layer.update()[⬇](data:text/plain;base64,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)def repeat_object(original: Layout, direction: Tuple[float, float, float], repetitions: int, distance: float) -> List[Layout]:    """    Creates a series of duplicated objects based on the original, repeating them in a specified direction at a set distance.    Args:    original (Layout): The original object to be repeated.    direction (Tuple[float, float, float]): The direction vector along which to repeat the object.    repetitions (int): The number of times the object should be repeated.    distance (float): The distance between each object.    Returns:    List[Layout]: A list of Layout objects representing the original and its duplicates.    """    repeated_objects = [original]  # Include the original object in the output list    for i in range(1, repetitions):        # Calculate the new location for each repeated object        new_location = (            original.location[0] + direction[0] * distance * i,            original.location[1] + direction[1] * distance * i,            original.location[2] + direction[2] * distance * i        )        # Create a new Layout instance for each repetition        new_object = Layout(            location=new_location,            min=original.min,            max=original.max,            orientation=original.orientation        )        repeated_objects.append(new_object)    return repeated_objectsdef add_camera(location: Tuple[float, float, float], target_point: Tuple[float, float, float], lens: float = 35) -> bpy.types.Object:    """    Add a camera to the Blender scene.    Args:        location (Vector): The location to place the camera.        target_point (Vector): The point the camera should be aimed at.        lens (float, optional): The lens size. Defaults to 35.    Returns:        bpy.types.Object: The created camera object.    Example:        camera = add_camera((10, 10, 10), (0, 0, 0))    """    # Create a new camera data object    cam_data = bpy.data.cameras.new(name="Camera")    cam_data.lens = lens  # Set the lens property    # Create a new camera object and link it to the scene    cam_object = bpy.data.objects.new(’Camera’, cam_data)    bpy.context.collection.objects.link(cam_object)    # Set the camera location    cam_object.location = location    # Calculate the direction vector from the camera to the target point    direction = Vector(target_point) - Vector(location)    # Orient the camera to look at the target point    rot_quat = direction.to_track_quat(’-Z’, ’Y’)    cam_object.rotation_euler = rot_quat.to_euler()    # Set the created camera as the active camera in the scene    bpy.context.scene.camera = cam_object    return cam_object'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Examples of annotated queries
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 注释查询示例
- en: 'Examples of the annotated queries as well as the per-scene scoring functions
    are shown in Figure [8](https://arxiv.org/html/2403.01248v1#A4.F8 "Figure 8 ‣
    Appendix D Examples of annotated queries ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code") and Figure [10](https://arxiv.org/html/2403.01248v1#A5.F10
    "Figure 10 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for
    Synthesizing 3D Scene as Blender Code").'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '注释查询的示例以及每个场景的评分函数如图[8](https://arxiv.org/html/2403.01248v1#A4.F8 "Figure 8
    ‣ Appendix D Examples of annotated queries ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")和图[10](https://arxiv.org/html/2403.01248v1#A5.F10 "Figure
    10 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for Synthesizing
    3D Scene as Blender Code")所示。'
- en: '[⬇](data:text/plain;base64,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)scene_1 = {    "description": "A book lying flat on a table, two chair on each side",    "assets": ["book", "table"],    "relationships": {        "relativity": {            "description": "The book should be on top of the table",            "involved_objects": ["book", "table"]        },        "alignment": {            "description": "The book should be aligned with the table in the x and y directions",            "involved_objects": ["book", "table"]        }    }}def score_1(locs):    # Extracting locations    x_book, y_book, z_book = locs[’book’][’x’], locs[’book’][’y’], locs[’book’][’z’]    x_table, y_table, z_table = locs[’table’][’x’], locs[’table’][’y’], locs[’table’][’z’]    # Relativity score (penalizing if book is below table surface)    relativity_score = max(0, z_table - z_book)  # positive if book is below table    # Alignment score (difference in x and y positions, zero if perfectly aligned)    alignment_score_x = abs(x_book - x_table)    alignment_score_y = abs(y_book - y_table)    # Total score (sum of individual scores)    total_score = relativity_score + alignment_score_x + alignment_score_y    return 1 -  total_score / 100.scene_2 = {    "description": "A busy airport terminal with people, seating areas, and information displays",    "assets": ["person1", "person2", "seating_area", "information_display"],    "relationships": {        "grouping": {            "description": "People should be grouped near the seating areas",            "involved_objects": ["person1", "person2", "seating_area"]        },        "alignment": {            "description": "Information displays should be aligned above the seating areas",            "involved_objects": ["seating_area", "information_display"]        },        "proximity": {            "description": "People should be close to information displays for visibility",            "involved_objects": ["person1", "person2", "information_display"]        }    }}def score_2(locs):    def distance(a, b):        return math.sqrt((a[’x’] - b[’x’])**2 + (a[’y’] - b[’y’])**2 + (a[’z’] - b[’z’])**2)    # Grouping score (distance of people from seating areas)    grouping_score = sum(distance(locs[p], locs[’seating_area’]) for p in [’person1’, ’person2’])    # Alignment score (information display above seating areas)    alignment_score = abs(locs[’seating_area’][’y’] - locs[’information_display’][’y’])    # Proximity score (people close to information displays)    proximity_score = sum(distance(locs[p], locs[’information_display’]) for p in [’person1’, ’person2’])    # Total score    total_score = grouping_score + alignment_score + proximity_score    return 1 - total_score / 100.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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_fb2JqZWN0cyI6IFsicGVyc29uMSIsICJwZXJzb24yIiwgInNlYXRpbmdfYXJlYSJdCiAgICAgICAgfSwKICAgICAgICAiYWxpZ25tZW50IjogewogICAgICAgICAgICAiZGVzY3JpcHRpb24iOiAiSW5mb3JtYXRpb24gZGlzcGxheXMgc2hvdWxkIGJlIGFsaWduZWQgYWJvdmUgdGhlIHNlYXRpbmcgYXJlYXMiLAogICAgICAgICAgICAiaW52b2x2ZWR_fb2JqZWN0cyI6IFsic2VhdGluZ19hcmVhIiwgImluZm9ybWF0aW9uX2Rpc3BsYXkiXQogICAgICAgIH0sCiAgICAgICAgInByb3hpbWl0eSI6IHsKICAgICAgICAgICAgImRlc2NyaXB0aW9uIjogIlBlb3BsZSBzaG91bGQgYmUgY2xvc2UgdG8gaW5mb3JtYXRpb24gZGlzcGxheXMgZm9yIHZpc2liaWxpdHkiLAogICAgICAgICAgICAiaW52b2x2ZWR_fb2JqZWN0cyI6IFsicGVyc29uMSIsICJwZXJzb24yIiwgImluZm9ybWF0aW9uX2Rpc3BsYXkiXQogICAgICAgIH0KICAgIH0KfQpkZWYgc2NvcmVfMihsb2NzKToKICAgIGRlZiBkaXN0YW5jZShhLCBiKToKICAgICAgICByZXR1cm4gbWF0aC5zcXJ0KChhWyd4J10gLSBiWyd4J10pKioyICsgKGFbJ3knXSAtIGJbJ3knXSkqKjIgKyAoYVsneiddIC0gYlsneiddKSoqMikKCiAgICAjIEdyb3VwaW5nIHNjb3JlIChkaXN0YW5jZSBvZiBwZW9wbGUgZnJvbSBzZWF0aW5nIGFyZWFzKQogICAgZ3JvdXBpbmc_XP'
- en: 'Figure 8: Example of annotated queries and scoring function'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：带注释的查询和评分函数示例
- en: '[⬇](data:text/plain;base64,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)scene_3 = {    "description": "Three boxes of different sizes, stacked on top of each other",    "assets": ["box1", "box2", "box3"],    "relationships": {        "hierarchy": {            "description": "The boxes should be in descending order of size from bottom to top",            "involved_objects": ["box1", "box2", "box3"]        },        "layering": {            "description": "The boxes should be placed one above the other",            "involved_objects": ["box1", "box2", "box3"]        }    },}def score_3(locs):    # Extracting locations    z_box1, z_box2, z_box3 = locs[’box1’][’z’], locs[’box2’][’z’], locs[’box3’][’z’]    w_box1, w_box2, w_box3 = locs[’box1’][’w’], locs[’box2’][’w’], locs[’box3’][’w’]    # Hierarchy score (sizes)    hierarchy_score = 0    if not (w_box1 > w_box2 > w_box3):        hierarchy_score = abs(w_box1 - w_box2) + abs(w_box2 - w_box3)    # Layering score (z-axis positioning)    layering_score = 0    if not (z_box1 < z_box2 < z_box3):        layering_score = abs(z_box1 - z_box2) + abs(z_box2 - z_box3)    # Total score    total_score = hierarchy_score + layering_score    return 1 - total_score / 100scene_4 = {    "description": "A new solar system with planets orbiting around a small star",    "assets": ["sun", "planet1", "planet2", "planet3"],    "relationships": {        "rotation": {            "description": "Planets should orbit around the sun",            "involved_objects": ["planet1", "planet2", "planet3", "sun"]        },        "scaling": {            "description": "Planets should vary in size",            "involved_objects": ["planet1", "planet2", "planet3"]        }    }}def score_4(locs):    import math    def distance(a, b):        return math.sqrt((a[’x’] - b[’x’])**2 + (a[’y’] - b[’y’])**2 + (a[’z’] - b[’z’])**2)    # Rotation score (distance from sun)    rotation_score = sum(distance(locs[p], locs[’sun’]) for p in [’planet1’, ’planet2’, ’planet3’])    # Scaling score (size of planets)    scaling_score = abs(locs[’planet1’][’size’] - locs[’planet2’][’size’]) + abs(locs[’planet2’][’size’] - locs[’planet3’][’size’])    # Total score    total_score = rotation_score + scaling_score    return 1 - total_score / 100'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)scene_3 = {    "description": "三个不同大小的盒子堆叠在一起",    "assets": ["box1",
    "box2", "box3"],    "relationships": {        "hierarchy": {            "description": "盒子应该按大小从下到上依次排列",            "involved_objects": ["box1",
    "box2", "box3"]        },        "layering": {            "description": "盒子应该堆叠在一起",            "involved_objects": ["box1",
    "box2", "box3"]        }    },}def score_3(locs):    #'
- en: 'Figure 9: Example of annotated queries and scoring function'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：带注释的查询和评分函数示例
- en: Appendix E Prompt Used at each stage
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 每个阶段使用的提示
- en: 'The prompt used in SceneCraft is shown in Figure [10](https://arxiv.org/html/2403.01248v1#A5.F10
    "Figure 10 ‣ Appendix E Prompt Used at each stage ‣ SceneCraft: An LLM Agent for
    Synthesizing 3D Scene as Blender Code")'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: SceneCraft 中使用的提示如图[10](https://arxiv.org/html/2403.01248v1#A5.F10 "图 10 ‣ 附录
    E 每个阶段使用的提示 ‣ SceneCraft：用于合成 3D 场景为 Blender 代码的 LLM 代理")所示
- en: '[⬇](data:text/plain;base64,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)query_find_assets = """I am writing several blender scripts to generate scenes for: %s.Please think step by step and then give me the assets (each is a single unit, avoid a composite set that contains multiple objects) that shall appear in these scenes.After explanation, structured in: Output: 1) x1: y1; 2) x2: y2; 3) ... Each with a general descriptive name (x) and a very detailed visual description (y)."""query_height_assets = """I am writing several blender scripts to generate scenes for %s.Below are the assets we’d like to use. Now we need to scale them to correct height, please generate a python dictionary called height_dict, where key is each asset’s name, and value is a number representing the height (measured in metre)%sOutput the complete python dict via height_dict = {asset_name: height, ...}, also give detailed explanation as comment before the value in the dict."""query_plan_assets = """I am writing several blender scripts to generate a scene for %s.Below are the assets I’d like to use:%sNow I want a concrete plan to put them into the scene. Please think step by step, and give me a multi-step plan to put assets into the scene.For each step, structure your output as:layout_plan_i = {    "title": title_i,    "asset_list" :        [asset_name_1, asset_name_2],    "description": desc_i}where title_i is the high-level name for this step, and desc is detailed visual text description of what it shall look like after layout. asset_list is the non-empty list of assets to be added in this step.Please think step by step, place assets from environmental ones to more details assets. Return me a list of python dictonaries layout_plan_1, layout_plan_2, ..."""prompt_graph = """You are tasked with constructing a relational bipartite graph for a 3D scene based on the provided description and asset list. Your goal is to identify the spatial and contextual relationships between assets and represent these relationships in a structured format. Follow these steps:1. Review the scene description and the list of assets.2. Determine the spatial and contextual relationships needed to accurately represent the scene’s layout. Consider relationships like proximity, alignment, parallelism, etc.3. Construct the relational bipartite graph ‘G(s) = (A, R, E)‘ where:   - ‘A‘ represents the set of assets.   - ‘R‘ represents the set of relations as nodes.   - ‘E‘ represents the edges connecting a relation node to a subset of assets ‘E(r)‘ in the scene that satisfies this relation.4. For each identified relationship, create a relation node and link it to the appropriate assets through edges in the graph.Output your findings in a structured format:- List of relation nodes ‘R‘ with their types and descriptions.- Edges ‘E‘ that link assets to their corresponding relation nodes.This process will guide the arrangement of assets in the 3D scene, ensuring they are positioned, scaled, and oriented correctly according to the scene’s requirements and the relationships between objects."""'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: 'Figure 10: Example of prompts being used'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：使用提示的示例
- en: '![Refer to caption](img/4bbf55f2afbdebb5d85f21daaf961acb.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/4bbf55f2afbdebb5d85f21daaf961acb.png)'
- en: 'Figure 11: Questionnaire Interface, with three questions, about 1) Text Fidelity;
    2) Composition; 3) Aesthetics'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：问卷界面，包含三个问题：1）文本忠实度；2）构成；3）美学
