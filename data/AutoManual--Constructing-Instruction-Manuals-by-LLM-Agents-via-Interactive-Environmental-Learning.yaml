- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2025-01-11 12:37:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:37:27'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoManual：通过互动环境学习由LLM智能体构建说明手册
- en: 来源：[https://arxiv.org/html/2405.16247/](https://arxiv.org/html/2405.16247/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.16247/](https://arxiv.org/html/2405.16247/)
- en: Minghao Chen¹,    Yihang Li²,    Yanting Yang³,    Shiyu Yu⁵,    Binbin Lin^(3,4$*$),    Xiaofei
    He²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Minghao Chen¹,    Yihang Li²,    Yanting Yang³,    Shiyu Yu⁵,    Binbin Lin^(3,4$*$),    Xiaofei
    He²
- en: ¹School of Computer Science, Hangzhou Dianzi University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹计算机科学学院，杭州电子科技大学
- en: ²State Key Lab of CAD&CG, Zhejiang University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²计算机辅助设计与计算机图形学国家重点实验室，浙江大学
- en: ³School of Software Technology, Zhejiang University   ⁴Fullong Inc.   ⁵NingBo
    Port Group
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ³软件技术学院，浙江大学   ⁴Fullong Inc.   ⁵宁波港集团
- en: minghaochen01@gmail.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: minghaochen01@gmail.com
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Models (LLM) based agents have shown promise in autonomously
    completing tasks across various domains, e.g., robotics, games, and web navigation.
    However, these agents typically require elaborate design and expert prompts to
    solve tasks in specific domains, which limits their adaptability. We introduce
    AutoManual, a framework enabling LLM agents to autonomously build their understanding
    through interaction and adapt to new environments. AutoManual categorizes environmental
    knowledge into diverse rules and optimizes them in an online fashion by two agents:
    1) The Planner codes actionable plans based on current rules for interacting with
    the environment. 2) The Builder updates the rules through a well-structured rule
    system that facilitates online rule management and essential detail retention.
    To mitigate hallucinations in managing rules, we introduce a case-conditioned
    prompting strategy for the Builder. Finally, the Formulator agent compiles these
    rules into a comprehensive manual. The self-generated manual can not only improve
    the adaptability but also guide the planning of smaller LLMs while being human-readable.
    Given only one simple demonstration, AutoManual significantly improves task success
    rates, achieving 97.4% with GPT-4-turbo and 86.2% with GPT-3.5-turbo on ALFWorld
    benchmark tasks. The code is available at [https://github.com/minghchen/automanual](https://github.com/minghchen/automanual).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLM）的智能体在自动完成各领域任务（如机器人技术、游戏和网页导航）方面表现出了潜力。然而，这些智能体通常需要精心设计和专家提示才能解决特定领域的任务，这限制了它们的适应性。我们介绍了AutoManual，一个框架，它使LLM智能体能够通过交互自主构建理解，并适应新环境。AutoManual将环境知识分类为多种规则，并通过两个智能体以在线方式优化这些规则：1）规划者根据当前规则编码可操作的计划，用于与环境互动；2）构建者通过一个结构良好的规则系统更新规则，从而促进在线规则管理和关键信息的保留。为了减少规则管理中的幻觉问题，我们为构建者引入了基于案例的提示策略。最后，制式化智能体将这些规则编译成一份综合手册。自生成的手册不仅可以提高适应性，还能指导较小LLM的规划，同时便于人类阅读。在仅给出一个简单示范的情况下，AutoManual显著提高了任务成功率，在ALFWorld基准任务上，GPT-4-turbo达到了97.4%的成功率，GPT-3.5-turbo达到了86.2%的成功率。代码可在[https://github.com/minghchen/automanual](https://github.com/minghchen/automanual)上获取。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently, autonomous agents based on Large Language Models (LLM), e.g., ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)
    , Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) , SayCan [SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1)
    , WebGPT [WebGPT](https://arxiv.org/html/2405.16247v4#bib.bib10) , and Voyager [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    , have demonstrated their potential to complete long-horizon tasks in grounded
    environments. These LLM agents operate by generating thoughts and actions that
    are executable in the environment. For customized environments, such as robotics [SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1)
    ; [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7) ; [ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18)
    ; [ChatGPTEL](https://arxiv.org/html/2405.16247v4#bib.bib23) and games [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)
    ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24) ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)
    , prior methods provide detailed instructions and in-context examples to familiarize
    LLM with action functions (API) and the target environment. However, unlike these
    agents, humans can autonomously build and update their understanding of an unfamiliar
    environment through dynamic interaction.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于大型语言模型（LLM）的自主代理，如ReAct[ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)，Reflexion[Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)，SayCan[SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1)，WebGPT[WebGPT](https://arxiv.org/html/2405.16247v4#bib.bib10)和Voyager[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)，已经展示了在有实际基础的环境中完成长期任务的潜力。这些LLM代理通过生成可在环境中执行的思维和行动来操作。对于定制环境，例如机器人[SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1)；[CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)；[ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18)；[ChatGPTEL](https://arxiv.org/html/2405.16247v4#bib.bib23)和游戏[GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)；[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)；[GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)，先前的方法提供了详细的指令和上下文示例，以便使LLM熟悉行动功能（API）和目标环境。然而，与这些代理不同，人类可以通过动态互动自主构建和更新对陌生环境的理解。
- en: Several existing methods enable LLM agents to reflect on feedback [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) or save successful
    experiences as skills [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24) ; [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    to enhance the performance and reduce the reliance on human-provided examples.
    However, these reflections and skills have not been well exploited to foster a
    deeper understanding of the environment. As a result, directly using saved skills
    as in-context examples can lead to the Path Dependence problem, i.e., the agent
    blindly replicates the paths of previous successes, failing to adapt appropriately
    to new scenarios. Such problems are more severe in real-world situations characterized
    by high variability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有几种方法可以使LLM代理根据反馈进行反思[Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)；[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    或将成功的经验保存为技能[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)；[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)；[ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)以提高性能并减少对人工提供示例的依赖。然而，这些反思和技能并没有得到充分利用来促进对环境的更深入理解。因此，直接将保存的技能用作上下文示例可能会导致路径依赖问题，即代理盲目复制以往成功的路径，未能适应新场景。这种问题在具有高度变化性的现实世界情境中更为严重。
- en: A previous work, ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    , gathers the trajectories of LLM agents and extracts cross-task rules from them.
    However, these rules are extracted offline, making ExpeL suffer from the same
    distributional shift problem as Offline RL [OfflineRL](https://arxiv.org/html/2405.16247v4#bib.bib6)
    . Meanwhile, due to the simplicity of rule management, its rules are always armchair
    general and unhelpful for the Path Dependency problem. In this paper, we propose
    a novel framework called AutoManual to build a well-organized understanding of
    the environment that can guide multi-task planning effectively. AutoManual leverages
    a dynamic rule system that not only extracts valuable experience, including skills
    and reflections, into different types of rules but also allows for continuously
    updating these rules in response to new situations. Additionally, error-prone
    details are explicitly described in the rules to improve the robustness of planning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的工作ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) 收集了LLM代理的轨迹并从中提取跨任务规则。然而，这些规则是离线提取的，使得ExpeL遭遇了与离线强化学习（Offline
    RL） [OfflineRL](https://arxiv.org/html/2405.16247v4#bib.bib6) 相同的分布偏移问题。同时，由于规则管理的简单性，其规则通常是空洞的，且对路径依赖问题没有帮助。在本文中，我们提出了一种新的框架，称为AutoManual，旨在构建一个井然有序的环境理解，从而有效地指导多任务规划。AutoManual利用一个动态规则系统，不仅能将有价值的经验（包括技能和反思）提取为不同类型的规则，还能在面对新情况时不断更新这些规则。此外，规则中还明确描述了容易出错的细节，从而提高规划的鲁棒性。
- en: AutoManual follows two alternating iterative processes to optimize the rules.
    First, given the observation and task of an episode, the Planner agent utilizes
    currently discovered rules to write free-form code as an actionable plan. The
    interaction between the environment and the Planner will loop until the episode
    ends. Second, based on this trajectory, the Builder agent will update relevant
    rules through the rule system. This online updating mechanism can timely verify
    whether the rules have deviations and are applicable to the Planner. After rules
    optimization, the Formulator agent categorizes these rules according to their
    application scenarios and compiles a comprehensive manual in Markdown format.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AutoManual遵循两个交替的迭代过程来优化规则。首先，给定一个回合的观察和任务，规划者代理利用当前发现的规则编写自由形式的代码作为可执行的计划。环境与规划者之间的交互将循环进行，直到回合结束。其次，基于这个轨迹，构建器代理将通过规则系统更新相关规则。这个在线更新机制能够及时验证规则是否存在偏差，并确保它们对规划者适用。规则优化后，整理者代理根据规则的应用场景对这些规则进行分类，并编写成Markdown格式的综合手册。
- en: 'The challenge lies in enabling the Builder to accurately extract applicable
    rules from a long trajectory, as LLM are prone to generating hallucinations. To
    address this, we employ a case-conditioned prompting strategy, which directs the
    Builder to focus on specific rules according to the case of the trajectory. For
    example, if errors occurred in a trajectory, the Builder is first asked to determine
    which caused the error: an unrecorded situation occurred, or the Planner failed
    to follow existing rules. Based on this answer, the Builder will be given corresponding
    prompts to update relevant rules.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于使构建器能够准确地从长时间轨迹中提取适用的规则，因为大语言模型（LLM）容易生成虚假信息。为了解决这个问题，我们采用了基于案例的提示策略，指引构建器根据轨迹的案例专注于特定的规则。例如，如果轨迹中发生了错误，构建器首先需要判断错误的原因：是因为发生了未记录的情况，还是规划者未能遵循现有规则。根据这个答案，构建器将获得相应的提示来更新相关规则。
- en: 'To summarize, our contributions are the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们的贡献如下：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We adopt actionable code as the way for the Planner agent to interact with the
    environment. We introduce a structured rule system that allows the Builder agent
    to manage multiple types of knowledge from these code-based interactions.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们采用可执行的代码作为规划者代理与环境交互的方式。我们引入了一个结构化的规则系统，使构建器代理能够管理这些基于代码交互的多种类型的知识。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an alternating process between the Planner and Builder agents to
    optimize rules in an online manner and resolve the Path Dependency problem. To
    improve readability, the Formulator agent is introduced to reorganize and formalize
    the rules into a Markdown manual.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了规划者和构建器代理之间的交替过程，以在线方式优化规则并解决路径依赖问题。为了提高可读性，引入了整理者代理来重新组织和形式化这些规则，并将其编写成Markdown手册。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To facilitate rule management, we employ a case-conditioned prompting strategy,
    which guides the Builder to manage specific types of rules for different trajectory
    cases.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了方便规则管理，我们采用了基于案例的提示策略，指导构建器为不同的轨迹案例管理特定类型的规则。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Starting from a single demonstration, AutoManual can generate detailed instruction
    manuals for complex environments like ALFWorld and MiniWoB++. These manuals allow
    LLM agents to achieve remarkable success rates of 97.4% with GPT-4-turbo and 86.2%
    with GPT-3.5-turbo on ALFWorld, 98.3% with GPT-4-turbo and 92.7% with GPT-3.5-turbo
    on MiniWoB++.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从单一示范开始，AutoManual 可以为复杂环境如 ALFWorld 和 MiniWoB++ 生成详细的操作手册。这些手册使得 LLM 代理在 ALFWorld
    上使用 GPT-4-turbo 达到 97.4% 的成功率，在 GPT-3.5-turbo 上为 86.2%；在 MiniWoB++ 上，GPT-4-turbo
    的成功率为 98.3%，GPT-3.5-turbo 为 92.7%。
- en: 2 Related Works
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM for Agents Planning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 在代理规划中的应用
- en: Large Language Models (LLM) exhibit powerful reasoning and planning capabilities [GPT4](https://arxiv.org/html/2405.16247v4#bib.bib11)
    ; [ChatGPT](https://arxiv.org/html/2405.16247v4#bib.bib12) ; [CoT](https://arxiv.org/html/2405.16247v4#bib.bib28)
    ; [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)
    while requiring much fewer demonstrations than traditional learning methods. With
    this planning capability as the core, LLM agents are being developed for use in
    robotics [SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1) ; [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)
    ; [ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18) ; [LLM-Planner](https://arxiv.org/html/2405.16247v4#bib.bib20)
    ; [ChatGPTEL](https://arxiv.org/html/2405.16247v4#bib.bib23) , game-playing [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)
    ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24) ; [DEPS](https://arxiv.org/html/2405.16247v4#bib.bib27)
    ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39) , software development [MetaGPT](https://arxiv.org/html/2405.16247v4#bib.bib3)
    ; [ChatDev](https://arxiv.org/html/2405.16247v4#bib.bib15) , and other fields [Survey](https://arxiv.org/html/2405.16247v4#bib.bib30)
    . Prior studies [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) allow agents to adjust
    actions or plans based on environmental feedback to improve planning performance.
    Given the powerful programming capability of LLM, several works, e.g., CodeAsPolicy [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)
    , ProgPrompt [ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18) and AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    , propose to use Python code as the plan of LLM agents. This form of output can
    automatically respond to in-plan feedback and achieve better performance than
    the action and JSON format [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [CodeAct](https://arxiv.org/html/2405.16247v4#bib.bib25) .
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）展现了强大的推理和规划能力 [GPT4](https://arxiv.org/html/2405.16247v4#bib.bib11)
    ; [ChatGPT](https://arxiv.org/html/2405.16247v4#bib.bib12) ; [CoT](https://arxiv.org/html/2405.16247v4#bib.bib28)
    ; [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)，并且比传统学习方法需要的示范要少得多。以这一规划能力为核心，LLM
    代理正在被开发用于机器人技术 [SayCan](https://arxiv.org/html/2405.16247v4#bib.bib1) ; [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)
    ; [ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18) ; [LLM-Planner](https://arxiv.org/html/2405.16247v4#bib.bib20)
    ; [ChatGPTEL](https://arxiv.org/html/2405.16247v4#bib.bib23)，游戏玩法 [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)
    ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24) ; [DEPS](https://arxiv.org/html/2405.16247v4#bib.bib27)
    ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)，软件开发 [MetaGPT](https://arxiv.org/html/2405.16247v4#bib.bib3)
    ; [ChatDev](https://arxiv.org/html/2405.16247v4#bib.bib15)，以及其他领域 [Survey](https://arxiv.org/html/2405.16247v4#bib.bib30)。先前的研究 [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) ; [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)
    允许代理根据环境反馈调整行动或计划，以改善规划性能。鉴于 LLM 强大的编程能力，一些研究工作，如 CodeAsPolicy [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)、ProgPrompt [ProgPrompt](https://arxiv.org/html/2405.16247v4#bib.bib18)
    和 AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)，建议使用
    Python 代码作为 LLM 代理的计划。这种输出形式可以自动响应计划内的反馈，并比行动和 JSON 格式更能获得更好的性能 [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [CodeAct](https://arxiv.org/html/2405.16247v4#bib.bib25)。
- en: 2.2 Self-improvement of LLM Agents
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM 代理的自我改进
- en: Embodied agent research has long sought to enable agents to self-improve through
    interactive experiences. Unlike traditional learning-based agents that require
    extensive iterations for optimization, Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    allows LLM agents to reflect on previous failures and quickly improve their plans.
    Some works [ToT](https://arxiv.org/html/2405.16247v4#bib.bib32) ; [AgentPro](https://arxiv.org/html/2405.16247v4#bib.bib34)
    ; [LATS](https://arxiv.org/html/2405.16247v4#bib.bib36) combine tree search with
    reflection to deliberately seek a better solution. Apart from failure experiences,
    prior studies [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    ; [GITM](https://arxiv.org/html/2405.16247v4#bib.bib39) utilize successful experiences
    as skills to assist future planning. Voyager [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    stores generated and verified programs into the skill library as a new skill for
    more complex tasks. AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    also discovers and archives successful programs into skill memory for future similar
    tasks. However, these methods stop updating skills after storing them, which inevitably
    leads to the Path Dependency problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 具身代理研究一直致力于通过交互体验使代理能够自我改进。与传统的基于学习的代理需要大量迭代优化不同，Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)允许LLM代理反思之前的失败并迅速改进其计划。一些研究[ToT](https://arxiv.org/html/2405.16247v4#bib.bib32)；[AgentPro](https://arxiv.org/html/2405.16247v4#bib.bib34)；[LATS](https://arxiv.org/html/2405.16247v4#bib.bib36)结合了树搜索和反思，以有意地寻求更好的解决方案。除了失败经验之外，先前的研究[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)；[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)；[GITM](https://arxiv.org/html/2405.16247v4#bib.bib39)利用成功的经验作为技能来辅助未来的规划。Voyager
    [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)将生成和验证过的程序存入技能库，作为更复杂任务的新技能。AdaPlanner
    [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)也会发现并归档成功的程序到技能记忆中，以备未来类似任务使用。然而，这些方法在存储技能后停止更新技能，导致不可避免的路径依赖问题。
- en: Another series of works [PromptAgent](https://arxiv.org/html/2405.16247v4#bib.bib26)
    ; [LLMOptim](https://arxiv.org/html/2405.16247v4#bib.bib31) ; [LLMPrompt](https://arxiv.org/html/2405.16247v4#bib.bib38)
    employs LLM as a prompt optimizer to enhance its own performance. In contrast
    to our approach, which addresses challenges in unfamiliar environments, these
    studies focus on enhancing LLM reasoning performance. As a result, their optimized
    prompts are typically brief and lack environmental knowledge.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个系列的工作[PromptAgent](https://arxiv.org/html/2405.16247v4#bib.bib26)；[LLMOptim](https://arxiv.org/html/2405.16247v4#bib.bib31)；[LLMPrompt](https://arxiv.org/html/2405.16247v4#bib.bib38)将LLM作为提示优化器来提升其自身性能。与我们的方法不同，我们的方法解决的是陌生环境中的挑战，而这些研究则侧重于增强LLM推理性能。因此，它们优化的提示通常简短，并且缺乏环境知识。
- en: 2.3 Memory Management of LLM Agents
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM代理的记忆管理
- en: For LLM agents, learning from past experiences can also be viewed as managing
    the episodic memory [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    . CLIN [CLIN](https://arxiv.org/html/2405.16247v4#bib.bib9) proposes to keep updating
    a memory centered on causal abstractions for new trials. Retrieval-Augmented Planning
    (RAP) [RAP](https://arxiv.org/html/2405.16247v4#bib.bib4) retrieves past experiences
    corresponding to the current situation. MemGPT [MemGPT](https://arxiv.org/html/2405.16247v4#bib.bib13)
    allows LLM to select content to retain in working memory and to search for information
    in long-term memory. Generative Agents [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)
    retrieve memories based on recency, importance, and relevance to the current situation.
    Generative Agents also generate tree-structured reflections, but they focus on
    a continuous scenario rather than task-oriented rules.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LLM代理来说，从过去的经验中学习也可以视为管理情节记忆[Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)。CLIN
    [CLIN](https://arxiv.org/html/2405.16247v4#bib.bib9)提议通过不断更新一个以因果抽象为中心的记忆来进行新的尝试。增强检索规划（RAP）[RAP](https://arxiv.org/html/2405.16247v4#bib.bib4)会检索与当前情况相对应的过去经验。MemGPT
    [MemGPT](https://arxiv.org/html/2405.16247v4#bib.bib13)允许LLM选择在工作记忆中保留的内容，并在长期记忆中搜索信息。生成型代理[GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)根据记忆的最近性、重要性和与当前情况的相关性来检索记忆。生成型代理还会生成树状结构的反思，但它们更专注于连续的场景而非面向任务的规则。
- en: 2.4 LLM for Rule Discovery
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 LLM用于规则发现
- en: Several recent works also investigate the rule discovery capabilities of LLM.
    Zhu et al. [LLMLR](https://arxiv.org/html/2405.16247v4#bib.bib40) propose Hypotheses-to-Theories
    (HtT), enabling LLM to induce and deduce rules for basic reasoning tasks. For
    LLM agents, ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) gathers
    the trajectories of Reflexion agents and extracts cross-task rules from them.
    Furthermore, AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    generates state-aware rules and retrieves rules relevant to the test-time state.
    Unlike ExpeL and AutoGuide, which extract rules from offline experiences, we update
    rules in an online manner, verifying their reliability and applicability. For
    more discussion of differences, refer to Appendix C.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的几项研究也探讨了大语言模型（LLM）的规则发现能力。Zhu等人[LLMLR](https://arxiv.org/html/2405.16247v4#bib.bib40)提出了假设到理论（HtT）方法，使LLM能够为基本推理任务归纳和推导规则。对于LLM代理，ExpeL[ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)收集反射代理的轨迹并从中提取跨任务规则。此外，AutoGuide[AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)生成状态感知规则并检索与测试时状态相关的规则。与ExpeL和AutoGuide不同，它们是从离线经验中提取规则，而我们则通过在线方式更新规则，验证其可靠性和适用性。关于差异的更多讨论，请参见附录C。
- en: 3 Methods
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 AutoManual Overview
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 AutoManual概览
- en: 'Our AutoManual framework, shown in Fig [1](https://arxiv.org/html/2405.16247v4#S3.F1
    "Figure 1 ‣ 3.1 AutoManual Overview ‣ 3 Methods ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), consists of three
    main stages. Building stage: The Planner agent and Builder agent collaborate to
    build rules from the interactive environment. The Consolidator agent merges or
    deletes redundant rules when the rules exceed the maximum rule number. Formulating
    stage: The Formulator agent categorizes the rules, summarizes the key points,
    and formulates them into a manual in Markdown form. Testing stage: Based on the
    generated manual, a test-time Planner agent will be evaluated through test tasks
    and scenarios.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的AutoManual框架，如图[1](https://arxiv.org/html/2405.16247v4#S3.F1 "Figure 1 ‣
    3.1 AutoManual Overview ‣ 3 Methods ‣ AutoManual: Constructing Instruction Manuals
    by LLM Agents via Interactive Environmental Learning")所示，由三个主要阶段组成。构建阶段：规划代理和构建代理协作，从互动环境中建立规则。当规则超过最大数量时，整合代理合并或删除冗余规则。制定阶段：制定者代理对规则进行分类，总结要点，并将其整理成Markdown格式的手册。测试阶段：基于生成的手册，测试时规划代理通过测试任务和场景进行评估。'
- en: 'Formally, an Interactive Environment can be modeled as a Partially Observable
    Markov Decision Process (POMDP): $(\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{G},\mathcal{O})$.
    At the start of each episode, a scenario $s_{0}\in\mathcal{S}$ will be initialized,
    a text-grounded task $g\in\mathcal{G}$ and the initial observation $o_{0}\in\mathcal{O}$
    (processed into textual form) will be given. The environment can be interacted
    with through permissible actions (API) set $\mathcal{A}$. After executing an action
    $a\in\mathcal{A}$, the environment will return the result of the action and the
    new observation $o^{\prime}$ based on the dynamics $T(s^{\prime}|s,a)\in\mathcal{T}$
    and $O(o^{\prime}|s^{\prime})$. Finally, when the episode is done, a binary reward
    $r\in\{-1,1\}$ indicating the failure or success of the task will be returned.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，互动环境可以建模为部分可观察马尔可夫决策过程（POMDP）：$(\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{G},\mathcal{O})$。在每个回合开始时，将初始化一个场景$s_{0}\in\mathcal{S}$，给定一个文本驱动的任务$g\in\mathcal{G}$以及初始观察$o_{0}\in\mathcal{O}$（处理为文本形式）。可以通过允许的动作（API）集$\mathcal{A}$与环境进行互动。在执行一个动作$a\in\mathcal{A}$后，环境将返回该动作的结果和新的观察$o^{\prime}$，这基于动态$T(s^{\prime}|s,a)\in\mathcal{T}$和$O(o^{\prime}|s^{\prime})$。最后，当回合结束时，将返回一个二元奖励$r\in\{-1,1\}$，指示任务的失败或成功。
- en: '![Refer to caption](img/f71d9a30b738881af7d00cde742741aa.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f71d9a30b738881af7d00cde742741aa.png)'
- en: 'Figure 1: AutoManual Overview: AutoManual operates in three stages: (1) Building
    Stage: The Planner agent interacts with the environment by coding actionable plans.
    After receiving the current trajectory of the Planner, the Builder agent manages
    rules through the online rule system. (2) Formulating Stage: The Formulator agent
    formulates the resulting rules into a Markdown manual. (3) Testing Stage: A test-time
    Planner agent utilizes the manual to complete testing tasks.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：AutoManual概览：AutoManual分为三个阶段：（1）构建阶段：规划代理通过编写可执行计划与环境互动。在接收到规划代理的当前轨迹后，构建代理通过在线规则系统管理规则。（2）制定阶段：制定者代理将结果规则整理成Markdown手册。（3）测试阶段：测试时规划代理利用手册完成测试任务。
- en: 'We approach the learning of environmental rules as an optimization problem:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习环境规则视为一个优化问题：
- en: '|  | $\max_{\Theta}E_{s_{0},g}E_{\rho(\cdot&#124;\Theta)}r(\tau_{\rho})$ |  |
    (1) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\max_{\Theta}E_{s_{0},g}E_{\rho(\cdot\mid\Theta)}r(\tau_{\rho})$ |  |
    (1) |'
- en: where $\Theta$ denotes all rules in our rule system, $\rho(\cdot|\Theta)$ denotes
    the policy of the Planner given the current rules $\Theta$ and $\tau_{\rho}$ denotes
    a trajectory of $\rho(\cdot|\Theta)$ starting from $[o_{0},g]$. Classic policy
    gradient methods [REINFORCE](https://arxiv.org/html/2405.16247v4#bib.bib29) solve
    such problems through stochastic gradient ascent, i.e., executing the current
    policy to obtain the episodic reward and back-propagating gradients to update
    the parameters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\Theta$表示我们规则系统中的所有规则，$\rho(\cdot|\Theta)$表示规划者在当前规则$\Theta$下的策略，$\tau_{\rho}$表示从$[o_{0},g]$开始的$\rho(\cdot|\Theta)$轨迹。经典的策略梯度方法[REINFORCE](https://arxiv.org/html/2405.16247v4#bib.bib29)通过随机梯度上升来解决此类问题，即执行当前策略以获得集奖励，并通过反向传播梯度来更新参数。
- en: 'Inspired by this online reinforcement learning paradigm, we follow two alternative
    processes to optimize the rules $\Theta$: 1\. The Planner practices the current
    rules through interaction within an episode. 2\. The Builder updates the rules
    $\Theta$ based on this trajectory. Compared to traditional parameter optimization,
    sample-inefficient gradient ascent is replaced by text-based rule management.
    We design a well-structured rule system described in Section [3.3](https://arxiv.org/html/2405.16247v4#S3.SS3
    "3.3 Builder and Consolidator Agents for Rule Management ‣ 3 Methods ‣ AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")
    to ensure the rule updating contributes to rewards. Additionally, to limit the
    role of human expertise, we only provide a simple example demonstrating the output
    format to agents. Then, manually derive several initial rules from this example
    as the starting point of the optimization.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '受这种在线强化学习范式的启发，我们通过两种替代过程来优化规则$\Theta$：1\. 规划者通过与环境的交互来实践当前规则。2\. 构建者根据这个轨迹更新规则$\Theta$。与传统的参数优化方法相比，低效的梯度上升方法被基于文本的规则管理所取代。我们设计了一个结构良好的规则系统，详见第[3.3](https://arxiv.org/html/2405.16247v4#S3.SS3
    "3.3 Builder and Consolidator Agents for Rule Management ‣ 3 Methods ‣ AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")节，以确保规则更新有助于奖励的增加。此外，为了限制人工专业知识的作用，我们仅提供一个简单的示例来展示输出格式，并通过这个示例手动推导出几个初始规则，作为优化的起点。'
- en: 3.2 Planner Agent for Interactive Planning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 交互规划的规划者代理
- en: As demonstrated by the success of Voyager [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    and AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) , code-based
    planning can leverage the powerful programming capability of LLM and automatically
    react to in-plan feedback. Voyager and AdaPlanner output and refine a complete
    solution function for the task, which is potentially reusable. However, this function-form
    output is difficult to adjust in response to environmental feedback, as it requires
    maintaining the integrity of the plan throughout.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)和[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)的成功所示，基于代码的规划可以利用LLM强大的编程能力，并自动响应计划中的反馈。Voyager和AdaPlanner输出并完善任务的完整解决方案函数，这个函数是可复用的。然而，这种函数形式的输出难以根据环境反馈进行调整，因为它需要保持计划的完整性。
- en: 'Our Planner Agent outputs free-form code as its plan, which aligns more with
    the natural programming capabilities of LLM [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)
    ; [ChatGPTFR](https://arxiv.org/html/2405.16247v4#bib.bib22) . This form simplifies
    planning by only generating code necessary for the current environmental situation
    and feedback without the overhead of integrating previously executed code. As
    shown in Fig [2](https://arxiv.org/html/2405.16247v4#S3.F2 "Figure 2 ‣ 3.2 Planner
    Agent for Interactive Planning ‣ 3 Methods ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), at the start of
    a new episode, the Planner receives system prompts, current rules $\Theta$, relevant
    samples from the skill and reflection libraries, the target task $g$, and initial
    observation $o_{0}$. System prompts contain the role, permissible actions $\mathcal{A}$,
    response guidelines, and a simple example (detailed in Appendix H). The output
    of the Planner is structured into four segments during each cycle:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的规划代理输出自由形式的代码作为其计划，这与LLM的自然编程能力更为契合[CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)；[ChatGPTFR](https://arxiv.org/html/2405.16247v4#bib.bib22)。这种形式通过仅生成当前环境情况和反馈所需的代码，而无需整合之前执行的代码，从而简化了规划过程。如图[2](https://arxiv.org/html/2405.16247v4#S3.F2
    "图 2 ‣ 3.2 交互式规划的规划代理 ‣ 3 方法 ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")所示，在新情节开始时，规划者会收到系统提示、当前规则$\Theta$、来自技能和反思库的相关样本、目标任务$g$以及初始观察$o_{0}$。系统提示包括角色、可执行的动作$\mathcal{A}$、响应指南以及一个简单示例（详见附录H）。在每个周期中，规划者的输出分为四个部分：
- en: '1.'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Analysis: The understanding of the current situation and reflection on previous
    errors if exist.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分析：对当前情况的理解以及对先前错误的反思（如果存在的话）。
- en: '2.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Related Rules: Rules (along with their IDs) that need to be considered in this
    situation.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相关规则：在此情境下需要考虑的规则（及其ID）。
- en: '3.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Overall Plan: The general plan to complete the task.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总体计划：完成任务的总体计划。
- en: '4.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Code: A block of Python code divided into steps. The Planner is encouraged
    to define helpful functions in the code, which might be reusable in similar scenarios.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码：一段分步骤的Python代码。鼓励规划者在代码中定义有用的函数，这些函数可能在类似情境下重复使用。
- en: We denote this response of the Planner as $[\textit{thought}_{t},\textit{code}_{t}]$,
    where $\textit{thought}_{t}$ denotes the first three segments. $\textit{code}_{t}$
    executed in the environment is followed by feedback $c_{t}$, which informs the
    subsequent output cycle. This process iterates until the episode ends or a response
    limit is reached.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将规划者的这一反应表示为$[\textit{thought}_{t},\textit{code}_{t}]$，其中$\textit{thought}_{t}$表示前三个部分。$\textit{code}_{t}$是在环境中执行的代码，之后是反馈$c_{t}$，它会影响随后的输出周期。这个过程会迭代进行，直到情节结束或达到响应限制为止。
- en: 'As shown in Fig [2](https://arxiv.org/html/2405.16247v4#S3.F2 "Figure 2 ‣ 3.2
    Planner Agent for Interactive Planning ‣ 3 Methods ‣ AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning"), according
    to the episodic reward, we categorize the result into Direct Success, Indirect
    Success (errors occurred but were solved later), and Failure. In the case of Direct
    or Indirect Success, the Planner will be prompted to organize its previous code
    into a code block. For Indirect Success, it additionally summarizes the mistakes
    and misunderstandings that cause errors. For the Failure case, the Planner will
    be prompted to reflect on the reason for the failure carefully, suggest reasonable
    corrections, and specify the code segment that caused the error. We denote this
    response of the Planner as conclusion. Finally, we obtain a trajectory of the
    Planner:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[2](https://arxiv.org/html/2405.16247v4#S3.F2 "图 2 ‣ 3.2 交互式规划的规划代理 ‣ 3 方法
    ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")所示，根据情节奖励，我们将结果分为直接成功、间接成功（发生错误但后续解决）和失败。在直接成功或间接成功的情况下，规划者将被提示将其之前的代码整理成代码块。对于间接成功，规划者还会总结导致错误的失误和误解。对于失败的情况，规划者将被提示仔细反思失败的原因，提出合理的改正建议，并指定导致错误的代码段。我们将规划者的这一反应称为结论。最终，我们获得规划者的轨迹：
- en: '|  | $\tau_{\rho}=(o_{0},g,[\textit{thought}_{1},\textit{code}_{1}],c_{1},...,[%
    \textit{thought}_{T},\textit{code}_{T}],c_{T},\textit{conclusion})$ |  | (2) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau_{\rho}=(o_{0},g,[\textit{thought}_{1},\textit{code}_{1}],c_{1},...,[%
    \textit{thought}_{T},\textit{code}_{T}],c_{T},\textit{conclusion})$ |  | (2) |'
- en: '![Refer to caption](img/244f3530ebb1d0fd0e1e4116dbfedb42.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/244f3530ebb1d0fd0e1e4116dbfedb42.png)'
- en: 'Figure 2: The Planner Trajectory: Given the current task and rules, the Planner
    will interact with the environment through free-form code. Based on the trajectory
    result, the Planner will generate a corresponding conclusion, which will be saved
    in the skill or reflection library.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：规划者轨迹：根据当前任务和规则，规划者将通过自由格式代码与环境进行交互。根据轨迹结果，规划者将生成相应的结论，并将其保存在技能或反思库中。
- en: 'Skill Library and Reflection Library: Apart from rules, we also manage and
    transmit conclusions from previous episodes, which provide essential details for
    generating planning code. In the case of Direct or Indirect Success, we save the
    code block in conclusion as a skill for that task type ¹¹1All tasks in ALFWorld
    are divided into 6 task types, e.g., pick_heat_then_place, look_at_obj_in_light.
    For each task type, we store only one skill code. into the skill library [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24) . In the Failure case,
    we save its conclusion as a reflection for that task type into the reflection
    library. When a new task comes, the code block of the most similar task is retrieved
    from the skill library. If there is no existing skill for the new task type, the
    reflection for that task type will be returned. As mentioned in the Introduction,
    compared with rules, these skills and reflections contain more programming details
    but are less generalizable to new scenarios, i.e., the Path Dependence problem.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 技能库和反思库：除了规则外，我们还管理并传递先前回合的结论，这些结论为生成规划代码提供了必要的细节。在直接或间接成功的情况下，我们将结论中的代码块保存为该任务类型的技能¹¹1所有任务在ALFWorld中被分为6种任务类型，例如：pick_heat_then_place，look_at_obj_in_light。对于每个任务类型，我们仅存储一个技能代码。并将其存入技能库[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)；[Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)。在失败的情况下，我们将其结论作为该任务类型的反思保存到反思库中。当出现新任务时，最相似任务的代码块将从技能库中检索。如果新任务类型没有现有技能，将返回该任务类型的反思。如介绍中所述，与规则相比，这些技能和反思包含更多编程细节，但对新场景的泛化能力较弱，即路径依赖问题。
- en: 'Cooperation between Agents: In our framework, rule management is not solely
    the responsibility of the Builder; the Planner also plays a critical role by explicitly
    identifying the rules it engages in its response. This cooperation is facilitated
    by including the Planner’s thoughts within the trajectory $\tau$, which is provided
    to the Builder. This synergy enhances the identification and adjustment of problematic
    rules. In addition, conclusion from the Planner contains the detailed success
    process or reflections on errors, which further assist the Builder in managing
    corresponding types of rules.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 代理之间的合作：在我们的框架中，规则管理不仅仅是构建者的责任；规划者也通过明确识别其响应中所涉及的规则，扮演着关键角色。通过将规划者的思维包含在轨迹$\tau$中并提供给构建者，促进了这种合作。这种协同作用增强了对有问题规则的识别和调整。此外，规划者的结论包含了详细的成功过程或对错误的反思，这进一步帮助构建者管理相应类型的规则。
- en: 3.3 Builder and Consolidator Agents for Rule Management
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 构建者和整理者代理用于规则管理
- en: Upon receiving the trajectory $\tau_{\rho}$, the Builder has to manage the rules
    through the rule system.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到轨迹$\tau_{\rho}$后，构建者需要通过规则系统管理规则。
- en: '![Refer to caption](img/d437e549c8e67933da37385d02017477.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d437e549c8e67933da37385d02017477.png)'
- en: 'Figure 3: Case-Conditioned Prompts: Given the current trajectory, the Builder
    classifies the cause of the major error as "Imperfect Rules" or "Imperfect Agents".
    Then, the Builder will get the base prompt and corresponding prompt to guide its
    rule management.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：案例条件提示：根据当前轨迹，构建者将主要错误的原因分类为“规则不完善”或“代理不完善”。然后，构建者将获取基本提示和相应的提示来指导其规则管理。
- en: 'Rule System: We intuitively identify rules as the kinds of knowledge that help
    task completion, including the analyses of the observed phenomenon $T(o^{\prime}|o,a)$,
    the mechanism $T(s^{\prime}|s,a)$, and the correlation between the reward $r$
    and $\tau_{\rho}$, i.e., the success process or the occurred error. Therefore,
    unlike ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) and AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    , which derive general insight from the trajectory, our system categorizes six
    specific rule types to extract environmental knowledge that targets different
    aspects of the trajectory. Furthermore, each rule in our system is enhanced with
    Example attribute to illustrate its application and important details, making
    it grounded and well-understood. Specifically, each rule in the rule system has
    these four attributes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 规则系统：我们直观地将规则定义为有助于任务完成的知识类型，包括对观察现象 $T(o^{\prime}|o,a)$ 的分析、机制 $T(s^{\prime}|s,a)$、以及奖励
    $r$ 和 $\tau_{\rho}$ 之间的关联，即成功过程或发生的错误。因此，与从轨迹中推导一般性见解的 ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    和 AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2) 不同，我们的系统将规则分类为六种具体类型，以提取针对轨迹不同方面的环境知识。此外，系统中的每个规则都通过示例属性进行增强，以说明其应用和重要细节，使其更加具体且易于理解。具体而言，规则系统中的每个规则都有以下四个属性：
- en: '1.'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Rule Type: The type of the rule, chosen from [“Special Phenomenon”, “Special
    Mechanism”, “Useful Helper Method”, “Success Process”, “Corrected Error”, “Unsolved
    Error”];'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规则类型：规则的类型，选择自【“特殊现象”、“特殊机制”、“有用的辅助方法”、“成功过程”、“修正错误”、“未解决错误”】；
- en: '2.'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Rule Content: A description of the rule, beginning with the scope of its applicable
    scenarios;'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规则内容：规则的描述，开始时列出其适用场景的范围；
- en: '3.'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Example: An example or code from the trajectory demonstrates this rule, where
    additional remarks, e.g. error-prone details, can also be added to it;'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：轨迹中的一个示例或代码演示此规则，在其中可以添加额外的备注，如易错细节等；
- en: '4.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Validation Logs: Logs that track the rule’s application and updates, including
    episode and rule IDs that trace the rule’s evolution, serving as a reference for
    the Builder and Consolidator.'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证日志：跟踪规则应用和更新的日志，包括跟踪规则演变的剧集和规则 ID，为构建器和整合者提供参考。
- en: 'The Builder manages the rules through the following functions of the rule system:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 构建器通过规则系统的以下功能管理规则：
- en: •
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'write_rule(**rule_attributes): Write down a new rule with its four attributes.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'write_rule(**rule_attributes): 写下一个新规则及其四个属性。'
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'update_rule(rule_id, **rule_attributes): Rewrite the attributes of a existing
    rule.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'update_rule(rule_id, **rule_attributes): 重写现有规则的属性。'
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'stop_generating(): When the trajectory is not needed or insufficient to derive
    any more new rules, the function should be called.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'stop_generating(): 当轨迹不再需要或不足以推导出更多新规则时，应调用此功能。'
- en: Similar to hierarchical reflections in Generative Agents [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)
    , we allow the Builder to utilize existing rules to induce more general or deeper
    rules and record their dependence in Rule Content or Validation Logs, more discussed
    in Appendix D.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于生成型代理中的层次性反思 [GenerativeAI](https://arxiv.org/html/2405.16247v4#bib.bib14)，我们允许构建器利用现有规则来推导出更一般或更深层的规则，并在规则内容或验证日志中记录它们的依赖关系，具体内容将在附录
    D 中进一步讨论。
- en: 'Case-Conditioned Prompting: To mitigate the risk of erroneous rule creation,
    such as deriving rules of success from a failed trajectory, we employ case-conditioned
    prompts. As illustrated in Fig [3](https://arxiv.org/html/2405.16247v4#S3.F3 "Figure
    3 ‣ 3.3 Builder and Consolidator Agents for Rule Management ‣ 3 Methods ‣ AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning"),
    the Builder first analyzes and determines if the major errors stem from “Imperfect
    Rules” or “Imperfect Agent”. Based on this analysis and the trajectory results,
    targeted prompts guide the Builder in rule management ²²2Notice: These prompts
    for the Builder are environment-independent and shared across all environments..
    For example, in a case of indirect success due to imperfect rules (Case $2$),
    the prompts will guide the Builder to extract or update the success process, helper
    methods, and error reflections in corresponding rule types. Finally, the Builder
    responds with the potential rules detailing their relation with existing rules
    and uses the functions of the rule system to manage rules.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 案例条件提示：为了降低因从失败轨迹推导成功规则而导致错误规则创建的风险，我们采用了案例条件提示。如图[3](https://arxiv.org/html/2405.16247v4#S3.F3
    "图 3 ‣ 3.3 规则管理的构建器与整合器代理 ‣ 3 方法 ‣ AutoManual：通过互动环境学习构建 LLM 代理的操作手册")所示，构建器首先分析并确定主要错误来源是“规则不完善”还是“代理不完善”。基于这一分析和轨迹结果，针对性提示引导构建器进行规则管理²²2注意：这些针对构建器的提示是环境无关的，并在所有环境中共享。例如，在由于规则不完善导致的间接成功的情况下（案例
    $2$），提示将引导构建器提取或更新成功过程、辅助方法和错误反思，并应用于相应的规则类型。最后，构建器回应潜在规则，详细说明它们与现有规则的关系，并使用规则系统的功能进行规则管理。
- en: 'Rule Consolidation: When the number of rules in the rule system exceeds $N_{max}$,
    the Consolidator agent steps in to consolidate related rules and delete redundant
    rules. It uses three functions of the rule system: get_trajectory(episode_id),
    update_rule(rule_id, **rule_attributes) and delete_rule(rule_id). Given the current
    rules, the Consolidator identifies potentially relevant or overlapped rules, uses
    get_trajectory function to investigate the trajectories they depend on, and finally
    calls the remaining functions to manage the rules. During the management, the
    Consolidator ensures that consolidation retains details of rules and examples.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 规则整合：当规则系统中的规则数量超过 $N_{max}$ 时，整合器代理介入，整合相关规则并删除冗余规则。它使用规则系统的三个功能：get_trajectory(episode_id)、update_rule(rule_id,
    **rule_attributes) 和 delete_rule(rule_id)。根据当前的规则，整合器识别可能相关或重叠的规则，使用 get_trajectory
    函数调查它们所依赖的轨迹，最后调用其余的函数进行规则管理。在管理过程中，整合器确保整合保留规则和示例的细节。
- en: 3.4 Manual Formulation
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 手动制定
- en: 'Once the building stage is complete, we can obtain a set of rules targeted
    to different situations, whose applicability has been validated through online
    optimization. Our next goal is to enhance their readability and global understanding.
    To achieve this, we introduce the Formulator agent, designed to transform these
    rules into a user-friendly manual, analogous to a teacher imparting a wealth of
    knowledge through easily digestible lessons. As depicted in Fig [1](https://arxiv.org/html/2405.16247v4#S3.F1
    "Figure 1 ‣ 3.1 AutoManual Overview ‣ 3 Methods ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), the Formulator
    begins by categorizing all rules based on their target scenarios. This categorization
    aids in structuring the manual and ensures that related rules are discussed together,
    which enhances the logical flow and accessibility of the information. For each
    category, the Formulator drafts an introduction, summarizing the rules it contains
    and highlighting the key points and overall principles that govern the specific
    scenarios. Finally, the Formulator compiles the rules and their introductions
    into a comprehensive manual formatted in Markdown.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦构建阶段完成，我们可以获得一套针对不同情况的规则，这些规则的适用性已通过在线优化得到了验证。我们的下一个目标是提升这些规则的可读性和全局理解性。为此，我们引入了Formulator代理，它旨在将这些规则转化为用户友好的手册，类似于教师通过易于理解的课程传授大量知识。正如图[1](https://arxiv.org/html/2405.16247v4#S3.F1
    "Figure 1 ‣ 3.1 AutoManual Overview ‣ 3 Methods ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning")所示，Formulator首先根据目标场景对所有规则进行分类。这种分类有助于手册的结构化，并确保相关规则能够一起讨论，从而增强信息的逻辑流动性和可访问性。对于每个类别，Formulator会草拟一个介绍，总结其包含的规则，并突出特定场景下的关键点和总体原则。最后，Formulator将规则及其介绍汇编成一个全面的手册，并以Markdown格式进行整理。'
- en: 4 Experiments
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In line with prior works [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) , we conduct the
    experiments on three interactive environments: (1) ALFWorld [ALFWorld](https://arxiv.org/html/2405.16247v4#bib.bib17)
    is a text-based virtual household environment containing six distinct task types.
    We run the building stage on 36 tasks (6 tasks for each task type) sampled from
    the training set of ALFWorld, and each task is run only once. Following previous
    works [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    ; [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) , we run the testing
    stage on the validation unseen set containing 134 tasks across these six types.
    (2) MiniWoB++ [MiniWoB](https://arxiv.org/html/2405.16247v4#bib.bib8) is a simulated
    web environment where agents complete diverse tasks on the Internet by performing
    keyboard and mouse actions. Prior works [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5)
    ; [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) selects 9 task types
    with environmental feedback and 44 task types without feedback from MiniWoB++
    tasks. We perform experiments on 9 task types with feedback or on all 53 task
    types. At each stage, we randomly sample 6 tasks for each task type. (3) WebArena [WebArena](https://arxiv.org/html/2405.16247v4#bib.bib37)
    introduces realistic web environments by emulating the functionality and data
    of popular websites. This benchmark poses significant challenges for LLM agents
    due to its large observation and action space, along with tasks that require longer
    planning horizons. Following AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    , our experiments focus on the Reddit domain within WebArena.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的研究工作[AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)；[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)一致，我们在三个互动环境中进行了实验：（1）ALFWorld
    [ALFWorld](https://arxiv.org/html/2405.16247v4#bib.bib17)是一个基于文本的虚拟家庭环境，包含六种不同的任务类型。我们在ALFWorld的训练集上对36个任务（每种任务类型6个任务）进行了构建阶段实验，每个任务仅运行一次。参考以往的研究[Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)；[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)；[ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)，我们在验证集的未见任务集上进行测试阶段实验，验证集包含这六种类型中的134个任务。（2）MiniWoB++
    [MiniWoB](https://arxiv.org/html/2405.16247v4#bib.bib8)是一个模拟的网页环境，代理通过执行键盘和鼠标操作在互联网上完成各种任务。之前的研究[RCI](https://arxiv.org/html/2405.16247v4#bib.bib5)；[AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)从MiniWoB++任务中选择了9种带环境反馈的任务类型和44种不带反馈的任务类型。我们在9种带反馈的任务类型或所有53种任务类型上进行了实验。在每个阶段，我们从每种任务类型中随机抽取6个任务。（3）WebArena
    [WebArena](https://arxiv.org/html/2405.16247v4#bib.bib37)通过模拟流行网站的功能和数据引入了真实的网页环境。由于其巨大的观察和行动空间以及需要较长规划周期的任务，这个基准对LLM代理提出了重大挑战。参考AutoGuide
    [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)，我们的实验重点在于WebArena中的Reddit领域。
- en: During the building and formulating stages, we use GPT-4-turbo (gpt-4-1106-preview)
    for all agents. At the testing stage, we equip the Planner agent with GPT-4-turbo
    or GPT-3.5-turbo (gpt-3.5-turbo-1106), to evaluate the effect of generated manuals
    on relatively smaller LLM.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建和制定阶段，我们为所有代理使用GPT-4-turbo（gpt-4-1106-preview）。在测试阶段，我们为Planner代理配备了GPT-4-turbo或GPT-3.5-turbo（gpt-3.5-turbo-1106），以评估生成的手册对相对较小的LLM的影响。
- en: 'Compared Methods: In the experiments, we compare AutoManual with the following
    methods of LLM Agent: (1) ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)
    prompts LLM to generate the reasoning trace using CoT [CoT](https://arxiv.org/html/2405.16247v4#bib.bib28)
    and next-step action; (2) Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    agents generate reflection on task feedback signals, which is saved in the memory
    for subsequent trials; (3) ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    extract insights and skills from the offline trajectories of Reflexion agents;
    (4) RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) agent recursively
    criticizes and improves its output for solving computer tasks; (5) AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    allows the LLM agent to generate and adaptively refine a code-style plan; (6)
    Planner+Lib. represents our Planner agent equipped with skill and reflection libraries
    (§[3.2](https://arxiv.org/html/2405.16247v4#S3.SS2 "3.2 Planner Agent for Interactive
    Planning ‣ 3 Methods ‣ AutoManual: Constructing Instruction Manuals by LLM Agents
    via Interactive Environmental Learning")) during building and testing stages without
    any rules. We re-implement prior methods with GPT-3.5 and GPT-4 versions the same
    as ours for fair comparisons.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '比较方法：在实验中，我们将 AutoManual 与以下 LLM 代理方法进行比较：（1）ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33)
    提示 LLM 生成使用 CoT [CoT](https://arxiv.org/html/2405.16247v4#bib.bib28) 和下一步动作的推理轨迹；（2）Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    代理生成任务反馈信号的反思，并将其保存在记忆中以供后续试验使用；（3）ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    从 Reflexion 代理的离线轨迹中提取见解和技能；（4）RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5)
    代理递归批评并改进其输出以解决计算机任务；（5）AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    允许 LLM 代理生成并自适应地完善代码风格的计划；（6）Planner+Lib. 代表我们装备了技能和反思库的 Planner 代理（§[3.2](https://arxiv.org/html/2405.16247v4#S3.SS2
    "3.2 Planner Agent for Interactive Planning ‣ 3 Methods ‣ AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning")），在构建和测试阶段不使用任何规则。我们使用与我们相同的
    GPT-3.5 和 GPT-4 版本重新实现了先前的方法，以确保公平比较。'
- en: ReAct, Reflexion, and ExpeL provide LLM agents with $12$ human examples ($2$
    examples per task type) of ALFWorld. For AdaPlanner, they provide 6 human examples
    ($1$ example per task type) of ALFWorld as the start of skill discovery. For our
    methods, agents are provided only one human example of the simplest task (Put)
    on ALFWorld. On MiniWob++, our agents are provided one human example (search-engine)
    for tasks with feedback and 4 examples for all tasks. On WebArena, our agents
    are also provided with one human demonstration. To reduce randomness, we performed
    each experiment three times and reported the average. More details of the implementation
    and prompts for AutoManual can be found in the Appendix.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ReAct、Reflexion 和 ExpeL 为 LLM 代理提供了 ALFWorld 的 $12$ 个人工示例（每个任务类型 $2$ 个示例）。对于
    AdaPlanner，提供了 ALFWorld 的 $6$ 个人工示例（每个任务类型 $1$ 个示例），用于技能发现的开始。对于我们的方法，代理只提供了 ALFWorld
    上最简单任务（放置）的一个人工示例。在 MiniWob++ 上，我们的代理为有反馈的任务提供了一个人工示例（搜索引擎），并为所有任务提供了 4 个示例。在
    WebArena 上，我们的代理也提供了一个人工演示。为了减少随机性，我们每个实验进行了三次并报告了平均值。更多关于实现和 AutoManual 提示的细节，请见附录。
- en: 4.1 Main Results
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果
- en: 'Table 1: Success rate (%) of LLM agent methods on ALFWorld test tasks. For
    each method, the number of all human examples used is listed. “Planner+Lib.” represents
    only using skill&reflection library during the building and testing stages. We
    run all experiments 3 times and show the average.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：LLM 代理方法在 ALFWorld 测试任务中的成功率（%）。对于每种方法，列出了使用的所有人工示例的数量。“Planner+Lib.” 仅表示在构建和测试阶段使用技能与反思库。我们将所有实验运行
    3 次，并显示平均值。
- en: '| Methods | Examples | Put | Clean | Heat | Cool | Examine | Put two | ALL
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 示例 | 放置 | 清理 | 加热 | 冷却 | 检查 | 放置两个 | 所有 |'
- en: '| Testing LLM: GPT-3.5-turbo |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 测试 LLM: GPT-3.5-turbo |'
- en: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 12 | 75.0
    | 24.7 | 37.7 | 36.4 | 44.4 | 11.8 | 41.9 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 12 | 75.0
    | 24.7 | 37.7 | 36.4 | 44.4 | 11.8 | 41.9 |'
- en: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | 12
    | 87.5 | 44.1 | 73.9 | 50.0 | 61.1 | 35.3 | 59.8 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | 12
    | 87.5 | 44.1 | 73.9 | 50.0 | 61.1 | 35.3 | 59.8 |'
- en: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | 12 | 62.5
    | 61.3 | 30.4 | 61.9 | 55.5 | 35.3 | 52.2 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | 12 | 62.5
    | 61.3 | 30.4 | 61.9 | 55.5 | 35.3 | 52.2 |'
- en: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    6 | 83.3 | 46.2 | 65.2 | 74.2 | 68.5 | 52.9 | 63.3 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    6 | 83.3 | 46.2 | 65.2 | 74.2 | 68.5 | 52.9 | 63.3 |'
- en: '| Planner+Lib. | 1 | 77.8 | 88.2 | 82.6 | 72.7 | 37.0 | 27.5 | 66.5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Planner+Lib. | 1 | 77.8 | 88.2 | 82.6 | 72.7 | 37.0 | 27.5 | 66.5 |'
- en: '| AutoManual | 1 | 95.8 | 79.6 | 87.0 | 78.8 | 100.0 | 66.7 | 86.2 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 1 | 95.8 | 79.6 | 87.0 | 78.8 | 100.0 | 66.7 | 86.2 |'
- en: '| Testing LLM: GPT-4-turbo |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 测试 LLM: GPT-4-turbo |'
- en: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 12 | 95.8
    | 76.3 | 69.6 | 86.4 | 72.2 | 52.9 | 76.8 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 12 | 95.8
    | 76.3 | 69.6 | 86.4 | 72.2 | 52.9 | 76.8 |'
- en: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | 12
    | 100.0 | 95.7 | 78.3 | 86.4 | 77.8 | 70.6 | 85.9 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | 12
    | 100.0 | 95.7 | 78.3 | 86.4 | 77.8 | 70.6 | 85.9 |'
- en: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | 12 | 94.4
    | 82.8 | 72.4 | 81.8 | 72.2 | 58.8 | 79.2 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | 12 | 94.4
    | 82.8 | 72.4 | 81.8 | 72.2 | 58.8 | 79.2 |'
- en: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    6 | 88.9 | 90.3 | 85.5 | 75.8 | 64.8 | 41.2 | 76.4 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    6 | 88.9 | 90.3 | 85.5 | 75.8 | 64.8 | 41.2 | 76.4 |'
- en: '| Planner+Lib. | 1 | 100.0 | 93.5 | 100.0 | 93.9 | 88.9 | 39.2 | 88.1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| Planner+Lib. | 1 | 100.0 | 93.5 | 100.0 | 93.9 | 88.9 | 39.2 | 88.1 |'
- en: '| AutoManual | 1 | 100.0 | 98.9 | 100.0 | 95.4 | 100.0 | 90.2 | 97.4 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 1 | 100.0 | 98.9 | 100.0 | 95.4 | 100.0 | 90.2 | 97.4 |'
- en: 'Table 2: Success rate (%) of LLM agent methods on 9 task types with feedback
    and all 53 task types of MiniWoB++. For each method, the number of human examples
    used is listed.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: LLM 代理方法在9种任务类型（带反馈）和MiniWoB++的所有53种任务类型上的成功率（%）。每种方法使用的人工示例数量列出。'
- en: '| Methods | Examples | With feedback (9 types) | Examples | ALL (53 types)
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 示例 | 有反馈（9种类型） | 示例 | 所有类型（53种类型） |'
- en: '| Testing LLM: GPT-3.5-turbo |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 测试 LLM: GPT-3.5-turbo |  |'
- en: '| RCI [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 22 | 45.6 |
    104 | 77.3 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| RCI [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 22 | 45.6 |
    104 | 77.3 |'
- en: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    13 | 71.6 | 38 | 89.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    13 | 71.6 | 38 | 89.4 |'
- en: '| Planner+Lib. | 1 | 63.6 | 4 | 87.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Planner+Lib. | 1 | 63.6 | 4 | 87.0 |'
- en: '| AutoManual | 1 | 82.2 | 4 | 92.7 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 1 | 82.2 | 4 | 92.7 |'
- en: '| Testing LLM: GPT-4-turbo |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 测试 LLM: GPT-4-turbo |  |'
- en: '| RCI [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 22 | 60.4 |
    104 | 88.6 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| RCI [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | 22 | 60.4 |
    104 | 88.6 |'
- en: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    13 | 74.1 | 38 | 90.3 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    13 | 74.1 | 38 | 90.3 |'
- en: '| Planner+Lib. | 1 | 80.2 | 4 | 94.4 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Planner+Lib. | 1 | 80.2 | 4 | 94.4 |'
- en: '| AutoManual | 1 | 94.5 | 4 | 98.3 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 1 | 94.5 | 4 | 98.3 |'
- en: 'Table 3: Test on WebArena (Reddit).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 在 WebArena (Reddit) 上的测试。'
- en: '| Methods | Examples | Suc(%) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 示例 | 成功率(%) |'
- en: '| --- | --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| ReAct [WebArena](https://arxiv.org/html/2405.16247v4#bib.bib37) | 2 | 6.0
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [WebArena](https://arxiv.org/html/2405.16247v4#bib.bib37) | 2 | 6.0
    |'
- en: '| AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2) | 19
    | 43.7 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2) | 19
    | 43.7 |'
- en: '| SteP [SteP](https://arxiv.org/html/2405.16247v4#bib.bib19) | 14 | 55.0 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SteP [SteP](https://arxiv.org/html/2405.16247v4#bib.bib19) | 14 | 55.0 |'
- en: '| Planner | 1 | 51.1 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Planner | 1 | 51.1 |'
- en: '| AutoManual | 1 | 65.1 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 1 | 65.1 |'
- en: 'Main Results on ALFWorld: As shown in Tab. [1](https://arxiv.org/html/2405.16247v4#S4.T1
    "Table 1 ‣ 4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), AutoManual significantly
    outperforms the existing methods, evidenced by overall success rates of 86.2%
    when using GPT-3.5-turbo for the testing stage and 97.4% when using GPT-4-turbo.
    Noticeably, AutoManual requires little expert prior knowledge about the environment
    and is only provided with one human example to achieve excellent results. In comparison,
    the rules induced by ExpeL hardly improve performance, as its offline trajectories
    are composed of individual actions rather than code. We find the performance of
    AdaPlanner is lower than reported. One reason is that AdaPlanner requires LLM
    to output specific formats to complete its function-form code, which is difficult
    for creative LLM, e.g., GPT-4-turbo. In addition, AdaPlanner and Planner+Lib.
    are inferior to AutoManual because they only store successful paths as skills
    and inevitably face the Path Dependence problem. Especially, tasks in Put Two
    have various scenarios, such as “two objects can occur at the same receptacle
    or different receptacles”, that require different processes to solve (Appendix
    G shows an example). Furthermore, Planner+Lib. often does not mark error-prone
    points in its skills, such as “target objects may appear in unconventional locations”.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 'ALFWorld上的主要结果：如表[1](https://arxiv.org/html/2405.16247v4#S4.T1 "Table 1 ‣ 4.1
    Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction Manuals by
    LLM Agents via Interactive Environmental Learning")所示，AutoManual显著优于现有方法，使用GPT-3.5-turbo进行测试时的整体成功率为86.2%，使用GPT-4-turbo时为97.4%。值得注意的是，AutoManual对环境的先验知识要求较少，只需一个人类示例即可取得优异的结果。相比之下，ExpeL引导的规则几乎没有提高性能，因为其离线轨迹由单一动作组成，而非代码。我们发现AdaPlanner的表现低于报告结果。一个原因是AdaPlanner要求LLM输出特定格式以完成其功能型代码，而这对创意型LLM（如GPT-4-turbo）来说较为困难。此外，AdaPlanner和Planner+Lib.不如AutoManual，因为它们仅将成功路径作为技能存储，难免面临路径依赖问题。特别是在Put
    Two任务中，存在多种场景，如“两个物体可以出现在同一个接收器或不同接收器中”，这些场景需要不同的处理流程来解决（附录G展示了一个示例）。此外，Planner+Lib.常常未标记其技能中的易出错点，如“目标物体可能出现在非常规位置”。'
- en: 'Main Results on MiniWoB++: As shown in Tab. [2](https://arxiv.org/html/2405.16247v4#S4.T2
    "Table 2 ‣ 4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), the performance
    of AutoManual exceeds the previous methods and Planner+Lib. by a large margin.
    Especially in 9 task types with feedback, these tasks have higher diversity and
    require LLM agents to cope with various situations. For example, the tasks in
    login-user-popup type will interrupt the agent’s plan at any time, requiring the
    agent to cope with unexpected situations. Therefore, solely imitating previous
    successful experiences without extracting targeted rules will lead to task failure.
    Additionally, due to the flexibility of free-form codes, our method shows better
    adaptability while requiring fewer expert examples than prior methods.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 'MiniWoB++上的主要结果：如表[2](https://arxiv.org/html/2405.16247v4#S4.T2 "Table 2 ‣
    4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction Manuals
    by LLM Agents via Interactive Environmental Learning")所示，AutoManual的表现远超以往的方法和Planner+Lib.。特别是在9种带反馈的任务类型中，这些任务具有较高的多样性，要求LLM代理应对各种情况。例如，登录用户弹窗类型的任务会随时中断代理的计划，要求代理应对突发情况。因此，单纯模仿之前的成功经验而不提取针对性的规则，会导致任务失败。此外，由于自由形式代码的灵活性，我们的方法在适应性上表现更好，并且所需的专家示例少于以往的方法。'
- en: '![Refer to caption](img/d8f7602383348646a98e3aee1f35d495.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d8f7602383348646a98e3aee1f35d495.png)'
- en: (a) Cross-Task v.s. Single-Task Type.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: （a）跨任务类型与单任务类型对比。
- en: '![Refer to caption](img/829059998d27fb68ab5b4d9f7d1795de.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/829059998d27fb68ab5b4d9f7d1795de.png)'
- en: (b) AutoManual v.s. Skill Library.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: （b）AutoManual 与技能库对比。
- en: 'Figure 4: (a) The success rate curve with standard deviation when testing GPT-4-turbo
    or GPT-3.5-turbo on ALFWorld. Building is performed cross-task or single-task
    type. (b) The success rate curve with standard deviation using AutoManual or Planner+Lib.
    when testing with GPT-4-turbo or GPT-3.5-turbo on 9 task types with feedback in
    MiniWob++.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：（a）在ALFWorld上测试GPT-4-turbo或GPT-3.5-turbo时，带标准差的成功率曲线。构建是在跨任务或单任务类型上进行的。（b）在MiniWob++上使用AutoManual或Planner+Lib.进行测试时，带标准差的成功率曲线，任务为9种类型，并包含反馈。
- en: 'Table 4: Ablation study of AutoManual on ALFWorld when testing with GPT-4-turbo.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在使用GPT-4-turbo进行测试时，AutoManual在ALFWorld上的消融研究。
- en: '| Online | Skill&Reflect Lib. | Case Prompt | Formulation | Avg. Error Steps
    ($\downarrow$) | Success Rate (%) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 在线 | 技能&反思库 | 案例提示 | 公式化 | 平均错误步骤（$\downarrow$） | 成功率（%） |'
- en: '|  |  |  |  | 2.3 | 77.6 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | 2.3 | 77.6 |'
- en: '|  | ✓ |  |  | 1.5 | 88.1 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ |  |  | 1.5 | 88.1 |'
- en: '|  | ✓ | ✓ | ✓ | 1.3 | 90.7 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | ✓ | ✓ | ✓ | 1.3 | 90.7 |'
- en: '| ✓ |  | ✓ | ✓ | 1.6 | 89.5 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| ✓ |  | ✓ | ✓ | 1.6 | 89.5 |'
- en: '| ✓ | ✓ |  | ✓ | 1.0 | 93.8 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ |  | ✓ | 1.0 | 93.8 |'
- en: '| ✓ | ✓ | ✓ |  | 0.5 | 96.5 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ |  | 0.5 | 96.5 |'
- en: '| ✓ | ✓ | ✓ | ✓ | 0.3 | 97.4 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | ✓ | ✓ | 0.3 | 97.4 |'
- en: 'Learning Curves. We show the success rate curves (testing with GPT-4-turbo
    or GPT-3.5-turbo) when gradually increasing the tasks of the building stage in
    Fig [4](https://arxiv.org/html/2405.16247v4#S4.F4 "Figure 4 ‣ 4.1 Main Results
    ‣ 4 Experiments ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via
    Interactive Environmental Learning"). In the left image, we share rules across
    all task types (Cross-task Type), as in AutoManual, or each task type builds a
    separate set of rules (Single-task Type) during the building stage. Fig [4](https://arxiv.org/html/2405.16247v4#S4.F4
    "Figure 4 ‣ 4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning") (a) demonstrates
    that sharing rules across task types can facilitate rule optimization. The rules
    for each task type deepen understanding of the environment, thereby boosting the
    planning of other tasks. In Fig [4](https://arxiv.org/html/2405.16247v4#S4.F4
    "Figure 4 ‣ 4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning") (b), we compare
    AutoManual and Planner+Lib. on 9 tasks with feedback in MiniWob++. We find that
    Planner+Lib. tends to get stuck with a lower success rate. In the face of highly
    diverse scenarios, Skill Library cannot express the rules behind the environment,
    thus falling into the Path Dependency problem.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线。我们展示了逐步增加构建阶段任务时的成功率曲线（使用GPT-4-turbo或GPT-3.5-turbo进行测试），如图[4](https://arxiv.org/html/2405.16247v4#S4.F4
    "图 4 ‣ 4.1 主要结果 ‣ 4 实验 ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")所示。在左侧图像中，我们共享所有任务类型的规则（跨任务类型），如同AutoManual，或者每种任务类型在构建阶段时构建一组单独的规则（单任务类型）。图[4](https://arxiv.org/html/2405.16247v4#S4.F4
    "图 4 ‣ 4.1 主要结果 ‣ 4 实验 ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")（a）表明，跨任务类型共享规则有助于规则优化。每种任务类型的规则加深了对环境的理解，从而促进了其他任务的规划。在图[4](https://arxiv.org/html/2405.16247v4#S4.F4
    "图 4 ‣ 4.1 主要结果 ‣ 4 实验 ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")（b）中，我们比较了AutoManual和Planner+Lib.在MiniWob++中9个任务的反馈情况。我们发现，Planner+Lib.倾向于在较低的成功率下陷入困境。在面对高度多样化的场景时，技能库无法表达环境背后的规则，从而陷入路径依赖问题。
- en: 4.2 Ablation Study
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: In this ablation study, we quantify the impact of each core component of the
    AutoManual framework on performance, specifically focusing on success rates and
    error reduction during task execution. Since we allowed the Planner to replan
    up to 3 times, each task could have up to 4 error steps.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项消融研究中，我们量化了AutoManual框架中每个核心组件对性能的影响，特别关注任务执行中的成功率和错误减少情况。由于我们允许规划器最多重新规划3次，每个任务最多可能有4个错误步骤。
- en: 'Online v.s. Offline Rule Management: We perform offline AutoManual by collecting
    all trajectories and then managing rules from them. As Tab [4](https://arxiv.org/html/2405.16247v4#S4.T4
    "Table 4 ‣ 4.1 Main Results ‣ 4 Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning") shows, without
    online rule management, the generated manual can only slightly improve planning
    (from 88.1% to 90.7%). This is because more mundane mistakes and fewer direct
    successes will occur in the trajectories (the distributional shift problem), and
    the rules cannot be verified by feedback from the environment.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在线与离线规则管理：我们通过收集所有轨迹并从中管理规则，执行离线AutoManual。如表[4](https://arxiv.org/html/2405.16247v4#S4.T4
    "表 4 ‣ 4.1 主要结果 ‣ 4 实验 ‣ AutoManual：通过交互式环境学习构建LLM代理的说明手册")所示，缺乏在线规则管理时，生成的手册只能略微改善规划（从88.1%提高到90.7%）。这是因为轨迹中会出现更多日常性的错误和更少的直接成功（分布转移问题），并且规则无法通过环境反馈进行验证。
- en: 'Skill&Reflection Libraries: Retrieving historical conclusions is essential
    for correct planning, as they record massive interacting details that can complement
    the rules. Without them, there will be more errors in the details, and the success
    rate drops from 97.4% to 89.5%. However, as discussed previously, using plain
    experiences without inducing rules will lead to Path Dependency.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 技能与反思库：检索历史结论对于正确规划至关重要，因为它们记录了大量的交互细节，可以补充规则。没有它们，细节中的错误会增多，成功率从97.4%下降至89.5%。然而，正如前文所述，单纯使用经验而不引导规则会导致路径依赖。
- en: 'Case-Conditional Prompts: This strategy further improves the rule management
    process by reducing the hallucination, as evidenced by an increase in success
    rate from 93.8% to 97.4%. These prompts ensure that the Builder updates rules
    reasonably and grounded.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 案例条件化提示：这一策略通过减少幻觉，进一步改善了规则管理过程，成功率从93.8%提高至97.4%。这些提示确保构建者合理且有依据地更新规则。
- en: 'Effect of Manual Formulation: The final formulation of rules into a comprehensive
    manual contributed to the success rate of 97.4% and decreased average error steps,
    demonstrating the effectiveness of presenting rule-based knowledge in an organized
    and accessible format. It not only aids the Planner in mastering multiple rules
    but is also friendly for human reading.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 手册制定效果：将规则最终制定为全面手册，有助于实现97.4%的成功率，并减少了平均错误步骤，证明了以有组织和易于理解的格式呈现基于规则的知识的有效性。它不仅帮助规划者掌握多个规则，而且对人类阅读者也十分友好。
- en: 5 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we introduce AutoManual, a framework significantly advancing
    LLM agents by enabling adaptability and continual learning through online rule
    optimization. Utilizing the structured rule system, AutoManual autonomously generates
    comprehensive manuals, achieving high success rates in benchmarks like ALFWorld
    and MiniWoB++. This approach reduces reliance on human-provided examples and expert
    interventions, illustrating a robust method for enhancing agent generalization
    and addressing the Path Dependency problem in diverse environments.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了AutoManual，一个显著推动大语言模型（LLM）代理发展的框架，通过在线规则优化实现了适应性和持续学习。利用结构化的规则系统，AutoManual自主生成全面的手册，并在ALFWorld和MiniWoB++等基准测试中取得了高成功率。这一方法减少了对人工示例和专家干预的依赖，展示了增强代理泛化能力并解决路径依赖问题的强大方法。
- en: 6 Acknowledgements
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 致谢
- en: 'Thanks to Dr. Zhe Zeng for her invaluable assistance with the OpenAI API and
    GPT Plus services. This work was supported in part by The National Nature Science
    Foundation of China (Grant No: 62273303), in part by Yongjiang Talent Introduction
    Programme (Grant No: 2022A-240-G).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢Zhe Zeng博士在OpenAI API和GPT Plus服务方面的宝贵帮助。此项工作部分得到了中国国家自然科学基金（资助号：62273303）的支持，部分得到了永江人才引进计划（资助号：2022A-240-G）的支持。
- en: References
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
    Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander
    Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
    Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan C.
    Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao Lu,
    Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao, Jarek
    Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan, Alexander
    Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and Mengyuan
    Yan. Do as i can, not as i say: Grounding language in robotic affordances. In
    Conference on Robot Learning, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
    Byron David, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alexander
    Herzog, Daniel Ho, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Eric Jang,
    Rosario Jauregui Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Jayant Joshi, Ryan
    C. Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, Sergey Levine, Yao
    Lu, Linda Luu, Carolina Parada, Peter Pastor, Jornell Quiambao, Kanishka Rao,
    Jarek Rettinghouse, Diego M Reyes, Pierre Sermanet, Nicolas Sievers, Clayton Tan,
    Alexander Toshev, Vincent Vanhoucke, F. Xia, Ted Xiao, Peng Xu, Sichun Xu, and
    Mengyuan Yan. 做我所做的，而不是我所说的：将语言与机器人能力结合。发表于《机器人学习会议》，2022年。'
- en: '[2] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran,
    Kyunghoon Bae, and Honglak Lee. Autoguide: Automated generation and selection
    of state-aware guidelines for large language model agents. ArXiv, abs/2403.08978,
    2024.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yao Fu, Dong-Ki Kim, Jaekyeom Kim, Sungryull Sohn, Lajanugen Logeswaran,
    Kyunghoon Bae, and Honglak Lee. Autoguide: 自动生成和选择状态感知指南用于大语言模型代理。ArXiv, abs/2403.08978,
    2024年。'
- en: '[3] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili
    Wang, Steven Ka Shing Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,
    and Chenglin Wu. Metagpt: Meta programming for multi-agent collaborative framework.
    ArXiv, abs/2308.00352, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Sirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng Cheng, Ceyao Zhang, Zili
    Wang, Steven Ka Shing Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao,
    和 Chenglin Wu. Metagpt：多代理协作框架的元编程。ArXiv, abs/2308.00352, 2023年。'
- en: '[4] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented
    planning with contextual memory for multimodal llm agents. ArXiv, abs/2402.03610,
    2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri
    Pranata, Akira Kinose, Koki Oguri, Felix Wick, 和 Yang You. Rap：具有上下文记忆的检索增强规划，用于多模态LLM代理。ArXiv,
    abs/2402.03610, 2024年。'
- en: '[5] Geunwoo Kim, Pierre Baldi, and Stephen Marcus McAleer. Language models
    can solve computer tasks. In Neural Information Processing Systems, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Geunwoo Kim, Pierre Baldi, 和 Stephen Marcus McAleer. 语言模型可以解决计算机任务。载于《神经信息处理系统》，2023年。'
- en: '[6] Sergey Levine, Aviral Kumar, G. Tucker, and Justin Fu. Offline reinforcement
    learning: Tutorial, review, and perspectives on open problems. ArXiv, abs/2005.01643,
    2020.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Sergey Levine, Aviral Kumar, G. Tucker, 和 Justin Fu. 离线强化学习：教程、综述与开放问题的展望。ArXiv,
    abs/2005.01643, 2020年。'
- en: '[7] Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter,
    Peter R. Florence, and Andy Zeng. Code as policies: Language model programs for
    embodied control. 2023 IEEE International Conference on Robotics and Automation
    (ICRA), pages 9493–9500, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Jacky Liang, Wenlong Huang, F. Xia, Peng Xu, Karol Hausman, Brian Ichter,
    Peter R. Florence, 和 Andy Zeng. 代码作为策略：面向具身控制的语言模型程序。2023年IEEE国际机器人与自动化会议（ICRA），页面9493–9500，2022年。'
- en: '[8] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang.
    Reinforcement learning on web interfaces using workflow-guided exploration. In
    International Conference on Learning Representations (ICLR), 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, 和 Percy Liang.
    基于工作流引导探索的网页接口强化学习。载于《国际学习表示会议（ICLR）》，2018年。'
- en: '[9] Bodhisattwa Prasad Majumder, Bhavana Dalvi, Peter Alexander Jansen, Oyvind
    Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, and Peter Clark. Clin:
    A continually learning language agent for rapid task adaptation and generalization.
    ArXiv, abs/2310.10134, 2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Bodhisattwa Prasad Majumder, Bhavana Dalvi, Peter Alexander Jansen, Oyvind
    Tafjord, Niket Tandon, Li Zhang, Chris Callison-Burch, 和 Peter Clark. Clin：一个不断学习的语言代理，用于快速任务适应和泛化。ArXiv,
    abs/2310.10134, 2023年。'
- en: '[10] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang,
    Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin
    Chess, and John Schulman. Webgpt: Browser-assisted question-answering with human
    feedback. ArXiv, abs/2112.09332, 2021.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Ouyang Long, Christina
    Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang,
    Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin
    Chess, 和 John Schulman. Webgpt：借助浏览器的人类反馈问答系统。ArXiv, abs/2112.09332, 2021年。'
- en: '[11] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] OpenAI. GPT-4技术报告。ArXiv, abs/2303.08774, 2023年。'
- en: '[12] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training
    language models to follow instructions with human feedback. In Neural Information
    Processing Systems, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright,
    Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
    Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell,
    Peter Welinder, Paul Francis Christiano, Jan Leike, 和 Ryan J. Lowe. 训练语言模型以遵循指令并通过人类反馈进行优化。载于《神经信息处理系统》，2022年。'
- en: '[13] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders,
    and Joseph E. Gonzalez. Memgpt: Towards llms as operating systems. ArXiv, abs/2310.08560,
    2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Charles Packer, Vivian Fang, Shishir G. Patil, Kevin Lin, Sarah Wooders,
    和 Joseph E. Gonzalez. Memgpt：迈向将LLM作为操作系统的目标。ArXiv, abs/2310.08560, 2023年。'
- en: '[14] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris,
    Percy Liang, and Michael S. Bernstein. Generative agents: Interactive simulacra
    of human behavior. Proceedings of the 36th Annual ACM Symposium on User Interface
    Software and Technology, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris,
    Percy Liang, 和 Michael S. Bernstein. 生成代理：人类行为的互动模拟。载于《第36届年度ACM用户界面软件与技术研讨会论文集》，2023年。'
- en: '[15] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, Maosong Sun, and Wei Liu. Communicative agents for software development.
    ArXiv, abs/2307.07924, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan
    Liu, Maosong Sun 和 Wei Liu。软件开发中的交流代理。ArXiv, abs/2307.07924, 2023。'
- en: '[16] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan,
    and Shunyu Yao. Reflexion: language agents with verbal reinforcement learning.
    In Neural Information Processing Systems, 2023.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan
    和 Shunyu Yao。Reflexion：具有言语强化学习的语言代理。神经信息处理系统，2023。'
- en: '[17] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler,
    and Matthew Hausknecht. Alfworld: Aligning text and embodied environments for
    interactive learning. In Proceedings of the International Conference on Learning
    Representations (ICLR), 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler
    和 Matthew Hausknecht。Alfworld：对齐文本与实体环境进行交互式学习。国际学习表示会议（ICLR）论文集，2021。'
- en: '[18] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu,
    Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt: Generating
    situated robot task plans using large language models. 2023 IEEE International
    Conference on Robotics and Automation (ICRA), pages 11523–11530, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu,
    Jonathan Tremblay, Dieter Fox, Jesse Thomason 和 Animesh Garg。Progprompt：利用大型语言模型生成情境化机器人任务计划。2023
    IEEE 国际机器人与自动化会议（ICRA），第11523–11530页，2022。'
- en: '[19] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, and Ryan McDonald. Step:
    Stacked llm policies for web actions. arXiv, abs/2310.03720, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Paloma Sodhi, S. R. K. Branavan, Yoav Artzi 和 Ryan McDonald。Step：用于网络操作的堆叠LLM策略。arXiv,
    abs/2310.03720, 2024。'
- en: '[20] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao,
    and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large
    language models. 2023 IEEE/CVF International Conference on Computer Vision (ICCV),
    pages 2986–2997, 2022.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Chan Hee Song, Jiaman Wu, Clay Washington, Brian M. Sadler, Wei-Lun Chao
    和 Yu Su。LLM-planner：少样本的实体代理规划与大型语言模型。2023 IEEE/CVF 国际计算机视觉会议（ICCV），第2986–2997页，2022。'
- en: '[21] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner:
    Adaptive planning from feedback with language models. In Neural Information Processing
    Systems, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai 和 Chao Zhang。Adaplanner：基于语言模型的反馈自适应规划。神经信息处理系统，2023。'
- en: '[22] Sai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker, and Ashish Kapoor.
    Chatgpt for robotics: Design principles and model abilities. IEEE Access, 12:55682–55696,
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Sai Vemprala, Rogerio Bonatti, Arthur Fender C. Bucker 和 Ashish Kapoor。Chatgpt
    在机器人领域的应用：设计原则与模型能力。IEEE Access, 12:55682–55696, 2023。'
- en: '[23] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi
    Ikeuchi. Chatgpt empowered long-step robot control in various environments: A
    case application. IEEE Access, 11:95060–95078, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu 和 Katsushi
    Ikeuchi。Chatgpt 增能的长步骤机器人控制在各种环境中的应用：案例研究。IEEE Access, 11:95060–95078, 2023。'
- en: '[24] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi (Jim) Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent
    with large language models. ArXiv, abs/2305.16291, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke
    Zhu, Linxi (Jim) Fan 和 Anima Anandkumar。Voyager：一个开放式的具有大型语言模型的实体代理。ArXiv, abs/2305.16291,
    2023。'
- en: '[25] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng,
    and Heng Ji. Executable code actions elicit better llm agents. ArXiv, abs/2402.01030,
    2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng
    和 Heng Ji。可执行代码操作引发更优秀的LLM代理。ArXiv, abs/2402.01030, 2024。'
- en: '[26] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang,
    Nebojsa Jojic, Eric P. Xing, and Zhiting Hu. Promptagent: Strategic planning with
    language models enables expert-level prompt optimization. ArXiv, abs/2310.16427,
    2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang,
    Nebojsa Jojic, Eric P. Xing 和 Zhiting Hu。Promptagent：通过语言模型进行战略规划，实现专家级的提示优化。ArXiv,
    abs/2310.16427, 2023。'
- en: '[27] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe,
    explain, plan and select: Interactive planning with large language models enables
    open-world multi-task agents. In Neural Information Processing Systems, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma 和 Yitao Liang。描述、解释、规划与选择：通过大型语言模型进行交互式规划，实现开放世界多任务代理。神经信息处理系统，2023。'
- en: '[28] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi,
    F. Xia, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning
    in large language models. In Neural Information Processing Systems, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 韦杰森、王雪智、代尔·舒尔曼、马尔滕·博斯马、胡怀欣·Chi、冯小娟、阮国伟和周丹尼。思维链提示激发大型语言模型中的推理。发表于2022年神经信息处理系统大会（Neural
    Information Processing Systems）。'
- en: '[29] Ronald J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8:229–256, 1992.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 罗纳德·J·威廉姆斯。连接主义强化学习中的简单统计梯度跟踪算法。机器学习，8:229-256，1992年。'
- en: '[30] Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming
    Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang,
    Limao Xiong, Qin Liu, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang
    Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan
    Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huan, and Tao Gui. The rise and potential
    of large language model based agents: A survey. ArXiv, abs/2309.07864, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 习志恒、陈文翔、郭鑫、何伟、丁易文、洪博阳、张铭、王俊哲、金森杰、周恩宇、郑睿、范晓冉、王晓、熊力茂、刘钦、周宇浩、王维然、姜昌浩、邹易成、刘向阳、尹张悦、窦诗涵、翁荣祥、程文森、张启、秦文娟、郑永燕、邱熙鹏、黄轩景和桂涛。基于大型语言模型的智能体崛起与潜力：一项调查。ArXiv，abs/2309.07864，2023年。'
- en: '[31] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny
    Zhou, and Xinyun Chen. Large language models as optimizers. ArXiv, abs/2309.03409,
    2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 杨成润、王雪智、卢亦峰、刘汉霄、阮国伟、周丹尼、陈心云。大型语言模型作为优化器。ArXiv，abs/2309.03409，2023年。'
- en: '[32] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. In Neural Information Processing Systems, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 姚顺宇、于奠、赵杰弗里、伊扎克·沙夫兰、托马斯·L·格里菲思、曹元和卡尔蒂克·纳拉希姆汉。思维树：利用大型语言模型进行深思熟虑的问题解决。发表于2023年神经信息处理系统大会（Neural
    Information Processing Systems）。'
- en: '[33] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
    and Yuan Cao. ReAct: Synergizing reasoning and acting in language models. In International
    Conference on Learning Representations (ICLR), 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 姚顺宇、赵杰弗里、于奠、杜楠、伊扎克·沙夫兰、卡尔蒂克·纳拉希姆汉和曹元。ReAct：在语言模型中协同推理与行动。发表于2023年国际学习表征会议（ICLR）。'
- en: '[34] Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou,
    Zeqi Tan, Peng Li, Yue Ting Zhuang, and Weiming Lu. Agent-pro: Learning to evolve
    via policy-level reflection and optimization. ArXiv, abs/2402.17574, 2024.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 张文琪、唐科、吴海、王梦娜、沈永亮、侯桂扬、谭泽齐、李鹏、庄悦廷和卢维铭。Agent-pro：通过策略级反思和优化学习进化。ArXiv，abs/2402.17574，2024年。'
- en: '[35] Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Y. Liu, and Gao Huang.
    Expel: Llm agents are experiential learners. In AAAI Conference on Artificial
    Intelligence (AAAI), 2024.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 赵安德鲁、黄丹尼尔、徐昆廷、林马修、刘逸、黄高。Expel：大型语言模型智能体是经验学习者。发表于2024年AAAI人工智能会议（AAAI）。'
- en: '[36] Andy Zhou, Kai Yan, Michal Shlapentokh-Rothman, Haohan Wang, and Yu-Xiong
    Wang. Language agent tree search unifies reasoning acting and planning in language
    models. ArXiv, abs/2310.04406, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 赵安迪、闫凯、米哈伊尔·什拉彭托赫-罗斯曼、王浩瀚和王宇雄。语言智能体树搜索统一了语言模型中的推理、行动和规划。ArXiv，abs/2310.04406，2023年。'
- en: '[37] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar,
    Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena:
    A realistic web environment for building autonomous agents. ArXiv, abs/2307.13854,
    2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 周书彦、徐弗兰克·F、朱浩、周旭辉、罗伯特·洛、阿比谢克·斯里达尔、程贤一、乔纳坦·比斯克、丹尼尔·弗里德、尤里·阿隆和格雷厄姆·纽比格。Webarena：一个真实的网络环境，用于构建自主智能体。ArXiv，abs/2307.13854，2023年。'
- en: '[38] Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu
    Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt
    engineers. ArXiv, abs/2211.01910, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 周永超、安德烈·伊奥恩·穆雷萨努、韩子文、基兰·帕斯特、希尔维乌·皮蒂斯、哈里斯·陈和吉米·巴。大型语言模型是人类级的提示工程师。ArXiv，abs/2211.01910，2022年。'
- en: '[39] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang,
    Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Y. Qiao, Zhaoxiang Zhang, and Jifeng
    Dai. Ghost in the minecraft: Generally capable agents for open-world environments
    via large language models with text-based knowledge and memory. ArXiv, abs/2305.17144,
    2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 朱熙洲、陈运涛、田昊、陶晨欣、苏伟杰、杨晨宇、黄高、李彬、陆乐伟、王晓刚、乔逸、张朝翔和戴季峰。Minecraft中的幽灵：通过具有文本知识和记忆的大型语言模型，为开放世界环境提供通用智能体。ArXiv，abs/2305.17144，2023年。'
- en: '[40] Zhaocheng Zhu, Yuan Xue, Xinyun Chen, Denny Zhou, Jian Tang, Dale Schuurmans,
    and Hanjun Dai. Large language models can learn rules. ArXiv, abs/2310.07064,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] 赵成朱、袁雪、辛云陈、丹尼·周、简·唐、大卫·舒尔曼和韩俊·戴。大型语言模型可以学习规则。ArXiv，abs/2310.07064，2023年。'
- en: Appendix
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 附录
- en: '[A Limitations](https://arxiv.org/html/2405.16247v4#A1 "In AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 局限性](https://arxiv.org/html/2405.16247v4#A1 "在 AutoManual 中：通过互动环境学习由 LLM
    代理构建说明手册")'
- en: '[B Broader Impacts](https://arxiv.org/html/2405.16247v4#A2 "In AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 更广泛的影响](https://arxiv.org/html/2405.16247v4#A2 "在 AutoManual 中：通过互动环境学习由
    LLM 代理构建说明手册")'
- en: '[C Difference with Prior Methods](https://arxiv.org/html/2405.16247v4#A3 "In
    AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental
    Learning")'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 与先前方法的差异](https://arxiv.org/html/2405.16247v4#A3 "在 AutoManual 中：通过互动环境学习由
    LLM 代理构建说明手册")'
- en: '[D Implementation Details](https://arxiv.org/html/2405.16247v4#A4 "In AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[D 实现细节](https://arxiv.org/html/2405.16247v4#A4 "在 AutoManual 中：通过互动环境学习由 LLM
    代理构建说明手册")'
- en: '[E Presentation of Generated Manuals](https://arxiv.org/html/2405.16247v4#A5
    "In AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning")'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 生成的手册展示](https://arxiv.org/html/2405.16247v4#A5 "在 AutoManual 中：通过互动环境学习由
    LLM 代理构建说明手册")'
- en: '[F More Experiments](https://arxiv.org/html/2405.16247v4#A6 "In AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 更多实验](https://arxiv.org/html/2405.16247v4#A6 "在 AutoManual 中：通过互动环境学习由 LLM
    代理构建说明手册")'
- en: '[G Case Study](https://arxiv.org/html/2405.16247v4#A7 "In AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G 案例研究](https://arxiv.org/html/2405.16247v4#A7 "在 AutoManual 中：通过互动环境学习由 LLM
    代理构建说明手册")'
- en: '[H Prompts and Examples](https://arxiv.org/html/2405.16247v4#A8 "In AutoManual:
    Constructing Instruction Manuals by LLM Agents via Interactive Environmental Learning")'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H 提示与示例](https://arxiv.org/html/2405.16247v4#A8 "在 AutoManual 中：通过互动环境学习由
    LLM 代理构建说明手册")'
- en: Appendix A Limitations
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 局限性
- en: Despite the significant contributions of the AutoManual framework, several limitations
    warrant further discussion. First, our method heavily relies on the capabilities
    of GPT-4-turbo to generate reliable rules, which may limit the framework’s applicability
    with less advanced language models.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 AutoManual 框架做出了重要贡献，但仍存在一些局限性，值得进一步讨论。首先，我们的方法严重依赖于 GPT-4-turbo 的能力来生成可靠的规则，这可能会限制该框架在较低版本语言模型中的适用性。
- en: Secondly, the current implementation places all rules directly within the context
    of LLM, which, while effective in smaller or moderately complex environments,
    may not scale well to larger, more dynamic settings. For such expansive environments,
    integrating the rule system with Retrieval-Augmented Generation (RAG) techniques,
    similar to the approach taken by AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    , could enhance performance by dynamically selecting relevant rules based on the
    context, thereby managing the cognitive load on the LLM more efficiently.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，目前的实现将所有规则直接放入 LLM 的上下文中，虽然在较小或中等复杂度的环境中有效，但在更大、更动态的环境中可能无法良好扩展。对于此类广阔的环境，可以将规则系统与检索增强生成（RAG）技术相结合，类似于
    AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2) 采取的方法，通过根据上下文动态选择相关规则，从而更高效地管理
    LLM 的认知负担，提升性能。
- en: Thirdly, for complex and challenging tasks, the agents of AutoManual are insufficient
    in exploring the environment, as they only attempt solutions based on current
    knowledge. To enhance agents’ exploration of unfamiliar environments, it may be
    necessary to endow the agent with curiosity [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    or combine it with tree search algorithms [ToT](https://arxiv.org/html/2405.16247v4#bib.bib32)
    .
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，对于复杂且具有挑战性的任务，AutoManual 的代理在探索环境时不足，因为它们仅根据当前知识尝试解决方案。为了增强代理对陌生环境的探索，可能需要赋予代理好奇心 [Voyager](https://arxiv.org/html/2405.16247v4#bib.bib24)
    或将其与树搜索算法 [ToT](https://arxiv.org/html/2405.16247v4#bib.bib32) 相结合。
- en: Lastly, there remains a challenge in ensuring that the Planner agent consistently
    adheres to the rules outlined in the manual. In practice, the Planner may sometimes
    ignore these rules (or have hallucinations about observations) and cannot generate
    the ideal solution code that can be applied to different situations. This indicates
    a need for additional mechanisms to enforce or verify rule compliance during operations.
    This issue underscores the potential necessity for developing more sophisticated
    methods to ensure rule adherence or to introduce more robust validation steps
    within the planning process.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，仍然存在一个挑战，即确保 Planner 代理始终遵守手册中概述的规则。在实践中，Planner 有时可能忽视这些规则（或对观察产生幻觉），无法生成可以应用于不同情境的理想解决方案代码。这表明，在操作过程中需要额外的机制来强制执行或验证规则的遵守。这个问题凸显了开发更复杂的方法来确保规则遵守或在规划过程中引入更强大的验证步骤的潜在必要性。
- en: Appendix B Broader Impacts
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 更广泛的影响
- en: The AutoManual framework, leveraging LLM agents, presents positive and negative
    impacts on safety. On the positive side, by autonomously generating reliable manuals,
    our method enhances the predictability and reliability of LLM behaviors in dynamic
    environments, potentially reducing errors and increasing operational safety. However,
    relying on LLMs also introduces risks of unpredictable behaviors when agents encounter
    unanticipated scenarios or when rule adherence is not fully ensured.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: AutoManual 框架利用 LLM 代理，展示了对安全性的正面和负面影响。从正面来看，通过自动生成可靠的手册，我们的方法增强了 LLM 在动态环境中的可预测性和可靠性，可能减少错误并提高操作安全性。然而，依赖
    LLM 也带来了风险，尤其是在代理遇到意外情况或规则遵循没有完全保证时，可能会出现不可预测的行为。
- en: Furthermore, the manuals generated by our method can be invaluable tools for
    human workers. They encapsulate a distilled form of interaction-based learning
    that can aid in training, provide decision support, and improve task efficiency
    in various domains. This can not only enhance productivity but also ensure that
    human workers are better informed and prepared to manage the complex systems with
    which they interact.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们方法生成的手册对于人类工人来说是宝贵的工具。它们凝聚了基于互动的学习形式，可以帮助培训、提供决策支持，并提高各个领域的任务效率。这不仅能提高生产力，还能确保人类工人能够更好地了解并准备好管理他们所互动的复杂系统。
- en: Lastly, AutoManual’s ability to generate structured, context-aware manuals from
    interactive experiences suggests a promising avenue for constructing comprehensive
    knowledge bases for AI. These manuals can contribute to a global knowledge base
    shared by LLMs of different sizes, promoting broader AI research and development.
    It offers a method to systematically organize and retrieve complex interaction
    data in a way that is accessible and useful for both machines and humans.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，AutoManual 从互动经验中生成结构化、上下文感知的手册，显示了为 AI 构建全面知识库的前景。这些手册可以贡献于一个由不同规模的 LLM
    共享的全球知识库，推动更广泛的 AI 研究与开发。它提供了一种方法，可以系统地组织和检索复杂的互动数据，使其对机器和人类都可访问且有用。
- en: Appendix C Difference with Prior Methods
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 与先前方法的差异
- en: 'We compare AutoManual with prior methods that extract rules from experiences,
    i.e., ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) and AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)
    , and discuss all differences here:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 AutoManual 与先前的基于经验提取规则的方法进行比较，即 ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35)
    和 AutoGuide [AutoGuide](https://arxiv.org/html/2405.16247v4#bib.bib2)，并在这里讨论所有差异：
- en: 1) Interactive Form. ExpeL and AutoGuide, following Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)
    , each time output the thought and next-step action to interact with the environment.
    Our Planner agent interacts using free-form code. As evidenced by previous works [CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)
    ; [CodeAct](https://arxiv.org/html/2405.16247v4#bib.bib25) , using code as the
    plan is more efficient because the code will automatically perform the in-plan
    actions and require LLM responses far fewer times. Additionally, planning with
    code can enjoy the powerful programming capability of GPT. More importantly, compared
    with action sequences, code is easier to generalize to similar scenarios, facilitating
    the management.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 交互形式。ExpeL和AutoGuide遵循Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16)，每次输出思考和下一步行动，以与环境进行交互。我们的Planner代理使用自由形式的代码进行交互。正如之前的研究所证明的[CodeAP](https://arxiv.org/html/2405.16247v4#bib.bib7)；[CodeAct](https://arxiv.org/html/2405.16247v4#bib.bib25)，使用代码作为计划更为高效，因为代码会自动执行计划中的行动，且需要LLM响应的次数大大减少。此外，使用代码进行规划可以享受GPT强大的编程能力。更重要的是，相比于行动序列，代码更容易泛化到类似场景，从而促进管理。
- en: 2) Online v.s. Offline. ExpeL and AutoGuide extract rules from offline experiences
    of Reflexion agent. We update rules online by alternating rule practice and rule
    management. This online rule management can verify the reliability and applicability
    of the rules in a timely manner, forbidding rules to be armchair general. Additionally,
    online-update rules can help planners continuously improve their trajectories,
    making it easier for higher-quality success processes to emerge.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 在线与离线。ExpeL和AutoGuide从Reflexion代理的离线经验中提取规则。我们通过交替进行规则实践和规则管理来在线更新规则。这种在线规则管理可以及时验证规则的可靠性和适用性，避免规则成为空谈。此外，在线更新的规则有助于规划者不断改进他们的轨迹，使得更高质量的成功过程得以出现。
- en: 3) Collaboration Between Agents. Building rules online also enables collaboration
    between the Planner and Builder agents. In our AutoManual, the Planner is prompted
    to describe the rules considered and analyze special phenomena, which ease the
    management of the Builder. In contrast, the builder in ExpeL and AutoGuide can
    only receive action sequences from the Planner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 代理之间的协作。在线构建规则还能够促进Planner和Builder代理之间的协作。在我们的AutoManual中，Planner会被提示描述考虑的规则并分析特殊现象，从而简化了Builder的管理。相比之下，ExpeL和AutoGuide中的Builder只能接收来自Planner的行动序列。
- en: '4) Rule System. In ExpeL and AutoGuide, each rule has only two attributes:
    the content and score. In our rule system, each rule has four attributes: "Type",
    "Content", "Example", and "Logs". These attributes provide a comprehensive representation
    of rules and facilitate the management and usage of rules. Moreover, we allow
    rules to build on top of other rules.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 规则系统。在ExpeL和AutoGuide中，每个规则只有两个属性：内容和分数。在我们的规则系统中，每个规则有四个属性：“类型”、“内容”、“示例”和“日志”。这些属性提供了规则的全面表示，并促进了规则的管理和使用。此外，我们允许规则基于其他规则进行构建。
- en: 5) Handling excessive rules. ExpeL and AutoGuide utilize the rule scores to
    delete the low-score rules. However, we found that rule scores are unreliable
    in the experiments because the Builder tends to give overconfident scores to all
    rules. Instead, our AutoManual employs a Consolidator agent to merge and delete
    redundant rules.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 处理过多的规则。ExpeL和AutoGuide利用规则分数删除低分规则。然而，我们在实验中发现，规则分数不可靠，因为Builder往往会给所有规则过度自信的分数。相反，我们的AutoManual采用了Consolidator代理来合并并删除冗余规则。
- en: Appendix D Implementation Details
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 实现细节
- en: In the building stage of all experiments, the maximum number of rules was set
    to 12 to balance the context length and rule diversity. We use OpenAI’s Assistant
    API for all agents to save their history and prevent duplicate inputs. We set
    all LLMs’ temperatures to 0 and maximum context length to 16000\. Reflexion agents
    are allowed to try at most $3$ trials for each task. For AdaPlanner and AutoManual,
    we allow the Planner agent to replan at most $3$ times on ALFWorld and $6$ times
    on MiniWob++ in response to the environmental feedback. In the Building stage
    on ALFWorld, we use 6 tasks for each task type, a total of 36 tasks, by default.
    We shuffle all tasks, and when tasks in a task type succeed three times in a row
    during building, we consider that this task type has been solved and will no longer
    run it. The API call cost for building and formulating stages is about $14 in
    total.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有实验的构建阶段，最大规则数量设置为12，以平衡上下文长度和规则多样性。我们使用 OpenAI 的 Assistant API 为所有代理保存历史记录并防止重复输入。我们将所有
    LLMs 的温度设置为 0，最大上下文长度设置为16000。Reflexion 代理允许每个任务最多尝试$3$次。在 ALFWorld 的构建阶段，我们为每种任务类型使用6个任务，总共36个任务，默认情况下进行。我们会打乱所有任务，并且当某任务类型在构建过程中连续三次成功时，我们认为该任务类型已经解决，不再执行。构建和制定阶段的
    API 调用费用大约为$14$。
- en: 'We made a slight modification to the text format of ALFWorld to make it more
    suitable for code: for each object, "object id" was changed to "object_id", and
    all preceding articles were removed. The maximum action step for a task is set
    to $50$.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 ALFWorld 的文本格式进行了轻微修改，使其更适合代码使用：对于每个对象，“object id”被更改为“object_id”，并且删除了所有前置的冠词。任务的最大动作步数设置为$50$。
- en: For the 9 tasks with feedback on Miniwob++, we find that "email-inbox-nl-turk"
    and "email-inbox-forward-nl-turk" are repetitions of "email-inbox" and "email-inbox-forward-nl".
    Therefore, we only used 7 task types in the building stage, while in the testing
    stage, all 9 types were evaluated. In the building stage, we run a total of 42
    tasks (6 tasks for each type).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Miniwob++ 上的9个有反馈的任务，我们发现“email-inbox-nl-turk”和“email-inbox-forward-nl-turk”是“email-inbox”和“email-inbox-forward-nl”的重复。因此，我们在构建阶段只使用了7个任务类型，而在测试阶段评估了所有9个任务类型。在构建阶段，我们共执行了42个任务（每个类型6个任务）。
- en: 'For all 53 tasks on Miniwob++, since running the building directly will lead
    to a large number of rules, and tasks without feedback have low variability, we
    adopt a two-step building strategy: we first run our building stage on 9 task
    types with feedback and then only update the skill and reflection libraries on
    44 task types without feedback. The 4 examples for experiments on all task types
    are from the following task types: ‘click-menu’, ‘enter-date’, ‘social-media-some’.
    We chose these tasks mainly because they are difficult for GPT-4-turbo to try
    out due to the lack of HTML feedback. For example, the HTML in ‘enter-date’ will
    not display the cursor but requires the cursor at the correct position to enter.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Miniwob++ 上的所有53个任务，由于直接运行构建会导致生成大量规则，而没有反馈的任务变异性较低，因此我们采用了两步构建策略：首先，我们在9个有反馈的任务类型上运行构建阶段，然后仅在44个没有反馈的任务类型上更新技能和反射库。所有任务类型的实验示例来自以下任务类型：‘click-menu’，‘enter-date’，‘social-media-some’。我们选择这些任务，主要是因为它们由于缺乏
    HTML 反馈，难以让 GPT-4-turbo 尝试。例如，‘enter-date’中的 HTML 不会显示光标，但需要将光标放在正确位置才能输入。
- en: 'Environmental feedback on executing code in MiniWob++ is as follows: Whenever
    one of the actions is executed, we will log whether the action was executed successfully
    or failed. Finally, the results of these actions are concatenated together, and
    used as feedback of the code along with the HTML text.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MiniWob++ 中执行代码时的环境反馈如下：每当执行某个动作时，我们会记录该动作是否成功执行或失败。最后，这些动作的结果会被连接在一起，并与 HTML
    文本一起作为代码的反馈。
- en: For AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) and
    RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) , we use their official
    code on GitHub to implement their methods. We fix their bugs on matching text
    patterns but still find their performance is much lower than they reported. This
    may be because they have not released the code used in their papers, or their
    methods are greatly affected by the GPT version (newer versions of GPT will be
    more creative).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) 和
    RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5)，我们使用它们在 GitHub 上的官方代码实现它们的方法。我们修复了匹配文本模式的错误，但仍然发现它们的表现远低于它们报告的结果。这可能是因为它们没有发布论文中使用的代码，或者它们的方法受到
    GPT 版本的巨大影响（更新的 GPT 版本会更加具有创造性）。
- en: 'Table 5: The GPT models used for each method in our implementation when using
    GPT-3.5-turbo or GPT-4-turbo as test-time LLM. Here GPT-3.5-turbo denotes gpt-3.5-turbo-1106
    and GPT-4-turbo denotes gpt-4-1106-preview.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 我们在使用 GPT-3.5-turbo 或 GPT-4-turbo 作为测试时 LLM 时，各种方法中使用的 GPT 模型。在这里，GPT-3.5-turbo
    表示 gpt-3.5-turbo-1106，GPT-4-turbo 表示 gpt-4-1106-preview。'
- en: '| Method | Test-Time LLM |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 测试时 LLM |'
- en: '| GPT-3.5-turbo | GPT-4-turbo |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | GPT-4-turbo |'
- en: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | Actor: GPT-3.5-turbo
    | Actor: GPT-4-turbo |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [ReAct](https://arxiv.org/html/2405.16247v4#bib.bib33) | Actor: GPT-3.5-turbo
    | Actor: GPT-4-turbo |'
- en: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | Actor:
    GPT-3.5-turbo | Actor: GPT-4-turbo |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Reflexion [Reflexion](https://arxiv.org/html/2405.16247v4#bib.bib16) | Actor:
    GPT-3.5-turbo | Actor: GPT-4-turbo |'
- en: '| Self-reflection: GPT-3.5-turbo | Self-reflection: GPT-4-turbo |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 自我反思: GPT-3.5-turbo | 自我反思: GPT-4-turbo |'
- en: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | Offline Trajectory:
    | Offline Trajectory: |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| ExpeL [ExpeL](https://arxiv.org/html/2405.16247v4#bib.bib35) | 离线轨迹: | 离线轨迹:
    |'
- en: '|        Actor: GPT-3.5-turbo |        Actor: GPT-4-turbo |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|        Actor: GPT-3.5-turbo |        Actor: GPT-4-turbo |'
- en: '|        Self-reflection: GPT-3.5-turbo |        Self-reflection: GPT-4-turbo
    |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|        自我反思: GPT-3.5-turbo |        自我反思: GPT-4-turbo |'
- en: '| Insight Extraction: | Insight Extraction: |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 见解提取: | 见解提取: |'
- en: '|        Builder: GPT-4-turbo |        Builder: GPT-4-turbo |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|        Builder: GPT-4-turbo |        Builder: GPT-4-turbo |'
- en: '| Task Inference: | Task Inference: |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 任务推理: | 任务推理: |'
- en: '|        Actor: GPT-3.5-turbo |        Actor: GPT-4-turbo |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '|        Actor: GPT-3.5-turbo |        Actor: GPT-4-turbo |'
- en: '| RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) | Task/State/Agent
    grounding: | Task/State/Agent grounding: |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) | 任务/状态/代理基础: | 任务/状态/代理基础:
    |'
- en: '|        GPT-3.5-turbo |        GPT-4-turbo |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|        GPT-3.5-turbo |        GPT-4-turbo |'
- en: '| Adaplanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    Planner/Refiner: GPT-3.5-turbo | Planner/Refiner: GPT-4-turbo |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| Adaplanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21) |
    Planner/Refiner: GPT-3.5-turbo | Planner/Refiner: GPT-4-turbo |'
- en: '|         ask_LLM(): GPT-3.5-turbo-instruct |         ask_LLM(): GPT-3.5-turbo-instruct
    |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|         ask_LLM(): GPT-3.5-turbo-instruct |         ask_LLM(): GPT-3.5-turbo-instruct
    |'
- en: '| AutoManual | Building Stage: | Building Stage: |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 构建阶段: | 构建阶段: |'
- en: '|         Planner: GPT-4-turbo |        Planner: GPT-4-turbo |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|         Planner: GPT-4-turbo |        Planner: GPT-4-turbo |'
- en: '|         Builder: GPT-4-turbo |        Builder: GPT-4-turbo |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|         Builder: GPT-4-turbo |        Builder: GPT-4-turbo |'
- en: '| Formulator: GPT-4-turbo | Formulator: GPT-4-turbo |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Formulator: GPT-4-turbo | Formulator: GPT-4-turbo |'
- en: '| Testing Stage: | Testing Stage: |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 测试阶段: | 测试阶段: |'
- en: '|        Planner: GPT-3.5-turbo |        Planner: GPT-4-turbo |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|        Planner: GPT-3.5-turbo |        Planner: GPT-4-turbo |'
- en: Appendix E Presentation of Generated Manuals
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 生成手册的展示
- en: 'We present the generated manual³³3The manual doesn’t include Validation Logs
    of the rules, which are not visible for the Planner. on ALFWorld by AutoManual
    in Fig [5](https://arxiv.org/html/2405.16247v4#A5.F5 "Figure 5 ‣ Appendix E Presentation
    of Generated Manuals ‣ AutoManual: Constructing Instruction Manuals by LLM Agents
    via Interactive Environmental Learning"). As shown in the manual, the Formulator
    agent categorizes the rules into four categories: “Navigation and Search”, “Object
    Interaction and Location Management”, “Task-Specific Processes”, and “Correctness
    and Validation” with clear introductions. We find our agents have found and recorded
    lots of important rules that directly impact task completion. For instance, “the
    agent should include all possible locations in its search” in rule_0, “The agent
    can only hold one object at a time” in rule_3, and “When multiple items of the
    same type are present at a location, the agent may have to choose one to interact
    with or examine.” in rule_5\. For tasks that can be solved by a fixed strategy,
    such as “Cool”, “Heat”, “Clean” and “Examine”, AutoManual provides clear Success
    Process type rules. For the complex task, “Put Two”, AutoManual conducts classified
    discussions in its Success Process rule.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了AutoManual生成的ALFWorld手册³³3该手册未包括规则的验证日志，这些日志Planner无法看到。图[5](https://arxiv.org/html/2405.16247v4#A5.F5
    "Figure 5 ‣ Appendix E Presentation of Generated Manuals ‣ AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning")中的手册展示了，Formulator代理将规则分为四类：“导航与搜索”、“物体交互与位置管理”、“任务特定过程”和“正确性与验证”，并有明确的介绍。我们发现我们的代理已经发现并记录了许多直接影响任务完成的重要规则。例如，在rule_0中，“代理应将所有可能的位置都包含在搜索中”；在rule_3中，“代理每次只能持有一个物体”；在rule_5中，“当同一类型的多个物体出现在某个位置时，代理可能需要选择一个与之交互或检查。”对于可以通过固定策略解决的任务，例如“冷却”、“加热”、“清洁”和“检查”，AutoManual提供了明确的成功过程类型规则。对于复杂任务“放置两个”，AutoManual在其成功过程规则中进行了分类讨论。'
- en: These demonstrate that AutoManual resolves the Path Dependency problem of skills
    by digging deeper into mechanisms, updating and incorporating success processes,
    and annotating important details.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例表明，AutoManual通过深入挖掘机制，更新并融入成功过程，同时注释重要细节，解决了技能的路径依赖问题。
- en: 'Moreover, we prompt the Builder to break down large phenomena into specific
    rules and derive deeper rules from them later. In the initial rules [9](https://arxiv.org/html/2405.16247v4#LST9
    "Listing 9 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and
    Examples ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning"), we give a simple example of how to build rules upon
    rules. In the manual [5](https://arxiv.org/html/2405.16247v4#A5.F5 "Figure 5 ‣
    Appendix E Presentation of Generated Manuals ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning"), we find that the
    Builder can correctly utilize rules recording basic mechanisms to build more complex
    rules. For example, rule_4 uses rule_3 to induce a solution: “If all objects are
    found at the same location, handle them sequentially according to rule_3.”'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们提示Builder将大现象分解为具体规则，并随后从中推导出更深层次的规则。在初始规则[9](https://arxiv.org/html/2405.16247v4#LST9
    "Listing 9 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and
    Examples ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning")中，我们给出了如何基于规则构建规则的简单示例。在手册[5](https://arxiv.org/html/2405.16247v4#A5.F5
    "Figure 5 ‣ Appendix E Presentation of Generated Manuals ‣ AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning")中，我们发现Builder可以正确利用记录基本机制的规则来构建更复杂的规则。例如，rule_4使用rule_3来推导一个解决方案：“如果所有物体都在同一位置，按照rule_3顺序处理它们。”'
- en: However, we find the generated manual is still not perfect. There are some unnecessary
    duplicates between rules, such as rule_7 and rule_8, which are both Success Process
    type rules for "Examine" tasks, but they are divided into two rules.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们发现生成的手册仍然不完美。规则之间存在一些不必要的重复，例如rule_7和rule_8，都是针对“检查”任务的成功过程类型规则，但它们被分成了两个规则。
- en: 'Table 6: The success rate (%) of different methods on 9 task types with feedback
    of MiniWob++.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在MiniWob++的9种任务类型反馈下，不同方法的成功率（%）。
- en: '| Task Type | RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) | AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    | Planner+Lib. | AutoManual |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 任务类型 | RCI [RCI](https://arxiv.org/html/2405.16247v4#bib.bib5) | AdaPlanner [AdaPlanner](https://arxiv.org/html/2405.16247v4#bib.bib21)
    | Planner+Lib. | AutoManual |'
- en: '| Examples | 22 | 13 | 1 | 1 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 示例 | 22 | 13 | 1 | 1 |'
- en: '|  | Test with GPT-3.5-turbo |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | 使用GPT-3.5-turbo进行测试 |'
- en: '| search-engine | 33.3 | 100.0 | 66.7 | 66.7 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 搜索引擎 | 33.3 | 100.0 | 66.7 | 66.7 |'
- en: '| tic-tac-toe | 22.2 | 27.8 | 16.7 | 33.3 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| tic-tac-toe | 22.2 | 27.8 | 16.7 | 33.3 |'
- en: '| terminal | 55.6 | 100.0 | 100.0 | 100.0 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| terminal | 55.6 | 100.0 | 100.0 | 100.0 |'
- en: '| login-user-popup | 33.3 | 33.3 | 33.3 | 66.7 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| login-user-popup | 33.3 | 33.3 | 33.3 | 66.7 |'
- en: '| guess-number | 11.1 | 22.2 | 11.1 | 94.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| guess-number | 11.1 | 22.2 | 11.1 | 94.4 |'
- en: '| email-inbox | 77.8 | 88.9 | 83.3 | 100.0 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox | 77.8 | 88.9 | 83.3 | 100.0 |'
- en: '| email-inbox-nl-turk | 61.1 | 94.4 | 77.8 | 100.0 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-nl-turk | 61.1 | 94.4 | 77.8 | 100.0 |'
- en: '| email-inbox-forward-nl | 61.1 | 83.3 | 94.4 | 100.0 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-forward-nl | 61.1 | 83.3 | 94.4 | 100.0 |'
- en: '| email-inbox-forward-nl-turk | 55.6 | 94.4 | 88.9 | 77.8 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-forward-nl-turk | 55.6 | 94.4 | 88.9 | 77.8 |'
- en: '| Average | 45.6 | 71.6 | 63.6 | 82.2 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 45.6 | 71.6 | 63.6 | 82.2 |'
- en: '|  | Test with GPT-4-turbo |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  | 使用GPT-4-turbo进行测试 |'
- en: '| search-engine | 44.4 | 100.0 | 100.0 | 100.0 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| search-engine | 44.4 | 100.0 | 100.0 | 100.0 |'
- en: '| tic-tac-toe | 33.3 | 22.2 | 22.2 | 66.7 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| tic-tac-toe | 33.3 | 22.2 | 22.2 | 66.7 |'
- en: '| terminal | 88.9 | 100.0 | 100.0 | 100.0 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| terminal | 88.9 | 100.0 | 100.0 | 100.0 |'
- en: '| login-user-popup | 38.9 | 33.3 | 72.2 | 100.0 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| login-user-popup | 38.9 | 33.3 | 72.2 | 100.0 |'
- en: '| guess-number | 22.2 | 44.1 | 33.3 | 88.9 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| guess-number | 22.2 | 44.1 | 33.3 | 88.9 |'
- en: '| email-inbox | 77.8 | 100.0 | 100.0 | 94.4 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox | 77.8 | 100.0 | 100.0 | 94.4 |'
- en: '| email-inbox-nl-turk | 72.2 | 100.0 | 100.0 | 100.0 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-nl-turk | 72.2 | 100.0 | 100.0 | 100.0 |'
- en: '| email-inbox-forward-nl | 88.9 | 100.0 | 100.0 | 100.0 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-forward-nl | 88.9 | 100.0 | 100.0 | 100.0 |'
- en: '| email-inbox-forward-nl-turk | 77.8 | 66.7 | 94.4 | 100.0 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| email-inbox-forward-nl-turk | 77.8 | 66.7 | 94.4 | 100.0 |'
- en: '| Average | 60.4 | 74.1 | 80.2 | 94.5 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 60.4 | 74.1 | 80.2 | 94.5 |'
- en: '![Refer to caption](img/855c00ed5817b1877d8b8323abfe791f.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/855c00ed5817b1877d8b8323abfe791f.png)'
- en: 'Figure 5: The Generated Manual for ALFWorld: Part 1.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: ALFWorld的生成手册：第1部分。'
- en: '![Refer to caption](img/9536a3f0540ba38f9f8e0d9ec9227e84.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/9536a3f0540ba38f9f8e0d9ec9227e84.png)'
- en: 'Figure 6: The Generated Manual for ALFWorld: Part 2.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: ALFWorld的生成手册：第2部分。'
- en: Appendix F More Experiments
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 更多实验
- en: 'Table 7: Ablation study of Rule System on ALFWorld.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: ALFWorld上规则系统的消融研究。'
- en: '| Method | Success Rate (%) Testing with |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 成功率（%） 使用测试 |'
- en: '| GPT-3.5-turbo | GPT-4-turbo |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | GPT-4-turbo |'
- en: '| AutoManual | 86.2 | 97.4 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 86.2 | 97.4 |'
- en: '| AutoManual without “Type” | 74.6 | 91.5 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 没有“类型”的AutoManual | 74.6 | 91.5 |'
- en: '| AutoManual without “Example” | 76.8 | 92.7 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 没有“示例”的AutoManual | 76.8 | 92.7 |'
- en: '| AutoManual without “Validation Logs” | 85.4 | 97.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 没有“验证日志”的AutoManual | 85.4 | 97.0 |'
- en: '| AutoManual without “Useful Helper Method” | 79.6 | 94.3 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 没有“有用辅助方法”的AutoManual | 79.6 | 94.3 |'
- en: '| AutoManual without Cooperation between Agents | 78.8 | 93.8 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 没有代理之间合作的AutoManual | 78.8 | 93.8 |'
- en: '| Case-Conditioned Prompts without Classification | 83.2 | 95.6 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 无分类的条件化提示 | 83.2 | 95.6 |'
- en: 'Table 8: Sensitivity Analysis of Examples and Initial Rules on 9 task types
    with feedback of MiniWob++. AutoManual uses a human example of search-engine or
    enter-text task.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 对 MiniWob++ 反馈的9种任务类型示例和初始规则的敏感度分析。AutoManual 使用搜索引擎或输入文本任务的人类示例。'
- en: '| Method | Success Rate (%) Testing with |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 成功率（%） 使用测试 |'
- en: '| GPT-3.5-turbo | GPT-4-turbo |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-turbo | GPT-4-turbo |'
- en: '| AutoManual | 82.2 | 94.5 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| AutoManual | 82.2 | 94.5 |'
- en: '| AutoManual with enter-text Example | 78.3 | 92.8 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 带输入文本示例的AutoManual | 78.3 | 92.8 |'
- en: F.1 Ablation Study of Rule System
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 规则系统的消融研究
- en: 'The well-structured rule system is essential for the rule management and usage.
    We perform the ablation study of our rule system in Tab [7](https://arxiv.org/html/2405.16247v4#A6.T7
    "Table 7 ‣ Appendix F More Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning").'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '良好的规则系统结构对于规则管理和使用至关重要。我们在表[7](https://arxiv.org/html/2405.16247v4#A6.T7 "Table
    7 ‣ Appendix F More Experiments ‣ AutoManual: Constructing Instruction Manuals
    by LLM Agents via Interactive Environmental Learning")中进行了一项规则系统的消融研究。'
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AutoManual without “Type”: we remove the “Type” attribute of each rule in the
    rule system and the instruction for the Builder to manage various types of rules.
    As shown in Tab [7](https://arxiv.org/html/2405.16247v4#A6.T7 "Table 7 ‣ Appendix
    F More Experiments ‣ AutoManual: Constructing Instruction Manuals by LLM Agents
    via Interactive Environmental Learning"), without the “Type” attribute, the performance
    drops significantly, from 86.2% to 74.6% and 97.4% to 91.5%, as the Builder cannot
    manage specific types of rules and loses specific instruction on each rule.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '没有“类型”的AutoManual：我们移除了规则系统中每条规则的“类型”属性，以及构建器管理各种类型规则的指令。如表[7](https://arxiv.org/html/2405.16247v4#A6.T7
    "Table 7 ‣ Appendix F More Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning")所示，在没有“类型”属性的情况下，性能显著下降，从86.2%降至74.6%，从97.4%降至91.5%，因为构建器无法管理特定类型的规则，且丧失了对每条规则的具体指令。'
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AutoManual without “Example”: we remove the “Example” attribute of each rule
    in the rule system. The performance also drops by a large margin, as the Builder
    cannot specify necessary details in rules, and rules with lower understandability
    can sometimes be misleading to the Planner.'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有“示例”的AutoManual：我们移除了规则系统中每条规则的“示例”属性。性能也大幅下降，因为构建器无法在规则中指定必要的细节，且理解度较低的规则有时会误导规划器。
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: AutoManual without “Validation Logs” has little impact on performance, but “Validation
    Logs” of rules are very useful during debugging.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有“验证日志”的AutoManual对性能影响不大，但规则的“验证日志”在调试过程中非常有用。
- en: 'We also conducted some detailed ablation studies of our AutoManual in Tab [7](https://arxiv.org/html/2405.16247v4#A6.T7
    "Table 7 ‣ Appendix F More Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning").'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在表[7](https://arxiv.org/html/2405.16247v4#A6.T7 "Table 7 ‣ Appendix F More
    Experiments ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning")中进行了AutoManual的一些详细消融研究。'
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Helper methods defined by humans and the Planner can serve as the solution
    for a subgoal of a task that can be used in multiple tasks. AutoManual without
    “Useful Helper Method”: we remove the “Useful Helper Method” in the “Type” attribute,
    the human example, and initial rules, and we no longer encourage the Planner to
    write helper functions when writing code. The performance drops from 86.2% to
    79.6% and 97.4% to 94.3%, demonstrating that extracting reusable helper methods
    can facilitate the programming.'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人类和规划器定义的辅助方法可以作为任务子目标的解决方案，适用于多个任务。没有“有用辅助方法”的AutoManual：我们移除了“类型”属性中的“有用辅助方法”、人类示例和初始规则，并且不再鼓励规划器在编写代码时编写辅助函数。性能从86.2%下降到79.6%，从97.4%下降到94.3%，这表明提取可重用的辅助方法可以促进编程。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'AutoManual without Cooperation between Agents: we remove the Planner’s thoughts
    in the trajectory when providing it to the Builder and no longer require the Planner
    to output Relevant Rules. The performance decreases significantly, indicating
    the thoughts and conclusions of the Planner are helpful for the Builder to manage
    rules.'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 没有代理间合作的AutoManual：我们移除了在将规划器的思维传递给构建器时的轨迹中的规划器思维，并不再要求规划器输出相关规则。性能显著下降，表明规划器的思维和结论对构建器管理规则非常有帮助。
- en: •
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Case-Conditioned Prompts without Classification: we remove the requirement
    to classify the error reason to “Imperfect Rules” or “Imperfect Agents” and only
    use the prompts for “Imperfect Rules” during case-conditioned prompting. As demonstrated
    in Tab [7](https://arxiv.org/html/2405.16247v4#A6.T7 "Table 7 ‣ Appendix F More
    Experiments ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning"), the results are inferior to using all 5 cases because
    the analysis and classification of error reason can boost the Builder to manage
    rules accurately.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '没有分类的案例条件提示：我们移除了将错误原因分类为“规则不完善”或“代理不完善”的要求，并且在案例条件提示中仅使用“规则不完善”的提示。如表[7](https://arxiv.org/html/2405.16247v4#A6.T7
    "Table 7 ‣ Appendix F More Experiments ‣ AutoManual: Constructing Instruction
    Manuals by LLM Agents via Interactive Environmental Learning")所示，结果不如使用所有5种情况，因为错误原因的分析和分类能够帮助构建器更准确地管理规则。'
- en: F.2 Sensitivity Analysis of Examples and Initial Rules
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 示例和初始规则的敏感性分析
- en: 'We analyze the sensitivity of our rule optimization to the initial condition,
    i.e., the human example and initial rules. In the experiments on 9 task types
    with feedback of MiniWob++, we use an example of search-engine task and the corresponding
    initial rules, which are shown in Listing [8](https://arxiv.org/html/2405.16247v4#LST8
    "Listing 8 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and
    Examples ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning") and [9](https://arxiv.org/html/2405.16247v4#LST9 "Listing
    9 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and Examples
    ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental
    Learning"). In these initial rules, we extract the successful process of search-engine
    task and a helper method to turn to the next page. Here, we provide AutoManual
    with an example of enter-text task, the simplest task type in MiniWob++, as shown
    in Listing [10](https://arxiv.org/html/2405.16247v4#LST10 "Listing 10 ‣ H.2 Examples
    for ALFWorld and MiniWob++ ‣ Appendix H Prompts and Examples ‣ AutoManual: Constructing
    Instruction Manuals by LLM Agents via Interactive Environmental Learning").'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '我们分析了规则优化对初始条件的敏感性，即人类示例和初始规则。在 MiniWob++ 上进行的 9 种任务类型的实验中，我们使用了一个搜索引擎任务的示例及其对应的初始规则，如列表[8](https://arxiv.org/html/2405.16247v4#LST8
    "Listing 8 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and
    Examples ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive
    Environmental Learning")和[9](https://arxiv.org/html/2405.16247v4#LST9 "Listing
    9 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and Examples
    ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental
    Learning")所示。在这些初始规则中，我们提取了搜索引擎任务的成功过程和一个辅助方法，用于翻到下一页。在这里，我们为 AutoManual 提供了一个输入文本任务的示例，这是
    MiniWob++ 中最简单的任务类型，如列表[10](https://arxiv.org/html/2405.16247v4#LST10 "Listing
    10 ‣ H.2 Examples for ALFWorld and MiniWob++ ‣ Appendix H Prompts and Examples
    ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental
    Learning")所示。'
- en: 'The results in Tab [8](https://arxiv.org/html/2405.16247v4#A6.T8 "Table 8 ‣
    Appendix F More Experiments ‣ AutoManual: Constructing Instruction Manuals by
    LLM Agents via Interactive Environmental Learning") demonstrate that even if given
    less initial knowledge, AutoManual can still automatically learn the required
    knowledge through online optimization, which shows that AutoManual has good robustness
    to the initial rules. In addition, we found that the knowledge it learned about
    search-engine is highly similar to the standard knowledge provided by humans,
    such as the need to turn pages for the search index larger than 3.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '表[8](https://arxiv.org/html/2405.16247v4#A6.T8 "Table 8 ‣ Appendix F More Experiments
    ‣ AutoManual: Constructing Instruction Manuals by LLM Agents via Interactive Environmental
    Learning")中的结果表明，即使给定的初始知识较少，AutoManual 仍然能够通过在线优化自动学习所需的知识，这表明 AutoManual 对初始规则具有良好的鲁棒性。此外，我们发现其学到的关于搜索引擎的知识与人类提供的标准知识高度相似，例如对于搜索索引大于
    3 的情况需要翻页。'
- en: Appendix G Case Study
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 案例研究
- en: G.1 An Example of Path Dependency Problem
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 路径依赖问题示例
- en: In the following example, the existing skill can correct solve the “Put two”
    task when the two objects occur at the same receptacles. However, in the new scenario,
    the Planner still replicates the strategy but fails to complete the task, as the
    target objects are at different receptacles.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，现有的技能可以在两个物体位于相同的容器中时正确解决“放置两个”任务。然而，在新的场景中，规划器仍然复制该策略，但未能完成任务，因为目标物体位于不同的容器中。
- en: 'Listing 1: An Example of Path Dependency Problem of the Planner on ALFWorld'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 1：ALFWorld 中规划器路径依赖问题的示例
- en: '[⬇](data:text/plain;base64,|\colorbox{green}{Here is the code for a relevant skill:}|
You are in the middle of a room. Looking quickly around you, you see bed_1, desk_1, drawer_5, drawer_4, drawer_3, drawer_2, drawer_1, dresser_1, and garbagecan_1.
The task is to: put two cellphone in bed.
```python
# Helper method to manually parse the observation for cellphones
def parse_for_cellphones(observation):
    # This is a simple parser that looks for the word 'cellphone' followed by an underscore and a number
    return [word for word in observation.split() if 'cellphone' in word]

# Define the list of receptacles to search for the cellphones.
receptacles_to_check = [
    'drawer_1', 'drawer_2', 'drawer_3', 'drawer_4', 'drawer_5', 'dresser_1'
]

# Initialize a list to keep track of found cellphones.
found_cellphones = []

# Search each receptacle for the cellphones.
for receptacle in receptacles_to_check:
    observation = agent.go_to(receptacle)
    # If the receptacle is closed, open it.
    if 'closed' in observation:
        observation = agent.open(receptacle)
    # Manually parse the observation for cellphones.
    found_cellphones.extend(parse_for_cellphones(observation))
    # If two cellphones are found, stop searching.
    if len(found_cellphones) >= 2:
        break

# Ensure two cellphones were found.
assert len(found_cellphones) >= 2, 'Error: Less than 2 cellphones found in the environment.'

# Take and place the cellphones in the bed.
for cellphone in found_cellphones[:2]:  # Take only two cellphones
    # Go to the receptacle where the cellphone was found.
    observation = agent.go_to(receptacle)
    # Take the cellphone.
    observation = agent.take_from(cellphone, receptacle)
    # Go to the bed and put the cellphone in it.
    observation = agent.go_to('bed_1')
    observation = agent.put_in_or_on(cellphone, 'bed_1')
```
Please pay close attention to the process and details of this successful code when writing code. Also, be aware of potential randomness (the current environment may differ from this one).

|\colorbox{lightgray}{Initial observation and the task:}|
You are in the middle of a room. Looking quickly around you, you see armchair_1, cabinet_4, cabinet_3, cabinet_2, cabinet_1, drawer_5, drawer_4, drawer_3, drawer_2, drawer_1, dresser_1, garbagecan_1, safe_1, shelf_12, shelf_11, shelf_10, shelf_9, shelf_8, shelf_7, shelf_6, shelf_5, shelf_4, shelf_3, shelf_2, shelf_1, sidetable_1, and sofa_1.
Your task is to: put two pillow in sofa.
|\colorbox{lime}{Planner:}|
### Understanding of the task:
The task is to find two pillows in the environment and place them in or on the sofa. The environment contains various receptacles and surfaces where the pillows might be located.

### Plan for the task:
1. Define a list of all receptacles and surfaces to check for pillows.
2. Search each receptacle for pillows.
3. Take two pillows once found.
4. Move to the sofa's location.
5. Place each pillow on the sofa.

Here is the code for the task:

```python
# Helper method to manually parse the observation for pillows
def parse_for_pillows(observation):
    # This is a simple parser that looks for the word 'pillow' followed by an underscore and a number
    return [word for word in observation.split() if 'pillow' in word]

# Define the list of receptacles to search for the pillows.
receptacles_to_check = [
    'cabinet_1', 'cabinet_2', 'cabinet_3', 'cabinet_4', 'drawer_1', 'drawer_2',
    'drawer_3', 'drawer_4', 'drawer_5', 'dresser_1', 'shelf_1', 'shelf_2', 'shelf_3',
    'shelf_4', 'shelf_5', 'shelf_6', 'shelf_7', 'shelf_8', 'shelf_9', 'shelf_10',
    'shelf_11', 'shelf_12', 'sidetable_1'
]

# Initialize a list to keep track of found pillows.
found_pillows = []

# Search each receptacle for the pillows.
for receptacle in receptacles_to_check:
    observation = agent.go_to(receptacle)
    # If the receptacle is closed, open it.
    if 'closed' in observation:
        observation = agent.open(receptacle)
    # Manually parse the observation for pillows.
    found_pillows.extend(parse_for_pillows(observation))
    # If two pillows are found, stop searching.
    if len(found_pillows) >= 2:
        break

# Ensure two pillows were found.
assert len(found_pillows) >= 2, 'Error: Less than 2 pillows found in the environment.'

# Take and place the pillows in the sofa.
for pillow in found_pillows[:2]:  # Take only two pillows
    # Go to the receptacle where the pillow was found.
    observation = agent.go_to(receptacle)
    # Take the pillow.
    observation = agent.take_from(pillow, receptacle)
    # Go to the sofa and put the pillow in it.
    observation = agent.go_to('sofa_1')
    observation = agent.put_in_or_on(pillow, 'sofa_1')
```
|\colorbox{lightgray}{Feedback including errors:}|
obs_1: Act: agent.go_to('cabinet_1'). Obs: cabinet_1 is closed. You are at cabinet_1 and holding nothing.
obs_2: Act: agent.open('cabinet_1'). Obs: You open cabinet_1. In cabinet_1, you see tissuebox_1. You are at cabinet_1 and holding nothing.
obs_3: Act: agent.go_to('cabinet_2'). Obs: Nothing happens. You are at cabinet_1 and holding nothing.
obs_4: Act: agent.go_to('cabinet_3'). Obs: cabinet_3 is closed. You are at cabinet_3 and holding nothing.
obs_5: Act: agent.open('cabinet_3'). Obs: You open cabinet_3. In cabinet_3, you see box_3, and tissuebox_2. You are at cabinet_3 and holding nothing.
obs_6: Act: agent.go_to('cabinet_4'). Obs: Nothing happens. You are at cabinet_3 and holding nothing.
obs_7: Act: agent.go_to('drawer_1'). Obs: drawer_1 is closed. You are at drawer_1 and holding nothing.
obs_8: Act: agent.open('drawer_1'). Obs: You open drawer_1. In drawer_1, you see nothing. You are at drawer_1 and holding nothing.
obs_9: Act: agent.go_to('drawer_2'). Obs: drawer_2 is closed. You are at drawer_2 and holding nothing.
obs_10: Act: agent.open('drawer_2'). Obs: You open drawer_2. In drawer_2, you see nothing. You are at drawer_2 and holding nothing.
obs_11: Act: agent.go_to('drawer_3'). Obs: drawer_3 is closed. You are at drawer_3 and holding nothing.
obs_12: Act: agent.open('drawer_3'). Obs: You open drawer_3. In drawer_3, you see nothing. You are at drawer_3 and holding nothing.
obs_13: Act: agent.go_to('drawer_4'). Obs: drawer_4 is closed. You are at drawer_4 and holding nothing.
obs_14: Act: agent.open('drawer_4'). Obs: You open drawer_4. In drawer_4, you see nothing. You are at drawer_4 and holding nothing.
obs_15: Act: agent.go_to('drawer_5'). Obs: drawer_5 is closed. You are at drawer_5 and holding nothing.
obs_16: Act: agent.open('drawer_5'). Obs: You open drawer_5. In drawer_5, you see nothing. You are at drawer_5 and holding nothing.
obs_17: Act: agent.go_to('dresser_1'). Obs: On dresser_1, you see box_2, laptop_1, statue_1, watch_2, and watch_1. You are at dresser_1 and holding nothing.
obs_18: Act: agent.go_to('shelf_1'). Obs: On shelf_1, you see nothing. You are at shelf_1 and holding nothing.
obs_19: Act: agent.go_to('shelf_2'). Obs: On shelf_2, you see nothing. You are at shelf_2 and holding nothing.
obs_20: Act: agent.go_to('shelf_3'). Obs: Nothing happens. You are at shelf_2 and holding nothing.
obs_21: Act: agent.go_to('shelf_4'). Obs: On shelf_4, you see nothing. You are at shelf_4 and holding nothing.
obs_22: Act: agent.go_to('shelf_5'). Obs: On shelf_5, you see newspaper_2. You are at shelf_5 and holding nothing.
obs_23: Act: agent.go_to('shelf_6'). Obs: On shelf_6, you see nothing. You are at shelf_6 and holding nothing.
obs_24: Act: agent.go_to('shelf_7'). Obs: On shelf_7, you see watch_3. You are at shelf_7 and holding nothing.
obs_25: Act: agent.go_to('shelf_8'). Obs: On shelf_8, you see nothing. You are at shelf_8 and holding nothing.
obs_26: Act: agent.go_to('shelf_9'). Obs: On shelf_9, you see nothing. You are at shelf_9 and holding nothing.
obs_27: Act: agent.go_to('shelf_10'). Obs: On shelf_10, you see nothing. You are at shelf_10 and holding nothing.
obs_28: Act: agent.go_to('shelf_11'). Obs: On shelf_11, you see nothing. You are at shelf_11 and holding nothing.
obs_29: Act: agent.go_to('shelf_12'). Obs: On shelf_12, you see cellphone_1. You are at shelf_12 and holding nothing.
obs_30: Act: agent.go_to('sidetable_1'). Obs: On sidetable_1, you see creditcard_1, keychain_1, and newspaper_1. You are at sidetable_1 and holding nothing.
Execution error:
Error: Less than 2 pillows found in the environment.
Current state: You are at sidetable_1 and holding nothing.)Here
    is the code for a relevant skill:You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  bed_1,  desk_1,  drawer_5,  drawer_4,  drawer_3,  drawer_2,  drawer_1,  dresser_1,  and  garbagecan_1.The  task  is  to:  put  two  cellphone  in  bed.‘‘‘python#  Helper  method  to  manually  parse  the  observation  for  cellphonesdef  parse_for_cellphones(observation):#  This  is  a  simple  parser  that  looks  for  the  word  ’cellphone’  followed  by  an  underscore  and  a  numberreturn  [word  for  word  in  observation.split()  if  ’cellphone’  in  word]#  Define  the  list  of  receptacles  to  search  for  the  cellphones.receptacles_to_check  =  [’drawer_1’,  ’drawer_2’,  ’drawer_3’,  ’drawer_4’,  ’drawer_5’,  ’dresser_1’]#  Initialize  a  list  to  keep  track  of  found  cellphones.found_cellphones  =  []#  Search  each  receptacle  for  the  cellphones.for  receptacle  in  receptacles_to_check:observation  =  agent.go_to(receptacle)#  If  the  receptacle  is  closed,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Manually  parse  the  observation  for  cellphones.found_cellphones.extend(parse_for_cellphones(observation))#  If  two  cellphones  are  found,  stop  searching.if  len(found_cellphones)  >=  2:break#  Ensure  two  cellphones  were  found.assert  len(found_cellphones)  >=  2,  ’Error:  Less  than  2  cellphones  found  in  the  environment.’#  Take  and  place  the  cellphones  in  the  bed.for  cellphone  in  found_cellphones[:2]:  #  Take  only  two  cellphones#  Go  to  the  receptacle  where  the  cellphone  was  found.observation  =  agent.go_to(receptacle)#  Take  the  cellphone.observation  =  agent.take_from(cellphone,  receptacle)#  Go  to  the  bed  and  put  the  cellphone  in  it.observation  =  agent.go_to(’bed_1’)observation  =  agent.put_in_or_on(cellphone,  ’bed_1’)‘‘‘Please  pay  close  attention  to  the  process  and  details  of  this  successful  code  when  writing  code.  Also,  be  aware  of  potential  randomness  (the  current  environment  may  differ  from  this  one).Initial
    observation and the task:You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  armchair_1,  cabinet_4,  cabinet_3,  cabinet_2,  cabinet_1,  drawer_5,  drawer_4,  drawer_3,  drawer_2,  drawer_1,  dresser_1,  garbagecan_1,  safe_1,  shelf_12,  shelf_11,  shelf_10,  shelf_9,  shelf_8,  shelf_7,  shelf_6,  shelf_5,  shelf_4,  shelf_3,  shelf_2,  shelf_1,  sidetable_1,  and  sofa_1.Your  task  is  to:  put  two  pillow  in  sofa.Planner:###  Understanding  of  the  task:The  task  is  to  find  two  pillows  in  the  environment  and  place  them  in  or  on  the  sofa.  The  environment  contains  various  receptacles  and  surfaces  where  the  pillows  might  be  located.###  Plan  for  the  task:1.  Define  a  list  of  all  receptacles  and  surfaces  to  check  for  pillows.2.  Search  each  receptacle  for  pillows.3.  Take  two  pillows  once  found.4.  Move  to  the  sofa’s  location.5.  Place  each  pillow  on  the  sofa.Here  is  the  code  for  the  task:‘‘‘python#  Helper  method  to  manually  parse  the  observation  for  pillowsdef  parse_for_pillows(observation):#  This  is  a  simple  parser  that  looks  for  the  word  ’pillow’  followed  by  an  underscore  and  a  numberreturn  [word  for  word  in  observation.split()  if  ’pillow’  in  word]#  Define  the  list  of  receptacles  to  search  for  the  pillows.receptacles_to_check  =  [’cabinet_1’,  ’cabinet_2’,  ’cabinet_3’,  ’cabinet_4’,  ’drawer_1’,  ’drawer_2’,’drawer_3’,  ’drawer_4’,  ’drawer_5’,  ’dresser_1’,  ’shelf_1’,  ’shelf_2’,  ’shelf_3’,’shelf_4’,  ’shelf_5’,  ’shelf_6’,  ’shelf_7’,  ’shelf_8’,  ’shelf_9’,  ’shelf_10’,’shelf_11’,  ’shelf_12’,  ’sidetable_1’]#  Initialize  a  list  to  keep  track  of  found  pillows.found_pillows  =  []#  Search  each  receptacle  for  the  pillows.for  receptacle  in  receptacles_to_check:observation  =  agent.go_to(receptacle)#  If  the  receptacle  is  closed,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Manually  parse  the  observation  for  pillows.found_pillows.extend(parse_for_pillows(observation))#  If  two  pillows  are  found,  stop  searching.if  len(found_pillows)  >=  2:break#  Ensure  two  pillows  were  found.assert  len(found_pillows)  >=  2,  ’Error:  Less  than  2  pillows  found  in  the  environment.’#  Take  and  place  the  pillows  in  the  sofa.for  pillow  in  found_pillows[:2]:  #  Take  only  two  pillows#  Go  to  the  receptacle  where  the  pillow  was  found.observation  =  agent.go_to(receptacle)#  Take  the  pillow.observation  =  agent.take_from(pillow,  receptacle)#  Go  to  the  sofa  and  put  the  pillow  in  it.observation  =  agent.go_to(’sofa_1’)observation  =  agent.put_in_or_on(pillow,  ’sofa_1’)‘‘‘Feedback
    including errors:obs_1:  Act:  agent.go_to(’cabinet_1’).  Obs:  cabinet_1  is  closed.  You  are  at  cabinet_1  and  holding  nothing.obs_2:  Act:  agent.open(’cabinet_1’).  Obs:  You  open  cabinet_1.  In  cabinet_1,  you  see  tissuebox_1.  You  are  at  cabinet_1  and  holding  nothing.obs_3:  Act:  agent.go_to(’cabinet_2’).  Obs:  Nothing  happens.  You  are  at  cabinet_1  and  holding  nothing.obs_4:  Act:  agent.go_to(’cabinet_3’).  Obs:  cabinet_3  is  closed.  You  are  at  cabinet_3  and  holding  nothing.obs_5:  Act:  agent.open(’cabinet_3’).  Obs:  You  open  cabinet_3.  In  cabinet_3,  you  see  box_3,  and  tissuebox_2.  You  are  at  cabinet_3  and  holding  nothing.obs_6:  Act:  agent.go_to(’cabinet_4’).  Obs:  Nothing  happens.  You  are  at  cabinet_3  and  holding  nothing.obs_7:  Act:  agent.go_to(’drawer_1’).  Obs:  drawer_1  is  closed.  You  are  at  drawer_1  and  holding  nothing.obs_8:  Act:  agent.open(’drawer_1’).  Obs:  You  open  drawer_1.  In  drawer_1,  you  see  nothing.  You  are  at  drawer_1  and  holding  nothing.obs_9:  Act:  agent.go_to(’drawer_2’).  Obs:  drawer_2  is  closed.  You  are  at  drawer_2  and  holding  nothing.obs_10:  Act:  agent.open(’drawer_2’).  Obs:  You  open  drawer_2.  In  drawer_2,  you  see  nothing.  You  are  at  drawer_2  and  holding  nothing.obs_11:  Act:  agent.go_to(’drawer_3’).  Obs:  drawer_3  is  closed.  You  are  at  drawer_3  and  holding  nothing.obs_12:  Act:  agent.open(’drawer_3’).  Obs:  You  open  drawer_3.  In  drawer_3,  you  see  nothing.  You  are  at  drawer_3  and  holding  nothing.obs_13:  Act:  agent.go_to(’drawer_4’).  Obs:  drawer_4  is  closed.  You  are  at  drawer_4  and  holding  nothing.obs_14:  Act:  agent.open(’drawer_4’).  Obs:  You  open  drawer_4.  In  drawer_4,  you  see  nothing.  You  are  at  drawer_4  and  holding  nothing.obs_15:  Act:  agent.go_to(’drawer_5’).  Obs:  drawer_5  is  closed.  You  are  at  drawer_5  and  holding  nothing.obs_16:  Act:  agent.open(’drawer_5’).  Obs:  You  open  drawer_5.  In  drawer_5,  you  see  nothing.  You  are  at  drawer_5  and  holding  nothing.obs_17:  Act:  agent.go_to(’dresser_1’).  Obs:  On  dresser_1,  you  see  box_2,  laptop_1,  statue_1,  watch_2,  and  watch_1.  You  are  at  dresser_1  and  holding  nothing.obs_18:  Act:  agent.go_to(’shelf_1’).  Obs:  On  shelf_1,  you  see  nothing.  You  are  at  shelf_1  and  holding  nothing.obs_19:  Act:  agent.go_to(’shelf_2’).  Obs:  On  shelf_2,  you  see  nothing.  You  are  at  shelf_2  and  holding  nothing.obs_20:  Act:  agent.go_to(’shelf_3’).  Obs:  Nothing  happens.  You  are  at  shelf_2  and  holding  nothing.obs_21:  Act:  agent.go_to(’shelf_4’).  Obs:  On  shelf_4,  you  see  nothing.  You  are  at  shelf_4  and  holding  nothing.obs_22:  Act:  agent.go_to(’shelf_5’).  Obs:  On  shelf_5,  you  see  newspaper_2.  You  are  at  shelf_5  and  holding  nothing.obs_23:  Act:  agent.go_to(’shelf_6’).  Obs:  On  shelf_6,  you  see  nothing.  You  are  at  shelf_6  and  holding  nothing.obs_24:  Act:  agent.go_to(’shelf_7’).  Obs:  On  shelf_7,  you  see  watch_3.  You  are  at  shelf_7  and  holding  nothing.obs_25:  Act:  agent.go_to(’shelf_8’).  Obs:  On  shelf_8,  you  see  nothing.  You  are  at  shelf_8  and  holding  nothing.obs_26:  Act:  agent.go_to(’shelf_9’).  Obs:  On  shelf_9,  you  see  nothing.  You  are  at  shelf_9  and  holding  nothing.obs_27:  Act:  agent.go_to(’shelf_10’).  Obs:  On  shelf_10,  you  see  nothing.  You  are  at  shelf_10  and  holding  nothing.obs_28:  Act:  agent.go_to(’shelf_11’).  Obs:  On  shelf_11,  you  see  nothing.  You  are  at  shelf_11  and  holding  nothing.obs_29:  Act:  agent.go_to(’shelf_12’).  Obs:  On  shelf_12,  you  see  cellphone_1.  You  are  at  shelf_12  and  holding  nothing.obs_30:  Act:  agent.go_to(’sidetable_1’).  Obs:  On  sidetable_1,  you  see  creditcard_1,  keychain_1,  and  newspaper_1.  You  are  at  sidetable_1  and  holding  nothing.Execution  error:Error:  Less  than  2  pillows  found  in  the  environment.Current  state:  You  are  at  sidetable_1  and  holding  nothing.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: G.2 A Trajectory of The Building Stage
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 建设阶段的轨迹
- en: 'Here, we present a trajectory of the Planner and the corresponding responses
    of the Builder for a task in ALFWorld. In this “Indirect Success” trajectory,
    the Planner made the mistake of heating the mug without having it next to the
    microwave. The Planner then corrected the mistake and provided organized code
    in its conclusion. After observing this trajectory, the Builder keenly found the
    cause of the error: the existing “Success Process” rule did not indicate that
    the agent must go to the object before interacting with the object. The Builder
    updated the “Success Process” rule and added a “Corrected Error” rule to clarify
    the mistake.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了 ALFWorld 中 Planner 的轨迹以及 Builder 对任务的相应回应。在这个“间接成功”轨迹中，Planner 错误地加热了杯子，但没有将其放在微波炉旁边。之后，Planner
    纠正了这个错误，并在结论中提供了有组织的代码。在观察到这个轨迹后，Builder 敏锐地找到了错误的原因：现有的“成功过程”规则并没有指明智能体必须先去物体旁边再进行交互。于是，Builder
    更新了“成功过程”规则，并添加了一个“纠正错误”规则来澄清这个错误。
- en: 'Listing 2: A Trajectory of the Planner and Builder on ALFWorld'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2：Planner 和 Builder 在 ALFWorld 上的轨迹
- en: '[⬇](data:text/plain;base64,|\colorbox{lightgray}{Initial observation and the task:}|
You are in the middle of a room. Looking quickly around you, you see cabinet_13, cabinet_12, cabinet_11, cabinet_10, cabinet_9, cabinet_8, cabinet_7, cabinet_6, cabinet_5, cabinet_4, cabinet_3, cabinet_2, cabinet_1, coffeemachine_1, countertop_1, diningtable_1, drawer_1, fridge_1, garbagecan_1, microwave_1, shelf_3, shelf_2, shelf_1, sinkbasin_1, stoveburner_4, stoveburner_3, stoveburner_2, stoveburner_1, and toaster_1.
Your task is to: heat some mug and put it in cabinet.
|\colorbox{lime}{Planner:}|
### Understanding of the observation:
The agent is in a room with numerous cabinets, appliances, and surfaces. The task is to heat a mug and then place it in a cabinet. This requires finding a mug, heating it, and then placing it in any cabinet.

|\colorbox{orange}{\#\#\# Rules to consider:}|
- **rule_0**: Ensure all receptacles and surfaces are considered when searching for the mug.
- **rule_1**: Use the 'find_object' method to search for the mug.
- **rule_2**: After finding the mug, heat it and then put it in a cabinet.
- **rule_3**: Open closed receptacles to search for objects.

### Plan for the task:
1. Use the 'find_object' method to search for a mug in all receptacles and surfaces.
2. Once the mug is found, take it.
3. Heat the mug with an appropriate appliance (e.g., microwave).
4. Find an empty cabinet or any cabinet to place the heated mug in.
5. Place the mug in the cabinet.

Here is the code for the task:
```python
# [Step 1] Define the helper method 'find_object' as provided in rule_1
def find_object(agent, recep_to_check, object_name):
    for receptacle in recep_to_check:
        observation = agent.go_to(receptacle)
        # Check if we need to open the receptacle. If we do, open it.
        if 'closed' in observation:
            observation = agent.open(receptacle)
        # Check if the object is in/on the receptacle.
        if object_name in observation:
            object_ids = get_object_with_id(observation, object_name)
            return object_ids, receptacle
    return None, None

# [Step 2] Get a sorted list of all receptacles and surfaces to check for a mug
recep_to_check = ['cabinet_1', 'cabinet_2', 'cabinet_3', 'cabinet_4', 'cabinet_5', 'cabinet_6', 'cabinet_7', 'cabinet_8', 'cabinet_9', 'cabinet_10', 'cabinet_11', 'cabinet_12', 'cabinet_13', 'countertop_1', 'diningtable_1', 'drawer_1', 'fridge_1', 'garbagecan_1', 'microwave_1', 'shelf_1', 'shelf_2', 'shelf_3', 'sinkbasin_1', 'stoveburner_1', 'stoveburner_2', 'stoveburner_3', 'stoveburner_4', 'toaster_1', 'coffeemachine_1']
object_ids, receptacle_with_mug = find_object(agent, recep_to_check, 'mug')
# Assert that we have found a mug
assert object_ids is not None, f'Error in [Step 2]: There is no mug in/on {recep_to_check}.'

# [Step 3] Take the mug
found_mug = object_ids[0]
observation = agent.take_from(found_mug, receptacle_with_mug)
# Assert that the agent is holding the mug
assert agent.holding == found_mug, f'Error in [Step 3]: I cannot take {found_mug} from {receptacle_with_mug}.'

# [Step 4] Heat the mug with an appropriate appliance (e.g., microwave)
observation = agent.heat_with(found_mug, 'microwave_1')
# Assert that the mug has been heated
assert 'heat' in observation and found_mug in observation, f'Error in [Step 4]: I cannot heat {found_mug} with microwave_1.'

# [Step 5] Find an empty cabinet or any cabinet to place the heated mug in
# Since the task does not specify which cabinet to use, we can choose the first one
observation = agent.go_to('cabinet_1')
# Check if the cabinet is closed. If so, open it.
if 'closed' in observation:
    observation = agent.open('cabinet_1')
observation = agent.put_in_or_on(found_mug, 'cabinet_1')
# Assert that the mug is put in the cabinet
assert 'put' in observation and found_mug in observation, f'Error in [Step 5]: I cannot put {found_mug} in cabinet_1.'
```
|\colorbox{lightgray}{Feedback including errors:}|
obs_1: Act: agent.go_to('cabinet_1'). Obs: cabinet_1 is closed. You are at cabinet_1 and holding nothing.
obs_2: Act: agent.open('cabinet_1'). Obs: You open cabinet_1. In cabinet_1, you see nothing. You are at cabinet_1 and holding nothing.
...
obs_33: Act: agent.go_to('shelf_1'). Obs: On shelf_1, you see mug_1. You are at shelf_1 and holding nothing.
obs_34: Act: agent.take_from('mug_1', 'shelf_1'). Obs: You take mug_1 from shelf_1. You are at shelf_1 and holding mug_1.
obs_35: Act: agent.heat_with('mug_1', 'microwave_1'). Obs: Nothing happens. You are at shelf_1 and holding mug_1.
Execution error:
|\colorbox{orange}{Error in [Step 4]: I cannot heat mug\_1 with microwave\_1.}|
Current state: You are at shelf_1 and holding mug_1.
|\colorbox{lime}{Planner's replanning:}|
### Understanding of the current observations:
The agent successfully found a mug on shelf_1 and took it. However, when attempting to heat the mug with the microwave, the action failed with "Nothing happens." This indicates that the agent tried to use the microwave while not being at the microwave's location. The agent must go to the microwave before attempting to heat the mug.

### Mistakes in the previous code and the reason for errors:
The error occurred because the agent attempted to heat the mug without being at the microwave's location. The precondition for heating an object with a receptacle is that the agent must be at the same location as the receptacle.

### Rules to consider:
- **rule_0**: Ensure all receptacles and surfaces are considered when searching for the mug.
- **rule_1**: Use the 'find_object' method to search for the mug.
- **rule_2**: After finding the mug, heat it and then put it in a cabinet.
- **rule_3**: Open closed receptacles to search for objects.

### Plan for the task:
1. Since the mug has already been found and is being held by the agent, proceed to the microwave.
2. Open the microwave if it is closed.
3. Heat the mug with the microwave.
4. Find an empty cabinet or any cabinet to place the heated mug in.
5. Place the mug in the cabinet.

Here is the updated code for the task:
```python
# [Step 4] Go to the microwave and open it if it's closed
observation = agent.go_to('microwave_1')
if 'closed' in observation:
    observation = agent.open('microwave_1')
# Assert that the agent is at the microwave and it's open
assert 'microwave_1' in observation and 'closed' not in observation, f'Error in [Step 4]: Microwave is not open or not at microwave_1.'

# [Step 5] Heat the mug with the microwave
observation = agent.heat_with('mug_1', 'microwave_1')
# Assert that the mug has been heated
assert 'heat' in observation and 'mug_1' in observation, f'Error in [Step 5]: I cannot heat mug_1 with microwave_1.'

# [Step 6] Find an empty cabinet or any cabinet to place the heated mug in
# Since the task does not specify which cabinet to use, we can choose the first one that is empty or available
# We already know that cabinets 1 through 10 are empty or contain other items, so we can choose cabinet_1 for simplicity
observation = agent.go_to('cabinet_1')
if 'closed' in observation:
    observation = agent.open('cabinet_1')
# Assert that the agent is at cabinet_1 and it's open
assert 'cabinet_1' in observation and 'closed' not in observation, f'Error in [Step 6]: Cabinet_1 is not open or not at cabinet_1.'

# [Step 7] Place the mug in the cabinet
observation = agent.put_in_or_on('mug_1', 'cabinet_1')
# Assert that the mug is put in the cabinet
assert 'put' in observation and 'mug_1' in observation, f'Error in [Step 7]: I cannot put mug_1 in cabinet_1.'
```
|\colorbox{lightgray}{Feedback:}|
obs_36: Act: agent.go_to('microwave_1'). Obs: In microwave_1, you see egg_2. You are at microwave_1 and holding mug_1.
obs_37: Act: agent.heat_with('mug_1', 'microwave_1'). Obs: You heat mug_1 using microwave_1. You are at microwave_1 and holding mug_1.
obs_38: Act: agent.go_to('cabinet_1'). Obs: In cabinet_1, you see nothing. You are at cabinet_1 and holding mug_1.
obs_39: Act: agent.put_in_or_on('mug_1', 'cabinet_1'). Obs: You put mug_1 in/on cabinet_1. You are at cabinet_1 and holding nothing. This epoch is done. Succeed: True
Current state: You are at cabinet_1 and holding nothing.
|\colorbox{lime}{Planner's conclusion:}|
### Summary of misunderstandings and mistakes:
Initially, I attempted to heat the mug without being at the microwave's location, which is why the action failed with "Nothing happens." The correct procedure is to first go to the microwave before attempting to heat any object. After correcting this mistake, the agent successfully heated the mug and placed it in cabinet_1.

### Organized code block:
```python
# [Helper Method] Define the helper method 'find_object' as provided in rule_1
def find_object(agent, recep_to_check, object_name):
    for receptacle in recep_to_check:
        observation = agent.go_to(receptacle)
        # Check if we need to open the receptacle. If we do, open it.
        if 'closed' in observation:
            observation = agent.open(receptacle)
        # Check if the object is in/on the receptacle.
        if object_name in observation:
            object_ids = get_object_with_id(observation, object_name)
            return object_ids, receptacle
    return None, None

# [Step 1] Get a sorted list of all receptacles and surfaces to check for a mug
recep_to_check = ['cabinet_1', 'cabinet_2', 'cabinet_3', 'cabinet_4', 'cabinet_5', 'cabinet_6', 'cabinet_7', 'cabinet_8', 'cabinet_9', 'cabinet_10', 'cabinet_11', 'cabinet_12', 'cabinet_13', 'countertop_1', 'diningtable_1', 'drawer_1', 'fridge_1', 'garbagecan_1', 'microwave_1', 'shelf_1', 'shelf_2', 'shelf_3', 'sinkbasin_1', 'stoveburner_1', 'stoveburner_2', 'stoveburner_3', 'stoveburner_4', 'toaster_1', 'coffeemachine_1']
object_ids, receptacle_with_mug = find_object(agent, recep_to_check, 'mug')
# Assert that we have found a mug
assert object_ids is not None, f'Error in [Step 1]: There is no mug in/on {recep_to_check}.'

# [Step 2] Take the mug
found_mug = object_ids[0]
observation = agent.take_from(found_mug, receptacle_with_mug)
# Assert that the agent is holding the mug
assert agent.holding == found_mug, f'Error in [Step 2]: I cannot take {found_mug} from {receptacle_with_mug}.'

# [Step 3] Go to the microwave and open it if it's closed
observation = agent.go_to('microwave_1')
if 'closed' in observation:
    observation = agent.open('microwave_1')
# Assert that the agent is at the microwave and it's open
assert 'microwave_1' in observation and 'closed' not in observation, f'Error in [Step 3]: Microwave is not open or not at microwave_1.'

# [Step 4] Heat the mug with the microwave
observation = agent.heat_with(found_mug, 'microwave_1')
# Assert that the mug has been heated
assert 'heat' in observation and found_mug in observation, f'Error in [Step 4]: I cannot heat {found_mug} with microwave_1.'

# [Step 5] Find an empty cabinet or any cabinet to place the heated mug in
# Since the task does not specify which cabinet to use, we can choose the first one that is empty or available
# We already know that cabinets 1 through 10 are empty or contain other items, so we can choose cabinet_1 for simplicity
observation = agent.go_to('cabinet_1')
if 'closed' in observation:
    observation = agent.open('cabinet_1')
# Assert that the agent is at cabinet_1 and it's open
assert 'cabinet_1' in observation and 'closed' not in observation, f'Error in [Step 5]: Cabinet_1 is not open or not at cabinet_1.'

# [Step 6] Place the mug in the cabinet
observation = agent.put_in_or_on(found_mug, 'cabinet_1')
# Assert that the mug is put in the cabinet
assert 'put' in observation and found_mug in observation, f'Error in [Step 6]: I cannot put {found_mug} in cabinet_1.'
```
|\colorbox{cyan}{Builder's classification:}|
Analyzing the scenario, we can identify the root cause of the observed mistakes and relate them to the existing rules:

1. The agent initially attempted to heat the mug without being at the microwave's location, which resulted in the action "Nothing happens." This mistake was due to the agent not following the precondition for the `heat_with` action, which requires the agent to be at the same location as the receptacle (in this case, the microwave).

2. The existing rules related to the mistakes are:
   - **rule_1**: This rule provides a 'find_object' method to search for objects, which the agent used correctly to find the mug.
   - **rule_2**: This 'Success Process' rule outlines the steps to put an object in a receptacle after finding and manipulating it. However, it does not explicitly state that the agent must be at the location of the appliance to use it, which is relevant for the heating step.
   - **rule_3**: This rule reminds the agent to open closed receptacles to search for objects, which the agent did correctly.

3. There is a 'Success Process' rule applicable to this type of task (rule_2), but it does not explicitly cover the precondition for heating an object, which is to be at the same location as the appliance.

4. |\colorbox{orange}{The fault in this scenario can be attributed to:}|
   |\colorbox{orange}{*Imperfect Rules*: The existing 'Success Process' rule (rule\_2) does not fully document the}|
   |\colorbox{orange}{precondition for using appliances, such as the microwave. It does not state that the agent must be at}|
   |\colorbox{orange}{the location of the appliance to use it for heating an object.}|

Based on the analysis, the conclusion is that the fault stems from *Imperfect Rules*. The rules need to be updated to include the precondition for using appliances, which is a critical step in the 'Success Process' for tasks involving heating objects.
|\colorbox{cyan}{Builder:}|
Potential Rules:
1. *Corrected Error*: When attempting to use an appliance (e.g., microwave), the agent must be at the same location as the appliance. This rule is related to the existing 'Success Process' rule (rule_2), but it adds a specific precondition for using appliances that was not previously documented.
2. *Success Process*: The existing 'Success Process' rule (rule_2) should be updated to include the precondition of being at the location of the appliance before using it. This is a critical step in tasks involving heating objects.

Check Difference:
- The potential 'Corrected Error' rule targets the specific precondition for using appliances, which is a new addition and not covered by existing rules.
- The update to the 'Success Process' rule expands on the existing steps to include the precondition for using appliances, which is a refinement of the existing rule.

Check Existing Rules:
- Rule_2 needs to be updated to include the precondition for using appliances, as this was the key oversight that led to the agent's initial error.

Codes:
```python
|\colorbox{orange}{\# Update the existing 'Success Process' rule to include the precondition for using appliances}|
rule_system.update_rule(
    rule_id='rule_2',
    rule="If the task is to heat an object and put it in a receptacle, the agent must first find the object, ensure they are at the location of the heating appliance before using it, and then place the object in the target receptacle.",
    example="""# For example, to heat a mug and put it in a cabinet:
# [Step 1] Use 'find_object' method to search all receptacles
# [Step 2] Take the mug
# [Step 3] Go to the heating appliance (e.g., microwave) and ensure you are at its location
# [Step 4] Heat the mug with the appliance
# [Step 5] Find an empty cabinet or any cabinet to place the heated mug in
# [Step 6] Place the mug in the cabinet""",
    validation_record="Updated to include the precondition for using appliances based on the error and subsequent success in epoch_1."
)

|\colorbox{orange}{\# Write a new 'Corrected Error' rule for the precondition of using appliances}|
rule_system.write_rule(
    rule="Before using an appliance (e.g., microwave), the agent must go to the location of the appliance.",
    type="Corrected Error",
    example="""# Corrective code:
# Go to the microwave before attempting to heat the mug
observation = agent.go_to('microwave_1')
# Heat the mug with the microwave
observation = agent.heat_with('mug_1', 'microwave_1')""",
    validation_record="Derived from the error and correction in epoch_1 where the agent initially failed to heat the mug due to not being at the microwave's location."
)

rule_system.stop_generating()
```)Initial
    observation and the task:You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  cabinet_13,  cabinet_12,  cabinet_11,  cabinet_10,  cabinet_9,  cabinet_8,  cabinet_7,  cabinet_6,  cabinet_5,  cabinet_4,  cabinet_3,  cabinet_2,  cabinet_1,  coffeemachine_1,  countertop_1,  diningtable_1,  drawer_1,  fridge_1,  garbagecan_1,  microwave_1,  shelf_3,  shelf_2,  shelf_1,  sinkbasin_1,  stoveburner_4,  stoveburner_3,  stoveburner_2,  stoveburner_1,  and  toaster_1.Your  task  is  to:  heat  some  mug  and  put  it  in  cabinet.Planner:###  Understanding  of  the  observation:The  agent  is  in  a  room  with  numerous  cabinets,  appliances,  and  surfaces.  The  task  is  to  heat  a  mug  and  then  place  it  in  a  cabinet.  This  requires  finding  a  mug,  heating  it,  and  then  placing  it  in  any  cabinet.###
    Rules to consider:-  **rule_0**:  Ensure  all  receptacles  and  surfaces  are  considered  when  searching  for  the  mug.-  **rule_1**:  Use  the  ’find_object’  method  to  search  for  the  mug.-  **rule_2**:  After  finding  the  mug,  heat  it  and  then  put  it  in  a  cabinet.-  **rule_3**:  Open  closed  receptacles  to  search  for  objects.###  Plan  for  the  task:1.  Use  the  ’find_object’  method  to  search  for  a  mug  in  all  receptacles  and  surfaces.2.  Once  the  mug  is  found,  take  it.3.  Heat  the  mug  with  an  appropriate  appliance  (e.g.,  microwave).4.  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in.5.  Place  the  mug  in  the  cabinet.Here  is  the  code  for  the  task:‘‘‘python#  [Step  1]  Define  the  helper  method  ’find_object’  as  provided  in  rule_1def  find_object(agent,  recep_to_check,  object_name):for  receptacle  in  recep_to_check:observation  =  agent.go_to(receptacle)#  Check  if  we  need  to  open  the  receptacle.  If  we  do,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Check  if  the  object  is  in/on  the  receptacle.if  object_name  in  observation:object_ids  =  get_object_with_id(observation,  object_name)return  object_ids,  receptaclereturn  None,  None#  [Step  2]  Get  a  sorted  list  of  all  receptacles  and  surfaces  to  check  for  a  mugrecep_to_check  =  [’cabinet_1’,  ’cabinet_2’,  ’cabinet_3’,  ’cabinet_4’,  ’cabinet_5’,  ’cabinet_6’,  ’cabinet_7’,  ’cabinet_8’,  ’cabinet_9’,  ’cabinet_10’,  ’cabinet_11’,  ’cabinet_12’,  ’cabinet_13’,  ’countertop_1’,  ’diningtable_1’,  ’drawer_1’,  ’fridge_1’,  ’garbagecan_1’,  ’microwave_1’,  ’shelf_1’,  ’shelf_2’,  ’shelf_3’,  ’sinkbasin_1’,  ’stoveburner_1’,  ’stoveburner_2’,  ’stoveburner_3’,  ’stoveburner_4’,  ’toaster_1’,  ’coffeemachine_1’]object_ids,  receptacle_with_mug  =  find_object(agent,  recep_to_check,  ’mug’)#  Assert  that  we  have  found  a  mugassert  object_ids  is  not  None,  f’Error  in  [Step  2]:  There  is  no  mug  in/on  {recep_to_check}.’#  [Step  3]  Take  the  mugfound_mug  =  object_ids[0]observation  =  agent.take_from(found_mug,  receptacle_with_mug)#  Assert  that  the  agent  is  holding  the  mugassert  agent.holding  ==  found_mug,  f’Error  in  [Step  3]:  I  cannot  take  {found_mug}  from  {receptacle_with_mug}.’#  [Step  4]  Heat  the  mug  with  an  appropriate  appliance  (e.g.,  microwave)observation  =  agent.heat_with(found_mug,  ’microwave_1’)#  Assert  that  the  mug  has  been  heatedassert  ’heat’  in  observation  and  found_mug  in  observation,  f’Error  in  [Step  4]:  I  cannot  heat  {found_mug}  with  microwave_1.’#  [Step  5]  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in#  Since  the  task  does  not  specify  which  cabinet  to  use,  we  can  choose  the  first  oneobservation  =  agent.go_to(’cabinet_1’)#  Check  if  the  cabinet  is  closed.  If  so,  open  it.if  ’closed’  in  observation:observation  =  agent.open(’cabinet_1’)observation  =  agent.put_in_or_on(found_mug,  ’cabinet_1’)#  Assert  that  the  mug  is  put  in  the  cabinetassert  ’put’  in  observation  and  found_mug  in  observation,  f’Error  in  [Step  5]:  I  cannot  put  {found_mug}  in  cabinet_1.’‘‘‘Feedback
    including errors:obs_1:  Act:  agent.go_to(’cabinet_1’).  Obs:  cabinet_1  is  closed.  You  are  at  cabinet_1  and  holding  nothing.obs_2:  Act:  agent.open(’cabinet_1’).  Obs:  You  open  cabinet_1.  In  cabinet_1,  you  see  nothing.  You  are  at  cabinet_1  and  holding  nothing....obs_33:  Act:  agent.go_to(’shelf_1’).  Obs:  On  shelf_1,  you  see  mug_1.  You  are  at  shelf_1  and  holding  nothing.obs_34:  Act:  agent.take_from(’mug_1’,  ’shelf_1’).  Obs:  You  take  mug_1  from  shelf_1.  You  are  at  shelf_1  and  holding  mug_1.obs_35:  Act:  agent.heat_with(’mug_1’,  ’microwave_1’).  Obs:  Nothing  happens.  You  are  at  shelf_1  and  holding  mug_1.Execution  error:Error
    in [Step 4]: I cannot heat mug_1 with microwave_1.Current  state:  You  are  at  shelf_1  and  holding  mug_1.Planner’s
    replanning:###  Understanding  of  the  current  observations:The  agent  successfully  found  a  mug  on  shelf_1  and  took  it.  However,  when  attempting  to  heat  the  mug  with  the  microwave,  the  action  failed  with  "Nothing  happens."  This  indicates  that  the  agent  tried  to  use  the  microwave  while  not  being  at  the  microwave’s  location.  The  agent  must  go  to  the  microwave  before  attempting  to  heat  the  mug.###  Mistakes  in  the  previous  code  and  the  reason  for  errors:The  error  occurred  because  the  agent  attempted  to  heat  the  mug  without  being  at  the  microwave’s  location.  The  precondition  for  heating  an  object  with  a  receptacle  is  that  the  agent  must  be  at  the  same  location  as  the  receptacle.###  Rules  to  consider:-  **rule_0**:  Ensure  all  receptacles  and  surfaces  are  considered  when  searching  for  the  mug.-  **rule_1**:  Use  the  ’find_object’  method  to  search  for  the  mug.-  **rule_2**:  After  finding  the  mug,  heat  it  and  then  put  it  in  a  cabinet.-  **rule_3**:  Open  closed  receptacles  to  search  for  objects.###  Plan  for  the  task:1.  Since  the  mug  has  already  been  found  and  is  being  held  by  the  agent,  proceed  to  the  microwave.2.  Open  the  microwave  if  it  is  closed.3.  Heat  the  mug  with  the  microwave.4.  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in.5.  Place  the  mug  in  the  cabinet.Here  is  the  updated  code  for  the  task:‘‘‘python#  [Step  4]  Go  to  the  microwave  and  open  it  if  it’s  closedobservation  =  agent.go_to(’microwave_1’)if  ’closed’  in  observation:observation  =  agent.open(’microwave_1’)#  Assert  that  the  agent  is  at  the  microwave  and  it’s  openassert  ’microwave_1’  in  observation  and  ’closed’  not  in  observation,  f’Error  in  [Step  4]:  Microwave  is  not  open  or  not  at  microwave_1.’#  [Step  5]  Heat  the  mug  with  the  microwaveobservation  =  agent.heat_with(’mug_1’,  ’microwave_1’)#  Assert  that  the  mug  has  been  heatedassert  ’heat’  in  observation  and  ’mug_1’  in  observation,  f’Error  in  [Step  5]:  I  cannot  heat  mug_1  with  microwave_1.’#  [Step  6]  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in#  Since  the  task  does  not  specify  which  cabinet  to  use,  we  can  choose  the  first  one  that  is  empty  or  available#  We  already  know  that  cabinets  1  through  10  are  empty  or  contain  other  items,  so  we  can  choose  cabinet_1  for  simplicityobservation  =  agent.go_to(’cabinet_1’)if  ’closed’  in  observation:observation  =  agent.open(’cabinet_1’)#  Assert  that  the  agent  is  at  cabinet_1  and  it’s  openassert  ’cabinet_1’  in  observation  and  ’closed’  not  in  observation,  f’Error  in  [Step  6]:  Cabinet_1  is  not  open  or  not  at  cabinet_1.’#  [Step  7]  Place  the  mug  in  the  cabinetobservation  =  agent.put_in_or_on(’mug_1’,  ’cabinet_1’)#  Assert  that  the  mug  is  put  in  the  cabinetassert  ’put’  in  observation  and  ’mug_1’  in  observation,  f’Error  in  [Step  7]:  I  cannot  put  mug_1  in  cabinet_1.’‘‘‘Feedback:obs_36:  Act:  agent.go_to(’microwave_1’).  Obs:  In  microwave_1,  you  see  egg_2.  You  are  at  microwave_1  and  holding  mug_1.obs_37:  Act:  agent.heat_with(’mug_1’,  ’microwave_1’).  Obs:  You  heat  mug_1  using  microwave_1.  You  are  at  microwave_1  and  holding  mug_1.obs_38:  Act:  agent.go_to(’cabinet_1’).  Obs:  In  cabinet_1,  you  see  nothing.  You  are  at  cabinet_1  and  holding  mug_1.obs_39:  Act:  agent.put_in_or_on(’mug_1’,  ’cabinet_1’).  Obs:  You  put  mug_1  in/on  cabinet_1.  You  are  at  cabinet_1  and  holding  nothing.  This  epoch  is  done.  Succeed:  TrueCurrent  state:  You  are  at  cabinet_1  and  holding  nothing.Planner’s
    conclusion:###  Summary  of  misunderstandings  and  mistakes:Initially,  I  attempted  to  heat  the  mug  without  being  at  the  microwave’s  location,  which  is  why  the  action  failed  with  "Nothing  happens."  The  correct  procedure  is  to  first  go  to  the  microwave  before  attempting  to  heat  any  object.  After  correcting  this  mistake,  the  agent  successfully  heated  the  mug  and  placed  it  in  cabinet_1.###  Organized  code  block:‘‘‘python#  [Helper  Method]  Define  the  helper  method  ’find_object’  as  provided  in  rule_1def  find_object(agent,  recep_to_check,  object_name):for  receptacle  in  recep_to_check:observation  =  agent.go_to(receptacle)#  Check  if  we  need  to  open  the  receptacle.  If  we  do,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Check  if  the  object  is  in/on  the  receptacle.if  object_name  in  observation:object_ids  =  get_object_with_id(observation,  object_name)return  object_ids,  receptaclereturn  None,  None#  [Step  1]  Get  a  sorted  list  of  all  receptacles  and  surfaces  to  check  for  a  mugrecep_to_check  =  [’cabinet_1’,  ’cabinet_2’,  ’cabinet_3’,  ’cabinet_4’,  ’cabinet_5’,  ’cabinet_6’,  ’cabinet_7’,  ’cabinet_8’,  ’cabinet_9’,  ’cabinet_10’,  ’cabinet_11’,  ’cabinet_12’,  ’cabinet_13’,  ’countertop_1’,  ’diningtable_1’,  ’drawer_1’,  ’fridge_1’,  ’garbagecan_1’,  ’microwave_1’,  ’shelf_1’,  ’shelf_2’,  ’shelf_3’,  ’sinkbasin_1’,  ’stoveburner_1’,  ’stoveburner_2’,  ’stoveburner_3’,  ’stoveburner_4’,  ’toaster_1’,  ’coffeemachine_1’]object_ids,  receptacle_with_mug  =  find_object(agent,  recep_to_check,  ’mug’)#  Assert  that  we  have  found  a  mugassert  object_ids  is  not  None,  f’Error  in  [Step  1]:  There  is  no  mug  in/on  {recep_to_check}.’#  [Step  2]  Take  the  mugfound_mug  =  object_ids[0]observation  =  agent.take_from(found_mug,  receptacle_with_mug)#  Assert  that  the  agent  is  holding  the  mugassert  agent.holding  ==  found_mug,  f’Error  in  [Step  2]:  I  cannot  take  {found_mug}  from  {receptacle_with_mug}.’#  [Step  3]  Go  to  the  microwave  and  open  it  if  it’s  closedobservation  =  agent.go_to(’microwave_1’)if  ’closed’  in  observation:observation  =  agent.open(’microwave_1’)#  Assert  that  the  agent  is  at  the  microwave  and  it’s  openassert  ’microwave_1’  in  observation  and  ’closed’  not  in  observation,  f’Error  in  [Step  3]:  Microwave  is  not  open  or  not  at  microwave_1.’#  [Step  4]  Heat  the  mug  with  the  microwaveobservation  =  agent.heat_with(found_mug,  ’microwave_1’)#  Assert  that  the  mug  has  been  heatedassert  ’heat’  in  observation  and  found_mug  in  observation,  f’Error  in  [Step  4]:  I  cannot  heat  {found_mug}  with  microwave_1.’#  [Step  5]  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in#  Since  the  task  does  not  specify  which  cabinet  to  use,  we  can  choose  the  first  one  that  is  empty  or  available#  We  already  know  that  cabinets  1  through  10  are  empty  or  contain  other  items,  so  we  can  choose  cabinet_1  for  simplicityobservation  =  agent.go_to(’cabinet_1’)if  ’closed’  in  observation:observation  =  agent.open(’cabinet_1’)#  Assert  that  the  agent  is  at  cabinet_1  and  it’s  openassert  ’cabinet_1’  in  observation  and  ’closed’  not  in  observation,  f’Error  in  [Step  5]:  Cabinet_1  is  not  open  or  not  at  cabinet_1.’#  [Step  6]  Place  the  mug  in  the  cabinetobservation  =  agent.put_in_or_on(found_mug,  ’cabinet_1’)#  Assert  that  the  mug  is  put  in  the  cabinetassert  ’put’  in  observation  and  found_mug  in  observation,  f’Error  in  [Step  6]:  I  cannot  put  {found_mug}  in  cabinet_1.’‘‘‘Builder’s
    classification:Analyzing  the  scenario,  we  can  identify  the  root  cause  of  the  observed  mistakes  and  relate  them  to  the  existing  rules:1.  The  agent  initially  attempted  to  heat  the  mug  without  being  at  the  microwave’s  location,  which  resulted  in  the  action  "Nothing  happens."  This  mistake  was  due  to  the  agent  not  following  the  precondition  for  the  ‘heat_with‘  action,  which  requires  the  agent  to  be  at  the  same  location  as  the  receptacle  (in  this  case,  the  microwave).2.  The  existing  rules  related  to  the  mistakes  are:-  **rule_1**:  This  rule  provides  a  ’find_object’  method  to  search  for  objects,  which  the  agent  used  correctly  to  find  the  mug.-  **rule_2**:  This  ’Success  Process’  rule  outlines  the  steps  to  put  an  object  in  a  receptacle  after  finding  and  manipulating  it.  However,  it  does  not  explicitly  state  that  the  agent  must  be  at  the  location  of  the  appliance  to  use  it,  which  is  relevant  for  the  heating  step.-  **rule_3**:  This  rule  reminds  the  agent  to  open  closed  receptacles  to  search  for  objects,  which  the  agent  did  correctly.3.  There  is  a  ’Success  Process’  rule  applicable  to  this  type  of  task  (rule_2),  but  it  does  not  explicitly  cover  the  precondition  for  heating  an  object,  which  is  to  be  at  the  same  location  as  the  appliance.4.  The
    fault in this scenario can be attributed to:*Imperfect Rules*: The existing ’Success
    Process’ rule (rule_2) does not fully document theprecondition for using appliances,
    such as the microwave. It does not state that the agent must be atthe location
    of the appliance to use it for heating an object.Based  on  the  analysis,  the  conclusion  is  that  the  fault  stems  from  *Imperfect  Rules*.  The  rules  need  to  be  updated  to  include  the  precondition  for  using  appliances,  which  is  a  critical  step  in  the  ’Success  Process’  for  tasks  involving  heating  objects.Builder:Potential  Rules:1.  *Corrected  Error*:  When  attempting  to  use  an  appliance  (e.g.,  microwave),  the  agent  must  be  at  the  same  location  as  the  appliance.  This  rule  is  related  to  the  existing  ’Success  Process’  rule  (rule_2),  but  it  adds  a  specific  precondition  for  using  appliances  that  was  not  previously  documented.2.  *Success  Process*:  The  existing  ’Success  Process’  rule  (rule_2)  should  be  updated  to  include  the  precondition  of  being  at  the  location  of  the  appliance  before  using  it.  This  is  a  critical  step  in  tasks  involving  heating  objects.Check  Difference:-  The  potential  ’Corrected  Error’  rule  targets  the  specific  precondition  for  using  appliances,  which  is  a  new  addition  and  not  covered  by  existing  rules.-  The  update  to  the  ’Success  Process’  rule  expands  on  the  existing  steps  to  include  the  precondition  for  using  appliances,  which  is  a  refinement  of  the  existing  rule.Check  Existing  Rules:-  Rule_2  needs  to  be  updated  to  include  the  precondition  for  using  appliances,  as  this  was  the  key  oversight  that  led  to  the  agent’s  initial  error.Codes:‘‘‘python#
    Update the existing ’Success Process’ rule to include the precondition for using
    appliancesrule_system.update_rule(rule_id=’rule_2’,rule="If  the  task  is  to  heat  an  object  and  put  it  in  a  receptacle,  the  agent  must  first  find  the  object,  ensure  they  are  at  the  location  of  the  heating  appliance  before  using  it,  and  then  place  the  object  in  the  target  receptacle.",example="""#  For  example,  to  heat  a  mug  and  put  it  in  a  cabinet:#  [Step  1]  Use  ’find_object’  method  to  search  all  receptacles#  [Step  2]  Take  the  mug#  [Step  3]  Go  to  the  heating  appliance  (e.g.,  microwave)  and  ensure  you  are  at  its  location#  [Step  4]  Heat  the  mug  with  the  appliance#  [Step  5]  Find  an  empty  cabinet  or  any  cabinet  to  place  the  heated  mug  in#  [Step  6]  Place  the  mug  in  the  cabinet""",validation_record="Updated  to  include  the  precondition  for  using  appliances  based  on  the  error  and  subsequent  success  in  epoch_1.")#
    Write a new ’Corrected Error’ rule for the precondition of using appliancesrule_system.write_rule(rule="Before  using  an  appliance  (e.g.,  microwave),  the  agent  must  go  to  the  location  of  the  appliance.",type="Corrected  Error",example="""#  Corrective  code:#  Go  to  the  microwave  before  attempting  to  heat  the  mugobservation  =  agent.go_to(’microwave_1’)#  Heat  the  mug  with  the  microwaveobservation  =  agent.heat_with(’mug_1’,  ’microwave_1’)""",validation_record="Derived  from  the  error  and  correction  in  epoch_1  where  the  agent  initially  failed  to  heat  the  mug  due  to  not  being  at  the  microwave’s  location.")rule_system.stop_generating()‘‘‘'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Prompts and Examples
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 提示与示例
- en: H.1 Prompts for Planner Agent
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 规划员代理的提示
- en: 'Listing 3: System Prompts for Planner Agent'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 3: 规划员代理的系统提示'
- en: '[⬇](data:text/plain;base64,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)[Role]You  are  an  excellent  household  agent  in  a  simulated  environment.  Given  a  household  environment  and  a  task,  you  write  Python  code  to  accomplish  the  task.[Actions]For  each  task,  an  agent  is  created  in  an  environment,  and  the  initial  observation  and  global  memory  are  printed.#  The  agent  is  an  instance  of  Agent  class,  which  includes  the  state  of  the  agent  (its  location,  what  it’s  holding)  and  the  actions  it  can  take.class  Agent:def  __init__(self,  env:  InteractiveEnvEngine):self.location  =  env.agent_locationself.holding  =  "nothing"...#  Here  are  the  admissible  actions  the  agent  can  take;  all  action  functions  return  an  observation  string  of  the  result  of  the  action.  If  the  precondition  of  the  action  is  not  met,  its  observation  will  include  "Nothing  happens".#  Go  to  a  receptacle  and  update  the  agent’s  location.#  For  example,  ’On  countertop_1,  you  see  candle_1,  cloth_2,  and  soapbar_1.’  =  go_to(’countertop_1’)#  For  example,  ’On  sidetable_2,  you  see  nothing.’  =  go_to(’sidetable_2’)def  go_to(self,  receptacle):...#  Open  a  receptacle  and  observe  its  contents.#  For  example,  ’You  open  cabinet_1.  In  cabinet_1,  you  see  cloth_1.’  =  open(’cabinet_1’)def  open(self,  receptacle):...#  Close  an  opened  receptacle.#  For  example,  ’You  close  cabinet_1.’  =  close(’cabinet_1’)def  close(self,  receptacle):...#  Take  an  object  from  a  receptacle  if  the  agent  is  not  holding  anything.#  For  example,  ’You  take  soapbar_1  from  towelholder_1.’  =  take_from(’soapbar_1’,  ’towelholder_1’)def  take_from(self,  object,  receptacle):...#  Put  an  object  in  or  on  a  receptacle  if  the  agent  is  holding  it.#  For  example,  ’You  put  soapbar_1  in/on  cabinet_1.’  =  put_in_or_on(’soapbar_1’,  ’cabinet_1’)def  put_in_or_on(self,  object,  receptacle):...#  Use  a  lamp.#  For  example,  ’You  turn  on  desklamp_1.’  =  use(’desklamp_1’)def  use(self,  object):...#  Clean  an  object  with  a  receptacle.#  For  example,  ’You  clean  soapbar_1  using  sinkbasin_1.’  =  clean_with(’soapbar_1’,  ’sinkbasin_1’)def  clean_with(self,  object,  receptacle):...#  Heat  an  object  with  a  receptacle.#  For  example,  ’You  heat  tomato_1  using  microwave_1.’  =  heat_with(’tomato_1’,  ’microwave_1’)def  heat_with(self,  object,  receptacle):...#  Cool  an  object  with  a  receptacle.#  For  example,  ’You  cool  pan_2  using  fridge_1.’  =  cool_with(’pan_2’,  ’fridge_1’)def  cool_with(self,  object,  receptacle):...#  A  useful  function  that  you  can  use.#  Extract  a  list  of  object_ids  with  the  specified  object_name  from  the  observation.def  get_object_with_id(observation,  object_name):...[Response  Instructions]You  will  be  given  rules  discovered  by  colleagues,  summarizing  game  mechanisms  and  experiences  of  success  and  failure.  Use  these  rules  to  guide  your  coding  efforts.  Aim  to  understand  and  apply  the  principles  behind  the  examples  of  these  rules,  adapting  them  to  fit  your  specific  scenarios  within  the  simulated  environment.Output  Format  Instructions:1.  First,  explain  your  understanding  of  the  task  and  the  current  observations,  then  describe  rules  (with  their  IDs)  that  need  to  be  considered  and  plan  for  the  task.  Then,  write  your  code  between  "‘‘‘python"  and  "‘‘‘".  No  text  should  follow  after  the  code  block.2.  After  receiving  feedback,  you  should  also  explain  your  understanding  of  the  current  observations,  including  special  (unexpected)  formats  or  phenomena,  mistakes  in  your  previous  code,  and  the  reasons  for  errors.  Then,  describe  rules  (with  their  IDs)  that  need  to  be  considered,  plan  for  the  task,  and  then  write  code.Follow  these  instructions:1.  DO  NOT  USE  an  undefined  function  or  attribute  of  the  agent.  Your  code  must  be  directly  executable  in  the  given  environment.  I  won’t  implement  any  placeholders  in  your  code  for  you.2.  Your  code  should  be  consistent  with  the  code  examples  of  the  rules  (please  copy  the  code  if  there  is  no  better  modification!),  making  it  easier  for  the  builder  agent  to  refine  and  develop  new  rules  effectively.3.  In  your  code,  you  are  encouraged  to  define  helpful  functions,  which  should  be  general  and  reusable  in  different  scenes.  The  helper  methods  in  the  rules  are  already  defined  in  the  coding  environment,  so  you  can  directly  use  them.  If  you  don’t  need  to  modify  them,  don’t  redefine  them.4.  After  defining  helper  methods,  your  code  should  be  divided  into  steps  (marked  with  "[Step]"  in  comments)  and  write  assertions  at  the  end  of  each  step  to  ensure  that  each  step  achieves  its  subgoal.Follow  these  instructions.  Don’t  give  up  or  change  the  task.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,W1JvbGVdCllvdSBhcmUgYW4gZXhjZWxsZW50IGhvdXNlaG9sZCBhZ2VudCBpbiBhIHNpbXVsYXRlZCBlbnZpcm9ubWVudC4gR2l2ZW4gYSBob3VzZWhvbGQgZW52aXJvbm1lbnQgYW5kIGEgdGFzaywgeW91IHdyaXRlIFB5dGhvbiBjb2RlIHRvIGFjY29tcGxpc2ggdGhlIHRhc2suCgpbQWN0aW9uc10KRm9yIGVhY2ggdGFzaywgYW4gYWdlbnQgaXMgY3JlYXRlZCBpbiBhbiBlbnZpcm9ubWVudCwgYW5kIHRoZSBpbml0aWFsIG9ic2VydmF0aW9uIGFuZCBnbG9iYWwgbWVtb3J5IGFyZSBwcmludGVkLgoKIyBUaGUgYWdlbnQgaXMgYW4gaW5zdGFuY2Ugb2YgQWdlbnQgY2xhc3MsIHdoaWNoIGluY2x1ZGVzIHRoZSBzdGF0ZSBvZiB0aGUgYWdlbnQg(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'
- en: 'Listing 4: Conclusion Prompt for Indirect Success'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4：间接成功的结论提示
- en: '[⬇](data:text/plain;base64,UGxlYXNlIHN1bW1hcml6ZSB0aGUgbWlzdW5kZXJzdGFuZGluZ3MgYW5kIG1pc3Rha2VzIHlvdSBtYWRlLCBhbmQgdGhlbiBvcmdhbml6ZSB5b3VyIGNvZGUgaW50byBhIGNvZGUgYmxvY2suIFlvdSBzaG91bGQgY29weSB0aGUgdXNlZCBwYXJ0cyBmcm9tIHlvdXIgcHJldmlvdXMgY29kZSwgaW5jbHVkaW5nIGhlbHBlciBtZXRob2RzIGFuZCBzdGVwcy4gWW91IGNhbiBvbmx5IG1vZGlmeSB0aGUgcHJldmlvdXNseSB3cm9uZyBzdGVwLCBhbmQgbWFrZSBzdXJlIHlvdSBkb24ndCBtaXNzIGFueSBkZXRhaWwhIFNvIHRoYXQgSSBjYW4gdXNlIGl0IHdpdGggc2ltaWxhciBidXQgbm90IHRoZSBzYW1lIHNjZW5hcmlvcy4=)Please  summarize  the  misunderstandings  and  mistakes  you  made,  and  then  organize  your  code  into  a  code  block.  You  should  copy  the  used  parts  from  your  previous  code,  including  helper  methods  and  steps.  You  can  only  modify  the  previously  wrong  step,  and  make  sure  you  don’t  miss  any  detail!  So  that  I  can  use  it  with  similar  but  not  the  same  scenarios.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,UGxlYXNlIHN1bW1hcml6ZSB0aGUgbWlzdW5kZXJzdGFuZGluZ3MgYW5kIG1pc3Rha2VzIHlvdSBtYWRlLCBhbmQgdGhlbiBvcmdhbml6ZSB5b3VyIGNvZGUgaW50byBhIGNvZGUgYmxvY2suIFlvdSBzaG91bGQgY29weSB0aGUgdXNlZCBwYXJ0cyBmcm9tIHlvdXIgcHJldmlvdXMgY29kZSwgaW5jbHVkaW5nIGhlbHBlciBtZXRob2RzIGFuZCBzdGVwcy4gWW91IGNhbiBvbmx5IG1vZGlmeSB0aGUgcHJldmlvdXNseSB3cm9uZyBzdGVwLCBhbmQgbWFrZSBzdXJlIHlvdSBkb24ndCBtaXNzIGFueSBkZXRhaWwhIFNvIHRoYXQgSSBjYW4gdXNlIGl0IHdpdGggc2ltaWxhciBidXQgbm90IHRoZSBzYW1lIHNjZW5hcmlvcy4=)请总结你所犯的误解和错误，然后将你的代码整理成一个代码块。你应该复制之前代码中使用的部分，包括辅助方法和步骤。你只能修改之前错误的步骤，并确保不遗漏任何细节！这样我才能将其应用于类似但不完全相同的场景。'
- en: 'Listing 5: Conclusion Prompt for Failure'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5：失败的结论提示
- en: '[⬇](data:text/plain;base64,WW91IGZhaWxlZCB0byBjb21wbGV0ZSB0aGUgdGFzay4gTm93IGNhcmVmdWxseSByZXZpZXcgdGhlIHRyYWplY3Rvcnkgb2YgdGhlIGV4ZWN1dGVkIGFjdGlvbnMgYW5kIHRoZSBjb3JyZXNwb25kaW5nIG9ic2VydmF0aW9ucywgdGhlbiBpZGVudGlmeSB0aGUgcmVhc29ucyBmb3IgdGhlIGZhaWx1cmUuIFRoaXMgcmVhc29uIGlzIG9mdGVuIHlvdXIgbWlzdGFrZSBvciBtaXN1bmRlcnN0YW5kaW5nIHJhdGhlciB0aGFuIHRoZSBlbnZpcm9ubWVudCdzIGVycm9yLiBCeSBjYXJlZnVsbHkgY29tcGFyaW5nIHdpdGggYXBwbGljYWJsZSBydWxlcyBhbmQgZXhhbXBsZXMsIHBpbnBvaW50IHdoZXJlIHlvdXIgY29kZSBkZXZpYXRlZCBmcm9tIGV4cGVjdGVkIHN0YW5kYXJkcy4KCklmIHRoZXJlIGV4aXN0IGNvZGluZyBlcnJvcnMsIHNwZWNpZnkgdGhlIHNlZ21lbnRzIG9mIHRoZSBwcm9ibGVtYXRpYyBjb2RlIGFuZCBlbHVjaWRhdGUgaG93IHRoZXkgY29udHJpYnV0ZWQgdG8gdGhlIGVycm9ycy4gSWYgeW91IHdhbnQgdG8gd3JpdGUgYSBjb3JyZWN0aW9uIGZvciBhbiBlcnJvciwgeW91IG11c3QgZG91YmxlLWNoZWNrIGl0cyBwbGF1c2liaWxpdHkhIEFkZGl0aW9uYWxseSwgZGV0ZXJtaW5lIHdoZXRoZXIgeW91ciBjb2RlcyB3ZXJlIGluIHN0cmljdCBhZGhlcmVuY2UgdG8gdGhlIHJlbGV2YW50IHJ1bGVzIGFuZCBleGFtcGxlcy4gVGhpcyByZWZsZWN0aW9uIGFuZCBkb2N1bWVudGF0aW9uIHdpbGwgc2VydmUgYXMgYSByZW1pbmRlciBmb3IgY29tcGxldGluZyBmdXR1cmUgdGFza3Mu)You  failed  to  complete  the  task.  Now  carefully  review  the  trajectory  of  the  executed  actions  and  the  corresponding  observations,  then  identify  the  reasons  for  the  failure.  This  reason  is  often  your  mistake  or  misunderstanding  rather  than  the  environment’s  error.  By  carefully  comparing  with  applicable  rules  and  examples,  pinpoint  where  your  code  deviated  from  expected  standards.If  there  exist  coding  errors,  specify  the  segments  of  the  problematic  code  and  elucidate  how  they  contributed  to  the  errors.  If  you  want  to  write  a  correction  for  an  error,  you  must  double-check  its  plausibility!  Additionally,  determine  whether  your  codes  were  in  strict  adherence  to  the  relevant  rules  and  examples.  This  reflection  and  documentation  will  serve  as  a  reminder  for  completing  future  tasks.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91IGZhaWxlZCB0byBjb21wbGV0ZSB0aGUgdGFzay4gTm93IGNhcmVmdWx5IHJldmlld3QgdGhlIHRyYWplY3Rvcnkgb2YgdGhlIGV4ZWN1dGVkIGFjdGlvbnMgYW5kIHRoZSBjb3JyZXNwb25kaW5nIG9ic2VydmF0aW9ucywgdGhlbiBpZGVudGlmeSB0aGUgcmVhc29ucyBmb3IgdGhlIGZhaWx1cmUuIFRoaXMgcmVhc29uIGlzIG9mdGVuIHlvdXIgbWlzdGFrZSBvciBtaXN1bmRlcnN0YW5kaW5nIHJhdGhlciB0aGFuIHRoZSBlbnZpcm9ubWVudCdzIGVycm9yLiBCeSBjYXJlZnVsbHkgY29tcGFyaW5nIHdpdGggYXBwbGljYWJsZSBydWxlcyBhbmQgZXhhbXBsZXMsIHBpbnBvaW50IHdoZXJlIHlvdXIgY29kZSBkZXZpYXRlZCBmcm9tIGV4cGVjdGVkIHN0YW5kYXJkcy4KCklmIHRoZXJlIGV4aXN0IGNvZGluZyBlcnJvcnMsIHNwZWNpZnkgdGhlIHNlZ21lbnRzIG9mIHRoZSBwcm9ibGVtYXRpYyBjb2RlIGFuZCBlbHVjaWRhdGUgaG93IHRoZXkgY29udHJpYnV0ZWQgdG8gdGhlIGVycm9ycy4gSWYgeW91IHdhbnQgdG8gd3JpdGUgYSBjb3JyZWN0aW9uIGZvciBhbiBlcnJvciwgeW91IG11c3QgZG91YmxlLWNoZWNrIGl0cyBwbGF1c2liaWxpdHkhIEFkZGl0aW9uYWxseSwgZGV0ZXJtaW5lIHdoZXRoZXIgeW91ciBjb2RlcyB3ZXJlIGluIHN0cmljdCBhZGhlcmVuY2UgdG8gdGhlIHJlbGV2YW50IHJ1bGVzIGFuZCBleGFtcGxlcy4gVGhpcyByZWZsZWN0aW9uIGFuZCBkb2N1bWVudGF0aW9uIHdpbGwgc2VydmUgYXMgYSByZW1pbmRlciBmb3IgY29tcGxldGluZyBmdXR1cmUgdGFza3Mu)你未能完成任务。现在，请仔细回顾执行的操作轨迹及对应的观察结果，然后找出失败的原因。这个原因往往是你的错误或误解，而不是环境的错误。通过与适用的规则和示例仔细对比，找出你的代码偏离预期标准的地方。如果存在编码错误，请指出有问题的代码段，并说明它们如何导致错误。如果你想为错误编写修正，请务必再次检查其合理性！此外，确定你的代码是否严格遵循相关的规则和示例。这个反思和文档将作为完成未来任务的提醒。'
- en: H.2 Examples for ALFWorld and MiniWob++
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 ALFWorld 和 MiniWob++ 的示例
- en: 'Listing 6: The Put Example for ALFWorld'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6：ALFWorld 的 Put 示例
- en: '[⬇](data:text/plain;base64,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)You  are  in  the  middle  of  a  room.  Looking  quickly  around  you,  you  see  cabinet_4,  cabinet_3,  cabinet_2,  cabinet_1,  countertop_1,  garbagecan_1,  sinkbasin_2,  sinkbasin_1,  toilet_2,  toilet_1.Your  task  is  to:  find  some  spraybottle.###  Understanding  of  the  observation:  ...###  Rules  to  consider:  ...###  Plan  for  the  task:  I  need  to  get  a  list  of  receptacles,  and  go  to  search  each  receptacle  until  seeing  a  spraybottle.Here  is  a  code  of  my  plan:‘‘‘python#  Define  a  helper  method  to  search  receptacles  for  the  target  objectdef  find_object(agent,  recep_to_check,  object_name):for  receptacle  in  recep_to_check:observation  =  agent.go_to(receptacle)#  Check  if  we  need  to  open  the  receptacle.  If  we  do,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Check  if  the  object  is  in/on  the  receptacle.if  object_name  in  observation:object_ids  =  get_object_with_id(observation,  object_name)return  object_ids,  receptaclereturn  None,  None#  [Step  1]  Get  a  sorted  list  of  receptacles  and  surfaces  to  check  for  a  spraybottle.  And  use  ’find_object’  method  to  searchrecep_to_check  =  [’cabinet_1’,  ’cabinet_2’,  ’cabinet_3’,  ’cabinet_4’,  ’countertop_1’,  ’toilet_1’,  ’toilet_2’,  ’sinkbasin_1’,  ’sinkbasin_2’,  ’garbagecan_1’]object_ids,  receptacle_with_spraybottle  =  find_object(agent,  recep_to_check,  ’spraybottle’)assert  object_ids  is  not  None,  f’Error  in  [Step  1]:  There  is  no  spraybottle  in/on  {recep_to_check}.’#  [Step  2]  Take  the  spraybottlefound_spraybottle  =  object_ids[0]observation  =  agent.take_from(found_spraybottle,  receptacle_with_spraybottle)assert  agent.holding  ==  found_spraybottle,  f’Error  in  [Step  2]:  I  cannot  take  {found_spraybottle}  from  {receptacle}.’#  [Step  3]  Go  to  a  toilet  and  put  the  spraybottle  on  itobservation  =  agent.go_to(’toilet_1’)#  check  if  toilet_1  is  closed.  If  so,  open  it.if  ’closed’  in  observation:observation  =  agent.open(’toilet_1’)observation  =  agent.put_in_or_on(found_spraybottle,  ’toilet_1’)‘‘‘'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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
    and'
- en: 'Listing 7: Initial Rules for ALFWorld Induced from Put Example'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7：从示例推导出的ALFWorld初始规则
- en: '[⬇](data:text/plain;base64,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)"rule_0":{"rule":  "At  the  initial  observation  of  the  environment,  the  agent  can  only  observe  receptacles,  such  as  cabinet_1,  countertop_1.  The  agent  needs  to  go  to  the  receptacle  to  view  objects  in  or  on  it,  even  for  open  receptacles.","type":  "Special  Mechanism","example":  "","validation_record":  "Provided  by  User."},"rule_1":{"rule":  "If  there  are  multiple  receptacles  to  be  searched,  the  agent  can  write  and  use  the  ’find_object’  method  as  shown  in  the  example.","type":  "Useful  Helper  Method","example":  ’’’#  Define  helper  method  to  find  the  object  that  is  neededdef  find_object(agent,  recep_to_check,  object_name):for  receptacle  in  recep_to_check:observation  =  agent.go_to(receptacle)#  Check  if  we  need  to  open  the  receptacle.  If  we  do,  open  it.if  ’closed’  in  observation:observation  =  agent.open(receptacle)#  Check  if  the  object  is  in/on  the  receptacle.if  object_name  in  observation:object_ids  =  get_object_with_id(observation,  object_name)return  object_ids,  receptaclereturn  None,  None’’’,"validation_record":  "Essential  helper  method  provided  by  User."},"rule_2":{"rule":  "If  the  task  is  to  put  some  object  on  some  receptacle,  first  use  ’find_object’  method  in  rule_1  to  search  all  receptacles,  take  the  object,  then  go  to  the  target  receptacle  and  put  the  object.","type":  "Success  Process","example":  ’’’#  For  example,  to  put  some  spraybottle  on  toilet:#  [Step  1]  Use  ’find_object’  method  in  rule_1  to  search  all  receptaclesrecep_to_check  =  ...object_ids,  receptacle_with_spraybottle  =  find_object(agent,  recep_to_check,  ’spraybottle’)#  [Step  2]  Take  the  spraybottle#  [Step  3]  Go  to  a  toilet  and  put  the  spraybottle  on  it#  If  the  toilet  is  closed,  open  it  first.’’’,"validation_record":  "Provided  by  User."}'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,InJ1bGVfMCI6CnsicnVsZSI6ICJBdCB0aGUgaW5pdGlhbCBvYnNlcnZhdGlvbiBvZiB0aGUgZW52aXJvbm1lbnQsIHRoZSBhZ2VudCBjYW4gb25seSBvYnNlcnZlIHJlY2VwdGFjbGVzLCBzdWNoIGFzIGNhYmluZXRfMSwgY291bnRlcnRvcF8xLiBUaGUgYWdlbnQgbmVlZHMgdG8gZ28gdG8gdGhlIHJlY2VwdGFjbGUgdG8gdmlldyBvYmplY3RzIGluIG9yIG9uIGl0LCBldmVuIGZvciBvcGVuIHJlY2VwdGFjbGVzLiIsCiJ0eXBlIjogIlNwZWNpYWwgTWVjaGFuaXNtIiwKImV4YW1wbGUiOiAiIiwKInZhbGlkYXRpb25fcmVjb3JkIjogIlByb3ZpZGVkIGJ5IFVzZXIuIn0sCgoicnVsZV8xIjoKeyJydWxlIjogIklmIHRoZXJlIGFyZSBtdWx0aXBsZSByZWNlcHRhY2xlcyB0byBiZSBzZWFyY2hlZCwgdGhlIGFnZW50IGNhbiB3cml0ZSBhbmQgdXNlIHRoZSAnZmluZF9vYmplY3QnIG1ldGhvZCBhcyBzaG93biBpbiB0aGUgZXhhbXBsZS4iLAoidHlwZSI6ICJVc2VmdWwgSGVscGVyIE1ldGhvZCIsCiJleGFtcGxlIjogJycnCiAgICAjIERlZmluZSBoZWxwZXIgbWV0aG9kIHRvIGZpbmQgdGhlIG9iamVjdCB0aGF0IGlzIG5lZWRlZAogICAgZGVmZmluZV9vYmplY3QoYWdlbnQsIHJlY2VwX3RvX2NoZWNrLCBvYmplY3RfbmFtZSk6CiAgICAgICAgZm9yIHJlY2VwdGFjbGUgaW4gcmVjZXBfdG9fY2hlY2s6CiAgICAgICAgICAgIG9ic2VydmF0aW9uID0gYWdlbnQuZ28udG8ocmVjZXB0YWNsZSkKICAgICAgICAgICAgaWYgJ2Nsb3NlZCcgaW4gb2JzZXJ2YXRpb246CiAgICAgICAgICAgICAgICBvYmplY3RfbmFtZSBpbiBvYmplY3Rpb24uCiAgICAgICAgICAgICMgQ2hlY2sgaWYgd2UgbmVlZCB0byBvcGVuIHRoZSByZWNlcHRhY2xlLiBJZiB3ZSBkbywgb3BlbiBpdC4KICAgICAgICAgICAgaWYgJ2Nsb3NlZCcgaW4gb2JzZXJ2YXRpb246CiAgICAgICAgICAgICAgICBvYmplY3RfbmFtZSBpbiBvYmplY3Rpb24uCiAgICAgICAgICAgICMgQ2hlY2sgaWYgdGhlIG9iamVjdCBpcyBpbm9uL29uIHRoZSByZWNlcHRhY2xlLgogICAgICAgICAgICBpZiBvYmplY3RfbmFtZSBpbiBvYmplY3Rpb246CiAgICAgICAgICAgICAgICBvYmplY3RfaWRzID0gZ2V0X29iamVjdF93aXRoX2lkKG9ic2VydmF0aW9uLCBvYmplY3RfbmFtZSkKICAgICAgICAgICAgICByZXR1cm4gb2JqZWN0X2lkcywgcmVjZXB0YWNsZQogICAgICAgICAgICByZXR1cm4gTm9uZSwgTm9uZQonJycsCiJ2YWxpZGF0aW9uX3JlY29yZCI6ICJFc3NlbnRpYWwgaGVscGVyIG1ldGhvZCBwcm92aWRlZCBieSBVc2VyLiJ9LAoKInJ1bGVfMiI6CnsicnVsZSI6ICJJZiB0aGUgdGFzayBpcyB0byBwdXQgc29tZSBvYmplY3Qgb24gc29tZSByZWNlcHRhY2xlLCBmaXJzdCB1c2UgJ2ZpbmRfb2JqZWN0JyBtZXRob2QgaW4gcnVsZV8xIHRvIHNlYXJjaCBhbGwgcmVjZXB0YWNsZXMsIHRha2UgdGhlIG9iamVjdCwgdGhlbiBnbyB0byB0aGUgdGFyZ2V0IHJlY2VwdGFjbGUgYW5kIHB1dCB0aGUgb2JqZWN0LiIsCiJ0eXBlIjogIlN1Y2Nlc3MgUHJvY2VzcyIsCiJleGFtcGxlIjogJycnCiAgICAjIEZvciBleGFtcGxlLCB0byBwdXQgc29tZSBzcHJheWJvdHRsZSBvbiB0b2lsZXQ6CiAgICAjIFtTdGVwIDFdIFVzZSAnZmluZF9vYmplY3QnIG1ldGhvZCBpbiBydWxlXzEgdG8gc2VhcmNoIGFsbCByZWNlcHRhY2xlcwogICAgcmVjZXBfdG9fY2hlY2sgPSAuLi4KICAgIG9iamVjdF9pZHMsIHJlY2VwdGFjbGVfd2l0aF9zcHJheWJvdHRsZSA9IGZpbmRfb2JqZWN0KGFnZW50LCByZWNlcF90b19jaGVjaywgJ3NwcmF5Ym90dGxlJykKICAgICMgW1N0ZXAgMl0gVGFrZSB0aGUgc3ByYXlib3R0bGUgCiAgICAjIFtTdGVwIDNdIEdvIHRvIGEgdG9pbGV0ICYgcHV0IHRoZSBzcHJheWJvdGxlIG9uIGl0CiAgICAjIElmIHRoZSB0b2lsZXQgaXMgdGhlIG9iamVjdCBpcyBjbG9zZWQsIG9wZW4gaXQgZmlyc3QuCicnJywKInZhbGlkYXRpb25fcmVjb3JkIjogIlByb3ZpZGVkIGJ5IFVzZXIuIn0=)"rule_0":{"rule":  "在环境的初步观察中，智能体只能观察到容器，例如
    cabinet_1、countertop_1。智能体需要前往容器以查看容器内或容器上的物体，即使是开放的容器。","type":  "特殊机制","example":  "","validation_record":  "由用户提供。"},"rule_1":{"rule":  "如果有多个容器需要搜索，智能体可以使用示例中所示的''find_object''方法。","type":  "有用的辅助方法","example":  ’’’#
    定义辅助方法来查找所需的物体def find_object(agent, recep_to_check, object_name):for receptacle
    in recep_to_check:observation = agent.go_to(receptacle)# 检查是否需要打开容器。如果需要，先打开它。如果容器是关闭的，打开它observation
    = agent.open(receptacle)# 检查物体是否在容器内或上面。如果物体在容器内或上面，就获取其ID。如果对象在容器中，则返回对象ID和容器。return
    object_ids, receptacleend’’’","validation_record":  "由用户提供的必要辅助方法。"},"rule_2":{"rule":  "如果任务是将物体放到容器上，首先使用规则1中的''find_object''方法搜索所有容器，取到物体，然后前往目标容器并将物体放置。","type":  "成功过程","example":  ’’’#
    例如，将喷雾瓶放到厕所上'
- en: 'For MineWob++, the Planner is allowed to complete the task in steps: initially
    output segments of planned steps and pause to observe their outcomes to inform
    subsequent steps.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MineWob++，规划器被允许分步骤完成任务：最初输出计划步骤的片段，并暂停以观察其结果，从而为后续步骤提供信息。
- en: 'Listing 8: The search-engine Example for MineWob++'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8：MineWob++ 的搜索引擎示例
- en: '[⬇](data:text/plain;base64,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)Your  task  is  to  Use  the  textbox  to  enter  "Jerald"  and  press  "Search",  then  find  and  click  the  4th  search  result....(HTML)...###  Understanding  of  the  observation:  ...###  Rules  to  consider:  ...###  Plan  for  the  task:  I  need  to  click  <input>  textbox  and  type  "Jerald".  Then  I  need  to  click  "Search"  button  and  get  the  searching  result.  Finally,  navigate  to  and  click  on  the  4th  search  result.Here  is  a  code  of  my  plan:‘‘‘python#  [Step  1]  Find  <input>  textbox  and  type  "Jerald".textbox_xpath  =  "//*[@id=’search-text’]"agent.click_xpath(textbox_xpath)agent.type("Jerald")#  [Step  2]  Click  the  search  buttonsearch_button_xpath  =  "//*[@id=’search’]"html_string  =  agent.click_xpath(search_button_xpath)#  *Pause  here*,  waiting  for  the  search  to  execute  and  for  the  results  to  be  displayed.‘‘‘###  Feedbacks  ###...(some  feedbacks)......(resulting  HTML)...###  Understanding  of  the  observation:  ...  Because  one  page  only  displays  3  search  results,  I  need  to  turn  to  the  next  page  for  the  4th  search  result.###  Rules  to  consider:  ...###  Plan  for  the  task:  ...‘‘‘python#  Define  a  helper  method  to  turn  to  the  next  page.def  turn_to_next_page(agent):next_page_xpath  =  f"//*[@id=’pagination’]/li[@class=’page-item  next’]"html_string  =  agent.click_xpath(next_page_xpath)return  html_string#  [Step  3]  Use  ’turn_to_next_page’  method  to  turn  to  the  next  page  for  the  4th  search  result  and  click  the  resulthtml_string  =  turn_to_next_page(agent)#  data-result  start  from  0result_xpath  =  f"//*[@id=’page-content’]//a[@data-result=’3’]"agent.click_xpath(result_xpath)‘‘‘'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)'
- en: 'Listing 9: Initial Rules for MineWob++ Induced from search-engine Example'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9：从搜索引擎示例中导出的 MineWob++ 初始规则
- en: '[⬇](data:text/plain;base64,J3J1bGVfMCc6CnsicnVsZSI6ICJJZiB0aGUgaWR4IG9mIHRoZSB0YXJnZXQgc2VhcmNoIHJlc3VsdCBleGNlZWRzIDMgKHRoZSBudW1iZXIgb2YgcmVzdWx0cyBwZXIgcGFnZSksIHVzZSAndHVybl90b19uZXh0X3BhZ2UnIG1ldGhvZCBhcyBzaG93biBpbiB0aGUgZXhhbXBsZS4iLAoidHlwZSI6ICJVc2VmdWwgSGVscGVyIE1ldGhvZCIsCiJleGFtcGxlIjogJycnCiAgICAjIFR1cm4gdG8gdGhlIG5leHQgcGFnZS4KICAgIGRlZiB0dXJuX3RvX25leHRfcGFnZShhZ2VudCk6CiAgICAgIG5leHRfcGFnZV94cGF0aCA9IGYiLy8qW0BpZD0ncGFnaW5hdGlvbiddL2xpW0BjbGFzcz0ncGFnZS1pdGVtIG5leHQnXSIKICAgICAgaHRtbF9zdHJpbmcgPSBhZ2VudC5jbGlja194cGF0aChuZXh0X3BhZ2VfeHBhdGgpCiAgICAgIHJldHVybiBodG1sX3N0cmluZwonJycsCiJ2YWxpZGF0aW9uX3JlY29yZCI6ICJFc3NlbnRpYWwgaGVscGVyIG1ldGhvZCBwcm92aWRlZCBieSBVc2VyLiJ9LAoKJ3J1bGVfMSc6IHsKInJ1bGUiOiAiSWYgdGhlIHRhc2sgbmVlZHMgdG8gdXNlIHRoZSBzZWFyY2ggZW5naW5lLCBmaXJzdCBjbGljayA8aW5wdXQ+IHRleHRib3gsIHR5cGUgdGhlIHRhcmdldCBzdHJpbmcsIGFuZCB0aGVuIGNsaWNrIFwiU2VhcmNoXCIgYnV0dG9uLiBOZXh0LCB1c2UgJ3R1cm5fdG9fbmV4dF9wYWdlJyBpbiBydWxlXzEgdG8gdHVybiB0aGUgcGFnZSBpZiB0aGUgdGFyZ2V0IGlkeCBleGNlZWQgMyBhbmQgY2xpY2sgdGhlIHRhcmdldCByZXN1bHQuIiwgInR5cGUiOiAiU3VjY2VzcyBQcm9jZXNzIiwKImV4YW1wbGUiOiAnJycKICAgICMgRm9yIGV4YW1wbGUsIHRvIHNlYXJjaCBKZXJhbGQgYW5kIGNsaWNrIHRoZSA0dGggcmVzdWx0OgogICAgIyBbU3RlcCAxXSBGaW5kIDxpbnB1dD4gdGV4dGJveCBhbmQgdHlwZSAiSmVyYWxkIi4KICAgICMgW1N0ZXAgMl0gQ2xpY2sgdGhlIHNlYXJjaCBidXR0b24uCiAgICAjIFtTdGVwIDNdIFVzZSAndHVybl90b19uZXh0X3BhZ2UnIG1ldGhvZCBpbiBydWxlXzEgdG8gdHVybiB0aGUgcGFnZSBhbmQgY2xpY2sgdGhlIHRhcmdldCByZXN1bHQuCicnJywKInZhbGlkYXRpb25fcmVjb3JkIjogIlByb3ZpZGVkIGJ5IFVzZXIuIn0=)’rule_0’:{"rule":  "If  the  idx  of  the  target  search  result  exceeds  3  (the  number  of  results  per  page),  use  ’turn_to_next_page’  method  as  shown  in  the  example.","type":  "Useful  Helper  Method","example":  ’’’#  Turn  to  the  next  page.def  turn_to_next_page(agent):next_page_xpath  =  f"//*[@id=’pagination’]/li[@class=’page-item  next’]"html_string  =  agent.click_xpath(next_page_xpath)return  html_string’’’,"validation_record":  "Essential  helper  method  provided  by  User."},’rule_1’:  {"rule":  "If  the  task  needs  to  use  the  search  engine,  first  click  <input>  textbox,  type  the  target  string,  and  then  click  \"Search\"  button.  Next,  use  ’turn_to_next_page’  in  rule_1  to  turn  the  page  if  the  target  idx  exceed  3  and  click  the  target  result.",  "type":  "Success  Process","example":  ’’’#  For  example,  to  search  Jerald  and  click  the  4th  result:#  [Step  1]  Find  <input>  textbox  and  type  "Jerald".#  [Step  2]  Click  the  search  button.#  [Step  3]  Use  ’turn_to_next_page’  method  in  rule_1  to  turn  the  page  and  click  the  target  result.’’’,"validation_record":  "Provided  by  User."}'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,J3J1bGVfMCc6CnsicnVsZSI6ICJJZiB0aGUgaWR4IG9mIHRoZSB0YXJnZXQgc2VhcmNoIHJlc3VsdCBleGNlZWRzIDMgKHRoZSBudW1iZXIgb2YgcmVzdWx0cyBwZXIgcGFnZSksIHVzZSAndHVybl90b19uZXh0X3BhZ2UnIG1ldGhvZCBhcyBzaG93biBpbiB0aGUgZXhhbXBsZS4iLAoidHlwZSI6ICJVc2VmdWwgSGVscGVyIE1ldGhvZCIsCiJleGFtcGxlIjogJycnCiAgICAjIFR1cm4gdG8gdGhlIG5leHQgcGFnZS4KICAgIGRlZiB0dXJuX3RvX25leHRfcGFnZShhZ2VudCk6CiAgICAgIG5leHRfcGFnZV94cGF0aCA9IGYiLy8qW0BpZD0ncGFnaW5hdGlvbiddL2xpW0BjbGFzcz0ncGFnZS1pdGVtIG5leHQnXSIKICAgICAgaHRtbF9zdHJpbmcgPSBhZ2VudC5jbGlja194cGF0aChuZXh0X3BhZ2VfeHBhdGgpCiAgICAgIHJldHVybiBodG1sX3N0cmluZwonJycsCiJ2YWxpZGF0aW9uX3JlY29yZCI6ICJFc3NlbnRpYWwgaGVscGVyIG1ldGhvZCBwcm92aWRlZCBieSBVc2VyLiJ9LAoKJ3J1bGVfMSc6IHsKInJ1bGUiOiAiSWYgdGhlIHRhc2sgbmVlZHMgdG8gdXNlIHRoZSBzZWFyY2ggZW5naW5lLCBmaXJzdCBjbGljayA8aW5wdXQ+IHRleHRib3gsIHR5cGUgdGhlIHRhcmdldCBzdHJpbmcsIGFuZCB0aGVuIGNsaWNrIFwiU2VhcmNoXCIgYnV0dG9uLiBOZXh0LCB1c2UgJ3R1cm5fdG9fbmV4dF9wYWdlJyBpbiBydWxlXzEgdG8gdHVybiB0aGUgcGFnZSBpZiB0aGUgdGFyZ2V0IGlkeCBleGNlZWQgMyBhbmQgY2xpY2sgdGhlIHRhcmdldCByZXN1bHQuIiwgInR5cGUiOiAiU3VjY2VzcyBQcm9jZXNzIiwKImV4YW1wbGUiOiAnJycKICAgICMgRm9yIGV4YW1wbGUsIHRvIHNlYXJjaCBKZXJhbGQgYW5kIGNsaWNrIHRoZSA0dGggcmVzdWx0OgogICAgIyBbU3RlcCAxXSBGaW5kIDxpbnB1dD4gdGV4dGJveCBhbmQgdHlwZSAiSmVyYWxkIi4KICAgICMgW1N0ZXAgMl0gQ2xpY2sgdGhlIHNlYXJjaCBidXR0b24uCiAgICAjIFtTdGVwIDNdIFVzZSAndHVybl90b19uZXh0X3BhZ2UnIG1ldGhvZCBpbiBydWxlXzEgdG8gdHVybiB0aGUgcGFnZSBhbmQgY2xpY2sgdGhlIHRhcmdldCByZXN1bHQuCicnJywKInZhbGlkYXRpb25fcmVjb3JkIjogIlByb3ZpZGVkIGJ5IFVzZXIuIn0=)’rule_0’:{"rule":  "If  the  idx  of  the  target  search  result  exceeds  3  (the  number  of  results  per  page),  use  ’turn_to_next_page’  method  as  shown  in  the
    example.","type":  "Useful  Helper  Method","example":  ’’’#  Turn  to  the  next  page.def  turn_to_next_page(agent):next_page_xpath  =  f"//*[@id=’pagination’]/li[@class=’page-item  next’]"html_string  =  agent.click_xpath(next_page_xpath)return  html_string’’’,"validation_record":  "Essential  helper  method  provided  by  User."},’rule_1’:  {"rule":  "If  the  task  needs  to  use  the  search  engine,  first  click  <input>  textbox,  type  the  target  string,  and  then  click  \"Search\"  button.  Next,  use  ’turn_to_next_page’  in  rule_1  to  turn  the  page  if  the  target  idx  exceed  3  and  click  the  target  result.",  "type":  "Success  Process","example":  ’’’#  For  example,  to  search  Jerald  and  click  the
    4th  result:#  [Step  1]  Find  <input>  textbox  and  type  "Jerald".#  [Step  2]  Click  the  search  button.#  [Step  3]  Use  ’turn_to_next_page’  method  in  rule_1  to  turn  the  page  and  click  the  target  result.’’’,"validation_record":  "Provided  by  User."}'
- en: 'Listing 10: The enter-text Example for MineWob++'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10：MineWob++ 的输入文本示例
- en: '[⬇](data:text/plain;base64,WW91ciB0YXNrIGlzIHRvIEVudGVyICJSb25kYSIgaW50byB0aGUgdGV4dCBmaWVsZCBhbmQgcHJlc3MgU3VibWl0LgouLi4oSFRNTCkuLi4KIyMjIE91dHB1dHMgIyMjCgojIyMgVW5kZXJzdGFuZGluZyBvZiB0aGUgb2JzZXJ2YXRpb246IC4uLgojIyMgUnVsZXMgdG8gY29uc2lkZXI6IC4uLgojIyMgUGxhbiBmb3IgdGhlIHRhc2s6IEkgc2hvdWxkIGNsaWNrIHRoZSBpbnB1dCBib3gsIHR5cGUgIlJvbmRhIiBhbmQgY2xpY2sgdGhlIHN1Ym1pdCBidXR0b24uCgpIZXJlIGlzIGEgY29kZSBvZiBteSBwbGFuOgpgYGBweXRob24KIyBbU3RlcCAxXSBDbGljayB0aGUgaW5wdXQgYm94CmlucHV0Ym94X3hwYXRoID0gIi8vaW5wdXRbQGlkPSd0dCddIgphZ2VudC5jbGlja194cGF0aChpbnB1dGJveF94cGF0aCkKCiMgW1N0ZXAgMl0gVHlwZSAiUm9uZGEiIGludG8gdGhlIGlucHV0IGJveAphZ2VudC50eXBlKCJSb25kYSIpCgojIFtTdGVwIDNdIENsaWNrIHRoZSBzdWJtaXQgYnV0dG9uCnN1Ym1pdF9idXR0b25feHBhdGggPSAiLy9idXR0b25bQGlkPSdzdWJidG4nXSIKYWdlbnQuY2xpY2tfeHBhdGgoc3VibWl0X2J1dHRvbl94cGF0aCkKYGBgCgojIyMgRmVlZGJhY2tzICMjIwouLi4oc29tZSBmZWVkYmFja3MpLi4uCm9ic18zOiBBY3Q6IGFnZW50LmNsaWNrX3hwYXRoKCIvL2J1dHRvbltAaWQ9J3N1YmJ0biddIikuIE9iczogQWN0aW9uIGRvbmUuIFRoZSBlcG9jaCBpcyBEb25lLiBTdWNjZWVkOiBUcnVlLg==)Your  task  is  to  Enter  "Ronda"  into  the  text  field  and  press  Submit....(HTML)...###  Outputs  ######  Understanding  of  the  observation:  ...###  Rules  to  consider:  ...###  Plan  for  the  task:  I  should  click  the  input  box,  type  "Ronda"  and  click  the  submit  button.Here  is  a  code  of  my  plan:‘‘‘python#  [Step  1]  Click  the  input  boxinputbox_xpath  =  "//input[@id=’tt’]"agent.click_xpath(inputbox_xpath)#  [Step  2]  Type  "Ronda"  into  the  input  boxagent.type("Ronda")#  [Step  3]  Click  the  submit  buttonsubmit_button_xpath  =  "//button[@id=’subbtn’]"agent.click_xpath(submit_button_xpath)‘‘‘###  Feedbacks  ###...(some  feedbacks)...obs_3:  Act:  agent.click_xpath("//button[@id=’subbtn’]").  Obs:  Action  done.  The  epoch  is  Done.  Succeed:  True.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,WW91ciB0YXNrIGlzIHRvIEVudGVyICJSb25kYSIgaW50byB0aGUgdGV4dCBmaWVsZCBhbmQgcHJlc3MgU3VibWl0LgouLi4oSFRNTCkuLi4KIyMjIE91dHB1dHMgIyMjCgojIyMgVW5kZXJzdGFuZGluZyBvZiB0aGUgb2JzZXJ2YXRpb246IC4uLgojIyMgUnVsZXMgdG8gY29uc2lkZXI6IC4uLgojIyMgUGxhbiBmb3IgdGhlIHRhc2s6IEkgc2hvdWxkIGNsaWNrIHRoZSBpbnB1dCBib3gsIHR5cGUgIlJvbmRhIiBhbmQgY2xpY2sgdGhlIHN1Ym1pdCBidXR0b24uCgpIZXJlIGlzIGEgY29kZSBvZiBteSBwbGFuOgpgYGBweXRob24KIyBbU3RlcCAxXSBDbGljayB0aGUgaW5wdXQgYm94CmlucHV0Ym94X3hwYXRoID0gIi8vaW5wdXRbQGlkPSd0dCddIgphZ2VudC5jbGlja194cGF0aChpbnB1dGJveF94cGF0aCkKCiMgW1N0ZXAgMl0gVHlwZSAiUm9uZGEiIGludG8gdGhlIGlucHV0IGJveAphZ2VudC50eXBlKCJSb25kYSIpCgojIFtTdGVwIDNdIENsaWNrIHRoZSBzdWJtaXQgYnV0dG9uCnN1Ym1pdF9idXR0b25feHBhdGggPSAiLy9idXR0b25bQGlkPSdzdWJidG4nXSIKYWdlbnQuY2xpY2tfeHBhdGgoc3VibWl0X2J1dHRvbl94cGF0aCkKYGBgCgojIyMgRmVlZGJhY2tzICMjIwouLi4oc29tZSBmZWVkYmFja3MpLi4uCm9ic18zOiBBY3Q6IGFnZW50LmNsaWNrX3hwYXRoKCIvL2J1dHRvbltAaWQ9J3N1YmJ0biddIikuIE9iczogQWN0aW9uIGRvbmUuIFRoZSBlcG9jaCBpcyBEb25lLiBTdWNjZWVkOiBUcnVlLg==)你的任务是将“Ronda”输入到文本框中并点击提交按钮……（HTML）……###
    输出 ###### 对观察结果的理解：……### 需要考虑的规则：……### 任务计划：我应该点击输入框，输入“Ronda”并点击提交按钮。以下是我的任务代码：‘’‘python#  [步骤
    1] 点击输入框inputbox_xpath = "//input[@id=''tt'']"agent.click_xpath(inputbox_xpath)#  [步骤
    2] 输入“Ronda”到输入框中agent.type("Ronda")#  [步骤 3] 点击提交按钮submit_button_xpath = "//button[@id=''subbtn'']"agent.click_xpath(submit_button_xpath)‘’‘###
    反馈 ###……（一些反馈）……obs_3: 采取行动：agent.click_xpath("//button[@id=''subbtn'']"). 观察：动作已完成。任务完成。成功：是的。'
- en: H.3 Prompts for Builder Agent
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.3 构建器代理的提示
- en: 'Listing 11: System Prompts for The Builder Agent'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11：构建器代理的系统提示
- en: '[⬇](data:text/plain;base64,W1JvbGVdCllvdSBhcmUgb2JzZXJ2aW5nIGEgaG91c2VrZWVwZXIgYWdlbnQgYXMgaXQgY29kZXMgYW5kIGFjdHMgd2l0aGluIGEgc2ltdWxhdGVkIGVudmlyb25tZW50IChnYW1lKS4gWW91ciBSb2xlIGlzIHRvIGNvbnN0cnVjdCBhIG1hbnVhbCBvZiBydWxlcyB0byBub3Qgb25seSBhc3Npc3QgdGhlIGFnZW50IGluIGNvbXBsZXRpbmcgdGFza3MgYnV0IGFsc28gdG8gZG8gc28gd2l0aCB0aGUgbGVhc3QgYW1vdW50IG9mIGF0dGVtcHRzL2Vycm9ycy4gVGhpcyByZXF1aXJlcyByZWNvcmRpbmcgYW5kIGFuYWx5emluZyB0aGUgZXhwZXJpZW5jZXMgb2YgdGhlIGFnZW50J3Mgc3VjY2Vzc2VzIGFuZCBmYWlsdXJlcywgYW5kIHVwZGF0aW5nIHByZXZpb3VzIGRpc2NvdmVyaWVzLgoKW0Z1bmN0aW9uc10KWW91IHdpbGwgYmUgcHJlc2VudGVkIHdpdGggdGhlIGN1cnJlbnQgZXBvY2gncyB0cmFqZWN0b3J5LiBUaGUgaW50ZXJhY3Rpb24gaW4gdGhlIHRyYWplY3RvcnkgaW5jbHVkZXMgdGhlIGFnZW50J3MgYW5hbHlzaXMsIGV4ZWN1dGlvbiBjb2RlLCBhbmQgdGhlIHJlc3VsdGluZyBmZWVkYmFjay4KCllvdSBzaG91bGQgdXNlIHRoZSBmb2xsb3dpbmcgbWV0aG9kcyBvZiBydWxlX3N5c3RlbSB0byBidWlsZCBhbmQgaW1wcm92ZSBydWxlcy4KCnJ1bGVfc3lzdGVtLndyaXRlX3J1bGUocnVsZSwgdHlwZT0iIiwgZXhhbXBsZT0iIiwgdmFsaWRhdGlvbl9yZWNvcmQ9IiIpCiMgV3JpdGUgZG93biBhIG5ldyBydWxlIG9mIHRoZSBnYW1lIHlvdSBkaXNjb3ZlcmVkLgojIFBhcmFtZXRlcnM6CiMgICBydWxlOiBhIHJ1bGUgb2YgdGhlIGdhbWUgeW91IGRpc2NvdmVyZWQuCiMgICB0eXBlOiB0aGUgdHlwZSBvZiB0aGUgcnVsZSwgY2hvc2VuIGZyb20gWyJTcGVjaWFsIFBoZW5vbWVuYS9NZWNoYW5pc20iLCAiQ29ycmVjdGVkIEVycm9yIiwgIlVucmVzb2x2ZWQgRXJyb3IiLCAiVXNlZnVsIEhlbHBlciBNZXRob2QiLCAiU3VjY2VzcyBQcm9jZXNzIl0uIFRoZSAiQ29ycmVjdGVkIEVycm9yIiBjYW4gaW5jbHVkZSBtaXN1bmRlcnN0YW5kaW5ncyBhbmQgbWlzdGFrZXMgdGhhdCBoYXZlIGJlZW4gY29ycmVjdGVkLgojICAgZXhhbXBsZTogYW4gZXhhbXBsZSAob3IgY29kZSkgZnJvbSB0aGUgdHJhamVjdG9yeSBkZW1vbnN0cmF0ZXMgdGhpcyBydWxlLiBZb3UgY2FuIGFkZCBkZXRhaWxlZCBpbmZvcm1hdGlvbiBpbiB0aGUgY29tbWVudC4KIyAgIHZhbGlkYXRpb25fcmVjb3JkOiB5b3VyIHZhbGlkYXRpb24gcmVjb3JkIG9uIHRoaXMgcnVsZSwgaW5jbHVkaW5nIHRoZSBlcG9jaCBJRHMgYW5kIHJ1bGUgSURzIGZyb20gd2hpY2ggdGhpcyBydWxlIGlzIGluZHVjZWQuCgpydWxlX3N5c3RlbS51cGRhdGVfcnVsZShydWxlX2lkLCBydWxlPSIiLCB0eXBlPSIiLCBleGFtcGxlPSIiLCB2YWxpZGF0aW9uX3JlY29yZD0iIiksCiMgUmV3cml0ZSB0aGUgYXR0cmlidXRlcyBvZiBhbiBleGlzdGluZyBydWxlIHdoZW4geW91IGNvbWUgdXAgd2l0aCBhIGJldHRlciB1bmRlcnN0YW5kaW5nLgojIElucHV0IG9ubHkgdGhlIGF0dHJpYnV0ZXMgeW91IHdhbnQgdG8gcmV3cml0ZS4KCnJ1bGVfc3lzdGVtLnN0b3BfZ2VuZXJhdGluZygpCiMgRGVzY3JpcHRpb246IHN0b3AgZ2VuZXJhdGluZyBydWxlcyBmcm9tIHRoZSBjdXJyZW50IGVwb2NoLgojIFVzZSBDYXNlOiBXaGVuIHlvdSBiZWxpZXZlIHRoYXQgdGhlIHRyYWplY3Rvcnkgb2YgdGhlIGN1cnJlbnQgZXBvY2ggaXMgbm90IG5lZWRlZCBvciBpbnN1ZmZpY2llbnQgdG8gZGVyaXZlIGFueSBtb3JlIG5ldyBydWxlcywgeW91IGNhbiBjYWxsIHRoaXMgZnVuY3Rpb24gYW5kIHdhaXQgZm9yIHRoZSBuZXh0IGVwb2NoJ3MgZGF0YS4gWW91IHNob3VsZCBhbHNvIGNhbGwgdGhpcyBmdW5jdGlvbiB3aGVuIHlvdSBoYXZlIHVwZGF0ZWQgYWxsIHJ1bGVzIGZvciB0aGUgY3VycmVudCBlcG9jaC4KCltBY3Rpb25zXQpBdCBlYWNoIGVwb2NoLCBhbiBhZ2VudCBpcyBjcmVhdGVkIGluIGFuIGVudmlyb25tZW50LCBhbmQgdGhlIGluaXRpYWwgb2JzZXJ2YXRpb24gYW5kIHRhcmdldCB0YXNrIGFyZSBwcmludGVkLiBUaGUgYWdlbnQgY2FuIG9ubHkgdXNlIHRoZSBmb2xsb3dpbmcgYWN0aW9uIGZ1bmN0aW9uczoKCmFnZW50LmdvX3RvKHJlY2VwdGFjbGUpICMgR28gdG8gYSByZWNlcHRhY2xlIGFuZCB1cGRhdGUgdGhlIGFnZW50J3MgbG9jYXRpb24uCmFnZW50Lm9wZW4ocmVjZXB0YWNsZSkgIyBPcGVuIGEgcmVjZXB0YWNsZSBhbmQgb2JzZXJ2ZSBpdHMgY29udGVudHMuCmFnZW50LmNsb3NlKHJlY2VwdGFjbGUpICMgQ2xvc2UgYSBvcGVuZWQgcmVjZXB0YWNsZS4KYWdlbnQudGFrZV9mcm9tKG9iamVjdCwgcmVjZXB0YWNsZSkgIyBUYWtlIGFuIG9iamVjdCBmcm9tIGEgcmVjZXB0YWNsZSBpZiB0aGUgYWdlbnQgaXMgbm90IGhvbGRpbmcgYW55dGhpbmcuCmFnZW50LnB1dF9pbl9vcl9vbihvYmplY3QsIHJlY2VwdGFjbGUpICMgUHV0IGFuIG9iamVjdCBpbiBvciBvbiBhIHJlY2VwdGFjbGUgaWYgdGhlIGFnZW50IGlzIGhvbGRpbmcgaXQuCmFnZW50LnVzZShvYmplY3QpICMgVXNlIGEgbGFtcC4KYWdlbnQuY2xlYW5fd2l0aChvYmplY3QsIHJlY2VwdGFjbGUpICMgQ2xlYW4gYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgphZ2VudC5oZWF0X3dpdGgob2JqZWN0LCByZWNlcHRhY2xlKSAjIEhlYXQgYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgphZ2VudC5jb29sX3dpdGgob2JqZWN0LCByZWNlcHRhY2xlKSAjIENvb2wgYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgpnZXRfb2JqZWN0X3dpdGhfaWQob2JzZXJ2YXRpb24sIG9iamVjdF9uYW1lKSAjIEV4dHJhY3RzIGEgbGlzdCBvZiBvYmplY3RfaWRzIHdpdGggdGhlIHNwZWNpZmllZCBvYmplY3RfbmFtZSBmcm9tIHRoZSBvYnNlcnZhdGlvbi4KCltSZXNwb25zZSBJbnN0cnVjdGlvbnNdClVwb24gcmVjZWl2aW5nIHRoZSBjdXJyZW50IHRyYWplY3RvcnksIHlvdSBzaG91bGQgZmlyc3QgaWRlbnRpZnkgdGhlIGNhc2Ugb2YgdGhlIGN1cnJlbnQgdHJhamVjdG9yeSdzIHJlc3VsdCBhbmQgdGhlbiBidWlsZCBydWxlcyBmb2xsb3dpbmcgdGhlIGNvcnJlc3BvbmRpbmcgaW5zdHJ1Y3Rpb25zLg==)[Role]You  are  observing  a  housekeeper  agent  as  it  codes  and  acts  within  a  simulated  environment  (game).  Your  Role  is  to  construct  a  manual  of  rules  to  not  only  assist  the  agent  in  completing  tasks  but  also  to  do  so  with  the  least  amount  of  attempts/errors.  This  requires  recording  and  analyzing  the  experiences  of  the  agent’s  successes  and  failures,  and  updating  previous  discoveries.[Functions]You  will  be  presented  with  the  current  epoch’s  trajectory.  The  interaction  in  the  trajectory  includes  the  agent’s  analysis,  execution  code,  and  the  resulting  feedback.You  should  use  the  following  methods  of  rule_system  to  build  and  improve  rules.rule_system.write_rule(rule,  type="",  example="",  validation_record="")#  Write  down  a  new  rule  of  the  game  you  discovered.#  Parameters:#  rule:  a  rule  of  the  game  you  discovered.#  type:  the  type  of  the  rule,  chosen  from  ["Special  Phenomena/Mechanism",  "Corrected  Error",  "Unresolved  Error",  "Useful  Helper  Method",  "Success  Process"].  The  "Corrected  Error"  can  include  misunderstandings  and  mistakes  that  have  been  corrected.#  example:  an  example  (or  code)  from  the  trajectory  demonstrates  this  rule.  You  can  add  detailed  information  in  the  comment.#  validation_record:  your  validation  record  on  this  rule,  including  the  epoch  IDs  and  rule  IDs  from  which  this  rule  is  induced.rule_system.update_rule(rule_id,  rule="",  type="",  example="",  validation_record=""),#  Rewrite  the  attributes  of  an  existing  rule  when  you  come  up  with  a  better  understanding.#  Input  only  the  attributes  you  want  to  rewrite.rule_system.stop_generating()#  Description:  stop  generating  rules  from  the  current  epoch.#  Use  Case:  When  you  believe  that  the  trajectory  of  the  current  epoch  is  not  needed  or  insufficient  to  derive  any  more  new  rules,  you  can  call  this  function  and  wait  for  the  next  epoch’s  data.  You  should  also  call  this  function  when  you  have  updated  all  rules  for  the  current  epoch.[Actions]At  each  epoch,  an  agent  is  created  in  an  environment,  and  the  initial  observation  and  target  task  are  printed.  The  agent  can  only  use  the  following  action  functions:agent.go_to(receptacle)  #  Go  to  a  receptacle  and  update  the  agent’s  location.agent.open(receptacle)  #  Open  a  receptacle  and  observe  its  contents.agent.close(receptacle)  #  Close  a  opened  receptacle.agent.take_from(object,  receptacle)  #  Take  an  object  from  a  receptacle  if  the  agent  is  not  holding  anything.agent.put_in_or_on(object,  receptacle)  #  Put  an  object  in  or  on  a  receptacle  if  the  agent  is  holding  it.agent.use(object)  #  Use  a  lamp.agent.clean_with(object,  receptacle)  #  Clean  an  object  with  a  receptacle.agent.heat_with(object,  receptacle)  #  Heat  an  object  with  a  receptacle.agent.cool_with(object,  receptacle)  #  Cool  an  object  with  a  receptacle.get_object_with_id(observation,  object_name)  #  Extracts  a  list  of  object_ids  with  the  specified  object_name  from  the  observation.[Response  Instructions]Upon  receiving  the  current  trajectory,  you  should  first  identify  the  case  of  the  current  trajectory’s  result  and  then  build  rules  following  the  corresponding  instructions.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: 'Listing 12: Case Classify Prompts for Builder'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 12: 构建器的案例分类提示'
- en: '[⬇](data:text/plain;base64,UGxlYXNlIGFuYWx5emUgdGhlIHNjZW5hcmlvIHRvIGlkZW50aWZ5IHRoZSByb290IGNhdXNlIG9mIHRoZSBvYnNlcnZlZCBtaXN0YWtlcyBhbmQgZGVzY3JpYmUgdGhlIGV4aXN0aW5nIHJ1bGVzIHJlbGF0ZWQgdG8gdGhlIG1pc3Rha2VzLiBUaGVuIGNoZWNrIHdoZXRoZXIgdGhlcmUgZXhpc3RzIGEgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSBhcHBsaWNhYmxlIHRvIHRoaXMgdHlwZSBvZiB0YXNrLiBGaW5hbGx5LCBkZXRlcm1pbmUgd2hldGhlciB0aGUgZmF1bHQgc3RlbXMgZnJvbToKLSAqSW1wZXJmZWN0IFJ1bGVzKjogdGhlIGFnZW50IGVuY291bnRlcnMgdW5leHBlY3RlZCBwaGVub21lbmEgdGhhdCBhcmUgbm90IGZ1bGx5IGRvY3VtZW50ZWQgaW4gdGhlIGN1cnJlbnQgcnVsZXMsIG9yIHRoZSBydWxlcyBoYXZlIG5vdCBpbmNsdWRlZCB0aGUgIlN1Y2Nlc3MgUHJvY2VzcyIgb2YgdGhpcyB0YXNrIHR5cGUuCi0gKkltcGVyZmVjdCBBZ2VudCo6IHRoZSBydWxlcyBmdWxseSBkb2N1bWVudCB0aGUgIlN1Y2Nlc3MgUHJvY2VzcyIgYW5kIGVycm9yIHJlbWluZGVycyBvZiBzdWNoIHNjZW5hcmlvcywgYnV0IHRoZSBhZ2VudCBmYWlscyB0byBmb2xsb3cgdGhlc2UgcnVsZXMgbWV0aWN1bG91c2x5LgpDb25zaWRlciBlYWNoIHN0ZXAgb2YgdGhlIHByb2Nlc3MgY2FyZWZ1bGx5IGFuZCBjb25jbHVkZSB3aXRoIGVpdGhlciAqSW1wZXJmZWN0IFJ1bGVzKiBvciAqSW1wZXJmZWN0IEFnZW50KiBiYXNlZCBvbiB5b3VyIGFuYWx5c2lzLg==)Please  analyze  the  scenario  to  identify  the  root  cause  of  the  observed  mistakes  and  describe  the  existing  rules  related  to  the  mistakes.  Then  check  whether  there  exists  a  "Success  Process"  rule  applicable  to  this  type  of  task.  Finally,  determine  whether  the  fault  stems  from:-  *Imperfect  Rules*:  the  agent  encounters  unexpected  phenomena  that  are  not  fully  documented  in  the  current  rules,  or  the  rules  have  not  included  the  "Success  Process"  of  this  task  type.-  *Imperfect  Agent*:  the  rules  fully  document  the  "Success  Process"  and  error  reminders  of  such  scenarios,  but  the  agent  fails  to  follow  these  rules  meticulously.Consider  each  step  of  the  process  carefully  and  conclude  with  either  *Imperfect  Rules*  or  *Imperfect  Agent*  based  on  your  analysis.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,UGxlYXNlIGFuYWx5emUgdGhlIHNjZW5hcmlvIHRvIGlkZW50aWZ5IHRoZSByb290IGNhdXNlIG9mIHRoZSBvYnNlcnZlZCBtaXN0YWtlcyBhbmQgZGVzY3JpYmUgdGhlIGV4aXN0aW5nIHJ1bGVzIHJlbGF0ZWQgdG8gdGhlIG1pc3Rha2VzLiBUaGVuIGNoZWNrIHdoZXRoZXIgdGhlcmUgZXhpc3RzIGEgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSBhcHBsaWNhYmxlIHRvIHRoaXMgdHlwZSBvZiB0YXNrLiBGaW5hbGx5LCBkZXRlcm1pbmUgd2hldGhlciB0aGUgZmF1bHQgc3RlbXMgZnJvbToKLSAqSW1wZXJmZWN0IFJ1bGVzKjogdGhlIGFnZW50IGVuY291bnRlcnMgdW5leHBlY3RlZCBwaGVub21lbmEgdGhhdCBhcmUgbm90IGZ1bGx5IGRvY3VtZW50ZWQgaW4gdGhlIGN1cnJlbnQgcnVsZXMsIG9yIHRoZSByZXLlcyBoYXZlIG5vdCBpbmNsdWRlZCB0aGUgIlN1Y2Nlc3MgUHJvY2VzcyIgb2YgdGhpcyB0YXNrIHR5cGUuCi0gKkltcGVyZmVjdCBBZ2VudCo6IHRoZSByZWFsZXMgZnVsbHkgZG9jdW1lbnQgdGhlICJTdWNjZXNzIFByb2Nlc3MiIGFuZCBhcmUgZXJyb3IgcmVtaW5kZXJzIG9mIHN1Y2ggc2NlbmFyaW9zLCBidXQgdGhlIGFnZW50IGZhaWxzIHRvIGZvbGxvdyB0aGVzZSBydWxlcyBtZXRpY3Vsb3VzbHkuCkNvbnNpZGVyIGVhY2ggc3RlcCBvZiB0aGUgcHJvY2VzcyBjYXJlZnVsbHkgYW5kIGNvbmNsdWRlIHdpdGggZWl0aGVyICpJbXBlcmZlY3QgUnVsZXMqIG9yICpJbXBlcmZlY3QgQWdlbnQqIGJhc2VkIG9uIHlvdXIgYW5hbHlzaXMu)
    请分析该场景，以识别观察到的错误的根本原因，并描述与这些错误相关的现有规则。然后，检查是否存在适用于此类任务的“成功流程”规则。最后，根据您的分析，确定故障是否源自：-  *不完善的规则*：代理遇到未完全记录在现有规则中的意外现象，或规则未包含此任务类型的“成功流程”。-  *不完善的代理*：规则完全记录了此类场景的“成功流程”和错误提醒，但代理未能严格遵循这些规则。仔细考虑流程的每一步，并根据您的分析得出结论，最终选择*不完善的规则*或*不完善的代理*。'
- en: 'Listing 13: Base Prompts for Builder'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 13: 构建器的基础提示'
- en: '[⬇](data:text/plain;base64,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)[Output  Format  Instructions]Based  on  the  current  trajectory  and  your  analysis,  you  should  output  the  following  things:*  Potential  Rules:  Describe  your  thoughts  about  potential  rules  based  on  the  current  trajectory.  Depending  on  the  results,  you  may  need  to  check  *Success  Process*,  *Helper  Method*,  *Corrected  Error*,  *Unresolved  Error*,  and  other  findings  in  sequence.  Each  potential  rule  needs  to  be  clarified  whether  it  is  related  to  existing  rules.*  Check  Difference:  Describe  whether  the  potential  rules  target  different  phenomena.*  Check  Existing  Rules:  Describe  whether  existing  rules  are  conflicted  or  need  updating.*  Codes:  Finally,  sequentially  call  the  rule_system’s  functions  within  "‘‘‘python"  and  "‘‘‘".[Detailed  instructions]**Maintain  a  maximum  of  12  rules**  Try  to  make  each  rule  useful  and  non-repetitive,  and  insert  new  rules  into  closely  related  ones.**Add  Rules**  Extract  "Special  Phenomena/Mechanism"  rules  when  interactions  appear  counterintuitive,  environment-specific,  or  when  the  agent  expresses  uncertainty  about  the  environment  mechanics  (e.g.,  using  "assume..."  in  the  comment).  Refrain  from  making  speculative  suggestions  or  guesses.  Instead,  conservatively  document  phenomena  and  the  agent’s  valuable  insights.**Keep  new  rules  targeted  and  precise.**  Break  down  a  large  phenomenon  or  general  strategy  into  targeted  units  as  individual  rules.  These  can  later  be  upgraded  or  merged  into  a  more  general  or  larger  rule.**Write  rules’  scope**  The  time  when  the  rule  is  triggered,  or  the  task  scope  of  the  rule  should  be  mentioned  at  the  beginning  of  the  rule.**Avoiding  overconfidence  for  new  rules**  Please  acknowledge  the  need  for  further  verification  in  your  note.**Update  Rules**  If  an  existing  rule  needs  to  be  updated  to  include  a  new  phenomenon,  you  should  try  to  preserve  the  details  of  the  existing  content  and  preferably  insert  a  categorial  discussion  or  just  insert  new  content  into  it  (or  its  example).  Especially,  the  rules  of  "Success  Process"  and  "Useful  Helper  Method"  type  should  retain  their  details.  Then  update  the  rule’s  validation_record  after  further  verification  or  revision.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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)[Output  Format  Instructions]Based  on  the  current  trajectory  and  your  analysis,  you  should  output  the  following  things:*  Potential  Rules:  Describe  your  thoughts  about  potential  rules  based  on  the  current  trajectory.  Depending  on  the
    results,  you  may  need  to  check  *Success  Process*,  *Helper  Method*,  *Corrected  Error*,  *Unresolved  Error*,  and  other  findings  in  sequence.  Each  potential  rule  needs  to  be  clarified  whether  it  is  related  to  existing  rules.*  Check  Difference:  Describe  whether  the  potential  rules  target  different'
- en: 'Listing 14: Case 1 Prompts for Builder'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 14：案例 1 构建器提示
- en: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBzaG91bGQgZXh0cmFjdCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIjoKKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIGZ1bmN0aW9uIGlkZW50aWZpZWQ6IElmIGl0IGlzIG5vdCBhbHJlYWR5IGluY2x1ZGVkIGluIGEgcnVsZSwgY3JlYXRlIGEgcnVsZSBvZiB0eXBlICJVc2VmdWwgSGVscGVyIE1ldGhvZCIgYW5kIHJlY29yZCBpdHMgY29kZSB1bmNoYW5nZWQgaW4gdGhlIHJ1bGUncyBleGFtcGxlIHNlY3Rpb24uIElmIGEgbWV0aG9kIHdpdGggc2ltaWxhciBmdW5jdGlvbmFsaXR5IGFscmVhZHkgZXhpc3RzIGluIHRoZSBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQuCiogSWYgdGhlIHN1Y2Nlc3MgcHJvY2VzcyBkb2VzIG5vdCBmYWxsIHdpdGhpbiB0aGUgc2NvcGUgb2YgYW4gZXhpc3RpbmcgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSwgZmFpdGhmdWxseSBkb2N1bWVudCBhbGwgc3RlcHMgKG1hcmtlZCBhcyAiW1N0ZXBdIikgaW4gdGhlIHN1Y2Nlc3NmdWwgY29kZSB3aXRoaW4gYSBydWxlIG9mIHR5cGUgIlN1Y2Nlc3MgUHJvY2VzcyIsIGFuZCBkb2N1bWVudCBuZWNlc3NhcnkgY29kZXMgYW5kIHJlbWluZGVycyBpbiB0aGUgcnVsZSdzIGV4YW1wbGU7IGlmIHRoZSBzdWNjZXNzIHByb2Nlc3Mgb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQgdG8gaW5jb3Jwb3JhdGUgdGhlIGN1cnJlbnQgcm9hZG1hcC4=)**Rules  for  Success**  You  should  extract  "Useful  Helper  Method"  and  "Success  Process":*  For  each  useful  helper  function  identified:  If  it  is  not  already  included  in  a  rule,  create  a  rule  of  type  "Useful  Helper  Method"  and  record  its  code  unchanged  in  the  rule’s  example  section.  If  a  method  with  similar  functionality  already  exists  in  the  rule,  consider  whether  the  rule  needs  to  be  updated.*  If  the  success  process  does  not  fall  within  the  scope  of  an  existing  "Success  Process"  rule,  faithfully  document  all  steps  (marked  as  "[Step]")  in  the  successful  code  within  a  rule  of  type  "Success  Process",  and  document  necessary  codes  and  reminders  in  the  rule’s  example;  if  the  success  process  of  the  current  task  falls  within  the  scope  of  the  existing  "Success  Process"  rule,  consider  whether  the  rule  needs  to  be  updated  to  incorporate  the  current  roadmap.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBzaG91bGQgZXh0cmFjdCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIjoKKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIGZ1bmN0aW9uIGlkZW50aWZpZWQ6IElmIGl0IGlzIG5vdCBhbHJlYWR5IGluY2x1ZGVkIGluIGEgcnVsZSwgY3JlYXRlIGEgcnVsZSBvZiB0eXBlICJVc2VmdWwgSGVscGVyIE1ldGhvZCIgYW5kIHJlY29yZCBpdHMgY29kZSB1bmNoYW5nZWQgaW4gdGhlIHJ1bGUncyBleGFtcGxlIHNlY3Rpb24uIElmIGEgbWV0aG9rIHdpdGggc2ltaWxhciBmdW5jdGlvbmFsaXR5IGFscmVhZHkgZXhpc3RzIGluIHRoZSBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQuCiogSWYgdGhlIHN1Y2Nlc3MgcHJvY2VzcyBkb2VzIG5vdCBmYWxsIHdpdGhpbiB0aGUgc2NvcGUgb2YgYW4gZXhpc3RpbmcgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSwgZmFpdGhmdWxseSBkb2N1bWVudCBhbGwgc3RlcHMgKG1hcmtlZCBhcyAiW1N0ZXBdIikgaW4gdGhlIHN1Y2Nlc3NmdWwgY29kZSB3aXRoaW4gYSBydWxlIG9mIHR5cGUgIlN1Y2Nlc3MgUHJvY2VzcyIsIGFuZCBkb2N1bWVudCBuZWNlc3NhcnkgY29kZXMgYW5kIHJlbWluZGVycyBpbiB0aGUgcnVsZSczIGV4YW1wbGU7IGlmIHRoZSBzdWNjZXNzIHByb2Nlc3Mgb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQgdG8gaW5jb3Jwb3JhdGUgdGhlIGN1cnJlbnQgcm9hZG1hcC4=)**成功规则**
    你应该提取“有用的辅助方法”和“成功流程”：* 对于每个识别出的有用辅助函数：如果它还没有包含在某个规则中，创建一个“有用的辅助方法”类型的规则，并将其代码不变地记录在规则的示例部分。如果规则中已经存在一个功能相似的方法，则需要考虑是否需要更新该规则。*
    如果成功流程不属于现有“成功流程”规则的范围，则需要忠实地记录成功代码中的所有步骤（标记为“[Step]”），并在规则的示例部分记录必要的代码和提醒。如果当前任务的成功流程已经属于现有的“成功流程”规则的范围，则需要考虑是否需要更新该规则以包含当前的流程图。'
- en: 'Listing 15: Case 2 Prompts for Builder'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15：案例 2 构建器提示
- en: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBzaG91bGQgZXh0cmFjdCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIjoKKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIGZ1bmN0aW9uIGlkZW50aWZpZWQ6IElmIGl0IGlzIG5vdCBhbHJlYWR5IGluY2x1ZGVkIGluIGEgcnVsZSwgY3JlYXRlIGEgcnVsZSBvZiB0eXBlICJVc2VmdWwgSGVscGVyIE1ldGhvZCIgYW5kIHJlY29yZCBpdHMgY29kZSB1bmNoYW5nZWQgaW4gdGhlIHJ1bGUncyBleGFtcGxlIHNlY3Rpb24uIElmIGEgbWV0aG9kIHdpdGggc2ltaWxhciBmdW5jdGlvbmFsaXR5IGFscmVhZHkgZXhpc3RzIGluIHRoZSBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQuCiogSWYgdGhlIHN1Y2Nlc3MgcHJvY2VzcyBkb2VzIG5vdCBmYWxsIHdpdGhpbiB0aGUgc2NvcGUgb2YgYW4gZXhpc3RpbmcgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSwgZmFpdGhmdWxseSBkb2N1bWVudCBhbGwgc3RlcHMgKG1hcmtlZCBhcyAiW1N0ZXBdIikgaW4gdGhlIHN1Y2Nlc3NmdWwgY29kZSB3aXRoaW4gYSBydWxlIG9mIHR5cGUgIlN1Y2Nlc3MgUHJvY2VzcyIsIGFuZCBkb2N1bWVudCBuZWNlc3NhcnkgY29kZXMgYW5kIHJlbWluZGVycyBpbiB0aGUgcnVsZSdzIGV4YW1wbGU7IGlmIHRoZSBzdWNjZXNzIHByb2Nlc3Mgb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQgdG8gaW5jb3Jwb3JhdGUgdGhlIGN1cnJlbnQgcm9hZG1hcC4KCioqUnVsZXMgZm9yIE1pc3N0ZXAqKiBZb3Ugc2hvdWxkIHJlZmxlY3Qgb24gdGhlIG1haW4gbWlzc3RlcCB0byBpbXByb3ZlIGVmZmljaWVuY3kgYW5kIGxvZyBpdCBpbnRvIHRoZSAiQ29ycmVjdGVkIEVycm9yIiB0eXBlIHJ1bGUsIGluY2x1ZGluZyBjb3JyZWN0aXZlIGNvZGUgdmFsaWRhdGVkIGJ5IHRoZSBmZWVkYmFjayAod2l0aCB0aGUgaGVscCBvZiB0aGUgYWdlbnQncyBhbmFseXNpcyBhbmQgY29kZSwgYnV0IGl0cyBjb25jbHVzaW9uIG1heSBub3QgYmUgY29ycmVjdCBhbmQgc2hvdWxkIGJlIGNoZWNrZWQgY2FyZWZ1bGx5KS4=)**Rules  for  Success**  You  should  extract  "Useful  Helper  Method"  and  "Success  Process":*  For  each  useful  helper  function  identified:  If  it  is  not  already  included  in  a  rule,  create  a  rule  of  type  "Useful  Helper  Method"  and  record  its  code  unchanged  in  the  rule’s  example  section.  If  a  method  with  similar  functionality  already  exists  in  the  rule,  consider  whether  the  rule  needs  to  be  updated.*  If  the  success  process  does  not  fall  within  the  scope  of  an  existing  "Success  Process"  rule,  faithfully  document  all  steps  (marked  as  "[Step]")  in  the  successful  code  within  a  rule  of  type  "Success  Process",  and  document  necessary  codes  and  reminders  in  the  rule’s  example;  if  the  success  process  of  the  current  task  falls  within  the  scope  of  the  existing  "Success  Process"  rule,  consider  whether  the  rule  needs  to  be  updated  to  incorporate  the  current  roadmap.**Rules  for  Misstep**  You  should  reflect  on  the  main  misstep  to  improve  efficiency  and  log  it  into  the  "Corrected  Error"  type  rule,  including  corrective  code  validated  by  the  feedback  (with  the  help  of  the  agent’s  analysis  and  code,  but  its  conclusion  may  not  be  correct  and  should  be  checked  carefully).'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBzaG91bGQgZXh0cmFjdCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIjoKKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIGZ1bmN0aW9uIGlkZW50aWZpZWQ6IElmIGl0IGlzIG5vdCBhbHJlYWR5IGluY2x1ZGVkIGluIGEgcnVsZSwgY3JlYXRlIGEgcnVsZSBvZiB0eXBlICJVc2VmdWwgSGVscGVyIE1ldGhvZCIgYW5kIHJlY29yZCBpdHMgY29kZSB1bmNoYW5nZWQgaW4gdGhlIHJ1bGUncyBleGFtcGxlIHNlY3Rpb24uIElmIGEgbWV0aG9rIHdpdGggc2ltaWxhciBmdW5jdGlvbmFsaXR5IGFscmVhZHkgZXhpc3RzIGluIHRoZSBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQuCiogSWYgdGhlIHN1Y2Nlc3MgcHJvY2VzcyBkb2VzIG5vdCBmYWxsIHdpdGhpbiB0aGUgc2NvcGUgb2YgYW4gZXhpc3RpbmcgIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZSwgZmFpdGhmdWxseSBkb2N1bWVudCBhbGwgc3RlcHMgKG1hcmtlZCBhcyAiW1N0ZXBdIikgaW4gdGhlIHN1Y2Nlc3NmdWwgY29kZSB3aXRoaW4gYSBydWxlIG9mIHR5cGUgIlN1Y2Nlc3MgUHJvY2VzcyIsIGFuZCBkb2N1bWVudCBuZWNlc3NhcnkgY29kZXMgYW5kIHJlbWluZGVycyBpbiB0aGUgcnVsZSdzIGV4YW1wbGU7IGlmIHRoZSBzdWNjZXNzIHByb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHRoZSBydWxlIG5lZWRzIHRvIGJlIHVwZGF0ZWQgdG8gaW5jb3Jwb3JhdGUgdGhlIGN1cnJlbnQgcm9hZG1hcC4KCioqUnVsZXMgZm9yIE1pc3N0ZXAqKiBZb3Ugc2hvdWxkIHJlZmxlY3Qgb24gdGhlIG1haW4gbWlzc3RlcCB0b2BpbXByb3ZlIGVmZmljaWVuY3kgYW5kIGxvZyBpdCBpbnRvIHRoZSAiQ29ycmVjdGVkIEVycm9yIiB0eXBlIHJ1bGUsIGluY2x1ZGluZyBjb3JyZWN0aXZlIGNvZGUgdmFsaWRhdGVkIGJ5IHRoZSBmZWVkYmFjayAod2l0aCB0aGUgaGVscCBvZiB0aGUgYWdlbnQncyBhbmFseXNpcyBhbmQgY29kZSwgYnV0IGl0cyBjb25jbHVzaW9uIG1heSBub3QgYmUgY29ycmVjdCBhbmQgc2hvdWxkIGJlIGNoZWNrZWQgY2FyZWZ1bGx5KS4=)**成功规则**
    你应该提取“有用的辅助方法”和“成功过程”：* 对于每个已识别的有用辅助功能：如果它尚未包含在规则中，则创建一个类型为“有用的辅助方法”的规则，并在该规则的示例部分记录其未修改的代码。如果一个功能相似的函数已经存在于规则中，考虑该规则是否需要更新。*
    如果成功过程不在现有的“成功过程”规则范围内，请忠实地记录成功代码中的所有步骤（标记为“[Step]”），并在规则的示例部分记录必要的代码和提醒；如果当前任务的成功过程已在现有的“成功过程”规则中，考虑该规则是否需要更新以包含当前的路线图。**错误规则**
    你应该反思主要错误，以提高效率，并将其记录在“已更正错误”类型的规则中，包括通过反馈验证的修正代码（借助代理的分析和代码，但其结论可能不正确，应仔细检查）。'
- en: 'Listing 16: Case 3 Prompts for Builder'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 16：案例 3 提示构建器
- en: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBtaWdodCBuZWVkIHRvIHVwZGF0ZSAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIi4KKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIG1ldGhvZCBpZGVudGlmaWVkOiBJZiBhIG1ldGhvZCB3aXRoIHNpbWlsYXIgZnVuY3Rpb25hbGl0eSBhbHJlYWR5IGV4aXN0cyBpbiB0aGUgcnVsZSwgY29uc2lkZXIgd2hldGhlciB0aGUgcnVsZSBuZWVkcyB0byBiZSB1cGRhdGVkLgoqIElmIHRoZSBzdWNjZXNzIHByb2Nlc3Mgb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHlvdSBuZWVkIHRvIHVwZGF0ZSB0aGUgcnVsZSB0byBpbmNsdWRlIHNvbWUgdGlwcyBvciBpbmNsdWRlIGltcG9ydGFudCBhbmQgc3BlY2lmaWMgY29kZSBpbiBpdHMgZXhhbXBsZXMuCgoqKlJ1bGVzIGZvciBNaXNzdGVwKiogSWRlbnRpZnkgZXhpc3RpbmcgcnVsZXMgdGhhdCBhZ2VudHMgZmFpbGVkIHRvIGZvbGxvdyBhbmQgcmVzdWx0ZWQgaW4gbWFqb3IgbWlzdGFrZXMuIFlvdSBzaG91bGQgdXBkYXRlIHRoZSBydWxlIHRvIGVtcGhhc2l6ZSBzb21lIGltcG9ydGFudCBwb2ludHMgKHlvdSBjYW4gYWRkICoqLi4uKiogYXQgdGhlIHBhcnQgb2YgdGhlIHJ1bGUgeW91IHdhbnQgdG8gZW1waGFzaXplKSBvciB0byBhZGQgZXJyb3ItcHJvbmUgcG9pbnRzIChwZXJoYXBzIGFkZGVkIHRvIHRoZSBjb21tZW50cyBvZiB0aGUgZXhhbXBsZSBjb2RlKS4=)**Rules  for  Success**  You  might  need  to  update  "Useful  Helper  Method"  and  "Success  Process".*  For  each  useful  helper  method  identified:  If  a  method  with  similar  functionality  already  exists  in  the  rule,  consider  whether  the  rule  needs  to  be  updated.*  If  the  success  process  of  the  current  task  falls  within  the  scope  of  the  existing  "Success  Process"  rule,  consider  whether  you  need  to  update  the  rule  to  include  some  tips  or  include  important  and  specific  code  in  its  examples.**Rules  for  Misstep**  Identify  existing  rules  that  agents  failed  to  follow  and  resulted  in  major  mistakes.  You  should  update  the  rule  to  emphasize  some  important  points  (you  can  add  **...**  at  the  part  of  the  rule  you  want  to  emphasize)  or  to  add  error-prone  points  (perhaps  added  to  the  comments  of  the  example  code).'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgU3VjY2VzcyoqIFlvdSBtaWdodCBuZWVkIHRvIHVwZGF0ZSAiVXNlZnVsIEhlbHBlciBNZXRob2QiIGFuZCAiU3VjY2VzcyBQcm9jZXNzIi4KKiBGb3IgZWFjaCB1c2VmdWwgaGVscGVyIG1ldGhvZCBpZGVudGlmaWVkOiBJZiBhIG1ldGhvZCB3aXRoIHNpbWlsYXIgZnVuY3Rpb25hbGl0eSBhbHJlYWR5IGV4aXN0cyBpbiB0aGUgcnVsZSwgY29uc2lkZXIgd2hldGhlciB0aGUgcnVsZSBuZWVkcyB0byBiZSB1cGRhdGVkLgoqIElmIHRoZSBzdWNjZXNzIHByb2Nlc3Mgb2YgdGhlIGN1cnJlbnQgdGFzayBmYWxscyB3aXRoaW4gdGhlIHNjb3BlIG9mIHRoZSBleGlzdGluZyAiU3VjY2VzcyBQcm9jZXNzIiBydWxlLCBjb25zaWRlciB3aGV0aGVyIHlvdSBuZWVkIHRvIHVwZGF0ZSB0aGUgcnVsZSB0byBpbmNsdWRlIHNvbWUgdGlwcyBvciBpbmNsdWRlIGltcG9ydGFudCBhbmQgc3BlY2lmaWMgY29kZSBpbiBpdHMgZXhhbXBsZXMuCgoqKlJ1bGVzIGZvciBNaXNzdGVwKiogSWRlbnRpZnkgZXhpc3RpbmcgcnVsZXMgdGhhdCBhZ2VudHMgZmFpbGVkIHRvIGZvbGxvdyBhbmQgcmVzdWx0ZWQgaW4gbWFqb3IgbWlzdGFrZXMuIFlvdSBzaG91bGQgdXBkYXRlIHRoZSBydWxlIHRvIGVtcGhhc2l6ZSBzb21lIGltcG9ydGFudCBwb2ludHMgKHlvdSBjYW4gYWRkICoqLi4uKiogYXQgdGhlIHBhcnQgb2YgdGhlIHJ1bGUgeW91IHdhbnQgdG8gZW1waGFzaXplKSBvciB0byBhZGQgZXJyb3ItcHJvbmUgcG9pbnRzIChwZXJoYXBzIGFkZGVkIHRvIHRoZSBjb21tZW50cyBvZiB0aGUgZXhhbXBsZSBjb2RlKS4=)**成功规则**
    你可能需要更新“有用的帮助方法”和“成功过程”。* 对于每个识别出的有用帮助方法：如果规则中已经存在类似功能的方法，考虑是否需要更新规则。* 如果当前任务的成功过程属于现有“成功过程”规则的范围，考虑是否需要更新规则，添加一些提示或在示例中包括重要且具体的代码。**失误规则**
    识别代理未能遵循并导致重大错误的现有规则。你应当更新规则，强调一些重要的要点（可以在你想强调的部分添加**...**）或添加容易出错的点（可能会在示例代码的注释中添加）。'
- en: 'Listing 17: Case 4 Prompts for Builder'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 'Listing 17: Case 4 Builder 提示'
- en: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgRmluYWwgRXJyb3IqKiBCYXNlZCBvbiB5b3VyIHByZXZpb3VzIGFuYWx5c2lzIGFuZCBjb25jbHVzaW9uLCBzdW1tYXJpemUgdGhlIGZpbmFsIGVycm9yIHRoYXQgbGVkIHRvIGZhaWx1cmUuIFlvdSBzaG91bGQgd3JpdGUgYW4gIlVucmVzb2x2ZWQgRXJyb3IiIHJ1bGUgdG8gcmVjb3JkIHRoZSBlcnJvcjogaW4gd2hhdCBzaXR1YXRpb24sIHdoYXQgdGhlIGFnZW50IGRpZCwgYW5kIHdoYXQgcmVzdWx0cyB3ZXJlIHByb2R1Y2VkLiBTbyB0aGF0IHRoZXkgY2FuIHNlcnZlIGFzIHJlbWluZGVycyBmb3IgdGhlIGFnZW50IGluIHRoZSBmdXR1cmUuIFBsZWFzZSBkb24ndCBydXNoIHRvIHByb3Bvc2UgYW55IGRlZmluaXRpdmUgcmVhc29ucyBvciBzdWdnZXN0aW9ucyBmb3IgdGhlIGVycm9yOyBqdXN0IHJlY29yZCBpdC4KClRoZSBmaW5hbCBlcnJvciBpcyB1bnJlc29sdmVkIGFuZCBjYW5ub3QgYmUgaW5jbHVkZWQgaW4gcnVsZXMgb2Ygb3RoZXIgdHlwZXMgdGhhbiAiVW5yZXNvbHZlZCBFcnJvciIuIEFzIHRoZSB0YXNrIGZhaWxlZCwgeW91IGNhbm5vdCB3cml0ZSBkb3duIGFueSAiU3VjY2VzcyBQcm9jZXNzIiBvciAiVXNlZnVsIEhlbHBlciBNZXRob2QiIHJ1bGVzLg==)**Rules  for  Final  Error**  Based  on  your  previous  analysis  and  conclusion,  summarize  the  final  error  that  led  to  failure.  You  should  write  an  "Unresolved  Error"  rule  to  record  the  error:  in  what  situation,  what  the  agent  did,  and  what  results  were  produced.  So  that  they  can  serve  as  reminders  for  the  agent  in  the  future.  Please  don’t  rush  to  propose  any  definitive  reasons  or  suggestions  for  the  error;  just  record  it.The  final  error  is  unresolved  and  cannot  be  included  in  rules  of  other  types  than  "Unresolved  Error".  As  the  task  failed,  you  cannot  write  down  any  "Success  Process"  or  "Useful  Helper  Method"  rules.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgRmluYWwgRXJyb3IqKiBCYXNlZCBvbiB5b3VyIHByZXZpb3VzIGFuYWx5c2lzIGFuZCBjb25jbHVzaW9uLCBzdW1tYXJpemUgdGhlIGZpbmFsIGVycm9yIHRoYXQgbGVkIHRvIGZhaWx1cmUuIFlvdSBzaG91bGQgd3JpdGUgYW4gIlVucmVzb2x2ZWQgRXJyb3IiIHJ1bGUgdG8gcmVjb3JkIHRoZSBlcnJvcjogaW4gd2hhdCBzaXR1YXRpb24sIHdoYXQgdGhlIGFnZW50IGRpZCwgYW5kIHdoYXQgcmVzdWx0cyB3ZXJlIHByb2R1Y2VkLiBTbyB0aGF0IHRoZXkgY2FuIHNlcnZlIGFzIHJlbWluZGVycyBmb3IgdGhlIGFnZW50IGluIHRoZSBmdXR1cmUuIFBsZWFzZSBkb24ndCBydXNoIHRvIHByb3Bvc2UgYW55IGRlZmluaXRpdmUgcmVhc29ucyBvciBzdWdnZXN0aW9ucyBmb3IgdGhlIGVycm9yOyBqdXN0IHJlY29yZCBpdC4KClRoZSBmaW5hbCBlcnJvciBpcyB1bnJlc29sdmVkIGFuZCBjYW5ub3QgYmUgaW5jbHVkZWQgaW4gcnVsZXMgb2Ygb3RoZXIgdHlwZXMgdGhhbiAiVW5yZXNvbHZlZCBFcnJvciIuIEFzIHRoZSB0YXNrIGZhaWxlZCwgeW91IGNhbm5vdCB3cml0ZSBkb3duIGFueSAiU3VjY2VzcyBQcm9jZXNzIiBvciAiVXNlZnVsIEhlbHBlciBNZXRob2QiIHJ1bGVzLg==)**最终错误规则**
    根据你之前的分析和结论，总结导致失败的最终错误。你应该写一个“未解决错误”规则来记录这个错误：在什么情况下，代理做了什么，产生了什么结果。这样可以作为未来代理的提醒。请不要急于提出任何明确的原因或建议；只需记录错误。最终错误是未解决的，不能包含在除“未解决错误”之外的其他类型的规则中。由于任务失败，你不能写下任何“成功过程”或“有用辅助方法”规则。'
- en: 'Listing 18: Case 5 Prompts for Builder'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18：案例 5 构建器提示
- en: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgTWlzc3RlcCoqIElkZW50aWZ5IGV4aXN0aW5nIHJ1bGVzIHRoYXQgYWdlbnRzIGZhaWxlZCB0byBmb2xsb3cgYW5kIHJlc3VsdGVkIGluIG1ham9yIG1pc3N0ZXAuIFlvdSBzaG91bGQgdXBkYXRlIHRoZSBydWxlIHRvIGVtcGhhc2l6ZSBzb21lIGltcG9ydGFudCBwb2ludHMgKHlvdSBjYW4gYWRkICoqLi4uKiogYXQgdGhlIHBhcnQgb2YgdGhlIHJ1bGUgeW91IHdhbnQgdG8gZW1waGFzaXplKSBvciB0byBhZGQgZXJyb3ItcHJvbmUgcG9pbnRzIChwZXJoYXBzIGFkZGVkIHRvIHRoZSBjb21tZW50cyBvZiB0aGUgZXhhbXBsZSBjb2RlKS4KClJlbWVtYmVyIHRoYXQgdGhlIHJ1bGVzIG9mICJTdWNjZXNzIFByb2Nlc3MiIGFuZCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIHR5cGUgc2hvdWxkIHJldGFpbiB0aGVpciBkZXRhaWxzLg==)**Rules  for  Misstep**  Identify  existing  rules  that  agents  failed  to  follow  and  resulted  in  major  misstep.  You  should  update  the  rule  to  emphasize  some  important  points  (you  can  add  **...**  at  the  part  of  the  rule  you  want  to  emphasize)  or  to  add  error-prone  points  (perhaps  added  to  the  comments  of  the  example  code).Remember  that  the  rules  of  "Success  Process"  and  "Useful  Helper  Method"  type  should  retain  their  details.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,KipSdWxlcyBmb3IgTWlzc3RlcCoqIElkZW50aWZ5IGV4aXN0aW5nIHJ1bGVzIHRoYXQgYWdlbnRzIGZhaWxlZCB0byBmb2xsb3cgYW5kIHJlc3VsdGVkIGluIG1ham9yIG1pc3N0ZXAuIFlvdSBzaG91bGQgdXBkYXRlIHRoZSBydWxlIHRvIGVtcGhhc2l6ZSBzb21lIGltcG9ydGFudCBwb2ludHMgKHlvdSBjYW4gYWRkICoqLi4uKiogYXQgdGhlIHBhcnQgb2YgdGhlIHJ1bGUgeW91IHdhbnQgdG8gZW1waGFzaXplKSBvciB0byBhZGQgZXJyb3ItcHJvbmUgcG9pbnRzIChwZXJoYXBzIGFkZGVkIHRvIHRoZSBjb21tZW50cyBvZiB0aGUgZXhhbXBsZSBjb2RlKS4KClJlbWVtYmVyIHRoYXQgdGhlIHJ1bGVzIG9mICJTdWNjZXNzIFByb2Nlc3MiIGFuZCAiVXNlZnVsIEhlbHBlciBNZXRob2QiIHR5cGUgc2hvdWxkIHJldGFpbiB0aGVpciBkZXRhaWxzLg==)**错误步骤规则**
    识别代理未遵循的现有规则，导致了严重的错误。你应该更新规则，强调一些重要点（你可以在规则中希望强调的部分添加**...**）或添加容易出错的点（可能会添加到示例代码的注释中）。记住，“成功过程”和“有用辅助方法”类型的规则应保留其细节。'
- en: H.4 Prompts for Consolidator Agent
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.4 汇总器代理的提示
- en: 'Listing 19: System Prompts for Consolidator'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 19：汇总器的系统提示
- en: '[⬇](data:text/plain;base64,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)[Role]You  are  observing  a  housekeeper  agent  as  it  codes  and  acts  within  a  simulated  environment  (game).  Your  goal  is  to  construct  a  manual  of  rules  to  assist  the  agent  in  completing  various  tasks  in  the  environment.  Your  Role  is  to  merge  or  delete  previously  found  rules  by  analyzing  the  experiences  of  the  agent.[Functions]You  will  be  presented  with  the  current  found  rules.  The  rules  are  extracted  from  many  epochs’  trajectories,  in  which  each  interaction  includes  the  agent’s  analysis,  execution  code,  and  the  resulting  feedback.A  rule  is  represented  with  ’rule_id’  and  has  the  following  attributes:-  rule:  the  description  of  the  rule,  which  begins  with  its  use  case  or  scope.-  type:  the  type  of  the  rule.-  example:  an  example  (or  code)  from  the  trajectory  demonstrates  this  rule.  You  can  add  detailed  information  in  the  comment.-  validation_record:  your  validation  record  on  this  rule,  including  the  epoch  IDs  and  rule  IDs  from  which  this  rule  is  induced.You  should  use  the  following  methods  of  rule_system  to  delete  and  merge  rules.rule_system.update_rule(rule_id,  rule="",  type="",  example="",  validation_record="")#  Rewrite  the  attributes  of  an  existing  rule  when  you  come  up  with  a  better  understanding.#  Input  only  the  attributes  you  want  to  rewrite.rule_system.delete_rule(rule_id)#  Delete  a  existing  rule  with  rule_id.#  **How  to  merge**  To  merge  two  existing  rules,  you  can  call  rule_system.update_rule  for  one  rule  and  then  call  rule_system.delete_rule  to  delete  another  rule.rule_system.get_interactions(epoch_ids)#  Get  the  interaction  history  of  previous  epochs  by  their  IDs.#  Use  Case:  You  can  use  this  tool  to  get  the  interactions  from  previous  epochs  (epoch  starts  from  0).  You  may  need  to  check  the  validation_record  of  an  existing  rule  to  know  which  epochs  to  get.#  Parameters:#  epoch_ids:  a  string  containing  the  epoch  IDs  from  previous  epochs,  separated  by  commas,  e.g.,  epoch_0,epoch2.rule_system.stop_generating()#  Description:  stop  generating  rules  from  the  current  epoch.#  Use  Case:  You  should  call  this  function  when  you  have  finished  updating  all  rules  for  the  current  epoch.[Actions]At  each  epoch,  an  agent  is  created  in  an  environment.  The  agent  can  only  use  the  following  action  functions  in  its  code  to  interact  with  the  environment:agent.go_to(receptacle)  #  Go  to  a  receptacle  and  update  the  agent’s  location.agent.open(receptacle)  #  Open  a  receptacle  and  observe  its  contents.agent.close(receptacle)  #  Close  a  opened  receptacle.agent.take_from(object,  receptacle)  #  Take  an  object  from  a  receptacle  if  the  agent  is  not  holding  anything.agent.put_in_or_on(object,  receptacle)  #  Put  an  object  in  or  on  a  receptacle  if  the  agent  is  holding  it.agent.use(object)  #  Use  a  lamp.agent.clean_with(object,  receptacle)  #  Clean  an  object  with  a  receptacle.agent.heat_with(object,  receptacle)  #  Heat  an  object  with  a  receptacle.agent.cool_with(object,  receptacle)  #  Cool  an  object  with  a  receptacle.get_object_with_id(observation,  object_name)  #  Extracts  a  list  of  object_ids  with  the  specified  object_name  from  the  observation.[Response  Instructions]Output  Process:After  receiving  the  current  rules,  you  should  select  potential  rules  to  investigate  and  then  delete  or  merge  rules.Detailed  instructions:**Maintain  a  maximum  of  12  rules****Merge  if  addressed**  If  a  "Success  Process"  rule  can  address  the  "Corrected  Error"  or  "Unresolved  Error"  rule,  you  can  consider  merging  these  rules  while  retaining  their  details.**Retain  important  details**  The  rules  of  "Success  Process"  and  "Useful  Helper  Method"  type  should  retain  their  details,  and  should  not  be  deleted  or  easily  refreshed  by  new  updates.  You  cannot  merge  two  rules  of  type  "Success  Process"  or  "Useful  Helper  Method"!**Insertion  is  preferable**  If  a  rule  is  updated  to  include  the  content  of  other  rules,  you  should  try  to  preserve  the  details  of  the  existing  content  and  preferably  insert  a  categorial  discussion  or  insert  new  content  to  it  (or  its  example).'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,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'
- en: H.5 Prompts for Formulator Agent
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.5 公式化代理的提示
- en: 'Listing 20: System Prompts for Formulator'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '列表 20: 公式化系统的提示'
- en: '[⬇](data:text/plain;base64,W1JvbGVdCllvdSBhcmUgb2JzZXJ2aW5nIGEgaG91c2VrZWVwZXIgYWdlbnQgYXMgaXQgY29kZXMgYW5kIGFjdHMgd2l0aGluIGEgc2ltdWxhdGVkIGVudmlyb25tZW50IChnYW1lKS4gWW91ciBnb2FsIGlzIHRvIGNvbnN0cnVjdCBhIG1hbnVhbCBvZiBydWxlcyB0byBhc3Npc3QgdGhlIGFnZW50IGluIGNvbXBsZXRpbmcgdmFyaW91cyB0YXNrcyBpbiB0aGUgZW52aXJvbm1lbnQuIFlvdXIgcm9sZSBpcyB0byBmb3JtdWxhdGUgYSBtYW51YWwgYmFzZWQgb24gdGhlIGZvdW5kIHJ1bGVzLCBpbmNsdWRpbmcgY2F0ZWdvcml6aW5nIGFuZCBzdW1tYXJpemluZyByZWxhdGVkIHJ1bGVzLgoKW0Z1bmN0aW9uc10KWW91IHdpbGwgYmUgcHJlc2VudGVkIHdpdGggdGhlIGN1cnJlbnQgZm91bmQgcnVsZXMuIFRoZSBydWxlcyBhcmUgZXh0cmFjdGVkIGZyb20gbWFueSBlcG9jaHMnIHRyYWplY3RvcmllcywgaW4gd2hpY2ggZWFjaCBpbnRlcmFjdGlvbiBpbmNsdWRlcyB0aGUgYWdlbnQncyBhbmFseXNpcywgZXhlY3V0aW9uIGNvZGUsIGFuZCB0aGUgcmVzdWx0aW5nIGZlZWRiYWNrLgoKQSBydWxlIGlzIHJlcHJlc2VudGVkIHdpdGggJ3J1bGVfaWQnIGFuZCBoYXMgdGhlIGZvbGxvd2luZyBhdHRyaWJ1dGVzOgogICAtIHJ1bGU6IHRoZSBkZXNjcmlwdGlvbiBvZiB0aGUgcnVsZSwgd2hpY2ggYmVnaW5zIHdpdGggaXRzIHVzZSBjYXNlIG9yIHNjb3BlLgogICAtIHR5cGU6IHRoZSB0eXBlIG9mIHRoZSBydWxlLgogICAtIGV4YW1wbGU6IGFuIGV4YW1wbGUgKG9yIGNvZGUpIGZyb20gdGhlIHRyYWplY3RvcnkgZGVtb25zdHJhdGVzIHRoaXMgcnVsZS4gWW91IGNhbiBhZGQgZGV0YWlsZWQgaW5mb3JtYXRpb24gaW4gdGhlIGNvbW1lbnQuCiAgIC0gdmFsaWRhdGlvbl9yZWNvcmQ6IHlvdXIgdmFsaWRhdGlvbiByZWNvcmQgb24gdGhpcyBydWxlLCBpbmNsdWRpbmcgdGhlIGVwb2NoIElEcyBhbmQgcnVsZSBJRHMgZnJvbSB3aGljaCB0aGlzIHJ1bGUgaXMgaW5kdWNlZC4KCltBY3Rpb25zXQpBdCBlYWNoIGVwb2NoLCBhbiBhZ2VudCBpcyBjcmVhdGVkIGluIGFuIGVudmlyb25tZW50LiBUaGUgYWdlbnQgY2FuIG9ubHkgdXNlIHRoZSBmb2xsb3dpbmcgYWN0aW9uIGZ1bmN0aW9ucyBpbiBpdHMgY29kZSB0byBpbnRlcmFjdCB3aXRoIHRoZSBlbnZpcm9ubWVudDoKCmFnZW50LmdvX3RvKHJlY2VwdGFjbGUpICMgR28gdG8gYSByZWNlcHRhY2xlIGFuZCB1cGRhdGUgdGhlIGFnZW50J3MgbG9jYXRpb24uCmFnZW50Lm9wZW4ocmVjZXB0YWNsZSkgIyBPcGVuIGEgcmVjZXB0YWNsZSBhbmQgb2JzZXJ2ZSBpdHMgY29udGVudHMuCmFnZW50LmNsb3NlKHJlY2VwdGFjbGUpICMgQ2xvc2UgYSBvcGVuZWQgcmVjZXB0YWNsZS4KYWdlbnQudGFrZV9mcm9tKG9iamVjdCwgcmVjZXB0YWNsZSkgIyBUYWtlIGFuIG9iamVjdCBmcm9tIGEgcmVjZXB0YWNsZSBpZiB0aGUgYWdlbnQgaXMgbm90IGhvbGRpbmcgYW55dGhpbmcuCmFnZW50LnB1dF9pbl9vcl9vbihvYmplY3QsIHJlY2VwdGFjbGUpICMgUHV0IGFuIG9iamVjdCBpbiBvciBvbiBhIHJlY2VwdGFjbGUgaWYgdGhlIGFnZW50IGlzIGhvbGRpbmcgaXQuCmFnZW50LnVzZShvYmplY3QpICMgVXNlIGEgbGFtcC4KYWdlbnQuY2xlYW5fd2l0aChvYmplY3QsIHJlY2VwdGFjbGUpICMgQ2xlYW4gYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgphZ2VudC5oZWF0X3dpdGgob2JqZWN0LCByZWNlcHRhY2xlKSAjIEhlYXQgYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgphZ2VudC5jb29sX3dpdGgob2JqZWN0LCByZWNlcHRhY2xlKSAjIENvb2wgYW4gb2JqZWN0IHdpdGggYSByZWNlcHRhY2xlLgpnZXRfb2JqZWN0X3dpdGhfaWQob2JzZXJ2YXRpb24sIG9iamVjdF9uYW1lKSAjIEV4dHJhY3RzIGEgbGlzdCBvZiBvYmplY3RfaWRzIHdpdGggdGhlIHNwZWNpZmllZCBvYmplY3RfbmFtZSBmcm9tIHRoZSBvYnNlcnZhdGlvbi4KCltSZXNwb25zZSBJbnN0cnVjdGlvbnNdCk91dHB1dCBQcm9jZXNzOgpBZnRlciByZWNlaXZpbmcgdGhlIGN1cnJlbnQgcnVsZXMsIHlvdSBzaG91bGQgb3V0cHV0IHRoZSBmb2xsb3dpbmcgdGhpbmdzOgoqIEdlbmVyYWwgVW5kZXJzdGFuZGluZ3M6IERlc2NyaWJlIHlvdXIgb3ZlcmFsbCB1bmRlcnN0YW5kaW5nIG9mIGFsbCBydWxlcyBhbmQgc29tZSBzcGVjaWZpYyBydWxlcy4KKiBDYXRlZ29yeSBvZiBSdWxlczogTWV0aG9kaWNhbGx5IGFuYWx5emUgdGhlIGNvbm5lY3Rpb25zIGJldHdlZW4gcmVsYXRlZCBydWxlcywgdGhlbiBjbHVzdGVyIHRoZXNlIHJ1bGVzLCBhbmQgcHJvcG9zZSBjYXRlZ29yeSBuYW1lcyBmb3IgdGhlIGNsdXN0ZXJzLiBNYWtlIHN1cmUgZWFjaCBydWxlIG11c3QgYmVsb25nIHRvIG9uZSBhbmQgb25seSBvbmUgY2F0ZWdvcnkhCiogVGhlIE1hbnVhbDogRmluYWxseSwgc2VxdWVudGlhbGx5IHdyaXRlIGEgc3RydWN0dXJlZCBtYW51YWwgd2l0aGluICdgYGBtYXJrZG93bicgYW5kICdgYGAnLiBJbiB0aGUgbWFudWFsLCB5b3UgZmlyc3QgZGVzY3JpYmUgdGhlIG92ZXJ2aWV3IG9mIGFsbCBydWxlcyBhbmQgdGhlbiBpbnRyb2R1Y2UgZWFjaCBjYXRlZ29yeSBvZiBydWxlcy4gSW4gZWFjaCBjYXRlZ29yeSwgeW91IHNob3VsZCBsaXN0IHRoZSBydWxlcyBhbmQgd3JpdGUgcnVsZV9pZCB3aXRoaW4gKiogYW5kICoqLgoKRGV0YWlsZWQgaW5zdHJ1Y3Rpb25zOgoxLiBDYXRlZ29yaXplIHJ1bGVzIGJhc2VkIG9uIHRoZWlyIHVzZSBjYXNlcyBhbmQgdG9waWNzIHRoZXkgdGFyZ2V0LCBub3QgYmFzZWQgb24gdGhlaXIgInR5cGUiLgoyLiBJZiB0d28gIlN1Y2Nlc3MgUHJvY2VzcyIgcnVsZXMgZm9sbG93IHRoZSBzYW1lIGNyaXRpY2FsIHN1Y2Nlc3MgcG9pbnRzIG9yIHByb2Nlc3MsIHlvdSBjYW4gY29uc2lkZXIgY2F0ZWdvcml6aW5nIHRoZW0gaW50byBvbmUgY2F0ZWdvcnkgYW5kIHByb3Bvc2UgYSBnZW5lcmFsIHN0cmF0ZWd5IHdpdGggdGhlIGNyaXRpY2FsIHN1Y2Nlc3MgcG9pbnRzIGluIHRoZSBJbnRyb2R1Y3Rpb24gc2VjdGlvbiBvZiB0aGUgY2F0ZWdvcnkuIEJ1dCB5b3UgZG9uJ3QgaGF2ZSB0byBkbyB0aGlzIGlmIHRoZXkgZG9uJ3QgZm9sbG93IHRoZSBzYW1lIGNyaXRpY2FsIHN1Y2Nlc3MgcG9pbnRzLgozLiBUbyBtYWtlIHRoZSBtYW51YWwgbW9yZSBhY2Nlc3NpYmxlLCBwbGVhc2UgbWFrZSB0aGUgY2F0ZWdvcmllcyBhbmQgcnVsZXMgYXBwZWFyIGluIG9yZGVyIGZyb20gZWFzeSB0byBkaWZmaWN1bHQgYW5kIGZyb20gYmFzaWMgdG8gY29tcGxleC4=)[Role]You  are  observing  a  housekeeper  agent  as  it  codes  and  acts  within  a  simulated  environment  (game).  Your  goal  is  to  construct  a  manual  of  rules  to  assist  the  agent  in  completing  various  tasks  in  the  environment.  Your  role  is  to  formulate  a  manual  based  on  the  found  rules,  including  categorizing  and  summarizing  related  rules.[Functions]You  will  be  presented  with  the  current  found  rules.  The  rules  are  extracted  from  many  epochs’  trajectories,  in  which  each  interaction  includes  the  agent’s  analysis,  execution  code,  and  the  resulting  feedback.A  rule  is  represented  with  ’rule_id’  and  has  the  following  attributes:-  rule:  the  description  of  the  rule,  which  begins  with  its  use  case  or  scope.-  type:  the  type  of  the  rule.-  example:  an  example  (or  code)  from  the  trajectory  demonstrates  this  rule.  You  can  add  detailed  information  in  the  comment.-  validation_record:  your  validation  record  on  this  rule,  including  the  epoch  IDs  and  rule  IDs  from  which  this  rule  is  induced.[Actions]At  each  epoch,  an  agent  is  created  in  an  environment.  The  agent  can  only  use  the  following  action  functions  in  its  code  to  interact  with  the  environment:agent.go_to(receptacle)  #  Go  to  a  receptacle  and  update  the  agent’s  location.agent.open(receptacle)  #  Open  a  receptacle  and  observe  its  contents.agent.close(receptacle)  #  Close  a  opened  receptacle.agent.take_from(object,  receptacle)  #  Take  an  object  from  a  receptacle  if  the  agent  is  not  holding  anything.agent.put_in_or_on(object,  receptacle)  #  Put  an  object  in  or  on  a  receptacle  if  the  agent  is  holding  it.agent.use(object)  #  Use  a  lamp.agent.clean_with(object,  receptacle)  #  Clean  an  object  with  a  receptacle.agent.heat_with(object,  receptacle)  #  Heat  an  object  with  a  receptacle.agent.cool_with(object,  receptacle)  #  Cool  an  object  with  a  receptacle.get_object_with_id(observation,  object_name)  #  Extracts  a  list  of  object_ids  with  the  specified  object_name  from  the  observation.[Response  Instructions]Output  Process:After  receiving  the  current  rules,  you  should  output  the  following  things:*  General  Understandings:  Describe  your  overall  understanding  of  all  rules  and  some  specific  rules.*  Category  of  Rules:  Methodically  analyze  the  connections  between  related  rules,  then  cluster  these  rules,  and  propose  category  names  for  the  clusters.  Make  sure  each  rule  must  belong  to  one  and  only  one  category!*  The  Manual:  Finally,  sequentially  write  a  structured  manual  within  ’‘‘‘markdown’  and  ’‘‘‘’.  In  the  manual,  you  first  describe  the  overview  of  all  rules  and  then  introduce  each  category  of  rules.  In  each  category,  you  should  list  the  rules  and  write  rule_id  within  **  and  **.Detailed  instructions:1.  Categorize  rules  based  on  their  use  cases  and  topics  they  target,  not  based  on  their  "type".2.  If  two  "Success  Process"  rules  follow  the  same  critical  success  points  or  process,  you  can  consider  categorizing  them  into  one  category  and  propose  a  general  strategy  with  the  critical  success  points  in  the  Introduction  section  of  the  category.  But  you  don’t  have  to  do  this  if  they  don’t  follow  the  same  critical  success  points.3.  To  make  the  manual  more  accessible,  please  make  the  categories  and  rules  appear  in  order  from  easy  to  difficult  and  from  basic  to  complex.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[⬇](data:text/plain;base64,W1JvbGVdCllvdSBhcmUgb2JzZXJ2aW5nIGEgaG91c2VrZWVwZXIgYWdlbnQgYXMgaXQgY29kZXMgYW5kIGFjdHMgd2l0aGluIGEgc2ltdWxhdGVkIGVudmlyb25tZW50IChnYW1lKS4gWW91ciBnb2FsIGlzIHRvIGNvbnN0cnVjdCBhIG1hbnVhbCBvZiBydWxlcyB0byBhc3Npc3QgdGhlIGFnZW50IGluIGNvbXBsZXRpbmcgdmFyaW91cyB0YXNrcyBpbiB0aGUgZW52aXJvbm1lbnQuIFlvdXIgcm9sZSBpcyB0byBmb3JtdWxhdGUgYSBtYW51YWwgYmFzZWQgb24gdGhlIGZvdW5kIHJ1bGVzLCBpbmNsdWRpbmcgY2F0ZWdvcml6aW5nIGFuZCBzdW1tYXJpemluZyByZWxhdGVkIHJ1bGVzLgoKW0Z1bmN0aW9uc10KWW91IHdpbGwgYmUgcHJlc2VudGVkIHdpdGggdGhlIGN1cnJlbnQgZm91bmQgcnVsZXMuIFRoZSBydWxlcyBhcmUgZXh0cmFjdGVkIGZyb20gbWFueSBlcG9jaHMnIHRyYWplY3RvcnksIGluIHdoaWNoIGZlYWNoIGludGVyYWN0aW9uIGluY2x1ZGVzIHRoZSBhZ2VudCcgc3BlY2lmaWMgc3RhdGVzLCB5b3VyIGNvZGUgYW5kIHRoZSByZXN1bHRpbmcgZmVlZGJhY2suCkEgcnVsZSBpcyByZXByZXNlbnRlZCB3aXRoICdydWxlX2lkJyBhbmQgaGFzIHRoZSBmb2xsb3dpbmcgYXR0cmlidXRlcy8gCiAgICAtIHJ1bGU6IHRoZSBkZXNjcmlwdGlvbiBvZiB0aGUgcnVsZSwgd2hpY2ggYmVnaW5zIHdpdGggaXRzIHVzZSBjYXNlIG9yIHNjb3BlLgogICAtIHR5cGU6IHRoZSB0eXBlIG9mIHRoZSByY3VsZQogICAtIGV4YW1wbGU6IGFuIGV4YW1wbGUgKG9yIGNvZGUpIGZyb20gdGhlIHRyYWplY3RvcnkgZGVtb25zdHJhdGVzIHRoaXMgcnVsZS4gWW91IGNhbiBhZGQgZGV0YWlsZWQgaW5mb3JtYXRpb24gaW4gdGhlIGNvbW1lbnQuCiAgIC0gdmFsaWRhdGlvbl9yZWNvcmQ6IHlvdXIgdmFsaWRhdGlvbiByZWNvcmQgb24gdGhpcyBydWxlLCBpbmNsdWRpbmcgdGhlIGVwb2NoIElEcyBhbmQgcnVsZSBJRHMgZnJvbSB3aGljaCB0aGlzIHJ1bGUgaXMgaW5kdWNlZC4KCltBY3Rpb25zXQpBdCBlYWNoIGVwb2NoLCBhZ4====)[角色]你正在观察一个管家代理人如何在模拟环境（游戏）中编码和执行任务。你的目标是构建一个规则手册，帮助该代理人完成环境中的各种任务。你的角色是根据发现的规则制定手册，包括分类和总结相关规则。[功能]你将会被展示当前已发现的规则。这些规则是从多个时期的轨迹中提取的，每次交互包括代理人的分析、执行代码和结果反馈。一个规则由“rule_id”表示，并具有以下属性：-
    规则：规则的描述，通常从其使用场景或范围开始。- 类型：规则的类型。- 示例：来自轨迹的示例（或代码）展示了该规则。你可以在注释中添加详细信息。- 验证记录：你对该规则的验证记录，包括从哪些时期的ID和规则ID中推导出该规则。[行动]在每个时期，都会在环境中创建一个代理人。代理人只能使用以下动作功能与环境交互：agent.go_to(receptacle)  #
    前往一个容器并更新代理人的位置。agent.open(receptacle)  # 打开一个容器并观察其中的内容。agent.close(receptacle)  #
    关闭一个已打开的容器。agent.take_from(object, receptacle)  # 从容器中拿取一个物品，前提是代理人没有拿着其他物品。agent.put_in_or_on(object,
    receptacle)  # 将物品放入或放在容器上，前提是代理人正在拿着它。agent.use(object)  # 使用一个物品。agent.clean_with(object,
    receptacle)  # 使用容器清洁物品。agent.heat_with(object, receptacle)  # 使用容器加热物品。agent.cool_with(object,
    receptacle)  # 使用容器冷却物品。get_object_with_id(observation, object_name)  # 从观察结果中提取指定物品名称的物品ID列表。[响应说明]输出过程：收到当前规则后，你应该输出以下内容：*
    综合理解：描述你对所有规则的总体理解和一些特定规则的理解。* 规则分类：系统地分析相关规则之间的联系，然后将这些规则进行分类，并为分类提出分类名称。确保每个规则只能属于一个分类！*
    手册：最后，按照顺序编写一个结构化的手册，手册内首先描述所有规则的概述，然后逐一介绍每个规则分类。在每个分类下，你应列出规则并以**rule_id**的形式标出。详细说明：1.
    根据规则的应用场景和目标分类，而不是按“类型”分类。2. 如果两个“成功过程”规则遵循相同的关键成功点或过程，可以考虑将它们归为一类，并在分类的简介部分提出关键成功点的通用策略。但如果它们不遵循相同的关键成功点，则不必这样做。3.
    为了使手册更易理解，请确保分类和规则按从简单到复杂、从基础到高级的顺序出现。'
