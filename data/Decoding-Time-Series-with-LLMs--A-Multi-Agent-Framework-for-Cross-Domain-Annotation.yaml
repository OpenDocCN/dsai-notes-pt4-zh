- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:02:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:02:56
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解码时间序列与LLMs：跨领域注释的多代理框架
- en: 来源：[https://arxiv.org/html/2410.17462/](https://arxiv.org/html/2410.17462/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.17462/](https://arxiv.org/html/2410.17462/)
- en: Minhua Lin¹, Zhengzhang Chen², Yanchi Liu², Xujiang Zhao²,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 林敏华¹、陈正章²、刘彦池²、赵旭江²
- en: Zongyu Wu¹, Junxiang Wang², Xiang Zhang¹, Suhang Wang¹, Haifeng Chen²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 吴宗宇¹、王俊翔²、张翔¹、王苏航¹、陈海峰²
- en: ¹The Pennsylvania State University ²NEC Laboratories America
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹宾夕法尼亚州立大学 ²NEC美国实验室
- en: '{mfl5681,zongyuwu,xzz89,szw494}@psu.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{mfl5681,zongyuwu,xzz89,szw494}@psu.edu'
- en: '{zchen,yanchi,xuzhao,junwang,haifeng}@nec-labs.com Work done during an internship
    at NEC Labs America.Corresponding author.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{zchen,yanchi,xuzhao,junwang,haifeng}@nec-labs.com 该工作在NEC美国实验室的实习期间完成。通讯作者。'
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Time series data is ubiquitous across various domains, including manufacturing,
    finance, and healthcare. High-quality annotations are essential for effectively
    understanding time series and facilitating downstream tasks; however, obtaining
    such annotations is challenging, particularly in mission-critical domains. In
    this paper, we propose TESSA, a multi-agent system designed to automatically generate
    both general and domain-specific annotations for time series data. TESSA introduces
    two agents: a general annotation agent and a domain-specific annotation agent.
    The general agent captures common patterns and knowledge across multiple source
    domains, leveraging both time-series-wise and text-wise features to generate general
    annotations. Meanwhile, the domain-specific agent utilizes limited annotations
    from the target domain to learn domain-specific terminology and generate targeted
    annotations. Extensive experiments on multiple synthetic and real-world datasets
    demonstrate that TESSA effectively generates high-quality annotations, outperforming
    existing methods.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据在各个领域中无处不在，包括制造业、金融业和医疗保健。高质量的注释对于有效理解时间序列并促进下游任务至关重要；然而，获取这样的注释是具有挑战性的，尤其是在关键任务领域。在本文中，我们提出了TESSA，一个多代理系统，旨在自动生成时间序列数据的通用和领域特定注释。TESSA引入了两个代理：一个通用注释代理和一个领域特定注释代理。通用代理通过利用时间序列特征和文本特征，捕捉跨多个源领域的常见模式和知识，从而生成通用注释。与此同时，领域特定代理利用目标领域的有限注释来学习领域特定术语，并生成针对性的注释。在多个合成和真实世界数据集上的广泛实验表明，TESSA能够有效地生成高质量的注释，表现超过现有方法。
- en: 'Decoding Time Series with LLMs: A Multi-Agent Framework'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 解码时间序列与LLMs：多代理框架
- en: for Cross-Domain Annotation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 用于跨领域注释
- en: 'Minhua Lin¹^†^†thanks: Work done during an internship at NEC Labs America.,
    Zhengzhang Chen²^†^†thanks: Corresponding author., Yanchi Liu², Xujiang Zhao²,
    Zongyu Wu¹, Junxiang Wang², Xiang Zhang¹, Suhang Wang¹, Haifeng Chen² ¹The Pennsylvania
    State University ²NEC Laboratories America {mfl5681,zongyuwu,xzz89,szw494}@psu.edu
    {zchen,yanchi,xuzhao,junwang,haifeng}@nec-labs.com'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 林敏华¹^†^†感谢：该工作在NEC美国实验室的实习期间完成。陈正章²^†^†感谢：通讯作者。刘彦池²、赵旭江²、吴宗宇¹、王俊翔²、张翔¹、王苏航¹、陈海峰²
    ¹宾夕法尼亚州立大学 ²NEC美国实验室 {mfl5681,zongyuwu,xzz89,szw494}@psu.edu {zchen,yanchi,xuzhao,junwang,haifeng}@nec-labs.com
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Time series data is prevalent in fields such as manufacturing Hsu and Liu ([2021](https://arxiv.org/html/2410.17462v1#bib.bib15)),
    finance Lee et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib19)), and
    healthcare Cascella et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib7)),
    where it captures critical temporal patterns essential for informed decision-making.
    However, general users frequently encounter difficulties in interpreting this
    data due to its inherent complexity, particularly in multivariate contexts where
    multiple variables interact over time. Furthermore, effective interpretation typically
    requires domain-specific knowledge to properly contextualize these patterns, thereby
    posing significant challenges for individuals without specialized expertise.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据在制造业Hsu和Liu（[2021](https://arxiv.org/html/2410.17462v1#bib.bib15)）、金融业Lee等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib19)）以及医疗保健Cascella等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib7)）等领域中普遍存在，在这些领域中，时间序列数据捕捉着决策所需的关键时间模式。然而，普通用户常常在解读这些数据时遇到困难，尤其是在多变量的情境下，多个变量随时间的相互作用使得数据变得复杂。此外，准确的解读通常需要领域特定的知识，以便正确地对这些模式进行上下文化处理，这给没有专业知识的人带来了重大挑战。
- en: High-quality annotations are crucial for addressing these interpretive challenges.
    Annotations provide meaningful context or insights into the time series data,
    highlighting important patterns, events, or anomalies. They facilitate accurate
    analysis, forecasting, and decision-making, enhancing the performance of downstream
    tasks such as anomaly detection, trend prediction, and automated reporting. For
    instance, in predictive maintenance, understanding sensor data trends is vital
    for preventing equipment failure, while in finance, interpreting stock price movements
    is crucial for informed investment strategies. Despite their importance, high-quality
    annotations are often scarce in real-world applications. This scarcity stems primarily
    from the reliance on domain experts for manual annotation, which is resource-intensive,
    costly, and prone to inconsistencies. Moreover, the need for precise and domain-specific
    terminology further complicates the annotation process, as different fields require
    highly specialized knowledge for accurate and contextually relevant interpretation.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的注释对于解决这些解释性挑战至关重要。注释提供了对时间序列数据的有意义的背景或见解，突出显示了重要的模式、事件或异常。它们有助于准确的分析、预测和决策，提升下游任务的性能，如异常检测、趋势预测和自动报告。例如，在预测性维护中，理解传感器数据趋势对于防止设备故障至关重要，而在金融领域，解释股票价格波动对于制定明智的投资策略至关重要。尽管注释非常重要，但在实际应用中，高质量的注释往往稀缺。这种稀缺性主要源于对领域专家进行手动注释的依赖，这既费时费力，又昂贵且容易产生不一致性。此外，精确且具有领域特定术语的需求进一步使得注释过程复杂化，因为不同领域需要高度专业化的知识来进行准确且符合上下文的解释。
- en: To alleviate the above issues, one straightforward approach is to leverage external
    resources to generate annotations Liu et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)).
    For example, Time-MMD Liu et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21))
    uses web searches to retrieve information as annotations, aiming to find similar
    patterns and descriptions from the internet. Others Jin et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib18));
    Liu et al. ([2024b](https://arxiv.org/html/2410.17462v1#bib.bib22)) directly apply
    large language models (LLMs) for annotation, leveraging their vast language understanding
    capabilities. Prototype-based methods, such as prototype networks Ni et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib25)),
    have also been employed to identify representative examples for annotation. However,
    these methods often fall short of producing high-quality annotations. Web search-based
    methods may retrieve irrelevant or inconsistent information. LLMs, while powerful,
    tend to generate annotations that are generic, capture only basic patterns, or
    even hallucinate, failing to account for the complex nature of time series data.
    Prototype networks rely on large amounts of data to train the network and identify
    representative prototypes, but the scarcity of high-quality annotations limits
    the quality and representativeness of these prototypes, making it difficult to
    generalize effectively to new or unseen patterns.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解上述问题，一种直接的方法是利用外部资源生成注释，Liu 等人（[2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)）提出了这种方法。例如，Time-MMD
    Liu 等人（[2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)）通过网络搜索来检索信息作为注释，旨在从互联网上寻找相似的模式和描述。其他研究如
    Jin 等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib18)）；Liu 等人（[2024b](https://arxiv.org/html/2410.17462v1#bib.bib22)）则直接应用大型语言模型（LLM）进行注释，利用它们强大的语言理解能力。基于原型的方法，如原型网络
    Ni 等人（[2021](https://arxiv.org/html/2410.17462v1#bib.bib25)），也被用于识别代表性示例进行注释。然而，这些方法往往未能生成高质量的注释。基于网络搜索的方法可能检索到无关或不一致的信息。LLMs
    尽管强大，但往往生成的注释过于通用，仅捕捉到基本模式，甚至会产生幻觉，无法考虑时间序列数据的复杂性。原型网络依赖大量数据训练网络并识别代表性原型，但高质量注释的稀缺性限制了这些原型的质量和代表性，使得其难以有效地推广到新的或未见过的模式。
- en: To address these limitations, we propose to extract knowledge from existing
    annotations across multiple source domains and transfer this knowledge to target
    domains with limited annotations. Specifically, we aim to develop a system that
    can automatically interpret time series data across various fields using either
    common or domain-specific language. Formally, given abundant annotations from
    multiple source domains and limited annotations from a target domain, our goal
    is to leverage both time-series-wise and text-wise knowledge to generate accurate
    and contextually appropriate annotations for the target domain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些局限性，我们提出了从多个源领域的现有标注中提取知识，并将这些知识转移到具有有限标注的目标领域。具体来说，我们旨在开发一个系统，能够使用通用语言或特定领域语言自动解释来自各个领域的时间序列数据。形式上，给定来自多个源领域的丰富标注和来自目标领域的有限标注，我们的目标是利用时间序列和文本的知识，为目标领域生成准确且语境恰当的标注。
- en: 'There are two major technical challenges in developing such a system: (i) How
    to extract common knowledge from source domains? (ii) How to learn domain-specific
    jargon from limited target-domain annotations? To tackle these challenges and
    overcome the limitations of existing methods, we propose TESSA, a multi-agent
    system designed for both general and domain-specific TimE SerieS Annotation. As
    illustrated in Figure [1](https://arxiv.org/html/2410.17462v1#S2.F1 "Figure 1
    ‣ 2 Related Work ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for
    Cross-Domain Annotation"), TESSA introduces two agents: a general annotation agent
    and a domain-specific annotation agent. The general annotation agent focuses on
    capturing common patterns and knowledge across various domains to generate annotations
    understandable by general users. To learn common knowledge from multiple domains,
    the general agent employs a time series-wise feature extractor and a text-wise
    feature extractor to extract both time-series-wise and text-wise features from
    time series data and domain-specific annotations from multiple source domains.
    To ensure important features are included in the general annotations, two feature
    selection methods—LLM-based and reinforcement learning-based selection—are introduced
    to effectively and efficiently select both the top-$k$ most important time-series-wise
    and text-wise features. The domain-specific agent leverages limited target-domain
    annotations to learn and generate annotations for specific domains using domain-specific
    terminologies (jargon). It incorporates a domain-specific term extractor to learn
    jargon from the limited target-domain annotations. Additionally, an annotation
    reviewer is proposed to maintain consistency between general annotations and domain-specific
    annotations.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '开发这样一个系统面临两个主要的技术挑战：（i）如何从源领域提取共通的知识？（ii）如何从有限的目标领域标注中学习特定领域的术语？为了应对这些挑战并克服现有方法的局限性，我们提出了TESSA，一个旨在进行通用和特定领域时间序列标注的多智能体系统。如图[1](https://arxiv.org/html/2410.17462v1#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")所示，TESSA引入了两个智能体：一个是通用标注智能体，另一个是特定领域标注智能体。通用标注智能体专注于捕捉跨多个领域的共通模式和知识，以生成普通用户可以理解的标注。为了从多个领域学习共通知识，通用智能体采用了时间序列特征提取器和文本特征提取器，从时间序列数据和来自多个源领域的特定领域标注中提取时间序列和文本特征。为了确保重要特征包含在通用标注中，提出了两种特征选择方法——基于LLM的选择和基于强化学习的选择——来有效且高效地选择最重要的前$k$个时间序列特征和文本特征。特定领域智能体利用有限的目标领域标注，使用特定领域术语（行话）来学习并生成针对特定领域的标注。它包含了一个特定领域术语提取器，用于从有限的目标领域标注中学习行话。此外，还提出了一个标注审查员，用于保持通用标注和特定领域标注之间的一致性。'
- en: 'Our contributions are: (i) Problem. We explore a novel problem in cross-domain
    multi-modal time series annotation, bridging the gap between general understanding
    and domain-specific interpretation; (ii) Framework. We propose a novel multi-agent
    system, TESSA, designed for both general and domain-specific time series annotation
    by leveraging both time-series-wise and text-wise knowledge from multiple domains;
    (iii) Datasets. We collect a real-world dataset from finance domain to leverage
    cross-domain knowledge, along with a synthetic dataset to evaluate TESSA. These
    datasets are released to support future research and development in this field.
    (iv) Experiments. Extensive experiments on multiple synthetic and real-world datasets
    demonstrate the quality of the general and domain-specific annotations generated
    by TESSA.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献包括：（i）问题。我们探索了跨领域多模态时间序列注释中的一个新问题，架起了通用理解与领域特定解释之间的桥梁；（ii）框架。我们提出了一个新颖的多代理系统TESSA，旨在通过利用来自多个领域的时间序列知识和文本知识，进行通用和领域特定的时间序列注释；（iii）数据集。我们从金融领域收集了一个真实世界的数据集，以便利用跨领域知识，并提供了一个合成数据集来评估TESSA。这些数据集已公开，以支持该领域未来的研究和开发。（iv）实验。在多个合成和真实世界数据集上进行的大量实验表明，TESSA生成的通用和领域特定注释的质量。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '![Refer to caption](img/98f6b401566a4f2681da645164d0b67b.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/98f6b401566a4f2681da645164d0b67b.png)'
- en: 'Figure 1: Overall framework of TESSA. It consists of two main agents: a general
    annotation agent, which generates domain-independent annotations by selecting
    salient time-series and textual features, and a domain-specific annotation agent,
    which refines these annotations by incorporating domain-specific terminology.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：TESSA的整体框架。它由两个主要代理组成：一个通用注释代理，通过选择显著的时间序列和文本特征生成与领域无关的注释；一个领域特定注释代理，通过引入领域特定术语来优化这些注释。
- en: Time Series Annotation. Time series annotation aims to assign labels or descriptions
    to specific segments, events, or patterns within a time series dataset to highlight
    significant features for further analysis. Traditionally, this process has relied
    on manual annotation Reining et al. ([2020](https://arxiv.org/html/2410.17462v1#bib.bib29)),
    which is often time-consuming, labor-intensive, and requires substantial domain
    expertise. To reduce the effort needed for creating large-scale, high-quality
    annotated datasets, several studies have proposed semi-automatic annotation approaches Cruz-Sandoval
    et al. ([2019](https://arxiv.org/html/2410.17462v1#bib.bib11)); Nino et al. ([2016](https://arxiv.org/html/2410.17462v1#bib.bib26))
    that require minimal manual input or post-annotation revisions. Despite these
    advancements, fully automated time series annotation remains underexplored due
    to the challenges of capturing semantic and contextual information from the data Yordanova
    and Krüger ([2018](https://arxiv.org/html/2410.17462v1#bib.bib36)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列注释。时间序列注释的目标是为时间序列数据集中的特定片段、事件或模式分配标签或描述，以突出显著特征，便于进一步分析。传统上，这一过程依赖于人工注释Reining等人（[2020](https://arxiv.org/html/2410.17462v1#bib.bib29)），通常是耗时、劳动密集且需要大量领域专长。为了减少创建大规模高质量注释数据集所需的努力，几项研究提出了半自动注释方法Cruz-Sandoval等人（[2019](https://arxiv.org/html/2410.17462v1#bib.bib11)）；Nino等人（[2016](https://arxiv.org/html/2410.17462v1#bib.bib26)），这些方法需要最少的人工输入或后期注释修改。尽管有这些进展，完全自动化的时间序列注释仍然未得到充分探索，因为从数据中捕捉语义和上下文信息仍然面临挑战Yordanova和Krüger（[2018](https://arxiv.org/html/2410.17462v1#bib.bib36)）。
- en: LLMs for Time Series Analysis. Recent advancements in LLMs have showcased their
    strong capabilities in sequential modeling and pattern recognition, opening up
    promising new directions for time series analysis. Several studies Xue and Salim
    ([2023](https://arxiv.org/html/2410.17462v1#bib.bib33)); Yu et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib37));
    Gruver et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib14)); Jin et al.
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib18)); Li et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib20))
    have explored how LLMs can be effectively leveraged in this context. For instance,
    PromptCast Xue and Salim ([2023](https://arxiv.org/html/2410.17462v1#bib.bib33))
    is a pioneering work that applies LLMs to general time series forecasting using
    a sentence-to-sentence approach. Yu et al. Yu et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib37))
    extend this by investigating the application of LLMs to domain-specific tasks,
    such as financial time series forecasting. LLMTime Gruver et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib14))
    demonstrates the efficacy of LLMs as time series learners by employing text-wise
    tokenization to represent time series data. Time-LLM Jin et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib18))
    reprograms time series data into textual prototypes for input into LLaMA-7B, enriched
    with natural language prompts that include domain expert knowledge and task-specific
    instructions. Additionally, Li et al. Li et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib20))
    illustrate how a frozen language model can enhance zero-shot learning in ECG time
    series analysis, showing the potential of LLMs to extract valuable features from
    complex time series data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析中的LLMs。最近LLMs的进展展示了它们在序列建模和模式识别方面的强大能力，为时间序列分析开辟了有前景的新方向。几项研究，Xue和Salim（[2023](https://arxiv.org/html/2410.17462v1#bib.bib33)）；Yu等（[2023](https://arxiv.org/html/2410.17462v1#bib.bib37)）；Gruver等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib14)）；Jin等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib18)）；Li等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib20)）探讨了如何有效地在此背景下利用LLMs。例如，PromptCast
    Xue和Salim（[2023](https://arxiv.org/html/2410.17462v1#bib.bib33)）是一个开创性的工作，使用句子到句子的方法将LLMs应用于一般的时间序列预测。Yu等（[2023](https://arxiv.org/html/2410.17462v1#bib.bib37)）通过研究LLMs在特定领域任务中的应用，如金融时间序列预测，扩展了这一工作。LLMTime
    Gruver等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib14)）通过采用逐文本标记化表示时间序列数据，展示了LLMs作为时间序列学习者的有效性。Time-LLM
    Jin等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib18)）将时间序列数据重新编程为文本原型，输入LLaMA-7B，并通过包含领域专家知识和任务特定指令的自然语言提示进行增强。此外，Li等（[2024](https://arxiv.org/html/2410.17462v1#bib.bib20)）展示了如何通过冻结的语言模型增强ECG时间序列分析中的零-shot学习，显示了LLMs从复杂时间序列数据中提取有价值特征的潜力。
- en: Cross-modality Knowledge Transfer Learning through Pre-trained Models. There
    has been growing interest in leveraging pre-trained models for cross-modality
    knowledge transfer, particularly between the language, vision, and time series
    domains Bao et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib3)); Lu
    et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib23)); Yang et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib35));
    Zhou et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib40)). For instance,
    Bao et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib3)) proposes a stagewise
    pre-training strategy that trains a language expert using frozen attention blocks
    pre-trained on image-only data. Similarly, Lu et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib23))
    examines the transferability of language models to other domains, while Zhou et al.
    ([2023](https://arxiv.org/html/2410.17462v1#bib.bib40)) applies pre-trained language
    and image models to time series analysis tasks. To the best of our knowledge,
    no previous work has specifically explored cross-modality knowledge transfer for
    time series annotation. Our work aims to fill this gap by investigating the application
    of cross-modality transfer learning in the context of automatic time series annotation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过预训练模型进行跨模态知识迁移学习。近年来，越来越多的研究关注如何利用预训练模型进行跨模态知识迁移，尤其是在语言、视觉和时间序列领域之间 Bao et
    al.（[2022](https://arxiv.org/html/2410.17462v1#bib.bib3)）；Lu et al.（[2022](https://arxiv.org/html/2410.17462v1#bib.bib23)）；Yang
    et al.（[2021](https://arxiv.org/html/2410.17462v1#bib.bib35)）；Zhou et al.（[2023](https://arxiv.org/html/2410.17462v1#bib.bib40)）。例如，Bao
    et al.（[2022](https://arxiv.org/html/2410.17462v1#bib.bib3)）提出了一种阶段性预训练策略，该策略通过冻结的注意力块对图像数据进行预训练，从而训练出一个语言专家。类似地，Lu
    et al.（[2022](https://arxiv.org/html/2410.17462v1#bib.bib23)）研究了语言模型向其他领域的迁移性，而Zhou
    et al.（[2023](https://arxiv.org/html/2410.17462v1#bib.bib40)）将预训练的语言和图像模型应用于时间序列分析任务。据我们所知，之前的研究并未专门探讨时间序列标注中的跨模态知识迁移。我们的工作旨在填补这一空白，研究跨模态迁移学习在自动时间序列标注中的应用。
- en: 3 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: In this section, we define the problem and present the details of our proposed
    TESSA framework, which aims to generate both general and domain-specific annotations
    for time series data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们定义了问题，并介绍了我们提出的TESSA框架的细节，旨在为时间序列数据生成通用和领域特定的标注。
- en: 'Cross-Domain Time Series Annotation Problem. Given several source domains $\{\mathcal{D}_{s_{1}},\mathcal{D}_{s_{2}},\ldots\}$
    and a target domain $\mathcal{D}_{t}$, let $\{e^{1}_{s_{i}},e^{2}_{s_{i}},\ldots\}$
    denote the domain-specific annotations from the source domain $\mathcal{D}_{s_{1}}$,
    and $\{e^{1}_{t},e^{2}_{t},\ldots\}$ represent the limited domain-specific annotations
    from the target domain $\mathcal{D}_{t}$. Suppose $\mathbf{X}=(\mathbf{x}_{1},\cdots,\mathbf{x}_{L})$
    is a time series in $\mathcal{D}_{t}$, where $L$ is the number of past timestamps
    and $\mathbf{x}_{i}=(x_{1i},\cdots,x_{Ci})^{T}\in\mathbb{R}^{C}$ represents the
    data from $C$ different channels at timestamp $i$. The objective of cross-domain
    time series annotation is to generate the general annotation $e_{g}$ and the domain-specific
    annotation $e_{s}$ for $\mathbf{X}$ based on the annotations from both the source
    and target domains. More notations are provided in Appendix [B](https://arxiv.org/html/2410.17462v1#A2
    "Appendix B Notations ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 跨领域时间序列标注问题。给定若干源领域$\{\mathcal{D}_{s_{1}},\mathcal{D}_{s_{2}},\ldots\}$和目标领域$\mathcal{D}_{t}$，令$\{e^{1}_{s_{i}},e^{2}_{s_{i}},\ldots\}$表示来自源领域$\mathcal{D}_{s_{1}}$的领域特定标注，$\{e^{1}_{t},e^{2}_{t},\ldots\}$表示来自目标领域$\mathcal{D}_{t}$的有限领域特定标注。假设$\mathbf{X}=(\mathbf{x}_{1},\cdots,\mathbf{x}_{L})$是目标领域$\mathcal{D}_{t}$中的一条时间序列，其中$L$是过去时间戳的数量，$\mathbf{x}_{i}=(x_{1i},\cdots,x_{Ci})^{T}\in\mathbb{R}^{C}$表示时间戳$i$处来自$C$个不同通道的数据。跨领域时间序列标注的目标是基于源领域和目标领域的标注，为$\mathbf{X}$生成通用标注$e_{g}$和领域特定标注$e_{s}$。更多符号的说明见附录 [B](https://arxiv.org/html/2410.17462v1#A2
    "附录 B 符号 ‣ 使用LLMs解码时间序列：一个多代理框架用于跨领域标注")。
- en: 'Overview of TESSA. As illustrated in Fig. [1](https://arxiv.org/html/2410.17462v1#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation"), the proposed TESSA comprises two key components:
    a general annotation agent and a domain-specific annotation agent. The general
    annotation agent is responsible for generating domain-independent annotations
    and consists of several modules: a time series feature extraction module to capture
    time-series-specific features, a domain decontextualization module to convert
    domain-specific text into common language, a text feature extraction module to
    retrieve textual features from the decontextualized text, two policy networks
    for selecting the top-$k$ most salient time-series and textual features, and a
    general annotator to produce general annotations based on the selected features.
    The domain-specific annotation agent refines the general annotations to generate
    domain-specific annotations. It includes a domain-specific term extractor to identify
    key terminology from a limited set of target-domain annotations and a domain-specific
    annotator to adjust the general annotations accordingly. An annotation reviewer
    further enhances the quality of the domain-specific annotations. Next, we introduce
    details of each component.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: TESSA 概述。如图[1](https://arxiv.org/html/2410.17462v1#S2.F1 "图 1 ‣ 2 相关工作 ‣ 解码时间序列与LLM：跨领域标注的多代理框架")所示，所提出的
    TESSA 包括两个关键组件：通用标注代理和领域特定标注代理。通用标注代理负责生成领域无关的标注，并由几个模块组成：一个时间序列特征提取模块用于捕捉时间序列特有的特征，一个领域去语境化模块将领域特定文本转换为通用语言，一个文本特征提取模块从去语境化文本中提取文本特征，两个策略网络用于选择最重要的
    $k$ 个时间序列和文本特征，以及一个通用标注器基于选定的特征生成通用标注。领域特定标注代理对通用标注进行优化，以生成领域特定的标注。它包括一个领域特定术语提取器，用于从有限的目标领域注释中识别关键术语，以及一个领域特定标注器，根据这些术语调整通用标注。一个标注审阅者进一步提高领域特定标注的质量。接下来，我们将介绍每个组件的详细信息。
- en: 3.1 Multi-modal Feature Extraction
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 多模态特征提取
- en: 'To address the challenge of extracting common knowledge from source domains,
    we introduce two feature extraction modules: a time-series feature extractor and
    a text-wise feature extractor, which extract features from time series data and
    source-domain annotations. We also propose a domain decontextualizer to enhance
    the extraction of common knowledge from multi-source annotations.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对从源领域中提取共同知识的挑战，我们引入了两个特征提取模块：时间序列特征提取器和文本特征提取器，分别从时间序列数据和源领域注释中提取特征。我们还提出了一个领域去语境化器，用于增强从多源注释中提取共同知识的能力。
- en: 'Time Series Feature Extraction. We develop a time series extraction toolbox
    $f_{t}$ to extract various features from given time series data. Formally, for
    each channel $c\in C$, the set of text-series features $\mathbf{F}_{t}$ is denoted
    as:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列特征提取。我们开发了一个时间序列提取工具箱 $f_{t}$ 来从给定的时间序列数据中提取各种特征。形式上，对于每个通道 $c\in C$，时间序列特征集
    $\mathbf{F}_{t}$ 表示为：
- en: '|  | $\mathbf{F}_{t}=\{f_{t}^{1},\cdots,f_{t}^{n_{t}}\}=\mathcal{M}_{r}(\mathbf{X}),$
    |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{F}_{t}=\{f_{t}^{1},\cdots,f_{t}^{n_{t}}\}=\mathcal{M}_{r}(\mathbf{X}),$
    |  | (1) |'
- en: 'where $f^{i}_{t}$ is the $i$-th extracted feature of $\mathbf{X}$, and $n_{t}$
    is the number of extracted features. For multivariate time series data, inter-variable
    features (e.g., Pearson correlation) are also included. More details on feature
    extraction can be found in Appendix [C](https://arxiv.org/html/2410.17462v1#A3
    "Appendix C More Details of Multi-modal Feature Extraction ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f^{i}_{t}$ 是从 $\mathbf{X}$ 提取的第 $i$ 个特征，$n_{t}$ 是提取特征的数量。对于多变量时间序列数据，还包括变量间的特征（例如，皮尔逊相关系数）。关于特征提取的更多细节，请参见附录[C](https://arxiv.org/html/2410.17462v1#A3
    "附录 C 多模态特征提取的更多细节 ‣ 解码时间序列与LLM：跨领域标注的多代理框架")。
- en: Domain Decontextualization. In addition to time-series-wise features, textual
    annotations from source domains often contain valuable information (such as support
    or resilience in finance time series annotations) for interpreting time series
    data. A straightforward method to extract this knowledge is to use LLMs on domain-specific
    annotations, leveraging their real-world knowledge. However, in practice, many
    domains lack sufficient high-quality annotations, and domain-specific terminology
    can further hinder effective extraction.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 领域去语境化。除了时间序列特征外，源领域的文本注释通常包含有价值的信息（例如金融时间序列注释中的支持或弹性），用于解释时间序列数据。提取这些知识的一种直接方法是使用LLM对领域特定注释进行处理，利用其现实世界的知识。然而，在实践中，许多领域缺乏足够高质量的注释，且领域特定术语会进一步妨碍有效的知识提取。
- en: 'To address these challenges and facilitate knowledge transfer from source to
    target domains, we introduce a domain decontextualization LLM to convert domain-specific
    annotations into general annotations by removing domain-specific terminology.
    This makes it easier to extract common knowledge across domains. Specifically,
    given a domain-specific annotation $e_{s}$ in domain $d_{i}$, the decontextualized
    annotation $e_{d}$ is obtained as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战并促进从源领域到目标领域的知识转移，我们引入了一种领域去语境化的LLM，通过去除领域特定的术语，将领域特定的注释转化为通用注释。这使得跨领域提取常识变得更加容易。具体来说，给定源领域$d_{i}$中的领域特定注释$e_{s}$，去语境化的注释$e_{d}$可表示为：
- en: '|  | $e_{d}=\mathcal{M}_{d}(p_{de}(e_{s},d_{i})),$ |  | (2) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{d}=\mathcal{M}_{d}(p_{de}(e_{s},d_{i})),$ |  | (2) |'
- en: where $\mathcal{M}_{d}$ is the domain decontextualization LLM.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{M}_{d}$ 是领域去语境化的大语言模型（LLM）。
- en: 'Text Feature Extraction. After decontextualization, we use an LLM $\mathcal{M}_{l}$
    to extract textual features from multiple source domains. Formally, given a set
    of decontextualized annotations $\{e_{d}^{i}\}_{i=1}^{n_{d}}$ and the text feature
    extractor $\mathcal{M}_{l}$, the extracted textual features are denoted as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 文本特征提取。在去语境化后，我们使用LLM $\mathcal{M}_{l}$ 从多个源领域提取文本特征。形式上，给定一组去语境化的注释$\{e_{d}^{i}\}_{i=1}^{n_{d}}$和文本特征提取器$\mathcal{M}_{l}$，提取出的文本特征表示为：
- en: '|  | $\mathbf{F}_{l}=\{f_{l}^{1},\cdots,f_{l}^{n}\}=\mathcal{M}_{l}(p_{l}(\{e_{d}^{i%
    }\}_{i=1}^{n_{d}})),$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{F}_{l}=\{f_{l}^{1},\cdots,f_{l}^{n}\}=\mathcal{M}_{l}(p_{l}(\{e_{d}^{i%
    }\}_{i=1}^{n_{d}})),$ |  | (3) |'
- en: where $p_{l}$ is the prompt for text feature extraction.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{l}$ 是用于文本特征提取的提示。
- en: 3.2 Adaptive Feature Selection
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自适应特征选择
- en: With a diverse set of features extracted from time series and text data, it
    becomes essential to focus on the most relevant ones to ensure the generated annotations
    remain concise and interpretable. Moreover, repeatedly querying LLMs with both
    the old and new data each time wastes computational resources and incurs additional
    costs, especially when using non-open-source models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于从时间序列和文本数据中提取了多样的特征，因此集中关注最相关的特征变得至关重要，以确保生成的注释保持简洁且具有可解释性。此外，每次用新旧数据反复查询LLM会浪费计算资源并增加额外成本，特别是在使用非开源模型时。
- en: To address these issues, we propose a hybrid strategy for adaptive feature selection
    that combines Offline LLM-based Feature Selection with Incremental Reinforcement
    Learning-based Feature Selection. The incremental method builds on the offline
    approach, minimizing the need to re-query LLMs with both old and new data as it
    arrives.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们提出了一种自适应特征选择的混合策略，结合了基于离线LLM的特征选择和基于增量强化学习的特征选择。增量方法以离线方法为基础，最大程度减少了随着新旧数据的到来，需要重新查询LLM的次数。
- en: Offline LLM-based Feature Selection. Leveraging LLMs’ reasoning abilities, we
    introduce a feature selection method using LLM-generated feature importance scores
    to identify the top-$k$ most important text-series-wise and text-wise features.
    Features mentioned more frequently—either explicitly or implicitly—in annotations
    are assigned higher importance scores.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基于离线LLM的特征选择。利用LLM的推理能力，我们引入了一种特征选择方法，通过LLM生成的特征重要性分数，识别出最重要的前$k$个特征，包括按文本系列和文本进行的特征。那些在注释中被更频繁提及的特征，无论是明确的还是隐含的，都被赋予更高的重要性分数。
- en: 'Specifically, given an LLM as the feature selector $\mathcal{M}_{sel}$, we
    prompt $\mathcal{M}_{sel}$ with domain-decontextualized annotations $\{e_{d}^{i}\}_{i=1}^{n_{d}}$
    and the extracted features $\{f_{t}^{i}\}_{i=1}^{n_{t}}$ and $\{f_{l}^{i}\}_{i=1}^{n_{l}}$
    to generate numerical feature importance scores: $\mathbf{s}_{t}=[s_{1},\cdots,s_{n_{t}}]$
    for time-series-wise features and $\mathbf{s}_{l}=[s_{1},\cdots,s_{n_{l}}]$ for
    text-wise features.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，给定一个作为特征选择器的LLM $\mathcal{M}_{sel}$，我们通过领域去情境化的注释 $\{e_{d}^{i}\}_{i=1}^{n_{d}}$
    和提取的特征 $\{f_{t}^{i}\}_{i=1}^{n_{t}}$ 以及 $\{f_{l}^{i}\}_{i=1}^{n_{l}}$ 提示 $\mathcal{M}_{sel}$
    生成数值特征重要性分数：时间序列特征的 $\mathbf{s}_{t}=[s_{1},\cdots,s_{n_{t}}]$ 和文本特征的 $\mathbf{s}_{l}=[s_{1},\cdots,s_{n_{l}}]$。
- en: '|  | $\displaystyle s_{j}=\mathcal{M}_{sel}({p}_{{score}}(f_{t}^{j},\{e_{d}^{i}\}_{i%
    =1}^{n_{d}})),\ \ \forall{j}\in\{1,\cdots,n_{t}\},$ |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{j}=\mathcal{M}_{sel}({p}_{{score}}(f_{t}^{j},\{e_{d}^{i}\}_{i%
    =1}^{n_{d}})),\ \ \forall{j}\in\{1,\cdots,n_{t}\},$ |  | (4) |'
- en: '|  | $\displaystyle s_{k}=\mathcal{M}_{sel}({p}_{{score}}(f_{l}^{k},\{e_{d}^{i}\}_{i%
    =1}^{n_{d}})),\ \ \forall{k}\in\{1,\cdots,n_{l}\},$ |  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s_{k}=\mathcal{M}_{sel}({p}_{{score}}(f_{l}^{k},\{e_{d}^{i}\}_{i%
    =1}^{n_{d}})),\ \ \forall{k}\in\{1,\cdots,n_{l}\},$ |  |'
- en: 'Here, $p_{score}$ is the prompt used to score feature importance. Higher scores,
    $s_{j}$ and $s_{k}\in\mathbb{R}^{+}$, indicate that the features $f_{t}^{j}$ and
    $f_{l}^{k}$ appear more frequently, either explicitly or implicitly, in the domain-decontextualized
    annotations $\{e_{d}^{i}\}_{i=1}^{n_{d}}$. To ensure that explicitly mentioned
    features receive higher importance scores, we instruct $\mathcal{M}_{sel}$ to
    assign greater weight to features that are explicitly referenced in the annotations.
    Further details are in Appendix [D.1](https://arxiv.org/html/2410.17462v1#A4.SS1
    "D.1 Offline LLM-based Feature Selection ‣ Appendix D More Details of Adaptive
    Feature Selection ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for
    Cross-Domain Annotation").'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$p_{score}$ 是用于评估特征重要性的提示。较高的分数 $s_{j}$ 和 $s_{k} \in \mathbb{R}^{+}$ 表示特征
    $f_{t}^{j}$ 和 $f_{l}^{k}$ 在领域去情境化的注释 $\{e_{d}^{i}\}_{i=1}^{n_{d}}$ 中出现得更频繁，无论是显式的还是隐式的。为了确保显式提到的特征获得更高的权重分数，我们指示
    $\mathcal{M}_{sel}$ 对在注释中显式引用的特征赋予更大的权重。更多细节见附录 [D.1](https://arxiv.org/html/2410.17462v1#A4.SS1
    "D.1 Offline LLM-based Feature Selection ‣ Appendix D More Details of Adaptive
    Feature Selection ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for
    Cross-Domain Annotation")。'
- en: Incremental Reinforcement Learning-based Feature Selection. When new data arrives,
    the offline LLM-based approach requires re-querying both old and new data, which
    becomes burdensome due to LLMs’ limited context window. As annotations increase,
    re-querying all data becomes impractical and costly, leading to higher resource
    consumption and reduced cost-effectiveness.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于增量强化学习的特征选择。当新数据到达时，离线基于LLM的方法需要重新查询旧数据和新数据，由于LLM的上下文窗口有限，这会变得非常繁琐。随着注释的增加，重新查询所有数据变得不切实际且成本高昂，导致资源消耗增加，性价比降低。
- en: 'To address the limitations of the offline approach, we propose an Incremental
    Reinforcement Learning-based Feature Selection method that is more cost-effective
    for dynamic environments with evolving data. Specifically, we introduce a multi-agent
    reinforcement learning (MARL) framework to train two policy networks, $\mathcal{F}_{t}$
    and $\mathcal{F}_{l}$, to select the top-$k$ most important time-series-wise and
    text-wise features, respectively. These policy networks store knowledge from existing
    annotations and are incrementally updated as new data arrives. This reduces the
    need to re-query the LLM with all the data, requiring only the new data during
    updates. As shown in Figure [1](https://arxiv.org/html/2410.17462v1#S2.F1 "Figure
    1 ‣ 2 Related Work ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for
    Cross-Domain Annotation"), each policy network is initialized with the first three
    layers of a small LLM, such as GPT-2 Radford et al. ([2019](https://arxiv.org/html/2410.17462v1#bib.bib28)),
    which remain frozen during training. A trainable multi-head attention layer and
    a language model (LM) head from GPT-2 follow these layers, using the smallest
    version of GPT-2 with 124M parameters.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决离线方法的局限性，我们提出了一种基于增量强化学习的特征选择方法，更适用于具有不断变化数据的动态环境。具体来说，我们引入了一个多智能体强化学习（MARL）框架，训练两个策略网络
    $\mathcal{F}_{t}$ 和 $\mathcal{F}_{l}$，分别选择最重要的前 $k$ 个时间序列级特征和文本级特征。这些策略网络存储现有注释中的知识，并在新数据到达时逐步更新。这样就减少了每次更新时需要重新查询所有数据的需求，仅需查询新数据。正如图 [1](https://arxiv.org/html/2410.17462v1#S2.F1
    "图 1 ‣ 2 相关工作 ‣ 使用LLM解码时间序列：一个多智能体框架用于跨领域注释")所示，每个策略网络都用一个小型LLM的前三层进行初始化，如GPT-2 Radford等人（[2019](https://arxiv.org/html/2410.17462v1#bib.bib28)），这些层在训练过程中保持冻结。接下来是一个可训练的多头注意力层和一个来自GPT-2的语言模型（LM）头，使用的是最小版本的GPT-2，具有124M参数。
- en: 'During training, only the multi-head attention layer is updated. For time-series-wise
    features, given the candidate features $\{f_{t}^{i}\}_{i=1}^{n_{t}}$ and their
    corresponding feature name tokens $\mathbf{Y}=\{y^{i}_{1},\cdots,y^{i}_{n_{t}}\}$,
    the policy network $\mathcal{F}_{t}$ computes action-values (Q-values) $\mathbf{q}_{z}=[q_{z,f_{t}^{1}},\cdots,q_{z,f_{t}^{n_{t}}}]$
    based on the mean logits of the feature names:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，只有多头注意力层会被更新。对于时间序列级特征，给定候选特征 $\{f_{t}^{i}\}_{i=1}^{n_{t}}$ 及其对应的特征名称标记
    $\mathbf{Y}=\{y^{i}_{1},\cdots,y^{i}_{n_{t}}\}$，策略网络 $\mathcal{F}_{t}$ 基于特征名称的均值
    logits 计算动作值（Q值）$\mathbf{q}_{z}=[q_{z,f_{t}^{1}},\cdots,q_{z,f_{t}^{n_{t}}}]$：
- en: '|  | $\displaystyle\mathbf{q}_{s}=\mathcal{F}_{t}(\{y_{i}\}_{i=1}^{n_{t}}),$
    |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbf{q}_{s}=\mathcal{F}_{t}(\{y_{i}\}_{i=1}^{n_{t}}),$
    |  | (5) |'
- en: A softmax function generates a probability distribution over the features, and
    the top-$k$ features are selected based on the highest probabilities.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数生成一个特征的概率分布，前 $k$ 个特征基于最高概率被选择。
- en: 'At each timestep, the selected top-$k$ features are passed to the LLM $\mathcal{M}_{sel}$
    to obtain their importance scores $s_{i},\forall i\in\{1,\cdots,k\}$. The agent
    receives a reward $r_{t}$ defined as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，选择的前 $k$ 个特征会传递给LLM $\mathcal{M}_{sel}$，以获得它们的重要性分数 $s_{i},\forall i\in\{1,\cdots,k\}$。智能体接收一个奖励
    $r_{t}$，其定义为：
- en: '|  | $\displaystyle r_{t}=\begin{cases}\sum_{i=1}^{k}s_{i},&s_{i}\geq\tau\\
    -0.5,&\text{otherwise},\end{cases}$ |  | (6) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r_{t}=\begin{cases}\sum_{i=1}^{k}s_{i},&s_{i}\geq\tau\\
    -0.5,&\text{否则},\end{cases}$ |  | (6) |'
- en: where $\tau$ is a threshold to discourage selecting unimportant features. The
    text-wise feature policy network $\mathcal{F}_{l}$ undergoes a similar training
    process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\tau$ 是一个阈值，用于抑制选择不重要的特征。文本级特征策略网络 $\mathcal{F}_{l}$ 经过类似的训练过程。
- en: After training, the policy networks are incrementally updated with only new
    data, eliminating the need to re-query the LLM with both old and new data. This
    approach improves the scalability and efficiency of feature selection while reducing
    computational costs, effectively overcoming the offline approach’s limitations.
    By incrementally updating the policy networks, we ensure that feature selection
    remains scalable and cost-effective in dynamic environments with evolving data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，策略网络仅使用新数据进行增量更新，消除了重新查询LLM的需要，不必每次都用旧数据和新数据一起查询。这种方法提高了特征选择的可扩展性和效率，同时降低了计算成本，有效克服了离线方法的局限性。通过增量更新策略网络，我们确保在数据不断变化的动态环境中，特征选择保持可扩展和具有成本效益。
- en: 3.3 General Annotation Generation
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 一般注释生成
- en: 'After selecting the top-$k$ most important features from both time-series and
    text, a general annotator is introduced to generate general annotations by analyzing
    these selected features. An LLM, serving as the general annotator, interprets
    the given time series data based on the selected features. Formally, given time
    series data $\mathbf{X}=\{\mathbf{x}_{i}\}_{i=1}^{L}$ and the selected time-series-wise
    and text-wise features $\{f_{t}^{i}\}_{i=1}^{k_{t}}$ and $\{f_{l}^{i}\}_{i=1}^{k_{l}}$,
    the generation of a general annotation $e_{g}$ is represented as:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在从时间序列和文本中选择出最重要的前$k$个特征之后，引入通用注释器，通过分析这些选定的特征来生成通用注释。作为通用注释器的LLM，根据所选特征解释给定的时间序列数据。形式上，给定时间序列数据$\mathbf{X}=\{\mathbf{x}_{i}\}_{i=1}^{L}$以及所选的时间序列特征和文本特征$\{f_{t}^{i}\}_{i=1}^{k_{t}}$和$\{f_{l}^{i}\}_{i=1}^{k_{l}}$，生成通用注释$e_{g}$的表示式为：
- en: '|  | $\displaystyle e_{g}=\mathcal{M}_{gen}(p_{gen}(\{\mathbf{x}_{i}\}_{i=1}^{L},\{f%
    _{t}^{i}\}_{i=1}^{k_{t}},\{f_{l}^{i}\}_{i=1}^{k_{l}})),$ |  | (7) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle e_{g}=\mathcal{M}_{gen}(p_{gen}(\{\mathbf{x}_{i}\}_{i=1}^{L},\{f_{t}^{i}\}_{i=1}^{k_{t}},\{f_{l}^{i}\}_{i=1}^{k_{l}})),$
    |  | (7) |'
- en: where $p_{gen}$ is the prompt for generating general annotations. By emphasizing
    the signal from the selected common knowledge, the general annotations capture
    richer patterns that may be overlooked when directly applying LLMs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{gen}$ 是生成通用注释的提示词。通过强调所选共识知识的信号，通用注释能够捕捉更丰富的模式，这些模式可能会在直接应用LLMs时被忽视。
- en: 3.4 Domain-specific Annotation Generation
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 领域特定注释生成
- en: Generating domain-specific annotations for time series is crucial as different
    domains rely on specialized jargon and context-specific terminology to accurately
    interpret and understand data. Time series data from financial markets, healthcare
    systems, or industrial processes can exhibit patterns, trends, and anomalies that
    are unique to each domain. General annotations may overlook critical nuances,
    whereas domain-specific annotations capture contextual relevance, improving the
    precision and reliability of downstream analysis or model predictions. By tailoring
    annotations to a domain’s specific lexicon, we can detect meaningful patterns
    more accurately and make informed decisions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为时间序列生成领域特定注释至关重要，因为不同领域依赖于专业术语和上下文特定的词汇来准确解释和理解数据。来自金融市场、医疗系统或工业过程的时间序列数据，可能展示出每个领域特有的模式、趋势和异常。通用注释可能忽视关键的细微差别，而领域特定注释能够捕捉到上下文的相关性，从而提高下游分析或模型预测的精度和可靠性。通过根据领域特定的词汇量身定制注释，我们可以更准确地检测有意义的模式，并做出明智的决策。
- en: 'Domain-specific Term Extractor. To address the challenge of learning domain-specific
    terminology, we introduce a domain-specific term extractor. Given limited domain-specific
    annotations $\{e_{t}^{i}\}_{i=1}^{n_{e_{t}}}$ from the target domain, an LLM $\mathcal{M}_{ext}$
    is employed to extract domain-specific terms. We prompt $\mathcal{M}_{ext}$ with
    the annotations $\{e_{t}^{i}\}_{i=1}^{n_{e_{t}}}$ to extract a set of domain-specific
    terms $\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}$:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定术语提取器。为了应对学习领域特定术语的挑战，我们引入了一个领域特定术语提取器。在有限的目标领域领域特定注释$\{e_{t}^{i}\}_{i=1}^{n_{e_{t}}}$的基础上，采用LLM
    $\mathcal{M}_{ext}$来提取领域特定术语。我们通过注释$\{e_{t}^{i}\}_{i=1}^{n_{e_{t}}}$来提示$\mathcal{M}_{ext}$，从中提取一组领域特定术语$\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}$：
- en: '|  | $\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}=\mathcal{M}_{ext}(p_{ext}(\{e_{t}^%
    {i}\}_{i=1}^{n_{e_{t}}})),$ |  | (8) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}=\mathcal{M}_{ext}(p_{ext}(\{e_{t}^{i}\}_{i=1}^{n_{e_{t}}})),$
    |  | (8) |'
- en: where $n_{\mathcal{J}}$ is the number of extracted terms, and $p_{ext}$ is the
    prompt for domain-specific term extraction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n_{\mathcal{J}}$ 是提取的术语数量，$p_{ext}$ 是领域特定术语提取的提示词。
- en: 'Domain-specific Annotator. To ensure alignment between domain-specific and
    general annotations, an LLM $\mathcal{M}_{spe}$, acting as a domain-specific annotator,
    applies the extracted terms $\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}$ to general
    annotations $e_{g}$, converting them into target-domain annotations $e_{t}$. Formally,
    this is represented as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 领域特定注释器。为了确保领域特定注释和通用注释之间的一致性，一个作为领域特定注释器的LLM $\mathcal{M}_{spe}$，将提取的术语$\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}}$应用于通用注释$e_{g}$，将其转换为目标领域注释$e_{t}$。形式上表示为：
- en: '|  | $e_{t}=\mathcal{M}_{spe}(p_{spe}(e_{g},\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J%
    }}})),$ |  | (9) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{t}=\mathcal{M}_{spe}(p_{spe}(e_{g},\{\mathcal{J}^{i}\}_{i=1}^{n_{\mathcal{J}}})),$
    |  | (9) |'
- en: where $p_{spe}$ is the prompt for generating domain-specific annotations.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{spe}$ 是生成领域特定注释的提示词。
- en: 'Annotation Reviewer. To improve the quality of domain-specific annotations
    and ensure better alignment with general annotations, we introduce an annotation
    reviewer. This LLM, $\mathcal{M}_{rev}$, reviews the generated annotations and
    extracted terms, providing feedback $e_{f}$ to the extractor and annotator:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注释审查员。为了提高领域特定注释的质量，并确保与通用注释的更好对齐，我们引入了一个注释审查员。这个 LLM，$\mathcal{M}_{rev}$，审查生成的注释和提取的术语，为提取器和注释者提供反馈
    $e_{f}$：
- en: '|  | $e_{f}=\mathcal{M}_{rev}(p_{rev}(e_{g},e_{t},\{\mathcal{J}^{i}\}_{i=1}^{n_{%
    \mathcal{J}}})),$ |  | (10) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $e_{f}=\mathcal{M}_{rev}(p_{rev}(e_{g},e_{t},\{\mathcal{J}^{i}\}_{i=1}^{n_{%
    \mathcal{J}}})),$ |  | (10) |'
- en: where $p_{rev}$ is the prompt for reviewing annotations. This feedback loop
    ensures more precise term extraction and better alignment between general and
    domain-specific annotations. Based on the feedback, the extractor $\mathcal{M}_{ext}$
    refines the extraction process, and the annotator $\mathcal{M}_{spe}$ enhances
    its annotations accordingly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{rev}$ 是用于审查注释的提示。这个反馈回路确保了更精确的术语提取，并且通用注释与领域特定注释之间的对齐更好。基于反馈，提取器 $\mathcal{M}_{ext}$
    改进了提取过程，而注释者 $\mathcal{M}_{spe}$ 相应地增强了其注释。
- en: 4 Experiments
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: This section presents the experimental results. We first evaluate the TESSA’s
    annotations in downstream tasks and on a synthetic dataset, then examine domain-specific
    annotations, and finally assess the contribution of key TESSA components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了实验结果。我们首先评估了 TESSA 在下游任务中的注释效果以及在合成数据集上的表现，然后检查了领域特定的注释，最后评估了关键 TESSA 组件的贡献。
- en: 4.1 Experimental Setup
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'Dataset. To evaluate the effectiveness of TESSA, five real-world datasets from
    distinct domains are considered: Stock, Health, Energy, Environment, and Social
    Good. Specifically, the stock dataset includes 1,935 US stocks with the recent
    6-year data, collected by ourselves. The other four datasets come from the public
    benchmark Time-MMD Liu et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)).
    In this paper, the Stock and Health datasets serve as the source domains, while
    the Energy, Environment, and Social Good datasets are treated as the target domains.
    Additionally, we generate a synthetic dataset containing both time series and
    ground-truth annotations to directly assess the quality of general annotations.
    More details on these datasets can be found in Appendix [E.1](https://arxiv.org/html/2410.17462v1#A5.SS1
    "E.1 Dataset Statistics ‣ Appendix E Experimental Settings ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The rest four
    datasets are from a public benchmark Time-MMD. Additionally, we create a synthetic
    dataset with both time series and ground-truth annotations to directly evaluate
    the quality of general annotations. More details of these datasets are in Appendix [E.1](https://arxiv.org/html/2410.17462v1#A5.SS1
    "E.1 Dataset Statistics ‣ Appendix E Experimental Settings ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。为了评估 TESSA 的有效性，考虑了来自不同领域的五个真实世界数据集：股票、健康、能源、环境和社会公益。具体来说，股票数据集包括 1,935
    个美国股票，涵盖最近六年的数据，由我们自己收集。其他四个数据集来自公共基准 Time-MMD Liu 等人 ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21))。在本文中，股票和健康数据集作为源领域，而能源、环境和社会公益数据集被视为目标领域。此外，我们生成了一个合成数据集，包含时间序列和真实标注，用于直接评估通用注释的质量。有关这些数据集的更多细节，请参见附录
    [E.1](https://arxiv.org/html/2410.17462v1#A5.SS1 "E.1 Dataset Statistics ‣ Appendix
    E Experimental Settings ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")。另外，其他四个数据集来自公共基准 Time-MMD。我们还创建了一个合成数据集，包含时间序列和真实标注，用于直接评估通用注释的质量。这些数据集的更多细节请见附录
    [E.1](https://arxiv.org/html/2410.17462v1#A5.SS1 "E.1 Dataset Statistics ‣ Appendix
    E Experimental Settings ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")。'
- en: LLMs. Our experiments utilize one closed-source model, GPT-4o Achiam et al.
    ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1)) and two open-source models,
    LLaMA3.1-8B Dubey et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib12))
    and Qwen2-7B Yang et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib34)).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs。我们的实验使用了一个闭源模型 GPT-4o Achiam 等人 ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1))
    和两个开源模型 LLaMA3.1-8B Dubey 等人 ([2024](https://arxiv.org/html/2410.17462v1#bib.bib12))
    以及 Qwen2-7B 杨等人 ([2024](https://arxiv.org/html/2410.17462v1#bib.bib34))。
- en: 4.2 Evaluating General Annotations in Downstream Tasks
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 在下游任务中评估通用注释
- en: 'In this subsection, to evaluate the quality of the general annotations, we
    apply the generated annotations to the multi-modal downstream tasks (i.e., time
    series forecasting and imputation) by following the experimental setup in Time-MMD Liu
    et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)). The implementation
    details are provided in Appendix  [F.1](https://arxiv.org/html/2410.17462v1#A6.SS1
    "F.1 Implementation Details ‣ Appendix F Additional Results for General Annotation
    Evaluation in Downstream Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation").'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '在本小节中，为了评估通用注释的质量，我们通过遵循《Time-MMD》Liu 等人（[2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)）中的实验设置，将生成的注释应用于多模态下游任务（即时间序列预测和填补）。实现细节请参见附录
    [F.1](https://arxiv.org/html/2410.17462v1#A6.SS1 "F.1 Implementation Details ‣
    Appendix F Additional Results for General Annotation Evaluation in Downstream
    Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation")。'
- en: 'Baselines. To the best of our knowledge, TESSA is the first work to study cross-domain
    multi-modal time series annotation. To demonstrate its effectiveness, we compare
    it with several representative methods, including No-Text, Time-MMD Liu et al.
    ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)), and DirectLLM as baselines.
    More details on these methods are in Appendix [E.2](https://arxiv.org/html/2410.17462v1#A5.SS2
    "E.2 Baseline Methods ‣ Appendix E Experimental Settings ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '基准方法。据我们所知，TESSA 是首个研究跨领域多模态时间序列注释的工作。为了展示其有效性，我们将其与几个代表性方法进行比较，包括 No-Text、Time-MMD
    Liu 等人（[2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)）和 DirectLLM 作为基准。更多关于这些方法的细节请参见附录
    [E.2](https://arxiv.org/html/2410.17462v1#A5.SS2 "E.2 Baseline Methods ‣ Appendix
    E Experimental Settings ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")。'
- en: Evaluation Metrics. For the time series forecasting task, we use MSE (Mean Squared
    Error) and MAE (Mean Absolute Error) as evaluation metrics, where lower values
    for both MSE and MAE mean better annotations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。对于时间序列预测任务，我们使用 MSE（均方误差）和 MAE（平均绝对误差）作为评估指标，其中 MSE 和 MAE 的值越低，表示注释效果越好。
- en: 'Experimental Results. Table [1](https://arxiv.org/html/2410.17462v1#S4.T1 "Table
    1 ‣ 4.2 Evaluating General Annotations in Downstream Tasks ‣ 4 Experiments ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") presents
    the comparison results for the time series forecasting task, where Informer Zhou
    et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib39)) is the forecasting
    model and GPT-4o Achiam et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1))
    serves as the LLM backbone. Additional forecasting results using different LLM
    backbones are available in Appendix [F.2](https://arxiv.org/html/2410.17462v1#A6.SS2
    "F.2 Evaluation in Time Series Forecasting Tasks ‣ Appendix F Additional Results
    for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The following observations
    can be made: (1) No-Text shows the worst performance across all datasets, validating
    the need for annotations to improve performance in downstream tasks. This suggests
    that better downstream task performance indicates higher-quality annotations.
    (2) TESSA achieves the best performance among all compared methods, demonstrating
    its effectiveness in generating high-quality general annotations. Additional results
    of time series imputation tasks, can be found in Appendix [F.3](https://arxiv.org/html/2410.17462v1#A6.SS3
    "F.3 Evaluation in Time Series Imputation Tasks ‣ Appendix F Additional Results
    for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果。表格 [1](https://arxiv.org/html/2410.17462v1#S4.T1 "Table 1 ‣ 4.2 Evaluating
    General Annotations in Downstream Tasks ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")展示了时间序列预测任务的比较结果，其中Informer Zhou
    et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib39))是预测模型，GPT-4o Achiam
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1))作为LLM骨干网络。使用不同LLM骨干网络的附加预测结果可在附录 [F.2](https://arxiv.org/html/2410.17462v1#A6.SS2
    "F.2 Evaluation in Time Series Forecasting Tasks ‣ Appendix F Additional Results
    for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中找到。可以得出以下观察结论：（1）No-Text在所有数据集上表现最差，验证了需要注释来提升下游任务的性能。这表明，更好的下游任务性能意味着更高质量的注释。（2）TESSA在所有比较方法中表现最佳，展示了其在生成高质量通用注释方面的有效性。时间序列填补任务的附加结果可在附录 [F.3](https://arxiv.org/html/2410.17462v1#A6.SS3
    "F.3 Evaluation in Time Series Imputation Tasks ‣ Appendix F Additional Results
    for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中找到。'
- en: 'Table 1: Forecasting results with GPT-4o as the LLM backbone. NT, TM, and DL
    refer to No-Text, Time-MMD, and DirectLLM, respectively. MSE is shown in the top
    half and MAE in the bottom half.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 1：使用GPT-4o作为LLM骨干网络的预测结果。NT、TM和DL分别表示No-Text、Time-MMD和DirectLLM。上半部分显示MSE，下半部分显示MAE。
- en: '| Domain | NT | TM | DL | TESSA |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | NT | TM | DL | TESSA |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Environment | 1.2542 | 0.8483 | 0.7714 | 0.4629 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 1.2542 | 0.8483 | 0.7714 | 0.4629 |'
- en: '| Energy | 2.0117 | 0.2172 | 0.0575 | 0.0482 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | 2.0117 | 0.2172 | 0.0575 | 0.0482 |'
- en: '| Social Good | 2.1457 | 1.6072 | 0.4639 | 0.1935 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | 2.1457 | 1.6072 | 0.4639 | 0.1935 |'
- en: '| Environment | 0.7387 | 0.6865 | 0.6604 | 0.4424 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 0.7387 | 0.6865 | 0.6604 | 0.4424 |'
- en: '| Energy | 1.1663 | 0.2139 | 0.0055 | 0.0040 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | 1.1663 | 0.2139 | 0.0055 | 0.0040 |'
- en: '| Social Good | 1.1205 | 0.9731 | 0.3801 | 0.0825 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | 1.1205 | 0.9731 | 0.3801 | 0.0825 |'
- en: 4.3 Evaluating General Annotations in Synthetic Datasets
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 在合成数据集上评估通用注释
- en: 'We construct a synthetic dataset with time series data and ground-truth annotations
    to validate TESSA’s performance. Implementation details are provided in Appendix [G.1](https://arxiv.org/html/2410.17462v1#A7.SS1
    "G.1 Implementation Details ‣ Appendix G Additional Details of General Annotation
    Evaluation in Synthetic Datasets ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation").'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '我们构建了一个包含时间序列数据和真实注释的合成数据集，以验证TESSA的性能。实施细节可在附录 [G.1](https://arxiv.org/html/2410.17462v1#A7.SS1
    "G.1 Implementation Details ‣ Appendix G Additional Details of General Annotation
    Evaluation in Synthetic Datasets ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")中找到。'
- en: 'Evaluation Metrics. We apply the LLM-as-a-judge approach Bubeck et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib5));
    Dubois et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib13)), evaluating
    two metrics: Clarity and Comprehensiveness. Two distinct LLMs score the generated
    annotations on a scale of $1$ to $5$ for each metric, with an overall score calculated
    as the mean of the two metrics.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们应用了LLM作为评审员的方法Bubeck等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib5)）；Dubois等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib13)），评估了两个指标：清晰度和全面性。两个不同的LLM对生成的注释在每个指标上进行$1$到$5$的评分，整体得分为两个指标的均值。
- en: 'Experimental Results. We compare TESSA and DirectLLM in Table [2](https://arxiv.org/html/2410.17462v1#S4.T2
    "Table 2 ‣ 4.3 Evaluating General Annotations in Synthetic Datasets ‣ 4 Experiments
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").
    The “Mean” represents the average score of generated annotations for each method,
    and P(T>D) is the percentage of TESSA’s annotations that receive higher scores
    than DirectLLM’s. The results show that TESSA outperforms DirectLLM across both
    metrics, with average scores of $3.90$ in Clarity and $4.44$ in Comprehensiveness,
    compared to DirectLLM’s $3.79$ and $1.55$. Additionally, $82.71\%$ of TESSA’s
    annotations receive higher scores, indicating that TESSA produces more essential
    and easily understandable features, further demonstrating its effectiveness.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果。我们在表[2](https://arxiv.org/html/2410.17462v1#S4.T2 "表2 ‣ 4.3 评估合成数据集中的通用注释
    ‣ 4 实验 ‣ 使用LLM解码时间序列：一个跨领域注释的多代理框架")中比较了TESSA和DirectLLM。“均值”表示每种方法生成的注释的平均得分，而P(T>D)是TESSA注释得分高于DirectLLM的比例。结果表明，TESSA在两个指标上均优于DirectLLM，清晰度平均得分为$3.90$，全面性平均得分为$4.44$，而DirectLLM的得分分别为$3.79$和$1.55$。此外，$82.71\%$的TESSA注释得分高于DirectLLM，表明TESSA生成了更重要且易于理解的特征，进一步证明了其有效性。
- en: 'Table 2: General annotation results on the synthetic dataset with GPT-4o as
    the LLM backbone.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用GPT-4o作为LLM主干的合成数据集上的通用注释结果。
- en: '| Metric | Method | Mean | P(T>D) (%) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 均值 | P(T>D) (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clarity | TESSA | 3.90 | 69.76 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 3.90 | 69.76 |'
- en: '| DirectLLM | 3.79 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.79 |'
- en: '| Compre. | TESSA | 4.44 | 87.10 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 理解度 | TESSA | 4.44 | 87.10 |'
- en: '| DirectLLM | 1.55 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 1.55 |'
- en: '| Overall | TESSA | 4.14 | 82.71 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | TESSA | 4.14 | 82.71 |'
- en: '| DirectLLM | 2.84 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 2.84 |'
- en: 4.4 Domain-specific Annotation Evaluation
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 特定领域注释评估
- en: 'In this subsection, we evaluate the quality of domain specific annotations.
    Similar to Section [4.3](https://arxiv.org/html/2410.17462v1#S4.SS3 "4.3 Evaluating
    General Annotations in Synthetic Datasets ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"), we adopt a LLM-as-a-Judger
    strategy to evaluate the performance of domain-specific annotation agent from
    three perspectives: Clarity, Comprehensiveness, and Domain-relevance. The overall
    score is the average of these three metrics. Further details on these metrics
    are provided in Appendix [H.1](https://arxiv.org/html/2410.17462v1#A8.SS1 "H.1
    Evaluation Metrics ‣ Appendix H Additional Results of Domain-specific Annotation
    Evaluation ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation").'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们评估特定领域注释的质量。与第[4.3节](https://arxiv.org/html/2410.17462v1#S4.SS3 "4.3
    评估合成数据集中的通用注释 ‣ 4 实验 ‣ 使用LLM解码时间序列：一个跨领域注释的多代理框架")类似，我们采用LLM作为评审员的策略，从三个角度评估特定领域注释代理的表现：清晰度、全面性和领域相关性。总体评分是这三个指标的平均值。关于这些指标的更多细节，请参见附录[H.1](https://arxiv.org/html/2410.17462v1#A8.SS1
    "H.1 评估指标 ‣ 附录H 特定领域注释评估的附加结果 ‣ 使用LLM解码时间序列：一个跨领域注释的多代理框架")。
- en: 'Experimental Results. We present the comparison results of TESSA and DirectLLM
    on the Environment dataset in Table [3](https://arxiv.org/html/2410.17462v1#S4.T3
    "Table 3 ‣ 4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"),
    with GPT-4o as the LLM backbone. The key observations are: (1) TESSA significantly
    outperforms DirectLLM across all metrics, achieving an overall score of $4.64$
    compared to DirectLLM’s $3.41$. Notably, $98.51\%$ of TESSA’s annotations receive
    higher scores, demonstrating its effectiveness in generating high-quality domain-specific
    annotations. (2) TESSA scores $4.74$ in Clarity and $4.38$ in Comprehensiveness,
    while DirectLLM scores $3.32$ and $3.01$, respectively. This shows that TESSA
    ’s annotations are clearer, more concise, and cover more important features. (3)
    TESSA also excels in domain relevance, with $94.72\%$ of its annotations scoring
    higher, achieving an average of $4.30$, significantly outperforming DirectLLM’s
    $3.41$. This indicates that TESSA produces highly accurate annotations that effectively
    use domain-specific terminology and maintain strong contextual relevance. Further
    results on other datasets are available in Appendix [H.2](https://arxiv.org/html/2410.17462v1#A8.SS2
    "H.2 Additional Results on Other LLM Backbones ‣ Appendix H Additional Results
    of Domain-specific Annotation Evaluation ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation").'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果：我们展示了TESSA与DirectLLM在环境数据集上的比较结果，见表[3](https://arxiv.org/html/2410.17462v1#S4.T3
    "Table 3 ‣ 4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")，其中GPT-4o作为LLM的骨干。主要观察结果如下：(1)
    TESSA在所有指标上显著优于DirectLLM，整体得分为$4.64$，而DirectLLM为$3.41$。值得注意的是，$98.51\%$的TESSA注释得分更高，证明了它在生成高质量领域特定注释方面的有效性。(2)
    TESSA在清晰度上得分为$4.74$，在全面性上得分为$4.38$，而DirectLLM分别为$3.32$和$3.01$。这表明TESSA的注释更清晰、更简洁，涵盖了更多重要特征。(3)
    TESSA在领域相关性上也表现出色，$94.72\%$的注释得分更高，平均得分为$4.30$，显著优于DirectLLM的$3.41$。这表明TESSA生成的注释在有效使用领域特定术语和保持强相关性的上下文方面非常准确。更多关于其他数据集的结果请参见附录[H.2](https://arxiv.org/html/2410.17462v1#A8.SS2
    "H.2 Additional Results on Other LLM Backbones ‣ Appendix H Additional Results
    of Domain-specific Annotation Evaluation ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")。'
- en: 'Table 3: Domain-specific annotation results on the Environment dataset with
    GPT-4o as the LLM backbone.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于GPT-4o作为LLM骨干的环境数据集上的领域特定注释结果。
- en: '| Metric | Method | Mean | P(T>D) (%) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 平均值 | P(T>D) (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clarity | TESSA | 4.74 | 99.81 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 4.74 | 99.81 |'
- en: '| DirectLLM | 3.32 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.32 |'
- en: '| Compre. | TESSA | 4.38 | 97.04 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Compre. | TESSA | 4.38 | 97.04 |'
- en: '| DirectLLM | 3.01 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.01 |'
- en: '| Dom. Rel. | TESSA | 4.30 | 94.72 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 领域相关性 | TESSA | 4.30 | 94.72 |'
- en: '| DirectLLM | 3.57 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.57 |'
- en: '| Overall | TESSA | 4.64 | 98.51 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | TESSA | 4.64 | 98.51 |'
- en: '| DirectLLM | 3.41 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.41 |'
- en: 4.5 In-depth Dissection of TESSA
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 TESSA的深入剖析
- en: 'Adaptive Feature Selections. We compare our two feature selection methods:
    offline LLM-based selection and incremental RL-based selection. To assess their
    effectiveness in selecting the top-$k$ most important features, we evaluate the
    quality of the generated general and domain-specific annotations, following the
    procedures in Sections [4.2](https://arxiv.org/html/2410.17462v1#S4.SS2 "4.2 Evaluating
    General Annotations in Downstream Tasks ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") and [4.4](https://arxiv.org/html/2410.17462v1#S4.SS4
    "4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"), respectively.
    Environment is set as the target domain, with results shown in Fig. [2](https://arxiv.org/html/2410.17462v1#S4.F2
    "Figure 2 ‣ 4.5 In-depth Dissection of TESSA ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The results
    indicate that TESSA performs comparably in both general and domain-specific annotation
    generation using either selection method. Specifically, as shown in Fig. [2](https://arxiv.org/html/2410.17462v1#S4.F2
    "Figure 2 ‣ 4.5 In-depth Dissection of TESSA ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")(a), both approaches
    achieve MSE and MAE around $0.46$ and $0.44$ for general annotations. Similarly,
    in Fig. [2](https://arxiv.org/html/2410.17462v1#S4.F2 "Figure 2 ‣ 4.5 In-depth
    Dissection of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")(b), both methods score consistently high
    across all domain-specific metrics, demonstrating their effectiveness in selecting
    important features. However, incremental RL-based selection proves more cost-effective
    by reducing redundant re-querying of previously used data.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应特征选择。我们比较了两种特征选择方法：离线基于LLM的选择和增量基于RL的选择。为了评估它们在选择最重要的前$k$个特征方面的效果，我们根据第[4.2](https://arxiv.org/html/2410.17462v1#S4.SS2
    "4.2 评估下游任务中的一般注释 ‣ 4 实验 ‣ 使用LLM解码时间序列：一种跨领域注释的多代理框架")节和第[4.4](https://arxiv.org/html/2410.17462v1#S4.SS4
    "4.4 特定领域注释评估 ‣ 4 实验 ‣ 使用LLM解码时间序列：一种跨领域注释的多代理框架")节的程序，评估生成的一般和特定领域注释的质量。环境被设定为目标领域，结果见图[2](https://arxiv.org/html/2410.17462v1#S4.F2
    "图2 ‣ 4.5 TESSA的深入剖析 ‣ 4 实验 ‣ 使用LLM解码时间序列：一种跨领域注释的多代理框架")。结果表明，TESSA在使用任何一种选择方法时，在生成一般注释和特定领域注释方面表现相当。具体来说，如图[2](https://arxiv.org/html/2410.17462v1#S4.F2
    "图2 ‣ 4.5 TESSA的深入剖析 ‣ 4 实验 ‣ 使用LLM解码时间序列：一种跨领域注释的多代理框架")(a)所示，两种方法在一般注释上都达到了大约$0.46$的MSE和$0.44$的MAE。同样，在图[2](https://arxiv.org/html/2410.17462v1#S4.F2
    "图2 ‣ 4.5 TESSA的深入剖析 ‣ 4 实验 ‣ 使用LLM解码时间序列：一种跨领域注释的多代理框架")(b)中，两种方法在所有特定领域指标上得分一致较高，展示了它们在选择重要特征方面的有效性。然而，增量基于RL的选择方法通过减少对先前使用过的数据的冗余重新查询，证明了其更具成本效益。
- en: '![Refer to caption](img/cbf3f63caf255c109c5cca363798107b.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cbf3f63caf255c109c5cca363798107b.png)'
- en: (a) General
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 一般
- en: '![Refer to caption](img/e8c594cd858de9f88f162a7ac12cdf15.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e8c594cd858de9f88f162a7ac12cdf15.png)'
- en: (b) Specific
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 特定
- en: 'Figure 2: Comparison of offline vs. incremental feature selection. GPT-4o is
    the LLM backbone, with Environment as the target domain. (a) General annotation
    results; (b) Domain-specific annotation results.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：离线与增量特征选择的比较。GPT-4o是LLM骨干，环境作为目标领域。(a) 一般注释结果；(b) 特定领域注释结果。
- en: 'Ablation Studies. We perform ablation studies to assess the importance of domain
    decontextualization and adaptive feature selection in TESSA. To evaluate domain
    decontextualization, we introduce a variant, TESSA/D, which bypasses the domain
    decontextualization LLM and directly extracts text-wise features from domain-specific
    annotations. Table [11](https://arxiv.org/html/2410.17462v1#A9.T11 "Table 11 ‣
    I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies ‣
    Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")
    shows that TESSA/D captures irrelevant features, such as higher prices over time
    and fun, which are unrelated to time series analysis. This supports our claim
    that domain-specific terminology can hinder the accurate extraction of time-series-relevant
    features.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '消融研究。我们进行消融研究，以评估领域去上下文化和自适应特征选择在 TESSA 中的重要性。为了评估领域去上下文化，我们引入了一个变体 TESSA/D，该变体绕过了领域去上下文化
    LLM，直接从领域特定注释中提取逐文本特征。表[11](https://arxiv.org/html/2410.17462v1#A9.T11 "Table
    11 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")显示，TESSA/D
    捕捉到了一些无关的特征，如随着时间推移的价格上涨和趣味性，而这些特征与时间序列分析无关。这支持了我们关于领域特定术语可能妨碍准确提取与时间序列相关特征的论点。'
- en: 'To prove the importance of adaptive feature selection in TESSA, we remove the
    adaptive feature selection module to create a variant, TESSA/F. We apply an LLM-as-a-judger
    to compare the quality of the generated annotations between TESSA and its variants.
    The evaluation metrics are introduced in Appendix [I.1](https://arxiv.org/html/2410.17462v1#A9.SS1
    "I.1 Evaluation Metric ‣ Appendix I Additional Details of Ablation Studies ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").
    We choose Social Good as the target dataset. The comparison results are presented
    in Table [4](https://arxiv.org/html/2410.17462v1#S4.T4 "Table 4 ‣ 4.5 In-depth
    Dissection of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation"), with qualitative examples provided in
    Appendix [I.2](https://arxiv.org/html/2410.17462v1#A9.SS2 "I.2 Qualitative Examples
    ‣ Appendix I Additional Details of Ablation Studies ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). We observe that:
    TESSA consistently outperforms TESSA/F. Specifically, TESSA achieves a clarity
    score of $4.41$, compared to $3.66$ for TESSA/F. This demonstrates the necessity
    of adaptive feature selection. Furthermore, according to Table [13](https://arxiv.org/html/2410.17462v1#A9.T13
    "Table 13 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation") in Appendix [I.2](https://arxiv.org/html/2410.17462v1#A9.SS2 "I.2
    Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"),
    the annotations generated by TESSA/F tend to include many features without proper
    analysis. This shows that involving too many features can hinder the clarity of
    the annotations, further emphasizing the importance of adaptive feature selection
    in improving annotation quality.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '为了证明自适应特征选择在 TESSA 中的重要性，我们去除了自适应特征选择模块，创建了一个变体 TESSA/F。我们应用 LLM 作为评判者，比较 TESSA
    与其变体生成的注释质量。评估指标在附录[I.1](https://arxiv.org/html/2410.17462v1#A9.SS1 "I.1 Evaluation
    Metric ‣ Appendix I Additional Details of Ablation Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中介绍。我们选择 Social
    Good 作为目标数据集。比较结果呈现在表[4](https://arxiv.org/html/2410.17462v1#S4.T4 "Table 4 ‣
    4.5 In-depth Dissection of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs:
    A Multi-Agent Framework for Cross-Domain Annotation")中，附录[I.2](https://arxiv.org/html/2410.17462v1#A9.SS2
    "I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")提供了定性示例。我们观察到：TESSA
    一直优于 TESSA/F。具体来说，TESSA 的清晰度得分为 $4.41$，而 TESSA/F 的得分为 $3.66$。这证明了自适应特征选择的必要性。此外，根据附录[I.2](https://arxiv.org/html/2410.17462v1#A9.SS2
    "I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中的表[13](https://arxiv.org/html/2410.17462v1#A9.T13
    "Table 13 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation")，TESSA/F 生成的注释往往包括许多未经充分分析的特征。这表明，涉及过多特征可能会妨碍注释的清晰度，进一步强调了自适应特征选择在提高注释质量中的重要性。'
- en: 'Table 4: Ablation studies in the SocialGood dataset. GPT-4o is the LLM backbone.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：SocialGood 数据集中的消融研究。GPT-4o 是 LLM 主干。
- en: '| Metric | Method | Mean | P(T>D) (%) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 平均值 | P(T>D) (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clarity | TESSA | 4.41 | 83.3 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 4.41 | 83.3 |'
- en: '| TESSA/F | 3.66 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| TESSA/F | 3.66 |'
- en: 4.6 Case Study of TESSA
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 TESSA的案例研究
- en: 'We conduct a case study to further validate the effectiveness of TESSA. A representative
    time series from the Social Good domain (Fig. [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "In Figure 3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")(b)) is selected,
    and both TESSA and DirectLLM are applied to generate general and domain-specific
    annotations, summarized in Table [14](https://arxiv.org/html/2410.17462v1#A10.T14
    "Table 14 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). To assess the
    quality of the annotations, we use an LLM-as-a-judger to evaluate the domain-specific
    annotations from both methods, with results shown in Table [12](https://arxiv.org/html/2410.17462v1#A9.T12
    "Table 12 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation"). Our findings indicate that: (1) TESSA’s general annotations capture
    more meaningful patterns, aiding user understanding and downstream tasks, whereas
    DirectLLM only highlights basic trends; and (2) TESSA’s domain-specific annotations
    consistently outperform DirectLLM across all metrics, offering clearer, more comprehensive,
    and contextually relevant insights. More case studies of multivariate time series
    data are provided in Appendix [J](https://arxiv.org/html/2410.17462v1#A10 "Appendix
    J Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation").'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一项案例研究，以进一步验证TESSA的有效性。从社会公益领域选取了一个代表性的时间序列（图[3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "在图3 ‣ 附录J 案例研究的附加细节 ‣ 使用LLMs解码时间序列：一个跨领域标注的多智能体框架")(b)），并应用TESSA和DirectLLM生成通用和特定领域的标注，结果总结在表[14](https://arxiv.org/html/2410.17462v1#A10.T14
    "表14 ‣ 附录J 案例研究的附加细节 ‣ 使用LLMs解码时间序列：一个跨领域标注的多智能体框架")中。为了评估标注的质量，我们使用LLM作为评判工具，评估两种方法的特定领域标注，结果如表[12](https://arxiv.org/html/2410.17462v1#A9.T12
    "表12 ‣ I.2 定性示例 ‣ 附录I 消融研究的附加细节 ‣ 使用LLMs解码时间序列：一个跨领域标注的多智能体框架")所示。我们的研究结果表明：(1)
    TESSA的通用标注能够捕捉到更有意义的模式，帮助用户理解和进行下游任务，而DirectLLM仅突出了基本趋势；(2) TESSA的特定领域标注在所有指标上均优于DirectLLM，提供了更清晰、更全面且与上下文相关的见解。更多关于多元时间序列数据的案例研究见附录[J](https://arxiv.org/html/2410.17462v1#A10
    "附录J 案例研究的附加细节 ‣ 使用LLMs解码时间序列：一个跨领域标注的多智能体框架")。
- en: 5 Conclusion
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce TESSA, a multi-agent system for automatic general
    and domain-specific time series annotation. TESSA incorporates two agents, a general
    annotation agent and a domain-specific annotation agent, to extract and leverage
    both time-series-wise and text-wise knowledge from multiple domains for annotations.
    TESSA overcomes the limitations of directly applying LLMs, which often capture
    only basic patterns and may hallucinate, by effectively identifying and emphasizing
    significant patterns in time series data. Our experiments on synthetic and real-world
    datasets from diverse domains demonstrate the effectiveness of TESSA in generating
    high-quality general and domain-specific annotations.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本工作中，我们介绍了TESSA，一个用于自动化通用和特定领域时间序列标注的多智能体系统。TESSA结合了两个智能体，一个通用标注智能体和一个特定领域标注智能体，能够从多个领域中提取并利用时间序列和文本的知识进行标注。TESSA克服了直接应用大型语言模型（LLMs）的局限性，后者通常只捕捉基本的模式，且可能会出现幻觉，通过有效识别并强调时间序列数据中的重要模式。我们在合成数据集和来自不同领域的真实世界数据集上的实验表明，TESSA在生成高质量的通用和特定领域标注方面具有有效性。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat等人。2023年。Gpt-4技术报告。*arXiv预印本 arXiv:2303.08774*。
- en: Almeida (1994) Luis B Almeida. 1994. The fractional fourier transform and time-frequency
    representations. *IEEE Transactions on signal processing*, 42(11):3084–3091.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Almeida（1994）Luis B Almeida。1994年。分数傅里叶变换和时频表示。*IEEE信号处理学报*，42(11):3084–3091。
- en: 'Bao et al. (2022) Hangbo Bao, Wenhui Wang, Li Dong, Qiang Liu, Owais Khan Mohammed,
    Kriti Aggarwal, Subhojit Som, Songhao Piao, and Furu Wei. 2022. [Vlmo: Unified
    vision-language pre-training with mixture-of-modality-experts](http://papers.nips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html).
    In *Proceedings of the Annual Conference on Neural Information Processing Systems*,
    pages 32897–32912.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等人（2022）Hangbo Bao、Wenhui Wang、Li Dong、Qiang Liu、Owais Khan Mohammed、Kriti
    Aggarwal、Subhojit Som、Songhao Piao 和 Furu Wei。2022年。[Vlmo：通过混合模态专家的统一视觉-语言预训练](http://papers.nips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html)。发表于*神经信息处理系统年会论文集*，第32897–32912页。
- en: 'Ben-David et al. (2022) Eyal Ben-David, Nadav Oved, and Roi Reichart. 2022.
    Pada: Example-based prompt learning for on-the-fly adaptation to unseen domains.
    *Transactions of the Association for Computational Linguistics*, 10:414–433.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben-David 等人（2022）Eyal Ben-David、Nadav Oved 和 Roi Reichart。2022年。Pada：基于示例的提示学习，用于快速适应未见领域。*计算语言学协会会刊*，10:414–433。
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. 2023. Sparks of artificial general intelligence: Early experiments with
    gpt-4. *arXiv preprint arXiv:2303.12712*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bubeck 等人（2023）Sébastien Bubeck、Varun Chandrasekaran、Ronen Eldan、Johannes Gehrke、Eric
    Horvitz、Ece Kamar、Peter Lee、Yin Tat Lee、Yuanzhi Li、Scott Lundberg 等人。2023年。人工通用智能的火花：与
    GPT-4 的早期实验。*arXiv 预印本 arXiv:2303.12712*。
- en: 'Cao et al. (2024) Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang
    Zheng, Wen Ye, and Yan Liu. 2024. Tempo: Prompt-based generative pre-trained transformer
    for time series forecasting. In *ICLR*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等人（2024）Defu Cao、Furong Jia、Sercan O Arik、Tomas Pfister、Yixiang Zheng、Wen
    Ye 和 Yan Liu。2024年。Tempo：基于提示的生成预训练变换器用于时间序列预测。发表于*ICLR*。
- en: 'Cascella et al. (2023) Marco Cascella, Jonathan Montomoli, Valentina Bellini,
    and Elena Bignami. 2023. Evaluating the feasibility of chatgpt in healthcare:
    an analysis of multiple clinical and research scenarios. *Journal of medical systems*,
    47(1):33.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cascella 等人（2023）Marco Cascella、Jonathan Montomoli、Valentina Bellini 和 Elena
    Bignami。2023年。评估 ChatGPT 在医疗保健中的可行性：多个临床和研究场景的分析。*医学系统杂志*，47(1)：33。
- en: 'Chang et al. (2023) Ching Chang, Wei-Yao Wang, Wen-Chih Peng, and Tien-Fu Chen.
    2023. Llm4ts: Aligning pre-trained llms as data-efficient time-series forecasters.
    *arXiv preprint arXiv:2308.08469*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人（2023）Ching Chang、Wei-Yao Wang、Wen-Chih Peng 和 Tien-Fu Chen。2023年。Llm4ts：将预训练大语言模型对齐为数据高效的时间序列预测器。*arXiv
    预印本 arXiv:2308.08469*。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人（2023）Aakanksha Chowdhery、Sharan Narang、Jacob Devlin、Maarten Bosma、Gaurav
    Mishra、Adam Roberts、Paul Barham、Hyung Won Chung、Charles Sutton、Sebastian Gehrmann
    等人。2023年。Palm：通过路径扩展语言建模。*机器学习研究杂志*，24(240)：1–113。
- en: 'Cleveland et al. (1990) Robert B Cleveland, William S Cleveland, Jean E McRae,
    Irma Terpenning, et al. 1990. Stl: A seasonal-trend decomposition. *J. off. Stat*,
    6(1):3–73.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cleveland 等人（1990）Robert B Cleveland、William S Cleveland、Jean E McRae、Irma Terpenning
    等人。1990年。Stl：季节性趋势分解。*J. off. Stat*，6(1)：3–73。
- en: Cruz-Sandoval et al. (2019) Dagoberto Cruz-Sandoval, Jessica Beltran-Marquez,
    Matias Garcia-Constantino, Luis A. Gonzalez-Jasso, Jesus Favela, Irvin Hussein
    Lopez-Nava, Ian Cleland, Andrew Ennis, Netzahualcoyotl Hernandez-Cruz, Joseph
    Rafferty, Jonathan Synnott, and Chris Nugent. 2019. Semi-automated data labeling
    for activity recognition in pervasive healthcare. *Sensors*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cruz-Sandoval 等人（2019）Dagoberto Cruz-Sandoval、Jessica Beltran-Marquez、Matias
    Garcia-Constantino、Luis A. Gonzalez-Jasso、Jesus Favela、Irvin Hussein Lopez-Nava、Ian
    Cleland、Andrew Ennis、Netzahualcoyotl Hernandez-Cruz、Joseph Rafferty、Jonathan Synnott
    和 Chris Nugent。2019年。用于普适医疗保健活动识别的半自动化数据标注。*传感器*。
- en: Dubey et al. (2024) Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek
    Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang,
    Angela Fan, et al. 2024. The llama 3 herd of models. *arXiv preprint arXiv:2407.21783*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等人（2024）Abhimanyu Dubey、Abhinav Jauhri、Abhinav Pandey、Abhishek Kadian、Ahmad
    Al-Dahle、Aiesha Letman、Akhil Mathur、Alan Schelten、Amy Yang、Angela Fan 等人。2024年。Llama
    3 模型群体。*arXiv 预印本 arXiv:2407.21783*。
- en: 'Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang,
    Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto.
    2024. Alpacafarm: A simulation framework for methods that learn from human feedback.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dubois et al. (2024) Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang,
    Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, 和 Tatsunori B Hashimoto.
    2024. Alpacafarm: 一个用于从人类反馈中学习方法的仿真框架。 *神经信息处理系统进展*，36。'
- en: Gruver et al. (2024) Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson.
    2024. Large language models are zero-shot time series forecasters. *Advances in
    Neural Information Processing Systems*, 36.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gruver et al. (2024) Nate Gruver, Marc Finzi, Shikai Qiu, 和 Andrew G Wilson.
    2024. 大型语言模型是零-shot时间序列预测器。 *神经信息处理系统进展*，36。
- en: Hsu and Liu (2021) Chia-Yu Hsu and Wei-Chen Liu. 2021. Multiple time-series
    convolutional neural network for fault detection and diagnosis and empirical study
    in semiconductor manufacturing. *Journal of Intelligent Manufacturing*, 32(3):823–836.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu and Liu (2021) Chia-Yu Hsu 和 Wei-Chen Liu. 2021. 用于故障检测和诊断的多时间序列卷积神经网络及半导体制造中的实证研究。
    *智能制造杂志*，32(3):823–836。
- en: 'Izacard et al. (2023) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    and Edouard Grave. 2023. Atlas: Few-shot learning with retrieval augmented language
    models. *Journal of Machine Learning Research*, 24(251):1–43.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Izacard et al. (2023) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini,
    Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel,
    和 Edouard Grave. 2023. Atlas: 使用检索增强语言模型进行少样本学习。 *机器学习研究杂志*，24(251):1–43。'
- en: 'Jiang et al. (2024) Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson
    Schneider, Yuriy Nevmyvaka, and Dongjin Song. 2024. Empowering time series analysis
    with large language models: A survey. *arXiv preprint arXiv:2402.03182*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) Yushan Jiang, Zijie Pan, Xikun Zhang, Sahil Garg, Anderson
    Schneider, Yuriy Nevmyvaka, 和 Dongjin Song. 2024. 通过大型语言模型增强时间序列分析：一项调查。 *arXiv预印本
    arXiv:2402.03182*。
- en: 'Jin et al. (2024) Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong
    Wen. 2024. Time-LLM: Time series forecasting by reprogramming large language models.
    In *ICLR*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jin et al. (2024) Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang,
    Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, 和 Qingsong
    Wen. 2024. Time-LLM: 通过重新编程大型语言模型进行时间序列预测。载于 *ICLR*。'
- en: Lee et al. (2024) Jean Lee, Nicholas Stevens, Soyeon Caren Han, and Minseok
    Song. 2024. A survey of large language models in finance (finllms). *arXiv preprint
    arXiv:2402.02315*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee et al. (2024) Jean Lee, Nicholas Stevens, Soyeon Caren Han, 和 Minseok Song.
    2024. 金融中大型语言模型的调查（finllms）。 *arXiv预印本 arXiv:2402.02315*。
- en: Li et al. (2024) Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong.
    2024. Frozen language model helps ecg zero-shot learning. In *Medical Imaging
    with Deep Learning*, pages 402–415\. PMLR.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, 和 Shenda Hong.
    2024. 冻结语言模型帮助ECG零-shot学习。载于 *深度学习医学影像*，第402–415页。PMLR。
- en: 'Liu et al. (2024a) Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan
    Kamarthi, Aditya B Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang,
    et al. 2024a. Time-mmd: A new multi-domain multimodal dataset for time series
    analysis. *arXiv preprint arXiv:2406.08627*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2024a) Haoxin Liu, Shangqing Xu, Zhiyuan Zhao, Lingkai Kong, Harshavardhan
    Kamarthi, Aditya B Sasanur, Megha Sharma, Jiaming Cui, Qingsong Wen, Chao Zhang,
    et al. 2024a. Time-mmd: 一个用于时间序列分析的新型多领域多模态数据集。 *arXiv预印本 arXiv:2406.08627*。'
- en: 'Liu et al. (2024b) Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang,
    Bryan Hooi, and Roger Zimmermann. 2024b. Unitime: A language-empowered unified
    model for cross-domain time series forecasting. In *Proceedings of the ACM on
    Web Conference 2024*, pages 4095–4106.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2024b) Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang,
    Bryan Hooi, 和 Roger Zimmermann. 2024b. Unitime: 一种通过语言增强的跨领域时间序列预测统一模型。载于 *2024年ACM
    Web会议论文集*，第4095–4106页。'
- en: Lu et al. (2022) Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch.
    2022. Frozen pretrained transformers as universal computation engines. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 36, pages 7628–7636.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2022) Kevin Lu, Aditya Grover, Pieter Abbeel, 和 Igor Mordatch. 2022.
    冻结的预训练变换器作为通用计算引擎。载于 *AAAI人工智能会议论文集*，第36卷，7628–7636页。
- en: Malik et al. (2023) Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen Kan, and
    Soujanya Poria. 2023. Udapter–efficient domain adaptation using adapters. *arXiv
    preprint arXiv:2302.03194*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Malik et al. (2023) Bhavitvya Malik, Abhinav Ramesh Kashyap, Min-Yen Kan, 和
    Soujanya Poria. 2023. Udapter–利用适配器的高效领域适应。 *arXiv预印本 arXiv:2302.03194*。
- en: Ni et al. (2021) Jingchao Ni, Zhengzhang Chen, Wei Cheng, Bo Zong, Dongjin Song,
    Yanchi Liu, Xuchao Zhang, and Haifeng Chen. 2021. Interpreting convolutional sequence
    model by learning local prototypes with adaptation regularization. In *Proceedings
    of the 30th ACM International Conference on Information & Knowledge Management*,
    pages 1366–1375.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ni等人（2021）倪靖超、陈正张、程伟、宗博、宋东金、刘燕池、张旭超和陈海峰。2021年。通过学习局部原型和自适应正则化解释卷积序列模型。载于*第30届ACM国际信息与知识管理大会论文集*，第1366–1375页。
- en: 'Nino et al. (2016) Jorge Nino, Andrés Frias-Velazquez, Nyan Bo Bo, Maarten
    Slembrouck, Junzhi Guan, Glen Debard, Bart Vanrumste, Tinne Tuytelaars, and Wilfried
    Philips. 2016. Scalable semi-automatic annotation for multi-camera person tracking.
    *IEEE transactions on image processing : a publication of the IEEE Signal Processing
    Society*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nino等人（2016）Jorge Nino、Andrés Frias-Velazquez、Nyan Bo Bo、Maarten Slembrouck、关俊智、Glen
    Debard、Bart Vanrumste、Tinne Tuytelaars和Wilfried Philips。2016年。可扩展的半自动多摄像头人员追踪注释。*IEEE图像处理学报：IEEE信号处理学会出版物*。
- en: 'Pfeiffer et al. (2020) Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun
    Cho, and Iryna Gurevych. 2020. Adapterfusion: Non-destructive task composition
    for transfer learning. *arXiv preprint arXiv:2005.00247*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfeiffer等人（2020）Jonas Pfeiffer、Aishwarya Kamath、Andreas Rücklé、Kyunghyun Cho和Iryna
    Gurevych。2020年。Adapterfusion：无损任务组合用于迁移学习。*arXiv预印本arXiv:2005.00247*。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford等人（2019）Alec Radford、Jeffrey Wu、Rewon Child、David Luan、Dario Amodei、Ilya
    Sutskever等人。2019年。语言模型是无监督的多任务学习者。*OpenAI博客*，1(8):9。
- en: Reining et al. (2020) Christopher Reining, Fernando Moya Rueda, Friedrich Niemann,
    Gernot A. Fink, and Michael ten Hompel. 2020. Annotation performance for multi-channel
    time series har dataset in logistics. In *2020 IEEE International Conference on
    Pervasive Computing and Communications Workshops (PerCom Workshops)*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reining等人（2020）Christopher Reining、Fernando Moya Rueda、Friedrich Niemann、Gernot
    A. Fink和Michael ten Hompel。2020年。在物流中多通道时间序列Har数据集的注释性能。载于*2020年IEEE国际普适计算与通信研讨会（PerCom
    Workshops）*。
- en: 'Sun et al. (2024) Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. 2024.
    Test: Text prototype aligned embedding to activate llm’s ability for time series.
    In *ICLR*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人（2024）孙晨曦、李红岩、李亚梁和洪申达。2024年。Test：文本原型对齐嵌入激活LLM的时间序列能力。载于*ICLR*。
- en: 'Wu et al. (2023) Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and
    Mingsheng Long. 2023. Timesnet: Temporal 2d-variation modeling for general time
    series analysis. In *International Conference on Learning Representations*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等人（2023）吴海旭、胡腾格、刘勇、周杭、王建民和龙名胜。2023年。Timesnet：用于一般时间序列分析的时间2D变化建模。载于*国际学习表示大会*。
- en: 'Xu et al. (2024) Junjie Xu, Zongyu Wu, Minhua Lin, Xiang Zhang, and Suhang
    Wang. 2024. Llm and gnn are complementary: Distilling llm for multimodal graph
    learning. *arXiv preprint arXiv:2406.01032*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人（2024）许俊杰、吴宗宇、林敏华、张翔和王苏航。2024年。LLM与GNN是互补的：为多模态图学习蒸馏LLM。*arXiv预印本arXiv:2406.01032*。
- en: 'Xue and Salim (2023) Hao Xue and Flora D Salim. 2023. Promptcast: A new prompt-based
    learning paradigm for time series forecasting. *IEEE Transactions on Knowledge
    and Data Engineering*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xue和Salim（2023）薛昊和Flora D Salim。2023年。Promptcast：一种基于提示的新型时间序列预测学习范式。*IEEE知识与数据工程学报*。
- en: Yang et al. (2024) An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang
    Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2
    technical report. *arXiv preprint arXiv:2407.10671*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杨等人（2024）杨安、杨宝松、会宾源、郑博、余博文、周畅、李成鹏、李承元、刘大义恒、黄飞等人。2024年。Qwen2技术报告。*arXiv预印本arXiv:2407.10671*。
- en: 'Yang et al. (2021) Chao-Han Huck Yang, Yun-Yun Tsai, and Pin-Yu Chen. 2021.
    Voice2series: Reprogramming acoustic models for time series classification. In
    *International conference on machine learning*, pages 11808–11819\. PMLR.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人（2021）杨超翰、蔡韵韵和陈品瑜。2021年。Voice2series：为时间序列分类重新编程声学模型。载于*国际机器学习会议*，第11808–11819页。PMLR。
- en: Yordanova and Krüger (2018) Kristina Yordanova and Frank Krüger. 2018. Creating
    and exploring semantic annotation for behaviour analysis. *Sensors*.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yordanova和Krüger（2018）Kristina Yordanova和Frank Krüger。2018年。为行为分析创建和探索语义注释。*Sensors*。
- en: Yu et al. (2023) Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu,
    and Yanbin Lu. 2023. Temporal data meets llm–explainable financial time series
    forecasting. *arXiv preprint arXiv:2306.11025*.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2023）余新立、陈正、凌源、董树静、刘宗义和卢燕斌。2023年。时间数据与LLM相遇——可解释的金融时间序列预测。*arXiv预印本arXiv:2306.11025*。
- en: Zhang et al. (2023) Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023.
    Automatic chain of thought prompting in large language models. In *ICLR*.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023）Zhuosheng Zhang，Aston Zhang，Mu Li，和Alex Smola。2023。大语言模型中的自动思维链提示。在*ICLR*。
- en: 'Zhou et al. (2021) Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin
    Li, Hui Xiong, and Wancai Zhang. 2021. Informer: Beyond efficient transformer
    for long sequence time-series forecasting. In *Proceedings of the AAAI conference
    on artificial intelligence*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2021）Haoyi Zhou，Shanghang Zhang，Jieqi Peng，Shuai Zhang，Jianxin Li，Hui
    Xiong，和Wancai Zhang。2021。Informer：超越高效变换器用于长序列时间序列预测。在*人工智能会议论文集*。
- en: 'Zhou et al. (2023) Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. 2023.
    One fits all: Power general time series analysis by pretrained lm. *Advances in
    neural information processing systems*, 36:43322–43355.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2023）Tian Zhou，Peisong Niu，Liang Sun，Rong Jin等人。2023。一刀切：通过预训练的语言模型推动通用时间序列分析。*神经信息处理系统进展*，36:43322–43355。
- en: Appendix A More Related Works
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 相关工作更多内容
- en: 'LLMs for Time Series Analysis. The rapid advancement of LLMs in natural language
    processing has unveiled unprecedented capabilities in sequential modeling and
    pattern recognition, which can be leveraged for time series analysis. Three primary
    approaches are commonly adopted Jiang et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib17)):
    direct querying of LLMs Xue and Salim ([2023](https://arxiv.org/html/2410.17462v1#bib.bib33));
    Yu et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib37)); Gruver et al.
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib14)), fine-tuning LLMs with
    task-specific modifications Chang et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib8));
    Cao et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib6)); Jin et al.
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib18)); Sun et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib30)),
    and incorporating LLMs into time series models to enhance feature extraction Li
    et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib20)).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs在时间序列分析中的应用。自然语言处理领域LLMs的快速发展揭示了前所未有的序列建模和模式识别能力，这些能力可以用于时间序列分析。三种主要的方法通常被采用：直接查询LLMs（Jiang等人[2024](https://arxiv.org/html/2410.17462v1#bib.bib17)）；Xue和Salim（[2023](https://arxiv.org/html/2410.17462v1#bib.bib33)）；Yu等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib37)）；Gruver等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib14)），对LLMs进行任务特定的微调（Chang等人[2023](https://arxiv.org/html/2410.17462v1#bib.bib8)）；Cao等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib6)）；Jin等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib18)）；Sun等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib30)），以及将LLMs融入时间序列模型中以增强特征提取（Li等人[2024](https://arxiv.org/html/2410.17462v1#bib.bib20)）。
- en: Direct querying involves using LLMs to generate predictions or identify patterns
    from the data without modifying the underlying architecture. For example, PromptCast Xue
    and Salim ([2023](https://arxiv.org/html/2410.17462v1#bib.bib33)) applies LLMs
    to time series forecasting through a sentence-to-sentence paradigm. [Yu et al.](https://arxiv.org/html/2410.17462v1#bib.bib37)
    explore the use of LLMs for domain-specific tasks like financial time series forecasting Yu
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib37)), while LLMTime Gruver
    et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib14)) demonstrates how
    LLMs can function as effective learners by tokenizing time series data in a text-like
    format.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 直接查询涉及使用LLMs生成预测或从数据中识别模式，而不修改底层架构。例如，PromptCast Xue和Salim（[2023](https://arxiv.org/html/2410.17462v1#bib.bib33)）通过句对句的范式应用LLMs进行时间序列预测。[Yu等人](https://arxiv.org/html/2410.17462v1#bib.bib37)探讨了LLMs在特定领域任务中的应用，如金融时间序列预测（Yu等人[2023](https://arxiv.org/html/2410.17462v1#bib.bib37)），而LLMTime
    Gruver等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib14)）展示了LLMs如何通过将时间序列数据标记化为类似文本的格式，成为有效的学习者。
- en: Fine-tuning LLMs enables them to better capture the intricacies of time series
    data by adapting them to specific datasets or tasks. For instance, LLM4TS Chang
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib8)) shows that fine-tuning
    pre-trained models can enhance forecasting performance. Additionally, TEMPO Cao
    et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib6)) and TEST Sun et al.
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib30)) introduce architectures
    tailored for time series prediction, further demonstrating the power of specialized
    designs.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 微调LLMs使它们能够更好地捕捉时间序列数据的复杂性，通过将它们适应于特定的数据集或任务。例如，LLM4TS Chang 等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib8)）展示了微调预训练模型可以增强预测性能。此外，TEMPO
    Cao 等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib6)）和TEST Sun 等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib30)）介绍了为时间序列预测量身定制的架构，进一步证明了专门设计的强大功能。
- en: Lastly, LLMs can also act as feature enhancers within traditional time series
    models, enriching data representations and boosting performance. For example,
    Li et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib20)) illustrates
    how a frozen LLM can augment zero-shot learning for ECG time series analysis,
    highlighting the potential of LLMs to provide valuable features for complex datasets.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LLM还可以作为传统时间序列模型中的特征增强器，丰富数据表示并提升性能。例如，Li 等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib20)）展示了冻结的LLM如何增强ECG时间序列分析的零样本学习，突出显示了LLM在为复杂数据集提供有价值特征方面的潜力。
- en: 'Domain Specialization of LLMs. Domain specialization of LLMs refers to the
    process of adapting broadly trained models to achieve optimal performance within
    a specific domain. This is generally categorized into three approaches: prompt
    crafting Ben-David et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib4));
    Zhang et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib38)); Xu et al.
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib32)), external augmentation Izacard
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib16)), and model fine-tuning Malik
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib24)); Pfeiffer et al.
    ([2020](https://arxiv.org/html/2410.17462v1#bib.bib27)). One of the earliest efforts
    in this area is PADA Ben-David et al. ([2022](https://arxiv.org/html/2410.17462v1#bib.bib4)),
    which enhances LLMs for unseen domains by generating domain-specific features
    from test queries and using them as prompts for task prediction. Auto-CoT Zhang
    et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib38)) advances domain
    specialization by prompting LLMs with the phrase “Let’s think step by step,” helping
    guide the models in generating reasoning chains. Additionally, Izacard et al.
    ([2023](https://arxiv.org/html/2410.17462v1#bib.bib16)) propose integrating a
    relatively lightweight LLM with an external knowledge base, achieving performance
    comparable to much larger models like PaLM Chowdhery et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib9)).
    These studies highlight the flexibility of LLMs in adapting to specific domains
    through various strategies for domain adaptation.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的领域专业化。LLM的领域专业化指的是将广泛训练的模型调整以在特定领域内实现最佳性能的过程。这通常分为三种方法：提示词设计 Ben-David 等人（[2022](https://arxiv.org/html/2410.17462v1#bib.bib4)）；Zhang
    等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib38)）；Xu 等人（[2024](https://arxiv.org/html/2410.17462v1#bib.bib32)），外部增强
    Izacard 等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib16)），以及模型微调 Malik
    等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib24)）；Pfeiffer 等人（[2020](https://arxiv.org/html/2410.17462v1#bib.bib27)）。该领域最早的努力之一是PADA
    Ben-David 等人（[2022](https://arxiv.org/html/2410.17462v1#bib.bib4)），该方法通过从测试查询生成领域特定的特征，并将其作为任务预测的提示，从而增强了LLM对未见过的领域的适应能力。Auto-CoT
    Zhang 等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib38)）通过提示LLM使用“让我们一步步思考”这一短语，推动了领域专业化，帮助引导模型生成推理链。此外，Izacard
    等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib16)）提出将一个相对轻量级的LLM与外部知识库集成，从而实现与更大模型（如PaLM
    Chowdhery 等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib9)））相当的性能。这些研究突显了LLM通过各种领域适应策略在适应特定领域方面的灵活性。
- en: Appendix B Notations
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 符号
- en: 'Table [5](https://arxiv.org/html/2410.17462v1#A2.T5 "Table 5 ‣ Appendix B Notations
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")
    presents all the notations we used in this paper.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](https://arxiv.org/html/2410.17462v1#A2.T5 "Table 5 ‣ Appendix B Notations
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")列出了我们在本文中使用的所有符号。'
- en: 'Table 5: Notation Table'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：符号表
- en: '| Symbol | Description |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\mathbf{x}$ | Input time series data |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| $\mathbf{x}$ | 输入的时间序列数据 |'
- en: '| $e_{s}$ | Domain-specific annotation from source domains |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| $e_{s}$ | 来源领域的领域特定注释 |'
- en: '| $e_{t}$ | Domain-specific annotation from target domain |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| $e_{t}$ | 目标领域的领域特定注释 |'
- en: '| $e_{d}$ | Domain-decontextualized annotation |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| $e_{d}$ | 领域去上下文化注释 |'
- en: '| $e_{g}$ | General annotation |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| $e_{g}$ | 一般注释 |'
- en: '| $f_{t}$ | Time-series-wise feature |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| $f_{t}$ | 时间序列特征 |'
- en: '| $f_{l}$ | Text-wise feature |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| $f_{l}$ | 文本特征 |'
- en: '| $\mathcal{J}$ | Domain-specific term (jargon) from target domain |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{J}$ | 目标领域的特定术语（行话） |'
- en: '| $\mathcal{M}_{d}$ | Domain decontextualizer |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{d}$ | 领域去上下文化器 |'
- en: '| $\mathcal{M}_{t}$ | Time-series-wise feature extractor |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{t}$ | 时间序列特征提取器 |'
- en: '| $\mathcal{M}_{l}$ | Text-wise feature extractor |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{l}$ | 文本特征提取器 |'
- en: '| $\mathcal{M}_{sel}$ | Feature selector |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{sel}$ | 特征选择器 |'
- en: '| $\mathcal{M}_{gen}$ | General annotator |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{gen}$ | 一般注释员 |'
- en: '| $\mathcal{M}_{jar}$ | domain-specific term extractor |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{jar}$ | 领域特定术语提取器 |'
- en: '| $\mathcal{M}_{spe}$ | Domain-specific annotator |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{spe}$ | 领域特定注释员 |'
- en: '| $\mathcal{M}_{rev}$ | Annotation reviewer |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}_{rev}$ | 注释审查器 |'
- en: '| $p_{de}$ | prompt of domain-decontextualization |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| $p_{de}$ | 领域去上下文化提示 |'
- en: '| $p_{l}$ | prompt of text-wise feature extraction |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| $p_{l}$ | 文本特征提取提示 |'
- en: '| $p_{score}$ | prompt of scoring |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| $p_{score}$ | 评分提示 |'
- en: '| $p_{gen}$ | prompt of general annotation |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| $p_{gen}$ | 一般注释提示 |'
- en: '| $p_{ext}$ | prompt of domain-specific term extraction |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| $p_{ext}$ | 领域特定术语提取提示 |'
- en: '| $p_{spe}$ | prompt of domain-specific annotation |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| $p_{spe}$ | 领域特定注释提示 |'
- en: '| $p_{rev}$ | prompt of annotation review |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| $p_{rev}$ | 注释审查提示 |'
- en: Appendix C More Details of Multi-modal Feature Extraction
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 多模态特征提取的更多细节
- en: C.1 Time-series Feature Extraction
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 时间序列特征提取
- en: Given a time series data $\mathbf{X}=\{(\mathbf{x}_{1},\cdots,\mathbf{x}_{L})\}$,
    we develop a time series extraction toolbox $\{f_{t}^{1},\ldots,f_{t}^{N_{t}}\}$
    to extract time-series-wise features from $\mathbf{X}$. Specifically, we include
    seasonality, trend, noise, moving average, lag feature, rolling window feature,
    and Fourier frequency as intra-variable time-series-wise features. For multivariate
    time series, we also consider inter-variable time-series-wise features, i.e.,
    mutual information, Pearson correlation, and canonical correlation.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 给定时间序列数据 $\mathbf{X}=\{(\mathbf{x}_{1},\cdots,\mathbf{x}_{L})\}$，我们开发了一个时间序列提取工具箱
    $\{f_{t}^{1},\ldots,f_{t}^{N_{t}}\}$ 用于从 $\mathbf{X}$ 中提取时间序列特征。具体而言，我们包括季节性、趋势、噪声、移动平均、滞后特征、滚动窗口特征和傅里叶频率作为变量内时间序列特征。对于多变量时间序列，我们还考虑变量间的时间序列特征，即互信息、皮尔逊相关系数和典型相关性。
- en: In particular, we employ Seasonal-Trend decomposition (STL) Cleveland et al.
    ([1990](https://arxiv.org/html/2410.17462v1#bib.bib10)) to extract seasonality,
    trend, and noise from the given time series data. To extract Fourier frequencies,
    the Fast Fourier Transform (FFT) Almeida ([1994](https://arxiv.org/html/2410.17462v1#bib.bib2))
    is applied to convert a time-domain signal into its frequency components. For
    the inter-variable time-series features, we use np.corrcoef to compute the Pearson
    correlation. To calculate mutual information, two time series are first discretized,
    followed by sklearn.metrics.mutual_info_score. To calculate canonical correlation,
    we first use sklearn.cross_decomposition to decompose two time series data, and
    then use np.corrcoef to obtain the correlation.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 特别地，我们采用季节性趋势分解（STL）Cleveland 等人（[1990](https://arxiv.org/html/2410.17462v1#bib.bib10)）来从给定的时间序列数据中提取季节性、趋势和噪声。为了提取傅里叶频率，应用快速傅里叶变换（FFT）Almeida（[1994](https://arxiv.org/html/2410.17462v1#bib.bib2)）将时域信号转换为其频率成分。对于变量间时间序列特征，我们使用
    np.corrcoef 来计算皮尔逊相关系数。为了计算互信息，我们首先将两个时间序列离散化，然后使用 sklearn.metrics.mutual_info_score。为了计算典型相关性，我们首先使用
    sklearn.cross_decomposition 对两个时间序列数据进行分解，然后使用 np.corrcoef 获取相关性。
- en: Appendix D More Details of Adaptive Feature Selection
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 自适应特征选择的更多细节
- en: D.1 Offline LLM-based Feature Selection
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 基于离线大语言模型的特征选择
- en: 'In some cases, we cannot input all the annotations to LLMs for calculating
    scores. We may split the annotations into several small batches and input the
    annotations in the small batches to calculate the score using Eq. ([4](https://arxiv.org/html/2410.17462v1#S3.E4
    "In 3.2 Adaptive Feature Selection ‣ 3 Methodology ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation")). After that, we will
    accumulate the scores from all batches to get the final scores of each feature/token
    and then select the features with the top-$k$ highest scores.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '在某些情况下，我们不能将所有注释输入到LLMs中以计算分数。我们可能会将注释拆分成几个小批次，然后将小批次的注释输入，以使用公式（[4](https://arxiv.org/html/2410.17462v1#S3.E4
    "In 3.2 Adaptive Feature Selection ‣ 3 Methodology ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation")）计算分数。之后，我们将从所有批次中累计分数，以获得每个特征/标记的最终分数，然后选择分数最高的前$k$个特征。'
- en: Appendix E Experimental Settings
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 实验设置
- en: E.1 Dataset Statistics
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 数据集统计
- en: 'Datasets. To evaluate the effectiveness of TESSA, five real-world datasets
    from distinct domains are considered: Stock, Health, Energy, Environment, and
    Social Good. Specifically, the stock dataset includes 1,935 US stocks with the
    recent 6-year data. The other four datasets come from the public benchmark Time-MMD Liu
    et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)). The dataset statistics
    are summarized in Table [6](https://arxiv.org/html/2410.17462v1#A5.T6 "Table 6
    ‣ E.1 Dataset Statistics ‣ Appendix E Experimental Settings ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '数据集。为了评估TESSA的有效性，我们考虑了来自不同领域的五个真实数据集：股票、健康、能源、环境和社会公益。具体而言，股票数据集包含1,935只美国股票，涵盖最近6年的数据。其他四个数据集来自公共基准Time-MMD
    Liu等人（[2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)）。数据集统计信息在表[6](https://arxiv.org/html/2410.17462v1#A5.T6
    "Table 6 ‣ E.1 Dataset Statistics ‣ Appendix E Experimental Settings ‣ Decoding
    Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中总结。'
- en: 'Additionally, we generate a synthetic dataset containing both time series and
    ground-truth annotations to directly assess the quality of the general annotations.
    The synthetic dataset is created by combining several key components from the
    time-series data:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们生成了一个合成数据集，包含时间序列和真实标签，以直接评估通用注释的质量。该合成数据集是通过将时间序列数据中的几个关键组件组合在一起创建的：
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Trend: Introduces an overall direction, which can be upward, downward, or mixed.'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 趋势：引入了一个整体方向，可以是向上、向下或混合的。
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Seasonality: Adds cyclical patterns, modeled using sine waves.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 季节性：加入周期性模式，通过正弦波进行建模。
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fourier Feature: Incorporates complex periodic behavior by combining multiple
    sine and cosine waves.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 傅里叶特征：通过结合多个正弦和余弦波，纳入复杂的周期性行为。
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Noise: Adds Gaussian noise to simulate random fluctuations and real-world imperfections.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 噪声：加入高斯噪声，以模拟随机波动和现实世界的缺陷。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rolling Window Features: Captures smoothed trends (mean) and local variability
    (max/min).'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 滚动窗口特征：捕捉平滑的趋势（均值）和局部的波动性（最大值/最小值）。
- en: •
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Lag Features: Uses past values to capture autocorrelation in the time series.'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 滞后特征：使用过去的值来捕捉时间序列中的自相关性。
- en: Ground-truth annotations are then generated by summarizing the key components
    of the synthetic time series.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 真实标签随后通过总结合成时间序列的关键组成部分来生成。
- en: In our synthetic dataset, we conduct $100$ times random generation of each components
    and then combine them together to get $100$ synthetic time series data, each with
    corresponding textual annotation.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的合成数据集中，我们进行每个组件的$100$次随机生成，并将其组合在一起，得到$100$个合成时间序列数据，每个数据都附带相应的文本注释。
- en: 'Table 6: Dataset Statistics'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：数据集统计
- en: '| Domain | Frequency | # Channels | # Timestamps | # Samples |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 频率 | 渠道数 | 时间戳数 | 样本数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| US Stock | Daily | 4 | 854,878 | 1,758 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 美国股票 | 每日 | 4 | 854,878 | 1,758 |'
- en: '| Health | Weekly | 1 | 1,389 | 1,356 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 健康 | 每周 | 1 | 1,389 | 1,356 |'
- en: '| Social Good | Monthly | 1 | 916 | 497 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | 每月 | 1 | 916 | 497 |'
- en: '| Energy | Daily | 1 | 1,622 | 1,586 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | 每日 | 1 | 1,622 | 1,586 |'
- en: '| Environment | Daily | 1 | 11,102 | 1,935 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | 每日 | 1 | 11,102 | 1,935 |'
- en: E.2 Baseline Methods
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 基准方法
- en: 'Three baselines are applied in our general annotation evaluation for downstream
    tasks:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的常规注释评估中，应用了三个基准模型用于下游任务：
- en: •
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'No Text: No textual data are utilized in the forecasting process.'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无文本：在预测过程中不使用任何文本数据。
- en: •
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Time-MMD Liu et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)):
    A multimodal benchmark for time series analysis that incorporates both time series
    and text data. To adapt this method to our setting, we apply the original text
    data from the target datasets in Liu et al. ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21))
    to the forecasting task.'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间-MMD 刘等人 ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21))：一个多模态基准，用于时间序列分析，融合了时间序列和文本数据。为了将此方法应用到我们的设置中，我们使用刘等人
    ([2024a](https://arxiv.org/html/2410.17462v1#bib.bib21)) 目标数据集中的原始文本数据进行预测任务。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'DirectLLM: Directly uses the annotations generated by LLMs for time series
    forecasting. In this paper, we compare several representative LLMs in our evaluations.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DirectLLM：直接使用LLM生成的注释进行时间序列预测。本文中，我们在评估中比较了几种代表性的LLM。
- en: Appendix F Additional Results for General Annotation Evaluation in Downstream
    Tasks
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 下游任务中通用注释评估的额外结果
- en: F.1 Implementation Details
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 实现细节
- en: Time Series Forecasting Models. We use Informer Zhou et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib39))
    as the forecasting model for the time series forecasting task. The model is configured
    with a dropout rate of $0.1$ and a learning rate of $0.0001$.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列预测模型。我们使用Informer Zhou等人 ([2021](https://arxiv.org/html/2410.17462v1#bib.bib39))
    作为时间序列预测任务的预测模型。该模型配置了$0.1$的dropout率和$0.0001$的学习率。
- en: 'Large Language Models. We utilize GPT-4o Achiam et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1)),
    along with two open-source models: LLaMA3.1-8B Dubey et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib12))
    and Qwen2-7B Yang et al. ([2024](https://arxiv.org/html/2410.17462v1#bib.bib34)).
    For the open-source models, we set temperature=1 and max_tokens=2048, while all
    other settings follow the defaults.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型。我们使用GPT-4o Achiam等人 ([2023](https://arxiv.org/html/2410.17462v1#bib.bib1))，以及两个开源模型：LLaMA3.1-8B
    Dubey等人 ([2024](https://arxiv.org/html/2410.17462v1#bib.bib12)) 和 Qwen2-7B Yang等人
    ([2024](https://arxiv.org/html/2410.17462v1#bib.bib34))。对于开源模型，我们设置temperature=1和max_tokens=2048，其它设置保持默认。
- en: Each experiment in our paper is conducted five times, with the average result
    reported. All models are trained on an Nvidia A6000 GPU with 48GB of memory.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本文中的每个实验都进行了五次，报告了平均结果。所有模型都在Nvidia A6000 GPU（48GB内存）上训练。
- en: F.2 Evaluation in Time Series Forecasting Tasks
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 时间序列预测任务中的评估
- en: 'The full results are presented in Table [7](https://arxiv.org/html/2410.17462v1#A6.T7
    "Table 7 ‣ F.2 Evaluation in Time Series Forecasting Tasks ‣ Appendix F Additional
    Results for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time
    Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). From
    the table, we can observe the following: (1) TESSA consistently outperforms all
    baselines across all settings, demonstrating its effectiveness in generating high-quality
    general annotations. (2) Among the three LLMs, GPT-4o-backed TESSA achieves the
    best performance, outperforming both LLaMA3.1-8B and Qwen2-7B. We attribute this
    to the higher quality of the annotations generated by GPT-4o compared to the other
    models, further emphasizing that high-quality annotations can significantly enhance
    downstream task performance.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '完整的结果展示在表格[7](https://arxiv.org/html/2410.17462v1#A6.T7 "Table 7 ‣ F.2 Evaluation
    in Time Series Forecasting Tasks ‣ Appendix F Additional Results for General Annotation
    Evaluation in Downstream Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")中。从表格中，我们可以观察到以下几点：（1）TESSA在所有设置下始终优于所有基准，展示了其在生成高质量通用注释方面的有效性。（2）在三种LLM中，基于GPT-4o的TESSA表现最佳，超越了LLaMA3.1-8B和Qwen2-7B。我们将其归因于GPT-4o生成的注释质量较其他模型更高，进一步强调了高质量的注释可以显著提升下游任务的表现。'
- en: 'Table 7: Comparison results in forecasting. Informer is the time series forecasting
    model.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 表格7：预测中的比较结果。Informer是时间序列预测模型。
- en: '| Domain | Backbone | Metrics | No Text | Time-MMD | DirectLLM | TESSA |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 主干 | 指标 | 无文本 | 时间-MMD | DirectLLM | TESSA |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Environment | GPT-4o | MSE | 1.2542 | 0.8483 | 0.7714 | 0.4629 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 环境 | GPT-4o | MSE | 1.2542 | 0.8483 | 0.7714 | 0.4629 |'
- en: '| MAE | 0.7387 | 0.6865 | 0.6604 | 0.4424 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 0.7387 | 0.6865 | 0.6604 | 0.4424 |'
- en: '| LLaMA3.1-8B | MSE | 1.2542 | 0.8483 | 0.8108 | 0.5654 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3.1-8B | MSE | 1.2542 | 0.8483 | 0.8108 | 0.5654 |'
- en: '| MAE | 0.7387 | 0.6865 | 0.6805 | 0.5128 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 0.7387 | 0.6865 | 0.6805 | 0.5128 |'
- en: '| Qwen2-7B | MSE | 1.2542 | 0.8483 | 0.7956 | 0.5824 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-7B | MSE | 1.2542 | 0.8483 | 0.7956 | 0.5824 |'
- en: '| MAE | 0.7387 | 0.6865 | 0.6729 | 0.5419 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 0.7387 | 0.6865 | 0.6729 | 0.5419 |'
- en: '| Energy | GPT-4o | MSE | 2.0117 | 0.2172 | 0.0575 | 0.0482 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | GPT-4o | MSE | 2.0117 | 0.2172 | 0.0575 | 0.0482 |'
- en: '| MAE | 1.1663 | 0.2139 | 0.0055 | 0.0040 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1663 | 0.2139 | 0.0055 | 0.0040 |'
- en: '| LLaMA3.1-8B | MSE | 2.0117 | 0.2172 | 0.1023 | 0.0531 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3.1-8B | MSE | 2.0117 | 0.2172 | 0.1023 | 0.0531 |'
- en: '| MAE | 1.1663 | 0.2139 | 0.0130 | 0.0049 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1663 | 0.2139 | 0.0130 | 0.0049 |'
- en: '| Qwen2-7B | MSE | 2.0117 | 0.2172 | 0.0824 | 0.0522 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-7B | MSE | 2.0117 | 0.2172 | 0.0824 | 0.0522 |'
- en: '| MAE | 1.1663 | 0.2139 | 0.0097 | 0.0048 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1663 | 0.2139 | 0.0097 | 0.0048 |'
- en: '| Social Good | GPT-4o | MSE | 2.1457 | 1.6072 | 0.4639 | 0.1935 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | GPT-4o | MSE | 2.1457 | 1.6072 | 0.4639 | 0.1935 |'
- en: '| MAE | 1.1205 | 0.9731 | 0.3801 | 0.0825 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1205 | 0.9731 | 0.3801 | 0.0825 |'
- en: '| LLaMA3.1-8B | MSE | 2.1457 | 1.6072 | 0.6720 | 0.3422 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3.1-8B | MSE | 2.1457 | 1.6072 | 0.6720 | 0.3422 |'
- en: '| MAE | 1.1205 | 0.9731 | 0.6138 | 0.2489 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1205 | 0.9731 | 0.6138 | 0.2489 |'
- en: '| Qwen2-7B | MSE | 2.1457 | 1.6072 | 0.5550 | 0.3651 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Qwen2-7B | MSE | 2.1457 | 1.6072 | 0.5550 | 0.3651 |'
- en: '| MAE | 1.1205 | 0.9731/ | 0.4850 | 0.2838 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 1.1205 | 0.9731/ | 0.4850 | 0.2838 |'
- en: F.3 Evaluation in Time Series Imputation Tasks
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 时间序列填充任务中的评估
- en: To demonstrate the effectiveness of TESSA in improving the performance of various
    downstream tasks, we further apply the generated general annotations in time series
    imputation task. Specifically, time series imputation task refers to the process
    of filling in missing or incomplete data points in a time series dataset, where
    some values are randomly mask.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明TESSA在提高各种下游任务性能方面的有效性，我们进一步将生成的通用注释应用于时间序列填充任务。具体而言，时间序列填充任务是指在时间序列数据集中填充缺失或不完整的数据点，其中一些值被随机遮蔽。
- en: 'Implementation Details. We implement the multi-modal time series imputation
    based on TSLib Wu et al. ([2023](https://arxiv.org/html/2410.17462v1#bib.bib31)).
    We use Informer Zhou et al. ([2021](https://arxiv.org/html/2410.17462v1#bib.bib39))
    as the forecasting model for the time series forecasting task. The model is configured
    with a dropout rate of $0.1$ and a learning rate of $0.0001$. GPT-4o is set as
    the LLM backbone. Other settings follow these in Section [F.1](https://arxiv.org/html/2410.17462v1#A6.SS1
    "F.1 Implementation Details ‣ Appendix F Additional Results for General Annotation
    Evaluation in Downstream Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation").'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '实现细节。我们基于TSLib Wu等人（[2023](https://arxiv.org/html/2410.17462v1#bib.bib31)）实现了多模态时间序列填充。我们使用Informer
    Zhou等人（[2021](https://arxiv.org/html/2410.17462v1#bib.bib39)）作为时间序列预测任务的预测模型。模型配置的dropout率为$0.1$，学习率为$0.0001$。GPT-4o被设置为LLM骨干网络。其他设置参见[F.1](https://arxiv.org/html/2410.17462v1#A6.SS1
    "F.1 Implementation Details ‣ Appendix F Additional Results for General Annotation
    Evaluation in Downstream Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")节中的内容。'
- en: 'Experimental Results. The experimental results are shown in Table [8](https://arxiv.org/html/2410.17462v1#A6.T8
    "Table 8 ‣ F.3 Evaluation in Time Series Imputation Tasks ‣ Appendix F Additional
    Results for General Annotation Evaluation in Downstream Tasks ‣ Decoding Time
    Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). From
    the table, we observe that TESSA consistently outperforms baselines in all datasets,
    demonstrating that TESSA’s annotations can significantly benefits various downstream
    tasks, including forecasting and imputation.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果。实验结果如表[8](https://arxiv.org/html/2410.17462v1#A6.T8 "Table 8 ‣ F.3 Evaluation
    in Time Series Imputation Tasks ‣ Appendix F Additional Results for General Annotation
    Evaluation in Downstream Tasks ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")所示。从表中可以看出，TESSA在所有数据集中的表现始终优于基线，证明TESSA的注释能够显著改善各种下游任务的性能，包括预测和填充。'
- en: 'Table 8: Imputation results with GPT-4o as the LLM backbone. Informer is the
    imputation model.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：使用GPT-4o作为LLM骨干网络的填充结果。Informer为填充模型。
- en: '| Metric | Domain | NoText | TimeMMD | DirectLLM | TESSA |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 域 | NoText | TimeMMD | DirectLLM | TESSA |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| MSE | Environment | 0.9718 | 0.9657 | 0.9453 | 0.5698 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| MSE | 环境 | 0.9718 | 0.9657 | 0.9453 | 0.5698 |'
- en: '| Energy | 0.9109 | 0.9081 | 0.9018 | 0.8690 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | 0.9109 | 0.9081 | 0.9018 | 0.8690 |'
- en: '| Social Good | 1.4971 | 0.9784 | 0.6873 | 0.5492 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | 1.4971 | 0.9784 | 0.6873 | 0.5492 |'
- en: '| MAE | Environment | 0.6872 | 0.6867 | 0.6973 | 0.5438 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| MAE | 环境 | 0.6872 | 0.6867 | 0.6973 | 0.5438 |'
- en: '| Energy | 0.8216 | 0.8176 | 0.8111 | 0.8075 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | 0.8216 | 0.8176 | 0.8111 | 0.8075 |'
- en: '| Social Good | 0.8371 | 0.7806 | 0.6036 | 0.5116 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 社会公益 | 0.8371 | 0.7806 | 0.6036 | 0.5116 |'
- en: Appendix G Additional Details of General Annotation Evaluation in Synthetic
    Datasets
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G：合成数据集中的通用注释评估的附加细节
- en: G.1 Implementation Details
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 实现细节
- en: 'To evaluate the effectiveness of TESSA in generating general annotations for
    synthetic time series using an LLM-as-a-judger approach, we set GPT-4o as the
    backbone of the judger. Two metrics, Clarity and Comprehensiveness, are used to
    assess the quality of the annotations:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估TESSA在使用LLM作为判断者的方法生成合成时间序列的通用注释的有效性，我们将GPT-4o设置为判断者的骨干网络。使用清晰度和完备性两个指标来评估注释的质量：
- en: •
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarity: Evaluates the clarity and readability of the annotations.'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清晰度：评估注释的清晰度和可读性。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensiveness: Assesses whether the annotations cover the most important
    patterns.'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完备性：评估注释是否覆盖了最重要的模式。
- en: Appendix H Additional Results of Domain-specific Annotation Evaluation
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 领域特定注释评估的附加结果
- en: H.1 Evaluation Metrics
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.1 评估指标
- en: 'We use the following three metrics to evaluate the quality of domain-specific
    annotations:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下三个指标来评估领域特定注释的质量：
- en: •
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Clarity: Assesses the clarity and readability of the annotations.'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清晰度：评估注释的清晰度和可读性。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Comprehensiveness: Checks whether the annotations cover the most important
    patterns.'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 完备性：检查注释是否覆盖了最重要的模式。
- en: •
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Domain-Relevance: Evaluates whether the annotations correctly apply domain-specific
    knowledge.'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 领域相关性：评估注释是否正确应用了领域特定知识。
- en: H.2 Additional Results on Other LLM Backbones
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H.2 其他LLM骨干网络的附加结果
- en: 'We report the evaluation results of the domain-specific annotations on the
    Energy and Social Good datasets in Table [9](https://arxiv.org/html/2410.17462v1#A8.T9
    "Table 9 ‣ H.2 Additional Results on Other LLM Backbones ‣ Appendix H Additional
    Results of Domain-specific Annotation Evaluation ‣ Decoding Time Series with LLMs:
    A Multi-Agent Framework for Cross-Domain Annotation") and Table [10](https://arxiv.org/html/2410.17462v1#A8.T10
    "Table 10 ‣ H.2 Additional Results on Other LLM Backbones ‣ Appendix H Additional
    Results of Domain-specific Annotation Evaluation ‣ Decoding Time Series with LLMs:
    A Multi-Agent Framework for Cross-Domain Annotation"), respectively. Similar observations
    are made in Section [4.4](https://arxiv.org/html/2410.17462v1#S4.SS4 "4.4 Domain-specific
    Annotation Evaluation ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation"), further demonstrating the effectiveness
    of TESSA in generating high-quality domain-specific annotations.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[9](https://arxiv.org/html/2410.17462v1#A8.T9 "Table 9 ‣ H.2 Additional
    Results on Other LLM Backbones ‣ Appendix H Additional Results of Domain-specific
    Annotation Evaluation ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")和表格[10](https://arxiv.org/html/2410.17462v1#A8.T10
    "Table 10 ‣ H.2 Additional Results on Other LLM Backbones ‣ Appendix H Additional
    Results of Domain-specific Annotation Evaluation ‣ Decoding Time Series with LLMs:
    A Multi-Agent Framework for Cross-Domain Annotation")中分别报告了在能源和社会公益数据集上进行的领域特定注释评估结果。第[4.4](https://arxiv.org/html/2410.17462v1#S4.SS4
    "4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")节中也做出了类似的观察，进一步证明了TESSA在生成高质量领域特定注释方面的有效性。'
- en: 'Table 9: Domain-specific annotation results on the Energy dataset with GPT-4o
    as the LLM backbone.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 表格9：以GPT-4o作为LLM骨干网络的能源数据集领域特定注释结果。
- en: '| Metric | Method | Mean | P(T>D) (%) |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 平均值 | P(T>D) (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clarity | TESSA | 4.79 | 99.35 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 4.79 | 99.35 |'
- en: '| DirectLLM | 3.48 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.48 |'
- en: '| Compre. | TESSA | 4.57 | 98.01 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 完备性 | TESSA | 4.57 | 98.01 |'
- en: '| DirectLLM | 3.10 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.10 |'
- en: '| Dom. Rel. | TESSA | 4.25 | 95.24 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 领域相关性 | TESSA | 4.25 | 95.24 |'
- en: '| DirectLLM | 3.01 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.01 |'
- en: '| Overall | TESSA | 4.57 | 98.31 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | TESSA | 4.57 | 98.31 |'
- en: '| DirectLLM | 3.35 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.35 |'
- en: 'Table 10: Domain-specific annotation results on the Social Good dataset with
    GPT-4o as the LLM backbone.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 表格10：以GPT-4o作为LLM骨干网络的社会公益数据集领域特定注释结果。
- en: '| Metric | Method | Mean | P(T>D) (%) |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 平均值 | P(T>D) (%) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clarity | TESSA | 4.68 | 99.61 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 4.68 | 99.61 |'
- en: '| DirectLLM | 3.28 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.28 |'
- en: '| Compre. | TESSA | 4.49 | 97.54 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 完备性 | TESSA | 4.49 | 97.54 |'
- en: '| DirectLLM | 3.26 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.26 |'
- en: '| Dom. Rel. | TESSA | 4.45 | 95.34 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 领域相关性 | TESSA | 4.45 | 95.34 |'
- en: '| DirectLLM | 3.33 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.33 |'
- en: '| Overall | TESSA | 4.48 | 97.16 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | TESSA | 4.48 | 97.16 |'
- en: '| DirectLLM | 3.29 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.29 |'
- en: Appendix I Additional Details of Ablation Studies
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录I 消融研究的附加细节
- en: I.1 Evaluation Metric
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.1 评估指标
- en: To evaluate the effectiveness of adaptive feature selection, we use an LLM-as-a-judger
    to assess the general annotations generated by TESSA and its variant, TESSA/F.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估自适应特征选择的有效性，我们使用LLM作为判断者来评估TESSA及其变种TESSA/F生成的通用注释。
- en: I.2 Qualitative Examples
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I.2 定性示例
- en: 'We present a qualitative example of extracting text-wise features using TESSA
    and TESSA/D, shown in Table [11](https://arxiv.org/html/2410.17462v1#A9.T11 "Table
    11 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies
    ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation").
    From the table, we observe that TESSA/D captures irrelevant features, such as
    higher prices over time and fun, which are unrelated to time series analysis.
    This supports our claim that domain-specific terminology can hinder the accurate
    extraction of time-series-relevant features.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了一个使用 TESSA 和 TESSA/D 提取逐文本特征的定性示例，如表 [11](https://arxiv.org/html/2410.17462v1#A9.T11
    "表 11 ‣ I.2 定性示例 ‣ 附录 I 消融研究附加详情 ‣ 使用 LLM 解码时间序列：跨领域注释的多代理框架") 所示。从表中可以看出，TESSA/D
    捕捉了无关特征，如随着时间推移的较高价格和趣味性，这些与时间序列分析无关。这支持了我们关于领域特定术语可能妨碍准确提取与时间序列相关特征的观点。
- en: 'We also provide another qualitative example in Table [13](https://arxiv.org/html/2410.17462v1#A9.T13
    "Table 13 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation") to demonstrate the effectiveness of adaptive feature selection. The
    annotations generated by TESSA/F tend to include numerous features without proper
    analysis. This illustrates that including too many features can reduce the clarity
    of the annotations, further emphasizing the importance of adaptive feature selection
    in improving annotation quality.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了另一个定性示例，在表 [13](https://arxiv.org/html/2410.17462v1#A9.T13 "表 13 ‣ I.2
    定性示例 ‣ 附录 I 消融研究附加详情 ‣ 使用 LLM 解码时间序列：跨领域注释的多代理框架") 中展示了自适应特征选择的有效性。TESSA/F 生成的注释往往包含大量特征，但没有进行适当分析。这说明包含过多特征会降低注释的清晰度，进一步强调了自适应特征选择在提高注释质量中的重要性。
- en: 'Table 11: Ablation studies of the impact of domain decontextualization. Red
    denotes the irrelevant features.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11：领域去语境化影响的消融研究。红色表示无关特征。
- en: '| TESSA’s extracted text-wise features: support level, resistance level, volume
    correlation, breakthrough, trend reversal, relative strength index, negative signal,
    positive signal, channel boundaries |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| TESSA 提取的逐文本特征：支撑位、阻力位、成交量相关性、突破、趋势反转、相对强弱指数、负信号、正信号、通道边界 |'
- en: '| TESSA/D’s extracted text-wise features: higher prices over time , autocorrelation,
    price increase, trend channel, stationary, fun, lower prices, outliers, breakdown,
    rising trend |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| TESSA/D 提取的逐文本特征：随着时间推移的较高价格、自动相关性、价格上涨、趋势通道、平稳性、趣味性、较低价格、离群值、崩溃、上升趋势 |'
- en: 'Table 12: Case study: Evaluation results of domain-specific annotation of time
    series data in Figure [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2 "In
    Figure 3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") from the Environment
    dataset. GPT-4o is the LLM backbone.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12：案例研究：图 [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2 "在图 3 ‣ 附录
    J 案例研究附加详情 ‣ 使用 LLM 解码时间序列：跨领域注释的多代理框架") 中环境数据集的时间序列数据领域特定注释的评估结果。GPT-4o 是 LLM
    主干。
- en: '| Metric | Method | Score |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 方法 | 得分 |'
- en: '| --- | --- | --- |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Clarity | TESSA | 5.0 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 清晰度 | TESSA | 5.0 |'
- en: '| DirectLLM | 3.0 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.0 |'
- en: '| Compre. | TESSA | 3.0 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Compre. | TESSA | 3.0 |'
- en: '| DirectLLM | 3.0 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.0 |'
- en: '| Dom. Rel. | TESSA | 5.0 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 领域关系 | TESSA | 5.0 |'
- en: '| DirectLLM | 3.0 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.0 |'
- en: '| Overall | TESSA | 4.3 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | TESSA | 4.3 |'
- en: '| DirectLLM | 3.0 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| DirectLLM | 3.0 |'
- en: 'Table 13: Ablation studies of the impact of adaptive feature selection. Two
    compared methods are used for generating general annotation ploted in Figure [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "In Figure 3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")(a). Purple highlights
    basic patterns, green highlights general patterns.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：自适应特征选择影响的消融研究。比较的两种方法用于生成图 [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "在图 3 ‣ 附录 J 案例研究附加详情 ‣ 使用 LLM 解码时间序列：跨领域注释的多代理框架") 中绘制的一般注释图（a）。紫色突出显示基本模式，绿色突出显示一般模式。
- en: '| TESSA’s generated general annotation: The time series data tracks a single
    variable over time, showing a gradual upward trend from around 6.96 to over 10.4\.
    There are clear seasonal patterns, with regular cycles of peaks and troughs, hinting
    at periodic influences like seasonal demand. The data exhibits strong support
    and resistance levels, where it hesitates at specific values before continuing
    its rise, demonstrating resilience. There is a consistent upward indicator trend,
    suggesting stable or improving underlying factors driving this increase. Overall,
    the data indicates a positive growth trajectory, with predictable cyclical fluctuations
    due to natural or recurring factors. |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| TESSA生成的总体注解：该时间序列数据跟踪了单一变量随时间的变化，显示出从约6.96到超过10.4的逐步上升趋势。数据中存在明显的季节性模式，具有规律的波峰和波谷周期，暗示着如季节性需求等周期性影响。数据表现出强劲的支撑位和阻力位，在特定数值附近有所犹豫，然后继续上涨，展现了其韧性。存在持续的上升指标趋势，表明推动这一增长的基本因素稳定或正在改善。总体来看，数据表明存在积极的增长轨迹，并由于自然或周期性因素产生可预测的周期性波动。
    |'
- en: '| TESSA/F’s generated general annotations: The provided time series data shows
    a clear upward trend, indicated by the gradual increase in trend values over time.
    Initially, fluctuations are minor, but as the series progresses, we observe larger
    increases, especially towards the end of the sequence, suggesting a strong positive
    momentum. The occasional deviations from the trend, as shown by the residue, hint
    at short-term fluctuations or potential anomalies. Seasonality seems to have minimal
    impact with slight cyclical patterns, possibly reflecting periodic but non-dominant
    effects. Overall, if this time series pertains to a scenario where higher values
    are desirable, such as sales or economic indicators, this upward trend and increased
    values could be considered as positive signals or a breakthrough. |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| TESSA/F生成的总体注解：提供的时间序列数据显示出明显的上升趋势，随着时间推移，趋势值逐渐增加。最初波动较小，但随着序列的推进，我们观察到更大的增长，尤其是在序列末尾，暗示了强劲的正向动力。偶尔的偏离趋势，如残差所示，暗示了短期波动或潜在的异常情况。季节性似乎影响较小，仅表现为轻微的周期性模式，可能反映了周期性但非主导的影响。总体来说，如果该时间序列与销售或经济指标等较高值较为理想的情景相关，那么这一上升趋势和增值可以被视为积极的信号或突破。
    |'
- en: Appendix J Additional Details of Case Studies
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 案例研究的附加细节
- en: 'More Details of the Case Studies in Section [4.6](https://arxiv.org/html/2410.17462v1#S4.SS6
    "4.6 Case Study of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") In this section, we provide additional
    details of the case studies in Section [4.6](https://arxiv.org/html/2410.17462v1#S4.SS6
    "4.6 Case Study of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation"). We select a representative time series
    from the Social Good domain, shown in Fig. [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "In Figure 3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")(b). In Table [14](https://arxiv.org/html/2410.17462v1#A10.T14
    "Table 14 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"), both the general
    and domain-specific annotations generated by DirectLLM and TESSA are reported.
    We also quantitatively evaluate the domain-specific annotations of TESSA and DirectLLM,
    following the setup outlined in Section [4.4](https://arxiv.org/html/2410.17462v1#S4.SS4
    "4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The evaluation
    results are presented in Table [12](https://arxiv.org/html/2410.17462v1#A9.T12
    "Table 12 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation").'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '更多的案例研究细节见于第[4.6](https://arxiv.org/html/2410.17462v1#S4.SS6 "4.6 Case Study
    of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent Framework
    for Cross-Domain Annotation")节。在这一节中，我们提供了第[4.6](https://arxiv.org/html/2410.17462v1#S4.SS6
    "4.6 Case Study of TESSA ‣ 4 Experiments ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")节案例研究的更多细节。我们从社会公益领域选择了一个具有代表性的时间序列，如图[3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "In Figure 3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")(b)所示。在表[14](https://arxiv.org/html/2410.17462v1#A10.T14
    "Table 14 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")中，报告了DirectLLM和TESSA生成的通用注释和领域特定注释。我们还根据第[4.4](https://arxiv.org/html/2410.17462v1#S4.SS4
    "4.4 Domain-specific Annotation Evaluation ‣ 4 Experiments ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")节中的设置，对TESSA和DirectLLM的领域特定注释进行了定量评估。评估结果呈现在表[12](https://arxiv.org/html/2410.17462v1#A9.T12
    "Table 12 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation")中。'
- en: From the table, we observe that (1) TESSA’s general annotations capture more
    meaningful patterns, enhancing user understanding and supporting downstream tasks,
    while DirectLLM only highlights basic trends; and (2) TESSA’s domain-specific
    annotations consistently outperform those of DirectLLM across all metrics, providing
    clearer, more comprehensive, and contextually relevant insights. Specifically,
    TESSA’s annotations are more fluent, more detailed and provide a richer analysis
    using domain-specific jargons, like economic momentum and labor market resilience,
    while the annotations of DirectLLM only simply analyze the trend of the unemployment
    rate, providing less insights.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 从表格中，我们可以观察到（1）TESSA的通用注释捕捉到更多有意义的模式，增强了用户理解并支持下游任务，而DirectLLM仅突出显示了基本趋势；（2）TESSA的特定领域注释在所有指标上始终优于DirectLLM，提供了更清晰、更全面且与上下文相关的见解。具体来说，TESSA的注释更加流畅、更加详细，并使用领域特定术语，如经济动能和劳动市场韧性，提供了更丰富的分析，而DirectLLM的注释仅仅简单分析了失业率的趋势，提供的见解较少。
- en: 'Case Study for Multivariate Time Series We then conduct a case study to demonstrate
    the effectiveness of TESSA in generating high-quality annotations for multivariate
    time series data. Specifically, we set the Finance dataset as the target domain.
    Health and Environment datasets are then applied in the source domains. The example
    multivariate time series data is shown in Fig. [5(d)](https://arxiv.org/html/2410.17462v1#A10.F5.sf4
    "In Figure 5 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"), where the multivarate
    time series data has four variables, i.e., price, volumn, relative strength index
    (RSI) and simple moving average (SMA). The generated annotations are shown in
    Table [15](https://arxiv.org/html/2410.17462v1#A10.T15 "Table 15 ‣ Appendix J
    Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation"). From the table, we observe that (i) TESSA’s
    generated annotations are more naturally than DirectLLM; (ii) DirectLLM interprets
    each variable independently by only focusing their trends. However, TESSA can
    capture the correlation between variables. This shows TESSA is able to analyze
    inter-variable patterns. These further imply the effectiveness of TESSA in generating
    high-quality domain-specific annotations for multivariate time series data.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量时间序列案例研究 我们接着进行一个案例研究，以展示TESSA在生成高质量的多变量时间序列数据注释方面的有效性。具体而言，我们将金融数据集设定为目标领域，健康和环境数据集则应用于源领域。示例的多变量时间序列数据如图[5(d)](https://arxiv.org/html/2410.17462v1#A10.F5.sf4
    "图5 ‣ 附录J 案例研究的更多细节 ‣ 使用LLMs解码时间序列：跨领域注释的多代理框架")所示，其中多变量时间序列数据包含四个变量：即价格、成交量、相对强弱指数（RSI）和简单移动平均线（SMA）。生成的注释如表[15](https://arxiv.org/html/2410.17462v1#A10.T15
    "表15 ‣ 附录J 案例研究的更多细节 ‣ 使用LLMs解码时间序列：跨领域注释的多代理框架")所示。从表中，我们观察到： (i) TESSA生成的注释比DirectLLM更自然；(ii)
    DirectLLM通过仅关注各变量的趋势，独立地解释每个变量。然而，TESSA能够捕捉变量之间的相关性。这表明TESSA能够分析变量间的模式。这进一步证明了TESSA在为多变量时间序列数据生成高质量领域特定注释方面的有效性。
- en: 'Case Study in the Synthetic Dataset. We further select an example from the
    synthetic dataset to conduct similar experiments to generate general annotations.
    The selected time series data is in Fig. [4](https://arxiv.org/html/2410.17462v1#A10.F4
    "Figure 4 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The qualitative
    example of the annotations of this time series data is shown in Table [16](https://arxiv.org/html/2410.17462v1#A10.T16
    "Table 16 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). From the table,
    we observe a discrepancy in DirectLLM’s analysis, as it detects 138 values in
    the time series data, despite there being only 120 values. This leads to inaccurate
    annotations. Moreover, DirectLLM captures only the basic trend of the time series,
    whereas TESSA identifies more significant patterns, such as the rolling window
    feature, seasonality, and resilience. This demonstrates the effectiveness of TESSA
    in providing more comprehensive and accurate annotations. We analyze the reason
    TESSA mitigates the hallucination seen in DirectLLM is that it highlights important
    patterns overlooked by LLMs, such as seasonality. By focusing on these patterns
    rather than just basic trends, LLMs can analyze and interpret time series data
    from multiple perspectives, leading to fewer hallucinations in the annotations.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 合成数据集中的案例研究 我们进一步从合成数据集中选择一个示例，进行类似的实验以生成通用注释。选定的时间序列数据如图[4](https://arxiv.org/html/2410.17462v1#A10.F4
    "图4 ‣ 附录J 案例研究的更多细节 ‣ 使用LLMs解码时间序列：跨领域注释的多代理框架")所示。该时间序列数据的定性注释示例如表[16](https://arxiv.org/html/2410.17462v1#A10.T16
    "表16 ‣ 附录J 案例研究的更多细节 ‣ 使用LLMs解码时间序列：跨领域注释的多代理框架")所示。从表中，我们可以看到DirectLLM分析中存在差异，因为它在时间序列数据中检测到138个值，尽管实际只有120个值。这导致了不准确的注释。此外，DirectLLM仅捕捉到时间序列的基本趋势，而TESSA能够识别出更显著的模式，例如滚动窗口特征、季节性和弹性。这证明了TESSA在提供更全面和准确注释方面的有效性。我们分析了TESSA能够减轻DirectLLM中出现的幻觉的原因，是因为它强调了LLMs忽视的重要模式，如季节性。通过聚焦于这些模式而非仅仅是基本趋势，LLMs可以从多个角度分析和解释时间序列数据，从而在注释中减少幻觉现象。
- en: 'Additional Examples on Various Domains. Additional examples are presented for
    the synthetic, environment, energy and social good datasets, respectively. Specifically,
    the general annotations of selected time series on the synthetic dataset that
    in Fig. [6](https://arxiv.org/html/2410.17462v1#A10.F6 "Figure 6 ‣ Appendix J
    Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") are shown in Tables [17](https://arxiv.org/html/2410.17462v1#A10.T17
    "Table 17 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") and [18](https://arxiv.org/html/2410.17462v1#A10.T18
    "Table 18 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). The domain-specific
    annotations of selected time series on the environment dataset (Fig. [7](https://arxiv.org/html/2410.17462v1#A10.F7
    "Figure 7 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")) are shown in
    Tables [19](https://arxiv.org/html/2410.17462v1#A10.T19 "Table 19 ‣ Appendix J
    Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") and [20](https://arxiv.org/html/2410.17462v1#A10.T20
    "Table 20 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). And the domain-specific
    annotations of time series on the energy dataset (Fig. [8](https://arxiv.org/html/2410.17462v1#A10.F8
    "Figure 8 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")) are shown in
    Tables [21](https://arxiv.org/html/2410.17462v1#A10.T21 "Table 21 ‣ Appendix J
    Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") and [22](https://arxiv.org/html/2410.17462v1#A10.T22
    "Table 22 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). Similarly, the
    domain-specific annotations of time series on the social good dataset (Fig. [9](https://arxiv.org/html/2410.17462v1#A10.F9
    "Figure 9 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")) are shown in
    Tables [23](https://arxiv.org/html/2410.17462v1#A10.T23 "Table 23 ‣ Appendix J
    Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") and [24](https://arxiv.org/html/2410.17462v1#A10.T24
    "Table 24 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation"). Similar observations
    to those in Table [12](https://arxiv.org/html/2410.17462v1#A9.T12 "Table 12 ‣
    I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation Studies ‣
    Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")
    and Table [16](https://arxiv.org/html/2410.17462v1#A10.T16 "Table 16 ‣ Appendix
    J Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation") are found.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '各领域的附加示例。分别为合成数据集、环境数据集、能源数据集和社会公益数据集提供了附加示例。具体而言，选定的合成数据集上的时间序列的一般注释，如图[6](https://arxiv.org/html/2410.17462v1#A10.F6
    "Figure 6 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")所示，见表[17](https://arxiv.org/html/2410.17462v1#A10.T17
    "Table 17 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")和[18](https://arxiv.org/html/2410.17462v1#A10.T18
    "Table 18 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")。环境数据集上选定的时间序列的领域特定注释（图[7](https://arxiv.org/html/2410.17462v1#A10.F7
    "Figure 7 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")）见表[19](https://arxiv.org/html/2410.17462v1#A10.T19
    "Table 19 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")和[20](https://arxiv.org/html/2410.17462v1#A10.T20
    "Table 20 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")。能源数据集上的时间序列的领域特定注释（图[8](https://arxiv.org/html/2410.17462v1#A10.F8
    "Figure 8 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")）见表[21](https://arxiv.org/html/2410.17462v1#A10.T21
    "Table 21 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")和[22](https://arxiv.org/html/2410.17462v1#A10.T22
    "Table 22 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")。类似地，社会公益数据集上的时间序列的领域特定注释（图[9](https://arxiv.org/html/2410.17462v1#A10.F9
    "Figure 9 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")）见表[23](https://arxiv.org/html/2410.17462v1#A10.T23
    "Table 23 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")和[24](https://arxiv.org/html/2410.17462v1#A10.T24
    "Table 24 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation")。与表[12](https://arxiv.org/html/2410.17462v1#A9.T12
    "Table 12 ‣ I.2 Qualitative Examples ‣ Appendix I Additional Details of Ablation
    Studies ‣ Decoding Time Series with LLMs: A Multi-Agent Framework for Cross-Domain
    Annotation")和表[16](https://arxiv.org/html/2410.17462v1#A10.T16 "Table 16 ‣ Appendix
    J Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation")中的观察结果类似。'
- en: '![Refer to caption](img/b9c1f596b5295e44dd9dfdc722d97c6c.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b9c1f596b5295e44dd9dfdc722d97c6c.png)'
- en: (a) Ablation study
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 消融研究
- en: '![Refer to caption](img/8cd167af1e6fb6b8e2c55bf1dcea2129.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8cd167af1e6fb6b8e2c55bf1dcea2129.png)'
- en: (b) Case study
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 案例研究
- en: 'Figure 3: Selected time series data from Social Good dataset for ablation studies
    and case studies. (a) for ablation studies ; and (b) is for case studies.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：来自 Social Good 数据集的选定时间序列数据，用于消融研究和案例研究。(a) 为消融研究；(b) 为案例研究。
- en: .
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: 'Table 14: A case study in Social Good dataset. The target time series data
    is shown in Figure [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2 "In Figure
    3 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights domain-specific
    patterns.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14：Social Good 数据集中的案例研究。目标时间序列数据如图 [3(b)](https://arxiv.org/html/2410.17462v1#A10.F3.sf2
    "图 3 ‣ 附录 J 案例研究附加细节 ‣ 使用 LLM 解码时间序列：一个跨领域注释的多代理框架") (a) 所示。紫色突出显示基本模式，绿色突出显示一般模式，黄色突出显示特定领域模式。
- en: '| TESSA’s general annotations The time series data shows a generally decreasing
    trend, initially rising until a midpoint before consistently declining. Seasonal
    patterns are evident, with regular peaks around specific intervals. The series
    experiences rapid positive and negative fluctuations, reflecting significant changes
    in the measured system. Support and resistance levels are observed, with values
    frequently pausing near 7.0 and struggling to exceed 10.5\. Past values have an
    impact on current data, indicating that recent patterns help predict future outcomes.
    DirectLLM’s general annotations The uni-variate time series data exhibits fluctuating
    values with an overall descending trend. Initially, the data is relatively stable
    around the 9.5 to 10.5 range, peaking at 11.4 before descending gradually. Midway
    through the series, after reaching a peak, there is a noticeable decline as values
    drop from approximately 11 to levels around 8\. By the end, the values stabilize
    somewhat but decline further, reaching a low near 7\. This pattern suggests an
    initial increase followed by a downward trend and final stabilization at lower
    levels, indicating a potential cycle or shift in the underlying phenomenon being
    measured. |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| TESSA 的一般注释：时间序列数据表现出一个普遍的下降趋势，初期上升直至达到中点，然后持续下降。季节性模式明显，特定区间周围有规律的峰值。该序列经历了快速的正负波动，反映了被测系统的显著变化。观察到支撑位和阻力位，值通常在
    7.0 附近暂停，并难以突破 10.5。过去的值对当前数据有影响，表明近期模式有助于预测未来结果。DirectLLM 的一般注释：单变量时间序列数据表现出波动的值并呈现总体下降趋势。最初，数据在
    9.5 到 10.5 范围内相对稳定，达到 11.4 的峰值后逐渐下降。在序列的中间部分，达到峰值后，值显著下降，从大约 11 降至接近 8 的水平。到序列末尾，值有所稳定，但进一步下降，达到接近
    7 的低点。该模式表明初期上升后出现下降趋势，最终稳定在较低的水平，暗示了被测现象可能存在周期性或变化。 |'
- en: '| TESSA’s domain-specific annotations The unemployment rate shows a pattern
    consistent with trends in labor market resilience and economic momentum. Initially
    stable, with rates between 9 and 10, there is a gradual upward movement indicating
    early signs of economic growth and potential tightening in the labor market, peaking
    around 11\. This is followed by a downturn, reflecting possible macroeconomic
    constraints or shifts that reduce economic momentum, as the rate drops to approximately
    7.5 towards the end of the period. The rates around 9 and 11 can be seen as support
    and resistance thresholds for employment, where the market experiences inertia.
    Notably, the subtle seasonal fluctuations suggest the presence of moderate cyclic
    factors influencing employment trends, helping to understand broader economic
    activity patterns. DirectLLM’s domain-specific annotations The time series data
    represents fluctuations in the unemployment rate over a specific period. Initially,
    the unemployment rate starts quite high, around 9.5%, and exhibits slight ups
    and downs but generally remains above 9% until a significant increase is observed,
    reaching its peak at 11.4%. Following this peak, the rate gradually declines,
    showing an overall downward trend with a notable reduction to as low as 7%. This
    decline suggests a potential improvement in the employment situation, perhaps
    due to economic recovery or effective employment policies. Towards the end of
    the series, the unemployment rate stabilizes around 7-8%, although a slight increase
    is seen again, indicating the dynamic nature of employment conditions. |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注释 失业率显示出与劳动力市场韧性和经济动能趋势一致的模式。最初稳定，失业率在9到10之间，随后逐渐上升，表明经济增长的初步迹象以及劳动力市场潜在的紧缩，峰值大约在11左右。接着是下滑，反映出可能的宏观经济约束或变化，这些变化减少了经济动能，失业率在周期末期降至大约7.5。失业率在9和11附近的水平可以视为就业的支撑和阻力阈值，市场在此处出现惯性。值得注意的是，微妙的季节性波动表明存在一定的周期性因素影响就业趋势，有助于理解更广泛的经济活动模式。
    DirectLLM的领域特定注释 该时间序列数据代表了失业率在特定时期内的波动。最初，失业率较高，约为9.5%，并且在稍有起伏的情况下，整体保持在9%以上，直到出现显著上升，峰值为11.4%。在这一峰值之后，失业率逐渐下降，呈现整体下行趋势，显著下降至约7%。这一下降表明就业状况可能有所改善，或许是由于经济复苏或有效的就业政策。到序列的末期，失业率稳定在7-8%之间，尽管再次出现轻微上升，表明就业状况的动态性质。
    |'
- en: 'Table 15: A case study in Finance dataset. The target multivariate time series
    data is shown in Figure [5(d)](https://arxiv.org/html/2410.17462v1#A10.F5.sf4
    "In Figure 5 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). GPT-4o is
    the LLM backbone. Purple highlights basic patterns, green highlights general patterns,
    yellow highlights domain-specific patterns and blue highlights correlations between
    variables in multivariate time series data.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '表15：财务数据集中的案例研究。目标多元时间序列数据见图[5(d)](https://arxiv.org/html/2410.17462v1#A10.F5.sf4
    "图5 ‣ 附录J案例研究附加细节 ‣ 解码时间序列与LLMs: 一个跨领域注释的多代理框架") (a)。GPT-4o是LLM的核心。紫色高亮显示基本模式，绿色高亮显示一般模式，黄色高亮显示领域特定模式，蓝色高亮显示多元时间序列数据中变量之间的相关性。'
- en: '| TESSA’s domain-specific annotations Compass Digital Acquisition Unit’s stock
    price shows a notable pattern of rising and falling periodically, indicating seasonality
    with stable long-term trends interrupted by short-term fluctuations. There are
    key resistance levels around intervals 134, 270, and 403, where prices peak before
    dipping. The stock volume demonstrates significant spikes at specific points,
    suggesting irregular activity, particularly around values 7000 and 80500, which
    may indicate volume bursts or unusual market events. The relative strength index
    (RSI) also reveals a recurring pattern, gradually trending upward before a sharp
    decline, reflecting a cycle of growth and subsequent drop. Overall, the mild positive
    correlation between stock price and RSI indicates that periodic changes in price
    are somewhat echoed in RSI patterns, potentially offering predictive insights
    for future stock movements. DirectLLM’s domain-specific annotations The provided
    time series data consists of three primary features: price, volume, and relative
    strength index (RSI). Over the observation period, the price demonstrates an overall
    upward trend, starting around $9.74, exhibiting fluctuations, and rising to hover
    around $10.81 towards the end. Notable price spikes correspond with significant
    increases in trading volume, indicating periods of high trading activity, such
    as jumps to 80,500 and 100,200 in volume. Additionally, the RSI values range sharply,
    highlighting areas of overbought conditions (RSI approaching or at 100) and oversold
    conditions (RSI dropping around or below 50). These RSI changes suggest periods
    of potential buying or selling pressure, mirroring the observed price moves. |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注释显示，Compass数字采集单元的股票价格呈现出明显的周期性波动模式，表明存在季节性趋势，长期稳定的趋势被短期波动所打断。关键的阻力位出现在134、270和403这些区间，价格在这些点之前会达到峰值然后下跌。股票交易量在特定时刻表现出显著的波动，暗示存在不规则的活动，尤其是在7000和80500这两个值附近，可能表示交易量的剧增或异常的市场事件。相对强弱指数（RSI）也显示出一个反复出现的模式，即在逐渐上升后出现急剧下降，反映了一个增长和随后的下跌周期。总体而言，股票价格与RSI之间的轻微正相关性表明，价格的周期性变化在一定程度上与RSI模式相呼应，可能为未来股票的走势提供预测性洞察。DirectLLM的领域特定注释提供的时间序列数据包括三个主要特征：价格、交易量和相对强弱指数（RSI）。在观察期内，股票价格呈现总体上升趋势，起始值大约为$9.74，经历波动后，最终价格徘徊在$10.81附近。显著的价格波动与交易量的大幅增加相对应，表明存在高交易活动期，例如交易量跃升至80,500和100,200。此外，RSI值变化剧烈，突出显示了超买状态（RSI接近或达到100）和超卖状态（RSI下降至50或以下）。这些RSI变化提示了可能的买入或卖出压力，与观察到的价格变化相吻合。
    |'
- en: 'Table 16: A case study in the synthetic dataset. The selected time series data
    is shown in Fig. [4](https://arxiv.org/html/2410.17462v1#A10.F4 "Figure 4 ‣ Appendix
    J Additional Details of Case Studies ‣ Decoding Time Series with LLMs: A Multi-Agent
    Framework for Cross-Domain Annotation"), where the time series data has 120 data
    points. Purple highlights basic patterns, green highlights general patterns, and
    red highlights hallucinations.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '表16：合成数据集的案例研究。所选的时间序列数据如图[4](https://arxiv.org/html/2410.17462v1#A10.F4 "Figure
    4 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series with
    LLMs: A Multi-Agent Framework for Cross-Domain Annotation")所示，时间序列数据包含120个数据点。紫色高亮显示基本模式，绿色高亮显示一般模式，红色高亮显示虚假模式。'
- en: '| TESSA’s general annotations This time series exhibits notable fluctuations
    with values ranging from 35.0 to 118.0\. The trend indicates a general decline,
    starting around 65.26 and ending around 53.58 reflecting a significant downward
    movement over time. Periodic peaks are captured by the rolling window feature
    showing intermittent spikes up to 118.0, suggesting instances of strong breakout
    behavior despite an overall support decrease. The seasonality component reveals
    cyclical patterns with both positive and negative offsets, indicating underlying
    periodic forces affecting the data, akin to an oscillator. The alignment between
    the lag feature and the actual values underscores a historical reliance or resilience
    of the present data points on past values. reflecting consistency in patterns.
    DirectLLM’s general annotations The provided time series data consists of 138
    values and exhibits significant fluctuations. There are notable peaks at several
    points such as 118.0 and 113.0, indicating occasional high spikes in values. Conversely,
    values also drop to lows around 35.0, suggesting substantial variability. Most
    data points seem to oscillate around a mid-range, between approximately 50.0 and
    70.0\. This pattern suggests periods of stability interspersed with sporadic increases
    and decreases. Analyzing such a dataset can be indicative of a highly dynamic
    system or process with frequent transitions between states of varying magnitude.
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的一般注释：此时间序列表现出显著的波动，数值范围从35.0到118.0。趋势表明总体下降，从大约65.26开始，结束时约为53.58，反映了随着时间推移的显著下行运动。通过滚动窗口特征捕捉到周期性的峰值，显示出偶尔的突发高峰，最高可达118.0，尽管整体支撑下降，仍能发现强势突破行为。季节性成分揭示了带有正负偏移的周期模式，表明潜在的周期性力量在影响数据，类似于振荡器。滞后特征与实际值之间的对齐强调了当前数据点在过去值上的历史依赖或韧性，反映出模式的一致性。DirectLLM的一般注释：提供的时间序列数据包含138个值，表现出显著波动。多个点上有明显的峰值，例如118.0和113.0，表明值出现偶尔的高峰。相反，数值也降至35.0左右，表明存在较大的变动。大多数数据点似乎围绕中值波动，大约在50.0到70.0之间。这个模式表明稳定期与零星的上涨和下跌交替出现。分析这样的数据集可以表明一个高度动态的系统或过程，具有频繁的状态转换，幅度不同。
    |'
- en: '![Refer to caption](img/750f5c841ccd7b54c286183d08a22692.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/750f5c841ccd7b54c286183d08a22692.png)'
- en: 'Figure 4: Case study: A selected time series data from the synthetic dataset.
    The time series data has 120 data points. OT denotes the target variable.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：案例研究：来自合成数据集的一个选定时间序列数据。该时间序列数据包含120个数据点。OT表示目标变量。
- en: '![Refer to caption](img/9f288e7679171eea1920fbe7e334c3b3.png)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f288e7679171eea1920fbe7e334c3b3.png)'
- en: (a) Price
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 价格
- en: '![Refer to caption](img/9816d9b7c2dd45dfd404fa3f6cc8f855.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9816d9b7c2dd45dfd404fa3f6cc8f855.png)'
- en: (b) Volume
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 成交量
- en: '![Refer to caption](img/66f2145478def5796070e995f8c0e156.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/66f2145478def5796070e995f8c0e156.png)'
- en: (c) RSI
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 相对强弱指数（RSI）
- en: '![Refer to caption](img/2681c377a8514b9b51ada9515dfb944a.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2681c377a8514b9b51ada9515dfb944a.png)'
- en: (d) SMA
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 简单移动平均线（SMA）
- en: 'Figure 5: Case study: a multivariate time series data from the Finance dataset,
    which has four variables, i.e., price, volumn, RSI and SMA.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：案例研究：来自金融数据集的多变量时间序列数据，包含四个变量：价格、成交量、RSI和SMA。
- en: .
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '![Refer to caption](img/8a75277c7ac577340f243c55a8a79c62.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a75277c7ac577340f243c55a8a79c62.png)'
- en: (a)
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/e3e266af0d2ba136bcfdfb2292cf18c9.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e3e266af0d2ba136bcfdfb2292cf18c9.png)'
- en: (b)
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 6: More selected time series data from the synthetic dataset. The time
    series data has 120 data points.'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：来自合成数据集的更多选定时间序列数据。该时间序列数据包含120个数据点。
- en: '![Refer to caption](img/86fae173d3b889335f354559880ed312.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86fae173d3b889335f354559880ed312.png)'
- en: (a)
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/545009b77cdc4a1c4a7579f47fc1b636.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/545009b77cdc4a1c4a7579f47fc1b636.png)'
- en: (b)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 7: More selected time series data from the Environment dataset. The
    time series data has 120 data points.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：来自环境数据集的更多选定时间序列数据。该时间序列数据包含120个数据点。
- en: '![Refer to caption](img/55b8118f48fd356e7ee41366ca902abd.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/55b8118f48fd356e7ee41366ca902abd.png)'
- en: (a)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/f0cb4f20891e1577716706f5d7d6defd.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f0cb4f20891e1577716706f5d7d6defd.png)'
- en: (b)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 8: More selected time series data from the Energy dataset. The time
    series data has 36 data points.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：来自能源数据集的更多选定时间序列数据。该时间序列数据包含36个数据点。
- en: '![Refer to caption](img/9287cf1a84acbaa9d8fdaf9ea8032359.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9287cf1a84acbaa9d8fdaf9ea8032359.png)'
- en: (a)
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/686a894ff56a398e6b9545af35b7f338.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/686a894ff56a398e6b9545af35b7f338.png)'
- en: (b)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 9: More selected time series data from the Social Good dataset. The
    time series data has 36 data points.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：来自社会公益数据集的更多选定时间序列数据。该时间序列数据有36个数据点。
- en: 'Table 17: One more example of general annotation generation in the synthetic
    dataset. The selected dataset is shown in Fig. [6](https://arxiv.org/html/2410.17462v1#A10.F6
    "Figure 6 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). Purple highlights
    basic patterns, green highlights general patterns, and red highlights hallucinations.
    DirectLLM only captures on the basic trend pattern for time series annotation,
    while TESSA considers more important general patterns.'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '表17：合成数据集中另一个通用注释生成的示例。选定的数据集显示在图[6](https://arxiv.org/html/2410.17462v1#A10.F6
    "Figure 6 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a)中。紫色突出显示基本模式，绿色突出显示一般模式，红色突出显示幻觉。DirectLLM仅捕捉时间序列注释中的基本趋势模式，而TESSA则考虑更重要的一般模式。'
- en: '| TESSA’s general annotations The time series exhibits a complex behavior with
    an overall downward trend, accompanied by periods of fluctuation and temporary
    upward corrections. Key points include support and resistance levels, where the
    trend either pauses or reverses, notably around values 10.834 and 8.929\. Increased
    volatility is evident, indicating active trading periods, which align with higher
    variability. Trend channels are apparent, where the data moves within upper and
    lower boundaries, particularly showing both descending and emerging upward trends.
    In some instances, breakout thresholds highlight significant changes, signaling
    momentum shifts. DirectLLM’s general annotations This uni-variate time series
    data exhibits a pattern with multiple phases of rise and fall, indicative of periodic
    fluctuations over time. Initially, the series starts at a moderate level, gradually
    ascending to a peak around the values of 7.58 and 7.52 before experiencing a gradual
    decline. The data then showcases another rise peaking just above 9 and 10 marks,
    followed by a sharp and continuous decline, entering negative territory around
    the value of -6.270 and continuing downwards. Near the end of the series, escalating
    towards positive values and climaxing at 5.091\. The overall structure suggests
    well-defined periodic or seasonal trends, potentially influenced by external or
    inherent factors. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的通用注释 该时间序列表现出复杂的行为，整体呈下降趋势，伴随有波动和暂时的上行修正。关键点包括支撑位和阻力位，在这些位置，趋势要么暂停，要么反转，特别是在10.834和8.929的值附近。波动性增加，表明活跃的交易时期，与较高的变异性一致。趋势通道显现，数据在上下边界之间波动，尤其显示出下降趋势和新兴的上行趋势。在某些情况下，突破阈值突出显示了显著变化，预示着动能的变化。
    DirectLLM的通用注释 该单变量时间序列数据展示了一个具有多个上涨和下跌阶段的模式，表明随时间的周期性波动。最初，序列从一个适中的水平开始，逐渐上升到大约7.58和7.52的峰值，然后经历逐渐下降。数据随后显示出另一次上升，峰值略高于9和10，接着是一个急剧且持续的下降，进入负区间，约在-6.270的值附近，并继续向下。接近序列结束时，数据逐渐上升，达到5.091的高点。整体结构表明了明确的周期性或季节性趋势，可能受到外部或固有因素的影响。'
- en: 'Table 18: One more example of general annotation generation in the synthetic
    dataset. The selected dataset is shown in Fig. [6](https://arxiv.org/html/2410.17462v1#A10.F6
    "Figure 6 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (b). Purple highlights
    basic patterns, green highlights general patterns, and red highlights hallucinations.
    DirectLLM only captures on the basic trend pattern for time series annotation,
    while TESSA considers more important general patterns.'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '表18：合成数据集中另一个通用注释生成的示例。选定的数据集显示在图[6](https://arxiv.org/html/2410.17462v1#A10.F6
    "Figure 6 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (b)中。紫色突出显示基本模式，绿色突出显示一般模式，红色突出显示幻觉。DirectLLM仅捕捉时间序列注释中的基本趋势模式，而TESSA则考虑更重要的一般模式。'
- en: '| TESSA’s general annotations The time series data illustrates an overall upward
    trend characterized by increasing values over time, signifying growth. Initially,
    the series displays stability with minor fluctuations, often not dropping below
    certain support levels, indicating consistency. As the series advances, breakthrough
    points become apparent, where values exceed previous resistance levels, suggesting
    heightened momentum and possible shifts in liquidity that drive this progress.
    The later parts of the data feature more pronounced scending trends, pointing
    to a stronger upward movement. Periodic trend reversals also appear, reflecting
    temporary changes before returning to the dominant upward trend, which highlights
    the series’ dynamic nature and potential for fluctuations. DirectLLM’s general
    annotations The given uni-variate time series data exhibits a fluctuating pattern
    with a general upward trend. Initially, from the first data point, there is a
    notable increase in values, reaching a peak around the seventh value. This is
    followed by a gradual decline and subsequent stabilization with minor fluctuations
    between the 12th and 31st values. Notably, around the 84th value, a significant
    surge in values begins, culminating in a prominent steep increase towards the
    end of the series, suggesting a potential exponential growth or shift occurring
    in the data. Overall, the time series transitions from more stable periods into
    a pronounced upward trend, signaling potential external influences or underlying
    factors driving the increase. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的总体注释：该时间序列数据展示了一个整体的上升趋势，表现为随着时间推移，数值不断增加，意味着增长。最初，序列显示出稳定性，波动较小，且通常不会跌破某些支撑水平，表明一致性。随着序列的推进，突破点逐渐显现，数值超过了先前的阻力水平，暗示着动力的增强和可能的流动性变化，这推动了这一进程。数据的后期部分呈现出更加明显的上升趋势，表明上升运动更为强劲。周期性的趋势反转也会出现，反映了暂时性的变化，然后回到主导的上升趋势，突显了该序列的动态特性和波动的潜力。DirectLLM的总体注释：给定的单变量时间序列数据表现出波动模式，具有整体上升的趋势。最初，从第一个数据点开始，数值显著增加，在第七个值附近达到顶峰。随后出现逐渐下降，并在第12到31个值之间稳定下来，波动较小。特别地，在第84个值附近，数值开始大幅上升，最终呈现出明显的陡峭增长，暗示数据中可能出现指数增长或变化。总体而言，时间序列从更稳定的时期过渡到显著的上升趋势，预示着可能存在外部因素或潜在因素推动这一增长。'
- en: 'Table 19: One more example of general annotation generation in the Environment
    dataset. The selected dataset is shown in Fig. [7](https://arxiv.org/html/2410.17462v1#A10.F7
    "Figure 7 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 表19：环境数据集中一般注释生成的另一个示例。所选数据集如图[7](https://arxiv.org/html/2410.17462v1#A10.F7
    "图7 ‣ 附录J 案例研究的额外细节 ‣ 使用LLM解码时间序列：跨领域注释的多代理框架")（a）所示。紫色标记基本模式，绿色标记一般模式，黄色标记特定领域的模式。
- en: '| TESSA’s domain-specific annotations The air quality index (AQI) data exhibits
    significant fluctuations, with values ranging from 53 to 235 over time, indicating
    variability in air quality. Support levels around values like 100 and 140 suggest
    periods when air quality temporarily stabilizes or improves. On the other hand,
    resistance levels near values like 200 and 235 show points where air quality struggles
    to improve further before worsening. Several distinct upward trends, particularly
    from AQI values 70 to 150 and 177 to 235, indicate temporary periods of improvement
    in air quality, whereas downward trends around values 166 to 123 and 208 to 84
    reflect deteriorating air quality after peaks. Monitoring these trends and critical
    thresholds will be essential for identifying and responding to significant pollution
    events effectively. DirectLLM’s domain-specific annotations The time series data
    represents the air quality index (AQI) over a series of observations, showing
    fluctuations in air pollution levels. Initially, the AQI values are moderate,
    transitioning to higher levels, peaking at alarming numbers such as 235 and 209,
    which indicate very unhealthy air quality. This indicates potential spikes in
    pollution that could be associated with environmental events or increased urban
    activity. Periods of lower AQI values suggest moments of improved air quality,
    but these are often followed by sharp increases, highlighting the inconsistency
    and poor air conditions in the observed timeframe. Overall, the data reflects
    significant air quality concerns, emphasizing the need for monitoring and potential
    interventions to safeguard public health. |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定标注 空气质量指数（AQI）数据显示显著波动，数值在53到235之间波动，表明空气质量的变化性。像100和140这样的支持水平表明空气质量暂时稳定或改善的时期。另一方面，像200和235这样的阻力水平则显示出空气质量在进一步改善前面临困难的节点。多个明显的上升趋势，尤其是从AQI值70到150以及177到235之间，表明空气质量的暂时改善期，而166到123和208到84之间的下降趋势则反映了在高峰之后空气质量恶化。监控这些趋势和关键阈值对于有效识别和应对重大污染事件至关重要。
    DirectLLM的领域特定标注 时间序列数据表示一系列观测值中的空气质量指数（AQI），显示空气污染水平的波动。最初，AQI值处于中等水平，逐渐升高，并在235和209等令人担忧的数值处达到峰值，表明空气质量极差。这表明可能出现污染高峰，可能与环境事件或城市活动增加有关。较低的AQI值代表空气质量改善的时刻，但这些通常会被急剧上升的数值跟随，突显出观察期内空气质量的不稳定和差。总体而言，数据反映了空气质量的重大问题，强调了监控和可能干预的必要性，以保护公众健康。
    |'
- en: 'Table 20: One more example of general annotation generation in the Environment
    dataset. The selected dataset is shown in Fig. [7](https://arxiv.org/html/2410.17462v1#A10.F7
    "Figure 7 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (b). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 表20：环境数据集中一般标注生成的另一个示例。所选数据集如图[7](https://arxiv.org/html/2410.17462v1#A10.F7
    "图7 ‣ 附录J 案例研究附加细节 ‣ 使用LLMs解码时间序列：一个跨领域标注的多代理框架")（b）所示。紫色高亮显示基本模式，绿色高亮显示一般模式，黄色高亮显示领域特定模式。
- en: '| TESSA’s domain-specific annotations The air quality index (AQI) exhibits
    noticeable upward and downward trends over time, indicating periods of improvement
    and decline in air quality. Regular seasonal patterns are apparent, with AQI values
    cyclically rising and falling, suggesting that certain times of the year may be
    more prone to higher pollution levels. Critical thresholds in AQI levels highlight
    points where air quality significantly improves or deteriorates, with some values
    acting as resistance levels that AQI struggles to surpass. Episodes of high variability
    in AQI indicate times of significant fluctuations, potentially due to varying
    pollution sources or climate conditions. Lastly, breakout patterns where AQI values
    suddenly shift indicate potential changes in pollution control effectiveness or
    new influences on air quality dynamics. DirectLLM’s domain-specific annotations
    The provided time series data represents fluctuations in the air quality index
    (AQI), with values indicating varying levels of air pollution over time. Initially,
    the AQI remains in a moderate range, typically below 100, but there is a noticeable
    spike as values reach up to 220, indicating very poor air quality. This suggests
    that certain periods experienced significantly higher pollution levels, which
    can have serious implications for public health and environmental quality. The
    data shows some improvements, but the peaks raise concerns about excessive pollution,
    particularly during certain months. Continuous monitoring and mitigation of pollution
    sources are essential to reduce the frequency and severity of these dangerous
    air quality levels. |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注释 空气质量指数（AQI）随时间呈现明显的上升和下降趋势，显示出空气质量的改善和恶化周期。定期的季节性模式显现，AQI值周期性地升高和降低，暗示一年中的某些时段可能更容易出现较高的污染水平。AQI值的关键阈值突出了空气质量显著改善或恶化的节点，一些值作为AQI难以突破的阻力水平。AQI的高波动时期表明空气质量有显著波动，可能与污染源变化或气候条件变化有关。最后，AQI值突然变化的突破模式表明污染控制效果的潜在变化或新的影响因素对空气质量动态产生影响。
    DirectLLM的领域特定注释 提供的时间序列数据表示空气质量指数（AQI）的波动，值表示空气污染的不同程度。最初，AQI保持在中等范围，通常低于100，但随着值的上升至220，出现明显的峰值，表明空气质量极差。这表明某些时段的污染水平显著较高，可能对公共健康和环境质量产生严重影响。数据展示了一些改善，但峰值依然令人担忧，特别是在某些月份的过度污染。持续的监测和污染源的减排对于减少这些危险空气质量水平的频率和严重性至关重要。 '
- en: 'Table 21: One more example of general annotation generation in the Energy dataset.
    The selected dataset is shown in Fig. [8](https://arxiv.org/html/2410.17462v1#A10.F8
    "Figure 8 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '表格21：能源数据集中生成的一般注释的另一个示例。所选数据集如图[8](https://arxiv.org/html/2410.17462v1#A10.F8
    "Figure 8 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a)所示。紫色突出显示基本模式，绿色突出显示一般模式，黄色突出显示领域特定模式。'
- en: '| TESSA’s domain-specific annotations Gas prices have exhibited an upward trend,
    peaking at $1.077 before experiencing a slight decline towards the latter part
    of the series. This indicates an initial growth phase followed by stabilization.
    Minor, yet consistent seasonal variations suggest periodic influences that slightly
    affect the price. The analysis of residuals reveals small but steady fluctuations,
    indicating some level of unpredictability in the market. Support and resistance
    analysis identifies critical thresholds around $1.003 and $1.092, highlighting
    potential reversal points and consolidation zones. DirectLLM’s domain-specific
    annotations The time series data represents fluctuations in gas prices over a
    period of time, listed in ascending order. Initially, the gas prices start around
    $1.06 but gradually show a downward trend, reaching as low as approximately $0.99
    before slightly rising again. This decline suggests that there may have been favorable
    market conditions or increased supply during this period, which typically leads
    to lower prices for consumers. Toward the end of the observed period, prices stabilize
    around the $1.00 mark, indicating a potential plateau or gradual recovery in price
    levels. Overall, the data illustrates a dynamic market scenario, highlighting
    the volatility and changing nature of gas prices influenced by various factors
    in the energy domain. |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注释：天然气价格呈上升趋势，最高达到$1.077，之后在系列的后期略有下降。这表明初期有增长阶段，随后趋于稳定。轻微但持续的季节性波动表明存在周期性影响，这些影响略微影响了价格。残差分析显示出小幅但稳定的波动，表明市场中存在一定程度的不可预测性。支撑与阻力分析确定了$1.003和$1.092附近的关键阈值，突出显示了潜在的反转点和整理区。DirectLLM的领域特定注释：时间序列数据表示一段时间内天然气价格的波动，按升序排列。最初，天然气价格约为$1.06，但逐渐显示出下降趋势，最低降至约$0.99，随后略有回升。这一下降趋势表明，在此期间可能存在有利的市场条件或供应增加，这通常会导致消费者价格的降低。观察期的末尾，价格稳定在$1.00左右，表明价格水平可能达到了平台期或正在逐渐恢复。总体来看，这些数据描绘了一个动态的市场情景，突显了天然气价格的波动性以及受能源领域各种因素影响的变化性。
    |'
- en: 'Table 22: One more example of general annotation generation in the Energy dataset.
    The selected dataset is shown in Fig. [8](https://arxiv.org/html/2410.17462v1#A10.F8
    "Figure 8 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (b). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 表格22：能源数据集中一般注释生成的另一个示例。所选数据集见图[8](https://arxiv.org/html/2410.17462v1#A10.F8
    "图8 ‣ 附录J 案例研究的附加细节 ‣ 使用LLM解码时间序列：跨领域注释的多代理框架")（b）。紫色突出显示基本模式，绿色突出显示一般模式，黄色突出显示领域特定模式。
- en: '| TESSA’s domain-specific annotations Gas prices show a generally upward trend,
    starting around 1.11 and gradually increasing over time, indicating a long-term
    positive movement. Minor seasonal fluctuations are observed, but they do not dominate
    the overall trend. There are key support levels around 1.113 and resistance levels
    at 1.133, which may act as pivotal points for future price movements. Residuals
    suggest occasional minor deviations from the trend due to random factors, resulting
    in a generally stable series with slight intraday volatility. The moving average
    values reinforce this steady climb, suggesting continued stability with periodic
    minor disruptions in gas prices. DirectLLM’s domain-specific annotations The time
    series data represents fluctuations in gas prices over a specific period, with
    values consistently hovering around the $1.11 to $1.18 range. Notably, the data
    shows slight increases and decreases in price, suggesting moderate volatility
    within this timeframe. The highest recorded price peaks at approximately $1.18,
    reflecting potential market adjustments or external factors influencing gas prices.
    This stability could indicate a balanced supply and demand scenario, although
    one should remain aware that various events, such as geopolitical developments,
    can lead to sudden shifts. Overall, these insights provide a snapshot of gas price
    trends, useful for consumers and industry stakeholders in making informed decisions
    related to energy expenditures. |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注释：天然气价格显示出一般的上升趋势，起始于1.11左右，并随时间逐渐增加，表明长期的正向走势。观察到轻微的季节性波动，但它们并未主导整体趋势。存在关键的支撑位在1.113左右，以及阻力位在1.133，可能成为未来价格走势的转折点。残差表明，因随机因素而偶尔出现的轻微偏离趋势，导致总体趋势稳定，日内波动较小。移动平均值强化了这一稳步上升的趋势，表明天然气价格将继续稳定，但偶尔会有轻微的波动。DirectLLM的领域特定注释：该时间序列数据表示特定时期内天然气价格的波动，数值始终保持在$1.11到$1.18之间。值得注意的是，数据展示了价格的轻微上涨和下跌，表明在此时间框架内的适度波动。最高记录的价格峰值约为$1.18，反映了可能的市场调整或外部因素对天然气价格的影响。这种稳定性可能表明供需平衡，尽管应注意，各种事件，如地缘政治发展，可能导致价格突然变化。总体而言，这些见解提供了天然气价格趋势的快照，有助于消费者和行业相关方在做出能源支出相关决策时提供参考。'
- en: 'Table 23: One more example of general annotation generation in the Social Good
    dataset. The selected dataset is shown in Fig. [9](https://arxiv.org/html/2410.17462v1#A10.F9
    "Figure 9 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (a). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 表格23：社交公益数据集中的一个通用注释生成示例。所选数据集如图[9](https://arxiv.org/html/2410.17462v1#A10.F9
    "图9 ‣ 附录J 案例研究的更多细节 ‣ 使用LLM解码时间序列：跨领域注释的多智能体框架")（a）所示。紫色高亮显示基本模式，绿色高亮显示一般模式，黄色高亮显示领域特定模式。
- en: '| TESSA’s domain-specific annotations The unemployment rate data shows a gentle
    upward trend from approximately 5.4% to 6.57% over the observed period, indicating
    a consistent rise in unemployment levels. This gradual increase implies a weakening
    labor market, which could be influenced by broader socioeconomic conditions. Although
    there are oscillations suggesting cyclical patterns, these are not strong and
    exhibit some irregularities, pointing to potential short-term fluctuations or
    external disruptions. The stable relationship between consecutive data points
    suggests that the unemployment rate changes are relatively steady without abrupt
    shifts, allowing for some degree of predictability. Overall, the trend may signal
    policymakers to consider implementing measures to address the rising unemployment,
    especially if the current trajectory continues. DirectLLM’s domain-specific annotations
    The time series data represents the unemployment rate over a period of time. Initially,
    the unemployment rate showed a slight fluctuation around the 5% mark, indicating
    a relatively stable job market. However, a noticeable increase began, with rates
    rising to over 7%, suggesting a worsening economic situation and potential job
    losses. After peaking at 7.3%, the data indicates a gradual decrease, though the
    rates remained relatively high around 6.5% towards the end. This pattern highlights
    a period of economic challenge, with gradual steps towards recovery. |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的特定领域注释 失业率数据显示，在观察期内，失业率从大约5.4%温和上升到6.57%，表明失业水平持续上升。这一逐步增加意味着劳动市场正在走弱，可能受到更广泛的社会经济条件的影响。尽管存在周期性模式的波动，但这些波动不强，且表现出一定的不规则性，指出可能存在短期波动或外部干扰。连续数据点之间的稳定关系表明失业率变化相对稳定，没有急剧变化，允许一定程度的可预测性。总体来看，这一趋势可能会促使政策制定者考虑采取措施应对失业上升，特别是如果当前趋势持续的话。
    DirectLLM的特定领域注释 该时间序列数据代表了一段时间内的失业率。最初，失业率在5%左右出现轻微波动，表明就业市场相对稳定。然而，失业率开始显著上升，超过7%，表明经济状况恶化，可能伴随有职位流失。在达到7.3%的峰值后，数据显示失业率逐渐下降，但到期末时仍相对较高，约为6.5%。这一模式突显了一个经济挑战期，并逐步向恢复迈进。
    |'
- en: 'Table 24: One more example of general annotation generation in the Social Good
    dataset. The selected dataset is shown in Fig. [9](https://arxiv.org/html/2410.17462v1#A10.F9
    "Figure 9 ‣ Appendix J Additional Details of Case Studies ‣ Decoding Time Series
    with LLMs: A Multi-Agent Framework for Cross-Domain Annotation") (b). Purple highlights
    basic patterns, green highlights general patterns, and yellow highlights the domain-specific
    patterns.'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 表 24：社会公益数据集中另一个通用注释生成的示例。所选数据集如图[9](https://arxiv.org/html/2410.17462v1#A10.F9
    "图 9 ‣ 附录 J 案例研究的附加细节 ‣ 使用LLMs解码时间序列：跨领域注释的多智能体框架")（b）所示。紫色高亮显示基本模式，绿色高亮显示通用模式，黄色高亮显示特定领域模式。
- en: '| TESSA’s domain-specific annotations The unemployment rate data reflects a
    positive economic momentum, as evidenced by a persistent upward trend starting
    from around 4.1 and moving above 6, suggesting a gradual increase in unemployment
    over the series. This trend could indicate rising employment participation pressure
    or an effectiveness of policy breakthrough effects aimed at fostering job growth,
    which could be interpreted as a counter-effect if not analyzed in tandem with
    job creation data. Despite the upward trend, the series shows seasonality patterns,
    with periodic fluctuations above and below a baseline, suggesting regular influences
    on employment rates, possibly due to economic cycles or policy adjustments. These
    fluctuations may align with safety net thresholds, which temporarily reduce unemployment
    rates during specific intervals. In summary, the data represents a complex interplay
    of economic dynamics, signaling opportunities for policy interventions to maintain
    a balance between growth and employment stability. DirectLLM’s domain-specific
    annotations The time series data illustrates the fluctuation of the unemployment
    rate over a specific period. Initially, the unemployment rate started at a level
    of 4.5% and experienced a gradual decline to a low of about 3.6%. However, this
    was followed by an upward trend, eventually reaching highs of around 6.3%. Such
    fluctuations might have been influenced by changing economic conditions, labor
    market policies, or external events impacting employment. Notably, the trend indicates
    periods of economic strengthening followed by downturns, reflecting possible cycles
    of growth and contraction in the job market. |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| TESSA的领域特定注解 失业率数据显示了一个积极的经济势头，体现为从大约4.1开始并持续上升至6以上的趋势，表明失业逐渐增加。这一趋势可能表明就业参与压力的上升，或者是旨在促进就业增长的政策突破效应的有效性，如果没有与就业创造数据一同分析，可能会被解读为一种反向效应。尽管存在上升趋势，该系列数据显示出季节性模式，失业率在基准线以上和以下的周期性波动，表明可能存在定期对就业率的影响，可能是由于经济周期或政策调整。这些波动可能与社会保障网的临界点一致，临时降低失业率，通常发生在特定时间间隔内。总的来说，数据代表了经济动态的复杂相互作用，表明政策干预的机会，以保持增长和就业稳定之间的平衡。
    DirectLLM的领域特定注解 该时间序列数据展示了失业率在特定时期内的波动。最初，失业率为4.5%，并逐渐下降至约3.6%的低点。然而，随后出现了上升趋势，最终达到了大约6.3%的高点。这些波动可能受经济条件变化、劳动市场政策或外部事件对就业的影响。值得注意的是，该趋势显示出经济加强后紧随其后的衰退阶段，反映出就业市场中可能存在的增长和收缩周期。
    |'
