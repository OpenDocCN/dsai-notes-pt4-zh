- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:51:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:51:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MuLan：用于渐进式和互动多物体扩散的多模态LLM代理
- en: 来源：[https://arxiv.org/html/2402.12741/](https://arxiv.org/html/2402.12741/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.12741/](https://arxiv.org/html/2402.12741/)
- en: Sen Li
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Sen Li
- en: slien@connect.ust.hk &Ruochen Wang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: slien@connect.ust.hk &Ruochen Wang
- en: ruocwang@g.ucla.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ruocwang@g.ucla.edu
- en: Cho-Jui Hsieh
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Cho-Jui Hsieh
- en: chohsieh@cs.ucla.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: chohsieh@cs.ucla.edu
- en: '&Minhao Cheng'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&Minhao Cheng'
- en: mmc7149@psu.edu Tianyi Zhou
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: mmc7149@psu.edu Tianyi Zhou
- en: tianyi@umd.edu Department of Computer Science & Engineering, HKUST, Hong Kong,
    ChinaDepartment of Computer Science, UCLA, USADepartment of Computer Science,
    UCLA, USACollege of Information Science & Technology, PSU, USADeparment of Computer
    Science, UMD, USA
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: tianyi@umd.edu 计算机科学与工程系，香港科技大学，中国；计算机科学系，洛杉矶加利福尼亚大学，美国；计算机科学系，洛杉矶加利福尼亚大学，美国；信息科学与技术学院，宾州州立大学，美国；计算机科学系，马里兰大学，美国
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Existing text-to-image models still struggle to generate images of multiple
    objects, especially in handling their spatial positions, relative sizes, overlapping,
    and attribute bindings. To efficiently address these challenges, we develop a
    training-free Multimodal-LLM agent (MuLan), as a human painter, that can progressively
    generate multi-object with intricate planning and feedback control. MuLan harnesses
    a large language model (LLM) to decompose a prompt to a sequence of sub-tasks,
    each generating only one object by stable diffusion, conditioned on previously
    generated objects. Unlike existing LLM-grounded methods, MuLan only produces a
    high-level plan at the beginning while the exact size and location of each object
    are determined upon each sub-task by an LLM and attention guidance. Moreover,
    MuLan adopts a vision-language model (VLM) to provide feedback to the image generated
    in each sub-task and control the diffusion model to re-generate the image if it
    violates the original prompt. Hence, each model in every step of MuLan only needs
    to address an easy sub-task it is specialized for. The multi-step process also
    allows human users to monitor the generation process and make preferred changes
    at any intermediate step via text prompts, thereby improving the human-AI collaboration
    experience. We collect 200 prompts containing multi-objects with spatial relationships
    and attribute bindings from different benchmarks to evaluate MuLan. The results
    demonstrate the superiority of MuLan in generating multiple objects over baselines
    and its creativity when collaborating with human users. The code is available
    at [https://github.com/measure-infinity/mulan-code](https://github.com/measure-infinity/mulan-code).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的文本到图像模型仍然难以生成多个物体的图像，特别是在处理它们的空间位置、相对大小、重叠和属性绑定时。为了高效解决这些挑战，我们开发了一种无训练的多模态大语言模型（LLM）代理（MuLan），它像一个人类画家一样，可以通过精细的规划和反馈控制逐步生成多物体图像。MuLan利用一个大语言模型（LLM）将提示语分解为一系列子任务，每个子任务仅通过稳定扩散生成一个物体，并基于之前生成的物体进行条件生成。与现有的基于LLM的方法不同，MuLan在开始时仅生成一个高层次的计划，而每个物体的精确大小和位置则由LLM和注意力引导在每个子任务中确定。此外，MuLan采用了视觉语言模型（VLM）为每个子任务中生成的图像提供反馈，并控制扩散模型重新生成图像，如果图像违背了原始提示。因此，MuLan中的每个模型在每一步只需要处理它专门化的简单子任务。这个多步骤的过程还允许用户通过文本提示在任何中间步骤监控生成过程，并做出偏好的更改，从而提升人机协作的体验。我们从不同的基准测试中收集了200个包含多物体、具有空间关系和属性绑定的提示语，以评估MuLan。结果表明，MuLan在生成多个物体方面优于基准方法，并且在与人类用户协作时展现了其创造力。代码可以在[https://github.com/measure-infinity/mulan-code](https://github.com/measure-infinity/mulan-code)找到。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Diffusion models [[19](https://arxiv.org/html/2402.12741v2#bib.bib19), [9](https://arxiv.org/html/2402.12741v2#bib.bib9),
    [20](https://arxiv.org/html/2402.12741v2#bib.bib20)] have shown growing potential
    in generative AI tasks, especially in creating diverse and high-quality images
    with text prompts [[18](https://arxiv.org/html/2402.12741v2#bib.bib18), [17](https://arxiv.org/html/2402.12741v2#bib.bib17)].
    However, current state-of-the-art text-to-image (T2I) models such as Stable Diffusion [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]
    and DALL-E 3 [[2](https://arxiv.org/html/2402.12741v2#bib.bib2)] still struggle
    to deal with complicated prompts involving multiple objects and lack precise control
    of their spatial relations, potential occlusions, relative sizes, etc. As shown
    in Figure [2](https://arxiv.org/html/2402.12741v2#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    to generate a sketch of “The orange pumpkin is on the right side of the black
    door”, even the SOTA open-source T2I model, Stable Diffusion XL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)],
    still generates wrong attribute-binding as well as incorrect spatial positions
    of several objects.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型 [[19](https://arxiv.org/html/2402.12741v2#bib.bib19), [9](https://arxiv.org/html/2402.12741v2#bib.bib9),
    [20](https://arxiv.org/html/2402.12741v2#bib.bib20)] 在生成AI任务中展现了越来越大的潜力，尤其是在使用文本提示生成多样化和高质量图像方面 [[18](https://arxiv.org/html/2402.12741v2#bib.bib18),
    [17](https://arxiv.org/html/2402.12741v2#bib.bib17)]。然而，当前最先进的文本到图像（T2I）模型，如Stable
    Diffusion [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] 和 DALL-E 3 [[2](https://arxiv.org/html/2402.12741v2#bib.bib2)]，仍然难以处理涉及多个对象的复杂提示，并且缺乏对它们空间关系、潜在遮挡、相对大小等的精确控制。如图 [2](https://arxiv.org/html/2402.12741v2#S1.F2
    "图 2 ‣ 1 引言 ‣ MuLan：用于渐进式和交互式多对象扩散的多模态-LLM代理")所示，要生成“橙色南瓜在黑色门的右侧”的草图，即使是最先进的开源T2I模型Stable
    Diffusion XL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)]，也会生成错误的属性绑定以及多个对象的错误空间位置。
- en: Among works that aim to improve the controllability of T2I models on complicated
    prompts, a recent promising line of research seeks to utilize large language models
    (LLMs), e.g., ChatGPT, GPT-4 [[1](https://arxiv.org/html/2402.12741v2#bib.bib1)],
    to guide the generation process [[12](https://arxiv.org/html/2402.12741v2#bib.bib12),
    [6](https://arxiv.org/html/2402.12741v2#bib.bib6)]. Specifically, an LLM is prompted
    to generate a layout for the given prompt, i.e., a bounding box for each object
    in the image, given detailed instructions or demonstrations if necessary. However,
    due to the limited spatial reasoning capability of LLMs as well as their lack
    of alignment with the diffusion models, it is still challenging for LLMs to directly
    generate a complete and precise layout for multiple objects. Without a feedback
    loop interacting with the generative process, the layout’s possible mistakes cannot
    be effectively detected and corrected. Moreover, the layout is often applied as
    an extra condition in addition to the original prompt (e.g., bounding boxes combined
    with GLIGEN [[11](https://arxiv.org/html/2402.12741v2#bib.bib11)]), so the diffusion
    models may still generate an incorrect image due to its misunderstanding of the
    complicated prompt.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在旨在提升T2I模型在复杂提示下可控性的研究工作中，最近有一条有前景的研究方向，旨在利用大型语言模型（LLM），例如ChatGPT、GPT-4 [[1](https://arxiv.org/html/2402.12741v2#bib.bib1)]，来引导生成过程 [[12](https://arxiv.org/html/2402.12741v2#bib.bib12),
    [6](https://arxiv.org/html/2402.12741v2#bib.bib6)]。具体而言，LLM被提示生成给定提示的布局，即图像中每个对象的边界框，并根据需要提供详细的说明或示范。然而，由于LLM的空间推理能力有限，以及它们与扩散模型的对齐不足，LLM仍然难以直接生成多个对象的完整且精确的布局。如果没有与生成过程交互的反馈环路，布局可能出现的错误无法有效地检测和纠正。此外，布局通常作为原始提示的附加条件（例如，边界框与GLIGEN结合使用 [[11](https://arxiv.org/html/2402.12741v2#bib.bib11)]），因此扩散模型可能由于误解复杂提示，仍然生成错误的图像。
- en: '![Refer to caption](img/baa0d6f8a760c5cab1fb7ff272af8a4c.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/baa0d6f8a760c5cab1fb7ff272af8a4c.png)'
- en: 'Figure 1: The proposed training-free Multimodal-LLM Agent (MuLan) for Progressive
    Multi-Object Diffusion. MuLan consists of three main components: (1) LLM planning;
    (2) Single-object diffusion with attention guidance; and (3) VLM-feedback control.
    MuLan first decomposes a complicated prompt into a sequence of sub-prompts each
    for one object, and then generates one object per step conditioned on a sub-prompt
    and previously generated objects, where LLM plans the rough layout of the object
    and attention guidance provides an accurate mask for it. The VLM-feedback control
    allows MuLan to correct mistakes in each step by adjusting hyperparameters in
    (2).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：提出的无训练多模态LLM代理（MuLan）用于渐进式多物体扩散。MuLan由三个主要组件组成：（1）LLM规划；（2）带注意力引导的单物体扩散；（3）VLM反馈控制。MuLan首先将一个复杂的提示分解为一系列子提示，每个子提示生成一个物体，然后根据子提示和先前生成的物体按步骤生成每个物体，其中LLM规划物体的大致布局，注意力引导为其提供精确的掩码。VLM反馈控制使得MuLan能够通过调整（2）中的超参数，在每一步纠正错误。
- en: To address the limitations and challenges of previous methods, we develop a
    training-free and controllable T2I generation paradigm that does not require demonstrations
    but mainly focuses on improving the tool usage of existing models. Our paradigm
    is built upon a progressive multi-object generation by a Multimodal-LLM agent
    (MuLan), which generates only one object per stage, conditioned on generated objects
    in the image and attention masks of the most plausible positions to place the
    new object. Unlike previous methods that add conditions to each model and make
    the task even more challenging, MuLan uses an LLM as a planner decomposing the
    original T2I task into a sequence of easier subtasks. Each subtask generates one
    single object, which can be easily handled by diffusion models. To be noted, the
    LLM applied at the beginning of MuLan only focuses on high-level planning rather
    than a precise layout of bounding boxes, while the exact size and position of
    each object are determined later in each stage by LLM and attention guidance based
    on the generated objects in the image. Hence, we can avoid mistakes in the planning
    stage and find a better placement for each object adaptive to the generated content
    and adhering to the original prompt. In addition, MuLan builds a feedback loop
    monitoring the generation process, which assesses the generated image per stage
    using a vision-language model (VLM). When the generated image violates the prompt,
    the VLM will adjust the diffusion model to re-generate the image so any mistake
    can be corrected before moving to the next stage. Furthermore, we develop a strategy
    applied in each stage to handle the overlapping between objects, which is commonly
    ignored by previous work [[12](https://arxiv.org/html/2402.12741v2#bib.bib12)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决以往方法的局限性和挑战，我们开发了一种无训练且可控的T2I生成范式，该范式无需示范，主要聚焦于改善现有模型的工具使用。我们的范式基于一个由多模态LLM代理（MuLan）驱动的渐进式多物体生成，该代理在每个阶段仅生成一个物体，并根据图像中已生成物体和最可能放置新物体的位置的注意力掩码进行条件生成。与之前在每个模型中添加条件并使任务更加复杂的方法不同，MuLan使用LLM作为规划者，将原始T2I任务分解为一系列更易处理的子任务。每个子任务生成一个单独的物体，这可以被扩散模型轻松处理。值得注意的是，MuLan开头使用的LLM只关注高级规划，而不是精确的边界框布局，而每个物体的确切大小和位置则在每个阶段由LLM和基于图像中生成物体的注意力引导来确定。因此，我们可以避免规划阶段的错误，并为每个物体找到一个更好的放置位置，这个位置适应生成的内容并遵循原始提示。此外，MuLan建立了一个反馈循环来监控生成过程，使用视觉-语言模型（VLM）评估每个阶段生成的图像。当生成的图像违反了提示时，VLM会调整扩散模型以重新生成图像，从而在进入下一个阶段之前纠正任何错误。此外，我们在每个阶段开发了一种策略来处理物体之间的重叠，这是以往工作中常常被忽视的问题[[12](https://arxiv.org/html/2402.12741v2#bib.bib12)]。
- en: 'Therefore, MuLan obtains better controllability of the multi-object composition.
    An illustration of the progressive generation process is shown in Figure [1](https://arxiv.org/html/2402.12741v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion"). Note that there is a concurrent work called RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)]
    sharing a similar high-level idea (i.e., decomposing the prompt into sub-tasks)
    with MuLan. However, there still exist substantial differences between ours and
    RPG. MuLan generates each object conditioned on previously generated objects while
    RPG generates all objects independently. MuLan does not require any manually designed
    demonstrations for in-context learning. In addition, as shown in Section [4.1](https://arxiv.org/html/2402.12741v2#S4.SS1
    "4.1 Main Results and Analysis ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for
    Progressive and Interactive Multi-Object Diffusion"), MuLan can be directly applied
    to human-agent interaction during generation, which greatly boosts the flexibility
    and effectiveness of the generation. To evaluate MuLan, we curate a dataset of
    intricate and challenging prompts from different benchmarks. To compare MuLan
    with existing approaches, we prompt GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]
    several questions based on the input texts to comprehensively evaluate the alignment
    of the generated images with the prompts from three aspects. We further conduct
    human evaluations of the generated images. Extensive experimental results show
    that MuLan can achieve better controllability over the generation process and
    generate high-quality images aligning better with the prompts than the baselines.
    Example images generated by different methods are shown in Figure [2](https://arxiv.org/html/2402.12741v2#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion"). Our main contributions are summarized as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，MuLan在多物体组合的可控性方面表现更好。逐步生成过程的示意图见图[1](https://arxiv.org/html/2402.12741v2#S1.F1
    "图 1 ‣ 1 介绍 ‣ MuLan：用于渐进式和交互式多物体扩散的多模态大语言模型代理")。请注意，还有一项并行工作叫做RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)]，它与MuLan有类似的高层次思想（即将提示分解为子任务）。然而，MuLan与RPG之间仍然存在显著的差异。MuLan在生成每个物体时，都是基于先前生成的物体进行条件生成，而RPG则是独立地生成所有物体。MuLan不需要任何人工设计的示范进行上下文学习。此外，如第[4.1](https://arxiv.org/html/2402.12741v2#S4.SS1
    "4.1 主要结果与分析 ‣ 4 实验 ‣ MuLan：用于渐进式和交互式多物体扩散的多模态大语言模型代理")节所示，MuLan可以直接应用于生成过程中的人机交互，这大大提高了生成的灵活性和有效性。为了评估MuLan，我们从不同的基准中精心挑选了复杂且具有挑战性的提示数据集。为了与现有方法进行比较，我们通过向GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]
    提出一些基于输入文本的问题，全面评估生成图像与提示在三个方面的对齐情况。我们还进行了生成图像的人工评估。大量实验结果表明，MuLan能够更好地控制生成过程，并生成与提示对齐更好的高质量图像，优于基线方法。不同方法生成的示例图像见图[2](https://arxiv.org/html/2402.12741v2#S1.F2
    "图 2 ‣ 1 介绍 ‣ MuLan：用于渐进式和交互式多物体扩散的多模态大语言模型代理")。我们的主要贡献总结如下：
- en: '![Refer to caption](img/6e554b578fe3b76f42f0f6f3d63067b4.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/6e554b578fe3b76f42f0f6f3d63067b4.png)'
- en: 'Figure 2: Examples of MuLan-generated images, compared to the original SD-v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)],
    the original SDXL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)], Structure
    diffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)], Promptist [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)],
    and PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：MuLan生成图像的示例，与原始的SD-v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]、原始的SDXL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)]、结构扩散 [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)]、Promptist [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)]
    和PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)]进行比较。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel training-free paradigm for text-to-image generation and a
    Multimodal-LLM agent. It achieves better control in generating images for complicated
    prompts consisting of multiple objects with specified spatial relationships and
    attribute bindings.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的无训练范式用于文本到图像的生成，并且提出了一种多模态大语言模型（Multimodal-LLM）代理。该方法在生成包含多个物体、具有指定空间关系和属性绑定的复杂提示时，能够实现更好的控制。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an effective strategy to handle multi-object occlusion in T2I generation,
    which improves the image quality and makes them more realistic.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种有效的策略来处理T2I生成中的多物体遮挡问题，旨在提高图像质量，使其更加逼真。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We curate a dataset of prompts to evaluate multi-object composition with spatial
    relationships and attribute bindings in T2I tasks. The quantitative results and
    human evaluation results show that our method can achieve better results compared
    to different controllable generation methods and general T2I generation methods.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们策划了一个数据集，用于评估T2I任务中多对象组合的空间关系和属性绑定。定量结果和人工评估结果表明，我们的方法相比不同的可控生成方法和通用T2I生成方法能够获得更好的效果。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We show that the proposed framework can be applied to human-agent interaction
    during generation. This enables users to effectively monitor and change/adjust
    the generation process during generation instead of waiting until all the generation
    is finished.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了所提出的框架可以应用于生成过程中的人机互动。这使得用户能够在生成过程中有效监控并改变/调整生成过程，而不需要等到所有生成完成之后。
- en: 2 Related Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Diffusion models
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散模型
- en: 'As a new family of generative models, diffusion models have attracting more
    and more attention due to its powerful creative capability. Text-to-image generation,
    which aims to generate the high-quality image aligning with given text prompts,
    is one of the most popular applications [[14](https://arxiv.org/html/2402.12741v2#bib.bib14),
    [18](https://arxiv.org/html/2402.12741v2#bib.bib18), [17](https://arxiv.org/html/2402.12741v2#bib.bib17),
    [2](https://arxiv.org/html/2402.12741v2#bib.bib2)]. Among different powerful diffusion
    models, the latent diffusion model [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]
    has shown amazing capability and has been widely used in practice due to the efficiency
    and superior performance, which is also the backbone of the current SOTA stable
    diffusion models. Different from the typical diffusion models which directly perform
    the diffusion and denoising process in the pixel space, the latent diffusion model
    perform the whole process in the encoded latent space [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)],
    which can greatly reduce the training and inference time. Recently, empowered
    by a significantly expanded model capacity, Stable Diffusion XL has demonstrated
    performance levels approaching commercial application standards [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)].
    Detailed background on the procedure of diffusion models is provided in Appendix [C](https://arxiv.org/html/2402.12741v2#A3
    "Appendix C Background on (Latent) Diffusion Models ‣ MuLan: Multimodal-LLM Agent
    for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '作为一类新的生成模型，扩散模型由于其强大的创作能力，越来越受到关注。文本到图像生成（Text-to-Image Generation），旨在根据给定的文本提示生成高质量的图像，是最受欢迎的应用之一[[14](https://arxiv.org/html/2402.12741v2#bib.bib14),
    [18](https://arxiv.org/html/2402.12741v2#bib.bib18), [17](https://arxiv.org/html/2402.12741v2#bib.bib17),
    [2](https://arxiv.org/html/2402.12741v2#bib.bib2)]。在各种强大的扩散模型中，潜在扩散模型[[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]表现出了惊人的能力，并且由于其高效性和优越的性能，已经在实践中得到了广泛应用，它也是当前SOTA稳定扩散模型的核心。不同于直接在像素空间进行扩散和去噪过程的典型扩散模型，潜在扩散模型在编码的潜在空间[[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]中完成整个过程，这可以大大减少训练和推理时间。最近，借助显著扩展的模型能力，Stable
    Diffusion XL展示了接近商业应用标准的性能[[16](https://arxiv.org/html/2402.12741v2#bib.bib16)]。有关扩散模型过程的详细背景，请参见附录[C](https://arxiv.org/html/2402.12741v2#A3
    "Appendix C Background on (Latent) Diffusion Models ‣ MuLan: Multimodal-LLM Agent
    for Progressive and Interactive Multi-Object Diffusion")。'
- en: Composed generation in diffusion models
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩散模型中的组合生成
- en: 'Although Stable Diffusion model has shown unprecedented performance on the
    T2I generation task, it still struggles with text prompts with multi-object, especially
    when there are several spatial relationships and attribute bindings in the prompts.
    To achieve more controllable and accurate image compositions, many compositional
    generation methods have been proposed. StructureDiffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)]
    proposed a training-free method to parse the input prompt and combine it with
    the cross-attention to achieve better control over attribute bindings and compositional
    generation. On the other hand, Promptist [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)]
    aimed to train a language model with the objective of optimizing input prompts,
    rendering them more comprehensible and facilitative for diffusion models. Several
    works utilize the large language model to directly generate the whole layout for
    the input prompt with in-context learning, and then generate the image conditioned
    on the layout [[12](https://arxiv.org/html/2402.12741v2#bib.bib12), [6](https://arxiv.org/html/2402.12741v2#bib.bib6)].
    While all the previous take the whole input prompt, we propose to turn the original
    complicated task into several easier sub-tasks. A training-free multimodal-LLM
    agent is utilized to progressively generate objects with feedback control so that
    the whole generation process would be better controlled. Very recently, a concurrent
    work RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)] also proposed to
    utilize LLM agent to decompose the prompt into different subtasks. However, MuLan
    generates each object step by step and correct mistakes after each step rather
    than treating all subtasks independently and does not need a well-designed in-context
    learning demonstrations. We defer a more thorough discussion with RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)]
    in Appendix [B](https://arxiv.org/html/2402.12741v2#A2 "Appendix B Differences
    between MuLan and the concurrent work RPG ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Stable Diffusion 模型在 T2I 生成任务中展现了前所未有的表现，但它在处理多对象的文本提示时仍然存在困难，特别是当提示中涉及多个空间关系和属性绑定时。为了实现更可控且精确的图像构图，许多组合生成方法应运而生。StructureDiffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)]
    提出了一个无训练方法，用于解析输入的提示，并将其与跨注意力机制结合，从而更好地控制属性绑定和组合生成。另一方面，Promptist [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)]
    旨在训练一个语言模型，优化输入提示，使其更易理解并促进扩散模型的应用。一些工作利用大型语言模型，通过上下文学习直接生成输入提示的完整布局，然后基于该布局生成图像 [[12](https://arxiv.org/html/2402.12741v2#bib.bib12),
    [6](https://arxiv.org/html/2402.12741v2#bib.bib6)]。尽管之前的工作都处理了整个输入提示，我们提出将原本复杂的任务分解为若干较易处理的子任务。我们采用一个无训练的多模态-LLM
    代理，逐步生成对象并进行反馈控制，以便更好地控制整个生成过程。最近，一项并行工作 RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)]
    也提出利用 LLM 代理将提示分解为不同的子任务。然而，MuLan 是逐步生成每个对象，并在每一步之后纠正错误，而不是将所有子任务独立处理，也不需要精心设计的上下文学习演示。我们将在附录 [B](https://arxiv.org/html/2402.12741v2#A2
    "附录 B MuLan 与并行工作 RPG 之间的差异 ‣ MuLan：用于渐进式和交互式多对象扩散的多模态 LLM 代理") 中对 RPG [[22](https://arxiv.org/html/2402.12741v2#bib.bib22)]
    进行更为详细的讨论。
- en: 3 Multimodal-LLM Agent (MuLan)
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 多模态-LLM 代理（MuLan）
- en: Existing diffusion models often struggle with complicated prompts but can handle
    simpler ones. Recent approaches train a model or apply in-context learning given
    similar examples to produce a detailed layout for the prompt in advance and the
    diffusion model can generate each part of the layout with a simpler prompt separately.
    Rather than generating all objects at once or in parallel, MuLan is inspired by
    many human painters, who start by making a high-level plan, painting objects one
    after another as planned, and correcting mistakes after each step if needed. Thereby,
    the constraints between objects can be naturally taken into account.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的扩散模型通常在处理复杂的提示时表现不佳，但能够处理简单的提示。最近的方法通过训练模型或应用上下文学习，给出类似的示例，以提前生成提示的详细布局，然后扩散模型可以基于简单的提示单独生成布局中的每个部分。与其一次性生成所有对象或并行生成，MuLan
    从许多人类画家的灵感中汲取，画家通常先做出一个高层次的计划，按计划逐一绘制对象，并在每一步之后如有需要进行错误修正。因此，能够自然地考虑对象之间的约束关系。
- en: 3.1 Overview
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: 'MuLan begins by strategically planning and decomposing an intricate input prompt
    into a manageable sequence of sub-prompts, each focusing on an easier sub-task
    generating one single object. MuLan then adopts a progressive strategy that generates
    one object in each stage conditioned on previously generated objects using a diffusion
    model. Simultaneously, a VLM offers insightful feedback and adaptively adjusts
    the generation process to guarantee precision in accomplishing each subtask. Compared
    to previous methods, MuLan is entirely training-free and does not require any
    in-context examples. As illustrated in Fig. [1](https://arxiv.org/html/2402.12741v2#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion"), MuLan is composed of three components:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 'MuLan首先通过战略性地规划和分解复杂的输入提示，将其转化为一系列可管理的子提示，每个子提示专注于生成一个单独的对象。然后，MuLan采用一种渐进式策略，在每个阶段基于之前生成的对象使用扩散模型生成一个对象。同时，VLM提供有见地的反馈，并自适应地调整生成过程，以确保完成每个子任务的精确性。与之前的方法相比，MuLan完全不需要训练，并且不需要任何上下文示例。如图[1](https://arxiv.org/html/2402.12741v2#S1.F1
    "图1 ‣ 1 引言 ‣ MuLan: 用于渐进式和互动多对象扩散的多模态LLM代理")所示，MuLan由三个组件组成：'
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prompt decomposition by LLM planning, which produces a sequence of sub-prompts,
    each focusing on generating one object in the prompt.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过LLM规划进行提示分解，生成一系列子提示，每个子提示专注于生成提示中的一个对象。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conditional single-object diffusion with LLM planning and attention guidance,
    which generates a new object conditioned on the previous step’s image using a
    stable diffusion model. While a sub-prompt from LLM planning provides text guidance,
    the object’s size and position are controlled by an attention mask, which guides
    the object to be correctly positioned and generated.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于LLM规划和注意力引导的条件单对象扩散，利用稳定的扩散模型根据上一步的图像生成新对象。尽管LLM规划中的一个子提示提供了文本引导，对象的大小和位置通过注意力掩码来控制，确保对象正确定位并生成。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Feedback control by interacting with VLM, which inspects the image generated
    per stage and adjusts hyperparameters and attention guidance to re-generate the
    image if it violates the original prompt.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过与VLM互动进行反馈控制，VLM检查每个阶段生成的图像，并在图像违反原始提示时调整超参数和注意力引导，以重新生成图像。
- en: 3.2 Prompt Decomposition by LLM Planning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 通过LLM规划进行提示分解
- en: 'Given a complex prompt p, MuLan first uses an LLM to automatically decompose
    p into $N$ object-wise sub-prompts $\texttt{p}_{1:N}$. During decompostion, MuLan
    specifically asks the LLM to produce a sequence of objects that will be created
    in the default order from left to right and bottom to top in the image. The LLM
    can easily finish this task by leveraging its prior knowledge to fill all objects
    of p to an empty list of the pre-defined order without in-context learning which
    requires manually designed examples. Let $\texttt{objs}=\{\texttt{obj}_{1},\cdots,\texttt{obj}_{n},\cdots,\texttt{obj}_{%
    N}\}$ be the LLM-planned $N$ objects extracted from p. For the first object, the
    sub-prompt is simply $\texttt{p}_{1}=$“$\{\texttt{obj}_{1}\}$”. For object-$n$
    with $n>1$, the subtask is to generate object-$n$ conditioned on previous objects
    and the textual sub-prompt is defined as $\texttt{p}_{n}=$“$\{\texttt{obj}_{n}\}\,\,\texttt{and}\,\,\{\texttt{obj}_{n-1}\}$”.
    MuLan conducts the above global planning by an LLM at the very beginning before
    generating any image. The detailed prompts and template for LLM planning can be
    found in Appendix [E](https://arxiv.org/html/2402.12741v2#A5 "Appendix E Detailed
    prompt template of the global planning by the LLM ‣ MuLan: Multimodal-LLM Agent
    for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个复杂的提示p，MuLan首先使用LLM自动将p分解为$N$个面向对象的子提示$\texttt{p}_{1:N}$。在分解过程中，MuLan特别要求LLM生成一系列对象，这些对象将按默认顺序从左到右、从下到上在图像中创建。LLM可以轻松完成此任务，利用其先验知识将p中的所有对象填充到预定义顺序的空列表中，而无需依赖上下文学习，也无需手动设计示例。设$\texttt{objs}=\{\texttt{obj}_{1},\cdots,\texttt{obj}_{n},\cdots,\texttt{obj}_{%
    N}\}$为从p中提取的LLM规划的$N$个对象。对于第一个对象，子提示是简单的$\texttt{p}_{1}=$“$\{\texttt{obj}_{1}\}$”。对于对象-$n$（其中$n>1$），子任务是基于前面生成的对象生成对象-$n$，文本子提示定义为$\texttt{p}_{n}=$“$\{\texttt{obj}_{n}\}\,\,\texttt{and}\,\,\{\texttt{obj}_{n-1}\}$”。MuLan在生成任何图像之前，首先通过LLM进行上述全局规划。LLM规划的详细提示和模板可以在附录[E](https://arxiv.org/html/2402.12741v2#A5
    "附录E LLM全局规划的详细提示模板 ‣ MuLan: 用于渐进式和互动多对象扩散的多模态LLM代理")中找到。'
- en: 'When generating each object in Section [3.3](https://arxiv.org/html/2402.12741v2#S3.SS3
    "3.3 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion"), we will use the LLM again as a local
    planner of the object’s position and size, i.e., by generating a mask in the image
    and coordinating its overlap with previous objects. Then a diffusion model is
    used to generate the object under the attention guidance of the mask. These will
    be further elaborated in Section [3.3](https://arxiv.org/html/2402.12741v2#S3.SS3
    "3.3 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '在生成第[3.3](https://arxiv.org/html/2402.12741v2#S3.SS3 "3.3 Conditional Single-Object
    Diffusion with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan)
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")节中的每个物体时，我们将再次使用LLM作为物体位置和大小的局部规划器，即通过在图像中生成一个掩膜，并协调其与前一个物体的重叠。然后，使用扩散模型在掩膜的注意力引导下生成该物体。具体内容将在第[3.3](https://arxiv.org/html/2402.12741v2#S3.SS3
    "3.3 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
    ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion")节中进一步阐述。'
- en: 3.3 Conditional Single-Object Diffusion with LLM Planning and Attention Guidance
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 基于LLM规划和注意力引导的条件单一物体扩散
- en: 'At stage-$n$, the diffusion model only focuses on generating $\texttt{obj}_{n}$
    according to the sub-prompt $\texttt{p}_{n}$, ensuring that $\texttt{obj}_{n}$
    can be correctly positioned and generated. To this end, MuLan utilizes the LLM
    to plan the relative position and size of $\texttt{obj}_{n}$, allocating a rough
    mask (i.e., a bounding box) $\bm{M}_{n}$ for $\texttt{obj}_{n}$. Then, cross-attention
    guidance is applied during the generation of $\texttt{obj}_{n}$ to ensure $\texttt{obj}_{n}$
    is appropriately positioned within $\bm{M}_{n}$. The pipeline is given in Figure [3](https://arxiv.org/html/2402.12741v2#S3.F3
    "Figure 3 ‣ 3.3 Conditional Single-Object Diffusion with LLM Planning and Attention
    Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion") with the complete procedure listed in
    Algorithm [1](https://arxiv.org/html/2402.12741v2#alg1 "Algorithm 1 ‣ Appendix
    D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion") in Appendix [D](https://arxiv.org/html/2402.12741v2#A4
    "Appendix D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion"). We will introduce
    it step by step in the following.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在第$n$阶段，扩散模型只关注根据子提示$\texttt{p}_{n}$生成$\texttt{obj}_{n}$，确保$\texttt{obj}_{n}$能够正确定位和生成。为此，MuLan利用LLM规划$\texttt{obj}_{n}$的相对位置和大小，并为$\texttt{obj}_{n}$分配一个粗略的掩膜（即边界框）$\bm{M}_{n}$。然后，在生成$\texttt{obj}_{n}$的过程中，应用跨注意力引导，确保$\texttt{obj}_{n}$正确地定位在$\bm{M}_{n}$内。该流程在图[3](https://arxiv.org/html/2402.12741v2#S3.F3
    "Figure 3 ‣ 3.3 Conditional Single-Object Diffusion with LLM Planning and Attention
    Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion")中给出，完整的过程在附录[D](https://arxiv.org/html/2402.12741v2#A4
    "Appendix D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")的算法[1](https://arxiv.org/html/2402.12741v2#alg1
    "Algorithm 1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")中列出。我们将在以下部分逐步介绍。'
- en: '![Refer to caption](img/47781204c2f194b2b33e44203f8ca02e.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/47781204c2f194b2b33e44203f8ca02e.png)'
- en: 'Figure 3: Single object diffusion with LLM planning and attention guidance
    for $\texttt{obj}_{n}$ (detailed procedure in Algorithm [1](https://arxiv.org/html/2402.12741v2#alg1
    "Algorithm 1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")
    in Appendix [D](https://arxiv.org/html/2402.12741v2#A4 "Appendix D Algorithm procedure
    of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：使用LLM规划和注意力引导进行的单一物体扩散（详细过程请参见附录[D](https://arxiv.org/html/2402.12741v2#A4
    "Appendix D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")中的算法[1](https://arxiv.org/html/2402.12741v2#alg1
    "Algorithm 1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")）。'
- en: LLM Planning of a Rough Mask for $\texttt{obj}_{n}$.
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM对$\texttt{obj}_{n}$的粗略掩膜规划。
- en: 'At stage-$n$, MuLan first allocates a rough mask as a bounding box ${\bm{M}}_{n}\triangleq(x_{n},y_{n},w_{n},h_{n})$
    (x/y coordinates of the top-left corner, width, and height) to guide the generation
    of $\texttt{obj}_{n}$ in the image. As shown in Figure [3](https://arxiv.org/html/2402.12741v2#S3.F3
    "Figure 3 ‣ 3.3 Conditional Single-Object Diffusion with LLM Planning and Attention
    Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion"), ${\bm{M}}_{n}$ can be derived from $\texttt{obj}_{n}$’s
    relative position $\texttt{opt}_{n}\in\texttt{Opts=\{left,right,top,bottom\}}$,
    the total number of objects $\texttt{Num}_{n}$ in the same position/region as
    $\texttt{obj}_{n}$, and current available space in the image. $\texttt{Num}_{n}$
    and current available space, combined together, determines the size of $\texttt{obj}_{n}$.
    MuLan utilizes the LLM planner to reason $\texttt{opt}_{n}$ and $\texttt{Num}_{n}$
    given the sub-prompt $\texttt{p}_{n}$¹¹1The detailed prompt template can be found
    in Appendix [F](https://arxiv.org/html/2402.12741v2#A6 "Appendix F Detailed prompt
    template of the local planning by the LLM ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion")., while the current available space can
    be determined by the precise mask $\bm{\tilde{M}}_{n-1}$ which describes the exact
    position of previously generated $\texttt{obj}_{n-1}$ and can be easily extracted
    from the cross-attention maps. It is worth noting that since there is no previously
    generated objects for the first object, the available space for $\texttt{obj}_{1}$
    is the whole image. For detailed computation of $\bm{M}_{n}$, please refer to
    Appendix [G](https://arxiv.org/html/2402.12741v2#A7 "Appendix G Details for the
    computation of rough masks ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在阶段-$n$，MuLan首先分配一个粗略掩码作为边界框${\bm{M}}_{n}\triangleq(x_{n},y_{n},w_{n},h_{n})$（左上角的x/y坐标、宽度和高度）来引导生成图像中的$\texttt{obj}_{n}$。如图[3](https://arxiv.org/html/2402.12741v2#S3.F3
    "图 3 ‣ 3.3 条件单物体扩散与LLM规划和注意力引导 ‣ 3 多模态-LLM代理（MuLan） ‣ MuLan: 逐步和交互式多物体扩散的多模态-LLM代理")所示，${\bm{M}}_{n}$可以通过$\texttt{obj}_{n}$的相对位置$\texttt{opt}_{n}\in\texttt{Opts=\{left,right,top,bottom\}}$、与$\texttt{obj}_{n}$处于相同位置/区域的物体总数$\texttt{Num}_{n}$以及图像中当前可用空间来推导。$\texttt{Num}_{n}$和当前可用空间，综合起来决定了$\texttt{obj}_{n}$的大小。MuLan利用LLM规划器推理$\texttt{opt}_{n}$和$\texttt{Num}_{n}$，给定子提示$\texttt{p}_{n}$¹¹1详细提示模板可以参见附录[F](https://arxiv.org/html/2402.12741v2#A6
    "附录 F LLM本地规划的详细提示模板 ‣ MuLan: 逐步和交互式多物体扩散的多模态-LLM代理")，而当前可用空间可以通过精确掩码$\bm{\tilde{M}}_{n-1}$来确定，该掩码描述了之前生成的$\texttt{obj}_{n-1}$的确切位置，并且可以轻松地从交叉注意力图中提取。值得注意的是，由于对于第一个物体没有之前生成的物体，$\texttt{obj}_{1}$的可用空间就是整个图像。有关$\bm{M}_{n}$的详细计算，请参见附录[G](https://arxiv.org/html/2402.12741v2#A7
    "附录 G 粗略掩码计算的详细信息 ‣ MuLan: 逐步和交互式多物体扩散的多模态-LLM代理")。'
- en: Once $\bm{M}_{n}$ is determined, the cross-attention guidance is utilized during
    generation of $\texttt{obj}_{n}$ to ensure $\texttt{obj}_{n}$ is correctly generated
    within $\bm{M}_{n}$, as elaborated in the following.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了$\bm{M}_{n}$，在生成$\texttt{obj}_{n}$的过程中会利用交叉注意力引导，以确保$\texttt{obj}_{n}$能够正确地生成在$\bm{M}_{n}$内，具体内容将在下文中详细阐述。
- en: Single-Object Generation with Attention Guidance.
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用注意力引导的单一物体生成。
- en: Given the rough mask $\bm{M}_{n}$ of $\texttt{obj}_{n}$, the next is to ensure
    the generated $\texttt{obj}_{n}$ will be correctly located within $\bm{M}_{n}$.
    A natural and intuitive way to achieve this in diffusion models is to guide the
    generation of the cross-attention map of $\texttt{obj}_{n}$, which builds the
    relevance between the text prompt and the location of generated object.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$\texttt{obj}_{n}$的粗略掩码$\bm{M}_{n}$，接下来需要确保生成的$\texttt{obj}_{n}$能够正确地定位在$\bm{M}_{n}$内。扩散模型中实现这一目标的自然且直观的方法是引导生成$\texttt{obj}_{n}$的交叉注意力图，从而建立文本提示与生成物体位置之间的关联。
- en: 'To this end, MuLan manipulates the cross-attention map of $\texttt{obj}_{n}$
    under the guidance of $\bm{M}_{n}$, using the backward guidance method [[4](https://arxiv.org/html/2402.12741v2#bib.bib4)],
    to maximize the relevance inside $\bm{M}_{n}$. Specifically, let $\bm{A}$ be the
    cross-attention map, $\bm{A}_{m,k}$ represents the relevance between the spatial
    location $m$ and token-$k$ that describes $\texttt{obj}_{n}$ in the prompt. Larger
    value in $\bm{A}_{m,k}$ indicates that $\texttt{obj}_{n}$ is more likely located
    at the spatial location of $m$. The goal is to maximize the relevance $\bm{A}_{m,k}$
    inside the mask $\bm{M}_{n}$ while minimizing the relevance outside the mask $\bm{M}_{n}$.
    Hence the following energy function is utilized:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，MuLan 在 $\bm{M}_{n}$ 的指导下，操控 $\texttt{obj}_{n}$ 的交叉注意力图，通过逆向引导方法[[4](https://arxiv.org/html/2402.12741v2#bib.bib4)]，最大化
    $\bm{M}_{n}$ 内部的相关性。具体来说，令 $\bm{A}$ 为交叉注意力图，$\bm{A}_{m,k}$ 表示空间位置 $m$ 与描述 $\texttt{obj}_{n}$
    的 token-$k$ 之间的相关性。$\bm{A}_{m,k}$ 的值越大，表示 $\texttt{obj}_{n}$ 更可能位于空间位置 $m$。目标是最大化
    $\bm{M}_{n}$ 内部的相关性 $\bm{A}_{m,k}$，同时最小化 $\bm{M}_{n}$ 外部的相关性。因此，采用以下能量函数：
- en: '|  | $\displaystyle\scriptstyle E(\bm{A},\bm{M}_{n},k)=\left(1-\frac{\sum_{m\in\bm{M%
    }_{n}}\bm{A}_{m,k}}{\sum_{m}\bm{A}_{m,k}}\right)^{2},$ |  | (1) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\scriptstyle E(\bm{A},\bm{M}_{n},k)=\left(1-\frac{\sum_{m\in\bm{M}_{n}}\bm{A}_{m,k}}{\sum_{m}\bm{A}_{m,k}}\right)^{2},$
    |  | (1) |'
- en: where $\sum_{m\in\bm{M}_{n}}$ denotes the summation over the spatial locations
    included in $\bm{M}_{n}$, and $\sum_{m}$ denotes the summation over all the spatial
    locations in the attention map. In every step-$t$ of the earlier generation process,
    MuLan applies gradient descent to minimize the energy by updating the input latent
    $\bm{z}_{n,t}$ for object $\texttt{obj}_{n}$. In this way, the cross-attention
    map corresponding to $\texttt{obj}_{n}$ will achieve the largest relevance inside
    $\bm{M}_{n}$, meaning $\texttt{obj}_{n}$ can be correctly positioned inside the
    rough mask.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sum_{m\in\bm{M}_{n}}$ 表示对 $\bm{M}_{n}$ 中包含的空间位置求和，而 $\sum_{m}$ 表示对注意力图中所有空间位置求和。在早期生成过程的每一步
    $t$ 中，MuLan 通过更新对象 $\texttt{obj}_{n}$ 的输入潜在变量 $\bm{z}_{n,t}$ 来应用梯度下降，最小化能量。这样，
    $\texttt{obj}_{n}$ 对应的交叉注意力图将在 $\bm{M}_{n}$ 内部达到最大的相关性，这意味着 $\texttt{obj}_{n}$
    可以正确地定位在粗略的掩模内。
- en: On the other hand, to take the previous objects and their constraints into account
    when generating $\texttt{obj}_{n}$, we further combine the latent of $\texttt{obj}_{n}$
    and $\texttt{obj}_{n-1}$. Specifically, after step-$t$ of reverse process ($t$
    varies from $T$ to $0$), we update the latent $\bm{z}_{n,(t-1)}$ by
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，为了在生成 $\texttt{obj}_{n}$ 时考虑前一对象及其约束，我们进一步结合了 $\texttt{obj}_{n}$ 和 $\texttt{obj}_{n-1}$
    的潜在变量。具体来说，在反向过程的第 $t$ 步（$t$ 从 $T$ 变化到 $0$）后，我们通过以下方式更新潜在变量 $\bm{z}_{n,(t-1)}$：
- en: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{%
    M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$ |  | (2) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{z}_{n,(t-1)}=\bm{M}^{\prime}_{n}\odot\bm{z}_{n,(t-1)}+(1-\bm{M}^{\prime}_{n})\odot\bm{z}_{(n-1),(t-1)},$
    |  | (2) |'
- en: where $\odot$ computes element-wise product and $[\bm{M}^{\prime}_{n}]_{uv}=\mathbbm{1}_{u\in[x_{n},x_{n}+w_{n}],v\in[y_{n},y_{%
    n}+h_{n}]}$ is the 0-1 indicator of whether coordinates $(u,v)$ is included in
    the bounding box of $\bm{M}_{n}$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\odot$ 计算逐元素积，而 $[\bm{M}^{\prime}_{n}]_{uv}=\mathbbm{1}_{u\in[x_{n},x_{n}+w_{n}],v\in[y_{n},y_{n}+h_{n}]}$
    是一个 0-1 指示符，表示坐标 $(u,v)$ 是否位于 $\bm{M}_{n}$ 的边界框内。
- en: 'MuLan applies the above single-object diffusion to each object one after another
    from $\texttt{obj}_{1}$ to $\texttt{obj}_{N}$, as planned by the LLM at the very
    beginning. The procedure of generating $\texttt{obj}_{n}$ is detailed in Algorithm [1](https://arxiv.org/html/2402.12741v2#alg1
    "Algorithm 1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 'MuLan 将上述单对象扩散依次应用于每个对象，从 $\texttt{obj}_{1}$ 到 $\texttt{obj}_{N}$，这一过程由 LLM
    在一开始的规划中确定。生成 $\texttt{obj}_{n}$ 的过程在算法[1](https://arxiv.org/html/2402.12741v2#alg1
    "算法 1 ‣ 附录 D 中 MuLan 的单对象扩散算法过程 ‣ MuLan: 用于渐进式和交互式多对象扩散的多模态 LLM 代理") 中详细描述。'
- en: Objects Overlapping.
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对象重叠。
- en: Overlapping between objects is a key challenge in text-to-image diffusion models.
    However, it lacks attention in previous methods [[12](https://arxiv.org/html/2402.12741v2#bib.bib12),
    [6](https://arxiv.org/html/2402.12741v2#bib.bib6)]. Instead, we propose an effective
    strategy that can be merged into the procedure above. Specifically, at the generation
    of object $\texttt{obj}_{n}$, we prompt the LLM to judge if there is overlapping
    between $\texttt{obj}_{n}$ and $\texttt{obj}_{n-1}$. If there is overlapping,
    we first compute three candidates for the rough mask $\{\bm{M}_{n,i}\}_{i=1}^{3}$,
    associated with three overlapping ratios $\{r_{i}\}_{i=1}^{3}=\{10\%,30\%,50\%\}$
    between $\texttt{obj}_{n-1}$ and $\texttt{obj}_{n}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对象之间的重叠是文本到图像扩散模型中的一个关键挑战。然而，先前的方法并未关注这一问题[[12](https://arxiv.org/html/2402.12741v2#bib.bib12),
    [6](https://arxiv.org/html/2402.12741v2#bib.bib6)]。相反，我们提出了一种有效的策略，可以合并到上述过程当中。具体而言，在生成对象$\texttt{obj}_{n}$时，我们提示
    LLM 判断是否存在 $\texttt{obj}_{n}$ 和 $\texttt{obj}_{n-1}$ 之间的重叠。如果存在重叠，我们首先计算三个候选的粗略掩码$\{\bm{M}_{n,i}\}_{i=1}^{3}$，它们与
    $\texttt{obj}_{n-1}$ 和 $\texttt{obj}_{n}$ 之间的三个重叠比率$\{r_{i}\}_{i=1}^{3}=\{10\%,30\%,50\%\}$相关。
- en: 'Given the three masks $\bm{M}_{n,i}$, MuLan generates three candidate images
    using Algorithm [1](https://arxiv.org/html/2402.12741v2#alg1 "Algorithm 1 ‣ Appendix
    D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion"). Then the CLIP
    scores [[8](https://arxiv.org/html/2402.12741v2#bib.bib8)] between the generated
    images and the input prompt $\texttt{p}_{n}$ are computed and the image with the
    maximal CLIP score is selected as the generated image for $\texttt{obj}_{n}$.
    An illustration is given in Figure [8](https://arxiv.org/html/2402.12741v2#A8.F8
    "Figure 8 ‣ Appendix H More details on the overlapping processing ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion") with more details
    of candidate masks in Appendix [H](https://arxiv.org/html/2402.12741v2#A8 "Appendix
    H More details on the overlapping processing ‣ MuLan: Multimodal-LLM Agent for
    Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '给定这三个掩码 $\bm{M}_{n,i}$，MuLan 使用算法[1](https://arxiv.org/html/2402.12741v2#alg1
    "Algorithm 1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")生成三个候选图像。然后，计算生成的图像与输入提示
    $\texttt{p}_{n}$ 之间的 CLIP 分数[[8](https://arxiv.org/html/2402.12741v2#bib.bib8)]，并选择具有最大
    CLIP 分数的图像作为生成的 $\texttt{obj}_{n}$ 图像。图8中给出了一个示意图，附录[H](https://arxiv.org/html/2402.12741v2#A8
    "Appendix H More details on the overlapping processing ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")中提供了更多候选掩码的详细信息。'
- en: 3.4 Interaction with VLM and Human Users during Generation
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 在生成过程中与 VLM 和用户的交互
- en: To correct the possible mistakes made in the sequential generation process,
    MuLan builds an adaptive feedback-loop control by interacting with a vision-language
    model (VLM). After each generation stage, MuLan queries the VLM to inspect the
    generated object(s) and its consistency with the input prompt. If they do not
    align well, MuLan will adjust the backward guidance of the current stage to re-generate
    the object. Such a close-loop control involves LLM, diffusion, and VLM and significantly
    automates the T2I generation for complicated prompts, leading to a more accurate
    generation in practice.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正顺序生成过程中可能出现的错误，MuLan 通过与视觉-语言模型（VLM）交互，建立了一个自适应的反馈回路控制。在每个生成阶段后，MuLan 查询
    VLM 检查生成的对象及其与输入提示的一致性。如果它们不一致，MuLan 将调整当前阶段的反向指导，以重新生成该对象。这样的闭环控制涉及到 LLM、扩散和
    VLM，并显著地自动化了针对复杂提示的 T2I 生成，从而在实践中实现了更准确的生成。
- en: '![Refer to caption](img/58afe3b1e45af0bc7e4580567b58293b.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/58afe3b1e45af0bc7e4580567b58293b.png)'
- en: 'Figure 4: An illustration tree for difference cases of human-agent interaction
    during generation. The middle branch (connected by blue arrows) shows the original
    generation process without human-agent interaction. The top and bottom branches
    show different complex composed human-agent interaction during generation for
    various adjustments, involving object adjustments, attribute adjustments, and
    spatial relationship adjustments, which demonstrate the flexibility and effectiveness
    of MuLan for human-agent interaction during generation.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：展示了生成过程中不同的人类-智能体交互情况的示意图。中间的分支（由蓝色箭头连接）展示了没有人类-智能体交互的原始生成过程。顶部和底部的分支展示了在生成过程中为了各种调整而进行的人类-智能体复杂交互，包括物体调整、属性调整和空间关系调整，展示了MuLan在人类-智能体交互方面的灵活性和有效性。
- en: 'In addition, the multi-step process naturally allows human-agent interaction/collaboration
    during generation in practice. Users can timely monitor the generation process.
    In this way, the interaction enables users to make preferred changes and adjustments
    to the generated images easily and effectively by providing adjusting prompts
    to MuLan at any intermediate step, such as attribute adjustment, object adjustment,
    and spatial relationship adjustment. With the adjusting prompts, MuLan will utilize
    the LLM to modify the original prompt accordingly and change the generation process
    to the preferred one. An illustration for different changes or adjustments during
    generation is shown in Figure [4](https://arxiv.org/html/2402.12741v2#S3.F4 "Figure
    4 ‣ 3.4 Interaction with VLM and Human Users during Generation ‣ 3 Multimodal-LLM
    Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object
    Diffusion"), which indicates MuLan can achieve both simple and composed complex
    adjustments with interaction. In contrast, for other existing generation and editing
    methods, users have to wait until the whole generation process is finished. Therefore,
    the proposed framework is more user-friendly and flexible in terms of human-agent
    interaction and collaboration.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，这一多步骤过程自然允许在生成过程中进行人类-智能体交互/协作。用户可以实时监控生成过程。通过这种方式，交互使得用户能够通过向MuLan提供调整提示，在任何中间步骤轻松有效地对生成的图像进行偏好的更改和调整，例如属性调整、物体调整和空间关系调整。通过这些调整提示，MuLan将利用LLM相应地修改原始提示，并将生成过程改变为偏好的生成方式。图[4](https://arxiv.org/html/2402.12741v2#S3.F4
    "图 4 ‣ 3.4 人类-智能体交互与VLM和用户协作 ‣ 3 多模态-LLM智能体 (MuLan) ‣ MuLan：用于渐进式和交互式多物体扩散的多模态-LLM智能体")展示了生成过程中不同变化或调整的示意图，表明MuLan能够在交互中实现简单和复杂的调整。相比之下，对于其他现有的生成和编辑方法，用户必须等待整个生成过程完成。因此，所提出的框架在人类-智能体交互和协作方面更加用户友好和灵活。
- en: 4 Experiments
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Dataset
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: To evaluate our framework, we construct a prompt dataset from different benchmarks.
    Specifically, since our focus is to achieve better generation for complex prompts
    containing multi-objects with both spatial relationships and attribute bindings,
    we first collect all complex spatial prompts from T2I-CompBench [[10](https://arxiv.org/html/2402.12741v2#bib.bib10)].
    To make the experiments more comprehensive, we let ChatGPT generate about 400
    prompts with different objects, spatial relationships, and attribute bindings
    so that the prompt sets consists of about 600 prompts. To further evaluate the
    capability of our framework on extremely complex and hard prompts, we manually
    add prompts that SDXL fails to generate, leading to a hard prompt dataset containing
    200 prompts. Similar to the complex spatial prompts in T2I-CompBench [[10](https://arxiv.org/html/2402.12741v2#bib.bib10)],
    each prompt in our curated dataset typically contains two objects with various
    spatial relationships, with each object containing attribute bindings randomly
    selected from {color,shape,texture}.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的框架，我们从不同的基准构建了一个提示数据集。具体来说，由于我们的重点是实现对包含多个物体、具有空间关系和属性绑定的复杂提示的更好生成，我们首先从T2I-CompBench收集所有复杂的空间提示[[10](https://arxiv.org/html/2402.12741v2#bib.bib10)]。为了使实验更加全面，我们让ChatGPT生成大约400个包含不同物体、空间关系和属性绑定的提示，使得提示集包含大约600个提示。为了进一步评估我们框架在处理极其复杂和困难的提示上的能力，我们手动添加了SDXL无法生成的提示，形成一个包含200个提示的困难提示数据集。与T2I-CompBench中的复杂空间提示[[10](https://arxiv.org/html/2402.12741v2#bib.bib10)]类似，我们整理的数据集中每个提示通常包含两个物体，并具有各种空间关系，每个物体包含从{颜色、形状、纹理}中随机选择的属性绑定。
- en: Models & Baseline
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型与基准
- en: As a training-free framework, MuLan can be incorporated into any existing diffusion
    models. We evaluate two stable diffusion models with our framework, Stable Diffusion
    v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] and the SOTA Stable
    Diffusion XL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)]. To verify
    the superiority of MuLan, we compare it with previous controllable generation
    methods and general T2I generation methods. Specifically, we evaluate Structure
    Diffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)], Promptist [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)],
    the original Stable Diffusion v1.4, the original SDXL, and the recent SOTA diffusion
    model PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)].
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个无训练框架，MuLan 可以与任何现有的扩散模型结合使用。我们在我们的框架下评估了两个稳定扩散模型，Stable Diffusion v1.4
    [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] 和 SOTA Stable Diffusion
    XL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)]。为了验证 MuLan 的优越性，我们将其与之前的可控生成方法和通用
    T2I 生成方法进行了比较。具体来说，我们评估了 Structure Diffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)]、Promptist
    [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)]、原始的 Stable Diffusion v1.4、原始的
    SDXL，以及最近的 SOTA 扩散模型 PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)]。
- en: Implementation Details
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实现细节
- en: MuLan use GPT-4 [[1](https://arxiv.org/html/2402.12741v2#bib.bib1)] as the LLM
    planner, and LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)] as
    the VLM checker to provide the feedback. We also conducted an ablation study to
    show the importance of the feedback control provided by the VLM and the effect
    of different VLMs. Moreover, we found the attention blocks utilized during the
    attention guidance are vital, which can be classified as near-input blocks, near-middle
    blocks, and near-output blocks. We utilize the near-middle blocks in our main
    experiments and also show the ablation results of different block. All the experiments
    are conducted on a single NVIDIA RTX A6000 GPU.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: MuLan 使用 GPT-4 [[1](https://arxiv.org/html/2402.12741v2#bib.bib1)] 作为 LLM 规划器，并使用
    LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)] 作为 VLM 检查器提供反馈。我们还进行了消融研究，以展示
    VLM 提供的反馈控制的重要性以及不同 VLM 的效果。此外，我们发现，在注意力引导过程中使用的注意力模块至关重要，这些模块可以分为接近输入的模块、接近中间的模块和接近输出的模块。我们在主要实验中使用了接近中间的模块，并展示了不同模块的消融结果。所有实验均在单个
    NVIDIA RTX A6000 GPU 上进行。
- en: Evaluation
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'Since the prompt dataset contains texts with complex compositions, we design
    a questionnaire to comprehensively investigate the alignment between the generated
    image and the corresponding input text. The questionnaire is composed of three
    aspects - object completeness, correctness of attribute bindings, and correctness
    of spatial relationships. We only set two options for each question (Yes or No),
    without any ambiguity. For detailed questions and examples of the evaluation,
    please refer to Appendix [I](https://arxiv.org/html/2402.12741v2#A9 "Appendix
    I More details on the evaluation questionnaire ‣ MuLan: Multimodal-LLM Agent for
    Progressive and Interactive Multi-Object Diffusion"). For each aspect of the evaluation,
    we compute the percentage of answers with “Yes”. Given the generated image, we
    assess the image’s quality using a questionnaire asking both the state-of-the-art
    multi-modal large language model (GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)])
    and the human evaluator.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '由于提示数据集包含了复杂构成的文本，我们设计了一份问卷，全面调查生成图像与相应输入文本之间的对齐情况。问卷包括三个方面——对象完整性、属性绑定的正确性和空间关系的正确性。每个问题仅设置了两个选项（是或否），没有任何歧义。有关评估的详细问题和示例，请参见附录
    [I](https://arxiv.org/html/2402.12741v2#A9 "附录 I 更多关于评估问卷的细节 ‣ MuLan: 用于渐进式和互动式多对象扩散的多模态
    LLM 代理")。对于每个评估方面，我们计算回答“是”的百分比。给定生成的图像后，我们通过问卷评估图像的质量，问卷由最新的多模态大型语言模型（GPT-4V
    [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]）和人工评估者共同完成。'
- en: 4.1 Main Results and Analysis
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果与分析
- en: Results on GPT Evaluation
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPT 评估结果
- en: 'Given the generated image, we prompt GPT-4V to answer the questions about the
    image in the questionnaire, where each only focuses on one of the three aspects.
    The results for different methods and different base models are shown in Table [1](https://arxiv.org/html/2402.12741v2#S4.T1
    "Table 1 ‣ Results on GPT Evaluation ‣ 4.1 Main Results and Analysis ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion").
    The results show that our framework can achieve the best performance compared
    to different controllable generation methods and T2I generation methods. In particular,
    in the two ‘harder’ aspects - attribute bindings and spatial relationships, MuLan
    can surpass other methods by a large margin. More qualitative results can be found
    in Figure [5](https://arxiv.org/html/2402.12741v2#S4.F5 "Figure 5 ‣ Results on
    GPT Evaluation ‣ 4.1 Main Results and Analysis ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion") and Appendix [J](https://arxiv.org/html/2402.12741v2#A10
    "Appendix J More qualitative results ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '给定生成的图像，我们通过问卷提示GPT-4V回答关于图像的问题，其中每个问题仅关注三大方面之一。不同方法和不同基础模型的结果如表[1](https://arxiv.org/html/2402.12741v2#S4.T1
    "表 1 ‣ GPT评估结果 ‣ 4.1 主要结果与分析 ‣ 4 实验 ‣ MuLan: 多模态LLM代理用于渐进式和交互式多物体扩散")所示。结果表明，我们的框架在与不同可控生成方法和T2I生成方法的比较中，能够实现最佳表现。特别是在两个“更难”的方面——属性绑定和空间关系，MuLan能够大幅度超越其他方法。更多定性结果可以在图[5](https://arxiv.org/html/2402.12741v2#S4.F5
    "图 5 ‣ GPT评估结果 ‣ 4.1 主要结果与分析 ‣ 4 实验 ‣ MuLan: 多模态LLM代理用于渐进式和交互式多物体扩散")和附录[J](https://arxiv.org/html/2402.12741v2#A10
    "附录 J 更多定性结果 ‣ MuLan: 多模态LLM代理用于渐进式和交互式多物体扩散")中找到。'
- en: 'Table 1: GPT-4V evaluation/human evaluation of images generated by different
    methods for complicated prompts.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：GPT-4V评估/人类评估，通过不同方法生成复杂提示下的图像。
- en: '| Method | Object completeness | Attribute bindings | Spatial relationships
    | Overall |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 物体完整性 | 属性绑定 | 空间关系 | 总体 |'
- en: '| Structure Diffusion [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)] |
    88.97%/87.37% | 54.62%/62.63% | 34.36%/24.24% | 64.31%/64.85% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 结构扩散 [[5](https://arxiv.org/html/2402.12741v2#bib.bib5)] | 88.97%/87.37%
    | 54.62%/62.63% | 34.36%/24.24% | 64.31%/64.85% |'
- en: '| Promptist-SD v1.4 [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)] | 80.36%/70.71%
    | 49.23%/52.02% | 24.49%/13.13% | 56.73%/51.72% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SD v1.4 [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)] | 80.36%/70.71%
    | 49.23%/52.02% | 24.49%/13.13% | 56.73%/51.72% |'
- en: '| Promptist-SDXL [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)] | 94.36%/93.94%
    | 70.00%/78.28% | 35.89%/33.33% | 72.92%/75.56% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Promptist-SDXL [[7](https://arxiv.org/html/2402.12741v2#bib.bib7)] | 94.36%/93.94%
    | 70.00%/78.28% | 35.89%/33.33% | 72.92%/75.56% |'
- en: '| SD v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] | 90.31%/74.49%
    | 57.14%/51.02% | 37.24%/32.65% | 66.43%/56.73% |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| SD v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] | 90.31%/74.49%
    | 57.14%/51.02% | 37.24%/32.65% | 66.43%/56.73% |'
- en: '| SDXL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)] | 94.64%/78.57%
    | 66.07%/53.06% | 41.14%/24.49% | 72.34%/57.55% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SDXL [[16](https://arxiv.org/html/2402.12741v2#bib.bib16)] | 94.64%/78.57%
    | 66.07%/53.06% | 41.14%/24.49% | 72.34%/57.55% |'
- en: '| PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)] | 92.09%/76.53%
    | 66.58%/61.22% | 34.69%/32.65% | 70.41%/61.63% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| PixArt-$\alpha$ [[3](https://arxiv.org/html/2402.12741v2#bib.bib3)] | 92.09%/76.53%
    | 66.58%/61.22% | 34.69%/32.65% | 70.41%/61.63% |'
- en: '| MuLan-SD v1.4 (Ours) | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SD v1.4（我们的） | 93.11%/86.36% | 74.23%/74.24% | 51.53%/54.54% | 77.24%/75.15%
    |'
- en: '| MuLan-SDXL (Ours) | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SDXL（我们的） | 96.17%/90.40% | 75.00%/79.29% | 39.29%/49.49% | 76.33%/77.78%
    |'
- en: '![Refer to caption](img/0bc06684c6b13f7a818319de4e958fca.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0bc06684c6b13f7a818319de4e958fca.png)'
- en: 'Figure 5: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同方法在复杂提示下生成的图像的更多定性示例。
- en: Results on Human Evaluation
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人类评估结果
- en: 'To further accurately evaluate the generated images about the alignments with
    human preferences, we further conduct a human evaluation by randomly sampling
    100 prompts from the prompt dataset. Similarly, we ask human evaluators to finish
    the questionnaire used in GPT evaluation. The results are shown in Table [1](https://arxiv.org/html/2402.12741v2#S4.T1
    "Table 1 ‣ Results on GPT Evaluation ‣ 4.1 Main Results and Analysis ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    which indicates that our method can still achieve the best performance and is
    consistent with the GPT-4V evaluation results.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步准确评估生成图像与人类偏好的一致性，我们还通过随机抽取100个提示从提示数据集中进行人工评估。同样，我们要求人工评估人员完成GPT评估中使用的问卷。结果如表[1](https://arxiv.org/html/2402.12741v2#S4.T1
    "Table 1 ‣ Results on GPT Evaluation ‣ 4.1 Main Results and Analysis ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")所示，结果表明我们的方法仍然可以达到最佳表现，并且与GPT-4V评估结果一致。'
- en: Results on Human-Agent Interaction
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人机交互结果
- en: 'To show MuLan is still very effective if users want to modify the input prompt
    or edit the generated images during the generation, i.e., the human-agent interaction,
    we use ChatGPT to mimic the user to generate various adjusting prompts for the
    interaction with MuLan on randomly sampled 50 prompts. SD v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]
    is utilized as the base model. The generated adjusting prompts focus on several
    aspect, i.e., attribute adjustment, object adjustment, and spatial relationship
    adjustment. We use GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]
    to quantitatively evaluate the performance of MuLan given the final generated
    images and final text prompts, as shown in Table [2](https://arxiv.org/html/2402.12741v2#S4.T2
    "Table 2 ‣ Results on Human-Agent Interaction ‣ 4.1 Main Results and Analysis
    ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion"). The results indicate that MuLan can still achieve high
    accuracy even with various adjustments/changes during generation.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示如果用户希望在生成过程中修改输入提示或编辑生成的图像时，MuLan仍然非常有效，即在人机交互的场景下，我们使用ChatGPT模拟用户，针对随机抽取的50个提示生成各种调整提示与MuLan进行交互。SD
    v1.4 [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)] 被用作基础模型。生成的调整提示聚焦于多个方面，即属性调整、物体调整和空间关系调整。我们使用GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]
    定量评估MuLan在给定最终生成图像和最终文本提示下的表现，如表[2](https://arxiv.org/html/2402.12741v2#S4.T2
    "Table 2 ‣ Results on Human-Agent Interaction ‣ 4.1 Main Results and Analysis
    ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive
    Multi-Object Diffusion")所示。结果表明，MuLan即便在生成过程中进行各种调整/变化时，仍能保持较高的准确性。'
- en: 'Table 2: GPT-4V evaluation of final generated images and final prompts after
    adjustments/changes. The results show that MuLan is still very effective with
    various adjustment of prompts during generation.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：GPT-4V对调整/变化后的最终生成图像和最终提示的评估。结果表明，MuLan在生成过程中进行各种提示调整时，仍然非常有效。
- en: '|  | Objects | Attributes | Spatial | Overall |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | 物体 | 属性 | 空间 | 总体 |'
- en: '| MuLan-SD v1.4 | 95.92% | 72.45% | 28.57% | 73.06% |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MuLan-SD v1.4 | 95.92% | 72.45% | 28.57% | 73.06% |'
- en: 4.2 Ablation study
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融研究
- en: In this section, we show ablation results on the effect of the attention blocks
    during diffusion generation and the importance of the VLM feedback control in
    the proposed framework. 50 prompts are randomly sampled from the prompt dataset
    for all experiments in the ablation study.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了在扩散生成过程中，注意力模块的作用及在所提出框架中VLM反馈控制的重要性的消融实验结果。所有消融研究中的实验都随机从提示数据集中抽取了50个提示。
- en: Ablation on the attention blocks
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力模块消融
- en: 'As we mentioned at the beginning of Section [4](https://arxiv.org/html/2402.12741v2#S4
    "4 Experiments ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object
    Diffusion"), there are three options for the attention blocks used for backward
    guidance, i.e., near-input blocks, near-middle blocks, and near-output blocks.
    We empirically found the near-middle blocks can achieve the best control and performance
    for the generation, which generally contains the richest semantics. Hence here
    we show the ablation results on different choices of the attention blocks. We
    utilize SD-v1.4 as the base model, and evaluate the performance of different attention
    blocks under our framework by GPT-4V. The results are shown in Table [3](https://arxiv.org/html/2402.12741v2#S4.T3
    "Table 3 ‣ Ablation on the attention blocks ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    which indicates the diffusion generation with near-middle blocks can achieve much
    better results compared to the other two options.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[4](https://arxiv.org/html/2402.12741v2#S4 "4 实验 ‣ MuLan：用于渐进和互动的多对象扩散的多模态大语言模型代理")节开始时提到的，注意力模块用于反向引导有三种选择，即近输入模块、近中间模块和近输出模块。我们通过经验发现，近中间模块在生成的控制和性能上表现最佳，通常包含最丰富的语义。因此，在这里我们展示了不同注意力模块选择的消融结果。我们使用
    SD-v1.4 作为基础模型，并通过 GPT-4V 评估了在我们框架下不同注意力模块的性能。结果显示在表[3](https://arxiv.org/html/2402.12741v2#S4.T3
    "表 3 ‣ 注意力模块的消融 ‣ 4.2 消融研究 ‣ 4 实验 ‣ MuLan：用于渐进和互动的多对象扩散的多模态大语言模型代理")中，表明采用近中间模块的扩散生成相比其他两种选择可以取得更好的结果。
- en: 'Table 3: Ablation study on attention blocks with SD-v1.4 as the base model.
    “Objects”, “Attributes”, and “Spatial” denote Object completeness, Attribute bindings,
    and Spatial relationships. The results (evaluated by GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)])
    show that near-middle attention blocks perform the best for attention guidance.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基于 SD-v1.4 作为基础模型的注意力模块消融研究。“对象”、“属性”和“空间”分别表示对象完整性、属性绑定和空间关系。通过 GPT-4V
    评估的结果[[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]表明，近中间注意力模块在注意力引导方面表现最佳。
- en: '| Guidance | Objects | Attributes | Spatial | Overall |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 引导 | 对象 | 属性 | 空间 | 综合 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| near-input | 83.67% | 55.10% | 14.29% | 58.37% |'
- en: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| near-middle | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| near-output | 72.45% | 45.92% | 22.45% | 51.84% |'
- en: Ablation on the VLM feedback control
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VLM 反馈控制的消融研究
- en: 'The VLM feedback control is a key componenet in MuLan to provide feedback and
    adjust the generation process to ensure the every stage’s correct generation.
    Here, we show the importance of the feedback by removing feedback control from
    the whole framework. As shown in Table [4](https://arxiv.org/html/2402.12741v2#S4.T4
    "Table 4 ‣ Ablation on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    after removing the VLM, the results would be much worse. It is because there is
    no guarantee or adaptive adjustment for each generation stage, which verifies
    that the feedback control provided by the VLM is essential to handle complex prompts.
    Moreover, we also test MuLan’s compatibility with different VLMs. As shown in
    Table [5](https://arxiv.org/html/2402.12741v2#S4.T5 "Table 5 ‣ Ablation on the
    VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion"), we compare the
    Mulan’s performance using different VLMs including LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)],
    GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)], and Gemini-Pro [[21](https://arxiv.org/html/2402.12741v2#bib.bib21)].
    The results show that MuLan could still maintain a good performance with different
    choices of the VLM and achieve good compatibility.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'VLM 反馈控制是 MuLan 中的一个关键组件，用于提供反馈并调整生成过程，以确保每个阶段的正确生成。这里，我们通过去除反馈控制来展示反馈的重要性。如表[4](https://arxiv.org/html/2402.12741v2#S4.T4
    "Table 4 ‣ Ablation on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")所示，去除
    VLM 后，结果会大幅下降。这是因为缺乏每个生成阶段的保障或自适应调整，验证了 VLM 提供的反馈控制对于处理复杂提示是至关重要的。此外，我们还测试了 MuLan
    与不同 VLM 的兼容性。如表[5](https://arxiv.org/html/2402.12741v2#S4.T5 "Table 5 ‣ Ablation
    on the VLM feedback control ‣ 4.2 Ablation study ‣ 4 Experiments ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")所示，我们比较了 MuLan 使用不同
    VLM（包括 LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)]，GPT-4V
    [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]，Gemini-Pro [[21](https://arxiv.org/html/2402.12741v2#bib.bib21)]）的表现。结果表明，MuLan
    在不同的 VLM 选择下仍能保持良好的性能，并实现良好的兼容性。'
- en: 'Table 4: Ablation study comparing MuLan with vs. without VLM feedback control,
    using SD-v1.4 as the diffusion model and GPT-4 as the judge in evaluations. It
    indicates that feedback control can significantly improve the performance.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：消融研究，比较使用与不使用 VLM 反馈控制的 MuLan，使用 SD-v1.4 作为扩散模型，GPT-4 作为评估中的判定标准。结果表明，反馈控制可以显著提高性能。
- en: '| MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| MuLan | 对象 | 属性 | 空间 | 总体 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| w/ Feedback | 97.96% | 80.61% | 30.61% | 77.55% |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 有反馈 | 97.96% | 80.61% | 30.61% | 77.55% |'
- en: '| w/o Feedback | 81.63% | 59.18% | 18.37% | 60.00% |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 无反馈 | 81.63% | 59.18% | 18.37% | 60.00% |'
- en: 'Table 5: Ablation study of the VLM used in MuLan, using SD-v1.4 as the diffusion
    model and GPT-4 as the judge in evaluations. The results show that the choice
    of the VLM would not affect the overall performance too much.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：MuLan 中使用的 VLM 的消融研究，使用 SD-v1.4 作为扩散模型，GPT-4 作为评估中的判定标准。结果表明，VLM 的选择对整体性能的影响并不大。
- en: '| VLM in MuLan | Objects | Attributes | Spatial | Overall |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| MuLan 中的 VLM | 对象 | 属性 | 空间 | 总体 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)] | 97.96%
    | 80.61% | 30.61% | 77.55% |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLaVA-1.5 [[13](https://arxiv.org/html/2402.12741v2#bib.bib13)] | 97.96%
    | 80.61% | 30.61% | 77.55% |'
- en: '| GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)] | 95.92% | 80.61%
    | 28.57% | 76.33% |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)] | 95.92% | 80.61%
    | 28.57% | 76.33% |'
- en: '| Gemini-Pro [[21](https://arxiv.org/html/2402.12741v2#bib.bib21)] | 95.92%
    | 83.67% | 38.78% | 79.59% |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-Pro [[21](https://arxiv.org/html/2402.12741v2#bib.bib21)] | 95.92%
    | 83.67% | 38.78% | 79.59% |'
- en: 5 Conclusions and Limitations
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与局限性
- en: In this paper, we propose a training-free multimodal-LLM agent (MuLan) to progressively
    generate objects contained in the complicated input prompt with closed-loop feedback
    control, achieving better and more precise control on the whole generation process.
    By first decomposing the complicated prompt into easier sub-tasks, our method
    takes turns to deal with each object, conditioned on the previous one. The VLM
    checker further provides a guarantee with feedback control and adaptive adjustment
    for correct generation at each stage. Moreover, the application to the human-agent
    interaction further enhances the significance of MuLan, making the generation
    more flexible and effective to align with the preferences of users. Extensive
    experiments demonstrate the superiority of MuLan over previous methods, showing
    the potential of MuLan as a new paradigm of controllable diffusion generation.
    However, there are still limitations to be further addressed in the future work.
    Since the whole generation contains multiple stages, depending on the number of
    objects, it will take a longer time than a one-stage generation approach. On the
    other hand, the LLM planner may mistakenly parse the input prompt which results
    in incorrect decomposition. This could be addressed by first re-writing the input
    prompt by the LLM to facilitate later processing.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种无需训练的多模态LLM代理（MuLan），通过闭环反馈控制逐步生成复杂输入提示中包含的对象，从而实现对整个生成过程的更好、更精确的控制。通过先将复杂的提示分解为更容易处理的子任务，我们的方法依次处理每个对象，并以之前的对象为条件进行生成。VLM检查器进一步通过反馈控制和自适应调整，为每个阶段的正确生成提供了保证。此外，将其应用于人机交互进一步增强了MuLan的意义，使生成更加灵活高效，能够与用户的偏好对齐。大量实验表明，MuLan在多方面优于之前的方法，展示了其作为一种可控扩散生成新范式的潜力。然而，仍然存在一些限制，需在未来的工作中进一步解决。由于整个生成过程包含多个阶段，具体耗时取决于对象的数量，因此可能比单阶段生成方法更耗时。另一方面，LLM规划器可能错误解析输入提示，从而导致错误的分解。这可以通过让LLM先重写输入提示来解决，以便后续处理更加顺利。
- en: References
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等人。Gpt-4技术报告。arXiv预印本
    arXiv:2303.08774, 2023。'
- en: '[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation
    with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3\.
    pdf, 2(3), 2023.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li,
    Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo 等人。通过更好的标题改进图像生成。计算机科学。https://cdn.openai.com/papers/dall-e-3.pdf,
    2(3), 2023。'
- en: '[3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-$\alpha$: Fast training
    of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint
    arXiv:2310.00426, 2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao
    Wang, James Kwok, Ping Luo, Huchuan Lu 等人。Pixart-$\alpha$: 用于逼真文本到图像合成的扩散变换器的快速训练。arXiv预印本
    arXiv:2310.00426, 2023。'
- en: '[4] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control
    with cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision, pages 5343–5353, 2024.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Minghao Chen, Iro Laina 和 Andrea Vedaldi。无需训练的布局控制与跨注意力引导。发表于IEEE/CVF计算机视觉应用冬季会议论文集，页面5343-5353,
    2024。'
- en: '[5] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna
    Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured
    diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032,
    2022.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna
    Narayana, Sugato Basu, Xin Eric Wang 和 William Yang Wang。无需训练的结构化扩散引导用于组合文本到图像合成。arXiv预印本
    arXiv:2212.05032, 2022。'
- en: '[6] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai
    He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional
    visual planning and generation with large language models. arXiv preprint arXiv:2305.15393,
    2023.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai
    He, Sugato Basu, Xin Eric Wang 和 William Yang Wang。Layoutgpt：使用大语言模型进行组合视觉规划与生成。arXiv预印本
    arXiv:2305.15393, 2023。'
- en: '[7] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing prompts for text-to-image
    generation. arXiv preprint arXiv:2212.09611, 2022.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yaru Hao, Zewen Chi, Li Dong 和 Furu Wei。优化文本到图像生成的提示词。arXiv预印本 arXiv:2212.09611,
    2022。'
- en: '[8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
    Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint
    arXiv:2104.08718, 2021.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, 和 Yejin Choi.
    Clipscore: 一种无参考图像描述评估指标. arXiv预印本 arXiv:2104.08718, 2021.'
- en: '[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models. Advances in neural information processing systems, 33:6840–6851, 2020.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jonathan Ho, Ajay Jain, 和 Pieter Abbeel. 去噪扩散概率模型. 神经信息处理系统进展, 33:6840–6851,
    2020.'
- en: '[10] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench:
    A comprehensive benchmark for open-world compositional text-to-image generation.
    arXiv preprint arXiv:2307.06350, 2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 黄凯毅, 孙凯跃, 谢恩泽, 李正国, 和 刘锡辉. T2i-compbench: 一个全面的开放世界组合文本到图像生成基准. arXiv预印本
    arXiv:2307.06350, 2023.'
- en: '[11] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng
    Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 22511–22521, 2023.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 李宇恒, 刘浩天, 吴清扬, 穆方舟, 杨建伟, 高建峰, 李春远, 和 李永宰. Gligen: 开放集基础的文本到图像生成. 在IEEE/CVF计算机视觉与模式识别会议论文集,
    页码 22511–22521, 2023.'
- en: '[12] Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded diffusion:
    Enhancing prompt understanding of text-to-image diffusion models with large language
    models. arXiv preprint arXiv:2305.13655, 2023.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 连龙, 李博一, Adam Yala, 和 Trevor Darrell. Llm-grounded扩散: 通过大型语言模型增强文本到图像扩散模型的提示理解.
    arXiv预印本 arXiv:2305.13655, 2023.'
- en: '[13] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 刘浩天, 李春远, 李宇恒, 和 李永宰. 通过视觉指令调优改进基准模型. arXiv预印本 arXiv:2310.03744, 2023.'
- en: '[14] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741,
    2021.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 亚历克斯·尼科尔, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, 和 Mark Chen. Glide: 朝着通过文本引导的扩散模型生成和编辑逼真图像的方向前进. arXiv预印本
    arXiv:2112.10741, 2021.'
- en: '[15] OpenAI. Gpt-4v(ision) system card. 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] OpenAI. Gpt-4v(ision)系统卡. 2023.'
- en: '[16] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, 和 Robin Rombach. Sdxl: 改进高分辨率图像合成的潜在扩散模型. arXiv预印本 arXiv:2307.01952,
    2023.'
- en: '[17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, 和 Björn
    Ommer. 使用潜在扩散模型进行高分辨率图像合成. 在IEEE/CVF计算机视觉与模式识别会议论文集, 页码 10684–10695, 2022.'
- en: '[18] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in Neural Information Processing Systems, 35:36479–36494, 2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily
    L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    等. 具有深度语言理解的逼真文本到图像扩散模型. 神经信息处理系统进展, 35:36479–36494, 2022.'
- en: '[19] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.
    Deep unsupervised learning using nonequilibrium thermodynamics. In International
    conference on machine learning, pages 2256–2265\. PMLR, 2015.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, 和 Surya Ganguli.
    使用非平衡热力学的深度无监督学习. 在国际机器学习大会上, 页码 2256–2265. PMLR, 2015.'
- en: '[20] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, and Ben Poole. Score-based generative modeling through stochastic differential
    equations. arXiv preprint arXiv:2011.13456, 2020.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 杨松, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano
    Ermon, 和 Ben Poole. 基于得分的生成建模通过随机微分方程. arXiv预印本 arXiv:2011.13456, 2020.'
- en: '[21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, 等。Gemini：一系列高度能力的多模态模型。arXiv预印本arXiv:2312.11805,
    2023。'
- en: '[22] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin
    Cui. Mastering text-to-image diffusion: Recaptioning, planning, and generating
    with multimodal llms. arXiv preprint arXiv:2401.11708, 2024.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin
    Cui. 精通文本到图像扩散：重新描述、规划和利用多模态LLMs生成。arXiv预印本arXiv:2401.11708, 2024。'
- en: Appendix A Broader Impact
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 更广泛的影响
- en: Our work will bring significant advantages to both the research community focused
    on diffusion models and the practical application of T2I generation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作将为专注于扩散模型的研究社区和T2I生成的实际应用带来重大优势。
- en: In terms of the research community, we present a new and novel controllable
    image generation paradigm that demonstrates exceptional controllability and produces
    remarkable results even when tackling challenging tasks. This pioneering approach
    can offer valuable insights for future investigations into diffusion models.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究社区方面，我们提出了一种新的、创新的可控图像生成范式，展示了卓越的可控性，并在处理挑战性任务时取得了显著成果。这一开创性方法可以为未来对扩散模型的研究提供宝贵的见解。
- en: Regarding industrial applications, our method can be readily employed by T2I
    generation service providers to enhance the performance of their models. Moreover,
    the diffusion models operating within our framework are less likely to generate
    harmful content due to the meticulous control exerted at each generation stage.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业应用方面，我们的方法可以被T2I生成服务提供商直接采用，以增强其模型的性能。此外，我们框架内运行的扩散模型由于在每个生成阶段都施加了精细的控制，因此更不容易生成有害内容。
- en: Appendix B Differences between MuLan and the concurrent work RPG
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B MuLan与并行工作RPG的差异
- en: As stated in Introduction and Related work, although we acknowledge that our
    proposed framework shares a similar high-level idea with RPG, we would like to
    emphasize that there are still substantial differences between ours and RPG.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在引言和相关工作中所述，尽管我们承认我们提出的框架与RPG在高层次上有相似的思想，但我们希望强调的是，我们的方法与RPG之间仍然存在显著差异。
- en: Firstly, our proposed MuLan aims to progressively generate each object given
    each subprompt. At the same time, the objects are generated conditioned on previously
    generated objects. In RPG, on the other hand, all objects are generated independently.
    In addition, different from RPG which requires manually designed in-context examples
    for the CoT reasoning, ours does not have such requirement. We directly utilize
    LLMs for the planning during generation, which is an easier task and can be done
    by LLMs without in-context learning. What’s more, MuLan can adaptively control
    and correct the generation results using feedback by the VLMs while RPG does not
    have the feedback for the generation. Also, for the common overlapping problem
    between objects, we propose a strategy to generate several candidates to deal
    with it. In contrast, in RPG, the overlapping parts are treated as a whole for
    generation.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提出的MuLan旨在根据每个子提示逐步生成每个对象。同时，这些对象是基于之前生成的对象进行生成的。而RPG则是将所有对象独立生成。此外，与需要手动设计上下文示例进行CoT推理的RPG不同，我们的方法不需要这样的要求。我们直接利用LLMs在生成过程中进行规划，这是一个更简单的任务，且LLMs无需进行上下文学习即可完成。更重要的是，MuLan可以通过VLMs的反馈来自适应地控制和修正生成结果，而RPG则没有这样的生成反馈。同时，针对对象之间常见的重叠问题，我们提出了一种策略，通过生成多个候选项来应对这一问题。相较之下，RPG将重叠部分视为整体进行生成。
- en: 'More importantly, as we show in Section [4.1](https://arxiv.org/html/2402.12741v2#S4.SS1
    "4.1 Main Results and Analysis ‣ 4 Experiments ‣ MuLan: Multimodal-LLM Agent for
    Progressive and Interactive Multi-Object Diffusion"), our proposed framework can
    be directly applied to human-agent interaction during generation to facilitate
    flexible and effective changes/adjustments of the process while RPG cannot achieve
    the interaction. To summarize, the main differences between MuLan and RPG are
    as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，正如我们在第[4.1节](https://arxiv.org/html/2402.12741v2#S4.SS1 "4.1 主要结果与分析 ‣
    4 实验 ‣ MuLan：用于渐进式和互动式多对象扩散的多模态LLM代理")中所示，我们提出的框架可以在生成过程中直接应用于人机交互，从而促进过程的灵活和有效调整，而RPG无法实现这种交互。总结来说，MuLan与RPG之间的主要差异如下：
- en: •
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our proposed MuLan generates each object conditioned on previously generated
    objects while RPG generates all objects in parallel independently.
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的 MuLan 在生成每个对象时，都会根据之前生成的对象进行条件生成，而 RPG 是独立地并行生成所有对象。
- en: •
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MuLan does not require any in-context learning during the whole generation;
    in RPG, specifically designed in-context examples are needed for Chain-of-Thought
    reasoning.
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MuLan 在整个生成过程中不需要任何上下文学习；而 RPG 在进行 Chain-of-Thought 推理时需要专门设计的上下文示例。
- en: •
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MuLan utilizes the VLM-based feedback control to ensure each object can be generated
    correctly while RPG does NOT have such a feedback mechanism.
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MuLan 利用基于 VLM 的反馈控制来确保每个对象能够正确生成，而 RPG 没有这种反馈机制。
- en: •
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a strategy to deal with overlapping/interaction between objects whereas
    RPG directly treats overlapping objects as a whole part to generate.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种策略来处理对象之间的重叠/交互，而 RPG 直接将重叠的对象视为一个整体来生成。
- en: •
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: MuLan can be directly applied to human-agent interaction during generation for
    flexible and various adjustments of the generation process while RPG cannot achieve
    it.
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MuLan 可以直接应用于生成过程中与人类代理的交互，灵活地调整生成过程，而 RPG 无法实现这一点。
- en: Appendix C Background on (Latent) Diffusion Models
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C (潜在) 扩散模型背景
- en: Consisting of the diffusion process and the reverse process, diffusion models
    have shown impressive capability for high-quality image generation by iteratively
    adding noise and denoising [[9](https://arxiv.org/html/2402.12741v2#bib.bib9)].
    Let $\bm{x}_{0}\sim q(\bm{x}_{0})$ be the true data distribution. Starting from
    $\bm{x}_{0}$, the diffusion process adds different levels of noise pre-defined
    by the schedule $\{\beta_{t}\}_{1}^{T}$, producing $\bm{x}_{1},\cdots,\bm{x}_{T}$.
    As $T\to\infty$, $\bm{x}_{T}$ will become the standard Gaussian distribution $\mathcal{N}(\bm{0},\bm{I})$.
    Accordingly, the reverse process aims to reverse the above process and reconstruct
    the true data distribution from $p(\bm{x}_{T})=\mathcal{N}(\bm{0},\bm{I})$ by
    a parameterized noise model $\epsilon_{\theta}(\cdot)$. With $\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})$,
    the training loss of the model can be simplified as
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型由扩散过程和反向过程组成，已证明通过迭代添加噪声和去噪，能够展示出高质量图像生成的强大能力[[9](https://arxiv.org/html/2402.12741v2#bib.bib9)]。设
    $\bm{x}_{0}\sim q(\bm{x}_{0})$ 为真实数据分布。从 $\bm{x}_{0}$ 开始，扩散过程会根据预定义的调度 $\{\beta_{t}\}_{1}^{T}$
    添加不同程度的噪声，生成 $\bm{x}_{1},\cdots,\bm{x}_{T}$。当 $T\to\infty$ 时，$\bm{x}_{T}$ 将趋近于标准高斯分布
    $\mathcal{N}(\bm{0},\bm{I})$。因此，反向过程旨在反转上述过程，并通过参数化噪声模型 $\epsilon_{\theta}(\cdot)$
    从 $p(\bm{x}_{T})=\mathcal{N}(\bm{0},\bm{I})$ 重建真实数据分布。设 $\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})$，则模型的训练损失可以简化为
- en: '|  | $\displaystyle L(\theta)=\mathbb{E}_{t,\bm{x}_{0},\bm{\epsilon}}\&#124;\bm{\epsilon}%
    -\bm{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}%
    _{t}}\bm{\epsilon},t)\&#124;^{2}.$ |  | (3) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L(\theta)=\mathbb{E}_{t,\bm{x}_{0},\bm{\epsilon}}\&#124;\bm{\epsilon}%
    -\bm{\epsilon}_{\theta}(\sqrt{\bar{\alpha}_{t}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}%
    _{t}}\bm{\epsilon},t)\&#124;^{2}.$ |  | (3) |'
- en: Latent diffusion models [[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]
    have recently attracted growing attention due to their efficiency and superior
    performance. Instead of performing diffusion and its reverse process in the pixel
    space, they add noise and denoise in a latent space of $\bm{z}$ encoded by a pre-trained
    encoder $\mathcal{E}$. Thereby, the diffusion process starts from $\bm{z}_{0}=\mathcal{E}(\bm{x}_{0})$
    and subsequently produces latent states $\bm{z}_{1},\cdots,\bm{z}_{t},\cdots,\bm{z}_{T}$.
    Accordingly, the training loss becomes
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在扩散模型[[17](https://arxiv.org/html/2402.12741v2#bib.bib17)]由于其高效性和卓越的性能，近年来受到了越来越多的关注。它们不是在像素空间中执行扩散及其反向过程，而是在由预训练编码器
    $\mathcal{E}$ 编码的潜在空间 $\bm{z}$ 中添加噪声并进行去噪。因此，扩散过程从 $\bm{z}_{0}=\mathcal{E}(\bm{x}_{0})$
    开始，随后生成潜在状态 $\bm{z}_{1},\cdots,\bm{z}_{t},\cdots,\bm{z}_{T}$。因此，训练损失变为
- en: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-%
    \bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$ |  | (4) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle L_{LDM}=\mathbb{E}_{\bm{z}_{0},\bm{\epsilon},t}\&#124;\bm{\epsilon}-%
    \bm{\epsilon}_{\theta}(\bm{z}_{t},t)\&#124;^{2}.$ |  | (4) |'
- en: Appendix D Algorithm procedure of single-object diffusion in MuLan
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 单对象扩散算法过程在 MuLan 中的应用
- en: 'The complete and detailed procedure of single object diffusion described in
    Section [3.3](https://arxiv.org/html/2402.12741v2#S3.SS3 "3.3 Conditional Single-Object
    Diffusion with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan)
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")
    is shown in Algorithm [1](https://arxiv.org/html/2402.12741v2#alg1 "Algorithm
    1 ‣ Appendix D Algorithm procedure of single-object diffusion in MuLan ‣ MuLan:
    Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 单一对象扩散的完整详细过程描述在第[3.3节](https://arxiv.org/html/2402.12741v2#S3.SS3 "3.3 条件单一对象扩散与LLM规划和注意力引导
    ‣ 3 多模态-LLM代理 (MuLan) ‣ MuLan：用于渐进式和交互式多对象扩散的多模态-LLM代理")中，展示在算法[1](https://arxiv.org/html/2402.12741v2#alg1
    "算法1 ‣ 附录D MuLan中单一对象扩散的算法过程 ‣ MuLan：用于渐进式和交互式多对象扩散的多模态-LLM代理")中。
- en: Algorithm 1 Single Object Diffusion in MuLan
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 MuLan中的单一对象扩散
- en: '1:  Input: Object number $n$, sub-prompt $\texttt{p}_{n}$, LLM planner Planner,
    precise mask $\bm{\tilde{M}}_{n-1}$ (only for $n>1$), latents $\{\bm{z}_{(n-1),(t-1)}\}_{t=1}^{T}$
    (only for $n>1$), attention guidance timestep threshold $T^{\prime}$, combination
    timestep threshold $T^{*}$ (only for $n>1$), learning rate $\eta$, diffusion model
    $\mathcal{D}$.2:  Output: Image with $\texttt{obj}_{n}$ and its precise mask $\bm{\tilde{M}}_{n}$.3:  if $n=1$ then4:     $\texttt{opt}_{1},\texttt{Num}_{1}=\texttt{Planner}(\texttt{p}_{1})$5:     Apply
    Eq. ([5](https://arxiv.org/html/2402.12741v2#A7.E5 "In Appendix G Details for
    the computation of rough masks ‣ MuLan: Multimodal-LLM Agent for Progressive and
    Interactive Multi-Object Diffusion")) to compute $\bm{M}_{1}$6:     for $t=T,\cdots,1$ do7:        if $t>T^{\prime}$ then8:           $\bm{z}_{1,t}=\bm{z}_{1,t}-\eta\cdot\nabla_{\bm{z}_{1,t}}E(\bm{A},\bm{M}_{1},k)$9:        end if10:        $\bm{z}_{1,(t-1)}=\mathcal{D}(\bm{z}_{1,t},t,\texttt{p}_{1})$
    {Single denoising step}11:     end for12:  else13:     $\texttt{opt}_{n},\texttt{Num}_{n}=\texttt{Planner}(\texttt{p}_{n},\{\texttt{%
    obj}_{i}\}_{i=1}^{n-1})$14:     Apply Eq. ([6](https://arxiv.org/html/2402.12741v2#A7.E6
    "In Appendix G Details for the computation of rough masks ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")) to compute $\bm{M}_{n}$15:     for $t=T,\cdots,1$ do16:        if $t>T^{\prime}$ then17:           $\bm{z}_{n,t}=\bm{z}_{n,t}-\eta\cdot\nabla_{\bm{z}_{n,t}}E(\bm{A},\bm{M}_{n},k)$18:        end if19:        $\bm{z}_{n,(t-1)}=\mathcal{D}(\bm{z}_{n,t},t,\texttt{p}_{n})$20:        if $t>T^{*}$ then21:           Apply
    Eq. ([2](https://arxiv.org/html/2402.12741v2#S3.E2 "In Single-Object Generation
    with Attention Guidance. ‣ 3.3 Conditional Single-Object Diffusion with LLM Planning
    and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")) to combine latent
    of $\texttt{obj}_{n}$ and $\texttt{obj}_{n-1}$22:        end if23:     end for24:  end if25:  $\texttt{obj}_{n}=\bm{z}_{n,0}$26:  $\bm{\tilde{M}}_{n}=(\tilde{x}_{n},\tilde{y}_{n},\tilde{w}_{n},\tilde{h}_{n})$,
    a bounding box based on thresholding of $\frac{1}{|B|}\sum_{j\in B}\bm{A}^{(j)}_{(:,k)}${Token-$k$
    corresponds to $\texttt{obj}_{n}$}'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入: 对象数量 $n$，子提示 $\texttt{p}_{n}$，LLM规划器 Planner，精确掩码 $\bm{\tilde{M}}_{n-1}$（仅当
    $n>1$ 时），潜在变量 $\{\bm{z}_{(n-1),(t-1)}\}_{t=1}^{T}$（仅当 $n>1$ 时），注意力引导时间步阈值 $T^{\prime}$，组合时间步阈值
    $T^{*}$（仅当 $n>1$ 时），学习率 $\eta$，扩散模型 $\mathcal{D}$。'
- en: Appendix E Detailed prompt template of the global planning by the LLM
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 由LLM进行全局规划的详细提示模板
- en: 'As stated in Section [3.2](https://arxiv.org/html/2402.12741v2#S3.SS2 "3.2
    Prompt Decomposition by LLM Planning ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan:
    Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    MuLan first conduct the global planning to decompose the input prompts into $N$
    objects before the whole generation process. To this end, given the input prompt
    p, we prompt the LLM using the following template:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[3.2](https://arxiv.org/html/2402.12741v2#S3.SS2 "3.2 Prompt Decomposition
    by LLM Planning ‣ 3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent
    for Progressive and Interactive Multi-Object Diffusion")节所述，MuLan首先进行全局规划，将输入的提示分解为$N$个物体，然后再开始整个生成过程。为此，给定输入提示p，我们使用以下模板来提示LLM：'
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. You only need to list the objects
    in the description by painting order, from left to right, from down to top. Do
    not list additional information other than the objects mentioned in the description.
    Description: {p}.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个优秀的画家。我将给你一些描述，你的任务是将描述转化为一幅画。你只需要按绘画顺序列出描述中的物体，从左到右，从下到上。不要列出描述中未提及的其他信息。描述：{p}。
- en: In this way, the LLM will decompose the input prompt p following the pre-defined
    order.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，LLM将按照预定义的顺序分解输入提示p。
- en: Appendix F Detailed prompt template of the local planning by the LLM
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 由LLM进行局部规划的详细提示模板
- en: 'As stated in Section [3.3](https://arxiv.org/html/2402.12741v2#S3.SS3 "3.3
    Conditional Single-Object Diffusion with LLM Planning and Attention Guidance ‣
    3 Multimodal-LLM Agent (MuLan) ‣ MuLan: Multimodal-LLM Agent for Progressive and
    Interactive Multi-Object Diffusion"), the LLM is also utilized during the generation
    stage for local planning of the object’s rough position and the object counting.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[3.3](https://arxiv.org/html/2402.12741v2#S3.SS3 "3.3 Conditional Single-Object
    Diffusion with LLM Planning and Attention Guidance ‣ 3 Multimodal-LLM Agent (MuLan)
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion")节所述，LLM也在生成阶段用于规划物体的大致位置和物体数量。'
- en: 'For the rough position $\texttt{opt}_{1}$ planning of the first object, we
    utilize the following template:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个物体的大致位置$\texttt{opt}_{1}$的规划，我们使用以下模板：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I want to paint the {$\texttt{obj}_{1}$} in the painting firstly, where to put
    the {$\texttt{obj}_{1}$}? Choose from left, right, top, and bottom. You can make
    reasonable guesses. Give one answer.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个优秀的画家。我将给你一些描述，你的任务是将描述转化为一幅画。现在给定描述：{p}。如果我首先想要画出画中的{$\texttt{obj}_{1}$}，应该把{$\texttt{obj}_{1}$}放在哪里？从左、右、上、下中选择。你可以做出合理的猜测。给出一个答案。
- en: Then the LLM is prompted to figure out the object number based on $\texttt{opt}_{1}$.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 然后LLM被提示根据$\texttt{opt}_{1}$来确定物体的数量。
- en: 'If $\texttt{opt}_{1}=\texttt{left}$, the prompt template for $\texttt{obj}_{1}$
    is:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\texttt{opt}_{1}=\texttt{left}$，则$\texttt{obj}_{1}$的提示模板为：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the horizontal direction? ONLY give
    the final number.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个优秀的画家。我将给你一些描述，你的任务是将描述转化为一幅画。现在给定描述：{p}。在水平位置上有多少个不重叠的物体？只需给出最终的数字。
- en: 'If $\texttt{opt}_{1}=\texttt{bottom}$, the prompt template would be:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果$\texttt{opt}_{1}=\texttt{bottom}$，则提示模板为：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. How
    many non-overlapping objects are there in the vertical direction? ONLY give the
    final number.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个优秀的画家。我将给你一些描述，你的任务是将描述转化为一幅画。现在给定描述：{p}。在垂直方向上有多少个不重叠的物体？只需给出最终的数字。
- en: 'For the rough position $\texttt{opt}_{n}(n\geq 2)$, we utilize the following
    template:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大致位置$\texttt{opt}_{n}(n\geq 2)$，我们使用以下模板：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$},
    what is the position of the {$\texttt{obj}_{n}$} relative to the {$\texttt{obj}_{n-1}$}?
    Choose from left, right, above, bottom, and none of above. You can make reasonable
    guesses. Give one answer.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个出色的画家。我将给你一些描述。你的任务是将这些描述转化为一幅画。现在给定描述：{p}。如果我已经有一幅包含{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}的画，$\texttt{obj}_{n}$相对于$\texttt{obj}_{n-1}$的位置是什么？从左、右、上、下和以上都没有中选择一个。你可以做出合理的猜测。给出一个答案。
- en: 'Then we prompt the LLM to figure out the object number by:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们提示LLM通过以下方式确定物体数量：
- en: 'You are an excellent painter. I will give you some descriptions. Your task
    is to turn the description into a painting. Now given the description: {p}. If
    I already have a painting that contains {$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$},
    how many objects are there in/on the {$\texttt{opt}_{n}$} of {$\texttt{obj}_{n-1}$}?
    Only give the final number.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个出色的画家。我将给你一些描述。你的任务是将这些描述转化为一幅画。现在给定描述：{p}。如果我已经有一幅包含{$\{\texttt{obj}_{i}\}_{i=1}^{n-1}$}的画，$\texttt{opt}_{n}$上的$\texttt{obj}_{n-1}$有多少个物体？只给出最终数字。
- en: Appendix G Details for the computation of rough masks
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 粗略掩模计算的详细信息
- en: 'When $n=1$, since there is no object generated yet, both the position $\texttt{opt}_{1}$
    and $\texttt{Num}_{1}$ are unrestricted and the LLM can be prompted to determine
    $\texttt{opt}_{1}$ and $\texttt{Num}_{1}$ given sub-prompt $\texttt{p}_{1}$. Since
    the object order starts from left to right and bottom to top, there will be only
    two position options $\texttt{opt}_{1}\in\{\texttt{left},\texttt{bottom}\}$ for
    $\texttt{obj}_{1}$. Once $\texttt{opt}_{1}$ determined, MuLan evenly splits the
    whole image’s width/height ($W/H$) to $\texttt{Num}_{1}$ parts and assigns the
    very left (bottom) part to $\texttt{obj}_{1}$, which leads to the following bounding
    box (an illustration for the computation is shown in Figure [6](https://arxiv.org/html/2402.12741v2#A7.F6
    "Figure 6 ‣ Appendix G Details for the computation of rough masks ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion")):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当$n=1$时，由于还没有生成任何物体，因此位置$\texttt{opt}_{1}$和$\texttt{Num}_{1}$都是不受限制的，LLM可以通过子提示$\texttt{p}_{1}$来确定$\texttt{opt}_{1}$和$\texttt{Num}_{1}$。由于物体的排列顺序是从左到右、从下到上，因此$\texttt{obj}_{1}$只有两个位置选项$\texttt{opt}_{1}\in\{\texttt{left},\texttt{bottom}\}$。一旦确定了$\texttt{opt}_{1}$，MuLan会将整个图像的宽度/高度($W/H$)均分成$\texttt{Num}_{1}$个部分，并将最左（最下）部分分配给$\texttt{obj}_{1}$，这就导致了以下边界框（计算示意图见图[6](https://arxiv.org/html/2402.12741v2#A7.F6
    "图6 ‣ 附录G 粗略掩模计算详细信息 ‣ MuLan：多模态LLM代理用于渐进式和交互式多物体扩散")）：
- en: '|  | $\displaystyle\bm{M}_{1}=\begin{cases}(0,0,\frac{W}{\texttt{Num}_{1}},H),&\text%
    {if }\texttt{opt}_{1}=\texttt{left},\\ (\frac{(\texttt{Num}_{1}-1)\cdot H}{\texttt{Num}_{1}},0,W,\frac{H}{\texttt{Num%
    }_{1}}),&\text{if }\texttt{opt}_{1}=\texttt{bottom}.\end{cases}$ |  | (5) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{M}_{1}=\begin{cases}(0,0,\frac{W}{\texttt{Num}_{1}},H),&\text%
    {if }\texttt{opt}_{1}=\texttt{left},\\ (\frac{(\texttt{Num}_{1}-1)\cdot H}{\texttt{Num}_{1}},0,W,\frac{H}{\texttt{Num%
    }_{1}}),&\text{if }\texttt{opt}_{1}=\texttt{bottom}.\end{cases}$ |  | (5) |'
- en: '![Refer to caption](img/7f00ec07a318a51fd0a41e318d5d55d8.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f00ec07a318a51fd0a41e318d5d55d8.png)'
- en: 'Figure 6: Illustration of the rough mask $\bm{M}_{1}$ of $\texttt{obj}_{1}$.
    There are only two options $\texttt{left},\texttt{bottom}$ for the mask since
    the LLM is prompted to plan the object order from left to right, bottom to top.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：$\texttt{obj}_{1}$的粗略掩模$\bm{M}_{1}$示意图。由于LLM被提示按从左到右、从下到上的顺序规划物体，掩模只有两个选项$\texttt{left},\texttt{bottom}$。
- en: When $n>1$, the position $\texttt{opt}_{n}$ denotes $\{\texttt{obj}\}_{n}$’s
    relational position to the previous object $\{\texttt{obj}\}_{n-1}$. Since MuLan
    generates objects from left to right and from bottom to top, $\texttt{opt}_{n}\in\texttt{\{right,top\}}$.
    Given sub-prompt $\texttt{p}_{n}$, an LLM is prompted to select $\texttt{opt}_{n}$
    and determine $\texttt{Num}_{n}$. Meanwhile, the precise mask $\bm{\tilde{M}}_{n-1}=(\tilde{x}_{n-1},\tilde{y}_{n-1},\tilde{w}_{n-1},\tilde{h%
    }_{n-1})$ of $\texttt{opt}_{n-1}$ can be extracted from the image with $\{\texttt{obj}\}_{n-1}$
    generated (e.g., by text-image cross-attention maps in the diffusion model), which
    is utilized as the condition for the computation of bounding box boundary of the
    rough mask $\bm{M}_{n}$. Hence, the rough mask $\bm{M}_{n}$ for $\texttt{obj}_{n}$
    can be derived from $\texttt{opt}_{n}$, $\texttt{Num}_{n}$, and $\bm{\tilde{M}}_{n-1}$
    as followings.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当$n>1$时，位置$\texttt{opt}_{n}$表示$\{\texttt{obj}\}_{n}$相对于前一个对象$\{\texttt{obj}\}_{n-1}$的关系位置。由于MuLan是从左到右、从下到上生成对象的，因此$\texttt{opt}_{n}
    \in \texttt{\{right,top\}}$。给定子提示$\texttt{p}_{n}$，一个LLM会选择$\texttt{opt}_{n}$并确定$\texttt{Num}_{n}$。与此同时，$\texttt{opt}_{n-1}$的精确掩码$\bm{\tilde{M}}_{n-1}=(\tilde{x}_{n-1},\tilde{y}_{n-1},\tilde{w}_{n-1},\tilde{h}_{n-1})$可以通过生成的$\{\texttt{obj}\}_{n-1}$从图像中提取（例如，通过扩散模型中的文本-图像跨注意力图），它作为计算粗略掩码$\bm{M}_{n}$边界框的条件。因此，粗略掩码$\bm{M}_{n}$可以根据$\texttt{opt}_{n}$、$\texttt{Num}_{n}$和$\bm{\tilde{M}}_{n-1}$如下计算得出。
- en: '|  | $\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{%
    W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),&\text{if }\texttt{opt%
    }_{n}=\texttt{right},\\ \\ (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{%
    \tilde{y}_{n-1}}{\texttt{Num}_{n}}),&\text{if }\texttt{opt}_{n}=\texttt{top}.%
    \end{cases}$ |  | (6) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{M}_{n}=\begin{cases}(\tilde{x}_{n-1}+\tilde{w}_{n-1},0,\frac{%
    W-\tilde{x}_{n-1}+\tilde{w}_{n-1}}{\texttt{Num}_{n}},H),&\text{如果 }\texttt{opt%
    }_{n}=\texttt{right},\\ \\ (0,\frac{\tilde{y}_{n-1}\cdot(\texttt{Num}_{n}-1)}{\texttt{Num}_{n}},W,\frac{%
    \tilde{y}_{n-1}}{\texttt{Num}_{n}}),&\text{如果 }\texttt{opt}_{n}=\texttt{top}.%
    \end{cases}$ |  | (6) |'
- en: 'Figure [7](https://arxiv.org/html/2402.12741v2#A7.F7 "Figure 7 ‣ Appendix G
    Details for the computation of rough masks ‣ MuLan: Multimodal-LLM Agent for Progressive
    and Interactive Multi-Object Diffusion") illustrates how the rough mask can be
    computed based on the precise mask of previous objects.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '图[7](https://arxiv.org/html/2402.12741v2#A7.F7 "图 7 ‣ 附录 G 粗略掩码计算细节 ‣ MuLan:
    逐步和交互式多对象扩散的多模态 LLM 智能体")展示了如何根据前一个对象的精确掩码计算粗略掩码。'
- en: '![Refer to caption](img/24687ef55a35cabf22933e914e9d19ea.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/24687ef55a35cabf22933e914e9d19ea.png)'
- en: 'Figure 7: The rough mask $\bm{M}_{n}$ of $\texttt{obj}_{n}(n>1)$ is derived
    from the precise mask $\bm{\tilde{M}}_{n-1}$ of the previously generated object
    $\texttt{obj}_{n-1}$.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：$\bm{M}_{n}$（$\texttt{obj}_{n}(n>1)$）的粗略掩码源自前一个生成的对象$\texttt{obj}_{n-1}$的精确掩码$\bm{\tilde{M}}_{n-1}$。
- en: Appendix H More details on the overlapping processing
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 更多关于重叠处理的细节
- en: Given $\texttt{opt}_{n}$ and $\bm{\tilde{M}}_{n-1}$, the rough mask $\bm{M}_{n,i}$
    can be computed as
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$\texttt{opt}_{n}$和$\bm{\tilde{M}}_{n-1}$，可以计算出粗略掩码$\bm{M}_{n,i}$，计算公式如下：
- en: '|  | $\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(%
    \tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}%
    \cdot r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}_{n}},\tilde{%
    h}_{n-1}\Big{)},\\ \text{if }\texttt{opt}_{n}=\texttt{right},\\ \\ \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}_{n}-1)\cdot\tilde{y}_{n-1}}{\texttt%
    {Num}_{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot r_{i}+\frac{\tilde{y}_{n-1}}{%
    \texttt{Num}_{n}}\Big{)},\\ \text{if }\texttt{opt}_{n}=\texttt{top}.\end{cases}$
    |  | (7) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{M}_{n,i}=\begin{cases}\Big{(}\tilde{x}_{n-1}\cdot r_{i}+(%
    \tilde{x}_{n-1}+\tilde{w}_{n-1})\cdot(1-r_{i}),\tilde{y}_{n-1},\tilde{w}_{n-1}%
    \cdot r_{i}+\frac{W-\tilde{x}_{n-1}-\tilde{w}_{n-1}}{\texttt{Num}_{n}},\tilde{%
    h}_{n-1}\Big{)},\\ \text{如果 }\texttt{opt}_{n}=\texttt{right},\\ \\ \Big{(}\tilde{x}_{n-1},\frac{(\texttt{Num}_{n}-1)\cdot\tilde{y}_{n-1}}{\texttt%
    {Num}_{n}},\tilde{w}_{n-1},\tilde{h}_{n-1}\cdot r_{i}+\frac{\tilde{y}_{n-1}}{%
    \texttt{Num}_{n}}\Big{)},\\ \text{如果 }\texttt{opt}_{n}=\texttt{top}.\end{cases}$
    |  | (7) |'
- en: 'The illustration for different overlapping ratios is shown in Figure [8](https://arxiv.org/html/2402.12741v2#A8.F8
    "Figure 8 ‣ Appendix H More details on the overlapping processing ‣ MuLan: Multimodal-LLM
    Agent for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '不同重叠比例的示意图见图[8](https://arxiv.org/html/2402.12741v2#A8.F8 "图 8 ‣ 附录 H 重叠处理的更多细节
    ‣ MuLan: 逐步和交互式多对象扩散的多模态 LLM 智能体")。'
- en: '![Refer to caption](img/da1ce48dac2a1c8be328e64f1ac895ec.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/da1ce48dac2a1c8be328e64f1ac895ec.png)'
- en: 'Figure 8: Three candidate masks $\bm{M}_{n,i}$ of $\texttt{obj}_{n}$ at position
    $\texttt{opt}_{n}=\texttt{top}$. They correspond to $\texttt{obj}_{n}$ overlapping
    with 10%, 30%, and 50% of $\texttt{obj}_{n-1}$.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：位置为 $\texttt{opt}_{n}=\texttt{top}$ 时，$\texttt{obj}_{n}$ 的三个候选掩码 $\bm{M}_{n,i}$。它们分别表示
    $\texttt{obj}_{n}$ 与 $\texttt{obj}_{n-1}$ 重叠的 10%、30% 和 50%。
- en: Appendix I More details on the evaluation questionnaire
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 评估问卷的更多细节
- en: 'As shown in Section [4](https://arxiv.org/html/2402.12741v2#S4 "4 Experiments
    ‣ MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion"),
    we design a questionnaire to comprehensively evaluate the alignment between the
    generated image and the text by GPT-4V [[15](https://arxiv.org/html/2402.12741v2#bib.bib15)]
    and human, from three aspects - object completeness, correctness of attribute
    bindings, and correctness of spatial relationships. Specifically, given an image
    and a text prompt, for object completeness, we will evaluate if the image contains
    each single object in the prompt. If the object appears in the image, we will
    then judge if the attribute bindings of the object in the image align with the
    corresponding attribute bindings in the text prompt, to evaluate the correctness
    of attribute bindings. We will also ask GPT-4V or human to judge if the spatial
    relationships are correct and match the text, as the evaluation of the spatial
    relationships.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4](https://arxiv.org/html/2402.12741v2#S4 "4 实验 ‣ MuLan：面向渐进式和互动式多物体扩散的多模态大语言模型代理")节中所示，我们设计了一份问卷，从三个方面综合评估生成图像与文本之间的对齐程度：物体完整性、属性绑定的正确性和空间关系的正确性。具体来说，给定一张图像和一个文本提示，对于物体完整性，我们将评估图像是否包含提示中的每一个物体。如果物体出现在图像中，我们接下来将判断图像中物体的属性绑定是否与文本提示中的相应属性绑定一致，从而评估属性绑定的正确性。我们还将要求
    GPT-4V 或人工判断空间关系是否正确，并与文本匹配，这是对空间关系的评估。
- en: Examples of the questionnaire for different images and text prompts are shown
    in Figure LABEL:fig:evaluation.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图 LABEL:fig:evaluation 显示了针对不同图像和文本提示的问卷示例。
- en: Appendix J More qualitative results
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J 更多定性结果
- en: 'We show more examples of different methods in Figure [11](https://arxiv.org/html/2402.12741v2#A10.F11
    "Figure 11 ‣ Appendix J More qualitative results ‣ MuLan: Multimodal-LLM Agent
    for Progressive and Interactive Multi-Object Diffusion").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图[11](https://arxiv.org/html/2402.12741v2#A10.F11 "图 11 ‣ 附录 J 更多定性结果 ‣ MuLan：面向渐进式和互动式多物体扩散的多模态大语言模型代理")中展示了不同方法的更多示例。
- en: '![Refer to caption](img/a13a94b45ecea3c8d1a19eab8cb384a6.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a13a94b45ecea3c8d1a19eab8cb384a6.png)'
- en: 'Figure 11: More qualitative examples of images generated by different methods
    on intricate prompts.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：不同方法在复杂提示下生成的图像的更多定性示例。
