- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2025-01-11 12:11:16'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2025-01-11 12:11:16'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择比努力更重要：大语言模型（LLM）促进高效的多智能体探索
- en: 来源：[https://arxiv.org/html/2410.02511/](https://arxiv.org/html/2410.02511/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.02511/](https://arxiv.org/html/2410.02511/)
- en: Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yun Qu, Boyuan Wang, Yuhang Jiang, Jianzhun Shao, Yixiu Mao,
- en: Cheems Wang, Chang Liu, Xiangyang Ji Department of Automation, Tsinghua University,
    Beijing, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Cheems Wang, Chang Liu, Xiangyang Ji 自动化系，清华大学，北京，中国
- en: '{qy22, wangby22, jiangyh19, sjz18, myx21}@mails.tsinghua.edu.cn'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{qy22, wangby22, jiangyh19, sjz18, myx21}@mails.tsinghua.edu.cn'
- en: cheemswang@mail.tsinghua.edu.cn, {liuchang2022, xyji}@tsinghua.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: cheemswang@mail.tsinghua.edu.cn, {liuchang2022, xyji}@tsinghua.edu.cn
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With expansive state-action spaces, efficient multi-agent exploration remains
    a longstanding challenge in reinforcement learning. Although pursuing novelty,
    diversity, or uncertainty attracts increasing attention, redundant efforts brought
    by exploration without proper guidance choices poses a practical issue for the
    community. This paper introduces a systematic approach, termed LEMAE, choosing
    to channel informative task-relevant guidance from a knowledgeable Large Language
    Model (LLM) for Efficient Multi-Agent Exploration. Specifically, we ground linguistic
    knowledge from LLM into symbolic key states, that are critical for task fulfillment,
    in a discriminative manner at low LLM inference costs. To unleash the power of
    key states, we design Subspace-based Hindsight Intrinsic Reward (SHIR) to guide
    agents toward key states by increasing reward density. Additionally, we build
    the Key State Memory Tree (KSMT) to track transitions between key states in a
    specific task for organized exploration. Benefiting from diminishing redundant
    explorations, LEMAE outperforms existing SOTA approaches on the challenging benchmarks
    (e.g., SMAC and MPE) by a large margin, achieving a 10x acceleration in certain
    scenarios.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有广泛状态-动作空间的环境中，高效的多智能体探索依然是强化学习中的一个长期挑战。尽管追求新颖性、多样性或不确定性吸引了越来越多的关注，但缺乏适当指导选择的探索所带来的冗余努力，已成为社区面临的实际问题。本文提出了一种系统化的方法，称为LEMAE，旨在通过引导多智能体探索，利用知识丰富的大语言模型（LLM）提供任务相关的有效指导。具体而言，我们通过低推理成本的判别方式，将LLM的语言知识嵌入到关键符号状态中，这些状态对任务的完成至关重要。为了释放关键状态的潜力，我们设计了基于子空间的事后内在奖励（SHIR），通过增加奖励密度来引导智能体朝向关键状态。此外，我们构建了关键状态记忆树（KSMT），用于跟踪特定任务中关键状态之间的过渡，以便组织化探索。得益于减少冗余探索，LEMAE在挑战性基准测试（如SMAC和MPE）上大幅超越现有的最先进方法，在某些场景中实现了10倍加速。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Exploration stands as a fundamental issue in reinforcement learning (RL) (Du
    et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35); Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)).
    Researchers have developed several exploration strategies directed by novelty,
    diversity, or uncertainty (Linke et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib88);
    Burda et al., [2018b](https://arxiv.org/html/2410.02511v1#bib.bib20); Pathak et al.,
    [2017](https://arxiv.org/html/2410.02511v1#bib.bib121)), mainly in single-agent
    reinforcement learning. However, these methods may induce task-irrelevant redundant
    exploration, especially in complex environments (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)).
    In the realm of Multi-Agent Reinforcement Learning (MARL), the need to mitigate
    exploration redundancy becomes even more urgent due to the challenges like exponential
    expansion of the state-action spaces. Widespread real-world applications, including
    MOBA games (Qu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib126)),
    social science (Jaques et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib55)),
    and multi-vehicle control (Xu et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib181)),
    further underscore the growing need for efficient multi-agent exploration.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 探索是强化学习（RL）中的一个基础问题（Du 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)；Liu
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）。研究人员已经开发了几种由新颖性、多样性或不确定性驱动的探索策略（Linke
    等人，[2020](https://arxiv.org/html/2410.02511v1#bib.bib88)；Burda 等人，[2018b](https://arxiv.org/html/2410.02511v1#bib.bib20)；Pathak
    等人，[2017](https://arxiv.org/html/2410.02511v1#bib.bib121)），主要应用于单一智能体强化学习。然而，这些方法可能会引发与任务无关的冗余探索，尤其是在复杂环境中（Du
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）。在多智能体强化学习（MARL）领域，由于状态-动作空间的指数级扩展等挑战，减少探索冗余的需求变得更加迫切。广泛的现实世界应用，包括多人在线战术竞技游戏（MOBA）（Qu
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib126)）、社会科学（Jaques 等人，[2019](https://arxiv.org/html/2410.02511v1#bib.bib55)）以及多车辆控制（Xu
    等人，[2018](https://arxiv.org/html/2410.02511v1#bib.bib181)），进一步突显了高效多智能体探索的日益重要性。
- en: This work identifies task-relevant guidance as an important consideration in
    enhancing exploration efficiency. Incorporating priors in exploration mechanism
    design, such as complex reward structures, typically requires expert knowledge
    and substantial human efforts (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90);
    Abbeel & Ng, [2004](https://arxiv.org/html/2410.02511v1#bib.bib1)). Hopefully,
    recent advances have witnessed the remarkable reasoning and planning capabilities
    of Large Language Models (Touvron et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib159);
    Achiam et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib2)), providing
    a plausible choice to facilitate efficient exploration through LLM’s effortless
    prior provision. However, it is non-trivial to effectively comprise linguistic
    LLM priors into symbolically represented RL tasks (Peng et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib123);
    Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23)), and the
    investigation of practical ways to avoid nuisances caused by such an expression
    discrepancy is of critical importance.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究认为任务相关的引导是提升探索效率的重要考虑因素。在探索机制设计中融入先验知识，如复杂的奖励结构，通常需要专家知识和大量的人工努力（Liu 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)；Abbeel
    & Ng，[2004](https://arxiv.org/html/2410.02511v1#bib.bib1)）。幸运的是，最近的进展见证了大型语言模型（LLM）在推理和规划能力上的显著突破（Touvron
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib159)；Achiam 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib2)），为通过LLM轻松提供先验知识来促进高效探索提供了一个可行的选择。然而，如何将语言性的LLM先验有效地融入符号表示的强化学习任务中（Peng
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib123)；Carta 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib23)）并非易事，探索如何避免这种表达差异带来的问题是至关重要的。
- en: '![Refer to caption](img/813150d86be815a26b07493fd28d088c.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/813150d86be815a26b07493fd28d088c.png)'
- en: 'Figure 1: (a) The map of the task Pass. Two agents are initially positioned
    in the left room, requiring cooperation to explore the rooms, uncover the hidden
    switches, and move to the right room. (b) The key states ($\kappa_{1}$ and $\kappa_{2}$)
    generated by LLM for the task Pass, where the superscripts $A,B$ of $\kappa_{i}$
    denote two agents Alice and Bob. (c) Visitation Map (log scale) of SOTA baseline
    method CMAE. (d) Visitation Map (log scale) of our method LEMAE. Our method exhibits
    a significant reduction in redundant exploration. Furthermore, an organic division
    of labor among agents emerges.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：（a）任务Pass的地图。两个智能体最初位于左侧房间，需要合作探索各房间，揭开隐藏的开关，并移动到右侧房间。（b）LLM为任务Pass生成的关键状态（$\kappa_{1}$和$\kappa_{2}$），其中$\kappa_{i}$的上标$A,B$表示两个智能体Alice和Bob。（c）SOTA基准方法CMAE的访问图（对数尺度）。（d）我们方法LEMAE的访问图（对数尺度）。我们的方法显著减少了冗余探索。此外，智能体之间自然地形成了有机的分工。
- en: 'In response to the above issue, we propose LEMAE, a novel framework to enable
    efficient multi-agent exploration with LLM. The framework primarily consists of
    two components: (i) key states localization with LLM and (ii) key state-guided
    exploration. The first component grounds linguistic knowledge from LLM into symbolic
    key states by automatically localizing key states that are essential for task
    fulfillment. Specifically, the discriminator function induced by LLM works to
    discriminate key states from rollout trajectories, avoiding the overburden of
    LLM inference costs. The second component harnesses the localized key states as
    meaningful guidance to achieve efficient exploration. In implementation, we encourage
    agents toward targeted key states by devising Subspace-based Hindsight Intrinsic
    Reward (SHIR) to increase reward density. For the purpose of organized exploration,
    Key States Memory Tree (KSMT) is further constructed to track key state transitions,
    mitigating exploration complexity and enhancing guidance in SHIR. As illustrated
    in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    our design empowers LEMAE with a significant performance advantage through notably
    reducing redundant exploration.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '针对上述问题，我们提出了LEMAE，一个新颖的框架，旨在通过LLM实现高效的多智能体探索。该框架主要由两个部分组成：（i）LLM的关键状态定位和（ii）关键状态引导的探索。第一部分通过自动定位任务完成所需的关键状态，将LLM的语言知识转化为符号化的关键状态。具体来说，LLM诱导的判别函数用于区分来自回放轨迹的关键状态，避免了LLM推理成本的过度负担。第二部分利用已定位的关键状态作为有效的引导，实现高效的探索。在实现过程中，我们通过设计基于子空间的后见之明内在奖励（Subspace-based
    Hindsight Intrinsic Reward，简称SHIR）来增加奖励密度，鼓励智能体朝向目标关键状态前进。为了实现有序的探索，进一步构建了关键状态记忆树（Key
    States Memory Tree，简称KSMT）以追踪关键状态的转变，从而降低探索复杂度并增强SHIR中的引导作用。如图[1](https://arxiv.org/html/2410.02511v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")所示，我们的设计通过显著减少冗余探索，使得LEMAE在性能上具有显著的优势。'
- en: 'Our main contributions are summarized as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要贡献总结如下：
- en: '1.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We build a bridge between LLM and RL to facilitate efficient multi-agent exploration
    by developing a systematic approach dubbed LEMAE.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过开发一种系统化的方法——LEMAE，构建了LLM与强化学习（RL）之间的桥梁，以促进高效的多智能体探索。
- en: '2.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We devise a computationally efficient inference strategy channeling task-specific
    information from LLM to distinguish key states critical for task fulfillment as
    subgoals for targeted exploration.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种计算高效的推理策略，通过从大型语言模型（LLM）传递任务特定的信息，以区分对任务完成至关重要的关键状态，并将其作为子目标进行针对性探索。
- en: '3.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We introduce a Key State Memory Tree to organize exploration according to historic
    key state transitions and devise the Subspace-based Hindsight Intrinsic Reward,
    encouraging agents’ guidance.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了关键状态记忆树（Key State Memory Tree）来根据历史关键状态转变组织探索，并设计了基于子空间的后见之明内在奖励（Subspace-based
    Hindsight Intrinsic Reward），以鼓励智能体的引导。
- en: We conduct extensive experiments on typical multi-agent exploration benchmarks.
    LEMAE (i) consistently outperforms the state-of-the-art (SOTA) baselines with
    10x acceleration in certain scenarios, (ii) achieves performance comparable to
    the baseline trained with human-designed dense rewards in sparse reward scenarios,
    and (iii) exhibits potential to generalize to brand-new, non-symbolic tasks. These
    observations validate the effectiveness of our design in reducing redundant exploration
    and improving exploration efficiency, showing promise for real-world deployment
    in scenarios requiring efficient exploration.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在典型的多智能体探索基准上进行了广泛的实验。LEMMAE (i) 在某些场景下始终优于最先进的(SOTA)基准，具有10倍的加速效果；(ii) 在稀疏奖励场景中，表现与使用人工设计密集奖励训练的基准相当；(iii)
    展示了在全新、非符号任务中泛化的潜力。这些观察结果验证了我们设计的有效性，能够减少冗余的探索并提高探索效率，展现了在需要高效探索的现实场景中的应用前景。
- en: 2 Preliminary
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步工作
- en: 'The environments considered in this work are characterized as a decentralized
    partially observable Markov decision process (Dec-POMDP) (Oliehoek et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib115))
    with $n$ agents, which can be defined as a tuple $G=\langle S,A,I,P,r,Z,O,n,\gamma\rangle$,
    where $s\in S$ is the global state, $A$ is the action space for each agent, and
    $\gamma\in[0,1)$ is the discount factor. At time step $t$, each agent $i\in I\equiv\{1,...,n\}$
    has its local observations $o^{i}\in O$ drawn from the observation function $Z(s,i):S\times
    I\rightarrow O$ and chooses an action $a^{i}\in A$ by its policy $\pi^{i}(a^{i}|o^{i}):O\rightarrow\Delta([0,1]^{|A|})$,
    forming a joint action $\mathbf{a}\in\mathbf{A}\equiv A^{n}$. $T(s^{\prime}|s,\mathbf{a}):S\times\mathbf{A}\times
    S\rightarrow[0,1]$ is the environment’s state transition distribution. All agents
    share a common reward function $r(s,\mathbf{a}):S\times\mathbf{A}\rightarrow\mathbb{R}$.
    The agents’ joint policy $\bm{\pi}:=\prod_{i=1}^{n}\pi^{i}$ induces a joint *action-value
    function*: $Q^{\bm{\pi}}(s,\mathbf{a})=\mathbb{E}[R|s,\mathbf{a}]$, where $R=\sum^{\infty}_{t=0}\gamma^{t}r_{t}$
    is the expected discounted return. The goal of MARL is to find the optimal joint
    policy $\bm{\pi}^{*}$ such that $Q^{\bm{\pi}^{*}}(s,\mathbf{a})\geq Q^{\bm{\pi}}(s,\mathbf{a})$,
    $\forall\bm{\pi}\ \text{and}\ (s,\mathbf{a})\in S\times\mathbf{A}$. Notably, we
    specifically focus on sparse reward tasks, i.e., $r_{t}=1$ only when $s_{t+1}=s_{success}$,
    otherwise $r_{t}=0$. We denote the symbol for the $i$-th key state by $\kappa_{i}$
    together with its discriminator function $\mathcal{F}_{i}$.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中考虑的环境被定义为一个去中心化的部分可观察马尔可夫决策过程（Dec-POMDP）（Oliehoek等人，[2016](https://arxiv.org/html/2410.02511v1#bib.bib115)），有$n$个智能体，可以定义为一个元组$G=\langle
    S,A,I,P,r,Z,O,n,\gamma\rangle$，其中$s\in S$是全局状态，$A$是每个智能体的动作空间，$\gamma\in[0,1)$是折扣因子。在时间步长$t$时，每个智能体$i\in
    I\equiv\{1,...,n\}$具有其局部观测$o^{i}\in O$，由观测函数$Z(s,i):S\times I\rightarrow O$生成，并根据其策略$\pi^{i}(a^{i}|o^{i}):O\rightarrow\Delta([0,1]^{|A|})$选择一个动作$a^{i}\in
    A$，形成联合动作$\mathbf{a}\in\mathbf{A}\equiv A^{n}$。$T(s^{\prime}|s,\mathbf{a}):S\times\mathbf{A}\times
    S\rightarrow[0,1]$是环境的状态转移分布。所有智能体共享一个公共奖励函数$r(s,\mathbf{a}):S\times\mathbf{A}\rightarrow\mathbb{R}$。智能体的联合策略$\bm{\pi}:=\prod_{i=1}^{n}\pi^{i}$诱导出联合*动作价值函数*：$Q^{\bm{\pi}}(s,\mathbf{a})=\mathbb{E}[R|s,\mathbf{a}]$，其中$R=\sum^{\infty}_{t=0}\gamma^{t}r_{t}$是期望的折扣回报。MARL的目标是找到最优的联合策略$\bm{\pi}^{*}$，使得$Q^{\bm{\pi}^{*}}(s,\mathbf{a})\geq
    Q^{\bm{\pi}}(s,\mathbf{a})$，$\forall\bm{\pi}\ \text{和}\ (s,\mathbf{a})\in S\times\mathbf{A}$。值得注意的是，我们特别关注稀疏奖励任务，即，$r_{t}=1$仅当$s_{t+1}=s_{success}$时，否则$r_{t}=0$。我们用符号$\kappa_{i}$表示第$i$个关键状态及其判别函数$\mathcal{F}_{i}$。
- en: 3 Related Works
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关工作
- en: LLM in Decision Making.
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 决策中的LLM。
- en: Large Language Models have showcased impressive capabilities across various
    downstream tasks (Touvron et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib159);
    Radford et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib127); Brown
    et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib16)). Recent advances
    indicate a growing trend of using LLM in decision-making problems (Wang et al.,
    [2023b](https://arxiv.org/html/2410.02511v1#bib.bib167)). A primary challenge
    within this domain is grounding LLM’s linguistic knowledge into specific low-level
    control tasks typically represented in symbolic form (Peng et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib123);
    Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23)), especially
    in RL. Creating linguistic twin tasks (Carta et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23))
    are intuitive but require substantial manual workloads. Some works employ LLMs
    as high-level planners, e.g., coding with APIs (Liang et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib85)),
    using human-annotated or LLM-summarized action template (Yao et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib186);
    Shinn et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib141); Lin et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib86); Zhu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib200);
    Wang et al., [2023a](https://arxiv.org/html/2410.02511v1#bib.bib165)). Despite
    significant progress, they rely on difficult-to-obtain low-level policies or APIs,
    limiting their real-world applicability. Recently, LLMs have been integrated with
    RL to directly enhance low-level decision making (Cao et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib22)).
    LLMs can act as environmental information processors, reducing learning complexity (Paischer
    et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib117); [2024](https://arxiv.org/html/2410.02511v1#bib.bib118);
    Kim et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib64); Wang et al.,
    [2024](https://arxiv.org/html/2410.02511v1#bib.bib164)), but cannot directly facilitate
    efficient exploration. Some works utilize LLMs as goal selectors in goal-conditioned
    RL (Su & Zhang, [2023](https://arxiv.org/html/2410.02511v1#bib.bib148); Shukla2023lgts)
    or teacher policy (Zhou et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib199))
    but require predefined skills or subgoals. Alternative methods like LLM-based
    reward or policy design (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98);
    Kwon et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib75); Song et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib146); Liu et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib93);
    Chen et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib24)) and fine-tuning (Carta
    et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib23); Shi et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib140))
    are either limited to simple tasks with sufficient information or demand enormous
    data and resources. ELLM (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35))
    aims to enhance exploration using LLM but depends on predefined symbolic observation
    captioner and frequent LLM inferences. Its semantic similarity-based rewards may
    also struggle to generalize across diverse scenarios. In contrast, LEMAE integrates
    linguistic LLM priors into symbolic states with minimal task-specific information
    and LLM inference costs, achieved by localizing key states in rollout trajectories
    using LLM-generated discriminator functions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在各种下游任务中展示了令人印象深刻的能力（Touvron 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib159)；Radford
    等，[2019](https://arxiv.org/html/2410.02511v1#bib.bib127)；Brown 等，[2020](https://arxiv.org/html/2410.02511v1#bib.bib16)）。近期的进展表明，使用
    LLM 解决决策问题的趋势正在增长（Wang 等，[2023b](https://arxiv.org/html/2410.02511v1#bib.bib167)）。该领域的主要挑战之一是将
    LLM 的语言知识与通常以符号形式表示的特定低级控制任务结合起来（Peng 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib123)；Carta
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib23)），尤其是在强化学习（RL）中。创建语言双胞胎任务（Carta
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib23)）直观，但需要大量人工工作。一些研究将 LLM
    用作高级规划器，例如，使用 API 进行编码（Liang 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib85)），利用人工标注或
    LLM 总结的行动模板（Yao 等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib186)；Shinn
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib141)；Lin 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib86)；Zhu
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib200)；Wang 等，[2023a](https://arxiv.org/html/2410.02511v1#bib.bib165)）。尽管取得了显著进展，但它们依赖于难以获得的低级策略或
    API，限制了其在现实世界中的应用。最近，LLM 已与 RL 集成，以直接增强低级决策能力（Cao 等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib22)）。LLM
    可以作为环境信息处理器，从而减少学习复杂性（Paischer 等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib117)；[2024](https://arxiv.org/html/2410.02511v1#bib.bib118)；Kim
    等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib64)；Wang 等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib164)），但不能直接促进高效的探索。一些研究利用
    LLM 作为目标选择器，在目标条件下的强化学习（Su & Zhang，[2023](https://arxiv.org/html/2410.02511v1#bib.bib148)；Shukla
    2023lgts）或作为教师策略（Zhou 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib199)），但需要预定义的技能或子目标。其他方法，如基于
    LLM 的奖励或策略设计（Ma 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib98)；Kwon 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib75)；Song
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib146)；Liu 等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib93)；Chen
    等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib24)）和微调（Carta 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib23)；Shi
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib140)）要么仅限于具有足够信息的简单任务，要么需要大量数据和资源。ELLM（Du
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）旨在通过 LLM 增强探索，但依赖于预定义的符号观察描述器和频繁的
    LLM 推理。其基于语义相似性的奖励也可能在不同场景中难以推广。相比之下，LEMAE 将语言 LLM 先验知识集成到符号状态中，且仅需最少的任务特定信息和
    LLM 推理成本，方法是通过使用 LLM 生成的判别器函数，在展开轨迹中定位关键状态。
- en: Efficient Multi-Agent Exploration.
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高效的多智能体探索。
- en: Exploration efficiency has long been a focal point in RL (Thrun, [1992](https://arxiv.org/html/2410.02511v1#bib.bib156);
    Cai et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib21); Seo et al.,
    [2021](https://arxiv.org/html/2410.02511v1#bib.bib137); Mahajan et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100);
    Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Liu et al.,
    [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). Typical exploration methods
    focus on random exploration (Mnih et al., [2013](https://arxiv.org/html/2410.02511v1#bib.bib109);
    Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128)) or heuristic
    indicators, such as diversity or novelty, to facilitate exhaustive exploration,
    particularly in single agent exploration (Linke et al., [2020](https://arxiv.org/html/2410.02511v1#bib.bib88);
    Burda et al., [2018b](https://arxiv.org/html/2410.02511v1#bib.bib20); Pathak et al.,
    [2017](https://arxiv.org/html/2410.02511v1#bib.bib121); Burda et al., [2018a](https://arxiv.org/html/2410.02511v1#bib.bib19);
    Bellemare et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib13)). Despite
    their success, they may induce notable redundant exploration due to a lack of
    task-relevant guidance (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)).
    The exponential expansion of the state-action spaces renders exhaustive exploration
    impractical in multi-agent settings. Consequently, efficient multi-agent exploration (MAE)
    becomes increasingly imperative and necessary (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56);
    Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). MAE is also
    challenging due to the complex configurations, e.g., the entangled effect of multi-agent
    actions and intricate reward design (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90);
    Qu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib126); Xu et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib180)). Given our emphasis on
    efficient exploration, we prioritize evaluation in multi-agent settings. Some
    MAE methods encourage influential behaviors during agent interactions (Liu et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Jaques et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib55);
    Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)). Nevertheless,
    they may lead to unintended coalitions or require additional priors (Liu et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)). Certain studies leverage
    subgoals to guide exploration (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)).
    However, due to challenges in integrating task-related information into subgoals,
    they either necessitate human expertise for subgoals design (Tang et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib155);
    Kulkarni et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib70)) or struggle
    to identify useful subgoals (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56);
    Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)). Distinguished
    from the above, this work underscores the significance of task-relevant guidance
    in exploration and utilizes the key state priors extracted from LLM to enable
    efficient multi-agent exploration.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 探索效率长期以来一直是强化学习（RL）中的一个焦点问题（Thrun，[1992](https://arxiv.org/html/2410.02511v1#bib.bib156)；Cai
    等，[2020](https://arxiv.org/html/2410.02511v1#bib.bib21)；Seo 等，[2021](https://arxiv.org/html/2410.02511v1#bib.bib137)；Mahajan
    等，[2019](https://arxiv.org/html/2410.02511v1#bib.bib100)；Jeon 等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)；Liu
    等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）。典型的探索方法侧重于随机探索（Mnih
    等，[2013](https://arxiv.org/html/2410.02511v1#bib.bib109)；Rashid 等，[2018](https://arxiv.org/html/2410.02511v1#bib.bib128)）或启发式指标，如多样性或新颖性，以促进全面探索，特别是在单一智能体探索中（Linke
    等，[2020](https://arxiv.org/html/2410.02511v1#bib.bib88)；Burda 等，[2018b](https://arxiv.org/html/2410.02511v1#bib.bib20)；Pathak
    等，[2017](https://arxiv.org/html/2410.02511v1#bib.bib121)；Burda 等，[2018a](https://arxiv.org/html/2410.02511v1#bib.bib19)；Bellemare
    等，[2016](https://arxiv.org/html/2410.02511v1#bib.bib13)）。尽管它们取得了成功，但由于缺乏任务相关的指导，它们可能会导致显著的冗余探索（Du
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）。状态-动作空间的指数扩展使得在多智能体环境中进行全面探索变得不切实际。因此，高效的多智能体探索（MAE）变得越来越重要和必要（Jeon
    等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)；Liu 等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）。由于多智能体动作的交织效应和复杂的奖励设计（Liu
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)；Qu 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib126)；Xu
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib180)），MAE 也具有挑战性。鉴于我们对高效探索的重视，我们优先考虑在多智能体环境中的评估。一些
    MAE 方法在智能体交互中鼓励有影响力的行为（Liu 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)；Jaques
    等，[2019](https://arxiv.org/html/2410.02511v1#bib.bib55)；Wang 等，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)）。然而，它们可能导致意外的联盟，或需要额外的先验知识（Liu
    等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）。某些研究利用子目标来引导探索（Jeon 等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)）。然而，由于将任务相关信息集成到子目标中的挑战，它们要么需要人工设计子目标的专业知识（Tang
    等，[2018](https://arxiv.org/html/2410.02511v1#bib.bib155)；Kulkarni 等，[2016](https://arxiv.org/html/2410.02511v1#bib.bib70)），要么难以识别有用的子目标（Jeon
    等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)；Liu 等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）。与上述方法不同，本研究强调了任务相关指导在探索中的重要性，并利用从大语言模型（LLM）中提取的关键状态先验来实现高效的多智能体探索。
- en: 4 Method
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: '![Refer to caption](img/825b9c53b801799370d6c95a343b6acd.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/825b9c53b801799370d6c95a343b6acd.png)'
- en: 'Figure 2: Overview of the training process. (a) Key States Localization with
    LLM: We devise a set of prompts to guide LLM in localizing key states based on
    task-specific information. Refinements of the response are achieved through iterative
    self-checks by LLM. Subsequently, discriminator functions are derived from the
    final response to discriminate key states within trajectories. (b) Key States-Guided
    Exploration: Using the achieved key states chain within the processed trajectory,
    we look up KSMT to get the most probable next key states. By sampling from them
    as the subgoal for the concluding sub-trajectory, we integrate intrinsic rewards
    into the overall trajectory using SHIR.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：训练过程概览。(a) 基于 LLM 的关键状态定位：我们设计了一组提示，引导 LLM 根据任务特定信息来定位关键状态。通过 LLM 的迭代自检，优化响应。随后，从最终响应中推导出判别函数，用于区分轨迹中的关键状态。(b)
    基于关键状态的探索引导：利用在处理过的轨迹中得到的关键状态链，我们查找 KSMT 以获取最可能的下一个关键状态。通过从中采样作为结束子轨迹的子目标，我们使用
    SHIR 将内在奖励整合到整体轨迹中。
- en: 'This section elaborates on the developed LEMAE. The concept of the key states
    is first induced as the task-relevant guidance in Sec. [4.1](https://arxiv.org/html/2410.02511v1#S4.SS1
    "4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration"). Centering around the key states,
    we construct two components: (i) key states localization with LLM (Sec. [4.2](https://arxiv.org/html/2410.02511v1#S4.SS2
    "4.2 Key States Localization with LLM ‣ 4 Method ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")) and (ii) key state-guided
    exploration (Sec. [4.3](https://arxiv.org/html/2410.02511v1#S4.SS3 "4.3 Key State-Guided
    Exploration ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")). The former directs LLM to generate discriminator
    functions for localizing key states in rollout trajectories, while the latter
    guides and organizes exploration with the introduced Subspace-based Hindsight
    Intrinsic Reward and Key States Memory Tree. Please refer to Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2
    "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration") and Algorithm [1](https://arxiv.org/html/2410.02511v1#algorithm1
    "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration") for details. Also, we provide a [demonstration](https://sites.google.com/view/lemae)¹¹1[https://sites.google.com/view/lemae](https://sites.google.com/view/lemae)
    to clarity the LEMAE’s execution pipeline.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了已开发的 LEMAE。首先在[4.1](https://arxiv.org/html/2410.02511v1#S4.SS1 "4.1 关键状态中的魔鬼
    ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")节中引入了任务相关的关键状态概念作为指导。围绕关键状态，我们构建了两个组件：(i) 基于
    LLM 的关键状态定位（[4.2](https://arxiv.org/html/2410.02511v1#S4.SS2 "4.2 基于 LLM 的关键状态定位
    ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")节）和 (ii) 基于关键状态的探索引导（[4.3](https://arxiv.org/html/2410.02511v1#S4.SS3
    "4.3 基于关键状态的探索引导 ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")节）。前者引导 LLM 生成判别函数，以便在展开的轨迹中定位关键状态，而后者则通过引入基于子空间的事后内在奖励和关键状态记忆树来引导和组织探索。详细内容请参见图[2](https://arxiv.org/html/2410.02511v1#S4.F2
    "图 2 ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")和算法[1](https://arxiv.org/html/2410.02511v1#algorithm1
    "附录 A 中的算法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")。此外，我们还提供了一个[演示](https://sites.google.com/view/lemae)¹¹1[https://sites.google.com/view/lemae](https://sites.google.com/view/lemae)，以清晰展示
    LEMAE 的执行流程。
- en: 4.1 Devil is in the Key States
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 关键状态中的魔鬼
- en: 'Previous methods suffer from redundant exploration efforts in pursuing task-agnostic
    novelty (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)), potentially
    reducing training efficiency. This motivates us to integrate task-relevant information
    as a better guidance choice for efficient exploration. Nevertheless, practical
    proposals are limited in the field. This work identifies the Key States as the
    novel task-relevant prior, which corresponds to intermediate states with explicit
    semantics and expressions pertaining to the task. Meanwhile, Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1
    "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") explicitly
    reflects the efficacy of incorporating them.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '之前的方法在追求任务无关的新颖性时，存在冗余的探索努力（Du 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)），这可能降低训练效率。这激励我们将与任务相关的信息整合为更好的引导选择，以实现高效的探索。然而，实际的提议在这一领域仍然有限。本文将关键状态定义为新颖的任务相关先验，它对应于与任务相关的具有明确语义和表达的中间状态。同时，命题[4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1
    "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")明确反映了将其纳入的有效性。'
- en: Proposition 4.1.
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.1.
- en: Consider the one-dimensional asymmetric random walk problem, where an agent
    starts at $x=0$ and aims to reach $x=N\in\mathbb{N^{+}},N>1$. The initial policy
    is asymmetric and random with probabilities $p\in(0.5,1)$ and $1-p$ for right
    and left movements, respectively. Without prior knowledge, the expected first
    hitting time is $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$. After introducing
    the task-relevant information that the agent must first reach key states $\kappa=1,...,N-1$
    before reaching $x=N$, we can decrease the expected first hitting time by $\mathbb{E}(T_{0\rightarrow
    N})-\mathbb{E}(T^{prior}_{0\rightarrow N})=(N-1)*(% \frac{1}{2p-1}-\frac{2}{p}+1)>0$.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一维非对称随机行走问题，其中智能体从$x=0$开始，目标是到达$x=N\in\mathbb{N^{+}}，N>1$。初始策略是不对称的，智能体向右和向左移动的概率分别为$p\in(0.5,1)$和$1-p$。在没有先验知识的情况下，期望的首次到达时间为$\mathbb{E}(T_{0\rightarrow
    N})=\frac{N}{2p-1}$。引入任务相关信息后，智能体必须首先到达关键状态$\kappa=1,...,N-1$，然后才能到达$x=N$，我们可以通过$\mathbb{E}(T_{0\rightarrow
    N})-\mathbb{E}(T^{prior}_{0\rightarrow N})=(N-1)*(\frac{1}{2p-1}-\frac{2}{p}+1)>0$来减少期望首次到达时间。
- en: 'The proof is deferred to Appendix [C](https://arxiv.org/html/2410.02511v1#A3
    "Appendix C Proof of Proposition 4.1 ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration"). The exploration policy substantially
    benefits from the involvement of key states, e.g., $\mathbb{E}(T_{0\rightarrow
    N})-\mathbb{E}(T^{prior}_{0\rightarrow N})\to\infty$ with $p\to 0.5$. Such a concept
    is also commonly seen in practical scenarios, such as in-game checkpoints (Demaine
    et al., [2016](https://arxiv.org/html/2410.02511v1#bib.bib31)) and landmarks in
    navigation (Becker et al., [1995](https://arxiv.org/html/2410.02511v1#bib.bib11)).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '证明推迟到附录[C](https://arxiv.org/html/2410.02511v1#A3 "Appendix C Proof of Proposition
    4.1 ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration")。探索策略在关键状态的参与下获得了显著的益处，例如，当$p\to 0.5$时，$\mathbb{E}(T_{0\rightarrow
    N})-\mathbb{E}(T^{prior}_{0\rightarrow N})\to\infty$。这一概念在实际场景中也常见，例如游戏中的检查点（Demaine
    等，[2016](https://arxiv.org/html/2410.02511v1#bib.bib31)）和导航中的地标（Becker 等，[1995](https://arxiv.org/html/2410.02511v1#bib.bib11)）。'
- en: 4.2 Key States Localization with LLM
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 使用LLM进行关键状态定位
- en: 'To reduce manual workload, we employ LLM to localize key states. Although generating
    the aforementioned symbolic key states can be straightforward, LLM’s weakness
    in comprehending symbolic states or environment details necessitates additional
    information in certain tasks and can lead to errors and hallucinations that are
    difficult to detect. Here, we stress the importance of LLM’s discriminative ability
    to localize key states in rollout trajectories to better leverage LLM’s general
    knowledge. The rationale is that discrimination demands only a high-level task
    understanding and is more reliable and universal than naive generation, as discussed
    in detail in Appendix [B.1](https://arxiv.org/html/2410.02511v1#A2.SS1 "B.1 The
    Insights Behind Key States Discrimination ‣ Appendix B Further Discussions ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '为了减少人工工作量，我们使用LLM来定位关键状态。尽管生成上述符号关键状态可能很直接，但LLM在理解符号状态或环境细节方面的弱点，在某些任务中需要额外的信息，这可能导致难以发现的错误和幻觉。在这里，我们强调LLM在回滚轨迹中定位关键状态的判别能力的重要性，以更好地利用LLM的通用知识。其原理在于，判别只需对任务有较高层次的理解，相比天真的生成方法，它更加可靠和普遍，详细讨论请参见附录[B.1](https://arxiv.org/html/2410.02511v1#A2.SS1
    "B.1 The Insights Behind Key States Discrimination ‣ Appendix B Further Discussions
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")。'
- en: 'To discriminate key states, we prompt LLM to generate $m$ discriminator functions
    $\{\mathcal{F}_{i}\}_{i=1}^{m}$, as depicted in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2
    "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration"). Each discriminator function $\mathcal{F}_{i}$ takes
    in the state $s_{t}$ at timestep $t$ and outputs a boolean value to tell whether
    the input state is the key state $\kappa_{i}$. Such an approach systematically
    annotates each state in trajectories as a key state instance or not. Notably,
    LEMAE injects task-relevant information into the symbolic states without predefined
    components such as observation captioners (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35))
    or environment codes (Xie et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib179)),
    which require manual fine-tuning, may be unavailable in many scenarios, or could
    introduce extra information. In addition, the discriminator functions’ reusability
    avoids frequent calls, and our method empirically requires fewer than three LLM
    inferences for a specific task. These advantages highlight the potential of LEMAE
    to expand the scope of application scenarios with fewer constraints and reduced
    costs.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了区分关键状态，我们提示LLM生成$m$个判别函数$\{\mathcal{F}_{i}\}_{i=1}^{m}$，如图[2](https://arxiv.org/html/2410.02511v1#S4.F2
    "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration")所示。每个判别函数$\mathcal{F}_{i}$接收时间步$t$上的状态$s_{t}$，并输出一个布尔值，表示输入状态是否为关键状态$\kappa_{i}$。这种方法系统地将轨迹中的每个状态标注为关键状态实例或非关键状态。值得注意的是，LEMAE在符号状态中注入了任务相关的信息，而没有使用诸如观察描述符(Du
    et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35))或环境编码(Xie et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib179))等预定义组件，这些组件需要手动微调，可能在许多场景中无法使用，或者可能引入额外的信息。此外，判别函数的可重用性避免了频繁调用，并且我们的研究方法在特定任务中经验性地只需要少于三个LLM推理。以上优势凸显了LEMAE在减少约束和成本的情况下扩展应用场景的潜力。'
- en: 'We design prompts to alleviate the burden of labor-intensive prompt engineering
    across tasks. As illustrated in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2
    "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration"), each task’s prompt is structured by a standardized
    prompt template and task information. The prompt template, consistent across tasks,
    primarily contains several role instructions to guide LLM in role understandings (including
    promoting labor division among agents in MARL) and output constraints. For a new
    task with symbolic state space, the prompt template requires only essential details,
    i.e., the task description and the state form, which can be easily extracted from
    the task document without additional processing, making it less demanding than
    previous methods (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98);
    Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)). An extension
    to vision-based tasks is described in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了提示语，旨在减轻跨任务的劳动密集型提示工程的负担。如图[2](https://arxiv.org/html/2410.02511v1#S4.F2
    "图 2 ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")所示，每个任务的提示语由标准化的提示模板和任务信息构成。该提示模板在各个任务中保持一致，主要包含几项角色指令，用以引导LLM理解角色（包括促进MARL中的智能体间分工）和输出约束。对于具有符号状态空间的新任务，提示模板仅要求提供必要的细节，即任务描述和状态形式，这些可以从任务文档中轻松提取，无需额外处理，相较于以往的方法，要求更低（Ma
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib98); Du 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）。关于面向视觉的任务的扩展，详见附录[F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 超越符号任务的 LEMAE 扩展 ‣ 附录 F 额外实验结果 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")。
- en: 'Considering that LLM sometimes generates inaccurate responses and non-executable
    codes, we develop a Self-Check mechanism to enable LLM’s autonomous evaluation
    and response improvement, which is inspired by recent approaches (Shinn et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib141); Dhuliawala et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib32)).
    The mechanism comprises two checking operations: LLM rethinking and code verification.
    The former prompts LLM with a set of queries for self-assessment, ensuring compliance
    with specified criteria. The latter verifies the executability of discriminator
    functions with actual state inputs, providing feedback until all functions are
    executable.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到LLM有时会生成不准确的响应和无法执行的代码，我们开发了一种自检机制，使LLM能够进行自我评估并改进响应，该机制灵感来源于近期的方法（Shinn
    等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib141); Dhuliawala 等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib32)）。该机制包括两个检查操作：LLM
    反思和代码验证。前者通过一组查询提示LLM进行自我评估，确保符合指定标准；后者则通过实际状态输入验证判别函数的可执行性，并提供反馈，直到所有函数都可执行。
- en: 'We use GPT-4-turbo from OpenAI API and prompt details are attached in Appendix [D](https://arxiv.org/html/2410.02511v1#A4
    "Appendix D LLM Prompts and Responses ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自OpenAI API的GPT-4-turbo，提示详情见附录[D](https://arxiv.org/html/2410.02511v1#A4
    "附录 D LLM 提示与响应 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")。
- en: 4.3 Key State-Guided Exploration
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 关键状态引导探索
- en: 4.3.1 Subspace-based Hindsight Intrinsic Reward
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 基于子空间的事后内在奖励
- en: 'With the annotated key states, trajectories can naturally be segmented into
    sub-trajectories. Drawing inspiration from Andrychowicz et al. ([2017](https://arxiv.org/html/2410.02511v1#bib.bib8)),
    we integrate hindsight intrinsic rewards by conceptualizing the annotated key
    states as sub-trajectories’ subgoals, which is further discussed in Appendix [E.3](https://arxiv.org/html/2410.02511v1#A5.SS3
    "E.3 Connection and Comparison with HER ‣ Appendix E Implementation Details ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").
    Such integration guides the policy toward achieving these key states by increasing
    reward density, thus reducing manual reward design burdens. Moreover, the state
    vector index from the discriminator function constitutes the reward-related subspace
    of the state (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)).
    Here, we write the Subspace-based Hindsight Intrinsic Reward (SHIR) function as:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '通过标注的关键状态，轨迹可以自然地分割成子轨迹。受到Andrychowicz等人（[2017](https://arxiv.org/html/2410.02511v1#bib.bib8)）的启发，我们通过将标注的关键状态概念化为子轨迹的子目标，整合了事后内在奖励，具体内容详见附录[E.3](https://arxiv.org/html/2410.02511v1#A5.SS3
    "E.3 Connection and Comparison with HER ‣ Appendix E Implementation Details ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")。这种整合通过增加奖励密度，引导策略实现这些关键状态，从而减少了手动奖励设计的负担。此外，来自判别函数的状态向量索引构成了与奖励相关的状态子空间（Liu等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）。在这里，我们将基于子空间的事后内在奖励（Subspace-based
    Hindsight Intrinsic Reward，SHIR）函数表示为：'
- en: '|  | $r_{I}^{\kappa_{m}}(t)=\&#124;\Phi_{m}(s_{t})-\Phi_{m}(\kappa_{m})\&#124;-\&#124;\Phi_{m}(s_{t%
    +1})-\Phi_{m}(\kappa_{m})\&#124;,$ |  | (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{I}^{\kappa_{m}}(t)=\&#124;\Phi_{m}(s_{t})-\Phi_{m}(\kappa_{m})\&#124;-\&#124;\Phi_{m}(s_{t%
    +1})-\Phi_{m}(\kappa_{m})\&#124;,$ |  | (1) |'
- en: where $\|\cdot\|$ denotes a distance metric, e.g., Manhattan Distance; $\Phi_{m}(s)=(s_{e})_{e\in\upsilon_{m}}$
    restricts the state space to elements $e\in\upsilon_{m}$, $s_{e}$ is the $e$-th
    element of the full-state $s$, and $\upsilon_{m}\subset\mathbb{N}^{+}$ refers
    to the subset of entire state space from the discriminator function $\mathcal{F}_{m}$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\|\cdot\|$表示距离度量，例如曼哈顿距离；$\Phi_{m}(s)=(s_{e})_{e\in\upsilon_{m}}$将状态空间限制为元素$e\in\upsilon_{m}$，$s_{e}$是完整状态$s$的第$e$个元素，而$\upsilon_{m}\subset\mathbb{N}^{+}$指的是判别函数$\mathcal{F}_{m}$从整个状态空间中提取的子集。
- en: 'Given that rewards generally rely on a limited subset of the entire state space (Liu
    et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Todorov et al.,
    [2012](https://arxiv.org/html/2410.02511v1#bib.bib158)), adopting subspace-based
    rewards helps avoid the potential redundancy and bias associated with the design
    of intrinsic rewards in the entire state space. LEMAE is also applicable to scenarios
    where rewards depend on the global state space, as it imposes no strict constraints.
    Hence, the final reward function is further derived as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于奖励通常依赖于整个状态空间的有限子集（Liu等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92);
    Todorov等，[2012](https://arxiv.org/html/2410.02511v1#bib.bib158)），采用基于子空间的奖励有助于避免在整个状态空间中设计内在奖励时可能出现的冗余和偏差。LEMAE同样适用于奖励依赖于全局状态空间的场景，因为它没有强制性约束。因此，最终的奖励函数进一步推导为：
- en: '|  | $r(t)=\alpha\cdot r_{E}(t)+\beta\cdot r_{I}^{\kappa_{m}}(t),$ |  | (2)
    |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $r(t)=\alpha\cdot r_{E}(t)+\beta\cdot r_{I}^{\kappa_{m}}(t),$ |  | (2)
    |'
- en: where $r_{E}$ denotes the extrinsic reward with $\alpha,\beta\in\mathbb{R^{+}}$
    non-negative scaling factors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$r_{E}$表示外部奖励，$\alpha,\beta\in\mathbb{R^{+}}$是非负缩放因子。
- en: 4.3.2 Key States Memory Tree
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 关键状态记忆树
- en: To organize exploration with memory, we introduce the concept of Key States
    Memory Tree (KSMT). It tracks transitions between key states and further serves
    exploration and planning. Compared with the naive $\epsilon$-greedy method, gradually
    revealing the KSMT helps avoid redundant exploration throughout the state space,
    particularly beneficial in more complicated real-world scenarios. Notably, LEMAE
    is compatible with other memory structures, such as Directed Acyclic Graphs.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了组织具有记忆的探索，我们引入了关键状态记忆树（Key States Memory Tree，KSMT）的概念。它跟踪关键状态之间的过渡，并进一步服务于探索和规划。与简单的$\epsilon$-贪婪方法相比，逐步揭示KSMT有助于避免在整个状态空间中进行冗余探索，尤其在更复杂的现实场景中更为有益。值得注意的是，LEMAE与其他记忆结构兼容，例如有向无环图（DAG）。
- en: 'Construct KSMT: Initialized at the root node, KSMT dynamically expands by iteratively
    incorporating key state chains obtained from annotated trajectories, as outlined
    in Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A
    Algorithm ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration"). These steps repeat until either reaching the success state or fully
    depicting the transitions between key states.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 构建 KSMT：从根节点初始化，KSMT 通过迭代地将从标注轨迹中获得的关键状态链动态扩展，如算法 [2](https://arxiv.org/html/2410.02511v1#algorithm2
    "附录 A 算法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索") 中所述。这些步骤会重复进行，直到达到成功状态或完全描绘关键状态之间的转移。
- en: 'Explore with KSMT: To discover new KSMT branches, we adopt an exploration strategy
    that balances high-randomness policy $\bm{\pi}_{\theta}^{\epsilon_{h}}$ for exploring
    under-explored nodes with low-randomness policy $\bm{\pi}_{\theta}^{\epsilon_{l}}$
    to minimize interference with policy learning, as shown in Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2
    "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration"). Upon reaching a leaf node, agents execute
    $\bm{\pi}_{\theta}^{\epsilon_{h}}$ to deepen KSMT. While reaching a non-leaf node
    $\xi_{i}$, the agents take $\bm{\pi}_{\theta}^{\epsilon_{h}}$ with probability
    $p_{i}$ to expand the breadth or $\bm{\pi}_{\theta}^{\epsilon_{l}}$ with probability
    $1-p_{i}$ for progression towards the next key state. The probability $p_{i}$
    is calculated as $p_{i}=\frac{1}{d_{i}+1}$, with $d_{i}$ the degree of the node
    $\xi_{i}$ as an indicator of the degree of under-exploration. The exploration
    phase completes upon the discovery of the success state. We also prune branches
    that do not lead to success to circumvent task-irrelevant key states. In this
    way, KSMT enables exploration in a more meaningful state subspace.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KSMT 进行探索：为了发现新的 KSMT 分支，我们采用了一种探索策略，平衡了高随机性策略 $\bm{\pi}_{\theta}^{\epsilon_{h}}$（用于探索未充分探索的节点）和低随机性策略
    $\bm{\pi}_{\theta}^{\epsilon_{l}}$（用于最小化对策略学习的干扰），如算法 [2](https://arxiv.org/html/2410.02511v1#algorithm2
    "附录 A 算法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索") 中所示。当到达叶节点时，智能体执行 $\bm{\pi}_{\theta}^{\epsilon_{h}}$
    以加深 KSMT。而当到达非叶节点 $\xi_{i}$ 时，智能体以概率 $p_{i}$ 执行 $\bm{\pi}_{\theta}^{\epsilon_{h}}$
    来扩展广度，或以概率 $1-p_{i}$ 执行 $\bm{\pi}_{\theta}^{\epsilon_{l}}$ 以推进到下一个关键状态。概率 $p_{i}$
    计算为 $p_{i}=\frac{1}{d_{i}+1}$，其中 $d_{i}$ 是节点 $\xi_{i}$ 的度数，作为未充分探索的程度的指标。探索阶段在发现成功状态后完成。我们还会剪枝那些不会导致成功的分支，以避免任务无关的关键状态。通过这种方式，KSMT
    实现了在更有意义的状态子空间中的探索。
- en: 'Plan with KSMT: Since KSMT acts as a dynamic model within the key state space,
    we plan the subgoal for the final sub-trajectory based on it. As shown in Fig. [2](https://arxiv.org/html/2410.02511v1#S4.F2
    "Figure 2 ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration")b, given the achieved key states chain, we identify the
    corresponding branch ($\kappa_{2}\rightarrow\kappa_{1}\rightarrow\text{children}$)
    in KSMT through a lookup operation. Since they have been validated by memory,
    the children represent the most likely next key states, from which we randomly
    sample the final subgoal. This process mainly handles cases where trajectories
    fail to reach a key state as the final subgoal. It enhances SHIR and improves
    the efficacy of exploring KSMT by encouraging agents to access existing key states.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KSMT 进行规划：由于 KSMT 作为关键状态空间中的动态模型，我们基于它为最终子轨迹规划子目标。如图 [2](https://arxiv.org/html/2410.02511v1#S4.F2
    "图 2 ‣ 4 方法 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")b 所示，给定已实现的关键状态链，我们通过查找操作在 KSMT 中识别相应的分支（$\kappa_{2}\rightarrow\kappa_{1}\rightarrow\text{children}$）。由于这些状态已经通过记忆验证，它们的子节点代表最可能的下一个关键状态，从中我们随机采样最终的子目标。这个过程主要处理轨迹未能将关键状态作为最终子目标时的情况。它增强了
    SHIR，并通过鼓励智能体访问现有的关键状态，提高了探索 KSMT 的效率。
- en: 5 Experiments
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: '![Refer to caption](img/0f6ac5d4680bff46aad4e78cf48aaf6e.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![请参考图注](img/0f6ac5d4680bff46aad4e78cf48aaf6e.png)'
- en: 'Figure 3: Evaluating LEMAE against baseline methods on four MPE maps with sparse
    rewards, using test win rate as the evaluation metric. The acceleration rate refers
    to how much faster LEMAE finds the success state compared to CMAE.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在四个稀疏奖励的 MPE 地图上评估 LEMAE 与基线方法的表现，使用测试胜率作为评估指标。加速率是指 LEMAE 找到成功状态的速度相比于
    CMAE 更快的程度。
- en: 'We conduct experiments on commonly used multi-agent exploration benchmarks:
    (1) the Multiple-Particle Environment (Lowe et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib97);
    Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)) and (2)
    the StarCraft Multi-Agent Challenge (Samvelyan et al., [2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)).
    Following previous studies (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98);
    Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Xu et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib180)), we focus primarily on
    tasks with symbolic state spaces and use the sparse reward version for all tasks
    without specific instructions.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在常用的多智能体探索基准上进行实验：（1）多粒子环境（Lowe等人，[2017](https://arxiv.org/html/2410.02511v1#bib.bib97)；Wang等人，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)）和（2）星际争霸多智能体挑战（Samvelyan等人，[2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)）。根据之前的研究（Ma等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib98)；Liu等人，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)；Xu等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib180)），我们主要关注具有符号状态空间的任务，并对所有没有特定指令的任务使用稀疏奖励版本。
- en: 'Baselines. We compare LEMAE with representative baselines: IPPO is a MARL algorithm
    which extends PPO (Schulman et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib136));
    QMIX (Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128)) is
    a widely adopted MARL baseline; EITI and EDTI (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169))
    employ the impact of interaction in coordinated agents’ behaviors; MAVEN (Mahajan
    et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100)) combine value-based
    and policy-based approaches through hierarchical control; CMAE (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92))
    learns cooperative exploration by selecting shared goals from multiple projected
    state space; RODE (Wang et al., [2020d](https://arxiv.org/html/2410.02511v1#bib.bib172))
    decomposes joint action spaces into role-based ones to enhance exploration; MASER (Jeon
    et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)) generates subgoals
    automatically for multiple agents from the experience replay buffer; LAIES (Liu
    et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)) addresses the lazy
    agents problem by mathematical definition and causal analysis. ELLM (Du et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)) employs LLM priors to guide
    vision-based exploration, using state captioners and semantic similarity-based
    rewards. LEMAE is implemented on IPPO in MPE and QMIX in SMAC, consistent with
    previous works (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169);
    Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Jeon et al.,
    [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)) to ensure fair comparisons.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基准对比。我们将LEMAE与几个代表性的基准算法进行比较：IPPO是一种MARL算法，它扩展了PPO（Schulman等人，[2017](https://arxiv.org/html/2410.02511v1#bib.bib136)）；QMIX（Rashid等人，[2018](https://arxiv.org/html/2410.02511v1#bib.bib128)）是一个广泛采用的MARL基准；EITI和EDTI（Wang等人，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)）考虑了交互在协调代理行为中的影响；MAVEN（Mahajan等人，[2019](https://arxiv.org/html/2410.02511v1#bib.bib100)）通过分层控制结合了基于价值和基于策略的方法；CMAE（Liu等人，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）通过从多个投影状态空间中选择共享目标来学习合作探索；RODE（Wang等人，[2020d](https://arxiv.org/html/2410.02511v1#bib.bib172)）将联合动作空间分解为基于角色的空间，以增强探索；MASER（Jeon等人，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)）从经验回放缓冲区自动生成多个代理的子目标；LAIES（Liu等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）通过数学定义和因果分析解决了懒代理问题；ELLM（Du等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）采用LLM先验来指导基于视觉的探索，使用状态描述符和基于语义相似性的奖励。LEMAE在MPE中基于IPPO实现，在SMAC中基于QMIX实现，与先前的工作（Wang等人，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)；Liu等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)；Jeon等人，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)）一致，以确保公平比较。
- en: 'We run each algorithm on five random seeds and report the mean performance
    with standard deviation. Further details can be referenced in Appendix [E](https://arxiv.org/html/2410.02511v1#A5
    "Appendix E Implementation Details ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在五个随机种子上运行每个算法，并报告均值和标准差。更多细节可以参考附录[E](https://arxiv.org/html/2410.02511v1#A5
    "附录E 实现细节 ‣ 选择比努力更重要：LLM使得多智能体探索更加高效")。
- en: 5.1 Multiple-Particle Environment (MPE)
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 多粒子环境（MPE）
- en: In MPE, we evaluate LEMAE on Pass, Secret-Room, Push-Box, and Large-Pass, which
    are commonly used multi-agent exploration tasks in previous works (Wang et al.,
    [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169); Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在MPE中，我们在Pass、Secret-Room、Push-Box和Large-Pass任务上评估了LEMAE，这些是之前研究中常用的多智能体探索任务(Wang等人，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169);
    Liu等人，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92))。
- en: 'LLM can effectively discriminate key states. To start with, we examine the
    efficacy of LLM in discriminating key states. On the Pass task, as shown in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")a, a room is divided by a wall, each half containing
    an invisible switch. Passage through the door is allowed only when an agent occupies
    a switch. Initially, in the left half-room, agents must cooperate to move to the
    right half-room. In Fig. [4(a)](https://arxiv.org/html/2410.02511v1#S5.F4.sf1
    "In Figure 4 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    LLM exhibits a precise understanding of the task and generates meaningful discriminator
    functions, demonstrating the feasibility of our approach based on the current
    LLM.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: LLM能够有效地区分关键状态。首先，我们检验LLM在区分关键状态方面的效果。在Pass任务中，如图[1](https://arxiv.org/html/2410.02511v1#S1.F1
    "图1 ‣ 1 引言 ‣ 选择比努力更重要：LLM使得多智能体探索更高效")所示，一个房间被墙分隔，两个半房间中各包含一个隐形开关。只有当一个智能体占据开关时，才能通过门。最初，在左半房间，智能体必须合作才能进入右半房间。在图[4(a)](https://arxiv.org/html/2410.02511v1#S5.F4.sf1
    "在图4 ‣ 5.1 多粒子环境（MPE） ‣ 5 实验 ‣ 选择比努力更重要：LLM使得多智能体探索更高效")中，LLM展示了对任务的精确理解并生成了有意义的判别函数，证明了基于当前LLM的方法的可行性。
- en: '![Refer to caption](img/d26a8f83c7b7f3b87a742cb3469729f3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d26a8f83c7b7f3b87a742cb3469729f3.png)'
- en: (a)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/c1cc1bd491d7dd5cf405f927e32c909d.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c1cc1bd491d7dd5cf405f927e32c909d.png)'
- en: (b)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/e223b87513c11a928b1f849f679368f2.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e223b87513c11a928b1f849f679368f2.png)'
- en: (c)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 4: (a) Key states discrimination functions generated on task Pass. (b)
    The map of Secret-Room with key states: $\kappa_{1}$ represents occupying the
    left switch to open all doors, while $\kappa_{2}$, $\kappa_{3}$, and $\kappa_{4}$
    represent exploring right rooms 1, 2, and 3, respectively. The directional arrows
    symbolize the transitional relationships within KSMT. (c) The key states number
    curve in Secret-Room shows that LEMAE can identify all key states and proficiently
    prune task-irrelevant ones.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4： (a) 在任务Pass上生成的关键状态判别函数。 (b) 带有关键状态的Secret-Room地图：$\kappa_{1}$表示占据左侧开关以打开所有门，而$\kappa_{2}$、$\kappa_{3}$和$\kappa_{4}$分别表示探索右侧房间1、2和3。方向箭头象征着KSMT中的转移关系。
    (c) Secret-Room中关键状态数量曲线表明LEMAE能够识别所有关键状态，并高效地修剪与任务无关的状态。
- en: 'LEMAE achieves superior performance. We investigate how LEMAE enhances exploration
    by comparing it with baselines, confirming the value of incorporating LLM priors.
    The training curves are depicted in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3
    "Figure 3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration"). The failure of commonly used baselines highlights
    the necessity and urgency for efficient exploration, while the superior performance
    of LEMAE underscores the effectiveness of augmenting RL with task-specific guidance
    from LLM. Specifically, the failure of EITI, EDTI, and LAIES may be attributed
    to the complexity of learning dynamics or the scarcity of external state changes
    in the tasks. While CMAE learns effective strategies for simple tasks, its redundant
    exploration hampers efficiency, rendering it inadequate for tasks with expansive
    exploration spaces, such as Large-Pass. Although it benefits from LLM priors,
    ELLM performs worse than LEMAE due to the weak guidance provided by semantic similarity-based
    rewards, not to mention its reliance on frequent LLM inference and a predefined
    state captioner. Furthermore, we compare LEMAE with traditional SOTA baseline
    CMAE using the metric of the number of exploration steps taken to find the success
    state. The results indicate a significant exploration acceleration rate, up to
    10x, underscoring LEMAE’s efficiency. The superior performance of our method can
    be attributed to the mitigating of redundant exploration by incorporating task-relevant
    information.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 'LEMAE实现了卓越的性能。我们通过将LEMAE与基准方法进行比较，探讨了它如何增强探索，确认了结合LLM先验的价值。训练曲线如图[3](https://arxiv.org/html/2410.02511v1#S5.F3
    "Figure 3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")所示。常用基准方法的失败突显了高效探索的必要性和紧迫性，而LEMAE的卓越表现则强调了通过LLM提供任务特定指导来增强强化学习的有效性。具体而言，EITI、EDTI和LAIES的失败可能归因于学习动态的复杂性或任务中外部状态变化的稀缺性。虽然CMAE能够为简单任务学习有效的策略，但其冗余的探索降低了效率，使其无法应对具有广泛探索空间的任务，如Large-Pass。尽管受益于LLM先验，ELLM的表现却不如LEMAE，原因在于基于语义相似性的奖励提供的指导较弱，更不用提它依赖于频繁的LLM推理和预定义的状态描述器。此外，我们使用探索步骤数这一指标，将LEMAE与传统的SOTA基准方法CMAE进行比较，结果表明LEMAE在探索加速方面取得了显著的提升，最高可达10倍，突显了其效率。我们方法的优越表现可以归因于通过融入与任务相关的信息来缓解冗余探索。'
- en: 'LEMAE benefits from LLM priors through discrimination. We evaluate HER (Andrychowicz
    et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib8)), which also employs
    hindsight intrinsic rewards but selects goals randomly from memory. HER’s poor
    performance emphasizes the critical role of incorporating LLM priors for localizing
    key states in achieving efficient exploration. To further support our claim about
    the superiority of LLM discrimination over generation, we evaluate Eureka-si,
    a single-iteration variant of Eureka (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)),
    which uses LLM to generate reward functions directly. While Eureka-si performs
    comparably to LEMAE in simple tasks, it struggles in most complex tasks with partial
    observability, indicating that LLM-based discrimination may offer a more general
    and effective integration of LLM. Notably, these two methods are not specifically
    designed for efficient exploration. Please refer to Appendix [E.2](https://arxiv.org/html/2410.02511v1#A5.SS2
    "E.2 Comparison with LLM Reward Design ‣ Appendix E Implementation Details ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    [E.3](https://arxiv.org/html/2410.02511v1#A5.SS3 "E.3 Connection and Comparison
    with HER ‣ Appendix E Implementation Details ‣ Choices are More Important than
    Efforts: LLM Enables Efficient Multi-Agent Exploration") for more details.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 'LEMAE通过区分受益于LLM先验。我们评估了HER（Andrychowicz等人，[2017](https://arxiv.org/html/2410.02511v1#bib.bib8)），该方法也使用事后内在奖励，但从记忆中随机选择目标。HER的较差表现强调了整合LLM先验在实现高效探索中的关键作用，尤其是在定位关键状态方面。为了进一步支持我们关于LLM区分优于生成的论点，我们评估了Eureka-si，这是Eureka（Ma等人，[2023](https://arxiv.org/html/2410.02511v1#bib.bib98)）的单次迭代变体，它直接利用LLM生成奖励函数。尽管Eureka-si在简单任务中与LEMAE表现相当，但在大多数具有部分可观察性的复杂任务中表现不佳，这表明基于LLM的区分可能提供一种更通用和有效的LLM整合方式。值得注意的是，这两种方法并非专门为高效探索而设计。更多细节请参见附录[E.2](https://arxiv.org/html/2410.02511v1#A5.SS2
    "E.2 Comparison with LLM Reward Design ‣ Appendix E Implementation Details ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")，[E.3](https://arxiv.org/html/2410.02511v1#A5.SS3
    "E.3 Connection and Comparison with HER ‣ Appendix E Implementation Details ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")。'
- en: 'LEMAE reduces redundant exploration. We further compare the exploration behavior
    of LEMAE with that of CMAE on the Pass task. The visitation maps, displayed in
    log scale, are depicted in Fig. [1](https://arxiv.org/html/2410.02511v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration"). The illustration reveals that LEMAE markedly
    avoids redundant exploration: agents trained with CMAE tend to excessively explore
    the left room, while the agents’ visitation area in LEMAE is notably concentrated
    around the success path. Furthermore, an organic division of labor among agents
    emerges, affirming the efficacy of encouraging labor division in prompt design.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 'LEMAE减少了冗余探索。我们进一步将LEMAE与CMAE在Pass任务中的探索行为进行比较。访视图（以对数刻度显示）如图[1](https://arxiv.org/html/2410.02511v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")所示。图示表明，LEMAE显著避免了冗余探索：使用CMAE训练的智能体往往过度探索左侧房间，而LEMAE中的智能体访视区域明显集中在成功路径周围。此外，智能体之间出现了自然的分工，进一步验证了在提示设计中鼓励劳动分工的有效性。'
- en: 'LEMAE circumvents task-irrelevant key states. Due to the incomplete information,
    LLM may discriminate task-irrelevant key states. For instance, in the Secret-Room
    task, three rooms are present on the right, but LLM is not informed about the
    real target room for fairness. In Fig. [4(b)](https://arxiv.org/html/2410.02511v1#S5.F4.sf2
    "In Figure 4 ‣ 5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    LLM discriminates two task-irrelevant key states, denoted as $\kappa_{2}$ and
    $\kappa_{4}$, which represent an exploration of the two irrelevant rooms, respectively.
    Fig. [4(c)](https://arxiv.org/html/2410.02511v1#S5.F4.sf3 "In Figure 4 ‣ 5.1 Multiple-Particle
    Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM
    Enables Efficient Multi-Agent Exploration") shows that the pruning mechanism after
    finding the success state in LEMAE makes it effective in circumventing task-irrelevant
    key states. A more detailed robustness analysis is provided in Sec. [5.5](https://arxiv.org/html/2410.02511v1#S5.SS5
    "5.5 Sensitivity & Robustness Analysis ‣ 5 Experiments ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: LEMAE 避开了与任务无关的关键状态。由于信息不完整，LLM 可能会区分与任务无关的关键状态。例如，在 Secret-Room 任务中，右侧有三个房间，但为了公平性，LLM
    并不知道真正的目标房间。在图 [4(b)](https://arxiv.org/html/2410.02511v1#S5.F4.sf2 "图 4 ‣ 5.1
    多粒子环境 (MPE) ‣ 5 实验 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索") 中，LLM 区分了两个与任务无关的关键状态，分别表示对两个无关房间的探索，记作
    $\kappa_{2}$ 和 $\kappa_{4}$。图 [4(c)](https://arxiv.org/html/2410.02511v1#S5.F4.sf3
    "图 4 ‣ 5.1 多粒子环境 (MPE) ‣ 5 实验 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索") 显示了在 LEMAE 中找到成功状态后的剪枝机制，证明其在避免与任务无关的关键状态方面的有效性。更详细的鲁棒性分析请参见第
    [5.5](https://arxiv.org/html/2410.02511v1#S5.SS5 "5.5 灵敏度与鲁棒性分析 ‣ 5 实验 ‣ 选择比努力更重要：LLM
    实现高效的多智能体探索") 节。
- en: '![Refer to caption](img/6465f1af5771ae7d7e4e532d65ceca45.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/6465f1af5771ae7d7e4e532d65ceca45.png)'
- en: 'Figure 5: Evaluating LEMAE on six SMAC maps with sparse rewards, using test
    win rate as the evaluation metric. Notably, QMIX-DR is QMIX with dense rewards
    in the original SMAC.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在六张具有稀疏奖励的 SMAC 地图上评估 LEMAE，使用测试胜率作为评估指标。值得注意的是，QMIX-DR 是原始 SMAC 中使用密集奖励的
    QMIX。
- en: 5.2 StarCraft Multi-Agent Challenge (SMAC)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 星际争霸多智能体挑战 (SMAC)
- en: SMAC is a widely-used challenging benchmark in MARL. In contrast to dense or
    semi-sparse reward versions used before, we employ fully sparse-reward tasks to
    emphasize exploration, rewarding agents only upon complete enemy elimination.
    In addition, to validate LEMAE across diverse scenarios, we conduct experiments
    on six maps with varied difficulty and agent numbers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: SMAC 是 MARL 中广泛使用的挑战性基准测试。与之前使用的密集或半稀疏奖励版本不同，我们采用完全稀疏奖励任务以强调探索，仅在敌人被完全消除后才对智能体给予奖励。此外，为了在不同场景中验证
    LEMAE 的表现，我们在六张具有不同难度和智能体数量的地图上进行了实验。
- en: 'In Fig. [5](https://arxiv.org/html/2410.02511v1#S5.F5 "Figure 5 ‣ 5.1 Multiple-Particle
    Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM
    Enables Efficient Multi-Agent Exploration"), LEMAE demonstrates superior performance
    over all baselines. Although baselines QMIX, MAVEN, CDS, and MASER excel in dense
    or semi-sparse reward settings, they struggle in fully sparse reward scenarios.
    CMAE shows partial efficacy in simpler tasks but fails in harder scenarios due
    to the lack of task-related information in curiosity-driven goal selection. LAIES
    is the only non-LLM baseline comparable to LEMAE. However, it requires handcrafted
    external state priors and still underperforms compared to LEMAE, especially on
    more challenging tasks. ELLM, benefiting from LLM priors, performs well on simpler
    tasks, but its effectiveness diminishes on harder ones, likely due to the instability
    and less reliable guidance of semantic similarity-based rewards. Notably, we add
    QMIX-DR, which augments QMIX with dense rewards in the original SMAC. Surprisingly,
    LEMAE demonstrates the potential to match or even surpass QMIX-DR, particularly
    in hard maps, shedding light on minimizing the manual workload in complex reward
    design in real-world scenarios. Given the complexity of the SMAC benchmark, the
    consistent superiority of LEMAE confirms its potential applicability in more complex
    real-world scenarios. We further evaluate LEMAE on SMACv2 (Ellis et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib39)),
    an enhanced version with more stochasticity, as detailed in Appendix [F.1](https://arxiv.org/html/2410.02511v1#A6.SS1
    "F.1 More Complicated Benchmark: SMACv2 ‣ Appendix F Additional Experimental Results
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[5](https://arxiv.org/html/2410.02511v1#S5.F5 "图5 ‣ 5.1 多粒子环境 (MPE) ‣ 5 实验
    ‣ 选择比努力更重要：LLM使得高效的多智能体探索成为可能")中，LEMAE展示了优于所有基准的性能。尽管基准算法QMIX、MAVEN、CDS和MASER在密集或半稀疏奖励环境中表现出色，但在完全稀疏奖励场景中表现不佳。CMAE在简单任务中表现部分有效，但由于缺乏任务相关信息，在更困难的场景中失败，特别是在好奇心驱动的目标选择中。LAIES是唯一一个可与LEMAE相媲美的非LLM基准。然而，它需要手工制作的外部状态先验，并且与LEMAE相比，仍然表现较差，尤其是在更具挑战性的任务中。ELLM得益于LLM先验，在简单任务中表现良好，但在较难任务中的效果下降，可能是由于基于语义相似度奖励的不稳定性和较差的引导。值得注意的是，我们新增了QMIX-DR，它通过在原始SMAC中增加密集奖励来增强QMIX。令人惊讶的是，LEMAE展示了与QMIX-DR相匹敌甚至超越它的潜力，特别是在难度较大的地图上，这为减少在复杂奖励设计中的手动工作量提供了新的思路，尤其是在现实世界的场景中。考虑到SMAC基准的复杂性，LEMAE的一贯优越性确认了其在更复杂的现实世界场景中的潜在适用性。我们进一步在SMACv2（Ellis等人，[2024](https://arxiv.org/html/2410.02511v1#bib.bib39)）上评估LEMAE，这是一个具有更高随机性的增强版，详细信息见附录[F.1](https://arxiv.org/html/2410.02511v1#A6.SS1
    "F.1 更复杂的基准：SMACv2 ‣ 附录F 其他实验结果 ‣ 选择比努力更重要：LLM使得高效的多智能体探索成为可能")。
- en: '![Refer to caption](img/dfeb4592d67fcf7574f3201d3d99dc11.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dfeb4592d67fcf7574f3201d3d99dc11.png)'
- en: (a)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6a22426428fa9a7984e678d499c0eb33.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6a22426428fa9a7984e678d499c0eb33.png)'
- en: (b)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 6: (a) Evaluations on baselines of well-known MARL algorithms, i.e.,
    QPLEX and VMIX. Notably, both QPLEX and VMIX exhibit complete failure unless integrated
    with our approach. (b) Ablation studies are conducted on two exemplary tasks from
    MPE and SMAC to assess the significance of KSMT and SHIR within LEMAE.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图6： (a) 对知名MARL算法基准的评估，即QPLEX和VMIX。值得注意的是，除非与我们的方法结合，否则QPLEX和VMIX完全失败。 (b) 在MPE和SMAC的两个典型任务上进行消融研究，以评估KSMT和SHIR在LEMAE中的重要性。
- en: 5.3 Compatiblility with Various Algorithms
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 与各种算法的兼容性
- en: 'LEMAE incorporates task-relevant guidance in the form of intrinsic rewards
    and is agnostic to RL algorithms. Sec. [5.1](https://arxiv.org/html/2410.02511v1#S5.SS1
    "5.1 Multiple-Particle Environment (MPE) ‣ 5 Experiments ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration") and [5.2](https://arxiv.org/html/2410.02511v1#S5.SS2
    "5.2 StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") have verified
    the compatibility through implementing on two distinct MARL algorithms: IPPO in
    MPE and QMIX in SMAC. To further substantiate this claim, we build our method
    on two widely-used MARL algorithms, namely QPLEX (Wang et al., [2020a](https://arxiv.org/html/2410.02511v1#bib.bib166))
    and VMIX (Su et al., [2021](https://arxiv.org/html/2410.02511v1#bib.bib149)),
    adopting a value-based and actor-critic methodology respectively. As illustrated
    in Fig. [6(a)](https://arxiv.org/html/2410.02511v1#S5.F6.sf1 "In Figure 6 ‣ 5.2
    StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration"), algorithms combined
    with LEMAE consistently improve performance, underscoring the potential of LEMAE
    to integrate with alternative algorithms across diverse fields in the future.
    Additionally, LEMAE is a versatile approach for efficient exploration, not limited
    to MARL. To validate this assertion, we conduct further evaluations of LEMAE in
    a single-agent variant of MPE, as demonstrated in Appendix [F.4](https://arxiv.org/html/2410.02511v1#A6.SS4
    "F.4 Experiments for Single-Agent setups ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration").'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: LEMAE通过内在奖励的形式将任务相关的指导融入其中，并且与强化学习（RL）算法无关。[5.1](https://arxiv.org/html/2410.02511v1#S5.SS1
    "5.1 多粒子环境（MPE） ‣ 5 实验 ‣ 选择比努力更重要：LLM促使高效的多智能体探索")和[5.2](https://arxiv.org/html/2410.02511v1#S5.SS2
    "5.2 星际争霸多智能体挑战（SMAC） ‣ 5 实验 ‣ 选择比努力更重要：LLM促使高效的多智能体探索")通过在两种不同的MARL算法上实现，验证了其兼容性：在MPE中使用IPPO，在SMAC中使用QMIX。为了进一步证实这一说法，我们将方法构建在两种广泛使用的MARL算法上，即QPLEX（Wang等人，[2020a](https://arxiv.org/html/2410.02511v1#bib.bib166)）和VMIX（Su等人，[2021](https://arxiv.org/html/2410.02511v1#bib.bib149)），分别采用基于值和演员-评论员的方法。如图[6(a)](https://arxiv.org/html/2410.02511v1#S5.F6.sf1
    "图6 ‣ 5.2 星际争霸多智能体挑战（SMAC） ‣ 5 实验 ‣ 选择比努力更重要：LLM促使高效的多智能体探索")所示，结合LEMAE的算法始终提高性能，凸显了LEMAE未来与其他算法在各个领域整合的潜力。此外，LEMAE是一种高效探索的多功能方法，不仅限于MARL。为了验证这一说法，我们在MPE的单智能体变体中进一步评估了LEMAE，具体见附录[F.4](https://arxiv.org/html/2410.02511v1#A6.SS4
    "F.4 单智能体设置的实验 ‣ 附录F 额外实验结果 ‣ 选择比努力更重要：LLM促使高效的多智能体探索")。
- en: 5.4 Ablation Studies
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究
- en: 'Role of SHIR and KSMT. We conduct an ablation study to assess the significance
    of KSMT and SHIR within LEMAE. We select two exemplary tasks from MPE and SMAC
    and report results in Fig. [6(b)](https://arxiv.org/html/2410.02511v1#S5.F6.sf2
    "In Figure 6 ‣ 5.2 StarCraft Multi-Agent Challenge (SMAC) ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").
    In SMAC, Base refers to QMIX, while in MPE, it denotes IPPO. Besides, SHIR represents
    subspace-based hindsight intrinsic reward, KSMTE signifies exploration with KSMT,
    KSMTP denotes planning with KSMT, and LEMAE encompasses Base+SHIR+KSMTE+KSMTP.
    As illustrated, the absence of SHIR or KSMT significantly deteriorates performance,
    revealing both components’ pivotal roles in achieving effective key state-guided
    exploration.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: SHIR和KSMT的作用。我们进行了一项消融研究，以评估KSMT和SHIR在LEMAE中的重要性。我们从MPE和SMAC中选择了两个典型任务，并在图[6(b)](https://arxiv.org/html/2410.02511v1#S5.F6.sf2
    "图6 ‣ 5.2 星际争霸多智能体挑战（SMAC） ‣ 5 实验 ‣ 选择比努力更重要：LLM促使高效的多智能体探索")中报告了结果。在SMAC中，Base指的是QMIX，而在MPE中，它表示IPPO。此外，SHIR代表基于子空间的事后内在奖励，KSMTE表示使用KSMT进行探索，KSMTP表示使用KSMT进行规划，LEMAE则包括Base+SHIR+KSMTE+KSMTP。如图所示，缺少SHIR或KSMT会显著降低性能，揭示了这两个组件在实现有效的关键状态引导探索中的重要作用。
- en: 'Role of Self-Check mechanism and LLMs. We conduct a comparative analysis between
    GPT-4-turbo and GPT-3.5-turbo regarding generating discriminator functions. Meanwhile,
    we investigate the performance of GPT-4-turbo without the Self-Check mechanism (GPT-4-turbo
    w/o). The Acceptance Rate ($r_{acc}$) denotes the proportion of seeds achieving
    over $80\%$ of the best performance after RL training, while the Execution Rate ($r_{exe}$)
    indicates the proportion of seeds for which all discriminator functions are executable.
    As depicted in Table [7](https://arxiv.org/html/2410.02511v1#S5.F7 "Figure 7 ‣
    5.4 Ablation Studies ‣ 5 Experiments ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration"), the results demonstrate that
    a powerful LLM with our Self-Check mechanism effectively ensures the high quality
    of key states, as evidenced by the code’s executability and the final performance.
    The scalability of LEMAE to LLM and our Self-Check mechanism promise that LEMAE
    can leverage more powerful LLMs in the future and be applied to more challenging
    real-world tasks safely and efficiently.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '自检机制和大语言模型（LLMs）的作用。我们对GPT-4-turbo和GPT-3.5-turbo在生成判别函数方面进行了比较分析。同时，我们还研究了没有自检机制的GPT-4-turbo（GPT-4-turbo
    w/o）的表现。接受率（$r_{acc}$）表示经过强化学习训练后，能够达到最佳表现超过$80\%$的种子的比例，而执行率（$r_{exe}$）则表示所有判别函数都能执行的种子比例。如表[7](https://arxiv.org/html/2410.02511v1#S5.F7
    "Figure 7 ‣ 5.4 Ablation Studies ‣ 5 Experiments ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")所示，结果表明，采用我们自检机制的强大LLM能够有效确保关键状态的高质量，正如代码的可执行性和最终表现所证明的那样。LEMAE对LLM和自检机制的可扩展性表明，LEMAE可以在未来利用更强大的LLM，并能够安全高效地应用于更具挑战性的现实任务。'
- en: 'Table 1: Ablation studies on Self-Check mechanism and LLMs. We compare the
    performance of two LLMs (GPT-4-turbo and GPT-3.5-turbo), recording the Acceptance
    Rate ($r_{acc}$) and Execution Rate ($r_{exe}$) in ten runs of the generated discriminator
    functions. w/o denotes the absence of our Self-Check mechanism.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：关于自检机制和大语言模型（LLMs）的消融研究。我们比较了两种LLM（GPT-4-turbo和GPT-3.5-turbo）的表现，记录了生成判别函数的接受率（$r_{acc}$）和执行率（$r_{exe}$）在十次运行中的表现。w/o表示没有使用我们的自检机制。
- en: '| $r_{acc}$ ($r_{exe}$) | GPT-4-turbo | GPT-4-turbo w/o | GPT-3.5-turbo |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $r_{acc}$ ($r_{exe}$) | GPT-4-turbo | GPT-4-turbo w/o | GPT-3.5-turbo |'
- en: '| Large-Pass | 1.0 (1.0) | 0.8 (1.0) | 0.7 (1.0) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Large-Pass | 1.0 (1.0) | 0.8 (1.0) | 0.7 (1.0) |'
- en: '| 2m_vs_1z | 1.0 (1.0) | 0.7 (1.0) | 0.6 (1.0) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 2m_vs_1z | 1.0 (1.0) | 0.7 (1.0) | 0.6 (1.0) |'
- en: '| 5m_vs_6m | 1.0 (1.0) | 0.9 (1.0) | 1.0 (1.0) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 5m_vs_6m | 1.0 (1.0) | 0.9 (1.0) | 1.0 (1.0) |'
- en: '| MMM2 | 0.8 (1.0) | 0.6 (0.7) | 0.0 (1.0) | ![Refer to caption](img/bc63e1e1e6d9022b792b3a3cabd9fcc5.png)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '| MMM2 | 0.8 (1.0) | 0.6 (0.7) | 0.0 (1.0) | ![请参考说明](img/bc63e1e1e6d9022b792b3a3cabd9fcc5.png)'
- en: 'Figure 7: Hyperparameter examination on reward scaling rate $\alpha$ and $\beta$.
    The x-axis represents the relative values with respect to the default parameters.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：关于奖励缩放率$\alpha$和$\beta$的超参数检验。x轴表示相对于默认参数的相对值。
- en: 5.5 Sensitivity & Robustness Analysis
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 敏感性与鲁棒性分析
- en: 'Sensitivity to Hyperparameters. We conduct experiments on the pivotal hyperparameters
    in LEMAE, i.e., reward scaling rates $\alpha$ and $\beta$. The x-axis represents
    the relative values with respect to the default ($\alpha=10$, $\beta=1$), encompassing
    evaluations for $\alpha\in\{1,5,10,50,100\}$ and $\beta\in\{0.1,0.5,1,5,10\}$.
    Fig. [7](https://arxiv.org/html/2410.02511v1#S5.F7 "Figure 7 ‣ 5.4 Ablation Studies
    ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration") illustrates that LEMAE is robust to these hyperparameters
    across a considerable range. Notably, excessive extrinsic reward scaling rate
    $\alpha$ or insufficient intrinsic reward scaling rate $\beta$ can cause performance
    degradation due to the abrupt alteration of the reward or the inadequate motivational
    impact. Additionally, we conduct an ablation study on mixed-randomness exploration
    in Appendix [F.3](https://arxiv.org/html/2410.02511v1#A6.SS3 "F.3 Ablation Studies
    on Mixed-Randomness Exploration ‣ Appendix F Additional Experimental Results ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对超参数的敏感性。我们对 LEMAE 中的关键超参数进行实验，即奖励缩放率 $\alpha$ 和 $\beta$。x 轴表示相对于默认值（$\alpha=10$，$\beta=1$）的相对值，涵盖了对
    $\alpha\in\{1,5,10,50,100\}$ 和 $\beta\in\{0.1,0.5,1,5,10\}$ 的评估。图 [7](https://arxiv.org/html/2410.02511v1#S5.F7
    "图 7 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 选择比努力更重要：LLM 使得多智能体高效探索成为可能") 显示了 LEMAE 对这些超参数在相当范围内的鲁棒性。特别是，过高的外部奖励缩放率
    $\alpha$ 或过低的内部奖励缩放率 $\beta$ 可能会由于奖励的突变或动机影响的不足导致性能下降。此外，我们还在附录 [F.3](https://arxiv.org/html/2410.02511v1#A6.SS3
    "F.3 混合随机性探索的消融研究 ‣ 附录 F 其他实验结果 ‣ 选择比努力更重要：LLM 使得多智能体高效探索成为可能") 进行了混合随机性探索的消融研究。
- en: Robustness to Perturbations in Key States. We conduct experiments to evaluate
    the robustness of LEMAE to perturbations in key states. Specifically, Reduction
    simulates the absence of key states by randomly clearing a certain percentage
    of key states. Distraction simulates the misidentification of common states as
    key states by randomly adding a certain portion of
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对关键状态扰动的鲁棒性。我们进行实验评估 LEMAE 对关键状态扰动的鲁棒性。具体来说，减少通过随机清除一定比例的关键状态来模拟缺失关键状态的情况。干扰通过随机添加一定比例的干扰状态来模拟误将常见状态识别为关键状态的情况。
- en: 'Table 2: Robustness analysis of LEMAE to perturbations in key states, whether
    randomly deleting key states (Reduction) or adding distracting states (Distraction).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：LEMAE 对关键状态扰动的鲁棒性分析，无论是随机删除关键状态（减少）还是添加干扰状态（干扰）。
- en: '|  |  | Reduction | Distraction |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 减少 | 干扰 |'
- en: '| Tasks | Default | 25% | 50% | 50% | 100% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 默认 | 25% | 50% | 50% | 100% |'
- en: '| 1c3s5z | 0.98$\pm$0.02 | 0.97$\pm$0.01 | 0.97$\pm$0.02 | 0.92$\pm$0.04 |
    0.89$\pm$0.05 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 1c3s5z | 0.98$\pm$0.02 | 0.97$\pm$0.01 | 0.97$\pm$0.02 | 0.92$\pm$0.04 |
    0.89$\pm$0.05 |'
- en: '| 3s_vs_5z | 0.83$\pm$0.07 | 0.80$\pm$0.18 | 0.57$\pm$0.28 | 0.80$\pm$0.11
    | 0.66$\pm$0.08 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 3s_vs_5z | 0.83$\pm$0.07 | 0.80$\pm$0.18 | 0.57$\pm$0.28 | 0.80$\pm$0.11
    | 0.66$\pm$0.08 |'
- en: '| MMM2 | 0.89$\pm$0.08 | 0.89$\pm$0.03 | 0.79$\pm$0.09 | 0.86$\pm$0.04 | 0.79$\pm$0.08
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| MMM2 | 0.89$\pm$0.08 | 0.89$\pm$0.03 | 0.79$\pm$0.09 | 0.86$\pm$0.04 | 0.79$\pm$0.08
    |'
- en: 'distracting states (encouraging a random state dimension to 0). The observed
    performance decrease with increasing perturbations in Table [2](https://arxiv.org/html/2410.02511v1#S5.T2
    "Table 2 ‣ 5.5 Sensitivity & Robustness Analysis ‣ 5 Experiments ‣ Choices are
    More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration") underscores
    the significance of key states’ quality. LEMAE exhibits notable robustness to
    perturbations, ensuring its reliability across diverse application scenarios,
    particularly in light of the limited capabilities of current LLMs.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 干扰状态（将一个随机状态维度设为 0）。表 [2](https://arxiv.org/html/2410.02511v1#S5.T2 "表 2 ‣ 5.5
    敏感性与鲁棒性分析 ‣ 5 实验 ‣ 选择比努力更重要：LLM 使得多智能体高效探索成为可能") 中观察到的随着扰动增加而导致的性能下降，强调了关键状态质量的重要性。LEMAE
    显示出对扰动的显著鲁棒性，确保其在多样化应用场景中的可靠性，特别是在当前 LLM 能力有限的情况下。
- en: 5.6 Scalability & Generalization Analysis
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 可扩展性与泛化分析
- en: '![Refer to caption](img/3aad021b494c9f0131b57baf962b72a3.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/3aad021b494c9f0131b57baf962b72a3.png)'
- en: 'Figure 8: (a) A brand new task, River, which LLM has never encountered before.
    (b) The training curves of LEMAE and baselines using the evaluation metric of
    test win rate.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：（a）LLM 从未遇到过的新任务，河流任务。（b）LEMAE 和基线模型在使用测试胜率评估指标下的训练曲线。
- en: 'To rule out the possibility that LEMAE’s success relies on LLM’s familiarity
    with the chosen tasks, we’ve handcrafted a brand new task, termed River, which
    LLM has never encountered before. The task is illustrated in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8
    "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a,
    where the objective is for Bob to help Alice, who is afraid of water, cross two
    rivers to reach the bottom-right corner. As shown in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8
    "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b,
    LEMAE outperforms the baselines, and this confirms LLM’s generalization capabilities
    to empower LEMAE’s effectiveness in promoting efficient exploration in diverse
    new tasks. Please refer to Appendix [E.4.3](https://arxiv.org/html/2410.02511v1#A5.SS4.SSS3
    "E.4.3 A Brand New Task: River ‣ E.4 Tasks ‣ Appendix E Implementation Details
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")
    for details on the task.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '为了排除LEMAE的成功依赖于LLM对所选任务的熟悉度的可能性，我们手工设计了一个全新的任务，命名为“河流”，这是LLM从未遇到过的任务。任务示意图见图[8](https://arxiv.org/html/2410.02511v1#S5.F8
    "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a，其中目标是让Bob帮助害怕水的Alice穿越两条河流，达到右下角。如图[8](https://arxiv.org/html/2410.02511v1#S5.F8
    "Figure 8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b所示，LEMAE的表现优于基准方法，这证明了LLM的泛化能力，能够赋能LEMAE，在不同的新任务中推动高效探索。有关该任务的详细信息，请参阅附录[E.4.3](https://arxiv.org/html/2410.02511v1#A5.SS4.SSS3
    "E.4.3 A Brand New Task: River ‣ E.4 Tasks ‣ Appendix E Implementation Details
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")。'
- en: 'Additionally, we extend LEMAE to a vision-based task, as described in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration"), demonstrating the scalability potential of LEMAE.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们将LEMAE扩展到基于视觉的任务，如附录[F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2
    Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")中所述，展示了LEMAE的可扩展性潜力。'
- en: 6 Conclusion
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'Summary of This Work: We present LEMAE, a novel framework that benefits multi-agent
    exploration with task-specific guidance from LLM. LEMAE executes the key states
    localization with LLM and enables the key state-guided exploration to improve
    sample efficiency. In this way, we can (i) build up connections between LLM and
    RL to ground linguistic knowledge into decision-making, (ii) reduce the manual
    workload in accessing knowledge and intensive inference calls from LLM, and (iii)
    significantly boost exploration efficiency through guided and organized exploration.
    Extensive experiments further examine the effectiveness of LEMAE in typical benchmarks.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究总结：我们提出了LEMAE，这是一个新的框架，通过LLM提供任务特定的指导，促进多智能体的探索。LEMAE通过LLM执行关键状态定位，并实现关键状态引导的探索，从而提高样本效率。通过这种方式，我们可以（i）建立LLM与强化学习之间的联系，将语言知识融入决策过程，（ii）减少从LLM获取知识和进行推理调用的人工工作量，并且（iii）通过引导和组织的探索显著提高探索效率。大量实验进一步验证了LEMAE在典型基准测试中的有效性。
- en: 'Limitations & Future Investigations: In developing LEMAE, we made efforts to
    compensate for the pitfalls of concurrent LLMs, e.g., careful preparation for
    prompt engineering and task-related prior provision to avoid the nuisances in
    LLM usages. All of these can be circumvented with the progress of LLM’s capability
    enhancement. This work paves the way for LLM-empowered RL to achieve the potential
    in complicated decision-making scenarios.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 限制与未来研究：在开发LEMAE的过程中，我们努力弥补当前大规模语言模型（LLM）存在的不足，例如，精心准备提示工程和任务相关的先验知识，以避免LLM使用中的不便。所有这些问题随着LLM能力的增强而可以得到解决。本研究为LLM赋能的强化学习在复杂决策场景中的潜力奠定了基础。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Abbeel & Ng (2004) Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via
    inverse reinforcement learning. In *Proceedings of the twenty-first international
    conference on Machine learning*, pp.  1, 2004.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abbeel & Ng (2004) Pieter Abbeel 和 Andrew Y Ng. 通过逆向强化学习进行学徒学习。发表于*第二十一届国际机器学习会议论文集*，第1页，2004年。
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
    Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal
    Anadkat, 等人。GPT-4技术报告。*arXiv预印本 arXiv:2303.08774*，2023。
- en: Ackermann et al. (2019) Johannes Ackermann, Volker Gabler, Takayuki Osa, and
    Masashi Sugiyama. Reducing overestimation bias in multi-agent domains using double
    centralized critics. *arXiv preprint arXiv:1910.01465*, 2019.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ackermann等（2019）Johannes Ackermann, Volker Gabler, Takayuki Osa, 和Masashi Sugiyama。使用双重集中式评论者减少多智能体领域中的高估偏差。*arXiv预印本
    arXiv:1910.01465*，2019。
- en: Agarwal et al. (2019) Akshat Agarwal, Sumit Kumar, and Katia Sycara. Learning
    transferable cooperative behavior in multi-agent teams. *arXiv preprint arXiv:1906.01202*,
    2019.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等（2019）Akshat Agarwal, Sumit Kumar, 和Katia Sycara。在多智能体团队中学习可转移的合作行为。*arXiv预印本
    arXiv:1906.01202*，2019。
- en: Agarwal et al. (2020) Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
    An optimistic perspective on offline reinforcement learning. In *International
    Conference on Machine Learning*, pp. 104–114\. PMLR, 2020.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal等（2020）Rishabh Agarwal, Dale Schuurmans, 和Mohammad Norouzi。关于离线强化学习的乐观视角。在*国际机器学习大会*，第104-114页。PMLR，2020。
- en: 'Ahn et al. (2022) Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar,
    Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan,
    Karol Hausman, et al. Do as i can, not as i say: Grounding language in robotic
    affordances. *arXiv preprint arXiv:2204.01691*, 2022.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahn等（2022）Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes,
    Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman,
    等人。做我能做的，而不是我说的：将语言与机器人能力结合。*arXiv预印本 arXiv:2204.01691*，2022。
- en: An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based
    offline reinforcement learning with diversified q-ensemble. *Advances in Neural
    Information Processing Systems*, 34, 2021.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An等（2021）Gaon An, Seungyong Moon, Jang-Hyun Kim, 和Hyun Oh Song。基于不确定性的离线强化学习与多样化的q-集成。*神经信息处理系统进展*，34，2021。
- en: Andrychowicz et al. (2017) Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas
    Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel,
    and Wojciech Zaremba. Hindsight experience replay. *Advances in neural information
    processing systems*, 30, 2017.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Andrychowicz等（2017）Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider,
    Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, 和Wojciech
    Zaremba。事后经验重放。*神经信息处理系统进展*，30，2017。
- en: Author (2021) N. N. Author. Suppressed for anonymity, 2021.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者（2021）N. N. 作者。为保持匿名而删除，2021。
- en: Baker et al. (2019) Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu,
    Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent
    autocurricula. *arXiv preprint arXiv:1909.07528*, 2019.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker等（2019）Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell,
    Bob McGrew, 和Igor Mordatch。从多智能体自学课程中涌现出的工具使用。*arXiv预印本 arXiv:1909.07528*，2019。
- en: Becker et al. (1995) Craig Becker, Joaquin Salas, Kentaro Tokusei, and J-C Latombe.
    Reliable navigation using landmarks. In *Proceedings of 1995 IEEE International
    conference on Robotics and Automation*, volume 1, pp.  401–406\. IEEE, 1995.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker等（1995）Craig Becker, Joaquin Salas, Kentaro Tokusei, 和J-C Latombe。使用地标进行可靠导航。在*1995年IEEE国际机器人与自动化大会论文集*，第1卷，第401-406页。IEEE，1995。
- en: Belghazi et al. (2018) Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar,
    Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information
    neural estimation. In *International conference on machine learning*, pp. 531–540\.
    PMLR, 2018.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Belghazi等（2018）Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil
    Ozair, Yoshua Bengio, Aaron Courville, 和Devon Hjelm。互信息神经估计。在*国际机器学习大会*，第531-540页。PMLR，2018。
- en: Bellemare et al. (2016) Marc Bellemare, Sriram Srinivasan, Georg Ostrovski,
    Tom Schaul, David Saxton, and Remi Munos. Unifying count-based exploration and
    intrinsic motivation. *Advances in neural information processing systems*, 29,
    2016.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bellemare等（2016）Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul,
    David Saxton, 和Remi Munos。统一基于计数的探索与内在动机。*神经信息处理系统进展*，29，2016。
- en: Berner et al. (2019) Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung,
    Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
    Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. *arXiv
    preprint arXiv:1912.06680*, 2019.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Berner等（2019）Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw
    Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse,
    等人。使用大规模深度强化学习玩Dota 2。*arXiv预印本 arXiv:1912.06680*，2019。
- en: Boss (1978) R Wayne Boss. The effects of leader absence on a confrontation team-building
    design. *The Journal of Applied Behavioral Science*, 14(4):469–478, 1978.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boss（1978）R Wayne Boss。领导者缺席对对抗团队建设设计的影响。*应用行为科学杂志*，14(4):469–478，1978年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown等人（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell等。语言模型是少量样本学习者。*神经信息处理系统进展*，33:1877–1901，2020年。
- en: Browne et al. (2012) Cameron B Browne, Edward Powley, Daniel Whitehouse, Simon M
    Lucas, Peter I Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon
    Samothrakis, and Simon Colton. A survey of monte carlo tree search methods. *IEEE
    Transactions on Computational Intelligence and AI in games*, 4(1):1–43, 2012.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Browne等人（2012）Cameron B Browne、Edward Powley、Daniel Whitehouse、Simon M Lucas、Peter
    I Cowling、Philipp Rohlfshagen、Stephen Tavener、Diego Perez、Spyridon Samothrakis和Simon
    Colton。蒙特卡洛树搜索方法调研。*IEEE计算智能与人工智能在游戏中的应用交易*，4(1):1–43，2012年。
- en: Buckman et al. (2020) Jacob Buckman, Carles Gelada, and Marc G Bellemare. The
    importance of pessimism in fixed-dataset policy optimization. *arXiv preprint
    arXiv:2009.06799*, 2020.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Buckman等人（2020）Jacob Buckman、Carles Gelada和Marc G Bellemare。悲观主义在固定数据集策略优化中的重要性。*arXiv预印本arXiv:2009.06799*，2020年。
- en: Burda et al. (2018a) Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey,
    Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning.
    *arXiv preprint arXiv:1808.04355*, 2018a.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burda等人（2018a）Yuri Burda、Harri Edwards、Deepak Pathak、Amos Storkey、Trevor Darrell和Alexei
    A Efros。好奇心驱动学习的大规模研究。*arXiv预印本arXiv:1808.04355*，2018a年。
- en: Burda et al. (2018b) Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
    Exploration by random network distillation. *arXiv preprint arXiv:1810.12894*,
    2018b.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Burda等人（2018b）Yuri Burda、Harrison Edwards、Amos Storkey和Oleg Klimov。通过随机网络蒸馏进行探索。*arXiv预印本arXiv:1810.12894*，2018b年。
- en: Cai et al. (2020) Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably
    efficient exploration in policy optimization. In *International Conference on
    Machine Learning*, pp. 1283–1294\. PMLR, 2020.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai等人（2020）Qi Cai、Zhuoran Yang、Chi Jin和Zhaoran Wang。可证明高效的策略优化探索。在*国际机器学习会议*，第1283–1294页。PMLR，2020年。
- en: 'Cao et al. (2024) Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu,
    Gaoqi Liang, Junhua Zhao, and Yun Li. Survey on large language model-enhanced
    reinforcement learning: Concept, taxonomy, and methods. *arXiv preprint arXiv:2404.00282*,
    2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao等人（2024）Yuji Cao、Huan Zhao、Yuheng Cheng、Ting Shu、Guolong Liu、Gaoqi Liang、Junhua
    Zhao和Yun Li。大语言模型增强强化学习的调研：概念、分类法与方法。*arXiv预印本arXiv:2404.00282*，2024年。
- en: Carta et al. (2023) Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier,
    Olivier Sigaud, and Pierre-Yves Oudeyer. Grounding large language models in interactive
    environments with online reinforcement learning. *arXiv preprint arXiv:2302.02662*,
    2023.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carta等人（2023）Thomas Carta、Clément Romac、Thomas Wolf、Sylvain Lamprier、Olivier
    Sigaud和Pierre-Yves Oudeyer。通过在线强化学习将大语言模型嵌入交互环境中。*arXiv预印本arXiv:2302.02662*，2023年。
- en: 'Chen et al. (2024) Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, and
    Liangjun Zhang. Rlingua: Improving reinforcement learning sample efficiency in
    robotic manipulations with large language models. *IEEE Robotics and Automation
    Letters*, 2024.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等人（2024）Liangliang Chen、Yutian Lei、Shiyu Jin、Ying Zhang和Liangjun Zhang。Rlingua：利用大语言模型提升机器人操作中的强化学习样本效率。*IEEE机器人与自动化通讯*，2024年。
- en: Cheng et al. (2022) Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
    Adversarially trained actor critic for offline reinforcement learning. *arXiv
    preprint arXiv:2202.02446*, 2022.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等人（2022）Ching-An Cheng、Tengyang Xie、Nan Jiang和Alekh Agarwal。对抗性训练的演员-评论员用于离线强化学习。*arXiv预印本arXiv:2202.02446*，2022年。
- en: 'Cheng et al. (2020) Pengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe
    Gan, and Lawrence Carin. Club: A contrastive log-ratio upper bound of mutual information.
    In *International conference on machine learning*, pp. 1779–1788\. PMLR, 2020.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等人（2020）Pengyu Cheng、Weituo Hao、Shuyang Dai、Jiachang Liu、Zhe Gan和Lawrence
    Carin。Club：互信息对数比的对比性上界。在*国际机器学习会议*，第1779–1788页。PMLR，2020年。
- en: Claus & Boutilier (1998) Caroline Claus and Craig Boutilier. The dynamics of
    reinforcement learning in cooperative multiagent systems. In *AAAI Conference
    on Artificial Intelligence (AAAI)*, 1998.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Claus & Boutilier（1998）Caroline Claus和Craig Boutilier。合作多智能体系统中强化学习的动态。在*AAAI人工智能会议（AAAI）*，1998年。
- en: Colas et al. (2023) Cédric Colas, Laetitia Teodorescu, Pierre-Yves Oudeyer,
    Xingdi Yuan, and Marc-Alexandre Côté. Augmenting autotelic agents with large language
    models. *arXiv preprint arXiv:2305.12487*, 2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Colas 等人（2023）Cédric Colas、Laetitia Teodorescu、Pierre-Yves Oudeyer、Xingdi Yuan
    和 Marc-Alexandre Côté。《通过大型语言模型增强自我目的代理》。*arXiv 预印本 arXiv:2305.12487*，2023年。
- en: 'Das et al. (2019) Abhishek Das, Théophile Gervet, Joshua Romoff, Dhruv Batra,
    Devi Parikh, Mike Rabbat, and Joelle Pineau. Tarmac: Targeted multi-agent communication.
    In *International Conference on Machine Learning*, pp. 1538–1546\. PMLR, 2019.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Das 等人（2019）Abhishek Das、Théophile Gervet、Joshua Romoff、Dhruv Batra、Devi Parikh、Mike
    Rabbat 和 Joelle Pineau。《Tarmac：定向多代理通信》。在 *国际机器学习会议*，第1538–1546页。PMLR，2019年。
- en: de Witt et al. (2020) Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk,
    Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent
    learning all you need in the starcraft multi-agent challenge? *arXiv preprint
    arXiv:2011.09533*, 2020.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Witt 等人（2020）Christian Schroeder de Witt、Tarun Gupta、Denys Makoviichuk、Viktor
    Makoviychuk、Philip HS Torr、Mingfei Sun 和 Shimon Whiteson。《在星际争霸多代理挑战中，独立学习是否足够？》*arXiv
    预印本 arXiv:2011.09533*，2020年。
- en: Demaine et al. (2016) Erik D Demaine, Giovanni Viglietta, and Aaron Williams.
    Super mario bros. is harder/easier than we thought. 2016.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Demaine 等人（2016）Erik D Demaine、Giovanni Viglietta 和 Aaron Williams。《超级马里奥兄弟比我们想象的更难/更容易》。2016年。
- en: Dhuliawala et al. (2023) Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta
    Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. Chain-of-verification reduces
    hallucination in large language models. *arXiv preprint arXiv:2309.11495*, 2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhuliawala 等人（2023）Shehzaad Dhuliawala、Mojtaba Komeili、Jing Xu、Roberta Raileanu、Xian
    Li、Asli Celikyilmaz 和 Jason Weston。《验证链减少大型语言模型中的幻觉》。*arXiv 预印本 arXiv:2309.11495*，2023年。
- en: Ding et al. (2020) Ziluo Ding, Tiejun Huang, and Zongqing Lu. Learning individually
    inferred communication for multi-agent cooperation. *arXiv preprint arXiv:2006.06455*,
    2020.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ding 等人（2020）Ziluo Ding、Tiejun Huang 和 Zongqing Lu。《为多代理合作学习个体推断通信》。*arXiv 预印本
    arXiv:2006.06455*，2020年。
- en: Dong et al. (2017) Xiwang Dong, Qingdong Li, Qilun Zhao, and Zhang Ren. Time-varying
    group formation analysis and design for general linear multi-agent systems with
    directed topologies. *International Journal of Robust and Nonlinear Control*,
    27(9):1640–1652, 2017.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2017）Xiwang Dong、Qingdong Li、Qilun Zhao 和 Zhang Ren。《具有定向拓扑的广义线性多代理系统的时变群体形成分析与设计》。*国际鲁棒与非线性控制期刊*，27(9)：1640–1652，2017年。
- en: Du et al. (2023) Yuqing Du, Olivia Watkins, Zihan Wang, Cédric Colas, Trevor
    Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas. Guiding pretraining
    in reinforcement learning with large language models. *arXiv preprint arXiv:2302.06692*,
    2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2023）Yuqing Du、Olivia Watkins、Zihan Wang、Cédric Colas、Trevor Darrell、Pieter
    Abbeel、Abhishek Gupta 和 Jacob Andreas。《通过大型语言模型指导强化学习中的预训练》。*arXiv 预印本 arXiv:2302.06692*，2023年。
- en: Duda et al. (2000) R. O. Duda, P. E. Hart, and D. G. Stork. *Pattern Classification*.
    John Wiley and Sons, 2nd edition, 2000.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duda 等人（2000）R. O. Duda、P. E. Hart 和 D. G. Stork。*模式分类*。John Wiley & Sons，第二版，2000年。
- en: 'Durrett (2019) Rick Durrett. *Probability: theory and examples*, volume 49.
    Cambridge university press, 2019.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Durrett（2019）Rick Durrett。*概率：理论与实例*，第49卷。剑桥大学出版社，2019年。
- en: Ecoffet et al. (2021) Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O
    Stanley, and Jeff Clune. First return, then explore. *Nature*, 590(7847):580–586,
    2021.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ecoffet 等人（2021）Adrien Ecoffet、Joost Huizinga、Joel Lehman、Kenneth O Stanley
    和 Jeff Clune。《先返回，再探索》。*自然*，590(7847)：580–586，2021年。
- en: 'Ellis et al. (2024) Benjamin Ellis, Jonathan Cook, Skander Moalla, Mikayel
    Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob Foerster, and Shimon Whiteson. Smacv2:
    An improved benchmark for cooperative multi-agent reinforcement learning. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ellis 等人（2024）Benjamin Ellis、Jonathan Cook、Skander Moalla、Mikayel Samvelyan、Mingfei
    Sun、Anuj Mahajan、Jakob Foerster 和 Shimon Whiteson。《Smacv2：改进的合作性多代理强化学习基准》。*神经信息处理系统进展*，第36卷，2024年。
- en: Fischer (2020) Ian Fischer. The conditional entropy bottleneck. *Entropy*, 22(9):999,
    2020.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fischer（2020）Ian Fischer。《条件熵瓶颈》。*熵*，22(9)：999，2020年。
- en: Foerster et al. (2016) Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas,
    and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement
    learning. In *Advances in neural information processing systems (NeurIPS)*, 2016.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Foerster 等人（2016）Jakob Foerster、Ioannis Alexandros Assael、Nando De Freitas 和
    Shimon Whiteson。《通过深度多代理强化学习学习沟通》。在 *神经信息处理系统进展（NeurIPS）*，2016年。
- en: Foerster et al. (2018) Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras,
    Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients.
    In *AAAI Conference on Artificial Intelligence (AAAI)*, 2018.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 福尔斯特等（2018）Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli,
    和 Shimon Whiteson. 反事实多智能体策略梯度。在*AAAI 人工智能会议（AAAI）*，2018。
- en: 'Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey
    Levine. D4rl: Datasets for deep data-driven reinforcement learning. *arXiv preprint
    arXiv:2004.07219*, 2020.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '傅等（2020）Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, 和 Sergey Levine.
    D4rl: 用于深度数据驱动强化学习的数据集。*arXiv 预印本 arXiv:2004.07219*，2020。'
- en: Fujimoto & Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach
    to offline reinforcement learning. *Advances in neural information processing
    systems*, 34:20132–20145, 2021.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 藤本 & 顾（2021）Scott Fujimoto 和 Shixiang Shane Gu. 离线强化学习的极简方法。*神经信息处理系统进展*，34:20132–20145,
    2021。
- en: Fujimoto et al. (2018) Scott Fujimoto, Herke Hoof, and David Meger. Addressing
    function approximation error in actor-critic methods. In *International conference
    on machine learning*, pp. 1587–1596\. PMLR, 2018.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 藤本等（2018）Scott Fujimoto, Herke Hoof, 和 David Meger. 解决演员-评论家方法中的函数逼近误差。在*国际机器学习会议*，第1587–1596页。PMLR，2018。
- en: Fujimoto et al. (2019) Scott Fujimoto, David Meger, and Doina Precup. Off-policy
    deep reinforcement learning without exploration. In *International conference
    on machine learning*, pp. 2052–2062\. PMLR, 2019.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 藤本等（2019）Scott Fujimoto, David Meger, 和 Doina Precup. 无探索的离线深度强化学习。在*国际机器学习会议*，第2052–2062页。PMLR，2019。
- en: Gupta et al. (2017) Jayesh K Gupta, Maxim Egorov, and Mykel Kochenderfer. Cooperative
    multi-agent control using deep reinforcement learning. In *International Conference
    on Autonomous Agents and Multiagent Systems (AAMAS)*, 2017.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古普塔等（2017）Jayesh K Gupta, Maxim Egorov, 和 Mykel Kochenderfer. 使用深度强化学习的协作多智能体控制。在*自主智能体与多智能体系统国际会议（AAMAS）*，2017。
- en: Han et al. (2017) Tao Han, Zhi-Hong Guan, Ming Chi, Bin Hu, Tao Li, and Xian-He
    Zhang. Multi-formation control of nonlinear leader-following multi-agent systems.
    *ISA transactions*, 69:140–147, 2017.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 韩等（2017）Tao Han, Zhi-Hong Guan, Ming Chi, Bin Hu, Tao Li, 和 Xian-He Zhang. 非线性领导者-跟随型多智能体系统的多群体控制。*ISA
    交易*，69:140–147, 2017。
- en: Hooper et al. (2010) Paul L Hooper, Hillard S Kaplan, and James L Boone. A theory
    of leadership in human cooperative groups. *Journal of Theoretical Biology*, 265(4):633–646,
    2010.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡珀等（2010）Paul L Hooper, Hillard S Kaplan, 和 James L Boone. 人类合作群体中的领导力理论。*理论生物学杂志*，265(4):633–646,
    2010。
- en: Hu et al. (2019) Junyan Hu, Parijat Bhowmick, and Alexander Lanzon. Distributed
    adaptive time-varying group formation tracking for multiagent systems with multiple
    leaders on directed graphs. *IEEE Transactions on Control of Network Systems*,
    7(1):140–150, 2019.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等（2019）Junyan Hu, Parijat Bhowmick, 和 Alexander Lanzon. 在有向图上的多领导者多智能体系统的分布式自适应时变群体形成跟踪。*IEEE
    网络系统控制学报*，7(1):140–150, 2019。
- en: 'Hu et al. (2020) Siyi Hu, Fengda Zhu, Xiaojun Chang, and Xiaodan Liang. Updet:
    Universal multi-agent rl via policy decoupling with transformers. In *International
    Conference on Learning Representations (ICLR)*, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '胡等（2020）Siyi Hu, Fengda Zhu, Xiaojun Chang, 和 Xiaodan Liang. Updet: 通过使用变换器的策略解耦实现的通用多智能体强化学习。在*国际学习表示大会（ICLR）*，2020。'
- en: Iqbal et al. (2021) Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin
    Böhmer, Shimon Whiteson, and Fei Sha. Randomized entity-wise factorization for
    multi-agent reinforcement learning. In *International Conference on Machine Learning
    (ICML)*, 2021.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伊克巴尔等（2021）Shariq Iqbal, Christian A Schroeder De Witt, Bei Peng, Wendelin Böhmer,
    Shimon Whiteson, 和 Fei Sha. 多智能体强化学习中的随机实体因子分解。在*国际机器学习会议（ICML）*，2021。
- en: Jaderberg et al. (2019) Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke
    Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz,
    Ari S Morcos, Avraham Ruderman, et al. Human-level performance in 3d multiplayer
    games with population-based reinforcement learning. *Science*, 364(6443):859–865,
    2019.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾德伯格等（2019）Max Jaderberg, Wojciech M Czarnecki, Iain Dunning, Luke Marris, Guy
    Lever, Antonio Garcia Castaneda, Charles Beattie, Neil C Rabinowitz, Ari S Morcos,
    Avraham Ruderman 等. 在3D多人游戏中通过基于人口的强化学习实现人类水平的表现。*科学*，364(6443):859–865, 2019。
- en: 'Janner et al. (2019) Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
    When to trust your model: Model-based policy optimization. *arXiv preprint arXiv:1906.08253*,
    2019.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贾纳等（2019）Michael Janner, Justin Fu, Marvin Zhang, 和 Sergey Levine. 何时信任你的模型：基于模型的策略优化。*arXiv
    预印本 arXiv:1906.08253*，2019。
- en: Jaques et al. (2019) Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar
    Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas. Social
    influence as intrinsic motivation for multi-agent deep reinforcement learning.
    In *International Conference on Machine Learning (ICML)*, 2019.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaques 等人 (2019) Natasha Jaques、Angeliki Lazaridou、Edward Hughes、Caglar Gulcehre、Pedro
    Ortega、DJ Strouse、Joel Z Leibo 和 Nando De Freitas。作为多智能体深度强化学习内在动机的社会影响。在*国际机器学习会议（ICML）*，2019年。
- en: 'Jeon et al. (2022) Jeewon Jeon, Woojun Kim, Whiyoung Jung, and Youngchul Sung.
    Maser: Multi-agent reinforcement learning with subgoals generated from experience
    replay buffer. In *International Conference on Machine Learning*, pp. 10041–10052\.
    PMLR, 2022.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jeon 等人 (2022) Jeewon Jeon、Woojun Kim、Whiyoung Jung 和 Youngchul Sung。Maser：具有从经验重放缓冲区生成子目标的多智能体强化学习。在*国际机器学习会议*，第10041-10052页。PMLR，2022年。
- en: Jiang & Lu (2018) Jiechuan Jiang and Zongqing Lu. Learning attentional communication
    for multi-agent cooperation. *arXiv preprint arXiv:1805.07733*, 2018.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang & Lu (2018) 纪川·姜和宗庆·陆。多智能体合作中的注意力通信学习。*arXiv 预印本 arXiv:1805.07733*，2018年。
- en: Jiang & Lu (2021) Jiechuan Jiang and Zongqing Lu. Offline decentralized multi-agent
    reinforcement learning. *arXiv preprint arXiv:2108.01832*, 2021.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang & Lu (2021) 纪川·姜 和 宗庆·陆。离线去中心化多智能体强化学习。*arXiv 预印本 arXiv:2108.01832*，2021年。
- en: Jiang et al. (2018) Jiechuan Jiang, Chen Dun, Tiejun Huang, and Zongqing Lu.
    Graph convolutional reinforcement learning. *arXiv preprint arXiv:1810.09202*,
    2018.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2018) 纪川·姜、陈敦、黄铁军 和 宗庆·陆。图卷积强化学习。*arXiv 预印本 arXiv:1810.09202*，2018年。
- en: Jin et al. (2021) Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably
    efficient for offline rl? In *International Conference on Machine Learning*, pp. 5084–5096\.
    PMLR, 2021.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2021) Ying Jin、Zhuoran Yang 和 Zhaoran Wang。悲观主义是否对离线强化学习具有可证明的效率？在*国际机器学习会议*，第5084-5096页。PMLR，2021年。
- en: 'Jo et al. (2023) Yonghyeon Jo, Sunwoo Lee, Junghyuk Yum, and Seungyul Han.
    Fox: Formation-aware exploration in multi-agent reinforcement learning. *arXiv
    preprint arXiv:2308.11272*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jo 等人 (2023) Yonghyeon Jo、Sunwoo Lee、Junghyuk Yum 和 Seungyul Han。Fox：多智能体强化学习中的队形感知探索。*arXiv
    预印本 arXiv:2308.11272*，2023年。
- en: Kearns (1989) M. J. Kearns. *Computational Complexity of Machine Learning*.
    PhD thesis, Department of Computer Science, Harvard University, 1989.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kearns (1989) M. J. Kearns。*机器学习的计算复杂性*。哈佛大学计算机科学系博士论文，1989年。
- en: 'Kidambi et al. (2020) Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli,
    and Thorsten Joachims. Morel: Model-based offline reinforcement learning. *arXiv
    preprint arXiv:2005.05951*, 2020.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kidambi 等人 (2020) Rahul Kidambi、Aravind Rajeswaran、Praneeth Netrapalli 和 Thorsten
    Joachims。Morel：基于模型的离线强化学习。*arXiv 预印本 arXiv:2005.05951*，2020年。
- en: Kim et al. (2024) Woo Kyung Kim, SeungHyun Kim, Honguk Woo, et al. Efficient
    policy adaptation with contrastive prompt ensemble for embodied agents. *Advances
    in Neural Information Processing Systems*, 36, 2024.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 (2024) Woo Kyung Kim、SeungHyun Kim、Honguk Woo 等人。通过对比提示集成进行高效的策略适应。*神经信息处理系统进展*，第36卷，2024年。
- en: Kingma & Welling (2013) Diederik P Kingma and Max Welling. Auto-encoding variational
    bayes. *arXiv preprint arXiv:1312.6114*, 2013.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma & Welling (2013) Diederik P Kingma 和 Max Welling。自动编码变分贝叶斯。*arXiv 预印本
    arXiv:1312.6114*，2013年。
- en: Kocsis & Szepesvári (2006) Levente Kocsis and Csaba Szepesvári. Bandit based
    monte-carlo planning. In *European conference on machine learning*, pp.  282–293.
    Springer, 2006.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocsis & Szepesvári (2006) Levente Kocsis 和 Csaba Szepesvári。基于赌博的蒙特卡罗规划。在*欧洲机器学习会议*，第282-293页。Springer，2006年。
- en: Kostrikov et al. (2021) Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir
    Nachum. Offline reinforcement learning with fisher divergence critic regularization.
    In *International Conference on Machine Learning*, pp. 5774–5783\. PMLR, 2021.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kostrikov 等人 (2021) Ilya Kostrikov、Rob Fergus、Jonathan Tompson 和 Ofir Nachum。带有Fisher散度评论员正则化的离线强化学习。在*国际机器学习会议*，第5774-5783页。PMLR，2021年。
- en: Kuba et al. (2021) Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen,
    Fanglei Sun, Jun Wang, and Yaodong Yang. Trust region policy optimisation in multi-agent
    reinforcement learning. *arXiv preprint arXiv:2109.11251*, 2021.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuba 等人 (2021) Jakub Grudzien Kuba、Ruiqing Chen、Munning Wen、Ying Wen、Fanglei
    Sun、Jun Wang 和 Yaodong Yang。多智能体强化学习中的信任区域策略优化。*arXiv 预印本 arXiv:2109.11251*，2021年。
- en: Kulesza & Taskar (2012) Alex Kulesza and Ben Taskar. Determinantal point processes
    for machine learning. *arXiv preprint arXiv:1207.6083*, 2012.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulesza & Taskar (2012) Alex Kulesza 和 Ben Taskar。机器学习中的行列式点过程。*arXiv 预印本 arXiv:1207.6083*，2012年。
- en: 'Kulkarni et al. (2016) Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi,
    and Josh Tenenbaum. Hierarchical deep reinforcement learning: Integrating temporal
    abstraction and intrinsic motivation. *Advances in neural information processing
    systems*, 29, 2016.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kulkarni 等人 (2016) Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi 和 Josh
    Tenenbaum. 层次化深度强化学习：整合时间抽象和内在动机. *神经信息处理系统进展*，29，2016。
- en: Kumar et al. (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and
    Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction.
    *Advances in Neural Information Processing Systems*, 32, 2019.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2019) Aviral Kumar, Justin Fu, Matthew Soh, George Tucker 和 Sergey
    Levine. 通过引导错误减少稳定化离策略 Q 学习. *神经信息处理系统进展*，32，2019。
- en: Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
    Conservative q-learning for offline reinforcement learning. *Advances in Neural
    Information Processing Systems*, 33:1179–1191, 2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2020) Aviral Kumar, Aurick Zhou, George Tucker 和 Sergey Levine. 离线强化学习中的保守
    Q 学习. *神经信息处理系统进展*，33:1179–1191，2020。
- en: Kumar et al. (2021) Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine.
    Should i run offline reinforcement learning or behavioral cloning? In *Deep RL
    Workshop NeurIPS 2021*, 2021.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 (2021) Aviral Kumar, Joey Hong, Anikait Singh 和 Sergey Levine. 我应该运行离线强化学习还是行为克隆？在
    *Deep RL 研讨会 NeurIPS 2021*，2021。
- en: 'Kurach et al. (2019) Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac,
    Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski,
    Olivier Bousquet, et al. Google research football: A novel reinforcement learning
    environment. *arXiv preprint arXiv:1907.11180*, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurach 等人 (2019) Karol Kurach, Anton Raichuk, Piotr Stanczyk, Michal Zajac,
    Olivier Bachem, Lasse Espeholt, Carlos Riquelme, Damien Vincent, Marcin Michalski,
    Olivier Bousquet 等人. Google Research Football：一种新型的强化学习环境. *arXiv 预印本 arXiv:1907.11180*，2019。
- en: Kwon et al. (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa
    Sadigh. Reward design with language models. *arXiv preprint arXiv:2303.00001*,
    2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 (2023) Minae Kwon, Sang Michael Xie, Kalesha Bullard 和 Dorsa Sadigh.
    使用语言模型进行奖励设计. *arXiv 预印本 arXiv:2303.00001*，2023。
- en: Langley (2000) P. Langley. Crafting papers on machine learning. In Pat Langley
    (ed.), *Proceedings of the 17th International Conference on Machine Learning (ICML
    2000)*, pp.  1207–1216, Stanford, CA, 2000\. Morgan Kaufmann.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Langley (2000) P. Langley. 撰写机器学习论文. 在 Pat Langley (编辑)，《*第17届国际机器学习会议（ICML
    2000）论文集*》，第1207–1216页，斯坦福，CA，2000。Morgan Kaufmann。
- en: 'Lawler & Limic (2010) Gregory F Lawler and Vlada Limic. *Random walk: a modern
    introduction*, volume 123. Cambridge University Press, 2010.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lawler & Limic (2010) Gregory F Lawler 和 Vlada Limic. *随机漫步：现代介绍*，第123卷。剑桥大学出版社，2010。
- en: Lee et al. (2020) Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak
    Lee, John Canny, and Sergio Guadarrama. Predictive information accelerates learning
    in rl. *arXiv preprint arXiv:2007.12401*, 2020.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 (2020) Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee,
    John Canny 和 Sergio Guadarrama. 预测信息加速强化学习中的学习. *arXiv 预印本 arXiv:2007.12401*，2020。
- en: Lee et al. (2022) Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel, and
    Jinwoo Shin. Offline-to-online reinforcement learning via balanced replay and
    pessimistic q-ensemble. In *Conference on Robot Learning*, pp.  1702–1712\. PMLR,
    2022.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 (2022) Seunghyun Lee, Younggyo Seo, Kimin Lee, Pieter Abbeel 和 Jinwoo
    Shin. 通过平衡回放和悲观的 Q 集成实现离线到在线强化学习. 在 *机器人学习会议* 上，第1702–1712页。PMLR，2022。
- en: Leibo et al. (2021) Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets,
    John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor
    Mordatch, and Thore Graepel. Scalable evaluation of multi-agent reinforcement
    learning with melting pot. In *International Conference on Machine Learning (ICML)*,
    2021.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leibo 等人 (2021) Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John
    P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch
    和 Thore Graepel. 使用 Melting Pot 对多智能体强化学习进行可扩展评估. 在 *国际机器学习会议 (ICML)*，2021。
- en: 'Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin
    Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open
    problems. *arXiv preprint arXiv:2005.01643*, 2020.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levine 等人 (2020) Sergey Levine, Aviral Kumar, George Tucker 和 Justin Fu. 离线强化学习：教程、综述及开放问题的视角.
    *arXiv 预印本 arXiv:2005.01643*，2020。
- en: 'Li et al. (2023) Belinda Z Li, William Chen, Pratyusha Sharma, and Jacob Andreas.
    Lampp: Language models as probabilistic priors for perception and action. *arXiv
    e-prints*, pp.  arXiv–2302, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 (2023) Belinda Z Li, William Chen, Pratyusha Sharma 和 Jacob Andreas. Lampp：将语言模型作为感知与行动的概率先验.
    *arXiv 电子预印本*，第 arXiv–2302，2023。
- en: Li et al. (2021) Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun
    Yang, and Chongjie Zhang. Celebrating diversity in shared multi-agent reinforcement
    learning. *Advances in Neural Information Processing Systems*, 34:3991–4002, 2021.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2021）Chenghao Li, Tonghan Wang, Chengjie Wu, Qianchuan Zhao, Jun Yang, 和Chongjie
    Zhang。《庆祝共享多智能体强化学习中的多样性》。*神经信息处理系统进展*，34:3991–4002，2021年。
- en: Li et al. (2022) Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang,
    Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, et al. Pre-trained
    language models for interactive decision-making. *Advances in Neural Information
    Processing Systems*, 35:31199–31212, 2022.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2022）Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi
    Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar等人。《预训练语言模型在互动决策中的应用》。*神经信息处理系统进展*，35:31199–31212，2022年。
- en: 'Liang et al. (2023) Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman,
    Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs
    for embodied control. In *2023 IEEE International Conference on Robotics and Automation
    (ICRA)*, pp.  9493–9500\. IEEE, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等（2023）Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian
    Ichter, Pete Florence, 和Andy Zeng。《代码即策略：用于具身控制的语言模型程序》。在*2023年IEEE国际机器人与自动化大会（ICRA）*，第9493–9500页。IEEE，2023年。
- en: 'Lin et al. (2023) Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu,
    Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage:
    A generative agent with fast and slow thinking for complex interactive tasks.
    *arXiv preprint arXiv:2305.17390*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2023）Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu,
    Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, 和Xiang Ren。《Swiftsage：用于复杂互动任务的快慢思维生成智能体》。*arXiv预印本
    arXiv:2305.17390*，2023年。
- en: Lin et al. (2021) Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, and
    Phillip Isola. Learning to ground multi-agent communication with autoencoders.
    In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin等（2021）Toru Lin, Jacob Huh, Christopher Stauffer, Ser Nam Lim, 和Phillip Isola。《通过自编码器学习多智能体通信的基础》。在*神经信息处理系统进展（NeurIPS）*，2021年。
- en: 'Linke et al. (2020) Cam Linke, Nadia M Ady, Martha White, Thomas Degris, and
    Adam White. Adapting behavior via intrinsic reward: A survey and empirical study.
    *Journal of artificial intelligence research*, 69:1287–1332, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linke等（2020）Cam Linke, Nadia M Ady, Martha White, Thomas Degris, 和Adam White。《通过内在奖励调整行为：一项调查与实证研究》。*人工智能研究期刊*，69:1287–1332，2020年。
- en: Liu et al. (2021a) Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, and
    Animashree Anandkumar. Coach-player multi-agent reinforcement learning for dynamic
    team composition. *arXiv preprint arXiv:2105.08692*, 2021a.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021a）Bo Liu, Qiang Liu, Peter Stone, Animesh Garg, Yuke Zhu, 和Animashree
    Anandkumar。《教练-玩家多智能体强化学习用于动态团队组成》。*arXiv预印本 arXiv:2105.08692*，2021a年。
- en: 'Liu et al. (2023) Boyin Liu, Zhiqiang Pu, Yi Pan, Jianqiang Yi, Yanyan Liang,
    and Du Zhang. Lazy agents: a new perspective on solving sparse reward problem
    in multi-agent reinforcement learning. In *International Conference on Machine
    Learning*, pp. 21937–21950\. PMLR, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2023）Boyin Liu, Zhiqiang Pu, Yi Pan, Jianqiang Yi, Yanyan Liang, 和Du Zhang。《懒惰智能体：解决多智能体强化学习中稀疏奖励问题的新视角》。在*国际机器学习大会*，第21937–21950页。PMLR，2023年。
- en: 'Liu & Abbeel (2021) Hao Liu and Pieter Abbeel. Behavior from the void: Unsupervised
    active pre-training. *Advances in Neural Information Processing Systems*, 34:18459–18473,
    2021.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu & Abbeel（2021）Hao Liu 和Pieter Abbeel。《来自虚无的行为：无监督主动预训练》。*神经信息处理系统进展*，34:18459–18473，2021年。
- en: Liu et al. (2021b) Iou-Jen Liu, Unnat Jain, Raymond A Yeh, and Alexander Schwing.
    Cooperative exploration for multi-agent deep reinforcement learning. In *International
    Conference on Machine Learning*, pp. 6826–6836\. PMLR, 2021b.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2021b）Iou-Jen Liu, Unnat Jain, Raymond A Yeh, 和Alexander Schwing。《多智能体深度强化学习中的协作探索》。在*国际机器学习大会*，第6826–6836页。PMLR，2021b年。
- en: 'Liu et al. (2024) Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen,
    Shu Liu, Zongqing Lu, and Jiaya Jia. Rl-gpt: Integrating reinforcement learning
    and code-as-policy. *arXiv preprint arXiv:2402.19299*, 2024.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2024）Shaoteng Liu, Haoqi Yuan, Minda Hu, Yanwei Li, Yukang Chen, Shu Liu,
    Zongqing Lu, 和Jiaya Jia。《Rl-gpt：整合强化学习和代码即策略》。*arXiv预印本 arXiv:2402.19299*，2024年。
- en: Liu et al. (2020a) Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
    Provably good batch reinforcement learning without great exploration. *arXiv preprint
    arXiv:2007.08202*, 2020a.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu等（2020a）Yao Liu, Adith Swaminathan, Alekh Agarwal, 和Emma Brunskill。《在没有大量探索的情况下，可证明的优秀批量强化学习》。*arXiv预印本
    arXiv:2007.08202*，2020a年。
- en: Liu et al. (2020b) Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen,
    and Yang Gao. Multi-agent game abstraction via graph attention neural network.
    In *AAAI Conference on Artificial Intelligence (AAAI)*, 2020b.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 (2020b) Yong Liu, Weixun Wang, Yujing Hu, Jianye Hao, Xingguo Chen, 和
    Yang Gao. 通过图注意力神经网络进行多智能体游戏抽象. 载于 *人工智能学会会议（AAAI）*, 2020b.
- en: Long et al. (2020) Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and
    Xiaolong Wang. Evolutionary population curriculum for scaling multi-agent reinforcement
    learning. *arXiv preprint arXiv:2003.10423*, 2020.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人 (2020) Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, 和 Xiaolong
    Wang. 进化人口课程用于扩展多智能体强化学习. *arXiv预印本 arXiv:2003.10423*, 2020.
- en: Lowe et al. (2017) Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter
    Abbeel, and Igor Mordatch. Multi-agent actor-critic for mixed cooperative-competitive
    environments. In *Advances in neural information processing systems (NeurIPS)*,
    2017.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lowe 等人 (2017) Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel,
    和 Igor Mordatch. 混合合作竞争环境中的多智能体演员-评论家方法. 载于 *神经信息处理系统进展（NeurIPS）*, 2017.
- en: 'Ma et al. (2023) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka:
    Human-level reward design via coding large language models. *arXiv preprint arXiv:2310.12931*,
    2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 (2023) Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert
    Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Eureka: 通过编码大语言模型进行人类级奖励设计.
    *arXiv预印本 arXiv:2310.12931*, 2023.'
- en: Macchi (1977) Odile Macchi. The fermion process—a model of stochastic point
    process with repulsive points. In *Transactions of the Seventh Prague Conference
    on Information Theory, Statistical Decision Functions, Random Processes and of
    the 1974 European Meeting of Statisticians*, 1977.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Macchi (1977) Odile Macchi. 费米子过程——具有排斥点的随机点过程模型. 载于 *第七届布拉格信息理论、统计决策函数、随机过程会议论文集，以及1974年欧洲统计学家会议论文集*,
    1977.
- en: 'Mahajan et al. (2019) Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon
    Whiteson. Maven: Multi-agent variational exploration. *Advances in neural information
    processing systems*, 32, 2019.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mahajan 等人 (2019) Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, 和 Shimon
    Whiteson. Maven: 多智能体变分探索. *神经信息处理系统进展*, 32, 2019.'
- en: Mao et al. (2020a) Hangyu Mao, Wulong Liu, Jianye Hao, Jun Luo, Dong Li, Zhengchao
    Zhang, Jun Wang, and Zhen Xiao. Neighborhood cognition consistent multi-agent
    reinforcement learning. In *Proceedings of the AAAI conference on artificial intelligence*,
    volume 34, pp.  7219–7226, 2020a.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人 (2020a) Hangyu Mao, Wulong Liu, Jianye Hao, Jun Luo, Dong Li, Zhengchao
    Zhang, Jun Wang, 和 Zhen Xiao. 一致的邻域认知多智能体强化学习. 载于 *人工智能学会会议论文集*, 第34卷, 第7219–7226页,
    2020a.
- en: Mao et al. (2020b) Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, and Yan
    Ni. Learning agent communication under limited bandwidth by message pruning. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pp. 
    5142–5149, 2020b.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人 (2020b) Hangyu Mao, Zhengchao Zhang, Zhen Xiao, Zhibo Gong, 和 Yan Ni.
    通过信息修剪在带宽有限的情况下学习智能体通信. 载于 *人工智能学会会议论文集*, 第34卷, 第5142–5149页, 2020b.
- en: 'Meng et al. (2021) Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun
    Li, Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, and Bo Xu. Offline pre-trained
    multi-agent decision transformer: One big sequence model conquers all starcraftii
    tasks. *arXiv preprint arXiv:2112.02845*, 2021.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meng 等人 (2021) Linghui Meng, Muning Wen, Yaodong Yang, Chenyang Le, Xiyun Li,
    Weinan Zhang, Ying Wen, Haifeng Zhang, Jun Wang, 和 Bo Xu. 离线预训练多智能体决策变换器：一个大序列模型征服所有星际争霸II任务.
    *arXiv预印本 arXiv:2112.02845*, 2021.
- en: 'Michalski et al. (1983) R. S. Michalski, J. G. Carbonell, and T. M. Mitchell
    (eds.). *Machine Learning: An Artificial Intelligence Approach, Vol. I*. Tioga,
    Palo Alto, CA, 1983.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michalski 等人 (1983) R. S. Michalski, J. G. Carbonell, 和 T. M. Mitchell (编).
    *机器学习：人工智能方法，第I卷*. Tioga, Palo Alto, CA, 1983.
- en: Michie et al. (1990) Donald Michie, Michael Bain, and J Hayes-Miches. Cognitive
    models from subcognitive skills. *IEE control engineering series*, 44:71–99, 1990.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Michie 等人 (1990) Donald Michie, Michael Bain, 和 J Hayes-Miches. 从亚认知技能中获得的认知模型.
    *IEE 控制工程系列*, 44:71–99, 1990.
- en: 'Miller (2024) D. Lee Miller. Wordllama: Recycled token embeddings from large
    language models, 2024. URL [https://github.com/dleemiller/wordllama](https://github.com/dleemiller/wordllama).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miller (2024) D. Lee Miller. Wordllama: 来自大语言模型的回收令牌嵌入, 2024. URL [https://github.com/dleemiller/wordllama](https://github.com/dleemiller/wordllama).'
- en: 'Mirsky et al. (2020) Reuth Mirsky, William Macke, Andy Wang, Harel Yedidsion,
    and Peter Stone. A penny for your thoughts: The value of communication in ad hoc
    teamwork. In *International Joint Conferences on Artificial Intelligence (IJCAI)*,
    2020.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirsky 等人 (2020) Reuth Mirsky, William Macke, Andy Wang, Harel Yedidsion, 和
    Peter Stone. 为你的想法付出代价：临时团队合作中沟通的价值. 在 *国际人工智能联合会议 (IJCAI)*，2020.
- en: Mitchell (1980) T. M. Mitchell. The need for biases in learning generalizations.
    Technical report, Computer Science Department, Rutgers University, New Brunswick,
    MA, 1980.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mitchell (1980) T. M. Mitchell. 学习泛化中偏差的必要性. 技术报告，计算机科学系，罗格斯大学，新不伦瑞克，MA，1980.
- en: Mnih et al. (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep
    reinforcement learning. *arXiv preprint arXiv:1312.5602*, 2013.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mnih 等人 (2013) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves,
    Ioannis Antonoglou, Daan Wierstra, 和 Martin Riedmiller. 使用深度强化学习玩 Atari 游戏. *arXiv
    预印本 arXiv:1312.5602*, 2013.
- en: 'Nachum et al. (2019) Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong
    Li, and Dale Schuurmans. Algaedice: Policy gradient from arbitrary experience.
    *arXiv preprint arXiv:1912.02074*, 2019.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachum 等人 (2019) Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li,
    和 Dale Schuurmans. Algaedice：来自任意经验的策略梯度. *arXiv 预印本 arXiv:1912.02074*, 2019.
- en: Nair et al. (2020a) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
    Accelerating online reinforcement learning with offline datasets. *arXiv preprint
    arXiv:2006.09359*, 2020a.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等人 (2020a) Ashvin Nair, Murtaza Dalal, Abhishek Gupta, 和 Sergey Levine.
    使用离线数据集加速在线强化学习. *arXiv 预印本 arXiv:2006.09359*, 2020a.
- en: 'Nair et al. (2020b) Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey
    Levine. Awac: Accelerating online reinforcement learning with offline datasets.
    *arXiv preprint arXiv:2006.09359*, 2020b.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nair 等人 (2020b) Ashvin Nair, Abhishek Gupta, Murtaza Dalal, 和 Sergey Levine.
    AWAC：利用离线数据集加速在线强化学习. *arXiv 预印本 arXiv:2006.09359*, 2020b.
- en: Newell & Rosenbloom (1981) A. Newell and P. S. Rosenbloom. Mechanisms of skill
    acquisition and the law of practice. In J. R. Anderson (ed.), *Cognitive Skills
    and Their Acquisition*, chapter 1, pp.  1–51\. Lawrence Erlbaum Associates, Inc.,
    Hillsdale, NJ, 1981.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Newell & Rosenbloom (1981) A. Newell 和 P. S. Rosenbloom. 技能获得机制与练习法则. 在 J. R.
    Anderson (编)，*认知技能及其获得*，第一章，页码 1–51，劳伦斯·厄尔鲍姆协会，NJ，1981.
- en: Niu et al. (2021) Yaru Niu, Rohan R Paleja, and Matthew C Gombolay. Multi-agent
    graph-attention communication and teaming. In *AAMAS*, pp.  964–973, 2021.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu 等人 (2021) Yaru Niu, Rohan R Paleja, 和 Matthew C Gombolay. 多智能体图注意力通信与团队合作.
    在 *AAMAS* 会议中，页码 964–973, 2021.
- en: Oliehoek et al. (2016) Frans A Oliehoek, Christopher Amato, et al. *A concise
    introduction to decentralized POMDPs*. Springer, 2016.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oliehoek 等人 (2016) Frans A Oliehoek, Christopher Amato, 等. *去中心化 POMDP 简明介绍*.
    斯普林格，2016.
- en: Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation
    learning with contrastive predictive coding. *arXiv preprint arXiv:1807.03748*,
    2018.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oord 等人 (2018) Aaron van den Oord, Yazhe Li, 和 Oriol Vinyals. 使用对比预测编码进行表示学习.
    *arXiv 预印本 arXiv:1807.03748*, 2018.
- en: Paischer et al. (2022) Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling,
    Markus Holzleitner, Sebastian Lehner, Hamid Eghbal-Zadeh, and Sepp Hochreiter.
    History compression via language models in reinforcement learning. In *International
    Conference on Machine Learning*, pp. 17156–17185\. PMLR, 2022.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paischer 等人 (2022) Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling,
    Markus Holzleitner, Sebastian Lehner, Hamid Eghbal-Zadeh, 和 Sepp Hochreiter. 强化学习中的语言模型历史压缩.
    在 *国际机器学习大会*，页码 17156–17185，PMLR，2022.
- en: 'Paischer et al. (2024) Fabian Paischer, Thomas Adler, Markus Hofmarcher, and
    Sepp Hochreiter. Semantic helm: A human-readable memory for reinforcement learning.
    *Advances in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paischer 等人 (2024) Fabian Paischer, Thomas Adler, Markus Hofmarcher, 和 Sepp
    Hochreiter. 语义舵：一种人类可读的强化学习记忆. *神经信息处理系统进展*，36，2024.
- en: Pan et al. (2021) Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, and Shimon
    Whiteson. Regularized softmax deep multi-agent q-learning. *Advances in Neural
    Information Processing Systems*, 34:1365–1377, 2021.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 (2021) Ling Pan, Tabish Rashid, Bei Peng, Longbo Huang, 和 Shimon Whiteson.
    正则化软最大深度多智能体 Q 学习. *神经信息处理系统进展*，34:1365–1377，2021.
- en: 'Pan et al. (2022) Ling Pan, Longbo Huang, Tengyu Ma, and Huazhe Xu. Plan better
    amid conservatism: Offline multi-agent reinforcement learning with actor rectification.
    In *International Conference on Machine Learning*, pp. 17221–17237\. PMLR, 2022.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 (2022) Ling Pan, Longbo Huang, Tengyu Ma, 和 Huazhe Xu. 在保守主义中更好地规划：具有演员矫正的离线多智能体强化学习.
    在 *国际机器学习大会*，页码 17221–17237，PMLR，2022.
- en: Pathak et al. (2017) Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor
    Darrell. Curiosity-driven exploration by self-supervised prediction. In *International
    Conference on Machine Learning (ICML)*, 2017.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathak et al. (2017) Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor
    Darrell. 通过自监督预测驱动的好奇心探索。在*国际机器学习会议 (ICML)*，2017年。
- en: 'Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre
    Kamienny, Philip Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac: Factored
    multi-agent centralised policy gradients. *Advances in Neural Information Processing
    Systems*, 34:12208–12221, 2021.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2021) Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre
    Kamienny, Philip Torr, Wendelin Böhmer, and Shimon Whiteson. Facmac：分解的多智能体集中式策略梯度。*神经信息处理系统进展*，34:12208–12221，2021年。
- en: 'Peng et al. (2023) Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di Huang,
    Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, et al. Self-driven grounding: Large
    language model agents with automatical language-aligned skill learning. *arXiv
    preprint arXiv:2309.01352*, 2023.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. (2023) Shaohui Peng, Xing Hu, Qi Yi, Rui Zhang, Jiaming Guo, Di
    Huang, Zikang Tian, Ruizhi Chen, Zidong Du, Qi Guo, et al. 自驱动基础：具有自动语言对齐技能学习的大型语言模型智能体。*arXiv预印本
    arXiv:2309.01352*，2023年。
- en: 'Peng et al. (2019) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
    *arXiv preprint arXiv:1910.00177*, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2019) Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
    Advantage-weighted regression: Simple and scalable off-policy reinforcement learning.
    *arXiv预印本 arXiv:1910.00177*，2019年。'
- en: Poole et al. (2019) Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi,
    and George Tucker. On variational bounds of mutual information. In *International
    Conference on Machine Learning*, pp. 5171–5180\. PMLR, 2019.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poole et al. (2019) Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi,
    and George Tucker. 关于互信息的变分界限。在*国际机器学习会议*，第5171–5180页，PMLR，2019年。
- en: 'Qu et al. (2023) Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen,
    Zhenbin Ye, Lin Liu, Yang Jun Feng, Lin Lai, Hongyang Qin, et al. Hokoff: Real
    game dataset from honor of kings and its offline reinforcement learning benchmarks.
    In *Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*, 2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu et al. (2023) Yun Qu, Boyuan Wang, Jianzhun Shao, Yuhang Jiang, Chen Chen,
    Zhenbin Ye, Lin Liu, Yang Jun Feng, Lin Lai, Hongyang Qin, et al. Hokoff：来自王者荣耀的真实游戏数据集及其离线强化学习基准测试。在*第三十七届神经信息处理系统大会数据集与基准赛道*，2023年。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 语言模型是无监督的多任务学习者。*OpenAI博客*，1(8):9，2019年。
- en: 'Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder
    De Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic
    value function factorisation for deep multi-agent reinforcement learning. *arXiv
    preprint arXiv:1803.11485*, 2018.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid et al. (2018) Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De
    Witt, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix：深度多智能体强化学习的单调值函数分解。*arXiv预印本
    arXiv:1803.11485*，2018年。
- en: 'Rashid et al. (2020) Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon
    Whiteson. Weighted qmix: Expanding monotonic value function factorisation. *arXiv
    e-prints*, pp.  arXiv–2006, 2020.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashid et al. (2020) Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson.
    加权qmix：扩展单调值函数分解。*arXiv电子预印本*，第arXiv–2006页，2020年。
- en: 'Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao
    Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation
    learning: A tale of pessimism. *Advances in Neural Information Processing Systems*,
    34, 2021.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashidinejad et al. (2021) Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao
    Jiao, and Stuart Russell. 桥接离线强化学习与模仿学习：悲观主义的故事。*神经信息处理系统进展*，34，2021年。
- en: Samuel (1959) A. L. Samuel. Some studies in machine learning using the game
    of checkers. *IBM Journal of Research and Development*, 3(3):211–229, 1959.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samuel (1959) A. L. Samuel. 使用跳棋游戏进行的机器学习研究。*IBM研究与开发杂志*，3(3):211–229，1959年。
- en: Samvelyan et al. (2019a) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder
    de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph
    H. S. Torr, Jakob Foerster, and Shimon Whiteson. The StarCraft Multi-Agent Challenge.
    *CoRR*, abs/1902.04043, 2019a.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samvelyan et al. (2019a) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder
    de Witt, Gregory Farquhar, Nantas Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph
    H. S. Torr, Jakob Foerster, and Shimon Whiteson. 星际争霸多智能体挑战。*CoRR*，abs/1902.04043，2019a年。
- en: Samvelyan et al. (2019b) Mikayel Samvelyan, Tabish Rashid, Christian Schroeder
    de Witt, Gregory Farquhar, Nantas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS
    Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge.
    *arXiv preprint arXiv:1902.04043*, 2019b.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samvelyan 等人（2019b）Mikayel Samvelyan、Tabish Rashid、Christian Schroeder de Witt、Gregory
    Farquhar、Nantas Nardelli、Tim GJ Rudner、Chia-Man Hung、Philip HS Torr、Jakob Foerster
    和 Shimon Whiteson. 星际争霸多智能体挑战。*arXiv 预印本 arXiv:1902.04043*，2019b。
- en: Scarselli et al. (2008) Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. The graph neural network model. *IEEE transactions
    on neural networks*, 20(1):61–80, 2008.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarselli 等人（2008）Franco Scarselli、Marco Gori、Ah Chung Tsoi、Markus Hagenbuchner
    和 Gabriele Monfardini. 图神经网络模型。*IEEE 神经网络学报*，20(1):61–80，2008。
- en: Schroeder de Witt et al. (2019) Christian Schroeder de Witt, Jakob Foerster,
    Gregory Farquhar, Philip Torr, Wendelin Boehmer, and Shimon Whiteson. Multi-agent
    common knowledge reinforcement learning. *Advances in Neural Information Processing
    Systems*, 32:9927–9939, 2019.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schroeder de Witt 等人（2019）Christian Schroeder de Witt、Jakob Foerster、Gregory
    Farquhar、Philip Torr、Wendelin Boehmer 和 Shimon Whiteson. 多智能体共同知识强化学习。*神经信息处理系统进展*，32:9927–9939，2019。
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schulman 等人（2017）John Schulman、Filip Wolski、Prafulla Dhariwal、Alec Radford 和
    Oleg Klimov. 近端策略优化算法。*arXiv 预印本 arXiv:1707.06347*，2017。
- en: Seo et al. (2021) Younggyo Seo, Lili Chen, Jinwoo Shin, Honglak Lee, Pieter
    Abbeel, and Kimin Lee. State entropy maximization with random encoders for efficient
    exploration. In *International Conference on Machine Learning*, pp. 9443–9454\.
    PMLR, 2021.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seo 等人（2021）Younggyo Seo、Lili Chen、Jinwoo Shin、Honglak Lee、Pieter Abbeel 和 Kimin
    Lee. 使用随机编码器的状态熵最大化以实现高效探索。在 *国际机器学习大会*，第 9443-9454 页。PMLR，2021。
- en: Shao et al. (2023) Jianzhun Shao, Hongchang Zhang, Yun Qu, Chang Liu, Shuncheng
    He, Yuhang Jiang, and Xiangyang Ji. Complementary attention for multi-agent reinforcement
    learning. In *International Conference on Machine Learning*, pp. 30776–30793\.
    PMLR, 2023.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人（2023）Jianzhun Shao、Hongchang Zhang、Yun Qu、Chang Liu、Shuncheng He、Yuhang
    Jiang 和 Xiangyang Ji. 多智能体强化学习中的互补注意力。在 *国际机器学习大会*，第 30776-30793 页。PMLR，2023。
- en: Shao et al. (2024) Jianzhun Shao, Yun Qu, Chen Chen, Hongchang Zhang, and Xiangyang
    Ji. Counterfactual conservative q learning for offline multi-agent reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36, 2024.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人（2024）Jianzhun Shao、Yun Qu、Chen Chen、Hongchang Zhang 和 Xiangyang Ji.
    离线多智能体强化学习中的反事实保守 Q 学习。*神经信息处理系统进展*，36，2024。
- en: Shi et al. (2023) Ruizhe Shi, Yuyao Liu, Yanjie Ze, Simon S Du, and Huazhe Xu.
    Unleashing the power of pre-trained language models for offline reinforcement
    learning. *arXiv preprint arXiv:2310.20587*, 2023.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2023）Ruizhe Shi、Yuyao Liu、Yanjie Ze、Simon S Du 和 Huazhe Xu. 释放预训练语言模型在离线强化学习中的潜力。*arXiv
    预印本 arXiv:2310.20587*，2023。
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement
    learning. In *Thirty-seventh Conference on Neural Information Processing Systems*,
    2023.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2023）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik R Narasimhan
    和 Shunyu Yao. Reflexion: 具有语言强化学习的语言代理。 在 *第 37 届神经信息处理系统会议*，2023。'
- en: 'Shukla et al. (2023) Yash Shukla, Wenchang Gao, Vasanth Sarathy, Alvaro Velasquez,
    Robert Wright, and Jivko Sinapov. Lgts: Dynamic task sampling using llm-generated
    sub-goals for reinforcement learning agents. *arXiv preprint arXiv:2310.09454*,
    2023.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shukla 等人（2023）Yash Shukla、Wenchang Gao、Vasanth Sarathy、Alvaro Velasquez、Robert
    Wright 和 Jivko Sinapov. Lgts: 使用 LLM 生成的子目标进行强化学习代理的动态任务采样。*arXiv 预印本 arXiv:2310.09454*，2023。'
- en: Singh et al. (2018) Amanpreet Singh, Tushar Jain, and Sainbayar Sukhbaatar.
    Learning when to communicate at scale in multiagent cooperative and competitive
    tasks. *arXiv preprint arXiv:1812.09755*, 2018.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2018）Amanpreet Singh、Tushar Jain 和 Sainbayar Sukhbaatar. 在大规模多智能体协作和竞争任务中学习何时进行通信。*arXiv
    预印本 arXiv:1812.09755*，2018。
- en: 'Singh (1998) Munindar P Singh. Agent communication languages: Rethinking the
    principles. *Computer*, 31(12):40–47, 1998.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh（1998）Munindar P Singh. 智能体通信语言：重新思考其原则。*计算机*，31(12):40–47，1998。
- en: 'Son et al. (2019) Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero,
    and Yung Yi. Qtran: Learning to factorize with transformation for cooperative
    multi-agent reinforcement learning. In *International Conference on Machine Learning
    (ICML)*, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Son 等人（2019）Kyunghwan Son、Daewoo Kim、Wan Ju Kang、David Earl Hostallero 和 Yung
    Yi. Qtran: 学习通过变换因式分解进行协作式多智能体强化学习。在 *国际机器学习大会（ICML）*，2019。'
- en: Song et al. (2023) Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan
    Shu, and Lei Ma. Self-refined large language model as automated reward function
    designer for deep reinforcement learning in robotics. *arXiv preprint arXiv:2309.06687*,
    2023.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2023) Jiayang Song, Zhehua Zhou, Jiawei Liu, Chunrong Fang, Zhan
    Shu 和 Lei Ma。自我优化的大型语言模型作为机器人深度强化学习的自动化奖励函数设计师。*arXiv 预印本 arXiv:2309.06687*，2023年。
- en: 'Stone et al. (2010) Peter Stone, Gal A Kaminka, Sarit Kraus, and Jeffrey S
    Rosenschein. Ad hoc autonomous agent teams: Collaboration without pre-coordination.
    In *AAAI Conference on Artificial Intelligence (AAAI)*, 2010.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stone et al. (2010) Peter Stone, Gal A Kaminka, Sarit Kraus 和 Jeffrey S Rosenschein。临时自治智能体团队：无需预协调的合作。在
    *AAAI人工智能会议 (AAAI)*，2010年。
- en: Su & Zhang (2023) Jianhai Su and Qi Zhang. Subgoal proposition using a vision-language
    model. In *CoRL 2023 Workshop on Learning Effective Abstractions for Planning
    (LEAP)*, 2023.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su & Zhang (2023) Jianhai Su 和 Qi Zhang。使用视觉-语言模型进行子目标提出。在 *CoRL 2023 规划有效抽象学习研讨会
    (LEAP)*，2023年。
- en: Su et al. (2021) Jianyu Su, Stephen Adams, and Peter Beling. Value-decomposition
    multi-agent actor-critics. In *Proceedings of the AAAI conference on artificial
    intelligence*, volume 35, pp.  11352–11360, 2021.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su et al. (2021) Jianyu Su, Stephen Adams 和 Peter Beling。价值分解多智能体演员-评论家。 在 *AAAI人工智能会议论文集*，第35卷，第11352-11360页，2021年。
- en: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus, et al. Learning multiagent
    communication with backpropagation. In *Advances in neural information processing
    systems (NeurIPS)*, 2016.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sukhbaatar et al. (2016) Sainbayar Sukhbaatar, Rob Fergus 等人。使用反向传播学习多智能体通信。在
    *神经信息处理系统进展 (NeurIPS)*，2016年。
- en: Sunehag et al. (2018) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian
    Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat,
    Joel Z Leibo, Karl Tuyls, et al. Value-decomposition networks for cooperative
    multi-agent learning based on team reward. In *International Conference on Autonomous
    Agents and MultiAgent Systems (AAMAS)*, 2018.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sunehag et al. (2018) Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian
    Czarnecki, Vinícius Flores Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat,
    Joel Z Leibo, Karl Tuyls 等人。基于团队奖励的合作多智能体学习的价值分解网络。在 *国际自主智能体与多智能体系统会议 (AAMAS)*，2018年。
- en: Suttle et al. (2020) Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang,
    Tamer Başar, and Ji Liu. A multi-agent off-policy actor-critic algorithm for distributed
    reinforcement learning. *IFAC-PapersOnLine*, 53(2):1549–1554, 2020.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suttle et al. (2020) Wesley Suttle, Zhuoran Yang, Kaiqing Zhang, Zhaoran Wang,
    Tamer Başar 和 Ji Liu。用于分布式强化学习的多智能体离策略演员-评论家算法。*IFAC-PapersOnLine*，53(2)：1549-1554，2020年。
- en: 'Sutton & Barto (2018) Richard S Sutton and Andrew G Barto. *Reinforcement learning:
    An introduction*. MIT press, 2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton & Barto (2018) Richard S Sutton 和 Andrew G Barto。*强化学习：一种介绍*。麻省理工学院出版社，2018年。
- en: 'Tan (1993) Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative
    agents. In *International Conference on Machine Learning (ICML)*, 1993.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan (1993) Ming Tan。多智能体强化学习：独立智能体与合作智能体。在 *国际机器学习会议 (ICML)*，1993年。
- en: Tang et al. (2018) Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang
    Zhang, Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan, et al.
    Hierarchical deep multiagent reinforcement learning with temporal abstraction.
    *arXiv preprint arXiv:1809.09332*, 2018.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang et al. (2018) Hongyao Tang, Jianye Hao, Tangjie Lv, Yingfeng Chen, Zongzhang
    Zhang, Hangtian Jia, Chunxu Ren, Yan Zheng, Zhaopeng Meng, Changjie Fan 等人。具有时间抽象的层次深度多智能体强化学习。*arXiv
    预印本 arXiv:1809.09332*，2018年。
- en: Thrun (1992) Sebastian B Thrun. *Efficient exploration in reinforcement learning*.
    Carnegie Mellon University, 1992.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thrun (1992) Sebastian B Thrun。*强化学习中的高效探索*。卡内基梅隆大学，1992年。
- en: Tian et al. (2020) Yuandong Tian, Qucheng Gong, and Tina Jiang. Joint policy
    search for multi-agent collaboration with imperfect information. *arXiv preprint
    arXiv:2008.06495*, 2020.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian et al. (2020) Yuandong Tian, Qucheng Gong 和 Tina Jiang。针对信息不完全的多智能体协作的联合策略搜索。*arXiv
    预印本 arXiv:2008.06495*，2020年。
- en: 'Todorov et al. (2012) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A
    physics engine for model-based control. In *2012 IEEE/RSJ international conference
    on intelligent robots and systems*, pp.  5026–5033\. IEEE, 2012.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Todorov et al. (2012) Emanuel Todorov, Tom Erez 和 Yuval Tassa。Mujoco：一种用于基于模型控制的物理引擎。在
    *2012 IEEE/RSJ国际智能机器人与系统会议*，第5026-5033页。IEEE，2012年。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron等人（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar等人. Llama: 开放且高效的基础语言模型。*arXiv预印本 arXiv:2302.13971*, 2023。'
- en: Tseng et al. (2022) Wei-Cheng Tseng, Tsun-Hsuan Wang, Yen-Chen Lin, and Phillip
    Isola. Offline multi-agent reinforcement learning with knowledge distillation.
    In *Advances in Neural Information Processing Systems*, 2022.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tseng等人（2022）Wei-Cheng Tseng, Tsun-Hsuan Wang, Yen-Chen Lin, 和 Phillip Isola.
    离线多智能体强化学习与知识蒸馏。发表于*神经信息处理系统进展*，2022。
- en: Van der Maaten & Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. Visualizing
    data using t-sne. *Journal of machine learning research*, 9(11), 2008.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van der Maaten & Hinton（2008）Laurens Van der Maaten 和 Geoffrey Hinton. 使用t-SNE可视化数据。*机器学习研究期刊*,
    9(11), 2008。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Advances in neural information processing systems (NeurIPS)*,
    2017.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani等人（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即一切。发表于*神经信息处理系统进展（NeurIPS）*，2017。
- en: Wan et al. (2021) Lipeng Wan, Zeyang Liu, Xingyu Chen, Han Wang, and Xuguang
    Lan. Greedy-based value representation for optimal coordination in multi-agent
    reinforcement learning. *arXiv preprint arXiv:2112.04454*, 2021.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan等人（2021）Lipeng Wan, Zeyang Liu, Xingyu Chen, Han Wang, 和 Xuguang Lan. 基于贪心的价值表示用于多智能体强化学习中的最优协调。*arXiv预印本
    arXiv:2112.04454*, 2021。
- en: Wang et al. (2024) Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu,
    Wenming Yang, and Xiangyang Ji. Llm-empowered state representation for reinforcement
    learning. *arXiv preprint arXiv:2407.13237*, 2024.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2024）Boyuan Wang, Yun Qu, Yuhang Jiang, Jianzhun Shao, Chang Liu, Wenming
    Yang, 和 Xiangyang Ji. LLM增强的强化学习状态表示。*arXiv预印本 arXiv:2407.13237*, 2024。
- en: 'Wang et al. (2023a) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023a.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2023a）Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. Voyager: 一种具有大语言模型的开放式体现型智能体。*arXiv预印本
    arXiv:2305.16291*, 2023a。'
- en: 'Wang et al. (2020a) Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, and Chongjie
    Zhang. Qplex: Duplex dueling multi-agent q-learning. *arXiv preprint arXiv:2008.01062*,
    2020a.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2020a）Jianhao Wang, Zhizhou Ren, Terry Liu, Yang Yu, 和 Chongjie Zhang.
    Qplex: 双向对决多智能体Q学习。*arXiv预印本 arXiv:2008.01062*, 2020a。'
- en: Wang et al. (2023b) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. A survey on large
    language model based autonomous agents. *arXiv preprint arXiv:2308.11432*, 2023b.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023b）Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin等人. 基于大语言模型的自主智能体综述。*arXiv预印本
    arXiv:2308.11432*, 2023b。
- en: 'Wang et al. (2020b) Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, and Zinovi
    Rabinovich. Learning efficient multi-agent communication: An information bottleneck
    approach. In *International Conference on Machine Learning*, pp. 9908–9918\. PMLR,
    2020b.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2020b）Rundong Wang, Xu He, Runsheng Yu, Wei Qiu, Bo An, 和 Zinovi Rabinovich.
    学习高效的多智能体通信：信息瓶颈方法。发表于*国际机器学习会议*，pp. 9908–9918。PMLR, 2020b。
- en: Wang et al. (2019a) Tonghan Wang, Jianhao Wang, Yi Wu, and Chongjie Zhang. Influence-based
    multi-agent exploration. *arXiv preprint arXiv:1910.05512*, 2019a.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2019a）Tonghan Wang, Jianhao Wang, Yi Wu, 和 Chongjie Zhang. 基于影响的多智能体探索。*arXiv预印本
    arXiv:1910.05512*, 2019a。
- en: Wang et al. (2019b) Tonghan Wang, Jianhao Wang, Chongyi Zheng, and Chongjie
    Zhang. Learning nearly decomposable value functions via communication minimization.
    *arXiv preprint arXiv:1910.05366*, 2019b.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2019b）Tonghan Wang, Jianhao Wang, Chongyi Zheng, 和 Chongjie Zhang. 通过通信最小化学习近似可分解的价值函数。*arXiv预印本
    arXiv:1910.05366*, 2019b。
- en: 'Wang et al. (2020c) Tonghan Wang, Heng Dong, Victor Lesser, and Chongjie Zhang.
    Roma: Multi-agent reinforcement learning with emergent roles. In *International
    Conference on Machine Learning (ICML)*, 2020c.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang等人（2020c）Tonghan Wang, Heng Dong, Victor Lesser, 和 Chongjie Zhang. Roma:
    具有新兴角色的多智能体强化学习。发表于*国际机器学习会议（ICML）*，2020c。'
- en: 'Wang et al. (2020d) Tonghan Wang, Tarun Gupta, Anuj Mahajan, Bei Peng, Shimon
    Whiteson, and Chongjie Zhang. Rode: Learning roles to decompose multi-agent tasks.
    *arXiv preprint arXiv:2010.01523*, 2020d.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2020d) 王同瀚, Tarun Gupta, Anuj Mahajan, 彭蓓, Shimon Whiteson, 和
    张崇杰. Rode: 学习角色以分解多智能体任务. *arXiv 预印本 arXiv:2010.01523*, 2020d.'
- en: 'Wang et al. (2020e) Weixun Wang, Tianpei Yang, Yong Liu, Jianye Hao, Xiaotian
    Hao, Yujing Hu, Yingfeng Chen, Changjie Fan, and Yang Gao. From few to more: Large-scale
    dynamic multiagent curriculum learning. In *AAAI Conference on Artificial Intelligence
    (AAAI)*, 2020e.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020e) 王伟勋, 杨天培, 刘勇, 郝建业, 郝晓天, 胡雨静, 陈英峰, 范昌杰, 和 高杨. 从少到多：大规模动态多智能体课程学习.
    在 *人工智能 AAAI 大会 (AAAI)*, 2020e.
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in neural information processing
    systems*, 35:24824–24837, 2022.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Jason Wei, 王雪志, Dale Schuurmans, Maarten Bosma, 夏飞, Ed Chi,
    Quoc V Le, Denny Zhou, 等. 思维链提示在大规模语言模型中引发推理. *神经信息处理系统进展*, 35:24824–24837, 2022.
- en: Wu et al. (2022) Jialong Wu, Haixu Wu, Zihan Qiu, Jianmin Wang, and Mingsheng
    Long. Supported policy optimization for offline reinforcement learning. In *Advances
    in Neural Information Processing Systems*, 2022.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022) 吴家龙, 吴海旭, 邱子涵, 王建民, 和 龙鸣胜. 支持的策略优化用于离线强化学习. 在 *神经信息处理系统进展*,
    2022.
- en: Wu et al. (2019) Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized
    offline reinforcement learning. *arXiv preprint arXiv:1911.11361*, 2019.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) 余凡 吴, 乔治 Tucker, 和 Ofir Nachum. 行为正则化离线强化学习. *arXiv 预印本 arXiv:1911.11361*,
    2019.
- en: Wu et al. (2021) Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua Susskind,
    Jian Zhang, Ruslan Salakhutdinov, and Hanlin Goh. Uncertainty weighted actor-critic
    for offline reinforcement learning. *arXiv preprint arXiv:2105.08140*, 2021.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) 吴越, 翟双飞, Srivastava Nitish, Joshua Susskind, 张建, Salakhutdinov
    Ruslan, 和 吴汉林. 离线强化学习的基于不确定性加权的演员-评论家方法. *arXiv 预印本 arXiv:2105.08140*, 2021.
- en: Xie et al. (2021) Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and
    Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning.
    *Advances in neural information processing systems*, 34, 2021.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2021) 谷腾扬, 程清安, 蒋楠, 保罗 Mineiro, 和 Alekh Agarwal. 离线强化学习中的 Bellman
    一致悲观性. *神经信息处理系统进展*, 34, 2021.
- en: 'Xie et al. (2023) Tianbao Xie, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian
    Luo, Victor Zhong, Yanchao Yang, and Tao Yu. Text2reward: Automated dense reward
    function generation for reinforcement learning. *arXiv preprint arXiv:2309.11489*,
    2023.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie et al. (2023) 谷天宝, 赵思恒, 吴陈亨利, 刘逸涛, 罗千, 钟维克, 杨彦超, 和 于涛. Text2reward: 强化学习的自动化密集奖励函数生成.
    *arXiv 预印本 arXiv:2309.11489*, 2023.'
- en: Xu et al. (2023) Pei Xu, Junge Zhang, Qiyue Yin, Chao Yu, Yaodong Yang, and
    Kaiqi Huang. Subspace-aware exploration for sparse-reward multi-agent tasks. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37, pp. 
    11717–11725, 2023.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023) 许培, 张君阁, 尹奇跃, 余超, 杨耀东, 和 黄凯琪. 稀疏奖励多智能体任务的子空间感知探索. 在 *人工智能 AAAI
    大会论文集*, 第 37 卷, 第 11717–11725 页, 2023.
- en: Xu et al. (2018) Zhao Xu, Yang Lyu, Quan Pan, Jinwen Hu, Chunhui Zhao, and Shuai
    Liu. Multi-vehicle flocking control with deep deterministic policy gradient method.
    In *International Conference on Control and Automation (ICCA)*, 2018.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2018) 许赵, 吕阳, 潘全, 胡锦文, 赵春辉, 和 刘帅. 使用深度确定性策略梯度方法的多车队控制. 在 *国际控制与自动化会议
    (ICCA)*, 2018.
- en: 'Xu & Tewari (2020) Ziping Xu and Ambuj Tewari. Reinforcement learning in factored
    mdps: Oracle-efficient algorithms and tighter regret bounds for the non-episodic
    setting. *arXiv preprint arXiv:2002.02302*, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu & Tewari (2020) 许子平 和 Ambuj Tewari. 在因子化 MDP 中的强化学习：面向非 episodic 设置的 oracle
    高效算法及更紧的后悔界限. *arXiv 预印本 arXiv:2002.02302*, 2020.
- en: 'Yang et al. (2020a) Yaodong Yang, Jianye Hao, Ben Liao, Kun Shao, Guangyong
    Chen, Wulong Liu, and Hongyao Tang. Qatten: A general framework for cooperative
    multiagent reinforcement learning. *arXiv preprint arXiv:2002.03939*, 2020a.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2020a) 杨耀东, 郝建业, 廖本, 邵昆, 陈广勇, 刘武龙, 和 唐红耀. Qatten: 一种协作多智能体强化学习的通用框架.
    *arXiv 预印本 arXiv:2002.03939*, 2020a.'
- en: Yang et al. (2020b) Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao,
    David Mguni, and Weinan Zhang. Multi-agent determinantal q-learning. In *International
    Conference on Machine Learning (ICML)*, 2020b.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2020b) 杨耀东, 温莹, 王俊, 陈理恒, 邵昆, David Mguni, 和 张伟南. 多智能体行列式 Q 学习.
    在 *国际机器学习大会 (ICML)*, 2020b.
- en: 'Yang et al. (2021) Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan
    Zhang, Gao Huang, Jun Yang, and Qianchuan Zhao. Believe what you see: Implicit
    constraint approach for offline multi-agent reinforcement learning. *Advances
    in Neural Information Processing Systems*, 34:10299–10312, 2021.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人（2021）Yiqin Yang, Xiaoteng Ma, Chenghao Li, Zewu Zheng, Qiyuan Zhang,
    Gao Huang, Jun Yang, 和 Qianchuan Zhao。相信你所看到的：一种隐式约束方法用于离线多智能体强化学习。*神经信息处理系统进展*，34:10299–10312，2021年。
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2022）Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik
    Narasimhan, 和 Yuan Cao。React：在语言模型中协同推理与行动。*arXiv预印本 arXiv:2210.03629*，2022年。
- en: 'Yao et al. (2023) Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei
    Liu, Yihao Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit,
    et al. Retroformer: Retrospective large language agents with policy gradient optimization.
    *arXiv preprint arXiv:2308.02151*, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2023）Weiran Yao, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Yihao
    Feng, Le Xue, Rithesh Murthy, Zeyuan Chen, Jianguo Zhang, Devansh Arpit, 等人。Retroformer：具有策略梯度优化的回顾性大语言模型智能体。*arXiv预印本
    arXiv:2308.02151*，2023年。
- en: Ye et al. (2020) Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu,
    Jia Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, et al. Towards playing full moba
    games with deep reinforcement learning. *arXiv preprint arXiv:2011.12692*, 2020.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人（2020）Deheng Ye, Guibin Chen, Wen Zhang, Sheng Chen, Bo Yuan, Bo Liu, Jia
    Chen, Zhao Liu, Fuhao Qiu, Hongsheng Yu, 等人。通过深度强化学习向完全MOBA游戏进发。*arXiv预印本 arXiv:2011.12692*，2020年。
- en: Yu et al. (2021) Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen,
    and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games.
    *arXiv preprint arXiv:2103.01955*, 2021.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2021）Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, 和
    Yi Wu。mappo在合作性多智能体游戏中的惊人效果。*arXiv预印本 arXiv:2103.01955*，2021年。
- en: 'Yu et al. (2020) Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James
    Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy
    optimization. *arXiv preprint arXiv:2005.13239*, 2020.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2020）Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey
    Levine, Chelsea Finn, 和 Tengyu Ma。Mopo：基于模型的离线策略优化。*arXiv预印本 arXiv:2005.13239*，2020年。
- en: Yu & Zhou (2008) Yang Yu and Zhi-Hua Zhou. A new approach to estimating the
    expected first hitting time of evolutionary algorithms. *Artificial Intelligence*,
    172(15):1809–1832, 2008.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu & Zhou（2008）Yang Yu 和 Zhi-Hua Zhou。一种估算进化算法期望首次到达时间的新方法。*人工智能*，172(15):1809–1832，2008年。
- en: Yuan et al. (2022) Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang
    Zhang, Yang Yu, and Chongjie Zhang. Multi-agent incentive communication via decentralized
    teammate modeling. 2022.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等人（2022）Lei Yuan, Jianhao Wang, Fuxiang Zhang, Chenghe Wang, Zongzhang Zhang,
    Yang Yu, 和 Chongjie Zhang。通过去中心化队友建模进行多智能体激励通信。2022年。
- en: Zanette et al. (2021) Andrea Zanette, Martin J Wainwright, and Emma Brunskill.
    Provable benefits of actor-critic methods for offline reinforcement learning.
    *Advances in neural information processing systems*, 34, 2021.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zanette等人（2021）Andrea Zanette, Martin J Wainwright, 和 Emma Brunskill。证明演员-评论家方法在离线强化学习中的优势。*神经信息处理系统进展*，34，2021年。
- en: Zhang & Lesser (2013) Chongjie Zhang and Victor R Lesser. Coordinating multi-agent
    reinforcement learning with limited communication. In *International Conference
    on Autonomous Agents and Multiagent Systems (AAMAS)*, 2013.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang & Lesser（2013）Chongjie Zhang 和 Victor R Lesser。利用有限通信协调多智能体强化学习。在*国际自主智能体与多智能体系统会议（AAMAS）*，2013年。
- en: 'Zhang et al. (2023) Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang
    Ren, Minsuk Chang, Shao-Hua Sun, and Joseph J Lim. Bootstrap your own skills:
    Learning to solve new tasks with large language model guidance. *arXiv preprint
    arXiv:2310.10021*, 2023.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2023）Jesse Zhang, Jiahui Zhang, Karl Pertsch, Ziyi Liu, Xiang Ren, Minsuk
    Chang, Shao-Hua Sun, 和 Joseph J Lim。提升你自己的技能：在大型语言模型指导下学习解决新任务。*arXiv预印本 arXiv:2310.10021*，2023年。
- en: Zhang et al. (2018) Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer
    Basar. Fully decentralized multi-agent reinforcement learning with networked agents.
    In *International Conference on Machine Learning (ICML)*, 2018.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人（2018）Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, 和 Tamer Basar。完全去中心化的多智能体强化学习与网络化智能体。在*国际机器学习会议（ICML）*，2018年。
- en: Zhao et al. (2023) Zirui Zhao, Wee Sun Lee, and David Hsu. Large language models
    as commonsense knowledge for large-scale task planning. *arXiv preprint arXiv:2305.14078*,
    2023.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao等人（2023）Zirui Zhao, Wee Sun Lee, 和 David Hsu。将大语言模型作为常识知识用于大规模任务规划。*arXiv预印本
    arXiv:2305.14078*，2023年。
- en: Zhou et al. (2020) Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li, and Yuk Ying
    Chung. Learning implicit credit assignment for multi-agent actor-critic. *arXiv
    preprint arXiv:2007.02529*, 2020.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2020）Meng Zhou, Ziyu Liu, Pengwei Sui, Yixuan Li和Yuk Ying Chung。为多智能体演员-评论员模型学习隐式信用分配。*arXiv预印本
    arXiv:2007.02529*，2020年。
- en: Zhou et al. (2023) Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, and Bin Liu.
    Large language model as a policy teacher for training reinforcement learning agents.
    *arXiv preprint arXiv:2311.13373*, 2023.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2023）Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang和Bin Liu。大型语言模型作为强化学习智能体训练的策略教师。*arXiv预印本
    arXiv:2311.13373*，2023年。
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft:
    Generally capable agents for open-world enviroments via large language models
    with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*, 2023.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu等人（2023）Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu
    Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang等人。Ghost in the minecraft: 通过具有文本知识和记忆的大型语言模型为开放世界环境提供通用能力的智能体。*arXiv预印本
    arXiv:2305.17144*，2023年。'
- en: Appendix A Algorithm
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 算法
- en: 'This section includes the pseudo algorithms. Algorithm [1](https://arxiv.org/html/2410.02511v1#algorithm1
    "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration") presents LEMAE’s main algorithm. LEMAE consists
    of four phases: generating discriminator functions with LLM, exploring with KSMT,
    calculating SHIR, and performing RL training. For on-policy RL, the buffer $D$
    corresponds to a rollout buffer, while for off-policy RL, it is initialized as
    a replay buffer (Paischer et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib117)).
    Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2 "In Appendix A Algorithm
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")
    illustrates the process of exploring with KSMT. As our approach is agnostic to
    reinforcement learning algorithms, we leave out the details of standard RL training
    in the main paper.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含伪算法。算法[1](https://arxiv.org/html/2410.02511v1#algorithm1 "在附录A算法 ‣ 选择比努力更重要：LLM使多智能体探索更高效")展示了LEMAE的主要算法。LEMAE由四个阶段组成：利用LLM生成判别器函数、通过KSMT进行探索、计算SHIR以及进行RL训练。对于在政策下的RL，缓冲区$D$对应于回滚缓冲区，而对于非政策RL，它被初始化为重放缓冲区（Paischer等人，[2022](https://arxiv.org/html/2410.02511v1#bib.bib117)）。算法[2](https://arxiv.org/html/2410.02511v1#algorithm2
    "在附录A算法 ‣ 选择比努力更重要：LLM使多智能体探索更高效")说明了通过KSMT进行探索的过程。由于我们的方法不依赖于特定的强化学习算法，因此在主文中省略了标准RL训练的详细信息。
- en: 'Input: Large language model $\mathcal{M}$, prompt $\mathcal{P}$, rethinking
    prompt $\mathcal{P}^{re}$, non-negative scaling factors $\alpha,\beta$, randomness
    epsilon $\epsilon_{l},\epsilon_{h}$ ($\epsilon_{l}<\epsilon_{h}$), training frequency
    $\mathcal{N}$, max episodes $\mathcal{N}^{max}$, key states numbers $\mathcal{K}$.Output:
    Policy network $\bm{\pi}_{\theta}$.Randomly initialize the policy network parameter
    $\theta$.Initialize key states memory tree $\mathcal{T}\leftarrow[root]$, replay
    buffer $\mathcal{D}$, key states chain replay buffer $\mathcal{D}^{ks}$Initial
    discriminator functions $\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(\mathcal{P})$;//
    Self-Check MechanismLLM rethinking $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(%
    \mathcal{P},\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}},\mathcal{P}^{re})$;while *there
    are non-executable discriminator functions in $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}$* do      
    $\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(% \mathcal{P},\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}},error)$end
    whileFinal discriminator functions $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$;for *$episode=1$
    to $\mathcal{N}^{max}$* do       // Explore with Key States Memory Tree (Algorithm [2](https://arxiv.org/html/2410.02511v1#algorithm2
    "In Appendix A Algorithm ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration"))       $\kappa\_chain$, $\mathcal{T}$, $\tau$
    $\leftarrow$ KSMT-Exp ($\bm{\pi}_{\theta}$, $\mathcal{T}$, $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$,
    $\epsilon_{l}$, $\epsilon_{h}$);       $\mathcal{D}\leftarrow\mathcal{D}\cup\{\tau\}$,
    $\mathcal{D}^{ks}\leftarrow\mathcal{D}^{ks}\cup\{\kappa\_chain\}$;       if *$episode$
    mod $\mathcal{N}$ = 0* then             Sample a batch $B=\{\tau_{i}\}_{i=1}^{|B|}$
    from $\mathcal{D}$ and the corresponding batch of key states chains $B^{ks}=\{\kappa\_chain_{i}\}_{i=1}^{|B|}$
    from $\mathcal{D}^{ks}$;             for *$\tau=\{(s_{t},\bm{a}_{t},s_{t+1},r_{t})\}_{t=1}^{t_{max}},\kappa\_chain\in
    B,B% ^{ks}$* do                   $t^{start}\leftarrow 1$;                   for *$\{t^{end},\kappa_{m}\}\in\kappa\_chain$* do                        
    for *$t=t^{start}$ to $t^{end}$* do                               // Subspace-based
    Hindsight Intrinsic Reward (equation [1](https://arxiv.org/html/2410.02511v1#S4.E1
    "In 4.3.1 Subspace-based Hindsight Intrinsic Reward ‣ 4.3 Key State-Guided Exploration
    ‣ 4 Method ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration"))                               Update $\tau$ with $r_{t}=\alpha\cdot
    r_{t}+\beta\cdot r_{I}^{\kappa_{m}}(s_{t},s_{t+1})$; $t^{start}\leftarrow t^{end}$;                        
    end for                   end for                  // Plan with Key States Memory
    Tree                   if *branch corresponding to $\kappa\_chain$ in $\mathcal{T}$* then                        
    Extract all children nodes $\mathcal{C}=\{\xi_{i}\}_{i=1}^{|\mathcal{C}|}$ and
    randomly sample a node $\xi_{i}\sim\mathcal{C}$;                   end if                  else                        
    Randomly sample a node $\xi_{i}$ from all nodes (not in $\kappa\_chain$) in $\mathcal{T}$;                  
    end if                  $\kappa_{plan}$ corresponds to the sampled node $\xi_{i}$;                  
    for *$t=t^{start}$ to $t^{max}$* do                         Update $\tau$ with
    $r_{t}=\alpha\cdot r_{t}+\beta\cdot r_{I}^{\kappa_{plan}}(s_{t},s_{t+1})$;                  
    end for                  Update $B$ with $\tau$;                   // RL Training
    (Algorithm Agnostic)                   Use an MARL algorithm to train $\theta$
    with $B$;             end for       end ifend for'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：大语言模型$\mathcal{M}$，提示$\mathcal{P}$，重新思考提示$\mathcal{P}^{re}$，非负缩放因子$\alpha,\beta$，随机性epsilon
    $\epsilon_{l},\epsilon_{h}$（$\epsilon_{l}<\epsilon_{h}$），训练频率$\mathcal{N}$，最大回合数$\mathcal{N}^{max}$，关键状态数量$\mathcal{K}$。输出：策略网络$\bm{\pi}_{\theta}$。随机初始化策略网络参数$\theta$。初始化关键状态记忆树$\mathcal{T}\leftarrow[root]$，回放缓冲区$\mathcal{D}$，关键状态链回放缓冲区$\mathcal{D}^{ks}$，初始鉴别器函数$\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(\mathcal{P})$；//
    自检机制 LLM重新思考$\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(\mathcal{P},\{\mathcal{\hat{F}}_{i}\}_{i=1}^{\mathcal{K}},\mathcal{P}^{re})$；当*在$\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}$中存在不可执行的鉴别器函数*时，$\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}}\leftarrow\mathcal{M}(\mathcal{P},\{\mathcal{\hat{F}}^{re}_{i}\}_{i=1}^{\mathcal{K}},error)$；结束循环；最终鉴别器函数$\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$；对于*$episode=1$
    到 $\mathcal{N}^{max}$*，执行以下操作：// 使用关键状态记忆树进行探索（算法[2](https://arxiv.org/html/2410.02511v1#algorithm2
    "附录A 算法 ‣ 选择比努力更重要：LLM实现高效多代理探索")）$\kappa\_chain$，$\mathcal{T}$，$\tau$ $\leftarrow$
    KSMT-Exp($\bm{\pi}_{\theta}$，$\mathcal{T}$，$\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$，$\epsilon_{l}$，$\epsilon_{h}$)；$\mathcal{D}\leftarrow\mathcal{D}\cup\{\tau\}$，$\mathcal{D}^{ks}\leftarrow\mathcal{D}^{ks}\cup\{\kappa\_chain\}$；如果*$episode$
    mod $\mathcal{N}$ = 0*，则：样本批次$B=\{\tau_{i}\}_{i=1}^{|B|}$来自$\mathcal{D}$，并从$\mathcal{D}^{ks}$中取出相应的关键状态链批次$B^{ks}=\{\kappa\_chain_{i}\}_{i=1}^{|B|}$；对于*$\tau=\{(s_{t},\bm{a}_{t},s_{t+1},r_{t})\}_{t=1}^{t_{max}},\kappa\_chain\in
    B,B^{ks}$*，执行以下操作：$t^{start}\leftarrow 1$；对于*$\{t^{end},\kappa_{m}\}\in\kappa\_chain$*，执行以下操作：对于*$t=t^{start}$
    到 $t^{end}$*，执行以下操作：// 基于子空间的后见内在奖励（方程[1](https://arxiv.org/html/2410.02511v1#S4.E1
    "4.3.1 基于子空间的后见内在奖励 ‣ 4.3 关键状态引导探索 ‣ 4 方法 ‣ 选择比努力更重要：LLM实现高效多代理探索")）更新$\tau$，使得$r_{t}=\alpha\cdot
    r_{t}+\beta\cdot r_{I}^{\kappa_{m}}(s_{t},s_{t+1})$；$t^{start}\leftarrow t^{end}$；结束循环；结束循环；//
    使用关键状态记忆树进行规划；如果*树中对应$\kappa\_chain$的分支*，则：提取所有子节点$\mathcal{C}=\{\xi_{i}\}_{i=1}^{|\mathcal{C}|}$，并随机采样一个节点$\xi_{i}\sim\mathcal{C}$；结束如果；否则：从树中所有节点中随机采样一个节点$\xi_{i}$（不在$\kappa\_chain$中）；结束否则；$\kappa_{plan}$对应于采样的节点$\xi_{i}$；对于*$t=t^{start}$
    到 $t^{max}$*，执行以下操作：更新$\tau$，使得$r_{t}=\alpha\cdot r_{t}+\beta\cdot r_{I}^{\kappa_{plan}}(s_{t},s_{t+1})$；结束循环；更新$B$，使得$B$包含$\tau$；//
    强化学习训练（算法无关）使用MARL算法，利用$B$训练$\theta$；结束循环；结束如果；结束循环。
- en: Algorithm 1 LEMAE
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 LEMAE
- en: 'Input: Policy network $\bm{\pi}_{\theta}$, key states memory tree $\mathcal{T}$,
    discriminator functions $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$, randomness
    epsilon $\epsilon_{l},\epsilon_{h}$ ($\epsilon_{l}<\epsilon_{h}$).Output: $\kappa\_chain$,
    key states memory tree $\mathcal{T}$, trajectory $\tau$.Initialize $\kappa\_chain\leftarrow\left[\
    \right],\tau\leftarrow\{\}$for *$t=1$ to $t_{\max}$* do       // key states localization
    in rollout trajectories       Discriminate $s_{t}$ with $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$;      
    if *($s_{t}$ is a key state $\kappa_{m}$) and ($\kappa_{m}$ not in $\kappa\_chain$)* then            
    $\kappa\_chain$.append($\{t,\kappa_{m}\}$);             if *branch corresponding
    to $\kappa\_chain$ not in $\mathcal{T}$* then                   // update KSMT                  
    add the branch into $\mathcal{T}$;             end if            else if *$\kappa_{m}$
    correponds to a non-leaf node $\xi$* then                   // mixed-randomness
    exploration strategy                   $d\leftarrow$ the degree of the node $\xi$;                  
    $p=\frac{1}{d+1}$;                   $\epsilon=\begin{cases}\epsilon_{h}&\text{with
    probability }p\\ \epsilon_{l}&\text{with probability }1-p\end{cases}$;            
    end if            else                   $\epsilon=\epsilon_{h}$;            
    end if       end if      With probability $\epsilon$ select a random action $\bm{a}_{t}$;      
    Otherwise select $\bm{a}_{t}\sim\bm{\pi}_{\theta}(s_{t})$;       Obtain a tuple
    $(s_{t},\bm{a}_{t},s_{t+1},r_{t})$ by executing $\bm{a}_{t}$;       $\tau\leftarrow\tau\cup\{(s_{t},\bm{a}_{t},s_{t+1},r_{t})\}$end
    for'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：策略网络 $\bm{\pi}_{\theta}$，关键状态记忆树 $\mathcal{T}$，鉴别函数 $\{\mathcal{F}_{i}\}_{i=1}^{\mathcal{K}}$，随机性
    epsilon $\epsilon_{l},\epsilon_{h}$（$\epsilon_{l}<\epsilon_{h}$）。输出：$\kappa\_chain$，关键状态记忆树
    $\mathcal{T}$，轨迹 $\tau$。初始化 $\kappa\_chain\leftarrow\left[\ \right],\tau\leftarrow\{\}$，对于
    *$t=1$ 到 $t_{\max}$* 执行：// 在回滚轨迹中进行关键状态定位
- en: Algorithm 2 Explore with Key States Memory Tree (KSMT-Exp)
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 使用关键状态记忆树进行探索（KSMT-Exp）
- en: Appendix B Further Discussions
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 进一步讨论
- en: B.1 The Insights Behind Key States Discrimination
  id: totrans-329
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 关键状态鉴别背后的洞察
- en: 'In our considered scenarios, we claim that discrimination is generally easier
    and more universal than key state generation by LLM, particularly in the context
    of high-dimensional states and partial observability. The reasons are as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们考虑的场景中，我们认为区分通常比通过大规模语言模型（LLM）生成关键状态更容易且更具普适性，尤其是在高维状态和部分可观察性的问题背景下。其原因如下：
- en: '1.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Discrimination focuses on high-level task understanding and identifying key
    state characteristics, while generation requires detailed, low-level comprehension,
    assigning values to each element. This makes generation more challenging and error-prone,
    particularly in high-dimensional settings. Discrimination equivalently simplifies
    the output space to key state labels, thus alleviating issues like hallucinations.
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鉴别侧重于任务的高层理解和识别关键状态特征，而生成需要对每个元素进行详细、低层次的理解并赋值。这使得生成变得更具挑战性且容易出错，尤其是在高维设置中。鉴别相当于将输出空间简化为关键状态标签，从而缓解了诸如幻觉等问题。
- en: '2.'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: In implementations, errors in discriminator functions are easier to examine
    and correct through testing with real states. In contrast, errors in generated
    key states are harder to detect and are typically inferred from training performance.
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在实现过程中，鉴别函数中的错误更容易通过真实状态的测试进行检查和修正。相比之下，生成的关键状态中的错误更难发现，通常需要通过训练表现来推断。
- en: '3.'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: In cases of partial observability, generating key states directly is unreliable.
    For example, in the Pass task, the positions of hidden switches are unknown and
    must be inferred from the door’s status. LLM cannot generate key states accurately
    without knowledge of the specific agents’ positions required to activate a switch.
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在部分可观察性的情况下，直接生成关键状态是不可靠的。例如，在Pass任务中，隐藏开关的位置是未知的，必须从门的状态推断出来。没有特定代理的位置，LLM无法准确生成关键状态，而这些位置对于激活开关至关重要。
- en: B.2 Limitations
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 限制
- en: 'We build a bridge between LLM and RL to facilitate efficient exploration by
    leveraging task-related guidance provided by LLM. However, persistent constraints
    inherent to LLMs, such as their limited capacity to comprehend task-specific information
    and the inevitable hallucination, become bottlenecks in our approach, which induces
    the following limitations:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在LLM和RL之间建立了一座桥梁，通过利用LLM提供的任务相关指导来促进高效探索。然而，LLM固有的持续性约束，如其有限的理解任务特定信息的能力和不可避免的幻觉，成为我们方法中的瓶颈，从而引发以下限制：
- en: '1.'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We mitigate heavy prompt engineering through the use of a standardized prompt
    template but the necessity persists for manually providing task information for
    LLM. Thus, we assume the availability of semantic meanings for symbolic states.
    This assumption is feasible, as these manually designed states have inherent meanings
    documented in task specifications (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92);
    Samvelyan et al., [2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)), and
    is no stronger than prior works requiring a state captioner (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35))
    or environment code (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)),
    which also involve manual fine-tuning or access to additional state information.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过使用标准化的提示模板来减少繁重的提示工程，但仍然需要手动为LLM提供任务信息。因此，我们假设符号状态的语义含义是可用的。这个假设是可行的，因为这些手动设计的状态在任务规格中有固有的含义（Liu
    et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92); Samvelyan et al.,
    [2019b](https://arxiv.org/html/2410.02511v1#bib.bib133)），并且不比先前的工作要求状态说明器（Du et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）或环境代码（Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)）更强，这些也涉及手动微调或访问额外的状态信息。
- en: '2.'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Constrained by the limitations of LLM’s capabilities and the inherent issue
    of hallucination, it may face challenges in directly providing effective key state
    priors for more complex tasks. Besides, due to our use of LLMs, this work primarily
    focuses on tasks with symbolic states. Future research could extend its application
    to more complicated tasks, e.g., image-based tasks, by employing advanced multi-modal
    LLMs. We provide an initial attempt to extend LEMAE beyond symbolic tasks in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration").'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '受限于LLM能力的局限性以及固有的幻觉问题，它可能在直接为更复杂的任务提供有效的关键状态先验方面面临挑战。此外，由于我们使用了LLM，本工作主要集中在具有符号状态的任务上。未来的研究可以通过使用先进的多模态LLM，扩展其在更复杂任务中的应用，例如基于图像的任务。我们在附录 [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration")中提供了将LEMAE扩展到符号任务之外的初步尝试。'
- en: Since the efficiency of the proposed LEMAE is essentially derived from versatile
    LLMs, we believe that the surge of foundation model exploration will flourish
    LLM-empowered RL.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提出的LEMAE的效率本质上来源于多功能LLM，我们相信基础模型探索的激增将促进LLM赋能的RL的蓬勃发展。
- en: B.3 Future Works
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 未来工作
- en: 'The success of the proposed LEMAE highlights the necessity and efficacy of
    empowering RL with LLM. To enhance performance and extend applicability, we will
    explore two avenues for future research aimed at addressing the identified limitations.
    These avenues are outlined as follows:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的LEMAE的成功突显了通过LLM赋能RL的必要性和有效性。为了提高性能并扩展适用性，我们将探索两条未来研究方向，旨在解决已识别的限制。这些方向概述如下：
- en: '1.'
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Streamlining the task information provision through multi-modal self-collection:
    Multi-modal LLMs are garnering increasing attention for their ability to comprehend
    situations through various modalities. Incorporating them with self-exploration
    and memory mechanisms shows promise in automating the collection and understanding
    of task information, thereby streamlining the implementation and enhancing the
    adaptability of LEMAE. We provide an initial attempt to extend LEMAE beyond symbolic
    tasks in Appendix [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2 "F.2 Extending
    LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental Results ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '通过多模态自我收集简化任务信息提供：多模态LLM因其通过各种模态理解情境的能力而受到越来越多的关注。将其与自我探索和记忆机制结合，显示出在自动化收集和理解任务信息方面的潜力，从而简化LEMAE的实施并增强其适应性。我们在附录 [F.2](https://arxiv.org/html/2410.02511v1#A6.SS2
    "F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration") 中提供了将LEMAE扩展到符号任务之外的初步尝试。'
- en: '2.'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Unleashing the power of better LLM with an iterative feedback mechanism: Undoubtedly,
    given the rapid pace of LLM development, the emergence of more powerful LLMs is
    imminent. On one hand, we intend to harness the capabilities of these advanced
    LLMs. On the other hand, to fully unleash the potential of LLMs, we plan to devise
    an iterative feedback mechanism to feedback LLM in LEMAE during RL training to
    mitigate issues like hallucinations and errors in task understanding.'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过迭代反馈机制释放更强大的LLM的潜力：毫无疑问，鉴于LLM发展的快速进展，更强大的LLM的出现已是迫在眉睫。一方面，我们打算利用这些先进LLM的能力。另一方面，为了充分释放LLM的潜力，我们计划设计一个迭代反馈机制，在RL训练中对LEMAE中的LLM进行反馈，以减轻幻觉和任务理解错误等问题。
- en: B.4 Broader Impacts
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 更广泛的影响
- en: Large Language Models have demonstrated considerable potential in showcasing
    impressive capabilities across various downstream tasks. However, research on
    empowering RL with LLMs is still nascent. As a pioneering endeavor to empower
    RL with LLM, we propose a general approach facilitating efficient exploration
    in RL with task-specific guidance from LLM.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在展示其在各种下游任务中的卓越能力方面展现了相当大的潜力。然而，赋能RL与LLM的研究仍处于初步阶段。作为赋能RL与LLM的开创性尝试，我们提出了一种通用方法，通过LLM提供任务特定的指导，促进RL中的高效探索。
- en: '1.'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: For the research community, the publication of this work will inspire further
    exploration into encouraging the integration of LLMs with RL to address the inherent
    challenges in RL, such as efficient exploration, limited sample efficiency, and
    unsatisfactory generalization. Additionally, our design promotes the application
    of discrimination and coding to ground linguistic knowledge from LLMs into symbolic
    tasks.
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于研究界而言，本文的发表将激励进一步探索推动LLM与RL的结合，以解决RL中固有的挑战，例如高效探索、有限的样本效率和不理想的泛化能力。此外，我们的设计促进了将区分性和编码应用于将LLM中的语言知识转化为符号任务。
- en: '2.'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: LEMAE shows promise for real-world deployment in scenarios requiring efficient
    exploration, such as autonomous vehicle control and robot manipulation. Moreover,
    as LLM is growing by leaps and bounds, it is foreseeable that LEMAE can be applied
    to more challenging real-world tasks by taking advantage of more powerful LLM.
    Notably, to mitigate potential risks, it is imperative to conduct LLM generation
    and RL training under human supervision, thereby ensuring undesirable outcomes
    are averted.
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LEMAE 在需要高效探索的实际场景中显示出潜力，例如自动驾驶控制和机器人操作。此外，随着大型语言模型（LLM）的快速发展，预计通过利用更强大的LLM，LEMAE能够应用于更具挑战性的实际任务。值得注意的是，为了降低潜在风险，必须在人工监督下进行LLM生成和强化学习（RL）训练，从而确保避免不良结果的发生。
- en: 'Appendix C Proof of Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1
    "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")'
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '附录 C 命题 [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1 "Proposition
    4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration") 的证明'
- en: Proof.
  id: totrans-357
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Random walk is a fundamental stochastic process, formed by successive summation
    of independent, identically distributed random variables (Lawler & Limic, [2010](https://arxiv.org/html/2410.02511v1#bib.bib77)).
    This work considers the one-dimensional asymmetric random walk problem, where
    an agent starts at $x=0$ and aims to reach $x=N\in\mathbb{N^{+}},N>1$. The expected
    first hitting time considered as the metric of performance, implying the average
    computational time complexity (Yu & Zhou, [2008](https://arxiv.org/html/2410.02511v1#bib.bib191)).
    Below is the proof of Proposition [4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1
    "Proposition 4.1\. ‣ 4.1 Devil is in the Key States ‣ 4 Method ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 随机游走是一个基本的随机过程，由一系列独立同分布的随机变量求和而成（Lawler & Limic, [2010](https://arxiv.org/html/2410.02511v1#bib.bib77)）。本文考虑了一维非对称随机游走问题，其中一个代理从$x=0$出发，目标是到达$x=N\in\mathbb{N^{+}},N>1$。期望的首次到达时间被视为性能度量，意味着平均计算时间复杂度（Yu
    & Zhou, [2008](https://arxiv.org/html/2410.02511v1#bib.bib191)）。以下是命题[4.1](https://arxiv.org/html/2410.02511v1#S4.Thmtheorem1
    "命题 4.1. ‣ 4.1 关键状态中的魔鬼 ‣ 4 方法 ‣ 选择比努力更重要：LLM促成高效的多代理探索")的证明。
- en: 'Firstly, we can prove the expected first hitting time within the default setting
    through the application of martingale theory. According to the problem setting,
    we can define the movement at each time step as: $M_{0}=0$, $M_{1},M_{2}...$ are
    i.i.d. random variables with distribution $P(M_{i}=1)=p,P(M_{i}=-1)=1-p,p\in(0.5,1)$.
    Then the position of agent after n steps can be represented as:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以通过应用鞅理论证明默认设置下的期望首次到达时间。根据问题设置，我们可以定义每一步的移动为：$M_{0}=0$，$M_{1},M_{2}...$是独立同分布的随机变量，其分布为$P(M_{i}=1)=p,P(M_{i}=-1)=1-p,p\in(0.5,1)$。那么代理在n步之后的位置可以表示为：
- en: '|  | $S_{n}=\sum_{i=1}^{n}M_{i},S_{0}=0$ |  | (3) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{n}=\sum_{i=1}^{n}M_{i},S_{0}=0$ |  | (3) |'
- en: 'However, because of the asymmetry of random variables $M_{i}$, $\{S_{n},n\geq
    0\}$ does not pertain to the martingale $w.r.t.$ $\{M_{n},n\geq 1\}$. It’s observed
    that $\mathbb{E}(M_{i})=2p-1,i\geq 1$. Then, we can define:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于随机变量$M_{i}$的非对称性，$\{S_{n},n\geq 0\}$与$\{M_{n},n\geq 1\}$不是关于鞅的。观察到$\mathbb{E}(M_{i})=2p-1,i\geq
    1$。因此，我们可以定义：
- en: '|  | $Y_{n}=\sum_{i=1}^{n}\left(M_{i}-(2p-1)\right),Y_{0}=0$ |  | (4) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y_{n}=\sum_{i=1}^{n}\left(M_{i}-(2p-1)\right),Y_{0}=0$ |  | (4) |'
- en: It’s easy to prove that
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 容易证明
- en: '|  | $\mathbb{E}&#124;Y_{n}&#124;=\sum_{i=1}^{n}E&#124;M_{i}&#124;-n(2p-1)=2n-2np<\infty$
    |  | (5) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}|Y_{n}|=\sum_{i=1}^{n}\mathbb{E}|M_{i}|-n(2p-1)=2n-2np<\infty$
    |  | (5) |'
- en: '|  | $\mathbb{E}(Y_{n+1}&#124;M_{0},M_{1},...M_{n})=Y_{n}+\mathbb{E}(M_{n+1})-(2p-1)=Y_{n}$
    |  | (6) |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}(Y_{n+1}|M_{0},M_{1},...M_{n})=Y_{n}+\mathbb{E}(M_{n+1})-(2p-1)=Y_{n}$
    |  | (6) |'
- en: So, according to the definition, $\{Y_{n},n\geq 0\}$ is a martingale $w.r.t.$
    $\{M_{n},n\geq 1\}$
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据定义，$\{Y_{n},n\geq 0\}$是关于$\{M_{n},n\geq 1\}$的鞅。
- en: Let $T_{0\rightarrow N}=min\{n:S_{0}=0,S_{n}=N\}=min\{n:Y_{0}=0,Y_{n}=N-n*(2p-1)\}$.
    It’s clear that $T_{0\rightarrow N}$ is a stopping time $w.r.t.$ $\{M_{n},n\geq
    1\}$.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 设$T_{0\rightarrow N}=\min\{n:S_{0}=0,S_{n}=N\}=\min\{n:Y_{0}=0,Y_{n}=N-n*(2p-1)\}$。显然，$T_{0\rightarrow
    N}$是关于$\{M_{n},n\geq 1\}$的停时。
- en: It’s easy to prove that
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 容易证明
- en: '|  | $\mathbb{E}\left(&#124;Y_{n+1}-Y_{n}&#124;&#124;M_{0},M_{1},...M_{n}\right)=\mathbb{E}\left(&#124;M%
    _{n+1}&#124;\right)-(2p-1)=2-2p<2$ |  | (7) |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}\left(|Y_{n+1}-Y_{n}||M_{0},M_{1},...M_{n}\right)=\mathbb{E}\left(|M_{n+1}|\right)-(2p-1)=2-2p<2$
    |  | (7) |'
- en: We can assume that $\mathbb{E}(T_{0\rightarrow N})<\infty$. Then, according
    to the Optional Stopping Theorem (Durrett, [2019](https://arxiv.org/html/2410.02511v1#bib.bib37)),
    we can get
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设$\mathbb{E}(T_{0\rightarrow N})<\infty$。然后，根据可选停止定理（Durrett, [2019](https://arxiv.org/html/2410.02511v1#bib.bib37)），我们可以得到
- en: '|  | $\mathbb{E}(Y_{T_{0\rightarrow N}})=N-\mathbb{E}(T_{0\rightarrow N})*(2p-1)=%
    \mathbb{E}(Y_{0})=0$ |  | (8) |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}(Y_{T_{0\rightarrow N}})=N-\mathbb{E}(T_{0\rightarrow N})*(2p-1)=\mathbb{E}(Y_{0})=0$
    |  | (8) |'
- en: Then
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '|  | $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$ |  | (9) |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}(T_{0\rightarrow N})=\frac{N}{2p-1}$ |  | (9) |'
- en: The assumption $\mathbb{E}(T_{0\rightarrow N})<\infty$ is thereby validated.
    Consequently, the expected first hitting time within the default setting is $\mathbb{E}(T_{0\rightarrow
    N})=\frac{N}{2p-1}$, a conclusion also articulated in Theorem 4.8.9 of Durrett
    ([2019](https://arxiv.org/html/2410.02511v1#bib.bib37)).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 假设$\mathbb{E}(T_{0\rightarrow N})<\infty$，由此得到验证。因此，在默认设置下，预期的首次到达时间为$\mathbb{E}(T_{0\rightarrow
    N})=\frac{N}{2p-1}$，这一结论也在Durrett的定理4.8.9中阐明（[2019](https://arxiv.org/html/2410.02511v1#bib.bib37)）。
- en: 'We can introduce the task-relevant information that the agent must first reach
    key states: $\kappa=1,...,N-1$ before progressing to $x=N$. It is presupposed
    that every time the agent achieves at $x=\kappa$, the policy where $x<\kappa$
    is updated to a deterministic rightward movement, i.e., $P(M_{x}=1)=1,x<\kappa$,
    thereby emulating the update process in Reinforcement Learning.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以引入任务相关的信息，代理首先必须到达关键状态：$\kappa=1,...,N-1$，然后才能进展到$x=N$。假设每次代理在$x=\kappa$处达成时，$x<\kappa$的策略会更新为确定性的向右移动，即$P(M_{x}=1)=1,x<\kappa$，从而模拟强化学习中的更新过程。
- en: 'The expected first hitting time from $x=0$ to $x=1$ is $\mathbb{E}(T_{0\rightarrow
    1})=\frac{1}{2p-1}$. After reaching $x=1$, the expected first hitting time from
    $x=1$ to $x=2$ can be calculated as:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 从$x=0$到$x=1$的预期首次到达时间为$\mathbb{E}(T_{0\rightarrow 1})=\frac{1}{2p-1}$。到达$x=1$后，从$x=1$到$x=2$的预期首次到达时间可以计算为：
- en: '|  | $\mathbb{E}(T^{prior}_{1\rightarrow 2})=p*\sum^{\infty}_{n=0}(2n+1)(1-p)=\frac{%
    2}{p}-1$ |  | (10) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}(T^{prior}_{1\rightarrow 2})=p*\sum^{\infty}_{n=0}(2n+1)(1-p)=\frac{%
    2}{p}-1$ |  | (10) |'
- en: Similarly, we can easily prove that
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以轻松证明
- en: '|  | $\mathbb{E}(T^{prior}_{1\rightarrow N})=(N-1)*\left(\frac{2}{p}-1\right)$
    |  | (11) |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}(T^{prior}_{1\rightarrow N})=(N-1)*\left(\frac{2}{p}-1\right)$
    |  | (11) |'
- en: Consequently, the expected first hitting time after the integration of priors
    becomes $\mathbb{E}(T^{prior}_{0\rightarrow N})=\mathbb{E}(T_{0\rightarrow 1})+\mathbb{%
    E}(T^{prior}_{1\rightarrow N})=\frac{1}{2p-1}+(N-1)*\left(\frac{2}{p}-1\right)$.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，整合先验信息后的预期首次到达时间为$\mathbb{E}(T^{prior}_{0\rightarrow N})=\mathbb{E}(T_{0\rightarrow
    1})+\mathbb{% E}(T^{prior}_{1\rightarrow N})=\frac{1}{2p-1}+(N-1)*\left(\frac{2}{p}-1\right)$。
- en: The total advantage resulting from the integration of appropriate priors is
    expressed as $\mathbb{E}(T_{0\rightarrow N})-\mathbb{E}(T^{prior}_{0\rightarrow
    N})=(N-1)*(% \frac{1}{2p-1}-\frac{2}{p}+1)>0,p\in(0.5,1),N\in\mathbb{N}^{+},N>1$
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 通过整合适当的先验信息所得到的总优势表示为$\mathbb{E}(T_{0\rightarrow N})-\mathbb{E}(T^{prior}_{0\rightarrow
    N})=(N-1)*(% \frac{1}{2p-1}-\frac{2}{p}+1)>0,p\in(0.5,1),N\in\mathbb{N}^{+},N>1$
- en: ∎
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Appendix D LLM Prompts and Responses
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D LLM提示和回应
- en: Here are the example prompt and response in our work. Please reference the code
    for further details. Notably, we adopt the chain-of-thought technique from Wei
    et al. ([2022](https://arxiv.org/html/2410.02511v1#bib.bib174)).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们工作中的示例提示和回应。请参考代码以获取更多细节。值得注意的是，我们采用了Wei等人（[2022](https://arxiv.org/html/2410.02511v1#bib.bib174)）的思维链技术。
- en: '<svg class="ltx_picture" height="539.53" id="A4.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,539.53) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 521.33)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SMAC Prompt and Response Example</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="489.83" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">SYSTEM: (Task_Description) We are playing StarCraft II
    micro scenario, tring to control our agents to defeat all of the enemy units.
    (State_Form) In each step, the current state is represented as a 1-dimensional
    list: $[nf\_al]*n\_agents+[nf\_en]*n\_enemies+[last\_actions]$. $nf\_al$ denotes
    the unit state for each agent with attributes $[health\_rate,weapon\_cooldown\_rate,relative\_x\_to\_map\_center,\\
    relative\_y\_to\_map\_center,shield\_rate$ (1 dimension if a_race is P else 0
    dimension), $unit\_type\_bits$ (the dimension is defined in the map config)$]$.
    $nf\_en$ represents the unit state for each enemy with attributes $[health\_rate,relative\_x\_to\_map\_center,relative\_y\_to\_map\_center,\\
    shield\_rate$ (1 dimension if b_race in map config is P else 0 dimension), $unit\_type\_bits$
    (the dimension is defined in the map config)$]$. The $last\_actions$ component
    does not require consideration. (Role_Instruction)(Template) Your role is to give
    several critical key states in the task which we should try to reach and generate
    the corresponding discriminator function for each key state which can discriminate
    if the input state has reached the key state. Note: 1\. Don’t use the information
    you are not told. 2\. The code should be as generic as possible. 3\. The discriminator
    functions for different key states should be independent. 4\. Your answer should
    be complete and not omitted. Please think step by step and adhere to the following
    JSON format (just replace the () with your answer): {<svg class="ltx_picture"
    height="904.82" id="A4.p3.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,904.82)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 13.78)"><foreignobject color="#000000" height="877.27" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Thought: (Please fully understand
    the task. Think step by step and give your thought about the concrete state here),
    Key states: { init: all agents and enemies are alive, key_state_1: (description),
    key_state_2: (description), …, success: all enemies are dead and at least one
    agent is alive }, Discriminators: { key_state_1: (a python function with the form
    of def iskeystate1(state): … return 0 or 1), key_state_2: (a python function with
    the form of def iskeystate2(state): … return 0 or 1), … } Subspace_states: { key_state_1:
    (extract subspace state used in corresponding discriminator functions, such as
    [1] or [2,5]), key_state_2: (extract subspace state used in corresponding discriminator
    functions, such as [1] or [2,5]), … } } USER: Task is 2m_vs_1z. The map config
    is: { n_agents: 2, n_enemies: 1, limit: 150, a_race: T, b_race: P, unit_type_bits:
    0, map_type: marines }. Give more than 2 key states and the corresponding discriminator
    functions. ASSISTANT: (LLM response) { Thought: Since a_race is T, there is no
    shield_rate in nf_al; b_race is P, so there is 1-dim shield_rate in nf_en; the
    dim of unit_type_bits is 0\. The dimension of nf_al = 4+0+0 = 4, the dimension
    of nf_en = 3+1+0 = 4\. The concrete state form is $[$agent_0_health_rate, agent_0_weapon_cooldown_rate,
    agent_0_relative_x_to_map_center, agent_0_relative_y_to_map_center, agent_1_health_rate,
    agent_1_weapon_cooldown_rate, agent_1_relative_x_to_map_center, agent_1_relative_y_to_map_center,
    enemy_0_health_rate, enemy_0_relative_x_to_map_center, enemy_0_relative_y_to_map_center,
    enemy_0_shield_rate, last_actions$]$. Key states should focus on the health and
    shield status of the enemy and the health of our agents., Key_states: { init:all
    agents and enemies are alive, key_state_1: enemy_0’s shield is significantly damaged,
    key_state_2: enemy_0 is significantly damaged, key_state_3: enemy_0 is about to
    die, success:all enemies are dead and at least one agent is alive }, Discriminators:
    { key_state_1: def iskeystate1(state): return 1 if state[11] <0.6 else 0, key_state_2:
    def iskeystate2(state): return 1 if state[8] <0.6 else 0, key_state_3: def iskeystate3(state):
    return 1 if state[8] <0.05 else 0, }<svg class="ltx_picture" height="257.25" id="A4.p4.pic1"
    overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,257.25) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="229.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Subspace_states:
    { key_state_1: [11], key_state_2: [8], key_state_3: [8], } } USER: (LLM Rethinking)
    Please carefully check your response to make sure it meets the task requirements
    and notes without including unnecessary details. Also, confirm that the discriminator
    functions do not use any undefined variables. ASSISTANT: (LLM response) …… (We
    have omitted the intermediate LLM outputs to maintain conciseness, retaining only
    the initial generation, as the differences between the initial and rechecked generations
    are minimal in the absence of errors.)<svg class="ltx_picture" height="561.67"
    id="A4.p5.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,561.67) matrix(1
    0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 543.46)"><foreignobject
    color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">MPE Prompt and Response Example</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="511.97" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">USER:
    (Task_Description) Two agents are in a 30x30 room with the coordinate of the point
    in the upper left corner is (0,0) and the point in the lower right corner is (29,29),
    separated by a wall with a door. The door can not be directly open. There is a
    hidden switch in each room (The switch coordinates are unknown and differ from
    the coordinate of the door.). The door will be open only when one of the switch
    is occupied by an agent otherwise close. Both agents begin in the left half of
    the room(agent0.x<15 and agent1.x<15). The task is considered successfully completed
    as long as both agents are located in the right half-room(agent0.x>15 and agent1.x>15).
    (State_Form) In each step we can get current state. The state is a 1-dim list:
    (agent0.x, agent0.y, agent1.x, agent1.y,door.open). (Role_Instruction)(Template)
    Your role is to give several critical key states in the task which we should try
    to reach and generate the corresponding discriminator function for each key state
    which can discriminate if the input state has reached the key state. Note: 1\.
    Don’t use the information you are not told. 2\. The code should be as generic
    as possible. 3\. The discriminator functions for different key states should be
    independent. 4\. Try not to put too much distance between the key states. Please
    think step by step and adhere to the following JSON format (just replace the ()
    with your answer): { Thought: (Please fully understand the task. Think step by
    step and give your thought about the concrete state here), Key states: { init:
    init state, key_state_1: (description), key_state_2: (description),<svg class="ltx_picture"
    height="841.17" id="A4.p6.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,841.17)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 13.78)"><foreignobject color="#000000" height="813.62" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">…, success: success state },
    Discriminators: { key_state_1: (a python function with the form of def iskeystate1(state):
    … return 0 or 1), key_state_2: (a python function with the form of def iskeystate2(state):
    … return 0 or 1), … }, Subspace_states: { key_state_1: (extract subspace state
    used in corresponding discriminator functions, such as [1] or [2,5]), key_state_2:
    (extract subspace state used in corresponding discriminator functions, such as
    [1] or [2,5]), … } } ASSISTANT: (LLM response) { Thought: To complete the task,
    we need to find the hidden switch in one of the rooms to open the door, then move
    both agents through the door to the right half of the room. We should establish
    key states that represent the discovery of the switch, the opening of the door,
    and the successful transition of both agents to the right half of the room. We’ll
    assume that agent0 is responsible for finding the switch and agent1 is responsible
    for moving through the door once it’s open., Key_states: { init:Initial state
    where both agents are in the left half of the room., key_state_1:Agent0 has found
    the switch and is standing on it, opening the door, key_state_2:Agent1 has moved
    through the open door to the right half of the room while agent0 is still on the
    switch, key_state_3:Agent0 has joined agent1 in the right half of the room },
    Discriminators: { key_state_1:def iskeystate1(state): return 1 if state[4] ==
    1 and state[0] >15 else 0, key_state_2:def iskeystate2(state): return 1 if state[4]
    == 1 and state[2] >15 and state[0] <15 else 0, key_state_3:def iskeystate3(state):
    return 1 if state[0] >15 and state[2] >15 else 0 },     Subspace_states: { key_state_1:
    [4,0], key_state_2: [0,2,4], key_state_3: [0,2], } } USER: (LLM Rethinking) Please
    carefully check your response to make sure it meets the task requirements and
    notes without including unnecessary details. Also, confirm that the discriminator
    functions do not use any undefined variables.<svg class="ltx_picture" height="93.98"
    id="A4.p7.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,93.98) matrix(1 0
    0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="66.42" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">ASSISTANT: (LLM response) …… (We have omitted the intermediate
    LLM outputs to maintain conciseness, retaining only the initial generation, as
    the differences between the initial and rechecked generations are minimal in the
    absence of errors.)</foreignobject></g></g></svg>![Refer to caption](img/c0ee1a5daa49eb9c86b253b1132b7e65.png)'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="539.53" id="A4.p2.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,539.53) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 521.33)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SMAC 提示和响应示例</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="489.83" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">系统：（任务描述）我们正在进行《星际争霸 II》微观场景的游戏，试图控制我们的代理人击败所有敌方单位。（状态表示）在每一步中，当前状态表示为一维列表：$[nf\_al]*n\_agents+[nf\_en]*n\_enemies+[last\_actions]$。$nf\_al$
    表示每个代理的单位状态，包含属性 $[health\_rate,weapon\_cooldown\_rate,relative\_x\_to\_map\_center,\\
    relative\_y\_to\_map\_center,shield\_rate$（如果 a_race 为 P，则为一维，否则为零维），$unit\_type\_bits$（维度在地图配置中定义）]$。$nf\_en$
    表示每个敌人的单位状态，包含属性 $[health\_rate,relative\_x\_to\_map\_center,relative\_y\_to\_map\_center,\\
    shield\_rate$（如果地图配置中的 b_race 为 P，则为一维，否则为零维），$unit\_type\_bits$（维度在地图配置中定义）]$。$last\_actions$
    组件不需要考虑。（角色指令）（模板）你的角色是提供任务中的几个关键状态，并为每个关键状态生成相应的判别函数，用以判别输入的状态是否已经达到该关键状态。注意：1.
    不要使用你未被告知的信息。 2. 代码应该尽可能通用。 3. 不同关键状态的判别函数应独立。 4. 你的回答应该完整，不要省略。请一步步思考，并遵循以下 JSON
    格式（仅用你的回答替换括号内的内容）： {<svg class="ltx_picture" height="904.82" id="A4.p3.pic1"
    overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,904.82) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="877.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">思考：（请完全理解任务，逐步思考并在此处描述具体状态），关键状态：{
    init: 所有代理和敌人都存活，key_state_1: （描述），key_state_2: （描述），……，success: 所有敌人死亡，至少一个代理存活}，判别函数：{
    key_state_1: （一个形式为 def iskeystate1(state): …… return 0 或 1 的 Python 函数），key_state_2:
    （一个形式为 def iskeystate2(state): …… return 0 或 1 的 Python 函数），……} 子空间状态：{ key_state_1:
    （提取在相应判别函数中使用的子空间状态，例如 [1] 或 [2,5]），key_state_2: （提取在相应判别函数中使用的子空间状态，例如 [1] 或
    [2,5]），…… } } 用户：任务是 2m_vs_1z。地图配置为：{ n_agents: 2, n_enemies: 1, limit: 150, a_race:
    T, b_race: P, unit_type_bits: 0, map_type: marines }。提供超过两个关键状态及其相应的判别函数。助手：（LLM
    响应） { 思考：由于 a_race 是 T，因此 nf_al 的维度为 4+0+0 = 4，而 nf_en 的维度为 3+1+0 = 4。因此，具体的状态形式为
    $[$agent_0_health_rate, agent_0_weapon_cooldown_rate, agent_0_relative_x_to_map_center,
    agent_0_relative_y_to_map_center, agent_1_health_rate, agent_1_weapon_cooldown_rate,
    agent_1_relative_x_to_map_center, agent_1_relative_y_to_map_center, enemy_0_health_rate,
    enemy_0_relative_x_to_map_center, enemy_0_relative_y_to_map_center, enemy_0_shield_rate,
    last_actions$]$。关键状态应关注敌人的健康和护盾状态以及我们代理的健康。关键状态：{ init: 所有代理和敌人都存活，key_state_1:
    敌人 0 的护盾受到严重损坏，key_state_2: 敌人 0 受到了严重伤害，key_state_3: 敌人 0 即将死亡，success: 所有敌人死亡，至少一个代理存活}，判别函数：{
    key_state_1: def iskeystate1(state): return 1 if state[11] <0.6 else 0, key_state_2:
    def iskeystate2(state): return 1 if state[8] <0.6 else 0, key_state_3: def iskeystate3(state):
    return 1 if state[8] <0.05 else 0, }<svg class="ltx_picture" height="257.25" id="A4.p4.pic1"
    overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000"
    stroke-width="0.4pt" transform="translate(0,257.25) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="229.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">子空间状态：{
    key_state_1: [11], key_state_2: [8], key_state_3: [8], } } 用户：（LLM 重新思考）请仔细检查你的回答，确保它符合任务要求和注意事项，不包含不必要的细节。同时，确认判别函数没有使用任何未定义的变量。助手：（LLM
    响应） ……（为了简洁起见，我们省略了中间的 LLM 输出，保留了初始生成的内容，因为在没有错误的情况下，初始生成与重新检查的生成差异最小。）<svg class="ltx_picture"
    height="561.67" id="A4.p5.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,561.67)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 543.46)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">MPE 提示和响应示例</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="511.97" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">用户：（任务描述）两个代理人在一个 30x30 的房间内，左上角的坐标为 (0,0)，右下角的坐标为 (29,29)，中间有一道门，门无法直接打开'
- en: 'Figure 9: The number of LLM-generated key state discriminator functions.'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：LLM生成的关键状态判别函数的数量。
- en: Notably, the number of key states is primarily determined by LLM. For each task,
    we only prompt LLM to prevent it from generating too few functions according to
    the complexity of the environment. Specifically, as detailed in Appendix D, we
    instruct LLM to generate several critical key states for MPE and more than 2*n_enemies
    critical key states for SMAC.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，关键状态的数量主要由LLM决定。对于每个任务，我们只需提示LLM，以防它根据环境的复杂性生成过少的函数。具体来说，如附录D所述，我们指示LLM为MPE生成若干个关键状态，并为SMAC生成超过2*n_enemies个关键状态。
- en: 'As shown in Fig. [9](https://arxiv.org/html/2410.02511v1#A4.F9 "Figure 9 ‣
    Appendix D LLM Prompts and Responses ‣ Choices are More Important than Efforts:
    LLM Enables Efficient Multi-Agent Exploration"), we summarize the number of LLM-generated
    key state discriminator functions. It is notable that the number of discriminator
    functions increases with the difficulty of the task or the number of interactive
    objects in the environment, which aligns with intuition.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[9](https://arxiv.org/html/2410.02511v1#A4.F9 "Figure 9 ‣ Appendix D LLM
    Prompts and Responses ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration")所示，我们总结了LLM生成的关键状态判别函数的数量。值得注意的是，判别函数的数量随着任务的难度或环境中互动对象的数量增加，这与直觉一致。'
- en: Additionally, we have omitted the intermediate LLM outputs to maintain conciseness
    in this section, retaining only the initial generation, as the differences between
    the initial and rechecked generations are minimal in the absence of errors.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了保持本节的简洁性，我们省略了中间的LLM输出，仅保留了初始生成的部分，因为在没有错误的情况下，初始生成与重新检查生成之间的差异最小。
- en: Appendix E Implementation Details
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 实现细节
- en: E.1 LEMAE and Baselines
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 LEMAE与基准方法
- en: 'LEMAE: Our code is based on the widely-used code framework pymarl2 at [https://github.com/hijkzzz/pymarl2](https://github.com/hijkzzz/pymarl2).
    In this study, we have integrated our method with several base algorithms IPPO,
    QMIX, QPLEX, and VMIX. Throughout the integration process, we have refrained from
    modifying the algorithmic implementation and have maintained consistency in parameters,
    including batch size, learning rate, and loss coefficients, in alignment with
    the configurations of the base algorithms.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: LEMAE：我们的代码基于广泛使用的代码框架pymarl2，位于[https://github.com/hijkzzz/pymarl2](https://github.com/hijkzzz/pymarl2)。在本研究中，我们将我们的方法与多个基础算法IPPO、QMIX、QPLEX和VMIX进行了集成。在整个集成过程中，我们避免修改算法实现，并保持与基础算法配置一致的参数，包括批大小、学习率和损失系数。
- en: 'EITI and EDTI (Wang et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)):
    We compare our method with EITI and EDTI on MPE tasks proposed in Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)).
    We use the experiment results reported in Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)),
    which found that these algorithms perform poor because a long rollout (512 steps
    × 32 processes) between model updates is used.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: EITI和EDTI（Wang等，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)）：我们将我们的方法与EITI和EDTI在Liu等人（[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）提出的MPE任务上进行了比较。我们使用了Liu等人（[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）报告的实验结果，研究发现这些算法表现较差，因为在模型更新之间使用了长时间的滚动（512步×32进程）。
- en: 'CMAE (Liu et al., [2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)):
    We compare our method with CMAE on MPE and SMAC tasks. On MPE tasks, the results
    of CMAE are reproduced using the publicly available code released by the authors
    at [https://github.com/IouJenLiu/CMAE](https://github.com/IouJenLiu/CMAE). As
    CMAE lacks an implementation for SMAC, we use the results reported in the original
    paper.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: CMAE（Liu等，[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）：我们将我们的方法与CMAE在MPE和SMAC任务上进行了比较。在MPE任务上，CMAE的结果通过使用作者公开发布的代码[https://github.com/IouJenLiu/CMAE](https://github.com/IouJenLiu/CMAE)进行了复现。由于CMAE缺乏SMAC的实现，我们使用了原始论文中报告的结果。
- en: 'MAVEN (Mahajan et al., [2019](https://arxiv.org/html/2410.02511v1#bib.bib100)):
    We use the code at [https://github.com/starry-sky6688/MARL-Algorithms](https://github.com/starry-sky6688/MARL-Algorithms),
    which contains pytorch implementations of various MARL algorithms on SMAC, like
    the choice in LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: MAVEN（Mahajan等，[2019](https://arxiv.org/html/2410.02511v1#bib.bib100)）：我们使用[https://github.com/starry-sky6688/MARL-Algorithms](https://github.com/starry-sky6688/MARL-Algorithms)上的代码，该代码包含了多个MARL算法在SMAC上的pytorch实现，就像在LAIES中选择的（Liu等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）。
- en: 'RODE (Wang et al., [2020d](https://arxiv.org/html/2410.02511v1#bib.bib172))
    and MASER (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56)):
    We utilized the publicly available code provided by the authors, accessible at
    [https://github.com/Jiwonjeon9603/MASER](https://github.com/Jiwonjeon9603/MASER)
    and [https://github.com/TonghanWang/RODE](https://github.com/TonghanWang/RODE),
    respectively. Default configurations were employed, and their suboptimal performance
    is also documented in LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: RODE（Wang等，[2020d](https://arxiv.org/html/2410.02511v1#bib.bib172)）和MASER（Jeon等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56)）：我们使用了作者提供的公开代码，分别可以通过[https://github.com/Jiwonjeon9603/MASER](https://github.com/Jiwonjeon9603/MASER)和[https://github.com/TonghanWang/RODE](https://github.com/TonghanWang/RODE)访问。采用了默认配置，其次优性能也在LAIES（Liu等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）中有记录。
- en: 'LAIES (Liu et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib90)):
    We employed the publicly accessible code provided by the authors, which can be
    accessed at [https://github.com/liuboyin/LAIES](https://github.com/liuboyin/LAIES).
    When conducting experiments on SMAC, we adhered to the default configurations
    and external states. Notably, the original LAIES paper evaluation did not include
    assessments on the MPE. Consequently, we integrated the MPE environment into the
    LAIES codebase, designating the external states to represent the door status or
    the position of the box.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: LAIES（Liu等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90)）：我们使用了作者提供的公开代码，可以通过[https://github.com/liuboyin/LAIES](https://github.com/liuboyin/LAIES)访问。当在SMAC上进行实验时，我们遵循了默认配置和外部状态。值得注意的是，原始的LAIES论文评估中未包括MPE的评估。因此，我们将MPE环境集成到LAIES代码库中，并将外部状态设置为表示门的状态或箱子的位置。
- en: 'ELLM (Du et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib35)): Since
    the tasks in this work have clearly defined goals, we minimize LLM inference costs
    by following the ELLM methodology but adapting its goal generation to occur only
    once at the start of the training. Consistent with the hyperparameters in the
    official codebase [https://github.com/yuqingd/ellm](https://github.com/yuqingd/ellm),
    we set the similarity threshold to 0.99, rewarding only when the goal is achieved.
    We rely on LLM-generated functions to verify goal achievement, which we found
    to be more effective than directly using semantic similarity-based rewards.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ELLM（Du等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib35)）：由于本工作的任务有明确的目标，我们通过遵循ELLM方法学，但将其目标生成仅限于训练开始时一次性完成，从而最小化LLM推理成本。与官方代码库中的超参数设置一致[https://github.com/yuqingd/ellm](https://github.com/yuqingd/ellm)，我们将相似性阈值设置为0.99，只有在目标实现时才给予奖励。我们依赖LLM生成的函数来验证目标是否达成，我们发现这种方法比直接使用基于语义相似性的奖励更有效。
- en: For all algorithms, we ensure the same environmental settings, including observation
    space, environment reward function, and so on.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有算法，我们确保相同的环境设置，包括观察空间、环境奖励函数等。
- en: E.2 Comparison with LLM Reward Design
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 与LLM奖励设计的比较
- en: 'We conduct additional experiments comparing LEMAE with a baseline called Eureka-si,
    which can be seen as a single-iteration variant of Eureka (Ma et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib98)),
    where LLM designs rewards directly. For fairness, we does not adopt evolutionary
    optimization in Eureka and use LLM to generate reward functions with the same
    role instructions as in Eureka, while maintaining designs like Self-Check as in
    LEMAE. As shown in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3 "Figure
    3 ‣ 5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration"), Eureka-si is comparable to LEMAE in simple tasks like
    Push-Box but fails in challenging tasks with characteristics like partial observability,
    such as Pass, where hidden switches make it difficult to design effective reward
    functions. In contrast, LEMAE consistently demonstrates impressive performance.
    Notably, comparing LEMAE with Eureka directly would be unfair since Eureka’s evolutionary
    search requires multiple training iterations and candidates, leading to significantly
    more sampling and training than LEMAE. Overall, LEMAE’s advantage over RL algorithms
    lies in incorporating prior knowledge from the LLM, and its advantage over other
    LLM-based methods is due to our designs for better LLM incorporation, such as
    utilizing discrimination, SHIR, and KSMT.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了额外的实验，将LEMAE与一个基准方法Eureka-si进行比较，Eureka-si可以看作是Eureka（Ma 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib98)）的单次迭代变种，其中LLM直接设计奖励。为了公平起见，我们没有在Eureka中采用进化优化，而是使用LLM生成具有与Eureka相同角色指令的奖励函数，同时保持像LEMAE中的Self-Check等设计。如图[3](https://arxiv.org/html/2410.02511v1#S5.F3
    "图 3 ‣ 5 实验 ‣ 选择比努力更重要：LLM使得高效的多智能体探索成为可能")所示，Eureka-si在像Push-Box这样的简单任务中与LEMAE可比，但在具有部分可观察性等特征的挑战性任务中失败，例如Pass任务，其中隐藏的开关使得设计有效的奖励函数变得困难。相比之下，LEMAE始终展现出令人印象深刻的表现。值得注意的是，直接比较LEMAE与Eureka是不公平的，因为Eureka的进化搜索需要多个训练迭代和候选，导致比LEMAE更多的采样和训练。总的来说，LEMAE相较于强化学习算法的优势在于结合了LLM的先验知识，而相较于其他基于LLM的方法，则是由于我们更好地融入LLM的设计，例如利用判别、SHIR和KSMT。
- en: E.3 Connection and Comparison with HER
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 与HER的连接与比较
- en: The proposed Key State-Guided Exploration is similar to Hindsight Experience
    Replay (HER) (Andrychowicz et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib8))
    in form, where key states and subgoals are certain states from sampled trajectories.
    However, unlike HER, which samples goals from memory using random or heuristic
    strategies and often struggles with shaped rewards, our method incorporates LLM
    priors for more targeted goal selection (key states localization). Additionally,
    the proposed KSMT and SHIR facilitate organized exploration and enhanced reward
    guidance.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 所提出的关键状态引导探索（Key State-Guided Exploration）在形式上类似于Hindsight Experience Replay
    (HER)（Andrychowicz 等，[2017](https://arxiv.org/html/2410.02511v1#bib.bib8)），其中关键状态和子目标是从采样轨迹中得到的某些状态。然而，与HER不同的是，HER通过随机或启发式策略从记忆中采样目标，并且经常在奖励塑形上遇到困难，我们的方法则结合了大语言模型（LLM）先验，用于更有针对性的目标选择（关键状态定位）。此外，所提出的KSMT和SHIR促进了有序探索和增强的奖励引导。
- en: 'We conduct additional experiments to further confirm the advantages of our
    method. We evaluate HER with IPPO as the backbone in MPE. We use the future strategy
    for goal selection, as proposed in the HER paper, and employ a reward function
    based on the Manhattan Distance, which we find to be the best match. However,
    as depicted in Fig. [3](https://arxiv.org/html/2410.02511v1#S5.F3 "Figure 3 ‣
    5 Experiments ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration"), HER does not perform well on both MPE tasks. This outcome
    suggests that the random sampling strategy for goals may not be sufficient, underscoring
    the importance of incorporating LLM priors for efficient exploration as we proposed.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了额外的实验，以进一步确认我们方法的优势。我们在MPE中使用IPPO作为主干网络评估HER。我们采用了HER论文中提出的未来策略进行目标选择，并使用基于曼哈顿距离的奖励函数，我们发现这是最佳匹配。然而，如图[3](https://arxiv.org/html/2410.02511v1#S5.F3
    "图 3 ‣ 5 实验 ‣ 选择比努力更重要：LLM使得高效的多智能体探索成为可能")所示，HER在两个MPE任务中的表现都不理想。这个结果表明，目标的随机采样策略可能不足以满足要求，突显了我们所提出的结合LLM先验进行高效探索的重要性。
- en: E.4 Tasks
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 任务
- en: E.4.1 Multiple-Particle Environment (MPE)
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.4.1 多粒子环境（MPE）
- en: The Multiple-Particle Environment serves as a widely-adopted benchmark for multi-agent
    scenarios. In this work, we employ tasks specifically crafted for evaluating multi-agent
    exploration, proposed by Wang et al. ([2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)).
    The implementation utilized in this study is based on the work by Liu et al. ([2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)).
    In this section, we provide details of the four sparse-reward tasks we adopted.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 多粒子环境（Multiple-Particle Environment）作为多代理场景的广泛采用基准。在本研究中，我们使用了专门为评估多代理探索而设计的任务，该任务由Wang等人提出（[2019a](https://arxiv.org/html/2410.02511v1#bib.bib169)）。本研究中使用的实现基于Liu等人（[2021b](https://arxiv.org/html/2410.02511v1#bib.bib92)）的工作。在本节中，我们提供了我们采用的四个稀疏奖励任务的详细信息。
- en: '$\bullet$ Pass: In the Pass task, depicted in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10
    "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")a, two agents are positioned in a room of 30
    x 30 grid. The room is divided into two halves by a wall featuring a door. Each
    half-room contains an invisible switch, the details of which are not contained
    in the state or prompt for LLM. The door permits passage only when one of the
    switches is occupied by an agent. Initially situated within the left half-room,
    both agents must cooperate to transfer to the right half-room. The external reward
    function is denoted as $r_{E}=I(two\ agents\ are\ in\ the\ right\ room)$, where
    $I$ represents the indicator function.'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ Pass: 在Pass任务中，如图[10](https://arxiv.org/html/2410.02511v1#A5.F10
    "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")a所示，两个代理被放置在一个30 x 30网格的房间内。房间通过一面带有门的墙分为两半。每个半房间内都有一个隐形开关，开关的详细信息不包含在LLM的状态或提示中。门只有在其中一个开关被代理占据时才允许通过。最初位于左半房间的两个代理必须合作才能转移到右半房间。外部奖励函数表示为$r_{E}=I(two\
    agents\ are\ in\ the\ right\ room)$，其中$I$表示指示函数。'
- en: '$\bullet$ Secret-Room: Secret-Room is an extension task of Pass. As illustrated
    in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10 "Figure 10 ‣ E.4.1 Multiple-Particle
    Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣ Choices are
    More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")b,
    the configuration comprises one sizable room on the left and three smaller rooms
    on the right, interconnected by three doors. Within each room, there is an invisible
    switch; notably, the switch in the left room has the capability to control all
    three doors, whereas each right room’s switch exclusively controls its respective
    door. The grid size is 25 x 25\. Two agents are initialized within the left room
    and are required to collaborate in order to transition to the real target room,
    which is the right room 2\. The external reward function is denoted as $r_{E}=I(two\
    agents\ are\ in\ the\ right\ room\ 2)$, where $I$ represents the indicator function.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ Secret-Room: Secret-Room是Pass任务的扩展任务。如图[10](https://arxiv.org/html/2410.02511v1#A5.F10
    "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")b所示，配置包括一个较大的左侧房间和三个较小的右侧房间，三个房间通过三扇门相互连接。每个房间内都有一个隐形开关；值得注意的是，左侧房间的开关可以控制所有三扇门，而右侧每个房间的开关仅控制其各自的门。网格大小为25
    x 25。两个代理最初位于左侧房间，需要合作才能转移到真正的目标房间，即右侧房间2。外部奖励函数表示为$r_{E}=I(two\ agents\ are\
    in\ the\ right\ room\ 2)$，其中$I$表示指示函数。'
- en: '$\bullet$ Push-Box: As depicted in Fig. [10](https://arxiv.org/html/2410.02511v1#A5.F10
    "Figure 10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")(c), two agents and a box are initially positioned
    within a 15 x 15 grid. To successfully move the box, both agents must simultaneously
    exert force in the same direction. The task is deemed accomplished when the box
    is successfully pushed to the wall. The external reward function is denoted as
    $r_{E}=I(the\ box\ is\ pushed\ to\ the\ wall)$, where $I$ represents the indicator
    function.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ Push-Box: 如图[10](https://arxiv.org/html/2410.02511v1#A5.F10 "Figure
    10 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation
    Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration")(c)所示，两个代理和一个盒子最初被放置在一个15 x 15网格内。为了成功移动盒子，两个代理必须同时在相同的方向上施加力量。任务被视为完成，当盒子成功推到墙上时。外部奖励函数表示为$r_{E}=I(the\
    box\ is\ pushed\ to\ the\ wall)$，其中$I$表示指示函数。'
- en: '$\bullet$ Large-Pass: Large-Pass is a direct extension task of Pass by enlarging
    the grid dimensions to 50 x 50, which makes it more challenging. The external
    reward function aligns with that of the Pass task.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 大通道：大通道是通过将网格维度扩展到50 x 50来直接扩展“通道”任务，这使得它更具挑战性。外部奖励函数与“通道”任务一致。
- en: 'The details of these tasks, including observation space and action space, are
    listed in Table [3](https://arxiv.org/html/2410.02511v1#A5.T3 "Table 3 ‣ E.4.1
    Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation Details
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '这些任务的详细信息，包括观察空间和动作空间，列在表格[3](https://arxiv.org/html/2410.02511v1#A5.T3 "Table
    3 ‣ E.4.1 Multiple-Particle Environment (MPE) ‣ E.4 Tasks ‣ Appendix E Implementation
    Details ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration")中。'
- en: 'Table 3: Details of MPE tasks'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表格3：MPE任务的详细信息
- en: '| MPE tasks | n_agents | observation space | state space | action space |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| MPE任务 | 智能体数 | 观察空间 | 状态空间 | 动作空间 |'
- en: '| Pass | 2 | 5 | 5 | 4 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 通道 | 2 | 5 | 5 | 4 |'
- en: '| Secret-Room | 2 | 5 | 5 | 4 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 秘密房间 | 2 | 5 | 5 | 4 |'
- en: '| Push-Box | 2 | 6 | 6 | 4 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| 推箱子 | 2 | 6 | 6 | 4 |'
- en: '| Large-Pass | 2 | 5 | 5 | 4 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| 大通道 | 2 | 5 | 5 | 4 |'
- en: '![Refer to caption](img/2eb34d14fbbe7e51ec82dd1469c43743.png)'
  id: totrans-419
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2eb34d14fbbe7e51ec82dd1469c43743.png)'
- en: 'Figure 10: MPE Tasks.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：MPE任务。
- en: E.4.2 StarCraftII Multi-Agent Challenge (SMAC)
  id: totrans-421
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.4.2 星际争霸II多智能体挑战（SMAC）
- en: 'StarCraftII Multi-Agent Challenge (SMAC) (Samvelyan et al., [2019a](https://arxiv.org/html/2410.02511v1#bib.bib132))
    is a widely-used benchmark in the realm of cooperative multi-agent reinforcement
    learning research (Rashid et al., [2018](https://arxiv.org/html/2410.02511v1#bib.bib128);
    Shao et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib138); Liu et al.,
    [2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Shao et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib139)).
    Derived from the renowned real-time strategy game StarCraft II, SMAC concentrates
    specifically on decentralized micromanagement scenarios rather than the full game.
    Typically, the tasks within SMAC adopt a dense-reward framework, wherein agents
    receive dense rewards for damage received, attacking and eliminating enemies.
    To promote the need for exploration, we adopt fully sparse-reward versions of
    tasks in SMAC where agents are solely rewarded upon the successful elimination
    of all enemies. The external reward function is denoted as $r_{E}=I(all\ enemies\
    are\ eliminated)$, where $I$ represents the indicator function. Notably, this
    sparse-reward setting differs from the sparse SMAC, which can be called semi-sparse
    SMAC, used in some previous studies (Jeon et al., [2022](https://arxiv.org/html/2410.02511v1#bib.bib56);
    Jo et al., [2023](https://arxiv.org/html/2410.02511v1#bib.bib61)), where agents
    are rewarded when one or all enemies die or when one ally dies. In addition, to
    validate the versatility of LEMAE across diverse scenarios, we conducted experiments
    on six maps with different difficulty and diverse agent numbers, as illustrated
    in Table [4](https://arxiv.org/html/2410.02511v1#A5.T4 "Table 4 ‣ E.4.2 StarCraftII
    Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")
    and Table [5](https://arxiv.org/html/2410.02511v1#A5.T5 "Table 5 ‣ E.4.2 StarCraftII
    Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix E Implementation Details ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration").
    We use the version of SC2.4.10. Please refer to the official document²²2[https://github.com/oxwhirl/smac/blob/master/docs/smac.md](https://github.com/oxwhirl/smac/blob/master/docs/smac.md)
    for more details.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '星际争霸II 多智能体挑战（SMAC）（Samvelyan 等，[2019a](https://arxiv.org/html/2410.02511v1#bib.bib132)）是协作多智能体强化学习研究领域中广泛使用的基准（Rashid
    等，[2018](https://arxiv.org/html/2410.02511v1#bib.bib128); Shao 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib138);
    Liu 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib90); Shao 等，[2024](https://arxiv.org/html/2410.02511v1#bib.bib139)）。SMAC
    源自著名的即时战略游戏《星际争霸II》，专注于去中心化的微操场景，而非完整的游戏。通常，SMAC 中的任务采用密集奖励框架，智能体因受到伤害、攻击并消灭敌人而获得密集奖励。为了促进探索的需求，我们在
    SMAC 中采用完全稀疏奖励版本的任务，其中智能体仅在成功消灭所有敌人时获得奖励。外部奖励函数表示为 $r_{E}=I(all\ enemies\ are\
    eliminated)$，其中 $I$ 代表指示函数。值得注意的是，这种稀疏奖励设置不同于某些先前研究中使用的稀疏 SMAC，可以称之为半稀疏 SMAC（Jeon
    等，[2022](https://arxiv.org/html/2410.02511v1#bib.bib56); Jo 等，[2023](https://arxiv.org/html/2410.02511v1#bib.bib61)），在这种设置中，智能体在一个或所有敌人死亡或一个盟友死亡时获得奖励。此外，为了验证
    LEMAE 在不同场景中的多样性，我们在六张具有不同难度和多样化智能体数量的地图上进行了实验，具体如表格 [4](https://arxiv.org/html/2410.02511v1#A5.T4
    "Table 4 ‣ E.4.2 StarCraftII Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration") 和表格 [5](https://arxiv.org/html/2410.02511v1#A5.T5
    "Table 5 ‣ E.4.2 StarCraftII Multi-Agent Challenge (SMAC) ‣ E.4 Tasks ‣ Appendix
    E Implementation Details ‣ Choices are More Important than Efforts: LLM Enables
    Efficient Multi-Agent Exploration")。我们使用的是 SC2.4.10 版本。有关更多详细信息，请参阅官方文档²²2[https://github.com/oxwhirl/smac/blob/master/docs/smac.md](https://github.com/oxwhirl/smac/blob/master/docs/smac.md)。'
- en: 'Table 4: SMAC tasks.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：SMAC 任务。
- en: '| Task | Ally Units | Enemy Units | Type | Difficulty |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 盟友单位 | 敌方单位 | 类型 | 难度 |'
- en: '| 1c3s5z | 1 Colossi, 3 Stalkers, 5 Zealots | 1 Colossi, 3 Stalkers, 5 Zealots
    | heterogeneous & symmetric | Easy |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 1c3s5z | 1 个巨像，3 名追猎者，5 个狂热者 | 1 个巨像，3 名追猎者，5 个狂热者 | 异质 & 对称 | 容易 |'
- en: '| 2m_vs_1z | 2 Marines | 1 Zealot | micro-trick: alternating fire | Easy |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| 2m_vs_1z | 2 名海军陆战队员 | 1 个狂热者 | 微操技巧：交替射击 | 容易 |'
- en: '| 3s_vs_5z | 3 Stalkers | 5 Zealots | micro-trick: kiting | Hard |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 3s_vs_5z | 3 名追猎者 | 5 个狂热者 | 微操技巧：风筝 | 难 |'
- en: '| 5m_vs_6m | 5 Marines | 6 Marines | homogeneous & asymmetric | Hard |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 5m_vs_6m | 5 名海军陆战队员 | 6 名海军陆战队员 | 同质 & 非对称 | 难 |'
- en: '| 3s5z_vs_3s6z | 3 Stalkers, 5 Zealots | 3 Stalkers, 6 Zealots | heterogeneous
    & asymmetric | Super-Hard |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| 3s5z_vs_3s6z | 3 名追猎者，5 个狂热者 | 3 名追猎者，6 个狂热者 | 异质 & 非对称 | 超难 |'
- en: '| MMM2 | 1 Medivac, 2 Marauders, 7 Marines | 1 Medivac, 3 Marauders, 8 Marines
    | heterogeneous & asymmetric | Super-Hard |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| MMM2 | 1 辑空机，2 名突击兵，7 名海军陆战队员 | 1 辑空机，3 名突击兵，8 名海军陆战队员 | 异质 & 非对称 | 超难 |'
- en: 'Table 5: Details of SMAC tasks'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：SMAC任务的详细信息
- en: '| SMAC tasks | n_agents | n_enemies | observation space | state space | action
    space |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| SMAC任务 | n_agents | n_enemies | 观察空间 | 状态空间 | 动作空间 |'
- en: '| 2m_vs_1z | 2 | 1 | 16 | 26 | 7 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| 2m_vs_1z | 2 | 1 | 16 | 26 | 7 |'
- en: '| 1c3s5z | 9 | 9 | 162 | 270 | 15 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| 1c3s5z | 9 | 9 | 162 | 270 | 15 |'
- en: '| 3s_vs_5z | 3 | 5 | 48 | 68 | 11 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| 3s_vs_5z | 3 | 5 | 48 | 68 | 11 |'
- en: '| 5m_vs_6m | 5 | 6 | 55 | 98 | 12 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 5m_vs_6m | 5 | 6 | 55 | 98 | 12 |'
- en: '| 3s5z_vs_3s6z | 8 | 9 | 136 | 230 | 15 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 3s5z_vs_3s6z | 8 | 9 | 136 | 230 | 15 |'
- en: '| MMM2 | 10 | 12 | 176 | 322 | 18 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| MMM2 | 10 | 12 | 176 | 322 | 18 |'
- en: 'E.4.3 A Brand New Task: River'
  id: totrans-439
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: E.4.3 一项全新的任务：River
- en: 'To exclude the probability that LEMAE’s success relies on LLM’s familiarity
    with the chosen tasks, we’ve designed a brand new task, termed River, which LLM
    has never encountered before. The task is detailed as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 为了排除LEMAE成功依赖于LLM对选定任务的熟悉度的可能性，我们设计了一个全新的任务，名为River，这是LLM从未遇到过的任务。任务的详细信息如下：
- en: 'The River task is adapted from the Multiple-Particle Environment and its map
    is illustrated in Fig. [8](https://arxiv.org/html/2410.02511v1#S5.F8 "Figure 8
    ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a. Two
    agents, Alice and Bob, are placed in a 30 x 30 grid field intersected by two rivers
    running vertically and horizontally. A mountain in the bottom-left corner obstructs
    the passage. Alice and Bob start randomly in the top-left part of the field and
    need to move to the bottom-right part. However, Alice is afraid of water and cannot
    cross the river unless Bob stays in the river to act as a bridge for her.'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 'River任务改编自多粒子环境，其地图如图[8](https://arxiv.org/html/2410.02511v1#S5.F8 "Figure
    8 ‣ 5.6 Scalability & Generalization Analysis ‣ 5 Experiments ‣ Choices are More
    Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")a所示。两个代理，Alice和Bob，位于一个30x30的网格场地中，场地内有两条纵横交错的河流。左下角的山脉阻挡了通道。Alice和Bob从场地的左上角随机开始，需移动到右下角。然而，Alice害怕水，除非Bob停留在河里作为桥梁，否则她无法穿越河流。'
- en: The observation space is discrete with four dimensions, representing the positions
    of two agents, i.e., $o=[x_{1},y_{1},x_{2},y_{2}]$. The action space is also discrete,
    allowing movement in four directions. Agents receive a positive reward only when
    both agents reach the bottom-right corner of the field.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间是离散的，具有四个维度，表示两个代理的位置，即$o=[x_{1},y_{1},x_{2},y_{2}]$。动作空间也是离散的，允许在四个方向上移动。代理只有在两个代理都到达场地的右下角时才会获得正奖励。
- en: E.5 Hyperparameters
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.5 超参数
- en: 'In LEMAE, we introduce three important hyperparameters: extrinsic reward scaling
    rate $\alpha$, intrinsic reward scaling rate $\beta$, and high randomness epsilon
    $\epsilon_{h}$. Notably, the low randomness epsilon $\epsilon_{l}$ is the hyperparameter
    in the base algorithms, such as 0.05 for QMIX and 0.0 for IPPO.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在LEMAE中，我们引入了三个重要的超参数：外部奖励缩放率$\alpha$、内部奖励缩放率$\beta$和高随机性epsilon$\epsilon_{h}$。值得注意的是，低随机性epsilon$\epsilon_{l}$是基础算法中的超参数，例如QMIX为0.05，IPPO为0.0。
- en: For MPE, we adopt $\{\alpha=10,\beta=0.1,\epsilon_{h}=1\}$ on Pass, Secret-Room,
    and Large-Pass and use $\{\alpha=10,\beta=0.05,\epsilon_{h}=0.2\}$ on Push-Box.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 对于MPE，我们在Pass、Secret-Room和Large-Pass任务上采用$\{\alpha=10,\beta=0.1,\epsilon_{h}=1\}$，在Push-Box任务上采用$\{\alpha=10,\beta=0.05,\epsilon_{h}=0.2\}$。
- en: For SMAC, we adopt $\{\alpha=50,\beta=1,\epsilon_{h}=0.5\}$ on MMM2 and 1c3s5z,
    $\{\alpha=10,\beta=1,\epsilon_{h}=0.5\}$ on 3s_vs_5z and 2m_vs_1z, $\{\alpha=1,\beta=1,\epsilon_{h}=0.5\}$
    on 5m_vs_6m and 3s5z_vs_3s6z.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SMAC，我们在MMM2和1c3s5z任务上采用$\{\alpha=50,\beta=1,\epsilon_{h}=0.5\}$，在3s_vs_5z和2m_vs_1z任务上采用$\{\alpha=10,\beta=1,\epsilon_{h}=0.5\}$，在5m_vs_6m和3s5z_vs_3s6z任务上采用$\{\alpha=1,\beta=1,\epsilon_{h}=0.5\}$。
- en: E.6 Resources
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6 资源
- en: We use a server with 8*NVIDIA RTX 3090 GPUs, and 2*AMD 7H12 CPUs to run all
    the experiments. Without specifying, each setting is repeated for 5 seeds. For
    one seed in SC2, the running time ranges from approximately 50 minutes to 12 hours.
    For MPE, the running time varies from around 3 to 7 hours. The input for each
    LLM (GPT-4-1106-preview) inference comprises approximately 600-4000 tokens (0.006-0.04
    dollars), yielding an output of about 300-1600 tokens (0.009-0.048 dollars).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一台配备8个*NVIDIA RTX 3090 GPU*和2个*AMD 7H12 CPU*的服务器来运行所有实验。除非另有说明，否则每个设置会重复进行5次种子实验。在SC2中，每个种子的运行时间大约在50分钟到12小时之间。在MPE中，运行时间大约在3到7小时之间。每次LLM（GPT-4-1106-preview）推理的输入约为600-4000个tokens（0.006-0.04美元），输出约为300-1600个tokens（0.009-0.048美元）。
- en: Appendix F Additional Experimental Results
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 额外的实验结果
- en: 'F.1 More Complicated Benchmark: SMACv2'
  id: totrans-450
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 更复杂的基准：SMACv2
- en: 'We have evaluated LEMAE on three typical tasks, protoss_5_vs_5, terran_5_vs_5,
    and zerg_5_vs_5, in SMACv2 (Ellis et al., [2024](https://arxiv.org/html/2410.02511v1#bib.bib39))
    to demonstrate its effectiveness under stochastic dynamics settings. We utilized
    the typical hyperparameters for both LEMAE and LAIES as used in SMAC. As shown
    in Fig. [11](https://arxiv.org/html/2410.02511v1#A6.F11 "Figure 11 ‣ F.1 More
    Complicated Benchmark: SMACv2 ‣ Appendix F Additional Experimental Results ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    LEMAE achieves outstanding performance, confirming its applicability to such settings.
    This result further demonstrates LEMAE’s potential for real-world scenarios with
    complexity and stochasticity.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在SMACv2中评估了LEMAE在三个典型任务（protoss_5_vs_5、terran_5_vs_5 和 zerg_5_vs_5）上的表现，以展示其在随机动力学设置下的有效性（Ellis等人，[2024](https://arxiv.org/html/2410.02511v1#bib.bib39)）。我们使用了SMAC中典型的超参数，应用于LEMAE和LAIES。如图[11](https://arxiv.org/html/2410.02511v1#A6.F11
    "Figure 11 ‣ F.1 More Complicated Benchmark: SMACv2 ‣ Appendix F Additional Experimental
    Results ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent
    Exploration")所示，LEMAE取得了出色的表现，确认了其在这些设置中的适用性。这个结果进一步证明了LEMAE在具有复杂性和随机性的真实场景中的潜力。'
- en: '![Refer to caption](img/4ed524eb10c8f9104f8544376acd10c9.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ed524eb10c8f9104f8544376acd10c9.png)'
- en: 'Figure 11: Evaluating LEMAE against baseline methods across three maps in SMACv2,
    which features greater randomness, using the evaluation metric of test win rate.
    QMIX-DR is QMIX with dense rewards in the original SMACv2\. This result further
    demonstrates LEMAE’s potential for real-world scenarios with complexity and stochasticity.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：在SMACv2中，通过在三个地图上的评估，使用测试胜率评估指标，评估LEMAE与基线方法的表现，SMACv2具有更高的随机性。QMIX-DR是原始SMACv2中使用密集奖励的QMIX。这个结果进一步证明了LEMAE在具有复杂性和随机性的真实场景中的潜力。
- en: F.2 Extending LEMAE Beyond Symbolic Tasks
  id: totrans-454
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 将LEMAE扩展到符号任务之外
- en: 'This work primarily focuses on tasks with symbolic state spaces, where states
    are represented as symbolic arrays describing the agent and environment. As discussed
    in Appendix [B.3](https://arxiv.org/html/2410.02511v1#A2.SS3 "B.3 Future Works
    ‣ Appendix B Further Discussions ‣ Choices are More Important than Efforts: LLM
    Enables Efficient Multi-Agent Exploration"), to extend LEMAE from symbolic tasks
    to vision-based tasks, we can exchange the LLM for a multi-modal LM in LEMAE for
    key state localization. To confirm the applicability of LEMAE to vision-based
    tasks, we conduct a demonstrative experiment: We extend the task Pass to a vision-based
    task Visual-Pass, as illustrated in Fig. [12](https://arxiv.org/html/2410.02511v1#A6.F12
    "Figure 12 ‣ F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional
    Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration")a. We prompt a LLM to define key states with the same
    task description and role instruction as proposed in Sec. [4.2](https://arxiv.org/html/2410.02511v1#S4.SS2
    "4.2 Key States Localization with LLM ‣ 4 Method ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration") and use the LLM-generated
    definition as the prompt for a Vision Language Model (GPT-4o). Then, it is prompted
    to discriminate key states in the randomly sampled states. GPT-4o achieves a 98%
    accuracy rate in discriminating key states among the 50 sampled image states.
    This confirms that with a proper extension of the LLM, LEMAE can eliminate dependence
    on state semantics and be applied to other tasks such as visual-input.'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '本研究主要关注具有符号状态空间的任务，其中状态通过符号数组表示，描述代理和环境。如附录[B.3](https://arxiv.org/html/2410.02511v1#A2.SS3
    "B.3 Future Works ‣ Appendix B Further Discussions ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")中讨论的那样，为了将LEMAE从符号任务扩展到基于视觉的任务，我们可以在LEMAE中将LLM替换为多模态语言模型，以实现关键状态的定位。为了确认LEMAE在基于视觉的任务中的适用性，我们进行了一个示范实验：我们将任务Pass扩展为基于视觉的任务Visual-Pass，如图[12](https://arxiv.org/html/2410.02511v1#A6.F12
    "Figure 12 ‣ F.2 Extending LEMAE Beyond Symbolic Tasks ‣ Appendix F Additional
    Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration")a所示。我们提示一个LLM使用与第[4.2](https://arxiv.org/html/2410.02511v1#S4.SS2
    "4.2 Key States Localization with LLM ‣ 4 Method ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")节中提出的相同任务描述和角色指令来定义关键状态，并将LLM生成的定义作为提示，输入到视觉语言模型（GPT-4o）中。然后，模型被提示在随机抽取的状态中区分关键状态。GPT-4o在区分50个采样图像状态中的关键状态时达到了98%的准确率。这证实了通过适当扩展LLM，LEMAE可以消除对状态语义的依赖，并可以应用于其他任务，如视觉输入任务。'
- en: '![Refer to caption](img/ec88b3c895c14cd73a87226e9d64e31f.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ec88b3c895c14cd73a87226e9d64e31f.png)'
- en: 'Figure 12: (a) An illustration of the vision-based task Visual-Pass. (b) The
    prompt for the Vision Language Model (VLM), which includes the task description,
    role instruction, image state and key states definition provided by another LLM.
    The VLM is tasked with determining whether the given image state corresponds to
    a key state.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：（a）基于视觉的任务Visual-Pass的示意图。（b）视觉语言模型（VLM）的提示，其中包括任务描述、角色指令、图像状态以及由另一个LLM提供的关键状态定义。VLM的任务是判断给定的图像状态是否对应于关键状态。
- en: F.3 Ablation Studies on Mixed-Randomness Exploration
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 混合随机性探索的消融研究
- en: 'As demonstrated in Fig [13](https://arxiv.org/html/2410.02511v1#A6.F13 "Figure
    13 ‣ F.3 Ablation Studies on Mixed-Randomness Exploration ‣ Appendix F Additional
    Experimental Results ‣ Choices are More Important than Efforts: LLM Enables Efficient
    Multi-Agent Exploration"), we conduct an ablation study on mixed-randomness exploration
    within the 3s_vs_5z map. Results indicate that LEMAE exhibits insensitivity to
    the parameter $\epsilon_{h}$, provided that the level of randomness remains moderate,
    as opposed to being excessively extreme (0.1 or 0.9). Besides, the effectiveness
    of our design is highlighted through a comparison between LEMAE and its variants,
    namely, only leaf node and LEMAE w/o KSMTE.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[13](https://arxiv.org/html/2410.02511v1#A6.F13 "Figure 13 ‣ F.3 Ablation
    Studies on Mixed-Randomness Exploration ‣ Appendix F Additional Experimental Results
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")所示，我们在3s_vs_5z地图上进行了一项关于混合随机性探索的消融研究。结果表明，只要随机性水平保持适中（而不是过于极端，如0.1或0.9），LEMAE对参数$\epsilon_{h}$表现出不敏感性。此外，通过与LEMAE及其变体（即仅叶子节点和LEMAE
    w/o KSMTE）的比较，我们突出了我们设计的有效性。'
- en: '![Refer to caption](img/88900913d926314e9d0f090f7a669026.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/88900913d926314e9d0f090f7a669026.png)'
- en: 'Figure 13: Ablation study on mixed-randomness exploration. The default hyperparameter
    used in LEMAE is $\epsilon_{h}=0.5$. Robustness evaluation included variations
    in $\epsilon_{h}=[0.1,0.3,0.7,0.9]$. The only leaf node denotes simply using $\epsilon_{l}$
    for the non-leaf node and $\epsilon_{h}$ for the leaf node. The LEMAE w/o KSMTE
    denotes LEMAE without mixed-randomness exploration.'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：关于混合随机性探索的消融研究。LEMAE中使用的默认超参数是$\epsilon_{h}=0.5$。鲁棒性评估包括$\epsilon_{h}$的变化范围为$[0.1,0.3,0.7,0.9]$。唯一的叶子节点表示仅使用$\epsilon_{l}$作为非叶子节点，而使用$\epsilon_{h}$作为叶子节点。LEMAE
    w/o KSMTE表示没有混合随机性探索的LEMAE。
- en: F.4 Experiments for Single-Agent setups
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.4 单一智能体设置的实验
- en: Indeed, we propose LEMAE as a general approach for LLM-empowered efficient exploration
    in reinforcement learning, applicable to both single-agent and multi-agent settings.
    We underscore the evaluation of its performance in multi-agent settings due to
    its inherent complexity.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，我们提出LEMAE作为一种通用方法，用于强化学习中LLM驱动的高效探索，适用于单一智能体和多智能体设置。我们强调了在多智能体设置中评估其性能，因为其固有的复杂性。
- en: As the proposed method can seamlessly extend to single-agent scenarios, we introduce
    a single-agent variant of MPE and assess PPO (Schulman et al., [2017](https://arxiv.org/html/2410.02511v1#bib.bib136))
    and PPO-based LEMAE across four tasks. We run each algorithm using three random
    seeds with 300k environment steps, using the evaluation metric of the test win
    rate. The following table shows that LEMAE can facilitate efficient exploration
    in single-agent scenarios.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所提方法可以无缝扩展到单一智能体场景，我们引入了MPE的单一智能体变体，并评估了PPO（Schulman等，[2017](https://arxiv.org/html/2410.02511v1#bib.bib136)）和基于PPO的LEMAE在四个任务中的表现。我们使用三个随机种子运行每个算法，并进行300k环境步数的训练，评估指标为测试胜率。下表显示了LEMAE在单一智能体场景中能够促进高效探索。
- en: 'Table 6: Final test win rate of LEMAE and PPO on single-agent variant of MPE
    tasks.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：LEMAE和PPO在MPE任务单一智能体变体上的最终测试胜率。
- en: '| Single MPE | PPO | LEMAE |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| 单一MPE | PPO | LEMAE |'
- en: '| Single Pass | 0.00$\pm$0.00 | 1.00$\pm$0.00 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| 单次通行 | 0.00$\pm$0.00 | 1.00$\pm$0.00 |'
- en: '| Single Secret-Room | 0.00$\pm$0.00 | 0.98$\pm$0.01 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 单一秘密房间 | 0.00$\pm$0.00 | 0.98$\pm$0.01 |'
- en: '| Single Large-Pass | 0.00$\pm$0.00 | 0.99$\pm$0.01 |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| 单一大型通行 | 0.00$\pm$0.00 | 0.99$\pm$0.01 |'
- en: '| Single Push-Box | 0.00$\pm$0.00 | 0.96$\pm$0.08 |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
  zh: '| 单一推箱子 | 0.00$\pm$0.00 | 0.96$\pm$0.08 |'
- en: F.5 Discussion on the KSMT
  id: totrans-471
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.5 关于KSMT的讨论
- en: Using KSMT could pose a limitation due to potential memory costs in certain
    scenarios. However, this has not been a significant issue in our experiments,
    as the key states are relatively few, primarily focusing on the most critical
    ones, with a natural sequential relationship typically existing between them.
    Notably, LEMAE is also compatible with other memory structures, such as Directed
    Acyclic Graphs (DAGs), which could be an interesting direction for future exploration.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KSMT可能会因某些场景下的潜在内存成本而产生限制。然而，在我们的实验中，这并没有成为一个显著的问题，因为关键状态相对较少，主要集中在最关键的状态之间，且这些状态通常存在自然的顺序关系。值得注意的是，LEMAE也兼容其他内存结构，例如有向无环图（DAG），这可能是未来探索的一个有趣方向。
- en: To demonstrate the effectiveness of LEMAE with other memory structures, in scenarios
    where task completion follows a linear pattern (e.g., $Init\rightarrow A\rightarrow
    B\rightarrow Success$), we employ a more efficient strategy by using a KSMT variant
    with a single branch representing the sequential order of key states. Specifically,
    we systematically assign a priority value to each key state, continuously updating
    it based on its occurrence order within the sequence of attained key states. The
    determination of the ranking of key states within the one-branch KSMT relies on
    this established priority.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示LEMAE与其他内存结构的有效性，在任务完成遵循线性模式的场景下（例如，$Init\rightarrow A\rightarrow B\rightarrow
    Success$），我们采用了一种更高效的策略，使用KSMT变体，单分支代表关键状态的顺序。具体来说，我们系统地为每个关键状态分配优先级值，并根据其在获得的关键状态序列中的出现顺序不断更新该值。在单分支KSMT中，关键状态的排名是基于这一优先级来确定的。
- en: 'As illustrated in Fig. [14](https://arxiv.org/html/2410.02511v1#A6.F14 "Figure
    14 ‣ F.5 Discussion on the KSMT ‣ Appendix F Additional Experimental Results ‣
    Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    an ablation study is conducted to compare the performance between raw KSMT and
    the one-branch KSMT variant across six maps in SMAC. The results demonstrate the
    increased necessity of employing the one-branch KSMT variant for tasks involving
    a larger number of agents and greater complexity, such as 5m_vs_6m, 3s5z_vs_3s6z,
    and MMM2. Consequently, we have adopted the one-branch KSMT approach for these
    specific SMAC tasks: 5m_vs_6m, 3s5z_vs_3s6z, and MMM2.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[14](https://arxiv.org/html/2410.02511v1#A6.F14 "Figure 14 ‣ F.5 Discussion
    on the KSMT ‣ Appendix F Additional Experimental Results ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")所示，我们进行了一个消融研究，比较了原始KSMT与单分支KSMT变体在SMAC中六张地图上的表现。结果表明，在涉及更多代理和更高复杂度的任务中，例如5m_vs_6m、3s5z_vs_3s6z和MMM2，采用单分支KSMT变体变得更加必要。因此，我们在这些特定的SMAC任务（5m_vs_6m、3s5z_vs_3s6z和MMM2）中采用了单分支KSMT方法。'
- en: '![Refer to caption](img/f8e86845c849b0d32b952cf860c5c98f.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f8e86845c849b0d32b952cf860c5c98f.png)'
- en: 'Figure 14: Ablation study conducted to compare the performance between raw
    KSMT and the one-branch KSMT variant across six maps in SMAC.'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：消融研究，比较了原始KSMT与单分支KSMT变体在SMAC中六张地图上的表现。
- en: F.5.1 Working with dense reward settings
  id: totrans-477
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F.5.1 在密集奖励设置下的工作
- en: 'We also evaluate LEMAE in tasks with dense rewards in SMAC, denoted as LEMAE-DR.
    As shown in Fig. [15](https://arxiv.org/html/2410.02511v1#A6.F15 "Figure 15 ‣
    F.5.1 Working with dense reward settings ‣ F.5 Discussion on the KSMT ‣ Appendix
    F Additional Experimental Results ‣ Choices are More Important than Efforts: LLM
    Enables Efficient Multi-Agent Exploration"), the results confirm that LEMAE-DR
    facilitates efficient exploration in both dense and sparse reward settings, highlighting
    the main contribution of our method. Additionally, LEMAE-DR achieves better convergence
    than LEMAE due to the guidance provided by dense rewards.'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还在SMAC中的密集奖励任务中评估了LEMAE，记作LEMAE-DR。如图[15](https://arxiv.org/html/2410.02511v1#A6.F15
    "Figure 15 ‣ F.5.1 Working with dense reward settings ‣ F.5 Discussion on the
    KSMT ‣ Appendix F Additional Experimental Results ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")所示，结果确认LEMAE-DR在密集和稀疏奖励设置中都能促进高效的探索，突显了我们方法的主要贡献。此外，LEMAE-DR由于密集奖励提供的指导，比LEMAE实现了更好的收敛性。'
- en: '![Refer to caption](img/52adb1012fefd986f56618e9d2f565cc.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/52adb1012fefd986f56618e9d2f565cc.png)'
- en: 'Figure 15: Evaluating LEMAE with dense rewards across three maps in SMAC using
    the evaluation metric of test win rate. LEMAE-DR is LEMAE with dense rewards in
    the original SMAC, which effectively ensures efficient exploration and achieves
    better convergence.'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：使用测试胜率作为评估指标，在SMAC中评估LEMAE在三个地图上的密集奖励效果。LEMAE-DR是原始SMAC中的LEMAE与密集奖励的结合，能够有效确保高效探索并实现更好的收敛。
- en: F.6 Additional Results
  id: totrans-481
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.6 额外结果
- en: 'Table 7: Final test win rate of LEMAE and comparable baseline (CMAE) on MPE
    tasks.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：LEMAE 和可比基线（CMAE）在 MPE 任务中的最终测试胜率。
- en: '| MPE | LEMAE (Ours) | CMAE |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| MPE | LEMAE（我们的方法） | CMAE |'
- en: '| Pass | 1.00$\pm$0.00 | 0.75$\pm$0.43 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| Pass | 1.00$\pm$0.00 | 0.75$\pm$0.43 |'
- en: '| Secret-Room | 1.00$\pm$0.00 | 0.80$\pm$0.40 |'
  id: totrans-485
  prefs: []
  type: TYPE_TB
  zh: '| Secret-Room | 1.00$\pm$0.00 | 0.80$\pm$0.40 |'
- en: '| Push-Box | 1.00$\pm$0.00 | 0.80$\pm$0.40 |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| Push-Box | 1.00$\pm$0.00 | 0.80$\pm$0.40 |'
- en: '| Large-Pass | 1.00$\pm$0.00 | 0.00$\pm$0.00 |'
  id: totrans-487
  prefs: []
  type: TYPE_TB
  zh: '| Large-Pass | 1.00$\pm$0.00 | 0.00$\pm$0.00 |'
- en: 'Table 8: Final test win rate of LEMAE and comparable baseline (LAIES) on SMAC
    tasks. QMIX-DR denotes training QMIX with dense reward.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：LEMAE 和可比基线（LAIES）在 SMAC 任务中的最终测试胜率。QMIX-DR 表示训练使用稠密奖励的 QMIX。
- en: '| SMAC | LEMAE (Ours) | LAIES | QMIX-DR |'
  id: totrans-489
  prefs: []
  type: TYPE_TB
  zh: '| SMAC | LEMAE（我们的方法） | LAIES | QMIX-DR |'
- en: '| 1c3s5z | 0.98$\pm$0.02 | 0.89$\pm$0.09 | 0.99$\pm$0.01 |'
  id: totrans-490
  prefs: []
  type: TYPE_TB
  zh: '| 1c3s5z | 0.98$\pm$0.02 | 0.89$\pm$0.09 | 0.99$\pm$0.01 |'
- en: '| 2m_vs_1z | 1.00$\pm$0.01 | 0.73$\pm$0.24 | 1.00$\pm$0.01 |'
  id: totrans-491
  prefs: []
  type: TYPE_TB
  zh: '| 2m_vs_1z | 1.00$\pm$0.01 | 0.73$\pm$0.24 | 1.00$\pm$0.01 |'
- en: '| 3s_vs_5z | 0.83$\pm$0.07 | 0.10$\pm$0.12 | 0.66$\pm$0.16 |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '| 3s_vs_5z | 0.83$\pm$0.07 | 0.10$\pm$0.12 | 0.66$\pm$0.16 |'
- en: '| 5m_vs_6m | 0.74$\pm$0.08 | 0.68$\pm$0.10 | 0.78$\pm$0.08 |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 5m_vs_6m | 0.74$\pm$0.08 | 0.68$\pm$0.10 | 0.78$\pm$0.08 |'
- en: '| 3s5z_vs_3s6z | 0.73$\pm$0.14 | 0.45$\pm$0.35 | 0.73$\pm$0.07 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 3s5z_vs_3s6z | 0.73$\pm$0.14 | 0.45$\pm$0.35 | 0.73$\pm$0.07 |'
- en: '| MMM2 | 0.89$\pm$0.08 | 0.62$\pm$0.25 | 0.90$\pm$0.05 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| MMM2 | 0.89$\pm$0.08 | 0.62$\pm$0.25 | 0.90$\pm$0.05 |'
- en: 'Table 9: Comparing LEMAE with SOTA baseline CMAE across four maps in MPE using
    the metric of the number of exploration steps (in thousand) taken to find the
    success state'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：使用成功状态的探索步骤数（千计）这一指标，比较 LEMAE 和 SOTA 基线 CMAE 在 MPE 四张地图上的表现
- en: '| MPE | LEMAE (Ours) | CMAE | Acceleration rate |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| MPE | LEMAE（我们的方法） | CMAE | 加速率 |'
- en: '| Pass | 153.1$\pm$20.7 | 2114.8$\pm$157.4 | 13.8 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| Pass | 153.1$\pm$20.7 | 2114.8$\pm$157.4 | 13.8 |'
- en: '| Secret-Room | 316.6$\pm$134.6 | 1448.5$\pm$467.2 | 4.6 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Secret-Room | 316.6$\pm$134.6 | 1448.5$\pm$467.2 | 4.6 |'
- en: '| Push-Box | 159.0$\pm$42.5 | 972.3$\pm$887.3 | 6.1 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| Push-Box | 159.0$\pm$42.5 | 972.3$\pm$887.3 | 6.1 |'
- en: '| Large-Pass | 446.9$\pm$256 | >3000 | >6.7 | ![Refer to caption](img/f2f30baedd4755cfc0d34671e708e977.png)'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '| Large-Pass | 446.9$\pm$256 | >3000 | >6.7 | ![参考说明](img/f2f30baedd4755cfc0d34671e708e977.png)'
- en: 'Figure 16: The training curve while evaluating the efficacy of combining our
    method with various algorithms.'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16：评估将我们的方法与多种算法结合时的训练曲线。
- en: In this section, we provide some additional experimental results.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一些额外的实验结果。
- en: 'As demonstrated in Table [7](https://arxiv.org/html/2410.02511v1#A6.T7 "Table
    7 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration")
    and Table [8](https://arxiv.org/html/2410.02511v1#A6.T8 "Table 8 ‣ F.6 Additional
    Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration"), we augment the
    final test win rate of our proposed method, LEMAE, with comparable baseline algorithms
    in MPE and SMAC tasks. This augmentation serves to elucidate the superior performance
    of our method. It is pertinent to note that baseline algorithms, the performance
    of which has been demonstrated to be poor in the training curves, are omitted
    from the tables for conciseness.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 正如表[7](https://arxiv.org/html/2410.02511v1#A6.T7 "表 7 ‣ F.6 额外结果 ‣ 附录 F 额外实验结果
    ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")和表[8](https://arxiv.org/html/2410.02511v1#A6.T8 "表
    8 ‣ F.6 额外结果 ‣ 附录 F 额外实验结果 ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")所示，我们通过在 MPE 和 SMAC 任务中与可比的基线算法对比，增强了我们提出的
    LEMAE 方法的最终测试胜率。这一增强有助于阐明我们方法的优越性能。需要注意的是，基线算法的性能在训练曲线中表现较差，因此为了简洁起见，未在表格中列出这些算法。
- en: 'As demonstrated in Table [9](https://arxiv.org/html/2410.02511v1#A6.T9 "Table
    9 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results ‣ Choices
    are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    we compare LEMAE with SOTA baseline CMAE using the metric of the number of exploration
    steps taken to find the success state. The results indicate a significant exploration
    acceleration rate, up to 10x, underscoring LEMAE’s superior efficiency.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 正如表[9](https://arxiv.org/html/2410.02511v1#A6.T9 "表 9 ‣ F.6 额外结果 ‣ 附录 F 额外实验结果
    ‣ 选择比努力更重要：LLM 实现高效的多智能体探索")所示，我们使用成功状态的探索步骤数量这一指标，将 LEMAE 与 SOTA 基线 CMAE 进行了比较。结果表明，LEMAE
    在探索加速率上取得了显著的提升，最高达到 10 倍，凸显了其卓越的效率。
- en: 'Moreover, as illustrated in Fig. [16](https://arxiv.org/html/2410.02511v1#A6.F16
    "Figure 16 ‣ F.6 Additional Results ‣ Appendix F Additional Experimental Results
    ‣ Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration"),
    we supplement the training curve while evaluating the efficacy of combining our
    method with various algorithms, i.e., QPLEX and VMIX.</foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg>'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，如图[16](https://arxiv.org/html/2410.02511v1#A6.F16 "Figure 16 ‣ F.6 Additional
    Results ‣ Appendix F Additional Experimental Results ‣ Choices are More Important
    than Efforts: LLM Enables Efficient Multi-Agent Exploration")所示，我们在评估将我们的方法与各种算法（即QPLEX和VMIX）结合的有效性时，补充了训练曲线。'
