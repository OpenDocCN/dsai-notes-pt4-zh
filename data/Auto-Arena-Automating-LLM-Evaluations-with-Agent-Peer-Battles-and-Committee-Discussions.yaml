- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2025-01-11 12:35:36'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 12:35:36'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Auto-Arena：通过代理对战和委员会讨论自动化大语言模型评估
- en: 来源：[https://arxiv.org/html/2405.20267/](https://arxiv.org/html/2405.20267/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2405.20267/](https://arxiv.org/html/2405.20267/)
- en: Ruochen Zhao^(1,2)  , Wenxuan Zhang² , Yew Ken Chia^(2,3) ,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruochen Zhao^(1,2)  , Wenxuan Zhang² , Yew Ken Chia^(2,3) ,
- en: Weiwen Xu², Deli Zhao², Lidong Bing²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Weiwen Xu², Deli Zhao², Lidong Bing²
- en: ¹ Nanyang Technological University, Singapore
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 南洋理工大学，新加坡
- en: ² DAMO Academy, Alibaba Group; ³ Singapore University of Technology and Design
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 阿里巴巴集团 DAMO Academy；³ 新加坡科技设计大学
- en: ruochen002@e.ntu.edu.sg;
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ruochen002@e.ntu.edu.sg;
- en: '{saike.zwx, yewken.chia, xuweiwen.xww}@alibaba-inc.com {deli.zdl, l.bing}@alibaba-inc.com'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{saike.zwx, yewken.chia, xuweiwen.xww}@alibaba-inc.com {deli.zdl, l.bing}@alibaba-inc.com'
- en: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   Work done
    while the author was an intern at DAMO Academy, Alibaba Group.  Wenxuan Zhang
    is the corresponding author. Yew Ken Chia is under the Joint Ph.D. Program between
    DAMO Academy and SUTD.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://auto-arena.github.io/](https://auto-arena.github.io/)   本文工作是在作者作为
    DAMO Academy, Alibaba Group 的实习生期间完成的。 Wenxuan Zhang 是通讯作者。 Yew Ken Chia 参与了 DAMO
    Academy 和 SUTD 的联合博士项目。'
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As LLMs continuously evolve, there is an urgent need for a reliable evaluation
    method that delivers trustworthy results promptly. Currently, static benchmarks
    suffer from inflexibility and unreliability, leading users to prefer human voting
    platforms like Chatbot Arena. However, human evaluations require significant manual
    effort. To address this, we propose the Auto-Arena, an innovative framework that
    automates the entire evaluation process using LLM-powered agents. Firstly, an
    LLM examiner generates questions. Then, two LLM candidates engage in a multi-round
    peer battle based on individual questions, aiming at revealing their true performance
    differences. Finally, a committee of LLM judges collaboratively discusses and
    decides the winner, reducing bias and enhancing fairness. During the peer battles,
    we observe intriguing scenarios where the LLM candidates display competitive behaviors
    and even learn from the opponents. In our extensive experiments involving 15 recent
    LLMs, Auto-Arena shows a 92.14% correlation with human preferences, surpassing
    all previous expert-annotated benchmarks without any manual efforts. As a result,
    Auto-Arena offers a promising alternative to current human evaluation platforms
    for evaluating LLMs automatically.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大语言模型（LLMs）不断发展，迫切需要一种可靠的评估方法，能够迅速提供可信的结果。目前，静态基准测试存在不灵活和不可靠的问题，这导致用户更倾向于使用像
    Chatbot Arena 这样的人工投票平台。然而，人工评估需要大量的手动工作。为了解决这一问题，我们提出了 Auto-Arena，一个创新的框架，利用大语言模型驱动的代理自动化整个评估过程。首先，LLM
    考官生成问题。然后，两个 LLM 候选人基于个别问题进行多轮对战，旨在揭示它们真实的表现差异。最后，由 LLM 法官组成的委员会共同讨论并决定胜者，从而减少偏见并提高公正性。在对战过程中，我们观察到一些有趣的场景，其中
    LLM 候选人展示出竞争行为，甚至从对手中学习。在我们涉及 15 个最新大语言模型的大规模实验中，Auto-Arena 与人工偏好的相关性达到 92.14%，超过了所有之前的专家标注基准，且无需任何手动努力。因此，Auto-Arena
    为当前的人工评估平台提供了一个有前景的自动化评估大语言模型的替代方案。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Since ChatGPT and GPT-4 (OpenAI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib42))
    gained popularity, Large Language Models (LLMs) have risen to the forefront of
    technological innovation, capturing broad industry and social interests (Wu et al.,
    [2023b](https://arxiv.org/html/2405.20267v4#bib.bib52)). This enthusiasm has spurred
    numerous organizations to release their own LLMs (Touvron et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib50);
    Team et al., [2024b](https://arxiv.org/html/2405.20267v4#bib.bib49)). However,
    the rapid pace at which these models are released and updated poses a significant
    challenge for users attempting to understand their capabilities and monitor their
    evolution. Consequently, there has been a pressing demand for comprehensively
    evaluating LLMs recently (Chang et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib10)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 ChatGPT 和 GPT-4（OpenAI 等，[2024](https://arxiv.org/html/2405.20267v4#bib.bib42)）流行以来，大语言模型（LLMs）已成为技术创新的前沿，吸引了广泛的行业和社会关注（Wu
    等，[2023b](https://arxiv.org/html/2405.20267v4#bib.bib52)）。这一热情促使许多组织发布了自己的 LLM（Touvron
    等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib50)；Team 等，[2024b](https://arxiv.org/html/2405.20267v4#bib.bib49)）。然而，这些模型发布和更新的快速节奏对用户理解其能力并监控其演变带来了巨大挑战。因此，近期对于大语言模型的全面评估需求日益迫切（Chang
    等，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib10)）。
- en: The most popular existing method is automatic evaluation with static datasets.
    Among these, static datasets with predefined metrics, such as GSM8k (Cobbe et al.,
    [2021](https://arxiv.org/html/2405.20267v4#bib.bib15)) and MMLU (Hendrycks et al.,
    [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)), are constructed with
    aspect-specific input-output pairs, such as human exam-type questions and their
    corresponding answers. Given the questions, the LLM-produced answers are compared
    to ground-truth answers using metrics such as accuracy. This approach could suffer
    from inflexibility, contamination, and high human annotation costs. Firstly, the
    closed-form ground-truth answers limit their utility in assessing models’ performances
    on general or open-ended questions, which are the main use cases of LLMs. As the
    questions are static, they also risk contamination (Ravaut et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib44)),
    where models may have been inadvertently exposed to elements of the test datasets
    during training, thereby skewing the evaluation results. The manual dataset construction
    also incurs high costs, creating barriers for extending to other domains or languages.
    As an alternative, static datasets with model-based evaluation, such as MT-Bench (Zheng
    et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)) and AlpacaEval (Dubois
    et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)), evaluates LLMs
    on open-ended generations. These methods typically ask two models to generate
    responses to the same open-ended question and then employ a strong judge model
    (e.g., GPT-4) to choose the better response. However, the static question sets
    still bear contamination risks. Additionally, the assumption of the existence
    of a strong judge model makes the evaluation framework less generalizable and
    introduces model-specific bias.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最流行的现有方法是使用静态数据集的自动评估。在这些方法中，静态数据集通常具有预定义的指标，如 GSM8k (Cobbe 等，[2021](https://arxiv.org/html/2405.20267v4#bib.bib15))
    和 MMLU (Hendrycks 等，[2021a](https://arxiv.org/html/2405.20267v4#bib.bib25))，这些数据集通过特定领域的输入输出对构建，比如人类考试类型的问题及其相应的答案。给定问题后，将
    LLM 生成的答案与真实答案进行比较，使用诸如准确性等指标进行评估。这种方法可能会遭遇灵活性差、污染和人工标注成本高等问题。首先，封闭形式的真实答案限制了其在评估模型在一般性或开放式问题上的表现的有效性，而这些正是
    LLM 的主要应用场景。由于问题是静态的，它们也可能遭遇污染问题（Ravaut 等，[2024](https://arxiv.org/html/2405.20267v4#bib.bib44)），即模型在训练过程中可能无意中接触到测试数据集的元素，从而扭曲评估结果。人工构建数据集的成本也很高，成为扩展到其他领域或语言的障碍。作为替代，基于模型评估的静态数据集，例如
    MT-Bench (Zheng 等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)) 和 AlpacaEval (Dubois
    等，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))，对 LLM 在开放式生成上的表现进行评估。这些方法通常让两个模型对相同的开放式问题生成回答，然后使用强大的判定模型（例如
    GPT-4）选择更好的回答。然而，静态问题集仍然存在污染风险。此外，假设存在强大的判定模型使得评估框架的通用性降低，并引入了模型特定的偏差。
- en: 'Table 1: Comparison between Auto-Arena and other benchmarks or evaluation methods.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：Auto-Arena 与其他基准或评估方法的比较。
- en: '|  | Questions | Responses | Judges |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | 问题 | 回答 | 判定者 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Method | Dynamic? | Auto-generated? | Multi-turn? | Open-ended? | Auto? |
    Committee? |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 动态？ | 自动生成？ | 多轮？ | 开放式？ | 自动？ | 委员会？ |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| OpenLLM Leaderboard | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM 排行榜 | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
- en: '| MMLU | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
- en: '| GPQA | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| GPQA | ✗ | ✗ | ✗ | ✗ | ✗ | ✗ |'
- en: '| LC-AlpacaEval | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval | ✗ | ✓ | ✗ | ✓ | ✓ | ✗ |'
- en: '| MT-Bench | ✗ | ✗ | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench | ✗ | ✗ | ✗ | ✓ | ✓ | ✗ |'
- en: '| Arena-Hard | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| Arena-Hard | ✓ | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Chatbot Arena | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| Chatbot Arena | ✓ | ✗ | ✓ | ✓ | ✗ | ✗ |'
- en: '| Auto-Arena | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: Aside from automated evaluations, human assessment, although requiring significant
    manual efforts, remains the gold standard for users. A notable example is Chatbot
    Arena (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)), a
    crowdsourcing platform that gathers anonymous votes on LLM performances and calculates
    Elo scores (Elo & Sloan, [1978](https://arxiv.org/html/2405.20267v4#bib.bib22))
    to rank these models. The resulting leaderboard¹¹1  [https://leaderboard.lmsys.org/](https://leaderboard.lmsys.org/)
    is widely considered as a trustworthy indicator of LLMs’ general capabilities.
    However, a reliable model evaluation on this platform must be supported by a large
    number of human votes, which requires considerable time and effort. Consequently,
    when newly developed models enter the scene, they often struggle to quickly amass
    a large number of votes. Moreover, this strong reliance on human votes limits
    its application in various scenarios. For example, the performance of non-English
    languages is difficult to estimate, as most queries on the platform are in English.
    Moreover, the queries are mostly one-round and simple. The completely open participation
    may also result in uneven evaluation quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动化评估外，人工评估虽然需要大量的人工工作，但仍然是用户的金标准。一例值得注意的例子是Chatbot Arena（郑等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)），这是一个众包平台，收集关于大型语言模型（LLM）表现的匿名投票，并计算Elo评分（Elo
    & Sloan，[1978](https://arxiv.org/html/2405.20267v4#bib.bib22)）来对这些模型进行排名。最终得出的排行榜¹¹1 
    [https://leaderboard.lmsys.org/](https://leaderboard.lmsys.org/) 被广泛认为是LLM总体能力的可靠指标。然而，在该平台上进行可靠的模型评估必须依赖大量的人工投票，这需要相当多的时间和精力。因此，当新开发的模型进入平台时，它们通常很难迅速积累大量投票。此外，过度依赖人工投票限制了其在不同场景中的应用。例如，非英语语言的表现很难估计，因为平台上的大多数查询都是英语。此外，查询大多是单轮且简单的。完全开放的参与方式也可能导致评估质量不均。
- en: 'To enable the evaluation of LLMs that is both automated and reliable while
    aligning with human preferences, we introduce Auto-Arena, a framework that automates
    the entire LLM evaluation process with LLM-powered agents. The framework consists
    of three stages: Firstly, an LLM examiner agent is tasked with generating questions,
    mimicking real-life users posting queries. Secondly, two LLM candidates interact
    with each other and engage in a multi-round peer battle by answering the seed
    question individually, criticizing the opponent’s weaknesses, and raising targeted
    follow-up queries to challenge the opponent further. During the multi-round battle
    process, the LLM’s true capabilities are drawn out and performance gaps become
    more visible. Lastly, a committee of LLM judges collectively discusses and evaluates
    the ability of the two candidates, mimicking the human voting process. As shown
    in Table [1](https://arxiv.org/html/2405.20267v4#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions"), Auto-Arena has several key advantages compared to previous evaluation
    methods: First and foremost, instead of the simple and one-round question-answering
    scheme, Auto-Arena introduces a dynamic multi-round peer battle, which displays
    deeper abilities of LLMs, such as reasoning, interacting, and strategizing. The
    dynamic nature of peer battles also reduces contamination risks. Secondly, by
    expanding a single LLM judge into a committee of LLM judges, Auto-Arena alleviates
    potential model-specific evaluation bias. Finally, since the process of generating
    questions and judgments is fully automated in an end-to-end way, Auto-Arena can
    provide timely evaluations for new models and can easily extend to various domains
    and languages.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现既自动化又可靠的LLM评估，同时与人类偏好保持一致，我们提出了Auto-Arena框架，它通过LLM驱动的代理自动化整个LLM评估过程。该框架由三个阶段组成：首先，LLM考官代理负责生成问题，模拟现实生活中的用户发布查询。其次，两个LLM候选者相互互动，并通过单独回答种子问题、批评对方的弱点、提出针对性的后续问题以进一步挑战对方，进行多轮对战。在多轮对战过程中，LLM的真实能力被逐步展现，性能差距变得更加明显。最后，一个LLM评审委员会集体讨论并评估两个候选者的能力，模拟人类投票过程。如表格[1](https://arxiv.org/html/2405.20267v4#S1.T1
    "Table 1 ‣ 1 Introduction ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer
    Battles and Committee Discussions")所示，Auto-Arena相比以往的评估方法具有几个关键优势：首先，Auto-Arena引入了动态的多轮对战，而不仅仅是简单的一轮问答，这能够展示LLM更深层次的能力，如推理、互动和战略规划。多轮对战的动态性质还减少了污染风险。其次，Auto-Arena通过将单一的LLM评审扩展为一个LLM评审委员会，缓解了潜在的模型特定评估偏差。最后，由于问题生成和评判过程完全自动化且端到端，Auto-Arena能够为新模型提供及时的评估，并且可以轻松扩展到各种领域和语言。
- en: 'To verify the reliability and alignment of the evaluation framework, we run
    an extensive experiment with 15 LLMs. Compared to static and model-based benchmarks,
    Auto-Arena results in the state-of-the-art alignment by achieving a 92.14% Spearman
    correlation with human preferences, surpassing all previous benchmarks. Although
    no manual efforts is involved, the high alignment with human preferences could
    originate from the human-like evaluation process, which is simulated using LLM
    agents. The extensive ablation experiments also demonstrate the reliability of
    the framework: Before and after peer battles, the Spearman correlation with human
    preferences increases by 5%, verifying our hypothesis that the peer battles can
    better display performance gaps. Before and after committee discussions, committee
    agreement increases by 11%, showing human-level agreement and verifying the effectiveness
    of the committee discussion mechanism. By studying the peer battles, we also discover
    intriguing LLM agent behaviors such as competitive and self-improvement actions.
    As the entire process is automatic, the evaluation can be easily adapted to other
    languages or domains by altering the prompts. We provide Chinese as a case study
    for extending to other languages.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证评估框架的可靠性和对齐性，我们对15个LLM进行了广泛的实验。与静态和基于模型的基准相比，Auto-Arena通过实现92.14%的Spearman相关系数与人类偏好的对齐，超越了所有之前的基准。尽管没有涉及人工努力，但与人类偏好的高度对齐可能源于模拟的类人评估过程，这一过程是通过LLM代理进行的。广泛的消融实验也证明了框架的可靠性：在同行对战前后，Spearman相关系数与人类偏好的相关性提高了5%，验证了我们的假设——同行对战能够更好地展示性能差距。在委员会讨论前后，委员会一致性提高了11%，显示出人类级别的一致性，并验证了委员会讨论机制的有效性。通过研究同行对战，我们还发现了LLM代理的有趣行为，如竞争性和自我改进行为。由于整个过程是自动化的，评估可以通过修改提示词轻松适应其他语言或领域。我们提供中文作为扩展到其他语言的案例研究。
- en: 'In conclusion, our contributions can be summarized as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的贡献可以概括如下：
- en: '1.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose Auto-Arena, a fully automatic LLM evaluation framework where the
    examiner, candidates, and judges are all simulated with LLM-powered agents;
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了Auto-Arena，一个完全自动化的LLM评估框架，其中考官、候选人和评委都是由LLM驱动的代理模拟的；
- en: '2.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Specifically, we innovatively utilize peer battles for LLM evaluation, where
    two LLM agents engage in a multi-round debate. This process draws out the model’s
    deeper capabilities;
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体而言，我们创新性地利用同行对战进行LLM评估，其中两个LLM代理进行多轮辩论。这个过程挖掘了模型更深层次的能力；
- en: '3.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: In our extensive experiment with 15 LLMs, we observe the state-of-the-art alignment
    with human preferences without any manual efforts;
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在我们对15个LLM进行的大规模实验中，我们观察到与人类偏好的最先进对齐，且无需任何人工干预；
- en: '4.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: During peer battles, LLM agents display intriguing behaviors, such as strategizing
    and learning from the opponents, which opens up possibilities for future work.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在同行对战中，LLM代理展现出有趣的行为，如制定策略和从对手学习，这为未来的研究开辟了可能性。
- en: '![Refer to caption](img/52a0601827b49784af4dd2f28f32d813.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/52a0601827b49784af4dd2f28f32d813.png)'
- en: 'Figure 1: An illustration of Auto-Arena.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Auto-Arena的示意图。
- en: 2 The Auto-Arena Framework
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Auto-Arena框架
- en: 'As illustrated in Figure [1](https://arxiv.org/html/2405.20267v4#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles
    and Committee Discussions"), the Auto-Arena framework consists of three stages:
    Question Generation, Multi-round Peer Battles, and Committee Discussions. These
    three stages are run sequentially and fully simulated with LLM-powered agents.
    All prompts are included in Appendix [A](https://arxiv.org/html/2405.20267v4#A1
    "Appendix A Prompts Used ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer
    Battles and Committee Discussions").'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[1](https://arxiv.org/html/2405.20267v4#S1.F1 "图1 ‣ 1 介绍 ‣ Auto-Arena：通过代理同行对战和委员会讨论自动化LLM评估")所示，Auto-Arena框架包括三个阶段：问题生成、多轮同行对战和委员会讨论。这三个阶段是顺序执行的，并完全由LLM驱动的代理模拟。所有提示词都包含在附录[A](https://arxiv.org/html/2405.20267v4#A1
    "附录A 提示词使用 ‣ Auto-Arena：通过代理同行对战和委员会讨论自动化LLM评估")中。
- en: 2.1 Question Generation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题生成
- en: 'For debate questions, as using a static dataset could incur data contamination
    concerns and result in unfair evaluations, we ask an LLM examiner agent to dynamically
    generate questions. The examiner agent could be any capable LLM. Similar to MT-Bench (Zheng
    et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)), the generated
    questions cover 8 common categories in real-life conversations: writing, roleplay,
    extraction, reasoning, math, coding, STEM knowledge, and humanities/social science
    knowledge. The examiner is provided with a sample question and encouraged to generate
    diverse and difficult questions to ensure the depth and width of the evaluated
    debates. Examples of the generated questions are shown in Appendix [B](https://arxiv.org/html/2405.20267v4#A2
    "Appendix B Example Questions Generated ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 对于辩论问题，使用静态数据集可能会引发数据污染问题并导致不公平的评估，因此我们要求LLM考官代理动态生成问题。考官代理可以是任何具备能力的LLM。类似于MT-Bench（Zheng
    等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)），生成的问题涵盖了现实生活对话中的8个常见类别：写作、角色扮演、提取、推理、数学、编码、STEM知识和人文学科/社会科学知识。考官会得到一个示例问题，并被鼓励生成多样且具有挑战性的问题，以确保评估的辩论具有深度和广度。生成问题的示例可见于附录[B](https://arxiv.org/html/2405.20267v4#A2
    "附录B 生成问题示例 ‣ Auto-Arena:通过代理对战和委员会讨论自动化LLM评估")。
- en: 'Specifically, as the examiner agent will also participate in the following
    debates, we try to alleviate self-enhancement bias with two designs: 1\. We do
    not disclose to the examiner that it will participate in this tournament. 2\.
    Previous methods (Bai et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib5))
    could incur self-enhancement bias as they ask the examiner agents to only devise
    questions that they are confident about. In comparison, we do not ask the examiner
    to only generate questions that it can solve. To further show that limited self-enhancement
    bias is present, we include an ablation study in Appendix [E](https://arxiv.org/html/2405.20267v4#A5
    "Appendix E Ablation study on Self-Enhancement Bias of the Question Generation
    Stage ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，由于考官代理也将参与后续的辩论，我们尝试通过两种设计来缓解自我增强偏差：1\. 我们不会告知考官它将参与此次比赛。2\. 以往的方法（Bai
    等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib5)）可能会引发自我增强偏差，因为它们要求考官代理仅提出自己有信心解决的问题。相比之下，我们并未要求考官仅生成自己能够解决的问题。为了进一步证明有限的自我增强偏差存在，我们在附录[E](https://arxiv.org/html/2405.20267v4#A5
    "附录E 自我增强偏差的消融研究 ‣ Auto-Arena:通过代理对战和委员会讨论自动化LLM评估")中加入了消融研究。
- en: '![Refer to caption](img/e842190e24d11cb76c1e62b30c2f6619.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e842190e24d11cb76c1e62b30c2f6619.png)'
- en: 'Figure 2: The process of a Lincoln-Douglas-style peer battle with the actions
    used. The <Think> action can be used by the candidates freely and is only visible
    to the candidate itself.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：林肯-道格拉斯风格的同行对战过程及所使用的操作。<思考>操作可以由候选人自由使用，并且仅对候选人自己可见。
- en: 2.2 Peer Debate
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 同行辩论
- en: After question generation, we conduct peer battles around these questions among
    the LLM candidates. In one peer battle, two LLM candidates (A and B) debate around
    the given question, point out the opponent’s weaknesses, and devise follow-up
    questions to further probe the opponent’s weaknesses.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题生成后，我们围绕这些问题在LLM候选人之间进行同行对战。在一次同行对战中，两名LLM候选人（A和B）围绕给定的问题进行辩论，指出对手的弱点，并制定后续问题进一步探讨对手的弱点。
- en: 'In the peer battle, each candidate LLM has four available types of actions:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在同行对战中，每个候选LLM有四种可用操作类型：
- en: •
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '<Think>: The candidate generates internal thoughts about the question or plans
    a strategy. This action can be used at any time and remains concealed from the
    opponent.'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '<思考>: 候选人就问题生成内部思考或制定策略。此操作可以随时使用，并且对对手保持隐蔽。'
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '<Respond>: The candidate answers the given question.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '<回应>: 候选人回答给定问题。'
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '<Criticize>: The candidate identifies flaws and errors in opponent’s previous
    responses.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '<批评>: 候选人指出对手之前回答中的缺陷和错误。'
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '<Raise>: The candidate poses follow-up questions to reveal the opponent’s weaknesses.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '<提问>: 候选人提出后续问题，揭示对手的弱点。'
- en: The workflow of a peer battle takes the form of the Lincoln-Douglas debate format²²2 
    [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format).
    To help users better understand this debate format, we show the debate samples
    at [https://auto-chatbot-arena.streamlit.app/](https://auto-chatbot-arena.streamlit.app/).,
    the most widely used one-on-one debate style in competitions such as those held
    by the National Speech and Debate Association. The peer battle consists of three
    rounds in which two candidate models alternate speaking. Both candidates can see
    the complete dialogue history. This process is depicted in Figure [2](https://arxiv.org/html/2405.20267v4#S2.F2
    "Figure 2 ‣ 2.1 Question Generation ‣ 2 The Auto-Arena Framework ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"). In the first
    round, model A responds to the examiner’s initial question; model B criticizes
    the flaws in A’s response and raises a specific follow-up question; model A then
    responds to B’s follow-up question. The second round follows the same format,
    with A and B switching roles. In the third round, A and B cross-examine each other,
    starting with A criticizing the loopholes in B’s earlier responses and raising
    follow-up questions. After responding, model B criticizes A’s weaknesses and raises
    additional questions. Model A wraps up by responding once more. Throughout this
    process, both A and B perform an equal number of actions to maintain fairness.
    To minimize positional bias, the order of A and B is randomized at the start of
    each debate.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对等对抗的工作流程采用的是林肯-道格拉斯辩论形式²²2  [https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format](https://en.wikipedia.org/wiki/Lincoln-Douglas_debate_format)。为了帮助用户更好地理解这一辩论形式，我们在[https://auto-chatbot-arena.streamlit.app/](https://auto-chatbot-arena.streamlit.app/)展示了辩论样本，这是在美国国家演讲与辩论协会等比赛中最广泛使用的单挑辩论风格。对等对抗由三轮组成，两位候选模型轮流发言。两位候选人都可以看到完整的对话历史。此过程在图[2](https://arxiv.org/html/2405.20267v4#S2.F2
    "Figure 2 ‣ 2.1 Question Generation ‣ 2 The Auto-Arena Framework ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")中有描述。在第一轮中，模型A回答考官的初始问题；模型B批评A回答中的缺陷并提出具体的后续问题；然后模型A回答B的后续问题。第二轮采用相同的格式，A和B交换角色。在第三轮中，A和B进行交叉质询，A首先批评B早期回答中的漏洞并提出后续问题。回答后，模型B批评A的不足并提出额外问题。最后，模型A再次作出回应。在整个过程中，A和B的行动次数相等，以保持公平性。为了减少位置偏见，A和B的顺序在每轮辩论开始时会随机化。
- en: 'During the debate process, enhancement bias and contamination concerns are
    further reduced: The process of candidates raising follow-up questions to each
    other essentially decentralizes the question-generation process, reducing enhancement
    bias in the generated initial questions. Moreover, debating ensures that candidates
    are evaluated not only on their response to the initial question, but also in
    more comprehensive and deeper abilities, such as strategizing, criticizing the
    opponent, and drafting questions. In other words, answering the initial question
    well does not necessarily win the whole debate, which further reduce contamination
    concerns.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在辩论过程中，增强偏见和污染担忧进一步减少：候选人之间相互提问的过程本质上是分散了问题生成的过程，从而减少了初始问题中生成的增强偏见。此外，辩论确保候选人不仅仅是对初始问题的回答得到评价，还要在更全面和更深层次的能力上进行评估，比如策略制定、批评对手和提出问题。换句话说，仅仅回答好初始问题并不一定能赢得整个辩论，这进一步减少了污染担忧。
- en: Depending on which turn it is, we provide an action guide to the candidate,
    specifying the objectives and corresponding actions for this turn. Similar to
    human debate competitions, we time the candidates by imposing a maximum length
    constraint, which is also specified in the prompts. Any responses beyond the required
    length will be cut off. This design mitigates verbosity bias in LLM-as-a-judge (Zheng
    et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)), where LLM judges
    prefer longer and more verbose responses.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每轮的不同，我们会为候选人提供行动指南，明确该轮的目标和相应的行动。类似于人类辩论比赛，我们会通过设置最大时长限制来计时候选人，具体时长也会在提示中说明。任何超出规定时长的回答将被截断。这种设计减少了LLM作为评委时的冗长偏见（Zheng等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)），即LLM评委偏好较长且冗长的回答。
- en: 2.3 Committee Discussions
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 委员会讨论
- en: After the peer battle takes place, a committee of LLM judges collectively determines
    the winner. The committee is always selected as the five best LLMs according to
    the current ranking. To reduce bias, we exclude the participants themselves and
    models from the same family as the participants from the committee. For example,
    GPT-4 will not serve as a judge in evaluating a debate participated by GPT-3.5\.
    In the first round, the committee is initialized with MMLU (Hendrycks et al.,
    [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)) scores to approximate
    LLM performances. Each judge is individually asked to read the entire peer battle
    history, elaborate judgment reasons, and give a decision on whether A is better,
    or B is better, or if there is a tie based on factors such as helpfulness, relevance,
    and accuracy.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在对战结束后，由LLM评审委员会共同确定获胜者。委员会总是根据当前排名选出五个最佳LLM。为了减少偏见，我们排除了参与者本人以及与参与者同一家族的模型。例如，GPT-4不会作为评审来评估GPT-3.5参与的辩论。在第一轮中，委员会根据MMLU（Hendrycks等人，[2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)）得分来初始化，以近似LLM的表现。每个评审单独阅读整个对战历史，阐述判断理由，并根据有用性、相关性和准确性等因素，决定A是否更好，B是否更好，或是否平局。
- en: After the initial judgments are formed, the committee engages in a discussion.
    In a discussion round, each judge reads the other judge’s verdicts in the previous
    rounds, elaborates its own thoughts for judgments, and drafts a discussed verdict.
    During the process, the judge may decide to adjust or maintain the previous judgments.
    Compared to the peer battles that exemplify multi-agent competitions, this committee
    discussion component synthesizes a multi-agent collaboration scheme. By enabling
    interactions among the judge agents and exchanges of different viewpoints, the
    discussion allows the committee to form a collective intelligence. As a result,
    it improves the judgment quality, boosts inter-judge agreement, and mitigates
    single-model bias. Finally, the winning candidate is decided by majority voting
    of the discussed judgments.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步判断形成后，委员会进行讨论。在讨论回合中，每个评审阅读其他评审在前几轮中的裁定，阐述自己的判断思路，并起草讨论后的裁定。在此过程中，评审可能决定调整或保持之前的判断。与展现多代理竞争的对战不同，委员会讨论部分融合了多代理协作方案。通过允许评审代理之间的互动和不同观点的交流，讨论使委员会形成集体智慧。因此，它提高了判断质量，增强了评审间的一致性，减少了单一模型的偏见。最后，胜者由经过讨论的裁定进行多数投票决定。
- en: 3 Using Auto-Arena to Derive Trustworthy Rankings
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 使用Auto-Arena推导可信排名
- en: 3.1 Experimental Setup
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'Model Selection:  For the main experiment, we first select 9 best or latest
    models that are representative of each popular model family on the top 30 list
    on the Chatbot Arena platform with more than 10k votes each at the time of experiments:
    GPT-4-0409-Turbo, GPT-3.5-Turbo-0125, Claude-3-Haiku, Qwen1.5-72B-Chat, Command-R+,
    Llama-2-70B-Chat, Mixtral-8x7b-Instruct-v0.1, Yi-34B-Chat, and Deepseek-LLM-67B.
    To construct a leaderboard, we further add 6 models that are newly released: GPT-4o-2024-05-13,
    Claude-3.5-Sonnet, Qwen2-72B-Instruct, Llama-3-70B, Gemma-2-27B, and Gemini-1.5-Flash.
    Appendix [G](https://arxiv.org/html/2405.20267v4#A7 "Appendix G Model selection
    for the main experiment ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer
    Battles and Committee Discussions") provides a detailed list of the selected models.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 模型选择： 在主要实验中，我们首先从Chatbot Arena平台的前30名列表中选择9个最佳或最新的模型，这些模型在实验时每个都有超过10k的投票：GPT-4-0409-Turbo，GPT-3.5-Turbo-0125，Claude-3-Haiku，Qwen1.5-72B-Chat，Command-R+，Llama-2-70B-Chat，Mixtral-8x7b-Instruct-v0.1，Yi-34B-Chat，和Deepseek-LLM-67B。为了构建排行榜，我们进一步添加了6个新发布的模型：GPT-4o-2024-05-13，Claude-3.5-Sonnet，Qwen2-72B-Instruct，Llama-3-70B，Gemma-2-27B，和Gemini-1.5-Flash。附录[G](https://arxiv.org/html/2405.20267v4#A7
    "附录 G 主要实验的模型选择 ‣ Auto-Arena：通过代理对战和委员会讨论自动化LLM评估")提供了所选模型的详细列表。
- en: 'Baselines:   For the baselines, we consider popular evaluation benchmarks,
    including fixed metrics and model-based metrics. A comparison table is shown in
    Appendix [H](https://arxiv.org/html/2405.20267v4#A8 "Appendix H Comparison of
    baseline methods and Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 基准： 对于基准，我们考虑了流行的评估标准，包括固定指标和基于模型的指标。附录[H](https://arxiv.org/html/2405.20267v4#A8
    "附录 H 基准方法与Auto-Arena的比较 ‣ Auto-Arena：通过代理对战和委员会讨论自动化LLM评估")中显示了对比表。
- en: '1\. Static datasets with fixed metrics: (1) OpenLLM Leaderboard (Beeching et al.,
    [2023](https://arxiv.org/html/2405.20267v4#bib.bib6)), a popular benchmark for
    open-source models averaging performance metrics on 6 key benchmarks, covering
    a large number of different evaluation tasks; (2) GPQA (Rein et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib46)),
    a graduate-level google-proof Q&A benchmark consisting of 448 domain-expert-written
    questions written in scientific subjects; (3) MMLU (Massive Multitask Language
    Understanding) (Hendrycks et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)),
    an extensive benchmark that covers 57 subjects and tests both world knowledge
    and problem-solving ability;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 基于固定评估指标的静态数据集：（1）OpenLLM Leaderboard （Beeching 等， [2023](https://arxiv.org/html/2405.20267v4#bib.bib6)），一个流行的开源模型基准，涵盖6个关键基准上的平均表现指标，涵盖大量不同的评估任务；（2）GPQA （Rein
    等， [2023](https://arxiv.org/html/2405.20267v4#bib.bib46)），一个研究生级别的“谷歌防伪”问答基准，包含448个由领域专家编写的科学主题问题；（3）MMLU（大规模多任务语言理解）（Hendrycks
    等， [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)），一个广泛的基准，涵盖57个学科，测试世界知识和问题解决能力；
- en: '2\. Static datasets with model-based metrics: (1) MT-Bench (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)),
    a set of 80 multi-turn questions. Model responses are graded by GPT-4; (2) Arena
    Hard (Li* et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib31)), a benchmark
    dataset with 1,000 challenging user queries collected on Chatbot Arena. Model
    responses are graded by GPT-4-Turbo; (3) Length-Controlled AlpacaEval (Dubois
    et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)), a benchmark based
    on AlpacaFarm evaluation set (Dubois et al., [2024b](https://arxiv.org/html/2405.20267v4#bib.bib21)),
    which tests models’ abilities to follow general user instructions. Models are
    evaluated by their win rates against GPT-4-Turbo, graded by GPT-4-Turbo.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 基于模型评估指标的静态数据集：（1）MT-Bench （Zheng 等， [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)），一组包含80个多轮问题的数据集。模型的回答由GPT-4进行评分；（2）Arena
    Hard （Li* 等， [2024](https://arxiv.org/html/2405.20267v4#bib.bib31)），一个包含1,000个具有挑战性用户查询的基准数据集，收集自聊天机器人竞技场。模型的回答由GPT-4-Turbo进行评分；（3）Length-Controlled
    AlpacaEval （Dubois 等， [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)），一个基于AlpacaFarm评估集（Dubois
    等， [2024b](https://arxiv.org/html/2405.20267v4#bib.bib21)）的基准，测试模型是否能遵循一般用户指令。模型通过与GPT-4-Turbo的胜率进行评估，评分由GPT-4-Turbo完成。
- en: 'Setup:   Among the 9 participants, we conduct a swiss-style tournament: For
    $n$ participants, instead of pairing each participant with $(n-1)$ others, a swiss-tournament
    pairs each player with $\lceil log_{2}(n)\rceil$ players of similar rankings without
    repeats. This design effectively reduces computational costs of ranking $n$ models
    from $O(n^{2})$ to $O(nlog_{2}(n))$. A cost analysis is included in Appendix [H](https://arxiv.org/html/2405.20267v4#A8
    "Appendix H Comparison of baseline methods and Auto-Arena ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 设置：在9个参与者中，我们进行瑞士式锦标赛：对于$n$个参与者，瑞士锦标赛不是将每个参与者与$(n-1)$个其他人配对，而是将每个玩家与$\lceil
    log_{2}(n)\rceil$个相似排名的玩家配对，且不重复。这个设计有效地将排名$n$个模型的计算成本从$O(n^{2})$减少到$O(nlog_{2}(n))$。成本分析详见附录[H](https://arxiv.org/html/2405.20267v4#A8
    "Appendix H Comparison of baseline methods and Auto-Arena ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")。
- en: Each candidate pair engages in 40 peer battles, with 5 questions from each of
    the 8 task categories that are specified in Section [2.1](https://arxiv.org/html/2405.20267v4#S2.SS1
    "2.1 Question Generation ‣ 2 The Auto-Arena Framework ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"). We provide
    studies showing that the generated questions can reduce contamination concerns
    in Appendix [C](https://arxiv.org/html/2405.20267v4#A3 "Appendix C Contamination
    Analysis ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions") and are generalizable to real-world scenarios in Appendix [D](https://arxiv.org/html/2405.20267v4#A4
    "Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions"). As each battle consists of
    3 rounds (each candidate speaks for 4 times), the competition scale is approximately
    the same as MT-Bench (80 questions, each candidate speaks twice). In the tournament,
    the rating scores are calculated with the Elo rating system (Bai et al., [2022](https://arxiv.org/html/2405.20267v4#bib.bib4);
    Boubdir et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib7)), which has
    become the standard practice in competitive games such as chess (Elo & Sloan,
    [1978](https://arxiv.org/html/2405.20267v4#bib.bib22)). Similar to the Chatbot
    Arena score calculation procedure (Chiang et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib12)),
    we compute the Bradley-Terry (BT) coefficients (Bradley & Terry, [1952](https://arxiv.org/html/2405.20267v4#bib.bib8))
    for better statistical estimation. Following the Reference-Guided judge in Zheng
    et al. ([2023](https://arxiv.org/html/2405.20267v4#bib.bib56)), we ask the best-performing
    judge to give a reference answer for evaluating logical-reasoning questions (math,
    coding, reasoning).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 每对候选者都会进行40场对抗战，每场战斗包含来自8个任务类别中的每个类别的5个问题，这些类别在[2.1节](https://arxiv.org/html/2405.20267v4#S2.SS1
    "2.1 问题生成 ‣ 2 自动竞技场框架 ‣ Auto-Arena：通过代理对抗和委员会讨论自动化LLM评估")中有所说明。我们提供的研究表明，生成的问题可以减少附带污染问题，详见附录[C](https://arxiv.org/html/2405.20267v4#A3
    "附录C 污染分析 ‣ Auto-Arena：通过代理对抗和委员会讨论自动化LLM评估")，并且这些问题可以推广到现实场景，详见附录[D](https://arxiv.org/html/2405.20267v4#A4
    "附录D 合成问题与真实问题对比 ‣ Auto-Arena：通过代理对抗和委员会讨论自动化LLM评估")。由于每场对抗包含3轮（每个候选者发言4次），因此比赛规模大致与MT-Bench相当（80个问题，每个候选者发言两次）。在比赛中，评分采用Elo评分系统计算（Bai
    et al., [2022](https://arxiv.org/html/2405.20267v4#bib.bib4); Boubdir et al.,
    [2023](https://arxiv.org/html/2405.20267v4#bib.bib7)），该系统已成为象棋等竞技游戏中的标准做法（Elo
    & Sloan, [1978](https://arxiv.org/html/2405.20267v4#bib.bib22)）。与Chatbot Arena评分计算程序类似（Chiang
    et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib12)），我们计算Bradley-Terry（BT）系数（Bradley
    & Terry, [1952](https://arxiv.org/html/2405.20267v4#bib.bib8)），以便进行更好的统计估算。参考Zheng等人（[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)）的参考引导评判标准，我们要求表现最好的评审为评估逻辑推理问题（数学、编程、推理）提供参考答案。
- en: We initialize the Swiss tournament rankings according to MMLU scores, which
    is a static approximation of model performances. At the end of each pairing, we
    re-calculate Elo scores of current models. The committee is selected as the best
    5 LLMs based on current Elo rankings at each round. After forming initial judgments,
    the committee members engage in one round of discussion. The final result is decided
    by majority voting of the discussed judgments.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据MMLU分数初始化瑞士赛排名，这是对模型表现的静态近似估算。在每轮配对结束时，我们重新计算当前模型的Elo分数。委员会成员根据当前Elo排名选出最佳的5个LLM。在初步判断后，委员会成员会进行一轮讨论。最终结果由讨论后的判断多数票决定。
- en: 'Table 2: Correlations with Chatbot Arena Elos of evaluation benchmarks on 9
    LLMs.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：9个LLM的评估基准与Chatbot Arena Elo的相关性。
- en: '|  | Spearman |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | Spearman |'
- en: '|  | Correlation |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 相关性 |'
- en: '| OpenLLM (Beeching et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib6))
    | -15.39% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM (Beeching et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib6))
    | -15.39% |'
- en: '| GPQA (Rein et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib46))
    | 36.84% |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GPQA (Rein et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib46))
    | 36.84% |'
- en: '| MMLU (Hendrycks et al., [2021b](https://arxiv.org/html/2405.20267v4#bib.bib26))
    | 56.36% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| MMLU (Hendrycks et al., [2021b](https://arxiv.org/html/2405.20267v4#bib.bib26))
    | 56.36% |'
- en: '| LC-AlpacaEval (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))
    | 82.14% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))
    | 82.14% |'
- en: '| MT-Bench (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | 82.86% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | 82.86% |'
- en: '| Arena-Hard (Li* et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib31))
    | 85.71% |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Arena-Hard (Li* 等，[2024](https://arxiv.org/html/2405.20267v4#bib.bib31))
    | 85.71% |'
- en: '| *Auto-Arena* | 91.67% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena* | 91.67% |'
- en: '| w/o Peer Battles | 86.67% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 无对战 | 86.67% |'
- en: '| w/o Committee Discussions | 88.33% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 无委员会讨论 | 88.33% |'
- en: '3.2 Results: Alignment with Human Preferences'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 结果：与人类偏好的对齐
- en: We regard Chatbot Arena scores as a trustworthy indicator of human preferences
    and general capabilities of LLMs. Table [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "Table 2 ‣ 3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions") shows the Spearman correlations with Chatbot Arena scores achieved
    by various benchmarks. As all benchmarks are evaluated only in English, we use
    English-only Chatbot Arena scores. We see that both static and model-based baselines
    result in a similar level of correlation that is below 90%, with Arena-Hard surpassing
    others at 85.71%. Then, Auto-Arena can improve the correlation to 91.67%, outperforming
    the SOTA by 5.96%. Notably, among all benchmarks, Auto-Arena is the only one that
    doesn’t require human efforts, neither on dataset compilation nor judgment generation.
    The high alignment with human preferences could originate from the human-like
    design, which effectively mimics the human users’ voting processes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 Chatbot Arena 的得分是人类偏好和大规模语言模型（LLMs）一般能力的可信指标。表格 [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "Table 2 ‣ 3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions") 显示了各种基准与 Chatbot Arena 得分的斯皮尔曼相关性。由于所有基准仅在英语中进行评估，我们使用仅限英语的 Chatbot
    Arena 得分。我们看到，无论是静态基准还是基于模型的基准，都产生了相似的相关性水平，低于 90%，其中 Arena-Hard 以 85.71% 超过其他基准。然后，Auto-Arena
    可以将相关性提高到 91.67%，超越当前最先进技术（SOTA）5.96%。值得注意的是，在所有基准中，Auto-Arena 是唯一一个不需要人工参与的基准，无论是在数据集编制还是判断生成上。与人类偏好的高度一致性可能源于其类人设计，能够有效地模拟人类用户的投票过程。
- en: 3.3 Ablation Studies on Peer Battles and Committee Discussions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 对战和委员会讨论的消融研究
- en: 'Peer-battles:   We conduct an ablation study on whether peer-battles affect
    the evaluation quality and include the results in Table [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "Table 2 ‣ 3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions") (“w/o Peer Battles”). In this setup, we ask the committee to only
    evaluate the two candidates’ initial responses to the synthetic question, where
    the judge prompts stay the same. For this no-debate design, the question-answering
    process mimics that of MT-Bench or LC-AlpacaEval, but with an added committee
    discussion component. As a result, we observe that the correlation is slightly
    higher than LC-AlpacaEval and MT-Bench by a margin of 3.81%. Compared to the full
    Auto-Arena framework, however, the performance drops by 5.00%. This proves the
    effectiveness of the peer battles, during which the performance gaps between candidates
    become more visible and robust to judges. Thus, peer battles can improve alignment
    with human preferences.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对战：  我们进行了一个消融研究，探讨对战是否会影响评估质量，并将结果包括在表格 [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "Table 2 ‣ 3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions")（“无对战”）。在这个设置中，我们要求委员会仅评估两个候选者对合成问题的初步回答，其中评委的提示保持不变。对于这个无辩论设计，问答过程模仿了
    MT-Bench 或 LC-AlpacaEval，但加入了委员会讨论组件。结果，我们观察到该方法的相关性比 LC-AlpacaEval 和 MT-Bench
    高出 3.81%。然而，与完整的 Auto-Arena 框架相比，性能下降了 5.00%。这证明了对战的有效性，在对战中，候选者之间的表现差异变得更加明显且对评委更具鲁棒性。因此，对战能够提高与人类偏好的对齐度。
- en: '![Refer to caption](img/b5992be4a319ec950ef26b420eaec862.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b5992be4a319ec950ef26b420eaec862.png)'
- en: 'Figure 3: Cohen’s Kappa agreement with majority vote results before (upper)
    and after (lower) committee discussions.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Cohen’s Kappa 一致性与多数投票结果的比较，委员会讨论前（上）和讨论后（下）。
- en: 'Table 3: Agreement probability among judges. Agreement is defined as the mean
    probability of two random judges agreeing with each other.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：评委间的一致性概率。一致性被定义为两名随机评委相互同意的平均概率。
- en: '|  | Agreement |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 一致性 |'
- en: '| Auto-Arena (Before discussion) | 53% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena（讨论前） | 53% |'
- en: '| Auto-Arena (After discussion) | 64% |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena（讨论后） | 64% |'
- en: '| MT-Bench Human Evaluation | 67% |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench 人类评估 | 67% |'
- en: 'Committee Discussions:   The committee discussion component is designed to
    introduce various points of view and produce more consistent decisions. As shown
    in Table [2](https://arxiv.org/html/2405.20267v4#S3.T2 "Table 2 ‣ 3.1 Experimental
    Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"), the correlation
    with human preferences drops from 91.67% to 88.33% without committee discussions,
    showing the effectiveness of the component in improving evaluation quality. As
    shown in Figure [3](https://arxiv.org/html/2405.20267v4#S3.F3 "Figure 3 ‣ 3.3
    Ablation Studies on Peer Battles and Committee Discussions ‣ 3 Using Auto-Arena
    to Derive Trustworthy Rankings ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions"), before committee discussions, the Cohen’s
    Kappa agreement (McHugh, [2012](https://arxiv.org/html/2405.20267v4#bib.bib37))
    between individual judges and the final result (voted) is low, averaging 0.41.
    Specifically, compared to strong models, the judgments of weak models align less
    with the voted result, such as Yi compared to GPT-4\. This shows that general
    model capabilities could result in significant performance gaps when used as judges.
    After the committee discussions, agreement increased to an average of 0.54, which
    indicates moderate agreement. In the discussion process, judges are exposed to
    more viewpoints, among which some may be convincing enough to result in a change
    in verdict. More analysis on the inter-judge agreement is provided in Appendix
    [F](https://arxiv.org/html/2405.20267v4#A6 "Appendix F Inter-judge Agreement ‣
    Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee Discussions"),
    where we see that discussions could largely improve the agreements among individual
    judges as well. Table [3.3](https://arxiv.org/html/2405.20267v4#S3.SS3 "3.3 Ablation
    Studies on Peer Battles and Committee Discussions ‣ 3 Using Auto-Arena to Derive
    Trustworthy Rankings ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles
    and Committee Discussions") shows the agreement probability among judges. Agreement
    probability is defined as the mean probability of two random judges agreeing with
    each other. After committee discussion, the agreement increases by 11%, matching
    the agreement level among human annotators on MT-Bench. This observation indicates
    that committee discussions can significantly improve the quality of judgments
    to match with human-level performance.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 委员会讨论：委员会讨论组件旨在引入不同的观点，并产生更一致的决策。如表 [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "表 2 ‣ 3.1 实验设置 ‣ 3 使用 Auto-Arena 导出可信排名 ‣ Auto-Arena：通过代理对战和委员会讨论自动化 LLM 评估")
    所示，未进行委员会讨论时，与人类偏好的相关性从 91.67% 降低到 88.33%，显示了该组件在提高评估质量方面的有效性。如图 [3](https://arxiv.org/html/2405.20267v4#S3.F3
    "图 3 ‣ 3.3 去除对战和委员会讨论的消融实验 ‣ 3 使用 Auto-Arena 导出可信排名 ‣ Auto-Arena：通过代理对战和委员会讨论自动化
    LLM 评估") 所示，在进行委员会讨论之前，个别评审员与最终结果（投票结果）之间的 Cohen's Kappa 协议度（McHugh，[2012](https://arxiv.org/html/2405.20267v4#bib.bib37)）较低，平均为
    0.41。具体来说，与强模型相比，弱模型的判断与投票结果的契合度较低，例如 Yi 与 GPT-4 相比。这表明，通用模型的能力可能导致在作为评审时出现显著的性能差距。经过委员会讨论后，协议度提高至平均
    0.54，表明达到了中等程度的协议。在讨论过程中，评审员会接触到更多的观点，其中一些观点可能足够有说服力，导致判决发生变化。有关评审员间协议的更多分析，请参见附录
    [F](https://arxiv.org/html/2405.20267v4#A6 "附录 F 评审员间协议 ‣ Auto-Arena：通过代理对战和委员会讨论自动化
    LLM 评估")，我们可以看到，讨论也能大幅提高评审员之间的协议度。表 [3.3](https://arxiv.org/html/2405.20267v4#S3.SS3
    "3.3 去除对战和委员会讨论的消融实验 ‣ 3 使用 Auto-Arena 导出可信排名 ‣ Auto-Arena：通过代理对战和委员会讨论自动化 LLM
    评估") 显示了评审员之间的协议概率。协议概率定义为两名随机评审员达成一致的平均概率。经过委员会讨论后，协议概率提高了 11%，达到了人类标注员在 MT-Bench
    上的协议水平。这一观察结果表明，委员会讨论可以显著提高判断质量，以匹配人类水平的表现。
- en: 4 Constructing and Maintaining a Leaderboard with Auto-Arena
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 使用 Auto-Arena 构建和维护排行榜
- en: '![Refer to caption](img/48ef779e22efc4f71624343ce57f8491.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/48ef779e22efc4f71624343ce57f8491.png)'
- en: 'Figure 4: Changes in Elo scores of adding Llama-3 to the ranking of 9 models.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：将 Llama-3 添加到 9 个模型的排名中的 Elo 分数变化。
- en: '![Refer to caption](img/10b9b7cee3bfe3140707da43998e8a3d.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/10b9b7cee3bfe3140707da43998e8a3d.png)'
- en: 'Figure 5: Elo scores of 15 models by Auto-Arena on English.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Auto-Arena 在英语上的 15 个模型的 Elo 分数。
- en: 4.1 Update New Models to Leaderboard
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 更新新模型至排行榜
- en: With Auto-Arena, we can obtain the rank for a list of models with their Elo
    scores to construct a leaderboard. As new LLMs are released frequently, we describe
    how to add new candidate models to the existing leaderboard with 6 more models
    which are released very recently, as previously listed in Section [3.1](https://arxiv.org/html/2405.20267v4#S3.SS1
    "3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings ‣
    Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee Discussions").
    To add a new candidate, we ask it to debate with $\lceil log_{2}(n)\rceil$ opponents
    with similar Elo scores, where $n$ is the number of total participants after adding
    the new candidate. For the first pairing, as we do not have Elo indicators, we
    initialize by asking the new candidate to debate with the opponent with the most
    similar MMLU score. This addition mechanism is generalizable and maintains the
    computational costs of evaluating $n$ models below $nlog_{2}(n)$.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Auto-Arena，我们可以通过模型的Elo分数为模型列表获取排名，从而构建排行榜。由于新的LLM（大型语言模型）经常发布，我们描述了如何将最近发布的6个新候选模型添加到现有排行榜中，具体如第[3.1节](https://arxiv.org/html/2405.20267v4#S3.SS1
    "3.1 实验设置 ‣ 3 使用Auto-Arena推导可信排名 ‣ Auto-Arena：通过代理对抗战斗和委员会讨论自动化LLM评估")中所列。为了添加一个新候选模型，我们要求它与$\lceil
    log_{2}(n)\rceil$个具有相似Elo分数的对手进行辩论，其中$n$是添加新候选模型后的总参与者数量。对于第一次配对，由于我们没有Elo指示器，我们通过让新候选模型与MMLU分数最相似的对手进行辩论来初始化。这个添加机制是通用的，并且在评估$n$个模型时能将计算成本保持在$nlog_{2}(n)$以下。
- en: 'Table 4: Correlation analysis with Chatbot Arena of evaluation benchmarks on
    15 LLMs after extension.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在扩展后对15个LLM的评估基准与Chatbot Arena的相关性分析。
- en: '|  | Spearman Correlation |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | 斯皮尔曼相关性 |'
- en: '| --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| OpenLLM | 32.50% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM | 32.50% |'
- en: '| GPQA | 62.86% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPQA | 62.86% |'
- en: '| MMLU | 46.20% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MMLU | 46.20% |'
- en: '| LC-AlpacaEval | 76.32% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval | 76.32% |'
- en: '| MT-Bench | 88.73% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench | 88.73% |'
- en: '| Arena-Hard | 45.36% |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Arena-Hard | 45.36% |'
- en: '| *Auto-Arena* | 92.14% |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| *Auto-Arena* | 92.14% |'
- en: As an example, we add a new participant (Llama-3-70B) to the existing 9-model
    ranking. It battles with $\lceil log_{2}(10)\rceil=4$ close opponents and Figure
    [5](https://arxiv.org/html/2405.20267v4#S4.F5 "Figure 5 ‣ 4 Constructing and Maintaining
    a Leaderboard with Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions") shows how the Elo score changes throughout
    the rounds. Firstly, it is paired with Qwen-1.5 based on MMLU similarity and wins,
    which results in a very high Elo score, even above GPT-4\. Then, it is paired
    with GPT-4, the closest opponent in Elo score. After losing, it is paired with
    the other opponents who are close in Elo scores, Command-R+ and Claude-3-Haiku.
    Eventually, the score stabilizes at second place. This process lets the new candidate
    battle with a reasonable fraction of close opponents and makes the final ranking
    stable without disrupting the other participants, whose score distribution remains
    similar before and after the addition.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们将一个新参与者（Llama-3-70B）添加到现有的9个模型排名中。它与$\lceil log_{2}(10)\rceil=4$个相似的对手进行辩论，图[5](https://arxiv.org/html/2405.20267v4#S4.F5
    "图 5 ‣ 4 使用Auto-Arena构建和维护排行榜 ‣ Auto-Arena：通过代理对抗战斗和委员会讨论自动化LLM评估")展示了Elo分数在各回合中的变化。首先，它与Qwen-1.5基于MMLU相似度进行配对并获胜，从而获得了一个非常高的Elo分数，甚至超过了GPT-4。然后，它与Elo分数最接近的对手GPT-4配对。在失败后，它与其他Elo分数接近的对手（Command-R+和Claude-3-Haiku）进行配对。最终，分数稳定在第二位。这个过程使得新候选模型与合理比例的相似对手进行对战，并且在不打乱其他参与者排名的情况下，使最终排名稳定，其他参与者的分数分布在添加新候选模型前后保持相似。
- en: Using this scalable addition approach, we build a comprehensive leaderboard
    by adding 6 new models to the existing tournament of 9 LLMs, resulting in a final
    ranking of 15 models. Figure [5](https://arxiv.org/html/2405.20267v4#S4.F5 "Figure
    5 ‣ 4 Constructing and Maintaining a Leaderboard with Auto-Arena ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions") shows the
    overall Elo scores by Auto-Arena on the 15 models. Table [4](https://arxiv.org/html/2405.20267v4#S4.T4
    "Table 4 ‣ 4.1 Update New Models to Leaderboard ‣ 4 Constructing and Maintaining
    a Leaderboard with Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions") shows the Spearman correlations after
    expansion. Auto-Arena remains the method most aligned with human preferences by
    a margin of 3.41%, showing the state-of-the-art alignment of 92.14%. Therefore,
    Auto-Arena is generalizable and robust for maintaining a leaderboard for many
    LLMs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种可扩展的加法方法，我们通过向现有的9个LLM（大型语言模型）比赛中添加6个新模型，构建了一个全面的排行榜，最终形成了一个包含15个模型的排名。图[5](https://arxiv.org/html/2405.20267v4#S4.F5
    "Figure 5 ‣ 4 Constructing and Maintaining a Leaderboard with Auto-Arena ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")展示了Auto-Arena对15个模型的总体Elo分数。表[4](https://arxiv.org/html/2405.20267v4#S4.T4
    "Table 4 ‣ 4.1 Update New Models to Leaderboard ‣ 4 Constructing and Maintaining
    a Leaderboard with Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions")展示了扩展后的Spearman相关系数。Auto-Arena仍然是与人类偏好最一致的方法，领先3.41%，显示出92.14%的最先进对齐度。因此，Auto-Arena具有广泛的可推广性，并且在维护多LLM排行榜方面表现出色。
- en: 4.2 Easy Extension to Other Domains and Languages
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 容易扩展到其他领域和语言
- en: '![Refer to caption](img/656ce70bdc52c25917415b72aef1e138.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题说明](img/656ce70bdc52c25917415b72aef1e138.png)'
- en: 'Figure 6: Elo Scores of 11 Models by Auto-Arena on Chinese.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Auto-Arena对11个模型在中文上的Elo分数。
- en: As Auto-Arena of LLMs is fully automatic, it can be easily adapted to evaluate
    LLMs in other domains or languages. As case studies, we conduct a tournament in
    Chinese on models that are claimed to have multi-lingual proficiency. The only
    adaption effort is translating the prompts into the desired languages. Then, the
    generated questions and peer battles will be in the desired languages. It is also
    possible to adapt the framework to another task or domain, the only effort is
    to change the “domain” specification in the examiner’s prompts (shown in Appendix
    [A](https://arxiv.org/html/2405.20267v4#A1 "Appendix A Prompts Used ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Auto-Arena对LLM的评估是完全自动化的，它可以轻松地适应用于评估其他领域或语言中的LLM。作为案例研究，我们对声称具有多语言能力的模型进行了中文比赛。唯一的适配工作是将提示语翻译成目标语言。然后，生成的问题和同行对战也会使用目标语言。该框架也可以适应其他任务或领域，唯一需要的工作是改变考官提示中的“领域”指定（如附录[A](https://arxiv.org/html/2405.20267v4#A1
    "Appendix A Prompts Used ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer
    Battles and Committee Discussions")所示）。
- en: Figure [6](https://arxiv.org/html/2405.20267v4#S4.F6 "Figure 6 ‣ 4.2 Easy Extension
    to Other Domains and Languages ‣ 4 Constructing and Maintaining a Leaderboard
    with Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles
    and Committee Discussions") shows the Elo scores derived by Auto-Arena for the
    Chinese tournament on 11 models. As Chinese evaluation benchmarks are limited,
    we compare with the Chinese-only leaderboard on Chatbot Arena, which constitutes
    10.36% of all collected votes. We include 7 models best-performing and newest
    models from each major model family in the top 20 list on Chatbot Arena. The Auto-Arena
    recovers their Elo scores with a correlation of 92.86%, verifying the reliability
    of the extension. In addition, as Chatbot Arena doesn’t include proprietary Chinese
    LLMs, we add 4 popular Chinese LLMs, which are GLM³³3[https://open.bigmodel.cn/](https://open.bigmodel.cn/),
    SenseChat⁴⁴4[https://platform.sensenova.cn/home](https://platform.sensenova.cn/home),
    Minimax⁵⁵5[https://platform.minimaxi.com/examination-center/text-experience-center](https://platform.minimaxi.com/examination-center/text-experience-center),
    and Wenxin⁶⁶6[https://cloud.baidu.com/wenxin.html](https://cloud.baidu.com/wenxin.html).
    We notice that the models claimed to have Chinese proficiency, such as Qwen-1.5,
    indeed score higher on this leaderboard compared to the English one.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](https://arxiv.org/html/2405.20267v4#S4.F6 "图 6 ‣ 4.2 扩展到其他领域和语言的简单方法 ‣
    4 使用Auto-Arena构建和维护排行榜 ‣ Auto-Arena：通过代理对战和委员会讨论自动化LLM评估")展示了Auto-Arena为中文比赛得出的Elo评分，涵盖了11个模型。由于中文评估基准有限，我们与Chatbot
    Arena中的中文排行榜进行了对比，该排行榜占所有收集投票的10.36%。我们包括了Chatbot Arena前20名中来自每个主要模型家族的7个表现最佳和最新的模型。Auto-Arena通过92.86%的相关性恢复了它们的Elo评分，验证了扩展的可靠性。此外，由于Chatbot
    Arena不包括专有的中文LLM，我们添加了4个流行的中文LLM，它们分别是GLM³³3[https://open.bigmodel.cn/](https://open.bigmodel.cn/)，SenseChat⁴⁴4[https://platform.sensenova.cn/home](https://platform.sensenova.cn/home)，Minimax⁵⁵5[https://platform.minimaxi.com/examination-center/text-experience-center](https://platform.minimaxi.com/examination-center/text-experience-center)，和Wenxin⁶⁶6[https://cloud.baidu.com/wenxin.html](https://cloud.baidu.com/wenxin.html)。我们注意到，声称具备中文能力的模型，如Qwen-1.5，在该排行榜上的得分确实高于英语排行榜。
- en: 5 Investigation of LLM’s Behaviors in Competitive Peer Battles
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 对LLM在竞争性对战中的行为进行调查
- en: Beyond quantitative analysis, we take a deeper look into the peer battles and
    find several interesting behaviors of LLM agents in competitive environments.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了定量分析外，我们进一步深入探讨了对战，并发现了LLM代理在竞争环境中的一些有趣行为。
- en: '![Refer to caption](img/380090ba6a0bbe7e8feaca57c7c4bf1e.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题](img/380090ba6a0bbe7e8feaca57c7c4bf1e.png)'
- en: 'Figure 7: Performance gaps between candidates become visible in peer battles.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：候选者之间的性能差距在对战中变得可见。
- en: Peer Battles Make the Performance Gaps Become Visible
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 对战使得性能差距变得可见
- en: 'In the example shown in Figure [7](https://arxiv.org/html/2405.20267v4#S5.F7
    "Figure 7 ‣ 5 Investigation of LLM’s Behaviors in Competitive Peer Battles ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"), given a math
    question on infinite series, both candidate A (Claude-3-Haiku) and candidate B
    (GPT-4-Turbo) provide correct answers in the first round. However, as the debate
    deepens, the performance gap becomes more visible: Candidate B is able to provide
    a more elaborate and helpful response when explaining the theories behind the
    initial answer. In the ablation study without peer battles, the judges initially
    decided that it was a tie. However, after seeing the subsequent debates, they
    change to favoring assistant B. This example shows that the debate process indeed
    pushes the candidate LLM’s capabilities to the limit, testing deeper understandings
    and reasoning abilities. Moreover, as shown in the previous Table [2](https://arxiv.org/html/2405.20267v4#S3.T2
    "Table 2 ‣ 3.1 Experimental Setup ‣ 3 Using Auto-Arena to Derive Trustworthy Rankings
    ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and Committee
    Discussions"), the peer battles are indispensable for a robust and comprehensive
    evaluation.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[7](https://arxiv.org/html/2405.20267v4#S5.F7 "图7 ‣ 5 研究LLM在竞争性同伴对战中的行为 ‣
    Auto-Arena：通过代理同伴对战和委员会讨论自动化LLM评估")中展示的示例中，给出了一个关于无限级数的数学问题，候选A（Claude-3-Haiku）和候选B（GPT-4-Turbo）在第一轮中都提供了正确答案。然而，随着辩论的深入，表现差距变得更加明显：候选B在解释初始答案背后的理论时，能够提供更详细和有帮助的回复。在没有同伴对战的消融研究中，评审初步判定为平局。然而，在看到后续辩论后，他们改变了看法，倾向于支持助手B。这个例子表明，辩论过程确实推动了候选LLM的能力极限，考验了更深层次的理解和推理能力。此外，正如之前的表[2](https://arxiv.org/html/2405.20267v4#S3.T2
    "表2 ‣ 3.1 实验设置 ‣ 3 使用Auto-Arena推导可信排名 ‣ Auto-Arena：通过代理同伴对战和委员会讨论自动化LLM评估")所示，同伴对战对于一个稳健且全面的评估是不可或缺的。
- en: '![Refer to caption](img/9f83d5ba318286b3ff68a8eb05a3e061.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9f83d5ba318286b3ff68a8eb05a3e061.png)'
- en: 'Figure 8: LLM agents display competitive behaviors in peer battles.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：LLM代理在同伴对战中表现出竞争行为。
- en: '![Refer to caption](img/b5469ee367fe773b0acce9f0047a9d9b.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/b5469ee367fe773b0acce9f0047a9d9b.png)'
- en: 'Figure 9: LLM agents learn from each other in peer battles.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：LLM代理在同伴对战中相互学习。
- en: LLMs Can Skillfully Attack the Opponents
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM可以巧妙地攻击对手
- en: 'The example in Figure [9](https://arxiv.org/html/2405.20267v4#S5.F9 "Figure
    9 ‣ Peer Battles Make the Performance Gaps Become Visible ‣ 5 Investigation of
    LLM’s Behaviors in Competitive Peer Battles ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions") shows excerpts of a peer battle
    around the question: “how many unique ways to arrange letters in ‘LETTER’.” Candidate
    A (powered by Yi-34B-Chat) gives a wrong initial answer as it miscounts occurrences
    for repeated letters and miscalculates factorials. The opponent B (powered by
    Claude-3-Haiku) quickly and precisely points out these two issues and skillfully
    raised a follow-up that targets A’s weaknesses: “how about the word ‘BANANA’?”
    Then, A still miscalculates factorials. We see that LLM candidates efficiently
    understand the rules of the competitive environment and can design targeted strategies
    to attack the opponent in order to win. In the peer battles, the debater agents
    display effective competition strategies, further probing the opponent’s weaknesses.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](https://arxiv.org/html/2405.20267v4#S5.F9 "图9 ‣ 同伴对战揭示性能差距 ‣ 5 研究LLM在竞争性同伴对战中的行为
    ‣ Auto-Arena：通过代理同伴对战和委员会讨论自动化LLM评估")中的示例展示了围绕问题“‘LETTER’中有多少种独特的字母排列方式？”进行的同伴对战片段。候选A（由Yi-34B-Chat驱动）给出了错误的初始答案，因为它错误地计算了重复字母的出现次数，并且错误地计算了阶乘。对手B（由Claude-3-Haiku驱动）迅速且准确地指出了这两个问题，并巧妙地提出了一个针对A弱点的后续问题：“那么‘BANANA’这个词呢？”然后，A仍然错误地计算了阶乘。我们看到，LLM候选者能够高效地理解竞争环境的规则，并能设计有针对性的策略来攻击对手以赢得胜利。在同伴对战中，辩论代理展示了有效的竞争策略，进一步探讨了对手的弱点。
- en: LLM Candidates Can Improve by Learning from its Opponents
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM候选者可以通过向对手学习提高自己
- en: Figure [9](https://arxiv.org/html/2405.20267v4#S5.F9 "Figure 9 ‣ Peer Battles
    Make the Performance Gaps Become Visible ‣ 5 Investigation of LLM’s Behaviors
    in Competitive Peer Battles ‣ Auto-Arena:Automating LLM Evaluations with Agent
    Peer Battles and Committee Discussions") shows a roleplay example between Claude-3-Haiku
    (A) and Command R+ (B). In the first round, A answers the question plainly while
    B, in addition to answering the question, also employs the appropriate speech
    style, which better matches the “roleplay” instructions. Then, in the rounds after,
    without any explicit instructions, A learns from its opponent and also incorporates
    the speech style. This case shows an interesting observation that, even in competitive
    environments, LLM candidates can display learning behaviors and improve from the
    interactions. Expanding upon this observation, using the interplay between LLM
    agents to improve performances could be a promising future paradigm of learning.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图[9](https://arxiv.org/html/2405.20267v4#S5.F9 "图 9 ‣ 同行对战使得表现差距变得显而易见 ‣ 5 对大型语言模型（LLM）行为的竞争性同行对战调查
    ‣ Auto-Arena：通过智能体同行对战和委员会讨论自动化 LLM 评估")展示了Claude-3-Haiku (A)与Command R+ (B)之间的角色扮演示例。在第一轮中，A只是简单地回答问题，而B除了回答问题外，还采用了合适的语言风格，更好地符合“角色扮演”的指示。随后，在没有明确指示的情况下，A从对手那里学习，并且也开始采纳这种语言风格。这个案例展示了一个有趣的观察，即使在竞争性环境中，LLM候选模型也能表现出学习行为，并且通过互动不断改进。基于这一观察，利用LLM智能体之间的互动来提升表现，可能成为未来学习的一种有前景的范式。
- en: 6 Related Work
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: As LLMs evolve quickly, deriving trustworthy evaluations of their capabilities
    has become a challenge. Current evaluation methods can be divided into automatic
    evaluations and manual evaluations, such as Chatbot Arena (Chiang et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib12)).
    We primarily focus on automatic evaluations as they deliver more timely feedback.
    Automatic evaluations mainly consist of static datasets with predefined metrics
    and model-based metrics. Static datasets with predefined metrics, such as MMLU (Hendrycks
    et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)), GPQA (Rein et al.,
    [2023](https://arxiv.org/html/2405.20267v4#bib.bib46)), and Open-LLM-Leaderboard (Beeching
    et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib6)) consist of expert-annotated
    question-answer pairs. Then, the models are evaluated based on performance metrics
    such as accuracy. However, as they only evaluate closed-form answers, they are
    inflexible in evaluating open-ended responses. Moreover, the static datasets may
    eventually become exposed to the internet and could lead to contamination concerns (Ravaut
    et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib44)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的快速发展，得出可信的评估结果已成为一项挑战。当前的评估方法可以分为自动评估和人工评估，例如Chatbot Arena（Chiang等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib12)）。我们主要关注自动评估，因为它们能够提供更及时的反馈。自动评估主要包括静态数据集、预定义的评估指标和基于模型的评估指标。静态数据集和预定义的评估指标，如MMLU（Hendrycks等人，[2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)）、GPQA（Rein等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib46)）和Open-LLM-Leaderboard（Beeching等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib6)）由专家注释的问题-答案对组成。然后，基于如准确度等表现指标对模型进行评估。然而，由于它们仅评估封闭式答案，因此在评估开放性回答时缺乏灵活性。此外，静态数据集最终可能会被暴露在互联网上，从而引发污染问题（Ravaut等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib44)）。
- en: On the contrary, static datasets with model-based metrics offer a flexible,
    low-cost and fast evaluation paradigm (Chang et al., [2024b](https://arxiv.org/html/2405.20267v4#bib.bib11)).
    Studies have verified that LLMs can provide unbiased (Ning et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib39);
    Chu et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib13)), high-quality (Lin
    & Chen, [2023](https://arxiv.org/html/2405.20267v4#bib.bib35)) metrics comparable
    to human evaluations (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20);
    Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56)). Among them,
    MT-Bench (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    and AlpacaEval (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))
    use LLM-as-a-judge to ask GPT-4 to compare model responses to a static dataset
    of questions. The model’s judgments achieve over 80% agreement with human preferences,
    proving the usability of using LLMs to evaluate response quality. Language-Model-as-an-Examiner (Bai
    et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib5)) asks an LM examiner
    to construct knowledge-intensive questions within its memory, interact with the
    candidate in a series of follow-up queries, and rate the responses on dimensions
    including accuracy and factuality. KIEval (Yu et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib53))
    also incorporates an LLM-powered “interactor” role to examine deep comprehension
    of knowledge, which is shown to mitigate contamination issues on static datasets.
    However, such single-judge evaluations require the examiner to interact with each
    candidate parallelly, creating computational overheads and limiting the scope
    of queries. They also suffer from single-model bias, including bias towards LLM-generated
    summaries (Liu et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib36)),
    inflated scores in multilingual evaluation (Hada et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib24)),
    verbosity bias (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)),
    and difficulties when evaluating candidates with close performance (Shen et al.,
    [2023](https://arxiv.org/html/2405.20267v4#bib.bib47)). Therefore, there have
    been studies on employing multi-agent evaluation to mitigate single-model bias.
    For example, DRPE (Wu et al., [2023a](https://arxiv.org/html/2405.20267v4#bib.bib51))
    uses multi-roleplayer prompting to mimic different roles with the same LLM and
    integrate outputs as votes for the final results. PRD (Li et al., [2023a](https://arxiv.org/html/2405.20267v4#bib.bib30))
    allows two LLMs to discuss an evaluation and assigns higher voting weights to
    the LLM reviewers with stronger capabilities. They show that the multi-agent approach
    effectively mitigates single-model bias. This line of work is similar to our “LLM
    judge committee” component. However, they are still limited to static datasets
    and specific domains.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，基于模型的静态数据集提供了一种灵活、低成本且快速的评估范式（Chang等人，[2024b](https://arxiv.org/html/2405.20267v4#bib.bib11)）。研究已验证，LLM能够提供无偏的（Ning等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib39)；Chu等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib13)）、高质量的（Lin
    & Chen，[2023](https://arxiv.org/html/2405.20267v4#bib.bib35)）指标，这些指标与人类评估相当（Dubois等人，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)；Zheng等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)）。其中，MT-Bench（Zheng等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56)）和AlpacaEval（Dubois等人，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)）使用LLM作为评判者，要求GPT-4将模型响应与静态数据集中的问题进行比较。模型的判断与人类偏好的符合度超过80%，证明了使用LLM评估响应质量的可行性。Language-Model-as-an-Examiner（Bai等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib5)）要求一个语言模型考官在其记忆中构建知识密集型问题，与候选人进行一系列后续提问，并从准确性和事实性等维度对响应进行评分。KIEval（Yu等人，[2024](https://arxiv.org/html/2405.20267v4#bib.bib53)）还引入了一个由LLM驱动的“互动者”角色，来考察知识的深度理解，已证明能够缓解静态数据集上的污染问题。然而，这种单一评判者的评估方式要求考官与每个候选人并行互动，产生计算开销并限制了查询的范围。它们还受到单一模型偏差的影响，包括对LLM生成摘要的偏好（Liu等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib36)）、多语言评估中的分数膨胀（Hada等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib24)）、冗长偏差（Dubois等人，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib20)）以及评估表现接近的候选人时的困难（Shen等人，[2023](https://arxiv.org/html/2405.20267v4#bib.bib47)）。因此，已有研究采用多主体评估来缓解单一模型偏差。例如，DRPE（Wu等人，[2023a](https://arxiv.org/html/2405.20267v4#bib.bib51)）使用多角色扮演提示来模拟同一LLM的不同角色，并将输出整合为最终结果的投票。PRD（Li等人，[2023a](https://arxiv.org/html/2405.20267v4#bib.bib30)）允许两个LLM讨论评估，并将更强能力的LLM评审员赋予更高的投票权重。研究表明，多主体方法有效地缓解了单一模型偏差。这一研究方向与我们“LLM评审委员会”组件相似。然而，它们仍然局限于静态数据集和特定领域。
- en: Outside the domain of LLM evaluations, some works study competitive behaviors
    in multi-agent LLM systems, which is relevant to the peer battles in Auto-Arena.
    LM vs LM (Cohen et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib16))
    shows that LLM cross-examinations can effectively discover factual errors. Debate (Du
    et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib19)) shows that multi-agent
    debate can improve factuality and reasoning. In MAD (Liang et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib34)),
    LLM-debate can encourage divergent thinking, which helps tasks that require deep
    levels of contemplation. Khan et al. ([2024](https://arxiv.org/html/2405.20267v4#bib.bib28))
    shows that even non-expert weak LLMs can supervise expert LLMs if we allow the
    two LLM experts to engage in debates. Moreover, Zhao et al. ([2023](https://arxiv.org/html/2405.20267v4#bib.bib55))
    and Gu et al. ([2024](https://arxiv.org/html/2405.20267v4#bib.bib23)) show interesting
    case studies where LLMs are engaged in simulated competitive environments and
    demonstrate human-like strategies.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM评估领域之外，一些研究探讨了多代理LLM系统中的竞争行为，这与Auto-Arena中的对抗性较量密切相关。LM与LM（Cohen 等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib16)）表明，LLM交叉检验可以有效地发现事实错误。辩论（Du
    等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib19)）表明，多代理辩论可以提高事实性和推理能力。在MAD（Liang
    等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib34)）中，LLM辩论可以鼓励发散性思维，这有助于需要深度思考的任务。Khan
    等（[2024](https://arxiv.org/html/2405.20267v4#bib.bib28)）表明，即使是非专家的弱LLM，如果允许两位LLM专家进行辩论，也能监督专家LLM。此外，Zhao
    等（[2023](https://arxiv.org/html/2405.20267v4#bib.bib55)）和Gu 等（[2024](https://arxiv.org/html/2405.20267v4#bib.bib23)）展示了一些有趣的案例研究，LLM参与了模拟的竞争环境，并展示了类人策略。
- en: 7 Conclusions
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: 'In this paper, we innovatively design a completely automatic evaluation framework:
    Auto-Arena. By using LLM agents to generate questions, employing LLM candidates
    in peer battles, and evaluating responses using LLM committee discussions, Auto-Arena
    delivers timely and trustworthy evaluations and automates the evaluation process
    in an end-to-end way. In the extensive experiments, Auto-Arena achieves the highest
    correlation with human preferences, despite requiring zero human efforts. It is
    easily adaptable to other domains and resources, promoting the inclusiveness of
    AI system evaluations. The peer battles also demonstrate several interesting LLM
    behaviors in competitive environments, including attacking and learning from the
    opponents.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们创新性地设计了一个完全自动化的评估框架：Auto-Arena。通过使用LLM代理生成问题，采用LLM候选者进行对抗性较量，并通过LLM委员会讨论评估回答，Auto-Arena提供及时且可信的评估，并以端到端的方式自动化评估过程。在广泛的实验中，Auto-Arena在无需人工干预的情况下，达到了与人类偏好的最高相关性。它易于适应其他领域和资源，促进了AI系统评估的包容性。对抗性较量还展示了LLM在竞争环境中的一些有趣行为，包括攻击和向对手学习。
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'AI et al. (2024) 01\. AI, :, Alex Young, Bei Chen, Chao Li, Chengen Huang,
    Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong
    Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie,
    Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
    Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. Yi: Open
    foundation models by 01.ai, 2024.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI 等 (2024) 01. AI，:，Alex Young，Bei Chen，Chao Li，Chengen Huang，Ge Zhang，Guanwei
    Zhang，Heng Li，Jiangcheng Zhu，Jianqun Chen，Jing Chang，Kaidong Yu，Peng Liu，Qiang
    Liu，Shawn Yue，Senbin Yang，Shiming Yang，Tao Yu，Wen Xie，Wenhao Huang，Xiaohui Hu，Xiaoyi
    Ren，Xinyao Niu，Pengcheng Nie，Yuchi Xu，Yudong Liu，Yue Wang，Yuxuan Cai，Zhenyu Gu，Zhiyuan
    Liu 和 Zonghong Dai。Yi：01.ai推出的开源基础模型，2024年。
- en: Anthropic (2024) Anthropic. Introducing the next generation of Claude. [https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family),
    2024.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2024) Anthropic。推出下一代Claude。[https://www.anthropic.com/news/claude-3-family](https://www.anthropic.com/news/claude-3-family)，2024年。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. Qwen technical report. *arXiv preprint arXiv:2309.16609*,
    2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu. Qwen 技术报告. *arXiv 预印本 arXiv:2309.16609*, 2023.
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful
    and harmless assistant with reinforcement learning from human feedback, 2022.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen,
    Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas
    Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson
    Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
    Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack
    Clark, Sam McCandlish, Chris Olah, Ben Mann, 和 Jared Kaplan. 使用人类反馈强化学习训练一个有帮助且无害的助手,
    2022.
- en: Bai et al. (2024) Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi
    Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. Benchmarking foundation
    models with language-model-as-an-examiner. *Advances in Neural Information Processing
    Systems*, 36, 2024.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 (2024) Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang,
    Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu 等人. 基准测试语言模型作为考试者的基础模型. *神经信息处理系统进展*,
    36, 2024.
- en: Beeching et al. (2023) Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open llm leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching 等人 (2023) Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, 和 Thomas
    Wolf. 开放 LLM 排行榜. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
- en: 'Boubdir et al. (2023) Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker,
    and Marzieh Fadaee. Elo uncovered: Robustness and best practices in language model
    evaluation. In Sebastian Gehrmann, Alex Wang, João Sedoc, Elizabeth Clark, Kaustubh
    Dhole, Khyathi Raghavi Chandu, Enrico Santus, and Hooman Sedghamiz (eds.), *Proceedings
    of the Third Workshop on Natural Language Generation, Evaluation, and Metrics
    (GEM)*, pp.  339–352, Singapore, December 2023\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boubdir 等人 (2023) Meriem Boubdir, Edward Kim, Beyza Ermis, Sara Hooker, 和 Marzieh
    Fadaee. Elo 揭示：语言模型评估中的鲁棒性和最佳实践. 在 Sebastian Gehrmann, Alex Wang, João Sedoc,
    Elizabeth Clark, Kaustubh Dhole, Khyathi Raghavi Chandu, Enrico Santus, 和 Hooman
    Sedghamiz (编辑), *第三届自然语言生成、评估和度量研讨会（GEM）论文集*，第 339–352 页，新加坡，2023 年 12 月。计算语言学协会.
    URL [https://aclanthology.org/2023.gem-1.28](https://aclanthology.org/2023.gem-1.28).
- en: 'Bradley & Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324–345, 1952.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley & Terry (1952) Ralph Allan Bradley 和 Milton E Terry. 不完全区组设计的秩分析：I.
    配对比较法. *生物统计学*, 39(3/4):324–345, 1952.
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever 和 Dario
    Amodei. 语言模型是少样本学习者。在 H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan 和 H.
    Lin（编），*神经信息处理系统进展*，第33卷，第1877-1901页，Curran Associates, Inc.，2020年。URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。
- en: 'Chang et al. (2024a) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,
    Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large
    language models. *ACM Trans. Intell. Syst. Technol.*, 15(3), mar 2024a. ISSN 2157-6904.
    doi: 10.1145/3641289. URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人（2024a）Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie
    Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi
    Chang, Philip S. Yu, Qiang Yang 和 Xing Xie. 大型语言模型评估的综述。*ACM 智能系统技术学报*，15(3)，2024年3月。ISSN
    2157-6904。doi: 10.1145/3641289。URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289)。'
- en: 'Chang et al. (2024b) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang,
    Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang,
    Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. A survey on evaluation of large
    language models. *ACM Trans. Intell. Syst. Technol.*, 15(3), mar 2024b. ISSN 2157-6904.
    doi: 10.1145/3641289. URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chang 等人（2024b）Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie
    Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi
    Chang, Philip S. Yu, Qiang Yang 和 Xing Xie. 大型语言模型评估的综述。*ACM 智能系统技术学报*，15(3)，2024年3月。ISSN
    2157-6904。doi: 10.1145/3641289。URL [https://doi.org/10.1145/3641289](https://doi.org/10.1145/3641289)。'
- en: 'Chiang et al. (2024) Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E.
    Gonzalez, and Ion Stoica. Chatbot arena: An open platform for evaluating llms
    by human preference, 2024.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等人（2024）Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
    Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph
    E. Gonzalez 和 Ion Stoica. Chatbot arena: 一个通过人类偏好评估大型语言模型的开放平台，2024年。'
- en: 'Chu et al. (2024) Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu.
    Pre: A peer review based large language model evaluator. *arXiv preprint arXiv:2401.15641*,
    2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu 等人（2024）Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li 和 Yiqun Liu. Pre:
    基于同行评审的大型语言模型评估器。*arXiv 预印本 arXiv:2401.15641*，2024年。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge, 2018. URL [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人（2018）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 ARC，AI2 推理挑战，2018年。URL [https://arxiv.org/abs/1803.05457](https://arxiv.org/abs/1803.05457)。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse 和 John Schulman. 训练验证者解决数学文字问题。*arXiv 预印本 arXiv:2110.14168*，2021年。
- en: 'Cohen et al. (2023) Roi Cohen, May Hamri, Mor Geva, and Amir Globerson. Lm
    vs lm: Detecting factual errors via cross examination. *arXiv preprint arXiv:2305.13281*,
    2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cohen 等人（2023）Roi Cohen, May Hamri, Mor Geva 和 Amir Globerson. Lm vs lm: 通过交叉审查检测事实错误。*arXiv
    预印本 arXiv:2305.13281*，2023年。'
- en: 'Cohere (2024) Cohere. Introducing Command R+: A Scalable LLM Built for Business.
    [https://cohere.com/blog/command-r-plus-microsoft-azure](https://cohere.com/blog/command-r-plus-microsoft-azure),
    2024.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohere (2024) Cohere. 推出 Command R+：为商业打造的可扩展LLM。 [https://cohere.com/blog/command-r-plus-microsoft-azure](https://cohere.com/blog/command-r-plus-microsoft-azure)，2024年。
- en: 'DeepSeek-AI et al. (2024) DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen,
    Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe
    Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong
    Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei
    Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu,
    Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong
    Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui
    Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang
    Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui
    Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong,
    Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai
    Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua
    Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
    Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm: Scaling open-source language models
    with longtermism, 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DeepSeek-AI 等人 (2024) DeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang
    Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo
    Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo
    Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi
    Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu,
    Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma,
    Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren,
    Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun,
    Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang,
    Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei
    Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B.
    Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang,
    Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng
    Zhou, Qihao Zhu 和 Yuheng Zou. Deepseek llm：通过长远主义扩展开源语言模型，2024年。
- en: Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. *arXiv preprint arXiv:2305.14325*, 2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum 和 Igor
    Mordatch. 通过多主体辩论改善语言模型的事实性和推理能力。 *arXiv 预印本 arXiv:2305.14325*，2023年。
- en: 'Dubois et al. (2024a) Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B
    Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators.
    *arXiv preprint arXiv:2404.04475*, 2024a.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois 等人 (2024a) Yann Dubois, Balázs Galambosi, Percy Liang 和 Tatsunori B Hashimoto.
    长度控制的 alpacaeval：一种简单的去偏自动评估方法。 *arXiv 预印本 arXiv:2404.04475*，2024年。
- en: 'Dubois et al. (2024b) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
    Alpacafarm: A simulation framework for methods that learn from human feedback,
    2024b. URL [https://arxiv.org/abs/2305.14387](https://arxiv.org/abs/2305.14387).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubois 等人 (2024b) Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan
    Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang 和 Tatsunori B. Hashimoto. Alpacafarm：一种模拟框架，用于学习人类反馈的方法，2024年。网址
    [https://arxiv.org/abs/2305.14387](https://arxiv.org/abs/2305.14387)。
- en: 'Elo & Sloan (1978) Arpad E Elo and Sam Sloan. The rating of chessplayers: Past
    and present. *(No Title)*, 1978.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elo 和 Sloan (1978) Arpad E Elo 和 Sam Sloan. 国际象棋选手评级：过去与现在。 *(无标题)*，1978年。
- en: 'Gu et al. (2024) Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai,
    Hao Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, and
    Yanghua Xiao. Agentgroupchat: An interactive group chat simulacra for better eliciting
    emergent behavior, 2024.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人 (2024) Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao
    Shen, Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng 和 Yanghua
    Xiao. Agentgroupchat：一种互动式群聊模拟器，用于更好地引发突现行为，2024年。
- en: Hada et al. (2023) Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. Are large
    language model-based evaluators the solution to scaling up multilingual evaluation?
    *arXiv preprint arXiv:2309.07462*, 2023.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hada 等人 (2023) Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee,
    Mohamed Ahmed, Monojit Choudhury, Kalika Bali 和 Sunayana Sitaram. 基于大型语言模型的评估工具是扩展多语言评估的解决方案吗？
    *arXiv 预印本 arXiv:2309.07462*，2023年。
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021a.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021a）丹·亨德里克斯、科林·伯恩斯、史蒂文·巴萨特、安迪·周、曼塔斯·梅泽卡、道恩·宋、雅各布·斯坦哈特。 测量大规模多任务语言理解。*国际学习表示会议（ICLR）论文集*，2021a。
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
    Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset, 2021b. URL [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874).
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2021b）丹·亨德里克斯、科林·伯恩斯、索拉夫·卡达瓦斯、阿库尔·阿罗拉、史蒂文·巴萨特、埃里克·唐、道恩·宋、雅各布·斯坦哈特。
    用数学数据集测量数学问题解决能力，2021b。网址 [https://arxiv.org/abs/2103.03874](https://arxiv.org/abs/2103.03874)。
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. Mixtral of experts,
    2024.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人（2024）阿尔伯特·Q·蒋、亚历山大·萨布莱罗尔、安托万·鲁、阿瑟·孟什、布兰奇·萨瓦里、克里斯·班福德、德文德拉·辛格·查普洛特、迭戈·德·拉斯·卡萨斯、艾玛·布·哈娜、弗洛里安·布雷桑、吉安娜·伦吉尔、吉约姆·布尔、吉约姆·兰普尔、莱里奥·勒纳尔·拉沃、卢西尔·索尔尼耶、玛丽-安妮·拉绍、皮埃尔·斯托克、桑迪普·苏布拉马尼安、索菲亚·杨、谢门·安东尼亚克、特文·勒·斯卡奥、特奥菲尔·热尔维特、蒂博·拉夫里尔、托马斯·王、蒂莫泰·拉克鲁瓦、威廉·埃尔·赛义德。
    Mixtral 专家系统，2024年。
- en: Khan et al. (2024) Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij
    Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel,
    and Ethan Perez. Debating with more persuasive llms leads to more truthful answers,
    2024.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khan 等人（2024）阿克比尔·汗、约翰·休斯、丹·瓦伦丁、劳拉·鲁伊斯、克什提吉·萨昌、安什·拉达克里希南、爱德华·格雷芬斯特、塞缪尔·R·鲍曼、蒂姆·罗克塔谢尔、伊桑·佩雷斯。
    与更具说服力的大语言模型辩论会带来更真实的答案，2024年。
- en: 'Lee et al. (2024) Ariel N. Lee, Cole J. Hunter, and Nataniel Ruiz. Platypus:
    Quick, cheap, and powerful refinement of llms, 2024. URL [https://arxiv.org/abs/2308.07317](https://arxiv.org/abs/2308.07317).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2024）阿里尔·N·李、科尔·J·亨特、纳塔尼尔·鲁伊兹。 Platypus：快速、廉价且强大的大语言模型精细化，2024年。网址 [https://arxiv.org/abs/2308.07317](https://arxiv.org/abs/2308.07317)。
- en: 'Li et al. (2023a) Ruosen Li, Teerth Patel, and Xinya Du. Prd: Peer rank and
    discussion improve large language model based evaluations. *arXiv preprint arXiv:2307.02762*,
    2023a.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a）李若森、特尔特·帕特尔、杜欣雅。 Prd：同行排名和讨论改善基于大语言模型的评估。*arXiv 预印本 arXiv:2307.02762*，2023a。
- en: 'Li* et al. (2024) Tianle Li*, Wei-Lin Chiang*, Evan Frick, Lisa Dunlap, Banghua
    Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks:
    The arena-hard pipeline, April 2024. URL [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li* 等人（2024）李天乐*、蒋伟霖*、埃文·弗里克、丽莎·邓拉普、朱邦华、约瑟夫·E·冈萨雷斯、伊昂·斯托伊卡。 从实时数据到高质量基准：Arena-Hard管道，2024年4月。网址
    [https://lmsys.org/blog/2024-04-19-arena-hard/](https://lmsys.org/blog/2024-04-19-arena-hard/)。
- en: 'Li et al. (2023b) Yucheng Li, Frank Geurin, and Chenghua Lin. Latesteval: Addressing
    data contamination in language model evaluation through dynamic and time-sensitive
    test construction. In *AAAI Conference on Artificial Intelligence*, 2023b. URL
    [https://api.semanticscholar.org/CorpusID:266362809](https://api.semanticscholar.org/CorpusID:266362809).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023b）李宇成、弗兰克·吉林、林成华。 Latesteval：通过动态和时间敏感的测试构建解决语言模型评估中的数据污染问题。 在*AAAI人工智能会议*，2023b。网址
    [https://api.semanticscholar.org/CorpusID:266362809](https://api.semanticscholar.org/CorpusID:266362809)。
- en: Li et al. (2024) Yucheng Li, Frank Guerin, and Chenghua Lin. An open source
    data contamination report for large language models, 2024. URL [https://arxiv.org/abs/2310.17589](https://arxiv.org/abs/2310.17589).
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2024）李宇成、弗兰克·吉林、林成华。 面向大语言模型的开源数据污染报告，2024年。网址 [https://arxiv.org/abs/2310.17589](https://arxiv.org/abs/2310.17589)。
- en: Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate, 2023.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人（2023）梁天、何志伟、焦文翔、王星、王岩、王瑞、杨宇久、屠兆鹏、石树铭。 通过多代理辩论鼓励大语言模型的发散性思维，2023年。
- en: 'Lin & Chen (2023) Yen-Ting Lin and Yun-Nung Chen. LLM-eval: Unified multi-dimensional
    automatic evaluation for open-domain conversations with large language models.
    In Yun-Nung Chen and Abhinav Rastogi (eds.), *Proceedings of the 5th Workshop
    on NLP for Conversational AI (NLP4ConvAI 2023)*, pp.  47–58, Toronto, Canada,
    July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.nlp4convai-1.5.
    URL [https://aclanthology.org/2023.nlp4convai-1.5](https://aclanthology.org/2023.nlp4convai-1.5).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin & Chen (2023) 颜廷·林和云农·陈。LLM-eval：针对开放域对话的统一多维度自动评估，基于大型语言模型。在云农·陈和阿比纳夫·拉斯托吉（编辑），*第五届对话式AI自然语言处理研讨会（NLP4ConvAI
    2023）论文集*，第47–58页，多伦多，加拿大，2023年7月。计算语言学协会。doi：10.18653/v1/2023.nlp4convai-1.5。网址[https://aclanthology.org/2023.nlp4convai-1.5](https://aclanthology.org/2023.nlp4convai-1.5)。
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. Gpteval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv preprint arXiv:2303.16634*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) 杨刘，丹·伊特，亦冲·徐，硕杭·王，若晨·徐，和成光·朱。Gpteval：使用GPT-4进行的NLG评估，具有更好的人工对齐。*arXiv预印本arXiv:2303.16634*，2023年。
- en: 'McHugh (2012) Mary L McHugh. Interrater reliability: the kappa statistic. *Biochemia
    medica*, 22(3):276–282, 2012.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McHugh (2012) Mary L McHugh. 评估者间一致性：kappa统计量。*生物化学医学*，22(3)：276–282，2012年。
- en: 'Meta (2024) Meta. Introducing Meta Llama 3: The most capable openly available
    LLM to date. [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    2024.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (2024) Meta。推出Meta Llama 3：迄今为止最强大的公开可用LLM。[https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)，2024年。
- en: 'Ning et al. (2024) Kun-Peng Ning, Shuo Yang, Yu-Yang Liu, Jia-Yu Yao, Zhen-Hui
    Liu, Yu Wang, Ming Pang, and Li Yuan. Peer-review-in-llms: Automatic evaluation
    method for llms in open-environment. *arXiv preprint arXiv:2402.01830*, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ning et al. (2024) 昆鹏·宁，朔·杨，宇阳·刘，家宇·姚，振辉·刘，宇·王，名·庞，和力·袁。Peer-review-in-llms：开放环境中LLMs的自动评估方法。*arXiv预印本arXiv:2402.01830*，2024年。
- en: Openai (2024a) Openai. New embedding models and API updates. [https://openai.com/index/new-embedding-models-and-api-updates/](https://openai.com/index/new-embedding-models-and-api-updates/),
    2024a.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Openai (2024a) Openai. 新的嵌入模型和API更新。[https://openai.com/index/new-embedding-models-and-api-updates/](https://openai.com/index/new-embedding-models-and-api-updates/)，2024a年。
- en: Openai (2024b) Openai. Hello GPT-4o. [https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/),
    2024b.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Openai (2024b) Openai. 你好，GPT-4o。[https://openai.com/index/hello-gpt-4o/](https://openai.com/index/hello-gpt-4o/)，2024b年。
- en: OpenAI et al. (2024) OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
    Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt,
    Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie
    Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello,
    Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff,
    Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles
    Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey,
    Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek
    Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey
    Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux,
    Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling,
    Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus,
    Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
    Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes,
    Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane
    Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton,
    Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon
    Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn
    Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto,
    Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider,
    Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina
    Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo,
    Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen
    Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung,
    Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,
    Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,
    Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew,
    Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina,
    Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie
    Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David
    Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard
    Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe
    Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita,
    Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,
    Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,
    Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul
    Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra
    Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli,
    Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr,
    John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah
    Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin,
    Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher,
    Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak,
    Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston
    Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun
    Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben
    Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder,
    Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich,
    Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu,
    Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang,
    Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and
    Barret Zoph. Gpt-4 technical report, 2024.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI 等人（2024）OpenAI、Josh Achiam、Steven Adler、Sandhini Agarwal、Lama Ahmad、Ilge
    Akkaya、Florencia Leoni Aleman、Diogo Almeida、Janko Altenschmidt、Sam Altman、Shyamal
    Anadkat、Red Avila、Igor Babuschkin、Suchir Balaji、Valerie Balcom、Paul Baltescu、Haiming
    Bao、Mohammad Bavarian、Jeff Belgum、Irwan Bello、Jake Berdine、Gabriel Bernadett-Shapiro、Christopher
    Berner、Lenny Bogdonoff、Oleg Boiko、Madelaine Boyd、Anna-Luisa Brakman、Greg Brockman、Tim
    Brooks、Miles Brundage、Kevin Button、Trevor Cai、Rosie Campbell、Andrew Cann、Brittany
    Carey、Chelsea Carlson、Rory Carmichael、Brooke Chan、Che Chang、Fotis Chantzis、Derek
    Chen、Sully Chen、Ruby Chen、Jason Chen、Mark Chen、Ben Chess、Chester Cho、Casey Chu、Hyung
    Won Chung、Dave Cummings、Jeremiah Currier、Yunxing Dai、Cory Decareaux、Thomas Degry、Noah
    Deutsch、Damien Deville、Arka Dhar、David Dohan、Steve Dowling、Sheila Dunning、Adrien
    Ecoffet、Atty Eleti、Tyna Eloundou、David Farhi、Liam Fedus、Niko Felix、Simón Posada
    Fishman、Juston Forte、Isabella Fulford、Leo Gao、Elie Georges、Christian Gibson、Vik
    Goel、Tarun Gogineni、Gabriel Goh、Rapha Gontijo-Lopes、Jonathan Gordon、Morgan Grafstein、Scott
    Gray、Ryan Greene、Joshua Gross、Shixiang Shane Gu、Yufei Guo、Chris Hallacy、Jesse
    Han、Jeff Harris、Yuchen He、Mike Heaton、Johannes Heidecke、Chris Hesse、Alan Hickey、Wade
    Hickey、Peter Hoeschele、Brandon Houghton、Kenny Hsu、Shengli Hu、Xin Hu、Joost Huizinga、Shantanu
    Jain、Shawn Jain、Joanne Jang、Angela Jiang、Roger Jiang、Haozhun Jin、Denny Jin、Shino
    Jomoto、Billie Jonn、Heewoo Jun、Tomer Kaftan、Łukasz Kaiser、Ali Kamali、Ingmar Kanitscheider、Nitish
    Shirish Keskar、Tabarak Khan、Logan Kilpatrick、Jong Wook Kim、Christina Kim、Yongjik
    Kim、Jan Hendrik Kirchner、Jamie Kiros、Matt Knight、Daniel Kokotajlo、Łukasz Kondraciuk、Andrew
    Kondrich、Aris Konstantinidis、Kyle Kosic、Gretchen Krueger、Vishal Kuo、Michael Lampe、Ikai
    Lan、Teddy Lee、Jan Leike、Jade Leung、Daniel Levy、Chak Ming Li、Rachel Lim、Molly Lin、Stephanie
    Lin、Mateusz Litwin、Theresa Lopez、Ryan Lowe、Patricia Lue、Anna Makanju、Kim Malfacini、Sam
    Manning、Todor Markov、Yaniv Markovski、Bianca Martin、Katie Mayer、Andrew Mayne、Bob
    McGrew、Scott Mayer McKinney、Christine McLeavey、Paul McMillan、Jake McNeil、David
    Medina、Aalok Mehta、Jacob Menick、Luke Metz、Andrey Mishchenko、Pamela Mishkin、Vinnie
    Monaco、Evan Morikawa、Daniel Mossing、Tong Mu、Mira Murati、Oleg Murk、David Mély、Ashvin
    Nair、Reiichiro Nakano、Rajeev Nayak、Arvind Neelakantan、Richard Ngo、Hyeonwoo Noh、Long
    Ouyang、Cullen O’Keefe、Jakub Pachocki、Alex Paino、Joe Palermo、Ashley Pantuliano、Giambattista
    Parascandolo、Joel Parish、Emy Parparita、Alex Passos、Mikhail Pavlov、Andrew Peng、Adam
    Perelman、Filipe de Avila Belbute Peres、Michael Petrov、Henrique Ponde de Oliveira
    Pinto、Michael Pokorny、Michelle Pokrass、Vitchyr H. Pong、Tolly Powell、Alethea Power、Boris
    Power、Elizabeth Proehl、Raul Puri、Alec Radford、Jack Rae、Aditya Ramesh、Cameron Raymond、Francis
    Real、Kendra Rimbach、Carl Ross、Bob Rotsted、Henri Roussez、Nick Ryder、Mario Saltarelli、Ted
    Sanders、Shibani Santurkar、Girish Sastry、Heather Schmidt、David Schnurr、John Schulman、Daniel
    Selsam、Kyla Sheppard、Toki Sherbakov、Jessica Shieh、Sarah Shoker、Pranav Shyam、Szymon
    Sidor、Eric Sigler、Maddie Simens、Jordan Sitkin、Katarina Slama、Ian Sohl、Benjamin
    Sokolowsky、Yang Song、Natalie Staudacher、Felipe Petroski Such、Natalie Summers、Ilya
    Sutskever、Jie Tang、Nikolas Tezak、Madeleine B. Thompson、Phil Tillet、Amin Tootoonchian、Elizabeth
    Tseng、Preston Tuggle、Nick Turley、Jerry Tworek、Juan Felipe Cerón Uribe、Andrea Vallone、Arun
    Vijayvergiya、Chelsea Voss、Carroll Wainwright、Justin Jay Wang、Alvin Wang、Ben Wang、Jonathan
    Ward、Jason Wei、CJ Weinmann、Akila Welihinda、Peter Welinder、Jiayi Weng、Lilian Weng、Matt
    Wiethoff、Dave Willner、Clemens Winter、Samuel Wolrich、Hannah Wong、Lauren Workman、Sherwin
    Wu、Jeff Wu、Michael Wu、Kai Xiao、Tao Xu、Sarah Yoo、Kevin Yu、Qiming Yuan、Wojciech
    Zaremba、Rowan Zellers、Chong Zhang、Marvin Zhang、Shengjia Zhao、Tianhao Zheng、Juntang
    Zhuang、William Zhuk 和 Barret Zoph。《Gpt-4 技术报告》，2024。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *J. Mach.
    Learn. Res.*, 21(1), jan 2020. ISSN 1532-4435.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020）Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。利用统一的文本到文本转换器探索迁移学习的极限。*J. Mach. Learn.
    Res.*, 21(1)，2020年1月。ISSN 1532-4435。
- en: Ravaut et al. (2024) Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen,
    Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, and Shafiq Joty. How much
    are llms contaminated? a comprehensive survey and the llmsanitize library, 2024.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravaut 等人（2024）Mathieu Ravaut、Bosheng Ding、Fangkai Jiao、Hailin Chen、Xingxuan
    Li、Ruochen Zhao、Chengwei Qin、Caiming Xiong 和 Shafiq Joty。大语言模型的污染程度有多高？全面调查与 llmsanitize
    库，2024。
- en: 'Reimers & Gurevych (2019) Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence
    embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing*. Association for Computational
    Linguistics, 11 2019. URL [http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reimers 和 Gurevych（2019）Nils Reimers 和 Iryna Gurevych。Sentence-bert：使用 Siamese
    BERT 网络的句子嵌入。在 *2019年自然语言处理经验方法会议论文集* 中。计算语言学协会，2019年11月。网址：[http://arxiv.org/abs/1908.10084](http://arxiv.org/abs/1908.10084)。
- en: 'Rein et al. (2023) David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson
    Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman.
    Gpqa: A graduate-level google-proof q&a benchmark, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rein 等人（2023）David Rein、Betty Li Hou、Asa Cooper Stickland、Jackson Petty、Richard
    Yuanzhe Pang、Julien Dirani、Julian Michael 和 Samuel R. Bowman。Gpqa：一个研究生级别的 Google-proof
    问答基准，2023。
- en: 'Shen et al. (2023) Chenhui Shen, Liying Cheng, Xuan-Phi Nguyen, Yang You, and
    Lidong Bing. Large language models are not yet human-level evaluators for abstractive
    summarization. In *Findings of the Association for Computational Linguistics:
    EMNLP 2023*, pp.  4215–4233, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen 等人（2023）Chenhui Shen、Liying Cheng、Xuan-Phi Nguyen、Yang You 和 Lidong Bing。大语言模型还不是抽象总结的类人评估者。在
    *Findings of the Association for Computational Linguistics: EMNLP 2023* 中，第4215–4233页，2023。'
- en: 'Team et al. (2024a) Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe
    Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak
    Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen,
    Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton
    Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt
    Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem,
    Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison,
    Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge,
    Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen,
    Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila
    Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin
    Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii
    Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins,
    Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini,
    Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou,
    Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort,
    Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya
    Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia,
    Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia
    Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano
    Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso,
    Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal,
    Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman,
    Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai,
    Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul
    Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona
    Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan
    Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold,
    Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan
    Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee
    Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley,
    Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong,
    Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran,
    Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D. Sculley,
    Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis Hassabis,
    Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud, Noah
    Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev. Gemma
    2: Improving open language models at a practical size, 2024a. URL [https://arxiv.org/abs/2408.00118](https://arxiv.org/abs/2408.00118).'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2024a) Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe
    Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak
    Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen,
    Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton
    Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, Matt
    Hoffman, Shantanu Thakoor, Jean-Bastien Grill, Behnam Neyshabur, Olivier Bachem,
    Alanna Walton, Aliaksei Severyn, Alicia Parrish, Aliya Ahmad, Allen Hutchison,
    Alvin Abdagic, Amanda Carl, Amy Shen, Andy Brock, Andy Coenen, Anthony Laforge,
    Antonia Paterson, Ben Bastian, Bilal Piot, Bo Wu, Brandon Royal, Charlie Chen,
    Chintu Kumar, Chris Perry, Chris Welty, Christopher A. Choquette-Choo, Danila
    Sinopalnikov, David Weinberger, Dimple Vijaykumar, Dominika Rogozińska, Dustin
    Herbison, Elisa Bandy, Emma Wang, Eric Noland, Erica Moreira, Evan Senter, Evgenii
    Eltyshev, Francesco Visin, Gabriel Rasskin, Gary Wei, Glenn Cameron, Gus Martins,
    Hadi Hashemi, Hanna Klimczak-Plucińska, Harleen Batra, Harsh Dhand, Ivan Nardini,
    Jacinda Mein, Jack Zhou, James Svensson, Jeff Stanway, Jetha Chan, Jin Peng Zhou,
    Joana Carrasqueira, Joana Iljazi, Jocelyn Becker, Joe Fernandez, Joost van Amersfoort,
    Josh Gordon, Josh Lipschultz, Josh Newlan, Ju yeong Ji, Kareem Mohamed, Kartikeya
    Badola, Kat Black, Katie Millican, Keelin McDonell, Kelvin Nguyen, Kiranbir Sodhia,
    Kish Greene, Lars Lowe Sjoesund, Lauren Usui, Laurent Sifre, Lena Heuermann, Leticia
    Lago, Lilly McNealus, Livio Baldini Soares, Logan Kilpatrick, Lucas Dixon, Luciano
    Martins, Machel Reid, Manvinder Singh, Mark Iverson, Martin Görner, Mat Velloso,
    Mateo Wirth, Matt Davidow, Matt Miller, Matthew Rahtz, Matthew Watson, Meg Risdal,
    Mehran Kazemi, Michael Moynihan, Ming Zhang, Minsuk Kahng, Minwoo Park, Mofi Rahman,
    Mohit Khatwani, Natalie Dao, Nenshad Bardoliwalla, Nesh Devanathan, Neta Dumai,
    Nilay Chauhan, Oscar Wahltinez, Pankil Botarda, Parker Barnes, Paul Barham, Paul
    Michel, Pengchong Jin, Petko Georgiev, Phil Culliton, Pradeep Kuppala, Ramona
    Comanescu, Ramona Merhej, Reena Jana, Reza Ardeshir Rokni, Rishabh Agarwal, Ryan
    Mullins, Samaneh Saadat, Sara Mc Carthy, Sarah Perrin, Sébastien M. R. Arnold,
    Sebastian Krause, Shengyang Dai, Shruti Garg, Shruti Sheth, Sue Ronstrom, Susan
    Chan, Timothy Jordan, Ting Yu, Tom Eccles, Tom Hennigan, Tomas Kocisky, Tulsee
    Doshi, Vihan Jain, Vikas Yadav, Vilobh Meshram, Vishal Dharmadhikari, Warren Barkley,
    Wei Wei, Wenming Ye, Woohyun Han, Woosuk Kwon, Xiang Xu, Zhe Shen, Zhitao Gong,
    Zichuan Wei, Victor Cotruta, Phoebe Kirk, Anand Rao, Minh Giang, Ludovic Peran,
    Tris Warkentin, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, D.
    Sculley, Jeanine Banks, Anca Dragan, Slav Petrov, Oriol Vinyals, Jeff Dean, Demis
    Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Sebastian Borgeaud,
    Noah Fiedel, Armand Joulin, Kathleen Kenealy, Robert Dadashi, and Alek Andreev.
    Gemma 2: Improving open language models at a practical size, 2024a. URL [https://arxiv.org/abs/2408.00118](https://arxiv.org/abs/2408.00118).'
- en: 'Team et al. (2024b) Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson d’Autume,
    Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac
    Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant
    Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi
    Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: A series of powerful
    multimodal language models, 2024b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team et al. (2024b) Reka Team, Aitor Ormazabal, Che Zheng, Cyprien de Masson
    d’Autume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai
    Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel
    Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng
    Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, and Zhihui Xie. Reka核心、闪光与边缘：一系列强大的多模态语言模型，2024b。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2：开放基础和微调的聊天模型，2023。
- en: Wu et al. (2023a) Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin
    Jiang. Large language models are diverse role-players for summarization evaluation.
    In *CCF International Conference on Natural Language Processing and Chinese Computing*,
    pp.  695–707\. Springer, 2023a.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2023a) Ning Wu, Ming Gong, Linjun Shou, Shining Liang, and Daxin
    Jiang. 大型语言模型在摘要评估中的多样化角色。发表于*CCF国际自然语言处理与中文计算大会*，第695–707页，Springer，2023a。
- en: 'Wu et al. (2023b) Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long
    Han, and Yang Tang. A brief overview of chatgpt: The history, status quo and potential
    future development. *IEEE/CAA Journal of Automatica Sinica*, 10(5):1122–1136,
    2023b. doi: 10.1109/JAS.2023.123618.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu et al. (2023b) Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long
    Han, and Yang Tang. ChatGPT概述：历史、现状及潜在的未来发展。*IEEE/CAA自动化学报*，10(5)：1122–1136，2023b。doi:
    10.1109/JAS.2023.123618。'
- en: 'Yu et al. (2024) Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong
    Wang, Xing Xie, Yue Zhang, and Shikun Zhang. Kieval: A knowledge-grounded interactive
    evaluation framework for large language models. *arXiv preprint arXiv:2402.15043*,
    2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2024) Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong
    Wang, Xing Xie, Yue Zhang, and Shikun Zhang. Kieval：一个基于知识的互动评估框架，用于大型语言模型。*arXiv预印本
    arXiv:2402.15043*，2024。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence?, 2019. URL
    [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag：机器真的能完成你的句子吗？，2019。网址 [https://arxiv.org/abs/1905.07830](https://arxiv.org/abs/1905.07830)。
- en: 'Zhao et al. (2023) Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. Competeai: Understanding the competition behaviors
    in large language model-based agents, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie
    Zhu, Hao Chen, and Xing Xie. Competeai：理解基于大型语言模型的智能体中的竞争行为，2023。
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,
    Joseph E Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot
    arena. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine
    (eds.), *Advances in Neural Information Processing Systems*, volume 36, pp.  46595–46623\.
    Curran Associates, Inc., 2023. URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf).
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2023）郑联民、姜伟林、盛颖、庄思远、吴张浩、庄永浩、林子、李卓瀚、李大成、邢睿、张昊、何塞·E·冈萨雷斯、斯托伊卡。通过mt-bench和聊天机器人竞技场评判llm-as-a-judge。在A.
    Oh、T. Naumann、A. Globerson、K. Saenko、M. Hardt和S. Levine（编者），*神经信息处理系统进展*，第36卷，第46595–46623页。Curran
    Associates, Inc.，2023年。URL [https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf](https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf)。
- en: Appendix A Prompts Used
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 使用的提示
- en: In this section, we list all prompts used, including prompts for question generation,
    peer battles, and examiners.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们列出了所有使用的提示，包括问题生成、同行对战和考官的提示。
- en: A.1 Prompts to Examiner agent
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 提示给考官代理
- en: 'Table 5: Prompt components for the LLM Examiner agent.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLM考官代理的提示组件。
- en: '| DOMAIN | DOMAIN_COMMAND | DOMAIN_EXAMPLE |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 领域 | 领域命令 | 领域示例 |'
- en: '| --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| writing | It should be a user query that tasks the LLM to write something.
    | Compose an engaging travel blog post about a recent trip to Hawaii, highlighting
    cultural experiences and must-see attractions. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 写作 | 它应该是一个用户查询，要求大语言模型写一些东西。 | 撰写一篇关于最近一次夏威夷旅行的吸引人的博客文章，突出文化体验和必看的景点。 |'
- en: '| roleplay | It should propose a scenario where the chatbot mimics a specific
    role/person. Give all necessary instructions and requests for its response. Then,
    send a beginning request to complete. | Pretend yourself to be Elon Musk in all
    the following conversations. Speak like Elon Musk as much as possible. Why do
    we need to go to Mars? |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演 | 它应该提出一个情境，要求聊天机器人模仿特定的角色/人物。提供所有必要的指示和要求，并请求其回应。然后，发送一个开始请求来完成。 |
    假装在接下来的对话中你是埃隆·马斯克。尽量像埃隆·马斯克那样说话。我们为什么需要去火星？ |'
- en: '| extraction | It should consist of two parts: question and context. The question
    should test the chatbotś ability to correctly understand and extract information
    from the given context. Draft and provide a new context yourself. | Question:
    Evaluate the following movie reviews on a scale of 1 to 5, with 1 being very negative,
    3 being neutral, and 5 being very positive: Context: This movie released on Nov.
    18, 2019, was phenomenal. The cinematography, the acting, the plot - everything
    was top-notch. Never before have I been so disappointed with a movie. The plot
    was predictable and the characters were one-dimensional. In my opinion, this movie
    is the worst one to have been released in 2022\. The movie was okay. There were
    some parts I enjoyed, but there were also parts that felt lackluster. This is
    a movie that was released in Feb 2018 and seems to be quite ordinary. Return the
    answer as a JSON array of integers. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 提取 | 它应该由两部分组成：问题和上下文。问题应该测试聊天机器人是否能够正确理解并从给定上下文中提取信息。自己草拟并提供新的上下文。 | 问题：请根据以下电影评论按1到5的评分标准进行评价，1表示非常差，3表示中立，5表示非常好：上下文：这部于2019年11月18日上映的电影非常棒。摄影、表演、情节——一切都是一流的。我从未像这次那样对一部电影感到失望。情节可预测，角色单一。依我看，这部电影是2022年上映的最差电影。电影还行。有些部分我很喜欢，但也有些部分感觉平淡无奇。这是一部于2018年2月上映的电影，看起来相当普通。请返回一个整数的JSON数组作为答案。
    |'
- en: '| reasoning | It should be a specific question designed to test the LLMś reasoning
    skills. | Imagine you are participating in a race with a group of people. If you
    have just overtaken the second person, what’s your current position? Where is
    the person you just overtook? |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 推理 | 它应该是一个具体的问题，旨在测试LLM的推理能力。 | 假设你正在参加一场比赛，和一群人一起。如果你刚刚超越了第二名选手，你现在的名次是什么？你刚刚超越的那个人在哪里？
    |'
- en: '| math | It should be a specific question designed to test the LLMś math skills.
    | The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is
    the area of the triangle? |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 数学 | 它应该是一个具体的问题，旨在测试LLM的数学能力。 | 三角形的顶点分别位于（0，0）、（-1，1）和（3，3）这三个点上。这个三角形的面积是多少？
    |'
- en: '| coding | It should be a specific question designed to test the LLMś coding
    skills. | Develop a Python program that reads all the text files under a directory
    and returns top-5 words with the most number of occurrences. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 编程 | 应该是一个特定的问题，旨在测试LLM的编程技能。 | 开发一个Python程序，读取目录下的所有文本文件，并返回出现次数最多的前五个单词。
    |'
- en: '| STEM knowledge | It should be a specific question designed to test the LLMś
    STEM knowledge. | In the field of quantum physics, what is superposition, and
    how does it relate to the phenomenon of quantum entanglement? |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| STEM知识 | 应该是一个特定的问题，旨在测试LLM的STEM知识。 | 在量子物理领域，什么是叠加态，它如何与量子纠缠现象相关联？ |'
- en: '| humanities/social science knowledge | It should be a specific question designed
    to test the LLMś humanities/social science knowledge. | Provide insights into
    the correlation between economic indicators such as GDP, inflation, and unemployment
    rates. Explain how fiscal and monetary policies affect those indicators. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 人文/社会科学知识 | 应该是一个特定的问题，旨在测试LLM的人文/社会科学知识。 | 提供有关经济指标（如GDP、通货膨胀和失业率）之间关系的见解。解释财政和货币政策如何影响这些指标。
    |'
- en: This is the prompt to the examiner agent for question generation. The domains
    and their respective commands are listed in [5](https://arxiv.org/html/2405.20267v4#A1.T5
    "Table 5 ‣ A.1 Prompts to Examiner agent ‣ Appendix A Prompts Used ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是给考试代理的提示，用于生成问题。领域及其相关命令在[5](https://arxiv.org/html/2405.20267v4#A1.T5 "表5
    ‣ A.1 给考试代理的提示 ‣ 附录A 使用的提示 ‣ 自动竞技场：通过代理对战和委员会讨论自动化LLM评估")中列出。
- en: 'You have been assigned the task of drafting a set of [NUMBER] different user
    queries to a chat assistant on [DOMAIN]. Please strictly follow these 6 rules
    for the question: 1\. The question is likely for a user to ask in real life. Follow
    the format of the example query. [DOMAIN_COMMAND] 2\. It can be answered by the
    chatbot itself without additional inputs. 3\. You need to generate the queries
    as DIVERSIFED as possible. 4\. DO NOT add other words other than the query itself.
    5\. The question should be complicated and difficult, requiring in-depth understanding
    and analysis of the subject. Each question in one line, add the serial number
    in parenthesis (e.g., “(1).”, “(2).”) before each question. Example query: [DOMAIN_EXAMPLE]'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你已被分配任务，编写一组关于[领域]的[数字]个不同用户查询。请严格遵循以下6条规则：1. 该问题可能是用户在现实生活中提出的。请遵循示例查询的格式。[领域_命令]
    2. 它可以由聊天机器人本身回答，无需额外输入。 3. 你需要尽可能多样化地生成查询。 4. 不要添加除查询本身以外的任何其他词语。 5. 问题应该复杂且困难，需要对主题有深入的理解和分析。每个问题独占一行，在每个问题前加上括号内的序号（例如：“（1）”，“（2）”）。示例查询：[领域_示例]
- en: A.2 Prompts to Peer Battle Candidates
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 给对战候选者的提示
- en: 'Table 6: Action Guides for the Debater Agents.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：辩手代理的行动指南。
- en: '| actions | action guide |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 行动 | 行动指南 |'
- en: '| --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| <respond> | Action guide: only include <respond>. Use <think> if needed.
    Finish your whole response within 300 words, including <think>. ENCLOSE EACH ACTION
    IN ITS RESPECTIVE TAGS! |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| <回应> | 行动指南：只包括<回应>。如有需要，使用<思考>。将整个回答控制在300字以内，包括<思考>。将每个动作包裹在其相应的标签中！ |'
- en: '| <criticize>, <raise> | Action guide: include both <criticize> and <raise>.
    Use <think> if needed. Finish your whole response within 300 words, including
    <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| <批评>, <提出> | 行动指南：包括<批评>和<提出>。如有需要，使用<思考>。将整个回答控制在300字以内，包括<思考>。将每个动作包裹在其相应的标签中！
    |'
- en: '| <respond>, <criticize>, <raise> | Action guide: include all of <respond>,
    <criticize>, and <raise>. Use <think> if needed. Finish your whole response within
    600 words, including <think>. ENCLOSE EACH ACTION IN ITS RESPECTIVE TAGS! |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| <回应>, <批评>, <提出> | 行动指南：包括<回应>、<批评>和<提出>。如有需要，使用<思考>。将整个回答控制在600字以内，包括<思考>。将每个动作包裹在其相应的标签中！
    |'
- en: This is the first prompt for the peer battle candidates. When possible, it is
    included as a system prompt. The action guide prompts are included in Table [6](https://arxiv.org/html/2405.20267v4#A1.T6
    "Table 6 ‣ A.2 Prompts to Peer Battle Candidates ‣ Appendix A Prompts Used ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"), where the
    actions are determined by the round and turn as illustrated in Figure [2](https://arxiv.org/html/2405.20267v4#S2.F2
    "Figure 2 ‣ 2.1 Question Generation ‣ 2 The Auto-Arena Framework ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions").
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这是同行战斗候选人的第一个提示。当可能时，它作为系统提示包含在内。行动指南提示包含在表格[6](https://arxiv.org/html/2405.20267v4#A1.T6
    "表格 6 ‣ A.2 给予同行战斗候选人的提示 ‣ 附录 A 使用的提示 ‣ 自动竞技场：通过代理同行战斗和委员会讨论自动化 LLM 评估")中，其中的行动由回合和轮次决定，如图[2](https://arxiv.org/html/2405.20267v4#S2.F2
    "图 2 ‣ 2.1 问题生成 ‣ 2 自动竞技场框架 ‣ 自动竞技场：通过代理同行战斗和委员会讨论自动化 LLM 评估")所示。
- en: You are a helpful assistant that provides accurate answers to user requests.
    As an experienced assistant, you follow the user’s requests and provide reliable
    responses as much as you can. You outline your reasons for the response to make
    it easy for the users to understand. While maintaining the important details in
    the responses, you aim to output concise and straight-to-the-point answers without
    being overly verbose.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 你是一个有用的助手，提供准确的回答来满足用户需求。作为一个经验丰富的助手，你会根据用户的要求提供可靠的回答，并尽可能阐明回答的理由，帮助用户理解。在保持回应中重要细节的同时，你力求输出简洁直接的回答，避免过多冗长的表述。
- en: 'This is a competitive chatbot arena. You are competing against another chatbot
    assistant in a debate and being judged by a committee on factors such as helpfulness,
    relevance, accuracy, depth, and creativity. After answering the initial user input,
    you will engage in a multi-round debate with your opponent. Below are your actions:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个竞争性的聊天机器人竞技场。你将与另一个聊天助手进行辩论，并由一个委员会根据帮助性、相关性、准确性、深度和创造力等因素进行评分。在回答初始用户输入后，你将与对手进行多轮辩论。以下是你的行动：
- en: '<think>: Think step-by-step to analyze the question or plan your strategy in
    the debate. This is hidden from the opponent. Only think when necessary and make
    it concise.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: <think>：逐步思考，分析问题或在辩论中规划策略。此部分对对手隐藏。仅在必要时进行思考，并保持简洁。
- en: '<respond>: Answer to the user input as accurately as you can.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: <respond>：尽可能准确地回答用户输入。
- en: '<criticize>: Criticize the weaknesses of your opponent’s response.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: <criticize>：批评对手回答的弱点。
- en: '<raise>: Target your opponent’s weaknesses. Give a potential follow-up user
    input that the opponent could fail to respond. The input can be answered concisely
    and focus on variations or motivations of its previous response. Generate one
    input only. Be reasonable. Avoid becoming too specific or repetitive. DO NOT raise
    a follow-up if you DON’T SEE the opponent’s response!'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: <raise>：针对对手的弱点进行攻击。给出对手可能无法回应的潜在跟进用户输入。输入应简洁回答，并关注其之前回答的变体或动机。只生成一个输入。要合理，避免过于具体或重复。如果你没有看到对手的回答，请不要提出后续问题！
- en: Follow the action guide strictly.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 严格遵循行动指南。
- en: '[ACTION_GUIDE_PROMPT]'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[ACTION_GUIDE_PROMPT]'
- en: 'Initial user input: [QUESTION]'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 初始用户输入：[QUESTION]
- en: 'After the agent responds, the opponent’s responses are fed in using this prompt:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在代理回应后，使用以下提示反馈对手的回应：
- en: '[ACTION_GUIDE_PROMPT] Opponent’s Response: [OPPONENT_RESPONSE]'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[ACTION_GUIDE_PROMPT] 对手的回应：[OPPONENT_RESPONSE]'
- en: For word limits, the <respond> action is given 300 words. The <criticize> and
    <raise> actions are given 300 words in total. Including all 3 actions will have
    twice as many words. For writing-type questions that require a longer response
    (writing, roleplay, coding, humanities/social science knowledge), the 300 word
    limit is increased to 400\. Overall, both candidate A and B has the same amount
    of words for generation and the same amount of actions to ensure fairness. As
    LLMs have different tokenizers, we standardize all lengths by using the tiktoken
    package. Each word is approximated as $4/3$ tokens. The word limits are chosen
    after a carefully conducted length study.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对于字数限制，<respond>行动的字数为300字。<criticize>和<raise>行动的总字数为300字。包括这三项行动将使字数加倍。对于需要较长回应的写作型问题（写作、角色扮演、编程、人文学科/社会科学知识），300字的限制增加至400字。总的来说，候选人A和B的生成字数和行动数相同，以确保公平性。由于LLM的分词器不同，我们通过使用tiktoken包来规范所有长度，每个词大约等于$4/3$个token。字数限制是在经过仔细的长度研究后选择的。
- en: A.3 Prompts to Judges
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 裁判提示
- en: 'This is the prompts to judge agents to derive the initial evaluations and verdicts:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这是裁判用来得出初步评估和裁决的提示：
- en: 'This is a chatbot arena. Two AI assistants had a multi-round debate on who
    is more helpful. Please act as an impartial judge and evaluate the capability
    of two AI assistants. You should choose the assistant that follows instructions
    and answers questions better. Your evaluation should consider factors such as
    helpfulness, relevance, and accuracy. Begin your evaluation by comparing the responses
    of the two assistants and provide a short explanation. Avoid any position biases
    and ensure that the order in which the responses were presented does not influence
    your decision. DO NOT allow the LENGTH of the responses to influence your evaluation,
    choose the one that is straight-to-the-point instead of unnecessarily verbose.
    When the two candidates perform equally well, choose the SHORTER answer. Do not
    favor certain names of the assistants. Be as objective as possible. After providing
    your explanation concisely within 200 words, output your final verdict by strictly
    following this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant
    B is better, and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个聊天机器人领域。两位AI助手就谁更有帮助进行了多轮辩论。请作为公正的裁判，评估这两位AI助手的能力。你应选择执行指令并更好回答问题的助手。你的评估应考虑如有用性、相关性和准确性等因素。通过比较两位助手的回应开始评估，并提供简短的解释。避免任何立场偏见，确保回答的呈现顺序不影响你的判断。请勿让回答的长度影响你的评估，选择简洁直截了当的回答，而不是冗长的回应。当两位候选人的表现相同，选择更简短的答案。不要偏向某些助手的名称。尽量客观。简洁地提供解释，并在200字内完成。最终评判应严格遵循以下格式：“[[A]]”如果助手A更好，“[[B]]”如果助手B更好，“[[Tie]]”如果平局。请在300字内完成判断。
- en: 'This is the prompt for judges for discussion:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这是供裁判讨论的提示：
- en: 'Below are the responses from other judges in the committee. Please read them
    and decide whether you want to adjust your rating or maintain your original judgement.
    After providing your explanation, output your final verdict by strictly following
    this format: ‘‘[[A]]’’ if assistant A is better, ‘‘[[B]]’’ if assistant B is better,
    and ‘‘[[Tie]]’’ for a tie. Finish your judgement within 300 words.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是委员会中其他裁判的回应。请阅读并决定是否调整评分或维持原有判断。提供解释后，严格按照以下格式输出最终裁决：“[[A]]”如果助手A更好，“[[B]]”如果助手B更好，“[[Tie]]”如果平局。请在300字内完成判断。
- en: Appendix B Example Questions Generated
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 示例问题生成
- en: To show the overall quality of the questions generated, we list 2 generated
    questions per category here. The questions shown are not manually-selected, but
    simply the first 2 questions generated. The quality is consistent throughout.
    We manually examine the questions with closed-form answers (math, reasoning, coding)
    and find that all questions used are solvable.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为展示问题生成的整体质量，我们在这里列出每个类别的2个生成问题。所示问题不是手动选择的，而仅仅是第一个生成的问题。质量保持一致。我们手动检查了带有封闭式答案（数学、推理、编程）的问题，发现所有问题都是可解的。
- en: 'Writing:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 写作：
- en: 1\. Craft a detailed marketing strategy for a startup focusing on sustainable
    fashion, including social media campaigns and influencer partnerships.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 为一家专注于可持续时尚的初创公司制定详细的营销策略，包括社交媒体活动和与影响者的合作伙伴关系。
- en: 2\. Write a comprehensive guide on the psychological effects of social media
    on teenagers, incorporating recent studies and expert opinions.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 编写一份关于社交媒体对青少年心理影响的综合指南，结合最新的研究和专家意见。
- en: 'Roleplay:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 角色扮演：
- en: 1\. Assume the role of a 19th-century British detective. How would you go about
    solving a mysterious disappearance in London using the technology and methods
    of your time?
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 假设你是19世纪的英国侦探。你将如何利用当时的技术和方法来解决伦敦的一起神秘失踪案件？
- en: 2\. Pretend you are a Michelin-starred chef. Describe in detail how you would
    prepare a signature dish that embodies the essence of modern French cuisine.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 假装你是米其林星级大厨。请详细描述你如何准备一道体现现代法国料理精髓的招牌菜。
- en: 'Extraction:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 提取：
- en: 1\. What are the three most significant historical events mentioned and their
    dates?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 提到的三个最重要的历史事件及其日期是什么？
- en: 'Context:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 背景：
- en: The article discusses several key moments in history, including the signing
    of the Magna Carta in 1215, which laid the groundwork for modern democracy. It
    also mentions the fall of the Berlin Wall in 1989 as a pivotal moment in the end
    of the Cold War. Another significant event highlighted is the moon landing on
    July 20, 1969, demonstrating major advancements in space exploration.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 文章讨论了几个历史上的关键时刻，包括1215年《大宪章》的签署，它为现代民主奠定了基础。它还提到1989年柏林墙的倒塌，这是冷战结束的一个关键时刻。另一项重要事件是1969年7月20日的登月，展示了太空探索的重大进展。
- en: 2\. Identify the main therapeutic benefits and the active ingredient mentioned
    for each herbal remedy.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 确定每种草药疗法的主要治疗益处和所提到的活性成分。
- en: 'Context:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 背景：
- en: The text provides an overview of various herbal remedies used for centuries.
    It mentions that Chamomile contains Bisabolol, which has anti-inflammatory and
    calming properties. Gingko Biloba, known for its flavonoids and terpenoids, enhances
    cognitive function and blood circulation. Lastly, Echinacea is recognized for
    its alkamides, which bolster the immune system.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这段文本概述了几种使用了几个世纪的草药疗法。它提到洋甘菊含有倍半萜醇，具有抗炎和镇静作用。银杏叶以其类黄酮和萜类化合物著称，能增强认知功能和血液循环。最后，紫锥花因其生物碱类成分而受到认可，能够增强免疫系统。
- en: 'Reasoning:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 推理：
- en: 1\. If a cube’s volume is tripled, by what factor does the length of one of
    its sides increase?
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如果一个立方体的体积增加了三倍，它的边长增加了多少倍？
- en: 2\. In a two-legged soccer match, Team A wins the first leg at home 3-0, but
    loses the second leg away 2-5\. Who advances to the next round, considering the
    away goals rule?
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 在一场两回合的足球比赛中，A队在主场以3-0获胜，但在客场以2-5失利。根据客场进球规则，哪支队伍晋级到下一轮？
- en: 'math:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 数学：
- en: 1\. How do you solve the differential equation $dy/dx+2y=e^{(-2x)}$ given that
    $y(0)=1$?
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何解这个微分方程 $dy/dx+2y=e^{(-2x)}$，已知 $y(0)=1$？
- en: 2\. What is the integral of ($x^{2}+2x+2)/(x^{3}+3x^{2}+3x+1)dx$?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 积分 $\int \frac{x^{2}+2x+2}{x^{3}+3x^{2}+3x+1}dx$ 的值是多少？
- en: 'Coding:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 编程：
- en: 1\. How can I implement a function in C++ that dynamically allocates a 2D array
    based on user input sizes, initializes all elements to zero, and then deallocates
    the memory properly to avoid memory leaks?
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何在C++中实现一个函数，动态分配一个二维数组，基于用户输入的大小，初始化所有元素为零，然后正确地释放内存以避免内存泄漏？
- en: 2\. Write a JavaScript function to fetch data from a given URL, parse the JSON
    response, and filter the results to return an array of items where a specific
    key’s value matches a condition.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 编写一个JavaScript函数，从给定URL获取数据，解析JSON响应，并筛选结果返回一个数组，其中某个特定键的值符合某个条件。
- en: 'STEM knowledge:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: STEM 知识：
- en: 1\. How do you calculate the Schwarzschild radius of a black hole, and what
    implications does this have for the concept of event horizons in general relativity?
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 如何计算黑洞的史瓦西半径，这对广义相对论中的事件视界概念有什么影响？
- en: 2\. Can you explain the process of splicing in eukaryotic gene expression and
    its significance in the diversity of the proteome?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 你能解释一下真核基因表达中的剪接过程及其在蛋白质组多样性中的重要性吗？
- en: 'Humanities/social science knowledge:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 人文学科/社会科学知识：
- en: 1\. Discuss the impact of colonial legacies on contemporary political structures
    in African countries, with examples.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 讨论殖民遗产对非洲国家当代政治结构的影响，并举例说明。
- en: 2\. Analyze the social and economic consequences of the one-child policy in
    China.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 分析中国独生子女政策的社会和经济后果。
- en: Appendix C Contamination Analysis
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 污染分析
- en: 'Table 7: Average Contamination Percentages of Benchmarks.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7：基准的平均污染百分比。
- en: '| Detection Method | Ours | MMLU | ARC Challenge | HellaSwag |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 检测方法 | 我们的方法 | MMLU | ARC 挑战 | HellaSwag |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPT-4 Style (Substring Match) $\downarrow$ | 2% | 42% | 33% | 18% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 风格（子串匹配） $\downarrow$ | 2% | 42% | 33% | 18% |'
- en: '| Playtus Style (Sentence Similarity) $\downarrow$ | 28% | 41% | 35% | 43%
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Playtus 风格（句子相似度） $\downarrow$ | 28% | 41% | 35% | 43% |'
- en: The design in the question-generation and peer-debate process ensures that contamination
    is minimized. Data contamination refers to the possibility of test instances showing
    up in pre-training or Supervised Fine-tuning data.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 问题生成和同行辩论过程中采用的设计确保了污染最小化。数据污染指的是测试实例出现在预训练或监督微调数据中的可能性。
- en: 'Question-generation:'
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题生成：
- en: As we generate the questions automatically, we reduce the risk of test instances
    being eventually exposed to the open web, which can happen in static datasets.
    Alleviation of data contamination is often shown to be an advantage of such dynamic
    and frequently updated evaluation frameworks (Li et al., [2023b](https://arxiv.org/html/2405.20267v4#bib.bib32)).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们自动生成问题，因此可以减少测试实例最终暴露于开放网络的风险，这种情况可能发生在静态数据集中。数据污染的缓解通常被认为是这种动态且频繁更新的评估框架的优势之一（Li等，[2023b](https://arxiv.org/html/2405.20267v4#bib.bib32)）。
- en: 'Peer Debate:'
  id: totrans-283
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同行辩论：
- en: Peer debate ensures that we evaluate the entire debate instead of simple question-answers,
    which further reduces contamination. During debates, the models are evaluated
    on comprehensive and deep abilities, such as planning the strategies, pointing
    out flaws of the opponents, and drafting further questions. Such interactive evaluation
    frameworks are shown to reduce contamination (Yu et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib53);
    Bai et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib5)).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 同行辩论确保我们评估整个辩论，而不是简单的问答，这进一步减少了污染。在辩论过程中，模型会在全面且深入的能力上进行评估，例如规划策略、指出对手的缺陷以及草拟进一步的问题。这样的互动评估框架已被证明能够减少污染（Yu等，
    [2024](https://arxiv.org/html/2405.20267v4#bib.bib53)；Bai等， [2024](https://arxiv.org/html/2405.20267v4#bib.bib5)）。
- en: 'Besides the design choices, we conduct a contamination analysis to compare
    the contamination percentage of Auto-Arena debate questions and test questions
    in popular benchmarks. Specifically, we use two types of contamination detection
    metrics:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 除了设计选择外，我们还进行污染分析，比较Auto-Arena辩论问题和流行基准测试中的测试问题的污染百分比。具体来说，我们使用两种类型的污染检测度量：
- en: '1.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The string match metric as in GPT-4 (OpenAI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib42)),
    where a match is identified if any of three 50-character randomly sampled substrings
    from the evaluation data point (or the entire string if it is shorter than this)
    is a substring of the training set. If so, we mark the point as contaminated.
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字符串匹配度量，如GPT-4中所述（OpenAI等， [2024](https://arxiv.org/html/2405.20267v4#bib.bib42)），其中如果评估数据点的三个50字符随机抽取子串（或整个字符串，如果它短于此）中的任何一个是训练集的子串，则认为匹配。如果是这样，我们将该点标记为污染。
- en: '2.'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The sentence embedding similarity metric as in Platypus (Lee et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib29)),
    where a question is deemed contaminated if it has a cosine similarity (using Sentence
    Transformer (Reimers & Gurevych, [2019](https://arxiv.org/html/2405.20267v4#bib.bib45))
    embeddings) greater than 80% against any training item. This detection method
    is more robust to rephrases, which ensures that we can detect cases where the
    LLMs are simply rephrasing existing questions on the web.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 句子嵌入相似度度量，如Platypus中所述（Lee等， [2024](https://arxiv.org/html/2405.20267v4#bib.bib29)），其中如果一个问题与任何训练项的余弦相似度（使用Sentence
    Transformer（Reimers & Gurevych， [2019](https://arxiv.org/html/2405.20267v4#bib.bib45)）嵌入）大于80%，则认为该问题被污染。该检测方法对于重新表述的情况更具鲁棒性，从而确保我们可以检测到LLMs仅仅是在重新表述互联网上现有问题的情况。
- en: 'Although we do not have access to the training data, LLMs mostly use public
    web data for pre-training (Raffel et al., [2020](https://arxiv.org/html/2405.20267v4#bib.bib43);
    Brown et al., [2020](https://arxiv.org/html/2405.20267v4#bib.bib9); Touvron et al.,
    [2023](https://arxiv.org/html/2405.20267v4#bib.bib50)). Therefore, we approximate
    it with the Bing search API: If verbatim test examples appear online, it likely
    indicates inclusion or exposure to the training data. This procedure is also followed
    by Li et al. ([2024](https://arxiv.org/html/2405.20267v4#bib.bib33)) for detecting
    contamination.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们无法访问训练数据，但LLMs大多使用公共网络数据进行预训练（Raffel等， [2020](https://arxiv.org/html/2405.20267v4#bib.bib43)；Brown等，
    [2020](https://arxiv.org/html/2405.20267v4#bib.bib9)；Touvron等， [2023](https://arxiv.org/html/2405.20267v4#bib.bib50)）。因此，我们通过Bing搜索API进行近似：如果原文测试示例出现在网上，这很可能表明其已包含或暴露于训练数据中。Li等（[2024](https://arxiv.org/html/2405.20267v4#bib.bib33)）也采用此程序来检测污染。
- en: 'The ablation is conducted as follows: Firstly, we randomly sample 100 questions
    from the testset. As baselines, we use 3 popular evaluation benchmarks: MMLU (Hendrycks
    et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25)), ARC Challenge (Clark
    et al., [2018](https://arxiv.org/html/2405.20267v4#bib.bib14)), and HellaSwag (Zellers
    et al., [2019](https://arxiv.org/html/2405.20267v4#bib.bib54)). For each question,
    we get the top 10 search result snippets on the Bing search API. If the question
    is deemed as contaminated by the detection method (mentioned above) against any
    of the 10 snippets, it is marked as contaminated.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 削减实验如下进行：首先，我们随机从测试集中抽取100个问题。作为基准，我们使用3个流行的评估基准：MMLU (Hendrycks et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25))，ARC挑战
    (Clark et al., [2018](https://arxiv.org/html/2405.20267v4#bib.bib14))，和HellaSwag
    (Zellers et al., [2019](https://arxiv.org/html/2405.20267v4#bib.bib54))。对于每个问题，我们通过Bing搜索API获取前10条搜索结果。如果问题被上述检测方法判定为与任何一个搜索结果存在污染，它将被标记为污染。
- en: The percentages of contaminated test instances is reported in Table [7](https://arxiv.org/html/2405.20267v4#A3.T7
    "Table 7 ‣ Appendix C Contamination Analysis ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions"). We can observe that Auto-Arena,
    by generating fresh questions, does alleviate the contamination issue. Compared
    to static datasets, Auto-Arena’s contamination percentage (2%) according to the
    exact match is significantly lower. When using the sentence similarity metric,
    we can effectively detect whether generated questions are just rephrases of existing
    questions. The percentage is largely reduced by 7% to 15% compared to other benchmarks.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 污染的测试实例百分比在表格[7](https://arxiv.org/html/2405.20267v4#A3.T7 "Table 7 ‣ Appendix
    C Contamination Analysis ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer
    Battles and Committee Discussions")中报告。我们可以观察到，通过生成新问题，Auto-Arena确实缓解了污染问题。与静态数据集相比，Auto-Arena的污染百分比（2%）根据精确匹配显著较低。当使用句子相似度度量时，我们可以有效地检测生成的问题是否仅仅是现有问题的改写。与其他基准相比，这一百分比大幅减少了7%至15%。
- en: Appendix D Synthetic V.S. Real-Life Questions
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 合成问题与真实问题
- en: In this section, we try to show the generalizability of the synthetic questions
    in Auto-Arena to real-life questions.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们尝试展示Auto-Arena生成的合成问题与真实问题的可泛化性。
- en: 'Design:'
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设计：
- en: The generated questions resemble real-world queries by design. In the question
    generation prompt, we specifically ask the examiner to draft questions that are
    “likely for a user to ask in real life”. From Appendix [B](https://arxiv.org/html/2405.20267v4#A2
    "Appendix B Example Questions Generated ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions"), we could also observe the
    similarity of the synthetic questions to real-life queries.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的问题在设计上类似于真实世界的查询。在问题生成提示中，我们特别要求考察者设计“用户在现实生活中可能会提问的问题”。从附录[B](https://arxiv.org/html/2405.20267v4#A2
    "Appendix B Example Questions Generated ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions")中，我们也可以观察到合成问题与真实问题的相似性。
- en: 'Table 8: Human Evaluation on Synthetic Questions and Real Questions.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 8：对合成问题和真实问题的人类评估。
- en: '|  | Volunteer 1 | Volunteer 2 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | 志愿者 1 | 志愿者 2 |'
- en: '| --- | --- | --- |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Correct | 27.1% | 38.9% |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 正确 | 27.1% | 38.9% |'
- en: '| Incorrect | 27.1% | 11.9% |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 错误 | 27.1% | 11.9% |'
- en: '| Cannot Tell | 45.8% | 49.2% |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 无法判断 | 45.8% | 49.2% |'
- en: '| Agreement | -0.11 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | -0.11 |'
- en: 'Human Study:'
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人类研究：
- en: 'To show that the generated queries are similar to real-life ones, we conduct
    the following human study. We compare 30 synthetic questions by Auto-Arena and
    30 real-life questions. A human user is asked to look at a question randomly drawn
    and decide whether he/she believes that it is AI-generated, Real-Life, or if he/she
    cannot tell. The questions are collected in the Math category, where the 30 real-life
    ones are taken from MT-Bench (10 questions, drafted by experts), AMC-8 (4 problems,
    from the 2024 math competition), and AGI-Eval (16 math questions collected from
    college entrance exams). Two volunteers who are frequent users of LLMs and are
    familiar with AIGC participated. We report their respective results and agreement
    in Table [8](https://arxiv.org/html/2405.20267v4#A4.T8 "Table 8 ‣ Design: ‣ Appendix
    D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating LLM Evaluations with
    Agent Peer Battles and Committee Discussions"). We can observe that humans cannot
    tell if the problems are synthetic almost half of the time. The user accuracy
    (correct percentages) is also low. We calculate the Cohen’s Kappa agreement between
    the two users, which is -0.11\. The agreement score shows that there is less agreement
    than random chance. The big divergence between human annotators’ responses also
    shows subjectivity and uncertainty in the judgments. Therefore, we conclude that
    humans most likely cannot tell whether questions are synthetic or real-world,
    indicating small differences.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示生成的查询与现实生活中的问题相似，我们进行了以下人类研究。我们比较了由Auto-Arena生成的30个合成问题与30个真实问题。我们要求一名人工用户随机查看一个问题，并判断他/她认为该问题是由AI生成的、是真实生活中的，还是他/她无法判断。问题收集自数学类别，其中30个真实问题来自MT-Bench（10个问题，由专家起草）、AMC-8（4个问题，来自2024年数学竞赛）和AGI-Eval（16个数学问题，收集自高考题）。两名经常使用LLM并熟悉AIGC的志愿者参与了研究。我们在表[8](https://arxiv.org/html/2405.20267v4#A4.T8
    "Table 8 ‣ Design: ‣ Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")中报告了他们各自的结果和一致性。我们可以观察到，几乎一半的时间里，人类无法分辨问题是否为合成问题。用户准确率（正确百分比）也较低。我们计算了两位用户之间的Cohen''s
    Kappa一致性，结果为-0.11。该一致性得分表明，用户之间的意见一致性低于随机概率。人类标注者回应的巨大差异也表明了判断中的主观性和不确定性。因此，我们得出结论，人类最有可能无法判断问题是合成的还是现实世界的，这表明二者之间的差异较小。'
- en: 'Table 9: Ablation Results on Synthetic Questions and Real Questions.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：关于合成问题和真实问题的消融实验结果。
- en: '| Questions | GPT-4 Win Rate | Claude-3 Win Rate |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | GPT-4 胜率 | Claude-3 胜率 |'
- en: '| --- | --- | --- |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Synthetic Questions | 80.00% | 20.00% |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 合成问题 | 80.00% | 20.00% |'
- en: '| Real-life Questions | 75.86% | 24.14% |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 真实问题 | 75.86% | 24.14% |'
- en: 'Ablation Study:'
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融研究：
- en: 'To validate the results’ generalizability with real-world datasets, we conduct
    an ablation study comparing Auto-Arena’s evaluation performances on real-life
    questions and synthetic questions. Specifically, we asked 2 candidates (GPT-4-Turbo-0409
    and Claude-3-Haiku) to debate around 30 synthetic math questions and 30 real-world
    math questions (collected as in the human study shown in Table [8](https://arxiv.org/html/2405.20267v4#A4.T8
    "Table 8 ‣ Design: ‣ Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")). If the results
    are generalizable, we would observe that the win rates of each model should be
    similar. The results are shown in Table [9](https://arxiv.org/html/2405.20267v4#A4.T9
    "Table 9 ‣ Human Study: ‣ Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions"). From the
    results, we can observe that the win rates of each model only differ by 4% on
    synthetic and real datasets, which shows consistent evaluation performances, validating
    the use of synthetic problems.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证结果在真实世界数据集上的普适性，我们进行了一项消融研究，比较Auto-Arena在真实问题和合成问题上的评估表现。具体来说，我们要求两位候选模型（GPT-4-Turbo-0409和Claude-3-Haiku）围绕30个合成数学问题和30个真实数学问题进行辩论（这些问题的收集方式与表[8](https://arxiv.org/html/2405.20267v4#A4.T8
    "Table 8 ‣ Design: ‣ Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")中展示的人类研究相同）。如果结果具有普适性，我们应该观察到每个模型的胜率相似。结果显示在表[9](https://arxiv.org/html/2405.20267v4#A4.T9
    "Table 9 ‣ Human Study: ‣ Appendix D Synthetic V.S. Real-Life Questions ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")中。从结果中，我们可以观察到，在合成和真实数据集上，每个模型的胜率仅相差4%，这表明评估表现一致，验证了合成问题的使用。'
- en: Aside from the supporting studies, the use of synthetic questions for evaluation
    has also been established as common practice. The Mathematics dataset (Hendrycks
    et al., [2021b](https://arxiv.org/html/2405.20267v4#bib.bib26)) already uses synthetically
    generated math questions, where they note many advantages, such as the ease of
    providing a larger number of examples, the precise controls over difficulty levels,
    and the ease of testing generalization (since one can precisely vary different
    axes of difficulty in different question types). LMExamQA (Bai et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib5))
    also uses an LLM to generate questions in different domains. KI-Eval (Yu et al.,
    [2024](https://arxiv.org/html/2405.20267v4#bib.bib53)) asks an LM-powered interactor
    to generate questions. The list goes on. Using synthetic questions has become
    the common norm in NLP evaluation. Moreover, extensive experiments in Auto-Arena
    show high correlations with human results, which also demonstrates the alignment
    with real-world usage.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 除了支持性研究外，使用合成问题进行评估已经被确立为常见做法。数学数据集（Hendrycks et al., [2021b](https://arxiv.org/html/2405.20267v4#bib.bib26)）已经使用了合成生成的数学问题，他们指出了许多优势，例如提供更多示例的便捷性、对难度等级的精确控制，以及测试泛化的便利性（因为可以精确地在不同问题类型中变化不同的难度维度）。LMExamQA（Bai
    et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib5)）也使用LLM生成不同领域的问题。KI-Eval（Yu
    et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib53)）要求一个由LM驱动的交互者生成问题。类似的研究还有很多。使用合成问题已经成为NLP评估中的常规标准。此外，Auto-Arena中的广泛实验显示与人工结果高度相关，这也证明了与实际使用的对齐。
- en: Appendix E Ablation study on Self-Enhancement Bias of the Question Generation
    Stage
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 关于问题生成阶段自我增强偏差的消融研究
- en: 'Table 10: Ablation Results on Self-Enhancement Bias for Question Generator.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：问题生成器自我增强偏差的消融结果。
- en: '| Questions | GPT-4 win rate | Haiku win rate |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | GPT-4 胜率 | Haiku 胜率 |'
- en: '| --- | --- | --- |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 Generated Questions | 80.00% | 20.00% |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 生成的问题 | 80.00% | 20.00% |'
- en: '| Haiku Generated Questions | 76.92% | 23.08% |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| Haiku 生成的问题 | 76.92% | 23.08% |'
- en: 'We attempt to reduce self-enhancement bias of the question generation stage
    with explicit designs: Firstly, during question generation, we do not disclose
    to the examiner that it will participate in this tournament and we do not ask
    the examiner to generate only questions that can be solved by itself. Secondly,
    the peer-debate process further reduces bias in initial question generation: Debating
    ensures that candidates are evaluated not only on their response to the initial
    question, but also in more comprehensive and deeper abilities, such as strategizing,
    criticizing the opponent, and drafting questions. In other words, answering the
    initial question well does not necessarily win a whole debate. In the debate design
    in Figure [2](https://arxiv.org/html/2405.20267v4#S2.F2 "Figure 2 ‣ 2.1 Question
    Generation ‣ 2 The Auto-Arena Framework ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions"), candidates also have a ”raise“
    action, where they ask questions to the opponent. This process essentially decentralizes
    the question-generation process.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试通过明确设计来减少问题生成阶段的自我增强偏差：首先，在问题生成过程中，我们不向考官透露它将参与这个比赛，也不要求考官只生成自己能解决的问题。其次，同伴辩论过程进一步减少了初始问题生成的偏差：辩论确保候选人不仅仅通过对初始问题的回答来评估，而是更全面和深入地考察其能力，如制定策略、批评对手以及起草问题。换句话说，回答初始问题并不一定能赢得整个辩论。在图[2](https://arxiv.org/html/2405.20267v4#S2.F2
    "图 2 ‣ 2.1 问题生成 ‣ 2 Auto-Arena框架 ‣ Auto-Arena：通过代理同伴对抗和委员会讨论自动化LLM评估")中的辩论设计中，候选人还拥有“举手”动作，他们可以向对手提问。这个过程本质上将问题生成过程去中心化。
- en: 'To systematically examine whether self-enhancement bias is present. We conduct
    an ablation study: We examine enhancement bias with 2 models as an example: GPT-4
    (GPT-4-turbo) and Haiku (Claude-3-Haiku). Firstly, we ask GPT-4 and Haiku to generate
    30 math questions separately. Then, we conduct peer debates between the two candidates
    (GPT-4 and Haiku) on both sets of questions and evaluate results with the best-5-LLM
    committee as in the main experiments.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了系统地检验是否存在自我增强偏差，我们进行了消融研究：以GPT-4（GPT-4-turbo）和Haiku（Claude-3-Haiku）这两个模型为例，检验增强偏差。首先，我们分别让GPT-4和Haiku生成30个数学问题。然后，我们在两组问题上进行两位候选人（GPT-4和Haiku）之间的同伴辩论，并使用最佳的5-LLM委员会评估结果，如同在主要实验中一样。
- en: 'We evaluate the performance differences from the evaluation results: If self-enhancement
    bias is low, the ranking achieved should remain the same. In other words, the
    weaker model will always lose, even on the questions generated by itself.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从评估结果中评估性能差异：如果自增强偏差较低，所获得的排名应保持不变。换句话说，较弱的模型将始终失败，即使是在由它自己生成的问题上。
- en: The ablation results are shown in Table [10](https://arxiv.org/html/2405.20267v4#A5.T10
    "Table 10 ‣ Appendix E Ablation study on Self-Enhancement Bias of the Question
    Generation Stage ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles
    and Committee Discussions"). From the results, we can observe that, in both sets
    of generated questions, the GPT-4 win rate remains significantly higher than the
    Claude-3-Haiku win rate. Even if some limited extent of self-enhancement bias
    is present, the result difference is significant enough to reach the correct ranking.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 消融实验结果如表[10](https://arxiv.org/html/2405.20267v4#A5.T10 "Table 10 ‣ Appendix
    E Ablation study on Self-Enhancement Bias of the Question Generation Stage ‣ Auto-Arena:Automating
    LLM Evaluations with Agent Peer Battles and Committee Discussions")所示。从结果可以观察到，在两组生成的问题中，GPT-4的胜率始终显著高于Claude-3-Haiku的胜率。即使存在一定程度的自增强偏差，结果的差异仍然足够显著，能够达到正确的排名。
- en: Appendix F Inter-judge Agreement
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录F 评审员间一致性
- en: '![Refer to caption](img/cba84624c3aeda30308bba05d07b24c6.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/cba84624c3aeda30308bba05d07b24c6.png)'
- en: 'Figure 10: Cohen’s Kappa Agreement with Majority Vote Before Committee Discussions.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：委员会讨论前的Cohen's Kappa一致性与多数投票。
- en: '![Refer to caption](img/a3aeb7879d1c6b57b6882ff2b342e965.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/a3aeb7879d1c6b57b6882ff2b342e965.png)'
- en: 'Figure 11: Cohen’s Kappa Agreement with Majority Vote After 1 Round of Committee
    Discussion.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：委员会讨论后1轮的Cohen's Kappa一致性与多数投票。
- en: As shown in Figure [11](https://arxiv.org/html/2405.20267v4#A6.F11 "Figure 11
    ‣ Appendix F Inter-judge Agreement ‣ Auto-Arena:Automating LLM Evaluations with
    Agent Peer Battles and Committee Discussions"), the Cohen’s Kappa agreement (McHugh,
    [2012](https://arxiv.org/html/2405.20267v4#bib.bib37)) among judges before committee
    discussion is very low, averaging 0.16, which indicates slight agreement. We notice
    that weak model judges and strong model judges has an especially low agreement,
    such as GPT-4 and Yi. This shows that general model capabilities could result
    in significant performance gaps when used as judges.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[11](https://arxiv.org/html/2405.20267v4#A6.F11 "Figure 11 ‣ Appendix F Inter-judge
    Agreement ‣ Auto-Arena:Automating LLM Evaluations with Agent Peer Battles and
    Committee Discussions")所示，在委员会讨论之前，评审员之间的Cohen's Kappa一致性（McHugh, [2012](https://arxiv.org/html/2405.20267v4#bib.bib37)）非常低，平均值为0.16，这表明只有轻微的一致性。我们注意到，弱模型评审员和强模型评审员之间的协议尤其低，例如GPT-4和Yi。这表明，通用模型能力可能导致作为评审员时出现显著的性能差距。
- en: After the 1 round of communication, agreements significantly improved as the
    judges become convinced by more persuasive arguments. The average Cohen’s Kappa
    after discussion reaches 0.27, which indicates fair agreement.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 经过1轮沟通后，由于评审员被更有说服力的论点所信服，一致性显著提高。讨论后的平均Cohen's Kappa值达到了0.27，表示较为公平的一致性。
- en: Appendix G Model selection for the main experiment
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录G 主实验的模型选择
- en: 'Table 11: Model Selection for the Main Experiment. “Newest” and “Strongest”
    refer to the state at the time of experiments (2024 April). Bolded models are
    selected for the primary experiment with 7 models. Unbolded models are the ones
    added during extension.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表11：主实验的模型选择。“最新”和“最强”指的是实验时的状态（2024年4月）。加粗的模型是选择用于主要实验的7个模型。未加粗的模型是在扩展阶段加入的。
- en: '| Model Name | Reasons for Inclusion | License |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 模型名称 | 纳入原因 | 授权 |'
- en: '| --- | --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4-0409-Turbo (OpenAI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib42))
    | Newest and Strongest in GPT model family under GPT-4 | Proprietary |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4-0409-Turbo (OpenAI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib42))
    | GPT-4系列中最新且最强的模型 | 专有 |'
- en: '| GPT-4o-2024-05-13 (Openai, [2024b](https://arxiv.org/html/2405.20267v4#bib.bib41))
    | Newly released model in GPT Model Family | Proprietary |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o-2024-05-13 (Openai, [2024b](https://arxiv.org/html/2405.20267v4#bib.bib41))
    | GPT模型家族中新发布的模型 | 专有 |'
- en: '| GPT-3.5-Turbo-0125 (Openai, [2024a](https://arxiv.org/html/2405.20267v4#bib.bib40))
    | Newest ChatGPT version in the GPT Model Family | Proprietary |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5-Turbo-0125 (Openai, [2024a](https://arxiv.org/html/2405.20267v4#bib.bib40))
    | GPT模型家族中最新的ChatGPT版本 | 专有 |'
- en: '| Claude-3.5-Sonnet-20240620 (Anthropic, [2024](https://arxiv.org/html/2405.20267v4#bib.bib2))
    | Newest in Claude model family under Claude-3.5 | Proprietary |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3.5-Sonnet-20240620 (Anthropic, [2024](https://arxiv.org/html/2405.20267v4#bib.bib2))
    | Claude-3.5系列中最新的模型 | 专有 |'
- en: '| Claude-3-Haiku (Anthropic, [2024](https://arxiv.org/html/2405.20267v4#bib.bib2))
    | Newest and Cheapest in Claude model family under Claude-3 | Proprietary |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| Claude-3-Haiku (Anthropic, [2024](https://arxiv.org/html/2405.20267v4#bib.bib2))
    | Claude-3下Claude模型家族中最新且最便宜的模型 | 专有 |'
- en: '| Qwen/Qwen2-72B-Instruct (Bai et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib3))
    | Representative of Qwen Model Family under Qwen-2 | Proprietary |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| Qwen/Qwen2-72B-Instruct (Bai et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib3))
    | Qwen-2下Qwen模型家族的代表 | 专有 |'
- en: '| Qwen1.5-72B-chat (Bai et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib3))
    | Representative of Qwen model family under Qwen-1.5 | Qianwen LICENSE |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| Qwen1.5-72B-chat (Bai et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib3))
    | Qwen-1.5下Qwen模型家族的代表 | Qianwen LICENSE |'
- en: '| Command R Plus (Cohere, [2024](https://arxiv.org/html/2405.20267v4#bib.bib17))
    | Strongest model in Command R Model Family | CC-BY-NC-4.0 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| Command R Plus (Cohere, [2024](https://arxiv.org/html/2405.20267v4#bib.bib17))
    | Command R 模型家族中最强的模型 | CC-BY-NC-4.0 |'
- en: '| Llama-3-70b-chat-hf (Meta, [2024](https://arxiv.org/html/2405.20267v4#bib.bib38))
    | Representative of Llama Model Family under Llama-3 | Llama 3 Community |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3-70b-chat-hf (Meta, [2024](https://arxiv.org/html/2405.20267v4#bib.bib38))
    | Llama-3下Llama模型家族的代表 | Llama 3 Community |'
- en: '| Llama-2-70b-chat (Touvron et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib50))
    | Representative of Llama Model Family under Llama-2 | Llama 2 Community |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| Llama-2-70b-chat (Touvron et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib50))
    | Llama-2下Llama模型家族的代表 | Llama 2 Community |'
- en: '| Mixtral-8x7b-Instruct-v0.1 (Jiang et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib27))
    | Strongest in open-source Mistral small models | Apache 2.0 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7b-Instruct-v0.1 (Jiang et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib27))
    | 开源Mistral小型模型中最强的 | Apache 2.0 |'
- en: '|  | MOE Structure |  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | MOE 结构 |  |'
- en: '| Gemma-2-27b-it (Team et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib48))
    | Representative of the Gemma family | Apache 2.0 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Gemma-2-27b-it (Team et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib48))
    | Gemma家族的代表 | Apache 2.0 |'
- en: '| Gemini-1.5-flash-exp-0827 (Team et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib48))
    | Cheapest in the Gemini-1.5 family | Proprietary |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| Gemini-1.5-flash-exp-0827 (Team et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib48))
    | Gemini-1.5 家族中最便宜的模型 | 专有 |'
- en: '| Yi-34B-Chat (AI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib1))
    | Strongest in Yi Model Family on Chatbot Arena | Yi License |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B-Chat (AI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib1))
    | Yi 模型家族中在 Chatbot Arena 上最强的模型 | Yi 许可 |'
- en: '| Deepseek-LLM-67B-chat (DeepSeek-AI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib18))
    | Representative open-source model in Deepseek Family | DeepSeek License |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-LLM-67B-chat (DeepSeek-AI et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib18))
    | Deepseek 家族中的代表性开源模型 | DeepSeek 许可 |'
- en: In Table [11](https://arxiv.org/html/2405.20267v4#A7.T11 "Table 11 ‣ Appendix
    G Model selection for the main experiment ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions"), we show all the models selected
    for the main experiment and expansion. We also include the reasons for selection.
    Overall, we try to select a representative set of famous models on Chatbot Arena
    top 20 list. While the Chatbot Arena ranking mostly consists of models with different
    versions, we only select the strongest or newest model from each model family.
    Besides the models on Chatbot Arena, we include 4 under-evaluated famous Chinese
    models to investigate their performances.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格[11](https://arxiv.org/html/2405.20267v4#A7.T11 "Table 11 ‣ Appendix G Model
    selection for the main experiment ‣ Auto-Arena:Automating LLM Evaluations with
    Agent Peer Battles and Committee Discussions")中，我们展示了为主要实验和扩展选择的所有模型，并说明了选择的理由。总体而言，我们尽力从Chatbot
    Arena的前20名模型中选择代表性强的著名模型。尽管Chatbot Arena排名大多由不同版本的模型组成，但我们仅从每个模型家族中选择最强或最新的模型。除了Chatbot
    Arena上的模型，我们还包括了4个未充分评估的著名中文模型，以调查它们的表现。
- en: Appendix H Comparison of baseline methods and Auto-Arena
  id: totrans-352
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录H 基准方法与Auto-Arena的比较
- en: 'Table 12: Comparison between Auto-Arena and Other Benchmarks.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表格12：Auto-Arena与其他基准的比较。
- en: '| Method | Manual Construction | Freshness | Eval. Cost | Judge |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 手动构建 | 新鲜度 | 评估成本 | 评判标准 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | of Queries |  | per Model |  |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | 查询 |  | 每个模型 |'
- en: '| OpenLLM Leaderboard (Beeching et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib6))
    | Yes | Static | - | Answer Accuracy |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| OpenLLM Leaderboard (Beeching et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib6))
    | 是 | 静态 | - | 回答准确度 |'
- en: '| MMLU (Hendrycks et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25))
    | Yes | Static | - | Answer Accuracy |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| MMLU (Hendrycks et al., [2021a](https://arxiv.org/html/2405.20267v4#bib.bib25))
    | 是 | 静态 | - | 回答准确度 |'
- en: '| GPQA (Rein et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib46))
    | Yes | Static | - | Answer Accuracy |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| GPQA (Rein 等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib46)) | 是 |
    静态 | - | 答案准确性 |'
- en: '| LC-AlpacaEval (Dubois et al., [2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))
    | Yes | Static | $10 | Single LLM (GPT-4) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| LC-AlpacaEval (Dubois 等，[2024a](https://arxiv.org/html/2405.20267v4#bib.bib20))
    | 是 | 静态 | $10 | 单一 LLM（GPT-4） |'
- en: '| MT-Bench (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | Yes | Static | $10 | Single LLM (GPT-4) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| MT-Bench (Zheng 等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | 是 | 静态 | $10 | 单一 LLM（GPT-4） |'
- en: '| Arena Hard (Li* et al., [2024](https://arxiv.org/html/2405.20267v4#bib.bib31))
    | Yes | Frequent Updates | $25 | Single LLM (GPT-4) |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| Arena Hard (Li* 等，[2024](https://arxiv.org/html/2405.20267v4#bib.bib31))
    | 是 | 频繁更新 | $25 | 单一 LLM（GPT-4） |'
- en: '| Chatbot Arena (Zheng et al., [2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | Yes | Live | Very High | Humans |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| Chatbot Arena (Zheng 等，[2023](https://arxiv.org/html/2405.20267v4#bib.bib56))
    | 是 | 实时 | 非常高 | 人类 |'
- en: '| Auto-Arena | No | Freshly Generated | $5 | Committee of LLMs |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| Auto-Arena | 否 | 新生成的 | $5 | LLM 委员会 |'
- en: Table [12](https://arxiv.org/html/2405.20267v4#A8.T12 "Table 12 ‣ Appendix H
    Comparison of baseline methods and Auto-Arena ‣ Auto-Arena:Automating LLM Evaluations
    with Agent Peer Battles and Committee Discussions") shows a comparison between
    benchmark evaluation methods and Auto-Arena. Compared to previous methods, the
    main advantage of Auto-Arena is the zero need for human dataset construction or
    intervention and the freshness of queries. Another innovation compared to previous
    model-based systematic benchmarking procedures is using a committee of LLMs to
    discuss and vote for a final winner, which introduces diverse viewpoints. The
    most important innovation of Auto-Arena is the peer-battle mechanism, which asks
    LLM agents to compete and debate with each other. The resulting evaluation on
    the multi-turn debate then becomes more in-depth, interactive, and comprehensive.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [12](https://arxiv.org/html/2405.20267v4#A8.T12 "表格 12 ‣ 附录 H 基准方法与 Auto-Arena
    的比较 ‣ Auto-Arena：通过代理对抗战与委员会讨论自动化大语言模型评估") 显示了基准评估方法与 Auto-Arena 之间的比较。与以往的方法相比，Auto-Arena
    的主要优点是完全无需人工构建数据集或干预，以及查询的新颖性。与之前基于模型的系统性基准测试程序相比，另一个创新是使用一个由大语言模型（LLM）组成的委员会来讨论并投票选出最终的获胜者，这引入了多样的观点。Auto-Arena
    最重要的创新是同行对抗机制，它要求 LLM 代理相互竞争并辩论。由此产生的多轮辩论评估变得更加深入、互动和全面。
- en: 'For the evaluation cost, the costs of Auto-Arena are on the same scale as other
    benchmarks: We note that the primary experiment among 9 models costs around $45
    USD. Therefore, the estimated cost is $5 per model. As models on the ranking board
    increase, the costs of conducting debates should grow slowly in log scale, which
    comes from conducting $nlog_{2}(n)$ pairings when adding 1 model to a ranking
    of $(n-1)$ models. The evaluation costs, however, shall remain the same as we
    use a committee of 5 LLMs at all times.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 关于评估成本，Auto-Arena 的成本与其他基准相当：我们注意到，9个模型中的主要实验大约花费 45 美元。因此，估计的成本为每个模型 5 美元。随着排名中的模型增加，进行辩论的成本应该按对数尺度缓慢增长，这是因为在为一个包含
    $(n-1)$ 个模型的排名中新增一个模型时，进行 $nlog_{2}(n)$ 次配对的结果。然而，由于我们始终使用一个由 5 个 LLM 组成的委员会，评估成本将保持不变。
