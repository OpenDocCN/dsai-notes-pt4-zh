- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P9ï¼š9.Audio ResearchTransformers for Applications
    in Audio, Speech, Music - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P9ï¼š9.Audio ResearchTransformers for Applications
    in Audio, Speech, Music - life_code - BV1X84y1Q7wV
- en: '![](img/e397dafe2290c08be55c621b4940569f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_0.png)'
- en: Thanks for inviting me for the talk today and I'll be just talking about Transers
    for music and audio which is very different than what all of us were doing in
    this past course I'm also the only speaker from Stanford so I have to do a good
    job so you see like a very good slides because I'm kind of representing the university
    in some senseã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢ä½ ä»¬é‚€è¯·æˆ‘ä»Šå¤©æ¥æ¼”è®²ï¼Œæˆ‘å°†è®¨è®ºéŸ³ä¹å’ŒéŸ³é¢‘çš„å˜å‹å™¨ï¼Œè¿™ä¸æˆ‘ä»¬è¿‡å»çš„è¯¾ç¨‹éå¸¸ä¸åŒã€‚æˆ‘ä¹Ÿæ˜¯å”¯ä¸€ä¸€ä½æ¥è‡ªæ–¯å¦ç¦çš„å‘è¨€äººï¼Œæ‰€ä»¥æˆ‘å¿…é¡»è¡¨ç°å‡ºè‰²ï¼Œç¡®ä¿ä½ ä»¬çœ‹åˆ°éå¸¸å¥½çš„å¹»ç¯ç‰‡ï¼Œå› ä¸ºåœ¨æŸç§æ„ä¹‰ä¸Šæˆ‘ä»£è¡¨ç€è¿™ä¸ªå¤§å­¦ã€‚
- en: So yeah so the flow of the talk for today is basically like I'll be throwing
    a lot of stuff it's kind of like a buffet style and then you feel free to like
    or dislike whatever you wantã€‚And I'll be talking mostly about like three papers
    of what I've been working onã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»Šå¤©æ¼”è®²çš„æµç¨‹åŸºæœ¬ä¸Šæ˜¯ï¼Œæˆ‘ä¼šè®²å¾ˆå¤šå†…å®¹ï¼Œåƒè‡ªåŠ©é¤ä¸€æ ·ï¼Œä½ å¯ä»¥éšæ„å–œæ¬¢æˆ–ä¸å–œæ¬¢ã€‚æˆ‘å°†ä¸»è¦è®¨è®ºæˆ‘æ‰€åšçš„ä¸‰ç¯‡è®ºæ–‡ã€‚
- en: I start with like introducing like what transformers are from a different perspectiveã€‚what
    audio representations areã€‚Talk about a generative model for audioã€‚which is just
    doing like language modeling on sample levelã€‚Then I'll talk about how can one
    do like language modeling for speech and audioã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘é¦–å…ˆä¼šä»ä¸åŒçš„è§’åº¦ä»‹ç»ä»€ä¹ˆæ˜¯å˜å‹å™¨ï¼Œä»¥åŠéŸ³é¢‘è¡¨ç¤ºæ˜¯ä»€ä¹ˆã€‚è°ˆè®ºä¸€ä¸ªç”¨äºéŸ³é¢‘çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®é™…ä¸Šæ˜¯åœ¨æ ·æœ¬çº§åˆ«ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚ç„¶åæˆ‘å°†è®¨è®ºå¦‚ä½•å¯¹è¯­éŸ³å’ŒéŸ³é¢‘è¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚
- en: which is different than what people do for textï¼Œ what are the current trends
    in the literatureã€‚![](img/e397dafe2290c08be55c621b4940569f_2.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸äººä»¬å¤„ç†æ–‡æœ¬çš„æ–¹å¼ä¸åŒï¼Œç›®å‰æ–‡çŒ®ä¸­çš„è¶‹åŠ¿æ˜¯ä»€ä¹ˆã€‚![](img/e397dafe2290c08be55c621b4940569f_2.png)
- en: Finallyï¼Œ I'll briefly mention similar stuff as to what was happening in Comp
    vision with regard to vision transformersã€‚or can we adapt similar ideas for audio
    transformers and throw in a bit of signal processing to improve the performanceã€‚Having
    told that the talk is about 35 to 40 minutes with about 15 minutes of Q&Aã€‚I should
    also say that all of the opinions of mine and Stanford or any other professor
    is not responsible for any of the mistake which I doã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæˆ‘å°†ç®€è¦æåŠä¸è§†è§‰å˜å‹å™¨ç›¸å…³çš„è®¡ç®—æœºè§†è§‰é¢†åŸŸå‘ç”Ÿçš„ç±»ä¼¼äº‹æƒ…ï¼Œæˆ–è€…æˆ‘ä»¬èƒ½å¦å°†ç±»ä¼¼çš„æ€è·¯åº”ç”¨äºéŸ³é¢‘å˜å‹å™¨ï¼Œå¹¶åŠ å…¥ä¸€äº›ä¿¡å·å¤„ç†ä»¥æé«˜æ€§èƒ½ã€‚è°ˆåˆ°è¿™é‡Œï¼Œæ¼”è®²å¤§çº¦ä¸º35åˆ°40åˆ†é’Ÿï¼ŒåŒ…å«çº¦15åˆ†é’Ÿçš„é—®ç­”ã€‚æˆ‘è¿˜è¦è¯´ï¼Œæˆ‘çš„æ‰€æœ‰è§‚ç‚¹ä»¥åŠæ–¯å¦ç¦æˆ–å…¶ä»–æ•™æˆçš„è§‚ç‚¹å¯¹æˆ‘æ‰€çŠ¯çš„é”™è¯¯ä¸æ‰¿æ‹…è´£ä»»ã€‚
- en: helã€‚So transformers kind of have kind of revolutionized in a way like everyone
    was approaching deep learning before that was all about CNNNs and mostly all of
    these prominent models have been coming in waves so there was a time when everyone
    was just applying CNNs then came a time where people started adapting CNNs in
    some sort of like dilated cons and slowly the recordcurrent networks were getting
    out of fashion now it seems like transformers and fashion all the time so it seems
    to be solving almost every single problem which is being thrown at themã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å—¨ã€‚æ‰€ä»¥å˜å‹å™¨åœ¨æŸç§ç¨‹åº¦ä¸Šé©å‘½æ€§åœ°æ”¹å˜äº†å¤§å®¶ä¹‹å‰åœ¨æ·±åº¦å­¦ä¹ é¢†åŸŸçš„åšæ³•ï¼Œä¹‹å‰çš„é‡ç‚¹éƒ½æ˜¯CNNsï¼Œæ‰€æœ‰è¿™äº›çªå‡ºçš„æ¨¡å‹éƒ½æ˜¯åœ¨ä¸€æ³¢æ³¢æ¶Œç°ï¼Œæ›¾ç»ä¸€æ®µæ—¶é—´æ¯ä¸ªäººéƒ½åœ¨åº”ç”¨CNNsï¼Œç„¶åå¼€å§‹é€‚åº”æŸç§æ‰©å¼ å·ç§¯ï¼Œæ…¢æ…¢åœ°å¾ªç¯ç¥ç»ç½‘ç»œä¹Ÿåœ¨å¤±å® ï¼Œç°åœ¨å˜å‹å™¨ä¼¼ä¹æ€»æ˜¯æ—¶å°šï¼Œå‡ ä¹è§£å†³äº†æ‰€æœ‰æå‡ºçš„é—®é¢˜ã€‚
- en: Soï¼Œ so what's special aboutï¼ŸOne of the fact that struck me was their simplicityã€‚which
    is if you think about itï¼Œ this and it has been like hugely popular also so it
    was just released in 2018 and within three years it has like about 30ã€‚000 citations
    and it is kind of solving every single problem in every single domainã€‚it has its
    limitations alsoï¼Œ but if you think about it in a way transformers are basically
    like a way of like just cascading self-at with feature learning and if we keep
    on doing it over and over againã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œç‰¹åˆ«ä¹‹å¤„åœ¨å“ªé‡Œï¼Ÿè®©æˆ‘å°è±¡æ·±åˆ»çš„ä¸€ä¸ªäº‹å®æ˜¯å®ƒä»¬çš„ç®€å•æ€§ã€‚å¦‚æœä½ æƒ³æƒ³çœ‹ï¼Œå®ƒéå¸¸å—æ¬¢è¿ï¼Œ2018å¹´åˆšå‘å¸ƒï¼Œä¸‰å¹´å†…å°±æœ‰å¤§çº¦30,000æ¬¡å¼•ç”¨ï¼Œå‡ ä¹è§£å†³äº†æ¯ä¸ªé¢†åŸŸçš„æ¯ä¸ªé—®é¢˜ã€‚å®ƒä¹Ÿæœ‰å±€é™æ€§ï¼Œä½†å¦‚æœä½ ä»æŸç§è§’åº¦æ¥çœ‹ï¼Œå˜å‹å™¨åŸºæœ¬ä¸Šå°±æ˜¯ä»¥ä¸€ç§çº§è”çš„è‡ªæ³¨æ„åŠ›è¿›è¡Œç‰¹å¾å­¦ä¹ ï¼Œå¦‚æœæˆ‘ä»¬ä¸æ–­è¿™æ ·åšã€‚
- en: then the model in a way learns which parts of the input are important and keep
    on transforming them removing the contents which are not important and just have
    the limited information which is just responsible for a particular taskã€‚å‘ƒã€‚And
    it has been very very difficult to keep up with the literatureï¼Œ you knowã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ¨¡å‹ä»¥æŸç§æ–¹å¼å­¦ä¹ å“ªäº›è¾“å…¥éƒ¨åˆ†æ˜¯é‡è¦çš„ï¼Œå¹¶æŒç»­è½¬æ¢å®ƒä»¬ï¼Œç§»é™¤ä¸é‡è¦çš„å†…å®¹ï¼Œåªä¿ç•™è´Ÿè´£ç‰¹å®šä»»åŠ¡çš„æœ‰é™ä¿¡æ¯ã€‚å‘ƒã€‚è·Ÿä¸Šæ–‡çŒ®çœŸçš„éå¸¸å›°éš¾ï¼Œä½ çŸ¥é“çš„ã€‚
- en: like I have put it as a joke here but then even Twitter's recommendation engine
    were kind of just getting out of like they were getting hay viruss to like why
    is Chris Manning just searching about transformers and that was way back in 2020
    so it has been like difficult for researchers also to keep up with a pace of what's
    going onã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™é‡Œå¼€äº†ä¸ªç©ç¬‘ï¼Œç”šè‡³è¿ Twitter çš„æ¨èå¼•æ“ä¹Ÿå¼€å§‹å…³æ³¨ï¼šä¸ºä»€ä¹ˆ Chris Manning ä¼šæœç´¢å˜æ¢å™¨ï¼Œè¿™è¿˜è¦è¿½æº¯åˆ° 2020 å¹´ï¼Œæ‰€ä»¥å¯¹äºç ”ç©¶äººå‘˜æ¥è¯´ï¼Œè·Ÿä¸Šå‘ç”Ÿçš„äº‹æƒ…çš„èŠ‚å¥ä¹Ÿå¾ˆå›°éš¾ã€‚
- en: Just before Transersã€‚All the LP community was just doing gaga about like bidirectional
    eteems with attentionã€‚so every single paper before 2017 was just like you have
    encode or LTM layersã€‚you keep on adding like multiple layers and then after that
    you have a attention mechanism which just loads at what's important and then just
    keeps on decoding sequentially one at a timeã€‚But this was not kind of like an
    ideal way to do itï¼Œ you knowã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Transformers ä¹‹å‰ï¼Œæ•´ä¸ªè¯­è¨€å¤„ç†ç¤¾åŒºéƒ½åœ¨ä¸ºåŒå‘ LSTM å’Œæ³¨æ„åŠ›æœºåˆ¶è€Œç–¯ç‹‚ã€‚æ‰€ä»¥åœ¨ 2017 å¹´ä¹‹å‰çš„æ¯ä¸€ç¯‡è®ºæ–‡éƒ½åªæ˜¯è¿™æ ·ï¼šä½ æœ‰ç¼–ç æˆ–
    LSTM å±‚ï¼Œä½ ä¸æ–­æ·»åŠ å¤šä¸ªå±‚ï¼Œç„¶ååœ¨ä¹‹åæœ‰ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œå®ƒåªæ˜¯å…³æ³¨é‡è¦çš„å†…å®¹ï¼Œç„¶åä¾æ¬¡è§£ç ã€‚ä½†è¿™å¹¶ä¸æ˜¯ä¸€ç§ç†æƒ³çš„æ–¹å¼ï¼Œä½ çŸ¥é“çš„ã€‚
- en: because what turns out is when we start throwing in longer sequencesï¼Œ the connections
    are no longerã€‚Storing the gradient updates in a way it should be doing so what
    what the researchers from Google said you know like instead of having just a attention
    layer at the very last encoding we would just have these attention mechanisms
    at every single layer which in a way would just learn what's important for a particular
    problem at that particular layer and we keep on doing it over and the over againã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºäº‹å®è¯æ˜ï¼Œå½“æˆ‘ä»¬å¼€å§‹å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œè¿æ¥ä¸å†å¦‚é¢„æœŸé‚£æ ·å­˜å‚¨æ¢¯åº¦æ›´æ–°ã€‚å› æ­¤ï¼Œè°·æ­Œçš„ç ”ç©¶äººå‘˜æåˆ°ï¼Œä¸å…¶åœ¨æœ€åä¸€ä¸ªç¼–ç å±‚åªæœ‰ä¸€ä¸ªæ³¨æ„åŠ›å±‚ï¼Œæˆ‘ä»¬ä¸å¦‚åœ¨æ¯ä¸€å±‚éƒ½æœ‰è¿™äº›æ³¨æ„åŠ›æœºåˆ¶ï¼Œè¿™æ ·å°±å¯ä»¥å­¦ä¹ åœ¨ç‰¹å®šå±‚ä¸­æŸä¸ªé—®é¢˜çš„é‡è¦æ€§ï¼Œå¹¶ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ã€‚
- en: So then then the whole idea of like transformers and retention mechanism cascaded
    one after the other came and I'll not go into the details because this is the
    last class of the coursesã€‚but then usual tricks to help across the neural net
    literatureã€‚which is like having multihad attentions having skip connection and
    layer norms so all of these things they are not only like giving gains for transformers
    themselvesã€‚but they can be just applied to any single other architecture alsoã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ç„¶åå˜æ¢å™¨å’Œæ³¨æ„æœºåˆ¶çš„æ•´ä½“æ€è·¯ç›¸ç»§å‡ºç°ï¼Œæˆ‘å°±ä¸è¯¦ç»†è®²äº†ï¼Œå› ä¸ºè¿™æ˜¯è¯¾ç¨‹çš„æœ€åä¸€èŠ‚è¯¾ã€‚ä½†å¸¸ç”¨çš„æŠ€å·§ä¹Ÿæœ‰åŠ©äºæ•´ä¸ªç¥ç»ç½‘ç»œæ–‡çŒ®ï¼Œæ¯”å¦‚å¤šå¤´æ³¨æ„åŠ›ã€è·³è·ƒè¿æ¥å’Œå±‚è§„èŒƒåŒ–ï¼Œè¿™äº›ä¸ä»…å¯¹å˜æ¢å™¨æœ¬èº«æœ‰å¥½å¤„ï¼Œè¿˜å¯ä»¥åº”ç”¨äºä»»ä½•å…¶ä»–æ¶æ„ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_4.png)'
- en: The other thing which is helping these research is basically the compute bar
    is getting better and better so all of these big companies are just throwing massive
    amounts of computing resources at solving very very simple in tasks the top of
    the hill being like the switch transformer which was discussed in the course alsoã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæ¨åŠ¨è¿™é¡¹ç ”ç©¶çš„å› ç´ æ˜¯è®¡ç®—èƒ½åŠ›ä¸æ–­æå‡ï¼Œæ‰€ä»¥æ‰€æœ‰è¿™äº›å¤§å…¬å¸éƒ½åœ¨æŠ•å…¥å¤§é‡è®¡ç®—èµ„æºæ¥è§£å†³éå¸¸ç®€å•çš„ä»»åŠ¡ï¼Œå…¶ä¸­æœ€é¡¶å°–çš„å°±æ˜¯è¯¾ç¨‹ä¸­è®¨è®ºçš„ Switch Transformerã€‚
- en: But but one of the thing which I think started all of the strength was Elmo
    which was just learning these contextualized representations for natural language
    processing and that model right here was perhaps one of the firstã€‚å‘ƒã€‚Kind of like
    like moralã€‚0ã€‚0 or something or 0ã€‚1 in terms of like bringing and ushering in the
    whole revolution you can see that how similar these kind of modelss look like
    Bt was basically like inspired heavily from Elmo in which they just replace some
    of the Lteem layers with transformer modulesã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘è®¤ä¸ºæ‰€æœ‰åŠ›é‡çš„èµ·æºä¹‹ä¸€æ˜¯ Elmoï¼Œå®ƒåªæ˜¯å­¦ä¹ äº†è¿™äº›ç”¨äºè‡ªç„¶è¯­è¨€å¤„ç†çš„ä¸Šä¸‹æ–‡åŒ–è¡¨ç¤ºï¼Œè¿™ä¸ªæ¨¡å‹å¯èƒ½æ˜¯æœ€æ—©çš„ä¹‹ä¸€ã€‚å‘ƒã€‚å°±åƒé“å¾·ä¸€æ ·ã€‚0ã€‚0 æˆ–è€… 0ã€‚1ï¼Œåœ¨å¼•å…¥æ•´ä¸ªé©å‘½æ–¹é¢ï¼Œä½ å¯ä»¥çœ‹åˆ°è¿™äº›æ¨¡å‹çš„ç›¸ä¼¼ä¹‹å¤„ã€‚Bt
    åŸºæœ¬ä¸Šæ˜¯å—åˆ° Elmo çš„å¯å‘ï¼Œä»–ä»¬åªæ˜¯ç”¨å˜æ¢æ¨¡å—æ›¿æ¢äº†ä¸€äº› LSTM å±‚ã€‚
- en: Soï¼Œ so a point to note also is likeã€‚Irespective of like natural language processinging
    or other domainã€‚these can be adapt in a variety of domains and for today's talk
    I'll be just adapting them to audioã€‚![](img/e397dafe2290c08be55c621b4940569f_6.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä¸€ä¸ªéœ€è¦æ³¨æ„çš„ç‚¹æ˜¯ï¼Œæ— è®ºæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†è¿˜æ˜¯å…¶ä»–é¢†åŸŸï¼Œè¿™äº›æ–¹æ³•éƒ½å¯ä»¥é€‚åº”å¤šç§é¢†åŸŸï¼Œè€Œä»Šå¤©çš„è®¨è®ºä¸­æˆ‘å°†å®ƒä»¬åº”ç”¨äºéŸ³é¢‘ã€‚![](img/e397dafe2290c08be55c621b4940569f_6.png)
- en: So I'll basically start with introducing people what audio representations are
    and just for the sake of completeness talk about spectrograms so like you can
    take any time domain signal and you can decompose that signal into a variety of
    basis functions and if you take up a Fourier transform your kind of like decomposing
    actual time domain signal into a soidal basis components so if you have like a
    waveform here like this which is a sum of three pure soidsã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘åŸºæœ¬ä¸Šå°†ä»ä»‹ç»éŸ³é¢‘è¡¨ç¤ºæ³•å¼€å§‹ï¼Œåªä¸ºå®Œæ•´èµ·è§è°ˆè°ˆè°±å›¾ã€‚ä½ å¯ä»¥å°†ä»»ä½•æ—¶åŸŸä¿¡å·åˆ†è§£ä¸ºå¤šç§åŸºå‡½æ•°ï¼Œå¦‚æœä½ è¿›è¡Œå‚…é‡Œå¶å˜æ¢ï¼Œä½ å°±åƒæ˜¯åœ¨å°†å®é™…æ—¶åŸŸä¿¡å·åˆ†è§£ä¸ºæ­£å¼¦åŸºç»„ä»¶ã€‚å› æ­¤ï¼Œå¦‚æœä½ è¿™é‡Œæœ‰ä¸€ä¸ªæ³¢å½¢ï¼Œå®ƒæ˜¯ä¸‰ä¸ªçº¯æ­£å¼¦æ³¢çš„æ€»å’Œã€‚
- en: Then there sum basically is this and you can see that when you take a freeier
    transform and its magnitudeã€‚you kind of have their strength of the individual
    componentsã€‚Sn here so you can take up another wave from let's say a square wave
    and what you have is basically much richer sinusoidal decomposition because it
    is kind of a discontinuous signal so you need like many more sinusoids to represent
    that particular signal as close to the actual signal as possibleã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè¿™é‡Œçš„æ€»å’ŒåŸºæœ¬ä¸Šå°±æ˜¯è¿™ä¸ªï¼Œä½ å¯ä»¥çœ‹åˆ°å½“ä½ è¿›è¡Œå‚…é‡Œå¶å˜æ¢åŠå…¶å¹…åº¦æ—¶ï¼Œä½ åŸºæœ¬ä¸Šæœ‰äº†å„ä¸ªæˆåˆ†çš„å¼ºåº¦ã€‚Snåœ¨è¿™é‡Œï¼Œæ‰€ä»¥ä½ å¯ä»¥ä»ä¸€ä¸ªæ­£æ–¹æ³¢å–å‡ºå¦ä¸€ä¸ªæ³¢ï¼Œä½ æ‰€æ‹¥æœ‰çš„åŸºæœ¬ä¸Šæ˜¯æ›´ä¸°å¯Œçš„æ­£å¼¦åˆ†è§£ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ç§ä¸è¿ç»­ä¿¡å·ï¼Œå› æ­¤ä½ éœ€è¦æ›´å¤šçš„æ­£å¼¦æ³¢æ¥å°½å¯èƒ½æ¥è¿‘å®é™…ä¿¡å·ã€‚
- en: And here also you can see that okay if this was a square wave then it is actually
    made up of a lot of sinusoids where each of the bar here represents the strength
    of the particular sinusoid from an optimization perspective I mean this right
    away is subop right because you are kind of fixing up the number of sinusoid you're
    using for representing a square wave I would have rather used a basis function
    which it was a square wave itself than sinusoid with signal right the second thing
    is like even if you are taking a sinusoid signal we kind of justã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä½ ä¹Ÿå¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœè¿™æ˜¯ä¸€ç§æ­£æ–¹æ³¢ï¼Œé‚£ä¹ˆå®ƒå®é™…ä¸Šç”±è®¸å¤šæ­£å¼¦æ³¢ç»„æˆï¼Œæ¯æ ¹æ¡å½¢ä»£è¡¨ç‰¹å®šæ­£å¼¦æ³¢çš„å¼ºåº¦ã€‚ä»ä¼˜åŒ–çš„è§’åº¦æ¥çœ‹ï¼Œè¿™æ ·åšæ˜¾ç„¶æ˜¯æ¬¡ä¼˜çš„ï¼Œå› ä¸ºä½ åœ¨ä¸ºè¡¨ç¤ºæ­£æ–¹æ³¢å›ºå®šäº†æ­£å¼¦æ³¢çš„æ•°é‡ã€‚æˆ‘æ›´å€¾å‘äºä½¿ç”¨æ­£æ–¹æ³¢æœ¬èº«çš„åŸºå‡½æ•°ï¼Œè€Œä¸æ˜¯æ­£å¼¦æ³¢ä¿¡å·ã€‚ç¬¬äºŒä»¶äº‹æ˜¯ï¼Œå³ä½¿ä½ åœ¨å¤„ç†æ­£å¼¦æ³¢ä¿¡å·ï¼Œæˆ‘ä»¬ä¹Ÿåªæ˜¯ã€‚
- en: Putting them in an equidistant space so you are kind of dividing the whole frequency
    access into like equidistant bins and each of the bins are responsible for like
    a particular sinusai a lotã€‚So that is like a traditional fur representation for
    representing any signalã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å®ƒä»¬æ”¾ç½®åœ¨ç­‰è·ç©ºé—´ä¸­ï¼Œä½ åŸºæœ¬ä¸Šæ˜¯åœ¨å°†æ•´ä¸ªé¢‘ç‡è½´åˆ’åˆ†ä¸ºç­‰è·çš„ binsï¼Œæ¯ä¸ª bin è´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„æ­£å¼¦æ³¢ã€‚æ‰€ä»¥è¿™å°±æ˜¯ä¼ ç»Ÿå‚…é‡Œå¶è¡¨ç¤ºæ³•ï¼Œç”¨äºè¡¨ç¤ºä»»ä½•ä¿¡å·ã€‚
- en: What we do for what a spectrogram so like but in reality all these signals are
    discontinuous all of these signals vary quite a bit right so you can have like
    a signal while i'm speaking which is like a square for a certain period of time
    and then it gets sinusoidal and then it becomes something else so what we really
    need is like in a way toã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨å¤„ç†è°±å›¾æ—¶ï¼Œå®é™…ä¸Šæ‰€æœ‰è¿™äº›ä¿¡å·éƒ½æ˜¯ä¸è¿ç»­çš„ï¼Œè¿™äº›ä¿¡å·çš„å˜åŒ–ç›¸å½“å¤§ã€‚å› æ­¤ï¼Œåœ¨æˆ‘è¯´è¯æ—¶ï¼Œä½ å¯ä»¥æœ‰ä¸€ä¸ªæ­£æ–¹æ³¢ï¼Œåœ¨æŸæ®µæ—¶é—´å†…ç„¶åå®ƒå˜å¾—æ­£å¼¦æ³¢çŠ¶ï¼Œæ¥ç€å˜æˆå…¶ä»–å½¢å¼ã€‚æ‰€ä»¥æˆ‘ä»¬çœŸæ­£éœ€è¦çš„æ˜¯ä¸€ç§æ–¹å¼æ¥ã€‚
- en: kindind of like take patches of input signal and take freeier transform of these
    individual patches I'm deliberately using the wood patches but you can like in
    traditional terms you're windowing the signal so right here you can see that you
    have a continuous signal you keep on windowing it you apply the freeier transform
    and what you get is basically like a spectrogram representation of the signal
    so right here what you're seeing basically is for each of the slices the signal
    kind of look like this after taking the freeier transform with a waveform which
    is there belowã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç±»ä¼¼äºè·å–è¾“å…¥ä¿¡å·çš„ç‰‡æ®µå¹¶å¯¹è¿™äº›å•ç‹¬çš„ç‰‡æ®µè¿›è¡Œå‚…é‡Œå¶å˜æ¢ï¼Œæˆ‘æ•…æ„ä½¿ç”¨â€œæœ¨ç‰‡â€è¿™ä¸ªè¯ï¼Œä½†åœ¨ä¼ ç»Ÿæœ¯è¯­ä¸­ä½ æ˜¯åœ¨å¯¹ä¿¡å·è¿›è¡Œçª—å‡½æ•°å¤„ç†ã€‚æ‰€ä»¥åœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°ï¼Œä½ æœ‰ä¸€ä¸ªè¿ç»­ä¿¡å·ï¼Œä½ ä¸æ–­å¯¹å…¶è¿›è¡Œçª—å‡½æ•°å¤„ç†ï¼Œåº”ç”¨å‚…é‡Œå¶å˜æ¢ï¼Œå¾—åˆ°çš„åŸºæœ¬ä¸Šæ˜¯ä¿¡å·çš„è°±å›¾è¡¨ç¤ºã€‚å› æ­¤ï¼Œåœ¨è¿™é‡Œä½ æ‰€çœ‹åˆ°çš„åŸºæœ¬ä¸Šæ˜¯æ¯ä¸ªåˆ‡ç‰‡çš„ä¿¡å·åœ¨è¿›è¡Œå‚…é‡Œå¶å˜æ¢åçš„æ ·å­ï¼Œä»¥åŠä¸‹é¢çš„æ³¢å½¢ã€‚
- en: And what you're do is for spectrogram representation you keep on stacking these
    Fourier trans slice the magnitude of the Fourier transform slices and in this
    way you kind of get like a 2D representation of audio signals and if you are coming
    from a vision background it is basically all of the things which you are doing
    in vision would just work well if you just apply them to these 2D spec representations
    and'll quickly play how these spectrograms look for a wide area of like common
    soundsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä½ è¦åšçš„æ˜¯ï¼Œå¯¹äºé¢‘è°±å›¾è¡¨ç¤ºï¼Œä½ ä¸æ–­å †å è¿™äº›å‚…é‡Œå¶å˜æ¢åˆ‡ç‰‡çš„å¹…åº¦ï¼Œä»¥è¿™ç§æ–¹å¼ä½ å¯ä»¥è·å¾—éŸ³é¢‘ä¿¡å·çš„äºŒç»´è¡¨ç¤ºï¼Œå¦‚æœä½ æ¥è‡ªè§†è§‰èƒŒæ™¯ï¼Œå®é™…ä¸Šä½ åœ¨è§†è§‰ä¸­æ‰€åšçš„æ‰€æœ‰äº‹æƒ…ï¼Œå¦‚æœä½ å°†å®ƒä»¬åº”ç”¨äºè¿™äº›äºŒç»´é¢‘è°±è¡¨ç¤ºï¼Œæ•ˆæœä¼šå¾ˆå¥½ï¼Œå¹¶ä¸”æˆ‘ä¼šå¿«é€Ÿå±•ç¤ºè¿™äº›é¢‘è°±å›¾åœ¨å¸¸è§å£°éŸ³çš„å¹¿æ³›èŒƒå›´å†…æ˜¯å¦‚ä½•å‘ˆç°çš„ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_8.png)'
- en: å—¯ã€‚Yeahã€‚ğŸ¼ï¼ŒğŸ¼ï¼ŒğŸ¼Yeahã€‚Wã€‚![](img/e397dafe2290c08be55c621b4940569f_10.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æ˜¯çš„ã€‚ğŸ¼ï¼ŒğŸ¼ï¼ŒğŸ¼æ˜¯çš„ã€‚Wã€‚![](img/e397dafe2290c08be55c621b4940569f_10.png)
- en: Okayã€‚So you could see like for spectrograms you have kind of like a time axis
    on your x axis and then you have a frequency axis on y axis and then for whatever
    is your signal of interest you're basically like putting these slices together
    and different sound gives you like different spectral representation so it's kind
    of a vision problem just in this sort of like Fourier spaceã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼Œå¯¹äºé¢‘è°±å›¾ï¼Œä½ çš„ x è½´æœ‰ä¸€ä¸ªæ—¶é—´è½´ï¼Œy è½´æœ‰ä¸€ä¸ªé¢‘ç‡è½´ï¼Œç„¶åå¯¹äºä½ æ„Ÿå…´è¶£çš„ä¿¡å·ï¼Œä½ åŸºæœ¬ä¸Šæ˜¯åœ¨å°†è¿™äº›åˆ‡ç‰‡ç»„åˆåœ¨ä¸€èµ·ï¼Œä¸åŒçš„å£°éŸ³ç»™ä½ ä¸åŒçš„é¢‘è°±è¡¨ç¤ºï¼Œå› æ­¤è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ä¸€ä¸ªè§†è§‰é—®é¢˜ï¼Œåªæ˜¯åœ¨è¿™ç§å‚…é‡Œå¶ç©ºé—´ä¸­ã€‚
- en: So there can be like different kinds of representations also so one you could
    you could just take these slices of coier transform and then do like a linear
    mapping to them so that you are kind of in a way making these as close to how
    humans here so you can have like log of the frequency and the wax instead sort
    of common frequency and then you get like a constant queue like representation
    the advantage of this being like you can see that for different frequencies the
    spacing between the harmonics kind of remains same so if you're like training
    convolutional filters and that's of a huge advantage because the signal like one
    component of the invaris is gone and you can just learn these filters which are
    catching onto these constant templates of Fourier slices you can have metal filter
    bank coefficient or you can have like the raw waveform also for raw waveforms
    basically there are two things which we have to keep in mind one is the sampling
    ratess so we kind of like take the continuous signal and then weã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¯ä»¥æœ‰ä¸åŒç±»å‹çš„è¡¨ç¤ºï¼Œå› æ­¤ä¸€ä¸ªæ–¹æ³•æ˜¯ä½ å¯ä»¥ç›´æ¥å–è¿™äº›å‚…é‡Œå¶å˜æ¢çš„åˆ‡ç‰‡ï¼Œç„¶åè¿›è¡Œçº¿æ€§æ˜ å°„ï¼Œä»æŸç§ç¨‹åº¦ä¸Šä½¿å…¶æ›´æ¥è¿‘äººç±»çš„å¬è§‰ï¼Œå› æ­¤ä½ å¯ä»¥æœ‰é¢‘ç‡çš„å¯¹æ•°è€Œä¸æ˜¯å¸¸è§„é¢‘ç‡ï¼Œç„¶åä½ å¾—åˆ°ä¸€ä¸ªæ’å®šçš„é˜Ÿåˆ—è¡¨ç¤ºï¼Œè¿™æ ·çš„ä¼˜åŠ¿åœ¨äºï¼Œä½ å¯ä»¥çœ‹åˆ°ä¸åŒé¢‘ç‡ä¹‹é—´çš„è°æ³¢é—´è·ä¿æŒä¸€è‡´ã€‚å› æ­¤ï¼Œå¦‚æœä½ åœ¨è®­ç»ƒå·ç§¯æ»¤æ³¢å™¨ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªå·¨å¤§çš„ä¼˜åŠ¿ï¼Œå› ä¸ºä¿¡å·çš„ä¸€ä¸ªæˆåˆ†å·²ç»æ¶ˆå¤±ï¼Œä½ å¯ä»¥å­¦ä¹ è¿™äº›æ•æ‰åˆ°è¿™äº›æ’å®šå‚…é‡Œå¶åˆ‡ç‰‡æ¨¡æ¿çš„æ»¤æ³¢å™¨ï¼Œä½ å¯ä»¥æœ‰é‡‘å±æ»¤æ³¢å™¨ç»„ç³»æ•°ï¼Œæˆ–è€…ä½ ä¹Ÿå¯ä»¥æœ‰åŸå§‹æ³¢å½¢ï¼Œå¯¹äºåŸå§‹æ³¢å½¢ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæœ‰ä¸¤ä»¶äº‹æƒ…éœ€è¦è®°ä½ï¼Œä¸€æ˜¯é‡‡æ ·ç‡ï¼Œæ‰€ä»¥æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯å°†è¿ç»­ä¿¡å·è¿›è¡Œå¤„ç†ï¼Œç„¶åæˆ‘ä»¬ã€‚
- en: Scritize the continuous signal so one way one parameter is like how fast we
    are sampling the continuous signalã€‚So that's typically on the order of like 16000
    or 8000 times a second if you're on telephonic speech the other thing which we
    also is like how how many levels we are dividing your vertical axis so in this
    case you can see that each of the dots is basically one level and typically people
    use 8 bit quants or 16 bit quants so in a way you can think about that for every
    one second audio which we would hear you would have like 16000 samples and then
    in each of the 16000 samples are allowed to take one of the levels between0 to
    55 and that's like if I can like take the problem of like continuous audio and
    just have it in terms of like this sort of like discrete space then basically
    like I'm just going to the territory of like doing language modelingã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ç»†åˆ†è¿ç»­ä¿¡å·ï¼Œæ‰€ä»¥å…¶ä¸­ä¸€ä¸ªå‚æ•°æ˜¯æˆ‘ä»¬é‡‡æ ·è¿ç»­ä¿¡å·çš„é€Ÿåº¦ã€‚å› æ­¤ï¼Œå¦‚æœä½ æ˜¯åœ¨ç”µè¯è¯­éŸ³ä¸­ï¼Œè¿™é€šå¸¸æ˜¯æ¯ç§’çº¦ 16000 æˆ– 8000 æ¬¡ï¼Œå¦ä¸€ä»¶æˆ‘ä»¬è¦è€ƒè™‘çš„äº‹æƒ…æ˜¯æˆ‘ä»¬å¦‚ä½•åˆ’åˆ†å‚ç›´è½´çš„çº§åˆ«ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥çœ‹åˆ°æ¯ä¸ªç‚¹åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªçº§åˆ«ï¼Œé€šå¸¸äººä»¬ä½¿ç”¨
    8 ä½é‡åŒ–æˆ– 16 ä½é‡åŒ–ï¼Œå› æ­¤å¯ä»¥è¿™æ ·æƒ³ï¼Œå¯¹äºæˆ‘ä»¬å¬åˆ°çš„æ¯ä¸€ç§’éŸ³é¢‘ï¼Œä½ å°†æœ‰å¤§çº¦ 16000 ä¸ªæ ·æœ¬ï¼Œç„¶ååœ¨æ¯ä¸ª 16000 ä¸ªæ ·æœ¬ä¸­ï¼Œæœ‰å¯èƒ½å– 0 åˆ° 55
    ä¹‹é—´çš„ä¸€ä¸ªçº§åˆ«ã€‚å¦‚æœæˆ‘èƒ½å°†è¿ç»­éŸ³é¢‘çš„é—®é¢˜è½¬åŒ–ä¸ºè¿™ç§ç¦»æ•£ç©ºé—´ï¼Œé‚£ä¹ˆåŸºæœ¬ä¸Šæˆ‘å°±è¿›å…¥äº†è¯­è¨€å»ºæ¨¡çš„é¢†åŸŸã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_12.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_12.png)'
- en: å‘ƒã€‚![](img/e397dafe2290c08be55c621b4940569f_14.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ƒã€‚![](img/e397dafe2290c08be55c621b4940569f_14.png)
- en: So one the first papers I' discuss is how can we do like generative modeling
    for raw audioã€‚which is similar to wavenets using transformers and we put in QR
    codes if you like the stuff what I'm doing and if you think that this is relevant
    to you please cite or please have a look in terms of like the QR codes so yeah
    so I'll start with the first subtopic of today's talk which is like what are wave
    nets and how do we do like this generative modeling of raw audioã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†è®¨è®ºçš„ç¬¬ä¸€ç¯‡è®ºæ–‡æ˜¯å¦‚ä½•å¯¹åŸå§‹éŸ³é¢‘è¿›è¡Œç”Ÿæˆå»ºæ¨¡ã€‚è¿™ç±»ä¼¼äºä½¿ç”¨å˜æ¢å™¨çš„ WaveNetï¼Œå¦‚æœä½ å–œæ¬¢æˆ‘æ­£åœ¨åšçš„ä¸œè¥¿ï¼Œè®¤ä¸ºè¿™å¯¹ä½ æœ‰ç›¸å…³æ€§ï¼Œè¯·å¼•ç”¨æˆ–è€…æŸ¥çœ‹äºŒç»´ç ã€‚æ‰€ä»¥æˆ‘å°†å¼€å§‹ä»Šå¤©æ¼”è®²çš„ç¬¬ä¸€ä¸ªå­ä¸»é¢˜ï¼Œå³ä»€ä¹ˆæ˜¯
    WaveNetï¼Œä»¥åŠæˆ‘ä»¬å¦‚ä½•è¿›è¡ŒåŸå§‹éŸ³é¢‘çš„ç”Ÿæˆå»ºæ¨¡ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_16.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_16.png)'
- en: So in single wordï¼Œ you can think about this as doing like language modeling
    over these 255 states of audioã€‚so you can throw in your favorite transform model
    like transform Excel or GPT or whatever you want to call it and just treat the
    problem as if you're trying to predict one of the levels of a 235 and you have
    to predict the next level given a certain contextã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œç”¨ä¸€ä¸ªè¯æ¥è¯´ï¼Œä½ å¯ä»¥æŠŠè¿™çœ‹ä½œæ˜¯åœ¨è¿™ 255 ä¸ªéŸ³é¢‘çŠ¶æ€ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚ä½ å¯ä»¥æŠ•å…¥ä½ å–œæ¬¢çš„å˜æ¢æ¨¡å‹ï¼Œæ¯”å¦‚ Transform Excel æˆ– GPTï¼Œæˆ–è€…ä½ æƒ³ç§°ä¹‹ä¸ºçš„ä»»ä½•åå­—ï¼ŒæŠŠè¿™ä¸ªé—®é¢˜è§†ä½œè¯•å›¾é¢„æµ‹
    255 ä¸ªçº§åˆ«ä¸­çš„ä¸€ä¸ªçº§åˆ«ï¼Œå¹¶æ ¹æ®ç‰¹å®šä¸Šä¸‹æ–‡é¢„æµ‹ä¸‹ä¸€ä¸ªçº§åˆ«ã€‚
- en: That's what Wanet was doing so the way you are modeling the probability distribution
    of a continuous space is basically you're trying to predict what's the probability
    of the next sample given some parts one text and Wavenet has been like hugely
    popular because it has over 3000 citations and it has been a code building block
    for almost like all speech and audio related problems like you can think about
    like speech to text text to speech synthesisã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯ WaveNet çš„ä½œç”¨ï¼Œæ¨¡å‹åœ¨è¿ç»­ç©ºé—´ä¸­çš„æ¦‚ç‡åˆ†å¸ƒåŸºæœ¬ä¸Šæ˜¯é¢„æµ‹ç»™å®šéƒ¨åˆ†æ–‡æœ¬çš„ä¸‹ä¸€ä¸ªæ ·æœ¬çš„æ¦‚ç‡ã€‚WaveNet ä¹‹æ‰€ä»¥å¹¿å—æ¬¢è¿ï¼Œæ˜¯å› ä¸ºå®ƒçš„å¼•ç”¨è¶…è¿‡
    3000 æ¬¡ï¼Œå¹¶ä¸”å‡ ä¹æˆä¸ºæ‰€æœ‰è¯­éŸ³å’ŒéŸ³é¢‘ç›¸å…³é—®é¢˜çš„æ„å»ºæ¨¡å—ï¼Œæ¯”å¦‚è¯­éŸ³è½¬æ–‡æœ¬å’Œæ–‡æœ¬è½¬è¯­éŸ³åˆæˆã€‚
- en: instrument conversion packet loss concealment with the internet speech denoizing
    so wherever there's some sort of element of modifying audio people have been using
    wavenet as a code building blockã€‚And raw away from synthesis has been difficult
    because just the magnitude of the problemã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨äº’è”ç½‘è¯­éŸ³å»å™ªä¸­ï¼Œä»ªå™¨è½¬æ¢çš„ä¸¢åŒ…éšè”½æŠ€æœ¯ä½¿å¾—éŸ³é¢‘çš„ä¿®æ”¹æˆä¸ºå¯èƒ½ï¼Œäººä»¬ä¸€ç›´åœ¨ä½¿ç”¨ WaveNet ä½œä¸ºæ„å»ºæ¨¡å—ã€‚è€ŒåŸå§‹éŸ³é¢‘åˆæˆä¸€ç›´å¾ˆå›°éš¾ï¼Œå› ä¸ºé—®é¢˜çš„å¤æ‚æ€§ã€‚
- en: if I'm just trying to synthesize 10 seconds of audioã€‚would just amount to like
    me having a probability distribution over 160ï¼Œ000 samplesã€‚And that itself is stuff
    because our ears are very very sensitive to subtle changes if I'm off by one pixel
    in an image the image like my eyes would not be as as susceptible to noticing
    that effect versus like if I'm off by say a few pixel a few samples in an audio
    it will just catch our ears pretty quickly people have been trying raw audio synthesis
    a lot in the past and before all of the wavenet and transformer based approaches
    kind of like wave rronance and sample rinance were kind of like like state of
    the art models on the right I' seen I've shown a sample rronN model which kind
    of likeã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘åªæ˜¯è¯•å›¾åˆæˆ 10 ç§’çš„éŸ³é¢‘ï¼Œé‚£ä¹ˆè¿™å°±ç›¸å½“äºæˆ‘éœ€è¦å¯¹ 160,000 ä¸ªæ ·æœ¬è¿›è¡Œæ¦‚ç‡åˆ†å¸ƒé¢„æµ‹ã€‚è€Œè¿™æœ¬èº«å°±æ˜¯ä¸ªæŒ‘æˆ˜ï¼Œå› ä¸ºæˆ‘ä»¬çš„è€³æœµå¯¹ç»†å¾®å˜åŒ–éå¸¸æ•æ„Ÿã€‚å¦‚æœæˆ‘åœ¨å›¾åƒä¸­åå·®ä¸€ä¸ªåƒç´ ï¼Œæˆ‘çš„çœ¼ç›å¯èƒ½ä¸ä¼šé‚£ä¹ˆå®¹æ˜“æ³¨æ„åˆ°é‚£ä¸ªæ•ˆæœï¼›ç„¶è€Œï¼Œå¦‚æœæˆ‘åœ¨éŸ³é¢‘ä¸­åå·®å‡ ä¸ªåƒç´ æˆ–æ ·æœ¬ï¼Œæˆ‘ä»¬çš„è€³æœµä¼šå¾ˆå¿«æ•æ‰åˆ°è¿™ä¸ªå˜åŒ–ã€‚è¿‡å»äººä»¬åœ¨åŸå§‹éŸ³é¢‘åˆæˆæ–¹é¢è¿›è¡Œäº†å¤§é‡å°è¯•ï¼Œä¹‹å‰çš„
    WaveNet å’ŒåŸºäºå˜æ¢å™¨çš„æ–¹æ³•ä¹‹å‰çš„çŠ¶æ€å¦‚ Wave Rronance å’Œ Sample Rronance ç±»ä¼¼æ˜¯å½“æ—¶çš„å…ˆè¿›æ¨¡å‹ã€‚æˆ‘å±•ç¤ºäº†ä¸€ä¸ª Sample
    RronN æ¨¡å‹ã€‚
- en: Models the probability distribution of what's going to come next given the past
    at multiple levelsã€‚And this was work done by Yoa Bnji at Nilaï¼Œ but you can closely
    see like if you just see this architecture versus a transformer architecture in
    a wayã€‚these are like starting to get very very similar because what you're trying
    to do is that for the probability distribution here you're trying to see a lot
    of like local substructures and then you keep on doing it over and over again
    and you can draw parallels like okay attention mechanism should also kind of be
    doing the same thing soã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å‹ä¼šæ ¹æ®è¿‡å»çš„å¤šä¸ªå±‚æ¬¡æ¥é¢„æµ‹æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆçš„æ¦‚ç‡åˆ†å¸ƒã€‚è¿™é¡¹å·¥ä½œæ˜¯ç”± Nila çš„ Yoa Bnji å®Œæˆçš„ï¼Œä½†ä½ å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°ï¼Œå¦‚æœå°†è¿™ä¸ªæ¶æ„ä¸å˜æ¢å™¨æ¶æ„è¿›è¡Œå¯¹æ¯”ï¼Œå®ƒä»¬å®é™…ä¸Šå¼€å§‹å˜å¾—éå¸¸ç›¸ä¼¼ï¼Œå› ä¸ºä½ è¯•å›¾åšçš„æ˜¯ï¼Œå¯¹äºè¿™é‡Œçš„æ¦‚ç‡åˆ†å¸ƒï¼Œä½ éœ€è¦è¯†åˆ«å¤§é‡çš„å±€éƒ¨å­ç»“æ„ï¼Œç„¶åä¸æ–­é‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œä½ å¯ä»¥å¾—å‡ºå¹³è¡Œå…³ç³»ï¼Œæ¯”å¦‚è¯´æ³¨æ„æœºåˆ¶ä¹Ÿåº”è¯¥åšç±»ä¼¼çš„äº‹æƒ…ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_18.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_18.png)'
- en: This was the this was the kind of like the literature in the past what we tried
    to do was we just had like the wavenet model and we try to see whether transformers
    can beat them and our intuition was like it should be able to read them because
    they are successful all over the other other domains like in language modeling
    so it should it should do that for raw waveformms also we also try to see whether
    we can circumvent the auto and square constrained byã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯è¿‡å»çš„æ–‡çŒ®ï¼Œæˆ‘ä»¬å°è¯•åšçš„å°±æ˜¯ä½¿ç”¨wavenetæ¨¡å‹ï¼Œçœ‹çœ‹å˜æ¢æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè¶…è¿‡å®ƒä»¬ï¼Œæˆ‘ä»¬çš„ç›´è§‰æ˜¯ï¼Œå®ƒåº”è¯¥èƒ½å¤Ÿè¶…è¶Šï¼Œå› ä¸ºå®ƒä»¬åœ¨å…¶ä»–é¢†åŸŸï¼Œå¦‚è¯­è¨€å»ºæ¨¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œæ‰€ä»¥å®ƒåº”è¯¥ä¹Ÿèƒ½åœ¨åŸå§‹æ³¢å½¢ä¸­åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬è¿˜å°è¯•çœ‹çœ‹æ˜¯å¦å¯ä»¥è§„é¿è‡ªåŠ¨å’Œå¹³æ–¹çº¦æŸã€‚
- en: Conditioning of the context itself and like we did not go for specific applications
    and we just saidã€‚okayï¼Œ just in terms of like modeling behaviorï¼Œ how will they
    doï¼Ÿ
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸Šä¸‹æ–‡çš„æ¡ä»¶å’Œæˆ‘ä»¬æ²¡æœ‰é’ˆå¯¹ç‰¹å®šåº”ç”¨è¿›è¡Œæ¢ç´¢ï¼Œæˆ‘ä»¬åªæ˜¯è¯´ï¼Œå¥½å§ï¼Œå°±åƒåœ¨å»ºæ¨¡è¡Œä¸ºæ–¹é¢ï¼Œä»–ä»¬ä¼šæ€ä¹ˆåšï¼Ÿ
- en: '![](img/e397dafe2290c08be55c621b4940569f_20.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_20.png)'
- en: So so the data set for this was just like real world data recordings so actualã€‚Sound
    should not matter because like the model is agnostic to what it has been thrown
    in and the setup was exactly the same like you're giving a certain context and
    I have to predict like the next sample you do the same thing with wavenets you
    do the exact same thing with transform based like GPT kind of like a model and
    see how well they doã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™ä¸ªæ•°æ®é›†å°±åƒæ˜¯ç°å®ä¸–ç•Œçš„æ•°æ®å½•éŸ³ï¼Œå®é™…ä¸Šï¼Œå£°éŸ³å¹¶ä¸é‡è¦ï¼Œå› ä¸ºæ¨¡å‹å¯¹è¾“å…¥çš„å†…å®¹æ˜¯æ— å…³çš„ï¼Œè®¾ç½®æ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œä½ ç»™å®šä¸€ä¸ªç‰¹å®šçš„ä¸Šä¸‹æ–‡ï¼Œæˆ‘å¿…é¡»é¢„æµ‹ä¸‹ä¸€ä¸ªæ ·æœ¬ï¼Œä½ ç”¨wavenetsåšåŒæ ·çš„äº‹æƒ…ï¼Œä½ ç”¨åŸºäºå˜æ¢çš„æ¨¡å‹ï¼Œæ¯”å¦‚GPTï¼Œåšå®Œå…¨ç›¸åŒçš„äº‹æƒ…ï¼Œçœ‹çœ‹å®ƒä»¬çš„è¡¨ç°å¦‚ä½•ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_22.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_22.png)'
- en: I'll briefly chat about like what wavength models are so wavevenett was kind
    of like a convolutional based model which was getting rid of like all of the vanishing
    gradient problem by just treating sequential problem as being learned by a convolutional
    model so what they did was basically like have this sort of like dilation layers
    or like a convolution with dis which is basically like I kind of skip in every
    subsequent layer by one sample so you can see like if I have dilation factor of
    two with a kernel size of two I would get this kind of a topology where my convolution
    filters in the very first layer I just like combining the first two samples and
    I skip by one in the next layer and then I skip by three which is like I look
    at like the fourth one and the next layer and so on the loss is still the same
    so I have this network I learn a latent space and then I have like a cross categorical
    crossenttropy loss which is basically have to predict the next sampleã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†ç®€è¦è®¨è®ºä¸€ä¸‹wavengthæ¨¡å‹ï¼Œæ‰€ä»¥wavenetæ˜¯ä¸€ç§åŸºäºå·ç§¯çš„æ¨¡å‹ï¼Œå®ƒé€šè¿‡å°†åºåˆ—é—®é¢˜è§†ä¸ºç”±å·ç§¯æ¨¡å‹å­¦ä¹ ï¼Œä»è€Œæ¶ˆé™¤äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚ä»–ä»¬æ‰€åšçš„åŸºæœ¬ä¸Šæ˜¯æœ‰è¿™ç§æ‰©å¼ å±‚æˆ–å·ç§¯ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯åœ¨æ¯ä¸ªåç»­å±‚ä¸­è·³è¿‡ä¸€ä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥ä½ å¯ä»¥çœ‹åˆ°ï¼Œå¦‚æœæˆ‘æœ‰ä¸€ä¸ªæ‰©å¼ å› å­ä¸º2ï¼Œå·ç§¯æ ¸å¤§å°ä¸º2ï¼Œæˆ‘ä¼šå¾—åˆ°è¿™ç§æ‹“æ‰‘ç»“æ„ï¼Œåœ¨ç¬¬ä¸€å±‚ä¸­ï¼Œæˆ‘åªæ˜¯ç»“åˆå‰ä¸¤ä¸ªæ ·æœ¬ï¼Œå¹¶åœ¨ä¸‹ä¸€å±‚ä¸­è·³è¿‡ä¸€ä¸ªï¼Œç„¶ååœ¨æ¥ä¸‹æ¥çš„å±‚ä¸­è·³è¿‡ä¸‰ä¸ªï¼Œä¾æ­¤ç±»æ¨ï¼ŒæŸå¤±ä»ç„¶æ˜¯ç›¸åŒçš„ï¼Œæ‰€ä»¥æˆ‘æœ‰è¿™ä¸ªç½‘ç»œï¼Œå­¦ä¹ ä¸€ä¸ªæ½œåœ¨ç©ºé—´ï¼Œç„¶åæˆ‘æœ‰ä¸€ä¸ªäº¤å‰åˆ†ç±»äº¤å‰ç†µæŸå¤±ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯è¦é¢„æµ‹ä¸‹ä¸€ä¸ªæ ·æœ¬ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_24.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_24.png)'
- en: In the previous oneã€‚And I just do the exact same thing with transformers also
    so but then I have to make sure that I do it in a causal manner so I have something
    which is very similar to GPT in which I have cause masks in my attention mechanism
    and I keep doing it over and over again so you have like selfper after that you
    have feed fer layersers you just have a stack of these transformer blocks and
    see how will they doã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„å®éªŒä¸­ï¼Œæˆ‘å¯¹å˜æ¢æ¨¡å‹åšäº†å®Œå…¨ç›¸åŒçš„äº‹æƒ…ï¼Œä½†æˆ‘å¿…é¡»ç¡®ä¿ä»¥å› æœçš„æ–¹å¼è¿›è¡Œï¼Œæ‰€ä»¥æˆ‘æœ‰ä¸€äº›éå¸¸ç±»ä¼¼äºGPTçš„ä¸œè¥¿ï¼Œåœ¨æˆ‘çš„æ³¨æ„æœºåˆ¶ä¸­æœ‰å› æœæ©ç ï¼Œæˆ‘ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæ‰€ä»¥ä½ æœ‰è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼Œç„¶åæ˜¯å‰é¦ˆå±‚ï¼Œä½ åªæ˜¯å †å è¿™äº›å˜æ¢å—ï¼Œçœ‹çœ‹å®ƒä»¬çš„è¡¨ç°å¦‚ä½•ã€‚
- en: So I said in it should work soã€‚Like it should be doing better than like ourã€‚Base
    wavenet models right because if you look at the topology we are kind of defining
    a topology on our own right so what what if like the current prediction Excel
    layer one world to depend on like very like way back samples here instead of the
    second sample the1 samples so we are kind of ignoring all of that topology which
    would have an important for prediction of this particular task whereas transform
    risk with the self-atten mechanism can just learn like okay which part of the
    samples are important and which are not and you can keep on doing it creatively
    so it made sense to us that okayã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è¯´ï¼Œè¿™åº”è¯¥æœ‰æ•ˆã€‚åƒæˆ‘ä»¬çš„åŸºæœ¬ WaveNet æ¨¡å‹åº”è¯¥è¡¨ç°å¾—æ›´å¥½ï¼Œå› ä¸ºå¦‚æœä½ çœ‹æ‹“æ‰‘ç»“æ„ï¼Œæˆ‘ä»¬åœ¨è‡ªå·±å®šä¹‰ä¸€ä¸ªæ‹“æ‰‘ç»“æ„ã€‚é‚£ä¹ˆï¼Œå¦‚æœå½“å‰çš„é¢„æµ‹ Excel
    å±‚ä¾èµ–äºå¾ˆä¹…ä»¥å‰çš„æ ·æœ¬ï¼Œè€Œä¸æ˜¯ç¬¬äºŒä¸ªæ ·æœ¬ï¼Œç¬¬ä¸€æ ·æœ¬ï¼Œæˆ‘ä»¬åœ¨æŸç§ç¨‹åº¦ä¸Šå¿½è§†äº†æ‰€æœ‰è¿™äº›æ‹“æ‰‘ç»“æ„ï¼Œè¿™å¯¹äºè¿™ä¸ªç‰¹å®šä»»åŠ¡çš„é¢„æµ‹æ˜¯é‡è¦çš„ï¼Œè€Œå˜æ¢å™¨é€šè¿‡è‡ªæ³¨æ„æœºåˆ¶å¯ä»¥å­¦ä¹ åˆ°å“ªäº›æ ·æœ¬æ˜¯é‡è¦çš„ï¼Œå“ªäº›ä¸æ˜¯ï¼Œä½ å¯ä»¥ä¸æ–­åˆ›é€ æ€§åœ°è¿›è¡Œè°ƒæ•´ï¼Œå› æ­¤è¿™å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯æœ‰é“ç†çš„ã€‚
- en: Transform layer should be doing way better than wavelength modelsã€‚å—¯ã€‚The second
    thing which we came across was like okay we cannot have a lot of contextã€‚for example
    the attention mechanism needs to store all of those of order n squares and in
    this case if I'm like storing data at 100 milliseconds then I have like about
    1600 samples and I need to store 1600 by 1600 at multiple layers and it just becomes
    like a huge problem with the data problem with the memory constraint so what we
    said was okay what if we just use the context itself as a latent code in order
    to like have like much better representation at every layer we cannot have like
    huge big attention matrices so what we said was we would just do sample by conditioning
    and through a CNN layers just to understand what the latent code would be so you
    still have like an attention mechanism or just a past context but then I'm alsoã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å±‚çš„è¡¨ç°åº”è¯¥è¿œå¥½äºæ³¢é•¿æ¨¡å‹ã€‚å—¯ã€‚æˆ‘ä»¬é‡åˆ°çš„ç¬¬äºŒä¸ªé—®é¢˜æ˜¯æˆ‘ä»¬ä¸èƒ½æœ‰å¤ªå¤šçš„ä¸Šä¸‹æ–‡ã€‚ä¾‹å¦‚ï¼Œæ³¨æ„æœºåˆ¶éœ€è¦å­˜å‚¨æ‰€æœ‰çš„ n å¹³æ–¹æ¬¡åºæ•°æ®ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¥
    100 æ¯«ç§’çš„é€Ÿåº¦å­˜å‚¨æ•°æ®ï¼Œé‚£ä¹ˆæˆ‘å¤§çº¦æœ‰ 1600 ä¸ªæ ·æœ¬ï¼Œæˆ‘éœ€è¦åœ¨å¤šä¸ªå±‚ä¸Šå­˜å‚¨ 1600 x 1600 çš„æ•°æ®ï¼Œè¿™åœ¨å†…å­˜é™åˆ¶æ–¹é¢ä¼šæˆä¸ºä¸€ä¸ªå·¨å¤§çš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä»¬è¯´ï¼Œå‡è®¾æˆ‘ä»¬å°†ä¸Šä¸‹æ–‡æœ¬èº«ä½œä¸ºæ½œåœ¨ç¼–ç æ¥ä½¿ç”¨ï¼Œä»¥ä¾¿åœ¨æ¯ä¸€å±‚æœ‰æ›´å¥½çš„è¡¨ç¤ºã€‚æˆ‘ä»¬ä¸èƒ½æœ‰å·¨å¤§ä¸”å¤æ‚çš„æ³¨æ„çŸ©é˜µï¼Œå› æ­¤æˆ‘ä»¬å†³å®šé€šè¿‡æ¡ä»¶å¤„ç†å’Œ
    CNN å±‚æ¥ç†è§£æ½œåœ¨ç¼–ç ï¼Œå› æ­¤ä½ ä»ç„¶æœ‰æ³¨æ„æœºåˆ¶æˆ–è€…ä»…ä»…æ˜¯è¿‡å»çš„ä¸Šä¸‹æ–‡ã€‚
- en: At every sample okay what the next sample should be given on this context in
    building and if you think about it in a way it is like okay if there are like
    five or six notess being played in a piano then Im kind of certain which notess
    will be played to a certain extent if I just throw in a CNN layer so I'll use
    that information along with what my transfers are learning and then I would condition
    it and I would just use that to predict the next sampleã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸€ä¸ªæ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬åº”è¯¥è€ƒè™‘åœ¨æ„å»ºä¸Šä¸‹æ–‡æ—¶ä¸‹ä¸€ä¸ªæ ·æœ¬åº”è¯¥æ˜¯ä»€ä¹ˆï¼Œå¦‚æœä½ æƒ³æƒ³ï¼Œè¿™å°±åƒåœ¨é’¢ç´ä¸Šå¼¹å¥äº”å…­ä¸ªéŸ³ç¬¦ï¼Œé‚£ä¹ˆæˆ‘å¯ä»¥ç¡®å®šå“ªäº›éŸ³ç¬¦ä¼šåœ¨ä¸€å®šç¨‹åº¦ä¸Šè¢«å¼¹å¥ï¼Œå¦‚æœæˆ‘åªæ˜¯åŠ ä¸Šä¸€ä¸ª
    CNN å±‚ã€‚å› æ­¤ï¼Œæˆ‘å°†ä½¿ç”¨è¿™äº›ä¿¡æ¯ä»¥åŠæˆ‘çš„ä¼ é€’å­¦ä¹ ï¼Œç„¶åæˆ‘ä¼šå¯¹å…¶è¿›è¡Œæ¡ä»¶å¤„ç†ï¼Œå¹¶ç”¨å®ƒæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ ·æœ¬ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_26.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_26.png)'
- en: So for the evaluation criteria we did not look for negative not likelihood scoresã€‚we
    just looked at how well our prediction prediction task was so we took up like
    stacked wave net which was implemented by deep mind and saw that okay what was
    the performance using their benchmarks and even like bigger stacked wave netsã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œåœ¨è¯„ä¼°æ ‡å‡†æ–¹é¢ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰å…³æ³¨è´Ÿä¼¼ç„¶å¾—åˆ†ã€‚æˆ‘ä»¬åªæ˜¯å…³æ³¨æˆ‘ä»¬çš„é¢„æµ‹ä»»åŠ¡æ•ˆæœå¦‚ä½•ï¼Œæ‰€ä»¥æˆ‘ä»¬é‡‡ç”¨äº†ç”± DeepMind å®ç°çš„å †å  WaveNetï¼Œå¹¶è§‚å¯Ÿäº†å®ƒåœ¨ä»–ä»¬åŸºå‡†æµ‹è¯•ä¸‹çš„è¡¨ç°ï¼Œç”šè‡³æ˜¯æ›´å¤§çš„å †å 
    WaveNetã€‚
- en: we then started to increase the complexity of transformers and started to see
    whatever we had proposed in terms of like conditioning on on the vanilla transformer
    architectures to see how well they do we did not look for like an application
    specific problem which is basically like we don't look at like how well perceptual
    task was part for like say text to speech synthesis or speech renoizing we just
    look at okay if we are trying to model this using a crossenttropy loss then with
    the same model with the same loss function how will they do on like similar kind
    of parametersï¼Ÿ
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¼€å§‹å¢åŠ å˜å‹å™¨çš„å¤æ‚æ€§ï¼Œå¹¶å¼€å§‹è§‚å¯Ÿæˆ‘ä»¬åœ¨æ¡ä»¶ä½œç”¨äºæ™®é€šå˜å‹å™¨æ¶æ„æ–¹é¢æå‡ºçš„å»ºè®®æ•ˆæœå¦‚ä½•ã€‚æˆ‘ä»¬å¹¶æ²¡æœ‰å¯»æ‰¾ç‰¹å®šåº”ç”¨çš„é—®é¢˜ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬æ²¡æœ‰å…³æ³¨åƒæ–‡æœ¬åˆ°è¯­éŸ³åˆæˆæˆ–è¯­éŸ³è¯†åˆ«è¿™æ ·çš„æ„ŸçŸ¥ä»»åŠ¡çš„è¡¨ç°ï¼Œæˆ‘ä»¬åªæ˜¯çœ‹å¦‚æœæˆ‘ä»¬è¯•å›¾ä½¿ç”¨äº¤å‰ç†µæŸå¤±æ¥å»ºæ¨¡ï¼Œé‚£ä¹ˆåœ¨ç›¸åŒæ¨¡å‹å’Œç›¸åŒæŸå¤±å‡½æ•°ä¸‹ï¼Œå®ƒä»¬åœ¨ç±»ä¼¼å‚æ•°ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ
- en: So this was the first kind of like subblock of like how can we use transformers
    for generatedative modelingï¼Ÿ
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¦‚ä½•åˆ©ç”¨å˜å‹å™¨è¿›è¡Œç”Ÿæˆå»ºæ¨¡çš„ç¬¬ä¸€ä¸ªå­å—ã€‚
- en: å“¦ã€‚For the second problemï¼Œ I'll do a quick headaway onã€‚How can we use like transformers
    for doing language modeling which is kind of becoming a really fancy term right
    now and this work was done by Julia Smith way back in 2020 and the goal of this
    was can we kind of in a way do language modeling with continuous audio sequences
    and I'll briefly mention about that this part of in this subb of the talkï¼Ÿ
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œæˆ‘ä¼šå¿«é€Ÿè®¨è®ºä¸€ä¸‹ã€‚æˆ‘ä»¬å¦‚ä½•åˆ©ç”¨å˜å‹å™¨è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼Œè¿™ä¸ªæ¦‚å¿µç°åœ¨å˜å¾—ç›¸å½“æ—¶å°šï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç”±**æœ±è‰äºšÂ·å²å¯†æ–¯**åœ¨2020å¹´å®Œæˆçš„ï¼Œç›®æ ‡æ˜¯æˆ‘ä»¬èƒ½å¦ä»¥æŸç§æ–¹å¼ç”¨è¿ç»­éŸ³é¢‘åºåˆ—è¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼Œæˆ‘å°†ç®€è¦æåŠè¿™ä¸€éƒ¨åˆ†ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_28.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_28.png)'
- en: å“¦ã€‚Soï¼Œ so and this is in regard for like solving acoustic scene understandingã€‚which
    is basically likeã€‚If I'm given a chunk of audioã€‚then I want to understand what's
    in there and if we could do that wellã€‚then in a way we can do a lot of fancy nice
    applications so for example like if you think about like cell driving cars so
    VMmo has started to incorporate microphones into the cell driving cars why because
    say if there is a ambulance coming or if there is aã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚å› æ­¤ï¼Œè¿™ä¸è§£å†³å£°å­¦åœºæ™¯ç†è§£æœ‰å…³ã€‚å¦‚æœæˆ‘å¾—åˆ°ä¸€æ®µéŸ³é¢‘ï¼Œæˆ‘æƒ³ç†è§£å…¶ä¸­çš„å†…å®¹ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå¾ˆå¥½åœ°åšåˆ°è¿™ä¸€ç‚¹ï¼Œé‚£ä¹ˆåœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥åšå¾ˆå¤šå¾ˆé…·çš„åº”ç”¨ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ è€ƒè™‘è‡ªåŠ¨é©¾é©¶æ±½è½¦ï¼Œ**VMmo**å·²ç»å¼€å§‹åœ¨è‡ªåŠ¨é©¾é©¶æ±½è½¦ä¸­åŠ å…¥éº¦å…‹é£ï¼ŒåŸå› æ˜¯å¦‚æœæœ‰æ•‘æŠ¤è½¦æ¥äº†ï¼Œæˆ–è€…æœ‰å…¶ä»–å£°éŸ³ã€‚
- en: File truck coming then that sound would be picked up way way before even than
    the Liarrs or even their sensors so they want to understand that and take actions
    based upon that Apple during COviID did a handwa detection on their Apple watch
    because if you could detect when someone is washing their hands then you can in
    a way like tell people that oh you need to wash hands for 20 seconds and then
    that that can be built upon as a cool applicationã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ–‡ä»¶å¡è½¦æ¥äº†ï¼Œé‚£ä¸ªå£°éŸ³ä¼šåœ¨æå°”æ–¯æˆ–ä»–ä»¬çš„ä¼ æ„Ÿå™¨ä¹‹å‰è¢«æ•æ‰åˆ°ï¼Œå› æ­¤ä»–ä»¬æƒ³äº†è§£è¿™ä¸€ç‚¹å¹¶æ ¹æ®æ­¤é‡‡å–è¡ŒåŠ¨ã€‚è‹¹æœåœ¨COVIDæœŸé—´å¯¹ä»–ä»¬çš„Apple Watchè¿›è¡Œäº†æ´—æ‰‹æ£€æµ‹ï¼Œå› ä¸ºå¦‚æœä½ èƒ½æ£€æµ‹åˆ°æŸäººæ´—æ‰‹ï¼Œé‚£ä¹ˆä½ å¯ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šå‘Šè¯‰äººä»¬â€œå“¦ï¼Œä½ éœ€è¦æ´—æ‰‹20ç§’â€ï¼Œè¿™å¯ä»¥ä½œä¸ºä¸€ä¸ªå¾ˆé…·çš„åº”ç”¨ã€‚
- en: It can be used for music recommendations so Spotify YouTube music kind of gives
    like veryã€‚very good songs which you're listening to which are similar in content
    that you would perhaps like it can also give like really cool applications like
    say people have tried like detecting depression from audio or I could detect whether
    I'm coughing or not or I'm sneezing or not and these can be like goodã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥ç”¨äºéŸ³ä¹æ¨èï¼Œå› æ­¤**Spotify**å’Œ**YouTubeéŸ³ä¹**ä¼šæ¨èéå¸¸é€‚åˆä½ æ­£åœ¨å¬çš„ã€å†…å®¹ç›¸ä¼¼çš„å¥½æ­Œï¼Œè¿™ä¹Ÿå¯ä»¥å¸¦æ¥å¾ˆé…·çš„åº”ç”¨ï¼Œæ¯”å¦‚æœ‰äººå°è¯•ä»éŸ³é¢‘ä¸­æ£€æµ‹æŠ‘éƒï¼Œæˆ–è€…æˆ‘å¯ä»¥æ£€æµ‹æˆ‘æ˜¯å¦åœ¨å’³å—½æˆ–æ‰“å–·åšï¼Œè¿™äº›éƒ½æ˜¯ä¸é”™çš„åº”ç”¨ã€‚
- en: Medical device medical applicationsï¼Œ which can be used along with the current
    diagnosis what doctor providesã€‚So the question was basically for us was like how
    can we do like language modeling in a continuous audio domain and secondlyã€‚like
    how can we train models or how should we approach doing thisï¼Ÿ
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ»ç–—è®¾å¤‡çš„åŒ»ç–—åº”ç”¨ï¼Œå¯ä»¥ä¸åŒ»ç”Ÿæä¾›çš„å½“å‰è¯Šæ–­ç»“åˆä½¿ç”¨ã€‚æ‰€ä»¥æˆ‘ä»¬åŸºæœ¬ä¸Šçš„é—®é¢˜æ˜¯ï¼Œå¦‚ä½•åœ¨è¿ç»­éŸ³é¢‘é¢†åŸŸè¿›è¡Œè¯­è¨€å»ºæ¨¡ï¼Œå…¶æ¬¡ï¼Œå¦‚ä½•è®­ç»ƒæ¨¡å‹æˆ–è€…æˆ‘ä»¬åº”è¯¥å¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Ÿ
- en: So so this kind of like recipe has become like veryã€‚very popular these days
    in terms of like how would you approach this problem it started with like open
    AI and to a certain extent deep proposing that in terms of like VQV models but
    it turns out like transformers love operating in discrete spaces as of now and
    what they kind of do isã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè¿™ç§é…æ–¹å¦‚ä»Šå˜å¾—éå¸¸æµè¡Œï¼Œå…³äºä½ å¦‚ä½•å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œèµ·åˆæ˜¯ç”±**OpenAI**æå‡ºçš„ï¼Œå¹¶ä¸”åœ¨æŸç§ç¨‹åº¦ä¸Š**æ·±åº¦å­¦ä¹ **ä¹Ÿæå‡ºäº†ç±»ä¼¼çš„**VQVæ¨¡å‹**ï¼Œä½†äº‹å®è¯æ˜å˜å‹å™¨ç°åœ¨æ›´å–œæ¬¢åœ¨ç¦»æ•£ç©ºé—´ä¸­æ“ä½œï¼Œå®ƒä»¬æ‰€åšçš„å°±æ˜¯ã€‚
- en: As long as your representations are discreteï¼Œ they are veryã€‚very good at modeling
    what's going to come nextã€‚So what people have been proposing as a workaround isã€‚You
    could take up like yourï¼Œ yourã€‚Favorite embedding in some manner you could take
    a Vq V E embeddings or you could take a wave to work or in terms of video you
    can just doã€‚Classic VGG or resonnet embeddings you can apply k means clustering
    to it and k means clustering would give you like discrete codes you do language
    modeling with those discrete codesã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: åªè¦ä½ çš„è¡¨ç¤ºæ˜¯ç¦»æ•£çš„ï¼Œå®ƒä»¬å°±éå¸¸æ“…é•¿å»ºæ¨¡æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å› æ­¤ï¼Œäººä»¬æå‡ºçš„è§£å†³æ–¹æ¡ˆæ˜¯ï¼Œä½ å¯ä»¥ä»¥æŸç§æ–¹å¼ä½¿ç”¨ä½ å–œæ¬¢çš„åµŒå…¥ï¼Œä½ å¯ä»¥ä½¿ç”¨VQ-VAEåµŒå…¥ï¼Œæˆ–è€…ä½ å¯ä»¥ä½¿ç”¨Wave2Vecï¼Œæˆ–è€…åœ¨è§†é¢‘æ–¹é¢ï¼Œä½ å¯ä»¥ä½¿ç”¨ç»å…¸çš„VGGæˆ–ResNetåµŒå…¥ï¼Œä½ å¯ä»¥å¯¹å®ƒåº”ç”¨Kå‡å€¼èšç±»ï¼ŒKå‡å€¼èšç±»ä¼šç»™ä½ ç¦»æ•£ä»£ç ï¼Œä½ å¯ä»¥ç”¨è¿™äº›ç¦»æ•£ä»£ç è¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚
- en: And you predict the next code and in a wayï¼Œ if youre doing thisã€‚then you're
    kind of doing language modeling over audio and if you need to get back to the
    audio thenã€‚You already saw its wavenet that you can condition the wavenet model
    to give continuous output so you can use those codes to get back to the audio
    similar to what Jubox and Open I didã€‚å‘ƒã€‚So I'll quickly mention about like what
    vector quantization isã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ é¢„æµ‹ä¸‹ä¸€ä¸ªä»£ç ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå¦‚æœä½ è¿™æ ·åšï¼Œé‚£ä¹ˆä½ å°±æ˜¯åœ¨éŸ³é¢‘ä¸Šè¿›è¡Œè¯­è¨€å»ºæ¨¡ã€‚å¦‚æœä½ éœ€è¦è¿”å›åˆ°éŸ³é¢‘ï¼Œé‚£ä¹ˆä½ å·²ç»çœ‹åˆ°äº†å®ƒçš„WaveNetï¼Œä½ å¯ä»¥å¯¹WaveNetæ¨¡å‹è¿›è¡Œæ¡ä»¶å¤„ç†ä»¥è·å¾—è¿ç»­è¾“å‡ºï¼Œå› æ­¤ä½ å¯ä»¥ä½¿ç”¨è¿™äº›ä»£ç æ¥å›åˆ°éŸ³é¢‘ï¼Œè¿™ä¸Jukeboxå’ŒOpenAIçš„åšæ³•ç±»ä¼¼ã€‚å‘ƒã€‚æˆ‘ä¼šå¿«é€Ÿæåˆ°ä¸€ä¸‹å‘é‡é‡åŒ–æ˜¯ä»€ä¹ˆã€‚
- en: It's it's one of the most like underutilized algorithms to be honest and what
    it does is basically gives in a way discrete codes to continue assembling spaces
    so how how does it do it so you basically haveã€‚å‘ƒã€‚Ebedding space let's say in 2D
    right here you define what what are the number of clusters you want to put each
    of them in you run K means and you would certainly get these patches of like where
    all of these embedding so what would be the representative embedding of a continuous
    embedding you can take all of those patches and you can see you can just number
    them or you can just like list them so in this case you can perhaps have like
    25 numbers or 20 numbers which are like in a way mapping from a continuous embedding
    to a discrete tokenã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: è¯´å®è¯ï¼Œè¿™æ˜¯æœ€å°‘è¢«åˆ©ç”¨çš„ç®—æ³•ä¹‹ä¸€ï¼Œå®ƒçš„åŸºæœ¬åŠŸèƒ½æ˜¯ä¸ºè¿ç»­çš„ç»„è£…ç©ºé—´æä¾›ç¦»æ•£ä»£ç ã€‚é‚£ä¹ˆå®ƒæ˜¯å¦‚ä½•åšåˆ°çš„å‘¢ï¼Ÿä½ åŸºæœ¬ä¸Šæœ‰ä¸€ä¸ªåµŒå…¥ç©ºé—´ï¼Œå‡è®¾åœ¨äºŒç»´ï¼Œè¿™é‡Œä½ å®šä¹‰äº†æƒ³è¦æ”¾ç½®æ¯ä¸ªé›†ç¾¤çš„æ•°é‡ï¼Œä½ è¿è¡ŒKå‡å€¼ï¼Œä½ è‚¯å®šä¼šå¾—åˆ°è¿™äº›è¡¥ä¸ï¼Œè¿™äº›è¡¥ä¸ä¸­åµŒå…¥äº†æ‰€æœ‰çš„å†…å®¹ã€‚é‚£ä¹ˆè¿ç»­åµŒå…¥çš„ä»£è¡¨æ€§åµŒå…¥æ˜¯ä»€ä¹ˆï¼Ÿä½ å¯ä»¥æ‹¿æ‰€æœ‰è¿™äº›è¡¥ä¸ï¼Œä½ å¯ä»¥ç»™å®ƒä»¬ç¼–å·ï¼Œæˆ–è€…ä½ å¯ä»¥åˆ—å‡ºå®ƒä»¬ã€‚å› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½ä¼šæœ‰25ä¸ªæ•°å­—æˆ–20ä¸ªæ•°å­—ï¼Œè¿™äº›æ•°å­—åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ä»è¿ç»­åµŒå…¥æ˜ å°„åˆ°ç¦»æ•£æ ‡è®°ã€‚
- en: This is another example right hereã€‚so in our case what we did was we look up
    patches of spectrogramã€‚which are basically like very smallã€‚Patches across time
    and then shared all across the frequency axis you take those patches you learn
    a embedding representation in our case it was just like three layer auto encodecoder
    fully connected encoders with three layers of decoders and have bottlene layer
    in between so that bottleneck layer basically is kind of similar to this kind
    of diagram in like say 64 dimensional space of 120 dimensional space you take
    up those bottle echos and then urine k means clustering on it certainly in a way
    you can findã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯å¦ä¸€ä¸ªä¾‹å­ã€‚æ‰€ä»¥åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬æŸ¥æ‰¾äº†å£°è°±å›¾çš„è¡¥ä¸ï¼Œè¿™åŸºæœ¬ä¸Šå°±åƒæ˜¯éå¸¸å°çš„æ—¶é—´æ®µè¡¥ä¸ï¼Œç„¶ååœ¨é¢‘ç‡è½´ä¸Šå…±äº«ã€‚ä½ æŠŠè¿™äº›è¡¥ä¸æ‹¿å‡ºæ¥ï¼Œå­¦ä¹ åµŒå…¥è¡¨ç¤ºï¼Œåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œå®ƒåªæ˜¯åƒä¸‰å±‚çš„å…¨è¿æ¥è‡ªç¼–ç å™¨ï¼Œå¸¦æœ‰ä¸‰å±‚è§£ç å™¨ï¼Œä¸­é—´æœ‰ä¸€ä¸ªç“¶é¢ˆå±‚ï¼Œè¿™ä¸ªç“¶é¢ˆå±‚åŸºæœ¬ä¸Šç±»ä¼¼äºåƒåœ¨64ç»´ç©ºé—´æˆ–120ç»´ç©ºé—´ä¸­çš„è¿™ç§å›¾ç¤ºã€‚ä½ æå–é‚£äº›ç“¶é¢ˆå›å£°ï¼Œç„¶åè¿›è¡ŒKå‡å€¼èšç±»ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œä½ å¯ä»¥æ‰¾åˆ°ã€‚
- en: Discrete codes for continuous embedding spaces or even continuous signals and
    since we know that transformers kind of love operating discrete spacesã€‚we can
    just apply language botling now and then you can see what you can doã€‚![](img/e397dafe2290c08be55c621b4940569f_30.png)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿ç»­åµŒå…¥ç©ºé—´ç”šè‡³è¿ç»­ä¿¡å·çš„ç¦»æ•£ä»£ç ï¼Œç”±äºæˆ‘ä»¬çŸ¥é“å˜å‹å™¨å–œæ¬¢åœ¨ç¦»æ•£ç©ºé—´ä¸­æ“ä½œï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥åº”ç”¨è¯­è¨€å»ºæ¨¡ï¼Œä½ å¯ä»¥çœ‹çœ‹ä½ èƒ½åšä»€ä¹ˆã€‚![](img/e397dafe2290c08be55c621b4940569f_30.png)
- en: So in our case we just have like very simple three layer fully connected auto
    encoder small patches the number of codes is important because if you have like
    too many codes then you are kind of just throwing in all kinds of noisy things
    i'll give an example of likeã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬å°±æœ‰éå¸¸ç®€å•çš„ä¸‰å±‚å…¨è¿æ¥è‡ªç¼–ç å™¨ï¼Œè¡¥ä¸çš„æ•°é‡å¾ˆé‡è¦ï¼Œå› ä¸ºå¦‚æœä½ æœ‰å¤ªå¤šçš„ä»£ç ï¼Œé‚£ä¹ˆä½ å°±æ˜¯åœ¨æ‰”å„ç§å™ªå£°ã€‚æˆ‘ä¼šä¸¾ä¸€ä¸ªä¾‹å­ã€‚
- en: Why the number of codes are important through some example and you have like
    two little codesã€‚what you're in a way doing is you're removing all of the information
    which was relevant and you're just kind of like averaging them all outã€‚å—¯ã€‚I'll
    start with like so this idea first was proposed by Juke Bï¼Œ which did it for musicã€‚So
    you do the exact same thing what I talked about in a slightly different manner
    in a way that okay you cannot learn codes for like longer sequencesã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä»£ç æ•°é‡é‡è¦ï¼Œé€šè¿‡ä¸€äº›ä¾‹å­æ¥çœ‹ï¼Œå¦‚æœä½ æœ‰ä¸¤ä¸ªå¾ˆå°‘çš„ä»£ç ï¼Œå®é™…ä¸Šä½ åœ¨åšçš„æ˜¯ç§»é™¤æ‰€æœ‰ç›¸å…³çš„ä¿¡æ¯ï¼Œå¹¶ä¸”åªæ˜¯å°†å®ƒä»¬å¹³å‡åŒ–ã€‚å—¯ã€‚æˆ‘å…ˆä»è¿™ä¸ªæƒ³æ³•å¼€å§‹ï¼Œè¿™ä¸ªæƒ³æ³•æœ€åˆæ˜¯ç”±Juke
    Bæå‡ºçš„ï¼Œä»–ä¸ºéŸ³ä¹åšäº†è¿™ä¸ªã€‚æ‰€ä»¥ä½ ä»¥ç¨å¾®ä¸åŒçš„æ–¹å¼åšå®Œå…¨ç›¸åŒçš„äº‹æƒ…ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¥½çš„ï¼Œä½ ä¸èƒ½ä¸ºè¾ƒé•¿çš„åºåˆ—å­¦ä¹ ä»£ç ã€‚
- en: so you know in a way learn sequences which are just moving slowly and which
    are looking at only a certain amount of audio so we kind of encode this in if
    these discrete levels which are basically like all of these basically are codes
    so at every point I define like okay this audio had perhaps like code number 55
    and in the next level perhaps it at code number two and the very top perhaps it
    add code number 2000ã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ çŸ¥é“ï¼Œä»¥æŸç§æ–¹å¼å­¦ä¹ çš„åºåˆ—åªæ˜¯ç¼“æ…¢ç§»åŠ¨ï¼Œå¹¶ä¸”ä»…æŸ¥çœ‹ä¸€å®šé‡çš„éŸ³é¢‘ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨è¿™äº›ç¦»æ•£çº§åˆ«ä¸­ç¼–ç è¿™äº›å†…å®¹ï¼Œè¿™äº›åŸºæœ¬ä¸Šå°±åƒæ‰€æœ‰è¿™äº›åŸºæœ¬ä¸Šéƒ½æ˜¯ä»£ç ã€‚å› æ­¤ï¼Œåœ¨æ¯ä¸ªç‚¹ä¸Šï¼Œæˆ‘å®šä¹‰ï¼Œæ¯”å¦‚è¯´è¿™ä¸ªéŸ³é¢‘å¯èƒ½æœ‰ä»£ç ç¼–å·55ï¼Œè€Œåœ¨ä¸‹ä¸€ä¸ªçº§åˆ«ï¼Œå®ƒå¯èƒ½æœ‰ä»£ç ç¼–å·2ï¼Œåœ¨æœ€ä¸Šé¢å¯èƒ½æœ‰ä»£ç ç¼–å·2000ã€‚
- en: So in a way I'm like discretizing the whole codes now what I do is I take up
    my favorite transform modelã€‚perhaps like a causal or autoregressive one and I
    say that okay given these codes try to predict what codes would come next and
    for sure transformers can do that so I would generate the codes in the future
    once I've generated the codes in the future I can say that okay this problem now
    is kind of like a text to speech problem right because I have like these discrete
    codes text to speech in a way is going from discrete letters to continuous audio
    so I would throw in like the fanciest which was wavenet and I would just get back
    the code and I would get the generated audio so this was in a way what I described
    that they take up a continuous audio they have these compressed codes which they
    encode using a CNN in this case the method doesn't matter you can throw in like
    the fanciest of like embeing or late representationã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæˆ‘åƒæ˜¯åœ¨ç¦»æ•£åŒ–æ•´ä¸ªä»£ç ã€‚ç°åœ¨æˆ‘åšçš„æ˜¯ï¼Œæˆ‘é€‰æ‹©æˆ‘æœ€å–œæ¬¢çš„å˜æ¢æ¨¡å‹ï¼Œå¯èƒ½æ˜¯å› æœæˆ–è‡ªå›å½’çš„ï¼Œç„¶åæˆ‘è¯´ï¼Œå¥½çš„ï¼Œç»™å®šè¿™äº›ä»£ç ï¼Œå°è¯•é¢„æµ‹æ¥ä¸‹æ¥ä¼šå‡ºç°ä»€ä¹ˆä»£ç ï¼Œå˜æ¢å™¨ç¡®å®å¯ä»¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œä¸€æ—¦æˆ‘ç”Ÿæˆäº†æœªæ¥çš„ä»£ç ï¼Œæˆ‘å¯ä»¥è¯´ï¼Œå¥½çš„ï¼Œè¿™ä¸ªé—®é¢˜ç°åœ¨æœ‰ç‚¹åƒæ–‡æœ¬åˆ°è¯­éŸ³çš„é—®é¢˜ï¼Œå› ä¸ºæˆ‘æœ‰è¿™äº›ç¦»æ•£ä»£ç ï¼Œæ–‡æœ¬åˆ°è¯­éŸ³åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ä»ç¦»æ•£å­—æ¯åˆ°è¿ç»­éŸ³é¢‘ï¼Œæ‰€ä»¥æˆ‘ä¼šæŠ•å…¥åƒWaveNetè¿™æ ·çš„é«˜çº§æŠ€æœ¯ï¼Œç„¶åæˆ‘å°±ä¼šå¾—åˆ°è¿”å›çš„ä»£ç ï¼Œå¹¶ç”ŸæˆéŸ³é¢‘ã€‚å› æ­¤ï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æˆ‘æè¿°çš„ï¼Œä»–ä»¬é‡‡ç”¨è¿ç»­éŸ³é¢‘ï¼Œæ‹¥æœ‰è¿™äº›å‹ç¼©ä»£ç ï¼Œå¹¶ä½¿ç”¨CNNè¿›è¡Œç¼–ç ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ–¹æ³•å¹¶ä¸é‡è¦ï¼Œä½ å¯ä»¥æŠ•å…¥åƒEmBeddingæˆ–æ½œåœ¨è¡¨ç¤ºè¿™æ ·çš„é«˜çº§æŠ€æœ¯ã€‚
- en: On those continuous codeï¼Œ you generate the patterns which are like what's going
    to happen next in the future and then decode back using a fancy wavenet state
    of the art auditã€‚![](img/e397dafe2290c08be55c621b4940569f_32.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™äº›è¿ç»­ä»£ç ä¸Šï¼Œä½ ç”Ÿæˆçš„æ¨¡å¼å°±åƒæ˜¯æœªæ¥å°†è¦å‘ç”Ÿçš„äº‹æƒ…ï¼Œç„¶åä½¿ç”¨å…ˆè¿›çš„WaveNetè¿›è¡Œè§£ç ã€‚![](img/e397dafe2290c08be55c621b4940569f_32.png)
- en: So so this was what they were doing for music synthesis what we said was you
    know like yeah this is good this can generate like good amount of musicã€‚but what
    can can can these balls be used for generatingã€‚Like good representation of the
    current audio and the goal there was like can language models learn representation
    which can just encapsulate whatever we are kind of like giving as an input signal
    so in this case what we tried after that was kind of like you kind of do exactly
    kind of similar ideas but instead of doing like on Wikiva end toend learned encodecodings
    we just apply vanillaular k means clustering similar to what I described earlier
    we do on spectrogram patches so you take up these spectrograms of audio and you
    just divide them into like very small chunks learn autoendcoder encodecodings
    for each of those chunks run means clustering and this case like let's say I am
    learning 16 codes represent the continuous audio in terms of the 16 codes have
    transformer which can perhaps predict the next code and if I keep on getting bitã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»–ä»¬åœ¨éŸ³ä¹åˆæˆä¸­åšçš„å°±æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬è¯´è¿™å¾ˆå¥½ï¼Œå¯ä»¥ç”Ÿæˆå¤§é‡éŸ³ä¹ã€‚ä½†è¿™äº›çƒèƒ½ç”¨äºç”Ÿæˆä»€ä¹ˆå‘¢ï¼Ÿå°±åƒæ˜¯èƒ½å¦è®©è¯­è¨€æ¨¡å‹å­¦ä¹ åˆ°èƒ½å°è£…æˆ‘ä»¬è¾“å…¥ä¿¡å·çš„è‰¯å¥½è¡¨ç¤ºã€‚æ‰€ä»¥åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°è¯•çš„æ˜¯ç±»ä¼¼çš„æƒ³æ³•ï¼Œä½†ä¸æ˜¯åšç«¯åˆ°ç«¯å­¦ä¹ çš„ç¼–ç ï¼Œæˆ‘ä»¬åªåº”ç”¨äº†æ™®é€šçš„Kå‡å€¼èšç±»ï¼Œç±»ä¼¼äºæˆ‘ä¹‹å‰æè¿°çš„ï¼Œæˆ‘ä»¬åœ¨è°±å›¾å—ä¸Šè¿›è¡Œæ“ä½œï¼Œå› æ­¤ä½ è·å–è¿™äº›éŸ³é¢‘çš„è°±å›¾ï¼Œç„¶åæŠŠå®ƒä»¬åˆ†æˆéå¸¸å°çš„å—ï¼Œä¸ºæ¯ä¸ªå—å­¦ä¹ è‡ªç¼–ç å™¨çš„ç¼–ç ï¼Œè¿›è¡Œå‡å€¼èšç±»ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ¯”å¦‚æˆ‘æ­£åœ¨å­¦ä¹ 16ä¸ªç¼–ç ï¼Œä»¥16ä¸ªç¼–ç è¡¨ç¤ºè¿ç»­éŸ³é¢‘ï¼Œæœ‰ä¸€ä¸ªå˜å‹å™¨å¯èƒ½é¢„æµ‹ä¸‹ä¸€ä¸ªç¼–ç ï¼Œå¦‚æœæˆ‘ç»§ç»­è·å–ä¸€äº›æ•°æ®ã€‚
- en: And better at predicting what's going to happen nextï¼Œ then in this linear layerã€‚I
    should be encapsulatingã€‚What's important or what'sï¼Ÿ
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªçº¿æ€§å±‚ä¸­ï¼Œæˆ‘åº”è¯¥å°è£…é‡è¦çš„æˆ–ä»€ä¹ˆå†…å®¹ã€‚
- en: A good summary of what has happened in the pastã€‚![](img/e397dafe2290c08be55c621b4940569f_34.png)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯è¿‡å»å‘ç”Ÿçš„äº‹æƒ…çš„ä¸€ä¸ªå¾ˆå¥½çš„æ€»ç»“ã€‚![](img/e397dafe2290c08be55c621b4940569f_34.png)
- en: å‘ƒã€‚So that was kind of like our intuition behind trying thisã€‚![](img/e397dafe2290c08be55c621b4940569f_36.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ƒã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬å°è¯•è¿™ä¸ªçš„ç›´è§‰ã€‚![](img/e397dafe2290c08be55c621b4940569f_36.png)
- en: And as I explained like the number of codes play a very important roleã€‚you can
    see here these are just two piano nodes switching one after the otherã€‚if I just
    have like say 16 number of codes it just have happens to have just a single line
    of encodingã€‚a single code assigned to all of thisï¼Œ whereas if I'm assigning like
    more codes than it becomes kind of like a fine grade prediction where I'm actually
    able to get what the individual notes areã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘æ‰€è§£é‡Šçš„ï¼Œç¼–ç æ•°é‡èµ·ç€éå¸¸é‡è¦çš„ä½œç”¨ã€‚ä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œè¿™åªæ˜¯ä¸¤ä¸ªé’¢ç´éŸ³ç¬¦ç›¸ç»§åˆ‡æ¢ã€‚å¦‚æœæˆ‘åªæœ‰16ä¸ªç¼–ç ï¼Œå®ƒå°±åªæœ‰ä¸€æ¡ç¼–ç ï¼Œä¸€ä¸ªç¼–ç åˆ†é…ç»™æ‰€æœ‰è¿™äº›ï¼Œè€Œå¦‚æœæˆ‘åˆ†é…æ›´å¤šç¼–ç ï¼Œå®ƒå°±å˜æˆäº†ä¸€ç§ç»†è‡´çš„é¢„æµ‹ï¼Œæˆ‘å®é™…ä¸Šèƒ½å¤Ÿè·å–åˆ°æ¯ä¸ªéŸ³ç¬¦ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_38.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_38.png)'
- en: Recentlyï¼Œ Facebook also said you know like okay they just had a different name
    to the whole thing which is would we can just call this as text class NLP also
    in a sense that okay you can do NLP without having access to text with the idea
    is very very similar you have an encoder which is exactly similar to say what
    openaiI was using you have a VQba wave to V or whatever you want to do you can
    apply k means clusterstering to it we apply language models to it and instead
    of a decoder being vnet they just have a decoder which is like a different version
    of text to speech which is like toptro in this case so as you can see like these
    are all like same wine and very different bottle but the core idea is almost exactly
    the sameã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€è¿‘ï¼ŒFacebookä¹Ÿè¡¨ç¤ºï¼Œä»–ä»¬å¯¹æ•´ä½“çš„è¯´æ³•æœ‰äº†ä¸åŒçš„åç§°ï¼Œå¯ä»¥ç§°ä¹‹ä¸ºæ–‡æœ¬ç±»NLPï¼Œå®é™…ä¸Šåœ¨æŸç§æ„ä¹‰ä¸Šå¯ä»¥è¯´ï¼Œä½ å¯ä»¥åœ¨æ²¡æœ‰æ–‡æœ¬çš„æƒ…å†µä¸‹è¿›è¡ŒNLPï¼Œç†å¿µéå¸¸ç›¸ä¼¼ï¼Œä½ æœ‰ä¸€ä¸ªç¼–ç å™¨ï¼Œä¸OpenAIä½¿ç”¨çš„å®Œå…¨ç›¸ä¼¼ï¼Œä½ æœ‰VQæ³¢åˆ°Vï¼Œæˆ–è€…ä½ æƒ³åšçš„ä»»ä½•äº‹æƒ…ï¼Œéƒ½å¯ä»¥å¯¹å…¶åº”ç”¨Kå‡å€¼èšç±»ï¼Œæˆ‘ä»¬å°†è¯­è¨€æ¨¡å‹åº”ç”¨äºæ­¤ï¼Œè§£ç å™¨ä¸æ˜¯VNetï¼Œè€Œæ˜¯ç±»ä¼¼äºæ–‡æœ¬åˆ°è¯­éŸ³çš„ä¸åŒç‰ˆæœ¬ï¼Œåƒæ˜¯Toptroã€‚å› æ­¤ï¼Œå¦‚ä½ æ‰€è§ï¼Œè¿™äº›éƒ½æ˜¯åŒä¸€é…’ä½†ç“¶å­ä¸åŒï¼Œä½†æ ¸å¿ƒç†å¿µå‡ ä¹å®Œå…¨ç›¸åŒã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_40.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_40.png)'
- en: So this was like it created a huge pro of like oh this is going to change an
    LPNã€‚but this is very very similar to what people have been doing the pastã€‚![](img/e397dafe2290c08be55c621b4940569f_42.png)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±åƒåˆ›é€ äº†ä¸€ä¸ªå·¨å¤§çš„ä¼˜åŠ¿ï¼Œå“¦ï¼Œè¿™å°†æ”¹å˜LPNï¼Œä½†è¿™ä¸è¿‡å»äººä»¬æ‰€åšçš„éå¸¸ç›¸ä¼¼ã€‚![](img/e397dafe2290c08be55c621b4940569f_42.png)
- en: So I've already explained what what this was so in our caseï¼Œ we just try to
    predictã€‚What's going to happen nextï¼Œ given the previous context and use that representation
    similar to every single like one short learning or zero short learning based methodã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å·²ç»è§£é‡Šäº†è¿™æ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬åªæ˜¯å°è¯•é¢„æµ‹ä¸‹ä¸€æ­¥ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œç»™å®šå‰æ–‡ä¸Šä¸‹æ–‡ï¼Œå¹¶ä½¿ç”¨è¿™ç§è¡¨ç¤ºæ³•ï¼Œç±»ä¼¼äºæ¯ä¸€ä¸ªçŸ­æœŸå­¦ä¹ æˆ–é›¶çŸ­æœŸå­¦ä¹ çš„åŸºäºæ–¹æ³•ã€‚
- en: I also explain like why the number of codes are important like if you have too
    small then you're just throwing up a lot of information if you have too large
    then you don't put in like it is no longer robust to noiseã€‚![](img/e397dafe2290c08be55c621b4940569f_44.png)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è¿˜è§£é‡Šäº†ä¸ºä»€ä¹ˆä»£ç çš„æ•°é‡å¾ˆé‡è¦ï¼Œå¦‚æœå¤ªå°‘ï¼Œä½ å°±æ‰”æ‰äº†å¾ˆå¤šä¿¡æ¯ï¼›å¦‚æœå¤ªå¤šï¼Œå®ƒå°±ä¸å†å¯¹å™ªå£°ç¨³å¥ã€‚![](img/e397dafe2290c08be55c621b4940569f_44.png)
- en: å“¦ã€‚So this was our setup and before I jump inï¼Œ I should add one of the tweets
    which I saw from one of the most prominent researchers at DeepMã€‚which is basically
    like a lot of times it is very very easy to bump up numbersï¼Œ you knowã€‚like I can
    have these details just not present in my paperã€‚which actually help a lot in terms
    of like improving the performance and kind of sometimes don't take into accountã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çš„è®¾ç½®ï¼Œåœ¨æˆ‘è·³è¿›å»ä¹‹å‰ï¼Œæˆ‘åº”è¯¥è¡¥å……ä¸€ä¸ªæˆ‘ä»DeepMçš„ä¸€ä½è‘—åç ”ç©¶äººå‘˜é‚£é‡Œçœ‹åˆ°çš„æ¨æ–‡ï¼ŒåŸºæœ¬ä¸Šå¾ˆå¤šæ—¶å€™æé«˜æ•°å­—éå¸¸ç®€å•ï¼Œä½ çŸ¥é“ã€‚æ¯”å¦‚æˆ‘å¯ä»¥ä¸åœ¨è®ºæ–‡ä¸­æä¾›è¿™äº›ç»†èŠ‚ï¼Œè¿™å®é™…ä¸Šåœ¨æé«˜æ€§èƒ½æ–¹é¢æœ‰å¾ˆå¤§å¸®åŠ©ï¼Œæœ‰æ—¶ä¸è¢«è€ƒè™‘ã€‚
- en: What the actual model is incorerating or what the model is contributing versus
    what the actual these tricks for training are incorerrating so for most of these
    methods what we have tried to see is we try to keep almost exactly the same approachã€‚no
    data augmentation no fancy label smoothing or moving average of weights or decay
    or whatever you just have similar based recipes to see how well we are doingã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…æ¨¡å‹æ‰€åŒ…å«çš„æˆ–æ¨¡å‹æ‰€è´¡çŒ®çš„ï¼Œä¸è¿™äº›è®­ç»ƒæŠ€å·§å®é™…åŒ…å«çš„ä¸œè¥¿ç›¸æ¯”ï¼Œå¯¹äºå¤§å¤šæ•°è¿™äº›æ–¹æ³•ï¼Œæˆ‘ä»¬å°è¯•ä¿æŒå‡ ä¹å®Œå…¨ç›¸åŒçš„æ–¹æ³•ã€‚æ²¡æœ‰æ•°æ®å¢å¼ºï¼Œæ²¡æœ‰åä¸½çš„æ ‡ç­¾å¹³æ»‘æˆ–æƒé‡çš„ç§»åŠ¨å¹³å‡æˆ–è¡°å‡ï¼Œæ— è®ºå¦‚ä½•ï¼Œä½ åªæ˜¯æœ‰ç±»ä¼¼çš„åŸºç¡€é…æ–¹æ¥çœ‹çœ‹æˆ‘ä»¬çš„è¡¨ç°å¦‚ä½•ã€‚
- en: So for this case the goal was to say that how well our models with respect to
    a purely supervised approach and how well it does with respect to a similar unsupervised
    approach so in the first case the model and all of the weights access to all of
    the labels which is just shown as VGD supervised which is basically you take up
    audio understanding data set and you see how well you are doing an accuracy metrics
    so that was the first one in the second one we applied Simclair which was proposed
    by Jeff In in which you can take up these multiple augmentations of the same input
    you can have like patches removed you can blur the signal you can flip the signal
    you learn embedded out of the last layer without access to the labels and then
    just have a linear head to predict what's happening by using that we got a 55%
    accuracy use the exact same thing we transform us you don't have access to labels
    you just run them while just predict the next code you take the linear layer apply
    the same linear headã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œç›®æ ‡æ˜¯è¯„ä¼°æˆ‘ä»¬çš„æ¨¡å‹ä¸çº¯ç›‘ç£æ–¹æ³•ç›¸æ¯”è¡¨ç°å¦‚ä½•ï¼Œä»¥åŠä¸ç±»ä¼¼çš„æ— ç›‘ç£æ–¹æ³•ç›¸æ¯”è¡¨ç°å¦‚ä½•ã€‚åœ¨ç¬¬ä¸€ä¸ªæ¡ˆä¾‹ä¸­ï¼Œæ¨¡å‹åŠå…¶æ‰€æœ‰æƒé‡è®¿é—®æ‰€æœ‰æ ‡ç­¾ï¼Œè¿™è¢«æ˜¾ç¤ºä¸ºVGDç›‘ç£ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä½ è·å–éŸ³é¢‘ç†è§£æ•°æ®é›†ï¼Œå¹¶æŸ¥çœ‹ä½ çš„å‡†ç¡®æ€§æŒ‡æ ‡è¡¨ç°å¦‚ä½•ã€‚è¿™æ˜¯ç¬¬ä¸€ä¸ªï¼›åœ¨ç¬¬äºŒä¸ªä¸­ï¼Œæˆ‘ä»¬åº”ç”¨äº†Jeff
    Inæå‡ºçš„Simclairï¼Œä½ å¯ä»¥å¯¹ç›¸åŒè¾“å…¥è¿›è¡Œå¤šç§å¢å¼ºå¤„ç†ï¼Œå¯ä»¥å»é™¤è¡¥ä¸ã€æ¨¡ç³Šä¿¡å·ã€ç¿»è½¬ä¿¡å·ï¼Œä½ ä»æœ€åä¸€å±‚å­¦ä¹ åµŒå…¥ï¼Œè€Œä¸è®¿é—®æ ‡ç­¾ï¼Œç„¶åä»…ä½¿ç”¨çº¿æ€§å¤´æ¥é¢„æµ‹å‘ç”Ÿäº†ä»€ä¹ˆï¼Œä½¿ç”¨è¿™ç§æ–¹æ³•æˆ‘ä»¬å¾—åˆ°äº†55%çš„å‡†ç¡®ç‡ã€‚
- en: And try to predict what's happening insideã€‚And with thatï¼Œ we got 60% accuracyã€‚So
    even though the results are not goodï¼Œ but the fact is the neural networks actuallyã€‚Are
    very very good at like getting better and better with throwing of huge amounts
    of data right so there is still 10% back gap between like purely supervised and
    purely unsupervised but then that's going to improve with throwing a lot of data
    to these models because it doesn't have access to any label as person so this
    is a famous paper by the analystly and Nelson also Morgan at Berkeley in which
    they actually showed way back in 1999 as to whyã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå°è¯•é¢„æµ‹å†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆã€‚é€šè¿‡è¿™äº›ï¼Œæˆ‘ä»¬å¾—åˆ°äº†60%çš„å‡†ç¡®ç‡ã€‚å°½ç®¡ç»“æœå¹¶ä¸å¥½ï¼Œä½†äº‹å®ä¸Šç¥ç»ç½‘ç»œå®é™…ä¸Šåœ¨å¤„ç†å¤§é‡æ•°æ®æ—¶éå¸¸å‡ºè‰²ï¼Œå› æ­¤åœ¨çº¯ç›‘ç£å’Œçº¯æ— ç›‘ç£ä¹‹é—´ä»æœ‰10%çš„å·®è·ï¼Œä½†éšç€å¤§é‡æ•°æ®çš„æŠ•å…¥ï¼Œè¿™ç§æƒ…å†µä¼šæ”¹å–„ï¼Œå› ä¸ºå®ƒæ²¡æœ‰ä»»ä½•æ ‡ç­¾çš„è®¿é—®æƒé™ã€‚è¿™æ˜¯åˆ†æå¸ˆNelsonå’ŒMorganåœ¨ä¼¯å…‹åˆ©çš„è‘—åè®ºæ–‡ï¼Œä»–ä»¬æ—©åœ¨1999å¹´å°±å±•ç¤ºäº†åŸå› ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_46.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_46.png)'
- en: Like size mattersï¼Œ four deep neural networks and also the number of data points
    which is present so as they kept on increasing the size of the data set and the
    parametersã€‚they kept on getting lower and lower whatever error rates and this
    has been true across any of the data set and that's why the whole excitement is
    about unsupervised learningã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå¤§å°è¿™ç§å› ç´ å¯¹äºæ·±åº¦ç¥ç»ç½‘ç»œå’Œæ•°æ®ç‚¹æ•°é‡å¾ˆé‡è¦ï¼Œå› æ­¤éšç€æ•°æ®é›†å’Œå‚æ•°å¤§å°çš„ä¸æ–­å¢åŠ ï¼Œå®ƒä»¬çš„é”™è¯¯ç‡è¶Šæ¥è¶Šä½ï¼Œè¿™åœ¨ä»»ä½•æ•°æ®é›†ä¸­éƒ½æ˜¯å¦‚æ­¤ï¼Œè¿™å°±æ˜¯æ— ç›‘ç£å­¦ä¹ ä»¤äººå…´å¥‹çš„åŸå› ã€‚
- en: So this wast a way flavor of like how can we do like language modeling and unsupervised
    learning on audio for continuous signalsï¼Ÿ
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªå…³äºå¦‚ä½•è¿›è¡Œè¯­è¨€å»ºæ¨¡å’Œæ— ç›‘ç£å­¦ä¹ åœ¨éŸ³é¢‘è¿ç»­ä¿¡å·ä¸Šçš„ä¸€ç§æ–¹æ³•é£å‘³ã€‚
- en: For the third up plotï¼Œ I' just quickly mentionã€‚Ideas which are very similar
    to what you would have seen in vision transformersã€‚but with the caveat that how
    can we use some sort of like signal processing to improve these performance a
    furtherï¼Ÿ
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºç¬¬ä¸‰ä¸ªä¸Šå›¾ï¼Œæˆ‘åªæƒ³å¿«é€ŸæåŠã€‚ä¸æ‚¨åœ¨è§†è§‰å˜æ¢å™¨ä¸­çœ‹åˆ°çš„éå¸¸ç›¸ä¼¼çš„æƒ³æ³•ï¼Œä½†æœ‰ä¸€ä¸ªè­¦å‘Šï¼šæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨æŸç§ä¿¡å·å¤„ç†æ¥è¿›ä¸€æ­¥æé«˜è¿™äº›æ€§èƒ½ï¼Ÿ
- en: So the basic approach still remains the same exactly as what you would have
    seen in vision transformersã€‚You have a signal of interest which you want to classify
    here they are raw wave font instead of imagesã€‚the goal is to predict what's there
    inside of it rightã€‚And also we don't have any conclusionsã€‚we don't have any other
    tricks which we were using beforeã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬æ–¹æ³•ä»ç„¶ä¸æ‚¨åœ¨è§†è§‰å˜æ¢å™¨ä¸­çœ‹åˆ°çš„å®Œå…¨ç›¸åŒã€‚æ‚¨æœ‰ä¸€ä¸ªæ„Ÿå…´è¶£çš„ä¿¡å·ï¼Œæ‚¨æƒ³åœ¨è¿™é‡Œå¯¹å…¶è¿›è¡Œåˆ†ç±»ï¼Œå®ƒä»¬æ˜¯åŸå§‹æ³¢å½¢è€Œä¸æ˜¯å›¾åƒã€‚ç›®æ ‡æ˜¯é¢„æµ‹é‡Œé¢æœ‰ä»€ä¹ˆï¼Œå¯¹å§ï¼Ÿæˆ‘ä»¬ä¹Ÿæ²¡æœ‰ä»»ä½•ç»“è®ºï¼Œæ²¡æœ‰ä¹‹å‰ä½¿ç”¨çš„å…¶ä»–æŠ€å·§ã€‚
- en: all we have to do is they can transform us stem solve this particular problemã€‚![](img/e397dafe2290c08be55c621b4940569f_48.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€éœ€è¦åšçš„å°±æ˜¯ï¼Œä»–ä»¬å¯ä»¥å°†æˆ‘ä»¬è½¬å˜ï¼Œè§£å†³è¿™ä¸ªç‰¹å®šçš„é—®é¢˜ã€‚![](img/e397dafe2290c08be55c621b4940569f_48.png)
- en: So for the data set and the whole setup are still the sameã€‚no data augmentation
    and no other forms of these tricksï¼Œ you are given like 40ã€‚000 snippets for training
    and 10ï¼Œ000 for validationã€‚Our job is to predict as good as possible as to what's
    there in the audio this problem is very similar to the sound which you heard and
    the video which you saw that given a patch you ought to predict given a spectrogram
    patch you have to predict what's there inside of itã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ•°æ®é›†å’Œæ•´ä¸ªè®¾ç½®ä»ç„¶æ˜¯ç›¸åŒçš„ï¼Œæ²¡æœ‰æ•°æ®å¢å¼ºï¼Œä¹Ÿæ²¡æœ‰å…¶ä»–è¿™äº›æŠ€å·§ï¼Œæ‚¨ä¼šè·å¾—40,000ä¸ªç‰‡æ®µç”¨äºè®­ç»ƒï¼Œ10,000ä¸ªç”¨äºéªŒè¯ã€‚æˆ‘ä»¬çš„å·¥ä½œæ˜¯å°½å¯èƒ½å‡†ç¡®åœ°é¢„æµ‹éŸ³é¢‘ä¸­å­˜åœ¨çš„å†…å®¹ï¼Œè¿™ä¸ªé—®é¢˜ä¸æ‚¨å¬åˆ°çš„å£°éŸ³å’Œæ‚¨çœ‹åˆ°çš„è§†é¢‘éå¸¸ç›¸ä¼¼ï¼Œç»™å®šä¸€ä¸ªè¡¥ä¸ï¼Œæ‚¨å¿…é¡»é¢„æµ‹ç»™å®šçš„é¢‘è°±å›¾è¡¥ä¸å†…éƒ¨çš„å†…å®¹ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_50.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_50.png)'
- en: å‘ƒã€‚![](img/e397dafe2290c08be55c621b4940569f_52.png)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å‘ƒã€‚![](img/e397dafe2290c08be55c621b4940569f_52.png)
- en: We kind of do one step further than whatã€‚What's just like a simple transformer
    model in a sense that we try to see whether some sort of like hierarchy over transform
    emtics would help us in any mannerã€‚so for that we use like wavelelet decomposition
    on the intermediate transformer ems so what is like a waveleng decompositionã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æŸç§æ„ä¹‰ä¸Šæ¯”ç®€å•çš„å˜æ¢æ¨¡å‹æ›´è¿›ä¸€æ­¥ï¼Œå°è¯•æŸ¥çœ‹æŸç§å±‚æ¬¡ç»“æ„æ˜¯å¦èƒ½ä»¥ä»»ä½•æ–¹å¼å¸®åŠ©æˆ‘ä»¬ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨ä¸­é—´å˜æ¢åµŒå…¥ä¸Šä½¿ç”¨äº†å°æ³¢åˆ†è§£ï¼Œé‚£ä¹ˆä»€ä¹ˆæ˜¯å°æ³¢åˆ†è§£å‘¢ï¼Ÿ
- en: '![](img/e397dafe2290c08be55c621b4940569f_54.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_54.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_55.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_55.png)'
- en: In very naive termsï¼Œ it can be like a way of decomposing the intermediate embeddings
    intoã€‚intermediate embedding in a sense that we are kind of putting these highways
    of like some embeddings are moving very slowly and some embeddings are moving
    very fast and some embeddings are retained exactly at the rate of what the original
    signal was and why this is important because you can think about that at every
    intermediate state you are in a whale learning some sort of hierarchy in the model
    so if if I look at what we do with the like waveleair decomposition before and
    after let's say like you had time across this and you had the embedding size across
    this and this whole patch was your output of say the end layer of the transformer
    what I say now is okay I would justã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç”¨éå¸¸ç®€å•çš„æœ¯è¯­æ¥è¯´ï¼Œè¿™å°±åƒæ˜¯å°†ä¸­é—´åµŒå…¥åˆ†è§£æˆçš„æ–¹å¼ã€‚åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œæˆ‘ä»¬å°†è¿™äº›åµŒå…¥çš„é«˜é€Ÿå…¬è·¯åˆ†æˆä¸€äº›ç§»åŠ¨å¾—å¾ˆæ…¢çš„åµŒå…¥å’Œä¸€äº›ç§»åŠ¨å¾—å¾ˆå¿«çš„åµŒå…¥ï¼Œè¿˜æœ‰ä¸€äº›åµŒå…¥çš„ä¿ç•™é€Ÿåº¦æ°å¥½ä¸åŸå§‹ä¿¡å·ç›¸åŒã€‚è¿™ä¸€ç‚¹å¾ˆé‡è¦ï¼Œå› ä¸ºä½ å¯ä»¥æƒ³åˆ°ï¼Œåœ¨æ¯ä¸€ä¸ªä¸­é—´çŠ¶æ€ä¸­ï¼Œä½ éƒ½åœ¨å­¦ä¹ æ¨¡å‹ä¸­çš„æŸç§å±‚æ¬¡ç»“æ„ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘æŸ¥çœ‹æˆ‘ä»¬åœ¨å°æ³¢åˆ†è§£å‰åçš„å·¥ä½œï¼Œæ¯”å¦‚ä½ åœ¨æ—¶é—´è½´ä¸Šæœ‰è¿™ä¸ªï¼Œè€Œåœ¨åµŒå…¥å¤§å°ä¸Šæœ‰è¿™ä¸ªï¼Œæ•´ä¸ªè¡¥ä¸æ˜¯ä½ æ‰€è¯´çš„å˜å‹å™¨æœ€åä¸€å±‚çš„è¾“å‡ºã€‚é‚£ä¹ˆæˆ‘ç°åœ¨è¯´çš„æ˜¯ï¼Œå¥½å§ï¼Œæˆ‘åªæ˜¯ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_57.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_57.png)'
- en: Have a mapping from this to the mapping of my interest using waveleth decomposition
    in which for half of the samples I just created the exact same embedding as what
    was learned by the transform model in the next half I would start combining two
    at a time so in a way i'm learning this sort of like a tree structure within a
    single layer of the transformer embedding and for now the wavelength or the B
    function which I use a simple averaging so let's say from all of the embedding
    layers in between I just need to have like one one embedding which is not moving
    at all which is just representative of whatever is thereã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è¿™é‡Œæ˜ å°„åˆ°æˆ‘æ„Ÿå…´è¶£çš„æ˜ å°„ï¼Œä½¿ç”¨æ³¢é•¿åˆ†è§£ï¼Œå…¶ä¸­å¯¹äºä¸€åŠçš„æ ·æœ¬ï¼Œæˆ‘åˆšåˆšåˆ›å»ºäº†ä¸å˜æ¢æ¨¡å‹å­¦ä¹ çš„å®Œå…¨ç›¸åŒçš„åµŒå…¥ï¼Œåœ¨ååŠéƒ¨åˆ†æˆ‘ä¼šå¼€å§‹æ¯æ¬¡ç»„åˆä¸¤ä¸ªï¼Œæ‰€ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæˆ‘æ­£åœ¨å­¦ä¹ è¿™ç§ç±»ä¼¼äºæ ‘ç»“æ„çš„ä¸œè¥¿ï¼Œåœ¨å˜æ¢å™¨åµŒå…¥çš„å•å±‚ä¸­ï¼Œå¯¹äºç°åœ¨ä½¿ç”¨çš„æ³¢é•¿æˆ–Bå‡½æ•°ï¼Œæˆ‘ç®€å•åœ°è¿›è¡Œäº†å¹³å‡ï¼Œæ‰€ä»¥å‡è®¾åœ¨æ‰€æœ‰åµŒå…¥å±‚ä¹‹é—´ï¼Œæˆ‘åªéœ€è¦æœ‰ä¸€ä¸ªå®Œå…¨ä¸åŠ¨çš„åµŒå…¥ï¼Œä»£è¡¨å­˜åœ¨çš„ä»»ä½•ä¸œè¥¿ã€‚
- en: Of the whole latent space in that n layerï¼Œ then in the next next layer I would
    just use like two at a time and then I would use like four at a timeã€‚Until I reached
    the exact resolution as what I hadã€‚Doing this operation doesn't add any para as
    whatsoever you're just like defining what your basis function would be or what
    a wavelength function would be in this case it is a hard waveleth and I start
    combining them and I kind of like learned a hierarchy at every single layer of
    the transformers and this kind of likeã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•´ä¸ªæ½œåœ¨ç©ºé—´çš„nå±‚ä¸­ï¼Œåœ¨æ¥ä¸‹æ¥çš„å±‚ä¸­ï¼Œæˆ‘ä¼šæ¯æ¬¡ä½¿ç”¨ä¸¤ä¸ªï¼Œç„¶åæ¯æ¬¡ä½¿ç”¨å››ä¸ªã€‚ç›´åˆ°æˆ‘è¾¾åˆ°ä¸ä¹‹å‰ç›¸åŒçš„ç²¾ç¡®åˆ†è¾¨ç‡ã€‚è¿›è¡Œè¿™ä¸ªæ“ä½œä¸ä¼šå¢åŠ ä»»ä½•å‚æ•°ï¼Œä½ åªæ˜¯åœ¨å®šä¹‰ä½ çš„åŸºå‡½æ•°æ˜¯ä»€ä¹ˆï¼Œæˆ–è€…åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ³¢é•¿å‡½æ•°æ˜¯ä»€ä¹ˆï¼Œè¿™æ˜¯ä¸€ä¸ªç¡¬æ³¢é•¿å‡½æ•°ï¼Œæˆ‘å¼€å§‹å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ï¼Œå¹¶ä¸”æˆ‘åœ¨æ¯ä¸€å±‚çš„å˜æ¢å™¨ä¸­éƒ½å­¦ä¹ åˆ°äº†ä¸€ç§å±‚æ¬¡ç»“æ„ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_59.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_59.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_60.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_60.png)'
- en: Improved our performance significantly as compared to not using them with addition
    of no extra parametersã€‚And I'll come to the results later also so so this is how
    the whole approach looks like you have a front end the front end is basically
    a single layer of 2000 neurons followed by dense layer of 64 neurons which is
    just to make sure to confirm it to the intermediate transformer readingsã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¸ä½¿ç”¨å®ƒä»¬å¹¶ä¸”æ²¡æœ‰é¢å¤–å‚æ•°çš„æƒ…å†µä¸‹ç›¸æ¯”ï¼Œè¿™æ˜¾è‘—æ”¹å–„äº†æˆ‘ä»¬çš„æ€§èƒ½ã€‚æˆ‘ç¨åä¼šæåˆ°ç»“æœï¼Œæ‰€ä»¥è¿™å°±æ˜¯æ•´ä¸ªæ–¹æ³•çš„æ ·å­ï¼Œä½ æœ‰ä¸€ä¸ªå‰ç«¯ï¼Œå‰ç«¯åŸºæœ¬ä¸Šæ˜¯2000ä¸ªç¥ç»å…ƒçš„ä¸€å±‚ï¼Œåé¢è·Ÿç€64ä¸ªç¥ç»å…ƒçš„ç¨ å¯†å±‚ï¼Œç›®çš„æ˜¯ç¡®è®¤ä¸ä¸­é—´å˜æ¢å™¨è¯»å–ä¸€è‡´ã€‚
- en: let's say for the transformers I define the bidding size to be 64 then that's
    the dimension which I'm like mapping them to so I take a broadway form I patch
    it in very small patches similar to how you do in vision transformers I would
    just have a single layer of 2000 neurons followed by dense layer of 64 neurons
    with the hope that the first layer kind of is learning like a Fourriier B function
    which should be adaptable according to what I'm learning after that I keep on
    doing this over and over again like I don't have a classification head or anything
    like that I keep onã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾å¯¹äºå˜æ¢å™¨ï¼Œæˆ‘å®šä¹‰çš„æ‰¹æ¬¡å¤§å°ä¸º64ï¼Œé‚£ä¹ˆè¿™æ˜¯æˆ‘æ˜ å°„åˆ°çš„ç»´åº¦ï¼Œæ‰€ä»¥æˆ‘ä»å®½å¸¦å½¢å¼å¼€å§‹ï¼Œå°†å…¶åˆ†æˆéå¸¸å°çš„è¡¥ä¸ï¼Œç±»ä¼¼äºä½ åœ¨è§†è§‰å˜æ¢å™¨ä¸­æ‰€åšçš„ï¼Œæˆ‘å°†æœ‰ä¸€å±‚2000ä¸ªç¥ç»å…ƒï¼Œåé¢æ¥ç€ä¸€ä¸ª64ä¸ªç¥ç»å…ƒçš„ç¨ å¯†å±‚ï¼Œå¸Œæœ›ç¬¬ä¸€å±‚èƒ½å¤Ÿåƒä¸€ä¸ªå‚…é‡Œå¶Bå‡½æ•°ä¸€æ ·å­¦ä¹ ï¼Œè¿™åº”è¯¥èƒ½æ ¹æ®æˆ‘çš„å­¦ä¹ è¿›è¡Œè°ƒæ•´ã€‚ä¹‹åæˆ‘ä¸€ç›´é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘æ²¡æœ‰åˆ†ç±»å¤´æˆ–å…¶ä»–ä¸œè¥¿ã€‚
- en: Adding multiple stacks of transformers after that butã€‚And then I have two approaches
    of like what I can do in terms of like adaptationã€‚I can do average pooling across
    time of these intermediate emddings because that is the idea is very similar to
    what we do in classical vision that each of the embleddings are looking at much
    much broader output in the subsequent layers or I could do w decomposition so
    what I do is that I take all of these embleddings and I define these highways
    so some of the emddings are moving fastã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæ·»åŠ å¤šä¸ªå˜æ¢å™¨å †æ ˆã€‚æˆ‘æœ‰ä¸¤ç§é€‚åº”çš„æ–¹æ³•ã€‚æˆ‘å¯ä»¥å¯¹è¿™äº›ä¸­é—´åµŒå…¥è¿›è¡Œæ—¶é—´ä¸Šçš„å¹³å‡æ± åŒ–ï¼Œå› ä¸ºè¿™ä¸ªæƒ³æ³•ä¸æˆ‘ä»¬åœ¨ç»å…¸è§†è§‰ä¸­æ‰€åšçš„éå¸¸ç›¸ä¼¼ï¼Œæ¯ä¸ªåµŒå…¥åœ¨åç»­å±‚ä¸­éƒ½åœ¨è§‚å¯Ÿæ›´å¹¿æ³›çš„è¾“å‡ºï¼Œæˆ–è€…æˆ‘å¯ä»¥è¿›è¡Œåˆ†è§£ï¼Œæ‰€ä»¥æˆ‘æŠŠæ‰€æœ‰è¿™äº›åµŒå…¥å–å‡ºå¹¶å®šä¹‰è¿™äº›é«˜é€Ÿå…¬è·¯ï¼Œå…¶ä¸­ä¸€äº›åµŒå…¥ç§»åŠ¨å¾—å¾ˆå¿«ã€‚
- en: some of them are moving very slow and some are retained at the exact same resolution
    as what the transform is learningã€‚And then I keep doing this over and over againï¼Œ
    I have a dense layerï¼Œ I have my soft max or sigmoidã€‚whatever is my classification
    headï¼Œ so so this is kind of what the approach looks likeã€‚We compare it with all
    of the traditional vision based architectureã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº›æ¨¡å‹è¿è¡Œéå¸¸æ…¢ï¼Œæœ‰äº›åˆ™ä¿æŒä¸å˜æ¢å™¨å­¦ä¹ çš„ç›¸åŒåˆ†è¾¨ç‡ã€‚ç„¶åæˆ‘ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘æœ‰ä¸€ä¸ªç¨ å¯†å±‚ï¼Œæˆ‘æœ‰æˆ‘çš„ Softmax æˆ– Sigmoidï¼Œæ— è®ºæˆ‘çš„åˆ†ç±»å¤´æ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥è¿™å°±æ˜¯è¿™ç§æ–¹æ³•çš„æ ·å­ã€‚æˆ‘ä»¬å°†å…¶ä¸æ‰€æœ‰ä¼ ç»Ÿçš„åŸºäºè§†è§‰çš„æ¶æ„è¿›è¡Œäº†æ¯”è¾ƒã€‚
- en: so the vision based models have been very good and the performance have been
    like similar in understanding audio also so we compare all of those models in
    terms like mean average precisionã€‚And we see that even the tiniest models of transformers
    were just like surpassing all of the state of the art CNN modelsã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸºäºè§†è§‰çš„æ¨¡å‹è¡¨ç°éå¸¸å¥½ï¼Œåœ¨ç†è§£éŸ³é¢‘æ–¹é¢ä¹Ÿè¡¨ç°ç›¸ä¼¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å¹³å‡ç²¾åº¦ç­‰æ–¹é¢æ¯”è¾ƒäº†æ‰€æœ‰è¿™äº›æ¨¡å‹ã€‚æˆ‘ä»¬å‘ç°å³ä½¿æ˜¯æœ€å°çš„å˜æ¢å™¨æ¨¡å‹ä¹Ÿè¶…è¶Šäº†æ‰€æœ‰æœ€å…ˆè¿›çš„
    CNN æ¨¡å‹ã€‚
- en: which was a very good signï¼Œ then we started to bump upã€‚okay the larger model
    should keep on improving the performance and with the multi-scale models as well
    as with the pooling layersã€‚they improve the performance even furtherï¼Œ which was
    kind of very surprising to us because the number of parameters are very smallã€‚these
    are very tiny architectures yet they are surpassing like things like even densenet
    which are like huge models with a lot of millions of parametersã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™éå¸¸å¥½ï¼Œæ‰€ä»¥æˆ‘ä»¬å¼€å§‹æå‡ã€‚å¥½å§ï¼Œæ›´å¤§çš„æ¨¡å‹åº”è¯¥ç»§ç»­æ”¹å–„æ€§èƒ½ï¼ŒåŒæ—¶å¤šå°ºåº¦æ¨¡å‹å’Œæ± åŒ–å±‚ä¹Ÿè¿›ä¸€æ­¥æå‡äº†æ€§èƒ½ï¼Œè¿™è®©æˆ‘ä»¬æ„Ÿåˆ°æƒŠè®¶ï¼Œå› ä¸ºå‚æ•°æ•°é‡éå¸¸å°‘ã€‚è¿™äº›æ¶æ„éå¸¸å°ï¼Œä½†å®ƒä»¬å´è¶…è¶Šäº†åƒ
    Densenet è¿™æ ·æ‹¥æœ‰æ•°ç™¾ä¸‡å‚æ•°çš„å¤§å‹æ¨¡å‹ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_62.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_62.png)'
- en: So after that we said and I'm going to conclude quickly after that we said that
    okayã€‚this is looking pretty coolï¼Œ are the what actually is the transformer or
    the first layer learning rightï¼Ÿ
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä¹‹åæˆ‘ä»¬è¯´ï¼Œæˆ‘å°†å¿«é€Ÿæ€»ç»“ï¼Œä¹‹åæˆ‘ä»¬è¯´ï¼Œå¥½å§ã€‚è¿™çœ‹èµ·æ¥å¾ˆé…·ï¼Œå˜æ¢å™¨çš„ç¬¬ä¸€å±‚å®é™…å­¦ä¹ äº†ä»€ä¹ˆå—ï¼Ÿ
- en: '![](img/e397dafe2290c08be55c621b4940569f_64.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_64.png)'
- en: In order toã€‚Make this plot what we said was okayï¼Œ if you were to take a classic
    Fourier transformã€‚Then if this axis is kind of like like frequencyï¼Œ this axis
    is the number of filters and this axis is the frequencyã€‚Then in a wayã€‚It should
    be connecting all of the points in a linear line and this is akin to the number
    of points in the 50 so how many points I'm defining hereã€‚if I'm defining 2000
    points hereï¼Œ then I would have like 2048 sineoidal B functions which are going
    from lower frequency to the most highest frequencyã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç»˜åˆ¶è¿™ä¸ªå›¾ï¼Œæˆ‘ä»¬è¯´å¥½å§ï¼Œå¦‚æœä½ è¿›è¡Œç»å…¸çš„å‚…é‡Œå¶å˜æ¢ã€‚é‚£ä¹ˆå¦‚æœè¿™ä¸ªè½´ç±»ä¼¼äºé¢‘ç‡ï¼Œè¿™ä¸ªè½´æ˜¯æ»¤æ³¢å™¨çš„æ•°é‡ï¼Œè€Œè¿™ä¸ªè½´æ˜¯é¢‘ç‡ã€‚é‚£ä¹ˆåœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œå®ƒåº”è¯¥è¿æ¥æ‰€æœ‰ç‚¹æˆä¸€æ¡ç›´çº¿ï¼Œè¿™ç±»ä¼¼äº
    50 ä¸­çš„ç‚¹æ•°ï¼Œæ‰€ä»¥æˆ‘åœ¨è¿™é‡Œå®šä¹‰å¤šå°‘ç‚¹ã€‚å¦‚æœæˆ‘åœ¨è¿™é‡Œå®šä¹‰ 2000 ä¸ªç‚¹ï¼Œé‚£ä¹ˆæˆ‘å°†æœ‰å¤§çº¦ 2048 ä¸ªä»ä½é¢‘åˆ°æœ€é«˜é¢‘çš„æ­£å¼¦ B å‡½æ•°ã€‚
- en: We said okay we'll do the exact same thing but now with filters so we have frequency
    along y axis and the number of points in my X axis and if it was a classic Fourier
    transform then it would be connecting right as a linear line but what we did was
    we take up the front end which is learned by transformer take its Fourier transform
    solved according to its center frequency as to what frequency it is activating
    the most and then keep on stacking them when we did this two problems we saw that
    we are kind of learning a different time frequency representation which is specific
    to a predict problem if I'm trying to understand what's there in the content of
    the audio I learn a representation which is very different than Fourier transform
    which should have in a straight line which is kind of like a curld exponential
    line like this and if I do like a polyphonnic pitch estimationã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¯´å¥½å§ï¼Œæˆ‘ä»¬å°†åšå®Œå…¨ç›¸åŒçš„äº‹æƒ…ï¼Œä½†ç°åœ¨åŠ ä¸Šæ»¤æ³¢å™¨ï¼Œæ‰€ä»¥æˆ‘ä»¬åœ¨ y è½´ä¸Šæœ‰é¢‘ç‡ï¼Œåœ¨ X è½´ä¸Šæœ‰ç‚¹æ•°ï¼Œå¦‚æœè¿™æ˜¯ç»å…¸çš„å‚…é‡Œå¶å˜æ¢ï¼Œé‚£ä¹ˆå®ƒåº”è¯¥è¿æ¥æˆä¸€æ¡ç›´çº¿ï¼Œä½†æˆ‘ä»¬æ‰€åšçš„æ˜¯ï¼Œæˆ‘ä»¬å–å‡ºå‰ç«¯ï¼Œé€šè¿‡å˜æ¢å™¨å­¦ä¹ çš„ï¼Œè¿›è¡Œå‚…é‡Œå¶å˜æ¢ï¼Œå¹¶æ ¹æ®å…¶ä¸­å¿ƒé¢‘ç‡è§£å†³ï¼Œå³å®ƒæœ€æ´»è·ƒçš„é¢‘ç‡ï¼Œç„¶åä¸æ–­å †å å®ƒä»¬ã€‚å½“æˆ‘ä»¬è¿™æ ·åšæ—¶ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬æ­£åœ¨å­¦ä¹ ä¸€ç§ä¸åŒçš„æ—¶é—´é¢‘ç‡è¡¨ç¤ºï¼Œè¿™å¯¹äºé¢„æµ‹é—®é¢˜æ˜¯ç‰¹å®šçš„ã€‚å¦‚æœæˆ‘è¯•å›¾ç†è§£éŸ³é¢‘å†…å®¹ï¼Œæˆ‘å­¦ä¹ çš„è¡¨ç¤ºä¸å‚…é‡Œå¶å˜æ¢éå¸¸ä¸åŒï¼Œå®ƒåº”è¯¥æ˜¯ä¸€æ¡ç›´çº¿ï¼Œè€Œæ›´åƒæ˜¯è¿™æ ·çš„å·æ›²æŒ‡æ•°çº¿ã€‚å¦‚æœæˆ‘åšç±»ä¼¼å¤šéŸ³è°ƒéŸ³é«˜ä¼°è®¡çš„äº‹æƒ…ã€‚
- en: I learn a very different front end which is adapting to that particular problem
    so this was like very exciting to us because like making computers here in a way
    in which they're adapting their ears according to a particular problem is a very
    cool idea second thing is we actually saw each of the filters as to what they
    were doing and these are basically just like single slices like this so this is
    what we would have been learned as a front end neurons we take up each of the
    neurons and we just plot them and for plotting this we basically take its free
    transform and then saw them according to where the center of frequency is when
    we just saw the neurons as to what they were learning in the front end we saw
    that it is learning properties which are very very closely matching with the traditional
    signal processing so you would have something like an onset detector learned right
    here you learning windowing function in a way it is learning toã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å­¦ä¹ äº†ä¸€ç§éå¸¸ä¸åŒçš„å‰ç«¯ï¼Œå®ƒé€‚åº”äºè¿™ä¸ªç‰¹å®šé—®é¢˜ï¼Œå› æ­¤å¯¹æˆ‘ä»¬æ¥è¯´è¿™éå¸¸ä»¤äººå…´å¥‹ï¼Œå› ä¸ºåœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œè®©è®¡ç®—æœºæ ¹æ®ç‰¹å®šé—®é¢˜è°ƒæ•´å…¶â€œè€³æœµâ€æ˜¯ä¸ªéå¸¸é…·çš„æƒ³æ³•ã€‚ç¬¬äºŒä»¶äº‹æ˜¯æˆ‘ä»¬å®é™…ä¸Šè§‚å¯Ÿäº†æ¯ä¸ªæ»¤æ³¢å™¨çš„ä½œç”¨ï¼Œè¿™åŸºæœ¬ä¸Šå°±åƒå•ä¸ªåˆ‡ç‰‡ä¸€æ ·ï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä»¬æ‰€å­¦ä¹ çš„å‰ç«¯ç¥ç»å…ƒï¼Œæˆ‘ä»¬å–å‡ºæ¯ä¸ªç¥ç»å…ƒå¹¶è¿›è¡Œç»˜å›¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šé‡‡ç”¨å…¶å‚…é‡Œå¶å˜æ¢ï¼Œç„¶åæ ¹æ®é¢‘ç‡ä¸­å¿ƒè¿›è¡Œåˆ†ç±»ã€‚å½“æˆ‘ä»¬è§‚å¯Ÿå‰ç«¯ç¥ç»å…ƒæ‰€å­¦ä¹ çš„å†…å®¹æ—¶ï¼Œæˆ‘ä»¬å‘ç°å®ƒå­¦ä¹ çš„å±æ€§ä¸ä¼ ç»Ÿä¿¡å·å¤„ç†éå¸¸ç›¸ä¼¼ï¼Œå› æ­¤ä½ ä¼šçœ‹åˆ°åƒä¸€ä¸ªèµ·å§‹æ£€æµ‹å™¨åœ¨è¿™é‡Œå­¦ä¹ çª—å£å‡½æ•°ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_66.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_66.png)'
- en: Have a kernel which is best for a time frequency representation what people
    have been using in signal processinging which is like a hamming or a hanging window
    we are learning these pure sinusoids which are responsible for activating a particular
    frequency so you can see the richness as compared to like having a fixed purely
    sinusoidal PCs function right hereã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¥æœ‰ä¸€ä¸ªæœ€ä½³çš„æ—¶é—´é¢‘ç‡è¡¨ç¤ºå†…æ ¸ï¼Œè®¸å¤šäººåœ¨ä¿¡å·å¤„ç†ä¸­ä½¿ç”¨ï¼Œè¿™ç±»ä¼¼äºæ±‰æ˜çª—æˆ–æ‚¬æŒ‚çª—ï¼Œæˆ‘ä»¬æ­£åœ¨å­¦ä¹ è¿™äº›çº¯æ­£çš„æ­£å¼¦æ³¢ï¼Œå®ƒä»¬è´Ÿè´£æ¿€æ´»ç‰¹å®šé¢‘ç‡ï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°å®ƒçš„ä¸°å¯Œæ€§ï¼Œä¸å›ºå®šçš„çº¯æ­£å¼¦æ³¢PCåŠŸèƒ½ç›¸æ¯”ã€‚
- en: So this was what we hadã€‚Doneï¼Œ and then to share the final thoughtsã€‚I'll conclude
    by saying that okayï¼Œ transformers are proving to be a major advancement in AI
    research across the fieldsã€‚Andã€‚It seems like they're solving everything for now
    and hopefully this is not the end and we should keep an eye out on something which
    would change and have impactã€‚which is more thanã€‚What transformers have putï¼ŸAnd
    who knows what's going to come nextï¼ŸYeahã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬æ‰€åšçš„ï¼Œç„¶ååˆ†äº«æœ€ç»ˆçš„æƒ³æ³•ã€‚æˆ‘å°†æ€»ç»“è¯´ï¼Œå˜æ¢å™¨åœ¨å„ä¸ªé¢†åŸŸçš„AIç ”ç©¶ä¸­æ­£è¯æ˜æ˜¯é‡å¤§çš„è¿›æ­¥ã€‚è€Œä¸”ï¼Œçœ‹èµ·æ¥ä»–ä»¬ç°åœ¨æ­£åœ¨è§£å†³ä¸€åˆ‡ï¼Œå¸Œæœ›è¿™ä¸æ˜¯ç»ˆç‚¹ï¼Œæˆ‘ä»¬åº”è¯¥å…³æ³¨å¯èƒ½ä¼šæ”¹å˜å’Œäº§ç”Ÿå½±å“çš„ä¸œè¥¿ï¼Œè¿™è¶…å‡ºäº†å˜æ¢å™¨çš„è´¡çŒ®ã€‚è°çŸ¥é“æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆå‘¢ï¼Ÿæ˜¯çš„ã€‚
- en: so by that I'll just conclude and I'll be happy to take questionsã€‚Thank you
    Praique that was a really good talk and you provided some really good insights
    about how like Transers work for the audio case and yeah thank you for the talk
    and now I would invite questions from from the class students let me just stop
    the recordingã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†æ€»ç»“ä¸€ä¸‹ï¼Œå¹¶ä¹æ„å›ç­”é—®é¢˜ã€‚è°¢è°¢Praiqueï¼Œè¿™æ˜¯ä¸€æ¬¡éå¸¸å¥½çš„æ¼”è®²ï¼Œä½ æä¾›äº†ä¸€äº›å…³äºå˜æ¢å™¨åœ¨éŸ³é¢‘æ¡ˆä¾‹ä¸­å¦‚ä½•å·¥ä½œçš„æ·±åˆ»è§è§£ï¼Œæ„Ÿè°¢ä½ çš„æ¼”è®²ï¼Œç°åœ¨æˆ‘é‚€è¯·ç­çº§çš„å­¦ç”Ÿæé—®ï¼Œè®©æˆ‘åœæ­¢å½•éŸ³ã€‚
- en: '![](img/e397dafe2290c08be55c621b4940569f_68.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_68.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_69.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_69.png)'
