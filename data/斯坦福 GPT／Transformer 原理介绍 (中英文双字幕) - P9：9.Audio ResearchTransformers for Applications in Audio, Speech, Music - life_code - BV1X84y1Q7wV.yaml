- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P9：9.Audio ResearchTransformers for Applications
    in Audio, Speech, Music - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P9：9.Audio ResearchTransformers for Applications
    in Audio, Speech, Music - life_code - BV1X84y1Q7wV
- en: '![](img/e397dafe2290c08be55c621b4940569f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_0.png)'
- en: Thanks for inviting me for the talk today and I'll be just talking about Transers
    for music and audio which is very different than what all of us were doing in
    this past course I'm also the only speaker from Stanford so I have to do a good
    job so you see like a very good slides because I'm kind of representing the university
    in some sense。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢你们邀请我今天来演讲，我将讨论音乐和音频的变压器，这与我们过去的课程非常不同。我也是唯一一位来自斯坦福的发言人，所以我必须表现出色，确保你们看到非常好的幻灯片，因为在某种意义上我代表着这个大学。
- en: So yeah so the flow of the talk for today is basically like I'll be throwing
    a lot of stuff it's kind of like a buffet style and then you feel free to like
    or dislike whatever you want。And I'll be talking mostly about like three papers
    of what I've been working on。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以今天演讲的流程基本上是，我会讲很多内容，像自助餐一样，你可以随意喜欢或不喜欢。我将主要讨论我所做的三篇论文。
- en: I start with like introducing like what transformers are from a different perspective。what
    audio representations are。Talk about a generative model for audio。which is just
    doing like language modeling on sample level。Then I'll talk about how can one
    do like language modeling for speech and audio。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先会从不同的角度介绍什么是变压器，以及音频表示是什么。谈论一个用于音频的生成模型，实际上是在样本级别上进行语言建模。然后我将讨论如何对语音和音频进行语言建模。
- en: which is different than what people do for text， what are the current trends
    in the literature。![](img/e397dafe2290c08be55c621b4940569f_2.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这与人们处理文本的方式不同，目前文献中的趋势是什么。![](img/e397dafe2290c08be55c621b4940569f_2.png)
- en: Finally， I'll briefly mention similar stuff as to what was happening in Comp
    vision with regard to vision transformers。or can we adapt similar ideas for audio
    transformers and throw in a bit of signal processing to improve the performance。Having
    told that the talk is about 35 to 40 minutes with about 15 minutes of Q&A。I should
    also say that all of the opinions of mine and Stanford or any other professor
    is not responsible for any of the mistake which I do。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将简要提及与视觉变压器相关的计算机视觉领域发生的类似事情，或者我们能否将类似的思路应用于音频变压器，并加入一些信号处理以提高性能。谈到这里，演讲大约为35到40分钟，包含约15分钟的问答。我还要说，我的所有观点以及斯坦福或其他教授的观点对我所犯的错误不承担责任。
- en: hel。So transformers kind of have kind of revolutionized in a way like everyone
    was approaching deep learning before that was all about CNNNs and mostly all of
    these prominent models have been coming in waves so there was a time when everyone
    was just applying CNNs then came a time where people started adapting CNNs in
    some sort of like dilated cons and slowly the recordcurrent networks were getting
    out of fashion now it seems like transformers and fashion all the time so it seems
    to be solving almost every single problem which is being thrown at them。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨。所以变压器在某种程度上革命性地改变了大家之前在深度学习领域的做法，之前的重点都是CNNs，所有这些突出的模型都是在一波波涌现，曾经一段时间每个人都在应用CNNs，然后开始适应某种扩张卷积，慢慢地循环神经网络也在失宠，现在变压器似乎总是时尚，几乎解决了所有提出的问题。
- en: So， so what's special about？One of the fact that struck me was their simplicity。which
    is if you think about it， this and it has been like hugely popular also so it
    was just released in 2018 and within three years it has like about 30。000 citations
    and it is kind of solving every single problem in every single domain。it has its
    limitations also， but if you think about it in a way transformers are basically
    like a way of like just cascading self-at with feature learning and if we keep
    on doing it over and over again。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，特别之处在哪里？让我印象深刻的一个事实是它们的简单性。如果你想想看，它非常受欢迎，2018年刚发布，三年内就有大约30,000次引用，几乎解决了每个领域的每个问题。它也有局限性，但如果你从某种角度来看，变压器基本上就是以一种级联的自注意力进行特征学习，如果我们不断这样做。
- en: then the model in a way learns which parts of the input are important and keep
    on transforming them removing the contents which are not important and just have
    the limited information which is just responsible for a particular task。呃。And
    it has been very very difficult to keep up with the literature， you know。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 然后模型以某种方式学习哪些输入部分是重要的，并持续转换它们，移除不重要的内容，只保留负责特定任务的有限信息。呃。跟上文献真的非常困难，你知道的。
- en: like I have put it as a joke here but then even Twitter's recommendation engine
    were kind of just getting out of like they were getting hay viruss to like why
    is Chris Manning just searching about transformers and that was way back in 2020
    so it has been like difficult for researchers also to keep up with a pace of what's
    going on。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里开了个玩笑，甚至连 Twitter 的推荐引擎也开始关注：为什么 Chris Manning 会搜索变换器，这还要追溯到 2020 年，所以对于研究人员来说，跟上发生的事情的节奏也很困难。
- en: Just before Transers。All the LP community was just doing gaga about like bidirectional
    eteems with attention。so every single paper before 2017 was just like you have
    encode or LTM layers。you keep on adding like multiple layers and then after that
    you have a attention mechanism which just loads at what's important and then just
    keeps on decoding sequentially one at a time。But this was not kind of like an
    ideal way to do it， you know。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Transformers 之前，整个语言处理社区都在为双向 LSTM 和注意力机制而疯狂。所以在 2017 年之前的每一篇论文都只是这样：你有编码或
    LSTM 层，你不断添加多个层，然后在之后有一个注意力机制，它只是关注重要的内容，然后依次解码。但这并不是一种理想的方式，你知道的。
- en: because what turns out is when we start throwing in longer sequences， the connections
    are no longer。Storing the gradient updates in a way it should be doing so what
    what the researchers from Google said you know like instead of having just a attention
    layer at the very last encoding we would just have these attention mechanisms
    at every single layer which in a way would just learn what's important for a particular
    problem at that particular layer and we keep on doing it over and the over again。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为事实证明，当我们开始处理更长的序列时，连接不再如预期那样存储梯度更新。因此，谷歌的研究人员提到，与其在最后一个编码层只有一个注意力层，我们不如在每一层都有这些注意力机制，这样就可以学习在特定层中某个问题的重要性，并不断重复这个过程。
- en: So then then the whole idea of like transformers and retention mechanism cascaded
    one after the other came and I'll not go into the details because this is the
    last class of the courses。but then usual tricks to help across the neural net
    literature。which is like having multihad attentions having skip connection and
    layer norms so all of these things they are not only like giving gains for transformers
    themselves。but they can be just applied to any single other architecture also。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以然后变换器和注意机制的整体思路相继出现，我就不详细讲了，因为这是课程的最后一节课。但常用的技巧也有助于整个神经网络文献，比如多头注意力、跳跃连接和层规范化，这些不仅对变换器本身有好处，还可以应用于任何其他架构。
- en: '![](img/e397dafe2290c08be55c621b4940569f_4.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_4.png)'
- en: The other thing which is helping these research is basically the compute bar
    is getting better and better so all of these big companies are just throwing massive
    amounts of computing resources at solving very very simple in tasks the top of
    the hill being like the switch transformer which was discussed in the course also。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个推动这项研究的因素是计算能力不断提升，所以所有这些大公司都在投入大量计算资源来解决非常简单的任务，其中最顶尖的就是课程中讨论的 Switch Transformer。
- en: But but one of the thing which I think started all of the strength was Elmo
    which was just learning these contextualized representations for natural language
    processing and that model right here was perhaps one of the first。呃。Kind of like
    like moral。0。0 or something or 0。1 in terms of like bringing and ushering in the
    whole revolution you can see that how similar these kind of modelss look like
    Bt was basically like inspired heavily from Elmo in which they just replace some
    of the Lteem layers with transformer modules。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为所有力量的起源之一是 Elmo，它只是学习了这些用于自然语言处理的上下文化表示，这个模型可能是最早的之一。呃。就像道德一样。0。0 或者 0。1，在引入整个革命方面，你可以看到这些模型的相似之处。Bt
    基本上是受到 Elmo 的启发，他们只是用变换模块替换了一些 LSTM 层。
- en: So， so a point to note also is like。Irespective of like natural language processinging
    or other domain。these can be adapt in a variety of domains and for today's talk
    I'll be just adapting them to audio。![](img/e397dafe2290c08be55c621b4940569f_6.png)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一个需要注意的点是，无论是自然语言处理还是其他领域，这些方法都可以适应多种领域，而今天的讨论中我将它们应用于音频。![](img/e397dafe2290c08be55c621b4940569f_6.png)
- en: So I'll basically start with introducing people what audio representations are
    and just for the sake of completeness talk about spectrograms so like you can
    take any time domain signal and you can decompose that signal into a variety of
    basis functions and if you take up a Fourier transform your kind of like decomposing
    actual time domain signal into a soidal basis components so if you have like a
    waveform here like this which is a sum of three pure soids。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我基本上将从介绍音频表示法开始，只为完整起见谈谈谱图。你可以将任何时域信号分解为多种基函数，如果你进行傅里叶变换，你就像是在将实际时域信号分解为正弦基组件。因此，如果你这里有一个波形，它是三个纯正弦波的总和。
- en: Then there sum basically is this and you can see that when you take a freeier
    transform and its magnitude。you kind of have their strength of the individual
    components。Sn here so you can take up another wave from let's say a square wave
    and what you have is basically much richer sinusoidal decomposition because it
    is kind of a discontinuous signal so you need like many more sinusoids to represent
    that particular signal as close to the actual signal as possible。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里的总和基本上就是这个，你可以看到当你进行傅里叶变换及其幅度时，你基本上有了各个成分的强度。Sn在这里，所以你可以从一个正方波取出另一个波，你所拥有的基本上是更丰富的正弦分解，因为它是一种不连续信号，因此你需要更多的正弦波来尽可能接近实际信号。
- en: And here also you can see that okay if this was a square wave then it is actually
    made up of a lot of sinusoids where each of the bar here represents the strength
    of the particular sinusoid from an optimization perspective I mean this right
    away is subop right because you are kind of fixing up the number of sinusoid you're
    using for representing a square wave I would have rather used a basis function
    which it was a square wave itself than sinusoid with signal right the second thing
    is like even if you are taking a sinusoid signal we kind of just。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里你也可以看到，如果这是一种正方波，那么它实际上由许多正弦波组成，每根条形代表特定正弦波的强度。从优化的角度来看，这样做显然是次优的，因为你在为表示正方波固定了正弦波的数量。我更倾向于使用正方波本身的基函数，而不是正弦波信号。第二件事是，即使你在处理正弦波信号，我们也只是。
- en: Putting them in an equidistant space so you are kind of dividing the whole frequency
    access into like equidistant bins and each of the bins are responsible for like
    a particular sinusai a lot。So that is like a traditional fur representation for
    representing any signal。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们放置在等距空间中，你基本上是在将整个频率轴划分为等距的 bins，每个 bin 负责一个特定的正弦波。所以这就是传统傅里叶表示法，用于表示任何信号。
- en: What we do for what a spectrogram so like but in reality all these signals are
    discontinuous all of these signals vary quite a bit right so you can have like
    a signal while i'm speaking which is like a square for a certain period of time
    and then it gets sinusoidal and then it becomes something else so what we really
    need is like in a way to。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在处理谱图时，实际上所有这些信号都是不连续的，这些信号的变化相当大。因此，在我说话时，你可以有一个正方波，在某段时间内然后它变得正弦波状，接着变成其他形式。所以我们真正需要的是一种方式来。
- en: kindind of like take patches of input signal and take freeier transform of these
    individual patches I'm deliberately using the wood patches but you can like in
    traditional terms you're windowing the signal so right here you can see that you
    have a continuous signal you keep on windowing it you apply the freeier transform
    and what you get is basically like a spectrogram representation of the signal
    so right here what you're seeing basically is for each of the slices the signal
    kind of look like this after taking the freeier transform with a waveform which
    is there below。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于获取输入信号的片段并对这些单独的片段进行傅里叶变换，我故意使用“木片”这个词，但在传统术语中你是在对信号进行窗函数处理。所以在这里你可以看到，你有一个连续信号，你不断对其进行窗函数处理，应用傅里叶变换，得到的基本上是信号的谱图表示。因此，在这里你所看到的基本上是每个切片的信号在进行傅里叶变换后的样子，以及下面的波形。
- en: And what you're do is for spectrogram representation you keep on stacking these
    Fourier trans slice the magnitude of the Fourier transform slices and in this
    way you kind of get like a 2D representation of audio signals and if you are coming
    from a vision background it is basically all of the things which you are doing
    in vision would just work well if you just apply them to these 2D spec representations
    and'll quickly play how these spectrograms look for a wide area of like common
    sounds。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 而你要做的是，对于频谱图表示，你不断堆叠这些傅里叶变换切片的幅度，以这种方式你可以获得音频信号的二维表示，如果你来自视觉背景，实际上你在视觉中所做的所有事情，如果你将它们应用于这些二维频谱表示，效果会很好，并且我会快速展示这些频谱图在常见声音的广泛范围内是如何呈现的。
- en: '![](img/e397dafe2290c08be55c621b4940569f_8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_8.png)'
- en: 嗯。Yeah。🎼，🎼，🎼Yeah。W。![](img/e397dafe2290c08be55c621b4940569f_10.png)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。是的。🎼，🎼，🎼是的。W。![](img/e397dafe2290c08be55c621b4940569f_10.png)
- en: Okay。So you could see like for spectrograms you have kind of like a time axis
    on your x axis and then you have a frequency axis on y axis and then for whatever
    is your signal of interest you're basically like putting these slices together
    and different sound gives you like different spectral representation so it's kind
    of a vision problem just in this sort of like Fourier space。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以你可以看到，对于频谱图，你的 x 轴有一个时间轴，y 轴有一个频率轴，然后对于你感兴趣的信号，你基本上是在将这些切片组合在一起，不同的声音给你不同的频谱表示，因此这在某种程度上是一个视觉问题，只是在这种傅里叶空间中。
- en: So there can be like different kinds of representations also so one you could
    you could just take these slices of coier transform and then do like a linear
    mapping to them so that you are kind of in a way making these as close to how
    humans here so you can have like log of the frequency and the wax instead sort
    of common frequency and then you get like a constant queue like representation
    the advantage of this being like you can see that for different frequencies the
    spacing between the harmonics kind of remains same so if you're like training
    convolutional filters and that's of a huge advantage because the signal like one
    component of the invaris is gone and you can just learn these filters which are
    catching onto these constant templates of Fourier slices you can have metal filter
    bank coefficient or you can have like the raw waveform also for raw waveforms
    basically there are two things which we have to keep in mind one is the sampling
    ratess so we kind of like take the continuous signal and then we。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以可以有不同类型的表示，因此一个方法是你可以直接取这些傅里叶变换的切片，然后进行线性映射，从某种程度上使其更接近人类的听觉，因此你可以有频率的对数而不是常规频率，然后你得到一个恒定的队列表示，这样的优势在于，你可以看到不同频率之间的谐波间距保持一致。因此，如果你在训练卷积滤波器，这就是一个巨大的优势，因为信号的一个成分已经消失，你可以学习这些捕捉到这些恒定傅里叶切片模板的滤波器，你可以有金属滤波器组系数，或者你也可以有原始波形，对于原始波形，我们基本上有两件事情需要记住，一是采样率，所以我们基本上是将连续信号进行处理，然后我们。
- en: Scritize the continuous signal so one way one parameter is like how fast we
    are sampling the continuous signal。So that's typically on the order of like 16000
    or 8000 times a second if you're on telephonic speech the other thing which we
    also is like how how many levels we are dividing your vertical axis so in this
    case you can see that each of the dots is basically one level and typically people
    use 8 bit quants or 16 bit quants so in a way you can think about that for every
    one second audio which we would hear you would have like 16000 samples and then
    in each of the 16000 samples are allowed to take one of the levels between0 to
    55 and that's like if I can like take the problem of like continuous audio and
    just have it in terms of like this sort of like discrete space then basically
    like I'm just going to the territory of like doing language modeling。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 细分连续信号，所以其中一个参数是我们采样连续信号的速度。因此，如果你是在电话语音中，这通常是每秒约 16000 或 8000 次，另一件我们要考虑的事情是我们如何划分垂直轴的级别，因此在这种情况下，你可以看到每个点基本上是一个级别，通常人们使用
    8 位量化或 16 位量化，因此可以这样想，对于我们听到的每一秒音频，你将有大约 16000 个样本，然后在每个 16000 个样本中，有可能取 0 到 55
    之间的一个级别。如果我能将连续音频的问题转化为这种离散空间，那么基本上我就进入了语言建模的领域。
- en: '![](img/e397dafe2290c08be55c621b4940569f_12.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_12.png)'
- en: 呃。![](img/e397dafe2290c08be55c621b4940569f_14.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 呃。![](img/e397dafe2290c08be55c621b4940569f_14.png)
- en: So one the first papers I' discuss is how can we do like generative modeling
    for raw audio。which is similar to wavenets using transformers and we put in QR
    codes if you like the stuff what I'm doing and if you think that this is relevant
    to you please cite or please have a look in terms of like the QR codes so yeah
    so I'll start with the first subtopic of today's talk which is like what are wave
    nets and how do we do like this generative modeling of raw audio。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我将讨论的第一篇论文是如何对原始音频进行生成建模。这类似于使用变换器的 WaveNet，如果你喜欢我正在做的东西，认为这对你有相关性，请引用或者查看二维码。所以我将开始今天演讲的第一个子主题，即什么是
    WaveNet，以及我们如何进行原始音频的生成建模。
- en: '![](img/e397dafe2290c08be55c621b4940569f_16.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_16.png)'
- en: So in single word， you can think about this as doing like language modeling
    over these 255 states of audio。so you can throw in your favorite transform model
    like transform Excel or GPT or whatever you want to call it and just treat the
    problem as if you're trying to predict one of the levels of a 235 and you have
    to predict the next level given a certain context。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，用一个词来说，你可以把这看作是在这 255 个音频状态上进行语言建模。你可以投入你喜欢的变换模型，比如 Transform Excel 或 GPT，或者你想称之为的任何名字，把这个问题视作试图预测
    255 个级别中的一个级别，并根据特定上下文预测下一个级别。
- en: That's what Wanet was doing so the way you are modeling the probability distribution
    of a continuous space is basically you're trying to predict what's the probability
    of the next sample given some parts one text and Wavenet has been like hugely
    popular because it has over 3000 citations and it has been a code building block
    for almost like all speech and audio related problems like you can think about
    like speech to text text to speech synthesis。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是 WaveNet 的作用，模型在连续空间中的概率分布基本上是预测给定部分文本的下一个样本的概率。WaveNet 之所以广受欢迎，是因为它的引用超过
    3000 次，并且几乎成为所有语音和音频相关问题的构建模块，比如语音转文本和文本转语音合成。
- en: instrument conversion packet loss concealment with the internet speech denoizing
    so wherever there's some sort of element of modifying audio people have been using
    wavenet as a code building block。And raw away from synthesis has been difficult
    because just the magnitude of the problem。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网语音去噪中，仪器转换的丢包隐蔽技术使得音频的修改成为可能，人们一直在使用 WaveNet 作为构建模块。而原始音频合成一直很困难，因为问题的复杂性。
- en: if I'm just trying to synthesize 10 seconds of audio。would just amount to like
    me having a probability distribution over 160，000 samples。And that itself is stuff
    because our ears are very very sensitive to subtle changes if I'm off by one pixel
    in an image the image like my eyes would not be as as susceptible to noticing
    that effect versus like if I'm off by say a few pixel a few samples in an audio
    it will just catch our ears pretty quickly people have been trying raw audio synthesis
    a lot in the past and before all of the wavenet and transformer based approaches
    kind of like wave rronance and sample rinance were kind of like like state of
    the art models on the right I' seen I've shown a sample rronN model which kind
    of like。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我只是试图合成 10 秒的音频，那么这就相当于我需要对 160,000 个样本进行概率分布预测。而这本身就是个挑战，因为我们的耳朵对细微变化非常敏感。如果我在图像中偏差一个像素，我的眼睛可能不会那么容易注意到那个效果；然而，如果我在音频中偏差几个像素或样本，我们的耳朵会很快捕捉到这个变化。过去人们在原始音频合成方面进行了大量尝试，之前的
    WaveNet 和基于变换器的方法之前的状态如 Wave Rronance 和 Sample Rronance 类似是当时的先进模型。我展示了一个 Sample
    RronN 模型。
- en: Models the probability distribution of what's going to come next given the past
    at multiple levels。And this was work done by Yoa Bnji at Nila， but you can closely
    see like if you just see this architecture versus a transformer architecture in
    a way。these are like starting to get very very similar because what you're trying
    to do is that for the probability distribution here you're trying to see a lot
    of like local substructures and then you keep on doing it over and over again
    and you can draw parallels like okay attention mechanism should also kind of be
    doing the same thing so。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 模型会根据过去的多个层次来预测接下来会发生什么的概率分布。这项工作是由 Nila 的 Yoa Bnji 完成的，但你可以清楚地看到，如果将这个架构与变换器架构进行对比，它们实际上开始变得非常相似，因为你试图做的是，对于这里的概率分布，你需要识别大量的局部子结构，然后不断重复这一过程，你可以得出平行关系，比如说注意机制也应该做类似的事情。
- en: '![](img/e397dafe2290c08be55c621b4940569f_18.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_18.png)'
- en: This was the this was the kind of like the literature in the past what we tried
    to do was we just had like the wavenet model and we try to see whether transformers
    can beat them and our intuition was like it should be able to read them because
    they are successful all over the other other domains like in language modeling
    so it should it should do that for raw waveformms also we also try to see whether
    we can circumvent the auto and square constrained by。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是过去的文献，我们尝试做的就是使用wavenet模型，看看变换模型是否能够超过它们，我们的直觉是，它应该能够超越，因为它们在其他领域，如语言建模中表现出色，所以它应该也能在原始波形中做到这一点。我们还尝试看看是否可以规避自动和平方约束。
- en: Conditioning of the context itself and like we did not go for specific applications
    and we just said。okay， just in terms of like modeling behavior， how will they
    do？
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文的条件和我们没有针对特定应用进行探索，我们只是说，好吧，就像在建模行为方面，他们会怎么做？
- en: '![](img/e397dafe2290c08be55c621b4940569f_20.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_20.png)'
- en: So so the data set for this was just like real world data recordings so actual。Sound
    should not matter because like the model is agnostic to what it has been thrown
    in and the setup was exactly the same like you're giving a certain context and
    I have to predict like the next sample you do the same thing with wavenets you
    do the exact same thing with transform based like GPT kind of like a model and
    see how well they do。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这个数据集就像是现实世界的数据录音，实际上，声音并不重要，因为模型对输入的内容是无关的，设置是完全相同的，你给定一个特定的上下文，我必须预测下一个样本，你用wavenets做同样的事情，你用基于变换的模型，比如GPT，做完全相同的事情，看看它们的表现如何。
- en: '![](img/e397dafe2290c08be55c621b4940569f_22.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_22.png)'
- en: I'll briefly chat about like what wavength models are so wavevenett was kind
    of like a convolutional based model which was getting rid of like all of the vanishing
    gradient problem by just treating sequential problem as being learned by a convolutional
    model so what they did was basically like have this sort of like dilation layers
    or like a convolution with dis which is basically like I kind of skip in every
    subsequent layer by one sample so you can see like if I have dilation factor of
    two with a kernel size of two I would get this kind of a topology where my convolution
    filters in the very first layer I just like combining the first two samples and
    I skip by one in the next layer and then I skip by three which is like I look
    at like the fourth one and the next layer and so on the loss is still the same
    so I have this network I learn a latent space and then I have like a cross categorical
    crossenttropy loss which is basically have to predict the next sample。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我将简要讨论一下wavength模型，所以wavenet是一种基于卷积的模型，它通过将序列问题视为由卷积模型学习，从而消除了梯度消失问题。他们所做的基本上是有这种扩张层或卷积，这基本上是在每个后续层中跳过一个样本，所以你可以看到，如果我有一个扩张因子为2，卷积核大小为2，我会得到这种拓扑结构，在第一层中，我只是结合前两个样本，并在下一层中跳过一个，然后在接下来的层中跳过三个，依此类推，损失仍然是相同的，所以我有这个网络，学习一个潜在空间，然后我有一个交叉分类交叉熵损失，这基本上是要预测下一个样本。
- en: '![](img/e397dafe2290c08be55c621b4940569f_24.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_24.png)'
- en: In the previous one。And I just do the exact same thing with transformers also
    so but then I have to make sure that I do it in a causal manner so I have something
    which is very similar to GPT in which I have cause masks in my attention mechanism
    and I keep doing it over and over again so you have like selfper after that you
    have feed fer layersers you just have a stack of these transformer blocks and
    see how will they do。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的实验中，我对变换模型做了完全相同的事情，但我必须确保以因果的方式进行，所以我有一些非常类似于GPT的东西，在我的注意机制中有因果掩码，我不断重复这个过程，所以你有自注意力机制，然后是前馈层，你只是堆叠这些变换块，看看它们的表现如何。
- en: So I said in it should work so。Like it should be doing better than like our。Base
    wavenet models right because if you look at the topology we are kind of defining
    a topology on our own right so what what if like the current prediction Excel
    layer one world to depend on like very like way back samples here instead of the
    second sample the1 samples so we are kind of ignoring all of that topology which
    would have an important for prediction of this particular task whereas transform
    risk with the self-atten mechanism can just learn like okay which part of the
    samples are important and which are not and you can keep on doing it creatively
    so it made sense to us that okay。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我说，这应该有效。像我们的基本 WaveNet 模型应该表现得更好，因为如果你看拓扑结构，我们在自己定义一个拓扑结构。那么，如果当前的预测 Excel
    层依赖于很久以前的样本，而不是第二个样本，第一样本，我们在某种程度上忽视了所有这些拓扑结构，这对于这个特定任务的预测是重要的，而变换器通过自注意机制可以学习到哪些样本是重要的，哪些不是，你可以不断创造性地进行调整，因此这对我们来说是有道理的。
- en: Transform layer should be doing way better than wavelength models。嗯。The second
    thing which we came across was like okay we cannot have a lot of context。for example
    the attention mechanism needs to store all of those of order n squares and in
    this case if I'm like storing data at 100 milliseconds then I have like about
    1600 samples and I need to store 1600 by 1600 at multiple layers and it just becomes
    like a huge problem with the data problem with the memory constraint so what we
    said was okay what if we just use the context itself as a latent code in order
    to like have like much better representation at every layer we cannot have like
    huge big attention matrices so what we said was we would just do sample by conditioning
    and through a CNN layers just to understand what the latent code would be so you
    still have like an attention mechanism or just a past context but then I'm also。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 变换层的表现应该远好于波长模型。嗯。我们遇到的第二个问题是我们不能有太多的上下文。例如，注意机制需要存储所有的 n 平方次序数据，在这种情况下，如果我以
    100 毫秒的速度存储数据，那么我大约有 1600 个样本，我需要在多个层上存储 1600 x 1600 的数据，这在内存限制方面会成为一个巨大的问题。所以我们说，假设我们将上下文本身作为潜在编码来使用，以便在每一层有更好的表示。我们不能有巨大且复杂的注意矩阵，因此我们决定通过条件处理和
    CNN 层来理解潜在编码，因此你仍然有注意机制或者仅仅是过去的上下文。
- en: At every sample okay what the next sample should be given on this context in
    building and if you think about it in a way it is like okay if there are like
    five or six notess being played in a piano then Im kind of certain which notess
    will be played to a certain extent if I just throw in a CNN layer so I'll use
    that information along with what my transfers are learning and then I would condition
    it and I would just use that to predict the next sample。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在每一个样本中，我们应该考虑在构建上下文时下一个样本应该是什么，如果你想想，这就像在钢琴上弹奏五六个音符，那么我可以确定哪些音符会在一定程度上被弹奏，如果我只是加上一个
    CNN 层。因此，我将使用这些信息以及我的传递学习，然后我会对其进行条件处理，并用它来预测下一个样本。
- en: '![](img/e397dafe2290c08be55c621b4940569f_26.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_26.png)'
- en: So for the evaluation criteria we did not look for negative not likelihood scores。we
    just looked at how well our prediction prediction task was so we took up like
    stacked wave net which was implemented by deep mind and saw that okay what was
    the performance using their benchmarks and even like bigger stacked wave nets。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在评估标准方面，我们并没有关注负似然得分。我们只是关注我们的预测任务效果如何，所以我们采用了由 DeepMind 实现的堆叠 WaveNet，并观察了它在他们基准测试下的表现，甚至是更大的堆叠
    WaveNet。
- en: we then started to increase the complexity of transformers and started to see
    whatever we had proposed in terms of like conditioning on on the vanilla transformer
    architectures to see how well they do we did not look for like an application
    specific problem which is basically like we don't look at like how well perceptual
    task was part for like say text to speech synthesis or speech renoizing we just
    look at okay if we are trying to model this using a crossenttropy loss then with
    the same model with the same loss function how will they do on like similar kind
    of parameters？
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们开始增加变压器的复杂性，并开始观察我们在条件作用于普通变压器架构方面提出的建议效果如何。我们并没有寻找特定应用的问题，也就是说，我们没有关注像文本到语音合成或语音识别这样的感知任务的表现，我们只是看如果我们试图使用交叉熵损失来建模，那么在相同模型和相同损失函数下，它们在类似参数上的表现如何？
- en: So this was the first kind of like subblock of like how can we use transformers
    for generatedative modeling？
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何利用变压器进行生成建模的第一个子块。
- en: 哦。For the second problem， I'll do a quick headaway on。How can we use like transformers
    for doing language modeling which is kind of becoming a really fancy term right
    now and this work was done by Julia Smith way back in 2020 and the goal of this
    was can we kind of in a way do language modeling with continuous audio sequences
    and I'll briefly mention about that this part of in this subb of the talk？
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 哦。对于第二个问题，我会快速讨论一下。我们如何利用变压器进行语言建模，这个概念现在变得相当时尚，这项工作是由**朱莉亚·史密斯**在2020年完成的，目标是我们能否以某种方式用连续音频序列进行语言建模，我将简要提及这一部分。
- en: '![](img/e397dafe2290c08be55c621b4940569f_28.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_28.png)'
- en: 哦。So， so and this is in regard for like solving acoustic scene understanding。which
    is basically like。If I'm given a chunk of audio。then I want to understand what's
    in there and if we could do that well。then in a way we can do a lot of fancy nice
    applications so for example like if you think about like cell driving cars so
    VMmo has started to incorporate microphones into the cell driving cars why because
    say if there is a ambulance coming or if there is a。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 哦。因此，这与解决声学场景理解有关。如果我得到一段音频，我想理解其中的内容，如果我们能够很好地做到这一点，那么在某种程度上，我们可以做很多很酷的应用，例如，如果你考虑自动驾驶汽车，**VMmo**已经开始在自动驾驶汽车中加入麦克风，原因是如果有救护车来了，或者有其他声音。
- en: File truck coming then that sound would be picked up way way before even than
    the Liarrs or even their sensors so they want to understand that and take actions
    based upon that Apple during COviID did a handwa detection on their Apple watch
    because if you could detect when someone is washing their hands then you can in
    a way like tell people that oh you need to wash hands for 20 seconds and then
    that that can be built upon as a cool application。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 文件卡车来了，那个声音会在李尔斯或他们的传感器之前被捕捉到，因此他们想了解这一点并根据此采取行动。苹果在COVID期间对他们的Apple Watch进行了洗手检测，因为如果你能检测到某人洗手，那么你可以在某种程度上告诉人们“哦，你需要洗手20秒”，这可以作为一个很酷的应用。
- en: It can be used for music recommendations so Spotify YouTube music kind of gives
    like very。very good songs which you're listening to which are similar in content
    that you would perhaps like it can also give like really cool applications like
    say people have tried like detecting depression from audio or I could detect whether
    I'm coughing or not or I'm sneezing or not and these can be like good。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以用于音乐推荐，因此**Spotify**和**YouTube音乐**会推荐非常适合你正在听的、内容相似的好歌，这也可以带来很酷的应用，比如有人尝试从音频中检测抑郁，或者我可以检测我是否在咳嗽或打喷嚏，这些都是不错的应用。
- en: Medical device medical applications， which can be used along with the current
    diagnosis what doctor provides。So the question was basically for us was like how
    can we do like language modeling in a continuous audio domain and secondly。like
    how can we train models or how should we approach doing this？
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗设备的医疗应用，可以与医生提供的当前诊断结合使用。所以我们基本上的问题是，如何在连续音频领域进行语言建模，其次，如何训练模型或者我们应该如何处理这个问题？
- en: So so this kind of like recipe has become like very。very popular these days
    in terms of like how would you approach this problem it started with like open
    AI and to a certain extent deep proposing that in terms of like VQV models but
    it turns out like transformers love operating in discrete spaces as of now and
    what they kind of do is。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这种配方如今变得非常流行，关于你如何处理这个问题，起初是由**OpenAI**提出的，并且在某种程度上**深度学习**也提出了类似的**VQV模型**，但事实证明变压器现在更喜欢在离散空间中操作，它们所做的就是。
- en: As long as your representations are discrete， they are very。very good at modeling
    what's going to come next。So what people have been proposing as a workaround is。You
    could take up like your， your。Favorite embedding in some manner you could take
    a Vq V E embeddings or you could take a wave to work or in terms of video you
    can just do。Classic VGG or resonnet embeddings you can apply k means clustering
    to it and k means clustering would give you like discrete codes you do language
    modeling with those discrete codes。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 只要你的表示是离散的，它们就非常擅长建模接下来会发生什么。因此，人们提出的解决方案是，你可以以某种方式使用你喜欢的嵌入，你可以使用VQ-VAE嵌入，或者你可以使用Wave2Vec，或者在视频方面，你可以使用经典的VGG或ResNet嵌入，你可以对它应用K均值聚类，K均值聚类会给你离散代码，你可以用这些离散代码进行语言建模。
- en: And you predict the next code and in a way， if youre doing this。then you're
    kind of doing language modeling over audio and if you need to get back to the
    audio then。You already saw its wavenet that you can condition the wavenet model
    to give continuous output so you can use those codes to get back to the audio
    similar to what Jubox and Open I did。呃。So I'll quickly mention about like what
    vector quantization is。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你预测下一个代码，从某种意义上说，如果你这样做，那么你就是在音频上进行语言建模。如果你需要返回到音频，那么你已经看到了它的WaveNet，你可以对WaveNet模型进行条件处理以获得连续输出，因此你可以使用这些代码来回到音频，这与Jukebox和OpenAI的做法类似。呃。我会快速提到一下向量量化是什么。
- en: It's it's one of the most like underutilized algorithms to be honest and what
    it does is basically gives in a way discrete codes to continue assembling spaces
    so how how does it do it so you basically have。呃。Ebedding space let's say in 2D
    right here you define what what are the number of clusters you want to put each
    of them in you run K means and you would certainly get these patches of like where
    all of these embedding so what would be the representative embedding of a continuous
    embedding you can take all of those patches and you can see you can just number
    them or you can just like list them so in this case you can perhaps have like
    25 numbers or 20 numbers which are like in a way mapping from a continuous embedding
    to a discrete token。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 说实话，这是最少被利用的算法之一，它的基本功能是为连续的组装空间提供离散代码。那么它是如何做到的呢？你基本上有一个嵌入空间，假设在二维，这里你定义了想要放置每个集群的数量，你运行K均值，你肯定会得到这些补丁，这些补丁中嵌入了所有的内容。那么连续嵌入的代表性嵌入是什么？你可以拿所有这些补丁，你可以给它们编号，或者你可以列出它们。因此在这种情况下，你可能会有25个数字或20个数字，这些数字在某种程度上是从连续嵌入映射到离散标记。
- en: This is another example right here。so in our case what we did was we look up
    patches of spectrogram。which are basically like very small。Patches across time
    and then shared all across the frequency axis you take those patches you learn
    a embedding representation in our case it was just like three layer auto encodecoder
    fully connected encoders with three layers of decoders and have bottlene layer
    in between so that bottleneck layer basically is kind of similar to this kind
    of diagram in like say 64 dimensional space of 120 dimensional space you take
    up those bottle echos and then urine k means clustering on it certainly in a way
    you can find。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个例子。所以在我们的案例中，我们查找了声谱图的补丁，这基本上就像是非常小的时间段补丁，然后在频率轴上共享。你把这些补丁拿出来，学习嵌入表示，在我们的案例中，它只是像三层的全连接自编码器，带有三层解码器，中间有一个瓶颈层，这个瓶颈层基本上类似于像在64维空间或120维空间中的这种图示。你提取那些瓶颈回声，然后进行K均值聚类，从某种意义上说，你可以找到。
- en: Discrete codes for continuous embedding spaces or even continuous signals and
    since we know that transformers kind of love operating discrete spaces。we can
    just apply language botling now and then you can see what you can do。![](img/e397dafe2290c08be55c621b4940569f_30.png)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续嵌入空间甚至连续信号的离散代码，由于我们知道变压器喜欢在离散空间中操作，我们现在可以应用语言建模，你可以看看你能做什么。![](img/e397dafe2290c08be55c621b4940569f_30.png)
- en: So in our case we just have like very simple three layer fully connected auto
    encoder small patches the number of codes is important because if you have like
    too many codes then you are kind of just throwing in all kinds of noisy things
    i'll give an example of like。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们就有非常简单的三层全连接自编码器，补丁的数量很重要，因为如果你有太多的代码，那么你就是在扔各种噪声。我会举一个例子。
- en: Why the number of codes are important through some example and you have like
    two little codes。what you're in a way doing is you're removing all of the information
    which was relevant and you're just kind of like averaging them all out。嗯。I'll
    start with like so this idea first was proposed by Juke B， which did it for music。So
    you do the exact same thing what I talked about in a slightly different manner
    in a way that okay you cannot learn codes for like longer sequences。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么代码数量重要，通过一些例子来看，如果你有两个很少的代码，实际上你在做的是移除所有相关的信息，并且只是将它们平均化。嗯。我先从这个想法开始，这个想法最初是由Juke
    B提出的，他为音乐做了这个。所以你以稍微不同的方式做完全相同的事情，也就是说，好的，你不能为较长的序列学习代码。
- en: so you know in a way learn sequences which are just moving slowly and which
    are looking at only a certain amount of audio so we kind of encode this in if
    these discrete levels which are basically like all of these basically are codes
    so at every point I define like okay this audio had perhaps like code number 55
    and in the next level perhaps it at code number two and the very top perhaps it
    add code number 2000。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你知道，以某种方式学习的序列只是缓慢移动，并且仅查看一定量的音频，所以我们在这些离散级别中编码这些内容，这些基本上就像所有这些基本上都是代码。因此，在每个点上，我定义，比如说这个音频可能有代码编号55，而在下一个级别，它可能有代码编号2，在最上面可能有代码编号2000。
- en: So in a way I'm like discretizing the whole codes now what I do is I take up
    my favorite transform model。perhaps like a causal or autoregressive one and I
    say that okay given these codes try to predict what codes would come next and
    for sure transformers can do that so I would generate the codes in the future
    once I've generated the codes in the future I can say that okay this problem now
    is kind of like a text to speech problem right because I have like these discrete
    codes text to speech in a way is going from discrete letters to continuous audio
    so I would throw in like the fanciest which was wavenet and I would just get back
    the code and I would get the generated audio so this was in a way what I described
    that they take up a continuous audio they have these compressed codes which they
    encode using a CNN in this case the method doesn't matter you can throw in like
    the fanciest of like embeing or late representation。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在某种程度上，我像是在离散化整个代码。现在我做的是，我选择我最喜欢的变换模型，可能是因果或自回归的，然后我说，好的，给定这些代码，尝试预测接下来会出现什么代码，变换器确实可以做到这一点。因此，一旦我生成了未来的代码，我可以说，好的，这个问题现在有点像文本到语音的问题，因为我有这些离散代码，文本到语音在某种程度上是从离散字母到连续音频，所以我会投入像WaveNet这样的高级技术，然后我就会得到返回的代码，并生成音频。因此，这在某种程度上是我描述的，他们采用连续音频，拥有这些压缩代码，并使用CNN进行编码，在这种情况下，方法并不重要，你可以投入像EmBedding或潜在表示这样的高级技术。
- en: On those continuous code， you generate the patterns which are like what's going
    to happen next in the future and then decode back using a fancy wavenet state
    of the art audit。![](img/e397dafe2290c08be55c621b4940569f_32.png)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些连续代码上，你生成的模式就像是未来将要发生的事情，然后使用先进的WaveNet进行解码。![](img/e397dafe2290c08be55c621b4940569f_32.png)
- en: So so this was what they were doing for music synthesis what we said was you
    know like yeah this is good this can generate like good amount of music。but what
    can can can these balls be used for generating。Like good representation of the
    current audio and the goal there was like can language models learn representation
    which can just encapsulate whatever we are kind of like giving as an input signal
    so in this case what we tried after that was kind of like you kind of do exactly
    kind of similar ideas but instead of doing like on Wikiva end toend learned encodecodings
    we just apply vanillaular k means clustering similar to what I described earlier
    we do on spectrogram patches so you take up these spectrograms of audio and you
    just divide them into like very small chunks learn autoendcoder encodecodings
    for each of those chunks run means clustering and this case like let's say I am
    learning 16 codes represent the continuous audio in terms of the 16 codes have
    transformer which can perhaps predict the next code and if I keep on getting bit。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他们在音乐合成中做的就是这样，我们说这很好，可以生成大量音乐。但这些球能用于生成什么呢？就像是能否让语言模型学习到能封装我们输入信号的良好表示。所以在这种情况下，我们尝试的是类似的想法，但不是做端到端学习的编码，我们只应用了普通的K均值聚类，类似于我之前描述的，我们在谱图块上进行操作，因此你获取这些音频的谱图，然后把它们分成非常小的块，为每个块学习自编码器的编码，进行均值聚类。在这种情况下，比如我正在学习16个编码，以16个编码表示连续音频，有一个变压器可能预测下一个编码，如果我继续获取一些数据。
- en: And better at predicting what's going to happen next， then in this linear layer。I
    should be encapsulating。What's important or what's？
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个线性层中，我应该封装重要的或什么内容。
- en: A good summary of what has happened in the past。![](img/e397dafe2290c08be55c621b4940569f_34.png)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这是过去发生的事情的一个很好的总结。![](img/e397dafe2290c08be55c621b4940569f_34.png)
- en: 呃。So that was kind of like our intuition behind trying this。![](img/e397dafe2290c08be55c621b4940569f_36.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 呃。所以这就是我们尝试这个的直觉。![](img/e397dafe2290c08be55c621b4940569f_36.png)
- en: And as I explained like the number of codes play a very important role。you can
    see here these are just two piano nodes switching one after the other。if I just
    have like say 16 number of codes it just have happens to have just a single line
    of encoding。a single code assigned to all of this， whereas if I'm assigning like
    more codes than it becomes kind of like a fine grade prediction where I'm actually
    able to get what the individual notes are。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所解释的，编码数量起着非常重要的作用。你可以看到这里这只是两个钢琴音符相继切换。如果我只有16个编码，它就只有一条编码，一个编码分配给所有这些，而如果我分配更多编码，它就变成了一种细致的预测，我实际上能够获取到每个音符。
- en: '![](img/e397dafe2290c08be55c621b4940569f_38.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_38.png)'
- en: Recently， Facebook also said you know like okay they just had a different name
    to the whole thing which is would we can just call this as text class NLP also
    in a sense that okay you can do NLP without having access to text with the idea
    is very very similar you have an encoder which is exactly similar to say what
    openaiI was using you have a VQba wave to V or whatever you want to do you can
    apply k means clusterstering to it we apply language models to it and instead
    of a decoder being vnet they just have a decoder which is like a different version
    of text to speech which is like toptro in this case so as you can see like these
    are all like same wine and very different bottle but the core idea is almost exactly
    the same。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Facebook也表示，他们对整体的说法有了不同的名称，可以称之为文本类NLP，实际上在某种意义上可以说，你可以在没有文本的情况下进行NLP，理念非常相似，你有一个编码器，与OpenAI使用的完全相似，你有VQ波到V，或者你想做的任何事情，都可以对其应用K均值聚类，我们将语言模型应用于此，解码器不是VNet，而是类似于文本到语音的不同版本，像是Toptro。因此，如你所见，这些都是同一酒但瓶子不同，但核心理念几乎完全相同。
- en: '![](img/e397dafe2290c08be55c621b4940569f_40.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_40.png)'
- en: So this was like it created a huge pro of like oh this is going to change an
    LPN。but this is very very similar to what people have been doing the past。![](img/e397dafe2290c08be55c621b4940569f_42.png)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像创造了一个巨大的优势，哦，这将改变LPN，但这与过去人们所做的非常相似。![](img/e397dafe2290c08be55c621b4940569f_42.png)
- en: So I've already explained what what this was so in our case， we just try to
    predict。What's going to happen next， given the previous context and use that representation
    similar to every single like one short learning or zero short learning based method。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经解释了这是什么，所以在我们的案例中，我们只是尝试预测下一步会发生什么，给定前文上下文，并使用这种表示法，类似于每一个短期学习或零短期学习的基于方法。
- en: I also explain like why the number of codes are important like if you have too
    small then you're just throwing up a lot of information if you have too large
    then you don't put in like it is no longer robust to noise。![](img/e397dafe2290c08be55c621b4940569f_44.png)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我还解释了为什么代码的数量很重要，如果太少，你就扔掉了很多信息；如果太多，它就不再对噪声稳健。![](img/e397dafe2290c08be55c621b4940569f_44.png)
- en: 哦。So this was our setup and before I jump in， I should add one of the tweets
    which I saw from one of the most prominent researchers at DeepM。which is basically
    like a lot of times it is very very easy to bump up numbers， you know。like I can
    have these details just not present in my paper。which actually help a lot in terms
    of like improving the performance and kind of sometimes don't take into account。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 哦。所以这是我们的设置，在我跳进去之前，我应该补充一个我从DeepM的一位著名研究人员那里看到的推文，基本上很多时候提高数字非常简单，你知道。比如我可以不在论文中提供这些细节，这实际上在提高性能方面有很大帮助，有时不被考虑。
- en: What the actual model is incorerating or what the model is contributing versus
    what the actual these tricks for training are incorerrating so for most of these
    methods what we have tried to see is we try to keep almost exactly the same approach。no
    data augmentation no fancy label smoothing or moving average of weights or decay
    or whatever you just have similar based recipes to see how well we are doing。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实际模型所包含的或模型所贡献的，与这些训练技巧实际包含的东西相比，对于大多数这些方法，我们尝试保持几乎完全相同的方法。没有数据增强，没有华丽的标签平滑或权重的移动平均或衰减，无论如何，你只是有类似的基础配方来看看我们的表现如何。
- en: So for this case the goal was to say that how well our models with respect to
    a purely supervised approach and how well it does with respect to a similar unsupervised
    approach so in the first case the model and all of the weights access to all of
    the labels which is just shown as VGD supervised which is basically you take up
    audio understanding data set and you see how well you are doing an accuracy metrics
    so that was the first one in the second one we applied Simclair which was proposed
    by Jeff In in which you can take up these multiple augmentations of the same input
    you can have like patches removed you can blur the signal you can flip the signal
    you learn embedded out of the last layer without access to the labels and then
    just have a linear head to predict what's happening by using that we got a 55%
    accuracy use the exact same thing we transform us you don't have access to labels
    you just run them while just predict the next code you take the linear layer apply
    the same linear head。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，目标是评估我们的模型与纯监督方法相比表现如何，以及与类似的无监督方法相比表现如何。在第一个案例中，模型及其所有权重访问所有标签，这被显示为VGD监督，基本上是你获取音频理解数据集，并查看你的准确性指标表现如何。这是第一个；在第二个中，我们应用了Jeff
    In提出的Simclair，你可以对相同输入进行多种增强处理，可以去除补丁、模糊信号、翻转信号，你从最后一层学习嵌入，而不访问标签，然后仅使用线性头来预测发生了什么，使用这种方法我们得到了55%的准确率。
- en: And try to predict what's happening inside。And with that， we got 60% accuracy。So
    even though the results are not good， but the fact is the neural networks actually。Are
    very very good at like getting better and better with throwing of huge amounts
    of data right so there is still 10% back gap between like purely supervised and
    purely unsupervised but then that's going to improve with throwing a lot of data
    to these models because it doesn't have access to any label as person so this
    is a famous paper by the analystly and Nelson also Morgan at Berkeley in which
    they actually showed way back in 1999 as to why。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后尝试预测内部发生了什么。通过这些，我们得到了60%的准确率。尽管结果并不好，但事实上神经网络实际上在处理大量数据时非常出色，因此在纯监督和纯无监督之间仍有10%的差距，但随着大量数据的投入，这种情况会改善，因为它没有任何标签的访问权限。这是分析师Nelson和Morgan在伯克利的著名论文，他们早在1999年就展示了原因。
- en: '![](img/e397dafe2290c08be55c621b4940569f_46.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_46.png)'
- en: Like size matters， four deep neural networks and also the number of data points
    which is present so as they kept on increasing the size of the data set and the
    parameters。they kept on getting lower and lower whatever error rates and this
    has been true across any of the data set and that's why the whole excitement is
    about unsupervised learning。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 像大小这种因素对于深度神经网络和数据点数量很重要，因此随着数据集和参数大小的不断增加，它们的错误率越来越低，这在任何数据集中都是如此，这就是无监督学习令人兴奋的原因。
- en: So this wast a way flavor of like how can we do like language modeling and unsupervised
    learning on audio for continuous signals？
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个关于如何进行语言建模和无监督学习在音频连续信号上的一种方法风味。
- en: For the third up plot， I' just quickly mention。Ideas which are very similar
    to what you would have seen in vision transformers。but with the caveat that how
    can we use some sort of like signal processing to improve these performance a
    further？
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第三个上图，我只想快速提及。与您在视觉变换器中看到的非常相似的想法，但有一个警告：我们如何使用某种信号处理来进一步提高这些性能？
- en: So the basic approach still remains the same exactly as what you would have
    seen in vision transformers。You have a signal of interest which you want to classify
    here they are raw wave font instead of images。the goal is to predict what's there
    inside of it right。And also we don't have any conclusions。we don't have any other
    tricks which we were using before。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基本方法仍然与您在视觉变换器中看到的完全相同。您有一个感兴趣的信号，您想在这里对其进行分类，它们是原始波形而不是图像。目标是预测里面有什么，对吧？我们也没有任何结论，没有之前使用的其他技巧。
- en: all we have to do is they can transform us stem solve this particular problem。![](img/e397dafe2290c08be55c621b4940569f_48.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所需要做的就是，他们可以将我们转变，解决这个特定的问题。![](img/e397dafe2290c08be55c621b4940569f_48.png)
- en: So for the data set and the whole setup are still the same。no data augmentation
    and no other forms of these tricks， you are given like 40。000 snippets for training
    and 10，000 for validation。Our job is to predict as good as possible as to what's
    there in the audio this problem is very similar to the sound which you heard and
    the video which you saw that given a patch you ought to predict given a spectrogram
    patch you have to predict what's there inside of it。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据集和整个设置仍然是相同的，没有数据增强，也没有其他这些技巧，您会获得40,000个片段用于训练，10,000个用于验证。我们的工作是尽可能准确地预测音频中存在的内容，这个问题与您听到的声音和您看到的视频非常相似，给定一个补丁，您必须预测给定的频谱图补丁内部的内容。
- en: '![](img/e397dafe2290c08be55c621b4940569f_50.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_50.png)'
- en: 呃。![](img/e397dafe2290c08be55c621b4940569f_52.png)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 呃。![](img/e397dafe2290c08be55c621b4940569f_52.png)
- en: We kind of do one step further than what。What's just like a simple transformer
    model in a sense that we try to see whether some sort of like hierarchy over transform
    emtics would help us in any manner。so for that we use like wavelelet decomposition
    on the intermediate transformer ems so what is like a waveleng decomposition。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在某种意义上比简单的变换模型更进一步，尝试查看某种层次结构是否能以任何方式帮助我们。因此，我们在中间变换嵌入上使用了小波分解，那么什么是小波分解呢？
- en: '![](img/e397dafe2290c08be55c621b4940569f_54.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_54.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_55.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_55.png)'
- en: In very naive terms， it can be like a way of decomposing the intermediate embeddings
    into。intermediate embedding in a sense that we are kind of putting these highways
    of like some embeddings are moving very slowly and some embeddings are moving
    very fast and some embeddings are retained exactly at the rate of what the original
    signal was and why this is important because you can think about that at every
    intermediate state you are in a whale learning some sort of hierarchy in the model
    so if if I look at what we do with the like waveleair decomposition before and
    after let's say like you had time across this and you had the embedding size across
    this and this whole patch was your output of say the end layer of the transformer
    what I say now is okay I would just。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 用非常简单的术语来说，这就像是将中间嵌入分解成的方式。在某种意义上，我们将这些嵌入的高速公路分成一些移动得很慢的嵌入和一些移动得很快的嵌入，还有一些嵌入的保留速度恰好与原始信号相同。这一点很重要，因为你可以想到，在每一个中间状态中，你都在学习模型中的某种层次结构。因此，如果我查看我们在小波分解前后的工作，比如你在时间轴上有这个，而在嵌入大小上有这个，整个补丁是你所说的变压器最后一层的输出。那么我现在说的是，好吧，我只是。
- en: '![](img/e397dafe2290c08be55c621b4940569f_57.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_57.png)'
- en: Have a mapping from this to the mapping of my interest using waveleth decomposition
    in which for half of the samples I just created the exact same embedding as what
    was learned by the transform model in the next half I would start combining two
    at a time so in a way i'm learning this sort of like a tree structure within a
    single layer of the transformer embedding and for now the wavelength or the B
    function which I use a simple averaging so let's say from all of the embedding
    layers in between I just need to have like one one embedding which is not moving
    at all which is just representative of whatever is there。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里映射到我感兴趣的映射，使用波长分解，其中对于一半的样本，我刚刚创建了与变换模型学习的完全相同的嵌入，在后半部分我会开始每次组合两个，所以在某种程度上，我正在学习这种类似于树结构的东西，在变换器嵌入的单层中，对于现在使用的波长或B函数，我简单地进行了平均，所以假设在所有嵌入层之间，我只需要有一个完全不动的嵌入，代表存在的任何东西。
- en: Of the whole latent space in that n layer， then in the next next layer I would
    just use like two at a time and then I would use like four at a time。Until I reached
    the exact resolution as what I had。Doing this operation doesn't add any para as
    whatsoever you're just like defining what your basis function would be or what
    a wavelength function would be in this case it is a hard waveleth and I start
    combining them and I kind of like learned a hierarchy at every single layer of
    the transformers and this kind of like。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个潜在空间的n层中，在接下来的层中，我会每次使用两个，然后每次使用四个。直到我达到与之前相同的精确分辨率。进行这个操作不会增加任何参数，你只是在定义你的基函数是什么，或者在这种情况下，波长函数是什么，这是一个硬波长函数，我开始将它们组合在一起，并且我在每一层的变换器中都学习到了一种层次结构。
- en: '![](img/e397dafe2290c08be55c621b4940569f_59.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_59.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_60.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_60.png)'
- en: Improved our performance significantly as compared to not using them with addition
    of no extra parameters。And I'll come to the results later also so so this is how
    the whole approach looks like you have a front end the front end is basically
    a single layer of 2000 neurons followed by dense layer of 64 neurons which is
    just to make sure to confirm it to the intermediate transformer readings。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与不使用它们并且没有额外参数的情况下相比，这显著改善了我们的性能。我稍后会提到结果，所以这就是整个方法的样子，你有一个前端，前端基本上是2000个神经元的一层，后面跟着64个神经元的稠密层，目的是确认与中间变换器读取一致。
- en: let's say for the transformers I define the bidding size to be 64 then that's
    the dimension which I'm like mapping them to so I take a broadway form I patch
    it in very small patches similar to how you do in vision transformers I would
    just have a single layer of 2000 neurons followed by dense layer of 64 neurons
    with the hope that the first layer kind of is learning like a Fourriier B function
    which should be adaptable according to what I'm learning after that I keep on
    doing this over and over again like I don't have a classification head or anything
    like that I keep on。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 假设对于变换器，我定义的批次大小为64，那么这是我映射到的维度，所以我从宽带形式开始，将其分成非常小的补丁，类似于你在视觉变换器中所做的，我将有一层2000个神经元，后面接着一个64个神经元的稠密层，希望第一层能够像一个傅里叶B函数一样学习，这应该能根据我的学习进行调整。之后我一直重复这个过程，我没有分类头或其他东西。
- en: Adding multiple stacks of transformers after that but。And then I have two approaches
    of like what I can do in terms of like adaptation。I can do average pooling across
    time of these intermediate emddings because that is the idea is very similar to
    what we do in classical vision that each of the embleddings are looking at much
    much broader output in the subsequent layers or I could do w decomposition so
    what I do is that I take all of these embleddings and I define these highways
    so some of the emddings are moving fast。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后添加多个变换器堆栈。我有两种适应的方法。我可以对这些中间嵌入进行时间上的平均池化，因为这个想法与我们在经典视觉中所做的非常相似，每个嵌入在后续层中都在观察更广泛的输出，或者我可以进行分解，所以我把所有这些嵌入取出并定义这些高速公路，其中一些嵌入移动得很快。
- en: some of them are moving very slow and some are retained at the exact same resolution
    as what the transform is learning。And then I keep doing this over and over again，
    I have a dense layer， I have my soft max or sigmoid。whatever is my classification
    head， so so this is kind of what the approach looks like。We compare it with all
    of the traditional vision based architecture。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有些模型运行非常慢，有些则保持与变换器学习的相同分辨率。然后我不断重复这个过程，我有一个稠密层，我有我的 Softmax 或 Sigmoid，无论我的分类头是什么，所以这就是这种方法的样子。我们将其与所有传统的基于视觉的架构进行了比较。
- en: so the vision based models have been very good and the performance have been
    like similar in understanding audio also so we compare all of those models in
    terms like mean average precision。And we see that even the tiniest models of transformers
    were just like surpassing all of the state of the art CNN models。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基于视觉的模型表现非常好，在理解音频方面也表现相似。因此，我们在平均精度等方面比较了所有这些模型。我们发现即使是最小的变换器模型也超越了所有最先进的
    CNN 模型。
- en: which was a very good sign， then we started to bump up。okay the larger model
    should keep on improving the performance and with the multi-scale models as well
    as with the pooling layers。they improve the performance even further， which was
    kind of very surprising to us because the number of parameters are very small。these
    are very tiny architectures yet they are surpassing like things like even densenet
    which are like huge models with a lot of millions of parameters。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常好，所以我们开始提升。好吧，更大的模型应该继续改善性能，同时多尺度模型和池化层也进一步提升了性能，这让我们感到惊讶，因为参数数量非常少。这些架构非常小，但它们却超越了像
    Densenet 这样拥有数百万参数的大型模型。
- en: '![](img/e397dafe2290c08be55c621b4940569f_62.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_62.png)'
- en: So after that we said and I'm going to conclude quickly after that we said that
    okay。this is looking pretty cool， are the what actually is the transformer or
    the first layer learning right？
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所以之后我们说，我将快速总结，之后我们说，好吧。这看起来很酷，变换器的第一层实际学习了什么吗？
- en: '![](img/e397dafe2290c08be55c621b4940569f_64.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_64.png)'
- en: In order to。Make this plot what we said was okay， if you were to take a classic
    Fourier transform。Then if this axis is kind of like like frequency， this axis
    is the number of filters and this axis is the frequency。Then in a way。It should
    be connecting all of the points in a linear line and this is akin to the number
    of points in the 50 so how many points I'm defining here。if I'm defining 2000
    points here， then I would have like 2048 sineoidal B functions which are going
    from lower frequency to the most highest frequency。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制这个图，我们说好吧，如果你进行经典的傅里叶变换。那么如果这个轴类似于频率，这个轴是滤波器的数量，而这个轴是频率。那么在某种程度上，它应该连接所有点成一条直线，这类似于
    50 中的点数，所以我在这里定义多少点。如果我在这里定义 2000 个点，那么我将有大约 2048 个从低频到最高频的正弦 B 函数。
- en: We said okay we'll do the exact same thing but now with filters so we have frequency
    along y axis and the number of points in my X axis and if it was a classic Fourier
    transform then it would be connecting right as a linear line but what we did was
    we take up the front end which is learned by transformer take its Fourier transform
    solved according to its center frequency as to what frequency it is activating
    the most and then keep on stacking them when we did this two problems we saw that
    we are kind of learning a different time frequency representation which is specific
    to a predict problem if I'm trying to understand what's there in the content of
    the audio I learn a representation which is very different than Fourier transform
    which should have in a straight line which is kind of like a curld exponential
    line like this and if I do like a polyphonnic pitch estimation。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说好吧，我们将做完全相同的事情，但现在加上滤波器，所以我们在 y 轴上有频率，在 X 轴上有点数，如果这是经典的傅里叶变换，那么它应该连接成一条直线，但我们所做的是，我们取出前端，通过变换器学习的，进行傅里叶变换，并根据其中心频率解决，即它最活跃的频率，然后不断堆叠它们。当我们这样做时，我们看到我们正在学习一种不同的时间频率表示，这对于预测问题是特定的。如果我试图理解音频内容，我学习的表示与傅里叶变换非常不同，它应该是一条直线，而更像是这样的卷曲指数线。如果我做类似多音调音高估计的事情。
- en: I learn a very different front end which is adapting to that particular problem
    so this was like very exciting to us because like making computers here in a way
    in which they're adapting their ears according to a particular problem is a very
    cool idea second thing is we actually saw each of the filters as to what they
    were doing and these are basically just like single slices like this so this is
    what we would have been learned as a front end neurons we take up each of the
    neurons and we just plot them and for plotting this we basically take its free
    transform and then saw them according to where the center of frequency is when
    we just saw the neurons as to what they were learning in the front end we saw
    that it is learning properties which are very very closely matching with the traditional
    signal processing so you would have something like an onset detector learned right
    here you learning windowing function in a way it is learning to。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我学习了一种非常不同的前端，它适应于这个特定问题，因此对我们来说这非常令人兴奋，因为在某种程度上，让计算机根据特定问题调整其“耳朵”是个非常酷的想法。第二件事是我们实际上观察了每个滤波器的作用，这基本上就像单个切片一样，所以这就是我们所学习的前端神经元，我们取出每个神经元并进行绘图。为此，我们基本上采用其傅里叶变换，然后根据频率中心进行分类。当我们观察前端神经元所学习的内容时，我们发现它学习的属性与传统信号处理非常相似，因此你会看到像一个起始检测器在这里学习窗口函数。
- en: '![](img/e397dafe2290c08be55c621b4940569f_66.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_66.png)'
- en: Have a kernel which is best for a time frequency representation what people
    have been using in signal processinging which is like a hamming or a hanging window
    we are learning these pure sinusoids which are responsible for activating a particular
    frequency so you can see the richness as compared to like having a fixed purely
    sinusoidal PCs function right here。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个最佳的时间频率表示内核，许多人在信号处理中使用，这类似于汉明窗或悬挂窗，我们正在学习这些纯正的正弦波，它们负责激活特定频率，因此你可以看到它的丰富性，与固定的纯正弦波PC功能相比。
- en: So this was what we had。Done， and then to share the final thoughts。I'll conclude
    by saying that okay， transformers are proving to be a major advancement in AI
    research across the fields。And。It seems like they're solving everything for now
    and hopefully this is not the end and we should keep an eye out on something which
    would change and have impact。which is more than。What transformers have put？And
    who knows what's going to come next？Yeah。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所做的，然后分享最终的想法。我将总结说，变换器在各个领域的AI研究中正证明是重大的进步。而且，看起来他们现在正在解决一切，希望这不是终点，我们应该关注可能会改变和产生影响的东西，这超出了变换器的贡献。谁知道接下来会发生什么呢？是的。
- en: so by that I'll just conclude and I'll be happy to take questions。Thank you
    Praique that was a really good talk and you provided some really good insights
    about how like Transers work for the audio case and yeah thank you for the talk
    and now I would invite questions from from the class students let me just stop
    the recording。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将总结一下，并乐意回答问题。谢谢Praique，这是一次非常好的演讲，你提供了一些关于变换器在音频案例中如何工作的深刻见解，感谢你的演讲，现在我邀请班级的学生提问，让我停止录音。
- en: '![](img/e397dafe2290c08be55c621b4940569f_68.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_68.png)'
- en: '![](img/e397dafe2290c08be55c621b4940569f_69.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e397dafe2290c08be55c621b4940569f_69.png)'
