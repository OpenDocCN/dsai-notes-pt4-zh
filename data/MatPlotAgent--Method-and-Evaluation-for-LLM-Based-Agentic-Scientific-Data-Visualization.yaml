- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:53:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:53:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific Data Visualization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MatPlotAgent：基于LLM的智能科学数据可视化方法与评估
- en: 来源：[https://arxiv.org/html/2402.11453/](https://arxiv.org/html/2402.11453/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2402.11453/](https://arxiv.org/html/2402.11453/)
- en: Zhiyu Yang${}^{*2}$  Zihan Zhou${}^{3}$  Shuo Wang${}^{{\dagger}1}$  Xin Cong${}^{1}$
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhiyu Yang${}^{*2}$  Zihan Zhou${}^{3}$  Shuo Wang${}^{{\dagger}1}$  Xin Cong${}^{1}$
- en: Xu Han${}^{1}$  Yukun Yan${}^{1}$  Zhenghao Liu${}^{4}$  Zhixing Tan${}^{5}$
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xu Han${}^{1}$  Yukun Yan${}^{1}$  Zhenghao Liu${}^{4}$  Zhixing Tan${}^{5}$
- en: Pengyuan Liu${}^{2}$  Dong Yu${}^{2}$  Zhiyuan Liu${}^{1}$  Xiaodong Shi${}^{3}$
    Maosong Sun${}^{1}$
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Pengyuan Liu${}^{2}$  Dong Yu${}^{2}$  Zhiyuan Liu${}^{1}$  Xiaodong Shi${}^{3}$
    Maosong Sun${}^{1}$
- en: ${}^{1}$Tsinghua University  ${}^{2}$Beijing Language and Culture University
     ${}^{3}$Xiamen University
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{1}$清华大学  ${}^{2}$北京语言大学  ${}^{3}$厦门大学
- en: ${}^{4}$Northeastern University, China  ${}^{5}$Zhongguancun Laboratory, Beijing,
    China   Equal contribution.  Corresponding authors.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ${}^{4}$东北大学，中国  ${}^{5}$中关村实验室，北京，中国   平等贡献。  对应作者。
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Scientific data visualization plays a crucial role in research by enabling
    the direct display of complex information and assisting researchers in identifying
    implicit patterns. Despite its importance, the use of Large Language Models (LLMs)
    for scientific data visualization remains rather unexplored. In this study, we
    introduce MatPlotAgent, an efficient model-agnostic LLM agent framework designed
    to automate scientific data visualization tasks. Leveraging the capabilities of
    both code LLMs and multi-modal LLMs, MatPlotAgent consists of three core modules:
    query understanding, code generation with iterative debugging, and a visual feedback
    mechanism for error correction. To address the lack of benchmarks in this field,
    we present MatPlotBench, a high-quality benchmark consisting of 100 human-verified
    test cases. Additionally, we introduce a scoring approach that utilizes GPT-4V
    for automatic evaluation. Experimental results demonstrate that MatPlotAgent can
    improve the performance of various LLMs, including both commercial and open-source
    models. Furthermore, the proposed evaluation method shows a strong correlation
    with human-annotated scores.¹¹1  MatPlotAgent and MatPlotBench are be publicly
    available at [https://github.com/thunlp/MatPlotAgent](https://github.com/thunlp/MatPlotAgent).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 科学数据可视化在研究中起着至关重要的作用，它能够直接展示复杂信息，并帮助研究人员识别隐含的模式。尽管其重要性不言而喻，然而将大语言模型（LLMs）应用于科学数据可视化的研究仍然较为匮乏。在本研究中，我们提出了MatPlotAgent，一个高效的与模型无关的LLM代理框架，旨在自动化科学数据可视化任务。MatPlotAgent结合了代码LLM和多模态LLM的能力，包含三个核心模块：查询理解、代码生成与迭代调试、以及用于错误修正的视觉反馈机制。为了解决该领域缺乏基准测试的问题，我们提出了MatPlotBench，一个包含100个人工验证测试案例的高质量基准。此外，我们还提出了一种评分方法，利用GPT-4V进行自动评估。实验结果表明，MatPlotAgent能够提升各种LLM的表现，包括商业和开源模型。此外，所提的评估方法与人工标注评分之间存在较强的相关性。¹¹1  MatPlotAgent和MatPlotBench将公开发布，地址为[https://github.com/thunlp/MatPlotAgent](https://github.com/thunlp/MatPlotAgent)。
- en: '![[Uncaptioned image]](img/8a7bfd784061ab086ea8ad974d9fef94.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![[未标注图片]](img/8a7bfd784061ab086ea8ad974d9fef94.png)'
- en: 'MatPlotAgent: Method and Evaluation for'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: MatPlotAgent：方法与评估
- en: LLM-Based Agentic Scientific Data Visualization
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的智能科学数据可视化
- en: 'Zhiyu Yang${}^{*2}$  Zihan Zhou^†^†thanks:   Equal contribution.${}^{3}$  Shuo
    Wang${}^{{\dagger}1}$  Xin Cong${}^{1}$ Xu Han${}^{1}$  Yukun Yan${}^{1}$  Zhenghao
    Liu${}^{4}$  Zhixing Tan${}^{5}$ Pengyuan Liu${}^{2}$  Dong Yu${}^{2}$  Zhiyuan
    Liu^†^†thanks:   Corresponding authors.${}^{1}$  Xiaodong Shi${}^{3}$ Maosong
    Sun${}^{1}$ ${}^{1}$Tsinghua University  ${}^{2}$Beijing Language and Culture
    University  ${}^{3}$Xiamen University ${}^{4}$Northeastern University, China  ${}^{5}$Zhongguancun
    Laboratory, Beijing, China'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Zhiyu Yang${}^{*2}$  Zihan Zhou^†^†感谢：平等贡献.${}^{3}$  Shuo Wang${}^{{\dagger}1}$  Xin
    Cong${}^{1}$  Xu Han${}^{1}$  Yukun Yan${}^{1}$  Zhenghao Liu${}^{4}$  Zhixing
    Tan${}^{5}$  Pengyuan Liu${}^{2}$  Dong Yu${}^{2}$  Zhiyuan Liu^†^†感谢：对应作者.${}^{1}$  Xiaodong
    Shi${}^{3}$  Maosong Sun${}^{1}$  ${}^{1}$清华大学  ${}^{2}$北京语言大学  ${}^{3}$厦门大学  ${}^{4}$东北大学，中国  ${}^{5}$中关村实验室，北京，中国
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: A picture is worth a thousand words. Data visualization is an essential process
    in scientific research, facilitating the more direct conveyance of complex information
    and aiding researchers in uncovering implicit patterns. There are many advanced
    toolkits, such as Matplotlib²²2[https://matplotlib.org](https://matplotlib.org)
    and Origin³³3[https://www.originlab.com](https://www.originlab.com), that can
    help researchers plot various types of figures for complex data distributions.
    However, transforming raw data into informative and easy-to-understand visualizations
    is still time-consuming and labor-intensive. Before the invention of large language
    models (LLMs) OpenAI ([2023](https://arxiv.org/html/2402.11453v3#bib.bib20)),
    automating this process with AI models is almost impossible.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一张图片胜过千言万语。数据可视化是科学研究中一个至关重要的过程，有助于更直接地传达复杂信息，并帮助研究人员揭示潜在的模式。当前有许多先进的工具包，如Matplotlib²²2[https://matplotlib.org](https://matplotlib.org)和Origin³³3[https://www.originlab.com](https://www.originlab.com)，它们能够帮助研究人员绘制各种类型的图形，以展示复杂的数据分布。然而，将原始数据转化为信息丰富且易于理解的可视化图形，依然是一个耗时且劳动密集的过程。在大语言模型（LLMs）发明之前（如OpenAI，[2023](https://arxiv.org/html/2402.11453v3#bib.bib20)），利用AI模型自动化这一过程几乎是不可能的。
- en: '![Refer to caption](img/f9305eed32327b0f8c6905cea4535ddb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![请参见图例](img/f9305eed32327b0f8c6905cea4535ddb.png)'
- en: 'Figure 1: Examples in the proposed MatPlotBench. Given the raw data and user
    queries, the AI agent is expected to generate a figure accordingly. We only display
    partial raw data and user queries due to space limitations.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：提出的MatPlotBench中的示例。给定原始数据和用户查询，AI代理预计会根据这些生成图形。由于空间限制，我们只展示了部分原始数据和用户查询。
- en: With large-scale parameters and extensive training data, LLMs have demonstrated
    remarkable capabilities in a wide range of complex tasks, including reasoning Wei
    et al. ([2022](https://arxiv.org/html/2402.11453v3#bib.bib34)); Kojima et al.
    ([2022a](https://arxiv.org/html/2402.11453v3#bib.bib9)); Yao et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib39)),
    mathematics Yu et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib41));
    Luo et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib17)); Azerbayev
    et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib2)); Shao et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib31))
    and coding Rozière et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib28));
    Luo et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib18)); Guo et al.
    ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8)); Wei et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35)).
    This breakthrough has unlocked new opportunities for utilizing LLMs as autonomous
    agents in a diverse range of practical scenarios, such as web browsing Nakano
    et al. ([2021](https://arxiv.org/html/2402.11453v3#bib.bib19)); Yao et al. ([2022](https://arxiv.org/html/2402.11453v3#bib.bib38));
    Qin et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib24)); Zhou et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib42)); Deng et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib6));
    Yao et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib40)); Xie et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib36)), social simulations Park
    et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib21)); Xu et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib37));
    Chen et al. ([2024a](https://arxiv.org/html/2402.11453v3#bib.bib4)); Wang et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib33)), tool utilization Qin
    et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib25)); Schick et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib30)); Liu et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib15));
    Li et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib13)); Lu et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib16)); Qian et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib23));
    Shinn et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib32)), and software
    development Qian et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib22)).
    Using LLMs to enhance human productivity in specialized areas is now a key research
    focus with great potential.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大规模参数和广泛的训练数据，LLM（大语言模型）在广泛的复杂任务中展示了卓越的能力，包括推理 魏等人（[2022](https://arxiv.org/html/2402.11453v3#bib.bib34)）；小岛等人（[2022a](https://arxiv.org/html/2402.11453v3#bib.bib9)）；姚等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib39)），数学 俞等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib41)）；罗等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib17)）；阿泽尔巴耶夫等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib2)）；邵等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib31)）和编码 罗济尔等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib28)）；罗等人（[2023b](https://arxiv.org/html/2402.11453v3#bib.bib18)）；郭等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib8)）；魏等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib35)）。这一突破为将LLM作为自主智能体应用于各种实际场景提供了新的机会，如网页浏览 中野等人（[2021](https://arxiv.org/html/2402.11453v3#bib.bib19)）；姚等人（[2022](https://arxiv.org/html/2402.11453v3#bib.bib38)）；秦等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib24)）；周等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib42)）；邓等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib6)）；姚等人（[2023b](https://arxiv.org/html/2402.11453v3#bib.bib40)）；谢等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib36)），社会模拟 朴等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib21)）；徐等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib37)）；陈等人（[2024a](https://arxiv.org/html/2402.11453v3#bib.bib4)）；王等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib33)），工具利用 秦等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib25)）；席克等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib30)）；刘等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib15)）；李等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib13)）；卢等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib16)）；钱等人（[2023b](https://arxiv.org/html/2402.11453v3#bib.bib23)）；申等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib32)），以及软件开发 钱等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib22)）。使用LLM提升人类在特定领域的生产力如今已成为一个重要的研究重点，且潜力巨大。
- en: Recent advancements in LLM-based agents inspire us to explore the utilization
    of LLMs for scientific data visualization, a realm that remains rather unexplored
    in existing studies. A closely related line of research is text-to-image generation Ramesh
    et al. ([2021](https://arxiv.org/html/2402.11453v3#bib.bib26)); Saharia et al.
    ([2022](https://arxiv.org/html/2402.11453v3#bib.bib29)), where diffusion models Rombach
    et al. ([2022](https://arxiv.org/html/2402.11453v3#bib.bib27)) have shown great
    potential in generating various types of images. However, existing text-to-image
    generation methods predominantly focus on artistic expression, potentially misaligning
    with the needs of scientific data visualization, where clarity and precision in
    conveying information are the most important principles. This work aims to automatically
    generate figures with precise information.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大语言模型（LLM）的代理的最新进展激发了我们探索LLM在科学数据可视化中的应用，这一领域在现有研究中仍然较为未被开发。一个紧密相关的研究方向是文本到图像生成，Ramesh等人（[2021](https://arxiv.org/html/2402.11453v3#bib.bib26)）；Saharia等人（[2022](https://arxiv.org/html/2402.11453v3#bib.bib29)），在其中，扩散模型Rombach等人（[2022](https://arxiv.org/html/2402.11453v3#bib.bib27)）已显示出在生成各种类型图像方面的巨大潜力。然而，现有的文本到图像生成方法主要集中于艺术表现，可能与科学数据可视化的需求不符，因为在科学数据可视化中，传达信息的清晰性和精确性是最重要的原则。本工作旨在自动生成具有精确信息的图形。
- en: 'We propose leveraging modern code LLMs and multi-modal LLMs to develop scientific
    data visualization agents that can significantly enhance human efficiency. The
    resulting MatPlotAgent⁴⁴4This name is in homage to the well-known Matplotlib.
    is comprised of three modules: (1) the query understanding that can thoroughly
    understand user-provided requirements; (2) the code generation module with iterative
    debugging capabilities that use code to precisely preprocess raw data and generate
    figures; and (3) the visual feedback module that possesses visual perceptual abilities
    to find errors in the plotted draft and provide visual feedback to the code generation
    module to rectify the errors. Our method is model-agnostic, which can be driven
    with any code LLMs and multi-modal LLMs. Through experiments, we find MatPlotAgent
    can work with both closed-source LLMs (e.g., GPT-4 OpenAI ([2023](https://arxiv.org/html/2402.11453v3#bib.bib20)))
    and open-source LLMs (e.g., Magicoder Wei et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35))).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出利用现代代码LLM和多模态LLM开发科学数据可视化代理，显著提升人类效率。由此产生的MatPlotAgent⁴⁴此名称向著名的Matplotlib致敬，由三个模块组成：(1)
    查询理解模块，能够彻底理解用户提供的需求；(2) 代码生成模块，具有迭代调试功能，使用代码精确预处理原始数据并生成图形；(3) 可视化反馈模块，具备视觉感知能力，能够在绘制草图中发现错误，并向代码生成模块提供可视反馈，以纠正错误。我们的方法是模型无关的，可以通过任何代码LLM和多模态LLM驱动。通过实验，我们发现MatPlotAgent可以与封闭源LLM（如GPT-4
    OpenAI（[2023](https://arxiv.org/html/2402.11453v3#bib.bib20)））以及开源LLM（如Magicoder
    Wei等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib35)））一起工作。
- en: Another critical challenge in the field of automatic scientific data visualization
    is the absence of benchmarks for evaluation purposes. To address this issue, we
    introduce a meticulously crafted benchmark called MatPlotBench to quantitatively
    evaluate the approaches involved. Specifically, MatPlotBench contains 100 carefully
    hand-crafted test examples, each of which contains a user query, the corresponding
    input data, and a ground-truth figure verified by human experts. We believe that
    high-quality test sets play a crucial role in driving advancements in the field.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化科学数据可视化领域的另一个关键挑战是缺乏用于评估的基准。为了解决这一问题，我们引入了一个精心设计的基准，名为MatPlotBench，用于定量评估相关方法。具体而言，MatPlotBench包含100个精心手工制作的测试示例，每个示例包含一个用户查询、相应的输入数据和一个由人工专家验证的真实图形。我们认为，高质量的测试集在推动该领域的进展中起着至关重要的作用。
- en: 'To facilitate automatic quantitative evaluation, we also design a scoring mechanism
    based on GPT-4V OpenAI ([2023](https://arxiv.org/html/2402.11453v3#bib.bib20)),
    which is one of the strongest multi-modal LLMs that can effectively understand
    text and figures. Specifically, GPT-4V is prompted to produce a score between
    0 and 100 based on the ground-truth figure and the one generated by AI models.
    Additionally, we conduct human evaluation and estimate the correlation coefficient
    between human-annotated scores and the automatically calculated scores. The results
    reveal a strong correlation between the automatic score and the human-annotated
    score, thus affirming the reliability of the scoring mechanism. In summary, our
    contribution can be listed as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于自动量化评估，我们还设计了一个基于GPT-4V OpenAI（[2023](https://arxiv.org/html/2402.11453v3#bib.bib20)）的评分机制，该模型是最强大的多模态LLM之一，能够有效理解文本和图形。具体而言，GPT-4V被提示根据真实图形和AI模型生成的图形之间的差异，给出一个0到100之间的评分。此外，我们进行了人工评估，并估算了人工标注分数与自动计算分数之间的相关系数。结果显示自动评分与人工标注分数之间存在强相关性，从而验证了评分机制的可靠性。总的来说，我们的贡献可以总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce MatPlotBench to enable automatic quantitative evaluation of AI
    methods designed for scientific data visualization. Through comparison with human
    evaluation, we observe that MatPlotBench can effectively capture the performance
    of AI approaches in this cutting-edge task.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了MatPlotBench，以便对为科学数据可视化设计的AI方法进行自动量化评估。通过与人工评估的比较，我们观察到MatPlotBench能够有效捕捉AI方法在这一前沿任务中的表现。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose an effective and generalizable LLM agent framework, MatPlotAgent,
    that can improve the performance of a wide range of LLMs based on the newly proposed
    visual feedback mechanism.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一个有效且具有良好泛化能力的LLM代理框架——MatPlotAgent，能够基于新提出的视觉反馈机制提高各种LLM的表现。
- en: 2 Task Description
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 任务描述
- en: 'We first introduce the scientific data visualization task investigated in this
    work. Given a user query $\mathbf{x}$ described in text and the corresponding
    data $\mathcal{D}$, the AI system is expected to output a figure $V$ that can
    satisfy the user’s demand:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍了本研究中调查的科学数据可视化任务。给定一个用户查询$\mathbf{x}$（以文本描述）和相应的数据$\mathcal{D}$，AI系统应该输出一个图形$V$，以满足用户的需求：
- en: '|  | $V=f(\mathbf{x},\mathcal{D}),$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $V=f(\mathbf{x},\mathcal{D}),$ |  | (1) |'
- en: where $f$ denotes the involved AI system that can be either an LLM or an LLM-based
    agent.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f$表示涉及的AI系统，可以是LLM或基于LLM的代理系统。
- en: 'Specifically, $\mathbf{x}$ specifies the visualization requirements, encompassing
    the visualization type, data to plot, structural or spatial requirements for individual
    elements or the entire plot, and aesthetic preferences. $\mathcal{D}$ represents
    the data, a collection of data points $\left\{d_{1},\cdots,d_{n}\right\}$ whether
    specified by the user or stored in the external data file. Figure [1](https://arxiv.org/html/2402.11453v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MatPlotAgent: Method and Evaluation for LLM-Based
    Agentic Scientific Data Visualization") provides some examples for this task.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，$\mathbf{x}$指定了可视化要求，包括可视化类型、绘制的数据、单个元素或整个图表的结构或空间要求以及美学偏好。$\mathcal{D}$表示数据，是一组数据点$\left\{d_{1},\cdots,d_{n}\right\}$，无论是由用户指定的还是存储在外部数据文件中的。图 [1](https://arxiv.org/html/2402.11453v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ MatPlotAgent: Method and Evaluation for LLM-Based
    Agentic Scientific Data Visualization")提供了这个任务的一些示例。'
- en: 3 MatPlotBench
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 MatPlotBench
- en: 'Automatic evaluation is important in AI tasks as it enables researchers to
    efficiently assess the performance of various methods, thereby guiding the development
    of the field. While the DS-1000 benchmark Lai et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib12))
    includes coding problems about Matplotlib, the solutions’ average length is merely
    three lines, rendering them too simplistic to gauge the proficiency of contemporary
    AI agents in tackling practical challenges. Therefore, we propose to construct
    MatPlotBench with complex data visualization problems that are more close to real-world
    scenarios. We will illustrate the data collection process in Section [3.1](https://arxiv.org/html/2402.11453v3#S3.SS1
    "3.1 Data Collection ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for
    LLM-Based Agentic Scientific Data Visualization") and then explain the scoring
    mechanism in Section [3.2](https://arxiv.org/html/2402.11453v3#S3.SS2 "3.2 Automatic
    Quantitative Evaluation ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation
    for LLM-Based Agentic Scientific Data Visualization").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 自动评估在人工智能任务中至关重要，因为它使研究人员能够高效地评估各种方法的表现，从而指导该领域的发展。尽管DS-1000基准测试Lai等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib12)）包含了关于Matplotlib的编码问题，但这些问题的平均解决方案仅为三行，过于简单，无法评估当代人工智能代理在应对实际挑战时的能力。因此，我们提议构建MatPlotBench，采用更接近真实世界场景的复杂数据可视化问题。我们将在第[3.1](https://arxiv.org/html/2402.11453v3#S3.SS1
    "3.1 数据收集 ‣ 3 MatPlotBench ‣ MatPlotAgent：基于LLM的代理科学数据可视化方法与评估")节中说明数据收集过程，然后在第[3.2](https://arxiv.org/html/2402.11453v3#S3.SS2
    "3.2 自动定量评估 ‣ 3 MatPlotBench ‣ MatPlotAgent：基于LLM的代理科学数据可视化方法与评估")节中解释评分机制。
- en: 3.1 Data Collection
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 数据收集
- en: Principles
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 原则
- en: 'To enhance the quality of MatPlotBench, we adhere to the following principles
    for data collection: (1) Covering diverse types: encompassing a broad range of
    plot types, including not only the most commonly used but also rare but useful
    ones; (2) Containing representative instances: ensuring that the test examples
    reflect the representative features of scientific data visualization, such as
    varying data complexity; and (3) Balancing easy and challenging problems: including
    problems of varying levels of difficulty in the benchmark.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高MatPlotBench的质量，我们遵循以下数据收集原则：（1）涵盖多种类型：包括不仅是最常用的图表类型，还包括一些稀有但有用的图表类型；（2）包含具有代表性的实例：确保测试示例反映科学数据可视化的代表性特征，如不同的数据复杂性；（3）平衡简单与具有挑战性的问题：在基准测试中包括不同难度级别的问题。
- en: Selecting Original Examples
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择原始示例
- en: In accordance with the principles outlined above, we first select some original
    examples from reputable online scientific data visualization forums. These examples
    are carefully selected from the Matplotlib Gallery and OriginLab GraphGallery,
    encompassing diverse and representative instances with varying levels of difficulty.
    Specifically, we select 1 or 2 examples from every section in the Matplotlib Gallery,
    covering bars, lines, markers, pie charts, polar plots, contour plots, statistics
    plots, 3D plots, text annotations, radar charts, shapes, scales, axes, spines,
    subplots, and so on. We also seek more advanced test examples from the OriginLab
    GraphGallery, focusing on those that are more aesthetically appealing or complex,
    such as Sankey diagrams, sunburst charts, radial plots, chord diagrams, streamplots,
    and others. Finally, 75 original examples come from the Matplotlib Gallery and
    the 25 other original examples come from the OriginLab GraphGallery. Subsequently,
    these examples undergo several modifications to become the final test cases in
    MatPlotBench.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述原则，我们首先从信誉良好的在线科学数据可视化论坛中选择一些原始示例。这些示例从Matplotlib Gallery和OriginLab GraphGallery中精心挑选，涵盖了不同难度级别的多样化和代表性实例。具体来说，我们从Matplotlib
    Gallery的每个部分中选择1或2个示例，涵盖条形图、折线图、标记、饼图、极坐标图、等高线图、统计图、3D图、文本注释、雷达图、形状、坐标轴、脊线、子图等内容。我们还从OriginLab
    GraphGallery中寻找更高级的测试示例，重点关注那些在美学上更具吸引力或更复杂的示例，如桑基图、旭日图、径向图、和弦图、流线图等。最后，75个原始示例来自Matplotlib
    Gallery，另外25个原始示例来自OriginLab GraphGallery。随后，这些示例会经过几次修改，成为MatPlotBench中的最终测试用例。
- en: Preliminary Query Generation
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 初步查询生成
- en: Based on the selected original examples, we use LLMs to generate preliminary
    queries, which are then revised by humans. For original examples from the Matplotlib
    Gallery, we use GPT-4 to convert the code in each original example into preliminary
    queries. For the examples from the OriginLab GraphGallery, there are only images.
    We thus use GPT-4V to convert each image into a preliminary query.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于选定的原始示例，我们使用大型语言模型（LLMs）生成初步查询，随后由人工进行修改。对于Matplotlib图库中的原始示例，我们使用GPT-4将每个原始示例中的代码转换为初步查询。对于OriginLab
    GraphGallery中的示例，仅有图像，因此我们使用GPT-4V将每张图像转换为初步查询。
- en: Data Replacement
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据替换
- en: Based on these preliminary queries, we begin data replacement for examples from
    the Matplotlib Gallery due to the observed phenomenon of memorization by GPT-4\.
    In this process, we replace the original data points with newly generated ones,
    while keeping other factors such as the plot type unchanged. For examples from
    OriginLab, we find that the data is inherently complex, and even GPT-4 does not
    exhibit memorization with these examples. As a result, we only perform data replacement
    for Matplotlib examples.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些初步查询，我们开始对Matplotlib图库中的示例进行数据替换，因为我们观察到GPT-4存在记忆化现象。在这个过程中，我们用新生成的数据点替换原始数据，同时保持图表类型等其他因素不变。对于OriginLab的示例，我们发现数据本身比较复杂，即使是GPT-4也不会在这些示例中表现出记忆化现象。因此，我们仅对Matplotlib示例进行数据替换。
- en: Human Modification
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工修改
- en: After completing the data replacement process, we engage human annotators to
    refine the preliminary queries. These annotators are tasked with correcting errors,
    eliminating ambiguity, and adding any omitted essential information. Each annotator
    involved has a minimum of three years of experience in coding and NLP. Furthermore,
    each query undergoes refinement by two independent human annotators.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 完成数据替换过程后，我们邀请人工标注人员来精炼初步查询。这些标注员的任务是纠正错误、消除歧义，并添加任何遗漏的关键信息。每位参与的标注员至少有三年的编码和NLP经验。此外，每个查询都会由两名独立的人工标注员进行修订。
- en: Updating Ground-Truth Figures
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 更新真实数据图像
- en: After obtaining the human-annotated queries, as the data in Matplotlib examples
    are altered, we cannot directly use the images in the original example as the
    ground truth. To this end, we manually wrote code to plot the ground truth for
    the Matplotlib examples. For examples from OriginLab, as the data remains unaltered,
    we extract the images from their website to serve as the ground truth.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取人工标注的查询后，由于Matplotlib示例中的数据已被修改，我们不能直接使用原始示例中的图像作为真实数据。为此，我们手动编写代码绘制Matplotlib示例的真实数据。对于OriginLab的示例，由于数据未经过修改，我们从其网站上提取图像作为真实数据。
- en: Human Verification
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人工验证
- en: After obtaining the queries and their corresponding ground truths, we performed
    a final round of manual verification. Three NLP researchers were asked to conduct
    this verification. In this turn, the focus is mainly on checking whether the user
    queries and the ground truths are well aligned. The researchers meticulously checked
    each element in the ground truth image and looked for their corresponding descriptions
    in the user query. Ill-described elements and those missing clarifications are
    corrected. Redundant and incorrect descriptions are removed. This process results
    in 100 high-quality (query, raw data, ground-truth figure) triples, which comprise
    our final benchmark.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取查询及其相应的真实数据后，我们进行了最终一轮的人工验证。三位NLP研究人员被要求进行这项验证。在这一轮中，重点主要是检查用户查询与真实数据是否对齐。研究人员仔细检查了真实数据图像中的每个元素，并查找其在用户查询中的对应描述。描述不清楚的元素和缺少说明的部分进行了修正。冗余和错误的描述被删除。这个过程产生了100个高质量的（查询、原始数据、真实数据图像）三元组，这些构成了我们的最终基准。
- en: 3.2 Automatic Quantitative Evaluation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 自动定量评估
- en: 'To ease the burden of manual evaluation and broaden the applicability of our
    benchmark for research purposes, we suggest employing GPT-4V, a cutting-edge multi-modal
    LLM, to conduct automatic evaluations on our proposed benchmark. We carefully
    prompt GPT-4V to give a score from 0 to 100 on model-generated visualizations
    using the corresponding ground truths as the reference. The prompt is shown in
    Figure [6](https://arxiv.org/html/2402.11453v3#A1.F6 "Figure 6 ‣ A.1 Evaluation
    Prompts ‣ Appendix A Detailed Prompts ‣ MatPlotAgent: Method and Evaluation for
    LLM-Based Agentic Scientific Data Visualization") in Appendix.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻人工评估的负担并扩大我们基准测试在研究中的适用性，我们建议使用最先进的多模态大语言模型GPT-4V对我们提出的基准进行自动评估。我们精心设计了提示词，要求GPT-4V根据相应的真实数据作为参考，给模型生成的可视化结果打分，评分范围为0到100。该提示词如图[6](https://arxiv.org/html/2402.11453v3#A1.F6
    "图6 ‣ A.1 评估提示词 ‣ 附录A 详细提示词 ‣ MatPlotAgent：基于LLM的科学数据可视化方法与评估")所示，见附录。
- en: Correlation with Human Evaluation
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与人工评估的相关性
- en: To assess the reliability of GPT-4V as an automatic evaluator for scientific
    visualizations, we calculate the correlation between the automatic scores and
    human-evaluated scores. Specifically, we employ GPT-3.5 and GPT-4 to generate
    figures on MatPlotBench, and then conduct both automatic and human evaluation
    for the generated figures. For each model, we iteratively sample a subset that
    consists of $n$ examples from the total benchmark, and then calculate the average
    score of both automatic and human evaluation. This process repeats $k$ times and
    we get $k$ data points for each type of evaluation, which can be represented by
    $\mathcal{A}=\{a_{1},\cdots,a_{k}\}$ and $\mathcal{H}=\{h_{1},\cdots,h_{k}\}$.
    $a_{i}$ denotes the average automatic score on the $i$-th randomly sampled subset,
    and $h_{i}$ represents the average human-evaluated score in the same subset. $n$
    and $k$ are set to 25 and 100, respectively.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估GPT-4V作为科学可视化自动评估工具的可靠性，我们计算了自动评分与人工评分之间的相关性。具体而言，我们使用GPT-3.5和GPT-4在MatPlotBench上生成图形，然后对生成的图形进行自动评估和人工评估。对于每个模型，我们从总基准中迭代抽取包含$n$个示例的子集，并计算自动评估和人工评估的平均分数。这个过程重复$k$次，我们得到每种评估类型的$k$个数据点，分别用$\mathcal{A}=\{a_{1},\cdots,a_{k}\}$和$\mathcal{H}=\{h_{1},\cdots,h_{k}\}$表示。$a_{i}$表示第$i$个随机抽取的子集上的平均自动评分，$h_{i}$表示同一子集上的平均人工评分。$n$和$k$分别设置为25和100。
- en: '![Refer to caption](img/116a112963ee268b4000452670259e3e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/116a112963ee268b4000452670259e3e.png)'
- en: 'Figure 2: Correlation between the proposed automatic evaluation mechanism and
    human evaluation.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提出的自动评估机制与人工评估之间的相关性。
- en: 'We utilize the statistical functions provided by scipy⁵⁵5[https://docs.scipy.org/doc/scipy/reference/stats.html](https://docs.scipy.org/doc/scipy/reference/stats.html)
    to compute the Pearson correlation coefficient $r$ and the corresponding p-value
    $p$. For GPT-4, we obtain $r$=0.876 and $p$=7.41e-33, while for GPT-3.5, the values
    are $r$=0.836 and $p$=2.67e-27\. Figure [2](https://arxiv.org/html/2402.11453v3#S3.F2
    "Figure 2 ‣ Correlation with Human Evaluation ‣ 3.2 Automatic Quantitative Evaluation
    ‣ 3 MatPlotBench ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization") shows the data points for GPT-4\. Given that $r>0.8$ and
    $p<$0.05, we conclude that the automatic evaluation scores are strongly correlated
    with human evaluation results. This demonstrates the reliability of the proposed
    scoring mechanism in assessing the quality of model-generated figures on MatPlotBench.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用scipy提供的统计函数⁵⁵5[https://docs.scipy.org/doc/scipy/reference/stats.html](https://docs.scipy.org/doc/scipy/reference/stats.html)计算皮尔逊相关系数$r$及其对应的$p$值。对于GPT-4，我们得到$r$=0.876，$p$=7.41e-33；而对于GPT-3.5，$r$=0.836，$p$=2.67e-27。图[2](https://arxiv.org/html/2402.11453v3#S3.F2
    "图2 ‣ 与人工评估的相关性 ‣ 3.2 自动定量评估 ‣ 3 MatPlotBench ‣ MatPlotAgent：基于LLM的科学数据可视化方法与评估")展示了GPT-4的数据显示点。鉴于$r>0.8$且$p<$0.05，我们得出结论：自动评估分数与人工评估结果之间有很强的相关性。这证明了我们提出的评分机制在评估MatPlotBench上模型生成的图形质量方面的可靠性。
- en: 4 MatPlotAgent
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 MatPlotAgent
- en: '![Refer to caption](img/eebfdce9dcf67032673877a23dff97a5.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/eebfdce9dcf67032673877a23dff97a5.png)'
- en: 'Figure 3: Workflow of MatPlotAgent: The query expansion module converts the
    user query into detailed multi-step instructions. These instructions are then
    passed to the code agent, which generates the plotting code. The visual agent
    provides informative feedback based on the current draft, guiding the refinement
    of the figure.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：MatPlotAgent的工作流程：查询扩展模块将用户查询转换为详细的多步骤指令。这些指令随后传递给代码代理，代码代理生成绘图代码。视觉代理根据当前草稿提供有价值的反馈，指导图形的优化。
- en: 'To improve the capabilities of LLMs for scientific data visualization, we propose
    an agentic framework that mimics the plotting process of human experts. The proposed
    MatPlotAgent is comprised of three modules, including the query expansion module,
    the code agent, and the visual agent. Figure [3](https://arxiv.org/html/2402.11453v3#S4.F3
    "Figure 3 ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based
    Agentic Scientific Data Visualization") illustrates the workflow of MatPlotAgent.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '为了提升LLMs在科学数据可视化方面的能力，我们提出了一个代理框架，模仿人类专家的绘图过程。所提出的MatPlotAgent由三个模块组成，包括查询扩展模块、代码代理和视觉代理。图[3](https://arxiv.org/html/2402.11453v3#S4.F3
    "图3 ‣ 4 MatPlotAgent ‣ MatPlotAgent: 方法与评估，基于LLM的代理科学数据可视化")展示了MatPlotAgent的工作流程。'
- en: 4.1 Query Expansion
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 查询扩展
- en: The query expansion module interprets and refines the user query, converting
    the high-level requirements into a sequence of explicit and detailed instructions
    that are easy for LLMs to follow. This module can also be viewed as a planning
    module, creating an overall plan before generating the figure. Specifically, this
    module is based on the involved code LLM, which is prompted to give detailed instructions
    on how to use code to fulfill the requirement specified by the user, including
    what libraries to import, what library functions to call, how to set the parameters
    in each function correctly, how to prepare the data, how to manipulate the data,
    and so on.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展模块解释并细化用户查询，将高层次的需求转化为易于大型语言模型（LLM）遵循的一系列明确且详细的指令。该模块也可以视为一个规划模块，在生成图形之前制定一个整体计划。具体而言，该模块基于涉及的代码LLM，提示其提供如何使用代码实现用户指定需求的详细指令，包括需要导入哪些库、调用哪些库函数、如何正确设置每个函数的参数、如何准备数据、如何处理数据等。
- en: 4.2 Code Agent
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 代码代理
- en: The code agent is the core component in MatPlotAgent, responsible for generating
    the code to plot figures. Given detailed instructions from the query expansion
    module, the code agent first generates the code using appropriate libraries and
    functions. To improve the success rate of the generated code, we also employ the
    self-debugging mechanism Chen et al. ([2024b](https://arxiv.org/html/2402.11453v3#bib.bib5)),
    which helps the involved code LLM iteratively identify and correct bugs in the
    code. To prevent an infinite loop, we set the maximum iterations of self-debugging
    to 3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 代码代理是MatPlotAgent的核心组件，负责生成绘制图形的代码。在查询扩展模块提供详细指令后，代码代理首先使用适当的库和函数生成代码。为了提高生成代码的成功率，我们还采用了自我调试机制 Chen等人（[2024b](https://arxiv.org/html/2402.11453v3#bib.bib5)），该机制帮助涉及的代码LLM迭代识别并修复代码中的错误。为了防止无限循环，我们将自我调试的最大迭代次数设置为3。
- en: 'Similar to humans, who need to repeatedly refine the figure based on current
    drafts, we also introduce a visual feedback mechanism. This mechanism employs
    multi-modal LLMs to provide suggestions to improve the figure and better fulfill
    the user’s queries. These suggestions, which we call visual feedback, are then
    provided to the code agent to further improve the code. Our experiments in Section [5.2](https://arxiv.org/html/2402.11453v3#S5.SS2
    "5.2 Main Results ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based
    Agentic Scientific Data Visualization") demonstrate that MatPlotAgent is compatible
    with several modern code LLMs, including both some well-known closed-source models
    and some open-source models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于人类需要根据当前草稿反复优化图形，我们也引入了视觉反馈机制。该机制利用多模态LLMs提供建议，以改善图形并更好地满足用户的查询。这些建议，我们称之为视觉反馈，随后被提供给代码代理，以进一步改进代码。我们在第[5.2节](https://arxiv.org/html/2402.11453v3#S5.SS2
    "5.2 主要结果 ‣ 5 实验 ‣ MatPlotAgent: 方法与评估，基于LLM的代理科学数据可视化")的实验表明，MatPlotAgent与几种现代代码LLM兼容，包括一些著名的闭源模型和一些开源模型。'
- en: '| Model | Direct | Zero-Shot | MatPlotAgent |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 直接 | 零-shot | MatPlotAgent |'
- en: '| Decod. | CoT | w/ GPT-4V |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 解码器 | CoT | 使用GPT-4V |'
- en: '| GPT-4 | 48.86 | 45.42 | $-$3.44 | 61.16 | $+$12.30 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 48.86 | 45.42 | $-$3.44 | 61.16 | $+$12.30 |'
- en: '| GPT-3.5 | 38.03 | 37.14 | $-$0.89 | 47.51 | $+$9.48 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 38.03 | 37.14 | $-$0.89 | 47.51 | $+$9.48 |'
- en: '| Magicoder-S-DS-6.7B Wei et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35))
    | 38.49 | 37.95 | $-$0.54 | 51.70 | $+$13.21 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Magicoder-S-DS-6.7B 魏等人 ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35))
    | 38.49 | 37.95 | $-$0.54 | 51.70 | $+$13.21 |'
- en: '| Deepseek-coder-6.7B-instruct Guo et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))
    | 31.53 | 29.16 | $-$2.37 | 39.45 | $+$7.92 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-coder-6.7B-instruct 郭等人 ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))
    | 31.53 | 29.16 | $-$2.37 | 39.45 | $+$7.92 |'
- en: '| CodeLlama-34B-Instruct Rozière et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib28))
    | 16.54 | 12.40 | $-$4.14 | 14.18 | $-$2.36 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| CodeLlama-34B-Instruct Rozière等人 ([2024](https://arxiv.org/html/2402.11453v3#bib.bib28))
    | 16.54 | 12.40 | $-$4.14 | 14.18 | $-$2.36 |'
- en: '| Deepseek-coder-33B-instruct Guo et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))
    | 30.88 | 36.10 | $+$5.22 | 32.18 | $+$1.30 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Deepseek-coder-33B-instruct 郭等人 ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))
    | 30.88 | 36.10 | $+$5.22 | 32.18 | $+$1.30 |'
- en: '| WizardCoder-Python-33B-V1.1 Luo et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib18))
    | 36.94 | 35.81 | $-$1.13 | 45.96 | $+$9.02 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| WizardCoder-Python-33B-V1.1 罗等人 ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib18))
    | 36.94 | 35.81 | $-$1.13 | 45.96 | $+$9.02 |'
- en: 'Table 1: Performance of different LLMs on MatPlotBench. For each model, improvements
    over the direct decoding are highlighted in red, while results worse than that
    of the direct decoding are highlighted in blue.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：不同大语言模型在MatPlotBench上的表现。对于每个模型，直接解码的改进用红色突出显示，而表现不如直接解码的结果则用蓝色突出显示。
- en: '| Model | Direct | MatPlotAgent |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 直接解码 | MatPlotAgent |'
- en: '| Decod. | w/ Gemini Pro Vision |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| Decod. | 使用Gemini Pro Vision |'
- en: '| GPT-4 | 48.86 |  | 56.73 | $+$7.87 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 48.86 |  | 56.73 | $+$7.87 |'
- en: '| GPT-3.5 | 38.03 |  | 43.48 | $+$5.45 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 | 38.03 |  | 43.48 | $+$5.45 |'
- en: 'Table 2: Performance of GPT-4 and GPT-3.5 using Gemini Pro Vision as visual
    agent on MatPlotBench.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：使用Gemini Pro Vision作为视觉代理在MatPlotBench上对GPT-4和GPT-3.5的性能。
- en: 4.3 Visual Agent
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 视觉代理
- en: The major difference between MatPlotAgent and previous LLM-based coding agents Qian
    et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib22)); Chen et al. ([2024b](https://arxiv.org/html/2402.11453v3#bib.bib5))
    is that we take the visual signal into account, which is important in scientific
    data visualization. Some errors or weaknesses may be difficult to identify in
    the code but become apparent when observing the output figure through “eyes”.
    The visual agent is the “eyes” for MatPlotAgent, while the aforementioned code
    agent acts as the “hands” for MatPlotAgent.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: MatPlotAgent与之前的基于大语言模型（LLM）的编码代理（如Qian等人 ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib22));
    Chen等人 ([2024b](https://arxiv.org/html/2402.11453v3#bib.bib5))）的主要区别在于，我们考虑了视觉信号，这在科学数据可视化中非常重要。某些错误或不足可能在代码中难以识别，但在通过“眼睛”观察输出图形时会变得显而易见。视觉代理是MatPlotAgent的“眼睛”，而前述的代码代理则充当MatPlotAgent的“手”。
- en: 'Specifically, the visual agent is powered by multi-modal LLMs. We introduce
    several guiding principles for the visual agent, including verifying whether the
    figure aligns with the provided data, and enhancing the colors or labels to improve
    the figure’s informativeness. Based on the principles, the user query, and the
    current draft of the figure, the visual agent generates some suggestions to refine
    to figure. These suggestions serve as feedback for the code agent to refine the
    code. Experimental results in Section [5.4](https://arxiv.org/html/2402.11453v3#S5.SS4
    "5.4 Ablation Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for
    LLM-Based Agentic Scientific Data Visualization") show that our visual feedback
    mechanism can significantly improve the quality of the plotted figures.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，视觉代理由多模态大语言模型（LLMs）提供支持。我们为视觉代理引入了几个指导原则，包括验证图形是否与提供的数据一致，以及增强颜色或标签以提高图形的信息量。基于这些原则、用户查询和当前图形草稿，视觉代理生成一些建议来优化图形。这些建议作为反馈供代码代理优化代码。实验结果在第[5.4节](https://arxiv.org/html/2402.11453v3#S5.SS4
    "5.4 Ablation Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for
    LLM-Based Agentic Scientific Data Visualization")中显示，我们的视觉反馈机制可以显著提高绘制图形的质量。'
- en: 5 Experiments
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Setup
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Models
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型
- en: Since the proposed MatPlotAgent is model-agnostic, we can employ various LLMs
    in this framework. The code LLMs we use in our experiments include GPT-4, GPT-3.5,
    Magicoder-S-DS-6.7B Wei et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35)),
    Deepseek-coder-6.7B-instruct Guo et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8)),
    Deepseek-coder-33B-instruct Guo et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8)),
    WizardCoder-Python-33B-V1.1 Luo et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib18)),
    and CodeLlama-34B-Instruct Rozière et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib28)).
    The decoding temperature is set to 0.0 for all the involved code LLMs. For GPT-4
    and GPT-3.5, we use the API provided by OpenAI⁶⁶6[https://openai.com/product](https://openai.com/product).
    For the other five open-source LLMs, we use vLLM Kwon et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib11))
    for model inference. For the visual agent, we utilize GPT-4V OpenAI ([2023](https://arxiv.org/html/2402.11453v3#bib.bib20))
    and Gemini Pro Vision Google ([2023](https://arxiv.org/html/2402.11453v3#bib.bib7)),
    two representative multi-modal LLMs. We leave the exploration of using open-source
    multi-modal LLMs to power the visual agent for future work.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所提出的MatPlotAgent是与模型无关的，我们可以在此框架中使用各种LLM。我们实验中使用的代码LLM包括GPT-4、GPT-3.5、Magicoder-S-DS-6.7B Wei等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib35)）、Deepseek-coder-6.7B-instruct Guo等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib8)）、Deepseek-coder-33B-instruct Guo等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib8)）、WizardCoder-Python-33B-V1.1 Luo等人（[2023b](https://arxiv.org/html/2402.11453v3#bib.bib18)）和CodeLlama-34B-Instruct Rozière等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib28)）。所有涉及的代码LLM的解码温度设置为0.0。对于GPT-4和GPT-3.5，我们使用OpenAI提供的API⁶⁶6[https://openai.com/product](https://openai.com/product)。对于其他五个开源LLM，我们使用vLLM Kwon等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib11)）进行模型推理。对于视觉代理，我们使用GPT-4V OpenAI（[2023](https://arxiv.org/html/2402.11453v3#bib.bib20)）和Gemini
    Pro Vision Google（[2023](https://arxiv.org/html/2402.11453v3#bib.bib7)），这两个是具有代表性的多模态LLM。我们将探索使用开源多模态LLM为视觉代理提供支持的工作留待未来。
- en: Evaluation
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'We evaluate the involved methods on MatPlotBench, using the proposed automatic
    scoring mechanism that is shown reliable in Section [3.2](https://arxiv.org/html/2402.11453v3#S3.SS2
    "3.2 Automatic Quantitative Evaluation ‣ 3 MatPlotBench ‣ MatPlotAgent: Method
    and Evaluation for LLM-Based Agentic Scientific Data Visualization"). For each
    code LLM, we evaluate its performance in three ways:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在MatPlotBench上评估所涉及的方法，使用在第[3.2](https://arxiv.org/html/2402.11453v3#S3.SS2
    "3.2 自动定量评估 ‣ 3 MatPlotBench ‣ MatPlotAgent: 基于LLM的代理科学数据可视化方法与评估")节中展示的可靠自动评分机制。对于每个代码LLM，我们从三方面评估其表现：'
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Direct decoding: given the query, the model directly generates the plotting
    code.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接解码：给定查询，模型直接生成绘图代码。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-Shot Chain-of-thought Kojima et al. ([2022b](https://arxiv.org/html/2402.11453v3#bib.bib10)):
    the model is prompted to inference with the zero-shot CoT mechanism.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot思维链Kojima等人（[2022b](https://arxiv.org/html/2402.11453v3#bib.bib10)）：模型通过零-shot思维链机制进行推理。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MatPlotAgent: the model is equipped with the proposed MatPlotAgent framework,
    driving the query expansion module and the code agent, as illustrated in Section [4](https://arxiv.org/html/2402.11453v3#S4
    "4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization").'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MatPlotAgent：模型配备了所提出的MatPlotAgent框架，驱动查询扩展模块和代码代理，如第[4](https://arxiv.org/html/2402.11453v3#S4
    "4 MatPlotAgent ‣ MatPlotAgent: 基于LLM的代理科学数据可视化方法与评估")节所示。'
- en: '| Model | Accuracy of Code Execution Results (%) |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 代码执行结果准确率（%） |'
- en: '| Visualization-Hard | Visualization-Easy | Average |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 可视化-困难 | 可视化-简单 | 平均 |'
- en: '| GPT-4 | 66.7 | 60.8 | 63.8 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 66.7 | 60.8 | 63.8 |'
- en: '|  + MatPlotAgent | 72.6 | 68.4 | 70.5 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  + MatPlotAgent | 72.6 | 68.4 | 70.5 |'
- en: '|      w/o Visual Feedback | 66.7 | 65.8 | 66.3 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|      无视觉反馈 | 66.7 | 65.8 | 66.3 |'
- en: 'Table 3: Effect of MatPlotAgent on the visualization subset of the Qwen-Agent
    Code Interpreter benchmark.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：MatPlotAgent对Qwen-Agent代码解释器基准中可视化子集的影响。
- en: 5.2 Main Results
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'Table [1](https://arxiv.org/html/2402.11453v3#S4.T1 "Table 1 ‣ 4.2 Code Agent
    ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization") presents the results of different methods on the scientific
    data visualization task. In the direct decoding setting, GPT-4 achieves the highest
    score of 48.86\. Surprisingly, the open-source model Magicoder-S-DS-6.7B Wei et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35)) achieves the second-best
    performance, surpassing models with substantially larger parameter sizes, such
    as WizardCoder-Python-33B-V1.1.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [1](https://arxiv.org/html/2402.11453v3#S4.T1 "Table 1 ‣ 4.2 Code Agent
    ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization")展示了不同方法在科学数据可视化任务上的结果。在直接解码设置下，GPT-4取得了最高分48.86。令人惊讶的是，开源模型Magicoder-S-DS-6.7B Wei
    et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35))取得了第二高的性能，超越了许多参数规模更大的模型，如WizardCoder-Python-33B-V1.1。'
- en: The results also suggest that the zero-shot CoT mechanism does not effectively
    enhance the performance of many recent code LLMs. Zero-shot CoT only improves
    the results of Deepseek-coder-33B-instruct Guo et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))
    from 30.88 to 36.10\. Conversely, for other models, implementing zero-shot CoT
    results in poorer performance. For example, when zero-shot CoT is applied, the
    performance of GPT-4 drops to 45.42, which is lower than the direct decoding result
    of 48.86.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 结果还表明，零-shot CoT机制并未有效提升许多近期代码LLM的性能。零-shot CoT仅将Deepseek-coder-33B-instruct Guo
    et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8))的结果从30.88提升到36.10。然而，对于其他模型，应用零-shot
    CoT反而导致性能下降。例如，当应用零-shot CoT时，GPT-4的性能降至45.42，低于其直接解码结果48.86。
- en: 'From Table [1](https://arxiv.org/html/2402.11453v3#S4.T1 "Table 1 ‣ 4.2 Code
    Agent ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic
    Scientific Data Visualization"), we find the proposed MatPlotAgent can improve
    the plotting capabilities of several models. For GPT-4 and GPT-3.5, MatPlotAgent
    leads to significant improvements of 12.30 and 9.48, respectively. For the other
    five open-source LLMs, MatPlotAgent improves the performance of four models. With
    MatPlotAgent, the open-source Magicoder-S-DS-6.7B model even surpasses GPT-4 with
    direct decoding (51.70 vs. 48.86), showcasing the effectiveness of our method.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '从表格 [1](https://arxiv.org/html/2402.11453v3#S4.T1 "Table 1 ‣ 4.2 Code Agent
    ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization")中，我们发现所提议的MatPlotAgent能够提升多个模型的绘图能力。对于GPT-4和GPT-3.5，MatPlotAgent分别带来了12.30和9.48的显著提升。对于其他五个开源LLM，MatPlotAgent提升了四个模型的性能。使用MatPlotAgent时，开源的Magicoder-S-DS-6.7B模型甚至超越了GPT-4的直接解码（51.70对48.86），展示了我们方法的有效性。'
- en: 'To investigate the generalizability of MatPlotAgent across various multi-modal
    LLMs, we present the results of employing Gemini Pro Vision as the visual agent
    in Table [2](https://arxiv.org/html/2402.11453v3#S4.T2 "Table 2 ‣ 4.2 Code Agent
    ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization"). We observe considerable improvements of 7.87 and 5.45, respectively,
    over the direct decoding baseline. This evidence further demonstrates the model-agnostic
    characteristic of our approach, leveraging various multi-modal LLMs to achieve
    enhanced performance.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究MatPlotAgent在各种多模态LLM中的泛化能力，我们在表格 [2](https://arxiv.org/html/2402.11453v3#S4.T2
    "Table 2 ‣ 4.2 Code Agent ‣ 4 MatPlotAgent ‣ MatPlotAgent: Method and Evaluation
    for LLM-Based Agentic Scientific Data Visualization")中展示了使用Gemini Pro Vision作为视觉代理的结果。我们观察到，相对于直接解码基准，分别有7.87和5.45的显著提升。这一证据进一步证明了我们方法的模型无关特性，利用各种多模态LLM来实现更高的性能。'
- en: '![Refer to caption](img/5b2074b82b55ae2c4403d4764198ab9d.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b2074b82b55ae2c4403d4764198ab9d.png)'
- en: 'Figure 4: Examples to illustrate the effect of visual feedback. To investigate
    the effect of the visual feedback mechanism on different models, we display the
    outputs of two representative LLMs. Case A, B, and C are generated by GPT-4\.
    Case D is generated by Magicoder-S-DS-6.7B.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：展示视觉反馈效果的示例。为了研究视觉反馈机制对不同模型的影响，我们展示了两个代表性LLM的输出。案例A、B和C由GPT-4生成。案例D由Magicoder-S-DS-6.7B生成。
- en: 5.3 Results on Qwen-Agent Code Interpreter Benchmark
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 Qwen-Agent代码解释器基准测试结果
- en: 'In Table [3](https://arxiv.org/html/2402.11453v3#S5.T3 "Table 3 ‣ Evaluation
    ‣ 5.1 Setup ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based
    Agentic Scientific Data Visualization"), we detail the performance of MatPlotAgent
    on the visualization subset of the Qwen-Agent Code Interpreter Benchmark⁷⁷7[https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark),
    which was recently published. According to their GitHub repository, GPT-4 achieved
    scores of 66.7 and 60.8 on the Visualization-Hard and Visualization-Easy subsets,
    respectively. Utilizing MatPlotAgent, we attained higher scores of 72.62 and 68.35
    on these subsets. When the visual feedback mechanism is disabled, MatPlotAgent
    reached scores of 66.67 and 65.82, reconfirming the necessity of visual feedback.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[3](https://arxiv.org/html/2402.11453v3#S5.T3 "Table 3 ‣ Evaluation ‣ 5.1
    Setup ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic
    Scientific Data Visualization")中，我们详细展示了MatPlotAgent在Qwen-Agent代码解释器基准测试的可视化子集上的表现⁷⁷7[https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark](https://github.com/QwenLM/Qwen-Agent/tree/main/benchmark)，该基准测试最近发布。根据他们的GitHub仓库，GPT-4在Visualization-Hard和Visualization-Easy子集上的得分分别为66.7和60.8。利用MatPlotAgent，我们在这些子集上分别达到了72.62和68.35的更高得分。当禁用可视反馈机制时，MatPlotAgent的得分为66.67和65.82，再次确认了可视反馈的必要性。'
- en: 5.4 Ablation Study
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究
- en: Compared to previous LLM-based coding agents Qian et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib22));
    Chen et al. ([2024b](https://arxiv.org/html/2402.11453v3#bib.bib5)), the major
    contribution of the work lies in the newly proposed visual feedback mechanism,
    expected to leverage visual signals to enhance the quality of the output figure.
    To gain a deeper understanding of the impact of the visual feedback mechanism,
    we conduct both qualitative and quantitative analyses in this section.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的基于LLM的编码代理Qian等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib22)）；Chen等人（[2024b](https://arxiv.org/html/2402.11453v3#bib.bib5)）相比，本工作的主要贡献在于新提出的可视反馈机制，旨在利用可视信号提升输出图形的质量。为了更深入地理解可视反馈机制的影响，我们在本节中进行了定性和定量分析。
- en: 'Figure [4](https://arxiv.org/html/2402.11453v3#S5.F4 "Figure 4 ‣ 5.2 Main Results
    ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization") presents examples plotted by LLMs both with and without the
    visual feedback mechanism. We observe a clear improvement in the quality of the
    output figure with the visual feedback. For example, in case C, the text in the
    figure is jumbled, but this issue is resolved with the assistance of visual feedback.
    It is important to note that the visual agent does not reference the ground-truth
    figure when generating feedback; it only examines the draft plotted by the model.
    Table [4](https://arxiv.org/html/2402.11453v3#S5.T4 "Table 4 ‣ 5.4 Ablation Study
    ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization") also presents quantitative results of the visual feedback
    mechanism, indicating that the absence of visual feedback would result in significantly
    poorer outcomes for both GPT-4 and GPT-3.5\. This reaffirms the importance of
    visual signals in the task of scientific data visualization.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](https://arxiv.org/html/2402.11453v3#S5.F4 "Figure 4 ‣ 5.2 Main Results
    ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization")展示了LLM在有无可视反馈机制下绘制的示例。我们观察到，使用可视反馈后，输出图形的质量明显改善。例如，在C案例中，图中的文本是混乱的，但在可视反馈的帮助下，这个问题得以解决。需要注意的是，视觉代理在生成反馈时并不参考真实图形，而只是检查模型绘制的草图。表[4](https://arxiv.org/html/2402.11453v3#S5.T4
    "Table 4 ‣ 5.4 Ablation Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation
    for LLM-Based Agentic Scientific Data Visualization")还展示了可视反馈机制的定量结果，表明缺少可视反馈会导致GPT-4和GPT-3.5的表现显著下降。这再次强调了可视信号在科学数据可视化任务中的重要性。'
- en: '| Model | GPT-4 | GPT-3.5 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GPT-4 | GPT-3.5 |'
- en: '| Direct Decod. | 48.86 | 38.03 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 直接解码 | 48.86 | 38.03 |'
- en: '| MatPlotAgent | 61.16 | 47.51 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| MatPlotAgent | 61.16 | 47.51 |'
- en: '|     w/o Visual Feedback | 53.44 | 41.57 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|     无可视反馈 | 53.44 | 41.57 |'
- en: 'Table 4: Effect of the visual feedback mechanism (GPT-4V visual agent).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：可视反馈机制的效果（GPT-4V视觉代理）。
- en: 5.5 Case Study
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 案例研究
- en: '![Refer to caption](img/35785b5adfd25ba88232facd5ad9f297.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/35785b5adfd25ba88232facd5ad9f297.png)'
- en: 'Figure 5: Case study of different models.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：不同模型的案例研究。
- en: 'We present output figures in Figure [5](https://arxiv.org/html/2402.11453v3#S5.F5
    "Figure 5 ‣ 5.5 Case Study ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation
    for LLM-Based Agentic Scientific Data Visualization"). The first example is relatively
    simple, correctly plotted by GPT-4 augmented with MatPlotAgent. The second example
    is more challenging; while GPT-4 and Magicoder-S-DS-6.7B can generate a draft,
    both omit some elements. The third example is the most difficult, where none of
    the three models can produce the correct result. These results indicate that the
    proposed MatPlotBench poses a significant challenge for current LLMs. Even the
    state-of-the-art LLM, GPT-4, equipped with MatPlotAgent, fails in some cases.
    We believe this benchmark will be effective not only for evaluating AI systems
    in scientific data visualization but also for assessing general capabilities such
    as coding and visual perception.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [5](https://arxiv.org/html/2402.11453v3#S5.F5 "Figure 5 ‣ 5.5 Case Study
    ‣ 5 Experiments ‣ MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
    Data Visualization")中展示了输出结果。第一个示例相对简单，由GPT-4和MatPlotAgent增强正确绘制。第二个示例较为复杂，尽管GPT-4和Magicoder-S-DS-6.7B能够生成草稿，但两者都忽略了一些元素。第三个示例是最困难的，三个模型都无法生成正确的结果。这些结果表明，提出的MatPlotBench对当前的LLM构成了显著挑战。即使是最先进的LLM——GPT-4，配备了MatPlotAgent，也在某些情况下失败。我们认为，这个基准不仅对于评估AI系统在科学数据可视化中的表现有效，也对于评估诸如编码和视觉感知等一般能力具有参考价值。'
- en: 6 Related Work
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 相关工作
- en: Code LLMs
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码LLM
- en: Since the release of Codex Chen et al. ([2021](https://arxiv.org/html/2402.11453v3#bib.bib3)),
    many closed- and open-source code LLMs have been published, pushing the boundaries
    of LLMs’ capabilities to write functional code. Early open-source efforts include
    SantaCoder Allal et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib1))
    and StarCoder Li et al. ([2023b](https://arxiv.org/html/2402.11453v3#bib.bib14)).
    More recently, the Code Llama Rozière et al. ([2024](https://arxiv.org/html/2402.11453v3#bib.bib28))
    series is released, including models of varying sizes. DeepSeekCoder Guo et al.
    ([2024](https://arxiv.org/html/2402.11453v3#bib.bib8)), a series of open-source
    code models ranging in size from 1.3B to 33B, has also garnered significant attention
    for its impressive performance on general coding benchmarks. Wei et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib35))
    introduce a novel data augmentation method for automatically creating high-quality
    fine-tuning data. The resulting Magicoder model surpasses a wide array of open-source
    code LLMs in performance.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Codex Chen等人发布以来（[2021](https://arxiv.org/html/2402.11453v3#bib.bib3)），许多闭源和开源的代码LLM已被发布，推动了LLM在编写功能代码方面的能力边界。早期的开源努力包括SantaCoder
    Allal等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib1)）和StarCoder Li等人（[2023b](https://arxiv.org/html/2402.11453v3#bib.bib14)）。最近，Code
    Llama Rozière等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib28)）系列发布，其中包括不同规模的模型。DeepSeekCoder
    Guo等人（[2024](https://arxiv.org/html/2402.11453v3#bib.bib8)）是一系列开源代码模型，规模从1.3B到33B不等，因其在通用编码基准上的出色表现而引起了广泛关注。Wei等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib35)）介绍了一种新颖的数据增强方法，用于自动创建高质量的微调数据。由此产生的Magicoder模型在性能上超越了多种开源代码LLM。
- en: LLM Agents
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM代理
- en: Recently, a wide range of LLM-based agent frameworks is proposed to explore
    LLMs’ potential in real-world scenarios Nakano et al. ([2021](https://arxiv.org/html/2402.11453v3#bib.bib19));
    Yao et al. ([2022](https://arxiv.org/html/2402.11453v3#bib.bib38)); Qin et al.
    ([2023](https://arxiv.org/html/2402.11453v3#bib.bib24)); Zhou et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib42)).
    OpenAgents Xie et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib36))
    proposed an open platform that leverages LLM agents in everyday situation by employing
    a Data Agent, a Plugins Agent, and a Web Agent. Park et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib21))
    proposed an interactive simulation of human behavior in which software agents
    emulate realistic human actions and interactions through computation. Voyager Wang
    et al. ([2023](https://arxiv.org/html/2402.11453v3#bib.bib33)) introduced the
    fisrt LLM model-driven autonomous agent in Minecraft, designed to perpetually
    explore the environment, master various skills, and uncover new insights independently,
    without any human guidance. ChatDev Qian et al. ([2023a](https://arxiv.org/html/2402.11453v3#bib.bib22))
    proposed creating a virtual, chat-driven software development enterprise that
    follows the traditional waterfall methodology. In this study, we explore the capabilities
    of LLM-based agents in the task of scientific data visualization, a critical and
    practical area for contemporary researchers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，提出了多种基于LLM的代理框架，以探索LLMs在现实场景中的潜力 Nakano 等人（[2021](https://arxiv.org/html/2402.11453v3#bib.bib19)）；Yao
    等人（[2022](https://arxiv.org/html/2402.11453v3#bib.bib38)）；Qin 等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib24)）；Zhou
    等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib42)）。OpenAgents Xie 等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib36)）提出了一个开放平台，通过数据代理、插件代理和网页代理来利用LLM代理处理日常情境。Park
    等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib21)）提出了一种人类行为的互动仿真，其中软件代理通过计算模拟现实的人类行为和互动。Voyager
    Wang 等人（[2023](https://arxiv.org/html/2402.11453v3#bib.bib33)）介绍了首个基于LLM模型驱动的Minecraft自主代理，旨在持续探索环境、掌握各种技能，并独立发掘新知识，无需任何人类指导。ChatDev
    Qian 等人（[2023a](https://arxiv.org/html/2402.11453v3#bib.bib22)）提出了一个虚拟的、由聊天驱动的软件开发企业，遵循传统的瀑布式方法。在本研究中，我们探讨了基于LLM的代理在科学数据可视化任务中的能力，这对于当代研究人员来说是一个关键且实际的领域。
- en: 7 Conclusion
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We propose to assess and enhance the capabilities of modern LLMs for scientific
    data visualization, a multifaceted task demanding coding and visual skills. We
    begin with the creation of MatPlotBench, a rigorous benchmark supporting automated
    quantitative evaluation that strongly aligns with human assessment. Additionally,
    we introduce MatPlotAgent, a model-agnostic mechanism employing visual feedback
    to enhance LLMs’ plotting abilities. Experimental results demonstrate that MatPlotAgent
    enhances the performance of various LLMs.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提议评估并提升现代大语言模型（LLMs）在科学数据可视化中的能力，这是一项多方面的任务，要求具备编程和视觉技能。我们首先创建了MatPlotBench，这是一个严谨的基准测试，支持自动化的定量评估，并与人类评估高度一致。此外，我们还介绍了MatPlotAgent，这是一个与模型无关的机制，通过视觉反馈来增强LLMs的绘图能力。实验结果表明，MatPlotAgent提高了各种LLMs的性能。
- en: 8 Limitations
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: In this paper, we introduce MatPlotBench, a benchmark designed for scientific
    data visualization. However, the demands of scientific data visualization can
    vary significantly across disciplines. Since MatPlotBench is developed for general
    scientific data visualization, it may not encompass all domain-specific requirements,
    potentially restricting its applicability to certain fields. In the future, the
    data construction and evaluation approaches can be customized for specific domains
    if necessary.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了MatPlotBench，这是一个专为科学数据可视化设计的基准测试。然而，科学数据可视化的需求在不同学科之间可能存在显著差异。由于MatPlotBench是为一般科学数据可视化开发的，它可能无法涵盖所有领域特定的要求，从而可能限制其在某些领域的适用性。未来，数据构建和评估方法可以根据需要定制，以适应特定领域。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Allal et al. (2023) Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao
    Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra,
    Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi,
    Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel
    Romero, Michael Lappert, Francesco De Toni, Bernardo García del Río, Qian Liu,
    Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco
    Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa,
    Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha,
    Harm de Vries, and Leandro von Werra. 2023. [Santacoder: don’t reach for the stars!](http://arxiv.org/abs/2301.03988)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Allal 等人（2023）Loubna Ben Allal、Raymond Li、Denis Kocetkov、Chenghao Mou、Christopher
    Akiki、Carlos Munoz Ferrandis、Niklas Muennighoff、Mayank Mishra、Alex Gu、Manan Dey、Logesh
    Kumar Umapathi、Carolyn Jane Anderson、Yangtian Zi、Joel Lamy Poirier、Hailey Schoelkopf、Sergey
    Troshin、Dmitry Abulkhanov、Manuel Romero、Michael Lappert、Francesco De Toni、Bernardo
    García del Río、Qian Liu、Shamik Bose、Urvashi Bhattacharyya、Terry Yue Zhuo、Ian Yu、Paulo
    Villegas、Marco Zocca、Sourab Mangrulkar、David Lansky、Huu Nguyen、Danish Contractor、Luis
    Villa、Jia Li、Dzmitry Bahdanau、Yacine Jernite、Sean Hughes、Daniel Fried、Arjun Guha、Harm
    de Vries 和 Leandro von Werra。2023年。[Santacoder: 不要去追寻星辰！](http://arxiv.org/abs/2301.03988)'
- en: 'Azerbayev et al. (2024) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster,
    Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman,
    and Sean Welleck. 2024. [Llemma: An open language model for mathematics](https://openreview.net/forum?id=4WnqRR915j).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Azerbayev 等人（2024）Zhangir Azerbayev、Hailey Schoelkopf、Keiran Paster、Marco Dos
    Santos、Stephen Marcus McAleer、Albert Q. Jiang、Jia Deng、Stella Biderman 和 Sean
    Welleck。2024年。[Llemma: 一个开源的数学语言模型](https://openreview.net/forum?id=4WnqRR915j)。发表于
    *第十二届国际学习表示会议*。'
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. [Evaluating large language models trained on code](http://arxiv.org/abs/2107.03374).
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de
    Oliveira Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman、Alex
    Ray、Raul Puri、Gretchen Krueger、Michael Petrov、Heidy Khlaaf、Girish Sastry、Pamela
    Mishkin、Brooke Chan、Scott Gray、Nick Ryder、Mikhail Pavlov、Alethea Power、Lukasz
    Kaiser、Mohammad Bavarian、Clemens Winter、Philippe Tillet、Felipe Petroski Such、Dave
    Cummings、Matthias Plappert、Fotios Chantzis、Elizabeth Barnes、Ariel Herbert-Voss、William
    Hebgen Guss、Alex Nichol、Alex Paino、Nikolas Tezak、Jie Tang、Igor Babuschkin、Suchir
    Balaji、Shantanu Jain、William Saunders、Christopher Hesse、Andrew N. Carr、Jan Leike、Josh
    Achiam、Vedant Misra、Evan Morikawa、Alec Radford、Matthew Knight、Miles Brundage、Mira
    Murati、Katie Mayer、Peter Welinder、Bob McGrew、Dario Amodei、Sam McCandlish、Ilya
    Sutskever 和 Wojciech Zaremba。2021年。[评估在代码上训练的大型语言模型](http://arxiv.org/abs/2107.03374)。
- en: 'Chen et al. (2024a) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin
    Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2024a. [Agentverse:
    Facilitating multi-agent collaboration and exploring emergent behaviors](https://openreview.net/forum?id=EHg5GDnyq1).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2024a）Weize Chen、Yusheng Su、Jingwei Zuo、Cheng Yang、Chenfei Yuan、Chi-Min
    Chan、Heyang Yu、Yaxi Lu、Yi-Hsin Hung、Chen Qian、Yujia Qin、Xin Cong、Ruobing Xie、Zhiyuan
    Liu、Maosong Sun 和 Jie Zhou。2024年a。[Agentverse: 促进多智能体协作并探索涌现行为](https://openreview.net/forum?id=EHg5GDnyq1)。发表于
    *第十二届国际学习表示会议*。'
- en: Chen et al. (2024b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2024b. [Teaching large language models to self-debug](https://openreview.net/forum?id=KuPixIqPiq).
    In *The Twelfth International Conference on Learning Representations*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024b）Xinyun Chen、Maxwell Lin、Nathanael Schärli 和 Denny Zhou。2024年b。[教会大型语言模型自我调试](https://openreview.net/forum?id=KuPixIqPiq)。发表于
    *第十二届国际学习表示会议*。
- en: 'Deng et al. (2023) Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens,
    Boshi Wang, Huan Sun, and Yu Su. 2023. [Mind2web: Towards a generalist agent for
    the web](https://openreview.net/forum?id=kiYqbO3wqw). In *Thirty-seventh Conference
    on Neural Information Processing Systems Datasets and Benchmarks Track*.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等（2023）Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi
    Wang, Huan Sun, 和Yu Su. 2023. [Mind2web：迈向面向Web的通用智能体](https://openreview.net/forum?id=kiYqbO3wqw)。收录于
    *第37届神经信息处理系统大会 数据集与基准轨道*。
- en: 'Google (2023) Gemini Team Google. 2023. [Gemini: A family of highly capable
    multimodal models](http://arxiv.org/abs/2312.11805).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google（2023）Gemini团队，Google。2023. [Gemini：一系列高效能的多模态模型](http://arxiv.org/abs/2312.11805)。
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. 2024. [Deepseek-coder: When the large language model meets programming
    – the rise of code intelligence](http://arxiv.org/abs/2401.14196).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等（2024）Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,
    Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, 和Wenfeng Liang.
    2024. [Deepseek-coder：当大语言模型遇上编程——代码智能的崛起](http://arxiv.org/abs/2401.14196)。
- en: Kojima et al. (2022a) Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022a. Large language models are zero-shot reasoners.
    In *Advances in Neural Information Processing Systems*, volume 35, pages 22199–22213.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima等（2022a）Takeshi Kojima, Shixiang（Shane）Gu, Machel Reid, Yutaka Matsuo,
    和Yusuke Iwasawa. 2022a. 大语言模型是零-shot推理者。收录于 *神经信息处理系统进展*，第35卷，第22199–22213页。
- en: Kojima et al. (2022b) Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022b. [Large language models are zero-shot reasoners](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima等（2022b）Takeshi Kojima, Shixiang（Shane）Gu, Machel Reid, Yutaka Matsuo,
    和Yusuke Iwasawa. 2022b. [大语言模型是零-shot推理者](https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf)。收录于
    *神经信息处理系统进展*。
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient
    memory management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon等（2023）Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, 和Ion Stoica. 2023. 大语言模型服务的高效内存管理，采用pagedattention技术。收录于
    *ACM SIGOPS第29届操作系统原理研讨会论文集*。
- en: 'Lai et al. (2023) Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi
    Zhong, Luke Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, and Tao Yu. 2023.
    DS-1000: A natural and reliable benchmark for data science code generation. In
    *Proceedings of the 40th International Conference on Machine Learning*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai等（2023）Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke
    Zettlemoyer, Wen-Tau Yih, Daniel Fried, Sida Wang, 和Tao Yu. 2023. DS-1000：一个自然且可靠的数据科学代码生成基准。收录于
    *第40届国际机器学习大会论文集*。
- en: 'Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. 2023a. [CAMEL: Communicative agents for ”mind”
    exploration of large language model society](https://openreview.net/forum?id=3IyL2XWDkG).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li等（2023a）Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin,
    和Bernard Ghanem. 2023a. [CAMEL：用于“大语言模型社会”探索的交流型智能体](https://openreview.net/forum?id=3IyL2XWDkG)。收录于
    *第37届神经信息处理系统大会*。
- en: 'Li et al. (2023b) Raymond Li, Loubna Ben allal, Yangtian Zi, Niklas Muennighoff,
    Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia LI, Jenny Chim,
    Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene,
    Joel Lamy-Poirier, Joao Monteiro, Nicolas Gontier, Ming-Ho Yee, Logesh Kumar Umapathi,
    Jian Zhu, Ben Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason T
    Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan
    Zhang, Urvashi Bhattacharyya, Wenhao Yu, Sasha Luccioni, Paulo Villegas, Fedor
    Zhdanov, Tony Lee, Nadav Timor, Jennifer Ding, Claire S Schlesinger, Hailey Schoelkopf,
    Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Carolyn Jane Anderson, Brendan Dolan-Gavitt,
    Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite,
    Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro Von Werra,
    and Harm de Vries. 2023b. [Starcoder: may the source be with you!](https://openreview.net/forum?id=KoFOg41haE)
    *Transactions on Machine Learning Research*. Reproducibility Certification.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '李等（2023b）雷蒙德·李、卢布娜·本·阿拉尔、杨天紫、尼克拉斯·穆宁霍夫、丹尼斯·科切特科夫、牟成浩、马克·马罗内、克里斯托弗·阿基基、李佳、珍妮·奇姆、刘乾、叶夫根尼·热尔托诺日基、朱浩天、托马斯·王、奥利维尔·德哈恩、乔尔·拉米-波里耶、若昂·蒙泰罗、尼古拉斯·贡捷、易铭豪、洛格什·库马尔·乌马帕西、朱剑、班·利普金、穆塔沙姆·奥布洛库洛夫、王志若、鲁德拉·穆尔西、杰森·T·斯蒂勒曼、希瓦·桑卡尔普·帕特尔、德米特里·阿布尔哈诺夫、马尔科·佐卡、马南·德伊、张志翰、厄尔瓦希·巴塔查里亚、余文浩、萨莎·卢乔尼、保罗·维尔赫加斯、费多尔·日丹诺夫、托尼·李、纳达夫·提莫尔、詹妮弗·丁、克莱尔·S·施莱辛格、海莉·斯科尔科普夫、简·厄伯特、三道、梅扬·米什拉、亚历克斯·顾、卡罗琳·简·安德森、布伦丹·多兰-加维特、丹尼斯·承包商、希瓦·雷迪、丹尼尔·弗里德、兹米特里·巴赫达瑙、亚辛·热尔尼特、卡洛斯·穆诺兹·费兰迪斯、肖恩·休斯、托马斯·沃尔夫、阿尔君·古哈、利安德罗·冯·维拉和哈姆·德·弗里斯。2023b年。[Starcoder:
    愿源代码与你同在！](https://openreview.net/forum?id=KoFOg41haE) *机器学习研究期刊*。可重复性认证。'
- en: 'Liu et al. (2024) Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu
    Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng,
    Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan
    Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2024. [Agentbench: Evaluating LLMs
    as agents](https://openreview.net/forum?id=zAdUB0aCTQ). In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '刘等（2024）肖刘、郝宇、张汉臣、许逸凡、雷轩宇、赖汉宇、顾宇、丁航亮、孟凯文、杨克娟、张书丹、邓翔、曾傲涵、杜正晓、张晨辉、沈胜、张天俊、苏宇、孙欢、黄敏磊、董宇晓和唐杰。2024年。[Agentbench:
    评估大语言模型作为智能体的能力](https://openreview.net/forum?id=zAdUB0aCTQ)。在*第十二届国际学习表示会议*。'
- en: 'Lu et al. (2023) Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang,
    Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023. [Chameleon: Plug-and-play
    compositional reasoning with large language models](https://openreview.net/forum?id=HtqnVSCj3q).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陆等（2023）陆磐、彭宝林、程浩、米歇尔·加利、凯-维·张、吴英年、朱松春和高剑锋。2023年。[Chameleon: 大语言模型的即插即用组合推理](https://openreview.net/forum?id=HtqnVSCj3q)。在*第三十七届神经信息处理系统会议*。'
- en: 'Luo et al. (2023a) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou,
    Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023a.
    Wizardmath: Empowering mathematical reasoning for large language models via reinforced
    evol-instruct. *arXiv preprint arXiv:2308.09583*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '罗等（2023a）罗海鹏、孙清峰、徐灿、赵普、娄建广、陶崇扬、耿修波、林庆伟、陈世峰和张东梅。2023a年。Wizardmath: 通过强化evol-instruct增强大语言模型的数学推理能力。*arXiv预印本arXiv:2308.09583*。'
- en: 'Luo et al. (2023b) Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023b. [Wizardcoder:
    Empowering code large language models with evol-instruct](http://arxiv.org/abs/2306.08568).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '罗等（2023b）罗子扬、徐灿、赵普、孙清峰、耿修波、胡文翔、陶崇扬、马晶、林庆伟和蒋大鑫。2023b年。[Wizardcoder: 通过evol-instruct增强代码的大语言模型](http://arxiv.org/abs/2306.08568)。'
- en: 'Nakano et al. (2021) Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
    Ouyang Long, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju,
    William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin
    Button, Matthew Knight, Benjamin Chess, and John Schulman. 2021. [Webgpt: Browser-assisted
    question-answering with human feedback](https://api.semanticscholar.org/CorpusID:245329531).
    *ArXiv*, abs/2112.09332.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '中野等（2021）中野礼一郎、雅各布·希尔顿、苏奇尔·巴拉吉、杰夫·吴、欧阳龙、克里斯蒂娜·金、克里斯托弗·赫塞、尚塔努·贾因、维内特·科萨拉朱、威廉·桑德斯、姜旭、卡尔·科比、泰娜·埃朗杜、格雷琴·克鲁格、凯文·巴顿、马修·奈特、班杰明·切斯和约翰·舒尔曼。2021年。[Webgpt:
    基于浏览器的问答与人工反馈](https://api.semanticscholar.org/CorpusID:245329531)。*ArXiv*，abs/2112.09332。'
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://api.semanticscholar.org/CorpusID:257532815).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023）OpenAI。2023年。[Gpt-4技术报告](https://api.semanticscholar.org/CorpusID:257532815)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://doi.org/10.1145/3586183.3606763). In *Proceedings
    of the 36th Annual ACM Symposium on User Interface Software and Technology*, UIST
    ’23, New York, NY, USA. Association for Computing Machinery.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park 等人 (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, 和 Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://doi.org/10.1145/3586183.3606763). 载于 *Proceedings
    of the 36th Annual ACM Symposium on User Interface Software and Technology*, UIST
    ’23, 纽约, NY, USA. 计算机协会。'
- en: Qian et al. (2023a) Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng
    Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, and Maosong Sun.
    2023a. [Communicative agents for software development](http://arxiv.org/abs/2307.07924).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2023a) Chen Qian, Xin Cong, Wei Liu, Cheng Yang, Weize Chen, Yusheng
    Su, Yufan Dang, Jiahao Li, Juyuan Xu, Dahai Li, Zhiyuan Liu, 和 Maosong Sun. 2023a.
    [Communicative agents for software development](http://arxiv.org/abs/2307.07924).
- en: 'Qian et al. (2023b) Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, and
    Heng Ji. 2023b. [CREATOR: Tool creation for disentangling abstract and concrete
    reasoning of large language models](https://doi.org/10.18653/v1/2023.findings-emnlp.462).
    In *Findings of the Association for Computational Linguistics: EMNLP 2023*, pages
    6922–6939, Singapore. Association for Computational Linguistics.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian 等人 (2023b) Cheng Qian, Chi Han, Yi Fung, Yujia Qin, Zhiyuan Liu, 和 Heng
    Ji. 2023b. [CREATOR: Tool creation for disentangling abstract and concrete reasoning
    of large language models](https://doi.org/10.18653/v1/2023.findings-emnlp.462).
    载于 *Findings of the Association for Computational Linguistics: EMNLP 2023*, 第6922–6939页,
    新加坡. 计算语言学协会。'
- en: 'Qin et al. (2023) Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun
    Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan
    Liu, Maosong Sun, and Jie Zhou. 2023. [WebCPM: Interactive web search for Chinese
    long-form question answering](https://doi.org/10.18653/v1/2023.acl-long.499).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8968–8988, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人 (2023) Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun
    Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan
    Liu, Maosong Sun, 和 Jie Zhou. 2023. [WebCPM: Interactive web search for Chinese
    long-form question answering](https://doi.org/10.18653/v1/2023.acl-long.499).
    载于 *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 第8968–8988页, 多伦多, 加拿大. 计算语言学协会。'
- en: 'Qin et al. (2024) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong,
    Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, and
    Maosong Sun. 2024. [ToolLLM: Facilitating large language models to master 16000+
    real-world APIs](https://openreview.net/forum?id=dHng2O0Jjr). In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin 等人 (2024) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi
    Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu
    Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, dahai li, Zhiyuan Liu, 和 Maosong Sun.
    2024. [ToolLLM: Facilitating large language models to master 16000+ real-world
    APIs](https://openreview.net/forum?id=dHng2O0Jjr). 载于 *The Twelfth International
    Conference on Learning Representations*.'
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. [Zero-shot text-to-image
    generation](http://arxiv.org/abs/2102.12092).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh 等人 (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea
    Voss, Alec Radford, Mark Chen, 和 Ilya Sutskever. 2021. [Zero-shot text-to-image
    generation](http://arxiv.org/abs/2102.12092).
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. [High-resolution image synthesis with latent diffusion
    models](http://arxiv.org/abs/2112.10752).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach 等人 (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, 和 Björn Ommer. 2022. [High-resolution image synthesis with latent diffusion
    models](http://arxiv.org/abs/2112.10752).
- en: 'Rozière et al. (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre,
    Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish
    Bhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez,
    Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas
    Scialom, and Gabriel Synnaeve. 2024. [Code llama: Open foundation models for code](http://arxiv.org/abs/2308.12950).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rozière 等人 (2024) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez,
    Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian
    Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet,
    Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, 和 Gabriel
    Synnaeve. 2024. [Code llama: Open foundation models for code](http://arxiv.org/abs/2308.12950).'
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
    Norouzi. 2022. [Photorealistic text-to-image diffusion models with deep language
    understanding](http://arxiv.org/abs/2205.11487).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saharia 等人（2022）Chitwan Saharia、William Chan、Saurabh Saxena、Lala Li、Jay Whang、Emily
    Denton、Seyed Kamyar Seyed Ghasemipour、Burcu Karagol Ayan、S. Sara Mahdavi、Rapha
    Gontijo Lopes、Tim Salimans、Jonathan Ho、David J Fleet 和 Mohammad Norouzi。2022年。[具有深度语言理解的超现实文本到图像扩散模型](http://arxiv.org/abs/2205.11487)。
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu,
    Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
    2023. [Toolformer: Language models can teach themselves to use tools](https://openreview.net/forum?id=Yacmpz84TH).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schick 等人（2023）Timo Schick、Jane Dwivedi-Yu、Roberto Dessi、Roberta Raileanu、Maria
    Lomeli、Eric Hambro、Luke Zettlemoyer、Nicola Cancedda 和 Thomas Scialom。2023年。[Toolformer:
    语言模型可以自我教授使用工具](https://openreview.net/forum?id=Yacmpz84TH)。在 *第37届神经信息处理系统会议*。'
- en: 'Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
    Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. [Deepseekmath: Pushing
    the limits of mathematical reasoning in open language models](http://arxiv.org/abs/2402.03300).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等人（2024）Zhihong Shao、Peiyi Wang、Qihao Zhu、Runxin Xu、Junxiao Song、Mingchuan
    Zhang、Y. K. Li、Y. Wu 和 Daya Guo。2024年。[Deepseekmath: 推动开放语言模型中的数学推理极限](http://arxiv.org/abs/2402.03300)。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik R
    Narasimhan, and Shunyu Yao. 2023. [Reflexion: language agents with verbal reinforcement
    learning](https://openreview.net/forum?id=vAElhFcKW6). In *Thirty-seventh Conference
    on Neural Information Processing Systems*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shinn 等人（2023）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik R Narasimhan
    和 Shunyu Yao。2023年。[Reflexion: 具有语言强化学习的语言代理](https://openreview.net/forum?id=vAElhFcKW6)。在
    *第37届神经信息处理系统会议*。'
- en: 'Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi (Jim) Fan, and Anima Anandkumar. 2023. [Voyager: An open-ended
    embodied agent with large language models](https://api.semanticscholar.org/CorpusID:258887849).
    *ArXiv*, abs/2305.16291.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2023）Guanzhi Wang、Yuqi Xie、Yunfan Jiang、Ajay Mandlekar、Chaowei Xiao、Yuke
    Zhu、Linxi（Jim）Fan 和 Anima Anandkumar。2023年。[Voyager: 一个开放式的具身代理，使用大型语言模型](https://api.semanticscholar.org/CorpusID:258887849)。*ArXiv*，abs/2305.16291。'
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022. [Chain-of-thought prompting
    elicits reasoning in large language models](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 24824–24837\.
    Curran Associates, Inc.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、brian ichter、Fei
    Xia、Ed Chi、Quoc V Le 和 Denny Zhou。2022年。[思维链提示引发大型语言模型的推理](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)。在
    *神经信息处理系统进展*，第35卷，第24824–24837页。Curran Associates, Inc.
- en: 'Wei et al. (2023) Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. 2023. [Magicoder: Source code is all you need](http://arxiv.org/abs/2312.02120).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等人（2023）Yuxiang Wei、Zhe Wang、Jiawei Liu、Yifeng Ding 和 Lingming Zhang。2023年。[Magicoder:
    只需源代码](http://arxiv.org/abs/2312.02120)。'
- en: 'Xie et al. (2023) Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng,
    Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, Leo Z. Liu, Yiheng Xu,
    Hongjin Su, Dongchan Shin, Caiming Xiong, and Tao Yu. 2023. [Openagents: An open
    platform for language agents in the wild](http://arxiv.org/abs/2310.10634).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xie 等人（2023）Tianbao Xie、Fan Zhou、Zhoujun Cheng、Peng Shi、Luoxuan Weng、Yitao
    Liu、Toh Jing Hua、Junning Zhao、Qian Liu、Che Liu、Leo Z. Liu、Yiheng Xu、Hongjin Su、Dongchan
    Shin、Caiming Xiong 和 Tao Yu。2023年。[Openagents: 一个开放的平台用于野外语言代理](http://arxiv.org/abs/2310.10634)。'
- en: 'Xu et al. (2023) Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang,
    Weidong Liu, and Yang Liu. 2023. [Exploring large language models for communication
    games: An empirical study on werewolf](http://arxiv.org/abs/2309.04658).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人（2023）Yuzhuang Xu、Shuo Wang、Peng Li、Fuwen Luo、Xiaolong Wang、Weidong Liu
    和 Yang Liu。2023年。[探索大型语言模型在交流游戏中的应用：狼人游戏的实证研究](http://arxiv.org/abs/2309.04658)。
- en: 'Yao et al. (2022) Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan.
    2022. [WebShop: Towards Scalable Real-World Web Interaction with Grounded Language
    Agents](https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 20744–20757\.
    Curran Associates, Inc.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2022年）Shunyu Yao、Howard Chen、John Yang和Karthik Narasimhan。2022年。[WebShop：面向可扩展的真实世界Web交互与基础语言代理](https://proceedings.neurips.cc/paper_files/paper/2022/file/82ad13ec01f9fe44c01cb91814fd7b8c-Paper-Conference.pdf)。发表于*神经信息处理系统进展*，第35卷，第20744-20757页。Curran
    Associates, Inc.
- en: 'Yao et al. (2023a) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a. [Tree of thoughts: Deliberate
    problem solving with large language models](https://openreview.net/forum?id=5Xc1ecxO1h).
    In *Thirty-seventh Conference on Neural Information Processing Systems*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2023a）Shunyu Yao、Dian Yu、Jeffrey Zhao、Izhak Shafran、Thomas L. Griffiths、Yuan
    Cao和Karthik R Narasimhan。2023a。[思维树：与大型语言模型共同进行深思熟虑的问题解决](https://openreview.net/forum?id=5Xc1ecxO1h)。发表于*第三十七届神经信息处理系统会议*。
- en: 'Yao et al. (2023b) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik R Narasimhan, and Yuan Cao. 2023b. [React: Synergizing reasoning and acting
    in language models](https://openreview.net/forum?id=WE_vluYUL-X). In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao等人（2023b）Shunyu Yao、Jeffrey Zhao、Dian Yu、Nan Du、Izhak Shafran、Karthik R Narasimhan和Yuan
    Cao。2023b。[React：在语言模型中协同推理和行动](https://openreview.net/forum?id=WE_vluYUL-X)。发表于*第十一届国际学习表征会议*。
- en: 'Yu et al. (2024) Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying
    Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. [Metamath:
    Bootstrap your own mathematical questions for large language models](https://openreview.net/forum?id=N8N0hgNDRt).
    In *The Twelfth International Conference on Learning Representations*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2024年）Longhui Yu、Weisen Jiang、Han Shi、Jincheng YU、Zhengying Liu、Yu Zhang、James
    Kwok、Zhenguo Li、Adrian Weller和Weiyang Liu。2024年。[Metamath：为大型语言模型自助生成数学问题](https://openreview.net/forum?id=N8N0hgNDRt)。发表于*第十二届国际学习表征会议*。
- en: 'Zhou et al. (2023) Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo,
    Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon,
    and Graham Neubig. 2023. [Webarena: A realistic web environment for building autonomous
    agents](https://openreview.net/forum?id=rmiwIL98uQ). In *Second Agent Learning
    in Open-Endedness Workshop*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人（2023年）Shuyan Zhou、Frank F. Xu、Hao Zhu、Xuhui Zhou、Robert Lo、Abishek Sridhar、Xianyi
    Cheng、Tianyue Ou、Yonatan Bisk、Daniel Fried、Uri Alon和Graham Neubig。2023年。[Webarena：为构建自主代理提供逼真的Web环境](https://openreview.net/forum?id=rmiwIL98uQ)。发表于*第二届开放性学习中代理学习研讨会*。
- en: Appendix A Detailed Prompts
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 详细提示
- en: To better understand MatPlotBench and MatPlotAgent, we list the prompts for
    automatic evaluation and the three modules in MatPlotAgent, including the query
    expansion module, the code agent, and the visual agent.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解MatPlotBench和MatPlotAgent，我们列出了自动评估提示和MatPlotAgent中的三个模块，包括查询扩展模块、代码代理和视觉代理。
- en: A.1 Evaluation Prompts
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 评估提示
- en: The automatic evaluation prompt primarily requires GPT-4V to provide a score
    between 0 and 100 for the model-generated plot, with reference to the ground truth
    plot.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 自动评估提示主要要求GPT-4V为模型生成的图表提供一个0到100之间的分数，并与真实图表进行对比。
- en: '<svg class="ltx_picture" height="390.09" id="A1.F6.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,390.09) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="362.53"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">You are
    an excellent judge at evaluating visualization plots between a model-generated
    plot and the ground truth. You will be giving scores on how well it matches the
    ground truth plot. The generated plot will be given to you as the first figure.
    If the first figure is blank, that means the code failed to generate a figure.
    Another plot will be given to you as the second figure, which is the desired outcome
    of the user query, meaning it is the ground truth for you to reference. Please
    compare the two figures head to head and rate them. Suppose the second figure
    has a score of 100, rate the first figure on a scale from 0 to 100. Scoring should
    be carried out regarding the plot correctness: Compare closely between the generated
    plot and the ground truth, the more resemblance the generated plot has compared
    to the ground truth, the higher the score. The score should be proportionate to
    the resemblance between the two plots. In some rare occurrences, see if the data
    points are generated randomly according to the query, if so, the generated plot
    may not perfectly match the ground truth, but it is correct nonetheless. Only
    rate the first figure, the second figure is only for reference. If the first figure
    is blank, that means the code failed to generate a figure. Give a score of 0 on
    the Plot correctness. After scoring from the above aspect, please give a final
    score. The final score is preceded by the [FINAL SCORE] token. For example [FINAL
    SCORE]: 40.</foreignobject></g></g></svg>'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="390.09" id="A1.F6.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,390.09) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="362.53"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">You are
    an excellent judge at evaluating visualization plots between a model-generated
    plot and the ground truth. You will be giving scores on how well it matches the
    ground truth plot. The generated plot will be given to you as the first figure.
    If the first figure is blank, that means the code failed to generate a figure.
    Another plot will be given to you as the second figure, which is the desired outcome
    of the user query, meaning it is the ground truth for you to reference. Please
    compare the two figures head to head and rate them. Suppose the second figure
    has a score of 100, rate the first figure on a scale from 0 to 100. Scoring should
    be carried out regarding the plot correctness: Compare closely between the generated
    plot and the ground truth, the more resemblance the generated plot has compared
    to the ground truth, the higher the score. The score should be proportionate to
    the resemblance between the two plots. In some rare occurrences, see if the data
    points are generated randomly according to the query, if so, the generated plot
    may not perfectly match the ground truth, but it is correct nonetheless. Only
    rate the first figure, the second figure is only for reference. If the first figure
    is blank, that means the code failed to generate a figure. Give a score of 0 on
    the Plot correctness. After scoring from the above aspect, please give a final
    score. The final score is preceded by the [FINAL SCORE] token. For example [FINAL
    SCORE]: 40.</foreignobject></g></g></svg>'
- en: 'Figure 6: Automatic evaluation prompt for GPT-4V.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：GPT-4V的自动评估提示。
- en: A.2 Prompts for MatPlotAgent
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 MatPlotAgent的提示
- en: 'The query expansion prompt mainly requires LLMs to generate step-by-step, detailed
    instructions on how to use Python code to fulfill the requirements specified by
    users, as shown in Figure [7](https://arxiv.org/html/2402.11453v3#A1.F7 "Figure
    7 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization").'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展提示主要要求LLMs生成逐步详细的说明，讲解如何使用Python代码来满足用户指定的需求，如图[7](https://arxiv.org/html/2402.11453v3#A1.F7
    "图7 ‣ A.2 MatPlotAgent的提示 ‣ 附录A 详细提示 ‣ MatPlotAgent：基于LLM的代理科学数据可视化的方法和评估")所示。
- en: 'For the code agent, there are two prompts for the code generation process and
    the self-debugging mechanism. The code generation prompt mainly requires LLMs
    to generate executable code according to the user query to plot and save the output
    figure, as shown in Figure [8](https://arxiv.org/html/2402.11453v3#A1.F8 "Figure
    8 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization"). The
    self-debugging prompt mainly requires LLMs to correct the buggy code according
    to the error message from a Python interpreter, as displayed in Figure [9](https://arxiv.org/html/2402.11453v3#A1.F9
    "Figure 9 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization").'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '对于代码代理，存在两个提示，一个用于代码生成过程，另一个用于自我调试机制。代码生成提示主要要求大型语言模型（LLMs）根据用户查询生成可执行代码，以绘制并保存输出图形，如图[8](https://arxiv.org/html/2402.11453v3#A1.F8
    "Figure 8 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization")所示。自我调试提示主要要求大型语言模型根据
    Python 解释器的错误消息修正有问题的代码，如图[9](https://arxiv.org/html/2402.11453v3#A1.F9 "Figure
    9 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization")所示。'
- en: 'The visual agent prompt mainly requires multi-modal LLMs to firstly understand
    the user query and analyze the draft plot, then generate the visual feedback to
    refine the draft, as shown in Figure [10](https://arxiv.org/html/2402.11453v3#A1.F10
    "Figure 10 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization").'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉代理提示主要要求多模态大型语言模型首先理解用户查询并分析草图，然后生成视觉反馈以完善草图，如图[10](https://arxiv.org/html/2402.11453v3#A1.F10
    "Figure 10 ‣ A.2 Prompts for MatPlotAgent ‣ Appendix A Detailed Prompts ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization")所示。'
- en: '<svg class="ltx_picture" height="205.9" id="A1.F7.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,205.9) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="178.34"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    According to the user query, expand and solidify the query into a step by step
    detailed instruction (or comment) on how to write python code to fulfill the user
    query’s requirements. Import the appropriate libraries. Pinpoint the correct library
    functions to call and set each parameter in every function call accordingly. USER
    PROMPT: Here is the user query: [User Query]: """ {{query}} """ You should understand
    what the query’s requirements are, and output step by step, detailed instructions
    on how to use python code to fulfill these requirements. Include what libraries
    to import, what library functions to call, how to set the parameters in each function
    correctly, how to prepare the data, how to manipulate the data so that it becomes
    appropriate for later functions to call etc,. Make sure the code to be executable
    and correctly generate the desired output in the user query.</foreignobject></g></g></svg>'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="205.9" id="A1.F7.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,205.9) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="178.34"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    According to the user query, expand and solidify the query into a step by step
    detailed instruction (or comment) on how to write python code to fulfill the user
    query’s requirements. Import the appropriate libraries. Pinpoint the correct library
    functions to call and set each parameter in every function call accordingly. USER
    PROMPT: Here is the user query: [User Query]: """ {{query}} """ You should understand
    what the query’s requirements are, and output step by step, detailed instructions
    on how to use python code to fulfill these requirements. Include what libraries
    to import, what library functions to call, how to set the parameters in each function
    correctly, how to prepare the data, how to manipulate the data so that it becomes
    appropriate for later functions to call etc,. Make sure the code to be executable
    and correctly generate the desired output in the user query.</foreignobject></g></g></svg>'
- en: 'Figure 7: The query expansion prompt in MatPlotAgent.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：MatPlotAgent 中的查询扩展提示。
- en: '<svg class="ltx_picture" height="157.63" id="A1.F8.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.63) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="130.07"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    You are a cutting-edge super capable code generation LLM. You will be given a
    natural language query, generate a runnable python code to satisfy all the requirements
    in the query. You can use any python library you want. When you complete a plot,
    remember to save it to a png file. USER PROMPT: Here is the query: """ {{query}}
    """ If the query requires data manipulation from a csv file, process the data
    from the csv file and draw the plot in one piece of code. When you complete a
    plot, remember to save it to a png file. The file name should be """{{file_name}}""".</foreignobject></g></g></svg>'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="157.63" id="A1.F8.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,157.63) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="130.07"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    You are a cutting-edge super capable code generation LLM. You will be given a
    natural language query, generate a runnable python code to satisfy all the requirements
    in the query. You can use any python library you want. When you complete a plot,
    remember to save it to a png file. USER PROMPT: Here is the query: """ {{query}}
    """ If the query requires data manipulation from a csv file, process the data
    from the csv file and draw the plot in one piece of code. When you complete a
    plot, remember to save it to a png file. The file name should be """{{file_name}}""".</foreignobject></g></g></svg>'
- en: 'Figure 8: The code generation prompt in MatPlotAgent.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：MatPlotAgent 中的代码生成提示。
- en: '<svg class="ltx_picture" height="73.07" id="A1.F9.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="45.51"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">USER PROMPT:
    There are some errors in the code you gave: {{error_message}} please correct the
    errors. Then give the complete code and don’t omit anything even though you have
    given it in the above code.</foreignobject></g></g></svg>'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="73.07" id="A1.F9.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,73.07) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="45.51"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">USER PROMPT:
    There are some errors in the code you gave: {{error_message}} please correct the
    errors. Then give the complete code and don’t omit anything even though you have
    given it in the above code.</foreignobject></g></g></svg>'
- en: 'Figure 9: The self-debugging prompt in MatPlotAgent.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：MatPlotAgent 中的自我调试提示。
- en: '<svg class="ltx_picture" height="273.86" id="A1.F10.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,273.86) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="246.3"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    Given a user query and an image of the current plot, please determine whether
    the plot has faithfully followed the user query. Your task is to provide instruction
    to make sure the plot has strictly completed the requirements of the query. Please
    output a detailed step by step instruction on how to use python code to enhance
    the plot. USER PROMPT: Here is the user query: [Query]: """ {{query}} """ Carefully
    read and analyze the user query to understand the specific requirements. Check
    if the plot aligns with the user query in terms of data selection, plot type,
    and any specific customization. Look at the provided image of the plot. Assess
    the plot type, the data it represents, labels, titles, colors, and any other visual
    elements. Compare these elements with the requirements specified in the user query.
    Note any differences between the user query requirements and the current plot.
    Based on the identified discrepancies, provide step-by-step instructions on how
    to modify the Python code to meet the user query requirements. Suggest improvements
    for better visualization practices, such as clarity, readability, and aesthetics,
    while ensuring the primary focus is on meeting the user’s specified requirements.
    Remember to save the plot to a png file. The file name should be """{{file_name}}"""</foreignobject></g></g></svg>'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="273.86" id="A1.F10.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,273.86) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="246.3"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">SYSTEM PROMPT:
    Given a user query and an image of the current plot, please determine whether
    the plot has faithfully followed the user query. Your task is to provide instruction
    to make sure the plot has strictly completed the requirements of the query. Please
    output a detailed step by step instruction on how to use python code to enhance
    the plot. USER PROMPT: Here is the user query: [Query]: """ {{query}} """ Carefully
    read and analyze the user query to understand the specific requirements. Check
    if the plot aligns with the user query in terms of data selection, plot type,
    and any specific customization. Look at the provided image of the plot. Assess
    the plot type, the data it represents, labels, titles, colors, and any other visual
    elements. Compare these elements with the requirements specified in the user query.
    Note any differences between the user query requirements and the current plot.
    Based on the identified discrepancies, provide step-by-step instructions on how
    to modify the Python code to meet the user query requirements. Suggest improvements
    for better visualization practices, such as clarity, readability, and aesthetics,
    while ensuring the primary focus is on meeting the user’s specified requirements.
    Remember to save the plot to a png file. The file name should be """{{file_name}}"""</foreignobject></g></g></svg>'
- en: 'Figure 10: Prompt for the visual agent.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：视觉代理的提示。
- en: Appendix B Human Evaluation Details
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 人工评估详情
- en: We engage human annotators from computer science departments at various universities
    via social media. They are compensated for their work at a rate slightly higher
    than the prevailing market rate. All human annotators involved are informed that
    the collected data will be used solely for academic research purposes, and their
    personal information will not be disclosed.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过社交媒体联系来自各大学计算机科学系的人工标注员。我们为他们提供的报酬稍高于市场普遍水平。所有参与的人工标注员都已被告知，收集的数据将仅用于学术研究目的，且他们的个人信息不会被公开。
- en: B.1 Evaluation Guide for Human Annotators
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 人工标注员评估指南
- en: 'Figure [11](https://arxiv.org/html/2402.11453v3#A2.F11 "Figure 11 ‣ B.1 Evaluation
    Guide for Human Annotators ‣ Appendix B Human Evaluation Details ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization") gives
    detailed instructions for human annotators when scoring the model-generated plots.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](https://arxiv.org/html/2402.11453v3#A2.F11 "Figure 11 ‣ B.1 Evaluation
    Guide for Human Annotators ‣ Appendix B Human Evaluation Details ‣ MatPlotAgent:
    Method and Evaluation for LLM-Based Agentic Scientific Data Visualization")为人工标注员在评分模型生成的图形时提供了详细的指导。'
- en: '<svg class="ltx_picture" height="287.47" id="A2.F11.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,287.47) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="240.46"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Plot Correctness
    (0-100 points) • Exact Match (90-100 points): The generated plot is nearly identical
    to the ground truth, with only minor, negligible differences. • High Resemblance
    (70-89 points): The generated plot closely resembles the ground truth with some
    small but noticeable differences in data representation or styling. • Moderate
    Resemblance (50-69 points): The generated plot has a moderate level of similarity
    to the ground truth, but there are several noticeable differences that impact
    the plot’s accuracy or interpretation. • Low Resemblance (30-49 points): The generated
    plot shares some similarities with the ground truth but has significant differences
    that change the overall message or interpretation of the data. • Poor Match (10-29
    points): The generated plot has very little in common with the ground truth, with
    major discrepancies in data representation. • No Resemblance (1-9 points): The
    generated plot is completely different from the ground truth, with no discernible
    similarities in data representation. • Failure to Generate (0 points): The first
    figure is blank, indicating a failure to generate any plot. Special Considerations
    • In cases where the generated plot includes random data points that are correct
    in the context of the query, the plot should be evaluated for its correctness
    based on the query’s intent, not solely on its visual match to the ground truth.
    [FINAL SCORE]: XX</foreignobject></g></g></svg>'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="287.47" id="A2.F11.pic1" overflow="visible"
    version="1.1" width="600"><g color="#000000" fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,287.47) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.000000"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject height="240.46"
    overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Plot Correctness
    (0-100 points) • Exact Match (90-100 points): The generated plot is nearly identical
    to the ground truth, with only minor, negligible differences. • High Resemblance
    (70-89 points): The generated plot closely resembles the ground truth with some
    small but noticeable differences in data representation or styling. • Moderate
    Resemblance (50-69 points): The generated plot has a moderate level of similarity
    to the ground truth, but there are several noticeable differences that impact
    the plot’s accuracy or interpretation. • Low Resemblance (30-49 points): The generated
    plot shares some similarities with the ground truth but has significant differences
    that change the overall message or interpretation of the data. • Poor Match (10-29
    points): The generated plot has very little in common with the ground truth, with
    major discrepancies in data representation. • No Resemblance (1-9 points): The
    generated plot is completely different from the ground truth, with no discernible
    similarities in data representation. • Failure to Generate (0 points): The first
    figure is blank, indicating a failure to generate any plot. Special Considerations
    • In cases where the generated plot includes random data points that are correct
    in the context of the query, the plot should be evaluated for its correctness
    based on the query’s intent, not solely on its visual match to the ground truth.
    [FINAL SCORE]: XX</foreignobject></g></g></svg>'
- en: 'Figure 11: Evaluation guide for human annotators when scoring the model-generated
    plots.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：人工标注员在评分模型生成图形时的评估指南。
