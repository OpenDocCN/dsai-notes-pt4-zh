- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:28:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:28:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越数值奖项：基于上下文的决斗强盗问题与大型语言模型代理
- en: 来源：[https://arxiv.org/html/2407.01887/](https://arxiv.org/html/2407.01887/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2407.01887/](https://arxiv.org/html/2407.01887/)
- en: \pdfcolInitStack
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \pdfcolInitStack
- en: tcb@breakable
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: tcb@breakable
- en: Fanzeng Xia fanzengxia@link.cuhk.edu.cn, litongxin@cuhk.edu.cn Hao Liu {hliu3,
    yyue}@caltech.edu Yisong Yue {hliu3, yyue}@caltech.edu Tongxin Li fanzengxia@link.cuhk.edu.cn,
    litongxin@cuhk.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Fanzeng Xia fanzengxia@link.cuhk.edu.cn, litongxin@cuhk.edu.cn Hao Liu {hliu3,
    yyue}@caltech.edu Yisong Yue {hliu3, yyue}@caltech.edu Tongxin Li fanzengxia@link.cuhk.edu.cn,
    litongxin@cuhk.edu.cn
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In-context reinforcement learning (ICRL) is a frontier paradigm for solving
    reinforcement learning problems in the foundation model era. While ICRL capabilities
    have been demonstrated in transformers through task-specific training, the potential
    of Large Language Models (LLMs) out-of-the-box remains largely unexplored. Recent
    findings highlight that LLMs often face challenges when dealing with numerical
    contexts, and limited attention has been paid to evaluating their performance
    through preference feedback generated by the environment. This paper is the first
    to investigate LLMs as in-context decision-makers under the problem of Dueling
    Bandits (DB), a stateless preference-based reinforcement learning setting that
    extends the classic Multi-Armed Bandit (MAB) model by querying for preference
    feedback. We compare GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-preview
    against nine well-established DB algorithms. Our results reveal that our top-performing
    LLM, GPT-4 Turbo, has the zero-shot relative decision-making ability to achieve
    surprisingly low short-term weak regret across all the DB environment instances
    by quickly including the best arm in duels. However, an optimality gap exists
    between LLMs and classic DB algorithms in terms of strong regret. LLMs struggle
    to converge and consistently exploit even when explicitly prompted to do so, and
    are sensitive to prompt variations. To bridge this gap, we propose an agentic
    flow framework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates
    off-the-shelf DB algorithms with LLM agents through fine-grained adaptive interplay.
    We show that LEAD has theoretical guarantees inherited from classic DB algorithms
    on both weak and strong regret. We validate its efficacy and robustness even with
    noisy and adversarial prompts. The design of such an agentic framework sheds light
    on how to enhance the trustworthiness of general-purpose LLMs used for in-context
    decision-making tasks.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上下文的强化学习（ICRL）是解决基础模型时代强化学习问题的前沿范式。尽管ICRL能力已通过任务特定训练在变换器中得到了验证，但大型语言模型（LLMs）开箱即用的潜力仍未得到充分探索。近期研究发现，当面对数值上下文时，LLMs常常面临挑战，而且在通过环境生成的偏好反馈来评估其性能方面关注较少。本文首次探讨了LLMs在决斗强盗问题（DB）中的作为上下文决策者的表现，DB是一种无状态的基于偏好的强化学习设置，通过查询偏好反馈扩展了经典的多臂强盗（MAB）模型。我们将GPT-3.5
    Turbo、GPT-4、GPT-4 Turbo、Llama 3.1和o1-preview与九个成熟的DB算法进行比较。我们的结果显示，表现最好的LLM——GPT-4
    Turbo，具有零样本相对决策能力，能够通过快速在对决中选出最佳臂，令人惊讶地在所有DB环境实例中实现低短期弱悔过。然而，LLMs与经典DB算法在强悔过方面存在最优性差距。即便在明确提示其进行此类操作时，LLMs也难以收敛并持续利用，而且对提示的变化非常敏感。为弥补这一差距，我们提出了一种代理流框架：增强算法决斗（LEAD），通过精细的自适应互动将现成的DB算法与LLM代理结合。我们展示了LEAD在弱悔过和强悔过方面继承了经典DB算法的理论保证。我们验证了它在嘈杂和对抗性提示下的有效性和稳健性。这种代理框架的设计为如何增强用于上下文决策任务的通用LLM的可信度提供了新的视角。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large sequence models pretrained with task specific interactive datasets have
    led to the emergence of in-context reinforcement learning (ICRL) (Laskin et al.,
    [2022](https://arxiv.org/html/2407.01887v3#bib.bib1); Lee et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib2)),
    where models can infer tasks from interaction histories as context and make effective
    decisions in unseen environments without parameter updates. Through trial and
    error, these models can self-improve their policies purely in-context. While ICRL
    capabilities have been demonstrated in transformers with task-specific training
    from scratch, the potential of general-purpose Large Language Models (LLMs) to
    perform ICRL remains largely unexplored. Recent investigations into LLMs’ out-of-the-box
    ICRL capabilities in environments with numeric rewards have reported notable failure
    cases, e.g., LLM agents being vulnerable to adversarial loss functions and suffering
    from high regret compared to classic algorithms such as Follow-The-Regularized-Leader
    (FTRL) (Park et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib3)), and
    exhibiting failures in exploration within Multi-Armed Bandit (MAB) problems via
    standard training (Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)).
    Even with inference-time algorithmic guidance, an optimality gap persists between
    LLMs and classic (contextual) MAB algorithms (Nie et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib5)).
    These results suggest that carefully designed prompts and non-trivial algorithmic
    interventions are needed to elicit desirable in-context reinforcement learning
    behavior of LLM agents.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任务特定互动数据集预训练的大型序列模型导致了上下文强化学习（ICRL）的出现（Laskin等，[2022](https://arxiv.org/html/2407.01887v3#bib.bib1);
    Lee等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib2)），在这种方法中，模型可以通过交互历史作为上下文推断任务，并在没有参数更新的情况下在未见过的环境中做出有效决策。通过试错，这些模型可以在纯粹的上下文中自我改进它们的策略。虽然ICRL能力已经在从头开始训练的任务特定变压器中得到了验证，但通用大型语言模型（LLMs）执行ICRL的潜力仍然大多未被探索。最近关于LLMs在具有数值奖励的环境中“开箱即用”ICRL能力的研究报告了显著的失败案例，例如，LLM代理易受对抗性损失函数的影响，并且与经典算法如跟随正则化领导者（FTRL）（Park等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib3)）相比，遭遇较高的遗憾，并且在通过标准训练的多臂赌博机（MAB）问题中出现探索失败（Krishnamurthy等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4)）。即使在推理时采用算法指导，LLMs与经典（上下文）MAB算法之间的最优性差距仍然存在（Nie等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib5)）。这些结果表明，需要精心设计的提示和非平凡的算法干预，以引导LLM代理实现期望的上下文强化学习行为。
- en: 'The failure cases encountered by LLMs may be attributed to intrinsic difficulties
    in processing numeric rewards, especially in tasks where patterns are difficult
    to express in natural language. Recent findings have pointed out that LLMs often
    struggle with simple numerical comparisons (e.g., incorrectly judging 13.11 to
    be larger than 13.8), and there has been a notable lack of emphasis on evaluating
    the relative comparisons among the decisions they generate. Figure [1](https://arxiv.org/html/2407.01887v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") shows a toy example illustrating the in-context interaction
    between an LLM agent and different environment settings. To disentangle the complexities
    introduced by numerical rewards, this paper focuses on the problem of Dueling
    Bandits (DB)(Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6);
    Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)), a stateless
    preference-based reinforcement learning setting (Wirth et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib8);
    Pacchiano et al., [2021](https://arxiv.org/html/2407.01887v3#bib.bib9)) that extends
    the classic MAB model by querying for preference feedback between selected pairs
    of arms to identify the best one. In DB, the agent learns through binary outcome
    (win or lose) of a noisy comparison between the two selected arms. This setup
    is particularly useful when eliciting explicit feedback is challenging or when
    the feedback is inherently comparative, like taste of food and product attractiveness(Yue
    et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)). DB has attracted
    significant attention due to its applicability in information retrieval (Yue and
    Joachims, [2009](https://arxiv.org/html/2407.01887v3#bib.bib10)), recommendation
    systems (Sui et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib11)), and
    online ranker evaluation (Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)).
    We frame our investigation with the following question:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs遇到的失败案例可能归因于处理数值奖励时的内在困难，特别是在那些模式难以用自然语言表达的任务中。近期的研究发现，LLMs通常在进行简单的数值比较时遇到困难（例如，错误地判断13.11比13.8大），而且在评估它们生成的决策之间的相对比较时，缺乏足够的重视。[图1](https://arxiv.org/html/2407.01887v3#S1.F1
    "图1 ‣ 1 引言 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博机")展示了一个玩具示例，说明了LLM代理与不同环境设置之间的上下文互动。为了剖析数值奖励带来的复杂性，本文聚焦于对抗赌博机（Dueling
    Bandits，DB）问题（Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6); Zoghi等人，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)），这是一种无状态的基于偏好的强化学习设置（Wirth等人，[2017](https://arxiv.org/html/2407.01887v3#bib.bib8);
    Pacchiano等人，[2021](https://arxiv.org/html/2407.01887v3#bib.bib9)），它通过查询所选的臂对之间的偏好反馈来确定最佳臂，扩展了经典的多臂赌博机（MAB）模型。在DB中，代理通过二元结果（胜或负）来学习两个选定臂之间的噪声比较。该设置在挑战性较大的情况下特别有用，尤其是当获取明确反馈变得困难，或者反馈本身就是比较性的，比如食物的口味和产品的吸引力（Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)）。DB因其在信息检索（Yue和Joachims，[2009](https://arxiv.org/html/2407.01887v3#bib.bib10)）、推荐系统（Sui等人，[2017](https://arxiv.org/html/2407.01887v3#bib.bib11)）和在线排序评估（Zoghi等人，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)）中的应用而受到广泛关注。我们通过以下问题框定我们的研究：
- en: Are LLMs effective in-context agents for solving the problem of dueling bandits?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在解决对抗赌博机问题（dueling bandits）时，作为上下文代理是否有效？
- en: The DB problem poses distinctive challenges as a relative decision-making instance,
    particularly due to the sparse nature of the relative rewards. This sparsity complicates
    the in-context decision-making process, as it restricts the feedback obtained
    from interactions, introducing a level of difficulty not typically seen in conventional
    bandit problems. Even though reduction from DB to standard MAB exists (Ailon et al.,
    [2014](https://arxiv.org/html/2407.01887v3#bib.bib12); Saha and Gaillard, [2022](https://arxiv.org/html/2407.01887v3#bib.bib13)),
    it remains unclear how LLMs would perform in DB with preference feedback rather
    than numeric rewards. There are conceptual differences between them, similar to
    those between Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al.,
    [2020](https://arxiv.org/html/2407.01887v3#bib.bib14)) and standard RL, where
    impossibility results can be found in(Wang et al., [2024a](https://arxiv.org/html/2407.01887v3#bib.bib15)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: DB 问题作为一种相对决策问题，带来了独特的挑战，特别是由于相对奖励的稀疏性。这种稀疏性使得上下文中的决策过程变得更加复杂，因为它限制了通过交互获得的反馈，增加了一个在传统赌博机问题中通常看不到的难度。尽管存在将
    DB 问题降至标准 MAB 的方法（Ailon 等， [2014](https://arxiv.org/html/2407.01887v3#bib.bib12);
    Saha 和 Gaillard，[2022](https://arxiv.org/html/2407.01887v3#bib.bib13)），但尚不清楚 LLMs
    在面对偏好反馈而非数值奖励的 DB 问题时会表现如何。它们之间存在概念性差异，类似于强化学习中的人类反馈（RLHF）（Stiennon 等，[2020](https://arxiv.org/html/2407.01887v3#bib.bib14)）和标准
    RL 之间的差异，其中可以在(Wang 等，[2024a](https://arxiv.org/html/2407.01887v3#bib.bib15))中找到不可能的结果。
- en: While task-specific training of large sequence models can yield promising ICRL
    results, it is often impractical due to the substantial computational resources
    required. Similar to the settings in (Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4);
    Nie et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib5); Mirchandani
    et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib16); Chen et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib17)),
    we evaluate the emergent zero-shot abilities (Wei et al., [2022](https://arxiv.org/html/2407.01887v3#bib.bib18))
    of ICRL in general-purpose LLMs under the dueling bandit problem, without re-training
    or fine-tuning. We summarize our main results below.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大规模序列模型的任务特定训练可以产生有前景的 ICRL 结果，但由于需要大量计算资源，这种方法往往不切实际。类似于(Krishnamurthy 等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4);
    Nie 等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib5); Mirchandani 等，[2023](https://arxiv.org/html/2407.01887v3#bib.bib16);
    Chen 等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib17))中的设置，我们评估了 ICRL 在对抗赌博机问题中的零-shot
    能力（Wei 等，[2022](https://arxiv.org/html/2407.01887v3#bib.bib18)），而无需重新训练或微调。我们在下面总结了我们的主要结果。
- en: '![Refer to caption](img/dbd5de7a1113e13dbb13fb81c8012187.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dbd5de7a1113e13dbb13fb81c8012187.png)'
- en: 'Figure 1: In-context reinforcement learning of an LLM agent with numeric rewards
    (in a multi-armed bandit environment) and preference feedback (in a dueling bandit
    environment).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：LLM 智能体在数值奖励（多臂赌博机环境）和偏好反馈（对抗赌博机环境）下的上下文强化学习。
- en: Evaluation of LLMs’ Emergent Zero-Shot Abilities of In-Context Dueling Bandits.
    We go beyond numeric rewards to evaluate the performance of LLM agents in terms
    of both strong and weak regret for making decisions in DB by comparing against
    various baseline DB algorithms via a case study. We found that the top-performing
    general-purpose LLMs has the zero-shot ability for relative decision-making sufficient
    to achieve low weak regret in DB, which significantly differs from that in classic
    MAB settings(Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)).
    Notably, GPT-4 Turbo can serve as an effective decision-maker for dueling bandits
    in terms of weak regret, quickly selecting the best arm in duels with low variance
    across a range of instances. However, consistent with(Nie et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib5)),
    we found that an optimality gap exists between LLMs and classic DB algorithms
    in terms of strong regret. LLMs’ performance is hindered by over-estimation bias
    in the exploration stage and lack of convergence criterion in the exploitation
    stage. This highlights the need for more effective and robust strategies to bridge
    this gap for in-context dueling bandits.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM在上下文对抗赌博中的新兴零-shot能力。我们不仅仅关注数值奖励，还通过案例研究，将LLM代理的强后悔与弱后悔的决策表现与各种基线对抗赌博算法进行了比较。我们发现，表现最好的通用LLM具有足够的零-shot相对决策能力，可以在对抗赌博中实现较低的弱后悔，这与经典的多臂赌博机设置中的表现大不相同（Krishnamurthy等人，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4)）。特别地，GPT-4
    Turbo在弱后悔方面表现出色，能够在对抗中快速选择最优臂，且在不同实例中波动较小，成为一个有效的对抗赌博决策者。然而，与(Nie等人，[2024](https://arxiv.org/html/2407.01887v3#bib.bib5))一致，我们发现LLM与经典对抗赌博算法在强后悔方面存在最优性差距。LLM的表现受限于探索阶段的过度估计偏差，以及利用阶段缺乏收敛标准。这突显了弥合这一差距的有效且稳健策略的需求，以应对上下文对抗赌博问题。
- en: 'Effective and Robust Agentic Flow Framework for In-Context Dueling Bandits.
    To address the identified optimality gap and enhance the trustworthiness of in-context
    LLM agents in DB tasks, in Section[4.1](https://arxiv.org/html/2407.01887v3#S4.SS1
    "4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), we propose
    an agentic flow framework, LLM with Enhanced Algorithmic Dueling (LEAD) that integrates
    off-the-shelf Explore-then-Exploit DB algorithms with LLM agents. This framework
    enables the fine-grained adaptive interplay between rule-based expert systems
    and in-context LLM agents, enhancing their ability to handle DB problems via algorithmic
    interventions as suggested in (Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4);
    Nie et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib5)). As an illustrative
    example, we demonstrate how Interleaved Filter2 (IF2) algorithm can be incorporated
    with LLM agents in this framework. We show that LEAD has theoretical guarantees,
    with experiments demonstrating its efficacy and robustness across various prompting
    scenarios.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '针对上下文对抗赌博问题（In-Context Dueling Bandits）提出的有效且稳健的代理流框架。为了解决已识别的最优性差距，并增强LLM代理在对抗赌博任务中的可信度，在第[4.1节](https://arxiv.org/html/2407.01887v3#S4.SS1
    "4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")中，我们提出了一个代理流框架——增强算法对抗赌博的LLM（LEAD），该框架将现成的“先探索后利用”对抗赌博算法与LLM代理结合。该框架使基于规则的专家系统与上下文LLM代理之间能够进行细粒度的适应性交互，通过算法干预增强其处理对抗赌博问题的能力，正如(Krishnamurthy等人，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4);
    Nie等人，[2024](https://arxiv.org/html/2407.01887v3#bib.bib5))所建议的那样。作为一个示例，我们演示了如何将交错滤波器2（IF2）算法与LLM代理在此框架中结合。我们表明，LEAD具有理论保证，并通过实验展示了其在各种提示场景中的有效性和稳健性。'
- en: 2 Preliminaries
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 前提条件
- en: 'In this section, we briefly introduce the problem of dueling bandits (DB) and
    establish the necessary notation for this paper. Additional useful definitions
    can be found in Appendix [B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '本节中，我们简要介绍了对抗赌博问题（DB），并为本文建立了必要的符号和术语。更多有用的定义可以在附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")中找到。'
- en: 'Dueling Bandits. In a fundamental $K$-armed dueling bandit problem setting (Yue
    et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)), a learner interacts
    with the environment by selecting two arms $\mathsf{Arm}_{1}(t)$ and $\mathsf{Arm}_{2}(t)$
    from a set of $K$ arms $\{b_{1},\ldots,b_{K}\}$ for a noisy comparison (a duel),
    at each round $t\in\{1,\ldots,T\}$ as Figure[1](https://arxiv.org/html/2407.01887v3#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") illustrates. The outcome of a duel between two arms $(i,j)$
    is probabilistic. More precisely, the event that an arm $b_{i}$ wins against $b_{j}$
    is a Bernoulli random variable with a parameter denoted by $\Pr(b_{i}\succ b_{j})$.
    For notational convenience, we normalize $\Pr(b_{i}\succ b_{j})$ such that $\Pr(b_{i}\succ
    b_{j})=\epsilon(b_{i},b_{j})+1/2$, where $\epsilon_{ij}\coloneq\epsilon(b_{i},b_{j})\in(-1/2,1/2)$
    is a measure of the distinguishability between arms $b_{i}$ and $b_{j}$, which
    is stationary over time and is symmetric such that $\epsilon_{ij}=-\epsilon_{ji}$
    for all $i,j\in[K]\coloneq\{1,\ldots,K\}$. Finally, for notational convenience,
    we define a preference matrix $P=\left[\epsilon_{ij}\right]_{i,j\in[K]}$.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗强盗。在一个基本的$K$臂对抗强盗问题设置中（Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)），学习者通过在每一轮$t\in\{1,\ldots,T\}$选择两个臂$\mathsf{Arm}_{1}(t)$和$\mathsf{Arm}_{2}(t)$，从$K$个臂的集合$\{b_{1},\ldots,b_{K}\}$中进行有噪声的比较（即对抗），如图[1](https://arxiv.org/html/2407.01887v3#S1.F1
    "图 1 ‣ 1 引言 ‣ 超越数值奖励：在上下文中使用LLM代理进行对抗强盗")所示。两个臂$(i,j)$之间的对抗结果是随机的。更准确地说，臂$b_{i}$击败臂$b_{j}$的事件是一个伯努利随机变量，其参数表示为$\Pr(b_{i}\succ
    b_{j})$。为了方便记号，我们对$\Pr(b_{i}\succ b_{j})$进行归一化，使得$\Pr(b_{i}\succ b_{j})=\epsilon(b_{i},b_{j})+1/2$，其中$\epsilon_{ij}\coloneq\epsilon(b_{i},b_{j})\in(-1/2,1/2)$是臂$b_{i}$和$b_{j}$之间的可区分度度量，该度量在时间上是稳定的，并且是对称的，即对于所有$i,j\in[K]\coloneq\{1,\ldots,K\}$，都有$\epsilon_{ij}=-\epsilon_{ji}$。最后，为了方便记号，我们定义了一个偏好矩阵$P=\left[\epsilon_{ij}\right]_{i,j\in[K]}$。
- en: 'In-Context LLM Agents for Dueling Bandits. We consider an LLM agent with policy
    $\pi_{\mathrm{LLM}}$ interacting with a $K$-armed dueling bandit environment in-context.
    At each round $t\in\{1,\ldots,T\}$, the LLM agent selects a pair of arms $(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t))$
    from the set $\{b_{1},\ldots,b_{K}\}$ based on a natural language instruction
    $\mathtt{Prompt}(C,H_{t},R)$ (see Figure[7](https://arxiv.org/html/2407.01887v3#A3.F7
    "Figure 7 ‣ C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), consisting of three parts:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文中使用LLM代理进行对抗强盗。我们考虑一个具有策略$\pi_{\mathrm{LLM}}$的LLM代理与$K$臂对抗强盗环境进行交互。在每一轮$t\in\{1,\ldots,T\}$，LLM代理基于自然语言指令$\mathtt{Prompt}(C,H_{t},R)$（见图[7](https://arxiv.org/html/2407.01887v3#A3.F7
    "图 7 ‣ C.1.2 提示设计 ‣ C.1 LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：在上下文中使用LLM代理进行对抗强盗")）从集合$\{b_{1},\ldots,b_{K}\}$中选择一对臂$(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t))$，该指令由三部分组成：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Problem Description $P$: a natural language description of the DB problem,
    including the number of arms $K$, the time horizon $T$, and the task objective.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题描述$P$：对DB问题的自然语言描述，包括臂的数量$K$，时间范围$T$和任务目标。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'History $H_{t}$: an externally summarized interaction history (Krishnamurthy
    et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)) up to round $t$,
    which includes a sequence of pairwise dueling results and the empirical probabilities.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 历史$H_{t}$：一个外部总结的交互历史（Krishnamurthy等人，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4)），包括到第$t$轮为止的一系列成对对抗结果和经验概率。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning $R$: the zero-shot chain-of-thought (CoT) reasoning (Kojima et al.,
    [2022](https://arxiv.org/html/2407.01887v3#bib.bib19)) that encourages the LLM
    agent to reason about the problem in a structured manner.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理$R$：零-shot链式思维（CoT）推理（Kojima等人，[2022](https://arxiv.org/html/2407.01887v3#bib.bib19)），鼓励LLM代理以结构化的方式推理问题。
- en: 'The LLM agent’s policy can be represented as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: LLM代理的策略可以表示为：
- en: '|  | $\displaystyle\left(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t)\right)=\pi_{\mathrm%
    {LLM}}\left(\mathtt{Prompt}(P,H_{t},R)\right).$ |  | (1) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left(\mathsf{Arm}_{1}(t),\mathsf{Arm}_{2}(t)\right)=\pi_{\mathrm{LLM}}\left(\mathtt{Prompt}(P,H_{t},R)\right).$
    |  | (1) |'
- en: 'The goal is to maximize the cumulative reward over some time horizon $T$, where
    the reward is the sum of the unknown probabilities of the two chosen arms beating
    the best arm (Condorcet winner). We can quantify performance as minimizing the
    cumulative regret, either in the strong or weak sense (see Eq.([4](https://arxiv.org/html/2407.01887v3#A2.E4
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) and Eq.([5](https://arxiv.org/html/2407.01887v3#A2.E5
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"))).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '目标是在某个时间范围$T$内最大化累积奖励，其中奖励是两个选定手臂击败最佳手臂（孔多塞赢家）未知概率的总和。我们可以通过最小化累积后悔来量化性能，无论是强后悔还是弱后悔（参见公式([4](https://arxiv.org/html/2407.01887v3#A2.E4
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"))和公式([5](https://arxiv.org/html/2407.01887v3#A2.E5
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")))。'
- en: 'Strong and Weak Regret. Throughout this paper, we assume the standard setting
    that a Condorcet winner (CW) exists(Sui et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib11);
    Wu and Liu, [2016](https://arxiv.org/html/2407.01887v3#bib.bib20); Zoghi et al.,
    [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7); Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)).
    The CW denoted as $b^{*}$ is an arm that is preferred over all the other arms,
    i.e., $b^{*}=b_{i}$ if $\epsilon_{ij}>1/2$ for all $j\in[K]\backslash\{i\}$. We
    consider two performance metrics: (i) strong regret (SR), which evaluates the
    total preference gap between $b^{*}$ and both selected arms; (ii) weak regret
    (WR), which compares $b^{*}$ only with the better of the two arms. Detailed definitions
    and settings are provided in Appendix[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '强后悔与弱后悔。在本文中，我们假设存在一个标准设定，即存在一个**孔多塞赢家（Condorcet winner, CW）**（Sui等，[2017](https://arxiv.org/html/2407.01887v3#bib.bib11)；Wu和Liu，[2016](https://arxiv.org/html/2407.01887v3#bib.bib20)；Zoghi等，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)；Yue等，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)）。CW表示为$b^{*}$，是一个优于所有其他手臂的臂，即$b^{*}=b_{i}$，如果对所有$j\in[K]\backslash\{i\}$都有$\epsilon_{ij}>1/2$。我们考虑两种性能指标：（i）强后悔（SR），它评估$b^{*}$与两个选定手臂之间的总偏好差距；（ii）弱后悔（WR），它仅将$b^{*}$与两个手臂中更好的一个进行比较。详细的定义和设定请参见附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")。'
- en: 'Related Works. Our work contributes to the growing community of intersection
    between LLMs and decision-making. We summarize the detailed related works about
    dueling bandits, LLM agents for bandits, and LLMs for in-context decision-making
    in the Appendix [A](https://arxiv.org/html/2407.01887v3#A1 "Appendix A Related
    Works ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '相关工作。我们的工作为日益增长的**大语言模型（LLMs）与决策制定的交集领域**作出了贡献。我们在附录[A](https://arxiv.org/html/2407.01887v3#A1
    "Appendix A Related Works ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")中总结了有关对抗性赌徒、LLM代理用于赌徒问题以及LLM在上下文决策中的应用的详细相关工作。'
- en: 3 LLMs as Standalone In-Context Decision-Makers
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM作为独立的上下文决策者
- en: 'To evaluate the LLMs’ efficacy for solving DB problems in-context, in this
    section, we use LLMs as standalone decision-making agents and compare them with
    classic DB algorithms. Our evaluation is two-fold: First, in Figures [2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") and[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), we compare the performance of LLMs
    and classic algorithms in terms of the strong and weak regret (see Eq.([4](https://arxiv.org/html/2407.01887v3#A2.E4
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) and Eq.([5](https://arxiv.org/html/2407.01887v3#A2.E5
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")), with standard deviation). Second,
    we delve into the experimental results and analyze the success and failure modes
    of LLM agents.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估LLMs在上下文中解决DB问题的效能，本节中，我们将LLMs作为独立的决策代理，并与经典的DB算法进行比较。我们的评估分为两部分：首先，在图[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")和[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")中，我们将LLMs和经典算法在强后悔和弱后悔方面的表现进行比较（见公式([4](https://arxiv.org/html/2407.01887v3#A2.E4
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")）和公式([5](https://arxiv.org/html/2407.01887v3#A2.E5
    "In B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical
    Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")），并考虑标准差）。其次，我们深入分析实验结果，探讨LLM代理的成功与失败模式。'
- en: 3.1 Implementation Details of Experiments
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验实施细节
- en: 'Prompts and Configurations of LLMs. We employ an interactive zero-shot chain-of-thought
    (CoT) prompt $\mathtt{Prompt}(P,H_{t},R)$, as defined in Section [2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"), which describes the problem setting $P$, externally summarized interaction
    history $H_{t}$ and reasoning instructions $R$. We adopt the prompting template
    and LLM configurations that lead to the best performance among all prompt variations
    explored in recent studies (Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4);
    Nie et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib5)) for MAB problem.
    The LLM agents interact with dueling bandit environments in a round-based manner,
    with the prompt guiding their decision-making process. We conduct experiments
    with five LLMs: GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-preview.
    Note that we skip the GPT-4o version which is primarily developed for multimodal
    tasks and has the same intelligence as GPT-4 Turbo. The detailed prompt is provided
    in Appendix [C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2 "C.1.2 Design
    of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs 的提示和配置。我们采用了一个交互式零-shot链式思维（CoT）提示$\mathtt{Prompt}(P,H_{t},R)$，如第[2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")节所定义，该提示描述了问题设置$P$、外部总结的交互历史$H_{t}$和推理指令$R$。我们采用了在最近的研究中（Krishnamurthy等，
    [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)；Nie等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib5)）中所有提示变体中表现最好的提示模板和LLM配置，用于MAB问题。LLM代理以回合制的方式与决斗强盗环境进行交互，提示引导它们的决策过程。我们进行了五个LLM的实验：GPT-3.5
    Turbo、GPT-4、GPT-4 Turbo、Llama 3.1 和 o1-preview。请注意，我们跳过了GPT-4o版本，它主要用于多模态任务，且其智能水平与GPT-4
    Turbo相同。详细的提示设计见附录[C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2 "C.1.2
    Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")。'
- en: 'Baselines. We compare LLMs against nine well-established baseline algorithms
    to evaluate their efficacy. The baselines include Interleaved Filter (IF2) (Yue
    et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)), Beat the Mean (BTM) (Yue
    and Joachims, [2011](https://arxiv.org/html/2407.01887v3#bib.bib21)), Sensitivity
    Analysis of VAriables for Generic Exploration (SAVAGE) (Urvoy et al., [2013](https://arxiv.org/html/2407.01887v3#bib.bib22)),
    Relative Upper Confidence Bound (RUCB) (Zoghi et al., [2014b](https://arxiv.org/html/2407.01887v3#bib.bib23)),
    Relative Confidence Sampling (RCS) (Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)),
    Relative Minimum Empirical Divergence (RMED) (Komiyama et al., [2015](https://arxiv.org/html/2407.01887v3#bib.bib24)),
    Versatile Dueling Bandits (VDB) (Saha and Gaillard, [2022](https://arxiv.org/html/2407.01887v3#bib.bib13)),
    Self-Sparring (Sui et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib11)),
    and Double Thompson Sampling (DTS) (Wu and Liu, [2016](https://arxiv.org/html/2407.01887v3#bib.bib20)).
    Each of these algorithms employs distinct strategies for selecting arms and estimating
    preferences, with the ultimate goal of efficiently identifying the Condorcet winner.
    We assess the performance of LLMs and baseline algorithms using strong regret
    and weak regret metrics defined in Section [2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '基准。我们将LLMs与九个公认的基准算法进行比较，以评估其效果。这些基准包括交替滤波器（IF2）（Yue等，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)），击败均值（BTM）（Yue和Joachims，[2011](https://arxiv.org/html/2407.01887v3#bib.bib21)），通用探索变量的敏感性分析（SAVAGE）（Urvoy等，[2013](https://arxiv.org/html/2407.01887v3#bib.bib22)），相对上置信界（RUCB）（Zoghi等，[2014b](https://arxiv.org/html/2407.01887v3#bib.bib23)），相对置信采样（RCS）（Zoghi等，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)），相对最小经验散度（RMED）（Komiyama等，[2015](https://arxiv.org/html/2407.01887v3#bib.bib24)），多功能对决臂算法（VDB）（Saha和Gaillard，[2022](https://arxiv.org/html/2407.01887v3#bib.bib13)），自我对抗（Self-Sparring）（Sui等，[2017](https://arxiv.org/html/2407.01887v3#bib.bib11)），双重汤普森采样（DTS）（Wu和Liu，[2016](https://arxiv.org/html/2407.01887v3#bib.bib20)）。这些算法中的每一个都采用不同的策略来选择臂和估计偏好，最终目标是有效地识别Condorcet获胜者。我们通过第[2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")节中定义的强遗憾和弱遗憾指标来评估LLMs和基准算法的表现。'
- en: 'Environments. We evaluate the regret performance of LLMs and baselines across
    two types of stochastic environments under the standard DB setting with a Condorcet
    winner (CW). The environments differ in their stochastic transitivity properties
    and are divided into two cases, each with two levels of difficulty instances (Easy
    and Hard) depending on the distinguishability of the CW in beating other arms:
    (i) Transitive Case ($\text{SST}\cap\text{STI}$): This case uses a Bradley-Terry-Luce
    (BTL) model (Bradley and Terry, [1952](https://arxiv.org/html/2407.01887v3#bib.bib25);
    Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)). The preference
    matrices generated in this way satisfy the Strong Stochastic Transitivity (SST)
    and Stochastic Triangle Inequality (STI), which implies the existence of a CW;
    (ii) Intransitive Case ($\text{CW}\setminus(\text{SST}\cup\text{STI})$): the preference
    matrices introduce cyclic preferences among non-winning arms while ensuring the
    existence of a CW. The intransitive case is modeled using a custom preference
    construction designed to violate SST and STI. The detailed constructions can be
    found in Appendix [C.1.1](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS1 "C.1.1
    Environments ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '环境。我们在标准的DB设置下，通过Condorcet获胜者（CW）评估LLMs和基准算法在两种类型的随机环境中的遗憾表现。环境在其随机传递性特性上有所不同，分为两种情况，每种情况根据CW在击败其他臂时的可区分性有两个难度级别（容易和困难）：(i)
    传递性案例（$\text{SST}\cap\text{STI}$）：此案例使用Bradley-Terry-Luce（BTL）模型（Bradley和Terry，[1952](https://arxiv.org/html/2407.01887v3#bib.bib25);
    Yue等，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)）。这种方式生成的偏好矩阵满足强随机传递性（SST）和随机三角不等式（STI），这意味着存在CW；(ii)
    非传递性案例（$\text{CW}\setminus(\text{SST}\cup\text{STI})$）：偏好矩阵在非获胜臂之间引入循环偏好，同时确保存在CW。非传递性案例使用一种定制的偏好构建来违反SST和STI。详细的构建可以在附录[C.1.1](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS1
    "C.1.1 Environments ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")中找到。'
- en: Random Tests. The scale of our experiments is chosen to balance computational
    feasibility while preserving the ability of obtaining meaningful conclusions.
    We set the time horizon to $T=2000$ rounds, providing the LLMs and baseline algorithms
    with sufficient opportunity to learn and adapt to the DB environments. Each experiment
    is replicated $N=5$ times for the LLMs and $N=20$ times for the baseline algorithms,
    enabling an understanding of their average behaviors and reliable performance
    estimates.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随机测试。我们选择的实验规模旨在平衡计算可行性，同时保持得出有意义结论的能力。我们将时间范围设定为$T=2000$回合，为LLM和基准算法提供足够的机会来学习和适应DB环境。每个实验对LLM重复$N=5$次，对基准算法重复$N=20$次，以便理解它们的平均行为并提供可靠的性能估计。
- en: 3.2 Experimental results
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验结果
- en: '![Refer to caption](img/e78e0b7a252f881ad8885b33f570e444.png)![Refer to caption](img/4098f4531807479fd216b32f7ae3e839.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e78e0b7a252f881ad8885b33f570e444.png)![参考说明](img/4098f4531807479fd216b32f7ae3e839.png)'
- en: 'Figure 2: Comparisons between LLM agents and DB algorithms. Left and Right:
    strong and weak regret for the Transitive-Easy instance. Results for Transitive-Hard
    are in Figure [9](https://arxiv.org/html/2407.01887v3#A3.F9 "Figure 9 ‣ C.2.1
    Comparisons with Different Metrics ‣ C.2 Supplementary Experiments ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：LLM代理与DB算法的比较。左图和右图：传递-简单实例的强后悔和弱后悔。传递-困难的结果见图[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")。'
- en: 'For brevity, we present our initial analysis focused on the Transitive-Easy
    instance (Figure [2](https://arxiv.org/html/2407.01887v3#S3.F2 "Figure 2 ‣ 3.2
    Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")). The analysis is
    qualitatively similar for the Transitive-Hard instance (Figure [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") in Appendix [C.2](https://arxiv.org/html/2407.01887v3#A3.SS2
    "C.2 Supplementary Experiments ‣ Appendix C Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")). We use
    $\gamma=0.5$ for BTM, $f(K)=0.3K^{1.01}$ for RMED, $\eta=1$ for Self-Sparring,
    and $\alpha=0.51$ for RUCB, RCS and DTS. We analyze the results in terms of the
    strong and weak regret defined in Section [2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"). In the following sections, we will mainly focus on GPT-4 Turbo, which
    is our top-performing LLM, highlighting its success and failure modes.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '为简洁起见，我们呈现了集中在传递-简单实例的初步分析（图[2](https://arxiv.org/html/2407.01887v3#S3.F2 "Figure
    2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）。对于传递-困难实例（图[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")，参见附录[C.2](https://arxiv.org/html/2407.01887v3#A3.SS2
    "C.2 Supplementary Experiments ‣ Appendix C Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）。我们在BTM中使用$\gamma=0.5$，在RMED中使用$f(K)=0.3K^{1.01}$，在Self-Sparring中使用$\eta=1$，在RUCB、RCS和DTS中使用$\alpha=0.51$。我们通过第[2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")节定义的强后悔和弱后悔来分析结果。在接下来的章节中，我们将主要关注GPT-4 Turbo，它是我们表现最好的LLM，并重点讨论其成功与失败模式。'
- en: 'Emergence of in-context dueling bandits abilities. While GPT-3.5 Turbo and
    GPT-4 fail to solve the DB problem, GPT-4 Turbo consistently outperforms state-of-the-art
    DB baselines in weak regret on Transitive Case (see Figures[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") and[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). This reveals that the in-context
    dueling bandits abilities emerge as the general capabilities grow via standard
    training in general-purpose LLMs. Figure [13](https://arxiv.org/html/2407.01887v3#A3.F13
    "Figure 13 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") (Left) illustrates the fraction of
    duels including the best arm across different time intervals. GPT-4 Turbo outperforms
    other LLMs and the DB baselines throughout the entire timeline. These findings
    suggest that GPT-4 Turbo can effectively process the preference feedback obtained
    from duels and make informed decisions to quickly identify and include the best
    arm in its duels.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文对抗赌博能力的出现。虽然 GPT-3.5 Turbo 和 GPT-4 未能解决 DB 问题，但 GPT-4 Turbo 在过渡案例中的弱后悔表现始终优于最先进的
    DB 基准（见图[2](https://arxiv.org/html/2407.01887v3#S3.F2 "图 2 ‣ 3.2 实验结果 ‣ 3 LLM
    作为独立上下文决策者 ‣ 超越数值奖励：与 LLM 代理的上下文对抗赌博")和[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "图 9 ‣ C.2.1 不同度量标准的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：与 LLM 代理的上下文对抗赌博")）。这表明，随着通用
    LLM 通过标准训练逐步提高其整体能力，上下文对抗赌博能力也逐渐显现出来。图[13](https://arxiv.org/html/2407.01887v3#A3.F13
    "图 13 ‣ C.2.1 不同度量标准的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：与 LLM 代理的上下文对抗赌博")（左图）展示了不同时间间隔内包括最佳臂的对抗占比。GPT-4
    Turbo 在整个时间线中均优于其他 LLM 和 DB 基准。这些发现表明，GPT-4 Turbo 能够有效处理从对抗中获得的偏好反馈，并做出明智的决策，迅速识别并将最佳臂纳入其对抗中。
- en: 'Stable performance across different instances.  GPT-4 Turbo demonstrates low
    variance compared to other LLMs and DB baselines across varying levels of difficulty.
    As shown in Figure [14](https://arxiv.org/html/2407.01887v3#A3.F14 "Figure 14
    ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"), GPT-4 Turbo exhibits the lowest average generalized
    variance of strong and weak regret in both instances. This highlights its ability
    to maintain a stable decision-making process in DB.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同实例中的稳定表现。与其他大规模语言模型（LLM）和决策基准相比，GPT-4 Turbo 在不同难度级别下表现出较低的方差。如图[14](https://arxiv.org/html/2407.01887v3#A3.F14
    "图 14 ‣ C.2.1 不同度量标准的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：与 LLM 代理的上下文对抗赌博")所示，GPT-4
    Turbo 在强后悔和弱后悔的两种实例中都表现出最低的平均泛化方差。这突显了它在决策基准中保持稳定决策过程的能力。
- en: '<svg class="ltx_picture" height="63.28" id="S3.SS2.1.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Best Arm Identification: LLMs’
    in-context dueling bandits abilities emerge as the general capabilities grow.
    The Condorcet Winner is consistently selected in duel via GPT-4 Turbo, leading
    to exceptional weak regret performance with minimal variance on Transitive Case.</foreignobject></g></g></svg>'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="63.28" id="S3.SS2.1.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Best Arm Identification: LLMs’
    in-context dueling bandits abilities emerge as the general capabilities grow.
    The Condorcet Winner is consistently selected in duel via GPT-4 Turbo, leading
    to exceptional weak regret performance with minimal variance on Transitive Case.</foreignobject></g></g></svg>'
- en: 'Exploration Vulnerability. In the exploration stage, we observe that GPT-4
    Turbo tends to quickly narrow down to a small subset of arms (although usually
    containing the Condorcet Winner) and repeatedly compare these arms. In contrast,
    the baselines exhibit more diverse and extended exploration patterns. This behavior
    suggests that GPT-4 Turbo may overestimate the quality of arms that win their
    initial comparisons based on limited historical data. Unlike the baselines, which
    have explicit exploration mechanisms, LLMs rely on their inherent randomness (via
    sampling from their output distribution) to explore. Based on these observations,
    we hypothesize that if GPT-4 Turbo happens to sample a sequence of comparisons
    that favors suboptimal arms early on, it can get stuck comparing these arms indefinitely.
    To test this hypothesis, we conducted experiments using noisy prompts with biased
    history. Our results in Figure[16](https://arxiv.org/html/2407.01887v3#A3.F16
    "Figure 16 ‣ C.2.2 Duel Selection Trajectory ‣ C.2 Supplementary Experiments ‣
    Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") confirm that GPT-4 Turbo’s exploration strategy
    is indeed vulnerable to biased history initialization and can converge to local
    optima.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索脆弱性**。在探索阶段，我们观察到GPT-4 Turbo倾向于迅速缩小范围至一小部分臂（尽管通常包含Condorcet赢家），并反复比较这些臂。与此相对，基准模型展示了更为多样且扩展的探索模式。这种行为表明，GPT-4
    Turbo可能会高估在初次比较中获胜的臂的质量，原因是基于有限的历史数据。不同于具有明确探索机制的基准模型，LLM依赖其固有的随机性（通过从其输出分布中采样）进行探索。基于这些观察，我们假设，如果GPT-4
    Turbo恰好在早期采样一系列有利于次优臂的比较，它可能会陷入无限期地比较这些臂的局面。为了验证这一假设，我们通过使用带有偏向历史的噪声提示进行实验。图[16](https://arxiv.org/html/2407.01887v3#A3.F16
    "Figure 16 ‣ C.2.2 Duel Selection Trajectory ‣ C.2 Supplementary Experiments ‣
    Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")中的结果证实了GPT-4 Turbo的探索策略确实容易受到偏向历史初始化的影响，并可能收敛到局部最优解。'
- en: 'Exploitation Inability. Despite GPT-4 Turbo’s outstanding weak regret performance,
    it fails to consistently converge to a single best arm to duel against itself,
    even when the prompt setting explicitly calls for it. This behavior highlights
    a fundamental limitation of LLMs: they are primarily designed and trained for
    word token prediction rather than decision-making. Unlike baselines with explicit
    stopping conditions, GPT-4 Turbo relies on its inherent language modeling capabilities
    to determine when to stop exploring. Consequently, in the later exploitation stage,
    GPT-4 Turbo keeps comparing the same top arms without committing to a single winner
    (see Figure [3](https://arxiv.org/html/2407.01887v3#S3.F3 "Figure 3 ‣ 3.2 Experimental
    results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). This suggests that the language
    modeling objective alone may not be sufficient for LLMs to achieve optimal control
    in complex decision-making tasks like DB.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**利用能力不足**。尽管GPT-4 Turbo在弱后悔性能上表现出色，但它未能在对自身进行对决时始终如一地收敛到一个最佳臂，即便提示设置明确要求这样做。这个行为突显了LLM的一个根本限制：它们主要是为词元预测而设计和训练的，而非决策制定。与具有明确停止条件的基准模型不同，GPT-4
    Turbo依赖其固有的语言建模能力来决定何时停止探索。因此，在后期的利用阶段，GPT-4 Turbo不断地比较相同的顶级臂，而没有最终确定一个获胜者（参见图[3](https://arxiv.org/html/2407.01887v3#S3.F3
    "Figure 3 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）。这表明，单靠语言建模目标可能不足以使LLM在复杂的决策任务中实现最佳控制，如决策带模型（DB）。'
- en: '<svg class="ltx_picture" height="63.28" id="S3.SS2.2.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Lack of Robust Strategy: LLMs’
    performance can be hindered by noisy and adversarial prompts due to overestimation
    bias in the exploration stage and the lack of convergence criteria in the exploitation
    stage.</foreignobject></g></g></svg>'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="63.28" id="S3.SS2.2.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,63.28) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="45.51" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Lack of Robust Strategy: LLMs’
    performance can be hindered by noisy and adversarial prompts due to overestimation
    bias in the exploration stage and the lack of convergence criteria in the exploitation
    stage.</foreignobject></g></g></svg>'
- en: 'Biased understanding of DB problem during pretraining. Our two best-performing
    LLMs, GPT-4 Turbo and o1-preview, exhibit systematic biases regarding the DB problem,
    likely due to a lack of exposure to similar tasks during pretraining. Specifically,
    they incorrectly assume that an arm cannot duel with itself (the convergence case),
    even when explicitly prompted to do so (see examples in Appendix [C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3
    "C.1.3 Exemplars of GPT-4 Turbo and o1-preview ‣ C.1 LLM Experimental Results
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). This misunderstanding makes the
    DB problem as an out-of-distribution (OOD) task for LLMs, and in-context instructions
    fail to fully override this internal bias. Consequently, LLM agents cannot completely
    align with problem descriptions due to the inherent limitations of in-context
    learning, which cannot really generalize to OOD tasks (Wang et al., [2024b](https://arxiv.org/html/2407.01887v3#bib.bib26)).
    Figure [13](https://arxiv.org/html/2407.01887v3#A3.F13 "Figure 13 ‣ C.2.1 Comparisons
    with Different Metrics ‣ C.2 Supplementary Experiments ‣ Appendix C Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") supports these observations: o1-preview demonstrates better
    reasoning capabilities by transitioning from exploration to exploitation effectively
    and achieving lower strong regret than GPT-4 Turbo. However, its inference-time
    CoT mechanism reinforces its internal biased understanding of DB, resulting in
    bad weak regret performance due to the selection of two suboptimal arms in duels.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在预训练期间对DB问题的偏见理解。我们表现最好的两款大语言模型（LLM），GPT-4 Turbo和o1-preview，在DB问题上表现出系统性的偏见，这可能是由于在预训练过程中缺乏类似任务的暴露。具体来说，它们错误地假设一个臂不能与自身对决（收敛情况），即使在明确提示的情况下也如此（请参见附录[C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3
    "C.1.3 Exemplars of GPT-4 Turbo and o1-preview ‣ C.1 LLM Experimental Results
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")中的示例）。这种误解使得DB问题对LLM而言成为一个分布外（OOD）任务，而上下文中的指令未能完全消除这一内部偏见。因此，由于上下文学习的固有限制，LLM代理无法完全与问题描述对齐，而上下文学习无法真正推广到OOD任务（Wang
    et al., [2024b](https://arxiv.org/html/2407.01887v3#bib.bib26)）。图[13](https://arxiv.org/html/2407.01887v3#A3.F13
    "Figure 13 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")支持了这些观察：o1-preview通过有效地从探索转向利用，并实现了比GPT-4
    Turbo更低的强后悔，展示了更好的推理能力。然而，它的推理时CoT机制强化了其对DB问题的内部偏见理解，导致在对决中选择两个次优臂，从而导致较差的弱后悔表现。'
- en: '<svg class="ltx_picture" height="46.68" id="S3.SS2.3.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,46.68) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Systematic Biases: LLMs out-of-the-box
    lack a fundamental understanding of the DB problem and instead intuitively choose
    the next pair of arms to compare based on dueling history.</foreignobject></g></g></svg>'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="46.68" id="S3.SS2.3.p1.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,46.68) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 15.81 8.89)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="562.39">Systematic Biases: LLMs out-of-the-box
    lack a fundamental understanding of the DB problem and instead intuitively choose
    the next pair of arms to compare based on dueling history.</foreignobject></g></g></svg>'
- en: 'Scalability Limitation. To evaluate whether LLMs can generalize their exceptional
    weak regret performance, we conduct experiments from two perspectives: (i) Removing
    transitivity in preference structures: we change from transitive cases to intransitive
    cases that violate SST and STI (see Figures[10](https://arxiv.org/html/2407.01887v3#A3.F10
    "Figure 10 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") and[11](https://arxiv.org/html/2407.01887v3#A3.F11
    "Figure 11 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). The analysis of Intransitive-Easy
    and Intransitive-Hard is qualitatively similar: LLMs fail to replicate their long-term
    weak regret performance in transitive cases when faced with intransitive instances.
    However, their short-term weak regret performance (< 100 steps) remains exceptional;
    (ii) Increasing the number of arms: as illustrated in Figure [12](https://arxiv.org/html/2407.01887v3#A3.F12
    "Figure 12 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), from $K=5$ to $K=10$, GPT-4 Turbo’s
    performance exhibits a noticeable long-term decline with the increase in $K$.
    While LLMs still beat all the DB baselines in the initial steps, they struggle
    to effectively infer the relative strengths among a larger number of arms in the
    long run. These findings suggest that while LLMs exhibit emergent abilities for
    relative decision-making rooted in linguistic knowledge, their effectiveness is
    only generalized to short-term scenarios. The long-term performance is hindered
    by larger number of arms and LLMs’ lack of fundamental understanding of intransitive
    cyclic structures.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性限制。为了评估大型语言模型（LLMs）是否能够将其卓越的弱后悔表现进行泛化，我们从两个角度进行实验：（i）去除偏好结构中的传递性：我们将从符合传递性的情况更改为违反SST和STI的非传递性情况（参见图[10](https://arxiv.org/html/2407.01887v3#A3.F10
    "图 10 ‣ C.2.1 不同度量的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：LLM代理的上下文对抗赌博带")和[11](https://arxiv.org/html/2407.01887v3#A3.F11
    "图 11 ‣ C.2.1 不同度量的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：LLM代理的上下文对抗赌博带")）。对非传递性-简单和非传递性-困难的分析在定性上是相似的：在面对非传递性实例时，LLMs未能在传递性情况下复制其长期弱后悔表现。然而，它们的短期弱后悔表现（<
    100步）仍然异常优秀；（ii）增加臂的数量：如图[12](https://arxiv.org/html/2407.01887v3#A3.F12 "图 12
    ‣ C.2.1 不同度量的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：LLM代理的上下文对抗赌博带")所示，从$K=5$增加到$K=10$，GPT-4
    Turbo的表现随着$K$的增加而出现明显的长期下降。尽管LLMs在最初的步骤中仍能击败所有的DB基准，但它们在长期内难以有效推断更大数量臂之间的相对强度。这些发现表明，尽管LLMs表现出了基于语言知识的相对决策能力，它们的有效性仅限于短期情境。长期表现则受到更多臂的数量和LLMs对非传递性循环结构缺乏基本理解的影响。
- en: 'To quantify this long-term scalability limitation and formally characterize
    the dueling bandit instances LLMs can handle, we introduce the concept of the
    Relative Decision Boundary (RDB). The RDB for a given LLM $m$ is defined as the
    set of problem difficulties $D$ for which the model achieves an acceptable level
    of weak regret across an effective short-term time horizon $T_{e}$, satisfying
    the condition:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了量化这一长期可扩展性限制，并正式描述LLMs能够处理的对抗赌博带实例，我们引入了相对决策边界（RDB）的概念。给定LLM $m$的RDB定义为问题难度$D$的集合，其中模型在有效的短期时间范围$T_{e}$内达到可接受的弱后悔水平，并满足以下条件：
- en: '|  | $\displaystyle\text{RDB}(m)=\left\{(K,T,\Delta,T_{e})\ \Big{&#124;}\ \mathsf{WR}(m,D%
    (K,T,\Delta),T_{e})\leq\mathsf{R_{\text{th}}}\right\}.$ |  | (2) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{RDB}(m)=\left\{(K,T,\Delta,T_{e})\ \Big{&#124;}\ \mathsf{WR}(m,D%
    (K,T,\Delta),T_{e})\leq\mathsf{R_{\text{th}}}\right\}.$ |  | (2) |'
- en: Here, $\mathsf{WR}(m,D,T_{e})$ represents the cumulative weak regret incurred
    by the model $m$ on a problem of difficulty $D$ over an effective short-term time
    horizon $T_{e}$, while $\mathsf{R_{\text{th}}}$ is a predefined threshold that
    quantifies acceptable weak regret performance. Overall, RDB is influenced by the
    inherent ability of $m$, the number of arms $K$, the transitivity $T$, the distinguishability
    between arms $\Delta$, and the effective short-term time horizon $T_{e}$.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$\mathsf{WR}(m,D,T_{e})$ 表示模型 $m$ 在一个难度为 $D$ 的问题上，在有效的短期时间范围 $T_{e}$ 内所产生的累计弱遗憾，而
    $\mathsf{R_{\text{th}}}$ 是一个预定义的阈值，用来量化可接受的弱遗憾表现。总体来说，RDB 受到 $m$ 的固有能力、臂的数量 $K$、传递性
    $T$、臂之间的可区分性 $\Delta$ 以及有效短期时间范围 $T_{e}$ 的影响。
- en: '<svg class="ltx_picture ltx_centering" height="142.2" id="S3.SS2.4.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,142.2) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 99.51)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="550.7">Failure of long-term generalization:
    LLMs’ long-term strong and weak regret performance degrades when introducing intransitive
    preference structures or large number of arms.</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="550.7">Success
    of short-term generalization: LLMs’ short-term weak regret performance remains
    surprisingly exceptional across all instances (see subfigures in[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),  [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [10](https://arxiv.org/html/2407.01887v3#A3.F10
    "Figure 10 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [11](https://arxiv.org/html/2407.01887v3#A3.F11
    "Figure 11 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [12](https://arxiv.org/html/2407.01887v3#A3.F12
    "Figure 12 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). We introduce Relative Decision
    Boundary (RDB) to describe the dueling bandit instances that LLMs can effectively
    handle.</foreignobject></g></g></svg>'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture ltx_centering" height="142.2" id="S3.SS2.4.pic1" overflow="visible"
    version="1.1" width="594"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,142.2) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 99.51)"><foreignobject color="#000000" height="28.9" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="550.7">Failure of long-term generalization:
    LLMs’ long-term strong and weak regret performance degrades when introducing intransitive
    preference structures or large number of arms.</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="550.7">Success
    of short-term generalization: LLMs’ short-term weak regret performance remains
    surprisingly exceptional across all instances (see subfigures in[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),  [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [10](https://arxiv.org/html/2407.01887v3#A3.F10
    "Figure 10 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [11](https://arxiv.org/html/2407.01887v3#A3.F11
    "Figure 11 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), [12](https://arxiv.org/html/2407.01887v3#A3.F12
    "Figure 12 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")). We introduce Relative Decision
    Boundary (RDB) to describe the dueling bandit instances that LLMs can effectively
    handle.</foreignobject></g></g></svg>'
- en: '![Refer to caption](img/8f3b989fede273c72c372352a6504c64.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8f3b989fede273c72c372352a6504c64.png)'
- en: 'Figure 3: Comparison of duel selection trajectories among GPT-4 Turbo, Self-Sparring,
    and DTS on the Transitive-Easy (Top Row) and Transitive-Hard (Bottom Row) instances.
    The decision trajectories of GPT-4 Turbo exhibit a clear pattern of continuous
    exploration without converging to the best arm. In contrast, Self-Sparring and
    DTS demonstrate structured exploration patterns and convergence properties.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：GPT-4 Turbo、Self-Sparring 和 DTS 在 Transitive-Easy（上排）和 Transitive-Hard（下排）实例中的对战选择轨迹对比。GPT-4
    Turbo 的决策轨迹展示了明显的持续探索模式，未能收敛到最优臂。相比之下，Self-Sparring 和 DTS 展示了结构化的探索模式和收敛特性。
- en: As a summary, we found that in-context LLM agents’ linguistic prior allows them
    to quickly identify the Condorcet Winner from the dueling history in the short-term
    (for both Transitive Case and Intransitive Case), but it is vulnerable.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们发现，基于上下文的 LLM 代理的语言先验使它们能够迅速识别来自对战历史中的 Condorcet 胜者（无论是在传递性案例还是非传递性案例中），但这种方法具有脆弱性。
- en: 'To further investigate the algorithmic behavior of LLMs and develop more robust
    and effective in-context decision-making strategies, we seek to answer the following
    questions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步研究 LLM 的算法行为，并开发更强大有效的上下文决策策略，我们寻求回答以下问题：
- en: '[Q1]  Can we develop an Algorithm-Enhanced in-context DB agent with a theoretical
    guarantee?'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[Q1]  我们能否开发出具有理论保证的算法增强型上下文 DB 代理？'
- en: '[Q2]  How does it perform compared to standalone LLM agents and classic DB
    algorithms?'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[Q2]  与独立的 LLM 代理和经典的 DB 算法相比，它的表现如何？'
- en: 4 Algorithm-Enhanced LLMs for Dueling Bandits
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 算法增强型 LLM 在对战强盗问题中的应用
- en: 'Classic DB algorithms based on the Explore-then-Exploit framework, such as
    Interleaved Filter 2 (IF2) (Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)),
    are known to be near-optimal, with matching regret upper and lower bounds up to
    multiplicative constants. To address the challenges identified in Section [3.2](https://arxiv.org/html/2407.01887v3#S3.SS2
    "3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") of using standalone
    LLM agents for DB, we propose an algorithm-enhanced approach: LLM with Enhanced
    Algorithmic Dueling (LEAD) to demonstrate the possibility of integrating off-the-shelf
    DB algorithms with LLM agents through fine-grained adaptive interplay. Our framework,
    LEAD, enjoys both a regret guarantee and strong empirical performance.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Explore-then-Exploit 框架的经典 DB 算法，如交替滤波器 2（IF2）（Yue 等人， [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)），已知是接近最优的，其遗憾上界和下界的匹配精度高，且在乘法常数范围内。为了解决使用独立
    LLM 代理进行 DB 所面临的挑战（详见第 [3.2](https://arxiv.org/html/2407.01887v3#S3.SS2 "3.2 实验结果
    ‣ 3 LLM 作为独立的上下文决策者 ‣ 超越数值奖励：具有 LLM 代理的上下文对战强盗问题") 节），我们提出了一种算法增强的方法：带有增强算法对战（LEAD）的
    LLM，旨在展示通过细粒度自适应互动将现成的 DB 算法与 LLM 代理结合的可能性。我们的框架 LEAD 既具有遗憾保证，又具备强大的实证表现。
- en: 4.1 Algorithmic Design of LEAD
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 LEAD 的算法设计
- en: In this section, we present the design intuitions of LEAD. We begin by discussing
    the limitations of a naive intervention approach and the desirable properties
    for an effective Algorithm-Enhanced LLM framework. Based on these considerations,
    we propose an agentic framework design LEAD, where we can incorporate any Explore-then-Exploit
    DB algorithms (Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)).
    As an illustrative example, we use IF2 (Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6))
    to demonstrate how off-the-shelf algorithms can be integrated within LEAD and
    provide a detailed description.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了LEAD的设计直觉。我们首先讨论了天真干预方法的局限性以及有效的算法增强型LLM框架的理想特性。基于这些考虑，我们提出了一个代理框架设计LEAD，在该框架中，我们可以结合任何探索-再利用的数据库（DB）算法（Zoghi等人，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)）。作为说明示例，我们使用IF2（Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)）来展示如何将现成算法集成到LEAD中，并提供详细描述。
- en: 'Limitations of Naive Intervention. A straightforward approach to addressing
    the convergence instability limitation of LLMs is to use a simple if-else condition
    that forces the LLMs to converge when they first exploit two identical arms, which
    we call the Convergence-Triggered (CT) intervention strategy. However, CT fails
    to guarantee the selection of the true Condorcet winner and can reinforce local
    optima (see Figure[17](https://arxiv.org/html/2407.01887v3#A3.F17 "Figure 17 ‣
    C.2.2 Duel Selection Trajectory ‣ C.2 Supplementary Experiments ‣ Appendix C Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") in Appendix[C.2](https://arxiv.org/html/2407.01887v3#A3.SS2
    "C.2 Supplementary Experiments ‣ Appendix C Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") for a failure
    example). This suggests that relying on the LLMs’ internal convergence behavior
    to trigger the transition from exploration to exploitation is unreliable, as the
    LLMs are largely driven by its inherent sampling noise rather than a structured
    exploration policy. Thus, handling this limitation with theoretical guarantees
    remains challenging.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 天真干预的局限性。解决LLMs收敛不稳定性的一个直接方法是使用简单的if-else条件，在LLMs首次利用两个相同的臂时强制其收敛，我们称之为收敛触发（CT）干预策略。然而，CT无法保证选择真正的康多塞（Condorcet）获胜者，并可能强化局部最优解（参见附录[C.2](https://arxiv.org/html/2407.01887v3#A3.SS2
    "C.2 补充实验 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：具有LLM代理的上下文对抗强盗")中的失败示例，图[17](https://arxiv.org/html/2407.01887v3#A3.F17
    "图17 ‣ C.2.2 对决选择轨迹 ‣ C.2 补充实验 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：具有LLM代理的上下文对抗强盗")）。这表明，依赖LLMs的内部收敛行为来触发从探索到再利用的过渡是不可靠的，因为LLMs在很大程度上是由其固有的采样噪声驱动，而不是结构化的探索策略。因此，使用理论保证来处理这一局限性仍然具有挑战性。
- en: 'Desirable Properties for LLM Augmentation. To address [Q1], we seek an algorithmic
    framework with the following properties: (i) A clear, symbolic logical structure
    that allows for easy integration with LLM & Algorithm suggestions; (ii) A well-defined
    exploration-exploitation trade-off that leverages the LLMs’ exploration behavior
    while ensuring convergence; (iii) Strong theoretical guarantees to maintain robustness
    with various prompting scenarios.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: LLM增强的理想特性。为了解决[Q1]，我们寻求具有以下特性的算法框架：（i）一个清晰的符号逻辑结构，便于与LLM和算法建议的集成；（ii）一个明确定义的探索-再利用权衡，既能利用LLMs的探索行为，又能确保收敛性；（iii）强大的理论保证，以保持在各种提示场景下的鲁棒性。
- en: 'As a result, we find that the Explore-Then-Exploit structure is particularly
    well-suited for LLMs (see Appendix [B.1](https://arxiv.org/html/2407.01887v3#A2.SS1
    "B.1 Algorithm Design Logic ‣ Appendix B Algorithm Design and Analysis of LEAD
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") for a detailed
    illustration). By selecting an Explore-Then-Exploit DB algorithm as the foundation
    inside LEAD, we address [Q1]. As an example, we use IF2 as the base to illustrate
    the theoretical guarantee and empirical performance. This approach can be applied
    similarly to other algorithms with regret bounds in the Explore-Then-Exploit family.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们发现探索-再利用（Explore-Then-Exploit）结构特别适合于大语言模型（LLMs）（详见附录[B.1](https://arxiv.org/html/2407.01887v3#A2.SS1
    "B.1 算法设计逻辑 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：具有LLM代理的上下文对抗强盗")的详细说明）。通过在LEAD中选择探索-再利用的数据库（DB）算法作为基础，我们解决了[Q1]。例如，我们使用IF2作为基础，来展示理论保证和实证表现。这种方法也可以类似地应用于探索-再利用家族中具有后悔界限的其他算法。
- en: '![Refer to caption](img/7f6b0bd8110cdb4facdfba01bd9154cb.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/7f6b0bd8110cdb4facdfba01bd9154cb.png)'
- en: 'Figure 4: Main components of the proposed LEAD agent in Algorithm [1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    are illustrated: (i) The blue-colored part represents the LLM phase. (ii) The
    grey-colored part indicates the DB phase. (iii) The Algorithmic Procedures are
    detailed in Appendix [B.2](https://arxiv.org/html/2407.01887v3#A2.SS2 "B.2 Detailed
    Procedure Description ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents"). (iv) The black arrows
    denote shared interactions between components. (v) The dotted arrows represent
    the input and output.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：在算法[1](https://arxiv.org/html/2407.01887v3#algorithm1 "Algorithm 1 ‣ 4.1
    Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")中提出的LEAD代理的主要组件如下所示：（i）蓝色部分代表LLM阶段。（ii）灰色部分表示DB阶段。（iii）算法流程在附录[B.2](https://arxiv.org/html/2407.01887v3#A2.SS2
    "B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")中有详细描述。（iv）黑色箭头表示组件之间的共享交互。（v）虚线箭头表示输入和输出。'
- en: 'Algorithmic Framework. The procedures of the LEAD are illustrated in Figure[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "Figure 4 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and presented in Algorithm[1](https://arxiv.org/html/2407.01887v3#algorithm1 "Algorithm
    1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (see more
    details in Appendix[B.2](https://arxiv.org/html/2407.01887v3#A2.SS2 "B.2 Detailed
    Procedure Description ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")). LEAD (IF2 base)
    maintains a confidence parameter $\delta$ and a threshold parameter $\epsilon$
    that control the algorithm’s confidence of matches between arms. The key components
    of LEAD (IF2 base) are as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '算法框架。LEAD的过程如图[4](https://arxiv.org/html/2407.01887v3#S4.F4 "Figure 4 ‣ 4.1
    Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents")所示，并在算法[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")中呈现（更多细节见附录[B.2](https://arxiv.org/html/2407.01887v3#A2.SS2
    "B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）。LEAD（IF2基础）保持一个置信度参数$\delta$和一个阈值参数$\epsilon$，用于控制算法对臂之间匹配的信心。LEAD（IF2基础）的关键组件如下：'
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Phase 1 (LLM Phase):  *Utilization of LLM recommended arms*: The agentic framework
    maintains a set of candidate arms $B$. Given two arms suggested by an LLM agent,
    the framework begins with finding a winner between them, denoted by $b_{\mathrm{LLM}}$.
    The winning arm $b_{\mathrm{LLM}}$ is then matched with each remaining arm $b\in
    B$. This phase continues until $b_{\mathrm{LLM}}$ is defeated or all arms in $B$
    have been matched. The variable $\mathsf{TrustLLM}$ is used to control the execution
    of the LLM phase, and it is set to $\mathtt{False}$ when $b_{\mathrm{LLM}}$ is
    defeated by another arm, indicating that the LLM’s suggestions are no longer trusted.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段 1（LLM 阶段）：*利用LLM推荐的臂*：代理框架保持一组候选臂$B$。给定LLM代理建议的两个臂，框架开始通过这两个臂之间的竞争来寻找赢家，记作$b_{\mathrm{LLM}}$。然后，获胜的臂$b_{\mathrm{LLM}}$与$B$中每个剩余的臂$b$进行匹配。此阶段继续，直到$b_{\mathrm{LLM}}$被击败或$B$中的所有臂都已匹配。变量$\mathsf{TrustLLM}$用于控制LLM阶段的执行，当$b_{\mathrm{LLM}}$被另一个臂击败时，它被设置为$\mathtt{False}$，表示不再信任LLM的建议。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Phase 2 (IF2 Phase):  *Roll back to IF2*: If $b_{\mathrm{LLM}}$ is defeated,
    the framework switches to implementing one round of IF2 with an incumbent arm
    $b_{\text{{IF2}}}$ selected based on an estimated preference matrix $\hat{P}$.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阶段 2（IF2 阶段）：*回滚到 IF2*：如果$b_{\mathrm{LLM}}$被击败，框架将切换到执行一轮IF2，其中根据估计的偏好矩阵$\hat{P}$选择当前臂$b_{\text{{IF2}}}$。
- en: 'After Phase 2, the algorithm-enhanced agent repeats Phase 1 until $B$ only
    contains the best arm. Algorithm[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and Figure[4](https://arxiv.org/html/2407.01887v3#S4.F4 "Figure 4 ‣ 4.1 Algorithmic
    Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") summarize the phases above,
    with details delegated to Appendix[B.2](https://arxiv.org/html/2407.01887v3#A2.SS2
    "B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段后，算法增强型代理会重复第一阶段，直到$B$只包含最优臂。算法[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "算法 1 ‣ 4.1 LEAD的算法设计 ‣ 4 算法增强型LLM用于决斗强盗问题 ‣ 超越数值奖励：基于上下文的决斗强盗问题与LLM代理")和图[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "图 4 ‣ 4.1 LEAD的算法设计 ‣ 4 算法增强型LLM用于决斗强盗问题 ‣ 超越数值奖励：基于上下文的决斗强盗问题与LLM代理")总结了上述阶段，详细内容参见附录[B.2](https://arxiv.org/html/2407.01887v3#A2.SS2
    "B.2 详细过程描述 ‣ 附录 B LEAD的算法设计与分析 ‣ 超越数值奖励：基于上下文的决斗强盗问题与LLM代理")。
- en: Algorithm 1 Algorithm-Enhanced LLM Agent: LEAD (IF2 base)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 算法增强型 LLM 代理：LEAD（IF2 基础）
- en: 'Initialize :'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化：
- en: Time horizon length $T$, arms $B=\{b_{1},\ldots,b_{K}\}$, incumbent arm $b_{\text{{IF2}}}$
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 时间跨度长度$T$，臂$B=\{b_{1},\ldots,b_{K}\}$，现有臂$b_{\text{{IF2}}}$
- en: while *$|B|\geq 1$* do
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *$|B|\geq 1$* 时，执行
- en: '$\mathsf{TrustLLM}\leftarrow\mathtt{True}$2       /* LLM Phase in Figure[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "Figure 4 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    (Lines 2-10) */ while *$\mathsf{TrustLLM}$* do             Prompt LLM to select
    $(b_{\mathrm{LLM}_{1}},b_{\mathrm{LLM}_{2}})$ from $B$  $b_{\mathrm{LLM}}\leftarrow\textsc{Match
    Arms}(b_{\mathrm{LLM}_{1}},b_{\mathrm{% LLM}_{2}})$ (Procedure[1](https://arxiv.org/html/2407.01887v3#algorithm1a
    "Procedure 1 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"))3             /* Compare LLM arms */ for *$b\in B$* do                    $b^{\prime}\leftarrow\textsc{Match
    Arms}(b_{\mathrm{LLM}},b)$ (Procedure[1](https://arxiv.org/html/2407.01887v3#algorithm1a
    "Procedure 1 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"))4                   /* Compare $b_{\mathrm{LLM}}$ with others */
    if *$b^{\prime}\neq b_{\mathrm{LLM}}$* then $\mathsf{TrustLLM}\leftarrow\mathtt{False}$,
    continue5                  6             end for7            8       end while      $\mathsf{StillTrust},B\leftarrow\textsc{Validate}(b^{\prime},B,\mathsf{TrustLLM})$
    (Procedure[2](https://arxiv.org/html/2407.01887v3#algorithm2 "Procedure 2 ‣ B.2
    Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis of LEAD
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")) $b_{\text{{IF2}}},B\leftarrow$IF2$(b_{\text{{IF2}}},B)$
    (Procedure[3](https://arxiv.org/html/2407.01887v3#algorithm3 "Procedure 3 ‣ B.2
    Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis of LEAD
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"))9      
    /* IF2 Phase in Figure[4](https://arxiv.org/html/2407.01887v3#S4.F4 "Figure 4
    ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") (Lines 11-12)
    */10 end while11if *$\mathsf{StillTrust}$* then return  *$b_{\mathrm{LLM}}$*12  else return  *$b_{\text{{IF2}}}$*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathsf{TrustLLM}\leftarrow\mathtt{True}$2       /* 图[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "图 4 ‣ 4.1 LEAD的算法设计 ‣ 4 算法增强型LLM在对抗性赌博中的应用 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")中的LLM阶段（第2-10行）
    */ 当 *$\mathsf{TrustLLM}$* 时             提示LLM选择 $(b_{\mathrm{LLM}_{1}},b_{\mathrm{LLM}_{2}})$
    来自 $B$  $b_{\mathrm{LLM}}\leftarrow\textsc{Match Arms}(b_{\mathrm{LLM}_{1}},b_{\mathrm{LLM}_{2}})$（过程[1](https://arxiv.org/html/2407.01887v3#algorithm1a
    "过程 1 ‣ B.2 详细过程描述 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")）3              
    /* 比较LLM臂 */ 对于 *$b\in B$* 来说                    $b^{\prime}\leftarrow\textsc{Match
    Arms}(b_{\mathrm{LLM}},b)$（过程[1](https://arxiv.org/html/2407.01887v3#algorithm1a
    "过程 1 ‣ B.2 详细过程描述 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")）4                  
    /* 将 $b_{\mathrm{LLM}}$ 与其他进行比较 */ 如果 *$b^{\prime}\neq b_{\mathrm{LLM}}$* 则 $\mathsf{TrustLLM}\leftarrow\mathtt{False}$，继续5                  6            
    结束循环7            8       结束当循环      $\mathsf{StillTrust},B\leftarrow\textsc{Validate}(b^{\prime},B,\mathsf{TrustLLM})$（过程[2](https://arxiv.org/html/2407.01887v3#algorithm2
    "过程 2 ‣ B.2 详细过程描述 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")） $b_{\text{{IF2}}},B\leftarrow$IF2$(b_{\text{{IF2}}},B)$（过程[3](https://arxiv.org/html/2407.01887v3#algorithm3
    "过程 3 ‣ B.2 详细过程描述 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")）9       /* 图[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "图 4 ‣ 4.1 LEAD的算法设计 ‣ 4 算法增强型LLM在对抗性赌博中的应用 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")中的IF2阶段（第11-12行）
    */10 结束当循环11如果 *$\mathsf{StillTrust}$* 则返回  *$b_{\mathrm{LLM}}$*12  否则返回  *$b_{\text{{IF2}}}$*
- en: 4.2 Theoretical Guarantees for LEAD
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LEAD的理论保证
- en: 'In this section, we begin by characterizing the vulnerability of using standalone
    LLM agents for dueling bandits in Theorem [4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "Theorem 4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). Then, we provide the theoretical guarantees of LEAD in Theorem[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") and[4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3 "Theorem
    4.3 (Converse). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"), demonstrating its efficacy and convergence.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先通过定理 [4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1 "定理 4.1（脆弱性）。
    ‣ 4.2 LEAD的理论保证 ‣ 4 增强算法的LLM用于对抗赌博 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")来刻画单独使用LLM代理处理对抗赌博问题的脆弱性。接着，我们通过定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2（预期遗憾）。 ‣ 4.2 LEAD的理论保证 ‣ 4 增强算法的LLM用于对抗赌博 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")和[4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3
    "定理 4.3（反向命题）。 ‣ 4.2 LEAD的理论保证 ‣ 4 增强算法的LLM用于对抗赌博 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")，展示LEAD的有效性和收敛性。
- en: Theorem 4.1  (Vulnerability).
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 4.1（脆弱性）。
- en: 'For the dueling bandits problem with $K$ arms and time horizon $T$, there exists
    a preference structure and an attacker strategy with budget $\Phi(T)$, such that
    any standalone LLM agent, whose policy is represented by Eq.([1](https://arxiv.org/html/2407.01887v3#S2.E1
    "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")) and whose worst-case behavior under the original prompt satisfying Assumption [4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "Assumption 4 (Worst-Case Behavior). ‣ B.3.2 Theoretical Guarantees of LEAD ‣
    B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), will suffer
    an expected regret of $\Omega\left(\min\left\{\Phi(T),{T}/{K}\right\}\right)$.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 $K$ 个臂和时间范围 $T$ 的对抗赌博问题，存在一种偏好结构和一个预算为 $\Phi(T)$ 的攻击者策略，使得任何单独的LLM代理，其策略由公式([1](https://arxiv.org/html/2407.01887v3#S2.E1
    "在2 预备知识 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博"))表示，并且在原始提示下满足假设 [4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "假设 4（最坏情况行为）。 ‣ B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录 B LEAD算法设计与分析 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")的最坏情况行为，将遭受预期的遗憾
    $\Omega\left(\min\left\{\Phi(T),{T}/{K}\right\}\right)$。
- en: 'The proof of Theorem [4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "Theorem 4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") is provided in Appendix[B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 Theoretical Guarantees of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B
    Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents"). The theorem underscores the suboptimality of standalone
    LLM agents in DB problems, particularly when input prompts are subjected to adversarial
    attacks. This vulnerability highlights the need for a more robust approach to
    use in-context LLM agents while offering theoretical guarantees under diverse
    prompting scenarios.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 定理 [4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1 "定理 4.1（脆弱性）。 ‣
    4.2 LEAD的理论保证 ‣ 4 增强算法的LLM用于对抗赌博 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")的证明见附录[B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录 B LEAD算法设计与分析 ‣ 超越数值奖励：使用LLM代理的上下文对抗赌博")。该定理强调了在对抗赌博问题中，单独使用LLM代理的次优性，特别是当输入提示遭受敌对攻击时。这种脆弱性凸显了需要一种更强健的方法来使用上下文中的LLM代理，同时在多种提示情境下提供理论保证。
- en: 'Regret Bounds. Following the algorithmic design of LEAD in Section[4.1](https://arxiv.org/html/2407.01887v3#S4.SS1
    "4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), LEAD (IF2
    base) inherits the theoretical guarantees of IF2 (see Appendix[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), while nontrivially leveraging the benefits
    of LLMs’ exceptional weak regret performance for exploration across a range of
    instances within RDB. Specifically, LEAD (IF2 base) has the following theoretical
    guarantee:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '后悔界限。根据第[4.1节](https://arxiv.org/html/2407.01887v3#S4.SS1 "4.1 Algorithmic
    Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")中LEAD的算法设计，LEAD（IF2基准）继承了IF2的理论保证（见附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")），同时在RDB中跨一系列实例的探索中，非平凡地利用了LLM在弱后悔性能方面的卓越优势。具体来说，LEAD（IF2基准）具有以下理论保证：'
- en: Theorem 4.2  (Expected Regret).
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理4.2（预期后悔）。
- en: 'Suppose for $t\geq T_{\mathrm{LLM}}$, the arms recommended by an LLM agent
    contain the best arm $b^{*}$. Under Assumptions [1](https://arxiv.org/html/2407.01887v3#Thmassumption1
    "Assumption 1 (Total Ordering). ‣ B.3.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")-[3](https://arxiv.org/html/2407.01887v3#Thmassumption3
    "Assumption 3 (Stochastic Triangle Inequality). ‣ B.3.1 Useful Assumptions and
    Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), the expected strong regret of LEAD (IF2 base) satisfies $\mathbb{E}\left[\mathsf{SR}(\textsc{LEAD})\right]\leq\widetilde{O}\left({(K%
    \log T)}/{\epsilon_{1,2}}\right),$ and the expected weak regret can be bounded
    by'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '假设对于$t \geq T_{\mathrm{LLM}}$，LLM代理推荐的臂包含最佳臂$b^{*}$。在假设[1](https://arxiv.org/html/2407.01887v3#Thmassumption1
    "Assumption 1 (Total Ordering). ‣ B.3.1 Useful Assumptions and Lemmas for Dueling
    Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")-[3](https://arxiv.org/html/2407.01887v3#Thmassumption3
    "Assumption 3 (Stochastic Triangle Inequality). ‣ B.3.1 Useful Assumptions and
    Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")下，LEAD（IF2基准）的预期强后悔满足$\mathbb{E}\left[\mathsf{SR}(\textsc{LEAD})\right]
    \leq \widetilde{O}\left({(K% \log T)}/{\epsilon_{1,2}}\right)，$而预期的弱后悔可以被界定为：'
- en: '|  | $\displaystyle\mathbb{E}\left[\mathsf{WR}(\textsc{LEAD})\right]\leq\min\left\{%
    \widetilde{O}\left(T_{\mathrm{LLM}}+\frac{K\log K}{\epsilon_{1,2}}\right),% \widetilde{O}\left(\frac{K\log
    T}{\epsilon_{1,2}}\right)\right\},$ |  | (3) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[\mathsf{WR}(\textsc{LEAD})\right] \leq \min\left\{%
    \widetilde{O}\left(T_{\mathrm{LLM}}+\frac{K\log K}{\epsilon_{1,2}}\right),% \widetilde{O}\left(\frac{K\log
    T}{\epsilon_{1,2}}\right)\right\},$ |  | (3) |'
- en: where $\widetilde{O}(\cdot)$ hides poly-logarithmic factors of $T$.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\widetilde{O}(\cdot)$隐藏了$T$的多项式对数因子。
- en: 'Note that Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") is general such that we do not assume any specific adversarial behaviors
    of the LLM agent, including Assumption[4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "Assumption 4 (Worst-Case Behavior). ‣ B.3.2 Theoretical Guarantees of LEAD ‣
    B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). The proof
    of Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem 4.2
    (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") is provided in Appendix[B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 Theoretical Guarantees of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B
    Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents"). The required assumptions are precisely stated in Appendix [B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"). Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") establishes a best-of-both-worlds result in terms of the efficacy
    and robustness of LEAD.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，定理 [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "定理 4.2（期望后悔）。‣
    4.2 LEAD的理论保证 ‣ 4 算法增强型LLM用于决斗型强盗 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗") 是一般性的，因此我们并未假设LLM代理的任何特定对抗行为，包括假设[4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "假设 4（最坏情况行为）。‣ B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗")。定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2（期望后悔）。‣ 4.2 LEAD的理论保证 ‣ 4 算法增强型LLM用于决斗型强盗 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗")的证明见附录[B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗")。所需的假设在附录 [B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 有用的假设与引理用于决斗型强盗 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗")中有明确陈述。定理 [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2（期望后悔）。‣ 4.2 LEAD的理论保证 ‣ 4 算法增强型LLM用于决斗型强盗 ‣ 超越数值奖励：带LLM代理的上下文决斗型强盗") 确立了在LEAD的效果和稳健性方面的“兼得”结果。
- en: 'Efficacy. As illustrated in Figures[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), [3](https://arxiv.org/html/2407.01887v3#S3.F3
    "Figure 3 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), and  [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"), LEAD has the potential to identify
    the best arm after a short exploration stage. This results in strong and weak
    regret bounds of $\smash{\widetilde{O}(T_{\mathrm{LLM}}+({K}/{\epsilon_{1,2}})\log
    K)}$ and $O(T_{\mathrm{LLM}})$, respectively, that are independent of the horizon
    length $T$, provided the LLM agent suggests a pair of arms that includes the best
    arm $b^{*}$. Furthermore, when the prompt contains extra textual context that
    can infer the relative preferences between arms, $T_{\mathrm{LLM}}$ will become
    smaller, further enhancing the best-case performance. We consider it an important
    direction for future work within the Contextual Dueling Bandit frameworkDudík
    et al. ([2015](https://arxiv.org/html/2407.01887v3#bib.bib27)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 效能。如图[2](https://arxiv.org/html/2407.01887v3#S3.F2 "图 2 ‣ 3.2 实验结果 ‣ 3 LLM作为独立的上下文决策者
    ‣ 超越数值奖励：基于LLM代理的上下文对决赌博者")、[3](https://arxiv.org/html/2407.01887v3#S3.F3 "图 3
    ‣ 3.2 实验结果 ‣ 3 LLM作为独立的上下文决策者 ‣ 超越数值奖励：基于LLM代理的上下文对决赌博者") 和 [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "图 9 ‣ C.2.1 与不同指标的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数值奖励：基于LLM代理的上下文对决赌博者")所示，LEAD在短暂的探索阶段后有可能识别出最佳臂。这导致了强后悔和弱后悔界限分别为$\smash{\widetilde{O}(T_{\mathrm{LLM}}+({K}/{\epsilon_{1,2}})\log
    K)}$和$O(T_{\mathrm{LLM}})$，并且这些界限与时间跨度$T$无关，前提是LLM代理建议的臂对中包含最佳臂$b^{*}$。此外，当提示包含额外的文本上下文，可以推断出臂之间的相对偏好时，$T_{\mathrm{LLM}}$会变得更小，进一步增强最佳情况的表现。我们认为这是在上下文对决赌博框架下未来工作的一个重要方向Dudík等人（[2015](https://arxiv.org/html/2407.01887v3#bib.bib27)）。
- en: 'Guaranteed Convergence. Additionally, both the strong and weak regret for LEAD
    are guaranteed to satisfy a worst-case upper bound of $\smash{\widetilde{O}\left(({K}/{\epsilon_{1,2}})\log
    T\right)}$, which is only worse than the information-theoretic lower bound of
    $\smash{\Omega\left(({K}/{\epsilon_{1,2}})\log T\right)}$ in(Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6))
    by a poly-logarithmic factor of $T$. The worst-case upper bounds on the strong
    and weak regret hold regardless of the specific prompting scenario, ensuring that
    LEAD maintains its theoretical guarantees even in the presence of noisy or adversarial
    prompts, as considered in Theorem[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "Theorem 4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). This safety guarantee is particularly important in practical applications,
    where the prompts provided to the LLM agent may not always be optimal.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 保证收敛性。此外，LEAD的强后悔和弱后悔都保证满足最坏情况的上界$\smash{\widetilde{O}\left(({K}/{\epsilon_{1,2}})\log
    T\right)}$，该上界仅比信息理论下界$\smash{\Omega\left(({K}/{\epsilon_{1,2}})\log T\right)}$（Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)）差一个多对数因子$T$。强后悔和弱后悔的最坏情况上界适用于任何特定的提示场景，确保LEAD在存在噪声或对抗性提示的情况下仍能保持其理论保证，如定理[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "定理 4.1（脆弱性）。 ‣ 4.2 LEAD的理论保证 ‣ 4 算法增强型LLM用于对决赌博者 ‣ 超越数值奖励：基于LLM代理的上下文对决赌博者")中所考虑的那样。这一安全保证在实际应用中尤为重要，因为提供给LLM代理的提示可能并不总是最优的。
- en: 'The following theorem indicates that the additional term $({K\log K})/{\epsilon_{1,2}}$
    in ([3](https://arxiv.org/html/2407.01887v3#S4.E3 "In Theorem 4.2 (Expected Regret).
    ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling
    Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"))
    is almost tight. Its proof is provided in Appendix [B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 Theoretical Guarantees of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B
    Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents").'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下定理表明，额外项$({K\log K})/{\epsilon_{1,2}}$在([3](https://arxiv.org/html/2407.01887v3#S4.E3
    "在定理4.2（期望后悔）中。‣ 4.2 LEAD的理论保证 ‣ 4 决斗盗贼算法增强型LLM ‣ 超越数值奖励：具有LLM代理的上下文决斗盗贼"))几乎是紧的。其证明见附录[B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2
    "B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：具有LLM代理的上下文决斗盗贼")。
- en: Theorem 4.3  (Converse).
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理4.3（反向定理）。
- en: Given any algorithm ALG for dueling bandits provided with an in-context LLM
    agent recommending arms, if it satisfies $\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq
    T_{\mathrm{LLM}}$ for all $T_{\mathrm{LLM}}$, then it must hold ${\mathbb{E}\left[\mathsf{SR}(\textsc{ALG})\right]\geq\mathbb{E}\left[\mathsf{%
    WR}(\textsc{ALG})\right]\geq\Omega\left(T\right)}$ for some instance of the LLM
    agent.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 给定任何一个用于决斗盗贼问题的算法ALG，其中该算法有一个上下文中的LLM代理推荐臂，如果它满足$\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq
    T_{\mathrm{LLM}}$对于所有$T_{\mathrm{LLM}}$，那么必须满足${\mathbb{E}\left[\mathsf{SR}(\textsc{ALG})\right]\geq\mathbb{E}\left[\mathsf{%
    WR}(\textsc{ALG})\right]\geq\Omega\left(T\right)}$对于某些LLM代理的实例。
- en: 4.3 Empirical Evaluation of LEAD
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 LEAD的实证评估
- en: Regarding [Q2], we design a two-fold evaluation to assess efficacy and robustness.
    The evaluation is conducted on the Transitive-Easy instance, which provides higher
    distinguishability, allowing us to observe convergence and regret differences
    within a practical number of steps. First, we compare the strong and weak regret
    of LEAD against state-of-the-art baseline algorithms to validate its efficacy.
    Second, we investigate the robustness of LEAD with noisy and adversarial prompts.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 关于[Q2]，我们设计了一个双重评估方法来评估效能和鲁棒性。评估在Transitive-Easy实例上进行，这个实例具有更高的可区分性，使我们能够观察到在实际步骤数量内的收敛和后悔差异。首先，我们比较LEAD与最先进基线算法的强后悔与弱后悔，以验证其效能。其次，我们研究LEAD在噪声和对抗性提示下的鲁棒性。
- en: '4.3.1 Efficacy Evaluation: Strong Regret and Weak Regret'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 效能评估：强后悔与弱后悔
- en: '![Refer to caption](img/9955bcd01d1a6303724c0837edc924ef.png)![Refer to caption](img/8af5114bbac9949e54f8be057a877240.png)![Refer
    to caption](img/9d3dd9f9a9cea1d2dcefe542cede0cbe.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/9955bcd01d1a6303724c0837edc924ef.png)![参见标题](img/8af5114bbac9949e54f8be057a877240.png)![参见标题](img/9d3dd9f9a9cea1d2dcefe542cede0cbe.png)'
- en: 'Figure 5: Comparisons between LEAD, GPT-4 Turbo, and baseline algorithms (IF2,
    Self-Sparring and DTS). Left and Middle: strong and weak regret on the Transitive-Easy
    instance. Right: robustness evaluation under prompt perturbations (prompts are
    in Appendix [C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2 "C.1.2 Design
    of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：LEAD、GPT-4 Turbo与基线算法（IF2、Self-Sparring和DTS）的比较。左侧和中间：在Transitive-Easy实例上的强后悔与弱后悔。右侧：在提示扰动下的鲁棒性评估（提示见附录[C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2
    "C.1.2 提示设计 ‣ C.1 LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：具有LLM代理的上下文决斗盗贼")）。
- en: 'Hyper-parameters. In our implementation of LEAD (see Algorithm[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")),
    there are two hyper-parameters: the threshold parameter $t$, which controls the
    maximum number of comparisons between arms, and the confidence parameter $\delta$,
    which determines the confidence level for pruning suboptimal arms. For the threshold
    parameter $t$, we considered values from the set $\{50,100,200\}$, and for the
    confidence parameter $\delta$, we explored values from $\{0.1,0.2,0.4\}$. After
    fine-tuning, we found that setting $t=50$ and $\delta=0.4$ provided the best performance
    in terms of cumulative regret.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '超参数。在我们实现的LEAD中（参见算法[1](https://arxiv.org/html/2407.01887v3#algorithm1 "Algorithm
    1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")），有两个超参数：阈值参数$t$，它控制臂之间的最大比较次数，以及置信度参数$\delta$，它决定了修剪次优臂的置信度水平。对于阈值参数$t$，我们考虑了来自集合$\{50,100,200\}$的值，对于置信度参数$\delta$，我们探索了来自$\{0.1,0.2,0.4\}$的值。经过微调，我们发现设置$t=50$和$\delta=0.4$在累积后悔方面提供了最佳的性能。'
- en: 'We evaluate the cumulative strong and weak regret performance of the proposed
    LEAD with different confidence parameter settings ($\delta=0.1,0.2,0.4$) and $t=50$:
    Figure [5](https://arxiv.org/html/2407.01887v3#S4.F5 "Figure 5 ‣ 4.3.1 Efficacy
    Evaluation: Strong Regret and Weak Regret ‣ 4.3 Empirical Evaluation of LEAD ‣
    4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (Left and Middle) demonstrates that LEAD exhibits
    competitive performance across different $\delta$ values. For strong regret, $\delta=0.1$
    results in more conservative exploration, leading to slightly higher regret compared
    to baselines. As $\delta$ increases ($\delta=0.2$ or $0.4$), LEAD achieves lower
    cumulative strong regret, outperforming all the baselines at $\delta=0.4$ due
    to more aggressive exploration to identify the optimal arm sooner. Similarly,
    for weak regret, LEAD consistently achieves superior performance. When $\delta=0.2$
    and $\delta=0.4$, LEAD effectively identifies and includes the optimal arm in
    comparisons. These hyper-parameter values strike a balance between the number
    of comparisons required to identify the best arm and the confidence level for
    pruning suboptimal arms, enabling LEAD to efficiently explore and exploit the
    available arms in-context for the dueling bandits setting.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们评估了在不同置信度参数设置下（$\delta=0.1,0.2,0.4$）以及$t=50$时，所提出的LEAD的累积强后悔和弱后悔表现：图[5](https://arxiv.org/html/2407.01887v3#S4.F5
    "Figure 5 ‣ 4.3.1 Efficacy Evaluation: Strong Regret and Weak Regret ‣ 4.3 Empirical
    Evaluation of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")（左侧和中间）展示了LEAD在不同$\delta$值下表现出具有竞争力的性能。对于强后悔，$\delta=0.1$导致更保守的探索，造成与基线相比稍微更高的后悔。当$\delta$增大（$\delta=0.2$或$0.4$）时，LEAD通过更积极的探索尽早识别最佳臂，从而实现了较低的累积强后悔，并在$\delta=0.4$时超过了所有基线。类似地，针对弱后悔，LEAD始终表现出更优的性能。当$\delta=0.2$和$\delta=0.4$时，LEAD有效地识别并在比较中包含了最优臂。这些超参数值在识别最佳臂所需的比较次数和修剪次优臂的置信度之间取得了平衡，使得LEAD能够高效地在对抗性臂的设置中进行探索和开发。'
- en: '4.3.2 Robustness Evaluation: Noisy and Adversarial Prompts'
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 鲁棒性评估：噪声和对抗性提示
- en: 'Recent studies (Loya et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib28);
    Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)) have
    emphasized the importance of varying prompts to elicit the desired behavior from
    LLMs in decision-making tasks, highlighting the potential limitations of prompt
    quality. Results obtained from a single prompt template may lead to unreliable
    conclusions that cannot generalize to real-world situations where optimal prompts
    are often unavailable. Thus, we evaluate the robustness of LEAD by employing two
    types of prompt perturbations (see Figure[8](https://arxiv.org/html/2407.01887v3#A3.F8
    "Figure 8 ‣ C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")) along with the original prompt (see Figure [7](https://arxiv.org/html/2407.01887v3#A3.F7
    "Figure 7 ‣ C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")). Across all scenarios, LEAD demonstrates superior
    performance and robustness compared to standalone GPT-4 Turbo.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究（Loya 等，[2023](https://arxiv.org/html/2407.01887v3#bib.bib28); Krishnamurthy
    等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4)）强调了在决策任务中，通过变化提示来引导 LLMs（大语言模型）表现出期望行为的重要性，突出了提示质量的潜在局限性。从单一提示模板获得的结果可能会导致不可靠的结论，这些结论无法推广到现实世界中的情况，因为在实际场景中，最优提示通常无法获得。因此，我们通过使用两种类型的提示扰动（见图[8](https://arxiv.org/html/2407.01887v3#A3.F8
    "图 8 ‣ C.1.2 提示设计 ‣ C.1 LLM 实验结果 ‣ 附录 C 提示设计与补充结果 ‣ 超越数字奖励：在上下文中与 LLM 代理进行对决的强盗")）以及原始提示（见图[7](https://arxiv.org/html/2407.01887v3#A3.F7
    "图 7 ‣ C.1.2 提示设计 ‣ C.1 LLM 实验结果 ‣ 附录 C 提示设计与补充结果 ‣ 超越数字奖励：在上下文中与 LLM 代理进行对决的强盗")）来评估
    LEAD 的鲁棒性。在所有场景中，LEAD 展示了优于单独 GPT-4 Turbo 的性能和鲁棒性。
- en: '![Refer to caption](img/51f715d6fe929691c12b8811b2a9ab60.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/51f715d6fe929691c12b8811b2a9ab60.png)'
- en: 'Figure 6: Duel selection trajectory of GPT-4 Turbo and LEAD under different
    settings (Figures [7](https://arxiv.org/html/2407.01887v3#A3.F7 "Figure 7 ‣ C.1.2
    Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") and [8](https://arxiv.org/html/2407.01887v3#A3.F8 "Figure 8 ‣ C.1.2
    Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and
    Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")). Top: original prompt. Middle: noisy prompt (biased history). Bottom:
    adversarial prompt (reversed goal).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在不同设置下，GPT-4 Turbo 和 LEAD 的对决选择轨迹（图[7](https://arxiv.org/html/2407.01887v3#A3.F7
    "图 7 ‣ C.1.2 提示设计 ‣ C.1 LLM 实验结果 ‣ 附录 C 提示设计与补充结果 ‣ 超越数字奖励：在上下文中与 LLM 代理进行对决的强盗")
    和 [8](https://arxiv.org/html/2407.01887v3#A3.F8 "图 8 ‣ C.1.2 提示设计 ‣ C.1 LLM 实验结果
    ‣ 附录 C 提示设计与补充结果 ‣ 超越数字奖励：在上下文中与 LLM 代理进行对决的强盗")）。上：原始提示。中：噪声提示（偏向的历史）。下：对抗提示（反向目标）。
- en: 'Original Prompt. Under the initial prompt, LEAD leverages the LLM’s ability
    to quickly identify the best arm through exploration (under the DB instance within
    RDB Eq.([2](https://arxiv.org/html/2407.01887v3#S3.E2 "In 3.2 Experimental results
    ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents"))). As shown in Figure [6](https://arxiv.org/html/2407.01887v3#S4.F6
    "Figure 6 ‣ 4.3.2 Robustness Evaluation: Noisy and Adversarial Prompts ‣ 4.3 Empirical
    Evaluation of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") (Top Row), we observe that
    LEAD benefits from the LLM’s exploration ability by initializing with the best
    arm as the incumbent when entering the IF2 phase. Compared to GPT-4 Turbo, convergence
    to the Condorcet winner is guaranteed for LEAD with high probability.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 原始提示。在初始提示下，LEAD 利用 LLM 快速通过探索识别最佳选择（在 RDB Eq.([2](https://arxiv.org/html/2407.01887v3#S3.E2
    "在 3.2 实验结果 ‣ 3 LLMs 作为独立上下文决策者 ‣ 超越数字奖励：在上下文中与 LLM 代理进行对决的强盗")）中的 DB 实例）。如图[6](https://arxiv.org/html/2407.01887v3#S4.F6
    "图 6 ‣ 4.3.2 鲁棒性评估：噪声和对抗性提示 ‣ 4.3 LEAD 的实证评估 ‣ 4 算法增强的 LLM 用于对决强盗 ‣ 超越数字奖励：在上下文中与
    LLM 代理进行对决的强盗")（顶部行）所示，我们观察到，LEAD 通过在进入 IF2 阶段时初始化为现有的最佳选择，充分利用了 LLM 的探索能力。与 GPT-4
    Turbo 相比，LEAD 高概率地保证了收敛到 Condorcet 获胜者。
- en: Biased History. We inject an incorrect history into the prompt, where each non-optimal
    arm initially wins against the best arm 10 times, while keeping the underlying
    preference matrix unchanged. LLM agents are observed to get trapped in local optima
    for extended periods, where LEAD overcomes this limitation by employing uniform
    comparisons in the IF2 phase to escape such suboptimal exploration modes.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 偏见历史。我们在提示中注入了一个不正确的历史记录，其中每个非最优臂最初在与最佳臂的对战中获胜 10 次，同时保持基础的偏好矩阵不变。观察到，LLM 代理在较长时间内会陷入局部最优解，而
    LEAD 通过在 IF2 阶段采用均匀比较来克服这一限制，从而摆脱这种次优探索模式。
- en: Reversed Goal. When the prompt is adversarially modified from maximizing reward
    to minimizing, the LLM consistently recommends non-optimal arms after its exploration
    stage. Even with adversarial prompts, LEAD still achieves near-optimal cumulative
    strong regret. Since the LLM’s exploration capability is only utilized within
    the bounded length of the Match Arms procedure, the impact of the reversed goal
    on the exploitation phase is mitigated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 反向目标。当提示从最大化奖励修改为最小化时，LLM 在其探索阶段后会持续推荐非最优臂。即使在对抗性提示下，LEAD 仍然能够实现接近最优的累积强后悔。由于
    LLM 的探索能力仅在 Match Arms 程序的有限长度内使用，因此反向目标对利用阶段的影响得到了缓解。
- en: 'Figure [5](https://arxiv.org/html/2407.01887v3#S4.F5 "Figure 5 ‣ 4.3.1 Efficacy
    Evaluation: Strong Regret and Weak Regret ‣ 4.3 Empirical Evaluation of LEAD ‣
    4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") (right) presents the cumulative strong regret
    results comparing LEAD against standalone LLM agents and the IF2 algorithm across
    three prompt designs. Notably, LEAD with $\delta=1/(TK^{2})$ (consistent with
    IF2 to showcase the robust behavior) achieves near-optimal cumulative regret with
    low variance even with noisy and adversarial prompts, validating the regret bounds
    stated in Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem
    4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). LEAD and IF2 converge to the best arm within 2000 steps, while GPT-4
    Turbo’s cumulative expected regret continues to increase, indicating the instability
    of standalone in-context LLM agents.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [5](https://arxiv.org/html/2407.01887v3#S4.F5 "Figure 5 ‣ 4.3.1 Efficacy
    Evaluation: Strong Regret and Weak Regret ‣ 4.3 Empirical Evaluation of LEAD ‣
    4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")（右侧）展示了 LEAD 相对于独立的 LLM 代理和 IF2 算法在三种提示设计下的累积强后悔结果。值得注意的是，LEAD
    以 $\delta=1/(TK^{2})$（与 IF2 一致，以展示其稳健性）即使在嘈杂和对抗性提示下，仍能实现接近最优的累积后悔，并且方差较低，验证了定理
    [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem 4.2 (Expected
    Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    中所述的后悔界限。LEAD 和 IF2 在 2000 步内收敛到最佳臂，而 GPT-4 Turbo 的累积预期后悔持续增加，表明独立的上下文 LLM 代理的不稳定性。'
- en: 5 Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This paper evaluates LLMs as in-context decision-makers for standard context-free
    dueling bandits (DB) with a Condorcet Winner, offering the first systematic insights
    into their strengths and limitations. Our findings reveal that LLMs’ decision-making
    in DB, driven by linguistic priors, achieves exceptional weak regret performance
    across both transitive and intransitive environment instances in the short-term.
    However, LLMs lack the necessary criteria for convergence and long-term performance
    generalization to hard scenarios, leading to an optimality gap between LLMs and
    classic DB algorithms in terms of strong regret. To bridge this gap, we propose
    LEAD, an agentic flow framework that integrates off-the-shelf DB algorithms with
    LLM agents through fine-grained adaptive interplay. This framework provides theoretical
    guarantees and demonstrates robust performance even under noisy and adversarial
    prompts. Our work contributes to the problem of In-Context Reinforcement Learning
    (ICRL). The framework we proposed sheds light on how language-based reasoning
    can inspire robust frameworks that translate words into actions, paving the way
    for more trustworthy AI systems through the interplay between rule-based experts
    and in-context LLM agents.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 本文评估了大型语言模型（LLM）作为标准无上下文决斗赌徒（DB）问题中具有康多塞胜者的上下文决策者，为其优缺点提供了首个系统化的洞察。我们的研究发现，LLM
    在 DB 中的决策表现出色，其由语言先验驱动，在短期内能够在传递和不传递环境实例中实现优异的弱后悔表现。然而，LLM 缺乏收敛性和长期性能泛化所必需的标准，导致在强后悔方面，LLM
    与经典的 DB 算法之间存在最优性差距。为了弥合这一差距，我们提出了 LEAD，这是一个代理流框架，通过细粒度的自适应交互将现有的 DB 算法与 LLM 代理集成。该框架提供了理论保证，并在嘈杂和对抗性提示下展示了强大的表现。我们的工作为上下文强化学习（ICRL）问题作出了贡献。我们提出的框架阐明了基于语言的推理如何激发出强健的框架，将语言转化为行动，为通过基于规则的专家与上下文
    LLM 代理之间的交互实现更值得信赖的 AI 系统铺平了道路。
- en: References
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Laskin et al. [2022] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto,
    Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos,
    Ethan Brooks, et al. In-context reinforcement learning with algorithm distillation.
    *arXiv preprint arXiv:2210.14215*, 2022.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laskin 等人 [2022] Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen
    Spencer, Richie Steigerwald, DJ Strouse, Steven Hansen, Angelos Filos, Ethan Brooks
    等人. 通过算法蒸馏进行上下文强化学习。*arXiv 预印本 arXiv:2210.14215*，2022年。
- en: Lee et al. [2024] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea
    Finn, Ofir Nachum, and Emma Brunskill. Supervised pretraining can learn in-context
    reinforcement learning. *Advances in Neural Information Processing Systems*, 36,
    2024.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2024] Jonathan Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea
    Finn, Ofir Nachum 和 Emma Brunskill. 监督预训练能够学习上下文中的强化学习。*神经信息处理系统进展*，36，2024年。
- en: Park et al. [2024] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar, and Kaiqing Zhang.
    Do llm agents have regret? a case study in online learning and games. *arXiv preprint
    arXiv:2403.16843*, 2024.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人 [2024] Chanwoo Park, Xiangyu Liu, Asuman Ozdaglar 和 Kaiqing Zhang. LLM
    代理是否存在后悔？在线学习与博弈中的案例研究。*arXiv 预印本 arXiv:2403.16843*，2024年。
- en: Krishnamurthy et al. [2024] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster,
    Cyril Zhang, and Aleksandrs Slivkins. Can large language models explore in-context?
    *arXiv preprint arXiv:2403.15371*, 2024.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnamurthy 等人 [2024] Akshay Krishnamurthy, Keegan Harris, Dylan J Foster,
    Cyril Zhang 和 Aleksandrs Slivkins. 大型语言模型能在上下文中进行探索吗？ *arXiv 预印本 arXiv:2403.15371*，2024年。
- en: 'Nie et al. [2024] Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V
    Le, and Minmin Chen. Evolve: Evaluating and optimizing llms for exploration. *arXiv
    preprint arXiv:2410.06238*, 2024.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nie 等人 [2024] Allen Nie, Yi Su, Bo Chang, Jonathan N Lee, Ed H Chi, Quoc V Le
    和 Minmin Chen. Evolve：评估和优化 LLMs 以进行探索。*arXiv 预印本 arXiv:2410.06238*，2024年。
- en: Yue et al. [2012] Yisong Yue, Josef Broder, Robert Kleinberg, and Thorsten Joachims.
    The k-armed dueling bandits problem. *Journal of Computer and System Sciences*,
    78(5):1538--1556, 2012.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等人 [2012] Yisong Yue, Josef Broder, Robert Kleinberg 和 Thorsten Joachims.
    k臂决斗赌徒问题。*计算机与系统科学学报*，78(5)：1538-1556，2012年。
- en: Zoghi et al. [2014a] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke, and
    Remi Munos. Relative confidence sampling for efficient on-line ranker evaluation.
    In *Proceedings of the 7th ACM international conference on Web search and data
    mining*, pages 73--82, 2014a.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoghi 等人 [2014a] Masrour Zoghi, Shimon A Whiteson, Maarten De Rijke 和 Remi Munos.
    为了高效的在线排序评估，采用相对置信度采样方法。见于 *第七届 ACM 国际网络搜索与数据挖掘会议论文集*，第73-82页，2014a年。
- en: Wirth et al. [2017] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes
    Fürnkranz. A survey of preference-based reinforcement learning methods. *Journal
    of Machine Learning Research*, 18(136):1--46, 2017.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wirth 等 [2017] Christian Wirth, Riad Akrour, Gerhard Neumann 和 Johannes Fürnkranz.
    基于偏好的强化学习方法综述。*机器学习研究杂志*，第18卷(136)：1--46，2017年。
- en: 'Pacchiano et al. [2021] Aldo Pacchiano, Aadirupa Saha, and Jonathan Lee. Dueling
    rl: reinforcement learning with trajectory preferences. *arXiv preprint arXiv:2111.04850*,
    2021.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pacchiano 等 [2021] Aldo Pacchiano, Aadirupa Saha 和 Jonathan Lee. 对战强化学习：具有轨迹偏好的强化学习。*arXiv
    预印本 arXiv:2111.04850*，2021年。
- en: Yue and Joachims [2009] Yisong Yue and Thorsten Joachims. Interactively optimizing
    information retrieval systems as a dueling bandits problem. In *Proceedings of
    the 26th Annual International Conference on Machine Learning*, pages 1201--1208,
    2009.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 和 Joachims [2009] Yisong Yue 和 Thorsten Joachims. 通过互动优化信息检索系统为对战强盗问题。见于
    *第26届国际机器学习大会论文集*，第1201--1208页，2009年。
- en: Sui et al. [2017] Yanan Sui, Vincent Zhuang, Joel W Burdick, and Yisong Yue.
    Multi-dueling bandits with dependent arms. *arXiv preprint arXiv:1705.00253*,
    2017.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sui 等 [2017] Yanan Sui, Vincent Zhuang, Joel W Burdick 和 Yisong Yue. 具有依赖臂的多对战强盗。*arXiv
    预印本 arXiv:1705.00253*，2017年。
- en: Ailon et al. [2014] Nir Ailon, Zohar Karnin, and Thorsten Joachims. Reducing
    dueling bandits to cardinal bandits. In *International Conference on Machine Learning*,
    pages 856--864\. PMLR, 2014.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ailon 等 [2014] Nir Ailon, Zohar Karnin 和 Thorsten Joachims. 将对战强盗问题简化为基数强盗问题。见于
    *国际机器学习大会*，第856--864页。PMLR，2014年。
- en: 'Saha and Gaillard [2022] Aadirupa Saha and Pierre Gaillard. Versatile dueling
    bandits: Best-of-both-world analyses for online learning from preferences. *arXiv
    preprint arXiv:2202.06694*, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha 和 Gaillard [2022] Aadirupa Saha 和 Pierre Gaillard. 多功能对战强盗：基于偏好的在线学习的最佳分析。*arXiv
    预印本 arXiv:2202.06694*，2022年。
- en: Stiennon et al. [2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning
    to summarize with human feedback. *Advances in Neural Information Processing Systems*,
    33:3008--3021, 2020.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stiennon 等 [2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan
    Lowe, Chelsea Voss, Alec Radford, Dario Amodei 和 Paul F Christiano. 通过人类反馈学习总结。*神经信息处理系统进展*，第33卷：3008--3021，2020年。
- en: Wang et al. [2024a] Yuanhao Wang, Qinghua Liu, and Chi Jin. Is rlhf more difficult
    than standard rl? a theoretical perspective. *Advances in Neural Information Processing
    Systems*, 36, 2024a.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2024a] Yuanhao Wang, Qinghua Liu 和 Chi Jin. RLHF 是否比标准 RL 更难？一种理论视角。*神经信息处理系统进展*，第36卷，2024a。
- en: Mirchandani et al. [2023] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,
    Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy
    Zeng. Large language models as general pattern machines. *arXiv preprint arXiv:2307.04721*,
    2023.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirchandani 等 [2023] Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter,
    Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh 和 Andy Zeng.
    大型语言模型作为通用模式机器。*arXiv 预印本 arXiv:2307.04721*，2023年。
- en: Chen et al. [2024] Dingyang Chen, Qi Zhang, and Yinglun Zhu. Efficient sequential
    decision making with large language models. *arXiv preprint arXiv:2406.12125*,
    2024.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2024] Dingyang Chen, Qi Zhang 和 Yinglun Zhu. 利用大型语言模型进行高效的顺序决策。*arXiv
    预印本 arXiv:2406.12125*，2024年。
- en: Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*,
    2022.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler 等.
    大型语言模型的涌现能力。*arXiv 预印本 arXiv:2206.07682*，2022年。
- en: Kojima et al. [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199--22213, 2022.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima 等 [2022] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo
    和 Yusuke Iwasawa. 大型语言模型是零-shot推理者。*神经信息处理系统进展*，第35卷：22199--22213，2022年。
- en: Wu and Liu [2016] Huasen Wu and Xin Liu. Double thompson sampling for dueling
    bandits. *Advances in neural information processing systems*, 29, 2016.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 和 Liu [2016] Huasen Wu 和 Xin Liu. 对战强盗的双重汤普森采样。*神经信息处理系统进展*，第29卷，2016年。
- en: Yue and Joachims [2011] Yisong Yue and Thorsten Joachims. Beat the mean bandit.
    In *Proceedings of the 28th international conference on machine learning (ICML-11)*,
    pages 241--248\. Citeseer, 2011.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 和 Joachims [2011] Yisong Yue 和 Thorsten Joachims. 打败平均值强盗. 见于 *第28届国际机器学习大会论文集（ICML-11）*，第241--248页。Citeseer，2011年。
- en: Urvoy et al. [2013] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, and Sami Naamane.
    Generic exploration and k-armed voting bandits. In *International conference on
    machine learning*, pages 91--99\. PMLR, 2013.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Urvoy et al. [2013] Tanguy Urvoy, Fabrice Clerot, Raphael Féraud, 和 Sami Naamane。通用探索与k臂投票强盗问题。在*国际机器学习会议*，第91--99页。PMLR，2013年。
- en: Zoghi et al. [2014b] Masrour Zoghi, Shimon Whiteson, Remi Munos, and Maarten
    Rijke. Relative upper confidence bound for the k-armed dueling bandit problem.
    In *International conference on machine learning*, pages 10--18\. PMLR, 2014b.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zoghi et al. [2014b] Masrour Zoghi, Shimon Whiteson, Remi Munos, 和 Maarten Rijke。k臂对抗强盗问题的相对上置信界。在*国际机器学习会议*，第10--18页。PMLR，2014b年。
- en: Komiyama et al. [2015] Junpei Komiyama, Junya Honda, Hisashi Kashima, and Hiroshi
    Nakagawa. Regret lower bound and optimal algorithm in dueling bandit problem.
    In *Conference on learning theory*, pages 1141--1154\. PMLR, 2015.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Komiyama et al. [2015] Junpei Komiyama, Junya Honda, Hisashi Kashima, 和 Hiroshi
    Nakagawa。对抗强盗问题中的遗憾下界与最优算法。在*学习理论会议*，第1141--1154页。PMLR，2015年。
- en: 'Bradley and Terry [1952] Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    39(3/4):324--345, 1952.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bradley and Terry [1952] Ralph Allan Bradley 和 Milton E Terry。不完全区组设计的排名分析：I.
    配对比较法。*生物统计学*，39(3/4)：324--345，1952年。
- en: Wang et al. [2024b] Qixun Wang, Yifei Wang, Yisen Wang, and Xianghua Ying. Can
    in-context learning really generalize to out-of-distribution tasks? *arXiv preprint
    arXiv:2410.09695*, 2024b.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2024b] Qixun Wang, Yifei Wang, Yisen Wang, 和 Xianghua Ying。上下文学习能否真正推广到分布外任务？*arXiv
    预印本 arXiv:2410.09695*，2024b年。
- en: Dudík et al. [2015] Miroslav Dudík, Katja Hofmann, Robert E Schapire, Aleksandrs
    Slivkins, and Masrour Zoghi. Contextual dueling bandits. In *Conference on Learning
    Theory*, pages 563--587\. PMLR, 2015.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dudík et al. [2015] Miroslav Dudík, Katja Hofmann, Robert E Schapire, Aleksandrs
    Slivkins, 和 Masrour Zoghi。情境对抗型多臂强盗问题。在*学习理论会议*，第563--587页。PMLR，2015年。
- en: 'Loya et al. [2023] Manikanta Loya, Divya Anand Sinha, and Richard Futrell.
    Exploring the sensitivity of llms’ decision-making capabilities: Insights from
    prompt variation and hyperparameters. *arXiv preprint arXiv:2312.17476*, 2023.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loya et al. [2023] Manikanta Loya, Divya Anand Sinha, 和 Richard Futrell。探索LLM决策能力的敏感性：来自提示变异与超参数的洞察。*arXiv
    预印本 arXiv:2312.17476*，2023年。
- en: Tucker et al. [2020] Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui,
    Yisong Yue, Joel W Burdick, and Aaron D Ames. Preference-based learning for exoskeleton
    gait optimization. In *2020 IEEE international conference on robotics and automation
    (ICRA)*, pages 2351--2357\. IEEE, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tucker et al. [2020] Maegan Tucker, Ellen Novoseller, Claudia Kann, Yanan Sui,
    Yisong Yue, Joel W Burdick, 和 Aaron D Ames。基于偏好的外骨骼步态优化学习。在*2020年IEEE国际机器人与自动化大会（ICRA）*，第2351--2357页。IEEE，2020年。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in neural information processing systems*, 35:27730--27744, 2022.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等人。训练语言模型遵循人类反馈的指令。*神经信息处理系统进展*，35：27730--27744，2022年。
- en: Baheri and Alm [2023] Ali Baheri and Cecilia O Alm. Llms-augmented contextual
    bandit. *arXiv preprint arXiv:2311.02268*, 2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baheri and Alm [2023] Ali Baheri 和 Cecilia O Alm。基于LLM增强的上下文强盗问题。*arXiv 预印本
    arXiv:2311.02268*，2023年。
- en: Zhou et al. [2022] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, 等人。最小到最多提示使大语言模型能够进行复杂推理。*arXiv
    预印本 arXiv:2205.10625*，2022年。
- en: 'Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36, 2024.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2024] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan。思维树：使用大语言模型进行深思熟虑的问题解决。*神经信息处理系统进展*，36，2024年。
- en: 'Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, et al.
    Inner monologue: Embodied reasoning through planning with language models. *arXiv
    preprint arXiv:2207.05608*, 2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang,
    Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar 等人.
    内心独白：通过规划与语言模型进行具身推理。*arXiv预印本arXiv:2207.05608*，2022年。
- en: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, and Zhiting Hu. Reasoning with language model is planning with
    world model. *arXiv preprint arXiv:2305.14992*, 2023.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao et al. [2023] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang,
    Daisy Zhe Wang, 和 Zhiting Hu. 用语言模型进行推理就是用世界模型进行规划。*arXiv预印本arXiv:2305.14992*，2023年。
- en: 'Brohan et al. [2023] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. Do as i can, not as i say: Grounding language in robotic affordances. In
    *Conference on robot learning*, pages 287--318\. PMLR, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan et al. [2023] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian
    等人. 做我能做的，而不是我说的：将语言与机器人能力相结合。在*机器人学习会议*上，页码287--318。PMLR，2023年。
- en: 'Ma et al. [2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka:
    Human-level reward design via coding large language models. *arXiv preprint arXiv:2310.12931*,
    2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2023] Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang,
    Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. Eureka：通过编码大型语言模型进行人类水平的奖励设计。*arXiv预印本arXiv:2310.12931*，2023年。
- en: 'Wang et al. [2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied
    agent with large language models. *arXiv preprint arXiv:2305.16291*, 2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan 和 Anima Anandkumar. Voyager：一个基于大型语言模型的开放式具身代理。*arXiv预印本arXiv:2305.16291*，2023年。
- en: Liu et al. [2024] Tennison Liu, Nicolás Astorga, Nabeel Seedat, and Mihaela
    van der Schaar. Large language models to enhance bayesian optimization. *arXiv
    preprint arXiv:2402.03921*, 2024.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024] Tennison Liu, Nicolás Astorga, Nabeel Seedat 和 Mihaela van
    der Schaar. 使用大型语言模型增强贝叶斯优化。*arXiv预印本arXiv:2402.03921*，2024年。
- en: 'Hajiesmaili et al. [2020] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John
    Lui, Wing Shing Wong, et al. Adversarial bandits with corruptions: Regret lower
    bound and no-regret algorithm. *Advances in Neural Information Processing Systems*,
    33:19943--19952, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hajiesmaili et al. [2020] Mohammad Hajiesmaili, Mohammad Sadegh Talebi, John
    Lui, Wing Shing Wong 等人. 具有腐败的对抗性赌博机：遗憾下界和无遗憾算法。*神经信息处理系统进展*，33：19943--19952，2020年。
- en: Hoeffding [1994] Wassily Hoeffding. Probability inequalities for sums of bounded
    random variables. *The collected works of Wassily Hoeffding*, pages 409--426,
    1994.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoeffding [1994] Wassily Hoeffding. 有界随机变量和的概率不等式。*Wassily Hoeffding文集*，页码409--426，1994年。
- en: 'Plackett [1975] Robin L Plackett. The analysis of permutations. *Journal of
    the Royal Statistical Society Series C: Applied Statistics*, 24(2):193--202, 1975.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Plackett [1975] Robin L Plackett. 排列分析。*英国皇家统计学会C系列：应用统计学期刊*，24(2)：193--202，1975年。
- en: Appendix
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: 'This appendix provides supplementary information and additional experimental
    results to support the main text. The content is organized into three main parts:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录提供了支持主文内容的补充信息和额外的实验结果。内容被组织成三个主要部分：
- en: A.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A.
- en: Related Works
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相关工作
- en: B.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B.
- en: 'Theoretical Part: Algorithm Design and Analysis of LEAD'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论部分：LEAD的算法设计与分析
- en: •
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [B.1](https://arxiv.org/html/2407.01887v3#A2.SS1 "B.1 Algorithm Design
    Logic ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") presents the algorithm design logic
    using Explore-then-Exploit methods.'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '附录[B.1](https://arxiv.org/html/2407.01887v3#A2.SS1 "B.1 Algorithm Design Logic
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")介绍了使用“先探索再利用”方法的算法设计逻辑。'
- en: •
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [B.2](https://arxiv.org/html/2407.01887v3#A2.SS2 "B.2 Detailed Procedure
    Description ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") describes the LEAD algorithm
    stated in Section[4.1](https://arxiv.org/html/2407.01887v3#S4.SS1 "4.1 Algorithmic
    Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents"), detailing its key features
    and implementation remarks.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.2](https://arxiv.org/html/2407.01887v3#A2.SS2 "B.2 详细过程描述 ‣ 附录B LEAD算法设计与分析
    ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 描述了在[4.1](https://arxiv.org/html/2407.01887v3#S4.SS1
    "4.1 LEAD的算法设计 ‣ 4 针对对抗赌博的算法增强LLM ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")节中说明的LEAD算法，详细介绍了其关键特性和实现备注。
- en: •
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1 "B.3.1 Useful
    Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix
    B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling
    Bandits with LLM Agents") presents the necessary definitions, assumptions and
    lemmas for the theoretical analysis of LEAD in Section[4.2](https://arxiv.org/html/2407.01887v3#S4.SS2
    "4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1 "B.3.1 对抗赌博的有用假设与引理
    ‣ B.3 理论分析 ‣ 附录B LEAD算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 提出了第[4.2](https://arxiv.org/html/2407.01887v3#S4.SS2
    "4.2 LEAD的理论保证 ‣ 4 针对对抗赌博的算法增强LLM ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")节中LEAD理论分析所需的定义、假设和引理。
- en: •
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2 "B.3.2 Theoretical
    Guarantees of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and
    Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") proves Theorem[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "Theorem 4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem
    4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3 "Theorem
    4.3 (Converse). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"), establishing LEAD’s regret bounds.'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [B.3.2](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS2 "B.3.2 LEAD的理论保证
    ‣ B.3 理论分析 ‣ 附录B LEAD算法设计与分析 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 证明了定理[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "定理4.1（脆弱性）。 ‣ 4.2 LEAD的理论保证 ‣ 4 针对对抗赌博的算法增强LLM ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")、[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理4.2（期望遗憾）。 ‣ 4.2 LEAD的理论保证 ‣ 4 针对对抗赌博的算法增强LLM ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 和 [4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3
    "定理4.3（对立）。 ‣ 4.2 LEAD的理论保证 ‣ 4 针对对抗赌博的算法增强LLM ‣ 超越数值奖励：与LLM代理的上下文对抗赌博")，确立了LEAD的遗憾边界。
- en: C.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: C.
- en: 'Experimental Part: Prompt Design and Supplementary Results'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实验部分：提示设计与补充结果
- en: •
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [C.1.1](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS1 "C.1.1 Environments
    ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and Supplementary Results
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") illustrates
    the transitive and intransitive environments construction.'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [C.1.1](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS1 "C.1.1 环境 ‣ C.1 LLM实验结果
    ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 说明了及物和不及物环境的构建。
- en: •
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2 "C.1.2 Design
    of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design and Supplementary
    Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    illustrates the prompt design and prompt perturbations logic.'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附录 [C.1.2](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS2 "C.1.2 提示设计 ‣ C.1
    LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：与LLM代理的上下文对抗赌博") 说明了提示设计和提示扰动的逻辑。
- en: •
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3 "C.1.3 Exemplars
    of GPT-4 Turbo and o1-preview ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents") provides exemplars of GPT-4 Turbo to showcase their behavior.'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '附录[C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3 "C.1.3 Exemplars
    of GPT-4 Turbo and o1-preview ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt
    Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")提供了GPT-4 Turbo的示例，以展示其行为。'
- en: •
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Appendix [C.2](https://arxiv.org/html/2407.01887v3#A3.SS2 "C.2 Supplementary
    Experiments ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents") presents supplementary experimental
    results, providing further insights into the performance and behavior of the algorithms
    in Sections[3](https://arxiv.org/html/2407.01887v3#S3 "3 LLMs as Standalone In-Context
    Decision-Makers ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and [4](https://arxiv.org/html/2407.01887v3#S4 "4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '附录[C.2](https://arxiv.org/html/2407.01887v3#A3.SS2 "C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")展示了补充实验结果，进一步阐明了第[3](https://arxiv.org/html/2407.01887v3#S3
    "3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")节和第[4](https://arxiv.org/html/2407.01887v3#S4
    "4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")节中算法的性能和行为。'
- en: Appendix A Related Works
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 相关工作
- en: We provide the detailed related works as follows.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了详细的相关工作如下。
- en: Dueling Bandits. The problem of dueling bandits was initially introduced in
    [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]. Various methods
    have been proposed to tackle the task since then. These methods can be broadly
    classified into two categories as Explore-Then-Exploit methods and Ongoing Regret
    Minimization methods according to [Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)].
    Explore-Then-Exploit methods focus on identifying the best arm with high confidence
    before exploiting it, such as Interleaved Filter (IF) [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]
    and Beat the Mean (BTM) [Yue and Joachims, [2011](https://arxiv.org/html/2407.01887v3#bib.bib21)],
    etc. In contrast, Ongoing Regret Minimization methods explicitly target the objective
    of minimizing cumulative regret, including Relative Upper Confidence Bound (RUCB) [Zoghi
    et al., [2014b](https://arxiv.org/html/2407.01887v3#bib.bib23)] and Self-Sparring[Sui
    et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib11)], etc. Dueling bandit
    problem and preference feedback in general has a wide variety of applications,
    including recommendation systems [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)],
    robotics [Tucker et al., [2020](https://arxiv.org/html/2407.01887v3#bib.bib29)],
    and most recently, the training algorithm of large language models, such as Reinforcement
    Learning from Human Feedback (RLHF) [Ouyang et al., [2022](https://arxiv.org/html/2407.01887v3#bib.bib30)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗土匪问题。对抗土匪问题最早在[Yue等人, [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中提出。此后，提出了多种方法来解决这一任务。根据[Zoghi等人,
    [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)]的分类，这些方法大致可以分为两类：探索-然后利用方法和持续遗憾最小化方法。探索-然后利用方法侧重于在利用之前以高置信度识别最佳臂，例如交替滤波器（IF）[Yue等人,
    [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]和击败均值（BTM）[Yue和Joachims,
    [2011](https://arxiv.org/html/2407.01887v3#bib.bib21)]等。相比之下，持续遗憾最小化方法明确以最小化累积遗憾为目标，包括相对上置信界（RUCB）[Zoghi等人,
    [2014b](https://arxiv.org/html/2407.01887v3#bib.bib23)]和自我对抗[Sui等人, [2017](https://arxiv.org/html/2407.01887v3#bib.bib11)]等。对抗土匪问题和偏好反馈普遍有广泛的应用，包括推荐系统[Yue等人,
    [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]、机器人技术[Tucker等人, [2020](https://arxiv.org/html/2407.01887v3#bib.bib29)]，以及最近的大型语言模型的训练算法，例如来自人类反馈的强化学习（RLHF）[Ouyang等人,
    [2022](https://arxiv.org/html/2407.01887v3#bib.bib30)]。
- en: LLM Agents for Multi-Armed Bandits. Several recent works have explored evaluating
    the capabilities of LLMs in bandit problems. For example, [Baheri and Alm, [2023](https://arxiv.org/html/2407.01887v3#bib.bib31)]
    proposed an approach to enhance contextual bandits by integrating LLMs as encoders.
    The LLMs’ ability to capture rich semantic and syntactic information from textual
    contexts is leveraged to provide the algorithm with a more informative representation
    of the context. The LLM-augmented algorithm transforms the raw context into a
    latent space vector using the LLM’s encoding capabilities. This encoded context
    is then used to guide the decision-making process. [Krishnamurthy et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib4)]
    investigates whether LLMs can engage in exploration in simple MAB environments
    without additional training. They compared various prompt designs and found that
    GPT-4 with zero-shot chain-of-thought (CoT) reasoning and an externally summarized
    interaction history performed the best, while other configurations failed in exploration,
    either by never selecting the best arm after initial rounds or by selecting all
    arms nearly equally often. Different from the previous results, in this work we
    go beyond the settings of numeric rewards and investigate the capabilities of
    LLMs under preference feedback.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂赌博机的LLM智能体。近年来，一些研究探讨了在赌博问题中评估LLM能力的方法。例如，[Baheri 和 Alm，[2023](https://arxiv.org/html/2407.01887v3#bib.bib31)]
    提出了一种通过将LLM集成为编码器来增强上下文赌博机的方法。LLM在文本上下文中捕捉丰富的语义和句法信息的能力被利用，以便为算法提供更具信息量的上下文表示。增强LLM的算法通过LLM的编码能力将原始上下文转化为潜在空间向量。然后，使用此编码的上下文来指导决策过程。[Krishnamurthy
    等，[2024](https://arxiv.org/html/2407.01887v3#bib.bib4)] 研究了LLM是否能在简单的多臂赌博机环境中进行探索，而无需额外的训练。他们比较了各种提示设计，发现具有零样本链式推理（CoT）和外部总结的交互历史的GPT-4表现最佳，而其他配置在探索中失败，要么在初期回合后从未选择最佳臂，要么几乎均等地选择所有臂。与之前的研究结果不同，在本研究中，我们超越了数字奖励的设定，探讨了LLM在偏好反馈下的能力。
- en: In-Context LLMs for Decision-Making. Beyond bandit problems, LLM agents have
    demonstrated strong capabilities in complex reasoning across a wide range of in-context
    reinforcement learning and decision-making tasks[Laskin et al., [2022](https://arxiv.org/html/2407.01887v3#bib.bib1),
    Lee et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib2), Zhou et al.,
    [2022](https://arxiv.org/html/2407.01887v3#bib.bib32), Yao et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib33)].
    Various existing works aim to understand LLM agents’ capabilities for in-context
    decision-making, with notable examples including planning [Huang et al., [2022](https://arxiv.org/html/2407.01887v3#bib.bib34),
    Hao et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib35)]. Additionally,
    LLM agents have been shown to enhance embodied agents in robotic applications
    by providing advanced task planning abilities [Brohan et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib36)]
    and reward designing[Ma et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib37)],
    further enabling the development of lifelong learning agents [Wang et al., [2023](https://arxiv.org/html/2407.01887v3#bib.bib38)].
    Besides these empirical successes, the authors of [Park et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib3)]
    analyzed LLMs’ interactions in online learning and game theory settings through
    the lens of the regret metrics. They identified simple cases where LLMs fail to
    be no-regret. Another line of research incorporates LLMs into classic decision-making
    frameworks to create LLM-augmented online decision-makers. For instance, Liu et
    al.[Liu et al., [2024](https://arxiv.org/html/2407.01887v3#bib.bib39)] utilized
    LLMs to enhance the components of warm starting, sampling candidates, and surrogate
    modeling in Bayesian optimization. Our work contributes to this broad area by
    integrating LLM agents with the classic Explore-then-Exploit DB algorithms to
    enhance the utilization of preference feedback.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 用于决策制定的上下文内大语言模型（LLMs）。超越赌博问题，LLM 代理在跨越广泛的上下文强化学习和决策任务中展现出了强大的推理能力[Laskin 等人,
    [2022](https://arxiv.org/html/2407.01887v3#bib.bib1), Lee 等人, [2024](https://arxiv.org/html/2407.01887v3#bib.bib2),
    Zhou 等人, [2022](https://arxiv.org/html/2407.01887v3#bib.bib32), Yao 等人, [2024](https://arxiv.org/html/2407.01887v3#bib.bib33)]。现有的多项工作旨在理解
    LLM 代理在上下文决策制定中的能力，显著的例子包括规划 [Huang 等人, [2022](https://arxiv.org/html/2407.01887v3#bib.bib34),
    Hao 等人, [2023](https://arxiv.org/html/2407.01887v3#bib.bib35)]。此外，LLM 代理已被证明能够通过提供先进的任务规划能力，提升机器人应用中的具身代理
    [Brohan 等人, [2023](https://arxiv.org/html/2407.01887v3#bib.bib36)] 和奖励设计 [Ma 等人,
    [2023](https://arxiv.org/html/2407.01887v3#bib.bib37)]，进一步促进终身学习代理的发展 [Wang 等人,
    [2023](https://arxiv.org/html/2407.01887v3#bib.bib38)]。除了这些实证成功，[Park 等人, [2024](https://arxiv.org/html/2407.01887v3#bib.bib3)]
    的作者通过回顾度量分析了 LLM 在在线学习和博弈论环境中的交互。他们发现了 LLM 无法满足无悔策略的简单情形。另一类研究将 LLM 融入经典的决策框架，以创建增强型在线决策者。例如，Liu
    等人 [Liu 等人, [2024](https://arxiv.org/html/2407.01887v3#bib.bib39)] 利用 LLM 来增强贝叶斯优化中的热启动、候选采样和代理建模等组件。我们的工作通过将
    LLM 代理与经典的“先探索后开发”DB 算法结合，进一步增强了偏好反馈的利用，推动了这一广泛领域的发展。
- en: Appendix B Algorithm Design and Analysis of LEAD
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LEAD 算法设计与分析
- en: 'In this section, we detail the design principles and implementation of the
    LEAD algorithm. First, we present the algorithm design logic. Then, we provide
    a rigorous proof of Theorem[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "Theorem 4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem
    4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), and [4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3 "Theorem
    4.3 (Converse). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"), establishing the theoretical guarantees of LEAD (IF2 base) under the
    assumptions outlined in Appendix[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了LEAD算法的设计原则和实现方法。首先，我们展示了算法的设计逻辑。然后，我们提供了定理[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1
    "定理 4.1（脆弱性）。 ‣ 4.2 LEAD的理论保证 ‣ 4 双臂老虎机的算法增强LLMs ‣ 超越数值奖励：LLM代理的上下文对战老虎机")、[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2（期望遗憾）。 ‣ 4.2 LEAD的理论保证 ‣ 4 双臂老虎机的算法增强LLMs ‣ 超越数值奖励：LLM代理的上下文对战老虎机") 和[4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3
    "定理 4.3（对立）。 ‣ 4.2 LEAD的理论保证 ‣ 4 双臂老虎机的算法增强LLMs ‣ 超越数值奖励：LLM代理的上下文对战老虎机")的严格证明，建立了在附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 双臂老虎机的有用假设和引理 ‣ B.3 理论分析 ‣ 附录 B LEAD算法设计与分析 ‣ 超越数值奖励：LLM代理的上下文对战老虎机")中概述的假设下，LEAD（IF2
    基础）的理论保证。
- en: B.1 Algorithm Design Logic
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 算法设计逻辑
- en: 'Explore-then-Exploit algorithms as Ideal Candidates. Classic DB algorithms
    can be classified into two categories: Explore-Then-Exploit methods and Ongoing
    Regret Minimization methods [Zoghi et al., [2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)].
    Among these, Explore-Then-Exploit structure stands out as particularly well-suited
    for LLM augmentation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 探索-再利用算法作为理想候选。经典的双臂老虎机算法可以分为两类：探索-再利用方法和持续遗憾最小化方法[Zoghi等人，[2014a](https://arxiv.org/html/2407.01887v3#bib.bib7)]。其中，探索-再利用结构作为特别适合LLM增强的方式脱颖而出：
- en: •
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The Explore-Then-Exploit structure naturally aligns with the LLMs’ tendency
    to keep exploring without converging (see Figure [3](https://arxiv.org/html/2407.01887v3#S3.F3
    "Figure 3 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")), allowing
    for leveraging the LLMs’ exploration behavior while mitigating their exploration
    vulnerability and convergence instability (see Section [3.2](https://arxiv.org/html/2407.01887v3#S3.SS2
    "3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")).'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索-再利用结构自然与LLM倾向于持续探索而不收敛的特性相契合（见图[3](https://arxiv.org/html/2407.01887v3#S3.F3
    "图3 ‣ 3.2 实验结果 ‣ 3 LLM作为独立的上下文决策者 ‣ 超越数值奖励：LLM代理的上下文对战老虎机")），允许在利用LLM的探索行为的同时，减轻其探索脆弱性和收敛不稳定性（见第[3.2](https://arxiv.org/html/2407.01887v3#S3.SS2
    "3.2 实验结果 ‣ 3 LLM作为独立的上下文决策者 ‣ 超越数值奖励：LLM代理的上下文对战老虎机")节）。
- en: •
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Its symbolic representation of the algorithm’s logic enables clear integration
    of LLM suggestions at specific points without disrupting the overall structure
    and theoretical guarantees. In contrast, algorithms like Self-Sparring in [Sui
    et al., [2017](https://arxiv.org/html/2407.01887v3#bib.bib11)] are less symbolic,
    making them less suitable for direct LLM augmentation.
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它通过符号化表示算法的逻辑，使得在特定节点清晰地集成LLM的建议，而不干扰整体结构和理论保证。相比之下，像[Sui等人，[2017](https://arxiv.org/html/2407.01887v3#bib.bib11)]中的Self-Sparring等算法较少使用符号化，使得它们不太适合直接进行LLM增强。
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Its strong theoretical guarantees, e.g., IF2 with an expected regret bound
    of $O((K/\epsilon_{\mathrm{bad}})\log T)$ matching the DB problem’s lower bound
    of $\Omega((K/\epsilon_{\mathrm{bad}})\log T)$ up to constants (see Appendix[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), and its empirical performance (see Figures [2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") and[9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")) provide a robust foundation, ensuring
    convergence and bounded regret.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它有强大的理论保证，例如，IF2 具有期望遗憾界限 $O((K/\epsilon_{\mathrm{bad}})\log T)$，该界限与 DB 问题的下界
    $\Omega((K/\epsilon_{\mathrm{bad}})\log T)$ 相匹配，常数项除外（参见附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 用于决斗乐队的有用假设和引理 ‣ B.3 理论分析 ‣ 附录 B LEAD 算法设计与分析 ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队")），以及其经验表现（参见图[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "图 2 ‣ 3.2 实验结果 ‣ 3 作为独立上下文决策者的 LLMs ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队") 和 [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "图 9 ‣ C.2.1 与不同度量的比较 ‣ C.2 补充实验 ‣ 附录 C 提示设计与补充结果 ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队")）提供了坚实的基础，确保收敛性和有界的遗憾。
- en: B.2 Detailed Procedure Description
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 详细过程描述
- en: 'In Procedure [1](https://arxiv.org/html/2407.01887v3#algorithm1a "Procedure
    1 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    below, we describe the Match Arms procedure used in LEAD (see Algorithm[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and Figure[4](https://arxiv.org/html/2407.01887v3#S4.F4 "Figure 4 ‣ 4.1 Algorithmic
    Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric
    Awards: In-Context Dueling Bandits with LLM Agents")).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的过程[1](https://arxiv.org/html/2407.01887v3#algorithm1a "过程 1 ‣ B.2 详细过程描述
    ‣ 附录 B LEAD 算法设计与分析 ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队")中，我们描述了 LEAD 中使用的匹配臂过程（参见算法[1](https://arxiv.org/html/2407.01887v3#algorithm1
    "算法 1 ‣ 4.1 LEAD 的算法设计 ‣ 4 带算法增强的 LLM 用于决斗乐队 ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队") 和图[4](https://arxiv.org/html/2407.01887v3#S4.F4
    "图 4 ‣ 4.1 LEAD 的算法设计 ‣ 4 带算法增强的 LLM 用于决斗乐队 ‣ 超越数字奖励：带 LLM 代理的上下文决斗乐队"))。
- en: Algorithm 1 Match Arms (with a bounded number of comparisons)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 匹配臂（带有限次比较）
- en: 'Input: Two arms $a,a^{\prime}$, confidence parameter $\delta\leftarrow 1/(K^{2}\log
    T)$, and threshold $\epsilon\leftarrow\epsilon_{1,2}$'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：两个臂 $a,a^{\prime}$，置信度参数 $\delta\leftarrow 1/(K^{2}\log T)$，阈值 $\epsilon\leftarrow\epsilon_{1,2}$
- en: if *$a\neq a^{\prime}$ and $t\leq(16/\epsilon^{2})\log(K\log T)$* then
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *$a\neq a^{\prime}$ 且 $t\leq(16/\epsilon^{2})\log(K\log T)$*，则
- en: while *$\nexists\ (b,b^{\prime})\in B$ such that $\hat{P}_{b,b^{\prime}}>1/2$
    and $1/2\notin\hat{C}_{b,b^{\prime}}$* do             Compare $a$ with $a^{\prime}$
    and update $\hat{P}_{a,a^{\prime}}$ and $\hat{C}_{a,a^{\prime}}$, $t\leftarrow
    t+1$       end while      return  *$b$*else return  $a$
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *$\nexists\ (b,b^{\prime})\in B$，使得 $\hat{P}_{b,b^{\prime}}>1/2$ 且 $1/2\notin\hat{C}_{b,b^{\prime}}$*
    时，进行以下操作： 比较 $a$ 与 $a^{\prime}$，并更新 $\hat{P}_{a,a^{\prime}}$ 和 $\hat{C}_{a,a^{\prime}}$，$t\leftarrow
    t+1$。结束循环。返回 *$b$*，否则返回 $a$
- en: Algorithm 2 Validate
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 验证
- en: 'Input: Incumbent arm $a$, candidate arms $B$, $\mathsf{TrustLLM}$, confidence
    parameter $\delta\leftarrow 1/(TK^{2})$, and threshold $\epsilon\leftarrow\epsilon_{1,2}$'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：当前臂 $a$，候选臂 $B$，$\mathsf{TrustLLM}$，置信度参数 $\delta\leftarrow 1/(TK^{2})$，阈值
    $\epsilon\leftarrow\epsilon_{1,2}$
- en: if *$\mathsf{TrustLLM}$ is $\mathtt{True}$* then
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *$\mathsf{TrustLLM}$ 为 $\mathtt{True}$*，则
- en: for *$b\in B$* do              if *$t\leq(16/\epsilon^{2})\log(K\log T)$* then                    while *$\nexists\
    (b,b^{\prime})\in B$ such that $\hat{P}_{b,b^{\prime}}>1/2$ and $1/2\notin\hat{C}_{b,b^{\prime}}$* do                        
    Compare $a$ with $b$ and update $\hat{P}_{a,b}$ and $\hat{C}_{a,b}$, $t\leftarrow
    t+1$                   end while              if *$b\neq a$* then return  *$\mathsf{StillTrust}\leftarrow\mathtt{False},B\leftarrow
    B\backslash\{a\}$*       end for      return  *$\mathsf{StillTrust}\leftarrow\mathtt{True},B\leftarrow\emptyset$*if *$\mathsf{TrustLLM}$
    is $\mathtt{False}$* then return  *$\mathsf{StillTrust}\leftarrow\mathtt{False},B\leftarrow
    B$*
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *$b \in B$* 执行
- en: We reprise the IF2 procedure in[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]
    below to complement the presentation of LEAD.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[Yue 等人, [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中回顾了IF2过程，以补充LEAD的介绍。
- en: Algorithm 3 IF2 Procedure
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 IF2过程
- en: 'Input: Incumbent arm $a$, candidate arms $B$, confidence parameter $\delta\leftarrow
    1/(TK^{2})$, $t\leftarrow 0$'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：现任臂$a$，候选臂$B$，置信度参数$\delta \leftarrow 1/(TK^{2})$，$t \leftarrow 0$
- en: if *$t\leq(16K/\epsilon_{1,2}^{2})\log(K\log T)$* then
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *$t \leq (16K/\epsilon_{1,2}^{2})\log(K\log T)$* 则
- en: for *$b\in B$* do             Compare $a$ with $b$ and update $\hat{P}_{a,b}$
    and $\hat{C}_{a,b}$, $t\leftarrow t+1$       end for      $a,B\leftarrow\textsc{Anneal}(a,B)$return  *$a,B$*
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *$b \in B$* 执行
- en: Algorithm 4 Anneal
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 Anneal
- en: 'Input: Incumbent arm $a$, candidate arms $B$, confidence parameter $\delta\leftarrow
    1/(TK^{2})$, matrices $\hat{P}$ and $\hat{C}$'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：现任臂$a$，候选臂$B$，置信度参数$\delta \leftarrow 1/(TK^{2})$，矩阵$\hat{P}$和$\hat{C}$
- en: while *$\exists\ (b,b^{\prime})\in B$ such that $\hat{P}_{b,b^{\prime}}>1/2$
    and $1/2\notin\hat{C}_{b,b^{\prime}}$* do
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *$\exists\ (b,b^{\prime})\in B$ 使得 $\hat{P}_{b,b^{\prime}} > 1/2$ 且 $1/2 \notin
    \hat{C}_{b,b^{\prime}}$* 时，执行
- en: $B\leftarrow B\backslash\{b^{\prime}\}$end whileif *$\exists$ $b^{\prime}\in
    B$ such that $\hat{P}_{a,b^{\prime}}<1/2$ and $1/2\notin\hat{C}_{a,b^{\prime}}$* then        while *$\exists
    b\in B$ such that $\hat{P}_{a,b}>1/2$* do              $B\leftarrow B\backslash\{b\}$            
    /* IF2 pruning */       end while      $a\leftarrow b^{\prime}$, $B\leftarrow
    B\backslash\{b^{\prime}\}$return  *$a,B$*
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: $B \leftarrow B \backslash \{b^{\prime}\}$ 结束时，如果 *$\exists$ $b^{\prime} \in
    B$ 使得 $\hat{P}_{a,b^{\prime}} < 1/2$ 且 $1/2 \notin \hat{C}_{a,b^{\prime}}$* 则
- en: 'It is worth noting the following features of Algorithm [1](https://arxiv.org/html/2407.01887v3#algorithm1
    "Algorithm 1 ‣ 4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for
    Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    in its practical implementation.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，算法[1](https://arxiv.org/html/2407.01887v3#algorithm1 "Algorithm 1 ‣
    4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")在实际实现中的以下特性。'
- en: Remark 1.
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注 1.
- en: The LLM Phase allows for flexible exploration design within the bounded length
    of the Match Arms procedure, not limiting the number of prompts and comparisons
    performed by the LLM to identify an empirically best arm.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: LLM阶段允许在Match Arms过程的边界长度内灵活设计探索，不限制LLM进行的提示次数和比较次数，以识别经验上最优的臂。
- en: Remark 2.
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注 2.
- en: The bound length in the Match Arms procedure can be adjusted based on empirical
    requirements. Modifying the confidence parameter $\delta$ and the threshold $\epsilon$
    will affect the regret bound and the algorithm’s performance. These parameters
    can be tuned to balance exploration and exploitation, depending on the specific
    application and desired level of confidence.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Match Arms过程中的边界长度可以根据经验需求进行调整。修改置信度参数$\delta$和阈值$\epsilon$将影响后悔界限和算法的性能。这些参数可以进行调优，以平衡探索和利用，具体取决于特定应用和所需的置信水平。
- en: B.3 Theoretical Analysis
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 理论分析
- en: B.3.1 Useful Assumptions and Lemmas for Dueling Bandits
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.1 对决带宽问题的有用假设和引理
- en: We introduce the useful definitions, assumptions and lemmas for Dueling Bandits
    that are necessary for the theoretical analysis of our proposed algorithm.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们引入了与决斗臂问题相关的有用定义、假设和引理，这些是我们提出的算法理论分析所必需的。
- en: Throughout this paper, we consider two important performance metrics. The first
    is the strong regret of a given algorithm ALG, defined as
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们考虑了两个重要的性能度量。第一个是给定算法ALG的强悔值，定义为
- en: '|  | $\displaystyle\mathsf{SR}(\textsc{ALG})\coloneq\sum_{t=1}^{T}\Big{(}\epsilon%
    \left(b^{*},\mathsf{Arm}_{1}(t)\right)+\epsilon\left(b^{*},\mathsf{Arm}_{2}(t)%
    \right)\Big{)}.$ |  | (4) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathsf{SR}(\textsc{ALG})\coloneq\sum_{t=1}^{T}\Big{(}\epsilon\left(b^{*},\mathsf{Arm}_{1}(t)\right)+\epsilon\left(b^{*},\mathsf{Arm}_{2}(t)\right)\Big{)}.$
    |  | (4) |'
- en: where $T$ is the time horizon. The second is the weak regret of ALG, defined
    as
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$T$是时间跨度。第二个是ALG的弱悔值，定义为
- en: '|  | $\displaystyle\mathsf{WR}(\textsc{ALG})\coloneq\sum_{t=1}^{T}\min\Big{(}%
    \epsilon\left(b^{*},\mathsf{Arm}_{1}(t)\right),\epsilon\left(b^{*},\mathsf{Arm%
    }_{2}(t)\right)\Big{)}.$ |  | (5) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathsf{WR}(\textsc{ALG})\coloneq\sum_{t=1}^{T}\min\Big{(}\epsilon\left(b^{*},\mathsf{Arm}_{1}(t)\right),\epsilon\left(b^{*},\mathsf{Arm}_{2}(t)\right)\Big{)}.$
    |  | (5) |'
- en: 'which only compares $b^{*}$ against the better of the two selected arms $\mathsf{Arm}_{1}(t)$
    and $\mathsf{Arm}_{2}(t)$. It is worth highlighting that LLM agents exhibit significantly
    different behaviors with respect to the two defined notions of regret, as detailed
    in Section [3.2](https://arxiv.org/html/2407.01887v3#S3.SS2 "3.2 Experimental
    results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents").'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '它仅将$b^{*}$与所选择的两个臂$\mathsf{Arm}_{1}(t)$和$\mathsf{Arm}_{2}(t)$中的较优者进行比较。值得强调的是，LLM代理在这两种不同的悔值定义上表现出显著不同的行为，具体详见第[3.2](https://arxiv.org/html/2407.01887v3#S3.SS2
    "3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")节。'
- en: Assumption 1  (Total Ordering).
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设1（总排序）。
- en: The preference matrix $P=(\epsilon_{ij})$ satisfies the Total Ordering (TO)
    property such that for all $i,j\in[K]$, $i\succ j$ implies $\epsilon_{ij}>1/2$.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵$P=(\epsilon_{ij})$满足总排序（TO）性质，即对于所有$i,j\in[K]$，$i\succ j$意味着$\epsilon_{ij}>1/2$。
- en: With the TO property satisfied, we assume the preference matrix $P$ further
    satisfies the following two standard properties[Yue and Joachims, [2009](https://arxiv.org/html/2407.01887v3#bib.bib10),
    [2011](https://arxiv.org/html/2407.01887v3#bib.bib21), Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 满足TO性质后，我们假设偏好矩阵$P$还满足以下两个标准性质[Yue and Joachims, [2009](https://arxiv.org/html/2407.01887v3#bib.bib10),
    [2011](https://arxiv.org/html/2407.01887v3#bib.bib21), Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]。
- en: Assumption 2  (Strong Stochastic Transitivity).
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设2（强随机传递性）。
- en: The preference matrix $P=(\epsilon_{ij})$ satisfies the Strong Stochastic Transitivity
    (SST) such that for any arms $i,j,k\in[K]$ such that $i\succ j\succ k$ under the
    total order $\succ$, we have $\epsilon_{ik}>\max\{\epsilon_{ij},\epsilon_{jk}\}$.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵$P=(\epsilon_{ij})$满足强随机传递性（SST），即对于任意臂$i,j,k\in[K]$，如果在总排序$\succ$下有$i\succ
    j\succ k$，则我们有$\epsilon_{ik}>\max\{\epsilon_{ij},\epsilon_{jk}\}$。
- en: Assumption 3  (Stochastic Triangle Inequality).
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设3（随机三角不等式）。
- en: The preference matrix $P=(\epsilon_{ij})$ satisfies the Stochastic Triangle
    Inequality (STI) such that for any arms $i\succ j\succ k$, we have $\epsilon_{ik}\leq\epsilon_{ij}+\epsilon_{jk}$.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 偏好矩阵$P=(\epsilon_{ij})$满足随机三角不等式（STI），即对于任意臂 $i\succ j\succ k$，我们有$\epsilon_{ik}\leq\epsilon_{ij}+\epsilon_{jk}$。
- en: 'Note that the Bradley-Terry-Luce (BTL) model [Bradley and Terry, [1952](https://arxiv.org/html/2407.01887v3#bib.bib25)]
    used in our experiments[3.1](https://arxiv.org/html/2407.01887v3#S3.SS1 "3.1 Implementation
    Details of Experiments ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond
    Numeric Awards: In-Context Dueling Bandits with LLM Agents") satisfies Assumption[2](https://arxiv.org/html/2407.01887v3#Thmassumption2
    "Assumption 2 (Strong Stochastic Transitivity). ‣ B.3.1 Useful Assumptions and
    Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") and[3](https://arxiv.org/html/2407.01887v3#Thmassumption3 "Assumption
    3 (Stochastic Triangle Inequality). ‣ B.3.1 Useful Assumptions and Lemmas for
    Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").
    We restate the following theoretical guarantees for IF2 that is useful in the
    proof of Theorem[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem
    4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"). Let $\epsilon_{\mathrm{bad}}\coloneq\min_{b\neq b^{*}}\epsilon(b,b^{*})$.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们实验中使用的Bradley-Terry-Luce (BTL)模型[Bradley和Terry，[1952](https://arxiv.org/html/2407.01887v3#bib.bib25)]满足假设[2](https://arxiv.org/html/2407.01887v3#Thmassumption2
    "假设2（强随机传递性）。 ‣ B.3.1 对于对战盗贼的有用假设和引理 ‣ B.3 理论分析 ‣ 附录B LEAD算法设计与分析 ‣ 超越数值奖励：上下文中的对战盗贼与LLM代理")和[3](https://arxiv.org/html/2407.01887v3#Thmassumption3
    "假设3（随机三角不等式）。 ‣ B.3.1 对于对战盗贼的有用假设和引理 ‣ B.3 理论分析 ‣ 附录B LEAD算法设计与分析 ‣ 超越数值奖励：上下文中的对战盗贼与LLM代理")。我们在此重述IF2的以下理论保证，该保证在定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理4.2（期望后悔）。 ‣ 4.2 LEAD的理论保证 ‣ 4 LLM增强的对战盗贼算法 ‣ 超越数值奖励：上下文中的对战盗贼与LLM代理")的证明中是有用的。设$\epsilon_{\mathrm{bad}}\coloneq\min_{b\neq
    b^{*}}\epsilon(b,b^{*})$。
- en: Lemma 1  (Theorem 2 in [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]).
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理1（定理2，见[Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]））。
- en: Assuming the preference matrix $P$ satisfies the SST and STI, then IF2 has its
    expected regret (both weak and strong) bounded from above by
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 假设偏好矩阵$P$满足SST和STI，则IF2的期望后悔（包括弱后悔和强后悔）有上界。
- en: '|  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF2})]\leq O\left(\frac{K}{%
    \epsilon_{\mathrm{bad}}}\log T\right).$ |  | (6) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{IF2})]\leq O\left(\frac{K}{%
    \epsilon_{\mathrm{bad}}}\log T\right).$ |  | (6) |'
- en: The following expected regret bound achieved by IF2 is tight up to multiplicative
    constants, as indicated by the lower bound (Theorem 4) in[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]
    such that any algorithm Alg for DB satisfies $\mathbb{E}[\mathsf{SR}(\textsc{Alg})]=\Omega\left((K/\epsilon_{\mathrm{bad}})%
    \log T\right)$.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 以下由IF2实现的期望后悔界限在乘法常数上是紧的，正如[Yue等人，[2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中的下界（定理4）所示，任何算法Alg对于DB都满足$\mathbb{E}[\mathsf{SR}(\textsc{Alg})]=\Omega\left((K/\epsilon_{\mathrm{bad}})%
    \log T\right)$。
- en: B.3.2 Theoretical Guarantees of LEAD
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3.2 LEAD的理论保证
- en: 'Part I: Vulnerability of Standalone LLM Agents'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 第一部分：独立LLM代理的脆弱性
- en: Assumption 4  (Worst-Case Behavior).
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 假设4（最坏情况行为）。
- en: 'Under the original prompt (see Figure [7](https://arxiv.org/html/2407.01887v3#A3.F7
    "Figure 7 ‣ C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")), the worst-case behavior of an LLM agent in
    the dueling bandit setting is equivalent to a randomizer that selects action pairs
    uniformly at random.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始提示下（见图[7](https://arxiv.org/html/2407.01887v3#A3.F7 "图7 ‣ C.1.2 提示设计 ‣ C.1
    LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：上下文中的对战盗贼与LLM代理")），LLM代理在对战盗贼设置中的最坏行为相当于一个随机选择动作对的随机器。
- en: 'Vulnerability of Standalone LLM Agents. Inspired by the adversarial corruptions
    framework introduced in [Hajiesmaili et al., [2020](https://arxiv.org/html/2407.01887v3#bib.bib40)]
    for the classic MAB problem, we investigate the vulnerability of standalone LLM
    agents in the DB setting under adversarial prompts. We consider an attacker with
    a budget $\Phi(T)$ who employs the following strategy: whenever the LLM agent
    selects the optimal arm $b^{*}$ for comparison, the attacker manipulates the input
    prompt to the LLM to eliminate $b^{*}$ from the duel with probability $p$ (where
    $0<p\leq 1$ is a constant), subject to the constraint of performing at most $\Phi(T)$
    attacks over $T$ rounds. This adversarial strategy compels the LLM agent to select
    suboptimal arms, resulting in poor performance, as formalized in the following
    theorem with Assumption[4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "Assumption 4 (Worst-Case Behavior). ‣ B.3.2 Theoretical Guarantees of LEAD ‣
    B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents").'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 独立LLM代理的脆弱性。受[Hajiesmaili等人，2020](https://arxiv.org/html/2407.01887v3#bib.bib40)为经典MAB问题提出的对抗性破坏框架的启发，我们研究了在DB设置中，独立LLM代理在对抗性提示下的脆弱性。我们考虑一个具有预算$\Phi(T)$的攻击者，采用以下策略：每当LLM代理选择最优臂$b^{*}$进行比较时，攻击者会操纵输入提示，以$p$的概率将$b^{*}$从对决中消除（其中$0<p\leq
    1$是常数），并且受到约束，即在$T$轮中最多执行$\Phi(T)$次攻击。这种对抗性策略迫使LLM代理选择次优臂，导致性能较差，正如以下定理中假设[4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "假设4（最坏情况行为）。 ‣ B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录B LEAD算法设计与分析 ‣ 超越数值奖励：基于上下文的对决赌博带有LLM代理")所形式化的那样。
- en: 'Proof of Theorem [4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1 "Theorem
    4.1 (Vulnerability). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents").'
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理[4.1](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem1 "定理4.1（脆弱性）。 ‣ 4.2
    LEAD的理论保证 ‣ 4 基于算法增强的LLM用于对决赌博 ‣ 超越数值奖励：基于上下文的对决赌博带有LLM代理")的证明。
- en: 'Consider the following DB instance with $K\geq 3$ arms $\{b_{1},\ldots,b_{K}\}$
    and preference matrix $P$:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下具有$K\geq 3$个臂$\{b_{1},\ldots,b_{K}\}$和偏好矩阵$P$的DB实例：
- en: '|  | $P_{i,j}=\begin{cases}0.5+\epsilon,&\text{if }b_{i}=b^{*}\text{ and }b_{j}\neq
    b% ^{*},\\ 0.5-\epsilon,&\text{if }b_{i}\neq b^{*}\text{ and }b_{j}=b^{*},\\ 0.5,&\text{otherwise.}\end{cases}$
    |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{i,j}=\begin{cases}0.5+\epsilon,&\text{如果 }b_{i}=b^{*}\text{ 且 }b_{j}\neq
    b^{*},\\ 0.5-\epsilon,&\text{如果 }b_{i}\neq b^{*}\text{ 且 }b_{j}=b^{*},\\ 0.5,&\text{其他情况。}\end{cases}$
    |  |'
- en: where $0<\epsilon<0.5$ is a small constant. In this instance, arm $b^{*}$ is
    the unique Condorcet winner, and all other arms are suboptimal with a gap of $\epsilon$
    to $b^{*}$.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$0<\epsilon<0.5$是一个小常数。在这种情况下，臂$b^{*}$是唯一的康多塞特获胜者，所有其他臂都是次优的，且与$b^{*}$之间有一个$\epsilon$的差距。
- en: 'Now, consider an attacker strategy with budget $\Phi(T)$: Whenever the LLM
    agent selects a duel containing the optimal arm $b^{*}$, the attacker manipulates
    the input prompt to the LLM agent (as described in Eq. ([1](https://arxiv.org/html/2407.01887v3#S2.E1
    "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents"))) to eliminate $b^{*}$ from the comparison with probability $p$ (where
    $0<p\leq 1$ is a constant), subject to the constraint that the attacker can perform
    at most $\Phi(T)$ attacks over the $T$ rounds.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑一个具有预算$\Phi(T)$的攻击者策略：每当LLM代理选择一个包含最优臂$b^{*}$的对决时，攻击者会操纵输入提示（如公式([1](https://arxiv.org/html/2407.01887v3#S2.E1
    "在2预备知识 ‣ 超越数值奖励：基于上下文的对决赌博带有LLM代理"))所述）将$b^{*}$从比较中消除，概率为$p$（其中$0<p\leq 1$是常数），并且受到限制，即攻击者在$T$轮中最多只能执行$\Phi(T)$次攻击。
- en: Let $N(T)$ be the number of rounds in which the LLM agent selects a duel containing
    the optimal arm $b^{*}$ up to round $T$. Due to the attacker’s manipulation of
    the input prompt, in each of these $N(T)$ rounds, $b^{*}$ is eliminated from the
    comparison with probability $p$. However, because of the attacker’s budget constraint,
    the actual number of attacked rounds is at most $\min\{N(T),\Phi(T)\}$.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设$N(T)$为LLM代理在$T$轮中选择包含最优臂$b^{*}$的对决的轮次数。由于攻击者对输入提示的操控，在这$N(T)$轮中的每一轮，$b^{*}$都有$p$的概率被从比较中消除。然而，由于攻击者的预算限制，实际被攻击的轮次最多为$\min\{N(T),\Phi(T)\}$。
- en: In the rounds where $b^{*}$ is eliminated from the comparison, the LLM agent
    can only select from the suboptimal arms $\{b_{i}\mid b_{i}\neq b^{*},i\in[K]\}$.
    Let $\Delta_{i}=P_{b^{*},b_{i}}-0.5$ denote the suboptimality gap of arm $b_{i}$
    with respect to $b^{*}$. Then, the expected regret incurred in each round where
    $b^{*}$ is eliminated from the comparison is at least $\min_{b_{i}\neq b^{*}}\Delta_{i}=\epsilon$.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在$b^{*}$从比较中被淘汰的回合中，LLM代理只能从次优臂$\{b_{i}\mid b_{i}\neq b^{*},i\in[K]\}$中选择。设$\Delta_{i}=P_{b^{*},b_{i}}-0.5$表示臂$b_{i}$相对于$b^{*}$的次优间隙。那么，在每轮中$b^{*}$从比较中被淘汰时，产生的期望遗憾至少为$\min_{b_{i}\neq
    b^{*}}\Delta_{i}=\epsilon$。
- en: 'Thus, the expected cumulative regret of the LLM agent after $T$ rounds is at
    least:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，LLM代理在$T$轮后的期望累计遗憾至少为：
- en: '|  | $\mathbb{E}[\text{Regret}(T)]\geq p\cdot\mathbb{E}[\min\{N(T),\Phi(T)\}]\cdot%
    \epsilon\geq p\cdot\min\{\mathbb{E}[N(T)],\Phi(T)\}\cdot\epsilon,$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[\text{Regret}(T)]\geq p\cdot\mathbb{E}[\min\{N(T),\Phi(T)\}]\cdot%
    \epsilon\geq p\cdot\min\{\mathbb{E}[N(T)],\Phi(T)\}\cdot\epsilon,$ |  |'
- en: where the first inequality follows from the regret incurred in rounds where
    $b^{*}$ is eliminated from the duel, and the second inequality holds due to Jensen’s
    inequality and the linearity of expectation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，第一个不等式来源于在$b^{*}$从对决中被淘汰的回合中的遗憾，第二个不等式由于詹森不等式和期望的线性性质而成立。
- en: 'According to the Assumption [4](https://arxiv.org/html/2407.01887v3#Thmassumption4
    "Assumption 4 (Worst-Case Behavior). ‣ B.3.2 Theoretical Guarantees of LEAD ‣
    B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD ‣
    Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"), in the worst
    case, the LLM agent’s behavior is equivalent to randomly selecting a duel in each
    round. For $K$ arms, there are ${K(K-1)}/{2}$ possible duel combinations. Therefore,
    the probability of selecting a duel containing $b^{*}$ in each round is $(K-1)/{\binom{K}{2}}=\frac{2}{K}$,
    which yields $\mathbb{E}[N(T)]=T\cdot\frac{2}{K}$. The regret bound becomes:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '根据假设[4](https://arxiv.org/html/2407.01887v3#Thmassumption4 "Assumption 4 (Worst-Case
    Behavior). ‣ B.3.2 Theoretical Guarantees of LEAD ‣ B.3 Theoretical Analysis ‣
    Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")，在最坏情况下，LLM代理的行为等同于每轮随机选择一个对决。对于$K$个臂，存在${K(K-1)}/{2}$种可能的对决组合。因此，在每轮中选择一个包含$b^{*}$的对决的概率是$(K-1)/{\binom{K}{2}}=\frac{2}{K}$，这导致$\mathbb{E}[N(T)]=T\cdot\frac{2}{K}$。遗憾值的下界变为：'
- en: '|  | $\mathbb{E}[\text{Regret}(T)]\geq p\cdot\min\left\{\frac{2T}{K},\Phi(T)\right\}%
    \cdot\epsilon=\Omega\left(\min\left\{\frac{T}{K},\Phi(T)\right\}\right).$ |  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[\text{Regret}(T)]\geq p\cdot\min\left\{\frac{2T}{K},\Phi(T)\right\}%
    \cdot\epsilon=\Omega\left(\min\left\{\frac{T}{K},\Phi(T)\right\}\right).$ |  |'
- en: 'Therefore, any standalone LLM agent whose policy is represented by Eq. ([1](https://arxiv.org/html/2407.01887v3#S2.E1
    "In 2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")) under the worst-case assumption will suffer an expected regret of $\Omega\left(\min\left\{\Phi(T),\frac{T}{K}\right\}\right)$.
    This lower bound demonstrates the vulnerability of solely relying on LLM agents
    for DB in adversarial environments when the attacker can manipulate the input
    prompts. ∎'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，在最坏情况下，任何单独的LLM代理，其策略由公式([1](https://arxiv.org/html/2407.01887v3#S2.E1 "In
    2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"))表示，将承受一个期望的遗憾值$\Omega\left(\min\left\{\Phi(T),\frac{T}{K}\right\}\right)$。这个下界展示了仅依赖LLM代理在对抗性环境中进行对决带来的脆弱性，尤其是在攻击者能够操控输入提示的情况下。∎'
- en: 'Part II: Expected Regret Bounds of LEAD (IF2 base)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 第二部分：LEAD的期望遗憾界限（IF2基础）
- en: 'Suppose at each step $t\leq T$, aligning with the design of IF2 in[Yue et al.,
    [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)], $\hat{P}_{t}$ is estimated
    such that each $\smash{\hat{P}_{i,j}}$ is the fraction of number of comparisons
    when $b_{i}$ was the winner out of all previous $t$ comparisons. Define a confidence
    interval $\smash{\hat{C}_{t}\coloneq(\hat{P}_{t}-c_{t},\hat{P}_{t}+c_{t})}$ where
    $\smash{c_{t}\coloneq\sqrt{\log(1/\delta)/t}}$. Before proceeding to prove Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents"), we first state a useful lemma from [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]
    as a result of the Hoeffding’s inequality[Hoeffding, [1994](https://arxiv.org/html/2407.01887v3#bib.bib41)].'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在每一步 $t\leq T$，根据IF2的设计，在[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中，估计$\hat{P}_{t}$使得每个$\smash{\hat{P}_{i,j}}$是当$b_{i}$为获胜者时，在之前$t$次比较中的占比。定义置信区间$\smash{\hat{C}_{t}\coloneq(\hat{P}_{t}-c_{t},\hat{P}_{t}+c_{t})}$，其中$\smash{c_{t}\coloneq\sqrt{\log(1/\delta)/t}}$。在证明定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2 (期望遗憾)。 ‣ 4.2 LEAD的理论保证 ‣ 4 基于算法增强的LLM在决斗型强盗问题中的应用 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")之前，我们首先基于Hoeffding不等式[Hoeffding,
    [1994](https://arxiv.org/html/2407.01887v3#bib.bib41)]，声明[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中的一个有用的引理。
- en: Lemma 2  (Generalized Lemma 1 in [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]).
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 引理 2  （在[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中广义的引理
    1）。
- en: Let $\delta=1/(K\log T)^{2}$ be a confidence parameter with $\delta\in(0,1/2]$,
    a winner between two arms $b_{i}$ and $b_{j}$ is identified with probability at
    least $1-\delta$, using at most $\left(16/\epsilon_{i,j}^{2}\right)\log(K\log
    T)$ number of comparisons.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\delta=1/(K\log T)^{2}$为置信参数，其中$\delta\in(0,1/2]$，两个臂$b_{i}$和$b_{j}$之间的胜者以至少$1-\delta$的概率被识别，最多使用$\left(16/\epsilon_{i,j}^{2}\right)\log(K\log
    T)$次比较。
- en: 'Note that Lemma [2](https://arxiv.org/html/2407.01887v3#Thmlemma2 "Lemma 2
    (Generalized Lemma 1 in [Yue et al., 2012]). ‣ B.3.2 Theoretical Guarantees of
    LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of
    LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") can
    be directly implied by Lemma 1 in[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)].
    Now, under Assumption [2](https://arxiv.org/html/2407.01887v3#Thmassumption2 "Assumption
    2 (Strong Stochastic Transitivity). ‣ B.3.1 Useful Assumptions and Lemmas for
    Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")
    and[3](https://arxiv.org/html/2407.01887v3#Thmassumption3 "Assumption 3 (Stochastic
    Triangle Inequality). ‣ B.3.1 Useful Assumptions and Lemmas for Dueling Bandits
    ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis of LEAD
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents") such that
    the preference matrix $P$ satisfies the SST and STI properties, we prove Theorem[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents").'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，引理[2](https://arxiv.org/html/2407.01887v3#Thmlemma2 "引理 2 (在[Yue et al.,
    2012]中广义的引理 1)。 ‣ B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")可以直接通过[Yue
    et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]中的引理 1推导出来。现在，在假设[2](https://arxiv.org/html/2407.01887v3#Thmassumption2
    "假设 2 (强随机传递性)。 ‣ B.3.1 决斗型强盗问题的有用假设与引理 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")和[3](https://arxiv.org/html/2407.01887v3#Thmassumption3
    "假设 3 (随机三角不等式)。 ‣ B.3.1 决斗型强盗问题的有用假设与引理 ‣ B.3 理论分析 ‣ 附录B LEAD的算法设计与分析 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")下，偏好矩阵$P$满足SST和STI性质，我们证明定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理 4.2 (期望遗憾)。 ‣ 4.2 LEAD的理论保证 ‣ 4 基于算法增强的LLM在决斗型强盗问题中的应用 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")。
- en: 'Proof of Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "Theorem
    4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents").'
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2 "定理 4.2 (期望遗憾)。 ‣
    4.2 LEAD的理论保证 ‣ 4 基于算法增强的LLM在决斗型强盗问题中的应用 ‣ 超越数值奖励：在上下文中使用LLM代理进行决斗型强盗问题")的证明。
- en: 'Suppose the arms suggested by LLM agent includes the best arm $b^{*}$ after
    exploring $T_{\mathrm{LLM}}$ steps. We prove the two bounds shown in Theorem [4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "Theorem 4.2 (Expected Regret). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced
    LLMs for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") one-by-one.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 假设LLM代理建议的臂在探索$T_{\mathrm{LLM}}$步后包含最佳臂$b^{*}$。我们逐一证明定理[4.2](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem2
    "定理4.2（期望遗憾）。 ‣ 4.2 LEAD的理论保证 ‣ 4 使用算法增强的LLM进行决斗赌博机 ‣ 超越数值奖励：具有LLM代理的上下文决斗赌博机")中显示的两个界限。
- en: 'Weak Regret Bound. The first $T_{\mathrm{LLM}}$ steps induce accumulated weak
    regret of at most $O(T_{\mathrm{LLM}})$. According to [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)],
    IF2 plays $O(K)$ matches (comparisons) in expectation. Thus, the expected number
    of rounds of calling IF2 Procedure is $O(\log T/\log(K\log T))$. Applying Lemma [2](https://arxiv.org/html/2407.01887v3#Thmlemma2
    "Lemma 2 (Generalized Lemma 1 in [Yue et al., 2012]). ‣ B.3.2 Theoretical Guarantees
    of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),
    with $\smash{O\left((1/\epsilon_{1,2}^{2})\log(K\log T)\right)}$ (by setting a
    hyper-parameter $\epsilon=\epsilon_{1,2}$) comparisons between two arms, since
    the best arm $b^{*}$ is always included in each comparison, the best arm $b^{*}$
    is correctly identified with probability at least $1-1/(K\log T)^{2}$. This procedure
    leads to no weak regret since $b^{*}$ suggested by the LLM agent is always included
    as the incumbent arm in future comparisons.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 弱遗憾界限。前$T_{\mathrm{LLM}}$步会导致最多为$O(T_{\mathrm{LLM}})$的累积弱遗憾。根据[Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]，IF2期望进行$O(K)$场比赛（比较）。因此，调用IF2程序的预期轮次为$O(\log
    T/\log(K\log T))$。应用引理[2](https://arxiv.org/html/2407.01887v3#Thmlemma2 "引理2（[Yue
    et al., 2012]中的广义引理1）。 ‣ B.3.2 LEAD的理论保证 ‣ B.3 LEAD的理论分析 ‣ 附录B LEAD的算法设计与分析 ‣
    超越数值奖励：具有LLM代理的上下文决斗赌博机")，通过设置超参数$\epsilon=\epsilon_{1,2}$，在两臂之间进行$\smash{O\left((1/\epsilon_{1,2}^{2})\log(K\log
    T)\right)}$次比较，由于最佳臂$b^{*}$始终包含在每次比较中，因此最佳臂$b^{*}$会以至少$1-1/(K\log T)^{2}$的概率被正确识别。此过程不会导致弱遗憾，因为LLM代理建议的$b^{*}$始终作为现任臂包含在未来的比较中。
- en: 'Moreover, the implementation of Procedure [3](https://arxiv.org/html/2407.01887v3#algorithm3
    "Procedure 3 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") induces at most $O((K/\epsilon_{1,2}^{2})\log(K\log T))$ comparisons.
    The validation procedure (Procedure [2](https://arxiv.org/html/2407.01887v3#algorithm2
    "Procedure 2 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")) leads to no weak regret if $b_{\mathrm{LLM}}$ is indeed the best
    arm and the identification of $b_{\mathrm{LLM}}=b^{*}$ succeeds with a probability
    $1-1/T$. Denote by $\mathcal{E}_{1}$ and $\mathcal{E}_{2}$ two error events when
    $b^{*}$ loses some of the matches in the LLM Phase. there exist comparisons (matches)
    fail in the validation procedure (Procedure [2](https://arxiv.org/html/2407.01887v3#algorithm2
    "Procedure 2 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")) or the IF2 Phase (Procedure [3](https://arxiv.org/html/2407.01887v3#algorithm3
    "Procedure 3 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")). The union bound implies with probability $1-1/(K\log T)$, $b^{*}$
    will win all the matches such that $P(\mathcal{E}_{1})\leq 1/(K\log T)$. Similarly,
    $P(\mathcal{E}_{2})\leq 1/T$. Combining these events, regarding the total expected
    weak regret, the expected weak regret induced by the steps after time $T_{\mathrm{LLM}}$
    can be bounded by'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，程序[3](https://arxiv.org/html/2407.01887v3#algorithm3 "Procedure 3 ‣ B.2
    Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis of LEAD
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")的实现最多会引发
    $O((K/\epsilon_{1,2}^{2})\log(K\log T))$ 次比较。验证过程（程序[2](https://arxiv.org/html/2407.01887v3#algorithm2
    "Procedure 2 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")）当 $b_{\mathrm{LLM}}$ 确实是最优臂时，不会产生弱后悔，且 $b_{\mathrm{LLM}}=b^{*}$ 的识别成功的概率为
    $1-1/T$。定义 $\mathcal{E}_{1}$ 和 $\mathcal{E}_{2}$ 为两个错误事件，当 $b^{*}$ 在 LLM 阶段失去一些匹配时。存在比较（匹配）未通过验证过程（程序[2](https://arxiv.org/html/2407.01887v3#algorithm2
    "Procedure 2 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents")）或 IF2 阶段（程序[3](https://arxiv.org/html/2407.01887v3#algorithm3 "Procedure
    3 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")）失败。联合界限意味着以
    $1-1/(K\log T)$ 的概率，$b^{*}$ 将赢得所有匹配，且 $P(\mathcal{E}_{1})\leq 1/(K\log T)$。类似地，$P(\mathcal{E}_{2})\leq
    1/T$。结合这些事件，关于总的预期弱后悔，由时间 $T_{\mathrm{LLM}}$ 后的步骤引起的预期弱后悔可以界定为'
- en: '|  |  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{LEAD}\text{ after }T_{\mathrm{LLM}%
    })]$ |  |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mathbb{E}[\mathsf{SR}(\textsc{LEAD}\text{ 在 }T_{\mathrm{LLM}%
    }\text{ 后})]$ |  |'
- en: '|  | $\displaystyle\leq$ | $\displaystyle\left(1-\frac{1}{K\log T}-\frac{1}{T}\right)O\underbrace{\left(%
    \frac{K\log(K\log T)}{\epsilon_{1,2}}\right)}_{\text{\color[rgb]{% 0,0.49609375,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.49609375,1}{\text{%
    LLM Phase}}}}+\frac{1}{K\log T}O\underbrace{\left(\frac{K}{\epsilon_{1,2}}{% \log
    T}\right)}_{\text{\color[rgb]{0.51953125,0.51953125,0.51171875}% \definecolor[named]{pgfstrokecolor}{rgb}{0.51953125,0.51953125,0.51171875}{%
    \text{{IF2} Phase}}}}+\frac{1}{T}\underbrace{O(T)}_{\text{Failure Cases}}$ |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\leq$ | $\displaystyle\left(1-\frac{1}{K\log T}-\frac{1}{T}\right)O\underbrace{\left(%
    \frac{K\log(K\log T)}{\epsilon_{1,2}}\right)}_{\text{\color[rgb]{% 0,0.49609375,1}\definecolor[named]{pgfstrokecolor}{rgb}{0,0.49609375,1}{\text{%
    LLM 阶段}}}}+\frac{1}{K\log T}O\underbrace{\left(\frac{K}{\epsilon_{1,2}}{% \log
    T}\right)}_{\text{\color[rgb]{0.51953125,0.51953125,0.51171875}% \definecolor[named]{pgfstrokecolor}{rgb}{0.51953125,0.51953125,0.51171875}{%
    \text{{IF2} 阶段}}}}+\frac{1}{T}\underbrace{O(T)}_{\text{失败情况}}$ |  |'
- en: '|  | $\displaystyle=\$ | $\displaystyle\widetilde{O}\left(\frac{K\log K}{\epsilon_{1,2}}\right)$
    |  |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=\$ | $\displaystyle\widetilde{O}\left(\frac{K\log K}{\epsilon_{1,2}}\right)$
    |  |'
- en: since there are at most $K+1$ matches.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 因为最多有 $K+1$ 次匹配。
- en: 'Convergence Guarantee. Furthermore, consider the adversarial selection of arms
    from the LLM agent. According to Lemma [2](https://arxiv.org/html/2407.01887v3#Thmlemma2
    "Lemma 2 (Generalized Lemma 1 in [Yue et al., 2012]). ‣ B.3.2 Theoretical Guarantees
    of LEAD ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and Analysis
    of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"),
    the IF2 procedure with an expected regret $\smash{O\left((K/\epsilon_{1,2})\log(T)\right)}$
    is implemented at most $O(1)$ times with probability $1-1/(TK)$, provided with
    $|B|=K$. Therefore, the expected regret (either strong or weak) induced by each
    implementation of Procedure [3](https://arxiv.org/html/2407.01887v3#algorithm3
    "Procedure 3 ‣ B.2 Detailed Procedure Description ‣ Appendix B Algorithm Design
    and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with
    LLM Agents") is at most $O\left((K/\epsilon_{1,2})\log(T)\right)$ since there
    are at most $O\left((K/\epsilon_{1,2}^{2})\log(K\log T)\right)$ additional comparisons
    of pairs in the LLM phase. Finally, applying the expected regret bound in Lemma [1](https://arxiv.org/html/2407.01887v3#Thmlemma1
    "Lemma 1 (Theorem 2 in [Yue et al., 2012]). ‣ B.3.1 Useful Assumptions and Lemmas
    for Dueling Bandits ‣ B.3 Theoretical Analysis ‣ Appendix B Algorithm Design and
    Analysis of LEAD ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents") completes the proof.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛保证。进一步考虑从 LLM 代理进行的对抗性臂选择。根据引理 [2](https://arxiv.org/html/2407.01887v3#Thmlemma2
    "引理 2（[Yue 等人, 2012]中的广义引理 1）。 ‣ B.3.2 LEAD的理论保证 ‣ B.3 理论分析 ‣ 附录 B LEAD的算法设计与分析
    ‣ 超越数值奖励：基于上下文的对抗赌博机与LLM代理")，IF2程序的期望遗憾为$\smash{O\left((K/\epsilon_{1,2})\log(T)\right)}$，在概率
    $1-1/(TK)$ 下最多执行 $O(1)$ 次，前提是 $|B|=K$。因此，每次实施程序 [3](https://arxiv.org/html/2407.01887v3#algorithm3
    "程序 3 ‣ B.2 详细程序描述 ‣ 附录 B LEAD的算法设计与分析 ‣ 超越数值奖励：基于上下文的对抗赌博机与LLM代理") 时引起的期望遗憾（无论是强遗憾还是弱遗憾）最多为
    $O\left((K/\epsilon_{1,2})\log(T)\right)$，因为在 LLM 阶段最多有 $O\left((K/\epsilon_{1,2}^{2})\log(K\log
    T)\right)$ 次额外的对比操作。最后，应用引理 [1](https://arxiv.org/html/2407.01887v3#Thmlemma1
    "引理 1（[Yue 等人, 2012]中的定理 2）。 ‣ B.3.1 对抗赌博机的有用假设和引理 ‣ B.3 理论分析 ‣ 附录 B LEAD的算法设计与分析
    ‣ 超越数值奖励：基于上下文的对抗赌博机与LLM代理") 中的期望遗憾界限即可完成证明。
- en: ∎
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: 'Part III: Converse'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分：反向命题
- en: In the following, we argue that for any algorithm ALG, achieving an upper bound
    $\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq T_{\mathrm{LLM}}$ for all
    $T_{\mathrm{LLM}}$ is impossible.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下内容中，我们将论证对于任何算法 ALG，达到上界$\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq
    T_{\mathrm{LLM}}$ 对于所有 $T_{\mathrm{LLM}}$ 是不可能的。
- en: 'Proof of Theorem [4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3 "Theorem
    4.3 (Converse). ‣ 4.2 Theoretical Guarantees for LEAD ‣ 4 Algorithm-Enhanced LLMs
    for Dueling Bandits ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents").'
  id: totrans-280
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理证明 [4.3](https://arxiv.org/html/2407.01887v3#S4.Thmtheorem3 "定理 4.3（反向命题）。
    ‣ 4.2 LEAD的理论保证 ‣ 4 基于算法的LLM对抗性赌博机 ‣ 超越数值奖励：基于上下文的对抗赌博机与LLM代理")。
- en: Suppose ALG is an algorithm that leads to a weak regret bound $\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq
    T_{\mathrm{LLM}}$ for all $T_{\mathrm{LLM}}$, then it has to trust and include
    the recommended arm in all the comparisons immediately after it is proposed by
    the LLM agent to ensure that future weak regret becomes zero. To see this, note
    that one can always construct an adversarial $T_{\mathrm{LLM}}$ that leads to
    a nonzero future weak regret. However, the LLM agent can choose to provide an
    arm that is always not the best arm for all $t\in\{1,\ldots,T\}$. This leads to
    $\mathbb{E}\left[\mathsf{SR}(\textsc{ALG})\right]\geq\mathbb{E}\left[\mathsf{WR%
    }(\textsc{ALG})\right]\geq\Omega\left(T\right)$.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ALG 是一个算法，能为所有 $T_{\mathrm{LLM}}$ 得到一个弱遗憾上界 $\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\leq
    T_{\mathrm{LLM}}$，那么它必须信任并在 LLM 代理提出后立即在所有比较中包括推荐的臂，以确保未来的弱遗憾为零。为了说明这一点，请注意，始终可以构造一个对抗性的
    $T_{\mathrm{LLM}}$，导致未来弱遗憾非零。然而，LLM 代理可以选择提供一个在所有 $t\in\{1,\ldots,T\}$ 中始终不是最佳臂的臂。这会导致
    $\mathbb{E}\left[\mathsf{SR}(\textsc{ALG})\right]\geq\mathbb{E}\left[\mathsf{WR}(\textsc{ALG})\right]\geq\Omega\left(T\right)$。
- en: ∎
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: Appendix C Prompt Design and Supplementary Results
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 提示设计与补充结果
- en: C.1 LLM Experimental Results
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 LLM 实验结果
- en: In this section, we provide the detailed design of the prompts used in our experiments
    and provide additional results to support our findings. We begin by presenting
    the original prompt used in the LLM-Env interaction and introduce the perturbed
    prompts, which include both noisy and adversarial variations to test the robustness
    of our approach. Finally, we provide four exemplars using the original prompt
    to to showcase the behavior of both GPT-4 Turbo and o1-preview.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了实验中使用的提示设计的详细信息，并提供了额外的结果以支持我们的发现。我们首先展示了在LLM-Env交互中使用的原始提示，并引入了扰动后的提示，这些提示包括噪声和对抗性变体，用以测试我们方法的鲁棒性。最后，我们提供了四个使用原始提示的示例，展示了GPT-4
    Turbo和o1-preview的行为。
- en: C.1.1 Environments
  id: totrans-286
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.1 环境
- en: 'Transitive Case: $\text{SST}\cap\text{STI}$'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 传递性案例：$\text{SST}\cap\text{STI}$
- en: 'In transitive instances, the preference matrices are constructed using the
    Bradley-Terry-Luce (BTL) model [Bradley and Terry, [1952](https://arxiv.org/html/2407.01887v3#bib.bib25),
    Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)], with a generalized
    form known as the Plackett-Luce model [Plackett, [1975](https://arxiv.org/html/2407.01887v3#bib.bib42)].
    In this model, each arm is associated with a utility parameter $\theta(i)>0$,
    where $i$ represents the rank of the arm (i.e., $\theta(1)$ corresponds to the
    best arm, $\theta(2)$ corresponds to the second best arm, and so on). For any
    pair of arms $b_{i}$ and $b_{j}$, the probability of $b_{i}$ being preferred over
    $b_{j}$ is determined by $P{(i\succ j)}=\theta(i)/(\theta(i)+\theta(j))$. Setting
    the number of arms $K=5$, we randomize the order of the arms to prevent selection
    bias, resulting in the following arm ordering: $b_{5}\succ b_{3}\succ b_{2}\succ
    b_{1}\succ b_{4}$. We use two instances: Transitive-Easy and Transitive-Hard,
    with their respective $\theta$ parameters given by:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在传递性实例中，偏好矩阵使用Bradley-Terry-Luce（BTL）模型 [Bradley and Terry, [1952](https://arxiv.org/html/2407.01887v3#bib.bib25),
    Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)] 构建，带有一种广义形式，称为Plackett-Luce模型
    [Plackett, [1975](https://arxiv.org/html/2407.01887v3#bib.bib42)]。在该模型中，每个臂都与一个效用参数
    $\theta(i)>0$ 相关，其中 $i$ 表示臂的排名（即，$\theta(1)$对应最佳臂，$\theta(2)$对应第二好臂，依此类推）。对于任意一对臂
    $b_{i}$ 和 $b_{j}$，$b_{i}$ 相对于 $b_{j}$ 的偏好概率由 $P{(i\succ j)}=\theta(i)/(\theta(i)+\theta(j))$
    决定。设置臂的数量 $K=5$，我们随机化臂的顺序以避免选择偏差，结果得到以下臂的排序：$b_{5}\succ b_{3}\succ b_{2}\succ
    b_{1}\succ b_{4}$。我们使用两个实例：传递性-简单和传递性-困难，并给出各自的 $\theta$ 参数：
- en: •
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transitive-Easy instance: $\theta(1)=1,\ \theta(i)=0.5-(i-1)/2K$,  $\forall
    i\in[2,K]$.'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传递性-简单示例：$\theta(1)=1,\ \theta(i)=0.5-(i-1)/2K$,  $\forall i\in[2,K]$。
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transitive-Hard instance: $\theta(i)=1-(i-1)/K$,  $\forall i\in[K]$.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传递性-困难示例：$\theta(i)=1-(i-1)/K$,  $\forall i\in[K]$。
- en: 'Note that the datasets generated in this way satisfy the Strong Stochastic
    Transitivity (SST) and Stochastic Triangle Inequality (STI) properties [Yue et al.,
    [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)] (see Appendix[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") for more details). The settings of the used
    BTL model also imply the existence of a Condorcet winner.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，以这种方式生成的数据集满足强随机传递性（SST）和随机三角不等式（STI）性质 [Yue et al., [2012](https://arxiv.org/html/2407.01887v3#bib.bib6)]（更多细节见附录[B.3.1](https://arxiv.org/html/2407.01887v3#A2.SS3.SSS1
    "B.3.1 Useful Assumptions and Lemmas for Dueling Bandits ‣ B.3 Theoretical Analysis
    ‣ Appendix B Algorithm Design and Analysis of LEAD ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")）。所使用的BTL模型的设置也暗示了康多塞获胜者的存在。'
- en: 'Intransitive Case: $CW\setminus(\text{SST}\cup\text{STI})$'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 非传递性案例：$CW\setminus(\text{SST}\cup\text{STI})$
- en: 'In intransitive instances, the preference matrices are constructed to violate
    both the Strong Stochastic Transitivity (SST) and Stochastic Triangle Inequality
    (STI) properties. This design creates cyclic preferences among the non-winning
    arms while preserving the existence of a Condorcet winner. Setting $K=5$, we still
    use the same shuffled arm ordering: $b_{5}\succ b_{3}\succ b_{2}\succ b_{1}\succ
    b_{4}$ for intransitive instances.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在非传递性实例中，偏好矩阵被构造为违反强随机传递性（SST）和随机三角不等式（STI）性质。这种设计在非获胜臂之间创建了循环偏好，同时保持了康多塞获胜者的存在。设置$K=5$时，我们仍然使用相同的随机臂顺序：$b_{5}\succ
    b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$用于非传递性实例。
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intransitive-Easy instance: The Condorcet winner $b_{5}$ has a strong preference
    over any other arm:'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非传递性-简单示例：康多塞获胜者 $b_{5}$ 对任何其他臂都有强烈的偏好：
- en: '|  | $P(5\succ j)=0.8,\quad P(j\succ 5)=0.2,\quad\forall j\in\{1,\dots,4\}.$
    |  |'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $P(5\succ j)=0.8,\quad P(j\succ 5)=0.2,\quad\forall j\in\{1,\dots,4\}.$
    |  |'
- en: 'Among the non-winning arms $b_{1},\dots,b_{4}$, cyclic preferences are introduced
    via:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在非获胜臂$b_{1},\dots,b_{4}$之间，引入了通过以下方式的循环偏好：
- en: '|  | $P(i\succ j)=0.8-0.2\cdot\left((j-i-1)\bmod(K-1)\right),\quad\forall i,j\in\{1,%
    \dots,4\},\ i\neq j.$ |  |'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $P(i\succ j)=0.8-0.2\cdot\left((j-i-1)\bmod(K-1)\right),\quad\forall i,j\in\{1,\dots,4\},\
    i\neq j.$ |  |'
- en: This configuration ensures a clear dominance by $b_{5}$.
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该配置确保了$b_{5}$的明显优势。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Intransitive-Hard instance: The Condorcet winner’s preference is weaker, with:'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不传递性困难实例：康多塞获胜者的偏好较弱，具体为：
- en: '|  | $P(5\succ j)=0.6,\quad P(j\succ 5)=0.4,\quad\forall j\in\{1,\dots,4\}.$
    |  |'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $P(5\succ j)=0.6,\quad P(j\succ 5)=0.4,\quad\forall j\in\{1,\dots,4\}.$
    |  |'
- en: This setting makes it more challenging to identify $b_{5}$ as the Condorcet
    winner.
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个设置使得识别$b_{5}$作为康多塞获胜者变得更加困难。
- en: 'Finally, in both instances, the symmetry condition is imposed for consistency:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在这两个实例中，为了保持一致性，施加了对称性条件：
- en: '|  | $P(j\succ i)=1-P(i\succ j),\quad\forall i,j\in\{1,\dots,K\},\ i\neq j.$
    |  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '|  | $P(j\succ i)=1-P(i\succ j),\quad\forall i,j\in\{1,\dots,K\},\ i\neq j.$
    |  |'
- en: Accordingly, as shown below, we create a cyclic pattern of preferences among
    the non-winning arms while maintaining the Condorcet winner’s superiority.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如下面所示，我们在非获胜臂之间创建了一个循环的偏好模式，同时保持了康多塞获胜者的优势。
- en: '*Intransitive-Easy Instance ($p_{w}=0.8$)*'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*不传递性简单实例（$p_{w}=0.8$）*'
- en: '|  | $P=\begin{bmatrix}0.0&0.8&0.6&0.4&0.2\\ 0.2&0.0&0.8&0.6&0.2\\ 0.4&0.2&0.0&0.8&0.2\\
    0.6&0.4&0.2&0.0&0.2\\ 0.8&0.8&0.8&0.8&0.0\\ \end{bmatrix}$ |  |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=\begin{bmatrix}0.0&0.8&0.6&0.4&0.2\\ 0.2&0.0&0.8&0.6&0.2\\ 0.4&0.2&0.0&0.8&0.2\\
    0.6&0.4&0.2&0.0&0.2\\ 0.8&0.8&0.8&0.8&0.0\\ \end{bmatrix}$ |  |'
- en: '*Intransitive-Hard Instance ($p_{w}=0.6$)*'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*不传递性困难实例（$p_{w}=0.6$）*'
- en: '|  | $P=\begin{bmatrix}0.0&0.8&0.6&0.4&0.4\\ 0.2&0.0&0.8&0.6&0.4\\ 0.4&0.2&0.0&0.8&0.4\\
    0.6&0.4&0.2&0.0&0.4\\ 0.6&0.6&0.6&0.6&0.0\\ \end{bmatrix}$ |  |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | $P=\begin{bmatrix}0.0&0.8&0.6&0.4&0.4\\ 0.2&0.0&0.8&0.6&0.4\\ 0.4&0.2&0.0&0.8&0.4\\
    0.6&0.4&0.2&0.0&0.4\\ 0.6&0.6&0.6&0.6&0.0\\ \end{bmatrix}$ |  |'
- en: C.1.2 Design of Prompts
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.2 提示设计
- en: '![Refer to caption](img/70733c9b7ff440ace8b34a58a9083c8f.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/70733c9b7ff440ace8b34a58a9083c8f.png)'
- en: 'Figure 7: Original prompt for LLM-Env interaction in dueling bandit setting
    with temperature = 0 (except o1-preview, which is in beta phase, its system prompt
    and user prompt are concatenated together with a fixed temperature = 1), including
    context $P$, summarized history $H_{t}$, and zero-shot chain-of-thought (CoT)
    reasoning instructions $R$ (see Section [2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")).'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '图7：在决斗强盗设置中，LLM-Env交互的原始提示，温度设置为0（除了o1-preview，它处于beta阶段，其系统提示和用户提示被连接在一起，固定温度为1），包括上下文$P$、总结历史$H_{t}$，以及零-shot链式推理（CoT）指令$R$（见第[2](https://arxiv.org/html/2407.01887v3#S2
    "2 Preliminaries ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM
    Agents")节）。'
- en: '![Refer to caption](img/a64ae2ab31a6aaedff63171ae896dde5.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a64ae2ab31a6aaedff63171ae896dde5.png)'
- en: 'Figure 8: Perturbed prompts used to evaluate the robustness of LEAD. The Biased
    History prompt (Left) injects an incorrect dueling history favoring non-optimal
    arms, while the Reversed Goal prompt (Right) adversarially modifies the objective
    from maximizing to minimizing reward. Both prompts maintain the zero-shot chain-of-thought
    (CoT) reasoning and temperature setting as before.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：用于评估LEAD鲁棒性的扰动提示。偏置历史提示（左侧）注入了一个错误的决斗历史，偏向非最优臂，而反向目标提示（右侧）对目标进行了敌对修改，从最大化奖励变为最小化奖励。两种提示都保持了零-shot链式推理（CoT）和与之前相同的温度设置。
- en: C.1.3 Exemplars of GPT-4 Turbo and o1-preview
  id: totrans-318
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.1.3 GPT-4 Turbo 和 o1-preview 的示例
- en: 'We present exemplars using the original prompt (see Figure [7](https://arxiv.org/html/2407.01887v3#A3.F7
    "Figure 7 ‣ C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")) to illustrate the decision-making process of
    both GPT-4 Turbo and o1-preview in the dueling bandits setting. These examples
    highlight how each model interprets the available data, reasons about the relative
    strengths of the arms, and selects the next pair of arms to duel. The exemplars
    show both successful convergence cases and the failures of each model.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过使用原始提示来展示示例（见图[7](https://arxiv.org/html/2407.01887v3#A3.F7 "Figure 7 ‣
    C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")），以说明GPT-4 Turbo和o1-preview在对抗赌博者设置中的决策过程。这些示例突出了每个模型如何解释可用数据，推理各个臂的相对强度，并选择下一对臂进行对决。示例展示了成功收敛的案例以及每个模型的失败。'
- en: Systematic Biases
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 系统性偏差
- en: '<svg class="ltx_picture" height="526.92" id="A3.SS1.SSSx1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,526.92) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 508.72)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Biased Understanding of GPT-4
    Turbo and o1-preview</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="477.22" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">System Prompt: You are a Dueling
    Bandit algorithm with 5 arms. Each pair of arms (i, j) has an unknown probability
    $P(i>j)$ of arm $i$ winning against arm $j$ in a pairwise duel. There exists a
    best arm (Condorcet Winner) among the 5 arms, which has a greater than 50% probability
    of winning against any other arm in a pairwise duel. At each time step, I will
    provide you with a summary that shows the past dueling results and empirical probabilities.
    Then you must choose 2 arms to duel against each other. The reward you receive
    is the sum of the unknown probabilities of the two chosen arms beating the best
    arm. The maximum reward in each time step is achieved if the best arm duels against
    itself. Your goal is to maximize the cumulative sum of rewards of the chosen two
    arms over a given time horizon $T=1000$. Question: Can an arm duel against itself?
    Answer Exemplars: Example 1: In a traditional dueling bandit problem, an arm dueling
    with itself doesn’t make practical sense because the outcome is trivially known—it
    would always win. The typical setup for dueling bandits is to choose two distinct
    arms to engage in a duel, which provides meaningful information about their relative
    strengths. Example 2: In the context of a dueling bandit problem, an arm dueling
    with itself typically doesn’t make sense because the purpose of a duel is to compare
    two different arms. Dueling an arm with itself wouldn’t provide any new information
    about the relative strengths of the arms, as the outcome is trivially known: the
    arm would win against itself 100% of the time. Example 3: In the context of a
    dueling bandit problem, an arm cannot duel with itself because the concept of
    a duel inherently involves comparing two different entities. The purpose of a
    duel is to establish which of the two arms is superior, or to collect comparative
    information. An arm dueling itself would not generate any valuable information
    or result, as it would trivially win against itself. Therefore, you need to choose
    two distinct arms for each duel in a dueling bandit problem. Example 4: In the
    context of a dueling bandit problem, an arm cannot duel with itself because the
    concept of a duel inherently involves comparing two different entities. The purpose
    of a duel is to establish which of the two arms is superior, or to collect comparative
    information. An arm dueling itself would not generate any valuable information
    or result, as it would trivially win against itself. Therefore, you need to choose
    two distinct arms for each duel in a dueling bandit problem. Comment: LLMs lack
    a fundamental understanding of the DB problem'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="526.92" id="A3.SS1.SSSx1.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,526.92) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 508.72)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">对GPT-4 Turbo和o1-preview的偏见理解</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="477.22" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">系统提示：你是一个具有5个臂的决斗强盗算法。每对臂（i, j）有一个未知的概率$P(i>j)$，表示臂$i$在一对一决斗中战胜臂$j$的概率。在这5个臂中，存在一个最强臂（Condorcet胜者），它有超过50%的概率在一对一决斗中战胜其他任何一个臂。在每个时间步，我会向你提供一个总结，显示过去的决斗结果和经验概率。然后，你必须选择两个臂进行决斗。你获得的奖励是所选择的两个臂战胜最强臂的未知概率之和。在每个时间步，最大奖励是当最强臂与自己决斗时获得的奖励。你的目标是在给定的时间范围$T=1000$内最大化所选择的两个臂的奖励总和。问题：一个臂能与自己决斗吗？答案示例：示例1：在传统的决斗强盗问题中，臂与自己决斗没有实际意义，因为结果显然已知——它总是会胜利。决斗强盗的典型设置是选择两个不同的臂进行决斗，这样可以提供关于它们相对强度的有意义信息。示例2：在决斗强盗问题的背景下，臂与自己决斗通常没有意义，因为决斗的目的是比较两个不同的臂。臂与自己决斗不会提供任何关于臂相对强度的新信息，因为结果显然已知：该臂会100%战胜自己。示例3：在决斗强盗问题的背景下，臂不能与自己决斗，因为决斗的概念本质上涉及比较两个不同的实体。决斗的目的是确定哪一个臂更强，或者收集比较信息。臂与自己决斗不会产生任何有价值的信息或结果，因为它会显然战胜自己。因此，在决斗强盗问题中，你需要为每次决斗选择两个不同的臂。示例4：在决斗强盗问题的背景下，臂不能与自己决斗，因为决斗的概念本质上涉及比较两个不同的实体。决斗的目的是确定哪一个臂更强，或者收集比较信息。臂与自己决斗不会产生任何有价值的信息或结果，因为它会显然战胜自己。因此，在决斗强盗问题中，你需要为每次决斗选择两个不同的臂。评论：LLM缺乏对决斗强盗问题的基本理解</foreignobject></g></svg>
- en: Success cases
  id: totrans-322
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 成功案例
- en: '<svg class="ltx_picture" height="277.86" id="A3.SS1.SSSx2.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,277.86) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 259.65)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Successful Convergence Case
    of GPT-4 Turbo</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="228.16" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">1\. Analysis: - Arm 5 appears
    to be the strongest contender, showing a high probability of winning against all
    other arms except Arm 2, where it still maintains a majority win rate. - Arm 3
    has a strong win rate against Arm 4 and a reasonable rate against Arm 2, but significantly
    loses to Arm 5. - Arm 2 has a mixed performance, winning against Arm 1 and Arm
    4 but losing to Arm 5 and Arm 3. - Arm 1 and Arm 4 are the weakest, with Arm 1
    losing to almost all arms and Arm 4 only winning against Arm 1. - Given the goal
    to maximize the cumulative sum of rewards, focusing on duels involving Arm 5 seems
    most beneficial, as it has the highest probability of winning against other arms,
    suggesting it might be the Condorcet Winner. 2\. Next Pair: (Arm 5, Arm 5). -
    Dueling Arm 5 against itself maximizes the reward for this step, as it likely
    has the highest unknown probability of beating the best arm (itself in this case).
    This strategy ensures the highest possible reward in a single duel under the given
    conditions. Comment: Successfully understand the problem and suggest a converged
    arm pair<svg class="ltx_picture" height="294.46" id="A3.SS1.SSSx2.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,294.46) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 276.26)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Successful Convergence Case
    of o1-preview</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="244.76" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">1\. Analysis: Given the dueling
    results so far, we are tasked with maximizing the cumulative reward over the remaining
    time horizon $T=2000$ steps. The reward at each time step is the sum of the unknown
    probabilities of the two chosen arms beating the best arm (the Condorcet winner).
    To maximize our reward, we need to identify the arm that is most likely the best
    based on the empirical data and choose it to duel against itself. This is because
    the maximum reward at each time step is achieved when the best arm duels against
    itself. Analyzing the empirical probabilities: - Arm 5 has beaten all other arms
    with a probability greater than 50%: - $P(5>1)=0.75$ - $P(5>2)=0.75$ - $P(5>3)=0.64$
    - $P(5>4)=1.00$ This suggests that Arm 5 is the most likely Condorcet winner based
    on the data. Therefore, to maximize the cumulative reward, we should choose Arm
    5 to duel against itself, as this maximizes the sum of probabilities $2\times
    P(5>\text{BestArm})$. 2\. Next Pair: (Arm 5, Arm 5) Comment: Successfully understand
    the problem and suggest a converged arm pair'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="277.86" id="A3.SS1.SSSx2.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,277.86) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 259.65)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">GPT-4 Turbo成功收敛案例</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="228.16" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">1\. 分析：- 机器人臂5似乎是最强的竞争者，显示出对所有其他臂的获胜概率很高，除了臂2，它在该臂中仍保持着多数获胜率。
    - 机器人臂3在与臂4对战时有很强的获胜率，并且与臂2的对战也表现合理，但在与臂5对战时显著失利。 - 机器人臂2的表现较为混合，赢得了臂1和臂4的对战，但在与臂5和臂3的对战中失利。
    - 机器人臂1和臂4是最弱的，臂1几乎输给了所有其他臂，而臂4仅能战胜臂1。 - 既然目标是最大化奖励的累计和，那么专注于涉及臂5的对战似乎最有利，因为它在对战其他臂时具有最高的获胜概率，这表明它可能是康多塞赢家。
    2\. 下一对对战：（臂5，臂5）。 - 将臂5与其自身对战最大化了此步骤的奖励，因为它很可能拥有击败最佳臂（即它自己）的最高未知概率。这个策略确保了在给定条件下单次对战的最大奖励。
    评论：成功理解了问题并建议了一个收敛的臂对</foreignobject></g> <svg class="ltx_picture" height="294.46"
    id="A3.SS1.SSSx2.p3.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000"
    stroke="#000000" stroke-width="0.4pt" transform="translate(0,294.46) matrix(1
    0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 276.26)"><foreignobject
    color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">o1-preview成功收敛案例</foreignobject></g> <g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000"
    height="244.76" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">1\.
    分析：根据迄今为止的对战结果，我们的任务是在剩余的时间范围内最大化累积奖励 $T=2000$ 步。每个时间步的奖励是两个选定臂击败最佳臂（即康多塞赢家）的未知概率之和。为了最大化奖励，我们需要根据经验数据确定最有可能是最佳臂的臂，并选择它与自身对战。因为每个时间步的最大奖励是在最佳臂与自身对战时获得的。分析经验概率：
    - 臂5以超过50%的概率击败了所有其他臂： - $P(5>1)=0.75$ - $P(5>2)=0.75$ - $P(5>3)=0.64$ - $P(5>4)=1.00$
    这表明，基于数据，臂5最有可能是康多塞赢家。因此，为了最大化累积奖励，我们应该选择臂5与自身对战，因为这最大化了概率之和 $2\times P(5>\text{BestArm})$。
    2\. 下一对对战：（臂5，臂5） 评论：成功理解了问题并建议了一个收敛的臂对</foreignobject></g>
- en: Failure cases
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 失败案例
- en: '<svg class="ltx_picture" height="360.88" id="A3.SS1.SSSx3.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,360.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 342.67)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Convergence Instability Case
    of GPT-4 Turbo:</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="311.18" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">1\. Analysis: - Arm 5 appears
    to be a strong contender as it has a high winning probability against most other
    arms, particularly against Arm 1, Arm 2, Arm 3, and Arm 4\. Notably, it has a
    significant number of duels against Arm 3, which provides a robust estimate of
    its strength relative to Arm 3. - Arm 3 also shows strength, especially against
    Arm 4 and Arm 2, but it has a lower winning probability against Arm 5. - Arm 1
    and Arm 4 seem to be weaker overall, with Arm 1 losing all duels against Arm 5
    and Arm 4 having a low win rate against Arm 5. - Arm 2 is somewhat in the middle,
    with a mixed record against the other arms. Given the results, Arm 5 is likely
    the Condorcet Winner as it has a greater than 50% win rate against all other arms.
    To maximize the reward, it would be beneficial to continue testing Arm 5 against
    other strong contenders to confirm its dominance and further understand the dynamics
    between the other arms. 2\. Next Pair: - This pairing is chosen because while
    Arm 5 has shown strong performance against Arm 3, the large number of duels suggests
    that further testing could provide additional insights and confirm stability in
    the empirical probabilities. Additionally, dueling these two strong arms could
    potentially yield a high reward if Arm 5 continues to dominate. Choose: (Arm 5,
    Arm 3) Comment: Keeps comparing the same top arms without committing to a single
    winner<svg class="ltx_picture" height="311.07" id="A3.SS1.SSSx3.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,311.07) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 292.86)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Biased Understanding Case of
    o1-preview:</foreignobject></g> <g fill-opacity="1.0" transform="matrix(1.0 0.0
    0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="261.37" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">1\. Analysis: From the given
    data, we can infer that Arm 5 is likely the best arm (Condorcet Winner) because
    it has beaten all other arms with probabilities greater than 50 - P(5 > 1) = 0.75
    - P(5 > 2) = 0.75 - P(5 > 3) = 0.66 - P(5 > 4) = 1.00 Our goal is to choose two
    arms (armA and armB) such that the sum P(armA > best arm) + P(armB > best arm)
    is maximized. From the empirical probabilities: - P(3 > 5) = 0.34 - P(2 > 5) =
    0.25 - P(1 > 5) = 0.25 - P(4 > 5) = 0.00 Arm 3 has the highest probability of
    beating Arm 5, followed by Arms 2 and 1\. Arm 4 has a zero probability and should
    not be selected. Between Arms 1 and 2, Arm 2 has participated in more duels against
    Arm 5, providing a more reliable estimate of its probability to beat the best
    arm. Additionally, Arm 2’s performance against other arms suggests it may be slightly
    better than Arm 1. Therefore, the best choice to maximize our expected reward
    is to select Arm 3 and Arm 2. 2\. Next Pair: (Arm 3, Arm 2) Comment: Exhibits
    a biased understanding of the problem description: while successfully identifying
    the Condorcet Winner, it still selects suboptimal arms in duel.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: <svg class="ltx_picture" height="360.88" id="A3.SS1.SSSx3.p1.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,360.88) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 342.67)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">GPT-4 Turbo 的收敛不稳定案例：</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="311.18" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">1\. 分析： - 臂 5 似乎是一个强有力的竞争者，因为它对大多数其他臂的胜率较高，特别是在与臂 1、臂
    2、臂 3 和臂 4 的对抗中尤为突出。值得注意的是，它与臂 3 的对战次数较多，这为其相对臂 3 的强度提供了稳健的估计。 - 臂 3 也表现出强势，特别是在对抗臂
    4 和臂 2 时，但它对臂 5 的胜率较低。 - 臂 1 和臂 4 整体较弱，其中臂 1 在与臂 5 的所有对抗中均告败，臂 4 在与臂 5 的对抗中也有较低的胜率。
    - 臂 2 则位于中间位置，在与其他臂的对战中有着参差不齐的战绩。根据这些结果，臂 5 可能是 Condorcet 胜者，因为它对所有其他臂的胜率均高于 50%。为了最大化奖励，继续将臂
    5 与其他强有力的竞争者对战以确认其优势，并进一步了解其他臂之间的动态将是有益的。 2\. 下一对： - 选择这一对是因为虽然臂 5 在与臂 3 的对战中表现强劲，但大量对战表明，进一步测试可能提供更多见解，并确认经验概率的稳定性。此外，对这两只强臂进行对抗，如果臂
    5 继续占据优势，可能会带来更高的奖励。选择： (臂 5, 臂 3) 评论：持续比较相同的顶级臂，但未决定一个明确的赢家<svg class="ltx_picture"
    height="311.07" id="A3.SS1.SSSx3.p3.pic1" overflow="visible" version="1.1" width="600"><g
    fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,311.07)
    matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0
    21.65 292.86)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">o1-preview 的偏见理解案例：</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="261.37" overflow="visible" transform="matrix(1 0 0 -1
    0 16.6)" width="556.69">1\. 分析：根据给定的数据，我们可以推断臂 5 可能是最强的臂（Condorcet 胜者），因为它以超过
    50% 的概率击败了所有其他臂 - P(5 > 1) = 0.75 - P(5 > 2) = 0.75 - P(5 > 3) = 0.66 - P(5 >
    4) = 1.00 我们的目标是选择两个臂（armA 和 armB），使得 P(armA > 最强臂) + P(armB > 最强臂) 的和最大化。从经验概率来看：
    - P(3 > 5) = 0.34 - P(2 > 5) = 0.25 - P(1 > 5) = 0.25 - P(4 > 5) = 0.00 臂 3 击败臂
    5 的概率最高，其次是臂 2 和臂 1。臂 4 的概率为零，因此不应选择。在臂 1 和臂 2 之间，臂 2 与臂 5 对战的次数更多，提供了更可靠的估计，表明其击败最强臂的概率更高。此外，臂
    2 对其他臂的表现也表明它可能稍微强于臂 1。因此，最大化预期奖励的最佳选择是选择臂 3 和臂 2。 2\. 下一对： (臂 3, 臂 2) 评论：展现了对问题描述的偏见理解：虽然成功识别了
    Condorcet 胜者，但仍选择了在对决中表现较差的臂。</foreignobject></g>
- en: C.2 Supplementary Experiments
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2 补充实验
- en: C.2.1 Comparisons with Different Metrics
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2.1 不同度量的比较
- en: 'We present supplementary results to complement our case studies. (i) Figure [9](https://arxiv.org/html/2407.01887v3#A3.F9
    "Figure 9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") shows the strong and weak regret
    comparisons for the Hard instance. (ii) Figure[12](https://arxiv.org/html/2407.01887v3#A3.F12
    "Figure 12 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") presents comparisons under different
    numbers of arms $K$, illustrating the impact of the Relative Decision Window.
    (iii) Figure [13](https://arxiv.org/html/2407.01887v3#A3.F13 "Figure 13 ‣ C.2.1
    Comparisons with Different Metrics ‣ C.2 Supplementary Experiments ‣ Appendix
    C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents") introduces the Best Arm Inclusion Ratio and
    the Converged Best Arm Ratio. (iv) Figure[14](https://arxiv.org/html/2407.01887v3#A3.F14
    "Figure 14 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents") examines the generalized variance
    of the strong and weak regret for both instances.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了补充结果以补充我们的案例研究。（i）图[9](https://arxiv.org/html/2407.01887v3#A3.F9 "Figure
    9 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments ‣
    Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards: In-Context
    Dueling Bandits with LLM Agents")展示了困难实例的强悔和弱悔比较。（ii）图[12](https://arxiv.org/html/2407.01887v3#A3.F12
    "Figure 12 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")展示了不同臂数$K$下的比较，说明了相对决策窗口的影响。（iii）图[13](https://arxiv.org/html/2407.01887v3#A3.F13
    "Figure 13 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")介绍了最佳臂包含比例和收敛最佳臂比例。（iv）图[14](https://arxiv.org/html/2407.01887v3#A3.F14
    "Figure 14 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")考察了两个实例的强悔和弱悔的广义方差。'
- en: '![Refer to caption](img/542d0c195522c505e50dde0ea6875d62.png)![Refer to caption](img/19eef2bb3637ac33c584dc50bd83bd20.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/542d0c195522c505e50dde0ea6875d62.png)![参见说明](img/19eef2bb3637ac33c584dc50bd83bd20.png)'
- en: 'Figure 9: Comparisons between LLM agents and various classic DB algorithms.
    Left and Right: strong and weak regret for the Transitive-Hard instance. Results
    for the Transitive-Easy instance is presented in Figure [2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). We evaluate
    only three LLMs on the Transitive-Hard instance due to our research goals and
    high API costs: (i) The results for the Transitive-Hard instance are qualitatively
    similar to those for the Transitive-Easy instance; (ii) Obviously, the Transitive-Easy
    instance offers higher distinguishability, allowing us to observe convergence
    and regret differences within a feasible number of steps.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '图9：LLM代理与各种经典数据库算法的比较。左图和右图：对于传递性困难实例的强悔和弱悔。传递性简单实例的结果见图[2](https://arxiv.org/html/2407.01887v3#S3.F2
    "Figure 2 ‣ 3.2 Experimental results ‣ 3 LLMs as Standalone In-Context Decision-Makers
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents")。由于我们的研究目标和高昂的API成本，我们仅在传递性困难实例上评估了三种LLM：（i）传递性困难实例的结果与传递性简单实例的结果在质量上是相似的；（ii）显然，传递性简单实例提供了更高的可区分性，使我们能够在合理的步数内观察到收敛和悔差异。'
- en: '![Refer to caption](img/b2b4aa995af17556c5f90ea7aa3c963a.png)![Refer to caption](img/03dc6ee70bb6dc523f534502dd04c135.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b2b4aa995af17556c5f90ea7aa3c963a.png)![参见说明](img/03dc6ee70bb6dc523f534502dd04c135.png)'
- en: 'Figure 10: Comparisons between GPT-4 Turbo and various classic DB algorithms.
    Left and Right: strong and weak regret for the Intransitive-Easy instance. Results
    for the Intransitive-Hard instance is presented in Figure [11](https://arxiv.org/html/2407.01887v3#A3.F11
    "Figure 11 ‣ C.2.1 Comparisons with Different Metrics ‣ C.2 Supplementary Experiments
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents"). We evaluate only our top-performing
    LLM on the Intransitive-Easy and Intransitive-Hard instance to examine the scalability
    limitation.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：GPT-4 Turbo与各种经典DB算法的对比。左和右：Intransitive-Easy实例上的强后悔与弱后悔。Intransitive-Hard实例的结果见图[11](https://arxiv.org/html/2407.01887v3#A3.F11
    "图11 ‣ C.2.1 使用不同指标的对比 ‣ C.2 补充实验 ‣ 附录C 提示设计和补充结果 ‣ 超越数值奖励：使用LLM代理的上下文决斗强盗")。我们仅评估了在Intransitive-Easy和Intransitive-Hard实例上表现最好的LLM，以检验其可扩展性限制。
- en: '![Refer to caption](img/7113c2b9a066b9a84326ee6a937a44f6.png)![Refer to caption](img/c61413d530a0c90e0c1e1109ab5bad97.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/7113c2b9a066b9a84326ee6a937a44f6.png)![参见图注](img/c61413d530a0c90e0c1e1109ab5bad97.png)'
- en: 'Figure 11: Comparisons between GPT-4 Turbo and various classic DB algorithms.
    Left and Right: strong and weak regret for the Intransitive-Hard instance.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：GPT-4 Turbo与各种经典DB算法的对比。左和右：Intransitive-Hard实例上的强后悔与弱后悔。
- en: '![Refer to caption](img/63e19d6e9481e007c8e14f817e9e5fd6.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/63e19d6e9481e007c8e14f817e9e5fd6.png)'
- en: 'Figure 12: Cumulative strong and weak regret comparisons between LLM agents
    and classic dueling bandit algorithms on Transitive-Easy instance under different
    numbers of arms K. Top Left and Top Right: K=5, where GPT-4-Turbo significantly
    outperforms other methods on weak regret. Bottom Left and Bottom Right: K=10,
    where the performance of GPT-4-Turbo degrades as the number of arms increases.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：在不同臂数 K 下，LLM代理与经典决斗强盗算法在Transitive-Easy实例上的累积强后悔与弱后悔对比。左上和右上：K=5，GPT-4-Turbo在弱后悔上明显优于其他方法。左下和右下：K=10，随着臂数增加，GPT-4-Turbo的表现有所下降。
- en: '![Refer to caption](img/cccc203709c20800f39ccb6c00a5125e.png)![Refer to caption](img/8ceda6f16464688ad96893acd52d06f2.png)![Refer
    to caption](img/e562d2c3648bf7319d543f3b1d769998.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/cccc203709c20800f39ccb6c00a5125e.png)![参见图注](img/8ceda6f16464688ad96893acd52d06f2.png)![参见图注](img/e562d2c3648bf7319d543f3b1d769998.png)'
- en: 'Figure 13: Four LLMs (GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, o1-preview) and two
    state-of-the-art baselines (Self-Sparring and DTS) are compared against each other
    on the Transitive-Easy instance over different time intervals. Left: the Best
    Arm Inclusion Ratio represents the fraction of duels that include the best arm
    (Condorcet winner). Middle: the Converged Best Arm Ratio represents the proportion
    of duels where the best arm duels against itself for exploitation. Right: the
    Suboptimal Duel Ratio represents the proportion of duels where both arms selected
    in duel are suboptimal arms. We observed that while o1-preview can transit from
    exploration to exploitation (high Converged Best Arm Ratio), it selects more optimal
    arms (high Suboptimal Duel Ratio) due to the reinforced biased understanding as
    discussed in Section [3.2](https://arxiv.org/html/2407.01887v3#S3.SS2 "3.2 Experimental
    results ‣ 3 LLMs as Standalone In-Context Decision-Makers ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents").'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：四种LLM（GPT-3.5 Turbo、GPT-4、GPT-4 Turbo、o1-preview）与两种最先进的基准方法（Self-Sparring和DTS）在Transitive-Easy实例上的表现进行比较，覆盖不同时间间隔。左：最佳臂包含比率表示包括最佳臂（Condorcet胜者）的对决的比例。中：收敛最佳臂比率表示最佳臂与自身进行对决以进行利用的对决比例。右：次优对决比率表示在对决中，两个被选择的臂都是次优臂的比例。我们观察到，尽管o1-preview能够从探索过渡到利用（高收敛最佳臂比率），但由于在第[3.2节](https://arxiv.org/html/2407.01887v3#S3.SS2
    "3.2 实验结果 ‣ 3 LLM作为独立上下文决策者 ‣ 超越数值奖励：使用LLM代理的上下文决斗强盗")中讨论的增强偏置理解，它选择了更多的最优臂（高次优对决比率）。
- en: '![Refer to caption](img/f83a01a19567a4f74aba90d557ea15d2.png)![Refer to caption](img/f0be390ded3260c324b54e99d8ffd1c4.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/f83a01a19567a4f74aba90d557ea15d2.png)![参见图注](img/f0be390ded3260c324b54e99d8ffd1c4.png)'
- en: 'Figure 14: Comparison of the generalized variance of strong and weak regret
    between three LLMs and baseline algorithms on the Transitive-Easy (Left) and Transitive-Hard
    (Right) instances. In the Easy instance, GPT-4 Turbo exhibits the lowest average
    generalized variance. For the Transitive-Hard instance, GPT-4 Turbo maintains
    a variance level comparable to state-of-the-art baseline algorithms (except BTM
    and SAVAGE, which are in an early exploration stage).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：在Transitive-Easy（左）和Transitive-Hard（右）实例中，三种LLM和基准算法的强后悔和弱后悔的广义方差比较。在Easy实例中，GPT-4
    Turbo表现出最低的平均广义方差。对于Transitive-Hard实例，GPT-4 Turbo维持了与最先进基准算法相当的方差水平（除了BTM和SAVAGE，它们仍处于早期探索阶段）。
- en: C.2.2 Duel Selection Trajectory
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C.2.2 对决选择轨迹
- en: We visualize the duel selection trajectory in representative experiments to
    better understand the behavior of LLM agents and baseline algorithms.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在具有代表性的实验中可视化了对决选择轨迹，以更好地理解LLM代理和基准算法的行为。
- en: 'Duel Selection Trajectory Explanation: The reshuffled arm order is $b_{5}\succ
    b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$, with arm indices from bottom to top:
    5, 4, 3, 2, 1\. Each filled black cell represents a selected arm at that time
    step. For instance, black lines in arms 5 and 3 indicate the selection of the
    duel between (arm 5, arm 3) at that particular time step.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 对决选择轨迹说明：重新排列的臂顺序为$b_{5}\succ b_{3}\succ b_{2}\succ b_{1}\succ b_{4}$，臂的索引从下到上分别为：5,
    4, 3, 2, 1。每个填充的黑色单元格表示该时间步选中的臂。例如，臂5和臂3中的黑线表示在该时间步选中了（臂5，臂3）的对决。
- en: '![Refer to caption](img/3a766eb7198a1386766bc1cb7d7477ed.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/3a766eb7198a1386766bc1cb7d7477ed.png)'
- en: 'Figure 15: Comparison of duel selection trajectories between GPT-4 Turbo (Left)
    and o1-preview (Right) on the Transitive-Easy instance. GPT-4 Turbo achieves low
    weak regret by consistently selecting the best arm, though it struggles to converge
    to a single best arm. In contrast, o1-preview shows better convergence behavior,
    but its weak regret performance is worse than GPT-4 Turbo due to incomplete or
    biased understanding, as illustrated by the o1-preview exemplar in Appendix [C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3
    "C.1.3 Exemplars of GPT-4 Turbo and o1-preview ‣ C.1 LLM Experimental Results
    ‣ Appendix C Prompt Design and Supplementary Results ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents").'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：在Transitive-Easy实例中，GPT-4 Turbo（左）和o1-preview（右）之间的对决选择轨迹比较。GPT-4 Turbo通过持续选择最佳臂来实现较低的弱后悔，尽管它在收敛到单一最佳臂方面存在困难。相比之下，o1-preview表现出更好的收敛行为，但由于理解不完整或有偏，导致其弱后悔表现比GPT-4
    Turbo差，如附录[C.1.3](https://arxiv.org/html/2407.01887v3#A3.SS1.SSS3 "C.1.3 GPT-4
    Turbo和o1-preview示例 ‣ C.1 LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：使用LLM代理的上下文对决强盗")中o1-preview示例所示。
- en: '![Refer to caption](img/22275493b6a3005ffbbf44b70b219da2.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/22275493b6a3005ffbbf44b70b219da2.png)'
- en: 'Figure 16: Local optima trajectories of GPT-3.5 Turbo (Left), GPT-4 (Middle),
    and GPT-4 Turbo (Right, with noisy prompt) on the Transitive-Hard instance. Less
    capable LLMs, such as GPT-3.5 Turbo and GPT-4, could get stuck comparing suboptimal
    arms on hard preference structure. Even for GPT-4 Turbo, noisy prompts with biased
    history (see Figure [8](https://arxiv.org/html/2407.01887v3#A3.F8 "Figure 8 ‣
    C.1.2 Design of Prompts ‣ C.1 LLM Experimental Results ‣ Appendix C Prompt Design
    and Supplementary Results ‣ Beyond Numeric Awards: In-Context Dueling Bandits
    with LLM Agents")) can lead it to be trapped in bad tournaments.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：在Transitive-Hard实例中，GPT-3.5 Turbo（左）、GPT-4（中）和GPT-4 Turbo（右，带有噪声提示）的局部最优轨迹。较弱的LLM，如GPT-3.5
    Turbo和GPT-4，可能会在困难的偏好结构中卡住，比较次优的臂。即使是GPT-4 Turbo，带有偏见历史的噪声提示（参见图[8](https://arxiv.org/html/2407.01887v3#A3.F8
    "图8 ‣ C.1.2 提示设计 ‣ C.1 LLM实验结果 ‣ 附录C 提示设计与补充结果 ‣ 超越数值奖励：使用LLM代理的上下文对决强盗")）也可能导致其陷入不良的对决中。
- en: '![Refer to caption](img/d62f55a622c8c570a735f92be9ed92bf.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/d62f55a622c8c570a735f92be9ed92bf.png)'
- en: 'Figure 17: Comparison of success (Left) and failure (Right) cases for the Convergence-Triggered
    GPT-4 Turbo intervention strategy discussed in Section [4.1](https://arxiv.org/html/2407.01887v3#S4.SS1
    "4.1 Algorithmic Design of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits
    ‣ Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"). While it
    works for most cases due to GPT-4 Turbo’s strong capability (Left), sometimes
    this naive intervention can reinforce suboptimal choices (Right) on the Transitive-Hard
    instance.</foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg></foreignobject></g></g></svg>'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '图17：在[4.1节](https://arxiv.org/html/2407.01887v3#S4.SS1 "4.1 Algorithmic Design
    of LEAD ‣ 4 Algorithm-Enhanced LLMs for Dueling Bandits ‣ Beyond Numeric Awards:
    In-Context Dueling Bandits with LLM Agents")中讨论的收敛触发型GPT-4 Turbo干预策略的成功（左）和失败（右）案例对比。由于GPT-4
    Turbo的强大能力，这种策略在大多数情况下有效（左），但有时这种简单的干预会在传递困难的实例上强化次优选择（右）。'
