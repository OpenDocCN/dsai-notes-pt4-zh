- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2025-01-11 11:45:29'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2025-01-11 11:45:29'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: On the Structural Memory of LLM Agents
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于LLM代理的结构性记忆
- en: 来源：[https://arxiv.org/html/2412.15266/](https://arxiv.org/html/2412.15266/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2412.15266/](https://arxiv.org/html/2412.15266/)
- en: Ruihong Zeng^(1∗), Jinyuan Fang^(1∗), Siwei Liu^(2†), Zaiqiao Meng^(1†)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruihong Zeng^(1∗), Jinyuan Fang^(1∗), Siwei Liu^(2†), Zaiqiao Meng^(1†)
- en: ¹University of Glasgow   ²University of Aberdeen
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹格拉斯哥大学   ²阿伯丁大学
- en: zengrh3@gmail.com,   j.fang.2@research.gla.ac.uk
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zengrh3@gmail.com,   j.fang.2@research.gla.ac.uk
- en: siwei.liu@abdn.ac.uk,   zaiqiao.meng@glasgow.ac.uk
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: siwei.liu@abdn.ac.uk,   zaiqiao.meng@glasgow.ac.uk
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: '^*^*footnotetext: Equal contribution.^($\dagger$)^($\dagger$)footnotetext:
    Corresponding author.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ^*^*脚注：等贡献。^($\dagger$)^($\dagger$)脚注：通讯作者。
- en: 'Memory plays a pivotal role in enabling large language model (LLM)-based agents
    to engage in complex and long-term interactions, such as question answering (QA)
    and dialogue systems. While various memory modules have been proposed for these
    tasks, the impact of different memory structures across tasks remains insufficiently
    explored. This paper investigates how memory structures and memory retrieval methods
    affect the performance of LLM-based agents. Specifically, we evaluate four types
    of memory structures, including chunks, knowledge triples, atomic facts, and summaries,
    along with mixed memory that combines these components. In addition, we evaluate
    three widely used memory retrieval methods: single-step retrieval, reranking,
    and iterative retrieval. Extensive experiments conducted across four tasks and
    six datasets yield the following key insights: (1) Different memory structures
    offer distinct advantages, enabling them to be tailored to specific tasks; (2)
    Mixed memory structures demonstrate remarkable resilience in noisy environments;
    (3) Iterative retrieval consistently outperforms other methods across various
    scenarios. Our investigation aims to inspire further research into the design
    of memory systems for LLM-based agents. ¹¹1All code and datasets are publicly
    available at: [https://github.com/zengrh3/StructuralMemory](https://github.com/zengrh3/StructuralMemory)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆在使基于大语言模型（LLM）的代理能够进行复杂和长期的互动（如问答系统（QA）和对话系统）中扮演着至关重要的角色。尽管已经提出了各种记忆模块用于这些任务，但不同记忆结构在不同任务中的影响尚未得到充分探索。本文探讨了记忆结构和记忆检索方法如何影响基于LLM的代理的性能。具体而言，我们评估了四种类型的记忆结构，包括块（chunks）、知识三元组（knowledge
    triples）、原子事实（atomic facts）和摘要（summaries），以及结合这些组件的混合记忆。此外，我们评估了三种广泛使用的记忆检索方法：单步检索、重排序和迭代检索。我们在四个任务和六个数据集上进行的广泛实验得出了以下关键见解：（1）不同的记忆结构具有不同的优势，能够针对特定任务进行调整；（2）混合记忆结构在噪声环境中表现出显著的弹性；（3）迭代检索在各种场景中始终优于其他方法。我们的研究旨在激发更多关于LLM代理记忆系统设计的研究。¹¹1所有代码和数据集都可以在以下网址公开获取：[https://github.com/zengrh3/StructuralMemory](https://github.com/zengrh3/StructuralMemory)
- en: On the Structural Memory of LLM Agents
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LLM代理的结构性记忆
- en: Ruihong Zeng^(1∗), Jinyuan Fang^(1∗), Siwei Liu^(2†), Zaiqiao Meng^(1†) ¹University
    of Glasgow   ²University of Aberdeen zengrh3@gmail.com,   j.fang.2@research.gla.ac.uk
    siwei.liu@abdn.ac.uk,   zaiqiao.meng@glasgow.ac.uk
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Ruihong Zeng^(1∗), Jinyuan Fang^(1∗), Siwei Liu^(2†), Zaiqiao Meng^(1†) ¹格拉斯哥大学
      ²阿伯丁大学 zengrh3@gmail.com,   j.fang.2@research.gla.ac.uk siwei.liu@abdn.ac.uk,
      zaiqiao.meng@glasgow.ac.uk
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) Minaee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib22))
    have attracted widespread attention in natural language tasks due to their remarkable
    capability. Recent advancements have significantly accelerated the development
    of LLM-based agents, with research primarily focusing on profile Park et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib26));
    [Hong et al.](https://arxiv.org/html/2412.15266v1#bib.bib11) , planning Qian et al.
    ([2024](https://arxiv.org/html/2412.15266v1#bib.bib27)); Qiao et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib28)),
    action Qin et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib29)); Wang
    et al. ([2024c](https://arxiv.org/html/2412.15266v1#bib.bib40)), self-evolving Zhang
    et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib45)) and memory Packer
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24)); Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15)).
    These innovations have unlocked a wide range of applications across diverse applications Li
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib19)); Wang et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib39));
    Chen et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib5)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）Minaee等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib22)）由于其卓越的能力，在自然语言任务中吸引了广泛的关注。近期的进展显著加速了基于LLM的智能体的发展，研究主要集中在个性化配置Park等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib26)）；[Hong等人](https://arxiv.org/html/2412.15266v1#bib.bib11)，规划Qian等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib27)）；Qiao等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib28)），行动Qin等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib29)）；Wang等人（[2024c](https://arxiv.org/html/2412.15266v1#bib.bib40)），自我进化Zhang等人（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib45)）和记忆Packer等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib24)）；Lee等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib15)）。这些创新已解锁了在各种应用领域的广泛应用Li等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib19)）；Wang等人（[2024b](https://arxiv.org/html/2412.15266v1#bib.bib39)）；Chen等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib5)）。
- en: A fundamental element that underpins the effectiveness of LLM-based agents is
    the memory module. In cognitive science Simon and Newell ([1971](https://arxiv.org/html/2412.15266v1#bib.bib35));
    Anderson ([2013](https://arxiv.org/html/2412.15266v1#bib.bib1)), memory is the
    cornerstone of human cognition, enabling the storage, retrieval, and drawing from
    past experiences for strategic thinking and decision-making. Similarly, the memory
    module is vital for LLM-based agents by facilitating the retention and organization
    of past interactions, supporting complex reasoning capabilities, e.g., multi-hop
    question answering (QA) Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17));
    Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15)), and ensuring
    consistency and continuity in user interactions Nuxoll and Laird ([2007](https://arxiv.org/html/2412.15266v1#bib.bib23)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 支撑基于LLM的智能体有效性的一个基本元素是记忆模块。在认知科学中，Simon和Newell（[1971](https://arxiv.org/html/2412.15266v1#bib.bib35)）；Anderson（[2013](https://arxiv.org/html/2412.15266v1#bib.bib1)）认为，记忆是人类认知的基石，使得我们能够存储、检索并从过去的经验中汲取战略性思维和决策的依据。同样，记忆模块对于基于LLM的智能体至关重要，它通过促进过去交互的存储和组织，支持复杂推理能力，如多跳问答（QA）Li等人（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)）；Lee等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib15)），并确保用户交互的一致性和连续性Nuxoll和Laird（[2007](https://arxiv.org/html/2412.15266v1#bib.bib23)）。
- en: '![Refer to caption](img/38488953a3a708af2b35cdea01d7a939.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/38488953a3a708af2b35cdea01d7a939.png)'
- en: 'Figure 1: The framework of LLM-based agents, where we focus on the study of
    memory modules, including memory structures and retrieval methods.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于LLM的智能体框架，其中我们重点研究记忆模块，包括记忆结构和检索方法。
- en: 'Developing an effective memory module in LLM-based agents typically involves
    two critical components: structural memory generation and memory retrieval methods Wang
    et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib38)); Zhang et al.
    ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib46)). Among the various memory
    structures used by agents, chunks Hu et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12)),
    knowledge triples Anokhin et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2)),
    atomic facts Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)),
    and summaries Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15))
    are the most prevalent. For instance, HiAgent (Hu et al., [2024](https://arxiv.org/html/2412.15266v1#bib.bib12))
    utilizes sub-goals as memory chunks to manage the working memory of LLM-based
    agents, ensuring task continuity and coherence, while Arigraph Anokhin et al.
    ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2)) adopts knowledge triples,
    which combine both semantic and episodic memories to store factual and detailed
    information, making it suitable for complex reasoning tasks. Meanwhile, ReadAgent Li
    et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)) compresses memory
    episodes into gits memory with summaries manner, organizing them within a structured
    memory directory.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于LLM的智能体中，开发有效的记忆模块通常涉及两个关键组成部分：结构化记忆生成和记忆检索方法 Wang et al.（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib38)）；Zhang
    et al.（[2024b](https://arxiv.org/html/2412.15266v1#bib.bib46)）。在智能体使用的各种记忆结构中，片段 Hu
    et al.（[2024](https://arxiv.org/html/2412.15266v1#bib.bib12)），知识三元组 Anokhin et
    al.（[2024](https://arxiv.org/html/2412.15266v1#bib.bib2)），原子事实 Li et al.（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)），以及摘要 Lee
    et al.（[2024](https://arxiv.org/html/2412.15266v1#bib.bib15)）是最为常见的。例如，HiAgent （Hu
    et al.，[2024](https://arxiv.org/html/2412.15266v1#bib.bib12)）使用子目标作为记忆片段来管理基于LLM的智能体的工作记忆，确保任务的连续性和一致性；而Arigraph Anokhin
    et al.（[2024](https://arxiv.org/html/2412.15266v1#bib.bib2)）采用知识三元组，将语义记忆和情节记忆结合在一起，存储事实性和详细信息，使其适用于复杂的推理任务。与此同时，ReadAgent Li
    et al.（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)）将记忆片段压缩为摘要式的gits记忆，并将其组织在结构化的记忆目录中。
- en: 'Upon reviewing the aforementioned memory structures, an important but under-explored
    question arises: Which memory structures are best suited for specific tasks, and
    how do their distinct characteristics impact the performance of LLM-based agents?
    This question mirrors how humans organize memory into distinct forms, such as
    episodic memory for recalling events and semantic memory for understanding relationships Simon
    and Newell ([1971](https://arxiv.org/html/2412.15266v1#bib.bib35)); Anderson ([2013](https://arxiv.org/html/2412.15266v1#bib.bib1)).
    Each form serves a unique purpose, enabling humans to tackle a variety of challenges
    with flexibility and precision. Moreover, humans rely on effective retrieval processes
    to access relevant memories, ensuring the accurate recall of past experiences
    for problem-solving. This highlights the need to jointly explore memory structures
    and retrieval methods to enhance the reasoning capabilities and overall effectiveness
    of LLM-based agents.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾上述记忆结构后，出现了一个重要但尚未深入探讨的问题：哪些记忆结构最适合特定任务，它们的不同特性如何影响基于LLM的智能体的表现？这个问题反映了人类如何将记忆组织成不同形式，例如用于回忆事件的情节记忆和用于理解关系的语义记忆 Simon
    and Newell（[1971](https://arxiv.org/html/2412.15266v1#bib.bib35)）；Anderson（[2013](https://arxiv.org/html/2412.15266v1#bib.bib1)）。每种形式都有其独特的用途，使人类能够灵活而精准地应对各种挑战。此外，人类依赖有效的检索过程来访问相关记忆，确保在解决问题时准确地回忆起过去的经验。这突显了共同探索记忆结构和检索方法的必要性，以提升基于LLM的智能体的推理能力和整体效能。
- en: 'To bridge this gap, we systematically explore the impact of various memory
    structures and retrieval methods in LLM-based agents. Specifically, we evaluate
    existing four types of memory structures: chunks Hu et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12)),
    knowledge triples Anokhin et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2)),
    atomic facts Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)),
    and summaries Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)).
    Building on these, we explore the potential of mixed memory structures, which
    combine multiple types of memories to examine whether their complementary characteristics
    can enhance performance. Additionally, we assess the robustness of these memory
    structures to noise, as understanding their reliability under such conditions
    is essential for ensuring effectiveness across diverse tasks. Furthermore, we
    investigate three memory retrieval methods, including single-step retrieval Packer
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24)), reranking Gao
    et al. ([2023a](https://arxiv.org/html/2412.15266v1#bib.bib8)), and iterative
    retrieval Li et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib18)),
    to uncover how different combinations of retrieval methods and memory structures
    influence overall performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一差距，我们系统地探讨了不同记忆结构和检索方法在基于LLM的智能体中的影响。具体而言，我们评估了现有的四种记忆结构：块状记忆 Hu et al.
    ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12))，知识三元组 Anokhin et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2))，原子事实 Li
    et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))，以及总结 Li et al.
    ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))。基于这些研究，我们探索了混合记忆结构的潜力，该结构结合了多种类型的记忆，以检查它们互补的特性是否能提升性能。此外，我们还评估了这些记忆结构对噪声的鲁棒性，因为了解它们在这种条件下的可靠性对于确保在各种任务中有效性至关重要。进一步地，我们调查了三种记忆检索方法，包括单步检索 Packer
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24))，重排序 Gao et al.
    ([2023a](https://arxiv.org/html/2412.15266v1#bib.bib8))，以及迭代检索 Li et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib18))，以揭示不同的检索方法和记忆结构组合如何影响整体性能。
- en: 'The main contributions of this work can be summarized as follows: (1) We present
    the first comprehensive study on the impact of memory structures and memory retrieval
    methods in LLM-based agents on six datasets across four tasks: multi-hop QA, single-hop
    QA, dialogue understanding, and reading comprehension. (2) Our findings reveal
    that mixed memory consistently achieves balanced and competitive performance across
    diverse tasks. Chunks and summaries excel in tasks involving extensive and lengthy
    context (e.g., reading comprehension and dialogue understanding), while knowledge
    triples and atomic facts are particularly effective for relational reasoning and
    precision in multi-hop and single-hop QA. Additionally, mixed memory demonstrates
    remarkable resilience to noise. (3) Iterative retrieval stands out as the most
    effective memory retrieval method across most tasks, such as multi-hop QA, dialogue
    understanding and reading comprehension.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的主要贡献可以总结如下：(1) 我们提出了首个全面研究记忆结构和记忆检索方法在基于LLM的智能体中影响的工作，涵盖了六个数据集和四个任务：多跳QA、单跳QA、对话理解和阅读理解。(2)
    我们的研究发现，混合记忆在不同任务中始终能取得平衡且具有竞争力的表现。块状记忆和总结在涉及大量且长篇上下文的任务中表现优异（例如，阅读理解和对话理解），而知识三元组和原子事实特别有效于关系推理以及多跳和单跳QA中的精确性。此外，混合记忆展示了对噪声的显著鲁棒性。(3)
    迭代检索在大多数任务中表现为最有效的记忆检索方法，如多跳QA、对话理解和阅读理解。
- en: '![Refer to caption](img/d07216df0d1a14e3fecd903222015b96.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d07216df0d1a14e3fecd903222015b96.png)'
- en: 'Figure 2: Overview of the memory module workflow in LLM-based agents. Raw information
    is organized into structural memories, which are processed through retrieval methods
    to identify the most relevant memories for the query, enabling the generation
    of precise and contextually enriched responses.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：基于LLM的智能体中记忆模块工作流概述。原始信息被组织成结构化记忆，并通过检索方法处理，识别与查询最相关的记忆，从而生成精确且富有上下文的回答。
- en: 2 Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM-based Agents
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 基于LLM的智能体
- en: The advent of Large Language Model (LLM) has positioned them as a transformative
    step towards achieving Artificial General Intelligence (AGI) Wang et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib38)),
    offering robust capabilities for the development of LLM-based agents Xi et al.
    ([2023](https://arxiv.org/html/2412.15266v1#bib.bib41)); Xu et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib42)).
    Current research in this field primarily focuses on agent planning Wang et al.
    ([2023](https://arxiv.org/html/2412.15266v1#bib.bib37)); Yao et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib44));
    Qian et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib27)); Qiao et al.
    ([2024](https://arxiv.org/html/2412.15266v1#bib.bib28)), reflection mechanisms Shinn
    et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib34)); Zhang et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib45)),
    external tools utilization Qin et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib29));
    Wang et al. ([2024c](https://arxiv.org/html/2412.15266v1#bib.bib40)), self-evolving
    capabilities Zhang et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib45))
    and memory modules Hu et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12));
    Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLM）的出现标志着朝着实现人工通用智能（AGI）迈出了具有变革性的步伐 Wang 等人 ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib38))，为基于LLM的智能体的发展提供了强大的能力 Xi
    等人 ([2023](https://arxiv.org/html/2412.15266v1#bib.bib41)); Xu 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib42))。当前该领域的研究主要集中在智能体规划 Wang
    等人 ([2023](https://arxiv.org/html/2412.15266v1#bib.bib37)); Yao 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib44));
    Qian 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib27)); Qiao 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib28))，反思机制 Shinn
    等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib34)); Zhang 等人 ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib45))，外部工具的利用 Qin
    等人 ([2023](https://arxiv.org/html/2412.15266v1#bib.bib29)); Wang 等人 ([2024c](https://arxiv.org/html/2412.15266v1#bib.bib40))，自我演化能力 Zhang
    等人 ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib45)) 和记忆模块 Hu 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12));
    Lee 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15))。
- en: 2.2 Memory Structures
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 记忆结构
- en: Memory module serves as the foundation of LLM-based agents, enabling them to
    structure knowledge, retrieve relevant information, and leverage prior experiences
    for reasoning tasks Zhang et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib46)).
    Among the widely adopted memory structures of memory module are chunks Packer
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24)); Liu et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib20));
    Hu et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12)), knowledge triples Anokhin
    et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2)), atomic facts Li
    et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)), and summaries Lee
    et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15)). For instance,
    HiAgent (Hu et al., [2024](https://arxiv.org/html/2412.15266v1#bib.bib12)) incorporates
    sub-goals as memory chunks to maintain task continuity and coherence across interactions.
    On the other hand, GraphReader Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))
    employs atomic facts to compress chunks into finer details, providing agents with
    highly granular information that improves precision in multi-hop question answering
    tasks. In this paper, we investigate how various memory structures impact the
    performance of LLM-based agents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆模块作为基于大语言模型（LLM）智能体的基础，能够使其构建知识、检索相关信息，并利用先前的经验进行推理任务 Zhang 等人 ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib46))。在记忆模块的广泛采用的记忆结构中，包括信息块 Packer
    等人 ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24)); Liu 等人 ([2023](https://arxiv.org/html/2412.15266v1#bib.bib20));
    Hu 等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib12))，知识三元组 Anokhin 等人
    ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2))，原子事实 Li 等人 ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))，以及摘要 Lee
    等人 ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15))。例如，HiAgent (Hu 等人,
    [2024](https://arxiv.org/html/2412.15266v1#bib.bib12))将子目标作为记忆块，维持任务的连续性和交互过程中的一致性。另一方面，GraphReader Li
    等人 ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))采用原子事实将信息块压缩为更细的细节，为智能体提供更为精细的内容，从而在多跳问题回答任务中提高精确度。本文将研究不同记忆结构如何影响基于LLM的智能体的表现。
- en: 2.3 Memory Retrieval
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 记忆检索
- en: The memory retrieval method is another critical component of the memory module,
    enabling LLM-based agents to retrieve relevant memories to advanced reasoning.
    To facilitate this, LLM-based agents often employ retrieval-augmented generation (RAG) Lewis
    et al. ([2020](https://arxiv.org/html/2412.15266v1#bib.bib16)); Fang et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib7)),
    where relevant memories are first retrieved and then used to generate answers
    with LLMs. In this setting, the retrieved memories are prepended to the queries
    and serve as input to the LLM to generate response Ram et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib30)).
    The most straightforward retrieval method is the single-step retrieval Packer
    et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib24)); Zhong et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib47)),
    which aims to identify the Top-$K$ most relevant memories for the query. Additionally,
    reranking Gao et al. ([2023a](https://arxiv.org/html/2412.15266v1#bib.bib8));
    Ji et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib13)) leverages the
    language understanding capabilities of LLMs to prioritize retrieved memories,
    while iterative retrieval Li et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib18));
    Shi et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib33)) focuses on
    reformulating queries to improve retrieval accuracy. These innovations make memory
    retrieval more adaptive and consistent with the query, maintaining effective performance
    across diverse and complex tasks. In this paper, we explore how different combinations
    of retrieval methods and memory structures influence overall performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆检索方法是内存模块的另一个关键组成部分，使得基于LLM的代理能够检索相关记忆以进行高级推理。为了实现这一点，基于LLM的代理通常采用检索增强生成（RAG）方法，Lewis
    等人（[2020](https://arxiv.org/html/2412.15266v1#bib.bib16)）；Fang 等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib7)），首先检索相关记忆，然后用LLM生成答案。在这种设置中，检索到的记忆会被加到查询前面，作为LLM的输入来生成响应，Ram
    等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib30)）。最简单的检索方法是单步检索，Packer
    等人（[2023](https://arxiv.org/html/2412.15266v1#bib.bib24)）；Zhong 等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib47)），旨在识别与查询最相关的Top-$K$记忆。此外，重排序，Gao
    等人（[2023a](https://arxiv.org/html/2412.15266v1#bib.bib8)）；Ji 等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib13)）利用LLM的语言理解能力来优先排序检索到的记忆，而迭代检索，Li
    等人（[2024b](https://arxiv.org/html/2412.15266v1#bib.bib18)）；Shi 等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib33)）则专注于重新表述查询以提高检索准确性。这些创新使得记忆检索变得更加适应性强，并且与查询一致，在各种复杂任务中保持有效的表现。本文探讨了不同的检索方法和记忆结构组合如何影响整体性能。
- en: 3 Methodology
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Figure [2](https://arxiv.org/html/2412.15266v1#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ On the Structural Memory of LLM Agents") illustrates the overview of the memory
    module within LLM-based agents, highlighting three key components: Structural
    Memory Generation, Memory Retrieval Methods and Answer Generation. This section
    begins with an introduction to structural memory generation in $\S$ [3.1](https://arxiv.org/html/2412.15266v1#S3.SS1
    "3.1 Structural Memory Generation ‣ 3 Methodology ‣ On the Structural Memory of
    LLM Agents"). Next, we introduce memory retrieval methods in $\S$ [3.2](https://arxiv.org/html/2412.15266v1#S3.SS2
    "3.2 Memory Retrieval Methods ‣ 3 Methodology ‣ On the Structural Memory of LLM
    Agents"). Finally, $\S$ [3.3](https://arxiv.org/html/2412.15266v1#S3.SS3 "3.3
    Answer Generation ‣ 3 Methodology ‣ On the Structural Memory of LLM Agents") discusses
    answer generation methods.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](https://arxiv.org/html/2412.15266v1#S1.F2 "图 2 ‣ 1 引言 ‣ 关于LLM代理的结构记忆")展示了基于LLM的代理内存模块的概述，重点介绍了三个关键组成部分：结构记忆生成、记忆检索方法和答案生成。本节首先介绍$\S$ [3.1](https://arxiv.org/html/2412.15266v1#S3.SS1
    "3.1 结构记忆生成 ‣ 3 方法论 ‣ 关于LLM代理的结构记忆")中的结构记忆生成。接下来，我们将在$\S$ [3.2](https://arxiv.org/html/2412.15266v1#S3.SS2
    "3.2 记忆检索方法 ‣ 3 方法论 ‣ 关于LLM代理的结构记忆")中介绍记忆检索方法。最后，$\S$ [3.3](https://arxiv.org/html/2412.15266v1#S3.SS3
    "3.3 答案生成 ‣ 3 方法论 ‣ 关于LLM代理的结构记忆")讨论答案生成方法。
- en: 3.1 Structural Memory Generation
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 结构记忆生成
- en: 'Structural memory generation enables agents to organize raw documents into
    structured representations. By transforming unstructured documents $\mathcal{D}_{q}$
    into structural memory $\mathcal{M}_{q}$, the agent gains the ability to store,
    retrieve, and reason over information more effectively. In this work, we explore
    four distinct forms of structural memory: chunks $\mathcal{C}_{q}$, knowledge
    triples $\mathcal{T}_{q}$, atomic facts $\mathcal{A}_{q}$, or summaries $\mathcal{S}_{q}$.
    The generation process for each structural memory is detailed as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化记忆生成使得智能体能够将原始文档组织成结构化的表示形式。通过将非结构化文档$\mathcal{D}_{q}$转化为结构化记忆$\mathcal{M}_{q}$，智能体能够更有效地存储、检索和推理信息。在这项工作中，我们探索了四种不同形式的结构化记忆：块$\mathcal{C}_{q}$、知识三元组$\mathcal{T}_{q}$、原子事实$\mathcal{A}_{q}$或摘要$\mathcal{S}_{q}$。每种结构化记忆的生成过程如下所述：
- en: 'Chunks ($\mathcal{C}_{q}$). Chunks Gao et al. ([2023b](https://arxiv.org/html/2412.15266v1#bib.bib9))
    are a widely used form of structural memory in LLM-based agents. Each chunk represents
    a continuous segment of text from a document, typically constrained to a fixed
    number of tokens $L$. Formally, raw documents $\mathcal{D}_{q}$ can be divided
    into a series of chunks, as defined: $\mathcal{C}_{q}(\mathcal{D}_{q})=\{c_{1},c_{2},\dots,c_{j}\}$,
    where each chunk $c_{j}$ contains at most $L$ tokens.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 块（$\mathcal{C}_{q}$）。块（Gao等人，[2023b](https://arxiv.org/html/2412.15266v1#bib.bib9)）是LLM基础的智能体中广泛使用的结构化记忆形式。每个块代表文档中的一个连续文本段，通常限制为固定数量的标记$L$。形式上，原始文档$\mathcal{D}_{q}$可以被划分为一系列块，定义为：$\mathcal{C}_{q}(\mathcal{D}_{q})=\{c_{1},c_{2},\dots,c_{j}\}$，其中每个块$c_{j}$包含最多$L$个标记。
- en: '<svg class="ltx_picture" height="104.68" id="S3.SS1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,104.68) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 89.17)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Chunks</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="57.67" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Chunks are continuous, fixed-length segments
    of text from the document. Example: Generated chunks $\mathcal{C}_{q}$: (1) Moneybomb
    (alternatively money bomb, money-bomb, or fundraising bomb) is a neologism coined
    in 2007; (2) to describe a grassroots fundraising effort over a brief fixed time
    period.</foreignobject></g></g></svg>'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="104.68" id="S3.SS1.p3.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,104.68) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 89.17)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Chunks</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="57.67" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Chunks are continuous, fixed-length segments
    of text from the document. Example: Generated chunks $\mathcal{C}_{q}$: (1) Moneybomb
    (alternatively money bomb, money-bomb, or fundraising bomb) is a neologism coined
    in 2007; (2) to describe a grassroots fundraising effort over a brief fixed time
    period.</foreignobject></g></g></svg>'
- en: 'Knowledge Triples ($\mathcal{T}_{q}$). Knowledge triples represent a structured
    form of memory that captures semantic relationships between entities. Each triple
    is composed of three components: a head entity, a relation, and a tail entity,
    represented in the format $\langle\text{{head}};\text{{relation}};\text{{tail
    entity}}\rangle$. Following previous works Anokhin et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib2));
    Fang et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib7)), raw documents
    $\mathcal{D}_{q}$ are processed by an LLM guided by a tailored prompt $\mathcal{P}_{\mathcal{T}}$
    to generate a set of semantic triples $\mathcal{T}_{q}$. The generation process
    can be formally defined as: $\mathcal{T}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{T}})$.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 知识三元组（$\mathcal{T}_{q}$）。知识三元组代表了捕捉实体之间语义关系的结构化记忆形式。每个三元组由三部分组成：头实体、关系和尾实体，表示为格式$\langle\text{{head}};\text{{relation}};\text{{tail
    entity}}\rangle$。根据以往的研究，Anokhin等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib2)）；Fang等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib7)）的方法，原始文档$\mathcal{D}_{q}$通过LLM在定制化提示$\mathcal{P}_{\mathcal{T}}$的指导下处理，生成一组语义三元组$\mathcal{T}_{q}$。生成过程可以形式化表示为：$\mathcal{T}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{T}})$。
- en: '<svg class="ltx_picture" height="96.75" id="S3.SS1.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,96.75) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 78.54)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Knowledge Triples</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Knowledge triples capture relationships between
    entities. Example: Generated triples $\mathcal{T}_{q}$: (1) $\langle$Moneybomb;
    type; neologism$\rangle$; (2) $\langle$Moneybomb; coined in; 2007$\rangle$.</foreignobject></g></g></svg>'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="96.75" id="S3.SS1.p5.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,96.75) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 78.54)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Knowledge Triples</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Knowledge triples capture relationships between
    entities. Example: Generated triples $\mathcal{T}_{q}$: (1) $\langle$Moneybomb;
    type; neologism$\rangle$; (2) $\langle$Moneybomb; coined in; 2007$\rangle$.</foreignobject></g></g></svg>'
- en: 'Atomic Facts ($\mathcal{A}_{q}$). Atomic facts are the smallest, indivisible
    units of information, presented as concise sentences that capture essential details.
    They represent a granular form of structural memory, simplifying raw documents
    by preserving critical entities, actions, and attributes. Following Li et al.
    ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)), atomic facts are generated
    from raw documents $\mathcal{D}_{q}$ using an LLM guided by a tailored prompt
    $\mathcal{P}_{\mathcal{A}}$, formally denoted as: $\mathcal{A}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{A}})$.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 原子事实（$\mathcal{A}_{q}$）。原子事实是最小的、不可分割的信息单元，通常以简洁的句子形式呈现，捕捉关键信息。它们代表了结构化记忆的一个颗粒化形式，通过保留关键的实体、动作和属性来简化原始文档。根据Li等人（[2024a](https://arxiv.org/html/2412.15266v1#bib.bib17)）的方法，原子事实是通过LLM在定制化提示$\mathcal{P}_{\mathcal{A}}$的指导下，从原始文档$\mathcal{D}_{q}$生成的，形式化表示为：$\mathcal{A}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{A}})$。
- en: '<svg class="ltx_picture" height="93.9" id="S3.SS1.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,93.9) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 78.54)"><foreignobject color="#FFFFFF" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Atomic Facts</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Atomic facts are the smallest units of indivisible
    information. Example: Generated atomic facts $\mathcal{A}_{q}$: (1) Moneybomb
    is also known as money bomb, money-bomb, or fundraising bomb; (2) Moneybomb is
    a neologism.</foreignobject></g></g></svg>'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="93.9" id="S3.SS1.p7.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,93.9) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 78.54)"><foreignobject color="#FFFFFF" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Atomic Facts</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="47.05" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Atomic facts are the smallest units of indivisible
    information. Example: Generated atomic facts $\mathcal{A}_{q}$: (1) Moneybomb
    is also known as money bomb, money-bomb, or fundraising bomb; (2) Moneybomb is
    a neologism.</foreignobject></g></g></svg>'
- en: 'Summaries ($\mathcal{S}_{q}$). Summaries provide a condensed and comprehensive
    description of documents, capturing both global content and key details. Following Lee
    et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15)), summaries are
    generated from raw documents $\mathcal{D}_{q}$ using an LLM guided by a tailored
    prompt $\mathcal{P}_{\mathcal{S}}$, defined as: $\mathcal{S}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{S}})$.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要（$\mathcal{S}_{q}$）。摘要提供了文档的精简和全面的描述，捕捉了文档的全局内容和关键信息。根据Lee等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib15)）的方法，摘要是通过LLM在定制化提示$\mathcal{P}_{\mathcal{S}}$的指导下，从原始文档$\mathcal{D}_{q}$生成的，定义为：$\mathcal{S}_{q}=\texttt{LLM}(\mathcal{D}_{q},\mathcal{P}_{\mathcal{S}})$。
- en: '<svg class="ltx_picture" height="108.97" id="S3.SS1.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,108.97) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summaries</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Summaries compress the document into a comprehensive
    description. Example: Generated summaries $\mathcal{S}_{q}$: Moneybomb, alternatively
    referred to as money bomb, money-bomb, or fundraising bomb, is a neologism coined
    in 2007\. It describes a grassroots fundraising effort that occurs over a brief
    fixed time period.</foreignobject></g></g></svg>'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg class="ltx_picture" height="108.97" id="S3.SS1.p9.pic1" overflow="visible"
    version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt"
    transform="translate(0,108.97) matrix(1 0 0 -1 0 0)"><g fill-opacity="1.0" transform="matrix(1.0
    0.0 0.0 1.0 21.65 93.61)"><foreignobject color="#FFFFFF" height="9.46" overflow="visible"
    transform="matrix(1 0 0 -1 0 16.6)" width="556.69">Summaries</foreignobject></g>
    <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject
    color="#000000" height="62.11" overflow="visible" transform="matrix(1 0 0 -1 0
    16.6)" width="556.69">Definition: Summaries compress the document into a comprehensive
    description. Example: Generated summaries $\mathcal{S}_{q}$: Moneybomb, alternatively
    referred to as money bomb, money-bomb, or fundraising bomb, is a neologism coined
    in 2007\. It describes a grassroots fundraising effort that occurs over a brief
    fixed time period.</foreignobject></g></g></svg>'
- en: 'Mixed ($\mathcal{M}_{q}^{\text{Mixed}}$). Mixed memories represent a composite
    form of structural memory, combining all the aforementioned types: chunks, knowledge
    triples, atomic facts, and summaries. This integration provides a comprehensive
    representation, formally defined as follows: $\mathcal{M}_{q}^{\text{Mixed}}=\mathcal{C}_{q}\cup\mathcal{T}_{q}\cup\mathcal{%
    A}_{q}\cup\mathcal{S}_{q}$.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 混合记忆（$\mathcal{M}_{q}^{\text{Mixed}}$）。混合记忆代表了一种结构化记忆的复合形式，结合了所有上述类型：块、知识三元组、原子事实和摘要。该整合提供了一个全面的表示，正式定义如下：$\mathcal{M}_{q}^{\text{Mixed}}=\mathcal{C}_{q}\cup\mathcal{T}_{q}\cup\mathcal{A}_{q}\cup\mathcal{S}_{q}$。
- en: Details of the prompts used by the LLM for generating each type of structural
    memory, e.g., $\mathcal{P}_{\mathcal{T}}$, $\mathcal{P}_{\mathcal{A}}$ and $\mathcal{P}_{\mathcal{S}}$,
    are provided in Appendix [B](https://arxiv.org/html/2412.15266v1#A2 "Appendix
    B Prompts ‣ On the Structural Memory of LLM Agents").
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成每种类型结构化记忆的LLM提示的详细信息，例如$\mathcal{P}_{\mathcal{T}}$，$\mathcal{P}_{\mathcal{A}}$和$\mathcal{P}_{\mathcal{S}}$，详见附录[B](https://arxiv.org/html/2412.15266v1#A2
    "Appendix B Prompts ‣ On the Structural Memory of LLM Agents")。
- en: 3.2 Memory Retrieval Methods
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 记忆检索方法
- en: 'Given the generated structural memories $\mathcal{M}_{q}$, we employ a memory
    retrieval method to identify and integrate the most relevant supporting memories
    $\mathcal{M}_{r}\subset\mathcal{M}_{q}$ for the query $q$. Without this step,
    the agent would need to process all available memories, leading to inefficiency
    and potential inaccuracies due to irrelevant information. Our study mainly focuses
    on three retrieval approaches: single-step retrieval Robertson et al. ([2009](https://arxiv.org/html/2412.15266v1#bib.bib31));
    Rubin et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib32)), reranking Gao
    et al. ([2023a](https://arxiv.org/html/2412.15266v1#bib.bib8)); Ji et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib13)),
    and iterative retrieval Li et al. ([2024b](https://arxiv.org/html/2412.15266v1#bib.bib18));
    Shi et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib33)). The details
    of each memory retrieval method are outlined as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 给定生成的结构化记忆$\mathcal{M}_{q}$，我们采用一种记忆检索方法来识别和整合与查询$q$最相关的支持记忆$\mathcal{M}_{r}\subset\mathcal{M}_{q}$。如果没有这一步骤，代理将需要处理所有可用记忆，这将导致低效并可能由于无关信息产生不准确性。我们的研究主要聚焦于三种检索方法：单步检索 Robertson等人（[2009](https://arxiv.org/html/2412.15266v1#bib.bib31)）；Rubin等人（[2022](https://arxiv.org/html/2412.15266v1#bib.bib32)），重新排序 Gao等人（[2023a](https://arxiv.org/html/2412.15266v1#bib.bib8)）；Ji等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib13)），以及迭代检索 Li等人（[2024b](https://arxiv.org/html/2412.15266v1#bib.bib18)）；Shi等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib33)）。每种记忆检索方法的详细信息如下：
- en: 'Single-step Retrieval. In the single-step retrieval process, the goal is to
    identify the Top-$K$ memories $\mathcal{M}_{r}$ that are most relevant to the
    query $q$. This process is formally defined as: $\mathcal{M}_{r}=\texttt{Retriever}(q,\mathcal{M}_{q},K)$,
    where the Retriever Robertson et al. ([2009](https://arxiv.org/html/2412.15266v1#bib.bib31));
    Rubin et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib32)) serves as
    the core component.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 单步检索。在单步检索过程中，目标是识别与查询$q$最相关的Top-$K$记忆$\mathcal{M}_{r}$。该过程的正式定义为：$\mathcal{M}_{r}=\texttt{Retriever}(q,\mathcal{M}_{q},K)$，其中检索器Retriever由Robertson等人（[2009](https://arxiv.org/html/2412.15266v1#bib.bib31)）；Rubin等人（[2022](https://arxiv.org/html/2412.15266v1#bib.bib32)）作为核心组件。
- en: 'Reranking. In the reranking process Gao et al. ([2023a](https://arxiv.org/html/2412.15266v1#bib.bib8));
    Dong et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib6)), an initial
    retriever selects a candidate set of Top-$K$ memories $\mathcal{M}_{i}$, which
    are then reranked by an LLM prompted with $\mathcal{P}_{\text{Rerank}}$ based
    on their relevance scores. From this reranked list, the Top-$R$ memories $\mathcal{M}_{r}$,
    selected in descending order of relevance scores, are identified as the most relevant.
    This step enhances retrieval precision by leveraging the LLM to strengthen query-memory
    connections, filtering out irrelevant memories, and prioritizing the most pertinent
    memories for the query. This process is formally defined as: $\mathcal{M}_{r}=\texttt{LLM}(q,\mathcal{M}_{i},R,\mathcal{P}_{R})\,$,
    where $\mathcal{M}_{i}=\texttt{Retriever}(q,\mathcal{M}_{q},K)$.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重新排序。在重新排序过程中，Gao 等人（[2023a](https://arxiv.org/html/2412.15266v1#bib.bib8)）；Dong
    等人（[2024](https://arxiv.org/html/2412.15266v1#bib.bib6)）采用初始检索器选择候选的前 $K$ 个记忆
    $\mathcal{M}_{i}$，然后通过 LLM 和提示 $\mathcal{P}_{\text{Rerank}}$ 根据它们的相关性评分进行重新排序。从重新排序后的列表中，按相关性评分降序选择前
    $R$ 个记忆 $\mathcal{M}_{r}$，被确定为最相关的记忆。此步骤通过利用 LLM 强化查询与记忆之间的联系，过滤掉无关记忆，并优先考虑与查询最相关的记忆，从而提高检索精度。这个过程的正式定义为：$\mathcal{M}_{r}=\texttt{LLM}(q,\mathcal{M}_{i},R,\mathcal{P}_{R})\,$，其中
    $\mathcal{M}_{i}=\texttt{Retriever}(q,\mathcal{M}_{q},K)$。
- en: 'Iterative Retrieval. The iterative retrieval approach Gao et al. ([2023b](https://arxiv.org/html/2412.15266v1#bib.bib9))
    begins with an initial query $q_{0}=q$ and retrieves the Top-$T$ most relevant
    structural memories $\mathcal{M}_{j}$. These retrieved memories are used to refine
    the query through an LLM prompted by $\mathcal{P}_{\text{Refine}}$. This process
    is repeated over $N$ iterations, refining the query to produce the final version
    $q_{N}$ that is informative for retrieving relevant memories. Formally, the iterative
    retrieval process can be defined as follows: $q_{j}=\texttt{LLM}(\mathcal{M}_{j},\mathcal{P}_{\text{Refine}})$,
    where $\mathcal{M}_{j}=\texttt{Retriever}(q_{j-1},\mathcal{M}_{q},T)$. After $N$
    iterations, the final refined query $q_{N}$ is used to retrieve the Top-$K$ most
    relevant memories for answer generation. This step can be expressed as: $\mathcal{M}_{r}=\texttt{Retriever}(q_{N},\mathcal{M}_{q},K)$.
    The detailed prompts $\mathcal{P}_{\text{Rerank}}$ and $\mathcal{P}_{\text{Refine}}$
    can be found in Appendix [B](https://arxiv.org/html/2412.15266v1#A2 "Appendix
    B Prompts ‣ On the Structural Memory of LLM Agents").'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代检索。Gao 等人（[2023b](https://arxiv.org/html/2412.15266v1#bib.bib9)）提出的迭代检索方法从初始查询
    $q_{0}=q$ 开始，检索出前 $T$ 个最相关的结构化记忆 $\mathcal{M}_{j}$。这些检索到的记忆通过由 $\mathcal{P}_{\text{Refine}}$
    提示的 LLM 用于优化查询。这个过程会在 $N$ 次迭代中反复进行，不断细化查询，最终产生用于检索相关记忆的最终版本 $q_{N}$。形式上，迭代检索过程可以定义为：$q_{j}=\texttt{LLM}(\mathcal{M}_{j},\mathcal{P}_{\text{Refine}})$，其中
    $\mathcal{M}_{j}=\texttt{Retriever}(q_{j-1},\mathcal{M}_{q},T)$。经过 $N$ 次迭代后，最终优化的查询
    $q_{N}$ 被用来检索生成答案所需的前 $K$ 个最相关的记忆。这一步可以表示为：$\mathcal{M}_{r}=\texttt{Retriever}(q_{N},\mathcal{M}_{q},K)$。详细的提示
    $\mathcal{P}_{\text{Rerank}}$ 和 $\mathcal{P}_{\text{Refine}}$ 可在附录 [B](https://arxiv.org/html/2412.15266v1#A2
    "Appendix B Prompts ‣ On the Structural Memory of LLM Agents") 中找到。
- en: 3.3 Answer Generation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 答案生成
- en: Finally, the agent leverages the LLM to generate the answer based on the retrieved
    memory. To achieve this, we propose two methods of answer generation. In the first
    method, termed Memory-Only, the retrieved memories $\mathcal{M}_{r}$ are directly
    utilized as the context for generating the answer. The second method, termed Memory-Doc,
    uses the retrieved memories to locate their corresponding original documents from
    $\mathcal{D}_{q}$. These documents then serve as the context for answer generation,
    providing the agent with more detailed and contextually enriched information.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，智能体利用 LLM 根据检索到的记忆生成答案。为此，我们提出了两种答案生成方法。在第一种方法（称为 Memory-Only）中，直接使用检索到的记忆
    $\mathcal{M}_{r}$ 作为生成答案的上下文。第二种方法（称为 Memory-Doc）使用检索到的记忆来定位它们在 $\mathcal{D}_{q}$
    中对应的原始文档。这些文档随后作为生成答案的上下文，为智能体提供更详细、更多语境信息。 '
- en: '| Memory Structure | HotPotQA | 2WikiMultihopQA | MuSiQue | NarrativeQA | LoCoMo
    | QuALITY |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 记忆结构 | HotPotQA | 2WikiMultihopQA | MuSiQue | NarrativeQA | LoCoMo | QuALITY
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| EM | F1 | EM | F1 | EM | F1 | EM | F1 | EM | F1 | ACC |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| EM | F1 | EM | F1 | EM | F1 | EM | F1 | EM | F1 | ACC |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Full Content | 55.50 | 75.77 | 44.00 | 54.33 | 36.00 | 51.60 | 7.00 | 24.99
    | 13.61 | 41.82 | 81.50 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 完整内容 | 55.50 | 75.77 | 44.00 | 54.33 | 36.00 | 51.60 | 7.00 | 24.99 | 13.61
    | 41.82 | 81.50 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Single-step Retrieval |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 单步检索 |'
- en: '| --- |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Chunks | 61.50 | 76.93 | 43.50 | 59.17 | 35.50 | 54.45 | 13.50 | 29.78 |
    9.95 | 40.63 | 76.00 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 61.50 | 76.93 | 43.50 | 59.17 | 35.50 | 54.45 | 13.50 | 29.78 | 9.95
    | 40.63 | 76.00 |'
- en: '| Triples | 59.50 | 74.09 | 44.50 | 60.82 | 31.00 | 50.13 | 11.50 | 22.04 |
    8.42 | 41.08 | 61.50 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 三元组 | 59.50 | 74.09 | 44.50 | 60.82 | 31.00 | 50.13 | 11.50 | 22.04 | 8.42
    | 41.08 | 61.50 |'
- en: '| Atomic Facts | 62.50 | 77.22 | 39.50 | 58.63 | 30.50 | 51.31 | 13.50 | 27.49
    | 9.42 | 42.92 | 71.50 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 原子事实 | 62.50 | 77.22 | 39.50 | 58.63 | 30.50 | 51.31 | 13.50 | 27.49 | 9.42
    | 42.92 | 71.50 |'
- en: '| Summaries | 57.00 | 74.81 | 42.00 | 57.21 | 34.00 | 52.83 | 16.50 | 32.93
    | 10.99 | 44.94 | 76.00 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | 57.00 | 74.81 | 42.00 | 57.21 | 34.00 | 52.83 | 16.50 | 32.93 | 10.99
    | 44.94 | 76.00 |'
- en: '| Mixed | 60.00 | 77.10 | 48.50 | 65.25 | 33.00 | 51.65 | 14.50 | 29.86 | 10.47
    | 44.73 | 78.00 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 混合 | 60.00 | 77.10 | 48.50 | 65.25 | 33.00 | 51.65 | 14.50 | 29.86 | 10.47
    | 44.73 | 78.00 |'
- en: '| Reranking |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 重新排序 |'
- en: '| Chunks | 63.00 | 77.35 | 45.00 | 61.31 | 37.00 | 55.32 | 16.00 | 31.63 |
    9.95 | 43.47 | 78.50 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 63.00 | 77.35 | 45.00 | 61.31 | 37.00 | 55.32 | 16.00 | 31.63 | 9.95
    | 43.47 | 78.50 |'
- en: '| Triples | 61.00 | 76.75 | 43.50 | 55.43 | 26.50 | 42.05 | 10.00 | 20.65 |
    8.83 | 41.82 | 60.00 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 三元组 | 61.00 | 76.75 | 43.50 | 55.43 | 26.50 | 42.05 | 10.00 | 20.65 | 8.83
    | 41.82 | 60.00 |'
- en: '| Atomic Facts | 63.00 | 78.31 | 40.50 | 59.31 | 28.50 | 49.95 | 14.00 | 28.19
    | 8.90 | 44.27 | 67.50 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 原子事实 | 63.00 | 78.31 | 40.50 | 59.31 | 28.50 | 49.95 | 14.00 | 28.19 | 8.90
    | 44.27 | 67.50 |'
- en: '| Summaries | 61.00 | 77.80 | 45.00 | 61.18 | 35.50 | 54.59 | 16.00 | 32.26
    | 12.04 | 44.83 | 75.00 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | 61.00 | 77.80 | 45.00 | 61.18 | 35.50 | 54.59 | 16.00 | 32.26 | 12.04
    | 44.83 | 75.00 |'
- en: '| Mixed | 65.00 | 78.58 | 45.50 | 61.77 | 34.00 | 52.45 | 11.98 | 28.02 | 9.42
    | 44.51 | 77.50 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 混合 | 65.00 | 78.58 | 45.50 | 61.77 | 34.00 | 52.45 | 11.98 | 28.02 | 9.42
    | 44.51 | 77.50 |'
- en: '| Iterative Retrieval |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 迭代检索 |'
- en: '| Chunks | 63.00 | 79.10 | 46.50 | 62.13 | 37.00 | 56.78 | 14.50 | 30.88 |
    10.47 | 45.14 | 77.00 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 块 | 63.00 | 79.10 | 46.50 | 62.13 | 37.00 | 56.78 | 14.50 | 30.88 | 10.47
    | 45.14 | 77.00 |'
- en: '| Triples | 64.00 | 78.78 | 47.50 | 62.06 | 38.00 | 55.93 | 10.50 | 21.67 |
    9.47 | 41.41 | 60.50 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 三元组 | 64.00 | 78.78 | 47.50 | 62.06 | 38.00 | 55.93 | 10.50 | 21.67 | 9.47
    | 41.41 | 60.50 |'
- en: '| Atomic Facts | 65.50 | 81.29 | 44.00 | 63.89 | 34.50 | 57.55 | 14.50 | 28.28
    | 9.95 | 43.62 | 67.50 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 原子事实 | 65.50 | 81.29 | 44.00 | 63.89 | 34.50 | 57.55 | 14.50 | 28.28 | 9.95
    | 43.62 | 67.50 |'
- en: '| Summaries | 60.50 | 78.11 | 46.50 | 62.35 | 33.50 | 53.12 | 17.00 | 31.79
    | 12.04 | 43.93 | 75.00 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 摘要 | 60.50 | 78.11 | 46.50 | 62.35 | 33.50 | 53.12 | 17.00 | 31.79 | 12.04
    | 43.93 | 75.00 |'
- en: '| Mixed | 67.00 | 82.11 | 51.00 | 68.15 | 39.00 | 61.38 | 12.50 | 28.36 | 7.85
    | 45.25 | 79.50 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 混合 | 67.00 | 82.11 | 51.00 | 68.15 | 39.00 | 61.38 | 12.50 | 28.36 | 7.85
    | 45.25 | 79.50 |'
- en: 'Table 1: Overall Performance (%) of various memory structures utilizing different
    retrieval methods across six datasets. The best performance is marked in boldface,
    while the second-best performance is underlined.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同检索方法在六个数据集上的各类记忆结构的整体表现（%）。最佳表现以粗体标出，第二好表现下划线标记。
- en: 4 Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets.
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集。
- en: We conduct experiments on six datasets across four tasks. For multi-hop long-context
    QA datasets, we experiment with HotPotQA Yang et al. ([2018](https://arxiv.org/html/2412.15266v1#bib.bib43)),
    2WikiMultihopQA Ho et al. ([2020](https://arxiv.org/html/2412.15266v1#bib.bib10)),
    and MuSiQue Trivedi et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib36)).
    The single-hop long-context QA task is evaluated with NarrativeQA Kočiskỳ et al.
    ([2018](https://arxiv.org/html/2412.15266v1#bib.bib14)) from Longbench Bai et al.
    ([2023](https://arxiv.org/html/2412.15266v1#bib.bib3)). Additionally, we leverage
    the LoCoMo dataset Maharana et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib21))
    for dialogue-based long-context QA task, while the QuALITY Pang et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib25))
    dataset is used for the reading comprehension QA task²²2More details and statistics
    about the datasets are provided in Appendix [A](https://arxiv.org/html/2412.15266v1#A1
    "Appendix A Datasets ‣ On the Structural Memory of LLM Agents")..
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在四个任务上对六个数据集进行了实验。对于多跳长文本问答数据集，我们在HotPotQA Yang et al. ([2018](https://arxiv.org/html/2412.15266v1#bib.bib43))、2WikiMultihopQA
    Ho et al. ([2020](https://arxiv.org/html/2412.15266v1#bib.bib10)) 和 MuSiQue Trivedi
    et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib36))上进行了实验。单跳长文本问答任务使用NarrativeQA
    Kočiskỳ et al. ([2018](https://arxiv.org/html/2412.15266v1#bib.bib14))，数据来自Longbench
    Bai et al. ([2023](https://arxiv.org/html/2412.15266v1#bib.bib3))。此外，我们还利用了LoCoMo数据集
    Maharana et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib21)) 来进行基于对话的长文本问答任务，而QuALITY
    Pang et al. ([2022](https://arxiv.org/html/2412.15266v1#bib.bib25))数据集则用于阅读理解问答任务²²2有关数据集的更多细节和统计数据请见附录 [A](https://arxiv.org/html/2412.15266v1#A1
    "Appendix A Datasets ‣ On the Structural Memory of LLM Agents")..
- en: 4.2 Evaluation.
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估。
- en: To evaluate QA performance, we follow previous work Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))
    and use standard metrics such as Exact Match (EM) score and F1 score for the datasets
    HotPotQA, 2WikiMultihopQA, MuSiQue, NarrativeQA and LoCoMo. For QuALITY, we follow
    the approach in Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15))
    and use accuracy as the evaluation metric, with $25\%$ indicating chance performance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估问答性能，我们遵循了之前的工作Li et al. ([2024a](https://arxiv.org/html/2412.15266v1#bib.bib17))，使用了标准的度量指标，如精确匹配（EM）得分和F1得分，适用于HotPotQA、2WikiMultihopQA、MuSiQue、NarrativeQA
    和 LoCoMo数据集。对于QuALITY数据集，我们参考了Lee et al. ([2024](https://arxiv.org/html/2412.15266v1#bib.bib15))的方法，使用准确率作为评估指标，其中$25\%$代表随机表现。
- en: 4.3 Implementation Details.
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实现细节。
- en: In our experiments, we use GPT-4o-mini-128k with a temperature setting of 0.2\.
    The input window is set to $4k$ tokens, while the maximum chunk size is up to
    $1k$ tokens. For text embedding, we employ the text-embedding-3-small model ³³3[https://platform.openai.com/docs/guides/embeddings/](https://platform.openai.com/docs/guides/embeddings/)
    from OpenAI and store the vectorized memories using LangChain Chase ([2022](https://arxiv.org/html/2412.15266v1#bib.bib4)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们使用了GPT-4o-mini-128k，并设置温度为0.2。输入窗口设置为$4k$个tokens，最大块大小为$1k$个tokens。对于文本嵌入，我们使用了来自OpenAI的text-embedding-3-small模型³³3[https://platform.openai.com/docs/guides/embeddings/](https://platform.openai.com/docs/guides/embeddings/)，并使用LangChain
    Chase ([2022](https://arxiv.org/html/2412.15266v1#bib.bib4))存储向量化的记忆。
- en: 5 Results and Analysis
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果与分析
- en: 5.1 Impact of Memory Structures
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 记忆结构的影响
- en: '![Refer to caption](img/8ffa419458d289c30f546fd81fa16838.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![请参见标题说明](img/8ffa419458d289c30f546fd81fa16838.png)'
- en: 'Figure 3: Performance across six datasets using two answer generation approaches:
    Memory-Only and Memory-Doc.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用两种答案生成方法（仅记忆和记忆-文档）在六个数据集上的表现。
- en: 'Finding 1: Mixed memories delivers more balanced performance. The results as
    presented in Table [1](https://arxiv.org/html/2412.15266v1#S3.T1 "Table 1 ‣ 3.3
    Answer Generation ‣ 3 Methodology ‣ On the Structural Memory of LLM Agents") reveal
    key insights into the impact of various memory structures on task performance:
    (1) Mixed memories consistently outperform other memory structures. This is particularly
    evident under iterative retrieval, where mixed memories achieve the highest F1
    scores of 82.11% on HotPotQA and 68.15% on 2WikiMultihopQA. (2) Chunks excel in
    tasks requiring a balance between concise and comprehensive contexts, as shown
    in datasets with long contexts. This is evidenced by its F1 score of 31.63% on
    NarrativeQA and an accuracy of 78.5% on QuALITY under reranking. Summaries, which
    condense large contexts, is effective for tasks demanding abstraction, as shown
    by its competitive F1 score of 32.26% on NarrativeQA and solid performance on
    LoCoMo. (3) Knowledge triples and atomic facts are particularly effective for
    relational reasoning and precision. Knowledge triples achieve an F1 score of 62.06%
    on 2WikiMultihopQA under iterative retrieval, while atomic facts achieve an F1
    score of 81.29% on HotPotQA. These findings emphasize the importance of tailoring
    memory structures to specific task requirements and demonstrate that integrating
    complementary memory types in mixed memories significantly enhances performance
    across tasks.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 1：混合记忆提供更平衡的性能。表格[1](https://arxiv.org/html/2412.15266v1#S3.T1 "Table 1 ‣
    3.3 Answer Generation ‣ 3 Methodology ‣ On the Structural Memory of LLM Agents")中呈现的结果揭示了不同记忆结构对任务性能的影响的关键见解：（1）混合记忆在性能上始终优于其他记忆结构，尤其在迭代检索下，混合记忆在HotPotQA任务上的F1分数为82.11%，在2WikiMultihopQA任务上的F1分数为68.15%，表现出色。（2）对于需要平衡简洁与全面上下文的任务，块状记忆表现优异，这在包含长上下文的数据集中得到了体现。例如，在NarrativeQA上的F1分数为31.63%，在QuALITY任务中的准确率为78.5%（重新排序）。总结性记忆通过压缩大量上下文，对于需要抽象化的任务非常有效，在NarrativeQA上的F1分数为32.26%，在LoCoMo任务中表现稳健。（3）知识三元组和原子事实特别适合关系推理和精准度要求。知识三元组在迭代检索下在2WikiMultihopQA上的F1分数为62.06%，原子事实在HotPotQA任务中的F1分数为81.29%。这些发现强调了根据任务要求定制记忆结构的重要性，并表明，结合互补记忆类型的混合记忆在各项任务中的性能显著提升。
- en: 5.2 Impact of Memory Retrieval Methods
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 记忆检索方法的影响
- en: 'Finding 2: Iterative retrieval as the optimal retrieval method. The results
    in Table [1](https://arxiv.org/html/2412.15266v1#S3.T1 "Table 1 ‣ 3.3 Answer Generation
    ‣ 3 Methodology ‣ On the Structural Memory of LLM Agents") demonstrate the significant
    influence of the retrieval method on performance: (1) Iterative retrieval consistently
    outperforms the others, achieving the highest scores across most datasets. Notably,
    with mixed memories, iterative retrieval achieved an F1 score of 82.11% on HotPotQA
    and 68.15% on 2WikiMultihopQA, showcasing its ability to refine queries iteratively
    for enhanced accuracy. (2) Reranking demonstrates strong performance on datasets
    with moderate complexity. For instance, it achieved F1 scores of 44.27% on LoCoMo
    and 28.19% on NarrativeQA with atomic fact memory. (3) In contrast, single-step
    retrieval performs competitively in tasks requiring minimal contextual integration.
    Using summary memory, it achieved an F1 score of 32.93% on NarrativeQA, leveraging
    abstraction to extract coherent information. These findings emphasize the importance
    of aligning retrieval mechanisms with task requirements, and iterative retrieval
    excels in reasoning tasks.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 2：迭代检索是最优的检索方法。表格[1](https://arxiv.org/html/2412.15266v1#S3.T1 "Table 1 ‣
    3.3 Answer Generation ‣ 3 Methodology ‣ On the Structural Memory of LLM Agents")中的结果展示了检索方法对性能的显著影响：（1）迭代检索始终优于其他方法，在大多数数据集上获得最高分数。特别是使用混合记忆时，迭代检索在HotPotQA任务上取得了82.11%的F1分数，在2WikiMultihopQA任务上取得了68.15%的F1分数，展示了其在迭代查询中提高准确度的能力。（2）重新排序在中等复杂度的数据集上表现强劲。例如，在使用原子事实记忆时，它在LoCoMo上的F1分数为44.27%，在NarrativeQA上的F1分数为28.19%。（3）相比之下，单步检索在需要最小化上下文整合的任务中表现竞争力。使用总结性记忆时，它在NarrativeQA上的F1分数为32.93%，利用抽象化来提取连贯的信息。这些发现强调了将检索机制与任务要求对齐的重要性，而迭代检索在推理任务中表现尤为突出。
- en: 5.3 Impact of Answer Generation Approaches
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 回答生成方法的影响
- en: 'Finding 3: Extensive Context tasks favor Memory-Doc, while precision tasks
    benefit from Memory-Only. As shown in Figure [3](https://arxiv.org/html/2412.15266v1#S5.F3
    "Figure 3 ‣ 5.1 Impact of Memory Structures ‣ 5 Results and Analysis ‣ On the
    Structural Memory of LLM Agents"), which compares their performance across various
    datasets. retrieving documents through retrieved memories provides a more comprehensive
    understanding, much like how humans integrate immediate recall with broader context
    to interpret complex narratives. In contrast, for datasets involving multi-hop
    reasoning and dialogue understanding, such as HotPotQA and LoCoMo, the Memory-Only
    approach proves to be the more effective strategy. These findings highlight that
    tasks requiring extensive context benefit from the Memory-Doc approach, which
    incorporates broader document-level information for enriched responses. On the
    other hand, tasks prioritizing precision are better suited to the Memory-Only
    approach, ensuring focused and accurate retrieval.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 发现 3：广泛上下文任务偏好Memory-Doc，而精度任务则受益于Memory-Only。如图[3](https://arxiv.org/html/2412.15266v1#S5.F3
    "图 3 ‣ 5.1 记忆结构的影响 ‣ 5 结果与分析 ‣ 大型语言模型代理的结构性记忆")所示，比较了它们在不同数据集上的表现。通过检索记忆检索文档能够提供更全面的理解，这类似于人类如何将即时回忆与更广泛的上下文结合来解释复杂的叙事。相反，对于涉及多跳推理和对话理解的数据集，如HotPotQA和LoCoMo，Memory-Only方法被证明是更有效的策略。这些发现表明，广泛上下文任务受益于Memory-Doc方法，该方法整合了更广泛的文档级信息以丰富响应。另一方面，精度优先的任务更适合Memory-Only方法，它确保了更集中的精确检索。
- en: '![Refer to caption](img/897bf9def1d51f9de822efc62ecde665.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/897bf9def1d51f9de822efc62ecde665.png)'
- en: 'Figure 4: Performance of different numbers of retrieved memories $K$ on HotPotQA
    and LoCoMo using single-step retrieval.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用单步检索时，不同数量的检索记忆$K$在HotPotQA和LoCoMo上的表现。
- en: '![Refer to caption](img/c965bc5cda14a3a203b7ec7a74ed6a76.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/c965bc5cda14a3a203b7ec7a74ed6a76.png)'
- en: 'Figure 5: Performance of different numbers of reranked memories $R$ on HotPotQA
    and LoCoMo in reranking.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：在HotPotQA和LoCoMo中的不同数量的重排记忆$R$在重排中的表现。
- en: 5.4 Hyperparameter Sensitivity
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 超参数敏感度
- en: Effect of Number of Retrieved Memories $K$. We first evaluate the impact of
    $K$ in single-step retrieval, with a limit of $K=200$ due to computational resource
    limitations. As depicted in Figure [4](https://arxiv.org/html/2412.15266v1#S5.F4
    "Figure 4 ‣ 5.3 Impact of Answer Generation Approaches ‣ 5 Results and Analysis
    ‣ On the Structural Memory of LLM Agents"), in HotPotQA, chunks demonstrate consistent
    performance, stabilizing around 77% across all $K$ values. In LoCoMo, the chunks
    show moderate gains up to $K=50$, whereas triples, atomics, and summaries improve
    up to $K=100$ but then declined at $K=200$, likely due to noise introduced by
    retrieving excessive memories. These findings indicate that the optimal $K$ depends
    on both the dataset and memory structure. While moderate $K$ values generally
    enhance performance, excessively large values can introduce irrelevant information,
    leading to a degraded performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 检索记忆数量$K$的影响。我们首先评估$K$在单步检索中的影响，由于计算资源的限制，$K$的上限为200。如图[4](https://arxiv.org/html/2412.15266v1#S5.F4
    "图 4 ‣ 5.3 回答生成方法的影响 ‣ 5 结果与分析 ‣ 大型语言模型代理的结构性记忆")所示，在HotPotQA中，片段表现稳定，在所有$K$值下约为77%。在LoCoMo中，片段的表现有适度的提升，直到$K=50$，而三元组、原子结构和摘要在$K=100$时表现最佳，但在$K=200$时有所下降，这可能是由于检索过多记忆引入的噪声。这些发现表明，最佳的$K$值取决于数据集和记忆结构。尽管适度的$K$值通常能提升性能，但过大的$K$值可能会引入无关信息，从而导致性能下降。
- en: Effect of Number of Reranked Memories $R$. To evaluate the impact of $R$ in
    reranking, we investigate performance across a range of values, with a maximum
    $R$ of 75 due to computational cost constraints, while fixing $K$ at 100. As depicted
    in Figure [5](https://arxiv.org/html/2412.15266v1#S5.F5 "Figure 5 ‣ 5.3 Impact
    of Answer Generation Approaches ‣ 5 Results and Analysis ‣ On the Structural Memory
    of LLM Agents"), the results highlight that increasing the number of reranked
    memories does not always lead to better performance. For instance, chunks achieve
    the highest F1 score at $R=10$ in HotPotQA, with a subsequent decline in performance
    beyond $R=50$. This pattern is consistent with triples and atomic facts, indicating
    that selecting a smaller number of highly relevant memories can outperform retrieving
    and reranking larger sets, which often introduces noise. A similar trend can be
    observed in LoCoMo. These findings suggest that reranking is more effective when
    it focuses on a smaller subset of highly relevant memories.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 重排序记忆数量 $R$ 的影响。为了评估 $R$ 在重排序中的影响，我们研究了不同值范围下的性能，最大 $R$ 设为 75，原因是计算成本的限制，同时将
    $K$ 固定为 100。如图 [5](https://arxiv.org/html/2412.15266v1#S5.F5 "图 5 ‣ 5.3 答案生成方法的影响
    ‣ 5 结果与分析 ‣ 关于 LLM 代理的结构记忆") 所示，结果表明，增加重排序的记忆数量并不总是能带来更好的性能。例如，在 HotPotQA 中，块（chunks）在
    $R=10$ 时达到最高的 F1 分数，而在 $R=50$ 之后，性能开始下降。这一趋势在三元组和原子事实中也有所体现，表明选择少量高度相关的记忆往往能优于检索并重排序更大的记忆集合，因为后者通常会引入噪声。在
    LoCoMo 中也可以观察到类似的趋势。这些发现表明，重排序在集中于少量高度相关的记忆时更加有效。
- en: '![Refer to caption](img/3898796534d05ef9ff3093e23868cd8b.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3898796534d05ef9ff3093e23868cd8b.png)'
- en: 'Figure 6: Performance of different numbers of retrieved memories $T$ in each
    interaction on HotPotQA and LoCoMo using iterative retrieval.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在 HotPotQA 和 LoCoMo 上使用迭代检索时，每次交互中检索到的不同数量的记忆 $T$ 的性能表现。
- en: Effect of Number of Retrieved Memories $T$ on Each Iteration. We first investigate
    performance across a range of values of $T$ using iterative retrieval, with a
    maximum $T$ of 75 and $N$ of 4 due to computational cost constraints while keeping
    $K$ fixed at 100. As illustrated in Figure [6](https://arxiv.org/html/2412.15266v1#S5.F6
    "Figure 6 ‣ 5.4 Hyperparameter Sensitivity ‣ 5 Results and Analysis ‣ On the Structural
    Memory of LLM Agents"), increasing the number of retrieved memories per iteration
    generally improves performance across datasets, though the gains diminish beyond
    a certain threshold. For instance, in HotPotQA, atomic facts achieve an F1 score
    of approximately 81% at $T=50$, with minimal additional gains from increasing
    $T$ further. Similarly, in LoCoMo, chunks improve up to $T=50$ before declining
    at $T=75$. These results indicate that while increasing $T$ can enhance query
    refinement and performance, excessively large $T$ values may introduce noise,
    ultimately reducing effectiveness.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中检索记忆数量 $T$ 的影响。我们首先研究了使用迭代检索时，不同 $T$ 值范围内的性能，最大 $T$ 设为 75，$N$ 为 4，原因是计算成本的限制，同时保持
    $K$ 固定为 100。如图 [6](https://arxiv.org/html/2412.15266v1#S5.F6 "图 6 ‣ 5.4 超参数敏感性
    ‣ 5 结果与分析 ‣ 关于 LLM 代理的结构记忆") 所示，通常增加每次迭代中检索到的记忆数量会改善数据集的性能，但超过某一阈值后，增益会逐渐减少。例如，在
    HotPotQA 中，原子事实在 $T=50$ 时达到约 81% 的 F1 分数，进一步增加 $T$ 值带来的增益微乎其微。同样，在 LoCoMo 中，块的性能在
    $T=50$ 时达到最高，然后在 $T=75$ 时开始下降。这些结果表明，虽然增加 $T$ 值可以改善查询的细化和性能，但过大的 $T$ 值可能引入噪声，最终降低效果。
- en: '![Refer to caption](img/a9998bed77a9771b65c82fd03bc63db8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a9998bed77a9771b65c82fd03bc63db8.png)'
- en: 'Figure 7: Performance of different numbers of retrieved memories $N$ in each
    interaction on HotPotQA and LoCoMo using iterative retrieval.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：在 HotPotQA 和 LoCoMo 上使用迭代检索时，每次交互中检索到的不同数量的记忆 $N$ 的性能表现。
- en: Effect of Number of Iteration Turns $N$. Next, we examine the impact of iteration
    turns $N$, with the number of retrieved memories $T$ fixed at 50. As depicted
    in Figure [6](https://arxiv.org/html/2412.15266v1#S5.F6 "Figure 6 ‣ 5.4 Hyperparameter
    Sensitivity ‣ 5 Results and Analysis ‣ On the Structural Memory of LLM Agents"),
    the results reveal that increasing $N$ initially enhances performance significantly,
    but the rate of improvement diminishes as $N$ continues to rise. For HotPotQA,
    both triples and summars show notable gains from $N=1$ to $N=3$, after which the
    improvements become marginal. In the case of LoCoMo, triples, atomic facts, and
    summaries reach a peak at $N=3$ and stop increasing afterwards. These results
    suggest that an intermediate number of iteration turns, typically between 2 and
    3, achieves optimal performance improvements, striking a balance between maximizing
    effectiveness and minimizing resource expenditure.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数$N$的影响。接下来，我们研究迭代次数$N$的影响，其中检索到的记忆数量$T$固定为50。如图[6](https://arxiv.org/html/2412.15266v1#S5.F6
    "Figure 6 ‣ 5.4 Hyperparameter Sensitivity ‣ 5 Results and Analysis ‣ On the Structural
    Memory of LLM Agents")所示，结果表明，增加$N$最初显著提高了性能，但随着$N$的增加，性能提升的速度逐渐减缓。对于HotPotQA，从$N=1$到$N=3$，三元组和摘要的表现都有显著提升，之后提升变得微弱。在LoCoMo的情况下，三元组、原子事实和摘要在$N=3$时达到了顶峰，之后不再增加。这些结果表明，适中的迭代次数，通常在2到3之间，可以实现最佳的性能提升，既能最大化效果，又能最小化资源消耗。
- en: '![Refer to caption](img/aee87d5ef9a26b3d061c6a511052c26d.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/aee87d5ef9a26b3d061c6a511052c26d.png)'
- en: 'Figure 8: Performance across varying numbers of noise documents using single-step
    retrieval.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：在使用单步检索时，不同数量噪声文档下的表现。
- en: 5.5 Impact of Noise Documents
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 噪声文档的影响
- en: 'Finding 4: Mix memory excels in noise resilience. Finally, we evaluate the
    robustness of various memory structures under increasing levels of noise using
    single-step retrieval with a fixed $K=100$. As depicted in Figure [8](https://arxiv.org/html/2412.15266v1#S5.F8
    "Figure 8 ‣ 5.4 Hyperparameter Sensitivity ‣ 5 Results and Analysis ‣ On the Structural
    Memory of LLM Agents"), the performance of all memory structures declines as the
    number of noise documents increases. For HotPotQA, the mix memory consistently
    achieves the highest F1 scores, demonstrating superior resilience to noise. While
    triples and summaries exhibit similar rates of decline, the chunks experience
    a slower decline, maintaining a competitive F1 score when increasing the number
    of noise documents. A similar pattern is shown in LoCoMo. These findings reveal
    the robustness of the mixed memory structure, which consistently outperforms others
    across datasets, making it the most effective choice in noisy environments.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 发现4：混合记忆在噪声抗性方面表现突出。最后，我们评估了在使用固定$K=100$的单步检索下，随着噪声级别的增加，各种记忆结构的鲁棒性。如图[8](https://arxiv.org/html/2412.15266v1#S5.F8
    "Figure 8 ‣ 5.4 Hyperparameter Sensitivity ‣ 5 Results and Analysis ‣ On the Structural
    Memory of LLM Agents")所示，随着噪声文档数量的增加，所有记忆结构的性能都有所下降。对于HotPotQA，混合记忆始终取得最高的F1分数，表现出对噪声的优越抗性。虽然三元组和摘要的下降速度相似，但数据块的下降速度较慢，在增加噪声文档的情况下，依然保持竞争力的F1分数。在LoCoMo中也呈现出类似的趋势。这些发现揭示了混合记忆结构的鲁棒性，其在多个数据集上始终优于其他结构，成为噪声环境中最有效的选择。
- en: 6 Conclusion & Future Work
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: 'In this paper, we present the first comprehensive study on the impact of structural
    memories and memory retrieval methods in LLM-based agents, aiming to identify
    the most suitable memory structures for specific tasks and explore how retrieval
    methods influence performance. This study yielded several key findings: (1) Mixed
    memories consistently deliver balanced performance. Chunks and summaries excel
    in tasks involving lengthy contexts, such as reading comprehension and dialogue
    understanding, while knowledge triples and atomic facts are effective for relational
    reasoning and precision in multi-hop and single-hop QA. (2) Mixed memories also
    demonstrate remarkable resilience to noise. (3) Iterative retrieval stands out
    as the most effective memory retrieval method, consistently outperforming in tasks
    such as multi-hop QA, dialogue understanding and reading comprehension. While
    these findings provide valuable insights, further research is needed to explore
    how memory impacts areas such as self-evolution and social simulation, highlighting
    the importance of investigating how structural memories and retrieval techniques
    support these applications.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了关于结构性记忆和记忆检索方法对基于LLM的代理影响的首个全面研究，旨在确定最适合特定任务的记忆结构，并探索检索方法如何影响性能。本研究得出了一些关键发现：(1)
    混合记忆始终提供均衡的表现。块状记忆和摘要在涉及长上下文的任务中表现突出，如阅读理解和对话理解，而知识三元组和原子事实对于关系推理和多跳与单跳问答的精确性有效。(2)
    混合记忆还表现出显著的噪声韧性。(3) 迭代检索被认为是最有效的记忆检索方法，在多跳问答、对话理解和阅读理解等任务中始终表现优异。尽管这些发现提供了有价值的见解，但仍需要进一步研究以探索记忆如何影响自我进化和社会模拟等领域，突显了研究结构性记忆和检索技术如何支持这些应用的重要性。
- en: Limitations
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: 'We identify the following limitations in our work: (1) Our experiments are
    limited to tasks such as multi-hop QA, single-hop QA, dialogue understanding,
    and reading comprehension, which restricts the applicability of our findings to
    other complex domains like self-evolving agents or social simulation. Investigating
    the role of memory structures and retrieval methods in these topics could provide
    broader insights; (2) The evaluation of memory robustness primarily considers
    random document noise, leaving other challenging noise types, such as irrelevant
    or contradictory information, unexplored. Investigating these addition noise in
    future studies could offer a more comprehensive understanding of memory resilience;
    (3) Due to computational constraints, we limit the hyperparameter ranges (e.g.,
    $K$, $R$, $T$, $N$) in memory retrieval methods. Expanding these ranges in future
    research could yield deeper insights into their impact on performance.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在工作中识别了以下限制：(1) 我们的实验仅限于多跳问答、单跳问答、对话理解和阅读理解等任务，这限制了我们的发现对其他复杂领域，如自我进化代理或社会模拟的适用性。研究记忆结构和检索方法在这些主题中的作用可能会提供更广泛的见解；(2)
    记忆鲁棒性的评估主要考虑了随机文档噪声，未探索其他具有挑战性的噪声类型，如无关或矛盾信息。未来研究中探讨这些额外的噪声可能会提供更全面的记忆韧性理解；(3)
    由于计算限制，我们限制了记忆检索方法中的超参数范围（例如，$K$、$R$、$T$、$N$）。在未来的研究中扩展这些范围可能会对它们对性能的影响提供更深入的见解。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anderson (2013) John R Anderson. 2013. *The architecture of cognition*. Psychology
    Press.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson (2013) John R Anderson. 2013. *认知架构*. 心理学出版社。
- en: 'Anokhin et al. (2024) Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry
    Evseev, Mikhail Burtsev, and Evgeny Burnaev. 2024. Arigraph: Learning knowledge
    graph world models with episodic memory for llm agents. *arXiv preprint arXiv:2407.04363*.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Anokhin et al. (2024) Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry
    Evseev, Mikhail Burtsev, 和 Evgeny Burnaev. 2024. Arigraph: 利用情节记忆学习知识图谱世界模型，针对LLM代理.
    *arXiv 预印本 arXiv:2407.04363*。'
- en: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
    A bilingual, multitask benchmark for long context understanding. *arXiv preprint
    arXiv:2308.14508*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bai et al. (2023) Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang,
    Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, 等. 2023. Longbench:
    一种用于长上下文理解的双语多任务基准. *arXiv 预印本 arXiv:2308.14508*。'
- en: Chase (2022) Harrison Chase. 2022. [LangChain](https://github.com/langchain-ai/langchain).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chase (2022) Harrison Chase. 2022. [LangChain](https://github.com/langchain-ai/langchain)。
- en: 'Chen et al. (2024) Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang,
    Wei Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu, et al. 2024. From persona
    to personalization: A survey on role-playing language agents. *arXiv preprint
    arXiv:2404.18231*.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024）Jiangjie Chen, Xintao Wang, Rui Xu, Siyu Yuan, Yikai Zhang, Wei
    Shi, Jian Xie, Shuang Li, Ruihan Yang, Tinghui Zhu 等人. 2024. 从角色到个性化：关于角色扮演语言代理的综述。*arXiv预印本
    arXiv:2404.18231*。
- en: Dong et al. (2024) Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang, and
    Anton Tsitsulin. 2024. Don’t forget to connect! improving rag with graph-based
    reranking. *arXiv preprint arXiv:2405.18414*.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等人（2024）Jialin Dong, Bahare Fatemi, Bryan Perozzi, Lin F Yang 和 Anton Tsitsulin.
    2024. 别忘了连接！通过基于图的重新排序改进RAG。*arXiv预印本 arXiv:2405.18414*。
- en: 'Fang et al. (2024) Jinyuan Fang, Zaiqiao Meng, and Craig MacDonald. 2024. TRACE
    the evidence: Constructing knowledge-grounded reasoning chains for retrieval-augmented
    generation. In *Findings of the Association for Computational Linguistics: EMNLP
    2024*, pages 8472–8494, Miami, Florida, USA. Association for Computational Linguistics.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang 等人（2024）Jinyuan Fang, Zaiqiao Meng 和 Craig MacDonald. 2024. TRACE证据：构建基于知识的推理链以增强检索生成。在
    *计算语言学协会发现：EMNLP 2024*，第8472–8494页，美国佛罗里达州迈阿密。计算语言学协会。
- en: 'Gao et al. (2023a) Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2023a.
    Precise zero-shot dense retrieval without relevance labels. In *Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers)*, pages 1762–1777.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2023a）Luyu Gao, Xueguang Ma, Jimmy Lin 和 Jamie Callan. 2023a. 无需相关标签的精确零-shot密集检索。收录于
    *第61届计算语言学协会年会会议论文集（第一卷：长篇论文）*，第1762–1777页。
- en: 'Gao et al. (2023b) Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu
    Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023b. Retrieval-augmented
    generation for large language models: A survey. *arXiv preprint arXiv:2312.10997*.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2023b）Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi
    Bi, Yi Dai, Jiawei Sun, Meng Wang 和 Haofen Wang. 2023b. 大型语言模型的检索增强生成：一项综述。*arXiv预印本
    arXiv:2312.10997*。
- en: Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.
    2020. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning
    steps. In *Proceedings of the 28th International Conference on Computational Linguistics*,
    pages 6609–6625.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ho 等人（2020）Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara 和 Akiko Aizawa. 2020.
    构建多跳问答数据集以全面评估推理步骤。收录于 *第28届国际计算语言学大会论文集*，第6609–6625页。
- en: '(11) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng,
    Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt:
    Meta programming for a multi-agent collaborative framework. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （11）Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin
    Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin 等人. Metagpt：多代理协作框架的元编程。收录于
    *第十二届国际学习表征会议*。
- en: 'Hu et al. (2024) Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao,
    and Ping Luo. 2024. Hiagent: Hierarchical working memory management for solving
    long-horizon agent tasks with large language model. *arXiv preprint arXiv:2408.09559*.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2024）Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao 和 Ping
    Luo. 2024. Hiagent：用于解决大语言模型长时程任务的层次化工作记忆管理。*arXiv预印本 arXiv:2408.09559*。
- en: Ji et al. (2024) Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin,
    Xuchen Pan, Yaliang Li, and Bolin Ding. 2024. Dynamic and textual graph generation
    via large-scale llm-based agent simulation. *arXiv preprint arXiv:2410.09824*.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等人（2024）Jiarui Ji, Runlin Lei, Jialing Bi, Zhewei Wei, Yankai Lin, Xuchen
    Pan, Yaliang Li 和 Bolin Ding. 2024. 通过大规模LLM基础的代理模拟生成动态文本图。*arXiv预印本 arXiv:2410.09824*。
- en: Kočiskỳ et al. (2018) Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer,
    Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa
    reading comprehension challenge. *Transactions of the Association for Computational
    Linguistics*, 6:317–328.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kočiskỳ 等人（2018）Tomáš Kočiskỳ, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl
    Moritz Hermann, Gábor Melis 和 Edward Grefenstette. 2018. NarrativeQA 阅读理解挑战。*计算语言学协会会刊*，6:317–328。
- en: Lee et al. (2024) Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny,
    and Ian Fischer. 2024. A human-inspired reading agent with gist memory of very
    long contexts. In *Forty-first International Conference on Machine Learning, ICML
    2024, Vienna, Austria, July 21-27, 2024*. OpenReview.net.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人（2024）Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John F. Canny 和 Ian
    Fischer. 2024. 一种受人类启发的阅读代理，具有非常长上下文的要点记忆。收录于 *第41届国际机器学习大会，ICML 2024，奥地利维也纳，2024年7月21日至27日*。OpenReview.net。
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 等人 (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel 等人. 2020. 增强检索生成用于知识密集型自然语言处理任务。*神经信息处理系统进展*，33:9459–9474。
- en: 'Li et al. (2024a) Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai,
    Jie Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang, et al. 2024a. Graphreader:
    Building graph-based agent to enhance long-context abilities of large language
    models. *arXiv preprint arXiv:2406.14550*.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2024a) Shilong Li, Yancheng He, Hangyu Guo, Xingyuan Bu, Ge Bai, Jie
    Liu, Jiaheng Liu, Xingwei Qu, Yangguang Li, Wanli Ouyang 等人. 2024a. Graphreader:
    构建基于图的代理以增强大语言模型的长上下文能力。*arXiv 预印本 arXiv:2406.14550*。'
- en: 'Li et al. (2024b) Xiaoxi Li, Zhicheng Dou, Yujia Zhou, and Fangchao Liu. 2024b.
    Corpuslm: Towards a unified language model on corpus for knowledge-intensive tasks.
    In *Proceedings of the 47th International ACM SIGIR Conference on Research and
    Development in Information Retrieval*, pages 26–37.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2024b) Xiaoxi Li, Zhicheng Dou, Yujia Zhou, 和 Fangchao Liu. 2024b. Corpuslm:
    面向知识密集型任务的统一语料库语言模型。载于 *第47届国际ACM SIGIR信息检索研究与开发会议论文集*，第26–37页。'
- en: 'Li et al. (2023) Yuan Li, Yixuan Zhang, and Lichao Sun. 2023. Metaagents: Simulating
    interactions of human behaviors for llm-based task-oriented coordination via collaborative
    generative agents. *arXiv preprint arXiv:2310.06500*.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 (2023) Yuan Li, Yixuan Zhang, 和 Lichao Sun. 2023. Metaagents: 通过协作生成代理模拟基于大语言模型的任务导向协调中的人类行为交互。*arXiv
    预印本 arXiv:2310.06500*。'
- en: 'Liu et al. (2023) Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang,
    Jinjie Gu, and Guannan Zhang. 2023. Think-in-memory: Recalling and post-thinking
    enable llms with long-term memory. *arXiv preprint arXiv:2311.08719*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2023) Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie
    Gu, 和 Guannan Zhang. 2023. Think-in-memory: 回忆与后续思考赋能大语言模型的长期记忆。*arXiv 预印本 arXiv:2311.08719*。'
- en: 'Maharana et al. (2024) Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit
    Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating very long-term conversational
    memory of LLM agents. In *Proceedings of the 62nd Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 13851–13870, Bangkok,
    Thailand. Association for Computational Linguistics.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maharana 等人 (2024) Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal,
    Francesco Barbieri, 和 Yuwei Fang. 2024. 评估大语言模型代理的超长期对话记忆。在 *第62届计算语言学协会年会（第1卷：长篇论文）*，第13851–13870页，泰国曼谷。计算语言学协会。
- en: 'Minaee et al. (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024. Large language models:
    A survey. *arXiv preprint arXiv:2402.06196*.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等人 (2024) Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu,
    Richard Socher, Xavier Amatriain, 和 Jianfeng Gao. 2024. 大型语言模型：综述。*arXiv 预印本 arXiv:2402.06196*。
- en: Nuxoll and Laird (2007) Andrew M Nuxoll and John E Laird. 2007. Extending cognitive
    architecture with episodic memory. In *AAAI*, pages 1560–1564.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nuxoll 和 Laird (2007) Andrew M Nuxoll 和 John E Laird. 2007. 通过情景记忆扩展认知架构。载于
    *AAAI*，第1560–1564页。
- en: 'Packer et al. (2023) Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang,
    Shishir G Patil, Ion Stoica, and Joseph E Gonzalez. 2023. Memgpt: Towards llms
    as operating systems. *arXiv preprint arXiv:2310.08560*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Packer 等人 (2023) Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir
    G Patil, Ion Stoica, 和 Joseph E Gonzalez. 2023. Memgpt: 朝着将大语言模型作为操作系统发展。*arXiv
    预印本 arXiv:2310.08560*。'
- en: 'Pang et al. (2022) Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita
    Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson,
    He He, et al. 2022. Quality: Question answering with long input texts, yes! In
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5336–5358.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang 等人 (2022) Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia,
    Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He
    等人. 2022. Quality: 使用长输入文本进行问题回答，没问题！载于 *2022年北美计算语言学协会人类语言技术会议论文集*，第5336–5358页。'
- en: 'Park et al. (2023) Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive
    simulacra of human behavior. In *Proceedings of the 36th annual acm symposium
    on user interface software and technology*, pages 1–22.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park、Joseph O’Brien、Carrie Jun Cai、Meredith Ringel Morris、Percy
    Liang 和 Michael S Bernstein。2023年。生成代理：人类行为的互动模拟。在 *第36届ACM用户界面软件与技术年会论文集*，第1–22页。
- en: 'Qian et al. (2024) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,
    Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. 2024. Chatdev:
    Communicative agents for software development. In *Proceedings of the 62nd Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 15174–15186.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人（2024）Chen Qian、Wei Liu、Hongzhang Liu、Nuo Chen、Yufan Dang、Jiahao Li、Cheng
    Yang、Weize Chen、Yusheng Su、Xin Cong 等人。2024年。Chatdev：软件开发中的交互式代理。在 *第62届计算语言学协会年会（第一卷：长篇论文集）*，第15174–15186页。
- en: 'Qiao et al. (2024) Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu
    Zhou, Yuchen Eleanor Jiang, Chengfei Lv, and Huajun Chen. 2024. Autoact: Automatic
    agent learning from scratch via self-planning. *arXiv preprint arXiv:2401.05268*.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao 等人（2024）Shuofei Qiao、Ningyu Zhang、Runnan Fang、Yujie Luo、Wangchunshu Zhou、Yuchen
    Eleanor Jiang、Chengfei Lv 和 Huajun Chen。2024年。Autoact：通过自我规划从零开始的自动化代理学习。*arXiv
    预印本 arXiv:2401.05268*。
- en: 'Qin et al. (2023) Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan,
    Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm:
    Facilitating large language models to master 16000+ real-world apis. *arXiv preprint
    arXiv:2307.16789*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等人（2023）Yujia Qin、Shihao Liang、Yining Ye、Kunlun Zhu、Lan Yan、Yaxi Lu、Yankai
    Lin、Xin Cong、Xiangru Tang、Bill Qian 等人。2023年。Toolllm：帮助大型语言模型掌握16000多个真实世界的API。*arXiv
    预印本 arXiv:2307.16789*。
- en: Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon
    Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented
    language models. *Transactions of the Association for Computational Linguistics*,
    11:1316–1331.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ram 等人（2023）Ori Ram、Yoav Levine、Itay Dalmedigos、Dor Muhlgay、Amnon Shashua、Kevin
    Leyton-Brown 和 Yoav Shoham。2023年。在-context 检索增强语言模型。*计算语言学会会刊*，11:1316–1331。
- en: 'Robertson et al. (2009) Stephen Robertson, Hugo Zaragoza, et al. 2009. The
    probabilistic relevance framework: Bm25 and beyond. *Foundations and Trends® in
    Information Retrieval*, 3(4):333–389.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robertson 等人（2009）Stephen Robertson、Hugo Zaragoza 等人。2009年。概率相关框架：Bm25 及其扩展。*信息检索的基础与趋势®*，3(4):333–389。
- en: 'Rubin et al. (2022) Ohad Rubin, Jonathan Herzig, and Jonathan Berant. 2022.
    Learning to retrieve prompts for in-context learning. In *Proceedings of the 2022
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies*, pages 2655–2671.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rubin 等人（2022）Ohad Rubin、Jonathan Herzig 和 Jonathan Berant。2022年。学习为在-context
    学习检索提示。在 *2022年北美计算语言学会年会：人类语言技术会议论文集*，第2655–2671页。
- en: 'Shi et al. (2024) Zhengliang Shi, Shuo Zhang, Weiwei Sun, Shen Gao, Pengjie
    Ren, Zhumin Chen, and Zhaochun Ren. 2024. Generate-then-ground in retrieval-augmented
    generation for multi-hop question answering. In *Proceedings of the 62nd Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 7339–7353.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi 等人（2024）Zhengliang Shi、Shuo Zhang、Weiwei Sun、Shen Gao、Pengjie Ren、Zhumin
    Chen 和 Zhaochun Ren。2024年。在检索增强生成中的生成-再确定方法用于多跳问答。在 *第62届计算语言学协会年会（第一卷：长篇论文集）*，第7339–7353页。
- en: 'Shinn et al. (2024) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. 2024. Reflexion: Language agents with verbal reinforcement
    learning. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn 等人（2024）Noah Shinn、Federico Cassano、Ashwin Gopinath、Karthik Narasimhan
    和 Shunyu Yao。2024年。Reflexion：具有语言强化学习的语言代理。*神经信息处理系统进展*，36。
- en: 'Simon and Newell (1971) Herbert A Simon and Allen Newell. 1971. Human problem
    solving: The state of the theory in 1970. *American psychologist*, 26(2):145.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simon 和 Newell（1971）Herbert A Simon 和 Allen Newell。1971年。人类问题解决：1970年理论现状。*美国心理学家*，26(2):145。
- en: 'Trivedi et al. (2022) Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
    and Ashish Sabharwal. 2022. Musique: Multihop questions via single-hop question
    composition. *Transactions of the Association for Computational Linguistics*,
    10:539–554.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trivedi 等人（2022）Harsh Trivedi、Niranjan Balasubramanian、Tushar Khot 和 Ashish
    Sabharwal。2022年。Musique：通过单跳问题组成实现多跳问题。*计算语言学会会刊*，10:539–554。
- en: 'Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023. Voyager: An open-ended
    embodied agent with large language models. *arXiv preprint arXiv:2305.16291*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei
    Xiao, Yuke Zhu, Linxi Fan, 和 Anima Anandkumar. 2023. Voyager: 一种基于大型语言模型的开放式具身代理。*arXiv预印本
    arXiv:2305.16291*。'
- en: Wang et al. (2024a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. 2024a. A survey
    on large language model based autonomous agents. *Frontiers of Computer Science*,
    18(6).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2024a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin 等人. 2024a. 基于大型语言模型的自主代理调查。*计算机科学前沿*，18(6)。
- en: 'Wang et al. (2024b) Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui
    Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, et al. 2024b. Incharacter:
    Evaluating personality fidelity in role-playing agents through psychological interviews.
    In *Proceedings of the 62nd Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 1840–1873.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024b) Xintao Wang, Yunze Xiao, Jen-tse Huang, Siyu Yuan, Rui
    Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang 等人. 2024b. Incharacter:
    通过心理访谈评估角色扮演代理的个性忠实度。见于*第62届计算语言学会年会论文集（第一卷：长篇论文）*，第1840–1873页。'
- en: 'Wang et al. (2024c) Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong,
    Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, and Qingsong Wen. 2024c. Rcagent:
    Cloud root cause analysis by autonomous agents with tool-augmented large language
    models. In *Proceedings of the 33rd ACM International Conference on Information
    and Knowledge Management*, pages 4966–4974.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2024c) Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong,
    Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, 和 Qingsong Wen. 2024c. Rcagent:
    基于工具增强的大型语言模型的自主代理进行云根因分析。见于*第33届ACM国际信息与知识管理会议论文集*，第4966–4974页。'
- en: 'Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. 2023. The rise and
    potential of large language model based agents: A survey. *arXiv preprint arXiv:2309.07864*.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi et al. (2023) Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang
    Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou 等人. 2023. 基于大型语言模型代理的崛起与潜力：一项调查。*arXiv预印本
    arXiv:2309.07864*。
- en: 'Xu et al. (2024) Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song,
    Hanghang Tong, Guang Liu, Kang Liu, and Jun Zhao. 2024. Generate-on-graph: Treat
    llm as both agent and kg in incomplete knowledge graph question answering. *arXiv
    preprint arXiv:2404.14741*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2024) Yao Xu, Shizhu He, Jiabei Chen, Zihao Wang, Yangqiu Song, Hanghang
    Tong, Guang Liu, Kang Liu, 和 Jun Zhao. 2024. Generate-on-graph：将LLM视为代理与KG，处理不完全知识图谱中的问答问题。*arXiv预印本
    arXiv:2404.14741*。
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: A dataset
    for diverse, explainable multi-hop question answering. In *Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing*, pages 2369–2380.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018. Hotpotqa: 一个用于多跳问答的多样化、可解释的数据集。见于*2018年自然语言处理实证方法会议论文集*，第2369–2380页。'
- en: 'Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, and Karthik Narasimhan. 2024. Tree of thoughts: Deliberate problem solving
    with large language models. *Advances in Neural Information Processing Systems*,
    36.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2024) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths,
    Yuan Cao, 和 Karthik Narasimhan. 2024. 思维树：与大型语言模型共同进行深思熟虑的解决问题。*神经信息处理系统进展*，36。
- en: 'Zhang et al. (2024a) Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, and Weiming Lu. 2024a. Agent-pro:
    Learning to evolve via policy-level reflection and optimization. *arXiv preprint
    arXiv:2402.17574*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2024a) Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen,
    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, 和 Weiming Lu. 2024a. Agent-pro:
    通过策略级反思与优化学习进化。*arXiv预印本 arXiv:2402.17574*。'
- en: Zhang et al. (2024b) Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. 2024b. A survey on the memory
    mechanism of large language model based agents. *arXiv preprint arXiv:2404.13501*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2024b) Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu
    Dai, Jieming Zhu, Zhenhua Dong, 和 Ji-Rong Wen. 2024b. 基于大型语言模型的代理的记忆机制调查。*arXiv预印本
    arXiv:2404.13501*。
- en: 'Zhong et al. (2024) Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin
    Wang. 2024. Memorybank: Enhancing large language models with long-term memory.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 38,
    pages 19724–19731.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhong 等人（2024）Wanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye 和 Yanlin Wang. 2024.
    Memorybank: 提升大语言模型的长期记忆能力。发表于 *人工智能学会会议论文集*，第 38 卷，页码 19724–19731。'
- en: Appendix A Datasets
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集
- en: We conduct experiments on the following six datasets across four tasks, including
    multi-hop QA, single-hop QA, dialogue understanding and reading comprehension.
    The statistical information of datasets is provided in Table [2](https://arxiv.org/html/2412.15266v1#A1.T2
    "Table 2 ‣ Appendix A Datasets ‣ On the Structural Memory of LLM Agents").
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下六个数据集上进行实验，涉及四个任务，包括多跳 QA、单跳 QA、对话理解和阅读理解。数据集的统计信息见表格 [2](https://arxiv.org/html/2412.15266v1#A1.T2
    "表 2 ‣ 附录 A 数据集 ‣ LLM 代理的结构性记忆").
- en: '| Task | Dataset | Avg. # Tokens | # Samples |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 平均 # 令牌 | # 样本 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Multi-hop QA | HotpotQA | 1,362 | 200 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 多跳 QA | HotpotQA | 1,362 | 200 |'
- en: '| Multi-hop QA | 2WikiMultihopQA | 985 | 200 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 多跳 QA | 2WikiMultihopQA | 985 | 200 |'
- en: '| Multi-hop QA | MuSiQue | 2,558 | 200 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 多跳 QA | MuSiQue | 2,558 | 200 |'
- en: '| Single-hop QA | NarrativeQA | 24,009 | 200 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 单跳 QA | NarrativeQA | 24,009 | 200 |'
- en: '| Dialogue Understanding | LoCoMo | 24,375 | 191 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 对话理解 | LoCoMo | 24,375 | 191 |'
- en: '| Reading Comprehension | QuALITY | 4,696 | 200 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 阅读理解 | QuALITY | 4,696 | 200 |'
- en: 'Table 2: The statistic and example of datasets.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：数据集的统计信息和示例。
- en: Appendix B Prompts
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 提示
- en: In this section, we present the prompts employed in our experiments, with detailed
    descriptions provided in the respective subsections.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了实验中使用的提示，并在各个子节中提供了详细描述。
- en: B.1 Prompt for Generating Knowledge Triples
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 生成知识三元组的提示
- en: The prompt used for extracting knowledge triples from a document is illustrated
    in Figure [9](https://arxiv.org/html/2412.15266v1#A2.F9 "Figure 9 ‣ B.5 Prompt
    for Iterative Refining Query ‣ Appendix B Prompts ‣ On the Structural Memory of
    LLM Agents").
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 用于从文档提取知识三元组的提示见图 [9](https://arxiv.org/html/2412.15266v1#A2.F9 "图 9 ‣ B.5 用于迭代优化查询的提示
    ‣ 附录 B 提示 ‣ LLM 代理的结构性记忆").
- en: B.2 Prompt for Generation Summaries
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 生成摘要的提示
- en: The prompt designed for generating document summaries is depicted in Figure [10](https://arxiv.org/html/2412.15266v1#A2.F10
    "Figure 10 ‣ B.5 Prompt for Iterative Refining Query ‣ Appendix B Prompts ‣ On
    the Structural Memory of LLM Agents").
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 用于生成文档摘要的提示见图 [10](https://arxiv.org/html/2412.15266v1#A2.F10 "图 10 ‣ B.5 用于迭代优化查询的提示
    ‣ 附录 B 提示 ‣ LLM 代理的结构性记忆").
- en: B.3 Prompt for Generating Atomic Facts
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 生成原子事实的提示
- en: The prompt for generating atomic facts from a document is shown in Figure [11](https://arxiv.org/html/2412.15266v1#A2.F11
    "Figure 11 ‣ B.5 Prompt for Iterative Refining Query ‣ Appendix B Prompts ‣ On
    the Structural Memory of LLM Agents").
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 用于从文档生成原子事实的提示见图 [11](https://arxiv.org/html/2412.15266v1#A2.F11 "图 11 ‣ B.5
    用于迭代优化查询的提示 ‣ 附录 B 提示 ‣ LLM 代理的结构性记忆").
- en: B.4 Prompt for Reranking Retrieved Memories
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 用于重新排序检索到的记忆的提示
- en: The prompt used for reranking retrieved memories is presented in Figure [12](https://arxiv.org/html/2412.15266v1#A2.F12
    "Figure 12 ‣ B.5 Prompt for Iterative Refining Query ‣ Appendix B Prompts ‣ On
    the Structural Memory of LLM Agents").
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 用于重新排序检索到的记忆的提示见图 [12](https://arxiv.org/html/2412.15266v1#A2.F12 "图 12 ‣ B.5
    用于迭代优化查询的提示 ‣ 附录 B 提示 ‣ LLM 代理的结构性记忆").
- en: B.5 Prompt for Iterative Refining Query
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.5 用于迭代优化查询的提示
- en: The prompt for iterative query refinement is provided in Figure [13](https://arxiv.org/html/2412.15266v1#A2.F13
    "Figure 13 ‣ B.5 Prompt for Iterative Refining Query ‣ Appendix B Prompts ‣ On
    the Structural Memory of LLM Agents").
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 用于迭代查询优化的提示见图 [13](https://arxiv.org/html/2412.15266v1#A2.F13 "图 13 ‣ B.5 用于迭代优化查询的提示
    ‣ 附录 B 提示 ‣ LLM 代理的结构性记忆").
- en: '![Refer to caption](img/b369f75bb5d8424602328192c949149b.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/b369f75bb5d8424602328192c949149b.png)'
- en: 'Figure 9: Prompt for generating knowledge triples from a document.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：用于从文档生成知识三元组的提示。
- en: '![Refer to caption](img/8d5ab76607c1292594eef4e7ec9c236a.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d5ab76607c1292594eef4e7ec9c236a.png)'
- en: 'Figure 10: Prompt for generating summaries from a document.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：用于从文档生成摘要的提示。
- en: '![Refer to caption](img/69a1685f85926f5c55aac88c839e67ce.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/69a1685f85926f5c55aac88c839e67ce.png)'
- en: 'Figure 11: Prompt for generating atomic facts from a document.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：用于从文档生成原子事实的提示。
- en: '![Refer to caption](img/0f1766a50a380c9e5c46810d4611e05f.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0f1766a50a380c9e5c46810d4611e05f.png)'
- en: 'Figure 12: Prompt for reranking retrieved memories.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：重新排序检索到的记忆的提示。
- en: '![Refer to caption](img/f9023bb905221ef920645b281c0ae9d4.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9023bb905221ef920645b281c0ae9d4.png)'
- en: 'Figure 13: Prompt for the iterative refining query.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：迭代优化查询的提示。
