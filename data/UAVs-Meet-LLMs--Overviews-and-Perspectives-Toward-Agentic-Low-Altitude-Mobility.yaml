- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2025-01-11 11:40:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 11:40:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《无人机与大型语言模型的融合：面向智能低空移动的概述与展望》
- en: 来源：[https://arxiv.org/html/2501.02341/](https://arxiv.org/html/2501.02341/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2501.02341/](https://arxiv.org/html/2501.02341/)
- en: 'Yonglin Tian¹¹footnotemark: 1 Fei Lin²²footnotemark: 2 Yiduo Li Tengchao Zhang
    Qiyao Zhang Xuan Fu Jun Huang Xingyuan Dai Yutong Wang Chunwei Tian Bai Li Yisheng
    Lv Levente Kovács Fei-Yue Wang'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '田永林¹¹footnotemark: 1 林飞²²footnotemark: 2 李一铎 张腾超 张启耀 傅轩 黄俊 戴星远 王宇彤 田春伟 李柏 易胜
    吕一生 科瓦奇·莱文特 王飞跃'
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Low-altitude mobility, exemplified by unmanned aerial vehicles (UAVs), has introduced
    transformative advancements across various domains, like transportation, logistics,
    and agriculture. Leveraging flexible perspectives and rapid maneuverability, UAVs
    extend traditional systems’ perception and action capabilities, garnering widespread
    attention from academia and industry. However, current UAV operations primarily
    depend on human control, with only limited autonomy in simple scenarios, and lack
    the intelligence and adaptability needed for more complex environments and tasks.
    The emergence of large language models (LLMs) demonstrates remarkable problem-solving
    and generalization capabilities, offering a promising pathway for advancing UAV
    intelligence. This paper explores the integration of LLMs and UAVs, beginning
    with an overview of UAV systems’ fundamental components and functionalities, followed
    by an overview of the state-of-the-art in LLM technology. Subsequently, it systematically
    highlights the multimodal data resources available for UAVs, which provide critical
    support for training and evaluation. Furthermore, it categorizes and analyzes
    key tasks and application scenarios where UAVs and LLMs converge. Finally, a reference
    roadmap towards agentic UAVs is proposed, aiming to enable UAVs to achieve agentic
    intelligence through autonomous perception, memory, reasoning, and tool utilization.
    Related resources are available at [https://github.com/Hub-Tian/UAVs_Meet_LLMs](https://github.com/Hub-Tian/UAVs_Meet_LLMs).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 低空移动性，以无人机（UAVs）为代表，已在多个领域带来了变革性的进展，如交通、物流和农业。无人机凭借灵活的视角和快速的机动性，扩展了传统系统的感知和行动能力，吸引了学术界和工业界的广泛关注。然而，当前的无人机操作主要依赖于人工控制，仅在简单场景中具备有限的自主性，缺乏在更复杂的环境和任务中所需的智能和适应性。大型语言模型（LLMs）的出现展示了出色的问题解决和泛化能力，为推动无人机智能化提供了有希望的路径。本文探讨了LLMs与无人机的集成，首先概述了无人机系统的基本组件和功能，然后回顾了LLM技术的最新进展。随后，系统地重点介绍了无人机可用的多模态数据资源，这些资源为训练和评估提供了关键支持。此外，本文对无人机与LLM结合的关键任务和应用场景进行了分类和分析。最后，提出了一条面向智能无人机的参考路线图，旨在通过自主感知、记忆、推理和工具利用，使无人机实现智能化。相关资源可通过[https://github.com/Hub-Tian/UAVs_Meet_LLMs](https://github.com/Hub-Tian/UAVs_Meet_LLMs)获取。
- en: 'keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Unmanned aerial vehicles , large language models , foundation intelligence
    , low altitude mobility systems^(cor1)^(cor1)footnotetext: Equal contribution\affiliation'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '无人机、大型语言模型、基础智能、低空移动系统^(cor1)^(cor1)footnotetext: 同等贡献\affiliation'
- en: '[label1]organization=The State Key Laboratory of Multimodal Artificial Intelligence
    Systems, Institute of Automation, Chinese Academy of Sciences,city=Beijing, postcode=100190,
    state=, country=China'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[label1]组织：中国科学院自动化研究所多模态人工智能系统国家重点实验室，城市：北京，邮政编码：100190，省份：，国家：中国'
- en: \affiliation
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[label2]organization=Department of Engineering Science, Faculty of Innovation
    Engineering, Macau University of Science and Technology,city=Macau, postcode=999078,
    state=, country=China \affiliation[label3]organization=School of Automation, Beijing
    Institute of Technology,city=Beijing, postcode=100081, state=, country=China \affiliation[label4]organization=School
    of Software, Northwestern Polytechnical University,city=Xi’an, postcode=710129,
    state=, country=China'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[label2]组织：澳门科技大学创新工程学院工程科学系，城市：澳门，邮政编码：999078，省份：，国家：中国 \affiliation[label3]组织：北京理工大学自动化学院，城市：北京，邮政编码：100081，省份：，国家：中国
    \affiliation[label4]组织：西北工业大学软件学院，城市：西安，邮政编码：710129，省份：，国家：中国'
- en: \affiliation
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: \affiliation
- en: '[label5]organization=College of Mechanical and Vehicle Engineering, Hunan University,city=Changsha,
    postcode=410082, state=, country=China \affiliation[label6]organization=John von
    Neumann Faculty of Informatics, Obuda University,city=Budapest, postcode=H-1034,
    state=, country=Hungary'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[label5]组织=湖南大学机械与车辆工程学院，城市=长沙，邮政编码=410082，省份=，国家=中国 \affiliation[label6]组织=奥布达大学约翰·冯·诺依曼信息学学院，城市=布达佩斯，邮政编码=H-1034，省份=，国家=匈牙利'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The rapid development of UAVs has introduced transformative solutions for monitoring
    and transportation across various sectors, including intelligent transportation,
    logistics, agriculture, and industrial inspection. With their flexible spatial
    mobility, UAVs significantly enhance the perception and decision-making capabilities
    of intelligent systems, offering a robust approach for upgrading traditional systems
    and improving operational efficiency. Given these advantages, UAV technology has
    attracted substantial attention from both academic researchers and industry practitioners.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机的快速发展为各个领域提供了变革性的监控和运输解决方案，包括智能交通、物流、农业和工业检测等。凭借其灵活的空间机动性，无人机显著提升了智能系统的感知和决策能力，为传统系统的升级和运营效率的提高提供了强有力的支持。鉴于这些优势，无人机技术已吸引了学术研究人员和行业从业者的广泛关注。
- en: Despite their promise, the majority of UAVs currently depend on human operators
    for flight control. This dependency not only incurs high labor costs but also
    introduces safety risks, as operators are limited by the range and sensitivity
    of onboard sensors when assessing environmental conditions. Such limitations impede
    the scalability and broader application of UAVs in complex environments. Furthermore,
    UAV flight control is inherently challenging due to the high degrees of freedom
    in movement and the need for precise navigation, obstacle avoidance, and real-time
    environmental perception, all of which complicate the path toward fully autonomous
    flight.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无人机（UAV）前景广阔，目前大多数无人机仍依赖人工操作进行飞行控制。这种依赖不仅带来了高昂的劳动成本，还增加了安全风险，因为操作员在评估环境条件时，受限于机载传感器的范围和敏感度。这些限制阻碍了无人机在复杂环境中的可扩展性和更广泛应用。此外，由于运动自由度高，需要精确的导航、避障和实时环境感知，UAV飞行控制本身就具有挑战性，这一切都使得完全自主飞行的实现道路更加复杂。
- en: '![Refer to caption](img/3e7f8ad11eb84c5136262324b67ba3be.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e7f8ad11eb84c5136262324b67ba3be.png)'
- en: 'Figure 1: Main sections and the structure of this paper'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本文的主要部分和结构
- en: Recent advancements in artificial intelligence, particularly in foundation models
    (FMs) such as ChatGPT, SORA, and various AI-generated content (AIGC) frameworks,
    have catalyzed significant transformations across industries. Large language models
    (LLMs) are endowed with near-human levels of commonsense reasoning and generalization
    capabilities, enabling advanced understanding, flexible adaptation, and real-time
    responsiveness in diverse applications. The integration of LLMs with UAV systems
    offers a promising avenue to enhance autonomy, providing UAVs with advanced reasoning
    capabilities and enabling more effective responses to dynamic environments.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，人工智能的快速发展，尤其是在基础模型（FM）领域，如ChatGPT、SORA和各种AI生成内容（AIGC）框架，已推动各行业发生重大变革。大型语言模型（LLM）具备接近人类的常识推理和泛化能力，使其能够在各种应用中实现高级理解、灵活适应和实时响应。将LLM与无人机系统集成，提供了一个有前景的途径来增强其自主性，使无人机具备更强的推理能力，从而更有效地应对动态环境。
- en: Initial studies have explored integrating LLMs with UAVs in areas such as navigation
    [[1](https://arxiv.org/html/2501.02341v1#bib.bib1), [2](https://arxiv.org/html/2501.02341v1#bib.bib2)],
    perception[[3](https://arxiv.org/html/2501.02341v1#bib.bib3), [4](https://arxiv.org/html/2501.02341v1#bib.bib4)],
    planning [[5](https://arxiv.org/html/2501.02341v1#bib.bib5), [6](https://arxiv.org/html/2501.02341v1#bib.bib6)].
    These early efforts highlight the potential of combining LLMs with UAV systems
    to foster more sophisticated autonomous behaviors. However, there remains a lack
    of systematic reviews on the integration of LLMs and UAVs, particularly regarding
    the frameworks and methodologies that support this interdisciplinary convergence.
    To advance the understanding of UAV and LLM integration, this paper provides a
    systematic review of the existing frameworks and methodologies, offering insights
    into the potential pathways for further advancing this interdisciplinary convergence.
    The main contributions of this paper are as follows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 初步研究探索了在导航[[1](https://arxiv.org/html/2501.02341v1#bib.bib1), [2](https://arxiv.org/html/2501.02341v1#bib.bib2)]、感知[[3](https://arxiv.org/html/2501.02341v1#bib.bib3),
    [4](https://arxiv.org/html/2501.02341v1#bib.bib4)]、规划[[5](https://arxiv.org/html/2501.02341v1#bib.bib5),
    [6](https://arxiv.org/html/2501.02341v1#bib.bib6)]等领域将大语言模型与无人机集成的研究。这些早期的努力凸显了将大语言模型与无人机系统结合的潜力，有助于促进更复杂的自主行为。然而，关于大语言模型与无人机集成的系统性综述仍然缺乏，尤其是在支持这一跨学科融合的框架和方法论方面。为了促进对无人机和大语言模型集成的理解，本文提供了现有框架和方法论的系统性回顾，提出了进一步推动这一跨学科融合的潜在路径。本文的主要贡献如下。
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: A comprehensive background on the integration of UAVs and FMs is provided, outlining
    the fundamental components and functional modules of UAV systems as well as the
    summary of typical FMs. Additionally, a detailed enumeration of publicly available
    dataset resources is provided, highlighting their critical role in supporting
    the development, training, and evaluation of intelligent UAV systems.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了关于无人机（UAV）与功能模块（FM）集成的全面背景，概述了无人机系统的基本组件和功能模块，并总结了典型功能模块的特点。此外，还详细列举了公开可用的数据集资源，强调了它们在支持智能无人机系统的开发、训练和评估中的关键作用。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: A comprehensive review of recent studies on the integration of LLMs with UAVs
    is conducted, identifying essential methodologies, diverse applications, and key
    challenges in areas such as navigation, perception, and planning tasks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对最近关于大语言模型（LLMs）与无人机（UAV）集成的研究进行了全面回顾，识别了在导航、感知和规划任务等领域的关键方法、不同应用和主要挑战。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: A design framework for Agentic UAVs is proposed, outlining the necessary architecture
    and capabilities to enable UAVs to achieve autonomous perception, reasoning, memory,
    and tool utilization, paving the way for their advancement into more intelligent
    and adaptable systems.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了一个面向代理型无人机的设计框架，概述了使无人机能够实现自主感知、推理、记忆和工具利用的必要架构和能力，为其向更智能和适应性更强的系统发展奠定了基础。
- en: 'Through these contributions, we aim to provide a foundational overview of the
    current research landscape at the intersection of UAV technology and LLMs, highlight
    emerging trends and challenges, and propose directions for future investigation.
    This survey aspires to serve as a reference for researchers and practitioners
    seeking to leverage LLM capabilities to advance UAV autonomy and broaden the application
    potential of unmanned low-altitude mobility systems. The organization of this
    paper is illustrated in Fig. [1](https://arxiv.org/html/2501.02341v1#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility"). The system knowledge of UAVs and Foundation Models (FMs)
    is introduced from three perspectives: system foundation, model foundation, and
    data foundation. Subsequently, the integration of UAVs with FMs is explored, highlighting
    the state of the arts (SOTAs) in various tasks and applications. Finally, the
    architecture of agentic UAVs is proposed, outlining the objectives for future
    development.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这些贡献，我们旨在提供关于无人机技术与大语言模型（LLM）交集的当前研究现状的基础性概述，突出新兴趋势和挑战，并提出未来研究的方向。本调查旨在为研究人员和实践者提供参考，帮助他们利用LLM的能力推进无人机自主性，并拓宽无人机低空移动系统的应用潜力。本文的组织结构如图[1](https://arxiv.org/html/2501.02341v1#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ UAVs Meet LLMs: Overviews and Perspectives Toward
    Agentic Low-Altitude Mobility")所示。无人机与基础模型（FMs）的系统知识从三个角度进行介绍：系统基础、模型基础和数据基础。随后，探讨了无人机与FMs的集成，重点介绍了各类任务和应用中的最新技术状态（SOTAs）。最后，提出了具备智能代理的无人机架构，并概述了未来发展的目标。'
- en: 2 Systematic Overview of UAV Systems
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 无人机系统的系统性概述
- en: 'This section provides a brief overview of intelligent UAVs from the perspectives
    of functional modules and embodied configurations. The functional modules encompass
    the core components of UAV systems, including the perception module, planning
    module, communication module, control module, navigation module, human-drone interaction
    module, and payload module, highlighting their roles and contributions to UAV
    functionality, as demonstrated in Fig. [2](https://arxiv.org/html/2501.02341v1#S2.F2
    "Figure 2 ‣ 2.1.7 Payload Module ‣ 2.1 Functional Modules of UAVs ‣ 2 Systematic
    Overview of UAV Systems ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility"). The embodied configuration aspect focuses on the structural
    characteristics of UAV systems, covering the designs and applications of fixed-wing
    UAVs[[7](https://arxiv.org/html/2501.02341v1#bib.bib7)], multirotor UAVs [[8](https://arxiv.org/html/2501.02341v1#bib.bib8),
    [9](https://arxiv.org/html/2501.02341v1#bib.bib9)], unmanned helicopters [[10](https://arxiv.org/html/2501.02341v1#bib.bib10)],
    and hybrid UAVs [[11](https://arxiv.org/html/2501.02341v1#bib.bib11)]. Furthermore,
    focusing on swarm intelligence for UAVs, this section summarizes the advancements
    in UAV swarm technologies, including communication strategies, formation control
    methods, and collaborative decision-making mechanisms.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '本节从功能模块和具象配置的角度简要概述了智能无人机（UAV）。功能模块包括无人机系统的核心组成部分，如感知模块、规划模块、通信模块、控制模块、导航模块、人机交互模块和载荷模块，重点介绍了它们在无人机功能中的作用和贡献，如图[2](https://arxiv.org/html/2501.02341v1#S2.F2
    "Figure 2 ‣ 2.1.7 Payload Module ‣ 2.1 Functional Modules of UAVs ‣ 2 Systematic
    Overview of UAV Systems ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")所示。具象配置方面则关注无人机系统的结构特征，涵盖了固定翼无人机[[7](https://arxiv.org/html/2501.02341v1#bib.bib7)]、多旋翼无人机[[8](https://arxiv.org/html/2501.02341v1#bib.bib8)、[9](https://arxiv.org/html/2501.02341v1#bib.bib9)]、无人直升机[[10](https://arxiv.org/html/2501.02341v1#bib.bib10)]以及混合型无人机[[11](https://arxiv.org/html/2501.02341v1#bib.bib11)]的设计与应用。此外，本节还着重介绍了无人机群体智能技术的进展，包括通信策略、队形控制方法和协作决策机制。'
- en: 2.1 Functional Modules of UAVs
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 无人机的功能模块
- en: 2.1.1 Perception Module
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 感知模块
- en: The Perception Module serves as the UAV’s “eyes and ears,” collecting and interpreting
    data from a variety of onboard sensors to build a comprehensive understanding
    of the surrounding environment. These sensors include RGB cameras, event-based
    cameras, thermal cameras, 3D cameras, LiDAR, radar, and ultrasonic sensors [[12](https://arxiv.org/html/2501.02341v1#bib.bib12)].
    By converting raw sensor data into actionable insights, such as detecting obstacles,
    identifying landmarks, and assessing terrain features. The Perception Module provides
    the situational awareness essential for safe and autonomous flight [[13](https://arxiv.org/html/2501.02341v1#bib.bib13),
    [14](https://arxiv.org/html/2501.02341v1#bib.bib14)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 感知模块充当无人机的“眼睛和耳朵”，收集并解释来自各种机载传感器的数据，以建立对周围环境的全面理解。这些传感器包括RGB相机、事件驱动相机、热成像相机、3D相机、激光雷达、雷达和超声波传感器[[12](https://arxiv.org/html/2501.02341v1#bib.bib12)]。通过将原始传感器数据转化为可操作的洞察，例如检测障碍物、识别地标和评估地形特征，感知模块提供了安全和自主飞行所必需的态势感知[[13](https://arxiv.org/html/2501.02341v1#bib.bib13)、[14](https://arxiv.org/html/2501.02341v1#bib.bib14)]。
- en: Beyond basic environmental monitoring, the Perception Module also supports collaborative
    tasks in multi-UAV operations, including the detection and tracking of other drones
    to facilitate coordinated swarm behavior. Advanced computer vision and machine
    learning techniques play a pivotal role in this process, enhancing the accuracy
    and robustness of object detection [[15](https://arxiv.org/html/2501.02341v1#bib.bib15),
    [16](https://arxiv.org/html/2501.02341v1#bib.bib16)], semantic segmentation [[17](https://arxiv.org/html/2501.02341v1#bib.bib17),
    [18](https://arxiv.org/html/2501.02341v1#bib.bib18)], and motion estimation [[19](https://arxiv.org/html/2501.02341v1#bib.bib19),
    [20](https://arxiv.org/html/2501.02341v1#bib.bib20)]. Sensor fusion methods are
    often employed to combine complementary data sources, such as fusing LiDAR depth
    maps with high-resolution camera imagery, thereby mitigating the limitations of
    individual sensors while capitalizing on their unique strengths [[21](https://arxiv.org/html/2501.02341v1#bib.bib21),
    [22](https://arxiv.org/html/2501.02341v1#bib.bib22)]. This robust, multimodal
    perception framework enables UAVs to adapt to changing conditions (e.g., varying
    lighting, dynamic environments) and carry out complex missions with minimal human
    intervention.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本的环境监测外，感知模块还支持多无人机操作中的协作任务，包括检测和追踪其他无人机，以促进协调的群体行为。先进的计算机视觉和机器学习技术在这一过程中发挥着关键作用，提高了物体检测[[15](https://arxiv.org/html/2501.02341v1#bib.bib15)、[16](https://arxiv.org/html/2501.02341v1#bib.bib16)]、语义分割[[17](https://arxiv.org/html/2501.02341v1#bib.bib17)、[18](https://arxiv.org/html/2501.02341v1#bib.bib18)]和运动估计[[19](https://arxiv.org/html/2501.02341v1#bib.bib19)、[20](https://arxiv.org/html/2501.02341v1#bib.bib20)]的准确性和鲁棒性。传感器融合方法通常用于结合互补的数据源，例如将激光雷达深度图与高分辨率相机图像融合，从而减轻单一传感器的局限性，同时利用它们的独特优势[[21](https://arxiv.org/html/2501.02341v1#bib.bib21)、[22](https://arxiv.org/html/2501.02341v1#bib.bib22)]。这一强大的多模态感知框架使得无人机能够适应变化的环境（例如，变化的光照、动态环境），并以最小的人为干预执行复杂的任务。
- en: 2.1.2 Navigation Module
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 导航模块
- en: he Navigation Module is responsible for translating the planned trajectories
    from the Planning Module into precise flight paths by continuously estimating
    and adjusting the UAV’s position, orientation, and velocity [[23](https://arxiv.org/html/2501.02341v1#bib.bib23)].
    To achieve this, it relies on a variety of onboard sensors [[24](https://arxiv.org/html/2501.02341v1#bib.bib24)],
    such as GPS [[25](https://arxiv.org/html/2501.02341v1#bib.bib25), [26](https://arxiv.org/html/2501.02341v1#bib.bib26)],
    inertial measurement units [[27](https://arxiv.org/html/2501.02341v1#bib.bib27),
    [28](https://arxiv.org/html/2501.02341v1#bib.bib28)], visual odometry, and barometric
    sensors or magnetometers to gather real-time information about the UAV’s state
    [[29](https://arxiv.org/html/2501.02341v1#bib.bib29), [30](https://arxiv.org/html/2501.02341v1#bib.bib30)].
    Sensor-fusion algorithms, including Kalman filters (e.g., Extended or Unscented
    Kalman Filters) and particle filters, are employed to integrate data from disparate
    sources, enhancing the reliability and accuracy of state estimation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 导航模块负责将规划模块中的计划轨迹转换为精确的飞行路径，方法是通过持续估算和调整无人机的位置、方向和速度[[23](https://arxiv.org/html/2501.02341v1#bib.bib23)]。为实现这一目标，它依赖于各种机载传感器[[24](https://arxiv.org/html/2501.02341v1#bib.bib24)]，如
    GPS[[25](https://arxiv.org/html/2501.02341v1#bib.bib25)、[26](https://arxiv.org/html/2501.02341v1#bib.bib26)]、惯性测量单元[[27](https://arxiv.org/html/2501.02341v1#bib.bib27)、[28](https://arxiv.org/html/2501.02341v1#bib.bib28)]、视觉里程计和气压传感器或磁力计等，用于收集关于无人机状态的实时信息[[29](https://arxiv.org/html/2501.02341v1#bib.bib29)、[30](https://arxiv.org/html/2501.02341v1#bib.bib30)]。融合算法，包括卡尔曼滤波器（如扩展卡尔曼滤波器或无迹卡尔曼滤波器）和粒子滤波器，被用来整合来自不同来源的数据，从而提高状态估计的可靠性和准确性。
- en: In GPS-denied or cluttered environments, the Navigation Module may employ simultaneous
    localization and mapping techniques or visual SLAM to provide robust localization
    and environment mapping [[31](https://arxiv.org/html/2501.02341v1#bib.bib31),
    [29](https://arxiv.org/html/2501.02341v1#bib.bib29), [32](https://arxiv.org/html/2501.02341v1#bib.bib32),
    [33](https://arxiv.org/html/2501.02341v1#bib.bib33), [34](https://arxiv.org/html/2501.02341v1#bib.bib34)].
    Such advanced solutions enable the UAV to maintain a high level of situational
    awareness even when traditional satellite-based positioning is unavailable or
    unreliable. By ensuring accurate state estimation and smooth trajectory tracking,
    the Navigation Module plays a critical role in maintaining overall flight stability
    and guaranteeing that the UAV adheres to the mission plan throughout its operational
    timeframe.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有 GPS 或在环境干扰严重的情况下，导航模块可能会使用同步定位与地图构建技术（SLAM）或视觉 SLAM，以提供强大的定位和环境建图能力[[31](https://arxiv.org/html/2501.02341v1#bib.bib31)、[29](https://arxiv.org/html/2501.02341v1#bib.bib29)、[32](https://arxiv.org/html/2501.02341v1#bib.bib32)、[33](https://arxiv.org/html/2501.02341v1#bib.bib33)、[34](https://arxiv.org/html/2501.02341v1#bib.bib34)]。这些先进的解决方案使得无人机即使在传统卫星定位不可用或不可靠的情况下，也能保持较高的态势感知水平。通过确保准确的状态估计和平滑的轨迹跟踪，导航模块在保持飞行稳定性和确保无人机在整个操作时间内执行任务计划方面发挥了关键作用。
- en: 2.1.3 Planning Module
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 规划模块
- en: The Planning Module is pivotal in translating high-level mission objectives
    into concrete flight trajectories and actions, relying on input from the Perception
    Module to ensure safe navigation [[35](https://arxiv.org/html/2501.02341v1#bib.bib35),
    [36](https://arxiv.org/html/2501.02341v1#bib.bib36)]. Path-planning algorithms
    span a broad range of techniques aimed at computing feasible and often optimized
    routes around obstacles. These methods include heuristic algorithms such as the
    $A*$ algorithm [[37](https://arxiv.org/html/2501.02341v1#bib.bib37)], Evolutionary
    Algorithms [[38](https://arxiv.org/html/2501.02341v1#bib.bib38)], Simulated Annealing
    [[39](https://arxiv.org/html/2501.02341v1#bib.bib39), [40](https://arxiv.org/html/2501.02341v1#bib.bib40)],
    Particle Swarm Optimization [[41](https://arxiv.org/html/2501.02341v1#bib.bib41),
    [42](https://arxiv.org/html/2501.02341v1#bib.bib42), [43](https://arxiv.org/html/2501.02341v1#bib.bib43)],
    Pigeon-Inspired Optimization [[44](https://arxiv.org/html/2501.02341v1#bib.bib44)],
    Artificial Bee Colony [[45](https://arxiv.org/html/2501.02341v1#bib.bib45), [46](https://arxiv.org/html/2501.02341v1#bib.bib46)],
    etc. Machine learning approaches, including Neural Networks [[47](https://arxiv.org/html/2501.02341v1#bib.bib47),
    [48](https://arxiv.org/html/2501.02341v1#bib.bib48), [49](https://arxiv.org/html/2501.02341v1#bib.bib49)],
    and Deep Reinforcement Learning [[50](https://arxiv.org/html/2501.02341v1#bib.bib50),
    [51](https://arxiv.org/html/2501.02341v1#bib.bib51)]are also employed for more
    adaptive and data-driven planning. Additionally, sampling-based strategies like
    Rapidly-exploring Random Trees offer flexible frameworks for dealing with high-dimensional
    or dynamically changing environments [[52](https://arxiv.org/html/2501.02341v1#bib.bib52)].
    By leveraging one or a combination of these methods, UAVs are able to devise safe,
    collision-free trajectories that optimize key performance metrics, such as travel
    time, energy consumption, or overall mission efficiency. [[53](https://arxiv.org/html/2501.02341v1#bib.bib53),
    [54](https://arxiv.org/html/2501.02341v1#bib.bib54), [44](https://arxiv.org/html/2501.02341v1#bib.bib44),
    [55](https://arxiv.org/html/2501.02341v1#bib.bib55)]. These techniques enable
    UAVs to operate autonomously within complex or uncertain environments by continuously
    adapting their planned path in real-time, particularly important when unforeseen
    changes occur in terrain, obstacle locations, or mission parameters.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 规划模块在将高层次的任务目标转化为具体的飞行轨迹和行动中起着至关重要的作用，依赖于感知模块的输入以确保安全导航[[35](https://arxiv.org/html/2501.02341v1#bib.bib35),
    [36](https://arxiv.org/html/2501.02341v1#bib.bib36)]。路径规划算法涵盖了广泛的技术，旨在计算可行的且通常是优化过的避障路线。这些方法包括启发式算法，如$A*$算法[[37](https://arxiv.org/html/2501.02341v1#bib.bib37)]，进化算法[[38](https://arxiv.org/html/2501.02341v1#bib.bib38)]，模拟退火[[39](https://arxiv.org/html/2501.02341v1#bib.bib39),
    [40](https://arxiv.org/html/2501.02341v1#bib.bib40)]，粒子群优化[[41](https://arxiv.org/html/2501.02341v1#bib.bib41),
    [42](https://arxiv.org/html/2501.02341v1#bib.bib42), [43](https://arxiv.org/html/2501.02341v1#bib.bib43)]，鸽子启发式优化[[44](https://arxiv.org/html/2501.02341v1#bib.bib44)]，人工蜂群[[45](https://arxiv.org/html/2501.02341v1#bib.bib45),
    [46](https://arxiv.org/html/2501.02341v1#bib.bib46)]等。机器学习方法，包括神经网络[[47](https://arxiv.org/html/2501.02341v1#bib.bib47),
    [48](https://arxiv.org/html/2501.02341v1#bib.bib48), [49](https://arxiv.org/html/2501.02341v1#bib.bib49)]，以及深度强化学习[[50](https://arxiv.org/html/2501.02341v1#bib.bib50),
    [51](https://arxiv.org/html/2501.02341v1#bib.bib51)]，也被用于更加自适应和数据驱动的规划。此外，基于采样的策略，如快速扩展随机树，提供了灵活的框架来处理高维或动态变化的环境[[52](https://arxiv.org/html/2501.02341v1#bib.bib52)]。通过利用这些方法中的一种或多种，无人机能够制定安全、无碰撞的轨迹，并优化关键性能指标，如旅行时间、能量消耗或总体任务效率[[53](https://arxiv.org/html/2501.02341v1#bib.bib53),
    [54](https://arxiv.org/html/2501.02341v1#bib.bib54), [44](https://arxiv.org/html/2501.02341v1#bib.bib44),
    [55](https://arxiv.org/html/2501.02341v1#bib.bib55)]。这些技术使得无人机能够在复杂或不确定的环境中自主操作，通过实时调整其规划路径，尤其在地形、障碍物位置或任务参数发生不可预见变化时显得尤为重要。
- en: In multi-UAV or swarm operations, the Planning Module also plays a key role
    in coordinating flight routes among individual drones, ensuring collision avoidance
    and maintaining cohesive group behaviors [[56](https://arxiv.org/html/2501.02341v1#bib.bib56),
    [57](https://arxiv.org/html/2501.02341v1#bib.bib57), [58](https://arxiv.org/html/2501.02341v1#bib.bib58)].
    This collaborative planning capability not only enhances mission efficiency but
    also reduces the risk of inter-UAV interference. By dynamically updating trajectories
    and sharing relevant information, the Planning Module underpins robust, reliable
    operations that align with overall mission goals.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在多无人机或群体操作中，规划模块还在协调各个无人机的飞行路线方面发挥关键作用，确保避免碰撞并保持群体行为的一致性[[56](https://arxiv.org/html/2501.02341v1#bib.bib56),
    [57](https://arxiv.org/html/2501.02341v1#bib.bib57), [58](https://arxiv.org/html/2501.02341v1#bib.bib58)]。这种协同规划能力不仅提升了任务效率，还降低了无人机间干扰的风险。通过动态更新轨迹和共享相关信息，规划模块为与整体任务目标一致的稳健可靠操作提供了支持。
- en: 2.1.4 Control Module
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 控制模块
- en: The Control Module is responsible for generating low-level commands that regulate
    the UAV’s actuators—including motors, servos, and other control surfaces—to maintain
    stable and responsive flight. Acting as the “muscle” of the system, it continuously
    adjusts key parameters such as altitude, velocity, orientation, and attitude in
    response to real-time feedback from onboard sensors. By closing the control loop
    with reference inputs provided by the Navigation and Planning Modules, the Control
    Module ensures that the UAV adheres to desired flight trajectories and mission
    objectives [[59](https://arxiv.org/html/2501.02341v1#bib.bib59), [60](https://arxiv.org/html/2501.02341v1#bib.bib60)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 控制模块负责生成低级指令，以调节无人机的执行器——包括电机、伺服电机和其他控制面——以保持稳定和响应迅速的飞行。作为系统的“肌肉”，它持续调整关键参数，如高度、速度、方向和姿态，以响应来自机载传感器的实时反馈。通过与导航与规划模块提供的参考输入闭合控制回路，控制模块确保无人机遵循预定的飞行轨迹和任务目标[[59](https://arxiv.org/html/2501.02341v1#bib.bib59),
    [60](https://arxiv.org/html/2501.02341v1#bib.bib60)]。
- en: To manage potential disturbances (e.g., wind gusts, payload variations) and
    compensate for modeling uncertainties, a variety of classical and modern control
    strategies are employed. Traditional approaches, such as Proportional–Integral–Derivative
    control [[61](https://arxiv.org/html/2501.02341v1#bib.bib61), [62](https://arxiv.org/html/2501.02341v1#bib.bib62)],
    offer simplicity and ease of implementation, while more advanced techniques like
    Model Predictive Control enable predictive action based on system dynamics and
    constraints. Adaptive control methods further enhance performance by adjusting
    control parameters in real time as the characteristics of the system evolve [[63](https://arxiv.org/html/2501.02341v1#bib.bib63),
    [64](https://arxiv.org/html/2501.02341v1#bib.bib64)]. Other robust strategies,
    such as sliding-mode control or nonlinear control can be used for particularly
    challenging operating conditions, providing resilience against sensor noise and
    sudden environmental changes [[65](https://arxiv.org/html/2501.02341v1#bib.bib65),
    [66](https://arxiv.org/html/2501.02341v1#bib.bib66)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了管理潜在的干扰（如阵风、载荷变化）并补偿建模不确定性，采用了多种经典和现代控制策略。传统方法，如比例-积分-微分控制[[61](https://arxiv.org/html/2501.02341v1#bib.bib61),
    [62](https://arxiv.org/html/2501.02341v1#bib.bib62)]，提供了简单性和易于实施的优点，而更先进的技术，如模型预测控制，则能够基于系统动态和约束进行预测性操作。自适应控制方法通过实时调整控制参数，进一步提升了系统在特性演变中的表现[[63](https://arxiv.org/html/2501.02341v1#bib.bib63),
    [64](https://arxiv.org/html/2501.02341v1#bib.bib64)]。其他鲁棒性策略，如滑模控制或非线性控制，可以在特别具有挑战性的操作条件下使用，提供对传感器噪声和突发环境变化的抗干扰能力[[65](https://arxiv.org/html/2501.02341v1#bib.bib65),
    [66](https://arxiv.org/html/2501.02341v1#bib.bib66)]。
- en: In multi-rotor UAVs, for example, the Control Module finely tunes individual
    motor speeds to achieve the appropriate thrust and torque distributions for stable
    flight, whereas in fixed-wing platforms, it manipulates aerodynamic surfaces to
    maintain or alter flight paths [[67](https://arxiv.org/html/2501.02341v1#bib.bib67),
    [68](https://arxiv.org/html/2501.02341v1#bib.bib68), [69](https://arxiv.org/html/2501.02341v1#bib.bib69)].
    This tight integration of sensor feedback, control algorithms, and actuator commands
    allows the UAV to respond quickly to deviations and external perturbations, ensuring
    smooth and reliable operations throughout the mission.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在多旋翼无人机中，例如，控制模块精确调节各个电机的转速，以实现适当的推力和扭矩分配，从而确保飞行稳定，而在固定翼平台中，它则通过操控气动表面来维持或改变飞行轨迹
    [[67](https://arxiv.org/html/2501.02341v1#bib.bib67), [68](https://arxiv.org/html/2501.02341v1#bib.bib68),
    [69](https://arxiv.org/html/2501.02341v1#bib.bib69)]。传感器反馈、控制算法和执行器命令的紧密集成，使得无人机能够迅速响应偏差和外部扰动，确保在整个任务中平稳且可靠的运行。
- en: 2.1.5 Communication Module
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5 通信模块
- en: The Communication Module underpins all data exchanges between the UAV, ground
    control stations, and other external entities, such as satellites, edge devices,
    or cloud-based services, ensuring that critical telemetry, control, and payload
    information flows seamlessly. Typical communication methods range from short-range
    radio frequency systems and Wi-Fi links to more sophisticated, longer-range networks
    like 4G, 5G, or even satellite-based links, each selected to meet the specific
    mission requirements regarding bandwidth, latency, and range [[70](https://arxiv.org/html/2501.02341v1#bib.bib70),
    [71](https://arxiv.org/html/2501.02341v1#bib.bib71), [72](https://arxiv.org/html/2501.02341v1#bib.bib72),
    [73](https://arxiv.org/html/2501.02341v1#bib.bib73)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通信模块支撑着无人机、地面控制站和其他外部实体（如卫星、边缘设备或基于云的服务）之间的所有数据交换，确保关键的遥测、控制和载荷信息流动无缝。典型的通信方式包括从短程无线电频率系统和Wi-Fi连接，到更复杂、更长距离的网络，如4G、5G，甚至卫星通信链路，每种方式的选择都旨在满足特定任务对带宽、延迟和范围的要求
    [[70](https://arxiv.org/html/2501.02341v1#bib.bib70), [71](https://arxiv.org/html/2501.02341v1#bib.bib71),
    [72](https://arxiv.org/html/2501.02341v1#bib.bib72), [73](https://arxiv.org/html/2501.02341v1#bib.bib73)]。
- en: In UAV swarm operations, the Communication Module becomes particularly vital,
    it relays commands to and from ground control and facilitates inter-UAV coordination
    by sharing situational data (e.g., positions, sensor readings) in real time [[73](https://arxiv.org/html/2501.02341v1#bib.bib73),
    [74](https://arxiv.org/html/2501.02341v1#bib.bib74)]. Robust communication protocols
    often augmented with encryption and authentication mechanisms guard against unauthorized
    access and malicious interference, while techniques like adaptive channel selection
    and multi-hop ad-hoc routing can mitigate signal degradation and ensure reliable
    connectivity in dynamic environments [[75](https://arxiv.org/html/2501.02341v1#bib.bib75)].
    By managing and prioritizing different data streams (telemetry, payload, command
    and control), the Communication Module serves as the backbone that keeps all subsystems
    in sync and supports the UAV’s overall operational objectives [[76](https://arxiv.org/html/2501.02341v1#bib.bib76)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在无人机集群操作中，通信模块变得尤为重要，它负责向地面控制发送指令，并促进无人机之间的协调，通过实时共享位置信息、传感器数据等情境信息 [[73](https://arxiv.org/html/2501.02341v1#bib.bib73),
    [74](https://arxiv.org/html/2501.02341v1#bib.bib74)]。强健的通信协议，通常配合加密和认证机制，以防止未经授权的访问和恶意干扰，同时，通过自适应信道选择和多跳临时路由等技术，可以减少信号衰减，并确保在动态环境中保持可靠的连接
    [[75](https://arxiv.org/html/2501.02341v1#bib.bib75)]。通过管理和优先处理不同的数据流（遥测、载荷、指令和控制），通信模块作为支撑整个子系统同步的骨干，支持无人机的整体操作目标
    [[76](https://arxiv.org/html/2501.02341v1#bib.bib76)]。
- en: 2.1.6 Interaction Module
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.6 互动模块
- en: The Interaction Module is designed to facilitate seamless communication and
    collaboration between the UAV and human operators or other agents in the operating
    environment [[77](https://arxiv.org/html/2501.02341v1#bib.bib77), [78](https://arxiv.org/html/2501.02341v1#bib.bib78)].
    It encompasses user interfaces and interaction paradigms that may include voice
    commands, gesture recognition, augmented or virtual reality displays, or touchscreen-based
    data visualization systems [[79](https://arxiv.org/html/2501.02341v1#bib.bib79),
    [80](https://arxiv.org/html/2501.02341v1#bib.bib80), [81](https://arxiv.org/html/2501.02341v1#bib.bib81),
    [82](https://arxiv.org/html/2501.02341v1#bib.bib82), [83](https://arxiv.org/html/2501.02341v1#bib.bib83)].
    Additional methods such as adaptive user interface design that tailors the displayed
    information to the operator’s skill level and workload, or haptic feedback mechanisms
    that provide tactile alerts for critical events can further enhance situational
    awareness and user experience [[84](https://arxiv.org/html/2501.02341v1#bib.bib84)].
    These interfaces enable ground personnel to issue high-level commands, review
    mission progress, and intervene when necessary, ensuring that operators maintain
    oversight and decision-making authority [[85](https://arxiv.org/html/2501.02341v1#bib.bib85)].
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 交互模块旨在促进无人机与人类操作员或操作环境中其他代理之间的无缝通信与协作[[77](https://arxiv.org/html/2501.02341v1#bib.bib77),
    [78](https://arxiv.org/html/2501.02341v1#bib.bib78)]。它包含了用户界面和交互范式，可能包括语音命令、手势识别、增强现实或虚拟现实显示，或者基于触摸屏的数据可视化系统[[79](https://arxiv.org/html/2501.02341v1#bib.bib79),
    [80](https://arxiv.org/html/2501.02341v1#bib.bib80), [81](https://arxiv.org/html/2501.02341v1#bib.bib81),
    [82](https://arxiv.org/html/2501.02341v1#bib.bib82), [83](https://arxiv.org/html/2501.02341v1#bib.bib83)]。其他方法，如自适应用户界面设计，根据操作员的技能水平和工作负荷调整显示信息，或者通过触觉反馈机制为关键事件提供触觉警报，可以进一步增强态势感知和用户体验[[84](https://arxiv.org/html/2501.02341v1#bib.bib84)]。这些界面使地面人员能够发出高层命令、审查任务进度，并在必要时进行干预，确保操作员保持监督和决策权[[85](https://arxiv.org/html/2501.02341v1#bib.bib85)]。
- en: In swarm or multi-UAV contexts, the Interaction Module becomes even more integral.
    It not only allows central decision makers to coordinate multiple drones but also
    enables human operators to receive aggregated situational data from across the
    swarm, potentially flagging anomalies or emergent behaviors in real time. These
    human-UAV interaction channels are particularly critical in collaborative missions
    (for example, search and rescue, environmental monitoring, or infrastructure inspection),
    where on-the-spot guidance or feedback may be required to adapt the UAVs’ behavior
    to evolving conditions [[86](https://arxiv.org/html/2501.02341v1#bib.bib86), [87](https://arxiv.org/html/2501.02341v1#bib.bib87),
    [88](https://arxiv.org/html/2501.02341v1#bib.bib88)]. By providing robust mechanisms
    for manual overrides and real-time communication, the Interaction Module strikes
    a balance between autonomous operation and human-in-the-loop supervision, enhancing
    both mission effectiveness and operational safety [[87](https://arxiv.org/html/2501.02341v1#bib.bib87),
    [89](https://arxiv.org/html/2501.02341v1#bib.bib89), [90](https://arxiv.org/html/2501.02341v1#bib.bib90)].
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在群体或多无人机（UAV）情境下，交互模块变得更加重要。它不仅允许中央决策者协调多架无人机，还使人类操作员能够接收来自整个群体的汇总态势数据，可能实时标出异常或新兴行为。这些人类-无人机交互通道在协作任务中尤为关键（例如，搜救、环境监测或基础设施检查），其中可能需要现场指导或反馈，以便根据不断变化的条件调整无人机的行为[[86](https://arxiv.org/html/2501.02341v1#bib.bib86),
    [87](https://arxiv.org/html/2501.02341v1#bib.bib87), [88](https://arxiv.org/html/2501.02341v1#bib.bib88)]。通过提供强大的手动覆盖和实时通信机制，交互模块在自主操作和人类监督之间取得了平衡，提升了任务效率和操作安全性[[87](https://arxiv.org/html/2501.02341v1#bib.bib87),
    [89](https://arxiv.org/html/2501.02341v1#bib.bib89), [90](https://arxiv.org/html/2501.02341v1#bib.bib90)]。
- en: 2.1.7 Payload Module
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.7 载荷模块
- en: The Payload Module oversees the equipment or cargo the UAV carries to accomplish
    its mission objectives. Depending on the task, these payloads may range from cameras
    for surveillance, to delivery packages, to advanced sensors for environmental
    monitoring, to specialized hardware for tasks such as search and rescue [[91](https://arxiv.org/html/2501.02341v1#bib.bib91)].
    Consequently, the Payload Module must address a variety of operational needs,
    including power supply, secure data transmission, mechanical support, and proper
    stabilization to ensure reliable performance under diverse conditions [[59](https://arxiv.org/html/2501.02341v1#bib.bib59),
    [92](https://arxiv.org/html/2501.02341v1#bib.bib92)].
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 有效载荷模块负责监督无人机携带的设备或货物，以完成其任务目标。根据任务的不同，这些有效载荷可能包括用于监视的相机、投递包裹、用于环境监测的先进传感器，或用于搜救等任务的专业硬件[[91](https://arxiv.org/html/2501.02341v1#bib.bib91)]。因此，有效载荷模块必须满足各种操作需求，包括电源供应、安全数据传输、机械支持以及适当的稳定性，以确保在不同条件下的可靠性能[[59](https://arxiv.org/html/2501.02341v1#bib.bib59)、[92](https://arxiv.org/html/2501.02341v1#bib.bib92)]。
- en: In practice, this module often integrates features such as vibration damping,
    thermal management, and secure mounting solutions to protect delicate components
    and maintain optimal functionality during flight. Moreover, in some UAV designs,
    the Payload Module is designed to be interchangeable. This modular approach, which
    typically employs standardized mounting and connectivity interfaces, enables rapid
    swapping of payloads and streamlines the process of configuring a UAV for different
    mission profiles. As a result, operators can expand the drone’s capabilities without
    requiring an entirely new platform, thereby enhancing flexibility and reducing
    both deployment time and cost [[93](https://arxiv.org/html/2501.02341v1#bib.bib93),
    [94](https://arxiv.org/html/2501.02341v1#bib.bib94), [95](https://arxiv.org/html/2501.02341v1#bib.bib95),
    [96](https://arxiv.org/html/2501.02341v1#bib.bib96)] .
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，该模块通常集成了诸如振动缓解、热管理和安全安装解决方案等功能，以保护精密组件并在飞行过程中保持最佳功能。此外，在某些无人机设计中，有效载荷模块被设计为可互换的。这种模块化方法通常采用标准化的安装和连接接口，可以快速更换有效载荷，并简化根据不同任务配置无人机的过程。因此，操作员可以在无需完全新平台的情况下扩展无人机的能力，从而提高灵活性并降低部署时间和成本[[93](https://arxiv.org/html/2501.02341v1#bib.bib93)、[94](https://arxiv.org/html/2501.02341v1#bib.bib94)、[95](https://arxiv.org/html/2501.02341v1#bib.bib95)、[96](https://arxiv.org/html/2501.02341v1#bib.bib96)]。
- en: Overall, the Payload Module plays a crucial role in bridging the UAV’s core
    flight systems with the mission-specific tools essential for achieving operational
    objectives. By accommodating a wide range of payloads and ensuring they are powered,
    protected, and efficiently connected, the Payload Module significantly extends
    the UAV’s applicability across various industries and mission types.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，有效载荷模块在连接无人机的核心飞行系统与实现操作目标所需的任务特定工具之间起着至关重要的作用。通过容纳各种有效载荷，并确保它们的电力供应、保护和高效连接，有效载荷模块显著扩展了无人机在各行业和任务类型中的应用。
- en: '![Refer to caption](img/34b0b86301038b0606ca2f7463bba401.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/34b0b86301038b0606ca2f7463bba401.png)'
- en: 'Figure 2: Key Functional modules of UAV systems'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：无人机系统的关键功能模块
- en: 2.2 Embodied Configurations of UAVs
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 无人机的体型配置
- en: UAVs can be categorized into several types based on their geometric configurations.
    These include fixed-wing UAVs, multirotor UAVs, unmanned helicopters, and other
    types. Below, we introduce these categories and summarize their characteristics.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机可以根据其几何结构分为几种类型，包括固翼无人机、多旋翼无人机、无人直升机等。以下，我们将介绍这些分类并总结其特点。
- en: 2.2.1 Fixed-Wing UAVs
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 固翼无人机
- en: Fixed-wing UAVs feature a predetermined wing shape that generates lift as air
    flows over the wings, enabling forward motion [[92](https://arxiv.org/html/2501.02341v1#bib.bib92)].
    These UAVs are known for their high speed, long endurance, and stable flight,
    making them ideal for long-duration missions. However, they require advanced piloting
    skills and cannot perform hovering [[97](https://arxiv.org/html/2501.02341v1#bib.bib97)].
    Fixed-wing UAVs are commonly used for monitoring fields, forests, highways, and
    railways [[92](https://arxiv.org/html/2501.02341v1#bib.bib92)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 固翼无人机具有预定的机翼形状，随着空气流过机翼产生升力，从而实现向前运动[[92](https://arxiv.org/html/2501.02341v1#bib.bib92)]。这些无人机以高速、长续航和稳定飞行著称，适用于长时间任务。然而，它们需要先进的飞行技能，并且无法悬停[[97](https://arxiv.org/html/2501.02341v1#bib.bib97)]。固翼无人机通常用于监控田野、森林、高速公路和铁路[[92](https://arxiv.org/html/2501.02341v1#bib.bib92)]。
- en: 2.2.2 Multirotor UAVs
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 多旋翼无人机
- en: Multirotor UAVs are one of the most prevalent types in daily life, typically
    equipped with multiple rotors (commonly four, six, or more) to generate lift through
    rotor rotation. Their advantages include low cost, ease of operation, and the
    ability for vertical take-off and landing (VTOL) and hovering, making them suitable
    for precision tasks. However, they have limited endurance and relatively low speed.
    Multirotor UAVs are often used for tasks such as photography, agricultural monitoring,
    and spraying.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 多旋翼无人机是日常生活中最常见的无人机类型之一，通常配备多个旋翼（通常为四个、六个或更多），通过旋翼旋转产生升力。它们的优点包括低成本、易操作，并且能够进行垂直起降（VTOL）和悬停，使其适用于精密任务。然而，它们的续航时间有限，速度相对较低。多旋翼无人机通常用于摄影、农业监测和喷洒等任务。
- en: 2.2.3 Unmanned Helicopters
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3 无人直升机
- en: Unmanned helicopters are equipped with one or two powered rotors to provide
    lift and enable attitude control. This design allows for vertical take-off, hovering,
    and high maneuverability. Compared to multirotor UAVs, unmanned helicopters have
    superior payload capacity, enabling them to carry heavier equipment or sensors.
    Their strengths include long endurance and excellent wind resistance, making them
    stable even in strong winds. The main limitation is their relatively low speed.
    Unmanned helicopters find widespread applications in areas such as traffic surveillance,
    resource exploration, forest fire prevention, and military reconnaissance.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 无人直升机配备一个或两个动力旋翼提供升力并实现姿态控制。这种设计允许垂直起飞、悬停和高机动性。与多旋翼无人机相比，无人直升机具有更强的载重能力，可以携带更重的设备或传感器。其优势包括较长的续航时间和出色的抗风能力，即使在强风中也能保持稳定。主要的局限性是其相对较低的速度。无人直升机广泛应用于交通监控、资源勘探、森林防火和军事侦察等领域。
- en: 2.2.4 Hybrid UAV
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4 混合型无人机
- en: Hybrid UAVs combine the strengths of both fixed-wing and multirotor UAVs, offering
    a versatile design that allows for VTOL while also achieving the long endurance
    and high speed typical of fixed-wing UAVs. These UAVs typically feature a combination
    of rotors for lift during vertical flight and wings for sustained forward motion.
    The main advantage of hybrid UAVs is their flexibility, enabling them to perform
    a wide range of missions, including those requiring both hovering and long-duration
    flight. However, the complexity of their design and mechanisms results in higher
    costs and more demanding maintenance.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 混合型无人机结合了固定翼无人机和多旋翼无人机的优点，提供了一种多功能的设计，既能实现垂直起降（VTOL），又能达到固定翼无人机典型的长续航和高速。这些无人机通常配备旋翼用于垂直飞行时的升力，机翼用于持续的前向飞行。混合型无人机的主要优势在于其灵活性，能够执行多种任务，包括同时要求悬停和长时间飞行的任务。然而，其设计和机制的复杂性导致了更高的成本和更苛刻的维护要求。
- en: 2.2.5 Flapping-Wing UAV
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.5 拍翅式无人机
- en: Flapping-wing UAVs are bio-inspired unmanned aerial vehicles that mimic the
    flight mechanisms of birds or insects. These UAVs rely on unsteady aerodynamic
    effects generated by wing flapping to achieve flight. They offer quieter operation,
    higher efficiency, and increased maneuverability compared to conventional UAVs.
    Their compact size is a notable advantage, but they generally have a lower payload
    capacity. Additionally, the design and control systems of flapping-wing UAVs are
    more complex due to the dynamic nature of their flight mechanics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 拍翅式无人机是受生物启发的无人飞行器，模仿鸟类或昆虫的飞行机制。这些无人机依靠翼拍产生的不稳定气动效应来实现飞行。与传统无人机相比，它们提供更安静的操作、更高的效率和更强的机动性。它们的小巧尺寸是一个显著优势，但通常具有较低的载重能力。此外，由于飞行力学的动态特性，拍翅式无人机的设计和控制系统较为复杂。
- en: 2.2.6 Unmanned Airship
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.6 无人飞艇
- en: Unmanned airships are a type of aerial vehicle that utilizes lightweight gases
    for buoyancy and employs propulsion and external structural elements for movement
    and directional control. These airships are highly cost-effective and produce
    low flight noise. However, their agility is limited, and they operate at relatively
    low speeds. Due to their large size, unmanned airships are highly susceptible
    to wind influences, which can affect their stability and operational reliability.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 无人飞艇是一种利用轻质气体提供浮力，并使用推进装置和外部结构元素来实现运动和方向控制的空中飞行器。这些飞艇具有高度的成本效益，并且飞行噪音较低。然而，它们的敏捷性有限，飞行速度相对较低。由于其体积较大，无人飞艇非常容易受到风力的影响，这可能会影响其稳定性和操作可靠性。
- en: 'Table 1: Typical configurations of UAV'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：无人机典型配置
- en: '| Category | Characteristics | Advantages | Disadvantages |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 特点 | 优点 | 缺点 |'
- en: '| Fixed-wing UAV | Fixed wings generate lift with forward motion. | High speed,
    long endurance, stable flight. | Cannot hover, high demands for takeoff/landing
    areas. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 固定翼无人机 | 通过前进的运动产生升力。 | 高速，长续航，稳定飞行。 | 无法悬停，对起降区域要求高。 |'
- en: '| Multirotor UAV | Multiple rotors provide lift and control. | Low cost, easy
    operation, capable of VTOL and hovering. | Limited flight time, low speed, small
    payload capacity. |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 多旋翼无人机 | 多个旋翼提供升力和控制。 | 低成本，操作简单，能够垂直起降和悬停。 | 飞行时间有限，速度较低，载荷能力小。 |'
- en: '| Unmanned Helicopter | Single or dual rotors allow vertical take-off and hovering.
    | High payload capacity, good wind resistance, long endurance, VTOL. | Complex
    structure, higher maintenance cost, slower than fixed-wing UAVs. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 无人直升机 | 单个或双旋翼可以实现垂直起飞和悬停。 | 高载荷能力，良好的抗风性，长续航，垂直起降。 | 结构复杂，维护成本较高，比固定翼无人机飞行速度慢。
    |'
- en: '| Hybrid UAV | Combines fixed-wing and multirotor capabilities. | Flexible
    missions, long endurance, VTOL. | Complex mechanisms, higher cost. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 混合型无人机 | 结合了固定翼和多旋翼的能力。 | 灵活的任务，长续航，垂直起降。 | 机制复杂，成本较高。 |'
- en: '| Flapping-wing UAV | Uses clap-and-fling mechanism for flight. | Low noise,
    high propulsion efficiency, high maneuverability. | Complex analysis and control,
    limited payload capacity. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 拍打翼无人机 | 使用拍击和抛掷机制进行飞行。 | 低噪音，高推进效率，高机动性。 | 分析和控制复杂，有限的载荷能力。 |'
- en: '| Unmanned airship | Aerostat aircraft with gasbag for lift. | Low cost, low
    noise. | low speed, low maneuverability, highly affected by wind. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 无人飞艇 | 采用气囊升力的气体飞艇。 | 低成本，低噪音。 | 低速，低机动性，易受风影响。 |'
- en: 2.3 UAV Swarm
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 无人机群
- en: UAV swarms involve multiple UAVs working collaboratively to achieve a shared
    objective, offering advantages in redundancy, scalability, and efficiency compared
    to individual UAV operations[[98](https://arxiv.org/html/2501.02341v1#bib.bib98)].
    The swarm approach relies on decentralized decision-making, allowing UAVs to adjust
    their behaviors in response to the actions of their peers and environmental changes.
    Swarm algorithms often draw inspiration from biological systems[[99](https://arxiv.org/html/2501.02341v1#bib.bib99)],
    such as flocks of birds or colonies of ants, utilizing techniques like consensus
    algorithms[[100](https://arxiv.org/html/2501.02341v1#bib.bib100)], particle swarm
    optimization[[101](https://arxiv.org/html/2501.02341v1#bib.bib101)], or behavior-based
    coordination[[102](https://arxiv.org/html/2501.02341v1#bib.bib102)]. Effective
    swarm operation requires seamless communication, robust control mechanisms, and
    cooperative planning to manage the complexities of distributed systems[[103](https://arxiv.org/html/2501.02341v1#bib.bib103)].
    This concept is particularly useful in applications like large-area surveillance,
    precision agriculture, and search and rescue, where multiple UAVs can cover a
    greater area more efficiently than a single vehicle.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机群是指多个无人机协作完成共同目标，相比单一无人机操作，具有冗余性、可扩展性和效率上的优势[[98](https://arxiv.org/html/2501.02341v1#bib.bib98)]。群体方法依赖于去中心化的决策机制，使得无人机能够根据同伴的行为和环境变化调整自身的行为。群体算法通常受到生物系统的启发[[99](https://arxiv.org/html/2501.02341v1#bib.bib99)]，如鸟群或蚂蚁群体，使用的技术包括共识算法[[100](https://arxiv.org/html/2501.02341v1#bib.bib100)]、粒子群优化[[101](https://arxiv.org/html/2501.02341v1#bib.bib101)]或基于行为的协调[[102](https://arxiv.org/html/2501.02341v1#bib.bib102)]。有效的群体操作需要无缝的通信、强大的控制机制和合作规划，以应对分布式系统的复杂性[[103](https://arxiv.org/html/2501.02341v1#bib.bib103)]。该概念在大范围监视、精确农业和搜索救援等应用中尤为重要，在这些场景下，多个无人机能够比单一飞行器更高效地覆盖更广的区域。
- en: In this section, we will discuss key components essential for effective UAV
    swarm operation, including task allocation, communication architecture, path planning,
    and formation control.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论有效无人机群操作的关键组成部分，包括任务分配、通信架构、路径规划和编队控制。
- en: 2.3.1 Task allocation in UAV swarm
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 无人机群中的任务分配
- en: UAV swarms can be regarded as a team tasked with executing a given mission,
    consisting of a set of tasks, and they must be responsible for task allocation
    among their members internally[[104](https://arxiv.org/html/2501.02341v1#bib.bib104)].
    Task allocation is one of the most important problems in UAV swarm operations,
    as it directly affects the efficiency of the mission. It has been proven that
    finding the optimal solution to the task allocation problem is NP-hard, and the
    difficulty increases exponentially with the scale of the UAV swarm and the number
    of tasks[[105](https://arxiv.org/html/2501.02341v1#bib.bib105)]. Typically, the
    UAV swarm task allocation problem is modeled as a Traveling Salesman Problem (TSP)[[106](https://arxiv.org/html/2501.02341v1#bib.bib106)]
    or Vehicle Routing Problem (VRP)[[107](https://arxiv.org/html/2501.02341v1#bib.bib107)],
    mixed-integer linear programming (MILP) model[[108](https://arxiv.org/html/2501.02341v1#bib.bib108)],
    or a cooperative multi-task allocation problem (CMTAP) model[[109](https://arxiv.org/html/2501.02341v1#bib.bib109)].
    Commonly used algorithms include heuristic algorithms, AI-based methods, mathematical
    programming methods, and market mechanism-based algorithm.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: UAV群体可以视为一个团队，负责执行特定任务，该任务由一系列子任务组成，并且它们必须负责成员之间的任务分配[[104](https://arxiv.org/html/2501.02341v1#bib.bib104)]。任务分配是UAV群体操作中的一个重要问题，因为它直接影响任务的效率。已经证明，解决任务分配问题的最优解是NP难的，并且随着UAV群体规模和任务数量的增加，难度呈指数增长[[105](https://arxiv.org/html/2501.02341v1#bib.bib105)]。通常，UAV群体任务分配问题被建模为旅行商问题（TSP）[[106](https://arxiv.org/html/2501.02341v1#bib.bib106)]、车辆路径问题（VRP）[[107](https://arxiv.org/html/2501.02341v1#bib.bib107)]、混合整数线性规划（MILP）模型[[108](https://arxiv.org/html/2501.02341v1#bib.bib108)]，或合作式多任务分配问题（CMTAP）模型[[109](https://arxiv.org/html/2501.02341v1#bib.bib109)]。常用的算法包括启发式算法、基于人工智能的方法、数学规划方法和基于市场机制的算法。
- en: Heuristic algorithms commonly applied to the task allocation problem include
    Genetic Algorithms (GA), Particle Swarm Optimization (PSO), and Simulated Annealing
    (SA), among others. These algorithms use random search methods to find feasible
    solutions, preventing the problem from getting trapped in local optima. Genetic
    algorithms generate random solutions through crossover and mutation strategies,
    and then iteratively improve these solutions by selecting the best candidates.
    Eventually, they converge to a solution close to the optimal. For example, Han
    et al.[[110](https://arxiv.org/html/2501.02341v1#bib.bib110)] proposed a Fuzzy
    Elite Strategy Genetic Algorithm (FESGA) to efficiently solve complex task allocation
    problems. Yan et al.[[111](https://arxiv.org/html/2501.02341v1#bib.bib111)] proposed
    an improved genetic algorithm to address the integrated task allocation and path
    planning problem for multi-UAVs attacking multiple targets.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 常用于任务分配问题的启发式算法包括遗传算法（GA）、粒子群优化（PSO）、模拟退火（SA）等。这些算法通过随机搜索方法寻找可行解，避免了问题陷入局部最优解。遗传算法通过交叉和变异策略生成随机解，然后通过选择最优候选解不断改进这些解，最终收敛到接近最优的解。例如，Han等人[[110](https://arxiv.org/html/2501.02341v1#bib.bib110)]提出了一种模糊精英策略遗传算法（FESGA），用于高效解决复杂的任务分配问题。Yan等人[[111](https://arxiv.org/html/2501.02341v1#bib.bib111)]提出了一种改进的遗传算法，用于解决多UAV攻击多个目标的集成任务分配和路径规划问题。
- en: PSO balances exploration and exploitation processes and iteratively produces
    solutions that approach the optimal. The algorithm is simple to implement, fast,
    and widely applied in task allocation problems. Jiang et al.[[112](https://arxiv.org/html/2501.02341v1#bib.bib112)]
    proposed an improved PSO algorithm to solve multi-constraint task allocation problems,
    which is suitable for solving complex combinatorial optimization problems. Since
    most research treats multi-UAV task allocation as a single-objective optimization
    problem, Gao et al.[[113](https://arxiv.org/html/2501.02341v1#bib.bib113)] applied
    an improved Multi-Objective PSO (MOPSO) algorithm to solve the task allocation
    problem for multiple UAVs.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 粒子群优化（PSO）平衡探索和开发过程，并通过迭代产生接近最优的解。该算法易于实现、运行快速，广泛应用于任务分配问题中。姜等人[[112](https://arxiv.org/html/2501.02341v1#bib.bib112)]提出了一种改进的PSO算法，用于解决多约束任务分配问题，该算法适用于解决复杂的组合优化问题。由于大多数研究将多UAV任务分配视为单目标优化问题，高等人[[113](https://arxiv.org/html/2501.02341v1#bib.bib113)]应用了一种改进的多目标粒子群优化（MOPSO）算法来解决多个UAV的任务分配问题。
- en: Mathematical programming methods offer the advantage of finding precise optimal
    solutions, but their solving time increases exponentially as the problem size
    grows. Choi et al.[[114](https://arxiv.org/html/2501.02341v1#bib.bib114)] modeled
    the task allocation problem, where multiple UAVs perform tasks across multiple
    targets, as a Mixed-Integer Linear Programming (MILP) model for solution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数学规划方法具有找到精确最优解的优势，但随着问题规模的增大，求解时间呈指数增长。Choi等人[[114](https://arxiv.org/html/2501.02341v1#bib.bib114)]将多无人机在多个目标之间执行任务的任务分配问题建模为一个混合整数线性规划（MILP）模型进行求解。
- en: AI-based methods, such as reinforcement learning and artificial neural networks,
    are advantageous in extracting environmental features. Yang et al.[[115](https://arxiv.org/html/2501.02341v1#bib.bib115)]
    proposed a task scheduling algorithm based on reinforcement learning, enabling
    UAVs to dynamically adjust their task strategies based on their task performance
    efficiency calculations. Yin et al.[[116](https://arxiv.org/html/2501.02341v1#bib.bib116)]
    applied deep transfer reinforcement learning to the multi-UAV task allocation
    problem.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基于人工智能的方法，如强化学习和人工神经网络，在提取环境特征方面具有优势。Yang等人[[115](https://arxiv.org/html/2501.02341v1#bib.bib115)]提出了一种基于强化学习的任务调度算法，使无人机能够根据任务执行效率的计算动态调整任务策略。Yin等人[[116](https://arxiv.org/html/2501.02341v1#bib.bib116)]将深度迁移强化学习应用于多无人机任务分配问题。
- en: Market mechanism-based methods are classical distributed approaches to task
    allocation, including auction-based algorithms and the Contract Net Protocol (CNP)
    algorithm[[117](https://arxiv.org/html/2501.02341v1#bib.bib117)]. These methods
    leverage incentive mechanisms to encourage agents to participate in the task allocation
    process, where agents bid based on the utility or cost of the task, aiming to
    maximize utility or minimize cost. The primary advantage of these methods is their
    computational efficiency[[118](https://arxiv.org/html/2501.02341v1#bib.bib118)].
    Qiao et al.[[119](https://arxiv.org/html/2501.02341v1#bib.bib119)] applied an
    auction algorithm for dynamic UAV task allocation, addressing the multi-constraint,
    multi-UAV dynamic task allocation problem. Duan et al.[[120](https://arxiv.org/html/2501.02341v1#bib.bib120)]
    proposed a hybrid “two-stage” auction algorithm with a hierarchical decision mechanism
    and improved objective functions, which enhanced performance in multi-UAV dynamic
    task allocation by simultaneously achieving heterogeneous task allocation and
    obstacle avoidance under resource constraints. This algorithm outperforms many
    state-of-the-art models in terms of efficiency and robustness. Zhang et al.[[121](https://arxiv.org/html/2501.02341v1#bib.bib121)]
    developed a multi-constraint model for dynamic multi-UAV task scheduling, focusing
    on maximizing total task profit, minimizing time consumption, and balancing task
    distribution among UAVs. They introduced a hybrid Contract Net Protocol-based
    task scheduling method, including buy-sell, swap, and replacement contracts. Wang
    et al.[[122](https://arxiv.org/html/2501.02341v1#bib.bib122)] proposed a two-stage
    distributed task allocation algorithm (TS-DTA) based on CNP, effectively addressing
    the task reallocation problem in dynamic environments.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基于市场机制的方法是经典的分布式任务分配方法，包括基于拍卖的算法和合同网协议（CNP）算法[[117](https://arxiv.org/html/2501.02341v1#bib.bib117)]。这些方法利用激励机制鼓励代理参与任务分配过程，代理根据任务的效用或成本进行竞标，旨在最大化效用或最小化成本。这些方法的主要优势在于其计算效率[[118](https://arxiv.org/html/2501.02341v1#bib.bib118)]。Qiao等人[[119](https://arxiv.org/html/2501.02341v1#bib.bib119)]应用了一种拍卖算法用于动态无人机任务分配，解决了多约束、多无人机动态任务分配问题。Duan等人[[120](https://arxiv.org/html/2501.02341v1#bib.bib120)]提出了一种混合的“二阶段”拍卖算法，该算法具有分层决策机制和改进的目标函数，在多无人机动态任务分配中通过同时实现异构任务分配和障碍规避，提升了性能。该算法在效率和鲁棒性方面优于许多最先进的模型。Zhang等人[[121](https://arxiv.org/html/2501.02341v1#bib.bib121)]开发了一种用于动态多无人机任务调度的多约束模型，重点优化了总任务利润的最大化、时间消耗的最小化以及任务在无人机之间的平衡分配。他们引入了一种基于混合合同网协议的任务调度方法，包括买卖、交换和替代合同。Wang等人[[122](https://arxiv.org/html/2501.02341v1#bib.bib122)]提出了一种基于CNP的二阶段分布式任务分配算法（TS-DTA），有效解决了动态环境下的任务重新分配问题。
- en: 2.3.2 Communication architecture in UAV swarm
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 无人机群体的通信架构
- en: 'For UAV swarms, communication is essential for coordination, enabling collaborative
    work and maintaining stability during operations. Communication can be achieved
    through two main approaches: infrastructure-based architectures [[123](https://arxiv.org/html/2501.02341v1#bib.bib123)]
    and Flying Ad-hoc Network (FANET) architectures [[124](https://arxiv.org/html/2501.02341v1#bib.bib124)].
    Each method offers unique advantages and challenges, which will be discussed below.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无人机群体，通信对于协调至关重要，它使得协同工作成为可能，并保持操作过程中的稳定性。通信可以通过两种主要方式实现：基于基础设施的架构[[123](https://arxiv.org/html/2501.02341v1#bib.bib123)]和飞行自组织网络（FANET）架构[[124](https://arxiv.org/html/2501.02341v1#bib.bib124)]。每种方法都有其独特的优点和挑战，以下将进行讨论。
- en: 'Infrastructure-based Architectures: This architecture depends on ground control
    stations (GCS) [[70](https://arxiv.org/html/2501.02341v1#bib.bib70)] to manage
    the swarm. The GCS collects telemetry data from UAVs and transmits commands, either
    in real-time or through pre-programmed instructions. Its key advantages include
    centralized computation and real-time optimization, eliminating the need for inter-drone
    communication networks [[123](https://arxiv.org/html/2501.02341v1#bib.bib123)].
    However, this approach has notable limitations: the entire system is vulnerable
    to single-point failures in the GCS, UAVs must remain within the GCS communication
    range, and the architecture lacks the flexibility of distributed decision-making
    [[123](https://arxiv.org/html/2501.02341v1#bib.bib123)].'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 基于基础设施的架构：该架构依赖地面控制站（GCS）[[70](https://arxiv.org/html/2501.02341v1#bib.bib70)]来管理无人机群。GCS收集来自无人机的遥测数据并传输命令，可以是实时的，也可以是通过预先编程的指令。其主要优势包括集中计算和实时优化，避免了无人机间通信网络的需求[[123](https://arxiv.org/html/2501.02341v1#bib.bib123)]。然而，这种方法也有明显的局限性：整个系统容易受到GCS单点故障的影响，无人机必须保持在GCS的通信范围内，并且该架构缺乏分布式决策的灵活性[[123](https://arxiv.org/html/2501.02341v1#bib.bib123)]。
- en: 'Flying Ad-hoc Network (FANET) Architecture: FANETs consist of UAVs communicating
    directly with one another without needing a central access point. This decentralized
    network enables UAVs to coordinate tasks autonomously, with at least one UAV maintaining
    a link to a ground base or satellite. FANETs benefit from flexibility, scalability,
    and reduced dependency on infrastructure. However, they require robust communication
    protocols and may face challenges in managing dynamic topologies and ensuring
    reliability [[124](https://arxiv.org/html/2501.02341v1#bib.bib124)].'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 飞行自组织网络（FANET）架构：FANET由无人机直接相互通信组成，无需中心接入点。这种去中心化的网络使得无人机能够自主协调任务，至少有一架无人机保持与地面基站或卫星的链接。FANET具有灵活性、可扩展性和减少对基础设施依赖的优势。然而，它们需要强大的通信协议，并可能面临动态拓扑管理和确保可靠性的挑战[[124](https://arxiv.org/html/2501.02341v1#bib.bib124)]。
- en: 2.3.3 Path planning in UAV swarm
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3 无人机群体路径规划
- en: 'UAV swarm path planning refers to selecting an optimal path for the UAV swarm
    from the starting position to all target positions, while ensuring the predefined
    distance between UAVs to avoid collisions[[125](https://arxiv.org/html/2501.02341v1#bib.bib125)].
    The optimal path generally refers to the shortest path length, shortest travel
    time, least energy consumption, and other event-specific constraints[[125](https://arxiv.org/html/2501.02341v1#bib.bib125)].
    The criteria for the optimal path need to be determined based on the actual problem.
    UAV path planning algorithms can generally be divided into three major categories:
    intelligent optimization algorithms, mathematical programming methods, and AI-based
    approaches. Below, we briefly introduce these three methods.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: UAV群体路径规划是指从起始位置到所有目标位置选择最佳路径，同时确保无人机之间的预定距离以避免碰撞[[125](https://arxiv.org/html/2501.02341v1#bib.bib125)]。最佳路径通常指的是最短路径长度、最短旅行时间、最小能量消耗以及其他特定事件的约束条件[[125](https://arxiv.org/html/2501.02341v1#bib.bib125)]。最佳路径的标准需要根据实际问题来确定。无人机路径规划算法通常可以分为三大类：智能优化算法、数学规划方法和基于人工智能的方法。以下简要介绍这三种方法。
- en: In nature, various group behaviors, such as flocks of birds, schools of fish,
    and ant colonies, follow specific rules that enable efficient food searching or
    migration. These behaviors can be abstracted into mathematical models for information
    transfer, path planning, and coordinated control, which are applicable to UAV
    swarm scheduling. Common intelligent optimization algorithms for UAV swarms include
    Ant Colony Optimization (ACO), Genetic Algorithms (GA), Simulated Annealing (SA),
    and Particle Swarm Optimization (PSO). For instance, ACO mimics the foraging behavior
    of ants, where ants probabilistically select paths based on pheromone concentration,
    ultimately finding optimal or near-optimal solutions. Researchers such as Turker
    et al.[[126](https://arxiv.org/html/2501.02341v1#bib.bib126)] have applied SA
    to UAV swarm path planning, while Wei et al.[[127](https://arxiv.org/html/2501.02341v1#bib.bib127)]
    used ACO for the same purpose.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然界中，各种群体行为，如鸟群、鱼群和蚂蚁群体，遵循特定的规则，这些规则使得它们能够高效地寻找食物或迁徙。这些行为可以抽象为信息传递、路径规划和协调控制的数学模型，这些模型也适用于无人机群体调度。常见的智能优化算法包括蚁群优化（ACO）、遗传算法（GA）、模拟退火（SA）和粒子群优化（PSO）。例如，ACO
    模拟蚂蚁的觅食行为，其中蚂蚁基于信息素浓度以概率方式选择路径，最终找到最优或近似最优解。像 Turker 等人[[126](https://arxiv.org/html/2501.02341v1#bib.bib126)]这样的研究人员已将
    SA 应用于无人机群体路径规划，而 Wei 等人[[127](https://arxiv.org/html/2501.02341v1#bib.bib127)]则使用
    ACO 进行相同的工作。
- en: Beyond heuristic algorithms inspired by nature, mathematical models like Mixed-Integer
    Linear Programming (MILP) and Nonlinear Programming can be directly applied to
    UAV swarm scheduling for precise solutions. For example, Ragi et al.[[128](https://arxiv.org/html/2501.02341v1#bib.bib128)]
    used Mixed-Integer Nonlinear Programming (MINLP) to address UAV path planning.
    While these methods are effective for small-scale problems, their computational
    complexity increases exponentially as the problem size grows.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了受到自然启发的启发式算法外，像混合整数线性规划（MILP）和非线性规划等数学模型也可以直接应用于无人机群体调度，以提供精确的解决方案。例如，Ragi
    等人[[128](https://arxiv.org/html/2501.02341v1#bib.bib128)]使用混合整数非线性规划（MINLP）来解决无人机路径规划问题。尽管这些方法对于小规模问题是有效的，但随着问题规模的增加，它们的计算复杂度会呈指数级增长。
- en: With the rise of machine learning, AI-based algorithms have also been applied
    to UAV swarm scheduling and optimization. Kool et al.[[129](https://arxiv.org/html/2501.02341v1#bib.bib129)]
    used deep learning for vehicle routing, and similar approaches have been adopted
    in UAV swarm path planning. Xia et al.[[130](https://arxiv.org/html/2501.02341v1#bib.bib130)]
    applied neural networks to UAV path planning, Sanna et al.[[131](https://arxiv.org/html/2501.02341v1#bib.bib131)]
    extended this to multi-UAV planning, and Puente-Castro et al.[[54](https://arxiv.org/html/2501.02341v1#bib.bib54)]
    applied reinforcement learning. By training on extensive datasets, neural networks
    can learn to model the environment, including obstacles and their dynamic changes,
    thereby improving the accuracy of path planning.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的兴起，基于人工智能的算法也已被应用于无人机群体调度和优化。Kool 等人[[129](https://arxiv.org/html/2501.02341v1#bib.bib129)]使用深度学习进行车辆路径规划，类似的方法也已被应用于无人机群体路径规划。Xia
    等人[[130](https://arxiv.org/html/2501.02341v1#bib.bib130)]将神经网络应用于无人机路径规划，Sanna
    等人[[131](https://arxiv.org/html/2501.02341v1#bib.bib131)]将其扩展到多无人机规划，而 Puente-Castro
    等人[[54](https://arxiv.org/html/2501.02341v1#bib.bib54)]则应用了强化学习。通过在大规模数据集上进行训练，神经网络能够学习建模环境，包括障碍物及其动态变化，从而提高路径规划的准确性。
- en: 2.3.4 UAV Swarm Formation Control Algorithm
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.4 无人机编队控制算法
- en: The UAV swarm relies on effective formation control algorithms that enable it
    to autonomously form and maintain a formation to perform tasks, and switch or
    rebuild the formation based on specific tasks[[132](https://arxiv.org/html/2501.02341v1#bib.bib132)].
    The primary approaches to formation control are centralized, decentralized, and
    distributed control algorithms [[133](https://arxiv.org/html/2501.02341v1#bib.bib133)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机群体依赖有效的编队控制算法，使其能够自主地形成和维持编队来执行任务，并根据特定任务切换或重建编队[[132](https://arxiv.org/html/2501.02341v1#bib.bib132)]。编队控制的主要方法包括集中控制、分散控制和分布式控制算法[[133](https://arxiv.org/html/2501.02341v1#bib.bib133)]。
- en: 'Centralized Control: Centralized control involves a central unit that oversees
    task allocation and resource management, with individual UAVs primarily responsible
    for data input, output, and storage [[132](https://arxiv.org/html/2501.02341v1#bib.bib132)].
    This approach simplifies decision-making, ensures coordinated actions, and is
    relatively straightforward to implement. However, it is susceptible to high communication
    overhead and single points of failure; if the central unit fails, the entire system
    may collapse. Common methods in centralized control include virtual structure
    [[134](https://arxiv.org/html/2501.02341v1#bib.bib134)] and leader-follower approaches
    [[135](https://arxiv.org/html/2501.02341v1#bib.bib135), [136](https://arxiv.org/html/2501.02341v1#bib.bib136)].'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 集中式控制：集中式控制涉及一个中央单元，负责任务分配和资源管理，个别无人机主要负责数据输入、输出和存储[[132](https://arxiv.org/html/2501.02341v1#bib.bib132)]。这种方法简化了决策过程，确保了协调行动，并且相对容易实施。然而，它容易受到高通信开销和单点故障的影响；如果中央单元出现故障，整个系统可能崩溃。集中式控制中的常见方法包括虚拟结构[[134](https://arxiv.org/html/2501.02341v1#bib.bib134)]和领导-跟随方法[[135](https://arxiv.org/html/2501.02341v1#bib.bib135),
    [136](https://arxiv.org/html/2501.02341v1#bib.bib136)]。
- en: 'Decentralized Control: In decentralized control, each UAV makes decisions based
    on local sensors and controllers, without requiring explicit communication with
    other UAVs [[137](https://arxiv.org/html/2501.02341v1#bib.bib137)]. UAVs adjust
    their movements to maintain formation based on local conditions and predefined
    rules. The primary advantages of this approach include flexibility and ease of
    adapting formations. However, the lack of access to global information results
    in poor control performance, requiring continuous iteration[[138](https://arxiv.org/html/2501.02341v1#bib.bib138)].'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 分散式控制：在分散式控制中，每个无人机根据局部传感器和控制器做出决策，无需与其他无人机进行显式通信[[137](https://arxiv.org/html/2501.02341v1#bib.bib137)]。无人机根据局部条件和预定义规则调整其运动以保持队形。这种方法的主要优点包括灵活性和易于调整队形。然而，由于无法获取全局信息，控制性能较差，需要持续迭代[[138](https://arxiv.org/html/2501.02341v1#bib.bib138)]。
- en: 'Distributed Control: Distributed control involves extensive communication between
    UAVs, enabling them to coordinate and maintain formation through shared information.
    UAVs work collaboratively to make optimal decisions based on both local data and
    pre-established rules. Compared to decentralized control, distributed control
    benefits from more robust collaboration and improved flexibility. However, it
    requires higher communication demands and more complex algorithms to manage coordination,
    which increases both the computational burden and the risk of communication failures.
    Typical methods include behavior method[[139](https://arxiv.org/html/2501.02341v1#bib.bib139)]
    and consistency method[[140](https://arxiv.org/html/2501.02341v1#bib.bib140)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式控制：分布式控制涉及无人机之间的广泛通信，使它们能够通过共享信息进行协调并保持队形。无人机通过协作，基于局部数据和预设规则做出最优决策。与分散控制相比，分布式控制具有更强的协作性和更高的灵活性。然而，它需要更高的通信要求和更复杂的算法来管理协调，这增加了计算负担和通信故障的风险。典型方法包括行为方法[[139](https://arxiv.org/html/2501.02341v1#bib.bib139)]和一致性方法[[140](https://arxiv.org/html/2501.02341v1#bib.bib140)]。
- en: 3 Preliminaries on Foundation models
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 基础模型的前提
- en: This section provides an overview of FMs, including LLMs, Vision Foundation
    Models (VFMs), and Vision Language Models (VLMs). It highlights their core characteristics
    and technical advantages, with the aim of offering foundational insights and guidance
    for the deep integration of these models with UAV systems.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了基础模型（FMs），包括大型语言模型（LLMs）、视觉基础模型（VFMs）和视觉语言模型（VLMs）。重点介绍了它们的核心特性和技术优势，旨在为这些模型与无人机系统的深度集成提供基础性见解和指导。
- en: 'Table 2: Summarization of LLMs, VLMs, and VFMs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：LLMs、VLMs 和 VFMs 的总结。
- en: '| Category | Subcategory | Model Name | Institution / Author |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 子类别 | 模型名称 | 机构 / 作者 |'
- en: '| LLMs | General | GPT-3[[141](https://arxiv.org/html/2501.02341v1#bib.bib141)],
    GPT-3.5[[142](https://arxiv.org/html/2501.02341v1#bib.bib142)], GPT-4[[143](https://arxiv.org/html/2501.02341v1#bib.bib143)]
    | [OpenAI](https://openai.com) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLMs | 通用 | GPT-3[[141](https://arxiv.org/html/2501.02341v1#bib.bib141)],
    GPT-3.5[[142](https://arxiv.org/html/2501.02341v1#bib.bib142)], GPT-4[[143](https://arxiv.org/html/2501.02341v1#bib.bib143)]
    | [OpenAI](https://openai.com) |'
- en: '|  |  | Claude 2, Claude 3[[144](https://arxiv.org/html/2501.02341v1#bib.bib144),
    [145](https://arxiv.org/html/2501.02341v1#bib.bib145), [146](https://arxiv.org/html/2501.02341v1#bib.bib146)]
    | [Anthropic](https://www.anthropic.com) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Claude 2, Claude 3[[144](https://arxiv.org/html/2501.02341v1#bib.bib144),
    [145](https://arxiv.org/html/2501.02341v1#bib.bib145), [146](https://arxiv.org/html/2501.02341v1#bib.bib146)]
    | [Anthropic](https://www.anthropic.com) |'
- en: '|  |  | Mistral series[[147](https://arxiv.org/html/2501.02341v1#bib.bib147),
    [148](https://arxiv.org/html/2501.02341v1#bib.bib148)] | [Mistral AI](https://www.mistral.ai)
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Mistral 系列[[147](https://arxiv.org/html/2501.02341v1#bib.bib147), [148](https://arxiv.org/html/2501.02341v1#bib.bib148)]
    | [Mistral AI](https://www.mistral.ai) |'
- en: '|  |  | PaLM series[[149](https://arxiv.org/html/2501.02341v1#bib.bib149),
    [150](https://arxiv.org/html/2501.02341v1#bib.bib150)], Gemini series[[151](https://arxiv.org/html/2501.02341v1#bib.bib151),
    [152](https://arxiv.org/html/2501.02341v1#bib.bib152)] | [Google Research](https://ai.google)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  |  | PaLM 系列[[149](https://arxiv.org/html/2501.02341v1#bib.bib149), [150](https://arxiv.org/html/2501.02341v1#bib.bib150)],
    Gemini 系列[[151](https://arxiv.org/html/2501.02341v1#bib.bib151), [152](https://arxiv.org/html/2501.02341v1#bib.bib152)]
    | [Google Research](https://ai.google) |'
- en: '|  |  | LLaMA[[153](https://arxiv.org/html/2501.02341v1#bib.bib153)], LLaMA2[[154](https://arxiv.org/html/2501.02341v1#bib.bib154)],
    LLaMA3[[155](https://arxiv.org/html/2501.02341v1#bib.bib155)] | [Meta AI](https://ai.meta.com)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaMA[[153](https://arxiv.org/html/2501.02341v1#bib.bib153)], LLaMA2[[154](https://arxiv.org/html/2501.02341v1#bib.bib154)],
    LLaMA3[[155](https://arxiv.org/html/2501.02341v1#bib.bib155)] | [Meta AI](https://ai.meta.com)
    |'
- en: '|  |  | Vicuna[[156](https://arxiv.org/html/2501.02341v1#bib.bib156)] | [Vicuna
    Team](https://vicuna.lmsys.org) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Vicuna[[156](https://arxiv.org/html/2501.02341v1#bib.bib156)] | [Vicuna
    团队](https://vicuna.lmsys.org) |'
- en: '|  |  | Qwen series[[157](https://arxiv.org/html/2501.02341v1#bib.bib157),
    [158](https://arxiv.org/html/2501.02341v1#bib.bib158)] | [Qwen Team, Alibaba Group](https://github.com/QwenLM)
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Qwen 系列[[157](https://arxiv.org/html/2501.02341v1#bib.bib157), [158](https://arxiv.org/html/2501.02341v1#bib.bib158)]
    | [Qwen 团队，阿里巴巴集团](https://github.com/QwenLM) |'
- en: '|  |  | InternLM[[159](https://arxiv.org/html/2501.02341v1#bib.bib159)] | [Shanghai
    AI Laboratory](https://github.com/InternLM/InternLM) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  |  | InternLM[[159](https://arxiv.org/html/2501.02341v1#bib.bib159)] | [上海
    AI 实验室](https://github.com/InternLM/InternLM) |'
- en: '|  |  | BuboGPT[[160](https://arxiv.org/html/2501.02341v1#bib.bib160)] | [Bytedance](https://github.com/magic-research/bubogpt)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BuboGPT[[160](https://arxiv.org/html/2501.02341v1#bib.bib160)] | [Bytedance](https://github.com/magic-research/bubogpt)
    |'
- en: '|  |  | ChatGLM[[161](https://arxiv.org/html/2501.02341v1#bib.bib161), [162](https://arxiv.org/html/2501.02341v1#bib.bib162),
    [163](https://arxiv.org/html/2501.02341v1#bib.bib163)] | [Zhipu AI](https://github.com/THUDM)
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ChatGLM[[161](https://arxiv.org/html/2501.02341v1#bib.bib161), [162](https://arxiv.org/html/2501.02341v1#bib.bib162),
    [163](https://arxiv.org/html/2501.02341v1#bib.bib163)] | [Zhipu AI](https://github.com/THUDM)
    |'
- en: '|  |  | DeepSeek series[[164](https://arxiv.org/html/2501.02341v1#bib.bib164),
    [165](https://arxiv.org/html/2501.02341v1#bib.bib165), [166](https://arxiv.org/html/2501.02341v1#bib.bib166)]
    | [DeepSeek](https://github.com/deepseek-ai) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DeepSeek 系列[[164](https://arxiv.org/html/2501.02341v1#bib.bib164),
    [165](https://arxiv.org/html/2501.02341v1#bib.bib165), [166](https://arxiv.org/html/2501.02341v1#bib.bib166)]
    | [DeepSeek](https://github.com/deepseek-ai) |'
- en: '| VLMs | General | GPT-4V[[167](https://arxiv.org/html/2501.02341v1#bib.bib167)],
    GPT-4o, GPT-4o mini, GPT o1-preview | [OpenAI](https://openai.com) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| VLMs | General | GPT-4V[[167](https://arxiv.org/html/2501.02341v1#bib.bib167)],
    GPT-4o, GPT-4o mini, GPT o1-preview | [OpenAI](https://openai.com) |'
- en: '|  |  | Claude 3 Opus, Claude 3.5 Sonnet[[168](https://arxiv.org/html/2501.02341v1#bib.bib168)]
    | [Anthropic](https://www.anthropic.com) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Claude 3 Opus, Claude 3.5 Sonnet[[168](https://arxiv.org/html/2501.02341v1#bib.bib168)]
    | [Anthropic](https://www.anthropic.com) |'
- en: '|  |  | Step-2 | [Jieyue Xingchen](https://www.stepfun.com/) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Step-2 | [Jieyue Xingchen](https://www.stepfun.com/) |'
- en: '|  |  | LLaVA[[169](https://arxiv.org/html/2501.02341v1#bib.bib169)], LLaVA-1.5[[170](https://arxiv.org/html/2501.02341v1#bib.bib170)],
    LLaVA-NeXT[[171](https://arxiv.org/html/2501.02341v1#bib.bib171)] | [Liu *et al*.](https://github.com/haotian-liu/LLaVA)
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaVA[[169](https://arxiv.org/html/2501.02341v1#bib.bib169)], LLaVA-1.5[[170](https://arxiv.org/html/2501.02341v1#bib.bib170)],
    LLaVA-NeXT[[171](https://arxiv.org/html/2501.02341v1#bib.bib171)] | [Liu *等*](https://github.com/haotian-liu/LLaVA)
    |'
- en: '|  |  | MoE-LLaVA[[172](https://arxiv.org/html/2501.02341v1#bib.bib172)] |
    [Lin *et al*.](https://github.com/PKU-YuanGroup/MoE-LLaVA) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MoE-LLaVA[[172](https://arxiv.org/html/2501.02341v1#bib.bib172)] |
    [Lin *等*](https://github.com/PKU-YuanGroup/MoE-LLaVA) |'
- en: '|  |  | LLaVA-CoT[[173](https://arxiv.org/html/2501.02341v1#bib.bib173)] |
    [Xu *et al*.](https://github.com/PKU-YuanGroup/LLaVA-CoT) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLaVA-CoT[[173](https://arxiv.org/html/2501.02341v1#bib.bib173)] |
    [Xu *等*](https://github.com/PKU-YuanGroup/LLaVA-CoT) |'
- en: '|  |  | Flamingo[[174](https://arxiv.org/html/2501.02341v1#bib.bib174)] | [Alayrac
    *et al*.](https://github.com/mlfoundations/open_flamingo) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Flamingo[[174](https://arxiv.org/html/2501.02341v1#bib.bib174)] | [Alayrac
    *等*](https://github.com/mlfoundations/open_flamingo) |'
- en: '|  |  | BLIP[[175](https://arxiv.org/html/2501.02341v1#bib.bib175)] | [Li *et
    al*.](https://github.com/salesforce/BLIP) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLIP[[175](https://arxiv.org/html/2501.02341v1#bib.bib175)] | [Li *等人*](https://github.com/salesforce/BLIP)
    |'
- en: '|  |  | BLIP-2[[176](https://arxiv.org/html/2501.02341v1#bib.bib176)] | [Li
    *et al*.](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  |  | BLIP-2[[176](https://arxiv.org/html/2501.02341v1#bib.bib176)] | [Li
    *等人*](https://github.com/salesforce/LAVIS/tree/main/projects/blip2) |'
- en: '|  |  | InstructBLIP[[177](https://arxiv.org/html/2501.02341v1#bib.bib177)]
    | [Dai *et al*.](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |  | InstructBLIP[[177](https://arxiv.org/html/2501.02341v1#bib.bib177)]
    | [Dai *等人*](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)
    |'
- en: '|  | Video Understanding | LLaMA-VID[[178](https://arxiv.org/html/2501.02341v1#bib.bib178)]
    | [Li *et al*.](https://github.com/dvlab-research/LLaMA-VID) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | 视频理解 | LLaMA-VID[[178](https://arxiv.org/html/2501.02341v1#bib.bib178)]
    | [Li *等人*](https://github.com/dvlab-research/LLaMA-VID) |'
- en: '|  |  | IG-VLM[[179](https://arxiv.org/html/2501.02341v1#bib.bib179)] | [Kim
    *et al*.](https://github.com/imagegridworth/IG-VLM) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  |  | IG-VLM[[179](https://arxiv.org/html/2501.02341v1#bib.bib179)] | [Kim
    *等人*](https://github.com/imagegridworth/IG-VLM) |'
- en: '|  |  | Video-ChatGPT[[180](https://arxiv.org/html/2501.02341v1#bib.bib180)]
    | [Maaz *et al*.](https://github.com/mbzuai-oryx/Video-ChatGPT) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Video-ChatGPT[[180](https://arxiv.org/html/2501.02341v1#bib.bib180)]
    | [Maaz *等人*](https://github.com/mbzuai-oryx/Video-ChatGPT) |'
- en: '|  |  | VideoTree[[181](https://arxiv.org/html/2501.02341v1#bib.bib181)] |
    [Wang *et al*.](https://github.com/Ziyang412/VideoTree) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VideoTree[[181](https://arxiv.org/html/2501.02341v1#bib.bib181)] |
    [Wang *等人*](https://github.com/Ziyang412/VideoTree) |'
- en: '|  | Visual Reasoning | X-VLM[[182](https://arxiv.org/html/2501.02341v1#bib.bib182)]
    | [Zeng *et al*.](https://github.com/zengyan-97/X-VLM) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉推理 | X-VLM[[182](https://arxiv.org/html/2501.02341v1#bib.bib182)] |
    [Zeng *等人*](https://github.com/zengyan-97/X-VLM) |'
- en: '|  |  | Chameleon[[183](https://arxiv.org/html/2501.02341v1#bib.bib183)] |
    [Lu *et al*.](https://chameleon-llm.github.io/) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Chameleon[[183](https://arxiv.org/html/2501.02341v1#bib.bib183)] |
    [Lu *等人*](https://chameleon-llm.github.io/) |'
- en: '|  |  | HYDRA[[184](https://arxiv.org/html/2501.02341v1#bib.bib184)] | [Ke
    *et al*.](https://hydra-vl4ai.github.io/) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  |  | HYDRA[[184](https://arxiv.org/html/2501.02341v1#bib.bib184)] | [Ke
    *等人*](https://hydra-vl4ai.github.io/) |'
- en: '|  |  | VISPROG[[185](https://arxiv.org/html/2501.02341v1#bib.bib185)] | [PRIOR
    @ Allen Institute for AI](https://prior.allenai.org/projects/visprog) |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VISPROG[[185](https://arxiv.org/html/2501.02341v1#bib.bib185)] | [PRIOR
    @ 艾伦人工智能研究所](https://prior.allenai.org/projects/visprog) |'
- en: '| VFMs | General | CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]
    | [OpenAI](https://github.com/OpenAI/CLIP) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| VFMs | 通用 | CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)] |
    [OpenAI](https://github.com/OpenAI/CLIP) |'
- en: '|  |  | FILIP[[187](https://arxiv.org/html/2501.02341v1#bib.bib187)] | Yao
    *et al*. |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  |  | FILIP[[187](https://arxiv.org/html/2501.02341v1#bib.bib187)] | Yao
    *等人* |'
- en: '|  |  | RegionCLIP[[188](https://arxiv.org/html/2501.02341v1#bib.bib188)] |
    [Microsoft Research](https://github.com/microsoft/RegionCLIP) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  |  | RegionCLIP[[188](https://arxiv.org/html/2501.02341v1#bib.bib188)] |
    [微软研究院](https://github.com/microsoft/RegionCLIP) |'
- en: '|  |  | EVA-CLIP[[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] | [Sun
    *et al*.](https://github.com/baaivision/EVA/tree/master/EVA-CLIP) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  |  | EVA-CLIP[[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] | [Sun
    *等人*](https://github.com/baaivision/EVA/tree/master/EVA-CLIP) |'
- en: '|  | Object Detection | GLIP[[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]
    | [Microsoft Research](https://github.com/microsoft/GLIP) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标检测 | GLIP[[190](https://arxiv.org/html/2501.02341v1#bib.bib190)] | [微软研究院](https://github.com/microsoft/GLIP)
    |'
- en: '|  |  | DINO[[191](https://arxiv.org/html/2501.02341v1#bib.bib191)] | [Zhang
    *et al*.](https://github.com/IDEA-Research/DINO) |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DINO[[191](https://arxiv.org/html/2501.02341v1#bib.bib191)] | [Zhang
    *等人*](https://github.com/IDEA-Research/DINO) |'
- en: '|  |  | Grounding-DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    | [Liu *et al*.](https://github.com/IDEA-Research/GroundingDINO) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Grounding-DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    | [Liu *等人*](https://github.com/IDEA-Research/GroundingDINO) |'
- en: '|  |  | DINOv2[[193](https://arxiv.org/html/2501.02341v1#bib.bib193)] | [Meta
    AI Research](https://github.com/facebookresearch/dinov2) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DINOv2[[193](https://arxiv.org/html/2501.02341v1#bib.bib193)] | [Meta
    AI 研究院](https://github.com/facebookresearch/dinov2) |'
- en: '|  |  | AM-RADIO[[194](https://arxiv.org/html/2501.02341v1#bib.bib194)] | [NVIDIA](https://github.com/NVlabs/RADIO)
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AM-RADIO[[194](https://arxiv.org/html/2501.02341v1#bib.bib194)] | [NVIDIA](https://github.com/NVlabs/RADIO)
    |'
- en: '|  |  | DINO-WM[[195](https://arxiv.org/html/2501.02341v1#bib.bib195)] | [Zhou
    *et al*.](https://dino-wm.github.io/) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  |  | DINO-WM[[195](https://arxiv.org/html/2501.02341v1#bib.bib195)] | [Zhou
    *等人*](https://dino-wm.github.io/) |'
- en: '|  |  | YOLO-World[[196](https://arxiv.org/html/2501.02341v1#bib.bib196)] |
    [Cheng *et al*.](https://github.com/AILabCVC/YOLO-World) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  |  | YOLO-World[[196](https://arxiv.org/html/2501.02341v1#bib.bib196)] |
    [Cheng *等人*](https://github.com/AILabCVC/YOLO-World) |'
- en: '|  | Image Segmentation | CLIPSeg[[197](https://arxiv.org/html/2501.02341v1#bib.bib197)]
    | [Lüdecke and Ecker](https://github.com/timojl/clipseg) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | 图像分割 | CLIPSeg[[197](https://arxiv.org/html/2501.02341v1#bib.bib197)]
    | [Lüdecke 和 Ecker](https://github.com/timojl/clipseg) |'
- en: '|  |  | SAM[[198](https://arxiv.org/html/2501.02341v1#bib.bib198)] | [Meta
    AI Research, FAIR](https://segment-anything.com) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SAM[[198](https://arxiv.org/html/2501.02341v1#bib.bib198)] | [Meta
    AI 研究，FAIR](https://segment-anything.com) |'
- en: '|  |  | Embodied-SAM[[199](https://arxiv.org/html/2501.02341v1#bib.bib199)]
    | [Xu *et al*.](https://github.com/xuxw98/ESAM) |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Embodied-SAM[[199](https://arxiv.org/html/2501.02341v1#bib.bib199)]
    | [Xu *等人*](https://github.com/xuxw98/ESAM) |'
- en: '|  |  | Point-SAM[[200](https://arxiv.org/html/2501.02341v1#bib.bib200)] |
    [Zhou *et al*.](https://github.com/zyc00/Point-SAM) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Point-SAM[[200](https://arxiv.org/html/2501.02341v1#bib.bib200)] |
    [Zhou *等人*](https://github.com/zyc00/Point-SAM) |'
- en: '|  |  | Open-Vocabulary SAM[[201](https://arxiv.org/html/2501.02341v1#bib.bib201)]
    | [Yuan *et al*.](https://www.mmlab-ntu.com/project/ovsam/) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Open-Vocabulary SAM[[201](https://arxiv.org/html/2501.02341v1#bib.bib201)]
    | [Yuan *等人*](https://www.mmlab-ntu.com/project/ovsam/) |'
- en: '|  |  | TAP[[202](https://arxiv.org/html/2501.02341v1#bib.bib202)] | [Pan *et
    al*.](https://github.com/baaivision/tokenize-anything) |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  |  | TAP[[202](https://arxiv.org/html/2501.02341v1#bib.bib202)] | [Pan *等人*](https://github.com/baaivision/tokenize-anything)
    |'
- en: '|  |  | EfficientSAM[[203](https://arxiv.org/html/2501.02341v1#bib.bib203)]
    | [Xiong *et al*.](https://yformer.github.io/efficient-sam/) |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |  | EfficientSAM[[203](https://arxiv.org/html/2501.02341v1#bib.bib203)]
    | [Xiong *等人*](https://yformer.github.io/efficient-sam/) |'
- en: '|  |  | MobileSAM[[204](https://arxiv.org/html/2501.02341v1#bib.bib204)] |
    [Zhang *et al*.](https://github.com/ChaoningZhang/MobileSAM) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MobileSAM[[204](https://arxiv.org/html/2501.02341v1#bib.bib204)] |
    [Zhang *等人*](https://github.com/ChaoningZhang/MobileSAM) |'
- en: '|  |  | SAM 2[[205](https://arxiv.org/html/2501.02341v1#bib.bib205)] | [Meta
    AI Research, FAIR](https://ai.meta.com/sam2/) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SAM 2[[205](https://arxiv.org/html/2501.02341v1#bib.bib205)] | [Meta
    AI 研究，FAIR](https://ai.meta.com/sam2/) |'
- en: '|  |  | SAMURAI[[206](https://arxiv.org/html/2501.02341v1#bib.bib206)] | [University
    of Washington](https://github.com/yangchris11/samurai) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SAMURAI[[206](https://arxiv.org/html/2501.02341v1#bib.bib206)] | [华盛顿大学](https://github.com/yangchris11/samurai)
    |'
- en: '|  |  | SegGPT[[207](https://arxiv.org/html/2501.02341v1#bib.bib207)] | [Wang
    *et al*.](https://github.com/baaivision/Painter) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SegGPT[[207](https://arxiv.org/html/2501.02341v1#bib.bib207)] | [Wang
    *等人*](https://github.com/baaivision/Painter) |'
- en: '|  |  | Osprey[[208](https://arxiv.org/html/2501.02341v1#bib.bib208)] | [Yuan
    *et al*.](https://github.com/CircleRadon/Osprey) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Osprey[[208](https://arxiv.org/html/2501.02341v1#bib.bib208)] | [Yuan
    *等人*](https://github.com/CircleRadon/Osprey) |'
- en: '|  |  | SEEM[[209](https://arxiv.org/html/2501.02341v1#bib.bib209)] | [Zou
    *et al*.](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SEEM[[209](https://arxiv.org/html/2501.02341v1#bib.bib209)] | [Zou
    *等人*](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
    |'
- en: '|  |  | Seal[[210](https://arxiv.org/html/2501.02341v1#bib.bib210)] | [Liu
    *et al*.](https://github.com/youquanl/Segment-Any-Point-Cloud) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Seal[[210](https://arxiv.org/html/2501.02341v1#bib.bib210)] | [Liu
    *等人*](https://github.com/youquanl/Segment-Any-Point-Cloud) |'
- en: '|  |  | LISA[[211](https://arxiv.org/html/2501.02341v1#bib.bib211)] | [Lai
    *et al*.](https://github.com/dvlabresearch/LISA) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LISA[[211](https://arxiv.org/html/2501.02341v1#bib.bib211)] | [Lai
    *等人*](https://github.com/dvlabresearch/LISA) |'
- en: '|  | Depth Estimation | ZoeDepth[[212](https://arxiv.org/html/2501.02341v1#bib.bib212)]
    | [Bhat *et al*.](https://github.com/isl-org/ZoeDepth) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 深度估计 | ZoeDepth[[212](https://arxiv.org/html/2501.02341v1#bib.bib212)]
    | [Bhat *等人*](https://github.com/isl-org/ZoeDepth) |'
- en: '|  |  | ScaleDepth[[213](https://arxiv.org/html/2501.02341v1#bib.bib213)] |
    [Zhu *et al*.](https://ruijiezhu94.github.io/ScaleDepth/) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ScaleDepth[[213](https://arxiv.org/html/2501.02341v1#bib.bib213)] |
    [Zhu *等人*](https://ruijiezhu94.github.io/ScaleDepth/) |'
- en: '|  |  | Depth Anything[[214](https://arxiv.org/html/2501.02341v1#bib.bib214)]
    | [Yang *et al*.](https://depth-anything.github.io) |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Depth Anything[[214](https://arxiv.org/html/2501.02341v1#bib.bib214)]
    | [Yang *等人*](https://depth-anything.github.io) |'
- en: '|  |  | Depth Anything V2[[215](https://arxiv.org/html/2501.02341v1#bib.bib215)]
    | [Yang *et al*.](https://depth-anything-v2.github.io/) |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Depth Anything V2[[215](https://arxiv.org/html/2501.02341v1#bib.bib215)]
    | [Yang *等人*](https://depth-anything-v2.github.io/) |'
- en: '|  |  | Depth Pro[[216](https://arxiv.org/html/2501.02341v1#bib.bib216)] |
    [Apple](https://github.com/apple/ml-depth-pro) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Depth Pro[[216](https://arxiv.org/html/2501.02341v1#bib.bib216)] |
    [苹果](https://github.com/apple/ml-depth-pro) |'
- en: 3.1 LLMs
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLMs
- en: In recent years, LLMs have seen rapid advancements, with increasingly larger
    models being trained on diverse, large-scale corpora [[217](https://arxiv.org/html/2501.02341v1#bib.bib217)].
    These models have consistently set new performance benchmarks in various NLP tasks
    and have been widely adopted in both academic research and industrial applications
    [[218](https://arxiv.org/html/2501.02341v1#bib.bib218), [219](https://arxiv.org/html/2501.02341v1#bib.bib219),
    [220](https://arxiv.org/html/2501.02341v1#bib.bib220), [221](https://arxiv.org/html/2501.02341v1#bib.bib221)].
    This section provides an overview of the core capabilities of LLMs, including
    their generalization and reasoning abilities, followed by an introduction to typical
    LLMs from leading research organizations.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，LLM在快速发展，越来越大的模型在多样化的大规模语料库上进行训练[[217](https://arxiv.org/html/2501.02341v1#bib.bib217)]。这些模型在各种NLP任务中不断设立新的性能基准，并且在学术研究和工业应用中得到了广泛采用[[218](https://arxiv.org/html/2501.02341v1#bib.bib218),
    [219](https://arxiv.org/html/2501.02341v1#bib.bib219), [220](https://arxiv.org/html/2501.02341v1#bib.bib220),
    [221](https://arxiv.org/html/2501.02341v1#bib.bib221)]。本节概述了LLM的核心能力，包括它们的泛化能力和推理能力，并介绍了来自领先研究机构的典型LLM。
- en: 3.1.1 Core Capabilities of LLMs
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 LLM的核心能力
- en: 'Generalization Capability: Benefiting from training on large-scale corpora
    and the substantial size of the models, LLMs exhibit strong transfer capabilities,
    including zero-shot and few-shot learning. These capabilities enable LLMs to generalize
    effectively to new tasks, either without task-specific examples or with limited
    guidance, making them versatile tools for a wide range of applications. In zero-shot
    learning, without additional task-specific training, LLMs can solve relevant problems
    solely through natural language instructions. In few-shot learning, the model
    can quickly adapt to new tasks by leveraging several examples from the support
    set along with the corresponding task instructions [[222](https://arxiv.org/html/2501.02341v1#bib.bib222)].'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化能力：得益于在大规模语料库上进行训练以及模型本身的庞大规模，LLM展现了强大的迁移能力，包括零-shot和few-shot学习。这些能力使得LLM能够有效地将学习迁移到新任务，无论是没有任务特定示例，还是有有限的指导，使其成为适用于广泛应用的多功能工具。在零-shot学习中，LLM无需额外的任务特定训练，仅通过自然语言指令就能解决相关问题。在few-shot学习中，模型可以通过借助来自支持集的几个示例和相应的任务指令，迅速适应新任务[[222](https://arxiv.org/html/2501.02341v1#bib.bib222)]。
- en: The design of natural language instructions or prompts is crucial in enhancing
    generalization capability. Prompts not only provide natural language descriptions
    of tasks but also guide the model to perform tasks accurately based on input examples
    [[223](https://arxiv.org/html/2501.02341v1#bib.bib223), [141](https://arxiv.org/html/2501.02341v1#bib.bib141),
    [143](https://arxiv.org/html/2501.02341v1#bib.bib143)]. Furthermore, LLMs exhibit
    in-context learning, which allows them to learn and adapt to new tasks directly
    from the context provided within the prompt, such as task instructions and examples,
    without requiring any explicit retraining or model updates [[141](https://arxiv.org/html/2501.02341v1#bib.bib141),
    [224](https://arxiv.org/html/2501.02341v1#bib.bib224), [225](https://arxiv.org/html/2501.02341v1#bib.bib225)].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言指令或提示的设计对于增强泛化能力至关重要。提示不仅提供任务的自然语言描述，还能引导模型根据输入示例准确地执行任务[[223](https://arxiv.org/html/2501.02341v1#bib.bib223),
    [141](https://arxiv.org/html/2501.02341v1#bib.bib141), [143](https://arxiv.org/html/2501.02341v1#bib.bib143)]。此外，LLM展现了上下文学习能力，使其能够直接从提示中提供的上下文（如任务指令和示例）学习并适应新任务，而无需任何显式的再训练或模型更新[[141](https://arxiv.org/html/2501.02341v1#bib.bib141),
    [224](https://arxiv.org/html/2501.02341v1#bib.bib224), [225](https://arxiv.org/html/2501.02341v1#bib.bib225)]。
- en: 'Complex Problem-Solving Capability: LLMs demonstrate a remarkable ability to
    solve complex problems by generating intermediate reasoning steps or structured
    logical pathways, facilitating a systematic and step-by-step approach to addressing
    challenges. This capability is exemplified by the Chain of Thought (CoT) framework,
    where intricate problems are decomposed into a series of manageable sub-tasks,
    each solved sequentially using examples of step-by-step reasoning [[226](https://arxiv.org/html/2501.02341v1#bib.bib226),
    [227](https://arxiv.org/html/2501.02341v1#bib.bib227), [228](https://arxiv.org/html/2501.02341v1#bib.bib228),
    [229](https://arxiv.org/html/2501.02341v1#bib.bib229)]. Besides, LLMs also demonstrate
    advanced capabilities in task planning and the orchestration of tools, enabling
    them to invoke appropriate resources to address specific sub-task requirements
    and efficiently integrate workflows to achieve comprehensive solutions [[230](https://arxiv.org/html/2501.02341v1#bib.bib230),
    [231](https://arxiv.org/html/2501.02341v1#bib.bib231), [232](https://arxiv.org/html/2501.02341v1#bib.bib232)].'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂问题解决能力：大语言模型（LLMs）展示了通过生成中间推理步骤或结构化逻辑路径来解决复杂问题的显著能力，从而促进了系统化、逐步解决挑战的方法。这一能力通过思维链（Chain
    of Thought，CoT）框架得以体现，其中复杂问题被分解为一系列可管理的子任务，每个子任务通过逐步推理的示例依次解决[[226](https://arxiv.org/html/2501.02341v1#bib.bib226),
    [227](https://arxiv.org/html/2501.02341v1#bib.bib227), [228](https://arxiv.org/html/2501.02341v1#bib.bib228),
    [229](https://arxiv.org/html/2501.02341v1#bib.bib229)]。此外，LLMs还展示了在任务规划和工具协调方面的高级能力，使其能够调用适当的资源来满足特定子任务需求，并高效地整合工作流以实现全面的解决方案[[230](https://arxiv.org/html/2501.02341v1#bib.bib230),
    [231](https://arxiv.org/html/2501.02341v1#bib.bib231), [232](https://arxiv.org/html/2501.02341v1#bib.bib232)]。
- en: 3.1.2 Typical LLMs
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 典型的大语言模型
- en: Several notable milestones have marked the development of LLMs. OpenAI’s GPT
    series, spanning GPT-3, GPT-3.5, and GPT-4, has set benchmarks in language understanding,
    generation, and reasoning tasks by leveraging extensive parameters and optimized
    architectures [[141](https://arxiv.org/html/2501.02341v1#bib.bib141), [142](https://arxiv.org/html/2501.02341v1#bib.bib142),
    [143](https://arxiv.org/html/2501.02341v1#bib.bib143)]. Anthropic’s Claude models,
    including Claude 2 and Claude 3, prioritize safety and controllability through
    reinforcement learning, excelling in multi-task generalization and robustness
    [[144](https://arxiv.org/html/2501.02341v1#bib.bib144), [145](https://arxiv.org/html/2501.02341v1#bib.bib145),
    [146](https://arxiv.org/html/2501.02341v1#bib.bib146)]. The Mistral series employs
    sparse activation techniques to balance efficiency with performance, emphasizing
    low-latency inference [[147](https://arxiv.org/html/2501.02341v1#bib.bib147),
    [148](https://arxiv.org/html/2501.02341v1#bib.bib148)].
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）的发展历程中有多个重要的里程碑。OpenAI的GPT系列，包括GPT-3、GPT-3.5和GPT-4，通过利用大量参数和优化架构，设定了语言理解、生成和推理任务的基准[[141](https://arxiv.org/html/2501.02341v1#bib.bib141),
    [142](https://arxiv.org/html/2501.02341v1#bib.bib142), [143](https://arxiv.org/html/2501.02341v1#bib.bib143)]。Anthropic的Claude系列模型，包括Claude
    2和Claude 3，注重通过强化学习提高安全性和可控性，在多任务泛化和鲁棒性方面表现优异[[144](https://arxiv.org/html/2501.02341v1#bib.bib144),
    [145](https://arxiv.org/html/2501.02341v1#bib.bib145), [146](https://arxiv.org/html/2501.02341v1#bib.bib146)]。Mistral系列采用稀疏激活技术，在效率与性能之间取得平衡，注重低延迟推理[[147](https://arxiv.org/html/2501.02341v1#bib.bib147),
    [148](https://arxiv.org/html/2501.02341v1#bib.bib148)]。
- en: Google’s PaLM series [[149](https://arxiv.org/html/2501.02341v1#bib.bib149),
    [150](https://arxiv.org/html/2501.02341v1#bib.bib150)] stands out for its multimodal
    capabilities and large-scale parameterization, while the subsequent Gemini series
    extends these features to improve generalizability and multilingual support [[151](https://arxiv.org/html/2501.02341v1#bib.bib151),
    [152](https://arxiv.org/html/2501.02341v1#bib.bib152)]. In the open-source ecosystem,
    Meta’s Llama models, including Llama 2 and Llama 3, excel in multilingual tasks
    and complex problem-solving. Derivative models like Vicuna enhance conversational
    abilities and task adaptability through fine-tuning on conversational datasets
    and techniques like Low-Rank Adaptation (LoRA) [[153](https://arxiv.org/html/2501.02341v1#bib.bib153),
    [154](https://arxiv.org/html/2501.02341v1#bib.bib154), [155](https://arxiv.org/html/2501.02341v1#bib.bib155),
    [156](https://arxiv.org/html/2501.02341v1#bib.bib156)]. Similarly, the Qwen series,
    pre-trained on multilingual datasets and instruction-tuned, demonstrates adaptability
    across diverse tasks [[157](https://arxiv.org/html/2501.02341v1#bib.bib157)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Google的PaLM系列 [[149](https://arxiv.org/html/2501.02341v1#bib.bib149), [150](https://arxiv.org/html/2501.02341v1#bib.bib150)]
    因其多模态能力和大规模参数化而脱颖而出，而后续的Gemini系列则在此基础上扩展了这些特性，提升了泛化能力和多语言支持 [[151](https://arxiv.org/html/2501.02341v1#bib.bib151),
    [152](https://arxiv.org/html/2501.02341v1#bib.bib152)]。在开源生态系统中，Meta的Llama模型，包括Llama
    2和Llama 3，在多语言任务和复杂问题解决方面表现突出。衍生模型如Vicuna通过在对话数据集上的微调和使用低秩适应（LoRA）技术 [[153](https://arxiv.org/html/2501.02341v1#bib.bib153),
    [154](https://arxiv.org/html/2501.02341v1#bib.bib154), [155](https://arxiv.org/html/2501.02341v1#bib.bib155),
    [156](https://arxiv.org/html/2501.02341v1#bib.bib156)]，增强了对话能力和任务适应性。同样，Qwen系列通过在多语言数据集上的预训练和指令微调，展示了其在多种任务中的适应性
    [[157](https://arxiv.org/html/2501.02341v1#bib.bib157)]。
- en: Several other models have achieved significant progress in specialized domains.
    InternLM [[159](https://arxiv.org/html/2501.02341v1#bib.bib159)], BuboGPT [[160](https://arxiv.org/html/2501.02341v1#bib.bib160)],
    ChatGLM [[161](https://arxiv.org/html/2501.02341v1#bib.bib161), [162](https://arxiv.org/html/2501.02341v1#bib.bib162),
    [163](https://arxiv.org/html/2501.02341v1#bib.bib163)], and DeepSeek [[164](https://arxiv.org/html/2501.02341v1#bib.bib164),
    [165](https://arxiv.org/html/2501.02341v1#bib.bib165), [166](https://arxiv.org/html/2501.02341v1#bib.bib166)]
    focus on domain-specific tasks such as knowledge-based Q&A, conversational generation,
    and information retrieval, enabled by task-specific fine-tuning and targeted extensions.
    Notably, LiveBench [[233](https://arxiv.org/html/2501.02341v1#bib.bib233)] has
    emerged as a comprehensive benchmarking platform, addressing limitations of previous
    evaluation standards. It systematically assesses LLMs’ real-world capabilities
    across multi-task scenarios, offering valuable insights for model development
    and application.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其他几个模型在特定领域取得了显著进展。InternLM [[159](https://arxiv.org/html/2501.02341v1#bib.bib159)]、BuboGPT
    [[160](https://arxiv.org/html/2501.02341v1#bib.bib160)]、ChatGLM [[161](https://arxiv.org/html/2501.02341v1#bib.bib161),
    [162](https://arxiv.org/html/2501.02341v1#bib.bib162), [163](https://arxiv.org/html/2501.02341v1#bib.bib163)]
    和 DeepSeek [[164](https://arxiv.org/html/2501.02341v1#bib.bib164), [165](https://arxiv.org/html/2501.02341v1#bib.bib165),
    [166](https://arxiv.org/html/2501.02341v1#bib.bib166)] 聚焦于特定领域的任务，如基于知识的问答、对话生成和信息检索，这得益于任务特定的微调和针对性的扩展。值得注意的是，LiveBench
    [[233](https://arxiv.org/html/2501.02341v1#bib.bib233)] 已成为一个综合性的基准评估平台，解决了以往评估标准的局限性。它系统地评估了LLM在多任务场景下的实际能力，为模型开发和应用提供了宝贵的见解。
- en: 3.2 VLMs
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 VLMs
- en: VLMs are multimodal models that extend the capabilities of LLMs by integrating
    visual and textual information[[234](https://arxiv.org/html/2501.02341v1#bib.bib234)].
    These models are designed to tackle a range of tasks that require both vision
    and language understanding, such as visual question answering (VQA) and image
    captioning [[235](https://arxiv.org/html/2501.02341v1#bib.bib235), [236](https://arxiv.org/html/2501.02341v1#bib.bib236),
    [237](https://arxiv.org/html/2501.02341v1#bib.bib237), [238](https://arxiv.org/html/2501.02341v1#bib.bib238),
    [239](https://arxiv.org/html/2501.02341v1#bib.bib239)]. This section introduces
    several typical VLM models highlighting their technical features and application
    scenarios.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: VLM（视觉语言模型）是多模态模型，通过整合视觉和文本信息扩展了LLM的能力 [[234](https://arxiv.org/html/2501.02341v1#bib.bib234)]。这些模型旨在解决需要视觉和语言理解的多种任务，如视觉问答（VQA）和图像描述
    [[235](https://arxiv.org/html/2501.02341v1#bib.bib235), [236](https://arxiv.org/html/2501.02341v1#bib.bib236),
    [237](https://arxiv.org/html/2501.02341v1#bib.bib237), [238](https://arxiv.org/html/2501.02341v1#bib.bib238),
    [239](https://arxiv.org/html/2501.02341v1#bib.bib239)]。本节介绍了几种典型的VLM模型，突出了它们的技术特点和应用场景。
- en: OpenAI’s GPT-4V [[167](https://arxiv.org/html/2501.02341v1#bib.bib167)] is a
    prominent representative in VLMs, demonstrating powerful visual perception capabilities
    [[240](https://arxiv.org/html/2501.02341v1#bib.bib240)]. The upgraded GPT-4o introduces
    more advanced optimization algorithms, allowing it to accept arbitrary combinations
    of text, audio, and image inputs while delivering rapid responses. The lightweight
    version, GPT-4o mini, is designed for mobile devices and edge computing scenarios,
    balancing efficient performance with deployability by reducing computational resource
    consumption [[241](https://arxiv.org/html/2501.02341v1#bib.bib241)]. GPT o1-preview
    excels in reasoning, particularly in programming and solving complex problems
    [[242](https://arxiv.org/html/2501.02341v1#bib.bib242)]. Anthropic’s Claude 3
    Opus exhibits robust multi-task generalization and controllability, while Claude
    3.5 Sonnet enhances practical value by optimizing reasoning speed and cost efficiency
    [[168](https://arxiv.org/html/2501.02341v1#bib.bib168)]. The Step-2 model employs
    an innovative Mixture of Experts (MoE) architecture, supporting efficient training
    at a trillion-parameter scale and significantly improving the handling of complex
    tasks and model scalability.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI的GPT-4V [[167](https://arxiv.org/html/2501.02341v1#bib.bib167)]是视觉语言模型（VLMs）中的重要代表，展示了强大的视觉感知能力[[240](https://arxiv.org/html/2501.02341v1#bib.bib240)]。升级版的GPT-4o引入了更先进的优化算法，使其能够接受文本、音频和图像输入的任意组合，同时提供快速响应。轻量级版本GPT-4o
    mini专为移动设备和边缘计算场景设计，通过减少计算资源的消耗，在高效性能和可部署性之间取得平衡[[241](https://arxiv.org/html/2501.02341v1#bib.bib241)]。GPT
    o1-preview在推理方面表现出色，特别是在编程和解决复杂问题方面[[242](https://arxiv.org/html/2501.02341v1#bib.bib242)]。Anthropic的Claude
    3 Opus展现了强大的多任务泛化能力和可控性，而Claude 3.5 Sonnet通过优化推理速度和成本效率，提升了实际应用价值[[168](https://arxiv.org/html/2501.02341v1#bib.bib168)]。Step-2模型采用了创新的专家混合（Mixture
    of Experts，MoE）架构，支持在万亿参数规模下高效训练，并显著提高了复杂任务的处理能力和模型的可扩展性。
- en: Liu *et al*. [[169](https://arxiv.org/html/2501.02341v1#bib.bib169)] proposed
    LLaVA, a representative VLM. This model leverages GPT-4 to generate instruction-following
    datasets and integrates CLIP visual encoder ViT-L/14 [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]
    with Vicuna [[243](https://arxiv.org/html/2501.02341v1#bib.bib243)], fine-tuning
    end-to-end instruction to enhance its performance in multimodal tasks. Its latest
    version, LLaVA-NeXT [[171](https://arxiv.org/html/2501.02341v1#bib.bib171)], builds
    upon LLaVA-1.5 [[170](https://arxiv.org/html/2501.02341v1#bib.bib170)] with significant
    improvements, notably enhancing the ability to capture visual details and excelling
    in complex visual and logical reasoning tasks. MoE-LLaVA replaces the language
    model in LLaVA with an MoE architecture, substantially improving inference efficiency
    and resource utilization in large-scale multi-task scenarios [[172](https://arxiv.org/html/2501.02341v1#bib.bib172)].
    LLaVA-CoT enhances accuracy in reasoning-intensive tasks through structured reasoning
    annotations of large-scale visual question-answering samples combined with beam
    search methods [[173](https://arxiv.org/html/2501.02341v1#bib.bib173)]. Another
    important class of architectures includes the Flamingo [[174](https://arxiv.org/html/2501.02341v1#bib.bib174)]
    and BLIP series [[175](https://arxiv.org/html/2501.02341v1#bib.bib175), [176](https://arxiv.org/html/2501.02341v1#bib.bib176)],
    which enable LLMs to generate corresponding textual outputs from multimodal inputs
    by combining pre-trained visual feature encoders with pre-trained LLMs. Flamingo
    introduces the Perceiver Resampler and Gated Cross-Attention mechanisms, effectively
    integrating visual, multimodal information with the language model, thereby significantly
    enhancing performance in multimodal tasks. BLIP-2 [[176](https://arxiv.org/html/2501.02341v1#bib.bib176)]
    adopts a pretraining strategy combining stage-wise frozen image encoders with
    LLMs and introduces a Query Transformer (Q-Former) to effectively address alignment
    issues between the visual and language modalities. InstructBLIP [[177](https://arxiv.org/html/2501.02341v1#bib.bib177)]
    incorporates large-scale task instruction fine-tuning mechanisms, further improving
    the model’s adaptability to multimodal tasks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人*（et al.）* [[169](https://arxiv.org/html/2501.02341v1#bib.bib169)] 提出了LLaVA，一种代表性的视觉语言模型（VLM）。该模型利用GPT-4生成遵循指令的数据集，并将CLIP视觉编码器ViT-L/14
    [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)] 与Vicuna [[243](https://arxiv.org/html/2501.02341v1#bib.bib243)]结合，端到端微调指令以增强其在多模态任务中的表现。其最新版本LLaVA-NeXT
    [[171](https://arxiv.org/html/2501.02341v1#bib.bib171)] 基于LLaVA-1.5 [[170](https://arxiv.org/html/2501.02341v1#bib.bib170)]进行构建，具有显著的改进，尤其是在捕捉视觉细节的能力方面有所提升，并在复杂的视觉和逻辑推理任务中表现出色。MoE-LLaVA用MoE架构替代LLaVA中的语言模型，显著提高了大规模多任务场景中的推理效率和资源利用率
    [[172](https://arxiv.org/html/2501.02341v1#bib.bib172)]。LLaVA-CoT通过结构化推理注释大规模视觉问答样本，并结合束搜索方法，提升了推理密集型任务的准确性
    [[173](https://arxiv.org/html/2501.02341v1#bib.bib173)]。另一类重要的架构包括Flamingo [[174](https://arxiv.org/html/2501.02341v1#bib.bib174)]
    和BLIP系列 [[175](https://arxiv.org/html/2501.02341v1#bib.bib175), [176](https://arxiv.org/html/2501.02341v1#bib.bib176)]，这些模型通过将预训练的视觉特征编码器与预训练的语言模型（LLM）结合，使LLM能够根据多模态输入生成相应的文本输出。Flamingo引入了Perceiver
    Resampler和Gated Cross-Attention机制，有效地将视觉和多模态信息与语言模型整合，从而显著提升多模态任务中的表现。BLIP-2 [[176](https://arxiv.org/html/2501.02341v1#bib.bib176)]
    采用了结合阶段性冻结图像编码器与LLM的预训练策略，并引入了查询变换器（Q-Former），有效解决了视觉和语言模态之间的对齐问题。InstructBLIP
    [[177](https://arxiv.org/html/2501.02341v1#bib.bib177)] 纳入了大规模任务指令微调机制，进一步提升了模型在多模态任务中的适应性。
- en: Additionally, VLMs have demonstrated broad application potential across various
    tasks and scenarios. In video understanding, representative models such as LLaMA-VID
    [[178](https://arxiv.org/html/2501.02341v1#bib.bib178)], IG-VLM [[179](https://arxiv.org/html/2501.02341v1#bib.bib179)],
    Video-ChatGPT [[180](https://arxiv.org/html/2501.02341v1#bib.bib180)], and VideoTree
    [[181](https://arxiv.org/html/2501.02341v1#bib.bib181)] exhibit outstanding performance
    in video content analysis and multimodal tasks. In visual reasoning, models like
    X-VLM [[182](https://arxiv.org/html/2501.02341v1#bib.bib182)], Chameleon [[183](https://arxiv.org/html/2501.02341v1#bib.bib183)],
    HYDRA [[184](https://arxiv.org/html/2501.02341v1#bib.bib184)], and VISPROG [[185](https://arxiv.org/html/2501.02341v1#bib.bib185)]
    enhance the accuracy and adaptability of complex visual reasoning tasks through
    innovative architectural designs and reasoning mechanisms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，VLMs 已展示出在多种任务和场景中的广泛应用潜力。在视频理解方面，代表性的模型如 LLaMA-VID [[178](https://arxiv.org/html/2501.02341v1#bib.bib178)]、IG-VLM
    [[179](https://arxiv.org/html/2501.02341v1#bib.bib179)]、Video-ChatGPT [[180](https://arxiv.org/html/2501.02341v1#bib.bib180)]
    和 VideoTree [[181](https://arxiv.org/html/2501.02341v1#bib.bib181)] 在视频内容分析和多模态任务中表现出色。在视觉推理方面，X-VLM
    [[182](https://arxiv.org/html/2501.02341v1#bib.bib182)]、Chameleon [[183](https://arxiv.org/html/2501.02341v1#bib.bib183)]、HYDRA
    [[184](https://arxiv.org/html/2501.02341v1#bib.bib184)] 和 VISPROG [[185](https://arxiv.org/html/2501.02341v1#bib.bib185)]
    等模型通过创新的架构设计和推理机制，提升了复杂视觉推理任务的准确性和适应性。
- en: 3.3 VFMs
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 VFMs
- en: In recent years, the concept of VFMs has emerged as a core technology in computer
    vision. The primary goal of VFMs is to extract diverse and highly expressive image
    features, making them directly applicable to various downstream tasks. These models
    are typically characterized by large-scale parameters, remarkable generalization
    capabilities, and outstanding cross-task transfer performance, albeit with relatively
    high training costs [[194](https://arxiv.org/html/2501.02341v1#bib.bib194)]. CLIP
    is a pioneering representative in the field of VFMs. By employing weakly supervised
    training on large-scale image-text pairs, it efficiently aligns visual and textual
    embeddings, laying a solid foundation for multimodal learning [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)].
    Subsequent works have further improved the training efficiency and performance
    of CLIP, including models such as FILIP [[187](https://arxiv.org/html/2501.02341v1#bib.bib187)],
    RegionCLIP [[188](https://arxiv.org/html/2501.02341v1#bib.bib188)], and EVA-CLIP
    [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)].
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，VFM 的概念已成为计算机视觉中的核心技术。VFM 的主要目标是提取多样化且高度表达性的图像特征，使其能够直接应用于各种下游任务。这些模型通常具有大规模的参数、卓越的泛化能力以及出色的跨任务迁移表现，尽管训练成本相对较高
    [[194](https://arxiv.org/html/2501.02341v1#bib.bib194)]。CLIP 是 VFM 领域的开创性代表。通过对大规模图像-文本对进行弱监督训练，它有效地对齐了视觉和文本嵌入，为多模态学习奠定了坚实基础
    [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]。随后的研究进一步提高了 CLIP 的训练效率和性能，包括
    FILIP [[187](https://arxiv.org/html/2501.02341v1#bib.bib187)]、RegionCLIP [[188](https://arxiv.org/html/2501.02341v1#bib.bib188)]
    和 EVA-CLIP [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] 等模型。
- en: 'VFMs have demonstrated exceptional adaptability, achieving remarkable results
    in various computer vision tasks, including zero-shot object detection, image
    segmentation, and depth estimation. As shown in Fig. [3](https://arxiv.org/html/2501.02341v1#S3.F3
    "Figure 3 ‣ 3.3 VFMs ‣ 3 Preliminaries on Foundation models ‣ UAVs Meet LLMs:
    Overviews and Perspectives Toward Agentic Low-Altitude Mobility"), we selected
    a sample image from the Town10HD scene in the SynDrone dataset [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)],
    specific to the UAV domain, to visually illustrate the performance of several
    VFMs under zero-shot conditions. This example provides strong support for understanding
    their practical application potential.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: VFMs 展现了卓越的适应性，在多个计算机视觉任务中取得了显著的成果，包括零-shot 物体检测、图像分割和深度估计。如图 [3](https://arxiv.org/html/2501.02341v1#S3.F3
    "图 3 ‣ 3.3 VFMs ‣ 3 基础模型概述 ‣ 无人机与大规模语言模型的结合：面向代理式低空移动的概览与展望") 所示，我们从 SynDrone
    数据集中的 Town10HD 场景 [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)] 中选取了一张特定于无人机领域的样本图像，以视觉方式展示几种
    VFMs 在零-shot 条件下的表现。此示例为理解其实际应用潜力提供了有力支持。
- en: '![Refer to caption](img/2aa8382c6d4de6aeffc24255a7a64283.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2aa8382c6d4de6aeffc24255a7a64283.png)'
- en: 'Figure 3: Demonstration of VFM models in various vision tasks. (a) The original
    image from the SynDrone dataset [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)];
    (b) object detection result using Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    with the natural language prompt “car” as the detection target; (c) semantic segmentation
    of the entire image using the SAM model [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)];
    (d) Depth image generated for the entire image using the ZoeDepth model [[212](https://arxiv.org/html/2501.02341v1#bib.bib212)].'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：VFM模型在各种视觉任务中的展示。(a) 来自SynDrone数据集的原始图像[[244](https://arxiv.org/html/2501.02341v1#bib.bib244)];
    (b) 使用Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 进行目标检测，检测目标为自然语言提示“car”；(c)
    使用SAM模型 [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)] 对整幅图像进行语义分割；(d)
    使用ZoeDepth模型 [[212](https://arxiv.org/html/2501.02341v1#bib.bib212)] 为整幅图像生成深度图像。
- en: 3.3.1 VFM for Object Detection
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 VFM在目标检测中的应用
- en: The core advantage of VFMs in object detection lies in their powerful zero-shot
    detection capabilities. GLIP [[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]
    unifies object detection and phrase grounding tasks, demonstrating exceptional
    zero-shot and few-shot transfer capabilities across various object-level recognition
    tasks. Zhang *et al*. [[191](https://arxiv.org/html/2501.02341v1#bib.bib191)]
    proposed DINO, which optimized the architecture of the DETR model [[245](https://arxiv.org/html/2501.02341v1#bib.bib245)],
    significantly enhancing detection performance and efficiency. Subsequent work,
    Grounding-DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)], introduced
    text supervision to improve accuracy. Additionally, DINOv2 [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)]
    adopted a discriminative self-supervised learning approach, enabling the extraction
    of robust image features and achieving excellent performance in downstream tasks
    without fine-tuning. AM-RADIO [[194](https://arxiv.org/html/2501.02341v1#bib.bib194)]
    integrated the capabilities of VFMs such as CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)],
    DINOv2 [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)], and SAM [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]
    through a multi-teacher distillation method, resulting in strong representational
    power to support complex visual tasks. DINO-WM [[195](https://arxiv.org/html/2501.02341v1#bib.bib195)]
    incorporated DINOv2 into world models, enabling zero-shot planning capabilities.
    Additionally, YOLO-World [[196](https://arxiv.org/html/2501.02341v1#bib.bib196)]
    enhances the generalization capability of YOLO detectors through an efficient
    pretraining scheme, achieving outstanding performance in open vocabulary and zero-shot
    detection tasks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: VFMs在目标检测中的核心优势在于其强大的零-shot检测能力。GLIP [[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]
    统一了目标检测和短语定位任务，展示了在各种目标级别识别任务中卓越的零-shot和少-shot迁移能力。Zhang *et al*.[[191](https://arxiv.org/html/2501.02341v1#bib.bib191)]
    提出了DINO，优化了DETR模型的架构[[245](https://arxiv.org/html/2501.02341v1#bib.bib245)]，显著提升了检测性能和效率。随后的工作Grounding-DINO
    [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 引入了文本监督以提高准确性。此外，DINOv2
    [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)] 采用了一种判别式自监督学习方法，能够提取出稳健的图像特征，并在下游任务中无需微调便能实现出色的性能。AM-RADIO
    [[194](https://arxiv.org/html/2501.02341v1#bib.bib194)] 通过多教师蒸馏方法集成了CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]、DINOv2
    [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)] 和SAM [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]
    等VFMs的能力，展现了强大的表示能力，支持复杂的视觉任务。DINO-WM [[195](https://arxiv.org/html/2501.02341v1#bib.bib195)]
    将DINOv2融入世界模型，赋予了零-shot规划能力。此外，YOLO-World [[196](https://arxiv.org/html/2501.02341v1#bib.bib196)]
    通过高效的预训练方案增强了YOLO检测器的泛化能力，在开放词汇和零-shot检测任务中表现出色。
- en: 3.3.2 VFM for Image Segmentation
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 VFM在图像分割中的应用
- en: VFMs have demonstrated significant improvements over traditional methods in
    image segmentation tasks. Lüdecke *et al*. [[197](https://arxiv.org/html/2501.02341v1#bib.bib197)]
    proposed CLIPSeg, based on the CLIP model, which supports semantic segmentation,
    instance segmentation, and zero-shot segmentation. Kirillov *et al*. [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]
    developed the Segment Anything Model (SAM), achieving zero-shot segmentation capabilities
    across diverse scenarios through pretraining on large-scale and diverse datasets.
    Subsequent research further extended SAM’s applications, such as Embodied-SAM
    [[199](https://arxiv.org/html/2501.02341v1#bib.bib199)] and Point-SAM [[200](https://arxiv.org/html/2501.02341v1#bib.bib200)],
    which expanded SAM’s functionality to 3D scenes. Open-Vocabulary SAM [[201](https://arxiv.org/html/2501.02341v1#bib.bib201)]
    combined SAM with CLIP’s knowledge transfer strategies, effectively optimizing
    segmentation and recognition tasks simultaneously. Pan *et al*. [[202](https://arxiv.org/html/2501.02341v1#bib.bib202)]
    proposed TAP (Tokenize Anything), a foundational model centered on visual perception,
    which improves the SAM architecture by introducing visual prompts to enable simultaneous
    completion of segmentation, recognition, and description tasks for arbitrary regions.
    EfficientSAM [[203](https://arxiv.org/html/2501.02341v1#bib.bib203)] and MobileSAM
    [[204](https://arxiv.org/html/2501.02341v1#bib.bib204)] optimize SAM’s representation,
    significantly reducing model complexity and achieving lightweight designs while
    maintaining excellent task performance. Recently, SAM 2 [[205](https://arxiv.org/html/2501.02341v1#bib.bib205)]
    introduced memory modules to the original model, enabling real-time segmentation
    for videos of arbitrary length while addressing complex challenges like occlusion
    and multi-object tracking. SAMURAI [[206](https://arxiv.org/html/2501.02341v1#bib.bib206)]
    builds upon SAM 2 by integrating a Kalman filter, addressing the limitations of
    memory management in SAM 2, and achieving superior video segmentation performance
    without requiring retraining or fine-tuning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: VFMs在图像分割任务中相较于传统方法展示了显著的改进。Lüdecke *等人* [[197](https://arxiv.org/html/2501.02341v1#bib.bib197)]
    提出了基于CLIP模型的CLIPSeg，支持语义分割、实例分割和零-shot分割。Kirillov *等人* [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]
    开发了Segment Anything Model (SAM)，通过在大规模和多样化的数据集上进行预训练，实现了跨多种场景的零-shot分割能力。随后的研究进一步扩展了SAM的应用，例如Embodied-SAM
    [[199](https://arxiv.org/html/2501.02341v1#bib.bib199)] 和Point-SAM [[200](https://arxiv.org/html/2501.02341v1#bib.bib200)]，将SAM的功能扩展到3D场景中。Open-Vocabulary
    SAM [[201](https://arxiv.org/html/2501.02341v1#bib.bib201)] 将SAM与CLIP的知识迁移策略结合，有效地同时优化了分割和识别任务。Pan
    *等人* [[202](https://arxiv.org/html/2501.02341v1#bib.bib202)] 提出了TAP（Tokenize Anything），一种以视觉感知为中心的基础模型，通过引入视觉提示改进了SAM架构，使其能够同时完成任意区域的分割、识别和描述任务。EfficientSAM
    [[203](https://arxiv.org/html/2501.02341v1#bib.bib203)] 和MobileSAM [[204](https://arxiv.org/html/2501.02341v1#bib.bib204)]
    优化了SAM的表示，显著降低了模型复杂性并实现了轻量级设计，同时保持了优秀的任务表现。最近，SAM 2 [[205](https://arxiv.org/html/2501.02341v1#bib.bib205)]
    在原有模型中引入了记忆模块，使得能够对任意长度的视频进行实时分割，同时解决了像遮挡和多目标跟踪等复杂挑战。SAMURAI [[206](https://arxiv.org/html/2501.02341v1#bib.bib206)]
    基于SAM 2，结合了卡尔曼滤波器，解决了SAM 2中记忆管理的局限性，并在不需要重新训练或微调的情况下，实现了更优的视频分割性能。
- en: Beyond the SAM series, other VFM architectures have also significantly advanced
    image segmentation. Models such as SegGPT [[207](https://arxiv.org/html/2501.02341v1#bib.bib207)],
    Osprey [[208](https://arxiv.org/html/2501.02341v1#bib.bib208)], and SEEM [[209](https://arxiv.org/html/2501.02341v1#bib.bib209)]
    have demonstrated notable adaptability in arbitrary segmentation tasks and multimodal
    scenarios. Additionally, VFMs have shown important applications in other segmentation
    tasks. For example, Liu *et al*. [[210](https://arxiv.org/html/2501.02341v1#bib.bib210)]
    proposed the Seal framework for segmenting point cloud sequences, while the LISA
    [[211](https://arxiv.org/html/2501.02341v1#bib.bib211)] adopted the Embedding-as-Mask
    approach to endow multimodal large models with reasoning-based segmentation capabilities.
    LISA can process complex natural language instructions and generate fine-grained
    segmentation results, expanding the scope and complexity of segmentation model
    applications.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 除了SAM系列，其他VFM架构也显著推动了图像分割的发展。像SegGPT [[207](https://arxiv.org/html/2501.02341v1#bib.bib207)]、Osprey
    [[208](https://arxiv.org/html/2501.02341v1#bib.bib208)]和SEEM [[209](https://arxiv.org/html/2501.02341v1#bib.bib209)]等模型在任意分割任务和多模态场景中展现了显著的适应性。此外，VFM在其他分割任务中也有重要应用。例如，Liu
    *et al* [[210](https://arxiv.org/html/2501.02341v1#bib.bib210)] 提出了用于点云序列分割的Seal框架，而LISA
    [[211](https://arxiv.org/html/2501.02341v1#bib.bib211)]采用了Embedding-as-Mask方法，使多模态大型模型具备了基于推理的分割能力。LISA能够处理复杂的自然语言指令并生成细粒度的分割结果，扩展了分割模型应用的范围和复杂度。
- en: 3.3.3 VFM for Monocular Depth Estimation
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 单目深度估计中的VFM
- en: 'In the field of monocular depth estimation, VFMs have also demonstrated significant
    technological advantages. ZoeDepth [[212](https://arxiv.org/html/2501.02341v1#bib.bib212)]
    achieves zero-shot depth estimation by combining relative and absolute depth estimation
    methods. ScaleDepth [[213](https://arxiv.org/html/2501.02341v1#bib.bib213)] decomposes
    depth estimation into two modules: scene scale prediction and relative depth estimation,
    achieving advanced performance in indoor, outdoor, unconstrained, and unseen scenarios.
    Additionally, Depth Anything [[214](https://arxiv.org/html/2501.02341v1#bib.bib214)]
    employs many unlabeled monocular images to train an efficient and robust depth
    estimation method, showcasing outstanding performance in zero-shot scenarios.
    Depth Anything V2 [[215](https://arxiv.org/html/2501.02341v1#bib.bib215)] introduces
    multiple optimizations to the original model, further improving prediction performance
    in complex scenes and enabling the generation of high-quality Depth images with
    rich details. Depth Pro [[216](https://arxiv.org/html/2501.02341v1#bib.bib216)],
    based on a multi-scale ViT architecture, can quickly produce metrically accurate
    Depth images with high resolution and high-frequency details, making it an effective
    tool for handling complex depth estimation tasks.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在单目深度估计领域，VFM也展示了显著的技术优势。ZoeDepth [[212](https://arxiv.org/html/2501.02341v1#bib.bib212)]通过结合相对和绝对深度估计方法，实现了零样本深度估计。ScaleDepth
    [[213](https://arxiv.org/html/2501.02341v1#bib.bib213)]将深度估计分解为两个模块：场景尺度预测和相对深度估计，在室内、室外、无约束和未见过的场景中取得了先进的表现。此外，Depth
    Anything [[214](https://arxiv.org/html/2501.02341v1#bib.bib214)]利用大量未标注的单目图像训练了一种高效且稳健的深度估计方法，在零样本场景中展现了出色的性能。Depth
    Anything V2 [[215](https://arxiv.org/html/2501.02341v1#bib.bib215)]对原始模型进行了多项优化，进一步提高了在复杂场景中的预测性能，并能够生成具有丰富细节的高质量深度图像。Depth
    Pro [[216](https://arxiv.org/html/2501.02341v1#bib.bib216)]基于多尺度ViT架构，可以快速生成具有高分辨率和高频细节的度量准确的深度图像，是处理复杂深度估计任务的有效工具。
- en: 4 Datasets and Platforms for UAVs
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 无人机的数据集与平台
- en: This section reviews publicly available datasets and simulation platforms relevant
    to UAV research, which serve as essential resources for advancing integrated studies
    on foundation model (FM)-based UAV systems. High-quality datasets form the cornerstone
    of UAV vision algorithms and autonomous behavior learning by offering diverse
    and comprehensive training data. Meanwhile, 3D simulation platforms provide safe
    and controlled virtual environments for the development, testing, and validation
    of UAV systems. These platforms can emulate complex scenarios and environmental
    conditions, enabling researchers to conduct experiments in a risk-free and cost-effective
    manner.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 本节回顾了与无人机（UAV）研究相关的公开数据集和仿真平台，这些平台是推动基于基础模型（FM）的无人机系统集成研究的重要资源。高质量的数据集为无人机视觉算法和自主行为学习奠定了基础，通过提供多样且全面的训练数据。同时，3D仿真平台为无人机系统的开发、测试和验证提供了安全和可控的虚拟环境。这些平台能够模拟复杂的场景和环境条件，使研究人员能够在无风险且成本效益高的情况下进行实验。
- en: 'We present a collection of open-source datasets, primarily utilized in the
    development of UAV systems, all of which have been verified as publicly accessible
    for download. The datasets are organized in Tables [3](https://arxiv.org/html/2501.02341v1#S4.T3
    "Table 3 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility"),[4](https://arxiv.org/html/2501.02341v1#S4.T4
    "Table 4 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility"),[5](https://arxiv.org/html/2501.02341v1#S4.T5
    "Table 5 ‣ 4.1.4 Action Recognition ‣ 4.1 General Domain Datasets ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility"),[6](https://arxiv.org/html/2501.02341v1#S4.T6 "Table 6
    ‣ 4.1.5 Navigation and Localization ‣ 4.1 General Domain Datasets ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility"),[7](https://arxiv.org/html/2501.02341v1#S4.T7 "Table 7
    ‣ 4.2.1 Transportation ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility"),[8](https://arxiv.org/html/2501.02341v1#S4.T8 "Table 8 ‣ 4.2.2 Remote
    Sensing ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms for UAVs ‣ UAVs
    Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility"),[9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility"). The ”Year” column indicates the most recent update for each dataset;
    if no update has been made, the publication year of the associated paper is listed
    instead. The images and videos in the ”Types” column default to RGB.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了一系列开源数据集，主要用于无人机（UAV）系统的开发，所有数据集都已确认可以公开访问并进行下载。这些数据集在表格[3](https://arxiv.org/html/2501.02341v1#S4.T3
    "Table 3 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")，[4](https://arxiv.org/html/2501.02341v1#S4.T4
    "Table 4 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")，[5](https://arxiv.org/html/2501.02341v1#S4.T5
    "Table 5 ‣ 4.1.4 Action Recognition ‣ 4.1 General Domain Datasets ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")，[6](https://arxiv.org/html/2501.02341v1#S4.T6 "Table 6
    ‣ 4.1.5 Navigation and Localization ‣ 4.1 General Domain Datasets ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")，[7](https://arxiv.org/html/2501.02341v1#S4.T7 "Table 7
    ‣ 4.2.1 Transportation ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility")，[8](https://arxiv.org/html/2501.02341v1#S4.T8 "Table 8 ‣ 4.2.2 Remote
    Sensing ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms for UAVs ‣ UAVs
    Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility")，[9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility")。 “年份”列表示每个数据集的最新更新日期；如果没有更新，则列出相关论文的出版年份。 "类型"列中的图像和视频默认为RGB。'
- en: 'The datasets cover a variety of formats, including video, RGB images (the default
    format in the tables), LiDAR point clouds, infrared images, Depth images, and
    textual data (such as descriptions or annotations). Video and RGB images are the
    predominant data types, while textual data is less common. Notably, some datasets
    have been updated to include new functionalities. For example, the EAR dataset
    [[246](https://arxiv.org/html/2501.02341v1#bib.bib246)] was enhanced with subtitles
    and question-answering capabilities, evolving into the CapEAR dataset [[247](https://arxiv.org/html/2501.02341v1#bib.bib247)],
    which is now suitable for VQA tasks. Most of the datasets listed in the tables
    were collected from outdoor environments and are categorized into two types: general
    domain datasets and domain-specific datasets.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集涵盖了多种格式，包括视频、RGB 图像（表格中的默认格式）、LiDAR 点云、红外图像、深度图像和文本数据（如描述或注释）。视频和 RGB 图像是主要的数据类型，而文本数据则较为少见。值得注意的是，一些数据集已更新以包括新功能。例如，EAR
    数据集 [[246](https://arxiv.org/html/2501.02341v1#bib.bib246)] 增强了字幕和问答功能，演变为 CapEAR
    数据集 [[247](https://arxiv.org/html/2501.02341v1#bib.bib247)]，现在适用于 VQA 任务。表格中列出的多数数据集来自户外环境，并分为两类：通用领域数据集和特定领域数据集。
- en: 'Table 3: UAV-oriented Datasets on Environmental Perception & Event Recognition'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：面向 UAV 的环境感知与事件识别数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| Environmental Perception |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 环境感知 |'
- en: '| AirFisheye [[248](https://arxiv.org/html/2501.02341v1#bib.bib248)] | 2024
    | Fisheye image Depth image'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '| AirFisheye [[248](https://arxiv.org/html/2501.02341v1#bib.bib248)] | 2024
    | 鱼眼图像 深度图像'
- en: Point cloud
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 点云
- en: IMU | Over 26,000 fisheye images in total. Data is collected at a rate of 10
    frames per second. [\faExternalLink](https://collaborating.tuhh.de/ilt/airfisheye-dataset)
    |
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: IMU | 共计 26,000 多张鱼眼图像，数据采集速率为每秒 10 帧。 [\faExternalLink](https://collaborating.tuhh.de/ilt/airfisheye-dataset)
    |
- en: '| SynDrone [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)] | 2023 |
    Image Depth image'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '| SynDrone [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)] | 2023 |
    图像 深度图像'
- en: Point cloud | Contains 72,000 annotation samples, providing 28 types of pixel-level
    and object-level annotations. [\faExternalLink](https://github.com/LTTM/Syndrone)
    |
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 点云 | 包含 72,000 个注释样本，提供 28 种像素级和对象级注释。 [\faExternalLink](https://github.com/LTTM/Syndrone)
    |
- en: '| WildUAV [[249](https://arxiv.org/html/2501.02341v1#bib.bib249)] | 2022 |
    Image Video'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '| WildUAV [[249](https://arxiv.org/html/2501.02341v1#bib.bib249)] | 2022 |
    图像 视频'
- en: Depth image
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 深度图像
- en: Metadata | Mapping images are provided as 24-bit PNG files, with the resolution
    of 5280x3956\. Video images are provided as JPG files at a resolution of 3840x2160\.
    There are 16 possible class labels detailed. [\faExternalLink](https://github.com/hrflr/wuav)
    |
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据 | 映射图像以 24 位 PNG 文件提供，分辨率为 5280x3956。视频图像以 JPG 格式提供，分辨率为 3840x2160。有 16
    个可能的类别标签，详细列出。 [\faExternalLink](https://github.com/hrflr/wuav) |
- en: '| Event Recognition |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 事件识别 |'
- en: '| CapERA [[247](https://arxiv.org/html/2501.02341v1#bib.bib247)] | 2023 | Video
    Text | 2864 videos, each with 5 descriptions, totaling 14,320 Texts. Each video
    lasts 5 seconds and is captured at 30 frames/second with a resolution of 640 ×
    640 pixels. [\faExternalLink](https://github.com/yakoubbazi/CapEra) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| CapERA [[247](https://arxiv.org/html/2501.02341v1#bib.bib247)] | 2023 | 视频
    文本 | 2864 个视频，每个视频有 5 个描述，总计 14,320 条文本。每个视频时长 5 秒，以每秒 30 帧的速度拍摄，分辨率为 640 × 640
    像素。 [\faExternalLink](https://github.com/yakoubbazi/CapEra) |'
- en: '| ERA [[246](https://arxiv.org/html/2501.02341v1#bib.bib246)] | 2020 | Video
    | A total of 2,864 videos, including disaster events, traffic accidents, sports
    competitions and other 25 categories. Each video is 24 frames/second for 5 seconds.
    [\faExternalLink](https://lcmou.github.io/ERA_Dataset) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| ERA [[246](https://arxiv.org/html/2501.02341v1#bib.bib246)] | 2020 | 视频 |
    共计 2,864 个视频，包括灾难事件、交通事故、体育竞赛等 25 类。每个视频 24 帧/秒，时长 5 秒。 [\faExternalLink](https://lcmou.github.io/ERA_Dataset)
    |'
- en: '| VIRAT [[250](https://arxiv.org/html/2501.02341v1#bib.bib250)] | 2016 | Video
    | 25 hours of static ground video and 4 hours of dynamic aerial video. There are
    23 event types involved. [\faExternalLink](https://viratdata.org/) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| VIRAT [[250](https://arxiv.org/html/2501.02341v1#bib.bib250)] | 2016 | 视频
    | 25 小时的静态地面视频和 4 小时的动态空中视频。涉及 23 种事件类型。 [\faExternalLink](https://viratdata.org/)
    |'
- en: 'Table 4: UAV-oriented Datasets on Object Tracking'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：面向 UAV 的目标跟踪数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| WebUAV-3M [[251](https://arxiv.org/html/2501.02341v1#bib.bib251)] | 2024
    | Video Text'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '| WebUAV-3M [[251](https://arxiv.org/html/2501.02341v1#bib.bib251)] | 2024
    | 视频 文本'
- en: Audio | 4,500 videos totaling more than 3.3 million frames with 223 target categories,
    providing natural language and audio descriptions. [\faExternalLink](https://github.com/983632847/WebUAV-3M)
    |
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 音频 | 4,500个视频，总计超过330万帧，涵盖223个目标类别，提供自然语言和音频描述。[ \faExternalLink](https://github.com/983632847/WebUAV-3M)
    |
- en: '| UAVDark135 [[252](https://arxiv.org/html/2501.02341v1#bib.bib252)] | 2022
    | Video | 135 video sequences with over 125,000 manually annotated frames. [\faExternalLink](https://vision4robotics.github.io/project/uavdark135/)
    |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| UAVDark135 [[252](https://arxiv.org/html/2501.02341v1#bib.bib252)] | 2022
    | 视频 | 135个视频序列，超过125,000帧手工标注的帧。[ \faExternalLink](https://vision4robotics.github.io/project/uavdark135/)
    |'
- en: '| DUT-VTUAV [[253](https://arxiv.org/html/2501.02341v1#bib.bib253)] | 2022
    | RGB-T Image | Nearly 1.7 million well-aligned visible-thermal (RGB-T) image
    pairs with 500 sequences for unveiling the power of RGB-T tracking. Including
    13 sub-classes and 15 scenes across 2 cities. [\faExternalLink](https://github.com/zhang-pengyu/DUT-VTUAV)
    |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| DUT-VTUAV [[253](https://arxiv.org/html/2501.02341v1#bib.bib253)] | 2022
    | RGB-T 图像 | 几乎170万对对齐良好的可见光-热红外（RGB-T）图像，包含500个序列，用于揭示RGB-T跟踪的强大能力。包括13个子类别和15个场景，跨越2个城市。[
    \faExternalLink](https://github.com/zhang-pengyu/DUT-VTUAV) |'
- en: '| TNL2K [[254](https://arxiv.org/html/2501.02341v1#bib.bib254)] | 2022 | Video
    Infrared video'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '| TNL2K [[254](https://arxiv.org/html/2501.02341v1#bib.bib254)] | 2022 | 视频
    红外视频  '
- en: Text | 2,000 video sequences, comprising 1,244,340 frames and 663 words. [\faExternalLink](https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit)
    |
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 文本 | 2,000个视频序列，包含1,244,340帧和663个单词。[ \faExternalLink](https://github.com/wangxiao5791509/TNL2K_evaluation_toolkit)
    |
- en: '| PRAI-1581 [[255](https://arxiv.org/html/2501.02341v1#bib.bib255)] | 2020
    | Image | 39,461 images of 1581 person identities. [\faExternalLink](https://github.com/stormyoung/PRAI-1581)
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| PRAI-1581 [[255](https://arxiv.org/html/2501.02341v1#bib.bib255)] | 2020
    | 图像 | 39,461张图像，涵盖1581个身份的人员。[ \faExternalLink](https://github.com/stormyoung/PRAI-1581)
    |'
- en: '| VOT-ST2020/ VOT-RT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)]
    | 2020 | Video | 1,000 sequences, each varying in length, with an average length
    of approximately 100 frames. [\faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| VOT-ST2020/ VOT-RT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)]
    | 2020 | 视频 | 1,000个序列，每个序列的长度不同，平均长度约为100帧。[ \faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
- en: '| VOT-LT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | Video | 50 sequences, each with a length of approximately 40,000 frames. [\faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| VOT-LT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | 视频 | 50个序列，每个序列的长度大约为40,000帧。[ \faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
- en: '| VOT-RGBT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | Video Infrared video | 50 sequences, each with a length of approximately 40,000
    frames. [\faExternalLink](https://votchallenge.net/vot2020/dataset.html) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| VOT-RGBT2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | 视频 红外视频 | 50个序列，每个序列的长度大约为40,000帧。[ \faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
- en: '| VOT-RGBD2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | Video Depth image | 80 sequences with a total of approximately 101,956 frames.
    [\faExternalLink](https://votchallenge.net/vot2020/dataset.html) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| VOT-RGBD2020 [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)] | 2020
    | 视频 深度图像 | 80个序列，总计约101,956帧。[ \faExternalLink](https://votchallenge.net/vot2020/dataset.html)
    |'
- en: '| GOT-10K [[257](https://arxiv.org/html/2501.02341v1#bib.bib257)] | 2019 |
    Image Video | 420 video clips belonging to 84 object categories and 31 motion
    categories. [\faExternalLink](http://got-10k.aitestunion.com/) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| GOT-10K [[257](https://arxiv.org/html/2501.02341v1#bib.bib257)] | 2019 |
    图像 视频 | 420个视频剪辑，属于84个物体类别和31个运动类别。[ \faExternalLink](http://got-10k.aitestunion.com/)
    |'
- en: '| DTB70 [[258](https://arxiv.org/html/2501.02341v1#bib.bib258)] | 2017 | Video
    | 70 video sequences, each consisting of multiple video frames, with each frame
    containing an RGB image at a resolution of 1280x720 pixels. [\faExternalLink](https://github.com/flyers/drone-tracking)
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| DTB70 [[258](https://arxiv.org/html/2501.02341v1#bib.bib258)] | 2017 | 视频
    | 70个视频序列，每个序列包含多个视频帧，每帧包含一个分辨率为1280x720像素的RGB图像。[ \faExternalLink](https://github.com/flyers/drone-tracking)
    |'
- en: '| Stanford Drone [[259](https://arxiv.org/html/2501.02341v1#bib.bib259)] |
    2016 | Video | 19,000 + target tracks, containing 6 types of targets, about 20,000
    target interactions, 40,000 target interactions with the environment, covering
    100 + scenes in the university campus. [\faExternalLink](https://cvgl.stanford.edu/projects/uav_data/)
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Stanford Drone [[259](https://arxiv.org/html/2501.02341v1#bib.bib259)] |
    2016 | 视频 | 19,000多个目标轨迹，涵盖6种目标类型，约20,000次目标交互，40,000次目标与环境交互，覆盖大学校园的100多个场景。[
    \faExternalLink](https://cvgl.stanford.edu/projects/uav_data/) |'
- en: '| COWC [[260](https://arxiv.org/html/2501.02341v1#bib.bib260)] | 2016 | Image
    | 32,716 unique vehicles and 58,247 non-vehicle targets were labeled. Covering
    6 different geographical areas. [\faExternalLink](https://gdo152.llnl.gov/cowc/)
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| COWC [[260](https://arxiv.org/html/2501.02341v1#bib.bib260)] | 2016 | 图像
    | 标注了32,716个独特的车辆和58,247个非车辆目标，涵盖了6个不同的地理区域。 [\faExternalLink](https://gdo152.llnl.gov/cowc/)
    |'
- en: 4.1 General Domain Datasets
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 一般领域数据集
- en: General domain datasets are designed to cater to a wide range of scenarios and
    are further categorized based on specific tasks, including Environmental Perception,
    Event Recognition, Object Tracking, Action Recognition, and Navigation. Within
    the Environmental Perception category, we focus on tasks such as object detection,
    segmentation, and depth estimation. Although tasks like event recognition, object
    tracking, and action recognition can also be considered part of Environmental
    Perception, we have listed them separately to provide a clearer presentation of
    the datasets.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一般领域数据集旨在适应广泛的场景，并根据特定任务进一步分类，包括环境感知、事件识别、目标跟踪、动作识别和导航。在环境感知类别中，我们重点关注物体检测、分割和深度估计等任务。虽然事件识别、目标跟踪和动作识别等任务也可以视为环境感知的一部分，但我们将它们单独列出，以便更清晰地展示数据集。
- en: 4.1.1 Environmental Perception
  id: totrans-233
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 环境感知
- en: 'This part presents the datasets used primarily for object detection, segmentation,
    and depth estimation, as shown in Table [3](https://arxiv.org/html/2501.02341v1#S4.T3
    "Table 3 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility"). For instance, the AirFisheye dataset [[248](https://arxiv.org/html/2501.02341v1#bib.bib248)]
    is specifically designed for tasks such as object detection, segmentation, and
    depth estimation in complex urban environments captured by UAVs. Its multimodal
    data, including visual, thermal imaging, and LiDAR, provide comprehensive information
    for analyzing scenes in these challenging urban settings. The SynDrone dataset
    [[244](https://arxiv.org/html/2501.02341v1#bib.bib244)] is a large-scale synthetic
    dataset generated using Carla, intended for detection and segmentation tasks in
    urban environments. The WildUAV dataset [[249](https://arxiv.org/html/2501.02341v1#bib.bib249)]
    provides high-resolution RGB images and depth ground truth data, focusing on monocular
    visual depth estimation while supporting precise drone flight control in complex
    environments.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '本部分展示了主要用于物体检测、分割和深度估计的数据集，如表[3](https://arxiv.org/html/2501.02341v1#S4.T3
    "Table 3 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")所示。例如，AirFisheye数据集[[248](https://arxiv.org/html/2501.02341v1#bib.bib248)]专门用于在复杂的城市环境中，由无人机捕获的物体检测、分割和深度估计任务。其多模态数据，包括视觉、热成像和激光雷达（LiDAR），为在这些挑战性的城市环境中分析场景提供了全面的信息。SynDrone数据集[[244](https://arxiv.org/html/2501.02341v1#bib.bib244)]是一个使用Carla生成的大规模合成数据集，旨在用于城市环境中的检测和分割任务。WildUAV数据集[[249](https://arxiv.org/html/2501.02341v1#bib.bib249)]提供了高分辨率的RGB图像和深度地面真实数据，重点关注单目视觉深度估计，同时支持在复杂环境中精确的无人机飞行控制。'
- en: 4.1.2 Event Recognition
  id: totrans-235
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 事件识别
- en: 'The typical datasets used for event recognition are listed in Table [3](https://arxiv.org/html/2501.02341v1#S4.T3
    "Table 3 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility"). The EAR dataset [[246](https://arxiv.org/html/2501.02341v1#bib.bib246)]
    serves as a video-based benchmark for event recognition, encompassing 25 event
    classes, including post-earthquake, flood, fire, landslide, mudslide, traffic
    collision, traffic congestion, harvesting, plowing, construction, police chase,
    conflict, various sports (e.g., baseball, basketball, cycling), and social activities
    (e.g., parties, concerts, protests, religious activities). It consists of 2,864
    videos, each lasting 5 seconds, collected from YouTube using ”drone” and ”UAV”
    as search keywords. Similarly, the VIRAT dataset [[250](https://arxiv.org/html/2501.02341v1#bib.bib250)]
    focuses on event recognition in surveillance videos, including events like traffic
    accidents and crowd gatherings. Although not captured by UAVs, the VIRAT dataset
    offers similar aerial perspectives, making it relevant for drone-based scene analysis.
    Together, these datasets provide valuable resources for advancing research in
    event detection and scene understanding, particularly in the context of integrating
    UAV applications with LLMs.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '用于事件识别的典型数据集列出了在表格[3](https://arxiv.org/html/2501.02341v1#S4.T3 "Table 3 ‣
    4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")中。EAR数据集[[246](https://arxiv.org/html/2501.02341v1#bib.bib246)]作为基于视频的事件识别基准，涵盖了25个事件类别，包括地震后、洪水、火灾、滑坡、泥石流、交通事故、交通拥堵、收割、耕作、建设、警察追捕、冲突、各种体育活动（如棒球、篮球、骑行）和社交活动（如聚会、音乐会、抗议、宗教活动）。该数据集包含2,864个视频，每个视频持续5秒，来自YouTube，搜索关键词为“drone”和“UAV”。类似地，VIRAT数据集[[250](https://arxiv.org/html/2501.02341v1#bib.bib250)]专注于监控视频中的事件识别，包括交通事故和人群聚集等事件。虽然该数据集并非由无人机拍摄，但它提供了类似的空中视角，使其在基于无人机的场景分析中具有相关性。这些数据集共同为事件检测和场景理解研究的进展提供了宝贵资源，尤其是在将UAV应用与LLM集成的背景下。'
- en: 4.1.3 Object Tracking
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3 目标跟踪
- en: 'Object tracking tasks rely on diverse datasets to advance research across various
    domains. Table [4](https://arxiv.org/html/2501.02341v1#S4.T4 "Table 4 ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility") lists typical datasets for this task. The WebUAV-3M dataset
    [[251](https://arxiv.org/html/2501.02341v1#bib.bib251)] is a large-scale benchmark
    for UAV object tracking, comprising 4,500 videos across 233 object categories.
    It serves as a solid foundation for UAV tracking in general scenarios and includes
    multimodal data, such as audio and natural language descriptions, enabling the
    exploration of multimodal UAV tracking approaches. The TNL2K dataset [[254](https://arxiv.org/html/2501.02341v1#bib.bib254)]
    focuses on natural language-guided object tracking and includes 2,000 video sequences
    annotated with bounding boxes and detailed natural language descriptions that
    capture the target object’s category, shape, attributes, features, and spatial
    location. To address challenging scenarios, TNL2K incorporates adversarial samples
    and sequences with significant appearance changes, offering both RGB and infrared
    modalities to support cross-modal tracking research. The VOT2020 dataset [[256](https://arxiv.org/html/2501.02341v1#bib.bib256)]
    provides a comprehensive collection of five specialized datasets tailored to specific
    tasks: short-term tracking, real-time tracking, long-term tracking, thermal tracking,
    and depth tracking. These datasets collectively address a wide range of tracking
    challenges, fostering innovation across different tracking paradigms.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '物体追踪任务依赖于多样化的数据集，以推动各个领域的研究发展。表格[4](https://arxiv.org/html/2501.02341v1#S4.T4
    "Table 4 ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")列出了该任务的典型数据集。WebUAV-3M数据集[[251](https://arxiv.org/html/2501.02341v1#bib.bib251)]是一个大规模的无人机物体追踪基准，包含4,500个视频，覆盖233个物体类别。它为一般场景下的无人机追踪提供了坚实的基础，并包括多模态数据，如音频和自然语言描述，使得探索多模态无人机追踪方法成为可能。TNL2K数据集[[254](https://arxiv.org/html/2501.02341v1#bib.bib254)]聚焦于自然语言引导的物体追踪，包含2,000个带有边界框和详细自然语言描述的视频序列，描述了目标物体的类别、形状、属性、特征和空间位置。为了应对具有挑战性的场景，TNL2K数据集包括对抗样本和外观变化较大的序列，提供RGB和红外两种模态，以支持跨模态追踪研究。VOT2020数据集[[256](https://arxiv.org/html/2501.02341v1#bib.bib256)]提供了五个专门为特定任务定制的数据集：短期追踪、实时追踪、长期追踪、热成像追踪和深度追踪。这些数据集共同解决了各种追踪挑战，促进了不同追踪范式的创新。'
- en: 4.1.4 Action Recognition
  id: totrans-239
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4 动作识别
- en: 'Enabling drones to comprehend human actions and interpret commands via gestures
    is a pivotal area of research. Table [5](https://arxiv.org/html/2501.02341v1#S4.T5
    "Table 5 ‣ 4.1.4 Action Recognition ‣ 4.1 General Domain Datasets ‣ 4 Datasets
    and Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility") lists UAV-oriented datasets for action recognition. The
    Aeriform In-Action dataset [[261](https://arxiv.org/html/2501.02341v1#bib.bib261)]
    targets human action recognition in aerial videos, featuring 32 high-resolution
    videos across 13 action categories. This dataset is specifically designed to address
    the unique challenges associated with action recognition in aerial surveillance.
    The MEVA dataset [[262](https://arxiv.org/html/2501.02341v1#bib.bib262)] offers
    a large-scale, multi-view, multimodal dataset comprising 9,300 hours of continuous
    video captured by UAVs and ground cameras. It covers 37 activity categories and
    facilitates advanced tasks, such as multi-view activity detection. Additionally,
    the UAV-Human dataset [[79](https://arxiv.org/html/2501.02341v1#bib.bib79)] provides
    67,428 multimodal video sequences, encompassing 119 subjects for action recognition.
    In addition to action recognition, it supports tasks such as pose estimation and
    person re-identification. With its diverse range of backgrounds, lighting conditions,
    and environments, the dataset serves as a comprehensive benchmark for drone-based
    human behavior analysis.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使无人机能够理解人类动作并通过手势解读命令是一个重要的研究领域。表[5](https://arxiv.org/html/2501.02341v1#S4.T5
    "表5 ‣ 4.1.4 动作识别 ‣ 4.1 一般领域数据集 ‣ 4 UAV数据集和平台 ‣ UAV与LLM相遇：低空行动智能化的概览与展望")列出了面向UAV的动作识别数据集。Aeriform
    In-Action数据集[[261](https://arxiv.org/html/2501.02341v1#bib.bib261)]针对空中视频中的人类动作识别，包含32个高分辨率视频，涵盖13种动作类别。该数据集专门设计用来解决与空中监视中的动作识别相关的独特挑战。MEVA数据集[[262](https://arxiv.org/html/2501.02341v1#bib.bib262)]提供了一个大规模、多视角、多模态数据集，包含9,300小时无人机和地面摄像机拍摄的连续视频。它涵盖了37种活动类别，并支持多视角活动检测等高级任务。此外，UAV-Human数据集[[79](https://arxiv.org/html/2501.02341v1#bib.bib79)]提供了67,428个多模态视频序列，涵盖119个受试者的动作识别。除了动作识别，它还支持姿态估计和人员重识别等任务。由于具有多样的背景、光照条件和环境，该数据集为基于无人机的人类行为分析提供了一个全面的基准。
- en: 'Table 5: UAV-oriented Datasets on Action Recognition'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：面向UAV的动作识别数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| Aeriform in-action [[261](https://arxiv.org/html/2501.02341v1#bib.bib261)]
    | 2023 | Video | 32 videos, 13 types of action, 55,477 frames, 40,000 callouts.
    [\faExternalLink](https://surbhi-31.github.io/Aeriform-in-action/) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Aeriform in-action [[261](https://arxiv.org/html/2501.02341v1#bib.bib261)]
    | 2023 | 视频 | 32个视频，13种动作类型，55,477帧，40,000次标注。[ \faExternalLink](https://surbhi-31.github.io/Aeriform-in-action/)
    |'
- en: '| MEVA [[262](https://arxiv.org/html/2501.02341v1#bib.bib262)] | 2021 | Video
    Infrared video'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '| MEVA [[262](https://arxiv.org/html/2501.02341v1#bib.bib262)] | 2021 | 视频
    红外视频'
- en: GPS
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: GPS
- en: Point cloud | Total 9,300 hours of video, 144 hours of activity notes, 37 activity
    types, over 2.7 million GPS track points. [\faExternalLink](https://mevadata.org/)
    |
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 点云 | 总计9,300小时的视频，144小时的活动记录，37种活动类型，超过270万个GPS轨迹点。[ \faExternalLink](https://mevadata.org/)
    |
- en: '| UAV-Human [[79](https://arxiv.org/html/2501.02341v1#bib.bib79)] | 2021 |
    Video Night-vision video'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '| UAV-Human [[79](https://arxiv.org/html/2501.02341v1#bib.bib79)] | 2021 |
    视频 夜视视频'
- en: Fisheye video
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 鱼眼视频
- en: Depth video
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 深度视频
- en: Infrared video
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 红外视频
- en: Skeleton | 67,428 videos (155 types of actions, 119 subjects), 22,476 frames
    of annotated key points (17 key points), 41,290 frames of people re-recognition
    (1,144 identities), 22,263 frames of attribute recognition (such as gender, hat,
    backpack, etc.). [\faExternalLink](https://github.com/SUTDCV/UAV-Human) |
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 骨架 | 67,428个视频（155种动作，119个受试者），22,476帧标注的关键点（17个关键点），41,290帧人员重识别（1,144个身份），22,263帧属性识别（如性别、帽子、背包等）。[
    \faExternalLink](https://github.com/SUTDCV/UAV-Human) |
- en: '| MOD20 [[263](https://arxiv.org/html/2501.02341v1#bib.bib263)] | 2020 | Video
    | 20 types of action, 2,324 videos, 503,086 frames. [\faExternalLink](https://asankagp.github.io/mod20/)
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| MOD20 [[263](https://arxiv.org/html/2501.02341v1#bib.bib263)] | 2020 | 视频
    | 20种动作，2,324个视频，503,086帧。[ \faExternalLink](https://asankagp.github.io/mod20/)
    |'
- en: '| NEC-DRONE [[264](https://arxiv.org/html/2501.02341v1#bib.bib264)] | 2020
    | Video | 5,250 videos containing 256 minutes of action videos involving 19 actors
    and 16 action categories [\faExternalLink](https://www.nec-labs.com/research/media-analytics/projects/unsupervised-semi-supervised-domain-adaptation-for-action-recognition-from-drones/)
    |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| NEC-DRONE [[264](https://arxiv.org/html/2501.02341v1#bib.bib264)] | 2020
    | 视频 | 包含5,250个视频，总计256分钟，涉及19名演员和16种动作类别的动作视频 [\faExternalLink](https://www.nec-labs.com/research/media-analytics/projects/unsupervised-semi-supervised-domain-adaptation-for-action-recognition-from-drones/)
    |'
- en: '| Drone-Action [[265](https://arxiv.org/html/2501.02341v1#bib.bib265)] | 2019
    | Video | 240 HD videos, 66,919 frames, 13 types of action. [\faExternalLink](https://asankagp.github.io/droneaction/)
    |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Drone-Action [[265](https://arxiv.org/html/2501.02341v1#bib.bib265)] | 2019
    | 视频 | 240个高清视频，66,919帧，13种动作类型。 [\faExternalLink](https://asankagp.github.io/droneaction/)
    |'
- en: '| UAV-GESTURE [[266](https://arxiv.org/html/2501.02341v1#bib.bib266)] | 2019
    | Video | 119 videos, 37,151 frames, 13 types of gestures, 10 actors. [\faExternalLink](https://asankagp.github.io/uavgesture/)
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| UAV-GESTURE [[266](https://arxiv.org/html/2501.02341v1#bib.bib266)] | 2019
    | 视频 | 119个视频，37,151帧，13种手势，10名演员。 [\faExternalLink](https://asankagp.github.io/uavgesture/)
    |'
- en: 4.1.5 Navigation and Localization
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5 导航和定位
- en: 'Table [6](https://arxiv.org/html/2501.02341v1#S4.T6 "Table 6 ‣ 4.1.5 Navigation
    and Localization ‣ 4.1 General Domain Datasets ‣ 4 Datasets and Platforms for
    UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility") presents UAV-oriented datasets for navigation and localization. The
    CityNav dataset [[267](https://arxiv.org/html/2501.02341v1#bib.bib267)] is a dataset
    designed for language-guided aerial navigation tasks, aimed at assisting drones
    in navigating city-scale 3D environments using natural language instructions.
    The dataset comprises 32,000 tasks, offering extensive geographic information
    and detailed urban environment models. The AerialVLN dataset [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)]
    focuses on drone navigation through the integration of visual and linguistic cues,
    enabling drones to perform flight tasks in complex environments based on natural
    language commands, thereby enhancing their adaptability in dynamic settings. The
    VIGOR dataset [[269](https://arxiv.org/html/2501.02341v1#bib.bib269)] provides
    a cross-view image localization dataset that facilitates accurate geographical
    positioning of drones from diverse perspectives, improving image matching and
    position calibration accuracy in complex geographical environments. The University-1652
    dataset [[270](https://arxiv.org/html/2501.02341v1#bib.bib270)] serves as a benchmark
    for cross-view geo-localization, bridging the visual gap between ground-level
    and satellite perspectives by incorporating drone-view images. It includes paired
    images from synthetic drones, satellites, and ground cameras for 1,652 universities,
    supporting two tasks: drone-view target localization and drone navigation.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [6](https://arxiv.org/html/2501.02341v1#S4.T6 "Table 6 ‣ 4.1.5 Navigation
    and Localization ‣ 4.1 General Domain Datasets ‣ 4 Datasets and Platforms for
    UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility") 展示了面向无人机的导航和定位数据集。CityNav数据集 [[267](https://arxiv.org/html/2501.02341v1#bib.bib267)]
    是一个为语言引导的空中导航任务设计的数据集，旨在帮助无人机利用自然语言指令在城市规模的3D环境中导航。该数据集包含32,000个任务，提供了广泛的地理信息和详细的城市环境模型。AerialVLN数据集
    [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)] 专注于通过视觉和语言线索的结合进行无人机导航，使无人机能够根据自然语言命令在复杂环境中执行飞行任务，从而提高其在动态环境中的适应能力。VIGOR数据集
    [[269](https://arxiv.org/html/2501.02341v1#bib.bib269)] 提供了一个跨视角图像定位数据集，帮助无人机从不同视角进行准确的地理定位，提高在复杂地理环境中的图像匹配和位置校准精度。University-1652数据集
    [[270](https://arxiv.org/html/2501.02341v1#bib.bib270)] 是一个用于跨视角地理定位的基准数据集，通过整合无人机视角图像，弥合地面视角和卫星视角之间的视觉差距。该数据集包括来自1,652所大学的合成无人机、卫星和地面摄像机的配对图像，支持两项任务：无人机视角目标定位和无人机导航。'
- en: 'Table 6: UAV-oriented Datasets on Navigation and Localization'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表格6：面向无人机的导航和定位数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| CityNav [[267](https://arxiv.org/html/2501.02341v1#bib.bib267)] | 2024 |
    Image Text | 32,000 natural language descriptions and companion tracks. [\faExternalLink](https://water-cookie.github.io/city-nav-proj/)
    |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| CityNav [[267](https://arxiv.org/html/2501.02341v1#bib.bib267)] | 2024 |
    图像文本 | 32,000条自然语言描述及伴随轨迹。 [\faExternalLink](https://water-cookie.github.io/city-nav-proj/)
    |'
- en: '| CNER-UAV [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)] | 2024 |
    Text | 12,000 labeled samples containing 5 types of address labels (e.g., building,
    unit, floor, room, etc.). [\faExternalLink](https://github.com/zhhvvv/CNER-UAV)
    |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CNER-UAV [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)] | 2024 |
    文本 | 12,000个标注样本，包含5种地址标签（例如，建筑物、单元、楼层、房间等）。[\\faExternalLink](https://github.com/zhhvvv/CNER-UAV)
    |'
- en: '| AerialVLN [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)] | 2023
    | Simulator path Text | It contains 25 city-level scenes, including urban areas,
    factories, parks and villages. A total of 8,446 paths. Each path is provided with
    3 natural language descriptions, totaling 25,338 instructions. [\faExternalLink](https://github.com/AirVLN/AirVLN)
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| AerialVLN [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)] | 2023
    | 仿真路径文本 | 它包含25个城市级场景，包括城市区域、工厂、公园和村庄，共8,446条路径。每条路径提供3个自然语言描述，总计25,338条指令。[\\faExternalLink](https://github.com/AirVLN/AirVLN)
    |'
- en: '| DenseUAV [[272](https://arxiv.org/html/2501.02341v1#bib.bib272)] | 2023 |
    Image | Training set: 6,768 UAV images, 13,536 satellite images. Test set: 2,331
    UAV query images and 4,662 satellite images. [\faExternalLink](https://github.com/Dmmm1997/DenseUAV)
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| DenseUAV [[272](https://arxiv.org/html/2501.02341v1#bib.bib272)] | 2023 |
    图像 | 训练集：6,768张无人机图像，13,536张卫星图像。测试集：2,331张无人机查询图像和4,662张卫星图像。[\\faExternalLink](https://github.com/Dmmm1997/DenseUAV)
    |'
- en: '| map2seq [[273](https://arxiv.org/html/2501.02341v1#bib.bib273)] | 2022 |
    Image Text'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '| map2seq [[273](https://arxiv.org/html/2501.02341v1#bib.bib273)] | 2022 |
    图像文本'
- en: Map path | 29,641 panoramic images, 7,672 navigation instruction Texts. [\faExternalLink](https://map2seq.schumann.pub/dataset/download/)
    |
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 地图路径 | 29,641张全景图像，7,672条导航指令文本。[\\faExternalLink](https://map2seq.schumann.pub/dataset/download/)
    |
- en: '| VIGOR [[269](https://arxiv.org/html/2501.02341v1#bib.bib269)] | 2021 | Image
    | 90,618 aerial images, 238,696 street panorama. [\faExternalLink](https://github.com/Jeff-Zilence/VIGOR)
    |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| VIGOR [[269](https://arxiv.org/html/2501.02341v1#bib.bib269)] | 2021 | 图像
    | 90,618张空中图像，238,696张街景全景图。[\\faExternalLink](https://github.com/Jeff-Zilence/VIGOR)
    |'
- en: '| University-1652 [[270](https://arxiv.org/html/2501.02341v1#bib.bib270)] |
    2020 | Image | 1,652 university buildings, involving 72 universities, 50,218 training
    images, 37,855 UAV perspective query images, 701 satellite perspective query images,
    and an additional 21,099 ordinary perspective and 5,580 street view perspective
    images were collected for training. [\faExternalLink](https://github.com/layumi/University1652-Baseline)
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| University-1652 [[270](https://arxiv.org/html/2501.02341v1#bib.bib270)] |
    2020 | 图像 | 收集了1,652个大学建筑，涉及72所大学，50,218张训练图像，37,855张无人机视角查询图像，701张卫星视角查询图像，以及另外21,099张普通视角和5,580张街景视角图像用于训练。[\\faExternalLink](https://github.com/layumi/University1652-Baseline)
    |'
- en: 4.2 Domain-specific Datasets
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 特定领域数据集
- en: Compared to general-domain datasets, domain-specific datasets are tailored for
    particular applications and categorized according to the specific domains they
    address, including Transportation, Remote Sensing, Agriculture, Industrial Applications,
    Emergency Response, Military Operations, and Wildlife.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 与通用领域数据集相比，特定领域数据集是针对特定应用量身定制的，并根据它们所涉及的具体领域进行分类，包括交通、遥感、农业、工业应用、应急响应、军事作战和野生动物。
- en: 4.2.1 Transportation
  id: totrans-270
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 交通
- en: 'Transportation scenes are among the most prevalent scenarios in UAV datasets
    and this part highlights datasets (as shown in Table [7](https://arxiv.org/html/2501.02341v1#S4.T7
    "Table 7 ‣ 4.2.1 Transportation ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and
    Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")) specifically designed for traffic monitoring, as well
    as vehicle and pedestrian detection tasks, which are key applications of UAV technology.
    The TrafficNight dataset [[274](https://arxiv.org/html/2501.02341v1#bib.bib274)]
    is an aerial multimodal dataset for nighttime vehicle monitoring, designed to
    address the limitations of existing aerial datasets in terms of lighting conditions
    and vehicle type representativeness. The dataset combines vertical RGB and thermal
    infrared imaging technologies, covering various scenes, including those with numerous
    semi-trailers, and provides specialized annotations. It also includes corresponding
    HD-MAP data for multi-vehicle tracking. The VisDrone dataset [[275](https://arxiv.org/html/2501.02341v1#bib.bib275)]
    is a large-scale benchmark supporting object detection and both single- and multi-object
    tracking in images and videos. Collected from 14 cities across China, it features
    high diversity and challenging scenarios, making it well-suited for evaluating
    algorithms in complex urban and suburban environments. The CADP dataset [[276](https://arxiv.org/html/2501.02341v1#bib.bib276)]
    emphasizes traffic accident analysis, enhancing small-object detection accuracy
    (e.g., pedestrians) using CCTV traffic monitoring videos. It integrates contextual
    mining techniques and an LSTM-based architecture for accident prediction. The
    CARPK dataset [[277](https://arxiv.org/html/2501.02341v1#bib.bib277)] introduces
    a novel method for parking lot vehicle counting using a spatially regularized
    region proposal network (LPN). It includes high-resolution UAV imagery with over
    90,000 vehicles, enhancing object detection and counting performance. The iSAID
    dataset [[278](https://arxiv.org/html/2501.02341v1#bib.bib278)] offers high-quality
    annotations for instance segmentation tasks, encompassing 655,451 labeled instances
    across 15 categories, thus supporting accurate object detection and scene analysis
    in UAV applications. Collectively, these datasets advance research in vehicle
    detection, object tracking, traffic monitoring, and UAV autonomous navigation,
    offering robust data resources for applications in intelligent transportation,
    UAV-based patrols, and delivery systems.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '交通场景是无人机数据集中最常见的场景之一，本部分重点介绍了专门用于交通监控以及车辆和行人检测任务的数据集，这些任务是无人机技术的关键应用。如表[7](https://arxiv.org/html/2501.02341v1#S4.T7
    "Table 7 ‣ 4.2.1 Transportation ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and
    Platforms for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")所示，TrafficNight数据集[[274](https://arxiv.org/html/2501.02341v1#bib.bib274)]是一个夜间车辆监控的空中多模态数据集，旨在解决现有空中数据集中光照条件和车辆类型代表性方面的局限性。该数据集结合了垂直RGB和热红外成像技术，涵盖了包括大量半挂车在内的各种场景，并提供了专业的标注。它还包括用于多车追踪的对应HD-MAP数据。VisDrone数据集[[275](https://arxiv.org/html/2501.02341v1#bib.bib275)]是一个支持图像和视频中的物体检测以及单一和多物体追踪的大规模基准数据集。该数据集来自中国14个城市，具有高度的多样性和挑战性场景，非常适合用于评估复杂的城市和郊区环境中的算法。CADP数据集[[276](https://arxiv.org/html/2501.02341v1#bib.bib276)]强调交通事故分析，通过使用CCTV交通监控视频提高小物体检测的准确性（例如行人）。它集成了上下文挖掘技术和基于LSTM的架构用于事故预测。CARPK数据集[[277](https://arxiv.org/html/2501.02341v1#bib.bib277)]提出了一种使用空间正则化区域提议网络（LPN）进行停车场车辆计数的新方法。它包括高分辨率的无人机影像，涵盖了超过90,000辆车辆，提升了物体检测和计数的表现。iSAID数据集[[278](https://arxiv.org/html/2501.02341v1#bib.bib278)]为实例分割任务提供了高质量的标注，涵盖了655,451个标注实例，涉及15个类别，从而支持在无人机应用中的准确物体检测和场景分析。这些数据集共同推动了车辆检测、物体追踪、交通监控和无人机自主导航的研究，为智能交通、无人机巡逻和配送系统等应用提供了强大的数据资源。'
- en: 'Table 7: UAV-oriented Datasets on Transportation'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：面向无人机的交通运输数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| TrafficNight [[274](https://arxiv.org/html/2501.02341v1#bib.bib274)] | 2024
    | Image Infrared Image'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '| TrafficNight [[274](https://arxiv.org/html/2501.02341v1#bib.bib274)] | 2024
    | 图像 红外图像'
- en: Video
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 视频
- en: Infrared Video
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 红外视频
- en: Map | The dataset consists of 2,200 pairs of annotated thermal infrared and
    sRGB image data, as well as video data from 7 traffic scenes, with a total duration
    of approximately 240 minutes. Each scene includes a high-precision map, providing
    a detailed layout and topological information. [\faExternalLink](https://github.com/AIMSPolyU/TrafficNight)
    |
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 地图 | 数据集包含2,200对标注的热红外和sRGB图像数据，以及来自7个交通场景的视频数据，总时长约240分钟。每个场景包括高精度地图，提供详细的布局和拓扑信息。[外部链接](https://github.com/AIMSPolyU/TrafficNight)
    |
- en: '| VisDrone [[275](https://arxiv.org/html/2501.02341v1#bib.bib275)] | 2022 |
    mage Video | 263 videos, 179,264 frames. 10,209 still images. More than 2,500,000
    object instance annotations. The data covers 14 different cities, covering a wide
    range of weather and light conditions. [\faExternalLink](http://aiskyeye.com/home/)
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone [[275](https://arxiv.org/html/2501.02341v1#bib.bib275)] | 2022 |
    图像视频 | 263个视频，179,264帧。10,209张静态图像。超过2,500,000个物体实例标注。数据覆盖14个不同的城市，涵盖了广泛的天气和光照条件。[外部链接](http://aiskyeye.com/home/)
    |'
- en: '| ITCVD [[279](https://arxiv.org/html/2501.02341v1#bib.bib279)] | 2020 | Image
    | A total of 173 aerial images were collected, including 135 in the training set
    with 23,543 vehicles and 38 in the test set with 5,545 vehicles. There is 60%
    regional overlap between the images, and there is no overlap between the training
    set and the test set. [\faExternalLink](https://research.utwente.nl/en/datasets/itcvd-dataset)
    |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| ITCVD [[279](https://arxiv.org/html/2501.02341v1#bib.bib279)] | 2020 | 图像
    | 共收集了173张空中图像，其中135张用于训练集，包含23,543辆车，38张用于测试集，包含5,545辆车。图像之间有60%的区域重叠，训练集和测试集之间没有重叠。[外部链接](https://research.utwente.nl/en/datasets/itcvd-dataset)
    |'
- en: '| UAVid [[280](https://arxiv.org/html/2501.02341v1#bib.bib280)] | 2020 | Image
    Video | 30 videos, 300 images, 8 semantic category annotations. [\faExternalLink](https://uavid.nl/)
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| UAVid [[280](https://arxiv.org/html/2501.02341v1#bib.bib280)] | 2020 | 图像视频
    | 30个视频，300张图像，8个语义类别标注。[外部链接](https://uavid.nl/) |'
- en: '| AU-AIR [[281](https://arxiv.org/html/2501.02341v1#bib.bib281)] | 2020 | Video
    GPS'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '| AU-AIR [[281](https://arxiv.org/html/2501.02341v1#bib.bib281)] | 2020 | 视频
    GPS'
- en: Altitude
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 高度
- en: IMU
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: IMU
- en: Speed | 32,823 frames of video, 1920x1080 resolution, 30 FPS, divided into 30,000
    training validation samples and 2,823 test samples. The total duration of the
    8 videos is about 2 hours, with a total of 132,034 instances, distributed in 8
    categories. [\faExternalLink](https://bozcani.github.io/auairdataset) |
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 速度 | 32,823帧视频，1920x1080分辨率，30帧每秒，分为30,000个训练验证样本和2,823个测试样本。8个视频的总时长约为2小时，共有132,034个实例，分布在8个类别中。[外部链接](https://bozcani.github.io/auairdataset)
    |
- en: '| iSAID [[278](https://arxiv.org/html/2501.02341v1#bib.bib278)] | 2020 | Image
    | Total images: 2,806\. Total number of instances: 655,451\. Test set: 935 images
    (not publicly labeled, used to evaluate the server). [\faExternalLink](https://captain-whu.github.io/iSAID/)
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| iSAID [[278](https://arxiv.org/html/2501.02341v1#bib.bib278)] | 2020 | 图像
    | 总计图像：2,806张。实例总数：655,451个。测试集：935张图像（未公开标注，用于评估服务器）。[外部链接](https://captain-whu.github.io/iSAID/)
    |'
- en: '| CARPK [[277](https://arxiv.org/html/2501.02341v1#bib.bib277)] | 2018 | Image
    | 1448 images, approx. 89,777 vehicles, providing box annotations. [\faExternalLink](https://lafi.github.io/LPN/)
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| CARPK [[277](https://arxiv.org/html/2501.02341v1#bib.bib277)] | 2018 | 图像
    | 1448张图像，约89,777辆车，提供框选标注。[外部链接](https://lafi.github.io/LPN/) |'
- en: '| highD [[282](https://arxiv.org/html/2501.02341v1#bib.bib282)] | 2018 | Video
    Trajectory | 16.5 hours, 110,000 vehicles, 5,600 lane changes, 45,000 km, totaling
    approximately 447 hours of vehicle travel data; 4 predefined driving behavior
    labels. [\faExternalLink](https://levelxdata.com/highd-dataset/) |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| highD [[282](https://arxiv.org/html/2501.02341v1#bib.bib282)] | 2018 | 视频轨迹
    | 16.5小时，110,000辆车，5,600次变道，45,000公里，总计约447小时的车辆行驶数据；4个预定义的驾驶行为标签。[外部链接](https://levelxdata.com/highd-dataset/)
    |'
- en: '| UAVDT [[283](https://arxiv.org/html/2501.02341v1#bib.bib283)] | 2018 | Video
    Weather'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '| UAVDT [[283](https://arxiv.org/html/2501.02341v1#bib.bib283)] | 2018 | 视频天气'
- en: Altitude
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 高度
- en: Camera angle | 100 videos, about 80,000 frames, 30 frames per second, containing
    841,500 target boxes, covering 2,700 targets. [\faExternalLink](https://sites.google.com/view/grli-uavdt/)
    |
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 摄像机角度 | 100个视频，约80,000帧，30帧每秒，包含841,500个目标框，覆盖2,700个目标。[外部链接](https://sites.google.com/view/grli-uavdt/)
    |
- en: '| CADP [[276](https://arxiv.org/html/2501.02341v1#bib.bib276)] | 2016 | Video
    | A total of 5.24 hours, 1,416 traffic accident clips, 205 full-time and space
    annotation videos. [\faExternalLink](https://ankitshah009.github.io/accident_forecasting_traffic_camera)
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| CADP [[276](https://arxiv.org/html/2501.02341v1#bib.bib276)] | 2016 | 视频
    | 总计5.24小时，1,416个交通事故片段，205个全时空注释视频。[ \faExternalLink](https://ankitshah009.github.io/accident_forecasting_traffic_camera)
    |'
- en: '| VEDAI [[284](https://arxiv.org/html/2501.02341v1#bib.bib284)] | 2016 | Image
    | 1,210 images (1024 × 1024 and 512 × 512 pixels), 9 types of vehicles, containing
    about 6,650 targets in total. [\faExternalLink](https://downloads.greyc.fr/vedai)
    |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| VEDAI [[284](https://arxiv.org/html/2501.02341v1#bib.bib284)] | 2016 | 图像
    | 1,210张图像（1024 × 1024和512 × 512像素），9种类型的车辆，总计约6,650个目标。[ \faExternalLink](https://downloads.greyc.fr/vedai)
    |'
- en: 4.2.2 Remote Sensing
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 遥感
- en: 'In the field of remote sensing, several innovative datasets, as shown in Table
    [8](https://arxiv.org/html/2501.02341v1#S4.T8 "Table 8 ‣ 4.2.2 Remote Sensing
    ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet
    LLMs: Overviews and Perspectives Toward Agentic Low-Altitude Mobility"), provide
    substantial support for tasks such as object detection, classification, localization,
    and image analysis [[14](https://arxiv.org/html/2501.02341v1#bib.bib14)]. The
    xView dataset [[285](https://arxiv.org/html/2501.02341v1#bib.bib285)] is a large-scale
    satellite image dataset that containing over one million annotations, across multiple
    object categories, making it particularly suitable for object detection and image
    segmentation tasks, especially in complex backgrounds and challenging environments.
    The DOTA dataset [[286](https://arxiv.org/html/2501.02341v1#bib.bib286)] focuses
    on object detection in high-resolution aerial images, covering multiple object
    categories such as aircraft, vehicles, and buildings, and is suitable for multi-object
    detection and classification tasks in complex scenes. The RSICD dataset [[287](https://arxiv.org/html/2501.02341v1#bib.bib287)]
    is primarily used for scene classification tasks in remote sensing images and
    supports language description generation, providing a standardized benchmark that
    promotes research in image understanding and automated annotation techniques.
    RemoteCLIP [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] introduces
    a remote sensing visual-language model that enhances semantic analysis and image
    retrieval of remote sensing images through self-supervised learning and masked
    image modeling, advancing the application of drones in remote sensing data analysis.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在遥感领域，如表[8](https://arxiv.org/html/2501.02341v1#S4.T8 "表 8 ‣ 4.2.2 遥感 ‣ 4.2
    领域特定数据集 ‣ 4 无人机的数据集与平台 ‣ 无人机与大型语言模型的结合：面向低空智能移动的概述与展望")所示，多个创新数据集为目标检测、分类、定位和图像分析等任务提供了重要支持[[14](https://arxiv.org/html/2501.02341v1#bib.bib14)]。xView数据集[[285](https://arxiv.org/html/2501.02341v1#bib.bib285)]是一个大规模的卫星图像数据集，包含超过一百万个标注，涵盖多个目标类别，特别适用于目标检测和图像分割任务，尤其在复杂背景和具有挑战性的环境中表现突出。DOTA数据集[[286](https://arxiv.org/html/2501.02341v1#bib.bib286)]专注于高分辨率航空图像中的目标检测，涵盖如飞机、车辆和建筑等多个目标类别，适用于复杂场景中的多目标检测和分类任务。RSICD数据集[[287](https://arxiv.org/html/2501.02341v1#bib.bib287)]主要用于遥感图像中的场景分类任务，并支持语言描述生成，提供了一个标准化的基准，推动了图像理解和自动标注技术的研究。RemoteCLIP
    [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)]引入了一个遥感视觉语言模型，通过自监督学习和遮蔽图像建模，增强了遥感图像的语义分析和图像检索，推动了无人机在遥感数据分析中的应用。
- en: 'Table 8: UAV-oriented Datasets on Remote Sensing'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：面向无人机的遥感数据集
- en: '| Name | Year | Types | Amount |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| RET-3 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | Image
    Text | Approximately 13,000 samples. Including RSICD, RSITMD and UCM. [\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| RET-3 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | 图像文本
    | 大约13,000个样本。包括RSICD、RSITMD和UCM。[ \faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
- en: '| DET-10 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | Image
    | In the object detection dataset, the number of objects per image ranges from
    1 to 70, totaling about 80,000 samples. [\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| DET-10 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | 图像
    | 在目标检测数据集中，每张图像中的目标数量从1到70不等，总计约80,000个样本。[ \faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
- en: '| SEG-4 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | Image
    | The segmented data set covers different regions and resolutions, totaling about
    72,000 samples. [\faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| SEG-4 [[288](https://arxiv.org/html/2501.02341v1#bib.bib288)] | 2024 | 图像
    | 分割数据集涵盖不同区域和分辨率，总计约72,000个样本。[ \faExternalLink](https://github.com/ChenDelong1999/RemoteCLIP?utm_source=chatgpt.com)
    |'
- en: '| DIOR [[289](https://arxiv.org/html/2501.02341v1#bib.bib289)] | 2020 | Image
    | 23,463 images, containing 192,472 target instances, covering 20 categories,
    including aircraft, vehicles, ships, bridges, etc., each category contains about
    1,200 instances. [\faExternalLink](http://www.escience.cn/people/gongcheng/DIOR.html)
    |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| DIOR [[289](https://arxiv.org/html/2501.02341v1#bib.bib289)] | 2020 | 图像
    | 23,463张图像，包含192,472个目标实例，涵盖20个类别，包括飞机、车辆、船只、桥梁等，每个类别包含约1,200个实例。[ \faExternalLink](http://www.escience.cn/people/gongcheng/DIOR.html)
    |'
- en: '| TGRS-HRRSD [[290](https://arxiv.org/html/2501.02341v1#bib.bib290)] | 2019
    | Image | Total images: 21,761\. 13 categories, including aircraft, vehicles,
    bridges, etc. The total number of targets is approximately 53,000 targets. [\faExternalLink](https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset)
    |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| TGRS-HRRSD [[290](https://arxiv.org/html/2501.02341v1#bib.bib290)] | 2019
    | 图像 | 总图像数：21,761。13个类别，包括飞机、车辆、桥梁等，目标总数约为53,000个。[ \faExternalLink](https://github.com/CrazyStoneonRoad/TGRS-HRRSD-Dataset)
    |'
- en: '| xView [[285](https://arxiv.org/html/2501.02341v1#bib.bib285)] | 2018 | Image
    | There are more than 1 million goals and 60 categories, including vehicles, buildings,
    facilities, boats and so on, which are divided into seven parent categories and
    several sub-categories. [\faExternalLink](https://xviewdataset.org/) |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| xView [[285](https://arxiv.org/html/2501.02341v1#bib.bib285)] | 2018 | 图像
    | 包含超过100万个目标和60个类别，包括车辆、建筑物、设施、船只等，分为七个主类别和若干子类别。[ \faExternalLink](https://xviewdataset.org/)
    |'
- en: '| DOTA [[286](https://arxiv.org/html/2501.02341v1#bib.bib286)] | 2018 | Image
    | 2806 images, 188, 282 targets, 15 categories. [\faExternalLink](https://captain-whu.github.io/DOTA/)
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| DOTA [[286](https://arxiv.org/html/2501.02341v1#bib.bib286)] | 2018 | 图像
    | 2,806张图像，188,282个目标，15个类别。[ \faExternalLink](https://captain-whu.github.io/DOTA/)
    |'
- en: '| RSICD [[287](https://arxiv.org/html/2501.02341v1#bib.bib287)] | 2018 | Image
    Text | 10,921 images, 54,605 descriptive sentences. [\faExternalLink](https://github.com/201528014227051/RSICD_optimal)
    |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| RSICD [[287](https://arxiv.org/html/2501.02341v1#bib.bib287)] | 2018 | 图像文本
    | 10,921张图像，54,605个描述性句子。[ \faExternalLink](https://github.com/201528014227051/RSICD_optimal)
    |'
- en: '| HRSC2016 [[291](https://arxiv.org/html/2501.02341v1#bib.bib291)] | 2017 |
    Image | 3,433 instances, totaling 1,061 images, including 70 pure ocean images
    and 991 images containing mixed land-sea areas. 2,876 marked vessel targets. 610
    unlabeled images. [\faExternalLink](http://www.escience.cn/people/liuzikun/DataSet.html)
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| HRSC2016 [[291](https://arxiv.org/html/2501.02341v1#bib.bib291)] | 2017 |
    图像 | 3,433个实例，总计1,061张图像，其中包括70张纯海洋图像和991张包含混合陆海区域的图像。2,876个标记的船只目标。610张未标记图像。[
    \faExternalLink](http://www.escience.cn/people/liuzikun/DataSet.html) |'
- en: '| RSOD [[292](https://arxiv.org/html/2501.02341v1#bib.bib292)] | 2017 | Image
    | Contains 4 types of targets (tank, aircraft, overpass, playground) with 12,000
    positive samples and 48,000 negative samples. [\faExternalLink](https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-)
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| RSOD [[292](https://arxiv.org/html/2501.02341v1#bib.bib292)] | 2017 | 图像
    | 包含4种类型的目标（坦克、飞机、立交桥、游乐场），共12,000个正样本和48,000个负样本。[ \faExternalLink](https://github.com/RSIA-LIESMARS-WHU/RSOD-Dataset-)
    |'
- en: '| NWPU-RESISC45 [[293](https://arxiv.org/html/2501.02341v1#bib.bib293)] | 2017
    | Image | A total of 31,500 images, covering 45 scene categories, 700 images per
    category, resolution 256 × 256 pixels, spatial resolution from 0.2m to 30m. [\faExternalLink](http://pan.baidu.com/s/1mifR6tU)
    |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| NWPU-RESISC45 [[293](https://arxiv.org/html/2501.02341v1#bib.bib293)] | 2017
    | 图像 | 总计31,500张图像，涵盖45个场景类别，每个类别700张图像，分辨率256 × 256像素，空间分辨率从0.2米到30米不等。[ \faExternalLink](http://pan.baidu.com/s/1mifR6tU)
    |'
- en: '| NWPU VHR-10 [[294](https://arxiv.org/html/2501.02341v1#bib.bib294)] | 2014
    | Image | 800 high-resolution images, of which 650 contain targets and 150 are
    background images, covering 10 categories (such as aircraft, ships, bridges, etc.),
    totaling more than 3,000 targets. [\faExternalLink](https://github.com/Gaoshuaikun/NWPU-VHR-10)
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| NWPU VHR-10 [[294](https://arxiv.org/html/2501.02341v1#bib.bib294)] | 2014
    | 图像 | 800张高分辨率图像，其中650张包含目标，150张为背景图像，涵盖10个类别（如飞机、船只、桥梁等），目标总数超过3,000个。[ \faExternalLink](https://github.com/Gaoshuaikun/NWPU-VHR-10)
    |'
- en: 4.2.3 Agriculture
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 农业
- en: 'The agricultural section summarizes only publicly available datasets from the
    past two years, as shown in Table [9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility"), as several reviews have covered datasets before 2023\. Agricultural
    datasets are commonly used for object detection to identify weeds, invasive plants,
    or plant diseases and pests, while semantic segmentation is often used for field
    division. The Avo-AirDB dataset [[295](https://arxiv.org/html/2501.02341v1#bib.bib295)]
    is specifically designed for agricultural image segmentation and classification,
    providing high-resolution images of avocado crops to support plant identification
    and health monitoring in precision agriculture. The CoFly-WeedDB dataset [[296](https://arxiv.org/html/2501.02341v1#bib.bib296)]
    consists of 201 aerial images, capturing three types of weeds that interfere with
    cotton crops, along with corresponding annotated images. The WEED-2C Dataset [[297](https://arxiv.org/html/2501.02341v1#bib.bib297)]
    focuses on training UAV images for species detection of weeds in soybean fields,
    automatically identifying two weed species.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '农业部分仅总结了过去两年内公开可用的数据集，如表[9](https://arxiv.org/html/2501.02341v1#S4.T9 "Table
    9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms for
    UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility")所示，因为已有多篇综述涵盖了2023年之前的数据集。农业数据集通常用于目标检测，用于识别杂草、入侵植物或植物疾病和害虫，而语义分割通常用于田地划分。Avo-AirDB数据集[[295](https://arxiv.org/html/2501.02341v1#bib.bib295)]专门用于农业图像分割和分类，提供了高分辨率的鳄梨作物图像，支持精准农业中的植物识别和健康监测。CoFly-WeedDB数据集[[296](https://arxiv.org/html/2501.02341v1#bib.bib296)]包含201张航拍图像，捕捉了三种干扰棉花作物的杂草，以及相应的标注图像。WEED-2C数据集[[297](https://arxiv.org/html/2501.02341v1#bib.bib297)]专注于训练无人机图像，用于大豆田中的杂草物种检测，自动识别两种杂草物种。'
- en: 4.2.4 Industry
  id: totrans-311
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 行业
- en: 'Using drone imagery for industrial inspections, particularly in infrastructure
    maintenance, has become increasingly important. Table [9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility") lists several typical datasets. The UAPD dataset [[298](https://arxiv.org/html/2501.02341v1#bib.bib298)]
    focuses on detecting asphalt pavement cracks through drone imagery and the YOLO
    architecture, aiming to enhance road and highway maintenance efficiency via automated
    crack detection. The InsPLAD dataset [[299](https://arxiv.org/html/2501.02341v1#bib.bib299)]
    is specifically designed for power line asset detection, containing drone images
    focused on infrastructure such as power lines, towers, and insulators. By providing
    images under diverse environmental conditions, this dataset supports the development
    of automated inspection systems to identify damage or aging in power equipment,
    thereby improving the efficiency and accuracy of power line inspections.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '使用无人机影像进行工业检查，特别是在基础设施维护方面，已变得越来越重要。表[9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility")列出了几个典型数据集。UAPD数据集[[298](https://arxiv.org/html/2501.02341v1#bib.bib298)]专注于通过无人机影像和YOLO架构检测沥青路面裂缝，旨在通过自动化裂缝检测提升道路和高速公路维护效率。InsPLAD数据集[[299](https://arxiv.org/html/2501.02341v1#bib.bib299)]专门用于电力线资产检测，包含了关注基础设施（如电力线、塔架和绝缘子）的无人机图像。通过提供多种环境条件下的图像，该数据集支持开发自动化检查系统，以识别电力设备的损坏或老化，从而提高电力线检查的效率和准确性。'
- en: 4.2.5 Emergency Response
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 应急响应
- en: 'These datasets as shown in Table [9](https://arxiv.org/html/2501.02341v1#S4.T9
    "Table 9 ‣ 4.2.7 Wildlife ‣ 4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms
    for UAVs ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic Low-Altitude
    Mobility") are typically used to enhance the visual understanding capabilities
    of drones in disaster rescue scenarios, particularly in post-disaster scene analysis,
    disaster area monitoring, environmental assessment, and rescue operations. They
    facilitate rapid image recognition, object detection, and scene understanding
    tasks. The Aerial SAR dataset [[300](https://arxiv.org/html/2501.02341v1#bib.bib300)]
    explores drone applications in natural disaster monitoring and search-and-rescue
    operations, highlighting the potential for rapid deployment and autonomous management
    of drones in disaster zones. The AFID dataset [[301](https://arxiv.org/html/2501.02341v1#bib.bib301)]
    provides aerial imagery for water channel surveillance and disaster early warning,
    supporting the training of deep semantic segmentation models. The FloodNet dataset
    [[302](https://arxiv.org/html/2501.02341v1#bib.bib302)] offers high-resolution
    aerial imagery for post-disaster scene understanding, primarily intended to assist
    in post-disaster assessments and emergency rescue operations. By utilizing these
    datasets, researchers can significantly improve image analysis capabilities in
    disaster response and advance the practical application of drone technology in
    disaster rescue efforts.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[9](https://arxiv.org/html/2501.02341v1#S4.T9 "Table 9 ‣ 4.2.7 Wildlife ‣
    4.2 Domain-specific Datasets ‣ 4 Datasets and Platforms for UAVs ‣ UAVs Meet LLMs:
    Overviews and Perspectives Toward Agentic Low-Altitude Mobility")所示，这些数据集通常用于增强无人机在灾难救援场景中的视觉理解能力，特别是在灾后场景分析、灾区监测、环境评估和救援行动中。它们有助于快速进行图像识别、物体检测和场景理解任务。Aerial
    SAR 数据集 [[300](https://arxiv.org/html/2501.02341v1#bib.bib300)] 探讨了无人机在自然灾害监测和搜救行动中的应用，突出展示了在灾区快速部署和无人机自主管理的潜力。AFID
    数据集 [[301](https://arxiv.org/html/2501.02341v1#bib.bib301)] 提供了水道监测和灾害预警的空中影像，支持深度语义分割模型的训练。FloodNet
    数据集 [[302](https://arxiv.org/html/2501.02341v1#bib.bib302)] 提供了高分辨率的空中影像，用于灾后场景理解，主要用于帮助灾后评估和紧急救援行动。通过利用这些数据集，研究人员可以显著提高灾害响应中的图像分析能力，并推动无人机技术在灾难救援工作中的实际应用。'
- en: 4.2.6 Military
  id: totrans-315
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6 军事
- en: The MOCO dataset [[303](https://arxiv.org/html/2501.02341v1#bib.bib303)] is
    designed for the Military Image Captioning (MilitIC) task, which focuses on generating
    textual intelligence from images captured by low-altitude UAVs (Unmanned Aerial
    Vehicles) and UGVs (Unmanned Ground Vehicles) in military contexts. The dataset
    includes a training set with 7,192 images and 35,960 captions, as well as a test
    set with 257 images and 1,285 captions. Military image captioning, as a vision-language
    learning task, aims to automatically generate descriptive captions for military
    images, thereby enhancing situational awareness and supporting decision-making.
    By integrating image data with textual descriptions, this approach improves intelligence
    capabilities and operational efficiency in the military domain.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: MOCO 数据集 [[303](https://arxiv.org/html/2501.02341v1#bib.bib303)] 是专为军事图像描述任务
    (MilitIC) 设计的，该任务聚焦于从低空无人机 (UAVs) 和无人地面车辆 (UGVs) 捕获的图像中生成文本情报，主要应用于军事场景。该数据集包含一个训练集，包含7,192张图像和35,960条描述，以及一个测试集，包含257张图像和1,285条描述。军事图像描述作为一种视觉-语言学习任务，旨在自动生成军事图像的描述性文字，从而增强态势感知并支持决策制定。通过将图像数据与文本描述相结合，这一方法提升了军事领域的情报能力和作战效率。
- en: 4.2.7 Wildlife
  id: totrans-317
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.7 野生动物
- en: The WAID [[304](https://arxiv.org/html/2501.02341v1#bib.bib304)] is a large-scale,
    multi-class, high-quality dataset specifically designed to support the use of
    drones in wildlife monitoring. The dataset includes 14,375 drone images captured
    under various environmental conditions, covering six species of wildlife and multiple
    types of habitats.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: WAID 数据集 [[304](https://arxiv.org/html/2501.02341v1#bib.bib304)] 是一个大规模、多类别、高质量的数据集，专门设计用于支持无人机在野生动物监测中的应用。该数据集包含14,375张在各种环境条件下捕获的无人机图像，涵盖六种野生动物物种和多种栖息地类型。
- en: 'Table 9: UAV-oriented Datasets on Agriculture & Industry & Emergency Response
    & Military & Wildlife'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 9：面向无人机的数据集：农业、工业、应急响应、军事与野生动物
- en: '| Name | Year | Types | Amount |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 年份 | 类型 | 数量 |'
- en: '| Agriculture |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 农业 |'
- en: '| WEED-2C [[297](https://arxiv.org/html/2501.02341v1#bib.bib297)] | 2024 |
    Image | Contains 4,129 labeled samples covering 2 weed species. [\faExternalLink](https://github.com/EvertonTetila/WEED2C-Dataset/?tab=readme-ov-file)
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| WEED-2C [[297](https://arxiv.org/html/2501.02341v1#bib.bib297)] | 2024 |
    图像 | 包含4,129个标注样本，涵盖2种杂草物种。 [\faExternalLink](https://github.com/EvertonTetila/WEED2C-Dataset/?tab=readme-ov-file)
    |'
- en: '| CoFly-WeedDB [[296](https://arxiv.org/html/2501.02341v1#bib.bib296)] | 2023
    | Image Health data | Consisting of 201 aerial images, different weed types of
    3 disturbed row crops (cotton) and their corresponding annotated images were captured.
    [\faExternalLink](https://github.com/CoFly-Project/CoFly-WeedDB/blob/main/README.md?utm_source=chatgpt.com)
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| CoFly-WeedDB [[296](https://arxiv.org/html/2501.02341v1#bib.bib296)] | 2023
    | 图像健康数据 | 由201张航拍图像组成，捕捉了3种干扰行作物（棉花）的不同杂草类型及其相应的标注图像。 [\faExternalLink](https://github.com/CoFly-Project/CoFly-WeedDB/blob/main/README.md?utm_source=chatgpt.com)
    |'
- en: '| Avo-AirDB [[295](https://arxiv.org/html/2501.02341v1#bib.bib295)] | 2022
    | Image | 984 high-resolution RGB images (5472 × 3648 pixels), 93 of which have
    detailed polygonal annotations, divided into 3 to 4 categories (small, medium,
    large, and background). [\faExternalLink](https://github.com/LCSkhalid/Avo-AirDB)
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Avo-AirDB [[295](https://arxiv.org/html/2501.02341v1#bib.bib295)] | 2022
    | 图像 | 984张高分辨率RGB图像（5472 × 3648像素），其中93张具有详细的多边形标注，分为3到4个类别（小型、中型、大型和背景）。 [\faExternalLink](https://github.com/LCSkhalid/Avo-AirDB)
    |'
- en: '| Industry |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 工业 |'
- en: '| UAPD [[298](https://arxiv.org/html/2501.02341v1#bib.bib298)] | 2021 | Image
    | There are 2,401 crack images in the original data and 4,479 crack images after
    data enhancement. [\faExternalLink](https://github.com/tantantetetao/UAPD-Pavement-Distress-Dataset)
    |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| UAPD [[298](https://arxiv.org/html/2501.02341v1#bib.bib298)] | 2021 | 图像
    | 原始数据中有2,401张裂缝图像，数据增强后有4,479张裂缝图像。 [\faExternalLink](https://github.com/tantantetetao/UAPD-Pavement-Distress-Dataset)
    |'
- en: '| InsPLAD [[299](https://arxiv.org/html/2501.02341v1#bib.bib299)] | 2023 |
    Image | 10,607 UAV images containing 17 classes of power assets with a total of
    28,933 labeled instances, and defect labels for 5 assets with a total of 402 defect
    samples classified into 6 defect types. [\faExternalLink](https://github.com/andreluizbvs/InsPLAD/)
    |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| InsPLAD [[299](https://arxiv.org/html/2501.02341v1#bib.bib299)] | 2023 |
    图像 | 10,607张无人机图像，包含17类电力资产，总共有28,933个标注实例，以及5种资产的缺陷标签，总计402个缺陷样本，分为6种缺陷类型。 [\faExternalLink](https://github.com/andreluizbvs/InsPLAD/)
    |'
- en: '| Emergency Response |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 应急响应 |'
- en: '| FloodNet [[302](https://arxiv.org/html/2501.02341v1#bib.bib302)] | 2021 |
    Image Text | The whole dataset has 2,343 images, divided into training ( 60%),
    validation ( 20%), and test ( 20%) sets. The semantic segmentation labels include:
    Background, Building Flooded, Building Non-Flooded, Road Flooded, Road Non-Flooded,
    Water, Tree, Vehicle, Pool, Grass. [\faExternalLink](https://github.com/BinaLab/FloodNet-Supervised_v1.0)
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| FloodNet [[302](https://arxiv.org/html/2501.02341v1#bib.bib302)] | 2021 |
    图像文本 | 整个数据集包含2,343张图像，分为训练集（60%）、验证集（20%）和测试集（20%）。语义分割标签包括：背景、建筑物被淹、建筑物未被淹、道路被淹、道路未被淹、水、树、车辆、池塘、草地。
    [\faExternalLink](https://github.com/BinaLab/FloodNet-Supervised_v1.0) |'
- en: '| AFID [[301](https://arxiv.org/html/2501.02341v1#bib.bib301)] | 2023 | Image
    | A total of 816 images with resolutions of 2720 × 1536 and 2560 × 1440\. Contains
    8 semantic segmentation categories. [\faExternalLink](https://purr.purdue.edu/publications/4105/1)
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| AFID [[301](https://arxiv.org/html/2501.02341v1#bib.bib301)] | 2023 | 图像
    | 总共有816张分辨率为2720 × 1536和2560 × 1440的图像。包含8个语义分割类别。 [\faExternalLink](https://purr.purdue.edu/publications/4105/1)
    |'
- en: '| Aerial SAR [[300](https://arxiv.org/html/2501.02341v1#bib.bib300)] | 2020
    | Image | 2,000 images with 30,000 action instances covering multiple human behaviors.
    [\faExternalLink](https://www.leadingindia.ai/data-set) |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| Aerial SAR [[300](https://arxiv.org/html/2501.02341v1#bib.bib300)] | 2020
    | 图像 | 2,000张图像，包含30,000个动作实例，涵盖多种人类行为。 [\faExternalLink](https://www.leadingindia.ai/data-set)
    |'
- en: '| Military |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 军事 |'
- en: '| MOCO [[303](https://arxiv.org/html/2501.02341v1#bib.bib303)] | 2024 | Image
    Text | 7,449 images, 37,245 captions. [\faExternalLink](https://github.com/Panlizhi/MOCO)
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| MOCO [[303](https://arxiv.org/html/2501.02341v1#bib.bib303)] | 2024 | 图像文本
    | 7,449张图像，37,245条说明。 [\faExternalLink](https://github.com/Panlizhi/MOCO) |'
- en: '| Wildlife |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 野生动物 |'
- en: '| WAID [[304](https://arxiv.org/html/2501.02341v1#bib.bib304)] | 2023 | Image
    | 14,375 UAV images covering 6 species of wildlife and multiple environment types.
    [\faExternalLink](https://github.com/xiaohuicui/WAID) |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| WAID [[304](https://arxiv.org/html/2501.02341v1#bib.bib304)] | 2023 | 图像
    | 14,375张无人机图像，涵盖6种野生动物和多种环境类型。 [\faExternalLink](https://github.com/xiaohuicui/WAID)
    |'
- en: 4.3 3D Simulation Platforms
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 3D模拟平台
- en: The 3D simulation platform plays a crucial role in the development and application
    of drones by providing safe, controllable, and diverse testing scenarios for intelligent
    drone training within highly simulated virtual environments. These environments
    encompass complex conditions such as varying weather, lighting, wind speed, terrain,
    and obstacles. Such platforms are capable of generating large-scale, accurately
    labeled multimodal datasets for training and validation. Additionally, the simulation
    platform supports the modeling of multi-machine collaborative tasks, assessing
    drones’ collaborative capabilities, communication, and collision avoidance strategies
    within shared spaces. This effectively reduces risks and costs associated with
    real-world testing. Hardware-in-the-loop (HIL) simulation further integrates virtual
    testing with real hardware, helping identify potential issues and verify system
    reliability. In summary, 3D simulation platforms are pivotal in intelligent training,
    dataset generation, collaborative task execution, and hardware verification, significantly
    accelerating the development and deployment of drone technology.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 3D 仿真平台在无人机的开发和应用中起着至关重要的作用，通过提供安全、可控和多样化的测试场景，支持在高度仿真的虚拟环境中进行智能无人机训练。这些环境涵盖了复杂的条件，如变化的天气、光照、风速、地形和障碍物。这类平台能够生成大规模、准确标注的多模态数据集，用于训练和验证。此外，仿真平台还支持多机协作任务的建模，评估无人机在共享空间中的协作能力、通信能力和避碰策略。这有效地减少了现实世界测试中的风险和成本。硬件在环（HIL）仿真进一步将虚拟测试与真实硬件结合，帮助识别潜在问题并验证系统的可靠性。总之，3D
    仿真平台在智能训练、数据集生成、协作任务执行和硬件验证中具有重要作用，显著加速了无人机技术的开发和部署。
- en: 4.3.1 AirSim
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 AirSim
- en: AirSim [[305](https://arxiv.org/html/2501.02341v1#bib.bib305)] is an open-source,
    cross-platform simulator developed by Microsoft, designed specifically for the
    research and development of drones, autonomous vehicles, and other autonomous
    systems. Built on the Unreal Engine, the platform offers highly realistic physical
    simulation environments and visual effects, allowing users to test and validate
    the performance of algorithms in virtual scenarios. AirSim supports the simulation
    of various devices and sensors, including cameras, LiDAR, IMUs, GPS, and more,
    while providing comprehensive control over the environment and vehicles through
    its powerful API. Developers can extend the platform using Python and C++, enabling
    integration of cutting-edge technologies from fields such as machine learning,
    computer vision, and robotics. In addition to simulating drones and ground vehicles,
    the platform can model complex dynamic scenarios, including weather changes, collision
    detection, and physical interactions, helping users accelerate prototype validation
    and algorithm optimization in a safe and controlled virtual environment.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: AirSim [[305](https://arxiv.org/html/2501.02341v1#bib.bib305)] 是一个由微软开发的开源跨平台模拟器，专门用于无人机、自动驾驶汽车和其他自动化系统的研究与开发。该平台基于虚幻引擎构建，提供高度逼真的物理仿真环境和视觉效果，使用户能够在虚拟场景中测试和验证算法的性能。AirSim
    支持多种设备和传感器的仿真，包括摄像头、激光雷达、惯性测量单元（IMU）、全球定位系统（GPS）等，同时通过其强大的 API 提供对环境和车辆的全面控制。开发者可以使用
    Python 和 C++ 扩展该平台，从而实现与机器学习、计算机视觉和机器人学等前沿技术的集成。除了模拟无人机和地面车辆外，该平台还能够模拟复杂的动态场景，包括天气变化、碰撞检测和物理交互，帮助用户在安全可控的虚拟环境中加速原型验证和算法优化。
- en: 4.3.2 Carla
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 Carla
- en: CARLA [[306](https://arxiv.org/html/2501.02341v1#bib.bib306)] is an open-source
    autonomous driving simulation platform built on Unreal Engine, widely used for
    the development, training, and validation of algorithms for intelligent systems.
    Its highly realistic simulation environment supports complex urban scenarios,
    including road networks, dynamic traffic, pedestrian behavior, and diverse weather
    and lighting conditions, providing a virtual testing ground for perception, localization,
    planning, and control algorithms. CARLA supports the simulation of various sensors
    such as cameras, LiDAR, radar, IMUs, and GPS, and allows users to access its Python
    or C++ APIs, as well as interfaces supporting ROS, enabling researchers to quickly
    develop and test algorithms for navigation, obstacle avoidance, path planning,
    and environmental perception. Additionally, CARLA offers data recording and playback
    functionalities, supports multi-agent tasks, and integrates reinforcement learning
    applications, providing a safe, efficient, and repeatable testing platform for
    algorithm development in scenarios such as low-altitude logistics, monitoring,
    and patrolling for UAVs.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: CARLA [[306](https://arxiv.org/html/2501.02341v1#bib.bib306)] 是一个开源的自动驾驶仿真平台，构建在虚幻引擎上，广泛用于智能系统的算法开发、训练和验证。其高度逼真的仿真环境支持复杂的城市场景，包括道路网络、动态交通、行人行为以及各种天气和光照条件，为感知、定位、规划和控制算法提供了虚拟测试平台。CARLA支持多种传感器的仿真，如摄像头、激光雷达、雷达、IMU和GPS，并允许用户访问其Python或C++
    API，以及支持ROS的接口，使研究人员能够快速开发和测试导航、避障、路径规划和环境感知算法。此外，CARLA还提供数据记录和回放功能，支持多智能体任务，并集成强化学习应用，为无人机在低空物流、监控和巡逻等场景中的算法开发提供了一个安全、高效和可重复的测试平台。
- en: 4.3.3 NVIDIA Isaac Sim
  id: totrans-342
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.3 NVIDIA Isaac Sim
- en: NVIDIA Isaac Sim [[307](https://arxiv.org/html/2501.02341v1#bib.bib307)] is
    a physics-based robotic simulation platform built on the NVIDIA Omniverse platform,
    providing a high-precision virtual environment for the development, testing, and
    validation of robots and autonomous systems. The platform leverages NVIDIA’s powerful
    GPU acceleration and physics engine technologies, including PhysX and RTX real-time
    rendering, to present highly realistic simulation scenes with accurate physical
    interactions, lighting effects, and multi-sensor data generation. Isaac Sim offers
    a wide range of tools and plugins, allowing integration with various robotic frameworks,
    and supports the full development process from perception and motion planning
    to control algorithms. In addition to its applications in traditional robotics,
    Isaac Sim can be extended to the UAV domain, supporting drone navigation, obstacle
    avoidance, target tracking, and multi-agent collaboration tasks through its flexible
    environmental configuration, sensor simulation (including cameras, LiDAR, IMUs,
    and GPS), and complex dynamics modeling. The platform combines simulation reinforcement
    learning capabilities, data collection features, and digital twin support for
    real-world scenarios, enabling accelerated algorithm development for UAVs in areas
    such as logistics, environmental monitoring, and disaster response, while providing
    researchers and developers with an efficient, safe, and scalable testing environment.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA Isaac Sim [[307](https://arxiv.org/html/2501.02341v1#bib.bib307)] 是一个基于物理的机器人仿真平台，构建在NVIDIA
    Omniverse平台上，提供一个高精度的虚拟环境，用于机器人和自主系统的开发、测试和验证。该平台利用NVIDIA强大的GPU加速和物理引擎技术，包括PhysX和RTX实时渲染，呈现出高度逼真的仿真场景，具有准确的物理交互、光照效果和多传感器数据生成。Isaac
    Sim提供了广泛的工具和插件，允许与各种机器人框架进行集成，并支持从感知、运动规划到控制算法的完整开发过程。除了在传统机器人领域的应用外，Isaac Sim还可以扩展到无人机领域，通过灵活的环境配置、传感器仿真（包括摄像头、激光雷达、IMU和GPS）以及复杂的动态建模，支持无人机导航、避障、目标跟踪和多智能体协作任务。该平台结合了仿真强化学习能力、数据采集功能和数字双胞胎支持真实世界场景，为无人机在物流、环境监测和灾难响应等领域的算法开发加速，同时为研究人员和开发者提供了一个高效、安全、可扩展的测试环境。
- en: 4.3.4 AerialVLN Simulator
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.4 AerialVLN Simulator
- en: The AerialVLN Simulator [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)]
    is a high-fidelity virtual simulation platform designed specifically for research
    on drone agents. It integrates Unreal Engine 4 and Microsoft AirSim technologies
    to realistically simulate typical 3D urban environments, including cities such
    as Shanghai, Shenzhen, campuses, and residential areas, with coverage ranging
    from 30 hectares to 3,700 hectares. The platform supports diverse environmental
    settings, including varying lighting conditions such as daytime, dusk, and nighttime,
    weather patterns such as clear skies, overcast, and light snow, as well as seasonal
    changes, allowing drone agents to train in environments that closely resemble
    real-world conditions. Equipped with built-in front, rear, left, right, and top
    multi-view cameras, the platform can generate high-resolution data such as RGB
    images, Depth images, and target segmentation maps, providing rich visual input
    for scene understanding and spatial modeling. Additionally, the AerialVLN Simulator
    supports dynamic flight operations for drones, offering precise control over their
    3D position, orientation, and speed, while allowing the execution of complex maneuvers
    such as turns, climbs, and obstacle avoidance, ensuring smooth and flexible flight
    actions. Based on the ”Real-to-Sim-to-Real” design philosophy, the platform significantly
    narrows the gap between virtual environments and real-world applications, making
    it especially suitable for research and optimization in core drone tasks such
    as scene perception, spatial reasoning, path planning, and motion decision-making.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: AerialVLN模拟器[[268](https://arxiv.org/html/2501.02341v1#bib.bib268)]是一个高保真虚拟仿真平台，专门为无人机代理的研究而设计。它结合了虚幻引擎4和微软AirSim技术，能够真实地模拟典型的3D城市环境，包括上海、深圳等城市、校园和住宅区，覆盖范围从30公顷到3,700公顷不等。该平台支持多样化的环境设置，包括不同的光照条件，如白天、黄昏和夜晚；不同的天气模式，如晴天、阴天和小雪；以及季节变化，使无人机代理能够在与现实世界环境相似的条件下进行训练。该平台配备了前、后、左、右和顶部的多视角相机，能够生成高分辨率数据，如RGB图像、深度图像和目标分割图，提供丰富的视觉输入，用于场景理解和空间建模。此外，AerialVLN模拟器支持无人机的动态飞行操作，精确控制其3D位置、姿态和速度，同时支持执行复杂的机动，如转弯、爬升和避障，确保飞行动作平稳灵活。基于“从真实到仿真再到真实”的设计理念，该平台显著缩小了虚拟环境与现实应用之间的差距，特别适用于无人机在核心任务领域的研究与优化，如场景感知、空间推理、路径规划和运动决策。
- en: 4.3.5 Embodied City
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.5 Embodied City
- en: 'Embodied City [[308](https://arxiv.org/html/2501.02341v1#bib.bib308)] is an
    advanced high-fidelity 3D urban simulation platform specifically designed for
    the evaluation and development of embodied intelligence. Its core feature is the
    realistic virtual environments built based on real-world urban areas, such as
    commercial districts in Beijing, including highly detailed building models, street
    networks, and dynamic simulations of pedestrian and vehicular traffic. The platform
    uses Unreal Engine as its technical foundation, combining historical data with
    simulation algorithms to provide continuous perception and interaction capabilities
    for various embodied agents, such as drones and ground vehicles. By integrating
    the AirSim interface, it supports multimodal input and output, including RGB images,
    Depth images, LiDAR, GPS, and IMU data, facilitating motion control and environmental
    exploration within simulations. The design encompasses five task areas: scene
    understanding, question answering, dialogue, visual language navigation, and task
    planning. Through an easy-to-use Python SDK and an online platform, users can
    conveniently remotely access and test a variety of agent behaviors, while supporting
    real-time operation of up to eight agents.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Embodied City [[308](https://arxiv.org/html/2501.02341v1#bib.bib308)]是一个先进的高保真3D城市仿真平台，专门用于评估和开发具身智能。其核心特点是基于真实城市区域构建的真实感虚拟环境，如北京的商业区，包含高度详细的建筑模型、街道网络以及行人和车辆交通的动态仿真。该平台以虚幻引擎为技术基础，结合历史数据和仿真算法，为各种具身代理提供持续的感知和交互能力，例如无人机和地面车辆。通过整合AirSim接口，平台支持多模态输入和输出，包括RGB图像、深度图像、LiDAR、GPS和IMU数据，促进在仿真环境中的运动控制和环境探索。设计涵盖了五个任务领域：场景理解、问答、对话、视觉语言导航和任务规划。通过易于使用的Python
    SDK和在线平台，用户可以方便地远程访问和测试各种代理行为，同时支持最多八个代理的实时操作。
- en: 5 Advances of FM-based UAV systems
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于FM的无人机系统的5个进展
- en: Integrating AI algorithms such as machine learning and deep learning into UAV
    systems has become a mainstream trend. However, applying traditional AI models
    in UAV tasks still faces numerous challenges. First, these models typically rely
    on task-specific datasets for training, resulting in insufficient generalization
    capability and poor robustness when significant discrepancies exist between the
    actual scenarios and the training distributions. Additionally, traditional AI
    models are often optimized for single tasks, making them less effective in addressing
    the complex requirements of multi-task collaboration. Furthermore, these models
    exhibit notable limitations in human-machine interaction and task collaboration
    [[309](https://arxiv.org/html/2501.02341v1#bib.bib309), [310](https://arxiv.org/html/2501.02341v1#bib.bib310),
    [311](https://arxiv.org/html/2501.02341v1#bib.bib311)].
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 将AI算法，如机器学习和深度学习，集成到UAV系统中已成为主流趋势。然而，传统AI模型在UAV任务中的应用仍面临诸多挑战。首先，这些模型通常依赖于特定任务的数据集进行训练，这导致当实际场景与训练分布之间存在较大差异时，模型的泛化能力不足，鲁棒性差。此外，传统AI模型通常针对单一任务进行优化，因此在解决多任务协作的复杂需求时效果不佳。更重要的是，这些模型在人与机器的交互和任务协作方面存在明显的局限性[[309](https://arxiv.org/html/2501.02341v1#bib.bib309),
    [310](https://arxiv.org/html/2501.02341v1#bib.bib310), [311](https://arxiv.org/html/2501.02341v1#bib.bib311)]。
- en: The introduction of LLMs, VFMs, and VLMs inject novel intelligent capabilities
    into UAV systems through natural language understanding, zero-shot adaptation,
    multimodal collaboration, and intuitive human-machine interaction.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs、VFMs和VLMs的引入通过自然语言理解、零-shot适应、多模态协作和直观的人机交互，为UAV系统注入了新的智能能力。
- en: '![Refer to caption](img/cdf51d6043fffe1e9bcbb1de420922e9.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cdf51d6043fffe1e9bcbb1de420922e9.png)'
- en: 'Figure 4: Typical works on FM-based UAV systems.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基于FM的UAV系统的典型工作。
- en: 'This section explores existing research on integrating LLMs, VFMs, and VLMs
    into UAV systems and analyzes the advantages these technologies bring to different
    tasks. Several typical works are illustrated in Figure [4](https://arxiv.org/html/2501.02341v1#S5.F4
    "Figure 4 ‣ 5 Advances of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and
    Perspectives Toward Agentic Low-Altitude Mobility"). Based on the technical types
    and task characteristics, UAV-related tasks are categorized into the following
    types:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '本节探讨了将LLMs、VFMs和VLMs集成到UAV系统中的现有研究，并分析了这些技术为不同任务带来的优势。图[4](https://arxiv.org/html/2501.02341v1#S5.F4
    "Figure 4 ‣ 5 Advances of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and
    Perspectives Toward Agentic Low-Altitude Mobility")中展示了几个典型的工作。根据技术类型和任务特征，UAV相关任务可以分为以下几类：'
- en: '1.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Visual Perception: These include object detection, semantic segmentation, depth
    estimation, visual caption, and VQA. Such tasks focus on environmental perception
    and semantic information extraction, serving as the foundation for high-level
    decision-making in UAV systems.'
  id: totrans-355
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉感知：这些任务包括物体检测、语义分割、深度估计、视觉描述和VQA。这类任务侧重于环境感知和语义信息提取，为UAV系统中的高级决策提供基础。
- en: '2.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Vision-Language Navigation (VLN): VLN represents a typical application of the
    deep integration of computer vision and natural language processing. Building
    on VLN tasks, more complex multimodal tasks, such as Vision-Language Tracking
    (VLT) and target search, have been developed. These tasks integrate multiple components,
    including perception, planning, decision-making, control, and human-machine interaction,
    forming the core framework for intelligent task execution in UAVs.'
  id: totrans-357
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视觉-语言导航（VLN）：VLN代表了计算机视觉与自然语言处理深度集成的典型应用。基于VLN任务，发展出了更复杂的多模态任务，如视觉-语言跟踪（VLT）和目标搜索。这些任务集成了多个组件，包括感知、规划、决策、控制和人机交互，构成了UAV智能任务执行的核心框架。
- en: '3.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Planning: This includes path optimization, task allocation, and adaptive task
    optimization in dynamic environments.'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规划：这包括路径优化、任务分配和在动态环境中的自适应任务优化。
- en: '4.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Flight Control: These involve low-level control tasks such as attitude stabilization,
    path tracking, and obstacle avoidance.'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 飞行控制：这些任务包括低级控制任务，如姿态稳定、路径跟踪和避障。
- en: '5.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Infrastructures: This focuses on providing comprehensive technical and data
    support for UAV systems, including the development of integrated frameworks and
    platforms, as well as the creation and processing of high-quality datasets. These
    efforts not only enhance the efficiency of UAV applications in multimodal tasks
    but also provide critical support for foundational research and technological
    innovation in the UAV domain.'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基础设施：这主要关注为无人机系统提供全面的技术和数据支持，包括集成框架和平台的开发，以及高质量数据集的创建和处理。这些努力不仅提高了无人机在多模态任务中的应用效率，还为无人机领域的基础研究和技术创新提供了关键支持。
- en: 'We provide a systematic comparison of relevant methods in Table [10](https://arxiv.org/html/2501.02341v1#S5.T10
    "Table 10 ‣ 5 Advances of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and
    Perspectives Toward Agentic Low-Altitude Mobility") to offer a high-level overview
    of this rapidly evolving field. It should be noted that the “Base Model” column
    in Table [10](https://arxiv.org/html/2501.02341v1#S5.T10 "Table 10 ‣ 5 Advances
    of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility") lacks specific model names (e.g., GPT, LLM) in some cases,
    as the original references did not specify the exact model versions. For certain
    models, reference citations are included because detailed descriptions of the
    models were not provided in Section [3](https://arxiv.org/html/2501.02341v1#S3
    "3 Preliminaries on Foundation models ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility"). Additionally, the notation “LLMs or VLMs”
    indicates that multiple types of base models were tested in the corresponding
    method.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[10](https://arxiv.org/html/2501.02341v1#S5.T10 "Table 10 ‣ 5 Advances
    of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and Perspectives Toward Agentic
    Low-Altitude Mobility")中提供了相关方法的系统比较，以提供这一快速发展的领域的高级概述。需要注意的是，表[10](https://arxiv.org/html/2501.02341v1#S5.T10
    "Table 10 ‣ 5 Advances of FM-based UAV systems ‣ UAVs Meet LLMs: Overviews and
    Perspectives Toward Agentic Low-Altitude Mobility")中的“基础模型”一栏在某些情况下缺少具体的模型名称（例如，GPT，LLM），因为原始参考文献未指定具体的模型版本。对于某些模型，由于没有在第[3](https://arxiv.org/html/2501.02341v1#S3
    "3 Preliminaries on Foundation models ‣ UAVs Meet LLMs: Overviews and Perspectives
    Toward Agentic Low-Altitude Mobility")节中提供模型的详细描述，因此包含了引用文献。此外，“LLMs或VLMs”这一标注表示在相应方法中测试了多种类型的基础模型。'
- en: 'Table 10: Advances of FM-based UAV Systems in Various Tasks'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：基于FM的无人机系统在各种任务中的进展
- en: '| Category | Subcategory | Method / Model Name | Type | Base Model |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 子类别 | 方法/模型名称 | 类型 | 基础模型 |'
- en: '| Visual Perception | Object Detection | Li *et al*.[[312](https://arxiv.org/html/2501.02341v1#bib.bib312)]
    | VFM | CLIP |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 视觉感知 | 目标检测 | Li *等人*[[312](https://arxiv.org/html/2501.02341v1#bib.bib312)]
    | VFM | CLIP |'
- en: '|  |  | Ma *et al*.[[313](https://arxiv.org/html/2501.02341v1#bib.bib313)]
    | VFM | Grounding DINO + CLIP |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Ma *等人*[[313](https://arxiv.org/html/2501.02341v1#bib.bib313)] | VFM
    | Grounding DINO + CLIP |'
- en: '|  |  | Limberg *et al*.[[314](https://arxiv.org/html/2501.02341v1#bib.bib314)]
    | VFM+VLM | YOLO-World + GPT-4V |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Limberg *等人*[[314](https://arxiv.org/html/2501.02341v1#bib.bib314)]
    | VFM+VLM | YOLO-World + GPT-4V |'
- en: '|  |  | Kim *et al*.[[315](https://arxiv.org/html/2501.02341v1#bib.bib315)]
    | VLM+VFM | LLaVA-1.5 + CLIP |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Kim *等人*[[315](https://arxiv.org/html/2501.02341v1#bib.bib315)] | VLM+VFM
    | LLaVA-1.5 + CLIP |'
- en: '|  |  | LGNet[[3](https://arxiv.org/html/2501.02341v1#bib.bib3)] | VFM | CLIP
    |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LGNet[[3](https://arxiv.org/html/2501.02341v1#bib.bib3)] | VFM | CLIP
    |'
- en: '|  |  | [[316](https://arxiv.org/html/2501.02341v1#bib.bib316)] | VLM+VFM |
    BLIP-2 + OvSeg[[317](https://arxiv.org/html/2501.02341v1#bib.bib317)] + ViLD[[318](https://arxiv.org/html/2501.02341v1#bib.bib318)]
    |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  |  | [[316](https://arxiv.org/html/2501.02341v1#bib.bib316)] | VLM+VFM |
    BLIP-2 + OvSeg[[317](https://arxiv.org/html/2501.02341v1#bib.bib317)] + ViLD[[318](https://arxiv.org/html/2501.02341v1#bib.bib318)]
    |'
- en: '|  | Segmentation | COMRP[[313](https://arxiv.org/html/2501.02341v1#bib.bib313)]
    | VFM | Grounding DINO + CLIP + SAM + DINOv2 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | 分割 | COMRP[[313](https://arxiv.org/html/2501.02341v1#bib.bib313)] | VFM
    | Grounding DINO + CLIP + SAM + DINOv2 |'
- en: '|  |  | CrossEarth[[319](https://arxiv.org/html/2501.02341v1#bib.bib319)] |
    VFM | DINOv2 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CrossEarth[[319](https://arxiv.org/html/2501.02341v1#bib.bib319)] |
    VFM | DINOv2 |'
- en: '|  | Depth Estimation | TanDepth[[320](https://arxiv.org/html/2501.02341v1#bib.bib320)]
    | VFM | Depth Anything |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '|  | 深度估计 | TanDepth[[320](https://arxiv.org/html/2501.02341v1#bib.bib320)]
    | VFM | Depth Anything |'
- en: '|  | Visual Caption/QA | DroneGPT[[4](https://arxiv.org/html/2501.02341v1#bib.bib4)]
    | VLM+LLM+VFM | VISPROG + GPT-3.5 + Grounding DINO |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | 视觉标题/问答 | DroneGPT[[4](https://arxiv.org/html/2501.02341v1#bib.bib4)]
    | VLM+LLM+VFM | VISPROG + GPT-3.5 + Grounding DINO |'
- en: '|  |  | de Zarzà *et al*  [[321](https://arxiv.org/html/2501.02341v1#bib.bib321)].
    | LLM | BLIP-2 + GPT-3.5 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  |  | de Zarzà *等人*[[321](https://arxiv.org/html/2501.02341v1#bib.bib321)]
    | LLM | BLIP-2 + GPT-3.5 |'
- en: '|  |  | AeroAgent[[322](https://arxiv.org/html/2501.02341v1#bib.bib322)] |
    VLM | GPT-4V |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AeroAgent[[322](https://arxiv.org/html/2501.02341v1#bib.bib322)] |
    VLM | GPT-4V |'
- en: '|  |  | RS-LLaVA[[323](https://arxiv.org/html/2501.02341v1#bib.bib323)] | VLM
    | LLaVA-1.5 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  |  | RS-LLaVA[[323](https://arxiv.org/html/2501.02341v1#bib.bib323)] | VLM
    | LLaVA-1.5 |'
- en: '|  |  | GeoRSCLIP[[324](https://arxiv.org/html/2501.02341v1#bib.bib324)] |
    VFM | CLIP |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GeoRSCLIP[[324](https://arxiv.org/html/2501.02341v1#bib.bib324)] |
    VFM | CLIP |'
- en: '|  |  | SkyEyeGPT[[325](https://arxiv.org/html/2501.02341v1#bib.bib325)] |
    VFM+LLM | EVA-CLIP + LLaMA2 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SkyEyeGPT[[325](https://arxiv.org/html/2501.02341v1#bib.bib325)] |
    VFM+LLM | EVA-CLIP + LLaMA2 |'
- en: '| VLN | Indoor | NaVid[[326](https://arxiv.org/html/2501.02341v1#bib.bib326)]
    | VFM+LLM | EVA-CLIP + Vicuna |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| VLN | 室内 | NaVid[[326](https://arxiv.org/html/2501.02341v1#bib.bib326)] |
    VFM+LLM | EVA-CLIP + Vicuna |'
- en: '|  |  | VLN-MP[[327](https://arxiv.org/html/2501.02341v1#bib.bib327)] | VFM
    | Grounding DINO / GLIP |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  |  | VLN-MP[[327](https://arxiv.org/html/2501.02341v1#bib.bib327)] | VFM
    | Grounding DINO / GLIP |'
- en: '|  | Outdoor | Gao *et al*.[[328](https://arxiv.org/html/2501.02341v1#bib.bib328)]
    | VFM+LLM | Grounding DINO + TAP + GPT-4o |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | 户外 | Gao *等*.[[328](https://arxiv.org/html/2501.02341v1#bib.bib328)] |
    VFM+LLM | Grounding DINO + TAP + GPT-4o |'
- en: '|  |  | MGP[[267](https://arxiv.org/html/2501.02341v1#bib.bib267)] | LLM+VFM
    | GPT-3.5 + Grounding DINO + MobileSAM |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  |  | MGP[[267](https://arxiv.org/html/2501.02341v1#bib.bib267)] | LLM+VFM
    | GPT-3.5 + Grounding DINO + MobileSAM |'
- en: '|  |  | UAV Navigation LLM[[329](https://arxiv.org/html/2501.02341v1#bib.bib329)]
    | LLM+VFM | Vicuna + EVA-CLIP |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  |  | UAV导航LLM[[329](https://arxiv.org/html/2501.02341v1#bib.bib329)] | LLM+VFM
    | Vicuna + EVA-CLIP |'
- en: '|  |  | GOMAA-Geo[[2](https://arxiv.org/html/2501.02341v1#bib.bib2)] | LLM+VFM
    | LLMs + CLIP |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GOMAA-Geo[[2](https://arxiv.org/html/2501.02341v1#bib.bib2)] | LLM+VFM
    | LLMs + CLIP |'
- en: '|  |  | NavAgent[[1](https://arxiv.org/html/2501.02341v1#bib.bib1)] | LLM+VFM+VLM
    | GLIP + BLIP-2 + GPT-4 + LLaMA2 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  |  | NavAgent[[1](https://arxiv.org/html/2501.02341v1#bib.bib1)] | LLM+VFM+VLM
    | GLIP + BLIP-2 + GPT-4 + LLaMA2 |'
- en: '|  |  | ASMA[[330](https://arxiv.org/html/2501.02341v1#bib.bib330)] | LLM+VFM
    | GPT-2 + CLIP |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  |  | ASMA[[330](https://arxiv.org/html/2501.02341v1#bib.bib330)] | LLM+VFM
    | GPT-2 + CLIP |'
- en: '|  |  | Zhang *et al*.[[331](https://arxiv.org/html/2501.02341v1#bib.bib331)]
    | VFM+LLM | GroundingDINO + LLM |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Zhang *等*.[[331](https://arxiv.org/html/2501.02341v1#bib.bib331)] |
    VFM+LLM | GroundingDINO + LLM |'
- en: '|  |  | Chen *et al*.[[332](https://arxiv.org/html/2501.02341v1#bib.bib332)]
    | LLM | GPT-3.5 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Chen *等*.[[332](https://arxiv.org/html/2501.02341v1#bib.bib332)] |
    LLM | GPT-3.5 |'
- en: '|  | Tracking | CloudTrack[[333](https://arxiv.org/html/2501.02341v1#bib.bib333)]
    | VFM+VLM | Grounding DINO + VLMs |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  | 跟踪 | CloudTrack[[333](https://arxiv.org/html/2501.02341v1#bib.bib333)]
    | VFM+VLM | Grounding DINO + VLMs |'
- en: '|  | Target Search | NEUSIS[[334](https://arxiv.org/html/2501.02341v1#bib.bib334)]
    | VFM+VLM | HYDRA + CLIP + Grounding DINO + EfficientSAM |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '|  | 目标搜索 | NEUSIS[[334](https://arxiv.org/html/2501.02341v1#bib.bib334)] |
    VFM+VLM | HYDRA + CLIP + Grounding DINO + EfficientSAM |'
- en: '|  |  | Say-REAPEx[[335](https://arxiv.org/html/2501.02341v1#bib.bib335)] |
    LLM | GPT-4o-mini / Llama3 / Claude3 / Gemini |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Say-REAPEx[[335](https://arxiv.org/html/2501.02341v1#bib.bib335)] |
    LLM | GPT-4o-mini / Llama3 / Claude3 / Gemini |'
- en: '| Planning | _ | TypeFly[[5](https://arxiv.org/html/2501.02341v1#bib.bib5)]
    | LLM | GPT-4 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 规划 | _ | TypeFly[[5](https://arxiv.org/html/2501.02341v1#bib.bib5)] | LLM
    | GPT-4 |'
- en: '|  |  | SPINE[[336](https://arxiv.org/html/2501.02341v1#bib.bib336)] | LLM+VFM+VLM
    | GPT-4 + Grounding DINO + LLaVA |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '|  |  | SPINE[[336](https://arxiv.org/html/2501.02341v1#bib.bib336)] | LLM+VFM+VLM
    | GPT-4 + Grounding DINO + LLaVA |'
- en: '|  |  | LEVIOSA[[337](https://arxiv.org/html/2501.02341v1#bib.bib337)] | LLM
    | Gemini 1.5 / GPT-4o |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LEVIOSA[[337](https://arxiv.org/html/2501.02341v1#bib.bib337)] | LLM
    | Gemini 1.5 / GPT-4o |'
- en: '|  |  | TPML[[338](https://arxiv.org/html/2501.02341v1#bib.bib338)] | LLM |
    GPT / PengCheng Mind[[339](https://arxiv.org/html/2501.02341v1#bib.bib339)] |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  |  | TPML[[338](https://arxiv.org/html/2501.02341v1#bib.bib338)] | LLM |
    GPT / PengCheng Mind[[339](https://arxiv.org/html/2501.02341v1#bib.bib339)] |'
- en: '|  |  | REAL[[6](https://arxiv.org/html/2501.02341v1#bib.bib6)] | LLM | GPT-4
    |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  |  | REAL[[6](https://arxiv.org/html/2501.02341v1#bib.bib6)] | LLM | GPT-4
    |'
- en: '|  |  | Liu *et al*.[[340](https://arxiv.org/html/2501.02341v1#bib.bib340)]
    | LLM | GPT-4 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Liu *等*.[[340](https://arxiv.org/html/2501.02341v1#bib.bib340)] | LLM
    | GPT-4 |'
- en: '| Flight Control | Single-agent | PromptCraft[[341](https://arxiv.org/html/2501.02341v1#bib.bib341)]
    | LLM | GPT |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 飞行控制 | 单一代理 | PromptCraft[[341](https://arxiv.org/html/2501.02341v1#bib.bib341)]
    | LLM | GPT |'
- en: '|  |  | Zhong *et al*.[[342](https://arxiv.org/html/2501.02341v1#bib.bib342)]
    | LLM | GPT |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Zhong *等*.[[342](https://arxiv.org/html/2501.02341v1#bib.bib342)] |
    LLM | GPT |'
- en: '|  |  | Tazir *et al*.[[343](https://arxiv.org/html/2501.02341v1#bib.bib343)]
    | LLM | GPT-3.5 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Tazir *等*.[[343](https://arxiv.org/html/2501.02341v1#bib.bib343)] |
    LLM | GPT-3.5 |'
- en: '|  |  | Phadke *et al*.[[344](https://arxiv.org/html/2501.02341v1#bib.bib344)]
    | LLM | _ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Phadke *等*.[[344](https://arxiv.org/html/2501.02341v1#bib.bib344)]
    | LLM | _ |'
- en: '|  |  | EAI-SIM[[345](https://arxiv.org/html/2501.02341v1#bib.bib345)] | LLM
    | GPT / PengCheng Mind[[339](https://arxiv.org/html/2501.02341v1#bib.bib339)]
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '|  |  | EAI-SIM[[345](https://arxiv.org/html/2501.02341v1#bib.bib345)] | LLM
    | GPT / PengCheng Mind[[339](https://arxiv.org/html/2501.02341v1#bib.bib339)]
    |'
- en: '|  |  | TAIiST[[346](https://arxiv.org/html/2501.02341v1#bib.bib346)] | LLM
    | GPT-3.5 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '|  |  | TAIiST[[346](https://arxiv.org/html/2501.02341v1#bib.bib346)] | LLM
    | GPT-3.5 |'
- en: '|  | Swarm | Swarm-GPT[[347](https://arxiv.org/html/2501.02341v1#bib.bib347)]
    | LLM | GPT-3.5 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '|  | Swarm | Swarm-GPT[[347](https://arxiv.org/html/2501.02341v1#bib.bib347)]
    | LLM | GPT-3.5 |'
- en: '|  |  | FlockGPT[[348](https://arxiv.org/html/2501.02341v1#bib.bib348)] | LLM
    | GPT-4 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '|  |  | FlockGPT[[348](https://arxiv.org/html/2501.02341v1#bib.bib348)] | LLM
    | GPT-4 |'
- en: '|  |  | CLIPSwarm[[349](https://arxiv.org/html/2501.02341v1#bib.bib349)] |
    VFM | CLIP |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '|  |  | CLIPSwarm[[349](https://arxiv.org/html/2501.02341v1#bib.bib349)] |
    VFM | CLIP |'
- en: '| Infrastructures | _ | DTLLM-VLT[[350](https://arxiv.org/html/2501.02341v1#bib.bib350)]
    | VFM+LLM | SAM + Osprey + LLaMA / Vicuna |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 基础设施 | _ | DTLLM-VLT[[350](https://arxiv.org/html/2501.02341v1#bib.bib350)]
    | VFM+LLM | SAM + Osprey + LLaMA / Vicuna |'
- en: '|  |  | Yao *et al*.[[271](https://arxiv.org/html/2501.02341v1#bib.bib271)]
    | LLM | GPT-3.5 / ChatGLM |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Yao *et al*.[[271](https://arxiv.org/html/2501.02341v1#bib.bib271)]
    | LLM | GPT-3.5 / ChatGLM |'
- en: '|  |  | GPG2A[[351](https://arxiv.org/html/2501.02341v1#bib.bib351)] | LLM
    | Gemini |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '|  |  | GPG2A[[351](https://arxiv.org/html/2501.02341v1#bib.bib351)] | LLM
    | Gemini |'
- en: '|  |  | AeroVerse[[352](https://arxiv.org/html/2501.02341v1#bib.bib352)] |
    VLM+LLM | VLMs + GPT-4 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '|  |  | AeroVerse[[352](https://arxiv.org/html/2501.02341v1#bib.bib352)] |
    VLM+LLM | VLMs + GPT-4 |'
- en: '|  |  | Tang *et al*.[[353](https://arxiv.org/html/2501.02341v1#bib.bib353)]
    | LLM | _ |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Tang *et al*.[[353](https://arxiv.org/html/2501.02341v1#bib.bib353)]
    | LLM | _ |'
- en: '|  |  | Xu *et al*.[[354](https://arxiv.org/html/2501.02341v1#bib.bib354)]
    | LLM | _ |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Xu *et al*.[[354](https://arxiv.org/html/2501.02341v1#bib.bib354)]
    | LLM | _ |'
- en: '|  |  | LLM-RS[[355](https://arxiv.org/html/2501.02341v1#bib.bib355)] | LLM
    | ChatGLM2 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|  |  | LLM-RS[[355](https://arxiv.org/html/2501.02341v1#bib.bib355)] | LLM
    | ChatGLM2 |'
- en: '|  |  | Pineli *et al*.[[356](https://arxiv.org/html/2501.02341v1#bib.bib356)]
    | LLM | LLaMA3 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Pineli *et al*.[[356](https://arxiv.org/html/2501.02341v1#bib.bib356)]
    | LLM | LLaMA3 |'
- en: 5.1 Visual Perception
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 视觉感知
- en: 5.1.1 Object Detection
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 物体检测
- en: Traditional object detection algorithms face significant challenges in UAV applications.
    The variations in flight altitude and viewing angles of UAVs result in changes
    in the visual field, making multi-scale object detection a key research focus
    [[357](https://arxiv.org/html/2501.02341v1#bib.bib357), [358](https://arxiv.org/html/2501.02341v1#bib.bib358),
    [359](https://arxiv.org/html/2501.02341v1#bib.bib359)]. However, dynamic environmental
    conditions, diverse shooting scenarios, and unpredictability further increase
    the complexity of detection tasks [[360](https://arxiv.org/html/2501.02341v1#bib.bib360),
    [361](https://arxiv.org/html/2501.02341v1#bib.bib361)]. Additionally, domain-specific
    characteristics of different scenes make it difficult for models to achieve robust
    generalization across diverse environments. To address these challenges, some
    studies attempt to enhance model robustness through improved training strategies,
    such as training dedicated models for specific UAV scenarios or introducing multi-task
    learning frameworks [[362](https://arxiv.org/html/2501.02341v1#bib.bib362), [363](https://arxiv.org/html/2501.02341v1#bib.bib363),
    [364](https://arxiv.org/html/2501.02341v1#bib.bib364)]. However, these methods
    often incur high training costs and still have limitations in generalization.
    Moreover, traditional supervised learning methods rely heavily on large amounts
    of manually labeled data, further increasing the time and resource costs associated
    with dataset construction.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的物体检测算法在无人机应用中面临显著的挑战。无人机的飞行高度和视角的变化导致视觉场的变化，使得多尺度物体检测成为一个关键的研究方向[[357](https://arxiv.org/html/2501.02341v1#bib.bib357),
    [358](https://arxiv.org/html/2501.02341v1#bib.bib358), [359](https://arxiv.org/html/2501.02341v1#bib.bib359)]。然而，动态的环境条件、多样化的拍摄场景以及不可预测性进一步增加了检测任务的复杂性[[360](https://arxiv.org/html/2501.02341v1#bib.bib360),
    [361](https://arxiv.org/html/2501.02341v1#bib.bib361)]。此外，不同场景的领域特征使得模型在不同环境中实现稳健的泛化变得困难。为了解决这些挑战，一些研究尝试通过改进训练策略来增强模型的鲁棒性，例如为特定的无人机场景训练专用模型，或引入多任务学习框架[[362](https://arxiv.org/html/2501.02341v1#bib.bib362),
    [363](https://arxiv.org/html/2501.02341v1#bib.bib363), [364](https://arxiv.org/html/2501.02341v1#bib.bib364)]。然而，这些方法通常会带来较高的训练成本，并且在泛化能力上仍存在局限性。此外，传统的监督学习方法过度依赖大量人工标注的数据，进一步增加了数据集构建所需的时间和资源成本。
- en: From a higher perspective, traditional bounding box-based object detection methods
    primarily focus on the geometric features of objects and lack the ability to model
    contextual information. Such discrete low-level feature representations struggle
    to capture complex semantic information, limiting their potential for high-level
    tasks. The introduction of natural language opens new avenues for UAV object detection
    tasks. The combination of natural language and vision leverages their complementary
    strengths. With the flexibility, zero-shot learning capabilities, contextual understanding,
    and strong generalization performance of VLMs and VFMs, these models can effectively
    tackle complex tasks and significantly improve the accuracy and adaptability of
    UAV object detection through multimodal integration.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 从更高的角度来看，传统的基于边界框的目标检测方法主要关注物体的几何特征，缺乏对上下文信息建模的能力。这些离散的低级特征表示难以捕捉复杂的语义信息，限制了它们在高级任务中的潜力。自然语言的引入为无人机（UAV）目标检测任务开辟了新的道路。自然语言与视觉的结合发挥了它们互补的优势。借助视觉语言模型（VLMs）和视觉语言模型（VFMs）的灵活性、零-shot学习能力、上下文理解能力以及强大的泛化性能，这些模型能够有效应对复杂任务，并通过多模态融合显著提高无人机目标检测的准确性和适应性。
- en: In specific applications, Li *et al*. [[312](https://arxiv.org/html/2501.02341v1#bib.bib312)]
    combined CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)] with traditional
    object tracking modules to accomplish natural language tracking (TNL) tasks for
    UAVs. Ma *et al*. [[313](https://arxiv.org/html/2501.02341v1#bib.bib313)] enhanced
    the accuracy of road scene detection in UAV imagery by integrating Grounding DINO
    [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] and CLIP. Limberg *et
    al*. [[314](https://arxiv.org/html/2501.02341v1#bib.bib314)] utilized the combination
    of YOLO-World [[196](https://arxiv.org/html/2501.02341v1#bib.bib196)] and GPT-4V
    [[167](https://arxiv.org/html/2501.02341v1#bib.bib167)] to achieve zero-shot human
    detection and action recognition in UAV imagery. Kim *et al*. [[315](https://arxiv.org/html/2501.02341v1#bib.bib315)]
    employed LLaVA-1.5 [[170](https://arxiv.org/html/2501.02341v1#bib.bib170)] to
    generate weather descriptions for UAV images by combining visual features with
    language prompts such as weather and lighting conditions. Using a CLIP encoder,
    they fused image features with weather-related information. Based on this framework,
    weather-aware object queries were implemented, effectively leveraging weather
    information in object detection tasks, thereby significantly improving detection
    accuracy and robustness.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在具体应用中，Li *et al*.[[312](https://arxiv.org/html/2501.02341v1#bib.bib312)] 将CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]与传统的目标跟踪模块相结合，实现了无人机的自然语言跟踪（TNL）任务。Ma
    *et al*.[[313](https://arxiv.org/html/2501.02341v1#bib.bib313)] 通过将Grounding DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]和CLIP结合，提高了无人机影像中道路场景检测的准确性。Limberg
    *et al*.[[314](https://arxiv.org/html/2501.02341v1#bib.bib314)] 利用YOLO-World[[196](https://arxiv.org/html/2501.02341v1#bib.bib196)]和GPT-4V[[167](https://arxiv.org/html/2501.02341v1#bib.bib167)]的组合，实现在无人机影像中进行零-shot人类检测和动作识别。Kim
    *et al*.[[315](https://arxiv.org/html/2501.02341v1#bib.bib315)] 使用LLaVA-1.5[[170](https://arxiv.org/html/2501.02341v1#bib.bib170)]
    结合视觉特征与天气和光照条件等语言提示，生成无人机影像的天气描述。他们通过CLIP编码器将图像特征与天气相关信息融合。在此框架下，实现了基于天气的目标查询，充分利用了天气信息在目标检测任务中的作用，从而显著提高了检测的准确性和鲁棒性。
- en: Notably, the multimodal representation capabilities of CLIP can generate high-quality
    domain-invariant features, providing strong support for training traditional object
    detection models. For example, LGNet [[3](https://arxiv.org/html/2501.02341v1#bib.bib3)]
    introduced CLIP’s multimodal features, significantly enhancing the robustness
    and performance of UAV object detection under diverse shooting conditions. Furthermore,
    LLMs, VLMs, and VFMs have accumulated extensive research experience in general
    object detection tasks, offering important insights for UAV object detection tasks.
    Examples include LLM-AR [[365](https://arxiv.org/html/2501.02341v1#bib.bib365)],
    Han *et al*. [[366](https://arxiv.org/html/2501.02341v1#bib.bib366)], Lin *et
    al*. [[367](https://arxiv.org/html/2501.02341v1#bib.bib367)], ContextDET [[368](https://arxiv.org/html/2501.02341v1#bib.bib368)],
    and LLMI3D [[369](https://arxiv.org/html/2501.02341v1#bib.bib369)].
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，CLIP的多模态表示能力能够生成高质量的领域不变特征，为训练传统的目标检测模型提供了有力支持。例如，LGNet [[3](https://arxiv.org/html/2501.02341v1#bib.bib3)]引入了CLIP的多模态特征，显著增强了无人机目标检测在不同拍摄条件下的鲁棒性和性能。此外，LLM、VLM和VFM在通用目标检测任务中积累了大量的研究经验，为无人机目标检测任务提供了重要的启示。例如，LLM-AR[[365](https://arxiv.org/html/2501.02341v1#bib.bib365)]、Han
    *et al*.[[366](https://arxiv.org/html/2501.02341v1#bib.bib366)]、Lin *et al*.[[367](https://arxiv.org/html/2501.02341v1#bib.bib367)]、ContextDET[[368](https://arxiv.org/html/2501.02341v1#bib.bib368)]和LLMI3D[[369](https://arxiv.org/html/2501.02341v1#bib.bib369)]等研究都在该领域提供了有价值的贡献。
- en: However, relying solely on VFMs or VLMs for object detection may lead to performance
    limitations in certain scenarios due to model hallucinations or inadequate task-specific
    adaptability [[370](https://arxiv.org/html/2501.02341v1#bib.bib370), [371](https://arxiv.org/html/2501.02341v1#bib.bib371),
    [372](https://arxiv.org/html/2501.02341v1#bib.bib372)]. While traditional deep
    learning models exhibit reliable performance in specific tasks, they lack cross-task
    generalization capabilities. A better solution is to adopt a “large model + small
    model” collaborative architecture, leveraging the strong generalization capabilities
    of large models and the domain specialization of small models. For example, the
    Hidetomo Sakaino Visual Recognition Group [[316](https://arxiv.org/html/2501.02341v1#bib.bib316)]
    proposed a method combining DL models with VLMs for visibility and weather condition
    estimation. This method effectively addresses challenges such as scale variation,
    perspective changes, and environmental disturbances in image processing, including
    sky background interference and long-distance small object detection. It demonstrated
    outstanding robustness and stability across various environments and weather conditions.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅依靠VFM或VLM进行目标检测可能会导致在某些场景中的性能限制，这主要是由于模型的幻觉现象或任务特定适应性不足[[370](https://arxiv.org/html/2501.02341v1#bib.bib370),
    [371](https://arxiv.org/html/2501.02341v1#bib.bib371), [372](https://arxiv.org/html/2501.02341v1#bib.bib372)]。虽然传统的深度学习模型在特定任务中表现出可靠的性能，但它们缺乏跨任务的泛化能力。一个更好的解决方案是采用“大模型
    + 小模型”协同架构，利用大模型强大的泛化能力和小模型的领域专长。例如，Hidetomo Sakaino视觉识别小组[[316](https://arxiv.org/html/2501.02341v1#bib.bib316)]提出了一种将深度学习模型与VLM结合用于可见度和天气条件估计的方法。这种方法有效地解决了图像处理中的尺度变化、视角变化和环境干扰等挑战，包括天空背景干扰和远距离小物体检测。该方法在各种环境和天气条件下展现了出色的鲁棒性和稳定性。
- en: 5.1.2 Semantic Segmentation
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 语义分割
- en: As a computer vision task, UAV semantic segmentation faces many challenges similar
    to those in object detection, such as insufficient generalization capabilities
    caused by adversarial visual conditions and a heavy reliance on manually labeled
    data. Additionally, UAV remote sensing images encounter complex foreground-background
    interactions across multi-scale scenarios in semantic segmentation tasks, posing
    unique challenges in this field [[373](https://arxiv.org/html/2501.02341v1#bib.bib373),
    [374](https://arxiv.org/html/2501.02341v1#bib.bib374)]. Although Domain Adaptation
    (DA) techniques have been widely applied to cross-domain semantic segmentation,
    these approaches primarily force the model to adjust from the source domain to
    a predefined target domain. However, such methods exhibit limited generalization
    to unseen domains, underscoring the urgent need for more flexible and robust strategies
    to enhance model performance in diverse scenarios [[375](https://arxiv.org/html/2501.02341v1#bib.bib375),
    [376](https://arxiv.org/html/2501.02341v1#bib.bib376)].
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉任务，无人机语义分割面临着与目标检测类似的许多挑战，如由于对抗性视觉条件造成的泛化能力不足，以及对人工标注数据的高度依赖。此外，无人机遥感图像在语义分割任务中遇到复杂的前景-背景交互，涵盖多尺度场景，给该领域带来了独特的挑战
    [[373](https://arxiv.org/html/2501.02341v1#bib.bib373), [374](https://arxiv.org/html/2501.02341v1#bib.bib374)]。尽管领域适应（DA）技术已广泛应用于跨领域语义分割，但这些方法主要强制模型从源领域调整到预定义的目标领域。然而，这些方法在未见过的领域中表现出有限的泛化能力，突显了迫切需要更加灵活和稳健的策略，以提高模型在多样化场景中的表现
    [[375](https://arxiv.org/html/2501.02341v1#bib.bib375), [376](https://arxiv.org/html/2501.02341v1#bib.bib376)]。
- en: The introduction of VLMs and VFMs into UAV semantic segmentation tasks has infused
    the field with new technological momentum. These models can efficiently perform
    zero-shot semantic segmentation while flexibly defining and guiding segmentation
    tasks through natural language interactions, demonstrating exceptional potential
    to meet diverse scenario requirements. For example, COMRP [[313](https://arxiv.org/html/2501.02341v1#bib.bib313)]
    focuses on parsing road scenes in high-resolution UAV imagery. Its method first
    utilizes Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    and CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)] to extract road-related
    regions and automatically generates segmentation masks using SAM [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)].
    Then, features are extracted using ResNet [[377](https://arxiv.org/html/2501.02341v1#bib.bib377)]
    and DINOv2 [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)], and mask feature
    vectors are clustered using a spectral clustering method to generate pseudo-labels.
    These labels are used to train a teacher model, iteratively optimizing the performance
    of a student model. COMRP eliminates the dependence on manual annotations, providing
    an efficient and automated solution for UAV road scene parsing.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: VLMs和VFM的引入为无人机语义分割任务注入了新的技术动力。这些模型可以高效地执行零-shot语义分割，并通过自然语言交互灵活地定义和指导分割任务，展示了在满足多样化场景需求方面的卓越潜力。例如，COMRP
    [[313](https://arxiv.org/html/2501.02341v1#bib.bib313)] 重点解析高分辨率无人机图像中的道路场景。其方法首先利用Grounding
    DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]和CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]提取与道路相关的区域，并使用SAM
    [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]自动生成分割掩模。然后，使用ResNet [[377](https://arxiv.org/html/2501.02341v1#bib.bib377)]和DINOv2
    [[193](https://arxiv.org/html/2501.02341v1#bib.bib193)]提取特征，并通过光谱聚类方法对掩模特征向量进行聚类，生成伪标签。这些标签用于训练教师模型，迭代优化学生模型的性能。COMRP消除了对人工标注的依赖，为无人机道路场景解析提供了一种高效且自动化的解决方案。
- en: 'Additionally, CrossEarth [[319](https://arxiv.org/html/2501.02341v1#bib.bib319)]
    is a cross-domain generalization semantic segmentation VFM designed for the remote
    sensing field. It combines two complementary strategies: Earth-style injection
    and multi-task training, significantly enhancing cross-domain generalization capabilities.
    Earth-style injection incorporates diverse styles from the Earth domain into the
    source domain data, extending the distribution range of the training data. Multi-task
    training leverages a shared DINOv2 backbone to optimize both semantic segmentation
    and mask image modeling tasks simultaneously, enabling the learning of robust
    semantic features.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，CrossEarth [[319](https://arxiv.org/html/2501.02341v1#bib.bib319)] 是为遥感领域设计的跨领域泛化语义分割VFM。它结合了两种互补策略：地球风格注入和多任务训练，显著增强了跨领域泛化能力。地球风格注入将来自地球领域的多样化风格融入源领域数据，扩展了训练数据的分布范围。多任务训练利用共享的DINOv2骨干网络，同时优化语义分割和掩模图像建模任务，从而实现了稳健语义特征的学习。
- en: 5.1.3 Depth Estimation
  id: totrans-429
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3 深度估计
- en: One of the core functionalities of UAV perception systems is to perform 3D modeling
    of terrain and natural environments, thereby generating consistent and accurate
    3D geometric representations of the flight area. In recent years, methods based
    on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have made significant
    progress in this task, exemplified by UAV-NeRF [[378](https://arxiv.org/html/2501.02341v1#bib.bib378)]
    and AGS [[379](https://arxiv.org/html/2501.02341v1#bib.bib379)]. However, these
    methods still face numerous challenges in large-scale scenarios. Against this
    backdrop, Monocular Depth Estimation (MDE) has gradually emerged as a more advantageous
    solution [[380](https://arxiv.org/html/2501.02341v1#bib.bib380), [381](https://arxiv.org/html/2501.02341v1#bib.bib381),
    [382](https://arxiv.org/html/2501.02341v1#bib.bib382)].
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机感知系统的核心功能之一是对地形和自然环境进行3D建模，从而生成飞行区域一致且准确的3D几何表示。近年来，基于神经辐射场（NeRF）和3D高斯点云（3DGS）的方法在这项任务中取得了显著进展，典型代表有UAV-NeRF
    [[378](https://arxiv.org/html/2501.02341v1#bib.bib378)] 和AGS [[379](https://arxiv.org/html/2501.02341v1#bib.bib379)]。然而，这些方法在大规模场景中仍面临诸多挑战。在这种背景下，单目深度估计（MDE）逐渐成为一种更具优势的解决方案
    [[380](https://arxiv.org/html/2501.02341v1#bib.bib380), [381](https://arxiv.org/html/2501.02341v1#bib.bib381),
    [382](https://arxiv.org/html/2501.02341v1#bib.bib382)]。
- en: Florea *et al*. [[320](https://arxiv.org/html/2501.02341v1#bib.bib320)] proposed
    the TanDepth framework, which combines the relative depth estimation of the Depth
    Anything[[215](https://arxiv.org/html/2501.02341v1#bib.bib215)] model with Global
    Digital Elevation Model (GDEM) data to generate high-precision Depth images with
    real-world dimensions using a scale recovery method. Experimental results on multiple
    UAV datasets demonstrate that TanDepth exhibits outstanding accuracy and robustness
    in complex terrains and dynamic flight environments. This approach opens up new
    technological directions for UAV depth estimation tasks, particularly showcasing
    its efficiency and adaptability in scenarios lacking high-precision depth sensors.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: Florea *等* [[320](https://arxiv.org/html/2501.02341v1#bib.bib320)] 提出了TanDepth框架，该框架结合了Depth
    Anything[[215](https://arxiv.org/html/2501.02341v1#bib.bib215)]模型的相对深度估计与全球数字高程模型（GDEM）数据，利用尺度恢复方法生成具有真实世界尺寸的高精度深度图像。在多个无人机数据集上的实验结果表明，TanDepth在复杂地形和动态飞行环境中表现出卓越的准确性和鲁棒性。这一方法为无人机深度估计任务开辟了新的技术方向，特别是在缺乏高精度深度传感器的场景中，展现了其高效性和适应性。
- en: 5.1.4 Visual Caption and VQA
  id: totrans-432
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4 视觉描述和视觉问答
- en: Visual caption and VQA belong to the cross-modal fusion domain of computer vision
    and natural language processing, focusing on semantic understanding and natural
    language representation of image and video content. Traditional methods are often
    based on deep learning frameworks, where visual feature extraction and language
    generation are designed as independent modules. However, such a separated design
    exhibits significant limitations in complex scenarios, open-domain problems, and
    fine-grained description generation, primarily constrained by the model’s expressive
    capabilities and the misalignment of multimodal features [[383](https://arxiv.org/html/2501.02341v1#bib.bib383),
    [384](https://arxiv.org/html/2501.02341v1#bib.bib384), [385](https://arxiv.org/html/2501.02341v1#bib.bib385),
    [386](https://arxiv.org/html/2501.02341v1#bib.bib386)]. With the rapid advancement
    of VLMs and VFMs, these models leverage joint representation learning of vision
    and language to significantly enhance their understanding of complex cross-modal
    information. Pretrained on large-scale multimodal datasets, VLMs and VFMs exhibit
    exceptional task generalization capabilities and can generate fine-grained semantic
    descriptions in complex scenarios, demonstrating high adaptability to open-domain
    tasks [[169](https://arxiv.org/html/2501.02341v1#bib.bib169), [174](https://arxiv.org/html/2501.02341v1#bib.bib174),
    [175](https://arxiv.org/html/2501.02341v1#bib.bib175), [176](https://arxiv.org/html/2501.02341v1#bib.bib176),
    [180](https://arxiv.org/html/2501.02341v1#bib.bib180), [185](https://arxiv.org/html/2501.02341v1#bib.bib185)].
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉标注和视觉问答（VQA）属于计算机视觉和自然语言处理的跨模态融合领域，重点关注图像和视频内容的语义理解与自然语言表示。传统方法通常基于深度学习框架，其中视觉特征提取和语言生成被设计为独立的模块。然而，这种分离的设计在复杂场景、开放领域问题以及细粒度描述生成中表现出显著的局限性，主要受到模型表达能力和多模态特征对齐问题的制约[[383](https://arxiv.org/html/2501.02341v1#bib.bib383)，[384](https://arxiv.org/html/2501.02341v1#bib.bib384)，[385](https://arxiv.org/html/2501.02341v1#bib.bib385)，[386](https://arxiv.org/html/2501.02341v1#bib.bib386)]。随着视觉语言模型（VLMs）和视觉功能模型（VFMs）的快速发展，这些模型利用视觉与语言的联合表示学习，显著提升了它们对复杂跨模态信息的理解能力。通过在大规模多模态数据集上进行预训练，VLMs和VFMs展现了卓越的任务泛化能力，能够在复杂场景中生成细粒度的语义描述，并展示了在开放领域任务中的高度适应性[[169](https://arxiv.org/html/2501.02341v1#bib.bib169)，[174](https://arxiv.org/html/2501.02341v1#bib.bib174)，[175](https://arxiv.org/html/2501.02341v1#bib.bib175)，[176](https://arxiv.org/html/2501.02341v1#bib.bib176)，[180](https://arxiv.org/html/2501.02341v1#bib.bib180)，[185](https://arxiv.org/html/2501.02341v1#bib.bib185)]。
- en: 'In UAV visual caption and visual question-answering tasks, research mainly
    focuses on two directions: the first is selecting or combining existing VLMs and
    VFMs to adapt them to UAV task scenarios based on specific UAV requirements; the
    second is training or fine-tuning VLMs or VFMs with domain-specific data to build
    specialized models for UAV vertical applications, addressing unique challenges.
    These research directions aim to further enhance UAVs’ visual perception, semantic
    reasoning, and task execution capabilities in complex environments, providing
    robust support for intelligent and user-friendly human-machine interaction.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在无人机视觉标注和视觉问答任务中，研究主要集中在两个方向：第一个是选择或结合现有的视觉语言模型（VLMs）和视觉功能模型（VFMs），根据特定的无人机任务需求调整它们以适应无人机任务场景；第二个是使用领域特定数据训练或微调VLMs或VFMs，构建针对无人机垂直应用的专业模型，解决独特的挑战。这些研究方向旨在进一步增强无人机在复杂环境中的视觉感知、语义推理和任务执行能力，为智能化和用户友好的人机交互提供强有力的支持。
- en: For the first research direction, several studies have explored combining existing
    VLMs and VFMs to adapt to UAV scenarios. For instance, Qiu *et al*. [[4](https://arxiv.org/html/2501.02341v1#bib.bib4)]
    proposed the DroneGPT framework based on the visual reasoning model VISPROG [[185](https://arxiv.org/html/2501.02341v1#bib.bib185)],
    where GPT-3.5 [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] converts
    user natural language queries into task logic codes. These codes invoke Grounding
    DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] to parse visual information
    and perform semantic reasoning, ultimately outputting clear and accurate visual
    question-answering results. De Zarzà *et al*. designed a framework combining BLIP-2
    [[176](https://arxiv.org/html/2501.02341v1#bib.bib176)] with GPT-3.5 for efficient
    UAV video scene understanding and semantic reasoning, where BLIP-2 extracts preliminary
    semantic information from each video frame, and GPT-3.5 generates high-level scene
    descriptions. The AeroAgent architecture [[322](https://arxiv.org/html/2501.02341v1#bib.bib322)]
    optimizes UAV visual question-answering modules from an agent perspective. Built
    on GPT-4V [[167](https://arxiv.org/html/2501.02341v1#bib.bib167)], it constructs
    a retrievable multimodal memory database (similar to the RAG framework), significantly
    improving comprehension and answer accuracy in complex scenarios while mitigating
    hallucination issues in generative models.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个研究方向，一些研究探讨了结合现有的VLM（视觉语言模型）和VFM（视觉推理模型）以适应无人机场景。例如，Qiu *等人* [[4](https://arxiv.org/html/2501.02341v1#bib.bib4)]
    提出了基于视觉推理模型VISPROG [[185](https://arxiv.org/html/2501.02341v1#bib.bib185)] 的DroneGPT框架，其中GPT-3.5
    [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] 将用户的自然语言查询转换为任务逻辑代码。这些代码调用Grounding
    DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 解析视觉信息并执行语义推理，最终输出清晰准确的视觉问答结果。De
    Zarzà *等人* 设计了一个将BLIP-2 [[176](https://arxiv.org/html/2501.02341v1#bib.bib176)]
    与GPT-3.5结合的框架，用于高效的无人机视频场景理解和语义推理，其中BLIP-2从每个视频帧中提取初步的语义信息，GPT-3.5则生成高级场景描述。AeroAgent架构
    [[322](https://arxiv.org/html/2501.02341v1#bib.bib322)] 从代理视角优化无人机视觉问答模块。该架构基于GPT-4V
    [[167](https://arxiv.org/html/2501.02341v1#bib.bib167)]，构建了一个可检索的多模态记忆数据库（类似于RAG框架），在复杂场景中显著提高了理解力和回答的准确性，同时减轻了生成模型中的幻觉问题。
- en: For the second research direction, existing work mainly focuses on the UAV remote
    sensing field, aiming to enhance the semantic understanding of remote sensing
    images by developing VLMs and VFMs tailored to this domain. Traditional remote
    sensing vision analysis methods heavily rely on domain expertise, incur high annotation
    costs, and show limited performance in handling complex scenarios and semantic
    interactions. Bazi *et al*. proposed RS-LLaVA [[323](https://arxiv.org/html/2501.02341v1#bib.bib323)],
    which pretrains and fine-tunes the LLaVA-1.5 [[170](https://arxiv.org/html/2501.02341v1#bib.bib170)]
    model for domain-specific tasks, enabling subtitle generation and VQA for remote
    sensing images. Zhang *et al*. [[324](https://arxiv.org/html/2501.02341v1#bib.bib324)]
    developed the GeoRSCLIP model by constructing a large-scale remote sensing image-text
    paired dataset, RS5M, and performing full fine-tuning or Parameter-Efficient Fine-Tuning
    (PEFT) on the CLIP [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)] model.
    The model demonstrated outstanding performance in zero-shot classification (ZSC),
    cross-modal retrieval (RSCTIR), and semantic localization (SeLo), showcasing robust
    domain adaptability and task generalization capabilities. SkyEyeGPT [[325](https://arxiv.org/html/2501.02341v1#bib.bib325)],
    a unified framework for visual-language tasks in the remote sensing field, employs
    EVA-CLIP [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] as the visual
    encoder to extract image features and LLaMA2 [[154](https://arxiv.org/html/2501.02341v1#bib.bib154)]
    as the language decoder for task outputs. Instruction-tuned datasets optimize
    the model to support tasks such as image description, VQA, and visual localization.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个研究方向，现有的工作主要集中在无人机遥感领域，旨在通过开发针对该领域的视觉-语言模型（VLMs）和视觉-融合模型（VFMs）来增强遥感图像的语义理解。传统的遥感视觉分析方法严重依赖领域专业知识，带来高昂的标注成本，并且在处理复杂场景和语义交互时表现有限。Bazi
    *et al*. 提出了RS-LLaVA [[323](https://arxiv.org/html/2501.02341v1#bib.bib323)]，该方法预训练并微调了LLaVA-1.5
    [[170](https://arxiv.org/html/2501.02341v1#bib.bib170)]模型以应对领域特定任务，支持遥感图像的字幕生成和视觉问答（VQA）。Zhang
    *et al*. [[324](https://arxiv.org/html/2501.02341v1#bib.bib324)] 开发了GeoRSCLIP模型，构建了大规模遥感图像-文本配对数据集RS5M，并对CLIP
    [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]模型进行了完全微调或参数高效微调（PEFT）。该模型在零样本分类（ZSC）、跨模态检索（RSCTIR）和语义定位（SeLo）任务中表现出色，展示了强大的领域适应能力和任务泛化能力。SkyEyeGPT
    [[325](https://arxiv.org/html/2501.02341v1#bib.bib325)] 是一个统一的遥感领域视觉-语言任务框架，采用EVA-CLIP
    [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)]作为视觉编码器提取图像特征，采用LLaMA2
    [[154](https://arxiv.org/html/2501.02341v1#bib.bib154)]作为语言解码器生成任务输出。通过指令调优的数据集优化该模型，以支持图像描述、视觉问答和视觉定位等任务。
- en: 5.2 Vision-Language Navigation
  id: totrans-437
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 视觉-语言导航
- en: In recent years, methods based on deep learning have achieved significant progress
    in VLN. For instance, vision and language fusion techniques based on the Transformer
    architecture have been widely applied in VLN and its derivative tasks [[268](https://arxiv.org/html/2501.02341v1#bib.bib268),
    [387](https://arxiv.org/html/2501.02341v1#bib.bib387), [388](https://arxiv.org/html/2501.02341v1#bib.bib388)].
    However, VLN tasks still face numerous challenges. On the one hand, the alignment
    and fusion of multimodal features remain a core difficulty, particularly in dynamic
    and complex scenarios where inconsistent features may lead to instability in task
    decision-making. On the other hand, the heavy reliance on existing methods of
    annotated data limits the model’s transferability to unannotated environments.
    Additionally, in open-domain tasks, the generalization and robustness of models
    still need further improvement.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，基于深度学习的方法在视觉-语言导航（VLN）领域取得了显著进展。例如，基于Transformer架构的视觉和语言融合技术已广泛应用于VLN及其衍生任务[[268](https://arxiv.org/html/2501.02341v1#bib.bib268),
    [387](https://arxiv.org/html/2501.02341v1#bib.bib387), [388](https://arxiv.org/html/2501.02341v1#bib.bib388)]。然而，VLN任务仍然面临许多挑战。一方面，多模态特征的对齐与融合仍然是核心难题，尤其是在动态复杂的场景中，不一致的特征可能导致任务决策的不稳定。另一方面，过度依赖现有的标注数据方法限制了模型在未标注环境中的迁移能力。此外，在开放领域任务中，模型的泛化能力和鲁棒性仍需要进一步提高。
- en: With the introduction of VLMs and VFMs, VLN and its derivative tasks have embraced
    a new developmental trajectory. Through large-scale pretraining, these models
    can effectively learn aligned representations of cross-modal features, significantly
    enhancing task understanding and execution capabilities. In complex and dynamic
    scenarios, they demonstrate outstanding generalization performance, providing
    stronger technical support for intelligent UAV navigation, target tracking, and
    target search.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 VLM 和 VFM 的引入，VLN 及其衍生任务迎来了新的发展轨迹。通过大规模预训练，这些模型可以有效地学习跨模态特征的对齐表示，显著增强任务理解和执行能力。在复杂和动态的场景中，它们展示了出色的泛化性能，为智能无人机导航、目标追踪和目标搜索提供了更强的技术支持。
- en: 'UAV VLN involves path planning in 3D space by combining visual inputs with
    natural language instructions. Compared to traditional ground-based navigation,
    aerial navigation requires consideration of flight altitude and the complexity
    of 3D spatial perception and reasoning. Additionally, UAV VLN tasks differ significantly
    across scenarios: indoor environments feature more defined geometric constraints,
    simplifying mission planning, while outdoor environments introduce greater complexity
    due to the scale of open spaces and dynamic environmental changes.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机 VLN 涉及通过将视觉输入与自然语言指令相结合来进行 3D 空间中的路径规划。与传统的地面导航相比，空中导航需要考虑飞行高度以及 3D 空间感知和推理的复杂性。此外，无人机
    VLN 任务在不同场景下差异显著：室内环境具有更明确的几何约束，简化了任务规划，而户外环境则由于开阔空间的规模和动态环境变化带来了更大的复杂性。
- en: 5.2.1 Indoor
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 室内
- en: For indoor UAV VLN, NaVid [[326](https://arxiv.org/html/2501.02341v1#bib.bib326)]
    utilizes EVA-CLIP [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] to extract
    visual features, combined with Q-Former [[175](https://arxiv.org/html/2501.02341v1#bib.bib175),
    [176](https://arxiv.org/html/2501.02341v1#bib.bib176)] to generate visual tokens
    and geometric tokens. Cross-modal projection aligns visual and language features,
    while Vicuna-7B [[243](https://arxiv.org/html/2501.02341v1#bib.bib243)] interprets
    natural language instructions and generates specific navigation actions. The system
    relies solely on monocular video streams without requiring maps, odometry, or
    depth information. By encoding historical observations as spatiotemporal context,
    it enables real-time reasoning for low-level navigation actions, demonstrating
    exceptional path planning and dynamic adjustment capabilities in indoor environments.
    Moreover, multimodal prompting shows significant potential in UAV VLN tasks. Hong
    *et al*. [[327](https://arxiv.org/html/2501.02341v1#bib.bib327)] proposed the
    VLN-MP framework, which enhances task understanding through multimodal prompts,
    reduces ambiguities in natural language instructions, and supports diverse and
    high-quality prompt settings. This system generates landmark-related image prompts
    using a data generation pipeline, combined with Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    or GLIP [[190](https://arxiv.org/html/2501.02341v1#bib.bib190)], while ControlNet
    [[389](https://arxiv.org/html/2501.02341v1#bib.bib389)] enhances data diversity.
    Finally, the system fuses image and text features via a visual encoder and multi-layer
    Transformer modules to generate precise navigation actions.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 对于室内无人机 VLN，NaVid [[326](https://arxiv.org/html/2501.02341v1#bib.bib326)] 使用
    EVA-CLIP [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)] 提取视觉特征，并结合 Q-Former
    [[175](https://arxiv.org/html/2501.02341v1#bib.bib175), [176](https://arxiv.org/html/2501.02341v1#bib.bib176)]
    生成视觉标记和几何标记。跨模态投影对齐了视觉和语言特征，而 Vicuna-7B [[243](https://arxiv.org/html/2501.02341v1#bib.bib243)]
    解释自然语言指令并生成具体的导航动作。该系统仅依赖单目视频流，不需要地图、里程计或深度信息。通过将历史观察编码为时空上下文，它能够进行实时推理以支持低级别的导航动作，在室内环境中展示了卓越的路径规划和动态调整能力。此外，多模态提示在无人机
    VLN 任务中展示了显著的潜力。Hong *et al*. [[327](https://arxiv.org/html/2501.02341v1#bib.bib327)]
    提出了 VLN-MP 框架，通过多模态提示增强任务理解，减少自然语言指令中的歧义，并支持多样化和高质量的提示设置。该系统使用数据生成管道生成与地标相关的图像提示，结合
    Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 或 GLIP
    [[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]，同时 ControlNet [[389](https://arxiv.org/html/2501.02341v1#bib.bib389)]
    增强了数据多样性。最后，系统通过视觉编码器和多层 Transformer 模块融合图像和文本特征，以生成精确的导航动作。
- en: 5.2.2 Outdoor
  id: totrans-443
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 户外
- en: For outdoor UAV VLN, Liu *et al*. [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)]
    proposed AerialVLN, addressing the gap in aerial navigation research. This task
    requires UAVs to navigate to target locations based on natural language instructions
    and first-person visual perception, treating all unoccupied points as navigable
    regions without preconstructed navigation maps. Based on this task, Liu *et al*.
    developed an extended baseline model built on conventional cross-modal alignment
    (CMA) navigation methods, providing an initial solution for aerial navigation.
    Subsequent research incorporated LLMs to enhance task performance. For example,
    Gao *et al*. [[328](https://arxiv.org/html/2501.02341v1#bib.bib328)] designed
    an LLM-based end-to-end UAV VLN framework. This system uses GPT-4o to decompose
    natural language instructions into multiple sub-goals and combines Grounding DINO
    [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] and Tokenize Anything
    (TAP)[[202](https://arxiv.org/html/2501.02341v1#bib.bib202)] to extract semantic
    masks and visual information. RGB images and Depth images are transformed into
    a semantic-topological-metric representation (STMR). With designed multimodal
    prompts, including task descriptions, historical trajectories, and semantic matrices,
    GPT-4o performs chain-of-thought reasoning to generate navigation actions (direction,
    rotation angle, and movement distance), significantly improving navigation success
    rates on the AerialVLN dataset.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 对于户外无人机视觉语言导航（UAV VLN），刘*等人* [[268](https://arxiv.org/html/2501.02341v1#bib.bib268)]
    提出了AerialVLN，解决了航空导航研究中的空白。该任务要求无人机根据自然语言指令和第一人称视觉感知导航到目标位置，将所有未被占用的点视为可导航区域，而无需预先构建导航地图。基于此任务，刘*等人*开发了一个基于传统跨模态对齐（CMA）导航方法的扩展基准模型，为航空导航提供了初步解决方案。后续研究通过引入大语言模型（LLMs）来提升任务性能。例如，高*等人*
    [[328](https://arxiv.org/html/2501.02341v1#bib.bib328)] 设计了一个基于大语言模型的端到端无人机视觉语言导航框架。该系统使用GPT-4o将自然语言指令分解为多个子目标，并结合Grounding
    DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 和Tokenize Anything
    (TAP) [[202](https://arxiv.org/html/2501.02341v1#bib.bib202)] 来提取语义掩模和视觉信息。RGB图像和深度图像被转化为语义-拓扑-度量表示（STMR）。通过设计的多模态提示，包括任务描述、历史轨迹和语义矩阵，GPT-4o执行链式思维推理，生成导航动作（方向、旋转角度和移动距离），显著提高了在AerialVLN数据集上的导航成功率。
- en: Other notable studies include the CityNav dataset and its accompanying model
    MGP proposed by Lee *et al*. [[267](https://arxiv.org/html/2501.02341v1#bib.bib267)].
    MGP uses GPT-3.5 [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] to interpret
    landmark names, spatial relationships, and task goals, combining Grounding DINO
    [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] and MobileSAM [[204](https://arxiv.org/html/2501.02341v1#bib.bib204)]
    to generate high-precision target regions for navigation map construction and
    path planning. Wang *et al*. [[329](https://arxiv.org/html/2501.02341v1#bib.bib329)]
    developed a system framework for UAV VLN, introducing the novel benchmark task
    UAV-Need-Help and constructing a related dataset via the OpenUAV simulation platform.
    Their UAV Navigation LLM model, based on Vicuna-7B [[243](https://arxiv.org/html/2501.02341v1#bib.bib243)]
    and EVA-CLIP [[189](https://arxiv.org/html/2501.02341v1#bib.bib189)], extracts
    visual features and employs a hierarchical trajectory generation mechanism for
    efficient natural language navigation. GOMAA-Geo[[2](https://arxiv.org/html/2501.02341v1#bib.bib2)]
    framework focuses on multimodal active geolocalization tasks by integrating various
    LLMs with CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]. It fully
    leverages multimodal target descriptions (such as natural language, ground images,
    and aerial images) and visual cues to achieve efficient and accurate target localization,
    demonstrating excellent zero-shot generalization capabilities. The NavAgent[[1](https://arxiv.org/html/2501.02341v1#bib.bib1)]
    framework incorporates advanced models such as LLaMA2[[154](https://arxiv.org/html/2501.02341v1#bib.bib154)],
    BLIP-2[[176](https://arxiv.org/html/2501.02341v1#bib.bib176)], GPT-4[[143](https://arxiv.org/html/2501.02341v1#bib.bib143)],
    and GLIP[[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]. Parsing natural
    language navigation instructions to extract landmark descriptions and utilizing
    a fine-tuned landmark recognition module achieve precise landmark localization
    in panoramic images. This framework excels in path planning and navigation tasks
    in urban outdoor scenarios, providing robust technical support for UAV navigation
    in complex environments. Related studies, such as ASMA [[330](https://arxiv.org/html/2501.02341v1#bib.bib330)],
    Zhang *et al*. [[331](https://arxiv.org/html/2501.02341v1#bib.bib331)], and Chen
    *et al*. [[332](https://arxiv.org/html/2501.02341v1#bib.bib332)] also explore
    UAV VLN solutions for outdoor environments and are worth further attention.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 其他显著的研究包括CityNav数据集及其配套模型MGP，该模型由Lee *等人* 提出[[267](https://arxiv.org/html/2501.02341v1#bib.bib267)]。MGP使用GPT-3.5[[141](https://arxiv.org/html/2501.02341v1#bib.bib141)]来解释地标名称、空间关系和任务目标，结合Grounding
    DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]和MobileSAM[[204](https://arxiv.org/html/2501.02341v1#bib.bib204)]生成高精度的目标区域，用于导航地图构建和路径规划。Wang
    *等人* [[329](https://arxiv.org/html/2501.02341v1#bib.bib329)]开发了一个无人机VLN的系统框架，引入了新的基准任务UAV-Need-Help，并通过OpenUAV仿真平台构建了相关数据集。他们的无人机导航LLM模型基于Vicuna-7B[[243](https://arxiv.org/html/2501.02341v1#bib.bib243)]和EVA-CLIP[[189](https://arxiv.org/html/2501.02341v1#bib.bib189)]，提取视觉特征并采用分层轨迹生成机制进行高效的自然语言导航。GOMAA-Geo[[2](https://arxiv.org/html/2501.02341v1#bib.bib2)]框架专注于通过将各种LLM与CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]集成，进行多模态主动地理定位任务。它充分利用多模态目标描述（如自然语言、地面图像和空中图像）以及视觉线索，实现高效而精确的目标定位，展示了卓越的零-shot泛化能力。NavAgent[[1](https://arxiv.org/html/2501.02341v1#bib.bib1)]框架整合了LLaMA2[[154](https://arxiv.org/html/2501.02341v1#bib.bib154)]、BLIP-2[[176](https://arxiv.org/html/2501.02341v1#bib.bib176)]、GPT-4[[143](https://arxiv.org/html/2501.02341v1#bib.bib143)]和GLIP[[190](https://arxiv.org/html/2501.02341v1#bib.bib190)]等先进模型。通过解析自然语言导航指令提取地标描述，并利用微调后的地标识别模块实现全景图像中的精确地标定位。该框架在城市户外场景中的路径规划和导航任务中表现出色，为复杂环境中的无人机导航提供了强有力的技术支持。相关研究，如ASMA[[330](https://arxiv.org/html/2501.02341v1#bib.bib330)]、Zhang
    *等人*[[331](https://arxiv.org/html/2501.02341v1#bib.bib331)]和Chen *等人*[[332](https://arxiv.org/html/2501.02341v1#bib.bib332)]，也探索了适用于户外环境的无人机VLN解决方案，值得进一步关注。
- en: Notably, Liu *et al*. [[390](https://arxiv.org/html/2501.02341v1#bib.bib390)]
    proposed the Volumetric Environment Representation (VER), offering an innovative
    perspective for UAV VLN tasks. This approach divides the environment into 3D voxel
    grids, aggregating multi-view 2D visual features into 3D space to generate a unified
    environment representation. Using a multi-task learning framework, the system
    predicts 3D occupancy, room layouts, and object-bounding boxes. VER-based systems
    estimate states through multi-layer Transformer modules, assisted by memory modules
    that store historical observations for global planning. Local and global action
    decision modules execute navigation tasks. This 3D spatial representation approach
    is not only suitable for indoor VLN tasks but also holds the potential for extension
    to outdoor open environments. By segmenting outdoor scenes and applying the same
    voxel modeling methodology with a temporal connection mechanism across segments,
    this approach could further support navigation tasks in complex dynamic environments.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，刘*等人* [[390](https://arxiv.org/html/2501.02341v1#bib.bib390)] 提出了体积环境表示（VER），为无人机VLN任务提供了创新的视角。该方法将环境划分为3D体素网格，将多视角的2D视觉特征聚合到3D空间中，从而生成统一的环境表示。通过多任务学习框架，系统预测3D占用、房间布局和物体边界框。基于VER的系统通过多层Transformer模块估计状态，并借助存储历史观察的记忆模块进行全局规划。局部和全局行动决策模块执行导航任务。该3D空间表示方法不仅适用于室内VLN任务，还具有扩展到户外开放环境的潜力。通过对户外场景进行分割，并应用相同的体素建模方法，结合跨片段的时间连接机制，该方法还可以进一步支持在复杂动态环境中的导航任务。
- en: 5.2.3 VLT
  id: totrans-447
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3 VLT
- en: The VLT task aims to achieve continuous target tracking based on multimodal
    inputs while dynamically adjusting flight paths to address challenges such as
    target occlusion and environmental interference. Currently, VLT tasks typically
    utilize Multimodal Attention Mechanisms to effectively integrate visual and linguistic
    signals. However, similar to VLN and object detection tasks, they still face challenges
    in cross-modal feature alignment, insufficient generalization capabilities, and
    adaptability to dynamic environments [[391](https://arxiv.org/html/2501.02341v1#bib.bib391),
    [392](https://arxiv.org/html/2501.02341v1#bib.bib392)].
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: VLT任务旨在基于多模态输入实现连续目标跟踪，同时动态调整飞行路径，以应对目标遮挡和环境干扰等挑战。目前，VLT任务通常利用多模态注意力机制来有效整合视觉和语言信号。然而，类似于VLN和物体检测任务，它们仍然面临跨模态特征对齐、泛化能力不足以及对动态环境的适应性等挑战[[391](https://arxiv.org/html/2501.02341v1#bib.bib391),
    [392](https://arxiv.org/html/2501.02341v1#bib.bib392)]。
- en: Li *et al*. [[312](https://arxiv.org/html/2501.02341v1#bib.bib312)] introduced
    the UAVNLT dataset and developed a baseline method for UAV natural language tracking
    (TNL) based on it. The visual localization module in this method employs CLIP
    [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)], leveraging its multimodal
    features to precisely locate the target in the first frame. Similar to VLN tasks,
    VLT tasks integrate natural language descriptions with target bounding boxes,
    using natural language as auxiliary information to reduce ambiguities introduced
    by bounding boxes. The natural language descriptions in the TNL system clearly
    specify target attributes, helping the system accurately identify and track targets
    in complex scenarios, thereby effectively addressing tracking challenges in dynamic
    environments. Blei *et al*. [[333](https://arxiv.org/html/2501.02341v1#bib.bib333)]
    proposed CloudTrack, an open-vocabulary target detection and tracking system for
    UAV search and rescue missions. This system adopts a cloud-edge collaborative
    architecture, combining Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    with VLMs to parse semantic descriptions, enabling the detection and filtering
    of complex targets. CloudTrack provides reliable technical support for intelligent
    UAV perception and dynamic task execution in resource-constrained environments,
    showcasing the potential of multimodal technologies in UAV intelligent missions.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Li *et al*. [[312](https://arxiv.org/html/2501.02341v1#bib.bib312)] 引入了UAVNLT数据集，并基于该数据集开发了一种用于无人机自然语言追踪（TNL）的基线方法。该方法中的视觉定位模块采用了CLIP
    [[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]，利用其多模态特性在第一帧中精确定位目标。与VLN任务类似，VLT任务将自然语言描述与目标边界框结合，利用自然语言作为辅助信息，减少边界框引入的歧义。TNL系统中的自然语言描述明确指定了目标属性，帮助系统在复杂场景中准确识别和追踪目标，从而有效应对动态环境中的追踪挑战。Blei
    *et al*. [[333](https://arxiv.org/html/2501.02341v1#bib.bib333)] 提出了CloudTrack，这是一个用于无人机搜索与救援任务的开放词汇目标检测与追踪系统。该系统采用云-边协同架构，将Grounding
    DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)] 与VLMs结合解析语义描述，从而实现复杂目标的检测和过滤。CloudTrack为资源受限环境中的智能无人机感知与动态任务执行提供了可靠的技术支持，展示了多模态技术在无人机智能任务中的潜力。
- en: 5.2.4 Target Search
  id: totrans-450
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4 目标搜索
- en: The target search task integrates multimodal target perception and intelligent
    mission planning, representing a complex high-level autonomous UAV mission. It
    can be viewed as a combination of “Vision-Language Navigation + Object Detection
    + Efficient Path Planning.” Compared to traditional VLN tasks, target search requires
    UAVs to efficiently perceive and locate targets while navigating [[393](https://arxiv.org/html/2501.02341v1#bib.bib393),
    [394](https://arxiv.org/html/2501.02341v1#bib.bib394)].
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 目标搜索任务融合了多模态目标感知和智能任务规划，代表了一项复杂的高级自主无人机任务。它可以被视为“视觉-语言导航 + 目标检测 + 高效路径规划”的组合。与传统的VLN任务相比，目标搜索要求无人机在导航的同时高效感知和定位目标[[393](https://arxiv.org/html/2501.02341v1#bib.bib393),
    [394](https://arxiv.org/html/2501.02341v1#bib.bib394)]。
- en: 'Cai et al.[[334](https://arxiv.org/html/2501.02341v1#bib.bib334)] proposed
    the NEUSIS framework, a neural-symbolic approach for target search tasks in complex
    environments, enabling UAVs to perform autonomous perception, reasoning, and planning
    under uncertainty. The framework comprises three main modules: First, the Perception,
    Localization, and 3D Reasoning Module (GRiD) integrates VFMs and neural-symbolic
    methods, such as HYDRA[[184](https://arxiv.org/html/2501.02341v1#bib.bib184)]
    for dynamic visual reasoning, CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]
    for target attribute classification, Grounding DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    for open-set target localization, and EfficientSAM[[203](https://arxiv.org/html/2501.02341v1#bib.bib203)]
    for efficient instance segmentation, to accomplish tasks like target detection,
    attribute recognition, and 3D projection. Second, the Probabilistic World Model
    Module employs Bayesian filtering and distribution ranking mechanisms to maintain
    probabilistic target maps and 3D environmental representations by fusing noisy
    data, thus supporting dynamic target localization and reliable report generation.
    Finally, the Selection, Navigation, and Coverage Module (SNaC) utilizes high-level
    region selection, mid-level path navigation, and low-level area coverage. Through
    the A^∗ algorithm and belief map-based optimization methods, it generates efficient
    path planning schemes, ensuring the UAV maximizes target search tasks within limited
    time constraints. Döschl *et al*. [[335](https://arxiv.org/html/2501.02341v1#bib.bib335)]
    introduced the Say-REAPEx framework for online mission planning and execution
    in UAV search-and-rescue tasks. This framework uses GPT-4o-mini as the primary
    language model and tests Llama3 [[155](https://arxiv.org/html/2501.02341v1#bib.bib155)],
    Claude3 [[168](https://arxiv.org/html/2501.02341v1#bib.bib168)], and Gemini [[152](https://arxiv.org/html/2501.02341v1#bib.bib152)]
    for parsing natural language mission instructions. It dynamically updates mission
    states using observational data and generates corresponding action plans. The
    framework also employs online heuristic search to optimize UAV mission paths,
    significantly enhancing real-time responsiveness and autonomous decision-making
    in dynamic environments. Say-REAPEx provides efficient and reliable technical
    solutions for complex tasks.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: Cai 等人[[334](https://arxiv.org/html/2501.02341v1#bib.bib334)] 提出了 NEUSIS 框架，一种用于复杂环境中目标搜索任务的神经符号方法，使无人机能够在不确定性下执行自主感知、推理和规划。该框架包括三个主要模块：首先，感知、定位和
    3D 推理模块（GRiD）集成了 VFMs 和神经符号方法，如用于动态视觉推理的 HYDRA[[184](https://arxiv.org/html/2501.02341v1#bib.bib184)]，用于目标属性分类的
    CLIP[[186](https://arxiv.org/html/2501.02341v1#bib.bib186)]，用于开放集目标定位的 Grounding
    DINO[[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]，以及用于高效实例分割的 EfficientSAM[[203](https://arxiv.org/html/2501.02341v1#bib.bib203)]，以完成目标检测、属性识别和
    3D 投影等任务。其次，概率世界模型模块采用贝叶斯滤波和分布排序机制，通过融合噪声数据维持概率目标地图和 3D 环境表示，从而支持动态目标定位和可靠的报告生成。最后，选择、导航和覆盖模块（SNaC）利用高层次区域选择、中层次路径导航和低层次区域覆盖。通过
    A^∗ 算法和基于信念图的优化方法，它生成高效的路径规划方案，确保无人机在有限的时间约束内最大化目标搜索任务。Döschl *等人* [[335](https://arxiv.org/html/2501.02341v1#bib.bib335)]
    引入了 Say-REAPEx 框架，用于无人机搜索与救援任务中的在线任务规划和执行。该框架使用 GPT-4o-mini 作为主要语言模型，并测试了 Llama3
    [[155](https://arxiv.org/html/2501.02341v1#bib.bib155)]、Claude3 [[168](https://arxiv.org/html/2501.02341v1#bib.bib168)]
    和 Gemini [[152](https://arxiv.org/html/2501.02341v1#bib.bib152)] 用于解析自然语言任务指令。它通过观测数据动态更新任务状态，并生成相应的行动计划。该框架还采用在线启发式搜索优化无人机任务路径，显著提高了实时响应能力和在动态环境中的自主决策能力。Say-REAPEx
    为复杂任务提供了高效可靠的技术解决方案。
- en: 5.3 Planning
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 规划
- en: Traditional UAV mission planning algorithms face significant challenges in adaptability
    and coordination in complex dynamic environments. Task planning for multi-UAV
    systems must comprehensively consider the capabilities, limitations, and sensing
    modes of each UAV while satisfying constraints such as energy consumption and
    collision avoidance to achieve efficient mission allocation and path planning
    [[395](https://arxiv.org/html/2501.02341v1#bib.bib395), [396](https://arxiv.org/html/2501.02341v1#bib.bib396)].
    However, despite the new technical approaches provided by deep learning, these
    methods still exhibit limitations, such as heavy reliance on large-scale annotated
    data, insufficient real-time adaptation to environmental dynamics, and limited
    capability to handle unexpected situations or undefined fault modes. Additionally,
    models trained for fixed missions or environments often struggle to generalize
    well to different scenarios [[88](https://arxiv.org/html/2501.02341v1#bib.bib88),
    [397](https://arxiv.org/html/2501.02341v1#bib.bib397), [115](https://arxiv.org/html/2501.02341v1#bib.bib115)].
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的无人机任务规划算法在复杂动态环境中的适应性和协调性面临着巨大挑战。多无人机系统的任务规划必须全面考虑每架无人机的能力、限制和感知模式，同时满足能源消耗和避撞等约束条件，以实现高效的任务分配和路径规划[[395](https://arxiv.org/html/2501.02341v1#bib.bib395),
    [396](https://arxiv.org/html/2501.02341v1#bib.bib396)]。然而，尽管深度学习提供了新的技术方法，这些方法仍然存在一些局限性，如对大规模标注数据的高度依赖、对环境动态的实时适应性不足，以及处理突发情况或未定义故障模式的能力有限。此外，为固定任务或环境训练的模型往往难以很好地泛化到不同的场景[[88](https://arxiv.org/html/2501.02341v1#bib.bib88),
    [397](https://arxiv.org/html/2501.02341v1#bib.bib397), [115](https://arxiv.org/html/2501.02341v1#bib.bib115)]。
- en: LLMs, leveraging the CoT framework [[228](https://arxiv.org/html/2501.02341v1#bib.bib228)],
    can decompose complex missions into a series of clear and executable subtasks,
    thereby providing a well-defined planning path and logical framework. With the
    advantages of in-context learning and few-shot learning, LLMs can flexibly adapt
    to diverse mission requirements and rapidly generate efficient planning strategies
    even without large-scale annotated data [[230](https://arxiv.org/html/2501.02341v1#bib.bib230),
    [231](https://arxiv.org/html/2501.02341v1#bib.bib231)]. Furthermore, LLMs’ outstanding
    performance in natural language understanding and generation enables real-time
    collaboration with operators through language instructions, significantly enhancing
    the intelligence and operational flexibility of mission planning.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs，利用CoT框架[[228](https://arxiv.org/html/2501.02341v1#bib.bib228)]，能够将复杂的任务分解为一系列清晰且可执行的子任务，从而提供明确的规划路径和逻辑框架。凭借上下文学习和少样本学习的优势，LLMs能够灵活适应不同的任务需求，甚至在没有大规模标注数据的情况下，也能快速生成高效的规划策略[[230](https://arxiv.org/html/2501.02341v1#bib.bib230),
    [231](https://arxiv.org/html/2501.02341v1#bib.bib231)]。此外，LLMs在自然语言理解和生成方面的出色表现使其能够通过语言指令与操作员进行实时协作，显著提升任务规划的智能性和操作灵活性。
- en: TypeFly [[5](https://arxiv.org/html/2501.02341v1#bib.bib5)] uses GPT-4 [[143](https://arxiv.org/html/2501.02341v1#bib.bib143)]
    to parse natural language instructions provided by users and generate precise
    mission planning scripts. It also introduces a lightweight mission planning language
    (MiniSpec) to optimize the number of tokens required for mission generation, thereby
    improving mission generation efficiency and response speed. The framework integrates
    a visual encoding module for real-time environmental perception and dynamic mission
    adjustment and includes a “Replan” mechanism to handle environmental changes during
    execution. SPINE [[336](https://arxiv.org/html/2501.02341v1#bib.bib336)], designed
    for mission planning in unstructured environments, combines GPT-4 and semantic
    topological maps to reason and dynamically plan from incomplete natural language
    mission descriptions. The framework employs Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    for object detection, LLaVA [[169](https://arxiv.org/html/2501.02341v1#bib.bib169),
    [170](https://arxiv.org/html/2501.02341v1#bib.bib170)] to enrich semantic information,
    and uses a Receding Horizon Framework to decompose complex missions into executable
    paths, enabling dynamic adjustments and efficient execution. LEVIOSA [[337](https://arxiv.org/html/2501.02341v1#bib.bib337)]
    generates UAV trajectories through natural language, using Gemini [[151](https://arxiv.org/html/2501.02341v1#bib.bib151),
    [152](https://arxiv.org/html/2501.02341v1#bib.bib152)] or GPT-4o to parse user
    text or voice inputs, translating mission requirements into high-level waypoint
    planning. The framework combines reinforcement learning with a multi-critic consensus
    mechanism to optimize trajectories, ensuring that the plans meet safety and energy
    efficiency requirements. It achieves end-to-end automation from natural language
    to 3D UAV trajectories, supporting dynamic environment adaptation and collaborative
    multi-UAV mission execution. Similar studies include TPML [[338](https://arxiv.org/html/2501.02341v1#bib.bib338)],
    REAL [[6](https://arxiv.org/html/2501.02341v1#bib.bib6)], and the work by Liu
    *et al*. [[340](https://arxiv.org/html/2501.02341v1#bib.bib340)], which further
    expands the applications of LLMs in UAV mission planning.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: TypeFly [[5](https://arxiv.org/html/2501.02341v1#bib.bib5)] 使用 GPT-4 [[143](https://arxiv.org/html/2501.02341v1#bib.bib143)]
    来解析用户提供的自然语言指令并生成精确的任务规划脚本。它还引入了一种轻量级任务规划语言（MiniSpec），用于优化任务生成所需的标记数量，从而提高任务生成效率和响应速度。该框架集成了一个视觉编码模块，用于实时环境感知和动态任务调整，并包括一个“重新规划”机制，以应对执行过程中的环境变化。SPINE
    [[336](https://arxiv.org/html/2501.02341v1#bib.bib336)] 旨在为非结构化环境中的任务规划提供支持，结合
    GPT-4 和语义拓扑图，利用不完整的自然语言任务描述进行推理和动态规划。该框架采用 Grounding DINO [[192](https://arxiv.org/html/2501.02341v1#bib.bib192)]
    进行物体检测，使用 LLaVA [[169](https://arxiv.org/html/2501.02341v1#bib.bib169), [170](https://arxiv.org/html/2501.02341v1#bib.bib170)]
    丰富语义信息，并采用退化视野框架（Receding Horizon Framework）将复杂任务分解为可执行路径，实现动态调整和高效执行。LEVIOSA
    [[337](https://arxiv.org/html/2501.02341v1#bib.bib337)] 通过自然语言生成无人机轨迹，使用 Gemini
    [[151](https://arxiv.org/html/2501.02341v1#bib.bib151), [152](https://arxiv.org/html/2501.02341v1#bib.bib152)]
    或 GPT-4o 解析用户的文本或语音输入，将任务需求转换为高级航点规划。该框架结合强化学习与多批评机制，优化轨迹，确保规划满足安全性和能效要求。它实现了从自然语言到
    3D 无人机轨迹的端到端自动化，支持动态环境适应和协同多无人机任务执行。类似的研究包括 TPML [[338](https://arxiv.org/html/2501.02341v1#bib.bib338)],
    REAL [[6](https://arxiv.org/html/2501.02341v1#bib.bib6)] 和 Liu *et al*. [[340](https://arxiv.org/html/2501.02341v1#bib.bib340)]
    的工作，进一步扩展了大规模语言模型（LLM）在无人机任务规划中的应用。
- en: 5.4 Flight Control
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 飞行控制
- en: 'UAV flight control tasks are generally categorized into two types: single-UAV
    flight control and swarm UAV flight control. In single-UAV flight control, imitation
    learning and reinforcement learning methods have gradually become mainstream,
    demonstrating significant potential in enhancing the intelligence of control strategies
    [[398](https://arxiv.org/html/2501.02341v1#bib.bib398), [399](https://arxiv.org/html/2501.02341v1#bib.bib399),
    [400](https://arxiv.org/html/2501.02341v1#bib.bib400)]. However, these methods
    typically rely on large-scale annotated data and face limitations in real-time
    performance and safety. In swarm UAV flight control, techniques such as multi-agent
    reinforcement learning and Graph Neural Networks (GNNs) provide powerful modeling
    capabilities for multi-UAV collaborative tasks, showing advantages in scenarios
    such as formation flying, task allocation, and dynamic obstacle avoidance [[401](https://arxiv.org/html/2501.02341v1#bib.bib401),
    [402](https://arxiv.org/html/2501.02341v1#bib.bib402), [403](https://arxiv.org/html/2501.02341v1#bib.bib403),
    [404](https://arxiv.org/html/2501.02341v1#bib.bib404)]. Nevertheless, these approaches
    still encounter significant challenges in communication delays, computational
    complexity, and global optimization capabilities.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机飞行控制任务通常分为两类：单无人机飞行控制和群体无人机飞行控制。在单无人机飞行控制中，模仿学习和强化学习方法逐渐成为主流，展示了在提升控制策略智能方面的巨大潜力[[398](https://arxiv.org/html/2501.02341v1#bib.bib398),
    [399](https://arxiv.org/html/2501.02341v1#bib.bib399), [400](https://arxiv.org/html/2501.02341v1#bib.bib400)]。然而，这些方法通常依赖大规模标注数据，并在实时性能和安全性方面存在局限性。在群体无人机飞行控制中，多智能体强化学习和图神经网络（GNN）等技术为多无人机协作任务提供了强大的建模能力，在编队飞行、任务分配和动态障碍物避让等场景中展现出优势[[401](https://arxiv.org/html/2501.02341v1#bib.bib401),
    [402](https://arxiv.org/html/2501.02341v1#bib.bib402), [403](https://arxiv.org/html/2501.02341v1#bib.bib403),
    [404](https://arxiv.org/html/2501.02341v1#bib.bib404)]。尽管如此，这些方法仍面临通信延迟、计算复杂性和全局优化能力等方面的重大挑战。
- en: Compared to traditional methods, LLM-based flight control introduces entirely
    new possibilities to the field. Leveraging few-shot learning capabilities, LLMs
    can quickly adapt to new task requirements; their in-context learning abilities
    enable models to dynamically analyze task environments and generate high-level
    flight strategies. Furthermore, semantic-based natural language interaction significantly
    enhances human-machine collaboration efficiency, supporting mission planning,
    real-time decision-making, and complex environment adaptation in UAVs. Although
    this research direction is still in its early exploratory stage, it has already
    shown tremendous potential in task scenarios requiring semantic understanding
    and high-level decision-making.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于传统方法，基于大语言模型（LLM）的飞行控制为该领域带来了全新的可能性。利用少样本学习能力，LLM能够迅速适应新的任务需求；其上下文学习能力使得模型能够动态分析任务环境，并生成高级飞行策略。此外，基于语义的自然语言交互显著提高了人机协作效率，支持无人机的任务规划、实时决策以及复杂环境适应。尽管这一研究方向仍处于早期探索阶段，但在需要语义理解和高层决策的任务场景中，已展现出巨大的潜力。
- en: In the domain of single-UAV flight control, early studies laid an important
    foundation for applying LLMs to this task. For example, Courbon *et al*. [[405](https://arxiv.org/html/2501.02341v1#bib.bib405)]
    proposed a vision-based navigation strategy that uses a monocular camera to observe
    natural landmarks, building a visual memory and enabling autonomous navigation
    in unknown environments by matching current visual images with pre-recorded keyframes.
    Vemprala *et al*. [[341](https://arxiv.org/html/2501.02341v1#bib.bib341)] developed
    the PromptCraft platform, a pioneering work applying LLMs to UAV flight control.
    This platform integrates ChatGPT with the Microsoft AirSim [[305](https://arxiv.org/html/2501.02341v1#bib.bib305)]
    simulation environment. By designing flight control-specific prompts and combining
    the ChatGPT API with the AirSim API, it enables natural language-driven flight
    control. Prompt design plays a critical role in this process, directly impacting
    the accuracy of task understanding and instruction generation. Similar studies
    include explorations by Zhong *et al*. [[342](https://arxiv.org/html/2501.02341v1#bib.bib342)],
    Tazir *et al*. [[343](https://arxiv.org/html/2501.02341v1#bib.bib343)], and Phadke
    *et al*.[[344](https://arxiv.org/html/2501.02341v1#bib.bib344)], as well as the
    development of frameworks like EAI-SIM [[345](https://arxiv.org/html/2501.02341v1#bib.bib345)]
    and TAIiST [[346](https://arxiv.org/html/2501.02341v1#bib.bib346)].
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 在单无人机飞行控制领域，早期的研究为将LLMs应用于该任务奠定了重要基础。例如，Courbon *等人* [[405](https://arxiv.org/html/2501.02341v1#bib.bib405)]
    提出了基于视觉的导航策略，利用单目摄像头观察自然地标，建立视觉记忆，通过将当前视觉图像与预录制的关键帧匹配，实现未知环境中的自主导航。Vemprala *等人*
    [[341](https://arxiv.org/html/2501.02341v1#bib.bib341)] 开发了PromptCraft平台，这是一个将LLMs应用于无人机飞行控制的开创性工作。该平台将ChatGPT与Microsoft
    AirSim [[305](https://arxiv.org/html/2501.02341v1#bib.bib305)] 仿真环境集成，通过设计特定于飞行控制的提示，并将ChatGPT
    API与AirSim API结合，实现自然语言驱动的飞行控制。提示设计在这一过程中起着关键作用，直接影响任务理解和指令生成的准确性。类似的研究还包括Zhong
    *等人* [[342](https://arxiv.org/html/2501.02341v1#bib.bib342)]、Tazir *等人* [[343](https://arxiv.org/html/2501.02341v1#bib.bib343)]
    和Phadke *等人* [[344](https://arxiv.org/html/2501.02341v1#bib.bib344)] 的探索，以及像EAI-SIM
    [[345](https://arxiv.org/html/2501.02341v1#bib.bib345)] 和TAIiST [[346](https://arxiv.org/html/2501.02341v1#bib.bib346)]
    这样的框架的开发。
- en: In the domain of swarm UAV flight control, Jiao *et al*. [[347](https://arxiv.org/html/2501.02341v1#bib.bib347)]
    proposed the Swarm-GPT system, which combines LLMs with model-based safe motion
    planning to build an innovative framework for swarm UAV flight control. This system
    uses GPT-3.5 [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] to generate
    time-series waypoints for UAVs and optimizes the paths through a safety planning
    module to satisfy physical constraints and collision avoidance requirements. Swarm-GPT
    allows users to dynamically modify flight paths through re-prompting, enabling
    flexible formation and dynamic adjustment of UAV swarms. Additionally, the system
    demonstrated the safety of trajectory planning and the artistic effects of formation
    performances in simulation environments. Similar research includes FlockGPT [[348](https://arxiv.org/html/2501.02341v1#bib.bib348)]
    and CLIPSwarm [[349](https://arxiv.org/html/2501.02341v1#bib.bib349)], which explore
    automated and creative control schemes to enhance the efficiency and operability
    of UAV swarm performances.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在群体无人机飞行控制领域，焦*等人*[[347](https://arxiv.org/html/2501.02341v1#bib.bib347)] 提出了Swarm-GPT系统，该系统将大型语言模型（LLMs）与基于模型的安全运动规划相结合，构建了一个创新的群体无人机飞行控制框架。该系统使用GPT-3.5
    [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] 为无人机生成时间序列航路点，并通过安全规划模块优化路径，以满足物理约束和避障要求。Swarm-GPT允许用户通过重新提示动态修改飞行路径，从而实现无人机群体的灵活编队和动态调整。此外，该系统在仿真环境中展示了轨迹规划的安全性和编队表演的艺术效果。类似的研究包括FlockGPT
    [[348](https://arxiv.org/html/2501.02341v1#bib.bib348)] 和CLIPSwarm [[349](https://arxiv.org/html/2501.02341v1#bib.bib349)]，这些研究探索了自动化和创造性的控制方案，以提高无人机群体表演的效率和可操作性。
- en: 5.5 Infrastructures
  id: totrans-462
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 基础设施
- en: The construction and processing of datasets are particularly critical in the
    foundational research of UAV systems. High-quality data resources and well-established
    data processing workflows are essential to ensuring the efficient application
    of LLM, VLM, and VFM technologies in UAV tasks. These research efforts not only
    lay a solid foundation for the application of UAVs in multimodal tasks but also
    provide strong support for technological innovation and methodological advancements
    in related fields.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的构建和处理在无人机系统的基础研究中尤为关键。高质量的数据资源和完善的数据处理工作流对于确保LLM、VLM和VFM技术在无人机任务中的高效应用至关重要。这些研究工作不仅为无人机在多模态任务中的应用奠定了坚实的基础，也为相关领域的技术创新和方法论进步提供了有力支持。
- en: 'DTLLM-VLT [[350](https://arxiv.org/html/2501.02341v1#bib.bib350)] is a framework
    designed to enhance VLT performance through multi-granularity text generation.
    The framework uses SAM [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)]
    to extract segmentation masks of targets, combined with Osprey [[208](https://arxiv.org/html/2501.02341v1#bib.bib208)]
    to generate initial visual descriptions. LLaMA [[153](https://arxiv.org/html/2501.02341v1#bib.bib153),
    [154](https://arxiv.org/html/2501.02341v1#bib.bib154), [155](https://arxiv.org/html/2501.02341v1#bib.bib155)]
    or Vicuna [[156](https://arxiv.org/html/2501.02341v1#bib.bib156)] then generates
    four types of granular text annotations: initial brief descriptions, initial detailed
    descriptions, dense brief descriptions, and dense detailed descriptions, covering
    target categories, colors, actions, and dynamic changes. These high-quality text
    data significantly enhance semantic support for multimodal tasks, improving tracking
    accuracy and robustness while reducing the time and cost of semantic annotation.
    Yao *et al*. [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)] developed
    the CNER-UAV dataset for fine-grained Chinese Named Entity Recognition in UAV
    delivery systems, leveraging GPT-3.5 [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)]
    and ChatGLM [[161](https://arxiv.org/html/2501.02341v1#bib.bib161), [162](https://arxiv.org/html/2501.02341v1#bib.bib162),
    [163](https://arxiv.org/html/2501.02341v1#bib.bib163)] to achieve precise address
    information recognition.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: DTLLM-VLT [[350](https://arxiv.org/html/2501.02341v1#bib.bib350)] 是一个框架，旨在通过多粒度文本生成提升VLT性能。该框架使用SAM
    [[198](https://arxiv.org/html/2501.02341v1#bib.bib198)] 提取目标的分割掩码，并结合Osprey [[208](https://arxiv.org/html/2501.02341v1#bib.bib208)]
    生成初步的视觉描述。接着，LLaMA [[153](https://arxiv.org/html/2501.02341v1#bib.bib153), [154](https://arxiv.org/html/2501.02341v1#bib.bib154),
    [155](https://arxiv.org/html/2501.02341v1#bib.bib155)] 或 Vicuna [[156](https://arxiv.org/html/2501.02341v1#bib.bib156)]
    生成四种类型的粒度文本注释：初步简短描述、初步详细描述、密集简短描述和密集详细描述，涵盖目标类别、颜色、动作和动态变化。这些高质量的文本数据显著增强了多模态任务的语义支持，提高了跟踪精度和鲁棒性，同时减少了语义标注的时间和成本。Yao
    *et al*. [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)] 开发了CNER-UAV数据集，用于无人机配送系统中的细粒度中文命名实体识别，利用GPT-3.5
    [[141](https://arxiv.org/html/2501.02341v1#bib.bib141)] 和ChatGLM [[161](https://arxiv.org/html/2501.02341v1#bib.bib161),
    [162](https://arxiv.org/html/2501.02341v1#bib.bib162), [163](https://arxiv.org/html/2501.02341v1#bib.bib163)]
    实现精确的地址信息识别。
- en: 'A noteworthy challenge in UAV systems is the high cost and labor-intensive
    effort of acquiring aerial imagery. To address this, Arrabi *et al*. [[351](https://arxiv.org/html/2501.02341v1#bib.bib351)]
    proposed the GPG2A model, which synthesizes aerial imagery from ground images
    using Ground-to-Aerial (G2A) techniques, overcoming the generation challenges
    posed by significant viewpoint differences. The model employs a two-stage generation
    framework: the first stage uses ConvNeXt-B[[406](https://arxiv.org/html/2501.02341v1#bib.bib406)]
    to extract ground image features and applies the polar coordinate transformation
    to generate bird’s-eye view (BEV) layout maps for capturing scene geometry explicitly.
    The second stage introduces a diffusion model to generate high-quality aerial
    imagery by combining BEV layout maps with textual descriptions. The textual descriptions
    are generated by Gemini [[151](https://arxiv.org/html/2501.02341v1#bib.bib151),
    [152](https://arxiv.org/html/2501.02341v1#bib.bib152)] and optimized into Dynamic
    Text Prompts using BERT [[407](https://arxiv.org/html/2501.02341v1#bib.bib407)],
    enhancing the semantic relevance and scene consistency of the generated imagery.
    This approach effectively addresses the challenges of viewpoint transformation
    and provides an innovative solution for efficiently acquiring aerial imagery,
    offering significant practical value.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机系统中的一个显著挑战是获取空中影像的高成本和劳动密集型工作。为了解决这个问题，Arrabi *等人* [[351](https://arxiv.org/html/2501.02341v1#bib.bib351)]
    提出了GPG2A模型，该模型利用地面到空中的（G2A）技术从地面影像合成空中影像，克服了由于视角差异较大而带来的生成挑战。该模型采用了两阶段生成框架：第一阶段使用ConvNeXt-B[[406](https://arxiv.org/html/2501.02341v1#bib.bib406)]提取地面影像特征，并应用极坐标变换生成鸟瞰图（BEV）布局图，从而显式捕获场景几何信息。第二阶段引入扩散模型，通过将BEV布局图与文本描述相结合，生成高质量的空中影像。文本描述由Gemini
    [[151](https://arxiv.org/html/2501.02341v1#bib.bib151), [152](https://arxiv.org/html/2501.02341v1#bib.bib152)]生成，并通过BERT
    [[407](https://arxiv.org/html/2501.02341v1#bib.bib407)]优化为动态文本提示，从而增强生成影像的语义相关性和场景一致性。这种方法有效解决了视角转换的挑战，为高效获取空中影像提供了一种创新解决方案，具有重要的实际价值。
- en: 'In terms of frameworks and platforms, related research demonstrates diverse
    development directions. Yao *et al*. [[352](https://arxiv.org/html/2501.02341v1#bib.bib352)]
    proposed AeroVerse, a highly referential platform designed as an aviation intelligence
    benchmark suite for UAV agents. AeroVerse integrates simulators, datasets, task
    definitions, and evaluation methodologies to advance UAV technologies in perception,
    cognition, planning, and decision-making. Its system architecture includes a high-precision
    simulation platform, AeroSimulator, based on Unreal Engine and AirSim. AeroSimulator
    generates multimodal datasets spanning real and virtual scenes and provides fine-tuned
    datasets customized for five core tasks: scene perception, spatial reasoning,
    navigation exploration, mission planning, and motion decision-making.'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在框架和平台方面，相关研究展示了多样化的发展方向。Yao *等人* [[352](https://arxiv.org/html/2501.02341v1#bib.bib352)]
    提出了AeroVerse，一个高度参考的平台，旨在作为无人机智能基准套件。AeroVerse整合了模拟器、数据集、任务定义和评估方法，推动无人机在感知、认知、规划和决策等技术的发展。其系统架构包括基于Unreal
    Engine和AirSim的高精度仿真平台AeroSimulator。AeroSimulator生成跨越真实和虚拟场景的多模态数据集，并提供为五个核心任务量身定制的精细调整数据集：场景感知、空间推理、导航探索、任务规划和运动决策。
- en: Additionally, several innovative frameworks combine LLMs with UAV-specific tasks.
    For example, Tang *et al*. [[353](https://arxiv.org/html/2501.02341v1#bib.bib353)]
    developed a safety assessment framework for UAV control; Xu *et al*. [[354](https://arxiv.org/html/2501.02341v1#bib.bib354)]
    designed an emergency communication network optimization framework for UAV deployment
    in dynamic environments; LLM-RS [[355](https://arxiv.org/html/2501.02341v1#bib.bib355)]
    focuses on UAV air combat simulation tasks, incorporating reward design and decision
    optimization to enhance system performance; Pineli *et al*. [[356](https://arxiv.org/html/2501.02341v1#bib.bib356)]
    proposed a UAV voice control framework, leveraging natural language processing
    technologies to maximize the potential of human-machine interaction. These works
    contribute to the development of UAV technologies from various dimensions, forming
    essential support for UAV intelligence and task diversification.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些创新框架将LLMs与无人机特定任务相结合。例如，Tang *等人* [[353](https://arxiv.org/html/2501.02341v1#bib.bib353)]
    开发了一个无人机控制的安全评估框架；Xu *等人* [[354](https://arxiv.org/html/2501.02341v1#bib.bib354)]
    设计了一个应急通信网络优化框架，用于动态环境中无人机的部署；LLM-RS [[355](https://arxiv.org/html/2501.02341v1#bib.bib355)]
    专注于无人机空战仿真任务，结合奖励设计和决策优化以提升系统性能；Pineli *等人* [[356](https://arxiv.org/html/2501.02341v1#bib.bib356)]
    提出了一个无人机语音控制框架，利用自然语言处理技术最大化人机交互的潜力。这些工作从各个维度推动了无人机技术的发展，为无人机的智能化和任务多样化提供了重要支持。
- en: 6 Application scenarios of FMs-based UAVs
  id: totrans-468
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 基于FM的无人机应用场景
- en: '![Refer to caption](img/7e3d2d95d19214c1a10fedd76c879e36.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/7e3d2d95d19214c1a10fedd76c879e36.png)'
- en: 'Figure 5: Typical applications on the integration of UAVs and FMs.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：无人机与FM集成的典型应用。
- en: This section focuses on the practical application scenarios of combining UAVs
    with LLMs. LLMs provide advanced cognitive and analytical capabilities for multimodal
    data, including image, audio, text, and even video data. Compared to UAVs integrated
    with traditional machine learning algorithms, incorporating LLMs into UAV systems
    significantly enhances their environmental perception capabilities [[408](https://arxiv.org/html/2501.02341v1#bib.bib408),
    [409](https://arxiv.org/html/2501.02341v1#bib.bib409)], enables smarter decision-making
    processes [[410](https://arxiv.org/html/2501.02341v1#bib.bib410)], and improves
    user experience by leveraging the strong comprehension abilities of LLMs in human-machine
    interaction [[344](https://arxiv.org/html/2501.02341v1#bib.bib344), [411](https://arxiv.org/html/2501.02341v1#bib.bib411)].
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 本节重点介绍将无人机与大型语言模型（LLMs）结合的实际应用场景。LLMs为多模态数据提供了先进的认知和分析能力，包括图像、音频、文本甚至视频数据。与传统机器学习算法集成的无人机相比，将LLMs融入无人机系统显著增强了其环境感知能力[[408](https://arxiv.org/html/2501.02341v1#bib.bib408)、[409](https://arxiv.org/html/2501.02341v1#bib.bib409)]，使决策过程更加智能[[410](https://arxiv.org/html/2501.02341v1#bib.bib410)]，并通过利用LLMs在人与机器交互中的强大理解能力，改善了用户体验[[344](https://arxiv.org/html/2501.02341v1#bib.bib344)、[411](https://arxiv.org/html/2501.02341v1#bib.bib411)]。
- en: 'Based on existing literature, we introduce typical works on the integration
    of FMs with UAVs as illustrated in Figure [5](https://arxiv.org/html/2501.02341v1#S6.F5
    "Figure 5 ‣ 6 Application scenarios of FMs-based UAVs ‣ UAVs Meet LLMs: Overviews
    and Perspectives Toward Agentic Low-Altitude Mobility"): Surveillance, Logistics
    and Emergency Response. These three categories presented are not exhaustive of
    all UAV applications but rather represent the current areas where the combination
    of UAV technology and advanced model capabilities has been particularly effective.
    They focus on improving three key capabilities: environmental perception, autonomous
    decision-making, and human-machine interaction.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 基于现有文献，我们介绍了图[5](https://arxiv.org/html/2501.02341v1#S6.F5 "图5 ‣ 6 基于FM的无人机应用场景
    ‣ 无人机与LLMs结合：代理性低空移动的概述与展望")中展示的无人机与FM集成的典型研究：监控、物流和应急响应。这三个类别并非涵盖所有无人机应用，而是代表了无人机技术与先进模型能力结合特别有效的当前领域。它们着重提升三个关键能力：环境感知、自动决策和人机交互。
- en: 6.1 Surveillance
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 监控
- en: For surveillance, UAVs are used for monitoring traffic scenarios, urban environments,
    and other regulatory tasks. Traditional methods for addressing UAV applications
    in monitoring tasks primarily rely on machine learning techniques. In recent years,
    substantial research has been conducted in this area, including vehicle trajectory
    monitoring [[412](https://arxiv.org/html/2501.02341v1#bib.bib412)], road condition
    monitoring [[413](https://arxiv.org/html/2501.02341v1#bib.bib413), [414](https://arxiv.org/html/2501.02341v1#bib.bib414)],
    road-side units (RSUs) communication [[415](https://arxiv.org/html/2501.02341v1#bib.bib415)],
    and applications and management in urban scenarios [[416](https://arxiv.org/html/2501.02341v1#bib.bib416)].
    However, H. Menouar *et al*. [[417](https://arxiv.org/html/2501.02341v1#bib.bib417)]
    pointed out that UAVs are expected to play a significant role in Intelligent Transportation
    Systems (ITS) and smart cities, but their effectiveness will depend on greater
    autonomy and automation. Similarly, Wang L *et al*. [[418](https://arxiv.org/html/2501.02341v1#bib.bib418)]
    emphasized the importance of UAVs in urban management and highlighted challenges
    such as automation and human-machine interaction. The emergence of FMs has recently
    led to research exploring how the integration of FMs with UAVs can enhance their
    usability and task performance.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在监视方面，UAV（无人机）被用于监控交通状况、城市环境和其他监管任务。传统的无人机监控应用方法主要依赖于机器学习技术。近年来，围绕这一领域进行了大量研究，包括车辆轨迹监控[[412](https://arxiv.org/html/2501.02341v1#bib.bib412)]、道路状况监控[[413](https://arxiv.org/html/2501.02341v1#bib.bib413)、[414](https://arxiv.org/html/2501.02341v1#bib.bib414)]、路边单元（RSUs）通信[[415](https://arxiv.org/html/2501.02341v1#bib.bib415)]以及城市场景中的应用与管理[[416](https://arxiv.org/html/2501.02341v1#bib.bib416)]。然而，H.
    Menouar *et al*.[[417](https://arxiv.org/html/2501.02341v1#bib.bib417)]指出，无人机预计将在智能交通系统（ITS）和智慧城市中发挥重要作用，但其有效性将取决于更高的自主性和自动化程度。同样，Wang
    L *et al*.[[418](https://arxiv.org/html/2501.02341v1#bib.bib418)]强调了无人机在城市管理中的重要性，并突出了诸如自动化和人机交互等挑战。最近，FM（功能模型）的出现促使研究者探索将FM与无人机结合如何提升其可用性和任务性能。
- en: In urban scenario monitoring, Yao J *et al*. [[419](https://arxiv.org/html/2501.02341v1#bib.bib419)]
    deployed VLMs rapidly for monitoring of the conditions of traffic signs using
    multimodal learning and large-scale pre-trained networks, achieving excellent
    results in both accuracy and cost efficiency. UAVs integrated with FMs excel in
    tasks such as vehicle detection, vehicle classification, pedestrian detection,
    cyclist detection, speed estimation, and vehicle counting. Yuan Z *et al*. [[420](https://arxiv.org/html/2501.02341v1#bib.bib420)]
    proposed the “Patrol Agent,” which leverages VLMs for visual information acquisition
    and LLMs for analysis and decision-making. This enables UAVs to autonomously conduct
    urban patrolling, identification, and tracking tasks. Additionally, UAVs integrated
    with LLMs have demonstrated outstanding performance in other monitoring tasks.
    In agricultural applications, Zhu H *et al*. [[421](https://arxiv.org/html/2501.02341v1#bib.bib421)]
    proposed using LLMs and VLMs to help farmers improve productivity and yields.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在城市场景监控中，Yao J *et al*.[[419](https://arxiv.org/html/2501.02341v1#bib.bib419)]通过使用多模态学习和大规模预训练网络，迅速部署了VLMs（视觉语言模型）来监控交通标志的状况，在准确性和成本效率方面都取得了优异的成绩。集成FM的无人机在诸如车辆检测、车辆分类、行人检测、骑行者检测、速度估计和车辆计数等任务中表现出色。Yuan
    Z *et al*.[[420](https://arxiv.org/html/2501.02341v1#bib.bib420)]提出了“巡逻代理”，该系统利用VLMs进行视觉信息采集，使用LLMs（大型语言模型）进行分析和决策，从而使无人机能够自主进行城市巡逻、识别和追踪任务。此外，集成LLMs的无人机在其他监控任务中也展现了卓越的性能。在农业应用中，Zhu
    H *et al*.[[421](https://arxiv.org/html/2501.02341v1#bib.bib421)]提出了使用LLMs和VLMs帮助农民提高生产力和产量的方案。
- en: 6.2 Logistics
  id: totrans-476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 物流
- en: 'For logistics, UAVs enable intelligent processes throughout the entire logistics
    chain, from decision-making to route planning and final delivery [[422](https://arxiv.org/html/2501.02341v1#bib.bib422)].
    The application of UAV in logistics and delivery is a key area of current research.
    Jiang H *et al*. [[423](https://arxiv.org/html/2501.02341v1#bib.bib423)] optimized
    UAV scheduling and route planning using advanced optimization algorithms. Huang
    H *et al*. [[424](https://arxiv.org/html/2501.02341v1#bib.bib424)] proposed a
    collaborative scheduling solution involving UAVs and public transportation systems,
    such as trams, which was proven to be NP-complete. They also introduced a precise
    algorithm based on dynamic programming to address this challenge. However, UAV
    logistics still faces several challenges. Wandelt S *et al*. [[425](https://arxiv.org/html/2501.02341v1#bib.bib425)]
    identified two primary issues: autonomous navigation and human-machine interaction,
    as well as real-time data analysis. The introduction of FMs provides a novel approach
    to addressing these challenges, offering the potential to enhance UAVs’ real-time
    decision-making and planning capabilities through FMs’ reasoning and decision-making
    power. Additionally, FMs’ strong comprehension capabilities improve human-machine
    interaction, providing a better user experience.'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在物流领域，无人机（UAV）使得整个物流链中的智能化流程成为可能，从决策到路线规划，再到最终交付[[422](https://arxiv.org/html/2501.02341v1#bib.bib422)]。无人机在物流和配送中的应用是当前研究的一个关键领域。蒋H
    *等人*[[423](https://arxiv.org/html/2501.02341v1#bib.bib423)]使用先进的优化算法优化了无人机调度和路线规划。黄H
    *等人*[[424](https://arxiv.org/html/2501.02341v1#bib.bib424)]提出了一种协作调度方案，涉及无人机和公共交通系统，如电车，并证明了该问题是NP完全的。他们还引入了一种基于动态规划的精确算法来解决这一挑战。然而，无人机物流仍面临着一些挑战。Wandelt
    S *等人*[[425](https://arxiv.org/html/2501.02341v1#bib.bib425)]识别出两个主要问题：自主导航与人机交互，以及实时数据分析。引入FM（功能模型）为解决这些挑战提供了一种新颖的方式，利用FM的推理和决策能力，有望增强无人机的实时决策和规划能力。此外，FM强大的理解能力改善了人机交互，提供了更好的用户体验。
- en: For logistic applications with FMs, Tagliabue A *et al*. [[6](https://arxiv.org/html/2501.02341v1#bib.bib6)]
    proposed a framework called REAL, leveraging prior knowledge from LLMs and employing
    zero-shot prompting. This method significantly improved UAV adaptability and decision-making,
    improving positional control performance and real-time task decision-making. Luo
    S *et al*. [[426](https://arxiv.org/html/2501.02341v1#bib.bib426)] utilized LLMs
    to process user-provided address information. As traditional methods struggle
    with fine-grained handling due to the lack of precision in user inputs, they fine-tuned
    LLMs to address this issue, thereby increasing the automation level and processing
    efficiency of UAV delivery systems. Zhong J *et al*. [[342](https://arxiv.org/html/2501.02341v1#bib.bib342)]
    focused on autonomous UAV planning and proposed a vision-based planning system
    integrated with LLMs. Their system combines dynamic obstacle tracking and trajectory
    prediction to achieve efficient and reliable autonomous flight. Additionally,
    integrating LLMs enhanced human-machine interaction, improving the overall user
    experience. Dong C *et al*. [[427](https://arxiv.org/html/2501.02341v1#bib.bib427)]
    approached the problem from a supply chain perspective, presenting an innovative
    intelligent delivery system for UAV logistics. By incorporating blockchain technology,
    they ensured system security and transparency. Furthermore, they utilized LLMs
    for route optimization and dynamic task management, and provided customer support
    services through natural language interaction, offering a framework for developing
    secure and efficient UAV delivery systems in the future.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 对于带有FM的物流应用，Tagliabue A *等人* [[6](https://arxiv.org/html/2501.02341v1#bib.bib6)]
    提出了一个名为REAL的框架，利用LLM的先验知识并采用零-shot提示。这种方法显著提高了无人机的适应性和决策能力，改善了位置控制性能和实时任务决策。Luo
    S *等人* [[426](https://arxiv.org/html/2501.02341v1#bib.bib426)] 利用LLM处理用户提供的地址信息。由于传统方法因缺乏精确的用户输入而难以进行细粒度处理，他们对LLM进行了微调以解决这一问题，从而提高了无人机配送系统的自动化程度和处理效率。Zhong
    J *等人* [[342](https://arxiv.org/html/2501.02341v1#bib.bib342)] 关注于自主无人机规划，提出了一种基于视觉的规划系统，结合了LLM。该系统结合了动态障碍物追踪和轨迹预测，实现了高效且可靠的自主飞行。此外，集成LLM还增强了人机交互，提升了整体用户体验。Dong
    C *等人* [[427](https://arxiv.org/html/2501.02341v1#bib.bib427)] 从供应链角度出发，提出了一个创新的无人机物流智能配送系统。通过整合区块链技术，他们确保了系统的安全性和透明性。此外，他们还利用LLM进行路径优化和动态任务管理，并通过自然语言交互提供客户支持服务，提供了未来开发安全高效无人机配送系统的框架。
- en: 6.3 Emergency Response
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 紧急响应
- en: UAVs possess inherent advantages in emergency response and disaster relief missions[[69](https://arxiv.org/html/2501.02341v1#bib.bib69)].
    Their highly flexible operational capabilities make them suitable for most emergency
    scenarios. Jin W *et al*. [[428](https://arxiv.org/html/2501.02341v1#bib.bib428)]
    analyzed the demands for UAV-based emergency response mechanisms, evaluating disaster
    types and the performance characteristics of UAVs, while providing recommendations.
    By equipping UAVs with different payloads and supplies, they can deliver customized
    support based on specific disaster scenarios and mission requirements. Goecks
    V G *et al*. [[429](https://arxiv.org/html/2501.02341v1#bib.bib429)] introduced
    DisasterResponse GPT, an LLM-based model that leverages contextual learning to
    accelerate disaster response by generating actionable plans and rapidly updating
    and adjusting them in real-time, enabling fast decision-making. De Curtò J *et
    al*. [[408](https://arxiv.org/html/2501.02341v1#bib.bib408)] capitalized on UAVs’
    ability to provide instantaneous visual feedback and high data throughput, developing
    a scene understanding model that combines LLMs and VLMs. This approach cost-effectively
    enhances UAVs’ real-time decision-making capabilities for handling complex and
    dynamic data. Furthermore, they integrated multiple sensors to autonomously execute
    complex tasks.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机在紧急响应和灾难救援任务中具有固有的优势[[69](https://arxiv.org/html/2501.02341v1#bib.bib69)]。它们高度灵活的操作能力使其适用于大多数紧急场景。金伟*等人*
    [[428](https://arxiv.org/html/2501.02341v1#bib.bib428)] 分析了基于无人机的紧急响应机制需求，评估了灾难类型和无人机的性能特点，并提出了建议。通过为无人机配备不同的有效载荷和物资，可以根据特定的灾难场景和任务需求提供定制化支持。戈克斯
    V G *等人* [[429](https://arxiv.org/html/2501.02341v1#bib.bib429)] 引入了DisasterResponse
    GPT，这是一个基于大语言模型的模型，通过上下文学习加速灾难响应，生成可操作的计划，并实时更新和调整这些计划，从而实现快速决策。德·库尔托 J *等人* [[408](https://arxiv.org/html/2501.02341v1#bib.bib408)]
    利用无人机提供即时视觉反馈和高数据吞吐量的能力，开发了一种将大语言模型和视觉语言模型相结合的场景理解模型。这种方法以成本效益高的方式增强了无人机在处理复杂动态数据时的实时决策能力。此外，他们还集成了多个传感器，以自动执行复杂任务。
- en: Beyond rescue missions, UAVs are increasingly studied as tools for establishing
    communication networks in response to connectivity challenges in disaster-stricken
    or remote areas. Such networks support network-dependent tasks and offline emergency
    responses. Fourati F *et al*. [[430](https://arxiv.org/html/2501.02341v1#bib.bib430)]
    highlighted the critical role of AI in communication engineering, including applications
    such as flow prediction and channel modeling. Xu Y *et al*. [[354](https://arxiv.org/html/2501.02341v1#bib.bib354)]
    utilized UAVs as mobile access points to assist urban communication systems with
    emergency network deployment in disaster scenarios. They further employed LLMs
    to enhance the modeling process and accelerate optimization workflows. Wang Y
    *et al*. [[431](https://arxiv.org/html/2501.02341v1#bib.bib431)] optimized UAV
    swarm deployment using structured prompts with LLMs. Compared to traditional methods,
    their approach reduces the number of iterations while ensuring strong network
    connectivity and quality of service through precise UAV positioning. The LLM-driven
    framework simplifies operational challenges for UAV network operators, paving
    the way for their application in more complex real-world scenarios.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 除了救援任务外，无人机在应对灾区或偏远地区连接性挑战时，越来越多地被研究作为建立通信网络的工具。这些网络支持依赖网络的任务和离线应急响应。弗拉蒂 F *等人*
    [[430](https://arxiv.org/html/2501.02341v1#bib.bib430)] 强调了人工智能在通信工程中的关键作用，包括流量预测和信道建模等应用。徐勇
    *等人* [[354](https://arxiv.org/html/2501.02341v1#bib.bib354)] 利用无人机作为移动接入点，帮助城市通信系统在灾难场景中进行紧急网络部署。他们进一步采用了大语言模型来增强建模过程并加速优化工作流程。王瑜
    *等人* [[431](https://arxiv.org/html/2501.02341v1#bib.bib431)] 使用结构化提示与大语言模型优化了无人机群部署。与传统方法相比，他们的方法减少了迭代次数，同时通过精确的无人机定位确保了强大的网络连接性和服务质量。该基于大语言模型的框架简化了无人机网络操作员的操作挑战，为其在更复杂的现实世界场景中的应用铺平了道路。
- en: '7 Agentic UAV: The General Pipeline Integrating FMs with UAV Systems'
  id: totrans-482
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 代理无人机：将大语言模型与无人机系统整合的一般流程
- en: '![Refer to caption](img/fd19ee95ecd6fce3004bc57fd0bf82b9.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fd19ee95ecd6fce3004bc57fd0bf82b9.png)'
- en: 'Figure 6: The framework of Agentic UAV.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：代理无人机框架。
- en: 'This section systematically explores the integration of LLMs and VLMs into
    traditional UAV pipelines and tasks. From the perspective of AI agents, we propose
    the framework of Agentic UAV that combines FMs with UAV systems, as illustrated
    in Figure [6](https://arxiv.org/html/2501.02341v1#S7.F6 "Figure 6 ‣ 7 Agentic
    UAV: The General Pipeline Integrating FMs with UAV Systems ‣ UAVs Meet LLMs: Overviews
    and Perspectives Toward Agentic Low-Altitude Mobility"). The framework comprises
    five key components: data module, knowledge module, tools module, FM module, and
    agent module. The data module focuses on creating new datasets or adapting existing
    data to formats suitable for fine-tuning and training FMs tailored to UAV-specific
    tasks. The knowledge module stores domain-specific information, such as airspace
    regulations and scenario libraries, essential for UAV operations. The tools module
    includes domain-specific tools or APIs required to address UAV tasks, thereby
    extending the agent’s problem-solving capabilities. The FM module concentrates
    on fine-tuning foundation models to enhance their adaptation and performance in
    UAV-related domains. The agent module is designed to create workflows incorporating
    perception, planning, and action for UAV tasks. This module also establishes reflective
    mechanisms to optimize processes based on feedback from task execution. Additionally,
    considering the frequent use of UAV swarm, the agent module integrates multi-agent
    designs, interaction, and communication units. To coordinate and manage these
    agents, the framework introduces a manager agent responsible for global task planning
    and allocation. Each of these modules is elaborated upon in the following subsections.'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '本节系统地探讨了大规模语言模型（LLMs）和视觉语言模型（VLMs）在传统无人机流程和任务中的集成。从AI代理的角度，我们提出了Agentic UAV框架，结合了基础模型与无人机系统，如图[6](https://arxiv.org/html/2501.02341v1#S7.F6
    "图 6 ‣ 7 Agentic UAV: 将基础模型与无人机系统整合的通用流程 ‣ 无人机与大规模语言模型的碰撞：代理低空移动的概述与前景")所示。该框架包括五个关键组件：数据模块、知识模块、工具模块、基础模型模块和代理模块。数据模块专注于创建新数据集或将现有数据适配为适合微调和训练基础模型的格式，以应对无人机特定任务。知识模块存储领域特定的信息，如空域管理规定和场景库，这对无人机操作至关重要。工具模块包括解决无人机任务所需的领域特定工具或API，从而扩展了代理的解决问题能力。基础模型模块专注于微调基础模型，以提高它们在无人机相关领域中的适应性和性能。代理模块设计用于创建包含感知、规划和行动的无人机任务工作流程。该模块还建立了反思机制，通过任务执行反馈来优化流程。此外，考虑到无人机群的频繁使用，代理模块集成了多代理设计、交互和通信单元。为了协调和管理这些代理，框架引入了一个经理代理，负责全球任务规划和分配。以下小节将详细阐述每个模块。'
- en: 7.1 Data Module
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 数据模块
- en: The data module focuses on adapting UAV-related data into conversational or
    question-answering formats suitable for fine-tuning and training FMs tailored
    to UAV-specific tasks. UAV data can be broadly categorized into multimodal sensor
    data generated by UAVs (e.g., images, videos, LiDAR, GPS, IMU) and natural language
    instructions provided by operators. However, natural language instructions are
    often absent in raw datasets and must be generated through manual annotation or
    automated methods.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模块专注于将无人机相关数据转换为适合微调和训练基础模型的对话或问答格式，进而满足无人机特定任务的需求。无人机数据大致可分为两类：无人机生成的多模态传感器数据（如图像、视频、激光雷达、GPS、IMU）和操作员提供的自然语言指令。然而，原始数据集通常缺乏自然语言指令，必须通过手动注释或自动化方法生成。
- en: Generating natural language instructions typically involves leveraging image
    captioning models or similar tools to create descriptive or question-based annotations
    for sensor data, such as generating questions about specific objects or events
    in UAV imagery. Advanced FMs, like GPT-based models, further enable automated
    generation of diverse and contextually rich instructions, reducing the reliance
    on manual efforts and significantly expanding the scope of dataset construction.
    These methods ensure that natural language instructions align with the multimodal
    data, facilitating the integration of FMs for tasks requiring both perception
    and reasoning.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 生成自然语言指令通常需要利用图像描述模型或类似工具，为传感器数据创建描述性或基于问题的注释，例如生成关于无人机图像中特定物体或事件的问题。像基于GPT的高级基础模型（FM）进一步实现了多样化和富有上下文的指令自动生成，减少了对人工工作的依赖，并显著扩展了数据集构建的范围。这些方法确保自然语言指令与多模态数据对齐，促进了基础模型在需要感知和推理的任务中的整合。
- en: Constructing UAV-specific datasets is crucial for training and fine-tuning models.
    For instance, benchmark datasets tailored for UAV navigation and geolocation tasks
    have been developed, such as the dataset proposed by Chu *et al.* [[388](https://arxiv.org/html/2501.02341v1#bib.bib388)],
    which extends existing resources with text-image-bounding box annotations to improve
    geolocation accuracy. Similarly, Yao *et al.* [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)]
    introduced a fine-grained Chinese address recognition dataset for UAV delivery,
    enhancing navigation precision in urban contexts. Furthermore, in remote sensing
    applications, UAV imagery has been extensively utilized for tasks like object
    detection, semantic segmentation, and environmental monitoring, with multimodal
    large models significantly improving task efficiency and accuracy [[432](https://arxiv.org/html/2501.02341v1#bib.bib432)].
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 构建特定于无人机的数据集对于训练和微调模型至关重要。例如，已经开发了针对无人机导航和地理定位任务的基准数据集，如Chu *et al.* [[388](https://arxiv.org/html/2501.02341v1#bib.bib388)]
    提出的数据集，该数据集通过扩展现有资源并添加文本-图像-边界框注释，以提高地理定位精度。类似地，Yao *et al.* [[271](https://arxiv.org/html/2501.02341v1#bib.bib271)]
    提出了一个针对无人机配送的细粒度中文地址识别数据集，从而提高了城市环境中的导航精度。此外，在遥感应用中，无人机影像已广泛用于物体检测、语义分割和环境监测等任务，其中多模态大型模型显著提高了任务效率和准确性
    [[432](https://arxiv.org/html/2501.02341v1#bib.bib432)]。
- en: 7.2 FM Module
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 FM模块
- en: 'The FM module for UAV tasks focuses on two core aspects: selecting appropriate
    models and optimizing them for specific tasks. This modular approach ensures that
    UAV systems can handle diverse and complex scenarios effectively while maintaining
    efficiency in execution.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机任务的FM模块聚焦于两个核心方面：选择合适的模型并对其进行任务优化。这种模块化方法确保无人机系统能够在保持执行效率的同时，处理多样且复杂的场景。
- en: 7.2.1 Model Selection
  id: totrans-492
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1 模型选择
- en: The process begins with identifying the task type and determining whether the
    data involves single-modal or multimodal inputs. For language-based tasks, LLMs
    such as ChatGPT and LLama provide a robust foundation for reasoning, decision-making,
    and natural language interaction. For multimodal tasks, such as those involving
    visual and linguistic data, VLMs such as GPT-4V, LLaVa, and Qwen2-VL, are often
    ideal. These models serve as foundational components, providing a capability backbone
    for intelligent agents.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程始于识别任务类型，并确定数据是涉及单模态还是多模态输入。对于基于语言的任务，像ChatGPT和LLama这样的LLM为推理、决策和自然语言交互提供了坚实的基础。对于多模态任务，例如涉及视觉和语言数据的任务，像GPT-4V、LLaVa和Qwen2-VL这样的VLM通常是理想选择。这些模型作为基础组件，为智能代理提供了能力支撑。
- en: In addition to language and vision-based models, recent advancements have explored
    large 3D models, which are particularly relevant to UAVs operating in 3D environments.
    These models integrate FMs with capabilities for interpreting 3D data and planning
    tasks. For instance, Hong *et al.* [[433](https://arxiv.org/html/2501.02341v1#bib.bib433)]
    proposed a 3D LLM capable of dense captioning, 3D question answering, and navigation
    using point clouds. Similarly, Agent3D-Zero [[434](https://arxiv.org/html/2501.02341v1#bib.bib434)]
    employs Set-of-Line Prompts (SoLP) to enhance scene geometry understanding by
    generating diverse observational perspectives. While most current research focuses
    on indoor and closed environments, expanding these models to open and dynamic
    UAV scenarios presents exciting future opportunities.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于语言和视觉的模型，最近的研究还探索了大型3D模型，这对于在三维环境中操作的无人机尤为相关。这些模型将FM与解释三维数据和规划任务的能力结合在一起。例如，Hong
    *et al.* [[433](https://arxiv.org/html/2501.02341v1#bib.bib433)] 提出了一个能够进行密集标注、3D问答和使用点云进行导航的3D
    LLM。类似地，Agent3D-Zero [[434](https://arxiv.org/html/2501.02341v1#bib.bib434)] 采用了Set-of-Line
    Prompts (SoLP)，通过生成多样的观察视角来增强场景几何理解。虽然目前大多数研究集中在室内和封闭环境中，但将这些模型扩展到开放和动态的无人机场景中，展现了未来令人兴奋的机会。
- en: 7.2.2 Model Optimization
  id: totrans-495
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2 模型优化
- en: Once the base model is selected, it is optimized and adapted to meet UAV-specific
    requirements through methods like instruction fine-tuning and prompt fine-tuning.
    Prompt tuning is a straightforward approach that involves creating task-specific
    templates to embed mission background knowledge—such as objectives, environmental
    features, and task decomposition—into the model’s interactions. Few-shot learning
    can complement this process by using carefully curated examples to help the model
    grasp task-specific goals. For complex challenges, such as multi-stage planning
    or dynamic scene understanding, the CoT approach [[228](https://arxiv.org/html/2501.02341v1#bib.bib228)]
    decomposes tasks into sequential subtasks, improving reasoning and execution.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了基础模型，就可以通过指令微调和提示微调等方法对其进行优化和调整，以满足无人机（UAV）特定需求。提示微调是一种直接的方法，涉及创建任务特定的模板，将任务背景知识——如目标、环境特征和任务分解——嵌入模型的交互中。少样本学习可以通过使用精心挑选的示例来帮助模型掌握任务特定的目标，从而补充这一过程。对于复杂的挑战，如多阶段规划或动态场景理解，CoT方法[[228](https://arxiv.org/html/2501.02341v1#bib.bib228)]将任务分解为顺序子任务，从而改善推理和执行能力。
- en: Instruction fine-tuning offers further adaptability by generating datasets tailored
    to UAV-specific tasks. For example, in Visual-Language Navigation (VLN) tasks,
    datasets can include question-answer pairs relevant to object detection or navigation
    in UAV missions. Techniques like Low-Rank Adaptation (LoRA) [[435](https://arxiv.org/html/2501.02341v1#bib.bib435)]
    optimize these models by fine-tuning only a subset of parameters, maintaining
    computational efficiency while improving performance. Additionally, layer-freezing
    techniques can preserve pre-trained knowledge and minimize overfitting on small
    task-specific datasets.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调通过生成针对无人机特定任务的数据集提供了进一步的适应性。例如，在视觉语言导航（VLN）任务中，数据集可以包括与无人机任务中的物体检测或导航相关的问答对。像低秩适应（LoRA）[[435](https://arxiv.org/html/2501.02341v1#bib.bib435)]等技术通过仅微调部分参数来优化这些模型，从而在保持计算效率的同时提高性能。此外，冻结层技术可以保留预训练的知识，并最小化在小规模任务特定数据集上的过拟合。
- en: Building on instruction fine-tuning, Reinforcement Learning from Human Feedback
    (RLHF) [[436](https://arxiv.org/html/2501.02341v1#bib.bib436)] enhances model
    alignment with human preferences and operational needs. By incorporating reward
    signals derived from human feedback, RLHF enables the model to address dynamic
    UAV challenges, such as path generation, task adjustment, and critical object
    detection. This approach enhances the real-time responsiveness and automation
    of UAV control, ultimately improving mission efficiency and adaptability.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在指令微调的基础上，基于人类反馈的强化学习（RLHF）[[436](https://arxiv.org/html/2501.02341v1#bib.bib436)]通过将来自人类反馈的奖励信号纳入其中，提升了模型与人类偏好和操作需求的对齐。RLHF使模型能够应对动态的无人机挑战，如路径生成、任务调整和关键物体检测。这一方法增强了无人机控制的实时响应能力和自动化程度，最终提升了任务的效率和适应性。
- en: 7.3 Knowledge Module
  id: totrans-499
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 知识模块
- en: Retrieval-augmented generation (RAG) is an emerging technology that integrates
    retrieval and generation capabilities. Its core functionality lies in retrieving
    relevant information from a knowledge base and fusing it with the output of a
    generative model, thereby enhancing the accuracy and domain adaptability of generated
    results. RAG models leverage a retrieval module to obtain information pertinent
    to the input content from external knowledge repositories and incorporate it as
    context for the generative module. This approach improves the quality and reliability
    of generated outputs. Unlike traditional generative models, RAG introduces a real-time
    retrieval mechanism to mitigate the ”hallucination” problem, wherein a model generates
    incorrect or fabricated information due to insufficient background knowledge.
    Moreover, the modular architecture of RAG allows for independent updates of the
    knowledge base and generative model, increasing system flexibility and ensuring
    the timeliness and accuracy of the information used in generation. Consequently,
    RAG demonstrates significant potential in tasks requiring high specialization,
    real-time information processing, or personalized outputs.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）是一项新兴技术，融合了检索和生成的能力。其核心功能在于从知识库中检索相关信息，并将其与生成模型的输出相结合，从而提高生成结果的准确性和领域适应性。RAG模型利用检索模块从外部知识库中获取与输入内容相关的信息，并将其作为生成模块的上下文。这种方法提高了生成输出的质量和可靠性。与传统生成模型不同，RAG引入了实时检索机制，旨在缓解“幻觉”问题，即由于背景知识不足，模型生成错误或捏造的信息。此外，RAG的模块化架构使得知识库和生成模型可以独立更新，从而增加了系统的灵活性，并确保生成过程中使用的信息及时且准确。因此，RAG在需要高度专业化、实时信息处理或个性化输出的任务中表现出巨大的潜力。
- en: Constructing RAG systems tailored for UAV-specific tasks is crucial because
    UAV operations involve diverse and complex scenarios. First, RAG can provide real-time
    access to up-to-date environmental data, such as meteorological conditions, terrain
    information, and air traffic updates, which are essential for tasks like flight
    planning and navigation. Second, integrating a domain-specific knowledge base
    into the RAG framework enables UAVs to perform advanced decision-making tasks,
    such as autonomous mission adjustments in dynamic environments or identifying
    unknown objects during surveillance missions. Finally, RAG can facilitate interaction
    with human operators by retrieving contextual data to clarify queries or enhance
    the interpretability of system decisions. For example, in UAV-based environmental
    monitoring tasks, RAG can retrieve historical data on pollution levels or land
    use patterns, combine this with current sensor data, and generate comprehensive
    reports. These capabilities illustrate how a well-constructed RAG framework can
    enhance the efficiency, accuracy, and adaptability of UAV systems, paving the
    way for more intelligent and autonomous UAV applications.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 为无人机特定任务构建定制的RAG系统至关重要，因为无人机操作涉及多种多样且复杂的场景。首先，RAG可以实时获取最新的环境数据，如气象条件、地形信息和空中交通更新，这些对飞行规划和导航等任务至关重要。其次，将领域特定的知识库集成到RAG框架中，使得无人机能够执行高级决策任务，如在动态环境中进行自主任务调整或在监视任务中识别未知物体。最后，RAG还可以通过检索上下文数据来与人工操作员进行交互，帮助澄清问题或增强系统决策的可解释性。例如，在基于无人机的环境监测任务中，RAG可以检索有关污染水平或土地使用模式的历史数据，将其与当前传感器数据结合，并生成全面的报告。这些功能展示了一个精心构建的RAG框架如何提高无人机系统的效率、准确性和适应性，为更智能和自主的无人机应用铺平道路。
- en: 7.4 Tools Module
  id: totrans-502
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 工具模块
- en: The Tools Module is designed to provide both general-purpose functionalities
    and task-specific capabilities to support UAV operations.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 工具模块旨在提供通用功能和特定任务能力，以支持无人机（UAV）操作。
- en: General Tools
  id: totrans-504
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 通用工具
- en: General tools focus on broad, multimodal functionalities to enhance the UAV
    system’s perception and interaction capabilities. Among these, VFMs serve as a
    cornerstone for addressing diverse visual tasks, leveraging their exceptional
    generalization and zero-shot learning capabilities. Unlike FMs that emphasize
    reasoning and decision-making, VFMs excel in understanding specific visual tasks,
    making them ideal as foundational tools rather than core ”FM-Brain” components.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 一般工具专注于广泛的、多模态的功能，以增强无人机系统的感知和交互能力。其中，视觉语言模型（VFM）作为解决多种视觉任务的基石，凭借其卓越的泛化能力和零-shot学习能力，发挥着重要作用。与侧重推理和决策的基础模型（FM）不同，VFM在理解特定视觉任务方面表现出色，使其更适合作为基础工具，而非核心的“FM-大脑”组件。
- en: VFMs offer significant advantages in UAV missions by aligning with specific
    task requirements. For instance, the CLIP series is well-suited for object recognition
    and scene understanding tasks due to its robust multimodal alignment, enabling
    open-vocabulary object detection and classification. The Segment Anything Model
    (SAM), renowned for its zero-shot segmentation capabilities, is ideal for image
    segmentation across varied environments and targets. Grounding DINO excels in
    object detection and localization tasks, providing efficient target tracking and
    detection in dynamic scenarios. These models can independently handle specific
    tasks or integrate with LLMs or VLMs to enhance UAV systems’ intelligence in mission
    planning, navigation, and environmental perception.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: VFMs通过与特定任务需求的对接，在无人机任务中提供了显著的优势。例如，CLIP系列模型非常适合物体识别和场景理解任务，因为其强大的多模态对齐能力，使得开放词汇物体检测和分类成为可能。以零-shot分割能力著称的Segment
    Anything Model（SAM）非常适合在不同环境和目标下进行图像分割。Grounding DINO则在物体检测和定位任务中表现突出，能够在动态场景中高效地进行目标跟踪和检测。这些模型可以独立执行特定任务，或与大型语言模型（LLMs）或视觉语言模型（VLMs）结合，以增强无人机系统在任务规划、导航和环境感知方面的智能。
- en: Moreover, VFMs can be fine-tuned to adapt to UAV-specific scenarios. For instance,
    fine-tuning the Grounding DINO model on specialized datasets improves its performance
    in complex multi-target tracking tasks. Additionally, VFMs can collaborate with
    traditional machine learning or deep learning models to form a ”large model +
    small model” strategy, balancing generalization with task-specific efficiency.
    For example, VFMs extract global semantic information, while smaller models focus
    on fine-grained details, achieving an effective combination of global and local
    analyses.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，VFM可以通过微调来适应无人机的特定场景。例如，在专业数据集上微调Grounding DINO模型，可以提升其在复杂的多目标跟踪任务中的表现。此外，VFM还可以与传统的机器学习或深度学习模型合作，形成“大型模型+小型模型”的策略，在泛化能力与任务特定效率之间找到平衡。例如，VFM提取全局语义信息，而较小的模型则专注于细粒度的细节，从而实现全局和局部分析的有效结合。
- en: Another innovative application of VFMs involves their use in generating instruction
    fine-tuning datasets for VLMs. By leveraging VFM outputs such as image captions,
    segmentation descriptions, and object depth information, these datasets can train
    VLMs for UAV-specific missions. For example, Chen *et al.* [[437](https://arxiv.org/html/2501.02341v1#bib.bib437)]
    created a 3D spatial instruction fine-tuning dataset using internet-scale spatial
    reasoning data from VFMs, training the SpatialVLM model. This approach highlights
    VFMs’ potential to generate high-quality datasets for large models, significantly
    enhancing UAV systems’ dynamic perception and mission planning capabilities.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: VFMs的另一个创新应用是在生成VLMs（视觉语言模型）的指令微调数据集方面。通过利用VFM的输出，如图像标题、分割描述和物体深度信息，这些数据集可以用来训练专门用于无人机任务的VLMs。例如，Chen
    *et al.* [[437](https://arxiv.org/html/2501.02341v1#bib.bib437)]使用来自VFM的互联网规模空间推理数据，创建了一个3D空间指令微调数据集，并用其训练了SpatialVLM模型。这一方法突出了VFM在生成高质量数据集方面的潜力，显著提升了无人机系统在动态感知和任务规划方面的能力。
- en: Task-Specific Tools
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 特定任务工具
- en: Task-specific tools are tailored to UAV-centric operations, focusing on flight
    control and mission execution. Key components include PX4 and Pixhawk, widely
    used open-source flight controllers. These tools provide UAVs with precise control,
    mission planning, and real-time adaptability, making them indispensable for complex
    aerial tasks. By combining these specialized tools with general functionalities,
    the UAV system achieves a high degree of flexibility and efficiency in addressing
    mission-specific challenges.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 特定任务的工具为以无人机为中心的操作量身定制，重点是飞行控制和任务执行。关键组件包括PX4和Pixhawk，这些是广泛使用的开源飞行控制器。这些工具为无人机提供精确控制、任务规划和实时适应能力，使其在复杂的空中任务中不可或缺。通过将这些专门化工具与通用功能相结合，无人机系统在应对特定任务挑战时实现了高度的灵活性和效率。
- en: 7.5 Agent Module
  id: totrans-511
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 代理模块
- en: The Agent Module is designed to provide intelligent decision-making and task
    execution capabilities within the UAV system. It integrates both high-level coordination
    and task-specific agent workflows to optimize UAV operations in complex missions.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理模块**旨在为无人机系统提供智能决策和任务执行能力。它整合了高层次的协调和特定任务的代理工作流程，以优化无人机在复杂任务中的操作。'
- en: Manager Agent
  id: totrans-513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 管理代理
- en: The Manage Agent is responsible for high-level task coordination and scheduling
    within the UAV swarm, ensuring that missions are executed efficiently across multiple
    UAVs. This agent takes on the role of global planning and overall task allocation,
    breaking down a large mission into smaller, manageable sub-tasks, which are then
    assigned to individual UAVs. Additionally, the Global Agent monitors the swarm’s
    status and dynamically adjusts the distribution of tasks based on real-time feedback,
    ensuring that each UAV operates effectively within the context of the broader
    mission.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 管理代理负责无人机群体中的高层任务协调和调度，确保多个无人机高效执行任务。该代理承担全局规划和整体任务分配的角色，将大型任务分解为更小的可管理子任务，并将其分配给各个无人机。此外，全球代理监控群体状态，并根据实时反馈动态调整任务分配，确保每架无人机在更广泛任务的背景下高效运作。
- en: UAV-Specific Agentic Workflow
  id: totrans-515
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 无人机特定的代理工作流程
- en: Each UAV in the swarm follows an autonomous Agentic Workflow that consists of
    a chain of agents designed to handle perception, planning, and control tasks.
    These agents operate in sequence, ensuring that each UAV processes the necessary
    data and executes its mission objectives effectively. The perception agent first
    processes sensor data, identifying obstacles, objects, and points of interest
    using advanced VFMs, such as CLIP for object recognition, SAM for segmentation,
    and Grounding DINO for localization.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 群体中的每架无人机都遵循一个自主的代理工作流程，该流程由一系列代理组成，旨在处理感知、规划和控制任务。这些代理按顺序操作，确保每架无人机处理必要的数据并有效执行其任务目标。感知代理首先处理传感器数据，利用先进的视觉语言模型（VFM）识别障碍物、物体和兴趣点，如用于物体识别的CLIP、用于分割的SAM和用于定位的Grounding
    DINO。
- en: Next, the planning agent takes the data from the perception agent to generate
    optimized flight paths and task strategies, ensuring that the UAV can navigate
    the environment and complete the assigned mission efficiently. Finally, the control
    agent converts the plans into actionable commands, controlling the UAV’s flight
    and task execution.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，规划代理使用感知代理的数据生成优化的飞行路径和任务策略，确保无人机能够有效地导航环境并完成分配的任务。最后，控制代理将计划转化为可执行的命令，控制无人机的飞行和任务执行。
- en: This workflow allows each UAV to operate independently while still contributing
    to the overall mission goals. Moreover, the UAV-Specific Agentic Workflow is adaptable
    to a wide variety of UAV missions, from search and rescue to surveillance, by
    fine-tuning the agents’ capabilities according to the specific requirements of
    each task. This adaptability enhances the UAV’s efficiency in handling complex,
    dynamic environments.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 该工作流程使每架无人机能够独立操作，同时仍能为整体任务目标做出贡献。此外，无人机特定的代理工作流程可以根据每个任务的具体需求，通过微调代理的能力，适应各种无人机任务，从搜索与救援到监视。这种适应性提高了无人机在处理复杂、动态环境中的效率。
- en: Agent Collaboration and Adaptability
  id: totrans-519
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代理协作与适应性
- en: The collaboration between the Global Agent and UAV-specific agents is crucial
    for optimizing mission execution. The Global Agent provides high-level directives
    that guide the overall mission strategy. These directives are broken down into
    detailed execution plans by individual UAV agents, ensuring that each UAV can
    operate autonomously while contributing to the collective mission goal. The UAV
    agents communicate with the Global Agent to receive updated instructions and report
    progress, enabling continuous task adaptation and dynamic adjustments to the mission
    plan in response to real-time data and changing conditions.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 全球代理与无人机特定代理之间的合作对于优化任务执行至关重要。全球代理提供高层次的指令，指导整体任务战略。这些指令被各个无人机代理分解为详细的执行计划，确保每个无人机在自主操作的同时，为集体任务目标做出贡献。无人机代理与全球代理进行通信，接收更新的指令并报告进度，从而实现任务的持续适应，并根据实时数据和变化的条件动态调整任务计划。
- en: Furthermore, UAV agents within the swarm can interact with each other to exchange
    information and coordinate their actions. This peer-to-peer communication enables
    the UAVs to adapt their behavior based on shared situational awareness, such as
    when multiple UAVs must avoid collisions or collaborate to accomplish a joint
    task. For example, one UAV might share its perception data with another to adjust
    flight paths or synchronize tasks in real-time. This interaction ensures that
    the UAV swarm operates cohesively, with each agent adjusting its actions based
    on both global guidance and local, real-time information from other agents.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，群体中的无人机（UAV）代理可以相互之间进行互动，交换信息并协调它们的行动。这种点对点通信使得无人机能够根据共享的情境意识调整其行为，例如当多个无人机必须避免碰撞或协作完成联合任务时。例如，一架无人机可能会将其感知数据分享给另一架无人机，以调整飞行路径或实时同步任务。这种互动确保了无人机群体能够协调运作，每个代理根据全球指导和来自其他代理的本地实时信息调整其行动。
- en: 8 Conclusion
  id: totrans-522
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This paper explores the promising integration of large language models (LLMs)
    with unmanned aerial vehicles (UAVs), highlighting the transformative potential
    of LLMs in enhancing UAVs’ decision-making, perception, and reasoning capabilities.
    The paper first provides an overview of UAV system components and the principles
    behind large models, laying the foundation for their integration. It then reviews
    the classification, research progress, and application scenarios of UAV systems
    enhanced by foundational LLMs. Additionally, we list key UAV-related datasets
    that support the development of intelligent UAV systems. Looking ahead, we emphasize
    the future direction of agentic UAVs, where multi-agent systems integrate knowledge
    and tool modules to create flexible UAVs capable of handling complex tasks and
    dynamic environments.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了大规模语言模型（LLMs）与无人机（UAV）结合的前景，突出了LLMs在提升无人机决策、感知和推理能力方面的变革潜力。本文首先概述了无人机系统的组成部分以及大模型背后的原理，为其整合奠定了基础。接着回顾了增强基础LLM的无人机系统的分类、研究进展及应用场景。此外，我们列出了支持智能无人机系统开发的关键无人机相关数据集。展望未来，我们强调了代理型无人机的未来方向，其中多智能体系统集成了知识和工具模块，以创造能够处理复杂任务和动态环境的灵活无人机。
- en: 9 Acknowledgement
  id: totrans-524
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: This work is partly supported by The Science and Technology Development Fund
    of Macau SAR (No. 0145/2023/RIA3 and 0093/2023/RIA2), National Natural Science
    Foundation of China (62303460), Beijing Natural Science Foundation-Fengtai Rail
    Transit Frontier Research Joint Fund (L231002), and the Young Elite Scientists
    Sponsorship Program of China Association of Science and Technology under Grant
    YESS20220372.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分得到了澳门特别行政区科技发展基金（编号：0145/2023/RIA3 和 0093/2023/RIA2）、中国国家自然科学基金（62303460）、北京自然科学基金-丰台轨道交通前沿研究联合基金（L231002）以及中国科学技术协会青年精英科学家资助计划（YESS20220372）的支持。
- en: References
  id: totrans-526
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Liu et al. [2024] Y. Liu, F. Yao, Y. Yue, G. Xu, X. Sun, K. Fu, Navagent: Multi-scale
    urban street view fusion for uav embodied vision-and-language navigation, arXiv
    preprint arXiv:2411.08579 (2024).'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024] Y. Liu, F. Yao, Y. Yue, G. Xu, X. Sun, K. Fu, Navagent: Multi-scale
    urban street view fusion for uav embodied vision-and-language navigation, arXiv
    preprint arXiv:2411.08579 (2024).'
- en: 'Sarkar et al. [2024] A. Sarkar, S. Sastry, A. Pirinen, C. Zhang, N. Jacobs,
    Y. Vorobeychik, Gomaa-geo: Goal modality agnostic active geo-localization, arXiv
    preprint arXiv:2406.01917 (2024).'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sarkar et al. [2024] A. Sarkar, S. Sastry, A. Pirinen, C. Zhang, N. Jacobs,
    Y. Vorobeychik, Gomaa-geo: Goal modality agnostic active geo-localization, arXiv
    preprint arXiv:2406.01917 (2024).'
- en: Liu et al. [2024] J. Liu, J. Cui, M. Ye, X. Zhu, S. Tang, Shooting condition
    insensitive unmanned aerial vehicle object detection, Expert Systems with Applications
    246 (2024) 123221.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2024] J. 刘, J. 崔, M. 叶, X. 朱, S. 唐, 不受拍摄条件影响的无人机物体检测，专家系统与应用 246 (2024)
    123221。
- en: 'Qiu et al. [2024] H. Qiu, J. Li, J. Gan, S. Zheng, L. Yan, Dronegpt: Zero-shot
    video question answering for drones, in: Proceedings of the International Conference
    on Computer Vision and Deep Learning, 2024, pp. 1–6.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邱等人 [2024] H. 邱, J. 李, J. 干, S. 郑, L. 颜, Dronegpt：无人机的视频零-shot问题回答，载于：国际计算机视觉与深度学习会议论文集，2024，第1–6页。
- en: 'Chen et al. [2023] G. Chen, X. Yu, N. Ling, L. Zhong, Typefly: Flying drones
    with large language model, arXiv preprint arXiv:2312.14950 (2023).'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023] G. 陈, X. 余, N. 凌, L. 钟, Typefly：使用大型语言模型飞行无人机，arXiv 预印本 arXiv:2312.14950
    (2023)。
- en: 'Tagliabue et al. [2023] A. Tagliabue, K. Kondo, T. Zhao, M. Peterson, C. T.
    Tewari, J. P. How, Real: Resilience and adaptation using large language models
    on autonomous aerial robots, arXiv preprint arXiv:2311.01403 (2023).'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tagliabue 等人 [2023] A. Tagliabue, K. Kondo, T. Zhao, M. Peterson, C. T. Tewari,
    J. P. How, Real：使用大型语言模型在自主空中机器人上实现的弹性和适应性，arXiv 预印本 arXiv:2311.01403 (2023)。
- en: Panagiotou and Yakinthos [2020] P. Panagiotou, K. Yakinthos, Aerodynamic efficiency
    and performance enhancement of fixed-wing uavs, Aerospace Science and Technology
    99 (2020) 105575.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Panagiotou 和 Yakinthos [2020] P. Panagiotou, K. Yakinthos, 固定翼无人机的空气动力效率和性能增强，航天科学与技术
    99 (2020) 105575。
- en: Villa et al. [2020] D. K. Villa, A. S. Brandao, M. Sarcinelli-Filho, A survey
    on load transportation using multirotor uavs, Journal of Intelligent & Robotic
    Systems 98 (2020) 267–296.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villa 等人 [2020] D. K. Villa, A. S. Brandao, M. Sarcinelli-Filho, 关于使用多旋翼无人机进行载荷运输的调查，智能与机器人系统杂志
    98 (2020) 267–296。
- en: 'Rashad et al. [2020] R. Rashad, J. Goerres, R. Aarts, J. B. Engelen, S. Stramigioli,
    Fully actuated multirotor uavs: A literature review, IEEE Robotics & Automation
    Magazine 27 (2020) 97–107.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashad 等人 [2020] R. Rashad, J. Goerres, R. Aarts, J. B. Engelen, S. Stramigioli,
    完全驱动的多旋翼无人机：文献综述，IEEE 机器人与自动化杂志 27 (2020) 97–107。
- en: Alvarenga et al. [2015] J. Alvarenga, N. I. Vitzilaios, K. P. Valavanis, M. J.
    Rutherford, Survey of unmanned helicopter model-based navigation and control techniques,
    Journal of Intelligent & Robotic Systems 80 (2015) 87–138.
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alvarenga 等人 [2015] J. Alvarenga, N. I. Vitzilaios, K. P. Valavanis, M. J. Rutherford,
    无人直升机模型导航与控制技术的综述，智能与机器人系统杂志 80 (2015) 87–138。
- en: Saeed et al. [2018] A. S. Saeed, A. B. Younes, C. Cai, G. Cai, A survey of hybrid
    unmanned aerial vehicles, Progress in Aerospace Sciences 98 (2018) 91–105.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saeed 等人 [2018] A. S. Saeed, A. B. Younes, C. Cai, G. Cai, 混合型无人机的调查，航空航天科学进展
    98 (2018) 91–105。
- en: Du et al. [2024] H. Du, L. Ren, Y. Wang, X. Cao, C. Sun, Advancements in perception
    system with multi-sensor fusion for embodied agents, Information Fusion (2024)
    102859.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2024] H. 杜, L. 任, Y. 王, X. 曹, C. 孙, 面向具身智能体的多传感器融合感知系统的进展，信息融合 (2024) 102859。
- en: Martinez-Carranza and Rascon [2020] J. Martinez-Carranza, C. Rascon, A review
    on auditory perception for unmanned aerial vehicles, Sensors 20 (2020) 7276.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martinez-Carranza 和 Rascon [2020] J. Martinez-Carranza, C. Rascon, 关于无人机听觉感知的综述，传感器
    20 (2020) 7276。
- en: Zhang et al. [2023] J. Zhang, S. Xu, Y. Zhao, J. Sun, S. Xu, X. Zhang, Aerial
    orthoimage generation for uav remote sensing, Information Fusion 89 (2023) 91–120.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023] J. 张, S. 徐, Y. 赵, J. 孙, S. 徐, X. 张, 无人机遥感的空中正射影像生成，信息融合 89 (2023)
    91–120。
- en: 'Mittal et al. [2020] P. Mittal, R. Singh, A. Sharma, Deep learning-based object
    detection in low-altitude uav datasets: A survey, Image and Vision computing 104
    (2020) 104046.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mittal 等人 [2020] P. Mittal, R. Singh, A. Sharma, 基于深度学习的低空无人机数据集中的物体检测：综述，图像与视觉计算
    104 (2020) 104046。
- en: 'Liu et al. [2020] M. Liu, X. Wang, A. Zhou, X. Fu, Y. Ma, C. Piao, Uav-yolo:
    Small object detection on unmanned aerial vehicle perspective, Sensors 20 (2020)
    2238.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2020] M. 刘, X. 王, A. 周, X. 傅, Y. 马, C. 朴, UAV-YOLO：基于无人机视角的小物体检测，传感器 20
    (2020) 2238。
- en: 'Girisha et al. [2019] S. Girisha, M. P. MM, U. Verma, R. M. Pai, Semantic segmentation
    of uav aerial videos using convolutional neural networks, in: 2019 IEEE Second
    International Conference on Artificial Intelligence and Knowledge Engineering
    (AIKE), IEEE, 2019, pp. 21–27.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Girisha 等人 [2019] S. Girisha, M. P. MM, U. Verma, R. M. Pai, 使用卷积神经网络进行无人机空中视频的语义分割，载于：2019
    IEEE 第二届国际人工智能与知识工程会议（AIKE），IEEE，2019，第21–27页。
- en: Liu et al. [2021] S. Liu, J. Cheng, L. Liang, H. Bai, W. Dang, Light-weight
    semantic segmentation network for uav remote sensing images, IEEE Journal of Selected
    Topics in Applied Earth Observations and Remote Sensing 14 (2021) 8287–8296.
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021] S. Liu, J. Cheng, L. Liang, H. Bai, W. Dang, 面向无人机遥感图像的轻量级语义分割网络，IEEE《应用地球观测与遥感精选主题期刊》
    14 (2021) 8287–8296。
- en: 'Li et al. [2010] Z. Li, N. Hovakimyan, V. Dobrokhodov, I. Kaminer, Vision-based
    target tracking and motion estimation using a small uav, in: 49th IEEE Conference
    on Decision and Control (CDC), IEEE, 2010, pp. 2505–2510.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2010] Z. Li, N. Hovakimyan, V. Dobrokhodov, I. Kaminer, 基于视觉的目标跟踪与运动估计方法，使用小型无人机，见：第49届IEEE决策与控制会议（CDC），IEEE，2010，页码2505–2510。
- en: 'Dobrokhodov et al. [2006] V. N. Dobrokhodov, I. I. Kaminer, K. D. Jones, R. Ghabcheloo,
    Vision-based tracking and motion estimation for moving targets using small uavs,
    in: 2006 American Control Conference, IEEE, 2006, pp. 6–pp.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dobrokhodov 等人 [2006] V. N. Dobrokhodov, I. I. Kaminer, K. D. Jones, R. Ghabcheloo,
    基于视觉的运动目标跟踪与估计，使用小型无人机，见：2006年美国控制会议，IEEE，2006，页码6–页码。
- en: 'Mascaro et al. [2018] R. Mascaro, L. Teixeira, T. Hinzmann, R. Siegwart, M. Chli,
    Gomsf: Graph-optimization based multi-sensor fusion for robust uav pose estimation,
    in: 2018 IEEE international conference on robotics and automation (ICRA), IEEE,
    2018, pp. 1421–1428.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mascaro 等人 [2018] R. Mascaro, L. Teixeira, T. Hinzmann, R. Siegwart, M. Chli,
    Gomsf：基于图优化的多传感器融合用于稳健的无人机姿态估计，见：2018 IEEE国际机器人与自动化会议（ICRA），IEEE，2018，页码1421–1428。
- en: 'Wan et al. [2022] L. Wan, R. Liu, L. Sun, H. Nie, X. Wang, Uav swarm based
    radar signal sorting via multi-source data fusion: A deep transfer learning framework,
    Information Fusion 78 (2022) 90–101.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等人 [2022] L. Wan, R. Liu, L. Sun, H. Nie, X. Wang, 基于无人机群体的雷达信号排序，通过多源数据融合：一种深度迁移学习框架，信息融合
    78 (2022) 90–101。
- en: 'Rezwan and Choi [2022] S. Rezwan, W. Choi, Artificial intelligence approaches
    for uav navigation: Recent advances and future challenges, IEEE access 10 (2022)
    26320–26339.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rezwan 和 Choi [2022] S. Rezwan, W. Choi, 面向无人机导航的人工智能方法：最新进展与未来挑战，IEEE Access
    10 (2022) 26320–26339。
- en: Gyagenda et al. [2022] N. Gyagenda, J. V. Hatilima, H. Roth, V. Zhmud, A review
    of gnss-independent uav navigation techniques, Robotics and Autonomous Systems
    152 (2022) 104069.
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gyagenda 等人 [2022] N. Gyagenda, J. V. Hatilima, H. Roth, V. Zhmud, GNSS独立的无人机导航技术综述，机器人与自主系统
    152 (2022) 104069。
- en: 'Balamurugan et al. [2016] G. Balamurugan, J. Valarmathi, V. Naidu, Survey on
    uav navigation in gps denied environments, in: 2016 International conference on
    signal processing, communication, power and embedded system (SCOPES), IEEE, 2016,
    pp. 198–204.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Balamurugan 等人 [2016] G. Balamurugan, J. Valarmathi, V. Naidu, 无人机在GPS失效环境下导航的调查，见：2016年信号处理、通信、电力与嵌入式系统国际会议（SCOPES），IEEE，2016，页码198–204。
- en: 'McEnroe et al. [2022] P. McEnroe, S. Wang, M. Liyanage, A survey on the convergence
    of edge computing and ai for uavs: Opportunities and challenges, IEEE Internet
    of Things Journal 9 (2022) 15435–15459.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McEnroe 等人 [2022] P. McEnroe, S. Wang, M. Liyanage, 无人机边缘计算与人工智能融合的调查：机遇与挑战，IEEE物联网期刊
    9 (2022) 15435–15459。
- en: 'Neumann and Bartholmai [2015] P. P. Neumann, M. Bartholmai, Real-time wind
    estimation on a micro unmanned aerial vehicle using its inertial measurement unit,
    Sensors and Actuators A: Physical 235 (2015) 300–310.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Neumann 和 Bartholmai [2015] P. P. Neumann, M. Bartholmai, 使用微型无人驾驶飞行器的惯性测量单元进行实时风速估计，传感器与执行器
    A: 物理 235 (2015) 300–310。'
- en: Barbieri et al. [2019] L. Barbieri, S. T. Kral, S. C. Bailey, A. E. Frazier,
    J. D. Jacob, J. Reuder, D. Brus, P. B. Chilson, C. Crick, C. Detweiler, et al.,
    Intercomparison of small unmanned aircraft system (suas) measurements for atmospheric
    science during the lapse-rate campaign, Sensors 19 (2019) 2179.
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barbieri 等人 [2019] L. Barbieri, S. T. Kral, S. C. Bailey, A. E. Frazier, J.
    D. Jacob, J. Reuder, D. Brus, P. B. Chilson, C. Crick, C. Detweiler 等人, 在温度梯度实验期间小型无人机系统（suas）在大气科学中的测量比较，传感器
    19 (2019) 2179。
- en: Couturier and Akhloufi [2021] A. Couturier, M. A. Akhloufi, A review on absolute
    visual localization for uav, Robotics and Autonomous Systems 135 (2021) 103666.
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Couturier 和 Akhloufi [2021] A. Couturier, M. A. Akhloufi, 关于无人机绝对视觉定位的综述，机器人与自主系统
    135 (2021) 103666。
- en: 'Rovira-Sugranes et al. [2022] A. Rovira-Sugranes, A. Razi, F. Afghah, J. Chakareski,
    A review of ai-enabled routing protocols for uav networks: Trends, challenges,
    and future outlook, Ad Hoc Networks 130 (2022) 102790.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rovira-Sugranes 等人 [2022] A. Rovira-Sugranes, A. Razi, F. Afghah, J. Chakareski,
    面向无人机网络的人工智能启用路由协议综述：趋势、挑战与未来展望，Ad Hoc 网络 130 (2022) 102790。
- en: Atif et al. [2021] M. Atif, R. Ahmad, W. Ahmad, L. Zhao, J. J. Rodrigues, Uav-assisted
    wireless localization for search and rescue, IEEE Systems Journal 15 (2021) 3261–3272.
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Atif 等人 [2021] M. Atif, R. Ahmad, W. Ahmad, L. Zhao, J. J. Rodrigues, 无人机辅助无线定位用于搜救,
    IEEE Systems Journal 15 (2021) 3261–3272。
- en: Lu et al. [2018] Y. Lu, Z. Xue, G.-S. Xia, L. Zhang, A survey on vision-based
    uav navigation, Geo-spatial information science 21 (2018) 21–32.
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2018] Y. Lu, Z. Xue, G.-S. Xia, L. Zhang, 基于视觉的无人机导航综述, Geo-spatial information
    science 21 (2018) 21–32。
- en: 'Gupta and Fernando [2022] A. Gupta, X. Fernando, Simultaneous localization
    and mapping (slam) and data fusion in unmanned aerial vehicles: Recent advances
    and challenges, Drones 6 (2022) 85.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 和 Fernando [2022] A. Gupta, X. Fernando, 无人机的同步定位与地图构建（SLAM）及数据融合：最新进展与挑战,
    Drones 6 (2022) 85。
- en: Kassas et al. [2024] Z. M. Kassas, N. Khairallah, J. J. Khalife, C. Lee, J. Jurado,
    S. Wachtel, J. Duede, Z. Hoeffner, T. Hulsey, R. Quirarte, et al., Aircraft navigation
    in gnss-denied environments via radio slam with terrestrial signals of opportunity,
    IEEE Transactions on Intelligent Transportation Systems (2024).
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kassas 等人 [2024] Z. M. Kassas, N. Khairallah, J. J. Khalife, C. Lee, J. Jurado,
    S. Wachtel, J. Duede, Z. Hoeffner, T. Hulsey, R. Quirarte 等, 在 GNSS 无信号环境下通过无线电
    SLAM 和地面信号进行航空器导航, IEEE Transactions on Intelligent Transportation Systems (2024)。
- en: Tisdale et al. [2009] J. Tisdale, Z. Kim, J. K. Hedrick, Autonomous uav path
    planning and estimation, IEEE Robotics & Automation Magazine 16 (2009) 35–42.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tisdale 等人 [2009] J. Tisdale, Z. Kim, J. K. Hedrick, 自主无人机路径规划与估计, IEEE Robotics
    & Automation Magazine 16 (2009) 35–42。
- en: Goerzen et al. [2010] C. Goerzen, Z. Kong, B. Mettler, A survey of motion planning
    algorithms from the perspective of autonomous uav guidance, Journal of Intelligent
    and Robotic Systems 57 (2010) 65–100.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goerzen 等人 [2010] C. Goerzen, Z. Kong, B. Mettler, 从自主无人机导航角度对运动规划算法的调查, Journal
    of Intelligent and Robotic Systems 57 (2010) 65–100。
- en: Hong et al. [2021] Y. Hong, S. Kim, Y. Kim, J. Cha, Quadrotor path planning
    using a* search algorithm and minimum snap trajectory generation, ETRI Journal
    43 (2021) 1013–1023.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 [2021] Y. Hong, S. Kim, Y. Kim, J. Cha, 四旋翼路径规划使用 A* 搜索算法和最小突变轨迹生成,
    ETRI Journal 43 (2021) 1013–1023。
- en: Chai et al. [2022] X. Chai, Z. Zheng, J. Xiao, L. Yan, B. Qu, P. Wen, H. Wang,
    Y. Zhou, H. Sun, Multi-strategy fusion differential evolution algorithm for uav
    path planning in complex environment, Aerospace Science and Technology 121 (2022)
    107287.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chai 等人 [2022] X. Chai, Z. Zheng, J. Xiao, L. Yan, B. Qu, P. Wen, H. Wang, Y.
    Zhou, H. Sun, 用于复杂环境下无人机路径规划的多策略融合差分进化算法, Aerospace Science and Technology 121
    (2022) 107287。
- en: Xiao et al. [2021] S. Xiao, X. Tan, J. Wang, A simulated annealing algorithm
    and grid map-based uav coverage path planning method for 3d reconstruction, Electronics
    10 (2021) 853.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2021] S. Xiao, X. Tan, J. Wang, 基于模拟退火算法和网格地图的无人机覆盖路径规划方法用于 3D 重建,
    Electronics 10 (2021) 853。
- en: Ait-Saadi et al. [2022] A. Ait-Saadi, Y. Meraihi, A. Soukane, A. Ramdane-Cherif,
    A. B. Gabis, A novel hybrid chaotic aquila optimization algorithm with simulated
    annealing for unmanned aerial vehicles path planning, Computers and Electrical
    Engineering 104 (2022) 108461.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ait-Saadi 等人 [2022] A. Ait-Saadi, Y. Meraihi, A. Soukane, A. Ramdane-Cherif,
    A. B. Gabis, 一种新型的混合混沌鹰优化算法与模拟退火用于无人机路径规划, Computers and Electrical Engineering
    104 (2022) 108461。
- en: Phung and Ha [2021] M. D. Phung, Q. P. Ha, Safety-enhanced uav path planning
    with spherical vector-based particle swarm optimization, Applied Soft Computing
    107 (2021) 107376.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phung 和 Ha [2021] M. D. Phung, Q. P. Ha, 基于球形向量粒子群优化的安全增强无人机路径规划, Applied Soft
    Computing 107 (2021) 107376。
- en: Yu et al. [2022] Z. Yu, Z. Si, X. Li, D. Wang, H. Song, A novel hybrid particle
    swarm optimization algorithm for path planning of uavs, IEEE Internet of Things
    Journal 9 (2022) 22547–22558.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2022] Z. Yu, Z. Si, X. Li, D. Wang, H. Song, 一种新型的混合粒子群优化算法用于无人机路径规划,
    IEEE Internet of Things Journal 9 (2022) 22547–22558。
- en: He et al. [2021] W. He, X. Qi, L. Liu, A novel hybrid particle swarm optimization
    for multi-uav cooperate path planning, Applied Intelligence 51 (2021) 7350–7364.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2021] W. He, X. Qi, L. Liu, 一种新型的混合粒子群优化算法用于多无人机协作路径规划, Applied Intelligence
    51 (2021) 7350–7364。
- en: 'Yang et al. [2023] Y. Yang, X. Xiong, Y. Yan, Uav formation trajectory planning
    algorithms: A review, Drones 7 (2023) 62.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023] Y. Yang, X. Xiong, Y. Yan, 无人机编队轨迹规划算法：综述, Drones 7 (2023) 62。
- en: 'Liu et al. [2021] H. Liu, J. Ge, Y. Wang, J. Li, K. Ding, Z. Zhang, Z. Guo,
    W. Li, J. Lan, Multi-uav optimal mission assignment and path planning for disaster
    rescue using adaptive genetic algorithm and improved artificial bee colony method,
    in: Actuators, volume 11, MDPI, 2021, p. 4.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2021] H. Liu, J. Ge, Y. Wang, J. Li, K. Ding, Z. Zhang, Z. Guo, W.
    Li, J. Lan, 灾难救援中多无人机的最优任务分配与路径规划，采用自适应遗传算法与改进的人工蜂群算法, in: Actuators, volume 11,
    MDPI, 2021, p. 4。'
- en: Han et al. [2022] Z. Han, M. Chen, S. Shao, Q. Wu, Improved artificial bee colony
    algorithm-based path planning of unmanned autonomous helicopter using multi-strategy
    evolutionary learning, Aerospace Science and Technology 122 (2022) 107374.
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人 [2022] Z. Han, M. Chen, S. Shao, Q. Wu, 基于改进人工蜂群算法的无人自主直升机路径规划及多策略进化学习，Aerospace
    Science and Technology 122 (2022) 107374。
- en: Pan et al. [2021] Y. Pan, Y. Yang, W. Li, A deep learning trained by genetic
    algorithm to improve the efficiency of path planning for data collection with
    multi-uav, Ieee Access 9 (2021) 7994–8005.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 等人 [2021] Y. Pan, Y. Yang, W. Li, 通过遗传算法训练的深度学习以提高多 UAV 数据收集路径规划效率，IEEE
    Access 9 (2021) 7994–8005。
- en: Cui and Wang [2021] Z. Cui, Y. Wang, Uav path planning based on multi-layer
    reinforcement learning technique, Ieee Access 9 (2021) 59486–59497.
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cui 和 Wang [2021] Z. Cui, Y. Wang, 基于多层强化学习技术的 UAV 路径规划，IEEE Access 9 (2021)
    59486–59497。
- en: 'Heidari et al. [2023] A. Heidari, N. Jafari Navimipour, M. Unal, G. Zhang,
    Machine learning applications in internet-of-drones: Systematic review, recent
    deployments, and open issues, ACM Computing Surveys 55 (2023) 1–45.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heidari 等人 [2023] A. Heidari, N. Jafari Navimipour, M. Unal, G. Zhang, 无人机互联网中的机器学习应用：系统性综述、近期部署与未解问题，ACM
    计算机调查 55 (2023) 1–45。
- en: He et al. [2021] L. He, N. Aouf, B. Song, Explainable deep reinforcement learning
    for uav autonomous path planning, Aerospace science and technology 118 (2021)
    107052.
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2021] L. He, N. Aouf, B. Song, 可解释的深度强化学习在 UAV 自主路径规划中的应用，Aerospace Science
    and Technology 118 (2021) 107052。
- en: Zhu et al. [2021] B. Zhu, E. Bedeer, H. H. Nguyen, R. Barton, J. Henry, Uav
    trajectory planning in wireless sensor networks for energy consumption minimization
    by deep reinforcement learning, IEEE Transactions on Vehicular Technology 70 (2021)
    9540–9554.
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2021] B. Zhu, E. Bedeer, H. H. Nguyen, R. Barton, J. Henry, 基于深度强化学习的无线传感器网络中
    UAV 轨迹规划以最小化能量消耗，IEEE 《车辆技术汇刊》 70 (2021) 9540–9554。
- en: 'Guo et al. [2023] Y. Guo, X. Liu, Q. Jia, X. Liu, W. Zhang, Hpo-rrt*: A sampling-based
    algorithm for uav real-time path planning in a dynamic environment, Complex &
    Intelligent Systems 9 (2023) 7133–7153.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等人 [2023] Y. Guo, X. Liu, Q. Jia, X. Liu, W. Zhang, Hpo-rrt*: 一种基于采样的动态环境中
    UAV 实时路径规划算法，Complex & Intelligent Systems 9 (2023) 7133–7153。'
- en: Lin and Saripalli [2017] Y. Lin, S. Saripalli, Sampling-based path planning
    for uav collision avoidance, IEEE Transactions on Intelligent Transportation Systems
    18 (2017) 3179–3192.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 和 Saripalli [2017] Y. Lin, S. Saripalli, 基于采样的 UAV 避碰路径规划，IEEE 《智能交通系统汇刊》
    18 (2017) 3179–3192。
- en: Puente-Castro et al. [2021] A. Puente-Castro, D. Rivero, A. Pazos, E. Fernandez-Blanco,
    Using reinforcement learning in the path planning of swarms of uavs for the photographic
    capture of terrains, Engineering Proceedings 7 (2021) 32.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puente-Castro 等人 [2021] A. Puente-Castro, D. Rivero, A. Pazos, E. Fernandez-Blanco,
    在 UAV 群体路径规划中使用强化学习进行地形摄影捕捉，Engineering Proceedings 7 (2021) 32。
- en: Puente-Castro et al. [2022] A. Puente-Castro, D. Rivero, A. Pazos, E. Fernandez-Blanco,
    A review of artificial intelligence applied to path planning in uav swarms, Neural
    Computing and Applications 34 (2022) 153–170.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puente-Castro 等人 [2022] A. Puente-Castro, D. Rivero, A. Pazos, E. Fernandez-Blanco,
    UAV 群体路径规划中人工智能的应用综述，Neural Computing and Applications 34 (2022) 153–170。
- en: 'Pan et al. [2021] Z. Pan, C. Zhang, Y. Xia, H. Xiong, X. Shao, An improved
    artificial potential field method for path planning and formation control of the
    multi-uav systems, IEEE Transactions on Circuits and Systems II: Express Briefs
    69 (2021) 1129–1133.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan 等人 [2021] Z. Pan, C. Zhang, Y. Xia, H. Xiong, X. Shao, 一种改进的人工势场法用于多 UAV
    系统的路径规划和编队控制，IEEE 《电路与系统 II: 快速简报》 69 (2021) 1129–1133。'
- en: 'Zhao et al. [2021] C. Zhao, J. Liu, M. Sheng, W. Teng, Y. Zheng, J. Li, Multi-uav
    trajectory planning for energy-efficient content coverage: A decentralized learning-based
    approach, IEEE Journal on Selected Areas in Communications 39 (2021) 3193–3207.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2021] C. Zhao, J. Liu, M. Sheng, W. Teng, Y. Zheng, J. Li, 面向能源效率的多
    UAV 轨迹规划：一种基于去中心化学习的方法，IEEE 《选定通信领域期刊》 39 (2021) 3193–3207。
- en: Li et al. [2024] K. Li, X. Yan, Y. Han, Multi-mechanism swarm optimization for
    multi-uav task assignment and path planning in transmission line inspection under
    multi-wind field, Applied Soft Computing 150 (2024) 111033.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2024] K. Li, X. Yan, Y. Han, 基于多机制群体优化的多 UAV 任务分配与路径规划在多风场下的输电线路巡检中的应用，Applied
    Soft Computing 150 (2024) 111033。
- en: Fahlstrom et al. [2022] P. G. Fahlstrom, T. J. Gleason, M. H. Sadraey, Introduction
    to UAV systems, John Wiley & Sons, 2022.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fahlstrom 等人 [2022] P. G. Fahlstrom, T. J. Gleason, M. H. Sadraey, 《UAV 系统导论》，John
    Wiley & Sons, 2022。
- en: Harvey et al. [2022] C. Harvey, L. L. Gamble, C. R. Bolander, D. F. Hunsaker,
    J. J. Joo, D. J. Inman, A review of avian-inspired morphing for uav flight control,
    Progress in Aerospace Sciences 132 (2022) 100825.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harvey等人[2022] C. Harvey, L. L. Gamble, C. R. Bolander, D. F. Hunsaker, J. J.
    Joo, D. J. Inman, 《基于鸟类启发的变形技术在无人机飞行控制中的应用回顾》，《航空航天科学进展》132(2022) 100825。
- en: Mahmoodabadi and Rezaee Babak [2020] M. J. Mahmoodabadi, N. Rezaee Babak, Fuzzy
    adaptive robust proportional–integral–derivative control optimized by the multi-objective
    grasshopper optimization algorithm for a nonlinear quadrotor, Journal of Vibration
    and Control 26 (2020) 1574–1589.
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahmoodabadi和Rezaee Babak[2020] M. J. Mahmoodabadi, N. Rezaee Babak, 《基于模糊自适应鲁棒比例-积分-微分控制的非线性四旋翼无人机优化控制》，《振动与控制期刊》26(2020)
    1574-1589。
- en: 'Bello et al. [2022] A. B. Bello, F. Navarro, J. Raposo, M. Miranda, A. Zazo,
    M. Álvarez, Fixed-wing uav flight operation under harsh weather conditions: A
    case study in livingston island glaciers, antarctica, Drones 6 (2022) 384.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bello等人[2022] A. B. Bello, F. Navarro, J. Raposo, M. Miranda, A. Zazo, M. Álvarez,
    《在恶劣天气条件下固定翼无人机的飞行操作：南极利文斯顿岛冰川的案例研究》，《无人机》6(2022) 384。
- en: Koksal et al. [2020] N. Koksal, H. An, B. Fidan, Backstepping-based adaptive
    control of a quadrotor uav with guaranteed tracking performance, ISA transactions
    105 (2020) 98–110.
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koksal等人[2020] N. Koksal, H. An, B. Fidan, 《基于反步法的四旋翼无人机自适应控制及跟踪性能保证》，《ISA交易》105(2020)
    98-110。
- en: 'Zuo et al. [2022] Z. Zuo, C. Liu, Q.-L. Han, J. Song, Unmanned aerial vehicles:
    Control methods and future challenges, IEEE/CAA Journal of Automatica Sinica 9
    (2022) 601–614.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuo等人[2022] Z. Zuo, C. Liu, Q.-L. Han, J. Song, 《无人机：控制方法与未来挑战》，《IEEE/CAA自动化学报》9(2022)
    601-614。
- en: Fei et al. [2021] J. Fei, Y. Chen, L. Liu, Y. Fang, Fuzzy multiple hidden layer
    recurrent neural control of nonlinear system using terminal sliding-mode controller,
    IEEE transactions on cybernetics 52 (2021) 9519–9534.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fei等人[2021] J. Fei, Y. Chen, L. Liu, Y. Fang, 《基于端滑模控制器的非线性系统模糊多隐层递归神经网络控制》，《IEEE网络控制学报》52(2021)
    9519-9534。
- en: Gambhire et al. [2021] S. Gambhire, D. R. Kishore, P. Londhe, S. Pawar, Review
    of sliding mode based control techniques for control system applications, International
    Journal of dynamics and control 9 (2021) 363–378.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gambhire等人[2021] S. Gambhire, D. R. Kishore, P. Londhe, S. Pawar, 《滑模控制技术在控制系统中的应用综述》，《国际动力学与控制杂志》9(2021)
    363-378。
- en: Jasim and Veres [2020] O. A. Jasim, S. M. Veres, A robust controller for multi
    rotor uavs, Aerospace Science and Technology 105 (2020) 106010.
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jasim和Veres[2020] O. A. Jasim, S. M. Veres, 《一种多旋翼无人机的鲁棒控制器》，《航空航天科学与技术》105(2020)
    106010。
- en: Basiri et al. [2022] A. Basiri, V. Mariani, G. Silano, M. Aatif, L. Iannelli,
    L. Glielmo, A survey on the application of path-planning algorithms for multi-rotor
    uavs in precision agriculture, The Journal of Navigation 75 (2022) 364–383.
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Basiri等人[2022] A. Basiri, V. Mariani, G. Silano, M. Aatif, L. Iannelli, L. Glielmo,
    《多旋翼无人机路径规划算法在精准农业中的应用综述》，《航海杂志》75(2022) 364-383。
- en: Boroujeni et al. [2024] S. P. H. Boroujeni, A. Razi, S. Khoshdel, F. Afghah,
    J. L. Coen, L. O’Neill, P. Fule, A. Watts, N.-M. T. Kokolakis, K. G. Vamvoudakis,
    A comprehensive survey of research towards ai-enabled unmanned aerial systems
    in pre-, active-, and post-wildfire management, Information Fusion (2024) 102369.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boroujeni等人[2024] S. P. H. Boroujeni, A. Razi, S. Khoshdel, F. Afghah, J. L.
    Coen, L. O’Neill, P. Fule, A. Watts, N.-M. T. Kokolakis, K. G. Vamvoudakis, 《人工智能驱动的无人机系统在野火前期、活动期和后期管理中的研究综述》，《信息融合》(2024)
    102369。
- en: 'Campion et al. [2018] M. Campion, P. Ranganathan, S. Faruque, Uav swarm communication
    and control architectures: a review, Journal of Unmanned Vehicle Systems 7 (2018)
    93–106.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campion等人[2018] M. Campion, P. Ranganathan, S. Faruque, 《无人机群体通信与控制架构：综述》，《无人驾驶系统期刊》7(2018)
    93-106。
- en: 'Sharma et al. [2020] A. Sharma, P. Vanjani, N. Paliwal, C. M. W. Basnayaka,
    D. N. K. Jayakody, H.-C. Wang, P. Muthuchidambaranathan, Communication and networking
    technologies for uavs: A survey, Journal of Network and Computer Applications
    168 (2020) 102739.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sharma等人[2020] A. Sharma, P. Vanjani, N. Paliwal, C. M. W. Basnayaka, D. N.
    K. Jayakody, H.-C. Wang, P. Muthuchidambaranathan, 《无人机的通信与网络技术：一项调查》，《网络与计算机应用期刊》168(2020)
    102739。
- en: Hentati and Fourati [2020] A. I. Hentati, L. C. Fourati, Comprehensive survey
    of uavs communication networks, Computer Standards & Interfaces 72 (2020) 103451.
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hentati和Fourati[2020] A. I. Hentati, L. C. Fourati, 《无人机通信网络的综合调查》，《计算机标准与接口》72(2020)
    103451。
- en: 'Wu et al. [2021] Q. Wu, J. Xu, Y. Zeng, D. W. K. Ng, N. Al-Dhahir, R. Schober,
    A. L. Swindlehurst, A comprehensive overview on 5g-and-beyond networks with uavs:
    From communications to sensing and intelligence, IEEE Journal on Selected Areas
    in Communications 39 (2021) 2912–2945.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2021] Q. Wu, J. Xu, Y. Zeng, D. W. K. Ng, N. Al-Dhahir, R. Schober,
    A. L. Swindlehurst, 关于5G及其后续网络与无人机的全面概述：从通信到感知与智能，《IEEE选定通信领域期刊》39 (2021) 2912–2945。
- en: 'Ullah et al. [2020] Z. Ullah, F. Al-Turjman, L. Mostarda, Cognition in uav-aided
    5g and beyond communications: A survey, IEEE Transactions on Cognitive Communications
    and Networking 6 (2020) 872–891.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ullah et al. [2020] Z. Ullah, F. Al-Turjman, L. Mostarda, 无人机辅助5G及其后续通信中的认知：一项调查，《IEEE认知通信与网络期刊》6
    (2020) 872–891。
- en: 'Alladi et al. [2020] T. Alladi, V. Chamola, N. Sahu, M. Guizani, Applications
    of blockchain in unmanned aerial vehicles: A review, Vehicular Communications
    23 (2020) 100249.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alladi et al. [2020] T. Alladi, V. Chamola, N. Sahu, M. Guizani, 区块链在无人机中的应用：一项综述，《车辆通信》23
    (2020) 100249。
- en: 'Kumar et al. [2021] R. Kumar, P. Kumar, R. Tripathi, G. P. Gupta, T. R. Gadekallu,
    G. Srivastava, Sp2f: A secured privacy-preserving framework for smart agricultural
    unmanned aerial vehicles, Computer Networks 187 (2021) 107819.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar et al. [2021] R. Kumar, P. Kumar, R. Tripathi, G. P. Gupta, T. R. Gadekallu,
    G. Srivastava, Sp2f：一种为智能农业无人机设计的安全隐私保护框架，《计算机网络》187 (2021) 107819。
- en: 'Messaoudi et al. [2023] K. Messaoudi, O. S. Oubbati, A. Rachedi, A. Lakas,
    T. Bendouma, N. Chaib, A survey of uav-based data collection: Challenges, solutions
    and future perspectives, Journal of network and computer applications 216 (2023)
    103670.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Messaoudi et al. [2023] K. Messaoudi, O. S. Oubbati, A. Rachedi, A. Lakas, T.
    Bendouma, N. Chaib, 无人机数据收集的调查：挑战、解决方案与未来展望，《网络与计算机应用期刊》216 (2023) 103670。
- en: Yoo et al. [2022] M. Yoo, Y. Na, H. Song, G. Kim, J. Yun, S. Kim, C. Moon, K. Jo,
    Motion estimation and hand gesture recognition-based human–uav interaction approach
    in real time, Sensors 22 (2022) 2513.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo et al. [2022] M. Yoo, Y. Na, H. Song, G. Kim, J. Yun, S. Kim, C. Moon, K.
    Jo, 基于运动估计和手势识别的实时人工-无人机交互方法，《传感器》22 (2022) 2513。
- en: 'Li et al. [2021] T. Li, J. Liu, W. Zhang, Y. Ni, W. Wang, Z. Li, Uav-human:
    A large benchmark for human behavior understanding with unmanned aerial vehicles,
    in: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    2021, pp. 16266–16275.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2021] T. Li, J. Liu, W. Zhang, Y. Ni, W. Wang, Z. Li, UAV-Human：一个用于理解人类行为的大型基准数据集，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2021，第16266–16275页。
- en: 'Sun et al. [2022] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, J. Liu,
    Human action recognition from various data modalities: A review, IEEE transactions
    on pattern analysis and machine intelligence 45 (2022) 3200–3225.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2022] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, J. Liu,
    来自多种数据模态的人类动作识别：一项综述，《IEEE模式分析与机器智能期刊》45 (2022) 3200–3225。
- en: 'Zhang et al. [2020] J. Zhang, Z. Yu, X. Wang, Y. Lyu, S. Mao, S. C. Periaswamy,
    J. Patton, X. Wang, Rfhui: An rfid based human-unmanned aerial vehicle interaction
    system in an indoor environment, Digital Communications and Networks 6 (2020)
    14–22.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2020] J. Zhang, Z. Yu, X. Wang, Y. Lyu, S. Mao, S. C. Periaswamy,
    J. Patton, X. Wang, RFHui：一种基于RFID的室内环境下人工-无人机交互系统，《数字通信与网络》6 (2020) 14–22。
- en: Deng et al. [2023] T. Deng, Z. Huo, L. Zhang, Z. Dong, L. Niu, X. Kang, X. Huang,
    A vr-based bci interactive system for uav swarm control, Biomedical Signal Processing
    and Control 85 (2023) 104944.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. [2023] T. Deng, Z. Huo, L. Zhang, Z. Dong, L. Niu, X. Kang, X. Huang,
    基于VR的脑机接口交互系统用于无人机群控制，《生物医学信号处理与控制》85 (2023) 104944。
- en: 'Xiao et al. [2024] Z. Xiao, P. Li, C. Liu, H. Gao, X. Wang, Macns: A generic
    graph neural network integrated deep reinforcement learning based multi-agent
    collaborative navigation system for dynamic trajectory planning, Information Fusion
    105 (2024) 102250.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. [2024] Z. Xiao, P. Li, C. Liu, H. Gao, X. Wang, Macns: 一种集成深度强化学习的通用图神经网络基础的多智能体协作导航系统，用于动态轨迹规划，《信息融合》105
    (2024) 102250。'
- en: Jiao et al. [2020] R. Jiao, Z. Wang, R. Chu, M. Dong, Y. Rong, W. Chou, An intuitive
    end-to-end human-uav interaction system for field exploration, Frontiers in Neurorobotics
    13 (2020) 117.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiao et al. [2020] R. Jiao, Z. Wang, R. Chu, M. Dong, Y. Rong, W. Chou, 一种直观的端到端人工-无人机交互系统用于实地探测，《神经机器人学前沿》13
    (2020) 117。
- en: Divband Soorati et al. [2021] M. Divband Soorati, J. Clark, J. Ghofrani, D. Tarapore,
    S. D. Ramchurn, Designing a user-centered interaction interface for human–swarm
    teaming, Drones 5 (2021) 131.
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Divband Soorati et al. [2021] M. Divband Soorati, J. Clark, J. Ghofrani, D.
    Tarapore, S. D. Ramchurn, 设计以用户为中心的人工-群体协作界面，《无人机》5 (2021) 131。
- en: Zheng et al. [2020] Y.-J. Zheng, Y.-C. Du, Z.-L. Su, H.-F. Ling, M.-X. Zhang,
    S.-Y. Chen, Evolutionary human-uav cooperation for transmission network restoration,
    IEEE Transactions on Industrial Informatics 17 (2020) 1648–1657.
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2020] Y.-J. Zheng, Y.-C. Du, Z.-L. Su, H.-F. Ling, M.-X. Zhang, S.-Y.
    Chen, 传输网络恢复中的进化型人机无人机合作, IEEE Transactions on Industrial Informatics 17 (2020)
    1648–1657.
- en: Lim et al. [2021] Y. Lim, N. Pongsakornsathien, A. Gardi, R. Sabatini, T. Kistan,
    N. Ezer, D. J. Bursch, Adaptive human-robot interactions for multiple unmanned
    aerial vehicles, Robotics 10 (2021) 12.
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim 等人 [2021] Y. Lim, N. Pongsakornsathien, A. Gardi, R. Sabatini, T. Kistan,
    N. Ezer, D. J. Bursch, 多无人机的自适应人机交互, Robotics 10 (2021) 12.
- en: Chang et al. [2020] W. Chang, W. Lizhen, Y. Chao, W. Zhichao, L. Han, Y. Chao,
    Coactive design of explainable agent-based task planning and deep reinforcement
    learning for human-uavs teamwork, Chinese Journal of Aeronautics 33 (2020) 2930–2945.
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 [2020] W. Chang, W. Lizhen, Y. Chao, W. Zhichao, L. Han, Y. Chao, 面向人机无人机团队合作的可解释代理任务规划与深度强化学习的共合作设计,
    Chinese Journal of Aeronautics 33 (2020) 2930–2945.
- en: Cauchard et al. [2021] J. R. Cauchard, M. Khamis, J. Garcia, M. Kljun, A. M.
    Brock, Toward a roadmap for human-drone interaction, Interactions 28 (2021) 76–81.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cauchard 等人 [2021] J. R. Cauchard, M. Khamis, J. Garcia, M. Kljun, A. M. Brock,
    面向人机无人机交互的路线图, Interactions 28 (2021) 76–81.
- en: Ribeiro et al. [2021] R. Ribeiro, J. Ramos, D. Safadinho, A. Reis, C. Rabadão,
    J. Barroso, A. Pereira, Web ar solution for uav pilot training and usability testing,
    Sensors 21 (2021) 1456.
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等人 [2021] R. Ribeiro, J. Ramos, D. Safadinho, A. Reis, C. Rabadão, J.
    Barroso, A. Pereira, 基于Web的无人机飞行员培训和可用性测试解决方案, Sensors 21 (2021) 1456.
- en: Mohiuddin et al. [2023] A. Mohiuddin, T. Taha, Y. Zweiri, D. Gan, Dual-uav payload
    transportation using optimized velocity profiles via real-time dynamic programming,
    Drones 7 (2023) 171.
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohiuddin 等人 [2023] A. Mohiuddin, T. Taha, Y. Zweiri, D. Gan, 基于实时动态规划优化的双无人机载荷运输速度轮廓,
    Drones 7 (2023) 171.
- en: 'González-Jorge et al. [2017] H. González-Jorge, J. Martínez-Sánchez, M. Bueno,
    P. Arias, Unmanned aerial systems for civil applications: A review, Drones 1 (2017)
    2.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: González-Jorge 等人 [2017] H. González-Jorge, J. Martínez-Sánchez, M. Bueno, P.
    Arias, 民用应用的无人机系统：综述, Drones 1 (2017) 2.
- en: Hadi et al. [2014] G. S. Hadi, R. Varianto, B. R. Trilaksono, A. Budiyono, Autonomous
    uav system development for payload dropping mission, Journal of Instrumentation,
    Automation and Systems 1 (2014) 72–77.
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadi 等人 [2014] G. S. Hadi, R. Varianto, B. R. Trilaksono, A. Budiyono, 用于载荷投放任务的自主无人机系统开发,
    Journal of Instrumentation, Automation and Systems 1 (2014) 72–77.
- en: Kusznir and Smoczek [2020] T. Kusznir, J. Smoczek, Sliding mode-based control
    of a uav quadrotor for suppressing the cable-suspended payload vibration, Journal
    of Control Science and Engineering 2020 (2020) 5058039.
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kusznir 和 Smoczek [2020] T. Kusznir, J. Smoczek, 基于滑模控制的无人机四旋翼电缆悬挂载荷振动抑制, Journal
    of Control Science and Engineering 2020 (2020) 5058039.
- en: Lee and Son [2020] S. Lee, H. Son, Antisway control of a multirotor with cable-suspended
    payload, IEEE Transactions on Control Systems Technology 29 (2020) 2630–2638.
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 和 Son [2020] S. Lee, H. Son, 电缆悬挂载荷的多旋翼反摆控制, IEEE Transactions on Control
    Systems Technology 29 (2020) 2630–2638.
- en: Mohammadi et al. [2020] K. Mohammadi, S. Sirouspour, A. Grivani, Control of
    multiple quad-copters with a cable-suspended payload subject to disturbances,
    IEEE/ASME Transactions on Mechatronics 25 (2020) 1709–1718.
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammadi 等人 [2020] K. Mohammadi, S. Sirouspour, A. Grivani, 受扰动影响的电缆悬挂载荷的多四旋翼控制,
    IEEE/ASME Transactions on Mechatronics 25 (2020) 1709–1718.
- en: 'Lee et al. [2021] C. Lee, S. Kim, B. Chu, A survey: Flight mechanism and mechanical
    structure of the uav, International Journal of Precision Engineering and Manufacturing
    22 (2021) 719–743.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2021] C. Lee, S. Kim, B. Chu, 一项调查：无人机的飞行机制和机械结构, International Journal
    of Precision Engineering and Manufacturing 22 (2021) 719–743.
- en: 'Zhou et al. [2020] Y. Zhou, B. Rao, W. Wang, Uav swarm intelligence: Recent
    advances and future trends, Ieee Access 8 (2020) 183856–183878.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2020] Y. Zhou, B. Rao, W. Wang, 无人机群体智能：最新进展与未来趋势, IEEE Access 8 (2020)
    183856–183878.
- en: 'Chakraborty and Kar [2017] A. Chakraborty, A. K. Kar, Swarm intelligence: A
    review of algorithms, Nature-inspired computing and optimization: Theory and applications
    (2017) 475–494.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakraborty 和 Kar [2017] A. Chakraborty, A. K. Kar, 群体智能：算法综述, 自然启发的计算与优化：理论与应用
    (2017) 475–494.
- en: Lamport [2001] L. Lamport, Paxos made simple, ACM SIGACT News (Distributed Computing
    Column) 32, 4 (Whole Number 121, December 2001) (2001) 51–58.
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lamport [2001] L. Lamport, 简化版Paxos算法, ACM SIGACT News (分布式计算专栏) 32, 4 (第121期,
    2001年12月) (2001) 51–58.
- en: 'Kennedy and Eberhart [1995] J. Kennedy, R. Eberhart, Particle swarm optimization,
    in: Proceedings of ICNN’95-international conference on neural networks, volume 4,
    ieee, 1995, pp. 1942–1948.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kennedy和Eberhart[1995] J. Kennedy, R. Eberhart, 粒子群优化，发表于：ICNN'95国际神经网络会议论文集，第4卷，IEEE，1995年，页码1942–1948。
- en: 'Jones and Matarić [2018] C. Jones, M. J. Matarić, Behavior-based coordination
    in multi-robot systems, in: Autonomous Mobile Robots, CRC Press, 2018, pp. 549–570.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jones和Matarić[2018] C. Jones, M. J. Matarić, 基于行为的多机器人系统协调，发表于：自主移动机器人，CRC出版社，2018年，页码549–570。
- en: 'Ma et al. [2022] L. Ma, B. Lin, W. Zhang, J. Tao, X. Zhu, H. Chen, A survey
    of research on the distributed cooperation method of the uav swarm based on swarm
    intelligence, in: 2022 IEEE 13th International Conference on Software Engineering
    and Service Science (ICSESS), IEEE, 2022, pp. 305–309.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma等人[2022] L. Ma, B. Lin, W. Zhang, J. Tao, X. Zhu, H. Chen, 基于群体智能的无人机群体分布式协作方法研究综述，发表于：2022年IEEE第13届国际软件工程与服务科学会议（ICSESS），IEEE，2022年，页码305–309。
- en: Schwarzrock et al. [2018] J. Schwarzrock, I. Zacarias, A. L. Bazzan, R. Q. de Araujo Fernandes,
    L. H. Moreira, E. P. de Freitas, Solving task allocation problem in multi unmanned
    aerial vehicles systems using swarm intelligence, Engineering Applications of
    Artificial Intelligence 72 (2018) 10–20.
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwarzrock等人[2018] J. Schwarzrock, I. Zacarias, A. L. Bazzan, R. Q. de Araujo
    Fernandes, L. H. Moreira, E. P. de Freitas, 使用群体智能解决多无人机系统中的任务分配问题，工程应用人工智能 72
    (2018) 10–20。
- en: Zhang and Chen [2021] X. Zhang, X. Chen, Uav task allocation based on clone
    selection algorithm, Wireless Communications and Mobile Computing 2021 (2021)
    5518927.
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang和Chen[2021] X. Zhang, X. Chen, 基于克隆选择算法的无人机任务分配，无线通信与移动计算 2021 (2021) 5518927。
- en: Kudo and Cai [2023] F. Kudo, K. Cai, A tsp-based online algorithm for multi-task
    multi-agent pickup and delivery, IEEE Robotics and Automation Letters (2023).
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kudo和Cai[2023] F. Kudo, K. Cai, 一种基于TSP的多任务多代理拾取与投递在线算法，IEEE机器人与自动化学报（2023）。
- en: 'Sarkar et al. [2018] C. Sarkar, H. S. Paul, A. Pal, A scalable multi-robot
    task allocation algorithm, in: 2018 IEEE International Conference on Robotics
    and Automation (ICRA), IEEE, 2018, pp. 5022–5027.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarkar等人[2018] C. Sarkar, H. S. Paul, A. Pal, 一种可扩展的多机器人任务分配算法，发表于：2018年IEEE国际机器人与自动化会议（ICRA），IEEE，2018年，页码5022–5027。
- en: 'Darrah et al. [2005] M. Darrah, W. Niland, B. Stolarik, Multiple uav dynamic
    task allocation using mixed integer linear programming in a sead mission, in:
    Infotech@ Aerospace, 2005, p. 7164.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Darrah等人[2005] M. Darrah, W. Niland, B. Stolarik, 使用混合整数线性规划进行多无人机动态任务分配的海上任务，发表于：Infotech@
    Aerospace，2005年，页码7164。
- en: Ye et al. [2020] F. Ye, J. Chen, Y. Tian, T. Jiang, Cooperative multiple task
    assignment of heterogeneous uavs using a modified genetic algorithm with multi-type-gene
    chromosome encoding strategy, Journal of intelligent & robotic systems 100 (2020)
    615–627.
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye等人[2020] F. Ye, J. Chen, Y. Tian, T. Jiang, 使用改进遗传算法和多类型基因染色体编码策略的异构无人机协作多任务分配，智能与机器人系统杂志
    100 (2020) 615–627。
- en: Han et al. [2021] S. Han, C. Fan, X. Li, X. Luo, Z. Liu, A modified genetic
    algorithm for task assignment of heterogeneous unmanned aerial vehicle system,
    Measurement and Control 54 (2021) 994–1014.
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han等人[2021] S. Han, C. Fan, X. Li, X. Luo, Z. Liu, 一种改进的遗传算法用于异构无人机系统的任务分配，测量与控制
    54 (2021) 994–1014。
- en: Yan et al. [2024] F. Yan, J. Chu, J. Hu, X. Zhu, Cooperative task allocation
    with simultaneous arrival and resource constraint for multi-uav using a genetic
    algorithm, Expert Systems with Applications 245 (2024) 123023.
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan等人[2024] F. Yan, J. Chu, J. Hu, X. Zhu, 使用遗传算法解决多无人机协作任务分配问题，考虑到同时到达和资源约束，专家系统与应用
    245 (2024) 123023。
- en: 'Jiang et al. [2017] X. Jiang, Q. Zhou, Y. Ye, Method of task assignment for
    uav based on particle swarm optimization in logistics, in: Proceedings of the
    2017 international conference on intelligent systems, metaheuristics & swarm intelligence,
    2017, pp. 113–117.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人[2017] X. Jiang, Q. Zhou, Y. Ye, 基于粒子群优化的无人机任务分配方法在物流中的应用，发表于：2017年国际智能系统、元启发式与群体智能会议论文集，2017年，页码113–117。
- en: 'Gao et al. [2018] Y. Gao, Y. Zhang, S. Zhu, Y. Sun, Multi-uav task allocation
    based on improved algorithm of multi-objective particle swarm optimization, in:
    2018 International conference on cyber-enabled distributed computing and knowledge
    discovery (CyberC), IEEE, 2018, pp. 443–4437.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人[2018] Y. Gao, Y. Zhang, S. Zhu, Y. Sun, 基于改进的多目标粒子群优化算法的多无人机任务分配，发表于：2018年国际网络支持的分布式计算与知识发现会议（CyberC），IEEE，2018年，页码443–4437。
- en: Choi et al. [2010] H.-J. Choi, J.-B. Seo, Y.-D. Kim, Task assignment of multiple
    uavs using milp and ga, Journal of the Korean Society for Aeronautical & Space
    Sciences 38 (2010) 427–436.
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi等人[2010] H.-J. Choi, J.-B. Seo, Y.-D. Kim, 基于MILP和GA的多无人机任务分配, 韩国航空航天学会学报
    38（2010）427-436。
- en: Yang et al. [2019] J. Yang, X. You, G. Wu, M. M. Hassan, A. Almogren, J. Guna,
    Application of reinforcement learning in uav cluster task scheduling, Future generation
    computer systems 95 (2019) 140–148.
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人[2019] J. Yang, X. You, G. Wu, M. M. Hassan, A. Almogren, J. Guna, 强化学习在无人机集群任务调度中的应用,
    未来一代计算机系统 95（2019）140-148。
- en: Yin et al. [2022] Y. Yin, Y. Guo, Q. Su, Z. Wang, Task allocation of multiple
    unmanned aerial vehicles based on deep transfer reinforcement learning, Drones
    6 (2022) 215.
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin等人[2022] Y. Yin, Y. Guo, Q. Su, Z. Wang, 基于深度迁移强化学习的多无人机任务分配, Drones 6（2022）215。
- en: Peng et al. [2021] Q. Peng, H. Wu, R. Xue, Review of dynamic task allocation
    methods for uav swarms oriented to ground targets, Complex System Modeling and
    Simulation 1 (2021) 163–175.
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng等人[2021] Q. Peng, H. Wu, R. Xue, 面向地面目标的无人机集群动态任务分配方法综述, 复杂系统建模与仿真 1（2021）163-175。
- en: Skaltsis et al. [2023] G. M. Skaltsis, H.-S. Shin, A. Tsourdos, A review of
    task allocation methods for uavs, Journal of Intelligent & Robotic Systems 109
    (2023) 76.
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skaltsis等人[2023] G. M. Skaltsis, H.-S. Shin, A. Tsourdos, 无人机任务分配方法综述, 智能与机器人系统杂志
    109（2023）76。
- en: 'Cheng et al. [2016] Q. Cheng, D. Yin, J. Yang, L. Shen, An auction-based multiple
    constraints task allocation algorithm for multi-uav system, in: 2016 International
    Conference on Cybernetics, Robotics and Control (CRC), IEEE, 2016, pp. 1–5.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等人[2016] Q. Cheng, D. Yin, J. Yang, L. Shen, 一种基于拍卖的多约束任务分配算法用于多无人机系统,
    收录于：2016年国际控制、机器人与网络学会议（CRC），IEEE，2016，第1-5页。
- en: Duan et al. [2019] X. Duan, H. Liu, H. Tang, Q. Cai, F. Zhang, X. Han, A novel
    hybrid auction algorithm for multi-uavs dynamic task assignment, IEEE access 8
    (2019) 86207–86222.
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan等人[2019] X. Duan, H. Liu, H. Tang, Q. Cai, F. Zhang, X. Han, 一种新型的混合拍卖算法用于多无人机动态任务分配,
    IEEE Access 8（2019）86207-86222。
- en: Zhang et al. [2022] Z. Zhang, H. Liu, G. Wu, A dynamic task scheduling method
    for multiple uavs based on contract net protocol, Sensors 22 (2022) 4486.
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人[2022] Z. Zhang, H. Liu, G. Wu, 一种基于合同网协议的多无人机动态任务调度方法, Sensors 22（2022）4486。
- en: Wang et al. [2023] G. Wang, X. Lv, X. Yan, A two-stage distributed task assignment
    algorithm based on contract net protocol for multi-uav cooperative reconnaissance
    task reassignment in dynamic environments, Sensors 23 (2023) 7980.
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人[2023] G. Wang, X. Lv, X. Yan, 基于合同网协议的两阶段分布式任务分配算法，用于动态环境中多无人机协同侦察任务重新分配,
    Sensors 23（2023）7980。
- en: 'Campion et al. [2018] M. Campion, P. Ranganathan, S. Faruque, A review and
    future directions of uav swarm communication architectures, in: 2018 IEEE international
    conference on electro/information technology (EIT), IEEE, 2018, pp. 0903–0908.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Campion等人[2018] M. Campion, P. Ranganathan, S. Faruque, 无人机集群通信架构的综述与未来发展方向,
    收录于：2018年IEEE国际电气/信息技术会议（EIT），IEEE，2018，第0903-0908页。
- en: 'Bekmezci et al. [2013] I. Bekmezci, O. K. Sahingoz, Ş. Temel, Flying ad-hoc
    networks (fanets): A survey, Ad Hoc Networks 11 (2013) 1254–1270.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bekmezci等人[2013] I. Bekmezci, O. K. Sahingoz, Ş. Temel, 飞行自组织网络（FANETs）：综述,
    自组织网络 11（2013）1254-1270。
- en: Javed et al. [2024] S. Javed, A. Hassan, R. Ahmad, W. Ahmed, R. Ahmed, A. Saadat,
    M. Guizani, State-of-the-art and future research challenges in uav swarms, IEEE
    Internet of Things Journal (2024).
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Javed等人[2024] S. Javed, A. Hassan, R. Ahmad, W. Ahmed, R. Ahmed, A. Saadat,
    M. Guizani, 无人机集群的最新进展及未来研究挑战, IEEE物联网期刊（2024）。
- en: 'Turker et al. [2016] T. Turker, G. Yilmaz, O. K. Sahingoz, Gpu-accelerated
    flight route planning for multi-uav systems using simulated annealing, in: Artificial
    Intelligence: Methodology, Systems, and Applications: 17th International Conference,
    AIMSA 2016, Varna, Bulgaria, September 7-10, 2016, Proceedings 17, Springer, 2016,
    pp. 279–288.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Turker等人[2016] T. Turker, G. Yilmaz, O. K. Sahingoz, 基于GPU加速的多无人机飞行路径规划使用模拟退火,
    收录于：人工智能：方法论、系统与应用：第17届国际会议AIMSA 2016，保加利亚瓦尔纳，2016年9月7-10日，会议录 17，Springer，2016，第279-288页。
- en: 'Wei and Wei [2009] L. Wei, Z. Wei, Path planning of uavs swarm using ant colony
    system, in: 2009 Fifth International Conference on Natural Computation, volume 5,
    IEEE, 2009, pp. 288–292.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei和Wei[2009] L. Wei, Z. Wei, 基于蚁群算法的无人机集群路径规划, 收录于：2009年第五届自然计算国际会议，第5卷，IEEE，2009，第288-292页。
- en: 'Ragi and Mittelmann [2017] S. Ragi, H. D. Mittelmann, Mixed-integer nonlinear
    programming formulation of a uav path optimization problem, in: 2017 American
    Control Conference (ACC), IEEE, 2017, pp. 406–411.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ragi 和 Mittelmann [2017] S. Ragi, H. D. Mittelmann, 无人机路径优化问题的混合整数非线性规划形式，见：2017年美国控制会议（ACC），IEEE，2017，页406–411。
- en: Kool et al. [2018] W. Kool, H. Van Hoof, M. Welling, Attention, learn to solve
    routing problems!, arXiv preprint arXiv:1803.08475 (2018).
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kool 等人 [2018] W. Kool, H. Van Hoof, M. Welling, 注意力，学会解决路径规划问题！, arXiv预印本 arXiv:1803.08475
    (2018)。
- en: 'Xia and Yudi [2018] C. Xia, A. Yudi, Multi—uav path planning based on improved
    neural network, in: 2018 Chinese Control And Decision Conference (CCDC), IEEE,
    2018, pp. 354–359.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 和 Yudi [2018] C. Xia, A. Yudi, 基于改进神经网络的多无人机路径规划，见：2018年中国控制与决策会议（CCDC），IEEE，2018，页354–359。
- en: 'Sanna et al. [2021] G. Sanna, S. Godio, G. Guglieri, Neural network based algorithm
    for multi-uav coverage path planning, in: 2021 International Conference on Unmanned
    Aircraft Systems (ICUAS), IEEE, 2021, pp. 1210–1217.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanna 等人 [2021] G. Sanna, S. Godio, G. Guglieri, 基于神经网络的多无人机覆盖路径规划算法，见：2021年国际无人机系统大会（ICUAS），IEEE，2021，页1210–1217。
- en: 'Ouyang et al. [2023] Q. Ouyang, Z. Wu, Y. Cong, Z. Wang, Formation control
    of unmanned aerial vehicle swarms: A comprehensive review, Asian Journal of Control
    25 (2023) 570–593.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人 [2023] Q. 欧阳, Z. 吴, Y. 丛, Z. 王, 无人机群体编队控制：一项全面的综述，《亚洲控制杂志》 25 (2023) 570–593。
- en: 'Bu et al. [2024] Y. Bu, Y. Yan, Y. Yang, Advancement challenges in uav swarm
    formation control: A comprehensive review, Drones 8 (2024) 320.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bu 等人 [2024] Y. Bu, Y. Yan, Y. Yang, 无人机群体编队控制中的进展挑战：一项全面的综述，《Drones》 8 (2024)
    320。
- en: Askari et al. [2015] A. Askari, M. Mortazavi, H. Talebi, Uav formation control
    via the virtual structure approach, Journal of Aerospace Engineering 28 (2015)
    04014047.
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Askari 等人 [2015] A. Askari, M. Mortazavi, H. Talebi, 通过虚拟结构方法实现无人机编队控制，《航空工程学报》
    28 (2015) 04014047。
- en: Lewis and Tan [1997] M. A. Lewis, K.-H. Tan, High precision formation control
    of mobile robots using virtual structures, Autonomous robots 4 (1997) 387–403.
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 和 Tan [1997] M. A. Lewis, K.-H. Tan, 使用虚拟结构进行移动机器人高精度编队控制，《自动化机器人》 4 (1997)
    387–403。
- en: 'Desai et al. [1998] J. P. Desai, J. Ostrowski, V. Kumar, Controlling formations
    of multiple mobile robots, in: Proceedings. 1998 IEEE International Conference
    on Robotics and Automation (Cat. No. 98CH36146), volume 4, IEEE, 1998, pp. 2864–2869.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Desai 等人 [1998] J. P. Desai, J. Ostrowski, V. Kumar, 多个移动机器人编队控制，见：1998年IEEE国际机器人与自动化会议论文集（CAT.
    No. 98CH36146），第4卷，IEEE，1998，页2864–2869。
- en: Huang et al. [2022] H. Huang, A. V. Savkin, W. Ni, Decentralized navigation
    of a uav team for collaborative covert eavesdropping on a group of mobile ground
    nodes, IEEE Transactions on Automation Science and Engineering 19 (2022) 3932–3941.
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄等人 [2022] H. 黄, A. V. Savkin, W. Ni, 无人机团队在协同隐蔽窃听移动地面节点中的去中心化导航，《IEEE自动化科学与工程学报》
    19 (2022) 3932–3941。
- en: Sun et al. [2022] S. Sun, Y. Liu, S. Guo, G. Li, X. Yuan, Observation-driven
    multiple uav coordinated standoff target tracking based on model predictive control,
    Tsinghua Science and Technology 27 (2022) 948–963.
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2022] S. Sun, Y. Liu, S. Guo, G. Li, X. Yuan, 基于模型预测控制的观察驱动多无人机协同远程目标跟踪，《清华科技》
    27 (2022) 948–963。
- en: Duan et al. [2021] H. Duan, L. Xin, Y. Shi, Homing pigeon-inspired autonomous
    navigation system for unmanned aerial vehicles, IEEE Transactions On Aerospace
    and Electronic Systems 57 (2021) 2218–2224.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等人 [2021] H. Duan, L. Xin, Y. Shi, 灵感来自信鸽的无人机自主导航系统，《IEEE航空航天与电子系统学报》 57
    (2021) 2218–2224。
- en: Tao et al. [2023] C. Tao, R. Zhang, Z. Song, B. Wang, Y. Jin, Multi-uav formation
    control in complex conditions based on improved consistency algorithm, Drones
    7 (2023) 185.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tao 等人 [2023] C. Tao, R. Zhang, Z. Song, B. Wang, Y. Jin, 基于改进一致性算法的复杂条件下多无人机编队控制，《Drones》
    7 (2023) 185。
- en: Brown [2020] T. B. Brown, Language models are few-shot learners, arXiv preprint
    arXiv:2005.14165 (2020).
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown [2020] T. B. Brown, 语言模型是少样本学习者, arXiv预印本 arXiv:2005.14165 (2020)。
- en: Ouyang et al. [2022] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,
    P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al., Training language
    models to follow instructions with human feedback, Advances in neural information
    processing systems 35 (2022) 27730–27744.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等人 [2022] L. 欧阳, J. 吴, X. 蒋, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray, 等人，训练语言模型遵循指令并获得人类反馈，《神经信息处理系统进展》 35 (2022) 27730–27744。
- en: Achiam et al. [2023] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.
    Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical
    report, arXiv preprint arXiv:2303.08774 (2023).
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L.
    Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat 等，《GPT-4 技术报告》，arXiv
    预印本 arXiv:2303.08774 (2023)。
- en: 'Anthropic [2023] Anthropic, Model card and evaluations for claude models, 2023\.
    URL: [https://www-cdn.anthropic.com/files/4zrzovbb/website/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf](https://www-cdn.anthropic.com/files/4zrzovbb/website/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf).'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic [2023] Anthropic，《Claude 模型的模型卡和评估》，2023年。网址：[https://www-cdn.anthropic.com/files/4zrzovbb/website/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf](https://www-cdn.anthropic.com/files/4zrzovbb/website/bd2a28d2535bfb0494cc8e2a3bf135d2e7523226.pdf)。
- en: 'Anthropic [2022] Anthropic, Constitutional ai: Harmlessness from ai feedback
    (2022). URL: [https://en.wikipedia.org/wiki/Claude_%28language_model%29](https://en.wikipedia.org/wiki/Claude_%28language_model%29).'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic [2022] Anthropic，《宪法 AI：通过 AI 反馈实现无害性》（2022年）。网址：[https://en.wikipedia.org/wiki/Claude_%28language_model%29](https://en.wikipedia.org/wiki/Claude_%28language_model%29)。
- en: 'Anthropic [2023] Anthropic, Mapping the mind of a large language model, 2023\.
    URL: [https://www.anthropic.com/research/mapping-mind-language-model](https://www.anthropic.com/research/mapping-mind-language-model).'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic [2023] Anthropic，《映射大型语言模型的思维》，2023年。网址：[https://www.anthropic.com/research/mapping-mind-language-model](https://www.anthropic.com/research/mapping-mind-language-model)。
- en: Jiang et al. [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S.
    Chaplot, D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al.,
    Mistral 7b, arXiv preprint arXiv:2310.06825 (2023).
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2023] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot,
    D. d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier 等，《Mistral 7b》，arXiv
    预印本 arXiv:2310.06825 (2023)。
- en: Jiang et al. [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand, et al., Mixtral
    of experts, arXiv preprint arXiv:2401.04088 (2024).
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2024] A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,
    C. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand 等，《Mixtral
    专家系统》，arXiv 预印本 arXiv:2401.04088 (2024)。
- en: 'Chowdhery et al. [2023] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
    A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al., Palm: Scaling
    language modeling with pathways, Journal of Machine Learning Research 24 (2023)
    1–113.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等人 [2023] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,
    A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann 等，《Palm：通过路径扩展语言建模》，《机器学习研究期刊》24
    (2023) 1–113。
- en: 'Driess et al. [2023] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery,
    B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, et al., Palm-e: An embodied
    multimodal language model, arXiv preprint arXiv:2303.03378 (2023).'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Driess 等人 [2023] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B.
    Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu 等，《Palm-e：一种具身的多模态语言模型》，arXiv 预印本
    arXiv:2303.03378 (2023)。
- en: 'Team et al. [2023] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican, et al., Gemini: a family of highly
    capable multimodal models, arXiv preprint arXiv:2312.11805 (2023).'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等人 [2023] G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut,
    J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican 等，《Gemini：一类高能力的多模态模型》，arXiv 预印本
    arXiv:2312.11805 (2023)。
- en: 'Reid et al. [2024] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap,
    J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser, et al., Gemini
    1.5: Unlocking multimodal understanding across millions of tokens of context,
    arXiv preprint arXiv:2403.05530 (2024).'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reid 等人 [2024] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap,
    J.-b. Alayrac, R. Soricut, A. Lazaridou, O. Firat, J. Schrittwieser 等，《Gemini
    1.5：解锁跨越数百万令牌的多模态理解》，arXiv 预印本 arXiv:2403.05530 (2024)。
- en: 'Touvron et al. [2023a] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.
    Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar, et al., Llama:
    Open and efficient foundation language models, arXiv preprint arXiv:2302.13971
    (2023a).'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023a] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,
    T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar 等，《Llama：开放且高效的基础语言模型》，arXiv
    预印本 arXiv:2302.13971 (2023a)。
- en: 'Touvron et al. [2023b] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open
    foundation and fine-tuned chat models, arXiv preprint arXiv:2307.09288 (2023b).'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人 [2023b] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi,
    Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale 等，《Llama 2：开放的基础和微调聊天模型》，arXiv
    预印本 arXiv:2307.09288 (2023b)。
- en: Dubey et al. [2024] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle,
    A. Letman, A. Mathur, A. Schelten, A. Yang, A. Fan, et al., The llama 3 herd of
    models, arXiv preprint arXiv:2407.21783 (2024).
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dubey 等人 [2024] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,
    A. Mathur, A. Schelten, A. Yang, A. Fan 等人，Llama 3 模型群体，arXiv 预印本 arXiv:2407.21783
    (2024)。
- en: 'Chiang et al. [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
    L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al., Vicuna: An open-source
    chatbot impressing gpt-4 with 90%* chatgpt quality, See https://vicuna. lmsys.
    org (accessed 14 April 2023) 2 (2023) 6.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等人 [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L.
    Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez 等人，Vicuna：一个开源聊天机器人，凭借 90%* ChatGPT
    的质量给 GPT-4 留下深刻印象，参见 [https://vicuna.lmsys.org](https://vicuna.lmsys.org)（访问时间：2023年4月14日）2
    (2023) 6。
- en: Bai et al. [2023] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan,
    W. Ge, Y. Han, F. Huang, et al., Qwen technical report, arXiv preprint arXiv:2309.16609
    (2023).
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai 等人 [2023] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,
    Y. Han, F. Huang 等人，Qwen 技术报告，arXiv 预印本 arXiv:2309.16609 (2023)。
- en: Yang et al. [2024] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li,
    C. Li, D. Liu, F. Huang, et al., Qwen2 technical report, arXiv preprint arXiv:2407.10671
    (2024).
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2024] A. Yang, B. Yang, B. Hui, B. Zheng, B. Yu, C. Zhou, C. Li, C.
    Li, D. Liu, F. Huang 等人，Qwen2 技术报告，arXiv 预印本 arXiv:2407.10671 (2024)。
- en: Cai et al. [2024] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen,
    Z. Chen, Z. Chen, P. Chu, et al., Internlm2 technical report, arXiv preprint arXiv:2403.17297
    (2024).
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人 [2024] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z.
    Chen, Z. Chen, P. Chu 等人，Internlm2 技术报告，arXiv 预印本 arXiv:2403.17297 (2024)。
- en: 'Zhao et al. [2023] Y. Zhao, Z. Lin, D. Zhou, Z. Huang, J. Feng, B. Kang, Bubogpt:
    Enabling visual grounding in multi-modal llms, arXiv preprint arXiv:2307.08581
    (2023).'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] Y. Zhao, Z. Lin, D. Zhou, Z. Huang, J. Feng, B. Kang，Bubogpt：在多模态
    LLMs 中启用视觉对接，arXiv 预印本 arXiv:2307.08581 (2023)。
- en: 'Du et al. [2021] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang,
    Glm: General language model pretraining with autoregressive blank infilling, arXiv
    preprint arXiv:2103.10360 (2021).'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 [2021] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, J. Tang，GLM：使用自回归空白填充进行的通用语言模型预训练，arXiv
    预印本 arXiv:2103.10360 (2021)。
- en: 'Zeng et al. [2022] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang,
    Y. Xu, W. Zheng, X. Xia, et al., Glm-130b: An open bilingual pre-trained model,
    arXiv preprint arXiv:2210.02414 (2022).'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人 [2022] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y.
    Xu, W. Zheng, X. Xia 等人，GLM-130b：一个开放的双语预训练模型，arXiv 预印本 arXiv:2210.02414 (2022)。
- en: 'GLM et al. [2024] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang,
    D. Rojas, G. Feng, H. Zhao, et al., Chatglm: A family of large language models
    from glm-130b to glm-4 all tools, arXiv preprint arXiv:2406.12793 (2024).'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GLM 等人 [2024] T. GLM, A. Zeng, B. Xu, B. Wang, C. Zhang, D. Yin, D. Zhang, D.
    Rojas, G. Feng, H. Zhao 等人，Chatglm：从 glm-130b 到 glm-4 全工具系列的大型语言模型，arXiv 预印本 arXiv:2406.12793
    (2024)。
- en: 'Bi et al. [2024] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding,
    K. Dong, Q. Du, Z. Fu, et al., Deepseek llm: Scaling open-source language models
    with longtermism, arXiv preprint arXiv:2401.02954 (2024).'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bi 等人 [2024] X. Bi, D. Chen, G. Chen, S. Chen, D. Dai, C. Deng, H. Ding, K.
    Dong, Q. Du, Z. Fu 等人，Deepseek llm：通过长期主义扩展开源语言模型，arXiv 预印本 arXiv:2401.02954 (2024)。
- en: 'Liu et al. [2024] A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr,
    C. Ruan, D. Dai, D. Guo, et al., Deepseek-v2: A strong, economical, and efficient
    mixture-of-experts language model, arXiv preprint arXiv:2405.04434 (2024).'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] A. Liu, B. Feng, B. Wang, B. Wang, B. Liu, C. Zhao, C. Dengr,
    C. Ruan, D. Dai, D. Guo 等人，Deepseek-v2：一种强大、经济且高效的专家混合语言模型，arXiv 预印本 arXiv:2405.04434
    (2024)。
- en: 'Guo et al. [2024] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen,
    X. Bi, Y. Wu, Y. Li, et al., Deepseek-coder: When the large language model meets
    programming–the rise of code intelligence, arXiv preprint arXiv:2401.14196 (2024).'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 [2024] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X.
    Bi, Y. Wu, Y. Li 等人，Deepseek-coder：当大型语言模型遇到编程——代码智能的崛起，arXiv 预印本 arXiv:2401.14196
    (2024)。
- en: 'OpenAI [2024] OpenAI, GPT-4V System Card, 2024\. URL: [https://openai.com/index/gpt-4v-system-card/](https://openai.com/index/gpt-4v-system-card/),
    accessed: 2024-11-16.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2024] OpenAI，GPT-4V 系统卡，2024年。网址：[https://openai.com/index/gpt-4v-system-card/](https://openai.com/index/gpt-4v-system-card/)，访问时间：2024年11月16日。
- en: 'Anthropic [2024] Anthropic, The claude 3 model family: Opus, sonnet, haiku,
    2024\. URL: [https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf),
    accessed on November 16, 2024.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic [2024] Anthropic，Claude 3 模型系列：Opus、Sonnet、Haiku，2024年。网址：[https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)，访问时间：2024年11月16日。
- en: Liu et al. [2024a] H. Liu, C. Li, Q. Wu, Y. J. Lee, Visual instruction tuning,
    Advances in neural information processing systems 36 (2024a).
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024a] H. Liu, C. Li, Q. Wu, Y. J. Lee, 视觉指令调优，神经信息处理系统进展 36 (2024a)。
- en: 'Liu et al. [2024b] H. Liu, C. Li, Y. Li, Y. J. Lee, Improved baselines with
    visual instruction tuning, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2024b, pp. 26296–26306.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024b] H. Liu, C. Li, Y. Li, Y. J. Lee, 改进的基线与视觉指令调优，发表于：IEEE/CVF计算机视觉与模式识别会议论文集，2024b年，页码26296–26306。
- en: 'Liu et al. [2024c] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, Y. J. Lee,
    Llava-next: Improved reasoning, ocr, and world knowledge, 2024c. URL: [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/),
    accessed: 2024-11-16.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024c] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, Y. J. Lee,
    Llava-next: 改进的推理、OCR和世界知识，2024c。网址: [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)，访问时间：2024-11-16。'
- en: 'Lin et al. [2024] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang,
    M. Ning, L. Yuan, Moe-llava: Mixture of experts for large vision-language models,
    arXiv preprint arXiv:2401.15947 (2024).'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2024] B. Lin, Z. Tang, Y. Ye, J. Cui, B. Zhu, P. Jin, J. Zhang,
    M. Ning, L. Yuan, Moe-llava: 面向大型视觉-语言模型的专家混合，arXiv预印本arXiv:2401.15947 (2024)。'
- en: 'Xu et al. [2024] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, L. Yuan, Llava-o1:
    Let vision language models reason step-by-step, arXiv preprint arXiv:2411.10440
    (2024).'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. [2024] G. Xu, P. Jin, L. Hao, Y. Song, L. Sun, L. Yuan, Llava-o1:
    让视觉语言模型逐步推理，arXiv预印本arXiv:2411.10440 (2024)。'
- en: 'Alayrac et al. [2022] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,
    Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: a visual
    language model for few-shot learning, Advances in neural information processing
    systems 35 (2022) 23716–23736.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alayrac et al. [2022] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,
    Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, 等人，Flamingo：一种面向少样本学习的视觉语言模型，神经信息处理系统进展
    35 (2022) 23716–23736。
- en: 'Li et al. [2022] J. Li, D. Li, C. Xiong, S. Hoi, Blip: Bootstrapping language-image
    pre-training for unified vision-language understanding and generation, in: International
    conference on machine learning, PMLR, 2022, pp. 12888–12900.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2022] J. Li, D. Li, C. Xiong, S. Hoi, Blip: 引导语言-图像预训练以实现统一的视觉-语言理解和生成，发表于：国际机器学习会议，PMLR，2022年，页码12888–12900。'
- en: 'Li et al. [2023] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: Bootstrapping language-image
    pre-training with frozen image encoders and large language models, in: International
    conference on machine learning, PMLR, 2023, pp. 19730–19742.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023] J. Li, D. Li, S. Savarese, S. Hoi, Blip-2: 通过冻结图像编码器和大型语言模型引导语言-图像预训练，发表于：国际机器学习会议，PMLR，2023年，页码19730–19742。'
- en: 'Dai et al. [2023] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li,
    P. Fung, S. Hoi, Instructblip: Towards general-purpose vision-language models
    with instruction tuning, 2023\. URL: [https://arxiv.org/abs/2305.06500](https://arxiv.org/abs/2305.06500).
    [arXiv:2305.06500](http://arxiv.org/abs/2305.06500).'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai et al. [2023] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B.
    Li, P. Fung, S. Hoi, Instructblip: 面向通用视觉-语言模型的指令调优，2023。网址: [https://arxiv.org/abs/2305.06500](https://arxiv.org/abs/2305.06500)。
    [arXiv:2305.06500](http://arxiv.org/abs/2305.06500)。'
- en: 'Li et al. [2025] Y. Li, C. Wang, J. Jia, Llama-vid: An image is worth 2 tokens
    in large language models, in: European Conference on Computer Vision, Springer,
    2025, pp. 323–340.'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2025] Y. Li, C. Wang, J. Jia, Llama-vid: 图像在大型语言模型中的价值是2个token，发表于：欧洲计算机视觉会议，Springer，2025年，页码323–340。'
- en: 'Kim et al. [2024] W. Kim, C. Choi, W. Lee, W. Rhee, An image grid can be worth
    a video: Zero-shot video question answering using a vlm, arXiv preprint arXiv:2403.18406
    (2024).'
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2024] W. Kim, C. Choi, W. Lee, W. Rhee, 图像网格可以胜过视频：使用视觉语言模型进行零-shot视频问答，arXiv预印本arXiv:2403.18406
    (2024)。
- en: 'Maaz et al. [2023] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt:
    Towards detailed video understanding via large vision and language models, arXiv
    preprint arXiv:2306.05424 (2023).'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Maaz et al. [2023] M. Maaz, H. Rasheed, S. Khan, F. S. Khan, Video-chatgpt:
    面向通过大型视觉和语言模型实现详细视频理解，arXiv预印本arXiv:2306.05424 (2023)。'
- en: 'Wang et al. [2024] Z. Wang, S. Yu, E. Stengel-Eskin, J. Yoon, F. Cheng, G. Bertasius,
    M. Bansal, Videotree: Adaptive tree-based video representation for llm reasoning
    on long videos, arXiv preprint arXiv:2405.19209 (2024).'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2024] Z. Wang, S. Yu, E. Stengel-Eskin, J. Yoon, F. Cheng, G.
    Bertasius, M. Bansal, Videotree: 基于适应性树的长视频推理表示，arXiv预印本arXiv:2405.19209 (2024)。'
- en: 'Zeng et al. [2021] Y. Zeng, X. Zhang, H. Li, Multi-grained vision language
    pre-training: Aligning texts with visual concepts, arXiv preprint arXiv:2111.08276
    (2021).'
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng et al. [2021] Y. Zeng, X. Zhang, H. Li, 多粒度视觉语言预训练：将文本与视觉概念对齐，arXiv预印本arXiv:2111.08276
    (2021)。
- en: 'Lu et al. [2024] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu,
    S.-C. Zhu, J. Gao, Chameleon: Plug-and-play compositional reasoning with large
    language models, Advances in Neural Information Processing Systems 36 (2024).'
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu 等人 [2024] P. Lu, B. Peng, H. Cheng, M. Galley, K.-W. Chang, Y. N. Wu, S.-C.
    Zhu, J. Gao，Chameleon：支持大型语言模型的即插即用组合推理，《神经信息处理系统进展》36期（2024）。
- en: 'Ke et al. [2024] F. Ke, Z. Cai, S. Jahangard, W. Wang, P. D. Haghighi, H. Rezatofighi,
    Hydra: A hyper agent for dynamic compositional visual reasoning, arXiv preprint
    arXiv:2403.12884 (2024).'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ke 等人 [2024] F. Ke, Z. Cai, S. Jahangard, W. Wang, P. D. Haghighi, H. Rezatofighi，Hydra：用于动态组合视觉推理的超代理，arXiv
    预印本 arXiv:2403.12884 (2024)。
- en: 'Gupta and Kembhavi [2023] T. Gupta, A. Kembhavi, Visual programming: Compositional
    visual reasoning without training, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2023, pp. 14953–14962.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gupta 和 Kembhavi [2023] T. Gupta, A. Kembhavi，视觉编程：无需训练的组合视觉推理，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2023，页14953–14962。
- en: 'Radford et al. [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,
    S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al., Learning transferable
    visual models from natural language supervision, in: International conference
    on machine learning, PMLR, 2021, pp. 8748–8763.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2021] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
    G. Sastry, A. Askell, P. Mishkin, J. Clark 等人，学习从自然语言监督中转移的视觉模型，发表于：国际机器学习会议，PMLR，2021，页8748–8763。
- en: 'Yao et al. [2021] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang,
    Z. Li, X. Jiang, C. Xu, Filip: Fine-grained interactive language-image pre-training,
    arXiv preprint arXiv:2111.07783 (2021).'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2021] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li,
    X. Jiang, C. Xu，Filip：细粒度交互式语言-图像预训练，arXiv 预印本 arXiv:2111.07783 (2021)。
- en: 'Zhong et al. [2022] Y. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li,
    L. Zhou, X. Dai, L. Yuan, Y. Li, et al., Regionclip: Region-based language-image
    pretraining, in: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, 2022, pp. 16793–16803.'
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等人 [2022] Y. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L.
    Zhou, X. Dai, L. Yuan, Y. Li 等人，Regionclip：基于区域的语言-图像预训练，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2022，页16793–16803。
- en: 'Sun et al. [2023] Q. Sun, Y. Fang, L. Wu, X. Wang, Y. Cao, Eva-clip: Improved
    training techniques for clip at scale, arXiv preprint arXiv:2303.15389 (2023).'
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2023] Q. Sun, Y. Fang, L. Wu, X. Wang, Y. Cao, Eva-clip：大规模 CLIP 模型的改进训练技巧，arXiv
    预印本 arXiv:2303.15389 (2023)。
- en: 'Li et al. [2022] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang,
    L. Yuan, L. Zhang, J.-N. Hwang, et al., Grounded language-image pre-training,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 10965–10975.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang,
    L. Yuan, L. Zhang, J.-N. Hwang 等人，基础语言-图像预训练，发表于：IEEE/CVF 计算机视觉与模式识别会议论文集，2022，页10965–10975。
- en: 'Zhang et al. [2022] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M.
    Ni, H.-Y. Shum, Dino: Detr with improved denoising anchor boxes for end-to-end
    object detection, arXiv preprint arXiv:2203.03605 (2022).'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu, L. M. Ni,
    H.-Y. Shum，Dino：带有改进去噪锚框的 DETR 进行端到端目标检测，arXiv 预印本 arXiv:2203.03605 (2022)。
- en: 'Liu et al. [2023] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang,
    C. Li, J. Yang, H. Su, et al., Grounding dino: Marrying dino with grounded pre-training
    for open-set object detection, arXiv preprint arXiv:2303.05499 (2023).'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, Q. Jiang, C.
    Li, J. Yang, H. Su 等人，Grounding dino：将 Dino 与基础预训练结合用于开放集目标检测，arXiv 预印本 arXiv:2303.05499
    (2023)。
- en: 'Oquab et al. [2023] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec,
    V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, et al., Dinov2: Learning
    robust visual features without supervision, arXiv preprint arXiv:2304.07193 (2023).'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oquab 等人 [2023] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V.
    Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby 等人，Dinov2：无需监督的鲁棒视觉特征学习，arXiv
    预印本 arXiv:2304.07193 (2023)。
- en: 'Ranzinger et al. [2024] M. Ranzinger, G. Heinrich, J. Kautz, P. Molchanov,
    Am-radio: Agglomerative vision foundation model reduce all domains into one, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 12490–12500.'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ranzinger 等人 [2024] M. Ranzinger, G. Heinrich, J. Kautz, P. Molchanov，Am-radio：聚合视觉基础模型将所有领域合并为一个，发表于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2024，页12490–12500。
- en: 'Zhou et al. [2024] G. Zhou, H. Pan, Y. LeCun, L. Pinto, Dino-wm: World models
    on pre-trained visual features enable zero-shot planning, arXiv preprint arXiv:2411.04983
    (2024).'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2024] G. Zhou, H. Pan, Y. LeCun, L. Pinto，Dino-wm：基于预训练视觉特征的世界模型实现零-shot
    规划，arXiv 预印本 arXiv:2411.04983 (2024)。
- en: 'Cheng et al. [2024] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, Y. Shan, Yolo-world:
    Real-time open-vocabulary object detection, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 16901–16911.'
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng等人 [2024] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, Y. Shan, Yolo-world：实时开放词汇目标检测，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2024年，第16901–16911页。
- en: 'Lüddecke and Ecker [2022] T. Lüddecke, A. Ecker, Image segmentation using text
    and image prompts, in: Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition, 2022, pp. 7086–7096.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lüddecke和Ecker [2022] T. Lüddecke, A. Ecker, 使用文本和图像提示进行图像分割，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2022年，第7086–7096页。
- en: 'Kirillov et al. [2023] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,
    L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, et al., Segment anything,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023,
    pp. 4015–4026.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kirillov等人 [2023] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,
    T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, 等人，分割一切，载于：IEEE/CVF计算机视觉国际会议论文集，2023年，第4015–4026页。
- en: 'Xu et al. [2024] X. Xu, H. Chen, L. Zhao, Z. Wang, J. Zhou, J. Lu, Embodiedsam:
    Online segment any 3d thing in real time, arXiv preprint arXiv:2408.11811 (2024).'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人 [2024] X. Xu, H. Chen, L. Zhao, Z. Wang, J. Zhou, J. Lu, Embodiedsam：实时在线分割任何3D物体，arXiv预印本
    arXiv:2408.11811（2024年）。
- en: 'Zhou et al. [2024] Y. Zhou, J. Gu, T. Y. Chiang, F. Xiang, H. Su, Point-sam:
    Promptable 3d segmentation model for point clouds, arXiv preprint arXiv:2406.17741
    (2024).'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou等人 [2024] Y. Zhou, J. Gu, T. Y. Chiang, F. Xiang, H. Su, Point-sam：可提示的3D点云分割模型，arXiv预印本
    arXiv:2406.17741（2024年）。
- en: 'Yuan et al. [2025] H. Yuan, X. Li, C. Zhou, Y. Li, K. Chen, C. C. Loy, Open-vocabulary
    sam: Segment and recognize twenty-thousand classes interactively, in: European
    Conference on Computer Vision, Springer, 2025, pp. 419–437.'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等人 [2025] H. Yuan, X. Li, C. Zhou, Y. Li, K. Chen, C. C. Loy, 开放词汇SAM：互动式分割和识别两万类，载于：欧洲计算机视觉大会，Springer，2025年，第419–437页。
- en: 'Pan et al. [2025] T. Pan, L. Tang, X. Wang, S. Shan, Tokenize anything via
    prompting, in: European Conference on Computer Vision, Springer, 2025, pp. 330–348.'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等人 [2025] T. Pan, L. Tang, X. Wang, S. Shan, 通过提示进行任何事物的标记化，载于：欧洲计算机视觉大会，Springer，2025年，第330–348页。
- en: 'Xiong et al. [2024] Y. Xiong, B. Varadarajan, L. Wu, X. Xiang, F. Xiao, C. Zhu,
    X. Dai, D. Wang, F. Sun, F. Iandola, et al., Efficientsam: Leveraged masked image
    pretraining for efficient segment anything, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 16111–16121.'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiong等人 [2024] Y. Xiong, B. Varadarajan, L. Wu, X. Xiang, F. Xiao, C. Zhu, X.
    Dai, D. Wang, F. Sun, F. Iandola, 等人，Efficientsam：利用掩蔽图像预训练实现高效的分割一切，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2024年，第16111–16121页。
- en: 'Zhang et al. [2023] C. Zhang, D. Han, Y. Qiao, J. U. Kim, S.-H. Bae, S. Lee,
    C. S. Hong, Faster segment anything: Towards lightweight sam for mobile applications,
    arXiv preprint arXiv:2306.14289 (2023).'
  id: totrans-730
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人 [2023] C. Zhang, D. Han, Y. Qiao, J. U. Kim, S.-H. Bae, S. Lee, C. S.
    Hong, 更快速地分割一切：面向移动应用的轻量级SAM，arXiv预印本 arXiv:2306.14289（2023年）。
- en: 'Ravi et al. [2024] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr,
    R. Rädle, C. Rolland, L. Gustafson, et al., Sam 2: Segment anything in images
    and videos, arXiv preprint arXiv:2408.00714 (2024).'
  id: totrans-731
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravi等人 [2024] N. Ravi, V. Gabeur, Y.-T. Hu, R. Hu, C. Ryali, T. Ma, H. Khedr,
    R. Rädle, C. Rolland, L. Gustafson, 等人，SAM 2：分割图像和视频中的一切，arXiv预印本 arXiv:2408.00714（2024年）。
- en: 'Yang et al. [2024] C.-Y. Yang, H.-W. Huang, W. Chai, Z. Jiang, J.-N. Hwang,
    Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware
    memory, arXiv preprint arXiv:2411.11922 (2024).'
  id: totrans-732
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人 [2024] C.-Y. Yang, H.-W. Huang, W. Chai, Z. Jiang, J.-N. Hwang, Samurai：为零样本视觉跟踪适应分割一切模型，arXiv预印本
    arXiv:2411.11922（2024年）。
- en: 'Wang et al. [2023] X. Wang, X. Zhang, Y. Cao, W. Wang, C. Shen, T. Huang, Seggpt:
    Segmenting everything in context, arXiv preprint arXiv:2304.03284 (2023).'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人 [2023] X. Wang, X. Zhang, Y. Cao, W. Wang, C. Shen, T. Huang, Seggpt：在上下文中分割一切，arXiv预印本
    arXiv:2304.03284（2023年）。
- en: 'Yuan et al. [2024] Y. Yuan, W. Li, J. Liu, D. Tang, X. Luo, C. Qin, L. Zhang,
    J. Zhu, Osprey: Pixel understanding with visual instruction tuning, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp.
    28202–28211.'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan等人 [2024] Y. Yuan, W. Li, J. Liu, D. Tang, X. Luo, C. Qin, L. Zhang, J.
    Zhu, Osprey：通过视觉指令微调进行像素理解，载于：IEEE/CVF计算机视觉与模式识别大会论文集，2024年，第28202–28211页。
- en: Zou et al. [2024] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Wang, L. Wang,
    J. Gao, Y. J. Lee, Segment everything everywhere all at once, Advances in Neural
    Information Processing Systems 36 (2024).
  id: totrans-735
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou等人 [2024] X. Zou, J. Yang, H. Zhang, F. Li, L. Li, J. Wang, L. Wang, J. Gao,
    Y. J. Lee, 分割一切，随时随地，神经信息处理系统进展第36期（2024年）。
- en: Liu et al. [2024] Y. Liu, L. Kong, J. Cen, R. Chen, W. Zhang, L. Pan, K. Chen,
    Z. Liu, Segment any point cloud sequences by distilling vision foundation models,
    Advances in Neural Information Processing Systems 36 (2024).
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] Y. Liu, L. Kong, J. Cen, R. Chen, W. Zhang, L. Pan, K. Chen, Z.
    Liu, 通过蒸馏视觉基础模型对任何点云序列进行分割, 神经信息处理系统进展 36 (2024)。
- en: 'Lai et al. [2024] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, J. Jia,
    Lisa: Reasoning segmentation via large language model, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 9579–9589.'
  id: totrans-737
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai 等人 [2024] X. Lai, Z. Tian, Y. Chen, Y. Li, Y. Yuan, S. Liu, J. Jia, Lisa:
    通过大型语言模型推理分割, 见：IEEE/CVF 计算机视觉与模式识别会议论文集，2024，第 9579–9589 页。'
- en: 'Bhat et al. [2023] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, M. Müller, Zoedepth:
    Zero-shot transfer by combining relative and metric depth, arXiv preprint arXiv:2302.12288
    (2023).'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bhat 等人 [2023] S. F. Bhat, R. Birkl, D. Wofk, P. Wonka, M. Müller, Zoedepth:
    通过结合相对深度和度量深度实现零-shot 转移, arXiv 预印本 arXiv:2302.12288 (2023)。'
- en: 'Zhu et al. [2024] R. Zhu, C. Wang, Z. Song, L. Liu, T. Zhang, Y. Zhang, Scaledepth:
    Decomposing metric depth estimation into scale prediction and relative depth estimation,
    arXiv preprint arXiv:2407.08187 (2024).'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人 [2024] R. Zhu, C. Wang, Z. Song, L. Liu, T. Zhang, Y. Zhang, Scaledepth:
    将度量深度估计分解为尺度预测和相对深度估计, arXiv 预印本 arXiv:2407.08187 (2024)。'
- en: 'Yang et al. [2024a] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, H. Zhao, Depth
    anything: Unleashing the power of large-scale unlabeled data, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024a,
    pp. 10371–10381.'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 [2024a] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, H. Zhao, Depth
    anything: 释放大规模未标注数据的力量, 见：IEEE/CVF 计算机视觉与模式识别会议论文集，2024a，第 10371–10381 页。'
- en: Yang et al. [2024b] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, H. Zhao,
    Depth anything v2, arXiv preprint arXiv:2406.09414 (2024b).
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2024b] L. Yang, B. Kang, Z. Huang, Z. Zhao, X. Xu, J. Feng, H. Zhao,
    Depth anything v2, arXiv 预印本 arXiv:2406.09414 (2024b)。
- en: 'Bochkovskii et al. [2024] A. Bochkovskii, A. Delaunoy, H. Germain, M. Santos,
    Y. Zhou, S. R. Richter, V. Koltun, Depth pro: Sharp monocular metric depth in
    less than a second, arXiv preprint arXiv:2410.02073 (2024).'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bochkovskii 等人 [2024] A. Bochkovskii, A. Delaunoy, H. Germain, M. Santos, Y.
    Zhou, S. R. Richter, V. Koltun, Depth pro: 在不到一秒的时间内获取精确的单目度量深度, arXiv 预印本 arXiv:2410.02073
    (2024)。'
- en: Vaswani [2017] A. Vaswani, Attention is all you need, Advances in Neural Information
    Processing Systems (2017).
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani [2017] A. Vaswani, Attention is all you need, 神经信息处理系统进展 (2017)。
- en: 'Minaee et al. [2024] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher,
    X. Amatriain, J. Gao, Large language models: A survey, arXiv preprint arXiv:2402.06196
    (2024).'
  id: totrans-744
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minaee 等人 [2024] S. Minaee, T. Mikolov, N. Nikzad, M. Chenaghlu, R. Socher,
    X. Amatriain, J. Gao, 大规模语言模型：综述, arXiv 预印本 arXiv:2402.06196 (2024)。
- en: Zhao et al. [2023] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
    B. Zhang, J. Zhang, Z. Dong, et al., A survey of large language models, arXiv
    preprint arXiv:2303.18223 (2023).
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,
    B. Zhang, J. Zhang, Z. Dong 等人, 大规模语言模型综述, arXiv 预印本 arXiv:2303.18223 (2023)。
- en: Chang et al. [2024] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen,
    X. Yi, C. Wang, Y. Wang, et al., A survey on evaluation of large language models,
    ACM Transactions on Intelligent Systems and Technology 15 (2024) 1–45.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 [2024] Y. Chang, X. Wang, J. Wang, Y. Wu, L. Yang, K. Zhu, H. Chen,
    X. Yi, C. Wang, Y. Wang 等人, 关于大规模语言模型评估的综述, ACM 《智能系统与技术学报》 15 (2024) 1–45。
- en: Naveed et al. [2023] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman,
    N. Akhtar, N. Barnes, A. Mian, A comprehensive overview of large language models,
    arXiv preprint arXiv:2307.06435 (2023).
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naveed 等人 [2023] H. Naveed, A. U. Khan, S. Qiu, M. Saqib, S. Anwar, M. Usman,
    N. Akhtar, N. Barnes, A. Mian, 大规模语言模型的全面概述, arXiv 预印本 arXiv:2307.06435 (2023)。
- en: Li et al. [2023] Y. Li, B. Hui, X. Xia, J. Yang, M. Yang, L. Zhang, S. Si, J. Liu,
    T. Liu, F. Huang, et al., One shot learning as instruction data prospector for
    large language models, arXiv preprint arXiv:2312.10302 (2023).
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023] Y. Li, B. Hui, X. Xia, J. Yang, M. Yang, L. Zhang, S. Si, J. Liu,
    T. Liu, F. Huang 等人, 一次性学习作为大型语言模型的指令数据探索者, arXiv 预印本 arXiv:2312.10302 (2023)。
- en: Radford et al. [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever,
    et al., Language models are unsupervised multitask learners, OpenAI blog 1 (2019)
    9.
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 [2019] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever
    等人, 语言模型是无监督的多任务学习者, OpenAI 博客 1 (2019) 9。
- en: Liu et al. [2021] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, What
    makes good in-context examples for gpt-$3$?, arXiv preprint arXiv:2101.06804 (2021).
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2021] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, W. Chen, 什么样的上下文示例适合
    GPT-$3$?，arXiv 预印本 arXiv:2101.06804 (2021)。
- en: Dong et al. [2022] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu,
    Z. Wu, T. Liu, et al., A survey on in-context learning, arXiv preprint arXiv:2301.00234
    (2022).
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. [2022] Q. Dong, L. Li, D. Dai, C. Zheng, J. Ma, R. Li, H. Xia, J. Xu,
    Z. Wu, T. Liu, 等，一项关于上下文学习的调查，arXiv 预印本 arXiv:2301.00234 (2022)。
- en: Kojima et al. [2022] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, Y. Iwasawa, Large
    language models are zero-shot reasoners, Advances in neural information processing
    systems 35 (2022) 22199–22213.
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. [2022] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, Y. Iwasawa, 大型语言模型是零-shot
    推理器，神经信息处理系统进展 35 (2022) 22199–22213。
- en: Zhang et al. [2022] Z. Zhang, A. Zhang, M. Li, A. Smola, Automatic chain of
    thought prompting in large language models, arXiv preprint arXiv:2210.03493 (2022).
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2022] Z. Zhang, A. Zhang, M. Li, A. Smola, 大型语言模型中的自动链式思维提示，arXiv
    预印本 arXiv:2210.03493 (2022)。
- en: Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,
    Q. V. Le, D. Zhou, et al., Chain-of-thought prompting elicits reasoning in large
    language models, Advances in neural information processing systems 35 (2022) 24824–24837.
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,
    Q. V. Le, D. Zhou, 等，链式思维提示在大型语言模型中引发推理，神经信息处理系统进展 35 (2022) 24824–24837。
- en: 'Feng et al. [2024] G. Feng, B. Zhang, Y. Gu, H. Ye, D. He, L. Wang, Towards
    revealing the mystery behind chain of thought: a theoretical perspective, Advances
    in Neural Information Processing Systems 36 (2024).'
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. [2024] G. Feng, B. Zhang, Y. Gu, H. Ye, D. He, L. Wang, 揭示链式思维背后的奥秘：一种理论视角，神经信息处理系统进展
    36 (2024)。
- en: 'Shen et al. [2024] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt:
    Solving ai tasks with chatgpt and its friends in hugging face, Advances in Neural
    Information Processing Systems 36 (2024).'
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shen et al. [2024] Y. Shen, K. Song, X. Tan, D. Li, W. Lu, Y. Zhuang, Hugginggpt:
    使用 ChatGPT 及其在 Hugging Face 上的朋友解决 AI 任务，神经信息处理系统进展 36 (2024)。'
- en: 'Khot et al. [2022] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson,
    P. Clark, A. Sabharwal, Decomposed prompting: A modular approach for solving complex
    tasks, arXiv preprint arXiv:2210.02406 (2022).'
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khot et al. [2022] T. Khot, H. Trivedi, M. Finlayson, Y. Fu, K. Richardson,
    P. Clark, A. Sabharwal, 分解提示：一种解决复杂任务的模块化方法，arXiv 预印本 arXiv:2210.02406 (2022)。
- en: 'Huang et al. [2024] X. Huang, W. Liu, X. Chen, X. Wang, H. Wang, D. Lian, Y. Wang,
    R. Tang, E. Chen, Understanding the planning of llm agents: A survey, arXiv preprint
    arXiv:2402.02716 (2024).'
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2024] X. Huang, W. Liu, X. Chen, X. Wang, H. Wang, D. Lian, Y. Wang,
    R. Tang, E. Chen, 理解 LLM 代理的规划：一项调查，arXiv 预印本 arXiv:2402.02716 (2024)。
- en: 'White et al. [2024] C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain,
    R. Shwartz-Ziv, N. Jain, K. Saifullah, S. Naidu, et al., Livebench: A challenging,
    contamination-free llm benchmark, arXiv preprint arXiv:2406.19314 (2024).'
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. [2024] C. White, S. Dooley, M. Roberts, A. Pal, B. Feuer, S. Jain,
    R. Shwartz-Ziv, N. Jain, K. Saifullah, S. Naidu, 等，Livebench：一个具有挑战性且不含污染的 LLM
    基准，arXiv 预印本 arXiv:2406.19314 (2024)。
- en: 'Ma et al. [2024] X. Ma, Y. Bhalgat, B. Smart, S. Chen, X. Li, J. Ding, J. Gu,
    D. Z. Chen, S. Peng, J.-W. Bian, et al., When llms step into the 3d world: A survey
    and meta-analysis of 3d tasks via multi-modal large language models, arXiv preprint
    arXiv:2405.10255 (2024).'
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2024] X. Ma, Y. Bhalgat, B. Smart, S. Chen, X. Li, J. Ding, J. Gu,
    D. Z. Chen, S. Peng, J.-W. Bian, 等，当 LLM 踏入 3D 世界：通过多模态大型语言模型进行 3D 任务的调查与元分析，arXiv
    预印本 arXiv:2405.10255 (2024)。
- en: Du et al. [2022] Y. Du, Z. Liu, J. Li, W. X. Zhao, A survey of vision-language
    pre-trained models, arXiv preprint arXiv:2202.10936 (2022).
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du et al. [2022] Y. Du, Z. Liu, J. Li, W. X. Zhao, 视觉-语言预训练模型的调查，arXiv 预印本 arXiv:2202.10936
    (2022)。
- en: 'Long et al. [2022] S. Long, F. Cao, S. C. Han, H. Yang, Vision-and-language
    pretrained models: A survey, arXiv preprint arXiv:2204.07356 (2022).'
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. [2022] S. Long, F. Cao, S. C. Han, H. Yang, 视觉与语言预训练模型：一项调查，arXiv
    预印本 arXiv:2204.07356 (2022)。
- en: Zhou et al. [2022] K. Zhou, J. Yang, C. C. Loy, Z. Liu, Learning to prompt for
    vision-language models, International Journal of Computer Vision 130 (2022) 2337–2348.
  id: totrans-763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] K. Zhou, J. Yang, C. C. Loy, Z. Liu, 学习如何为视觉-语言模型设计提示，国际计算机视觉杂志
    130 (2022) 2337–2348。
- en: Yin et al. [2024] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, A survey
    on multimodal large language models, National Science Review (2024) nwae403.
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. [2024] S. Yin, C. Fu, S. Zhao, K. Li, X. Sun, T. Xu, E. Chen, 多模态大型语言模型的调查，国家科学评论
    (2024) nwae403。
- en: 'Zhang et al. [2024] J. Zhang, J. Huang, S. Jin, S. Lu, Vision-language models
    for vision tasks: A survey, IEEE Transactions on Pattern Analysis and Machine
    Intelligence (2024).'
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2024] J. Zhang, J. Huang, S. Jin, S. Lu, 视觉-语言模型在视觉任务中的应用：一项调查，IEEE
    模式分析与机器智能学报 (2024)。
- en: 'Yang et al. [2023] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, L. Wang,
    The dawn of lmms: Preliminary explorations with gpt-4v (ision), arXiv preprint
    arXiv:2309.17421 9 (2023) 1.'
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023] Z. Yang, L. Li, K. Lin, J. Wang, C.-C. Lin, Z. Liu, L. Wang,
    LMMS的曙光：与GPT-4V（视觉）的初步探索，arXiv 预印本 arXiv:2309.17421 9（2023）1。
- en: 'Islam and Moushi [2024] R. Islam, O. M. Moushi, Gpt-4o: The cutting-edge advancement
    in multimodal llm, Authorea Preprints (2024).'
  id: totrans-767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Islam 和 Moushi [2024] R. Islam, O. M. Moushi, GPT-4O：多模态LLM的前沿进展，Authorea 预印本（2024）。
- en: Latif et al. [2024] E. Latif, Y. Zhou, S. Guo, Y. Gao, L. Shi, M. Nayaaba, G. Lee,
    L. Zhang, A. Bewersdorff, L. Fang, et al., A systematic assessment of openai o1-preview
    for higher order thinking in education, arXiv preprint arXiv:2410.21287 (2024).
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Latif 等人 [2024] E. Latif, Y. Zhou, S. Guo, Y. Gao, L. Shi, M. Nayaaba, G. Lee,
    L. Zhang, A. Bewersdorff, L. Fang 等人，OpenAI O1-Preview在教育中对高阶思维的系统评估，arXiv 预印本
    arXiv:2410.21287（2024）。
- en: 'Chiang et al. [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,
    L. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna:
    An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023\. URL:
    [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等人 [2023] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L.
    Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, I. Stoica, E. P. Xing, Vicuna：一个开源聊天机器人，凭借90%*ChatGPT质量打动GPT-4，2023年。网址：[https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'Rizzoli et al. [2023] G. Rizzoli, F. Barbato, M. Caligiuri, P. Zanuttigh, Syndrone-multi-modal
    uav dataset for urban scenarios, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2023, pp. 2210–2220.'
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rizzoli 等人 [2023] G. Rizzoli, F. Barbato, M. Caligiuri, P. Zanuttigh, Syndrone-多模态无人机数据集用于城市场景，收录于：IEEE/CVF
    国际计算机视觉会议论文集，2023年，页码2210–2220。
- en: 'Carion et al. [2020] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,
    S. Zagoruyko, End-to-end object detection with transformers, in: European conference
    on computer vision, Springer, 2020, pp. 213–229.'
  id: totrans-771
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carion 等人 [2020] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov,
    S. Zagoruyko, 基于变换器的端到端物体检测，收录于：欧洲计算机视觉会议，Springer，2020年，页码213–229。
- en: 'Mou et al. [2020] L. Mou, Y. Hua, P. Jin, X. X. Zhu, Era: A data set and deep
    learning benchmark for event recognition in aerial videos [software and data sets],
    IEEE Geoscience and Remote Sensing Magazine 8 (2020) 125–133.'
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mou 等人 [2020] L. Mou, Y. Hua, P. Jin, X. X. Zhu, Era：用于空中视频事件识别的一个数据集和深度学习基准
    [软件和数据集]，IEEE 地球科学与遥感杂志 8（2020）125–133。
- en: 'Bashmal et al. [2023] L. Bashmal, Y. Bazi, M. M. Al Rahhal, M. Zuair, F. Melgani,
    Capera: Captioning events in aerial videos, Remote Sensing 15 (2023) 2139.'
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bashmal 等人 [2023] L. Bashmal, Y. Bazi, M. M. Al Rahhal, M. Zuair, F. Melgani,
    Capera：空中视频事件字幕，遥感 15（2023）2139。
- en: 'Jaisawal et al. [2024] P. K. Jaisawal, S. Papakonstantinou, V. Gollnick, Airfisheye
    dataset: A multi-model fisheye dataset for uav applications, in: 2024 IEEE International
    Conference on Robotics and Automation (ICRA), IEEE, 2024, pp. 11818–11824.'
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jaisawal 等人 [2024] P. K. Jaisawal, S. Papakonstantinou, V. Gollnick, Airfisheye
    数据集：一个用于无人机应用的多模型鱼眼数据集，收录于：2024 IEEE 国际机器人与自动化会议（ICRA），IEEE，2024年，页码11818–11824。
- en: 'Florea et al. [2021] H. Florea, V.-C. Miclea, S. Nedevschi, Wilduav: Monocular
    uav dataset for depth estimation tasks, in: 2021 IEEE 17th International Conference
    on Intelligent Computer Communication and Processing (ICCP), IEEE, 2021, pp. 291–298.'
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florea 等人 [2021] H. Florea, V.-C. Miclea, S. Nedevschi, Wilduav：单目无人机数据集用于深度估计任务，收录于：2021
    IEEE 第17届国际智能计算机通信与处理会议（ICCP），IEEE，2021年，页码291–298。
- en: 'Oh et al. [2011] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T.
    Lee, S. Mukherjee, J. Aggarwal, H. Lee, L. Davis, et al., A large-scale benchmark
    dataset for event recognition in surveillance video, in: CVPR 2011, IEEE, 2011,
    pp. 3153–3160.'
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oh 等人 [2011] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee,
    S. Mukherjee, J. Aggarwal, H. Lee, L. Davis 等人，面向监控视频的事件识别的大规模基准数据集，收录于：CVPR 2011，IEEE，2011年，页码3153–3160。
- en: 'Zhang et al. [2022] C. Zhang, G. Huang, L. Liu, S. Huang, Y. Yang, X. Wan,
    S. Ge, D. Tao, Webuav-3m: A benchmark for unveiling the power of million-scale
    deep uav tracking, IEEE Transactions on Pattern Analysis and Machine Intelligence
    45 (2022) 9186–9205.'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] C. Zhang, G. Huang, L. Liu, S. Huang, Y. Yang, X. Wan, S. Ge,
    D. Tao, Webuav-3M：一个揭示百万规模深度无人机追踪能力的基准，IEEE 图案分析与机器智能交易 45（2022）9186–9205。
- en: Li et al. [2022] B. Li, C. Fu, F. Ding, J. Ye, F. Lin, All-day object tracking
    for unmanned aerial vehicle, IEEE Transactions on Mobile Computing 22 (2022) 4515–4529.
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022] B. Li, C. Fu, F. Ding, J. Ye, F. Lin, 全天候物体追踪用于无人驾驶飞行器，IEEE 移动计算交易
    22（2022）4515–4529。
- en: 'Zhang et al. [2022] P. Zhang, J. Zhao, D. Wang, H. Lu, X. Ruan, Visible-thermal
    uav tracking: A large-scale benchmark and new baseline, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 8886–8895.'
  id: totrans-779
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2022] P. Zhang, J. Zhao, D. Wang, H. Lu, X. Ruan，《可见光-热成像 UAV 跟踪：一个大规模基准和新基线》，载于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2022年，页8886-8895。
- en: 'Wang et al. [2021] X. Wang, X. Shu, Z. Zhang, B. Jiang, Y. Wang, Y. Tian, F. Wu,
    Towards more flexible and accurate object tracking with natural language: Algorithms
    and benchmark, in: Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, 2021, pp. 13763–13773.'
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2021] X. Wang, X. Shu, Z. Zhang, B. Jiang, Y. Wang, Y. Tian, F. Wu，《通过自然语言实现更灵活、更精确的目标跟踪：算法与基准》，载于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021年，页13763-13773。
- en: Zhang et al. [2020] S. Zhang, Q. Zhang, Y. Yang, X. Wei, P. Wang, B. Jiao, Y. Zhang,
    Person re-identification in aerial imagery, IEEE Transactions on Multimedia 23
    (2020) 281–291.
  id: totrans-781
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2020] S. Zhang, Q. Zhang, Y. Yang, X. Wei, P. Wang, B. Jiao, Y. Zhang，《航空图像中的行人重识别》，IEEE
    多媒体学报 23（2020年）281-291。
- en: 'Kristan et al. [2020] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder,
    J.-K. Kämäräinen, M. Danelljan, L. Č. Zajc, A. Lukežič, O. Drbohlav, et al., The
    eighth visual object tracking vot2020 challenge results, in: Computer Vision–ECCV
    2020 Workshops: Glasgow, UK, August 23–28, 2020, Proceedings, Part V 16, Springer,
    2020, pp. 547–601.'
  id: totrans-782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kristan 等人 [2020] M. Kristan, A. Leonardis, J. Matas, M. Felsberg, R. Pflugfelder,
    J.-K. Kämäräinen, M. Danelljan, L. Č. Zajc, A. Lukežič, O. Drbohlav 等，《第八届视觉目标跟踪
    vot2020 挑战赛结果》，载于：计算机视觉–ECCV 2020 工作坊：英国格拉斯哥，2020年8月23日至28日，会议录，第五部分16，Springer，2020年，页547-601。
- en: 'Huang et al. [2019] L. Huang, X. Zhao, K. Huang, Got-10k: A large high-diversity
    benchmark for generic object tracking in the wild, IEEE transactions on pattern
    analysis and machine intelligence 43 (2019) 1562–1577.'
  id: totrans-783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2019] L. Huang, X. Zhao, K. Huang，《Got-10k：一个高多样性的大规模野外通用目标跟踪基准》，IEEE
    模式分析与机器智能学报 43（2019年）1562-1577。
- en: 'Li and Yeung [2017] S. Li, D.-Y. Yeung, Visual object tracking for unmanned
    aerial vehicles: A benchmark and new motion models, in: Proceedings of the AAAI
    Conference on Artificial Intelligence, volume 31, 2017.'
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Yeung [2017] S. Li, D.-Y. Yeung，《无人机视觉目标跟踪：一个基准与新运动模型》，载于：AAAI 人工智能会议论文集，第31卷，2017年。
- en: 'Robicquet et al. [2016] A. Robicquet, A. Sadeghian, A. Alahi, S. Savarese,
    Learning social etiquette: Human trajectory understanding in crowded scenes, in:
    Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,
    October 11-14, 2016, Proceedings, Part VIII 14, Springer, 2016, pp. 549–565.'
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Robicquet 等人 [2016] A. Robicquet, A. Sadeghian, A. Alahi, S. Savarese，《学习社交礼仪：在人群场景中的人类轨迹理解》，载于：计算机视觉–ECCV
    2016：第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11日至14日，会议录，第八部分14，Springer，2016年，页549-565。
- en: 'Mundhenk et al. [2016] T. N. Mundhenk, G. Konjevod, W. A. Sakla, K. Boakye,
    A large contextual dataset for classification, detection and counting of cars
    with deep learning, in: Computer Vision–ECCV 2016: 14th European Conference, Amsterdam,
    The Netherlands, October 11-14, 2016, Proceedings, Part III 14, Springer, 2016,
    pp. 785–800.'
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mundhenk 等人 [2016] T. N. Mundhenk, G. Konjevod, W. A. Sakla, K. Boakye，《一个大型上下文数据集，用于分类、检测和计数汽车的深度学习》，载于：计算机视觉–ECCV
    2016：第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11日至14日，会议录，第三部分14，Springer，2016年，页785-800。
- en: 'Kapoor et al. [2023] S. Kapoor, A. Sharma, A. Verma, S. Singh, Aeriform in-action:
    A novel dataset for human action recognition in aerial videos, Pattern Recognition
    140 (2023) 109505.'
  id: totrans-787
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kapoor 等人 [2023] S. Kapoor, A. Sharma, A. Verma, S. Singh，《Aeriform in-action：一个新的人类动作识别数据集，用于航空视频中的动作识别》，模式识别
    140（2023年）109505。
- en: 'Corona et al. [2021] K. Corona, K. Osterdahl, R. Collins, A. Hoogs, Meva: A
    large-scale multiview, multimodal video dataset for activity detection, in: Proceedings
    of the IEEE/CVF winter conference on applications of computer vision, 2021, pp.
    1060–1068.'
  id: totrans-788
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Corona 等人 [2021] K. Corona, K. Osterdahl, R. Collins, A. Hoogs，《Meva：一个大规模多视角、多模态视频数据集，用于活动检测》，载于：IEEE/CVF
    计算机视觉应用冬季会议论文集，2021年，页1060-1068。
- en: Perera et al. [2020] A. G. Perera, Y. W. Law, T. T. Ogunwa, J. Chahl, A multiviewpoint
    outdoor dataset for human action recognition, IEEE Transactions on Human-Machine
    Systems 50 (2020) 405–413.
  id: totrans-789
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perera 等人 [2020] A. G. Perera, Y. W. Law, T. T. Ogunwa, J. Chahl，《用于人类动作识别的多视角户外数据集》，IEEE
    人机系统学报 50（2020年）405-413。
- en: 'Choi et al. [2020] J. Choi, G. Sharma, M. Chandraker, J.-B. Huang, Unsupervised
    and semi-supervised domain adaptation for action recognition from drones, in:
    Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,
    2020, pp. 1717–1726.'
  id: totrans-790
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等人 [2020] J. Choi, G. Sharma, M. Chandraker, J.-B. Huang, 无监督与半监督领域适应用于无人机动作识别，载于：IEEE/CVF
    冬季计算机视觉应用会议论文集，2020，页 1717–1726。
- en: 'Perera et al. [2019] A. G. Perera, Y. W. Law, J. Chahl, Drone-action: An outdoor
    recorded drone video dataset for action recognition, Drones 3 (2019) 82.'
  id: totrans-791
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perera 等人 [2019] A. G. Perera, Y. W. Law, J. Chahl, Drone-action：一个用于动作识别的户外录制无人机视频数据集，Drones
    3（2019）82。
- en: 'Perera et al. [2018] A. G. Perera, Y. Wei Law, J. Chahl, Uav-gesture: A dataset
    for uav control and gesture recognition, in: Proceedings of the European Conference
    on Computer Vision (ECCV) Workshops, 2018, pp. 0–0.'
  id: totrans-792
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perera 等人 [2018] A. G. Perera, Y. Wei Law, J. Chahl, UAV-gesture：用于无人机控制与手势识别的数据集，载于：欧洲计算机视觉会议（ECCV）研讨会论文集，2018，页
    0–0。
- en: 'Lee et al. [2024] J. Lee, T. Miyanishi, S. Kurita, K. Sakamoto, D. Azuma, Y. Matsuo,
    N. Inoue, Citynav: Language-goal aerial navigation dataset with geographic information,
    arXiv preprint arXiv:2406.14240 (2024).'
  id: totrans-793
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2024] J. Lee, T. Miyanishi, S. Kurita, K. Sakamoto, D. Azuma, Y. Matsuo,
    N. Inoue, Citynav：具有地理信息的语言目标空中导航数据集，arXiv 预印本 arXiv:2406.14240（2024）。
- en: 'Liu et al. [2023] S. Liu, H. Zhang, Y. Qi, P. Wang, Y. Zhang, Q. Wu, Aerialvln:
    Vision-and-language navigation for uavs, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2023, pp. 15384–15394.'
  id: totrans-794
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2023] S. Liu, H. Zhang, Y. Qi, P. Wang, Y. Zhang, Q. Wu, Aerialvln：无人机的视觉与语言导航，载于：IEEE/CVF
    国际计算机视觉会议论文集，2023，页 15384–15394。
- en: 'Zhu et al. [2021] S. Zhu, T. Yang, C. Chen, Vigor: Cross-view image geo-localization
    beyond one-to-one retrieval, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 3640–3649.'
  id: totrans-795
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2021] S. Zhu, T. Yang, C. Chen, Vigor：超越一对一检索的跨视角图像地理定位，载于：IEEE/CVF
    计算机视觉与模式识别会议论文集，2021，页 3640–3649。
- en: 'Zheng et al. [2020] Z. Zheng, Y. Wei, Y. Yang, University-1652: A multi-view
    multi-source benchmark for drone-based geo-localization, in: Proceedings of the
    28th ACM international conference on Multimedia, 2020, pp. 1395–1403.'
  id: totrans-796
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等人 [2020] Z. Zheng, Y. Wei, Y. Yang, University-1652：一个基于无人机的地理定位多视角多源基准，载于：第28届
    ACM 国际多媒体会议论文集，2020，页 1395–1403。
- en: 'Yao et al. [2024] Y. Yao, S. Luo, H. Zhao, G. Deng, L. Song, Can llm substitute
    human labeling? a case study of fine-grained chinese address entity recognition
    dataset for uav delivery, in: Companion Proceedings of the ACM on Web Conference
    2024, 2024, pp. 1099–1102.'
  id: totrans-797
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2024] Y. Yao, S. Luo, H. Zhao, G. Deng, L. Song, 大型语言模型能替代人工标注吗？无人机配送的细粒度中文地址实体识别数据集案例研究，载于：2024年
    ACM 网会议伴随论文集，2024，页 1099–1102。
- en: Dai et al. [2023] M. Dai, E. Zheng, Z. Feng, L. Qi, J. Zhuang, W. Yang, Vision-based
    uav self-positioning in low-altitude urban environments, IEEE Transactions on
    Image Processing (2023).
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2023] M. Dai, E. Zheng, Z. Feng, L. Qi, J. Zhuang, W. Yang, 基于视觉的低空城市环境无人机自定位，IEEE
    图像处理学报（2023）。
- en: Schumann and Riezler [2022] R. Schumann, S. Riezler, Analyzing generalization
    of vision and language navigation to unseen outdoor areas, arXiv preprint arXiv:2203.13838
    (2022).
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schumann 和 Riezler [2022] R. Schumann, S. Riezler, 分析视觉与语言导航在未见过的户外区域的泛化，arXiv
    预印本 arXiv:2203.13838（2022）。
- en: 'Zhang et al. [2025] G. Zhang, Y. Liu, X. Yang, H. Huang, C. Huang, Trafficnight:
    An aerial multimodal benchmark for nighttime vehicle surveillance, in: European
    Conference on Computer Vision, Springer, 2025, pp. 36–48.'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2025] G. Zhang, Y. Liu, X. Yang, H. Huang, C. Huang, Trafficnight：一个用于夜间车辆监控的空中多模态基准，载于：欧洲计算机视觉会议，Springer，2025，页
    36–48。
- en: Zhu et al. [2021] P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, H. Ling, Detection
    and tracking meet drones challenge, IEEE Transactions on Pattern Analysis and
    Machine Intelligence 44 (2021) 7380–7399.
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2021] P. Zhu, L. Wen, D. Du, X. Bian, H. Fan, Q. Hu, H. Ling, 无人机检测与跟踪挑战，IEEE
    模式分析与机器智能学报 44 (2021) 7380–7399。
- en: 'Shah et al. [2018] A. P. Shah, J.-B. Lamare, T. Nguyen-Anh, A. Hauptmann, Cadp:
    A novel dataset for cctv traffic camera based accident analysis, in: 2018 15th
    IEEE International Conference on Advanced Video and Signal Based Surveillance
    (AVSS), IEEE, 2018, pp. 1–9.'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah 等人 [2018] A. P. Shah, J.-B. Lamare, T. Nguyen-Anh, A. Hauptmann, CADP：一个基于闭路电视交通摄像头的新型交通事故分析数据集，载于：2018年第15届
    IEEE 国际视频与信号基础监控会议（AVSS），IEEE，2018，页 1–9。
- en: 'Hsieh et al. [2017] M.-R. Hsieh, Y.-L. Lin, W. H. Hsu, Drone-based object counting
    by spatially regularized regional proposal network, in: Proceedings of the IEEE
    international conference on computer vision, 2017, pp. 4145–4153.'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh等人[2017] M.-R. Hsieh, Y.-L. Lin, W. H. Hsu, 基于无人机的物体计数方法，通过空间正则化区域提议网络，发表于：2017年IEEE国际计算机视觉会议论文集，页码4145–4153。
- en: 'Waqas Zamir et al. [2019] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun,
    F. Shahbaz Khan, F. Zhu, L. Shao, G.-S. Xia, X. Bai, isaid: A large-scale dataset
    for instance segmentation in aerial images, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshops, 2019, pp. 28–37.'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Waqas Zamir等人[2019] S. Waqas Zamir, A. Arora, A. Gupta, S. Khan, G. Sun, F.
    Shahbaz Khan, F. Zhu, L. Shao, G.-S. Xia, X. Bai, isaid: 用于航空影像中实例分割的大规模数据集，发表于：2019年IEEE/CVF计算机视觉与模式识别会议工作坊论文集，页码28–37。'
- en: 'Yang et al. [2018] M. Y. Yang, W. Liao, X. Li, B. Rosenhahn, Deep learning
    for vehicle detection in aerial images, in: 2018 25th IEEE International Conference
    on Image Processing (ICIP), IEEE, 2018, pp. 3079–3083.'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang等人[2018] M. Y. Yang, W. Liao, X. Li, B. Rosenhahn, 深度学习在航空影像中进行车辆检测的应用，发表于：2018年第25届IEEE图像处理国际会议（ICIP），IEEE，2018年，页码3079–3083。
- en: 'Lyu et al. [2020] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, M. Y. Yang, Uavid:
    A semantic segmentation dataset for uav imagery, ISPRS journal of photogrammetry
    and remote sensing 165 (2020) 108–119.'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lyu等人[2020] Y. Lyu, G. Vosselman, G.-S. Xia, A. Yilmaz, M. Y. Yang, Uavid:
    用于无人机影像的语义分割数据集，ISPRS测量与遥感杂志165（2020）108–119。'
- en: 'Bozcan and Kayacan [2020] I. Bozcan, E. Kayacan, Au-air: A multi-modal unmanned
    aerial vehicle dataset for low altitude traffic surveillance, in: 2020 IEEE International
    Conference on Robotics and Automation (ICRA), IEEE, 2020, pp. 8504–8510.'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bozcan和Kayacan[2020] I. Bozcan, E. Kayacan, Au-air: 一个多模态无人机数据集，用于低空交通监控，发表于：2020年IEEE国际机器人与自动化会议（ICRA），IEEE，2020年，页码8504–8510。'
- en: 'Krajewski et al. [2018] R. Krajewski, J. Bock, L. Kloeker, L. Eckstein, The
    highd dataset: A drone dataset of naturalistic vehicle trajectories on german
    highways for validation of highly automated driving systems, in: 2018 21st international
    conference on intelligent transportation systems (ITSC), IEEE, 2018, pp. 2118–2125.'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krajewski等人[2018] R. Krajewski, J. Bock, L. Kloeker, L. Eckstein, 高速数据集：一个关于德国高速公路自然车辆轨迹的无人机数据集，用于验证高度自动化驾驶系统，发表于：2018年第21届国际智能交通系统会议（ITSC），IEEE，2018年，页码2118–2125。
- en: 'Du et al. [2018] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang,
    Q. Tian, The unmanned aerial vehicle benchmark: Object detection and tracking,
    in: Proceedings of the European conference on computer vision (ECCV), 2018, pp.
    370–386.'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人[2018] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang,
    Q. Tian, 无人机基准：物体检测与跟踪，发表于：2018年欧洲计算机视觉会议（ECCV）论文集，页码370–386。
- en: 'Razakarivony and Jurie [2016] S. Razakarivony, F. Jurie, Vehicle detection
    in aerial imagery: A small target detection benchmark, Journal of Visual Communication
    and Image Representation 34 (2016) 187–203.'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Razakarivony和Jurie[2016] S. Razakarivony, F. Jurie, 航空影像中的车辆检测：小目标检测基准，视觉通信与图像表示杂志34（2016）187–203。
- en: 'Lam et al. [2018] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric,
    Y. Bulatov, B. McCord, xview: Objects in context in overhead imagery, arXiv preprint
    arXiv:1802.07856 (2018).'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lam等人[2018] D. Lam, R. Kuzma, K. McGee, S. Dooley, M. Laielli, M. Klaric, Y.
    Bulatov, B. McCord, xview: 上空影像中的情境物体，arXiv预印本arXiv:1802.07856（2018）。'
- en: 'Xia et al. [2018] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo,
    M. Datcu, M. Pelillo, L. Zhang, Dota: A large-scale dataset for object detection
    in aerial images, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 3974–3983.'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia等人[2018] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu,
    M. Pelillo, L. Zhang, Dota: 用于航空影像中物体检测的大规模数据集，发表于：2018年IEEE计算机视觉与模式识别会议论文集，页码3974–3983。'
- en: Lu et al. [2017] X. Lu, B. Wang, X. Zheng, X. Li, Exploring models and data
    for remote sensing image caption generation, IEEE Transactions on Geoscience and
    Remote Sensing 56 (2017) 2183–2195.
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu等人[2017] X. Lu, B. Wang, X. Zheng, X. Li, 探索遥感影像标题生成的模型与数据，IEEE地球科学与遥感学报56（2017）2183–2195。
- en: 'Liu et al. [2024] F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, Q. Ye, L. Fu,
    J. Zhou, Remoteclip: A vision language foundation model for remote sensing, IEEE
    Transactions on Geoscience and Remote Sensing (2024).'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu等人[2024] F. Liu, D. Chen, Z. Guan, X. Zhou, J. Zhu, Q. Ye, L. Fu, J. Zhou,
    Remoteclip: 一个用于遥感的视觉语言基础模型，IEEE地球科学与遥感学报（2024）。'
- en: 'Li et al. [2020] K. Li, G. Wan, G. Cheng, L. Meng, J. Han, Object detection
    in optical remote sensing images: A survey and a new benchmark, ISPRS journal
    of photogrammetry and remote sensing 159 (2020) 296–307.'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020] K. Li, G. Wan, G. Cheng, L. Meng, J. Han, 光学遥感图像中的物体检测：综述与新基准，ISPRS摄影测量与遥感学报
    159 (2020) 296–307。
- en: Zhang et al. [2019] Y. Zhang, Y. Yuan, Y. Feng, X. Lu, Hierarchical and robust
    convolutional neural network for very high-resolution remote sensing object detection,
    IEEE Transactions on Geoscience and Remote Sensing 57 (2019) 5535–5548.
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2019] Y. Zhang, Y. Yuan, Y. Feng, X. Lu, 层次化和鲁棒的卷积神经网络用于超高分辨率遥感物体检测，IEEE地球科学与遥感学报
    57 (2019) 5535–5548。
- en: 'Liu et al. [2017] Z. Liu, L. Yuan, L. Weng, Y. Yang, A high resolution optical
    satellite image dataset for ship recognition and some new baselines, in: International
    conference on pattern recognition applications and methods, volume 2, SciTePress,
    2017, pp. 324–331.'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2017] Z. Liu, L. Yuan, L. Weng, Y. Yang, 一个高分辨率光学卫星图像数据集用于船舶识别及一些新基准，发表于：国际模式识别应用与方法会议，第2卷，SciTePress，2017年，第324–331页。
- en: Long et al. [2017] Y. Long, Y. Gong, Z. Xiao, Q. Liu, Accurate object localization
    in remote sensing images based on convolutional neural networks, IEEE Transactions
    on Geoscience and Remote Sensing 55 (2017) 2486–2498.
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long 等人 [2017] Y. Long, Y. Gong, Z. Xiao, Q. Liu, 基于卷积神经网络的遥感图像精确物体定位，IEEE地球科学与遥感学报
    55 (2017) 2486–2498。
- en: 'Cheng et al. [2017] G. Cheng, J. Han, X. Lu, Remote sensing image scene classification:
    Benchmark and state of the art, Proceedings of the IEEE 105 (2017) 1865–1883.'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2017] G. Cheng, J. Han, X. Lu, 遥感图像场景分类：基准和最新进展，IEEE会议录 105 (2017)
    1865–1883。
- en: Cheng et al. [2014] G. Cheng, J. Han, P. Zhou, L. Guo, Multi-class geospatial
    object detection and geographic image classification based on collection of part
    detectors, ISPRS Journal of Photogrammetry and Remote Sensing 98 (2014) 119–132.
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng 等人 [2014] G. Cheng, J. Han, P. Zhou, L. Guo, 基于部分检测器集合的多类别地理空间物体检测与地理图像分类，ISPRS摄影测量与遥感学报
    98 (2014) 119–132。
- en: 'Amraoui et al. [2022] K. E. Amraoui, M. Lghoul, A. Ezzaki, L. Masmoudi, M. Hadri,
    H. Elbelrhiti, A. A. Simo, Avo-airdb: An avocado uav database for agricultural
    image segmentation and classification, Data in Brief 45 (2022) 108738.'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amraoui 等人 [2022] K. E. Amraoui, M. Lghoul, A. Ezzaki, L. Masmoudi, M. Hadri,
    H. Elbelrhiti, A. A. Simo, Avo-airdb：一个用于农业图像分割与分类的鳄梨无人机数据库，Data in Brief 45 (2022)
    108738。
- en: Raptis et al. [2023] E. K. Raptis, M. Krestenitis, K. Egglezos, O. Kypris, K. Ioannidis,
    L. Doitsidis, A. C. Kapoutsis, S. Vrochidis, I. Kompatsiaris, E. B. Kosmatopoulos,
    End-to-end precision agriculture uav-based functionalities tailored to field characteristics,
    Journal of Intelligent & Robotic Systems 107 (2023) 23.
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raptis 等人 [2023] E. K. Raptis, M. Krestenitis, K. Egglezos, O. Kypris, K. Ioannidis,
    L. Doitsidis, A. C. Kapoutsis, S. Vrochidis, I. Kompatsiaris, E. B. Kosmatopoulos,
    面向精准农业的端到端无人机功能，针对田间特征量身定制，智能与机器人系统学报 107 (2023) 23。
- en: Tetila et al. [2024] E. C. Tetila, B. L. Moro, G. Astolfi, A. B. da Costa, W. P.
    Amorim, N. A. de Souza Belete, H. Pistori, J. G. A. Barbedo, Real-time detection
    of weeds by species in soybean using uav images, Crop Protection 184 (2024) 106846.
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tetila 等人 [2024] E. C. Tetila, B. L. Moro, G. Astolfi, A. B. da Costa, W. P.
    Amorim, N. A. de Souza Belete, H. Pistori, J. G. A. Barbedo, 使用无人机图像实时检测大豆中的杂草种类，作物保护
    184 (2024) 106846。
- en: 'Ödübek and Atik [2024] E. Ödübek, M. E. Atik, Detection of asphalt pavement
    cracks with yolo architectures from unmanned aerial vehicle images, in: 2024 32nd
    Signal Processing and Communications Applications Conference (SIU), IEEE, 2024,
    pp. 1–4.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ödübek 和 Atik [2024] E. Ödübek, M. E. Atik, 基于 YOLO 架构从无人机图像中检测沥青路面裂缝，发表于：2024年第32届信号处理与通信应用会议（SIU），IEEE，2024年，第1–4页。
- en: 'Vieira e Silva et al. [2023] A. L. B. Vieira e Silva, H. de Castro Felix, F. P. M.
    Simões, V. Teichrieb, M. dos Santos, H. Santiago, V. Sgotti, H. Lott Neto, Insplad:
    A dataset and benchmark for power line asset inspection in uav images, International
    journal of remote sensing 44 (2023) 7294–7320.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vieira e Silva 等人 [2023] A. L. B. Vieira e Silva, H. de Castro Felix, F. P.
    M. Simões, V. Teichrieb, M. dos Santos, H. Santiago, V. Sgotti, H. Lott Neto,
    Insplad：一个用于无人机图像中电力线资产检查的数据集和基准，国际遥感期刊 44 (2023) 7294–7320。
- en: Mishra et al. [2020] B. Mishra, D. Garg, P. Narang, V. Mishra, Drone-surveillance
    for search and rescue in natural disaster, Computer Communications 156 (2020)
    1–10.
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等人 [2020] B. Mishra, D. Garg, P. Narang, V. Mishra, 无人机监控用于自然灾害中的搜救，计算机通信
    156 (2020) 1–10。
- en: Wang and Mahmoudian [2023] Z. Wang, N. Mahmoudian, Aerial fluvial image dataset
    for deep semantic segmentation neural networks and its benchmarks, IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing 16 (2023)
    4755–4766.
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Mahmoudian [2023] Z. Wang, N. Mahmoudian, 用于深度语义分割神经网络的空中河流图像数据集及其基准，IEEE《地球观测与遥感应用选择主题期刊》16
    (2023) 4755–4766。
- en: 'Rahnemoonfar et al. [2021] M. Rahnemoonfar, T. Chowdhury, A. Sarkar, D. Varshney,
    M. Yari, R. R. Murphy, Floodnet: A high resolution aerial imagery dataset for
    post flood scene understanding, IEEE Access 9 (2021) 89644–89654.'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahnemoonfar et al. [2021] M. Rahnemoonfar, T. Chowdhury, A. Sarkar, D. Varshney,
    M. Yari, R. R. Murphy, Floodnet：一个用于洪灾后场景理解的高分辨率空中图像数据集，《IEEE Access》9 (2021)
    89644–89654。
- en: Pan et al. [2024] L. Pan, C. Song, X. Gan, K. Xu, Y. Xie, Military image captioning
    for low-altitude uav or ugv perspectives, Drones 8 (2024) 421.
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. [2024] L. Pan, C. Song, X. Gan, K. Xu, Y. Xie, 针对低空无人机或地面车辆视角的军事图像字幕生成，《无人机》8
    (2024) 421。
- en: 'Mou et al. [2023] C. Mou, T. Liu, C. Zhu, X. Cui, Waid: A large-scale dataset
    for wildlife detection with drones, Applied Sciences 13 (2023) 10397.'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mou et al. [2023] C. Mou, T. Liu, C. Zhu, X. Cui, Waid: 一个用于野生动物检测的大规模数据集，应用科学
    13 (2023) 10397。'
- en: 'Shah et al. [2018] S. Shah, D. Dey, C. Lovett, A. Kapoor, Airsim: High-fidelity
    visual and physical simulation for autonomous vehicles, in: Field and Service
    Robotics: Results of the 11th International Conference, Springer, 2018, pp. 621–635.'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shah et al. [2018] S. Shah, D. Dey, C. Lovett, A. Kapoor, Airsim：用于自动驾驶车辆的高保真视觉与物理仿真，领域与服务机器人：第11届国际会议成果，Springer，2018，第621–635页。
- en: 'Dosovitskiy et al. [2017] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, V. Koltun,
    Carla: An open urban driving simulator, in: Conference on robot learning, PMLR,
    2017, pp. 1–16.'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy et al. [2017] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, V.
    Koltun, Carla：一个开放的城市驾驶模拟器，机器人学习会议，PMLR，2017，第1–16页。
- en: 'NVIDIA [2024] NVIDIA, Isaac sim robotics simulator, 2024\. URL: [https://developer.nvidia.com/isaac/sim](https://developer.nvidia.com/isaac/sim),
    accessed: 2024-11-01.'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2024] NVIDIA, Isaac Sim 机器人仿真器，2024。网址：[https://developer.nvidia.com/isaac/sim](https://developer.nvidia.com/isaac/sim)，访问时间：2024-11-01。
- en: 'Gao et al. [2024] C. Gao, B. Zhao, W. Zhang, J. Zhang, J. Mao, Z. Zheng, F. Man,
    J. Fang, Z. Zhou, J. Cui, X. Chen, Y. Li, Embodiedcity: A benchmark platform for
    embodied agent in real-world city environment, arXiv preprint (2024).'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. [2024] C. Gao, B. Zhao, W. Zhang, J. Zhang, J. Mao, Z. Zheng, F.
    Man, J. Fang, Z. Zhou, J. Cui, X. Chen, Y. Li, Embodiedcity: 现实城市环境中具身智能体的基准平台，arXiv预印本
    (2024)。'
- en: Zhang et al. [2021] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, Understanding
    deep learning (still) requires rethinking generalization, Communications of the
    ACM 64 (2021) 107–115.
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2021] C. Zhang, S. Bengio, M. Hardt, B. Recht, O. Vinyals, 深度学习的理解（依然）需要重新思考泛化问题，《ACM通讯》64
    (2021) 107–115。
- en: 'Crawshaw [2020] M. Crawshaw, Multi-task learning with deep neural networks:
    A survey, arXiv preprint arXiv:2009.09796 (2020).'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Crawshaw [2020] M. Crawshaw, 深度神经网络的多任务学习：一项调查，arXiv预印本 arXiv:2009.09796 (2020)。
- en: Gehrmann et al. [2019] S. Gehrmann, H. Strobelt, R. Krüger, H. Pfister, A. M.
    Rush, Visual interaction with deep learning models through collaborative semantic
    inference, IEEE transactions on visualization and computer graphics 26 (2019)
    884–894.
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gehrmann et al. [2019] S. Gehrmann, H. Strobelt, R. Krüger, H. Pfister, A. M.
    Rush, 通过协同语义推理与深度学习模型进行视觉交互，《IEEE视觉化与计算机图形学交易》26 (2019) 884–894。
- en: Li et al. [2024] H. Li, X. Liu, G. Li, A benchmark for uav-view natural language-guided
    tracking, Electronics 13 (2024) 1706.
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024] H. Li, X. Liu, G. Li, 一种无人机视角自然语言引导追踪的基准，电子学报 13 (2024) 1706。
- en: Ma et al. [2024] Z. Ma, Y. Li, R. Ma, C. Liang, Unsupervised semantic segmentation
    of high-resolution uav imagery for road scene parsing, arXiv preprint arXiv:2402.02985
    (2024).
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. [2024] Z. Ma, Y. Li, R. Ma, C. Liang, 无监督高分辨率无人机图像语义分割用于道路场景解析，arXiv预印本
    arXiv:2402.02985 (2024)。
- en: Limberg et al. [2024] C. Limberg, A. Gonçalves, B. Rigault, H. Prendinger, Leveraging
    yolo-world and gpt-4v lmms for zero-shot person detection and action recognition
    in drone imagery, arXiv preprint arXiv:2404.01571 (2024).
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Limberg et al. [2024] C. Limberg, A. Gonçalves, B. Rigault, H. Prendinger, 利用YOLO-World和GPT-4V
    LMMS进行无人机图像中的零-shot人员检测与动作识别，arXiv预印本 arXiv:2404.01571 (2024)。
- en: 'Kim et al. [2024] H. Kim, D. Lee, S. Park, Y. M. Ro, Weather-aware drone-view
    object detection via environmental context understanding, in: 2024 IEEE International
    Conference on Image Processing (ICIP), IEEE, 2024, pp. 549–555.'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2024] H. Kim, D. Lee, S. Park, Y. M. Ro, 基于环境上下文理解的天气感知无人机视角物体检测，2024
    IEEE国际图像处理大会（ICIP），IEEE，2024，第549–555页。
- en: 'Sakaino [2023] H. Sakaino, Dynamic texts from uav perspective natural images,
    in: Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023,
    pp. 2070–2081.'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaino [2023] H. Sakaino, 从无人机视角的自然图像生成动态文本，见：IEEE/CVF 国际计算机视觉大会论文集，2023，pp.
    2070–2081。
- en: 'Liang et al. [2023] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang,
    P. Vajda, D. Marculescu, Open-vocabulary semantic segmentation with mask-adapted
    clip, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2023, pp. 7061–7070.'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2023] F. Liang, B. Wu, X. Dai, K. Li, Y. Zhao, H. Zhang, P. Zhang,
    P. Vajda, D. Marculescu, 开放词汇语义分割与掩码适配的 Clip，见：IEEE/CVF 计算机视觉与模式识别大会论文集，2023，pp.
    7061–7070。
- en: Gu et al. [2021] X. Gu, T.-Y. Lin, W. Kuo, Y. Cui, Open-vocabulary object detection
    via vision and language knowledge distillation, arXiv preprint arXiv:2104.13921
    (2021).
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu 等人 [2021] X. Gu, T.-Y. Lin, W. Kuo, Y. Cui, 通过视觉和语言知识蒸馏实现开放词汇物体检测，arXiv 预印本
    arXiv:2104.13921 (2021)。
- en: 'Gong et al. [2024] Z. Gong, Z. Wei, D. Wang, X. Ma, H. Chen, Y. Jia, Y. Deng,
    Z. Ji, X. Zhu, N. Yokoya, et al., Crossearth: Geospatial vision foundation model
    for domain generalizable remote sensing semantic segmentation, arXiv preprint
    arXiv:2410.22629 (2024).'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gong 等人 [2024] Z. Gong, Z. Wei, D. Wang, X. Ma, H. Chen, Y. Jia, Y. Deng, Z.
    Ji, X. Zhu, N. Yokoya 等人，Crossearth: 用于领域可泛化遥感语义分割的地理空间视觉基础模型，arXiv 预印本 arXiv:2410.22629
    (2024)。'
- en: 'Florea and Nedevschi [2024] H. Florea, S. Nedevschi, Tandepth: Leveraging global
    dems for metric monocular depth estimation in uavs, arXiv preprint arXiv:2409.05142
    (2024).'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Florea 和 Nedevschi [2024] H. Florea, S. Nedevschi, Tandepth: 利用全球数字高程模型（DEMs）进行无人机的单目深度估计，arXiv
    预印本 arXiv:2409.05142 (2024)。'
- en: de Zarzà et al. [2023] I. de Zarzà, J. de Curtò, C. T. Calafate, Socratic video
    understanding on unmanned aerial vehicles, Procedia Computer Science 225 (2023)
    144–154.
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Zarzà 等人 [2023] I. de Zarzà, J. de Curtò, C. T. Calafate, 无人机上的苏格拉底视频理解，Procedia
    Computer Science 225 (2023) 144–154。
- en: 'Zhao et al. [2023] H. Zhao, F. Pan, H. Ping, Y. Zhou, Agent as cerebrum, controller
    as cerebellum: Implementing an embodied lmm-based agent on drones, arXiv preprint
    arXiv:2311.15033 (2023).'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] H. Zhao, F. Pan, H. Ping, Y. Zhou, 智能体作为大脑，控制器作为小脑：在无人机上实现一个基于具身大语言模型的智能体，arXiv
    预印本 arXiv:2311.15033 (2023)。
- en: 'Bazi et al. [2024] Y. Bazi, L. Bashmal, M. M. Al Rahhal, R. Ricci, F. Melgani,
    Rs-llava: A large vision-language model for joint captioning and question answering
    in remote sensing imagery, Remote Sensing 16 (2024) 1477.'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bazi 等人 [2024] Y. Bazi, L. Bashmal, M. M. Al Rahhal, R. Ricci, F. Melgani, Rs-llava：一个大型视觉-语言模型，用于遥感图像的联合图像描述和问题回答，Remote
    Sensing 16 (2024) 1477。
- en: 'Zhang et al. [2024] Z. Zhang, T. Zhao, Y. Guo, J. Yin, Rs5m and georsclip:
    A large scale vision-language dataset and a large vision-language model for remote
    sensing, IEEE Transactions on Geoscience and Remote Sensing (2024).'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] Z. Zhang, T. Zhao, Y. Guo, J. Yin, Rs5m 和 georsclip：一个大规模视觉-语言数据集和一个大型视觉-语言模型用于遥感，IEEE
    地球科学与遥感学报 (2024)。
- en: 'Zhan et al. [2024] Y. Zhan, Z. Xiong, Y. Yuan, Skyeyegpt: Unifying remote sensing
    vision-language tasks via instruction tuning with large language model, arXiv
    preprint arXiv:2401.09712 (2024).'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhan 等人 [2024] Y. Zhan, Z. Xiong, Y. Yuan, Skyeyegpt: 通过使用大语言模型指令微调统一遥感视觉-语言任务，arXiv
    预印本 arXiv:2401.09712 (2024)。'
- en: 'Zhang et al. [2024] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu,
    Z. Zhang, H. Wang, Navid: Video-based vlm plans the next step for vision-and-language
    navigation, arXiv preprint arXiv:2402.15852 (2024).'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人 [2024] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu,
    Z. Zhang, H. Wang, Navid: 基于视频的视觉-语言模型为视觉-语言导航规划下一步，arXiv 预印本 arXiv:2402.15852
    (2024)。'
- en: 'Hong et al. [2024] H. Hong, S. Wang, Z. Huang, Q. Wu, J. Liu, Why only text:
    Empowering vision-and-language navigation with multi-modal prompts, arXiv preprint
    arXiv:2406.02208 (2024).'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人 [2024] H. Hong, S. Wang, Z. Huang, Q. Wu, J. Liu, 为什么只有文本：通过多模态提示赋能视觉和语言导航，arXiv
    预印本 arXiv:2406.02208 (2024)。
- en: Gao et al. [2024] Y. Gao, Z. Wang, L. Jing, D. Wang, X. Li, B. Zhao, Aerial
    vision-and-language navigation via semantic-topo-metric representation guided
    llm reasoning, arXiv preprint arXiv:2410.08500 (2024).
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2024] Y. Gao, Z. Wang, L. Jing, D. Wang, X. Li, B. Zhao, 通过语义拓扑度量表示引导的大型语言模型推理实现空中视觉和语言导航，arXiv
    预印本 arXiv:2410.08500 (2024)。
- en: 'Wang et al. [2024] X. Wang, D. Yang, Z. Wang, H. Kwan, J. Chen, W. Wu, H. Li,
    Y. Liao, S. Liu, Towards realistic uav vision-language navigation: Platform, benchmark,
    and methodology, arXiv preprint arXiv:2410.07087 (2024).'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2024] X. Wang, D. Yang, Z. Wang, H. Kwan, J. Chen, W. Wu, H. Li, Y.
    Liao, S. Liu, 朝向现实的无人机视觉-语言导航：平台、基准和方法论，arXiv 预印本 arXiv:2410.07087 (2024)。
- en: 'Sanyal and Roy [2024] S. Sanyal, K. Roy, Asma: An adaptive safety margin algorithm
    for vision-language drone navigation via scene-aware control barrier functions,
    arXiv preprint arXiv:2409.10283 (2024).'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanyal和Roy [2024] S. Sanyal, K. Roy, Asma：一种用于视觉-语言无人机导航的自适应安全边际算法，通过场景感知控制障碍函数，arXiv预印本arXiv:2409.10283（2024）。
- en: 'Zhang et al. [2024] W. Zhang, Y. Liu, X. Wang, X. Chen, C. Gao, X. Chen, Demo
    abstract: Embodied aerial agent for city-level visual language navigation using
    large language model, in: 2024 23rd ACM/IEEE International Conference on Information
    Processing in Sensor Networks (IPSN), IEEE, 2024, pp. 265–266.'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2024] W. 张, Y. 刘, X. 王, X. 陈, C. 高, X. 陈, 演示摘要：基于体现的空中代理，用大语言模型进行城市级视觉语言导航，载于：2024年第23届ACM/IEEE传感器网络信息处理国际会议（IPSN），IEEE，2024，第265-266页。
- en: 'Chen et al. [2023] Z. Chen, J. Li, F. Fukumoto, P. Liu, Y. Suzuki, Vision-language
    navigation for quadcopters with conditional transformer and prompt-based text
    rephraser, in: Proceedings of the 5th ACM International Conference on Multimedia
    in Asia, 2023, pp. 1–7.'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023] Z. 陈, J. 李, F. 福本, P. 刘, Y. 铃木, 基于条件变换器和基于提示的文本改写器的四旋翼视觉-语言导航，载于：第5届ACM亚洲多媒体国际会议论文集，2023，第1-7页。
- en: 'Blei et al. [2024] Y. Blei, M. Krawez, N. Nilavadi, T. K. Kaiser, W. Burgard,
    Cloudtrack: Scalable uav tracking with cloud semantics, arXiv preprint arXiv:2409.16111
    (2024).'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blei等人 [2024] Y. Blei, M. Krawez, N. Nilavadi, T. K. Kaiser, W. Burgard, Cloudtrack：基于云语义的可扩展无人机跟踪，arXiv预印本arXiv:2409.16111（2024）。
- en: 'Cai et al. [2024] Z. Cai, C. R. Cardenas, K. Leo, C. Zhang, K. Backman, H. Li,
    B. Li, M. Ghorbanali, S. Datta, L. Qu, et al., Neusis: A compositional neuro-symbolic
    framework for autonomous perception, reasoning, and planning in complex uav search
    missions, arXiv preprint arXiv:2409.10196 (2024).'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等人 [2024] Z. 蔡, C. R. Cardenas, K. Leo, C. 张, K. Backman, H. 李, B. 李, M. Ghorbanali,
    S. Datta, L. Qu 等人，Neusis：一个用于自主感知、推理和复杂无人机搜索任务规划的组合神经符号框架，arXiv预印本arXiv:2409.10196（2024）。
- en: 'Döschl and Kiam [2024] B. Döschl, J. J. Kiam, Say-reapex: An llm-modulo uav
    online planning framework for search and rescue, in: 2nd CoRL Workshop on Learning
    Effective Abstractions for Planning, 2024.'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Döschl和Kiam [2024] B. Döschl, J. J. Kiam, Say-reapex：一种LLM-modulo无人机在线规划框架，用于搜索与救援，载于：第二届CoRL研讨会，学习规划的有效抽象，2024。
- en: 'Ravichandran et al. [2024] Z. Ravichandran, V. Murali, M. Tzes, G. J. Pappas,
    V. Kumar, Spine: Online semantic planning for missions with incomplete natural
    language specifications in unstructured environments, arXiv preprint arXiv:2410.03035
    (2024).'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravichandran等人 [2024] Z. Ravichandran, V. Murali, M. Tzes, G. J. Pappas, V.
    Kumar, Spine：用于不完全自然语言规格和非结构化环境任务的在线语义规划，arXiv预印本arXiv:2410.03035（2024）。
- en: 'Aikins et al. [2024] G. Aikins, M. P. Dao, K. J. Moukpe, T. C. Eskridge, K.-D.
    Nguyen, Leviosa: Natural language-based uncrewed aerial vehicle trajectory generation,
    Electronics 13 (2024) 4508.'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aikins等人 [2024] G. Aikins, M. P. Dao, K. J. Moukpe, T. C. Eskridge, K.-D. Nguyen,
    Leviosa：基于自然语言的无人机轨迹生成，电子学杂志13（2024）4508。
- en: 'Cui et al. [2024] J. Cui, G. Liu, H. Wang, Y. Yu, J. Yang, Tpml: Task planning
    for multi-uav system with large language models, in: 2024 IEEE 18th International
    Conference on Control & Automation (ICCA), IEEE, 2024, pp. 886–891.'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 崔等人 [2024] J. 崔, G. 刘, H. 王, Y. 于, J. 杨, Tpml：基于大语言模型的多无人机系统任务规划，载于：2024年IEEE第18届控制与自动化国际会议（ICCA），IEEE，2024，第886-891页。
- en: 'pen [2023] PCL. Peng Cheng Mind. Accessed: 2023-02-08, 2023\. URL: [https://openi.pcl.ac.cn/PengChengMind/PengCheng.Mind](https://openi.pcl.ac.cn/PengChengMind/PengCheng.Mind).'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pen [2023] PCL. 彭程Mind. 访问时间：2023-02-08，2023。网址：[https://openi.pcl.ac.cn/PengChengMind/PengCheng.Mind](https://openi.pcl.ac.cn/PengChengMind/PengCheng.Mind)。
- en: Liu et al. [2024] Y. Liu, Z. Zhou, J. Liu, L. Chen, J. Wang, Multi-agent formation
    control using large language models, Authorea Preprints (2024).
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人 [2024] Y. 刘, Z. 周, J. 刘, L. 陈, J. 王, 基于大语言模型的多智能体编队控制，Authorea预印本（2024）。
- en: 'Vemprala et al. [2024] S. H. Vemprala, R. Bonatti, A. Bucker, A. Kapoor, Chatgpt
    for robotics: Design principles and model abilities, IEEE Access (2024).'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vemprala等人 [2024] S. H. Vemprala, R. Bonatti, A. Bucker, A. Kapoor, ChatGPT用于机器人技术：设计原则与模型能力，IEEE
    Access（2024）。
- en: 'Zhong et al. [2024] J. Zhong, M. Li, Y. Chen, Z. Wei, F. Yang, H. Shen, A safer
    vision-based autonomous planning system for quadrotor uavs with dynamic obstacle
    trajectory prediction and its application with llms, in: Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, 2024, pp. 920–929.'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人 [2024] J. 钟, M. 李, Y. 陈, Z. 魏, F. 杨, H. 沈, 一种更安全的基于视觉的四旋翼无人机自主规划系统，结合动态障碍物轨迹预测及其与大语言模型的应用，载于：IEEE/CVF计算机视觉应用冬季会议论文集，2024，第920-929页。
- en: 'TAZIR et al. [2023] M. L. TAZIR, M. MANCAS, T. DUTOIT, From words to flight:
    Integrating openai chatgpt with px4/gazebo for natural language-based drone control,
    in: International Workshop on Computer Science and Engineering, 2023.'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TAZIR 等人 [2023] M. L. TAZIR, M. MANCAS, T. DUTOIT, 从文字到飞行：将 OpenAI ChatGPT 与
    PX4/Gazebo 集成用于基于自然语言的无人机控制，载于：2023年国际计算机科学与工程研讨会。
- en: 'Phadke et al. [2024] A. Phadke, A. Hadimlioglu, T. Chu, C. N. Sekharan, Integrating
    large language models for uav control in simulated environments: A modular interaction
    approach, arXiv preprint arXiv:2410.17602 (2024).'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Phadke 等人 [2024] A. Phadke, A. Hadimlioglu, T. Chu, C. N. Sekharan, 在模拟环境中集成大型语言模型用于无人机控制：一种模块化交互方法，arXiv
    预印本 arXiv:2410.17602 (2024)。
- en: 'Liu et al. [2024] G. Liu, T. Sun, W. Li, X. Li, X. Liu, J. Cui, Eai-sim: An
    open-source embodied ai simulation framework with large language models, in: 2024
    IEEE 18th International Conference on Control & Automation (ICCA), IEEE, 2024,
    pp. 994–999.'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 [2024] G. Liu, T. Sun, W. Li, X. Li, X. Liu, J. Cui, Eai-sim: 一个开源的具身
    AI 仿真框架，结合大型语言模型，载于：2024 IEEE 第18届国际控制与自动化会议（ICCA），IEEE，2024年，pp. 994–999。'
- en: 'Zhu et al. [2024] T. Zhu, W. Newton, S. Embury, Y. Sun, Taiist cps-uav at the
    sbft tool competition 2024, in: Proceedings of the 17th ACM/IEEE International
    Workshop on Search-Based and Fuzz Testing, 2024, pp. 51–52.'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2024] T. Zhu, W. Newton, S. Embury, Y. Sun, Taiist cps-uav 在 2024 年
    SBFT 工具竞赛中的表现，载于：第17届 ACM/IEEE 国际基于搜索与模糊测试工作坊论文集，2024年，pp. 51–52。
- en: 'Jiao et al. [2023] A. Jiao, T. P. Patel, S. Khurana, A.-M. Korol, L. Brunke,
    V. K. Adajania, U. Culha, S. Zhou, A. P. Schoellig, Swarm-gpt: Combining large
    language models with safe motion planning for robot choreography design, arXiv
    preprint arXiv:2312.01059 (2023).'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiao 等人 [2023] A. Jiao, T. P. Patel, S. Khurana, A.-M. Korol, L. Brunke, V.
    K. Adajania, U. Culha, S. Zhou, A. P. Schoellig, *Swarm-gpt*: 将大型语言模型与安全运动规划结合用于机器人编舞设计，arXiv
    预印本 arXiv:2312.01059 (2023)。'
- en: 'Lykov et al. [2024] A. Lykov, S. Karaf, M. Martynov, V. Serpiva, A. Fedoseev,
    M. Konenkov, D. Tsetserukou, Flockgpt: Guiding uav flocking with linguistic orchestration,
    arXiv preprint arXiv:2405.05872 (2024).'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lykov 等人 [2024] A. Lykov, S. Karaf, M. Martynov, V. Serpiva, A. Fedoseev, M.
    Konenkov, D. Tsetserukou, *Flockgpt*: 通过语言协调引导无人机群体，arXiv 预印本 arXiv:2405.05872
    (2024)。'
- en: 'Pueyo et al. [2024] P. Pueyo, E. Montijano, A. C. Murillo, M. Schwager, Clipswarm:
    Generating drone shows from text prompts with vision-language models, arXiv preprint
    arXiv:2403.13467 (2024).'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pueyo 等人 [2024] P. Pueyo, E. Montijano, A. C. Murillo, M. Schwager, *Clipswarm*:
    基于视觉-语言模型从文本提示生成无人机表演，arXiv 预印本 arXiv:2403.13467 (2024)。'
- en: 'Li et al. [2024] X. Li, X. Feng, S. Hu, M. Wu, D. Zhang, J. Zhang, K. Huang,
    Dtllm-vlt: Diverse text generation for visual language tracking based on llm,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 7283–7292.'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2024] X. Li, X. Feng, S. Hu, M. Wu, D. Zhang, J. Zhang, K. Huang, Dtllm-vlt:
    基于 LLM 的视觉语言追踪的多样化文本生成，载于：2024 IEEE/CVF 计算机视觉与模式识别会议论文集，2024年，pp. 7283–7292。'
- en: 'Arrabi et al. [2024] A. Arrabi, X. Zhang, W. Sultani, C. Chen, S. Wshah, Cross-view
    meets diffusion: Aerial image synthesis with geometry and text guidance, arXiv
    preprint arXiv:2408.04224 (2024).'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Arrabi 等人 [2024] A. Arrabi, X. Zhang, W. Sultani, C. Chen, S. Wshah, Cross-view
    meets diffusion: 利用几何和文本指导的航空影像合成，arXiv 预印本 arXiv:2408.04224 (2024)。'
- en: 'Yao et al. [2024] F. Yao, Y. Yue, Y. Liu, X. Sun, K. Fu, Aeroverse: Uav-agent
    benchmark suite for simulating, pre-training, finetuning, and evaluating aerospace
    embodied world models, arXiv preprint arXiv:2408.15511 (2024).'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人 [2024] F. Yao, Y. Yue, Y. Liu, X. Sun, K. Fu, Aeroverse: 用于模拟、预训练、微调和评估航空体感世界模型的无人机代理基准套件，arXiv
    预印本 arXiv:2408.15511 (2024)。'
- en: Tang et al. [2024] Y.-C. Tang, P.-Y. Chen, T.-Y. Ho, Defining and evaluating
    physical safety for large language models, arXiv preprint arXiv:2411.02317 (2024).
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang 等人 [2024] Y.-C. Tang, P.-Y. Chen, T.-Y. Ho, 为大型语言模型定义与评估物理安全，arXiv 预印本
    arXiv:2411.02317 (2024)。
- en: 'Xu et al. [2024] Y. Xu, Z. Jian, J. Zha, X. Chen, Emergency networking using
    uavs: A reinforcement learning approach with large language model, in: 2024 23rd
    ACM/IEEE International Conference on Information Processing in Sensor Networks
    (IPSN), IEEE, 2024, pp. 281–282.'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu 等人 [2024] Y. Xu, Z. Jian, J. Zha, X. Chen, 使用无人机的紧急网络：结合大型语言模型的强化学习方法，载于：2024年第23届
    ACM/IEEE 国际传感器网络信息处理会议（IPSN），IEEE，2024年，pp. 281–282。
- en: 'Xiang et al. [2024] X. Xiang, J. Xue, L. Zhao, Y. Lei, C. Yue, K. Lu, Real-time
    integration of fine-tuned large language model for improved decision-making in
    reinforcement learning, in: 2024 International Joint Conference on Neural Networks
    (IJCNN), IEEE, 2024, pp. 1–8.'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiang 等人 [2024] X. Xiang, J. Xue, L. Zhao, Y. Lei, C. Yue, K. Lu, 实时集成微调的大型语言模型以提高强化学习中的决策能力，载于：2024国际神经网络联合会议（IJCNN），IEEE，2024年，pp.
    1–8。
- en: 'Pineli Simões et al. [2024] L. E. Pineli Simões, L. Brandão Rodrigues, R. Mota Silva,
    G. Rodrigues da Silva, Evaluating voice command pipelines for drone control: From
    stt and llm to direct classification and siamese networks, arXiv e-prints (2024)
    arXiv–2407.'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pineli Simões et al. [2024] L. E. Pineli Simões, L. Brandão Rodrigues, R. Mota
    Silva, G. Rodrigues da Silva, 评估无人机控制的语音命令管道：从语音识别和大语言模型到直接分类和孪生网络，arXiv e-prints
    (2024) arXiv–2407。
- en: 'Huang et al. [2022] Y. Huang, J. Chen, D. Huang, Ufpmp-det: Toward accurate
    and efficient object detection on drone imagery, in: Proceedings of the AAAI conference
    on artificial intelligence, volume 36, 2022, pp. 1026–1033.'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2022] Y. Huang, J. Chen, D. Huang, Ufpmp-det：面向无人机影像上的精确与高效物体检测，见：AAAI
    人工智能会议论文集，第36卷，2022，页码 1026–1033。
- en: 'Zhu et al. [2021] X. Zhu, S. Lyu, X. Wang, Q. Zhao, Tph-yolov5: Improved yolov5
    based on transformer prediction head for object detection on drone-captured scenarios,
    in: Proceedings of the IEEE/CVF international conference on computer vision, 2021,
    pp. 2778–2788.'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu et al. [2021] X. Zhu, S. Lyu, X. Wang, Q. Zhao, Tph-yolov5: 基于变换器预测头的改进
    yolov5，用于无人机拍摄场景中的物体检测，见：IEEE/CVF 国际计算机视觉会议论文集，2021，页码 2778–2788。'
- en: 'Fu et al. [2023] X. Fu, G. Wei, X. Yuan, Y. Liang, Y. Bo, Efficient yolov7-drone:
    an enhanced object detection approach for drone aerial imagery, Drones 7 (2023)
    616.'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. [2023] X. Fu, G. Wei, X. Yuan, Y. Liang, Y. Bo, 高效的 yolov7-drone：一种改进的无人机空中图像物体检测方法，Drones
    7 (2023) 616。
- en: 'Yang et al. [2020] W. Yang, Y. Yuan, W. Ren, J. Liu, W. J. Scheirer, Z. Wang,
    T. Zhang, Q. Zhong, D. Xie, S. Pu, et al., Advancing image understanding in poor
    visibility environments: A collective benchmark study, IEEE Transactions on Image
    Processing 29 (2020) 5737–5752.'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2020] W. Yang, Y. Yuan, W. Ren, J. Liu, W. J. Scheirer, Z. Wang,
    T. Zhang, Q. Zhong, D. Xie, S. Pu 等人，提升恶劣可见度环境下的图像理解：一项集体基准研究，IEEE Transactions
    on Image Processing 29 (2020) 5737–5752。
- en: 'Tan et al. [2021] L. Tan, X. Lv, X. Lian, G. Wang, Yolov4_drone: Uav image
    target detection based on an improved yolov4 algorithm, Computers & Electrical
    Engineering 93 (2021) 107261.'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. [2021] L. Tan, X. Lv, X. Lian, G. Wang, Yolov4_drone: 基于改进的 yolov4
    算法的无人机图像目标检测，Computers & Electrical Engineering 93 (2021) 107261。'
- en: Fang et al. [2023] W. Fang, G. Zhang, Y. Zheng, Y. Chen, Multi-task learning
    for uav aerial object detection in foggy weather condition, Remote Sensing 15
    (2023) 4617.
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. [2023] W. Fang, G. Zhang, Y. Zheng, Y. Chen, 雾霾天气条件下无人机空中物体检测的多任务学习，Remote
    Sensing 15 (2023) 4617。
- en: Hoanh and Pham [2024] N. Hoanh, T. V. Pham, A multi-task framework for car detection
    from high-resolution uav imagery focusing on road regions, IEEE Transactions on
    Intelligent Transportation Systems (2024).
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoanh 和 Pham [2024] N. Hoanh, T. V. Pham, 一种用于从高分辨率无人机影像中检测汽车的多任务框架，IEEE Transactions
    on Intelligent Transportation Systems (2024)。
- en: Jing et al. [2022] H. Jing, Y. Cheng, H. Wu, H. Wang, Radar target detection
    with multi-task learning in heterogeneous environment, IEEE Geoscience and Remote
    Sensing Letters 19 (2022) 1–5.
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jing et al. [2022] H. Jing, Y. Cheng, H. Wu, H. Wang, 在异质环境下的多任务学习雷达目标检测，IEEE
    Geoscience and Remote Sensing Letters 19 (2022) 1–5。
- en: 'Qu et al. [2024] H. Qu, Y. Cai, J. Liu, Llms are good action recognizers, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 18395–18406.'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qu et al. [2024] H. Qu, Y. Cai, J. Liu, Llms are good action recognizers, 见：IEEE/CVF
    计算机视觉与模式识别大会论文集，2024，页码 18395–18406。
- en: 'Han and Lim [2024] G. Han, S.-N. Lim, Few-shot object detection with foundation
    models, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2024, pp. 28608–28618.'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 和 Lim [2024] G. Han, S.-N. Lim, 基于基础模型的少样本物体检测，见：IEEE/CVF 计算机视觉与模式识别大会论文集，2024，页码
    28608–28618。
- en: 'Lin et al. [2024] C. Lin, Y. Jiang, L. Qu, Z. Yuan, J. Cai, Generative region-language
    pretraining for open-ended object detection, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 13958–13968.'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2024] C. Lin, Y. Jiang, L. Qu, Z. Yuan, J. Cai, 基于生成区域-语言预训练的开放式物体检测，见：IEEE/CVF
    计算机视觉与模式识别大会论文集，2024，页码 13958–13968。
- en: Zang et al. [2024] Y. Zang, W. Li, J. Han, K. Zhou, C. C. Loy, Contextual object
    detection with multimodal large language models, International Journal of Computer
    Vision (2024) 1–19.
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zang et al. [2024] Y. Zang, W. Li, J. Han, K. Zhou, C. C. Loy, 基于多模态大语言模型的上下文物体检测，International
    Journal of Computer Vision (2024) 1–19。
- en: 'Yang et al. [2024] F. Yang, S. Zhao, Y. Zhang, H. Chen, H. Chen, W. Tang, H. Lu,
    P. Xu, Z. Yang, J. Han, et al., Llmi3d: Empowering llm with 3d perception from
    a single 2d image, arXiv preprint arXiv:2408.07422 (2024).'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2024] F. Yang, S. Zhao, Y. Zhang, H. Chen, H. Chen, W. Tang, H.
    Lu, P. Xu, Z. Yang, J. Han 等人，Llmi3d：通过单张二维图像赋能大语言模型的三维感知，arXiv 预印本 arXiv:2408.07422
    (2024)。
- en: 'Huang et al. [2023] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
    W. Peng, X. Feng, B. Qin, et al., A survey on hallucination in large language
    models: Principles, taxonomy, challenges, and open questions, ACM Transactions
    on Information Systems (2023).'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. [2023] L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen,
    W. Peng, X. Feng, B. Qin, 等，关于大规模语言模型中幻觉现象的调查：原理、分类、挑战与开放问题，ACM信息系统学报 (2023)。
- en: Liu et al. [2024] H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou,
    R. Li, W. Peng, A survey on hallucination in large vision-language models, arXiv
    preprint arXiv:2402.00253 (2024).
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2024] H. Liu, W. Xue, Y. Chen, D. Chen, X. Zhao, K. Wang, L. Hou,
    R. Li, W. Peng, 关于大规模视觉语言模型中幻觉现象的调查，arXiv预印本 arXiv:2402.00253 (2024)。
- en: 'Favero et al. [2024] A. Favero, L. Zancato, M. Trager, S. Choudhary, P. Perera,
    A. Achille, A. Swaminathan, S. Soatto, Multi-modal hallucination control by visual
    information grounding, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2024, pp. 14303–14312.'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Favero et al. [2024] A. Favero, L. Zancato, M. Trager, S. Choudhary, P. Perera,
    A. Achille, A. Swaminathan, S. Soatto, 通过视觉信息基础控制多模态幻觉，载于：IEEE/CVF计算机视觉与模式识别会议论文集，2024年，页14303–14312。
- en: Zhao et al. [2021] Q. Zhao, J. Liu, Y. Li, H. Zhang, Semantic segmentation with
    attention mechanism for remote sensing images, IEEE Transactions on Geoscience
    and Remote Sensing 60 (2021) 1–13.
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. [2021] Q. Zhao, J. Liu, Y. Li, H. Zhang, 基于注意力机制的遥感影像语义分割，IEEE地球科学与遥感学报
    60 (2021) 1–13。
- en: Yuan et al. [2021] X. Yuan, J. Shi, L. Gu, A review of deep learning methods
    for semantic segmentation of remote sensing imagery, Expert Systems with Applications
    169 (2021) 114417.
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. [2021] X. Yuan, J. Shi, L. Gu, 遥感影像语义分割的深度学习方法综述，Expert Systems
    with Applications 169 (2021) 114417。
- en: 'Cai et al. [2022] Y. Cai, Y. Yang, Y. Shang, Z. Chen, Z. Shen, J. Yin, Iterdanet:
    Iterative intra-domain adaptation for semantic segmentation of remote sensing
    images, IEEE Transactions on Geoscience and Remote Sensing 60 (2022) 1–17.'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai et al. [2022] Y. Cai, Y. Yang, Y. Shang, Z. Chen, Z. Shen, J. Yin, Iterdanet：基于迭代的领域内适应方法，进行遥感图像的语义分割，IEEE地球科学与遥感学报
    60 (2022) 1–17。
- en: 'Bai et al. [2022] L. Bai, S. Du, X. Zhang, H. Wang, B. Liu, S. Ouyang, Domain
    adaptation for remote sensing image semantic segmentation: An integrated approach
    of contrastive learning and adversarial learning, IEEE Transactions on Geoscience
    and Remote Sensing 60 (2022) 1–13.'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. [2022] L. Bai, S. Du, X. Zhang, H. Wang, B. Liu, S. Ouyang, 遥感影像语义分割的领域适应：对比学习与对抗学习的集成方法，IEEE地球科学与遥感学报
    60 (2022) 1–13。
- en: 'He et al. [2016] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for
    image recognition, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2016, pp. 770–778.'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2016] K. He, X. Zhang, S. Ren, J. Sun, 图像识别的深度残差学习，载于：IEEE计算机视觉与模式识别会议论文集，2016年，页770–778。
- en: 'Li et al. [2024] L. Li, Y. Zhang, Z. Jiang, Z. Wang, L. Zhang, H. Gao, Unmanned
    aerial vehicle-neural radiance field (uav-nerf): Learning multiview drone three-dimensional
    reconstruction with neural radiance field, Remote Sensing 16 (2024) 4168.'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024] L. Li, Y. Zhang, Z. Jiang, Z. Wang, L. Zhang, H. Gao, 无人机神经辐射场（uav-nerf）：利用神经辐射场进行多视角无人机三维重建，遥感
    16 (2024) 4168。
- en: Wu et al. [2024] Y. Wu, J. Liu, S. Ji, 3d gaussian splatting for large-scale
    surface reconstruction from aerial images, arXiv preprint arXiv:2409.00381 (2024).
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2024] Y. Wu, J. Liu, S. Ji, 基于3D高斯溅射的大规模地面重建方法，从航空影像重建表面，arXiv预印本
    arXiv:2409.00381 (2024)。
- en: 'Florea and Nedevschi [2022] H. Florea, S. Nedevschi, Survey on monocular depth
    estimation for unmanned aerial vehicles using deep learning, in: 2022 IEEE 18th
    International Conference on Intelligent Computer Communication and Processing
    (ICCP), IEEE, 2022, pp. 319–326.'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Florea and Nedevschi [2022] H. Florea, S. Nedevschi, 基于深度学习的无人机单目深度估计调查，载于：2022
    IEEE第18届智能计算通信与处理国际会议论文集，IEEE，2022年，页319–326。
- en: Chang et al. [2023] R. Chang, K. Yu, Y. Yang, Self-supervised monocular depth
    estimation using global and local mixed multi-scale feature enhancement network
    for low-altitude uav remote sensing, Remote Sensing 15 (2023) 3275.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang et al. [2023] R. Chang, K. Yu, Y. Yang, 自监督单目深度估计，使用全局与局部混合多尺度特征增强网络进行低空无人机遥感，遥感
    15 (2023) 3275。
- en: Yu et al. [2023] K. Yu, H. Li, L. Xing, T. Wen, D. Fu, Y. Yang, C. Zhou, R. Chang,
    S. Zhao, L. Xing, et al., Scene-aware refinement network for unsupervised monocular
    depth estimation in ultra-low altitude oblique photography of uav, ISPRS Journal
    of Photogrammetry and Remote Sensing 205 (2023) 284–300.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等人 [2023] K. Yu, H. Li, L. Xing, T. Wen, D. Fu, Y. Yang, C. Zhou, R. Chang,
    S. Zhao, L. Xing 等, 面向场景的细化网络用于无人机超低空斜视摄影中的无监督单目深度估计, 《ISPRS 摄影测量与遥感杂志》 205 (2023)
    284–300。
- en: 'Antol et al. [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L.
    Zitnick, D. Parikh, Vqa: Visual question answering, in: Proceedings of the IEEE
    international conference on computer vision, 2015, pp. 2425–2433.'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Antol 等人 [2015] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick,
    D. Parikh, VQA: 视觉问答, 收录于 IEEE 国际计算机视觉会议论文集, 2015, 页码 2425–2433。'
- en: 'Goyal et al. [2017] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh,
    Making the v in vqa matter: Elevating the role of image understanding in visual
    question answering, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 6904–6913.'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal 等人 [2017] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, D. Parikh, 让
    VQA 中的 "V" 更具意义: 提升图像理解在视觉问答中的角色, 收录于 IEEE 计算机视觉与模式识别会议论文集, 2017, 页码 6904–6913。'
- en: 'Zhou et al. [2020] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, J. Gao,
    Unified vision-language pre-training for image captioning and vqa, in: Proceedings
    of the AAAI conference on artificial intelligence, volume 34, 2020, pp. 13041–13049.'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2020] L. Zhou, H. Palangi, L. Zhang, H. Hu, J. Corso, J. Gao, 统一视觉语言预训练用于图像描述和
    VQA, 收录于 AAAI 人工智能会议论文集, 第 34 卷, 2020, 页码 13041–13049。
- en: 'Hu et al. [2022] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, L. Wang, Scaling
    up vision-language pre-training for image captioning, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, 2022, pp. 17980–17989.'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2022] X. Hu, Z. Gan, J. Wang, Z. Yang, Z. Liu, Y. Lu, L. Wang, 扩大视觉语言预训练规模以提高图像描述性能,
    收录于 IEEE/CVF 计算机视觉与模式识别会议论文集, 2022, 页码 17980–17989。
- en: Wang et al. [2023] X. Wang, X. Cui, D. Li, F. Liu, L. Jiao, Multi-model fusion
    for aerial vision and dialog navigation based on human attention aids, arXiv preprint
    arXiv:2308.14064 (2023).
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] X. Wang, X. Cui, D. Li, F. Liu, L. Jiao, 基于人类注意力辅助的空中视觉与对话导航的多模型融合,
    arXiv 预印本 arXiv:2308.14064 (2023)。
- en: 'Chu et al. [2025] M. Chu, Z. Zheng, W. Ji, T. Wang, T.-S. Chua, Towards natural
    language-guided drones: Geotext-1652 benchmark with spatial relation matching,
    in: European Conference on Computer Vision, Springer, 2025, pp. 213–231.'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chu 等人 [2025] M. Chu, Z. Zheng, W. Ji, T. Wang, T.-S. Chua, 朝着自然语言引导的无人机: Geotext-1652
    基准与空间关系匹配, 收录于欧洲计算机视觉会议, Springer, 2025, 页码 213–231。'
- en: 'Zhang et al. [2023] L. Zhang, A. Rao, M. Agrawala, Adding conditional control
    to text-to-image diffusion models, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2023, pp. 3836–3847.'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023] L. Zhang, A. Rao, M. Agrawala, 向文本到图像扩散模型中添加条件控制, 收录于 IEEE/CVF
    国际计算机视觉会议论文集, 2023, 页码 3836–3847。
- en: 'Liu et al. [2024] R. Liu, W. Wang, Y. Yang, Volumetric environment representation
    for vision-language navigation, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, 2024, pp. 16317–16328.'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2024] R. Liu, W. Wang, Y. Yang, 体积环境表示用于视觉语言导航, 收录于 IEEE/CVF 计算机视觉与模式识别会议论文集,
    2024, 页码 16317–16328。
- en: 'Li et al. [2024] X. Li, S. Hu, X. Feng, D. Zhang, M. Wu, J. Zhang, K. Huang,
    Dtvlt: A multi-modal diverse text benchmark for visual language tracking based
    on llm, arXiv preprint arXiv:2410.02492 (2024).'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人 [2024] X. Li, S. Hu, X. Feng, D. Zhang, M. Wu, J. Zhang, K. Huang, DTVLT:
    基于大语言模型的视觉语言追踪多模态多样化文本基准, arXiv 预印本 arXiv:2410.02492 (2024)。'
- en: Sun et al. [2024] L. Sun, X. Li, Z. Yang, D. Gao, Visual object tracking based
    on the motion prediction and block search in uav videos, Drones 8 (2024) 252.
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人 [2024] L. Sun, X. Li, Z. Yang, D. Gao, 基于运动预测和块搜索的无人机视频视觉目标跟踪, 《Drones》
    8 (2024) 252。
- en: Wu et al. [2019] C. Wu, B. Ju, Y. Wu, X. Lin, N. Xiong, G. Xu, H. Li, X. Liang,
    Uav autonomous target search based on deep reinforcement learning in complex disaster
    scene, IEEE Access 7 (2019) 117227–117245.
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2019] C. Wu, B. Ju, Y. Wu, X. Lin, N. Xiong, G. Xu, H. Li, X. Liang,
    基于深度强化学习的无人机自主目标搜索在复杂灾难场景中的应用, 《IEEE Access》 7 (2019) 117227–117245。
- en: 'Hou et al. [2023] Y. Hou, J. Zhao, R. Zhang, X. Cheng, L. Yang, Uav swarm cooperative
    target search: A multi-agent reinforcement learning approach, IEEE Transactions
    on Intelligent Vehicles (2023).'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hou 等人 [2023] Y. Hou, J. Zhao, R. Zhang, X. Cheng, L. Yang, 无人机群体协同目标搜索: 一种多智能体强化学习方法,
    《IEEE 智能车辆学报》 (2023)。'
- en: Bethke et al. [2008] B. Bethke, M. Valenti, J. P. How, Uav task assignment,
    IEEE robotics & automation magazine 15 (2008) 39–44.
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bethke 等人 [2008] B. Bethke, M. Valenti, J. P. How, 无人机任务分配，IEEE Robotics & Automation
    Magazine 15 (2008) 39–44。
- en: 'Zhou et al. [2018] Z. Zhou, J. Feng, B. Gu, B. Ai, S. Mumtaz, J. Rodriguez,
    M. Guizani, When mobile crowd sensing meets uav: Energy-efficient task assignment
    and route planning, IEEE Transactions on Communications 66 (2018) 5526–5538.'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2018] Z. Zhou, J. Feng, B. Gu, B. Ai, S. Mumtaz, J. Rodriguez, M. Guizani,
    当移动众包感知遇到无人机：节能的任务分配和路径规划，IEEE Transactions on Communications 66 (2018) 5526–5538。
- en: 'Mao et al. [2024] X. Mao, G. Wu, M. Fan, Z. Cao, W. Pedrycz, Dl-drl: A double-level
    deep reinforcement learning approach for large-scale task scheduling of multi-uav,
    IEEE Transactions on Automation Science and Engineering (2024).'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao 等人 [2024] X. Mao, G. Wu, M. Fan, Z. Cao, W. Pedrycz, Dl-drl：一种用于多无人机大规模任务调度的双层深度强化学习方法，IEEE
    Transactions on Automation Science and Engineering (2024)。
- en: Tejaswi and Lee [2022] K. Tejaswi, T. Lee, Constrained imitation learning for
    a flapping wing unmanned aerial vehicle, IEEE Robotics and Automation Letters
    7 (2022) 10534–10541.
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tejaswi 和 Lee [2022] K. Tejaswi, T. Lee, 用于拍翼无人机的受限模仿学习，IEEE Robotics and Automation
    Letters 7 (2022) 10534–10541。
- en: 'Shukla et al. [2020] D. Shukla, S. Keshmiri, N. Beckage, Imitation learning
    for neural network autopilot in fixed-wing unmanned aerial systems, in: 2020 International
    Conference on Unmanned Aircraft Systems (ICUAS), IEEE, 2020, pp. 1508–1517.'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shukla 等人 [2020] D. Shukla, S. Keshmiri, N. Beckage, 固定翼无人系统中神经网络自动驾驶仪的模仿学习，在：2020国际无人机系统大会（ICUAS），IEEE，2020年，第1508–1517页。
- en: Choi and Ahn [2020] U. Choi, J. Ahn, Imitation learning-based unmanned aerial
    vehicle planning for multitarget reconnaissance under uncertainty, Journal of
    Aerospace Information Systems 17 (2020) 36–50.
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 和 Ahn [2020] U. Choi, J. Ahn, 基于模仿学习的无人机多目标侦察规划方法，Journal of Aerospace
    Information Systems 17 (2020) 36–50。
- en: Liang et al. [2023] Z. Liang, Q. Li, G. Fu, Multi-uav collaborative search and
    attack mission decision-making in unknown environments, Sensors 23 (2023) 7398.
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等人 [2023] Z. Liang, Q. Li, G. Fu, 在未知环境中多无人机协同搜索与攻击任务决策，Sensors 23 (2023)
    7398。
- en: Wang and Wang [2024] H. Wang, J. Wang, Enhancing multi-uav air combat decision
    making via hierarchical reinforcement learning, Scientific Reports 14 (2024) 4458.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Wang [2024] H. Wang, J. Wang, 通过层次强化学习增强多无人机空战决策，Scientific Reports 14
    (2024) 4458。
- en: 'Du et al. [2024] Y. Du, N. Qi, X. Li, M. Xiao, A.-A. A. Boulogeorgos, T. A.
    Tsiftsis, Q. Wu, Distributed multi-uav trajectory planning for downlink transmission:
    a gnn-enhanced drl approach, IEEE Wireless Communications Letters (2024).'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人 [2024] Y. Du, N. Qi, X. Li, M. Xiao, A.-A. A. Boulogeorgos, T. A. Tsiftsis,
    Q. Wu, 用于下行链路传输的分布式多无人机轨迹规划：一种基于GNN增强的DRL方法，IEEE Wireless Communications Letters
    (2024)。
- en: Li et al. [2022] K. Li, W. Ni, X. Yuan, A. Noor, A. Jamalipour, Deep-graph-based
    reinforcement learning for joint cruise control and task offloading for aerial
    edge internet of things (edgeiot), IEEE Internet of Things Journal 9 (2022) 21676–21686.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2022] K. Li, W. Ni, X. Yuan, A. Noor, A. Jamalipour, 基于深度图的强化学习用于无人机边缘物联网（edgeiot）的联合巡航控制和任务卸载，IEEE
    Internet of Things Journal 9 (2022) 21676–21686。
- en: Courbon et al. [2010] J. Courbon, Y. Mezouar, N. Guénard, P. Martinet, Vision-based
    navigation of unmanned aerial vehicles, Control engineering practice 18 (2010)
    789–799.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courbon 等人 [2010] J. Courbon, Y. Mezouar, N. Guénard, P. Martinet, 基于视觉的无人机导航，Control
    Engineering Practice 18 (2010) 789–799。
- en: 'Liu et al. [2022] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, S. Xie,
    A convnet for the 2020s, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 11976–11986.'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2022] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, S. Xie,
    2020年代的卷积神经网络，在：IEEE/CVF 计算机视觉与模式识别会议论文集，2022年，第11976–11986页。
- en: 'Devlin [2018] J. Devlin, Bert: Pre-training of deep bidirectional transformers
    for language understanding, arXiv preprint arXiv:1810.04805 (2018).'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin [2018] J. Devlin, Bert：用于语言理解的深度双向变换器的预训练，arXiv 预印本 arXiv:1810.04805
    (2018)。
- en: De Curtò et al. [2023] J. De Curtò, I. De Zarza, C. T. Calafate, Semantic scene
    understanding with large language models on unmanned aerial vehicles, Drones 7
    (2023) 114.
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: De Curtò 等人 [2023] J. De Curtò, I. De Zarza, C. T. Calafate, 使用大型语言模型进行无人机语义场景理解，Drones
    7 (2023) 114。
- en: Wang et al. [2023] C. Wang, Z. Zhong, X. Xiang, Y. Zhu, L. Wu, D. Yin, J. Li,
    Uav path planning in multi-task environments with risks through natural language
    understanding, Drones 7 (2023) 147.
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] C. Wang, Z. Zhong, X. Xiang, Y. Zhu, L. Wu, D. Yin, J. Li, 基于自然语言理解的多任务环境中无人机路径规划与风险管理，Drones
    7 (2023) 147。
- en: 'Kuwertz et al. [2018] A. Kuwertz, D. Mühlenberg, J. Sander, W. Müller, Applying
    knowledge-based reasoning for information fusion in intelligence, surveillance,
    and reconnaissance, in: Multisensor Fusion and Integration in the Wake of Big
    Data, Deep Learning and Cyber Physical System: An Edition of the Selected Papers
    from the 2017 IEEE International Conference on Multisensor Fusion and Integration
    for Intelligent Systems (MFI 2017), Springer, 2018, pp. 119–139.'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuwertz 等人 [2018] A. Kuwertz, D. Mühlenberg, J. Sander, W. Müller, 在情报、监视和侦察中的信息融合应用基于知识推理，在：大数据、深度学习和网络物理系统后时代的多传感器融合与集成：2017
    IEEE 国际会议多传感器融合与集成论文集（MFI 2017），Springer，2018，第119-139页。
- en: 'Feng et al. [2023] Y. Feng, H. Snoussi, J. Teng, A. Cherouat, T. Wang, Large
    language model-based multi-task uavs-towards distilled real-time interactive control,
    in: IET Conference Proceedings CP870, volume 2023, IET, 2023, pp. 114–118.'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 [2023] Y. Feng, H. Snoussi, J. Teng, A. Cherouat, T. Wang, 基于大语言模型的多任务无人机——迈向精炼的实时交互控制，在：IET
    会议论文集 CP870，2023年，IET，2023，第114-118页。
- en: Mahajan et al. [2023] V. Mahajan, E. Barmpounakis, M. R. Alam, N. Geroliminis,
    C. Antoniou, Treating noise and anomalies in vehicle trajectories from an experiment
    with a swarm of drones, IEEE Transactions on Intelligent Transportation Systems
    24 (2023) 9055–9067.
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahajan 等人 [2023] V. Mahajan, E. Barmpounakis, M. R. Alam, N. Geroliminis, C.
    Antoniou, 处理来自无人机群体实验的车辆轨迹中的噪声和异常，IEEE 智能交通系统学报 24 (2023) 9055-9067。
- en: 'Telikani et al. [2024] A. Telikani, A. Sarkar, B. Du, J. Shen, Machine learning
    for uav-aided its: A review with comparative study, IEEE Transactions on Intelligent
    Transportation Systems (2024).'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Telikani 等人 [2024] A. Telikani, A. Sarkar, B. Du, J. Shen, 无人机辅助智能交通系统的机器学习：综述与比较研究，IEEE
    智能交通系统学报 (2024)。
- en: Bisio et al. [2022] I. Bisio, C. Garibotto, H. Haleem, F. Lavagetto, A. Sciarrone,
    A systematic review of drone based road traffic monitoring system, Ieee Access
    10 (2022) 101537–101555.
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisio 等人 [2022] I. Bisio, C. Garibotto, H. Haleem, F. Lavagetto, A. Sciarrone,
    基于无人机的道路交通监测系统的系统性综述，IEEE Access 10 (2022) 101537-101555。
- en: 'Saputro et al. [2018] N. Saputro, K. Akkaya, R. Algin, S. Uluagac, Drone-assisted
    multi-purpose roadside units for intelligent transportation systems, in: 2018
    IEEE 88th Vehicular Technology Conference (VTC-Fall), IEEE, 2018, pp. 1–5.'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saputro 等人 [2018] N. Saputro, K. Akkaya, R. Algin, S. Uluagac, 无人机辅助的多用途路侧单元用于智能交通系统，在：2018
    IEEE 第88届车载技术大会（VTC-Fall），IEEE，2018，第1-5页。
- en: Dung [2019] N. D. Dung, Developing models for managing drones in the transportation
    system in smart cities, Electrical, Control and Communication Engineering 15 (2019)
    71–78.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dung [2019] N. D. Dung, 智慧城市中运输系统无人机管理模型的开发，电气、控制与通信工程 15 (2019) 71-78。
- en: 'Menouar et al. [2017] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri,
    A. Tuncer, Uav-enabled intelligent transportation systems for the smart city:
    Applications and challenges, IEEE Communications Magazine 55 (2017) 22–28.'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Menouar 等人 [2017] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri,
    A. Tuncer, 无人机支持的智能交通系统在智慧城市中的应用与挑战，IEEE 通信杂志 55 (2017) 22-28。
- en: 'Wang et al. [2023] L. Wang, X. Deng, J. Gui, P. Jiang, F. Zeng, S. Wan, A review
    of urban air mobility-enabled intelligent transportation systems: Mechanisms,
    applications and challenges, Journal of Systems Architecture 141 (2023) 102902.'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] L. Wang, X. Deng, J. Gui, P. Jiang, F. Zeng, S. Wan, 基于城市空中出行的智能交通系统综述：机制、应用与挑战，系统架构杂志
    141 (2023) 102902。
- en: 'Yao et al. [2024] J. Yao, J. Li, Y. Li, M. Zhang, C. Zuo, S. Dong, Z. Dai,
    A vision–language model-based traffic sign detection method for high-resolution
    drone images: A case study in guyuan, china, Sensors 24 (2024) 5800.'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2024] J. Yao, J. Li, Y. Li, M. Zhang, C. Zuo, S. Dong, Z. Dai, 基于视觉-语言模型的高分辨率无人机图像交通标志检测方法：以中国固原市为例，传感器
    24 (2024) 5800。
- en: 'Yuan et al. [2024] Z. Yuan, F. Xie, T. Ji, Patrol agent: An autonomous uav
    framework for urban patrol using on board vision language model and on cloud large
    language model, in: 2024 6th International Conference on Robotics and Computer
    Vision (ICRCV), IEEE, 2024, pp. 237–242.'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2024] Z. Yuan, F. Xie, T. Ji, 巡逻代理：基于机载视觉语言模型和云端大语言模型的城市巡逻自主无人机框架，在：2024年第6届国际机器人与计算机视觉会议（ICRCV），IEEE，2024，第237-242页。
- en: 'Zhu et al. [2024] H. Zhu, S. Qin, M. Su, C. Lin, A. Li, J. Gao, Harnessing
    large vision and language models in agriculture: A review, arXiv preprint arXiv:2407.19679
    (2024).'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2024] H. Zhu, S. Qin, M. Su, C. Lin, A. Li, J. Gao, 在农业中利用大规模视觉和语言模型：综述，arXiv
    预印本 arXiv:2407.19679 (2024)。
- en: 'Tian et al. [2024] Y. Tian, F. Lin, X. Zhang, J. Ge, Y. Wang, X. Dai, Y. Lv,
    F.-Y. Wang, Logisticsvista: 3d terminal delivery services with uavs, ugvs and
    usvs based on foundation models and scenarios engineering, IEEE International
    Conference on Service Operations and Logistics, and Informatics (SOLI) (2024).'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tian等人[2024] Y. Tian, F. Lin, X. Zhang, J. Ge, Y. Wang, X. Dai, Y. Lv, F.-Y.
    Wang, 《Logisticsvista：基于基础模型与场景工程的无人机、无人地面车辆和无人水面船舶的3D终端配送服务》，IEEE国际服务运作与物流信息学会议（SOLI）（2024）。
- en: Jiang et al. [2024] H. Jiang, T. Wu, X. Ren, L. Gou, Optimisation of multi-type
    logistics uav scheduling under high demand, Promet-Traffic&Transportation 36 (2024)
    115–131.
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang等人[2024] H. Jiang, T. Wu, X. Ren, L. Gou, 《高需求下多类型物流无人机调度优化》，《交通与运输学报》36（2024）115-131。
- en: Huang et al. [2020] H. Huang, A. V. Savkin, C. Huang, Scheduling of a parcel
    delivery system consisting of an aerial drone interacting with public transportation
    vehicles, Sensors 20 (2020) 2045.
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人[2020] H. Huang, A. V. Savkin, C. Huang, 《由空中无人机与公共交通工具互动组成的包裹配送系统调度》，《传感器》20（2020）2045。
- en: 'Wandelt et al. [2023] S. Wandelt, S. Wang, C. Zheng, X. Sun, Aerial: A meta
    review and discussion of challenges toward unmanned aerial vehicle operations
    in logistics, mobility, and monitoring, IEEE Transactions on Intelligent Transportation
    Systems (2023).'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wandelt等人[2023] S. Wandelt, S. Wang, C. Zheng, X. Sun, 《Aerial：关于无人机在物流、移动性和监控中的操作挑战的元评审与讨论》，《IEEE智能交通系统学报》（2023）。
- en: Luo et al. [2024] S. Luo, Y. Yao, H. Zhao, L. Song, A language model-based fine-grained
    address resolution framework in uav delivery system, IEEE Journal of Selected
    Topics in Signal Processing (2024).
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等人[2024] S. Luo, Y. Yao, H. Zhao, L. Song, 《基于语言模型的无人机配送系统细粒度地址解析框架》，《IEEE信号处理精选主题期刊》（2024）。
- en: 'Dong et al. [2024] C. Dong, N. Syed, F. Jiang, R. Elphick-Darling, S. Chen,
    J. Zhang, M. Lu, X. Liu, Securing uav delivery systems with blockchain and large
    language models: an innovative logistics solution, in: 2024 11th International
    Conference on Machine Intelligence Theory and Applications (MiTA), IEEE, 2024,
    pp. 1–8.'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人[2024] C. Dong, N. Syed, F. Jiang, R. Elphick-Darling, S. Chen, J. Zhang,
    M. Lu, X. Liu, 《利用区块链和大型语言模型保障无人机配送系统的安全：一种创新的物流解决方案》，载于：2024年第11届国际机器智能理论与应用会议（MiTA），IEEE，2024，第1-8页。
- en: 'Jin et al. [2020] W. Jin, J. Yang, Y. Fang, W. Feng, Research on application
    and deployment of uav in emergency response, in: 2020 IEEE 10th International
    Conference on Electronics Information and Emergency Communication (ICEIEC), IEEE,
    2020, pp. 277–280.'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin等人[2020] W. Jin, J. Yang, Y. Fang, W. Feng, 《无人机在应急响应中的应用与部署研究》，载于：2020 IEEE第十届国际电子信息与应急通信会议（ICEIEC），IEEE，2020，第277-280页。
- en: 'Goecks and Waytowich [2023] V. G. Goecks, N. R. Waytowich, Disasterresponsegpt:
    Large language models for accelerated plan of action development in disaster response
    scenarios, arXiv preprint arXiv:2306.17271 (2023).'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goecks和Waytowich[2023] V. G. Goecks, N. R. Waytowich, 《Disasterresponsegpt：用于灾难响应场景中的行动计划加速发展的大型语言模型》，arXiv预印本arXiv:2306.17271（2023）。
- en: 'Fourati and Alouini [2021] F. Fourati, M.-S. Alouini, Artificial intelligence
    for satellite communication: A review, Intelligent and Converged Networks 2 (2021)
    213–243.'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fourati和Alouini[2021] F. Fourati, M.-S. Alouini, 《卫星通信中的人工智能：综述》，《智能与融合网络》2（2021）213-243。
- en: Wang et al. [2024] Y. Wang, J. Farooq, H. Ghazzai, G. Setti, Multi-uav placement
    for integrated access and backhauling using llm-driven optimization (2024).
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人[2024] Y. Wang, J. Farooq, H. Ghazzai, G. Setti, 《基于LLM驱动优化的多无人机布局用于集成接入和回程》（2024）。
- en: 'Gong et al. [2024] Z. Gong, Z. Wei, D. Wang, X. Ma, H. Chen, Y. Jia, Y. Deng,
    Z. Ji, X. Zhu, N. Yokoya, J. Zhang, B. Du, L. Zhang, Crossearth: Geospatial vision
    foundation model for domain generalizable remote sensing semantic segmentation,
    arXiv preprint arXiv:2410.22629 (2024).'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong等人[2024] Z. Gong, Z. Wei, D. Wang, X. Ma, H. Chen, Y. Jia, Y. Deng, Z. Ji,
    X. Zhu, N. Yokoya, J. Zhang, B. Du, L. Zhang, 《Crossearth：用于领域通用遥感语义分割的地理空间视觉基础模型》，arXiv预印本arXiv:2410.22629（2024）。
- en: 'Hong et al. [2023] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, C. Gan,
    3d-llm: Injecting the 3d world into large language models, Advances in Neural
    Information Processing Systems 36 (2023) 20482–20494.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong等人[2023] Y. Hong, H. Zhen, P. Chen, S. Zheng, Y. Du, Z. Chen, C. Gan, 《3d-llm：将三维世界注入大型语言模型》，《神经信息处理系统进展》36（2023）20482-20494。
- en: 'Zhang et al. [2024] S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, T. He,
    Y. Zhang, Agent3d-zero: An agent for zero-shot 3d understanding, arXiv preprint
    arXiv:2403.11835 (2024).'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang等人[2024] S. Zhang, D. Huang, J. Deng, S. Tang, W. Ouyang, T. He, Y. Zhang,
    《Agent3d-zero：一个用于零样本三维理解的智能体》，arXiv预印本arXiv:2403.11835（2024）。
- en: 'Hu et al. [2021] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang,
    L. Wang, W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint
    arXiv:2106.09685 (2021).'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '胡等人 [2021] E. J. 胡, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,
    W. Chen, *Lora*: 大型语言模型的低秩适应, arXiv预印本 arXiv:2106.09685 (2021)。'
- en: Casper et al. [2023] S. Casper, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer,
    J. Rando, R. Freedman, T. Korbak, D. Lindner, P. Freire, et al., Open problems
    and fundamental limitations of reinforcement learning from human feedback, arXiv
    preprint arXiv:2307.15217 (2023).
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡斯珀等人 [2023] S. 卡斯珀, X. Davies, C. Shi, T. K. Gilbert, J. Scheurer, J. Rando,
    R. Freedman, T. Korbak, D. Lindner, P. Freire, 等, 强化学习中的开放问题与基础性限制：来自人类反馈的学习,
    arXiv预印本 arXiv:2307.15217 (2023)。
- en: 'Chen et al. [2024] B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Sadigh, L. Guibas,
    F. Xia, Spatialvlm: Endowing vision-language models with spatial reasoning capabilities,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 14455–14465.'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2024] B. 陈, Z. 徐, S. Kirmani, B. Ichter, D. Sadigh, L. Guibas, F. Xia,
    *Spatialvlm*: 为视觉-语言模型赋予空间推理能力, 载于：IEEE/CVF计算机视觉与模式识别大会论文集, 2024, 第14455-14465页。'
