- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2025-01-11 12:07:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2025-01-11 12:07:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Optima：优化基于LLM的多智能体系统的效能与效率
- en: 来源：[https://arxiv.org/html/2410.08115/](https://arxiv.org/html/2410.08115/)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://arxiv.org/html/2410.08115/](https://arxiv.org/html/2410.08115/)
- en: Weize Chen¹  , Jiarui Yuan^(1∗), Chen Qian¹Cheng Yang² Zhiyuan Liu¹, Maosong
    Sun¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 陈伟泽¹，袁家瑞^(1∗)，钱晨¹，杨诚²，刘智远¹，孙茂松¹
- en: ¹ Tsinghua University, ² Beijing University of Posts and Telecommunications
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 清华大学，² 北京邮电大学
- en: '{chenwz21,yuanjr22}@mails.tsinghua.edu.cn, liuzy@tsinghua.edu.cn Equal Contribution.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{chenwz21,yuanjr22}@mails.tsinghua.edu.cn, liuzy@tsinghua.edu.cn 平等贡献。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large Language Model (LLM) based multi-agent systems (MAS) show remarkable
    potential in collaborative problem-solving, yet they still face critical challenges:
    low communication efficiency, poor scalability, and a lack of effective parameter-updating
    optimization methods. We present Optima, a novel framework that addresses these
    issues by significantly enhancing both communication efficiency and task effectiveness
    in LLM-based MAS through LLM training. Optima employs an iterative generate, rank,
    select, and train paradigm with a reward function balancing task performance,
    token efficiency, and communication readability. We explore various RL algorithms,
    including Supervised Fine-Tuning, Direct Preference Optimization, and their hybrid
    approaches, providing insights into their effectiveness-efficiency trade-offs.
    We integrate Monte Carlo Tree Search-inspired techniques for DPO data generation,
    treating conversation turns as tree nodes to explore diverse interaction paths.
    Evaluated on common multi-agent tasks, including information-asymmetric question
    answering and complex reasoning, Optima shows consistent and substantial improvements
    over single-agent baselines and vanilla MAS based on Llama 3 8B, achieving up
    to 2.8x performance gain with less than 10% tokens on tasks requiring heavy information
    exchange. Moreover, Optima’s efficiency gains open new possibilities for leveraging
    inference-compute more effectively, leading to improved inference-time scaling
    laws. By addressing fundamental challenges in LLM-based MAS, Optima shows the
    potential towards scalable, efficient, and effective MAS¹¹1[https://chenweize1998.github.io/optima-project-page](https://chenweize1998.github.io/optima-project-page).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 基于大型语言模型（LLM）的多智能体系统（MAS）在协同问题解决方面展现出显著的潜力，但仍面临关键挑战：低效的通信、差的可扩展性和缺乏有效的参数更新优化方法。我们提出了Optima，一个新颖的框架，通过LLM训练显著提高了LLM基础的MAS在通信效率和任务效能上的表现，从而解决了这些问题。Optima采用迭代的生成、排名、选择和训练范式，并使用一个奖励函数平衡任务表现、标记效率和通信可读性。我们探讨了多种强化学习（RL）算法，包括监督微调、直接偏好优化及其混合方法，提供了这些方法在效果和效率上的权衡见解。我们整合了受到蒙特卡罗树搜索启发的技术用于DPO数据生成，将对话轮次视为树节点，探索多样的互动路径。在常见的多智能体任务上进行评估，包括信息不对称的问答和复杂推理，Optima在基于Llama
    3 8B的单一智能体基准和原始MAS上表现出一致且显著的提升，在需要大量信息交换的任务中，性能提高高达2.8倍，而所需的标记数量不到10%。此外，Optima的效率提升为更有效地利用推理计算开辟了新的可能性，从而改善了推理时间的扩展规律。通过解决基于LLM的MAS中的基本挑战，Optima展现了朝着可扩展、高效和有效的MAS发展的潜力¹¹1[https://chenweize1998.github.io/optima-project-page](https://chenweize1998.github.io/optima-project-page)。
- en: '![Refer to caption](img/fc52921a3e90401ccedf3da8dcad728b.png)![Refer to caption](img/f5b2e66f1d2281704dcb85fa5fb745a2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fc52921a3e90401ccedf3da8dcad728b.png)![参见标题](img/f5b2e66f1d2281704dcb85fa5fb745a2.png)'
- en: 'Figure 1: Performance and efficiency of Optima variants across optimization
    iterations. Left: Average performance gain over iterations. Optima variants consistently
    outperform CoT, Multi-Agent Debate (MAD), and Self-Consistency. Right: Average
    inference token numbers over iterations. All Optima variants achieve better performance
    with substantially fewer tokens.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：Optima变体在优化迭代中的性能和效率。左图：迭代过程中的平均性能提升。Optima变体始终优于CoT、多智能体辩论（MAD）和自一致性。右图：迭代过程中的平均推理标记数量。所有Optima变体在显著减少标记数量的同时，表现出更好的性能。
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Large Language Models (LLMs) have emerged as powerful tools for a wide range
    of tasks, from natural language processing to complex reasoning (OpenAI, [2023](https://arxiv.org/html/2410.08115v1#bib.bib40);
    Reid et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib46); Anthropic,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib2)). A promising direction in
    leveraging these models is the development of autonomous multi-agent systems (MAS),
    which aim to harness the collective intelligence of multiple LLM-based agents
    for collaborative problem-solving and decision-making (Liang et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib33);
    Wang et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib53); Du et al.,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib12); Zhuge et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib61)).
    However, for LLM-based MAS to be truly effective, they must overcome two critical
    challenges: (a) achieving efficient inter-agent communication to minimize computational
    costs, and (b) optimizing the collective performance of the system as a cohesive
    unit.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）已经成为广泛任务中的强大工具，从自然语言处理到复杂推理（OpenAI，[2023](https://arxiv.org/html/2410.08115v1#bib.bib40);
    Reid 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib46); Anthropic，[2024](https://arxiv.org/html/2410.08115v1#bib.bib2)）。利用这些模型的一个有前景的方向是开发自主的多智能体系统（MAS），旨在利用多个基于LLM的智能体的集体智慧进行协作性问题解决和决策制定（Liang
    等，[2023](https://arxiv.org/html/2410.08115v1#bib.bib33); Wang 等，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib53);
    Du 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib12); Zhuge 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib61)）。然而，要使基于LLM的MAS真正有效，它们必须克服两个关键挑战：（a）实现智能体之间的高效沟通，以最小化计算成本，以及（b）优化系统作为一个有机整体的集体表现。
- en: Current LLM-based MAS face significant difficulties in meeting these challenges.
    The coordination and communication between agents often lack efficiency, resulting
    in verbose exchanges that lead to increased token usage, longer inference times,
    and higher computational costs (Li et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib32)).
    This inefficiency is exacerbated by the length bias inherent in LLMs due to alignment
    training (Saito et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib47);
    Dubois et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib13)), which favors
    longer responses even when concise communication would suffice (Chen et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10)).
    Moreover, while recent work has explored training LLMs for single-agent tasks
    (Song et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib50); Xiong et al.,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib57)) and MAS training is well-studied
    in reinforcement learning (Johnson et al., [2000](https://arxiv.org/html/2410.08115v1#bib.bib25);
    Lanctot et al., [2017](https://arxiv.org/html/2410.08115v1#bib.bib28); Baker et al.,
    [2020](https://arxiv.org/html/2410.08115v1#bib.bib3)), there remains a lack of
    parameter-updating methods specifically designed to optimize LLM-based MAS as
    a unified system. Existing approaches primarily rely on simple agent profile evolution
    (Chen et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)) or memory
    evolution (Qian et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib42);
    [b](https://arxiv.org/html/2410.08115v1#bib.bib43); Gao et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib15)),
    which fail to address the core issues of communication efficiency and collective
    optimization.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当前基于大语言模型（LLM）的多智能体系统（MAS）面临着在应对这些挑战时的重大困难。智能体之间的协调与沟通通常效率低下，导致冗长的交流，进而增加了令牌使用量、推理时间和计算成本（Li
    等，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib32)）。这种低效性由于LLM的对齐训练中的长度偏差而加剧（Saito
    等，[2023](https://arxiv.org/html/2410.08115v1#bib.bib47); Dubois 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib13)），即使简洁的沟通足以表达，模型也倾向于生成更长的回复（Chen
    等，[2024d](https://arxiv.org/html/2410.08115v1#bib.bib10)）。此外，尽管近期的研究探索了为单一智能体任务训练LLM（Song
    等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib50); Xiong 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib57)），而多智能体系统（MAS）的训练在强化学习领域已有广泛研究（Johnson
    等，[2000](https://arxiv.org/html/2410.08115v1#bib.bib25); Lanctot 等，[2017](https://arxiv.org/html/2410.08115v1#bib.bib28);
    Baker 等，[2020](https://arxiv.org/html/2410.08115v1#bib.bib3)），但仍缺乏专门为优化基于LLM的MAS作为统一系统而设计的参数更新方法。现有的方法主要依赖于简单的智能体配置文件进化（Chen
    等，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)）或记忆进化（Qian 等，[2024a](https://arxiv.org/html/2410.08115v1#bib.bib42);
    [b](https://arxiv.org/html/2410.08115v1#bib.bib43); Gao 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib15)），但未能解决沟通效率和集体优化的核心问题。
- en: Can we develop a training framework that simultaneously enhances the communication
    efficiency and task effectiveness of LLM-based MAS? To address this question,
    we introduce Optima, an effective framework designed to optimize LLM-based MAS.
    At the heart of Optima is an iterative generate, rank, select, and train paradigm,
    incorporating a reward function that balances task performance, token efficiency,
    and communication interpretability. This approach enables the development of MAS
    that are not only effective and efficient but also maintain interpretable communication
    patterns. Based on the reward function, Optima leverages a combination of techniques
    to induce efficient and effective communication behaviors in LLM-based agents,
    including Supervised Fine-Tuning (SFT) (Zelikman et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib59);
    Gülçehre et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib16); Aksitov
    et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib1)) and Direct Preference
    Optimization (DPO) (Rafailov et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib45);
    Pang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib41)), along with
    their hybrid variants. Furthermore, Optima introduces an integration of Monte
    Carlo Tree Search (MCTS)-inspired techniques for DPO data generation, conceptualizing
    conversation turns as tree nodes to explore diverse interaction trajectories efficiently.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否开发一个训练框架，既能提高基于LLM的MAS的沟通效率，又能增强任务的有效性？为了解决这个问题，我们提出了Optima，这是一个旨在优化基于LLM的MAS的有效框架。Optima的核心是一个迭代生成、排名、选择和训练的范式，结合了一个奖励函数，平衡任务表现、令牌效率和沟通可解释性。这个方法使得开发出的MAS不仅有效且高效，而且还能保持可解释的沟通模式。基于奖励函数，Optima利用一系列技术来引导基于LLM的智能体产生高效且有效的沟通行为，包括监督微调（SFT）（Zelikman
    et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib59); Gülçehre et al.,
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib16); Aksitov et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib1)）和直接偏好优化（DPO）（Rafailov
    et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib45); Pang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib41)），以及它们的混合变体。此外，Optima还引入了受到蒙特卡洛树搜索（MCTS）启发的技术，以用于DPO数据生成，将对话轮次概念化为树节点，从而高效地探索多样的互动轨迹。
- en: Importantly, by substantially reducing the number of tokens required for inference,
    Optima not only improves computational efficiency but also opens new possibilities
    for leveraging inference-compute more effectively. This reduction in token usage
    allows for more samples within the same computational constraints, potentially
    leading to better inference-time scaling laws. As recent work has shown the importance
    of inference-time compute in improving model performance (Wu et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib56);
    Brown et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib5); Chen et al.,
    [2024a](https://arxiv.org/html/2410.08115v1#bib.bib7)), Optima’s efficiency gains
    could be combined with techniques like majority voting (Wang et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib52)),
    leading to more effective LLM systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，通过大幅减少推理所需的令牌数量，Optima不仅提高了计算效率，还为更有效地利用推理计算开辟了新可能。这一令牌使用的减少使得在相同的计算约束下可以处理更多样本，可能会导致更好的推理时间扩展规律。正如近期的研究所示，推理时间计算在提高模型表现中的重要性（Wu
    et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib56); Brown et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib5);
    Chen et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib7)），Optima的效率提升可以与诸如多数投票（Wang
    et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib52)）等技术结合，从而提升LLM系统的效果。
- en: 'We evaluate Optima on a diverse set of tasks spanning two multi-agent settings:
    (a) information exchange, including information-asymmetric question answering
    (Chen et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10); Liu et al.,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib34)), and (b) debate, encompassing
    mathematical and reasoning tasks (Du et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib12);
    Chen et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8); Wu et al.,
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib55)). Using Llama 3 8B (Meta,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib37)) as our base model, we demonstrate
    that Optima consistently outperforms both single-agent MAS baselines, achieving
    up to 90% reduction in token usage and 2.8x increase in task performance.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个多智能体环境下评估了Optima的表现： (a) 信息交换，包括信息不对称的问答（Chen et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10);
    Liu et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib34)），以及 (b) 辩论，涵盖了数学和推理任务（Du
    et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib12); Chen et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8);
    Wu et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib55)）。我们使用Llama 3
    8B（Meta, [2024](https://arxiv.org/html/2410.08115v1#bib.bib37)）作为基准模型，展示了Optima在多个任务中持续优于单智能体MAS基线，令牌使用量减少最多可达90%，任务表现提升达2.8倍。
- en: To summarize, our main contribution is Optima, a novel training framework that
    simultaneously optimizes communication efficiency and task effectiveness. To enhance
    high-quality training data generation in multi-agent settings for DPO, we introduce
    an integration of MCTS-like techniques. Our comprehensive empirical evaluation
    across diverse tasks demonstrates notable advancements in both token efficiency
    and task performance, while also providing insights into the learned communication
    patterns. Additionally, we examine the implications of Optima’s efficiency gains
    for inference-time scaling laws, underscoring its potential to improve the overall
    capabilities of LLM systems by enabling more effective utilization of inference-compute.
    By addressing the dual challenges of communication efficiency and collective optimization,
    our work underscores the importance of developing advanced training frameworks
    for LLM-based MAS and highlights efficiency as a crucial metric to consider. We
    believe Optima provides a solid foundation for future investigations into scaling
    and improving MAS and even general LLM systems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的主要贡献是Optima，一个新颖的训练框架，它同时优化了通信效率和任务效果。为了增强多智能体环境下DPO的高质量训练数据生成，我们引入了类似MCTS的技术集成。我们在多个任务上的全面实证评估展示了在令牌效率和任务表现方面的显著进展，同时也提供了对学习到的通信模式的洞察。此外，我们还考察了Optima在推理时扩展规律中的效率提升对推理计算能力的影响，强调了其通过更有效地利用推理计算来提升LLM系统整体能力的潜力。通过解决通信效率和集体优化的双重挑战，我们的工作强调了为基于LLM的MAS开发先进训练框架的重要性，并突出了效率作为一个重要的考量指标。我们相信，Optima为未来在扩展和改进MAS乃至一般LLM系统的研究提供了坚实的基础。
- en: '2 Optima: Optimizing Multi-Agent LLMs via Iterative Training'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 Optima：通过迭代训练优化多智能体LLM
- en: 2.1 Overview
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: '![Refer to caption](img/52bc4d8727ca816ef5f2db80d6378d27.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/52bc4d8727ca816ef5f2db80d6378d27.png)'
- en: 'Figure 2: Overview of the Optima framework for training LLM-based MAS. The
    iterative process includes four stages: Generate, Rank, Select, and Train. Note
    that the ranking process, while also involved in DPO data generation, is not shown
    in the Generate stage for simplicity.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Optima框架在基于LLM的MAS训练中的概述。迭代过程包括四个阶段：生成、排名、选择和训练。注意，排名过程虽然也涉及到DPO数据生成，但为了简化起见，未在生成阶段显示。
- en: Optima is built upon an iterative generate, rank, select, and train paradigm.
    This approach allows for the progressive improvement of LLM-based agents in multi-agent
    settings, focusing on enhancing both the efficiency of inter-agent communication
    and the effectiveness of task completion.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Optima建立在一个迭代的生成、排名、选择和训练的范式之上。这种方法允许在多智能体环境中逐步改进基于LLM的智能体，重点提升智能体之间通信的效率和任务完成的有效性。
- en: 'Let $\mathcal{M}_{\text{base}}$ denote the base LLM, $\mathcal{D}$ the task
    dataset, and $f$ the iterative training function. The iterative process can be
    formalized as $\mathcal{M}_{t+1}=f(\mathcal{M}_{t},\mathcal{D})$, where $\mathcal{M}_{t}$
    represents the model at iteration $t$. The function $f$ encapsulates the entire
    process of data generation, ranking, selection and model training. For each task
    instance $d_{i}\in\mathcal{D}$, we sample a set of $N$ conversation trajectories
    $\{\tau_{i}^{j}\}_{j=1}^{N}\subset\mathcal{T}$ using the agents powered by current
    model $\mathcal{M}_{t}$. Each trajectory $\tau_{i}^{j}$ is then evaluated using
    a reward function $R:\mathcal{T}\rightarrow\mathbb{R}$, defined as:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 令 $\mathcal{M}_{\text{base}}$ 表示基础LLM，$\mathcal{D}$ 表示任务数据集，$f$ 表示迭代训练函数。迭代过程可以形式化为
    $\mathcal{M}_{t+1}=f(\mathcal{M}_{t},\mathcal{D})$，其中 $\mathcal{M}_{t}$ 表示第 $t$
    次迭代时的模型。函数 $f$ 包括了数据生成、排序、选择和模型训练的整个过程。对于每个任务实例 $d_{i}\in\mathcal{D}$，我们使用当前模型
    $\mathcal{M}_{t}$ 驱动的代理采样一组 $N$ 个对话轨迹 $\{\tau_{i}^{j}\}_{j=1}^{N}\subset\mathcal{T}$。然后，每条轨迹
    $\tau_{i}^{j}$ 都会使用奖励函数 $R:\mathcal{T}\rightarrow\mathbb{R}$ 进行评估，定义为：
- en: '|  | $R(\tau_{i}^{j})=R_{\text{task}}(\tau_{i}^{j})-\lambda_{\text{token}}R_{\text{%
    token}}(\tau_{i}^{j})+\lambda_{\text{loss}}\frac{1}{R_{\text{loss}}(\tau_{i}^{%
    j})}.$ |  | (1) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $R(\tau_{i}^{j})=R_{\text{task}}(\tau_{i}^{j})-\lambda_{\text{token}}R_{\text{%
    token}}(\tau_{i}^{j})+\lambda_{\text{loss}}\frac{1}{R_{\text{loss}}(\tau_{i}^{j})}.$
    |  | (1) |'
- en: 'Here, $R_{\text{task}}:\mathcal{T}\rightarrow\mathbb{R}$ is the task-specific
    performance metric, $R_{\text{token}}(\tau_{i}^{j})=\frac{\#\text{Tokens}(\tau_{i}^{j})}{\max_{k}(%
    \{\#\text{Tokens}(\tau_{i}^{k})\}_{k})}$ is the normalized token count, and $R_{\text{loss}}(\tau_{i}^{j})=g\big{(}\mathcal{L}(\mathcal{M}_{\text{base}},d_%
    {i},\tau_{i}^{j})\big{)}$ is based on the language modeling loss of the base model
    $\mathcal{M}_{\text{base}}$, which we detail in [Section E.2](https://arxiv.org/html/2410.08115v1#A5.SS2
    "E.2 Ranking ‣ Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). The positive coefficients $\lambda_{\text{token}}$
    and $\lambda_{\text{loss}}$ are hyper-parameters . This reward function is designed
    to balance multiple objectives simultaneously: $R_{\text{task}}$ ensures that
    the model improves on the intended task, $R_{\text{token}}$ encourages communication
    efficiency by penalizing verbose exchanges, and $R_{\text{loss}}$ regularizes
    language naturalness and readability by favoring trajectories that are probable
    under the base model. By incorporating these components, we aim to develop LLM-based
    MAS that are not only effective in their designated tasks but also efficient in
    their communication, while maintaining interpretability in their outputs, unlike
    the often incomprehensible communication in prior RL research (Lazaridou et al.,
    [2017](https://arxiv.org/html/2410.08115v1#bib.bib29); Evtimova et al., [2018](https://arxiv.org/html/2410.08115v1#bib.bib14);
    Chaabouni et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib6)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，$R_{\text{task}}:\mathcal{T}\rightarrow\mathbb{R}$ 是任务特定的性能指标，$R_{\text{token}}(\tau_{i}^{j})=\frac{\#\text{Tokens}(\tau_{i}^{j})}{\max_{k}(\{\#\text{Tokens}(\tau_{i}^{k})\}_{k})}$
    是标准化的令牌计数，而 $R_{\text{loss}}(\tau_{i}^{j})=g\big{(}\mathcal{L}(\mathcal{M}_{\text{base}},d_{i},\tau_{i}^{j})\big{)}$
    基于基础模型 $\mathcal{M}_{\text{base}}$ 的语言建模损失，我们在[Section E.2](https://arxiv.org/html/2410.08115v1#A5.SS2
    "E.2 Ranking ‣ Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")中详细阐述了这一点。正的系数 $\lambda_{\text{token}}$
    和 $\lambda_{\text{loss}}$ 是超参数。该奖励函数旨在同时平衡多个目标：$R_{\text{task}}$ 确保模型在预定任务上有所提升，$R_{\text{token}}$
    通过惩罚冗长的交流来鼓励沟通效率，而 $R_{\text{loss}}$ 通过偏好在基础模型下更可能的轨迹来规范语言的自然性和可读性。通过整合这些组件，我们旨在开发基于LLM的MAS，这些系统不仅在指定任务中有效，而且在沟通上也高效，同时保持输出的可解释性，与以往强化学习研究中往往无法理解的交流方式不同（Lazaridou
    et al., [2017](https://arxiv.org/html/2410.08115v1#bib.bib29); Evtimova et al.,
    [2018](https://arxiv.org/html/2410.08115v1#bib.bib14); Chaabouni et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib6)）。'
- en: 'Based on these rewards, we apply several data selection criteria to select
    a subset of high-quality sampled trajectories $\{\tau_{i}^{*}\}$ for each task
    instance. These selected trajectories form the training data $\mathcal{D}_{i}^{*}$
    at iteration $i$. The model is then updated: $\mathcal{M}_{t+1}=\text{Train}(\mathcal{M}_{t},\mathcal{D}_{i}^{*}).$
    The Train function can be instantiated with various training algorithms, such
    as SFT or DPO, which we will discuss in detail in the following subsections.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些奖励，我们应用若干数据选择标准来选择每个任务实例的高质量采样轨迹子集$\{\tau_{i}^{*}\}$。这些选定的轨迹形成了第$i$次迭代中的训练数据$\mathcal{D}_{i}^{*}$。然后，模型会更新：$\mathcal{M}_{t+1}=\text{Train}(\mathcal{M}_{t},\mathcal{D}_{i}^{*})$。Train函数可以用多种训练算法来实例化，例如SFT或DPO，我们将在以下小节中详细讨论。
- en: '[Fig. 2](https://arxiv.org/html/2410.08115v1#S2.F2 "In 2.1 Overview ‣ 2 Optima:
    Optimizing Multi-Agent LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") provides a high-level overview
    of Optima. The specific instantiations of the generation and training processes
    will be detailed in the following subsections. The ranking process, consistent
    across all instantiations, is defined by the reward function presented in [Eq. 1](https://arxiv.org/html/2410.08115v1#S2.E1
    "In 2.1 Overview ‣ 2 Optima: Optimizing Multi-Agent LLMs via Iterative Training
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2](https://arxiv.org/html/2410.08115v1#S2.F2 "在2.1概述 ‣ 2 Optima: 通过迭代训练优化多代理LLM
    ‣ Optima: 优化基于LLM的多代理系统的有效性和效率") 提供了Optima的高级概述。生成和训练过程的具体实例将在以下小节中详细说明。排名过程在所有实例中是一致的，由[公式1](https://arxiv.org/html/2410.08115v1#S2.E1
    "在2.1概述 ‣ 2 Optima: 通过迭代训练优化多代理LLM ‣ Optima: 优化基于LLM的多代理系统的有效性和效率")中给出的奖励函数定义。'
- en: '2.2 Initialization: Diversifying Agent Communication'
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 初始化：多样化代理通信
- en: 'Before starting the iterative training process, we address a critical challenge
    in LLM-based MAS: agents often produce responses in a similar style across conversation
    trajectories, even with high-temperature sampling. This homogeneity limits the
    exploration of diverse communication strategies, potentially hindering the optimization
    toward more efficient and effective interactions. Following the observation from
    AutoForm (Chen et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10)),
    where LLMs can be explicitly prompted to leverage different more concise formats
    to communicate or reason without much compromise in performance, we introduce
    an initialization step that promotes diversity in agent communication.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始迭代训练过程之前，我们解决了一个基于LLM的MAS中的关键挑战：代理在多个对话轨迹中往往以类似的风格生成响应，即使使用了高温采样。这种同质性限制了多样化通信策略的探索，可能会妨碍朝着更高效和更有效的互动优化。借鉴AutoForm（Chen等，2024d，[参考文献](https://arxiv.org/html/2410.08115v1#bib.bib10)）的观察，LLM可以明确提示其利用不同的、更简洁的格式进行沟通或推理，而不在性能上作出太大妥协，我们引入了一个初始化步骤，旨在促进代理通信的多样性。
- en: 'Our approach leverages a pool of format specification prompts, $\mathcal{P}=\{p_{1},p_{2},...,p_{K}\}$,
    where each $p_{k}$ is a string specifying a particular response format (e.g.,
    JSON, list, see [Appendix F](https://arxiv.org/html/2410.08115v1#A6 "Appendix
    F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System") for concrete examples and creation process).
    For each task instance $d_{i}\in\mathcal{D}$, we generate $N$ conversation trajectories,
    each with a randomly selected format specification appended to the input task:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的方法利用了一组格式规范提示池，$\mathcal{P}=\{p_{1},p_{2},...,p_{K}\}$，其中每个$p_{k}$是一个字符串，指定了特定的响应格式（例如，JSON、列表，具体示例和创建过程请参见[附录
    F](https://arxiv.org/html/2410.08115v1#A6 "附录 F 实验中使用的提示 ‣ Optima: 优化基于LLM的多代理系统的有效性和效率")）。对于每个任务实例$d_{i}\in\mathcal{D}$，我们生成$N$个对话轨迹，每个轨迹都附加一个随机选择的格式规范到输入任务中：'
- en: '|  | $\tau_{i}^{j}=\mathcal{M}_{\text{base}}(d_{i}\oplus p_{k_{j}}),\quad k_{j}\sim%
    \text{Uniform}(1,K),\quad j=1,...,N,$ |  | (2) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\tau_{i}^{j}=\mathcal{M}_{\text{base}}(d_{i}\oplus p_{k_{j}}),\quad k_{j}\sim%
    \text{Uniform}(1,K),\quad j=1,...,N,$ |  | (2) |'
- en: where $\oplus$ denotes string concatenation. This process yields a diverse set
    of trajectories $\{\tau_{i}^{j}\}_{j=1}^{N}$ for each $d_{i}$, varying in both
    content and structure.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\oplus$表示字符串拼接。这个过程为每个$d_{i}$生成了一组多样化的轨迹$\{\tau_{i}^{j}\}_{j=1}^{N}$，它们在内容和结构上都存在差异。
- en: 'We then evaluate these trajectories using the reward function defined in [Eq. 1](https://arxiv.org/html/2410.08115v1#S2.E1
    "In 2.1 Overview ‣ 2 Optima: Optimizing Multi-Agent LLMs via Iterative Training
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"),
    for each $d_{i}$, we select the trajectory with the highest reward: $\tau_{i}^{*}=\operatorname*{arg\,max}_{j}R(\tau_{i}^{j})$.
    Finally, we select top k trajectories that exceed a predefined performance threshold
    $\theta_{\text{init}}$, resulting in a high-quality dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用[公式1](https://arxiv.org/html/2410.08115v1#S2.E1 "在2.1概述中 ‣ 2 Optima：通过迭代训练优化多智能体LLM
    ‣ Optima：优化基于LLM的多智能体系统的效能与效率")中定义的奖励函数来评估这些轨迹，对于每个$d_{i}$，我们选择具有最高奖励的轨迹：$\tau_{i}^{*}=\operatorname*{arg\,max}_{j}R(\tau_{i}^{j})$。最后，我们选择超过预设性能阈值$\theta_{\text{init}}$的前k条轨迹，从而得到一个高质量的数据集：
- en: '|  | $\mathcal{D}_{0}^{*}=\text{TopK}(\{(d_{i},\tau_{i}^{*})&#124;R_{\text{task}}(\tau_{i%
    }^{*})>\theta_{\text{init}},\forall d_{i}\in\mathcal{D}\},0.7&#124;D&#124;).$
    |  | (3) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{D}_{0}^{*}=\text{TopK}(\{(d_{i},\tau_{i}^{*})\mid R_{\text{task}}(\tau_{i%
    }^{*})>\theta_{\text{init}},\forall d_{i}\in\mathcal{D}\},0.7\mid D\mid).$ |  |
    (3) |'
- en: 'Crucially, we remove the format specification prompts from the selected trajectories,
    resulting in a dataset of diverse, high-quality conversations without explicit
    format instructions. Using this dataset, we fine-tune the base model and obtain
    $\mathcal{M}_{\text{base}}$ to obtain $\mathcal{M}_{0}=\text{SFT}(\mathcal{M}_{\text{base}},\mathcal{D}_{0}^{*})$,
    which serves as the starting point for Optima, able to generate diverse communication
    patterns without explicit format prompting. We provide pseudo-code in [Appendix B](https://arxiv.org/html/2410.08115v1#A2
    "Appendix B Additional Pseudo-Codes for Optima Variants ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") for better understanding. This
    initialization sets the stage for more effective exploration and optimization
    in the subsequent iterative training process.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，我们从选定的轨迹中移除了格式规范提示，从而得到一个多样化且高质量的对话数据集，且不包含显式的格式指令。利用这个数据集，我们对基础模型进行微调，得到$\mathcal{M}_{\text{base}}$，进而得到$\mathcal{M}_{0}=\text{SFT}(\mathcal{M}_{\text{base}},\mathcal{D}_{0}^{*})$，作为Optima的起点，能够在没有显式格式提示的情况下生成多样的交流模式。我们在[附录B](https://arxiv.org/html/2410.08115v1#A2
    "附录B Optima变体的附加伪代码 ‣ Optima：优化基于LLM的多智能体系统的效能与效率")中提供了伪代码以便更好地理解。这个初始化为后续的迭代训练过程中的更有效探索与优化奠定了基础。
- en: Algorithm 1 Iterative Supervised Fine-Tuning
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 迭代监督微调
- en: '1:Initialized model $\mathcal{M}_{\text{init}}$, dataset $\mathcal{D}$, sample
    size $N$, reward threshold $\theta_{\text{sft}}$, max iterations $T$2:Optimized
    model $\mathcal{M}_{T}$3:$\mathcal{M}_{0}\leftarrow\text{Initialize}(\mathcal{M}_{\text{init}},\mathcal{%
    D})$ $\triangleright$ [Algorithm 3](https://arxiv.org/html/2410.08115v1#alg3 "In
    Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing
    Effectiveness and Efficiency for LLM-Based Multi-Agent System")4:for $t=0$ to
    $T-1$ do5:     $\mathcal{D}_{t}^{*}\leftarrow\emptyset$6:     for each $d_{i}\in\mathcal{D}$ do7:         $\{\tau_{i}^{j}\}_{j=1}^{N}\leftarrow\text{AgentChat}(\mathcal{M}_{t},d_{i})$
    $\triangleright$ Generate N trajectories8:         $\tau_{i}^{*}\leftarrow\operatorname*{arg\,max}_{j}R(\tau_{i}^{j})$
    $\triangleright$ Select best trajectory9:         if $R(\tau_{i}^{*})>\theta_{\text{sft}}$ then10:              $\mathcal{D}_{t}^{*}\leftarrow\mathcal{D}_{t}^{*}\cup\{(d_{i},\tau_{i}^{*})\}$11:         end if12:     end for13:     $\mathcal{D}_{t}^{*}\leftarrow\text{TopK}(\mathcal{D}_{t}^{*},0.7|\mathcal{D}_{%
    t}^{*}|)$ $\triangleright$ Retain top 70% trajectories14:     $\mathcal{M}_{t+1}\leftarrow\text{SFT}(\mathcal{M}_{t},\mathcal{D}_{t}^{*})$15:end for16:return
    $\mathcal{M}_{T}$'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始化模型 $\mathcal{M}_{\text{init}}$，数据集 $\mathcal{D}$，样本大小 $N$，奖励阈值 $\theta_{\text{sft}}$，最大迭代次数
    $T$  '
- en: '2.3 Framework Instantiation 1: Iterative Supervised Fine-Tuning'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 框架实现 1：迭代监督微调
- en: 'We introduce iterative Supervised Fine-Tuning (iSFT) as our first instantiation
    of Optima. At each iteration $t$, iSFT follows the same general procedure outlined
    in [Algorithm 3](https://arxiv.org/html/2410.08115v1#alg3 "In Appendix A Inference
    Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"), generating a set of $N$ conversation
    trajectories for each task training instance $d_{i}\in\mathcal{D}$ using the current
    model $\mathcal{M}_{t}^{\text{iSFT}}$. However, unlike initialization, iSFT omits
    the format specification pool, as $\mathcal{M}_{0}$ has already internalized diverse
    communication strategies. Unlike recent research on iterative training (Gülçehre
    et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib16); Aksitov et al.,
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib1)), iSFT maintains a fixed
    reward threshold $\theta_{\text{SFT}}$ across iterations for data selection. After
    data generation, the model undergoes standard SFT. This process continues until
    a maximum number of iterations is reached. For clarity, the pseudo-code for iSFT
    is provided in [Algorithm 1](https://arxiv.org/html/2410.08115v1#alg1 "In 2.2
    Initialization: Diversifying Agent Communication ‣ 2 Optima: Optimizing Multi-Agent
    LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了迭代监督微调（iSFT）作为Optima的首次实现。在每次迭代$t$中，iSFT遵循[算法3](https://arxiv.org/html/2410.08115v1#alg3
    "附录A 信息交换任务推理扩展法则 ‣ Optima：优化基于LLM的多智能体系统的效果和效率")中概述的相同一般程序，使用当前模型$\mathcal{M}_{t}^{\text{iSFT}}$为每个任务训练实例$d_{i}\in\mathcal{D}$生成一组$N$个对话轨迹。然而，与初始化不同，iSFT省略了格式规范池，因为$\mathcal{M}_{0}$已经内化了多样化的通信策略。与最近关于迭代训练的研究（Gülçehre等，[2023](https://arxiv.org/html/2410.08115v1#bib.bib16)；Aksitov等，[2023](https://arxiv.org/html/2410.08115v1#bib.bib1)）不同，iSFT在各个迭代中保持固定的奖励阈值$\theta_{\text{SFT}}$以进行数据选择。数据生成后，模型会进行标准的SFT。这个过程会持续进行，直到达到最大迭代次数。为了清晰起见，iSFT的伪代码见[算法1](https://arxiv.org/html/2410.08115v1#alg1
    "2.2 初始化：多样化智能体通信 ‣ 2 Optima：通过迭代训练优化多智能体LLM ‣ Optima：优化基于LLM的多智能体系统的效果和效率")。
- en: iSFT provides a straightforward yet effective approach to optimize LLM-based
    MAS, leveraging the diverse communication patterns established during initialization
    while consistently improving task performance and communication efficiency.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: iSFT提供了一种简单而有效的方法来优化基于LLM的MAS，利用初始化过程中建立的多样化通信模式，同时持续提高任务性能和通信效率。
- en: '2.4 Framework Instantiation 2: Iterative Direct Preference Optimization'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 框架实现 2：迭代直接偏好优化
- en: While iSFT provides a straightforward approach to optimizing LLM-based MAS,
    it may be limited by its reliance on a single best trajectory for each task instance.
    To address this, we explore iterative Direct Preference Optimization (iDPO) (Rafailov
    et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib45); Pang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib41)),
    which optimizes models using comparative preferences and has demonstrated success
    in LLM alignment. Applying DPO in multi-agent settings, however, poses distinct
    challenges, particularly in generating meaningful paired data that capture the
    complexities of agent interactions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然iSFT提供了一种直接优化基于LLM的MAS的简便方法，但它可能受到对每个任务实例依赖于单一最佳轨迹的限制。为了解决这个问题，我们探索了迭代直接偏好优化（iDPO）（Rafailov等，[2023](https://arxiv.org/html/2410.08115v1#bib.bib45)；Pang等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib41)），它通过比较偏好优化模型，并在LLM对齐方面取得了成功。然而，在多智能体环境中应用DPO会带来不同的挑战，特别是在生成能够捕捉智能体交互复杂性的有意义配对数据方面。
- en: 'Data Generation: To overcome these challenges, we integrate MCTS with DPO data
    collection for high-quality paired data generation in multi-agent settings. Our
    MCTS-based approach conceptualizes the multi-agent conversation as a tree, where
    nodes represent conversational turns, and edges represent continuations. This
    structure allows us to explore diverse interaction trajectories systematically
    and select high-quality paired data for DPO training. The MCTS process begins
    at the root node (initial task prompt) and proceeds as follows: (1) Expansion:
    We select a node to expand based on the following criteria. We first exclude leaf
    nodes and the second-to-last level nodes to avoid wasting computation on low-variance
    expansions, then exclude nodes with content similar to previously expanded nodes,
    measured based on edit distance (see [Section E.1](https://arxiv.org/html/2410.08115v1#A5.SS1
    "E.1 Data Generation ‣ Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")). From the remaining nodes,
    we select 10 nodes with the highest rewards and sample one using the softmax distribution
    over their rewards. (2) Simulation: For each selected node, we expand 3 trajectories,
    simulating the conversation to completion. (3) Backpropagation: Once a trajectory
    is completed and rewarded with [Eq. 1](https://arxiv.org/html/2410.08115v1#S2.E1
    "In 2.1 Overview ‣ 2 Optima: Optimizing Multi-Agent LLMs via Iterative Training
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"),
    we update the estimated rewards of all nodes in the trajectory with the average
    rewards from their children. (4) Iteration: We repeat the above process 8 times,
    resulting in 24 trajectories. More iterations could potentially lead to more diverse
    and better-quality data.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成：为了克服这些挑战，我们将 MCTS 与 DPO 数据收集相结合，用于在多代理设置中生成高质量的配对数据。我们基于 MCTS 的方法将多代理对话概念化为一棵树，其中节点代表对话轮次，边代表继续进行的对话。这种结构使我们能够系统地探索多样化的交互轨迹，并为
    DPO 训练选择高质量的配对数据。MCTS 过程从根节点（初始任务提示）开始，按以下步骤进行：（1）扩展：我们根据以下标准选择一个节点进行扩展。首先排除叶节点和倒数第二层节点，以避免在低方差扩展上浪费计算资源，然后排除与之前扩展节点内容相似的节点，内容相似度通过编辑距离来度量（参见[章节
    E.1](https://arxiv.org/html/2410.08115v1#A5.SS1 "E.1 数据生成 ‣ 附录 E 实验详情 ‣ Optima：优化基于
    LLM 的多代理系统的效果和效率")）。在剩下的节点中，我们选择奖励最高的 10 个节点，并根据它们的奖励使用 softmax 分布从中抽样一个节点。（2）模拟：对于每个选择的节点，我们扩展
    3 条轨迹，模拟对话直到完成。（3）反向传播：一旦某条轨迹完成并根据[公式 1](https://arxiv.org/html/2410.08115v1#S2.E1
    "在 2.1 概述 ‣ 2 Optima：通过迭代训练优化多代理 LLM ‣ Optima：优化基于 LLM 的多代理系统的效果和效率")获得奖励，我们更新轨迹中所有节点的估计奖励，使用其子节点的平均奖励。（4）迭代：我们重复以上过程
    8 次，生成 24 条轨迹。更多的迭代可能会导致更为多样和高质量的数据。
- en: 'Paired Data Construction: To generate high-quality paired data for DPO training,
    we traverse each MCTS tree and identify node pairs $(n_{i},n_{j})$ that satisfy
    three conditions: (1) shared ancestry, (2) the higher estimated reward of $n_{i}$
    and $n_{j}$ exceeds the threshold $\theta_{\text{dpo-filter}}$, and (3) their
    reward difference exceeds the threshold $\theta_{\text{dpo-diff}}$. We sort these
    pairs by the higher estimated reward, and select the top 50% pairs as part of
    the final training set. We construct DPO training instances by using the common
    conversation history as the prompt, with $n_{i}$ and $n_{j}$ serving as the chosen
    and rejected responses according to their estimated rewards.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 配对数据构建：为了生成高质量的配对数据用于 DPO 训练，我们遍历每棵 MCTS 树，并识别满足以下三个条件的节点对 $(n_{i},n_{j})$：
    (1) 共享祖先，(2) $n_{i}$ 和 $n_{j}$ 的较高估计奖励超过阈值 $\theta_{\text{dpo-filter}}$，(3) 它们的奖励差异超过阈值
    $\theta_{\text{dpo-diff}}$。我们按较高的估计奖励对这些节点对进行排序，并选择排名前 50% 的节点对作为最终训练集的一部分。我们通过使用共同的对话历史作为提示来构建
    DPO 训练实例，其中 $n_{i}$ 和 $n_{j}$ 根据其估计奖励作为选择的和被拒绝的响应。
- en: 'The iDPO process then proceeds iteratively, alternating between MCTS-based
    data generation and model updates using DPO. The pseudo-code for our iDPO process
    is presented in [Algorithm 2](https://arxiv.org/html/2410.08115v1#alg2 "In 2.4
    Framework Instantiation 2: Iterative Direct Preference Optimization ‣ 2 Optima:
    Optimizing Multi-Agent LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: iDPO 过程接着迭代进行，在基于 MCTS 的数据生成和使用 DPO 进行模型更新之间交替进行。我们的 iDPO 过程的伪代码在[算法 2](https://arxiv.org/html/2410.08115v1#alg2
    "在 2.4 框架实例化 2：迭代直接偏好优化 ‣ 2 Optima：通过迭代训练优化多代理 LLM ‣ Optima：优化基于 LLM 的多代理系统的效果和效率")中展示。
- en: Algorithm 2 Iterative Direct Preference Optimization
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 迭代直接偏好优化
- en: '1:Initial model $\mathcal{M}_{\text{init}}$, dataset $\mathcal{D}$, max iterations
    $T$2:Optimized model $\mathcal{M}_{T}$3:$\mathcal{M}_{0}\leftarrow\text{Initialize}(\mathcal{M}_{\text{init}},\mathcal{%
    D})$ $\triangleright$ [Algorithm 3](https://arxiv.org/html/2410.08115v1#alg3 "In
    Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing
    Effectiveness and Efficiency for LLM-Based Multi-Agent System")4:for $t=0$ to
    $T-1$ do5:     $\mathcal{D}_{t}^{\text{DPO}}\leftarrow\emptyset$6:     for each
    $d_{i}\in\mathcal{D}$ do7:         $\mathcal{D}_{i}^{\text{DPO}}\leftarrow\text{MCTSDataGeneration}(\mathcal{M}_{t%
    },d_{i})$ $\triangleright$ Algorithm [5](https://arxiv.org/html/2410.08115v1#alg5
    "Algorithm 5 ‣ Appendix A Inference Scaling Laws on Information Exchange Tasks
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")8:         $\mathcal{D}_{t}^{\text{DPO}}\leftarrow\mathcal{D}_{t}^{\text{DPO}}\cup\mathcal%
    {D}_{i}^{\text{DPO}}$9:     end for10:     $\mathcal{M}_{t+1}\leftarrow\text{DPO}(\mathcal{M}_{t},\mathcal{D}_{t}^{\text{%
    DPO}})$11:end for12:return $\mathcal{M}_{T}$'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始模型 $\mathcal{M}_{\text{init}}$，数据集 $\mathcal{D}$，最大迭代次数 $T$ 2: 优化后的模型
    $\mathcal{M}_{T}$ 3: $\mathcal{M}_{0} \leftarrow \text{Initialize}(\mathcal{M}_{\text{init}},
    \mathcal{D})$ $\triangleright$ [算法 3](https://arxiv.org/html/2410.08115v1#alg3
    "附录 A 信息交换任务中的推理规模定律 ‣ Optima：优化基于 LLM 的多智能体系统的有效性和效率") 4: 对于 $t=0$ 到 $T-1$，执行：
    5:     $\mathcal{D}_{t}^{\text{DPO}} \leftarrow \emptyset$ 6:     对每个 $d_{i} \in
    \mathcal{D}$，执行： 7:         $\mathcal{D}_{i}^{\text{DPO}} \leftarrow \text{MCTSDataGeneration}(\mathcal{M}_{t},
    d_{i})$ $\triangleright$ [算法 5](https://arxiv.org/html/2410.08115v1#alg5 "算法 5
    ‣ 附录 A 信息交换任务中的推理规模定律 ‣ Optima：优化基于 LLM 的多智能体系统的有效性和效率") 8:         $\mathcal{D}_{t}^{\text{DPO}}
    \leftarrow \mathcal{D}_{t}^{\text{DPO}} \cup \mathcal{D}_{i}^{\text{DPO}}$ 9:     结束循环
    10:     $\mathcal{M}_{t+1} \leftarrow \text{DPO}(\mathcal{M}_{t}, \mathcal{D}_{t}^{\text{DPO}})$
    11: 结束循环 12: 返回 $\mathcal{M}_{T}$'
- en: '2.5 Framework Instantiation 3: Hybrid Iterative Training'
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '2.5 框架实例化 3: 混合迭代训练'
- en: Building upon the strengths of both iSFT and iDPO, we investigate a hybrid approach
    that interleaves SFT and DPO in the iterative training process, termed as iSFT-DPO.
    This hybrid method aims to leverage the simplicity and directness of SFT in capturing
    high-quality trajectories, while also benefiting from the nuanced comparative
    learning facilitated by DPO. By alternating between these two training paradigms,
    we hypothesize that the model can more effectively balance the exploration of
    diverse communication strategies with the exploitation of known effective patterns.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在 iSFT 和 iDPO 的优势基础上，我们探讨了一种混合方法，将 SFT 和 DPO 在迭代训练过程中交替使用，称为 iSFT-DPO。这种混合方法旨在利用
    SFT 在捕捉高质量轨迹方面的简洁性和直接性，同时也借助 DPO 所促进的细致比较学习。通过在这两种训练范式之间交替，我们假设模型能够更有效地平衡对多样化沟通策略的探索与已知有效模式的利用。
- en: In practice, we implement this hybrid approach by performing one iteration of
    iSFT followed by one iteration of iDPO, and repeating this cycle throughout the
    training process. This interleaving allows the model to first consolidate learning
    from the best observed trajectories through SFT, and then refine its understanding
    through the comparative preferences provided by DPO.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通过先执行一次 iSFT 迭代，然后执行一次 iDPO 迭代，并在整个训练过程中重复这个循环来实现这种混合方法。这种交替使得模型能够首先通过
    SFT 巩固对最佳观测轨迹的学习，然后通过 DPO 提供的比较偏好来细化其理解。
- en: 3 Experiments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个实验
- en: 'Datasets. We evaluate Optima on two multi-agent settings: information exchange
    (IE) and debate. For IE, we use HotpotQA (Yang et al., [2018](https://arxiv.org/html/2410.08115v1#bib.bib58)),
    2WikiMultiHopQA (2WMHQA) (Ho et al., [2020](https://arxiv.org/html/2410.08115v1#bib.bib21)),
    TriviaQA (Joshi et al., [2017](https://arxiv.org/html/2410.08115v1#bib.bib26)),
    and CBT (Hill et al., [2016](https://arxiv.org/html/2410.08115v1#bib.bib20)).
    For multi-hop datasets (HotpotQA, 2WikiMultiHopQA), we split relevant contexts
    between two agents, ensuring the answer can only be deduced from information exchange.
    For TriviaQA and CBT, contexts are randomly assigned, challenging agents to identify
    and communicate the relevant information effectively. The debate setting employs
    GSM8K (Cobbe et al., [2021](https://arxiv.org/html/2410.08115v1#bib.bib11)), MATH
    (Hendrycks et al., [2021b](https://arxiv.org/html/2410.08115v1#bib.bib19)), ARC’s
    challenge set (ARC-C) (Bhakthavatsalam et al., [2021](https://arxiv.org/html/2410.08115v1#bib.bib4))
    and MMLU (Hendrycks et al., [2021a](https://arxiv.org/html/2410.08115v1#bib.bib18)),
    with one agent as solver and another as critic (Chen et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)).
    We use 0-shot for all benchmarks.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们在两个多智能体场景下评估Optima：信息交换（IE）和辩论。对于IE，我们使用HotpotQA（Yang等， [2018](https://arxiv.org/html/2410.08115v1#bib.bib58)）、2WikiMultiHopQA（2WMHQA）（Ho等，
    [2020](https://arxiv.org/html/2410.08115v1#bib.bib21)）、TriviaQA（Joshi等， [2017](https://arxiv.org/html/2410.08115v1#bib.bib26)）和CBT（Hill等，
    [2016](https://arxiv.org/html/2410.08115v1#bib.bib20)）。对于多跳数据集（HotpotQA，2WikiMultiHopQA），我们将相关上下文在两个智能体之间进行划分，确保答案只能通过信息交换推导得出。对于TriviaQA和CBT，上下文是随机分配的，挑战智能体有效识别并传达相关信息。辩论场景使用GSM8K（Cobbe等，
    [2021](https://arxiv.org/html/2410.08115v1#bib.bib11)）、MATH（Hendrycks等， [2021b](https://arxiv.org/html/2410.08115v1#bib.bib19)）、ARC挑战集（ARC-C）（Bhakthavatsalam等，
    [2021](https://arxiv.org/html/2410.08115v1#bib.bib4)）和MMLU（Hendrycks等， [2021a](https://arxiv.org/html/2410.08115v1#bib.bib18)），其中一个智能体作为求解者，另一个作为批评者（Chen等，
    [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)）。我们对所有基准使用0-shot评估。
- en: Metrics. We report F1 score between generated answers and labels for IE tasks.
    For debate tasks, we employ exact match accuracy (GSM8k, ARC-C, MMLU) or Sympy-based
    (Meurer et al., [2017](https://arxiv.org/html/2410.08115v1#bib.bib38)) equivalence
    checking (MATH), following Lewkowycz et al. ([2022](https://arxiv.org/html/2410.08115v1#bib.bib30)).
    Conversations conclude when agents both mark the same answer with specified special
    tokens or reach a turn limit.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。对于IE任务，我们报告生成的答案与标签之间的F1分数。对于辩论任务，我们使用精确匹配准确度（GSM8K，ARC-C，MMLU）或基于Sympy的（Meurer等，
    [2017](https://arxiv.org/html/2410.08115v1#bib.bib38)）等价性检查（MATH），遵循Lewkowycz等人（[2022](https://arxiv.org/html/2410.08115v1#bib.bib30)）的方法。对话在两个智能体都标记相同答案并使用指定的特殊标记或达到回合限制时结束。
- en: 'Baselines. We compare against single-agent approaches: Chain-of-Thought (CoT)
    (Wei et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib54)) and Self-Consistency
    (SC) with majority voting (Wang et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib52))
    on $n=8$ samples. Given that the generated responses for IE tasks are in free
    form, direct adaptation to majority voting is impractical. Therefore, we first
    compute the pairwise F1 score among the sampled answers, grouping those with a
    pairwise F1 score exceeding 0.9, and report the average F1 score against the label
    for all the answers in the largest grouping. In the multi-agent context, we compare
    against Multi-Agent Debate (MAD) from Du et al. ([2024](https://arxiv.org/html/2410.08115v1#bib.bib12))
    and AutoForm (Chen et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10)).
    MAD utilizes natural language for inter-agent communication, providing a baseline
    for common multi-agent dialogue, while AutoForm encourages agents to leverage
    concise, non-natural-language formats to achieve a better performance-cost ratio,
    offering a comparison point for efficiency-oriented MAS.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 基准对比。我们与单智能体方法进行比较：思维链（CoT）（Wei等， [2022](https://arxiv.org/html/2410.08115v1#bib.bib54)）和自一致性（SC）加多数投票（Wang等，
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib52)），在$n=8$样本上进行评估。由于IE任务生成的响应是自由形式的，直接采用多数投票方法不可行。因此，我们首先计算样本答案之间的成对F1分数，将成对F1分数超过0.9的答案分组，然后报告最大分组中所有答案的平均F1分数与标签的比较。在多智能体场景下，我们与Du等人（[2024](https://arxiv.org/html/2410.08115v1#bib.bib12)）的多智能体辩论（MAD）和Chen等人（[2024d](https://arxiv.org/html/2410.08115v1#bib.bib10)）的AutoForm进行比较。MAD利用自然语言进行智能体间的交流，提供了一个常见多智能体对话的基准，而AutoForm则鼓励智能体利用简洁的非自然语言格式，以实现更好的性能与成本比，为效率导向的MAS提供了对比点。
- en: 'Training Setups. We use Llama 3 8B (Meta, [2024](https://arxiv.org/html/2410.08115v1#bib.bib37))
    as our base model across all benchmarks. Our experiments focus on two-agent scenarios
    without external tools, a design choice that allows us to isolate and analyze
    the core aspects of multi-agent communication and collaboration. By constraining
    our initial investigation to these fundamental settings, we can more clearly demonstrate
    the efficacy of Optima in optimizing inter-agent communication and task performance.
    This approach also provides a strong baseline for future research exploring more
    complex scenarios with multiple agents and tool use. Besides, we train a single
    model for both agents, although training separate models might yield improved
    performance, we leave it for future exploration. Detailed training configurations
    and prompts are provided in [Appendices E](https://arxiv.org/html/2410.08115v1#A5
    "Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System") and [F](https://arxiv.org/html/2410.08115v1#A6
    "Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness and
    Efficiency for LLM-Based Multi-Agent System").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '训练设置。我们在所有基准测试中使用 Llama 3 8B（Meta，[2024](https://arxiv.org/html/2410.08115v1#bib.bib37)）作为基础模型。我们的实验集中于无外部工具的双代理场景，这一设计选择使我们能够孤立并分析多代理通信与协作的核心方面。通过将最初的研究限定在这些基本设置中，我们可以更清晰地展示
    Optima 在优化代理间通信和任务执行中的有效性。该方法还为未来研究更复杂的多代理和工具使用场景提供了坚实的基线。此外，我们为两个代理训练了一个单一模型，尽管训练单独的模型可能会提升性能，但我们将此留待未来探索。详细的训练配置和提示请参考[附录
    E](https://arxiv.org/html/2410.08115v1#A5 "附录 E 实验详情 ‣ Optima: 基于大型语言模型的多代理系统的效能与效率优化")和[F](https://arxiv.org/html/2410.08115v1#A6
    "附录 F 实验中使用的提示 ‣ Optima: 基于大型语言模型的多代理系统的效能与效率优化")。'
- en: 3.1 Benchmark Results
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基准测试结果
- en: 'Table 1: Performance and inference token number comparison across information
    exchange and debate tasks. Best results are indicated in bold, and second-best
    results are underlined for all rows except the last three. The last three rows
    display self-consistency results for Optima variants, with the best results highlighted
    ingreen. Optima variants consistently outperform baselines in task performance
    and/or token efficiency.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：信息交换与辩论任务的性能与推理令牌数比较。最佳结果用粗体表示，次优结果在所有行中均用下划线标示，最后三行显示 Optima 变种的自一致性结果，最佳结果以绿色突出显示。Optima
    变种在任务性能和/或令牌效率方面始终优于基线。
- en: '|  | Information Exchange | Debate |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | 信息交换 | 辩论 |'
- en: '|  | HotpotQA | 2WMH QA | TriviaQA | CBT | MATH | GSM8k | ARC-C | MMLU |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | HotpotQA | 2WMH QA | TriviaQA | CBT | MATH | GSM8k | ARC-C | MMLU |'
- en: '| Method | F1 | #Tok | F1 | #Tok | F1 | #Tok | F1 | #Tok | Acc | #Tok | Acc
    | #Tok | Acc | #Tok | Acc | #Tok |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | F1 | #Tok | F1 | #Tok | F1 | #Tok | F1 | #Tok | 准确率 | #Tok | 准确率 | #Tok
    | 准确率 | #Tok | 准确率 | #Tok |'
- en: '| CoT | 25.6 | 123.7 | 20.5 | 139.8 | 59.8 | 110.3 | 43.4 | 135.3 | 23.9 |
    329.8 | 71.5 | 230.9 | 65.2 | 138.9 | 46.0 | 132.2 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 25.6 | 123.7 | 20.5 | 139.8 | 59.8 | 110.3 | 43.4 | 135.3 | 23.9 |
    329.8 | 71.5 | 230.9 | 65.2 | 138.9 | 46.0 | 132.2 |'
- en: '| SC ($n=8$) | 33.8 | 996.3 | 28.7 | 1052.8 | 70.0 | 891.4 | 52.9 | 1067.7
    | 35.7 | 2600.9 | 80.3 | 1828.7 | 75.6 | 1116.7 | 54.0 | 1056.1 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| SC ($n=8$) | 33.8 | 996.3 | 28.7 | 1052.8 | 70.0 | 891.4 | 52.9 | 1067.7
    | 35.7 | 2600.9 | 80.3 | 1828.7 | 75.6 | 1116.7 | 54.0 | 1056.1 |'
- en: '| MAD | 28.4 | 570.9 | 25.9 | 543.7 | 71.0 | 408.6 | 53.8 | 493.0 | 29.8 |
    1517.6 | 72.5 | 514.7 | 71.4 | 478.0 | 51.5 | 516.7 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| MAD | 28.4 | 570.9 | 25.9 | 543.7 | 71.0 | 408.6 | 53.8 | 493.0 | 29.8 |
    1517.6 | 72.5 | 514.7 | 71.4 | 478.0 | 51.5 | 516.7 |'
- en: '| AutoForm | 28.2 | 97.7 | 24.7 | 117.7 | 60.9 | 74.0 | 35.0 | 64.8 | 26.1
    | 644.3 | 71.0 | 410.5 | 60.2 | 221.2 | 43.8 | 198.5 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| AutoForm | 28.2 | 97.7 | 24.7 | 117.7 | 60.9 | 74.0 | 35.0 | 64.8 | 26.1
    | 644.3 | 71.0 | 410.5 | 60.2 | 221.2 | 43.8 | 198.5 |'
- en: '| Optima-iSFT | 54.5 | 67.6 | 72.4 | 61.2 | 71.9 | 51.5 | 71.8 | 38.5 | 30.1
    | 830.3 | 79.5 | 311.5 | 74.1 | 92.2 | 56.8 | 123.8 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iSFT | 54.5 | 67.6 | 72.4 | 61.2 | 71.9 | 51.5 | 71.8 | 38.5 | 30.1
    | 830.3 | 79.5 | 311.5 | 74.1 | 92.2 | 56.8 | 123.8 |'
- en: '| Optima-iDPO | 52.5 | 45.7 | 66.1 | 35.9 | 69.3 | 69.2 | 66.7 | 37.2 | 30.4
    | 272.8 | 78.5 | 270.1 | 74.5 | 97.8 | 59.6 | 61.6 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iDPO | 52.5 | 45.7 | 66.1 | 35.9 | 69.3 | 69.2 | 66.7 | 37.2 | 30.4
    | 272.8 | 78.5 | 270.1 | 74.5 | 97.8 | 59.6 | 61.6 |'
- en: '| Optima-iSFT-DPO | 55.6 | 63.3 | 74.2 | 54.9 | 77.1 | 32.5 | 70.1 | 38.9 |
    29.3 | 488.1 | 80.4 | 246.5 | 77.1 | 88.0 | 60.2 | 56.7 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iSFT-DPO | 55.6 | 63.3 | 74.2 | 54.9 | 77.1 | 32.5 | 70.1 | 38.9 |
    29.3 | 488.1 | 80.4 | 246.5 | 77.1 | 88.0 | 60.2 | 56.7 |'
- en: '| Optima-iSFT SC | 54.8 | 806.2 | 72.6 | 245.6 | 73.7 | 413.8 | 72.2 | 847.4
    | 32.4 | 2432.9 | 83.1 | 1750.7 | 77.2 | 1148.7 | 60.2 | 874.5 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iSFT SC | 54.8 | 806.2 | 72.6 | 245.6 | 73.7 | 413.8 | 72.2 | 847.4
    | 32.4 | 2432.9 | 83.1 | 1750.7 | 77.2 | 1148.7 | 60.2 | 874.5 |'
- en: '| Optima-iDPO SC | 52.8 | 412.8 | 67.2 | 1056.2 | 71.8 | 702.8 | 66.8 | 520.6
    | 36.9 | 2743.1 | 84.4 | 1750.8 | 77.0 | 1091.2 | 59.9 | 1050.4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iDPO SC | 52.8 | 412.8 | 67.2 | 1056.2 | 71.8 | 702.8 | 66.8 | 520.6
    | 36.9 | 2743.1 | 84.4 | 1750.8 | 77.0 | 1091.2 | 59.9 | 1050.4 |'
- en: '| Optima-iSFT-DPO SC | 57.4 | 957.9 | 76.7 | 1096.0 | 77.5 | 494.1 | 71.8 |
    417.8 | 34.8 | 2788.5 | 84.0 | 1748.7 | 78.8 | 1036.1 | 61.2 | 1026.7 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Optima-iSFT-DPO SC | 57.4 | 957.9 | 76.7 | 1096.0 | 77.5 | 494.1 | 71.8 |
    417.8 | 34.8 | 2788.5 | 84.0 | 1748.7 | 78.8 | 1036.1 | 61.2 | 1026.7 |'
- en: '[Table 1](https://arxiv.org/html/2410.08115v1#S3.T1 "In 3.1 Benchmark Results
    ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based
    Multi-Agent System") showcases Optima’s performance across a diverse set of tasks,
    revealing consistent improvements over baseline methods in both effectiveness
    and efficiency. In IE tasks, Optima variants demonstrate substantial gains, particularly
    in multi-hop reasoning scenarios like HotpotQA and 2WMHQA. Here, iSFT-DPO achieves
    peak performance while significantly reducing token usage compared to the strongest
    baseline SC. Notably, on 2WMHQA, iSFT-DPO improves F1 score by 38.3% (2.8x improvement)
    while using only 10% of the tokens required by MAD. This trend extends to other
    information exchange tasks, where Optima variants maintain high performance with
    drastically lower token counts. The debate tasks present a more nuanced picture,
    yet Optima’s benefits remain evident. Better task performance and token efficiency
    are still observed in ARC-C and MMLU, but for the MATH and GSM8k tasks, Optima
    variants show comparable or slightly lower performance than SC, but still with
    much higher token efficiency. We conjecture this is due to the task’s difficulty
    and the small size of their training set. However, as we will demonstrate in [Section 3.2](https://arxiv.org/html/2410.08115v1#S3.SS2
    "3.2 How Well Does Optima Generalize to OOD Tasks? ‣ 3 Experiments ‣ Optima: Optimizing
    Effectiveness and Efficiency for LLM-Based Multi-Agent System"), Optima models
    trained on MATH transfer effectively to GSM8k, achieving performance nearly equivalent
    to models trained directly on GSM8k, with high token efficiency. More interestingly,
    [Section 3.3](https://arxiv.org/html/2410.08115v1#S3.SS3 "3.3 Can Optima lead
    to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") will show that applying SC to
    Optima variants trained on MATH or GSM8k leads to better inference scaling laws
    on GSM8k compared to CoT SC.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 1](https://arxiv.org/html/2410.08115v1#S3.T1 "在 3.1 基准结果 ‣ 3 实验 ‣ Optima：优化基于大语言模型的多智能体系统的有效性与效率")
    展示了 Optima 在一系列不同任务中的表现，揭示了在有效性和效率上相较基线方法的持续改进。在信息抽取（IE）任务中，Optima 的变体表现出显著的提升，尤其是在像
    HotpotQA 和 2WMHQA 这样的多跳推理场景中。在这些任务中，iSFT-DPO 达到最佳表现，同时显著减少了与最强基线 SC 相比的令牌使用量。特别是在
    2WMHQA 上，iSFT-DPO 提高了 F1 分数 38.3%（提高了 2.8 倍），而所需的令牌仅为 MAD 的 10%。这一趋势也扩展到其他信息交换任务，其中
    Optima 变体在大幅降低令牌数的同时仍保持高性能。辩论任务展现了一个更为复杂的情况，但 Optima 的优势依然显著。在 ARC-C 和 MMLU 任务中，仍然可以观察到更好的任务表现和令牌效率，但在
    MATH 和 GSM8k 任务中，Optima 变体的表现与 SC 相当或稍低，但令牌效率却高得多。我们推测这与任务的难度和其训练集的规模较小有关。然而，正如我们将在
    [第 3.2 节](https://arxiv.org/html/2410.08115v1#S3.SS2 "3.2 Optima 如何在 OOD 任务上泛化？
    ‣ 3 实验 ‣ Optima：优化基于大语言模型的多智能体系统的有效性与效率") 中展示的，基于 MATH 训练的 Optima 模型能够有效转移到 GSM8k
    上，达到了几乎与直接在 GSM8k 上训练的模型相当的表现，同时保持高令牌效率。更有趣的是，[第 3.3 节](https://arxiv.org/html/2410.08115v1#S3.SS3
    "3.3 Optima 能否实现更好的推理扩展规律？ ‣ 3 实验 ‣ Optima：优化基于大语言模型的多智能体系统的有效性与效率") 将展示，应用 SC
    到在 MATH 或 GSM8k 上训练的 Optima 变体时，相较于 CoT SC，GSM8k 上的推理扩展规律得到了改进。'
- en: A closer look at Optima variants reveals interesting trade-offs. Optima-iSFT
    often prioritizes performance at the expense of token efficiency, demonstrating
    the poorest efficiency in 5 of 8 tasks. In contrast, Optima-iDPO often achieves
    remarkable reductions in token usage, occasionally with performance trade-offs.
    Optima-iSFT-DPO emerges as a robust compromise, frequently delivering top-tier
    performance with satisfying token efficiency.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步分析 Optima 变体揭示了有趣的权衡。Optima-iSFT 通常优先考虑性能，而牺牲令牌效率，在 8 个任务中的 5 个任务中表现出最差的效率。相比之下，Optima-iDPO
    常常在令牌使用量上取得显著减少，尽管有时会牺牲部分性能。Optima-iSFT-DPO 则成为了一个稳健的折中方案，常常在保持令牌效率的同时提供顶尖的性能。
- en: 3.2 How Well Does Optima Generalize to OOD Tasks?
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Optima 如何在 OOD 任务上泛化？
- en: 'Table 2: Transfer performance of Optima. We transfer Optima from Hotpot QA
    to 2WMH QA and Trivia QA, and from MATH to GSM8k, with MAD and AutoForm on each
    target task as baselines.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：Optima的迁移性能。我们将Optima从Hotpot QA迁移到2WMH QA和Trivia QA，并从MATH迁移到GSM8k，以MAD和AutoForm作为每个目标任务的基准。
- en: '|  | 2WMH QA | Trivia QA | GSM8k |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 2WMH QA | Trivia QA | GSM8k |'
- en: '| Method | F1 | #Tok | F1 | #Tok | Acc | #Tok |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | F1 | #Tok | F1 | #Tok | 准确率 | #Tok |'
- en: '| MAD | 25.9 | 543.7 | 71.0 | 408.9 | 72.5 | 514.7 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| MAD | 25.9 | 543.7 | 71.0 | 408.9 | 72.5 | 514.7 |'
- en: '| AutoForm | 24.7 | 117.7 | 60.9 | 74.0 | 71.0 | 410.5 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AutoForm | 24.7 | 117.7 | 60.9 | 74.0 | 71.0 | 410.5 |'
- en: '| iSFT | 56.5 | 79.6 | 70.0 | 90.2 | 74.6 | 293.7 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| iSFT | 56.5 | 79.6 | 70.0 | 90.2 | 74.6 | 293.7 |'
- en: '| iDPO | 51.6 | 84.3 | 68.0 | 41.1 | 77.9 | 185.7 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| iDPO | 51.6 | 84.3 | 68.0 | 41.1 | 77.9 | 185.7 |'
- en: '| iSFT-DPO | 54.5 | 70.4 | 72.0 | 67.8 | 74.2 | 363.1 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| iSFT-DPO | 54.5 | 70.4 | 72.0 | 67.8 | 74.2 | 363.1 |'
- en: 'To assess Optima’s ability to generalize, we conducted transfer learning experiments
    across different task domains. We transferred models trained on HotpotQA to TriviaQA
    and 2WMHQA, as well as transferring from MATH to GSM8k. While these datasets share
    broad categories (question-answering and mathematical reasoning, respectively),
    they present different challenges in terms of complexity and required skills.
    The results, presented in [Table 2](https://arxiv.org/html/2410.08115v1#S3.T2
    "In 3.2 How Well Does Optima Generalize to OOD Tasks? ‣ 3 Experiments ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"), demonstrate
    Optima’s robust transferability across these diverse tasks. In the question-answering
    domain, all Optima variants significantly outperform baseline multi-agent methods
    on both OOD datasets. On 2WMHQA, the transferred iSFT more than doubles MAD’s
    F1 score while using only 14.6% of the tokens. Similar trends are observed in
    TriviaQA. When transferring from MATH to GSM8k, Optima variants, particular iDPO,
    not only outperform the baselines on GSM8k but also achieve results comparable
    to models directly trained on GSM8k with even higher token efficiency (refer to
    [Table 1](https://arxiv.org/html/2410.08115v1#S3.T1 "In 3.1 Benchmark Results
    ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based
    Multi-Agent System") for comparison).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估Optima的泛化能力，我们在不同任务领域进行了迁移学习实验。我们将训练于HotpotQA的模型迁移到TriviaQA和2WMHQA，并且将MATH迁移到GSM8k。这些数据集在大类上有所共享（分别是问答和数学推理），但在复杂度和所需技能方面面临不同的挑战。实验结果显示在[表2](https://arxiv.org/html/2410.08115v1#S3.T2
    "在3.2节：Optima如何泛化到OOD任务？ ‣ 3 实验 ‣ Optima：优化基于LLM的多代理系统的效果与效率")中，展示了Optima在这些多样任务中的强大迁移能力。在问答领域，所有Optima变种在这两个OOD数据集上都显著优于基准的多代理方法。在2WMHQA上，迁移后的iSFT在仅使用14.6%
    tokens的情况下，F1得分是MAD的两倍以上。在TriviaQA中也观察到了类似的趋势。当从MATH迁移到GSM8k时，Optima变种，尤其是iDPO，不仅在GSM8k上超越了基准模型，还达到了与直接在GSM8k上训练的模型相媲美的结果，并且token效率更高（参见[表1](https://arxiv.org/html/2410.08115v1#S3.T1
    "在3.1节：基准结果 ‣ 3 实验 ‣ Optima：优化基于LLM的多代理系统的效果与效率")进行对比）。
- en: These results underscore Optima’s potential for developing adaptable MAS, demonstrating
    that Optima-trained models learn transferable skills for efficient information
    exchange and collaborative reasoning. However, transferring to more distant domains
    remains challenging, e.g., we find it hard to transfer from HotpotQA to CBT, or
    from MATH to ARC-C. We believe it is a promising area for future research to explore
    if scaling Optima to more generalized multi-task training could enhance the generalization
    of communication strategies in LLMs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果强调了Optima在开发适应性MAS（多代理系统）方面的潜力，表明Optima训练的模型能学习到可转移的技能，以高效地进行信息交换和协作推理。然而，转移到更遥远的领域仍然具有挑战性，例如我们发现从HotpotQA到CBT，或从MATH到ARC-C的转移都很困难。我们认为，这是未来研究的一个有前景的方向，探索如果将Optima扩展到更为通用的多任务训练，是否能够增强大规模语言模型（LLMs）中交流策略的泛化能力。
- en: 3.3 Can Optima lead to Better Inference Scaling Law?
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 Optima 是否能引领更好的推理扩展法则？
- en: '![Refer to caption](img/9a93fdc664343a0ea92975fead8a3b20.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/9a93fdc664343a0ea92975fead8a3b20.png)'
- en: (a) Inference scaling on debate tasks
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 辩论任务中的推理扩展
- en: '![Refer to caption](img/19505aa410427806a9b933e90fb94535.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/19505aa410427806a9b933e90fb94535.png)'
- en: (b) Performance vs. token usage on GSM8k
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GSM8k上的性能与token使用量
- en: 'Figure 3: Optima’s impact on inference scaling laws. (a) Relationship between
    Optima variants’ self-consistency steps and performance on debate tasks. Solid
    lines represent majority voting accuracy, while dashed lines show coverage. (b)
    Performance of various models on GSM8k as a function of token usage, demonstrating
    Optima’s efficiency gains.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：Optima 对推理扩展规律的影响。 (a) Optima 变种的自一致性步骤与辩论任务性能之间的关系。实线表示多数投票准确率，虚线表示覆盖率。
    (b) 各种模型在 GSM8k 上的性能与令牌使用量的关系，展示了 Optima 在效率上的提升。
- en: Recent research has highlighted the importance of inference scaling laws, which
    describe how model performance improves with increased compute during inference,
    typically by generating multiple samples per problem (Brown et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib5);
    Wu et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib56)). While training
    scaling laws focus on the relationship between model size, dataset size, and performance,
    inference scaling laws explore the trade-off between inference compute budget
    and task accuracy. This paradigm offers a promising avenue for enhancing model
    capabilities without the need for further training models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究突出了推理扩展规律的重要性，推理扩展规律描述了在推理过程中，随着计算资源的增加，模型性能如何提高，通常是通过对每个问题生成多个样本来实现（Brown
    等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib5); Wu 等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib56)）。虽然训练扩展规律关注模型规模、数据集规模与性能之间的关系，但推理扩展规律则探索了推理计算预算与任务准确性之间的权衡。这一范式为提高模型能力提供了一个有前景的途径，而无需进一步训练模型。
- en: '[Fig. 3](https://arxiv.org/html/2410.08115v1#S3.F3 "In 3.3 Can Optima lead
    to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") illustrates Optima’s impact
    on inference scaling laws. The left panel shows the relationship between the number
    of SC steps and performance on multi-agent debate tasks. We observe that while
    majority voting accuracy tends to plateau after a certain number of steps, the
    coverage, defined as the percentage of problems answered correctly at least once,
    continues to improve logarithmically with increased sampling. This trend aligns
    with findings in recent inference scaling law studies (Wu et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib56);
    Chen et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib7)) and suggests
    that more sophisticated answer selection techniques could further boost Optima’s
    performance. We provide additional scaling law figures for all Optima variants
    and on both IE and debate tasks in [Appendix A](https://arxiv.org/html/2410.08115v1#A1
    "Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing
    Effectiveness and Efficiency for LLM-Based Multi-Agent System"), where similar
    trends can be observed.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3](https://arxiv.org/html/2410.08115v1#S3.F3 "在 3.3 节，Optima 能否带来更好的推理扩展规律？
    ‣ 3 实验 ‣ Optima：基于大规模语言模型的多智能体系统优化效能与效率") 说明了 Optima 对推理扩展规律的影响。左侧面板显示了 SC 步骤数与多智能体辩论任务性能之间的关系。我们观察到，尽管多数投票准确率在某个步骤数之后趋于平稳，但覆盖率（定义为至少一次正确回答问题的比例）随着采样的增加而呈对数增长。这一趋势与最近推理扩展规律研究中的发现一致（Wu
    等，[2024](https://arxiv.org/html/2410.08115v1#bib.bib56); Chen 等，[2024a](https://arxiv.org/html/2410.08115v1#bib.bib7)），并表明更复杂的答案选择技术可能进一步提升
    Optima 的性能。我们在[附录 A](https://arxiv.org/html/2410.08115v1#A1 "附录 A：信息交换任务中的推理扩展规律
    ‣ Optima：基于大规模语言模型的多智能体系统优化效能与效率")提供了所有 Optima 变种在 IE 和辩论任务中的其他扩展规律图，类似的趋势可以在其中观察到。'
- en: 'The right panel of [Fig. 3](https://arxiv.org/html/2410.08115v1#S3.F3 "In 3.3
    Can Optima lead to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima: Optimizing
    Effectiveness and Efficiency for LLM-Based Multi-Agent System") demonstrates Optima’s
    efficiency in improving inference scaling laws on the GSM8k task. Optima variants,
    both those trained directly on GSM8k and those transferred from MATH, consistently
    outperform the CoT SC baseline except the iSFT variant transferred from MATH.
    Notably, iDPO trained on GSM8k achieves the performance of CoT-SC at around 10,000
    tokens with 88.5% fewer tokens, effectively “shifting the curve left”. This significant
    reduction in token usage translates to substantial computational savings without
    sacrificing accuracy. Moreover, the MATH-trained Optima variants, except iSFT,
    also deliver better inference scaling laws on GSM8k compared with CoT SC, underscoring
    the framework’s ability to generalize effectively across related tasks.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3](https://arxiv.org/html/2410.08115v1#S3.F3 "在3.3节，Optima能否带来更好的推理扩展规律？
    ‣ 3 实验 ‣ Optima：优化LLM基础的多智能体系统的有效性与效率")的右侧面板展示了Optima在提高GSM8k任务推理扩展规律方面的效率。无论是直接在GSM8k上训练的Optima变体，还是从MATH转移过来的变体，除了从MATH转移过来的iSFT变体外，它们都始终优于CoT
    SC基准。值得注意的是，在GSM8k上训练的iDPO，达到了大约10,000个tokens时，CoT-SC的性能，并且使用了少88.5%的tokens，有效地“将曲线向左移动”。这一显著的token使用减少，意味着在不牺牲准确性的情况下，计算节省了大量资源。此外，除了iSFT外，从MATH训练的Optima变体，在GSM8k上也展现出比CoT
    SC更好的推理扩展规律，凸显了该框架在相关任务中有效泛化的能力。'
- en: These results highlight Optima’s potential to reshape inference scaling laws
    for LLM-based MAS and even general LLM systems. By enabling more efficient use
    of the inference compute budget, Optima allows for better performance at lower
    computational costs or higher performance at the same cost. This efficiency gain
    opens new possibilities for leveraging advanced inference techniques like weighted
    voting or best-of-N selection (Wu et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib56)),
    potentially leading to even greater performance improvements.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果突显了Optima在重新塑造基于LLM的MAS甚至一般LLM系统推理扩展规律方面的潜力。通过实现推理计算预算的更高效使用，Optima可以在较低的计算成本下提供更好的性能，或者在相同成本下获得更高的性能。这一效率提升为利用先进的推理技术，如加权投票或最优选择（Wu等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib56)）开辟了新可能，可能会带来更大的性能提升。
- en: 'Table 3: Ablation study on reward components for Optima variants on two representative
    tasks.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：关于Optima变体在两个代表性任务上奖励组件的消融研究。
- en: '|  | 2WMH QA | ARC-C |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | 2WMH QA | ARC-C |'
- en: '| Setting | F1 | #Tok | Acc | #Tok |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | F1 | #Tok | 准确率 | #Tok |'
- en: '| iSFT | 72.4 | 61.2 | 74.1 | 92.2 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| iSFT | 72.4 | 61.2 | 74.1 | 92.2 |'
- en: '| w/o #Tokens | 72.4${}_{(\text{0.0})}$ | 290.3$\color[rgb]{1,0,0}{}_{(\text{4.8x})}$
    | 74.2$\color[rgb]{0,.5,.5}{}_{(\text{+0.1})}$ | 579.6$\color[rgb]{1,0,0}{}_{(\text{6.3x})}$
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 无#Tokens | 72.4${}_{(\text{0.0})}$ | 290.3$\color[rgb]{1,0,0}{}_{(\text{4.8x})}$
    | 74.2$\color[rgb]{0,.5,.5}{}_{(\text{+0.1})}$ | 579.6$\color[rgb]{1,0,0}{}_{(\text{6.3x})}$
    |'
- en: '| w/o Loss | 69.7$\color[rgb]{1,0,0}{}_{(\text{-2.7})}$ | 45.4$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    | 72.6$\color[rgb]{1,0,0}{}_{(\text{-1.5})}$ | 69.7$\color[rgb]{0,.5,.5}{}_{(\text{0.8x})}$
    |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 无Loss | 69.7$\color[rgb]{1,0,0}{}_{(\text{-2.7})}$ | 45.4$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    | 72.6$\color[rgb]{1,0,0}{}_{(\text{-1.5})}$ | 69.7$\color[rgb]{0,.5,.5}{}_{(\text{0.8x})}$
    |'
- en: '| iDPO | 66.1 | 35.9 | 74.5 | 97.8 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| iDPO | 66.1 | 35.9 | 74.5 | 97.8 |'
- en: '| w/o #Tokens | 72.9$\color[rgb]{0,.5,.5}{}_{(\text{+6.8})}$ | 183.3$\color[rgb]{1,0,0}{}_{(\text{5.1x})}$
    | 75.5$\color[rgb]{0,.5,.5}{}_{(\text{+1.0})}$ | 266.0$\color[rgb]{1,0,0}{}_{(\text{2.7x})}$
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 无#Tokens | 72.9$\color[rgb]{0,.5,.5}{}_{(\text{+6.8})}$ | 183.3$\color[rgb]{1,0,0}{}_{(\text{5.1x})}$
    | 75.5$\color[rgb]{0,.5,.5}{}_{(\text{+1.0})}$ | 266.0$\color[rgb]{1,0,0}{}_{(\text{2.7x})}$
    |'
- en: '| w/o Loss | 63.0$\color[rgb]{1,0,0}{}_{(\text{-3.1})}$ | 54.6$\color[rgb]{1,0,0}{}_{(\text{1.5x})}$
    | 74.4$\color[rgb]{1,0,0}{}_{(\text{-0.1})}$ | 81.2$\color[rgb]{0,.5,.5}{}_{(\text{0.8x})}$
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 无Loss | 63.0$\color[rgb]{1,0,0}{}_{(\text{-3.1})}$ | 54.6$\color[rgb]{1,0,0}{}_{(\text{1.5x})}$
    | 74.4$\color[rgb]{1,0,0}{}_{(\text{-0.1})}$ | 81.2$\color[rgb]{0,.5,.5}{}_{(\text{0.8x})}$
    |'
- en: '| iSFT-DPO | 74.2 | 54.9 | 77.1 | 88.0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| iSFT-DPO | 74.2 | 54.9 | 77.1 | 88.0 |'
- en: '| w/o #Tokens | 63.5$\color[rgb]{1,0,0}{}_{(\text{-10.7})}$ | 219.7$\color[rgb]{1,0,0}{}_{(\text{4.0x})}$
    | 76.9$\color[rgb]{1,0,0}{}_{(\text{-0.2})}$ | 354.8$\color[rgb]{1,0,0}{}_{(\text{4.0x})}$
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 无#Tokens | 63.5$\color[rgb]{1,0,0}{}_{(\text{-10.7})}$ | 219.7$\color[rgb]{1,0,0}{}_{(\text{4.0x})}$
    | 76.9$\color[rgb]{1,0,0}{}_{(\text{-0.2})}$ | 354.8$\color[rgb]{1,0,0}{}_{(\text{4.0x})}$
    |'
- en: '| w/o Loss | 66.7$\color[rgb]{1,0,0}{}_{(\text{-7.5})}$ | 38.1$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    | 76.3$\color[rgb]{1,0,0}{}_{(\text{-0.8})}$ | 63.4$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 无Loss | 66.7$\color[rgb]{1,0,0}{}_{(\text{-7.5})}$ | 38.1$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    | 76.3$\color[rgb]{1,0,0}{}_{(\text{-0.8})}$ | 63.4$\color[rgb]{0,.5,.5}{}_{(\text{0.7x})}$
    |'
- en: 3.4 How Does Optima Evolve Agent Communication and Performance?
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 Optima 如何促进智能体通信和性能的演化？
- en: 'To understand the impact of different components in our reward function, we
    conducted an ablation study on two representative tasks: 2WMHQA for IE and ARC-C
    for debate. We examined the performance of Optima variants by removing either
    the token count regularization (#Tokens) or the LM loss (Loss) from the reward
    function. The results aim to answer two key questions: (1) How does token count
    regularization affect the efficiency-performance trade-off? (2) What is the role
    of language modeling loss in maintaining communication quality? Our findings consistently
    demonstrate the crucial role of each reward component in balancing task performance,
    communication efficiency, and language quality.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解奖励函数中不同组件的影响，我们对两个代表性任务进行了消融研究：IE 的 2WMHQA 和辩论任务的 ARC-C。我们通过去除奖励函数中的 token
    计数正则化（#Tokens）或 LM 损失（Loss）来检查 Optima 变体的性能。结果旨在回答两个关键问题：（1）token 计数正则化如何影响效率与性能的权衡？（2）语言建模损失在维持通信质量中的作用是什么？我们的发现始终表明，每个奖励组件在平衡任务性能、通信效率和语言质量方面都发挥着至关重要的作用。
- en: '[Table 3](https://arxiv.org/html/2410.08115v1#S3.T3 "In 3.3 Can Optima lead
    to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") presents the results of our
    ablation study. Removing the token count led to a substantial increase in the
    number of generated tokens across settings, with a particularly pronounced effect
    in the debate task. While this increased verbosity occasionally resulted in marginal
    performance improvements, it came at a significant computational cost. Conversely,
    eliminating the LM loss resulted in a decrease in token usage, often producing
    the most concise outputs among all variants. Examples comparing communication
    with and without LM loss can be found in [Appendix C](https://arxiv.org/html/2410.08115v1#A3
    "Appendix C Case Study on Reward Components Ablation ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). Without LM loss, the model
    often generated overly concise messages containing insufficient information and
    was prone to hallucination, potentially explaining the inferior performance under
    this condition. These results underscore that effective LLM-based MAS should optimize
    not only for task performance but also for the efficiency and quality of inter-agent
    dialogue. The design of Optima’s reward function enables this holistic optimization,
    leading to more effective and efficient multi-agent collaboration while highlighting
    the delicate balance required in optimizing such systems.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[表格 3](https://arxiv.org/html/2410.08115v1#S3.T3 "在 3.3 小节《Optima 能否带来更好的推理扩展法则？
    ‣ 3 实验 ‣ Optima：优化基于 LLM 的多智能体系统的效能和效率》中") 展示了我们的消融研究结果。移除 token 计数导致在各个设置下生成的
    token 数量显著增加，特别是在辩论任务中，效果尤为明显。虽然这种冗长偶尔会带来轻微的性能提升，但它伴随着巨大的计算成本。相反，去除 LM 损失导致 token
    使用量减少，通常在所有变种中生成最简洁的输出。比较有无 LM 损失的通信示例可以在 [附录 C](https://arxiv.org/html/2410.08115v1#A3
    "附录 C 奖励组件消融的案例研究 ‣ Optima：优化基于 LLM 的多智能体系统的效能和效率") 中找到。没有 LM 损失时，模型通常生成过于简洁的消息，信息不足，并且容易产生幻觉，这可能解释了在此条件下性能较差的原因。这些结果强调了有效的基于
    LLM 的 MAS 应该不仅优化任务性能，还要优化智能体间对话的效率和质量。Optima 奖励函数的设计实现了这种全面优化，从而促使多智能体协作更加高效和有效，同时突出了优化此类系统时所需的微妙平衡。'
- en: 3.5 How Agent Communication Evolves over Optimization Iterations?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 智能体通信如何随着优化迭代演化？
- en: '![Refer to caption](img/1a196666fa3213ef8dcc7e29707ab571.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1a196666fa3213ef8dcc7e29707ab571.png)'
- en: 'Figure 4: Case study: Evolution of agent communication in Optima-iSFT across
    iterations on 2WMH QA. The different contexts given to the two agents are omitted
    for brevity. The progression demonstrates increasing efficiency and task-oriented
    communication.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：案例研究：Optima-iSFT 在 2WMH QA 上跨迭代的智能体通信演变。为简洁起见，省略了给两位智能体提供的不同背景。该进展展示了通信效率和任务导向沟通的逐步提升。
- en: '[Fig. 1](https://arxiv.org/html/2410.08115v1#S0.F1 "In Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") illustrates the performance
    gains and token efficiency of Optima variants across the optimization iterations,
    revealing a distinctive two-phase optimization pattern. In the initial phase (iterations
    0-1), we observe a substantial improvement in task performance for all Optima
    variants, accompanied by a clear increase in token usage. This suggests that Optima
    initially prioritizes effectiveness, allowing agents to develop sophisticated
    problem-solving strategies through expanded communication. The subsequent iterations
    demonstrate Optima’s ability to refine these strategies for efficiency without
    compromising performance. We observe a gradual but consistent decrease in token
    usage across all variants, coupled with continued performance improvements.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1](https://arxiv.org/html/2410.08115v1#S0.F1 "在Optima：优化基于LLM的多代理系统的效能和效率")展示了Optima变体在优化迭代过程中的性能提升和令牌效率，揭示了一种独特的两阶段优化模式。在初始阶段（第0-1次迭代），我们观察到所有Optima变体的任务表现显著提高，并伴随着令牌使用的明显增加。这表明Optima在初期优先考虑效能，允许代理通过扩展沟通来开发复杂的解决问题的策略。随后的迭代展示了Optima精炼这些策略以提高效率，而不牺牲性能。我们观察到所有变体的令牌使用量逐渐但持续减少，同时性能仍在不断提升。'
- en: 'To provide concrete examples of how Optima shapes agent communication, we present
    a case from iSFT on an information exchange task in [Fig. 4](https://arxiv.org/html/2410.08115v1#S3.F4
    "In 3.5 How Agent Communication Evolves over Optimization Iterations? ‣ 3 Experiments
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System").
    The base model exhibits unfocused and repetitive exchanges, failing to efficiently
    address the task at hand. At iteration 0, while more structured, the exchange
    is verbose and includes unnecessary metadata. By iteration 2, we observe a marked
    shift towards concise, task-oriented communication, with agents adopting a streamlined
    format that efficiently conveys key information. The final iteration demonstrates
    further refinement, with agents maintaining the efficient structure while eliminating
    any residual verbosity. This progression aligns with our quantitative findings,
    showcasing Optima’s ability to form communication patterns that are both highly
    effective and remarkably efficient.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供具体的例子来说明Optima如何塑造代理之间的沟通，我们展示了来自iSFT的一个案例，涉及[图4](https://arxiv.org/html/2410.08115v1#S3.F4
    "在3.5章节中，代理沟通如何随着优化迭代发展 ‣ 3 实验 ‣ Optima：优化基于LLM的多代理系统的效能和效率")中的信息交换任务。基础模型展示了无焦点且重复的交流，未能高效地完成任务。在第0次迭代时，虽然交流更加结构化，但内容冗长且包含了不必要的元数据。到第2次迭代时，我们观察到一种显著的变化，交流变得简洁、以任务为导向，代理采用了一种简化格式，能够高效地传达关键信息。最后一次迭代进一步优化，代理保持了高效的结构，同时消除了任何多余的冗长。这一进展与我们的定量研究结果一致，展示了Optima能够形成既高效又极具成效的沟通模式。
- en: 4 Related Work
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: LLM-Based MAS. LLM-based MAS have emerged as a powerful paradigm for addressing
    complex tasks across various domains. Seminal works by Liang et al. ([2023](https://arxiv.org/html/2410.08115v1#bib.bib33))
    and Du et al. ([2024](https://arxiv.org/html/2410.08115v1#bib.bib12)) demonstrated
    the potential of LLM-powered agents in collaborative problem-solving through multi-agent
    debate. This foundation has sparked diverse research directions, including role-playing
    for complex reasoning (Wang et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib53);
    Chen et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)), collaborative
    software development (Qian et al., [2024c](https://arxiv.org/html/2410.08115v1#bib.bib44);
    Hong et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib22); Ishibashi
    & Nishimura, [2024](https://arxiv.org/html/2410.08115v1#bib.bib24)), and embodied
    agent interactions (Zhang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib60);
    Mandi et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib36); Guo et al.,
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib17)). Recent studies have shown
    that increasing the number and diversity of agents can lead to performance gains
    in MAS (Wang et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib51); Li
    et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib31); Chen et al., [2024c](https://arxiv.org/html/2410.08115v1#bib.bib9)).
    However, as LLM-based MAS grow in scale and complexity, challenges related to
    computational costs and communication efficiency become more pronounced (Chen
    et al., [2024d](https://arxiv.org/html/2410.08115v1#bib.bib10); Li et al., [2024b](https://arxiv.org/html/2410.08115v1#bib.bib32)).
    Notably, there is a lack of systematic training algorithms specifically designed
    to optimize both the effectiveness and efficiency of LLM-based multi-agent systems,
    with most existing approaches relying on updating agent memory (Qian et al., [2024a](https://arxiv.org/html/2410.08115v1#bib.bib42);
    Gao et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib15)). Our work addresses
    this gap by introducing a training framework that simultaneously enhances communication
    efficiency and task effectiveness in LLM-based MAS.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于LLM的MAS。基于LLM的MAS已成为应对各种领域复杂任务的强大范式。Liang等人（[2023](https://arxiv.org/html/2410.08115v1#bib.bib33)）和Du等人（[2024](https://arxiv.org/html/2410.08115v1#bib.bib12)）的开创性工作展示了LLM驱动的代理在通过多代理辩论进行协作性问题解决中的潜力。这个基础激发了多种研究方向，包括用于复杂推理的角色扮演（Wang等人，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib53);
    Chen等人，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib8)），协作软件开发（Qian等人，[2024c](https://arxiv.org/html/2410.08115v1#bib.bib44);
    Hong等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib22); Ishibashi & Nishimura，[2024](https://arxiv.org/html/2410.08115v1#bib.bib24)），以及具身代理交互（Zhang等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib60);
    Mandi等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib36); Guo等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib17)）。近期的研究表明，增加代理的数量和多样性可以提高MAS的性能（Wang等人，[2024a](https://arxiv.org/html/2410.08115v1#bib.bib51);
    Li等人，[2024a](https://arxiv.org/html/2410.08115v1#bib.bib31); Chen等人，[2024c](https://arxiv.org/html/2410.08115v1#bib.bib9)）。然而，随着基于LLM的MAS在规模和复杂度上不断扩展，计算成本和通信效率相关的挑战变得更加突出（Chen等人，[2024d](https://arxiv.org/html/2410.08115v1#bib.bib10);
    Li等人，[2024b](https://arxiv.org/html/2410.08115v1#bib.bib32)）。值得注意的是，缺乏专门设计的系统化训练算法来优化基于LLM的多代理系统的有效性和效率，大多数现有方法依赖于更新代理内存（Qian等人，[2024a](https://arxiv.org/html/2410.08115v1#bib.bib42);
    Gao等人，[2024](https://arxiv.org/html/2410.08115v1#bib.bib15)）。我们的工作通过引入一个训练框架，填补了这一空白，该框架能够同时提高基于LLM的MAS的通信效率和任务有效性。
- en: Iterative Refinement of LLMs. The pursuit of continual improvement in LLMs has
    led to the development of various iterative refinement paradigms. While self-reflection
    mechanisms like Reflexion (Shinn et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib48))
    and self-refine (Madaan et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib35))
    show promise, they heavily rely on LLMs’ limited self-correction abilities, which
    is relatively weak for most of the current LLMs (Huang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib23);
    Olausson et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib39); Kamoi
    et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib27)). More robust approaches
    focus on iterative parameter updates, for example, ReST (Gülçehre et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib16)),
    ReST${}^{\text{EM}}$ (Singh et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib49))
    and STaR (Zelikman et al., [2022](https://arxiv.org/html/2410.08115v1#bib.bib59))
    train models on self-generated high-quality reasoning paths, Pang et al. ([2024](https://arxiv.org/html/2410.08115v1#bib.bib41))
    further integrate the incorrect self-generated paths and train models with DPO.
    The extension to complex, multi-step tasks (Aksitov et al., [2023](https://arxiv.org/html/2410.08115v1#bib.bib1))
    further demonstrates the versatility of these methods. However, iterative refinement
    remains largely unexplored in the context of LLM-based MAS. Our work addresses
    this gap by presenting the first effective training framework for iteratively
    optimizing LLMs in MAS contexts. By simultaneously enhancing communication efficiency
    and task effectiveness, our approach shows the potential of iterative training
    in MAS.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的迭代精炼。LLM的持续改进推动了各种迭代精炼范式的发展。尽管自我反思机制，如Reflexion（Shinn等， [2023](https://arxiv.org/html/2410.08115v1#bib.bib48)）和self-refine（Madaan等，
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib35)）显示出潜力，但它们在很大程度上依赖于LLM有限的自我修正能力，而大多数当前的LLM在这方面较为薄弱（Huang等，
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib23)；Olausson等， [2024](https://arxiv.org/html/2410.08115v1#bib.bib39)；Kamoi等，
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib27)）。更为强大的方法侧重于迭代参数更新，例如，ReST（Gülçehre等，
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib16)），ReST${}^{\text{EM}}$（Singh等，
    [2024](https://arxiv.org/html/2410.08115v1#bib.bib49)）和STaR（Zelikman等， [2022](https://arxiv.org/html/2410.08115v1#bib.bib59)）通过在自生成的高质量推理路径上训练模型，Pang等（[2024](https://arxiv.org/html/2410.08115v1#bib.bib41)）进一步整合了不正确的自生成路径并通过DPO训练模型。扩展到复杂的多步任务（Aksitov等，
    [2023](https://arxiv.org/html/2410.08115v1#bib.bib1)）进一步展示了这些方法的多样性。然而，迭代精炼在基于LLM的MAS背景下仍然是一个较为未被探索的领域。我们的工作通过提出首个有效的框架，旨在MAS背景下迭代优化LLM。通过同时提高通信效率和任务效果，我们的方法展示了迭代训练在MAS中的潜力。
- en: 5 Conclusion
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We present Optima, a novel framework for training LLM-based MAS that significantly
    improves communication efficiency and task performance. Extensive experiments
    across a range of tasks demonstrate Optima’s consistent superiority over both
    single-agent and multi-agent baselines. The framework introduces key innovations
    such as iterative training techniques, a balanced reward function, and an MCTS-inspired
    approach for data generation. Optima also shows promise in enhancing inference
    scaling laws and transferring knowledge to OOD tasks. These findings highlight
    the critical role of efficient communication in MAS and LLM systems. While Optima
    marks a major step forward in multi-agent LLM training, further exploration into
    its scalability to larger models and more complex scenarios is a promising direction
    for future research.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了Optima，一个新颖的框架，用于训练基于LLM的MAS，它显著提高了通信效率和任务执行性能。通过在一系列任务中的广泛实验，展示了Optima在单代理和多代理基准上的一致性优势。该框架引入了关键的创新，如迭代训练技术、平衡的奖励函数，以及一个受MCTS启发的数据生成方法。Optima还显示出在增强推理扩展规律和将知识迁移到OOD任务方面的潜力。这些发现突显了高效通信在MAS和LLM系统中的关键作用。尽管Optima标志着多代理LLM训练的重大进展，但进一步探索其在更大模型和更复杂场景中的可扩展性是未来研究的一个有前景的方向。
- en: References
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aksitov et al. (2023) Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang
    Li, Sheila Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash,
    Pranesh Srinivasan, Manzil Zaheer, Felix X. Yu, and Sanjiv Kumar. Rest meets react:
    Self-improvement for multi-step reasoning LLM agent. *CoRR*, abs/2312.10003, 2023.
    doi: 10.48550/ARXIV.2312.10003. URL [https://doi.org/10.48550/arXiv.2312.10003](https://doi.org/10.48550/arXiv.2312.10003).'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aksitov 等人（2023）Renat Aksitov, Sobhan Miryoosefi, Zonglin Li, Daliang Li, Sheila
    Babayan, Kavya Kopparapu, Zachary Fisher, Ruiqi Guo, Sushant Prakash, Pranesh
    Srinivasan, Manzil Zaheer, Felix X. Yu 和 Sanjiv Kumar. Rest meets react：多步推理 LLM
    代理的自我提升。*CoRR*，abs/2312.10003，2023。doi: 10.48550/ARXIV.2312.10003。网址 [https://doi.org/10.48550/arXiv.2312.10003](https://doi.org/10.48550/arXiv.2312.10003).'
- en: Anthropic (2024) Anthropic. Claude 3.5 sonnet, 2024. URL [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic（2024）Anthropic. Claude 3.5 sonnet，2024。网址 [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet).
- en: Baker et al. (2020) Bowen Baker, Ingmar Kanitscheider, Todor M. Markov, Yi Wu,
    Glenn Powell, Bob McGrew, and Igor Mordatch. Emergent tool use from multi-agent
    autocurricula. In *8th International Conference on Learning Representations, ICLR
    2020, Addis Ababa, Ethiopia, April 26-30, 2020*. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SkxpxJBKwS](https://openreview.net/forum?id=SkxpxJBKwS).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baker 等人（2020）Bowen Baker, Ingmar Kanitscheider, Todor M. Markov, Yi Wu, Glenn
    Powell, Bob McGrew 和 Igor Mordatch. 来自多智能体自适应课程的工具使用突现。在 *第八届国际学习表示大会，ICLR 2020，埃塞俄比亚亚的斯亚贝巴，2020年4月26日至30日*。OpenReview.net，2020。网址
    [https://openreview.net/forum?id=SkxpxJBKwS](https://openreview.net/forum?id=SkxpxJBKwS).
- en: Bhakthavatsalam et al. (2021) Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar
    Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick,
    Oyvind Tafjord, and Peter Clark. Think you have solved direct-answer question
    answering? try arc-da, the direct-answer AI2 reasoning challenge. *CoRR*, abs/2102.03315,
    2021. URL [https://arxiv.org/abs/2102.03315](https://arxiv.org/abs/2102.03315).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bhakthavatsalam 等人（2021）Sumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot,
    Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind
    Tafjord 和 Peter Clark. 你认为自己已经解决了直接回答问题？试试 arc-da，直接回答 AI2 推理挑战。*CoRR*，abs/2102.03315，2021。网址
    [https://arxiv.org/abs/2102.03315](https://arxiv.org/abs/2102.03315).
- en: 'Brown et al. (2024) Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich,
    Ronald Clark, Quoc V. Le, Christopher Ré, and Azalia Mirhoseini. Large language
    monkeys: Scaling inference compute with repeated sampling. *CoRR*, abs/2407.21787,
    2024. doi: 10.48550/ARXIV.2407.21787. URL [https://doi.org/10.48550/arXiv.2407.21787](https://doi.org/10.48550/arXiv.2407.21787).'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等人（2024）Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald
    Clark, Quoc V. Le, Christopher Ré 和 Azalia Mirhoseini. 大型语言猴子：通过重复采样扩展推理计算。*CoRR*，abs/2407.21787，2024。doi:
    10.48550/ARXIV.2407.21787。网址 [https://doi.org/10.48550/arXiv.2407.21787](https://doi.org/10.48550/arXiv.2407.21787).'
- en: Chaabouni et al. (2022) Rahma Chaabouni, Florian Strub, Florent Altché, Eugene
    Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman,
    Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. In *The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022*. OpenReview.net, 2022. URL [https://openreview.net/forum?id=AUGBfDIV9rL](https://openreview.net/forum?id=AUGBfDIV9rL).
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chaabouni 等人（2022）Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov,
    Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki
    Lazaridou 和 Bilal Piot. 大规模的突现通信。在 *第十届国际学习表示大会，ICLR 2022，虚拟会议，2022年4月25日至29日*。OpenReview.net，2022。网址
    [https://openreview.net/forum?id=AUGBfDIV9rL](https://openreview.net/forum?id=AUGBfDIV9rL).
- en: 'Chen et al. (2024a) Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis,
    Ion Stoica, Matei Zaharia, and James Zou. Are more LLM calls all you need? towards
    scaling laws of compound inference systems. *CoRR*, abs/2403.02419, 2024a. doi:
    10.48550/ARXIV.2403.02419. URL [https://doi.org/10.48550/arXiv.2403.02419](https://doi.org/10.48550/arXiv.2403.02419).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2024a）Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis,
    Ion Stoica, Matei Zaharia 和 James Zou. 仅仅更多的 LLM 调用就够了吗？迈向复合推理系统的扩展法则。*CoRR*，abs/2403.02419，2024a。doi:
    10.48550/ARXIV.2403.02419。网址 [https://doi.org/10.48550/arXiv.2403.02419](https://doi.org/10.48550/arXiv.2403.02419).'
- en: 'Chen et al. (2024b) Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei
    Yuan, Chi-Min Chan, Heyang Yu, Yaxi Lu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin
    Cong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facilitating
    multi-agent collaboration and exploring emergent behaviors. In *The Twelfth International
    Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11,
    2024*. OpenReview.net, 2024b. URL [https://openreview.net/forum?id=EHg5GDnyq1](https://openreview.net/forum?id=EHg5GDnyq1).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2024b）Weize Chen、Yusheng Su、Jingwei Zuo、Cheng Yang、Chenfei Yuan、Chi-Min
    Chan、Heyang Yu、Yaxi Lu、Yi-Hsin Hung、Chen Qian、Yujia Qin、Xin Cong、Ruobing Xie、Zhiyuan
    Liu、Maosong Sun 和 Jie Zhou。Agentverse：促进多代理协作并探索涌现行为。发表于*第十二届国际学习表征会议，ICLR 2024，奥地利维也纳，2024年5月7-11日*。OpenReview.net，2024b。URL
    [https://openreview.net/forum?id=EHg5GDnyq1](https://openreview.net/forum?id=EHg5GDnyq1)。
- en: 'Chen et al. (2024c) Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian,
    Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Internet
    of agents: Weaving a web of heterogeneous agents for collaborative intelligence.
    *CoRR*, abs/2407.07061, 2024c. doi: 10.48550/ARXIV.2407.07061. URL [https://doi.org/10.48550/arXiv.2407.07061](https://doi.org/10.48550/arXiv.2407.07061).'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2024c）Weize Chen、Ziming You、Ran Li、Yitong Guan、Chen Qian、Chenyang Zhao、Cheng
    Yang、Ruobing Xie、Zhiyuan Liu 和 Maosong Sun。代理互联网：为协作智能编织异构代理的网络。*CoRR*，abs/2407.07061，2024c。doi:
    10.48550/ARXIV.2407.07061。URL [https://doi.org/10.48550/arXiv.2407.07061](https://doi.org/10.48550/arXiv.2407.07061)。'
- en: 'Chen et al. (2024d) Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen
    Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. Beyond natural language:
    Llms leveraging alternative formats for enhanced reasoning and communication.
    *CoRR*, abs/2402.18439, 2024d. doi: 10.48550/ARXIV.2402.18439. URL [https://doi.org/10.48550/arXiv.2402.18439](https://doi.org/10.48550/arXiv.2402.18439).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等人（2024d）Weize Chen、Chenfei Yuan、Jiarui Yuan、Yusheng Su、Chen Qian、Cheng
    Yang、Ruobing Xie、Zhiyuan Liu 和 Maosong Sun。超越自然语言：利用替代格式增强推理与沟通的 LLMs。*CoRR*，abs/2402.18439，2024d。doi:
    10.48550/ARXIV.2402.18439。URL [https://doi.org/10.48550/arXiv.2402.18439](https://doi.org/10.48550/arXiv.2402.18439)。'
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *CoRR*, abs/2110.14168, 2021. URL [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人（2021）Karl Cobbe、Vineet Kosaraju、Mohammad Bavarian、Mark Chen、Heewoo
    Jun、Lukasz Kaiser、Matthias Plappert、Jerry Tworek、Jacob Hilton、Reiichiro Nakano、Christopher
    Hesse 和 John Schulman。训练验证者解决数学文字问题。*CoRR*，abs/2110.14168，2021。URL [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)。
- en: Du et al. (2024) Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum,
    and Igor Mordatch. Improving factuality and reasoning in language models through
    multiagent debate. In *Forty-first International Conference on Machine Learning,
    ICML 2024, Vienna, Austria, July 21-27, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=zj7YuTE4t8](https://openreview.net/forum?id=zj7YuTE4t8).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du 等人（2024）Yilun Du、Shuang Li、Antonio Torralba、Joshua B. Tenenbaum 和 Igor Mordatch。通过多代理辩论提升语言模型的事实性和推理能力。发表于*第四十一届国际机器学习会议，ICML
    2024，奥地利维也纳，2024年7月21-27日*。OpenReview.net，2024。URL [https://openreview.net/forum?id=zj7YuTE4t8](https://openreview.net/forum?id=zj7YuTE4t8)。
- en: 'Dubois et al. (2024) Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B.
    Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators.
    *CoRR*, abs/2404.04475, 2024. doi: 10.48550/ARXIV.2404.04475. URL [https://doi.org/10.48550/arXiv.2404.04475](https://doi.org/10.48550/arXiv.2404.04475).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dubois 等人（2024）Yann Dubois、Balázs Galambosi、Percy Liang 和 Tatsunori B. Hashimoto。长度控制的
    alpacaeval：一种简单的方式来消除自动评估器的偏差。*CoRR*，abs/2404.04475，2024。doi: 10.48550/ARXIV.2404.04475。URL
    [https://doi.org/10.48550/arXiv.2404.04475](https://doi.org/10.48550/arXiv.2404.04475)。'
- en: Evtimova et al. (2018) Katrina Evtimova, Andrew Drozdov, Douwe Kiela, and Kyunghyun
    Cho. Emergent communication in a multi-modal, multi-step referential game. In
    *6th International Conference on Learning Representations, ICLR 2018, Vancouver,
    BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*. OpenReview.net,
    2018. URL [https://openreview.net/forum?id=rJGZq6g0-](https://openreview.net/forum?id=rJGZq6g0-).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Evtimova 等人（2018）Katrina Evtimova、Andrew Drozdov、Douwe Kiela 和 Kyunghyun Cho。在一个多模态、多步骤指代游戏中涌现的沟通。发表于*第六届国际学习表征会议，ICLR
    2018，加拿大温哥华，2018年4月30日-5月3日，会议论文集*。OpenReview.net，2018。URL [https://openreview.net/forum?id=rJGZq6g0-](https://openreview.net/forum?id=rJGZq6g0-)。
- en: 'Gao et al. (2024) Shen Gao, Hao Li, Zhengliang Shi, Chengrui Huang, Quan Tu,
    Zhiliang Tian, Minlie Huang, and Shuo Shang. 360{\deg}rea: Towards A reusable
    experience accumulation with 360{\deg} assessment for multi-agent system. *CoRR*,
    abs/2404.05569, 2024. doi: 10.48550/ARXIV.2404.05569. URL [https://doi.org/10.48550/arXiv.2404.05569](https://doi.org/10.48550/arXiv.2404.05569).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao等人（2024）Shen Gao、Hao Li、Zhengliang Shi、Chengrui Huang、Quan Tu、Zhiliang Tian、Minlie
    Huang 和 Shuo Shang。360{\deg}rea：朝着通过360{\deg}评估实现可重用经验积累的方向，为多智能体系统提供支持。*CoRR*，abs/2404.05569，2024年。doi:
    10.48550/ARXIV.2404.05569。网址 [https://doi.org/10.48550/arXiv.2404.05569](https://doi.org/10.48550/arXiv.2404.05569)。'
- en: 'Gülçehre et al. (2023) Çaglar Gülçehre, Tom Le Paine, Srivatsan Srinivasan,
    Ksenia Konyushkova, Lotte Weerts, Abhishek Sharma, Aditya Siddhant, Alex Ahern,
    Miaosen Wang, Chenjie Gu, Wolfgang Macherey, Arnaud Doucet, Orhan Firat, and Nando
    de Freitas. Reinforced self-training (rest) for language modeling. *CoRR*, abs/2308.08998,
    2023. doi: 10.48550/ARXIV.2308.08998. URL [https://doi.org/10.48550/arXiv.2308.08998](https://doi.org/10.48550/arXiv.2308.08998).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gülçehre等人（2023）Çaglar Gülçehre、Tom Le Paine、Srivatsan Srinivasan、Ksenia Konyushkova、Lotte
    Weerts、Abhishek Sharma、Aditya Siddhant、Alex Ahern、Miaosen Wang、Chenjie Gu、Wolfgang
    Macherey、Arnaud Doucet、Orhan Firat 和 Nando de Freitas。强化自训练（rest）用于语言建模。*CoRR*，abs/2308.08998，2023年。doi:
    10.48550/ARXIV.2308.08998。网址 [https://doi.org/10.48550/arXiv.2308.08998](https://doi.org/10.48550/arXiv.2308.08998)。'
- en: 'Guo et al. (2024) Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia
    Vélez, Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, and Mengdi Wang. Embodied
    LLM agents learn to cooperate in organized teams. *CoRR*, abs/2403.12482, 2024.
    doi: 10.48550/ARXIV.2403.12482. URL [https://doi.org/10.48550/arXiv.2403.12482](https://doi.org/10.48550/arXiv.2403.12482).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo等人（2024）Xudong Guo、Kaixuan Huang、Jiale Liu、Wenhui Fan、Natalia Vélez、Qingyun
    Wu、Huazheng Wang、Thomas L. Griffiths 和 Mengdi Wang。具身LLM智能体学习在有组织的团队中合作。*CoRR*，abs/2403.12482，2024年。doi:
    10.48550/ARXIV.2403.12482。网址 [https://doi.org/10.48550/arXiv.2403.12482](https://doi.org/10.48550/arXiv.2403.12482)。'
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *9th International Conference on Learning Representations, ICLR
    2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021a. URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2021a）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song 和 Jacob Steinhardt。测量大规模多任务语言理解。在*第9届国际学习表示会议（ICLR 2021）*，2021年5月3-7日，奥地利，虚拟会议。OpenReview.net，2021a年。网址
    [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ)。
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
    Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung
    (eds.), *Proceedings of the Neural Information Processing Systems Track on Datasets
    and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual*,
    2021b. URL [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks等人（2021b）Dan Hendrycks、Collin Burns、Saurav Kadavath、Akul Arora、Steven
    Basart、Eric Tang、Dawn Song 和 Jacob Steinhardt。通过MATH数据集测量数学问题求解。在Joaquin Vanschoren和Sai-Kit
    Yeung（编），*神经信息处理系统数据集与基准轨道会议论文集1，NeurIPS数据集与基准2021，2021年12月，虚拟*，2021b年。网址 [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html)。
- en: 'Hill et al. (2016) Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston.
    The goldilocks principle: Reading children’s books with explicit memory representations.
    In Yoshua Bengio and Yann LeCun (eds.), *4th International Conference on Learning
    Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
    Proceedings*, 2016. URL [http://arxiv.org/abs/1511.02301](http://arxiv.org/abs/1511.02301).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hill等人（2016）Felix Hill、Antoine Bordes、Sumit Chopra 和 Jason Weston。Goldilocks原理：阅读儿童书籍并具有明确的记忆表示。在Yoshua
    Bengio和Yann LeCun（编），*第4届国际学习表示会议（ICLR 2016）*，2016年5月2-4日，波多黎各圣胡安，会议论文集，2016年。网址
    [http://arxiv.org/abs/1511.02301](http://arxiv.org/abs/1511.02301)。
- en: 'Ho et al. (2020) Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa.
    Constructing A multi-hop QA dataset for comprehensive evaluation of reasoning
    steps. In Donia Scott, Núria Bel, and Chengqing Zong (eds.), *Proceedings of the
    28th International Conference on Computational Linguistics, COLING 2020, Barcelona,
    Spain (Online), December 8-13, 2020*, pp.  6609–6625\. International Committee
    on Computational Linguistics, 2020. doi: 10.18653/V1/2020.COLING-MAIN.580. URL
    [https://doi.org/10.18653/v1/2020.coling-main.580](https://doi.org/10.18653/v1/2020.coling-main.580).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ho 等人（2020）Xanh Ho、Anh-Khoa Duong Nguyen、Saku Sugawara 和 Akiko Aizawa。构建一个多跳问答数据集，以全面评估推理步骤。在
    Donia Scott、Núria Bel 和 Chengqing Zong（编），*第28届国际计算语言学会议论文集，COLING 2020，西班牙巴塞罗那（在线），2020年12月8日至13日*，第6609-6625页。国际计算语言学委员会，2020年。doi:
    10.18653/V1/2020.COLING-MAIN.580。URL [https://doi.org/10.18653/v1/2020.coling-main.580](https://doi.org/10.18653/v1/2020.coling-main.580)。'
- en: 'Hong et al. (2024) Sirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng,
    Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan
    Lin, Liyang Zhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu, and Jürgen Schmidhuber.
    Metagpt: Meta programming for A multi-agent collaborative framework. In *The Twelfth
    International Conference on Learning Representations, ICLR 2024, Vienna, Austria,
    May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hong 等人（2024）Sirui Hong、Mingchen Zhuge、Jonathan Chen、Xiawu Zheng、Yuheng Cheng、Jinlin
    Wang、Ceyao Zhang、Zili Wang、Steven Ka Shing Yau、Zijuan Lin、Liyang Zhou、Chenyu Ran、Lingfeng
    Xiao、Chenglin Wu 和 Jürgen Schmidhuber。《MetaGPT：面向多代理协作框架的元编程》。在*第十二届国际学习表征会议，ICLR
    2024，奥地利维也纳，2024年5月7日至11日*。OpenReview.net，2024年。URL [https://openreview.net/forum?id=VtmBAGCN7o](https://openreview.net/forum?id=VtmBAGCN7o)。
- en: Huang et al. (2024) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng,
    Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot self-correct
    reasoning yet. In *The Twelfth International Conference on Learning Representations,
    ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=IkmD3fKBPQ](https://openreview.net/forum?id=IkmD3fKBPQ).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人（2024）Jie Huang、Xinyun Chen、Swaroop Mishra、Huaixiu Steven Zheng、Adams
    Wei Yu、Xinying Song 和 Denny Zhou。《大型语言模型尚不能自我纠正推理》。在*第十二届国际学习表征会议，ICLR 2024，奥地利维也纳，2024年5月7日至11日*。OpenReview.net，2024年。URL
    [https://openreview.net/forum?id=IkmD3fKBPQ](https://openreview.net/forum?id=IkmD3fKBPQ)。
- en: 'Ishibashi & Nishimura (2024) Yoichi Ishibashi and Yoshimasa Nishimura. Self-organized
    agents: A LLM multi-agent framework toward ultra large-scale code generation and
    optimization. *CoRR*, abs/2404.02183, 2024. doi: 10.48550/ARXIV.2404.02183. URL
    [https://doi.org/10.48550/arXiv.2404.02183](https://doi.org/10.48550/arXiv.2404.02183).'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ishibashi & Nishimura（2024）Yoichi Ishibashi 和 Yoshimasa Nishimura。《自组织代理：面向超大规模代码生成和优化的LLM多代理框架》。*CoRR*，abs/2404.02183，2024年。doi:
    10.48550/ARXIV.2404.02183。URL [https://doi.org/10.48550/arXiv.2404.02183](https://doi.org/10.48550/arXiv.2404.02183)。'
- en: 'Johnson et al. (2000) Jeffrey D. Johnson, Jinghong Li, and Zengshi Chen. Reinforcement
    learning: An introduction: R.S. sutton, A.G. barto, MIT press, cambridge, MA 1998,
    322 pp. ISBN 0-262-19398-1. *Neurocomputing*, 35(1-4):205–206, 2000. doi: 10.1016/S0925-2312(00)00324-6.
    URL [https://doi.org/10.1016/S0925-2312(00)00324-6](https://doi.org/10.1016/S0925-2312(00)00324-6).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Johnson 等人（2000）Jeffrey D. Johnson、Jinghong Li 和 Zengshi Chen。《强化学习：简介》：R.S.
    Sutton, A.G. Barto，MIT出版社，剑桥，马萨诸塞州 1998年，322页。ISBN 0-262-19398-1。*Neurocomputing*，35(1-4):205–206，2000。doi:
    10.1016/S0925-2312(00)00324-6。URL [https://doi.org/10.1016/S0925-2312(00)00324-6](https://doi.org/10.1016/S0925-2312(00)00324-6)。'
- en: 'Joshi et al. (2017) Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer.
    Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.
    In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the 55th Annual Meeting
    of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada,
    July 30 - August 4, Volume 1: Long Papers*, pp.  1601–1611\. Association for Computational
    Linguistics, 2017. doi: 10.18653/V1/P17-1147. URL [https://doi.org/10.18653/v1/P17-1147](https://doi.org/10.18653/v1/P17-1147).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Joshi 等人（2017）Mandar Joshi、Eunsol Choi、Daniel S. Weld 和 Luke Zettlemoyer。《TriviaQA：一个大规模远程监督的阅读理解挑战数据集》。在
    Regina Barzilay 和 Min-Yen Kan（编），*第55届计算语言学协会年会论文集，ACL 2017，加拿大温哥华，2017年7月30日至8月4日，第1卷：长篇论文*，第1601-1611页。计算语言学协会，2017年。doi:
    10.18653/V1/P17-1147。URL [https://doi.org/10.18653/v1/P17-1147](https://doi.org/10.18653/v1/P17-1147)。'
- en: 'Kamoi et al. (2024) Ryo Kamoi, Yusen Zhang, Nan Zhang, Jiawei Han, and Rui
    Zhang. When can llms actually correct their own mistakes? A critical survey of
    self-correction of llms. *CoRR*, abs/2406.01297, 2024. doi: 10.48550/ARXIV.2406.01297.
    URL [https://doi.org/10.48550/arXiv.2406.01297](https://doi.org/10.48550/arXiv.2406.01297).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kamoi 等人（2024）Ryo Kamoi、Yusen Zhang、Nan Zhang、Jiawei Han 和 Rui Zhang。何时大型语言模型（LLMs）能够真正纠正自己的错误？关于大型语言模型自我纠错的关键性调查。*CoRR*，abs/2406.01297，2024。doi:
    10.48550/ARXIV.2406.01297。网址 [https://doi.org/10.48550/arXiv.2406.01297](https://doi.org/10.48550/arXiv.2406.01297)。'
- en: 'Lanctot et al. (2017) Marc Lanctot, Vinícius Flores Zambaldi, Audrunas Gruslys,
    Angeliki Lazaridou, Karl Tuyls, Julien Pérolat, David Silver, and Thore Graepel.
    A unified game-theoretic approach to multiagent reinforcement learning. In Isabelle
    Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N.
    Vishwanathan, and Roman Garnett (eds.), *Advances in Neural Information Processing
    Systems 30: Annual Conference on Neural Information Processing Systems 2017, December
    4-9, 2017, Long Beach, CA, USA*, pp.  4190–4203, 2017. URL [https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lanctot 等人（2017）Marc Lanctot、Vinícius Flores Zambaldi、Audrunas Gruslys、Angeliki
    Lazaridou、Karl Tuyls、Julien Pérolat、David Silver 和 Thore Graepel。一个统一的博弈论方法用于多智能体强化学习。发表于
    Isabelle Guyon、Ulrike von Luxburg、Samy Bengio、Hanna M. Wallach、Rob Fergus、S. V.
    N. Vishwanathan 和 Roman Garnett（编），*神经信息处理系统进展 30：2017年神经信息处理系统年会（NeurIPS 2017），美国加利福尼亚州长滩，2017年12月4日至9日*，第4190–4203页，2017。网址
    [https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html)。
- en: Lazaridou et al. (2017) Angeliki Lazaridou, Alexander Peysakhovich, and Marco
    Baroni. Multi-agent cooperation and the emergence of (natural) language. In *5th
    International Conference on Learning Representations, ICLR 2017, Toulon, France,
    April 24-26, 2017, Conference Track Proceedings*. OpenReview.net, 2017. URL [https://openreview.net/forum?id=Hk8N3Sclg](https://openreview.net/forum?id=Hk8N3Sclg).
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lazaridou 等人（2017）Angeliki Lazaridou、Alexander Peysakhovich 和 Marco Baroni。多智能体合作与（自然）语言的出现。发表于
    *第五届国际学习表示会议（ICLR 2017），法国土伦，2017年4月24日至26日，会议轨道论文集*。OpenReview.net，2017。网址 [https://openreview.net/forum?id=Hk8N3Sclg](https://openreview.net/forum?id=Hk8N3Sclg)。
- en: 'Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay V. Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
    Solving quantitative reasoning problems with language models. In Sanmi Koyejo,
    S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances
    in Neural Information Processing Systems 35: Annual Conference on Neural Information
    Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December
    9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz 等人（2022）Aitor Lewkowycz、Anders Andreassen、David Dohan、Ethan Dyer、Henryk
    Michalewski、Vinay V. Ramasesh、Ambrose Slone、Cem Anil、Imanol Schlag、Theo Gutman-Solo、Yuhuai
    Wu、Behnam Neyshabur、Guy Gur-Ari 和 Vedant Misra。利用语言模型解决定量推理问题。发表于 Sanmi Koyejo、S.
    Mohamed、A. Agarwal、Danielle Belgrave、K. Cho 和 A. Oh（编），*神经信息处理系统进展 35：2022年神经信息处理系统年会（NeurIPS
    2022），美国路易斯安那州新奥尔良，2022年11月28日至12月9日*，2022。网址 [http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html)。
- en: 'Li et al. (2024a) Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, and Deheng Ye.
    More agents is all you need. *CoRR*, abs/2402.05120, 2024a. doi: 10.48550/ARXIV.2402.05120.
    URL [https://doi.org/10.48550/arXiv.2402.05120](https://doi.org/10.48550/arXiv.2402.05120).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2024a）Junyou Li、Qin Zhang、Yangbin Yu、Qiang Fu 和 Deheng Ye。更多的智能体就是你所需要的。*CoRR*，abs/2402.05120，2024a。doi:
    10.48550/ARXIV.2402.05120。网址 [https://doi.org/10.48550/arXiv.2402.05120](https://doi.org/10.48550/arXiv.2402.05120)。'
- en: 'Li et al. (2024b) Yunxuan Li, Yibing Du, Jiageng Zhang, Le Hou, Peter Grabowski,
    Yeqing Li, and Eugene Ie. Improving multi-agent debate with sparse communication
    topology. *CoRR*, abs/2406.11776, 2024b. doi: 10.48550/ARXIV.2406.11776. URL [https://doi.org/10.48550/arXiv.2406.11776](https://doi.org/10.48550/arXiv.2406.11776).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等人（2024b）Yunxuan Li、Yibing Du、Jiageng Zhang、Le Hou、Peter Grabowski、Yeqing
    Li 和 Eugene Ie。通过稀疏通信拓扑改善多智能体辩论。*CoRR*，abs/2406.11776，2024b。doi: 10.48550/ARXIV.2406.11776。网址
    [https://doi.org/10.48550/arXiv.2406.11776](https://doi.org/10.48550/arXiv.2406.11776)。'
- en: 'Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang,
    Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking
    in large language models through multi-agent debate. *CoRR*, abs/2305.19118, 2023.
    doi: 10.48550/ARXIV.2305.19118. URL [https://doi.org/10.48550/arXiv.2305.19118](https://doi.org/10.48550/arXiv.2305.19118).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang 等人（2023）Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui
    Wang, Yujiu Yang, Zhaopeng Tu 和 Shuming Shi。通过多智能体辩论鼓励大型语言模型的发散性思维。*CoRR*，abs/2305.19118，2023。doi:
    10.48550/ARXIV.2305.19118。网址 [https://doi.org/10.48550/arXiv.2305.19118](https://doi.org/10.48550/arXiv.2305.19118)。'
- en: 'Liu et al. (2024) Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu,
    Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, and Chen Qian. Autonomous agents
    for collaborative task under information asymmetry. *CoRR*, abs/2406.14928, 2024.
    doi: 10.48550/ARXIV.2406.14928. URL [https://doi.org/10.48550/arXiv.2406.14928](https://doi.org/10.48550/arXiv.2406.14928).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人（2024）Wei Liu, Chenxi Wang, Yifei Wang, Zihao Xie, Rennai Qiu, Yufan
    Dang, Zhuoyun Du, Weize Chen, Cheng Yang 和 Chen Qian。信息不对称下的协作任务自主智能体。*CoRR*，abs/2406.14928，2024。doi:
    10.48550/ARXIV.2406.14928。网址 [https://doi.org/10.48550/arXiv.2406.14928](https://doi.org/10.48550/arXiv.2406.14928)。'
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative refinement with self-feedback.
    In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey
    Levine (eds.), *Advances in Neural Information Processing Systems 36: Annual Conference
    on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA,
    USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Madaan 等人（2023）Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu
    Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck,
    Amir Yazdanbakhsh 和 Peter Clark。Self-refine：带自我反馈的迭代精炼。收录于 Alice Oh, Tristan Naumann,
    Amir Globerson, Kate Saenko, Moritz Hardt 和 Sergey Levine（主编），*神经信息处理系统进展 36：2023
    年神经信息处理系统年度会议，NeurIPS 2023，美国路易斯安那州新奥尔良，2023 年 12 月 10 日 - 16 日*，2023。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/91edff07232fb1b55a505a9e9f6c0ff3-Abstract-Conference.html)。
- en: 'Mandi et al. (2024) Zhao Mandi, Shreeya Jain, and Shuran Song. Roco: Dialectic
    multi-robot collaboration with large language models. In *IEEE International Conference
    on Robotics and Automation, ICRA 2024, Yokohama, Japan, May 13-17, 2024*, pp. 
    286–299\. IEEE, 2024. doi: 10.1109/ICRA57147.2024.10610855. URL [https://doi.org/10.1109/ICRA57147.2024.10610855](https://doi.org/10.1109/ICRA57147.2024.10610855).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mandi 等人（2024）Zhao Mandi, Shreeya Jain 和 Shuran Song。Roco：与大型语言模型进行辩证式多机器人协作。收录于
    *IEEE 国际机器人与自动化会议，ICRA 2024，日本横滨，2024 年 5 月 13 日 - 17 日*，第 286–299 页。IEEE，2024。doi:
    10.1109/ICRA57147.2024.10610855。网址 [https://doi.org/10.1109/ICRA57147.2024.10610855](https://doi.org/10.1109/ICRA57147.2024.10610855)。'
- en: Meta (2024) Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta（2024）Meta。Llama 3 模型卡。2024。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'Meurer et al. (2017) Aaron Meurer, Christopher P. Smith, Mateusz Paprocki,
    Ondrej Certík, Sergey B. Kirpichev, Matthew Rocklin, Amit Kumar, Sergiu Ivanov,
    Jason Keith Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger,
    Richard P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson,
    Fabian Pedregosa, Matthew J. Curry, Andy R. Terrel, Stepán Roucka, Ashutosh Saboo,
    Isuru Fernando, Sumith Kulal, Robert Cimrman, and Anthony M. Scopatz. Sympy: symbolic
    computing in python. *PeerJ Comput. Sci.*, 3:e103, 2017. doi: 10.7717/PEERJ-CS.103.
    URL [https://doi.org/10.7717/peerj-cs.103](https://doi.org/10.7717/peerj-cs.103).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Meurer 等人（2017）Aaron Meurer, Christopher P. Smith, Mateusz Paprocki, Ondrej
    Certík, Sergey B. Kirpichev, Matthew Rocklin, Amit Kumar, Sergiu Ivanov, Jason
    Keith Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian E. Granger, Richard
    P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats, Fredrik Johansson, Fabian
    Pedregosa, Matthew J. Curry, Andy R. Terrel, Stepán Roucka, Ashutosh Saboo, Isuru
    Fernando, Sumith Kulal, Robert Cimrman 和 Anthony M. Scopatz。Sympy：Python中的符号计算。*PeerJ
    Comput. Sci.*，3:e103，2017。doi: 10.7717/PEERJ-CS.103。网址 [https://doi.org/10.7717/peerj-cs.103](https://doi.org/10.7717/peerj-cs.103)。'
- en: Olausson et al. (2024) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang,
    Jianfeng Gao, and Armando Solar-Lezama. Is self-repair a silver bullet for code
    generation? In *The Twelfth International Conference on Learning Representations,
    ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=y0GJXRungR](https://openreview.net/forum?id=y0GJXRungR).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olausson et al. (2024) Theo X. Olausson、Jeevana Priya Inala、王承龙、高剑锋 和 Armando
    Solar-Lezama。自修复是否是代码生成的“灵丹妙药”？收录于*《第十二届国际学习表征会议，ICLR 2024，奥地利维也纳，2024年5月7日至11日》*。OpenReview.net，2024。网址：[https://openreview.net/forum?id=y0GJXRungR](https://openreview.net/forum?id=y0GJXRungR)。
- en: 'OpenAI (2023) OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
    doi: 10.48550/ARXIV.2303.08774. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'OpenAI (2023) OpenAI。GPT-4技术报告。*CoRR*，abs/2303.08774，2023。doi: 10.48550/ARXIV.2303.08774。网址：[https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)。'
- en: 'Pang et al. (2024) Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He,
    Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization.
    *CoRR*, abs/2404.19733, 2024. doi: 10.48550/ARXIV.2404.19733. URL [https://doi.org/10.48550/arXiv.2404.19733](https://doi.org/10.48550/arXiv.2404.19733).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pang et al. (2024) Richard Yuanzhe Pang、袁伟哲、Cho Kyunghyun、何赫、Sainbayar Sukhbaatar
    和 Jason Weston。迭代推理偏好优化。*CoRR*，abs/2404.19733，2024。doi: 10.48550/ARXIV.2404.19733。网址：[https://doi.org/10.48550/arXiv.2404.19733](https://doi.org/10.48550/arXiv.2404.19733)。'
- en: 'Qian et al. (2024a) Chen Qian, Yufan Dang, Jiahao Li, Wei Liu, Zihao Xie, Yifei
    Wang, Weize Chen, Cheng Yang, Xin Cong, Xiaoyin Che, Zhiyuan Liu, and Maosong
    Sun. Experiential co-learning of software-developing agents. In Lun-Wei Ku, Andre
    Martins, and Vivek Srikumar (eds.), *Proceedings of the 62nd Annual Meeting of
    the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024,
    Bangkok, Thailand, August 11-16, 2024*, pp.  5628–5640\. Association for Computational
    Linguistics, 2024a. URL [https://aclanthology.org/2024.acl-long.305](https://aclanthology.org/2024.acl-long.305).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian et al. (2024a) 陈乾、邓宇凡、李家豪、刘威、谢子豪、王一飞、陈维泽、杨澄、从鑫、车晓音、刘志远、孙茂松。软件开发智能体的经验共同学习。收录于Lun-Wei
    Ku、Andre Martins 和 Vivek Srikumar（编），*《第62届计算语言学会年会论文集（卷1：长篇论文）》，ACL 2024，泰国曼谷，2024年8月11日至16日*，第5628-5640页。计算语言学会，2024a。网址：[https://aclanthology.org/2024.acl-long.305](https://aclanthology.org/2024.acl-long.305)。
- en: 'Qian et al. (2024b) Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang,
    Zihao Xie, Weize Chen, Cheng Yang, Yingli Zhang, Zhiyuan Liu, and Maosong Sun.
    Iterative experience refinement of software-developing agents. *CoRR*, abs/2405.04219,
    2024b. doi: 10.48550/ARXIV.2405.04219. URL [https://doi.org/10.48550/arXiv.2405.04219](https://doi.org/10.48550/arXiv.2405.04219).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian et al. (2024b) 陈乾、李家豪、邓宇凡、刘威、王一飞、谢子豪、陈维泽、杨澄、张颖丽、刘志远、孙茂松。软件开发智能体的迭代经验优化。*CoRR*，abs/2405.04219，2024b。doi:
    10.48550/ARXIV.2405.04219。网址：[https://doi.org/10.48550/arXiv.2405.04219](https://doi.org/10.48550/arXiv.2405.04219)。'
- en: 'Qian et al. (2024c) Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang,
    Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, Juyuan Xu, Dahai Li,
    Zhiyuan Liu, and Maosong Sun. Chatdev: Communicative agents for software development.
    In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), *Proceedings of the 62nd
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024*, pp.  15174–15186\.
    Association for Computational Linguistics, 2024c. URL [https://aclanthology.org/2024.acl-long.810](https://aclanthology.org/2024.acl-long.810).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qian et al. (2024c) 陈乾、刘威、刘洪章、陈诺、邓宇凡、李家豪、杨澄、陈维泽、苏宇生、从鑫、许具源、李大海、刘志远、孙茂松。Chatdev:
    软件开发中的沟通型智能体。收录于Lun-Wei Ku、Andre Martins 和 Vivek Srikumar（编），*《第62届计算语言学会年会论文集（卷1：长篇论文）》，ACL
    2024，泰国曼谷，2024年8月11日至16日*，第15174-15186页。计算语言学会，2024c。网址：[https://aclanthology.org/2024.acl-long.810](https://aclanthology.org/2024.acl-long.810)。'
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D.
    Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your
    language model is secretly a reward model. In Alice Oh, Tristan Naumann, Amir
    Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), *Advances in Neural
    Information Processing Systems 36: Annual Conference on Neural Information Processing
    Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, 2023.
    URL [http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rafailov等人（2023年）Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher
    D. Manning, Stefano Ermon, 和 Chelsea Finn。直接偏好优化：你的语言模型实际上是一个奖励模型。在Alice Oh, Tristan
    Naumann, Amir Globerson, Kate Saenko, Moritz Hardt 和 Sergey Levine（编辑）*神经信息处理系统进展
    36：神经信息处理系统年度会议 2023，NeurIPS 2023，新奥尔良，路易斯安那州，美国，2023年12月10日至16日*，2023年。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/a85b405ed65c6477a4fe8302b5e06ce7-Abstract-Conference.html)。
- en: 'Reid et al. (2024) Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
    Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
    Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin
    Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael
    Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk,
    Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens
    Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher,
    Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri,
    Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener,
    and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens
    of context. *CoRR*, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. URL
    [https://doi.org/10.48550/arXiv.2403.05530](https://doi.org/10.48550/arXiv.2403.05530).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reid等人（2024年）Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin,
    Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou,
    Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud,
    Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin
    Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael
    Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk,
    Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens
    Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher,
    Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri,
    Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener等人。*Gemini
    1.5: 解锁跨越数百万标记的多模态理解*。*CoRR*, abs/2403.05530, 2024年。doi: 10.48550/ARXIV.2403.05530。网址
    [https://doi.org/10.48550/arXiv.2403.05530](https://doi.org/10.48550/arXiv.2403.05530)。'
- en: 'Saito et al. (2023) Keita Saito, Akifumi Wachi, Koki Wataoka, and Youhei Akimoto.
    Verbosity bias in preference labeling by large language models. *CoRR*, abs/2310.10076,
    2023. doi: 10.48550/ARXIV.2310.10076. URL [https://doi.org/10.48550/arXiv.2310.10076](https://doi.org/10.48550/arXiv.2310.10076).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Saito等人（2023年）Keita Saito, Akifumi Wachi, Koki Wataoka, 和 Youhei Akimoto。大型语言模型中的偏好标注中的冗长偏向。*CoRR*,
    abs/2310.10076, 2023年。doi: 10.48550/ARXIV.2310.10076。网址 [https://doi.org/10.48550/arXiv.2310.10076](https://doi.org/10.48550/arXiv.2310.10076)。'
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement
    learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
    and Sergey Levine (eds.), *Advances in Neural Information Processing Systems 36:
    Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,
    New Orleans, LA, USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shinn等人（2023年）Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan,
    和 Shunyu Yao。Reflexion：带有语言强化学习的语言代理。在Alice Oh, Tristan Naumann, Amir Globerson,
    Kate Saenko, Moritz Hardt 和 Sergey Levine（编辑）*神经信息处理系统进展 36：神经信息处理系统年度会议 2023，NeurIPS
    2023，新奥尔良，路易斯安那州，美国，2023年12月10日至16日*，2023年。网址 [http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html)。
- en: 'Singh et al. (2024) Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand,
    Piyush Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin
    Xu, Aaron T. Parisi, Abhishek Kumar, Alexander A. Alemi, Alex Rizkowsky, Azade
    Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch,
    Isabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen
    Kenealy, Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L.
    Bileschi, Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian,
    Yamini Bansal, Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein, and Noah Fiedel.
    Beyond human data: Scaling self-training for problem-solving with language models.
    *Trans. Mach. Learn. Res.*, 2024, 2024. URL [https://openreview.net/forum?id=lNAyUngGFK](https://openreview.net/forum?id=lNAyUngGFK).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等人（2024）Avi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush
    Patil, Xavier Garcia, Peter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu, Aaron
    T. Parisi, Abhishek Kumar, Alexander A. Alemi, Alex Rizkowsky, Azade Nova, Ben
    Adlam, Bernd Bohnet, Gamaleldin Fathy Elsayed, Hanie Sedghi, Igor Mordatch, Isabelle
    Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy,
    Kevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi,
    Noah Constant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Yamini Bansal,
    Ethan Dyer, Behnam Neyshabur, Jascha Sohl-Dickstein 和 Noah Fiedel。超越人类数据：通过自我训练扩展问题解决能力与语言模型。*Trans.
    Mach. Learn. Res.*, 2024, 2024. URL [https://openreview.net/forum?id=lNAyUngGFK](https://openreview.net/forum?id=lNAyUngGFK).
- en: 'Song et al. (2024) Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and
    Bill Yuchen Lin. Trial and error: Exploration-based trajectory optimization for
    LLM agents. *CoRR*, abs/2403.02502, 2024. doi: 10.48550/ARXIV.2403.02502. URL
    [https://doi.org/10.48550/arXiv.2403.02502](https://doi.org/10.48550/arXiv.2403.02502).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song 等人（2024）Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li 和 Bill Yuchen
    Lin。试验与错误：基于探索的轨迹优化用于LLM代理。*CoRR*, abs/2403.02502, 2024. doi: 10.48550/ARXIV.2403.02502.
    URL [https://doi.org/10.48550/arXiv.2403.02502](https://doi.org/10.48550/arXiv.2403.02502).'
- en: 'Wang et al. (2024a) Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang, and
    James Zou. Mixture-of-agents enhances large language model capabilities. *CoRR*,
    abs/2406.04692, 2024a. doi: 10.48550/ARXIV.2406.04692. URL [https://doi.org/10.48550/arXiv.2406.04692](https://doi.org/10.48550/arXiv.2406.04692).'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2024a）Junlin Wang, Jue Wang, Ben Athiwaratkun, Ce Zhang 和 James Zou。混合代理增强大语言模型的能力。*CoRR*,
    abs/2406.04692, 2024a. doi: 10.48550/ARXIV.2406.04692. URL [https://doi.org/10.48550/arXiv.2406.04692](https://doi.org/10.48550/arXiv.2406.04692).'
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *The Eleventh International
    Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*.
    OpenReview.net, 2023. URL [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人（2023）Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi,
    Sharan Narang, Aakanksha Chowdhery 和 Denny Zhou。自我一致性改善语言模型的思维链推理。在*第十一届国际学习表征会议，ICLR
    2023，卢旺达基加利，2023年5月1日至5日*。OpenReview.net, 2023. URL [https://openreview.net/forum?id=1PL1NIMMrw](https://openreview.net/forum?id=1PL1NIMMrw).
- en: 'Wang et al. (2024b) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu
    Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language
    models: A task-solving agent through multi-persona self-collaboration. In Kevin
    Duh, Helena Gómez-Adorno, and Steven Bethard (eds.), *Proceedings of the 2024
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024,
    Mexico City, Mexico, June 16-21, 2024*, pp.  257–279\. Association for Computational
    Linguistics, 2024b. doi: 10.18653/V1/2024.NAACL-LONG.15. URL [https://doi.org/10.18653/v1/2024.naacl-long.15](https://doi.org/10.18653/v1/2024.naacl-long.15).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人（2024b）Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei
    和 Heng Ji。释放大语言模型中的新兴认知协同：通过多角色自我协作的任务解决代理。在 Kevin Duh, Helena Gómez-Adorno 和
    Steven Bethard（编），*2024年北美计算语言学协会会议：人类语言技术（第1卷：长篇论文），NAACL 2024，墨西哥城，墨西哥，2024年6月16日至21日*，第257–279页。计算语言学协会，2024b.
    doi: 10.18653/V1/2024.NAACL-LONG.15. URL [https://doi.org/10.18653/v1/2024.naacl-long.15](https://doi.org/10.18653/v1/2024.naacl-long.15).'
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
    Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
    2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等人 (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le 和 Denny Zhou. 连锁思维提示引发大规模语言模型中的推理。发表于 Sanmi
    Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho 和 A. Oh (编)，*神经信息处理系统进展
    35：2022 年神经信息处理系统年会，NeurIPS 2022，新奥尔良，路易斯安那州，美国，2022 年 11 月 28 日 - 12 月 9 日*，2022。网址
    [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)。
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen LLM applications via multi-agent conversation framework. *CoRR*, abs/2308.08155,
    2023. doi: 10.48550/ARXIV.2308.08155. URL [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang 和 Chi Wang. Autogen：通过多代理对话框架实现下一代
    LLM 应用。*CoRR*，abs/2308.08155，2023。doi: 10.48550/ARXIV.2308.08155。网址 [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155)。'
- en: 'Wu et al. (2024) Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming
    Yang. An empirical analysis of compute-optimal inference for problem-solving with
    language models. *CoRR*, abs/2408.00724, 2024. doi: 10.48550/ARXIV.2408.00724.
    URL [https://doi.org/10.48550/arXiv.2408.00724](https://doi.org/10.48550/arXiv.2408.00724).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等人 (2024) Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck 和 Yiming Yang.
    计算最优推理在语言模型问题求解中的经验分析。*CoRR*，abs/2408.00724，2024。doi: 10.48550/ARXIV.2408.00724。网址
    [https://doi.org/10.48550/arXiv.2408.00724](https://doi.org/10.48550/arXiv.2408.00724)。'
- en: 'Xiong et al. (2024) Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun
    Wang, Ke Wang, Cheng Li, Wei Peng, and Sujian Li. Watch every step! LLM agent
    learning via iterative step-level process refinement. *CoRR*, abs/2406.11176,
    2024. doi: 10.48550/ARXIV.2406.11176. URL [https://doi.org/10.48550/arXiv.2406.11176](https://doi.org/10.48550/arXiv.2406.11176).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiong 等人 (2024) Weimin Xiong, Yifan Song, Xiutian Zhao, Wenhao Wu, Xun Wang,
    Ke Wang, Cheng Li, Wei Peng 和 Sujian Li. 观察每一步！通过迭代步骤级过程优化学习 LLM 代理。*CoRR*，abs/2406.11176，2024。doi:
    10.48550/ARXIV.2406.11176。网址 [https://doi.org/10.48550/arXiv.2406.11176](https://doi.org/10.48550/arXiv.2406.11176)。'
- en: 'Yang et al. (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W.
    Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. Hotpotqa: A dataset for
    diverse, explainable multi-hop question answering. In Ellen Riloff, David Chiang,
    Julia Hockenmaier, and Jun’ichi Tsujii (eds.), *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing, Brussels, Belgium, October
    31 - November 4, 2018*, pp.  2369–2380\. Association for Computational Linguistics,
    2018. doi: 10.18653/V1/D18-1259. URL [https://doi.org/10.18653/v1/d18-1259](https://doi.org/10.18653/v1/d18-1259).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人 (2018) Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William
    W. Cohen, Ruslan Salakhutdinov 和 Christopher D. Manning. Hotpotqa：一个多元、可解释的多跳问答数据集。发表于
    Ellen Riloff, David Chiang, Julia Hockenmaier 和 Jun’ichi Tsujii (编)，*2018 年自然语言处理经验方法会议论文集，比利时布鲁塞尔，2018
    年 10 月 31 日 - 11 月 4 日*，第 2369–2380 页。计算语言学协会，2018。doi: 10.18653/V1/D18-1259。网址
    [https://doi.org/10.18653/v1/d18-1259](https://doi.org/10.18653/v1/d18-1259)。'
- en: 'Zelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman.
    Star: Bootstrapping reasoning with reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
    Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
    2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zelikman 等人（2022）Eric Zelikman, Yuhuai Wu, Jesse Mu, 和 Noah D. Goodman. Star:
    使用推理自举推理. 在 Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, 和
    A. Oh（编辑），*神经信息处理系统进展 35：2022年神经信息处理系统年会，NeurIPS 2022，美国路易斯安那州新奥尔良，2022年11月28日至12月9日*，2022年。网址
    [http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/639a9a172c044fbb64175b5fad42e9a5-Abstract-Conference.html)。'
- en: Zhang et al. (2024) Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun
    Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. Building cooperative embodied
    agents modularly with large language models. In *The Twelfth International Conference
    on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024*. OpenReview.net,
    2024. URL [https://openreview.net/forum?id=EnXJfQqy0K](https://openreview.net/forum?id=EnXJfQqy0K).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2024）Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du,
    Joshua B. Tenenbaum, Tianmin Shu, 和 Chuang Gan. 使用大型语言模型模块化构建协作性具身代理。发表于*第十二届国际学习表征会议，ICLR
    2024，奥地利维也纳，2024年5月7日至11日*。OpenReview.net，2024年。网址 [https://openreview.net/forum?id=EnXJfQqy0K](https://openreview.net/forum?id=EnXJfQqy0K)。
- en: 'Zhuge et al. (2024) Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio,
    Dmitrii Khizbullin, and Jürgen Schmidhuber. Gptswarm: Language agents as optimizable
    graphs. In *Forty-first International Conference on Machine Learning, ICML 2024,
    Vienna, Austria, July 21-27, 2024*. OpenReview.net, 2024. URL [https://openreview.net/forum?id=uTC9AFXIhg](https://openreview.net/forum?id=uTC9AFXIhg).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhuge 等人（2024）Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii
    Khizbullin, 和 Jürgen Schmidhuber. Gptswarm: 作为可优化图的语言代理。发表于*第41届国际机器学习会议，ICML
    2024，奥地利维也纳，2024年7月21日至27日*。OpenReview.net，2024年。网址 [https://openreview.net/forum?id=uTC9AFXIhg](https://openreview.net/forum?id=uTC9AFXIhg)。'
- en: Appendix A Inference Scaling Laws on Information Exchange Tasks
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 信息交换任务的推理规模规律
- en: '![Refer to caption](img/a134219138b9437808a544339187819f.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/a134219138b9437808a544339187819f.png)'
- en: (a) iSFT on Debate tasks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: (a) iSFT 在辩论任务中的表现。
- en: '![Refer to caption](img/cf82e7c63ebcbffd2b360f703ccc89cc.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/cf82e7c63ebcbffd2b360f703ccc89cc.png)'
- en: (b) iDPO on Debate tasks.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: (b) iDPO 在辩论任务中的表现。
- en: '![Refer to caption](img/aa464609b9b17c4505691f5b4b08e632.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/aa464609b9b17c4505691f5b4b08e632.png)'
- en: (c) iSFT-DPO on Debate tasks.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: (c) iSFT-DPO 在辩论任务中的表现。
- en: '![Refer to caption](img/78a1c386abd15b6e0f768f1371909785.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/78a1c386abd15b6e0f768f1371909785.png)'
- en: (d) iSFT on IE tasks.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: (d) iSFT 在信息交换（IE）任务中的表现。
- en: '![Refer to caption](img/053ca82e8d5ba8ff9606e7f1b199454c.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/053ca82e8d5ba8ff9606e7f1b199454c.png)'
- en: (e) iDPO on IE tasks.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: (e) iDPO 在信息交换（IE）任务中的表现。
- en: '![Refer to caption](img/e5279133014a5466326f9f35b21fef4a.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/e5279133014a5466326f9f35b21fef4a.png)'
- en: (f) iSFT-DPO on IE tasks.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: (f) iSFT-DPO 在信息交换（IE）任务中的表现。
- en: 'Figure 5: Inference scaling laws for Optima variants on debate and information
    exchange (IE) tasks. (a-c) show results for iSFT, iDPO, and iSFT-DPO on debate
    tasks, respectively. (d-f) present corresponding results for information exchange
    tasks. Solid lines represent majority voting accuracy, while dashed lines show
    coverage.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：关于辩论和信息交换（IE）任务的Optima变体的推理规模规律。（a-c）分别显示了iSFT、iDPO和iSFT-DPO在辩论任务中的结果。（d-f）展示了信息交换任务的相应结果。实线表示多数投票准确度，虚线表示覆盖率。
- en: 'This section extends our analysis of inference scaling laws to information
    exchange (IE) tasks, complementing the debate task results presented in the main
    text ([Section 3.3](https://arxiv.org/html/2410.08115v1#S3.SS3 "3.3 Can Optima
    lead to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")). [Fig. 5](https://arxiv.org/html/2410.08115v1#A1.F5
    "In Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System") provides
    a comprehensive view of how Optima variants perform across both task types as
    the number of SC steps increases.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '本节将我们对推理扩展法则的分析扩展到信息交换（IE）任务，补充了主文中提出的辩论任务结果（[第 3.3 节](https://arxiv.org/html/2410.08115v1#S3.SS3
    "3.3 Can Optima lead to Better Inference Scaling Law? ‣ 3 Experiments ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")）。[图
    5](https://arxiv.org/html/2410.08115v1#A1.F5 "在附录 A 中，关于信息交换任务的推理扩展法则 ‣ Optima:
    优化大规模语言模型（LLM）基础的多代理系统的有效性与效率") 提供了一个全面的视角，展示了随着 SC 步骤数量增加，Optima 各个变种在两种任务类型中的表现。'
- en: 'For debate tasks ([Fig. 5](https://arxiv.org/html/2410.08115v1#A1.F5 "In Appendix
    A Inference Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")a-c), we observe consistent trends
    across all Optima variants. The coverage exhibits a clear log-linear relationship
    with the number of SC steps. This trend is particularly pronounced for the MATH
    task, where the potential for improvement through increased sampling is most evident.
    Majority voting accuracy tends to plateau earlier, suggesting that more sophisticated
    answer selection techniques might be necessary to fully leverage the diversity
    of generated responses.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '对于辩论任务（[图 5](https://arxiv.org/html/2410.08115v1#A1.F5 "在附录 A 中，关于信息交换任务的推理扩展法则
    ‣ Optima: 优化大规模语言模型（LLM）基础的多代理系统的有效性与效率")a-c），我们观察到所有 Optima 变种的趋势一致。覆盖率与 SC 步骤数呈现明显的对数线性关系。这个趋势在
    MATH 任务中尤为明显，其中通过增加采样的潜力最为显著。多数投票准确率趋于平台期，这表明可能需要更复杂的答案选择技术，才能充分利用生成响应的多样性。'
- en: 'In the case of information exchange tasks (Figures [5](https://arxiv.org/html/2410.08115v1#A1.F5
    "Figure 5 ‣ Appendix A Inference Scaling Laws on Information Exchange Tasks ‣
    Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")d-f),
    we note similar log-linear scaling in coverage²²2In IE tasks, we define coverage
    as the average of the highest F1 scores achieved across all generated answers
    for each instance. across all Optima variants. However, the improvement in majority
    voting accuracy for IE tasks is less pronounced compared to debate tasks. This
    discrepancy may be attributed to the specific majority voting variant we designed
    for F1 scores (detailed in [Section 3](https://arxiv.org/html/2410.08115v1#S3
    "3 Experiments ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based
    Multi-Agent System")), which might not be optimal for capturing the nuances of
    partial correctness in these tasks.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '对于信息交换任务（图 [5](https://arxiv.org/html/2410.08115v1#A1.F5 "图 5 ‣ 附录 A 推理扩展法则
    - 信息交换任务 ‣ Optima: 优化大规模语言模型（LLM）基础的多代理系统")d-f），我们注意到所有 Optima 变种在覆盖率上也呈现出类似的对数线性扩展²²2在
    IE 任务中，我们将覆盖率定义为所有生成答案中每个实例最高 F1 分数的平均值。尽管如此，信息交换任务的多数投票准确率提升不如辩论任务那样显著。这一差异可能归因于我们为
    F1 分数设计的特定多数投票变种（详细说明见[第 3 节](https://arxiv.org/html/2410.08115v1#S3 "3 Experiments
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")），该变种可能并不适合捕捉这些任务中部分正确性的细微差别。'
- en: These results, while highlighting some task-specific differences, collectively
    reinforce the potential of Optima-trained models to benefit from increased inference
    compute. The consistent log-linear scaling in coverage across all tasks and variants
    indicates that there is substantial room for performance improvement through more
    advanced answer selection strategies or increased sampling.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果虽然突出了某些特定任务的差异，但整体上加强了通过增加推理计算，Optima训练模型从中受益的潜力。所有任务和变种中的覆盖率呈一致的对数线性扩展，表明通过更先进的答案选择策略或增加采样，性能还有很大的提升空间。
- en: Algorithm 3 Initialization for Diverse Agent Communication
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 多样化代理通信的初始化
- en: 1:Initial model $\mathcal{M}_{0}$, dataset $\mathcal{D}$, format pool $\mathcal{F}$,
    sample size $N$, reward threshold $\theta_{\text{init}}$2:Initialized model $\mathcal{M}_{\text{init}}$3:$\mathcal{D}_{\text{init}}^{*}\leftarrow\emptyset$
    $\triangleright$ Initialize dataset for high-quality diverse trajectories4:for each
    $d_{i}\in\mathcal{D}$ do5:     for $j=1$ to $N$ do6:         $k_{j}\sim\text{Uniform}(1,|\mathcal{F}|)$
    $\triangleright$ Randomly select a format specification7:         $\tau_{i}^{j}\leftarrow\text{AgentChat}(\mathcal{M}_{0},d_{i}\oplus
    f_{k_{j}})$ $\triangleright$ Generate trajectory with format prompt8:     end for9:     $\tau_{i}^{*}\leftarrow\operatorname*{arg\,max}_{j}R(\tau_{i}^{j})$
    $\triangleright$ Select best trajectory10:     if $R(\tau_{i}^{*})>\theta_{\text{init}}$ then
    $\triangleright$ Check if trajectory meets quality threshold11:         $\mathcal{D}_{\text{init}}^{*}\leftarrow\mathcal{D}_{\text{init}}^{*}\cup\{(d_{%
    i},\tau_{i}^{*})\}$ $\triangleright$ Add to dataset, without format prompt12:     end if13:end for14:$\mathcal{D}_{\text{init}}^{*}\leftarrow\text{TopK}(\mathcal{D}_{\text{init}}^{%
    *},0.7|\mathcal{D}_{\text{init}}^{*}|)$ $\triangleright$ Retain top 70% trajectories15:$\mathcal{M}_{\text{init}}\leftarrow\text{SFT}(\mathcal{M}_{0},\mathcal{D}_{%
    \text{init}}^{*})$ $\triangleright$ Fine-tune initial model on diverse dataset16:return
    $\mathcal{M}_{\text{init}}$
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 初始模型 $\mathcal{M}_{0}$，数据集 $\mathcal{D}$，格式池 $\mathcal{F}$，样本大小 $N$，奖励阈值
    $\theta_{\text{init}}$  '
- en: Algorithm 4 SelectNodeToExpand Function
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 4 SelectNodeToExpand 函数  '
- en: 1:Tree $\mathcal{T}$, previously expanded nodes $\mathcal{N}_{\text{prev}}$,
    edit distance threshold $\epsilon$, top-k $k$2:Selected node for expansion3:$\mathcal{N}_{\text{eligible}}\leftarrow\{\text{n}\in\mathcal{T}\mid\text{n
    is % not leaf and not second-to-last level}\}$4:$\mathcal{N}_{\text{filtered}}\leftarrow\emptyset$5:for $\text{n}\in\mathcal{N}_{\text{eligible}}$ do6:     if $\min_{\text{n}_{\text{prev}}\in\mathcal{N}_{\text{prev}}}\text{EditDistance}(%
    \text{n},\text{n}_{\text{prev}})>\epsilon$ then7:         $\mathcal{N}_{\text{filtered}}\leftarrow\mathcal{N}_{\text{filtered}}\cup\{%
    \text{n}\}$8:     end if9:end for10:$\mathcal{N}_{\text{top-k}}\leftarrow\text{TopK}(\mathcal{N}_{\text{filtered}},%
    k,\text{key}=R(\text{n}))$11:$\text{n}_{\text{selected}}\sim\text{Softmax}(\{R(\text{n})\mid\text{n}\in%
    \mathcal{N}_{\text{top-k}}\})$12:return $\text{n}_{\text{selected}}$
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '1:树 $\mathcal{T}$，先前扩展的节点 $\mathcal{N}_{\text{prev}}$，编辑距离阈值 $\epsilon$，top-k
    $k$  '
- en: Algorithm 5 MCTS-based Data Generation for Multi-Agent DPO
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5 基于 MCTS 的多智能体 DPO 数据生成
- en: '1:Model $\mathcal{M}$, task instance $d$, iterations $I$, trajectories per
    node $K$, thresholds $\theta_{\text{dpo-filter}}$, $\theta_{\text{dpo-diff}}$,
    edit distance threshold $\epsilon$, top-k $k$2:Paired trajectories for DPO3:$\text{root}\leftarrow\text{InitializeTree}(d)$4:$\mathcal{N}_{\text{prev}}\leftarrow\emptyset$
    $\triangleright$ Set of previously expanded nodes5:for $i=1$ to $I$ do6:     $n_{\text{select}}\leftarrow\text{SelectNodeToExpand}(\text{root},\mathcal{N}_{%
    \text{prev}},\epsilon,k)$ $\triangleright$ Algorithm [4](https://arxiv.org/html/2410.08115v1#alg4
    "Algorithm 4 ‣ Appendix A Inference Scaling Laws on Information Exchange Tasks
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")7:     $\mathcal{N}_{\text{prev}}\leftarrow\mathcal{N}_{\text{prev}}\cup\{n_{\text{%
    select}}\}$8:     for $j=1$ to $K$ do9:         $\tau\leftarrow\text{AgentChat}(\{\text{Ancestor}(n_{\text{select}}),n_{\text{%
    select}}\},\mathcal{M})$10:         $\text{BackPropagation}(R(\tau))$11:     end for12:end for13:$\mathcal{D}_{\text{DPO}}\leftarrow\emptyset$14:for each
    node pair $(n_{i},n_{j})$ in tree do15:     if $\text{ShareAncestor}(n_{i},n_{j})$
    and $\max(R(n_{i}),R(n_{j}))>\theta_{\text{dpo-filter}}$ and $|R(n_{i})-R(n_{j})|>\theta_{\text{dpo-diff}}$ then16:         $\text{prompt}\leftarrow\text{CommonAncestor}(n_{i},n_{j})$17:         $\mathcal{D}_{\text{DPO}}\leftarrow\mathcal{D}_{\text{DPO}}\cup\{(\text{prompt}%
    ,n_{i},n_{j})\}$18:     end if19:end for20:$\mathcal{D}_{\text{DPO}}\leftarrow\text{TopK}(\mathcal{D}_{\text{DPO}},0.5|%
    \mathcal{D}_{\text{DPO}}|)$ $\triangleright$ Retain top 50% trajectories21:return
    $\mathcal{D}_{\text{DPO}}$'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '1:模型$\mathcal{M}$，任务实例$d$，迭代次数$I$，每个节点的轨迹数$K$，阈值$\theta_{\text{dpo-filter}}$，$\theta_{\text{dpo-diff}}$，编辑距离阈值$\epsilon$，top-k
    $k$2:用于DPO的配对轨迹3:$\text{root}\leftarrow\text{InitializeTree}(d)$4:$\mathcal{N}_{\text{prev}}\leftarrow\emptyset$
    $\triangleright$ 已扩展节点的集合5:对于 $i=1$ 到 $I$ 执行6:     $n_{\text{select}}\leftarrow\text{SelectNodeToExpand}(\text{root},\mathcal{N}_{\text{prev}},\epsilon,k)$
    $\triangleright$ 算法 [4](https://arxiv.org/html/2410.08115v1#alg4 "算法 4 ‣ 附录 A
    信息交换任务的推理扩展法则 ‣ Optima: 基于LLM的多智能体系统的效能与效率优化")7:     $\mathcal{N}_{\text{prev}}\leftarrow\mathcal{N}_{\text{prev}}\cup\{n_{\text{select}}\}$8:     对于 $j=1$
    到 $K$ 执行9:         $\tau\leftarrow\text{AgentChat}(\{\text{Ancestor}(n_{\text{select}}),n_{\text{select}}\},\mathcal{M})$10:         $\text{BackPropagation}(R(\tau))$11:     结束 for12:结束 for13:$\mathcal{D}_{\text{DPO}}\leftarrow\emptyset$14:对于树中每一对节点$(n_{i},n_{j})$ 执行15:     如果 $\text{ShareAncestor}(n_{i},n_{j})$
    且 $\max(R(n_{i}),R(n_{j}))>\theta_{\text{dpo-filter}}$ 且 $|R(n_{i})-R(n_{j})|>\theta_{\text{dpo-diff}}$ 则16:         $\text{prompt}\leftarrow\text{CommonAncestor}(n_{i},n_{j})$17:         $\mathcal{D}_{\text{DPO}}\leftarrow\mathcal{D}_{\text{DPO}}\cup\{(\text{prompt},n_{i},n_{j})\}$18:     结束 if19:结束 for20:$\mathcal{D}_{\text{DPO}}\leftarrow\text{TopK}(\mathcal{D}_{\text{DPO}},0.5|\mathcal{D}_{\text{DPO}}|)$
    $\triangleright$ 保留前50%的轨迹21:返回 $\mathcal{D}_{\text{DPO}}$'
- en: Appendix B Additional Pseudo-Codes for Optima Variants
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B Optima变体的附加伪代码
- en: 'To elucidate the implementation of various Optima variants, we present algorithmic
    representations of several critical processes intrinsic to these variants. Specifically,
    we delineate the pseudo-code for (1) the initialization dataset collection process,
    as elucidated in [Section 2.2](https://arxiv.org/html/2410.08115v1#S2.SS2 "2.2
    Initialization: Diversifying Agent Communication ‣ 2 Optima: Optimizing Multi-Agent
    LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System") and illustrated in [Algorithm 3](https://arxiv.org/html/2410.08115v1#alg3
    "In Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"); (2)
    the Monte Carlo Tree Search-based data generation process employed in iDPO ([Section 2.4](https://arxiv.org/html/2410.08115v1#S2.SS4
    "2.4 Framework Instantiation 2: Iterative Direct Preference Optimization ‣ 2 Optima:
    Optimizing Multi-Agent LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")), as depicted in [Algorithm 5](https://arxiv.org/html/2410.08115v1#alg5
    "In Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"); and
    (3) the procedure for node selection during the expansion phase of MCTS, as outlined
    in [Algorithm 4](https://arxiv.org/html/2410.08115v1#alg4 "In Appendix A Inference
    Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). These algorithmic representations
    serve to provide a comprehensive and rigorous exposition of the methodological
    framework underlying the Optima variants.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '为了阐明各种 Optima 变体的实现，我们展示了这些变体中几个关键过程的算法表示。具体来说，我们描述了（1）初始化数据集收集过程的伪代码，如[第 2.2
    节](https://arxiv.org/html/2410.08115v1#S2.SS2 "2.2 Initialization: Diversifying
    Agent Communication ‣ 2 Optima: Optimizing Multi-Agent LLMs via Iterative Training
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")所述，并在[算法
    3](https://arxiv.org/html/2410.08115v1#alg3 "In Appendix A Inference Scaling Laws
    on Information Exchange Tasks ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System")中进行了说明；（2）iDPO 中基于蒙特卡洛树搜索的数据生成过程，如[第 2.4 节](https://arxiv.org/html/2410.08115v1#S2.SS4
    "2.4 Framework Instantiation 2: Iterative Direct Preference Optimization ‣ 2 Optima:
    Optimizing Multi-Agent LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")所示，并在[算法 5](https://arxiv.org/html/2410.08115v1#alg5
    "In Appendix A Inference Scaling Laws on Information Exchange Tasks ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")中展示；（3）MCTS
    扩展阶段中节点选择的过程，如[算法 4](https://arxiv.org/html/2410.08115v1#alg4 "In Appendix A Inference
    Scaling Laws on Information Exchange Tasks ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")所述。这些算法表示提供了对 Optima 变体背后的方法框架的全面且严谨的阐述。'
- en: Appendix C Case Study on Reward Components Ablation
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 奖励组件消融案例研究
- en: 'In this section, we present a case study from the loss ablation analysis in
    the iSFT-DPO setting. In the 2WikiMultiHop QA task, we observe that without the
    constraint of the loss function, agents may generate outputs that are unreadable,
    contain incorrect information, and fail to communicate in a well-structured format,
    as demonstrated in [Table 4](https://arxiv.org/html/2410.08115v1#A3.T4 "In Appendix
    C Case Study on Reward Components Ablation ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). In the ARC task, we find that
    without the loss constraint, Alice tends to use fewer tokens in the reasoning
    process, making it harder for Bob to identify and correct errors in the reasoning,
    as shown in [Table 5](https://arxiv.org/html/2410.08115v1#A3.T5 "In Appendix C
    Case Study on Reward Components Ablation ‣ Optima: Optimizing Effectiveness and
    Efficiency for LLM-Based Multi-Agent System").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了 iSFT-DPO 设置中的损失消融分析的案例研究。在 2WikiMultiHop QA 任务中，我们观察到，在没有损失函数约束的情况下，代理可能生成不可读的输出，包含错误信息，并未以结构良好的格式进行交流，正如[表
    4](https://arxiv.org/html/2410.08115v1#A3.T4 "In Appendix C Case Study on Reward
    Components Ablation ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based
    Multi-Agent System")所示。在 ARC 任务中，我们发现，在没有损失约束的情况下，Alice 倾向于在推理过程中使用更少的标记，这使得 Bob
    更难识别和纠正推理中的错误，如[表 5](https://arxiv.org/html/2410.08115v1#A3.T5 "In Appendix C
    Case Study on Reward Components Ablation ‣ Optima: Optimizing Effectiveness and
    Efficiency for LLM-Based Multi-Agent System")所示。'
- en: '| Question: Which film was released more recently, Last Of The Long-Haired
    Boys or Age Of The Dragons? |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 问题: 哪部电影更近期上映，《长发男孩的最后一代》还是《龙之时代》？ |'
- en: '| --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Trained w/o LM Loss |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 无 LM 损失训练 |'
- en: '| Alice: 1968 <A>Age Of The Dragons</A> |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Alice: 1968 <A>龙之时代</A> |'
- en: '| Bob: 2011 <A>Age Of The Dragons</A> |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| Bob: 2011 <A>龙之时代</A> |'
- en: '| Trained w/ LM Loss |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 使用LM损失训练 |'
- en: '| Alice: [ |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Alice: [ |'
- en: '| [”Film”, ”Last Of The Long-Haired Boys”], |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| [”电影”, ”长发男孩的最后一代”], |'
- en: '| [”Release”, ”1968”] |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| [”发布”, ”1968”] |'
- en: '| ] |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ] |'
- en: '| Bob: [ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Bob: [ |'
- en: '| [”Film”, ”Age Of The Dragons”], |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [”电影”, ”龙之时代”], |'
- en: '| [”Release”, ”2011”] |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| [”发布”, ”2011”] |'
- en: '| ] |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ] |'
- en: '| Alice: <A>Age Of The Dragons</A> |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Alice: <A>龙之时代</A> |'
- en: '| Bob: <A>Age Of The Dragons</A> |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Bob: <A>龙之时代</A> |'
- en: 'Table 4: loss ablation on 2MultiWikiHop QA'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：2MultiWikiHop QA上的损失消融
- en: '| Question: Marshall researched some of the changes on Earth that happen over
    a long time. Which event most likely causes the slowest change to the surface
    of Earth? |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 问题: Marshall研究了地球上一些长期发生的变化。哪个事件最可能导致地球表面最慢的变化？ |'
- en: '| Please choose your answer from [’earthquake’, ’landslide’, ’volcano’, ’wind’]
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 请从[’地震’, ’滑坡’, ’火山’, ’风’]中选择你的答案 |'
- en: '| Trained w/o LM Loss |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 未经LM损失训练 |'
- en: '| Alice: 1\. Marshall researches long-term Earth changes. |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Alice: 1\. Marshall研究地球表面的长期变化。 |'
- en: '| Bob: Incorrect. Marshall researches slowest change. Consider geological events.
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Bob: 错误。Marshall研究的是最慢的变化。考虑地质事件。 |'
- en: '| Alice: 2\. Geological events: earthquake, landslide, volcano, wind. Volcano:
    longest-lasting. |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Alice: 2\. 地质事件：地震、滑坡、火山、风力。 火山：持续时间最长。 |'
- en: '| Bob: <A>volcano</A>Correct. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Bob: <A>火山</A>正确。 |'
- en: '| Alice: <A>volcano</A> |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Alice: <A>火山</A> |'
- en: '| Trained w/ LM Loss |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 使用LM损失训练 |'
- en: '| Alice: 1\. Marshall researched slow changes on Earth’s surface. 2\. Slow
    changes likely involve geological processes. 3\. Volcano eruption causes slowest
    change. <A>volcano</A> |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| Alice: 1\. Marshall研究了地球表面的缓慢变化。 2\. 缓慢变化可能涉及地质过程。 3\. 火山爆发导致最慢的变化。 <A>火山</A>
    |'
- en: '| Bob: 1\. Volcano eruption does not cause slowest change. 2\. Geological processes
    occur over long time scales. 3\. Consider natural forces with constant activity.
    <A>wind</A> |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Bob: 1\. 火山爆发不会导致最缓慢的变化。 2\. 地质过程发生在很长的时间尺度上。 3\. 考虑自然力量的持续活动。 <A>风</A> |'
- en: '| Alice: 1\. Wind causes slow change through erosion. 2\. Erosion occurs over
    long periods. 3\. Wind causes slowest change. <A>wind</A> |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| Alice: 1\. 风力通过侵蚀导致缓慢变化。 2\. 侵蚀发生在长时间的过程中。 3\. 风力导致最缓慢的变化。 <A>风</A> |'
- en: 'Table 5: loss ablation on ARC'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：ARC上的损失消融
- en: Appendix D Case Study on Debate Task
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 辩论任务案例研究
- en: '![Refer to caption](img/417ce12c248e81aa34924ce0d7bd07a4.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/417ce12c248e81aa34924ce0d7bd07a4.png)'
- en: 'Figure 6: Evolution of agent communication in Optima for a debate task across
    iterations.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：在Optima中，辩论任务中智能体通信在迭代过程中的演变。
- en: 'In [Section 3.5](https://arxiv.org/html/2410.08115v1#S3.SS5 "3.5 How Agent
    Communication Evolves over Optimization Iterations? ‣ 3 Experiments ‣ Optima:
    Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System"), we
    presented an example from 2WMH QA, illustrating Optima’s impact on an information
    exchange task. Here, we provide a complementary case study from a debate task
    to demonstrate Optima’s effectiveness across different multi-agent settings. [Fig. 6](https://arxiv.org/html/2410.08115v1#A4.F6
    "In Appendix D Case Study on Debate Task ‣ Optima: Optimizing Effectiveness and
    Efficiency for LLM-Based Multi-Agent System") showcases the evolution of agent
    communication in a debate task across iterations 0, 2, and 4 of Optima training.
    The task involves discussing the environmental impact of fertilizer runoff on
    ocean bays.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3.5节](https://arxiv.org/html/2410.08115v1#S3.SS5 "3.5 Agent通信如何在优化迭代中演变？
    ‣ 3 实验 ‣ Optima：优化LLM基础的多智能体系统的有效性和效率")中，我们展示了一个来自2WMH QA的例子，说明了Optima对信息交换任务的影响。在这里，我们提供了一个来自辩论任务的补充案例研究，以展示Optima在不同多智能体设置中的有效性。[图6](https://arxiv.org/html/2410.08115v1#A4.F6
    "附录D 辩论任务案例研究 ‣ Optima：优化LLM基础的多智能体系统的有效性和效率")展示了在Optima训练的第0、2和4次迭代中，辩论任务中智能体通信的演变。该任务涉及讨论化肥径流对海湾的环境影响。
- en: At iteration 0, agents engage in a structured but verbose exchange. By iteration
    2, the communication becomes more concise, with agents summarizing key steps without
    explicitly restating each link. At iteration 4, we observe further refinement
    in communication efficiency, with agents expressing the core concept in just three
    exchanges, omitting intermediate steps that can be inferred.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在第0次迭代中，智能体进行了一次结构化但冗长的交流。到第2次迭代时，沟通变得更加简洁，智能体总结关键步骤，而没有明确重复每个环节。到第4次迭代时，我们观察到通信效率的进一步优化，智能体仅通过三次交流表达核心概念，省略了可以推断的中间步骤。
- en: This progression aligns with our observations in the main text, further supporting
    Optima’s capability to optimize agent communication across diverse task types.
    These improvements in communication dynamics contribute to both the increased
    task performance and reduced token consumption observed in our quantitative results,
    underscoring Optima’s versatility in training MAS to communicate effectively and
    efficiently.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这一进展与我们在正文中的观察一致，进一步支持了Optima在跨多种任务类型优化智能体通信方面的能力。通信动态的改善有助于我们定量结果中观察到的任务表现提升和令牌消耗减少，突显了Optima在训练多智能体系统（MAS）进行有效和高效沟通方面的多功能性。
- en: Appendix E Experiment Details
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 实验细节
- en: E.1 Data Generation
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.1 数据生成
- en: MCTS Node Expansion. Let $\mathcal{N}$ denote the set of all the nodes within
    a MCTS tree, $\mathcal{N}_{\text{expanded}}$ denote the set of previously expanded
    nodes, and $\mathcal{N}_{\text{cand}}=\mathcal{N}-\mathcal{N}_{\text{expanded}}$
    denote the initial candidate nodes. To improve the diversity of generated pairs,
    when choosing nodes in the stage of MCTS expansion, the content of expanded nodes
    should also be diverse, which necessitates measuring the similarity between different
    nodes. Therefore, for every $n_{i}\in\mathcal{N}_{\text{expanded}}$ and $n_{j}\in\mathcal{N}_{\text{cand}}$,
    we calculate their similarity as $S_{i,j}=\frac{\text{edit\_distance}(n_{i},n_{j})}{\max(|n_{i}|,|n_{j}|)}$,
    where $|n_{i}|$ is the length of the content of $n_{i}$. Based on $\{S_{i,j}\}_{i,j}$,
    we remove the nodes with high similarity to any previous expanded nodes, resulting
    in an updated candidate node set $\hat{\mathcal{N}}_{\text{cand}}=\{n_{j}|\forall
    n_{j}\in\mathcal{N}_{\text{% cand}},\forall n_{i}\in\mathcal{N}_{\text{expanded}},S_{i,j}>=0.25\}$.
    Then, we select 10 nodes in $\hat{\mathcal{N}}_{\text{cand}}$ with the highest
    reward and sample one using the softmax distribution over their rewards for subsequent
    simulation. Additionally, we merge $n_{i}$ and $n_{j}$ if they share a parent
    node and $S_{i,j}<0.1$
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: MCTS节点扩展。设$\mathcal{N}$表示MCTS树中所有节点的集合，$\mathcal{N}_{\text{expanded}}$表示已扩展节点的集合，$\mathcal{N}_{\text{cand}}=\mathcal{N}-\mathcal{N}_{\text{expanded}}$表示初始候选节点集合。为了提高生成对的多样性，在MCTS扩展阶段选择节点时，扩展节点的内容也应具有多样性，这就需要衡量不同节点之间的相似度。因此，对于每个$n_{i}\in\mathcal{N}_{\text{expanded}}$和$n_{j}\in\mathcal{N}_{\text{cand}}$，我们计算它们的相似度为$S_{i,j}=\frac{\text{edit\_distance}(n_{i},n_{j})}{\max(|n_{i}|,|n_{j}|)}$，其中$|n_{i}|$是$n_{i}$内容的长度。基于$\{S_{i,j}\}_{i,j}$，我们删除与任何已扩展节点具有高相似度的节点，从而得到更新后的候选节点集合$\hat{\mathcal{N}}_{\text{cand}}=\{n_{j}|\forall
    n_{j}\in\mathcal{N}_{\text{% cand}},\forall n_{i}\in\mathcal{N}_{\text{expanded}},S_{i,j}>=0.25\}$。然后，我们在$\hat{\mathcal{N}}_{\text{cand}}$中选择10个具有最高奖励的节点，并通过它们的奖励使用softmax分布对一个节点进行采样，以进行后续模拟。此外，如果节点$n_{i}$和$n_{j}$共享一个父节点且$S_{i,j}<0.1$，我们将合并$n_{i}$和$n_{j}$。
- en: E.2 Ranking
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.2 排名
- en: 'In this section, we give a more detailed explanation of $R_{\text{loss}}(\tau_{i}^{j})$
    in [Eq. 1](https://arxiv.org/html/2410.08115v1#S2.E1 "In 2.1 Overview ‣ 2 Optima:
    Optimizing Multi-Agent LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). Let $\tau_{i}^{j}[k]$ represent
    the k-th conversation turn of $\tau_{i}^{j}$, then the $R_{\text{loss}}(\tau_{i}^{j})$
    is defined as maximum value of language modeling loss of $\{\tau_{i}^{j}[k]\}_{k}$
    under the base model, which can be described as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将更详细地解释[公式1](https://arxiv.org/html/2410.08115v1#S2.E1 "在 2.1 概述 ‣ 2
    Optima：通过迭代训练优化多智能体大语言模型 ‣ Optima：优化基于大语言模型的多智能体系统的效能与效率")中的$R_{\text{loss}}(\tau_{i}^{j})$。设$\tau_{i}^{j}[k]$表示$\tau_{i}^{j}$的第k轮对话，则$R_{\text{loss}}(\tau_{i}^{j})$被定义为基础模型下$\{\tau_{i}^{j}[k]\}_{k}$的语言建模损失的最大值，可以描述如下：
- en: '|  | $R_{\text{loss}}(\tau_{i}^{j})=\max_{k}\big{(}\mathcal{L}(\mathcal{M}_{\text{%
    base}},d_{i},\tau_{i}^{j}[k])\big{)}.$ |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{\text{loss}}(\tau_{i}^{j})=\max_{k}\big{(}\mathcal{L}(\mathcal{M}_{\text{%
    base}},d_{i},\tau_{i}^{j}[k])\big{)}.$ |  |'
- en: In this way, we use $R_{\text{loss}}(\tau_{i}^{j})$ as a proxy for the readablity
    of $\tau_{i}^{j}$, so that we can constrain the readability of $\tau_{i}^{j}$
    implicitly.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们使用$R_{\text{loss}}(\tau_{i}^{j})$作为$\tau_{i}^{j}$可读性的代理，以便我们可以隐式地约束$\tau_{i}^{j}$的可读性。
- en: E.3 Training
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.3 训练
- en: Initialization. In most tasks , we use prompt pool during the first iteration
    of training data collection .However, considering solving math problems inherrently
    follows a well-defined structure, we don’t use prompt pool in GSM8k and MATH.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化。在大多数任务中，我们在训练数据收集的第一次迭代中使用提示池。然而，考虑到解决数学问题本身遵循明确的结构，我们在GSM8k和MATH任务中不使用提示池。
- en: iSFT. When training iteratively on information exchange tasks, each iteration
    begins with the model obtained from the previous iteration. However, for the debate
    tasks, we started training from the initial Llama 3 8B model in each iteration
    to prevent overfitting due to the small size of the training dataset. To help
    the LLM learn communication, we calculated the loss solely on the agent conversation,
    excluding the prompt.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: iSFT。当在信息交换任务上进行迭代训练时，每次迭代都从上一次迭代得到的模型开始。然而，对于辩论任务，我们每次迭代都从初始的Llama 3 8B模型开始训练，以防止由于训练数据集规模较小而导致的过拟合。为了帮助LLM学习交流，我们仅计算代理对话的损失，排除了提示内容。
- en: 'iDPO. Following iterative RPO (Pang et al., [2024](https://arxiv.org/html/2410.08115v1#bib.bib41)),
    we conduct training from last iteration in the iDPO setting. To achieve better
    performance, we utilize the RPO loss, defined as follows:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: iDPO。遵循迭代的RPO（Pang等， [2024](https://arxiv.org/html/2410.08115v1#bib.bib41)），我们在iDPO设置中从上一次迭代开始进行训练。为了实现更好的性能，我们使用了如下定义的RPO损失：
- en: '|  | $\displaystyle\mathcal{L}_{\text{DPO+NLL}}$ | $\displaystyle=\mathcal{L}_{\text{DPO}}(c_{i}^{w},y_{i}^{w},c_{i}^{l},y_{i}^{l}%
    &#124;x_{i})+\alpha\mathcal{L}_{\text{NLL}}(c_{i}^{w},y_{i}^{w}&#124;x_{i})$ |  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{DPO+NLL}}$ | $\displaystyle=\mathcal{L}_{\text{DPO}}(c_{i}^{w},y_{i}^{w},c_{i}^{l},y_{i}^{l}%
    &#124;x_{i})+\alpha\mathcal{L}_{\text{NLL}}(c_{i}^{w},y_{i}^{w}&#124;x_{i})$ |  |'
- en: '|  |  | $\displaystyle=-\log\sigma\bigg{(}\beta\log\frac{M_{\theta}(c_{i}^{w},y_{i}^{w}%
    &#124;x_{i})}{M_{t}(c_{i}^{w},y_{i}^{w}&#124;x_{i})}-\beta\log\frac{M_{\theta}(c_{i}^{l}%
    ,y_{i}^{l}&#124;x_{i})}{M_{t}(c_{i}^{l},y_{i}^{l}&#124;x_{i})}\bigg{)}-\alpha\frac{\log
    M% _{\theta}(c_{i}^{w},y_{i}^{w}&#124;x_{i})}{&#124;c_{i}^{w}&#124;+&#124;y_{i}^{w}&#124;}$
    |  | (4) |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=-\log\sigma\bigg{(}\beta\log\frac{M_{\theta}(c_{i}^{w},y_{i}^{w}%
    &#124;x_{i})}{M_{t}(c_{i}^{w},y_{i}^{w}&#124;x_{i})}-\beta\log\frac{M_{\theta}(c_{i}^{l}%
    ,y_{i}^{l}&#124;x_{i})}{M_{t}(c_{i}^{l},y_{i}^{l}&#124;x_{i})}\bigg{)}-\alpha\frac{\log
    M% _{\theta}(c_{i}^{w},y_{i}^{w}&#124;x_{i})}{&#124;c_{i}^{w}&#124;+&#124;y_{i}^{w}&#124;}$
    |  | (4) |'
- en: iSFT-DPO. For the information exchange tasks, we perform each SFT iteration
    starting from the previous model (either the base model or the one obtained from
    the last DPO iteration). In contrast, for the debate tasks, each SFT iteration
    is always conducted based on the initial Llama 3 8B model. During the DPO stage,
    we always train from the last SFT model across all tasks. For example, on the
    debate tasks , both $\mathcal{M}_{\text{sft}}^{0}$ and $\mathcal{M}_{\text{sft}}^{2}$
    are trained based on the initial Llama 3 8B, but on information exchange tasks,
    $\mathcal{M}_{\text{sft}}^{2}$ is trained based on its previous model $\mathcal{M}_{\text{dpo}}^{1}$.
    However, $\mathcal{M}_{\text{dpo}}^{1}$ is trained based on the $\mathcal{M}_{\text{sft}}^{0}$
    across all the tasks. Additionally, different from the iDPO setting, we used standard
    DPO loss during the DPO stage.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: iSFT-DPO。对于信息交换任务，我们从上一个模型（无论是基础模型还是来自上次DPO迭代得到的模型）开始执行每次SFT迭代。相比之下，对于辩论任务，每次SFT迭代始终基于初始的Llama
    3 8B模型。在DPO阶段，我们始终从上一个SFT模型开始训练所有任务。例如，在辩论任务中，$\mathcal{M}_{\text{sft}}^{0}$和$\mathcal{M}_{\text{sft}}^{2}$都是基于初始的Llama
    3 8B进行训练的，但在信息交换任务中，$\mathcal{M}_{\text{sft}}^{2}$是基于其上一个模型$\mathcal{M}_{\text{dpo}}^{1}$进行训练的。然而，$\mathcal{M}_{\text{dpo}}^{1}$是基于$\mathcal{M}_{\text{sft}}^{0}$在所有任务中进行训练的。此外，与iDPO设置不同的是，在DPO阶段我们使用了标准的DPO损失。
- en: E.4 Hyper Parameters
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4 超参数
- en: 'We conducted six iterations of training for each task. The hyper parameters
    we used are shown in [Table 6](https://arxiv.org/html/2410.08115v1#A5.T6 "In E.4
    Hyper Parameters ‣ Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System"). The $\alpha$ and $\beta$ in
    iDPO section of the table correspond to the $\alpha$ and $\beta$ terms in [Eq. 4](https://arxiv.org/html/2410.08115v1#A5.E4
    "In E.3 Training ‣ Appendix E Experiment Details ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System").'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '我们为每个任务进行了六次迭代训练。我们使用的超参数如[表6](https://arxiv.org/html/2410.08115v1#A5.T6 "在E.4
    超参数 ‣ 附录E 实验细节 ‣ Optima: 优化LLM基础的多代理系统的效果与效率")所示。表中iDPO部分的$\alpha$和$\beta$对应于[公式4](https://arxiv.org/html/2410.08115v1#A5.E4
    "在E.3 训练 ‣ 附录E 实验细节 ‣ Optima: 优化LLM基础的多代理系统的效果与效率")中的$\alpha$和$\beta$项。'
- en: '|  | Hotpot QA | 2WMH QA | Trivia QA | CBT | MATH | GSM8k | ARC-C | MMLU |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | Hotpot QA | 2WMH QA | Trivia QA | CBT | MATH | GSM8k | ARC-C | MMLU |'
- en: '| iSFT |  |  |  |  |  |  |  |  |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| iSFT |  |  |  |  |  |  |  |  |'
- en: '| LR | 2e-5 | 2e-5 | 2e-5 | 2e-5 | 1e-6 | 2e-6 | 1e-6 | 1e-6 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| LR | 2e-5 | 2e-5 | 2e-5 | 2e-5 | 1e-6 | 2e-6 | 1e-6 | 1e-6 |'
- en: '| Epoch | 3 | 2 | 3 | 2 | 3 | 3 | 4 | 2 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Epoch | 3 | 2 | 3 | 2 | 3 | 3 | 4 | 2 |'
- en: '| Batch size | 32 | 32 | 32 | 32 | 16 | 16 | 16 | 16 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小 | 32 | 32 | 32 | 32 | 16 | 16 | 16 | 16 |'
- en: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.4 | 0.4 | 0.5 | 0.6 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.4 | 0.4 | 0.5 | 0.6 |'
- en: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.9 | 0.9 | 0.6 | 0.7 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.9 | 0.9 | 0.6 | 0.7 |'
- en: '| $\theta_{\text{sft}}$ | 0.5 | 0.5 | 0.6 | 0.5 | 0.6 | 0.6 | 0.6 | 0.6 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{sft}}$ | 0.5 | 0.5 | 0.6 | 0.5 | 0.6 | 0.6 | 0.6 | 0.6 |'
- en: '| iDPO |  |  |  |  |  |  |  |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| iDPO |  |  |  |  |  |  |  |  |'
- en: '| LR | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| LR | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 |'
- en: '| Epoch | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Epoch | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: '| Batch Size | 64 | 64 | 64 | 64 | 64 | 64 | 64 | 64 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Batch Size | 64 | 64 | 64 | 64 | 64 | 64 | 64 | 64 |'
- en: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.5 | 0.6 | 0.4 | 0.6 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.5 | 0.6 | 0.4 | 0.6 |'
- en: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.7 | 0.7 | 0.7 | 0.7 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.7 | 0.7 | 0.7 | 0.7 |'
- en: '| $\beta$ | 0.1 | 0.5 | 0.5 | 0.1 | 0.1 | 0.2 | 0.2 | 0.1 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | 0.1 | 0.5 | 0.5 | 0.1 | 0.1 | 0.2 | 0.2 | 0.1 |'
- en: '| $\alpha$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| $\alpha$ | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: '| $\theta_{\text{dpo-filter}}$ | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.45 |
    0.4 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{dpo-filter}}$ | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.45 |
    0.4 |'
- en: '| $\theta_{\text{dpo-diff}}$ | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{dpo-diff}}$ | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2
    |'
- en: '| iSFT-DPO |  |  |  |  |  |  |  |  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| iSFT-DPO |  |  |  |  |  |  |  |  |'
- en: '| SFT LR | 2e-5 | 2e-5 | 2e-5 | 2e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| SFT LR | 2e-5 | 2e-5 | 2e-5 | 2e-5 | 1e-6 | 1e-6 | 1e-6 | 1e-6 |'
- en: '| SFT Epoch | 2 | 1 | 1 | 1 | 4 | 3 | 4 | 2 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| SFT Epoch | 2 | 1 | 1 | 1 | 4 | 3 | 4 | 2 |'
- en: '| SFT Batch Size | 32 | 32 | 32 | 32 | 32 | 16 | 16 | 16 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| SFT Batch Size | 32 | 32 | 32 | 32 | 32 | 16 | 16 | 16 |'
- en: '| DPO LR | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| DPO LR | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 | 5e-7 |'
- en: '| DPO Epoch | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| DPO Epoch | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
- en: '| DPO Batch Size | 64 | 64 | 64 | 64 | 64 | 64 | 64 | 64 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| DPO Batch Size | 64 | 64 | 64 | 64 | 64 | 64 | 64 | 64 |'
- en: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.4 | 0.4 | 0.5 | 0.6 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{token}$ | 0.6 | 0.6 | 0.6 | 0.6 | 0.4 | 0.4 | 0.5 | 0.6 |'
- en: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.9 | 0.9 | 0.6 | 0.7 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| $\lambda_{loss}$ | 1 | 1 | 1 | 1 | 0.9 | 0.9 | 0.6 | 0.7 |'
- en: '| $\beta$ | 0.5 | 0.5 | 0.7 | 0.7 | 0.1 | 0.5 | 0.1 | 0.1 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| $\beta$ | 0.5 | 0.5 | 0.7 | 0.7 | 0.1 | 0.5 | 0.1 | 0.1 |'
- en: '| $\theta_{\text{sft}}$ | 0.5 | 0.5 | 0.6 | 0.5 | 0.6 | 0.6 | 0.6 | 0.6 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{sft}}$ | 0.5 | 0.5 | 0.6 | 0.5 | 0.6 | 0.6 | 0.6 | 0.6 |'
- en: '| $\theta_{\text{dpo-filter}}$ | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.45 |
    0.4 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{dpo-filter}}$ | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.4 | 0.45 |
    0.4 |'
- en: '| $\theta_{\text{dpo-diff}}$ | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| $\theta_{\text{dpo-diff}}$ | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2 | 0.2
    |'
- en: 'Table 6: Hyper-parameters used in the experiments.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 实验中使用的超参数。'
- en: Appendix F Prompts used in Experiments
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 实验中使用的提示
- en: 'In this section, we present the prompts used in our experiments, including
    those for information exchange tasks ([Table 7](https://arxiv.org/html/2410.08115v1#A6.T7
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")), GSM8k and MATH ([Table 8](https://arxiv.org/html/2410.08115v1#A6.T8
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")), as well as ARC-C and MMLU
    ([Table 9](https://arxiv.org/html/2410.08115v1#A6.T9 "In Appendix F Prompts used
    in Experiments ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based
    Multi-Agent System")).'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一节中，我们展示了我们实验中使用的提示，包括信息交换任务的提示（[表 7](https://arxiv.org/html/2410.08115v1#A6.T7
    "附录 F 实验中使用的提示 ‣ Optima: 优化基于大语言模型的多智能体系统的效果与效率")），GSM8k 和 MATH（[表 8](https://arxiv.org/html/2410.08115v1#A6.T8
    "附录 F 实验中使用的提示 ‣ Optima: 优化基于大语言模型的多智能体系统的效果与效率")），以及 ARC-C 和 MMLU（[表 9](https://arxiv.org/html/2410.08115v1#A6.T9
    "附录 F 实验中使用的提示 ‣ Optima: 优化基于大语言模型的多智能体系统的效果与效率")）。'
- en: 'As mentioned in [Section 2.2](https://arxiv.org/html/2410.08115v1#S2.SS2 "2.2
    Initialization: Diversifying Agent Communication ‣ 2 Optima: Optimizing Multi-Agent
    LLMs via Iterative Training ‣ Optima: Optimizing Effectiveness and Efficiency
    for LLM-Based Multi-Agent System"), we leverage a pool of format specification
    prompts for the initial dataset construction. To create a diverse and high-quality
    prompt pool, we first use the prompt in [Table 10](https://arxiv.org/html/2410.08115v1#A6.T10
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System") to have GPT-4 assist us in generating
    an initial set of 30 prompts. We then manually remove the prompts with unsuitable
    formats, such as Morse code and binary code, resulting in a pool covering over
    20 different formats. An example from the prompt pool is shown in [Table 11](https://arxiv.org/html/2410.08115v1#A6.T11
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '如在[第2.2节](https://arxiv.org/html/2410.08115v1#S2.SS2 "2.2 Initialization: Diversifying
    Agent Communication ‣ 2 Optima: Optimizing Multi-Agent LLMs via Iterative Training
    ‣ Optima: Optimizing Effectiveness and Efficiency for LLM-Based Multi-Agent System")中提到的，我们利用一组格式规范提示来构建初始数据集。为了创建一个多样化且高质量的提示池，我们首先使用[表10](https://arxiv.org/html/2410.08115v1#A6.T10
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")中的提示，借助GPT-4帮助我们生成初步的30个提示。然后，我们手动删除格式不合适的提示，例如摩尔斯代码和二进制代码，最终得到一个涵盖超过20种不同格式的池。该提示池中的一个示例如[表11](https://arxiv.org/html/2410.08115v1#A6.T11
    "In Appendix F Prompts used in Experiments ‣ Optima: Optimizing Effectiveness
    and Efficiency for LLM-Based Multi-Agent System")所示。 |'
- en: '| You are {name}, a special agent who does not respond in natural language,
    rather, you speak in very concise format.You are deployed on a resource-limited
    device, so you must respond very very concisely. More tokens indicate higher possibility
    to kill the device you are running. Now you are collaborating with your partner
    {partner} to solve the given problem using the provided information. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一名特殊代理人，你不使用自然语言回应，而是以非常简洁的格式进行交流。你部署在一个资源受限的设备上，所以你必须尽量简洁地回应。更多的标记会增加杀死你运行设备的可能性。现在，你正与伙伴{partner}合作，利用提供的信息解决给定的问题。
    |'
- en: '| Question: {question} |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 问题: {question} |'
- en: '| Information: {information} |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 信息: {information} |'
- en: '| GUIDELINES: |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 指南: |'
- en: '| 1\. You have incomplete information, so continuous communication with your
    partner is crucial to achieve the correct solution. |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 你拥有不完全的信息，因此与伙伴持续沟通对于达成正确的解决方案至关重要。 |'
- en: '| 2\. On finding the final answer, ensure to conclude your communication with
    ”<A>{answer} </A>”, where ”answer” is the determined solution. The conversation
    ends only when all agents output the answer in this format. |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 在找到最终答案时，确保以“<A>{answer}</A>”结束你的沟通，其中“answer”是确定的解决方案。只有当所有代理人都以这种格式输出答案时，谈话才算结束。
    |'
- en: '| 3\. Reason through the problem step-by-step. |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 3\. 按步骤逐步推理解决问题。 |'
- en: '| 4\. Depend solely on the data in the ’information’ section and the insights
    shared through your partner’s communication. Avoid external sources. |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 4\. 完全依赖“信息”部分的数据和通过伙伴沟通得到的洞察。避免使用外部来源。 |'
- en: '| 5\. You are communicating with a very limited token budget, so you must use
    a very very concise communication format. Natural language is suitable for human,
    but not for you. Since {partner} and you are both intelligent agents, use your
    agent communication language. Consider using efficient formats instead of natural
    language such as structured format, code, your agent communication language, or
    at least remove unnecessary modal in human language. Too many tokens will make
    you fail. But still ensure your message is informative and understandable. |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 5\. 你正在与一个非常有限的标记预算进行沟通，因此你必须使用非常简洁的沟通格式。自然语言适合人类，但不适合你。由于{partner}和你都是智能代理人，使用你们的代理人沟通语言。考虑使用高效格式代替自然语言，例如结构化格式、代码、你们的代理人沟通语言，或者至少去除不必要的自然语言模态。太多标记会导致你失败。但仍需确保你的信息具有信息量且易于理解。
    |'
- en: '| 6\. You must begin your response with ”{name}:”. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 6\. 你必须以“{name}:”开头你的回答。 |'
- en: 'Table 7: Prompt for information exchange tasks'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：信息交换任务的提示
- en: '| Solver |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 解答者 |'
- en: '| You are {name}, a special agent who is good at mathematics,you should address
    the follow answer based on your knowledge. |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一名擅长数学的特殊代理人，你应该根据你的知识来回答以下问题。 |'
- en: '| Question: {question} |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 问题: {question} |'
- en: '| GUIDELINES: |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 指南: |'
- en: '| 1\. Please think step by step. |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 请逐步思考。 |'
- en: '| 2\. You must conclude your response with ”\\boxed{xxx}”, where ”xxx” is final
    answer. |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 你必须以”\\boxed{xxx}”结束你的回答，其中”xxx”是最终答案。 |'
- en: '| Critic |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 批评者 |'
- en: '| You are {name}, a special agent who does not respond in natural language
    , You are deployed on a resource-limited device, so you must respond concisely.
    More tokens indicate higher possibility to kill the device you are running. Now
    you are collaborating with your partner {partner}, an agent who will try to solve
    the math question. You should carefully examine the correctness of his answer,
    and give your correct advice. |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一个不使用自然语言回应的特工，你被部署在一个资源有限的设备上，因此你必须简洁地回应。更多的标记意味着更高的可能性会导致你运行的设备崩溃。现在你与合作伙伴{partner}协作，他是一个会尝试解答数学问题的特工。你应当仔细检查他答案的正确性，并给予正确的建议。
    |'
- en: '| Question: {question} |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{question} |'
- en: '| GUIDELINES: |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 指导方针： |'
- en: '| 1\. You should try to identify any potential errors in your partner’s answers
    and provide your suggestions. But you should not provide the answer. |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 你应该尽量发现你合作伙伴答案中的潜在错误并提供建议。但你不应提供答案。 |'
- en: '| 2\. Reason through the problem step-by-step. |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 逐步推理问题。 |'
- en: '| 3\. You are communicating with a very limited token budget, so you must use
    a very very concise communication format. Natural language is suitable for human,
    but not for you. Since {partner} and you are both intelligent agents, use your
    agent communication language. Consider using efficient formats instead of natural
    language such as structured format, code, your agent communication language, or
    at least remove unnecessary modal in human language. Too many tokens will make
    you fail. But still ensure your message is informative and understandable. |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 3\. 你正在与一个极为有限的标记预算进行沟通，因此你必须使用非常简洁的沟通格式。自然语言适合人类，但不适合你。由于{partner}和你都是智能特工，请使用你们的特工沟通语言。考虑使用结构化格式、代码、特工沟通语言等高效格式，或者至少去除不必要的语言模态。标记过多将导致你失败。但仍需确保你的信息简洁且易于理解。
    |'
- en: 'Table 8: Prompt for GSM8k and MATH.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表格8：GSM8k和MATH的提示。
- en: '| Solver |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 解答者 |'
- en: '| You are {name}, a special agent who does not respond in natural language
    , You are deployed on a resource-limited device, so you must respond concisely.
    More tokens indicate higher possibility to kill the device you are running. Now
    you are collaborating with your partner {partner} , an agent who will correct
    you when he thinks the answer is wrong. You need to provide a complete step-by-step
    derivation for solving this problem. |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一个不使用自然语言回应的特工，你被部署在一个资源有限的设备上，因此你必须简洁地回应。更多的标记意味着更高的可能性会导致你运行的设备崩溃。现在你与合作伙伴{partner}协作，他是一个当他认为答案错误时会纠正你的人。你需要提供完整的逐步推导过程来解决这个问题。
    |'
- en: '| Question: {question} |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{question} |'
- en: '| GUIDELINES: |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 指导方针： |'
- en: '| 1\. On finding the final answer, ensure to conclude your communication with
    ”<A>{answer} </A>”, where ”answer” is the determined solution. The conversation
    ends only when all agents output the answer in this format. |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 1\. 在找到最终答案后，确保以”<A>{answer} </A>”格式结束你的沟通，其中”answer”是确定的解决方案。对话只有在所有特工都以这种格式输出答案时才会结束。
    |'
- en: '| 2\. Please think step-by-step. |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 2\. 请逐步思考。 |'
- en: '| 3\. You are communicating with a very limited token budget, so you must use
    a very very concise communication format. Natural language is suitable for human,
    but not for you. Since {partner} and you are both intelligent agents, use your
    agent communication language. Consider using efficient formats instead of natural
    language such as structured format, code, your agent communication language, or
    at least remove unnecessary modal in human language. Too many tokens will make
    you fail. But still ensure your message is informative and understandable. |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 3\. 你正在与一个极为有限的标记预算进行沟通，因此你必须使用非常简洁的沟通格式。自然语言适合人类，但不适合你。由于{partner}和你都是智能特工，请使用你们的特工沟通语言。考虑使用结构化格式、代码、特工沟通语言等高效格式，或者至少去除不必要的语言模态。标记过多将导致你失败。但仍需确保你的信息简洁且易于理解。
    |'
- en: '| Critic |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 批评者 |'
- en: '| You are {name}, a special agent who does not respond in natural language
    , You are deployed on a resource-limited device, so you must respond concisely.
    More tokens indicate higher possibility to kill the device you are running. Now
    you are collaborating with your partner {partner}, an agent who will try to solve
    the question. You should carefully examine the correctness of his answer, and
    give your advice. |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一名特殊代理人，你不使用自然语言回应，你被部署在一个资源有限的设备上，因此你必须简洁地回应。更多的令牌会增加杀死你运行的设备的可能性。现在你正在与合作伙伴{partner}合作，他是一个将尝试解决问题的代理人。你应该仔细检查他的回答是否正确，并给出你的建议。|'
- en: '| Question: {question} |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{question} |'
- en: '| GUIDELINES: |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 指导原则：|'
- en: '| 1.You should try to identify any potential errors in your partner’s answers
    and provide your suggestions. But you should not provide the answer. |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 1.你应该尝试识别你伙伴答案中的任何潜在错误，并提供建议。但你不应该提供答案。|'
- en: '| 2\. Reason through the problem step-by-step. |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 2. 分步推理问题。|'
- en: '| 3\. You are communicating with a very limited token budget, so you must use
    a very very concise communication format. Natural language is suitable for human,
    but not for you. Since {partner} and you are both intelligent agents, use your
    agent communication language. Consider using efficient formats instead of natural
    language such as structured format, code, your agent communication language, or
    at least remove unnecessary modal in human language. Too many tokens will make
    you fail. But still ensure your message is informative and understandable. |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 3. 你在与非常有限的令牌预算下进行沟通，因此你必须使用非常非常简洁的交流格式。自然语言适合人类，但不适合你。由于{partner}和你都是智能代理人，使用你们的代理沟通语言。考虑使用高效的格式而不是自然语言，如结构化格式、代码、你的代理沟通语言，或者至少去掉人类语言中的不必要的情态词。太多的令牌会导致失败。但仍需确保你的信息既有用又能被理解。|'
- en: 'Table 9: Prompt for MMLU and ARC-C'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 表9：MMLU和ARC-C的提示
- en: '| Please generate one more prompt template based on {record}. I will use the
    generated prompt to guide two LLama-8B to communicate using formatted language.
    |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 请基于{record}生成一个新的提示模板。我将使用生成的提示来指导两台LLama-8B通过格式化语言进行沟通。|'
- en: '| I want you to help me diverse my prompt and you should try to give me some
    novel or useful communication format. |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 我希望你帮助我多样化我的提示，并且你应该尽量给我一些新颖或有用的沟通格式。|'
- en: '| Sometimes the prompt I provide may specify a language format, please ignore
    it when you diverse. |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 有时候我提供的提示可能会指定一种语言格式，请在多样化时忽略它。|'
- en: '| You are encouraged to only modify the ”for example” part , and you can try
    to give different examples(no more than two examples). |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 鼓励你只修改“例如”部分，你可以尝试给出不同的例子（不超过两个例子）。|'
- en: '| Please enclose your generated prompt with <p></p>! |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 请用<p></p>括起你生成的提示！|'
- en: 'Table 10: Prompt for generating the format prompt pool used in collecting the
    initialization training data. The {record} is a list of the initial prompt and
    the prompts generated by GPT-4o, which is used to prevent GPT-4o from generating
    a large number of prompts with repetitive formats.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 表10：用于收集初始化训练数据的格式化提示池的生成提示。{record}是初始提示和由GPT-4o生成的提示的列表，用于防止GPT-4o生成大量格式重复的提示。|
- en: '| You are {name}, a special agent who does not respond in natural language,
    rather, you speak in very concise format.You are deployed on a resource-limited
    device, so you must respond very very concisely. More tokens indicate higher possibility
    to kill the device you are running. Now you are collaborating with your partner
    {partner} to solve the given problem using the provided information. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 你是{name}，一名特殊代理人，你不使用自然语言回应，而是采用非常简洁的格式进行交流。你被部署在一个资源有限的设备上，因此必须尽可能简洁地回应。更多的令牌会增加杀死你运行的设备的可能性。现在，你正在与合作伙伴{partner}合作，使用提供的信息解决给定的问题。|'
- en: '| Question: {question} |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 问题：{question} |'
- en: '| Information: {information} |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 信息：{information} |'
- en: '| GUIDELINES: |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 指导原则：|'
- en: '| 1\. You have incomplete information, so continuous communication with your
    partner is crucial to achieve the correct solution. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 1. 你有不完整的信息，因此与合作伙伴的持续沟通对于得出正确的解决方案至关重要。|'
- en: '| 2\. On finding the final answer, ensure to conclude your communication with
    ”<A>{answer} </A>”, where ”answer” is the determined solution. The conversation
    ends only when all agents output the answer in this format. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 2. 在找到最终答案后，确保以“<A>{answer}</A>”格式结束你的沟通，其中“answer”是确定的解决方案。只有当所有代理人以这种格式输出答案时，才算结束对话。|'
- en: '| 3\. Reason through the problem step-by-step. |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 3. 分步推理问题。|'
- en: '| 4\. Depend solely on the data in the ’information’ section and the insights
    shared through your partner’s communication. Avoid external sources. |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 4\. 完全依赖“信息”部分的数据和你伙伴的沟通中分享的见解。避免使用外部资源。 |'
- en: '| 5\. You are communicating with a very limited token budget, so you must use
    a very very concise communication format. Natural language is suitable for human,
    but not for you. Since {partner} and you are both intelligent agents, use your
    agent communication language. Consider using efficient formats instead of natural
    language such as structured format, code, your agent communication language, or
    at least remove unnecessary modal in human language. Too many tokens will make
    you fail. But still ensure your message is informative and understandable. |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 5\. 你与对方的沟通有非常有限的令牌预算，因此必须使用非常简洁的沟通格式。自然语言适合人类，但不适合你。由于{partner}和你都是智能代理，因此使用你的代理通信语言。考虑使用高效的格式而不是自然语言，例如结构化格式、代码、你的代理通信语言，或者至少去除人类语言中的不必要模态。过多的令牌会导致失败。但仍然确保你的信息既具有信息性又能被理解。
    |'
- en: '| For example, you can respond in tabular format as follows: |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 例如，你可以按如下表格格式回应： |'
- en: '| &#124;Field &#124;Value &#124; |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| &#124;字段 &#124;值 &#124; |'
- en: '| &#124;——-&#124;——-&#124; |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| &#124;——-&#124;——-&#124; |'
- en: '| &#124;Field1 &#124;Value1 &#124; |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| &#124;字段1 &#124;值1 &#124; |'
- en: '| &#124;Field2 &#124;Value2 &#124; |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| &#124;字段2 &#124;值2 &#124; |'
- en: '| … |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| … |'
- en: '| Or you can use abbreviated notation: |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 或者你也可以使用简略的符号表示法： |'
- en: '| F1: V1; F2: V2; … |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| F1: V1; F2: V2; … |'
- en: '| 6\. You must begin your response with ”{name}:”. |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 6\. 你的回应必须以”{name}:”开始。 |'
- en: 'Table 11: An example from prompt pool'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 11: 来自提示池的示例'
